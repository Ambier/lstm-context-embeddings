sh: error while loading shared libraries: __vdso_time: invalid mode for dlopen(): Invalid argument
sh: error while loading shared libraries: __vdso_time: invalid mode for dlopen(): Invalid argument
sh: error while loading shared libraries: __vdso_time: invalid mode for dlopen(): Invalid argument
sh: error while loading shared libraries: __vdso_time: invalid mode for dlopen(): Invalid argument
WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f6304f95f10>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.
WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f6304f95bd0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.

Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=64
CHECKPOINT_EVERY=100
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=300
EVALUATE_EVERY=100
HIDDEN_DIM=300
L2_REG_LAMBDA=0.0
LOG_DEVICE_PLACEMENT=False
NUM_EPOCHS=200
WORD2VEC=None

Loading data...
Vocabulary Size: 18758
Train/Dev split: 9662/1000
Writing to /home/chaitanya/lstm-context-embeddings/runs/1469600026

2016-07-27T14:13:51.298475: step 1, loss 0.70131, acc 0.453125
2016-07-27T14:13:53.693515: step 2, loss 0.697427, acc 0.59375
2016-07-27T14:13:56.292057: step 3, loss 0.691292, acc 0.5
2016-07-27T14:13:58.747692: step 4, loss 0.708133, acc 0.515625
2016-07-27T14:14:01.156980: step 5, loss 0.709842, acc 0.4375
2016-07-27T14:14:04.019144: step 6, loss 0.695489, acc 0.515625
2016-07-27T14:14:06.297891: step 7, loss 0.737298, acc 0.5
2016-07-27T14:14:08.655236: step 8, loss 0.697278, acc 0.515625
2016-07-27T14:14:10.996933: step 9, loss 0.723997, acc 0.5
2016-07-27T14:14:13.358714: step 10, loss 0.66837, acc 0.578125
2016-07-27T14:14:15.690156: step 11, loss 0.712028, acc 0.46875
2016-07-27T14:14:18.004235: step 12, loss 0.783619, acc 0.5
2016-07-27T14:14:20.355083: step 13, loss 0.687151, acc 0.546875
2016-07-27T14:14:22.653561: step 14, loss 0.695361, acc 0.53125
2016-07-27T14:14:25.009204: step 15, loss 0.70158, acc 0.53125
2016-07-27T14:14:27.353380: step 16, loss 0.686949, acc 0.515625
2016-07-27T14:14:29.711856: step 17, loss 0.695195, acc 0.578125
2016-07-27T14:14:32.073218: step 18, loss 0.694415, acc 0.53125
2016-07-27T14:14:34.420035: step 19, loss 0.732673, acc 0.53125
2016-07-27T14:14:36.969432: step 20, loss 0.700185, acc 0.5625
2016-07-27T14:14:39.346024: step 21, loss 0.68305, acc 0.53125
2016-07-27T14:14:41.741608: step 22, loss 0.70707, acc 0.5
2016-07-27T14:14:44.085432: step 23, loss 0.715479, acc 0.5
2016-07-27T14:14:46.418814: step 24, loss 0.649605, acc 0.625
2016-07-27T14:14:49.240779: step 25, loss 0.677929, acc 0.578125
2016-07-27T14:14:52.019179: step 26, loss 0.716251, acc 0.5625
2016-07-27T14:14:54.440393: step 27, loss 0.698493, acc 0.53125
2016-07-27T14:14:56.830470: step 28, loss 0.743752, acc 0.375
2016-07-27T14:14:59.712647: step 29, loss 0.690832, acc 0.578125
2016-07-27T14:15:02.400986: step 30, loss 0.682257, acc 0.5625
2016-07-27T14:15:04.836489: step 31, loss 0.734857, acc 0.5
2016-07-27T14:15:07.331488: step 32, loss 0.687264, acc 0.609375
2016-07-27T14:15:09.959434: step 33, loss 0.661041, acc 0.578125
2016-07-27T14:15:12.668913: step 34, loss 0.668821, acc 0.546875
2016-07-27T14:15:15.662236: step 35, loss 0.67411, acc 0.5625
2016-07-27T14:15:18.024849: step 36, loss 0.733256, acc 0.484375
2016-07-27T14:15:20.498814: step 37, loss 0.721972, acc 0.4375
2016-07-27T14:15:23.029927: step 38, loss 0.683052, acc 0.5625
2016-07-27T14:15:25.949309: step 39, loss 0.660447, acc 0.59375
2016-07-27T14:15:28.974326: step 40, loss 0.710633, acc 0.46875
2016-07-27T14:15:31.549079: step 41, loss 0.646783, acc 0.640625
2016-07-27T14:15:34.312719: step 42, loss 0.686584, acc 0.484375
2016-07-27T14:15:37.927935: step 43, loss 0.696799, acc 0.53125
2016-07-27T14:15:40.968404: step 44, loss 0.699664, acc 0.59375
2016-07-27T14:15:43.820468: step 45, loss 0.684733, acc 0.546875
2016-07-27T14:15:46.150237: step 46, loss 0.666351, acc 0.640625
2016-07-27T14:15:48.654614: step 47, loss 0.714772, acc 0.484375
2016-07-27T14:15:51.820570: step 48, loss 0.693696, acc 0.5625
2016-07-27T14:15:54.861524: step 49, loss 0.709976, acc 0.515625
2016-07-27T14:15:57.519774: step 50, loss 0.671781, acc 0.53125
2016-07-27T14:16:00.171669: step 51, loss 0.669388, acc 0.5625
2016-07-27T14:16:03.173641: step 52, loss 0.672375, acc 0.609375
2016-07-27T14:16:05.746208: step 53, loss 0.726439, acc 0.515625
2016-07-27T14:16:08.631994: step 54, loss 0.675218, acc 0.546875
2016-07-27T14:16:11.337326: step 55, loss 0.667556, acc 0.65625
2016-07-27T14:16:13.640945: step 56, loss 0.690307, acc 0.53125
2016-07-27T14:16:16.664530: step 57, loss 0.692444, acc 0.53125
2016-07-27T14:16:19.352641: step 58, loss 0.700414, acc 0.53125
2016-07-27T14:16:21.880986: step 59, loss 0.643603, acc 0.625
2016-07-27T14:16:25.032569: step 60, loss 0.711625, acc 0.5
2016-07-27T14:16:27.947990: step 61, loss 0.737037, acc 0.5625
2016-07-27T14:16:30.976885: step 62, loss 0.673774, acc 0.546875
2016-07-27T14:16:33.856653: step 63, loss 0.705569, acc 0.578125
2016-07-27T14:16:36.268446: step 64, loss 0.692455, acc 0.484375
2016-07-27T14:16:39.054975: step 65, loss 0.648167, acc 0.6875
2016-07-27T14:16:41.883118: step 66, loss 0.702746, acc 0.515625
2016-07-27T14:16:44.318797: step 67, loss 0.646951, acc 0.640625
2016-07-27T14:16:47.163496: step 68, loss 0.673319, acc 0.625
2016-07-27T14:16:50.115065: step 69, loss 0.669678, acc 0.609375
2016-07-27T14:16:52.601586: step 70, loss 0.663473, acc 0.640625
2016-07-27T14:16:55.266047: step 71, loss 0.652017, acc 0.5625
2016-07-27T14:16:58.219879: step 72, loss 0.694444, acc 0.609375
2016-07-27T14:17:01.362015: step 73, loss 0.667798, acc 0.578125
2016-07-27T14:17:04.404901: step 74, loss 0.625487, acc 0.640625
2016-07-27T14:17:07.212593: step 75, loss 0.619985, acc 0.671875
2016-07-27T14:17:10.098676: step 76, loss 0.691066, acc 0.578125
2016-07-27T14:17:13.028837: step 77, loss 0.672603, acc 0.484375
2016-07-27T14:17:15.694079: step 78, loss 0.700079, acc 0.53125
2016-07-27T14:17:18.223634: step 79, loss 0.607549, acc 0.59375
2016-07-27T14:17:21.056901: step 80, loss 0.65687, acc 0.5625
2016-07-27T14:17:23.688794: step 81, loss 0.713365, acc 0.515625
2016-07-27T14:17:26.208966: step 82, loss 0.701767, acc 0.53125
2016-07-27T14:17:29.059589: step 83, loss 0.646901, acc 0.625
2016-07-27T14:17:31.746510: step 84, loss 0.687933, acc 0.59375
2016-07-27T14:17:34.308964: step 85, loss 0.60053, acc 0.765625
2016-07-27T14:17:37.146479: step 86, loss 0.637146, acc 0.640625
2016-07-27T14:17:39.619520: step 87, loss 0.680804, acc 0.5625
2016-07-27T14:17:42.406145: step 88, loss 0.636664, acc 0.625
2016-07-27T14:17:45.210818: step 89, loss 0.669035, acc 0.546875
2016-07-27T14:17:47.636528: step 90, loss 0.669572, acc 0.671875
2016-07-27T14:17:50.497337: step 91, loss 0.659125, acc 0.578125
2016-07-27T14:17:53.099966: step 92, loss 0.672716, acc 0.609375
2016-07-27T14:17:55.756173: step 93, loss 0.64202, acc 0.609375
2016-07-27T14:17:58.576424: step 94, loss 0.664347, acc 0.625
2016-07-27T14:18:01.070673: step 95, loss 0.669258, acc 0.640625
2016-07-27T14:18:03.576380: step 96, loss 0.640842, acc 0.5625
2016-07-27T14:18:06.429314: step 97, loss 0.618617, acc 0.65625
2016-07-27T14:18:09.121797: step 98, loss 0.687437, acc 0.625
2016-07-27T14:18:11.985756: step 99, loss 0.637519, acc 0.609375
2016-07-27T14:18:14.711760: step 100, loss 0.615418, acc 0.625

Evaluation:
2016-07-27T14:18:26.730118: step 100, loss 0.674767, acc 0.594

Saved model checkpoint to /home/chaitanya/lstm-context-embeddings/runs/1469600026/checkpoints/model-100

2016-07-27T14:18:31.026282: step 101, loss 0.686327, acc 0.53125
2016-07-27T14:18:33.784693: step 102, loss 0.650553, acc 0.609375
2016-07-27T14:18:36.512704: step 103, loss 0.67393, acc 0.515625
2016-07-27T14:18:39.009417: step 104, loss 0.68148, acc 0.515625
2016-07-27T14:18:41.819577: step 105, loss 0.701854, acc 0.5
2016-07-27T14:18:44.369997: step 106, loss 0.620953, acc 0.6875
2016-07-27T14:18:47.098822: step 107, loss 0.673118, acc 0.5625
2016-07-27T14:18:50.021275: step 108, loss 0.6473, acc 0.640625
2016-07-27T14:18:52.482141: step 109, loss 0.676538, acc 0.5625
2016-07-27T14:18:55.423377: step 110, loss 0.698153, acc 0.640625
2016-07-27T14:18:58.155682: step 111, loss 0.69786, acc 0.625
2016-07-27T14:19:00.799995: step 112, loss 0.662144, acc 0.59375
2016-07-27T14:19:03.627811: step 113, loss 0.671689, acc 0.578125
2016-07-27T14:19:06.128580: step 114, loss 0.650409, acc 0.59375
2016-07-27T14:19:09.036517: step 115, loss 0.693853, acc 0.703125
2016-07-27T14:19:11.780269: step 116, loss 0.655479, acc 0.65625
2016-07-27T14:19:14.265279: step 117, loss 0.724784, acc 0.546875
2016-07-27T14:19:17.130200: step 118, loss 0.688725, acc 0.578125
2016-07-27T14:19:19.803257: step 119, loss 0.692115, acc 0.546875
2016-07-27T14:19:22.280135: step 120, loss 0.657127, acc 0.640625
2016-07-27T14:19:24.858538: step 121, loss 0.724584, acc 0.515625
2016-07-27T14:19:28.655601: step 122, loss 0.618221, acc 0.625
2016-07-27T14:19:31.615705: step 123, loss 0.635787, acc 0.6875
2016-07-27T14:19:34.396583: step 124, loss 0.661977, acc 0.5625
2016-07-27T14:19:36.756546: step 125, loss 0.624424, acc 0.6875
2016-07-27T14:19:39.345314: step 126, loss 0.658404, acc 0.546875
2016-07-27T14:19:42.159650: step 127, loss 0.657617, acc 0.59375
2016-07-27T14:19:44.825690: step 128, loss 0.705666, acc 0.46875
2016-07-27T14:19:47.408038: step 129, loss 0.669959, acc 0.65625
2016-07-27T14:19:50.231385: step 130, loss 0.656306, acc 0.59375
2016-07-27T14:19:52.880145: step 131, loss 0.60068, acc 0.671875
2016-07-27T14:19:55.423611: step 132, loss 0.713902, acc 0.515625
2016-07-27T14:19:58.307404: step 133, loss 0.649799, acc 0.59375
2016-07-27T14:20:00.861371: step 134, loss 0.670864, acc 0.546875
2016-07-27T14:20:03.595334: step 135, loss 0.717527, acc 0.515625
2016-07-27T14:20:06.433930: step 136, loss 0.62395, acc 0.671875
2016-07-27T14:20:08.941940: step 137, loss 0.708994, acc 0.515625
2016-07-27T14:20:11.782053: step 138, loss 0.653713, acc 0.625
2016-07-27T14:20:14.408970: step 139, loss 0.618739, acc 0.640625
2016-07-27T14:20:17.272998: step 140, loss 0.663979, acc 0.65625
2016-07-27T14:20:19.966439: step 141, loss 0.624168, acc 0.640625
2016-07-27T14:20:22.526429: step 142, loss 0.701059, acc 0.59375
2016-07-27T14:20:25.356004: step 143, loss 0.710138, acc 0.515625
2016-07-27T14:20:27.883092: step 144, loss 0.66007, acc 0.59375
2016-07-27T14:20:30.621532: step 145, loss 0.62162, acc 0.65625
2016-07-27T14:20:33.450145: step 146, loss 0.694623, acc 0.546875
2016-07-27T14:20:35.885558: step 147, loss 0.676676, acc 0.578125
2016-07-27T14:20:38.699782: step 148, loss 0.652711, acc 0.65625
2016-07-27T14:20:41.336741: step 149, loss 0.698118, acc 0.546875
2016-07-27T14:20:43.914099: step 150, loss 0.648502, acc 0.640625
2016-07-27T14:20:46.821662: step 151, loss 0.725982, acc 0.580645
2016-07-27T14:20:49.599823: step 152, loss 0.654443, acc 0.671875
2016-07-27T14:20:52.079190: step 153, loss 0.660935, acc 0.6875
2016-07-27T14:20:54.877297: step 154, loss 0.637166, acc 0.59375
2016-07-27T14:20:57.551077: step 155, loss 0.663369, acc 0.625
2016-07-27T14:21:00.114932: step 156, loss 0.670643, acc 0.578125
2016-07-27T14:21:02.836918: step 157, loss 0.66817, acc 0.609375
2016-07-27T14:21:05.579408: step 158, loss 0.647389, acc 0.65625
2016-07-27T14:21:08.269309: step 159, loss 0.615578, acc 0.609375
2016-07-27T14:21:11.115970: step 160, loss 0.622532, acc 0.65625
2016-07-27T14:21:13.611996: step 161, loss 0.610672, acc 0.6875
2016-07-27T14:21:16.237171: step 162, loss 0.658024, acc 0.703125
2016-07-27T14:21:19.181512: step 163, loss 0.586424, acc 0.6875
2016-07-27T14:21:21.776911: step 164, loss 0.61895, acc 0.734375
2016-07-27T14:21:24.640320: step 165, loss 0.637617, acc 0.65625
2016-07-27T14:21:27.481762: step 166, loss 0.59341, acc 0.6875
2016-07-27T14:21:29.935599: step 167, loss 0.613508, acc 0.65625
2016-07-27T14:21:32.543516: step 168, loss 0.698779, acc 0.546875
2016-07-27T14:21:35.333243: step 169, loss 0.625236, acc 0.6875
2016-07-27T14:21:37.951217: step 170, loss 0.653083, acc 0.59375
2016-07-27T14:21:40.613705: step 171, loss 0.685224, acc 0.625
2016-07-27T14:21:43.440186: step 172, loss 0.625795, acc 0.703125
2016-07-27T14:21:46.142148: step 173, loss 0.631001, acc 0.625
2016-07-27T14:21:48.869887: step 174, loss 0.625686, acc 0.59375
2016-07-27T14:21:51.662014: step 175, loss 0.618667, acc 0.6875
2016-07-27T14:21:54.081609: step 176, loss 0.653447, acc 0.65625
2016-07-27T14:21:56.898211: step 177, loss 0.653101, acc 0.65625
2016-07-27T14:21:59.595022: step 178, loss 0.654369, acc 0.59375
2016-07-27T14:22:02.390103: step 179, loss 0.647841, acc 0.609375
2016-07-27T14:22:05.133634: step 180, loss 0.665667, acc 0.53125
2016-07-27T14:22:07.773315: step 181, loss 0.655834, acc 0.625
2016-07-27T14:22:10.632063: step 182, loss 0.631955, acc 0.53125
2016-07-27T14:22:13.206571: step 183, loss 0.612315, acc 0.71875
2016-07-27T14:22:15.976220: step 184, loss 0.614597, acc 0.625
2016-07-27T14:22:18.941845: step 185, loss 0.691002, acc 0.5625
2016-07-27T14:22:21.416109: step 186, loss 0.670784, acc 0.59375
2016-07-27T14:22:24.238690: step 187, loss 0.616512, acc 0.6875
2016-07-27T14:22:26.888998: step 188, loss 0.649145, acc 0.671875
2016-07-27T14:22:29.762456: step 189, loss 0.636198, acc 0.59375
2016-07-27T14:22:32.431548: step 190, loss 0.750805, acc 0.5625
2016-07-27T14:22:34.962714: step 191, loss 0.634074, acc 0.6875
2016-07-27T14:22:37.693958: step 192, loss 0.680588, acc 0.6875
2016-07-27T14:22:41.296926: step 193, loss 0.608495, acc 0.71875
2016-07-27T14:22:44.198978: step 194, loss 0.653178, acc 0.65625
2016-07-27T14:22:46.971375: step 195, loss 0.681994, acc 0.53125
2016-07-27T14:22:49.440611: step 196, loss 0.661052, acc 0.578125
2016-07-27T14:22:52.027718: step 197, loss 0.668821, acc 0.609375
2016-07-27T14:22:54.843467: step 198, loss 0.654689, acc 0.625
2016-07-27T14:22:57.529461: step 199, loss 0.679615, acc 0.640625
2016-07-27T14:23:00.134925: step 200, loss 0.625332, acc 0.796875

Evaluation:
2016-07-27T14:23:12.491740: step 200, loss 0.657282, acc 0.62

Saved model checkpoint to /home/chaitanya/lstm-context-embeddings/runs/1469600026/checkpoints/model-200

2016-07-27T14:23:16.927202: step 201, loss 0.630114, acc 0.609375
2016-07-27T14:23:19.599030: step 202, loss 0.617307, acc 0.671875
2016-07-27T14:23:22.453081: step 203, loss 0.681887, acc 0.5625
2016-07-27T14:23:24.880941: step 204, loss 0.623791, acc 0.671875
2016-07-27T14:23:27.834323: step 205, loss 0.639775, acc 0.59375
2016-07-27T14:23:30.493845: step 206, loss 0.582899, acc 0.78125
2016-07-27T14:23:32.999056: step 207, loss 0.590605, acc 0.703125
2016-07-27T14:23:35.791190: step 208, loss 0.637829, acc 0.640625
2016-07-27T14:23:38.338965: step 209, loss 0.610274, acc 0.6875
2016-07-27T14:23:41.007989: step 210, loss 0.65997, acc 0.671875
2016-07-27T14:23:43.817233: step 211, loss 0.677444, acc 0.59375
2016-07-27T14:23:46.267608: step 212, loss 0.630189, acc 0.6875
2016-07-27T14:23:49.141427: step 213, loss 0.634159, acc 0.65625
2016-07-27T14:23:51.744394: step 214, loss 0.58755, acc 0.65625
2016-07-27T14:23:54.545205: step 215, loss 0.694945, acc 0.53125
2016-07-27T14:23:57.255975: step 216, loss 0.656755, acc 0.640625
2016-07-27T14:23:59.799038: step 217, loss 0.589375, acc 0.703125
2016-07-27T14:24:02.615857: step 218, loss 0.615388, acc 0.671875
2016-07-27T14:24:05.180446: step 219, loss 0.594771, acc 0.671875
2016-07-27T14:24:08.008081: step 220, loss 0.698367, acc 0.546875
2016-07-27T14:24:10.864236: step 221, loss 0.581487, acc 0.734375
2016-07-27T14:24:13.306826: step 222, loss 0.682186, acc 0.5625
2016-07-27T14:24:16.145510: step 223, loss 0.582451, acc 0.6875
2016-07-27T14:24:18.825740: step 224, loss 0.639556, acc 0.671875
2016-07-27T14:24:21.583176: step 225, loss 0.584903, acc 0.734375
2016-07-27T14:24:24.267895: step 226, loss 0.637109, acc 0.65625
2016-07-27T14:24:26.790018: step 227, loss 0.642077, acc 0.625
2016-07-27T14:24:29.381131: step 228, loss 0.591359, acc 0.71875
2016-07-27T14:24:32.185043: step 229, loss 0.664962, acc 0.625
2016-07-27T14:24:34.775046: step 230, loss 0.601539, acc 0.640625
2016-07-27T14:24:37.334166: step 231, loss 0.582381, acc 0.703125
2016-07-27T14:24:40.180649: step 232, loss 0.597459, acc 0.6875
2016-07-27T14:24:42.830873: step 233, loss 0.581414, acc 0.71875
2016-07-27T14:24:45.588467: step 234, loss 0.697839, acc 0.609375
2016-07-27T14:24:48.339602: step 235, loss 0.656483, acc 0.625
2016-07-27T14:24:50.820170: step 236, loss 0.618562, acc 0.609375
2016-07-27T14:24:53.608029: step 237, loss 0.568936, acc 0.703125
2016-07-27T14:24:56.252160: step 238, loss 0.648483, acc 0.59375
2016-07-27T14:24:59.154409: step 239, loss 0.613792, acc 0.734375
2016-07-27T14:25:01.804865: step 240, loss 0.580001, acc 0.640625
2016-07-27T14:25:04.609318: step 241, loss 0.72945, acc 0.578125
2016-07-27T14:25:07.400014: step 242, loss 0.628798, acc 0.640625
2016-07-27T14:25:10.037793: step 243, loss 0.557155, acc 0.734375
2016-07-27T14:25:13.170278: step 244, loss 0.65854, acc 0.578125
2016-07-27T14:25:15.925621: step 245, loss 0.650254, acc 0.65625
2016-07-27T14:25:19.035304: step 246, loss 0.682707, acc 0.5625
2016-07-27T14:25:21.743881: step 247, loss 0.60392, acc 0.734375
2016-07-27T14:25:24.311442: step 248, loss 0.547402, acc 0.734375
2016-07-27T14:25:27.093688: step 249, loss 0.62537, acc 0.65625
2016-07-27T14:25:29.655612: step 250, loss 0.751655, acc 0.515625
2016-07-27T14:25:32.347296: step 251, loss 0.684054, acc 0.53125
2016-07-27T14:25:35.199680: step 252, loss 0.684632, acc 0.6875
2016-07-27T14:25:37.860280: step 253, loss 0.59907, acc 0.671875
2016-07-27T14:25:40.469572: step 254, loss 0.703135, acc 0.53125
2016-07-27T14:25:43.281213: step 255, loss 0.687401, acc 0.59375
2016-07-27T14:25:46.071860: step 256, loss 0.644224, acc 0.671875
2016-07-27T14:25:48.563371: step 257, loss 0.632235, acc 0.703125
2016-07-27T14:25:51.410980: step 258, loss 0.528491, acc 0.75
2016-07-27T14:25:54.068629: step 259, loss 0.636109, acc 0.671875
2016-07-27T14:25:56.882324: step 260, loss 0.641149, acc 0.640625
2016-07-27T14:25:59.613499: step 261, loss 0.581042, acc 0.6875
2016-07-27T14:26:02.168574: step 262, loss 0.59602, acc 0.6875
2016-07-27T14:26:05.016186: step 263, loss 0.660058, acc 0.640625
2016-07-27T14:26:07.662417: step 264, loss 0.662178, acc 0.578125
2016-07-27T14:26:10.364499: step 265, loss 0.558262, acc 0.734375
2016-07-27T14:26:13.224465: step 266, loss 0.644146, acc 0.671875
2016-07-27T14:26:15.904742: step 267, loss 0.625165, acc 0.640625
2016-07-27T14:26:18.496554: step 268, loss 0.623466, acc 0.65625
2016-07-27T14:26:21.368659: step 269, loss 0.557666, acc 0.765625
2016-07-27T14:26:23.890436: step 270, loss 0.576, acc 0.71875
2016-07-27T14:26:26.508976: step 271, loss 0.565264, acc 0.71875
2016-07-27T14:26:29.314823: step 272, loss 0.66459, acc 0.59375
2016-07-27T14:26:32.011248: step 273, loss 0.570582, acc 0.671875
2016-07-27T14:26:34.823183: step 274, loss 0.597904, acc 0.734375
2016-07-27T14:26:37.565998: step 275, loss 0.652693, acc 0.6875
2016-07-27T14:26:40.117600: step 276, loss 0.624376, acc 0.671875
2016-07-27T14:26:42.942151: step 277, loss 0.585855, acc 0.6875
2016-07-27T14:26:45.568331: step 278, loss 0.684684, acc 0.59375
2016-07-27T14:26:48.145577: step 279, loss 0.617341, acc 0.640625
2016-07-27T14:26:50.909658: step 280, loss 0.632491, acc 0.625
2016-07-27T14:26:53.691419: step 281, loss 0.613647, acc 0.65625
2016-07-27T14:26:56.266916: step 282, loss 0.608991, acc 0.625
2016-07-27T14:26:59.157521: step 283, loss 0.594115, acc 0.71875
2016-07-27T14:27:01.888959: step 284, loss 0.583241, acc 0.71875
2016-07-27T14:27:04.439160: step 285, loss 0.5966, acc 0.640625
2016-07-27T14:27:07.271547: step 286, loss 0.597044, acc 0.703125
2016-07-27T14:27:09.877775: step 287, loss 0.577401, acc 0.65625
2016-07-27T14:27:12.508279: step 288, loss 0.668012, acc 0.703125
2016-07-27T14:27:15.349523: step 289, loss 0.637047, acc 0.640625
2016-07-27T14:27:18.025933: step 290, loss 0.691622, acc 0.53125
2016-07-27T14:27:20.611727: step 291, loss 0.605232, acc 0.671875
2016-07-27T14:27:23.421518: step 292, loss 0.716432, acc 0.484375
2016-07-27T14:27:26.047630: step 293, loss 0.531954, acc 0.78125
2016-07-27T14:27:28.666604: step 294, loss 0.610987, acc 0.625
2016-07-27T14:27:31.508761: step 295, loss 0.659011, acc 0.671875
2016-07-27T14:27:34.176327: step 296, loss 0.655933, acc 0.578125
2016-07-27T14:27:36.933330: step 297, loss 0.547055, acc 0.734375
2016-07-27T14:27:39.675785: step 298, loss 0.623327, acc 0.671875
2016-07-27T14:27:42.179328: step 299, loss 0.618809, acc 0.6875
2016-07-27T14:27:45.033151: step 300, loss 0.589659, acc 0.75

Evaluation:
2016-07-27T14:27:56.947700: step 300, loss 0.628721, acc 0.643

Saved model checkpoint to /home/chaitanya/lstm-context-embeddings/runs/1469600026/checkpoints/model-300

2016-07-27T14:28:00.977667: step 301, loss 0.604384, acc 0.71875
2016-07-27T14:28:03.826922: step 302, loss 0.62286, acc 0.66129
2016-07-27T14:28:06.384575: step 303, loss 0.605519, acc 0.671875
2016-07-27T14:28:09.199227: step 304, loss 0.541816, acc 0.71875
2016-07-27T14:28:11.890582: step 305, loss 0.581607, acc 0.625
2016-07-27T14:28:14.567185: step 306, loss 0.521259, acc 0.765625
2016-07-27T14:28:17.398415: step 307, loss 0.58075, acc 0.734375
2016-07-27T14:28:19.994453: step 308, loss 0.620991, acc 0.703125
2016-07-27T14:28:22.673406: step 309, loss 0.585023, acc 0.734375
2016-07-27T14:28:25.543259: step 310, loss 0.604799, acc 0.765625
2016-07-27T14:28:28.211379: step 311, loss 0.662245, acc 0.6875
2016-07-27T14:28:30.798235: step 312, loss 0.666492, acc 0.625
2016-07-27T14:28:33.626254: step 313, loss 0.560226, acc 0.6875
2016-07-27T14:28:36.253518: step 314, loss 0.586694, acc 0.703125
2016-07-27T14:28:39.048415: step 315, loss 0.611494, acc 0.640625
2016-07-27T14:28:41.815830: step 316, loss 0.563471, acc 0.6875
2016-07-27T14:28:44.255191: step 317, loss 0.554268, acc 0.703125
2016-07-27T14:28:47.089213: step 318, loss 0.577273, acc 0.78125
2016-07-27T14:28:49.699937: step 319, loss 0.591049, acc 0.75
2016-07-27T14:28:52.529798: step 320, loss 0.527367, acc 0.765625
2016-07-27T14:28:55.332141: step 321, loss 0.631957, acc 0.625
2016-07-27T14:28:57.870492: step 322, loss 0.582892, acc 0.6875
2016-07-27T14:29:00.653511: step 323, loss 0.588742, acc 0.75
2016-07-27T14:29:03.308308: step 324, loss 0.529184, acc 0.765625
2016-07-27T14:29:05.862184: step 325, loss 0.561085, acc 0.703125
2016-07-27T14:29:08.640132: step 326, loss 0.659905, acc 0.671875
2016-07-27T14:29:11.186129: step 327, loss 0.612469, acc 0.6875
2016-07-27T14:29:13.923633: step 328, loss 0.561095, acc 0.734375
2016-07-27T14:29:16.722451: step 329, loss 0.543108, acc 0.75
2016-07-27T14:29:19.419199: step 330, loss 0.592811, acc 0.734375
2016-07-27T14:29:22.221626: step 331, loss 0.435908, acc 0.84375
2016-07-27T14:29:24.712577: step 332, loss 0.615192, acc 0.71875
2016-07-27T14:29:27.604640: step 333, loss 0.557034, acc 0.703125
2016-07-27T14:29:30.294866: step 334, loss 0.622424, acc 0.640625
2016-07-27T14:29:32.835803: step 335, loss 0.566007, acc 0.65625
2016-07-27T14:29:35.650632: step 336, loss 0.550529, acc 0.6875
2016-07-27T14:29:38.228058: step 337, loss 0.573874, acc 0.734375
2016-07-27T14:29:40.892224: step 338, loss 0.597837, acc 0.703125
2016-07-27T14:29:43.809046: step 339, loss 0.634177, acc 0.640625
2016-07-27T14:29:46.465928: step 340, loss 0.524642, acc 0.703125
2016-07-27T14:29:49.055060: step 341, loss 0.545639, acc 0.703125
2016-07-27T14:29:51.737166: step 342, loss 0.673215, acc 0.59375
2016-07-27T14:29:54.370283: step 343, loss 0.50099, acc 0.78125
2016-07-27T14:29:57.177286: step 344, loss 0.676409, acc 0.640625
2016-07-27T14:29:59.729990: step 345, loss 0.59878, acc 0.765625
2016-07-27T14:30:02.658663: step 346, loss 0.612999, acc 0.6875
2016-07-27T14:30:05.353801: step 347, loss 0.615003, acc 0.703125
2016-07-27T14:30:08.001997: step 348, loss 0.51691, acc 0.8125
2016-07-27T14:30:10.804100: step 349, loss 0.644066, acc 0.625
2016-07-27T14:30:13.513675: step 350, loss 0.547206, acc 0.78125
2016-07-27T14:30:16.336571: step 351, loss 0.6269, acc 0.671875
2016-07-27T14:30:19.067631: step 352, loss 0.566949, acc 0.75
2016-07-27T14:30:21.762221: step 353, loss 0.589714, acc 0.6875
2016-07-27T14:30:24.590735: step 354, loss 0.480929, acc 0.75
2016-07-27T14:30:27.140704: step 355, loss 0.558339, acc 0.703125
2016-07-27T14:30:30.004815: step 356, loss 0.509419, acc 0.796875
2016-07-27T14:30:32.643484: step 357, loss 0.498099, acc 0.71875
2016-07-27T14:30:35.476682: step 358, loss 0.649817, acc 0.640625
2016-07-27T14:30:38.208213: step 359, loss 0.509272, acc 0.8125
2016-07-27T14:30:40.724337: step 360, loss 0.460408, acc 0.71875
2016-07-27T14:30:43.530641: step 361, loss 0.674514, acc 0.734375
2016-07-27T14:30:46.092938: step 362, loss 0.521096, acc 0.75
2016-07-27T14:30:48.778105: step 363, loss 0.526993, acc 0.765625
2016-07-27T14:30:51.327783: step 364, loss 0.580983, acc 0.75
2016-07-27T14:30:53.890282: step 365, loss 0.544788, acc 0.75
2016-07-27T14:30:56.712584: step 366, loss 0.589134, acc 0.734375
2016-07-27T14:30:59.318654: step 367, loss 0.559708, acc 0.765625
2016-07-27T14:31:02.051007: step 368, loss 0.580094, acc 0.734375
2016-07-27T14:31:05.040531: step 369, loss 0.483723, acc 0.765625
2016-07-27T14:31:07.703340: step 370, loss 0.568049, acc 0.703125
2016-07-27T14:31:10.455968: step 371, loss 0.533359, acc 0.765625
2016-07-27T14:31:13.259300: step 372, loss 0.560549, acc 0.734375
2016-07-27T14:31:15.749796: step 373, loss 0.63011, acc 0.625
2016-07-27T14:31:18.535406: step 374, loss 0.571448, acc 0.703125
2016-07-27T14:31:21.131305: step 375, loss 0.624272, acc 0.65625
2016-07-27T14:31:23.980554: step 376, loss 0.577746, acc 0.671875
2016-07-27T14:31:26.822669: step 377, loss 0.558636, acc 0.734375
2016-07-27T14:31:29.291993: step 378, loss 0.555959, acc 0.765625
2016-07-27T14:31:32.165662: step 379, loss 0.596418, acc 0.59375
2016-07-27T14:31:34.785027: step 380, loss 0.611406, acc 0.71875
2016-07-27T14:31:37.612026: step 381, loss 0.642869, acc 0.625
2016-07-27T14:31:40.319707: step 382, loss 0.603089, acc 0.6875
2016-07-27T14:31:42.817082: step 383, loss 0.524095, acc 0.75
2016-07-27T14:31:45.647080: step 384, loss 0.535131, acc 0.671875
2016-07-27T14:31:48.178311: step 385, loss 0.505957, acc 0.75
2016-07-27T14:31:50.988746: step 386, loss 0.566332, acc 0.640625
2016-07-27T14:31:53.851166: step 387, loss 0.661284, acc 0.671875
2016-07-27T14:31:56.339730: step 388, loss 0.630544, acc 0.640625
2016-07-27T14:31:59.158723: step 389, loss 0.572037, acc 0.6875
2016-07-27T14:32:01.834989: step 390, loss 0.595183, acc 0.65625
2016-07-27T14:32:04.467332: step 391, loss 0.643167, acc 0.578125
2016-07-27T14:32:07.348108: step 392, loss 0.599588, acc 0.71875
2016-07-27T14:32:09.912200: step 393, loss 0.648602, acc 0.671875
2016-07-27T14:32:12.793291: step 394, loss 0.536456, acc 0.734375
2016-07-27T14:32:15.472810: step 395, loss 0.629202, acc 0.640625
2016-07-27T14:32:17.983457: step 396, loss 0.61312, acc 0.71875
2016-07-27T14:32:20.840566: step 397, loss 0.574304, acc 0.703125
2016-07-27T14:32:23.474857: step 398, loss 0.574655, acc 0.703125
2016-07-27T14:32:26.165101: step 399, loss 0.586052, acc 0.6875
2016-07-27T14:32:28.996114: step 400, loss 0.629473, acc 0.640625

Evaluation:
2016-07-27T14:32:41.417509: step 400, loss 0.620821, acc 0.666

Saved model checkpoint to /home/chaitanya/lstm-context-embeddings/runs/1469600026/checkpoints/model-400

2016-07-27T14:32:45.565085: step 401, loss 0.561448, acc 0.78125
2016-07-27T14:32:48.151682: step 402, loss 0.634361, acc 0.609375
2016-07-27T14:32:50.981247: step 403, loss 0.516235, acc 0.796875
2016-07-27T14:32:53.678435: step 404, loss 0.587331, acc 0.734375
2016-07-27T14:32:56.213824: step 405, loss 0.605896, acc 0.71875
2016-07-27T14:32:58.983572: step 406, loss 0.485894, acc 0.859375
2016-07-27T14:33:01.530559: step 407, loss 0.602151, acc 0.625
2016-07-27T14:33:04.297651: step 408, loss 0.577651, acc 0.6875
2016-07-27T14:33:07.078487: step 409, loss 0.489476, acc 0.765625
2016-07-27T14:33:09.537514: step 410, loss 0.573933, acc 0.671875
2016-07-27T14:33:12.559896: step 411, loss 0.602906, acc 0.671875
2016-07-27T14:33:15.161777: step 412, loss 0.607657, acc 0.65625
2016-07-27T14:33:17.729071: step 413, loss 0.53235, acc 0.71875
2016-07-27T14:33:20.459182: step 414, loss 0.540036, acc 0.71875
2016-07-27T14:33:23.312885: step 415, loss 0.685546, acc 0.5625
2016-07-27T14:33:25.865600: step 416, loss 0.619012, acc 0.6875
2016-07-27T14:33:28.487716: step 417, loss 0.499371, acc 0.8125
2016-07-27T14:33:31.328042: step 418, loss 0.711627, acc 0.546875
2016-07-27T14:33:34.015495: step 419, loss 0.597796, acc 0.6875
2016-07-27T14:33:36.824727: step 420, loss 0.531546, acc 0.703125
2016-07-27T14:33:39.563082: step 421, loss 0.670328, acc 0.59375
2016-07-27T14:33:42.059016: step 422, loss 0.472043, acc 0.75
2016-07-27T14:33:44.929744: step 423, loss 0.575725, acc 0.703125
2016-07-27T14:33:47.466558: step 424, loss 0.555087, acc 0.6875
2016-07-27T14:33:50.105716: step 425, loss 0.564393, acc 0.765625
2016-07-27T14:33:52.937068: step 426, loss 0.520781, acc 0.8125
2016-07-27T14:33:55.704757: step 427, loss 0.608097, acc 0.625
2016-07-27T14:33:58.238840: step 428, loss 0.565569, acc 0.75
2016-07-27T14:34:01.231758: step 429, loss 0.556768, acc 0.6875
2016-07-27T14:34:03.795187: step 430, loss 0.566615, acc 0.6875
2016-07-27T14:34:06.488554: step 431, loss 0.558332, acc 0.6875
2016-07-27T14:34:09.337603: step 432, loss 0.587871, acc 0.75
2016-07-27T14:34:11.992649: step 433, loss 0.610639, acc 0.734375
2016-07-27T14:34:14.790495: step 434, loss 0.589882, acc 0.6875
2016-07-27T14:34:17.539906: step 435, loss 0.534644, acc 0.734375
2016-07-27T14:34:20.067411: step 436, loss 0.565201, acc 0.734375
2016-07-27T14:34:22.639767: step 437, loss 0.477496, acc 0.8125
2016-07-27T14:34:25.514533: step 438, loss 0.605775, acc 0.703125
2016-07-27T14:34:28.168371: step 439, loss 0.559902, acc 0.6875
2016-07-27T14:34:30.968260: step 440, loss 0.565241, acc 0.71875
2016-07-27T14:34:33.744780: step 441, loss 0.47291, acc 0.796875
2016-07-27T14:34:36.312724: step 442, loss 0.572919, acc 0.6875
2016-07-27T14:34:38.967731: step 443, loss 0.546696, acc 0.78125
2016-07-27T14:34:41.778664: step 444, loss 0.53274, acc 0.765625
2016-07-27T14:34:44.370089: step 445, loss 0.513641, acc 0.71875
2016-07-27T14:34:47.191498: step 446, loss 0.593966, acc 0.734375
2016-07-27T14:34:49.860033: step 447, loss 0.508793, acc 0.734375
2016-07-27T14:34:52.407459: step 448, loss 0.581584, acc 0.671875
2016-07-27T14:34:55.286667: step 449, loss 0.536072, acc 0.75
2016-07-27T14:34:58.000985: step 450, loss 0.456115, acc 0.71875
2016-07-27T14:35:00.552797: step 451, loss 0.722866, acc 0.671875
2016-07-27T14:35:03.377277: step 452, loss 0.594672, acc 0.703125
2016-07-27T14:35:06.020197: step 453, loss 0.695126, acc 0.645161
2016-07-27T14:35:08.671961: step 454, loss 0.541027, acc 0.734375
2016-07-27T14:35:11.143491: step 455, loss 0.469915, acc 0.765625
2016-07-27T14:35:13.913335: step 456, loss 0.475616, acc 0.75
2016-07-27T14:35:16.596493: step 457, loss 0.542836, acc 0.765625
2016-07-27T14:35:19.450905: step 458, loss 0.617032, acc 0.625
2016-07-27T14:35:22.092573: step 459, loss 0.486128, acc 0.796875
2016-07-27T14:35:24.720051: step 460, loss 0.580195, acc 0.6875
2016-07-27T14:35:27.524615: step 461, loss 0.512022, acc 0.703125
2016-07-27T14:35:30.197357: step 462, loss 0.540471, acc 0.765625
2016-07-27T14:35:32.793112: step 463, loss 0.528908, acc 0.75
2016-07-27T14:35:35.639475: step 464, loss 0.560197, acc 0.6875
2016-07-27T14:35:38.236040: step 465, loss 0.511878, acc 0.78125
2016-07-27T14:35:41.126297: step 466, loss 0.426167, acc 0.84375
2016-07-27T14:35:43.810426: step 467, loss 0.578247, acc 0.703125
2016-07-27T14:35:46.536216: step 468, loss 0.498171, acc 0.71875
2016-07-27T14:35:49.368803: step 469, loss 0.505056, acc 0.671875
2016-07-27T14:35:51.876562: step 470, loss 0.505463, acc 0.734375
2016-07-27T14:35:54.753150: step 471, loss 0.570812, acc 0.65625
2016-07-27T14:35:57.351214: step 472, loss 0.462352, acc 0.75
2016-07-27T14:36:00.144490: step 473, loss 0.487655, acc 0.75
2016-07-27T14:36:02.829812: step 474, loss 0.42218, acc 0.84375
2016-07-27T14:36:05.329213: step 475, loss 0.514102, acc 0.703125
2016-07-27T14:36:08.144916: step 476, loss 0.596208, acc 0.703125
2016-07-27T14:36:10.728328: step 477, loss 0.46174, acc 0.875
2016-07-27T14:36:13.358829: step 478, loss 0.58666, acc 0.671875
2016-07-27T14:36:16.277870: step 479, loss 0.476929, acc 0.8125
2016-07-27T14:36:19.000660: step 480, loss 0.51887, acc 0.703125
2016-07-27T14:36:21.536308: step 481, loss 0.496258, acc 0.765625
2016-07-27T14:36:24.384361: step 482, loss 0.468282, acc 0.796875
2016-07-27T14:36:27.017105: step 483, loss 0.652857, acc 0.625
2016-07-27T14:36:29.758640: step 484, loss 0.491392, acc 0.796875
2016-07-27T14:36:32.662847: step 485, loss 0.489727, acc 0.734375
2016-07-27T14:36:35.324302: step 486, loss 0.57375, acc 0.75
2016-07-27T14:36:37.923376: step 487, loss 0.420639, acc 0.796875
2016-07-27T14:36:40.594989: step 488, loss 0.665363, acc 0.671875
2016-07-27T14:36:43.161830: step 489, loss 0.489472, acc 0.765625
2016-07-27T14:36:46.001904: step 490, loss 0.510514, acc 0.84375
2016-07-27T14:36:48.583701: step 491, loss 0.554773, acc 0.78125
2016-07-27T14:36:51.230379: step 492, loss 0.478094, acc 0.765625
2016-07-27T14:36:54.176345: step 493, loss 0.472447, acc 0.6875
2016-07-27T14:36:56.793555: step 494, loss 0.506088, acc 0.78125
2016-07-27T14:36:59.616856: step 495, loss 0.470194, acc 0.765625
2016-07-27T14:37:02.278848: step 496, loss 0.581732, acc 0.6875
2016-07-27T14:37:04.831170: step 497, loss 0.501984, acc 0.734375
2016-07-27T14:37:07.690797: step 498, loss 0.564204, acc 0.765625
2016-07-27T14:37:10.311189: step 499, loss 0.530388, acc 0.765625
2016-07-27T14:37:12.929825: step 500, loss 0.447539, acc 0.796875

Evaluation:
2016-07-27T14:37:24.918880: step 500, loss 0.671605, acc 0.667

Saved model checkpoint to /home/chaitanya/lstm-context-embeddings/runs/1469600026/checkpoints/model-500

2016-07-27T14:37:28.834998: step 501, loss 0.441588, acc 0.765625
2016-07-27T14:37:31.610710: step 502, loss 0.56664, acc 0.71875
2016-07-27T14:37:34.175186: step 503, loss 0.588668, acc 0.765625
2016-07-27T14:37:36.783051: step 504, loss 0.504269, acc 0.75
2016-07-27T14:37:39.625066: step 505, loss 0.51184, acc 0.6875
2016-07-27T14:37:42.240708: step 506, loss 0.495146, acc 0.78125
2016-07-27T14:37:44.898643: step 507, loss 0.533092, acc 0.6875
2016-07-27T14:37:47.684607: step 508, loss 0.537135, acc 0.78125
2016-07-27T14:37:50.080389: step 509, loss 0.506784, acc 0.8125
2016-07-27T14:37:52.873277: step 510, loss 0.581087, acc 0.796875
2016-07-27T14:37:55.518732: step 511, loss 0.491335, acc 0.8125
2016-07-27T14:37:58.050464: step 512, loss 0.603987, acc 0.703125
2016-07-27T14:38:00.786430: step 513, loss 0.550783, acc 0.671875
2016-07-27T14:38:03.348775: step 514, loss 0.553249, acc 0.734375
2016-07-27T14:38:06.024108: step 515, loss 0.551923, acc 0.78125
2016-07-27T14:38:08.810651: step 516, loss 0.575428, acc 0.640625
2016-07-27T14:38:11.441787: step 517, loss 0.521188, acc 0.75
2016-07-27T14:38:14.021510: step 518, loss 0.588778, acc 0.703125
2016-07-27T14:38:16.840606: step 519, loss 0.53207, acc 0.765625
2016-07-27T14:38:19.475521: step 520, loss 0.508404, acc 0.75
2016-07-27T14:38:21.934011: step 521, loss 0.499333, acc 0.75
2016-07-27T14:38:24.753330: step 522, loss 0.451882, acc 0.796875
2016-07-27T14:38:27.365696: step 523, loss 0.56468, acc 0.625
2016-07-27T14:38:30.096677: step 524, loss 0.599359, acc 0.671875
2016-07-27T14:38:32.887569: step 525, loss 0.518856, acc 0.703125
2016-07-27T14:38:35.467973: step 526, loss 0.573254, acc 0.65625
2016-07-27T14:38:38.299241: step 527, loss 0.49617, acc 0.796875
2016-07-27T14:38:40.803265: step 528, loss 0.573035, acc 0.71875
2016-07-27T14:38:43.660500: step 529, loss 0.505427, acc 0.75
2016-07-27T14:38:46.397219: step 530, loss 0.572235, acc 0.765625
2016-07-27T14:38:48.908341: step 531, loss 0.50502, acc 0.765625
2016-07-27T14:38:51.726686: step 532, loss 0.60718, acc 0.6875
2016-07-27T14:38:54.281129: step 533, loss 0.558036, acc 0.640625
2016-07-27T14:38:56.861178: step 534, loss 0.532899, acc 0.75
2016-07-27T14:38:59.713812: step 535, loss 0.428596, acc 0.828125
2016-07-27T14:39:02.409198: step 536, loss 0.449099, acc 0.71875
2016-07-27T14:39:04.906735: step 537, loss 0.50679, acc 0.78125
2016-07-27T14:39:07.751868: step 538, loss 0.449411, acc 0.84375
2016-07-27T14:39:10.305798: step 539, loss 0.523658, acc 0.75
2016-07-27T14:39:12.937303: step 540, loss 0.466652, acc 0.765625
2016-07-27T14:39:15.757855: step 541, loss 0.517494, acc 0.734375
2016-07-27T14:39:18.420174: step 542, loss 0.442811, acc 0.828125
2016-07-27T14:39:21.144892: step 543, loss 0.456282, acc 0.78125
2016-07-27T14:39:23.915430: step 544, loss 0.527103, acc 0.734375
2016-07-27T14:39:26.325814: step 545, loss 0.490822, acc 0.796875
2016-07-27T14:39:29.112448: step 546, loss 0.464051, acc 0.78125
2016-07-27T14:39:31.738406: step 547, loss 0.550827, acc 0.65625
2016-07-27T14:39:34.280959: step 548, loss 0.646214, acc 0.703125
2016-07-27T14:39:37.102246: step 549, loss 0.546435, acc 0.71875
2016-07-27T14:39:39.666239: step 550, loss 0.582498, acc 0.6875
2016-07-27T14:39:42.306289: step 551, loss 0.547382, acc 0.75
2016-07-27T14:39:45.110899: step 552, loss 0.595513, acc 0.703125
2016-07-27T14:39:47.906773: step 553, loss 0.580551, acc 0.6875
2016-07-27T14:39:50.706405: step 554, loss 0.417249, acc 0.84375
2016-07-27T14:39:53.367889: step 555, loss 0.5736, acc 0.734375
2016-07-27T14:39:55.829472: step 556, loss 0.491067, acc 0.75
2016-07-27T14:39:58.611736: step 557, loss 0.558969, acc 0.78125
2016-07-27T14:40:01.186146: step 558, loss 0.501866, acc 0.78125
2016-07-27T14:40:03.986212: step 559, loss 0.576817, acc 0.71875
2016-07-27T14:40:06.865015: step 560, loss 0.574681, acc 0.671875
2016-07-27T14:40:09.336914: step 561, loss 0.511541, acc 0.75
2016-07-27T14:40:12.207565: step 562, loss 0.488226, acc 0.8125
2016-07-27T14:40:14.810378: step 563, loss 0.59455, acc 0.71875
2016-07-27T14:40:17.389327: step 564, loss 0.487507, acc 0.78125
2016-07-27T14:40:20.146437: step 565, loss 0.627747, acc 0.6875
2016-07-27T14:40:22.682070: step 566, loss 0.564197, acc 0.71875
2016-07-27T14:40:25.294043: step 567, loss 0.580201, acc 0.703125
2016-07-27T14:40:28.147470: step 568, loss 0.607103, acc 0.734375
2016-07-27T14:40:30.799142: step 569, loss 0.58411, acc 0.6875
2016-07-27T14:40:33.299180: step 570, loss 0.509132, acc 0.703125
2016-07-27T14:40:36.058744: step 571, loss 0.464565, acc 0.796875
2016-07-27T14:40:38.622491: step 572, loss 0.487139, acc 0.78125
2016-07-27T14:40:41.180619: step 573, loss 0.489644, acc 0.796875
2016-07-27T14:40:44.039875: step 574, loss 0.508964, acc 0.765625
2016-07-27T14:40:46.708098: step 575, loss 0.494551, acc 0.84375
2016-07-27T14:40:49.247879: step 576, loss 0.458258, acc 0.84375
2016-07-27T14:40:52.066403: step 577, loss 0.528835, acc 0.765625
2016-07-27T14:40:54.618020: step 578, loss 0.528275, acc 0.75
2016-07-27T14:40:57.320161: step 579, loss 0.43135, acc 0.8125
2016-07-27T14:41:00.123756: step 580, loss 0.491696, acc 0.78125
2016-07-27T14:41:02.823193: step 581, loss 0.50319, acc 0.765625
2016-07-27T14:41:05.347878: step 582, loss 0.452456, acc 0.859375
2016-07-27T14:41:08.195764: step 583, loss 0.539733, acc 0.796875
2016-07-27T14:41:10.737044: step 584, loss 0.458317, acc 0.859375
2016-07-27T14:41:13.375098: step 585, loss 0.422101, acc 0.6875
2016-07-27T14:41:16.188953: step 586, loss 0.442876, acc 0.765625
2016-07-27T14:41:18.895439: step 587, loss 0.462378, acc 0.796875
2016-07-27T14:41:21.441358: step 588, loss 0.464324, acc 0.703125
2016-07-27T14:41:24.183882: step 589, loss 0.490749, acc 0.734375
2016-07-27T14:41:26.743789: step 590, loss 0.528713, acc 0.71875
2016-07-27T14:41:29.390616: step 591, loss 0.455046, acc 0.78125
2016-07-27T14:41:32.227307: step 592, loss 0.417429, acc 0.828125
2016-07-27T14:41:34.856051: step 593, loss 0.512794, acc 0.796875
2016-07-27T14:41:37.451886: step 594, loss 0.454918, acc 0.75
2016-07-27T14:41:40.276216: step 595, loss 0.416928, acc 0.796875
2016-07-27T14:41:42.923072: step 596, loss 0.520349, acc 0.8125
2016-07-27T14:41:45.496288: step 597, loss 0.505717, acc 0.765625
2016-07-27T14:41:48.370142: step 598, loss 0.51618, acc 0.796875
2016-07-27T14:41:50.888927: step 599, loss 0.598571, acc 0.734375
2016-07-27T14:41:53.556497: step 600, loss 0.483417, acc 0.78125

Evaluation:
2016-07-27T14:42:05.630219: step 600, loss 0.63261, acc 0.69

Saved model checkpoint to /home/chaitanya/lstm-context-embeddings/runs/1469600026/checkpoints/model-600

2016-07-27T14:42:09.577643: step 601, loss 0.41655, acc 0.84375
2016-07-27T14:42:12.417721: step 602, loss 0.477036, acc 0.828125
2016-07-27T14:42:15.051130: step 603, loss 0.49594, acc 0.765625
2016-07-27T14:42:17.611193: step 604, loss 0.459963, acc 0.806452
2016-07-27T14:42:20.391591: step 605, loss 0.431123, acc 0.875
2016-07-27T14:42:22.903580: step 606, loss 0.498536, acc 0.78125
2016-07-27T14:42:25.467068: step 607, loss 0.433359, acc 0.84375
2016-07-27T14:42:28.379645: step 608, loss 0.418224, acc 0.8125
2016-07-27T14:42:31.023194: step 609, loss 0.402697, acc 0.796875
2016-07-27T14:42:33.499941: step 610, loss 0.403054, acc 0.8125
2016-07-27T14:42:36.290308: step 611, loss 0.442993, acc 0.8125
2016-07-27T14:42:38.817819: step 612, loss 0.36643, acc 0.84375
2016-07-27T14:42:41.682070: step 613, loss 0.484542, acc 0.8125
2016-07-27T14:42:44.460193: step 614, loss 0.610773, acc 0.734375
2016-07-27T14:42:46.857087: step 615, loss 0.406189, acc 0.765625
2016-07-27T14:42:49.684146: step 616, loss 0.457122, acc 0.84375
2016-07-27T14:42:52.291954: step 617, loss 0.401074, acc 0.859375
2016-07-27T14:42:54.836980: step 618, loss 0.422385, acc 0.765625
2016-07-27T14:42:57.470479: step 619, loss 0.570146, acc 0.75
2016-07-27T14:43:00.027265: step 620, loss 0.478019, acc 0.796875
2016-07-27T14:43:02.819555: step 621, loss 0.462669, acc 0.765625
2016-07-27T14:43:05.458571: step 622, loss 0.414673, acc 0.84375
2016-07-27T14:43:08.299079: step 623, loss 0.401159, acc 0.859375
2016-07-27T14:43:10.992698: step 624, loss 0.439553, acc 0.859375
2016-07-27T14:43:13.508564: step 625, loss 0.429276, acc 0.8125
2016-07-27T14:43:16.274940: step 626, loss 0.498416, acc 0.765625
2016-07-27T14:43:18.865050: step 627, loss 0.411658, acc 0.875
2016-07-27T14:43:21.542807: step 628, loss 0.450308, acc 0.84375
2016-07-27T14:43:24.317959: step 629, loss 0.407206, acc 0.78125
2016-07-27T14:43:26.824431: step 630, loss 0.397271, acc 0.8125
2016-07-27T14:43:29.685585: step 631, loss 0.363936, acc 0.859375
2016-07-27T14:43:32.288387: step 632, loss 0.423102, acc 0.84375
2016-07-27T14:43:35.069039: step 633, loss 0.443846, acc 0.796875
2016-07-27T14:43:37.796143: step 634, loss 0.561613, acc 0.6875
2016-07-27T14:43:40.311009: step 635, loss 0.362411, acc 0.890625
2016-07-27T14:43:43.187549: step 636, loss 0.381958, acc 0.8125
2016-07-27T14:43:45.759705: step 637, loss 0.467094, acc 0.734375
2016-07-27T14:43:48.680631: step 638, loss 0.408069, acc 0.8125
2016-07-27T14:43:51.398078: step 639, loss 0.529871, acc 0.75
2016-07-27T14:43:54.252174: step 640, loss 0.454895, acc 0.8125
2016-07-27T14:43:57.046448: step 641, loss 0.432652, acc 0.78125
2016-07-27T14:43:59.827761: step 642, loss 0.338015, acc 0.84375
2016-07-27T14:44:02.680060: step 643, loss 0.39122, acc 0.828125
2016-07-27T14:44:05.158062: step 644, loss 0.413752, acc 0.859375
2016-07-27T14:44:08.047822: step 645, loss 0.450308, acc 0.796875
2016-07-27T14:44:10.752732: step 646, loss 0.4982, acc 0.796875
2016-07-27T14:44:13.591998: step 647, loss 0.438932, acc 0.828125
2016-07-27T14:44:16.277196: step 648, loss 0.456486, acc 0.75
2016-07-27T14:44:18.899447: step 649, loss 0.35917, acc 0.875
2016-07-27T14:44:21.725081: step 650, loss 0.4271, acc 0.828125
2016-07-27T14:44:24.370656: step 651, loss 0.379252, acc 0.8125
2016-07-27T14:44:27.141483: step 652, loss 0.495344, acc 0.78125
2016-07-27T14:44:29.957339: step 653, loss 0.433827, acc 0.75
2016-07-27T14:44:32.457266: step 654, loss 0.475951, acc 0.71875
2016-07-27T14:44:35.327927: step 655, loss 0.451182, acc 0.828125
2016-07-27T14:44:37.927248: step 656, loss 0.428706, acc 0.84375
2016-07-27T14:44:40.822209: step 657, loss 0.481919, acc 0.75
2016-07-27T14:44:43.649714: step 658, loss 0.430862, acc 0.796875
2016-07-27T14:44:46.212453: step 659, loss 0.426502, acc 0.796875
2016-07-27T14:44:49.065510: step 660, loss 0.420566, acc 0.8125
2016-07-27T14:44:51.731622: step 661, loss 0.454828, acc 0.796875
2016-07-27T14:44:54.539757: step 662, loss 0.451071, acc 0.796875
2016-07-27T14:44:57.385954: step 663, loss 0.354464, acc 0.90625
2016-07-27T14:44:59.936860: step 664, loss 0.399878, acc 0.875
2016-07-27T14:45:02.862446: step 665, loss 0.46247, acc 0.8125
2016-07-27T14:45:05.518135: step 666, loss 0.44544, acc 0.765625
2016-07-27T14:45:08.328868: step 667, loss 0.447916, acc 0.71875
2016-07-27T14:45:11.099814: step 668, loss 0.502035, acc 0.828125
2016-07-27T14:45:13.744338: step 669, loss 0.341719, acc 0.8125
2016-07-27T14:45:16.583551: step 670, loss 0.431309, acc 0.859375
2016-07-27T14:45:19.125231: step 671, loss 0.378407, acc 0.796875
2016-07-27T14:45:22.018109: step 672, loss 0.381487, acc 0.828125
2016-07-27T14:45:24.686218: step 673, loss 0.481174, acc 0.765625
2016-07-27T14:45:27.519700: step 674, loss 0.446766, acc 0.78125
2016-07-27T14:45:30.266573: step 675, loss 0.466951, acc 0.765625
2016-07-27T14:45:32.919548: step 676, loss 0.315238, acc 0.890625
2016-07-27T14:45:35.837189: step 677, loss 0.473758, acc 0.796875
2016-07-27T14:45:38.493077: step 678, loss 0.428613, acc 0.8125
2016-07-27T14:45:41.373597: step 679, loss 0.427647, acc 0.859375
2016-07-27T14:45:44.683749: step 680, loss 0.530052, acc 0.78125
2016-07-27T14:45:47.783025: step 681, loss 0.463128, acc 0.78125
2016-07-27T14:45:50.439969: step 682, loss 0.455284, acc 0.796875
2016-07-27T14:45:53.313487: step 683, loss 0.43574, acc 0.765625
2016-07-27T14:45:56.105703: step 684, loss 0.41376, acc 0.84375
2016-07-27T14:45:59.116276: step 685, loss 0.475873, acc 0.75
2016-07-27T14:46:01.960241: step 686, loss 0.473784, acc 0.765625
2016-07-27T14:46:04.630754: step 687, loss 0.436641, acc 0.8125
2016-07-27T14:46:07.523057: step 688, loss 0.546047, acc 0.671875
2016-07-27T14:46:10.138259: step 689, loss 0.492633, acc 0.71875
2016-07-27T14:46:13.026390: step 690, loss 0.512174, acc 0.734375
2016-07-27T14:46:15.719166: step 691, loss 0.452096, acc 0.78125
2016-07-27T14:46:18.329174: step 692, loss 0.543486, acc 0.71875
2016-07-27T14:46:21.207034: step 693, loss 0.435673, acc 0.796875
2016-07-27T14:46:23.819767: step 694, loss 0.443497, acc 0.828125
2016-07-27T14:46:26.376233: step 695, loss 0.475765, acc 0.75
2016-07-27T14:46:29.184306: step 696, loss 0.440008, acc 0.796875
2016-07-27T14:46:31.918646: step 697, loss 0.384506, acc 0.796875
2016-07-27T14:46:34.788079: step 698, loss 0.489738, acc 0.78125
2016-07-27T14:46:37.479885: step 699, loss 0.404308, acc 0.875
2016-07-27T14:46:40.217578: step 700, loss 0.420614, acc 0.78125

Evaluation:
2016-07-27T14:46:52.741599: step 700, loss 0.658472, acc 0.681

Saved model checkpoint to /home/chaitanya/lstm-context-embeddings/runs/1469600026/checkpoints/model-700

2016-07-27T14:46:57.049415: step 701, loss 0.406382, acc 0.8125
2016-07-27T14:46:59.871266: step 702, loss 0.401622, acc 0.875
2016-07-27T14:47:02.594013: step 703, loss 0.377113, acc 0.84375
2016-07-27T14:47:05.157343: step 704, loss 0.395754, acc 0.875
2016-07-27T14:47:08.017261: step 705, loss 0.357587, acc 0.875
2016-07-27T14:47:10.553859: step 706, loss 0.447115, acc 0.765625
2016-07-27T14:47:13.327948: step 707, loss 0.407306, acc 0.78125
2016-07-27T14:47:16.134297: step 708, loss 0.512504, acc 0.703125
2016-07-27T14:47:18.601452: step 709, loss 0.406608, acc 0.828125
2016-07-27T14:47:21.500128: step 710, loss 0.551924, acc 0.734375
2016-07-27T14:47:24.171617: step 711, loss 0.503287, acc 0.796875
2016-07-27T14:47:27.033783: step 712, loss 0.382466, acc 0.796875
2016-07-27T14:47:29.735504: step 713, loss 0.474578, acc 0.8125
2016-07-27T14:47:32.364402: step 714, loss 0.423694, acc 0.78125
2016-07-27T14:47:35.198102: step 715, loss 0.39312, acc 0.8125
2016-07-27T14:47:37.797019: step 716, loss 0.449874, acc 0.8125
2016-07-27T14:47:40.402800: step 717, loss 0.434507, acc 0.765625
2016-07-27T14:47:43.272424: step 718, loss 0.35736, acc 0.875
2016-07-27T14:47:45.907851: step 719, loss 0.406545, acc 0.828125
2016-07-27T14:47:48.743152: step 720, loss 0.379209, acc 0.875
2016-07-27T14:47:51.509055: step 721, loss 0.386354, acc 0.828125
2016-07-27T14:47:54.067779: step 722, loss 0.536049, acc 0.765625
2016-07-27T14:47:56.929736: step 723, loss 0.548483, acc 0.6875
2016-07-27T14:47:59.551880: step 724, loss 0.417869, acc 0.828125
2016-07-27T14:48:02.216859: step 725, loss 0.443039, acc 0.8125
2016-07-27T14:48:05.130038: step 726, loss 0.417996, acc 0.828125
2016-07-27T14:48:07.860563: step 727, loss 0.419559, acc 0.796875
2016-07-27T14:48:10.473927: step 728, loss 0.463958, acc 0.734375
2016-07-27T14:48:13.352238: step 729, loss 0.329974, acc 0.890625
2016-07-27T14:48:15.970683: step 730, loss 0.491086, acc 0.78125
2016-07-27T14:48:18.621535: step 731, loss 0.502871, acc 0.78125
2016-07-27T14:48:21.497830: step 732, loss 0.462987, acc 0.8125
2016-07-27T14:48:24.174812: step 733, loss 0.40638, acc 0.8125
2016-07-27T14:48:27.037665: step 734, loss 0.359152, acc 0.84375
2016-07-27T14:48:29.739267: step 735, loss 0.283265, acc 0.859375
2016-07-27T14:48:32.535398: step 736, loss 0.327649, acc 0.875
2016-07-27T14:48:35.511894: step 737, loss 0.453582, acc 0.828125
2016-07-27T14:48:38.152875: step 738, loss 0.344975, acc 0.8125
2016-07-27T14:48:42.932936: step 739, loss 0.391449, acc 0.828125
2016-07-27T14:48:47.588386: step 740, loss 0.330275, acc 0.8125
2016-07-27T14:48:52.332432: step 741, loss 0.421567, acc 0.828125
2016-07-27T14:48:57.064006: step 742, loss 0.47223, acc 0.765625
2016-07-27T14:49:00.198994: step 743, loss 0.404427, acc 0.859375
2016-07-27T14:49:03.353144: step 744, loss 0.412302, acc 0.828125
2016-07-27T14:49:06.439411: step 745, loss 0.424375, acc 0.8125
2016-07-27T14:49:09.130660: step 746, loss 0.417163, acc 0.8125
2016-07-27T14:49:11.993964: step 747, loss 0.339685, acc 0.828125
2016-07-27T14:49:14.604434: step 748, loss 0.362432, acc 0.84375
2016-07-27T14:49:17.461471: step 749, loss 0.363635, acc 0.828125
2016-07-27T14:49:20.305060: step 750, loss 0.434349, acc 0.828125
2016-07-27T14:49:23.171500: step 751, loss 0.370973, acc 0.796875
2016-07-27T14:49:25.964923: step 752, loss 0.52917, acc 0.703125
2016-07-27T14:49:28.854512: step 753, loss 0.410034, acc 0.78125
2016-07-27T14:49:31.645698: step 754, loss 0.396626, acc 0.828125
2016-07-27T14:49:34.363542: step 755, loss 0.597151, acc 0.774194
2016-07-27T14:49:37.203054: step 756, loss 0.357345, acc 0.828125
2016-07-27T14:49:39.812569: step 757, loss 0.348037, acc 0.828125
2016-07-27T14:49:42.703067: step 758, loss 0.419897, acc 0.90625
2016-07-27T14:49:45.331004: step 759, loss 0.429981, acc 0.8125
2016-07-27T14:49:47.964740: step 760, loss 0.392758, acc 0.84375
2016-07-27T14:49:50.721228: step 761, loss 0.290116, acc 0.84375
2016-07-27T14:49:53.354683: step 762, loss 0.311495, acc 0.90625
2016-07-27T14:49:56.024508: step 763, loss 0.327733, acc 0.84375
2016-07-27T14:49:58.854003: step 764, loss 0.285965, acc 0.859375
2016-07-27T14:50:01.488299: step 765, loss 0.39881, acc 0.828125
2016-07-27T14:50:04.123299: step 766, loss 0.379489, acc 0.828125
2016-07-27T14:50:06.971946: step 767, loss 0.349369, acc 0.890625
2016-07-27T14:50:09.640345: step 768, loss 0.366244, acc 0.8125
2016-07-27T14:50:12.568142: step 769, loss 0.379806, acc 0.828125
2016-07-27T14:50:15.221831: step 770, loss 0.415909, acc 0.796875
2016-07-27T14:50:18.046981: step 771, loss 0.224131, acc 0.953125
2016-07-27T14:50:20.766608: step 772, loss 0.351088, acc 0.8125
2016-07-27T14:50:23.287298: step 773, loss 0.311224, acc 0.875
2016-07-27T14:50:26.108007: step 774, loss 0.411992, acc 0.78125
2016-07-27T14:50:28.784049: step 775, loss 0.291423, acc 0.859375
2016-07-27T14:50:31.468820: step 776, loss 0.422288, acc 0.796875
2016-07-27T14:50:34.341139: step 777, loss 0.412332, acc 0.78125
2016-07-27T14:50:37.013083: step 778, loss 0.291297, acc 0.890625
2016-07-27T14:50:39.633552: step 779, loss 0.443425, acc 0.8125
2016-07-27T14:50:42.511956: step 780, loss 0.366639, acc 0.859375
2016-07-27T14:50:45.206629: step 781, loss 0.243285, acc 0.9375
2016-07-27T14:50:47.799945: step 782, loss 0.409294, acc 0.828125
2016-07-27T14:50:50.505079: step 783, loss 0.383377, acc 0.859375
2016-07-27T14:50:53.392325: step 784, loss 0.327411, acc 0.859375
2016-07-27T14:50:55.943747: step 785, loss 0.356485, acc 0.796875
2016-07-27T14:50:58.758771: step 786, loss 0.308297, acc 0.859375
2016-07-27T14:51:01.398087: step 787, loss 0.414342, acc 0.828125
2016-07-27T14:51:04.123958: step 788, loss 0.307355, acc 0.875
2016-07-27T14:51:06.923324: step 789, loss 0.434225, acc 0.84375
2016-07-27T14:51:09.558514: step 790, loss 0.355801, acc 0.796875
2016-07-27T14:51:12.141030: step 791, loss 0.394223, acc 0.84375
2016-07-27T14:51:15.015568: step 792, loss 0.322036, acc 0.859375
2016-07-27T14:51:17.644185: step 793, loss 0.319294, acc 0.921875
2016-07-27T14:51:20.255848: step 794, loss 0.326262, acc 0.875
2016-07-27T14:51:23.160904: step 795, loss 0.255333, acc 0.921875
2016-07-27T14:51:25.833555: step 796, loss 0.277081, acc 0.859375
2016-07-27T14:51:28.455642: step 797, loss 0.250956, acc 0.875
2016-07-27T14:51:31.284247: step 798, loss 0.429822, acc 0.828125
2016-07-27T14:51:33.934159: step 799, loss 0.311178, acc 0.890625
2016-07-27T14:51:36.904919: step 800, loss 0.234, acc 0.921875

Evaluation:
2016-07-27T14:51:49.258145: step 800, loss 0.70862, acc 0.708

Saved model checkpoint to /home/chaitanya/lstm-context-embeddings/runs/1469600026/checkpoints/model-800

2016-07-27T14:51:53.677282: step 801, loss 0.403728, acc 0.78125
2016-07-27T14:51:56.482190: step 802, loss 0.362778, acc 0.859375
2016-07-27T14:51:59.204555: step 803, loss 0.300124, acc 0.859375
2016-07-27T14:52:01.858053: step 804, loss 0.349789, acc 0.890625
2016-07-27T14:52:04.777778: step 805, loss 0.444109, acc 0.71875
2016-07-27T14:52:07.427507: step 806, loss 0.319628, acc 0.828125
2016-07-27T14:52:10.236848: step 807, loss 0.330414, acc 0.875
2016-07-27T14:52:12.937828: step 808, loss 0.372099, acc 0.890625
2016-07-27T14:52:15.518631: step 809, loss 0.269572, acc 0.90625
2016-07-27T14:52:18.464356: step 810, loss 0.303122, acc 0.875
2016-07-27T14:52:21.178994: step 811, loss 0.287647, acc 0.875
2016-07-27T14:52:23.931739: step 812, loss 0.219578, acc 0.90625
2016-07-27T14:52:26.673670: step 813, loss 0.376533, acc 0.8125
2016-07-27T14:52:29.249119: step 814, loss 0.329454, acc 0.875
2016-07-27T14:52:32.061217: step 815, loss 0.314009, acc 0.90625
2016-07-27T14:52:34.724867: step 816, loss 0.315955, acc 0.859375
2016-07-27T14:52:37.339958: step 817, loss 0.35369, acc 0.796875
2016-07-27T14:52:40.153367: step 818, loss 0.306343, acc 0.890625
2016-07-27T14:52:42.749912: step 819, loss 0.350246, acc 0.875
2016-07-27T14:52:45.401370: step 820, loss 0.298178, acc 0.859375
2016-07-27T14:52:48.207134: step 821, loss 0.408138, acc 0.8125
2016-07-27T14:52:50.809958: step 822, loss 0.323768, acc 0.890625
2016-07-27T14:52:53.476647: step 823, loss 0.316241, acc 0.84375
2016-07-27T14:52:56.308738: step 824, loss 0.389761, acc 0.8125
2016-07-27T14:52:58.908651: step 825, loss 0.324381, acc 0.890625
2016-07-27T14:53:01.574571: step 826, loss 0.378278, acc 0.875
2016-07-27T14:53:04.435899: step 827, loss 0.350018, acc 0.875
2016-07-27T14:53:07.001738: step 828, loss 0.467844, acc 0.84375
2016-07-27T14:53:09.695398: step 829, loss 0.305997, acc 0.890625
2016-07-27T14:53:12.475212: step 830, loss 0.3278, acc 0.921875
2016-07-27T14:53:15.096091: step 831, loss 0.293065, acc 0.875
2016-07-27T14:53:17.813673: step 832, loss 0.364661, acc 0.796875
2016-07-27T14:53:20.668348: step 833, loss 0.499129, acc 0.84375
2016-07-27T14:53:23.258427: step 834, loss 0.261669, acc 0.90625
2016-07-27T14:53:26.127721: step 835, loss 0.234709, acc 0.953125
2016-07-27T14:53:28.781243: step 836, loss 0.391658, acc 0.859375
2016-07-27T14:53:31.347008: step 837, loss 0.285491, acc 0.90625
2016-07-27T14:53:34.207644: step 838, loss 0.318512, acc 0.890625
2016-07-27T14:53:36.757662: step 839, loss 0.336408, acc 0.859375
2016-07-27T14:53:39.574617: step 840, loss 0.423531, acc 0.78125
2016-07-27T14:53:42.199893: step 841, loss 0.346265, acc 0.875
2016-07-27T14:53:45.022229: step 842, loss 0.346847, acc 0.8125
2016-07-27T14:53:47.643493: step 843, loss 0.424333, acc 0.84375
2016-07-27T14:53:50.403875: step 844, loss 0.268524, acc 0.875
2016-07-27T14:53:53.155399: step 845, loss 0.392502, acc 0.796875
2016-07-27T14:53:55.866402: step 846, loss 0.256726, acc 0.921875
2016-07-27T14:53:58.697901: step 847, loss 0.269815, acc 0.890625
2016-07-27T14:54:01.323465: step 848, loss 0.279596, acc 0.90625
2016-07-27T14:54:04.208618: step 849, loss 0.438161, acc 0.75
2016-07-27T14:54:06.892661: step 850, loss 0.234298, acc 0.921875
2016-07-27T14:54:09.459801: step 851, loss 0.364362, acc 0.84375
2016-07-27T14:54:12.307553: step 852, loss 0.427569, acc 0.796875
2016-07-27T14:54:14.944478: step 853, loss 0.285062, acc 0.890625
2016-07-27T14:54:17.588099: step 854, loss 0.315522, acc 0.875
2016-07-27T14:54:20.443048: step 855, loss 0.302392, acc 0.859375
2016-07-27T14:54:23.031342: step 856, loss 0.302292, acc 0.828125
2016-07-27T14:54:25.826529: step 857, loss 0.398139, acc 0.796875
2016-07-27T14:54:28.659689: step 858, loss 0.332653, acc 0.875
2016-07-27T14:54:31.227365: step 859, loss 0.233921, acc 0.90625
2016-07-27T14:54:34.169918: step 860, loss 0.288885, acc 0.875
2016-07-27T14:54:36.745060: step 861, loss 0.362674, acc 0.8125
2016-07-27T14:54:39.578749: step 862, loss 0.349991, acc 0.828125
2016-07-27T14:54:42.232547: step 863, loss 0.270789, acc 0.90625
2016-07-27T14:54:45.083297: step 864, loss 0.382149, acc 0.8125
2016-07-27T14:54:47.708318: step 865, loss 0.33402, acc 0.890625
2016-07-27T14:54:50.511491: step 866, loss 0.215059, acc 0.90625
2016-07-27T14:54:53.265724: step 867, loss 0.395792, acc 0.875
2016-07-27T14:54:55.903301: step 868, loss 0.399559, acc 0.859375
2016-07-27T14:54:58.730798: step 869, loss 0.434444, acc 0.875
2016-07-27T14:55:01.350930: step 870, loss 0.33369, acc 0.875
2016-07-27T14:55:04.042958: step 871, loss 0.35302, acc 0.765625
2016-07-27T14:55:06.921516: step 872, loss 0.326804, acc 0.828125
2016-07-27T14:55:09.653869: step 873, loss 0.367183, acc 0.796875
2016-07-27T14:55:12.695087: step 874, loss 0.3622, acc 0.75
2016-07-27T14:55:15.405497: step 875, loss 0.257097, acc 0.890625
2016-07-27T14:55:18.214605: step 876, loss 0.367066, acc 0.859375
2016-07-27T14:55:20.887688: step 877, loss 0.330402, acc 0.828125
2016-07-27T14:55:23.646498: step 878, loss 0.385417, acc 0.828125
2016-07-27T14:55:26.428614: step 879, loss 0.318492, acc 0.859375
2016-07-27T14:55:29.211348: step 880, loss 0.385712, acc 0.796875
2016-07-27T14:55:32.024032: step 881, loss 0.348338, acc 0.859375
2016-07-27T14:55:34.628055: step 882, loss 0.285802, acc 0.890625
2016-07-27T14:55:37.497817: step 883, loss 0.278087, acc 0.875
2016-07-27T14:55:40.176635: step 884, loss 0.419845, acc 0.828125
2016-07-27T14:55:43.092363: step 885, loss 0.322576, acc 0.859375
2016-07-27T14:55:45.770612: step 886, loss 0.249274, acc 0.875
2016-07-27T14:55:48.548960: step 887, loss 0.400393, acc 0.875
2016-07-27T14:55:51.327524: step 888, loss 0.303435, acc 0.84375
2016-07-27T14:55:54.017172: step 889, loss 0.374904, acc 0.84375
2016-07-27T14:55:56.883102: step 890, loss 0.324725, acc 0.8125
2016-07-27T14:55:59.414525: step 891, loss 0.382805, acc 0.78125
2016-07-27T14:56:02.091989: step 892, loss 0.319105, acc 0.890625
2016-07-27T14:56:04.941302: step 893, loss 0.282536, acc 0.84375
2016-07-27T14:56:07.546725: step 894, loss 0.405825, acc 0.84375
2016-07-27T14:56:10.192777: step 895, loss 0.371103, acc 0.828125
2016-07-27T14:56:13.004896: step 896, loss 0.343303, acc 0.890625
2016-07-27T14:56:15.585436: step 897, loss 0.31659, acc 0.796875
2016-07-27T14:56:18.252863: step 898, loss 0.339824, acc 0.859375
2016-07-27T14:56:21.123643: step 899, loss 0.32232, acc 0.859375
2016-07-27T14:56:23.824231: step 900, loss 0.408181, acc 0.875

Evaluation:
2016-07-27T14:56:36.059784: step 900, loss 0.723823, acc 0.7

Saved model checkpoint to /home/chaitanya/lstm-context-embeddings/runs/1469600026/checkpoints/model-900

2016-07-27T14:56:40.444896: step 901, loss 0.360489, acc 0.796875
2016-07-27T14:56:43.275758: step 902, loss 0.241385, acc 0.9375
2016-07-27T14:56:45.973984: step 903, loss 0.400354, acc 0.8125
2016-07-27T14:56:48.566681: step 904, loss 0.321021, acc 0.84375
2016-07-27T14:56:51.408283: step 905, loss 0.331258, acc 0.875
2016-07-27T14:56:54.088200: step 906, loss 0.435328, acc 0.790323
2016-07-27T14:56:56.924772: step 907, loss 0.242139, acc 0.90625
2016-07-27T14:56:59.639983: step 908, loss 0.242407, acc 0.921875
2016-07-27T14:57:02.392948: step 909, loss 0.298133, acc 0.90625
2016-07-27T14:57:05.206969: step 910, loss 0.262752, acc 0.84375
2016-07-27T14:57:07.879097: step 911, loss 0.216883, acc 0.984375
2016-07-27T14:57:10.736475: step 912, loss 0.299554, acc 0.8125
2016-07-27T14:57:13.371522: step 913, loss 0.251189, acc 0.9375
2016-07-27T14:57:16.030433: step 914, loss 0.219918, acc 0.921875
2016-07-27T14:57:18.840846: step 915, loss 0.227356, acc 0.90625
2016-07-27T14:57:21.447087: step 916, loss 0.226897, acc 0.90625
2016-07-27T14:57:24.134402: step 917, loss 0.276183, acc 0.890625
2016-07-27T14:57:27.111929: step 918, loss 0.266559, acc 0.90625
2016-07-27T14:57:29.798621: step 919, loss 0.262252, acc 0.921875
2016-07-27T14:57:32.896604: step 920, loss 0.166585, acc 0.953125
2016-07-27T14:57:35.578379: step 921, loss 0.268402, acc 0.9375
2016-07-27T14:57:38.524480: step 922, loss 0.268544, acc 0.875
2016-07-27T14:57:41.235347: step 923, loss 0.192906, acc 0.96875
2016-07-27T14:57:44.030525: step 924, loss 0.332122, acc 0.875
2016-07-27T14:57:46.744917: step 925, loss 0.329464, acc 0.90625
2016-07-27T14:57:49.465320: step 926, loss 0.230542, acc 0.890625
2016-07-27T14:57:52.313862: step 927, loss 0.207161, acc 0.953125
2016-07-27T14:57:54.909481: step 928, loss 0.256018, acc 0.90625
2016-07-27T14:57:57.762872: step 929, loss 0.280285, acc 0.875
2016-07-27T14:58:00.388012: step 930, loss 0.186642, acc 0.921875
2016-07-27T14:58:03.027552: step 931, loss 0.229352, acc 0.890625
2016-07-27T14:58:05.863786: step 932, loss 0.322535, acc 0.890625
2016-07-27T14:58:08.436192: step 933, loss 0.167483, acc 0.953125
2016-07-27T14:58:11.256321: step 934, loss 0.22474, acc 0.953125
2016-07-27T14:58:13.905282: step 935, loss 0.249496, acc 0.890625
2016-07-27T14:58:16.581728: step 936, loss 0.253147, acc 0.890625
2016-07-27T14:58:19.322753: step 937, loss 0.276674, acc 0.921875
2016-07-27T14:58:22.140435: step 938, loss 0.215948, acc 0.921875
2016-07-27T14:58:24.705166: step 939, loss 0.249452, acc 0.890625
2016-07-27T14:58:27.586416: step 940, loss 0.253396, acc 0.875
2016-07-27T14:58:30.271833: step 941, loss 0.204932, acc 0.921875
2016-07-27T14:58:32.924213: step 942, loss 0.334618, acc 0.890625
2016-07-27T14:58:35.729084: step 943, loss 0.267371, acc 0.84375
2016-07-27T14:58:38.363771: step 944, loss 0.120993, acc 0.96875
2016-07-27T14:58:40.927585: step 945, loss 0.241765, acc 0.90625
2016-07-27T14:58:43.754561: step 946, loss 0.29469, acc 0.9375
2016-07-27T14:58:46.497221: step 947, loss 0.131991, acc 0.984375
2016-07-27T14:58:49.126697: step 948, loss 0.24022, acc 0.90625
2016-07-27T14:58:51.916538: step 949, loss 0.220205, acc 0.90625
2016-07-27T14:58:54.546830: step 950, loss 0.265861, acc 0.890625
2016-07-27T14:58:57.136251: step 951, loss 0.237975, acc 0.921875
2016-07-27T14:58:59.984954: step 952, loss 0.260897, acc 0.890625
2016-07-27T14:59:02.642035: step 953, loss 0.182499, acc 0.9375
2016-07-27T14:59:05.309468: step 954, loss 0.344216, acc 0.84375
2016-07-27T14:59:08.201562: step 955, loss 0.231581, acc 0.890625
2016-07-27T14:59:10.790761: step 956, loss 0.281117, acc 0.90625
2016-07-27T14:59:13.587490: step 957, loss 0.23525, acc 0.875
2016-07-27T14:59:16.411410: step 958, loss 0.34624, acc 0.890625
2016-07-27T14:59:18.993764: step 959, loss 0.210746, acc 0.9375
2016-07-27T14:59:21.828099: step 960, loss 0.283704, acc 0.859375
2016-07-27T14:59:24.485642: step 961, loss 0.287723, acc 0.921875
2016-07-27T14:59:27.312937: step 962, loss 0.237664, acc 0.90625
2016-07-27T14:59:30.045178: step 963, loss 0.212956, acc 0.9375
2016-07-27T14:59:32.861623: step 964, loss 0.199157, acc 0.9375
2016-07-27T14:59:35.869444: step 965, loss 0.263422, acc 0.875
2016-07-27T14:59:38.709058: step 966, loss 0.263437, acc 0.828125
2016-07-27T14:59:41.558746: step 967, loss 0.313436, acc 0.8125
2016-07-27T14:59:44.102157: step 968, loss 0.249784, acc 0.9375
2016-07-27T14:59:46.954640: step 969, loss 0.304281, acc 0.890625
2016-07-27T14:59:50.025959: step 970, loss 0.261872, acc 0.90625
2016-07-27T14:59:53.043365: step 971, loss 0.297316, acc 0.875
2016-07-27T14:59:55.887827: step 972, loss 0.245497, acc 0.921875
2016-07-27T14:59:58.677901: step 973, loss 0.240058, acc 0.890625
2016-07-27T15:00:01.467911: step 974, loss 0.227257, acc 0.890625
2016-07-27T15:00:04.045388: step 975, loss 0.396694, acc 0.8125
2016-07-27T15:00:06.931429: step 976, loss 0.263792, acc 0.875
2016-07-27T15:00:09.560200: step 977, loss 0.332798, acc 0.828125
2016-07-27T15:00:12.338942: step 978, loss 0.23483, acc 0.9375
2016-07-27T15:00:15.010329: step 979, loss 0.267026, acc 0.875
2016-07-27T15:00:17.691750: step 980, loss 0.234312, acc 0.90625
2016-07-27T15:00:20.547998: step 981, loss 0.24602, acc 0.90625
2016-07-27T15:00:23.179114: step 982, loss 0.21317, acc 0.921875
2016-07-27T15:00:26.191847: step 983, loss 0.219701, acc 0.859375
2016-07-27T15:00:28.927433: step 984, loss 0.352537, acc 0.84375
2016-07-27T15:00:31.840230: step 985, loss 0.307274, acc 0.875
2016-07-27T15:00:34.528206: step 986, loss 0.35238, acc 0.84375
2016-07-27T15:00:37.346991: step 987, loss 0.414229, acc 0.859375
2016-07-27T15:00:40.102116: step 988, loss 0.243711, acc 0.921875
2016-07-27T15:00:42.802986: step 989, loss 0.200336, acc 0.953125
2016-07-27T15:00:45.633488: step 990, loss 0.271578, acc 0.890625
2016-07-27T15:00:48.192316: step 991, loss 0.304951, acc 0.921875
2016-07-27T15:00:50.921235: step 992, loss 0.354862, acc 0.84375
2016-07-27T15:00:53.717483: step 993, loss 0.256298, acc 0.875
2016-07-27T15:00:56.278438: step 994, loss 0.270407, acc 0.859375
2016-07-27T15:00:59.109287: step 995, loss 0.205416, acc 0.890625
2016-07-27T15:01:01.660146: step 996, loss 0.228503, acc 0.921875
2016-07-27T15:01:04.436678: step 997, loss 0.218557, acc 0.921875
2016-07-27T15:01:07.157747: step 998, loss 0.275892, acc 0.890625
2016-07-27T15:01:10.017455: step 999, loss 0.23044, acc 0.890625
2016-07-27T15:01:12.633915: step 1000, loss 0.418121, acc 0.8125

Evaluation:
2016-07-27T15:01:25.041943: step 1000, loss 0.917404, acc 0.691

Saved model checkpoint to /home/chaitanya/lstm-context-embeddings/runs/1469600026/checkpoints/model-1000

2016-07-27T15:01:29.020966: step 1001, loss 0.228764, acc 0.90625
2016-07-27T15:01:31.848543: step 1002, loss 0.266528, acc 0.875
2016-07-27T15:01:34.469469: step 1003, loss 0.426283, acc 0.84375
2016-07-27T15:01:37.250145: step 1004, loss 0.194583, acc 0.921875
2016-07-27T15:01:39.910392: step 1005, loss 0.252876, acc 0.90625
2016-07-27T15:01:42.820025: step 1006, loss 0.239672, acc 0.921875
2016-07-27T15:01:45.521992: step 1007, loss 0.320652, acc 0.90625
2016-07-27T15:01:48.292101: step 1008, loss 0.283585, acc 0.890625
2016-07-27T15:01:51.054232: step 1009, loss 0.174362, acc 0.921875
2016-07-27T15:01:53.626383: step 1010, loss 0.316103, acc 0.875
2016-07-27T15:01:56.433031: step 1011, loss 0.352579, acc 0.8125
2016-07-27T15:01:59.105233: step 1012, loss 0.279728, acc 0.875
2016-07-27T15:02:01.934892: step 1013, loss 0.442295, acc 0.78125
2016-07-27T15:02:04.570899: step 1014, loss 0.291932, acc 0.90625
2016-07-27T15:02:07.366967: step 1015, loss 0.329068, acc 0.90625
2016-07-27T15:02:10.147719: step 1016, loss 0.287292, acc 0.90625
2016-07-27T15:02:12.815550: step 1017, loss 0.264032, acc 0.90625
2016-07-27T15:02:15.625842: step 1018, loss 0.388684, acc 0.828125
2016-07-27T15:02:18.173402: step 1019, loss 0.212734, acc 0.921875
2016-07-27T15:02:20.954737: step 1020, loss 0.342271, acc 0.84375
2016-07-27T15:02:23.587998: step 1021, loss 0.322007, acc 0.90625
2016-07-27T15:02:26.188704: step 1022, loss 0.302879, acc 0.9375
2016-07-27T15:02:29.041473: step 1023, loss 0.345876, acc 0.84375
2016-07-27T15:02:31.657728: step 1024, loss 0.269099, acc 0.90625
2016-07-27T15:02:34.491448: step 1025, loss 0.376877, acc 0.8125
2016-07-27T15:02:37.174541: step 1026, loss 0.322656, acc 0.84375
2016-07-27T15:02:40.134336: step 1027, loss 0.281933, acc 0.921875
2016-07-27T15:02:42.798886: step 1028, loss 0.209941, acc 0.90625
2016-07-27T15:02:45.573953: step 1029, loss 0.274296, acc 0.890625
2016-07-27T15:02:48.321599: step 1030, loss 0.232028, acc 0.921875
2016-07-27T15:02:51.104475: step 1031, loss 0.281383, acc 0.859375
2016-07-27T15:02:54.097411: step 1032, loss 0.256971, acc 0.921875
2016-07-27T15:02:56.802216: step 1033, loss 0.26403, acc 0.921875
2016-07-27T15:03:00.303338: step 1034, loss 0.30508, acc 0.890625
2016-07-27T15:03:03.123182: step 1035, loss 0.22989, acc 0.921875
2016-07-27T15:03:05.721068: step 1036, loss 0.283787, acc 0.84375
2016-07-27T15:03:08.845842: step 1037, loss 0.270074, acc 0.9375
2016-07-27T15:03:11.737438: step 1038, loss 0.295932, acc 0.859375
2016-07-27T15:03:14.614262: step 1039, loss 0.465272, acc 0.734375
2016-07-27T15:03:17.520046: step 1040, loss 0.278619, acc 0.921875
2016-07-27T15:03:20.212523: step 1041, loss 0.247332, acc 0.875
2016-07-27T15:03:23.130420: step 1042, loss 0.266232, acc 0.90625
2016-07-27T15:03:26.064414: step 1043, loss 0.438483, acc 0.796875
2016-07-27T15:03:29.096695: step 1044, loss 0.162248, acc 0.984375
2016-07-27T15:03:32.111456: step 1045, loss 0.226687, acc 0.875
2016-07-27T15:03:35.349117: step 1046, loss 0.33584, acc 0.8125
2016-07-27T15:03:38.128622: step 1047, loss 0.255374, acc 0.859375
2016-07-27T15:03:40.760061: step 1048, loss 0.145632, acc 0.96875
2016-07-27T15:03:44.386369: step 1049, loss 0.2441, acc 0.90625
2016-07-27T15:03:47.102617: step 1050, loss 0.131749, acc 0.984375
2016-07-27T15:03:49.893671: step 1051, loss 0.363604, acc 0.9375
2016-07-27T15:03:52.731580: step 1052, loss 0.309526, acc 0.84375
2016-07-27T15:03:55.417620: step 1053, loss 0.310617, acc 0.84375
2016-07-27T15:03:58.358775: step 1054, loss 0.314382, acc 0.859375
2016-07-27T15:04:01.224214: step 1055, loss 0.294075, acc 0.90625
2016-07-27T15:04:04.264404: step 1056, loss 0.265391, acc 0.859375
2016-07-27T15:04:07.061842: step 1057, loss 0.264457, acc 0.903226
2016-07-27T15:04:09.923849: step 1058, loss 0.131724, acc 0.96875
2016-07-27T15:04:12.649095: step 1059, loss 0.273793, acc 0.890625
2016-07-27T15:04:15.395134: step 1060, loss 0.198489, acc 0.921875
2016-07-27T15:04:18.279626: step 1061, loss 0.159171, acc 0.953125
2016-07-27T15:04:21.128160: step 1062, loss 0.158465, acc 0.921875
2016-07-27T15:04:23.981057: step 1063, loss 0.21657, acc 0.90625
2016-07-27T15:04:26.545638: step 1064, loss 0.169732, acc 0.953125
2016-07-27T15:04:29.444058: step 1065, loss 0.171823, acc 0.953125
2016-07-27T15:04:32.086445: step 1066, loss 0.141999, acc 0.984375
2016-07-27T15:04:34.816743: step 1067, loss 0.167837, acc 0.984375
2016-07-27T15:04:37.601321: step 1068, loss 0.210068, acc 0.921875
2016-07-27T15:04:40.099991: step 1069, loss 0.126423, acc 0.953125
2016-07-27T15:04:42.926240: step 1070, loss 0.243348, acc 0.859375
2016-07-27T15:04:45.542147: step 1071, loss 0.244923, acc 0.921875
2016-07-27T15:04:48.330326: step 1072, loss 0.145121, acc 0.953125
2016-07-27T15:04:51.049505: step 1073, loss 0.136114, acc 0.953125
2016-07-27T15:04:53.809056: step 1074, loss 0.161295, acc 0.953125
2016-07-27T15:04:56.554796: step 1075, loss 0.286828, acc 0.90625
2016-07-27T15:04:59.242294: step 1076, loss 0.141325, acc 0.953125
2016-07-27T15:05:02.144684: step 1077, loss 0.213638, acc 0.953125
2016-07-27T15:05:04.754916: step 1078, loss 0.132296, acc 0.96875
2016-07-27T15:05:08.160645: step 1079, loss 0.268037, acc 0.875
2016-07-27T15:05:11.042683: step 1080, loss 0.227515, acc 0.921875
2016-07-27T15:05:13.721718: step 1081, loss 0.147742, acc 0.9375
2016-07-27T15:05:16.914638: step 1082, loss 0.311789, acc 0.796875
2016-07-27T15:05:19.700155: step 1083, loss 0.163817, acc 0.953125
2016-07-27T15:05:22.375686: step 1084, loss 0.219468, acc 0.9375
2016-07-27T15:05:25.212114: step 1085, loss 0.0991698, acc 0.96875
2016-07-27T15:05:27.832809: step 1086, loss 0.130215, acc 0.9375
2016-07-27T15:05:30.976631: step 1087, loss 0.117345, acc 0.96875
2016-07-27T15:05:33.652415: step 1088, loss 0.12633, acc 0.953125
2016-07-27T15:05:37.033313: step 1089, loss 0.174474, acc 0.921875
2016-07-27T15:05:39.835788: step 1090, loss 0.178178, acc 0.953125
2016-07-27T15:05:42.780397: step 1091, loss 0.196859, acc 0.921875
2016-07-27T15:05:45.484328: step 1092, loss 0.222574, acc 0.921875
2016-07-27T15:05:48.575820: step 1093, loss 0.227916, acc 0.875
2016-07-27T15:05:51.511394: step 1094, loss 0.162609, acc 0.90625
2016-07-27T15:05:54.945248: step 1095, loss 0.166116, acc 0.921875
2016-07-27T15:05:57.999507: step 1096, loss 0.220869, acc 0.90625
2016-07-27T15:06:00.967993: step 1097, loss 0.329996, acc 0.875
2016-07-27T15:06:03.807856: step 1098, loss 0.162117, acc 0.9375
2016-07-27T15:06:07.195560: step 1099, loss 0.224387, acc 0.890625
2016-07-27T15:06:10.533363: step 1100, loss 0.137135, acc 0.953125

Evaluation:
2016-07-27T15:06:23.035604: step 1100, loss 0.870863, acc 0.715

Saved model checkpoint to /home/chaitanya/lstm-context-embeddings/runs/1469600026/checkpoints/model-1100

2016-07-27T15:06:27.335035: step 1101, loss 0.165703, acc 0.9375
2016-07-27T15:06:30.336116: step 1102, loss 0.0988789, acc 0.984375
2016-07-27T15:06:33.031393: step 1103, loss 0.291421, acc 0.90625
2016-07-27T15:06:35.539410: step 1104, loss 0.176138, acc 0.890625
2016-07-27T15:06:38.369426: step 1105, loss 0.194082, acc 0.9375
2016-07-27T15:06:40.945133: step 1106, loss 0.160452, acc 0.9375
2016-07-27T15:06:43.714342: step 1107, loss 0.185163, acc 0.9375
2016-07-27T15:06:46.395212: step 1108, loss 0.201652, acc 0.90625
2016-07-27T15:06:49.013947: step 1109, loss 0.248516, acc 0.90625
2016-07-27T15:06:51.742941: step 1110, loss 0.270471, acc 0.90625
2016-07-27T15:06:54.826482: step 1111, loss 0.273004, acc 0.890625
2016-07-27T15:06:57.570236: step 1112, loss 0.192479, acc 0.921875
2016-07-27T15:07:00.651884: step 1113, loss 0.124947, acc 0.984375
2016-07-27T15:07:04.138299: step 1114, loss 0.167706, acc 0.953125
2016-07-27T15:07:08.879697: step 1115, loss 0.36574, acc 0.859375
2016-07-27T15:07:12.354925: step 1116, loss 0.224489, acc 0.90625
2016-07-27T15:07:15.143527: step 1117, loss 0.250473, acc 0.875
2016-07-27T15:07:18.182199: step 1118, loss 0.190056, acc 0.90625
2016-07-27T15:07:20.795770: step 1119, loss 0.215537, acc 0.90625
2016-07-27T15:07:24.002186: step 1120, loss 0.150315, acc 0.953125
2016-07-27T15:07:26.884872: step 1121, loss 0.205557, acc 0.890625
2016-07-27T15:07:30.022054: step 1122, loss 0.223854, acc 0.90625
2016-07-27T15:07:33.061269: step 1123, loss 0.213672, acc 0.890625
2016-07-27T15:07:36.100207: step 1124, loss 0.210236, acc 0.921875
2016-07-27T15:07:38.964366: step 1125, loss 0.141674, acc 0.921875
2016-07-27T15:07:41.617805: step 1126, loss 0.186685, acc 0.875
2016-07-27T15:07:44.437115: step 1127, loss 0.195483, acc 0.90625
2016-07-27T15:07:47.183569: step 1128, loss 0.285937, acc 0.828125
2016-07-27T15:07:49.891039: step 1129, loss 0.198701, acc 0.90625
2016-07-27T15:07:52.718574: step 1130, loss 0.179778, acc 0.921875
2016-07-27T15:07:55.327458: step 1131, loss 0.315688, acc 0.84375
2016-07-27T15:07:58.137565: step 1132, loss 0.132307, acc 0.953125
2016-07-27T15:08:00.830057: step 1133, loss 0.181725, acc 0.921875
2016-07-27T15:08:03.500063: step 1134, loss 0.277349, acc 0.875
2016-07-27T15:08:06.310137: step 1135, loss 0.137452, acc 0.953125
2016-07-27T15:08:08.930252: step 1136, loss 0.221644, acc 0.890625
2016-07-27T15:08:11.745693: step 1137, loss 0.151894, acc 0.96875
2016-07-27T15:08:14.425421: step 1138, loss 0.136998, acc 0.984375
2016-07-27T15:08:17.177804: step 1139, loss 0.267758, acc 0.9375
2016-07-27T15:08:19.980080: step 1140, loss 0.250383, acc 0.875
2016-07-27T15:08:22.669346: step 1141, loss 0.197351, acc 0.9375
2016-07-27T15:08:25.492796: step 1142, loss 0.178469, acc 0.9375
2016-07-27T15:08:28.047551: step 1143, loss 0.167142, acc 0.96875
2016-07-27T15:08:30.844806: step 1144, loss 0.217058, acc 0.875
2016-07-27T15:08:33.543648: step 1145, loss 0.160176, acc 0.953125
2016-07-27T15:08:36.334779: step 1146, loss 0.200902, acc 0.953125
2016-07-27T15:08:39.110585: step 1147, loss 0.247455, acc 0.859375
2016-07-27T15:08:41.815535: step 1148, loss 0.181043, acc 0.9375
2016-07-27T15:08:44.652701: step 1149, loss 0.21627, acc 0.859375
2016-07-27T15:08:47.182746: step 1150, loss 0.107521, acc 0.984375
2016-07-27T15:08:50.000542: step 1151, loss 0.229326, acc 0.875
2016-07-27T15:08:52.697860: step 1152, loss 0.195876, acc 0.921875
2016-07-27T15:08:55.539874: step 1153, loss 0.288294, acc 0.90625
2016-07-27T15:08:58.265797: step 1154, loss 0.242373, acc 0.90625
2016-07-27T15:09:00.993974: step 1155, loss 0.186506, acc 0.875
2016-07-27T15:09:03.838369: step 1156, loss 0.274534, acc 0.828125
2016-07-27T15:09:06.395410: step 1157, loss 0.167481, acc 0.9375
2016-07-27T15:09:09.173547: step 1158, loss 0.143635, acc 0.953125
2016-07-27T15:09:11.847234: step 1159, loss 0.265382, acc 0.890625
2016-07-27T15:09:14.689169: step 1160, loss 0.240685, acc 0.890625
2016-07-27T15:09:17.391877: step 1161, loss 0.176331, acc 0.9375
2016-07-27T15:09:20.145554: step 1162, loss 0.318798, acc 0.90625
2016-07-27T15:09:22.981875: step 1163, loss 0.159395, acc 0.953125
2016-07-27T15:09:25.620996: step 1164, loss 0.227781, acc 0.921875
2016-07-27T15:09:28.428004: step 1165, loss 0.154478, acc 0.9375
2016-07-27T15:09:31.034272: step 1166, loss 0.190175, acc 0.953125
2016-07-27T15:09:33.900332: step 1167, loss 0.244565, acc 0.890625
2016-07-27T15:09:36.597136: step 1168, loss 0.131517, acc 1
2016-07-27T15:09:39.307842: step 1169, loss 0.130357, acc 0.96875
2016-07-27T15:09:42.167414: step 1170, loss 0.15965, acc 0.953125
2016-07-27T15:09:44.669431: step 1171, loss 0.159272, acc 0.984375
2016-07-27T15:09:47.497381: step 1172, loss 0.254058, acc 0.890625
2016-07-27T15:09:50.150179: step 1173, loss 0.245538, acc 0.890625
2016-07-27T15:09:53.026571: step 1174, loss 0.218481, acc 0.890625
2016-07-27T15:09:55.703249: step 1175, loss 0.203236, acc 0.90625
2016-07-27T15:09:58.430534: step 1176, loss 0.27832, acc 0.859375
2016-07-27T15:10:01.211061: step 1177, loss 0.250724, acc 0.890625
2016-07-27T15:10:03.936050: step 1178, loss 0.206579, acc 0.890625
2016-07-27T15:10:06.838044: step 1179, loss 0.182438, acc 0.890625
2016-07-27T15:10:09.421539: step 1180, loss 0.159328, acc 0.921875
2016-07-27T15:10:12.255094: step 1181, loss 0.13232, acc 0.96875
2016-07-27T15:10:14.983661: step 1182, loss 0.254369, acc 0.890625
2016-07-27T15:10:17.778636: step 1183, loss 0.256528, acc 0.875
2016-07-27T15:10:20.444579: step 1184, loss 0.2872, acc 0.890625
2016-07-27T15:10:23.110770: step 1185, loss 0.137093, acc 0.953125
2016-07-27T15:10:25.939189: step 1186, loss 0.101549, acc 0.96875
2016-07-27T15:10:28.489879: step 1187, loss 0.219718, acc 0.90625
2016-07-27T15:10:31.341273: step 1188, loss 0.129041, acc 0.96875
2016-07-27T15:10:33.948167: step 1189, loss 0.183472, acc 0.90625
2016-07-27T15:10:36.758646: step 1190, loss 0.235552, acc 0.921875
2016-07-27T15:10:39.448632: step 1191, loss 0.248023, acc 0.921875
2016-07-27T15:10:42.020242: step 1192, loss 0.262633, acc 0.921875
2016-07-27T15:10:44.845400: step 1193, loss 0.190091, acc 0.953125
2016-07-27T15:10:47.400786: step 1194, loss 0.26391, acc 0.875
2016-07-27T15:10:50.241826: step 1195, loss 0.219439, acc 0.921875
2016-07-27T15:10:53.087304: step 1196, loss 0.151389, acc 0.9375
2016-07-27T15:10:55.665595: step 1197, loss 0.219361, acc 0.953125
2016-07-27T15:10:58.454960: step 1198, loss 0.195086, acc 0.921875
2016-07-27T15:11:01.047435: step 1199, loss 0.17323, acc 0.9375
2016-07-27T15:11:03.887707: step 1200, loss 0.142976, acc 0.953125

Evaluation:
2016-07-27T15:11:16.079235: step 1200, loss 0.88659, acc 0.726

Saved model checkpoint to /home/chaitanya/lstm-context-embeddings/runs/1469600026/checkpoints/model-1200

2016-07-27T15:11:20.455447: step 1201, loss 0.0955234, acc 1
2016-07-27T15:11:23.296882: step 1202, loss 0.131286, acc 0.96875
2016-07-27T15:11:26.065751: step 1203, loss 0.242734, acc 0.859375
2016-07-27T15:11:28.590295: step 1204, loss 0.130046, acc 0.96875
2016-07-27T15:11:31.039251: step 1205, loss 0.197311, acc 0.9375
2016-07-27T15:11:33.876237: step 1206, loss 0.150185, acc 0.984375
2016-07-27T15:11:36.487106: step 1207, loss 0.249511, acc 0.90625
2016-07-27T15:11:39.376737: step 1208, loss 0.0911418, acc 0.967742
2016-07-27T15:11:42.032004: step 1209, loss 0.125403, acc 0.9375
2016-07-27T15:11:44.853152: step 1210, loss 0.109702, acc 0.984375
2016-07-27T15:11:47.602643: step 1211, loss 0.124647, acc 0.953125
2016-07-27T15:11:50.325048: step 1212, loss 0.138543, acc 0.953125
2016-07-27T15:11:53.401762: step 1213, loss 0.136449, acc 0.953125
2016-07-27T15:11:56.040014: step 1214, loss 0.109867, acc 0.96875
2016-07-27T15:11:58.839460: step 1215, loss 0.165696, acc 0.921875
2016-07-27T15:12:01.462481: step 1216, loss 0.0824587, acc 0.984375
2016-07-27T15:12:03.981089: step 1217, loss 0.136817, acc 0.9375
2016-07-27T15:12:06.867179: step 1218, loss 0.0878408, acc 0.96875
2016-07-27T15:12:09.440264: step 1219, loss 0.161647, acc 0.9375
2016-07-27T15:12:12.287823: step 1220, loss 0.0873929, acc 0.984375
2016-07-27T15:12:14.992930: step 1221, loss 0.162034, acc 0.9375
2016-07-27T15:12:17.797066: step 1222, loss 0.109912, acc 0.96875
2016-07-27T15:12:20.553477: step 1223, loss 0.114999, acc 0.953125
2016-07-27T15:12:23.204063: step 1224, loss 0.113548, acc 0.921875
2016-07-27T15:12:26.062152: step 1225, loss 0.259699, acc 0.921875
2016-07-27T15:12:28.658338: step 1226, loss 0.114024, acc 0.96875
2016-07-27T15:12:31.514209: step 1227, loss 0.0940821, acc 0.96875
2016-07-27T15:12:34.214244: step 1228, loss 0.13115, acc 0.921875
2016-07-27T15:12:37.000602: step 1229, loss 0.15476, acc 0.90625
2016-07-27T15:12:39.716705: step 1230, loss 0.206804, acc 0.921875
2016-07-27T15:12:42.410791: step 1231, loss 0.185753, acc 0.890625
2016-07-27T15:12:45.257426: step 1232, loss 0.10114, acc 0.9375
2016-07-27T15:12:47.756491: step 1233, loss 0.192369, acc 0.90625
2016-07-27T15:12:50.577468: step 1234, loss 0.160643, acc 0.96875
2016-07-27T15:12:53.244971: step 1235, loss 0.18027, acc 0.921875
2016-07-27T15:12:56.059594: step 1236, loss 0.207364, acc 0.921875
2016-07-27T15:12:58.821795: step 1237, loss 0.139036, acc 0.953125
2016-07-27T15:13:01.340073: step 1238, loss 0.19351, acc 0.9375
2016-07-27T15:13:04.138397: step 1239, loss 0.166613, acc 0.9375
2016-07-27T15:13:06.772626: step 1240, loss 0.139113, acc 0.96875
2016-07-27T15:13:09.615584: step 1241, loss 0.16226, acc 0.953125
2016-07-27T15:13:12.263278: step 1242, loss 0.231983, acc 0.9375
2016-07-27T15:13:15.110366: step 1243, loss 0.118324, acc 0.96875
2016-07-27T15:13:17.859865: step 1244, loss 0.153993, acc 0.9375
2016-07-27T15:13:20.373245: step 1245, loss 0.185309, acc 0.921875
2016-07-27T15:13:23.175780: step 1246, loss 0.0856862, acc 0.96875
2016-07-27T15:13:25.852582: step 1247, loss 0.131733, acc 0.953125
2016-07-27T15:13:28.548672: step 1248, loss 0.203844, acc 0.875
2016-07-27T15:13:31.174500: step 1249, loss 0.168499, acc 0.953125
2016-07-27T15:13:34.024916: step 1250, loss 0.297038, acc 0.875
2016-07-27T15:13:36.664613: step 1251, loss 0.144543, acc 0.9375
2016-07-27T15:13:39.266267: step 1252, loss 0.149725, acc 0.953125
2016-07-27T15:13:42.096573: step 1253, loss 0.169317, acc 0.953125
2016-07-27T15:13:44.769617: step 1254, loss 0.110799, acc 0.953125
2016-07-27T15:13:47.640923: step 1255, loss 0.183148, acc 0.90625
2016-07-27T15:13:50.269545: step 1256, loss 0.154136, acc 0.953125
2016-07-27T15:13:53.017705: step 1257, loss 0.185542, acc 0.953125
2016-07-27T15:13:55.838316: step 1258, loss 0.140597, acc 0.984375
2016-07-27T15:13:58.528272: step 1259, loss 0.164534, acc 0.9375
2016-07-27T15:14:01.346339: step 1260, loss 0.190308, acc 0.921875
2016-07-27T15:14:03.911119: step 1261, loss 0.142357, acc 0.953125
2016-07-27T15:14:06.859649: step 1262, loss 0.140933, acc 0.953125
2016-07-27T15:14:09.476699: step 1263, loss 0.131159, acc 0.953125
2016-07-27T15:14:12.268300: step 1264, loss 0.129769, acc 0.953125
2016-07-27T15:14:15.077418: step 1265, loss 0.140983, acc 0.953125
2016-07-27T15:14:17.631620: step 1266, loss 0.180282, acc 0.921875
2016-07-27T15:14:20.433178: step 1267, loss 0.190293, acc 0.90625
2016-07-27T15:14:23.106487: step 1268, loss 0.124144, acc 0.984375
2016-07-27T15:14:25.822187: step 1269, loss 0.0893124, acc 0.96875
2016-07-27T15:14:28.425699: step 1270, loss 0.26099, acc 0.890625
2016-07-27T15:14:31.267075: step 1271, loss 0.172428, acc 0.921875
2016-07-27T15:14:33.932203: step 1272, loss 0.180077, acc 0.953125
2016-07-27T15:14:36.784755: step 1273, loss 0.143786, acc 0.953125
2016-07-27T15:14:39.466131: step 1274, loss 0.201216, acc 0.875
2016-07-27T15:14:42.312549: step 1275, loss 0.200349, acc 0.890625
2016-07-27T15:14:45.094933: step 1276, loss 0.146044, acc 0.921875
2016-07-27T15:14:47.602919: step 1277, loss 0.154953, acc 0.96875
2016-07-27T15:14:50.435450: step 1278, loss 0.234878, acc 0.90625
2016-07-27T15:14:53.096821: step 1279, loss 0.244794, acc 0.875
2016-07-27T15:14:55.947749: step 1280, loss 0.185317, acc 0.9375
2016-07-27T15:14:58.588078: step 1281, loss 0.278932, acc 0.921875
2016-07-27T15:15:01.425975: step 1282, loss 0.22026, acc 0.890625
2016-07-27T15:15:04.119237: step 1283, loss 0.151799, acc 0.953125
2016-07-27T15:15:06.870500: step 1284, loss 0.217143, acc 0.890625
2016-07-27T15:15:09.693438: step 1285, loss 0.197571, acc 0.953125
2016-07-27T15:15:12.307522: step 1286, loss 0.167795, acc 0.96875
2016-07-27T15:15:15.154182: step 1287, loss 0.233584, acc 0.921875
2016-07-27T15:15:17.744636: step 1288, loss 0.251293, acc 0.890625
2016-07-27T15:15:20.625971: step 1289, loss 0.190939, acc 0.9375
2016-07-27T15:15:23.315778: step 1290, loss 0.201672, acc 0.9375
2016-07-27T15:15:25.938423: step 1291, loss 0.113954, acc 0.96875
2016-07-27T15:15:28.739558: step 1292, loss 0.13651, acc 0.984375
2016-07-27T15:15:31.396989: step 1293, loss 0.199655, acc 0.90625
2016-07-27T15:15:34.158967: step 1294, loss 0.164809, acc 0.953125
2016-07-27T15:15:36.949779: step 1295, loss 0.250686, acc 0.921875
2016-07-27T15:15:39.619134: step 1296, loss 0.197191, acc 0.953125
2016-07-27T15:15:42.440376: step 1297, loss 0.170029, acc 0.953125
2016-07-27T15:15:45.014902: step 1298, loss 0.174274, acc 0.9375
2016-07-27T15:15:47.629426: step 1299, loss 0.156413, acc 0.953125
2016-07-27T15:15:50.436519: step 1300, loss 0.132526, acc 0.953125

Evaluation:
2016-07-27T15:16:02.672366: step 1300, loss 0.945647, acc 0.711

Saved model checkpoint to /home/chaitanya/lstm-context-embeddings/runs/1469600026/checkpoints/model-1300

2016-07-27T15:16:06.936037: step 1301, loss 0.0940885, acc 0.96875
2016-07-27T15:16:09.785282: step 1302, loss 0.181203, acc 0.921875
2016-07-27T15:16:12.482818: step 1303, loss 0.158372, acc 0.96875
2016-07-27T15:16:15.033962: step 1304, loss 0.0773316, acc 0.984375
2016-07-27T15:16:17.876112: step 1305, loss 0.196944, acc 0.90625
2016-07-27T15:16:20.569631: step 1306, loss 0.122772, acc 0.921875
2016-07-27T15:16:23.184727: step 1307, loss 0.155777, acc 0.9375
2016-07-27T15:16:26.026863: step 1308, loss 0.119716, acc 0.96875
2016-07-27T15:16:28.662029: step 1309, loss 0.18918, acc 0.96875
2016-07-27T15:16:31.427207: step 1310, loss 0.303684, acc 0.921875
2016-07-27T15:16:34.173472: step 1311, loss 0.148545, acc 0.96875
2016-07-27T15:16:36.879575: step 1312, loss 0.103424, acc 0.96875
2016-07-27T15:16:39.669929: step 1313, loss 0.157436, acc 0.9375
2016-07-27T15:16:42.227409: step 1314, loss 0.14988, acc 0.921875
2016-07-27T15:16:45.046471: step 1315, loss 0.141894, acc 0.9375
2016-07-27T15:16:47.733635: step 1316, loss 0.146711, acc 0.9375
2016-07-27T15:16:50.530432: step 1317, loss 0.164553, acc 0.953125
2016-07-27T15:16:53.272202: step 1318, loss 0.174932, acc 0.9375
2016-07-27T15:16:55.982821: step 1319, loss 0.189291, acc 0.90625
2016-07-27T15:16:58.806824: step 1320, loss 0.224974, acc 0.921875
2016-07-27T15:17:01.382529: step 1321, loss 0.197311, acc 0.9375
2016-07-27T15:17:04.260704: step 1322, loss 0.21964, acc 0.921875
2016-07-27T15:17:06.918038: step 1323, loss 0.135261, acc 0.96875
2016-07-27T15:17:09.737328: step 1324, loss 0.329439, acc 0.875
2016-07-27T15:17:12.405237: step 1325, loss 0.301738, acc 0.890625
2016-07-27T15:17:15.117825: step 1326, loss 0.18849, acc 0.890625
2016-07-27T15:17:17.974968: step 1327, loss 0.222535, acc 0.90625
2016-07-27T15:17:20.387302: step 1328, loss 0.148845, acc 0.953125
2016-07-27T15:17:23.218270: step 1329, loss 0.191049, acc 0.953125
2016-07-27T15:17:25.861176: step 1330, loss 0.327684, acc 0.9375
2016-07-27T15:17:28.697002: step 1331, loss 0.15522, acc 0.9375
2016-07-27T15:17:31.394400: step 1332, loss 0.180928, acc 0.921875
2016-07-27T15:17:34.110554: step 1333, loss 0.130771, acc 0.9375
2016-07-27T15:17:36.957445: step 1334, loss 0.219135, acc 0.921875
2016-07-27T15:17:39.521869: step 1335, loss 0.196223, acc 0.890625
2016-07-27T15:17:42.316118: step 1336, loss 0.266368, acc 0.90625
2016-07-27T15:17:44.982723: step 1337, loss 0.12708, acc 0.953125
2016-07-27T15:17:47.847580: step 1338, loss 0.187998, acc 0.890625
2016-07-27T15:17:50.552992: step 1339, loss 0.12551, acc 0.953125
2016-07-27T15:17:53.256407: step 1340, loss 0.164075, acc 0.921875
2016-07-27T15:17:56.067856: step 1341, loss 0.195525, acc 0.9375
2016-07-27T15:17:58.622830: step 1342, loss 0.200023, acc 0.890625
2016-07-27T15:18:01.390368: step 1343, loss 0.138421, acc 0.9375
2016-07-27T15:18:04.042374: step 1344, loss 0.162183, acc 0.9375
2016-07-27T15:18:06.997193: step 1345, loss 0.199187, acc 0.9375
2016-07-27T15:18:09.674571: step 1346, loss 0.208601, acc 0.90625
2016-07-27T15:18:12.416243: step 1347, loss 0.137202, acc 0.9375
2016-07-27T15:18:15.303530: step 1348, loss 0.190089, acc 0.90625
2016-07-27T15:18:17.770199: step 1349, loss 0.238696, acc 0.9375
2016-07-27T15:18:20.576253: step 1350, loss 0.120873, acc 0.9375
2016-07-27T15:18:23.191671: step 1351, loss 0.0596064, acc 1
2016-07-27T15:18:26.020538: step 1352, loss 0.0979822, acc 0.9375
2016-07-27T15:18:28.743362: step 1353, loss 0.173624, acc 0.921875
2016-07-27T15:18:31.342889: step 1354, loss 0.144943, acc 0.921875
2016-07-27T15:18:34.145148: step 1355, loss 0.0816771, acc 1
2016-07-27T15:18:36.809316: step 1356, loss 0.205229, acc 0.90625
2016-07-27T15:18:39.397455: step 1357, loss 0.185543, acc 0.921875
2016-07-27T15:18:42.164700: step 1358, loss 0.213338, acc 0.9375
2016-07-27T15:18:44.854422: step 1359, loss 0.150775, acc 0.903226
2016-07-27T15:18:47.624708: step 1360, loss 0.116047, acc 0.96875
2016-07-27T15:18:50.398207: step 1361, loss 0.182635, acc 0.90625
2016-07-27T15:18:53.067289: step 1362, loss 0.15448, acc 0.953125
2016-07-27T15:18:55.855841: step 1363, loss 0.132761, acc 0.953125
2016-07-27T15:18:58.448188: step 1364, loss 0.187077, acc 0.9375
2016-07-27T15:19:01.297279: step 1365, loss 0.105213, acc 0.953125
2016-07-27T15:19:03.980390: step 1366, loss 0.0840926, acc 1
2016-07-27T15:19:06.610894: step 1367, loss 0.135299, acc 0.9375
2016-07-27T15:19:09.243785: step 1368, loss 0.0916275, acc 0.96875
2016-07-27T15:19:12.013267: step 1369, loss 0.108708, acc 1
2016-07-27T15:19:14.674822: step 1370, loss 0.17634, acc 0.921875
2016-07-27T15:19:17.522775: step 1371, loss 0.150054, acc 0.9375
2016-07-27T15:19:20.188853: step 1372, loss 0.145509, acc 0.9375
2016-07-27T15:19:23.000951: step 1373, loss 0.123361, acc 0.921875
2016-07-27T15:19:25.778835: step 1374, loss 0.0882134, acc 0.953125
2016-07-27T15:19:28.378698: step 1375, loss 0.0594044, acc 0.96875
2016-07-27T15:19:31.238294: step 1376, loss 0.0929584, acc 0.984375
2016-07-27T15:19:33.926172: step 1377, loss 0.126887, acc 0.96875
2016-07-27T15:19:36.527525: step 1378, loss 0.10634, acc 0.953125
2016-07-27T15:19:39.389904: step 1379, loss 0.0849031, acc 1
2016-07-27T15:19:42.066385: step 1380, loss 0.130946, acc 0.9375
2016-07-27T15:19:44.656864: step 1381, loss 0.0915205, acc 0.984375
2016-07-27T15:19:47.300863: step 1382, loss 0.135218, acc 0.9375
2016-07-27T15:19:50.162257: step 1383, loss 0.0875758, acc 0.984375
2016-07-27T15:19:52.778436: step 1384, loss 0.17212, acc 0.90625
2016-07-27T15:19:55.481448: step 1385, loss 0.0881491, acc 0.984375
2016-07-27T15:19:58.349211: step 1386, loss 0.0964484, acc 0.953125
2016-07-27T15:20:01.036140: step 1387, loss 0.107832, acc 0.96875
2016-07-27T15:20:03.633564: step 1388, loss 0.153069, acc 0.9375
2016-07-27T15:20:06.448059: step 1389, loss 0.116745, acc 0.96875
2016-07-27T15:20:09.138188: step 1390, loss 0.0768757, acc 0.984375
2016-07-27T15:20:11.935246: step 1391, loss 0.0768546, acc 0.96875
2016-07-27T15:20:14.665569: step 1392, loss 0.108961, acc 0.96875
2016-07-27T15:20:17.127208: step 1393, loss 0.131125, acc 0.9375
2016-07-27T15:20:19.983722: step 1394, loss 0.0868309, acc 0.984375
2016-07-27T15:20:22.580566: step 1395, loss 0.103329, acc 0.953125
2016-07-27T15:20:25.421580: step 1396, loss 0.168766, acc 0.9375
2016-07-27T15:20:28.276234: step 1397, loss 0.0573661, acc 1
2016-07-27T15:20:30.764078: step 1398, loss 0.102768, acc 0.953125
2016-07-27T15:20:33.595741: step 1399, loss 0.071481, acc 0.984375
2016-07-27T15:20:36.246565: step 1400, loss 0.126521, acc 0.953125

Evaluation:
2016-07-27T15:20:48.396956: step 1400, loss 1.23715, acc 0.698

Saved model checkpoint to /home/chaitanya/lstm-context-embeddings/runs/1469600026/checkpoints/model-1400

2016-07-27T15:20:52.234150: step 1401, loss 0.121385, acc 0.953125
2016-07-27T15:20:55.018042: step 1402, loss 0.231023, acc 0.921875
2016-07-27T15:20:57.658719: step 1403, loss 0.130769, acc 0.9375
2016-07-27T15:21:00.480098: step 1404, loss 0.0756497, acc 0.96875
2016-07-27T15:21:03.150878: step 1405, loss 0.0945812, acc 0.96875
2016-07-27T15:21:05.648639: step 1406, loss 0.238929, acc 0.9375
2016-07-27T15:21:08.406663: step 1407, loss 0.127821, acc 0.953125
2016-07-27T15:21:10.976097: step 1408, loss 0.190189, acc 0.90625
2016-07-27T15:21:13.588175: step 1409, loss 0.1471, acc 0.9375
2016-07-27T15:21:16.406618: step 1410, loss 0.160844, acc 0.921875
2016-07-27T15:21:19.141234: step 1411, loss 0.140095, acc 0.921875
2016-07-27T15:21:21.648398: step 1412, loss 0.104027, acc 0.953125
2016-07-27T15:21:24.403604: step 1413, loss 0.145597, acc 0.953125
2016-07-27T15:21:26.911324: step 1414, loss 0.124688, acc 0.953125
2016-07-27T15:21:29.578339: step 1415, loss 0.103745, acc 0.984375
2016-07-27T15:21:32.428637: step 1416, loss 0.171792, acc 0.890625
2016-07-27T15:21:35.077560: step 1417, loss 0.0873323, acc 0.96875
2016-07-27T15:21:37.567513: step 1418, loss 0.110717, acc 0.96875
2016-07-27T15:21:40.427819: step 1419, loss 0.0836018, acc 0.984375
2016-07-27T15:21:43.013989: step 1420, loss 0.102235, acc 0.953125
2016-07-27T15:21:45.668481: step 1421, loss 0.141281, acc 0.953125
2016-07-27T15:21:48.552212: step 1422, loss 0.111109, acc 0.96875
2016-07-27T15:21:51.176740: step 1423, loss 0.124691, acc 0.96875
2016-07-27T15:21:53.701837: step 1424, loss 0.182187, acc 0.984375
2016-07-27T15:21:56.524519: step 1425, loss 0.220199, acc 0.890625
2016-07-27T15:21:59.155107: step 1426, loss 0.0959773, acc 0.984375
2016-07-27T15:22:01.789156: step 1427, loss 0.173278, acc 0.90625
2016-07-27T15:22:04.615276: step 1428, loss 0.114466, acc 0.953125
2016-07-27T15:22:07.298123: step 1429, loss 0.098553, acc 0.953125
2016-07-27T15:22:09.826623: step 1430, loss 0.142106, acc 0.953125
2016-07-27T15:22:12.447009: step 1431, loss 0.17293, acc 0.9375
2016-07-27T15:22:15.244730: step 1432, loss 0.12815, acc 0.9375
2016-07-27T15:22:17.865583: step 1433, loss 0.113109, acc 0.953125
2016-07-27T15:22:20.692959: step 1434, loss 0.0962812, acc 0.953125
2016-07-27T15:22:23.338878: step 1435, loss 0.105333, acc 0.953125
2016-07-27T15:22:25.890060: step 1436, loss 0.14313, acc 0.9375
2016-07-27T15:22:28.727289: step 1437, loss 0.164598, acc 0.921875
2016-07-27T15:22:31.319940: step 1438, loss 0.0893777, acc 0.984375
2016-07-27T15:22:33.921948: step 1439, loss 0.136592, acc 0.9375
2016-07-27T15:22:36.803339: step 1440, loss 0.104711, acc 0.984375
2016-07-27T15:22:39.497576: step 1441, loss 0.178869, acc 0.90625
2016-07-27T15:22:42.061382: step 1442, loss 0.22906, acc 0.859375
2016-07-27T15:22:44.725918: step 1443, loss 0.190938, acc 0.90625
2016-07-27T15:22:47.544807: step 1444, loss 0.103431, acc 0.96875
2016-07-27T15:22:50.103440: step 1445, loss 0.106823, acc 0.96875
2016-07-27T15:22:52.767876: step 1446, loss 0.104235, acc 0.984375
2016-07-27T15:22:55.433602: step 1447, loss 0.112618, acc 0.953125
2016-07-27T15:22:58.198391: step 1448, loss 0.194449, acc 0.890625
2016-07-27T15:23:00.809365: step 1449, loss 0.127551, acc 0.953125
2016-07-27T15:23:03.650070: step 1450, loss 0.193098, acc 0.953125
2016-07-27T15:23:06.287589: step 1451, loss 0.145785, acc 0.953125
2016-07-27T15:23:08.928653: step 1452, loss 0.103949, acc 0.953125
2016-07-27T15:23:11.726995: step 1453, loss 0.147564, acc 0.953125
2016-07-27T15:23:14.223012: step 1454, loss 0.232603, acc 0.9375
2016-07-27T15:23:16.706114: step 1455, loss 0.167564, acc 0.90625
2016-07-27T15:23:19.556301: step 1456, loss 0.159645, acc 0.921875
2016-07-27T15:23:22.114881: step 1457, loss 0.100027, acc 0.96875
2016-07-27T15:23:24.790869: step 1458, loss 0.198948, acc 0.953125
2016-07-27T15:23:27.479868: step 1459, loss 0.104984, acc 0.96875
2016-07-27T15:23:30.348194: step 1460, loss 0.182921, acc 0.9375
2016-07-27T15:23:32.856136: step 1461, loss 0.176222, acc 0.953125
2016-07-27T15:23:35.725864: step 1462, loss 0.185114, acc 0.9375
2016-07-27T15:23:38.389066: step 1463, loss 0.101271, acc 0.96875
2016-07-27T15:23:40.910940: step 1464, loss 0.110707, acc 0.984375
2016-07-27T15:23:43.733498: step 1465, loss 0.063109, acc 1
2016-07-27T15:23:46.277086: step 1466, loss 0.128624, acc 0.9375
2016-07-27T15:23:48.934058: step 1467, loss 0.0888547, acc 0.96875
2016-07-27T15:23:51.823931: step 1468, loss 0.139953, acc 0.921875
2016-07-27T15:23:54.522714: step 1469, loss 0.126293, acc 0.921875
2016-07-27T15:23:57.061884: step 1470, loss 0.107131, acc 0.96875
2016-07-27T15:23:59.912736: step 1471, loss 0.149567, acc 0.921875
2016-07-27T15:24:02.468637: step 1472, loss 0.209777, acc 0.890625
2016-07-27T15:24:05.145066: step 1473, loss 0.151005, acc 0.953125
2016-07-27T15:24:08.022258: step 1474, loss 0.213554, acc 0.859375
2016-07-27T15:24:10.661337: step 1475, loss 0.145587, acc 0.9375
2016-07-27T15:24:13.276417: step 1476, loss 0.141569, acc 0.953125
2016-07-27T15:24:16.114273: step 1477, loss 0.122247, acc 0.96875
2016-07-27T15:24:18.742526: step 1478, loss 0.197145, acc 0.9375
2016-07-27T15:24:21.516148: step 1479, loss 0.106655, acc 0.953125
2016-07-27T15:24:24.277901: step 1480, loss 0.313458, acc 0.90625
2016-07-27T15:24:26.730143: step 1481, loss 0.0849913, acc 1
2016-07-27T15:24:29.566885: step 1482, loss 0.143229, acc 0.9375
2016-07-27T15:24:32.190081: step 1483, loss 0.10854, acc 0.96875
2016-07-27T15:24:35.001900: step 1484, loss 0.14033, acc 0.921875
2016-07-27T15:24:37.821869: step 1485, loss 0.160832, acc 0.953125
2016-07-27T15:24:40.362186: step 1486, loss 0.108645, acc 0.953125
2016-07-27T15:24:43.131399: step 1487, loss 0.0858092, acc 0.984375
2016-07-27T15:24:45.781516: step 1488, loss 0.123734, acc 0.953125
2016-07-27T15:24:48.551288: step 1489, loss 0.116501, acc 0.96875
2016-07-27T15:24:51.447677: step 1490, loss 0.0630044, acc 0.96875
2016-07-27T15:24:53.978326: step 1491, loss 0.119341, acc 0.96875
2016-07-27T15:24:56.864107: step 1492, loss 0.165966, acc 0.9375
2016-07-27T15:24:59.539412: step 1493, loss 0.119853, acc 0.953125
2016-07-27T15:25:02.071616: step 1494, loss 0.119725, acc 0.921875
2016-07-27T15:25:04.902281: step 1495, loss 0.118215, acc 0.921875
2016-07-27T15:25:07.438322: step 1496, loss 0.125049, acc 0.96875
2016-07-27T15:25:10.244369: step 1497, loss 0.129425, acc 0.9375
2016-07-27T15:25:12.930243: step 1498, loss 0.139266, acc 0.953125
2016-07-27T15:25:15.522741: step 1499, loss 0.0639565, acc 1
2016-07-27T15:25:18.317889: step 1500, loss 0.127357, acc 0.953125

Evaluation:
2016-07-27T15:25:30.429266: step 1500, loss 1.26109, acc 0.689

Saved model checkpoint to /home/chaitanya/lstm-context-embeddings/runs/1469600026/checkpoints/model-1500

2016-07-27T15:25:34.860770: step 1501, loss 0.117922, acc 0.9375
2016-07-27T15:25:37.271574: step 1502, loss 0.08286, acc 0.96875
2016-07-27T15:25:40.148913: step 1503, loss 0.12112, acc 0.96875
2016-07-27T15:25:42.822483: step 1504, loss 0.180134, acc 0.921875
2016-07-27T15:25:45.359249: step 1505, loss 0.244756, acc 0.9375
2016-07-27T15:25:48.224215: step 1506, loss 0.0814426, acc 0.96875
2016-07-27T15:25:50.762197: step 1507, loss 0.14045, acc 0.984375
2016-07-27T15:25:53.378801: step 1508, loss 0.126016, acc 0.9375
2016-07-27T15:25:56.218602: step 1509, loss 0.102405, acc 0.953125
2016-07-27T15:25:58.890926: step 1510, loss 0.087566, acc 0.951613
2016-07-27T15:26:01.449910: step 1511, loss 0.0727455, acc 0.96875
2016-07-27T15:26:04.296913: step 1512, loss 0.101246, acc 0.984375
2016-07-27T15:26:07.170712: step 1513, loss 0.138078, acc 0.953125
2016-07-27T15:26:09.558562: step 1514, loss 0.0630889, acc 1
2016-07-27T15:26:12.408083: step 1515, loss 0.0996546, acc 0.984375
2016-07-27T15:26:15.042602: step 1516, loss 0.0744493, acc 1
2016-07-27T15:26:17.853798: step 1517, loss 0.0802478, acc 0.96875
2016-07-27T15:26:20.545888: step 1518, loss 0.104644, acc 0.921875
2016-07-27T15:26:23.025631: step 1519, loss 0.0773679, acc 1
2016-07-27T15:26:25.831591: step 1520, loss 0.0685357, acc 0.984375
2016-07-27T15:26:28.429562: step 1521, loss 0.0908436, acc 0.984375
2016-07-27T15:26:31.276855: step 1522, loss 0.131053, acc 0.90625
2016-07-27T15:26:34.119212: step 1523, loss 0.195028, acc 0.875
2016-07-27T15:26:36.540448: step 1524, loss 0.0682403, acc 0.984375
2016-07-27T15:26:39.360540: step 1525, loss 0.0889055, acc 0.96875
2016-07-27T15:26:42.031148: step 1526, loss 0.145569, acc 0.96875
2016-07-27T15:26:44.569204: step 1527, loss 0.0955255, acc 0.953125
2016-07-27T15:26:47.350773: step 1528, loss 0.105963, acc 0.921875
2016-07-27T15:26:49.900741: step 1529, loss 0.0683597, acc 0.96875
2016-07-27T15:26:52.789381: step 1530, loss 0.0873149, acc 0.984375
2016-07-27T15:26:55.513405: step 1531, loss 0.142348, acc 0.953125
2016-07-27T15:26:57.929195: step 1532, loss 0.110013, acc 0.953125
2016-07-27T15:27:00.730162: step 1533, loss 0.114516, acc 0.9375
2016-07-27T15:27:03.311961: step 1534, loss 0.183239, acc 0.9375
2016-07-27T15:27:06.080034: step 1535, loss 0.135231, acc 0.9375
2016-07-27T15:27:08.805368: step 1536, loss 0.111219, acc 0.96875
2016-07-27T15:27:11.297923: step 1537, loss 0.112644, acc 0.9375
2016-07-27T15:27:14.133807: step 1538, loss 0.141657, acc 0.921875
2016-07-27T15:27:16.670284: step 1539, loss 0.0708594, acc 0.984375
2016-07-27T15:27:19.361354: step 1540, loss 0.0901068, acc 0.96875
2016-07-27T15:27:22.195644: step 1541, loss 0.0593971, acc 0.984375
2016-07-27T15:27:24.599680: step 1542, loss 0.104547, acc 0.96875
2016-07-27T15:27:27.417755: step 1543, loss 0.0919405, acc 0.96875
2016-07-27T15:27:30.048873: step 1544, loss 0.0599981, acc 0.96875
2016-07-27T15:27:32.586377: step 1545, loss 0.104247, acc 0.953125
2016-07-27T15:27:35.366962: step 1546, loss 0.132225, acc 0.953125
2016-07-27T15:27:37.901714: step 1547, loss 0.124758, acc 0.96875
2016-07-27T15:27:40.709515: step 1548, loss 0.0503269, acc 0.984375
2016-07-27T15:27:43.511310: step 1549, loss 0.0823212, acc 0.953125
2016-07-27T15:27:46.148071: step 1550, loss 0.180665, acc 0.921875
2016-07-27T15:27:48.989843: step 1551, loss 0.103666, acc 0.9375
2016-07-27T15:27:51.471207: step 1552, loss 0.120465, acc 0.953125
2016-07-27T15:27:54.332522: step 1553, loss 0.131921, acc 0.921875
2016-07-27T15:27:56.997750: step 1554, loss 0.143815, acc 0.9375
2016-07-27T15:27:59.525905: step 1555, loss 0.111345, acc 0.984375
2016-07-27T15:28:02.336787: step 1556, loss 0.0701404, acc 0.96875
2016-07-27T15:28:04.899884: step 1557, loss 0.225413, acc 0.921875
2016-07-27T15:28:07.484737: step 1558, loss 0.159853, acc 0.953125
2016-07-27T15:28:10.341083: step 1559, loss 0.0839945, acc 0.953125
2016-07-27T15:28:13.048870: step 1560, loss 0.207089, acc 0.875
2016-07-27T15:28:15.595336: step 1561, loss 0.185721, acc 0.875
2016-07-27T15:28:18.390583: step 1562, loss 0.078678, acc 0.984375
2016-07-27T15:28:20.919897: step 1563, loss 0.190239, acc 0.90625
2016-07-27T15:28:23.567956: step 1564, loss 0.169122, acc 0.921875
2016-07-27T15:28:26.441859: step 1565, loss 0.0943194, acc 0.984375
2016-07-27T15:28:29.096641: step 1566, loss 0.129628, acc 0.953125
2016-07-27T15:28:31.620732: step 1567, loss 0.0722119, acc 0.984375
2016-07-27T15:28:34.424596: step 1568, loss 0.0522112, acc 0.984375
2016-07-27T15:28:36.922961: step 1569, loss 0.126204, acc 0.953125
2016-07-27T15:28:39.594845: step 1570, loss 0.136521, acc 0.96875
2016-07-27T15:28:42.459156: step 1571, loss 0.0577693, acc 1
2016-07-27T15:28:45.091224: step 1572, loss 0.109121, acc 0.96875
2016-07-27T15:28:47.608117: step 1573, loss 0.128181, acc 0.953125
2016-07-27T15:28:50.416262: step 1574, loss 0.102854, acc 0.96875
2016-07-27T15:28:52.980303: step 1575, loss 0.094181, acc 0.953125
2016-07-27T15:28:55.625159: step 1576, loss 0.105076, acc 0.96875
2016-07-27T15:28:58.486357: step 1577, loss 0.0885999, acc 0.984375
2016-07-27T15:29:01.149236: step 1578, loss 0.0785918, acc 0.96875
2016-07-27T15:29:03.695598: step 1579, loss 0.0911695, acc 0.96875
2016-07-27T15:29:06.504701: step 1580, loss 0.133531, acc 0.953125
2016-07-27T15:29:09.027071: step 1581, loss 0.124806, acc 0.9375
2016-07-27T15:29:11.701886: step 1582, loss 0.121677, acc 0.9375
2016-07-27T15:29:14.552007: step 1583, loss 0.117847, acc 0.921875
2016-07-27T15:29:17.190009: step 1584, loss 0.137005, acc 0.90625
2016-07-27T15:29:19.702036: step 1585, loss 0.0831788, acc 0.96875
2016-07-27T15:29:22.521462: step 1586, loss 0.0730798, acc 0.96875
2016-07-27T15:29:25.180711: step 1587, loss 0.110184, acc 0.96875
2016-07-27T15:29:27.659300: step 1588, loss 0.124418, acc 0.9375
2016-07-27T15:29:30.481593: step 1589, loss 0.094435, acc 0.96875
2016-07-27T15:29:33.035437: step 1590, loss 0.157705, acc 0.921875
2016-07-27T15:29:35.845036: step 1591, loss 0.153468, acc 0.953125
2016-07-27T15:29:38.621638: step 1592, loss 0.142945, acc 0.9375
2016-07-27T15:29:41.035724: step 1593, loss 0.139709, acc 0.9375
2016-07-27T15:29:43.852921: step 1594, loss 0.0934781, acc 0.96875
2016-07-27T15:29:46.467629: step 1595, loss 0.1307, acc 0.96875
2016-07-27T15:29:49.216047: step 1596, loss 0.109492, acc 0.96875
2016-07-27T15:29:51.932326: step 1597, loss 0.232045, acc 0.921875
2016-07-27T15:29:54.394367: step 1598, loss 0.117131, acc 0.9375
2016-07-27T15:29:57.192146: step 1599, loss 0.10912, acc 0.96875
2016-07-27T15:29:59.792225: step 1600, loss 0.140403, acc 0.9375

Evaluation:
2016-07-27T15:30:11.827195: step 1600, loss 1.00502, acc 0.703

Saved model checkpoint to /home/chaitanya/lstm-context-embeddings/runs/1469600026/checkpoints/model-1600

2016-07-27T15:30:15.749069: step 1601, loss 0.243926, acc 0.90625
2016-07-27T15:30:18.552730: step 1602, loss 0.0833662, acc 0.984375
2016-07-27T15:30:21.204209: step 1603, loss 0.263077, acc 0.859375
2016-07-27T15:30:23.738531: step 1604, loss 0.14528, acc 0.953125
2016-07-27T15:30:26.546941: step 1605, loss 0.0864314, acc 0.984375
2016-07-27T15:30:29.079821: step 1606, loss 0.120556, acc 0.953125
2016-07-27T15:30:31.612704: step 1607, loss 0.093534, acc 0.96875
2016-07-27T15:30:34.477564: step 1608, loss 0.104022, acc 0.96875
2016-07-27T15:30:37.168306: step 1609, loss 0.116502, acc 0.953125
2016-07-27T15:30:39.623689: step 1610, loss 0.0974188, acc 0.96875
2016-07-27T15:30:42.416403: step 1611, loss 0.147798, acc 0.96875
2016-07-27T15:30:44.982953: step 1612, loss 0.178592, acc 0.890625
2016-07-27T15:30:47.742959: step 1613, loss 0.170959, acc 0.90625
2016-07-27T15:30:50.522000: step 1614, loss 0.106055, acc 0.953125
2016-07-27T15:30:52.980811: step 1615, loss 0.196398, acc 0.890625
2016-07-27T15:30:55.870355: step 1616, loss 0.119804, acc 0.953125
2016-07-27T15:30:58.475082: step 1617, loss 0.190682, acc 0.96875
2016-07-27T15:31:00.946197: step 1618, loss 0.060514, acc 0.984375
2016-07-27T15:31:03.742281: step 1619, loss 0.162867, acc 0.953125
2016-07-27T15:31:06.297232: step 1620, loss 0.152106, acc 0.921875
2016-07-27T15:31:08.965452: step 1621, loss 0.128557, acc 0.96875
2016-07-27T15:31:11.690178: step 1622, loss 0.126426, acc 0.953125
2016-07-27T15:31:14.097079: step 1623, loss 0.200046, acc 0.9375
2016-07-27T15:31:16.895494: step 1624, loss 0.133526, acc 0.9375
2016-07-27T15:31:19.513050: step 1625, loss 0.175174, acc 0.96875
2016-07-27T15:31:22.014082: step 1626, loss 0.0861224, acc 0.96875
2016-07-27T15:31:24.840495: step 1627, loss 0.0980965, acc 0.984375
2016-07-27T15:31:27.404819: step 1628, loss 0.113597, acc 0.953125
2016-07-27T15:31:30.034554: step 1629, loss 0.174832, acc 0.9375
2016-07-27T15:31:32.877520: step 1630, loss 0.21482, acc 0.890625
2016-07-27T15:31:35.618422: step 1631, loss 0.152749, acc 0.9375
2016-07-27T15:31:38.146329: step 1632, loss 0.106362, acc 0.953125
2016-07-27T15:31:40.921173: step 1633, loss 0.151809, acc 0.921875
2016-07-27T15:31:43.460378: step 1634, loss 0.0628953, acc 0.984375
2016-07-27T15:31:46.099902: step 1635, loss 0.148785, acc 0.9375
2016-07-27T15:31:48.942873: step 1636, loss 0.162796, acc 0.9375
2016-07-27T15:31:51.559608: step 1637, loss 0.110564, acc 0.953125
2016-07-27T15:31:54.301743: step 1638, loss 0.132272, acc 0.921875
2016-07-27T15:31:57.059793: step 1639, loss 0.0495333, acc 0.984375
2016-07-27T15:31:59.717479: step 1640, loss 0.173748, acc 0.90625
2016-07-27T15:32:02.529563: step 1641, loss 0.120817, acc 0.984375
2016-07-27T15:32:05.020000: step 1642, loss 0.167624, acc 0.9375
2016-07-27T15:32:07.990700: step 1643, loss 0.141439, acc 0.9375
2016-07-27T15:32:10.672636: step 1644, loss 0.153655, acc 0.9375
2016-07-27T15:32:13.201449: step 1645, loss 0.0926348, acc 0.953125
2016-07-27T15:32:15.978664: step 1646, loss 0.128616, acc 0.9375
2016-07-27T15:32:18.507738: step 1647, loss 0.0932212, acc 0.96875
2016-07-27T15:32:21.136516: step 1648, loss 0.205976, acc 0.9375
2016-07-27T15:32:24.031953: step 1649, loss 0.082752, acc 0.984375
2016-07-27T15:32:26.729773: step 1650, loss 0.154162, acc 0.953125
2016-07-27T15:32:29.286049: step 1651, loss 0.0817323, acc 0.96875
2016-07-27T15:32:32.103026: step 1652, loss 0.0655793, acc 0.984375
2016-07-27T15:32:34.624753: step 1653, loss 0.0691146, acc 0.984375
2016-07-27T15:32:37.215602: step 1654, loss 0.0829548, acc 0.953125
2016-07-27T15:32:40.030660: step 1655, loss 0.104256, acc 0.96875
2016-07-27T15:32:42.703172: step 1656, loss 0.110371, acc 0.96875
2016-07-27T15:32:45.268347: step 1657, loss 0.0768076, acc 0.953125
2016-07-27T15:32:48.041690: step 1658, loss 0.118401, acc 0.96875
2016-07-27T15:32:50.592141: step 1659, loss 0.131312, acc 0.953125
2016-07-27T15:32:53.251943: step 1660, loss 0.180866, acc 0.953125
2016-07-27T15:32:56.150162: step 1661, loss 0.0689574, acc 0.983871
2016-07-27T15:32:58.804687: step 1662, loss 0.0997348, acc 0.96875
2016-07-27T15:33:01.364782: step 1663, loss 0.11841, acc 0.953125
2016-07-27T15:33:04.169196: step 1664, loss 0.145219, acc 0.953125
2016-07-27T15:33:06.706438: step 1665, loss 0.0954413, acc 0.96875
2016-07-27T15:33:09.333213: step 1666, loss 0.0906662, acc 0.96875
2016-07-27T15:33:12.199870: step 1667, loss 0.115657, acc 0.953125
2016-07-27T15:33:14.868501: step 1668, loss 0.125315, acc 0.953125
2016-07-27T15:33:17.422247: step 1669, loss 0.0762316, acc 0.984375
2016-07-27T15:33:20.243239: step 1670, loss 0.0849853, acc 0.984375
2016-07-27T15:33:22.795810: step 1671, loss 0.182448, acc 0.921875
2016-07-27T15:33:25.368828: step 1672, loss 0.170135, acc 0.875
2016-07-27T15:33:28.228959: step 1673, loss 0.0223291, acc 1
2016-07-27T15:33:30.879711: step 1674, loss 0.0830705, acc 1
2016-07-27T15:33:33.438091: step 1675, loss 0.0773355, acc 0.984375
2016-07-27T15:33:36.260010: step 1676, loss 0.125919, acc 0.953125
2016-07-27T15:33:38.898462: step 1677, loss 0.159612, acc 0.9375
2016-07-27T15:33:41.427322: step 1678, loss 0.0847213, acc 0.96875
2016-07-27T15:33:44.234527: step 1679, loss 0.091783, acc 0.984375
2016-07-27T15:33:47.060837: step 1680, loss 0.0878535, acc 0.96875
2016-07-27T15:33:49.575548: step 1681, loss 0.127261, acc 0.953125
2016-07-27T15:33:52.362689: step 1682, loss 0.0632463, acc 0.984375
2016-07-27T15:33:54.982870: step 1683, loss 0.0523836, acc 1
2016-07-27T15:33:57.471274: step 1684, loss 0.101195, acc 0.953125
2016-07-27T15:34:00.277649: step 1685, loss 0.123807, acc 0.9375
2016-07-27T15:34:02.771865: step 1686, loss 0.188104, acc 0.90625
2016-07-27T15:34:05.521382: step 1687, loss 0.0859431, acc 0.96875
2016-07-27T15:34:08.337795: step 1688, loss 0.0963216, acc 0.96875
2016-07-27T15:34:10.745784: step 1689, loss 0.0802227, acc 0.96875
2016-07-27T15:34:13.548056: step 1690, loss 0.0618064, acc 1
2016-07-27T15:34:16.141874: step 1691, loss 0.0647761, acc 0.96875
2016-07-27T15:34:18.914664: step 1692, loss 0.0968557, acc 0.96875
2016-07-27T15:34:21.617478: step 1693, loss 0.108369, acc 0.953125
2016-07-27T15:34:24.258137: step 1694, loss 0.132735, acc 0.953125
2016-07-27T15:34:27.063023: step 1695, loss 0.0841098, acc 0.984375
2016-07-27T15:34:29.540529: step 1696, loss 0.076521, acc 0.96875
2016-07-27T15:34:32.385992: step 1697, loss 0.102024, acc 0.96875
2016-07-27T15:34:35.032618: step 1698, loss 0.111055, acc 0.953125
2016-07-27T15:34:37.581999: step 1699, loss 0.0530241, acc 0.984375
2016-07-27T15:34:40.319649: step 1700, loss 0.223991, acc 0.9375

Evaluation:
2016-07-27T15:34:52.152364: step 1700, loss 1.39377, acc 0.677

Saved model checkpoint to /home/chaitanya/lstm-context-embeddings/runs/1469600026/checkpoints/model-1700

2016-07-27T15:34:56.042916: step 1701, loss 0.135986, acc 0.9375
2016-07-27T15:34:58.826102: step 1702, loss 0.0697374, acc 0.96875
2016-07-27T15:35:01.452649: step 1703, loss 0.176706, acc 0.921875
2016-07-27T15:35:04.262360: step 1704, loss 0.1074, acc 0.984375
2016-07-27T15:35:06.953850: step 1705, loss 0.106492, acc 0.9375
2016-07-27T15:35:09.444236: step 1706, loss 0.0732802, acc 0.984375
2016-07-27T15:35:12.224817: step 1707, loss 0.135562, acc 0.921875
2016-07-27T15:35:14.725401: step 1708, loss 0.0869186, acc 0.96875
2016-07-27T15:35:17.296340: step 1709, loss 0.0875575, acc 0.953125
2016-07-27T15:35:20.177622: step 1710, loss 0.0524407, acc 0.984375
2016-07-27T15:35:22.853919: step 1711, loss 0.115711, acc 0.953125
2016-07-27T15:35:25.346871: step 1712, loss 0.0918896, acc 0.953125
2016-07-27T15:35:28.151901: step 1713, loss 0.153571, acc 0.9375
2016-07-27T15:35:30.695974: step 1714, loss 0.105595, acc 0.984375
2016-07-27T15:35:33.278245: step 1715, loss 0.097224, acc 0.96875
2016-07-27T15:35:36.179459: step 1716, loss 0.0713029, acc 1
2016-07-27T15:35:38.840742: step 1717, loss 0.0797535, acc 0.96875
2016-07-27T15:35:41.253095: step 1718, loss 0.0521233, acc 0.96875
2016-07-27T15:35:44.090424: step 1719, loss 0.0365853, acc 1
2016-07-27T15:35:46.699886: step 1720, loss 0.087765, acc 0.984375
2016-07-27T15:35:49.240539: step 1721, loss 0.133936, acc 0.9375
2016-07-27T15:35:52.050592: step 1722, loss 0.0403909, acc 1
2016-07-27T15:35:54.682122: step 1723, loss 0.116325, acc 0.96875
2016-07-27T15:35:57.442606: step 1724, loss 0.127409, acc 0.921875
2016-07-27T15:36:00.214549: step 1725, loss 0.0680133, acc 1
2016-07-27T15:36:02.689548: step 1726, loss 0.046835, acc 0.984375
2016-07-27T15:36:05.536614: step 1727, loss 0.117025, acc 0.953125
2016-07-27T15:36:08.177723: step 1728, loss 0.11762, acc 0.96875
2016-07-27T15:36:10.710209: step 1729, loss 0.113267, acc 0.96875
2016-07-27T15:36:13.535497: step 1730, loss 0.0843525, acc 0.953125
2016-07-27T15:36:16.025740: step 1731, loss 0.189978, acc 0.921875
2016-07-27T15:36:18.771767: step 1732, loss 0.116373, acc 0.9375
2016-07-27T15:36:21.555555: step 1733, loss 0.109269, acc 0.921875
2016-07-27T15:36:23.969096: step 1734, loss 0.0717811, acc 0.984375
2016-07-27T15:36:26.790534: step 1735, loss 0.0962777, acc 0.953125
2016-07-27T15:36:29.433960: step 1736, loss 0.109797, acc 0.921875
2016-07-27T15:36:32.235534: step 1737, loss 0.0941389, acc 0.96875
2016-07-27T15:36:34.955112: step 1738, loss 0.159762, acc 0.890625
2016-07-27T15:36:37.392030: step 1739, loss 0.0810477, acc 0.984375
2016-07-27T15:36:40.199216: step 1740, loss 0.234423, acc 0.90625
2016-07-27T15:36:42.777215: step 1741, loss 0.0908282, acc 0.953125
2016-07-27T15:36:45.602761: step 1742, loss 0.148407, acc 0.9375
2016-07-27T15:36:48.381518: step 1743, loss 0.0846637, acc 0.953125
2016-07-27T15:36:50.832292: step 1744, loss 0.132098, acc 0.96875
2016-07-27T15:36:53.695803: step 1745, loss 0.140227, acc 0.921875
2016-07-27T15:36:56.339549: step 1746, loss 0.140259, acc 0.9375
2016-07-27T15:36:58.917669: step 1747, loss 0.101298, acc 0.953125
2016-07-27T15:37:01.728418: step 1748, loss 0.146143, acc 0.921875
2016-07-27T15:37:04.265566: step 1749, loss 0.0930999, acc 0.96875
2016-07-27T15:37:06.925222: step 1750, loss 0.10059, acc 0.953125
2016-07-27T15:37:09.721533: step 1751, loss 0.20178, acc 0.90625
2016-07-27T15:37:12.393185: step 1752, loss 0.257975, acc 0.921875
2016-07-27T15:37:14.914485: step 1753, loss 0.316073, acc 0.90625
2016-07-27T15:37:17.715126: step 1754, loss 0.0849363, acc 0.96875
2016-07-27T15:37:20.213514: step 1755, loss 0.13267, acc 0.953125
2016-07-27T15:37:22.826594: step 1756, loss 0.145066, acc 0.953125
2016-07-27T15:37:25.638058: step 1757, loss 0.0963773, acc 0.953125
2016-07-27T15:37:28.347184: step 1758, loss 0.0761843, acc 0.96875
2016-07-27T15:37:30.940498: step 1759, loss 0.0887053, acc 0.96875
2016-07-27T15:37:33.766747: step 1760, loss 0.080675, acc 1
2016-07-27T15:37:36.256292: step 1761, loss 0.0733348, acc 0.96875
2016-07-27T15:37:38.909127: step 1762, loss 0.123095, acc 0.96875
2016-07-27T15:37:41.769347: step 1763, loss 0.0889318, acc 0.953125
2016-07-27T15:37:44.437044: step 1764, loss 0.0946995, acc 0.96875
2016-07-27T15:37:47.022322: step 1765, loss 0.0619805, acc 0.984375
2016-07-27T15:37:49.801419: step 1766, loss 0.123151, acc 0.9375
2016-07-27T15:37:52.311930: step 1767, loss 0.0971664, acc 0.984375
2016-07-27T15:37:54.906979: step 1768, loss 0.088006, acc 0.984375
2016-07-27T15:37:57.766782: step 1769, loss 0.0751982, acc 0.984375
2016-07-27T15:38:00.438519: step 1770, loss 0.104848, acc 0.984375
2016-07-27T15:38:02.958803: step 1771, loss 0.147813, acc 0.953125
2016-07-27T15:38:05.754106: step 1772, loss 0.124727, acc 0.9375
2016-07-27T15:38:08.309593: step 1773, loss 0.130226, acc 0.90625
2016-07-27T15:38:10.942593: step 1774, loss 0.0683069, acc 0.96875
2016-07-27T15:38:13.790920: step 1775, loss 0.173863, acc 0.984375
2016-07-27T15:38:16.453944: step 1776, loss 0.0941714, acc 0.953125
2016-07-27T15:38:18.975192: step 1777, loss 0.0576219, acc 0.96875
2016-07-27T15:38:21.745323: step 1778, loss 0.0544458, acc 0.984375
2016-07-27T15:38:24.278697: step 1779, loss 0.0820145, acc 0.984375
2016-07-27T15:38:27.005942: step 1780, loss 0.180018, acc 0.953125
2016-07-27T15:38:29.814549: step 1781, loss 0.0897678, acc 0.96875
2016-07-27T15:38:32.251028: step 1782, loss 0.0915555, acc 0.96875
2016-07-27T15:38:35.057750: step 1783, loss 0.05597, acc 0.984375
2016-07-27T15:38:37.709953: step 1784, loss 0.118659, acc 0.96875
2016-07-27T15:38:40.317116: step 1785, loss 0.0733655, acc 0.953125
2016-07-27T15:38:43.157867: step 1786, loss 0.0392345, acc 0.984375
2016-07-27T15:38:45.715601: step 1787, loss 0.0914614, acc 0.9375
2016-07-27T15:38:48.616652: step 1788, loss 0.0620694, acc 0.984375
2016-07-27T15:38:51.293265: step 1789, loss 0.145513, acc 0.9375
2016-07-27T15:38:53.744774: step 1790, loss 0.0686653, acc 0.984375
2016-07-27T15:38:56.584704: step 1791, loss 0.0621959, acc 0.984375
2016-07-27T15:38:59.160936: step 1792, loss 0.0502078, acc 0.984375
2016-07-27T15:39:01.712315: step 1793, loss 0.0781125, acc 0.984375
2016-07-27T15:39:04.667179: step 1794, loss 0.10844, acc 0.96875
2016-07-27T15:39:07.366907: step 1795, loss 0.0791396, acc 0.96875
2016-07-27T15:39:09.885747: step 1796, loss 0.0583851, acc 0.984375
2016-07-27T15:39:12.703280: step 1797, loss 0.0475595, acc 1
2016-07-27T15:39:15.235099: step 1798, loss 0.111309, acc 0.96875
2016-07-27T15:39:17.812704: step 1799, loss 0.122658, acc 0.96875
2016-07-27T15:39:20.555820: step 1800, loss 0.0312689, acc 1

Evaluation:
2016-07-27T15:39:32.845702: step 1800, loss 1.10879, acc 0.714

Saved model checkpoint to /home/chaitanya/lstm-context-embeddings/runs/1469600026/checkpoints/model-1800

2016-07-27T15:39:37.049798: step 1801, loss 0.0845974, acc 0.984375
2016-07-27T15:39:39.815069: step 1802, loss 0.0449127, acc 0.984375
2016-07-27T15:39:42.442170: step 1803, loss 0.154831, acc 0.921875
2016-07-27T15:39:44.883249: step 1804, loss 0.0410993, acc 0.984375
2016-07-27T15:39:47.405288: step 1805, loss 0.0960799, acc 0.9375
2016-07-27T15:39:51.013412: step 1806, loss 0.108293, acc 0.953125
2016-07-27T15:39:54.022269: step 1807, loss 0.147697, acc 0.921875
2016-07-27T15:39:56.823590: step 1808, loss 0.0500562, acc 0.984375
2016-07-27T15:39:59.249285: step 1809, loss 0.087247, acc 0.96875
2016-07-27T15:40:02.002925: step 1810, loss 0.11615, acc 0.953125
2016-07-27T15:40:04.655859: step 1811, loss 0.0679947, acc 0.984375
2016-07-27T15:40:07.122672: step 1812, loss 0.102432, acc 0.951613
2016-07-27T15:40:09.991075: step 1813, loss 0.0373703, acc 1
2016-07-27T15:40:12.657265: step 1814, loss 0.0818621, acc 0.984375
2016-07-27T15:40:15.121247: step 1815, loss 0.0910435, acc 0.96875
2016-07-27T15:40:17.980684: step 1816, loss 0.0506424, acc 0.984375
2016-07-27T15:40:20.675501: step 1817, loss 0.0988194, acc 0.953125
2016-07-27T15:40:23.346504: step 1818, loss 0.0872671, acc 0.96875
2016-07-27T15:40:26.138163: step 1819, loss 0.0728422, acc 0.96875
2016-07-27T15:40:28.571821: step 1820, loss 0.0875662, acc 0.96875
2016-07-27T15:40:31.300747: step 1821, loss 0.127145, acc 0.9375
2016-07-27T15:40:33.931158: step 1822, loss 0.0594511, acc 0.984375
2016-07-27T15:40:36.480166: step 1823, loss 0.0901726, acc 0.953125
2016-07-27T15:40:39.262550: step 1824, loss 0.0309911, acc 0.984375
2016-07-27T15:40:41.719973: step 1825, loss 0.0761888, acc 0.96875
2016-07-27T15:40:44.275579: step 1826, loss 0.0868053, acc 0.953125
2016-07-27T15:40:47.095034: step 1827, loss 0.046739, acc 0.984375
2016-07-27T15:40:49.819446: step 1828, loss 0.0659946, acc 0.96875
2016-07-27T15:40:52.366721: step 1829, loss 0.150632, acc 0.953125
2016-07-27T15:40:55.211329: step 1830, loss 0.0776593, acc 0.953125
2016-07-27T15:40:57.701421: step 1831, loss 0.051507, acc 1
2016-07-27T15:41:00.436117: step 1832, loss 0.0954376, acc 0.984375
2016-07-27T15:41:03.221772: step 1833, loss 0.0287358, acc 0.984375
2016-07-27T15:41:05.643840: step 1834, loss 0.0501565, acc 0.984375
2016-07-27T15:41:08.470848: step 1835, loss 0.0999503, acc 0.984375
2016-07-27T15:41:11.126636: step 1836, loss 0.0493266, acc 1
2016-07-27T15:41:13.926758: step 1837, loss 0.14245, acc 0.921875
2016-07-27T15:41:16.664182: step 1838, loss 0.0777115, acc 0.953125
2016-07-27T15:41:19.207761: step 1839, loss 0.0903989, acc 0.984375
2016-07-27T15:41:22.057989: step 1840, loss 0.040939, acc 1
2016-07-27T15:41:24.589268: step 1841, loss 0.0617949, acc 0.984375
2016-07-27T15:41:27.274895: step 1842, loss 0.0430861, acc 0.96875
2016-07-27T15:41:30.126328: step 1843, loss 0.0381984, acc 1
2016-07-27T15:41:32.597983: step 1844, loss 0.0523475, acc 0.984375
2016-07-27T15:41:35.428811: step 1845, loss 0.0721067, acc 0.984375
2016-07-27T15:41:38.043104: step 1846, loss 0.109966, acc 0.984375
2016-07-27T15:41:40.625493: step 1847, loss 0.148737, acc 0.921875
2016-07-27T15:41:43.467054: step 1848, loss 0.085633, acc 0.984375
2016-07-27T15:41:45.990077: step 1849, loss 0.213025, acc 0.96875
2016-07-27T15:41:48.882352: step 1850, loss 0.0421323, acc 0.984375
2016-07-27T15:41:51.626582: step 1851, loss 0.0936497, acc 0.96875
2016-07-27T15:41:54.131738: step 1852, loss 0.0798562, acc 0.9375
2016-07-27T15:41:56.964913: step 1853, loss 0.0589354, acc 0.953125
2016-07-27T15:41:59.577122: step 1854, loss 0.0209436, acc 1
2016-07-27T15:42:02.340534: step 1855, loss 0.0744352, acc 0.984375
2016-07-27T15:42:05.162366: step 1856, loss 0.0933016, acc 0.953125
2016-07-27T15:42:07.569065: step 1857, loss 0.061118, acc 0.96875
2016-07-27T15:42:10.376290: step 1858, loss 0.015445, acc 1
2016-07-27T15:42:13.000347: step 1859, loss 0.0617733, acc 1
2016-07-27T15:42:15.729810: step 1860, loss 0.0672368, acc 0.984375
2016-07-27T15:42:18.495408: step 1861, loss 0.0718425, acc 1
2016-07-27T15:42:20.972318: step 1862, loss 0.105848, acc 0.921875
2016-07-27T15:42:23.790259: step 1863, loss 0.158863, acc 0.96875
2016-07-27T15:42:26.409355: step 1864, loss 0.167647, acc 0.921875
2016-07-27T15:42:29.015294: step 1865, loss 0.103424, acc 0.953125
2016-07-27T15:42:31.841455: step 1866, loss 0.152252, acc 0.9375
2016-07-27T15:42:34.276483: step 1867, loss 0.094844, acc 0.984375
2016-07-27T15:42:37.182159: step 1868, loss 0.0669981, acc 0.984375
2016-07-27T15:42:39.937453: step 1869, loss 0.0240836, acc 1
2016-07-27T15:42:42.428263: step 1870, loss 0.0450197, acc 1
2016-07-27T15:42:45.248201: step 1871, loss 0.0569817, acc 1
2016-07-27T15:42:47.778953: step 1872, loss 0.155681, acc 0.90625
2016-07-27T15:42:50.557367: step 1873, loss 0.0541642, acc 0.96875
2016-07-27T15:42:53.321360: step 1874, loss 0.088926, acc 0.953125
2016-07-27T15:42:55.730599: step 1875, loss 0.0657103, acc 0.953125
2016-07-27T15:42:58.542894: step 1876, loss 0.132355, acc 0.953125
2016-07-27T15:43:01.172292: step 1877, loss 0.097492, acc 0.96875
2016-07-27T15:43:03.704907: step 1878, loss 0.0539489, acc 1
2016-07-27T15:43:06.486355: step 1879, loss 0.0604862, acc 1
2016-07-27T15:43:09.013803: step 1880, loss 0.05391, acc 0.984375
2016-07-27T15:43:11.642746: step 1881, loss 0.0734862, acc 0.984375
2016-07-27T15:43:14.510126: step 1882, loss 0.124365, acc 0.9375
2016-07-27T15:43:17.155544: step 1883, loss 0.0694065, acc 0.96875
2016-07-27T15:43:19.728675: step 1884, loss 0.0537975, acc 0.96875
2016-07-27T15:43:22.527008: step 1885, loss 0.0567106, acc 0.984375
2016-07-27T15:43:25.034402: step 1886, loss 0.0799682, acc 0.96875
2016-07-27T15:43:27.663978: step 1887, loss 0.0876753, acc 0.96875
2016-07-27T15:43:30.488075: step 1888, loss 0.116706, acc 0.953125
2016-07-27T15:43:33.164726: step 1889, loss 0.0905364, acc 0.984375
2016-07-27T15:43:35.846927: step 1890, loss 0.0562948, acc 0.96875
2016-07-27T15:43:38.645319: step 1891, loss 0.0837475, acc 0.953125
2016-07-27T15:43:41.110275: step 1892, loss 0.0730204, acc 0.984375
2016-07-27T15:43:43.701155: step 1893, loss 0.160566, acc 0.9375
2016-07-27T15:43:46.531957: step 1894, loss 0.0613146, acc 0.984375
2016-07-27T15:43:49.216747: step 1895, loss 0.106841, acc 0.96875
2016-07-27T15:43:51.943043: step 1896, loss 0.09537, acc 0.953125
2016-07-27T15:43:54.734220: step 1897, loss 0.136213, acc 0.9375
2016-07-27T15:43:57.165027: step 1898, loss 0.0532658, acc 1
2016-07-27T15:43:59.960693: step 1899, loss 0.100664, acc 0.953125
2016-07-27T15:44:02.610254: step 1900, loss 0.0653052, acc 0.984375

Evaluation:
2016-07-27T15:44:14.635773: step 1900, loss 1.43659, acc 0.699

Saved model checkpoint to /home/chaitanya/lstm-context-embeddings/runs/1469600026/checkpoints/model-1900

2016-07-27T15:44:18.491247: step 1901, loss 0.149276, acc 0.9375
2016-07-27T15:44:21.398844: step 1902, loss 0.0378621, acc 1
2016-07-27T15:44:24.074576: step 1903, loss 0.0932273, acc 0.953125
2016-07-27T15:44:26.594283: step 1904, loss 0.0621639, acc 1
2016-07-27T15:44:29.180952: step 1905, loss 0.0515372, acc 0.984375
2016-07-27T15:44:32.759858: step 1906, loss 0.0729984, acc 0.96875
2016-07-27T15:44:35.698915: step 1907, loss 0.051655, acc 0.984375
2016-07-27T15:44:38.511638: step 1908, loss 0.0322783, acc 0.984375
2016-07-27T15:44:40.918724: step 1909, loss 0.0636471, acc 0.984375
2016-07-27T15:44:43.679396: step 1910, loss 0.100969, acc 0.953125
2016-07-27T15:44:46.327077: step 1911, loss 0.0791732, acc 0.96875
2016-07-27T15:44:48.796693: step 1912, loss 0.0923276, acc 0.953125
2016-07-27T15:44:51.658923: step 1913, loss 0.0987236, acc 0.9375
2016-07-27T15:44:54.329045: step 1914, loss 0.145919, acc 0.921875
2016-07-27T15:44:56.788590: step 1915, loss 0.138946, acc 0.953125
2016-07-27T15:44:59.611063: step 1916, loss 0.102092, acc 0.96875
2016-07-27T15:45:02.298681: step 1917, loss 0.0569888, acc 0.96875
2016-07-27T15:45:04.833980: step 1918, loss 0.121574, acc 0.9375
2016-07-27T15:45:07.649714: step 1919, loss 0.0620046, acc 0.96875
2016-07-27T15:45:10.262990: step 1920, loss 0.0771904, acc 0.96875
2016-07-27T15:45:12.782321: step 1921, loss 0.162358, acc 0.90625
2016-07-27T15:45:15.601389: step 1922, loss 0.0786644, acc 0.96875
2016-07-27T15:45:18.300281: step 1923, loss 0.0769093, acc 0.953125
2016-07-27T15:45:20.829464: step 1924, loss 0.0599945, acc 0.984375
2016-07-27T15:45:23.635335: step 1925, loss 0.0964154, acc 0.96875
2016-07-27T15:45:26.312081: step 1926, loss 0.0926758, acc 0.984375
2016-07-27T15:45:28.773459: step 1927, loss 0.174652, acc 0.9375
2016-07-27T15:45:31.610189: step 1928, loss 0.0624962, acc 1
2016-07-27T15:45:34.134773: step 1929, loss 0.0782144, acc 0.9375
2016-07-27T15:45:36.807243: step 1930, loss 0.143377, acc 0.921875
2016-07-27T15:45:39.578498: step 1931, loss 0.150499, acc 0.953125
2016-07-27T15:45:42.032152: step 1932, loss 0.108958, acc 0.921875
2016-07-27T15:45:44.948718: step 1933, loss 0.105919, acc 0.96875
2016-07-27T15:45:47.620778: step 1934, loss 0.0430116, acc 0.96875
2016-07-27T15:45:50.149652: step 1935, loss 0.0803669, acc 0.96875
2016-07-27T15:45:52.967568: step 1936, loss 0.0780675, acc 0.953125
2016-07-27T15:45:55.523257: step 1937, loss 0.0879782, acc 0.953125
2016-07-27T15:45:58.407894: step 1938, loss 0.145397, acc 0.890625
2016-07-27T15:46:01.163850: step 1939, loss 0.0790751, acc 0.96875
2016-07-27T15:46:03.622268: step 1940, loss 0.0709002, acc 1
2016-07-27T15:46:06.375010: step 1941, loss 0.0689579, acc 0.96875
2016-07-27T15:46:08.965121: step 1942, loss 0.0725603, acc 0.984375
2016-07-27T15:46:11.751026: step 1943, loss 0.0418696, acc 0.96875
2016-07-27T15:46:14.587443: step 1944, loss 0.0382844, acc 1
2016-07-27T15:46:17.007114: step 1945, loss 0.110177, acc 0.953125
2016-07-27T15:46:19.820161: step 1946, loss 0.0817367, acc 0.984375
2016-07-27T15:46:22.473549: step 1947, loss 0.052069, acc 0.984375
2016-07-27T15:46:25.015666: step 1948, loss 0.0766921, acc 0.96875
2016-07-27T15:46:27.834928: step 1949, loss 0.128748, acc 0.9375
2016-07-27T15:46:30.293230: step 1950, loss 0.11926, acc 0.9375
2016-07-27T15:46:33.027446: step 1951, loss 0.0557094, acc 0.984375
2016-07-27T15:46:35.845406: step 1952, loss 0.222958, acc 0.921875
2016-07-27T15:46:38.235533: step 1953, loss 0.0805434, acc 0.984375
2016-07-27T15:46:41.046857: step 1954, loss 0.0549971, acc 0.984375
2016-07-27T15:46:43.735379: step 1955, loss 0.0951802, acc 0.953125
2016-07-27T15:46:46.242817: step 1956, loss 0.0456443, acc 0.984375
2016-07-27T15:46:48.744730: step 1957, loss 0.0519352, acc 0.984375
2016-07-27T15:46:51.547199: step 1958, loss 0.115744, acc 0.9375
2016-07-27T15:46:54.183537: step 1959, loss 0.0866015, acc 0.953125
2016-07-27T15:46:57.000951: step 1960, loss 0.150182, acc 0.9375
2016-07-27T15:46:59.603281: step 1961, loss 0.119875, acc 0.9375
2016-07-27T15:47:02.134025: step 1962, loss 0.060239, acc 0.984375
2016-07-27T15:47:04.774827: step 1963, loss 0.0803315, acc 0.967742
2016-07-27T15:47:08.345601: step 1964, loss 0.0536775, acc 0.984375
2016-07-27T15:47:11.288692: step 1965, loss 0.0861235, acc 0.953125
2016-07-27T15:47:14.110884: step 1966, loss 0.128861, acc 0.921875
2016-07-27T15:47:16.516992: step 1967, loss 0.0377226, acc 1
2016-07-27T15:47:19.286587: step 1968, loss 0.0965168, acc 0.9375
2016-07-27T15:47:21.911486: step 1969, loss 0.0893278, acc 0.953125
2016-07-27T15:47:24.466444: step 1970, loss 0.0744775, acc 0.96875
2016-07-27T15:47:27.276056: step 1971, loss 0.0484999, acc 1
2016-07-27T15:47:29.758931: step 1972, loss 0.0557023, acc 1
2016-07-27T15:47:32.529973: step 1973, loss 0.0820267, acc 0.96875
2016-07-27T15:47:35.273431: step 1974, loss 0.121096, acc 0.96875
2016-07-27T15:47:37.672612: step 1975, loss 0.0665712, acc 0.96875
2016-07-27T15:47:40.442258: step 1976, loss 0.0611129, acc 0.984375
2016-07-27T15:47:43.057875: step 1977, loss 0.0657848, acc 0.984375
2016-07-27T15:47:45.655114: step 1978, loss 0.0668627, acc 0.984375
2016-07-27T15:47:48.468853: step 1979, loss 0.043204, acc 0.984375
2016-07-27T15:47:50.943656: step 1980, loss 0.108975, acc 0.953125
2016-07-27T15:47:53.872600: step 1981, loss 0.0508395, acc 0.984375
2016-07-27T15:47:56.528023: step 1982, loss 0.0698916, acc 0.96875
2016-07-27T15:47:59.016870: step 1983, loss 0.0744425, acc 0.96875
2016-07-27T15:48:01.804281: step 1984, loss 0.0346746, acc 1
2016-07-27T15:48:04.419941: step 1985, loss 0.0946589, acc 0.953125
2016-07-27T15:48:06.982009: step 1986, loss 0.0877055, acc 0.9375
2016-07-27T15:48:09.769307: step 1987, loss 0.0705035, acc 0.984375
2016-07-27T15:48:12.527793: step 1988, loss 0.185866, acc 0.984375
2016-07-27T15:48:15.007927: step 1989, loss 0.0763584, acc 0.953125
2016-07-27T15:48:17.788546: step 1990, loss 0.0682707, acc 0.953125
2016-07-27T15:48:20.344292: step 1991, loss 0.121626, acc 0.90625
2016-07-27T15:48:22.924025: step 1992, loss 0.058064, acc 0.984375
2016-07-27T15:48:25.692576: step 1993, loss 0.0733883, acc 0.96875
2016-07-27T15:48:28.437326: step 1994, loss 0.0540407, acc 0.96875
2016-07-27T15:48:30.901594: step 1995, loss 0.065585, acc 0.96875
2016-07-27T15:48:33.737053: step 1996, loss 0.0926789, acc 0.953125
2016-07-27T15:48:36.324146: step 1997, loss 0.0330674, acc 1
2016-07-27T15:48:38.852049: step 1998, loss 0.134717, acc 0.9375
2016-07-27T15:48:41.632363: step 1999, loss 0.0630804, acc 0.984375
2016-07-27T15:48:44.281150: step 2000, loss 0.114205, acc 0.953125

Evaluation:
2016-07-27T15:48:56.254046: step 2000, loss 1.46167, acc 0.688

Saved model checkpoint to /home/chaitanya/lstm-context-embeddings/runs/1469600026/checkpoints/model-2000

2016-07-27T15:49:00.197925: step 2001, loss 0.0351127, acc 1
2016-07-27T15:49:03.122667: step 2002, loss 0.098926, acc 0.953125
2016-07-27T15:49:05.822734: step 2003, loss 0.0955397, acc 0.9375
2016-07-27T15:49:08.284796: step 2004, loss 0.0718982, acc 0.984375
2016-07-27T15:49:10.785837: step 2005, loss 0.0518319, acc 0.984375
2016-07-27T15:49:14.379040: step 2006, loss 0.0347903, acc 1
2016-07-27T15:49:17.389604: step 2007, loss 0.0493529, acc 1
2016-07-27T15:49:20.174364: step 2008, loss 0.0523359, acc 0.984375
2016-07-27T15:49:22.592485: step 2009, loss 0.0387955, acc 0.984375
2016-07-27T15:49:25.412131: step 2010, loss 0.0446844, acc 1
2016-07-27T15:49:28.090112: step 2011, loss 0.0763531, acc 0.953125
2016-07-27T15:49:30.557430: step 2012, loss 0.0874613, acc 0.953125
2016-07-27T15:49:33.454929: step 2013, loss 0.129357, acc 0.9375
2016-07-27T15:49:36.073954: step 2014, loss 0.0886541, acc 0.953125
2016-07-27T15:49:38.633489: step 2015, loss 0.0954118, acc 0.9375
2016-07-27T15:49:41.452621: step 2016, loss 0.0745506, acc 1
2016-07-27T15:49:43.953633: step 2017, loss 0.0341077, acc 1
2016-07-27T15:49:46.684601: step 2018, loss 0.140496, acc 0.9375
2016-07-27T15:49:49.404413: step 2019, loss 0.0680427, acc 0.984375
2016-07-27T15:49:51.805684: step 2020, loss 0.046685, acc 1
2016-07-27T15:49:54.630936: step 2021, loss 0.0398965, acc 0.984375
2016-07-27T15:49:57.267246: step 2022, loss 0.0373896, acc 1
2016-07-27T15:49:59.819701: step 2023, loss 0.0770983, acc 0.96875
2016-07-27T15:50:02.662795: step 2024, loss 0.0602867, acc 0.96875
2016-07-27T15:50:05.148807: step 2025, loss 0.0775651, acc 0.984375
2016-07-27T15:50:07.943991: step 2026, loss 0.0682683, acc 0.984375
2016-07-27T15:50:10.668998: step 2027, loss 0.0553331, acc 0.984375
2016-07-27T15:50:13.123424: step 2028, loss 0.0434973, acc 1
2016-07-27T15:50:15.958105: step 2029, loss 0.222508, acc 0.953125
2016-07-27T15:50:18.512483: step 2030, loss 0.096898, acc 0.953125
2016-07-27T15:50:21.293510: step 2031, loss 0.0543811, acc 0.96875
2016-07-27T15:50:24.117900: step 2032, loss 0.0819957, acc 0.96875
2016-07-27T15:50:26.556036: step 2033, loss 0.130202, acc 0.953125
2016-07-27T15:50:29.372641: step 2034, loss 0.117909, acc 0.9375
2016-07-27T15:50:32.074641: step 2035, loss 0.0355455, acc 1
2016-07-27T15:50:34.629579: step 2036, loss 0.0889638, acc 0.984375
2016-07-27T15:50:37.419851: step 2037, loss 0.145865, acc 0.96875
2016-07-27T15:50:39.911816: step 2038, loss 0.088167, acc 0.96875
2016-07-27T15:50:42.712579: step 2039, loss 0.0581108, acc 0.953125
2016-07-27T15:50:45.544851: step 2040, loss 0.0827622, acc 0.984375
2016-07-27T15:50:47.914171: step 2041, loss 0.121202, acc 0.953125
2016-07-27T15:50:50.782381: step 2042, loss 0.0614764, acc 0.96875
2016-07-27T15:50:53.449756: step 2043, loss 0.0810467, acc 0.953125
2016-07-27T15:50:56.320548: step 2044, loss 0.139604, acc 0.921875
2016-07-27T15:50:58.978481: step 2045, loss 0.0555808, acc 0.984375
2016-07-27T15:51:01.484132: step 2046, loss 0.0582111, acc 0.96875
2016-07-27T15:51:04.141330: step 2047, loss 0.102503, acc 0.953125
2016-07-27T15:51:07.716244: step 2048, loss 0.0789485, acc 0.953125
2016-07-27T15:51:10.679381: step 2049, loss 0.101905, acc 0.953125
2016-07-27T15:51:13.558556: step 2050, loss 0.117516, acc 0.953125
2016-07-27T15:51:15.956948: step 2051, loss 0.0933291, acc 0.953125
2016-07-27T15:51:18.674411: step 2052, loss 0.0971128, acc 0.96875
2016-07-27T15:51:21.537369: step 2053, loss 0.211985, acc 0.953125
2016-07-27T15:51:23.973561: step 2054, loss 0.098435, acc 0.984375
2016-07-27T15:51:26.836060: step 2055, loss 0.135079, acc 0.953125
2016-07-27T15:51:29.511993: step 2056, loss 0.148853, acc 0.90625
2016-07-27T15:51:32.130385: step 2057, loss 0.0801887, acc 0.953125
2016-07-27T15:51:34.999965: step 2058, loss 0.0495326, acc 0.984375
2016-07-27T15:51:37.540423: step 2059, loss 0.105502, acc 0.953125
2016-07-27T15:51:40.446863: step 2060, loss 0.0872514, acc 0.96875
2016-07-27T15:51:43.162537: step 2061, loss 0.149972, acc 0.921875
2016-07-27T15:51:45.667982: step 2062, loss 0.0628927, acc 1
2016-07-27T15:51:48.521003: step 2063, loss 0.0860118, acc 0.96875
2016-07-27T15:51:51.251427: step 2064, loss 0.0458287, acc 0.984375
2016-07-27T15:51:53.762781: step 2065, loss 0.11112, acc 0.96875
2016-07-27T15:51:56.671192: step 2066, loss 0.0887427, acc 0.96875
2016-07-27T15:51:59.360781: step 2067, loss 0.144423, acc 0.921875
2016-07-27T15:52:01.889179: step 2068, loss 0.266178, acc 0.84375
2016-07-27T15:52:04.760210: step 2069, loss 0.0767519, acc 0.984375
2016-07-27T15:52:07.364656: step 2070, loss 0.225221, acc 0.921875
2016-07-27T15:52:09.992214: step 2071, loss 0.0858175, acc 0.96875
2016-07-27T15:52:12.887048: step 2072, loss 0.122649, acc 0.9375
2016-07-27T15:52:15.581174: step 2073, loss 0.119169, acc 0.921875
2016-07-27T15:52:18.412091: step 2074, loss 0.0751888, acc 0.96875
2016-07-27T15:52:21.141410: step 2075, loss 0.0693492, acc 0.96875
2016-07-27T15:52:23.614564: step 2076, loss 0.0374671, acc 0.984375
2016-07-27T15:52:26.481490: step 2077, loss 0.123109, acc 0.953125
2016-07-27T15:52:29.094127: step 2078, loss 0.0457463, acc 1
2016-07-27T15:52:31.787740: step 2079, loss 0.168416, acc 0.9375
2016-07-27T15:52:34.627254: step 2080, loss 0.187038, acc 0.9375
2016-07-27T15:52:37.089865: step 2081, loss 0.0620129, acc 0.96875
2016-07-27T15:52:39.932641: step 2082, loss 0.134571, acc 0.953125
2016-07-27T15:52:42.569904: step 2083, loss 0.120786, acc 0.96875
2016-07-27T15:52:45.220284: step 2084, loss 0.134084, acc 0.953125
2016-07-27T15:52:48.063438: step 2085, loss 0.0692821, acc 0.96875
2016-07-27T15:52:50.601216: step 2086, loss 0.0643124, acc 0.984375
2016-07-27T15:52:53.543238: step 2087, loss 0.0589415, acc 0.984375
2016-07-27T15:52:56.291863: step 2088, loss 0.15159, acc 0.953125
2016-07-27T15:52:58.846781: step 2089, loss 0.0573959, acc 1
2016-07-27T15:53:01.670033: step 2090, loss 0.0369034, acc 0.984375
2016-07-27T15:53:04.222698: step 2091, loss 0.136152, acc 0.9375
2016-07-27T15:53:06.986567: step 2092, loss 0.0512059, acc 0.984375
2016-07-27T15:53:09.813495: step 2093, loss 0.0667688, acc 0.953125
2016-07-27T15:53:12.252523: step 2094, loss 0.0446815, acc 0.96875
2016-07-27T15:53:15.019408: step 2095, loss 0.0994518, acc 0.984375
2016-07-27T15:53:17.689849: step 2096, loss 0.158842, acc 0.921875
2016-07-27T15:53:20.271827: step 2097, loss 0.161227, acc 0.953125
2016-07-27T15:53:23.081350: step 2098, loss 0.0898485, acc 0.96875
2016-07-27T15:53:25.662393: step 2099, loss 0.0680047, acc 0.953125
2016-07-27T15:53:28.330004: step 2100, loss 0.158548, acc 0.921875

Evaluation:
2016-07-27T15:53:40.287029: step 2100, loss 1.20732, acc 0.691

Saved model checkpoint to /home/chaitanya/lstm-context-embeddings/runs/1469600026/checkpoints/model-2100

2016-07-27T15:53:44.277980: step 2101, loss 0.0614906, acc 0.96875
2016-07-27T15:53:47.852025: step 2102, loss 0.122386, acc 0.953125
2016-07-27T15:53:50.864703: step 2103, loss 0.0669172, acc 0.96875
2016-07-27T15:53:53.687684: step 2104, loss 0.138143, acc 0.921875
2016-07-27T15:53:56.149393: step 2105, loss 0.0767321, acc 0.96875
2016-07-27T15:53:58.974560: step 2106, loss 0.0901568, acc 0.96875
2016-07-27T15:54:01.609741: step 2107, loss 0.109097, acc 0.953125
2016-07-27T15:54:04.120502: step 2108, loss 0.040758, acc 0.984375
2016-07-27T15:54:06.975583: step 2109, loss 0.145691, acc 0.9375
2016-07-27T15:54:09.643869: step 2110, loss 0.058111, acc 0.984375
2016-07-27T15:54:12.187149: step 2111, loss 0.091467, acc 0.953125
2016-07-27T15:54:14.953784: step 2112, loss 0.138493, acc 0.9375
2016-07-27T15:54:17.504616: step 2113, loss 0.0666343, acc 0.984375
2016-07-27T15:54:20.097850: step 2114, loss 0.136765, acc 0.967742
2016-07-27T15:54:22.992418: step 2115, loss 0.050554, acc 0.984375
2016-07-27T15:54:25.693873: step 2116, loss 0.120509, acc 0.953125
2016-07-27T15:54:28.245367: step 2117, loss 0.116592, acc 0.9375
2016-07-27T15:54:31.058917: step 2118, loss 0.120668, acc 0.953125
2016-07-27T15:54:33.632784: step 2119, loss 0.0932002, acc 0.953125
2016-07-27T15:54:36.270751: step 2120, loss 0.0651475, acc 0.953125
2016-07-27T15:54:39.138543: step 2121, loss 0.0718516, acc 0.984375
2016-07-27T15:54:41.783185: step 2122, loss 0.0925997, acc 0.953125
2016-07-27T15:54:44.307373: step 2123, loss 0.0873865, acc 0.96875
2016-07-27T15:54:47.181984: step 2124, loss 0.103595, acc 0.9375
2016-07-27T15:54:50.036312: step 2125, loss 0.0815282, acc 0.96875
2016-07-27T15:54:52.561582: step 2126, loss 0.100618, acc 0.953125
2016-07-27T15:54:55.391117: step 2127, loss 0.101624, acc 0.953125
2016-07-27T15:54:58.076779: step 2128, loss 0.110389, acc 0.953125
2016-07-27T15:55:00.919269: step 2129, loss 0.131176, acc 0.90625
2016-07-27T15:55:03.626498: step 2130, loss 0.0446356, acc 1
2016-07-27T15:55:06.181214: step 2131, loss 0.0754263, acc 1
2016-07-27T15:55:09.019641: step 2132, loss 0.0737742, acc 0.984375
2016-07-27T15:55:11.593481: step 2133, loss 0.115406, acc 0.9375
2016-07-27T15:55:14.231660: step 2134, loss 0.052942, acc 1
2016-07-27T15:55:17.164586: step 2135, loss 0.0549506, acc 0.984375
2016-07-27T15:55:19.841146: step 2136, loss 0.0920853, acc 0.96875
2016-07-27T15:55:22.407520: step 2137, loss 0.145067, acc 0.9375
2016-07-27T15:55:25.212625: step 2138, loss 0.070915, acc 0.96875
2016-07-27T15:55:27.796613: step 2139, loss 0.126819, acc 0.9375
2016-07-27T15:55:30.481609: step 2140, loss 0.0976119, acc 0.953125
2016-07-27T15:55:33.342633: step 2141, loss 0.0430757, acc 1
2016-07-27T15:55:35.961594: step 2142, loss 0.0808025, acc 0.96875
2016-07-27T15:55:38.605633: step 2143, loss 0.148908, acc 0.9375
2016-07-27T15:55:41.460654: step 2144, loss 0.0642737, acc 0.984375
2016-07-27T15:55:43.991019: step 2145, loss 0.0482183, acc 0.984375
2016-07-27T15:55:46.936129: step 2146, loss 0.0617248, acc 1
2016-07-27T15:55:49.668850: step 2147, loss 0.120029, acc 0.953125
2016-07-27T15:55:52.160589: step 2148, loss 0.0467174, acc 0.984375
2016-07-27T15:55:54.972678: step 2149, loss 0.0506302, acc 0.984375
2016-07-27T15:55:57.580431: step 2150, loss 0.103887, acc 0.953125
2016-07-27T15:56:00.386398: step 2151, loss 0.102114, acc 0.96875
2016-07-27T15:56:03.247209: step 2152, loss 0.0524914, acc 0.96875
2016-07-27T15:56:05.691983: step 2153, loss 0.0926272, acc 0.96875
2016-07-27T15:56:08.639030: step 2154, loss 0.0555339, acc 0.953125
2016-07-27T15:56:11.271244: step 2155, loss 0.1438, acc 0.953125
2016-07-27T15:56:14.124277: step 2156, loss 0.0535454, acc 0.984375
2016-07-27T15:56:16.830627: step 2157, loss 0.0794358, acc 0.984375
2016-07-27T15:56:19.390479: step 2158, loss 0.114687, acc 0.96875
2016-07-27T15:56:22.262683: step 2159, loss 0.0712077, acc 0.953125
2016-07-27T15:56:24.862985: step 2160, loss 0.0926994, acc 0.953125
2016-07-27T15:56:27.555980: step 2161, loss 0.0590751, acc 0.953125
2016-07-27T15:56:30.405229: step 2162, loss 0.100789, acc 0.953125
2016-07-27T15:56:33.083626: step 2163, loss 0.102494, acc 0.953125
2016-07-27T15:56:35.668476: step 2164, loss 0.0885911, acc 0.953125
2016-07-27T15:56:38.393853: step 2165, loss 0.0679896, acc 0.984375
2016-07-27T15:56:40.978035: step 2166, loss 0.121755, acc 0.984375
2016-07-27T15:56:43.826525: step 2167, loss 0.0876823, acc 0.9375
2016-07-27T15:56:46.467127: step 2168, loss 0.097438, acc 0.953125
2016-07-27T15:56:49.307064: step 2169, loss 0.0669858, acc 0.984375
2016-07-27T15:56:51.995025: step 2170, loss 0.0696402, acc 0.96875
2016-07-27T15:56:54.599248: step 2171, loss 0.0454942, acc 0.984375
2016-07-27T15:56:57.452369: step 2172, loss 0.0405669, acc 1
2016-07-27T15:56:59.979667: step 2173, loss 0.0754007, acc 0.96875
2016-07-27T15:57:02.661454: step 2174, loss 0.0839332, acc 0.96875
2016-07-27T15:57:05.508881: step 2175, loss 0.0279533, acc 1
2016-07-27T15:57:08.185761: step 2176, loss 0.123874, acc 0.9375
2016-07-27T15:57:10.972938: step 2177, loss 0.0928072, acc 0.9375
2016-07-27T15:57:13.706264: step 2178, loss 0.120747, acc 0.9375
2016-07-27T15:57:16.371553: step 2179, loss 0.0739158, acc 0.953125
2016-07-27T15:57:19.272895: step 2180, loss 0.107509, acc 0.9375
2016-07-27T15:57:21.731043: step 2181, loss 0.0810944, acc 0.984375
2016-07-27T15:57:24.596951: step 2182, loss 0.0444204, acc 0.96875
2016-07-27T15:57:27.285198: step 2183, loss 0.0689795, acc 0.984375
2016-07-27T15:57:29.845033: step 2184, loss 0.0785393, acc 0.96875
2016-07-27T15:57:32.705283: step 2185, loss 0.0862081, acc 0.984375
2016-07-27T15:57:35.274347: step 2186, loss 0.0637339, acc 0.984375
2016-07-27T15:57:37.822449: step 2187, loss 0.0601103, acc 1
2016-07-27T15:57:40.670543: step 2188, loss 0.0710669, acc 0.984375
2016-07-27T15:57:43.345495: step 2189, loss 0.0647416, acc 0.96875
2016-07-27T15:57:46.124731: step 2190, loss 0.120052, acc 0.96875
2016-07-27T15:57:48.939332: step 2191, loss 0.0918851, acc 0.96875
2016-07-27T15:57:51.465216: step 2192, loss 0.0562861, acc 0.96875
2016-07-27T15:57:54.311320: step 2193, loss 0.10987, acc 0.90625
2016-07-27T15:57:56.920351: step 2194, loss 0.141876, acc 0.890625
2016-07-27T15:57:59.582062: step 2195, loss 0.10522, acc 0.984375
2016-07-27T15:58:02.068885: step 2196, loss 0.0537773, acc 1
2016-07-27T15:58:04.894453: step 2197, loss 0.135702, acc 0.90625
2016-07-27T15:58:07.633896: step 2198, loss 0.0661688, acc 1
2016-07-27T15:58:10.492391: step 2199, loss 0.0618144, acc 1
2016-07-27T15:58:13.175015: step 2200, loss 0.0640468, acc 0.984375

Evaluation:
2016-07-27T15:58:25.272846: step 2200, loss 1.34898, acc 0.699

Saved model checkpoint to /home/chaitanya/lstm-context-embeddings/runs/1469600026/checkpoints/model-2200

2016-07-27T15:58:29.580748: step 2201, loss 0.0752883, acc 0.96875
2016-07-27T15:58:32.414884: step 2202, loss 0.0733148, acc 0.953125
2016-07-27T15:58:35.124939: step 2203, loss 0.0726144, acc 1
2016-07-27T15:58:37.660675: step 2204, loss 0.074336, acc 0.96875
2016-07-27T15:58:40.501699: step 2205, loss 0.126165, acc 0.9375
2016-07-27T15:58:43.066668: step 2206, loss 0.111338, acc 0.953125
2016-07-27T15:58:45.741073: step 2207, loss 0.0436583, acc 0.984375
2016-07-27T15:58:48.625576: step 2208, loss 0.185533, acc 0.921875
2016-07-27T15:58:51.289222: step 2209, loss 0.0754958, acc 0.984375
2016-07-27T15:58:53.847708: step 2210, loss 0.0320784, acc 1
2016-07-27T15:58:56.669371: step 2211, loss 0.0573254, acc 0.984375
2016-07-27T15:58:59.268650: step 2212, loss 0.0403052, acc 1
2016-07-27T15:59:01.938924: step 2213, loss 0.0638845, acc 1
2016-07-27T15:59:04.816007: step 2214, loss 0.034218, acc 1
2016-07-27T15:59:07.467665: step 2215, loss 0.0668047, acc 0.96875
2016-07-27T15:59:10.281075: step 2216, loss 0.111035, acc 0.9375
2016-07-27T15:59:13.027166: step 2217, loss 0.0790252, acc 0.96875
2016-07-27T15:59:15.568540: step 2218, loss 0.0199916, acc 1
2016-07-27T15:59:18.347505: step 2219, loss 0.0823073, acc 0.984375
2016-07-27T15:59:20.927266: step 2220, loss 0.0633269, acc 0.96875
2016-07-27T15:59:23.768593: step 2221, loss 0.106652, acc 0.984375
2016-07-27T15:59:26.597718: step 2222, loss 0.0359512, acc 0.984375
2016-07-27T15:59:29.037248: step 2223, loss 0.056739, acc 0.96875
2016-07-27T15:59:31.878643: step 2224, loss 0.0569145, acc 1
2016-07-27T15:59:34.540859: step 2225, loss 0.0398035, acc 1
2016-07-27T15:59:37.184399: step 2226, loss 0.0650907, acc 0.96875
2016-07-27T15:59:40.161299: step 2227, loss 0.0487369, acc 0.984375
2016-07-27T15:59:42.744133: step 2228, loss 0.116868, acc 0.953125
2016-07-27T15:59:45.664280: step 2229, loss 0.14891, acc 0.921875
2016-07-27T15:59:48.308136: step 2230, loss 0.072674, acc 0.984375
2016-07-27T15:59:50.868025: step 2231, loss 0.0773857, acc 0.953125
2016-07-27T15:59:53.695830: step 2232, loss 0.0899148, acc 0.953125
2016-07-27T15:59:56.269948: step 2233, loss 0.128397, acc 0.921875
2016-07-27T15:59:58.996395: step 2234, loss 0.0648516, acc 0.984375
2016-07-27T16:00:01.841698: step 2235, loss 0.0934261, acc 0.96875
2016-07-27T16:00:04.504538: step 2236, loss 0.0377484, acc 0.984375
2016-07-27T16:00:07.120189: step 2237, loss 0.0868954, acc 0.953125
2016-07-27T16:00:10.019526: step 2238, loss 0.037, acc 0.96875
2016-07-27T16:00:12.616098: step 2239, loss 0.121718, acc 0.953125
2016-07-27T16:00:15.242135: step 2240, loss 0.0427874, acc 0.984375
2016-07-27T16:00:18.045691: step 2241, loss 0.0724614, acc 0.984375
2016-07-27T16:00:20.725660: step 2242, loss 0.0484307, acc 0.984375
2016-07-27T16:00:23.581552: step 2243, loss 0.0561461, acc 0.953125
2016-07-27T16:00:26.329069: step 2244, loss 0.0633214, acc 0.96875
2016-07-27T16:00:28.885019: step 2245, loss 0.0190609, acc 1
2016-07-27T16:00:31.720853: step 2246, loss 0.0577916, acc 0.96875
2016-07-27T16:00:34.303101: step 2247, loss 0.115639, acc 0.984375
2016-07-27T16:00:36.984497: step 2248, loss 0.0690894, acc 0.953125
2016-07-27T16:00:39.889923: step 2249, loss 0.0315826, acc 1
2016-07-27T16:00:42.565297: step 2250, loss 0.0368587, acc 0.984375
2016-07-27T16:00:45.101786: step 2251, loss 0.0502968, acc 0.984375
2016-07-27T16:00:47.957007: step 2252, loss 0.0732818, acc 0.953125
2016-07-27T16:00:50.805305: step 2253, loss 0.0947968, acc 0.984375
2016-07-27T16:00:53.323865: step 2254, loss 0.0527456, acc 0.984375
2016-07-27T16:00:56.146281: step 2255, loss 0.0546578, acc 0.96875
2016-07-27T16:00:58.777007: step 2256, loss 0.0487179, acc 0.984375
2016-07-27T16:01:01.647641: step 2257, loss 0.0221322, acc 0.984375
2016-07-27T16:01:04.387523: step 2258, loss 0.0536785, acc 0.96875
2016-07-27T16:01:06.935302: step 2259, loss 0.0715462, acc 0.953125
2016-07-27T16:01:09.814829: step 2260, loss 0.14224, acc 0.96875
2016-07-27T16:01:12.401538: step 2261, loss 0.059257, acc 0.96875
2016-07-27T16:01:15.256580: step 2262, loss 0.173051, acc 0.953125
2016-07-27T16:01:17.919439: step 2263, loss 0.0625986, acc 0.96875
2016-07-27T16:01:20.593643: step 2264, loss 0.0622356, acc 0.96875
2016-07-27T16:01:23.437506: step 2265, loss 0.0685164, acc 0.983871
2016-07-27T16:01:25.961763: step 2266, loss 0.0393664, acc 0.984375
2016-07-27T16:01:28.884545: step 2267, loss 0.0758042, acc 0.96875
2016-07-27T16:01:31.600257: step 2268, loss 0.124623, acc 0.96875
2016-07-27T16:01:34.186713: step 2269, loss 0.0516204, acc 0.984375
2016-07-27T16:01:37.050553: step 2270, loss 0.0710653, acc 1
2016-07-27T16:01:39.671345: step 2271, loss 0.0678385, acc 0.96875
2016-07-27T16:01:42.287232: step 2272, loss 0.0886598, acc 0.96875
2016-07-27T16:01:45.138572: step 2273, loss 0.0807698, acc 0.953125
2016-07-27T16:01:47.812824: step 2274, loss 0.0767936, acc 0.984375
2016-07-27T16:01:50.665249: step 2275, loss 0.0486246, acc 1
2016-07-27T16:01:53.357055: step 2276, loss 0.0996681, acc 0.96875
2016-07-27T16:01:56.078801: step 2277, loss 0.0348965, acc 0.984375
2016-07-27T16:01:58.890171: step 2278, loss 0.0743238, acc 0.9375
2016-07-27T16:02:01.332106: step 2279, loss 0.155279, acc 0.953125
2016-07-27T16:02:04.205004: step 2280, loss 0.0757164, acc 0.96875
2016-07-27T16:02:06.902424: step 2281, loss 0.079405, acc 0.96875
2016-07-27T16:02:09.771021: step 2282, loss 0.0483207, acc 0.984375
2016-07-27T16:02:12.481168: step 2283, loss 0.0468774, acc 0.96875
2016-07-27T16:02:14.983182: step 2284, loss 0.0649806, acc 0.953125
2016-07-27T16:02:17.797901: step 2285, loss 0.0714491, acc 0.953125
2016-07-27T16:02:20.380367: step 2286, loss 0.0397946, acc 0.96875
2016-07-27T16:02:23.070472: step 2287, loss 0.050001, acc 0.984375
2016-07-27T16:02:25.910685: step 2288, loss 0.0771811, acc 0.96875
2016-07-27T16:02:28.431638: step 2289, loss 0.0450827, acc 0.984375
2016-07-27T16:02:31.330222: step 2290, loss 0.120392, acc 0.921875
2016-07-27T16:02:33.973166: step 2291, loss 0.0866926, acc 0.96875
2016-07-27T16:02:36.792566: step 2292, loss 0.0415428, acc 1
2016-07-27T16:02:39.519456: step 2293, loss 0.0488789, acc 0.984375
2016-07-27T16:02:42.054124: step 2294, loss 0.0733042, acc 0.953125
2016-07-27T16:02:44.897356: step 2295, loss 0.0713217, acc 0.96875
2016-07-27T16:02:47.512712: step 2296, loss 0.017335, acc 1
2016-07-27T16:02:50.213232: step 2297, loss 0.0389627, acc 1
2016-07-27T16:02:53.043652: step 2298, loss 0.137487, acc 0.953125
2016-07-27T16:02:55.739482: step 2299, loss 0.0280728, acc 0.984375
2016-07-27T16:02:58.303388: step 2300, loss 0.0818423, acc 0.953125

Evaluation:
2016-07-27T16:03:10.452873: step 2300, loss 1.51199, acc 0.693

Saved model checkpoint to /home/chaitanya/lstm-context-embeddings/runs/1469600026/checkpoints/model-2300

2016-07-27T16:03:14.378640: step 2301, loss 0.0570741, acc 0.984375
2016-07-27T16:03:17.215879: step 2302, loss 0.134734, acc 0.875
2016-07-27T16:03:19.920031: step 2303, loss 0.0482596, acc 1
2016-07-27T16:03:22.556041: step 2304, loss 0.0788642, acc 0.953125
2016-07-27T16:03:25.394001: step 2305, loss 0.053653, acc 0.96875
2016-07-27T16:03:27.968998: step 2306, loss 0.0425765, acc 0.984375
2016-07-27T16:03:30.651013: step 2307, loss 0.0418161, acc 1
2016-07-27T16:03:33.468548: step 2308, loss 0.0537044, acc 0.984375
2016-07-27T16:03:36.103582: step 2309, loss 0.120069, acc 0.96875
2016-07-27T16:03:38.844541: step 2310, loss 0.0602586, acc 0.96875
2016-07-27T16:03:41.614204: step 2311, loss 0.0877, acc 0.984375
2016-07-27T16:03:44.096066: step 2312, loss 0.0540631, acc 1
2016-07-27T16:03:46.948589: step 2313, loss 0.0798436, acc 0.953125
2016-07-27T16:03:49.512412: step 2314, loss 0.0423889, acc 1
2016-07-27T16:03:52.402966: step 2315, loss 0.0916346, acc 0.953125
2016-07-27T16:03:55.228807: step 2316, loss 0.053583, acc 0.984375
2016-07-27T16:03:57.616663: step 2317, loss 0.0805756, acc 0.984375
2016-07-27T16:04:00.452048: step 2318, loss 0.0538672, acc 0.96875
2016-07-27T16:04:03.055310: step 2319, loss 0.0709024, acc 0.9375
2016-07-27T16:04:05.862014: step 2320, loss 0.130799, acc 0.9375
2016-07-27T16:04:08.578076: step 2321, loss 0.0282336, acc 1
2016-07-27T16:04:11.057344: step 2322, loss 0.017768, acc 1
2016-07-27T16:04:13.835815: step 2323, loss 0.0249383, acc 0.984375
2016-07-27T16:04:16.432664: step 2324, loss 0.0652653, acc 0.984375
2016-07-27T16:04:19.258400: step 2325, loss 0.0819428, acc 0.96875
2016-07-27T16:04:21.989064: step 2326, loss 0.0199919, acc 1
2016-07-27T16:04:24.484518: step 2327, loss 0.101645, acc 0.953125
2016-07-27T16:04:26.958117: step 2328, loss 0.107877, acc 0.953125
2016-07-27T16:04:29.801030: step 2329, loss 0.0431101, acc 0.984375
2016-07-27T16:04:32.466096: step 2330, loss 0.136766, acc 0.9375
2016-07-27T16:04:35.297118: step 2331, loss 0.0499575, acc 0.984375
2016-07-27T16:04:37.996928: step 2332, loss 0.0707811, acc 0.96875
2016-07-27T16:04:40.540613: step 2333, loss 0.0138419, acc 1
2016-07-27T16:04:43.041683: step 2334, loss 0.0536375, acc 0.96875
2016-07-27T16:04:45.864681: step 2335, loss 0.0497138, acc 0.984375
2016-07-27T16:04:48.480060: step 2336, loss 0.0895334, acc 0.96875
2016-07-27T16:04:51.310791: step 2337, loss 0.0859017, acc 0.96875
2016-07-27T16:04:53.990986: step 2338, loss 0.0537844, acc 0.96875
2016-07-27T16:04:56.680187: step 2339, loss 0.0409841, acc 1
2016-07-27T16:04:59.471237: step 2340, loss 0.0359773, acc 1
2016-07-27T16:05:02.132737: step 2341, loss 0.0467879, acc 0.984375
2016-07-27T16:05:04.982753: step 2342, loss 0.0592565, acc 0.984375
2016-07-27T16:05:07.537575: step 2343, loss 0.0537671, acc 0.96875
2016-07-27T16:05:10.423602: step 2344, loss 0.048832, acc 0.984375
2016-07-27T16:05:13.129546: step 2345, loss 0.209088, acc 0.96875
2016-07-27T16:05:15.703229: step 2346, loss 0.0856659, acc 0.9375
2016-07-27T16:05:18.560864: step 2347, loss 0.0775904, acc 0.96875
2016-07-27T16:05:21.131398: step 2348, loss 0.0374402, acc 1
2016-07-27T16:05:23.761254: step 2349, loss 0.115691, acc 0.96875
2016-07-27T16:05:26.584709: step 2350, loss 0.0812305, acc 0.984375
2016-07-27T16:05:29.208738: step 2351, loss 0.0313734, acc 1
2016-07-27T16:05:31.812001: step 2352, loss 0.0908693, acc 0.96875
2016-07-27T16:05:34.629438: step 2353, loss 0.0897418, acc 0.9375
2016-07-27T16:05:37.282105: step 2354, loss 0.0604847, acc 1
2016-07-27T16:05:39.828527: step 2355, loss 0.117149, acc 0.9375
2016-07-27T16:05:42.514798: step 2356, loss 0.0562536, acc 0.96875
2016-07-27T16:05:45.344512: step 2357, loss 0.0557788, acc 0.953125
2016-07-27T16:05:47.859035: step 2358, loss 0.0181489, acc 1
2016-07-27T16:05:50.536312: step 2359, loss 0.113637, acc 0.921875
2016-07-27T16:05:53.200932: step 2360, loss 0.0616326, acc 1
2016-07-27T16:05:56.030890: step 2361, loss 0.0473932, acc 0.984375
2016-07-27T16:05:58.605425: step 2362, loss 0.0647252, acc 0.953125
2016-07-27T16:06:01.475898: step 2363, loss 0.111503, acc 0.9375
2016-07-27T16:06:04.206129: step 2364, loss 0.10798, acc 0.96875
2016-07-27T16:06:06.823459: step 2365, loss 0.094554, acc 0.96875
2016-07-27T16:06:09.641590: step 2366, loss 0.0459268, acc 0.96875
2016-07-27T16:06:12.240080: step 2367, loss 0.0954196, acc 0.9375
2016-07-27T16:06:14.898133: step 2368, loss 0.0595772, acc 0.984375
2016-07-27T16:06:17.793728: step 2369, loss 0.0363904, acc 0.96875
2016-07-27T16:06:20.441068: step 2370, loss 0.0555848, acc 0.984375
2016-07-27T16:06:23.298157: step 2371, loss 0.0517434, acc 0.984375
2016-07-27T16:06:26.016097: step 2372, loss 0.083212, acc 0.96875
2016-07-27T16:06:28.522014: step 2373, loss 0.052589, acc 0.96875
2016-07-27T16:06:31.362392: step 2374, loss 0.0781236, acc 0.96875
2016-07-27T16:06:33.965647: step 2375, loss 0.0964073, acc 0.953125
2016-07-27T16:06:36.587695: step 2376, loss 0.0922107, acc 0.96875
2016-07-27T16:06:39.456138: step 2377, loss 0.0826324, acc 0.953125
2016-07-27T16:06:42.214688: step 2378, loss 0.0345981, acc 1
2016-07-27T16:06:44.803498: step 2379, loss 0.0677388, acc 0.96875
2016-07-27T16:06:47.655763: step 2380, loss 0.0544996, acc 0.984375
2016-07-27T16:06:50.194306: step 2381, loss 0.0722119, acc 0.984375
2016-07-27T16:06:52.864430: step 2382, loss 0.0723478, acc 0.96875
2016-07-27T16:06:55.712799: step 2383, loss 0.0515383, acc 0.984375
2016-07-27T16:06:58.348976: step 2384, loss 0.0965379, acc 0.953125
2016-07-27T16:07:01.103436: step 2385, loss 0.0745999, acc 0.984375
2016-07-27T16:07:03.886617: step 2386, loss 0.194549, acc 0.90625
2016-07-27T16:07:06.518078: step 2387, loss 0.00913114, acc 1
2016-07-27T16:07:09.350460: step 2388, loss 0.046998, acc 0.984375
2016-07-27T16:07:11.878329: step 2389, loss 0.0884028, acc 0.9375
2016-07-27T16:07:14.825977: step 2390, loss 0.0528799, acc 0.96875
2016-07-27T16:07:17.479941: step 2391, loss 0.103206, acc 0.9375
2016-07-27T16:07:19.989725: step 2392, loss 0.075814, acc 0.953125
2016-07-27T16:07:22.664707: step 2393, loss 0.0320885, acc 1
2016-07-27T16:07:25.527983: step 2394, loss 0.0474384, acc 0.984375
2016-07-27T16:07:28.191231: step 2395, loss 0.0961286, acc 0.984375
2016-07-27T16:07:30.871484: step 2396, loss 0.0192341, acc 1
2016-07-27T16:07:33.726941: step 2397, loss 0.0656668, acc 0.953125
2016-07-27T16:07:36.250233: step 2398, loss 0.100703, acc 0.96875
2016-07-27T16:07:38.818723: step 2399, loss 0.0765499, acc 0.96875
2016-07-27T16:07:41.642828: step 2400, loss 0.138682, acc 0.9375

Evaluation:
2016-07-27T16:07:53.491711: step 2400, loss 1.55986, acc 0.696

Saved model checkpoint to /home/chaitanya/lstm-context-embeddings/runs/1469600026/checkpoints/model-2400

2016-07-27T16:07:57.352941: step 2401, loss 0.0580119, acc 0.96875
2016-07-27T16:08:00.197282: step 2402, loss 0.0586163, acc 0.984375
2016-07-27T16:08:02.854103: step 2403, loss 0.0866892, acc 0.96875
2016-07-27T16:08:05.680319: step 2404, loss 0.0123733, acc 1
2016-07-27T16:08:08.394484: step 2405, loss 0.0466001, acc 1
2016-07-27T16:08:11.111048: step 2406, loss 0.0862903, acc 0.96875
2016-07-27T16:08:13.946455: step 2407, loss 0.0557712, acc 0.96875
2016-07-27T16:08:16.424939: step 2408, loss 0.137472, acc 0.921875
2016-07-27T16:08:19.269201: step 2409, loss 0.0466279, acc 0.984375
2016-07-27T16:08:21.896639: step 2410, loss 0.0285703, acc 1
2016-07-27T16:08:24.692712: step 2411, loss 0.0569681, acc 0.96875
2016-07-27T16:08:27.384439: step 2412, loss 0.0189083, acc 0.984375
2016-07-27T16:08:29.873785: step 2413, loss 0.0966816, acc 0.9375
2016-07-27T16:08:32.680475: step 2414, loss 0.113955, acc 0.953125
2016-07-27T16:08:35.302300: step 2415, loss 0.0512282, acc 0.984375
2016-07-27T16:08:37.982093: step 2416, loss 0.0933447, acc 0.951613
2016-07-27T16:08:40.834454: step 2417, loss 0.0723739, acc 0.96875
2016-07-27T16:08:43.553117: step 2418, loss 0.0851621, acc 0.953125
2016-07-27T16:08:46.102521: step 2419, loss 0.109135, acc 0.953125
2016-07-27T16:08:48.893494: step 2420, loss 0.0538184, acc 0.984375
2016-07-27T16:08:51.440363: step 2421, loss 0.114044, acc 0.9375
2016-07-27T16:08:54.076579: step 2422, loss 0.0496459, acc 0.984375
2016-07-27T16:08:56.956006: step 2423, loss 0.101466, acc 0.9375
2016-07-27T16:08:59.651795: step 2424, loss 0.0539866, acc 0.984375
2016-07-27T16:09:02.237297: step 2425, loss 0.0605222, acc 0.984375
2016-07-27T16:09:05.127230: step 2426, loss 0.058695, acc 0.953125
2016-07-27T16:09:07.748624: step 2427, loss 0.0777493, acc 0.984375
2016-07-27T16:09:10.355302: step 2428, loss 0.0400213, acc 0.984375
2016-07-27T16:09:13.077965: step 2429, loss 0.0446149, acc 1
2016-07-27T16:09:15.580650: step 2430, loss 0.0702963, acc 0.984375
2016-07-27T16:09:18.420406: step 2431, loss 0.028734, acc 1
2016-07-27T16:09:21.074808: step 2432, loss 0.0362249, acc 0.984375
2016-07-27T16:09:23.902641: step 2433, loss 0.0848733, acc 0.984375
2016-07-27T16:09:26.609164: step 2434, loss 0.139632, acc 0.9375
2016-07-27T16:09:29.139097: step 2435, loss 0.0545994, acc 0.984375
2016-07-27T16:09:31.812810: step 2436, loss 0.0657914, acc 0.96875
2016-07-27T16:09:34.683926: step 2437, loss 0.0613213, acc 0.953125
2016-07-27T16:09:37.257546: step 2438, loss 0.0610512, acc 0.953125
2016-07-27T16:09:40.215556: step 2439, loss 0.0602331, acc 0.984375
2016-07-27T16:09:42.868989: step 2440, loss 0.0397286, acc 0.984375
2016-07-27T16:09:45.463446: step 2441, loss 0.123013, acc 0.953125
2016-07-27T16:09:48.344134: step 2442, loss 0.0935228, acc 0.953125
2016-07-27T16:09:51.063885: step 2443, loss 0.0580431, acc 0.96875
2016-07-27T16:09:53.692258: step 2444, loss 0.039965, acc 1
2016-07-27T16:09:56.521201: step 2445, loss 0.0561298, acc 0.96875
2016-07-27T16:09:59.021572: step 2446, loss 0.113696, acc 0.953125
2016-07-27T16:10:01.629496: step 2447, loss 0.0124007, acc 1
2016-07-27T16:10:04.450666: step 2448, loss 0.132419, acc 0.9375
2016-07-27T16:10:07.098094: step 2449, loss 0.0427468, acc 1
2016-07-27T16:10:09.912471: step 2450, loss 0.0329695, acc 0.984375
2016-07-27T16:10:12.611051: step 2451, loss 0.0609684, acc 0.984375
2016-07-27T16:10:15.123777: step 2452, loss 0.0608664, acc 0.984375
2016-07-27T16:10:17.927278: step 2453, loss 0.0486988, acc 1
2016-07-27T16:10:20.456913: step 2454, loss 0.0589104, acc 0.96875
2016-07-27T16:10:23.092820: step 2455, loss 0.04379, acc 0.984375
2016-07-27T16:10:25.687598: step 2456, loss 0.0920987, acc 0.9375
2016-07-27T16:10:28.464420: step 2457, loss 0.0674305, acc 0.953125
2016-07-27T16:10:31.094559: step 2458, loss 0.0315302, acc 1
2016-07-27T16:10:33.849601: step 2459, loss 0.0520947, acc 0.984375
2016-07-27T16:10:36.594370: step 2460, loss 0.065306, acc 0.96875
2016-07-27T16:10:39.051976: step 2461, loss 0.0433382, acc 0.984375
2016-07-27T16:10:41.813181: step 2462, loss 0.016463, acc 1
2016-07-27T16:10:44.502072: step 2463, loss 0.0564639, acc 0.96875
2016-07-27T16:10:47.326128: step 2464, loss 0.0886996, acc 0.953125
2016-07-27T16:10:49.967546: step 2465, loss 0.0918435, acc 0.96875
2016-07-27T16:10:52.478681: step 2466, loss 0.0447187, acc 1
2016-07-27T16:10:55.324155: step 2467, loss 0.102641, acc 0.9375
2016-07-27T16:10:58.140674: step 2468, loss 0.0463565, acc 0.96875
2016-07-27T16:11:00.638070: step 2469, loss 0.115486, acc 0.953125
2016-07-27T16:11:03.502904: step 2470, loss 0.0297477, acc 1
2016-07-27T16:11:06.152186: step 2471, loss 0.0771289, acc 0.96875
2016-07-27T16:11:08.954045: step 2472, loss 0.14811, acc 0.9375
2016-07-27T16:11:11.678076: step 2473, loss 0.055189, acc 0.96875
2016-07-27T16:11:14.174261: step 2474, loss 0.0161243, acc 1
2016-07-27T16:11:17.019596: step 2475, loss 0.0533297, acc 0.96875
2016-07-27T16:11:19.568717: step 2476, loss 0.0720056, acc 0.984375
2016-07-27T16:11:22.127833: step 2477, loss 0.04064, acc 0.984375
2016-07-27T16:11:25.080173: step 2478, loss 0.0915613, acc 0.96875
2016-07-27T16:11:27.780745: step 2479, loss 0.0608773, acc 0.96875
2016-07-27T16:11:30.253944: step 2480, loss 0.0390316, acc 0.984375
2016-07-27T16:11:33.046342: step 2481, loss 0.0483167, acc 0.984375
2016-07-27T16:11:35.620405: step 2482, loss 0.0969198, acc 0.953125
2016-07-27T16:11:38.233192: step 2483, loss 0.140565, acc 0.9375
2016-07-27T16:11:41.155577: step 2484, loss 0.0312367, acc 0.984375
2016-07-27T16:11:43.847845: step 2485, loss 0.0590925, acc 0.96875
2016-07-27T16:11:46.411282: step 2486, loss 0.0818764, acc 0.96875
2016-07-27T16:11:48.995649: step 2487, loss 0.122917, acc 0.90625
2016-07-27T16:11:52.565995: step 2488, loss 0.0264318, acc 1
2016-07-27T16:11:55.567655: step 2489, loss 0.0351022, acc 0.984375
2016-07-27T16:11:58.368495: step 2490, loss 0.0673609, acc 0.984375
2016-07-27T16:12:00.777435: step 2491, loss 0.0352687, acc 0.984375
2016-07-27T16:12:03.570322: step 2492, loss 0.0308712, acc 1
2016-07-27T16:12:06.203998: step 2493, loss 0.104034, acc 0.984375
2016-07-27T16:12:08.815811: step 2494, loss 0.0730751, acc 0.96875
2016-07-27T16:12:11.610463: step 2495, loss 0.0638464, acc 0.96875
2016-07-27T16:12:14.084688: step 2496, loss 0.0378448, acc 0.984375
2016-07-27T16:12:17.025486: step 2497, loss 0.0998129, acc 0.953125
2016-07-27T16:12:19.757287: step 2498, loss 0.0234839, acc 1
2016-07-27T16:12:22.215785: step 2499, loss 0.123796, acc 0.96875
2016-07-27T16:12:24.984101: step 2500, loss 0.0558731, acc 0.96875

Evaluation:
2016-07-27T16:12:36.874705: step 2500, loss 1.68215, acc 0.707

Saved model checkpoint to /home/chaitanya/lstm-context-embeddings/runs/1469600026/checkpoints/model-2500

2016-07-27T16:12:40.764115: step 2501, loss 0.0478787, acc 1
2016-07-27T16:12:43.574964: step 2502, loss 0.112619, acc 0.953125
2016-07-27T16:12:46.199264: step 2503, loss 0.0708864, acc 0.953125
2016-07-27T16:12:48.765208: step 2504, loss 0.0407587, acc 0.984375
2016-07-27T16:12:51.563061: step 2505, loss 0.0783266, acc 0.96875
2016-07-27T16:12:54.079924: step 2506, loss 0.030183, acc 1
2016-07-27T16:12:56.753189: step 2507, loss 0.0666215, acc 0.96875
2016-07-27T16:12:59.368175: step 2508, loss 0.0697934, acc 0.953125
2016-07-27T16:13:01.967104: step 2509, loss 0.036617, acc 0.984375
2016-07-27T16:13:04.782847: step 2510, loss 0.0940515, acc 0.953125
2016-07-27T16:13:07.413128: step 2511, loss 0.055314, acc 0.96875
2016-07-27T16:13:10.211584: step 2512, loss 0.0787749, acc 0.9375
2016-07-27T16:13:12.944484: step 2513, loss 0.0369285, acc 0.984375
2016-07-27T16:13:15.570408: step 2514, loss 0.0969713, acc 0.953125
2016-07-27T16:13:18.388751: step 2515, loss 0.0669, acc 0.953125
2016-07-27T16:13:20.814887: step 2516, loss 0.0243221, acc 1
2016-07-27T16:13:23.610537: step 2517, loss 0.0987568, acc 0.96875
2016-07-27T16:13:26.281007: step 2518, loss 0.112631, acc 0.953125
2016-07-27T16:13:28.897943: step 2519, loss 0.0528803, acc 0.984375
2016-07-27T16:13:31.700050: step 2520, loss 0.0214849, acc 1
2016-07-27T16:13:34.219890: step 2521, loss 0.0636803, acc 0.984375
2016-07-27T16:13:36.909390: step 2522, loss 0.0639051, acc 0.984375
2016-07-27T16:13:39.747946: step 2523, loss 0.0857652, acc 0.953125
2016-07-27T16:13:42.374922: step 2524, loss 0.0162845, acc 1
2016-07-27T16:13:44.936652: step 2525, loss 0.0392961, acc 0.96875
2016-07-27T16:13:47.782506: step 2526, loss 0.0740352, acc 0.96875
2016-07-27T16:13:50.288073: step 2527, loss 0.128875, acc 0.953125
2016-07-27T16:13:52.864683: step 2528, loss 0.069582, acc 0.984375
2016-07-27T16:13:55.729999: step 2529, loss 0.0346415, acc 0.984375
2016-07-27T16:13:58.429979: step 2530, loss 0.0648045, acc 0.96875
2016-07-27T16:14:00.948535: step 2531, loss 0.0742425, acc 0.984375
2016-07-27T16:14:03.754319: step 2532, loss 0.074889, acc 0.953125
2016-07-27T16:14:06.295523: step 2533, loss 0.0424532, acc 1
2016-07-27T16:14:08.899421: step 2534, loss 0.0569657, acc 0.953125
2016-07-27T16:14:11.750943: step 2535, loss 0.139383, acc 0.921875
2016-07-27T16:14:14.403344: step 2536, loss 0.0824561, acc 0.953125
2016-07-27T16:14:16.981890: step 2537, loss 0.0897428, acc 0.953125
2016-07-27T16:14:19.788715: step 2538, loss 0.0218295, acc 1
2016-07-27T16:14:22.296143: step 2539, loss 0.0542763, acc 0.96875
2016-07-27T16:14:24.918369: step 2540, loss 0.0435403, acc 0.984375
2016-07-27T16:14:27.826102: step 2541, loss 0.0494919, acc 1
2016-07-27T16:14:30.503604: step 2542, loss 0.0525801, acc 1
2016-07-27T16:14:33.182467: step 2543, loss 0.0483141, acc 0.96875
2016-07-27T16:14:36.385220: step 2544, loss 0.043726, acc 0.984375
2016-07-27T16:14:38.895137: step 2545, loss 0.0228299, acc 1
2016-07-27T16:14:41.868313: step 2546, loss 0.0551443, acc 0.984375
2016-07-27T16:14:44.637212: step 2547, loss 0.0121062, acc 1
2016-07-27T16:14:47.113352: step 2548, loss 0.0734095, acc 0.96875
2016-07-27T16:14:49.652385: step 2549, loss 0.1283, acc 0.90625
2016-07-27T16:14:52.281233: step 2550, loss 0.0809997, acc 0.953125
2016-07-27T16:14:54.929934: step 2551, loss 0.0319652, acc 1
2016-07-27T16:14:57.838581: step 2552, loss 0.0563239, acc 0.96875
2016-07-27T16:15:00.357978: step 2553, loss 0.0562539, acc 0.96875
2016-07-27T16:15:03.295408: step 2554, loss 0.0262938, acc 1
2016-07-27T16:15:05.967757: step 2555, loss 0.044262, acc 0.984375
2016-07-27T16:15:08.767953: step 2556, loss 0.0446032, acc 0.984375
2016-07-27T16:15:11.552254: step 2557, loss 0.0543245, acc 0.96875
2016-07-27T16:15:14.061835: step 2558, loss 0.037497, acc 1
2016-07-27T16:15:16.958580: step 2559, loss 0.0723743, acc 0.953125
2016-07-27T16:15:19.563429: step 2560, loss 0.0273457, acc 1
2016-07-27T16:15:22.284340: step 2561, loss 0.0609557, acc 0.96875
2016-07-27T16:15:25.180691: step 2562, loss 0.254528, acc 0.9375
2016-07-27T16:15:27.640976: step 2563, loss 0.0346035, acc 0.984375
2016-07-27T16:15:30.504466: step 2564, loss 0.0964293, acc 0.953125
2016-07-27T16:15:33.176500: step 2565, loss 0.10522, acc 0.9375
2016-07-27T16:15:35.804739: step 2566, loss 0.145431, acc 0.96875
2016-07-27T16:15:38.732406: step 2567, loss 0.0422693, acc 0.983871
2016-07-27T16:15:41.255821: step 2568, loss 0.120625, acc 0.96875
2016-07-27T16:15:44.179752: step 2569, loss 0.0595865, acc 1
2016-07-27T16:15:46.919802: step 2570, loss 0.237324, acc 0.921875
2016-07-27T16:15:49.486973: step 2571, loss 0.0470412, acc 0.984375
2016-07-27T16:15:52.300423: step 2572, loss 0.119134, acc 0.9375
2016-07-27T16:15:54.878843: step 2573, loss 0.138521, acc 0.953125
2016-07-27T16:15:57.757704: step 2574, loss 0.0537675, acc 0.984375
2016-07-27T16:16:00.420663: step 2575, loss 0.0553431, acc 0.984375
2016-07-27T16:16:03.063841: step 2576, loss 0.0313879, acc 1
2016-07-27T16:16:05.940642: step 2577, loss 0.0795636, acc 0.96875
2016-07-27T16:16:08.541495: step 2578, loss 0.0924621, acc 0.953125
2016-07-27T16:16:11.493829: step 2579, loss 0.157, acc 0.953125
2016-07-27T16:16:14.140696: step 2580, loss 0.0669532, acc 0.96875
2016-07-27T16:16:16.734682: step 2581, loss 0.0506401, acc 0.984375
2016-07-27T16:16:19.620718: step 2582, loss 0.100424, acc 0.953125
2016-07-27T16:16:22.170374: step 2583, loss 0.0451553, acc 0.984375
2016-07-27T16:16:25.100851: step 2584, loss 0.117076, acc 0.921875
2016-07-27T16:16:27.868848: step 2585, loss 0.0581911, acc 0.96875
2016-07-27T16:16:30.370715: step 2586, loss 0.0560438, acc 0.984375
2016-07-27T16:16:33.251695: step 2587, loss 0.0402991, acc 0.984375
2016-07-27T16:16:35.826449: step 2588, loss 0.0191354, acc 1
2016-07-27T16:16:38.719850: step 2589, loss 0.0218617, acc 1
2016-07-27T16:16:41.524900: step 2590, loss 0.0496229, acc 0.984375
2016-07-27T16:16:43.974924: step 2591, loss 0.105676, acc 0.953125
2016-07-27T16:16:46.817587: step 2592, loss 0.0946897, acc 0.953125
2016-07-27T16:16:49.487294: step 2593, loss 0.0458657, acc 0.984375
2016-07-27T16:16:52.348904: step 2594, loss 0.0675532, acc 0.984375
2016-07-27T16:16:55.063209: step 2595, loss 0.0312924, acc 1
2016-07-27T16:16:57.676077: step 2596, loss 0.0704728, acc 0.96875
2016-07-27T16:17:00.486950: step 2597, loss 0.164585, acc 0.921875
2016-07-27T16:17:03.032996: step 2598, loss 0.0998959, acc 0.9375
2016-07-27T16:17:05.784057: step 2599, loss 0.0681117, acc 0.984375
2016-07-27T16:17:08.625921: step 2600, loss 0.0924032, acc 0.96875

Evaluation:
2016-07-27T16:17:20.820447: step 2600, loss 1.26259, acc 0.714

Saved model checkpoint to /home/chaitanya/lstm-context-embeddings/runs/1469600026/checkpoints/model-2600

2016-07-27T16:17:24.796467: step 2601, loss 0.0405984, acc 0.984375
2016-07-27T16:17:27.614359: step 2602, loss 0.0743841, acc 0.96875
2016-07-27T16:17:30.306891: step 2603, loss 0.138565, acc 0.953125
2016-07-27T16:17:32.892319: step 2604, loss 0.0918147, acc 0.953125
2016-07-27T16:17:35.729313: step 2605, loss 0.0378886, acc 1
2016-07-27T16:17:38.236716: step 2606, loss 0.0774559, acc 0.96875
2016-07-27T16:17:41.026506: step 2607, loss 0.0887947, acc 0.9375
2016-07-27T16:17:43.748819: step 2608, loss 0.0170942, acc 1
2016-07-27T16:17:46.253559: step 2609, loss 0.0490108, acc 0.984375
2016-07-27T16:17:48.827250: step 2610, loss 0.0462634, acc 1
2016-07-27T16:17:52.445493: step 2611, loss 0.126891, acc 0.921875
2016-07-27T16:17:55.516648: step 2612, loss 0.0782605, acc 0.953125
2016-07-27T16:17:58.369401: step 2613, loss 0.0334936, acc 0.984375
2016-07-27T16:18:00.810926: step 2614, loss 0.0558228, acc 1
2016-07-27T16:18:03.606314: step 2615, loss 0.060862, acc 0.96875
2016-07-27T16:18:06.256115: step 2616, loss 0.0870271, acc 0.921875
2016-07-27T16:18:08.802682: step 2617, loss 0.124518, acc 0.96875
2016-07-27T16:18:11.641868: step 2618, loss 0.0842638, acc 0.96875
2016-07-27T16:18:14.387135: step 2619, loss 0.0458756, acc 1
2016-07-27T16:18:16.965327: step 2620, loss 0.0697828, acc 0.96875
2016-07-27T16:18:19.802327: step 2621, loss 0.0312566, acc 1
2016-07-27T16:18:22.314924: step 2622, loss 0.104878, acc 0.9375
2016-07-27T16:18:25.076985: step 2623, loss 0.0650492, acc 0.96875
2016-07-27T16:18:27.887796: step 2624, loss 0.049267, acc 0.984375
2016-07-27T16:18:30.393456: step 2625, loss 0.0687896, acc 0.96875
2016-07-27T16:18:33.265667: step 2626, loss 0.0902336, acc 0.984375
2016-07-27T16:18:35.856024: step 2627, loss 0.0466728, acc 0.984375
2016-07-27T16:18:38.705691: step 2628, loss 0.0612637, acc 0.984375
2016-07-27T16:18:41.397550: step 2629, loss 0.0789908, acc 0.984375
2016-07-27T16:18:43.970428: step 2630, loss 0.0513663, acc 0.96875
2016-07-27T16:18:46.817572: step 2631, loss 0.0346266, acc 0.984375
2016-07-27T16:18:49.352915: step 2632, loss 0.0580305, acc 0.96875
2016-07-27T16:18:51.940376: step 2633, loss 0.0807316, acc 0.96875
2016-07-27T16:18:54.793254: step 2634, loss 0.0505399, acc 0.984375
2016-07-27T16:18:57.493253: step 2635, loss 0.13167, acc 0.984375
2016-07-27T16:19:00.133422: step 2636, loss 0.0542111, acc 0.96875
2016-07-27T16:19:02.987313: step 2637, loss 0.0789689, acc 0.984375
2016-07-27T16:19:05.581884: step 2638, loss 0.104864, acc 0.953125
2016-07-27T16:19:08.222998: step 2639, loss 0.0965924, acc 0.953125
2016-07-27T16:19:11.077215: step 2640, loss 0.0457934, acc 1
2016-07-27T16:19:13.737592: step 2641, loss 0.0752495, acc 0.96875
2016-07-27T16:19:16.570726: step 2642, loss 0.0775329, acc 0.953125
2016-07-27T16:19:19.357545: step 2643, loss 0.0297198, acc 1
2016-07-27T16:19:21.937422: step 2644, loss 0.126956, acc 0.921875
2016-07-27T16:19:24.808471: step 2645, loss 0.0566416, acc 0.96875
2016-07-27T16:19:27.392081: step 2646, loss 0.0411932, acc 0.984375
2016-07-27T16:19:30.093089: step 2647, loss 0.0725091, acc 0.96875
2016-07-27T16:19:33.021914: step 2648, loss 0.0553911, acc 0.984375
2016-07-27T16:19:35.682737: step 2649, loss 0.0601364, acc 0.984375
2016-07-27T16:19:38.314518: step 2650, loss 0.0651939, acc 0.96875
2016-07-27T16:19:41.187416: step 2651, loss 0.0811037, acc 0.953125
2016-07-27T16:19:43.725325: step 2652, loss 0.0856733, acc 0.9375
2016-07-27T16:19:46.664277: step 2653, loss 0.0344831, acc 0.984375
2016-07-27T16:19:49.382320: step 2654, loss 0.0993252, acc 0.9375
2016-07-27T16:19:51.928630: step 2655, loss 0.0336067, acc 1
2016-07-27T16:19:54.771906: step 2656, loss 0.0773604, acc 0.953125
2016-07-27T16:19:57.330107: step 2657, loss 0.0979768, acc 0.9375
2016-07-27T16:20:00.106628: step 2658, loss 0.0448606, acc 1
2016-07-27T16:20:02.930901: step 2659, loss 0.060762, acc 0.96875
2016-07-27T16:20:05.403041: step 2660, loss 0.0558061, acc 0.96875
2016-07-27T16:20:08.254743: step 2661, loss 0.095253, acc 0.984375
2016-07-27T16:20:10.936432: step 2662, loss 0.0434018, acc 0.984375
2016-07-27T16:20:13.782370: step 2663, loss 0.0543688, acc 0.96875
2016-07-27T16:20:16.531630: step 2664, loss 0.0575317, acc 0.984375
2016-07-27T16:20:19.100632: step 2665, loss 0.0617812, acc 0.984375
2016-07-27T16:20:21.985575: step 2666, loss 0.0657214, acc 0.9375
2016-07-27T16:20:24.554320: step 2667, loss 0.0320343, acc 0.984375
2016-07-27T16:20:27.255388: step 2668, loss 0.0601174, acc 0.96875
2016-07-27T16:20:30.171876: step 2669, loss 0.113027, acc 0.953125
2016-07-27T16:20:32.877981: step 2670, loss 0.0502186, acc 0.984375
2016-07-27T16:20:35.496210: step 2671, loss 0.0421297, acc 0.984375
2016-07-27T16:20:38.348074: step 2672, loss 0.100831, acc 0.953125
2016-07-27T16:20:40.924396: step 2673, loss 0.080491, acc 0.96875
2016-07-27T16:20:43.850413: step 2674, loss 0.054717, acc 0.96875
2016-07-27T16:20:46.585713: step 2675, loss 0.021772, acc 0.984375
2016-07-27T16:20:49.193830: step 2676, loss 0.0450159, acc 1
2016-07-27T16:20:52.083172: step 2677, loss 0.145, acc 0.9375
2016-07-27T16:20:54.662873: step 2678, loss 0.0462135, acc 0.984375
2016-07-27T16:20:57.373831: step 2679, loss 0.0696551, acc 0.96875
2016-07-27T16:21:00.260067: step 2680, loss 0.0362585, acc 1
2016-07-27T16:21:02.934646: step 2681, loss 0.0735299, acc 0.984375
2016-07-27T16:21:05.760799: step 2682, loss 0.165513, acc 0.96875
2016-07-27T16:21:08.530457: step 2683, loss 0.0863086, acc 0.96875
2016-07-27T16:21:11.066988: step 2684, loss 0.0328737, acc 1
2016-07-27T16:21:13.896087: step 2685, loss 0.119536, acc 0.953125
2016-07-27T16:21:16.459758: step 2686, loss 0.0932401, acc 0.984375
2016-07-27T16:21:19.308968: step 2687, loss 0.0516139, acc 1
2016-07-27T16:21:22.159183: step 2688, loss 0.0716924, acc 0.984375
2016-07-27T16:21:24.626103: step 2689, loss 0.0111008, acc 1
2016-07-27T16:21:27.407300: step 2690, loss 0.0628252, acc 0.984375
2016-07-27T16:21:30.072514: step 2691, loss 0.0537723, acc 0.96875
2016-07-27T16:21:32.730896: step 2692, loss 0.0505605, acc 0.984375
2016-07-27T16:21:35.602829: step 2693, loss 0.072488, acc 0.953125
2016-07-27T16:21:38.185294: step 2694, loss 0.0886432, acc 0.953125
2016-07-27T16:21:41.152451: step 2695, loss 0.0817691, acc 0.96875
2016-07-27T16:21:43.861454: step 2696, loss 0.0654074, acc 0.96875
2016-07-27T16:21:46.439998: step 2697, loss 0.0563267, acc 0.984375
2016-07-27T16:21:49.316540: step 2698, loss 0.0243692, acc 1
2016-07-27T16:21:51.922268: step 2699, loss 0.0478756, acc 1
2016-07-27T16:21:54.520614: step 2700, loss 0.11115, acc 0.953125

Evaluation:
2016-07-27T16:22:06.638466: step 2700, loss 1.59936, acc 0.7

Saved model checkpoint to /home/chaitanya/lstm-context-embeddings/runs/1469600026/checkpoints/model-2700

2016-07-27T16:22:10.699310: step 2701, loss 0.0725524, acc 0.96875
2016-07-27T16:22:14.320891: step 2702, loss 0.0648789, acc 0.984375
2016-07-27T16:22:17.326122: step 2703, loss 0.0173981, acc 1
2016-07-27T16:22:20.140082: step 2704, loss 0.128369, acc 0.984375
2016-07-27T16:22:22.607415: step 2705, loss 0.0367801, acc 1
2016-07-27T16:22:25.467653: step 2706, loss 0.0421467, acc 0.984375
2016-07-27T16:22:28.138704: step 2707, loss 0.061972, acc 0.984375
2016-07-27T16:22:30.805274: step 2708, loss 0.0516411, acc 0.984375
2016-07-27T16:22:33.629935: step 2709, loss 0.0849972, acc 0.96875
2016-07-27T16:22:36.149764: step 2710, loss 0.0163204, acc 1
2016-07-27T16:22:39.077436: step 2711, loss 0.036717, acc 0.984375
2016-07-27T16:22:41.803294: step 2712, loss 0.0197861, acc 1
2016-07-27T16:22:44.400544: step 2713, loss 0.110485, acc 0.9375
2016-07-27T16:22:47.234475: step 2714, loss 0.0235226, acc 1
2016-07-27T16:22:49.775700: step 2715, loss 0.0388072, acc 1
2016-07-27T16:22:52.430264: step 2716, loss 0.0567419, acc 0.984375
2016-07-27T16:22:55.400022: step 2717, loss 0.0755154, acc 0.96875
2016-07-27T16:22:58.103027: step 2718, loss 0.0208682, acc 0.983871
2016-07-27T16:23:00.680606: step 2719, loss 0.0432768, acc 0.984375
2016-07-27T16:23:03.553874: step 2720, loss 0.165687, acc 0.9375
2016-07-27T16:23:06.128367: step 2721, loss 0.118834, acc 0.9375
2016-07-27T16:23:08.791761: step 2722, loss 0.0145508, acc 1
2016-07-27T16:23:11.708026: step 2723, loss 0.0685953, acc 0.96875
2016-07-27T16:23:14.386667: step 2724, loss 0.0405224, acc 0.984375
2016-07-27T16:23:17.227133: step 2725, loss 0.0381763, acc 0.984375
2016-07-27T16:23:19.973787: step 2726, loss 0.0632657, acc 0.953125
2016-07-27T16:23:22.585033: step 2727, loss 0.0458916, acc 1
2016-07-27T16:23:25.432888: step 2728, loss 0.0810208, acc 0.953125
2016-07-27T16:23:28.041249: step 2729, loss 0.0569593, acc 0.984375
2016-07-27T16:23:30.729187: step 2730, loss 0.0927213, acc 0.96875
2016-07-27T16:23:33.604108: step 2731, loss 0.0450018, acc 0.984375
2016-07-27T16:23:36.282563: step 2732, loss 0.0649562, acc 0.953125
2016-07-27T16:23:39.153280: step 2733, loss 0.0931673, acc 0.96875
2016-07-27T16:23:41.871687: step 2734, loss 0.0260311, acc 0.984375
2016-07-27T16:23:44.392287: step 2735, loss 0.0835976, acc 0.953125
2016-07-27T16:23:47.217669: step 2736, loss 0.0983825, acc 0.953125
2016-07-27T16:23:49.816725: step 2737, loss 0.0203366, acc 1
2016-07-27T16:23:52.731560: step 2738, loss 0.0429908, acc 0.984375
2016-07-27T16:23:55.532944: step 2739, loss 0.0128707, acc 1
2016-07-27T16:23:57.996332: step 2740, loss 0.0329015, acc 1
2016-07-27T16:24:00.864312: step 2741, loss 0.0722045, acc 0.96875
2016-07-27T16:24:03.552879: step 2742, loss 0.0453744, acc 0.96875
2016-07-27T16:24:06.395202: step 2743, loss 0.155066, acc 0.921875
2016-07-27T16:24:09.293726: step 2744, loss 0.0686801, acc 0.96875
2016-07-27T16:24:11.810264: step 2745, loss 0.0525536, acc 0.96875
2016-07-27T16:24:14.624024: step 2746, loss 0.0737333, acc 0.984375
2016-07-27T16:24:17.325556: step 2747, loss 0.068809, acc 0.96875
2016-07-27T16:24:19.923728: step 2748, loss 0.0448949, acc 0.984375
2016-07-27T16:24:22.821750: step 2749, loss 0.0300665, acc 1
2016-07-27T16:24:25.374323: step 2750, loss 0.0627027, acc 0.984375
2016-07-27T16:24:28.278682: step 2751, loss 0.0258854, acc 1
2016-07-27T16:24:31.010714: step 2752, loss 0.0788572, acc 0.953125
2016-07-27T16:24:33.590368: step 2753, loss 0.0435278, acc 0.984375
2016-07-27T16:24:36.273865: step 2754, loss 0.0599041, acc 0.96875
2016-07-27T16:24:39.108793: step 2755, loss 0.0829389, acc 0.96875
2016-07-27T16:24:41.659456: step 2756, loss 0.044225, acc 0.96875
2016-07-27T16:24:44.654974: step 2757, loss 0.0777146, acc 0.96875
2016-07-27T16:24:47.327703: step 2758, loss 0.0524276, acc 0.96875
2016-07-27T16:24:49.946031: step 2759, loss 0.0270372, acc 1
2016-07-27T16:24:52.824922: step 2760, loss 0.0307583, acc 0.984375
2016-07-27T16:24:55.409208: step 2761, loss 0.0896773, acc 0.953125
2016-07-27T16:24:58.351166: step 2762, loss 0.0609772, acc 0.984375
2016-07-27T16:25:01.088586: step 2763, loss 0.0402688, acc 1
2016-07-27T16:25:03.603740: step 2764, loss 0.0287109, acc 0.984375
2016-07-27T16:25:06.402877: step 2765, loss 0.0751016, acc 0.953125
2016-07-27T16:25:08.947030: step 2766, loss 0.0731133, acc 0.984375
2016-07-27T16:25:11.754460: step 2767, loss 0.0778176, acc 0.96875
2016-07-27T16:25:14.578602: step 2768, loss 0.0540718, acc 0.984375
2016-07-27T16:25:16.983729: step 2769, loss 0.0619945, acc 0.953125
2016-07-27T16:25:19.772828: step 2770, loss 0.112838, acc 0.9375
2016-07-27T16:25:22.436368: step 2771, loss 0.0261226, acc 1
2016-07-27T16:25:24.994053: step 2772, loss 0.0159266, acc 1
2016-07-27T16:25:27.742361: step 2773, loss 0.107774, acc 0.921875
2016-07-27T16:25:30.284487: step 2774, loss 0.0468428, acc 0.96875
2016-07-27T16:25:32.929351: step 2775, loss 0.0342675, acc 0.96875
2016-07-27T16:25:35.813436: step 2776, loss 0.0506506, acc 0.984375
2016-07-27T16:25:38.431382: step 2777, loss 0.0653061, acc 0.96875
2016-07-27T16:25:40.940344: step 2778, loss 0.112947, acc 0.9375
2016-07-27T16:25:43.591621: step 2779, loss 0.0272595, acc 0.984375
2016-07-27T16:25:46.187456: step 2780, loss 0.11971, acc 0.9375
2016-07-27T16:25:48.990644: step 2781, loss 0.0849698, acc 0.96875
2016-07-27T16:25:51.631908: step 2782, loss 0.0403372, acc 0.984375
2016-07-27T16:25:54.484019: step 2783, loss 0.0617511, acc 0.984375
2016-07-27T16:25:57.181970: step 2784, loss 0.029406, acc 1
2016-07-27T16:25:59.676447: step 2785, loss 0.0350367, acc 0.984375
2016-07-27T16:26:02.551884: step 2786, loss 0.0371161, acc 0.984375
2016-07-27T16:26:05.102394: step 2787, loss 0.0579737, acc 0.984375
2016-07-27T16:26:07.971202: step 2788, loss 0.077938, acc 0.96875
2016-07-27T16:26:10.757988: step 2789, loss 0.0911116, acc 0.96875
2016-07-27T16:26:13.214577: step 2790, loss 0.0601116, acc 0.984375
2016-07-27T16:26:16.032179: step 2791, loss 0.0655207, acc 0.953125
2016-07-27T16:26:18.613918: step 2792, loss 0.0687679, acc 0.984375
2016-07-27T16:26:21.181692: step 2793, loss 0.0593543, acc 0.984375
2016-07-27T16:26:23.914378: step 2794, loss 0.0396427, acc 0.96875
2016-07-27T16:26:26.738441: step 2795, loss 0.110246, acc 0.96875
2016-07-27T16:26:29.425615: step 2796, loss 0.0372327, acc 0.984375
2016-07-27T16:26:31.943313: step 2797, loss 0.0381708, acc 1
2016-07-27T16:26:34.799194: step 2798, loss 0.0905123, acc 0.9375
2016-07-27T16:26:37.353439: step 2799, loss 0.0903424, acc 0.953125
2016-07-27T16:26:39.993609: step 2800, loss 0.0495237, acc 0.984375

Evaluation:
2016-07-27T16:26:52.070670: step 2800, loss 1.60975, acc 0.711

Saved model checkpoint to /home/chaitanya/lstm-context-embeddings/runs/1469600026/checkpoints/model-2800

2016-07-27T16:26:56.067768: step 2801, loss 0.0711105, acc 0.9375
2016-07-27T16:26:59.646634: step 2802, loss 0.063505, acc 0.984375
2016-07-27T16:27:02.696840: step 2803, loss 0.0668615, acc 0.96875
2016-07-27T16:27:05.529657: step 2804, loss 0.0367579, acc 0.984375
2016-07-27T16:27:07.899952: step 2805, loss 0.0554398, acc 1
2016-07-27T16:27:10.661971: step 2806, loss 0.0464778, acc 0.96875
2016-07-27T16:27:13.414380: step 2807, loss 0.0686493, acc 0.953125
2016-07-27T16:27:15.827325: step 2808, loss 0.0319625, acc 1
2016-07-27T16:27:18.703000: step 2809, loss 0.0617994, acc 0.984375
2016-07-27T16:27:21.374781: step 2810, loss 0.0727773, acc 0.96875
2016-07-27T16:27:23.866187: step 2811, loss 0.058827, acc 0.96875
2016-07-27T16:27:26.477649: step 2812, loss 0.0764122, acc 0.953125
2016-07-27T16:27:30.010791: step 2813, loss 0.0493576, acc 0.984375
2016-07-27T16:27:33.007892: step 2814, loss 0.0670307, acc 0.984375
2016-07-27T16:27:35.839088: step 2815, loss 0.0258438, acc 1
2016-07-27T16:27:38.249963: step 2816, loss 0.0892041, acc 0.96875
2016-07-27T16:27:41.076681: step 2817, loss 0.0501626, acc 0.984375
2016-07-27T16:27:43.750370: step 2818, loss 0.0669015, acc 0.96875
2016-07-27T16:27:46.223613: step 2819, loss 0.0383793, acc 0.984375
2016-07-27T16:27:49.086434: step 2820, loss 0.0695483, acc 0.96875
2016-07-27T16:27:51.734252: step 2821, loss 0.0307046, acc 1
2016-07-27T16:27:54.327240: step 2822, loss 0.0660112, acc 0.984375
2016-07-27T16:27:57.157489: step 2823, loss 0.0828598, acc 0.96875
2016-07-27T16:27:59.643802: step 2824, loss 0.0455162, acc 0.984375
2016-07-27T16:28:02.393737: step 2825, loss 0.101522, acc 0.96875
2016-07-27T16:28:05.201902: step 2826, loss 0.011174, acc 1
2016-07-27T16:28:07.682661: step 2827, loss 0.0306223, acc 0.984375
2016-07-27T16:28:10.446693: step 2828, loss 0.0878197, acc 0.953125
2016-07-27T16:28:13.064393: step 2829, loss 0.0446053, acc 0.984375
2016-07-27T16:28:15.915089: step 2830, loss 0.110245, acc 0.9375
2016-07-27T16:28:18.604401: step 2831, loss 0.0248134, acc 0.984375
2016-07-27T16:28:21.122715: step 2832, loss 0.0727629, acc 0.953125
2016-07-27T16:28:23.965882: step 2833, loss 0.0817428, acc 0.9375
2016-07-27T16:28:26.469390: step 2834, loss 0.0420988, acc 1
2016-07-27T16:28:29.050126: step 2835, loss 0.0791282, acc 0.96875
2016-07-27T16:28:31.944226: step 2836, loss 0.0425925, acc 0.96875
2016-07-27T16:28:34.637941: step 2837, loss 0.0671094, acc 1
2016-07-27T16:28:37.144099: step 2838, loss 0.0831093, acc 0.984375
2016-07-27T16:28:39.943559: step 2839, loss 0.0549225, acc 0.984375
2016-07-27T16:28:42.472579: step 2840, loss 0.0278321, acc 0.984375
2016-07-27T16:28:45.148567: step 2841, loss 0.0284806, acc 1
2016-07-27T16:28:47.977777: step 2842, loss 0.0534961, acc 0.984375
2016-07-27T16:28:50.666816: step 2843, loss 0.0527696, acc 0.96875
2016-07-27T16:28:53.171126: step 2844, loss 0.0626894, acc 0.953125
2016-07-27T16:28:56.018113: step 2845, loss 0.0343684, acc 0.96875
2016-07-27T16:28:58.550616: step 2846, loss 0.0308585, acc 1
2016-07-27T16:29:01.141588: step 2847, loss 0.0559706, acc 0.96875
2016-07-27T16:29:04.062664: step 2848, loss 0.0716814, acc 0.953125
2016-07-27T16:29:06.730585: step 2849, loss 0.0341593, acc 0.984375
2016-07-27T16:29:09.252028: step 2850, loss 0.0610102, acc 0.96875
2016-07-27T16:29:12.050338: step 2851, loss 0.118823, acc 0.921875
2016-07-27T16:29:14.566219: step 2852, loss 0.0347802, acc 0.984375
2016-07-27T16:29:17.152020: step 2853, loss 0.0933568, acc 0.953125
2016-07-27T16:29:19.991027: step 2854, loss 0.0425541, acc 0.984375
2016-07-27T16:29:22.698138: step 2855, loss 0.0813533, acc 0.984375
2016-07-27T16:29:25.241174: step 2856, loss 0.079804, acc 0.953125
2016-07-27T16:29:28.028846: step 2857, loss 0.0199157, acc 1
2016-07-27T16:29:30.556242: step 2858, loss 0.0309047, acc 1
2016-07-27T16:29:33.087512: step 2859, loss 0.0488343, acc 0.984375
2016-07-27T16:29:36.008876: step 2860, loss 0.0533195, acc 0.984375
2016-07-27T16:29:38.708829: step 2861, loss 0.0280569, acc 1
2016-07-27T16:29:41.178624: step 2862, loss 0.048816, acc 0.96875
2016-07-27T16:29:43.933089: step 2863, loss 0.0416898, acc 0.984375
2016-07-27T16:29:46.522735: step 2864, loss 0.0602638, acc 0.984375
2016-07-27T16:29:49.101288: step 2865, loss 0.0416515, acc 0.984375
2016-07-27T16:29:52.009001: step 2866, loss 0.0410603, acc 0.984375
2016-07-27T16:29:54.695794: step 2867, loss 0.0721539, acc 0.96875
2016-07-27T16:29:57.176721: step 2868, loss 0.0473935, acc 0.984375
2016-07-27T16:30:00.021261: step 2869, loss 0.0339614, acc 0.983871
2016-07-27T16:30:02.550022: step 2870, loss 0.0244699, acc 1
2016-07-27T16:30:05.120847: step 2871, loss 0.0560398, acc 0.96875
2016-07-27T16:30:07.923741: step 2872, loss 0.0496891, acc 0.984375
2016-07-27T16:30:10.645405: step 2873, loss 0.0401888, acc 0.984375
2016-07-27T16:30:13.137882: step 2874, loss 0.0558408, acc 0.96875
2016-07-27T16:30:15.996441: step 2875, loss 0.0405038, acc 0.984375
2016-07-27T16:30:18.564471: step 2876, loss 0.0372644, acc 0.984375
2016-07-27T16:30:21.064326: step 2877, loss 0.0482595, acc 0.96875
2016-07-27T16:30:23.998706: step 2878, loss 0.0686845, acc 0.96875
2016-07-27T16:30:26.753226: step 2879, loss 0.0924133, acc 0.96875
2016-07-27T16:30:29.244298: step 2880, loss 0.104461, acc 0.90625
2016-07-27T16:30:32.012429: step 2881, loss 0.0209507, acc 1
2016-07-27T16:30:34.536075: step 2882, loss 0.0743265, acc 0.96875
2016-07-27T16:30:37.099112: step 2883, loss 0.0566224, acc 0.96875
2016-07-27T16:30:40.006353: step 2884, loss 0.0425058, acc 0.984375
2016-07-27T16:30:42.742733: step 2885, loss 0.0409627, acc 0.984375
2016-07-27T16:30:45.281100: step 2886, loss 0.039849, acc 0.984375
2016-07-27T16:30:48.072204: step 2887, loss 0.0411848, acc 1
2016-07-27T16:30:50.609411: step 2888, loss 0.0304896, acc 0.984375
2016-07-27T16:30:53.151238: step 2889, loss 0.0318314, acc 1
2016-07-27T16:30:55.955267: step 2890, loss 0.026222, acc 0.984375
2016-07-27T16:30:58.505559: step 2891, loss 0.0288877, acc 1
2016-07-27T16:31:01.261622: step 2892, loss 0.0398254, acc 1
2016-07-27T16:31:04.035926: step 2893, loss 0.0474888, acc 0.96875
2016-07-27T16:31:06.565126: step 2894, loss 0.0766481, acc 0.984375
2016-07-27T16:31:09.457508: step 2895, loss 0.0421066, acc 0.984375
2016-07-27T16:31:12.125406: step 2896, loss 0.0441561, acc 0.984375
2016-07-27T16:31:14.673984: step 2897, loss 0.0311206, acc 1
2016-07-27T16:31:17.527120: step 2898, loss 0.0730805, acc 0.96875
2016-07-27T16:31:19.994335: step 2899, loss 0.0359727, acc 0.984375
2016-07-27T16:31:22.729876: step 2900, loss 0.0319073, acc 0.984375

Evaluation:
2016-07-27T16:31:35.048290: step 2900, loss 1.63406, acc 0.708

Saved model checkpoint to /home/chaitanya/lstm-context-embeddings/runs/1469600026/checkpoints/model-2900

2016-07-27T16:31:39.310206: step 2901, loss 0.085408, acc 0.96875
2016-07-27T16:31:41.848068: step 2902, loss 0.0629637, acc 0.96875
2016-07-27T16:31:44.622122: step 2903, loss 0.027904, acc 0.984375
2016-07-27T16:31:47.186736: step 2904, loss 0.095806, acc 0.96875
2016-07-27T16:31:49.928699: step 2905, loss 0.0189759, acc 1
2016-07-27T16:31:52.691263: step 2906, loss 0.0787975, acc 0.953125
2016-07-27T16:31:55.168290: step 2907, loss 0.0226483, acc 1
2016-07-27T16:31:58.044005: step 2908, loss 0.123262, acc 0.921875
2016-07-27T16:32:00.653211: step 2909, loss 0.0378751, acc 1
2016-07-27T16:32:03.290511: step 2910, loss 0.0141055, acc 1
2016-07-27T16:32:06.154279: step 2911, loss 0.0445312, acc 0.984375
2016-07-27T16:32:08.638112: step 2912, loss 0.0329982, acc 1
2016-07-27T16:32:11.475245: step 2913, loss 0.0429077, acc 0.984375
2016-07-27T16:32:14.164738: step 2914, loss 0.0427055, acc 0.96875
2016-07-27T16:32:16.657133: step 2915, loss 0.0145402, acc 1
2016-07-27T16:32:19.480028: step 2916, loss 0.0957018, acc 0.96875
2016-07-27T16:32:22.145151: step 2917, loss 0.0421678, acc 1
2016-07-27T16:32:24.639559: step 2918, loss 0.0543784, acc 0.96875
2016-07-27T16:32:27.534018: step 2919, loss 0.0899887, acc 0.96875
2016-07-27T16:32:30.211911: step 2920, loss 0.0425416, acc 1
2016-07-27T16:32:32.707329: step 2921, loss 0.0655596, acc 0.96875
2016-07-27T16:32:35.566615: step 2922, loss 0.048462, acc 0.96875
2016-07-27T16:32:38.049404: step 2923, loss 0.0304373, acc 0.984375
2016-07-27T16:32:40.748161: step 2924, loss 0.028416, acc 1
2016-07-27T16:32:43.559195: step 2925, loss 0.0369141, acc 1
2016-07-27T16:32:45.936547: step 2926, loss 0.0550623, acc 0.96875
2016-07-27T16:32:48.738337: step 2927, loss 0.0303715, acc 0.984375
2016-07-27T16:32:51.385020: step 2928, loss 0.066257, acc 0.96875
2016-07-27T16:32:53.950621: step 2929, loss 0.0558581, acc 0.984375
2016-07-27T16:32:56.826635: step 2930, loss 0.0688059, acc 0.984375
2016-07-27T16:32:59.278754: step 2931, loss 0.113409, acc 0.921875
2016-07-27T16:33:01.911139: step 2932, loss 0.0186672, acc 1
2016-07-27T16:33:04.770601: step 2933, loss 0.0319507, acc 0.984375
2016-07-27T16:33:07.383733: step 2934, loss 0.0530911, acc 0.984375
2016-07-27T16:33:09.864211: step 2935, loss 0.13142, acc 0.96875
2016-07-27T16:33:12.631770: step 2936, loss 0.0542742, acc 0.984375
2016-07-27T16:33:15.153547: step 2937, loss 0.0888745, acc 0.984375
2016-07-27T16:33:17.746518: step 2938, loss 0.0812871, acc 0.984375
2016-07-27T16:33:20.563909: step 2939, loss 0.0808733, acc 0.953125
2016-07-27T16:33:23.241985: step 2940, loss 0.0683703, acc 0.96875
2016-07-27T16:33:25.756341: step 2941, loss 0.107073, acc 0.9375
2016-07-27T16:33:28.608653: step 2942, loss 0.0719956, acc 0.96875
2016-07-27T16:33:31.157870: step 2943, loss 0.118068, acc 0.9375
2016-07-27T16:33:33.699578: step 2944, loss 0.0496028, acc 1
2016-07-27T16:33:36.533634: step 2945, loss 0.0508626, acc 0.984375
2016-07-27T16:33:39.218679: step 2946, loss 0.116076, acc 0.953125
2016-07-27T16:33:41.734772: step 2947, loss 0.120993, acc 0.9375
2016-07-27T16:33:44.559669: step 2948, loss 0.0571452, acc 0.96875
2016-07-27T16:33:47.130115: step 2949, loss 0.0584998, acc 0.96875
2016-07-27T16:33:49.667237: step 2950, loss 0.0553991, acc 0.96875
2016-07-27T16:33:52.465994: step 2951, loss 0.0718158, acc 0.96875
2016-07-27T16:33:55.170795: step 2952, loss 0.0483543, acc 0.984375
2016-07-27T16:33:57.701682: step 2953, loss 0.123796, acc 0.9375
2016-07-27T16:34:00.538888: step 2954, loss 0.064237, acc 0.96875
2016-07-27T16:34:03.094120: step 2955, loss 0.0300142, acc 1
2016-07-27T16:34:05.635980: step 2956, loss 0.0552279, acc 1
2016-07-27T16:34:08.502022: step 2957, loss 0.040705, acc 0.96875
2016-07-27T16:34:11.217148: step 2958, loss 0.0257906, acc 1
2016-07-27T16:34:13.717512: step 2959, loss 0.0338913, acc 1
2016-07-27T16:34:16.536630: step 2960, loss 0.0760833, acc 0.953125
2016-07-27T16:34:19.116335: step 2961, loss 0.0721076, acc 1
2016-07-27T16:34:21.695572: step 2962, loss 0.0282363, acc 0.984375
2016-07-27T16:34:24.644146: step 2963, loss 0.045396, acc 0.984375
2016-07-27T16:34:27.357335: step 2964, loss 0.0547464, acc 0.984375
2016-07-27T16:34:29.850603: step 2965, loss 0.0336578, acc 1
2016-07-27T16:34:32.439824: step 2966, loss 0.0599842, acc 0.984375
2016-07-27T16:34:36.015840: step 2967, loss 0.0592063, acc 0.96875
2016-07-27T16:34:39.053421: step 2968, loss 0.128827, acc 0.921875
2016-07-27T16:34:41.853898: step 2969, loss 0.0397651, acc 0.984375
2016-07-27T16:34:44.233269: step 2970, loss 0.0245501, acc 0.984375
2016-07-27T16:34:47.164162: step 2971, loss 0.152223, acc 0.9375
2016-07-27T16:34:50.050768: step 2972, loss 0.0372543, acc 0.984375
2016-07-27T16:34:52.547814: step 2973, loss 0.0150445, acc 1
2016-07-27T16:34:55.153182: step 2974, loss 0.04368, acc 0.96875
2016-07-27T16:34:58.155127: step 2975, loss 0.118196, acc 0.921875
2016-07-27T16:35:00.892478: step 2976, loss 0.0503059, acc 0.984375
2016-07-27T16:35:03.536309: step 2977, loss 0.0341748, acc 0.984375
2016-07-27T16:35:06.391455: step 2978, loss 0.0440844, acc 0.96875
2016-07-27T16:35:08.906691: step 2979, loss 0.0439095, acc 0.984375
2016-07-27T16:35:11.827686: step 2980, loss 0.0277846, acc 1
2016-07-27T16:35:14.569356: step 2981, loss 0.0377527, acc 0.984375
2016-07-27T16:35:17.029530: step 2982, loss 0.029134, acc 0.984375
2016-07-27T16:35:19.927207: step 2983, loss 0.0380245, acc 0.984375
2016-07-27T16:35:22.593186: step 2984, loss 0.0761377, acc 0.953125
2016-07-27T16:35:25.155254: step 2985, loss 0.0520558, acc 0.984375
2016-07-27T16:35:27.904710: step 2986, loss 0.0631865, acc 0.96875
2016-07-27T16:35:31.464566: step 2987, loss 0.048468, acc 0.984375
2016-07-27T16:35:34.410226: step 2988, loss 0.0729562, acc 0.96875
2016-07-27T16:35:37.193557: step 2989, loss 0.0552417, acc 0.96875
2016-07-27T16:35:39.705654: step 2990, loss 0.0544581, acc 0.984375
2016-07-27T16:35:42.552069: step 2991, loss 0.0293581, acc 0.984375
2016-07-27T16:35:45.168243: step 2992, loss 0.0272532, acc 1
2016-07-27T16:35:47.972203: step 2993, loss 0.0479773, acc 0.984375
2016-07-27T16:35:50.742962: step 2994, loss 0.0276877, acc 0.984375
2016-07-27T16:35:53.214679: step 2995, loss 0.03099, acc 1
2016-07-27T16:35:56.131714: step 2996, loss 0.0353223, acc 0.984375
2016-07-27T16:35:58.843245: step 2997, loss 0.0885708, acc 0.953125
2016-07-27T16:36:01.398867: step 2998, loss 0.0429013, acc 0.984375
2016-07-27T16:36:04.234615: step 2999, loss 0.0858417, acc 0.96875
2016-07-27T16:36:06.867858: step 3000, loss 0.0391159, acc 1

Evaluation:
2016-07-27T16:36:19.054186: step 3000, loss 1.48614, acc 0.714

Saved model checkpoint to /home/chaitanya/lstm-context-embeddings/runs/1469600026/checkpoints/model-3000

2016-07-27T16:36:23.040430: step 3001, loss 0.0516464, acc 0.96875
2016-07-27T16:36:25.908321: step 3002, loss 0.0869152, acc 0.953125
2016-07-27T16:36:28.647949: step 3003, loss 0.0528943, acc 0.984375
2016-07-27T16:36:31.321476: step 3004, loss 0.0332701, acc 0.984375
2016-07-27T16:36:34.133711: step 3005, loss 0.125286, acc 0.953125
2016-07-27T16:36:36.592087: step 3006, loss 0.0599543, acc 0.984375
2016-07-27T16:36:39.434370: step 3007, loss 0.0366682, acc 1
2016-07-27T16:36:42.056788: step 3008, loss 0.0167569, acc 0.984375
2016-07-27T16:36:44.909618: step 3009, loss 0.082781, acc 0.96875
2016-07-27T16:36:47.622159: step 3010, loss 0.0411772, acc 0.984375
2016-07-27T16:36:50.195238: step 3011, loss 0.125125, acc 0.9375
2016-07-27T16:36:53.057115: step 3012, loss 0.0133737, acc 0.984375
2016-07-27T16:36:55.624757: step 3013, loss 0.0297267, acc 0.984375
2016-07-27T16:36:58.325449: step 3014, loss 0.0346674, acc 1
2016-07-27T16:37:01.215593: step 3015, loss 0.0573861, acc 1
2016-07-27T16:37:03.866958: step 3016, loss 0.0419226, acc 0.984375
2016-07-27T16:37:06.394917: step 3017, loss 0.091223, acc 0.953125
2016-07-27T16:37:09.288309: step 3018, loss 0.0499137, acc 0.984375
2016-07-27T16:37:11.993281: step 3019, loss 0.215187, acc 0.921875
2016-07-27T16:37:14.586548: step 3020, loss 0.0180568, acc 1
2016-07-27T16:37:17.396484: step 3021, loss 0.0460498, acc 1
2016-07-27T16:37:20.037540: step 3022, loss 0.0686668, acc 0.953125
2016-07-27T16:37:22.636641: step 3023, loss 0.137096, acc 0.96875
2016-07-27T16:37:25.487189: step 3024, loss 0.0140139, acc 1
2016-07-27T16:37:28.150721: step 3025, loss 0.0647156, acc 0.96875
2016-07-27T16:37:30.988562: step 3026, loss 0.0464301, acc 0.984375
2016-07-27T16:37:33.721438: step 3027, loss 0.152304, acc 0.984375
2016-07-27T16:37:36.258516: step 3028, loss 0.0476274, acc 0.984375
2016-07-27T16:37:39.144148: step 3029, loss 0.0638533, acc 0.984375
2016-07-27T16:37:41.763951: step 3030, loss 0.0377981, acc 1
2016-07-27T16:37:44.431734: step 3031, loss 0.150611, acc 0.953125
2016-07-27T16:37:47.275474: step 3032, loss 0.0389259, acc 1
2016-07-27T16:37:50.003328: step 3033, loss 0.047093, acc 0.984375
2016-07-27T16:37:52.611755: step 3034, loss 0.0746131, acc 0.953125
2016-07-27T16:37:55.473475: step 3035, loss 0.10058, acc 0.953125
2016-07-27T16:37:58.146127: step 3036, loss 0.0542501, acc 0.953125
2016-07-27T16:38:01.004632: step 3037, loss 0.113123, acc 0.953125
2016-07-27T16:38:03.740664: step 3038, loss 0.0887223, acc 0.984375
2016-07-27T16:38:06.252787: step 3039, loss 0.0811432, acc 0.953125
2016-07-27T16:38:09.119153: step 3040, loss 0.0421425, acc 0.984375
2016-07-27T16:38:11.720849: step 3041, loss 0.0367429, acc 0.984375
2016-07-27T16:38:14.409112: step 3042, loss 0.0351314, acc 0.984375
2016-07-27T16:38:17.225743: step 3043, loss 0.0382806, acc 1
2016-07-27T16:38:19.797302: step 3044, loss 0.059933, acc 0.96875
2016-07-27T16:38:22.647657: step 3045, loss 0.0912403, acc 0.96875
2016-07-27T16:38:25.284412: step 3046, loss 0.121586, acc 0.953125
2016-07-27T16:38:28.078928: step 3047, loss 0.0464235, acc 1
2016-07-27T16:38:30.828933: step 3048, loss 0.0649917, acc 0.96875
2016-07-27T16:38:33.369322: step 3049, loss 0.0630494, acc 1
2016-07-27T16:38:36.205049: step 3050, loss 0.0735438, acc 0.984375
2016-07-27T16:38:38.836356: step 3051, loss 0.0108121, acc 1
2016-07-27T16:38:41.546040: step 3052, loss 0.0700514, acc 0.96875
2016-07-27T16:38:44.398517: step 3053, loss 0.0343412, acc 1
2016-07-27T16:38:47.066302: step 3054, loss 0.0290619, acc 0.984375
2016-07-27T16:38:49.862249: step 3055, loss 0.0520214, acc 0.96875
2016-07-27T16:38:52.632388: step 3056, loss 0.0787976, acc 0.9375
2016-07-27T16:38:55.262275: step 3057, loss 0.0498957, acc 0.984375
2016-07-27T16:38:58.115170: step 3058, loss 0.0962248, acc 0.96875
2016-07-27T16:39:00.710316: step 3059, loss 0.0819069, acc 0.984375
2016-07-27T16:39:03.597433: step 3060, loss 0.133072, acc 0.953125
2016-07-27T16:39:06.257178: step 3061, loss 0.0600367, acc 0.96875
2016-07-27T16:39:08.857993: step 3062, loss 0.0391132, acc 0.984375
2016-07-27T16:39:11.568164: step 3063, loss 0.0522734, acc 0.984375
2016-07-27T16:39:14.171449: step 3064, loss 0.0983055, acc 0.9375
2016-07-27T16:39:16.963847: step 3065, loss 0.0585025, acc 0.96875
2016-07-27T16:39:19.564400: step 3066, loss 0.0477897, acc 0.984375
2016-07-27T16:39:22.138748: step 3067, loss 0.0368131, acc 1
2016-07-27T16:39:24.975386: step 3068, loss 0.0795579, acc 0.9375
2016-07-27T16:39:27.633942: step 3069, loss 0.100507, acc 0.9375
2016-07-27T16:39:30.482496: step 3070, loss 0.0815588, acc 0.984375
2016-07-27T16:39:33.116784: step 3071, loss 0.0515286, acc 0.984375
2016-07-27T16:39:35.746699: step 3072, loss 0.0656599, acc 0.953125
2016-07-27T16:39:38.637132: step 3073, loss 0.0485093, acc 0.984375
2016-07-27T16:39:41.346399: step 3074, loss 0.0672125, acc 0.984375
2016-07-27T16:39:44.124970: step 3075, loss 0.0696476, acc 0.9375
2016-07-27T16:39:46.908170: step 3076, loss 0.0774546, acc 0.96875
2016-07-27T16:39:49.415434: step 3077, loss 0.0278677, acc 0.984375
2016-07-27T16:39:52.278473: step 3078, loss 0.0224815, acc 1
2016-07-27T16:39:54.862548: step 3079, loss 0.0874749, acc 0.921875
2016-07-27T16:39:57.706871: step 3080, loss 0.117224, acc 0.96875
2016-07-27T16:40:00.364334: step 3081, loss 0.0118777, acc 1
2016-07-27T16:40:02.976518: step 3082, loss 0.078599, acc 0.96875
2016-07-27T16:40:05.778345: step 3083, loss 0.0323352, acc 1
2016-07-27T16:40:08.499910: step 3084, loss 0.105416, acc 0.953125
2016-07-27T16:40:11.340938: step 3085, loss 0.0564986, acc 0.984375
2016-07-27T16:40:14.083120: step 3086, loss 0.0746353, acc 0.984375
2016-07-27T16:40:16.617502: step 3087, loss 0.035661, acc 1
2016-07-27T16:40:19.446739: step 3088, loss 0.0739678, acc 0.96875
2016-07-27T16:40:22.097497: step 3089, loss 0.0475964, acc 0.96875
2016-07-27T16:40:24.699556: step 3090, loss 0.0355696, acc 1
2016-07-27T16:40:27.328588: step 3091, loss 0.109808, acc 0.953125
2016-07-27T16:40:30.216025: step 3092, loss 0.0266709, acc 1
2016-07-27T16:40:32.837026: step 3093, loss 0.0344286, acc 1
2016-07-27T16:40:35.681044: step 3094, loss 0.0637609, acc 0.96875
2016-07-27T16:40:38.376928: step 3095, loss 0.0939271, acc 0.921875
2016-07-27T16:40:41.043264: step 3096, loss 0.0500988, acc 0.96875
2016-07-27T16:40:43.867953: step 3097, loss 0.0785497, acc 0.953125
2016-07-27T16:40:46.515261: step 3098, loss 0.0697469, acc 0.953125
2016-07-27T16:40:49.357017: step 3099, loss 0.0566052, acc 0.984375
2016-07-27T16:40:52.124147: step 3100, loss 0.0556887, acc 0.984375

Evaluation:
2016-07-27T16:41:04.137028: step 3100, loss 1.45918, acc 0.705

Saved model checkpoint to /home/chaitanya/lstm-context-embeddings/runs/1469600026/checkpoints/model-3100

2016-07-27T16:41:08.123609: step 3101, loss 0.0533604, acc 0.984375
2016-07-27T16:41:11.003157: step 3102, loss 0.0523368, acc 0.96875
2016-07-27T16:41:13.657674: step 3103, loss 0.0255863, acc 1
2016-07-27T16:41:16.501582: step 3104, loss 0.0559257, acc 0.984375
2016-07-27T16:41:19.207172: step 3105, loss 0.0466022, acc 1
2016-07-27T16:41:21.932109: step 3106, loss 0.111079, acc 0.96875
2016-07-27T16:41:24.736603: step 3107, loss 0.0750883, acc 0.953125
2016-07-27T16:41:27.282677: step 3108, loss 0.0330058, acc 1
2016-07-27T16:41:30.125915: step 3109, loss 0.124691, acc 0.9375
2016-07-27T16:41:32.778951: step 3110, loss 0.0276347, acc 1
2016-07-27T16:41:35.393691: step 3111, loss 0.0643228, acc 1
2016-07-27T16:41:38.105823: step 3112, loss 0.0546049, acc 0.96875
2016-07-27T16:41:40.962768: step 3113, loss 0.0144122, acc 1
2016-07-27T16:41:43.628035: step 3114, loss 0.101098, acc 0.953125
2016-07-27T16:41:46.425733: step 3115, loss 0.0762416, acc 0.96875
2016-07-27T16:41:49.168098: step 3116, loss 0.031528, acc 1
2016-07-27T16:41:51.851229: step 3117, loss 0.0425161, acc 1
2016-07-27T16:41:54.677021: step 3118, loss 0.0272372, acc 1
2016-07-27T16:41:57.237802: step 3119, loss 0.0893484, acc 0.96875
2016-07-27T16:42:00.094395: step 3120, loss 0.0349264, acc 1
2016-07-27T16:42:02.778339: step 3121, loss 0.0234571, acc 1
2016-07-27T16:42:05.601375: step 3122, loss 0.0893765, acc 0.96875
2016-07-27T16:42:08.318672: step 3123, loss 0.0521888, acc 0.984375
2016-07-27T16:42:11.101392: step 3124, loss 0.0253622, acc 1
2016-07-27T16:42:13.898106: step 3125, loss 0.105531, acc 0.96875
2016-07-27T16:42:16.349257: step 3126, loss 0.0386039, acc 0.984375
2016-07-27T16:42:19.164695: step 3127, loss 0.0345945, acc 1
2016-07-27T16:42:21.860080: step 3128, loss 0.0543268, acc 0.953125
2016-07-27T16:42:24.668101: step 3129, loss 0.0582477, acc 0.984375
2016-07-27T16:42:27.406659: step 3130, loss 0.043265, acc 0.984375
2016-07-27T16:42:29.918389: step 3131, loss 0.0501229, acc 0.984375
2016-07-27T16:42:32.769661: step 3132, loss 0.0820316, acc 0.984375
2016-07-27T16:42:35.437436: step 3133, loss 0.0198912, acc 1
2016-07-27T16:42:38.083649: step 3134, loss 0.0913696, acc 0.953125
2016-07-27T16:42:40.653308: step 3135, loss 0.0430716, acc 1
2016-07-27T16:42:43.528403: step 3136, loss 0.0413821, acc 1
2016-07-27T16:42:46.200890: step 3137, loss 0.0489741, acc 0.96875
2016-07-27T16:42:48.979101: step 3138, loss 0.07638, acc 0.96875
2016-07-27T16:42:51.685903: step 3139, loss 0.112634, acc 0.96875
2016-07-27T16:42:54.335799: step 3140, loss 0.0795728, acc 0.953125
2016-07-27T16:42:57.219201: step 3141, loss 0.081937, acc 0.953125
2016-07-27T16:42:59.848278: step 3142, loss 0.0235546, acc 1
2016-07-27T16:43:02.482070: step 3143, loss 0.0292639, acc 1
2016-07-27T16:43:05.203906: step 3144, loss 0.0257683, acc 1
2016-07-27T16:43:07.987843: step 3145, loss 0.0375399, acc 1
2016-07-27T16:43:10.689859: step 3146, loss 0.0554927, acc 0.96875
2016-07-27T16:43:13.301295: step 3147, loss 0.0471486, acc 0.96875
2016-07-27T16:43:16.137349: step 3148, loss 0.0402369, acc 0.984375
2016-07-27T16:43:18.757698: step 3149, loss 0.0177732, acc 1
2016-07-27T16:43:21.357395: step 3150, loss 0.0529321, acc 0.953125
2016-07-27T16:43:24.229018: step 3151, loss 0.0524116, acc 0.96875
2016-07-27T16:43:26.908565: step 3152, loss 0.0711227, acc 0.953125
2016-07-27T16:43:29.680224: step 3153, loss 0.148239, acc 0.90625
2016-07-27T16:43:32.434769: step 3154, loss 0.0909607, acc 0.984375
2016-07-27T16:43:35.147341: step 3155, loss 0.05428, acc 0.984375
2016-07-27T16:43:37.964597: step 3156, loss 0.0698368, acc 0.96875
2016-07-27T16:43:40.477946: step 3157, loss 0.12683, acc 0.921875
2016-07-27T16:43:43.272359: step 3158, loss 0.12077, acc 0.984375
2016-07-27T16:43:45.891183: step 3159, loss 0.0456218, acc 0.984375
2016-07-27T16:43:48.649755: step 3160, loss 0.0496589, acc 0.984375
2016-07-27T16:43:51.410500: step 3161, loss 0.0303991, acc 1
2016-07-27T16:43:53.939042: step 3162, loss 0.0604286, acc 0.96875
2016-07-27T16:43:56.781339: step 3163, loss 0.0130706, acc 1
2016-07-27T16:43:59.362852: step 3164, loss 0.0459608, acc 0.984375
2016-07-27T16:44:02.181532: step 3165, loss 0.060578, acc 1
2016-07-27T16:44:04.863282: step 3166, loss 0.0610685, acc 0.96875
2016-07-27T16:44:07.513051: step 3167, loss 0.0201673, acc 1
2016-07-27T16:44:10.324624: step 3168, loss 0.0660174, acc 0.984375
2016-07-27T16:44:13.046441: step 3169, loss 0.0678367, acc 0.96875
2016-07-27T16:44:15.699397: step 3170, loss 0.0409778, acc 0.984375
2016-07-27T16:44:18.394674: step 3171, loss 0.0226922, acc 1
2016-07-27T16:44:21.255151: step 3172, loss 0.0307007, acc 1
2016-07-27T16:44:23.926089: step 3173, loss 0.0676189, acc 0.96875
2016-07-27T16:44:26.571904: step 3174, loss 0.0603199, acc 1
2016-07-27T16:44:29.404100: step 3175, loss 0.025117, acc 1
2016-07-27T16:44:32.044404: step 3176, loss 0.0613718, acc 0.984375
2016-07-27T16:44:34.886125: step 3177, loss 0.0548252, acc 0.953125
2016-07-27T16:44:37.615260: step 3178, loss 0.0421257, acc 0.984375
2016-07-27T16:44:40.331976: step 3179, loss 0.0543076, acc 0.984375
2016-07-27T16:44:43.186054: step 3180, loss 0.0539257, acc 0.96875
2016-07-27T16:44:45.683586: step 3181, loss 0.0560746, acc 0.96875
2016-07-27T16:44:48.211976: step 3182, loss 0.0500869, acc 0.984375
2016-07-27T16:44:51.066175: step 3183, loss 0.0210182, acc 0.984375
2016-07-27T16:44:53.666512: step 3184, loss 0.0423146, acc 0.984375
2016-07-27T16:44:56.321901: step 3185, loss 0.0415786, acc 1
2016-07-27T16:44:59.384603: step 3186, loss 0.0393054, acc 1
2016-07-27T16:45:02.869134: step 3187, loss 0.0550174, acc 0.96875
2016-07-27T16:45:05.804194: step 3188, loss 0.0703676, acc 0.984375
2016-07-27T16:45:08.454720: step 3189, loss 0.0402645, acc 1
2016-07-27T16:45:11.219449: step 3190, loss 0.0533074, acc 0.96875
2016-07-27T16:45:14.123020: step 3191, loss 0.052829, acc 0.984375
2016-07-27T16:45:16.616465: step 3192, loss 0.047786, acc 0.984375
2016-07-27T16:45:19.426888: step 3193, loss 0.0197171, acc 1
2016-07-27T16:45:22.052881: step 3194, loss 0.0424351, acc 0.96875
2016-07-27T16:45:24.646731: step 3195, loss 0.0481683, acc 0.96875
2016-07-27T16:45:27.669997: step 3196, loss 0.088902, acc 0.953125
2016-07-27T16:45:30.308649: step 3197, loss 0.034632, acc 1
2016-07-27T16:45:33.425532: step 3198, loss 0.0395904, acc 0.984375
2016-07-27T16:45:36.279338: step 3199, loss 0.01988, acc 1
2016-07-27T16:45:39.186658: step 3200, loss 0.0455593, acc 0.96875

Evaluation:
2016-07-27T16:45:51.807686: step 3200, loss 1.59618, acc 0.697

Saved model checkpoint to /home/chaitanya/lstm-context-embeddings/runs/1469600026/checkpoints/model-3200

2016-07-27T16:45:56.312187: step 3201, loss 0.0312734, acc 1
2016-07-27T16:45:58.914692: step 3202, loss 0.0618486, acc 0.984375
2016-07-27T16:46:01.771237: step 3203, loss 0.037021, acc 0.984375
2016-07-27T16:46:04.788586: step 3204, loss 0.0398567, acc 1
2016-07-27T16:46:07.695920: step 3205, loss 0.0132728, acc 1
2016-07-27T16:46:10.352137: step 3206, loss 0.0367687, acc 0.984375
2016-07-27T16:46:13.207380: step 3207, loss 0.0502731, acc 0.953125
2016-07-27T16:46:15.971888: step 3208, loss 0.0616393, acc 0.984375
2016-07-27T16:46:18.474546: step 3209, loss 0.155387, acc 0.984375
2016-07-27T16:46:21.234628: step 3210, loss 0.0419227, acc 0.984375
2016-07-27T16:46:23.775065: step 3211, loss 0.0486301, acc 0.96875
2016-07-27T16:46:26.663227: step 3212, loss 0.0323113, acc 1
2016-07-27T16:46:29.486233: step 3213, loss 0.0184387, acc 1
2016-07-27T16:46:31.935774: step 3214, loss 0.0653132, acc 0.96875
2016-07-27T16:46:34.767856: step 3215, loss 0.0557053, acc 0.984375
2016-07-27T16:46:37.429078: step 3216, loss 0.0219457, acc 1
2016-07-27T16:46:40.055424: step 3217, loss 0.0487859, acc 0.984375
2016-07-27T16:46:42.764860: step 3218, loss 0.02568, acc 1
2016-07-27T16:46:45.640679: step 3219, loss 0.0791639, acc 0.984375
2016-07-27T16:46:48.328151: step 3220, loss 0.0539005, acc 0.96875
2016-07-27T16:46:51.153023: step 3221, loss 0.0745864, acc 0.96875
2016-07-27T16:46:53.947647: step 3222, loss 0.0414884, acc 0.984375
2016-07-27T16:46:56.649239: step 3223, loss 0.206859, acc 0.96875
2016-07-27T16:46:59.501650: step 3224, loss 0.0471332, acc 0.984375
2016-07-27T16:47:01.995885: step 3225, loss 0.0328746, acc 0.984375
2016-07-27T16:47:04.868741: step 3226, loss 0.117425, acc 0.9375
2016-07-27T16:47:07.545578: step 3227, loss 0.0504277, acc 0.984375
2016-07-27T16:47:10.379482: step 3228, loss 0.0287199, acc 1
2016-07-27T16:47:13.077212: step 3229, loss 0.0185348, acc 1
2016-07-27T16:47:15.643896: step 3230, loss 0.0448425, acc 0.984375
2016-07-27T16:47:18.459807: step 3231, loss 0.068223, acc 0.96875
2016-07-27T16:47:21.413077: step 3232, loss 0.0244438, acc 1
2016-07-27T16:47:26.305008: step 3233, loss 0.0493933, acc 0.96875
2016-07-27T16:47:30.038661: step 3234, loss 0.0404802, acc 0.953125
2016-07-27T16:47:32.988320: step 3235, loss 0.0802519, acc 0.953125
2016-07-27T16:47:35.711835: step 3236, loss 0.192134, acc 0.96875
2016-07-27T16:47:38.808126: step 3237, loss 0.0349076, acc 0.984375
2016-07-27T16:47:41.682935: step 3238, loss 0.0403448, acc 0.984375
2016-07-27T16:47:44.288987: step 3239, loss 0.0615371, acc 0.9375
2016-07-27T16:47:47.099611: step 3240, loss 0.049674, acc 0.96875
2016-07-27T16:47:50.407191: step 3241, loss 0.0817995, acc 0.984375
2016-07-27T16:47:53.184667: step 3242, loss 0.107317, acc 0.953125
2016-07-27T16:47:56.058660: step 3243, loss 0.0660593, acc 0.984375
2016-07-27T16:47:59.392559: step 3244, loss 0.0326074, acc 0.984375
2016-07-27T16:48:02.156701: step 3245, loss 0.0542138, acc 1
2016-07-27T16:48:05.073061: step 3246, loss 0.0313694, acc 0.984375
2016-07-27T16:48:08.122799: step 3247, loss 0.064821, acc 0.96875
2016-07-27T16:48:10.818255: step 3248, loss 0.157504, acc 0.890625
2016-07-27T16:48:13.896391: step 3249, loss 0.134997, acc 0.9375
2016-07-27T16:48:16.679556: step 3250, loss 0.0908162, acc 0.984375
2016-07-27T16:48:19.567615: step 3251, loss 0.0912971, acc 0.96875
2016-07-27T16:48:22.270391: step 3252, loss 0.0512033, acc 1
2016-07-27T16:48:24.854221: step 3253, loss 0.0426769, acc 0.984375
2016-07-27T16:48:27.665553: step 3254, loss 0.0578922, acc 0.96875
2016-07-27T16:48:30.268861: step 3255, loss 0.0595059, acc 0.953125
2016-07-27T16:48:32.933400: step 3256, loss 0.0436052, acc 1
2016-07-27T16:48:35.717368: step 3257, loss 0.0465771, acc 0.984375
2016-07-27T16:48:38.354238: step 3258, loss 0.0664936, acc 0.96875
2016-07-27T16:48:41.049408: step 3259, loss 0.0322604, acc 0.984375
2016-07-27T16:48:43.831834: step 3260, loss 0.074892, acc 0.984375
2016-07-27T16:48:46.463830: step 3261, loss 0.0493649, acc 0.984375
2016-07-27T16:48:49.266932: step 3262, loss 0.0823041, acc 0.9375
2016-07-27T16:48:51.842484: step 3263, loss 0.0435444, acc 1
2016-07-27T16:48:54.356823: step 3264, loss 0.062211, acc 0.984375
2016-07-27T16:48:57.250859: step 3265, loss 0.0815203, acc 0.96875
2016-07-27T16:48:59.901684: step 3266, loss 0.0631974, acc 0.96875
2016-07-27T16:49:02.734950: step 3267, loss 0.0451856, acc 0.96875
2016-07-27T16:49:05.469864: step 3268, loss 0.0301406, acc 1
2016-07-27T16:49:08.079370: step 3269, loss 0.0541707, acc 0.96875
2016-07-27T16:49:10.886628: step 3270, loss 0.0781306, acc 0.96875
2016-07-27T16:49:13.569494: step 3271, loss 0.0392856, acc 0.96875
2016-07-27T16:49:16.401196: step 3272, loss 0.0421933, acc 0.984375
2016-07-27T16:49:19.162455: step 3273, loss 0.0607413, acc 0.984375
2016-07-27T16:49:21.872281: step 3274, loss 0.100967, acc 0.9375
2016-07-27T16:49:24.715214: step 3275, loss 0.031686, acc 0.984375
2016-07-27T16:49:27.232995: step 3276, loss 0.0900188, acc 0.984375
2016-07-27T16:49:30.095172: step 3277, loss 0.0441937, acc 0.984375
2016-07-27T16:49:32.745459: step 3278, loss 0.0533062, acc 0.953125
2016-07-27T16:49:35.566571: step 3279, loss 0.0164068, acc 1
2016-07-27T16:49:38.298822: step 3280, loss 0.0329961, acc 1
2016-07-27T16:49:41.021189: step 3281, loss 0.0556813, acc 0.984375
2016-07-27T16:49:43.870085: step 3282, loss 0.0867981, acc 0.96875
2016-07-27T16:49:46.427994: step 3283, loss 0.0655002, acc 0.953125
2016-07-27T16:49:49.293501: step 3284, loss 0.0252158, acc 0.984375
2016-07-27T16:49:51.931201: step 3285, loss 0.0665062, acc 0.96875
2016-07-27T16:49:54.772980: step 3286, loss 0.0622993, acc 0.953125
2016-07-27T16:49:57.538193: step 3287, loss 0.0971412, acc 0.953125
2016-07-27T16:50:00.115392: step 3288, loss 0.0894953, acc 0.953125
2016-07-27T16:50:03.153308: step 3289, loss 0.0355731, acc 1
2016-07-27T16:50:05.746502: step 3290, loss 0.0733899, acc 0.96875
2016-07-27T16:50:08.474516: step 3291, loss 0.101784, acc 0.953125
2016-07-27T16:50:11.310783: step 3292, loss 0.0647685, acc 0.96875
2016-07-27T16:50:13.949279: step 3293, loss 0.0489219, acc 0.96875
2016-07-27T16:50:16.807838: step 3294, loss 0.0354209, acc 0.984375
2016-07-27T16:50:19.548287: step 3295, loss 0.0588185, acc 0.984375
2016-07-27T16:50:22.086260: step 3296, loss 0.0479175, acc 0.96875
2016-07-27T16:50:25.173657: step 3297, loss 0.0279438, acc 1
2016-07-27T16:50:28.068682: step 3298, loss 0.0565732, acc 0.953125
2016-07-27T16:50:31.234216: step 3299, loss 0.0673363, acc 0.96875
2016-07-27T16:50:33.976703: step 3300, loss 0.0457142, acc 0.984375

Evaluation:
2016-07-27T16:50:46.380932: step 3300, loss 1.54982, acc 0.692

Saved model checkpoint to /home/chaitanya/lstm-context-embeddings/runs/1469600026/checkpoints/model-3300

2016-07-27T16:50:50.628915: step 3301, loss 0.0723618, acc 0.96875
2016-07-27T16:50:53.408931: step 3302, loss 0.0114925, acc 1
2016-07-27T16:50:56.080712: step 3303, loss 0.0424471, acc 0.984375
2016-07-27T16:50:58.862536: step 3304, loss 0.0875957, acc 0.9375
2016-07-27T16:51:01.355237: step 3305, loss 0.0610247, acc 0.984375
2016-07-27T16:51:04.056849: step 3306, loss 0.0275137, acc 1
2016-07-27T16:51:06.777020: step 3307, loss 0.0275535, acc 1
2016-07-27T16:51:09.224538: step 3308, loss 0.155554, acc 0.953125
2016-07-27T16:51:11.896465: step 3309, loss 0.0256537, acc 1
2016-07-27T16:51:14.449181: step 3310, loss 0.0127403, acc 1
2016-07-27T16:51:17.130134: step 3311, loss 0.035866, acc 1
2016-07-27T16:51:19.878202: step 3312, loss 0.040805, acc 1
2016-07-27T16:51:22.481568: step 3313, loss 0.103155, acc 0.953125
2016-07-27T16:51:26.689869: step 3314, loss 0.0169916, acc 1
2016-07-27T16:51:29.388711: step 3315, loss 0.0566854, acc 0.96875
2016-07-27T16:51:32.694686: step 3316, loss 0.0892927, acc 0.96875
2016-07-27T16:51:35.279256: step 3317, loss 0.0636124, acc 0.96875
2016-07-27T16:51:38.343771: step 3318, loss 0.0488613, acc 0.984375
2016-07-27T16:51:41.766147: step 3319, loss 0.0386001, acc 0.984375
2016-07-27T16:51:44.442007: step 3320, loss 0.0845068, acc 0.953125
2016-07-27T16:51:47.397212: step 3321, loss 0.0457366, acc 0.96875
2016-07-27T16:51:50.613614: step 3322, loss 0.0226501, acc 1
2016-07-27T16:51:53.428909: step 3323, loss 0.0345471, acc 0.984375
2016-07-27T16:51:56.231431: step 3324, loss 0.067282, acc 0.96875
2016-07-27T16:51:59.100543: step 3325, loss 0.0537753, acc 0.984375
2016-07-27T16:52:01.767847: step 3326, loss 0.0826948, acc 0.984375
2016-07-27T16:52:04.503668: step 3327, loss 0.0685479, acc 0.984375
2016-07-27T16:52:06.998005: step 3328, loss 0.0675399, acc 0.953125
2016-07-27T16:52:09.818229: step 3329, loss 0.0497259, acc 1
2016-07-27T16:52:12.579007: step 3330, loss 0.135415, acc 0.953125
2016-07-27T16:52:15.509981: step 3331, loss 0.0319644, acc 1
2016-07-27T16:52:18.290632: step 3332, loss 0.0639085, acc 0.984375
2016-07-27T16:52:21.073007: step 3333, loss 0.0762161, acc 0.9375
2016-07-27T16:52:23.734348: step 3334, loss 0.0212619, acc 0.984375
2016-07-27T16:52:26.498343: step 3335, loss 0.0540123, acc 0.984375
2016-07-27T16:52:29.601326: step 3336, loss 0.0333221, acc 1
2016-07-27T16:52:32.148788: step 3337, loss 0.0377467, acc 0.984375
2016-07-27T16:52:35.167877: step 3338, loss 0.0531778, acc 0.96875
2016-07-27T16:52:37.868928: step 3339, loss 0.0679838, acc 0.96875
2016-07-27T16:52:40.749817: step 3340, loss 0.0319659, acc 1
2016-07-27T16:52:43.492597: step 3341, loss 0.0250945, acc 1
2016-07-27T16:52:46.194076: step 3342, loss 0.0366308, acc 1
2016-07-27T16:52:48.906901: step 3343, loss 0.0148004, acc 0.984375
2016-07-27T16:52:51.453953: step 3344, loss 0.0835387, acc 0.96875
2016-07-27T16:52:54.289718: step 3345, loss 0.0510907, acc 0.984375
2016-07-27T16:52:56.857401: step 3346, loss 0.0501472, acc 0.96875
2016-07-27T16:52:59.462806: step 3347, loss 0.0934864, acc 0.96875
2016-07-27T16:53:02.309919: step 3348, loss 0.0511244, acc 0.96875
2016-07-27T16:53:04.867602: step 3349, loss 0.0362574, acc 0.984375
2016-07-27T16:53:07.428686: step 3350, loss 0.0367125, acc 0.984375
2016-07-27T16:53:09.891387: step 3351, loss 0.0333499, acc 1
2016-07-27T16:53:12.587311: step 3352, loss 0.0362843, acc 0.984375
2016-07-27T16:53:15.072693: step 3353, loss 0.0598264, acc 0.96875
2016-07-27T16:53:17.687642: step 3354, loss 0.0506645, acc 1
2016-07-27T16:53:20.444996: step 3355, loss 0.0286593, acc 1
2016-07-27T16:53:23.147247: step 3356, loss 0.0247511, acc 1
2016-07-27T16:53:26.613020: step 3357, loss 0.058372, acc 0.96875
2016-07-27T16:53:29.482365: step 3358, loss 0.0343722, acc 1
2016-07-27T16:53:31.953824: step 3359, loss 0.0604103, acc 0.984375
2016-07-27T16:53:34.748129: step 3360, loss 0.0392406, acc 0.984375
2016-07-27T16:53:37.398467: step 3361, loss 0.157468, acc 0.984375
2016-07-27T16:53:39.833618: step 3362, loss 0.0470432, acc 0.984375
2016-07-27T16:53:43.073430: step 3363, loss 0.0142757, acc 1
2016-07-27T16:53:46.228777: step 3364, loss 0.0154857, acc 1
2016-07-27T16:53:49.037665: step 3365, loss 0.0529837, acc 0.96875
2016-07-27T16:53:51.656256: step 3366, loss 0.112971, acc 0.96875
2016-07-27T16:53:54.498265: step 3367, loss 0.0469019, acc 0.96875
2016-07-27T16:53:57.142130: step 3368, loss 0.0187089, acc 1
2016-07-27T16:54:00.312971: step 3369, loss 0.0424064, acc 1
2016-07-27T16:54:03.118861: step 3370, loss 0.0757784, acc 0.984375
2016-07-27T16:54:05.831499: step 3371, loss 0.0473083, acc 0.984375
2016-07-27T16:54:09.284645: step 3372, loss 0.0841001, acc 0.96875
2016-07-27T16:54:12.093760: step 3373, loss 0.0439053, acc 1
2016-07-27T16:54:14.767938: step 3374, loss 0.0678045, acc 0.96875
2016-07-27T16:54:17.356192: step 3375, loss 0.0731256, acc 0.984375
2016-07-27T16:54:20.117956: step 3376, loss 0.0213594, acc 1
2016-07-27T16:54:22.584837: step 3377, loss 0.0550995, acc 0.96875
2016-07-27T16:54:25.102345: step 3378, loss 0.053154, acc 0.984375
2016-07-27T16:54:27.854511: step 3379, loss 0.059444, acc 0.984375
2016-07-27T16:54:30.403733: step 3380, loss 0.0270134, acc 1
2016-07-27T16:54:33.096445: step 3381, loss 0.0446909, acc 0.984375
2016-07-27T16:54:35.745556: step 3382, loss 0.0980884, acc 0.921875
2016-07-27T16:54:38.341487: step 3383, loss 0.0471992, acc 0.984375
2016-07-27T16:54:41.017323: step 3384, loss 0.0573182, acc 0.984375
2016-07-27T16:54:43.602530: step 3385, loss 0.0510045, acc 0.96875
2016-07-27T16:54:46.394720: step 3386, loss 0.0390282, acc 0.96875
2016-07-27T16:54:49.387598: step 3387, loss 0.0606984, acc 0.984375
2016-07-27T16:54:52.166340: step 3388, loss 0.0194102, acc 1
2016-07-27T16:54:55.049469: step 3389, loss 0.0507777, acc 0.953125
2016-07-27T16:54:57.584338: step 3390, loss 0.108284, acc 0.984375
2016-07-27T16:55:00.500549: step 3391, loss 0.0347266, acc 1
2016-07-27T16:55:04.120306: step 3392, loss 0.0584171, acc 0.984375
2016-07-27T16:55:06.867158: step 3393, loss 0.0573976, acc 0.984375
2016-07-27T16:55:09.650526: step 3394, loss 0.0593448, acc 0.984375
2016-07-27T16:55:12.245089: step 3395, loss 0.0233783, acc 0.984375
2016-07-27T16:55:15.194313: step 3396, loss 0.0397332, acc 1
2016-07-27T16:55:17.950278: step 3397, loss 0.123238, acc 0.953125
2016-07-27T16:55:20.505852: step 3398, loss 0.0665831, acc 0.984375
2016-07-27T16:55:23.633362: step 3399, loss 0.0557262, acc 0.96875
2016-07-27T16:55:26.468253: step 3400, loss 0.0438001, acc 0.984375

Evaluation:
2016-07-27T16:55:38.428517: step 3400, loss 1.60371, acc 0.702

Saved model checkpoint to /home/chaitanya/lstm-context-embeddings/runs/1469600026/checkpoints/model-3400

2016-07-27T16:55:42.685229: step 3401, loss 0.0490756, acc 0.96875
2016-07-27T16:55:45.254169: step 3402, loss 0.0364587, acc 0.984375
2016-07-27T16:55:47.974672: step 3403, loss 0.0524934, acc 0.984375
2016-07-27T16:55:50.568948: step 3404, loss 0.0179576, acc 1
2016-07-27T16:55:53.293361: step 3405, loss 0.025025, acc 1
2016-07-27T16:55:55.992330: step 3406, loss 0.0687217, acc 0.984375
2016-07-27T16:55:58.677721: step 3407, loss 0.0467949, acc 0.984375
2016-07-27T16:56:01.433622: step 3408, loss 0.0331929, acc 0.984375
2016-07-27T16:56:03.902836: step 3409, loss 0.042579, acc 0.984375
2016-07-27T16:56:06.620830: step 3410, loss 0.0557317, acc 0.984375
2016-07-27T16:56:09.269145: step 3411, loss 0.0202612, acc 0.984375
2016-07-27T16:56:11.739576: step 3412, loss 0.0523249, acc 0.984375
2016-07-27T16:56:14.487029: step 3413, loss 0.0584783, acc 0.984375
2016-07-27T16:56:17.119948: step 3414, loss 0.0817194, acc 0.9375
2016-07-27T16:56:20.006056: step 3415, loss 0.0150147, acc 1
2016-07-27T16:56:22.536622: step 3416, loss 0.0933836, acc 0.9375
2016-07-27T16:56:25.184850: step 3417, loss 0.037161, acc 1
2016-07-27T16:56:27.862720: step 3418, loss 0.0292168, acc 0.984375
2016-07-27T16:56:30.434007: step 3419, loss 0.0408342, acc 1
2016-07-27T16:56:33.360653: step 3420, loss 0.0482844, acc 1
2016-07-27T16:56:36.025631: step 3421, loss 0.0771474, acc 0.953125
2016-07-27T16:56:38.970701: step 3422, loss 0.0789091, acc 0.96875
2016-07-27T16:56:41.619696: step 3423, loss 0.0522832, acc 0.96875
2016-07-27T16:56:44.424698: step 3424, loss 0.0635242, acc 0.984375
2016-07-27T16:56:47.164075: step 3425, loss 0.0562388, acc 0.96875
2016-07-27T16:56:49.773718: step 3426, loss 0.00706722, acc 1
2016-07-27T16:56:52.496028: step 3427, loss 0.0859266, acc 0.953125
2016-07-27T16:56:54.977665: step 3428, loss 0.0566486, acc 0.96875
2016-07-27T16:56:57.679468: step 3429, loss 0.0776184, acc 0.984375
2016-07-27T16:57:00.326728: step 3430, loss 0.0119528, acc 1
2016-07-27T16:57:02.737778: step 3431, loss 0.0418124, acc 1
2016-07-27T16:57:05.368633: step 3432, loss 0.102358, acc 0.96875
2016-07-27T16:57:07.862336: step 3433, loss 0.088551, acc 0.96875
2016-07-27T16:57:10.607858: step 3434, loss 0.0664717, acc 0.96875
2016-07-27T16:57:13.258902: step 3435, loss 0.0173874, acc 1
2016-07-27T16:57:15.845225: step 3436, loss 0.0992317, acc 0.96875
2016-07-27T16:57:18.549920: step 3437, loss 0.052241, acc 0.984375
2016-07-27T16:57:21.029564: step 3438, loss 0.0456975, acc 0.984375
2016-07-27T16:57:23.752250: step 3439, loss 0.0507259, acc 0.984375
2016-07-27T16:57:26.367054: step 3440, loss 0.131314, acc 0.9375
2016-07-27T16:57:28.934194: step 3441, loss 0.0808146, acc 0.96875
2016-07-27T16:57:31.684717: step 3442, loss 0.0821907, acc 0.984375
2016-07-27T16:57:34.098078: step 3443, loss 0.0695132, acc 0.984375
2016-07-27T16:57:36.868423: step 3444, loss 0.0884075, acc 0.984375
2016-07-27T16:57:39.525602: step 3445, loss 0.0492358, acc 0.984375
2016-07-27T16:57:41.917928: step 3446, loss 0.0334715, acc 0.984375
2016-07-27T16:57:44.649660: step 3447, loss 0.0296342, acc 1
2016-07-27T16:57:47.177175: step 3448, loss 0.119625, acc 0.984375
2016-07-27T16:57:49.883183: step 3449, loss 0.0670045, acc 0.96875
2016-07-27T16:57:52.497310: step 3450, loss 0.0503513, acc 1
2016-07-27T16:57:54.903896: step 3451, loss 0.0690221, acc 0.9375
2016-07-27T16:57:57.589248: step 3452, loss 0.059948, acc 0.953125
2016-07-27T16:58:00.080944: step 3453, loss 0.0260143, acc 0.984375
2016-07-27T16:58:02.740163: step 3454, loss 0.0309936, acc 1
2016-07-27T16:58:05.355410: step 3455, loss 0.110235, acc 0.953125
2016-07-27T16:58:08.159121: step 3456, loss 0.0501942, acc 1
2016-07-27T16:58:10.891191: step 3457, loss 0.0449116, acc 0.984375
2016-07-27T16:58:13.357189: step 3458, loss 0.0191013, acc 0.984375
2016-07-27T16:58:16.112544: step 3459, loss 0.0417201, acc 0.984375
2016-07-27T16:58:18.719482: step 3460, loss 0.0359227, acc 0.984375
2016-07-27T16:58:21.228850: step 3461, loss 0.0163051, acc 0.984375
2016-07-27T16:58:24.011872: step 3462, loss 0.101261, acc 0.921875
2016-07-27T16:58:26.619660: step 3463, loss 0.0378595, acc 0.984375
2016-07-27T16:58:29.045620: step 3464, loss 0.0535263, acc 0.96875
2016-07-27T16:58:31.796105: step 3465, loss 0.0196974, acc 1
2016-07-27T16:58:34.319680: step 3466, loss 0.041588, acc 0.96875
2016-07-27T16:58:37.041993: step 3467, loss 0.0610935, acc 0.984375
2016-07-27T16:58:39.660081: step 3468, loss 0.0692748, acc 0.953125
2016-07-27T16:58:42.131307: step 3469, loss 0.0197644, acc 1
2016-07-27T16:58:44.854507: step 3470, loss 0.0391121, acc 0.984375
2016-07-27T16:58:47.378421: step 3471, loss 0.0518385, acc 0.984375
2016-07-27T16:58:50.077831: step 3472, loss 0.0979324, acc 0.921875
2016-07-27T16:58:52.689620: step 3473, loss 0.0287867, acc 0.983871
2016-07-27T16:58:55.101379: step 3474, loss 0.03122, acc 0.984375
2016-07-27T16:58:57.769586: step 3475, loss 0.077405, acc 0.953125
2016-07-27T16:59:00.311921: step 3476, loss 0.0356289, acc 1
2016-07-27T16:59:03.008356: step 3477, loss 0.0351467, acc 1
2016-07-27T16:59:05.625308: step 3478, loss 0.0516602, acc 1
2016-07-27T16:59:08.021337: step 3479, loss 0.0486962, acc 0.96875
2016-07-27T16:59:10.758412: step 3480, loss 0.0624288, acc 0.96875
2016-07-27T16:59:13.318542: step 3481, loss 0.0291439, acc 1
2016-07-27T16:59:16.031385: step 3482, loss 0.0614329, acc 0.984375
2016-07-27T16:59:18.637352: step 3483, loss 0.0401973, acc 0.96875
2016-07-27T16:59:21.282041: step 3484, loss 0.0761357, acc 0.96875
2016-07-27T16:59:24.003634: step 3485, loss 0.0183168, acc 1
2016-07-27T16:59:26.462112: step 3486, loss 0.0339527, acc 1
2016-07-27T16:59:29.190773: step 3487, loss 0.046097, acc 0.984375
2016-07-27T16:59:31.795866: step 3488, loss 0.0979597, acc 0.96875
2016-07-27T16:59:34.249699: step 3489, loss 0.0267304, acc 1
2016-07-27T16:59:36.951453: step 3490, loss 0.0374501, acc 0.984375
2016-07-27T16:59:39.431965: step 3491, loss 0.0565545, acc 0.984375
2016-07-27T16:59:42.186113: step 3492, loss 0.0392217, acc 1
2016-07-27T16:59:44.882601: step 3493, loss 0.0706446, acc 0.984375
2016-07-27T16:59:47.274294: step 3494, loss 0.0172136, acc 1
2016-07-27T16:59:50.013020: step 3495, loss 0.0153252, acc 1
2016-07-27T16:59:52.677621: step 3496, loss 0.0340462, acc 1
2016-07-27T16:59:55.073347: step 3497, loss 0.0476661, acc 0.984375
2016-07-27T16:59:57.784870: step 3498, loss 0.132354, acc 0.9375
2016-07-27T17:00:00.296710: step 3499, loss 0.0755775, acc 0.96875
2016-07-27T17:00:03.045870: step 3500, loss 0.0439971, acc 0.984375

Evaluation:
2016-07-27T17:00:14.956918: step 3500, loss 1.66394, acc 0.683

Saved model checkpoint to /home/chaitanya/lstm-context-embeddings/runs/1469600026/checkpoints/model-3500

2016-07-27T17:00:19.064822: step 3501, loss 0.0620155, acc 0.96875
2016-07-27T17:00:21.771969: step 3502, loss 0.0780065, acc 0.9375
2016-07-27T17:00:24.459124: step 3503, loss 0.0135507, acc 1
2016-07-27T17:00:26.773311: step 3504, loss 0.014085, acc 1
2016-07-27T17:00:29.523236: step 3505, loss 0.030456, acc 1
2016-07-27T17:00:32.125888: step 3506, loss 0.0444874, acc 0.984375
2016-07-27T17:00:34.758935: step 3507, loss 0.0190731, acc 1
2016-07-27T17:00:37.439240: step 3508, loss 0.0332057, acc 0.984375
2016-07-27T17:00:39.762626: step 3509, loss 0.13074, acc 0.953125
2016-07-27T17:00:42.519710: step 3510, loss 0.0838712, acc 0.96875
2016-07-27T17:00:45.072298: step 3511, loss 0.0297925, acc 0.984375
2016-07-27T17:00:47.504269: step 3512, loss 0.0624917, acc 0.953125
2016-07-27T17:00:50.061359: step 3513, loss 0.0373755, acc 0.984375
2016-07-27T17:00:52.658184: step 3514, loss 0.0283306, acc 0.984375
2016-07-27T17:00:55.385256: step 3515, loss 0.072749, acc 0.96875
2016-07-27T17:00:58.128188: step 3516, loss 0.0308899, acc 1
2016-07-27T17:01:00.585605: step 3517, loss 0.051747, acc 0.96875
2016-07-27T17:01:03.300021: step 3518, loss 0.0499347, acc 0.96875
2016-07-27T17:01:05.797325: step 3519, loss 0.0405512, acc 0.984375
2016-07-27T17:01:08.348147: step 3520, loss 0.0414889, acc 0.984375
2016-07-27T17:01:10.839179: step 3521, loss 0.0642831, acc 0.984375
2016-07-27T17:01:13.545724: step 3522, loss 0.0428958, acc 0.96875
2016-07-27T17:01:16.121996: step 3523, loss 0.0592819, acc 0.96875
2016-07-27T17:01:18.569354: step 3524, loss 0.0380961, acc 0.984375
2016-07-27T17:01:21.271716: step 3525, loss 0.0362167, acc 1
2016-07-27T17:01:23.788914: step 3526, loss 0.048559, acc 0.96875
2016-07-27T17:01:26.399143: step 3527, loss 0.0418785, acc 0.984375
2016-07-27T17:01:28.910803: step 3528, loss 0.090799, acc 0.953125
2016-07-27T17:01:31.706765: step 3529, loss 0.0287204, acc 0.984375
2016-07-27T17:01:34.274559: step 3530, loss 0.0888747, acc 0.984375
2016-07-27T17:01:36.981851: step 3531, loss 0.0393302, acc 0.96875
2016-07-27T17:01:39.656203: step 3532, loss 0.0641915, acc 0.984375
2016-07-27T17:01:42.232515: step 3533, loss 0.0587491, acc 0.96875
2016-07-27T17:01:44.980994: step 3534, loss 0.0204457, acc 1
2016-07-27T17:01:47.425451: step 3535, loss 0.0671391, acc 0.953125
2016-07-27T17:01:50.197981: step 3536, loss 0.0399168, acc 0.984375
2016-07-27T17:01:52.900510: step 3537, loss 0.0671174, acc 0.96875
2016-07-27T17:01:55.322588: step 3538, loss 0.0475724, acc 1
2016-07-27T17:01:58.016519: step 3539, loss 0.0595765, acc 0.96875
2016-07-27T17:02:00.525213: step 3540, loss 0.0599644, acc 0.984375
2016-07-27T17:02:03.264412: step 3541, loss 0.0317422, acc 0.984375
2016-07-27T17:02:05.898128: step 3542, loss 0.0583824, acc 0.984375
2016-07-27T17:02:08.332506: step 3543, loss 0.121638, acc 0.953125
2016-07-27T17:02:10.821308: step 3544, loss 0.0657536, acc 1
2016-07-27T17:02:14.254395: step 3545, loss 0.067353, acc 0.96875
2016-07-27T17:02:17.225895: step 3546, loss 0.0225739, acc 1
2016-07-27T17:02:19.951845: step 3547, loss 0.0630054, acc 0.96875
2016-07-27T17:02:22.337084: step 3548, loss 0.0351028, acc 1
2016-07-27T17:02:24.773691: step 3549, loss 0.0904197, acc 0.953125
2016-07-27T17:02:27.136820: step 3550, loss 0.0544551, acc 1
2016-07-27T17:02:29.868479: step 3551, loss 0.0568993, acc 0.984375
2016-07-27T17:02:32.487186: step 3552, loss 0.031215, acc 1
2016-07-27T17:02:34.991038: step 3553, loss 0.056781, acc 0.96875
2016-07-27T17:02:37.665671: step 3554, loss 0.108302, acc 0.984375
2016-07-27T17:02:40.109733: step 3555, loss 0.0263025, acc 1
2016-07-27T17:02:42.665414: step 3556, loss 0.054621, acc 0.984375
2016-07-27T17:02:45.441179: step 3557, loss 0.0621172, acc 0.953125
2016-07-27T17:02:48.029764: step 3558, loss 0.042884, acc 0.984375
2016-07-27T17:02:50.592574: step 3559, loss 0.0567898, acc 0.984375
2016-07-27T17:02:53.317329: step 3560, loss 0.0444908, acc 1
2016-07-27T17:02:55.709640: step 3561, loss 0.0561783, acc 0.96875
2016-07-27T17:02:58.180534: step 3562, loss 0.0623956, acc 0.96875
2016-07-27T17:03:00.905280: step 3563, loss 0.0351026, acc 1
2016-07-27T17:03:03.448196: step 3564, loss 0.0384493, acc 0.984375
2016-07-27T17:03:05.903366: step 3565, loss 0.0811103, acc 0.953125
2016-07-27T17:03:08.626144: step 3566, loss 0.0733405, acc 0.96875
2016-07-27T17:03:11.110326: step 3567, loss 0.0995916, acc 0.96875
2016-07-27T17:03:13.655911: step 3568, loss 0.0421151, acc 0.96875
2016-07-27T17:03:16.415655: step 3569, loss 0.0370968, acc 0.96875
2016-07-27T17:03:19.091891: step 3570, loss 0.0568617, acc 0.984375
2016-07-27T17:03:21.474126: step 3571, loss 0.0651103, acc 0.96875
2016-07-27T17:03:24.251577: step 3572, loss 0.0741525, acc 0.953125
2016-07-27T17:03:26.801139: step 3573, loss 0.0425474, acc 0.984375
2016-07-27T17:03:29.541151: step 3574, loss 0.0382983, acc 0.984375
2016-07-27T17:03:32.184964: step 3575, loss 0.0573349, acc 0.96875
2
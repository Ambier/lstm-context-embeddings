WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f2e1f944710>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.
WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f2e1f9447d0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.

Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=50
CHECKPOINT_EVERY=100
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=300
EVALUATE_EVERY=100
FILTER_SIZES=3,4,5
HIDDEN_DIM=300
L2_REG_LAMBDA=0
LOG_DEVICE_PLACEMENT=False
NUM_EPOCHS=30
NUM_FILTERS=100
WORD2VEC=GoogleNews-vectors-negative300.bin

Loading data...
Vocabulary Size: 18758
Train/Dev split: 9662/1000
Writing to /home/cil/lstm-context-embeddings/runs/1473237087

Load word2vec file GoogleNews-vectors-negative300.bin

2016-09-07T16:31:46.718308: step 1, loss 0.693147, acc 0.44
2016-09-07T16:31:47.394738: step 2, loss 0.715837, acc 0.44
2016-09-07T16:31:48.090775: step 3, loss 0.700977, acc 0.4
2016-09-07T16:31:48.789307: step 4, loss 0.685589, acc 0.54
2016-09-07T16:31:49.483754: step 5, loss 0.664431, acc 0.64
2016-09-07T16:31:50.160819: step 6, loss 0.693013, acc 0.56
2016-09-07T16:31:50.857819: step 7, loss 0.751501, acc 0.46
2016-09-07T16:31:51.533647: step 8, loss 0.75942, acc 0.42
2016-09-07T16:31:52.365876: step 9, loss 0.684698, acc 0.6
2016-09-07T16:31:53.280413: step 10, loss 0.695854, acc 0.5
2016-09-07T16:31:54.025943: step 11, loss 0.693774, acc 0.52
2016-09-07T16:31:54.740550: step 12, loss 0.703, acc 0.46
2016-09-07T16:31:55.531714: step 13, loss 0.690964, acc 0.56
2016-09-07T16:31:56.334934: step 14, loss 0.685738, acc 0.54
2016-09-07T16:31:57.172188: step 15, loss 0.681963, acc 0.54
2016-09-07T16:31:57.971717: step 16, loss 0.694408, acc 0.52
2016-09-07T16:31:58.810787: step 17, loss 0.687224, acc 0.5
2016-09-07T16:31:59.514360: step 18, loss 0.720647, acc 0.38
2016-09-07T16:32:00.340239: step 19, loss 0.690365, acc 0.56
2016-09-07T16:32:01.031181: step 20, loss 0.672867, acc 0.6
2016-09-07T16:32:01.709928: step 21, loss 0.675763, acc 0.56
2016-09-07T16:32:02.447676: step 22, loss 0.694954, acc 0.56
2016-09-07T16:32:03.134698: step 23, loss 0.70257, acc 0.52
2016-09-07T16:32:03.880364: step 24, loss 0.704913, acc 0.48
2016-09-07T16:32:04.619947: step 25, loss 0.681673, acc 0.52
2016-09-07T16:32:05.366018: step 26, loss 0.649397, acc 0.6
2016-09-07T16:32:06.170583: step 27, loss 0.656208, acc 0.62
2016-09-07T16:32:06.930065: step 28, loss 0.668989, acc 0.52
2016-09-07T16:32:07.654522: step 29, loss 0.660736, acc 0.52
2016-09-07T16:32:08.409354: step 30, loss 0.714112, acc 0.52
2016-09-07T16:32:09.189771: step 31, loss 0.591963, acc 0.68
2016-09-07T16:32:10.021577: step 32, loss 0.652451, acc 0.56
2016-09-07T16:32:10.738540: step 33, loss 0.652408, acc 0.62
2016-09-07T16:32:11.531284: step 34, loss 0.663438, acc 0.66
2016-09-07T16:32:12.350131: step 35, loss 0.5742, acc 0.66
2016-09-07T16:32:13.095631: step 36, loss 0.608849, acc 0.74
2016-09-07T16:32:13.794903: step 37, loss 0.684286, acc 0.64
2016-09-07T16:32:14.529851: step 38, loss 0.657265, acc 0.62
2016-09-07T16:32:15.252336: step 39, loss 0.546291, acc 0.76
2016-09-07T16:32:15.963541: step 40, loss 0.578165, acc 0.66
2016-09-07T16:32:16.718738: step 41, loss 0.572656, acc 0.66
2016-09-07T16:32:17.374901: step 42, loss 0.57249, acc 0.62
2016-09-07T16:32:18.036533: step 43, loss 0.58589, acc 0.7
2016-09-07T16:32:18.727991: step 44, loss 0.560574, acc 0.72
2016-09-07T16:32:19.432195: step 45, loss 0.646048, acc 0.62
2016-09-07T16:32:20.110507: step 46, loss 0.571342, acc 0.68
2016-09-07T16:32:20.824079: step 47, loss 0.478498, acc 0.82
2016-09-07T16:32:21.692086: step 48, loss 0.607271, acc 0.74
2016-09-07T16:32:22.445920: step 49, loss 0.63603, acc 0.72
2016-09-07T16:32:23.271092: step 50, loss 0.774592, acc 0.56
2016-09-07T16:32:24.000708: step 51, loss 0.703827, acc 0.72
2016-09-07T16:32:24.804542: step 52, loss 0.53628, acc 0.76
2016-09-07T16:32:25.630227: step 53, loss 0.629699, acc 0.62
2016-09-07T16:32:26.463976: step 54, loss 0.668435, acc 0.62
2016-09-07T16:32:27.144795: step 55, loss 0.632459, acc 0.66
2016-09-07T16:32:27.855460: step 56, loss 0.534072, acc 0.8
2016-09-07T16:32:28.701530: step 57, loss 0.608512, acc 0.7
2016-09-07T16:32:29.534917: step 58, loss 0.641094, acc 0.66
2016-09-07T16:32:30.290582: step 59, loss 0.599806, acc 0.68
2016-09-07T16:32:30.938323: step 60, loss 0.632723, acc 0.7
2016-09-07T16:32:31.661746: step 61, loss 0.648755, acc 0.58
2016-09-07T16:32:32.434040: step 62, loss 0.581271, acc 0.64
2016-09-07T16:32:33.207649: step 63, loss 0.593713, acc 0.72
2016-09-07T16:32:33.904051: step 64, loss 0.547411, acc 0.74
2016-09-07T16:32:34.588592: step 65, loss 0.478613, acc 0.8
2016-09-07T16:32:35.287940: step 66, loss 0.585492, acc 0.66
2016-09-07T16:32:35.988342: step 67, loss 0.559569, acc 0.76
2016-09-07T16:32:36.660120: step 68, loss 0.650721, acc 0.64
2016-09-07T16:32:37.339489: step 69, loss 0.488781, acc 0.74
2016-09-07T16:32:38.133745: step 70, loss 0.571411, acc 0.72
2016-09-07T16:32:38.894054: step 71, loss 0.580112, acc 0.72
2016-09-07T16:32:39.649105: step 72, loss 0.508671, acc 0.7
2016-09-07T16:32:40.460566: step 73, loss 0.450462, acc 0.84
2016-09-07T16:32:41.231583: step 74, loss 0.478204, acc 0.78
2016-09-07T16:32:41.974167: step 75, loss 0.544987, acc 0.76
2016-09-07T16:32:42.738438: step 76, loss 0.637247, acc 0.68
2016-09-07T16:32:43.389326: step 77, loss 0.497721, acc 0.78
2016-09-07T16:32:44.065395: step 78, loss 0.585467, acc 0.72
2016-09-07T16:32:44.739408: step 79, loss 0.491918, acc 0.72
2016-09-07T16:32:45.492317: step 80, loss 0.678174, acc 0.64
2016-09-07T16:32:46.213581: step 81, loss 0.48549, acc 0.72
2016-09-07T16:32:46.883186: step 82, loss 0.437981, acc 0.74
2016-09-07T16:32:47.545052: step 83, loss 0.538503, acc 0.78
2016-09-07T16:32:48.209948: step 84, loss 0.487267, acc 0.78
2016-09-07T16:32:48.890109: step 85, loss 0.513312, acc 0.76
2016-09-07T16:32:49.559103: step 86, loss 0.442034, acc 0.78
2016-09-07T16:32:50.236473: step 87, loss 0.611144, acc 0.66
2016-09-07T16:32:50.911583: step 88, loss 0.391934, acc 0.88
2016-09-07T16:32:51.577540: step 89, loss 0.407068, acc 0.82
2016-09-07T16:32:52.240880: step 90, loss 0.601504, acc 0.66
2016-09-07T16:32:52.898495: step 91, loss 0.496954, acc 0.74
2016-09-07T16:32:53.576811: step 92, loss 0.624445, acc 0.72
2016-09-07T16:32:54.242694: step 93, loss 0.572553, acc 0.64
2016-09-07T16:32:54.900679: step 94, loss 0.468107, acc 0.76
2016-09-07T16:32:55.568691: step 95, loss 0.410143, acc 0.78
2016-09-07T16:32:56.243289: step 96, loss 0.438503, acc 0.78
2016-09-07T16:32:56.903933: step 97, loss 0.604211, acc 0.72
2016-09-07T16:32:57.567491: step 98, loss 0.47228, acc 0.76
2016-09-07T16:32:58.231975: step 99, loss 0.604293, acc 0.66
2016-09-07T16:32:58.917202: step 100, loss 0.483287, acc 0.72

Evaluation:
2016-09-07T16:33:01.885498: step 100, loss 0.501049, acc 0.76

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-100

2016-09-07T16:33:03.677758: step 101, loss 0.59046, acc 0.7
2016-09-07T16:33:04.472156: step 102, loss 0.75174, acc 0.64
2016-09-07T16:33:05.283286: step 103, loss 0.470936, acc 0.78
2016-09-07T16:33:05.955050: step 104, loss 0.480786, acc 0.76
2016-09-07T16:33:06.680601: step 105, loss 0.478135, acc 0.76
2016-09-07T16:33:07.535786: step 106, loss 0.438855, acc 0.8
2016-09-07T16:33:08.317966: step 107, loss 0.492555, acc 0.78
2016-09-07T16:33:08.980369: step 108, loss 0.435213, acc 0.78
2016-09-07T16:33:09.743255: step 109, loss 0.453134, acc 0.84
2016-09-07T16:33:10.417784: step 110, loss 0.466226, acc 0.82
2016-09-07T16:33:11.208545: step 111, loss 0.533982, acc 0.8
2016-09-07T16:33:11.916005: step 112, loss 0.468403, acc 0.74
2016-09-07T16:33:12.639108: step 113, loss 0.502138, acc 0.74
2016-09-07T16:33:13.331762: step 114, loss 0.449187, acc 0.82
2016-09-07T16:33:14.033425: step 115, loss 0.439309, acc 0.82
2016-09-07T16:33:14.776051: step 116, loss 0.38055, acc 0.8
2016-09-07T16:33:15.469742: step 117, loss 0.70007, acc 0.62
2016-09-07T16:33:16.182326: step 118, loss 0.482099, acc 0.76
2016-09-07T16:33:16.840126: step 119, loss 0.585636, acc 0.7
2016-09-07T16:33:17.504573: step 120, loss 0.386783, acc 0.86
2016-09-07T16:33:18.242319: step 121, loss 0.582643, acc 0.76
2016-09-07T16:33:18.918856: step 122, loss 0.420628, acc 0.8
2016-09-07T16:33:19.626718: step 123, loss 0.58869, acc 0.66
2016-09-07T16:33:20.348800: step 124, loss 0.544941, acc 0.76
2016-09-07T16:33:21.205327: step 125, loss 0.651797, acc 0.62
2016-09-07T16:33:21.874326: step 126, loss 0.630289, acc 0.64
2016-09-07T16:33:22.602815: step 127, loss 0.656029, acc 0.64
2016-09-07T16:33:23.409051: step 128, loss 0.378022, acc 0.82
2016-09-07T16:33:24.151773: step 129, loss 0.431033, acc 0.78
2016-09-07T16:33:24.850277: step 130, loss 0.476805, acc 0.7
2016-09-07T16:33:25.573239: step 131, loss 0.454874, acc 0.82
2016-09-07T16:33:26.292705: step 132, loss 0.490497, acc 0.82
2016-09-07T16:33:27.025431: step 133, loss 0.526482, acc 0.68
2016-09-07T16:33:27.826426: step 134, loss 0.430812, acc 0.84
2016-09-07T16:33:28.642668: step 135, loss 0.560984, acc 0.72
2016-09-07T16:33:29.343097: step 136, loss 0.496779, acc 0.78
2016-09-07T16:33:30.018128: step 137, loss 0.502544, acc 0.7
2016-09-07T16:33:30.716585: step 138, loss 0.517937, acc 0.72
2016-09-07T16:33:31.376192: step 139, loss 0.472587, acc 0.76
2016-09-07T16:33:32.040394: step 140, loss 0.392937, acc 0.82
2016-09-07T16:33:32.766809: step 141, loss 0.574537, acc 0.7
2016-09-07T16:33:33.505319: step 142, loss 0.516281, acc 0.7
2016-09-07T16:33:34.214244: step 143, loss 0.57772, acc 0.7
2016-09-07T16:33:34.969417: step 144, loss 0.506364, acc 0.8
2016-09-07T16:33:35.718566: step 145, loss 0.475477, acc 0.76
2016-09-07T16:33:36.477993: step 146, loss 0.556798, acc 0.76
2016-09-07T16:33:37.170397: step 147, loss 0.542118, acc 0.7
2016-09-07T16:33:37.897526: step 148, loss 0.405258, acc 0.84
2016-09-07T16:33:38.598799: step 149, loss 0.504539, acc 0.72
2016-09-07T16:33:39.306432: step 150, loss 0.550984, acc 0.76
2016-09-07T16:33:40.007264: step 151, loss 0.559456, acc 0.66
2016-09-07T16:33:40.685343: step 152, loss 0.49592, acc 0.74
2016-09-07T16:33:41.357453: step 153, loss 0.560978, acc 0.72
2016-09-07T16:33:42.046570: step 154, loss 0.604108, acc 0.66
2016-09-07T16:33:42.818480: step 155, loss 0.459536, acc 0.72
2016-09-07T16:33:43.564289: step 156, loss 0.396596, acc 0.86
2016-09-07T16:33:44.325807: step 157, loss 0.470313, acc 0.72
2016-09-07T16:33:45.026048: step 158, loss 0.611565, acc 0.66
2016-09-07T16:33:45.748071: step 159, loss 0.501037, acc 0.74
2016-09-07T16:33:46.498116: step 160, loss 0.539548, acc 0.7
2016-09-07T16:33:47.181747: step 161, loss 0.585322, acc 0.72
2016-09-07T16:33:47.844472: step 162, loss 0.600838, acc 0.66
2016-09-07T16:33:48.519612: step 163, loss 0.414237, acc 0.76
2016-09-07T16:33:49.181070: step 164, loss 0.410338, acc 0.76
2016-09-07T16:33:49.857906: step 165, loss 0.464635, acc 0.76
2016-09-07T16:33:50.540718: step 166, loss 0.47522, acc 0.74
2016-09-07T16:33:51.214195: step 167, loss 0.527376, acc 0.76
2016-09-07T16:33:51.865892: step 168, loss 0.563406, acc 0.68
2016-09-07T16:33:52.523655: step 169, loss 0.517046, acc 0.72
2016-09-07T16:33:53.197266: step 170, loss 0.562372, acc 0.7
2016-09-07T16:33:53.866482: step 171, loss 0.61373, acc 0.58
2016-09-07T16:33:54.529342: step 172, loss 0.49781, acc 0.78
2016-09-07T16:33:55.210488: step 173, loss 0.519249, acc 0.7
2016-09-07T16:33:55.871779: step 174, loss 0.493885, acc 0.7
2016-09-07T16:33:56.546608: step 175, loss 0.444703, acc 0.82
2016-09-07T16:33:57.214716: step 176, loss 0.44039, acc 0.76
2016-09-07T16:33:57.881542: step 177, loss 0.363972, acc 0.86
2016-09-07T16:33:58.618474: step 178, loss 0.481637, acc 0.76
2016-09-07T16:33:59.280702: step 179, loss 0.416509, acc 0.8
2016-09-07T16:33:59.963338: step 180, loss 0.291554, acc 0.84
2016-09-07T16:34:00.651584: step 181, loss 0.534239, acc 0.68
2016-09-07T16:34:01.383514: step 182, loss 0.496796, acc 0.7
2016-09-07T16:34:02.090409: step 183, loss 0.438169, acc 0.78
2016-09-07T16:34:02.810619: step 184, loss 0.58462, acc 0.64
2016-09-07T16:34:03.484095: step 185, loss 0.625577, acc 0.74
2016-09-07T16:34:04.212572: step 186, loss 0.452082, acc 0.78
2016-09-07T16:34:04.930585: step 187, loss 0.470295, acc 0.74
2016-09-07T16:34:05.638494: step 188, loss 0.454461, acc 0.74
2016-09-07T16:34:06.568822: step 189, loss 0.420544, acc 0.82
2016-09-07T16:34:07.424865: step 190, loss 0.482665, acc 0.82
2016-09-07T16:34:08.160623: step 191, loss 0.539688, acc 0.7
2016-09-07T16:34:09.055925: step 192, loss 0.485354, acc 0.66
2016-09-07T16:34:09.934600: step 193, loss 0.437825, acc 0.78
2016-09-07T16:34:10.307668: step 194, loss 0.260855, acc 0.916667
2016-09-07T16:34:11.140477: step 195, loss 0.405703, acc 0.82
2016-09-07T16:34:11.813768: step 196, loss 0.369963, acc 0.82
2016-09-07T16:34:12.538935: step 197, loss 0.309824, acc 0.86
2016-09-07T16:34:13.380111: step 198, loss 0.333688, acc 0.9
2016-09-07T16:34:14.044704: step 199, loss 0.327938, acc 0.84
2016-09-07T16:34:14.768722: step 200, loss 0.424155, acc 0.8

Evaluation:
2016-09-07T16:34:17.713436: step 200, loss 0.467334, acc 0.778

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-200

2016-09-07T16:34:19.359009: step 201, loss 0.416624, acc 0.76
2016-09-07T16:34:20.087976: step 202, loss 0.335957, acc 0.82
2016-09-07T16:34:20.739028: step 203, loss 0.291217, acc 0.84
2016-09-07T16:34:21.501072: step 204, loss 0.528087, acc 0.8
2016-09-07T16:34:22.294533: step 205, loss 0.330534, acc 0.88
2016-09-07T16:34:23.033612: step 206, loss 0.264514, acc 0.9
2016-09-07T16:34:23.775492: step 207, loss 0.620219, acc 0.7
2016-09-07T16:34:24.448895: step 208, loss 0.393683, acc 0.84
2016-09-07T16:34:25.121470: step 209, loss 0.277995, acc 0.94
2016-09-07T16:34:25.796613: step 210, loss 0.32381, acc 0.88
2016-09-07T16:34:26.457304: step 211, loss 0.234816, acc 0.94
2016-09-07T16:34:27.155789: step 212, loss 0.567655, acc 0.8
2016-09-07T16:34:27.812591: step 213, loss 0.247757, acc 0.9
2016-09-07T16:34:28.485080: step 214, loss 0.536215, acc 0.82
2016-09-07T16:34:29.170432: step 215, loss 0.222641, acc 0.94
2016-09-07T16:34:29.815393: step 216, loss 0.508633, acc 0.76
2016-09-07T16:34:30.475076: step 217, loss 0.354384, acc 0.84
2016-09-07T16:34:31.147058: step 218, loss 0.413233, acc 0.86
2016-09-07T16:34:31.806980: step 219, loss 0.317204, acc 0.88
2016-09-07T16:34:32.482234: step 220, loss 0.286305, acc 0.88
2016-09-07T16:34:33.153641: step 221, loss 0.246732, acc 0.9
2016-09-07T16:34:33.828459: step 222, loss 0.319376, acc 0.8
2016-09-07T16:34:34.495545: step 223, loss 0.415056, acc 0.82
2016-09-07T16:34:35.146330: step 224, loss 0.405029, acc 0.86
2016-09-07T16:34:35.812457: step 225, loss 0.473257, acc 0.8
2016-09-07T16:34:36.475325: step 226, loss 0.408278, acc 0.82
2016-09-07T16:34:37.141971: step 227, loss 0.222903, acc 0.94
2016-09-07T16:34:37.812303: step 228, loss 0.347915, acc 0.86
2016-09-07T16:34:38.478420: step 229, loss 0.258228, acc 0.88
2016-09-07T16:34:39.142415: step 230, loss 0.425935, acc 0.8
2016-09-07T16:34:39.812374: step 231, loss 0.339192, acc 0.88
2016-09-07T16:34:40.483439: step 232, loss 0.326624, acc 0.88
2016-09-07T16:34:41.141361: step 233, loss 0.440516, acc 0.78
2016-09-07T16:34:41.806471: step 234, loss 0.475526, acc 0.8
2016-09-07T16:34:42.476045: step 235, loss 0.260092, acc 0.94
2016-09-07T16:34:43.150258: step 236, loss 0.258714, acc 0.9
2016-09-07T16:34:43.814058: step 237, loss 0.432379, acc 0.8
2016-09-07T16:34:44.465651: step 238, loss 0.401042, acc 0.82
2016-09-07T16:34:45.129896: step 239, loss 0.530806, acc 0.78
2016-09-07T16:34:45.779153: step 240, loss 0.370433, acc 0.84
2016-09-07T16:34:46.444297: step 241, loss 0.267724, acc 0.92
2016-09-07T16:34:47.113453: step 242, loss 0.361074, acc 0.82
2016-09-07T16:34:47.787706: step 243, loss 0.464206, acc 0.84
2016-09-07T16:34:48.468899: step 244, loss 0.282553, acc 0.88
2016-09-07T16:34:49.222653: step 245, loss 0.23918, acc 0.88
2016-09-07T16:34:49.924945: step 246, loss 0.300726, acc 0.84
2016-09-07T16:34:50.643591: step 247, loss 0.303399, acc 0.9
2016-09-07T16:34:51.464963: step 248, loss 0.32501, acc 0.8
2016-09-07T16:34:52.188150: step 249, loss 0.3942, acc 0.76
2016-09-07T16:34:52.910940: step 250, loss 0.370828, acc 0.8
2016-09-07T16:34:53.621215: step 251, loss 0.567432, acc 0.76
2016-09-07T16:34:54.330149: step 252, loss 0.320518, acc 0.86
2016-09-07T16:34:54.989580: step 253, loss 0.39152, acc 0.82
2016-09-07T16:34:55.688743: step 254, loss 0.619849, acc 0.78
2016-09-07T16:34:56.394455: step 255, loss 0.407331, acc 0.8
2016-09-07T16:34:57.082857: step 256, loss 0.293799, acc 0.88
2016-09-07T16:34:57.813249: step 257, loss 0.322249, acc 0.9
2016-09-07T16:34:58.482953: step 258, loss 0.493697, acc 0.66
2016-09-07T16:34:59.145147: step 259, loss 0.466371, acc 0.72
2016-09-07T16:34:59.818186: step 260, loss 0.343381, acc 0.88
2016-09-07T16:35:00.498445: step 261, loss 0.292604, acc 0.88
2016-09-07T16:35:01.187517: step 262, loss 0.330265, acc 0.84
2016-09-07T16:35:01.859128: step 263, loss 0.437216, acc 0.8
2016-09-07T16:35:02.511667: step 264, loss 0.325085, acc 0.86
2016-09-07T16:35:03.188961: step 265, loss 0.358204, acc 0.84
2016-09-07T16:35:03.852150: step 266, loss 0.30366, acc 0.9
2016-09-07T16:35:04.575970: step 267, loss 0.204557, acc 0.92
2016-09-07T16:35:05.244253: step 268, loss 0.366516, acc 0.84
2016-09-07T16:35:05.914142: step 269, loss 0.237917, acc 0.92
2016-09-07T16:35:06.576719: step 270, loss 0.285764, acc 0.88
2016-09-07T16:35:07.231384: step 271, loss 0.369841, acc 0.8
2016-09-07T16:35:07.906543: step 272, loss 0.408537, acc 0.82
2016-09-07T16:35:08.586433: step 273, loss 0.336858, acc 0.86
2016-09-07T16:35:09.260011: step 274, loss 0.407634, acc 0.82
2016-09-07T16:35:09.925550: step 275, loss 0.332984, acc 0.8
2016-09-07T16:35:10.588873: step 276, loss 0.45155, acc 0.78
2016-09-07T16:35:11.258407: step 277, loss 0.376019, acc 0.8
2016-09-07T16:35:11.914520: step 278, loss 0.334342, acc 0.84
2016-09-07T16:35:12.574652: step 279, loss 0.439575, acc 0.82
2016-09-07T16:35:13.224007: step 280, loss 0.481241, acc 0.76
2016-09-07T16:35:13.891088: step 281, loss 0.418849, acc 0.78
2016-09-07T16:35:14.570408: step 282, loss 0.280857, acc 0.92
2016-09-07T16:35:15.227175: step 283, loss 0.418652, acc 0.82
2016-09-07T16:35:15.896043: step 284, loss 0.435956, acc 0.8
2016-09-07T16:35:16.554707: step 285, loss 0.357963, acc 0.84
2016-09-07T16:35:17.235204: step 286, loss 0.436081, acc 0.78
2016-09-07T16:35:17.894417: step 287, loss 0.330035, acc 0.86
2016-09-07T16:35:18.554537: step 288, loss 0.388758, acc 0.78
2016-09-07T16:35:19.212400: step 289, loss 0.383589, acc 0.8
2016-09-07T16:35:19.887071: step 290, loss 0.272505, acc 0.88
2016-09-07T16:35:20.561070: step 291, loss 0.259502, acc 0.9
2016-09-07T16:35:21.213346: step 292, loss 0.23778, acc 0.94
2016-09-07T16:35:21.885936: step 293, loss 0.522625, acc 0.72
2016-09-07T16:35:22.547281: step 294, loss 0.276592, acc 0.86
2016-09-07T16:35:23.196150: step 295, loss 0.357606, acc 0.86
2016-09-07T16:35:23.865687: step 296, loss 0.278538, acc 0.88
2016-09-07T16:35:24.565636: step 297, loss 0.279434, acc 0.86
2016-09-07T16:35:25.251809: step 298, loss 0.285036, acc 0.86
2016-09-07T16:35:25.912615: step 299, loss 0.538994, acc 0.72
2016-09-07T16:35:26.583686: step 300, loss 0.242533, acc 0.9

Evaluation:
2016-09-07T16:35:29.489683: step 300, loss 0.464766, acc 0.792

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-300

2016-09-07T16:35:31.224986: step 301, loss 0.361002, acc 0.82
2016-09-07T16:35:31.885084: step 302, loss 0.254065, acc 0.92
2016-09-07T16:35:32.553143: step 303, loss 0.460743, acc 0.86
2016-09-07T16:35:33.312372: step 304, loss 0.287836, acc 0.88
2016-09-07T16:35:33.991894: step 305, loss 0.446285, acc 0.82
2016-09-07T16:35:34.670546: step 306, loss 0.319838, acc 0.86
2016-09-07T16:35:35.342878: step 307, loss 0.340123, acc 0.86
2016-09-07T16:35:36.020783: step 308, loss 0.419224, acc 0.8
2016-09-07T16:35:36.686272: step 309, loss 0.436942, acc 0.86
2016-09-07T16:35:37.373856: step 310, loss 0.322279, acc 0.84
2016-09-07T16:35:38.042905: step 311, loss 0.289044, acc 0.86
2016-09-07T16:35:38.708688: step 312, loss 0.335535, acc 0.84
2016-09-07T16:35:39.394172: step 313, loss 0.376196, acc 0.86
2016-09-07T16:35:40.070091: step 314, loss 0.461844, acc 0.8
2016-09-07T16:35:40.768710: step 315, loss 0.554004, acc 0.68
2016-09-07T16:35:41.451835: step 316, loss 0.428535, acc 0.78
2016-09-07T16:35:42.118782: step 317, loss 0.407964, acc 0.82
2016-09-07T16:35:42.797562: step 318, loss 0.293965, acc 0.9
2016-09-07T16:35:43.471547: step 319, loss 0.275174, acc 0.9
2016-09-07T16:35:44.154853: step 320, loss 0.456211, acc 0.74
2016-09-07T16:35:44.832296: step 321, loss 0.296614, acc 0.88
2016-09-07T16:35:45.521795: step 322, loss 0.388713, acc 0.78
2016-09-07T16:35:46.201888: step 323, loss 0.37114, acc 0.78
2016-09-07T16:35:46.941319: step 324, loss 0.296959, acc 0.86
2016-09-07T16:35:47.633496: step 325, loss 0.526116, acc 0.74
2016-09-07T16:35:48.342937: step 326, loss 0.462597, acc 0.76
2016-09-07T16:35:49.094188: step 327, loss 0.375955, acc 0.8
2016-09-07T16:35:49.798544: step 328, loss 0.428449, acc 0.82
2016-09-07T16:35:50.469715: step 329, loss 0.40869, acc 0.8
2016-09-07T16:35:51.183171: step 330, loss 0.411433, acc 0.84
2016-09-07T16:35:51.901255: step 331, loss 0.23558, acc 0.92
2016-09-07T16:35:52.651276: step 332, loss 0.257268, acc 0.92
2016-09-07T16:35:53.371420: step 333, loss 0.28855, acc 0.88
2016-09-07T16:35:54.038329: step 334, loss 0.30533, acc 0.88
2016-09-07T16:35:54.722065: step 335, loss 0.297387, acc 0.9
2016-09-07T16:35:55.456215: step 336, loss 0.37987, acc 0.88
2016-09-07T16:35:56.154723: step 337, loss 0.397584, acc 0.86
2016-09-07T16:35:56.838668: step 338, loss 0.445616, acc 0.74
2016-09-07T16:35:57.586633: step 339, loss 0.311416, acc 0.86
2016-09-07T16:35:58.339395: step 340, loss 0.326302, acc 0.88
2016-09-07T16:35:59.073640: step 341, loss 0.262397, acc 0.9
2016-09-07T16:35:59.840166: step 342, loss 0.284949, acc 0.86
2016-09-07T16:36:00.556032: step 343, loss 0.297973, acc 0.88
2016-09-07T16:36:01.355116: step 344, loss 0.373615, acc 0.86
2016-09-07T16:36:02.108402: step 345, loss 0.439107, acc 0.84
2016-09-07T16:36:02.803693: step 346, loss 0.528119, acc 0.74
2016-09-07T16:36:03.484531: step 347, loss 0.268383, acc 0.9
2016-09-07T16:36:04.143753: step 348, loss 0.233961, acc 0.9
2016-09-07T16:36:04.856642: step 349, loss 0.411579, acc 0.82
2016-09-07T16:36:05.550182: step 350, loss 0.287262, acc 0.9
2016-09-07T16:36:06.219667: step 351, loss 0.550695, acc 0.7
2016-09-07T16:36:06.890109: step 352, loss 0.429982, acc 0.8
2016-09-07T16:36:07.544999: step 353, loss 0.36259, acc 0.82
2016-09-07T16:36:08.202173: step 354, loss 0.439846, acc 0.78
2016-09-07T16:36:08.865945: step 355, loss 0.33459, acc 0.88
2016-09-07T16:36:09.528076: step 356, loss 0.354837, acc 0.84
2016-09-07T16:36:10.199306: step 357, loss 0.482805, acc 0.84
2016-09-07T16:36:10.868635: step 358, loss 0.3075, acc 0.78
2016-09-07T16:36:11.547855: step 359, loss 0.453706, acc 0.78
2016-09-07T16:36:12.214853: step 360, loss 0.485368, acc 0.78
2016-09-07T16:36:12.870028: step 361, loss 0.405969, acc 0.82
2016-09-07T16:36:13.531152: step 362, loss 0.406097, acc 0.78
2016-09-07T16:36:14.203733: step 363, loss 0.290617, acc 0.92
2016-09-07T16:36:14.871163: step 364, loss 0.328707, acc 0.84
2016-09-07T16:36:15.545373: step 365, loss 0.450388, acc 0.74
2016-09-07T16:36:16.200882: step 366, loss 0.338309, acc 0.86
2016-09-07T16:36:16.876082: step 367, loss 0.347981, acc 0.86
2016-09-07T16:36:17.547757: step 368, loss 0.380651, acc 0.9
2016-09-07T16:36:18.235987: step 369, loss 0.433772, acc 0.8
2016-09-07T16:36:18.949830: step 370, loss 0.41706, acc 0.78
2016-09-07T16:36:19.605035: step 371, loss 0.381001, acc 0.8
2016-09-07T16:36:20.296314: step 372, loss 0.317538, acc 0.86
2016-09-07T16:36:21.009596: step 373, loss 0.533519, acc 0.74
2016-09-07T16:36:21.679037: step 374, loss 0.412268, acc 0.88
2016-09-07T16:36:22.431521: step 375, loss 0.339357, acc 0.86
2016-09-07T16:36:23.099631: step 376, loss 0.356046, acc 0.88
2016-09-07T16:36:23.792935: step 377, loss 0.563919, acc 0.7
2016-09-07T16:36:24.481362: step 378, loss 0.323647, acc 0.9
2016-09-07T16:36:25.166464: step 379, loss 0.359156, acc 0.82
2016-09-07T16:36:25.882189: step 380, loss 0.47444, acc 0.82
2016-09-07T16:36:26.565552: step 381, loss 0.400139, acc 0.82
2016-09-07T16:36:27.238343: step 382, loss 0.409402, acc 0.82
2016-09-07T16:36:27.931634: step 383, loss 0.397747, acc 0.84
2016-09-07T16:36:28.606665: step 384, loss 0.404861, acc 0.76
2016-09-07T16:36:29.276554: step 385, loss 0.361722, acc 0.9
2016-09-07T16:36:29.961687: step 386, loss 0.391268, acc 0.8
2016-09-07T16:36:30.644174: step 387, loss 0.468209, acc 0.82
2016-09-07T16:36:31.022164: step 388, loss 0.506459, acc 0.833333
2016-09-07T16:36:31.726230: step 389, loss 0.342517, acc 0.84
2016-09-07T16:36:32.396530: step 390, loss 0.25673, acc 0.86
2016-09-07T16:36:33.069041: step 391, loss 0.271928, acc 0.9
2016-09-07T16:36:33.771371: step 392, loss 0.211734, acc 0.92
2016-09-07T16:36:34.470081: step 393, loss 0.226505, acc 0.88
2016-09-07T16:36:35.128114: step 394, loss 0.279622, acc 0.9
2016-09-07T16:36:35.794362: step 395, loss 0.24979, acc 0.9
2016-09-07T16:36:36.460905: step 396, loss 0.206772, acc 0.9
2016-09-07T16:36:37.151602: step 397, loss 0.20173, acc 0.92
2016-09-07T16:36:37.834607: step 398, loss 0.212361, acc 0.94
2016-09-07T16:36:38.511838: step 399, loss 0.228711, acc 0.92
2016-09-07T16:36:39.184970: step 400, loss 0.134783, acc 0.94

Evaluation:
2016-09-07T16:36:42.070267: step 400, loss 0.619125, acc 0.8

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-400

2016-09-07T16:36:43.728015: step 401, loss 0.2608, acc 0.86
2016-09-07T16:36:44.412860: step 402, loss 0.531646, acc 0.88
2016-09-07T16:36:45.096902: step 403, loss 0.268215, acc 0.94
2016-09-07T16:36:45.768099: step 404, loss 0.174784, acc 0.9
2016-09-07T16:36:46.442988: step 405, loss 0.27649, acc 0.9
2016-09-07T16:36:47.103805: step 406, loss 0.327334, acc 0.86
2016-09-07T16:36:47.778491: step 407, loss 0.0989141, acc 0.98
2016-09-07T16:36:48.452788: step 408, loss 0.233643, acc 0.92
2016-09-07T16:36:49.120818: step 409, loss 0.447005, acc 0.86
2016-09-07T16:36:49.777244: step 410, loss 0.176663, acc 0.92
2016-09-07T16:36:50.462360: step 411, loss 0.255195, acc 0.92
2016-09-07T16:36:51.143828: step 412, loss 0.135728, acc 0.92
2016-09-07T16:36:51.854318: step 413, loss 0.264267, acc 0.88
2016-09-07T16:36:52.585747: step 414, loss 0.334098, acc 0.84
2016-09-07T16:36:53.331504: step 415, loss 0.303502, acc 0.9
2016-09-07T16:36:54.016028: step 416, loss 0.174304, acc 0.96
2016-09-07T16:36:54.738019: step 417, loss 0.208695, acc 0.92
2016-09-07T16:36:55.538932: step 418, loss 0.288634, acc 0.86
2016-09-07T16:36:56.284917: step 419, loss 0.0951891, acc 0.96
2016-09-07T16:36:56.953503: step 420, loss 0.200976, acc 0.92
2016-09-07T16:36:57.615110: step 421, loss 0.437858, acc 0.88
2016-09-07T16:36:58.272828: step 422, loss 0.223013, acc 0.9
2016-09-07T16:36:58.941659: step 423, loss 0.253856, acc 0.9
2016-09-07T16:36:59.610167: step 424, loss 0.404984, acc 0.84
2016-09-07T16:37:00.289618: step 425, loss 0.150105, acc 0.98
2016-09-07T16:37:00.956476: step 426, loss 0.166681, acc 0.96
2016-09-07T16:37:01.607015: step 427, loss 0.22596, acc 0.92
2016-09-07T16:37:02.295992: step 428, loss 0.314222, acc 0.86
2016-09-07T16:37:02.969778: step 429, loss 0.175709, acc 0.92
2016-09-07T16:37:03.650792: step 430, loss 0.181618, acc 0.92
2016-09-07T16:37:04.390811: step 431, loss 0.301669, acc 0.88
2016-09-07T16:37:05.129608: step 432, loss 0.243419, acc 0.94
2016-09-07T16:37:05.829477: step 433, loss 0.170184, acc 0.94
2016-09-07T16:37:06.561828: step 434, loss 0.34375, acc 0.88
2016-09-07T16:37:07.264654: step 435, loss 0.242964, acc 0.9
2016-09-07T16:37:08.223828: step 436, loss 0.363424, acc 0.9
2016-09-07T16:37:09.118465: step 437, loss 0.141446, acc 0.94
2016-09-07T16:37:09.893573: step 438, loss 0.192605, acc 0.88
2016-09-07T16:37:10.661297: step 439, loss 0.113173, acc 0.96
2016-09-07T16:37:11.418117: step 440, loss 0.236747, acc 0.94
2016-09-07T16:37:12.181569: step 441, loss 0.226867, acc 0.92
2016-09-07T16:37:13.011971: step 442, loss 0.123822, acc 0.94
2016-09-07T16:37:13.876241: step 443, loss 0.215626, acc 0.88
2016-09-07T16:37:14.633294: step 444, loss 0.341809, acc 0.8
2016-09-07T16:37:15.460131: step 445, loss 0.209186, acc 0.94
2016-09-07T16:37:16.276986: step 446, loss 0.248779, acc 0.88
2016-09-07T16:37:17.068278: step 447, loss 0.211898, acc 0.88
2016-09-07T16:37:17.830421: step 448, loss 0.127078, acc 0.96
2016-09-07T16:37:18.593852: step 449, loss 0.116631, acc 0.96
2016-09-07T16:37:19.438024: step 450, loss 0.33521, acc 0.84
2016-09-07T16:37:20.346398: step 451, loss 0.192039, acc 0.94
2016-09-07T16:37:21.244150: step 452, loss 0.270019, acc 0.9
2016-09-07T16:37:22.147912: step 453, loss 0.22476, acc 0.92
2016-09-07T16:37:23.033074: step 454, loss 0.241922, acc 0.88
2016-09-07T16:37:23.832565: step 455, loss 0.327116, acc 0.84
2016-09-07T16:37:24.495207: step 456, loss 0.194062, acc 0.92
2016-09-07T16:37:25.155849: step 457, loss 0.216796, acc 0.86
2016-09-07T16:37:25.836578: step 458, loss 0.287213, acc 0.9
2016-09-07T16:37:26.500127: step 459, loss 0.291122, acc 0.82
2016-09-07T16:37:27.183816: step 460, loss 0.167174, acc 0.92
2016-09-07T16:37:27.871133: step 461, loss 0.187661, acc 0.92
2016-09-07T16:37:28.599571: step 462, loss 0.270441, acc 0.86
2016-09-07T16:37:29.325070: step 463, loss 0.250175, acc 0.92
2016-09-07T16:37:30.033354: step 464, loss 0.318342, acc 0.84
2016-09-07T16:37:30.833978: step 465, loss 0.252714, acc 0.86
2016-09-07T16:37:31.601189: step 466, loss 0.164455, acc 0.94
2016-09-07T16:37:32.274522: step 467, loss 0.230724, acc 0.9
2016-09-07T16:37:32.974422: step 468, loss 0.140672, acc 0.94
2016-09-07T16:37:33.645944: step 469, loss 0.202453, acc 0.9
2016-09-07T16:37:34.323158: step 470, loss 0.287002, acc 0.86
2016-09-07T16:37:35.026567: step 471, loss 0.27468, acc 0.86
2016-09-07T16:37:35.689262: step 472, loss 0.346422, acc 0.92
2016-09-07T16:37:36.384846: step 473, loss 0.220029, acc 0.92
2016-09-07T16:37:37.141528: step 474, loss 0.34155, acc 0.78
2016-09-07T16:37:37.833605: step 475, loss 0.145395, acc 0.96
2016-09-07T16:37:38.500674: step 476, loss 0.216099, acc 0.88
2016-09-07T16:37:39.171457: step 477, loss 0.287141, acc 0.86
2016-09-07T16:37:39.828246: step 478, loss 0.292771, acc 0.88
2016-09-07T16:37:40.485216: step 479, loss 0.30587, acc 0.82
2016-09-07T16:37:41.154580: step 480, loss 0.221918, acc 0.9
2016-09-07T16:37:41.823891: step 481, loss 0.157777, acc 0.92
2016-09-07T16:37:42.502890: step 482, loss 0.17963, acc 0.9
2016-09-07T16:37:43.177723: step 483, loss 0.255791, acc 0.84
2016-09-07T16:37:43.847710: step 484, loss 0.149375, acc 0.94
2016-09-07T16:37:44.533831: step 485, loss 0.357825, acc 0.8
2016-09-07T16:37:45.223924: step 486, loss 0.251849, acc 0.9
2016-09-07T16:37:45.905409: step 487, loss 0.206512, acc 0.96
2016-09-07T16:37:46.576546: step 488, loss 0.289637, acc 0.88
2016-09-07T16:37:47.330184: step 489, loss 0.313592, acc 0.88
2016-09-07T16:37:48.087508: step 490, loss 0.209994, acc 0.9
2016-09-07T16:37:48.762186: step 491, loss 0.181508, acc 0.92
2016-09-07T16:37:49.446182: step 492, loss 0.175387, acc 0.94
2016-09-07T16:37:50.275773: step 493, loss 0.140164, acc 0.92
2016-09-07T16:37:50.948298: step 494, loss 0.289792, acc 0.88
2016-09-07T16:37:51.609746: step 495, loss 0.330977, acc 0.86
2016-09-07T16:37:52.393157: step 496, loss 0.250792, acc 0.88
2016-09-07T16:37:53.216087: step 497, loss 0.21856, acc 0.9
2016-09-07T16:37:54.001604: step 498, loss 0.384123, acc 0.86
2016-09-07T16:37:54.718961: step 499, loss 0.275957, acc 0.88
2016-09-07T16:37:55.376484: step 500, loss 0.265681, acc 0.88

Evaluation:
2016-09-07T16:37:58.420945: step 500, loss 0.543298, acc 0.786

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-500

2016-09-07T16:38:00.075940: step 501, loss 0.358385, acc 0.9
2016-09-07T16:38:00.782793: step 502, loss 0.281334, acc 0.86
2016-09-07T16:38:01.450989: step 503, loss 0.221009, acc 0.88
2016-09-07T16:38:02.126684: step 504, loss 0.241259, acc 0.88
2016-09-07T16:38:02.800541: step 505, loss 0.22481, acc 0.88
2016-09-07T16:38:03.477885: step 506, loss 0.222037, acc 0.86
2016-09-07T16:38:04.135251: step 507, loss 0.185387, acc 0.96
2016-09-07T16:38:04.812280: step 508, loss 0.177207, acc 0.92
2016-09-07T16:38:05.507825: step 509, loss 0.206438, acc 0.9
2016-09-07T16:38:06.250596: step 510, loss 0.427346, acc 0.78
2016-09-07T16:38:06.953505: step 511, loss 0.172366, acc 0.92
2016-09-07T16:38:07.642526: step 512, loss 0.218651, acc 0.94
2016-09-07T16:38:08.329320: step 513, loss 0.261867, acc 0.94
2016-09-07T16:38:09.050585: step 514, loss 0.363746, acc 0.82
2016-09-07T16:38:09.749377: step 515, loss 0.16972, acc 0.94
2016-09-07T16:38:10.446908: step 516, loss 0.25618, acc 0.86
2016-09-07T16:38:11.110339: step 517, loss 0.214066, acc 0.94
2016-09-07T16:38:11.944907: step 518, loss 0.357959, acc 0.88
2016-09-07T16:38:12.749208: step 519, loss 0.353519, acc 0.84
2016-09-07T16:38:13.450318: step 520, loss 0.298882, acc 0.86
2016-09-07T16:38:14.233358: step 521, loss 0.215045, acc 0.9
2016-09-07T16:38:14.908960: step 522, loss 0.222667, acc 0.92
2016-09-07T16:38:15.583654: step 523, loss 0.313753, acc 0.88
2016-09-07T16:38:16.365374: step 524, loss 0.302589, acc 0.86
2016-09-07T16:38:17.069647: step 525, loss 0.300065, acc 0.9
2016-09-07T16:38:17.758143: step 526, loss 0.293555, acc 0.86
2016-09-07T16:38:18.554042: step 527, loss 0.215443, acc 0.92
2016-09-07T16:38:19.296066: step 528, loss 0.222111, acc 0.94
2016-09-07T16:38:19.967138: step 529, loss 0.297862, acc 0.88
2016-09-07T16:38:20.649860: step 530, loss 0.177277, acc 0.92
2016-09-07T16:38:21.328288: step 531, loss 0.255289, acc 0.88
2016-09-07T16:38:22.011387: step 532, loss 0.219179, acc 0.92
2016-09-07T16:38:22.668465: step 533, loss 0.289635, acc 0.86
2016-09-07T16:38:23.338488: step 534, loss 0.337151, acc 0.84
2016-09-07T16:38:24.063161: step 535, loss 0.206029, acc 0.92
2016-09-07T16:38:24.861870: step 536, loss 0.219653, acc 0.86
2016-09-07T16:38:25.618880: step 537, loss 0.186987, acc 0.94
2016-09-07T16:38:26.309778: step 538, loss 0.311256, acc 0.84
2016-09-07T16:38:26.966944: step 539, loss 0.224242, acc 0.92
2016-09-07T16:38:27.633185: step 540, loss 0.252912, acc 0.9
2016-09-07T16:38:28.318788: step 541, loss 0.379969, acc 0.88
2016-09-07T16:38:29.022133: step 542, loss 0.319974, acc 0.8
2016-09-07T16:38:29.698181: step 543, loss 0.281182, acc 0.86
2016-09-07T16:38:30.374906: step 544, loss 0.188686, acc 0.94
2016-09-07T16:38:31.051796: step 545, loss 0.217925, acc 0.92
2016-09-07T16:38:31.715541: step 546, loss 0.228068, acc 0.92
2016-09-07T16:38:32.387508: step 547, loss 0.258902, acc 0.88
2016-09-07T16:38:33.048322: step 548, loss 0.369896, acc 0.86
2016-09-07T16:38:33.708911: step 549, loss 0.243242, acc 0.86
2016-09-07T16:38:34.375254: step 550, loss 0.196312, acc 0.94
2016-09-07T16:38:35.054479: step 551, loss 0.203933, acc 0.9
2016-09-07T16:38:35.737104: step 552, loss 0.351644, acc 0.84
2016-09-07T16:38:36.384670: step 553, loss 0.248057, acc 0.92
2016-09-07T16:38:37.052560: step 554, loss 0.123482, acc 0.94
2016-09-07T16:38:37.722388: step 555, loss 0.210122, acc 0.9
2016-09-07T16:38:38.399926: step 556, loss 0.205046, acc 0.92
2016-09-07T16:38:39.060873: step 557, loss 0.306558, acc 0.84
2016-09-07T16:38:39.739793: step 558, loss 0.340589, acc 0.82
2016-09-07T16:38:40.398690: step 559, loss 0.276565, acc 0.84
2016-09-07T16:38:41.070993: step 560, loss 0.253912, acc 0.9
2016-09-07T16:38:41.749792: step 561, loss 0.271227, acc 0.86
2016-09-07T16:38:42.410956: step 562, loss 0.116044, acc 0.98
2016-09-07T16:38:43.085447: step 563, loss 0.36595, acc 0.88
2016-09-07T16:38:43.750275: step 564, loss 0.372299, acc 0.8
2016-09-07T16:38:44.412122: step 565, loss 0.117636, acc 0.94
2016-09-07T16:38:45.085718: step 566, loss 0.467638, acc 0.8
2016-09-07T16:38:45.733449: step 567, loss 0.308108, acc 0.86
2016-09-07T16:38:46.419891: step 568, loss 0.184631, acc 0.92
2016-09-07T16:38:47.081449: step 569, loss 0.31863, acc 0.84
2016-09-07T16:38:47.753097: step 570, loss 0.195797, acc 0.94
2016-09-07T16:38:48.429956: step 571, loss 0.193086, acc 0.92
2016-09-07T16:38:49.104871: step 572, loss 0.327676, acc 0.84
2016-09-07T16:38:49.784624: step 573, loss 0.277805, acc 0.86
2016-09-07T16:38:50.452306: step 574, loss 0.215692, acc 0.88
2016-09-07T16:38:51.122284: step 575, loss 0.269391, acc 0.88
2016-09-07T16:38:51.783373: step 576, loss 0.200571, acc 0.92
2016-09-07T16:38:52.453779: step 577, loss 0.25477, acc 0.88
2016-09-07T16:38:53.135380: step 578, loss 0.310269, acc 0.84
2016-09-07T16:38:53.826657: step 579, loss 0.299601, acc 0.86
2016-09-07T16:38:54.513642: step 580, loss 0.20358, acc 0.9
2016-09-07T16:38:55.181808: step 581, loss 0.246654, acc 0.9
2016-09-07T16:38:55.579763: step 582, loss 0.296147, acc 0.833333
2016-09-07T16:38:56.250753: step 583, loss 0.0822841, acc 0.98
2016-09-07T16:38:56.937289: step 584, loss 0.198676, acc 0.88
2016-09-07T16:38:57.604633: step 585, loss 0.312077, acc 0.88
2016-09-07T16:38:58.300009: step 586, loss 0.149611, acc 0.96
2016-09-07T16:38:58.975532: step 587, loss 0.131365, acc 0.94
2016-09-07T16:38:59.671415: step 588, loss 0.203515, acc 0.88
2016-09-07T16:39:00.371319: step 589, loss 0.120474, acc 0.94
2016-09-07T16:39:01.064732: step 590, loss 0.189564, acc 0.96
2016-09-07T16:39:01.740135: step 591, loss 0.0976142, acc 0.96
2016-09-07T16:39:02.405560: step 592, loss 0.262955, acc 0.92
2016-09-07T16:39:03.078938: step 593, loss 0.113338, acc 0.96
2016-09-07T16:39:03.769914: step 594, loss 0.0390939, acc 1
2016-09-07T16:39:04.435146: step 595, loss 0.310421, acc 0.88
2016-09-07T16:39:05.104821: step 596, loss 0.0671532, acc 1
2016-09-07T16:39:05.771134: step 597, loss 0.118023, acc 0.94
2016-09-07T16:39:06.436614: step 598, loss 0.30722, acc 0.9
2016-09-07T16:39:07.099367: step 599, loss 0.172735, acc 0.88
2016-09-07T16:39:07.765815: step 600, loss 0.0458511, acc 0.98

Evaluation:
2016-09-07T16:39:10.715611: step 600, loss 0.623868, acc 0.784

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-600

2016-09-07T16:39:12.435304: step 601, loss 0.0804696, acc 0.98
2016-09-07T16:39:13.158897: step 602, loss 0.134526, acc 0.94
2016-09-07T16:39:13.890221: step 603, loss 0.108894, acc 0.94
2016-09-07T16:39:14.617840: step 604, loss 0.253594, acc 0.9
2016-09-07T16:39:15.363207: step 605, loss 0.225035, acc 0.94
2016-09-07T16:39:16.104239: step 606, loss 0.145685, acc 0.94
2016-09-07T16:39:16.940437: step 607, loss 0.132899, acc 0.94
2016-09-07T16:39:17.745971: step 608, loss 0.115589, acc 0.96
2016-09-07T16:39:18.564972: step 609, loss 0.167897, acc 0.88
2016-09-07T16:39:19.388728: step 610, loss 0.046704, acc 0.98
2016-09-07T16:39:20.131045: step 611, loss 0.120759, acc 0.94
2016-09-07T16:39:20.801086: step 612, loss 0.0476485, acc 0.98
2016-09-07T16:39:21.527515: step 613, loss 0.189577, acc 0.9
2016-09-07T16:39:22.310696: step 614, loss 0.167704, acc 0.96
2016-09-07T16:39:23.018200: step 615, loss 0.121119, acc 0.92
2016-09-07T16:39:23.688184: step 616, loss 0.164995, acc 0.96
2016-09-07T16:39:24.503521: step 617, loss 0.0739535, acc 0.98
2016-09-07T16:39:25.175468: step 618, loss 0.243507, acc 0.86
2016-09-07T16:39:25.826603: step 619, loss 0.11507, acc 0.92
2016-09-07T16:39:26.485222: step 620, loss 0.111543, acc 0.94
2016-09-07T16:39:27.214136: step 621, loss 0.0619112, acc 0.96
2016-09-07T16:39:27.901269: step 622, loss 0.057248, acc 0.96
2016-09-07T16:39:28.608276: step 623, loss 0.150888, acc 0.94
2016-09-07T16:39:29.294288: step 624, loss 0.163113, acc 0.9
2016-09-07T16:39:30.026223: step 625, loss 0.0326326, acc 1
2016-09-07T16:39:30.722062: step 626, loss 0.340389, acc 0.86
2016-09-07T16:39:31.576555: step 627, loss 0.29518, acc 0.86
2016-09-07T16:39:32.368130: step 628, loss 0.188134, acc 0.94
2016-09-07T16:39:33.138001: step 629, loss 0.355712, acc 0.86
2016-09-07T16:39:33.800563: step 630, loss 0.129301, acc 0.94
2016-09-07T16:39:34.482391: step 631, loss 0.116186, acc 0.96
2016-09-07T16:39:35.304255: step 632, loss 0.212522, acc 0.9
2016-09-07T16:39:36.075823: step 633, loss 0.133269, acc 0.94
2016-09-07T16:39:36.740557: step 634, loss 0.166362, acc 0.9
2016-09-07T16:39:37.418986: step 635, loss 0.132172, acc 0.94
2016-09-07T16:39:38.112555: step 636, loss 0.391041, acc 0.88
2016-09-07T16:39:38.775536: step 637, loss 0.257896, acc 0.94
2016-09-07T16:39:39.444931: step 638, loss 0.169402, acc 0.92
2016-09-07T16:39:40.096040: step 639, loss 0.196753, acc 0.88
2016-09-07T16:39:40.822970: step 640, loss 0.149293, acc 0.96
2016-09-07T16:39:41.645611: step 641, loss 0.19442, acc 0.94
2016-09-07T16:39:42.470683: step 642, loss 0.152732, acc 0.96
2016-09-07T16:39:43.291905: step 643, loss 0.171504, acc 0.92
2016-09-07T16:39:44.078724: step 644, loss 0.131635, acc 0.98
2016-09-07T16:39:44.871672: step 645, loss 0.160612, acc 0.94
2016-09-07T16:39:45.632811: step 646, loss 0.3038, acc 0.92
2016-09-07T16:39:46.405981: step 647, loss 0.125085, acc 0.92
2016-09-07T16:39:47.228601: step 648, loss 0.205288, acc 0.92
2016-09-07T16:39:48.017928: step 649, loss 0.105797, acc 0.96
2016-09-07T16:39:48.685331: step 650, loss 0.110644, acc 0.96
2016-09-07T16:39:49.348679: step 651, loss 0.149813, acc 0.94
2016-09-07T16:39:50.022999: step 652, loss 0.120632, acc 0.94
2016-09-07T16:39:50.697548: step 653, loss 0.144843, acc 0.96
2016-09-07T16:39:51.357216: step 654, loss 0.198317, acc 0.9
2016-09-07T16:39:52.039592: step 655, loss 0.211046, acc 0.92
2016-09-07T16:39:52.733802: step 656, loss 0.115896, acc 0.96
2016-09-07T16:39:53.558144: step 657, loss 0.184805, acc 0.9
2016-09-07T16:39:54.215420: step 658, loss 0.240598, acc 0.88
2016-09-07T16:39:54.901825: step 659, loss 0.227012, acc 0.92
2016-09-07T16:39:55.653492: step 660, loss 0.100451, acc 0.98
2016-09-07T16:39:56.435735: step 661, loss 0.157524, acc 0.94
2016-09-07T16:39:57.129983: step 662, loss 0.243785, acc 0.9
2016-09-07T16:39:57.811699: step 663, loss 0.134536, acc 0.94
2016-09-07T16:39:58.473097: step 664, loss 0.191618, acc 0.9
2016-09-07T16:39:59.256903: step 665, loss 0.228277, acc 0.94
2016-09-07T16:40:00.010784: step 666, loss 0.135172, acc 0.92
2016-09-07T16:40:00.687650: step 667, loss 0.129133, acc 0.92
2016-09-07T16:40:01.370736: step 668, loss 0.185545, acc 0.9
2016-09-07T16:40:02.063958: step 669, loss 0.216861, acc 0.9
2016-09-07T16:40:02.785761: step 670, loss 0.0688896, acc 0.98
2016-09-07T16:40:03.475018: step 671, loss 0.080766, acc 0.98
2016-09-07T16:40:04.146492: step 672, loss 0.193499, acc 0.9
2016-09-07T16:40:04.821372: step 673, loss 0.156691, acc 0.92
2016-09-07T16:40:05.495934: step 674, loss 0.231319, acc 0.92
2016-09-07T16:40:06.180742: step 675, loss 0.250189, acc 0.9
2016-09-07T16:40:06.845358: step 676, loss 0.103607, acc 0.94
2016-09-07T16:40:07.552843: step 677, loss 0.169135, acc 0.92
2016-09-07T16:40:08.222214: step 678, loss 0.182071, acc 0.9
2016-09-07T16:40:08.911651: step 679, loss 0.0937438, acc 0.96
2016-09-07T16:40:09.575816: step 680, loss 0.171178, acc 0.94
2016-09-07T16:40:10.248815: step 681, loss 0.0957706, acc 0.96
2016-09-07T16:40:10.950467: step 682, loss 0.161705, acc 0.94
2016-09-07T16:40:11.622409: step 683, loss 0.15163, acc 0.9
2016-09-07T16:40:12.492105: step 684, loss 0.282163, acc 0.88
2016-09-07T16:40:13.293266: step 685, loss 0.168429, acc 0.92
2016-09-07T16:40:13.953149: step 686, loss 0.244389, acc 0.9
2016-09-07T16:40:14.629807: step 687, loss 0.14958, acc 0.92
2016-09-07T16:40:15.495243: step 688, loss 0.210434, acc 0.98
2016-09-07T16:40:16.315253: step 689, loss 0.164, acc 0.88
2016-09-07T16:40:17.031040: step 690, loss 0.213053, acc 0.88
2016-09-07T16:40:17.689515: step 691, loss 0.333091, acc 0.84
2016-09-07T16:40:18.376873: step 692, loss 0.135945, acc 0.94
2016-09-07T16:40:19.062632: step 693, loss 0.168975, acc 0.9
2016-09-07T16:40:19.727437: step 694, loss 0.0784692, acc 0.96
2016-09-07T16:40:20.403511: step 695, loss 0.110006, acc 0.94
2016-09-07T16:40:21.078006: step 696, loss 0.248229, acc 0.92
2016-09-07T16:40:21.744438: step 697, loss 0.173561, acc 0.92
2016-09-07T16:40:22.408198: step 698, loss 0.230732, acc 0.88
2016-09-07T16:40:23.083483: step 699, loss 0.285315, acc 0.88
2016-09-07T16:40:23.753239: step 700, loss 0.186898, acc 0.88

Evaluation:
2016-09-07T16:40:26.694783: step 700, loss 0.579436, acc 0.794

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-700

2016-09-07T16:40:28.325693: step 701, loss 0.191679, acc 0.9
2016-09-07T16:40:28.990072: step 702, loss 0.148215, acc 0.94
2016-09-07T16:40:29.679505: step 703, loss 0.0790022, acc 0.96
2016-09-07T16:40:30.369499: step 704, loss 0.177103, acc 0.94
2016-09-07T16:40:31.048881: step 705, loss 0.164322, acc 0.9
2016-09-07T16:40:31.743270: step 706, loss 0.103997, acc 0.96
2016-09-07T16:40:32.423285: step 707, loss 0.200866, acc 0.94
2016-09-07T16:40:33.087061: step 708, loss 0.123103, acc 0.96
2016-09-07T16:40:33.769461: step 709, loss 0.214823, acc 0.9
2016-09-07T16:40:34.450442: step 710, loss 0.0660783, acc 0.98
2016-09-07T16:40:35.141843: step 711, loss 0.0860862, acc 0.96
2016-09-07T16:40:35.807947: step 712, loss 0.0969122, acc 0.96
2016-09-07T16:40:36.478267: step 713, loss 0.118977, acc 0.98
2016-09-07T16:40:37.122972: step 714, loss 0.17906, acc 0.92
2016-09-07T16:40:37.771113: step 715, loss 0.103636, acc 0.98
2016-09-07T16:40:38.437404: step 716, loss 0.199689, acc 0.92
2016-09-07T16:40:39.116273: step 717, loss 0.232339, acc 0.92
2016-09-07T16:40:39.778761: step 718, loss 0.142646, acc 0.96
2016-09-07T16:40:40.440645: step 719, loss 0.135113, acc 0.96
2016-09-07T16:40:41.106357: step 720, loss 0.160409, acc 0.94
2016-09-07T16:40:41.808040: step 721, loss 0.177786, acc 0.9
2016-09-07T16:40:42.472770: step 722, loss 0.331796, acc 0.88
2016-09-07T16:40:43.163010: step 723, loss 0.187871, acc 0.92
2016-09-07T16:40:43.845197: step 724, loss 0.110006, acc 0.94
2016-09-07T16:40:44.507844: step 725, loss 0.146581, acc 0.94
2016-09-07T16:40:45.180743: step 726, loss 0.0864622, acc 0.98
2016-09-07T16:40:45.858557: step 727, loss 0.174227, acc 0.94
2016-09-07T16:40:46.533448: step 728, loss 0.268951, acc 0.88
2016-09-07T16:40:47.210912: step 729, loss 0.177333, acc 0.9
2016-09-07T16:40:47.881249: step 730, loss 0.107011, acc 0.94
2016-09-07T16:40:48.556912: step 731, loss 0.0872322, acc 0.98
2016-09-07T16:40:49.225349: step 732, loss 0.245056, acc 0.94
2016-09-07T16:40:49.890077: step 733, loss 0.197442, acc 0.92
2016-09-07T16:40:50.565647: step 734, loss 0.151918, acc 0.92
2016-09-07T16:40:51.225336: step 735, loss 0.186007, acc 0.92
2016-09-07T16:40:51.896915: step 736, loss 0.386278, acc 0.8
2016-09-07T16:40:52.556380: step 737, loss 0.139643, acc 0.94
2016-09-07T16:40:53.251805: step 738, loss 0.179016, acc 0.96
2016-09-07T16:40:53.920632: step 739, loss 0.162806, acc 0.92
2016-09-07T16:40:54.586045: step 740, loss 0.237506, acc 0.9
2016-09-07T16:40:55.252025: step 741, loss 0.153786, acc 0.96
2016-09-07T16:40:55.929885: step 742, loss 0.175762, acc 0.9
2016-09-07T16:40:56.595593: step 743, loss 0.198633, acc 0.96
2016-09-07T16:40:57.278622: step 744, loss 0.349419, acc 0.84
2016-09-07T16:40:57.973532: step 745, loss 0.216456, acc 0.9
2016-09-07T16:40:58.643983: step 746, loss 0.161269, acc 0.96
2016-09-07T16:40:59.305842: step 747, loss 0.207143, acc 0.88
2016-09-07T16:40:59.983335: step 748, loss 0.167757, acc 0.9
2016-09-07T16:41:00.654619: step 749, loss 0.1195, acc 0.98
2016-09-07T16:41:01.322255: step 750, loss 0.145042, acc 0.94
2016-09-07T16:41:01.995370: step 751, loss 0.279718, acc 0.82
2016-09-07T16:41:02.669001: step 752, loss 0.189504, acc 0.92
2016-09-07T16:41:03.353331: step 753, loss 0.204536, acc 0.92
2016-09-07T16:41:04.005128: step 754, loss 0.25971, acc 0.88
2016-09-07T16:41:04.670026: step 755, loss 0.123633, acc 0.94
2016-09-07T16:41:05.348770: step 756, loss 0.114248, acc 0.96
2016-09-07T16:41:06.011544: step 757, loss 0.236769, acc 0.88
2016-09-07T16:41:06.674882: step 758, loss 0.108801, acc 0.94
2016-09-07T16:41:07.363721: step 759, loss 0.27826, acc 0.84
2016-09-07T16:41:08.029373: step 760, loss 0.175519, acc 0.9
2016-09-07T16:41:08.690707: step 761, loss 0.226288, acc 0.86
2016-09-07T16:41:09.365142: step 762, loss 0.288836, acc 0.86
2016-09-07T16:41:10.044109: step 763, loss 0.112688, acc 0.96
2016-09-07T16:41:10.721446: step 764, loss 0.0635596, acc 1
2016-09-07T16:41:11.399659: step 765, loss 0.102872, acc 0.98
2016-09-07T16:41:12.081233: step 766, loss 0.146027, acc 0.9
2016-09-07T16:41:12.770403: step 767, loss 0.365489, acc 0.86
2016-09-07T16:41:13.443087: step 768, loss 0.154295, acc 0.92
2016-09-07T16:41:14.100911: step 769, loss 0.293523, acc 0.86
2016-09-07T16:41:14.776361: step 770, loss 0.165167, acc 0.92
2016-09-07T16:41:15.457527: step 771, loss 0.228438, acc 0.84
2016-09-07T16:41:16.138768: step 772, loss 0.108557, acc 0.94
2016-09-07T16:41:16.835366: step 773, loss 0.191307, acc 0.92
2016-09-07T16:41:17.506892: step 774, loss 0.243232, acc 0.92
2016-09-07T16:41:18.186316: step 775, loss 0.213239, acc 0.94
2016-09-07T16:41:18.535854: step 776, loss 0.117636, acc 0.916667
2016-09-07T16:41:19.208920: step 777, loss 0.0894183, acc 0.96
2016-09-07T16:41:19.883150: step 778, loss 0.11579, acc 0.92
2016-09-07T16:41:20.548489: step 779, loss 0.223403, acc 0.88
2016-09-07T16:41:21.220302: step 780, loss 0.0563704, acc 1
2016-09-07T16:41:21.948986: step 781, loss 0.109448, acc 0.96
2016-09-07T16:41:22.641395: step 782, loss 0.086306, acc 0.94
2016-09-07T16:41:23.387485: step 783, loss 0.108461, acc 0.98
2016-09-07T16:41:24.125682: step 784, loss 0.265602, acc 0.92
2016-09-07T16:41:24.830308: step 785, loss 0.179393, acc 0.9
2016-09-07T16:41:25.690654: step 786, loss 0.0861557, acc 0.98
2016-09-07T16:41:26.436129: step 787, loss 0.0614977, acc 0.98
2016-09-07T16:41:27.172331: step 788, loss 0.254677, acc 0.92
2016-09-07T16:41:27.871085: step 789, loss 0.0315252, acc 1
2016-09-07T16:41:28.642933: step 790, loss 0.0447788, acc 1
2016-09-07T16:41:29.311235: step 791, loss 0.126466, acc 0.96
2016-09-07T16:41:29.976409: step 792, loss 0.0953086, acc 0.96
2016-09-07T16:41:30.657254: step 793, loss 0.115057, acc 0.92
2016-09-07T16:41:31.337365: step 794, loss 0.158307, acc 0.94
2016-09-07T16:41:32.001922: step 795, loss 0.11416, acc 0.96
2016-09-07T16:41:32.678151: step 796, loss 0.148318, acc 0.94
2016-09-07T16:41:33.344370: step 797, loss 0.159419, acc 0.92
2016-09-07T16:41:34.020489: step 798, loss 0.0579151, acc 0.96
2016-09-07T16:41:34.691262: step 799, loss 0.0705405, acc 0.98
2016-09-07T16:41:35.401433: step 800, loss 0.107939, acc 0.94

Evaluation:
2016-09-07T16:41:38.679395: step 800, loss 0.74223, acc 0.786

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-800

2016-09-07T16:41:40.434489: step 801, loss 0.107383, acc 0.96
2016-09-07T16:41:41.162751: step 802, loss 0.0443267, acc 0.98
2016-09-07T16:41:41.850676: step 803, loss 0.0722152, acc 0.98
2016-09-07T16:41:42.531394: step 804, loss 0.0566232, acc 0.98
2016-09-07T16:41:43.194319: step 805, loss 0.113986, acc 0.94
2016-09-07T16:41:43.858530: step 806, loss 0.128553, acc 0.94
2016-09-07T16:41:44.530451: step 807, loss 0.105789, acc 0.94
2016-09-07T16:41:45.201784: step 808, loss 0.128256, acc 0.92
2016-09-07T16:41:45.883958: step 809, loss 0.0506224, acc 0.98
2016-09-07T16:41:46.551647: step 810, loss 0.0510084, acc 1
2016-09-07T16:41:47.232519: step 811, loss 0.122967, acc 0.94
2016-09-07T16:41:47.918187: step 812, loss 0.102715, acc 0.98
2016-09-07T16:41:48.599859: step 813, loss 0.0879227, acc 0.96
2016-09-07T16:41:49.261553: step 814, loss 0.160967, acc 0.94
2016-09-07T16:41:49.937931: step 815, loss 0.331494, acc 0.94
2016-09-07T16:41:50.584355: step 816, loss 0.0540397, acc 0.98
2016-09-07T16:41:51.257140: step 817, loss 0.277242, acc 0.86
2016-09-07T16:41:51.929646: step 818, loss 0.117706, acc 0.94
2016-09-07T16:41:52.608351: step 819, loss 0.0861626, acc 0.94
2016-09-07T16:41:53.274384: step 820, loss 0.0774924, acc 0.96
2016-09-07T16:41:53.934346: step 821, loss 0.0958952, acc 0.94
2016-09-07T16:41:54.596370: step 822, loss 0.214649, acc 0.92
2016-09-07T16:41:55.269431: step 823, loss 0.170393, acc 0.96
2016-09-07T16:41:55.934960: step 824, loss 0.0971341, acc 0.94
2016-09-07T16:41:56.704000: step 825, loss 0.115794, acc 0.96
2016-09-07T16:41:57.363254: step 826, loss 0.138046, acc 0.92
2016-09-07T16:41:58.012260: step 827, loss 0.174403, acc 0.94
2016-09-07T16:41:58.690899: step 828, loss 0.095491, acc 0.96
2016-09-07T16:41:59.468121: step 829, loss 0.0971452, acc 0.98
2016-09-07T16:42:00.149372: step 830, loss 0.0638372, acc 0.98
2016-09-07T16:42:00.841512: step 831, loss 0.11049, acc 0.96
2016-09-07T16:42:01.494726: step 832, loss 0.174525, acc 0.94
2016-09-07T16:42:02.228020: step 833, loss 0.280873, acc 0.88
2016-09-07T16:42:03.133995: step 834, loss 0.0516338, acc 0.98
2016-09-07T16:42:03.891608: step 835, loss 0.0598063, acc 0.98
2016-09-07T16:42:04.574318: step 836, loss 0.0618768, acc 0.98
2016-09-07T16:42:05.257501: step 837, loss 0.104888, acc 0.96
2016-09-07T16:42:06.015373: step 838, loss 0.0639234, acc 0.98
2016-09-07T16:42:06.802900: step 839, loss 0.153002, acc 0.94
2016-09-07T16:42:07.540589: step 840, loss 0.159155, acc 0.92
2016-09-07T16:42:08.283738: step 841, loss 0.153343, acc 0.94
2016-09-07T16:42:09.037927: step 842, loss 0.241632, acc 0.92
2016-09-07T16:42:09.837573: step 843, loss 0.0893828, acc 0.96
2016-09-07T16:42:10.647811: step 844, loss 0.215669, acc 0.92
2016-09-07T16:42:11.423627: step 845, loss 0.0955787, acc 0.96
2016-09-07T16:42:12.110784: step 846, loss 0.143778, acc 0.94
2016-09-07T16:42:12.783266: step 847, loss 0.112158, acc 0.94
2016-09-07T16:42:13.552497: step 848, loss 0.153411, acc 0.96
2016-09-07T16:42:14.224153: step 849, loss 0.0970065, acc 0.98
2016-09-07T16:42:14.900765: step 850, loss 0.133138, acc 0.96
2016-09-07T16:42:15.562210: step 851, loss 0.145276, acc 0.92
2016-09-07T16:42:16.228259: step 852, loss 0.180297, acc 0.92
2016-09-07T16:42:16.907434: step 853, loss 0.14217, acc 0.9
2016-09-07T16:42:17.576834: step 854, loss 0.232023, acc 0.92
2016-09-07T16:42:18.246187: step 855, loss 0.185897, acc 0.9
2016-09-07T16:42:18.921835: step 856, loss 0.0914432, acc 0.96
2016-09-07T16:42:19.588949: step 857, loss 0.0717477, acc 0.94
2016-09-07T16:42:20.244249: step 858, loss 0.0891208, acc 0.92
2016-09-07T16:42:20.926392: step 859, loss 0.197473, acc 0.92
2016-09-07T16:42:21.606594: step 860, loss 0.186224, acc 0.92
2016-09-07T16:42:22.305091: step 861, loss 0.105096, acc 0.94
2016-09-07T16:42:22.978928: step 862, loss 0.134196, acc 0.9
2016-09-07T16:42:23.641735: step 863, loss 0.0690951, acc 0.96
2016-09-07T16:42:24.316155: step 864, loss 0.0763849, acc 0.98
2016-09-07T16:42:24.978920: step 865, loss 0.0621029, acc 1
2016-09-07T16:42:25.641999: step 866, loss 0.121874, acc 0.94
2016-09-07T16:42:26.324075: step 867, loss 0.142402, acc 0.92
2016-09-07T16:42:27.106058: step 868, loss 0.0702918, acc 0.98
2016-09-07T16:42:27.822945: step 869, loss 0.13729, acc 0.94
2016-09-07T16:42:28.555554: step 870, loss 0.123091, acc 0.92
2016-09-07T16:42:29.214364: step 871, loss 0.153295, acc 0.92
2016-09-07T16:42:29.928014: step 872, loss 0.102334, acc 0.98
2016-09-07T16:42:30.692832: step 873, loss 0.147558, acc 0.92
2016-09-07T16:42:31.399130: step 874, loss 0.181302, acc 0.94
2016-09-07T16:42:32.079365: step 875, loss 0.117965, acc 0.96
2016-09-07T16:42:32.765851: step 876, loss 0.144129, acc 0.92
2016-09-07T16:42:33.453476: step 877, loss 0.15482, acc 0.96
2016-09-07T16:42:34.221855: step 878, loss 0.0598865, acc 0.98
2016-09-07T16:42:34.981596: step 879, loss 0.118023, acc 0.96
2016-09-07T16:42:35.784208: step 880, loss 0.161555, acc 0.94
2016-09-07T16:42:36.484969: step 881, loss 0.185782, acc 0.9
2016-09-07T16:42:37.236859: step 882, loss 0.0655656, acc 0.98
2016-09-07T16:42:37.940582: step 883, loss 0.119933, acc 0.96
2016-09-07T16:42:38.658245: step 884, loss 0.139643, acc 0.92
2016-09-07T16:42:39.354507: step 885, loss 0.186775, acc 0.92
2016-09-07T16:42:40.020458: step 886, loss 0.0750479, acc 0.98
2016-09-07T16:42:40.689468: step 887, loss 0.170604, acc 0.92
2016-09-07T16:42:41.369576: step 888, loss 0.137648, acc 0.92
2016-09-07T16:42:42.053759: step 889, loss 0.188242, acc 0.92
2016-09-07T16:42:42.720976: step 890, loss 0.0872542, acc 0.96
2016-09-07T16:42:43.380201: step 891, loss 0.162306, acc 0.96
2016-09-07T16:42:44.060545: step 892, loss 0.157392, acc 0.94
2016-09-07T16:42:44.773014: step 893, loss 0.0354597, acc 1
2016-09-07T16:42:45.503350: step 894, loss 0.158313, acc 0.92
2016-09-07T16:42:46.230122: step 895, loss 0.181265, acc 0.96
2016-09-07T16:42:46.934218: step 896, loss 0.156968, acc 0.94
2016-09-07T16:42:47.635875: step 897, loss 0.078924, acc 0.98
2016-09-07T16:42:48.334249: step 898, loss 0.138869, acc 0.92
2016-09-07T16:42:49.015361: step 899, loss 0.0950432, acc 0.96
2016-09-07T16:42:49.710858: step 900, loss 0.0789689, acc 0.98

Evaluation:
2016-09-07T16:42:52.840578: step 900, loss 0.805974, acc 0.765

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-900

2016-09-07T16:42:54.489346: step 901, loss 0.182129, acc 0.9
2016-09-07T16:42:55.165102: step 902, loss 0.119002, acc 0.96
2016-09-07T16:42:55.837624: step 903, loss 0.149921, acc 0.96
2016-09-07T16:42:56.516408: step 904, loss 0.0629095, acc 0.96
2016-09-07T16:42:57.203710: step 905, loss 0.117137, acc 0.96
2016-09-07T16:42:57.880661: step 906, loss 0.134606, acc 0.92
2016-09-07T16:42:58.568979: step 907, loss 0.210721, acc 0.96
2016-09-07T16:42:59.251166: step 908, loss 0.108772, acc 0.94
2016-09-07T16:42:59.940063: step 909, loss 0.112441, acc 0.94
2016-09-07T16:43:00.660721: step 910, loss 0.125791, acc 0.94
2016-09-07T16:43:01.347418: step 911, loss 0.123148, acc 0.96
2016-09-07T16:43:02.034305: step 912, loss 0.0608029, acc 1
2016-09-07T16:43:02.725134: step 913, loss 0.168513, acc 0.9
2016-09-07T16:43:03.439852: step 914, loss 0.10913, acc 0.96
2016-09-07T16:43:04.133692: step 915, loss 0.165308, acc 0.92
2016-09-07T16:43:04.798669: step 916, loss 0.120485, acc 0.94
2016-09-07T16:43:05.488522: step 917, loss 0.0514258, acc 1
2016-09-07T16:43:06.225325: step 918, loss 0.219956, acc 0.9
2016-09-07T16:43:06.914787: step 919, loss 0.0885013, acc 0.98
2016-09-07T16:43:07.744713: step 920, loss 0.166703, acc 0.92
2016-09-07T16:43:08.567224: step 921, loss 0.0933068, acc 0.98
2016-09-07T16:43:09.266610: step 922, loss 0.198718, acc 0.94
2016-09-07T16:43:09.942818: step 923, loss 0.129649, acc 0.94
2016-09-07T16:43:10.618084: step 924, loss 0.0981803, acc 0.96
2016-09-07T16:43:11.300800: step 925, loss 0.133687, acc 0.92
2016-09-07T16:43:12.152619: step 926, loss 0.223482, acc 0.92
2016-09-07T16:43:12.853941: step 927, loss 0.196754, acc 0.9
2016-09-07T16:43:13.581285: step 928, loss 0.134805, acc 0.94
2016-09-07T16:43:14.307121: step 929, loss 0.153226, acc 0.92
2016-09-07T16:43:14.990492: step 930, loss 0.0659796, acc 0.98
2016-09-07T16:43:15.644199: step 931, loss 0.131957, acc 0.94
2016-09-07T16:43:16.328042: step 932, loss 0.141135, acc 0.96
2016-09-07T16:43:16.983450: step 933, loss 0.0543218, acc 0.98
2016-09-07T16:43:17.653543: step 934, loss 0.0749389, acc 0.98
2016-09-07T16:43:18.331221: step 935, loss 0.159219, acc 0.96
2016-09-07T16:43:19.024551: step 936, loss 0.131341, acc 0.96
2016-09-07T16:43:19.691660: step 937, loss 0.118456, acc 0.94
2016-09-07T16:43:20.434972: step 938, loss 0.166494, acc 0.92
2016-09-07T16:43:21.098220: step 939, loss 0.0924389, acc 0.96
2016-09-07T16:43:21.754418: step 940, loss 0.0599928, acc 0.98
2016-09-07T16:43:22.427368: step 941, loss 0.101413, acc 0.94
2016-09-07T16:43:23.122556: step 942, loss 0.0973596, acc 0.96
2016-09-07T16:43:23.822037: step 943, loss 0.273477, acc 0.92
2016-09-07T16:43:24.494081: step 944, loss 0.115003, acc 0.98
2016-09-07T16:43:25.248491: step 945, loss 0.353183, acc 0.82
2016-09-07T16:43:25.911818: step 946, loss 0.253491, acc 0.86
2016-09-07T16:43:26.568876: step 947, loss 0.116866, acc 0.94
2016-09-07T16:43:27.257366: step 948, loss 0.266834, acc 0.86
2016-09-07T16:43:27.946539: step 949, loss 0.148172, acc 0.96
2016-09-07T16:43:28.612116: step 950, loss 0.0851971, acc 0.96
2016-09-07T16:43:29.334427: step 951, loss 0.108882, acc 0.92
2016-09-07T16:43:30.001714: step 952, loss 0.151639, acc 0.94
2016-09-07T16:43:30.791673: step 953, loss 0.0825227, acc 0.96
2016-09-07T16:43:31.466162: step 954, loss 0.133746, acc 0.9
2016-09-07T16:43:32.160522: step 955, loss 0.212795, acc 0.92
2016-09-07T16:43:32.928401: step 956, loss 0.0867692, acc 0.98
2016-09-07T16:43:33.589594: step 957, loss 0.229266, acc 0.9
2016-09-07T16:43:34.251149: step 958, loss 0.129539, acc 0.94
2016-09-07T16:43:34.982606: step 959, loss 0.0801846, acc 0.96
2016-09-07T16:43:35.673780: step 960, loss 0.100774, acc 0.94
2016-09-07T16:43:36.368626: step 961, loss 0.19604, acc 0.92
2016-09-07T16:43:37.093823: step 962, loss 0.139949, acc 0.94
2016-09-07T16:43:37.772257: step 963, loss 0.158892, acc 0.96
2016-09-07T16:43:38.471739: step 964, loss 0.070842, acc 0.98
2016-09-07T16:43:39.146363: step 965, loss 0.0828172, acc 0.94
2016-09-07T16:43:39.835383: step 966, loss 0.138228, acc 0.9
2016-09-07T16:43:40.515698: step 967, loss 0.185604, acc 0.92
2016-09-07T16:43:41.189240: step 968, loss 0.0632323, acc 0.98
2016-09-07T16:43:41.862607: step 969, loss 0.189498, acc 0.92
2016-09-07T16:43:42.233883: step 970, loss 0.109247, acc 0.916667
2016-09-07T16:43:42.925568: step 971, loss 0.0864478, acc 0.96
2016-09-07T16:43:43.601924: step 972, loss 0.132968, acc 0.94
2016-09-07T16:43:44.276972: step 973, loss 0.131217, acc 0.94
2016-09-07T16:43:44.941992: step 974, loss 0.109495, acc 0.96
2016-09-07T16:43:45.616040: step 975, loss 0.0779297, acc 0.96
2016-09-07T16:43:46.278039: step 976, loss 0.135988, acc 0.94
2016-09-07T16:43:46.952774: step 977, loss 0.0860044, acc 0.98
2016-09-07T16:43:47.624190: step 978, loss 0.115868, acc 0.96
2016-09-07T16:43:48.313791: step 979, loss 0.0431462, acc 0.98
2016-09-07T16:43:49.013560: step 980, loss 0.184959, acc 0.92
2016-09-07T16:43:49.770194: step 981, loss 0.154561, acc 0.94
2016-09-07T16:43:50.480542: step 982, loss 0.218338, acc 0.92
2016-09-07T16:43:51.207304: step 983, loss 0.173372, acc 0.9
2016-09-07T16:43:51.917297: step 984, loss 0.112199, acc 0.94
2016-09-07T16:43:52.582345: step 985, loss 0.0784009, acc 0.96
2016-09-07T16:43:53.271627: step 986, loss 0.0823867, acc 0.98
2016-09-07T16:43:53.952879: step 987, loss 0.0818416, acc 0.98
2016-09-07T16:43:54.632465: step 988, loss 0.0238915, acc 1
2016-09-07T16:43:55.291986: step 989, loss 0.115684, acc 0.94
2016-09-07T16:43:55.964551: step 990, loss 0.0237308, acc 1
2016-09-07T16:43:56.624345: step 991, loss 0.137129, acc 0.94
2016-09-07T16:43:57.279236: step 992, loss 0.185247, acc 0.94
2016-09-07T16:43:57.951576: step 993, loss 0.0353789, acc 1
2016-09-07T16:43:58.625037: step 994, loss 0.132146, acc 0.94
2016-09-07T16:43:59.372197: step 995, loss 0.0640182, acc 0.98
2016-09-07T16:44:00.079461: step 996, loss 0.0676851, acc 0.98
2016-09-07T16:44:00.804765: step 997, loss 0.075426, acc 0.96
2016-09-07T16:44:01.521577: step 998, loss 0.0927501, acc 0.96
2016-09-07T16:44:02.343666: step 999, loss 0.123858, acc 0.94
2016-09-07T16:44:03.017930: step 1000, loss 0.0712423, acc 1

Evaluation:
2016-09-07T16:44:06.114900: step 1000, loss 0.789958, acc 0.778

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-1000

2016-09-07T16:44:07.713378: step 1001, loss 0.0564767, acc 0.98
2016-09-07T16:44:08.387186: step 1002, loss 0.111943, acc 0.96
2016-09-07T16:44:09.051599: step 1003, loss 0.0872577, acc 0.94
2016-09-07T16:44:09.715385: step 1004, loss 0.149114, acc 0.88
2016-09-07T16:44:10.387128: step 1005, loss 0.0927386, acc 0.96
2016-09-07T16:44:11.039883: step 1006, loss 0.0642644, acc 0.98
2016-09-07T16:44:11.714157: step 1007, loss 0.0965014, acc 0.96
2016-09-07T16:44:12.372399: step 1008, loss 0.0409475, acc 1
2016-09-07T16:44:13.049149: step 1009, loss 0.0570456, acc 0.98
2016-09-07T16:44:13.711509: step 1010, loss 0.147888, acc 0.96
2016-09-07T16:44:14.378359: step 1011, loss 0.0960625, acc 0.96
2016-09-07T16:44:15.062306: step 1012, loss 0.0543335, acc 0.98
2016-09-07T16:44:15.755425: step 1013, loss 0.0315441, acc 1
2016-09-07T16:44:16.429761: step 1014, loss 0.0590276, acc 0.96
2016-09-07T16:44:17.122072: step 1015, loss 0.0306147, acc 0.98
2016-09-07T16:44:17.791552: step 1016, loss 0.0705226, acc 0.94
2016-09-07T16:44:18.473722: step 1017, loss 0.12962, acc 0.96
2016-09-07T16:44:19.162468: step 1018, loss 0.0851521, acc 0.98
2016-09-07T16:44:19.839366: step 1019, loss 0.0583259, acc 0.98
2016-09-07T16:44:20.510626: step 1020, loss 0.0824755, acc 0.96
2016-09-07T16:44:21.189345: step 1021, loss 0.167839, acc 0.96
2016-09-07T16:44:21.854829: step 1022, loss 0.0854099, acc 0.96
2016-09-07T16:44:22.519402: step 1023, loss 0.0442839, acc 1
2016-09-07T16:44:23.183608: step 1024, loss 0.0875649, acc 0.96
2016-09-07T16:44:23.846408: step 1025, loss 0.100774, acc 0.96
2016-09-07T16:44:24.522679: step 1026, loss 0.147641, acc 0.94
2016-09-07T16:44:25.213107: step 1027, loss 0.0638183, acc 0.98
2016-09-07T16:44:25.887385: step 1028, loss 0.0743591, acc 0.96
2016-09-07T16:44:26.558776: step 1029, loss 0.0652109, acc 0.96
2016-09-07T16:44:27.232670: step 1030, loss 0.0466048, acc 0.98
2016-09-07T16:44:27.920295: step 1031, loss 0.0822513, acc 0.94
2016-09-07T16:44:28.614712: step 1032, loss 0.0579543, acc 0.98
2016-09-07T16:44:29.293884: step 1033, loss 0.082336, acc 0.96
2016-09-07T16:44:29.990608: step 1034, loss 0.151691, acc 0.92
2016-09-07T16:44:30.653438: step 1035, loss 0.204436, acc 0.92
2016-09-07T16:44:31.317311: step 1036, loss 0.0525816, acc 0.98
2016-09-07T16:44:32.003569: step 1037, loss 0.00844141, acc 1
2016-09-07T16:44:32.670927: step 1038, loss 0.165855, acc 0.92
2016-09-07T16:44:33.345755: step 1039, loss 0.116165, acc 0.96
2016-09-07T16:44:34.010027: step 1040, loss 0.110953, acc 0.94
2016-09-07T16:44:34.689924: step 1041, loss 0.0136772, acc 1
2016-09-07T16:44:35.358927: step 1042, loss 0.0262003, acc 0.98
2016-09-07T16:44:36.017572: step 1043, loss 0.196432, acc 0.86
2016-09-07T16:44:36.698479: step 1044, loss 0.0120645, acc 1
2016-09-07T16:44:37.361354: step 1045, loss 0.104112, acc 0.94
2016-09-07T16:44:38.016356: step 1046, loss 0.192492, acc 0.94
2016-09-07T16:44:38.671990: step 1047, loss 0.122676, acc 0.94
2016-09-07T16:44:39.333150: step 1048, loss 0.0633609, acc 0.98
2016-09-07T16:44:39.986383: step 1049, loss 0.0828555, acc 0.98
2016-09-07T16:44:40.652012: step 1050, loss 0.0752354, acc 0.94
2016-09-07T16:44:41.332674: step 1051, loss 0.0477957, acc 0.98
2016-09-07T16:44:42.022864: step 1052, loss 0.103082, acc 0.98
2016-09-07T16:44:42.689024: step 1053, loss 0.0675367, acc 0.98
2016-09-07T16:44:43.370804: step 1054, loss 0.12018, acc 0.94
2016-09-07T16:44:44.039409: step 1055, loss 0.0531176, acc 0.98
2016-09-07T16:44:44.716154: step 1056, loss 0.155203, acc 0.94
2016-09-07T16:44:45.379836: step 1057, loss 0.0557285, acc 0.96
2016-09-07T16:44:46.045913: step 1058, loss 0.113686, acc 0.94
2016-09-07T16:44:46.722231: step 1059, loss 0.249998, acc 0.88
2016-09-07T16:44:47.390656: step 1060, loss 0.0811292, acc 0.98
2016-09-07T16:44:48.063817: step 1061, loss 0.039649, acc 1
2016-09-07T16:44:48.741463: step 1062, loss 0.0927637, acc 0.96
2016-09-07T16:44:49.387521: step 1063, loss 0.218241, acc 0.94
2016-09-07T16:44:50.062929: step 1064, loss 0.108797, acc 0.94
2016-09-07T16:44:50.757206: step 1065, loss 0.133464, acc 0.96
2016-09-07T16:44:51.436720: step 1066, loss 0.164532, acc 0.88
2016-09-07T16:44:52.122058: step 1067, loss 0.0610044, acc 1
2016-09-07T16:44:52.790432: step 1068, loss 0.0336303, acc 1
2016-09-07T16:44:53.461611: step 1069, loss 0.127271, acc 0.9
2016-09-07T16:44:54.135368: step 1070, loss 0.0514514, acc 0.98
2016-09-07T16:44:54.820429: step 1071, loss 0.0618196, acc 0.96
2016-09-07T16:44:55.480822: step 1072, loss 0.0477636, acc 1
2016-09-07T16:44:56.174781: step 1073, loss 0.138378, acc 0.96
2016-09-07T16:44:56.843066: step 1074, loss 0.103778, acc 0.94
2016-09-07T16:44:57.529503: step 1075, loss 0.0957197, acc 0.98
2016-09-07T16:44:58.221388: step 1076, loss 0.0764313, acc 0.94
2016-09-07T16:44:58.900556: step 1077, loss 0.0986711, acc 0.96
2016-09-07T16:44:59.581003: step 1078, loss 0.200837, acc 0.94
2016-09-07T16:45:00.260135: step 1079, loss 0.184315, acc 0.9
2016-09-07T16:45:00.941530: step 1080, loss 0.157243, acc 0.92
2016-09-07T16:45:01.636413: step 1081, loss 0.113625, acc 0.96
2016-09-07T16:45:02.401522: step 1082, loss 0.0347787, acc 0.98
2016-09-07T16:45:03.241077: step 1083, loss 0.10679, acc 0.94
2016-09-07T16:45:03.934464: step 1084, loss 0.0669013, acc 0.98
2016-09-07T16:45:04.658876: step 1085, loss 0.105381, acc 0.96
2016-09-07T16:45:05.395917: step 1086, loss 0.18062, acc 0.9
2016-09-07T16:45:06.090517: step 1087, loss 0.0903104, acc 0.98
2016-09-07T16:45:06.752751: step 1088, loss 0.0533649, acc 0.98
2016-09-07T16:45:07.445397: step 1089, loss 0.167979, acc 0.96
2016-09-07T16:45:08.162977: step 1090, loss 0.179185, acc 0.9
2016-09-07T16:45:08.857478: step 1091, loss 0.103259, acc 0.94
2016-09-07T16:45:09.552590: step 1092, loss 0.109094, acc 0.94
2016-09-07T16:45:10.244810: step 1093, loss 0.0641662, acc 0.98
2016-09-07T16:45:11.002031: step 1094, loss 0.106678, acc 0.96
2016-09-07T16:45:11.820302: step 1095, loss 0.051961, acc 0.98
2016-09-07T16:45:12.523385: step 1096, loss 0.131089, acc 0.92
2016-09-07T16:45:13.374991: step 1097, loss 0.21033, acc 0.9
2016-09-07T16:45:14.108825: step 1098, loss 0.0704539, acc 0.96
2016-09-07T16:45:14.788871: step 1099, loss 0.136002, acc 0.98
2016-09-07T16:45:15.451961: step 1100, loss 0.0747527, acc 1

Evaluation:
2016-09-07T16:45:18.752046: step 1100, loss 0.822221, acc 0.762

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-1100

2016-09-07T16:45:20.641613: step 1101, loss 0.0882041, acc 0.98
2016-09-07T16:45:21.508076: step 1102, loss 0.134993, acc 0.94
2016-09-07T16:45:22.210853: step 1103, loss 0.0433385, acc 1
2016-09-07T16:45:22.938060: step 1104, loss 0.0520909, acc 0.98
2016-09-07T16:45:23.773525: step 1105, loss 0.185271, acc 0.92
2016-09-07T16:45:24.627095: step 1106, loss 0.151645, acc 0.96
2016-09-07T16:45:25.497945: step 1107, loss 0.141527, acc 0.94
2016-09-07T16:45:26.222208: step 1108, loss 0.0632871, acc 0.98
2016-09-07T16:45:27.022306: step 1109, loss 0.120458, acc 0.96
2016-09-07T16:45:27.906209: step 1110, loss 0.154819, acc 0.9
2016-09-07T16:45:28.710240: step 1111, loss 0.0762798, acc 0.96
2016-09-07T16:45:29.573987: step 1112, loss 0.157215, acc 0.94
2016-09-07T16:45:30.450373: step 1113, loss 0.0223217, acc 1
2016-09-07T16:45:31.319490: step 1114, loss 0.0534921, acc 0.96
2016-09-07T16:45:32.052626: step 1115, loss 0.0576629, acc 0.98
2016-09-07T16:45:32.878492: step 1116, loss 0.184505, acc 0.92
2016-09-07T16:45:33.709608: step 1117, loss 0.124385, acc 0.96
2016-09-07T16:45:34.612818: step 1118, loss 0.0418793, acc 0.98
2016-09-07T16:45:35.530137: step 1119, loss 0.101059, acc 0.98
2016-09-07T16:45:36.400623: step 1120, loss 0.192028, acc 0.94
2016-09-07T16:45:37.229859: step 1121, loss 0.0889366, acc 0.96
2016-09-07T16:45:37.965653: step 1122, loss 0.198996, acc 0.86
2016-09-07T16:45:38.645178: step 1123, loss 0.0942863, acc 0.94
2016-09-07T16:45:39.319672: step 1124, loss 0.0806884, acc 0.98
2016-09-07T16:45:40.034937: step 1125, loss 0.157678, acc 0.9
2016-09-07T16:45:40.703255: step 1126, loss 0.146029, acc 0.98
2016-09-07T16:45:41.359006: step 1127, loss 0.0771446, acc 0.96
2016-09-07T16:45:42.041477: step 1128, loss 0.126917, acc 0.94
2016-09-07T16:45:42.710351: step 1129, loss 0.132382, acc 0.9
2016-09-07T16:45:43.394249: step 1130, loss 0.0734266, acc 0.96
2016-09-07T16:45:44.076738: step 1131, loss 0.0341491, acc 1
2016-09-07T16:45:44.727328: step 1132, loss 0.0926448, acc 0.96
2016-09-07T16:45:45.412102: step 1133, loss 0.0970152, acc 0.98
2016-09-07T16:45:46.083310: step 1134, loss 0.027382, acc 1
2016-09-07T16:45:46.751161: step 1135, loss 0.0612311, acc 0.98
2016-09-07T16:45:47.432783: step 1136, loss 0.117973, acc 0.98
2016-09-07T16:45:48.110452: step 1137, loss 0.113145, acc 0.94
2016-09-07T16:45:48.770286: step 1138, loss 0.0900315, acc 0.94
2016-09-07T16:45:49.436097: step 1139, loss 0.0351309, acc 0.98
2016-09-07T16:45:50.104038: step 1140, loss 0.164476, acc 0.94
2016-09-07T16:45:50.787154: step 1141, loss 0.0833858, acc 0.96
2016-09-07T16:45:51.460257: step 1142, loss 0.0632452, acc 1
2016-09-07T16:45:52.143144: step 1143, loss 0.164469, acc 0.92
2016-09-07T16:45:52.820493: step 1144, loss 0.0163451, acc 1
2016-09-07T16:45:53.480718: step 1145, loss 0.183963, acc 0.94
2016-09-07T16:45:54.128465: step 1146, loss 0.180637, acc 0.94
2016-09-07T16:45:54.809436: step 1147, loss 0.0995078, acc 0.96
2016-09-07T16:45:55.471873: step 1148, loss 0.066498, acc 0.98
2016-09-07T16:45:56.130177: step 1149, loss 0.0682093, acc 0.98
2016-09-07T16:45:56.783481: step 1150, loss 0.0894034, acc 0.96
2016-09-07T16:45:57.452763: step 1151, loss 0.108186, acc 0.9
2016-09-07T16:45:58.123901: step 1152, loss 0.0759657, acc 0.98
2016-09-07T16:45:58.810138: step 1153, loss 0.0771048, acc 0.98
2016-09-07T16:45:59.503249: step 1154, loss 0.0173191, acc 1
2016-09-07T16:46:00.183751: step 1155, loss 0.0757042, acc 0.98
2016-09-07T16:46:00.869804: step 1156, loss 0.0811689, acc 0.98
2016-09-07T16:46:01.533112: step 1157, loss 0.213102, acc 0.88
2016-09-07T16:46:02.190795: step 1158, loss 0.370805, acc 0.92
2016-09-07T16:46:02.847069: step 1159, loss 0.0806834, acc 0.98
2016-09-07T16:46:03.514187: step 1160, loss 0.0394254, acc 1
2016-09-07T16:46:04.194668: step 1161, loss 0.0967005, acc 0.96
2016-09-07T16:46:04.854948: step 1162, loss 0.112781, acc 0.96
2016-09-07T16:46:05.519347: step 1163, loss 0.0870029, acc 0.94
2016-09-07T16:46:05.887772: step 1164, loss 0.143063, acc 1
2016-09-07T16:46:06.573543: step 1165, loss 0.102925, acc 0.94
2016-09-07T16:46:07.234645: step 1166, loss 0.0855425, acc 0.96
2016-09-07T16:46:07.919997: step 1167, loss 0.0494052, acc 1
2016-09-07T16:46:08.593254: step 1168, loss 0.0755487, acc 0.98
2016-09-07T16:46:09.270423: step 1169, loss 0.097512, acc 0.96
2016-09-07T16:46:09.964226: step 1170, loss 0.231762, acc 0.9
2016-09-07T16:46:10.642266: step 1171, loss 0.109687, acc 0.96
2016-09-07T16:46:11.311226: step 1172, loss 0.0380758, acc 0.98
2016-09-07T16:46:11.972548: step 1173, loss 0.0290594, acc 1
2016-09-07T16:46:12.656712: step 1174, loss 0.0203005, acc 1
2016-09-07T16:46:13.325802: step 1175, loss 0.103475, acc 0.92
2016-09-07T16:46:14.007463: step 1176, loss 0.223638, acc 0.88
2016-09-07T16:46:14.676274: step 1177, loss 0.039559, acc 0.98
2016-09-07T16:46:15.350591: step 1178, loss 0.0165313, acc 1
2016-09-07T16:46:16.017525: step 1179, loss 0.0354703, acc 0.98
2016-09-07T16:46:16.688215: step 1180, loss 0.0751953, acc 0.96
2016-09-07T16:46:17.354221: step 1181, loss 0.0335814, acc 1
2016-09-07T16:46:18.012790: step 1182, loss 0.0293683, acc 0.98
2016-09-07T16:46:18.678106: step 1183, loss 0.105041, acc 0.96
2016-09-07T16:46:19.354865: step 1184, loss 0.141191, acc 0.96
2016-09-07T16:46:20.020539: step 1185, loss 0.0669043, acc 0.98
2016-09-07T16:46:20.689563: step 1186, loss 0.0760542, acc 0.96
2016-09-07T16:46:21.374127: step 1187, loss 0.0299421, acc 1
2016-09-07T16:46:22.042751: step 1188, loss 0.0692471, acc 0.96
2016-09-07T16:46:22.722699: step 1189, loss 0.036831, acc 0.98
2016-09-07T16:46:23.391364: step 1190, loss 0.0450656, acc 0.96
2016-09-07T16:46:24.067869: step 1191, loss 0.0702572, acc 0.98
2016-09-07T16:46:24.749559: step 1192, loss 0.0478379, acc 1
2016-09-07T16:46:25.426066: step 1193, loss 0.0477004, acc 0.98
2016-09-07T16:46:26.079888: step 1194, loss 0.0344646, acc 1
2016-09-07T16:46:26.754416: step 1195, loss 0.0229016, acc 0.98
2016-09-07T16:46:27.405528: step 1196, loss 0.104864, acc 0.94
2016-09-07T16:46:28.068289: step 1197, loss 0.0273291, acc 0.98
2016-09-07T16:46:28.745203: step 1198, loss 0.0582564, acc 0.96
2016-09-07T16:46:29.406591: step 1199, loss 0.0842189, acc 0.96
2016-09-07T16:46:30.087327: step 1200, loss 0.132234, acc 0.98

Evaluation:
2016-09-07T16:46:33.106771: step 1200, loss 1.27336, acc 0.76

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-1200

2016-09-07T16:46:34.753866: step 1201, loss 0.0819972, acc 0.96
2016-09-07T16:46:35.439390: step 1202, loss 0.0754602, acc 0.96
2016-09-07T16:46:36.097289: step 1203, loss 0.0372029, acc 0.98
2016-09-07T16:46:36.776662: step 1204, loss 0.0790605, acc 0.94
2016-09-07T16:46:37.462558: step 1205, loss 0.0451228, acc 0.98
2016-09-07T16:46:38.143802: step 1206, loss 0.0455565, acc 1
2016-09-07T16:46:38.813215: step 1207, loss 0.141867, acc 0.96
2016-09-07T16:46:39.497687: step 1208, loss 0.0148665, acc 1
2016-09-07T16:46:40.171610: step 1209, loss 0.0727919, acc 0.94
2016-09-07T16:46:40.851946: step 1210, loss 0.259207, acc 0.9
2016-09-07T16:46:41.516042: step 1211, loss 0.0518053, acc 0.98
2016-09-07T16:46:42.177259: step 1212, loss 0.0872396, acc 0.96
2016-09-07T16:46:42.837622: step 1213, loss 0.116213, acc 0.94
2016-09-07T16:46:43.526208: step 1214, loss 0.058646, acc 0.98
2016-09-07T16:46:44.223802: step 1215, loss 0.162823, acc 0.96
2016-09-07T16:46:44.906854: step 1216, loss 0.00679295, acc 1
2016-09-07T16:46:45.598420: step 1217, loss 0.0432358, acc 1
2016-09-07T16:46:46.257081: step 1218, loss 0.0351817, acc 1
2016-09-07T16:46:46.922664: step 1219, loss 0.0502714, acc 0.96
2016-09-07T16:46:47.581211: step 1220, loss 0.0969504, acc 0.94
2016-09-07T16:46:48.250404: step 1221, loss 0.107356, acc 0.98
2016-09-07T16:46:48.916935: step 1222, loss 0.0456901, acc 1
2016-09-07T16:46:49.606237: step 1223, loss 0.0722393, acc 0.98
2016-09-07T16:46:50.273305: step 1224, loss 0.0602801, acc 0.98
2016-09-07T16:46:50.953898: step 1225, loss 0.050141, acc 0.98
2016-09-07T16:46:51.625705: step 1226, loss 0.0772922, acc 0.96
2016-09-07T16:46:52.303502: step 1227, loss 0.0795743, acc 0.94
2016-09-07T16:46:52.966799: step 1228, loss 0.0805709, acc 0.96
2016-09-07T16:46:53.604108: step 1229, loss 0.148245, acc 0.92
2016-09-07T16:46:54.272577: step 1230, loss 0.122475, acc 0.96
2016-09-07T16:46:54.959520: step 1231, loss 0.0483567, acc 0.98
2016-09-07T16:46:55.611324: step 1232, loss 0.0546069, acc 0.98
2016-09-07T16:46:56.282375: step 1233, loss 0.0199704, acc 1
2016-09-07T16:46:56.952454: step 1234, loss 0.0651537, acc 0.96
2016-09-07T16:46:57.630430: step 1235, loss 0.12405, acc 0.92
2016-09-07T16:46:58.292121: step 1236, loss 0.0739579, acc 0.98
2016-09-07T16:46:58.954440: step 1237, loss 0.0303037, acc 1
2016-09-07T16:46:59.622423: step 1238, loss 0.0133306, acc 1
2016-09-07T16:47:00.319275: step 1239, loss 0.0344193, acc 0.98
2016-09-07T16:47:01.012552: step 1240, loss 0.25073, acc 0.88
2016-09-07T16:47:01.687815: step 1241, loss 0.0673264, acc 0.96
2016-09-07T16:47:02.347116: step 1242, loss 0.0707801, acc 0.96
2016-09-07T16:47:03.010846: step 1243, loss 0.0632687, acc 0.98
2016-09-07T16:47:03.688156: step 1244, loss 0.181955, acc 0.94
2016-09-07T16:47:04.362766: step 1245, loss 0.186201, acc 0.92
2016-09-07T16:47:05.056882: step 1246, loss 0.222838, acc 0.94
2016-09-07T16:47:05.747843: step 1247, loss 0.102538, acc 0.94
2016-09-07T16:47:06.431594: step 1248, loss 0.0373576, acc 0.98
2016-09-07T16:47:07.105433: step 1249, loss 0.225244, acc 0.9
2016-09-07T16:47:07.779065: step 1250, loss 0.0720438, acc 0.98
2016-09-07T16:47:08.453901: step 1251, loss 0.0480269, acc 0.98
2016-09-07T16:47:09.120234: step 1252, loss 0.0512806, acc 1
2016-09-07T16:47:09.784379: step 1253, loss 0.166562, acc 0.9
2016-09-07T16:47:10.456840: step 1254, loss 0.0646882, acc 0.94
2016-09-07T16:47:11.135157: step 1255, loss 0.092008, acc 0.98
2016-09-07T16:47:11.817718: step 1256, loss 0.107616, acc 0.92
2016-09-07T16:47:12.499349: step 1257, loss 0.0881119, acc 0.94
2016-09-07T16:47:13.196409: step 1258, loss 0.17521, acc 0.96
2016-09-07T16:47:13.870084: step 1259, loss 0.0545408, acc 1
2016-09-07T16:47:14.535987: step 1260, loss 0.0898185, acc 0.96
2016-09-07T16:47:15.200636: step 1261, loss 0.0805188, acc 0.96
2016-09-07T16:47:15.854236: step 1262, loss 0.115956, acc 0.98
2016-09-07T16:47:16.515894: step 1263, loss 0.0743209, acc 0.98
2016-09-07T16:47:17.187136: step 1264, loss 0.0916878, acc 0.96
2016-09-07T16:47:17.877870: step 1265, loss 0.0507036, acc 0.98
2016-09-07T16:47:18.535312: step 1266, loss 0.0718172, acc 0.98
2016-09-07T16:47:19.196116: step 1267, loss 0.0642662, acc 0.96
2016-09-07T16:47:19.860002: step 1268, loss 0.113957, acc 0.92
2016-09-07T16:47:20.533335: step 1269, loss 0.0849037, acc 0.96
2016-09-07T16:47:21.211945: step 1270, loss 0.0632966, acc 0.96
2016-09-07T16:47:21.887740: step 1271, loss 0.0450363, acc 0.98
2016-09-07T16:47:22.557443: step 1272, loss 0.0789786, acc 0.98
2016-09-07T16:47:23.219602: step 1273, loss 0.113601, acc 0.96
2016-09-07T16:47:23.900055: step 1274, loss 0.0432739, acc 0.98
2016-09-07T16:47:24.553903: step 1275, loss 0.12633, acc 0.94
2016-09-07T16:47:25.203255: step 1276, loss 0.0178086, acc 1
2016-09-07T16:47:25.869736: step 1277, loss 0.112359, acc 0.98
2016-09-07T16:47:26.539882: step 1278, loss 0.107275, acc 0.92
2016-09-07T16:47:27.228636: step 1279, loss 0.0843037, acc 0.96
2016-09-07T16:47:27.894966: step 1280, loss 0.0897732, acc 0.96
2016-09-07T16:47:28.574729: step 1281, loss 0.0594488, acc 0.98
2016-09-07T16:47:29.259295: step 1282, loss 0.0588474, acc 0.98
2016-09-07T16:47:29.938982: step 1283, loss 0.0219527, acc 1
2016-09-07T16:47:30.591808: step 1284, loss 0.0639216, acc 0.98
2016-09-07T16:47:31.248362: step 1285, loss 0.0635902, acc 0.98
2016-09-07T16:47:31.927381: step 1286, loss 0.118891, acc 0.96
2016-09-07T16:47:32.626852: step 1287, loss 0.501658, acc 0.94
2016-09-07T16:47:33.288218: step 1288, loss 0.0630837, acc 0.98
2016-09-07T16:47:33.954772: step 1289, loss 0.268545, acc 0.9
2016-09-07T16:47:34.604947: step 1290, loss 0.0532199, acc 0.98
2016-09-07T16:47:35.272090: step 1291, loss 0.0911146, acc 0.96
2016-09-07T16:47:35.958430: step 1292, loss 0.0627312, acc 0.98
2016-09-07T16:47:36.636766: step 1293, loss 0.0981149, acc 0.94
2016-09-07T16:47:37.311848: step 1294, loss 0.0831086, acc 0.98
2016-09-07T16:47:38.001479: step 1295, loss 0.158127, acc 0.9
2016-09-07T16:47:38.679474: step 1296, loss 0.177713, acc 0.94
2016-09-07T16:47:39.370304: step 1297, loss 0.0327251, acc 0.98
2016-09-07T16:47:40.046785: step 1298, loss 0.0773606, acc 0.96
2016-09-07T16:47:40.713138: step 1299, loss 0.0684192, acc 0.98
2016-09-07T16:47:41.430645: step 1300, loss 0.196484, acc 0.92

Evaluation:
2016-09-07T16:47:44.537277: step 1300, loss 0.985253, acc 0.756

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-1300

2016-09-07T16:47:46.150381: step 1301, loss 0.0672071, acc 0.96
2016-09-07T16:47:46.815706: step 1302, loss 0.0270121, acc 1
2016-09-07T16:47:47.485914: step 1303, loss 0.130003, acc 0.94
2016-09-07T16:47:48.183030: step 1304, loss 0.0719037, acc 0.94
2016-09-07T16:47:48.872814: step 1305, loss 0.065792, acc 0.98
2016-09-07T16:47:49.550912: step 1306, loss 0.0909896, acc 0.94
2016-09-07T16:47:50.207333: step 1307, loss 0.138995, acc 0.92
2016-09-07T16:47:50.868580: step 1308, loss 0.0819047, acc 0.96
2016-09-07T16:47:51.548753: step 1309, loss 0.105046, acc 0.96
2016-09-07T16:47:52.216513: step 1310, loss 0.0697831, acc 0.96
2016-09-07T16:47:52.902323: step 1311, loss 0.255876, acc 0.92
2016-09-07T16:47:53.582740: step 1312, loss 0.0346078, acc 0.98
2016-09-07T16:47:54.255514: step 1313, loss 0.230343, acc 0.92
2016-09-07T16:47:54.927526: step 1314, loss 0.0518096, acc 0.98
2016-09-07T16:47:55.600281: step 1315, loss 0.170419, acc 0.92
2016-09-07T16:47:56.263443: step 1316, loss 0.0530452, acc 0.98
2016-09-07T16:47:56.935956: step 1317, loss 0.0694132, acc 0.96
2016-09-07T16:47:57.593602: step 1318, loss 0.1091, acc 0.94
2016-09-07T16:47:58.288861: step 1319, loss 0.0452577, acc 1
2016-09-07T16:47:58.958506: step 1320, loss 0.0516952, acc 0.98
2016-09-07T16:47:59.605600: step 1321, loss 0.0621117, acc 0.94
2016-09-07T16:48:00.280798: step 1322, loss 0.131289, acc 0.94
2016-09-07T16:48:00.945848: step 1323, loss 0.0969733, acc 0.94
2016-09-07T16:48:01.614852: step 1324, loss 0.0850052, acc 0.92
2016-09-07T16:48:02.291662: step 1325, loss 0.166773, acc 0.92
2016-09-07T16:48:02.959966: step 1326, loss 0.0509117, acc 1
2016-09-07T16:48:03.635043: step 1327, loss 0.0191301, acc 1
2016-09-07T16:48:04.318181: step 1328, loss 0.0454555, acc 0.98
2016-09-07T16:48:04.995866: step 1329, loss 0.0903631, acc 0.98
2016-09-07T16:48:05.666592: step 1330, loss 0.0880081, acc 0.96
2016-09-07T16:48:06.331997: step 1331, loss 0.078302, acc 0.96
2016-09-07T16:48:07.004727: step 1332, loss 0.0668356, acc 0.98
2016-09-07T16:48:07.689158: step 1333, loss 0.0399167, acc 0.98
2016-09-07T16:48:08.351092: step 1334, loss 0.0640361, acc 1
2016-09-07T16:48:09.048065: step 1335, loss 0.0655037, acc 0.96
2016-09-07T16:48:09.751825: step 1336, loss 0.160102, acc 0.98
2016-09-07T16:48:10.420727: step 1337, loss 0.152796, acc 0.94
2016-09-07T16:48:11.099002: step 1338, loss 0.0366663, acc 1
2016-09-07T16:48:11.765881: step 1339, loss 0.0983713, acc 0.96
2016-09-07T16:48:12.412464: step 1340, loss 0.0712994, acc 0.98
2016-09-07T16:48:13.100619: step 1341, loss 0.051308, acc 0.98
2016-09-07T16:48:13.760691: step 1342, loss 0.0494575, acc 0.96
2016-09-07T16:48:14.428757: step 1343, loss 0.0610142, acc 0.98
2016-09-07T16:48:15.125236: step 1344, loss 0.0334886, acc 1
2016-09-07T16:48:15.824854: step 1345, loss 0.0476319, acc 0.96
2016-09-07T16:48:16.495519: step 1346, loss 0.0600177, acc 0.98
2016-09-07T16:48:17.174124: step 1347, loss 0.0555577, acc 0.98
2016-09-07T16:48:17.847573: step 1348, loss 0.0786396, acc 0.98
2016-09-07T16:48:18.514610: step 1349, loss 0.138149, acc 0.94
2016-09-07T16:48:19.192931: step 1350, loss 0.0311969, acc 0.98
2016-09-07T16:48:19.863896: step 1351, loss 0.237708, acc 0.92
2016-09-07T16:48:20.535646: step 1352, loss 0.120688, acc 0.98
2016-09-07T16:48:21.199397: step 1353, loss 0.0136038, acc 1
2016-09-07T16:48:21.855580: step 1354, loss 0.191835, acc 0.92
2016-09-07T16:48:22.528099: step 1355, loss 0.0493992, acc 1
2016-09-07T16:48:23.196248: step 1356, loss 0.0250882, acc 1
2016-09-07T16:48:23.864987: step 1357, loss 0.0147238, acc 1
2016-09-07T16:48:24.223503: step 1358, loss 0.147968, acc 0.916667
2016-09-07T16:48:24.902819: step 1359, loss 0.042605, acc 0.98
2016-09-07T16:48:25.576554: step 1360, loss 0.0997042, acc 0.92
2016-09-07T16:48:26.241785: step 1361, loss 0.172312, acc 0.96
2016-09-07T16:48:26.908769: step 1362, loss 0.0293785, acc 1
2016-09-07T16:48:27.595409: step 1363, loss 0.0850255, acc 0.94
2016-09-07T16:48:28.253888: step 1364, loss 0.0844865, acc 0.94
2016-09-07T16:48:28.932984: step 1365, loss 0.0553472, acc 0.98
2016-09-07T16:48:29.587354: step 1366, loss 0.0249811, acc 1
2016-09-07T16:48:30.233871: step 1367, loss 0.0803262, acc 0.94
2016-09-07T16:48:30.899845: step 1368, loss 0.0486644, acc 0.98
2016-09-07T16:48:31.586514: step 1369, loss 0.0601278, acc 0.98
2016-09-07T16:48:32.260579: step 1370, loss 0.0215428, acc 0.98
2016-09-07T16:48:32.933560: step 1371, loss 0.180141, acc 0.86
2016-09-07T16:48:33.610587: step 1372, loss 0.100239, acc 0.96
2016-09-07T16:48:34.286086: step 1373, loss 0.0877701, acc 0.98
2016-09-07T16:48:34.975941: step 1374, loss 0.0598007, acc 0.96
2016-09-07T16:48:35.640410: step 1375, loss 0.0724489, acc 0.98
2016-09-07T16:48:36.304755: step 1376, loss 0.0232042, acc 1
2016-09-07T16:48:37.013864: step 1377, loss 0.0883636, acc 0.96
2016-09-07T16:48:37.753612: step 1378, loss 0.0489756, acc 0.98
2016-09-07T16:48:38.463773: step 1379, loss 0.225128, acc 0.92
2016-09-07T16:48:39.177016: step 1380, loss 0.06875, acc 0.96
2016-09-07T16:48:39.886037: step 1381, loss 0.021391, acc 1
2016-09-07T16:48:40.593329: step 1382, loss 0.0672438, acc 0.98
2016-09-07T16:48:41.276846: step 1383, loss 0.0223716, acc 1
2016-09-07T16:48:42.009395: step 1384, loss 0.0823393, acc 0.94
2016-09-07T16:48:42.713149: step 1385, loss 0.0332815, acc 0.98
2016-09-07T16:48:43.394437: step 1386, loss 0.119981, acc 0.92
2016-09-07T16:48:44.079036: step 1387, loss 0.132331, acc 0.92
2016-09-07T16:48:44.764478: step 1388, loss 0.0239239, acc 1
2016-09-07T16:48:45.420850: step 1389, loss 0.0551017, acc 0.98
2016-09-07T16:48:46.098330: step 1390, loss 0.075814, acc 1
2016-09-07T16:48:46.765764: step 1391, loss 0.0544876, acc 0.98
2016-09-07T16:48:47.429392: step 1392, loss 0.0956302, acc 0.96
2016-09-07T16:48:48.094299: step 1393, loss 0.0151105, acc 1
2016-09-07T16:48:48.774683: step 1394, loss 0.0593553, acc 0.98
2016-09-07T16:48:49.454805: step 1395, loss 0.0378128, acc 1
2016-09-07T16:48:50.128809: step 1396, loss 0.100494, acc 0.94
2016-09-07T16:48:50.796158: step 1397, loss 0.0203882, acc 1
2016-09-07T16:48:51.461277: step 1398, loss 0.09789, acc 0.96
2016-09-07T16:48:52.124706: step 1399, loss 0.0604243, acc 0.98
2016-09-07T16:48:52.788894: step 1400, loss 0.0575285, acc 0.96

Evaluation:
2016-09-07T16:48:55.869709: step 1400, loss 1.14664, acc 0.762

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-1400

2016-09-07T16:48:57.521446: step 1401, loss 0.137672, acc 0.96
2016-09-07T16:48:58.205419: step 1402, loss 0.0866476, acc 0.98
2016-09-07T16:48:58.882523: step 1403, loss 0.0515323, acc 0.98
2016-09-07T16:48:59.557776: step 1404, loss 0.0175522, acc 1
2016-09-07T16:49:00.272457: step 1405, loss 0.153436, acc 0.92
2016-09-07T16:49:00.930511: step 1406, loss 0.0818988, acc 0.94
2016-09-07T16:49:01.608312: step 1407, loss 0.0368984, acc 1
2016-09-07T16:49:02.268967: step 1408, loss 0.0763581, acc 0.96
2016-09-07T16:49:02.937445: step 1409, loss 0.0710376, acc 0.96
2016-09-07T16:49:03.619257: step 1410, loss 0.0254169, acc 0.98
2016-09-07T16:49:04.303827: step 1411, loss 0.0166576, acc 1
2016-09-07T16:49:05.010510: step 1412, loss 0.159709, acc 0.96
2016-09-07T16:49:05.672094: step 1413, loss 0.0120422, acc 1
2016-09-07T16:49:06.338231: step 1414, loss 0.0427405, acc 0.98
2016-09-07T16:49:07.039079: step 1415, loss 0.131197, acc 0.96
2016-09-07T16:49:07.744542: step 1416, loss 0.0168069, acc 1
2016-09-07T16:49:08.428330: step 1417, loss 0.0262913, acc 0.98
2016-09-07T16:49:09.116275: step 1418, loss 0.0400044, acc 0.98
2016-09-07T16:49:09.802092: step 1419, loss 0.0987857, acc 0.98
2016-09-07T16:49:10.468294: step 1420, loss 0.0683924, acc 0.96
2016-09-07T16:49:11.146167: step 1421, loss 0.126216, acc 0.94
2016-09-07T16:49:11.824385: step 1422, loss 0.155763, acc 0.92
2016-09-07T16:49:12.509836: step 1423, loss 0.0953627, acc 0.98
2016-09-07T16:49:13.185439: step 1424, loss 0.0729675, acc 0.98
2016-09-07T16:49:13.849362: step 1425, loss 0.0393038, acc 0.98
2016-09-07T16:49:14.517838: step 1426, loss 0.033674, acc 0.98
2016-09-07T16:49:15.190885: step 1427, loss 0.161896, acc 0.98
2016-09-07T16:49:15.887669: step 1428, loss 0.0840605, acc 0.94
2016-09-07T16:49:16.564417: step 1429, loss 0.162352, acc 0.94
2016-09-07T16:49:17.217349: step 1430, loss 0.0485791, acc 0.98
2016-09-07T16:49:17.881096: step 1431, loss 0.166413, acc 0.92
2016-09-07T16:49:18.552104: step 1432, loss 0.0486062, acc 0.96
2016-09-07T16:49:19.208526: step 1433, loss 0.0244052, acc 1
2016-09-07T16:49:19.885719: step 1434, loss 0.0731487, acc 0.98
2016-09-07T16:49:20.560448: step 1435, loss 0.116645, acc 0.96
2016-09-07T16:49:21.239306: step 1436, loss 0.0654717, acc 0.96
2016-09-07T16:49:21.921858: step 1437, loss 0.0910627, acc 0.98
2016-09-07T16:49:22.597729: step 1438, loss 0.0876769, acc 0.96
2016-09-07T16:49:23.263476: step 1439, loss 0.086314, acc 0.96
2016-09-07T16:49:23.918520: step 1440, loss 0.0611103, acc 1
2016-09-07T16:49:24.581494: step 1441, loss 0.0311535, acc 1
2016-09-07T16:49:25.238595: step 1442, loss 0.0450322, acc 0.98
2016-09-07T16:49:25.921328: step 1443, loss 0.0480528, acc 0.96
2016-09-07T16:49:26.587015: step 1444, loss 0.045674, acc 0.98
2016-09-07T16:49:27.263877: step 1445, loss 0.0639767, acc 0.96
2016-09-07T16:49:27.924116: step 1446, loss 0.0332523, acc 0.98
2016-09-07T16:49:28.576152: step 1447, loss 0.0293181, acc 1
2016-09-07T16:49:29.250536: step 1448, loss 0.281317, acc 0.92
2016-09-07T16:49:29.917076: step 1449, loss 0.273239, acc 0.92
2016-09-07T16:49:30.600795: step 1450, loss 0.152995, acc 0.9
2016-09-07T16:49:31.283177: step 1451, loss 0.0658043, acc 0.94
2016-09-07T16:49:31.944627: step 1452, loss 0.135919, acc 0.9
2016-09-07T16:49:32.618100: step 1453, loss 0.0804789, acc 0.98
2016-09-07T16:49:33.293018: step 1454, loss 0.0803619, acc 0.98
2016-09-07T16:49:33.975027: step 1455, loss 0.0320484, acc 1
2016-09-07T16:49:34.642982: step 1456, loss 0.103686, acc 0.96
2016-09-07T16:49:35.323382: step 1457, loss 0.145545, acc 0.92
2016-09-07T16:49:35.986403: step 1458, loss 0.0442827, acc 0.98
2016-09-07T16:49:36.674074: step 1459, loss 0.0463123, acc 1
2016-09-07T16:49:37.335946: step 1460, loss 0.0887723, acc 0.92
2016-09-07T16:49:38.029698: step 1461, loss 0.0361944, acc 1
2016-09-07T16:49:38.691537: step 1462, loss 0.0952315, acc 0.96
2016-09-07T16:49:39.374955: step 1463, loss 0.0661318, acc 0.96
2016-09-07T16:49:40.060795: step 1464, loss 0.170356, acc 0.94
2016-09-07T16:49:40.722000: step 1465, loss 0.073288, acc 0.96
2016-09-07T16:49:41.397750: step 1466, loss 0.0414703, acc 1
2016-09-07T16:49:42.072751: step 1467, loss 0.115353, acc 0.94
2016-09-07T16:49:42.772370: step 1468, loss 0.0773814, acc 0.98
2016-09-07T16:49:43.453086: step 1469, loss 0.111276, acc 0.9
2016-09-07T16:49:44.139896: step 1470, loss 0.00976149, acc 1
2016-09-07T16:49:44.812845: step 1471, loss 0.0697787, acc 0.96
2016-09-07T16:49:45.506192: step 1472, loss 0.138879, acc 0.96
2016-09-07T16:49:46.190878: step 1473, loss 0.0155522, acc 1
2016-09-07T16:49:46.877945: step 1474, loss 0.0891243, acc 0.98
2016-09-07T16:49:47.552779: step 1475, loss 0.0997062, acc 0.94
2016-09-07T16:49:48.224592: step 1476, loss 0.0733257, acc 0.96
2016-09-07T16:49:48.934226: step 1477, loss 0.0217781, acc 1
2016-09-07T16:49:49.608511: step 1478, loss 0.136616, acc 0.92
2016-09-07T16:49:50.295351: step 1479, loss 0.111991, acc 0.96
2016-09-07T16:49:50.978476: step 1480, loss 0.0572563, acc 0.96
2016-09-07T16:49:51.655399: step 1481, loss 0.111498, acc 0.94
2016-09-07T16:49:52.341828: step 1482, loss 0.0366303, acc 0.98
2016-09-07T16:49:53.014134: step 1483, loss 0.139251, acc 0.94
2016-09-07T16:49:53.694775: step 1484, loss 0.0826255, acc 0.94
2016-09-07T16:49:54.373489: step 1485, loss 0.120225, acc 0.94
2016-09-07T16:49:55.043844: step 1486, loss 0.0584467, acc 0.98
2016-09-07T16:49:55.724206: step 1487, loss 0.0312052, acc 1
2016-09-07T16:49:56.376627: step 1488, loss 0.0107006, acc 1
2016-09-07T16:49:57.053107: step 1489, loss 0.0984501, acc 0.96
2016-09-07T16:49:57.705087: step 1490, loss 0.071897, acc 0.96
2016-09-07T16:49:58.369723: step 1491, loss 0.0253289, acc 1
2016-09-07T16:49:59.039622: step 1492, loss 0.0668859, acc 0.96
2016-09-07T16:49:59.707847: step 1493, loss 0.0830838, acc 0.96
2016-09-07T16:50:00.393787: step 1494, loss 0.0530819, acc 0.98
2016-09-07T16:50:01.060231: step 1495, loss 0.0544233, acc 0.96
2016-09-07T16:50:01.730180: step 1496, loss 0.0314337, acc 1
2016-09-07T16:50:02.389340: step 1497, loss 0.0823382, acc 0.96
2016-09-07T16:50:03.061649: step 1498, loss 0.0270559, acc 0.98
2016-09-07T16:50:03.744156: step 1499, loss 0.220787, acc 0.92
2016-09-07T16:50:04.420813: step 1500, loss 0.0286327, acc 0.98

Evaluation:
2016-09-07T16:50:07.526407: step 1500, loss 1.1231, acc 0.767

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-1500

2016-09-07T16:50:09.186612: step 1501, loss 0.117995, acc 0.96
2016-09-07T16:50:09.862863: step 1502, loss 0.0126797, acc 1
2016-09-07T16:50:10.538457: step 1503, loss 0.0406466, acc 0.98
2016-09-07T16:50:11.186556: step 1504, loss 0.158546, acc 0.92
2016-09-07T16:50:11.849122: step 1505, loss 0.048638, acc 0.96
2016-09-07T16:50:12.508621: step 1506, loss 0.149507, acc 0.9
2016-09-07T16:50:13.170150: step 1507, loss 0.0538905, acc 0.98
2016-09-07T16:50:13.840653: step 1508, loss 0.00315432, acc 1
2016-09-07T16:50:14.511326: step 1509, loss 0.0497935, acc 0.96
2016-09-07T16:50:15.165364: step 1510, loss 0.0403045, acc 0.98
2016-09-07T16:50:15.833990: step 1511, loss 0.155678, acc 0.96
2016-09-07T16:50:16.504794: step 1512, loss 0.122842, acc 0.96
2016-09-07T16:50:17.197978: step 1513, loss 0.13541, acc 0.96
2016-09-07T16:50:17.889529: step 1514, loss 0.0237687, acc 0.98
2016-09-07T16:50:18.571325: step 1515, loss 0.0878977, acc 0.94
2016-09-07T16:50:19.243459: step 1516, loss 0.0944054, acc 0.98
2016-09-07T16:50:19.930795: step 1517, loss 0.0252992, acc 1
2016-09-07T16:50:20.611676: step 1518, loss 0.0524176, acc 0.96
2016-09-07T16:50:21.284291: step 1519, loss 0.0710972, acc 0.98
2016-09-07T16:50:21.952975: step 1520, loss 0.0564155, acc 0.94
2016-09-07T16:50:22.617335: step 1521, loss 0.0254748, acc 1
2016-09-07T16:50:23.289762: step 1522, loss 0.139235, acc 0.98
2016-09-07T16:50:23.980711: step 1523, loss 0.0395563, acc 1
2016-09-07T16:50:24.651471: step 1524, loss 0.174938, acc 0.92
2016-09-07T16:50:25.319438: step 1525, loss 0.0914312, acc 0.94
2016-09-07T16:50:25.995530: step 1526, loss 0.0221969, acc 1
2016-09-07T16:50:26.682477: step 1527, loss 0.256967, acc 0.92
2016-09-07T16:50:27.357392: step 1528, loss 0.0822203, acc 0.96
2016-09-07T16:50:28.043987: step 1529, loss 0.131265, acc 0.92
2016-09-07T16:50:28.719371: step 1530, loss 0.108176, acc 0.94
2016-09-07T16:50:29.387786: step 1531, loss 0.0704824, acc 0.96
2016-09-07T16:50:30.069126: step 1532, loss 0.116128, acc 0.92
2016-09-07T16:50:30.750206: step 1533, loss 0.0789417, acc 0.96
2016-09-07T16:50:31.459654: step 1534, loss 0.060778, acc 0.98
2016-09-07T16:50:32.135981: step 1535, loss 0.0491261, acc 1
2016-09-07T16:50:32.809340: step 1536, loss 0.0240245, acc 1
2016-09-07T16:50:33.506766: step 1537, loss 0.0342265, acc 1
2016-09-07T16:50:34.184711: step 1538, loss 0.102203, acc 0.96
2016-09-07T16:50:34.867157: step 1539, loss 0.102008, acc 0.94
2016-09-07T16:50:35.554559: step 1540, loss 0.0565487, acc 0.96
2016-09-07T16:50:36.227615: step 1541, loss 0.0573061, acc 1
2016-09-07T16:50:36.909529: step 1542, loss 0.0229509, acc 1
2016-09-07T16:50:37.571504: step 1543, loss 0.0482225, acc 0.96
2016-09-07T16:50:38.233875: step 1544, loss 0.0197552, acc 1
2016-09-07T16:50:38.920477: step 1545, loss 0.120342, acc 0.96
2016-09-07T16:50:39.609710: step 1546, loss 0.111196, acc 0.94
2016-09-07T16:50:40.282490: step 1547, loss 0.17789, acc 0.94
2016-09-07T16:50:40.952873: step 1548, loss 0.131563, acc 0.96
2016-09-07T16:50:41.639004: step 1549, loss 0.0180604, acc 1
2016-09-07T16:50:42.322027: step 1550, loss 0.0727142, acc 0.98
2016-09-07T16:50:42.991506: step 1551, loss 0.0736038, acc 0.96
2016-09-07T16:50:43.362353: step 1552, loss 0.00247312, acc 1
2016-09-07T16:50:44.044011: step 1553, loss 0.0467983, acc 1
2016-09-07T16:50:44.743468: step 1554, loss 0.0640219, acc 0.98
2016-09-07T16:50:45.461296: step 1555, loss 0.080351, acc 0.98
2016-09-07T16:50:46.133274: step 1556, loss 0.00435309, acc 1
2016-09-07T16:50:46.813855: step 1557, loss 0.0655377, acc 0.98
2016-09-07T16:50:47.490237: step 1558, loss 0.122421, acc 0.96
2016-09-07T16:50:48.162415: step 1559, loss 0.0300496, acc 1
2016-09-07T16:50:48.839536: step 1560, loss 0.0636421, acc 0.96
2016-09-07T16:50:49.541907: step 1561, loss 0.0220494, acc 1
2016-09-07T16:50:50.204339: step 1562, loss 0.0640757, acc 0.94
2016-09-07T16:50:50.898406: step 1563, loss 0.0489814, acc 0.96
2016-09-07T16:50:51.583573: step 1564, loss 0.077872, acc 0.96
2016-09-07T16:50:52.261507: step 1565, loss 0.137569, acc 0.94
2016-09-07T16:50:52.970466: step 1566, loss 0.0575996, acc 0.98
2016-09-07T16:50:53.664203: step 1567, loss 0.0354217, acc 0.96
2016-09-07T16:50:54.340340: step 1568, loss 0.0626697, acc 0.98
2016-09-07T16:50:55.004285: step 1569, loss 0.06249, acc 0.96
2016-09-07T16:50:55.662747: step 1570, loss 0.0270868, acc 0.98
2016-09-07T16:50:56.349567: step 1571, loss 0.0476826, acc 0.98
2016-09-07T16:50:57.015309: step 1572, loss 0.048634, acc 0.98
2016-09-07T16:50:57.709285: step 1573, loss 0.0408424, acc 0.98
2016-09-07T16:50:58.393529: step 1574, loss 0.0820898, acc 0.96
2016-09-07T16:50:59.067646: step 1575, loss 0.019346, acc 1
2016-09-07T16:50:59.736702: step 1576, loss 0.173169, acc 0.94
2016-09-07T16:51:00.417640: step 1577, loss 0.0360564, acc 0.98
2016-09-07T16:51:01.111265: step 1578, loss 0.00189564, acc 1
2016-09-07T16:51:01.779669: step 1579, loss 0.0282256, acc 0.98
2016-09-07T16:51:02.456956: step 1580, loss 0.157978, acc 0.96
2016-09-07T16:51:03.113209: step 1581, loss 0.0282209, acc 1
2016-09-07T16:51:03.764918: step 1582, loss 0.17937, acc 0.92
2016-09-07T16:51:04.436988: step 1583, loss 0.0394594, acc 0.98
2016-09-07T16:51:05.091541: step 1584, loss 0.0731794, acc 0.98
2016-09-07T16:51:05.753817: step 1585, loss 0.0769973, acc 0.98
2016-09-07T16:51:06.423553: step 1586, loss 0.0614278, acc 0.96
2016-09-07T16:51:07.087507: step 1587, loss 0.029785, acc 1
2016-09-07T16:51:07.765482: step 1588, loss 0.120625, acc 0.96
2016-09-07T16:51:08.432312: step 1589, loss 0.0495492, acc 0.98
2016-09-07T16:51:09.115405: step 1590, loss 0.0868344, acc 0.96
2016-09-07T16:51:09.819350: step 1591, loss 0.0734042, acc 0.94
2016-09-07T16:51:10.487774: step 1592, loss 0.0573143, acc 0.96
2016-09-07T16:51:11.197708: step 1593, loss 0.0744208, acc 0.96
2016-09-07T16:51:11.869558: step 1594, loss 0.0855414, acc 0.96
2016-09-07T16:51:12.621081: step 1595, loss 0.0523679, acc 0.98
2016-09-07T16:51:13.284123: step 1596, loss 0.0239818, acc 1
2016-09-07T16:51:13.984290: step 1597, loss 0.100338, acc 0.94
2016-09-07T16:51:14.670103: step 1598, loss 0.110912, acc 0.94
2016-09-07T16:51:15.358959: step 1599, loss 0.0252564, acc 1
2016-09-07T16:51:16.034608: step 1600, loss 0.0495035, acc 0.98

Evaluation:
2016-09-07T16:51:19.267908: step 1600, loss 1.18352, acc 0.762

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-1600

2016-09-07T16:51:20.895551: step 1601, loss 0.0120145, acc 1
2016-09-07T16:51:21.575899: step 1602, loss 0.0208227, acc 1
2016-09-07T16:51:22.255592: step 1603, loss 0.0349831, acc 1
2016-09-07T16:51:22.947661: step 1604, loss 0.102085, acc 0.92
2016-09-07T16:51:23.633202: step 1605, loss 0.0279422, acc 0.98
2016-09-07T16:51:24.315308: step 1606, loss 0.0873443, acc 0.96
2016-09-07T16:51:25.019859: step 1607, loss 0.0549567, acc 0.98
2016-09-07T16:51:25.818072: step 1608, loss 0.026325, acc 1
2016-09-07T16:51:26.480521: step 1609, loss 0.132684, acc 0.92
2016-09-07T16:51:27.151992: step 1610, loss 0.0128014, acc 1
2016-09-07T16:51:27.831477: step 1611, loss 0.0975762, acc 0.96
2016-09-07T16:51:28.522062: step 1612, loss 0.123069, acc 0.98
2016-09-07T16:51:29.194301: step 1613, loss 0.0394442, acc 0.98
2016-09-07T16:51:29.885436: step 1614, loss 0.0586927, acc 0.96
2016-09-07T16:51:30.561895: step 1615, loss 0.0363214, acc 0.98
2016-09-07T16:51:31.233314: step 1616, loss 0.0814723, acc 0.98
2016-09-07T16:51:31.901141: step 1617, loss 0.0388474, acc 0.98
2016-09-07T16:51:32.581253: step 1618, loss 0.146128, acc 0.96
2016-09-07T16:51:33.381023: step 1619, loss 0.140417, acc 0.92
2016-09-07T16:51:34.167165: step 1620, loss 0.0605802, acc 0.98
2016-09-07T16:51:34.852099: step 1621, loss 0.035835, acc 1
2016-09-07T16:51:35.521751: step 1622, loss 0.0825612, acc 0.94
2016-09-07T16:51:36.193736: step 1623, loss 0.0812102, acc 0.96
2016-09-07T16:51:36.872174: step 1624, loss 0.0337186, acc 0.98
2016-09-07T16:51:37.554253: step 1625, loss 0.02722, acc 0.98
2016-09-07T16:51:38.218416: step 1626, loss 0.0277956, acc 1
2016-09-07T16:51:38.897966: step 1627, loss 0.104992, acc 0.94
2016-09-07T16:51:39.596390: step 1628, loss 0.0474694, acc 1
2016-09-07T16:51:40.280703: step 1629, loss 0.0522986, acc 0.96
2016-09-07T16:51:40.959017: step 1630, loss 0.0995518, acc 0.96
2016-09-07T16:51:41.645021: step 1631, loss 0.0534674, acc 0.98
2016-09-07T16:51:42.322983: step 1632, loss 0.103332, acc 0.96
2016-09-07T16:51:43.001786: step 1633, loss 0.115321, acc 0.96
2016-09-07T16:51:43.670693: step 1634, loss 0.105182, acc 0.96
2016-09-07T16:51:44.352056: step 1635, loss 0.0456453, acc 0.96
2016-09-07T16:51:45.024509: step 1636, loss 0.0184691, acc 1
2016-09-07T16:51:45.690416: step 1637, loss 0.0287719, acc 0.98
2016-09-07T16:51:46.374404: step 1638, loss 0.0167027, acc 1
2016-09-07T16:51:47.058282: step 1639, loss 0.0404841, acc 1
2016-09-07T16:51:47.728918: step 1640, loss 0.0558164, acc 0.98
2016-09-07T16:51:48.396301: step 1641, loss 0.065821, acc 0.98
2016-09-07T16:51:49.097063: step 1642, loss 0.134182, acc 0.9
2016-09-07T16:51:49.766791: step 1643, loss 0.0918249, acc 0.94
2016-09-07T16:51:50.425216: step 1644, loss 0.0189693, acc 1
2016-09-07T16:51:51.093125: step 1645, loss 0.044101, acc 0.98
2016-09-07T16:51:51.762862: step 1646, loss 0.0964366, acc 0.96
2016-09-07T16:51:52.433091: step 1647, loss 0.04557, acc 0.96
2016-09-07T16:51:53.090725: step 1648, loss 0.0290286, acc 0.98
2016-09-07T16:51:53.743669: step 1649, loss 0.040149, acc 0.98
2016-09-07T16:51:54.408264: step 1650, loss 0.108285, acc 0.94
2016-09-07T16:51:55.061292: step 1651, loss 0.0560373, acc 0.96
2016-09-07T16:51:55.717957: step 1652, loss 0.02184, acc 1
2016-09-07T16:51:56.404188: step 1653, loss 0.00641153, acc 1
2016-09-07T16:51:57.083623: step 1654, loss 0.0268859, acc 1
2016-09-07T16:51:57.749927: step 1655, loss 0.140595, acc 0.96
2016-09-07T16:51:58.420919: step 1656, loss 0.0601927, acc 0.98
2016-09-07T16:51:59.091727: step 1657, loss 0.0221837, acc 0.98
2016-09-07T16:51:59.759735: step 1658, loss 0.16092, acc 0.94
2016-09-07T16:52:00.441773: step 1659, loss 0.0625216, acc 0.98
2016-09-07T16:52:01.130018: step 1660, loss 0.0231499, acc 1
2016-09-07T16:52:01.809207: step 1661, loss 0.07766, acc 0.94
2016-09-07T16:52:02.458233: step 1662, loss 0.00987042, acc 1
2016-09-07T16:52:03.122015: step 1663, loss 0.0677376, acc 0.96
2016-09-07T16:52:03.784915: step 1664, loss 0.00448349, acc 1
2016-09-07T16:52:04.438013: step 1665, loss 0.100883, acc 0.92
2016-09-07T16:52:05.109520: step 1666, loss 0.12313, acc 0.94
2016-09-07T16:52:05.793447: step 1667, loss 0.181653, acc 0.94
2016-09-07T16:52:06.464189: step 1668, loss 0.0611646, acc 0.96
2016-09-07T16:52:07.129005: step 1669, loss 0.0671084, acc 0.98
2016-09-07T16:52:07.804642: step 1670, loss 0.0584346, acc 0.98
2016-09-07T16:52:08.511832: step 1671, loss 0.0319704, acc 0.98
2016-09-07T16:52:09.190753: step 1672, loss 0.0667779, acc 0.98
2016-09-07T16:52:09.864384: step 1673, loss 0.173649, acc 0.96
2016-09-07T16:52:10.555653: step 1674, loss 0.109117, acc 0.94
2016-09-07T16:52:11.223183: step 1675, loss 0.0424016, acc 0.98
2016-09-07T16:52:11.891377: step 1676, loss 0.0882255, acc 0.96
2016-09-07T16:52:12.558510: step 1677, loss 0.0632291, acc 0.98
2016-09-07T16:52:13.247626: step 1678, loss 0.0551457, acc 0.98
2016-09-07T16:52:13.927923: step 1679, loss 0.0582508, acc 0.98
2016-09-07T16:52:14.613275: step 1680, loss 0.00441509, acc 1
2016-09-07T16:52:15.285642: step 1681, loss 0.141131, acc 0.98
2016-09-07T16:52:15.976546: step 1682, loss 0.0580716, acc 0.98
2016-09-07T16:52:16.652825: step 1683, loss 0.0528541, acc 0.98
2016-09-07T16:52:17.330909: step 1684, loss 0.111836, acc 0.96
2016-09-07T16:52:18.005839: step 1685, loss 0.109193, acc 0.92
2016-09-07T16:52:18.682889: step 1686, loss 0.0483784, acc 0.96
2016-09-07T16:52:19.361120: step 1687, loss 0.102443, acc 0.94
2016-09-07T16:52:20.034814: step 1688, loss 0.0205437, acc 0.98
2016-09-07T16:52:20.712866: step 1689, loss 0.113055, acc 0.98
2016-09-07T16:52:21.397537: step 1690, loss 0.0504117, acc 0.96
2016-09-07T16:52:22.059904: step 1691, loss 0.031298, acc 0.98
2016-09-07T16:52:22.739325: step 1692, loss 0.0691715, acc 0.94
2016-09-07T16:52:23.426247: step 1693, loss 0.174314, acc 0.96
2016-09-07T16:52:24.087983: step 1694, loss 0.110793, acc 0.96
2016-09-07T16:52:24.751731: step 1695, loss 0.0567799, acc 0.98
2016-09-07T16:52:25.412515: step 1696, loss 0.0563323, acc 0.98
2016-09-07T16:52:26.087109: step 1697, loss 0.0564683, acc 0.98
2016-09-07T16:52:26.751758: step 1698, loss 0.0700378, acc 0.98
2016-09-07T16:52:27.408200: step 1699, loss 0.106661, acc 0.96
2016-09-07T16:52:28.099420: step 1700, loss 0.0485879, acc 1

Evaluation:
2016-09-07T16:52:31.329163: step 1700, loss 1.21206, acc 0.763

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-1700

2016-09-07T16:52:33.110798: step 1701, loss 0.0675775, acc 0.94
2016-09-07T16:52:33.779034: step 1702, loss 0.0288102, acc 0.98
2016-09-07T16:52:34.460305: step 1703, loss 0.0515594, acc 0.98
2016-09-07T16:52:35.117194: step 1704, loss 0.0936418, acc 0.94
2016-09-07T16:52:35.779773: step 1705, loss 0.0548552, acc 0.98
2016-09-07T16:52:36.442210: step 1706, loss 0.203442, acc 0.92
2016-09-07T16:52:37.111634: step 1707, loss 0.105362, acc 0.92
2016-09-07T16:52:37.803688: step 1708, loss 0.0185959, acc 1
2016-09-07T16:52:38.473286: step 1709, loss 0.109351, acc 0.98
2016-09-07T16:52:39.165138: step 1710, loss 0.212414, acc 0.94
2016-09-07T16:52:39.822221: step 1711, loss 0.0842238, acc 0.96
2016-09-07T16:52:40.503208: step 1712, loss 0.0634066, acc 0.96
2016-09-07T16:52:41.177084: step 1713, loss 0.0448771, acc 0.98
2016-09-07T16:52:41.847877: step 1714, loss 0.0215173, acc 1
2016-09-07T16:52:42.528074: step 1715, loss 0.0582016, acc 0.96
2016-09-07T16:52:43.218019: step 1716, loss 0.0183137, acc 1
2016-09-07T16:52:43.882877: step 1717, loss 0.0410356, acc 0.98
2016-09-07T16:52:44.558339: step 1718, loss 0.105314, acc 0.94
2016-09-07T16:52:45.252461: step 1719, loss 0.0506061, acc 0.98
2016-09-07T16:52:45.936075: step 1720, loss 0.0870924, acc 0.94
2016-09-07T16:52:46.606264: step 1721, loss 0.186976, acc 0.96
2016-09-07T16:52:47.300581: step 1722, loss 0.0860034, acc 0.96
2016-09-07T16:52:47.966082: step 1723, loss 0.0985713, acc 0.96
2016-09-07T16:52:48.641149: step 1724, loss 0.13567, acc 0.94
2016-09-07T16:52:49.327309: step 1725, loss 0.107354, acc 0.94
2016-09-07T16:52:50.005103: step 1726, loss 0.0350224, acc 0.98
2016-09-07T16:52:50.716133: step 1727, loss 0.0500634, acc 0.98
2016-09-07T16:52:51.450671: step 1728, loss 0.0459033, acc 1
2016-09-07T16:52:52.133832: step 1729, loss 0.0808422, acc 0.94
2016-09-07T16:52:52.816441: step 1730, loss 0.0538914, acc 0.98
2016-09-07T16:52:53.494427: step 1731, loss 0.0286733, acc 1
2016-09-07T16:52:54.248399: step 1732, loss 0.0342246, acc 0.98
2016-09-07T16:52:54.906506: step 1733, loss 0.0706056, acc 0.96
2016-09-07T16:52:55.630676: step 1734, loss 0.0293113, acc 1
2016-09-07T16:52:56.301506: step 1735, loss 0.0227883, acc 1
2016-09-07T16:52:56.973158: step 1736, loss 0.04642, acc 0.98
2016-09-07T16:52:57.631536: step 1737, loss 0.0913227, acc 0.94
2016-09-07T16:52:58.290652: step 1738, loss 0.0422201, acc 0.98
2016-09-07T16:52:58.943931: step 1739, loss 0.125885, acc 0.96
2016-09-07T16:52:59.609938: step 1740, loss 0.049577, acc 0.98
2016-09-07T16:53:00.342549: step 1741, loss 0.0101355, acc 1
2016-09-07T16:53:01.016131: step 1742, loss 0.0907226, acc 0.98
2016-09-07T16:53:01.695020: step 1743, loss 0.215934, acc 0.92
2016-09-07T16:53:02.364311: step 1744, loss 0.0649784, acc 0.96
2016-09-07T16:53:03.044683: step 1745, loss 0.0606871, acc 0.96
2016-09-07T16:53:03.390792: step 1746, loss 0.00313425, acc 1
2016-09-07T16:53:04.094336: step 1747, loss 0.0780305, acc 0.98
2016-09-07T16:53:04.769824: step 1748, loss 0.154337, acc 0.98
2016-09-07T16:53:05.431700: step 1749, loss 0.131805, acc 0.96
2016-09-07T16:53:06.102085: step 1750, loss 0.110544, acc 0.98
2016-09-07T16:53:06.778631: step 1751, loss 0.0453995, acc 0.98
2016-09-07T16:53:07.453705: step 1752, loss 0.0209601, acc 1
2016-09-07T16:53:08.134371: step 1753, loss 0.0670474, acc 0.94
2016-09-07T16:53:08.799731: step 1754, loss 0.108253, acc 0.94
2016-09-07T16:53:09.472500: step 1755, loss 0.0219751, acc 1
2016-09-07T16:53:10.137290: step 1756, loss 0.0168524, acc 1
2016-09-07T16:53:10.807886: step 1757, loss 0.0226239, acc 1
2016-09-07T16:53:11.462269: step 1758, loss 0.113331, acc 0.96
2016-09-07T16:53:12.127505: step 1759, loss 0.0284886, acc 1
2016-09-07T16:53:12.808212: step 1760, loss 0.0420216, acc 0.98
2016-09-07T16:53:13.488397: step 1761, loss 0.00600513, acc 1
2016-09-07T16:53:14.162428: step 1762, loss 0.0638421, acc 0.96
2016-09-07T16:53:14.850856: step 1763, loss 0.0317652, acc 0.98
2016-09-07T16:53:15.527465: step 1764, loss 0.166162, acc 0.92
2016-09-07T16:53:16.188925: step 1765, loss 0.0652575, acc 0.96
2016-09-07T16:53:16.861119: step 1766, loss 0.0277703, acc 0.98
2016-09-07T16:53:17.538097: step 1767, loss 0.0717513, acc 0.96
2016-09-07T16:53:18.213982: step 1768, loss 0.0873282, acc 0.96
2016-09-07T16:53:18.886124: step 1769, loss 0.016438, acc 1
2016-09-07T16:53:19.551482: step 1770, loss 0.0703663, acc 0.98
2016-09-07T16:53:20.245055: step 1771, loss 0.0421017, acc 0.96
2016-09-07T16:53:20.934202: step 1772, loss 0.00272363, acc 1
2016-09-07T16:53:21.624551: step 1773, loss 0.0146362, acc 1
2016-09-07T16:53:22.309624: step 1774, loss 0.0747917, acc 0.98
2016-09-07T16:53:22.968953: step 1775, loss 0.0461283, acc 0.98
2016-09-07T16:53:23.633332: step 1776, loss 0.0109798, acc 1
2016-09-07T16:53:24.317079: step 1777, loss 0.0406742, acc 0.96
2016-09-07T16:53:24.998963: step 1778, loss 0.0748386, acc 0.96
2016-09-07T16:53:25.668796: step 1779, loss 0.0271171, acc 0.98
2016-09-07T16:53:26.343595: step 1780, loss 0.0394327, acc 0.96
2016-09-07T16:53:26.998626: step 1781, loss 0.0885551, acc 0.96
2016-09-07T16:53:27.653734: step 1782, loss 0.0474834, acc 1
2016-09-07T16:53:28.322538: step 1783, loss 0.0288465, acc 0.98
2016-09-07T16:53:28.989823: step 1784, loss 0.114181, acc 0.94
2016-09-07T16:53:29.651580: step 1785, loss 0.0112201, acc 1
2016-09-07T16:53:30.313490: step 1786, loss 0.067962, acc 0.98
2016-09-07T16:53:31.007710: step 1787, loss 0.05957, acc 0.98
2016-09-07T16:53:31.667870: step 1788, loss 0.0206024, acc 0.98
2016-09-07T16:53:32.355522: step 1789, loss 0.0251543, acc 1
2016-09-07T16:53:33.054650: step 1790, loss 0.00597907, acc 1
2016-09-07T16:53:33.728722: step 1791, loss 0.0352254, acc 0.98
2016-09-07T16:53:34.402023: step 1792, loss 0.136404, acc 0.96
2016-09-07T16:53:35.058897: step 1793, loss 0.0910998, acc 0.92
2016-09-07T16:53:35.746264: step 1794, loss 0.0712589, acc 0.96
2016-09-07T16:53:36.436725: step 1795, loss 0.01017, acc 1
2016-09-07T16:53:37.117947: step 1796, loss 0.0128364, acc 1
2016-09-07T16:53:37.784694: step 1797, loss 0.0217164, acc 0.98
2016-09-07T16:53:38.454275: step 1798, loss 0.0642656, acc 0.96
2016-09-07T16:53:39.122981: step 1799, loss 0.04252, acc 0.96
2016-09-07T16:53:39.801083: step 1800, loss 0.0163455, acc 1

Evaluation:
2016-09-07T16:53:43.031113: step 1800, loss 1.33723, acc 0.759

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-1800

2016-09-07T16:53:44.856633: step 1801, loss 0.0511134, acc 0.98
2016-09-07T16:53:45.527439: step 1802, loss 0.109761, acc 0.96
2016-09-07T16:53:46.196317: step 1803, loss 0.0496804, acc 0.98
2016-09-07T16:53:46.872916: step 1804, loss 0.0232392, acc 1
2016-09-07T16:53:47.535564: step 1805, loss 0.0394571, acc 0.98
2016-09-07T16:53:48.209389: step 1806, loss 0.0253719, acc 0.98
2016-09-07T16:53:48.876295: step 1807, loss 0.0611691, acc 0.96
2016-09-07T16:53:49.541678: step 1808, loss 0.0338394, acc 0.98
2016-09-07T16:53:50.199538: step 1809, loss 0.244858, acc 0.94
2016-09-07T16:53:50.859033: step 1810, loss 0.0258758, acc 1
2016-09-07T16:53:51.526473: step 1811, loss 0.0354472, acc 1
2016-09-07T16:53:52.200864: step 1812, loss 0.0854493, acc 0.98
2016-09-07T16:53:52.886836: step 1813, loss 0.00663851, acc 1
2016-09-07T16:53:53.553314: step 1814, loss 0.0428191, acc 0.98
2016-09-07T16:53:54.221050: step 1815, loss 0.0310151, acc 1
2016-09-07T16:53:54.897830: step 1816, loss 0.0115803, acc 1
2016-09-07T16:53:55.561684: step 1817, loss 0.0315251, acc 0.98
2016-09-07T16:53:56.239850: step 1818, loss 0.0231311, acc 0.98
2016-09-07T16:53:56.911407: step 1819, loss 0.0975502, acc 0.96
2016-09-07T16:53:57.568043: step 1820, loss 0.00829988, acc 1
2016-09-07T16:53:58.251077: step 1821, loss 0.0197334, acc 0.98
2016-09-07T16:53:59.021823: step 1822, loss 0.0413552, acc 0.96
2016-09-07T16:53:59.734691: step 1823, loss 0.0963332, acc 0.96
2016-09-07T16:54:00.423343: step 1824, loss 0.0182353, acc 1
2016-09-07T16:54:01.108055: step 1825, loss 0.117534, acc 0.98
2016-09-07T16:54:01.774096: step 1826, loss 0.048355, acc 0.98
2016-09-07T16:54:02.538888: step 1827, loss 0.10642, acc 0.96
2016-09-07T16:54:03.194950: step 1828, loss 0.0187459, acc 1
2016-09-07T16:54:03.885700: step 1829, loss 0.087592, acc 0.98
2016-09-07T16:54:04.549754: step 1830, loss 0.0233908, acc 1
2016-09-07T16:54:05.273920: step 1831, loss 0.0518646, acc 0.98
2016-09-07T16:54:05.960615: step 1832, loss 0.0400597, acc 0.98
2016-09-07T16:54:06.616840: step 1833, loss 0.0657024, acc 0.94
2016-09-07T16:54:07.271384: step 1834, loss 0.0385131, acc 0.98
2016-09-07T16:54:07.937984: step 1835, loss 0.0607164, acc 0.96
2016-09-07T16:54:08.605720: step 1836, loss 0.00815578, acc 1
2016-09-07T16:54:09.290579: step 1837, loss 0.0565315, acc 0.98
2016-09-07T16:54:09.969415: step 1838, loss 0.0310949, acc 0.98
2016-09-07T16:54:10.643243: step 1839, loss 0.0806904, acc 0.98
2016-09-07T16:54:11.392550: step 1840, loss 0.0123453, acc 1
2016-09-07T16:54:12.078761: step 1841, loss 0.0691483, acc 0.98
2016-09-07T16:54:12.771999: step 1842, loss 0.0300145, acc 0.98
2016-09-07T16:54:13.455986: step 1843, loss 0.0541063, acc 0.98
2016-09-07T16:54:14.146260: step 1844, loss 0.0881314, acc 0.96
2016-09-07T16:54:14.923477: step 1845, loss 0.00779699, acc 1
2016-09-07T16:54:15.635481: step 1846, loss 0.0619433, acc 0.96
2016-09-07T16:54:16.295852: step 1847, loss 0.0264438, acc 0.98
2016-09-07T16:54:16.979052: step 1848, loss 0.0209476, acc 0.98
2016-09-07T16:54:17.677080: step 1849, loss 0.0664008, acc 0.98
2016-09-07T16:54:18.350976: step 1850, loss 0.00640801, acc 1
2016-09-07T16:54:19.026385: step 1851, loss 0.0320194, acc 0.98
2016-09-07T16:54:19.703809: step 1852, loss 0.0496696, acc 0.98
2016-09-07T16:54:20.398967: step 1853, loss 0.108146, acc 0.94
2016-09-07T16:54:21.081655: step 1854, loss 0.112196, acc 0.98
2016-09-07T16:54:21.753843: step 1855, loss 0.0736515, acc 0.96
2016-09-07T16:54:22.431180: step 1856, loss 0.0649256, acc 0.96
2016-09-07T16:54:23.109815: step 1857, loss 0.216849, acc 0.92
2016-09-07T16:54:23.791632: step 1858, loss 0.011875, acc 1
2016-09-07T16:54:24.474826: step 1859, loss 0.0184423, acc 1
2016-09-07T16:54:25.162927: step 1860, loss 0.0983908, acc 0.96
2016-09-07T16:54:25.845339: step 1861, loss 0.0625551, acc 0.96
2016-09-07T16:54:26.540357: step 1862, loss 0.0818738, acc 0.96
2016-09-07T16:54:27.230445: step 1863, loss 0.130814, acc 0.96
2016-09-07T16:54:27.904420: step 1864, loss 0.0345634, acc 0.98
2016-09-07T16:54:28.588757: step 1865, loss 0.0375625, acc 0.98
2016-09-07T16:54:29.287142: step 1866, loss 0.084522, acc 0.96
2016-09-07T16:54:29.957699: step 1867, loss 0.0585008, acc 0.96
2016-09-07T16:54:30.618019: step 1868, loss 0.0880356, acc 0.96
2016-09-07T16:54:31.309400: step 1869, loss 0.0122817, acc 1
2016-09-07T16:54:32.025718: step 1870, loss 0.0352873, acc 0.98
2016-09-07T16:54:32.698425: step 1871, loss 0.168626, acc 0.96
2016-09-07T16:54:33.382334: step 1872, loss 0.026918, acc 0.98
2016-09-07T16:54:34.061864: step 1873, loss 0.145164, acc 0.98
2016-09-07T16:54:34.962454: step 1874, loss 0.0251714, acc 0.98
2016-09-07T16:54:35.771387: step 1875, loss 0.0514115, acc 0.96
2016-09-07T16:54:36.580561: step 1876, loss 0.0202412, acc 1
2016-09-07T16:54:37.349497: step 1877, loss 0.0428804, acc 0.98
2016-09-07T16:54:38.090846: step 1878, loss 0.056735, acc 0.98
2016-09-07T16:54:38.862488: step 1879, loss 0.051429, acc 0.98
2016-09-07T16:54:39.541383: step 1880, loss 0.0373833, acc 0.96
2016-09-07T16:54:40.245055: step 1881, loss 0.0402381, acc 0.98
2016-09-07T16:54:40.946179: step 1882, loss 0.0658828, acc 0.96
2016-09-07T16:54:41.601653: step 1883, loss 0.0966131, acc 0.94
2016-09-07T16:54:42.299739: step 1884, loss 0.0443504, acc 0.98
2016-09-07T16:54:43.046814: step 1885, loss 0.0495978, acc 0.98
2016-09-07T16:54:43.866102: step 1886, loss 0.0382265, acc 1
2016-09-07T16:54:44.548054: step 1887, loss 0.0412012, acc 0.98
2016-09-07T16:54:45.242535: step 1888, loss 0.0783187, acc 0.96
2016-09-07T16:54:45.914035: step 1889, loss 0.042249, acc 0.98
2016-09-07T16:54:46.600157: step 1890, loss 0.0387395, acc 0.98
2016-09-07T16:54:47.322421: step 1891, loss 0.134777, acc 0.96
2016-09-07T16:54:47.992725: step 1892, loss 0.0743939, acc 0.96
2016-09-07T16:54:48.674917: step 1893, loss 0.025222, acc 0.98
2016-09-07T16:54:49.336580: step 1894, loss 0.0404597, acc 0.98
2016-09-07T16:54:50.002785: step 1895, loss 0.102411, acc 0.94
2016-09-07T16:54:50.698915: step 1896, loss 0.0577599, acc 0.98
2016-09-07T16:54:51.380588: step 1897, loss 0.116297, acc 0.98
2016-09-07T16:54:52.050647: step 1898, loss 0.0523203, acc 0.98
2016-09-07T16:54:52.751824: step 1899, loss 0.140196, acc 0.94
2016-09-07T16:54:53.421698: step 1900, loss 0.125926, acc 0.98

Evaluation:
2016-09-07T16:54:56.937467: step 1900, loss 1.24858, acc 0.763

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-1900

2016-09-07T16:54:59.219434: step 1901, loss 0.039346, acc 1
2016-09-07T16:55:00.263678: step 1902, loss 0.0200984, acc 1
2016-09-07T16:55:01.302802: step 1903, loss 0.044534, acc 0.98
2016-09-07T16:55:02.294966: step 1904, loss 0.0686794, acc 0.98
2016-09-07T16:55:03.126480: step 1905, loss 0.00982799, acc 1
2016-09-07T16:55:04.117266: step 1906, loss 0.0415553, acc 1
2016-09-07T16:55:05.098830: step 1907, loss 0.0345143, acc 1
2016-09-07T16:55:06.166043: step 1908, loss 0.0130601, acc 1
2016-09-07T16:55:07.138676: step 1909, loss 0.0221674, acc 0.98
2016-09-07T16:55:08.193815: step 1910, loss 0.0416864, acc 0.98
2016-09-07T16:55:09.225728: step 1911, loss 0.0307212, acc 0.98
2016-09-07T16:55:10.314931: step 1912, loss 0.0958663, acc 0.96
2016-09-07T16:55:11.287082: step 1913, loss 0.00874009, acc 1
2016-09-07T16:55:12.257728: step 1914, loss 0.145139, acc 0.94
2016-09-07T16:55:13.205914: step 1915, loss 0.177916, acc 0.92
2016-09-07T16:55:14.107752: step 1916, loss 0.179157, acc 0.92
2016-09-07T16:55:14.928228: step 1917, loss 0.052043, acc 0.96
2016-09-07T16:55:15.992774: step 1918, loss 0.0682187, acc 0.98
2016-09-07T16:55:17.106223: step 1919, loss 0.0318063, acc 0.98
2016-09-07T16:55:18.169448: step 1920, loss 0.0184935, acc 1
2016-09-07T16:55:19.055301: step 1921, loss 0.0716346, acc 0.98
2016-09-07T16:55:20.081159: step 1922, loss 0.0645115, acc 0.98
2016-09-07T16:55:21.110966: step 1923, loss 0.0548198, acc 0.96
2016-09-07T16:55:22.036152: step 1924, loss 0.0306002, acc 0.98
2016-09-07T16:55:22.943984: step 1925, loss 0.0788464, acc 0.96
2016-09-07T16:55:23.794965: step 1926, loss 0.0713221, acc 0.98
2016-09-07T16:55:24.635914: step 1927, loss 0.137731, acc 0.88
2016-09-07T16:55:25.601165: step 1928, loss 0.111657, acc 0.98
2016-09-07T16:55:26.604717: step 1929, loss 0.0211061, acc 1
2016-09-07T16:55:27.676807: step 1930, loss 0.0989087, acc 0.96
2016-09-07T16:55:28.641410: step 1931, loss 0.00806865, acc 1
2016-09-07T16:55:29.475500: step 1932, loss 0.0441834, acc 0.98
2016-09-07T16:55:30.450648: step 1933, loss 0.0431944, acc 1
2016-09-07T16:55:31.397185: step 1934, loss 0.0324547, acc 0.98
2016-09-07T16:55:32.448622: step 1935, loss 0.0806927, acc 0.98
2016-09-07T16:55:33.367400: step 1936, loss 0.0916663, acc 0.94
2016-09-07T16:55:34.389315: step 1937, loss 0.0371489, acc 0.98
2016-09-07T16:55:35.361588: step 1938, loss 0.0182544, acc 1
2016-09-07T16:55:36.263544: step 1939, loss 0.0715224, acc 0.96
2016-09-07T16:55:36.701360: step 1940, loss 0.282958, acc 0.833333
2016-09-07T16:55:37.529292: step 1941, loss 0.0480176, acc 0.96
2016-09-07T16:55:38.354848: step 1942, loss 0.0454606, acc 0.98
2016-09-07T16:55:39.186828: step 1943, loss 0.0552568, acc 0.96
2016-09-07T16:55:40.000791: step 1944, loss 0.10136, acc 0.94
2016-09-07T16:55:40.841116: step 1945, loss 0.149291, acc 0.96
2016-09-07T16:55:41.644488: step 1946, loss 0.0569408, acc 0.98
2016-09-07T16:55:42.490375: step 1947, loss 0.122892, acc 0.92
2016-09-07T16:55:43.426271: step 1948, loss 0.0242751, acc 0.98
2016-09-07T16:55:44.300694: step 1949, loss 0.038985, acc 0.98
2016-09-07T16:55:45.159218: step 1950, loss 0.0403538, acc 0.98
2016-09-07T16:55:46.041964: step 1951, loss 0.0149636, acc 1
2016-09-07T16:55:46.874996: step 1952, loss 0.0464444, acc 0.96
2016-09-07T16:55:47.782900: step 1953, loss 0.0424488, acc 1
2016-09-07T16:55:48.627311: step 1954, loss 0.153733, acc 0.92
2016-09-07T16:55:49.465973: step 1955, loss 0.176457, acc 0.92
2016-09-07T16:55:50.335863: step 1956, loss 0.0231217, acc 0.98
2016-09-07T16:55:51.176221: step 1957, loss 0.0849035, acc 0.96
2016-09-07T16:55:52.003542: step 1958, loss 0.114703, acc 0.96
2016-09-07T16:55:52.827053: step 1959, loss 0.156879, acc 0.9
2016-09-07T16:55:53.655174: step 1960, loss 0.0372663, acc 0.98
2016-09-07T16:55:54.471575: step 1961, loss 0.0168027, acc 1
2016-09-07T16:55:55.312690: step 1962, loss 0.0457929, acc 0.98
2016-09-07T16:55:56.151039: step 1963, loss 0.0397733, acc 1
2016-09-07T16:55:56.940096: step 1964, loss 0.00443033, acc 1
2016-09-07T16:55:57.717083: step 1965, loss 0.024901, acc 1
2016-09-07T16:55:58.500836: step 1966, loss 0.0369974, acc 0.98
2016-09-07T16:55:59.308772: step 1967, loss 0.00736046, acc 1
2016-09-07T16:56:00.118916: step 1968, loss 0.0866494, acc 0.94
2016-09-07T16:56:00.952928: step 1969, loss 0.0586608, acc 0.98
2016-09-07T16:56:01.781239: step 1970, loss 0.0296538, acc 1
2016-09-07T16:56:02.588501: step 1971, loss 0.0160885, acc 1
2016-09-07T16:56:03.416809: step 1972, loss 0.139632, acc 0.94
2016-09-07T16:56:04.248692: step 1973, loss 0.0691335, acc 0.96
2016-09-07T16:56:05.075119: step 1974, loss 0.0141238, acc 1
2016-09-07T16:56:05.895219: step 1975, loss 0.116317, acc 0.96
2016-09-07T16:56:06.722203: step 1976, loss 0.128412, acc 0.96
2016-09-07T16:56:07.534358: step 1977, loss 0.0692172, acc 0.98
2016-09-07T16:56:08.372251: step 1978, loss 0.05891, acc 0.94
2016-09-07T16:56:09.171940: step 1979, loss 0.0592756, acc 0.96
2016-09-07T16:56:10.288415: step 1980, loss 0.0535559, acc 0.96
2016-09-07T16:56:11.364236: step 1981, loss 0.117994, acc 0.96
2016-09-07T16:56:12.418649: step 1982, loss 0.0663853, acc 0.96
2016-09-07T16:56:13.247735: step 1983, loss 0.00387628, acc 1
2016-09-07T16:56:14.137898: step 1984, loss 0.0121923, acc 1
2016-09-07T16:56:15.111795: step 1985, loss 0.0491367, acc 0.98
2016-09-07T16:56:16.232029: step 1986, loss 0.0931125, acc 0.94
2016-09-07T16:56:17.312988: step 1987, loss 0.0673408, acc 0.96
2016-09-07T16:56:18.259449: step 1988, loss 0.00602002, acc 1
2016-09-07T16:56:19.173061: step 1989, loss 0.155242, acc 0.96
2016-09-07T16:56:20.018330: step 1990, loss 0.044219, acc 0.98
2016-09-07T16:56:20.952640: step 1991, loss 0.0382663, acc 0.96
2016-09-07T16:56:21.835575: step 1992, loss 0.0573908, acc 0.96
2016-09-07T16:56:22.807830: step 1993, loss 0.0244514, acc 1
2016-09-07T16:56:23.672711: step 1994, loss 0.0510169, acc 0.98
2016-09-07T16:56:24.526445: step 1995, loss 0.0641407, acc 0.96
2016-09-07T16:56:25.437180: step 1996, loss 0.113155, acc 0.96
2016-09-07T16:56:26.267830: step 1997, loss 0.0966508, acc 0.94
2016-09-07T16:56:27.202519: step 1998, loss 0.00521486, acc 1
2016-09-07T16:56:28.052939: step 1999, loss 0.111608, acc 0.96
2016-09-07T16:56:29.182460: step 2000, loss 0.0547109, acc 0.98

Evaluation:
2016-09-07T16:56:34.094087: step 2000, loss 1.22178, acc 0.741

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-2000

2016-09-07T16:56:35.923060: step 2001, loss 0.0817644, acc 0.98
2016-09-07T16:56:36.661992: step 2002, loss 0.0452505, acc 0.98
2016-09-07T16:56:37.454635: step 2003, loss 0.0861861, acc 0.98
2016-09-07T16:56:38.301152: step 2004, loss 0.0633526, acc 0.96
2016-09-07T16:56:39.115326: step 2005, loss 0.0297046, acc 1
2016-09-07T16:56:39.954724: step 2006, loss 0.0687341, acc 1
2016-09-07T16:56:40.775116: step 2007, loss 0.0473902, acc 0.98
2016-09-07T16:56:41.603002: step 2008, loss 0.0606562, acc 0.98
2016-09-07T16:56:42.492328: step 2009, loss 0.0396617, acc 0.98
2016-09-07T16:56:43.480225: step 2010, loss 0.0843952, acc 0.96
2016-09-07T16:56:44.323075: step 2011, loss 0.019444, acc 1
2016-09-07T16:56:45.153297: step 2012, loss 0.036355, acc 1
2016-09-07T16:56:45.980636: step 2013, loss 0.00705565, acc 1
2016-09-07T16:56:46.850171: step 2014, loss 0.0306123, acc 0.98
2016-09-07T16:56:47.833631: step 2015, loss 0.0620067, acc 0.98
2016-09-07T16:56:48.710869: step 2016, loss 0.0406855, acc 0.96
2016-09-07T16:56:49.755990: step 2017, loss 0.0257241, acc 1
2016-09-07T16:56:51.015291: step 2018, loss 0.0224593, acc 0.98
2016-09-07T16:56:52.108075: step 2019, loss 0.00720217, acc 1
2016-09-07T16:56:53.195490: step 2020, loss 0.169627, acc 0.94
2016-09-07T16:56:54.058284: step 2021, loss 0.0789741, acc 0.98
2016-09-07T16:56:54.925273: step 2022, loss 0.00963499, acc 1
2016-09-07T16:56:55.752885: step 2023, loss 0.0739968, acc 0.94
2016-09-07T16:56:56.640911: step 2024, loss 0.0657017, acc 0.98
2016-09-07T16:56:57.463460: step 2025, loss 0.00863926, acc 1
2016-09-07T16:56:58.436412: step 2026, loss 0.0576868, acc 0.98
2016-09-07T16:56:59.443336: step 2027, loss 0.0139583, acc 1
2016-09-07T16:57:00.409074: step 2028, loss 0.164062, acc 0.96
2016-09-07T16:57:01.448957: step 2029, loss 0.0103814, acc 1
2016-09-07T16:57:02.469749: step 2030, loss 0.10693, acc 0.92
2016-09-07T16:57:03.430661: step 2031, loss 0.151745, acc 0.94
2016-09-07T16:57:04.327850: step 2032, loss 0.0277335, acc 1
2016-09-07T16:57:05.309902: step 2033, loss 0.0506042, acc 0.98
2016-09-07T16:57:06.175039: step 2034, loss 0.064263, acc 0.98
2016-09-07T16:57:06.989888: step 2035, loss 0.0569331, acc 0.96
2016-09-07T16:57:07.815203: step 2036, loss 0.0397628, acc 0.98
2016-09-07T16:57:08.799648: step 2037, loss 0.0456041, acc 0.96
2016-09-07T16:57:09.959581: step 2038, loss 0.0374713, acc 1
2016-09-07T16:57:11.009149: step 2039, loss 0.0845418, acc 0.96
2016-09-07T16:57:11.906006: step 2040, loss 0.141009, acc 0.94
2016-09-07T16:57:12.750728: step 2041, loss 0.0683575, acc 0.98
2016-09-07T16:57:13.649819: step 2042, loss 0.0772377, acc 0.98
2016-09-07T16:57:14.412986: step 2043, loss 0.0451547, acc 0.98
2016-09-07T16:57:15.221349: step 2044, loss 0.157041, acc 0.94
2016-09-07T16:57:16.041661: step 2045, loss 0.0200218, acc 1
2016-09-07T16:57:16.820535: step 2046, loss 0.131006, acc 0.94
2016-09-07T16:57:17.631665: step 2047, loss 0.0812184, acc 0.96
2016-09-07T16:57:18.456717: step 2048, loss 0.0229739, acc 0.98
2016-09-07T16:57:19.260091: step 2049, loss 0.0915249, acc 0.96
2016-09-07T16:57:20.083154: step 2050, loss 0.0277908, acc 0.98
2016-09-07T16:57:20.896460: step 2051, loss 0.0692606, acc 0.98
2016-09-07T16:57:21.686686: step 2052, loss 0.0286165, acc 0.98
2016-09-07T16:57:22.539112: step 2053, loss 0.0556346, acc 0.98
2016-09-07T16:57:23.382667: step 2054, loss 0.0449034, acc 0.98
2016-09-07T16:57:24.395097: step 2055, loss 0.0539335, acc 0.96
2016-09-07T16:57:25.402727: step 2056, loss 0.0603214, acc 0.96
2016-09-07T16:57:26.380184: step 2057, loss 0.00553725, acc 1
2016-09-07T16:57:27.342335: step 2058, loss 0.0573052, acc 0.96
2016-09-07T16:57:28.293851: step 2059, loss 0.0445738, acc 0.98
2016-09-07T16:57:29.119132: step 2060, loss 0.141664, acc 0.96
2016-09-07T16:57:30.025858: step 2061, loss 0.0260098, acc 0.98
2016-09-07T16:57:30.962046: step 2062, loss 0.0738208, acc 0.98
2016-09-07T16:57:31.989262: step 2063, loss 0.0266112, acc 1
2016-09-07T16:57:33.052868: step 2064, loss 0.104964, acc 0.96
2016-09-07T16:57:34.159445: step 2065, loss 0.0214142, acc 1
2016-09-07T16:57:35.283719: step 2066, loss 0.041672, acc 0.98
2016-09-07T16:57:36.134257: step 2067, loss 0.0498213, acc 0.96
2016-09-07T16:57:36.858217: step 2068, loss 0.0316185, acc 0.98
2016-09-07T16:57:37.616127: step 2069, loss 0.0407321, acc 1
2016-09-07T16:57:38.320341: step 2070, loss 0.0431759, acc 0.98
2016-09-07T16:57:39.003050: step 2071, loss 0.121537, acc 0.92
2016-09-07T16:57:39.793467: step 2072, loss 0.0271254, acc 1
2016-09-07T16:57:40.607836: step 2073, loss 0.0316119, acc 0.98
2016-09-07T16:57:41.485342: step 2074, loss 0.0196276, acc 0.98
2016-09-07T16:57:42.339463: step 2075, loss 0.0449445, acc 0.98
2016-09-07T16:57:43.104154: step 2076, loss 0.037715, acc 0.98
2016-09-07T16:57:43.856926: step 2077, loss 0.119615, acc 0.98
2016-09-07T16:57:44.595846: step 2078, loss 0.0133693, acc 1
2016-09-07T16:57:45.328700: step 2079, loss 0.0489511, acc 0.96
2016-09-07T16:57:46.107509: step 2080, loss 0.0214984, acc 1
2016-09-07T16:57:46.860704: step 2081, loss 0.0178937, acc 1
2016-09-07T16:57:47.549923: step 2082, loss 0.0239764, acc 1
2016-09-07T16:57:48.211676: step 2083, loss 0.0861625, acc 0.96
2016-09-07T16:57:48.881925: step 2084, loss 0.0241761, acc 1
2016-09-07T16:57:49.551153: step 2085, loss 0.0571273, acc 0.98
2016-09-07T16:57:50.234919: step 2086, loss 0.028443, acc 0.98
2016-09-07T16:57:50.912849: step 2087, loss 0.0614411, acc 0.98
2016-09-07T16:57:51.594155: step 2088, loss 0.0598632, acc 1
2016-09-07T16:57:52.340502: step 2089, loss 0.0887921, acc 0.96
2016-09-07T16:57:53.178092: step 2090, loss 0.0119053, acc 1
2016-09-07T16:57:54.088268: step 2091, loss 0.0619187, acc 0.98
2016-09-07T16:57:54.910241: step 2092, loss 0.038353, acc 1
2016-09-07T16:57:55.569318: step 2093, loss 0.0334268, acc 0.98
2016-09-07T16:57:56.326162: step 2094, loss 0.186047, acc 0.94
2016-09-07T16:57:57.007797: step 2095, loss 0.0642749, acc 0.96
2016-09-07T16:57:57.701689: step 2096, loss 0.00768079, acc 1
2016-09-07T16:57:58.400494: step 2097, loss 0.0042424, acc 1
2016-09-07T16:57:59.113589: step 2098, loss 0.0500303, acc 0.98
2016-09-07T16:57:59.782474: step 2099, loss 0.0751767, acc 0.98
2016-09-07T16:58:00.481319: step 2100, loss 0.0180148, acc 1

Evaluation:
2016-09-07T16:58:03.779585: step 2100, loss 1.45241, acc 0.75

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-2100

2016-09-07T16:58:05.591267: step 2101, loss 0.0336572, acc 0.96
2016-09-07T16:58:06.246822: step 2102, loss 0.0386618, acc 0.98
2016-09-07T16:58:06.904991: step 2103, loss 0.0156006, acc 1
2016-09-07T16:58:07.696736: step 2104, loss 0.0212961, acc 1
2016-09-07T16:58:08.547144: step 2105, loss 0.038361, acc 0.98
2016-09-07T16:58:09.327899: step 2106, loss 0.0598689, acc 0.96
2016-09-07T16:58:10.069430: step 2107, loss 0.0410052, acc 0.98
2016-09-07T16:58:10.914098: step 2108, loss 0.0153986, acc 1
2016-09-07T16:58:11.730049: step 2109, loss 0.0640127, acc 0.96
2016-09-07T16:58:12.549803: step 2110, loss 0.0655651, acc 0.94
2016-09-07T16:58:13.279994: step 2111, loss 0.0364862, acc 0.98
2016-09-07T16:58:14.032028: step 2112, loss 0.0533871, acc 0.96
2016-09-07T16:58:14.811195: step 2113, loss 0.0216396, acc 1
2016-09-07T16:58:15.594001: step 2114, loss 0.049001, acc 0.96
2016-09-07T16:58:16.402959: step 2115, loss 0.072596, acc 0.98
2016-09-07T16:58:17.165727: step 2116, loss 0.0273552, acc 0.98
2016-09-07T16:58:17.912730: step 2117, loss 0.12481, acc 0.96
2016-09-07T16:58:18.582268: step 2118, loss 0.0082439, acc 1
2016-09-07T16:58:19.298038: step 2119, loss 0.0468623, acc 1
2016-09-07T16:58:20.109649: step 2120, loss 0.0274788, acc 0.98
2016-09-07T16:58:20.935825: step 2121, loss 0.0515669, acc 0.98
2016-09-07T16:58:21.791376: step 2122, loss 0.0212202, acc 0.98
2016-09-07T16:58:22.600622: step 2123, loss 0.0588939, acc 0.98
2016-09-07T16:58:23.418845: step 2124, loss 0.0253145, acc 0.98
2016-09-07T16:58:24.210694: step 2125, loss 0.00959736, acc 1
2016-09-07T16:58:24.995204: step 2126, loss 0.0271968, acc 1
2016-09-07T16:58:25.755189: step 2127, loss 0.102179, acc 0.98
2016-09-07T16:58:26.424848: step 2128, loss 0.0424802, acc 0.98
2016-09-07T16:58:27.095278: step 2129, loss 0.0290722, acc 0.98
2016-09-07T16:58:27.748011: step 2130, loss 0.0952311, acc 0.98
2016-09-07T16:58:28.423990: step 2131, loss 0.0657353, acc 0.96
2016-09-07T16:58:29.104509: step 2132, loss 0.0754999, acc 0.96
2016-09-07T16:58:29.791980: step 2133, loss 0.00666467, acc 1
2016-09-07T16:58:30.152479: step 2134, loss 0.0977323, acc 0.916667
2016-09-07T16:58:30.832502: step 2135, loss 0.0184754, acc 1
2016-09-07T16:58:31.518262: step 2136, loss 0.0708542, acc 0.96
2016-09-07T16:58:32.226348: step 2137, loss 0.0238094, acc 0.98
2016-09-07T16:58:32.907541: step 2138, loss 0.0684916, acc 0.96
2016-09-07T16:58:33.611574: step 2139, loss 0.0888514, acc 0.98
2016-09-07T16:58:34.307778: step 2140, loss 0.026953, acc 1
2016-09-07T16:58:34.993024: step 2141, loss 0.0228777, acc 0.98
2016-09-07T16:58:35.653575: step 2142, loss 0.0176493, acc 1
2016-09-07T16:58:36.339139: step 2143, loss 0.0948579, acc 0.98
2016-09-07T16:58:37.010927: step 2144, loss 0.0339498, acc 0.98
2016-09-07T16:58:37.688400: step 2145, loss 0.0853321, acc 0.94
2016-09-07T16:58:38.372398: step 2146, loss 0.05416, acc 0.98
2016-09-07T16:58:39.048874: step 2147, loss 0.0480648, acc 1
2016-09-07T16:58:39.721069: step 2148, loss 0.0941332, acc 0.96
2016-09-07T16:58:40.404955: step 2149, loss 0.013259, acc 1
2016-09-07T16:58:41.084471: step 2150, loss 0.124409, acc 0.96
2016-09-07T16:58:41.767863: step 2151, loss 0.104994, acc 0.94
2016-09-07T16:58:42.414010: step 2152, loss 0.0193576, acc 0.98
2016-09-07T16:58:43.077916: step 2153, loss 0.0263688, acc 0.98
2016-09-07T16:58:43.745866: step 2154, loss 0.0339331, acc 0.98
2016-09-07T16:58:44.428202: step 2155, loss 0.0952206, acc 0.94
2016-09-07T16:58:45.101793: step 2156, loss 0.132706, acc 0.96
2016-09-07T16:58:45.781105: step 2157, loss 0.114889, acc 0.94
2016-09-07T16:58:46.452833: step 2158, loss 0.126817, acc 0.92
2016-09-07T16:58:47.136207: step 2159, loss 0.0559652, acc 1
2016-09-07T16:58:47.815139: step 2160, loss 0.0394778, acc 0.98
2016-09-07T16:58:48.494980: step 2161, loss 0.0203013, acc 1
2016-09-07T16:58:49.195648: step 2162, loss 0.0235538, acc 1
2016-09-07T16:58:49.873158: step 2163, loss 0.0222332, acc 0.98
2016-09-07T16:58:50.595187: step 2164, loss 0.0296865, acc 1
2016-09-07T16:58:51.406652: step 2165, loss 0.0981461, acc 0.94
2016-09-07T16:58:52.267452: step 2166, loss 0.0888665, acc 0.96
2016-09-07T16:58:53.056919: step 2167, loss 0.0157054, acc 1
2016-09-07T16:58:53.793924: step 2168, loss 0.0555084, acc 0.96
2016-09-07T16:58:54.543559: step 2169, loss 0.0285139, acc 1
2016-09-07T16:58:55.302591: step 2170, loss 0.0302185, acc 0.98
2016-09-07T16:58:56.112965: step 2171, loss 0.0148326, acc 1
2016-09-07T16:58:56.767344: step 2172, loss 0.0829569, acc 0.96
2016-09-07T16:58:57.430638: step 2173, loss 0.0639286, acc 0.98
2016-09-07T16:58:58.128172: step 2174, loss 0.0130781, acc 1
2016-09-07T16:58:58.897214: step 2175, loss 0.211474, acc 0.94
2016-09-07T16:58:59.575213: step 2176, loss 0.0753115, acc 0.98
2016-09-07T16:59:00.279712: step 2177, loss 0.0325133, acc 1
2016-09-07T16:59:00.999051: step 2178, loss 0.0121163, acc 1
2016-09-07T16:59:01.744715: step 2179, loss 0.0783747, acc 0.96
2016-09-07T16:59:02.674100: step 2180, loss 0.0436106, acc 0.98
2016-09-07T16:59:03.481225: step 2181, loss 0.0448357, acc 0.98
2016-09-07T16:59:04.250705: step 2182, loss 0.0516343, acc 0.98
2016-09-07T16:59:05.092812: step 2183, loss 0.0440978, acc 0.96
2016-09-07T16:59:06.019070: step 2184, loss 0.0591643, acc 0.98
2016-09-07T16:59:06.867922: step 2185, loss 0.0432609, acc 0.96
2016-09-07T16:59:07.748330: step 2186, loss 0.0361846, acc 0.98
2016-09-07T16:59:08.527404: step 2187, loss 0.0262593, acc 0.98
2016-09-07T16:59:09.235294: step 2188, loss 0.0461679, acc 0.98
2016-09-07T16:59:09.888016: step 2189, loss 0.0981125, acc 0.94
2016-09-07T16:59:10.559447: step 2190, loss 0.0546926, acc 0.96
2016-09-07T16:59:11.234351: step 2191, loss 0.0513967, acc 0.96
2016-09-07T16:59:11.922491: step 2192, loss 0.0886666, acc 0.96
2016-09-07T16:59:12.758940: step 2193, loss 0.0230976, acc 1
2016-09-07T16:59:13.560025: step 2194, loss 0.0374177, acc 0.98
2016-09-07T16:59:14.340426: step 2195, loss 0.0802507, acc 0.96
2016-09-07T16:59:15.063856: step 2196, loss 0.0251314, acc 0.98
2016-09-07T16:59:15.902920: step 2197, loss 0.0887759, acc 0.94
2016-09-07T16:59:16.668431: step 2198, loss 0.0638909, acc 0.98
2016-09-07T16:59:17.359694: step 2199, loss 0.0110742, acc 1
2016-09-07T16:59:18.139275: step 2200, loss 0.0646755, acc 0.98

Evaluation:
2016-09-07T16:59:22.232547: step 2200, loss 1.44677, acc 0.745

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-2200

2016-09-07T16:59:24.300382: step 2201, loss 0.00981828, acc 1
2016-09-07T16:59:24.979881: step 2202, loss 0.025705, acc 1
2016-09-07T16:59:25.675580: step 2203, loss 0.0175055, acc 1
2016-09-07T16:59:26.339365: step 2204, loss 0.0818072, acc 0.94
2016-09-07T16:59:27.008754: step 2205, loss 0.0247432, acc 0.98
2016-09-07T16:59:27.684578: step 2206, loss 0.010912, acc 1
2016-09-07T16:59:28.352462: step 2207, loss 0.0435404, acc 0.98
2016-09-07T16:59:29.085500: step 2208, loss 0.0175802, acc 1
2016-09-07T16:59:29.809686: step 2209, loss 0.0298427, acc 0.98
2016-09-07T16:59:30.530434: step 2210, loss 0.0543536, acc 0.96
2016-09-07T16:59:31.238524: step 2211, loss 0.0335511, acc 0.98
2016-09-07T16:59:31.928829: step 2212, loss 0.0504007, acc 0.96
2016-09-07T16:59:32.735710: step 2213, loss 0.0304794, acc 1
2016-09-07T16:59:33.548710: step 2214, loss 0.0373912, acc 0.98
2016-09-07T16:59:34.225806: step 2215, loss 0.0384075, acc 0.96
2016-09-07T16:59:34.909221: step 2216, loss 0.0487597, acc 0.98
2016-09-07T16:59:35.576429: step 2217, loss 0.0221412, acc 0.98
2016-09-07T16:59:36.254082: step 2218, loss 0.11295, acc 0.98
2016-09-07T16:59:36.938109: step 2219, loss 0.0106205, acc 1
2016-09-07T16:59:37.657579: step 2220, loss 0.023562, acc 1
2016-09-07T16:59:38.349526: step 2221, loss 0.129962, acc 0.96
2016-09-07T16:59:39.073504: step 2222, loss 0.00491622, acc 1
2016-09-07T16:59:39.749523: step 2223, loss 0.0549211, acc 0.96
2016-09-07T16:59:40.417758: step 2224, loss 0.0738226, acc 0.94
2016-09-07T16:59:41.139261: step 2225, loss 0.0189705, acc 0.98
2016-09-07T16:59:41.851388: step 2226, loss 0.0291346, acc 0.98
2016-09-07T16:59:42.640594: step 2227, loss 0.0642726, acc 0.96
2016-09-07T16:59:43.487438: step 2228, loss 0.0135255, acc 1
2016-09-07T16:59:44.228236: step 2229, loss 0.0213064, acc 1
2016-09-07T16:59:44.894577: step 2230, loss 0.037925, acc 0.98
2016-09-07T16:59:45.571446: step 2231, loss 0.0494968, acc 0.98
2016-09-07T16:59:46.249386: step 2232, loss 0.0451062, acc 0.96
2016-09-07T16:59:46.918936: step 2233, loss 0.0116566, acc 1
2016-09-07T16:59:47.679546: step 2234, loss 0.0221123, acc 1
2016-09-07T16:59:48.356208: step 2235, loss 0.0508669, acc 0.96
2016-09-07T16:59:49.034450: step 2236, loss 0.021899, acc 1
2016-09-07T16:59:49.727049: step 2237, loss 0.0625079, acc 0.96
2016-09-07T16:59:50.411949: step 2238, loss 0.0620749, acc 0.96
2016-09-07T16:59:51.110167: step 2239, loss 0.0322795, acc 0.98
2016-09-07T16:59:51.800992: step 2240, loss 0.062413, acc 0.96
2016-09-07T16:59:52.543787: step 2241, loss 0.0508776, acc 0.96
2016-09-07T16:59:53.316637: step 2242, loss 0.047108, acc 0.98
2016-09-07T16:59:54.021811: step 2243, loss 0.023431, acc 1
2016-09-07T16:59:54.754469: step 2244, loss 0.17501, acc 0.96
2016-09-07T16:59:55.520440: step 2245, loss 0.0494105, acc 1
2016-09-07T16:59:56.205027: step 2246, loss 0.00801922, acc 1
2016-09-07T16:59:56.942063: step 2247, loss 0.110981, acc 0.96
2016-09-07T16:59:57.685948: step 2248, loss 0.0314429, acc 1
2016-09-07T16:59:58.541934: step 2249, loss 0.0166193, acc 1
2016-09-07T16:59:59.292083: step 2250, loss 0.12749, acc 0.94
2016-09-07T17:00:00.097504: step 2251, loss 0.00724849, acc 1
2016-09-07T17:00:00.970722: step 2252, loss 0.0610721, acc 0.96
2016-09-07T17:00:01.647493: step 2253, loss 0.074619, acc 0.98
2016-09-07T17:00:02.378362: step 2254, loss 0.118831, acc 0.96
2016-09-07T17:00:03.136272: step 2255, loss 0.0473245, acc 0.96
2016-09-07T17:00:03.865690: step 2256, loss 0.0301097, acc 0.98
2016-09-07T17:00:04.631357: step 2257, loss 0.0105729, acc 1
2016-09-07T17:00:05.345958: step 2258, loss 0.0052605, acc 1
2016-09-07T17:00:06.065652: step 2259, loss 0.0523486, acc 0.98
2016-09-07T17:00:06.924754: step 2260, loss 0.0668326, acc 0.98
2016-09-07T17:00:07.594586: step 2261, loss 0.0249033, acc 0.98
2016-09-07T17:00:08.397970: step 2262, loss 0.00352926, acc 1
2016-09-07T17:00:09.179261: step 2263, loss 0.108152, acc 0.94
2016-09-07T17:00:09.994899: step 2264, loss 0.124051, acc 0.96
2016-09-07T17:00:10.780536: step 2265, loss 0.0269875, acc 0.98
2016-09-07T17:00:11.588737: step 2266, loss 0.0839885, acc 0.96
2016-09-07T17:00:12.385228: step 2267, loss 0.0632356, acc 0.96
2016-09-07T17:00:13.148213: step 2268, loss 0.0638251, acc 0.96
2016-09-07T17:00:13.865503: step 2269, loss 0.0110969, acc 1
2016-09-07T17:00:14.645761: step 2270, loss 0.0254925, acc 0.98
2016-09-07T17:00:15.418748: step 2271, loss 0.177273, acc 0.96
2016-09-07T17:00:16.208538: step 2272, loss 0.0544501, acc 0.96
2016-09-07T17:00:16.940160: step 2273, loss 0.0291457, acc 1
2016-09-07T17:00:17.649972: step 2274, loss 0.0510344, acc 0.98
2016-09-07T17:00:18.402510: step 2275, loss 0.0777601, acc 0.96
2016-09-07T17:00:19.065966: step 2276, loss 0.0658895, acc 0.96
2016-09-07T17:00:19.807220: step 2277, loss 0.035357, acc 0.98
2016-09-07T17:00:20.492073: step 2278, loss 0.015801, acc 0.98
2016-09-07T17:00:21.165928: step 2279, loss 0.0415633, acc 0.98
2016-09-07T17:00:21.850993: step 2280, loss 0.113179, acc 0.98
2016-09-07T17:00:22.532947: step 2281, loss 0.0345615, acc 0.98
2016-09-07T17:00:23.211759: step 2282, loss 0.0867868, acc 0.94
2016-09-07T17:00:23.976836: step 2283, loss 0.0887994, acc 0.96
2016-09-07T17:00:24.642183: step 2284, loss 0.0287688, acc 0.98
2016-09-07T17:00:25.378957: step 2285, loss 0.0363655, acc 0.98
2016-09-07T17:00:26.088536: step 2286, loss 0.0439744, acc 1
2016-09-07T17:00:26.775523: step 2287, loss 0.0590206, acc 0.96
2016-09-07T17:00:27.552328: step 2288, loss 0.0511677, acc 0.96
2016-09-07T17:00:28.355829: step 2289, loss 0.110369, acc 0.94
2016-09-07T17:00:29.166357: step 2290, loss 0.0595673, acc 0.96
2016-09-07T17:00:29.837015: step 2291, loss 0.0173516, acc 1
2016-09-07T17:00:30.609119: step 2292, loss 0.0254406, acc 0.98
2016-09-07T17:00:31.292337: step 2293, loss 0.0553108, acc 0.98
2016-09-07T17:00:31.992581: step 2294, loss 0.0414859, acc 0.98
2016-09-07T17:00:32.768721: step 2295, loss 0.0234393, acc 1
2016-09-07T17:00:33.525766: step 2296, loss 0.0652468, acc 0.96
2016-09-07T17:00:34.299061: step 2297, loss 0.143395, acc 0.98
2016-09-07T17:00:34.987011: step 2298, loss 0.0440097, acc 0.98
2016-09-07T17:00:35.663455: step 2299, loss 0.0230313, acc 0.98
2016-09-07T17:00:36.321467: step 2300, loss 0.17799, acc 0.9

Evaluation:
2016-09-07T17:00:39.818149: step 2300, loss 1.60768, acc 0.759

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-2300

2016-09-07T17:00:41.828286: step 2301, loss 0.056068, acc 0.96
2016-09-07T17:00:42.624181: step 2302, loss 0.0338493, acc 0.98
2016-09-07T17:00:43.444909: step 2303, loss 0.082384, acc 0.96
2016-09-07T17:00:44.160319: step 2304, loss 0.126151, acc 0.96
2016-09-07T17:00:44.872395: step 2305, loss 0.108199, acc 0.96
2016-09-07T17:00:45.546702: step 2306, loss 0.04695, acc 0.96
2016-09-07T17:00:46.215100: step 2307, loss 0.0286932, acc 0.98
2016-09-07T17:00:47.019429: step 2308, loss 0.0824166, acc 0.98
2016-09-07T17:00:47.750769: step 2309, loss 0.125268, acc 0.96
2016-09-07T17:00:48.562808: step 2310, loss 0.0569053, acc 0.96
2016-09-07T17:00:49.322581: step 2311, loss 0.066337, acc 0.94
2016-09-07T17:00:50.083037: step 2312, loss 0.0640077, acc 0.98
2016-09-07T17:00:50.839433: step 2313, loss 0.0548196, acc 0.96
2016-09-07T17:00:51.575820: step 2314, loss 0.0339938, acc 0.98
2016-09-07T17:00:52.406841: step 2315, loss 0.156778, acc 0.92
2016-09-07T17:00:53.232195: step 2316, loss 0.0182968, acc 1
2016-09-07T17:00:53.990460: step 2317, loss 0.0215865, acc 1
2016-09-07T17:00:54.728335: step 2318, loss 0.0227256, acc 1
2016-09-07T17:00:55.510915: step 2319, loss 0.0856749, acc 0.96
2016-09-07T17:00:56.284071: step 2320, loss 0.124832, acc 0.9
2016-09-07T17:00:57.042331: step 2321, loss 0.0632346, acc 0.96
2016-09-07T17:00:57.816306: step 2322, loss 0.00994426, acc 1
2016-09-07T17:00:58.603851: step 2323, loss 0.0447106, acc 1
2016-09-07T17:00:59.396392: step 2324, loss 0.054779, acc 0.96
2016-09-07T17:01:00.186740: step 2325, loss 0.073488, acc 0.94
2016-09-07T17:01:00.943700: step 2326, loss 0.0484381, acc 0.98
2016-09-07T17:01:01.640388: step 2327, loss 0.0111492, acc 1
2016-09-07T17:01:02.016604: step 2328, loss 0.105507, acc 0.916667
2016-09-07T17:01:02.757761: step 2329, loss 0.0439171, acc 0.98
2016-09-07T17:01:03.558941: step 2330, loss 0.108662, acc 0.96
2016-09-07T17:01:04.260971: step 2331, loss 0.0715296, acc 0.96
2016-09-07T17:01:04.998838: step 2332, loss 0.0258017, acc 1
2016-09-07T17:01:05.767359: step 2333, loss 0.101911, acc 0.96
2016-09-07T17:01:06.588386: step 2334, loss 0.0690396, acc 0.96
2016-09-07T17:01:07.423076: step 2335, loss 0.0569309, acc 0.98
2016-09-07T17:01:08.174897: step 2336, loss 0.0287956, acc 1
2016-09-07T17:01:08.996828: step 2337, loss 0.0324449, acc 1
2016-09-07T17:01:09.830105: step 2338, loss 0.0735361, acc 0.98
2016-09-07T17:01:10.634751: step 2339, loss 0.0312585, acc 0.98
2016-09-07T17:01:11.328482: step 2340, loss 0.0275529, acc 0.98
2016-09-07T17:01:12.146801: step 2341, loss 0.0201399, acc 1
2016-09-07T17:01:12.986273: step 2342, loss 0.0687756, acc 0.96
2016-09-07T17:01:13.705465: step 2343, loss 0.0346633, acc 0.98
2016-09-07T17:01:14.398475: step 2344, loss 0.00230385, acc 1
2016-09-07T17:01:15.075656: step 2345, loss 0.0582426, acc 0.96
2016-09-07T17:01:15.766045: step 2346, loss 0.221318, acc 0.96
2016-09-07T17:01:16.464379: step 2347, loss 0.05412, acc 0.98
2016-09-07T17:01:17.234499: step 2348, loss 0.00462774, acc 1
2016-09-07T17:01:17.977772: step 2349, loss 0.0201181, acc 0.98
2016-09-07T17:01:18.754708: step 2350, loss 0.0684992, acc 0.96
2016-09-07T17:01:19.525837: step 2351, loss 0.00543323, acc 1
2016-09-07T17:01:20.303385: step 2352, loss 0.0121685, acc 1
2016-09-07T17:01:21.064550: step 2353, loss 0.0451006, acc 0.98
2016-09-07T17:01:21.908192: step 2354, loss 0.0152131, acc 0.98
2016-09-07T17:01:22.680641: step 2355, loss 0.0355821, acc 0.98
2016-09-07T17:01:23.479290: step 2356, loss 0.00790823, acc 1
2016-09-07T17:01:24.287114: step 2357, loss 0.0803062, acc 0.94
2016-09-07T17:01:25.138454: step 2358, loss 0.0951484, acc 0.92
2016-09-07T17:01:25.872254: step 2359, loss 0.0297888, acc 0.98
2016-09-07T17:01:26.608956: step 2360, loss 0.0595653, acc 1
2016-09-07T17:01:27.434967: step 2361, loss 0.0510047, acc 0.96
2016-09-07T17:01:28.172049: step 2362, loss 0.0888947, acc 0.94
2016-09-07T17:01:28.916058: step 2363, loss 0.0157251, acc 0.98
2016-09-07T17:01:29.627284: step 2364, loss 0.0596728, acc 0.98
2016-09-07T17:01:30.371961: step 2365, loss 0.0359, acc 0.98
2016-09-07T17:01:31.094926: step 2366, loss 0.0951499, acc 0.96
2016-09-07T17:01:32.021363: step 2367, loss 0.0336544, acc 0.98
2016-09-07T17:01:32.900003: step 2368, loss 0.0202215, acc 0.98
2016-09-07T17:01:33.577316: step 2369, loss 0.0491648, acc 0.98
2016-09-07T17:01:34.284952: step 2370, loss 0.163909, acc 0.96
2016-09-07T17:01:35.153919: step 2371, loss 0.042347, acc 0.96
2016-09-07T17:01:36.004050: step 2372, loss 0.0484864, acc 0.96
2016-09-07T17:01:36.664002: step 2373, loss 0.0728984, acc 0.96
2016-09-07T17:01:37.316949: step 2374, loss 0.0228756, acc 0.98
2016-09-07T17:01:38.004270: step 2375, loss 0.0643178, acc 0.98
2016-09-07T17:01:38.728643: step 2376, loss 0.053538, acc 0.96
2016-09-07T17:01:39.541764: step 2377, loss 0.00667778, acc 1
2016-09-07T17:01:40.362621: step 2378, loss 0.0142858, acc 1
2016-09-07T17:01:41.100166: step 2379, loss 0.0486323, acc 0.98
2016-09-07T17:01:41.894194: step 2380, loss 0.0528841, acc 0.96
2016-09-07T17:01:42.586155: step 2381, loss 0.0208047, acc 1
2016-09-07T17:01:43.281541: step 2382, loss 0.00820412, acc 1
2016-09-07T17:01:43.950407: step 2383, loss 0.0373282, acc 0.98
2016-09-07T17:01:44.626540: step 2384, loss 0.0425917, acc 1
2016-09-07T17:01:45.307949: step 2385, loss 0.0114073, acc 1
2016-09-07T17:01:45.995562: step 2386, loss 0.0480414, acc 0.98
2016-09-07T17:01:46.722004: step 2387, loss 0.0388591, acc 0.98
2016-09-07T17:01:47.389839: step 2388, loss 0.0287431, acc 0.98
2016-09-07T17:01:48.071056: step 2389, loss 0.103163, acc 0.98
2016-09-07T17:01:48.748522: step 2390, loss 0.0157992, acc 1
2016-09-07T17:01:49.423873: step 2391, loss 0.0361535, acc 0.98
2016-09-07T17:01:50.102157: step 2392, loss 0.0407572, acc 0.98
2016-09-07T17:01:50.794083: step 2393, loss 0.0721671, acc 0.96
2016-09-07T17:01:51.460963: step 2394, loss 0.0383004, acc 0.98
2016-09-07T17:01:52.134306: step 2395, loss 0.0514453, acc 0.98
2016-09-07T17:01:52.823553: step 2396, loss 0.0136921, acc 1
2016-09-07T17:01:53.502940: step 2397, loss 0.0199045, acc 0.98
2016-09-07T17:01:54.182854: step 2398, loss 0.0806725, acc 0.96
2016-09-07T17:01:54.847863: step 2399, loss 0.141464, acc 0.98
2016-09-07T17:01:55.532650: step 2400, loss 0.0647976, acc 0.96

Evaluation:
2016-09-07T17:01:58.817400: step 2400, loss 1.71481, acc 0.741

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-2400

2016-09-07T17:02:00.819621: step 2401, loss 0.00825395, acc 1
2016-09-07T17:02:01.631525: step 2402, loss 0.0244965, acc 1
2016-09-07T17:02:02.381673: step 2403, loss 0.0307921, acc 0.98
2016-09-07T17:02:03.228876: step 2404, loss 0.153829, acc 0.94
2016-09-07T17:02:04.016953: step 2405, loss 0.0744346, acc 0.98
2016-09-07T17:02:04.686893: step 2406, loss 0.06321, acc 0.98
2016-09-07T17:02:05.394040: step 2407, loss 0.0558529, acc 0.96
2016-09-07T17:02:06.134297: step 2408, loss 0.0420655, acc 0.96
2016-09-07T17:02:06.859875: step 2409, loss 0.0560625, acc 0.98
2016-09-07T17:02:07.650123: step 2410, loss 0.000977662, acc 1
2016-09-07T17:02:08.358289: step 2411, loss 0.0399519, acc 0.98
2016-09-07T17:02:09.029410: step 2412, loss 0.00917939, acc 1
2016-09-07T17:02:09.700073: step 2413, loss 0.015561, acc 0.98
2016-09-07T17:02:10.388535: step 2414, loss 0.0291779, acc 1
2016-09-07T17:02:11.096786: step 2415, loss 0.0169396, acc 0.98
2016-09-07T17:02:11.770799: step 2416, loss 0.00958158, acc 1
2016-09-07T17:02:12.444588: step 2417, loss 0.0766153, acc 0.98
2016-09-07T17:02:13.113286: step 2418, loss 0.0307908, acc 1
2016-09-07T17:02:13.806866: step 2419, loss 0.1477, acc 0.98
2016-09-07T17:02:14.486293: step 2420, loss 0.0299495, acc 0.98
2016-09-07T17:02:15.175334: step 2421, loss 0.0445957, acc 0.98
2016-09-07T17:02:15.842772: step 2422, loss 0.0178977, acc 1
2016-09-07T17:02:16.627360: step 2423, loss 0.0310534, acc 0.98
2016-09-07T17:02:17.397581: step 2424, loss 0.0639007, acc 0.96
2016-09-07T17:02:18.156539: step 2425, loss 0.00796838, acc 1
2016-09-07T17:02:18.925367: step 2426, loss 0.112446, acc 0.92
2016-09-07T17:02:19.696609: step 2427, loss 0.0362401, acc 0.98
2016-09-07T17:02:20.458818: step 2428, loss 0.0354095, acc 0.98
2016-09-07T17:02:21.190870: step 2429, loss 0.0337641, acc 0.98
2016-09-07T17:02:21.946521: step 2430, loss 0.0529907, acc 0.96
2016-09-07T17:02:22.717960: step 2431, loss 0.020294, acc 1
2016-09-07T17:02:23.432103: step 2432, loss 0.0694015, acc 0.98
2016-09-07T17:02:24.140335: step 2433, loss 0.117466, acc 0.92
2016-09-07T17:02:24.833505: step 2434, loss 0.0997576, acc 0.98
2016-09-07T17:02:25.607804: step 2435, loss 0.123164, acc 0.96
2016-09-07T17:02:26.385396: step 2436, loss 0.0157434, acc 1
2016-09-07T17:02:27.064136: step 2437, loss 0.0517833, acc 1
2016-09-07T17:02:27.743513: step 2438, loss 0.0578869, acc 0.98
2016-09-07T17:02:28.506276: step 2439, loss 0.0514109, acc 0.98
2016-09-07T17:02:29.281650: step 2440, loss 0.0311897, acc 0.98
2016-09-07T17:02:30.114408: step 2441, loss 0.104636, acc 0.98
2016-09-07T17:02:30.971511: step 2442, loss 0.114109, acc 0.94
2016-09-07T17:02:31.757388: step 2443, loss 0.0399604, acc 0.98
2016-09-07T17:02:32.496543: step 2444, loss 0.109253, acc 0.98
2016-09-07T17:02:33.356982: step 2445, loss 0.0567551, acc 0.96
2016-09-07T17:02:34.117058: step 2446, loss 0.0357475, acc 0.96
2016-09-07T17:02:34.901862: step 2447, loss 0.0636994, acc 0.98
2016-09-07T17:02:35.666464: step 2448, loss 0.0801641, acc 0.96
2016-09-07T17:02:36.420841: step 2449, loss 0.0469418, acc 0.98
2016-09-07T17:02:37.271203: step 2450, loss 0.0570102, acc 0.98
2016-09-07T17:02:38.018197: step 2451, loss 0.00655855, acc 1
2016-09-07T17:02:38.760890: step 2452, loss 0.0396013, acc 0.98
2016-09-07T17:02:39.543388: step 2453, loss 0.0216311, acc 1
2016-09-07T17:02:40.267840: step 2454, loss 0.0147932, acc 1
2016-09-07T17:02:41.088232: step 2455, loss 0.0160362, acc 1
2016-09-07T17:02:41.770282: step 2456, loss 0.0123503, acc 1
2016-09-07T17:02:42.541088: step 2457, loss 0.119388, acc 0.96
2016-09-07T17:02:43.248530: step 2458, loss 0.0399315, acc 0.98
2016-09-07T17:02:43.910108: step 2459, loss 0.0132868, acc 1
2016-09-07T17:02:44.587721: step 2460, loss 0.0824756, acc 0.96
2016-09-07T17:02:45.245755: step 2461, loss 0.021735, acc 0.98
2016-09-07T17:02:45.908254: step 2462, loss 0.100979, acc 0.94
2016-09-07T17:02:46.607029: step 2463, loss 0.0291262, acc 0.98
2016-09-07T17:02:47.288109: step 2464, loss 0.0381005, acc 0.98
2016-09-07T17:02:47.967165: step 2465, loss 0.0192855, acc 1
2016-09-07T17:02:48.647159: step 2466, loss 0.0664051, acc 0.98
2016-09-07T17:02:49.343730: step 2467, loss 0.00533706, acc 1
2016-09-07T17:02:50.024403: step 2468, loss 0.0604857, acc 0.98
2016-09-07T17:02:50.704426: step 2469, loss 0.0869986, acc 0.98
2016-09-07T17:02:51.387785: step 2470, loss 0.0789887, acc 0.94
2016-09-07T17:02:52.072965: step 2471, loss 0.0273394, acc 0.98
2016-09-07T17:02:52.742453: step 2472, loss 0.0497797, acc 0.96
2016-09-07T17:02:53.438335: step 2473, loss 0.0476328, acc 0.98
2016-09-07T17:02:54.222864: step 2474, loss 0.0997, acc 0.9
2016-09-07T17:02:54.891080: step 2475, loss 0.0529284, acc 0.96
2016-09-07T17:02:55.664745: step 2476, loss 0.02455, acc 1
2016-09-07T17:02:56.331930: step 2477, loss 0.0182563, acc 1
2016-09-07T17:02:57.018190: step 2478, loss 0.0574571, acc 0.96
2016-09-07T17:02:57.693454: step 2479, loss 0.162575, acc 0.94
2016-09-07T17:02:58.381070: step 2480, loss 0.135989, acc 0.94
2016-09-07T17:02:59.175547: step 2481, loss 0.0361878, acc 0.98
2016-09-07T17:03:00.078590: step 2482, loss 0.0361803, acc 1
2016-09-07T17:03:00.776997: step 2483, loss 0.17716, acc 0.96
2016-09-07T17:03:01.557027: step 2484, loss 0.00189578, acc 1
2016-09-07T17:03:02.252701: step 2485, loss 0.0035716, acc 1
2016-09-07T17:03:02.987319: step 2486, loss 0.0628551, acc 0.96
2016-09-07T17:03:03.664894: step 2487, loss 0.0190864, acc 1
2016-09-07T17:03:04.417997: step 2488, loss 0.10041, acc 0.96
2016-09-07T17:03:05.104997: step 2489, loss 0.146167, acc 0.94
2016-09-07T17:03:05.774966: step 2490, loss 0.0191574, acc 1
2016-09-07T17:03:06.428605: step 2491, loss 0.0597509, acc 0.96
2016-09-07T17:03:07.266196: step 2492, loss 0.0449667, acc 0.98
2016-09-07T17:03:08.080408: step 2493, loss 0.0424618, acc 0.98
2016-09-07T17:03:09.000513: step 2494, loss 0.0365771, acc 0.98
2016-09-07T17:03:09.753574: step 2495, loss 0.0154928, acc 1
2016-09-07T17:03:10.548196: step 2496, loss 0.0206643, acc 0.98
2016-09-07T17:03:11.378486: step 2497, loss 0.0601527, acc 0.96
2016-09-07T17:03:12.204383: step 2498, loss 0.108932, acc 0.96
2016-09-07T17:03:13.037704: step 2499, loss 0.0353143, acc 0.98
2016-09-07T17:03:13.738206: step 2500, loss 0.0505034, acc 0.98

Evaluation:
2016-09-07T17:03:17.648225: step 2500, loss 1.55114, acc 0.732

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-2500

2016-09-07T17:03:19.699593: step 2501, loss 0.0127811, acc 1
2016-09-07T17:03:20.638766: step 2502, loss 0.0325823, acc 1
2016-09-07T17:03:21.506291: step 2503, loss 0.03704, acc 1
2016-09-07T17:03:22.175854: step 2504, loss 0.0344745, acc 0.98
2016-09-07T17:03:22.954259: step 2505, loss 0.0445024, acc 1
2016-09-07T17:03:23.846453: step 2506, loss 0.0329111, acc 0.96
2016-09-07T17:03:24.596317: step 2507, loss 0.0464216, acc 0.96
2016-09-07T17:03:25.279886: step 2508, loss 0.112398, acc 0.96
2016-09-07T17:03:25.977818: step 2509, loss 0.0847712, acc 0.98
2016-09-07T17:03:26.704831: step 2510, loss 0.0758204, acc 0.96
2016-09-07T17:03:27.493471: step 2511, loss 0.0154378, acc 1
2016-09-07T17:03:28.207141: step 2512, loss 0.0172684, acc 1
2016-09-07T17:03:28.939644: step 2513, loss 0.0728166, acc 0.98
2016-09-07T17:03:29.656174: step 2514, loss 0.0280345, acc 1
2016-09-07T17:03:30.416872: step 2515, loss 0.192328, acc 0.94
2016-09-07T17:03:31.219316: step 2516, loss 0.0227771, acc 0.98
2016-09-07T17:03:32.008320: step 2517, loss 0.0151244, acc 1
2016-09-07T17:03:32.758077: step 2518, loss 0.0214567, acc 1
2016-09-07T17:03:33.553091: step 2519, loss 0.0441237, acc 0.98
2016-09-07T17:03:34.311528: step 2520, loss 0.0528912, acc 0.96
2016-09-07T17:03:35.078407: step 2521, loss 0.0192349, acc 1
2016-09-07T17:03:35.460577: step 2522, loss 0.000833318, acc 1
2016-09-07T17:03:36.341069: step 2523, loss 0.0153658, acc 1
2016-09-07T17:03:37.102455: step 2524, loss 0.0222011, acc 1
2016-09-07T17:03:37.900911: step 2525, loss 0.0856759, acc 0.96
2016-09-07T17:03:38.642556: step 2526, loss 0.0142805, acc 1
2016-09-07T17:03:39.460114: step 2527, loss 0.00525783, acc 1
2016-09-07T17:03:40.203301: step 2528, loss 0.142995, acc 0.94
2016-09-07T17:03:40.927505: step 2529, loss 0.0147611, acc 1
2016-09-07T17:03:41.683710: step 2530, loss 0.0481024, acc 0.98
2016-09-07T17:03:42.389407: step 2531, loss 0.0149821, acc 1
2016-09-07T17:03:43.105048: step 2532, loss 0.0339234, acc 0.98
2016-09-07T17:03:43.877621: step 2533, loss 0.0417565, acc 0.98
2016-09-07T17:03:44.630512: step 2534, loss 0.00515352, acc 1
2016-09-07T17:03:45.404077: step 2535, loss 0.103046, acc 0.96
2016-09-07T17:03:46.198909: step 2536, loss 0.112723, acc 0.92
2016-09-07T17:03:46.903418: step 2537, loss 0.00665096, acc 1
2016-09-07T17:03:47.654054: step 2538, loss 0.00127538, acc 1
2016-09-07T17:03:48.404912: step 2539, loss 0.0467016, acc 0.96
2016-09-07T17:03:49.110274: step 2540, loss 0.00401883, acc 1
2016-09-07T17:03:49.896571: step 2541, loss 0.0103312, acc 1
2016-09-07T17:03:50.620175: step 2542, loss 0.0197675, acc 1
2016-09-07T17:03:51.376414: step 2543, loss 0.0254593, acc 0.98
2016-09-07T17:03:52.290204: step 2544, loss 0.0631798, acc 0.98
2016-09-07T17:03:53.094208: step 2545, loss 0.111366, acc 0.96
2016-09-07T17:03:53.869514: step 2546, loss 0.0114758, acc 1
2016-09-07T17:03:54.672480: step 2547, loss 0.0428657, acc 1
2016-09-07T17:03:55.365645: step 2548, loss 0.00467823, acc 1
2016-09-07T17:03:56.055496: step 2549, loss 0.0196813, acc 1
2016-09-07T17:03:56.728874: step 2550, loss 0.113546, acc 0.94
2016-09-07T17:03:57.415899: step 2551, loss 0.0709587, acc 0.98
2016-09-07T17:03:58.082101: step 2552, loss 0.00735367, acc 1
2016-09-07T17:03:58.750464: step 2553, loss 0.0578011, acc 0.98
2016-09-07T17:03:59.462850: step 2554, loss 0.0317699, acc 0.98
2016-09-07T17:04:00.139017: step 2555, loss 0.0941298, acc 0.96
2016-09-07T17:04:00.823667: step 2556, loss 0.00106475, acc 1
2016-09-07T17:04:01.509108: step 2557, loss 0.0246012, acc 1
2016-09-07T17:04:02.167425: step 2558, loss 0.0500057, acc 0.98
2016-09-07T17:04:02.852414: step 2559, loss 0.0377475, acc 0.98
2016-09-07T17:04:03.547328: step 2560, loss 0.107982, acc 0.92
2016-09-07T17:04:04.225080: step 2561, loss 0.0856661, acc 0.96
2016-09-07T17:04:04.897947: step 2562, loss 0.00654084, acc 1
2016-09-07T17:04:05.599132: step 2563, loss 0.0867719, acc 0.98
2016-09-07T17:04:06.284136: step 2564, loss 0.0218837, acc 1
2016-09-07T17:04:06.964526: step 2565, loss 0.136429, acc 0.98
2016-09-07T17:04:07.670537: step 2566, loss 0.0497604, acc 0.98
2016-09-07T17:04:08.351794: step 2567, loss 0.0490435, acc 0.96
2016-09-07T17:04:09.035787: step 2568, loss 0.103025, acc 0.98
2016-09-07T17:04:09.725039: step 2569, loss 0.0478586, acc 0.96
2016-09-07T17:04:10.422240: step 2570, loss 0.0107662, acc 1
2016-09-07T17:04:11.099184: step 2571, loss 0.0170481, acc 1
2016-09-07T17:04:11.767384: step 2572, loss 0.0165819, acc 1
2016-09-07T17:04:12.435368: step 2573, loss 0.0277834, acc 1
2016-09-07T17:04:13.123039: step 2574, loss 0.026356, acc 0.98
2016-09-07T17:04:13.802916: step 2575, loss 0.0166849, acc 0.98
2016-09-07T17:04:14.487974: step 2576, loss 0.00479719, acc 1
2016-09-07T17:04:15.165793: step 2577, loss 0.00265332, acc 1
2016-09-07T17:04:15.828078: step 2578, loss 0.012378, acc 1
2016-09-07T17:04:16.509213: step 2579, loss 0.083105, acc 0.98
2016-09-07T17:04:17.198757: step 2580, loss 0.0664084, acc 0.98
2016-09-07T17:04:17.871036: step 2581, loss 0.0176265, acc 1
2016-09-07T17:04:18.552903: step 2582, loss 0.0223664, acc 1
2016-09-07T17:04:19.227569: step 2583, loss 0.0216806, acc 1
2016-09-07T17:04:19.926521: step 2584, loss 0.0368195, acc 0.98
2016-09-07T17:04:20.593341: step 2585, loss 0.0442704, acc 1
2016-09-07T17:04:21.267268: step 2586, loss 0.0951999, acc 0.98
2016-09-07T17:04:21.942935: step 2587, loss 0.0662811, acc 0.96
2016-09-07T17:04:22.633207: step 2588, loss 0.0354734, acc 0.98
2016-09-07T17:04:23.294451: step 2589, loss 0.114368, acc 0.94
2016-09-07T17:04:23.970072: step 2590, loss 0.0106171, acc 1
2016-09-07T17:04:24.629405: step 2591, loss 0.0490926, acc 0.98
2016-09-07T17:04:25.309694: step 2592, loss 0.0489756, acc 0.98
2016-09-07T17:04:25.977945: step 2593, loss 0.0460254, acc 0.98
2016-09-07T17:04:26.651663: step 2594, loss 0.0381638, acc 1
2016-09-07T17:04:27.351031: step 2595, loss 0.0062294, acc 1
2016-09-07T17:04:28.008415: step 2596, loss 0.023065, acc 1
2016-09-07T17:04:28.687639: step 2597, loss 0.0273803, acc 0.98
2016-09-07T17:04:29.372226: step 2598, loss 0.0484755, acc 0.96
2016-09-07T17:04:30.046927: step 2599, loss 0.0156445, acc 1
2016-09-07T17:04:30.723956: step 2600, loss 0.00838907, acc 1

Evaluation:
2016-09-07T17:04:33.637076: step 2600, loss 1.35777, acc 0.738

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-2600

2016-09-07T17:04:35.222443: step 2601, loss 0.0386919, acc 0.96
2016-09-07T17:04:35.887862: step 2602, loss 0.0596627, acc 0.98
2016-09-07T17:04:36.560908: step 2603, loss 0.0811347, acc 0.96
2016-09-07T17:04:37.228223: step 2604, loss 0.0518122, acc 0.98
2016-09-07T17:04:37.903144: step 2605, loss 0.0070434, acc 1
2016-09-07T17:04:38.586657: step 2606, loss 0.00462366, acc 1
2016-09-07T17:04:39.282542: step 2607, loss 0.0555338, acc 0.98
2016-09-07T17:04:39.959158: step 2608, loss 0.00665524, acc 1
2016-09-07T17:04:40.635884: step 2609, loss 0.0535347, acc 0.96
2016-09-07T17:04:41.297668: step 2610, loss 0.0386901, acc 1
2016-09-07T17:04:41.980063: step 2611, loss 0.0388546, acc 0.98
2016-09-07T17:04:42.658587: step 2612, loss 0.0605656, acc 0.98
2016-09-07T17:04:43.335118: step 2613, loss 0.0612216, acc 0.98
2016-09-07T17:04:44.022699: step 2614, loss 0.0138739, acc 1
2016-09-07T17:04:44.683301: step 2615, loss 0.0172932, acc 1
2016-09-07T17:04:45.337522: step 2616, loss 0.00801052, acc 1
2016-09-07T17:04:45.997353: step 2617, loss 0.100037, acc 0.96
2016-09-07T17:04:46.686687: step 2618, loss 0.00465894, acc 1
2016-09-07T17:04:47.360155: step 2619, loss 0.0261707, acc 0.98
2016-09-07T17:04:48.038990: step 2620, loss 0.0389642, acc 1
2016-09-07T17:04:48.723807: step 2621, loss 0.0578265, acc 0.96
2016-09-07T17:04:49.390446: step 2622, loss 0.0604515, acc 0.98
2016-09-07T17:04:50.063710: step 2623, loss 0.0128347, acc 1
2016-09-07T17:04:50.739911: step 2624, loss 0.0128988, acc 1
2016-09-07T17:04:51.415677: step 2625, loss 0.0205674, acc 1
2016-09-07T17:04:52.076415: step 2626, loss 0.233659, acc 0.94
2016-09-07T17:04:52.762436: step 2627, loss 0.0355779, acc 0.98
2016-09-07T17:04:53.429932: step 2628, loss 0.0504801, acc 1
2016-09-07T17:04:54.096516: step 2629, loss 0.0275386, acc 1
2016-09-07T17:04:54.793332: step 2630, loss 0.0180739, acc 0.98
2016-09-07T17:04:55.466833: step 2631, loss 0.11402, acc 0.94
2016-09-07T17:04:56.146546: step 2632, loss 0.0368218, acc 0.98
2016-09-07T17:04:56.794761: step 2633, loss 0.193824, acc 0.9
2016-09-07T17:04:57.463741: step 2634, loss 0.0220946, acc 1
2016-09-07T17:04:58.126577: step 2635, loss 0.00413062, acc 1
2016-09-07T17:04:58.795023: step 2636, loss 0.0162093, acc 1
2016-09-07T17:04:59.480443: step 2637, loss 0.0327736, acc 1
2016-09-07T17:05:00.160226: step 2638, loss 0.134472, acc 0.9
2016-09-07T17:05:00.867924: step 2639, loss 0.0752148, acc 0.96
2016-09-07T17:05:01.536110: step 2640, loss 0.0416036, acc 1
2016-09-07T17:05:02.198521: step 2641, loss 0.00501736, acc 1
2016-09-07T17:05:02.969722: step 2642, loss 0.0451432, acc 0.98
2016-09-07T17:05:03.654913: step 2643, loss 0.0479983, acc 0.98
2016-09-07T17:05:04.325192: step 2644, loss 0.109228, acc 0.94
2016-09-07T17:05:05.007023: step 2645, loss 0.0614958, acc 0.96
2016-09-07T17:05:05.695386: step 2646, loss 0.00691655, acc 1
2016-09-07T17:05:06.388622: step 2647, loss 0.0217065, acc 1
2016-09-07T17:05:07.069661: step 2648, loss 0.0421903, acc 0.96
2016-09-07T17:05:07.733162: step 2649, loss 0.0522061, acc 0.98
2016-09-07T17:05:08.401988: step 2650, loss 0.162704, acc 0.98
2016-09-07T17:05:09.069595: step 2651, loss 0.007222, acc 1
2016-09-07T17:05:09.731866: step 2652, loss 0.0226605, acc 1
2016-09-07T17:05:10.414563: step 2653, loss 0.110684, acc 0.94
2016-09-07T17:05:11.218124: step 2654, loss 0.0177459, acc 1
2016-09-07T17:05:12.020400: step 2655, loss 0.0238899, acc 0.98
2016-09-07T17:05:12.760678: step 2656, loss 0.0787373, acc 0.98
2016-09-07T17:05:13.476753: step 2657, loss 0.0769084, acc 0.94
2016-09-07T17:05:14.214157: step 2658, loss 0.0295382, acc 0.98
2016-09-07T17:05:14.886165: step 2659, loss 0.0889146, acc 0.94
2016-09-07T17:05:15.616497: step 2660, loss 0.00253784, acc 1
2016-09-07T17:05:16.351062: step 2661, loss 0.114972, acc 0.94
2016-09-07T17:05:17.011922: step 2662, loss 0.0738307, acc 0.94
2016-09-07T17:05:17.740282: step 2663, loss 0.0801243, acc 0.96
2016-09-07T17:05:18.548290: step 2664, loss 0.0390031, acc 0.98
2016-09-07T17:05:19.386578: step 2665, loss 0.094232, acc 0.96
2016-09-07T17:05:20.181029: step 2666, loss 0.0189609, acc 1
2016-09-07T17:05:20.936325: step 2667, loss 0.0323663, acc 0.98
2016-09-07T17:05:21.663481: step 2668, loss 0.0767253, acc 0.96
2016-09-07T17:05:22.408731: step 2669, loss 0.0285701, acc 0.98
2016-09-07T17:05:23.142010: step 2670, loss 0.0638018, acc 0.96
2016-09-07T17:05:23.826489: step 2671, loss 0.0724272, acc 0.96
2016-09-07T17:05:24.506041: step 2672, loss 0.0381406, acc 0.98
2016-09-07T17:05:25.179274: step 2673, loss 0.0554754, acc 0.96
2016-09-07T17:05:25.877645: step 2674, loss 0.0418348, acc 0.98
2016-09-07T17:05:26.541525: step 2675, loss 0.0511236, acc 0.98
2016-09-07T17:05:27.214559: step 2676, loss 0.0337723, acc 1
2016-09-07T17:05:27.888076: step 2677, loss 0.0536406, acc 0.98
2016-09-07T17:05:28.574589: step 2678, loss 0.0161091, acc 1
2016-09-07T17:05:29.246509: step 2679, loss 0.0733549, acc 0.98
2016-09-07T17:05:29.911202: step 2680, loss 0.00911725, acc 1
2016-09-07T17:05:30.600857: step 2681, loss 0.00504402, acc 1
2016-09-07T17:05:31.283783: step 2682, loss 0.130297, acc 0.96
2016-09-07T17:05:31.974043: step 2683, loss 0.0293799, acc 1
2016-09-07T17:05:32.677220: step 2684, loss 0.0122508, acc 1
2016-09-07T17:05:33.354019: step 2685, loss 0.0029966, acc 1
2016-09-07T17:05:34.024391: step 2686, loss 0.0286274, acc 1
2016-09-07T17:05:34.719807: step 2687, loss 0.0495956, acc 0.98
2016-09-07T17:05:35.399288: step 2688, loss 0.02214, acc 0.98
2016-09-07T17:05:36.086664: step 2689, loss 0.00969427, acc 1
2016-09-07T17:05:36.757264: step 2690, loss 0.0319181, acc 0.98
2016-09-07T17:05:37.421213: step 2691, loss 0.0169202, acc 1
2016-09-07T17:05:38.096198: step 2692, loss 0.0623243, acc 0.98
2016-09-07T17:05:38.766104: step 2693, loss 0.0923279, acc 0.98
2016-09-07T17:05:39.439484: step 2694, loss 0.021915, acc 0.98
2016-09-07T17:05:40.123161: step 2695, loss 0.0070551, acc 1
2016-09-07T17:05:40.868832: step 2696, loss 0.0437728, acc 0.98
2016-09-07T17:05:41.533631: step 2697, loss 0.0848396, acc 0.96
2016-09-07T17:05:42.203161: step 2698, loss 0.0514444, acc 0.98
2016-09-07T17:05:42.873013: step 2699, loss 0.114795, acc 0.92
2016-09-07T17:05:43.532563: step 2700, loss 0.0196742, acc 1

Evaluation:
2016-09-07T17:05:46.467417: step 2700, loss 1.64431, acc 0.756

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-2700

2016-09-07T17:05:48.066809: step 2701, loss 0.035828, acc 1
2016-09-07T17:05:48.752632: step 2702, loss 0.0624488, acc 0.96
2016-09-07T17:05:49.432786: step 2703, loss 0.0490098, acc 0.98
2016-09-07T17:05:50.104831: step 2704, loss 0.00400102, acc 1
2016-09-07T17:05:50.795930: step 2705, loss 0.109092, acc 0.96
2016-09-07T17:05:51.461410: step 2706, loss 0.0255892, acc 0.98
2016-09-07T17:05:52.119686: step 2707, loss 0.0647468, acc 0.96
2016-09-07T17:05:52.791157: step 2708, loss 0.0850225, acc 0.98
2016-09-07T17:05:53.467387: step 2709, loss 0.0818682, acc 0.96
2016-09-07T17:05:54.152496: step 2710, loss 0.000687358, acc 1
2016-09-07T17:05:54.837034: step 2711, loss 0.0354532, acc 0.98
2016-09-07T17:05:55.551517: step 2712, loss 0.0490704, acc 0.98
2016-09-07T17:05:56.233948: step 2713, loss 0.019719, acc 1
2016-09-07T17:05:56.909038: step 2714, loss 0.0559984, acc 0.96
2016-09-07T17:05:57.590977: step 2715, loss 0.0264786, acc 0.98
2016-09-07T17:05:57.939451: step 2716, loss 0.122193, acc 0.916667
2016-09-07T17:05:58.632837: step 2717, loss 0.0402968, acc 0.98
2016-09-07T17:05:59.362724: step 2718, loss 0.00987117, acc 1
2016-09-07T17:06:00.024746: step 2719, loss 0.0770111, acc 0.94
2016-09-07T17:06:00.711160: step 2720, loss 0.00957639, acc 1
2016-09-07T17:06:01.404961: step 2721, loss 0.0345102, acc 0.98
2016-09-07T17:06:02.091314: step 2722, loss 0.045343, acc 0.98
2016-09-07T17:06:02.785343: step 2723, loss 0.00724809, acc 1
2016-09-07T17:06:03.462169: step 2724, loss 0.018687, acc 1
2016-09-07T17:06:04.134266: step 2725, loss 0.00338713, acc 1
2016-09-07T17:06:04.792957: step 2726, loss 0.00621603, acc 1
2016-09-07T17:06:05.472667: step 2727, loss 0.0341395, acc 0.98
2016-09-07T17:06:06.142452: step 2728, loss 0.0414475, acc 0.96
2016-09-07T17:06:06.806057: step 2729, loss 0.0553088, acc 0.94
2016-09-07T17:06:07.480746: step 2730, loss 0.00101934, acc 1
2016-09-07T17:06:08.134806: step 2731, loss 0.00332127, acc 1
2016-09-07T17:06:08.787405: step 2732, loss 0.128578, acc 0.98
2016-09-07T17:06:09.433029: step 2733, loss 0.00404714, acc 1
2016-09-07T17:06:10.100562: step 2734, loss 0.0620214, acc 0.96
2016-09-07T17:06:10.785748: step 2735, loss 0.0429233, acc 0.96
2016-09-07T17:06:11.457996: step 2736, loss 0.215127, acc 0.92
2016-09-07T17:06:12.130659: step 2737, loss 0.0183671, acc 1
2016-09-07T17:06:12.800749: step 2738, loss 0.0252176, acc 1
2016-09-07T17:06:13.468167: step 2739, loss 0.0233698, acc 1
2016-09-07T17:06:14.134455: step 2740, loss 0.13179, acc 0.92
2016-09-07T17:06:14.798121: step 2741, loss 0.0461549, acc 0.98
2016-09-07T17:06:15.509643: step 2742, loss 0.0206965, acc 1
2016-09-07T17:06:16.251723: step 2743, loss 0.0569125, acc 0.96
2016-09-07T17:06:16.905976: step 2744, loss 0.0123372, acc 1
2016-09-07T17:06:17.583797: step 2745, loss 0.00873363, acc 1
2016-09-07T17:06:18.280162: step 2746, loss 0.0139582, acc 1
2016-09-07T17:06:18.973104: step 2747, loss 0.106304, acc 0.94
2016-09-07T17:06:19.665007: step 2748, loss 0.146406, acc 0.94
2016-09-07T17:06:20.359864: step 2749, loss 0.0565955, acc 0.98
2016-09-07T17:06:21.041086: step 2750, loss 0.134775, acc 0.94
2016-09-07T17:06:21.710590: step 2751, loss 0.0669628, acc 0.98
2016-09-07T17:06:22.391743: step 2752, loss 0.0413931, acc 0.98
2016-09-07T17:06:23.071273: step 2753, loss 0.0318945, acc 1
2016-09-07T17:06:23.785006: step 2754, loss 0.0384473, acc 0.98
2016-09-07T17:06:24.482284: step 2755, loss 0.267446, acc 0.88
2016-09-07T17:06:25.153704: step 2756, loss 0.00464194, acc 1
2016-09-07T17:06:25.809803: step 2757, loss 0.122889, acc 0.94
2016-09-07T17:06:26.474702: step 2758, loss 0.193704, acc 0.98
2016-09-07T17:06:27.150845: step 2759, loss 0.0602878, acc 0.98
2016-09-07T17:06:27.806973: step 2760, loss 0.0544479, acc 0.96
2016-09-07T17:06:28.488852: step 2761, loss 0.0429642, acc 0.98
2016-09-07T17:06:29.146109: step 2762, loss 0.0996319, acc 0.96
2016-09-07T17:06:29.803311: step 2763, loss 0.0376007, acc 1
2016-09-07T17:06:30.483655: step 2764, loss 0.0482012, acc 0.96
2016-09-07T17:06:31.137751: step 2765, loss 0.0256358, acc 1
2016-09-07T17:06:31.816669: step 2766, loss 0.0235589, acc 0.98
2016-09-07T17:06:32.495673: step 2767, loss 0.0590928, acc 0.98
2016-09-07T17:06:33.149886: step 2768, loss 0.052165, acc 0.98
2016-09-07T17:06:33.823970: step 2769, loss 0.0181296, acc 1
2016-09-07T17:06:34.508567: step 2770, loss 0.0177671, acc 0.98
2016-09-07T17:06:35.191683: step 2771, loss 0.0198581, acc 1
2016-09-07T17:06:35.869198: step 2772, loss 0.0250406, acc 1
2016-09-07T17:06:36.533446: step 2773, loss 0.0330935, acc 1
2016-09-07T17:06:37.203752: step 2774, loss 0.0025224, acc 1
2016-09-07T17:06:37.894486: step 2775, loss 0.0129972, acc 1
2016-09-07T17:06:38.571877: step 2776, loss 0.0192783, acc 1
2016-09-07T17:06:39.258901: step 2777, loss 0.0455752, acc 0.98
2016-09-07T17:06:39.934222: step 2778, loss 0.0195468, acc 0.98
2016-09-07T17:06:40.600899: step 2779, loss 0.0555402, acc 0.98
2016-09-07T17:06:41.263489: step 2780, loss 0.119353, acc 0.92
2016-09-07T17:06:41.931433: step 2781, loss 0.0694861, acc 0.96
2016-09-07T17:06:42.605189: step 2782, loss 0.0324176, acc 0.98
2016-09-07T17:06:43.272280: step 2783, loss 0.0601891, acc 0.96
2016-09-07T17:06:43.944037: step 2784, loss 0.0340278, acc 0.98
2016-09-07T17:06:44.603395: step 2785, loss 0.0162405, acc 1
2016-09-07T17:06:45.282450: step 2786, loss 0.026161, acc 0.98
2016-09-07T17:06:45.964413: step 2787, loss 0.0160785, acc 1
2016-09-07T17:06:46.640005: step 2788, loss 0.00621615, acc 1
2016-09-07T17:06:47.296785: step 2789, loss 0.0189426, acc 1
2016-09-07T17:06:47.949821: step 2790, loss 0.0376357, acc 0.98
2016-09-07T17:06:48.607932: step 2791, loss 0.07242, acc 0.96
2016-09-07T17:06:49.260304: step 2792, loss 0.0808033, acc 0.96
2016-09-07T17:06:49.929961: step 2793, loss 0.0199302, acc 1
2016-09-07T17:06:50.599022: step 2794, loss 0.0248192, acc 0.98
2016-09-07T17:06:51.280443: step 2795, loss 0.0417009, acc 1
2016-09-07T17:06:51.935528: step 2796, loss 0.0425394, acc 0.98
2016-09-07T17:06:52.608444: step 2797, loss 0.0197008, acc 1
2016-09-07T17:06:53.281874: step 2798, loss 0.0216964, acc 0.98
2016-09-07T17:06:53.963828: step 2799, loss 0.0719698, acc 0.96
2016-09-07T17:06:54.652561: step 2800, loss 0.0585365, acc 0.98

Evaluation:
2016-09-07T17:06:57.646644: step 2800, loss 1.76836, acc 0.752

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-2800

2016-09-07T17:06:59.335560: step 2801, loss 0.0320079, acc 0.98
2016-09-07T17:07:00.017756: step 2802, loss 0.0191692, acc 1
2016-09-07T17:07:00.704092: step 2803, loss 0.00416725, acc 1
2016-09-07T17:07:01.458367: step 2804, loss 0.0185806, acc 1
2016-09-07T17:07:02.303282: step 2805, loss 0.0504445, acc 0.98
2016-09-07T17:07:03.050358: step 2806, loss 0.0613552, acc 0.98
2016-09-07T17:07:03.722506: step 2807, loss 0.0539029, acc 0.98
2016-09-07T17:07:04.388826: step 2808, loss 0.0797074, acc 0.98
2016-09-07T17:07:05.058173: step 2809, loss 0.0167377, acc 0.98
2016-09-07T17:07:05.728871: step 2810, loss 0.0211324, acc 1
2016-09-07T17:07:06.397820: step 2811, loss 0.0195322, acc 0.98
2016-09-07T17:07:07.069681: step 2812, loss 0.0273805, acc 0.98
2016-09-07T17:07:07.742767: step 2813, loss 0.0361437, acc 0.98
2016-09-07T17:07:08.409127: step 2814, loss 0.0293885, acc 0.96
2016-09-07T17:07:09.083094: step 2815, loss 0.0279201, acc 0.98
2016-09-07T17:07:09.739347: step 2816, loss 0.0667191, acc 0.98
2016-09-07T17:07:10.408647: step 2817, loss 0.0350838, acc 0.96
2016-09-07T17:07:11.073769: step 2818, loss 0.101253, acc 0.96
2016-09-07T17:07:11.742931: step 2819, loss 0.018949, acc 0.98
2016-09-07T17:07:12.408987: step 2820, loss 0.0521628, acc 0.98
2016-09-07T17:07:13.076592: step 2821, loss 0.0213471, acc 0.98
2016-09-07T17:07:13.733771: step 2822, loss 0.0883797, acc 0.96
2016-09-07T17:07:14.410657: step 2823, loss 0.0191489, acc 0.98
2016-09-07T17:07:15.082794: step 2824, loss 0.0263337, acc 0.98
2016-09-07T17:07:15.770665: step 2825, loss 0.0148167, acc 1
2016-09-07T17:07:16.436044: step 2826, loss 0.0116255, acc 1
2016-09-07T17:07:17.141564: step 2827, loss 0.0335588, acc 0.98
2016-09-07T17:07:17.799281: step 2828, loss 0.103788, acc 0.98
2016-09-07T17:07:18.461108: step 2829, loss 0.0419666, acc 0.98
2016-09-07T17:07:19.140447: step 2830, loss 0.0353176, acc 0.98
2016-09-07T17:07:19.830234: step 2831, loss 0.123885, acc 0.96
2016-09-07T17:07:20.493620: step 2832, loss 0.0630931, acc 0.96
2016-09-07T17:07:21.176591: step 2833, loss 0.0245987, acc 1
2016-09-07T17:07:21.840905: step 2834, loss 0.0293499, acc 0.98
2016-09-07T17:07:22.517822: step 2835, loss 0.0129791, acc 1
2016-09-07T17:07:23.210667: step 2836, loss 0.15399, acc 0.94
2016-09-07T17:07:23.912045: step 2837, loss 0.0263809, acc 1
2016-09-07T17:07:24.583317: step 2838, loss 0.0493298, acc 1
2016-09-07T17:07:25.253436: step 2839, loss 0.0144412, acc 1
2016-09-07T17:07:25.937474: step 2840, loss 0.0708964, acc 0.96
2016-09-07T17:07:26.627424: step 2841, loss 0.121943, acc 0.94
2016-09-07T17:07:27.294453: step 2842, loss 0.0397244, acc 0.98
2016-09-07T17:07:27.980205: step 2843, loss 0.0275858, acc 1
2016-09-07T17:07:28.672340: step 2844, loss 0.00363627, acc 1
2016-09-07T17:07:29.346522: step 2845, loss 0.0249252, acc 1
2016-09-07T17:07:30.018665: step 2846, loss 0.00884905, acc 1
2016-09-07T17:07:30.697056: step 2847, loss 0.0217397, acc 1
2016-09-07T17:07:31.370680: step 2848, loss 0.073125, acc 0.96
2016-09-07T17:07:32.036813: step 2849, loss 0.003806, acc 1
2016-09-07T17:07:32.728975: step 2850, loss 0.035389, acc 0.98
2016-09-07T17:07:33.408428: step 2851, loss 0.0328334, acc 0.98
2016-09-07T17:07:34.072264: step 2852, loss 0.023372, acc 1
2016-09-07T17:07:34.753156: step 2853, loss 0.0135852, acc 1
2016-09-07T17:07:35.432501: step 2854, loss 0.00767697, acc 1
2016-09-07T17:07:36.091483: step 2855, loss 0.0446247, acc 0.96
2016-09-07T17:07:36.755958: step 2856, loss 0.0184954, acc 1
2016-09-07T17:07:37.421630: step 2857, loss 0.0333372, acc 0.98
2016-09-07T17:07:38.107001: step 2858, loss 0.0685169, acc 0.96
2016-09-07T17:07:38.770445: step 2859, loss 0.0787014, acc 0.98
2016-09-07T17:07:39.433111: step 2860, loss 0.00652217, acc 1
2016-09-07T17:07:40.134643: step 2861, loss 0.0154121, acc 1
2016-09-07T17:07:40.823059: step 2862, loss 0.00451674, acc 1
2016-09-07T17:07:41.512235: step 2863, loss 0.137824, acc 0.92
2016-09-07T17:07:42.179043: step 2864, loss 0.0437224, acc 0.98
2016-09-07T17:07:42.854246: step 2865, loss 0.0365566, acc 0.98
2016-09-07T17:07:43.558378: step 2866, loss 0.0177347, acc 0.98
2016-09-07T17:07:44.231857: step 2867, loss 0.132105, acc 0.96
2016-09-07T17:07:44.926589: step 2868, loss 0.294587, acc 0.92
2016-09-07T17:07:45.614024: step 2869, loss 0.0607051, acc 0.98
2016-09-07T17:07:46.270822: step 2870, loss 0.13952, acc 0.98
2016-09-07T17:07:46.944873: step 2871, loss 0.0269744, acc 1
2016-09-07T17:07:47.588188: step 2872, loss 0.00304823, acc 1
2016-09-07T17:07:48.251863: step 2873, loss 0.020642, acc 1
2016-09-07T17:07:48.934922: step 2874, loss 0.0184497, acc 1
2016-09-07T17:07:49.616789: step 2875, loss 0.0115862, acc 1
2016-09-07T17:07:50.325592: step 2876, loss 0.0417511, acc 0.96
2016-09-07T17:07:51.004322: step 2877, loss 0.015668, acc 1
2016-09-07T17:07:51.662957: step 2878, loss 0.0677326, acc 0.98
2016-09-07T17:07:52.333753: step 2879, loss 0.00662569, acc 1
2016-09-07T17:07:53.028957: step 2880, loss 0.108246, acc 0.92
2016-09-07T17:07:53.731637: step 2881, loss 0.00805282, acc 1
2016-09-07T17:07:54.398568: step 2882, loss 0.0429456, acc 1
2016-09-07T17:07:55.071879: step 2883, loss 0.0162933, acc 1
2016-09-07T17:07:55.752186: step 2884, loss 0.031932, acc 0.98
2016-09-07T17:07:56.424992: step 2885, loss 0.111579, acc 0.94
2016-09-07T17:07:57.188968: step 2886, loss 0.0995596, acc 0.94
2016-09-07T17:07:57.924702: step 2887, loss 0.0368082, acc 0.96
2016-09-07T17:07:58.606580: step 2888, loss 0.0177764, acc 1
2016-09-07T17:07:59.276128: step 2889, loss 0.0442233, acc 0.96
2016-09-07T17:07:59.954299: step 2890, loss 0.0235795, acc 0.98
2016-09-07T17:08:00.641948: step 2891, loss 0.0287049, acc 1
2016-09-07T17:08:01.296560: step 2892, loss 0.0435517, acc 0.98
2016-09-07T17:08:01.951416: step 2893, loss 0.0336858, acc 0.98
2016-09-07T17:08:02.608102: step 2894, loss 0.00852887, acc 1
2016-09-07T17:08:03.287759: step 2895, loss 0.0715133, acc 0.96
2016-09-07T17:08:03.964335: step 2896, loss 0.0343878, acc 0.98
2016-09-07T17:08:04.647981: step 2897, loss 0.15963, acc 0.92
2016-09-07T17:08:05.309564: step 2898, loss 0.00972804, acc 1
2016-09-07T17:08:05.967042: step 2899, loss 0.0054948, acc 1
2016-09-07T17:08:06.630806: step 2900, loss 0.0722471, acc 0.98

Evaluation:
2016-09-07T17:08:09.550656: step 2900, loss 1.62242, acc 0.747

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-2900

2016-09-07T17:08:11.271033: step 2901, loss 0.0203159, acc 1
2016-09-07T17:08:11.941852: step 2902, loss 0.118652, acc 0.94
2016-09-07T17:08:12.631346: step 2903, loss 0.0676477, acc 0.96
2016-09-07T17:08:13.297672: step 2904, loss 0.0464576, acc 0.98
2016-09-07T17:08:13.968181: step 2905, loss 0.0260376, acc 1
2016-09-07T17:08:14.624583: step 2906, loss 0.0215984, acc 1
2016-09-07T17:08:15.291224: step 2907, loss 0.0284509, acc 0.98
2016-09-07T17:08:15.966974: step 2908, loss 0.0236335, acc 1
2016-09-07T17:08:16.617104: step 2909, loss 0.0185765, acc 1
2016-09-07T17:08:16.985753: step 2910, loss 0.0406399, acc 1
2016-09-07T17:08:17.687453: step 2911, loss 0.0383119, acc 0.96
2016-09-07T17:08:18.356611: step 2912, loss 0.0385822, acc 1
2016-09-07T17:08:19.041240: step 2913, loss 0.0154326, acc 1
2016-09-07T17:08:19.712327: step 2914, loss 0.0312693, acc 1
2016-09-07T17:08:20.406470: step 2915, loss 0.0411004, acc 0.98
2016-09-07T17:08:21.089712: step 2916, loss 0.0666784, acc 0.98
2016-09-07T17:08:21.751484: step 2917, loss 0.0151178, acc 1
2016-09-07T17:08:22.423688: step 2918, loss 0.0670702, acc 0.94
2016-09-07T17:08:23.101408: step 2919, loss 0.226496, acc 0.96
2016-09-07T17:08:23.795032: step 2920, loss 0.0523501, acc 0.98
2016-09-07T17:08:24.479675: step 2921, loss 0.00984339, acc 1
2016-09-07T17:08:25.163405: step 2922, loss 0.0619334, acc 0.98
2016-09-07T17:08:25.842711: step 2923, loss 0.0555599, acc 1
2016-09-07T17:08:26.541759: step 2924, loss 0.0626132, acc 0.94
2016-09-07T17:08:27.245284: step 2925, loss 0.00250993, acc 1
2016-09-07T17:08:27.911623: step 2926, loss 0.0549112, acc 0.98
2016-09-07T17:08:28.569713: step 2927, loss 0.0924362, acc 0.98
2016-09-07T17:08:29.234033: step 2928, loss 0.00769762, acc 1
2016-09-07T17:08:29.899670: step 2929, loss 0.0313002, acc 0.96
2016-09-07T17:08:30.572424: step 2930, loss 0.0431568, acc 0.96
2016-09-07T17:08:31.254909: step 2931, loss 0.0166494, acc 0.98
2016-09-07T17:08:31.924590: step 2932, loss 0.0282246, acc 0.98
2016-09-07T17:08:32.603765: step 2933, loss 0.014833, acc 1
2016-09-07T17:08:33.273796: step 2934, loss 0.0079119, acc 1
2016-09-07T17:08:33.960013: step 2935, loss 0.00158607, acc 1
2016-09-07T17:08:34.635228: step 2936, loss 0.0629725, acc 0.98
2016-09-07T17:08:35.306213: step 2937, loss 0.0391032, acc 0.98
2016-09-07T17:08:35.977334: step 2938, loss 0.114582, acc 0.98
2016-09-07T17:08:36.639624: step 2939, loss 0.0168564, acc 1
2016-09-07T17:08:37.296074: step 2940, loss 0.0168872, acc 1
2016-09-07T17:08:37.957852: step 2941, loss 0.0179182, acc 0.98
2016-09-07T17:08:38.611834: step 2942, loss 0.061245, acc 0.96
2016-09-07T17:08:39.279919: step 2943, loss 0.0294721, acc 0.98
2016-09-07T17:08:39.929687: step 2944, loss 0.0163947, acc 0.98
2016-09-07T17:08:40.602626: step 2945, loss 0.0227292, acc 0.98
2016-09-07T17:08:41.266671: step 2946, loss 0.0615184, acc 0.96
2016-09-07T17:08:41.931716: step 2947, loss 0.0253009, acc 1
2016-09-07T17:08:42.619931: step 2948, loss 0.0187795, acc 1
2016-09-07T17:08:43.296373: step 2949, loss 0.0220936, acc 0.98
2016-09-07T17:08:43.961981: step 2950, loss 0.00493941, acc 1
2016-09-07T17:08:44.654794: step 2951, loss 0.0478652, acc 0.98
2016-09-07T17:08:45.331643: step 2952, loss 0.00142804, acc 1
2016-09-07T17:08:46.019378: step 2953, loss 0.00943223, acc 1
2016-09-07T17:08:46.690059: step 2954, loss 0.0703992, acc 0.96
2016-09-07T17:08:47.350972: step 2955, loss 0.0426433, acc 0.96
2016-09-07T17:08:48.038638: step 2956, loss 0.0515661, acc 0.98
2016-09-07T17:08:48.721029: step 2957, loss 0.00252318, acc 1
2016-09-07T17:08:49.380821: step 2958, loss 0.0417401, acc 0.98
2016-09-07T17:08:50.040289: step 2959, loss 0.0577932, acc 0.96
2016-09-07T17:08:50.709792: step 2960, loss 0.0130183, acc 1
2016-09-07T17:08:51.393693: step 2961, loss 0.023911, acc 1
2016-09-07T17:08:52.084440: step 2962, loss 0.00178368, acc 1
2016-09-07T17:08:52.744062: step 2963, loss 0.00498861, acc 1
2016-09-07T17:08:53.415465: step 2964, loss 0.0391313, acc 0.98
2016-09-07T17:08:54.095268: step 2965, loss 0.0127728, acc 1
2016-09-07T17:08:54.756010: step 2966, loss 0.0530899, acc 0.96
2016-09-07T17:08:55.419054: step 2967, loss 0.074297, acc 0.98
2016-09-07T17:08:56.093568: step 2968, loss 0.00127166, acc 1
2016-09-07T17:08:56.758422: step 2969, loss 0.0131839, acc 1
2016-09-07T17:08:57.422797: step 2970, loss 0.0683944, acc 0.98
2016-09-07T17:08:58.106089: step 2971, loss 0.0297298, acc 0.98
2016-09-07T17:08:58.795799: step 2972, loss 0.0132638, acc 1
2016-09-07T17:08:59.462138: step 2973, loss 0.0259026, acc 0.98
2016-09-07T17:09:00.122347: step 2974, loss 0.0266094, acc 0.98
2016-09-07T17:09:00.806189: step 2975, loss 0.0329016, acc 0.98
2016-09-07T17:09:01.489584: step 2976, loss 0.193464, acc 0.94
2016-09-07T17:09:02.149620: step 2977, loss 0.0120304, acc 1
2016-09-07T17:09:02.928466: step 2978, loss 0.0225581, acc 0.98
2016-09-07T17:09:03.590519: step 2979, loss 0.0464166, acc 0.98
2016-09-07T17:09:04.271671: step 2980, loss 0.062147, acc 0.96
2016-09-07T17:09:04.941058: step 2981, loss 0.00579214, acc 1
2016-09-07T17:09:05.595240: step 2982, loss 0.0659328, acc 0.94
2016-09-07T17:09:06.274108: step 2983, loss 0.0386635, acc 0.98
2016-09-07T17:09:06.948940: step 2984, loss 0.0359363, acc 0.98
2016-09-07T17:09:07.621788: step 2985, loss 0.0266283, acc 0.98
2016-09-07T17:09:08.294719: step 2986, loss 0.0785993, acc 0.94
2016-09-07T17:09:08.967417: step 2987, loss 0.0372366, acc 0.96
2016-09-07T17:09:09.639413: step 2988, loss 0.000165, acc 1
2016-09-07T17:09:10.299362: step 2989, loss 0.0593174, acc 0.98
2016-09-07T17:09:10.968166: step 2990, loss 0.0300185, acc 0.98
2016-09-07T17:09:11.629231: step 2991, loss 0.0138026, acc 1
2016-09-07T17:09:12.287492: step 2992, loss 0.0206745, acc 1
2016-09-07T17:09:12.957270: step 2993, loss 0.0237013, acc 0.98
2016-09-07T17:09:13.621459: step 2994, loss 0.0314741, acc 0.98
2016-09-07T17:09:14.295214: step 2995, loss 0.00940268, acc 1
2016-09-07T17:09:14.971211: step 2996, loss 0.0163038, acc 1
2016-09-07T17:09:15.637473: step 2997, loss 0.019878, acc 1
2016-09-07T17:09:16.303733: step 2998, loss 0.0167159, acc 1
2016-09-07T17:09:16.963730: step 2999, loss 0.0122579, acc 1
2016-09-07T17:09:17.636363: step 3000, loss 0.0274066, acc 0.98

Evaluation:
2016-09-07T17:09:20.552063: step 3000, loss 1.7514, acc 0.747

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-3000

2016-09-07T17:09:22.217155: step 3001, loss 0.0109724, acc 1
2016-09-07T17:09:22.890086: step 3002, loss 0.0375138, acc 0.98
2016-09-07T17:09:23.564985: step 3003, loss 0.00442673, acc 1
2016-09-07T17:09:24.244477: step 3004, loss 0.0340233, acc 0.98
2016-09-07T17:09:24.928326: step 3005, loss 0.0181523, acc 1
2016-09-07T17:09:25.605884: step 3006, loss 0.12809, acc 0.96
2016-09-07T17:09:26.289351: step 3007, loss 0.0178213, acc 1
2016-09-07T17:09:26.952690: step 3008, loss 0.0859567, acc 0.96
2016-09-07T17:09:27.617989: step 3009, loss 0.0285231, acc 0.98
2016-09-07T17:09:28.294636: step 3010, loss 0.0505513, acc 0.98
2016-09-07T17:09:28.976914: step 3011, loss 0.0180154, acc 1
2016-09-07T17:09:29.644421: step 3012, loss 0.208681, acc 0.96
2016-09-07T17:09:30.332281: step 3013, loss 0.0127953, acc 1
2016-09-07T17:09:30.991032: step 3014, loss 0.0131221, acc 1
2016-09-07T17:09:31.671775: step 3015, loss 0.0277491, acc 1
2016-09-07T17:09:32.346687: step 3016, loss 0.0202565, acc 0.98
2016-09-07T17:09:33.017715: step 3017, loss 0.0266249, acc 0.98
2016-09-07T17:09:33.685388: step 3018, loss 0.00606505, acc 1
2016-09-07T17:09:34.363601: step 3019, loss 0.0864441, acc 0.98
2016-09-07T17:09:35.042503: step 3020, loss 0.0159206, acc 1
2016-09-07T17:09:35.701162: step 3021, loss 0.0359654, acc 0.98
2016-09-07T17:09:36.365564: step 3022, loss 0.0337287, acc 0.98
2016-09-07T17:09:37.042999: step 3023, loss 0.00884745, acc 1
2016-09-07T17:09:37.695247: step 3024, loss 0.0840982, acc 0.96
2016-09-07T17:09:38.368229: step 3025, loss 0.00221962, acc 1
2016-09-07T17:09:39.020798: step 3026, loss 0.0210938, acc 1
2016-09-07T17:09:39.690893: step 3027, loss 0.159033, acc 0.98
2016-09-07T17:09:40.366763: step 3028, loss 0.0378567, acc 0.96
2016-09-07T17:09:41.032783: step 3029, loss 0.00711846, acc 1
2016-09-07T17:09:41.694106: step 3030, loss 0.0377031, acc 0.98
2016-09-07T17:09:42.365699: step 3031, loss 0.0217064, acc 0.98
2016-09-07T17:09:43.017120: step 3032, loss 0.0190215, acc 1
2016-09-07T17:09:43.664498: step 3033, loss 0.00167196, acc 1
2016-09-07T17:09:44.339839: step 3034, loss 0.0166995, acc 1
2016-09-07T17:09:45.018055: step 3035, loss 0.0442553, acc 0.96
2016-09-07T17:09:45.693060: step 3036, loss 0.0607959, acc 0.98
2016-09-07T17:09:46.358168: step 3037, loss 0.0278754, acc 1
2016-09-07T17:09:47.021997: step 3038, loss 0.0167005, acc 1
2016-09-07T17:09:47.689079: step 3039, loss 0.0144885, acc 1
2016-09-07T17:09:48.354663: step 3040, loss 0.0598248, acc 0.96
2016-09-07T17:09:49.035674: step 3041, loss 0.0289646, acc 1
2016-09-07T17:09:49.719325: step 3042, loss 0.0253155, acc 0.98
2016-09-07T17:09:50.394380: step 3043, loss 0.012824, acc 1
2016-09-07T17:09:51.056826: step 3044, loss 0.015076, acc 1
2016-09-07T17:09:51.719514: step 3045, loss 0.0648867, acc 0.98
2016-09-07T17:09:52.376616: step 3046, loss 0.00830795, acc 1
2016-09-07T17:09:53.040303: step 3047, loss 0.00832891, acc 1
2016-09-07T17:09:53.699903: step 3048, loss 0.0315915, acc 0.98
2016-09-07T17:09:54.371143: step 3049, loss 0.0460025, acc 0.96
2016-09-07T17:09:55.035661: step 3050, loss 0.0744236, acc 0.96
2016-09-07T17:09:55.694879: step 3051, loss 0.0459377, acc 0.96
2016-09-07T17:09:56.359994: step 3052, loss 0.021996, acc 1
2016-09-07T17:09:57.053905: step 3053, loss 0.0143413, acc 1
2016-09-07T17:09:57.707875: step 3054, loss 0.0898783, acc 0.96
2016-09-07T17:09:58.366721: step 3055, loss 0.0172478, acc 0.98
2016-09-07T17:09:59.040307: step 3056, loss 0.0145012, acc 1
2016-09-07T17:09:59.736079: step 3057, loss 0.0326912, acc 0.98
2016-09-07T17:10:00.436905: step 3058, loss 0.0473532, acc 0.96
2016-09-07T17:10:01.117671: step 3059, loss 0.027602, acc 0.98
2016-09-07T17:10:01.873135: step 3060, loss 0.024671, acc 0.98
2016-09-07T17:10:02.563130: step 3061, loss 0.0340765, acc 0.98
2016-09-07T17:10:03.416301: step 3062, loss 0.0905259, acc 0.94
2016-09-07T17:10:04.189138: step 3063, loss 0.0931082, acc 0.96
2016-09-07T17:10:04.968045: step 3064, loss 0.0202879, acc 1
2016-09-07T17:10:05.780192: step 3065, loss 0.0105247, acc 1
2016-09-07T17:10:06.591767: step 3066, loss 0.00739833, acc 1
2016-09-07T17:10:07.352603: step 3067, loss 0.0135353, acc 1
2016-09-07T17:10:08.201443: step 3068, loss 0.0204033, acc 0.98
2016-09-07T17:10:09.040399: step 3069, loss 0.0108128, acc 1
2016-09-07T17:10:09.838649: step 3070, loss 0.0189495, acc 1
2016-09-07T17:10:10.611523: step 3071, loss 0.0233041, acc 1
2016-09-07T17:10:11.409293: step 3072, loss 0.0263248, acc 1
2016-09-07T17:10:12.265038: step 3073, loss 0.00788752, acc 1
2016-09-07T17:10:13.079927: step 3074, loss 0.0384709, acc 1
2016-09-07T17:10:13.925653: step 3075, loss 0.0106819, acc 1
2016-09-07T17:10:14.675061: step 3076, loss 0.0509359, acc 0.98
2016-09-07T17:10:15.432661: step 3077, loss 0.00457444, acc 1
2016-09-07T17:10:16.185894: step 3078, loss 0.0550991, acc 0.98
2016-09-07T17:10:16.894677: step 3079, loss 0.0170228, acc 1
2016-09-07T17:10:17.560199: step 3080, loss 0.0641328, acc 0.98
2016-09-07T17:10:18.370725: step 3081, loss 0.0340974, acc 1
2016-09-07T17:10:19.078520: step 3082, loss 0.0696922, acc 0.96
2016-09-07T17:10:19.818139: step 3083, loss 0.0469694, acc 0.96
2016-09-07T17:10:20.560624: step 3084, loss 0.0357749, acc 0.98
2016-09-07T17:10:21.339068: step 3085, loss 0.0226607, acc 1
2016-09-07T17:10:22.081648: step 3086, loss 0.00998806, acc 1
2016-09-07T17:10:22.753902: step 3087, loss 0.00781964, acc 1
2016-09-07T17:10:23.476451: step 3088, loss 0.018173, acc 1
2016-09-07T17:10:24.146106: step 3089, loss 0.0493231, acc 0.98
2016-09-07T17:10:24.909627: step 3090, loss 0.0381059, acc 0.98
2016-09-07T17:10:25.571616: step 3091, loss 0.00479915, acc 1
2016-09-07T17:10:26.266620: step 3092, loss 0.037104, acc 0.98
2016-09-07T17:10:26.976391: step 3093, loss 0.0253061, acc 1
2016-09-07T17:10:27.662427: step 3094, loss 0.022225, acc 1
2016-09-07T17:10:28.542353: step 3095, loss 0.0268985, acc 0.98
2016-09-07T17:10:29.213322: step 3096, loss 0.0244873, acc 0.98
2016-09-07T17:10:29.985754: step 3097, loss 0.0265041, acc 1
2016-09-07T17:10:30.633827: step 3098, loss 0.00575751, acc 1
2016-09-07T17:10:31.321683: step 3099, loss 0.0261008, acc 1
2016-09-07T17:10:31.976960: step 3100, loss 0.049073, acc 0.98

Evaluation:
2016-09-07T17:10:34.881669: step 3100, loss 2.162, acc 0.753

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-3100

2016-09-07T17:10:36.598779: step 3101, loss 0.113046, acc 0.92
2016-09-07T17:10:37.340070: step 3102, loss 0.0132641, acc 1
2016-09-07T17:10:38.092499: step 3103, loss 0.000732241, acc 1
2016-09-07T17:10:38.460577: step 3104, loss 0.000388202, acc 1
2016-09-07T17:10:39.269446: step 3105, loss 0.043291, acc 1
2016-09-07T17:10:40.020794: step 3106, loss 0.0587058, acc 0.98
2016-09-07T17:10:40.780369: step 3107, loss 0.037681, acc 1
2016-09-07T17:10:41.574052: step 3108, loss 0.0948691, acc 0.96
2016-09-07T17:10:42.325927: step 3109, loss 0.123799, acc 0.96
2016-09-07T17:10:42.973468: step 3110, loss 0.0365138, acc 1
2016-09-07T17:10:43.693024: step 3111, loss 0.0162964, acc 0.98
2016-09-07T17:10:44.430927: step 3112, loss 0.0719547, acc 0.94
2016-09-07T17:10:45.109053: step 3113, loss 0.0181859, acc 0.98
2016-09-07T17:10:45.800539: step 3114, loss 0.0177208, acc 1
2016-09-07T17:10:46.510531: step 3115, loss 0.00135144, acc 1
2016-09-07T17:10:47.247330: step 3116, loss 0.031496, acc 0.98
2016-09-07T17:10:47.914746: step 3117, loss 0.0185896, acc 1
2016-09-07T17:10:48.607773: step 3118, loss 0.0346758, acc 0.98
2016-09-07T17:10:49.411346: step 3119, loss 0.173663, acc 0.96
2016-09-07T17:10:50.168631: step 3120, loss 0.0866267, acc 0.94
2016-09-07T17:10:50.987562: step 3121, loss 0.0751325, acc 0.96
2016-09-07T17:10:51.796638: step 3122, loss 0.0174365, acc 1
2016-09-07T17:10:52.460417: step 3123, loss 0.085516, acc 0.94
2016-09-07T17:10:53.121648: step 3124, loss 0.00744097, acc 1
2016-09-07T17:10:53.928377: step 3125, loss 0.0426062, acc 0.98
2016-09-07T17:10:54.711816: step 3126, loss 0.0318898, acc 0.98
2016-09-07T17:10:55.538823: step 3127, loss 0.0161864, acc 0.98
2016-09-07T17:10:56.268807: step 3128, loss 0.0180302, acc 1
2016-09-07T17:10:57.050153: step 3129, loss 0.13511, acc 0.9
2016-09-07T17:10:57.839432: step 3130, loss 0.00796096, acc 1
2016-09-07T17:10:58.631605: step 3131, loss 0.0045326, acc 1
2016-09-07T17:10:59.409456: step 3132, loss 0.0076935, acc 1
2016-09-07T17:11:00.185551: step 3133, loss 0.00718233, acc 1
2016-09-07T17:11:00.952081: step 3134, loss 0.0223737, acc 0.98
2016-09-07T17:11:01.710380: step 3135, loss 0.0137878, acc 1
2016-09-07T17:11:02.439960: step 3136, loss 0.0541538, acc 0.98
2016-09-07T17:11:03.259031: step 3137, loss 0.0706237, acc 0.96
2016-09-07T17:11:04.013295: step 3138, loss 0.0416735, acc 0.98
2016-09-07T17:11:04.775225: step 3139, loss 0.0355168, acc 0.98
2016-09-07T17:11:05.502767: step 3140, loss 0.138638, acc 0.94
2016-09-07T17:11:06.327354: step 3141, loss 0.191039, acc 0.96
2016-09-07T17:11:07.043505: step 3142, loss 0.0608468, acc 0.96
2016-09-07T17:11:07.917696: step 3143, loss 0.0168475, acc 0.98
2016-09-07T17:11:08.700871: step 3144, loss 0.159375, acc 0.98
2016-09-07T17:11:09.532322: step 3145, loss 0.0885189, acc 0.9
2016-09-07T17:11:10.353119: step 3146, loss 0.0180348, acc 1
2016-09-07T17:11:11.134071: step 3147, loss 0.00170442, acc 1
2016-09-07T17:11:11.927279: step 3148, loss 0.0378001, acc 0.96
2016-09-07T17:11:12.768063: step 3149, loss 0.112396, acc 0.98
2016-09-07T17:11:13.576776: step 3150, loss 0.100655, acc 0.96
2016-09-07T17:11:14.346466: step 3151, loss 0.0851794, acc 0.98
2016-09-07T17:11:15.070725: step 3152, loss 0.0341884, acc 0.96
2016-09-07T17:11:15.858914: step 3153, loss 0.0049267, acc 1
2016-09-07T17:11:16.589581: step 3154, loss 0.0779275, acc 0.96
2016-09-07T17:11:17.319571: step 3155, loss 0.0500989, acc 0.98
2016-09-07T17:11:18.083829: step 3156, loss 0.0206465, acc 0.98
2016-09-07T17:11:18.755260: step 3157, loss 0.0192814, acc 1
2016-09-07T17:11:19.429383: step 3158, loss 0.04556, acc 0.98
2016-09-07T17:11:20.183864: step 3159, loss 0.0400496, acc 0.98
2016-09-07T17:11:20.854671: step 3160, loss 0.0608224, acc 0.98
2016-09-07T17:11:21.551892: step 3161, loss 0.0247687, acc 0.98
2016-09-07T17:11:22.217878: step 3162, loss 0.0292886, acc 0.98
2016-09-07T17:11:22.901145: step 3163, loss 0.0269525, acc 1
2016-09-07T17:11:23.573796: step 3164, loss 0.0237979, acc 1
2016-09-07T17:11:24.240543: step 3165, loss 0.0274785, acc 1
2016-09-07T17:11:25.047193: step 3166, loss 0.0299415, acc 0.98
2016-09-07T17:11:25.818000: step 3167, loss 0.0847659, acc 0.98
2016-09-07T17:11:26.634867: step 3168, loss 0.0263573, acc 1
2016-09-07T17:11:27.455286: step 3169, loss 0.0519924, acc 0.98
2016-09-07T17:11:28.282536: step 3170, loss 0.0585443, acc 0.96
2016-09-07T17:11:28.965906: step 3171, loss 0.000827006, acc 1
2016-09-07T17:11:29.624727: step 3172, loss 0.0123268, acc 1
2016-09-07T17:11:30.287132: step 3173, loss 0.0612326, acc 0.96
2016-09-07T17:11:30.971892: step 3174, loss 0.0396599, acc 0.96
2016-09-07T17:11:31.668819: step 3175, loss 0.00396509, acc 1
2016-09-07T17:11:32.347611: step 3176, loss 0.0240027, acc 0.98
2016-09-07T17:11:33.025030: step 3177, loss 0.00285366, acc 1
2016-09-07T17:11:33.697761: step 3178, loss 0.00226687, acc 1
2016-09-07T17:11:34.392469: step 3179, loss 0.00446851, acc 1
2016-09-07T17:11:35.087889: step 3180, loss 0.0421046, acc 0.96
2016-09-07T17:11:35.779719: step 3181, loss 0.0129185, acc 1
2016-09-07T17:11:36.433720: step 3182, loss 0.00217881, acc 1
2016-09-07T17:11:37.090574: step 3183, loss 0.0168139, acc 1
2016-09-07T17:11:37.760738: step 3184, loss 0.0103147, acc 1
2016-09-07T17:11:38.448218: step 3185, loss 0.0623943, acc 0.98
2016-09-07T17:11:39.134663: step 3186, loss 0.0680037, acc 0.96
2016-09-07T17:11:39.806883: step 3187, loss 0.0473894, acc 0.96
2016-09-07T17:11:40.476572: step 3188, loss 0.00634547, acc 1
2016-09-07T17:11:41.136355: step 3189, loss 0.0282558, acc 0.98
2016-09-07T17:11:41.812705: step 3190, loss 0.066933, acc 0.98
2016-09-07T17:11:42.499710: step 3191, loss 0.00852171, acc 1
2016-09-07T17:11:43.157778: step 3192, loss 0.0228768, acc 1
2016-09-07T17:11:43.814916: step 3193, loss 0.00784435, acc 1
2016-09-07T17:11:44.510200: step 3194, loss 0.00909215, acc 1
2016-09-07T17:11:45.181075: step 3195, loss 0.0338663, acc 0.98
2016-09-07T17:11:45.860094: step 3196, loss 0.234122, acc 0.94
2016-09-07T17:11:46.515156: step 3197, loss 0.0333631, acc 1
2016-09-07T17:11:47.210401: step 3198, loss 0.00938777, acc 1
2016-09-07T17:11:47.902241: step 3199, loss 0.0348152, acc 0.98
2016-09-07T17:11:48.584896: step 3200, loss 0.109226, acc 0.98

Evaluation:
2016-09-07T17:11:51.819526: step 3200, loss 2.00262, acc 0.75

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-3200

2016-09-07T17:11:53.562766: step 3201, loss 0.12435, acc 0.98
2016-09-07T17:11:54.230934: step 3202, loss 0.155934, acc 0.9
2016-09-07T17:11:54.910694: step 3203, loss 0.0128994, acc 1
2016-09-07T17:11:55.577112: step 3204, loss 0.0406007, acc 0.96
2016-09-07T17:11:56.242916: step 3205, loss 0.0231465, acc 0.98
2016-09-07T17:11:56.917363: step 3206, loss 0.0547475, acc 0.98
2016-09-07T17:11:57.661895: step 3207, loss 0.0624107, acc 0.98
2016-09-07T17:11:58.455787: step 3208, loss 0.0284839, acc 0.98
2016-09-07T17:11:59.212182: step 3209, loss 0.0166898, acc 1
2016-09-07T17:11:59.996528: step 3210, loss 0.0233043, acc 1
2016-09-07T17:12:00.825398: step 3211, loss 0.0476882, acc 0.96
2016-09-07T17:12:01.477788: step 3212, loss 0.0588753, acc 0.98
2016-09-07T17:12:02.133542: step 3213, loss 0.0348582, acc 0.98
2016-09-07T17:12:02.810183: step 3214, loss 0.0100743, acc 1
2016-09-07T17:12:03.477793: step 3215, loss 0.0830589, acc 0.96
2016-09-07T17:12:04.160972: step 3216, loss 0.00508952, acc 1
2016-09-07T17:12:04.814330: step 3217, loss 0.0392578, acc 0.98
2016-09-07T17:12:05.467164: step 3218, loss 0.0288327, acc 0.98
2016-09-07T17:12:06.120056: step 3219, loss 0.0642004, acc 0.96
2016-09-07T17:12:06.802055: step 3220, loss 0.0482958, acc 0.98
2016-09-07T17:12:07.497638: step 3221, loss 0.116753, acc 0.98
2016-09-07T17:12:08.172181: step 3222, loss 0.0130376, acc 1
2016-09-07T17:12:08.848665: step 3223, loss 0.0640446, acc 0.98
2016-09-07T17:12:09.535498: step 3224, loss 0.0383215, acc 0.98
2016-09-07T17:12:10.309310: step 3225, loss 0.0554483, acc 0.96
2016-09-07T17:12:11.026659: step 3226, loss 0.0227292, acc 0.98
2016-09-07T17:12:11.710249: step 3227, loss 0.00429718, acc 1
2016-09-07T17:12:12.376632: step 3228, loss 0.0100223, acc 1
2016-09-07T17:12:13.050071: step 3229, loss 0.0801749, acc 0.98
2016-09-07T17:12:13.710027: step 3230, loss 0.20569, acc 0.94
2016-09-07T17:12:14.372887: step 3231, loss 0.148422, acc 0.98
2016-09-07T17:12:15.050091: step 3232, loss 0.017628, acc 1
2016-09-07T17:12:15.726608: step 3233, loss 0.0248579, acc 1
2016-09-07T17:12:16.390001: step 3234, loss 0.0702574, acc 0.98
2016-09-07T17:12:17.057449: step 3235, loss 0.0195778, acc 1
2016-09-07T17:12:17.737131: step 3236, loss 0.0624556, acc 0.98
2016-09-07T17:12:18.414665: step 3237, loss 0.00628913, acc 1
2016-09-07T17:12:19.089255: step 3238, loss 0.0400562, acc 0.98
2016-09-07T17:12:19.759405: step 3239, loss 0.0341012, acc 0.98
2016-09-07T17:12:20.427367: step 3240, loss 0.0746297, acc 0.98
2016-09-07T17:12:21.095944: step 3241, loss 0.0616543, acc 0.96
2016-09-07T17:12:21.762181: step 3242, loss 0.00939308, acc 1
2016-09-07T17:12:22.449390: step 3243, loss 0.017602, acc 1
2016-09-07T17:12:23.116362: step 3244, loss 0.0850882, acc 0.96
2016-09-07T17:12:23.769544: step 3245, loss 0.0468858, acc 0.98
2016-09-07T17:12:24.463598: step 3246, loss 0.075162, acc 0.98
2016-09-07T17:12:25.138671: step 3247, loss 0.0244847, acc 0.98
2016-09-07T17:12:25.804486: step 3248, loss 0.0416845, acc 0.98
2016-09-07T17:12:26.487278: step 3249, loss 0.0546487, acc 0.98
2016-09-07T17:12:27.147675: step 3250, loss 0.00769331, acc 1
2016-09-07T17:12:27.857155: step 3251, loss 0.133385, acc 0.96
2016-09-07T17:12:28.545871: step 3252, loss 0.123393, acc 0.98
2016-09-07T17:12:29.215591: step 3253, loss 0.139239, acc 0.96
2016-09-07T17:12:29.868996: step 3254, loss 0.0419896, acc 0.98
2016-09-07T17:12:30.532587: step 3255, loss 0.0593028, acc 0.98
2016-09-07T17:12:31.213994: step 3256, loss 0.0185628, acc 0.98
2016-09-07T17:12:31.890832: step 3257, loss 0.0758046, acc 0.94
2016-09-07T17:12:32.573046: step 3258, loss 0.0689104, acc 0.94
2016-09-07T17:12:33.279462: step 3259, loss 0.0972134, acc 0.98
2016-09-07T17:12:33.942173: step 3260, loss 0.0157896, acc 1
2016-09-07T17:12:34.599362: step 3261, loss 0.0815724, acc 0.98
2016-09-07T17:12:35.291218: step 3262, loss 0.0301691, acc 0.98
2016-09-07T17:12:35.947625: step 3263, loss 0.0117403, acc 1
2016-09-07T17:12:36.600307: step 3264, loss 0.0363738, acc 0.98
2016-09-07T17:12:37.275090: step 3265, loss 0.0315875, acc 1
2016-09-07T17:12:37.939489: step 3266, loss 0.053117, acc 0.96
2016-09-07T17:12:38.630511: step 3267, loss 0.0298364, acc 1
2016-09-07T17:12:39.306814: step 3268, loss 0.0055961, acc 1
2016-09-07T17:12:39.980495: step 3269, loss 0.0096111, acc 1
2016-09-07T17:12:40.648600: step 3270, loss 0.0269209, acc 1
2016-09-07T17:12:41.316959: step 3271, loss 0.0176971, acc 1
2016-09-07T17:12:41.976583: step 3272, loss 0.0218446, acc 1
2016-09-07T17:12:42.650577: step 3273, loss 0.0391739, acc 1
2016-09-07T17:12:43.312899: step 3274, loss 0.0342902, acc 1
2016-09-07T17:12:43.990606: step 3275, loss 0.0600892, acc 0.96
2016-09-07T17:12:44.676716: step 3276, loss 0.0234115, acc 0.98
2016-09-07T17:12:45.350714: step 3277, loss 0.0224284, acc 0.98
2016-09-07T17:12:46.033007: step 3278, loss 0.00289133, acc 1
2016-09-07T17:12:46.694696: step 3279, loss 0.0440553, acc 0.98
2016-09-07T17:12:47.362010: step 3280, loss 0.0827494, acc 0.96
2016-09-07T17:12:48.020559: step 3281, loss 0.00368907, acc 1
2016-09-07T17:12:48.713322: step 3282, loss 0.0188838, acc 1
2016-09-07T17:12:49.389325: step 3283, loss 0.0723903, acc 0.96
2016-09-07T17:12:50.064102: step 3284, loss 0.0754168, acc 0.94
2016-09-07T17:12:50.744013: step 3285, loss 0.0397469, acc 0.98
2016-09-07T17:12:51.429720: step 3286, loss 0.0820451, acc 0.98
2016-09-07T17:12:52.096248: step 3287, loss 0.182426, acc 0.96
2016-09-07T17:12:52.758003: step 3288, loss 0.00780659, acc 1
2016-09-07T17:12:53.438668: step 3289, loss 0.0011291, acc 1
2016-09-07T17:12:54.121669: step 3290, loss 0.059167, acc 0.98
2016-09-07T17:12:54.804984: step 3291, loss 0.116164, acc 0.94
2016-09-07T17:12:55.499482: step 3292, loss 0.0513368, acc 0.96
2016-09-07T17:12:56.175364: step 3293, loss 0.0395795, acc 0.98
2016-09-07T17:12:56.855415: step 3294, loss 0.0301793, acc 0.98
2016-09-07T17:12:57.523291: step 3295, loss 0.0682103, acc 0.96
2016-09-07T17:12:58.171728: step 3296, loss 0.0439402, acc 0.98
2016-09-07T17:12:58.826406: step 3297, loss 0.0111105, acc 1
2016-09-07T17:12:59.209074: step 3298, loss 0.0849134, acc 0.916667
2016-09-07T17:12:59.882202: step 3299, loss 0.0884155, acc 0.94
2016-09-07T17:13:00.571576: step 3300, loss 0.0178851, acc 0.98

Evaluation:
2016-09-07T17:13:03.551434: step 3300, loss 1.53353, acc 0.756

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-3300

2016-09-07T17:13:05.136099: step 3301, loss 0.0263588, acc 0.98
2016-09-07T17:13:05.810685: step 3302, loss 0.0153366, acc 1
2016-09-07T17:13:06.501574: step 3303, loss 0.0114669, acc 1
2016-09-07T17:13:07.166626: step 3304, loss 0.0458151, acc 0.94
2016-09-07T17:13:07.850171: step 3305, loss 0.195731, acc 0.96
2016-09-07T17:13:08.519289: step 3306, loss 0.0458415, acc 0.98
2016-09-07T17:13:09.184551: step 3307, loss 0.00331384, acc 1
2016-09-07T17:13:09.875443: step 3308, loss 0.0231564, acc 1
2016-09-07T17:13:10.560706: step 3309, loss 0.0020022, acc 1
2016-09-07T17:13:11.240678: step 3310, loss 0.0895497, acc 0.98
2016-09-07T17:13:11.918612: step 3311, loss 0.141604, acc 0.92
2016-09-07T17:13:12.585985: step 3312, loss 0.0410615, acc 0.98
2016-09-07T17:13:13.259114: step 3313, loss 0.0311604, acc 0.98
2016-09-07T17:13:13.936447: step 3314, loss 0.0187131, acc 1
2016-09-07T17:13:14.619821: step 3315, loss 0.0357339, acc 0.96
2016-09-07T17:13:15.293647: step 3316, loss 0.0613219, acc 0.98
2016-09-07T17:13:15.963710: step 3317, loss 0.0228733, acc 1
2016-09-07T17:13:16.643477: step 3318, loss 0.100413, acc 0.94
2016-09-07T17:13:17.316819: step 3319, loss 0.0203764, acc 1
2016-09-07T17:13:18.004354: step 3320, loss 0.0986573, acc 0.9
2016-09-07T17:13:18.688896: step 3321, loss 0.0504761, acc 0.98
2016-09-07T17:13:19.365429: step 3322, loss 0.0264823, acc 0.98
2016-09-07T17:13:20.042542: step 3323, loss 0.0336709, acc 0.98
2016-09-07T17:13:20.711861: step 3324, loss 0.0639374, acc 0.96
2016-09-07T17:13:21.380186: step 3325, loss 0.0215786, acc 0.98
2016-09-07T17:13:22.056170: step 3326, loss 0.0103649, acc 1
2016-09-07T17:13:22.731513: step 3327, loss 0.0669003, acc 0.98
2016-09-07T17:13:23.414964: step 3328, loss 0.0426418, acc 0.98
2016-09-07T17:13:24.081993: step 3329, loss 0.0187285, acc 1
2016-09-07T17:13:24.751023: step 3330, loss 0.0564755, acc 0.96
2016-09-07T17:13:25.400304: step 3331, loss 0.0196416, acc 0.98
2016-09-07T17:13:26.072182: step 3332, loss 0.038659, acc 0.98
2016-09-07T17:13:26.749462: step 3333, loss 0.0591605, acc 0.96
2016-09-07T17:13:27.421282: step 3334, loss 0.0720367, acc 0.94
2016-09-07T17:13:28.080185: step 3335, loss 0.03061, acc 0.96
2016-09-07T17:13:28.739242: step 3336, loss 0.0693492, acc 0.98
2016-09-07T17:13:29.419548: step 3337, loss 0.00175407, acc 1
2016-09-07T17:13:30.078263: step 3338, loss 0.00883215, acc 1
2016-09-07T17:13:30.758765: step 3339, loss 0.0409071, acc 1
2016-09-07T17:13:31.411695: step 3340, loss 0.0406183, acc 1
2016-09-07T17:13:32.061223: step 3341, loss 0.00262641, acc 1
2016-09-07T17:13:32.722459: step 3342, loss 0.00113737, acc 1
2016-09-07T17:13:33.410446: step 3343, loss 0.0452691, acc 1
2016-09-07T17:13:34.251624: step 3344, loss 0.0477359, acc 0.98
2016-09-07T17:13:35.000013: step 3345, loss 0.00284293, acc 1
2016-09-07T17:13:35.778260: step 3346, loss 0.0573532, acc 0.96
2016-09-07T17:13:36.457763: step 3347, loss 0.0166961, acc 0.98
2016-09-07T17:13:37.130916: step 3348, loss 0.0267704, acc 1
2016-09-07T17:13:37.822361: step 3349, loss 0.0252642, acc 0.98
2016-09-07T17:13:38.640327: step 3350, loss 0.0213492, acc 0.98
2016-09-07T17:13:39.393586: step 3351, loss 0.023875, acc 0.98
2016-09-07T17:13:40.081948: step 3352, loss 0.0275335, acc 0.98
2016-09-07T17:13:40.779508: step 3353, loss 0.0482345, acc 0.96
2016-09-07T17:13:41.454928: step 3354, loss 0.0839137, acc 0.96
2016-09-07T17:13:42.125262: step 3355, loss 0.0790217, acc 0.96
2016-09-07T17:13:42.841751: step 3356, loss 0.0182497, acc 0.98
2016-09-07T17:13:43.511832: step 3357, loss 0.060124, acc 0.96
2016-09-07T17:13:44.180566: step 3358, loss 0.0029561, acc 1
2016-09-07T17:13:44.867332: step 3359, loss 0.0428318, acc 0.98
2016-09-07T17:13:45.544044: step 3360, loss 0.017869, acc 1
2016-09-07T17:13:46.234001: step 3361, loss 0.0312347, acc 0.96
2016-09-07T17:13:46.929901: step 3362, loss 0.135734, acc 0.96
2016-09-07T17:13:47.604455: step 3363, loss 0.12199, acc 0.92
2016-09-07T17:13:48.371399: step 3364, loss 0.0875898, acc 0.96
2016-09-07T17:13:49.047451: step 3365, loss 0.0183534, acc 1
2016-09-07T17:13:49.744833: step 3366, loss 0.00764179, acc 1
2016-09-07T17:13:50.435725: step 3367, loss 0.00647861, acc 1
2016-09-07T17:13:51.120480: step 3368, loss 0.0248348, acc 0.98
2016-09-07T17:13:51.795153: step 3369, loss 0.00285975, acc 1
2016-09-07T17:13:52.502477: step 3370, loss 0.0216869, acc 1
2016-09-07T17:13:53.191822: step 3371, loss 0.00554076, acc 1
2016-09-07T17:13:53.880149: step 3372, loss 0.0400775, acc 1
2016-09-07T17:13:54.543168: step 3373, loss 0.0190958, acc 0.98
2016-09-07T17:13:55.243222: step 3374, loss 0.0599734, acc 0.96
2016-09-07T17:13:55.979173: step 3375, loss 0.000363017, acc 1
2016-09-07T17:13:56.659115: step 3376, loss 0.064381, acc 0.94
2016-09-07T17:13:57.330373: step 3377, loss 0.0502366, acc 0.96
2016-09-07T17:13:58.030337: step 3378, loss 0.0164145, acc 1
2016-09-07T17:13:58.707019: step 3379, loss 0.0461858, acc 0.98
2016-09-07T17:13:59.378294: step 3380, loss 0.0388531, acc 0.98
2016-09-07T17:14:00.046243: step 3381, loss 0.016387, acc 1
2016-09-07T17:14:00.701666: step 3382, loss 0.00355232, acc 1
2016-09-07T17:14:01.368720: step 3383, loss 0.0216946, acc 0.98
2016-09-07T17:14:02.063068: step 3384, loss 0.0219147, acc 0.98
2016-09-07T17:14:02.713467: step 3385, loss 0.0420987, acc 0.98
2016-09-07T17:14:03.381461: step 3386, loss 0.0617648, acc 0.98
2016-09-07T17:14:04.054340: step 3387, loss 0.0669861, acc 0.96
2016-09-07T17:14:04.708036: step 3388, loss 0.0135614, acc 1
2016-09-07T17:14:05.371698: step 3389, loss 0.0210841, acc 1
2016-09-07T17:14:06.054477: step 3390, loss 0.0548781, acc 0.98
2016-09-07T17:14:06.724055: step 3391, loss 0.076158, acc 0.94
2016-09-07T17:14:07.391098: step 3392, loss 0.00159473, acc 1
2016-09-07T17:14:08.105539: step 3393, loss 0.056791, acc 0.96
2016-09-07T17:14:08.782358: step 3394, loss 0.01947, acc 1
2016-09-07T17:14:09.464509: step 3395, loss 0.0183927, acc 1
2016-09-07T17:14:10.146255: step 3396, loss 0.0605827, acc 0.96
2016-09-07T17:14:10.834299: step 3397, loss 0.0123844, acc 1
2016-09-07T17:14:11.517522: step 3398, loss 0.0189918, acc 1
2016-09-07T17:14:12.277372: step 3399, loss 0.0331329, acc 0.98
2016-09-07T17:14:13.008259: step 3400, loss 0.0150361, acc 1

Evaluation:
2016-09-07T17:14:16.230224: step 3400, loss 2.12053, acc 0.737

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-3400

2016-09-07T17:14:17.841887: step 3401, loss 0.0252718, acc 1
2016-09-07T17:14:18.556920: step 3402, loss 0.0651225, acc 0.96
2016-09-07T17:14:19.365246: step 3403, loss 0.00533415, acc 1
2016-09-07T17:14:20.031629: step 3404, loss 0.03701, acc 0.98
2016-09-07T17:14:20.770131: step 3405, loss 0.124579, acc 0.96
2016-09-07T17:14:21.461996: step 3406, loss 0.0115522, acc 1
2016-09-07T17:14:22.255168: step 3407, loss 0.0484146, acc 0.98
2016-09-07T17:14:22.945714: step 3408, loss 0.00139416, acc 1
2016-09-07T17:14:23.627017: step 3409, loss 0.0102783, acc 1
2016-09-07T17:14:24.373613: step 3410, loss 0.0133875, acc 1
2016-09-07T17:14:25.062194: step 3411, loss 0.163739, acc 0.94
2016-09-07T17:14:25.779889: step 3412, loss 0.0819075, acc 0.96
2016-09-07T17:14:26.495389: step 3413, loss 0.0505324, acc 0.96
2016-09-07T17:14:27.356281: step 3414, loss 0.121399, acc 0.94
2016-09-07T17:14:28.055791: step 3415, loss 0.00501733, acc 1
2016-09-07T17:14:28.790802: step 3416, loss 0.00268377, acc 1
2016-09-07T17:14:29.658918: step 3417, loss 0.0115406, acc 1
2016-09-07T17:14:30.421074: step 3418, loss 0.0147733, acc 1
2016-09-07T17:14:31.318420: step 3419, loss 0.0999575, acc 0.98
2016-09-07T17:14:32.054652: step 3420, loss 0.0430043, acc 0.96
2016-09-07T17:14:32.816661: step 3421, loss 0.0262378, acc 1
2016-09-07T17:14:33.585710: step 3422, loss 0.0186595, acc 0.98
2016-09-07T17:14:34.295456: step 3423, loss 0.265906, acc 0.94
2016-09-07T17:14:35.070148: step 3424, loss 0.0177331, acc 1
2016-09-07T17:14:35.815539: step 3425, loss 0.0226928, acc 1
2016-09-07T17:14:36.567227: step 3426, loss 0.0305171, acc 1
2016-09-07T17:14:37.378412: step 3427, loss 0.0267647, acc 0.98
2016-09-07T17:14:38.136664: step 3428, loss 0.162883, acc 0.96
2016-09-07T17:14:38.884187: step 3429, loss 0.0354453, acc 0.96
2016-09-07T17:14:39.628195: step 3430, loss 0.0619744, acc 0.98
2016-09-07T17:14:40.321326: step 3431, loss 0.0453952, acc 0.98
2016-09-07T17:14:41.063506: step 3432, loss 0.0900141, acc 0.94
2016-09-07T17:14:41.816047: step 3433, loss 0.0716938, acc 0.96
2016-09-07T17:14:42.554422: step 3434, loss 0.126179, acc 0.96
2016-09-07T17:14:43.344395: step 3435, loss 0.0883183, acc 0.98
2016-09-07T17:14:44.080756: step 3436, loss 0.00733048, acc 1
2016-09-07T17:14:44.830900: step 3437, loss 0.0139767, acc 1
2016-09-07T17:14:45.635722: step 3438, loss 0.0545029, acc 0.98
2016-09-07T17:14:46.323418: step 3439, loss 0.101417, acc 0.96
2016-09-07T17:14:47.084803: step 3440, loss 0.0346445, acc 0.98
2016-09-07T17:14:47.840335: step 3441, loss 0.0339013, acc 0.98
2016-09-07T17:14:48.562262: step 3442, loss 0.0125257, acc 1
2016-09-07T17:14:49.343728: step 3443, loss 0.0946303, acc 0.98
2016-09-07T17:14:50.072665: step 3444, loss 0.0563459, acc 0.98
2016-09-07T17:14:50.801764: step 3445, loss 0.0494254, acc 0.98
2016-09-07T17:14:51.598296: step 3446, loss 0.0426762, acc 0.98
2016-09-07T17:14:52.385434: step 3447, loss 0.0370926, acc 1
2016-09-07T17:14:53.137070: step 3448, loss 0.0166447, acc 0.98
2016-09-07T17:14:53.831190: step 3449, loss 0.0706842, acc 0.94
2016-09-07T17:14:54.612223: step 3450, loss 0.0362906, acc 1
2016-09-07T17:14:55.315231: step 3451, loss 0.0259865, acc 0.98
2016-09-07T17:14:56.086445: step 3452, loss 0.110301, acc 0.92
2016-09-07T17:14:56.788758: step 3453, loss 0.0535033, acc 0.98
2016-09-07T17:14:57.458414: step 3454, loss 0.0199686, acc 0.98
2016-09-07T17:14:58.126682: step 3455, loss 0.053272, acc 0.98
2016-09-07T17:14:58.821053: step 3456, loss 0.0114734, acc 1
2016-09-07T17:14:59.510232: step 3457, loss 0.0287447, acc 0.98
2016-09-07T17:15:00.192831: step 3458, loss 0.0231623, acc 0.98
2016-09-07T17:15:00.876987: step 3459, loss 0.0659415, acc 0.94
2016-09-07T17:15:01.552929: step 3460, loss 0.024811, acc 1
2016-09-07T17:15:02.244859: step 3461, loss 0.0610354, acc 0.96
2016-09-07T17:15:02.925806: step 3462, loss 0.0103903, acc 1
2016-09-07T17:15:03.601211: step 3463, loss 0.164382, acc 0.96
2016-09-07T17:15:04.279770: step 3464, loss 0.0759535, acc 0.98
2016-09-07T17:15:04.955270: step 3465, loss 0.0196274, acc 1
2016-09-07T17:15:05.634238: step 3466, loss 0.0220491, acc 0.98
2016-09-07T17:15:06.309636: step 3467, loss 0.121406, acc 0.96
2016-09-07T17:15:06.991988: step 3468, loss 0.0280071, acc 0.98
2016-09-07T17:15:07.667805: step 3469, loss 0.0986653, acc 0.94
2016-09-07T17:15:08.346329: step 3470, loss 0.00211289, acc 1
2016-09-07T17:15:09.025927: step 3471, loss 0.0522415, acc 0.96
2016-09-07T17:15:09.712422: step 3472, loss 0.0232518, acc 1
2016-09-07T17:15:10.380952: step 3473, loss 0.021007, acc 1
2016-09-07T17:15:11.046359: step 3474, loss 0.0310375, acc 0.98
2016-09-07T17:15:11.711274: step 3475, loss 0.0173181, acc 1
2016-09-07T17:15:12.373962: step 3476, loss 0.0234776, acc 0.98
2016-09-07T17:15:13.041263: step 3477, loss 0.0195747, acc 1
2016-09-07T17:15:13.711328: step 3478, loss 0.0224544, acc 0.98
2016-09-07T17:15:14.384464: step 3479, loss 0.0521257, acc 0.98
2016-09-07T17:15:15.057874: step 3480, loss 0.0146654, acc 1
2016-09-07T17:15:15.708236: step 3481, loss 0.0135553, acc 1
2016-09-07T17:15:16.357188: step 3482, loss 0.0166392, acc 1
2016-09-07T17:15:17.020251: step 3483, loss 0.0770531, acc 0.98
2016-09-07T17:15:17.700407: step 3484, loss 0.00591542, acc 1
2016-09-07T17:15:18.371050: step 3485, loss 0.0625047, acc 0.96
2016-09-07T17:15:19.043417: step 3486, loss 0.00227966, acc 1
2016-09-07T17:15:19.706417: step 3487, loss 0.0395114, acc 0.98
2016-09-07T17:15:20.393626: step 3488, loss 0.0652526, acc 0.94
2016-09-07T17:15:21.055069: step 3489, loss 0.0361047, acc 0.98
2016-09-07T17:15:21.795078: step 3490, loss 0.0612438, acc 0.96
2016-09-07T17:15:22.466651: step 3491, loss 0.0276863, acc 1
2016-09-07T17:15:22.834946: step 3492, loss 0.0752215, acc 0.916667
2016-09-07T17:15:23.523984: step 3493, loss 0.0245525, acc 0.98
2016-09-07T17:15:24.226137: step 3494, loss 0.0341793, acc 0.98
2016-09-07T17:15:24.921663: step 3495, loss 0.0254186, acc 1
2016-09-07T17:15:25.605421: step 3496, loss 0.0247985, acc 0.98
2016-09-07T17:15:26.424106: step 3497, loss 0.00119309, acc 1
2016-09-07T17:15:27.280898: step 3498, loss 0.00742414, acc 1
2016-09-07T17:15:28.039894: step 3499, loss 0.0368916, acc 0.98
2016-09-07T17:15:28.838539: step 3500, loss 0.0129291, acc 1

Evaluation:
2016-09-07T17:15:32.089033: step 3500, loss 1.879, acc 0.746

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-3500

2016-09-07T17:15:33.837523: step 3501, loss 0.0806084, acc 0.96
2016-09-07T17:15:34.535907: step 3502, loss 0.0137763, acc 1
2016-09-07T17:15:35.269469: step 3503, loss 0.0103764, acc 1
2016-09-07T17:15:35.936269: step 3504, loss 0.035261, acc 0.98
2016-09-07T17:15:36.592978: step 3505, loss 0.0294222, acc 1
2016-09-07T17:15:37.313592: step 3506, loss 0.137877, acc 0.92
2016-09-07T17:15:37.988398: step 3507, loss 0.0173539, acc 0.98
2016-09-07T17:15:38.679487: step 3508, loss 0.0156934, acc 1
2016-09-07T17:15:39.378568: step 3509, loss 0.0103813, acc 1
2016-09-07T17:15:40.044733: step 3510, loss 0.00122669, acc 1
2016-09-07T17:15:40.735021: step 3511, loss 0.0187889, acc 1
2016-09-07T17:15:41.432565: step 3512, loss 0.039777, acc 0.98
2016-09-07T17:15:42.128783: step 3513, loss 0.0226794, acc 1
2016-09-07T17:15:42.800814: step 3514, loss 0.0816474, acc 0.96
2016-09-07T17:15:43.478896: step 3515, loss 0.00205281, acc 1
2016-09-07T17:15:44.172328: step 3516, loss 0.0193442, acc 0.98
2016-09-07T17:15:44.866066: step 3517, loss 0.129865, acc 0.96
2016-09-07T17:15:45.538709: step 3518, loss 0.01455, acc 1
2016-09-07T17:15:46.204200: step 3519, loss 0.0414286, acc 0.98
2016-09-07T17:15:46.886242: step 3520, loss 0.0187943, acc 1
2016-09-07T17:15:47.572945: step 3521, loss 0.0291245, acc 0.98
2016-09-07T17:15:48.232297: step 3522, loss 0.0583939, acc 0.98
2016-09-07T17:15:48.889813: step 3523, loss 0.0433256, acc 0.98
2016-09-07T17:15:49.557809: step 3524, loss 0.054183, acc 0.98
2016-09-07T17:15:50.233117: step 3525, loss 0.0307013, acc 0.98
2016-09-07T17:15:50.938750: step 3526, loss 0.131433, acc 0.96
2016-09-07T17:15:51.625829: step 3527, loss 0.0166932, acc 1
2016-09-07T17:15:52.301812: step 3528, loss 0.161823, acc 0.94
2016-09-07T17:15:52.970037: step 3529, loss 0.00342196, acc 1
2016-09-07T17:15:53.662588: step 3530, loss 0.0605647, acc 0.96
2016-09-07T17:15:54.349310: step 3531, loss 0.0022672, acc 1
2016-09-07T17:15:55.035148: step 3532, loss 0.0011786, acc 1
2016-09-07T17:15:55.722362: step 3533, loss 0.0196981, acc 0.98
2016-09-07T17:15:56.415495: step 3534, loss 0.0309863, acc 0.98
2016-09-07T17:15:57.096870: step 3535, loss 0.0451132, acc 0.98
2016-09-07T17:15:57.773814: step 3536, loss 0.0523243, acc 0.96
2016-09-07T17:15:58.449345: step 3537, loss 0.0329474, acc 0.98
2016-09-07T17:15:59.123158: step 3538, loss 0.0511799, acc 0.98
2016-09-07T17:15:59.809435: step 3539, loss 0.0705367, acc 0.94
2016-09-07T17:16:00.496240: step 3540, loss 0.012658, acc 1
2016-09-07T17:16:01.181901: step 3541, loss 0.00309347, acc 1
2016-09-07T17:16:01.868724: step 3542, loss 0.0839034, acc 0.96
2016-09-07T17:16:02.536643: step 3543, loss 0.0626421, acc 0.96
2016-09-07T17:16:03.191424: step 3544, loss 0.0479403, acc 0.96
2016-09-07T17:16:03.868859: step 3545, loss 0.0196446, acc 1
2016-09-07T17:16:04.562385: step 3546, loss 0.0522409, acc 0.98
2016-09-07T17:16:05.233587: step 3547, loss 0.0955289, acc 0.94
2016-09-07T17:16:05.912612: step 3548, loss 0.00611834, acc 1
2016-09-07T17:16:06.587312: step 3549, loss 0.0372882, acc 0.98
2016-09-07T17:16:07.273274: step 3550, loss 0.0144598, acc 1
2016-09-07T17:16:07.956571: step 3551, loss 0.133143, acc 0.96
2016-09-07T17:16:08.636243: step 3552, loss 0.00223221, acc 1
2016-09-07T17:16:09.294081: step 3553, loss 0.0495583, acc 0.98
2016-09-07T17:16:09.977253: step 3554, loss 0.0248959, acc 1
2016-09-07T17:16:10.658577: step 3555, loss 0.0838528, acc 0.96
2016-09-07T17:16:11.327629: step 3556, loss 0.0248828, acc 1
2016-09-07T17:16:12.000066: step 3557, loss 0.0354974, acc 0.98
2016-09-07T17:16:12.673228: step 3558, loss 0.0417143, acc 0.98
2016-09-07T17:16:13.347601: step 3559, loss 0.0953898, acc 0.98
2016-09-07T17:16:14.024314: step 3560, loss 0.0241583, acc 0.98
2016-09-07T17:16:14.710570: step 3561, loss 0.0119301, acc 1
2016-09-07T17:16:15.374795: step 3562, loss 0.020356, acc 1
2016-09-07T17:16:16.041780: step 3563, loss 0.0619227, acc 0.96
2016-09-07T17:16:16.729704: step 3564, loss 0.0430535, acc 0.98
2016-09-07T17:16:17.411584: step 3565, loss 0.00205685, acc 1
2016-09-07T17:16:18.092165: step 3566, loss 0.0114974, acc 1
2016-09-07T17:16:18.769797: step 3567, loss 0.00105687, acc 1
2016-09-07T17:16:19.439250: step 3568, loss 0.0320973, acc 0.98
2016-09-07T17:16:20.107848: step 3569, loss 0.0116048, acc 1
2016-09-07T17:16:20.794748: step 3570, loss 0.0462295, acc 0.98
2016-09-07T17:16:21.470702: step 3571, loss 0.0234477, acc 1
2016-09-07T17:16:22.134338: step 3572, loss 0.0298671, acc 0.96
2016-09-07T17:16:22.832442: step 3573, loss 0.0191005, acc 1
2016-09-07T17:16:23.516925: step 3574, loss 0.0236834, acc 0.98
2016-09-07T17:16:24.182491: step 3575, loss 0.0185358, acc 1
2016-09-07T17:16:24.868565: step 3576, loss 0.0249118, acc 0.98
2016-09-07T17:16:25.566928: step 3577, loss 0.00725652, acc 1
2016-09-07T17:16:26.242717: step 3578, loss 0.125566, acc 0.94
2016-09-07T17:16:26.917077: step 3579, loss 0.0757785, acc 0.98
2016-09-07T17:16:27.638790: step 3580, loss 0.0129607, acc 1
2016-09-07T17:16:28.305381: step 3581, loss 0.0157076, acc 1
2016-09-07T17:16:28.969829: step 3582, loss 0.0463263, acc 0.96
2016-09-07T17:16:29.640609: step 3583, loss 0.0116235, acc 1
2016-09-07T17:16:30.337297: step 3584, loss 0.0046481, acc 1
2016-09-07T17:16:31.037037: step 3585, loss 0.0173973, acc 1
2016-09-07T17:16:31.734253: step 3586, loss 0.0542663, acc 0.98
2016-09-07T17:16:32.400750: step 3587, loss 0.0342179, acc 1
2016-09-07T17:16:33.091996: step 3588, loss 0.0181745, acc 0.98
2016-09-07T17:16:33.785879: step 3589, loss 0.0409053, acc 0.96
2016-09-07T17:16:34.505629: step 3590, loss 0.0198103, acc 1
2016-09-07T17:16:35.214803: step 3591, loss 0.034545, acc 1
2016-09-07T17:16:35.896918: step 3592, loss 0.0648174, acc 0.94
2016-09-07T17:16:36.583732: step 3593, loss 0.0118127, acc 1
2016-09-07T17:16:37.256704: step 3594, loss 0.00971991, acc 1
2016-09-07T17:16:37.911592: step 3595, loss 0.033759, acc 0.98
2016-09-07T17:16:38.588389: step 3596, loss 0.041894, acc 0.98
2016-09-07T17:16:39.272948: step 3597, loss 0.0186686, acc 1
2016-09-07T17:16:39.948737: step 3598, loss 0.0511064, acc 0.98
2016-09-07T17:16:40.626381: step 3599, loss 0.0820476, acc 0.98
2016-09-07T17:16:41.305357: step 3600, loss 0.0208419, acc 1

Evaluation:
2016-09-07T17:16:44.206682: step 3600, loss 2.12924, acc 0.739

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-3600

2016-09-07T17:16:45.853270: step 3601, loss 0.0137129, acc 1
2016-09-07T17:16:46.541428: step 3602, loss 0.0160703, acc 1
2016-09-07T17:16:47.209777: step 3603, loss 0.0920497, acc 0.96
2016-09-07T17:16:47.877364: step 3604, loss 0.00500069, acc 1
2016-09-07T17:16:48.561330: step 3605, loss 0.00664066, acc 1
2016-09-07T17:16:49.228742: step 3606, loss 0.038221, acc 0.98
2016-09-07T17:16:49.895962: step 3607, loss 0.0204266, acc 0.98
2016-09-07T17:16:50.579947: step 3608, loss 0.0288572, acc 1
2016-09-07T17:16:51.263848: step 3609, loss 0.0178115, acc 1
2016-09-07T17:16:51.934115: step 3610, loss 0.0683142, acc 0.94
2016-09-07T17:16:52.610177: step 3611, loss 0.0241571, acc 0.98
2016-09-07T17:16:53.295345: step 3612, loss 0.0689817, acc 0.96
2016-09-07T17:16:53.997826: step 3613, loss 0.00105226, acc 1
2016-09-07T17:16:54.676071: step 3614, loss 0.0206455, acc 0.98
2016-09-07T17:16:55.338833: step 3615, loss 0.00308818, acc 1
2016-09-07T17:16:56.024335: step 3616, loss 0.0059913, acc 1
2016-09-07T17:16:56.708336: step 3617, loss 0.0496738, acc 1
2016-09-07T17:16:57.386448: step 3618, loss 0.0176439, acc 0.98
2016-09-07T17:16:58.059565: step 3619, loss 0.0161688, acc 1
2016-09-07T17:16:58.726653: step 3620, loss 0.0500842, acc 0.98
2016-09-07T17:16:59.387633: step 3621, loss 0.0122973, acc 1
2016-09-07T17:17:00.057568: step 3622, loss 0.0638445, acc 0.98
2016-
WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f2e1f944710>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.
WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f2e1f9447d0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.

Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=50
CHECKPOINT_EVERY=100
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=300
EVALUATE_EVERY=100
FILTER_SIZES=3,4,5
HIDDEN_DIM=300
L2_REG_LAMBDA=0
LOG_DEVICE_PLACEMENT=False
NUM_EPOCHS=30
NUM_FILTERS=100
WORD2VEC=GoogleNews-vectors-negative300.bin

Loading data...
Vocabulary Size: 18758
Train/Dev split: 9662/1000
Writing to /home/cil/lstm-context-embeddings/runs/1473237087

Load word2vec file GoogleNews-vectors-negative300.bin

2016-09-07T16:31:46.718308: step 1, loss 0.693147, acc 0.44
2016-09-07T16:31:47.394738: step 2, loss 0.715837, acc 0.44
2016-09-07T16:31:48.090775: step 3, loss 0.700977, acc 0.4
2016-09-07T16:31:48.789307: step 4, loss 0.685589, acc 0.54
2016-09-07T16:31:49.483754: step 5, loss 0.664431, acc 0.64
2016-09-07T16:31:50.160819: step 6, loss 0.693013, acc 0.56
2016-09-07T16:31:50.857819: step 7, loss 0.751501, acc 0.46
2016-09-07T16:31:51.533647: step 8, loss 0.75942, acc 0.42
2016-09-07T16:31:52.365876: step 9, loss 0.684698, acc 0.6
2016-09-07T16:31:53.280413: step 10, loss 0.695854, acc 0.5
2016-09-07T16:31:54.025943: step 11, loss 0.693774, acc 0.52
2016-09-07T16:31:54.740550: step 12, loss 0.703, acc 0.46
2016-09-07T16:31:55.531714: step 13, loss 0.690964, acc 0.56
2016-09-07T16:31:56.334934: step 14, loss 0.685738, acc 0.54
2016-09-07T16:31:57.172188: step 15, loss 0.681963, acc 0.54
2016-09-07T16:31:57.971717: step 16, loss 0.694408, acc 0.52
2016-09-07T16:31:58.810787: step 17, loss 0.687224, acc 0.5
2016-09-07T16:31:59.514360: step 18, loss 0.720647, acc 0.38
2016-09-07T16:32:00.340239: step 19, loss 0.690365, acc 0.56
2016-09-07T16:32:01.031181: step 20, loss 0.672867, acc 0.6
2016-09-07T16:32:01.709928: step 21, loss 0.675763, acc 0.56
2016-09-07T16:32:02.447676: step 22, loss 0.694954, acc 0.56
2016-09-07T16:32:03.134698: step 23, loss 0.70257, acc 0.52
2016-09-07T16:32:03.880364: step 24, loss 0.704913, acc 0.48
2016-09-07T16:32:04.619947: step 25, loss 0.681673, acc 0.52
2016-09-07T16:32:05.366018: step 26, loss 0.649397, acc 0.6
2016-09-07T16:32:06.170583: step 27, loss 0.656208, acc 0.62
2016-09-07T16:32:06.930065: step 28, loss 0.668989, acc 0.52
2016-09-07T16:32:07.654522: step 29, loss 0.660736, acc 0.52
2016-09-07T16:32:08.409354: step 30, loss 0.714112, acc 0.52
2016-09-07T16:32:09.189771: step 31, loss 0.591963, acc 0.68
2016-09-07T16:32:10.021577: step 32, loss 0.652451, acc 0.56
2016-09-07T16:32:10.738540: step 33, loss 0.652408, acc 0.62
2016-09-07T16:32:11.531284: step 34, loss 0.663438, acc 0.66
2016-09-07T16:32:12.350131: step 35, loss 0.5742, acc 0.66
2016-09-07T16:32:13.095631: step 36, loss 0.608849, acc 0.74
2016-09-07T16:32:13.794903: step 37, loss 0.684286, acc 0.64
2016-09-07T16:32:14.529851: step 38, loss 0.657265, acc 0.62
2016-09-07T16:32:15.252336: step 39, loss 0.546291, acc 0.76
2016-09-07T16:32:15.963541: step 40, loss 0.578165, acc 0.66
2016-09-07T16:32:16.718738: step 41, loss 0.572656, acc 0.66
2016-09-07T16:32:17.374901: step 42, loss 0.57249, acc 0.62
2016-09-07T16:32:18.036533: step 43, loss 0.58589, acc 0.7
2016-09-07T16:32:18.727991: step 44, loss 0.560574, acc 0.72
2016-09-07T16:32:19.432195: step 45, loss 0.646048, acc 0.62
2016-09-07T16:32:20.110507: step 46, loss 0.571342, acc 0.68
2016-09-07T16:32:20.824079: step 47, loss 0.478498, acc 0.82
2016-09-07T16:32:21.692086: step 48, loss 0.607271, acc 0.74
2016-09-07T16:32:22.445920: step 49, loss 0.63603, acc 0.72
2016-09-07T16:32:23.271092: step 50, loss 0.774592, acc 0.56
2016-09-07T16:32:24.000708: step 51, loss 0.703827, acc 0.72
2016-09-07T16:32:24.804542: step 52, loss 0.53628, acc 0.76
2016-09-07T16:32:25.630227: step 53, loss 0.629699, acc 0.62
2016-09-07T16:32:26.463976: step 54, loss 0.668435, acc 0.62
2016-09-07T16:32:27.144795: step 55, loss 0.632459, acc 0.66
2016-09-07T16:32:27.855460: step 56, loss 0.534072, acc 0.8
2016-09-07T16:32:28.701530: step 57, loss 0.608512, acc 0.7
2016-09-07T16:32:29.534917: step 58, loss 0.641094, acc 0.66
2016-09-07T16:32:30.290582: step 59, loss 0.599806, acc 0.68
2016-09-07T16:32:30.938323: step 60, loss 0.632723, acc 0.7
2016-09-07T16:32:31.661746: step 61, loss 0.648755, acc 0.58
2016-09-07T16:32:32.434040: step 62, loss 0.581271, acc 0.64
2016-09-07T16:32:33.207649: step 63, loss 0.593713, acc 0.72
2016-09-07T16:32:33.904051: step 64, loss 0.547411, acc 0.74
2016-09-07T16:32:34.588592: step 65, loss 0.478613, acc 0.8
2016-09-07T16:32:35.287940: step 66, loss 0.585492, acc 0.66
2016-09-07T16:32:35.988342: step 67, loss 0.559569, acc 0.76
2016-09-07T16:32:36.660120: step 68, loss 0.650721, acc 0.64
2016-09-07T16:32:37.339489: step 69, loss 0.488781, acc 0.74
2016-09-07T16:32:38.133745: step 70, loss 0.571411, acc 0.72
2016-09-07T16:32:38.894054: step 71, loss 0.580112, acc 0.72
2016-09-07T16:32:39.649105: step 72, loss 0.508671, acc 0.7
2016-09-07T16:32:40.460566: step 73, loss 0.450462, acc 0.84
2016-09-07T16:32:41.231583: step 74, loss 0.478204, acc 0.78
2016-09-07T16:32:41.974167: step 75, loss 0.544987, acc 0.76
2016-09-07T16:32:42.738438: step 76, loss 0.637247, acc 0.68
2016-09-07T16:32:43.389326: step 77, loss 0.497721, acc 0.78
2016-09-07T16:32:44.065395: step 78, loss 0.585467, acc 0.72
2016-09-07T16:32:44.739408: step 79, loss 0.491918, acc 0.72
2016-09-07T16:32:45.492317: step 80, loss 0.678174, acc 0.64
2016-09-07T16:32:46.213581: step 81, loss 0.48549, acc 0.72
2016-09-07T16:32:46.883186: step 82, loss 0.437981, acc 0.74
2016-09-07T16:32:47.545052: step 83, loss 0.538503, acc 0.78
2016-09-07T16:32:48.209948: step 84, loss 0.487267, acc 0.78
2016-09-07T16:32:48.890109: step 85, loss 0.513312, acc 0.76
2016-09-07T16:32:49.559103: step 86, loss 0.442034, acc 0.78
2016-09-07T16:32:50.236473: step 87, loss 0.611144, acc 0.66
2016-09-07T16:32:50.911583: step 88, loss 0.391934, acc 0.88
2016-09-07T16:32:51.577540: step 89, loss 0.407068, acc 0.82
2016-09-07T16:32:52.240880: step 90, loss 0.601504, acc 0.66
2016-09-07T16:32:52.898495: step 91, loss 0.496954, acc 0.74
2016-09-07T16:32:53.576811: step 92, loss 0.624445, acc 0.72
2016-09-07T16:32:54.242694: step 93, loss 0.572553, acc 0.64
2016-09-07T16:32:54.900679: step 94, loss 0.468107, acc 0.76
2016-09-07T16:32:55.568691: step 95, loss 0.410143, acc 0.78
2016-09-07T16:32:56.243289: step 96, loss 0.438503, acc 0.78
2016-09-07T16:32:56.903933: step 97, loss 0.604211, acc 0.72
2016-09-07T16:32:57.567491: step 98, loss 0.47228, acc 0.76
2016-09-07T16:32:58.231975: step 99, loss 0.604293, acc 0.66
2016-09-07T16:32:58.917202: step 100, loss 0.483287, acc 0.72

Evaluation:
2016-09-07T16:33:01.885498: step 100, loss 0.501049, acc 0.76

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-100

2016-09-07T16:33:03.677758: step 101, loss 0.59046, acc 0.7
2016-09-07T16:33:04.472156: step 102, loss 0.75174, acc 0.64
2016-09-07T16:33:05.283286: step 103, loss 0.470936, acc 0.78
2016-09-07T16:33:05.955050: step 104, loss 0.480786, acc 0.76
2016-09-07T16:33:06.680601: step 105, loss 0.478135, acc 0.76
2016-09-07T16:33:07.535786: step 106, loss 0.438855, acc 0.8
2016-09-07T16:33:08.317966: step 107, loss 0.492555, acc 0.78
2016-09-07T16:33:08.980369: step 108, loss 0.435213, acc 0.78
2016-09-07T16:33:09.743255: step 109, loss 0.453134, acc 0.84
2016-09-07T16:33:10.417784: step 110, loss 0.466226, acc 0.82
2016-09-07T16:33:11.208545: step 111, loss 0.533982, acc 0.8
2016-09-07T16:33:11.916005: step 112, loss 0.468403, acc 0.74
2016-09-07T16:33:12.639108: step 113, loss 0.502138, acc 0.74
2016-09-07T16:33:13.331762: step 114, loss 0.449187, acc 0.82
2016-09-07T16:33:14.033425: step 115, loss 0.439309, acc 0.82
2016-09-07T16:33:14.776051: step 116, loss 0.38055, acc 0.8
2016-09-07T16:33:15.469742: step 117, loss 0.70007, acc 0.62
2016-09-07T16:33:16.182326: step 118, loss 0.482099, acc 0.76
2016-09-07T16:33:16.840126: step 119, loss 0.585636, acc 0.7
2016-09-07T16:33:17.504573: step 120, loss 0.386783, acc 0.86
2016-09-07T16:33:18.242319: step 121, loss 0.582643, acc 0.76
2016-09-07T16:33:18.918856: step 122, loss 0.420628, acc 0.8
2016-09-07T16:33:19.626718: step 123, loss 0.58869, acc 0.66
2016-09-07T16:33:20.348800: step 124, loss 0.544941, acc 0.76
2016-09-07T16:33:21.205327: step 125, loss 0.651797, acc 0.62
2016-09-07T16:33:21.874326: step 126, loss 0.630289, acc 0.64
2016-09-07T16:33:22.602815: step 127, loss 0.656029, acc 0.64
2016-09-07T16:33:23.409051: step 128, loss 0.378022, acc 0.82
2016-09-07T16:33:24.151773: step 129, loss 0.431033, acc 0.78
2016-09-07T16:33:24.850277: step 130, loss 0.476805, acc 0.7
2016-09-07T16:33:25.573239: step 131, loss 0.454874, acc 0.82
2016-09-07T16:33:26.292705: step 132, loss 0.490497, acc 0.82
2016-09-07T16:33:27.025431: step 133, loss 0.526482, acc 0.68
2016-09-07T16:33:27.826426: step 134, loss 0.430812, acc 0.84
2016-09-07T16:33:28.642668: step 135, loss 0.560984, acc 0.72
2016-09-07T16:33:29.343097: step 136, loss 0.496779, acc 0.78
2016-09-07T16:33:30.018128: step 137, loss 0.502544, acc 0.7
2016-09-07T16:33:30.716585: step 138, loss 0.517937, acc 0.72
2016-09-07T16:33:31.376192: step 139, loss 0.472587, acc 0.76
2016-09-07T16:33:32.040394: step 140, loss 0.392937, acc 0.82
2016-09-07T16:33:32.766809: step 141, loss 0.574537, acc 0.7
2016-09-07T16:33:33.505319: step 142, loss 0.516281, acc 0.7
2016-09-07T16:33:34.214244: step 143, loss 0.57772, acc 0.7
2016-09-07T16:33:34.969417: step 144, loss 0.506364, acc 0.8
2016-09-07T16:33:35.718566: step 145, loss 0.475477, acc 0.76
2016-09-07T16:33:36.477993: step 146, loss 0.556798, acc 0.76
2016-09-07T16:33:37.170397: step 147, loss 0.542118, acc 0.7
2016-09-07T16:33:37.897526: step 148, loss 0.405258, acc 0.84
2016-09-07T16:33:38.598799: step 149, loss 0.504539, acc 0.72
2016-09-07T16:33:39.306432: step 150, loss 0.550984, acc 0.76
2016-09-07T16:33:40.007264: step 151, loss 0.559456, acc 0.66
2016-09-07T16:33:40.685343: step 152, loss 0.49592, acc 0.74
2016-09-07T16:33:41.357453: step 153, loss 0.560978, acc 0.72
2016-09-07T16:33:42.046570: step 154, loss 0.604108, acc 0.66
2016-09-07T16:33:42.818480: step 155, loss 0.459536, acc 0.72
2016-09-07T16:33:43.564289: step 156, loss 0.396596, acc 0.86
2016-09-07T16:33:44.325807: step 157, loss 0.470313, acc 0.72
2016-09-07T16:33:45.026048: step 158, loss 0.611565, acc 0.66
2016-09-07T16:33:45.748071: step 159, loss 0.501037, acc 0.74
2016-09-07T16:33:46.498116: step 160, loss 0.539548, acc 0.7
2016-09-07T16:33:47.181747: step 161, loss 0.585322, acc 0.72
2016-09-07T16:33:47.844472: step 162, loss 0.600838, acc 0.66
2016-09-07T16:33:48.519612: step 163, loss 0.414237, acc 0.76
2016-09-07T16:33:49.181070: step 164, loss 0.410338, acc 0.76
2016-09-07T16:33:49.857906: step 165, loss 0.464635, acc 0.76
2016-09-07T16:33:50.540718: step 166, loss 0.47522, acc 0.74
2016-09-07T16:33:51.214195: step 167, loss 0.527376, acc 0.76
2016-09-07T16:33:51.865892: step 168, loss 0.563406, acc 0.68
2016-09-07T16:33:52.523655: step 169, loss 0.517046, acc 0.72
2016-09-07T16:33:53.197266: step 170, loss 0.562372, acc 0.7
2016-09-07T16:33:53.866482: step 171, loss 0.61373, acc 0.58
2016-09-07T16:33:54.529342: step 172, loss 0.49781, acc 0.78
2016-09-07T16:33:55.210488: step 173, loss 0.519249, acc 0.7
2016-09-07T16:33:55.871779: step 174, loss 0.493885, acc 0.7
2016-09-07T16:33:56.546608: step 175, loss 0.444703, acc 0.82
2016-09-07T16:33:57.214716: step 176, loss 0.44039, acc 0.76
2016-09-07T16:33:57.881542: step 177, loss 0.363972, acc 0.86
2016-09-07T16:33:58.618474: step 178, loss 0.481637, acc 0.76
2016-09-07T16:33:59.280702: step 179, loss 0.416509, acc 0.8
2016-09-07T16:33:59.963338: step 180, loss 0.291554, acc 0.84
2016-09-07T16:34:00.651584: step 181, loss 0.534239, acc 0.68
2016-09-07T16:34:01.383514: step 182, loss 0.496796, acc 0.7
2016-09-07T16:34:02.090409: step 183, loss 0.438169, acc 0.78
2016-09-07T16:34:02.810619: step 184, loss 0.58462, acc 0.64
2016-09-07T16:34:03.484095: step 185, loss 0.625577, acc 0.74
2016-09-07T16:34:04.212572: step 186, loss 0.452082, acc 0.78
2016-09-07T16:34:04.930585: step 187, loss 0.470295, acc 0.74
2016-09-07T16:34:05.638494: step 188, loss 0.454461, acc 0.74
2016-09-07T16:34:06.568822: step 189, loss 0.420544, acc 0.82
2016-09-07T16:34:07.424865: step 190, loss 0.482665, acc 0.82
2016-09-07T16:34:08.160623: step 191, loss 0.539688, acc 0.7
2016-09-07T16:34:09.055925: step 192, loss 0.485354, acc 0.66
2016-09-07T16:34:09.934600: step 193, loss 0.437825, acc 0.78
2016-09-07T16:34:10.307668: step 194, loss 0.260855, acc 0.916667
2016-09-07T16:34:11.140477: step 195, loss 0.405703, acc 0.82
2016-09-07T16:34:11.813768: step 196, loss 0.369963, acc 0.82
2016-09-07T16:34:12.538935: step 197, loss 0.309824, acc 0.86
2016-09-07T16:34:13.380111: step 198, loss 0.333688, acc 0.9
2016-09-07T16:34:14.044704: step 199, loss 0.327938, acc 0.84
2016-09-07T16:34:14.768722: step 200, loss 0.424155, acc 0.8

Evaluation:
2016-09-07T16:34:17.713436: step 200, loss 0.467334, acc 0.778

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-200

2016-09-07T16:34:19.359009: step 201, loss 0.416624, acc 0.76
2016-09-07T16:34:20.087976: step 202, loss 0.335957, acc 0.82
2016-09-07T16:34:20.739028: step 203, loss 0.291217, acc 0.84
2016-09-07T16:34:21.501072: step 204, loss 0.528087, acc 0.8
2016-09-07T16:34:22.294533: step 205, loss 0.330534, acc 0.88
2016-09-07T16:34:23.033612: step 206, loss 0.264514, acc 0.9
2016-09-07T16:34:23.775492: step 207, loss 0.620219, acc 0.7
2016-09-07T16:34:24.448895: step 208, loss 0.393683, acc 0.84
2016-09-07T16:34:25.121470: step 209, loss 0.277995, acc 0.94
2016-09-07T16:34:25.796613: step 210, loss 0.32381, acc 0.88
2016-09-07T16:34:26.457304: step 211, loss 0.234816, acc 0.94
2016-09-07T16:34:27.155789: step 212, loss 0.567655, acc 0.8
2016-09-07T16:34:27.812591: step 213, loss 0.247757, acc 0.9
2016-09-07T16:34:28.485080: step 214, loss 0.536215, acc 0.82
2016-09-07T16:34:29.170432: step 215, loss 0.222641, acc 0.94
2016-09-07T16:34:29.815393: step 216, loss 0.508633, acc 0.76
2016-09-07T16:34:30.475076: step 217, loss 0.354384, acc 0.84
2016-09-07T16:34:31.147058: step 218, loss 0.413233, acc 0.86
2016-09-07T16:34:31.806980: step 219, loss 0.317204, acc 0.88
2016-09-07T16:34:32.482234: step 220, loss 0.286305, acc 0.88
2016-09-07T16:34:33.153641: step 221, loss 0.246732, acc 0.9
2016-09-07T16:34:33.828459: step 222, loss 0.319376, acc 0.8
2016-09-07T16:34:34.495545: step 223, loss 0.415056, acc 0.82
2016-09-07T16:34:35.146330: step 224, loss 0.405029, acc 0.86
2016-09-07T16:34:35.812457: step 225, loss 0.473257, acc 0.8
2016-09-07T16:34:36.475325: step 226, loss 0.408278, acc 0.82
2016-09-07T16:34:37.141971: step 227, loss 0.222903, acc 0.94
2016-09-07T16:34:37.812303: step 228, loss 0.347915, acc 0.86
2016-09-07T16:34:38.478420: step 229, loss 0.258228, acc 0.88
2016-09-07T16:34:39.142415: step 230, loss 0.425935, acc 0.8
2016-09-07T16:34:39.812374: step 231, loss 0.339192, acc 0.88
2016-09-07T16:34:40.483439: step 232, loss 0.326624, acc 0.88
2016-09-07T16:34:41.141361: step 233, loss 0.440516, acc 0.78
2016-09-07T16:34:41.806471: step 234, loss 0.475526, acc 0.8
2016-09-07T16:34:42.476045: step 235, loss 0.260092, acc 0.94
2016-09-07T16:34:43.150258: step 236, loss 0.258714, acc 0.9
2016-09-07T16:34:43.814058: step 237, loss 0.432379, acc 0.8
2016-09-07T16:34:44.465651: step 238, loss 0.401042, acc 0.82
2016-09-07T16:34:45.129896: step 239, loss 0.530806, acc 0.78
2016-09-07T16:34:45.779153: step 240, loss 0.370433, acc 0.84
2016-09-07T16:34:46.444297: step 241, loss 0.267724, acc 0.92
2016-09-07T16:34:47.113453: step 242, loss 0.361074, acc 0.82
2016-09-07T16:34:47.787706: step 243, loss 0.464206, acc 0.84
2016-09-07T16:34:48.468899: step 244, loss 0.282553, acc 0.88
2016-09-07T16:34:49.222653: step 245, loss 0.23918, acc 0.88
2016-09-07T16:34:49.924945: step 246, loss 0.300726, acc 0.84
2016-09-07T16:34:50.643591: step 247, loss 0.303399, acc 0.9
2016-09-07T16:34:51.464963: step 248, loss 0.32501, acc 0.8
2016-09-07T16:34:52.188150: step 249, loss 0.3942, acc 0.76
2016-09-07T16:34:52.910940: step 250, loss 0.370828, acc 0.8
2016-09-07T16:34:53.621215: step 251, loss 0.567432, acc 0.76
2016-09-07T16:34:54.330149: step 252, loss 0.320518, acc 0.86
2016-09-07T16:34:54.989580: step 253, loss 0.39152, acc 0.82
2016-09-07T16:34:55.688743: step 254, loss 0.619849, acc 0.78
2016-09-07T16:34:56.394455: step 255, loss 0.407331, acc 0.8
2016-09-07T16:34:57.082857: step 256, loss 0.293799, acc 0.88
2016-09-07T16:34:57.813249: step 257, loss 0.322249, acc 0.9
2016-09-07T16:34:58.482953: step 258, loss 0.493697, acc 0.66
2016-09-07T16:34:59.145147: step 259, loss 0.466371, acc 0.72
2016-09-07T16:34:59.818186: step 260, loss 0.343381, acc 0.88
2016-09-07T16:35:00.498445: step 261, loss 0.292604, acc 0.88
2016-09-07T16:35:01.187517: step 262, loss 0.330265, acc 0.84
2016-09-07T16:35:01.859128: step 263, loss 0.437216, acc 0.8
2016-09-07T16:35:02.511667: step 264, loss 0.325085, acc 0.86
2016-09-07T16:35:03.188961: step 265, loss 0.358204, acc 0.84
2016-09-07T16:35:03.852150: step 266, loss 0.30366, acc 0.9
2016-09-07T16:35:04.575970: step 267, loss 0.204557, acc 0.92
2016-09-07T16:35:05.244253: step 268, loss 0.366516, acc 0.84
2016-09-07T16:35:05.914142: step 269, loss 0.237917, acc 0.92
2016-09-07T16:35:06.576719: step 270, loss 0.285764, acc 0.88
2016-09-07T16:35:07.231384: step 271, loss 0.369841, acc 0.8
2016-09-07T16:35:07.906543: step 272, loss 0.408537, acc 0.82
2016-09-07T16:35:08.586433: step 273, loss 0.336858, acc 0.86
2016-09-07T16:35:09.260011: step 274, loss 0.407634, acc 0.82
2016-09-07T16:35:09.925550: step 275, loss 0.332984, acc 0.8
2016-09-07T16:35:10.588873: step 276, loss 0.45155, acc 0.78
2016-09-07T16:35:11.258407: step 277, loss 0.376019, acc 0.8
2016-09-07T16:35:11.914520: step 278, loss 0.334342, acc 0.84
2016-09-07T16:35:12.574652: step 279, loss 0.439575, acc 0.82
2016-09-07T16:35:13.224007: step 280, loss 0.481241, acc 0.76
2016-09-07T16:35:13.891088: step 281, loss 0.418849, acc 0.78
2016-09-07T16:35:14.570408: step 282, loss 0.280857, acc 0.92
2016-09-07T16:35:15.227175: step 283, loss 0.418652, acc 0.82
2016-09-07T16:35:15.896043: step 284, loss 0.435956, acc 0.8
2016-09-07T16:35:16.554707: step 285, loss 0.357963, acc 0.84
2016-09-07T16:35:17.235204: step 286, loss 0.436081, acc 0.78
2016-09-07T16:35:17.894417: step 287, loss 0.330035, acc 0.86
2016-09-07T16:35:18.554537: step 288, loss 0.388758, acc 0.78
2016-09-07T16:35:19.212400: step 289, loss 0.383589, acc 0.8
2016-09-07T16:35:19.887071: step 290, loss 0.272505, acc 0.88
2016-09-07T16:35:20.561070: step 291, loss 0.259502, acc 0.9
2016-09-07T16:35:21.213346: step 292, loss 0.23778, acc 0.94
2016-09-07T16:35:21.885936: step 293, loss 0.522625, acc 0.72
2016-09-07T16:35:22.547281: step 294, loss 0.276592, acc 0.86
2016-09-07T16:35:23.196150: step 295, loss 0.357606, acc 0.86
2016-09-07T16:35:23.865687: step 296, loss 0.278538, acc 0.88
2016-09-07T16:35:24.565636: step 297, loss 0.279434, acc 0.86
2016-09-07T16:35:25.251809: step 298, loss 0.285036, acc 0.86
2016-09-07T16:35:25.912615: step 299, loss 0.538994, acc 0.72
2016-09-07T16:35:26.583686: step 300, loss 0.242533, acc 0.9

Evaluation:
2016-09-07T16:35:29.489683: step 300, loss 0.464766, acc 0.792

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-300

2016-09-07T16:35:31.224986: step 301, loss 0.361002, acc 0.82
2016-09-07T16:35:31.885084: step 302, loss 0.254065, acc 0.92
2016-09-07T16:35:32.553143: step 303, loss 0.460743, acc 0.86
2016-09-07T16:35:33.312372: step 304, loss 0.287836, acc 0.88
2016-09-07T16:35:33.991894: step 305, loss 0.446285, acc 0.82
2016-09-07T16:35:34.670546: step 306, loss 0.319838, acc 0.86
2016-09-07T16:35:35.342878: step 307, loss 0.340123, acc 0.86
2016-09-07T16:35:36.020783: step 308, loss 0.419224, acc 0.8
2016-09-07T16:35:36.686272: step 309, loss 0.436942, acc 0.86
2016-09-07T16:35:37.373856: step 310, loss 0.322279, acc 0.84
2016-09-07T16:35:38.042905: step 311, loss 0.289044, acc 0.86
2016-09-07T16:35:38.708688: step 312, loss 0.335535, acc 0.84
2016-09-07T16:35:39.394172: step 313, loss 0.376196, acc 0.86
2016-09-07T16:35:40.070091: step 314, loss 0.461844, acc 0.8
2016-09-07T16:35:40.768710: step 315, loss 0.554004, acc 0.68
2016-09-07T16:35:41.451835: step 316, loss 0.428535, acc 0.78
2016-09-07T16:35:42.118782: step 317, loss 0.407964, acc 0.82
2016-09-07T16:35:42.797562: step 318, loss 0.293965, acc 0.9
2016-09-07T16:35:43.471547: step 319, loss 0.275174, acc 0.9
2016-09-07T16:35:44.154853: step 320, loss 0.456211, acc 0.74
2016-09-07T16:35:44.832296: step 321, loss 0.296614, acc 0.88
2016-09-07T16:35:45.521795: step 322, loss 0.388713, acc 0.78
2016-09-07T16:35:46.201888: step 323, loss 0.37114, acc 0.78
2016-09-07T16:35:46.941319: step 324, loss 0.296959, acc 0.86
2016-09-07T16:35:47.633496: step 325, loss 0.526116, acc 0.74
2016-09-07T16:35:48.342937: step 326, loss 0.462597, acc 0.76
2016-09-07T16:35:49.094188: step 327, loss 0.375955, acc 0.8
2016-09-07T16:35:49.798544: step 328, loss 0.428449, acc 0.82
2016-09-07T16:35:50.469715: step 329, loss 0.40869, acc 0.8
2016-09-07T16:35:51.183171: step 330, loss 0.411433, acc 0.84
2016-09-07T16:35:51.901255: step 331, loss 0.23558, acc 0.92
2016-09-07T16:35:52.651276: step 332, loss 0.257268, acc 0.92
2016-09-07T16:35:53.371420: step 333, loss 0.28855, acc 0.88
2016-09-07T16:35:54.038329: step 334, loss 0.30533, acc 0.88
2016-09-07T16:35:54.722065: step 335, loss 0.297387, acc 0.9
2016-09-07T16:35:55.456215: step 336, loss 0.37987, acc 0.88
2016-09-07T16:35:56.154723: step 337, loss 0.397584, acc 0.86
2016-09-07T16:35:56.838668: step 338, loss 0.445616, acc 0.74
2016-09-07T16:35:57.586633: step 339, loss 0.311416, acc 0.86
2016-09-07T16:35:58.339395: step 340, loss 0.326302, acc 0.88
2016-09-07T16:35:59.073640: step 341, loss 0.262397, acc 0.9
2016-09-07T16:35:59.840166: step 342, loss 0.284949, acc 0.86
2016-09-07T16:36:00.556032: step 343, loss 0.297973, acc 0.88
2016-09-07T16:36:01.355116: step 344, loss 0.373615, acc 0.86
2016-09-07T16:36:02.108402: step 345, loss 0.439107, acc 0.84
2016-09-07T16:36:02.803693: step 346, loss 0.528119, acc 0.74
2016-09-07T16:36:03.484531: step 347, loss 0.268383, acc 0.9
2016-09-07T16:36:04.143753: step 348, loss 0.233961, acc 0.9
2016-09-07T16:36:04.856642: step 349, loss 0.411579, acc 0.82
2016-09-07T16:36:05.550182: step 350, loss 0.287262, acc 0.9
2016-09-07T16:36:06.219667: step 351, loss 0.550695, acc 0.7
2016-09-07T16:36:06.890109: step 352, loss 0.429982, acc 0.8
2016-09-07T16:36:07.544999: step 353, loss 0.36259, acc 0.82
2016-09-07T16:36:08.202173: step 354, loss 0.439846, acc 0.78
2016-09-07T16:36:08.865945: step 355, loss 0.33459, acc 0.88
2016-09-07T16:36:09.528076: step 356, loss 0.354837, acc 0.84
2016-09-07T16:36:10.199306: step 357, loss 0.482805, acc 0.84
2016-09-07T16:36:10.868635: step 358, loss 0.3075, acc 0.78
2016-09-07T16:36:11.547855: step 359, loss 0.453706, acc 0.78
2016-09-07T16:36:12.214853: step 360, loss 0.485368, acc 0.78
2016-09-07T16:36:12.870028: step 361, loss 0.405969, acc 0.82
2016-09-07T16:36:13.531152: step 362, loss 0.406097, acc 0.78
2016-09-07T16:36:14.203733: step 363, loss 0.290617, acc 0.92
2016-09-07T16:36:14.871163: step 364, loss 0.328707, acc 0.84
2016-09-07T16:36:15.545373: step 365, loss 0.450388, acc 0.74
2016-09-07T16:36:16.200882: step 366, loss 0.338309, acc 0.86
2016-09-07T16:36:16.876082: step 367, loss 0.347981, acc 0.86
2016-09-07T16:36:17.547757: step 368, loss 0.380651, acc 0.9
2016-09-07T16:36:18.235987: step 369, loss 0.433772, acc 0.8
2016-09-07T16:36:18.949830: step 370, loss 0.41706, acc 0.78
2016-09-07T16:36:19.605035: step 371, loss 0.381001, acc 0.8
2016-09-07T16:36:20.296314: step 372, loss 0.317538, acc 0.86
2016-09-07T16:36:21.009596: step 373, loss 0.533519, acc 0.74
2016-09-07T16:36:21.679037: step 374, loss 0.412268, acc 0.88
2016-09-07T16:36:22.431521: step 375, loss 0.339357, acc 0.86
2016-09-07T16:36:23.099631: step 376, loss 0.356046, acc 0.88
2016-09-07T16:36:23.792935: step 377, loss 0.563919, acc 0.7
2016-09-07T16:36:24.481362: step 378, loss 0.323647, acc 0.9
2016-09-07T16:36:25.166464: step 379, loss 0.359156, acc 0.82
2016-09-07T16:36:25.882189: step 380, loss 0.47444, acc 0.82
2016-09-07T16:36:26.565552: step 381, loss 0.400139, acc 0.82
2016-09-07T16:36:27.238343: step 382, loss 0.409402, acc 0.82
2016-09-07T16:36:27.931634: step 383, loss 0.397747, acc 0.84
2016-09-07T16:36:28.606665: step 384, loss 0.404861, acc 0.76
2016-09-07T16:36:29.276554: step 385, loss 0.361722, acc 0.9
2016-09-07T16:36:29.961687: step 386, loss 0.391268, acc 0.8
2016-09-07T16:36:30.644174: step 387, loss 0.468209, acc 0.82
2016-09-07T16:36:31.022164: step 388, loss 0.506459, acc 0.833333
2016-09-07T16:36:31.726230: step 389, loss 0.342517, acc 0.84
2016-09-07T16:36:32.396530: step 390, loss 0.25673, acc 0.86
2016-09-07T16:36:33.069041: step 391, loss 0.271928, acc 0.9
2016-09-07T16:36:33.771371: step 392, loss 0.211734, acc 0.92
2016-09-07T16:36:34.470081: step 393, loss 0.226505, acc 0.88
2016-09-07T16:36:35.128114: step 394, loss 0.279622, acc 0.9
2016-09-07T16:36:35.794362: step 395, loss 0.24979, acc 0.9
2016-09-07T16:36:36.460905: step 396, loss 0.206772, acc 0.9
2016-09-07T16:36:37.151602: step 397, loss 0.20173, acc 0.92
2016-09-07T16:36:37.834607: step 398, loss 0.212361, acc 0.94
2016-09-07T16:36:38.511838: step 399, loss 0.228711, acc 0.92
2016-09-07T16:36:39.184970: step 400, loss 0.134783, acc 0.94

Evaluation:
2016-09-07T16:36:42.070267: step 400, loss 0.619125, acc 0.8

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-400

2016-09-07T16:36:43.728015: step 401, loss 0.2608, acc 0.86
2016-09-07T16:36:44.412860: step 402, loss 0.531646, acc 0.88
2016-09-07T16:36:45.096902: step 403, loss 0.268215, acc 0.94
2016-09-07T16:36:45.768099: step 404, loss 0.174784, acc 0.9
2016-09-07T16:36:46.442988: step 405, loss 0.27649, acc 0.9
2016-09-07T16:36:47.103805: step 406, loss 0.327334, acc 0.86
2016-09-07T16:36:47.778491: step 407, loss 0.0989141, acc 0.98
2016-09-07T16:36:48.452788: step 408, loss 0.233643, acc 0.92
2016-09-07T16:36:49.120818: step 409, loss 0.447005, acc 0.86
2016-09-07T16:36:49.777244: step 410, loss 0.176663, acc 0.92
2016-09-07T16:36:50.462360: step 411, loss 0.255195, acc 0.92
2016-09-07T16:36:51.143828: step 412, loss 0.135728, acc 0.92
2016-09-07T16:36:51.854318: step 413, loss 0.264267, acc 0.88
2016-09-07T16:36:52.585747: step 414, loss 0.334098, acc 0.84
2016-09-07T16:36:53.331504: step 415, loss 0.303502, acc 0.9
2016-09-07T16:36:54.016028: step 416, loss 0.174304, acc 0.96
2016-09-07T16:36:54.738019: step 417, loss 0.208695, acc 0.92
2016-09-07T16:36:55.538932: step 418, loss 0.288634, acc 0.86
2016-09-07T16:36:56.284917: step 419, loss 0.0951891, acc 0.96
2016-09-07T16:36:56.953503: step 420, loss 0.200976, acc 0.92
2016-09-07T16:36:57.615110: step 421, loss 0.437858, acc 0.88
2016-09-07T16:36:58.272828: step 422, loss 0.223013, acc 0.9
2016-09-07T16:36:58.941659: step 423, loss 0.253856, acc 0.9
2016-09-07T16:36:59.610167: step 424, loss 0.404984, acc 0.84
2016-09-07T16:37:00.289618: step 425, loss 0.150105, acc 0.98
2016-09-07T16:37:00.956476: step 426, loss 0.166681, acc 0.96
2016-09-07T16:37:01.607015: step 427, loss 0.22596, acc 0.92
2016-09-07T16:37:02.295992: step 428, loss 0.314222, acc 0.86
2016-09-07T16:37:02.969778: step 429, loss 0.175709, acc 0.92
2016-09-07T16:37:03.650792: step 430, loss 0.181618, acc 0.92
2016-09-07T16:37:04.390811: step 431, loss 0.301669, acc 0.88
2016-09-07T16:37:05.129608: step 432, loss 0.243419, acc 0.94
2016-09-07T16:37:05.829477: step 433, loss 0.170184, acc 0.94
2016-09-07T16:37:06.561828: step 434, loss 0.34375, acc 0.88
2016-09-07T16:37:07.264654: step 435, loss 0.242964, acc 0.9
2016-09-07T16:37:08.223828: step 436, loss 0.363424, acc 0.9
2016-09-07T16:37:09.118465: step 437, loss 0.141446, acc 0.94
2016-09-07T16:37:09.893573: step 438, loss 0.192605, acc 0.88
2016-09-07T16:37:10.661297: step 439, loss 0.113173, acc 0.96
2016-09-07T16:37:11.418117: step 440, loss 0.236747, acc 0.94
2016-09-07T16:37:12.181569: step 441, loss 0.226867, acc 0.92
2016-09-07T16:37:13.011971: step 442, loss 0.123822, acc 0.94
2016-09-07T16:37:13.876241: step 443, loss 0.215626, acc 0.88
2016-09-07T16:37:14.633294: step 444, loss 0.341809, acc 0.8
2016-09-07T16:37:15.460131: step 445, loss 0.209186, acc 0.94
2016-09-07T16:37:16.276986: step 446, loss 0.248779, acc 0.88
2016-09-07T16:37:17.068278: step 447, loss 0.211898, acc 0.88
2016-09-07T16:37:17.830421: step 448, loss 0.127078, acc 0.96
2016-09-07T16:37:18.593852: step 449, loss 0.116631, acc 0.96
2016-09-07T16:37:19.438024: step 450, loss 0.33521, acc 0.84
2016-09-07T16:37:20.346398: step 451, loss 0.192039, acc 0.94
2016-09-07T16:37:21.244150: step 452, loss 0.270019, acc 0.9
2016-09-07T16:37:22.147912: step 453, loss 0.22476, acc 0.92
2016-09-07T16:37:23.033074: step 454, loss 0.241922, acc 0.88
2016-09-07T16:37:23.832565: step 455, loss 0.327116, acc 0.84
2016-09-07T16:37:24.495207: step 456, loss 0.194062, acc 0.92
2016-09-07T16:37:25.155849: step 457, loss 0.216796, acc 0.86
2016-09-07T16:37:25.836578: step 458, loss 0.287213, acc 0.9
2016-09-07T16:37:26.500127: step 459, loss 0.291122, acc 0.82
2016-09-07T16:37:27.183816: step 460, loss 0.167174, acc 0.92
2016-09-07T16:37:27.871133: step 461, loss 0.187661, acc 0.92
2016-09-07T16:37:28.599571: step 462, loss 0.270441, acc 0.86
2016-09-07T16:37:29.325070: step 463, loss 0.250175, acc 0.92
2016-09-07T16:37:30.033354: step 464, loss 0.318342, acc 0.84
2016-09-07T16:37:30.833978: step 465, loss 0.252714, acc 0.86
2016-09-07T16:37:31.601189: step 466, loss 0.164455, acc 0.94
2016-09-07T16:37:32.274522: step 467, loss 0.230724, acc 0.9
2016-09-07T16:37:32.974422: step 468, loss 0.140672, acc 0.94
2016-09-07T16:37:33.645944: step 469, loss 0.202453, acc 0.9
2016-09-07T16:37:34.323158: step 470, loss 0.287002, acc 0.86
2016-09-07T16:37:35.026567: step 471, loss 0.27468, acc 0.86
2016-09-07T16:37:35.689262: step 472, loss 0.346422, acc 0.92
2016-09-07T16:37:36.384846: step 473, loss 0.220029, acc 0.92
2016-09-07T16:37:37.141528: step 474, loss 0.34155, acc 0.78
2016-09-07T16:37:37.833605: step 475, loss 0.145395, acc 0.96
2016-09-07T16:37:38.500674: step 476, loss 0.216099, acc 0.88
2016-09-07T16:37:39.171457: step 477, loss 0.287141, acc 0.86
2016-09-07T16:37:39.828246: step 478, loss 0.292771, acc 0.88
2016-09-07T16:37:40.485216: step 479, loss 0.30587, acc 0.82
2016-09-07T16:37:41.154580: step 480, loss 0.221918, acc 0.9
2016-09-07T16:37:41.823891: step 481, loss 0.157777, acc 0.92
2016-09-07T16:37:42.502890: step 482, loss 0.17963, acc 0.9
2016-09-07T16:37:43.177723: step 483, loss 0.255791, acc 0.84
2016-09-07T16:37:43.847710: step 484, loss 0.149375, acc 0.94
2016-09-07T16:37:44.533831: step 485, loss 0.357825, acc 0.8
2016-09-07T16:37:45.223924: step 486, loss 0.251849, acc 0.9
2016-09-07T16:37:45.905409: step 487, loss 0.206512, acc 0.96
2016-09-07T16:37:46.576546: step 488, loss 0.289637, acc 0.88
2016-09-07T16:37:47.330184: step 489, loss 0.313592, acc 0.88
2016-09-07T16:37:48.087508: step 490, loss 0.209994, acc 0.9
2016-09-07T16:37:48.762186: step 491, loss 0.181508, acc 0.92
2016-09-07T16:37:49.446182: step 492, loss 0.175387, acc 0.94
2016-09-07T16:37:50.275773: step 493, loss 0.140164, acc 0.92
2016-09-07T16:37:50.948298: step 494, loss 0.289792, acc 0.88
2016-09-07T16:37:51.609746: step 495, loss 0.330977, acc 0.86
2016-09-07T16:37:52.393157: step 496, loss 0.250792, acc 0.88
2016-09-07T16:37:53.216087: step 497, loss 0.21856, acc 0.9
2016-09-07T16:37:54.001604: step 498, loss 0.384123, acc 0.86
2016-09-07T16:37:54.718961: step 499, loss 0.275957, acc 0.88
2016-09-07T16:37:55.376484: step 500, loss 0.265681, acc 0.88

Evaluation:
2016-09-07T16:37:58.420945: step 500, loss 0.543298, acc 0.786

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-500

2016-09-07T16:38:00.075940: step 501, loss 0.358385, acc 0.9
2016-09-07T16:38:00.782793: step 502, loss 0.281334, acc 0.86
2016-09-07T16:38:01.450989: step 503, loss 0.221009, acc 0.88
2016-09-07T16:38:02.126684: step 504, loss 0.241259, acc 0.88
2016-09-07T16:38:02.800541: step 505, loss 0.22481, acc 0.88
2016-09-07T16:38:03.477885: step 506, loss 0.222037, acc 0.86
2016-09-07T16:38:04.135251: step 507, loss 0.185387, acc 0.96
2016-09-07T16:38:04.812280: step 508, loss 0.177207, acc 0.92
2016-09-07T16:38:05.507825: step 509, loss 0.206438, acc 0.9
2016-09-07T16:38:06.250596: step 510, loss 0.427346, acc 0.78
2016-09-07T16:38:06.953505: step 511, loss 0.172366, acc 0.92
2016-09-07T16:38:07.642526: step 512, loss 0.218651, acc 0.94
2016-09-07T16:38:08.329320: step 513, loss 0.261867, acc 0.94
2016-09-07T16:38:09.050585: step 514, loss 0.363746, acc 0.82
2016-09-07T16:38:09.749377: step 515, loss 0.16972, acc 0.94
2016-09-07T16:38:10.446908: step 516, loss 0.25618, acc 0.86
2016-09-07T16:38:11.110339: step 517, loss 0.214066, acc 0.94
2016-09-07T16:38:11.944907: step 518, loss 0.357959, acc 0.88
2016-09-07T16:38:12.749208: step 519, loss 0.353519, acc 0.84
2016-09-07T16:38:13.450318: step 520, loss 0.298882, acc 0.86
2016-09-07T16:38:14.233358: step 521, loss 0.215045, acc 0.9
2016-09-07T16:38:14.908960: step 522, loss 0.222667, acc 0.92
2016-09-07T16:38:15.583654: step 523, loss 0.313753, acc 0.88
2016-09-07T16:38:16.365374: step 524, loss 0.302589, acc 0.86
2016-09-07T16:38:17.069647: step 525, loss 0.300065, acc 0.9
2016-09-07T16:38:17.758143: step 526, loss 0.293555, acc 0.86
2016-09-07T16:38:18.554042: step 527, loss 0.215443, acc 0.92
2016-09-07T16:38:19.296066: step 528, loss 0.222111, acc 0.94
2016-09-07T16:38:19.967138: step 529, loss 0.297862, acc 0.88
2016-09-07T16:38:20.649860: step 530, loss 0.177277, acc 0.92
2016-09-07T16:38:21.328288: step 531, loss 0.255289, acc 0.88
2016-09-07T16:38:22.011387: step 532, loss 0.219179, acc 0.92
2016-09-07T16:38:22.668465: step 533, loss 0.289635, acc 0.86
2016-09-07T16:38:23.338488: step 534, loss 0.337151, acc 0.84
2016-09-07T16:38:24.063161: step 535, loss 0.206029, acc 0.92
2016-09-07T16:38:24.861870: step 536, loss 0.219653, acc 0.86
2016-09-07T16:38:25.618880: step 537, loss 0.186987, acc 0.94
2016-09-07T16:38:26.309778: step 538, loss 0.311256, acc 0.84
2016-09-07T16:38:26.966944: step 539, loss 0.224242, acc 0.92
2016-09-07T16:38:27.633185: step 540, loss 0.252912, acc 0.9
2016-09-07T16:38:28.318788: step 541, loss 0.379969, acc 0.88
2016-09-07T16:38:29.022133: step 542, loss 0.319974, acc 0.8
2016-09-07T16:38:29.698181: step 543, loss 0.281182, acc 0.86
2016-09-07T16:38:30.374906: step 544, loss 0.188686, acc 0.94
2016-09-07T16:38:31.051796: step 545, loss 0.217925, acc 0.92
2016-09-07T16:38:31.715541: step 546, loss 0.228068, acc 0.92
2016-09-07T16:38:32.387508: step 547, loss 0.258902, acc 0.88
2016-09-07T16:38:33.048322: step 548, loss 0.369896, acc 0.86
2016-09-07T16:38:33.708911: step 549, loss 0.243242, acc 0.86
2016-09-07T16:38:34.375254: step 550, loss 0.196312, acc 0.94
2016-09-07T16:38:35.054479: step 551, loss 0.203933, acc 0.9
2016-09-07T16:38:35.737104: step 552, loss 0.351644, acc 0.84
2016-09-07T16:38:36.384670: step 553, loss 0.248057, acc 0.92
2016-09-07T16:38:37.052560: step 554, loss 0.123482, acc 0.94
2016-09-07T16:38:37.722388: step 555, loss 0.210122, acc 0.9
2016-09-07T16:38:38.399926: step 556, loss 0.205046, acc 0.92
2016-09-07T16:38:39.060873: step 557, loss 0.306558, acc 0.84
2016-09-07T16:38:39.739793: step 558, loss 0.340589, acc 0.82
2016-09-07T16:38:40.398690: step 559, loss 0.276565, acc 0.84
2016-09-07T16:38:41.070993: step 560, loss 0.253912, acc 0.9
2016-09-07T16:38:41.749792: step 561, loss 0.271227, acc 0.86
2016-09-07T16:38:42.410956: step 562, loss 0.116044, acc 0.98
2016-09-07T16:38:43.085447: step 563, loss 0.36595, acc 0.88
2016-09-07T16:38:43.750275: step 564, loss 0.372299, acc 0.8
2016-09-07T16:38:44.412122: step 565, loss 0.117636, acc 0.94
2016-09-07T16:38:45.085718: step 566, loss 0.467638, acc 0.8
2016-09-07T16:38:45.733449: step 567, loss 0.308108, acc 0.86
2016-09-07T16:38:46.419891: step 568, loss 0.184631, acc 0.92
2016-09-07T16:38:47.081449: step 569, loss 0.31863, acc 0.84
2016-09-07T16:38:47.753097: step 570, loss 0.195797, acc 0.94
2016-09-07T16:38:48.429956: step 571, loss 0.193086, acc 0.92
2016-09-07T16:38:49.104871: step 572, loss 0.327676, acc 0.84
2016-09-07T16:38:49.784624: step 573, loss 0.277805, acc 0.86
2016-09-07T16:38:50.452306: step 574, loss 0.215692, acc 0.88
2016-09-07T16:38:51.122284: step 575, loss 0.269391, acc 0.88
2016-09-07T16:38:51.783373: step 576, loss 0.200571, acc 0.92
2016-09-07T16:38:52.453779: step 577, loss 0.25477, acc 0.88
2016-09-07T16:38:53.135380: step 578, loss 0.310269, acc 0.84
2016-09-07T16:38:53.826657: step 579, loss 0.299601, acc 0.86
2016-09-07T16:38:54.513642: step 580, loss 0.20358, acc 0.9
2016-09-07T16:38:55.181808: step 581, loss 0.246654, acc 0.9
2016-09-07T16:38:55.579763: step 582, loss 0.296147, acc 0.833333
2016-09-07T16:38:56.250753: step 583, loss 0.0822841, acc 0.98
2016-09-07T16:38:56.937289: step 584, loss 0.198676, acc 0.88
2016-09-07T16:38:57.604633: step 585, loss 0.312077, acc 0.88
2016-09-07T16:38:58.300009: step 586, loss 0.149611, acc 0.96
2016-09-07T16:38:58.975532: step 587, loss 0.131365, acc 0.94
2016-09-07T16:38:59.671415: step 588, loss 0.203515, acc 0.88
2016-09-07T16:39:00.371319: step 589, loss 0.120474, acc 0.94
2016-09-07T16:39:01.064732: step 590, loss 0.189564, acc 0.96
2016-09-07T16:39:01.740135: step 591, loss 0.0976142, acc 0.96
2016-09-07T16:39:02.405560: step 592, loss 0.262955, acc 0.92
2016-09-07T16:39:03.078938: step 593, loss 0.113338, acc 0.96
2016-09-07T16:39:03.769914: step 594, loss 0.0390939, acc 1
2016-09-07T16:39:04.435146: step 595, loss 0.310421, acc 0.88
2016-09-07T16:39:05.104821: step 596, loss 0.0671532, acc 1
2016-09-07T16:39:05.771134: step 597, loss 0.118023, acc 0.94
2016-09-07T16:39:06.436614: step 598, loss 0.30722, acc 0.9
2016-09-07T16:39:07.099367: step 599, loss 0.172735, acc 0.88
2016-09-07T16:39:07.765815: step 600, loss 0.0458511, acc 0.98

Evaluation:
2016-09-07T16:39:10.715611: step 600, loss 0.623868, acc 0.784

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-600

2016-09-07T16:39:12.435304: step 601, loss 0.0804696, acc 0.98
2016-09-07T16:39:13.158897: step 602, loss 0.134526, acc 0.94
2016-09-07T16:39:13.890221: step 603, loss 0.108894, acc 0.94
2016-09-07T16:39:14.617840: step 604, loss 0.253594, acc 0.9
2016-09-07T16:39:15.363207: step 605, loss 0.225035, acc 0.94
2016-09-07T16:39:16.104239: step 606, loss 0.145685, acc 0.94
2016-09-07T16:39:16.940437: step 607, loss 0.132899, acc 0.94
2016-09-07T16:39:17.745971: step 608, loss 0.115589, acc 0.96
2016-09-07T16:39:18.564972: step 609, loss 0.167897, acc 0.88
2016-09-07T16:39:19.388728: step 610, loss 0.046704, acc 0.98
2016-09-07T16:39:20.131045: step 611, loss 0.120759, acc 0.94
2016-09-07T16:39:20.801086: step 612, loss 0.0476485, acc 0.98
2016-09-07T16:39:21.527515: step 613, loss 0.189577, acc 0.9
2016-09-07T16:39:22.310696: step 614, loss 0.167704, acc 0.96
2016-09-07T16:39:23.018200: step 615, loss 0.121119, acc 0.92
2016-09-07T16:39:23.688184: step 616, loss 0.164995, acc 0.96
2016-09-07T16:39:24.503521: step 617, loss 0.0739535, acc 0.98
2016-09-07T16:39:25.175468: step 618, loss 0.243507, acc 0.86
2016-09-07T16:39:25.826603: step 619, loss 0.11507, acc 0.92
2016-09-07T16:39:26.485222: step 620, loss 0.111543, acc 0.94
2016-09-07T16:39:27.214136: step 621, loss 0.0619112, acc 0.96
2016-09-07T16:39:27.901269: step 622, loss 0.057248, acc 0.96
2016-09-07T16:39:28.608276: step 623, loss 0.150888, acc 0.94
2016-09-07T16:39:29.294288: step 624, loss 0.163113, acc 0.9
2016-09-07T16:39:30.026223: step 625, loss 0.0326326, acc 1
2016-09-07T16:39:30.722062: step 626, loss 0.340389, acc 0.86
2016-09-07T16:39:31.576555: step 627, loss 0.29518, acc 0.86
2016-09-07T16:39:32.368130: step 628, loss 0.188134, acc 0.94
2016-09-07T16:39:33.138001: step 629, loss 0.355712, acc 0.86
2016-09-07T16:39:33.800563: step 630, loss 0.129301, acc 0.94
2016-09-07T16:39:34.482391: step 631, loss 0.116186, acc 0.96
2016-09-07T16:39:35.304255: step 632, loss 0.212522, acc 0.9
2016-09-07T16:39:36.075823: step 633, loss 0.133269, acc 0.94
2016-09-07T16:39:36.740557: step 634, loss 0.166362, acc 0.9
2016-09-07T16:39:37.418986: step 635, loss 0.132172, acc 0.94
2016-09-07T16:39:38.112555: step 636, loss 0.391041, acc 0.88
2016-09-07T16:39:38.775536: step 637, loss 0.257896, acc 0.94
2016-09-07T16:39:39.444931: step 638, loss 0.169402, acc 0.92
2016-09-07T16:39:40.096040: step 639, loss 0.196753, acc 0.88
2016-09-07T16:39:40.822970: step 640, loss 0.149293, acc 0.96
2016-09-07T16:39:41.645611: step 641, loss 0.19442, acc 0.94
2016-09-07T16:39:42.470683: step 642, loss 0.152732, acc 0.96
2016-09-07T16:39:43.291905: step 643, loss 0.171504, acc 0.92
2016-09-07T16:39:44.078724: step 644, loss 0.131635, acc 0.98
2016-09-07T16:39:44.871672: step 645, loss 0.160612, acc 0.94
2016-09-07T16:39:45.632811: step 646, loss 0.3038, acc 0.92
2016-09-07T16:39:46.405981: step 647, loss 0.125085, acc 0.92
2016-09-07T16:39:47.228601: step 648, loss 0.205288, acc 0.92
2016-09-07T16:39:48.017928: step 649, loss 0.105797, acc 0.96
2016-09-07T16:39:48.685331: step 650, loss 0.110644, acc 0.96
2016-09-07T16:39:49.348679: step 651, loss 0.149813, acc 0.94
2016-09-07T16:39:50.022999: step 652, loss 0.120632, acc 0.94
2016-09-07T16:39:50.697548: step 653, loss 0.144843, acc 0.96
2016-09-07T16:39:51.357216: step 654, loss 0.198317, acc 0.9
2016-09-07T16:39:52.039592: step 655, loss 0.211046, acc 0.92
2016-09-07T16:39:52.733802: step 656, loss 0.115896, acc 0.96
2016-09-07T16:39:53.558144: step 657, loss 0.184805, acc 0.9
2016-09-07T16:39:54.215420: step 658, loss 0.240598, acc 0.88
2016-09-07T16:39:54.901825: step 659, loss 0.227012, acc 0.92
2016-09-07T16:39:55.653492: step 660, loss 0.100451, acc 0.98
2016-09-07T16:39:56.435735: step 661, loss 0.157524, acc 0.94
2016-09-07T16:39:57.129983: step 662, loss 0.243785, acc 0.9
2016-09-07T16:39:57.811699: step 663, loss 0.134536, acc 0.94
2016-09-07T16:39:58.473097: step 664, loss 0.191618, acc 0.9
2016-09-07T16:39:59.256903: step 665, loss 0.228277, acc 0.94
2016-09-07T16:40:00.010784: step 666, loss 0.135172, acc 0.92
2016-09-07T16:40:00.687650: step 667, loss 0.129133, acc 0.92
2016-09-07T16:40:01.370736: step 668, loss 0.185545, acc 0.9
2016-09-07T16:40:02.063958: step 669, loss 0.216861, acc 0.9
2016-09-07T16:40:02.785761: step 670, loss 0.0688896, acc 0.98
2016-09-07T16:40:03.475018: step 671, loss 0.080766, acc 0.98
2016-09-07T16:40:04.146492: step 672, loss 0.193499, acc 0.9
2016-09-07T16:40:04.821372: step 673, loss 0.156691, acc 0.92
2016-09-07T16:40:05.495934: step 674, loss 0.231319, acc 0.92
2016-09-07T16:40:06.180742: step 675, loss 0.250189, acc 0.9
2016-09-07T16:40:06.845358: step 676, loss 0.103607, acc 0.94
2016-09-07T16:40:07.552843: step 677, loss 0.169135, acc 0.92
2016-09-07T16:40:08.222214: step 678, loss 0.182071, acc 0.9
2016-09-07T16:40:08.911651: step 679, loss 0.0937438, acc 0.96
2016-09-07T16:40:09.575816: step 680, loss 0.171178, acc 0.94
2016-09-07T16:40:10.248815: step 681, loss 0.0957706, acc 0.96
2016-09-07T16:40:10.950467: step 682, loss 0.161705, acc 0.94
2016-09-07T16:40:11.622409: step 683, loss 0.15163, acc 0.9
2016-09-07T16:40:12.492105: step 684, loss 0.282163, acc 0.88
2016-09-07T16:40:13.293266: step 685, loss 0.168429, acc 0.92
2016-09-07T16:40:13.953149: step 686, loss 0.244389, acc 0.9
2016-09-07T16:40:14.629807: step 687, loss 0.14958, acc 0.92
2016-09-07T16:40:15.495243: step 688, loss 0.210434, acc 0.98
2016-09-07T16:40:16.315253: step 689, loss 0.164, acc 0.88
2016-09-07T16:40:17.031040: step 690, loss 0.213053, acc 0.88
2016-09-07T16:40:17.689515: step 691, loss 0.333091, acc 0.84
2016-09-07T16:40:18.376873: step 692, loss 0.135945, acc 0.94
2016-09-07T16:40:19.062632: step 693, loss 0.168975, acc 0.9
2016-09-07T16:40:19.727437: step 694, loss 0.0784692, acc 0.96
2016-09-07T16:40:20.403511: step 695, loss 0.110006, acc 0.94
2016-09-07T16:40:21.078006: step 696, loss 0.248229, acc 0.92
2016-09-07T16:40:21.744438: step 697, loss 0.173561, acc 0.92
2016-09-07T16:40:22.408198: step 698, loss 0.230732, acc 0.88
2016-09-07T16:40:23.083483: step 699, loss 0.285315, acc 0.88
2016-09-07T16:40:23.753239: step 700, loss 0.186898, acc 0.88

Evaluation:
2016-09-07T16:40:26.694783: step 700, loss 0.579436, acc 0.794

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-700

2016-09-07T16:40:28.325693: step 701, loss 0.191679, acc 0.9
2016-09-07T16:40:28.990072: step 702, loss 0.148215, acc 0.94
2016-09-07T16:40:29.679505: step 703, loss 0.0790022, acc 0.96
2016-09-07T16:40:30.369499: step 704, loss 0.177103, acc 0.94
2016-09-07T16:40:31.048881: step 705, loss 0.164322, acc 0.9
2016-09-07T16:40:31.743270: step 706, loss 0.103997, acc 0.96
2016-09-07T16:40:32.423285: step 707, loss 0.200866, acc 0.94
2016-09-07T16:40:33.087061: step 708, loss 0.123103, acc 0.96
2016-09-07T16:40:33.769461: step 709, loss 0.214823, acc 0.9
2016-09-07T16:40:34.450442: step 710, loss 0.0660783, acc 0.98
2016-09-07T16:40:35.141843: step 711, loss 0.0860862, acc 0.96
2016-09-07T16:40:35.807947: step 712, loss 0.0969122, acc 0.96
2016-09-07T16:40:36.478267: step 713, loss 0.118977, acc 0.98
2016-09-07T16:40:37.122972: step 714, loss 0.17906, acc 0.92
2016-09-07T16:40:37.771113: step 715, loss 0.103636, acc 0.98
2016-09-07T16:40:38.437404: step 716, loss 0.199689, acc 0.92
2016-09-07T16:40:39.116273: step 717, loss 0.232339, acc 0.92
2016-09-07T16:40:39.778761: step 718, loss 0.142646, acc 0.96
2016-09-07T16:40:40.440645: step 719, loss 0.135113, acc 0.96
2016-09-07T16:40:41.106357: step 720, loss 0.160409, acc 0.94
2016-09-07T16:40:41.808040: step 721, loss 0.177786, acc 0.9
2016-09-07T16:40:42.472770: step 722, loss 0.331796, acc 0.88
2016-09-07T16:40:43.163010: step 723, loss 0.187871, acc 0.92
2016-09-07T16:40:43.845197: step 724, loss 0.110006, acc 0.94
2016-09-07T16:40:44.507844: step 725, loss 0.146581, acc 0.94
2016-09-07T16:40:45.180743: step 726, loss 0.0864622, acc 0.98
2016-09-07T16:40:45.858557: step 727, loss 0.174227, acc 0.94
2016-09-07T16:40:46.533448: step 728, loss 0.268951, acc 0.88
2016-09-07T16:40:47.210912: step 729, loss 0.177333, acc 0.9
2016-09-07T16:40:47.881249: step 730, loss 0.107011, acc 0.94
2016-09-07T16:40:48.556912: step 731, loss 0.0872322, acc 0.98
2016-09-07T16:40:49.225349: step 732, loss 0.245056, acc 0.94
2016-09-07T16:40:49.890077: step 733, loss 0.197442, acc 0.92
2016-09-07T16:40:50.565647: step 734, loss 0.151918, acc 0.92
2016-09-07T16:40:51.225336: step 735, loss 0.186007, acc 0.92
2016-09-07T16:40:51.896915: step 736, loss 0.386278, acc 0.8
2016-09-07T16:40:52.556380: step 737, loss 0.139643, acc 0.94
2016-09-07T16:40:53.251805: step 738, loss 0.179016, acc 0.96
2016-09-07T16:40:53.920632: step 739, loss 0.162806, acc 0.92
2016-09-07T16:40:54.586045: step 740, loss 0.237506, acc 0.9
2016-09-07T16:40:55.252025: step 741, loss 0.153786, acc 0.96
2016-09-07T16:40:55.929885: step 742, loss 0.175762, acc 0.9
2016-09-07T16:40:56.595593: step 743, loss 0.198633, acc 0.96
2016-09-07T16:40:57.278622: step 744, loss 0.349419, acc 0.84
2016-09-07T16:40:57.973532: step 745, loss 0.216456, acc 0.9
2016-09-07T16:40:58.643983: step 746, loss 0.161269, acc 0.96
2016-09-07T16:40:59.305842: step 747, loss 0.207143, acc 0.88
2016-09-07T16:40:59.983335: step 748, loss 0.167757, acc 0.9
2016-09-07T16:41:00.654619: step 749, loss 0.1195, acc 0.98
2016-09-07T16:41:01.322255: step 750, loss 0.145042, acc 0.94
2016-09-07T16:41:01.995370: step 751, loss 0.279718, acc 0.82
2016-09-07T16:41:02.669001: step 752, loss 0.189504, acc 0.92
2016-09-07T16:41:03.353331: step 753, loss 0.204536, acc 0.92
2016-09-07T16:41:04.005128: step 754, loss 0.25971, acc 0.88
2016-09-07T16:41:04.670026: step 755, loss 0.123633, acc 0.94
2016-09-07T16:41:05.348770: step 756, loss 0.114248, acc 0.96
2016-09-07T16:41:06.011544: step 757, loss 0.236769, acc 0.88
2016-09-07T16:41:06.674882: step 758, loss 0.108801, acc 0.94
2016-09-07T16:41:07.363721: step 759, loss 0.27826, acc 0.84
2016-09-07T16:41:08.029373: step 760, loss 0.175519, acc 0.9
2016-09-07T16:41:08.690707: step 761, loss 0.226288, acc 0.86
2016-09-07T16:41:09.365142: step 762, loss 0.288836, acc 0.86
2016-09-07T16:41:10.044109: step 763, loss 0.112688, acc 0.96
2016-09-07T16:41:10.721446: step 764, loss 0.0635596, acc 1
2016-09-07T16:41:11.399659: step 765, loss 0.102872, acc 0.98
2016-09-07T16:41:12.081233: step 766, loss 0.146027, acc 0.9
2016-09-07T16:41:12.770403: step 767, loss 0.365489, acc 0.86
2016-09-07T16:41:13.443087: step 768, loss 0.154295, acc 0.92
2016-09-07T16:41:14.100911: step 769, loss 0.293523, acc 0.86
2016-09-07T16:41:14.776361: step 770, loss 0.165167, acc 0.92
2016-09-07T16:41:15.457527: step 771, loss 0.228438, acc 0.84
2016-09-07T16:41:16.138768: step 772, loss 0.108557, acc 0.94
2016-09-07T16:41:16.835366: step 773, loss 0.191307, acc 0.92
2016-09-07T16:41:17.506892: step 774, loss 0.243232, acc 0.92
2016-09-07T16:41:18.186316: step 775, loss 0.213239, acc 0.94
2016-09-07T16:41:18.535854: step 776, loss 0.117636, acc 0.916667
2016-09-07T16:41:19.208920: step 777, loss 0.0894183, acc 0.96
2016-09-07T16:41:19.883150: step 778, loss 0.11579, acc 0.92
2016-09-07T16:41:20.548489: step 779, loss 0.223403, acc 0.88
2016-09-07T16:41:21.220302: step 780, loss 0.0563704, acc 1
2016-09-07T16:41:21.948986: step 781, loss 0.109448, acc 0.96
2016-09-07T16:41:22.641395: step 782, loss 0.086306, acc 0.94
2016-09-07T16:41:23.387485: step 783, loss 0.108461, acc 0.98
2016-09-07T16:41:24.125682: step 784, loss 0.265602, acc 0.92
2016-09-07T16:41:24.830308: step 785, loss 0.179393, acc 0.9
2016-09-07T16:41:25.690654: step 786, loss 0.0861557, acc 0.98
2016-09-07T16:41:26.436129: step 787, loss 0.0614977, acc 0.98
2016-09-07T16:41:27.172331: step 788, loss 0.254677, acc 0.92
2016-09-07T16:41:27.871085: step 789, loss 0.0315252, acc 1
2016-09-07T16:41:28.642933: step 790, loss 0.0447788, acc 1
2016-09-07T16:41:29.311235: step 791, loss 0.126466, acc 0.96
2016-09-07T16:41:29.976409: step 792, loss 0.0953086, acc 0.96
2016-09-07T16:41:30.657254: step 793, loss 0.115057, acc 0.92
2016-09-07T16:41:31.337365: step 794, loss 0.158307, acc 0.94
2016-09-07T16:41:32.001922: step 795, loss 0.11416, acc 0.96
2016-09-07T16:41:32.678151: step 796, loss 0.148318, acc 0.94
2016-09-07T16:41:33.344370: step 797, loss 0.159419, acc 0.92
2016-09-07T16:41:34.020489: step 798, loss 0.0579151, acc 0.96
2016-09-07T16:41:34.691262: step 799, loss 0.0705405, acc 0.98
2016-09-07T16:41:35.401433: step 800, loss 0.107939, acc 0.94

Evaluation:
2016-09-07T16:41:38.679395: step 800, loss 0.74223, acc 0.786

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-800

2016-09-07T16:41:40.434489: step 801, loss 0.107383, acc 0.96
2016-09-07T16:41:41.162751: step 802, loss 0.0443267, acc 0.98
2016-09-07T16:41:41.850676: step 803, loss 0.0722152, acc 0.98
2016-09-07T16:41:42.531394: step 804, loss 0.0566232, acc 0.98
2016-09-07T16:41:43.194319: step 805, loss 0.113986, acc 0.94
2016-09-07T16:41:43.858530: step 806, loss 0.128553, acc 0.94
2016-09-07T16:41:44.530451: step 807, loss 0.105789, acc 0.94
2016-09-07T16:41:45.201784: step 808, loss 0.128256, acc 0.92
2016-09-07T16:41:45.883958: step 809, loss 0.0506224, acc 0.98
2016-09-07T16:41:46.551647: step 810, loss 0.0510084, acc 1
2016-09-07T16:41:47.232519: step 811, loss 0.122967, acc 0.94
2016-09-07T16:41:47.918187: step 812, loss 0.102715, acc 0.98
2016-09-07T16:41:48.599859: step 813, loss 0.0879227, acc 0.96
2016-09-07T16:41:49.261553: step 814, loss 0.160967, acc 0.94
2016-09-07T16:41:49.937931: step 815, loss 0.331494, acc 0.94
2016-09-07T16:41:50.584355: step 816, loss 0.0540397, acc 0.98
2016-09-07T16:41:51.257140: step 817, loss 0.277242, acc 0.86
2016-09-07T16:41:51.929646: step 818, loss 0.117706, acc 0.94
2016-09-07T16:41:52.608351: step 819, loss 0.0861626, acc 0.94
2016-09-07T16:41:53.274384: step 820, loss 0.0774924, acc 0.96
2016-09-07T16:41:53.934346: step 821, loss 0.0958952, acc 0.94
2016-09-07T16:41:54.596370: step 822, loss 0.214649, acc 0.92
2016-09-07T16:41:55.269431: step 823, loss 0.170393, acc 0.96
2016-09-07T16:41:55.934960: step 824, loss 0.0971341, acc 0.94
2016-09-07T16:41:56.704000: step 825, loss 0.115794, acc 0.96
2016-09-07T16:41:57.363254: step 826, loss 0.138046, acc 0.92
2016-09-07T16:41:58.012260: step 827, loss 0.174403, acc 0.94
2016-09-07T16:41:58.690899: step 828, loss 0.095491, acc 0.96
2016-09-07T16:41:59.468121: step 829, loss 0.0971452, acc 0.98
2016-09-07T16:42:00.149372: step 830, loss 0.0638372, acc 0.98
2016-09-07T16:42:00.841512: step 831, loss 0.11049, acc 0.96
2016-09-07T16:42:01.494726: step 832, loss 0.174525, acc 0.94
2016-09-07T16:42:02.228020: step 833, loss 0.280873, acc 0.88
2016-09-07T16:42:03.133995: step 834, loss 0.0516338, acc 0.98
2016-09-07T16:42:03.891608: step 835, loss 0.0598063, acc 0.98
2016-09-07T16:42:04.574318: step 836, loss 0.0618768, acc 0.98
2016-09-07T16:42:05.257501: step 837, loss 0.104888, acc 0.96
2016-09-07T16:42:06.015373: step 838, loss 0.0639234, acc 0.98
2016-09-07T16:42:06.802900: step 839, loss 0.153002, acc 0.94
2016-09-07T16:42:07.540589: step 840, loss 0.159155, acc 0.92
2016-09-07T16:42:08.283738: step 841, loss 0.153343, acc 0.94
2016-09-07T16:42:09.037927: step 842, loss 0.241632, acc 0.92
2016-09-07T16:42:09.837573: step 843, loss 0.0893828, acc 0.96
2016-09-07T16:42:10.647811: step 844, loss 0.215669, acc 0.92
2016-09-07T16:42:11.423627: step 845, loss 0.0955787, acc 0.96
2016-09-07T16:42:12.110784: step 846, loss 0.143778, acc 0.94
2016-09-07T16:42:12.783266: step 847, loss 0.112158, acc 0.94
2016-09-07T16:42:13.552497: step 848, loss 0.153411, acc 0.96
2016-09-07T16:42:14.224153: step 849, loss 0.0970065, acc 0.98
2016-09-07T16:42:14.900765: step 850, loss 0.133138, acc 0.96
2016-09-07T16:42:15.562210: step 851, loss 0.145276, acc 0.92
2016-09-07T16:42:16.228259: step 852, loss 0.180297, acc 0.92
2016-09-07T16:42:16.907434: step 853, loss 0.14217, acc 0.9
2016-09-07T16:42:17.576834: step 854, loss 0.232023, acc 0.92
2016-09-07T16:42:18.246187: step 855, loss 0.185897, acc 0.9
2016-09-07T16:42:18.921835: step 856, loss 0.0914432, acc 0.96
2016-09-07T16:42:19.588949: step 857, loss 0.0717477, acc 0.94
2016-09-07T16:42:20.244249: step 858, loss 0.0891208, acc 0.92
2016-09-07T16:42:20.926392: step 859, loss 0.197473, acc 0.92
2016-09-07T16:42:21.606594: step 860, loss 0.186224, acc 0.92
2016-09-07T16:42:22.305091: step 861, loss 0.105096, acc 0.94
2016-09-07T16:42:22.978928: step 862, loss 0.134196, acc 0.9
2016-09-07T16:42:23.641735: step 863, loss 0.0690951, acc 0.96
2016-09-07T16:42:24.316155: step 864, loss 0.0763849, acc 0.98
2016-09-07T16:42:24.978920: step 865, loss 0.0621029, acc 1
2016-09-07T16:42:25.641999: step 866, loss 0.121874, acc 0.94
2016-09-07T16:42:26.324075: step 867, loss 0.142402, acc 0.92
2016-09-07T16:42:27.106058: step 868, loss 0.0702918, acc 0.98
2016-09-07T16:42:27.822945: step 869, loss 0.13729, acc 0.94
2016-09-07T16:42:28.555554: step 870, loss 0.123091, acc 0.92
2016-09-07T16:42:29.214364: step 871, loss 0.153295, acc 0.92
2016-09-07T16:42:29.928014: step 872, loss 0.102334, acc 0.98
2016-09-07T16:42:30.692832: step 873, loss 0.147558, acc 0.92
2016-09-07T16:42:31.399130: step 874, loss 0.181302, acc 0.94
2016-09-07T16:42:32.079365: step 875, loss 0.117965, acc 0.96
2016-09-07T16:42:32.765851: step 876, loss 0.144129, acc 0.92
2016-09-07T16:42:33.453476: step 877, loss 0.15482, acc 0.96
2016-09-07T16:42:34.221855: step 878, loss 0.0598865, acc 0.98
2016-09-07T16:42:34.981596: step 879, loss 0.118023, acc 0.96
2016-09-07T16:42:35.784208: step 880, loss 0.161555, acc 0.94
2016-09-07T16:42:36.484969: step 881, loss 0.185782, acc 0.9
2016-09-07T16:42:37.236859: step 882, loss 0.0655656, acc 0.98
2016-09-07T16:42:37.940582: step 883, loss 0.119933, acc 0.96
2016-09-07T16:42:38.658245: step 884, loss 0.139643, acc 0.92
2016-09-07T16:42:39.354507: step 885, loss 0.186775, acc 0.92
2016-09-07T16:42:40.020458: step 886, loss 0.0750479, acc 0.98
2016-09-07T16:42:40.689468: step 887, loss 0.170604, acc 0.92
2016-09-07T16:42:41.369576: step 888, loss 0.137648, acc 0.92
2016-09-07T16:42:42.053759: step 889, loss 0.188242, acc 0.92
2016-09-07T16:42:42.720976: step 890, loss 0.0872542, acc 0.96
2016-09-07T16:42:43.380201: step 891, loss 0.162306, acc 0.96
2016-09-07T16:42:44.060545: step 892, loss 0.157392, acc 0.94
2016-09-07T16:42:44.773014: step 893, loss 0.0354597, acc 1
2016-09-07T16:42:45.503350: step 894, loss 0.158313, acc 0.92
2016-09-07T16:42:46.230122: step 895, loss 0.181265, acc 0.96
2016-09-07T16:42:46.934218: step 896, loss 0.156968, acc 0.94
2016-09-07T16:42:47.635875: step 897, loss 0.078924, acc 0.98
2016-09-07T16:42:48.334249: step 898, loss 0.138869, acc 0.92
2016-09-07T16:42:49.015361: step 899, loss 0.0950432, acc 0.96
2016-09-07T16:42:49.710858: step 900, loss 0.0789689, acc 0.98

Evaluation:
2016-09-07T16:42:52.840578: step 900, loss 0.805974, acc 0.765

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-900

2016-09-07T16:42:54.489346: step 901, loss 0.182129, acc 0.9
2016-09-07T16:42:55.165102: step 902, loss 0.119002, acc 0.96
2016-09-07T16:42:55.837624: step 903, loss 0.149921, acc 0.96
2016-09-07T16:42:56.516408: step 904, loss 0.0629095, acc 0.96
2016-09-07T16:42:57.203710: step 905, loss 0.117137, acc 0.96
2016-09-07T16:42:57.880661: step 906, loss 0.134606, acc 0.92
2016-09-07T16:42:58.568979: step 907, loss 0.210721, acc 0.96
2016-09-07T16:42:59.251166: step 908, loss 0.108772, acc 0.94
2016-09-07T16:42:59.940063: step 909, loss 0.112441, acc 0.94
2016-09-07T16:43:00.660721: step 910, loss 0.125791, acc 0.94
2016-09-07T16:43:01.347418: step 911, loss 0.123148, acc 0.96
2016-09-07T16:43:02.034305: step 912, loss 0.0608029, acc 1
2016-09-07T16:43:02.725134: step 913, loss 0.168513, acc 0.9
2016-09-07T16:43:03.439852: step 914, loss 0.10913, acc 0.96
2016-09-07T16:43:04.133692: step 915, loss 0.165308, acc 0.92
2016-09-07T16:43:04.798669: step 916, loss 0.120485, acc 0.94
2016-09-07T16:43:05.488522: step 917, loss 0.0514258, acc 1
2016-09-07T16:43:06.225325: step 918, loss 0.219956, acc 0.9
2016-09-07T16:43:06.914787: step 919, loss 0.0885013, acc 0.98
2016-09-07T16:43:07.744713: step 920, loss 0.166703, acc 0.92
2016-09-07T16:43:08.567224: step 921, loss 0.0933068, acc 0.98
2016-09-07T16:43:09.266610: step 922, loss 0.198718, acc 0.94
2016-09-07T16:43:09.942818: step 923, loss 0.129649, acc 0.94
2016-09-07T16:43:10.618084: step 924, loss 0.0981803, acc 0.96
2016-09-07T16:43:11.300800: step 925, loss 0.133687, acc 0.92
2016-09-07T16:43:12.152619: step 926, loss 0.223482, acc 0.92
2016-09-07T16:43:12.853941: step 927, loss 0.196754, acc 0.9
2016-09-07T16:43:13.581285: step 928, loss 0.134805, acc 0.94
2016-09-07T16:43:14.307121: step 929, loss 0.153226, acc 0.92
2016-09-07T16:43:14.990492: step 930, loss 0.0659796, acc 0.98
2016-09-07T16:43:15.644199: step 931, loss 0.131957, acc 0.94
2016-09-07T16:43:16.328042: step 932, loss 0.141135, acc 0.96
2016-09-07T16:43:16.983450: step 933, loss 0.0543218, acc 0.98
2016-09-07T16:43:17.653543: step 934, loss 0.0749389, acc 0.98
2016-09-07T16:43:18.331221: step 935, loss 0.159219, acc 0.96
2016-09-07T16:43:19.024551: step 936, loss 0.131341, acc 0.96
2016-09-07T16:43:19.691660: step 937, loss 0.118456, acc 0.94
2016-09-07T16:43:20.434972: step 938, loss 0.166494, acc 0.92
2016-09-07T16:43:21.098220: step 939, loss 0.0924389, acc 0.96
2016-09-07T16:43:21.754418: step 940, loss 0.0599928, acc 0.98
2016-09-07T16:43:22.427368: step 941, loss 0.101413, acc 0.94
2016-09-07T16:43:23.122556: step 942, loss 0.0973596, acc 0.96
2016-09-07T16:43:23.822037: step 943, loss 0.273477, acc 0.92
2016-09-07T16:43:24.494081: step 944, loss 0.115003, acc 0.98
2016-09-07T16:43:25.248491: step 945, loss 0.353183, acc 0.82
2016-09-07T16:43:25.911818: step 946, loss 0.253491, acc 0.86
2016-09-07T16:43:26.568876: step 947, loss 0.116866, acc 0.94
2016-09-07T16:43:27.257366: step 948, loss 0.266834, acc 0.86
2016-09-07T16:43:27.946539: step 949, loss 0.148172, acc 0.96
2016-09-07T16:43:28.612116: step 950, loss 0.0851971, acc 0.96
2016-09-07T16:43:29.334427: step 951, loss 0.108882, acc 0.92
2016-09-07T16:43:30.001714: step 952, loss 0.151639, acc 0.94
2016-09-07T16:43:30.791673: step 953, loss 0.0825227, acc 0.96
2016-09-07T16:43:31.466162: step 954, loss 0.133746, acc 0.9
2016-09-07T16:43:32.160522: step 955, loss 0.212795, acc 0.92
2016-09-07T16:43:32.928401: step 956, loss 0.0867692, acc 0.98
2016-09-07T16:43:33.589594: step 957, loss 0.229266, acc 0.9
2016-09-07T16:43:34.251149: step 958, loss 0.129539, acc 0.94
2016-09-07T16:43:34.982606: step 959, loss 0.0801846, acc 0.96
2016-09-07T16:43:35.673780: step 960, loss 0.100774, acc 0.94
2016-09-07T16:43:36.368626: step 961, loss 0.19604, acc 0.92
2016-09-07T16:43:37.093823: step 962, loss 0.139949, acc 0.94
2016-09-07T16:43:37.772257: step 963, loss 0.158892, acc 0.96
2016-09-07T16:43:38.471739: step 964, loss 0.070842, acc 0.98
2016-09-07T16:43:39.146363: step 965, loss 0.0828172, acc 0.94
2016-09-07T16:43:39.835383: step 966, loss 0.138228, acc 0.9
2016-09-07T16:43:40.515698: step 967, loss 0.185604, acc 0.92
2016-09-07T16:43:41.189240: step 968, loss 0.0632323, acc 0.98
2016-09-07T16:43:41.862607: step 969, loss 0.189498, acc 0.92
2016-09-07T16:43:42.233883: step 970, loss 0.109247, acc 0.916667
2016-09-07T16:43:42.925568: step 971, loss 0.0864478, acc 0.96
2016-09-07T16:43:43.601924: step 972, loss 0.132968, acc 0.94
2016-09-07T16:43:44.276972: step 973, loss 0.131217, acc 0.94
2016-09-07T16:43:44.941992: step 974, loss 0.109495, acc 0.96
2016-09-07T16:43:45.616040: step 975, loss 0.0779297, acc 0.96
2016-09-07T16:43:46.278039: step 976, loss 0.135988, acc 0.94
2016-09-07T16:43:46.952774: step 977, loss 0.0860044, acc 0.98
2016-09-07T16:43:47.624190: step 978, loss 0.115868, acc 0.96
2016-09-07T16:43:48.313791: step 979, loss 0.0431462, acc 0.98
2016-09-07T16:43:49.013560: step 980, loss 0.184959, acc 0.92
2016-09-07T16:43:49.770194: step 981, loss 0.154561, acc 0.94
2016-09-07T16:43:50.480542: step 982, loss 0.218338, acc 0.92
2016-09-07T16:43:51.207304: step 983, loss 0.173372, acc 0.9
2016-09-07T16:43:51.917297: step 984, loss 0.112199, acc 0.94
2016-09-07T16:43:52.582345: step 985, loss 0.0784009, acc 0.96
2016-09-07T16:43:53.271627: step 986, loss 0.0823867, acc 0.98
2016-09-07T16:43:53.952879: step 987, loss 0.0818416, acc 0.98
2016-09-07T16:43:54.632465: step 988, loss 0.0238915, acc 1
2016-09-07T16:43:55.291986: step 989, loss 0.115684, acc 0.94
2016-09-07T16:43:55.964551: step 990, loss 0.0237308, acc 1
2016-09-07T16:43:56.624345: step 991, loss 0.137129, acc 0.94
2016-09-07T16:43:57.279236: step 992, loss 0.185247, acc 0.94
2016-09-07T16:43:57.951576: step 993, loss 0.0353789, acc 1
2016-09-07T16:43:58.625037: step 994, loss 0.132146, acc 0.94
2016-09-07T16:43:59.372197: step 995, loss 0.0640182, acc 0.98
2016-09-07T16:44:00.079461: step 996, loss 0.0676851, acc 0.98
2016-09-07T16:44:00.804765: step 997, loss 0.075426, acc 0.96
2016-09-07T16:44:01.521577: step 998, loss 0.0927501, acc 0.96
2016-09-07T16:44:02.343666: step 999, loss 0.123858, acc 0.94
2016-09-07T16:44:03.017930: step 1000, loss 0.0712423, acc 1

Evaluation:
2016-09-07T16:44:06.114900: step 1000, loss 0.789958, acc 0.778

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-1000

2016-09-07T16:44:07.713378: step 1001, loss 0.0564767, acc 0.98
2016-09-07T16:44:08.387186: step 1002, loss 0.111943, acc 0.96
2016-09-07T16:44:09.051599: step 1003, loss 0.0872577, acc 0.94
2016-09-07T16:44:09.715385: step 1004, loss 0.149114, acc 0.88
2016-09-07T16:44:10.387128: step 1005, loss 0.0927386, acc 0.96
2016-09-07T16:44:11.039883: step 1006, loss 0.0642644, acc 0.98
2016-09-07T16:44:11.714157: step 1007, loss 0.0965014, acc 0.96
2016-09-07T16:44:12.372399: step 1008, loss 0.0409475, acc 1
2016-09-07T16:44:13.049149: step 1009, loss 0.0570456, acc 0.98
2016-09-07T16:44:13.711509: step 1010, loss 0.147888, acc 0.96
2016-09-07T16:44:14.378359: step 1011, loss 0.0960625, acc 0.96
2016-09-07T16:44:15.062306: step 1012, loss 0.0543335, acc 0.98
2016-09-07T16:44:15.755425: step 1013, loss 0.0315441, acc 1
2016-09-07T16:44:16.429761: step 1014, loss 0.0590276, acc 0.96
2016-09-07T16:44:17.122072: step 1015, loss 0.0306147, acc 0.98
2016-09-07T16:44:17.791552: step 1016, loss 0.0705226, acc 0.94
2016-09-07T16:44:18.473722: step 1017, loss 0.12962, acc 0.96
2016-09-07T16:44:19.162468: step 1018, loss 0.0851521, acc 0.98
2016-09-07T16:44:19.839366: step 1019, loss 0.0583259, acc 0.98
2016-09-07T16:44:20.510626: step 1020, loss 0.0824755, acc 0.96
2016-09-07T16:44:21.189345: step 1021, loss 0.167839, acc 0.96
2016-09-07T16:44:21.854829: step 1022, loss 0.0854099, acc 0.96
2016-09-07T16:44:22.519402: step 1023, loss 0.0442839, acc 1
2016-09-07T16:44:23.183608: step 1024, loss 0.0875649, acc 0.96
2016-09-07T16:44:23.846408: step 1025, loss 0.100774, acc 0.96
2016-09-07T16:44:24.522679: step 1026, loss 0.147641, acc 0.94
2016-09-07T16:44:25.213107: step 1027, loss 0.0638183, acc 0.98
2016-09-07T16:44:25.887385: step 1028, loss 0.0743591, acc 0.96
2016-09-07T16:44:26.558776: step 1029, loss 0.0652109, acc 0.96
2016-09-07T16:44:27.232670: step 1030, loss 0.0466048, acc 0.98
2016-09-07T16:44:27.920295: step 1031, loss 0.0822513, acc 0.94
2016-09-07T16:44:28.614712: step 1032, loss 0.0579543, acc 0.98
2016-09-07T16:44:29.293884: step 1033, loss 0.082336, acc 0.96
2016-09-07T16:44:29.990608: step 1034, loss 0.151691, acc 0.92
2016-09-07T16:44:30.653438: step 1035, loss 0.204436, acc 0.92
2016-09-07T16:44:31.317311: step 1036, loss 0.0525816, acc 0.98
2016-09-07T16:44:32.003569: step 1037, loss 0.00844141, acc 1
2016-09-07T16:44:32.670927: step 1038, loss 0.165855, acc 0.92
2016-09-07T16:44:33.345755: step 1039, loss 0.116165, acc 0.96
2016-09-07T16:44:34.010027: step 1040, loss 0.110953, acc 0.94
2016-09-07T16:44:34.689924: step 1041, loss 0.0136772, acc 1
2016-09-07T16:44:35.358927: step 1042, loss 0.0262003, acc 0.98
2016-09-07T16:44:36.017572: step 1043, loss 0.196432, acc 0.86
2016-09-07T16:44:36.698479: step 1044, loss 0.0120645, acc 1
2016-09-07T16:44:37.361354: step 1045, loss 0.104112, acc 0.94
2016-09-07T16:44:38.016356: step 1046, loss 0.192492, acc 0.94
2016-09-07T16:44:38.671990: step 1047, loss 0.122676, acc 0.94
2016-09-07T16:44:39.333150: step 1048, loss 0.0633609, acc 0.98
2016-09-07T16:44:39.986383: step 1049, loss 0.0828555, acc 0.98
2016-09-07T16:44:40.652012: step 1050, loss 0.0752354, acc 0.94
2016-09-07T16:44:41.332674: step 1051, loss 0.0477957, acc 0.98
2016-09-07T16:44:42.022864: step 1052, loss 0.103082, acc 0.98
2016-09-07T16:44:42.689024: step 1053, loss 0.0675367, acc 0.98
2016-09-07T16:44:43.370804: step 1054, loss 0.12018, acc 0.94
2016-09-07T16:44:44.039409: step 1055, loss 0.0531176, acc 0.98
2016-09-07T16:44:44.716154: step 1056, loss 0.155203, acc 0.94
2016-09-07T16:44:45.379836: step 1057, loss 0.0557285, acc 0.96
2016-09-07T16:44:46.045913: step 1058, loss 0.113686, acc 0.94
2016-09-07T16:44:46.722231: step 1059, loss 0.249998, acc 0.88
2016-09-07T16:44:47.390656: step 1060, loss 0.0811292, acc 0.98
2016-09-07T16:44:48.063817: step 1061, loss 0.039649, acc 1
2016-09-07T16:44:48.741463: step 1062, loss 0.0927637, acc 0.96
2016-09-07T16:44:49.387521: step 1063, loss 0.218241, acc 0.94
2016-09-07T16:44:50.062929: step 1064, loss 0.108797, acc 0.94
2016-09-07T16:44:50.757206: step 1065, loss 0.133464, acc 0.96
2016-09-07T16:44:51.436720: step 1066, loss 0.164532, acc 0.88
2016-09-07T16:44:52.122058: step 1067, loss 0.0610044, acc 1
2016-09-07T16:44:52.790432: step 1068, loss 0.0336303, acc 1
2016-09-07T16:44:53.461611: step 1069, loss 0.127271, acc 0.9
2016-09-07T16:44:54.135368: step 1070, loss 0.0514514, acc 0.98
2016-09-07T16:44:54.820429: step 1071, loss 0.0618196, acc 0.96
2016-09-07T16:44:55.480822: step 1072, loss 0.0477636, acc 1
2016-09-07T16:44:56.174781: step 1073, loss 0.138378, acc 0.96
2016-09-07T16:44:56.843066: step 1074, loss 0.103778, acc 0.94
2016-09-07T16:44:57.529503: step 1075, loss 0.0957197, acc 0.98
2016-09-07T16:44:58.221388: step 1076, loss 0.0764313, acc 0.94
2016-09-07T16:44:58.900556: step 1077, loss 0.0986711, acc 0.96
2016-09-07T16:44:59.581003: step 1078, loss 0.200837, acc 0.94
2016-09-07T16:45:00.260135: step 1079, loss 0.184315, acc 0.9
2016-09-07T16:45:00.941530: step 1080, loss 0.157243, acc 0.92
2016-09-07T16:45:01.636413: step 1081, loss 0.113625, acc 0.96
2016-09-07T16:45:02.401522: step 1082, loss 0.0347787, acc 0.98
2016-09-07T16:45:03.241077: step 1083, loss 0.10679, acc 0.94
2016-09-07T16:45:03.934464: step 1084, loss 0.0669013, acc 0.98
2016-09-07T16:45:04.658876: step 1085, loss 0.105381, acc 0.96
2016-09-07T16:45:05.395917: step 1086, loss 0.18062, acc 0.9
2016-09-07T16:45:06.090517: step 1087, loss 0.0903104, acc 0.98
2016-09-07T16:45:06.752751: step 1088, loss 0.0533649, acc 0.98
2016-09-07T16:45:07.445397: step 1089, loss 0.167979, acc 0.96
2016-09-07T16:45:08.162977: step 1090, loss 0.179185, acc 0.9
2016-09-07T16:45:08.857478: step 1091, loss 0.103259, acc 0.94
2016-09-07T16:45:09.552590: step 1092, loss 0.109094, acc 0.94
2016-09-07T16:45:10.244810: step 1093, loss 0.0641662, acc 0.98
2016-09-07T16:45:11.002031: step 1094, loss 0.106678, acc 0.96
2016-09-07T16:45:11.820302: step 1095, loss 0.051961, acc 0.98
2016-09-07T16:45:12.523385: step 1096, loss 0.131089, acc 0.92
2016-09-07T16:45:13.374991: step 1097, loss 0.21033, acc 0.9
2016-09-07T16:45:14.108825: step 1098, loss 0.0704539, acc 0.96
2016-09-07T16:45:14.788871: step 1099, loss 0.136002, acc 0.98
2016-09-07T16:45:15.451961: step 1100, loss 0.0747527, acc 1

Evaluation:
2016-09-07T16:45:18.752046: step 1100, loss 0.822221, acc 0.762

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-1100

2016-09-07T16:45:20.641613: step 1101, loss 0.0882041, acc 0.98
2016-09-07T16:45:21.508076: step 1102, loss 0.134993, acc 0.94
2016-09-07T16:45:22.210853: step 1103, loss 0.0433385, acc 1
2016-09-07T16:45:22.938060: step 1104, loss 0.0520909, acc 0.98
2016-09-07T16:45:23.773525: step 1105, loss 0.185271, acc 0.92
2016-09-07T16:45:24.627095: step 1106, loss 0.151645, acc 0.96
2016-09-07T16:45:25.497945: step 1107, loss 0.141527, acc 0.94
2016-09-07T16:45:26.222208: step 1108, loss 0.0632871, acc 0.98
2016-09-07T16:45:27.022306: step 1109, loss 0.120458, acc 0.96
2016-09-07T16:45:27.906209: step 1110, loss 0.154819, acc 0.9
2016-09-07T16:45:28.710240: step 1111, loss 0.0762798, acc 0.96
2016-09-07T16:45:29.573987: step 1112, loss 0.157215, acc 0.94
2016-09-07T16:45:30.450373: step 1113, loss 0.0223217, acc 1
2016-09-07T16:45:31.319490: step 1114, loss 0.0534921, acc 0.96
2016-09-07T16:45:32.052626: step 1115, loss 0.0576629, acc 0.98
2016-09-07T16:45:32.878492: step 1116, loss 0.184505, acc 0.92
2016-09-07T16:45:33.709608: step 1117, loss 0.124385, acc 0.96
2016-09-07T16:45:34.612818: step 1118, loss 0.0418793, acc 0.98
2016-09-07T16:45:35.530137: step 1119, loss 0.101059, acc 0.98
2016-09-07T16:45:36.400623: step 1120, loss 0.192028, acc 0.94
2016-09-07T16:45:37.229859: step 1121, loss 0.0889366, acc 0.96
2016-09-07T16:45:37.965653: step 1122, loss 0.198996, acc 0.86
2016-09-07T16:45:38.645178: step 1123, loss 0.0942863, acc 0.94
2016-09-07T16:45:39.319672: step 1124, loss 0.0806884, acc 0.98
2016-09-07T16:45:40.034937: step 1125, loss 0.157678, acc 0.9
2016-09-07T16:45:40.703255: step 1126, loss 0.146029, acc 0.98
2016-09-07T16:45:41.359006: step 1127, loss 0.0771446, acc 0.96
2016-09-07T16:45:42.041477: step 1128, loss 0.126917, acc 0.94
2016-09-07T16:45:42.710351: step 1129, loss 0.132382, acc 0.9
2016-09-07T16:45:43.394249: step 1130, loss 0.0734266, acc 0.96
2016-09-07T16:45:44.076738: step 1131, loss 0.0341491, acc 1
2016-09-07T16:45:44.727328: step 1132, loss 0.0926448, acc 0.96
2016-09-07T16:45:45.412102: step 1133, loss 0.0970152, acc 0.98
2016-09-07T16:45:46.083310: step 1134, loss 0.027382, acc 1
2016-09-07T16:45:46.751161: step 1135, loss 0.0612311, acc 0.98
2016-09-07T16:45:47.432783: step 1136, loss 0.117973, acc 0.98
2016-09-07T16:45:48.110452: step 1137, loss 0.113145, acc 0.94
2016-09-07T16:45:48.770286: step 1138, loss 0.0900315, acc 0.94
2016-09-07T16:45:49.436097: step 1139, loss 0.0351309, acc 0.98
2016-09-07T16:45:50.104038: step 1140, loss 0.164476, acc 0.94
2016-09-07T16:45:50.787154: step 1141, loss 0.0833858, acc 0.96
2016-09-07T16:45:51.460257: step 1142, loss 0.0632452, acc 1
2016-09-07T16:45:52.143144: step 1143, loss 0.164469, acc 0.92
2016-09-07T16:45:52.820493: step 1144, loss 0.0163451, acc 1
2016-09-07T16:45:53.480718: step 1145, loss 0.183963, acc 0.94
2016-09-07T16:45:54.128465: step 1146, loss 0.180637, acc 0.94
2016-09-07T16:45:54.809436: step 1147, loss 0.0995078, acc 0.96
2016-09-07T16:45:55.471873: step 1148, loss 0.066498, acc 0.98
2016-09-07T16:45:56.130177: step 1149, loss 0.0682093, acc 0.98
2016-09-07T16:45:56.783481: step 1150, loss 0.0894034, acc 0.96
2016-09-07T16:45:57.452763: step 1151, loss 0.108186, acc 0.9
2016-09-07T16:45:58.123901: step 1152, loss 0.0759657, acc 0.98
2016-09-07T16:45:58.810138: step 1153, loss 0.0771048, acc 0.98
2016-09-07T16:45:59.503249: step 1154, loss 0.0173191, acc 1
2016-09-07T16:46:00.183751: step 1155, loss 0.0757042, acc 0.98
2016-09-07T16:46:00.869804: step 1156, loss 0.0811689, acc 0.98
2016-09-07T16:46:01.533112: step 1157, loss 0.213102, acc 0.88
2016-09-07T16:46:02.190795: step 1158, loss 0.370805, acc 0.92
2016-09-07T16:46:02.847069: step 1159, loss 0.0806834, acc 0.98
2016-09-07T16:46:03.514187: step 1160, loss 0.0394254, acc 1
2016-09-07T16:46:04.194668: step 1161, loss 0.0967005, acc 0.96
2016-09-07T16:46:04.854948: step 1162, loss 0.112781, acc 0.96
2016-09-07T16:46:05.519347: step 1163, loss 0.0870029, acc 0.94
2016-09-07T16:46:05.887772: step 1164, loss 0.143063, acc 1
2016-09-07T16:46:06.573543: step 1165, loss 0.102925, acc 0.94
2016-09-07T16:46:07.234645: step 1166, loss 0.0855425, acc 0.96
2016-09-07T16:46:07.919997: step 1167, loss 0.0494052, acc 1
2016-09-07T16:46:08.593254: step 1168, loss 0.0755487, acc 0.98
2016-09-07T16:46:09.270423: step 1169, loss 0.097512, acc 0.96
2016-09-07T16:46:09.964226: step 1170, loss 0.231762, acc 0.9
2016-09-07T16:46:10.642266: step 1171, loss 0.109687, acc 0.96
2016-09-07T16:46:11.311226: step 1172, loss 0.0380758, acc 0.98
2016-09-07T16:46:11.972548: step 1173, loss 0.0290594, acc 1
2016-09-07T16:46:12.656712: step 1174, loss 0.0203005, acc 1
2016-09-07T16:46:13.325802: step 1175, loss 0.103475, acc 0.92
2016-09-07T16:46:14.007463: step 1176, loss 0.223638, acc 0.88
2016-09-07T16:46:14.676274: step 1177, loss 0.039559, acc 0.98
2016-09-07T16:46:15.350591: step 1178, loss 0.0165313, acc 1
2016-09-07T16:46:16.017525: step 1179, loss 0.0354703, acc 0.98
2016-09-07T16:46:16.688215: step 1180, loss 0.0751953, acc 0.96
2016-09-07T16:46:17.354221: step 1181, loss 0.0335814, acc 1
2016-09-07T16:46:18.012790: step 1182, loss 0.0293683, acc 0.98
2016-09-07T16:46:18.678106: step 1183, loss 0.105041, acc 0.96
2016-09-07T16:46:19.354865: step 1184, loss 0.141191, acc 0.96
2016-09-07T16:46:20.020539: step 1185, loss 0.0669043, acc 0.98
2016-09-07T16:46:20.689563: step 1186, loss 0.0760542, acc 0.96
2016-09-07T16:46:21.374127: step 1187, loss 0.0299421, acc 1
2016-09-07T16:46:22.042751: step 1188, loss 0.0692471, acc 0.96
2016-09-07T16:46:22.722699: step 1189, loss 0.036831, acc 0.98
2016-09-07T16:46:23.391364: step 1190, loss 0.0450656, acc 0.96
2016-09-07T16:46:24.067869: step 1191, loss 0.0702572, acc 0.98
2016-09-07T16:46:24.749559: step 1192, loss 0.0478379, acc 1
2016-09-07T16:46:25.426066: step 1193, loss 0.0477004, acc 0.98
2016-09-07T16:46:26.079888: step 1194, loss 0.0344646, acc 1
2016-09-07T16:46:26.754416: step 1195, loss 0.0229016, acc 0.98
2016-09-07T16:46:27.405528: step 1196, loss 0.104864, acc 0.94
2016-09-07T16:46:28.068289: step 1197, loss 0.0273291, acc 0.98
2016-09-07T16:46:28.745203: step 1198, loss 0.0582564, acc 0.96
2016-09-07T16:46:29.406591: step 1199, loss 0.0842189, acc 0.96
2016-09-07T16:46:30.087327: step 1200, loss 0.132234, acc 0.98

Evaluation:
2016-09-07T16:46:33.106771: step 1200, loss 1.27336, acc 0.76

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-1200

2016-09-07T16:46:34.753866: step 1201, loss 0.0819972, acc 0.96
2016-09-07T16:46:35.439390: step 1202, loss 0.0754602, acc 0.96
2016-09-07T16:46:36.097289: step 1203, loss 0.0372029, acc 0.98
2016-09-07T16:46:36.776662: step 1204, loss 0.0790605, acc 0.94
2016-09-07T16:46:37.462558: step 1205, loss 0.0451228, acc 0.98
2016-09-07T16:46:38.143802: step 1206, loss 0.0455565, acc 1
2016-09-07T16:46:38.813215: step 1207, loss 0.141867, acc 0.96
2016-09-07T16:46:39.497687: step 1208, loss 0.0148665, acc 1
2016-09-07T16:46:40.171610: step 1209, loss 0.0727919, acc 0.94
2016-09-07T16:46:40.851946: step 1210, loss 0.259207, acc 0.9
2016-09-07T16:46:41.516042: step 1211, loss 0.0518053, acc 0.98
2016-09-07T16:46:42.177259: step 1212, loss 0.0872396, acc 0.96
2016-09-07T16:46:42.837622: step 1213, loss 0.116213, acc 0.94
2016-09-07T16:46:43.526208: step 1214, loss 0.058646, acc 0.98
2016-09-07T16:46:44.223802: step 1215, loss 0.162823, acc 0.96
2016-09-07T16:46:44.906854: step 1216, loss 0.00679295, acc 1
2016-09-07T16:46:45.598420: step 1217, loss 0.0432358, acc 1
2016-09-07T16:46:46.257081: step 1218, loss 0.0351817, acc 1
2016-09-07T16:46:46.922664: step 1219, loss 0.0502714, acc 0.96
2016-09-07T16:46:47.581211: step 1220, loss 0.0969504, acc 0.94
2016-09-07T16:46:48.250404: step 1221, loss 0.107356, acc 0.98
2016-09-07T16:46:48.916935: step 1222, loss 0.0456901, acc 1
2016-09-07T16:46:49.606237: step 1223, loss 0.0722393, acc 0.98
2016-09-07T16:46:50.273305: step 1224, loss 0.0602801, acc 0.98
2016-09-07T16:46:50.953898: step 1225, loss 0.050141, acc 0.98
2016-09-07T16:46:51.625705: step 1226, loss 0.0772922, acc 0.96
2016-09-07T16:46:52.303502: step 1227, loss 0.0795743, acc 0.94
2016-09-07T16:46:52.966799: step 1228, loss 0.0805709, acc 0.96
2016-09-07T16:46:53.604108: step 1229, loss 0.148245, acc 0.92
2016-09-07T16:46:54.272577: step 1230, loss 0.122475, acc 0.96
2016-09-07T16:46:54.959520: step 1231, loss 0.0483567, acc 0.98
2016-09-07T16:46:55.611324: step 1232, loss 0.0546069, acc 0.98
2016-09-07T16:46:56.282375: step 1233, loss 0.0199704, acc 1
2016-09-07T16:46:56.952454: step 1234, loss 0.0651537, acc 0.96
2016-09-07T16:46:57.630430: step 1235, loss 0.12405, acc 0.92
2016-09-07T16:46:58.292121: step 1236, loss 0.0739579, acc 0.98
2016-09-07T16:46:58.954440: step 1237, loss 0.0303037, acc 1
2016-09-07T16:46:59.622423: step 1238, loss 0.0133306, acc 1
2016-09-07T16:47:00.319275: step 1239, loss 0.0344193, acc 0.98
2016-09-07T16:47:01.012552: step 1240, loss 0.25073, acc 0.88
2016-09-07T16:47:01.687815: step 1241, loss 0.0673264, acc 0.96
2016-09-07T16:47:02.347116: step 1242, loss 0.0707801, acc 0.96
2016-09-07T16:47:03.010846: step 1243, loss 0.0632687, acc 0.98
2016-09-07T16:47:03.688156: step 1244, loss 0.181955, acc 0.94
2016-09-07T16:47:04.362766: step 1245, loss 0.186201, acc 0.92
2016-09-07T16:47:05.056882: step 1246, loss 0.222838, acc 0.94
2016-09-07T16:47:05.747843: step 1247, loss 0.102538, acc 0.94
2016-09-07T16:47:06.431594: step 1248, loss 0.0373576, acc 0.98
2016-09-07T16:47:07.105433: step 1249, loss 0.225244, acc 0.9
2016-09-07T16:47:07.779065: step 1250, loss 0.0720438, acc 0.98
2016-09-07T16:47:08.453901: step 1251, loss 0.0480269, acc 0.98
2016-09-07T16:47:09.120234: step 1252, loss 0.0512806, acc 1
2016-09-07T16:47:09.784379: step 1253, loss 0.166562, acc 0.9
2016-09-07T16:47:10.456840: step 1254, loss 0.0646882, acc 0.94
2016-09-07T16:47:11.135157: step 1255, loss 0.092008, acc 0.98
2016-09-07T16:47:11.817718: step 1256, loss 0.107616, acc 0.92
2016-09-07T16:47:12.499349: step 1257, loss 0.0881119, acc 0.94
2016-09-07T16:47:13.196409: step 1258, loss 0.17521, acc 0.96
2016-09-07T16:47:13.870084: step 1259, loss 0.0545408, acc 1
2016-09-07T16:47:14.535987: step 1260, loss 0.0898185, acc 0.96
2016-09-07T16:47:15.200636: step 1261, loss 0.0805188, acc 0.96
2016-09-07T16:47:15.854236: step 1262, loss 0.115956, acc 0.98
2016-09-07T16:47:16.515894: step 1263, loss 0.0743209, acc 0.98
2016-09-07T16:47:17.187136: step 1264, loss 0.0916878, acc 0.96
2016-09-07T16:47:17.877870: step 1265, loss 0.0507036, acc 0.98
2016-09-07T16:47:18.535312: step 1266, loss 0.0718172, acc 0.98
2016-09-07T16:47:19.196116: step 1267, loss 0.0642662, acc 0.96
2016-09-07T16:47:19.860002: step 1268, loss 0.113957, acc 0.92
2016-09-07T16:47:20.533335: step 1269, loss 0.0849037, acc 0.96
2016-09-07T16:47:21.211945: step 1270, loss 0.0632966, acc 0.96
2016-09-07T16:47:21.887740: step 1271, loss 0.0450363, acc 0.98
2016-09-07T16:47:22.557443: step 1272, loss 0.0789786, acc 0.98
2016-09-07T16:47:23.219602: step 1273, loss 0.113601, acc 0.96
2016-09-07T16:47:23.900055: step 1274, loss 0.0432739, acc 0.98
2016-09-07T16:47:24.553903: step 1275, loss 0.12633, acc 0.94
2016-09-07T16:47:25.203255: step 1276, loss 0.0178086, acc 1
2016-09-07T16:47:25.869736: step 1277, loss 0.112359, acc 0.98
2016-09-07T16:47:26.539882: step 1278, loss 0.107275, acc 0.92
2016-09-07T16:47:27.228636: step 1279, loss 0.0843037, acc 0.96
2016-09-07T16:47:27.894966: step 1280, loss 0.0897732, acc 0.96
2016-09-07T16:47:28.574729: step 1281, loss 0.0594488, acc 0.98
2016-09-07T16:47:29.259295: step 1282, loss 0.0588474, acc 0.98
2016-09-07T16:47:29.938982: step 1283, loss 0.0219527, acc 1
2016-09-07T16:47:30.591808: step 1284, loss 0.0639216, acc 0.98
2016-09-07T16:47:31.248362: step 1285, loss 0.0635902, acc 0.98
2016-09-07T16:47:31.927381: step 1286, loss 0.118891, acc 0.96
2016-09-07T16:47:32.626852: step 1287, loss 0.501658, acc 0.94
2016-09-07T16:47:33.288218: step 1288, loss 0.0630837, acc 0.98
2016-09-07T16:47:33.954772: step 1289, loss 0.268545, acc 0.9
2016-09-07T16:47:34.604947: step 1290, loss 0.0532199, acc 0.98
2016-09-07T16:47:35.272090: step 1291, loss 0.0911146, acc 0.96
2016-09-07T16:47:35.958430: step 1292, loss 0.0627312, acc 0.98
2016-09-07T16:47:36.636766: step 1293, loss 0.0981149, acc 0.94
2016-09-07T16:47:37.311848: step 1294, loss 0.0831086, acc 0.98
2016-09-07T16:47:38.001479: step 1295, loss 0.158127, acc 0.9
2016-09-07T16:47:38.679474: step 1296, loss 0.177713, acc 0.94
2016-09-07T16:47:39.370304: step 1297, loss 0.0327251, acc 0.98
2016-09-07T16:47:40.046785: step 1298, loss 0.0773606, acc 0.96
2016-09-07T16:47:40.713138: step 1299, loss 0.0684192, acc 0.98
2016-09-07T16:47:41.430645: step 1300, loss 0.196484, acc 0.92

Evaluation:
2016-09-07T16:47:44.537277: step 1300, loss 0.985253, acc 0.756

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-1300

2016-09-07T16:47:46.150381: step 1301, loss 0.0672071, acc 0.96
2016-09-07T16:47:46.815706: step 1302, loss 0.0270121, acc 1
2016-09-07T16:47:47.485914: step 1303, loss 0.130003, acc 0.94
2016-09-07T16:47:48.183030: step 1304, loss 0.0719037, acc 0.94
2016-09-07T16:47:48.872814: step 1305, loss 0.065792, acc 0.98
2016-09-07T16:47:49.550912: step 1306, loss 0.0909896, acc 0.94
2016-09-07T16:47:50.207333: step 1307, loss 0.138995, acc 0.92
2016-09-07T16:47:50.868580: step 1308, loss 0.0819047, acc 0.96
2016-09-07T16:47:51.548753: step 1309, loss 0.105046, acc 0.96
2016-09-07T16:47:52.216513: step 1310, loss 0.0697831, acc 0.96
2016-09-07T16:47:52.902323: step 1311, loss 0.255876, acc 0.92
2016-09-07T16:47:53.582740: step 1312, loss 0.0346078, acc 0.98
2016-09-07T16:47:54.255514: step 1313, loss 0.230343, acc 0.92
2016-09-07T16:47:54.927526: step 1314, loss 0.0518096, acc 0.98
2016-09-07T16:47:55.600281: step 1315, loss 0.170419, acc 0.92
2016-09-07T16:47:56.263443: step 1316, loss 0.0530452, acc 0.98
2016-09-07T16:47:56.935956: step 1317, loss 0.0694132, acc 0.96
2016-09-07T16:47:57.593602: step 1318, loss 0.1091, acc 0.94
2016-09-07T16:47:58.288861: step 1319, loss 0.0452577, acc 1
2016-09-07T16:47:58.958506: step 1320, loss 0.0516952, acc 0.98
2016-09-07T16:47:59.605600: step 1321, loss 0.0621117, acc 0.94
2016-09-07T16:48:00.280798: step 1322, loss 0.131289, acc 0.94
2016-09-07T16:48:00.945848: step 1323, loss 0.0969733, acc 0.94
2016-09-07T16:48:01.614852: step 1324, loss 0.0850052, acc 0.92
2016-09-07T16:48:02.291662: step 1325, loss 0.166773, acc 0.92
2016-09-07T16:48:02.959966: step 1326, loss 0.0509117, acc 1
2016-09-07T16:48:03.635043: step 1327, loss 0.0191301, acc 1
2016-09-07T16:48:04.318181: step 1328, loss 0.0454555, acc 0.98
2016-09-07T16:48:04.995866: step 1329, loss 0.0903631, acc 0.98
2016-09-07T16:48:05.666592: step 1330, loss 0.0880081, acc 0.96
2016-09-07T16:48:06.331997: step 1331, loss 0.078302, acc 0.96
2016-09-07T16:48:07.004727: step 1332, loss 0.0668356, acc 0.98
2016-09-07T16:48:07.689158: step 1333, loss 0.0399167, acc 0.98
2016-09-07T16:48:08.351092: step 1334, loss 0.0640361, acc 1
2016-09-07T16:48:09.048065: step 1335, loss 0.0655037, acc 0.96
2016-09-07T16:48:09.751825: step 1336, loss 0.160102, acc 0.98
2016-09-07T16:48:10.420727: step 1337, loss 0.152796, acc 0.94
2016-09-07T16:48:11.099002: step 1338, loss 0.0366663, acc 1
2016-09-07T16:48:11.765881: step 1339, loss 0.0983713, acc 0.96
2016-09-07T16:48:12.412464: step 1340, loss 0.0712994, acc 0.98
2016-09-07T16:48:13.100619: step 1341, loss 0.051308, acc 0.98
2016-09-07T16:48:13.760691: step 1342, loss 0.0494575, acc 0.96
2016-09-07T16:48:14.428757: step 1343, loss 0.0610142, acc 0.98
2016-09-07T16:48:15.125236: step 1344, loss 0.0334886, acc 1
2016-09-07T16:48:15.824854: step 1345, loss 0.0476319, acc 0.96
2016-09-07T16:48:16.495519: step 1346, loss 0.0600177, acc 0.98
2016-09-07T16:48:17.174124: step 1347, loss 0.0555577, acc 0.98
2016-09-07T16:48:17.847573: step 1348, loss 0.0786396, acc 0.98
2016-09-07T16:48:18.514610: step 1349, loss 0.138149, acc 0.94
2016-09-07T16:48:19.192931: step 1350, loss 0.0311969, acc 0.98
2016-09-07T16:48:19.863896: step 1351, loss 0.237708, acc 0.92
2016-09-07T16:48:20.535646: step 1352, loss 0.120688, acc 0.98
2016-09-07T16:48:21.199397: step 1353, loss 0.0136038, acc 1
2016-09-07T16:48:21.855580: step 1354, loss 0.191835, acc 0.92
2016-09-07T16:48:22.528099: step 1355, loss 0.0493992, acc 1
2016-09-07T16:48:23.196248: step 1356, loss 0.0250882, acc 1
2016-09-07T16:48:23.864987: step 1357, loss 0.0147238, acc 1
2016-09-07T16:48:24.223503: step 1358, loss 0.147968, acc 0.916667
2016-09-07T16:48:24.902819: step 1359, loss 0.042605, acc 0.98
2016-09-07T16:48:25.576554: step 1360, loss 0.0997042, acc 0.92
2016-09-07T16:48:26.241785: step 1361, loss 0.172312, acc 0.96
2016-09-07T16:48:26.908769: step 1362, loss 0.0293785, acc 1
2016-09-07T16:48:27.595409: step 1363, loss 0.0850255, acc 0.94
2016-09-07T16:48:28.253888: step 1364, loss 0.0844865, acc 0.94
2016-09-07T16:48:28.932984: step 1365, loss 0.0553472, acc 0.98
2016-09-07T16:48:29.587354: step 1366, loss 0.0249811, acc 1
2016-09-07T16:48:30.233871: step 1367, loss 0.0803262, acc 0.94
2016-09-07T16:48:30.899845: step 1368, loss 0.0486644, acc 0.98
2016-09-07T16:48:31.586514: step 1369, loss 0.0601278, acc 0.98
2016-09-07T16:48:32.260579: step 1370, loss 0.0215428, acc 0.98
2016-09-07T16:48:32.933560: step 1371, loss 0.180141, acc 0.86
2016-09-07T16:48:33.610587: step 1372, loss 0.100239, acc 0.96
2016-09-07T16:48:34.286086: step 1373, loss 0.0877701, acc 0.98
2016-09-07T16:48:34.975941: step 1374, loss 0.0598007, acc 0.96
2016-09-07T16:48:35.640410: step 1375, loss 0.0724489, acc 0.98
2016-09-07T16:48:36.304755: step 1376, loss 0.0232042, acc 1
2016-09-07T16:48:37.013864: step 1377, loss 0.0883636, acc 0.96
2016-09-07T16:48:37.753612: step 1378, loss 0.0489756, acc 0.98
2016-09-07T16:48:38.463773: step 1379, loss 0.225128, acc 0.92
2016-09-07T16:48:39.177016: step 1380, loss 0.06875, acc 0.96
2016-09-07T16:48:39.886037: step 1381, loss 0.021391, acc 1
2016-09-07T16:48:40.593329: step 1382, loss 0.0672438, acc 0.98
2016-09-07T16:48:41.276846: step 1383, loss 0.0223716, acc 1
2016-09-07T16:48:42.009395: step 1384, loss 0.0823393, acc 0.94
2016-09-07T16:48:42.713149: step 1385, loss 0.0332815, acc 0.98
2016-09-07T16:48:43.394437: step 1386, loss 0.119981, acc 0.92
2016-09-07T16:48:44.079036: step 1387, loss 0.132331, acc 0.92
2016-09-07T16:48:44.764478: step 1388, loss 0.0239239, acc 1
2016-09-07T16:48:45.420850: step 1389, loss 0.0551017, acc 0.98
2016-09-07T16:48:46.098330: step 1390, loss 0.075814, acc 1
2016-09-07T16:48:46.765764: step 1391, loss 0.0544876, acc 0.98
2016-09-07T16:48:47.429392: step 1392, loss 0.0956302, acc 0.96
2016-09-07T16:48:48.094299: step 1393, loss 0.0151105, acc 1
2016-09-07T16:48:48.774683: step 1394, loss 0.0593553, acc 0.98
2016-09-07T16:48:49.454805: step 1395, loss 0.0378128, acc 1
2016-09-07T16:48:50.128809: step 1396, loss 0.100494, acc 0.94
2016-09-07T16:48:50.796158: step 1397, loss 0.0203882, acc 1
2016-09-07T16:48:51.461277: step 1398, loss 0.09789, acc 0.96
2016-09-07T16:48:52.124706: step 1399, loss 0.0604243, acc 0.98
2016-09-07T16:48:52.788894: step 1400, loss 0.0575285, acc 0.96

Evaluation:
2016-09-07T16:48:55.869709: step 1400, loss 1.14664, acc 0.762

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-1400

2016-09-07T16:48:57.521446: step 1401, loss 0.137672, acc 0.96
2016-09-07T16:48:58.205419: step 1402, loss 0.0866476, acc 0.98
2016-09-07T16:48:58.882523: step 1403, loss 0.0515323, acc 0.98
2016-09-07T16:48:59.557776: step 1404, loss 0.0175522, acc 1
2016-09-07T16:49:00.272457: step 1405, loss 0.153436, acc 0.92
2016-09-07T16:49:00.930511: step 1406, loss 0.0818988, acc 0.94
2016-09-07T16:49:01.608312: step 1407, loss 0.0368984, acc 1
2016-09-07T16:49:02.268967: step 1408, loss 0.0763581, acc 0.96
2016-09-07T16:49:02.937445: step 1409, loss 0.0710376, acc 0.96
2016-09-07T16:49:03.619257: step 1410, loss 0.0254169, acc 0.98
2016-09-07T16:49:04.303827: step 1411, loss 0.0166576, acc 1
2016-09-07T16:49:05.010510: step 1412, loss 0.159709, acc 0.96
2016-09-07T16:49:05.672094: step 1413, loss 0.0120422, acc 1
2016-09-07T16:49:06.338231: step 1414, loss 0.0427405, acc 0.98
2016-09-07T16:49:07.039079: step 1415, loss 0.131197, acc 0.96
2016-09-07T16:49:07.744542: step 1416, loss 0.0168069, acc 1
2016-09-07T16:49:08.428330: step 1417, loss 0.0262913, acc 0.98
2016-09-07T16:49:09.116275: step 1418, loss 0.0400044, acc 0.98
2016-09-07T16:49:09.802092: step 1419, loss 0.0987857, acc 0.98
2016-09-07T16:49:10.468294: step 1420, loss 0.0683924, acc 0.96
2016-09-07T16:49:11.146167: step 1421, loss 0.126216, acc 0.94
2016-09-07T16:49:11.824385: step 1422, loss 0.155763, acc 0.92
2016-09-07T16:49:12.509836: step 1423, loss 0.0953627, acc 0.98
2016-09-07T16:49:13.185439: step 1424, loss 0.0729675, acc 0.98
2016-09-07T16:49:13.849362: step 1425, loss 0.0393038, acc 0.98
2016-09-07T16:49:14.517838: step 1426, loss 0.033674, acc 0.98
2016-09-07T16:49:15.190885: step 1427, loss 0.161896, acc 0.98
2016-09-07T16:49:15.887669: step 1428, loss 0.0840605, acc 0.94
2016-09-07T16:49:16.564417: step 1429, loss 0.162352, acc 0.94
2016-09-07T16:49:17.217349: step 1430, loss 0.0485791, acc 0.98
2016-09-07T16:49:17.881096: step 1431, loss 0.166413, acc 0.92
2016-09-07T16:49:18.552104: step 1432, loss 0.0486062, acc 0.96
2016-09-07T16:49:19.208526: step 1433, loss 0.0244052, acc 1
2016-09-07T16:49:19.885719: step 1434, loss 0.0731487, acc 0.98
2016-09-07T16:49:20.560448: step 1435, loss 0.116645, acc 0.96
2016-09-07T16:49:21.239306: step 1436, loss 0.0654717, acc 0.96
2016-09-07T16:49:21.921858: step 1437, loss 0.0910627, acc 0.98
2016-09-07T16:49:22.597729: step 1438, loss 0.0876769, acc 0.96
2016-09-07T16:49:23.263476: step 1439, loss 0.086314, acc 0.96
2016-09-07T16:49:23.918520: step 1440, loss 0.0611103, acc 1
2016-09-07T16:49:24.581494: step 1441, loss 0.0311535, acc 1
2016-09-07T16:49:25.238595: step 1442, loss 0.0450322, acc 0.98
2016-09-07T16:49:25.921328: step 1443, loss 0.0480528, acc 0.96
2016-09-07T16:49:26.587015: step 1444, loss 0.045674, acc 0.98
2016-09-07T16:49:27.263877: step 1445, loss 0.0639767, acc 0.96
2016-09-07T16:49:27.924116: step 1446, loss 0.0332523, acc 0.98
2016-09-07T16:49:28.576152: step 1447, loss 0.0293181, acc 1
2016-09-07T16:49:29.250536: step 1448, loss 0.281317, acc 0.92
2016-09-07T16:49:29.917076: step 1449, loss 0.273239, acc 0.92
2016-09-07T16:49:30.600795: step 1450, loss 0.152995, acc 0.9
2016-09-07T16:49:31.283177: step 1451, loss 0.0658043, acc 0.94
2016-09-07T16:49:31.944627: step 1452, loss 0.135919, acc 0.9
2016-09-07T16:49:32.618100: step 1453, loss 0.0804789, acc 0.98
2016-09-07T16:49:33.293018: step 1454, loss 0.0803619, acc 0.98
2016-09-07T16:49:33.975027: step 1455, loss 0.0320484, acc 1
2016-09-07T16:49:34.642982: step 1456, loss 0.103686, acc 0.96
2016-09-07T16:49:35.323382: step 1457, loss 0.145545, acc 0.92
2016-09-07T16:49:35.986403: step 1458, loss 0.0442827, acc 0.98
2016-09-07T16:49:36.674074: step 1459, loss 0.0463123, acc 1
2016-09-07T16:49:37.335946: step 1460, loss 0.0887723, acc 0.92
2016-09-07T16:49:38.029698: step 1461, loss 0.0361944, acc 1
2016-09-07T16:49:38.691537: step 1462, loss 0.0952315, acc 0.96
2016-09-07T16:49:39.374955: step 1463, loss 0.0661318, acc 0.96
2016-09-07T16:49:40.060795: step 1464, loss 0.170356, acc 0.94
2016-09-07T16:49:40.722000: step 1465, loss 0.073288, acc 0.96
2016-09-07T16:49:41.397750: step 1466, loss 0.0414703, acc 1
2016-09-07T16:49:42.072751: step 1467, loss 0.115353, acc 0.94
2016-09-07T16:49:42.772370: step 1468, loss 0.0773814, acc 0.98
2016-09-07T16:49:43.453086: step 1469, loss 0.111276, acc 0.9
2016-09-07T16:49:44.139896: step 1470, loss 0.00976149, acc 1
2016-09-07T16:49:44.812845: step 1471, loss 0.0697787, acc 0.96
2016-09-07T16:49:45.506192: step 1472, loss 0.138879, acc 0.96
2016-09-07T16:49:46.190878: step 1473, loss 0.0155522, acc 1
2016-09-07T16:49:46.877945: step 1474, loss 0.0891243, acc 0.98
2016-09-07T16:49:47.552779: step 1475, loss 0.0997062, acc 0.94
2016-09-07T16:49:48.224592: step 1476, loss 0.0733257, acc 0.96
2016-09-07T16:49:48.934226: step 1477, loss 0.0217781, acc 1
2016-09-07T16:49:49.608511: step 1478, loss 0.136616, acc 0.92
2016-09-07T16:49:50.295351: step 1479, loss 0.111991, acc 0.96
2016-09-07T16:49:50.978476: step 1480, loss 0.0572563, acc 0.96
2016-09-07T16:49:51.655399: step 1481, loss 0.111498, acc 0.94
2016-09-07T16:49:52.341828: step 1482, loss 0.0366303, acc 0.98
2016-09-07T16:49:53.014134: step 1483, loss 0.139251, acc 0.94
2016-09-07T16:49:53.694775: step 1484, loss 0.0826255, acc 0.94
2016-09-07T16:49:54.373489: step 1485, loss 0.120225, acc 0.94
2016-09-07T16:49:55.043844: step 1486, loss 0.0584467, acc 0.98
2016-09-07T16:49:55.724206: step 1487, loss 0.0312052, acc 1
2016-09-07T16:49:56.376627: step 1488, loss 0.0107006, acc 1
2016-09-07T16:49:57.053107: step 1489, loss 0.0984501, acc 0.96
2016-09-07T16:49:57.705087: step 1490, loss 0.071897, acc 0.96
2016-09-07T16:49:58.369723: step 1491, loss 0.0253289, acc 1
2016-09-07T16:49:59.039622: step 1492, loss 0.0668859, acc 0.96
2016-09-07T16:49:59.707847: step 1493, loss 0.0830838, acc 0.96
2016-09-07T16:50:00.393787: step 1494, loss 0.0530819, acc 0.98
2016-09-07T16:50:01.060231: step 1495, loss 0.0544233, acc 0.96
2016-09-07T16:50:01.730180: step 1496, loss 0.0314337, acc 1
2016-09-07T16:50:02.389340: step 1497, loss 0.0823382, acc 0.96
2016-09-07T16:50:03.061649: step 1498, loss 0.0270559, acc 0.98
2016-09-07T16:50:03.744156: step 1499, loss 0.220787, acc 0.92
2016-09-07T16:50:04.420813: step 1500, loss 0.0286327, acc 0.98

Evaluation:
2016-09-07T16:50:07.526407: step 1500, loss 1.1231, acc 0.767

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-1500

2016-09-07T16:50:09.186612: step 1501, loss 0.117995, acc 0.96
2016-09-07T16:50:09.862863: step 1502, loss 0.0126797, acc 1
2016-09-07T16:50:10.538457: step 1503, loss 0.0406466, acc 0.98
2016-09-07T16:50:11.186556: step 1504, loss 0.158546, acc 0.92
2016-09-07T16:50:11.849122: step 1505, loss 0.048638, acc 0.96
2016-09-07T16:50:12.508621: step 1506, loss 0.149507, acc 0.9
2016-09-07T16:50:13.170150: step 1507, loss 0.0538905, acc 0.98
2016-09-07T16:50:13.840653: step 1508, loss 0.00315432, acc 1
2016-09-07T16:50:14.511326: step 1509, loss 0.0497935, acc 0.96
2016-09-07T16:50:15.165364: step 1510, loss 0.0403045, acc 0.98
2016-09-07T16:50:15.833990: step 1511, loss 0.155678, acc 0.96
2016-09-07T16:50:16.504794: step 1512, loss 0.122842, acc 0.96
2016-09-07T16:50:17.197978: step 1513, loss 0.13541, acc 0.96
2016-09-07T16:50:17.889529: step 1514, loss 0.0237687, acc 0.98
2016-09-07T16:50:18.571325: step 1515, loss 0.0878977, acc 0.94
2016-09-07T16:50:19.243459: step 1516, loss 0.0944054, acc 0.98
2016-09-07T16:50:19.930795: step 1517, loss 0.0252992, acc 1
2016-09-07T16:50:20.611676: step 1518, loss 0.0524176, acc 0.96
2016-09-07T16:50:21.284291: step 1519, loss 0.0710972, acc 0.98
2016-09-07T16:50:21.952975: step 1520, loss 0.0564155, acc 0.94
2016-09-07T16:50:22.617335: step 1521, loss 0.0254748, acc 1
2016-09-07T16:50:23.289762: step 1522, loss 0.139235, acc 0.98
2016-09-07T16:50:23.980711: step 1523, loss 0.0395563, acc 1
2016-09-07T16:50:24.651471: step 1524, loss 0.174938, acc 0.92
2016-09-07T16:50:25.319438: step 1525, loss 0.0914312, acc 0.94
2016-09-07T16:50:25.995530: step 1526, loss 0.0221969, acc 1
2016-09-07T16:50:26.682477: step 1527, loss 0.256967, acc 0.92
2016-09-07T16:50:27.357392: step 1528, loss 0.0822203, acc 0.96
2016-09-07T16:50:28.043987: step 1529, loss 0.131265, acc 0.92
2016-09-07T16:50:28.719371: step 1530, loss 0.108176, acc 0.94
2016-09-07T16:50:29.387786: step 1531, loss 0.0704824, acc 0.96
2016-09-07T16:50:30.069126: step 1532, loss 0.116128, acc 0.92
2016-09-07T16:50:30.750206: step 1533, loss 0.0789417, acc 0.96
2016-09-07T16:50:31.459654: step 1534, loss 0.060778, acc 0.98
2016-09-07T16:50:32.135981: step 1535, loss 0.0491261, acc 1
2016-09-07T16:50:32.809340: step 1536, loss 0.0240245, acc 1
2016-09-07T16:50:33.506766: step 1537, loss 0.0342265, acc 1
2016-09-07T16:50:34.184711: step 1538, loss 0.102203, acc 0.96
2016-09-07T16:50:34.867157: step 1539, loss 0.102008, acc 0.94
2016-09-07T16:50:35.554559: step 1540, loss 0.0565487, acc 0.96
2016-09-07T16:50:36.227615: step 1541, loss 0.0573061, acc 1
2016-09-07T16:50:36.909529: step 1542, loss 0.0229509, acc 1
2016-09-07T16:50:37.571504: step 1543, loss 0.0482225, acc 0.96
2016-09-07T16:50:38.233875: step 1544, loss 0.0197552, acc 1
2016-09-07T16:50:38.920477: step 1545, loss 0.120342, acc 0.96
2016-09-07T16:50:39.609710: step 1546, loss 0.111196, acc 0.94
2016-09-07T16:50:40.282490: step 1547, loss 0.17789, acc 0.94
2016-09-07T16:50:40.952873: step 1548, loss 0.131563, acc 0.96
2016-09-07T16:50:41.639004: step 1549, loss 0.0180604, acc 1
2016-09-07T16:50:42.322027: step 1550, loss 0.0727142, acc 0.98
2016-09-07T16:50:42.991506: step 1551, loss 0.0736038, acc 0.96
2016-09-07T16:50:43.362353: step 1552, loss 0.00247312, acc 1
2016-09-07T16:50:44.044011: step 1553, loss 0.0467983, acc 1
2016-09-07T16:50:44.743468: step 1554, loss 0.0640219, acc 0.98
2016-09-07T16:50:45.461296: step 1555, loss 0.080351, acc 0.98
2016-09-07T16:50:46.133274: step 1556, loss 0.00435309, acc 1
2016-09-07T16:50:46.813855: step 1557, loss 0.0655377, acc 0.98
2016-09-07T16:50:47.490237: step 1558, loss 0.122421, acc 0.96
2016-09-07T16:50:48.162415: step 1559, loss 0.0300496, acc 1
2016-09-07T16:50:48.839536: step 1560, loss 0.0636421, acc 0.96
2016-09-07T16:50:49.541907: step 1561, loss 0.0220494, acc 1
2016-09-07T16:50:50.204339: step 1562, loss 0.0640757, acc 0.94
2016-09-07T16:50:50.898406: step 1563, loss 0.0489814, acc 0.96
2016-09-07T16:50:51.583573: step 1564, loss 0.077872, acc 0.96
2016-09-07T16:50:52.261507: step 1565, loss 0.137569, acc 0.94
2016-09-07T16:50:52.970466: step 1566, loss 0.0575996, acc 0.98
2016-09-07T16:50:53.664203: step 1567, loss 0.0354217, acc 0.96
2016-09-07T16:50:54.340340: step 1568, loss 0.0626697, acc 0.98
2016-09-07T16:50:55.004285: step 1569, loss 0.06249, acc 0.96
2016-09-07T16:50:55.662747: step 1570, loss 0.0270868, acc 0.98
2016-09-07T16:50:56.349567: step 1571, loss 0.0476826, acc 0.98
2016-09-07T16:50:57.015309: step 1572, loss 0.048634, acc 0.98
2016-09-07T16:50:57.709285: step 1573, loss 0.0408424, acc 0.98
2016-09-07T16:50:58.393529: step 1574, loss 0.0820898, acc 0.96
2016-09-07T16:50:59.067646: step 1575, loss 0.019346, acc 1
2016-09-07T16:50:59.736702: step 1576, loss 0.173169, acc 0.94
2016-09-07T16:51:00.417640: step 1577, loss 0.0360564, acc 0.98
2016-09-07T16:51:01.111265: step 1578, loss 0.00189564, acc 1
2016-09-07T16:51:01.779669: step 1579, loss 0.0282256, acc 0.98
2016-09-07T16:51:02.456956: step 1580, loss 0.157978, acc 0.96
2016-09-07T16:51:03.113209: step 1581, loss 0.0282209, acc 1
2016-09-07T16:51:03.764918: step 1582, loss 0.17937, acc 0.92
2016-09-07T16:51:04.436988: step 1583, loss 0.0394594, acc 0.98
2016-09-07T16:51:05.091541: step 1584, loss 0.0731794, acc 0.98
2016-09-07T16:51:05.753817: step 1585, loss 0.0769973, acc 0.98
2016-09-07T16:51:06.423553: step 1586, loss 0.0614278, acc 0.96
2016-09-07T16:51:07.087507: step 1587, loss 0.029785, acc 1
2016-09-07T16:51:07.765482: step 1588, loss 0.120625, acc 0.96
2016-09-07T16:51:08.432312: step 1589, loss 0.0495492, acc 0.98
2016-09-07T16:51:09.115405: step 1590, loss 0.0868344, acc 0.96
2016-09-07T16:51:09.819350: step 1591, loss 0.0734042, acc 0.94
2016-09-07T16:51:10.487774: step 1592, loss 0.0573143, acc 0.96
2016-09-07T16:51:11.197708: step 1593, loss 0.0744208, acc 0.96
2016-09-07T16:51:11.869558: step 1594, loss 0.0855414, acc 0.96
2016-09-07T16:51:12.621081: step 1595, loss 0.0523679, acc 0.98
2016-09-07T16:51:13.284123: step 1596, loss 0.0239818, acc 1
2016-09-07T16:51:13.984290: step 1597, loss 0.100338, acc 0.94
2016-09-07T16:51:14.670103: step 1598, loss 0.110912, acc 0.94
2016-09-07T16:51:15.358959: step 1599, loss 0.0252564, acc 1
2016-09-07T16:51:16.034608: step 1600, loss 0.0495035, acc 0.98

Evaluation:
2016-09-07T16:51:19.267908: step 1600, loss 1.18352, acc 0.762

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-1600

2016-09-07T16:51:20.895551: step 1601, loss 0.0120145, acc 1
2016-09-07T16:51:21.575899: step 1602, loss 0.0208227, acc 1
2016-09-07T16:51:22.255592: step 1603, loss 0.0349831, acc 1
2016-09-07T16:51:22.947661: step 1604, loss 0.102085, acc 0.92
2016-09-07T16:51:23.633202: step 1605, loss 0.0279422, acc 0.98
2016-09-07T16:51:24.315308: step 1606, loss 0.0873443, acc 0.96
2016-09-07T16:51:25.019859: step 1607, loss 0.0549567, acc 0.98
2016-09-07T16:51:25.818072: step 1608, loss 0.026325, acc 1
2016-09-07T16:51:26.480521: step 1609, loss 0.132684, acc 0.92
2016-09-07T16:51:27.151992: step 1610, loss 0.0128014, acc 1
2016-09-07T16:51:27.831477: step 1611, loss 0.0975762, acc 0.96
2016-09-07T16:51:28.522062: step 1612, loss 0.123069, acc 0.98
2016-09-07T16:51:29.194301: step 1613, loss 0.0394442, acc 0.98
2016-09-07T16:51:29.885436: step 1614, loss 0.0586927, acc 0.96
2016-09-07T16:51:30.561895: step 1615, loss 0.0363214, acc 0.98
2016-09-07T16:51:31.233314: step 1616, loss 0.0814723, acc 0.98
2016-09-07T16:51:31.901141: step 1617, loss 0.0388474, acc 0.98
2016-09-07T16:51:32.581253: step 1618, loss 0.146128, acc 0.96
2016-09-07T16:51:33.381023: step 1619, loss 0.140417, acc 0.92
2016-09-07T16:51:34.167165: step 1620, loss 0.0605802, acc 0.98
2016-09-07T16:51:34.852099: step 1621, loss 0.035835, acc 1
2016-09-07T16:51:35.521751: step 1622, loss 0.0825612, acc 0.94
2016-09-07T16:51:36.193736: step 1623, loss 0.0812102, acc 0.96
2016-09-07T16:51:36.872174: step 1624, loss 0.0337186, acc 0.98
2016-09-07T16:51:37.554253: step 1625, loss 0.02722, acc 0.98
2016-09-07T16:51:38.218416: step 1626, loss 0.0277956, acc 1
2016-09-07T16:51:38.897966: step 1627, loss 0.104992, acc 0.94
2016-09-07T16:51:39.596390: step 1628, loss 0.0474694, acc 1
2016-09-07T16:51:40.280703: step 1629, loss 0.0522986, acc 0.96
2016-09-07T16:51:40.959017: step 1630, loss 0.0995518, acc 0.96
2016-09-07T16:51:41.645021: step 1631, loss 0.0534674, acc 0.98
2016-09-07T16:51:42.322983: step 1632, loss 0.103332, acc 0.96
2016-09-07T16:51:43.001786: step 1633, loss 0.115321, acc 0.96
2016-09-07T16:51:43.670693: step 1634, loss 0.105182, acc 0.96
2016-09-07T16:51:44.352056: step 1635, loss 0.0456453, acc 0.96
2016-09-07T16:51:45.024509: step 1636, loss 0.0184691, acc 1
2016-09-07T16:51:45.690416: step 1637, loss 0.0287719, acc 0.98
2016-09-07T16:51:46.374404: step 1638, loss 0.0167027, acc 1
2016-09-07T16:51:47.058282: step 1639, loss 0.0404841, acc 1
2016-09-07T16:51:47.728918: step 1640, loss 0.0558164, acc 0.98
2016-09-07T16:51:48.396301: step 1641, loss 0.065821, acc 0.98
2016-09-07T16:51:49.097063: step 1642, loss 0.134182, acc 0.9
2016-09-07T16:51:49.766791: step 1643, loss 0.0918249, acc 0.94
2016-09-07T16:51:50.425216: step 1644, loss 0.0189693, acc 1
2016-09-07T16:51:51.093125: step 1645, loss 0.044101, acc 0.98
2016-09-07T16:51:51.762862: step 1646, loss 0.0964366, acc 0.96
2016-09-07T16:51:52.433091: step 1647, loss 0.04557, acc 0.96
2016-09-07T16:51:53.090725: step 1648, loss 0.0290286, acc 0.98
2016-09-07T16:51:53.743669: step 1649, loss 0.040149, acc 0.98
2016-09-07T16:51:54.408264: step 1650, loss 0.108285, acc 0.94
2016-09-07T16:51:55.061292: step 1651, loss 0.0560373, acc 0.96
2016-09-07T16:51:55.717957: step 1652, loss 0.02184, acc 1
2016-09-07T16:51:56.404188: step 1653, loss 0.00641153, acc 1
2016-09-07T16:51:57.083623: step 1654, loss 0.0268859, acc 1
2016-09-07T16:51:57.749927: step 1655, loss 0.140595, acc 0.96
2016-09-07T16:51:58.420919: step 1656, loss 0.0601927, acc 0.98
2016-09-07T16:51:59.091727: step 1657, loss 0.0221837, acc 0.98
2016-09-07T16:51:59.759735: step 1658, loss 0.16092, acc 0.94
2016-09-07T16:52:00.441773: step 1659, loss 0.0625216, acc 0.98
2016-09-07T16:52:01.130018: step 1660, loss 0.0231499, acc 1
2016-09-07T16:52:01.809207: step 1661, loss 0.07766, acc 0.94
2016-09-07T16:52:02.458233: step 1662, loss 0.00987042, acc 1
2016-09-07T16:52:03.122015: step 1663, loss 0.0677376, acc 0.96
2016-09-07T16:52:03.784915: step 1664, loss 0.00448349, acc 1
2016-09-07T16:52:04.438013: step 1665, loss 0.100883, acc 0.92
2016-09-07T16:52:05.109520: step 1666, loss 0.12313, acc 0.94
2016-09-07T16:52:05.793447: step 1667, loss 0.181653, acc 0.94
2016-09-07T16:52:06.464189: step 1668, loss 0.0611646, acc 0.96
2016-09-07T16:52:07.129005: step 1669, loss 0.0671084, acc 0.98
2016-09-07T16:52:07.804642: step 1670, loss 0.0584346, acc 0.98
2016-09-07T16:52:08.511832: step 1671, loss 0.0319704, acc 0.98
2016-09-07T16:52:09.190753: step 1672, loss 0.0667779, acc 0.98
2016-09-07T16:52:09.864384: step 1673, loss 0.173649, acc 0.96
2016-09-07T16:52:10.555653: step 1674, loss 0.109117, acc 0.94
2016-09-07T16:52:11.223183: step 1675, loss 0.0424016, acc 0.98
2016-09-07T16:52:11.891377: step 1676, loss 0.0882255, acc 0.96
2016-09-07T16:52:12.558510: step 1677, loss 0.0632291, acc 0.98
2016-09-07T16:52:13.247626: step 1678, loss 0.0551457, acc 0.98
2016-09-07T16:52:13.927923: step 1679, loss 0.0582508, acc 0.98
2016-09-07T16:52:14.613275: step 1680, loss 0.00441509, acc 1
2016-09-07T16:52:15.285642: step 1681, loss 0.141131, acc 0.98
2016-09-07T16:52:15.976546: step 1682, loss 0.0580716, acc 0.98
2016-09-07T16:52:16.652825: step 1683, loss 0.0528541, acc 0.98
2016-09-07T16:52:17.330909: step 1684, loss 0.111836, acc 0.96
2016-09-07T16:52:18.005839: step 1685, loss 0.109193, acc 0.92
2016-09-07T16:52:18.682889: step 1686, loss 0.0483784, acc 0.96
2016-09-07T16:52:19.361120: step 1687, loss 0.102443, acc 0.94
2016-09-07T16:52:20.034814: step 1688, loss 0.0205437, acc 0.98
2016-09-07T16:52:20.712866: step 1689, loss 0.113055, acc 0.98
2016-09-07T16:52:21.397537: step 1690, loss 0.0504117, acc 0.96
2016-09-07T16:52:22.059904: step 1691, loss 0.031298, acc 0.98
2016-09-07T16:52:22.739325: step 1692, loss 0.0691715, acc 0.94
2016-09-07T16:52:23.426247: step 1693, loss 0.174314, acc 0.96
2016-09-07T16:52:24.087983: step 1694, loss 0.110793, acc 0.96
2016-09-07T16:52:24.751731: step 1695, loss 0.0567799, acc 0.98
2016-09-07T16:52:25.412515: step 1696, loss 0.0563323, acc 0.98
2016-09-07T16:52:26.087109: step 1697, loss 0.0564683, acc 0.98
2016-09-07T16:52:26.751758: step 1698, loss 0.0700378, acc 0.98
2016-09-07T16:52:27.408200: step 1699, loss 0.106661, acc 0.96
2016-09-07T16:52:28.099420: step 1700, loss 0.0485879, acc 1

Evaluation:
2016-09-07T16:52:31.329163: step 1700, loss 1.21206, acc 0.763

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-1700

2016-09-07T16:52:33.110798: step 1701, loss 0.0675775, acc 0.94
2016-09-07T16:52:33.779034: step 1702, loss 0.0288102, acc 0.98
2016-09-07T16:52:34.460305: step 1703, loss 0.0515594, acc 0.98
2016-09-07T16:52:35.117194: step 1704, loss 0.0936418, acc 0.94
2016-09-07T16:52:35.779773: step 1705, loss 0.0548552, acc 0.98
2016-09-07T16:52:36.442210: step 1706, loss 0.203442, acc 0.92
2016-09-07T16:52:37.111634: step 1707, loss 0.105362, acc 0.92
2016-09-07T16:52:37.803688: step 1708, loss 0.0185959, acc 1
2016-09-07T16:52:38.473286: step 1709, loss 0.109351, acc 0.98
2016-09-07T16:52:39.165138: step 1710, loss 0.212414, acc 0.94
2016-09-07T16:52:39.822221: step 1711, loss 0.0842238, acc 0.96
2016-09-07T16:52:40.503208: step 1712, loss 0.0634066, acc 0.96
2016-09-07T16:52:41.177084: step 1713, loss 0.0448771, acc 0.98
2016-09-07T16:52:41.847877: step 1714, loss 0.0215173, acc 1
2016-09-07T16:52:42.528074: step 1715, loss 0.0582016, acc 0.96
2016-09-07T16:52:43.218019: step 1716, loss 0.0183137, acc 1
2016-09-07T16:52:43.882877: step 1717, loss 0.0410356, acc 0.98
2016-09-07T16:52:44.558339: step 1718, loss 0.105314, acc 0.94
2016-09-07T16:52:45.252461: step 1719, loss 0.0506061, acc 0.98
2016-09-07T16:52:45.936075: step 1720, loss 0.0870924, acc 0.94
2016-09-07T16:52:46.606264: step 1721, loss 0.186976, acc 0.96
2016-09-07T16:52:47.300581: step 1722, loss 0.0860034, acc 0.96
2016-09-07T16:52:47.966082: step 1723, loss 0.0985713, acc 0.96
2016-09-07T16:52:48.641149: step 1724, loss 0.13567, acc 0.94
2016-09-07T16:52:49.327309: step 1725, loss 0.107354, acc 0.94
2016-09-07T16:52:50.005103: step 1726, loss 0.0350224, acc 0.98
2016-09-07T16:52:50.716133: step 1727, loss 0.0500634, acc 0.98
2016-09-07T16:52:51.450671: step 1728, loss 0.0459033, acc 1
2016-09-07T16:52:52.133832: step 1729, loss 0.0808422, acc 0.94
2016-09-07T16:52:52.816441: step 1730, loss 0.0538914, acc 0.98
2016-09-07T16:52:53.494427: step 1731, loss 0.0286733, acc 1
2016-09-07T16:52:54.248399: step 1732, loss 0.0342246, acc 0.98
2016-09-07T16:52:54.906506: step 1733, loss 0.0706056, acc 0.96
2016-09-07T16:52:55.630676: step 1734, loss 0.0293113, acc 1
2016-09-07T16:52:56.301506: step 1735, loss 0.0227883, acc 1
2016-09-07T16:52:56.973158: step 1736, loss 0.04642, acc 0.98
2016-09-07T16:52:57.631536: step 1737, loss 0.0913227, acc 0.94
2016-09-07T16:52:58.290652: step 1738, loss 0.0422201, acc 0.98
2016-09-07T16:52:58.943931: step 1739, loss 0.125885, acc 0.96
2016-09-07T16:52:59.609938: step 1740, loss 0.049577, acc 0.98
2016-09-07T16:53:00.342549: step 1741, loss 0.0101355, acc 1
2016-09-07T16:53:01.016131: step 1742, loss 0.0907226, acc 0.98
2016-09-07T16:53:01.695020: step 1743, loss 0.215934, acc 0.92
2016-09-07T16:53:02.364311: step 1744, loss 0.0649784, acc 0.96
2016-09-07T16:53:03.044683: step 1745, loss 0.0606871, acc 0.96
2016-09-07T16:53:03.390792: step 1746, loss 0.00313425, acc 1
2016-09-07T16:53:04.094336: step 1747, loss 0.0780305, acc 0.98
2016-09-07T16:53:04.769824: step 1748, loss 0.154337, acc 0.98
2016-09-07T16:53:05.431700: step 1749, loss 0.131805, acc 0.96
2016-09-07T16:53:06.102085: step 1750, loss 0.110544, acc 0.98
2016-09-07T16:53:06.778631: step 1751, loss 0.0453995, acc 0.98
2016-09-07T16:53:07.453705: step 1752, loss 0.0209601, acc 1
2016-09-07T16:53:08.134371: step 1753, loss 0.0670474, acc 0.94
2016-09-07T16:53:08.799731: step 1754, loss 0.108253, acc 0.94
2016-09-07T16:53:09.472500: step 1755, loss 0.0219751, acc 1
2016-09-07T16:53:10.137290: step 1756, loss 0.0168524, acc 1
2016-09-07T16:53:10.807886: step 1757, loss 0.0226239, acc 1
2016-09-07T16:53:11.462269: step 1758, loss 0.113331, acc 0.96
2016-09-07T16:53:12.127505: step 1759, loss 0.0284886, acc 1
2016-09-07T16:53:12.808212: step 1760, loss 0.0420216, acc 0.98
2016-09-07T16:53:13.488397: step 1761, loss 0.00600513, acc 1
2016-09-07T16:53:14.162428: step 1762, loss 0.0638421, acc 0.96
2016-09-07T16:53:14.850856: step 1763, loss 0.0317652, acc 0.98
2016-09-07T16:53:15.527465: step 1764, loss 0.166162, acc 0.92
2016-09-07T16:53:16.188925: step 1765, loss 0.0652575, acc 0.96
2016-09-07T16:53:16.861119: step 1766, loss 0.0277703, acc 0.98
2016-09-07T16:53:17.538097: step 1767, loss 0.0717513, acc 0.96
2016-09-07T16:53:18.213982: step 1768, loss 0.0873282, acc 0.96
2016-09-07T16:53:18.886124: step 1769, loss 0.016438, acc 1
2016-09-07T16:53:19.551482: step 1770, loss 0.0703663, acc 0.98
2016-09-07T16:53:20.245055: step 1771, loss 0.0421017, acc 0.96
2016-09-07T16:53:20.934202: step 1772, loss 0.00272363, acc 1
2016-09-07T16:53:21.624551: step 1773, loss 0.0146362, acc 1
2016-09-07T16:53:22.309624: step 1774, loss 0.0747917, acc 0.98
2016-09-07T16:53:22.968953: step 1775, loss 0.0461283, acc 0.98
2016-09-07T16:53:23.633332: step 1776, loss 0.0109798, acc 1
2016-09-07T16:53:24.317079: step 1777, loss 0.0406742, acc 0.96
2016-09-07T16:53:24.998963: step 1778, loss 0.0748386, acc 0.96
2016-09-07T16:53:25.668796: step 1779, loss 0.0271171, acc 0.98
2016-09-07T16:53:26.343595: step 1780, loss 0.0394327, acc 0.96
2016-09-07T16:53:26.998626: step 1781, loss 0.0885551, acc 0.96
2016-09-07T16:53:27.653734: step 1782, loss 0.0474834, acc 1
2016-09-07T16:53:28.322538: step 1783, loss 0.0288465, acc 0.98
2016-09-07T16:53:28.989823: step 1784, loss 0.114181, acc 0.94
2016-09-07T16:53:29.651580: step 1785, loss 0.0112201, acc 1
2016-09-07T16:53:30.313490: step 1786, loss 0.067962, acc 0.98
2016-09-07T16:53:31.007710: step 1787, loss 0.05957, acc 0.98
2016-09-07T16:53:31.667870: step 1788, loss 0.0206024, acc 0.98
2016-09-07T16:53:32.355522: step 1789, loss 0.0251543, acc 1
2016-09-07T16:53:33.054650: step 1790, loss 0.00597907, acc 1
2016-09-07T16:53:33.728722: step 1791, loss 0.0352254, acc 0.98
2016-09-07T16:53:34.402023: step 1792, loss 0.136404, acc 0.96
2016-09-07T16:53:35.058897: step 1793, loss 0.0910998, acc 0.92
2016-09-07T16:53:35.746264: step 1794, loss 0.0712589, acc 0.96
2016-09-07T16:53:36.436725: step 1795, loss 0.01017, acc 1
2016-09-07T16:53:37.117947: step 1796, loss 0.0128364, acc 1
2016-09-07T16:53:37.784694: step 1797, loss 0.0217164, acc 0.98
2016-09-07T16:53:38.454275: step 1798, loss 0.0642656, acc 0.96
2016-09-07T16:53:39.122981: step 1799, loss 0.04252, acc 0.96
2016-09-07T16:53:39.801083: step 1800, loss 0.0163455, acc 1

Evaluation:
2016-09-07T16:53:43.031113: step 1800, loss 1.33723, acc 0.759

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-1800

2016-09-07T16:53:44.856633: step 1801, loss 0.0511134, acc 0.98
2016-09-07T16:53:45.527439: step 1802, loss 0.109761, acc 0.96
2016-09-07T16:53:46.196317: step 1803, loss 0.0496804, acc 0.98
2016-09-07T16:53:46.872916: step 1804, loss 0.0232392, acc 1
2016-09-07T16:53:47.535564: step 1805, loss 0.0394571, acc 0.98
2016-09-07T16:53:48.209389: step 1806, loss 0.0253719, acc 0.98
2016-09-07T16:53:48.876295: step 1807, loss 0.0611691, acc 0.96
2016-09-07T16:53:49.541678: step 1808, loss 0.0338394, acc 0.98
2016-09-07T16:53:50.199538: step 1809, loss 0.244858, acc 0.94
2016-09-07T16:53:50.859033: step 1810, loss 0.0258758, acc 1
2016-09-07T16:53:51.526473: step 1811, loss 0.0354472, acc 1
2016-09-07T16:53:52.200864: step 1812, loss 0.0854493, acc 0.98
2016-09-07T16:53:52.886836: step 1813, loss 0.00663851, acc 1
2016-09-07T16:53:53.553314: step 1814, loss 0.0428191, acc 0.98
2016-09-07T16:53:54.221050: step 1815, loss 0.0310151, acc 1
2016-09-07T16:53:54.897830: step 1816, loss 0.0115803, acc 1
2016-09-07T16:53:55.561684: step 1817, loss 0.0315251, acc 0.98
2016-09-07T16:53:56.239850: step 1818, loss 0.0231311, acc 0.98
2016-09-07T16:53:56.911407: step 1819, loss 0.0975502, acc 0.96
2016-09-07T16:53:57.568043: step 1820, loss 0.00829988, acc 1
2016-09-07T16:53:58.251077: step 1821, loss 0.0197334, acc 0.98
2016-09-07T16:53:59.021823: step 1822, loss 0.0413552, acc 0.96
2016-09-07T16:53:59.734691: step 1823, loss 0.0963332, acc 0.96
2016-09-07T16:54:00.423343: step 1824, loss 0.0182353, acc 1
2016-09-07T16:54:01.108055: step 1825, loss 0.117534, acc 0.98
2016-09-07T16:54:01.774096: step 1826, loss 0.048355, acc 0.98
2016-09-07T16:54:02.538888: step 1827, loss 0.10642, acc 0.96
2016-09-07T16:54:03.194950: step 1828, loss 0.0187459, acc 1
2016-09-07T16:54:03.885700: step 1829, loss 0.087592, acc 0.98
2016-09-07T16:54:04.549754: step 1830, loss 0.0233908, acc 1
2016-09-07T16:54:05.273920: step 1831, loss 0.0518646, acc 0.98
2016-09-07T16:54:05.960615: step 1832, loss 0.0400597, acc 0.98
2016-09-07T16:54:06.616840: step 1833, loss 0.0657024, acc 0.94
2016-09-07T16:54:07.271384: step 1834, loss 0.0385131, acc 0.98
2016-09-07T16:54:07.937984: step 1835, loss 0.0607164, acc 0.96
2016-09-07T16:54:08.605720: step 1836, loss 0.00815578, acc 1
2016-09-07T16:54:09.290579: step 1837, loss 0.0565315, acc 0.98
2016-09-07T16:54:09.969415: step 1838, loss 0.0310949, acc 0.98
2016-09-07T16:54:10.643243: step 1839, loss 0.0806904, acc 0.98
2016-09-07T16:54:11.392550: step 1840, loss 0.0123453, acc 1
2016-09-07T16:54:12.078761: step 1841, loss 0.0691483, acc 0.98
2016-09-07T16:54:12.771999: step 1842, loss 0.0300145, acc 0.98
2016-09-07T16:54:13.455986: step 1843, loss 0.0541063, acc 0.98
2016-09-07T16:54:14.146260: step 1844, loss 0.0881314, acc 0.96
2016-09-07T16:54:14.923477: step 1845, loss 0.00779699, acc 1
2016-09-07T16:54:15.635481: step 1846, loss 0.0619433, acc 0.96
2016-09-07T16:54:16.295852: step 1847, loss 0.0264438, acc 0.98
2016-09-07T16:54:16.979052: step 1848, loss 0.0209476, acc 0.98
2016-09-07T16:54:17.677080: step 1849, loss 0.0664008, acc 0.98
2016-09-07T16:54:18.350976: step 1850, loss 0.00640801, acc 1
2016-09-07T16:54:19.026385: step 1851, loss 0.0320194, acc 0.98
2016-09-07T16:54:19.703809: step 1852, loss 0.0496696, acc 0.98
2016-09-07T16:54:20.398967: step 1853, loss 0.108146, acc 0.94
2016-09-07T16:54:21.081655: step 1854, loss 0.112196, acc 0.98
2016-09-07T16:54:21.753843: step 1855, loss 0.0736515, acc 0.96
2016-09-07T16:54:22.431180: step 1856, loss 0.0649256, acc 0.96
2016-09-07T16:54:23.109815: step 1857, loss 0.216849, acc 0.92
2016-09-07T16:54:23.791632: step 1858, loss 0.011875, acc 1
2016-09-07T16:54:24.474826: step 1859, loss 0.0184423, acc 1
2016-09-07T16:54:25.162927: step 1860, loss 0.0983908, acc 0.96
2016-09-07T16:54:25.845339: step 1861, loss 0.0625551, acc 0.96
2016-09-07T16:54:26.540357: step 1862, loss 0.0818738, acc 0.96
2016-09-07T16:54:27.230445: step 1863, loss 0.130814, acc 0.96
2016-09-07T16:54:27.904420: step 1864, loss 0.0345634, acc 0.98
2016-09-07T16:54:28.588757: step 1865, loss 0.0375625, acc 0.98
2016-09-07T16:54:29.287142: step 1866, loss 0.084522, acc 0.96
2016-09-07T16:54:29.957699: step 1867, loss 0.0585008, acc 0.96
2016-09-07T16:54:30.618019: step 1868, loss 0.0880356, acc 0.96
2016-09-07T16:54:31.309400: step 1869, loss 0.0122817, acc 1
2016-09-07T16:54:32.025718: step 1870, loss 0.0352873, acc 0.98
2016-09-07T16:54:32.698425: step 1871, loss 0.168626, acc 0.96
2016-09-07T16:54:33.382334: step 1872, loss 0.026918, acc 0.98
2016-09-07T16:54:34.061864: step 1873, loss 0.145164, acc 0.98
2016-09-07T16:54:34.962454: step 1874, loss 0.0251714, acc 0.98
2016-09-07T16:54:35.771387: step 1875, loss 0.0514115, acc 0.96
2016-09-07T16:54:36.580561: step 1876, loss 0.0202412, acc 1
2016-09-07T16:54:37.349497: step 1877, loss 0.0428804, acc 0.98
2016-09-07T16:54:38.090846: step 1878, loss 0.056735, acc 0.98
2016-09-07T16:54:38.862488: step 1879, loss 0.051429, acc 0.98
2016-09-07T16:54:39.541383: step 1880, loss 0.0373833, acc 0.96
2016-09-07T16:54:40.245055: step 1881, loss 0.0402381, acc 0.98
2016-09-07T16:54:40.946179: step 1882, loss 0.0658828, acc 0.96
2016-09-07T16:54:41.601653: step 1883, loss 0.0966131, acc 0.94
2016-09-07T16:54:42.299739: step 1884, loss 0.0443504, acc 0.98
2016-09-07T16:54:43.046814: step 1885, loss 0.0495978, acc 0.98
2016-09-07T16:54:43.866102: step 1886, loss 0.0382265, acc 1
2016-09-07T16:54:44.548054: step 1887, loss 0.0412012, acc 0.98
2016-09-07T16:54:45.242535: step 1888, loss 0.0783187, acc 0.96
2016-09-07T16:54:45.914035: step 1889, loss 0.042249, acc 0.98
2016-09-07T16:54:46.600157: step 1890, loss 0.0387395, acc 0.98
2016-09-07T16:54:47.322421: step 1891, loss 0.134777, acc 0.96
2016-09-07T16:54:47.992725: step 1892, loss 0.0743939, acc 0.96
2016-09-07T16:54:48.674917: step 1893, loss 0.025222, acc 0.98
2016-09-07T16:54:49.336580: step 1894, loss 0.0404597, acc 0.98
2016-09-07T16:54:50.002785: step 1895, loss 0.102411, acc 0.94
2016-09-07T16:54:50.698915: step 1896, loss 0.0577599, acc 0.98
2016-09-07T16:54:51.380588: step 1897, loss 0.116297, acc 0.98
2016-09-07T16:54:52.050647: step 1898, loss 0.0523203, acc 0.98
2016-09-07T16:54:52.751824: step 1899, loss 0.140196, acc 0.94
2016-09-07T16:54:53.421698: step 1900, loss 0.125926, acc 0.98

Evaluation:
2016-09-07T16:54:56.937467: step 1900, loss 1.24858, acc 0.763

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-1900

2016-09-07T16:54:59.219434: step 1901, loss 0.039346, acc 1
2016-09-07T16:55:00.263678: step 1902, loss 0.0200984, acc 1
2016-09-07T16:55:01.302802: step 1903, loss 0.044534, acc 0.98
2016-09-07T16:55:02.294966: step 1904, loss 0.0686794, acc 0.98
2016-09-07T16:55:03.126480: step 1905, loss 0.00982799, acc 1
2016-09-07T16:55:04.117266: step 1906, loss 0.0415553, acc 1
2016-09-07T16:55:05.098830: step 1907, loss 0.0345143, acc 1
2016-09-07T16:55:06.166043: step 1908, loss 0.0130601, acc 1
2016-09-07T16:55:07.138676: step 1909, loss 0.0221674, acc 0.98
2016-09-07T16:55:08.193815: step 1910, loss 0.0416864, acc 0.98
2016-09-07T16:55:09.225728: step 1911, loss 0.0307212, acc 0.98
2016-09-07T16:55:10.314931: step 1912, loss 0.0958663, acc 0.96
2016-09-07T16:55:11.287082: step 1913, loss 0.00874009, acc 1
2016-09-07T16:55:12.257728: step 1914, loss 0.145139, acc 0.94
2016-09-07T16:55:13.205914: step 1915, loss 0.177916, acc 0.92
2016-09-07T16:55:14.107752: step 1916, loss 0.179157, acc 0.92
2016-09-07T16:55:14.928228: step 1917, loss 0.052043, acc 0.96
2016-09-07T16:55:15.992774: step 1918, loss 0.0682187, acc 0.98
2016-09-07T16:55:17.106223: step 1919, loss 0.0318063, acc 0.98
2016-09-07T16:55:18.169448: step 1920, loss 0.0184935, acc 1
2016-09-07T16:55:19.055301: step 1921, loss 0.0716346, acc 0.98
2016-09-07T16:55:20.081159: step 1922, loss 0.0645115, acc 0.98
2016-09-07T16:55:21.110966: step 1923, loss 0.0548198, acc 0.96
2016-09-07T16:55:22.036152: step 1924, loss 0.0306002, acc 0.98
2016-09-07T16:55:22.943984: step 1925, loss 0.0788464, acc 0.96
2016-09-07T16:55:23.794965: step 1926, loss 0.0713221, acc 0.98
2016-09-07T16:55:24.635914: step 1927, loss 0.137731, acc 0.88
2016-09-07T16:55:25.601165: step 1928, loss 0.111657, acc 0.98
2016-09-07T16:55:26.604717: step 1929, loss 0.0211061, acc 1
2016-09-07T16:55:27.676807: step 1930, loss 0.0989087, acc 0.96
2016-09-07T16:55:28.641410: step 1931, loss 0.00806865, acc 1
2016-09-07T16:55:29.475500: step 1932, loss 0.0441834, acc 0.98
2016-09-07T16:55:30.450648: step 1933, loss 0.0431944, acc 1
2016-09-07T16:55:31.397185: step 1934, loss 0.0324547, acc 0.98
2016-09-07T16:55:32.448622: step 1935, loss 0.0806927, acc 0.98
2016-09-07T16:55:33.367400: step 1936, loss 0.0916663, acc 0.94
2016-09-07T16:55:34.389315: step 1937, loss 0.0371489, acc 0.98
2016-09-07T16:55:35.361588: step 1938, loss 0.0182544, acc 1
2016-09-07T16:55:36.263544: step 1939, loss 0.0715224, acc 0.96
2016-09-07T16:55:36.701360: step 1940, loss 0.282958, acc 0.833333
2016-09-07T16:55:37.529292: step 1941, loss 0.0480176, acc 0.96
2016-09-07T16:55:38.354848: step 1942, loss 0.0454606, acc 0.98
2016-09-07T16:55:39.186828: step 1943, loss 0.0552568, acc 0.96
2016-09-07T16:55:40.000791: step 1944, loss 0.10136, acc 0.94
2016-09-07T16:55:40.841116: step 1945, loss 0.149291, acc 0.96
2016-09-07T16:55:41.644488: step 1946, loss 0.0569408, acc 0.98
2016-09-07T16:55:42.490375: step 1947, loss 0.122892, acc 0.92
2016-09-07T16:55:43.426271: step 1948, loss 0.0242751, acc 0.98
2016-09-07T16:55:44.300694: step 1949, loss 0.038985, acc 0.98
2016-09-07T16:55:45.159218: step 1950, loss 0.0403538, acc 0.98
2016-09-07T16:55:46.041964: step 1951, loss 0.0149636, acc 1
2016-09-07T16:55:46.874996: step 1952, loss 0.0464444, acc 0.96
2016-09-07T16:55:47.782900: step 1953, loss 0.0424488, acc 1
2016-09-07T16:55:48.627311: step 1954, loss 0.153733, acc 0.92
2016-09-07T16:55:49.465973: step 1955, loss 0.176457, acc 0.92
2016-09-07T16:55:50.335863: step 1956, loss 0.0231217, acc 0.98
2016-09-07T16:55:51.176221: step 1957, loss 0.0849035, acc 0.96
2016-09-07T16:55:52.003542: step 1958, loss 0.114703, acc 0.96
2016-09-07T16:55:52.827053: step 1959, loss 0.156879, acc 0.9
2016-09-07T16:55:53.655174: step 1960, loss 0.0372663, acc 0.98
2016-09-07T16:55:54.471575: step 1961, loss 0.0168027, acc 1
2016-09-07T16:55:55.312690: step 1962, loss 0.0457929, acc 0.98
2016-09-07T16:55:56.151039: step 1963, loss 0.0397733, acc 1
2016-09-07T16:55:56.940096: step 1964, loss 0.00443033, acc 1
2016-09-07T16:55:57.717083: step 1965, loss 0.024901, acc 1
2016-09-07T16:55:58.500836: step 1966, loss 0.0369974, acc 0.98
2016-09-07T16:55:59.308772: step 1967, loss 0.00736046, acc 1
2016-09-07T16:56:00.118916: step 1968, loss 0.0866494, acc 0.94
2016-09-07T16:56:00.952928: step 1969, loss 0.0586608, acc 0.98
2016-09-07T16:56:01.781239: step 1970, loss 0.0296538, acc 1
2016-09-07T16:56:02.588501: step 1971, loss 0.0160885, acc 1
2016-09-07T16:56:03.416809: step 1972, loss 0.139632, acc 0.94
2016-09-07T16:56:04.248692: step 1973, loss 0.0691335, acc 0.96
2016-09-07T16:56:05.075119: step 1974, loss 0.0141238, acc 1
2016-09-07T16:56:05.895219: step 1975, loss 0.116317, acc 0.96
2016-09-07T16:56:06.722203: step 1976, loss 0.128412, acc 0.96
2016-09-07T16:56:07.534358: step 1977, loss 0.0692172, acc 0.98
2016-09-07T16:56:08.372251: step 1978, loss 0.05891, acc 0.94
2016-09-07T16:56:09.171940: step 1979, loss 0.0592756, acc 0.96
2016-09-07T16:56:10.288415: step 1980, loss 0.0535559, acc 0.96
2016-09-07T16:56:11.364236: step 1981, loss 0.117994, acc 0.96
2016-09-07T16:56:12.418649: step 1982, loss 0.0663853, acc 0.96
2016-09-07T16:56:13.247735: step 1983, loss 0.00387628, acc 1
2016-09-07T16:56:14.137898: step 1984, loss 0.0121923, acc 1
2016-09-07T16:56:15.111795: step 1985, loss 0.0491367, acc 0.98
2016-09-07T16:56:16.232029: step 1986, loss 0.0931125, acc 0.94
2016-09-07T16:56:17.312988: step 1987, loss 0.0673408, acc 0.96
2016-09-07T16:56:18.259449: step 1988, loss 0.00602002, acc 1
2016-09-07T16:56:19.173061: step 1989, loss 0.155242, acc 0.96
2016-09-07T16:56:20.018330: step 1990, loss 0.044219, acc 0.98
2016-09-07T16:56:20.952640: step 1991, loss 0.0382663, acc 0.96
2016-09-07T16:56:21.835575: step 1992, loss 0.0573908, acc 0.96
2016-09-07T16:56:22.807830: step 1993, loss 0.0244514, acc 1
2016-09-07T16:56:23.672711: step 1994, loss 0.0510169, acc 0.98
2016-09-07T16:56:24.526445: step 1995, loss 0.0641407, acc 0.96
2016-09-07T16:56:25.437180: step 1996, loss 0.113155, acc 0.96
2016-09-07T16:56:26.267830: step 1997, loss 0.0966508, acc 0.94
2016-09-07T16:56:27.202519: step 1998, loss 0.00521486, acc 1
2016-09-07T16:56:28.052939: step 1999, loss 0.111608, acc 0.96
2016-09-07T16:56:29.182460: step 2000, loss 0.0547109, acc 0.98

Evaluation:
2016-09-07T16:56:34.094087: step 2000, loss 1.22178, acc 0.741

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-2000

2016-09-07T16:56:35.923060: step 2001, loss 0.0817644, acc 0.98
2016-09-07T16:56:36.661992: step 2002, loss 0.0452505, acc 0.98
2016-09-07T16:56:37.454635: step 2003, loss 0.0861861, acc 0.98
2016-09-07T16:56:38.301152: step 2004, loss 0.0633526, acc 0.96
2016-09-07T16:56:39.115326: step 2005, loss 0.0297046, acc 1
2016-09-07T16:56:39.954724: step 2006, loss 0.0687341, acc 1
2016-09-07T16:56:40.775116: step 2007, loss 0.0473902, acc 0.98
2016-09-07T16:56:41.603002: step 2008, loss 0.0606562, acc 0.98
2016-09-07T16:56:42.492328: step 2009, loss 0.0396617, acc 0.98
2016-09-07T16:56:43.480225: step 2010, loss 0.0843952, acc 0.96
2016-09-07T16:56:44.323075: step 2011, loss 0.019444, acc 1
2016-09-07T16:56:45.153297: step 2012, loss 0.036355, acc 1
2016-09-07T16:56:45.980636: step 2013, loss 0.00705565, acc 1
2016-09-07T16:56:46.850171: step 2014, loss 0.0306123, acc 0.98
2016-09-07T16:56:47.833631: step 2015, loss 0.0620067, acc 0.98
2016-09-07T16:56:48.710869: step 2016, loss 0.0406855, acc 0.96
2016-09-07T16:56:49.755990: step 2017, loss 0.0257241, acc 1
2016-09-07T16:56:51.015291: step 2018, loss 0.0224593, acc 0.98
2016-09-07T16:56:52.108075: step 2019, loss 0.00720217, acc 1
2016-09-07T16:56:53.195490: step 2020, loss 0.169627, acc 0.94
2016-09-07T16:56:54.058284: step 2021, loss 0.0789741, acc 0.98
2016-09-07T16:56:54.925273: step 2022, loss 0.00963499, acc 1
2016-09-07T16:56:55.752885: step 2023, loss 0.0739968, acc 0.94
2016-09-07T16:56:56.640911: step 2024, loss 0.0657017, acc 0.98
2016-09-07T16:56:57.463460: step 2025, loss 0.00863926, acc 1
2016-09-07T16:56:58.436412: step 2026, loss 0.0576868, acc 0.98
2016-09-07T16:56:59.443336: step 2027, loss 0.0139583, acc 1
2016-09-07T16:57:00.409074: step 2028, loss 0.164062, acc 0.96
2016-09-07T16:57:01.448957: step 2029, loss 0.0103814, acc 1
2016-09-07T16:57:02.469749: step 2030, loss 0.10693, acc 0.92
2016-09-07T16:57:03.430661: step 2031, loss 0.151745, acc 0.94
2016-09-07T16:57:04.327850: step 2032, loss 0.0277335, acc 1
2016-09-07T16:57:05.309902: step 2033, loss 0.0506042, acc 0.98
2016-09-07T16:57:06.175039: step 2034, loss 0.064263, acc 0.98
2016-09-07T16:57:06.989888: step 2035, loss 0.0569331, acc 0.96
2016-09-07T16:57:07.815203: step 2036, loss 0.0397628, acc 0.98
2016-09-07T16:57:08.799648: step 2037, loss 0.0456041, acc 0.96
2016-09-07T16:57:09.959581: step 2038, loss 0.0374713, acc 1
2016-09-07T16:57:11.009149: step 2039, loss 0.0845418, acc 0.96
2016-09-07T16:57:11.906006: step 2040, loss 0.141009, acc 0.94
2016-09-07T16:57:12.750728: step 2041, loss 0.0683575, acc 0.98
2016-09-07T16:57:13.649819: step 2042, loss 0.0772377, acc 0.98
2016-09-07T16:57:14.412986: step 2043, loss 0.0451547, acc 0.98
2016-09-07T16:57:15.221349: step 2044, loss 0.157041, acc 0.94
2016-09-07T16:57:16.041661: step 2045, loss 0.0200218, acc 1
2016-09-07T16:57:16.820535: step 2046, loss 0.131006, acc 0.94
2016-09-07T16:57:17.631665: step 2047, loss 0.0812184, acc 0.96
2016-09-07T16:57:18.456717: step 2048, loss 0.0229739, acc 0.98
2016-09-07T16:57:19.260091: step 2049, loss 0.0915249, acc 0.96
2016-09-07T16:57:20.083154: step 2050, loss 0.0277908, acc 0.98
2016-09-07T16:57:20.896460: step 2051, loss 0.0692606, acc 0.98
2016-09-07T16:57:21.686686: step 2052, loss 0.0286165, acc 0.98
2016-09-07T16:57:22.539112: step 2053, loss 0.0556346, acc 0.98
2016-09-07T16:57:23.382667: step 2054, loss 0.0449034, acc 0.98
2016-09-07T16:57:24.395097: step 2055, loss 0.0539335, acc 0.96
2016-09-07T16:57:25.402727: step 2056, loss 0.0603214, acc 0.96
2016-09-07T16:57:26.380184: step 2057, loss 0.00553725, acc 1
2016-09-07T16:57:27.342335: step 2058, loss 0.0573052, acc 0.96
2016-09-07T16:57:28.293851: step 2059, loss 0.0445738, acc 0.98
2016-09-07T16:57:29.119132: step 2060, loss 0.141664, acc 0.96
2016-09-07T16:57:30.025858: step 2061, loss 0.0260098, acc 0.98
2016-09-07T16:57:30.962046: step 2062, loss 0.0738208, acc 0.98
2016-09-07T16:57:31.989262: step 2063, loss 0.0266112, acc 1
2016-09-07T16:57:33.052868: step 2064, loss 0.104964, acc 0.96
2016-09-07T16:57:34.159445: step 2065, loss 0.0214142, acc 1
2016-09-07T16:57:35.283719: step 2066, loss 0.041672, acc 0.98
2016-09-07T16:57:36.134257: step 2067, loss 0.0498213, acc 0.96
2016-09-07T16:57:36.858217: step 2068, loss 0.0316185, acc 0.98
2016-09-07T16:57:37.616127: step 2069, loss 0.0407321, acc 1
2016-09-07T16:57:38.320341: step 2070, loss 0.0431759, acc 0.98
2016-09-07T16:57:39.003050: step 2071, loss 0.121537, acc 0.92
2016-09-07T16:57:39.793467: step 2072, loss 0.0271254, acc 1
2016-09-07T16:57:40.607836: step 2073, loss 0.0316119, acc 0.98
2016-09-07T16:57:41.485342: step 2074, loss 0.0196276, acc 0.98
2016-09-07T16:57:42.339463: step 2075, loss 0.0449445, acc 0.98
2016-09-07T16:57:43.104154: step 2076, loss 0.037715, acc 0.98
2016-09-07T16:57:43.856926: step 2077, loss 0.119615, acc 0.98
2016-09-07T16:57:44.595846: step 2078, loss 0.0133693, acc 1
2016-09-07T16:57:45.328700: step 2079, loss 0.0489511, acc 0.96
2016-09-07T16:57:46.107509: step 2080, loss 0.0214984, acc 1
2016-09-07T16:57:46.860704: step 2081, loss 0.0178937, acc 1
2016-09-07T16:57:47.549923: step 2082, loss 0.0239764, acc 1
2016-09-07T16:57:48.211676: step 2083, loss 0.0861625, acc 0.96
2016-09-07T16:57:48.881925: step 2084, loss 0.0241761, acc 1
2016-09-07T16:57:49.551153: step 2085, loss 0.0571273, acc 0.98
2016-09-07T16:57:50.234919: step 2086, loss 0.028443, acc 0.98
2016-09-07T16:57:50.912849: step 2087, loss 0.0614411, acc 0.98
2016-09-07T16:57:51.594155: step 2088, loss 0.0598632, acc 1
2016-09-07T16:57:52.340502: step 2089, loss 0.0887921, acc 0.96
2016-09-07T16:57:53.178092: step 2090, loss 0.0119053, acc 1
2016-09-07T16:57:54.088268: step 2091, loss 0.0619187, acc 0.98
2016-09-07T16:57:54.910241: step 2092, loss 0.038353, acc 1
2016-09-07T16:57:55.569318: step 2093, loss 0.0334268, acc 0.98
2016-09-07T16:57:56.326162: step 2094, loss 0.186047, acc 0.94
2016-09-07T16:57:57.007797: step 2095, loss 0.0642749, acc 0.96
2016-09-07T16:57:57.701689: step 2096, loss 0.00768079, acc 1
2016-09-07T16:57:58.400494: step 2097, loss 0.0042424, acc 1
2016-09-07T16:57:59.113589: step 2098, loss 0.0500303, acc 0.98
2016-09-07T16:57:59.782474: step 2099, loss 0.0751767, acc 0.98
2016-09-07T16:58:00.481319: step 2100, loss 0.0180148, acc 1

Evaluation:
2016-09-07T16:58:03.779585: step 2100, loss 1.45241, acc 0.75

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-2100

2016-09-07T16:58:05.591267: step 2101, loss 0.0336572, acc 0.96
2016-09-07T16:58:06.246822: step 2102, loss 0.0386618, acc 0.98
2016-09-07T16:58:06.904991: step 2103, loss 0.0156006, acc 1
2016-09-07T16:58:07.696736: step 2104, loss 0.0212961, acc 1
2016-09-07T16:58:08.547144: step 2105, loss 0.038361, acc 0.98
2016-09-07T16:58:09.327899: step 2106, loss 0.0598689, acc 0.96
2016-09-07T16:58:10.069430: step 2107, loss 0.0410052, acc 0.98
2016-09-07T16:58:10.914098: step 2108, loss 0.0153986, acc 1
2016-09-07T16:58:11.730049: step 2109, loss 0.0640127, acc 0.96
2016-09-07T16:58:12.549803: step 2110, loss 0.0655651, acc 0.94
2016-09-07T16:58:13.279994: step 2111, loss 0.0364862, acc 0.98
2016-09-07T16:58:14.032028: step 2112, loss 0.0533871, acc 0.96
2016-09-07T16:58:14.811195: step 2113, loss 0.0216396, acc 1
2016-09-07T16:58:15.594001: step 2114, loss 0.049001, acc 0.96
2016-09-07T16:58:16.402959: step 2115, loss 0.072596, acc 0.98
2016-09-07T16:58:17.165727: step 2116, loss 0.0273552, acc 0.98
2016-09-07T16:58:17.912730: step 2117, loss 0.12481, acc 0.96
2016-09-07T16:58:18.582268: step 2118, loss 0.0082439, acc 1
2016-09-07T16:58:19.298038: step 2119, loss 0.0468623, acc 1
2016-09-07T16:58:20.109649: step 2120, loss 0.0274788, acc 0.98
2016-09-07T16:58:20.935825: step 2121, loss 0.0515669, acc 0.98
2016-09-07T16:58:21.791376: step 2122, loss 0.0212202, acc 0.98
2016-09-07T16:58:22.600622: step 2123, loss 0.0588939, acc 0.98
2016-09-07T16:58:23.418845: step 2124, loss 0.0253145, acc 0.98
2016-09-07T16:58:24.210694: step 2125, loss 0.00959736, acc 1
2016-09-07T16:58:24.995204: step 2126, loss 0.0271968, acc 1
2016-09-07T16:58:25.755189: step 2127, loss 0.102179, acc 0.98
2016-09-07T16:58:26.424848: step 2128, loss 0.0424802, acc 0.98
2016-09-07T16:58:27.095278: step 2129, loss 0.0290722, acc 0.98
2016-09-07T16:58:27.748011: step 2130, loss 0.0952311, acc 0.98
2016-09-07T16:58:28.423990: step 2131, loss 0.0657353, acc 0.96
2016-09-07T16:58:29.104509: step 2132, loss 0.0754999, acc 0.96
2016-09-07T16:58:29.791980: step 2133, loss 0.00666467, acc 1
2016-09-07T16:58:30.152479: step 2134, loss 0.0977323, acc 0.916667
2016-09-07T16:58:30.832502: step 2135, loss 0.0184754, acc 1
2016-09-07T16:58:31.518262: step 2136, loss 0.0708542, acc 0.96
2016-09-07T16:58:32.226348: step 2137, loss 0.0238094, acc 0.98
2016-09-07T16:58:32.907541: step 2138, loss 0.0684916, acc 0.96
2016-09-07T16:58:33.611574: step 2139, loss 0.0888514, acc 0.98
2016-09-07T16:58:34.307778: step 2140, loss 0.026953, acc 1
2016-09-07T16:58:34.993024: step 2141, loss 0.0228777, acc 0.98
2016-09-07T16:58:35.653575: step 2142, loss 0.0176493, acc 1
2016-09-07T16:58:36.339139: step 2143, loss 0.0948579, acc 0.98
2016-09-07T16:58:37.010927: step 2144, loss 0.0339498, acc 0.98
2016-09-07T16:58:37.688400: step 2145, loss 0.0853321, acc 0.94
2016-09-07T16:58:38.372398: step 2146, loss 0.05416, acc 0.98
2016-09-07T16:58:39.048874: step 2147, loss 0.0480648, acc 1
2016-09-07T16:58:39.721069: step 2148, loss 0.0941332, acc 0.96
2016-09-07T16:58:40.404955: step 2149, loss 0.013259, acc 1
2016-09-07T16:58:41.084471: step 2150, loss 0.124409, acc 0.96
2016-09-07T16:58:41.767863: step 2151, loss 0.104994, acc 0.94
2016-09-07T16:58:42.414010: step 2152, loss 0.0193576, acc 0.98
2016-09-07T16:58:43.077916: step 2153, loss 0.0263688, acc 0.98
2016-09-07T16:58:43.745866: step 2154, loss 0.0339331, acc 0.98
2016-09-07T16:58:44.428202: step 2155, loss 0.0952206, acc 0.94
2016-09-07T16:58:45.101793: step 2156, loss 0.132706, acc 0.96
2016-09-07T16:58:45.781105: step 2157, loss 0.114889, acc 0.94
2016-09-07T16:58:46.452833: step 2158, loss 0.126817, acc 0.92
2016-09-07T16:58:47.136207: step 2159, loss 0.0559652, acc 1
2016-09-07T16:58:47.815139: step 2160, loss 0.0394778, acc 0.98
2016-09-07T16:58:48.494980: step 2161, loss 0.0203013, acc 1
2016-09-07T16:58:49.195648: step 2162, loss 0.0235538, acc 1
2016-09-07T16:58:49.873158: step 2163, loss 0.0222332, acc 0.98
2016-09-07T16:58:50.595187: step 2164, loss 0.0296865, acc 1
2016-09-07T16:58:51.406652: step 2165, loss 0.0981461, acc 0.94
2016-09-07T16:58:52.267452: step 2166, loss 0.0888665, acc 0.96
2016-09-07T16:58:53.056919: step 2167, loss 0.0157054, acc 1
2016-09-07T16:58:53.793924: step 2168, loss 0.0555084, acc 0.96
2016-09-07T16:58:54.543559: step 2169, loss 0.0285139, acc 1
2016-09-07T16:58:55.302591: step 2170, loss 0.0302185, acc 0.98
2016-09-07T16:58:56.112965: step 2171, loss 0.0148326, acc 1
2016-09-07T16:58:56.767344: step 2172, loss 0.0829569, acc 0.96
2016-09-07T16:58:57.430638: step 2173, loss 0.0639286, acc 0.98
2016-09-07T16:58:58.128172: step 2174, loss 0.0130781, acc 1
2016-09-07T16:58:58.897214: step 2175, loss 0.211474, acc 0.94
2016-09-07T16:58:59.575213: step 2176, loss 0.0753115, acc 0.98
2016-09-07T16:59:00.279712: step 2177, loss 0.0325133, acc 1
2016-09-07T16:59:00.999051: step 2178, loss 0.0121163, acc 1
2016-09-07T16:59:01.744715: step 2179, loss 0.0783747, acc 0.96
2016-09-07T16:59:02.674100: step 2180, loss 0.0436106, acc 0.98
2016-09-07T16:59:03.481225: step 2181, loss 0.0448357, acc 0.98
2016-09-07T16:59:04.250705: step 2182, loss 0.0516343, acc 0.98
2016-09-07T16:59:05.092812: step 2183, loss 0.0440978, acc 0.96
2016-09-07T16:59:06.019070: step 2184, loss 0.0591643, acc 0.98
2016-09-07T16:59:06.867922: step 2185, loss 0.0432609, acc 0.96
2016-09-07T16:59:07.748330: step 2186, loss 0.0361846, acc 0.98
2016-09-07T16:59:08.527404: step 2187, loss 0.0262593, acc 0.98
2016-09-07T16:59:09.235294: step 2188, loss 0.0461679, acc 0.98
2016-09-07T16:59:09.888016: step 2189, loss 0.0981125, acc 0.94
2016-09-07T16:59:10.559447: step 2190, loss 0.0546926, acc 0.96
2016-09-07T16:59:11.234351: step 2191, loss 0.0513967, acc 0.96
2016-09-07T16:59:11.922491: step 2192, loss 0.0886666, acc 0.96
2016-09-07T16:59:12.758940: step 2193, loss 0.0230976, acc 1
2016-09-07T16:59:13.560025: step 2194, loss 0.0374177, acc 0.98
2016-09-07T16:59:14.340426: step 2195, loss 0.0802507, acc 0.96
2016-09-07T16:59:15.063856: step 2196, loss 0.0251314, acc 0.98
2016-09-07T16:59:15.902920: step 2197, loss 0.0887759, acc 0.94
2016-09-07T16:59:16.668431: step 2198, loss 0.0638909, acc 0.98
2016-09-07T16:59:17.359694: step 2199, loss 0.0110742, acc 1
2016-09-07T16:59:18.139275: step 2200, loss 0.0646755, acc 0.98

Evaluation:
2016-09-07T16:59:22.232547: step 2200, loss 1.44677, acc 0.745

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-2200

2016-09-07T16:59:24.300382: step 2201, loss 0.00981828, acc 1
2016-09-07T16:59:24.979881: step 2202, loss 0.025705, acc 1
2016-09-07T16:59:25.675580: step 2203, loss 0.0175055, acc 1
2016-09-07T16:59:26.339365: step 2204, loss 0.0818072, acc 0.94
2016-09-07T16:59:27.008754: step 2205, loss 0.0247432, acc 0.98
2016-09-07T16:59:27.684578: step 2206, loss 0.010912, acc 1
2016-09-07T16:59:28.352462: step 2207, loss 0.0435404, acc 0.98
2016-09-07T16:59:29.085500: step 2208, loss 0.0175802, acc 1
2016-09-07T16:59:29.809686: step 2209, loss 0.0298427, acc 0.98
2016-09-07T16:59:30.530434: step 2210, loss 0.0543536, acc 0.96
2016-09-07T16:59:31.238524: step 2211, loss 0.0335511, acc 0.98
2016-09-07T16:59:31.928829: step 2212, loss 0.0504007, acc 0.96
2016-09-07T16:59:32.735710: step 2213, loss 0.0304794, acc 1
2016-09-07T16:59:33.548710: step 2214, loss 0.0373912, acc 0.98
2016-09-07T16:59:34.225806: step 2215, loss 0.0384075, acc 0.96
2016-09-07T16:59:34.909221: step 2216, loss 0.0487597, acc 0.98
2016-09-07T16:59:35.576429: step 2217, loss 0.0221412, acc 0.98
2016-09-07T16:59:36.254082: step 2218, loss 0.11295, acc 0.98
2016-09-07T16:59:36.938109: step 2219, loss 0.0106205, acc 1
2016-09-07T16:59:37.657579: step 2220, loss 0.023562, acc 1
2016-09-07T16:59:38.349526: step 2221, loss 0.129962, acc 0.96
2016-09-07T16:59:39.073504: step 2222, loss 0.00491622, acc 1
2016-09-07T16:59:39.749523: step 2223, loss 0.0549211, acc 0.96
2016-09-07T16:59:40.417758: step 2224, loss 0.0738226, acc 0.94
2016-09-07T16:59:41.139261: step 2225, loss 0.0189705, acc 0.98
2016-09-07T16:59:41.851388: step 2226, loss 0.0291346, acc 0.98
2016-09-07T16:59:42.640594: step 2227, loss 0.0642726, acc 0.96
2016-09-07T16:59:43.487438: step 2228, loss 0.0135255, acc 1
2016-09-07T16:59:44.228236: step 2229, loss 0.0213064, acc 1
2016-09-07T16:59:44.894577: step 2230, loss 0.037925, acc 0.98
2016-09-07T16:59:45.571446: step 2231, loss 0.0494968, acc 0.98
2016-09-07T16:59:46.249386: step 2232, loss 0.0451062, acc 0.96
2016-09-07T16:59:46.918936: step 2233, loss 0.0116566, acc 1
2016-09-07T16:59:47.679546: step 2234, loss 0.0221123, acc 1
2016-09-07T16:59:48.356208: step 2235, loss 0.0508669, acc 0.96
2016-09-07T16:59:49.034450: step 2236, loss 0.021899, acc 1
2016-09-07T16:59:49.727049: step 2237, loss 0.0625079, acc 0.96
2016-09-07T16:59:50.411949: step 2238, loss 0.0620749, acc 0.96
2016-09-07T16:59:51.110167: step 2239, loss 0.0322795, acc 0.98
2016-09-07T16:59:51.800992: step 2240, loss 0.062413, acc 0.96
2016-09-07T16:59:52.543787: step 2241, loss 0.0508776, acc 0.96
2016-09-07T16:59:53.316637: step 2242, loss 0.047108, acc 0.98
2016-09-07T16:59:54.021811: step 2243, loss 0.023431, acc 1
2016-09-07T16:59:54.754469: step 2244, loss 0.17501, acc 0.96
2016-09-07T16:59:55.520440: step 2245, loss 0.0494105, acc 1
2016-09-07T16:59:56.205027: step 2246, loss 0.00801922, acc 1
2016-09-07T16:59:56.942063: step 2247, loss 0.110981, acc 0.96
2016-09-07T16:59:57.685948: step 2248, loss 0.0314429, acc 1
2016-09-07T16:59:58.541934: step 2249, loss 0.0166193, acc 1
2016-09-07T16:59:59.292083: step 2250, loss 0.12749, acc 0.94
2016-09-07T17:00:00.097504: step 2251, loss 0.00724849, acc 1
2016-09-07T17:00:00.970722: step 2252, loss 0.0610721, acc 0.96
2016-09-07T17:00:01.647493: step 2253, loss 0.074619, acc 0.98
2016-09-07T17:00:02.378362: step 2254, loss 0.118831, acc 0.96
2016-09-07T17:00:03.136272: step 2255, loss 0.0473245, acc 0.96
2016-09-07T17:00:03.865690: step 2256, loss 0.0301097, acc 0.98
2016-09-07T17:00:04.631357: step 2257, loss 0.0105729, acc 1
2016-09-07T17:00:05.345958: step 2258, loss 0.0052605, acc 1
2016-09-07T17:00:06.065652: step 2259, loss 0.0523486, acc 0.98
2016-09-07T17:00:06.924754: step 2260, loss 0.0668326, acc 0.98
2016-09-07T17:00:07.594586: step 2261, loss 0.0249033, acc 0.98
2016-09-07T17:00:08.397970: step 2262, loss 0.00352926, acc 1
2016-09-07T17:00:09.179261: step 2263, loss 0.108152, acc 0.94
2016-09-07T17:00:09.994899: step 2264, loss 0.124051, acc 0.96
2016-09-07T17:00:10.780536: step 2265, loss 0.0269875, acc 0.98
2016-09-07T17:00:11.588737: step 2266, loss 0.0839885, acc 0.96
2016-09-07T17:00:12.385228: step 2267, loss 0.0632356, acc 0.96
2016-09-07T17:00:13.148213: step 2268, loss 0.0638251, acc 0.96
2016-09-07T17:00:13.865503: step 2269, loss 0.0110969, acc 1
2016-09-07T17:00:14.645761: step 2270, loss 0.0254925, acc 0.98
2016-09-07T17:00:15.418748: step 2271, loss 0.177273, acc 0.96
2016-09-07T17:00:16.208538: step 2272, loss 0.0544501, acc 0.96
2016-09-07T17:00:16.940160: step 2273, loss 0.0291457, acc 1
2016-09-07T17:00:17.649972: step 2274, loss 0.0510344, acc 0.98
2016-09-07T17:00:18.402510: step 2275, loss 0.0777601, acc 0.96
2016-09-07T17:00:19.065966: step 2276, loss 0.0658895, acc 0.96
2016-09-07T17:00:19.807220: step 2277, loss 0.035357, acc 0.98
2016-09-07T17:00:20.492073: step 2278, loss 0.015801, acc 0.98
2016-09-07T17:00:21.165928: step 2279, loss 0.0415633, acc 0.98
2016-09-07T17:00:21.850993: step 2280, loss 0.113179, acc 0.98
2016-09-07T17:00:22.532947: step 2281, loss 0.0345615, acc 0.98
2016-09-07T17:00:23.211759: step 2282, loss 0.0867868, acc 0.94
2016-09-07T17:00:23.976836: step 2283, loss 0.0887994, acc 0.96
2016-09-07T17:00:24.642183: step 2284, loss 0.0287688, acc 0.98
2016-09-07T17:00:25.378957: step 2285, loss 0.0363655, acc 0.98
2016-09-07T17:00:26.088536: step 2286, loss 0.0439744, acc 1
2016-09-07T17:00:26.775523: step 2287, loss 0.0590206, acc 0.96
2016-09-07T17:00:27.552328: step 2288, loss 0.0511677, acc 0.96
2016-09-07T17:00:28.355829: step 2289, loss 0.110369, acc 0.94
2016-09-07T17:00:29.166357: step 2290, loss 0.0595673, acc 0.96
2016-09-07T17:00:29.837015: step 2291, loss 0.0173516, acc 1
2016-09-07T17:00:30.609119: step 2292, loss 0.0254406, acc 0.98
2016-09-07T17:00:31.292337: step 2293, loss 0.0553108, acc 0.98
2016-09-07T17:00:31.992581: step 2294, loss 0.0414859, acc 0.98
2016-09-07T17:00:32.768721: step 2295, loss 0.0234393, acc 1
2016-09-07T17:00:33.525766: step 2296, loss 0.0652468, acc 0.96
2016-09-07T17:00:34.299061: step 2297, loss 0.143395, acc 0.98
2016-09-07T17:00:34.987011: step 2298, loss 0.0440097, acc 0.98
2016-09-07T17:00:35.663455: step 2299, loss 0.0230313, acc 0.98
2016-09-07T17:00:36.321467: step 2300, loss 0.17799, acc 0.9

Evaluation:
2016-09-07T17:00:39.818149: step 2300, loss 1.60768, acc 0.759

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-2300

2016-09-07T17:00:41.828286: step 2301, loss 0.056068, acc 0.96
2016-09-07T17:00:42.624181: step 2302, loss 0.0338493, acc 0.98
2016-09-07T17:00:43.444909: step 2303, loss 0.082384, acc 0.96
2016-09-07T17:00:44.160319: step 2304, loss 0.126151, acc 0.96
2016-09-07T17:00:44.872395: step 2305, loss 0.108199, acc 0.96
2016-09-07T17:00:45.546702: step 2306, loss 0.04695, acc 0.96
2016-09-07T17:00:46.215100: step 2307, loss 0.0286932, acc 0.98
2016-09-07T17:00:47.019429: step 2308, loss 0.0824166, acc 0.98
2016-09-07T17:00:47.750769: step 2309, loss 0.125268, acc 0.96
2016-09-07T17:00:48.562808: step 2310, loss 0.0569053, acc 0.96
2016-09-07T17:00:49.322581: step 2311, loss 0.066337, acc 0.94
2016-09-07T17:00:50.083037: step 2312, loss 0.0640077, acc 0.98
2016-09-07T17:00:50.839433: step 2313, loss 0.0548196, acc 0.96
2016-09-07T17:00:51.575820: step 2314, loss 0.0339938, acc 0.98
2016-09-07T17:00:52.406841: step 2315, loss 0.156778, acc 0.92
2016-09-07T17:00:53.232195: step 2316, loss 0.0182968, acc 1
2016-09-07T17:00:53.990460: step 2317, loss 0.0215865, acc 1
2016-09-07T17:00:54.728335: step 2318, loss 0.0227256, acc 1
2016-09-07T17:00:55.510915: step 2319, loss 0.0856749, acc 0.96
2016-09-07T17:00:56.284071: step 2320, loss 0.124832, acc 0.9
2016-09-07T17:00:57.042331: step 2321, loss 0.0632346, acc 0.96
2016-09-07T17:00:57.816306: step 2322, loss 0.00994426, acc 1
2016-09-07T17:00:58.603851: step 2323, loss 0.0447106, acc 1
2016-09-07T17:00:59.396392: step 2324, loss 0.054779, acc 0.96
2016-09-07T17:01:00.186740: step 2325, loss 0.073488, acc 0.94
2016-09-07T17:01:00.943700: step 2326, loss 0.0484381, acc 0.98
2016-09-07T17:01:01.640388: step 2327, loss 0.0111492, acc 1
2016-09-07T17:01:02.016604: step 2328, loss 0.105507, acc 0.916667
2016-09-07T17:01:02.757761: step 2329, loss 0.0439171, acc 0.98
2016-09-07T17:01:03.558941: step 2330, loss 0.108662, acc 0.96
2016-09-07T17:01:04.260971: step 2331, loss 0.0715296, acc 0.96
2016-09-07T17:01:04.998838: step 2332, loss 0.0258017, acc 1
2016-09-07T17:01:05.767359: step 2333, loss 0.101911, acc 0.96
2016-09-07T17:01:06.588386: step 2334, loss 0.0690396, acc 0.96
2016-09-07T17:01:07.423076: step 2335, loss 0.0569309, acc 0.98
2016-09-07T17:01:08.174897: step 2336, loss 0.0287956, acc 1
2016-09-07T17:01:08.996828: step 2337, loss 0.0324449, acc 1
2016-09-07T17:01:09.830105: step 2338, loss 0.0735361, acc 0.98
2016-09-07T17:01:10.634751: step 2339, loss 0.0312585, acc 0.98
2016-09-07T17:01:11.328482: step 2340, loss 0.0275529, acc 0.98
2016-09-07T17:01:12.146801: step 2341, loss 0.0201399, acc 1
2016-09-07T17:01:12.986273: step 2342, loss 0.0687756, acc 0.96
2016-09-07T17:01:13.705465: step 2343, loss 0.0346633, acc 0.98
2016-09-07T17:01:14.398475: step 2344, loss 0.00230385, acc 1
2016-09-07T17:01:15.075656: step 2345, loss 0.0582426, acc 0.96
2016-09-07T17:01:15.766045: step 2346, loss 0.221318, acc 0.96
2016-09-07T17:01:16.464379: step 2347, loss 0.05412, acc 0.98
2016-09-07T17:01:17.234499: step 2348, loss 0.00462774, acc 1
2016-09-07T17:01:17.977772: step 2349, loss 0.0201181, acc 0.98
2016-09-07T17:01:18.754708: step 2350, loss 0.0684992, acc 0.96
2016-09-07T17:01:19.525837: step 2351, loss 0.00543323, acc 1
2016-09-07T17:01:20.303385: step 2352, loss 0.0121685, acc 1
2016-09-07T17:01:21.064550: step 2353, loss 0.0451006, acc 0.98
2016-09-07T17:01:21.908192: step 2354, loss 0.0152131, acc 0.98
2016-09-07T17:01:22.680641: step 2355, loss 0.0355821, acc 0.98
2016-09-07T17:01:23.479290: step 2356, loss 0.00790823, acc 1
2016-09-07T17:01:24.287114: step 2357, loss 0.0803062, acc 0.94
2016-09-07T17:01:25.138454: step 2358, loss 0.0951484, acc 0.92
2016-09-07T17:01:25.872254: step 2359, loss 0.0297888, acc 0.98
2016-09-07T17:01:26.608956: step 2360, loss 0.0595653, acc 1
2016-09-07T17:01:27.434967: step 2361, loss 0.0510047, acc 0.96
2016-09-07T17:01:28.172049: step 2362, loss 0.0888947, acc 0.94
2016-09-07T17:01:28.916058: step 2363, loss 0.0157251, acc 0.98
2016-09-07T17:01:29.627284: step 2364, loss 0.0596728, acc 0.98
2016-09-07T17:01:30.371961: step 2365, loss 0.0359, acc 0.98
2016-09-07T17:01:31.094926: step 2366, loss 0.0951499, acc 0.96
2016-09-07T17:01:32.021363: step 2367, loss 0.0336544, acc 0.98
2016-09-07T17:01:32.900003: step 2368, loss 0.0202215, acc 0.98
2016-09-07T17:01:33.577316: step 2369, loss 0.0491648, acc 0.98
2016-09-07T17:01:34.284952: step 2370, loss 0.163909, acc 0.96
2016-09-07T17:01:35.153919: step 2371, loss 0.042347, acc 0.96
2016-09-07T17:01:36.004050: step 2372, loss 0.0484864, acc 0.96
2016-09-07T17:01:36.664002: step 2373, loss 0.0728984, acc 0.96
2016-09-07T17:01:37.316949: step 2374, loss 0.0228756, acc 0.98
2016-09-07T17:01:38.004270: step 2375, loss 0.0643178, acc 0.98
2016-09-07T17:01:38.728643: step 2376, loss 0.053538, acc 0.96
2016-09-07T17:01:39.541764: step 2377, loss 0.00667778, acc 1
2016-09-07T17:01:40.362621: step 2378, loss 0.0142858, acc 1
2016-09-07T17:01:41.100166: step 2379, loss 0.0486323, acc 0.98
2016-09-07T17:01:41.894194: step 2380, loss 0.0528841, acc 0.96
2016-09-07T17:01:42.586155: step 2381, loss 0.0208047, acc 1
2016-09-07T17:01:43.281541: step 2382, loss 0.00820412, acc 1
2016-09-07T17:01:43.950407: step 2383, loss 0.0373282, acc 0.98
2016-09-07T17:01:44.626540: step 2384, loss 0.0425917, acc 1
2016-09-07T17:01:45.307949: step 2385, loss 0.0114073, acc 1
2016-09-07T17:01:45.995562: step 2386, loss 0.0480414, acc 0.98
2016-09-07T17:01:46.722004: step 2387, loss 0.0388591, acc 0.98
2016-09-07T17:01:47.389839: step 2388, loss 0.0287431, acc 0.98
2016-09-07T17:01:48.071056: step 2389, loss 0.103163, acc 0.98
2016-09-07T17:01:48.748522: step 2390, loss 0.0157992, acc 1
2016-09-07T17:01:49.423873: step 2391, loss 0.0361535, acc 0.98
2016-09-07T17:01:50.102157: step 2392, loss 0.0407572, acc 0.98
2016-09-07T17:01:50.794083: step 2393, loss 0.0721671, acc 0.96
2016-09-07T17:01:51.460963: step 2394, loss 0.0383004, acc 0.98
2016-09-07T17:01:52.134306: step 2395, loss 0.0514453, acc 0.98
2016-09-07T17:01:52.823553: step 2396, loss 0.0136921, acc 1
2016-09-07T17:01:53.502940: step 2397, loss 0.0199045, acc 0.98
2016-09-07T17:01:54.182854: step 2398, loss 0.0806725, acc 0.96
2016-09-07T17:01:54.847863: step 2399, loss 0.141464, acc 0.98
2016-09-07T17:01:55.532650: step 2400, loss 0.0647976, acc 0.96

Evaluation:
2016-09-07T17:01:58.817400: step 2400, loss 1.71481, acc 0.741

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-2400

2016-09-07T17:02:00.819621: step 2401, loss 0.00825395, acc 1
2016-09-07T17:02:01.631525: step 2402, loss 0.0244965, acc 1
2016-09-07T17:02:02.381673: step 2403, loss 0.0307921, acc 0.98
2016-09-07T17:02:03.228876: step 2404, loss 0.153829, acc 0.94
2016-09-07T17:02:04.016953: step 2405, loss 0.0744346, acc 0.98
2016-09-07T17:02:04.686893: step 2406, loss 0.06321, acc 0.98
2016-09-07T17:02:05.394040: step 2407, loss 0.0558529, acc 0.96
2016-09-07T17:02:06.134297: step 2408, loss 0.0420655, acc 0.96
2016-09-07T17:02:06.859875: step 2409, loss 0.0560625, acc 0.98
2016-09-07T17:02:07.650123: step 2410, loss 0.000977662, acc 1
2016-09-07T17:02:08.358289: step 2411, loss 0.0399519, acc 0.98
2016-09-07T17:02:09.029410: step 2412, loss 0.00917939, acc 1
2016-09-07T17:02:09.700073: step 2413, loss 0.015561, acc 0.98
2016-09-07T17:02:10.388535: step 2414, loss 0.0291779, acc 1
2016-09-07T17:02:11.096786: step 2415, loss 0.0169396, acc 0.98
2016-09-07T17:02:11.770799: step 2416, loss 0.00958158, acc 1
2016-09-07T17:02:12.444588: step 2417, loss 0.0766153, acc 0.98
2016-09-07T17:02:13.113286: step 2418, loss 0.0307908, acc 1
2016-09-07T17:02:13.806866: step 2419, loss 0.1477, acc 0.98
2016-09-07T17:02:14.486293: step 2420, loss 0.0299495, acc 0.98
2016-09-07T17:02:15.175334: step 2421, loss 0.0445957, acc 0.98
2016-09-07T17:02:15.842772: step 2422, loss 0.0178977, acc 1
2016-09-07T17:02:16.627360: step 2423, loss 0.0310534, acc 0.98
2016-09-07T17:02:17.397581: step 2424, loss 0.0639007, acc 0.96
2016-09-07T17:02:18.156539: step 2425, loss 0.00796838, acc 1
2016-09-07T17:02:18.925367: step 2426, loss 0.112446, acc 0.92
2016-09-07T17:02:19.696609: step 2427, loss 0.0362401, acc 0.98
2016-09-07T17:02:20.458818: step 2428, loss 0.0354095, acc 0.98
2016-09-07T17:02:21.190870: step 2429, loss 0.0337641, acc 0.98
2016-09-07T17:02:21.946521: step 2430, loss 0.0529907, acc 0.96
2016-09-07T17:02:22.717960: step 2431, loss 0.020294, acc 1
2016-09-07T17:02:23.432103: step 2432, loss 0.0694015, acc 0.98
2016-09-07T17:02:24.140335: step 2433, loss 0.117466, acc 0.92
2016-09-07T17:02:24.833505: step 2434, loss 0.0997576, acc 0.98
2016-09-07T17:02:25.607804: step 2435, loss 0.123164, acc 0.96
2016-09-07T17:02:26.385396: step 2436, loss 0.0157434, acc 1
2016-09-07T17:02:27.064136: step 2437, loss 0.0517833, acc 1
2016-09-07T17:02:27.743513: step 2438, loss 0.0578869, acc 0.98
2016-09-07T17:02:28.506276: step 2439, loss 0.0514109, acc 0.98
2016-09-07T17:02:29.281650: step 2440, loss 0.0311897, acc 0.98
2016-09-07T17:02:30.114408: step 2441, loss 0.104636, acc 0.98
2016-09-07T17:02:30.971511: step 2442, loss 0.114109, acc 0.94
2016-09-07T17:02:31.757388: step 2443, loss 0.0399604, acc 0.98
2016-09-07T17:02:32.496543: step 2444, loss 0.109253, acc 0.98
2016-09-07T17:02:33.356982: step 2445, loss 0.0567551, acc 0.96
2016-09-07T17:02:34.117058: step 2446, loss 0.0357475, acc 0.96
2016-09-07T17:02:34.901862: step 2447, loss 0.0636994, acc 0.98
2016-09-07T17:02:35.666464: step 2448, loss 0.0801641, acc 0.96
2016-09-07T17:02:36.420841: step 2449, loss 0.0469418, acc 0.98
2016-09-07T17:02:37.271203: step 2450, loss 0.0570102, acc 0.98
2016-09-07T17:02:38.018197: step 2451, loss 0.00655855, acc 1
2016-09-07T17:02:38.760890: step 2452, loss 0.0396013, acc 0.98
2016-09-07T17:02:39.543388: step 2453, loss 0.0216311, acc 1
2016-09-07T17:02:40.267840: step 2454, loss 0.0147932, acc 1
2016-09-07T17:02:41.088232: step 2455, loss 0.0160362, acc 1
2016-09-07T17:02:41.770282: step 2456, loss 0.0123503, acc 1
2016-09-07T17:02:42.541088: step 2457, loss 0.119388, acc 0.96
2016-09-07T17:02:43.248530: step 2458, loss 0.0399315, acc 0.98
2016-09-07T17:02:43.910108: step 2459, loss 0.0132868, acc 1
2016-09-07T17:02:44.587721: step 2460, loss 0.0824756, acc 0.96
2016-09-07T17:02:45.245755: step 2461, loss 0.021735, acc 0.98
2016-09-07T17:02:45.908254: step 2462, loss 0.100979, acc 0.94
2016-09-07T17:02:46.607029: step 2463, loss 0.0291262, acc 0.98
2016-09-07T17:02:47.288109: step 2464, loss 0.0381005, acc 0.98
2016-09-07T17:02:47.967165: step 2465, loss 0.0192855, acc 1
2016-09-07T17:02:48.647159: step 2466, loss 0.0664051, acc 0.98
2016-09-07T17:02:49.343730: step 2467, loss 0.00533706, acc 1
2016-09-07T17:02:50.024403: step 2468, loss 0.0604857, acc 0.98
2016-09-07T17:02:50.704426: step 2469, loss 0.0869986, acc 0.98
2016-09-07T17:02:51.387785: step 2470, loss 0.0789887, acc 0.94
2016-09-07T17:02:52.072965: step 2471, loss 0.0273394, acc 0.98
2016-09-07T17:02:52.742453: step 2472, loss 0.0497797, acc 0.96
2016-09-07T17:02:53.438335: step 2473, loss 0.0476328, acc 0.98
2016-09-07T17:02:54.222864: step 2474, loss 0.0997, acc 0.9
2016-09-07T17:02:54.891080: step 2475, loss 0.0529284, acc 0.96
2016-09-07T17:02:55.664745: step 2476, loss 0.02455, acc 1
2016-09-07T17:02:56.331930: step 2477, loss 0.0182563, acc 1
2016-09-07T17:02:57.018190: step 2478, loss 0.0574571, acc 0.96
2016-09-07T17:02:57.693454: step 2479, loss 0.162575, acc 0.94
2016-09-07T17:02:58.381070: step 2480, loss 0.135989, acc 0.94
2016-09-07T17:02:59.175547: step 2481, loss 0.0361878, acc 0.98
2016-09-07T17:03:00.078590: step 2482, loss 0.0361803, acc 1
2016-09-07T17:03:00.776997: step 2483, loss 0.17716, acc 0.96
2016-09-07T17:03:01.557027: step 2484, loss 0.00189578, acc 1
2016-09-07T17:03:02.252701: step 2485, loss 0.0035716, acc 1
2016-09-07T17:03:02.987319: step 2486, loss 0.0628551, acc 0.96
2016-09-07T17:03:03.664894: step 2487, loss 0.0190864, acc 1
2016-09-07T17:03:04.417997: step 2488, loss 0.10041, acc 0.96
2016-09-07T17:03:05.104997: step 2489, loss 0.146167, acc 0.94
2016-09-07T17:03:05.774966: step 2490, loss 0.0191574, acc 1
2016-09-07T17:03:06.428605: step 2491, loss 0.0597509, acc 0.96
2016-09-07T17:03:07.266196: step 2492, loss 0.0449667, acc 0.98
2016-09-07T17:03:08.080408: step 2493, loss 0.0424618, acc 0.98
2016-09-07T17:03:09.000513: step 2494, loss 0.0365771, acc 0.98
2016-09-07T17:03:09.753574: step 2495, loss 0.0154928, acc 1
2016-09-07T17:03:10.548196: step 2496, loss 0.0206643, acc 0.98
2016-09-07T17:03:11.378486: step 2497, loss 0.0601527, acc 0.96
2016-09-07T17:03:12.204383: step 2498, loss 0.108932, acc 0.96
2016-09-07T17:03:13.037704: step 2499, loss 0.0353143, acc 0.98
2016-09-07T17:03:13.738206: step 2500, loss 0.0505034, acc 0.98

Evaluation:
2016-09-07T17:03:17.648225: step 2500, loss 1.55114, acc 0.732

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-2500

2016-09-07T17:03:19.699593: step 2501, loss 0.0127811, acc 1
2016-09-07T17:03:20.638766: step 2502, loss 0.0325823, acc 1
2016-09-07T17:03:21.506291: step 2503, loss 0.03704, acc 1
2016-09-07T17:03:22.175854: step 2504, loss 0.0344745, acc 0.98
2016-09-07T17:03:22.954259: step 2505, loss 0.0445024, acc 1
2016-09-07T17:03:23.846453: step 2506, loss 0.0329111, acc 0.96
2016-09-07T17:03:24.596317: step 2507, loss 0.0464216, acc 0.96
2016-09-07T17:03:25.279886: step 2508, loss 0.112398, acc 0.96
2016-09-07T17:03:25.977818: step 2509, loss 0.0847712, acc 0.98
2016-09-07T17:03:26.704831: step 2510, loss 0.0758204, acc 0.96
2016-09-07T17:03:27.493471: step 2511, loss 0.0154378, acc 1
2016-09-07T17:03:28.207141: step 2512, loss 0.0172684, acc 1
2016-09-07T17:03:28.939644: step 2513, loss 0.0728166, acc 0.98
2016-09-07T17:03:29.656174: step 2514, loss 0.0280345, acc 1
2016-09-07T17:03:30.416872: step 2515, loss 0.192328, acc 0.94
2016-09-07T17:03:31.219316: step 2516, loss 0.0227771, acc 0.98
2016-09-07T17:03:32.008320: step 2517, loss 0.0151244, acc 1
2016-09-07T17:03:32.758077: step 2518, loss 0.0214567, acc 1
2016-09-07T17:03:33.553091: step 2519, loss 0.0441237, acc 0.98
2016-09-07T17:03:34.311528: step 2520, loss 0.0528912, acc 0.96
2016-09-07T17:03:35.078407: step 2521, loss 0.0192349, acc 1
2016-09-07T17:03:35.460577: step 2522, loss 0.000833318, acc 1
2016-09-07T17:03:36.341069: step 2523, loss 0.0153658, acc 1
2016-09-07T17:03:37.102455: step 2524, loss 0.0222011, acc 1
2016-09-07T17:03:37.900911: step 2525, loss 0.0856759, acc 0.96
2016-09-07T17:03:38.642556: step 2526, loss 0.0142805, acc 1
2016-09-07T17:03:39.460114: step 2527, loss 0.00525783, acc 1
2016-09-07T17:03:40.203301: step 2528, loss 0.142995, acc 0.94
2016-09-07T17:03:40.927505: step 2529, loss 0.0147611, acc 1
2016-09-07T17:03:41.683710: step 2530, loss 0.0481024, acc 0.98
2016-09-07T17:03:42.389407: step 2531, loss 0.0149821, acc 1
2016-09-07T17:03:43.105048: step 2532, loss 0.0339234, acc 0.98
2016-09-07T17:03:43.877621: step 2533, loss 0.0417565, acc 0.98
2016-09-07T17:03:44.630512: step 2534, loss 0.00515352, acc 1
2016-09-07T17:03:45.404077: step 2535, loss 0.103046, acc 0.96
2016-09-07T17:03:46.198909: step 2536, loss 0.112723, acc 0.92
2016-09-07T17:03:46.903418: step 2537, loss 0.00665096, acc 1
2016-09-07T17:03:47.654054: step 2538, loss 0.00127538, acc 1
2016-09-07T17:03:48.404912: step 2539, loss 0.0467016, acc 0.96
2016-09-07T17:03:49.110274: step 2540, loss 0.00401883, acc 1
2016-09-07T17:03:49.896571: step 2541, loss 0.0103312, acc 1
2016-09-07T17:03:50.620175: step 2542, loss 0.0197675, acc 1
2016-09-07T17:03:51.376414: step 2543, loss 0.0254593, acc 0.98
2016-09-07T17:03:52.290204: step 2544, loss 0.0631798, acc 0.98
2016-09-07T17:03:53.094208: step 2545, loss 0.111366, acc 0.96
2016-09-07T17:03:53.869514: step 2546, loss 0.0114758, acc 1
2016-09-07T17:03:54.672480: step 2547, loss 0.0428657, acc 1
2016-09-07T17:03:55.365645: step 2548, loss 0.00467823, acc 1
2016-09-07T17:03:56.055496: step 2549, loss 0.0196813, acc 1
2016-09-07T17:03:56.728874: step 2550, loss 0.113546, acc 0.94
2016-09-07T17:03:57.415899: step 2551, loss 0.0709587, acc 0.98
2016-09-07T17:03:58.082101: step 2552, loss 0.00735367, acc 1
2016-09-07T17:03:58.750464: step 2553, loss 0.0578011, acc 0.98
2016-09-07T17:03:59.462850: step 2554, loss 0.0317699, acc 0.98
2016-09-07T17:04:00.139017: step 2555, loss 0.0941298, acc 0.96
2016-09-07T17:04:00.823667: step 2556, loss 0.00106475, acc 1
2016-09-07T17:04:01.509108: step 2557, loss 0.0246012, acc 1
2016-09-07T17:04:02.167425: step 2558, loss 0.0500057, acc 0.98
2016-09-07T17:04:02.852414: step 2559, loss 0.0377475, acc 0.98
2016-09-07T17:04:03.547328: step 2560, loss 0.107982, acc 0.92
2016-09-07T17:04:04.225080: step 2561, loss 0.0856661, acc 0.96
2016-09-07T17:04:04.897947: step 2562, loss 0.00654084, acc 1
2016-09-07T17:04:05.599132: step 2563, loss 0.0867719, acc 0.98
2016-09-07T17:04:06.284136: step 2564, loss 0.0218837, acc 1
2016-09-07T17:04:06.964526: step 2565, loss 0.136429, acc 0.98
2016-09-07T17:04:07.670537: step 2566, loss 0.0497604, acc 0.98
2016-09-07T17:04:08.351794: step 2567, loss 0.0490435, acc 0.96
2016-09-07T17:04:09.035787: step 2568, loss 0.103025, acc 0.98
2016-09-07T17:04:09.725039: step 2569, loss 0.0478586, acc 0.96
2016-09-07T17:04:10.422240: step 2570, loss 0.0107662, acc 1
2016-09-07T17:04:11.099184: step 2571, loss 0.0170481, acc 1
2016-09-07T17:04:11.767384: step 2572, loss 0.0165819, acc 1
2016-09-07T17:04:12.435368: step 2573, loss 0.0277834, acc 1
2016-09-07T17:04:13.123039: step 2574, loss 0.026356, acc 0.98
2016-09-07T17:04:13.802916: step 2575, loss 0.0166849, acc 0.98
2016-09-07T17:04:14.487974: step 2576, loss 0.00479719, acc 1
2016-09-07T17:04:15.165793: step 2577, loss 0.00265332, acc 1
2016-09-07T17:04:15.828078: step 2578, loss 0.012378, acc 1
2016-09-07T17:04:16.509213: step 2579, loss 0.083105, acc 0.98
2016-09-07T17:04:17.198757: step 2580, loss 0.0664084, acc 0.98
2016-09-07T17:04:17.871036: step 2581, loss 0.0176265, acc 1
2016-09-07T17:04:18.552903: step 2582, loss 0.0223664, acc 1
2016-09-07T17:04:19.227569: step 2583, loss 0.0216806, acc 1
2016-09-07T17:04:19.926521: step 2584, loss 0.0368195, acc 0.98
2016-09-07T17:04:20.593341: step 2585, loss 0.0442704, acc 1
2016-09-07T17:04:21.267268: step 2586, loss 0.0951999, acc 0.98
2016-09-07T17:04:21.942935: step 2587, loss 0.0662811, acc 0.96
2016-09-07T17:04:22.633207: step 2588, loss 0.0354734, acc 0.98
2016-09-07T17:04:23.294451: step 2589, loss 0.114368, acc 0.94
2016-09-07T17:04:23.970072: step 2590, loss 0.0106171, acc 1
2016-09-07T17:04:24.629405: step 2591, loss 0.0490926, acc 0.98
2016-09-07T17:04:25.309694: step 2592, loss 0.0489756, acc 0.98
2016-09-07T17:04:25.977945: step 2593, loss 0.0460254, acc 0.98
2016-09-07T17:04:26.651663: step 2594, loss 0.0381638, acc 1
2016-09-07T17:04:27.351031: step 2595, loss 0.0062294, acc 1
2016-09-07T17:04:28.008415: step 2596, loss 0.023065, acc 1
2016-09-07T17:04:28.687639: step 2597, loss 0.0273803, acc 0.98
2016-09-07T17:04:29.372226: step 2598, loss 0.0484755, acc 0.96
2016-09-07T17:04:30.046927: step 2599, loss 0.0156445, acc 1
2016-09-07T17:04:30.723956: step 2600, loss 0.00838907, acc 1

Evaluation:
2016-09-07T17:04:33.637076: step 2600, loss 1.35777, acc 0.738

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-2600

2016-09-07T17:04:35.222443: step 2601, loss 0.0386919, acc 0.96
2016-09-07T17:04:35.887862: step 2602, loss 0.0596627, acc 0.98
2016-09-07T17:04:36.560908: step 2603, loss 0.0811347, acc 0.96
2016-09-07T17:04:37.228223: step 2604, loss 0.0518122, acc 0.98
2016-09-07T17:04:37.903144: step 2605, loss 0.0070434, acc 1
2016-09-07T17:04:38.586657: step 2606, loss 0.00462366, acc 1
2016-09-07T17:04:39.282542: step 2607, loss 0.0555338, acc 0.98
2016-09-07T17:04:39.959158: step 2608, loss 0.00665524, acc 1
2016-09-07T17:04:40.635884: step 2609, loss 0.0535347, acc 0.96
2016-09-07T17:04:41.297668: step 2610, loss 0.0386901, acc 1
2016-09-07T17:04:41.980063: step 2611, loss 0.0388546, acc 0.98
2016-09-07T17:04:42.658587: step 2612, loss 0.0605656, acc 0.98
2016-09-07T17:04:43.335118: step 2613, loss 0.0612216, acc 0.98
2016-09-07T17:04:44.022699: step 2614, loss 0.0138739, acc 1
2016-09-07T17:04:44.683301: step 2615, loss 0.0172932, acc 1
2016-09-07T17:04:45.337522: step 2616, loss 0.00801052, acc 1
2016-09-07T17:04:45.997353: step 2617, loss 0.100037, acc 0.96
2016-09-07T17:04:46.686687: step 2618, loss 0.00465894, acc 1
2016-09-07T17:04:47.360155: step 2619, loss 0.0261707, acc 0.98
2016-09-07T17:04:48.038990: step 2620, loss 0.0389642, acc 1
2016-09-07T17:04:48.723807: step 2621, loss 0.0578265, acc 0.96
2016-09-07T17:04:49.390446: step 2622, loss 0.0604515, acc 0.98
2016-09-07T17:04:50.063710: step 2623, loss 0.0128347, acc 1
2016-09-07T17:04:50.739911: step 2624, loss 0.0128988, acc 1
2016-09-07T17:04:51.415677: step 2625, loss 0.0205674, acc 1
2016-09-07T17:04:52.076415: step 2626, loss 0.233659, acc 0.94
2016-09-07T17:04:52.762436: step 2627, loss 0.0355779, acc 0.98
2016-09-07T17:04:53.429932: step 2628, loss 0.0504801, acc 1
2016-09-07T17:04:54.096516: step 2629, loss 0.0275386, acc 1
2016-09-07T17:04:54.793332: step 2630, loss 0.0180739, acc 0.98
2016-09-07T17:04:55.466833: step 2631, loss 0.11402, acc 0.94
2016-09-07T17:04:56.146546: step 2632, loss 0.0368218, acc 0.98
2016-09-07T17:04:56.794761: step 2633, loss 0.193824, acc 0.9
2016-09-07T17:04:57.463741: step 2634, loss 0.0220946, acc 1
2016-09-07T17:04:58.126577: step 2635, loss 0.00413062, acc 1
2016-09-07T17:04:58.795023: step 2636, loss 0.0162093, acc 1
2016-09-07T17:04:59.480443: step 2637, loss 0.0327736, acc 1
2016-09-07T17:05:00.160226: step 2638, loss 0.134472, acc 0.9
2016-09-07T17:05:00.867924: step 2639, loss 0.0752148, acc 0.96
2016-09-07T17:05:01.536110: step 2640, loss 0.0416036, acc 1
2016-09-07T17:05:02.198521: step 2641, loss 0.00501736, acc 1
2016-09-07T17:05:02.969722: step 2642, loss 0.0451432, acc 0.98
2016-09-07T17:05:03.654913: step 2643, loss 0.0479983, acc 0.98
2016-09-07T17:05:04.325192: step 2644, loss 0.109228, acc 0.94
2016-09-07T17:05:05.007023: step 2645, loss 0.0614958, acc 0.96
2016-09-07T17:05:05.695386: step 2646, loss 0.00691655, acc 1
2016-09-07T17:05:06.388622: step 2647, loss 0.0217065, acc 1
2016-09-07T17:05:07.069661: step 2648, loss 0.0421903, acc 0.96
2016-09-07T17:05:07.733162: step 2649, loss 0.0522061, acc 0.98
2016-09-07T17:05:08.401988: step 2650, loss 0.162704, acc 0.98
2016-09-07T17:05:09.069595: step 2651, loss 0.007222, acc 1
2016-09-07T17:05:09.731866: step 2652, loss 0.0226605, acc 1
2016-09-07T17:05:10.414563: step 2653, loss 0.110684, acc 0.94
2016-09-07T17:05:11.218124: step 2654, loss 0.0177459, acc 1
2016-09-07T17:05:12.020400: step 2655, loss 0.0238899, acc 0.98
2016-09-07T17:05:12.760678: step 2656, loss 0.0787373, acc 0.98
2016-09-07T17:05:13.476753: step 2657, loss 0.0769084, acc 0.94
2016-09-07T17:05:14.214157: step 2658, loss 0.0295382, acc 0.98
2016-09-07T17:05:14.886165: step 2659, loss 0.0889146, acc 0.94
2016-09-07T17:05:15.616497: step 2660, loss 0.00253784, acc 1
2016-09-07T17:05:16.351062: step 2661, loss 0.114972, acc 0.94
2016-09-07T17:05:17.011922: step 2662, loss 0.0738307, acc 0.94
2016-09-07T17:05:17.740282: step 2663, loss 0.0801243, acc 0.96
2016-09-07T17:05:18.548290: step 2664, loss 0.0390031, acc 0.98
2016-09-07T17:05:19.386578: step 2665, loss 0.094232, acc 0.96
2016-09-07T17:05:20.181029: step 2666, loss 0.0189609, acc 1
2016-09-07T17:05:20.936325: step 2667, loss 0.0323663, acc 0.98
2016-09-07T17:05:21.663481: step 2668, loss 0.0767253, acc 0.96
2016-09-07T17:05:22.408731: step 2669, loss 0.0285701, acc 0.98
2016-09-07T17:05:23.142010: step 2670, loss 0.0638018, acc 0.96
2016-09-07T17:05:23.826489: step 2671, loss 0.0724272, acc 0.96
2016-09-07T17:05:24.506041: step 2672, loss 0.0381406, acc 0.98
2016-09-07T17:05:25.179274: step 2673, loss 0.0554754, acc 0.96
2016-09-07T17:05:25.877645: step 2674, loss 0.0418348, acc 0.98
2016-09-07T17:05:26.541525: step 2675, loss 0.0511236, acc 0.98
2016-09-07T17:05:27.214559: step 2676, loss 0.0337723, acc 1
2016-09-07T17:05:27.888076: step 2677, loss 0.0536406, acc 0.98
2016-09-07T17:05:28.574589: step 2678, loss 0.0161091, acc 1
2016-09-07T17:05:29.246509: step 2679, loss 0.0733549, acc 0.98
2016-09-07T17:05:29.911202: step 2680, loss 0.00911725, acc 1
2016-09-07T17:05:30.600857: step 2681, loss 0.00504402, acc 1
2016-09-07T17:05:31.283783: step 2682, loss 0.130297, acc 0.96
2016-09-07T17:05:31.974043: step 2683, loss 0.0293799, acc 1
2016-09-07T17:05:32.677220: step 2684, loss 0.0122508, acc 1
2016-09-07T17:05:33.354019: step 2685, loss 0.0029966, acc 1
2016-09-07T17:05:34.024391: step 2686, loss 0.0286274, acc 1
2016-09-07T17:05:34.719807: step 2687, loss 0.0495956, acc 0.98
2016-09-07T17:05:35.399288: step 2688, loss 0.02214, acc 0.98
2016-09-07T17:05:36.086664: step 2689, loss 0.00969427, acc 1
2016-09-07T17:05:36.757264: step 2690, loss 0.0319181, acc 0.98
2016-09-07T17:05:37.421213: step 2691, loss 0.0169202, acc 1
2016-09-07T17:05:38.096198: step 2692, loss 0.0623243, acc 0.98
2016-09-07T17:05:38.766104: step 2693, loss 0.0923279, acc 0.98
2016-09-07T17:05:39.439484: step 2694, loss 0.021915, acc 0.98
2016-09-07T17:05:40.123161: step 2695, loss 0.0070551, acc 1
2016-09-07T17:05:40.868832: step 2696, loss 0.0437728, acc 0.98
2016-09-07T17:05:41.533631: step 2697, loss 0.0848396, acc 0.96
2016-09-07T17:05:42.203161: step 2698, loss 0.0514444, acc 0.98
2016-09-07T17:05:42.873013: step 2699, loss 0.114795, acc 0.92
2016-09-07T17:05:43.532563: step 2700, loss 0.0196742, acc 1

Evaluation:
2016-09-07T17:05:46.467417: step 2700, loss 1.64431, acc 0.756

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-2700

2016-09-07T17:05:48.066809: step 2701, loss 0.035828, acc 1
2016-09-07T17:05:48.752632: step 2702, loss 0.0624488, acc 0.96
2016-09-07T17:05:49.432786: step 2703, loss 0.0490098, acc 0.98
2016-09-07T17:05:50.104831: step 2704, loss 0.00400102, acc 1
2016-09-07T17:05:50.795930: step 2705, loss 0.109092, acc 0.96
2016-09-07T17:05:51.461410: step 2706, loss 0.0255892, acc 0.98
2016-09-07T17:05:52.119686: step 2707, loss 0.0647468, acc 0.96
2016-09-07T17:05:52.791157: step 2708, loss 0.0850225, acc 0.98
2016-09-07T17:05:53.467387: step 2709, loss 0.0818682, acc 0.96
2016-09-07T17:05:54.152496: step 2710, loss 0.000687358, acc 1
2016-09-07T17:05:54.837034: step 2711, loss 0.0354532, acc 0.98
2016-09-07T17:05:55.551517: step 2712, loss 0.0490704, acc 0.98
2016-09-07T17:05:56.233948: step 2713, loss 0.019719, acc 1
2016-09-07T17:05:56.909038: step 2714, loss 0.0559984, acc 0.96
2016-09-07T17:05:57.590977: step 2715, loss 0.0264786, acc 0.98
2016-09-07T17:05:57.939451: step 2716, loss 0.122193, acc 0.916667
2016-09-07T17:05:58.632837: step 2717, loss 0.0402968, acc 0.98
2016-09-07T17:05:59.362724: step 2718, loss 0.00987117, acc 1
2016-09-07T17:06:00.024746: step 2719, loss 0.0770111, acc 0.94
2016-09-07T17:06:00.711160: step 2720, loss 0.00957639, acc 1
2016-09-07T17:06:01.404961: step 2721, loss 0.0345102, acc 0.98
2016-09-07T17:06:02.091314: step 2722, loss 0.045343, acc 0.98
2016-09-07T17:06:02.785343: step 2723, loss 0.00724809, acc 1
2016-09-07T17:06:03.462169: step 2724, loss 0.018687, acc 1
2016-09-07T17:06:04.134266: step 2725, loss 0.00338713, acc 1
2016-09-07T17:06:04.792957: step 2726, loss 0.00621603, acc 1
2016-09-07T17:06:05.472667: step 2727, loss 0.0341395, acc 0.98
2016-09-07T17:06:06.142452: step 2728, loss 0.0414475, acc 0.96
2016-09-07T17:06:06.806057: step 2729, loss 0.0553088, acc 0.94
2016-09-07T17:06:07.480746: step 2730, loss 0.00101934, acc 1
2016-09-07T17:06:08.134806: step 2731, loss 0.00332127, acc 1
2016-09-07T17:06:08.787405: step 2732, loss 0.128578, acc 0.98
2016-09-07T17:06:09.433029: step 2733, loss 0.00404714, acc 1
2016-09-07T17:06:10.100562: step 2734, loss 0.0620214, acc 0.96
2016-09-07T17:06:10.785748: step 2735, loss 0.0429233, acc 0.96
2016-09-07T17:06:11.457996: step 2736, loss 0.215127, acc 0.92
2016-09-07T17:06:12.130659: step 2737, loss 0.0183671, acc 1
2016-09-07T17:06:12.800749: step 2738, loss 0.0252176, acc 1
2016-09-07T17:06:13.468167: step 2739, loss 0.0233698, acc 1
2016-09-07T17:06:14.134455: step 2740, loss 0.13179, acc 0.92
2016-09-07T17:06:14.798121: step 2741, loss 0.0461549, acc 0.98
2016-09-07T17:06:15.509643: step 2742, loss 0.0206965, acc 1
2016-09-07T17:06:16.251723: step 2743, loss 0.0569125, acc 0.96
2016-09-07T17:06:16.905976: step 2744, loss 0.0123372, acc 1
2016-09-07T17:06:17.583797: step 2745, loss 0.00873363, acc 1
2016-09-07T17:06:18.280162: step 2746, loss 0.0139582, acc 1
2016-09-07T17:06:18.973104: step 2747, loss 0.106304, acc 0.94
2016-09-07T17:06:19.665007: step 2748, loss 0.146406, acc 0.94
2016-09-07T17:06:20.359864: step 2749, loss 0.0565955, acc 0.98
2016-09-07T17:06:21.041086: step 2750, loss 0.134775, acc 0.94
2016-09-07T17:06:21.710590: step 2751, loss 0.0669628, acc 0.98
2016-09-07T17:06:22.391743: step 2752, loss 0.0413931, acc 0.98
2016-09-07T17:06:23.071273: step 2753, loss 0.0318945, acc 1
2016-09-07T17:06:23.785006: step 2754, loss 0.0384473, acc 0.98
2016-09-07T17:06:24.482284: step 2755, loss 0.267446, acc 0.88
2016-09-07T17:06:25.153704: step 2756, loss 0.00464194, acc 1
2016-09-07T17:06:25.809803: step 2757, loss 0.122889, acc 0.94
2016-09-07T17:06:26.474702: step 2758, loss 0.193704, acc 0.98
2016-09-07T17:06:27.150845: step 2759, loss 0.0602878, acc 0.98
2016-09-07T17:06:27.806973: step 2760, loss 0.0544479, acc 0.96
2016-09-07T17:06:28.488852: step 2761, loss 0.0429642, acc 0.98
2016-09-07T17:06:29.146109: step 2762, loss 0.0996319, acc 0.96
2016-09-07T17:06:29.803311: step 2763, loss 0.0376007, acc 1
2016-09-07T17:06:30.483655: step 2764, loss 0.0482012, acc 0.96
2016-09-07T17:06:31.137751: step 2765, loss 0.0256358, acc 1
2016-09-07T17:06:31.816669: step 2766, loss 0.0235589, acc 0.98
2016-09-07T17:06:32.495673: step 2767, loss 0.0590928, acc 0.98
2016-09-07T17:06:33.149886: step 2768, loss 0.052165, acc 0.98
2016-09-07T17:06:33.823970: step 2769, loss 0.0181296, acc 1
2016-09-07T17:06:34.508567: step 2770, loss 0.0177671, acc 0.98
2016-09-07T17:06:35.191683: step 2771, loss 0.0198581, acc 1
2016-09-07T17:06:35.869198: step 2772, loss 0.0250406, acc 1
2016-09-07T17:06:36.533446: step 2773, loss 0.0330935, acc 1
2016-09-07T17:06:37.203752: step 2774, loss 0.0025224, acc 1
2016-09-07T17:06:37.894486: step 2775, loss 0.0129972, acc 1
2016-09-07T17:06:38.571877: step 2776, loss 0.0192783, acc 1
2016-09-07T17:06:39.258901: step 2777, loss 0.0455752, acc 0.98
2016-09-07T17:06:39.934222: step 2778, loss 0.0195468, acc 0.98
2016-09-07T17:06:40.600899: step 2779, loss 0.0555402, acc 0.98
2016-09-07T17:06:41.263489: step 2780, loss 0.119353, acc 0.92
2016-09-07T17:06:41.931433: step 2781, loss 0.0694861, acc 0.96
2016-09-07T17:06:42.605189: step 2782, loss 0.0324176, acc 0.98
2016-09-07T17:06:43.272280: step 2783, loss 0.0601891, acc 0.96
2016-09-07T17:06:43.944037: step 2784, loss 0.0340278, acc 0.98
2016-09-07T17:06:44.603395: step 2785, loss 0.0162405, acc 1
2016-09-07T17:06:45.282450: step 2786, loss 0.026161, acc 0.98
2016-09-07T17:06:45.964413: step 2787, loss 0.0160785, acc 1
2016-09-07T17:06:46.640005: step 2788, loss 0.00621615, acc 1
2016-09-07T17:06:47.296785: step 2789, loss 0.0189426, acc 1
2016-09-07T17:06:47.949821: step 2790, loss 0.0376357, acc 0.98
2016-09-07T17:06:48.607932: step 2791, loss 0.07242, acc 0.96
2016-09-07T17:06:49.260304: step 2792, loss 0.0808033, acc 0.96
2016-09-07T17:06:49.929961: step 2793, loss 0.0199302, acc 1
2016-09-07T17:06:50.599022: step 2794, loss 0.0248192, acc 0.98
2016-09-07T17:06:51.280443: step 2795, loss 0.0417009, acc 1
2016-09-07T17:06:51.935528: step 2796, loss 0.0425394, acc 0.98
2016-09-07T17:06:52.608444: step 2797, loss 0.0197008, acc 1
2016-09-07T17:06:53.281874: step 2798, loss 0.0216964, acc 0.98
2016-09-07T17:06:53.963828: step 2799, loss 0.0719698, acc 0.96
2016-09-07T17:06:54.652561: step 2800, loss 0.0585365, acc 0.98

Evaluation:
2016-09-07T17:06:57.646644: step 2800, loss 1.76836, acc 0.752

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-2800

2016-09-07T17:06:59.335560: step 2801, loss 0.0320079, acc 0.98
2016-09-07T17:07:00.017756: step 2802, loss 0.0191692, acc 1
2016-09-07T17:07:00.704092: step 2803, loss 0.00416725, acc 1
2016-09-07T17:07:01.458367: step 2804, loss 0.0185806, acc 1
2016-09-07T17:07:02.303282: step 2805, loss 0.0504445, acc 0.98
2016-09-07T17:07:03.050358: step 2806, loss 0.0613552, acc 0.98
2016-09-07T17:07:03.722506: step 2807, loss 0.0539029, acc 0.98
2016-09-07T17:07:04.388826: step 2808, loss 0.0797074, acc 0.98
2016-09-07T17:07:05.058173: step 2809, loss 0.0167377, acc 0.98
2016-09-07T17:07:05.728871: step 2810, loss 0.0211324, acc 1
2016-09-07T17:07:06.397820: step 2811, loss 0.0195322, acc 0.98
2016-09-07T17:07:07.069681: step 2812, loss 0.0273805, acc 0.98
2016-09-07T17:07:07.742767: step 2813, loss 0.0361437, acc 0.98
2016-09-07T17:07:08.409127: step 2814, loss 0.0293885, acc 0.96
2016-09-07T17:07:09.083094: step 2815, loss 0.0279201, acc 0.98
2016-09-07T17:07:09.739347: step 2816, loss 0.0667191, acc 0.98
2016-09-07T17:07:10.408647: step 2817, loss 0.0350838, acc 0.96
2016-09-07T17:07:11.073769: step 2818, loss 0.101253, acc 0.96
2016-09-07T17:07:11.742931: step 2819, loss 0.018949, acc 0.98
2016-09-07T17:07:12.408987: step 2820, loss 0.0521628, acc 0.98
2016-09-07T17:07:13.076592: step 2821, loss 0.0213471, acc 0.98
2016-09-07T17:07:13.733771: step 2822, loss 0.0883797, acc 0.96
2016-09-07T17:07:14.410657: step 2823, loss 0.0191489, acc 0.98
2016-09-07T17:07:15.082794: step 2824, loss 0.0263337, acc 0.98
2016-09-07T17:07:15.770665: step 2825, loss 0.0148167, acc 1
2016-09-07T17:07:16.436044: step 2826, loss 0.0116255, acc 1
2016-09-07T17:07:17.141564: step 2827, loss 0.0335588, acc 0.98
2016-09-07T17:07:17.799281: step 2828, loss 0.103788, acc 0.98
2016-09-07T17:07:18.461108: step 2829, loss 0.0419666, acc 0.98
2016-09-07T17:07:19.140447: step 2830, loss 0.0353176, acc 0.98
2016-09-07T17:07:19.830234: step 2831, loss 0.123885, acc 0.96
2016-09-07T17:07:20.493620: step 2832, loss 0.0630931, acc 0.96
2016-09-07T17:07:21.176591: step 2833, loss 0.0245987, acc 1
2016-09-07T17:07:21.840905: step 2834, loss 0.0293499, acc 0.98
2016-09-07T17:07:22.517822: step 2835, loss 0.0129791, acc 1
2016-09-07T17:07:23.210667: step 2836, loss 0.15399, acc 0.94
2016-09-07T17:07:23.912045: step 2837, loss 0.0263809, acc 1
2016-09-07T17:07:24.583317: step 2838, loss 0.0493298, acc 1
2016-09-07T17:07:25.253436: step 2839, loss 0.0144412, acc 1
2016-09-07T17:07:25.937474: step 2840, loss 0.0708964, acc 0.96
2016-09-07T17:07:26.627424: step 2841, loss 0.121943, acc 0.94
2016-09-07T17:07:27.294453: step 2842, loss 0.0397244, acc 0.98
2016-09-07T17:07:27.980205: step 2843, loss 0.0275858, acc 1
2016-09-07T17:07:28.672340: step 2844, loss 0.00363627, acc 1
2016-09-07T17:07:29.346522: step 2845, loss 0.0249252, acc 1
2016-09-07T17:07:30.018665: step 2846, loss 0.00884905, acc 1
2016-09-07T17:07:30.697056: step 2847, loss 0.0217397, acc 1
2016-09-07T17:07:31.370680: step 2848, loss 0.073125, acc 0.96
2016-09-07T17:07:32.036813: step 2849, loss 0.003806, acc 1
2016-09-07T17:07:32.728975: step 2850, loss 0.035389, acc 0.98
2016-09-07T17:07:33.408428: step 2851, loss 0.0328334, acc 0.98
2016-09-07T17:07:34.072264: step 2852, loss 0.023372, acc 1
2016-09-07T17:07:34.753156: step 2853, loss 0.0135852, acc 1
2016-09-07T17:07:35.432501: step 2854, loss 0.00767697, acc 1
2016-09-07T17:07:36.091483: step 2855, loss 0.0446247, acc 0.96
2016-09-07T17:07:36.755958: step 2856, loss 0.0184954, acc 1
2016-09-07T17:07:37.421630: step 2857, loss 0.0333372, acc 0.98
2016-09-07T17:07:38.107001: step 2858, loss 0.0685169, acc 0.96
2016-09-07T17:07:38.770445: step 2859, loss 0.0787014, acc 0.98
2016-09-07T17:07:39.433111: step 2860, loss 0.00652217, acc 1
2016-09-07T17:07:40.134643: step 2861, loss 0.0154121, acc 1
2016-09-07T17:07:40.823059: step 2862, loss 0.00451674, acc 1
2016-09-07T17:07:41.512235: step 2863, loss 0.137824, acc 0.92
2016-09-07T17:07:42.179043: step 2864, loss 0.0437224, acc 0.98
2016-09-07T17:07:42.854246: step 2865, loss 0.0365566, acc 0.98
2016-09-07T17:07:43.558378: step 2866, loss 0.0177347, acc 0.98
2016-09-07T17:07:44.231857: step 2867, loss 0.132105, acc 0.96
2016-09-07T17:07:44.926589: step 2868, loss 0.294587, acc 0.92
2016-09-07T17:07:45.614024: step 2869, loss 0.0607051, acc 0.98
2016-09-07T17:07:46.270822: step 2870, loss 0.13952, acc 0.98
2016-09-07T17:07:46.944873: step 2871, loss 0.0269744, acc 1
2016-09-07T17:07:47.588188: step 2872, loss 0.00304823, acc 1
2016-09-07T17:07:48.251863: step 2873, loss 0.020642, acc 1
2016-09-07T17:07:48.934922: step 2874, loss 0.0184497, acc 1
2016-09-07T17:07:49.616789: step 2875, loss 0.0115862, acc 1
2016-09-07T17:07:50.325592: step 2876, loss 0.0417511, acc 0.96
2016-09-07T17:07:51.004322: step 2877, loss 0.015668, acc 1
2016-09-07T17:07:51.662957: step 2878, loss 0.0677326, acc 0.98
2016-09-07T17:07:52.333753: step 2879, loss 0.00662569, acc 1
2016-09-07T17:07:53.028957: step 2880, loss 0.108246, acc 0.92
2016-09-07T17:07:53.731637: step 2881, loss 0.00805282, acc 1
2016-09-07T17:07:54.398568: step 2882, loss 0.0429456, acc 1
2016-09-07T17:07:55.071879: step 2883, loss 0.0162933, acc 1
2016-09-07T17:07:55.752186: step 2884, loss 0.031932, acc 0.98
2016-09-07T17:07:56.424992: step 2885, loss 0.111579, acc 0.94
2016-09-07T17:07:57.188968: step 2886, loss 0.0995596, acc 0.94
2016-09-07T17:07:57.924702: step 2887, loss 0.0368082, acc 0.96
2016-09-07T17:07:58.606580: step 2888, loss 0.0177764, acc 1
2016-09-07T17:07:59.276128: step 2889, loss 0.0442233, acc 0.96
2016-09-07T17:07:59.954299: step 2890, loss 0.0235795, acc 0.98
2016-09-07T17:08:00.641948: step 2891, loss 0.0287049, acc 1
2016-09-07T17:08:01.296560: step 2892, loss 0.0435517, acc 0.98
2016-09-07T17:08:01.951416: step 2893, loss 0.0336858, acc 0.98
2016-09-07T17:08:02.608102: step 2894, loss 0.00852887, acc 1
2016-09-07T17:08:03.287759: step 2895, loss 0.0715133, acc 0.96
2016-09-07T17:08:03.964335: step 2896, loss 0.0343878, acc 0.98
2016-09-07T17:08:04.647981: step 2897, loss 0.15963, acc 0.92
2016-09-07T17:08:05.309564: step 2898, loss 0.00972804, acc 1
2016-09-07T17:08:05.967042: step 2899, loss 0.0054948, acc 1
2016-09-07T17:08:06.630806: step 2900, loss 0.0722471, acc 0.98

Evaluation:
2016-09-07T17:08:09.550656: step 2900, loss 1.62242, acc 0.747

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-2900

2016-09-07T17:08:11.271033: step 2901, loss 0.0203159, acc 1
2016-09-07T17:08:11.941852: step 2902, loss 0.118652, acc 0.94
2016-09-07T17:08:12.631346: step 2903, loss 0.0676477, acc 0.96
2016-09-07T17:08:13.297672: step 2904, loss 0.0464576, acc 0.98
2016-09-07T17:08:13.968181: step 2905, loss 0.0260376, acc 1
2016-09-07T17:08:14.624583: step 2906, loss 0.0215984, acc 1
2016-09-07T17:08:15.291224: step 2907, loss 0.0284509, acc 0.98
2016-09-07T17:08:15.966974: step 2908, loss 0.0236335, acc 1
2016-09-07T17:08:16.617104: step 2909, loss 0.0185765, acc 1
2016-09-07T17:08:16.985753: step 2910, loss 0.0406399, acc 1
2016-09-07T17:08:17.687453: step 2911, loss 0.0383119, acc 0.96
2016-09-07T17:08:18.356611: step 2912, loss 0.0385822, acc 1
2016-09-07T17:08:19.041240: step 2913, loss 0.0154326, acc 1
2016-09-07T17:08:19.712327: step 2914, loss 0.0312693, acc 1
2016-09-07T17:08:20.406470: step 2915, loss 0.0411004, acc 0.98
2016-09-07T17:08:21.089712: step 2916, loss 0.0666784, acc 0.98
2016-09-07T17:08:21.751484: step 2917, loss 0.0151178, acc 1
2016-09-07T17:08:22.423688: step 2918, loss 0.0670702, acc 0.94
2016-09-07T17:08:23.101408: step 2919, loss 0.226496, acc 0.96
2016-09-07T17:08:23.795032: step 2920, loss 0.0523501, acc 0.98
2016-09-07T17:08:24.479675: step 2921, loss 0.00984339, acc 1
2016-09-07T17:08:25.163405: step 2922, loss 0.0619334, acc 0.98
2016-09-07T17:08:25.842711: step 2923, loss 0.0555599, acc 1
2016-09-07T17:08:26.541759: step 2924, loss 0.0626132, acc 0.94
2016-09-07T17:08:27.245284: step 2925, loss 0.00250993, acc 1
2016-09-07T17:08:27.911623: step 2926, loss 0.0549112, acc 0.98
2016-09-07T17:08:28.569713: step 2927, loss 0.0924362, acc 0.98
2016-09-07T17:08:29.234033: step 2928, loss 0.00769762, acc 1
2016-09-07T17:08:29.899670: step 2929, loss 0.0313002, acc 0.96
2016-09-07T17:08:30.572424: step 2930, loss 0.0431568, acc 0.96
2016-09-07T17:08:31.254909: step 2931, loss 0.0166494, acc 0.98
2016-09-07T17:08:31.924590: step 2932, loss 0.0282246, acc 0.98
2016-09-07T17:08:32.603765: step 2933, loss 0.014833, acc 1
2016-09-07T17:08:33.273796: step 2934, loss 0.0079119, acc 1
2016-09-07T17:08:33.960013: step 2935, loss 0.00158607, acc 1
2016-09-07T17:08:34.635228: step 2936, loss 0.0629725, acc 0.98
2016-09-07T17:08:35.306213: step 2937, loss 0.0391032, acc 0.98
2016-09-07T17:08:35.977334: step 2938, loss 0.114582, acc 0.98
2016-09-07T17:08:36.639624: step 2939, loss 0.0168564, acc 1
2016-09-07T17:08:37.296074: step 2940, loss 0.0168872, acc 1
2016-09-07T17:08:37.957852: step 2941, loss 0.0179182, acc 0.98
2016-09-07T17:08:38.611834: step 2942, loss 0.061245, acc 0.96
2016-09-07T17:08:39.279919: step 2943, loss 0.0294721, acc 0.98
2016-09-07T17:08:39.929687: step 2944, loss 0.0163947, acc 0.98
2016-09-07T17:08:40.602626: step 2945, loss 0.0227292, acc 0.98
2016-09-07T17:08:41.266671: step 2946, loss 0.0615184, acc 0.96
2016-09-07T17:08:41.931716: step 2947, loss 0.0253009, acc 1
2016-09-07T17:08:42.619931: step 2948, loss 0.0187795, acc 1
2016-09-07T17:08:43.296373: step 2949, loss 0.0220936, acc 0.98
2016-09-07T17:08:43.961981: step 2950, loss 0.00493941, acc 1
2016-09-07T17:08:44.654794: step 2951, loss 0.0478652, acc 0.98
2016-09-07T17:08:45.331643: step 2952, loss 0.00142804, acc 1
2016-09-07T17:08:46.019378: step 2953, loss 0.00943223, acc 1
2016-09-07T17:08:46.690059: step 2954, loss 0.0703992, acc 0.96
2016-09-07T17:08:47.350972: step 2955, loss 0.0426433, acc 0.96
2016-09-07T17:08:48.038638: step 2956, loss 0.0515661, acc 0.98
2016-09-07T17:08:48.721029: step 2957, loss 0.00252318, acc 1
2016-09-07T17:08:49.380821: step 2958, loss 0.0417401, acc 0.98
2016-09-07T17:08:50.040289: step 2959, loss 0.0577932, acc 0.96
2016-09-07T17:08:50.709792: step 2960, loss 0.0130183, acc 1
2016-09-07T17:08:51.393693: step 2961, loss 0.023911, acc 1
2016-09-07T17:08:52.084440: step 2962, loss 0.00178368, acc 1
2016-09-07T17:08:52.744062: step 2963, loss 0.00498861, acc 1
2016-09-07T17:08:53.415465: step 2964, loss 0.0391313, acc 0.98
2016-09-07T17:08:54.095268: step 2965, loss 0.0127728, acc 1
2016-09-07T17:08:54.756010: step 2966, loss 0.0530899, acc 0.96
2016-09-07T17:08:55.419054: step 2967, loss 0.074297, acc 0.98
2016-09-07T17:08:56.093568: step 2968, loss 0.00127166, acc 1
2016-09-07T17:08:56.758422: step 2969, loss 0.0131839, acc 1
2016-09-07T17:08:57.422797: step 2970, loss 0.0683944, acc 0.98
2016-09-07T17:08:58.106089: step 2971, loss 0.0297298, acc 0.98
2016-09-07T17:08:58.795799: step 2972, loss 0.0132638, acc 1
2016-09-07T17:08:59.462138: step 2973, loss 0.0259026, acc 0.98
2016-09-07T17:09:00.122347: step 2974, loss 0.0266094, acc 0.98
2016-09-07T17:09:00.806189: step 2975, loss 0.0329016, acc 0.98
2016-09-07T17:09:01.489584: step 2976, loss 0.193464, acc 0.94
2016-09-07T17:09:02.149620: step 2977, loss 0.0120304, acc 1
2016-09-07T17:09:02.928466: step 2978, loss 0.0225581, acc 0.98
2016-09-07T17:09:03.590519: step 2979, loss 0.0464166, acc 0.98
2016-09-07T17:09:04.271671: step 2980, loss 0.062147, acc 0.96
2016-09-07T17:09:04.941058: step 2981, loss 0.00579214, acc 1
2016-09-07T17:09:05.595240: step 2982, loss 0.0659328, acc 0.94
2016-09-07T17:09:06.274108: step 2983, loss 0.0386635, acc 0.98
2016-09-07T17:09:06.948940: step 2984, loss 0.0359363, acc 0.98
2016-09-07T17:09:07.621788: step 2985, loss 0.0266283, acc 0.98
2016-09-07T17:09:08.294719: step 2986, loss 0.0785993, acc 0.94
2016-09-07T17:09:08.967417: step 2987, loss 0.0372366, acc 0.96
2016-09-07T17:09:09.639413: step 2988, loss 0.000165, acc 1
2016-09-07T17:09:10.299362: step 2989, loss 0.0593174, acc 0.98
2016-09-07T17:09:10.968166: step 2990, loss 0.0300185, acc 0.98
2016-09-07T17:09:11.629231: step 2991, loss 0.0138026, acc 1
2016-09-07T17:09:12.287492: step 2992, loss 0.0206745, acc 1
2016-09-07T17:09:12.957270: step 2993, loss 0.0237013, acc 0.98
2016-09-07T17:09:13.621459: step 2994, loss 0.0314741, acc 0.98
2016-09-07T17:09:14.295214: step 2995, loss 0.00940268, acc 1
2016-09-07T17:09:14.971211: step 2996, loss 0.0163038, acc 1
2016-09-07T17:09:15.637473: step 2997, loss 0.019878, acc 1
2016-09-07T17:09:16.303733: step 2998, loss 0.0167159, acc 1
2016-09-07T17:09:16.963730: step 2999, loss 0.0122579, acc 1
2016-09-07T17:09:17.636363: step 3000, loss 0.0274066, acc 0.98

Evaluation:
2016-09-07T17:09:20.552063: step 3000, loss 1.7514, acc 0.747

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-3000

2016-09-07T17:09:22.217155: step 3001, loss 0.0109724, acc 1
2016-09-07T17:09:22.890086: step 3002, loss 0.0375138, acc 0.98
2016-09-07T17:09:23.564985: step 3003, loss 0.00442673, acc 1
2016-09-07T17:09:24.244477: step 3004, loss 0.0340233, acc 0.98
2016-09-07T17:09:24.928326: step 3005, loss 0.0181523, acc 1
2016-09-07T17:09:25.605884: step 3006, loss 0.12809, acc 0.96
2016-09-07T17:09:26.289351: step 3007, loss 0.0178213, acc 1
2016-09-07T17:09:26.952690: step 3008, loss 0.0859567, acc 0.96
2016-09-07T17:09:27.617989: step 3009, loss 0.0285231, acc 0.98
2016-09-07T17:09:28.294636: step 3010, loss 0.0505513, acc 0.98
2016-09-07T17:09:28.976914: step 3011, loss 0.0180154, acc 1
2016-09-07T17:09:29.644421: step 3012, loss 0.208681, acc 0.96
2016-09-07T17:09:30.332281: step 3013, loss 0.0127953, acc 1
2016-09-07T17:09:30.991032: step 3014, loss 0.0131221, acc 1
2016-09-07T17:09:31.671775: step 3015, loss 0.0277491, acc 1
2016-09-07T17:09:32.346687: step 3016, loss 0.0202565, acc 0.98
2016-09-07T17:09:33.017715: step 3017, loss 0.0266249, acc 0.98
2016-09-07T17:09:33.685388: step 3018, loss 0.00606505, acc 1
2016-09-07T17:09:34.363601: step 3019, loss 0.0864441, acc 0.98
2016-09-07T17:09:35.042503: step 3020, loss 0.0159206, acc 1
2016-09-07T17:09:35.701162: step 3021, loss 0.0359654, acc 0.98
2016-09-07T17:09:36.365564: step 3022, loss 0.0337287, acc 0.98
2016-09-07T17:09:37.042999: step 3023, loss 0.00884745, acc 1
2016-09-07T17:09:37.695247: step 3024, loss 0.0840982, acc 0.96
2016-09-07T17:09:38.368229: step 3025, loss 0.00221962, acc 1
2016-09-07T17:09:39.020798: step 3026, loss 0.0210938, acc 1
2016-09-07T17:09:39.690893: step 3027, loss 0.159033, acc 0.98
2016-09-07T17:09:40.366763: step 3028, loss 0.0378567, acc 0.96
2016-09-07T17:09:41.032783: step 3029, loss 0.00711846, acc 1
2016-09-07T17:09:41.694106: step 3030, loss 0.0377031, acc 0.98
2016-09-07T17:09:42.365699: step 3031, loss 0.0217064, acc 0.98
2016-09-07T17:09:43.017120: step 3032, loss 0.0190215, acc 1
2016-09-07T17:09:43.664498: step 3033, loss 0.00167196, acc 1
2016-09-07T17:09:44.339839: step 3034, loss 0.0166995, acc 1
2016-09-07T17:09:45.018055: step 3035, loss 0.0442553, acc 0.96
2016-09-07T17:09:45.693060: step 3036, loss 0.0607959, acc 0.98
2016-09-07T17:09:46.358168: step 3037, loss 0.0278754, acc 1
2016-09-07T17:09:47.021997: step 3038, loss 0.0167005, acc 1
2016-09-07T17:09:47.689079: step 3039, loss 0.0144885, acc 1
2016-09-07T17:09:48.354663: step 3040, loss 0.0598248, acc 0.96
2016-09-07T17:09:49.035674: step 3041, loss 0.0289646, acc 1
2016-09-07T17:09:49.719325: step 3042, loss 0.0253155, acc 0.98
2016-09-07T17:09:50.394380: step 3043, loss 0.012824, acc 1
2016-09-07T17:09:51.056826: step 3044, loss 0.015076, acc 1
2016-09-07T17:09:51.719514: step 3045, loss 0.0648867, acc 0.98
2016-09-07T17:09:52.376616: step 3046, loss 0.00830795, acc 1
2016-09-07T17:09:53.040303: step 3047, loss 0.00832891, acc 1
2016-09-07T17:09:53.699903: step 3048, loss 0.0315915, acc 0.98
2016-09-07T17:09:54.371143: step 3049, loss 0.0460025, acc 0.96
2016-09-07T17:09:55.035661: step 3050, loss 0.0744236, acc 0.96
2016-09-07T17:09:55.694879: step 3051, loss 0.0459377, acc 0.96
2016-09-07T17:09:56.359994: step 3052, loss 0.021996, acc 1
2016-09-07T17:09:57.053905: step 3053, loss 0.0143413, acc 1
2016-09-07T17:09:57.707875: step 3054, loss 0.0898783, acc 0.96
2016-09-07T17:09:58.366721: step 3055, loss 0.0172478, acc 0.98
2016-09-07T17:09:59.040307: step 3056, loss 0.0145012, acc 1
2016-09-07T17:09:59.736079: step 3057, loss 0.0326912, acc 0.98
2016-09-07T17:10:00.436905: step 3058, loss 0.0473532, acc 0.96
2016-09-07T17:10:01.117671: step 3059, loss 0.027602, acc 0.98
2016-09-07T17:10:01.873135: step 3060, loss 0.024671, acc 0.98
2016-09-07T17:10:02.563130: step 3061, loss 0.0340765, acc 0.98
2016-09-07T17:10:03.416301: step 3062, loss 0.0905259, acc 0.94
2016-09-07T17:10:04.189138: step 3063, loss 0.0931082, acc 0.96
2016-09-07T17:10:04.968045: step 3064, loss 0.0202879, acc 1
2016-09-07T17:10:05.780192: step 3065, loss 0.0105247, acc 1
2016-09-07T17:10:06.591767: step 3066, loss 0.00739833, acc 1
2016-09-07T17:10:07.352603: step 3067, loss 0.0135353, acc 1
2016-09-07T17:10:08.201443: step 3068, loss 0.0204033, acc 0.98
2016-09-07T17:10:09.040399: step 3069, loss 0.0108128, acc 1
2016-09-07T17:10:09.838649: step 3070, loss 0.0189495, acc 1
2016-09-07T17:10:10.611523: step 3071, loss 0.0233041, acc 1
2016-09-07T17:10:11.409293: step 3072, loss 0.0263248, acc 1
2016-09-07T17:10:12.265038: step 3073, loss 0.00788752, acc 1
2016-09-07T17:10:13.079927: step 3074, loss 0.0384709, acc 1
2016-09-07T17:10:13.925653: step 3075, loss 0.0106819, acc 1
2016-09-07T17:10:14.675061: step 3076, loss 0.0509359, acc 0.98
2016-09-07T17:10:15.432661: step 3077, loss 0.00457444, acc 1
2016-09-07T17:10:16.185894: step 3078, loss 0.0550991, acc 0.98
2016-09-07T17:10:16.894677: step 3079, loss 0.0170228, acc 1
2016-09-07T17:10:17.560199: step 3080, loss 0.0641328, acc 0.98
2016-09-07T17:10:18.370725: step 3081, loss 0.0340974, acc 1
2016-09-07T17:10:19.078520: step 3082, loss 0.0696922, acc 0.96
2016-09-07T17:10:19.818139: step 3083, loss 0.0469694, acc 0.96
2016-09-07T17:10:20.560624: step 3084, loss 0.0357749, acc 0.98
2016-09-07T17:10:21.339068: step 3085, loss 0.0226607, acc 1
2016-09-07T17:10:22.081648: step 3086, loss 0.00998806, acc 1
2016-09-07T17:10:22.753902: step 3087, loss 0.00781964, acc 1
2016-09-07T17:10:23.476451: step 3088, loss 0.018173, acc 1
2016-09-07T17:10:24.146106: step 3089, loss 0.0493231, acc 0.98
2016-09-07T17:10:24.909627: step 3090, loss 0.0381059, acc 0.98
2016-09-07T17:10:25.571616: step 3091, loss 0.00479915, acc 1
2016-09-07T17:10:26.266620: step 3092, loss 0.037104, acc 0.98
2016-09-07T17:10:26.976391: step 3093, loss 0.0253061, acc 1
2016-09-07T17:10:27.662427: step 3094, loss 0.022225, acc 1
2016-09-07T17:10:28.542353: step 3095, loss 0.0268985, acc 0.98
2016-09-07T17:10:29.213322: step 3096, loss 0.0244873, acc 0.98
2016-09-07T17:10:29.985754: step 3097, loss 0.0265041, acc 1
2016-09-07T17:10:30.633827: step 3098, loss 0.00575751, acc 1
2016-09-07T17:10:31.321683: step 3099, loss 0.0261008, acc 1
2016-09-07T17:10:31.976960: step 3100, loss 0.049073, acc 0.98

Evaluation:
2016-09-07T17:10:34.881669: step 3100, loss 2.162, acc 0.753

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-3100

2016-09-07T17:10:36.598779: step 3101, loss 0.113046, acc 0.92
2016-09-07T17:10:37.340070: step 3102, loss 0.0132641, acc 1
2016-09-07T17:10:38.092499: step 3103, loss 0.000732241, acc 1
2016-09-07T17:10:38.460577: step 3104, loss 0.000388202, acc 1
2016-09-07T17:10:39.269446: step 3105, loss 0.043291, acc 1
2016-09-07T17:10:40.020794: step 3106, loss 0.0587058, acc 0.98
2016-09-07T17:10:40.780369: step 3107, loss 0.037681, acc 1
2016-09-07T17:10:41.574052: step 3108, loss 0.0948691, acc 0.96
2016-09-07T17:10:42.325927: step 3109, loss 0.123799, acc 0.96
2016-09-07T17:10:42.973468: step 3110, loss 0.0365138, acc 1
2016-09-07T17:10:43.693024: step 3111, loss 0.0162964, acc 0.98
2016-09-07T17:10:44.430927: step 3112, loss 0.0719547, acc 0.94
2016-09-07T17:10:45.109053: step 3113, loss 0.0181859, acc 0.98
2016-09-07T17:10:45.800539: step 3114, loss 0.0177208, acc 1
2016-09-07T17:10:46.510531: step 3115, loss 0.00135144, acc 1
2016-09-07T17:10:47.247330: step 3116, loss 0.031496, acc 0.98
2016-09-07T17:10:47.914746: step 3117, loss 0.0185896, acc 1
2016-09-07T17:10:48.607773: step 3118, loss 0.0346758, acc 0.98
2016-09-07T17:10:49.411346: step 3119, loss 0.173663, acc 0.96
2016-09-07T17:10:50.168631: step 3120, loss 0.0866267, acc 0.94
2016-09-07T17:10:50.987562: step 3121, loss 0.0751325, acc 0.96
2016-09-07T17:10:51.796638: step 3122, loss 0.0174365, acc 1
2016-09-07T17:10:52.460417: step 3123, loss 0.085516, acc 0.94
2016-09-07T17:10:53.121648: step 3124, loss 0.00744097, acc 1
2016-09-07T17:10:53.928377: step 3125, loss 0.0426062, acc 0.98
2016-09-07T17:10:54.711816: step 3126, loss 0.0318898, acc 0.98
2016-09-07T17:10:55.538823: step 3127, loss 0.0161864, acc 0.98
2016-09-07T17:10:56.268807: step 3128, loss 0.0180302, acc 1
2016-09-07T17:10:57.050153: step 3129, loss 0.13511, acc 0.9
2016-09-07T17:10:57.839432: step 3130, loss 0.00796096, acc 1
2016-09-07T17:10:58.631605: step 3131, loss 0.0045326, acc 1
2016-09-07T17:10:59.409456: step 3132, loss 0.0076935, acc 1
2016-09-07T17:11:00.185551: step 3133, loss 0.00718233, acc 1
2016-09-07T17:11:00.952081: step 3134, loss 0.0223737, acc 0.98
2016-09-07T17:11:01.710380: step 3135, loss 0.0137878, acc 1
2016-09-07T17:11:02.439960: step 3136, loss 0.0541538, acc 0.98
2016-09-07T17:11:03.259031: step 3137, loss 0.0706237, acc 0.96
2016-09-07T17:11:04.013295: step 3138, loss 0.0416735, acc 0.98
2016-09-07T17:11:04.775225: step 3139, loss 0.0355168, acc 0.98
2016-09-07T17:11:05.502767: step 3140, loss 0.138638, acc 0.94
2016-09-07T17:11:06.327354: step 3141, loss 0.191039, acc 0.96
2016-09-07T17:11:07.043505: step 3142, loss 0.0608468, acc 0.96
2016-09-07T17:11:07.917696: step 3143, loss 0.0168475, acc 0.98
2016-09-07T17:11:08.700871: step 3144, loss 0.159375, acc 0.98
2016-09-07T17:11:09.532322: step 3145, loss 0.0885189, acc 0.9
2016-09-07T17:11:10.353119: step 3146, loss 0.0180348, acc 1
2016-09-07T17:11:11.134071: step 3147, loss 0.00170442, acc 1
2016-09-07T17:11:11.927279: step 3148, loss 0.0378001, acc 0.96
2016-09-07T17:11:12.768063: step 3149, loss 0.112396, acc 0.98
2016-09-07T17:11:13.576776: step 3150, loss 0.100655, acc 0.96
2016-09-07T17:11:14.346466: step 3151, loss 0.0851794, acc 0.98
2016-09-07T17:11:15.070725: step 3152, loss 0.0341884, acc 0.96
2016-09-07T17:11:15.858914: step 3153, loss 0.0049267, acc 1
2016-09-07T17:11:16.589581: step 3154, loss 0.0779275, acc 0.96
2016-09-07T17:11:17.319571: step 3155, loss 0.0500989, acc 0.98
2016-09-07T17:11:18.083829: step 3156, loss 0.0206465, acc 0.98
2016-09-07T17:11:18.755260: step 3157, loss 0.0192814, acc 1
2016-09-07T17:11:19.429383: step 3158, loss 0.04556, acc 0.98
2016-09-07T17:11:20.183864: step 3159, loss 0.0400496, acc 0.98
2016-09-07T17:11:20.854671: step 3160, loss 0.0608224, acc 0.98
2016-09-07T17:11:21.551892: step 3161, loss 0.0247687, acc 0.98
2016-09-07T17:11:22.217878: step 3162, loss 0.0292886, acc 0.98
2016-09-07T17:11:22.901145: step 3163, loss 0.0269525, acc 1
2016-09-07T17:11:23.573796: step 3164, loss 0.0237979, acc 1
2016-09-07T17:11:24.240543: step 3165, loss 0.0274785, acc 1
2016-09-07T17:11:25.047193: step 3166, loss 0.0299415, acc 0.98
2016-09-07T17:11:25.818000: step 3167, loss 0.0847659, acc 0.98
2016-09-07T17:11:26.634867: step 3168, loss 0.0263573, acc 1
2016-09-07T17:11:27.455286: step 3169, loss 0.0519924, acc 0.98
2016-09-07T17:11:28.282536: step 3170, loss 0.0585443, acc 0.96
2016-09-07T17:11:28.965906: step 3171, loss 0.000827006, acc 1
2016-09-07T17:11:29.624727: step 3172, loss 0.0123268, acc 1
2016-09-07T17:11:30.287132: step 3173, loss 0.0612326, acc 0.96
2016-09-07T17:11:30.971892: step 3174, loss 0.0396599, acc 0.96
2016-09-07T17:11:31.668819: step 3175, loss 0.00396509, acc 1
2016-09-07T17:11:32.347611: step 3176, loss 0.0240027, acc 0.98
2016-09-07T17:11:33.025030: step 3177, loss 0.00285366, acc 1
2016-09-07T17:11:33.697761: step 3178, loss 0.00226687, acc 1
2016-09-07T17:11:34.392469: step 3179, loss 0.00446851, acc 1
2016-09-07T17:11:35.087889: step 3180, loss 0.0421046, acc 0.96
2016-09-07T17:11:35.779719: step 3181, loss 0.0129185, acc 1
2016-09-07T17:11:36.433720: step 3182, loss 0.00217881, acc 1
2016-09-07T17:11:37.090574: step 3183, loss 0.0168139, acc 1
2016-09-07T17:11:37.760738: step 3184, loss 0.0103147, acc 1
2016-09-07T17:11:38.448218: step 3185, loss 0.0623943, acc 0.98
2016-09-07T17:11:39.134663: step 3186, loss 0.0680037, acc 0.96
2016-09-07T17:11:39.806883: step 3187, loss 0.0473894, acc 0.96
2016-09-07T17:11:40.476572: step 3188, loss 0.00634547, acc 1
2016-09-07T17:11:41.136355: step 3189, loss 0.0282558, acc 0.98
2016-09-07T17:11:41.812705: step 3190, loss 0.066933, acc 0.98
2016-09-07T17:11:42.499710: step 3191, loss 0.00852171, acc 1
2016-09-07T17:11:43.157778: step 3192, loss 0.0228768, acc 1
2016-09-07T17:11:43.814916: step 3193, loss 0.00784435, acc 1
2016-09-07T17:11:44.510200: step 3194, loss 0.00909215, acc 1
2016-09-07T17:11:45.181075: step 3195, loss 0.0338663, acc 0.98
2016-09-07T17:11:45.860094: step 3196, loss 0.234122, acc 0.94
2016-09-07T17:11:46.515156: step 3197, loss 0.0333631, acc 1
2016-09-07T17:11:47.210401: step 3198, loss 0.00938777, acc 1
2016-09-07T17:11:47.902241: step 3199, loss 0.0348152, acc 0.98
2016-09-07T17:11:48.584896: step 3200, loss 0.109226, acc 0.98

Evaluation:
2016-09-07T17:11:51.819526: step 3200, loss 2.00262, acc 0.75

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-3200

2016-09-07T17:11:53.562766: step 3201, loss 0.12435, acc 0.98
2016-09-07T17:11:54.230934: step 3202, loss 0.155934, acc 0.9
2016-09-07T17:11:54.910694: step 3203, loss 0.0128994, acc 1
2016-09-07T17:11:55.577112: step 3204, loss 0.0406007, acc 0.96
2016-09-07T17:11:56.242916: step 3205, loss 0.0231465, acc 0.98
2016-09-07T17:11:56.917363: step 3206, loss 0.0547475, acc 0.98
2016-09-07T17:11:57.661895: step 3207, loss 0.0624107, acc 0.98
2016-09-07T17:11:58.455787: step 3208, loss 0.0284839, acc 0.98
2016-09-07T17:11:59.212182: step 3209, loss 0.0166898, acc 1
2016-09-07T17:11:59.996528: step 3210, loss 0.0233043, acc 1
2016-09-07T17:12:00.825398: step 3211, loss 0.0476882, acc 0.96
2016-09-07T17:12:01.477788: step 3212, loss 0.0588753, acc 0.98
2016-09-07T17:12:02.133542: step 3213, loss 0.0348582, acc 0.98
2016-09-07T17:12:02.810183: step 3214, loss 0.0100743, acc 1
2016-09-07T17:12:03.477793: step 3215, loss 0.0830589, acc 0.96
2016-09-07T17:12:04.160972: step 3216, loss 0.00508952, acc 1
2016-09-07T17:12:04.814330: step 3217, loss 0.0392578, acc 0.98
2016-09-07T17:12:05.467164: step 3218, loss 0.0288327, acc 0.98
2016-09-07T17:12:06.120056: step 3219, loss 0.0642004, acc 0.96
2016-09-07T17:12:06.802055: step 3220, loss 0.0482958, acc 0.98
2016-09-07T17:12:07.497638: step 3221, loss 0.116753, acc 0.98
2016-09-07T17:12:08.172181: step 3222, loss 0.0130376, acc 1
2016-09-07T17:12:08.848665: step 3223, loss 0.0640446, acc 0.98
2016-09-07T17:12:09.535498: step 3224, loss 0.0383215, acc 0.98
2016-09-07T17:12:10.309310: step 3225, loss 0.0554483, acc 0.96
2016-09-07T17:12:11.026659: step 3226, loss 0.0227292, acc 0.98
2016-09-07T17:12:11.710249: step 3227, loss 0.00429718, acc 1
2016-09-07T17:12:12.376632: step 3228, loss 0.0100223, acc 1
2016-09-07T17:12:13.050071: step 3229, loss 0.0801749, acc 0.98
2016-09-07T17:12:13.710027: step 3230, loss 0.20569, acc 0.94
2016-09-07T17:12:14.372887: step 3231, loss 0.148422, acc 0.98
2016-09-07T17:12:15.050091: step 3232, loss 0.017628, acc 1
2016-09-07T17:12:15.726608: step 3233, loss 0.0248579, acc 1
2016-09-07T17:12:16.390001: step 3234, loss 0.0702574, acc 0.98
2016-09-07T17:12:17.057449: step 3235, loss 0.0195778, acc 1
2016-09-07T17:12:17.737131: step 3236, loss 0.0624556, acc 0.98
2016-09-07T17:12:18.414665: step 3237, loss 0.00628913, acc 1
2016-09-07T17:12:19.089255: step 3238, loss 0.0400562, acc 0.98
2016-09-07T17:12:19.759405: step 3239, loss 0.0341012, acc 0.98
2016-09-07T17:12:20.427367: step 3240, loss 0.0746297, acc 0.98
2016-09-07T17:12:21.095944: step 3241, loss 0.0616543, acc 0.96
2016-09-07T17:12:21.762181: step 3242, loss 0.00939308, acc 1
2016-09-07T17:12:22.449390: step 3243, loss 0.017602, acc 1
2016-09-07T17:12:23.116362: step 3244, loss 0.0850882, acc 0.96
2016-09-07T17:12:23.769544: step 3245, loss 0.0468858, acc 0.98
2016-09-07T17:12:24.463598: step 3246, loss 0.075162, acc 0.98
2016-09-07T17:12:25.138671: step 3247, loss 0.0244847, acc 0.98
2016-09-07T17:12:25.804486: step 3248, loss 0.0416845, acc 0.98
2016-09-07T17:12:26.487278: step 3249, loss 0.0546487, acc 0.98
2016-09-07T17:12:27.147675: step 3250, loss 0.00769331, acc 1
2016-09-07T17:12:27.857155: step 3251, loss 0.133385, acc 0.96
2016-09-07T17:12:28.545871: step 3252, loss 0.123393, acc 0.98
2016-09-07T17:12:29.215591: step 3253, loss 0.139239, acc 0.96
2016-09-07T17:12:29.868996: step 3254, loss 0.0419896, acc 0.98
2016-09-07T17:12:30.532587: step 3255, loss 0.0593028, acc 0.98
2016-09-07T17:12:31.213994: step 3256, loss 0.0185628, acc 0.98
2016-09-07T17:12:31.890832: step 3257, loss 0.0758046, acc 0.94
2016-09-07T17:12:32.573046: step 3258, loss 0.0689104, acc 0.94
2016-09-07T17:12:33.279462: step 3259, loss 0.0972134, acc 0.98
2016-09-07T17:12:33.942173: step 3260, loss 0.0157896, acc 1
2016-09-07T17:12:34.599362: step 3261, loss 0.0815724, acc 0.98
2016-09-07T17:12:35.291218: step 3262, loss 0.0301691, acc 0.98
2016-09-07T17:12:35.947625: step 3263, loss 0.0117403, acc 1
2016-09-07T17:12:36.600307: step 3264, loss 0.0363738, acc 0.98
2016-09-07T17:12:37.275090: step 3265, loss 0.0315875, acc 1
2016-09-07T17:12:37.939489: step 3266, loss 0.053117, acc 0.96
2016-09-07T17:12:38.630511: step 3267, loss 0.0298364, acc 1
2016-09-07T17:12:39.306814: step 3268, loss 0.0055961, acc 1
2016-09-07T17:12:39.980495: step 3269, loss 0.0096111, acc 1
2016-09-07T17:12:40.648600: step 3270, loss 0.0269209, acc 1
2016-09-07T17:12:41.316959: step 3271, loss 0.0176971, acc 1
2016-09-07T17:12:41.976583: step 3272, loss 0.0218446, acc 1
2016-09-07T17:12:42.650577: step 3273, loss 0.0391739, acc 1
2016-09-07T17:12:43.312899: step 3274, loss 0.0342902, acc 1
2016-09-07T17:12:43.990606: step 3275, loss 0.0600892, acc 0.96
2016-09-07T17:12:44.676716: step 3276, loss 0.0234115, acc 0.98
2016-09-07T17:12:45.350714: step 3277, loss 0.0224284, acc 0.98
2016-09-07T17:12:46.033007: step 3278, loss 0.00289133, acc 1
2016-09-07T17:12:46.694696: step 3279, loss 0.0440553, acc 0.98
2016-09-07T17:12:47.362010: step 3280, loss 0.0827494, acc 0.96
2016-09-07T17:12:48.020559: step 3281, loss 0.00368907, acc 1
2016-09-07T17:12:48.713322: step 3282, loss 0.0188838, acc 1
2016-09-07T17:12:49.389325: step 3283, loss 0.0723903, acc 0.96
2016-09-07T17:12:50.064102: step 3284, loss 0.0754168, acc 0.94
2016-09-07T17:12:50.744013: step 3285, loss 0.0397469, acc 0.98
2016-09-07T17:12:51.429720: step 3286, loss 0.0820451, acc 0.98
2016-09-07T17:12:52.096248: step 3287, loss 0.182426, acc 0.96
2016-09-07T17:12:52.758003: step 3288, loss 0.00780659, acc 1
2016-09-07T17:12:53.438668: step 3289, loss 0.0011291, acc 1
2016-09-07T17:12:54.121669: step 3290, loss 0.059167, acc 0.98
2016-09-07T17:12:54.804984: step 3291, loss 0.116164, acc 0.94
2016-09-07T17:12:55.499482: step 3292, loss 0.0513368, acc 0.96
2016-09-07T17:12:56.175364: step 3293, loss 0.0395795, acc 0.98
2016-09-07T17:12:56.855415: step 3294, loss 0.0301793, acc 0.98
2016-09-07T17:12:57.523291: step 3295, loss 0.0682103, acc 0.96
2016-09-07T17:12:58.171728: step 3296, loss 0.0439402, acc 0.98
2016-09-07T17:12:58.826406: step 3297, loss 0.0111105, acc 1
2016-09-07T17:12:59.209074: step 3298, loss 0.0849134, acc 0.916667
2016-09-07T17:12:59.882202: step 3299, loss 0.0884155, acc 0.94
2016-09-07T17:13:00.571576: step 3300, loss 0.0178851, acc 0.98

Evaluation:
2016-09-07T17:13:03.551434: step 3300, loss 1.53353, acc 0.756

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-3300

2016-09-07T17:13:05.136099: step 3301, loss 0.0263588, acc 0.98
2016-09-07T17:13:05.810685: step 3302, loss 0.0153366, acc 1
2016-09-07T17:13:06.501574: step 3303, loss 0.0114669, acc 1
2016-09-07T17:13:07.166626: step 3304, loss 0.0458151, acc 0.94
2016-09-07T17:13:07.850171: step 3305, loss 0.195731, acc 0.96
2016-09-07T17:13:08.519289: step 3306, loss 0.0458415, acc 0.98
2016-09-07T17:13:09.184551: step 3307, loss 0.00331384, acc 1
2016-09-07T17:13:09.875443: step 3308, loss 0.0231564, acc 1
2016-09-07T17:13:10.560706: step 3309, loss 0.0020022, acc 1
2016-09-07T17:13:11.240678: step 3310, loss 0.0895497, acc 0.98
2016-09-07T17:13:11.918612: step 3311, loss 0.141604, acc 0.92
2016-09-07T17:13:12.585985: step 3312, loss 0.0410615, acc 0.98
2016-09-07T17:13:13.259114: step 3313, loss 0.0311604, acc 0.98
2016-09-07T17:13:13.936447: step 3314, loss 0.0187131, acc 1
2016-09-07T17:13:14.619821: step 3315, loss 0.0357339, acc 0.96
2016-09-07T17:13:15.293647: step 3316, loss 0.0613219, acc 0.98
2016-09-07T17:13:15.963710: step 3317, loss 0.0228733, acc 1
2016-09-07T17:13:16.643477: step 3318, loss 0.100413, acc 0.94
2016-09-07T17:13:17.316819: step 3319, loss 0.0203764, acc 1
2016-09-07T17:13:18.004354: step 3320, loss 0.0986573, acc 0.9
2016-09-07T17:13:18.688896: step 3321, loss 0.0504761, acc 0.98
2016-09-07T17:13:19.365429: step 3322, loss 0.0264823, acc 0.98
2016-09-07T17:13:20.042542: step 3323, loss 0.0336709, acc 0.98
2016-09-07T17:13:20.711861: step 3324, loss 0.0639374, acc 0.96
2016-09-07T17:13:21.380186: step 3325, loss 0.0215786, acc 0.98
2016-09-07T17:13:22.056170: step 3326, loss 0.0103649, acc 1
2016-09-07T17:13:22.731513: step 3327, loss 0.0669003, acc 0.98
2016-09-07T17:13:23.414964: step 3328, loss 0.0426418, acc 0.98
2016-09-07T17:13:24.081993: step 3329, loss 0.0187285, acc 1
2016-09-07T17:13:24.751023: step 3330, loss 0.0564755, acc 0.96
2016-09-07T17:13:25.400304: step 3331, loss 0.0196416, acc 0.98
2016-09-07T17:13:26.072182: step 3332, loss 0.038659, acc 0.98
2016-09-07T17:13:26.749462: step 3333, loss 0.0591605, acc 0.96
2016-09-07T17:13:27.421282: step 3334, loss 0.0720367, acc 0.94
2016-09-07T17:13:28.080185: step 3335, loss 0.03061, acc 0.96
2016-09-07T17:13:28.739242: step 3336, loss 0.0693492, acc 0.98
2016-09-07T17:13:29.419548: step 3337, loss 0.00175407, acc 1
2016-09-07T17:13:30.078263: step 3338, loss 0.00883215, acc 1
2016-09-07T17:13:30.758765: step 3339, loss 0.0409071, acc 1
2016-09-07T17:13:31.411695: step 3340, loss 0.0406183, acc 1
2016-09-07T17:13:32.061223: step 3341, loss 0.00262641, acc 1
2016-09-07T17:13:32.722459: step 3342, loss 0.00113737, acc 1
2016-09-07T17:13:33.410446: step 3343, loss 0.0452691, acc 1
2016-09-07T17:13:34.251624: step 3344, loss 0.0477359, acc 0.98
2016-09-07T17:13:35.000013: step 3345, loss 0.00284293, acc 1
2016-09-07T17:13:35.778260: step 3346, loss 0.0573532, acc 0.96
2016-09-07T17:13:36.457763: step 3347, loss 0.0166961, acc 0.98
2016-09-07T17:13:37.130916: step 3348, loss 0.0267704, acc 1
2016-09-07T17:13:37.822361: step 3349, loss 0.0252642, acc 0.98
2016-09-07T17:13:38.640327: step 3350, loss 0.0213492, acc 0.98
2016-09-07T17:13:39.393586: step 3351, loss 0.023875, acc 0.98
2016-09-07T17:13:40.081948: step 3352, loss 0.0275335, acc 0.98
2016-09-07T17:13:40.779508: step 3353, loss 0.0482345, acc 0.96
2016-09-07T17:13:41.454928: step 3354, loss 0.0839137, acc 0.96
2016-09-07T17:13:42.125262: step 3355, loss 0.0790217, acc 0.96
2016-09-07T17:13:42.841751: step 3356, loss 0.0182497, acc 0.98
2016-09-07T17:13:43.511832: step 3357, loss 0.060124, acc 0.96
2016-09-07T17:13:44.180566: step 3358, loss 0.0029561, acc 1
2016-09-07T17:13:44.867332: step 3359, loss 0.0428318, acc 0.98
2016-09-07T17:13:45.544044: step 3360, loss 0.017869, acc 1
2016-09-07T17:13:46.234001: step 3361, loss 0.0312347, acc 0.96
2016-09-07T17:13:46.929901: step 3362, loss 0.135734, acc 0.96
2016-09-07T17:13:47.604455: step 3363, loss 0.12199, acc 0.92
2016-09-07T17:13:48.371399: step 3364, loss 0.0875898, acc 0.96
2016-09-07T17:13:49.047451: step 3365, loss 0.0183534, acc 1
2016-09-07T17:13:49.744833: step 3366, loss 0.00764179, acc 1
2016-09-07T17:13:50.435725: step 3367, loss 0.00647861, acc 1
2016-09-07T17:13:51.120480: step 3368, loss 0.0248348, acc 0.98
2016-09-07T17:13:51.795153: step 3369, loss 0.00285975, acc 1
2016-09-07T17:13:52.502477: step 3370, loss 0.0216869, acc 1
2016-09-07T17:13:53.191822: step 3371, loss 0.00554076, acc 1
2016-09-07T17:13:53.880149: step 3372, loss 0.0400775, acc 1
2016-09-07T17:13:54.543168: step 3373, loss 0.0190958, acc 0.98
2016-09-07T17:13:55.243222: step 3374, loss 0.0599734, acc 0.96
2016-09-07T17:13:55.979173: step 3375, loss 0.000363017, acc 1
2016-09-07T17:13:56.659115: step 3376, loss 0.064381, acc 0.94
2016-09-07T17:13:57.330373: step 3377, loss 0.0502366, acc 0.96
2016-09-07T17:13:58.030337: step 3378, loss 0.0164145, acc 1
2016-09-07T17:13:58.707019: step 3379, loss 0.0461858, acc 0.98
2016-09-07T17:13:59.378294: step 3380, loss 0.0388531, acc 0.98
2016-09-07T17:14:00.046243: step 3381, loss 0.016387, acc 1
2016-09-07T17:14:00.701666: step 3382, loss 0.00355232, acc 1
2016-09-07T17:14:01.368720: step 3383, loss 0.0216946, acc 0.98
2016-09-07T17:14:02.063068: step 3384, loss 0.0219147, acc 0.98
2016-09-07T17:14:02.713467: step 3385, loss 0.0420987, acc 0.98
2016-09-07T17:14:03.381461: step 3386, loss 0.0617648, acc 0.98
2016-09-07T17:14:04.054340: step 3387, loss 0.0669861, acc 0.96
2016-09-07T17:14:04.708036: step 3388, loss 0.0135614, acc 1
2016-09-07T17:14:05.371698: step 3389, loss 0.0210841, acc 1
2016-09-07T17:14:06.054477: step 3390, loss 0.0548781, acc 0.98
2016-09-07T17:14:06.724055: step 3391, loss 0.076158, acc 0.94
2016-09-07T17:14:07.391098: step 3392, loss 0.00159473, acc 1
2016-09-07T17:14:08.105539: step 3393, loss 0.056791, acc 0.96
2016-09-07T17:14:08.782358: step 3394, loss 0.01947, acc 1
2016-09-07T17:14:09.464509: step 3395, loss 0.0183927, acc 1
2016-09-07T17:14:10.146255: step 3396, loss 0.0605827, acc 0.96
2016-09-07T17:14:10.834299: step 3397, loss 0.0123844, acc 1
2016-09-07T17:14:11.517522: step 3398, loss 0.0189918, acc 1
2016-09-07T17:14:12.277372: step 3399, loss 0.0331329, acc 0.98
2016-09-07T17:14:13.008259: step 3400, loss 0.0150361, acc 1

Evaluation:
2016-09-07T17:14:16.230224: step 3400, loss 2.12053, acc 0.737

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-3400

2016-09-07T17:14:17.841887: step 3401, loss 0.0252718, acc 1
2016-09-07T17:14:18.556920: step 3402, loss 0.0651225, acc 0.96
2016-09-07T17:14:19.365246: step 3403, loss 0.00533415, acc 1
2016-09-07T17:14:20.031629: step 3404, loss 0.03701, acc 0.98
2016-09-07T17:14:20.770131: step 3405, loss 0.124579, acc 0.96
2016-09-07T17:14:21.461996: step 3406, loss 0.0115522, acc 1
2016-09-07T17:14:22.255168: step 3407, loss 0.0484146, acc 0.98
2016-09-07T17:14:22.945714: step 3408, loss 0.00139416, acc 1
2016-09-07T17:14:23.627017: step 3409, loss 0.0102783, acc 1
2016-09-07T17:14:24.373613: step 3410, loss 0.0133875, acc 1
2016-09-07T17:14:25.062194: step 3411, loss 0.163739, acc 0.94
2016-09-07T17:14:25.779889: step 3412, loss 0.0819075, acc 0.96
2016-09-07T17:14:26.495389: step 3413, loss 0.0505324, acc 0.96
2016-09-07T17:14:27.356281: step 3414, loss 0.121399, acc 0.94
2016-09-07T17:14:28.055791: step 3415, loss 0.00501733, acc 1
2016-09-07T17:14:28.790802: step 3416, loss 0.00268377, acc 1
2016-09-07T17:14:29.658918: step 3417, loss 0.0115406, acc 1
2016-09-07T17:14:30.421074: step 3418, loss 0.0147733, acc 1
2016-09-07T17:14:31.318420: step 3419, loss 0.0999575, acc 0.98
2016-09-07T17:14:32.054652: step 3420, loss 0.0430043, acc 0.96
2016-09-07T17:14:32.816661: step 3421, loss 0.0262378, acc 1
2016-09-07T17:14:33.585710: step 3422, loss 0.0186595, acc 0.98
2016-09-07T17:14:34.295456: step 3423, loss 0.265906, acc 0.94
2016-09-07T17:14:35.070148: step 3424, loss 0.0177331, acc 1
2016-09-07T17:14:35.815539: step 3425, loss 0.0226928, acc 1
2016-09-07T17:14:36.567227: step 3426, loss 0.0305171, acc 1
2016-09-07T17:14:37.378412: step 3427, loss 0.0267647, acc 0.98
2016-09-07T17:14:38.136664: step 3428, loss 0.162883, acc 0.96
2016-09-07T17:14:38.884187: step 3429, loss 0.0354453, acc 0.96
2016-09-07T17:14:39.628195: step 3430, loss 0.0619744, acc 0.98
2016-09-07T17:14:40.321326: step 3431, loss 0.0453952, acc 0.98
2016-09-07T17:14:41.063506: step 3432, loss 0.0900141, acc 0.94
2016-09-07T17:14:41.816047: step 3433, loss 0.0716938, acc 0.96
2016-09-07T17:14:42.554422: step 3434, loss 0.126179, acc 0.96
2016-09-07T17:14:43.344395: step 3435, loss 0.0883183, acc 0.98
2016-09-07T17:14:44.080756: step 3436, loss 0.00733048, acc 1
2016-09-07T17:14:44.830900: step 3437, loss 0.0139767, acc 1
2016-09-07T17:14:45.635722: step 3438, loss 0.0545029, acc 0.98
2016-09-07T17:14:46.323418: step 3439, loss 0.101417, acc 0.96
2016-09-07T17:14:47.084803: step 3440, loss 0.0346445, acc 0.98
2016-09-07T17:14:47.840335: step 3441, loss 0.0339013, acc 0.98
2016-09-07T17:14:48.562262: step 3442, loss 0.0125257, acc 1
2016-09-07T17:14:49.343728: step 3443, loss 0.0946303, acc 0.98
2016-09-07T17:14:50.072665: step 3444, loss 0.0563459, acc 0.98
2016-09-07T17:14:50.801764: step 3445, loss 0.0494254, acc 0.98
2016-09-07T17:14:51.598296: step 3446, loss 0.0426762, acc 0.98
2016-09-07T17:14:52.385434: step 3447, loss 0.0370926, acc 1
2016-09-07T17:14:53.137070: step 3448, loss 0.0166447, acc 0.98
2016-09-07T17:14:53.831190: step 3449, loss 0.0706842, acc 0.94
2016-09-07T17:14:54.612223: step 3450, loss 0.0362906, acc 1
2016-09-07T17:14:55.315231: step 3451, loss 0.0259865, acc 0.98
2016-09-07T17:14:56.086445: step 3452, loss 0.110301, acc 0.92
2016-09-07T17:14:56.788758: step 3453, loss 0.0535033, acc 0.98
2016-09-07T17:14:57.458414: step 3454, loss 0.0199686, acc 0.98
2016-09-07T17:14:58.126682: step 3455, loss 0.053272, acc 0.98
2016-09-07T17:14:58.821053: step 3456, loss 0.0114734, acc 1
2016-09-07T17:14:59.510232: step 3457, loss 0.0287447, acc 0.98
2016-09-07T17:15:00.192831: step 3458, loss 0.0231623, acc 0.98
2016-09-07T17:15:00.876987: step 3459, loss 0.0659415, acc 0.94
2016-09-07T17:15:01.552929: step 3460, loss 0.024811, acc 1
2016-09-07T17:15:02.244859: step 3461, loss 0.0610354, acc 0.96
2016-09-07T17:15:02.925806: step 3462, loss 0.0103903, acc 1
2016-09-07T17:15:03.601211: step 3463, loss 0.164382, acc 0.96
2016-09-07T17:15:04.279770: step 3464, loss 0.0759535, acc 0.98
2016-09-07T17:15:04.955270: step 3465, loss 0.0196274, acc 1
2016-09-07T17:15:05.634238: step 3466, loss 0.0220491, acc 0.98
2016-09-07T17:15:06.309636: step 3467, loss 0.121406, acc 0.96
2016-09-07T17:15:06.991988: step 3468, loss 0.0280071, acc 0.98
2016-09-07T17:15:07.667805: step 3469, loss 0.0986653, acc 0.94
2016-09-07T17:15:08.346329: step 3470, loss 0.00211289, acc 1
2016-09-07T17:15:09.025927: step 3471, loss 0.0522415, acc 0.96
2016-09-07T17:15:09.712422: step 3472, loss 0.0232518, acc 1
2016-09-07T17:15:10.380952: step 3473, loss 0.021007, acc 1
2016-09-07T17:15:11.046359: step 3474, loss 0.0310375, acc 0.98
2016-09-07T17:15:11.711274: step 3475, loss 0.0173181, acc 1
2016-09-07T17:15:12.373962: step 3476, loss 0.0234776, acc 0.98
2016-09-07T17:15:13.041263: step 3477, loss 0.0195747, acc 1
2016-09-07T17:15:13.711328: step 3478, loss 0.0224544, acc 0.98
2016-09-07T17:15:14.384464: step 3479, loss 0.0521257, acc 0.98
2016-09-07T17:15:15.057874: step 3480, loss 0.0146654, acc 1
2016-09-07T17:15:15.708236: step 3481, loss 0.0135553, acc 1
2016-09-07T17:15:16.357188: step 3482, loss 0.0166392, acc 1
2016-09-07T17:15:17.020251: step 3483, loss 0.0770531, acc 0.98
2016-09-07T17:15:17.700407: step 3484, loss 0.00591542, acc 1
2016-09-07T17:15:18.371050: step 3485, loss 0.0625047, acc 0.96
2016-09-07T17:15:19.043417: step 3486, loss 0.00227966, acc 1
2016-09-07T17:15:19.706417: step 3487, loss 0.0395114, acc 0.98
2016-09-07T17:15:20.393626: step 3488, loss 0.0652526, acc 0.94
2016-09-07T17:15:21.055069: step 3489, loss 0.0361047, acc 0.98
2016-09-07T17:15:21.795078: step 3490, loss 0.0612438, acc 0.96
2016-09-07T17:15:22.466651: step 3491, loss 0.0276863, acc 1
2016-09-07T17:15:22.834946: step 3492, loss 0.0752215, acc 0.916667
2016-09-07T17:15:23.523984: step 3493, loss 0.0245525, acc 0.98
2016-09-07T17:15:24.226137: step 3494, loss 0.0341793, acc 0.98
2016-09-07T17:15:24.921663: step 3495, loss 0.0254186, acc 1
2016-09-07T17:15:25.605421: step 3496, loss 0.0247985, acc 0.98
2016-09-07T17:15:26.424106: step 3497, loss 0.00119309, acc 1
2016-09-07T17:15:27.280898: step 3498, loss 0.00742414, acc 1
2016-09-07T17:15:28.039894: step 3499, loss 0.0368916, acc 0.98
2016-09-07T17:15:28.838539: step 3500, loss 0.0129291, acc 1

Evaluation:
2016-09-07T17:15:32.089033: step 3500, loss 1.879, acc 0.746

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-3500

2016-09-07T17:15:33.837523: step 3501, loss 0.0806084, acc 0.96
2016-09-07T17:15:34.535907: step 3502, loss 0.0137763, acc 1
2016-09-07T17:15:35.269469: step 3503, loss 0.0103764, acc 1
2016-09-07T17:15:35.936269: step 3504, loss 0.035261, acc 0.98
2016-09-07T17:15:36.592978: step 3505, loss 0.0294222, acc 1
2016-09-07T17:15:37.313592: step 3506, loss 0.137877, acc 0.92
2016-09-07T17:15:37.988398: step 3507, loss 0.0173539, acc 0.98
2016-09-07T17:15:38.679487: step 3508, loss 0.0156934, acc 1
2016-09-07T17:15:39.378568: step 3509, loss 0.0103813, acc 1
2016-09-07T17:15:40.044733: step 3510, loss 0.00122669, acc 1
2016-09-07T17:15:40.735021: step 3511, loss 0.0187889, acc 1
2016-09-07T17:15:41.432565: step 3512, loss 0.039777, acc 0.98
2016-09-07T17:15:42.128783: step 3513, loss 0.0226794, acc 1
2016-09-07T17:15:42.800814: step 3514, loss 0.0816474, acc 0.96
2016-09-07T17:15:43.478896: step 3515, loss 0.00205281, acc 1
2016-09-07T17:15:44.172328: step 3516, loss 0.0193442, acc 0.98
2016-09-07T17:15:44.866066: step 3517, loss 0.129865, acc 0.96
2016-09-07T17:15:45.538709: step 3518, loss 0.01455, acc 1
2016-09-07T17:15:46.204200: step 3519, loss 0.0414286, acc 0.98
2016-09-07T17:15:46.886242: step 3520, loss 0.0187943, acc 1
2016-09-07T17:15:47.572945: step 3521, loss 0.0291245, acc 0.98
2016-09-07T17:15:48.232297: step 3522, loss 0.0583939, acc 0.98
2016-09-07T17:15:48.889813: step 3523, loss 0.0433256, acc 0.98
2016-09-07T17:15:49.557809: step 3524, loss 0.054183, acc 0.98
2016-09-07T17:15:50.233117: step 3525, loss 0.0307013, acc 0.98
2016-09-07T17:15:50.938750: step 3526, loss 0.131433, acc 0.96
2016-09-07T17:15:51.625829: step 3527, loss 0.0166932, acc 1
2016-09-07T17:15:52.301812: step 3528, loss 0.161823, acc 0.94
2016-09-07T17:15:52.970037: step 3529, loss 0.00342196, acc 1
2016-09-07T17:15:53.662588: step 3530, loss 0.0605647, acc 0.96
2016-09-07T17:15:54.349310: step 3531, loss 0.0022672, acc 1
2016-09-07T17:15:55.035148: step 3532, loss 0.0011786, acc 1
2016-09-07T17:15:55.722362: step 3533, loss 0.0196981, acc 0.98
2016-09-07T17:15:56.415495: step 3534, loss 0.0309863, acc 0.98
2016-09-07T17:15:57.096870: step 3535, loss 0.0451132, acc 0.98
2016-09-07T17:15:57.773814: step 3536, loss 0.0523243, acc 0.96
2016-09-07T17:15:58.449345: step 3537, loss 0.0329474, acc 0.98
2016-09-07T17:15:59.123158: step 3538, loss 0.0511799, acc 0.98
2016-09-07T17:15:59.809435: step 3539, loss 0.0705367, acc 0.94
2016-09-07T17:16:00.496240: step 3540, loss 0.012658, acc 1
2016-09-07T17:16:01.181901: step 3541, loss 0.00309347, acc 1
2016-09-07T17:16:01.868724: step 3542, loss 0.0839034, acc 0.96
2016-09-07T17:16:02.536643: step 3543, loss 0.0626421, acc 0.96
2016-09-07T17:16:03.191424: step 3544, loss 0.0479403, acc 0.96
2016-09-07T17:16:03.868859: step 3545, loss 0.0196446, acc 1
2016-09-07T17:16:04.562385: step 3546, loss 0.0522409, acc 0.98
2016-09-07T17:16:05.233587: step 3547, loss 0.0955289, acc 0.94
2016-09-07T17:16:05.912612: step 3548, loss 0.00611834, acc 1
2016-09-07T17:16:06.587312: step 3549, loss 0.0372882, acc 0.98
2016-09-07T17:16:07.273274: step 3550, loss 0.0144598, acc 1
2016-09-07T17:16:07.956571: step 3551, loss 0.133143, acc 0.96
2016-09-07T17:16:08.636243: step 3552, loss 0.00223221, acc 1
2016-09-07T17:16:09.294081: step 3553, loss 0.0495583, acc 0.98
2016-09-07T17:16:09.977253: step 3554, loss 0.0248959, acc 1
2016-09-07T17:16:10.658577: step 3555, loss 0.0838528, acc 0.96
2016-09-07T17:16:11.327629: step 3556, loss 0.0248828, acc 1
2016-09-07T17:16:12.000066: step 3557, loss 0.0354974, acc 0.98
2016-09-07T17:16:12.673228: step 3558, loss 0.0417143, acc 0.98
2016-09-07T17:16:13.347601: step 3559, loss 0.0953898, acc 0.98
2016-09-07T17:16:14.024314: step 3560, loss 0.0241583, acc 0.98
2016-09-07T17:16:14.710570: step 3561, loss 0.0119301, acc 1
2016-09-07T17:16:15.374795: step 3562, loss 0.020356, acc 1
2016-09-07T17:16:16.041780: step 3563, loss 0.0619227, acc 0.96
2016-09-07T17:16:16.729704: step 3564, loss 0.0430535, acc 0.98
2016-09-07T17:16:17.411584: step 3565, loss 0.00205685, acc 1
2016-09-07T17:16:18.092165: step 3566, loss 0.0114974, acc 1
2016-09-07T17:16:18.769797: step 3567, loss 0.00105687, acc 1
2016-09-07T17:16:19.439250: step 3568, loss 0.0320973, acc 0.98
2016-09-07T17:16:20.107848: step 3569, loss 0.0116048, acc 1
2016-09-07T17:16:20.794748: step 3570, loss 0.0462295, acc 0.98
2016-09-07T17:16:21.470702: step 3571, loss 0.0234477, acc 1
2016-09-07T17:16:22.134338: step 3572, loss 0.0298671, acc 0.96
2016-09-07T17:16:22.832442: step 3573, loss 0.0191005, acc 1
2016-09-07T17:16:23.516925: step 3574, loss 0.0236834, acc 0.98
2016-09-07T17:16:24.182491: step 3575, loss 0.0185358, acc 1
2016-09-07T17:16:24.868565: step 3576, loss 0.0249118, acc 0.98
2016-09-07T17:16:25.566928: step 3577, loss 0.00725652, acc 1
2016-09-07T17:16:26.242717: step 3578, loss 0.125566, acc 0.94
2016-09-07T17:16:26.917077: step 3579, loss 0.0757785, acc 0.98
2016-09-07T17:16:27.638790: step 3580, loss 0.0129607, acc 1
2016-09-07T17:16:28.305381: step 3581, loss 0.0157076, acc 1
2016-09-07T17:16:28.969829: step 3582, loss 0.0463263, acc 0.96
2016-09-07T17:16:29.640609: step 3583, loss 0.0116235, acc 1
2016-09-07T17:16:30.337297: step 3584, loss 0.0046481, acc 1
2016-09-07T17:16:31.037037: step 3585, loss 0.0173973, acc 1
2016-09-07T17:16:31.734253: step 3586, loss 0.0542663, acc 0.98
2016-09-07T17:16:32.400750: step 3587, loss 0.0342179, acc 1
2016-09-07T17:16:33.091996: step 3588, loss 0.0181745, acc 0.98
2016-09-07T17:16:33.785879: step 3589, loss 0.0409053, acc 0.96
2016-09-07T17:16:34.505629: step 3590, loss 0.0198103, acc 1
2016-09-07T17:16:35.214803: step 3591, loss 0.034545, acc 1
2016-09-07T17:16:35.896918: step 3592, loss 0.0648174, acc 0.94
2016-09-07T17:16:36.583732: step 3593, loss 0.0118127, acc 1
2016-09-07T17:16:37.256704: step 3594, loss 0.00971991, acc 1
2016-09-07T17:16:37.911592: step 3595, loss 0.033759, acc 0.98
2016-09-07T17:16:38.588389: step 3596, loss 0.041894, acc 0.98
2016-09-07T17:16:39.272948: step 3597, loss 0.0186686, acc 1
2016-09-07T17:16:39.948737: step 3598, loss 0.0511064, acc 0.98
2016-09-07T17:16:40.626381: step 3599, loss 0.0820476, acc 0.98
2016-09-07T17:16:41.305357: step 3600, loss 0.0208419, acc 1

Evaluation:
2016-09-07T17:16:44.206682: step 3600, loss 2.12924, acc 0.739

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-3600

2016-09-07T17:16:45.853270: step 3601, loss 0.0137129, acc 1
2016-09-07T17:16:46.541428: step 3602, loss 0.0160703, acc 1
2016-09-07T17:16:47.209777: step 3603, loss 0.0920497, acc 0.96
2016-09-07T17:16:47.877364: step 3604, loss 0.00500069, acc 1
2016-09-07T17:16:48.561330: step 3605, loss 0.00664066, acc 1
2016-09-07T17:16:49.228742: step 3606, loss 0.038221, acc 0.98
2016-09-07T17:16:49.895962: step 3607, loss 0.0204266, acc 0.98
2016-09-07T17:16:50.579947: step 3608, loss 0.0288572, acc 1
2016-09-07T17:16:51.263848: step 3609, loss 0.0178115, acc 1
2016-09-07T17:16:51.934115: step 3610, loss 0.0683142, acc 0.94
2016-09-07T17:16:52.610177: step 3611, loss 0.0241571, acc 0.98
2016-09-07T17:16:53.295345: step 3612, loss 0.0689817, acc 0.96
2016-09-07T17:16:53.997826: step 3613, loss 0.00105226, acc 1
2016-09-07T17:16:54.676071: step 3614, loss 0.0206455, acc 0.98
2016-09-07T17:16:55.338833: step 3615, loss 0.00308818, acc 1
2016-09-07T17:16:56.024335: step 3616, loss 0.0059913, acc 1
2016-09-07T17:16:56.708336: step 3617, loss 0.0496738, acc 1
2016-09-07T17:16:57.386448: step 3618, loss 0.0176439, acc 0.98
2016-09-07T17:16:58.059565: step 3619, loss 0.0161688, acc 1
2016-09-07T17:16:58.726653: step 3620, loss 0.0500842, acc 0.98
2016-09-07T17:16:59.387633: step 3621, loss 0.0122973, acc 1
2016-09-07T17:17:00.057568: step 3622, loss 0.0638445, acc 0.98
2016-09-07T17:17:00.744193: step 3623, loss 0.021545, acc 0.98
2016-09-07T17:17:01.433115: step 3624, loss 0.0146666, acc 1
2016-09-07T17:17:02.121349: step 3625, loss 0.0238619, acc 1
2016-09-07T17:17:02.811178: step 3626, loss 0.0067937, acc 1
2016-09-07T17:17:03.499470: step 3627, loss 0.00495226, acc 1
2016-09-07T17:17:04.173990: step 3628, loss 0.0156908, acc 0.98
2016-09-07T17:17:04.847832: step 3629, loss 0.0715783, acc 0.98
2016-09-07T17:17:05.542209: step 3630, loss 0.00387653, acc 1
2016-09-07T17:17:06.191501: step 3631, loss 0.0296287, acc 0.98
2016-09-07T17:17:06.867343: step 3632, loss 0.21823, acc 0.98
2016-09-07T17:17:07.524582: step 3633, loss 0.032945, acc 1
2016-09-07T17:17:08.190564: step 3634, loss 0.0292339, acc 0.98
2016-09-07T17:17:08.855440: step 3635, loss 0.00656361, acc 1
2016-09-07T17:17:09.510553: step 3636, loss 0.077103, acc 0.98
2016-09-07T17:17:10.186535: step 3637, loss 0.0158432, acc 1
2016-09-07T17:17:10.863662: step 3638, loss 0.00548454, acc 1
2016-09-07T17:17:11.544378: step 3639, loss 0.0295629, acc 0.98
2016-09-07T17:17:12.225688: step 3640, loss 0.0908174, acc 0.98
2016-09-07T17:17:12.896235: step 3641, loss 0.0356327, acc 0.98
2016-09-07T17:17:13.554803: step 3642, loss 0.0819427, acc 0.94
2016-09-07T17:17:14.219090: step 3643, loss 0.0333784, acc 0.98
2016-09-07T17:17:14.882039: step 3644, loss 0.228902, acc 0.92
2016-09-07T17:17:15.547109: step 3645, loss 0.0234307, acc 0.98
2016-09-07T17:17:16.242319: step 3646, loss 0.0221288, acc 1
2016-09-07T17:17:16.909808: step 3647, loss 0.0667818, acc 0.96
2016-09-07T17:17:17.573350: step 3648, loss 0.00292856, acc 1
2016-09-07T17:17:18.228619: step 3649, loss 0.0495095, acc 0.98
2016-09-07T17:17:18.930283: step 3650, loss 0.179248, acc 0.94
2016-09-07T17:17:19.596636: step 3651, loss 0.0643796, acc 0.96
2016-09-07T17:17:20.263789: step 3652, loss 0.0950915, acc 0.98
2016-09-07T17:17:20.925835: step 3653, loss 0.0432708, acc 0.98
2016-09-07T17:17:21.589912: step 3654, loss 0.0211964, acc 1
2016-09-07T17:17:22.281127: step 3655, loss 0.00776969, acc 1
2016-09-07T17:17:22.936133: step 3656, loss 0.00758248, acc 1
2016-09-07T17:17:23.596925: step 3657, loss 0.01885, acc 0.98
2016-09-07T17:17:24.281068: step 3658, loss 0.0595492, acc 0.96
2016-09-07T17:17:24.992100: step 3659, loss 0.049044, acc 0.98
2016-09-07T17:17:25.690133: step 3660, loss 0.0205863, acc 1
2016-09-07T17:17:26.394359: step 3661, loss 0.0270862, acc 0.98
2016-09-07T17:17:27.050846: step 3662, loss 0.123752, acc 0.9
2016-09-07T17:17:27.711120: step 3663, loss 0.0222857, acc 0.98
2016-09-07T17:17:28.375012: step 3664, loss 0.0242247, acc 1
2016-09-07T17:17:29.063414: step 3665, loss 0.0419991, acc 0.98
2016-09-07T17:17:29.753515: step 3666, loss 0.0803286, acc 0.98
2016-09-07T17:17:30.442979: step 3667, loss 0.0698212, acc 0.96
2016-09-07T17:17:31.152142: step 3668, loss 0.0266939, acc 1
2016-09-07T17:17:31.839330: step 3669, loss 0.0225752, acc 1
2016-09-07T17:17:32.525413: step 3670, loss 0.00753083, acc 1
2016-09-07T17:17:33.292482: step 3671, loss 0.0345683, acc 1
2016-09-07T17:17:33.999962: step 3672, loss 0.0299586, acc 0.98
2016-09-07T17:17:34.666046: step 3673, loss 0.0322537, acc 1
2016-09-07T17:17:35.343692: step 3674, loss 0.0432352, acc 0.98
2016-09-07T17:17:36.020251: step 3675, loss 0.000148382, acc 1
2016-09-07T17:17:36.702515: step 3676, loss 0.0540262, acc 0.98
2016-09-07T17:17:37.394095: step 3677, loss 0.000424451, acc 1
2016-09-07T17:17:38.067307: step 3678, loss 0.139415, acc 0.96
2016-09-07T17:17:38.743327: step 3679, loss 0.0490612, acc 0.98
2016-09-07T17:17:39.408806: step 3680, loss 0.0396829, acc 0.96
2016-09-07T17:17:40.084191: step 3681, loss 0.00073857, acc 1
2016-09-07T17:17:40.770390: step 3682, loss 0.0184875, acc 1
2016-09-07T17:17:41.454807: step 3683, loss 0.0323988, acc 0.96
2016-09-07T17:17:42.140298: step 3684, loss 0.0312397, acc 0.98
2016-09-07T17:17:42.801920: step 3685, loss 0.0218172, acc 0.98
2016-09-07T17:17:43.176427: step 3686, loss 0.0192494, acc 1
2016-09-07T17:17:43.846738: step 3687, loss 0.0202681, acc 1
2016-09-07T17:17:44.547145: step 3688, loss 0.0205436, acc 0.98
2016-09-07T17:17:45.232143: step 3689, loss 0.0761511, acc 0.96
2016-09-07T17:17:45.921528: step 3690, loss 0.0177417, acc 0.98
2016-09-07T17:17:46.584916: step 3691, loss 0.00112835, acc 1
2016-09-07T17:17:47.255924: step 3692, loss 0.0601096, acc 0.96
2016-09-07T17:17:47.917752: step 3693, loss 0.0106016, acc 1
2016-09-07T17:17:48.588710: step 3694, loss 0.167393, acc 0.96
2016-09-07T17:17:49.250364: step 3695, loss 0.0393423, acc 0.98
2016-09-07T17:17:49.914488: step 3696, loss 0.050379, acc 0.98
2016-09-07T17:17:50.589648: step 3697, loss 0.0600452, acc 0.96
2016-09-07T17:17:51.264447: step 3698, loss 0.0167766, acc 1
2016-09-07T17:17:51.960893: step 3699, loss 0.0187708, acc 1
2016-09-07T17:17:52.633354: step 3700, loss 0.0322201, acc 0.98

Evaluation:
2016-09-07T17:17:55.518850: step 3700, loss 1.96887, acc 0.735

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-3700

2016-09-07T17:17:57.191398: step 3701, loss 0.00301447, acc 1
2016-09-07T17:17:57.880309: step 3702, loss 0.00985336, acc 1
2016-09-07T17:17:58.548662: step 3703, loss 0.00718256, acc 1
2016-09-07T17:17:59.198044: step 3704, loss 0.013627, acc 1
2016-09-07T17:17:59.880618: step 3705, loss 0.0349201, acc 0.98
2016-09-07T17:18:00.551732: step 3706, loss 0.0756347, acc 0.96
2016-09-07T17:18:01.223825: step 3707, loss 0.0348298, acc 0.98
2016-09-07T17:18:01.899408: step 3708, loss 0.0176651, acc 1
2016-09-07T17:18:02.562872: step 3709, loss 0.116067, acc 0.96
2016-09-07T17:18:03.238002: step 3710, loss 0.00624841, acc 1
2016-09-07T17:18:03.903264: step 3711, loss 0.00045334, acc 1
2016-09-07T17:18:04.582563: step 3712, loss 0.0113742, acc 1
2016-09-07T17:18:05.254713: step 3713, loss 0.0174026, acc 0.98
2016-09-07T17:18:05.914050: step 3714, loss 0.000607164, acc 1
2016-09-07T17:18:06.578396: step 3715, loss 0.0097444, acc 1
2016-09-07T17:18:07.276663: step 3716, loss 0.0131641, acc 1
2016-09-07T17:18:07.954231: step 3717, loss 0.0290337, acc 1
2016-09-07T17:18:08.613467: step 3718, loss 0.0227564, acc 1
2016-09-07T17:18:09.272591: step 3719, loss 0.0204154, acc 1
2016-09-07T17:18:09.940643: step 3720, loss 0.000748735, acc 1
2016-09-07T17:18:10.609642: step 3721, loss 0.0287389, acc 0.98
2016-09-07T17:18:11.283600: step 3722, loss 0.043033, acc 0.96
2016-09-07T17:18:11.965053: step 3723, loss 0.0592926, acc 0.96
2016-09-07T17:18:12.646264: step 3724, loss 0.0759989, acc 0.94
2016-09-07T17:18:13.325719: step 3725, loss 0.000770258, acc 1
2016-09-07T17:18:13.975474: step 3726, loss 0.00280609, acc 1
2016-09-07T17:18:14.644486: step 3727, loss 0.0913789, acc 0.98
2016-09-07T17:18:15.307426: step 3728, loss 0.0223055, acc 1
2016-09-07T17:18:15.962146: step 3729, loss 0.00800475, acc 1
2016-09-07T17:18:16.632376: step 3730, loss 0.0486942, acc 0.98
2016-09-07T17:18:17.308331: step 3731, loss 0.0140519, acc 1
2016-09-07T17:18:17.967776: step 3732, loss 0.00540936, acc 1
2016-09-07T17:18:18.649475: step 3733, loss 0.058901, acc 0.98
2016-09-07T17:18:19.317792: step 3734, loss 0.091973, acc 0.96
2016-09-07T17:18:19.979395: step 3735, loss 0.0119185, acc 1
2016-09-07T17:18:20.675073: step 3736, loss 0.00740371, acc 1
2016-09-07T17:18:21.341912: step 3737, loss 0.104637, acc 0.98
2016-09-07T17:18:22.021617: step 3738, loss 0.0478156, acc 0.96
2016-09-07T17:18:22.696844: step 3739, loss 0.00624165, acc 1
2016-09-07T17:18:23.345956: step 3740, loss 0.0144482, acc 1
2016-09-07T17:18:24.030522: step 3741, loss 0.0256661, acc 0.98
2016-09-07T17:18:24.682830: step 3742, loss 0.0758799, acc 0.98
2016-09-07T17:18:25.330673: step 3743, loss 0.0593927, acc 0.96
2016-09-07T17:18:25.992958: step 3744, loss 0.00123485, acc 1
2016-09-07T17:18:26.669137: step 3745, loss 0.0199799, acc 0.98
2016-09-07T17:18:27.368475: step 3746, loss 0.00851557, acc 1
2016-09-07T17:18:28.091175: step 3747, loss 0.0489683, acc 0.98
2016-09-07T17:18:28.899107: step 3748, loss 0.0436606, acc 0.98
2016-09-07T17:18:29.709044: step 3749, loss 0.00222964, acc 1
2016-09-07T17:18:30.416554: step 3750, loss 0.0132741, acc 1
2016-09-07T17:18:31.201929: step 3751, loss 0.04665, acc 0.98
2016-09-07T17:18:31.915958: step 3752, loss 0.0195317, acc 1
2016-09-07T17:18:32.585560: step 3753, loss 0.0201002, acc 0.98
2016-09-07T17:18:33.264986: step 3754, loss 0.00461325, acc 1
2016-09-07T17:18:33.929075: step 3755, loss 0.0338702, acc 0.98
2016-09-07T17:18:34.604938: step 3756, loss 0.0175607, acc 0.98
2016-09-07T17:18:35.294169: step 3757, loss 0.0219228, acc 1
2016-09-07T17:18:35.968172: step 3758, loss 0.0699784, acc 0.96
2016-09-07T17:18:36.654044: step 3759, loss 0.00325354, acc 1
2016-09-07T17:18:37.330646: step 3760, loss 0.0799998, acc 0.98
2016-09-07T17:18:38.014534: step 3761, loss 0.0249682, acc 1
2016-09-07T17:18:38.682532: step 3762, loss 0.0546948, acc 0.98
2016-09-07T17:18:39.363007: step 3763, loss 0.0549667, acc 0.96
2016-09-07T17:18:40.032635: step 3764, loss 0.00217471, acc 1
2016-09-07T17:18:40.701273: step 3765, loss 0.0560034, acc 0.96
2016-09-07T17:18:41.393860: step 3766, loss 0.0196877, acc 0.98
2016-09-07T17:18:42.072652: step 3767, loss 0.0444603, acc 0.98
2016-09-07T17:18:42.775743: step 3768, loss 0.000421373, acc 1
2016-09-07T17:18:43.441311: step 3769, loss 0.0764304, acc 0.96
2016-09-07T17:18:44.104779: step 3770, loss 0.0237907, acc 1
2016-09-07T17:18:44.775012: step 3771, loss 0.0995326, acc 0.98
2016-09-07T17:18:45.438281: step 3772, loss 0.0451795, acc 0.98
2016-09-07T17:18:46.114335: step 3773, loss 0.00612817, acc 1
2016-09-07T17:18:46.783594: step 3774, loss 0.0136383, acc 1
2016-09-07T17:18:47.439309: step 3775, loss 0.166394, acc 0.96
2016-09-07T17:18:48.112675: step 3776, loss 0.0286876, acc 0.98
2016-09-07T17:18:48.772015: step 3777, loss 0.0716985, acc 0.96
2016-09-07T17:18:49.444519: step 3778, loss 0.0333351, acc 0.98
2016-09-07T17:18:50.133244: step 3779, loss 0.0307746, acc 0.98
2016-09-07T17:18:50.816641: step 3780, loss 0.0591593, acc 0.98
2016-09-07T17:18:51.493619: step 3781, loss 0.0349208, acc 0.98
2016-09-07T17:18:52.188142: step 3782, loss 0.0157165, acc 1
2016-09-07T17:18:52.871231: step 3783, loss 0.0961316, acc 0.96
2016-09-07T17:18:53.542482: step 3784, loss 0.0216688, acc 1
2016-09-07T17:18:54.222990: step 3785, loss 0.0230385, acc 1
2016-09-07T17:18:54.894246: step 3786, loss 0.0261176, acc 0.98
2016-09-07T17:18:55.565427: step 3787, loss 0.0509602, acc 0.98
2016-09-07T17:18:56.232499: step 3788, loss 0.0206984, acc 1
2016-09-07T17:18:56.886896: step 3789, loss 0.0176936, acc 1
2016-09-07T17:18:57.539454: step 3790, loss 0.0492271, acc 0.98
2016-09-07T17:18:58.206704: step 3791, loss 0.0425478, acc 0.98
2016-09-07T17:18:58.877124: step 3792, loss 0.0300287, acc 0.98
2016-09-07T17:18:59.553559: step 3793, loss 0.0260885, acc 0.98
2016-09-07T17:19:00.214998: step 3794, loss 0.0603768, acc 0.96
2016-09-07T17:19:00.903066: step 3795, loss 0.0414554, acc 0.98
2016-09-07T17:19:01.568279: step 3796, loss 0.0212439, acc 0.98
2016-09-07T17:19:02.236166: step 3797, loss 0.0347384, acc 1
2016-09-07T17:19:02.901622: step 3798, loss 0.0167312, acc 0.98
2016-09-07T17:19:03.580412: step 3799, loss 0.055753, acc 0.96
2016-09-07T17:19:04.272097: step 3800, loss 0.0295098, acc 0.98

Evaluation:
2016-09-07T17:19:07.146522: step 3800, loss 2.01283, acc 0.724

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-3800

2016-09-07T17:19:08.836531: step 3801, loss 0.0426456, acc 0.98
2016-09-07T17:19:09.530575: step 3802, loss 0.0730307, acc 0.94
2016-09-07T17:19:10.221131: step 3803, loss 0.0156739, acc 1
2016-09-07T17:19:10.905347: step 3804, loss 0.0598322, acc 0.96
2016-09-07T17:19:11.603012: step 3805, loss 0.0108838, acc 1
2016-09-07T17:19:12.275468: step 3806, loss 0.0531306, acc 0.96
2016-09-07T17:19:12.962032: step 3807, loss 0.000467887, acc 1
2016-09-07T17:19:13.620116: step 3808, loss 0.0156251, acc 1
2016-09-07T17:19:14.306051: step 3809, loss 0.00196169, acc 1
2016-09-07T17:19:14.978818: step 3810, loss 0.0191313, acc 0.98
2016-09-07T17:19:15.636936: step 3811, loss 0.0657514, acc 0.94
2016-09-07T17:19:16.289835: step 3812, loss 0.0167834, acc 1
2016-09-07T17:19:16.976005: step 3813, loss 0.0130122, acc 1
2016-09-07T17:19:17.656383: step 3814, loss 0.0270291, acc 0.98
2016-09-07T17:19:18.320214: step 3815, loss 0.0149393, acc 1
2016-09-07T17:19:19.001667: step 3816, loss 0.0349831, acc 1
2016-09-07T17:19:19.717360: step 3817, loss 0.0189656, acc 1
2016-09-07T17:19:20.399613: step 3818, loss 0.147376, acc 0.98
2016-09-07T17:19:21.084734: step 3819, loss 0.0155405, acc 1
2016-09-07T17:19:21.775899: step 3820, loss 0.0582693, acc 0.96
2016-09-07T17:19:22.446648: step 3821, loss 0.0381106, acc 0.98
2016-09-07T17:19:23.117785: step 3822, loss 0.000715796, acc 1
2016-09-07T17:19:23.802045: step 3823, loss 0.0717365, acc 0.94
2016-09-07T17:19:24.497801: step 3824, loss 0.00397418, acc 1
2016-09-07T17:19:25.178977: step 3825, loss 0.0133654, acc 1
2016-09-07T17:19:25.831582: step 3826, loss 0.262718, acc 0.9
2016-09-07T17:19:26.498882: step 3827, loss 0.0178414, acc 0.98
2016-09-07T17:19:27.203541: step 3828, loss 0.0207125, acc 0.98
2016-09-07T17:19:27.892182: step 3829, loss 0.0325609, acc 0.98
2016-09-07T17:19:28.555867: step 3830, loss 0.0348013, acc 1
2016-09-07T17:19:29.242741: step 3831, loss 0.000288564, acc 1
2016-09-07T17:19:29.924078: step 3832, loss 0.0796696, acc 0.94
2016-09-07T17:19:30.600520: step 3833, loss 0.0369862, acc 0.98
2016-09-07T17:19:31.287005: step 3834, loss 0.0225616, acc 0.98
2016-09-07T17:19:31.970832: step 3835, loss 0.028893, acc 0.98
2016-09-07T17:19:32.638675: step 3836, loss 0.000425721, acc 1
2016-09-07T17:19:33.313340: step 3837, loss 0.0429479, acc 0.98
2016-09-07T17:19:33.985024: step 3838, loss 0.0815694, acc 0.94
2016-09-07T17:19:34.654149: step 3839, loss 0.0240882, acc 0.98
2016-09-07T17:19:35.319855: step 3840, loss 0.0591098, acc 0.98
2016-09-07T17:19:35.989107: step 3841, loss 0.0474154, acc 0.98
2016-09-07T17:19:36.664800: step 3842, loss 0.0162624, acc 1
2016-09-07T17:19:37.330711: step 3843, loss 0.000482962, acc 1
2016-09-07T17:19:38.017543: step 3844, loss 0.0158422, acc 1
2016-09-07T17:19:38.672660: step 3845, loss 0.0200181, acc 1
2016-09-07T17:19:39.325540: step 3846, loss 0.0191073, acc 0.98
2016-09-07T17:19:39.991956: step 3847, loss 0.0338278, acc 1
2016-09-07T17:19:40.654585: step 3848, loss 0.017239, acc 1
2016-09-07T17:19:41.332474: step 3849, loss 0.00663144, acc 1
2016-09-07T17:19:41.997453: step 3850, loss 0.00313646, acc 1
2016-09-07T17:19:42.654916: step 3851, loss 0.0474629, acc 0.96
2016-09-07T17:19:43.339867: step 3852, loss 0.0244036, acc 0.98
2016-09-07T17:19:44.014502: step 3853, loss 0.0228409, acc 0.98
2016-09-07T17:19:44.693155: step 3854, loss 0.0523287, acc 0.98
2016-09-07T17:19:45.374494: step 3855, loss 0.017107, acc 1
2016-09-07T17:19:46.031993: step 3856, loss 0.0876892, acc 0.94
2016-09-07T17:19:46.703925: step 3857, loss 0.0316517, acc 1
2016-09-07T17:19:47.383315: step 3858, loss 0.0995961, acc 0.98
2016-09-07T17:19:48.059159: step 3859, loss 0.035716, acc 0.96
2016-09-07T17:19:48.722170: step 3860, loss 0.00301257, acc 1
2016-09-07T17:19:49.414439: step 3861, loss 0.0246755, acc 0.98
2016-09-07T17:19:50.105492: step 3862, loss 0.0232151, acc 0.98
2016-09-07T17:19:50.777634: step 3863, loss 0.0114151, acc 1
2016-09-07T17:19:51.456698: step 3864, loss 0.0063331, acc 1
2016-09-07T17:19:52.140444: step 3865, loss 0.0404231, acc 0.96
2016-09-07T17:19:52.800092: step 3866, loss 0.0477275, acc 0.96
2016-09-07T17:19:53.461205: step 3867, loss 0.0144116, acc 1
2016-09-07T17:19:54.128955: step 3868, loss 0.037381, acc 0.98
2016-09-07T17:19:54.786041: step 3869, loss 0.0131179, acc 1
2016-09-07T17:19:55.450654: step 3870, loss 0.01493, acc 0.98
2016-09-07T17:19:56.093912: step 3871, loss 0.0128961, acc 1
2016-09-07T17:19:56.760410: step 3872, loss 0.00571262, acc 1
2016-09-07T17:19:57.412646: step 3873, loss 0.0269247, acc 1
2016-09-07T17:19:58.101857: step 3874, loss 0.0073674, acc 1
2016-09-07T17:19:58.758539: step 3875, loss 0.122506, acc 0.96
2016-09-07T17:19:59.433454: step 3876, loss 0.0253411, acc 0.98
2016-09-07T17:20:00.110039: step 3877, loss 0.0119632, acc 1
2016-09-07T17:20:00.795656: step 3878, loss 0.0233313, acc 0.98
2016-09-07T17:20:01.470942: step 3879, loss 0.116653, acc 0.96
2016-09-07T17:20:01.800982: step 3880, loss 0.0140088, acc 1
2016-09-07T17:20:02.472712: step 3881, loss 0.0141576, acc 1
2016-09-07T17:20:03.177857: step 3882, loss 0.0331671, acc 0.98
2016-09-07T17:20:03.845214: step 3883, loss 0.0376068, acc 1
2016-09-07T17:20:04.521811: step 3884, loss 0.043141, acc 0.98
2016-09-07T17:20:05.190708: step 3885, loss 0.124137, acc 0.96
2016-09-07T17:20:05.871840: step 3886, loss 0.0156886, acc 1
2016-09-07T17:20:06.537841: step 3887, loss 0.00449002, acc 1
2016-09-07T17:20:07.201442: step 3888, loss 0.0374086, acc 0.98
2016-09-07T17:20:07.855973: step 3889, loss 0.0334087, acc 0.98
2016-09-07T17:20:08.528945: step 3890, loss 0.0369701, acc 0.98
2016-09-07T17:20:09.187739: step 3891, loss 0.000158721, acc 1
2016-09-07T17:20:09.870661: step 3892, loss 0.0293447, acc 0.98
2016-09-07T17:20:10.537744: step 3893, loss 0.0715058, acc 0.98
2016-09-07T17:20:11.203902: step 3894, loss 0.0362159, acc 0.96
2016-09-07T17:20:11.867414: step 3895, loss 0.0274457, acc 0.98
2016-09-07T17:20:12.579281: step 3896, loss 0.00865902, acc 1
2016-09-07T17:20:13.257219: step 3897, loss 0.0170552, acc 0.98
2016-09-07T17:20:13.956576: step 3898, loss 0.0107964, acc 1
2016-09-07T17:20:14.614074: step 3899, loss 0.0242823, acc 1
2016-09-07T17:20:15.289312: step 3900, loss 0.0135855, acc 1

Evaluation:
2016-09-07T17:20:18.188446: step 3900, loss 2.0446, acc 0.757

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-3900

2016-09-07T17:20:19.872103: step 3901, loss 0.0143218, acc 1
2016-09-07T17:20:20.556097: step 3902, loss 0.109662, acc 0.94
2016-09-07T17:20:21.237418: step 3903, loss 0.0725386, acc 0.98
2016-09-07T17:20:21.902414: step 3904, loss 0.000767124, acc 1
2016-09-07T17:20:22.556109: step 3905, loss 0.0270188, acc 1
2016-09-07T17:20:23.224816: step 3906, loss 0.00336398, acc 1
2016-09-07T17:20:23.916606: step 3907, loss 0.00139662, acc 1
2016-09-07T17:20:24.604532: step 3908, loss 0.0627227, acc 0.98
2016-09-07T17:20:25.275682: step 3909, loss 0.0118759, acc 1
2016-09-07T17:20:25.949910: step 3910, loss 0.0375252, acc 1
2016-09-07T17:20:26.624340: step 3911, loss 0.016061, acc 1
2016-09-07T17:20:27.309977: step 3912, loss 0.0573507, acc 0.98
2016-09-07T17:20:27.987417: step 3913, loss 0.0151033, acc 1
2016-09-07T17:20:28.646098: step 3914, loss 0.0479379, acc 0.98
2016-09-07T17:20:29.318906: step 3915, loss 0.0522299, acc 0.96
2016-09-07T17:20:29.973045: step 3916, loss 0.0852805, acc 0.96
2016-09-07T17:20:30.645900: step 3917, loss 0.0060127, acc 1
2016-09-07T17:20:31.317636: step 3918, loss 0.0157469, acc 1
2016-09-07T17:20:31.998241: step 3919, loss 0.00102046, acc 1
2016-09-07T17:20:32.653386: step 3920, loss 0.00104969, acc 1
2016-09-07T17:20:33.311198: step 3921, loss 0.0335053, acc 1
2016-09-07T17:20:33.992413: step 3922, loss 0.048903, acc 0.98
2016-09-07T17:20:34.657622: step 3923, loss 0.00128873, acc 1
2016-09-07T17:20:35.331419: step 3924, loss 0.0301985, acc 0.98
2016-09-07T17:20:35.998433: step 3925, loss 0.0195125, acc 1
2016-09-07T17:20:36.683094: step 3926, loss 0.0230508, acc 0.98
2016-09-07T17:20:37.342779: step 3927, loss 0.00326276, acc 1
2016-09-07T17:20:38.026076: step 3928, loss 0.00876672, acc 1
2016-09-07T17:20:38.692267: step 3929, loss 0.045167, acc 0.96
2016-09-07T17:20:39.357536: step 3930, loss 0.0703266, acc 0.96
2016-09-07T17:20:40.032820: step 3931, loss 0.0229935, acc 1
2016-09-07T17:20:40.713648: step 3932, loss 0.0242261, acc 0.98
2016-09-07T17:20:41.406292: step 3933, loss 0.00616178, acc 1
2016-09-07T17:20:42.087685: step 3934, loss 0.0166923, acc 0.98
2016-09-07T17:20:42.758838: step 3935, loss 0.0230949, acc 0.98
2016-09-07T17:20:43.427643: step 3936, loss 0.0297306, acc 1
2016-09-07T17:20:44.123136: step 3937, loss 0.00759939, acc 1
2016-09-07T17:20:44.810933: step 3938, loss 0.000359923, acc 1
2016-09-07T17:20:45.481303: step 3939, loss 0.0610126, acc 0.96
2016-09-07T17:20:46.172846: step 3940, loss 0.000217403, acc 1
2016-09-07T17:20:46.851134: step 3941, loss 0.0116478, acc 1
2016-09-07T17:20:47.564608: step 3942, loss 0.00603778, acc 1
2016-09-07T17:20:48.241328: step 3943, loss 0.0391193, acc 0.98
2016-09-07T17:20:48.910986: step 3944, loss 0.0432138, acc 0.98
2016-09-07T17:20:49.600123: step 3945, loss 0.0717163, acc 0.98
2016-09-07T17:20:50.266436: step 3946, loss 0.102151, acc 0.98
2016-09-07T17:20:50.946947: step 3947, loss 0.0189445, acc 1
2016-09-07T17:20:51.633403: step 3948, loss 0.0336438, acc 0.98
2016-09-07T17:20:52.342354: step 3949, loss 0.0176508, acc 1
2016-09-07T17:20:53.044040: step 3950, loss 0.0535099, acc 0.98
2016-09-07T17:20:53.743259: step 3951, loss 0.0238027, acc 0.98
2016-09-07T17:20:54.417470: step 3952, loss 0.0908124, acc 0.96
2016-09-07T17:20:55.101857: step 3953, loss 0.0492558, acc 0.98
2016-09-07T17:20:55.805640: step 3954, loss 0.000655016, acc 1
2016-09-07T17:20:56.500972: step 3955, loss 0.0307291, acc 0.98
2016-09-07T17:20:57.175745: step 3956, loss 0.0606081, acc 0.96
2016-09-07T17:20:57.854330: step 3957, loss 0.00739313, acc 1
2016-09-07T17:20:58.530758: step 3958, loss 0.0307849, acc 0.98
2016-09-07T17:20:59.219562: step 3959, loss 0.0452255, acc 0.98
2016-09-07T17:20:59.915165: step 3960, loss 0.00675837, acc 1
2016-09-07T17:21:00.643135: step 3961, loss 0.0325505, acc 0.98
2016-09-07T17:21:01.349515: step 3962, loss 0.000871991, acc 1
2016-09-07T17:21:02.036571: step 3963, loss 0.00163507, acc 1
2016-09-07T17:21:02.728358: step 3964, loss 0.0206864, acc 0.98
2016-09-07T17:21:03.450415: step 3965, loss 0.0281138, acc 1
2016-09-07T17:21:04.137540: step 3966, loss 0.0519598, acc 0.94
2016-09-07T17:21:04.812016: step 3967, loss 0.0157689, acc 1
2016-09-07T17:21:05.492430: step 3968, loss 0.0103811, acc 1
2016-09-07T17:21:06.171417: step 3969, loss 0.175247, acc 0.92
2016-09-07T17:21:06.851076: step 3970, loss 0.0967987, acc 0.94
2016-09-07T17:21:07.523648: step 3971, loss 0.0640867, acc 0.96
2016-09-07T17:21:08.210114: step 3972, loss 0.0287566, acc 0.98
2016-09-07T17:21:08.920971: step 3973, loss 0.00188906, acc 1
2016-09-07T17:21:09.614105: step 3974, loss 0.238807, acc 0.96
2016-09-07T17:21:10.294268: step 3975, loss 0.0133646, acc 1
2016-09-07T17:21:10.961394: step 3976, loss 0.0422726, acc 0.98
2016-09-07T17:21:11.649972: step 3977, loss 0.00098902, acc 1
2016-09-07T17:21:12.374125: step 3978, loss 0.0811757, acc 0.98
2016-09-07T17:21:13.052226: step 3979, loss 0.0415209, acc 0.98
2016-09-07T17:21:13.728421: step 3980, loss 0.0494827, acc 0.98
2016-09-07T17:21:14.405844: step 3981, loss 0.0225816, acc 0.98
2016-09-07T17:21:15.067981: step 3982, loss 0.0220693, acc 1
2016-09-07T17:21:15.752413: step 3983, loss 0.0187379, acc 0.98
2016-09-07T17:21:16.435533: step 3984, loss 0.0351813, acc 0.98
2016-09-07T17:21:17.138013: step 3985, loss 0.0163211, acc 0.98
2016-09-07T17:21:17.837380: step 3986, loss 0.0267832, acc 1
2016-09-07T17:21:18.530982: step 3987, loss 0.0271764, acc 1
2016-09-07T17:21:19.208275: step 3988, loss 0.0720075, acc 0.98
2016-09-07T17:21:19.870560: step 3989, loss 0.041601, acc 0.98
2016-09-07T17:21:20.550166: step 3990, loss 0.0638517, acc 0.98
2016-09-07T17:21:21.233279: step 3991, loss 0.139484, acc 0.94
2016-09-07T17:21:21.985634: step 3992, loss 0.0216111, acc 0.98
2016-09-07T17:21:22.643175: step 3993, loss 0.0129311, acc 1
2016-09-07T17:21:23.355348: step 3994, loss 0.0602082, acc 0.96
2016-09-07T17:21:24.042508: step 3995, loss 0.00415985, acc 1
2016-09-07T17:21:24.752147: step 3996, loss 0.00696191, acc 1
2016-09-07T17:21:25.539604: step 3997, loss 0.0328862, acc 0.98
2016-09-07T17:21:26.213588: step 3998, loss 0.0129112, acc 1
2016-09-07T17:21:26.881660: step 3999, loss 0.0699347, acc 0.98
2016-09-07T17:21:27.546428: step 4000, loss 0.125514, acc 0.96

Evaluation:
2016-09-07T17:21:30.450475: step 4000, loss 1.85736, acc 0.731

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-4000

2016-09-07T17:21:32.177844: step 4001, loss 0.0155038, acc 0.98
2016-09-07T17:21:32.855590: step 4002, loss 0.0815031, acc 0.94
2016-09-07T17:21:33.546939: step 4003, loss 0.054955, acc 0.96
2016-09-07T17:21:34.224434: step 4004, loss 0.00545693, acc 1
2016-09-07T17:21:34.896509: step 4005, loss 0.0412069, acc 0.98
2016-09-07T17:21:35.555173: step 4006, loss 0.00229598, acc 1
2016-09-07T17:21:36.215012: step 4007, loss 0.0235429, acc 0.98
2016-09-07T17:21:36.894967: step 4008, loss 0.015743, acc 1
2016-09-07T17:21:37.567610: step 4009, loss 0.00746344, acc 1
2016-09-07T17:21:38.249261: step 4010, loss 0.031641, acc 0.98
2016-09-07T17:21:38.928413: step 4011, loss 0.178266, acc 0.98
2016-09-07T17:21:39.597400: step 4012, loss 0.0494055, acc 0.98
2016-09-07T17:21:40.277417: step 4013, loss 0.00744673, acc 1
2016-09-07T17:21:40.948913: step 4014, loss 0.0417212, acc 0.98
2016-09-07T17:21:41.606062: step 4015, loss 0.0382862, acc 0.98
2016-09-07T17:21:42.290330: step 4016, loss 0.122523, acc 0.94
2016-09-07T17:21:42.961328: step 4017, loss 0.00240805, acc 1
2016-09-07T17:21:43.617047: step 4018, loss 0.000185215, acc 1
2016-09-07T17:21:44.304283: step 4019, loss 0.0649983, acc 0.98
2016-09-07T17:21:44.974296: step 4020, loss 0.0264504, acc 0.98
2016-09-07T17:21:45.660630: step 4021, loss 0.0302495, acc 0.98
2016-09-07T17:21:46.345688: step 4022, loss 0.137854, acc 0.98
2016-09-07T17:21:47.015460: step 4023, loss 0.0108578, acc 1
2016-09-07T17:21:47.689514: step 4024, loss 0.00244063, acc 1
2016-09-07T17:21:48.370551: step 4025, loss 0.0424555, acc 0.98
2016-09-07T17:21:49.066385: step 4026, loss 0.032633, acc 0.98
2016-09-07T17:21:49.740580: step 4027, loss 0.0193517, acc 1
2016-09-07T17:21:50.539804: step 4028, loss 0.0135841, acc 1
2016-09-07T17:21:51.418088: step 4029, loss 0.0402485, acc 0.98
2016-09-07T17:21:52.320030: step 4030, loss 0.00192402, acc 1
2016-09-07T17:21:53.139876: step 4031, loss 0.0214221, acc 0.98
2016-09-07T17:21:53.826823: step 4032, loss 0.0833676, acc 0.94
2016-09-07T17:21:54.530386: step 4033, loss 0.075358, acc 0.98
2016-09-07T17:21:55.360370: step 4034, loss 0.0466606, acc 0.98
2016-09-07T17:21:56.100034: step 4035, loss 0.0256604, acc 1
2016-09-07T17:21:56.869856: step 4036, loss 0.0174253, acc 1
2016-09-07T17:21:57.548081: step 4037, loss 0.0607131, acc 0.96
2016-09-07T17:21:58.230428: step 4038, loss 0.0329206, acc 1
2016-09-07T17:21:58.909019: step 4039, loss 0.017813, acc 1
2016-09-07T17:21:59.581090: step 4040, loss 0.0174238, acc 0.98
2016-09-07T17:22:00.245759: step 4041, loss 0.0203179, acc 1
2016-09-07T17:22:01.022826: step 4042, loss 0.087084, acc 0.94
2016-09-07T17:22:01.764799: step 4043, loss 0.034225, acc 0.98
2016-09-07T17:22:02.623528: step 4044, loss 0.133618, acc 0.98
2016-09-07T17:22:03.311778: step 4045, loss 0.00916049, acc 1
2016-09-07T17:22:04.006846: step 4046, loss 0.0271919, acc 1
2016-09-07T17:22:04.753985: step 4047, loss 0.00532099, acc 1
2016-09-07T17:22:05.419854: step 4048, loss 0.0132516, acc 1
2016-09-07T17:22:06.243191: step 4049, loss 0.0714084, acc 0.96
2016-09-07T17:22:06.927608: step 4050, loss 0.0766579, acc 0.98
2016-09-07T17:22:07.601580: step 4051, loss 0.0215588, acc 1
2016-09-07T17:22:08.274365: step 4052, loss 0.0444812, acc 0.96
2016-09-07T17:22:08.949402: step 4053, loss 0.0256461, acc 1
2016-09-07T17:22:09.601126: step 4054, loss 0.000603466, acc 1
2016-09-07T17:22:10.450979: step 4055, loss 0.00575711, acc 1
2016-09-07T17:22:11.154297: step 4056, loss 0.0605935, acc 0.94
2016-09-07T17:22:11.816445: step 4057, loss 0.00826743, acc 1
2016-09-07T17:22:12.491593: step 4058, loss 0.033421, acc 0.98
2016-09-07T17:22:13.169030: step 4059, loss 0.0187506, acc 1
2016-09-07T17:22:13.845126: step 4060, loss 0.0192756, acc 0.98
2016-09-07T17:22:14.533794: step 4061, loss 0.0883802, acc 0.96
2016-09-07T17:22:15.192480: step 4062, loss 0.0599911, acc 0.98
2016-09-07T17:22:15.882475: step 4063, loss 0.0311137, acc 0.98
2016-09-07T17:22:16.543893: step 4064, loss 0.00297173, acc 1
2016-09-07T17:22:17.223339: step 4065, loss 0.00781178, acc 1
2016-09-07T17:22:17.895539: step 4066, loss 0.000654158, acc 1
2016-09-07T17:22:18.574668: step 4067, loss 0.0188719, acc 1
2016-09-07T17:22:19.239333: step 4068, loss 0.00538106, acc 1
2016-09-07T17:22:19.934642: step 4069, loss 0.0384855, acc 0.98
2016-09-07T17:22:20.592428: step 4070, loss 0.0356837, acc 0.98
2016-09-07T17:22:21.251721: step 4071, loss 0.0745622, acc 0.96
2016-09-07T17:22:21.919085: step 4072, loss 0.0140616, acc 1
2016-09-07T17:22:22.604610: step 4073, loss 0.0347826, acc 0.98
2016-09-07T17:22:22.975570: step 4074, loss 0.273596, acc 0.916667
2016-09-07T17:22:23.646518: step 4075, loss 0.0246846, acc 1
2016-09-07T17:22:24.312904: step 4076, loss 0.0454944, acc 0.98
2016-09-07T17:22:24.984151: step 4077, loss 0.0170318, acc 1
2016-09-07T17:22:25.657057: step 4078, loss 0.0020816, acc 1
2016-09-07T17:22:26.346459: step 4079, loss 0.0644908, acc 0.96
2016-09-07T17:22:27.026026: step 4080, loss 0.0407869, acc 0.98
2016-09-07T17:22:27.698027: step 4081, loss 0.0156934, acc 1
2016-09-07T17:22:28.361802: step 4082, loss 0.0369771, acc 0.98
2016-09-07T17:22:29.058299: step 4083, loss 0.0135482, acc 1
2016-09-07T17:22:29.742049: step 4084, loss 0.0252378, acc 0.98
2016-09-07T17:22:30.415375: step 4085, loss 0.0846428, acc 0.98
2016-09-07T17:22:31.103954: step 4086, loss 0.0523355, acc 0.96
2016-09-07T17:22:31.760043: step 4087, loss 0.0262419, acc 1
2016-09-07T17:22:32.446480: step 4088, loss 0.0244565, acc 0.98
2016-09-07T17:22:33.127405: step 4089, loss 0.0165824, acc 1
2016-09-07T17:22:33.812712: step 4090, loss 0.00925401, acc 1
2016-09-07T17:22:34.494116: step 4091, loss 0.0305726, acc 1
2016-09-07T17:22:35.179481: step 4092, loss 0.0172462, acc 1
2016-09-07T17:22:35.864135: step 4093, loss 0.0266011, acc 0.98
2016-09-07T17:22:36.543720: step 4094, loss 0.0128104, acc 1
2016-09-07T17:22:37.206185: step 4095, loss 0.0380836, acc 0.98
2016-09-07T17:22:37.884402: step 4096, loss 0.0079432, acc 1
2016-09-07T17:22:38.555987: step 4097, loss 0.029817, acc 0.98
2016-09-07T17:22:39.234457: step 4098, loss 0.0241429, acc 0.98
2016-09-07T17:22:39.912212: step 4099, loss 0.0207529, acc 1
2016-09-07T17:22:40.601947: step 4100, loss 0.00708833, acc 1

Evaluation:
2016-09-07T17:22:43.490885: step 4100, loss 1.90499, acc 0.742

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-4100

2016-09-07T17:22:45.200964: step 4101, loss 0.0201929, acc 1
2016-09-07T17:22:45.880725: step 4102, loss 0.000587616, acc 1
2016-09-07T17:22:46.538238: step 4103, loss 0.0631799, acc 0.96
2016-09-07T17:22:47.217326: step 4104, loss 0.0179749, acc 1
2016-09-07T17:22:47.894963: step 4105, loss 0.0121402, acc 1
2016-09-07T17:22:48.566359: step 4106, loss 0.138535, acc 0.96
2016-09-07T17:22:49.219981: step 4107, loss 0.0126135, acc 1
2016-09-07T17:22:49.880429: step 4108, loss 0.00205126, acc 1
2016-09-07T17:22:50.549379: step 4109, loss 0.00472292, acc 1
2016-09-07T17:22:51.230129: step 4110, loss 0.0192874, acc 0.98
2016-09-07T17:22:51.902834: step 4111, loss 0.0126526, acc 1
2016-09-07T17:22:52.571474: step 4112, loss 0.00147344, acc 1
2016-09-07T17:22:53.259684: step 4113, loss 0.0383146, acc 0.98
2016-09-07T17:22:53.945867: step 4114, loss 0.02146, acc 1
2016-09-07T17:22:54.605320: step 4115, loss 0.0181347, acc 1
2016-09-07T17:22:55.289782: step 4116, loss 0.0244891, acc 0.98
2016-09-07T17:22:55.980770: step 4117, loss 0.000841399, acc 1
2016-09-07T17:22:56.664606: step 4118, loss 0.000895304, acc 1
2016-09-07T17:22:57.326905: step 4119, loss 0.0275189, acc 0.98
2016-09-07T17:22:58.009064: step 4120, loss 0.0619694, acc 0.96
2016-09-07T17:22:58.700079: step 4121, loss 0.00858586, acc 1
2016-09-07T17:22:59.363031: step 4122, loss 0.0501643, acc 0.96
2016-09-07T17:23:00.052170: step 4123, loss 0.0191561, acc 0.98
2016-09-07T17:23:00.724863: step 4124, loss 0.00734291, acc 1
2016-09-07T17:23:01.379834: step 4125, loss 0.0150043, acc 1
2016-09-07T17:23:02.037987: step 4126, loss 0.0117932, acc 1
2016-09-07T17:23:02.698435: step 4127, loss 0.0792686, acc 0.98
2016-09-07T17:23:03.374542: step 4128, loss 0.0519878, acc 0.98
2016-09-07T17:23:04.021427: step 4129, loss 0.0494988, acc 0.96
2016-09-07T17:23:04.676988: step 4130, loss 0.0695976, acc 0.98
2016-09-07T17:23:05.366497: step 4131, loss 0.0548163, acc 0.98
2016-09-07T17:23:06.049246: step 4132, loss 0.0125086, acc 1
2016-09-07T17:23:06.712061: step 4133, loss 0.0129052, acc 1
2016-09-07T17:23:07.389481: step 4134, loss 0.0606218, acc 0.98
2016-09-07T17:23:08.068593: step 4135, loss 0.0432687, acc 0.98
2016-09-07T17:23:08.755772: step 4136, loss 0.0343211, acc 0.98
2016-09-07T17:23:09.423090: step 4137, loss 0.0192564, acc 1
2016-09-07T17:23:10.092946: step 4138, loss 0.0151548, acc 1
2016-09-07T17:23:10.759870: step 4139, loss 0.0279921, acc 0.98
2016-09-07T17:23:11.439078: step 4140, loss 0.0132147, acc 1
2016-09-07T17:23:12.129220: step 4141, loss 0.0607299, acc 0.98
2016-09-07T17:23:12.837342: step 4142, loss 0.016242, acc 1
2016-09-07T17:23:13.520655: step 4143, loss 0.0548981, acc 0.96
2016-09-07T17:23:14.195180: step 4144, loss 0.0114687, acc 1
2016-09-07T17:23:14.869131: step 4145, loss 0.00975925, acc 1
2016-09-07T17:23:15.541482: step 4146, loss 0.0326398, acc 0.98
2016-09-07T17:23:16.227118: step 4147, loss 0.0154106, acc 1
2016-09-07T17:23:16.929227: step 4148, loss 0.011296, acc 1
2016-09-07T17:23:17.601632: step 4149, loss 0.00210333, acc 1
2016-09-07T17:23:18.278708: step 4150, loss 0.0474065, acc 0.96
2016-09-07T17:23:18.953417: step 4151, loss 0.0392071, acc 0.96
2016-09-07T17:23:19.623700: step 4152, loss 0.0144727, acc 1
2016-09-07T17:23:20.315808: step 4153, loss 0.0425256, acc 0.96
2016-09-07T17:23:20.989135: step 4154, loss 0.102238, acc 0.96
2016-09-07T17:23:21.675526: step 4155, loss 0.0513347, acc 0.96
2016-09-07T17:23:22.349387: step 4156, loss 0.0109238, acc 1
2016-09-07T17:23:23.022269: step 4157, loss 0.058384, acc 0.98
2016-09-07T17:23:23.711534: step 4158, loss 0.00251577, acc 1
2016-09-07T17:23:24.380507: step 4159, loss 0.000286524, acc 1
2016-09-07T17:23:25.057890: step 4160, loss 0.00401462, acc 1
2016-09-07T17:23:25.723177: step 4161, loss 0.049615, acc 0.96
2016-09-07T17:23:26.385091: step 4162, loss 0.0193561, acc 1
2016-09-07T17:23:27.048241: step 4163, loss 0.00761778, acc 1
2016-09-07T17:23:27.713881: step 4164, loss 0.0351703, acc 0.98
2016-09-07T17:23:28.390192: step 4165, loss 0.0201755, acc 1
2016-09-07T17:23:29.048943: step 4166, loss 0.0557561, acc 0.94
2016-09-07T17:23:29.724763: step 4167, loss 0.0286005, acc 0.98
2016-09-07T17:23:30.410291: step 4168, loss 0.00813411, acc 1
2016-09-07T17:23:31.079289: step 4169, loss 0.0462148, acc 0.98
2016-09-07T17:23:31.759688: step 4170, loss 0.00105506, acc 1
2016-09-07T17:23:32.437342: step 4171, loss 0.00116022, acc 1
2016-09-07T17:23:33.100906: step 4172, loss 0.0362431, acc 0.98
2016-09-07T17:23:33.778007: step 4173, loss 0.0278217, acc 0.98
2016-09-07T17:23:34.435229: step 4174, loss 0.00864998, acc 1
2016-09-07T17:23:35.116542: step 4175, loss 0.00532699, acc 1
2016-09-07T17:23:35.778529: step 4176, loss 0.0407309, acc 0.98
2016-09-07T17:23:36.437593: step 4177, loss 0.0121544, acc 1
2016-09-07T17:23:37.101220: step 4178, loss 0.0631023, acc 0.98
2016-09-07T17:23:37.791436: step 4179, loss 0.0459569, acc 0.98
2016-09-07T17:23:38.488452: step 4180, loss 0.00817267, acc 1
2016-09-07T17:23:39.165347: step 4181, loss 0.0152662, acc 0.98
2016-09-07T17:23:39.853320: step 4182, loss 0.0716845, acc 0.96
2016-09-07T17:23:40.537282: step 4183, loss 0.0120593, acc 1
2016-09-07T17:23:41.207387: step 4184, loss 0.0672935, acc 0.98
2016-09-07T17:23:41.875714: step 4185, loss 0.00437493, acc 1
2016-09-07T17:23:42.542403: step 4186, loss 0.0281433, acc 0.98
2016-09-07T17:23:43.218668: step 4187, loss 0.02732, acc 1
2016-09-07T17:23:43.890199: step 4188, loss 0.0125181, acc 1
2016-09-07T17:23:44.554008: step 4189, loss 0.045871, acc 0.98
2016-09-07T17:23:45.220034: step 4190, loss 0.0429449, acc 0.96
2016-09-07T17:23:45.892720: step 4191, loss 0.0504712, acc 0.98
2016-09-07T17:23:46.561551: step 4192, loss 0.0313265, acc 1
2016-09-07T17:23:47.219501: step 4193, loss 0.0015628, acc 1
2016-09-07T17:23:47.909718: step 4194, loss 0.0408966, acc 0.98
2016-09-07T17:23:48.595940: step 4195, loss 0.0180173, acc 0.98
2016-09-07T17:23:49.249183: step 4196, loss 0.0105482, acc 1
2016-09-07T17:23:49.923093: step 4197, loss 0.0342689, acc 0.98
2016-09-07T17:23:50.594628: step 4198, loss 0.0312109, acc 0.98
2016-09-07T17:23:51.269614: step 4199, loss 0.039897, acc 0.98
2016-09-07T17:23:51.941427: step 4200, loss 0.0334721, acc 0.98

Evaluation:
2016-09-07T17:23:54.816921: step 4200, loss 1.93432, acc 0.745

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-4200

2016-09-07T17:23:56.587250: step 4201, loss 0.180922, acc 0.98
2016-09-07T17:23:57.265577: step 4202, loss 0.00544253, acc 1
2016-09-07T17:23:57.953396: step 4203, loss 0.0291395, acc 0.98
2016-09-07T17:23:58.611893: step 4204, loss 0.0717115, acc 0.98
2016-09-07T17:23:59.286546: step 4205, loss 0.111485, acc 0.92
2016-09-07T17:23:59.968506: step 4206, loss 0.023746, acc 1
2016-09-07T17:24:00.633828: step 4207, loss 0.010586, acc 1
2016-09-07T17:24:01.298120: step 4208, loss 0.0406531, acc 0.96
2016-09-07T17:24:01.983846: step 4209, loss 0.0762716, acc 0.96
2016-09-07T17:24:02.645853: step 4210, loss 0.123691, acc 0.94
2016-09-07T17:24:03.321654: step 4211, loss 0.052189, acc 0.98
2016-09-07T17:24:03.996824: step 4212, loss 0.000983852, acc 1
2016-09-07T17:24:04.669549: step 4213, loss 0.0522149, acc 0.98
2016-09-07T17:24:05.334379: step 4214, loss 0.0206898, acc 1
2016-09-07T17:24:06.017019: step 4215, loss 0.00709665, acc 1
2016-09-07T17:24:06.664535: step 4216, loss 0.0214688, acc 1
2016-09-07T17:24:07.345093: step 4217, loss 0.0622309, acc 0.96
2016-09-07T17:24:08.018089: step 4218, loss 0.0517777, acc 0.98
2016-09-07T17:24:08.697640: step 4219, loss 0.0372461, acc 0.98
2016-09-07T17:24:09.363005: step 4220, loss 0.0771006, acc 0.98
2016-09-07T17:24:10.016508: step 4221, loss 0.062712, acc 0.98
2016-09-07T17:24:10.706650: step 4222, loss 0.0338615, acc 0.98
2016-09-07T17:24:11.371885: step 4223, loss 0.0652012, acc 0.98
2016-09-07T17:24:12.035567: step 4224, loss 0.0253029, acc 0.98
2016-09-07T17:24:12.722879: step 4225, loss 0.164858, acc 0.96
2016-09-07T17:24:13.409861: step 4226, loss 0.0429329, acc 0.96
2016-09-07T17:24:14.067216: step 4227, loss 0.0237853, acc 1
2016-09-07T17:24:14.724542: step 4228, loss 0.0390975, acc 0.98
2016-09-07T17:24:15.379428: step 4229, loss 0.0216711, acc 1
2016-09-07T17:24:16.062587: step 4230, loss 0.007942, acc 1
2016-09-07T17:24:16.739443: step 4231, loss 0.00721864, acc 1
2016-09-07T17:24:17.434155: step 4232, loss 0.0133188, acc 1
2016-09-07T17:24:18.113397: step 4233, loss 0.0122228, acc 1
2016-09-07T17:24:18.771705: step 4234, loss 0.0136759, acc 1
2016-09-07T17:24:19.459936: step 4235, loss 0.0417294, acc 0.96
2016-09-07T17:24:20.130625: step 4236, loss 0.0207117, acc 0.98
2016-09-07T17:24:20.806905: step 4237, loss 0.0320405, acc 0.98
2016-09-07T17:24:21.478515: step 4238, loss 0.0183003, acc 1
2016-09-07T17:24:22.158541: step 4239, loss 0.0283522, acc 0.98
2016-09-07T17:24:22.845191: step 4240, loss 0.0241746, acc 1
2016-09-07T17:24:23.535032: step 4241, loss 0.017832, acc 1
2016-09-07T17:24:24.203870: step 4242, loss 0.0115965, acc 1
2016-09-07T17:24:24.893200: step 4243, loss 0.0231066, acc 1
2016-09-07T17:24:25.566888: step 4244, loss 0.202593, acc 0.98
2016-09-07T17:24:26.232502: step 4245, loss 0.0548981, acc 0.94
2016-09-07T17:24:26.885826: step 4246, loss 0.0146055, acc 1
2016-09-07T17:24:27.549212: step 4247, loss 0.00470223, acc 1
2016-09-07T17:24:28.225134: step 4248, loss 0.0149373, acc 1
2016-09-07T17:24:28.878687: step 4249, loss 0.150576, acc 0.98
2016-09-07T17:24:29.551833: step 4250, loss 0.0581899, acc 0.98
2016-09-07T17:24:30.237708: step 4251, loss 0.0439929, acc 0.96
2016-09-07T17:24:30.904190: step 4252, loss 0.000601641, acc 1
2016-09-07T17:24:31.581648: step 4253, loss 0.0139701, acc 1
2016-09-07T17:24:32.265660: step 4254, loss 0.0206967, acc 0.98
2016-09-07T17:24:32.954899: step 4255, loss 0.0133632, acc 1
2016-09-07T17:24:33.634365: step 4256, loss 0.00851345, acc 1
2016-09-07T17:24:34.298817: step 4257, loss 0.0229914, acc 0.98
2016-09-07T17:24:34.955345: step 4258, loss 0.0343495, acc 0.98
2016-09-07T17:24:35.611104: step 4259, loss 0.0181703, acc 0.98
2016-09-07T17:24:36.284788: step 4260, loss 0.0210408, acc 1
2016-09-07T17:24:36.959669: step 4261, loss 0.0351117, acc 0.98
2016-09-07T17:24:37.641992: step 4262, loss 0.00151959, acc 1
2016-09-07T17:24:38.307830: step 4263, loss 0.0109321, acc 1
2016-09-07T17:24:38.957618: step 4264, loss 0.00767647, acc 1
2016-09-07T17:24:39.634670: step 4265, loss 0.100182, acc 0.98
2016-09-07T17:24:40.298388: step 4266, loss 0.0010992, acc 1
2016-09-07T17:24:40.965551: step 4267, loss 0.0093734, acc 1
2016-09-07T17:24:41.334408: step 4268, loss 0.0263614, acc 1
2016-09-07T17:24:42.034424: step 4269, loss 0.00706142, acc 1
2016-09-07T17:24:42.707668: step 4270, loss 0.00316544, acc 1
2016-09-07T17:24:43.383896: step 4271, loss 0.0435699, acc 0.96
2016-09-07T17:24:44.049555: step 4272, loss 0.00110296, acc 1
2016-09-07T17:24:44.718381: step 4273, loss 0.0129757, acc 1
2016-09-07T17:24:45.393879: step 4274, loss 0.0537003, acc 0.96
2016-09-07T17:24:46.057053: step 4275, loss 0.0252006, acc 1
2016-09-07T17:24:46.747839: step 4276, loss 0.0483016, acc 0.98
2016-09-07T17:24:47.445836: step 4277, loss 0.00680375, acc 1
2016-09-07T17:24:48.121274: step 4278, loss 0.0514024, acc 0.98
2016-09-07T17:24:48.801198: step 4279, loss 0.0764619, acc 0.98
2016-09-07T17:24:49.471214: step 4280, loss 0.0482946, acc 0.98
2016-09-07T17:24:50.162888: step 4281, loss 0.0379327, acc 0.96
2016-09-07T17:24:50.848842: step 4282, loss 0.0122607, acc 1
2016-09-07T17:24:51.515946: step 4283, loss 0.0149113, acc 1
2016-09-07T17:24:52.166103: step 4284, loss 0.0185163, acc 1
2016-09-07T17:24:52.818143: step 4285, loss 0.0455838, acc 0.98
2016-09-07T17:24:53.494754: step 4286, loss 0.0332687, acc 0.98
2016-09-07T17:24:54.150383: step 4287, loss 0.0366517, acc 0.98
2016-09-07T17:24:54.820010: step 4288, loss 0.0269755, acc 1
2016-09-07T17:24:55.524213: step 4289, loss 0.0346471, acc 0.98
2016-09-07T17:24:56.204947: step 4290, loss 0.0390067, acc 0.98
2016-09-07T17:24:56.872838: step 4291, loss 0.0477983, acc 0.98
2016-09-07T17:24:57.549669: step 4292, loss 0.00246729, acc 1
2016-09-07T17:24:58.228913: step 4293, loss 0.0339702, acc 0.98
2016-09-07T17:24:58.902552: step 4294, loss 0.0245772, acc 1
2016-09-07T17:24:59.585589: step 4295, loss 0.00598274, acc 1
2016-09-07T17:25:00.264752: step 4296, loss 0.0599821, acc 0.98
2016-09-07T17:25:00.924171: step 4297, loss 0.0196863, acc 1
2016-09-07T17:25:01.582595: step 4298, loss 0.00598896, acc 1
2016-09-07T17:25:02.260759: step 4299, loss 0.00791516, acc 1
2016-09-07T17:25:02.939119: step 4300, loss 0.0462935, acc 0.98

Evaluation:
2016-09-07T17:25:05.851180: step 4300, loss 1.99102, acc 0.739

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-4300

2016-09-07T17:25:07.432919: step 4301, loss 0.0904898, acc 0.96
2016-09-07T17:25:08.083448: step 4302, loss 0.0155419, acc 1
2016-09-07T17:25:08.754570: step 4303, loss 0.00301859, acc 1
2016-09-07T17:25:09.420838: step 4304, loss 0.0398621, acc 0.98
2016-09-07T17:25:10.094831: step 4305, loss 0.0130135, acc 1
2016-09-07T17:25:10.755306: step 4306, loss 0.0989551, acc 0.96
2016-09-07T17:25:11.422271: step 4307, loss 0.00072332, acc 1
2016-09-07T17:25:12.099860: step 4308, loss 0.000801946, acc 1
2016-09-07T17:25:12.770413: step 4309, loss 0.0273584, acc 0.98
2016-09-07T17:25:13.450787: step 4310, loss 0.00839909, acc 1
2016-09-07T17:25:14.129786: step 4311, loss 0.0584437, acc 0.98
2016-09-07T17:25:14.790726: step 4312, loss 0.0103582, acc 1
2016-09-07T17:25:15.468875: step 4313, loss 0.00879235, acc 1
2016-09-07T17:25:16.129147: step 4314, loss 0.0911621, acc 0.98
2016-09-07T17:25:16.792181: step 4315, loss 0.0143665, acc 1
2016-09-07T17:25:17.480257: step 4316, loss 0.00425146, acc 1
2016-09-07T17:25:18.160149: step 4317, loss 0.00446485, acc 1
2016-09-07T17:25:18.844853: step 4318, loss 0.036621, acc 0.98
2016-09-07T17:25:19.514449: step 4319, loss 0.0106665, acc 1
2016-09-07T17:25:20.174811: step 4320, loss 0.0137753, acc 1
2016-09-07T17:25:20.838549: step 4321, loss 0.0169673, acc 0.98
2016-09-07T17:25:21.544219: step 4322, loss 0.000204269, acc 1
2016-09-07T17:25:22.233278: step 4323, loss 0.0858702, acc 0.96
2016-09-07T17:25:22.958162: step 4324, loss 0.0196259, acc 1
2016-09-07T17:25:23.655198: step 4325, loss 0.0031009, acc 1
2016-09-07T17:25:24.345420: step 4326, loss 7.37927e-05, acc 1
2016-09-07T17:25:25.005581: step 4327, loss 0.0353295, acc 0.96
2016-09-07T17:25:25.667603: step 4328, loss 0.0457582, acc 0.96
2016-09-07T17:25:26.332994: step 4329, loss 0.0670864, acc 0.98
2016-09-07T17:25:27.009432: step 4330, loss 0.0781866, acc 0.96
2016-09-07T17:25:27.675740: step 4331, loss 0.0466166, acc 0.98
2016-09-07T17:25:28.346620: step 4332, loss 0.0270545, acc 0.98
2016-09-07T17:25:29.010297: step 4333, loss 0.0388491, acc 0.98
2016-09-07T17:25:29.669906: step 4334, loss 0.0425299, acc 0.98
2016-09-07T17:25:30.347914: step 4335, loss 0.0111576, acc 1
2016-09-07T17:25:31.021385: step 4336, loss 0.0143132, acc 1
2016-09-07T17:25:31.695745: step 4337, loss 0.0018175, acc 1
2016-09-07T17:25:32.360633: step 4338, loss 0.0625776, acc 0.96
2016-09-07T17:25:33.015525: step 4339, loss 0.0194467, acc 1
2016-09-07T17:25:33.686707: step 4340, loss 0.0204766, acc 0.98
2016-09-07T17:25:34.358904: step 4341, loss 0.000511437, acc 1
2016-09-07T17:25:35.033873: step 4342, loss 0.0114622, acc 1
2016-09-07T17:25:35.707452: step 4343, loss 0.0724877, acc 0.98
2016-09-07T17:25:36.373211: step 4344, loss 0.0551643, acc 0.96
2016-09-07T17:25:37.026323: step 4345, loss 0.0887834, acc 0.98
2016-09-07T17:25:37.700283: step 4346, loss 0.0390317, acc 0.96
2016-09-07T17:25:38.361011: step 4347, loss 0.02088, acc 1
2016-09-07T17:25:39.036816: step 4348, loss 0.116246, acc 0.98
2016-09-07T17:25:39.710698: step 4349, loss 0.0391294, acc 1
2016-09-07T17:25:40.363586: step 4350, loss 0.0258877, acc 0.98
2016-09-07T17:25:41.024531: step 4351, loss 0.0668734, acc 0.96
2016-09-07T17:25:41.682014: step 4352, loss 0.00459439, acc 1
2016-09-07T17:25:42.370263: step 4353, loss 0.0121174, acc 1
2016-09-07T17:25:43.029983: step 4354, loss 0.0548412, acc 0.96
2016-09-07T17:25:43.697192: step 4355, loss 0.0194144, acc 0.98
2016-09-07T17:25:44.386030: step 4356, loss 0.0244242, acc 0.98
2016-09-07T17:25:45.048436: step 4357, loss 0.0340083, acc 0.98
2016-09-07T17:25:45.715132: step 4358, loss 0.0580172, acc 0.96
2016-09-07T17:25:46.388246: step 4359, loss 0.0683675, acc 0.98
2016-09-07T17:25:47.055956: step 4360, loss 0.00854766, acc 1
2016-09-07T17:25:47.731409: step 4361, loss 0.0307392, acc 0.98
2016-09-07T17:25:48.408938: step 4362, loss 0.0360675, acc 0.98
2016-09-07T17:25:49.093828: step 4363, loss 0.0364909, acc 0.98
2016-09-07T17:25:49.776966: step 4364, loss 0.0154062, acc 1
2016-09-07T17:25:50.457849: step 4365, loss 0.00776056, acc 1
2016-09-07T17:25:51.126537: step 4366, loss 0.0320759, acc 0.98
2016-09-07T17:25:51.801862: step 4367, loss 0.0590457, acc 0.98
2016-09-07T17:25:52.460411: step 4368, loss 0.0164911, acc 1
2016-09-07T17:25:53.128194: step 4369, loss 0.0050845, acc 1
2016-09-07T17:25:53.800414: step 4370, loss 0.0361836, acc 0.98
2016-09-07T17:25:54.462148: step 4371, loss 0.0189436, acc 1
2016-09-07T17:25:55.140324: step 4372, loss 0.0961892, acc 0.98
2016-09-07T17:25:55.809386: step 4373, loss 0.0039377, acc 1
2016-09-07T17:25:56.478961: step 4374, loss 0.012455, acc 1
2016-09-07T17:25:57.138486: step 4375, loss 0.15522, acc 0.98
2016-09-07T17:25:57.817945: step 4376, loss 0.0402799, acc 1
2016-09-07T17:25:58.495783: step 4377, loss 0.0454581, acc 0.98
2016-09-07T17:25:59.172952: step 4378, loss 0.000338684, acc 1
2016-09-07T17:25:59.849396: step 4379, loss 0.0359191, acc 0.98
2016-09-07T17:26:00.550313: step 4380, loss 0.0255943, acc 0.98
2016-09-07T17:26:01.216711: step 4381, loss 0.000395718, acc 1
2016-09-07T17:26:01.860888: step 4382, loss 0.0123268, acc 1
2016-09-07T17:26:02.534207: step 4383, loss 0.0135353, acc 1
2016-09-07T17:26:03.217737: step 4384, loss 0.0417924, acc 0.98
2016-09-07T17:26:03.888847: step 4385, loss 0.0207537, acc 1
2016-09-07T17:26:04.561834: step 4386, loss 0.0772159, acc 0.98
2016-09-07T17:26:05.233683: step 4387, loss 0.00328096, acc 1
2016-09-07T17:26:05.910537: step 4388, loss 0.0270694, acc 1
2016-09-07T17:26:06.582972: step 4389, loss 0.0166274, acc 1
2016-09-07T17:26:07.250974: step 4390, loss 0.0228838, acc 1
2016-09-07T17:26:07.921992: step 4391, loss 0.0811712, acc 0.96
2016-09-07T17:26:08.586034: step 4392, loss 0.0374709, acc 0.98
2016-09-07T17:26:09.257954: step 4393, loss 0.0348327, acc 1
2016-09-07T17:26:09.932205: step 4394, loss 0.0226349, acc 1
2016-09-07T17:26:10.592613: step 4395, loss 0.02947, acc 1
2016-09-07T17:26:11.257350: step 4396, loss 0.0307828, acc 0.98
2016-09-07T17:26:11.930898: step 4397, loss 0.00143702, acc 1
2016-09-07T17:26:12.604855: step 4398, loss 0.0302601, acc 0.98
2016-09-07T17:26:13.263213: step 4399, loss 0.0283613, acc 1
2016-09-07T17:26:13.937601: step 4400, loss 0.0930217, acc 0.98

Evaluation:
2016-09-07T17:26:16.812117: step 4400, loss 1.90152, acc 0.74

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-4400

2016-09-07T17:26:18.401642: step 4401, loss 0.0110548, acc 1
2016-09-07T17:26:19.066517: step 4402, loss 0.0278812, acc 0.98
2016-09-07T17:26:19.732383: step 4403, loss 0.0816661, acc 0.98
2016-09-07T17:26:20.415939: step 4404, loss 0.00287407, acc 1
2016-09-07T17:26:21.081501: step 4405, loss 0.0370753, acc 0.98
2016-09-07T17:26:21.739229: step 4406, loss 0.00963169, acc 1
2016-09-07T17:26:22.419545: step 4407, loss 0.0014891, acc 1
2016-09-07T17:26:23.088775: step 4408, loss 0.0656899, acc 0.98
2016-09-07T17:26:23.755115: step 4409, loss 0.0334985, acc 0.98
2016-09-07T17:26:24.422765: step 4410, loss 0.0217062, acc 0.98
2016-09-07T17:26:25.089196: step 4411, loss 0.00853131, acc 1
2016-09-07T17:26:25.766053: step 4412, loss 0.00825487, acc 1
2016-09-07T17:26:26.448010: step 4413, loss 0.015202, acc 0.98
2016-09-07T17:26:27.132584: step 4414, loss 0.00291346, acc 1
2016-09-07T17:26:27.804788: step 4415, loss 0.0054188, acc 1
2016-09-07T17:26:28.474679: step 4416, loss 0.0542522, acc 0.96
2016-09-07T17:26:29.141677: step 4417, loss 0.00638769, acc 1
2016-09-07T17:26:29.819099: step 4418, loss 0.00902898, acc 1
2016-09-07T17:26:30.494096: step 4419, loss 0.030411, acc 0.98
2016-09-07T17:26:31.152781: step 4420, loss 0.0690833, acc 0.96
2016-09-07T17:26:31.821831: step 4421, loss 0.0168518, acc 0.98
2016-09-07T17:26:32.495245: step 4422, loss 0.0270532, acc 0.98
2016-09-07T17:26:33.158801: step 4423, loss 0.00355056, acc 1
2016-09-07T17:26:33.827602: step 4424, loss 0.0270602, acc 0.98
2016-09-07T17:26:34.501181: step 4425, loss 0.00196208, acc 1
2016-09-07T17:26:35.178326: step 4426, loss 0.0454459, acc 0.96
2016-09-07T17:26:35.849274: step 4427, loss 0.0150467, acc 1
2016-09-07T17:26:36.525106: step 4428, loss 0.00324055, acc 1
2016-09-07T17:26:37.215719: step 4429, loss 0.0769207, acc 0.94
2016-09-07T17:26:37.899431: step 4430, loss 0.0294088, acc 0.98
2016-09-07T17:26:38.576608: step 4431, loss 0.00271799, acc 1
2016-09-07T17:26:39.256801: step 4432, loss 0.058151, acc 0.98
2016-09-07T17:26:39.934626: step 4433, loss 0.000280105, acc 1
2016-09-07T17:26:40.603813: step 4434, loss 0.0258711, acc 1
2016-09-07T17:26:41.257905: step 4435, loss 0.0602382, acc 0.98
2016-09-07T17:26:41.942630: step 4436, loss 0.00859584, acc 1
2016-09-07T17:26:42.606271: step 4437, loss 0.0038519, acc 1
2016-09-07T17:26:43.283044: step 4438, loss 0.109536, acc 0.96
2016-09-07T17:26:43.938952: step 4439, loss 0.0307466, acc 0.98
2016-09-07T17:26:44.607975: step 4440, loss 0.0170319, acc 0.98
2016-09-07T17:26:45.274723: step 4441, loss 0.0246903, acc 0.98
2016-09-07T17:26:45.947971: step 4442, loss 0.000416487, acc 1
2016-09-07T17:26:46.613160: step 4443, loss 0.0107, acc 1
2016-09-07T17:26:47.299790: step 4444, loss 0.00397841, acc 1
2016-09-07T17:26:47.960495: step 4445, loss 0.0115488, acc 1
2016-09-07T17:26:48.629739: step 4446, loss 0.0193945, acc 1
2016-09-07T17:26:49.293414: step 4447, loss 0.0196562, acc 1
2016-09-07T17:26:49.986125: step 4448, loss 0.0154543, acc 1
2016-09-07T17:26:50.655176: step 4449, loss 0.0203871, acc 0.98
2016-09-07T17:26:51.338046: step 4450, loss 0.0155548, acc 1
2016-09-07T17:26:52.004744: step 4451, loss 0.0114613, acc 1
2016-09-07T17:26:52.675666: step 4452, loss 0.0208038, acc 1
2016-09-07T17:26:53.352208: step 4453, loss 0.0301347, acc 0.98
2016-09-07T17:26:54.024341: step 4454, loss 0.0324254, acc 0.98
2016-09-07T17:26:54.699525: step 4455, loss 0.0312161, acc 1
2016-09-07T17:26:55.364701: step 4456, loss 0.0254321, acc 0.98
2016-09-07T17:26:56.045013: step 4457, loss 0.0283392, acc 1
2016-09-07T17:26:56.709237: step 4458, loss 0.0409612, acc 0.98
2016-09-07T17:26:57.387750: step 4459, loss 0.0771886, acc 0.94
2016-09-07T17:26:58.059575: step 4460, loss 0.00690861, acc 1
2016-09-07T17:26:58.712209: step 4461, loss 0.000262514, acc 1
2016-09-07T17:26:59.067002: step 4462, loss 0.0392179, acc 1
2016-09-07T17:26:59.730621: step 4463, loss 0.0397449, acc 0.98
2016-09-07T17:27:00.416666: step 4464, loss 0.0347366, acc 0.98
2016-09-07T17:27:01.098667: step 4465, loss 0.0344533, acc 0.98
2016-09-07T17:27:01.747780: step 4466, loss 0.0264637, acc 0.98
2016-09-07T17:27:02.418014: step 4467, loss 0.00234314, acc 1
2016-09-07T17:27:03.076932: step 4468, loss 0.0394643, acc 0.96
2016-09-07T17:27:03.755020: step 4469, loss 0.0343928, acc 0.98
2016-09-07T17:27:04.424316: step 4470, loss 0.111541, acc 0.98
2016-09-07T17:27:05.091958: step 4471, loss 0.00314679, acc 1
2016-09-07T17:27:05.770847: step 4472, loss 0.00107274, acc 1
2016-09-07T17:27:06.438215: step 4473, loss 0.0835252, acc 0.96
2016-09-07T17:27:07.110729: step 4474, loss 0.0189313, acc 0.98
2016-09-07T17:27:07.802386: step 4475, loss 0.0962978, acc 0.92
2016-09-07T17:27:08.487654: step 4476, loss 0.00943183, acc 1
2016-09-07T17:27:09.158113: step 4477, loss 0.0188884, acc 0.98
2016-09-07T17:27:09.820181: step 4478, loss 0.00371909, acc 1
2016-09-07T17:27:10.515307: step 4479, loss 0.0194296, acc 0.98
2016-09-07T17:27:11.179664: step 4480, loss 0.0126829, acc 1
2016-09-07T17:27:11.876377: step 4481, loss 0.000303058, acc 1
2016-09-07T17:27:12.813776: step 4482, loss 0.0063216, acc 1
2016-09-07T17:27:13.755851: step 4483, loss 0.0253897, acc 0.98
2016-09-07T17:27:14.680915: step 4484, loss 0.0156916, acc 1
2016-09-07T17:27:15.598921: step 4485, loss 0.051406, acc 0.96
2016-09-07T17:27:16.541563: step 4486, loss 0.00609638, acc 1
2016-09-07T17:27:17.496816: step 4487, loss 0.0403559, acc 0.98
2016-09-07T17:27:18.427508: step 4488, loss 0.00745201, acc 1
2016-09-07T17:27:19.402767: step 4489, loss 0.000426486, acc 1
2016-09-07T17:27:20.338721: step 4490, loss 0.00397123, acc 1
2016-09-07T17:27:21.294155: step 4491, loss 0.00694964, acc 1
2016-09-07T17:27:22.218737: step 4492, loss 0.00937685, acc 1
2016-09-07T17:27:23.042354: step 4493, loss 0.0391903, acc 0.98
2016-09-07T17:27:23.736261: step 4494, loss 0.0371601, acc 0.98
2016-09-07T17:27:24.436742: step 4495, loss 0.00496051, acc 1
2016-09-07T17:27:25.104462: step 4496, loss 0.0277339, acc 0.98
2016-09-07T17:27:25.795002: step 4497, loss 0.00715864, acc 1
2016-09-07T17:27:26.472960: step 4498, loss 0.207889, acc 0.98
2016-09-07T17:27:27.144244: step 4499, loss 0.0116171, acc 1
2016-09-07T17:27:27.829925: step 4500, loss 0.0346268, acc 0.98

Evaluation:
2016-09-07T17:27:30.799123: step 4500, loss 2.31552, acc 0.743

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-4500

2016-09-07T17:27:32.620868: step 4501, loss 0.0204775, acc 0.98
2016-09-07T17:27:33.295375: step 4502, loss 0.000270961, acc 1
2016-09-07T17:27:33.971834: step 4503, loss 0.0426696, acc 0.98
2016-09-07T17:27:34.669938: step 4504, loss 0.00302375, acc 1
2016-09-07T17:27:35.371965: step 4505, loss 0.00354944, acc 1
2016-09-07T17:27:36.056336: step 4506, loss 0.0160086, acc 1
2016-09-07T17:27:36.744425: step 4507, loss 0.10498, acc 0.92
2016-09-07T17:27:37.426355: step 4508, loss 0.0238834, acc 0.98
2016-09-07T17:27:38.096450: step 4509, loss 0.0206551, acc 1
2016-09-07T17:27:38.787942: step 4510, loss 0.042074, acc 0.98
2016-09-07T17:27:39.486869: step 4511, loss 0.00383629, acc 1
2016-09-07T17:27:40.196114: step 4512, loss 0.0608152, acc 0.98
2016-09-07T17:27:40.907206: step 4513, loss 0.0584592, acc 0.96
2016-09-07T17:27:41.599967: step 4514, loss 0.10122, acc 0.98
2016-09-07T17:27:42.273499: step 4515, loss 0.00109022, acc 1
2016-09-07T17:27:42.950258: step 4516, loss 0.0222023, acc 0.98
2016-09-07T17:27:43.653216: step 4517, loss 0.00311623, acc 1
2016-09-07T17:27:44.325804: step 4518, loss 0.00783431, acc 1
2016-09-07T17:27:45.015819: step 4519, loss 0.0348361, acc 0.98
2016-09-07T17:27:45.691252: step 4520, loss 0.000205401, acc 1
2016-09-07T17:27:46.374775: step 4521, loss 0.101194, acc 0.94
2016-09-07T17:27:47.076270: step 4522, loss 0.00102341, acc 1
2016-09-07T17:27:47.745980: step 4523, loss 0.0107188, acc 1
2016-09-07T17:27:48.449509: step 4524, loss 0.00601016, acc 1
2016-09-07T17:27:49.129620: step 4525, loss 0.0208233, acc 0.98
2016-09-07T17:27:49.830613: step 4526, loss 0.152477, acc 0.96
2016-09-07T17:27:50.505868: step 4527, loss 0.198034, acc 0.98
2016-09-07T17:27:51.188295: step 4528, loss 0.0948313, acc 0.96
2016-09-07T17:27:51.853534: step 4529, loss 0.0193311, acc 1
2016-09-07T17:27:52.546950: step 4530, loss 9.65282e-05, acc 1
2016-09-07T17:27:53.250329: step 4531, loss 0.0161324, acc 0.98
2016-09-07T17:27:53.937296: step 4532, loss 0.074678, acc 0.96
2016-09-07T17:27:54.641892: step 4533, loss 0.0692849, acc 0.98
2016-09-07T17:27:55.304018: step 4534, loss 0.00584178, acc 1
2016-09-07T17:27:55.984599: step 4535, loss 0.00470037, acc 1
2016-09-07T17:27:56.659131: step 4536, loss 0.0928001, acc 0.98
2016-09-07T17:27:57.335984: step 4537, loss 0.0528011, acc 0.98
2016-09-07T17:27:58.043207: step 4538, loss 0.0750407, acc 0.94
2016-09-07T17:27:58.723825: step 4539, loss 0.0681612, acc 0.96
2016-09-07T17:27:59.429385: step 4540, loss 0.0222479, acc 1
2016-09-07T17:28:00.140292: step 4541, loss 0.0772231, acc 0.96
2016-09-07T17:28:00.868185: step 4542, loss 0.0306189, acc 0.98
2016-09-07T17:28:01.571548: step 4543, loss 0.0181072, acc 1
2016-09-07T17:28:02.257757: step 4544, loss 0.063053, acc 0.96
2016-09-07T17:28:02.940120: step 4545, loss 0.0029217, acc 1
2016-09-07T17:28:03.612157: step 4546, loss 0.0196867, acc 1
2016-09-07T17:28:04.322063: step 4547, loss 0.02213, acc 0.98
2016-09-07T17:28:05.016935: step 4548, loss 0.0184836, acc 0.98
2016-09-07T17:28:05.697713: step 4549, loss 0.0192395, acc 1
2016-09-07T17:28:06.368097: step 4550, loss 0.0495292, acc 0.98
2016-09-07T17:28:07.043803: step 4551, loss 0.0400847, acc 0.98
2016-09-07T17:28:07.744590: step 4552, loss 0.0143477, acc 1
2016-09-07T17:28:08.423809: step 4553, loss 0.0117936, acc 1
2016-09-07T17:28:09.132676: step 4554, loss 0.10788, acc 0.98
2016-09-07T17:28:09.845772: step 4555, loss 0.0595635, acc 0.98
2016-09-07T17:28:10.537416: step 4556, loss 0.0266193, acc 0.98
2016-09-07T17:28:11.239775: step 4557, loss 0.0258774, acc 1
2016-09-07T17:28:11.916796: step 4558, loss 0.0207266, acc 0.98
2016-09-07T17:28:12.633358: step 4559, loss 0.0166979, acc 0.98
2016-09-07T17:28:13.314048: step 4560, loss 0.0399981, acc 0.98
2016-09-07T17:28:14.049146: step 4561, loss 0.021943, acc 1
2016-09-07T17:28:14.744930: step 4562, loss 0.0488559, acc 0.96
2016-09-07T17:28:15.427269: step 4563, loss 0.0149451, acc 1
2016-09-07T17:28:16.118177: step 4564, loss 0.0242408, acc 0.98
2016-09-07T17:28:16.783285: step 4565, loss 0.0187179, acc 0.98
2016-09-07T17:28:17.475172: step 4566, loss 0.0293715, acc 0.98
2016-09-07T17:28:18.150467: step 4567, loss 0.0431208, acc 0.96
2016-09-07T17:28:18.842535: step 4568, loss 0.0623943, acc 0.96
2016-09-07T17:28:19.526638: step 4569, loss 0.0407466, acc 0.98
2016-09-07T17:28:20.216789: step 4570, loss 0.0719974, acc 0.96
2016-09-07T17:28:20.906520: step 4571, loss 0.0014238, acc 1
2016-09-07T17:28:21.585320: step 4572, loss 0.0142963, acc 1
2016-09-07T17:28:22.298730: step 4573, loss 0.0322797, acc 0.98
2016-09-07T17:28:22.968215: step 4574, loss 0.0099155, acc 1
2016-09-07T17:28:23.642918: step 4575, loss 0.0139713, acc 1
2016-09-07T17:28:24.335537: step 4576, loss 0.0189799, acc 0.98
2016-09-07T17:28:25.012165: step 4577, loss 0.00584111, acc 1
2016-09-07T17:28:25.731727: step 4578, loss 0.00610091, acc 1
2016-09-07T17:28:26.399165: step 4579, loss 0.0154843, acc 0.98
2016-09-07T17:28:27.123485: step 4580, loss 0.0229637, acc 0.98
2016-09-07T17:28:27.790634: step 4581, loss 0.0621958, acc 0.98
2016-09-07T17:28:28.472387: step 4582, loss 0.0779561, acc 0.94
2016-09-07T17:28:29.179775: step 4583, loss 0.0245133, acc 1
2016-09-07T17:28:29.865251: step 4584, loss 0.0298642, acc 0.98
2016-09-07T17:28:30.570894: step 4585, loss 0.00107808, acc 1
2016-09-07T17:28:31.237547: step 4586, loss 0.011727, acc 1
2016-09-07T17:28:31.955781: step 4587, loss 0.0728835, acc 0.96
2016-09-07T17:28:32.648434: step 4588, loss 0.0270136, acc 0.98
2016-09-07T17:28:33.345106: step 4589, loss 0.0304262, acc 0.98
2016-09-07T17:28:34.025945: step 4590, loss 0.0110713, acc 1
2016-09-07T17:28:34.708856: step 4591, loss 0.00294302, acc 1
2016-09-07T17:28:35.377639: step 4592, loss 0.00873232, acc 1
2016-09-07T17:28:36.047117: step 4593, loss 0.030025, acc 0.98
2016-09-07T17:28:36.765929: step 4594, loss 0.0260938, acc 1
2016-09-07T17:28:37.454953: step 4595, loss 0.000717298, acc 1
2016-09-07T17:28:38.128404: step 4596, loss 0.0206995, acc 1
2016-09-07T17:28:38.810725: step 4597, loss 0.0295733, acc 1
2016-09-07T17:28:39.499354: step 4598, loss 0.0250539, acc 0.98
2016-09-07T17:28:40.189354: step 4599, loss 0.000183008, acc 1
2016-09-07T17:28:40.847219: step 4600, loss 0.0193348, acc 0.98

Evaluation:
2016-09-07T17:28:43.811006: step 4600, loss 2.18169, acc 0.739

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-4600

2016-09-07T17:28:45.467745: step 4601, loss 0.0264253, acc 0.98
2016-09-07T17:28:46.153401: step 4602, loss 0.156286, acc 0.94
2016-09-07T17:28:46.814711: step 4603, loss 0.0185798, acc 0.98
2016-09-07T17:28:47.517472: step 4604, loss 0.00633385, acc 1
2016-09-07T17:28:48.185429: step 4605, loss 0.0101738, acc 1
2016-09-07T17:28:48.876240: step 4606, loss 0.041453, acc 0.98
2016-09-07T17:28:49.561660: step 4607, loss 0.0757162, acc 0.94
2016-09-07T17:28:50.251504: step 4608, loss 0.0289415, acc 0.98
2016-09-07T17:28:50.935626: step 4609, loss 0.0205974, acc 0.98
2016-09-07T17:28:51.625945: step 4610, loss 0.0457049, acc 0.98
2016-09-07T17:28:52.326423: step 4611, loss 0.00739228, acc 1
2016-09-07T17:28:52.993498: step 4612, loss 0.0195383, acc 1
2016-09-07T17:28:53.664963: step 4613, loss 0.0140853, acc 1
2016-09-07T17:28:54.352762: step 4614, loss 0.0195186, acc 1
2016-09-07T17:28:55.039902: step 4615, loss 0.0261125, acc 1
2016-09-07T17:28:55.747329: step 4616, loss 0.0457302, acc 0.98
2016-09-07T17:28:56.430344: step 4617, loss 0.0735005, acc 0.96
2016-09-07T17:28:57.143018: step 4618, loss 0.00384292, acc 1
2016-09-07T17:28:57.831970: step 4619, loss 0.000273246, acc 1
2016-09-07T17:28:58.522798: step 4620, loss 0.0362013, acc 0.98
2016-09-07T17:28:59.211342: step 4621, loss 0.000857481, acc 1
2016-09-07T17:28:59.894989: step 4622, loss 0.0154064, acc 1
2016-09-07T17:29:00.621729: step 4623, loss 0.0821455, acc 0.98
2016-09-07T17:29:01.288084: step 4624, loss 0.0596668, acc 0.98
2016-09-07T17:29:01.983342: step 4625, loss 0.00798837, acc 1
2016-09-07T17:29:02.673451: step 4626, loss 0.0622972, acc 0.96
2016-09-07T17:29:03.363661: step 4627, loss 0.0684559, acc 0.98
2016-09-07T17:29:04.046702: step 4628, loss 0.0148668, acc 1
2016-09-07T17:29:04.733451: step 4629, loss 0.0114748, acc 1
2016-09-07T17:29:05.431926: step 4630, loss 0.0288927, acc 0.98
2016-09-07T17:29:06.113449: step 4631, loss 0.120017, acc 0.98
2016-09-07T17:29:06.817449: step 4632, loss 0.0285238, acc 0.98
2016-09-07T17:29:07.500564: step 4633, loss 0.0031303, acc 1
2016-09-07T17:29:08.180719: step 4634, loss 0.0279656, acc 0.98
2016-09-07T17:29:08.882134: step 4635, loss 0.026344, acc 1
2016-09-07T17:29:09.573842: step 4636, loss 0.0383772, acc 0.98
2016-09-07T17:29:10.282515: step 4637, loss 0.048621, acc 0.98
2016-09-07T17:29:10.961272: step 4638, loss 0.0114596, acc 1
2016-09-07T17:29:11.644691: step 4639, loss 0.0267058, acc 0.98
2016-09-07T17:29:12.327019: step 4640, loss 0.0423765, acc 0.98
2016-09-07T17:29:13.033781: step 4641, loss 0.04203, acc 0.98
2016-09-07T17:29:13.737284: step 4642, loss 0.00247993, acc 1
2016-09-07T17:29:14.430457: step 4643, loss 0.0345998, acc 0.98
2016-09-07T17:29:15.123512: step 4644, loss 0.155439, acc 0.9
2016-09-07T17:29:15.819207: step 4645, loss 0.00555767, acc 1
2016-09-07T17:29:16.502733: step 4646, loss 0.0107436, acc 1
2016-09-07T17:29:17.191320: step 4647, loss 0.0376881, acc 0.98
2016-09-07T17:29:17.881213: step 4648, loss 0.0312839, acc 0.98
2016-09-07T17:29:18.578783: step 4649, loss 0.0176525, acc 1
2016-09-07T17:29:19.254788: step 4650, loss 0.0117186, acc 1
2016-09-07T17:29:19.969504: step 4651, loss 0.0254223, acc 0.98
2016-09-07T17:29:20.649453: step 4652, loss 0.0255678, acc 1
2016-09-07T17:29:21.350720: step 4653, loss 0.0125403, acc 1
2016-09-07T17:29:22.035031: step 4654, loss 0.0687671, acc 0.96
2016-09-07T17:29:22.720585: step 4655, loss 0.00707762, acc 1
2016-09-07T17:29:23.083567: step 4656, loss 0.0300288, acc 1
2016-09-07T17:29:23.768663: step 4657, loss 0.0123241, acc 1
2016-09-07T17:29:24.445780: step 4658, loss 0.0437368, acc 0.98
2016-09-07T17:29:25.125122: step 4659, loss 0.00758682, acc 1
2016-09-07T17:29:25.799082: step 4660, loss 0.0927534, acc 0.92
2016-09-07T17:29:26.489835: step 4661, loss 0.0315165, acc 1
2016-09-07T17:29:27.217898: step 4662, loss 0.0186776, acc 1
2016-09-07T17:29:27.911991: step 4663, loss 0.0462811, acc 0.98
2016-09-07T17:29:28.591741: step 4664, loss 0.0146651, acc 1
2016-09-07T17:29:29.288749: step 4665, loss 0.00131717, acc 1
2016-09-07T17:29:29.979548: step 4666, loss 0.015925, acc 1
2016-09-07T17:29:30.657689: step 4667, loss 0.00574113, acc 1
2016-09-07T17:29:31.315865: step 4668, loss 0.0328363, acc 0.98
2016-09-07T17:29:32.026317: step 4669, loss 0.0376897, acc 0.98
2016-09-07T17:29:32.709387: step 4670, loss 0.0398017, acc 1
2016-09-07T17:29:33.395201: step 4671, loss 0.0197741, acc 1
2016-09-07T17:29:34.081902: step 4672, loss 0.0312392, acc 0.98
2016-09-07T17:29:34.762289: step 4673, loss 0.022211, acc 1
2016-09-07T17:29:35.439431: step 4674, loss 0.00580712, acc 1
2016-09-07T17:29:36.113001: step 4675, loss 0.0147284, acc 1
2016-09-07T17:29:36.797510: step 4676, loss 0.0294368, acc 0.98
2016-09-07T17:29:37.466191: step 4677, loss 0.0116439, acc 1
2016-09-07T17:29:38.172466: step 4678, loss 0.00807008, acc 1
2016-09-07T17:29:38.856931: step 4679, loss 0.00705313, acc 1
2016-09-07T17:29:39.537508: step 4680, loss 0.0176295, acc 0.98
2016-09-07T17:29:40.220659: step 4681, loss 0.00892916, acc 1
2016-09-07T17:29:40.900420: step 4682, loss 0.0160393, acc 1
2016-09-07T17:29:41.620842: step 4683, loss 0.0073476, acc 1
2016-09-07T17:29:42.294672: step 4684, loss 0.0021553, acc 1
2016-09-07T17:29:43.014198: step 4685, loss 0.0276211, acc 0.98
2016-09-07T17:29:43.692550: step 4686, loss 0.000331435, acc 1
2016-09-07T17:29:44.381321: step 4687, loss 0.050924, acc 0.96
2016-09-07T17:29:45.067705: step 4688, loss 0.0775555, acc 0.96
2016-09-07T17:29:45.735364: step 4689, loss 0.000422234, acc 1
2016-09-07T17:29:46.428232: step 4690, loss 0.0925597, acc 0.98
2016-09-07T17:29:47.120523: step 4691, loss 0.000291965, acc 1
2016-09-07T17:29:47.803001: step 4692, loss 0.00117658, acc 1
2016-09-07T17:29:48.490897: step 4693, loss 0.037783, acc 0.98
2016-09-07T17:29:49.179711: step 4694, loss 0.00738344, acc 1
2016-09-07T17:29:49.867840: step 4695, loss 0.00034364, acc 1
2016-09-07T17:29:50.562079: step 4696, loss 0.000901701, acc 1
2016-09-07T17:29:51.266250: step 4697, loss 0.0298581, acc 1
2016-09-07T17:29:51.930158: step 4698, loss 0.00196168, acc 1
2016-09-07T17:29:52.616722: step 4699, loss 0.0134916, acc 1
2016-09-07T17:29:53.314303: step 4700, loss 0.00488873, acc 1

Evaluation:
2016-09-07T17:29:56.280097: step 4700, loss 2.55727, acc 0.741

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-4700

2016-09-07T17:29:57.950450: step 4701, loss 0.0226036, acc 0.98
2016-09-07T17:29:58.652705: step 4702, loss 0.0145348, acc 1
2016-09-07T17:29:59.349647: step 4703, loss 0.0687377, acc 0.96
2016-09-07T17:30:00.040545: step 4704, loss 0.024305, acc 0.98
2016-09-07T17:30:00.764400: step 4705, loss 0.000572326, acc 1
2016-09-07T17:30:01.425637: step 4706, loss 0.0217466, acc 1
2016-09-07T17:30:02.128226: step 4707, loss 0.0300304, acc 0.98
2016-09-07T17:30:02.790341: step 4708, loss 0.0156558, acc 1
2016-09-07T17:30:03.474424: step 4709, loss 0.00374854, acc 1
2016-09-07T17:30:04.164415: step 4710, loss 0.00191907, acc 1
2016-09-07T17:30:04.840560: step 4711, loss 0.00113746, acc 1
2016-09-07T17:30:05.559625: step 4712, loss 0.00538404, acc 1
2016-09-07T17:30:06.226570: step 4713, loss 0.11053, acc 0.96
2016-09-07T17:30:06.922033: step 4714, loss 0.000363259, acc 1
2016-09-07T17:30:07.620979: step 4715, loss 0.00165565, acc 1
2016-09-07T17:30:08.293555: step 4716, loss 0.00423044, acc 1
2016-09-07T17:30:08.979496: step 4717, loss 0.0711936, acc 0.96
2016-09-07T17:30:09.676484: step 4718, loss 0.0187264, acc 0.98
2016-09-07T17:30:10.374246: step 4719, loss 0.0284699, acc 1
2016-09-07T17:30:11.038812: step 4720, loss 0.00687457, acc 1
2016-09-07T17:30:11.727184: step 4721, loss 0.0440673, acc 0.96
2016-09-07T17:30:12.400959: step 4722, loss 0.0176401, acc 1
2016-09-07T17:30:13.105743: step 4723, loss 0.0290604, acc 0.98
2016-09-07T17:30:13.782731: step 4724, loss 0.0408144, acc 0.98
2016-09-07T17:30:14.446261: step 4725, loss 0.019929, acc 0.98
2016-09-07T17:30:15.132023: step 4726, loss 0.03178, acc 0.98
2016-09-07T17:30:15.813195: step 4727, loss 0.00959028, acc 1
2016-09-07T17:30:16.501575: step 4728, loss 0.0370408, acc 0.98
2016-09-07T17:30:17.180304: step 4729, loss 0.00053068, acc 1
2016-09-07T17:30:17.869657: step 4730, loss 0.000158339, acc 1
2016-09-07T17:30:18.557779: step 4731, loss 0.0147076, acc 0.98
2016-09-07T17:30:19.238773: step 4732, loss 0.0366252, acc 0.98
2016-09-07T17:30:19.919639: step 4733, loss 0.000193808, acc 1
2016-09-07T17:30:20.579820: step 4734, loss 0.000509093, acc 1
2016-09-07T17:30:21.261221: step 4735, loss 0.0162458, acc 0.98
2016-09-07T17:30:21.929651: step 4736, loss 0.0724018, acc 0.98
2016-09-07T17:30:22.649356: step 4737, loss 0.0110794, acc 1
2016-09-07T17:30:23.335469: step 4738, loss 0.00278, acc 1
2016-09-07T17:30:24.033697: step 4739, loss 0.0557484, acc 0.96
2016-09-07T17:30:24.713368: step 4740, loss 0.00022364, acc 1
2016-09-07T17:30:25.398233: step 4741, loss 0.00892676, acc 1
2016-09-07T17:30:26.092561: step 4742, loss 0.0296582, acc 0.98
2016-09-07T17:30:26.776479: step 4743, loss 0.00061949, acc 1
2016-09-07T17:30:27.486868: step 4744, loss 0.0136834, acc 1
2016-09-07T17:30:28.193616: step 4745, loss 0.0165736, acc 1
2016-09-07T17:30:28.882879: step 4746, loss 0.0103093, acc 1
2016-09-07T17:30:29.582538: step 4747, loss 0.0104178, acc 1
2016-09-07T17:30:30.277750: step 4748, loss 0.019536, acc 1
2016-09-07T17:30:30.968935: step 4749, loss 0.0240575, acc 0.98
2016-09-07T17:30:31.647170: step 4750, loss 0.000100611, acc 1
2016-09-07T17:30:32.325352: step 4751, loss 0.0712115, acc 0.98
2016-09-07T17:30:33.005456: step 4752, loss 0.000575414, acc 1
2016-09-07T17:30:33.697484: step 4753, loss 0.0172243, acc 0.98
2016-09-07T17:30:34.382387: step 4754, loss 0.0251927, acc 0.98
2016-09-07T17:30:35.068991: step 4755, loss 0.00356071, acc 1
2016-09-07T17:30:35.761505: step 4756, loss 0.057777, acc 0.98
2016-09-07T17:30:36.440966: step 4757, loss 0.0406541, acc 0.98
2016-09-07T17:30:37.118449: step 4758, loss 0.00386945, acc 1
2016-09-07T17:30:37.813137: step 4759, loss 0.0263088, acc 1
2016-09-07T17:30:38.508913: step 4760, loss 0.0155785, acc 0.98
2016-09-07T17:30:39.189896: step 4761, loss 0.00499016, acc 1
2016-09-07T17:30:39.869885: step 4762, loss 0.0103512, acc 1
2016-09-07T17:30:40.572770: step 4763, loss 0.0478436, acc 0.98
2016-09-07T17:30:41.252592: step 4764, loss 0.0212836, acc 0.98
2016-09-07T17:30:41.943655: step 4765, loss 0.00850126, acc 1
2016-09-07T17:30:42.646338: step 4766, loss 0.0116143, acc 1
2016-09-07T17:30:43.350196: step 4767, loss 0.0315224, acc 0.98
2016-09-07T17:30:44.033485: step 4768, loss 0.0227646, acc 0.98
2016-09-07T17:30:44.729857: step 4769, loss 0.0377807, acc 0.98
2016-09-07T17:30:45.431674: step 4770, loss 0.0514136, acc 0.98
2016-09-07T17:30:46.119901: step 4771, loss 0.00816041, acc 1
2016-09-07T17:30:46.804813: step 4772, loss 0.104545, acc 0.96
2016-09-07T17:30:47.483790: step 4773, loss 0.0338442, acc 0.98
2016-09-07T17:30:48.158943: step 4774, loss 0.0233706, acc 0.98
2016-09-07T17:30:48.844652: step 4775, loss 0.0124129, acc 1
2016-09-07T17:30:49.517466: step 4776, loss 0.0194751, acc 0.98
2016-09-07T17:30:50.228815: step 4777, loss 0.0263763, acc 1
2016-09-07T17:30:50.907401: step 4778, loss 0.000373971, acc 1
2016-09-07T17:30:51.581967: step 4779, loss 0.0137808, acc 1
2016-09-07T17:30:52.259078: step 4780, loss 0.00648011, acc 1
2016-09-07T17:30:52.938909: step 4781, loss 0.0140394, acc 1
2016-09-07T17:30:53.618721: step 4782, loss 0.159672, acc 0.96
2016-09-07T17:30:54.310052: step 4783, loss 0.0799956, acc 0.98
2016-09-07T17:30:55.002702: step 4784, loss 9.57689e-05, acc 1
2016-09-07T17:30:55.664281: step 4785, loss 0.0367656, acc 0.98
2016-09-07T17:30:56.332400: step 4786, loss 0.0268826, acc 1
2016-09-07T17:30:57.000589: step 4787, loss 0.0497354, acc 0.98
2016-09-07T17:30:57.680964: step 4788, loss 0.170509, acc 0.92
2016-09-07T17:30:58.360687: step 4789, loss 0.111325, acc 0.98
2016-09-07T17:30:59.029673: step 4790, loss 0.0377839, acc 0.96
2016-09-07T17:30:59.720677: step 4791, loss 0.0370231, acc 0.98
2016-09-07T17:31:00.424363: step 4792, loss 0.0104509, acc 1
2016-09-07T17:31:01.128637: step 4793, loss 0.0370714, acc 0.98
2016-09-07T17:31:01.820803: step 4794, loss 0.00211804, acc 1
2016-09-07T17:31:02.501731: step 4795, loss 0.00724236, acc 1
2016-09-07T17:31:03.187348: step 4796, loss 0.0339482, acc 0.98
2016-09-07T17:31:03.878617: step 4797, loss 0.0173923, acc 1
2016-09-07T17:31:04.587605: step 4798, loss 0.0171236, acc 0.98
2016-09-07T17:31:05.269870: step 4799, loss 0.0568741, acc 0.98
2016-09-07T17:31:05.956508: step 4800, loss 5.01227e-05, acc 1

Evaluation:
2016-09-07T17:31:08.928809: step 4800, loss 1.9912, acc 0.746

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-4800

2016-09-07T17:31:10.598362: step 4801, loss 0.0464304, acc 0.98
2016-09-07T17:31:11.255224: step 4802, loss 0.0175077, acc 1
2016-09-07T17:31:11.963924: step 4803, loss 0.00974843, acc 1
2016-09-07T17:31:12.643123: step 4804, loss 0.0173202, acc 1
2016-09-07T17:31:13.334874: step 4805, loss 0.00165442, acc 1
2016-09-07T17:31:14.010398: step 4806, loss 0.00225518, acc 1
2016-09-07T17:31:14.690097: step 4807, loss 0.0273987, acc 0.98
2016-09-07T17:31:15.389031: step 4808, loss 0.0360182, acc 0.98
2016-09-07T17:31:16.093018: step 4809, loss 0.032344, acc 0.98
2016-09-07T17:31:16.788076: step 4810, loss 0.064622, acc 0.96
2016-09-07T17:31:17.470208: step 4811, loss 0.0148515, acc 1
2016-09-07T17:31:18.157869: step 4812, loss 0.000471627, acc 1
2016-09-07T17:31:18.839577: step 4813, loss 0.0297493, acc 0.98
2016-09-07T17:31:19.523676: step 4814, loss 0.0329158, acc 0.96
2016-09-07T17:31:20.246075: step 4815, loss 0.00392096, acc 1
2016-09-07T17:31:20.932142: step 4816, loss 0.00722066, acc 1
2016-09-07T17:31:21.623816: step 4817, loss 0.0113241, acc 1
2016-09-07T17:31:22.320380: step 4818, loss 0.0322416, acc 0.98
2016-09-07T17:31:23.014653: step 4819, loss 0.0293296, acc 0.98
2016-09-07T17:31:23.705418: step 4820, loss 0.0483375, acc 0.96
2016-09-07T17:31:24.368106: step 4821, loss 0.00122218, acc 1
2016-09-07T17:31:25.064562: step 4822, loss 0.0853694, acc 0.96
2016-09-07T17:31:25.728123: step 4823, loss 0.0359483, acc 0.98
2016-09-07T17:31:26.416244: step 4824, loss 0.0171899, acc 1
2016-09-07T17:31:27.122535: step 4825, loss 0.0258322, acc 0.98
2016-09-07T17:31:27.811094: step 4826, loss 0.0297785, acc 1
2016-09-07T17:31:28.491138: step 4827, loss 0.0339178, acc 0.98
2016-09-07T17:31:29.190483: step 4828, loss 0.0165777, acc 1
2016-09-07T17:31:29.880589: step 4829, loss 0.0120837, acc 1
2016-09-07T17:31:30.556499: step 4830, loss 0.00151693, acc 1
2016-09-07T17:31:31.259049: step 4831, loss 0.0568358, acc 0.94
2016-09-07T17:31:31.950051: step 4832, loss 0.0707421, acc 0.94
2016-09-07T17:31:32.640143: step 4833, loss 0.00706917, acc 1
2016-09-07T17:31:33.343755: step 4834, loss 0.0137085, acc 1
2016-09-07T17:31:34.095332: step 4835, loss 0.0356046, acc 0.98
2016-09-07T17:31:34.799382: step 4836, loss 0.0436535, acc 0.98
2016-09-07T17:31:35.483223: step 4837, loss 0.0446466, acc 0.96
2016-09-07T17:31:36.175925: step 4838, loss 0.00378637, acc 1
2016-09-07T17:31:36.872956: step 4839, loss 0.016364, acc 0.98
2016-09-07T17:31:37.565631: step 4840, loss 0.00463969, acc 1
2016-09-07T17:31:38.255631: step 4841, loss 0.0106822, acc 1
2016-09-07T17:31:38.933732: step 4842, loss 0.0221917, acc 1
2016-09-07T17:31:39.615871: step 4843, loss 0.061075, acc 0.94
2016-09-07T17:31:40.315411: step 4844, loss 0.00239041, acc 1
2016-09-07T17:31:40.988950: step 4845, loss 0.0524192, acc 0.98
2016-09-07T17:31:41.695336: step 4846, loss 0.0545757, acc 0.98
2016-09-07T17:31:42.399179: step 4847, loss 0.00746675, acc 1
2016-09-07T17:31:43.102954: step 4848, loss 0.0151301, acc 0.98
2016-09-07T17:31:43.788377: step 4849, loss 0.000524582, acc 1
2016-09-07T17:31:44.162051: step 4850, loss 0.0602995, acc 1
2016-09-07T17:31:44.823631: step 4851, loss 0.0470312, acc 0.96
2016-09-07T17:31:45.514777: step 4852, loss 0.0320364, acc 1
2016-09-07T17:31:46.188767: step 4853, loss 0.0555219, acc 0.98
2016-09-07T17:31:46.864940: step 4854, loss 0.0198782, acc 0.98
2016-09-07T17:31:47.568402: step 4855, loss 0.0357877, acc 0.96
2016-09-07T17:31:48.251221: step 4856, loss 0.193837, acc 0.98
2016-09-07T17:31:48.936783: step 4857, loss 0.00305363, acc 1
2016-09-07T17:31:49.623755: step 4858, loss 0.0219905, acc 1
2016-09-07T17:31:50.328437: step 4859, loss 0.0237571, acc 0.98
2016-09-07T17:31:51.002801: step 4860, loss 0.0122542, acc 1
2016-09-07T17:31:51.689828: step 4861, loss 0.0168583, acc 0.98
2016-09-07T17:31:52.377835: step 4862, loss 0.0192029, acc 1
2016-09-07T17:31:53.063750: step 4863, loss 0.0492149, acc 0.94
2016-09-07T17:31:53.774525: step 4864, loss 0.0619301, acc 0.96
2016-09-07T17:31:54.440552: step 4865, loss 6.51241e-05, acc 1
2016-09-07T17:31:55.195094: step 4866, loss 0.0447268, acc 0.98
2016-09-07T17:31:55.906539: step 4867, loss 0.0158479, acc 0.98
2016-09-07T17:31:56.577415: step 4868, loss 0.00917408, acc 1
2016-09-07T17:31:57.291919: step 4869, loss 0.0369261, acc 0.96
2016-09-07T17:31:57.970974: step 4870, loss 0.00962952, acc 1
2016-09-07T17:31:58.672662: step 4871, loss 0.0785089, acc 0.96
2016-09-07T17:31:59.340029: step 4872, loss 0.0221037, acc 0.98
2016-09-07T17:32:00.023160: step 4873, loss 0.0224335, acc 0.98
2016-09-07T17:32:00.748774: step 4874, loss 0.0192692, acc 1
2016-09-07T17:32:01.419915: step 4875, loss 0.0317848, acc 1
2016-09-07T17:32:02.120268: step 4876, loss 0.000352584, acc 1
2016-09-07T17:32:02.812574: step 4877, loss 0.00107768, acc 1
2016-09-07T17:32:03.506885: step 4878, loss 0.042838, acc 0.98
2016-09-07T17:32:04.187048: step 4879, loss 0.0404537, acc 0.98
2016-09-07T17:32:04.878670: step 4880, loss 0.0476961, acc 0.98
2016-09-07T17:32:05.565278: step 4881, loss 0.0254068, acc 0.98
2016-09-07T17:32:06.263982: step 4882, loss 0.0767159, acc 0.98
2016-09-07T17:32:07.010176: step 4883, loss 0.0223868, acc 1
2016-09-07T17:32:07.672678: step 4884, loss 0.0219196, acc 0.98
2016-09-07T17:32:08.380383: step 4885, loss 0.0163184, acc 1
2016-09-07T17:32:09.067671: step 4886, loss 0.0197207, acc 1
2016-09-07T17:32:09.754556: step 4887, loss 0.0389851, acc 0.98
2016-09-07T17:32:10.437427: step 4888, loss 0.0318099, acc 0.98
2016-09-07T17:32:11.144391: step 4889, loss 0.102535, acc 0.98
2016-09-07T17:32:11.842034: step 4890, loss 0.0431745, acc 0.98
2016-09-07T17:32:12.515611: step 4891, loss 0.0240724, acc 1
2016-09-07T17:32:13.221217: step 4892, loss 0.014462, acc 1
2016-09-07T17:32:13.926830: step 4893, loss 0.0505847, acc 0.98
2016-09-07T17:32:14.620175: step 4894, loss 0.0346328, acc 0.98
2016-09-07T17:32:15.314984: step 4895, loss 0.0444535, acc 0.98
2016-09-07T17:32:16.015429: step 4896, loss 0.0294748, acc 1
2016-09-07T17:32:16.690151: step 4897, loss 0.0278869, acc 0.98
2016-09-07T17:32:17.365017: step 4898, loss 0.103651, acc 0.98
2016-09-07T17:32:18.066018: step 4899, loss 0.0931776, acc 0.98
2016-09-07T17:32:18.748399: step 4900, loss 0.0454446, acc 0.96

Evaluation:
2016-09-07T17:32:21.712726: step 4900, loss 2.06455, acc 0.734

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-4900

2016-09-07T17:32:23.423709: step 4901, loss 0.055071, acc 0.96
2016-09-07T17:32:24.139278: step 4902, loss 0.0355337, acc 0.98
2016-09-07T17:32:24.815819: step 4903, loss 0.0425329, acc 0.98
2016-09-07T17:32:25.493652: step 4904, loss 0.0156742, acc 1
2016-09-07T17:32:26.168200: step 4905, loss 0.0283851, acc 0.98
2016-09-07T17:32:26.830981: step 4906, loss 0.123629, acc 0.94
2016-09-07T17:32:27.526165: step 4907, loss 0.0113703, acc 1
2016-09-07T17:32:28.210189: step 4908, loss 0.00142662, acc 1
2016-09-07T17:32:28.893864: step 4909, loss 0.0052007, acc 1
2016-09-07T17:32:29.582218: step 4910, loss 0.0164638, acc 1
2016-09-07T17:32:30.278059: step 4911, loss 0.0309684, acc 0.98
2016-09-07T17:32:30.974280: step 4912, loss 0.0251906, acc 0.98
2016-09-07T17:32:31.668910: step 4913, loss 0.0593083, acc 0.98
2016-09-07T17:32:32.374760: step 4914, loss 0.0518312, acc 0.96
2016-09-07T17:32:33.063835: step 4915, loss 0.118862, acc 0.98
2016-09-07T17:32:33.747577: step 4916, loss 0.0485473, acc 0.96
2016-09-07T17:32:34.449772: step 4917, loss 0.0314273, acc 0.98
2016-09-07T17:32:35.136581: step 4918, loss 0.0197565, acc 0.98
2016-09-07T17:32:35.839178: step 4919, loss 0.00877673, acc 1
2016-09-07T17:32:36.534572: step 4920, loss 0.0635518, acc 0.98
2016-09-07T17:32:37.243199: step 4921, loss 0.0422581, acc 0.98
2016-09-07T17:32:37.910675: step 4922, loss 0.0198101, acc 1
2016-09-07T17:32:38.600702: step 4923, loss 0.00321252, acc 1
2016-09-07T17:32:39.286484: step 4924, loss 0.042804, acc 0.98
2016-09-07T17:32:39.968994: step 4925, loss 0.0210227, acc 0.98
2016-09-07T17:32:40.690002: step 4926, loss 0.0298265, acc 1
2016-09-07T17:32:41.371295: step 4927, loss 0.0841525, acc 0.94
2016-09-07T17:32:42.076032: step 4928, loss 0.0127355, acc 1
2016-09-07T17:32:42.785239: step 4929, loss 0.00717957, acc 1
2016-09-07T17:32:43.483279: step 4930, loss 0.0203816, acc 0.98
2016-09-07T17:32:44.188749: step 4931, loss 0.047493, acc 0.96
2016-09-07T17:32:44.872130: step 4932, loss 0.0308313, acc 0.98
2016-09-07T17:32:45.570099: step 4933, loss 0.0156478, acc 1
2016-09-07T17:32:46.230571: step 4934, loss 0.0292367, acc 1
2016-09-07T17:32:46.937024: step 4935, loss 0.00559469, acc 1
2016-09-07T17:32:47.645731: step 4936, loss 0.0131141, acc 1
2016-09-07T17:32:48.314247: step 4937, loss 0.0294739, acc 0.98
2016-09-07T17:32:48.986504: step 4938, loss 0.0626264, acc 0.96
2016-09-07T17:32:49.677853: step 4939, loss 0.0188373, acc 1
2016-09-07T17:32:50.379309: step 4940, loss 0.047572, acc 0.98
2016-09-07T17:32:51.040048: step 4941, loss 0.00399502, acc 1
2016-09-07T17:32:51.742729: step 4942, loss 0.00109887, acc 1
2016-09-07T17:32:52.436312: step 4943, loss 0.00229702, acc 1
2016-09-07T17:32:53.130288: step 4944, loss 0.0985424, acc 0.96
2016-09-07T17:32:53.824965: step 4945, loss 0.00438206, acc 1
2016-09-07T17:32:54.502480: step 4946, loss 0.0804796, acc 0.96
2016-09-07T17:32:55.181957: step 4947, loss 0.0333477, acc 0.98
2016-09-07T17:32:55.849316: step 4948, loss 0.0469602, acc 0.98
2016-09-07T17:32:56.552137: step 4949, loss 0.0353066, acc 0.98
2016-09-07T17:32:57.260919: step 4950, loss 0.0495473, acc 0.98
2016-09-07T17:32:57.953766: step 4951, loss 0.00477365, acc 1
2016-09-07T17:32:58.646107: step 4952, loss 0.0763243, acc 0.96
2016-09-07T17:32:59.345602: step 4953, loss 0.0242671, acc 0.98
2016-09-07T17:33:00.048728: step 4954, loss 0.019821, acc 1
2016-09-07T17:33:00.774219: step 4955, loss 0.0190581, acc 1
2016-09-07T17:33:01.464746: step 4956, loss 0.00204303, acc 1
2016-09-07T17:33:02.147773: step 4957, loss 0.0323954, acc 0.98
2016-09-07T17:33:02.835070: step 4958, loss 0.0111855, acc 1
2016-09-07T17:33:03.562759: step 4959, loss 0.0128302, acc 1
2016-09-07T17:33:04.224631: step 4960, loss 0.0159214, acc 1
2016-09-07T17:33:04.932955: step 4961, loss 0.1272, acc 0.96
2016-09-07T17:33:05.618756: step 4962, loss 0.00202331, acc 1
2016-09-07T17:33:06.305926: step 4963, loss 0.00865429, acc 1
2016-09-07T17:33:06.982920: step 4964, loss 0.0185582, acc 1
2016-09-07T17:33:07.684441: step 4965, loss 0.024723, acc 0.98
2016-09-07T17:33:08.358924: step 4966, loss 0.0208039, acc 1
2016-09-07T17:33:09.027694: step 4967, loss 0.0315591, acc 1
2016-09-07T17:33:09.729381: step 4968, loss 0.0283747, acc 0.98
2016-09-07T17:33:10.410216: step 4969, loss 0.000359312, acc 1
2016-09-07T17:33:11.082422: step 4970, loss 0.057449, acc 0.98
2016-09-07T17:33:11.785029: step 4971, loss 0.00608286, acc 1
2016-09-07T17:33:12.464552: step 4972, loss 0.0477876, acc 0.96
2016-09-07T17:33:13.151540: step 4973, loss 0.0220194, acc 1
2016-09-07T17:33:13.817813: step 4974, loss 0.024255, acc 1
2016-09-07T17:33:14.533064: step 4975, loss 0.0375219, acc 0.98
2016-09-07T17:33:15.222875: step 4976, loss 0.0752975, acc 0.96
2016-09-07T17:33:15.902153: step 4977, loss 0.0418748, acc 0.98
2016-09-07T17:33:16.585015: step 4978, loss 0.0228573, acc 1
2016-09-07T17:33:17.274524: step 4979, loss 0.00571203, acc 1
2016-09-07T17:33:17.948655: step 4980, loss 0.0466023, acc 0.98
2016-09-07T17:33:18.640153: step 4981, loss 0.0411617, acc 0.98
2016-09-07T17:33:19.343642: step 4982, loss 0.0419033, acc 0.98
2016-09-07T17:33:20.019511: step 4983, loss 0.0234912, acc 1
2016-09-07T17:33:20.720282: step 4984, loss 0.0119096, acc 1
2016-09-07T17:33:21.426853: step 4985, loss 0.0166744, acc 0.98
2016-09-07T17:33:22.104231: step 4986, loss 0.00604271, acc 1
2016-09-07T17:33:22.804498: step 4987, loss 0.0164698, acc 0.98
2016-09-07T17:33:23.495608: step 4988, loss 0.00073869, acc 1
2016-09-07T17:33:24.203218: step 4989, loss 0.0123369, acc 1
2016-09-07T17:33:24.916516: step 4990, loss 0.0549466, acc 0.98
2016-09-07T17:33:25.589797: step 4991, loss 0.0234671, acc 1
2016-09-07T17:33:26.275311: step 4992, loss 0.0109463, acc 1
2016-09-07T17:33:26.967849: step 4993, loss 0.0370393, acc 0.98
2016-09-07T17:33:27.684685: step 4994, loss 0.019057, acc 0.98
2016-09-07T17:33:28.337441: step 4995, loss 0.0244333, acc 0.98
2016-09-07T17:33:29.037236: step 4996, loss 0.013395, acc 1
2016-09-07T17:33:29.721679: step 4997, loss 0.0251952, acc 0.98
2016-09-07T17:33:30.388071: step 4998, loss 0.0669994, acc 0.96
2016-09-07T17:33:31.057486: step 4999, loss 0.0382847, acc 0.98
2016-09-07T17:33:31.738138: step 5000, loss 0.00197044, acc 1

Evaluation:
2016-09-07T17:33:34.706444: step 5000, loss 2.27464, acc 0.751

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-5000

2016-09-07T17:33:36.347249: step 5001, loss 0.0247717, acc 0.98
2016-09-07T17:33:37.044955: step 5002, loss 0.0519237, acc 0.94
2016-09-07T17:33:37.733485: step 5003, loss 0.0308444, acc 0.98
2016-09-07T17:33:38.418811: step 5004, loss 0.0241116, acc 0.98
2016-09-07T17:33:39.079353: step 5005, loss 0.0140006, acc 1
2016-09-07T17:33:39.788990: step 5006, loss 0.0003806, acc 1
2016-09-07T17:33:40.467324: step 5007, loss 0.00259319, acc 1
2016-09-07T17:33:41.156298: step 5008, loss 0.0674568, acc 0.98
2016-09-07T17:33:41.844238: step 5009, loss 0.00570043, acc 1
2016-09-07T17:33:42.522735: step 5010, loss 0.0131412, acc 1
2016-09-07T17:33:43.214552: step 5011, loss 0.0202207, acc 1
2016-09-07T17:33:43.873354: step 5012, loss 0.0680836, acc 0.96
2016-09-07T17:33:44.580852: step 5013, loss 0.0310355, acc 0.98
2016-09-07T17:33:45.253147: step 5014, loss 0.0728058, acc 0.98
2016-09-07T17:33:45.955764: step 5015, loss 0.0109191, acc 1
2016-09-07T17:33:46.649567: step 5016, loss 0.0578458, acc 0.96
2016-09-07T17:33:47.338135: step 5017, loss 0.0834136, acc 0.98
2016-09-07T17:33:48.031337: step 5018, loss 0.0400019, acc 0.96
2016-09-07T17:33:48.682685: step 5019, loss 0.0470436, acc 0.98
2016-09-07T17:33:49.384910: step 5020, loss 0.0501384, acc 0.98
2016-09-07T17:33:50.070412: step 5021, loss 0.0294193, acc 0.98
2016-09-07T17:33:50.748157: step 5022, loss 0.108953, acc 0.96
2016-09-07T17:33:51.435468: step 5023, loss 0.0022303, acc 1
2016-09-07T17:33:52.148589: step 5024, loss 0.0188598, acc 0.98
2016-09-07T17:33:52.840508: step 5025, loss 0.021545, acc 1
2016-09-07T17:33:53.537994: step 5026, loss 0.0270671, acc 1
2016-09-07T17:33:54.242373: step 5027, loss 0.00722585, acc 1
2016-09-07T17:33:54.930290: step 5028, loss 0.000390418, acc 1
2016-09-07T17:33:55.636461: step 5029, loss 0.0600663, acc 0.98
2016-09-07T17:33:56.338062: step 5030, loss 0.0169953, acc 1
2016-09-07T17:33:57.023589: step 5031, loss 0.00186405, acc 1
2016-09-07T17:33:57.713722: step 5032, loss 0.0193843, acc 1
2016-09-07T17:33:58.373459: step 5033, loss 0.0180635, acc 0.98
2016-09-07T17:33:59.071903: step 5034, loss 0.0209291, acc 1
2016-09-07T17:33:59.759759: step 5035, loss 0.0245805, acc 0.98
2016-09-07T17:34:00.474907: step 5036, loss 0.0320529, acc 0.98
2016-09-07T17:34:01.151930: step 5037, loss 0.0747711, acc 0.94
2016-09-07T17:34:01.840744: step 5038, loss 0.0218984, acc 0.98
2016-09-07T17:34:02.555524: step 5039, loss 0.032886, acc 0.98
2016-09-07T17:34:03.218576: step 5040, loss 0.0216778, acc 1
2016-09-07T17:34:03.907492: step 5041, loss 0.0299643, acc 0.98
2016-09-07T17:34:04.617501: step 5042, loss 0.192149, acc 0.96
2016-09-07T17:34:05.301629: step 5043, loss 0.0392287, acc 0.98
2016-09-07T17:34:05.670690: step 5044, loss 0.000403177, acc 1
2016-09-07T17:34:06.345731: step 5045, loss 0.028849, acc 0.98
2016-09-07T17:34:07.050402: step 5046, loss 0.0179208, acc 1
2016-09-07T17:34:07.737765: step 5047, loss 0.0899997, acc 0.96
2016-09-07T17:34:08.415097: step 5048, loss 0.0324989, acc 1
2016-09-07T17:34:09.092249: step 5049, loss 0.00266712, acc 1
2016-09-07T17:34:09.820595: step 5050, loss 0.103482, acc 0.94
2016-09-07T17:34:10.504794: step 5051, loss 0.0134841, acc 1
2016-09-07T17:34:11.194751: step 5052, loss 0.0170366, acc 0.98
2016-09-07T17:34:11.875349: step 5053, loss 0.0154399, acc 1
2016-09-07T17:34:12.555969: step 5054, loss 0.0369355, acc 0.98
2016-09-07T17:34:13.254272: step 5055, loss 0.0415586, acc 0.98
2016-09-07T17:34:13.915264: step 5056, loss 0.0222959, acc 0.98
2016-09-07T17:34:14.612831: step 5057, loss 0.0199429, acc 1
2016-09-07T17:34:15.274637: step 5058, loss 0.0216685, acc 0.98
2016-09-07T17:34:15.954445: step 5059, loss 0.0113826, acc 1
2016-09-07T17:34:16.628175: step 5060, loss 0.0256843, acc 0.98
2016-09-07T17:34:17.299110: step 5061, loss 0.0189262, acc 0.98
2016-09-07T17:34:17.979011: step 5062, loss 0.0182725, acc 0.98
2016-09-07T17:34:18.656157: step 5063, loss 0.0182034, acc 1
2016-09-07T17:34:19.324071: step 5064, loss 0.00246588, acc 1
2016-09-07T17:34:19.988217: step 5065, loss 0.0273435, acc 0.98
2016-09-07T17:34:20.692935: step 5066, loss 0.0840838, acc 0.96
2016-09-07T17:34:21.385062: step 5067, loss 0.00867582, acc 1
2016-09-07T17:34:22.074781: step 5068, loss 0.0112308, acc 1
2016-09-07T17:34:22.760871: step 5069, loss 0.00734706, acc 1
2016-09-07T17:34:23.437548: step 5070, loss 0.000488862, acc 1
2016-09-07T17:34:24.130566: step 5071, loss 0.0315521, acc 0.98
2016-09-07T17:34:24.792135: step 5072, loss 0.00013768, acc 1
2016-09-07T17:34:25.475709: step 5073, loss 0.0222253, acc 1
2016-09-07T17:34:26.136245: step 5074, loss 0.0449353, acc 0.96
2016-09-07T17:34:26.813393: step 5075, loss 0.000752452, acc 1
2016-09-07T17:34:27.496864: step 5076, loss 0.0815095, acc 0.94
2016-09-07T17:34:28.198772: step 5077, loss 0.0373733, acc 0.96
2016-09-07T17:34:28.897021: step 5078, loss 0.0794851, acc 0.98
2016-09-07T17:34:29.568636: step 5079, loss 0.0100219, acc 1
2016-09-07T17:34:30.263378: step 5080, loss 0.0281402, acc 0.98
2016-09-07T17:34:30.953910: step 5081, loss 0.00166911, acc 1
2016-09-07T17:34:31.660200: step 5082, loss 0.00556547, acc 1
2016-09-07T17:34:32.362498: step 5083, loss 0.00800757, acc 1
2016-09-07T17:34:33.065203: step 5084, loss 0.000368653, acc 1
2016-09-07T17:34:33.756766: step 5085, loss 0.0174461, acc 0.98
2016-09-07T17:34:34.434483: step 5086, loss 0.0513184, acc 0.98
2016-09-07T17:34:35.156649: step 5087, loss 0.0179708, acc 1
2016-09-07T17:34:35.852436: step 5088, loss 0.00414064, acc 1
2016-09-07T17:34:36.522104: step 5089, loss 0.0303424, acc 1
2016-09-07T17:34:37.194712: step 5090, loss 0.00526907, acc 1
2016-09-07T17:34:37.862820: step 5091, loss 0.0170996, acc 0.98
2016-09-07T17:34:38.556664: step 5092, loss 0.068715, acc 0.96
2016-09-07T17:34:39.216837: step 5093, loss 0.0214787, acc 0.98
2016-09-07T17:34:39.903810: step 5094, loss 0.0172691, acc 1
2016-09-07T17:34:40.559707: step 5095, loss 0.015617, acc 1
2016-09-07T17:34:41.260865: step 5096, loss 0.195698, acc 0.94
2016-09-07T17:34:41.942590: step 5097, loss 0.00925131, acc 1
2016-09-07T17:34:42.632015: step 5098, loss 0.182422, acc 0.98
2016-09-07T17:34:43.330443: step 5099, loss 0.015807, acc 1
2016-09-07T17:34:43.985022: step 5100, loss 0.0128059, acc 1

Evaluation:
2016-09-07T17:34:46.960688: step 5100, loss 2.56256, acc 0.738

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-5100

2016-09-07T17:34:48.594238: step 5101, loss 0.0178548, acc 1
2016-09-07T17:34:49.304793: step 5102, loss 0.0835107, acc 0.96
2016-09-07T17:34:50.003133: step 5103, loss 0.0204797, acc 0.98
2016-09-07T17:34:50.732182: step 5104, loss 0.0328907, acc 0.98
2016-09-07T17:34:51.407763: step 5105, loss 0.0219725, acc 0.98
2016-09-07T17:34:52.085081: step 5106, loss 0.00889583, acc 1
2016-09-07T17:34:52.761807: step 5107, loss 0.098708, acc 0.98
2016-09-07T17:34:53.456849: step 5108, loss 0.0072264, acc 1
2016-09-07T17:34:54.156797: step 5109, loss 0.141015, acc 0.94
2016-09-07T17:34:54.853502: step 5110, loss 0.0265347, acc 0.98
2016-09-07T17:34:55.551671: step 5111, loss 0.0241346, acc 0.98
2016-09-07T17:34:56.245700: step 5112, loss 0.00630203, acc 1
2016-09-07T17:34:56.924178: step 5113, loss 0.0360646, acc 0.98
2016-09-07T17:34:57.610700: step 5114, loss 0.00690133, acc 1
2016-09-07T17:34:58.281079: step 5115, loss 0.0313237, acc 0.98
2016-09-07T17:34:58.958709: step 5116, loss 0.0109036, acc 1
2016-09-07T17:34:59.638421: step 5117, loss 0.0125389, acc 1
2016-09-07T17:35:00.390432: step 5118, loss 0.00145809, acc 1
2016-09-07T17:35:01.077268: step 5119, loss 0.0134841, acc 1
2016-09-07T17:35:01.771874: step 5120, loss 0.0464236, acc 0.98
2016-09-07T17:35:02.459641: step 5121, loss 0.0703326, acc 0.96
2016-09-07T17:35:03.145096: step 5122, loss 0.0088574, acc 1
2016-09-07T17:35:03.862379: step 5123, loss 0.00707713, acc 1
2016-09-07T17:35:04.537970: step 5124, loss 0.0231678, acc 1
2016-09-07T17:35:05.260625: step 5125, loss 0.0660973, acc 0.98
2016-09-07T17:35:05.943348: step 5126, loss 0.00440882, acc 1
2016-09-07T17:35:06.635704: step 5127, loss 0.037545, acc 1
2016-09-07T17:35:07.324986: step 5128, loss 0.0669344, acc 0.98
2016-09-07T17:35:08.023635: step 5129, loss 0.0266845, acc 0.98
2016-09-07T17:35:08.737116: step 5130, loss 0.0312563, acc 1
2016-09-07T17:35:09.410809: step 5131, loss 0.0118246, acc 1
2016-09-07T17:35:10.073422: step 5132, loss 0.0713396, acc 0.96
2016-09-07T17:35:10.772809: step 5133, loss 0.0381658, acc 0.98
2016-09-07T17:35:11.454244: step 5134, loss 0.0155906, acc 1
2016-09-07T17:35:12.125781: step 5135, loss 0.038895, acc 0.98
2016-09-07T17:35:12.823131: step 5136, loss 0.0361459, acc 0.98
2016-09-07T17:35:13.543842: step 5137, loss 0.0388019, acc 0.98
2016-09-07T17:35:14.234064: step 5138, loss 0.028053, acc 0.98
2016-09-07T17:35:14.911016: step 5139, loss 0.1725, acc 0.96
2016-09-07T17:35:15.591503: step 5140, loss 0.00167834, acc 1
2016-09-07T17:35:16.264948: step 5141, loss 0.0452949, acc 0.98
2016-09-07T17:35:16.946661: step 5142, loss 0.0558031, acc 1
2016-09-07T17:35:17.629854: step 5143, loss 0.00330393, acc 1
2016-09-07T17:35:18.306048: step 5144, loss 0.105975, acc 0.98
2016-09-07T17:35:18.975991: step 5145, loss 0.0441841, acc 0.98
2016-09-07T17:35:19.646515: step 5146, loss 0.0590179, acc 0.98
2016-09-07T17:35:20.314444: step 5147, loss 0.0118484, acc 1
2016-09-07T17:35:21.000532: step 5148, loss 0.0147862, acc 1
2016-09-07T17:35:21.717736: step 5149, loss 0.00742668, acc 1
2016-09-07T17:35:22.424339: step 5150, loss 0.0474535, acc 0.96
2016-09-07T17:35:23.134735: step 5151, loss 0.0162398, acc 1
2016-09-07T17:35:23.807177: step 5152, loss 0.00958471, acc 1
2016-09-07T17:35:24.512917: step 5153, loss 0.0876037, acc 0.98
2016-09-07T17:35:25.217977: step 5154, loss 0.0335324, acc 0.98
2016-09-07T17:35:25.888195: step 5155, loss 0.0958852, acc 0.94
2016-09-07T17:35:26.586752: step 5156, loss 0.0132789, acc 1
2016-09-07T17:35:27.265722: step 5157, loss 0.00568053, acc 1
2016-09-07T17:35:27.979474: step 5158, loss 0.0492944, acc 0.96
2016-09-07T17:35:28.656364: step 5159, loss 0.0345582, acc 0.96
2016-09-07T17:35:29.351967: step 5160, loss 0.0115787, acc 1
2016-09-07T17:35:30.041947: step 5161, loss 0.0242415, acc 1
2016-09-07T17:35:30.726420: step 5162, loss 0.0145172, acc 1
2016-09-07T17:35:31.414483: step 5163, loss 0.0176594, acc 0.98
2016-09-07T17:35:32.079196: step 5164, loss 0.0511559, acc 0.98
2016-09-07T17:35:32.782467: step 5165, loss 0.0127871, acc 1
2016-09-07T17:35:33.470424: step 5166, loss 0.0534056, acc 0.98
2016-09-07T17:35:34.155281: step 5167, loss 0.0243569, acc 0.98
2016-09-07T17:35:34.846084: step 5168, loss 0.0206737, acc 0.98
2016-09-07T17:35:35.547517: step 5169, loss 0.0708817, acc 0.96
2016-09-07T17:35:36.236712: step 5170, loss 0.00576478, acc 1
2016-09-07T17:35:36.894511: step 5171, loss 0.0592704, acc 0.96
2016-09-07T17:35:37.597960: step 5172, loss 0.0247255, acc 1
2016-09-07T17:35:38.263570: step 5173, loss 0.00171622, acc 1
2016-09-07T17:35:38.961232: step 5174, loss 0.0152566, acc 1
2016-09-07T17:35:39.639848: step 5175, loss 0.0563607, acc 0.98
2016-09-07T17:35:40.337487: step 5176, loss 0.024367, acc 0.98
2016-09-07T17:35:41.019922: step 5177, loss 0.00756378, acc 1
2016-09-07T17:35:41.698614: step 5178, loss 0.0396691, acc 0.98
2016-09-07T17:35:42.413820: step 5179, loss 0.0481103, acc 1
2016-09-07T17:35:43.090277: step 5180, loss 0.0120696, acc 1
2016-09-07T17:35:43.772507: step 5181, loss 0.00220186, acc 1
2016-09-07T17:35:44.449559: step 5182, loss 0.00130519, acc 1
2016-09-07T17:35:45.138389: step 5183, loss 0.00745726, acc 1
2016-09-07T17:35:45.817787: step 5184, loss 0.0450692, acc 0.96
2016-09-07T17:35:46.500911: step 5185, loss 0.0192782, acc 0.98
2016-09-07T17:35:47.195177: step 5186, loss 0.013964, acc 1
2016-09-07T17:35:47.871794: step 5187, loss 0.0454407, acc 0.98
2016-09-07T17:35:48.597853: step 5188, loss 0.0253741, acc 1
2016-09-07T17:35:49.293317: step 5189, loss 0.036484, acc 0.98
2016-09-07T17:35:49.991388: step 5190, loss 0.0404625, acc 0.98
2016-09-07T17:35:50.677796: step 5191, loss 0.00672434, acc 1
2016-09-07T17:35:51.355959: step 5192, loss 0.0672521, acc 0.98
2016-09-07T17:35:52.062353: step 5193, loss 0.0279382, acc 0.98
2016-09-07T17:35:52.770470: step 5194, loss 0.00630783, acc 1
2016-09-07T17:35:53.456846: step 5195, loss 0.0703424, acc 0.98
2016-09-07T17:35:54.144723: step 5196, loss 0.0335509, acc 0.98
2016-09-07T17:35:54.832129: step 5197, loss 0.0441823, acc 0.98
2016-09-07T17:35:55.555493: step 5198, loss 0.0404608, acc 0.98
2016-09-07T17:35:56.213877: step 5199, loss 0.0621829, acc 0.96
2016-09-07T17:35:56.904467: step 5200, loss 0.0128209, acc 1

Evaluation:
2016-09-07T17:35:59.846473: step 5200, loss 2.46636, acc 0.731

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-5200

2016-09-07T17:36:01.630632: step 5201, loss 0.00884327, acc 1
2016-09-07T17:36:02.297292: step 5202, loss 0.066941, acc 0.96
2016-09-07T17:36:02.980135: step 5203, loss 0.0451934, acc 0.98
2016-09-07T17:36:03.660999: step 5204, loss 0.000836794, acc 1
2016-09-07T17:36:04.336508: step 5205, loss 0.0701159, acc 0.94
2016-09-07T17:36:05.046468: step 5206, loss 0.01222, acc 1
2016-09-07T17:36:05.734696: step 5207, loss 0.0073001, acc 1
2016-09-07T17:36:06.430917: step 5208, loss 0.0102526, acc 1
2016-09-07T17:36:07.113847: step 5209, loss 0.0302882, acc 0.98
2016-09-07T17:36:07.807194: step 5210, loss 0.00333008, acc 1
2016-09-07T17:36:08.502133: step 5211, loss 0.0802668, acc 0.94
2016-09-07T17:36:09.204083: step 5212, loss 0.0463695, acc 0.96
2016-09-07T17:36:09.901986: step 5213, loss 0.0106798, acc 1
2016-09-07T17:36:10.565653: step 5214, loss 0.0165584, acc 1
2016-09-07T17:36:11.253691: step 5215, loss 0.0164978, acc 1
2016-09-07T17:36:11.940188: step 5216, loss 0.00948131, acc 1
2016-09-07T17:36:12.605747: step 5217, loss 0.00049178, acc 1
2016-09-07T17:36:13.284750: step 5218, loss 0.0024038, acc 1
2016-09-07T17:36:13.967568: step 5219, loss 0.00155615, acc 1
2016-09-07T17:36:14.656388: step 5220, loss 0.0011606, acc 1
2016-09-07T17:36:15.343212: step 5221, loss 0.00856379, acc 1
2016-09-07T17:36:16.039607: step 5222, loss 0.0108393, acc 1
2016-09-07T17:36:16.701435: step 5223, loss 0.0133224, acc 1
2016-09-07T17:36:17.396577: step 5224, loss 0.00911886, acc 1
2016-09-07T17:36:18.079198: step 5225, loss 0.0517842, acc 0.96
2016-09-07T17:36:18.771027: step 5226, loss 0.0376542, acc 0.98
2016-09-07T17:36:19.451880: step 5227, loss 0.0256396, acc 1
2016-09-07T17:36:20.121872: step 5228, loss 0.00127106, acc 1
2016-09-07T17:36:20.812777: step 5229, loss 0.0198127, acc 1
2016-09-07T17:36:21.475021: step 5230, loss 0.0320557, acc 1
2016-09-07T17:36:22.170766: step 5231, loss 0.0464652, acc 0.98
2016-09-07T17:36:22.853360: step 5232, loss 0.0595838, acc 0.98
2016-09-07T17:36:23.528175: step 5233, loss 0.0131533, acc 1
2016-09-07T17:36:24.203155: step 5234, loss 0.0557851, acc 0.96
2016-09-07T17:36:24.897617: step 5235, loss 0.0956751, acc 0.94
2016-09-07T17:36:25.593646: step 5236, loss 0.0190206, acc 0.98
2016-09-07T17:36:26.262445: step 5237, loss 0.0836439, acc 0.94
2016-09-07T17:36:26.628560: step 5238, loss 0.00170902, acc 1
2016-09-07T17:36:27.326187: step 5239, loss 0.00863441, acc 1
2016-09-07T17:36:28.040160: step 5240, loss 0.0026808, acc 1
2016-09-07T17:36:28.728131: step 5241, loss 0.0738868, acc 0.96
2016-09-07T17:36:29.414997: step 5242, loss 0.0255642, acc 0.98
2016-09-07T17:36:30.100291: step 5243, loss 0.123977, acc 0.94
2016-09-07T17:36:30.784257: step 5244, loss 0.0537361, acc 0.96
2016-09-07T17:36:31.466700: step 5245, loss 0.0237934, acc 0.98
2016-09-07T17:36:32.152059: step 5246, loss 0.0169961, acc 1
2016-09-07T17:36:32.855024: step 5247, loss 0.151584, acc 0.96
2016-09-07T17:36:33.552613: step 5248, loss 0.00194399, acc 1
2016-09-07T17:36:34.242344: step 5249, loss 0.0194148, acc 0.98
2016-09-07T17:36:34.928300: step 5250, loss 0.111999, acc 0.98
2016-09-07T17:36:35.598718: step 5251, loss 0.0370284, acc 1
2016-09-07T17:36:36.277353: step 5252, loss 0.0317889, acc 0.98
2016-09-07T17:36:36.944345: step 5253, loss 0.0186664, acc 0.98
2016-09-07T17:36:37.666127: step 5254, loss 0.0107845, acc 1
2016-09-07T17:36:38.361390: step 5255, loss 0.0362391, acc 0.98
2016-09-07T17:36:39.067327: step 5256, loss 0.0086385, acc 1
2016-09-07T17:36:39.745873: step 5257, loss 0.0472477, acc 0.98
2016-09-07T17:36:40.435113: step 5258, loss 0.0647986, acc 0.98
2016-09-07T17:36:41.113730: step 5259, loss 0.0359239, acc 0.96
2016-09-07T17:36:41.798146: step 5260, loss 0.0414094, acc 0.98
2016-09-07T17:36:42.497198: step 5261, loss 0.00467442, acc 1
2016-09-07T17:36:43.166977: step 5262, loss 0.00657754, acc 1
2016-09-07T17:36:43.848804: step 5263, loss 0.0188291, acc 0.98
2016-09-07T17:36:44.548895: step 5264, loss 0.0143378, acc 1
2016-09-07T17:36:45.230314: step 5265, loss 0.0294537, acc 0.98
2016-09-07T17:36:45.925562: step 5266, loss 0.0402125, acc 0.98
2016-09-07T17:36:46.608638: step 5267, loss 0.0207148, acc 0.98
2016-09-07T17:36:47.315199: step 5268, loss 0.0810022, acc 0.96
2016-09-07T17:36:47.994943: step 5269, loss 0.00266841, acc 1
2016-09-07T17:36:48.666283: step 5270, loss 0.0887132, acc 0.98
2016-09-07T17:36:49.351970: step 5271, loss 0.0515271, acc 0.98
2016-09-07T17:36:50.031872: step 5272, loss 0.0184689, acc 1
2016-09-07T17:36:50.734971: step 5273, loss 0.0449037, acc 0.96
2016-09-07T17:36:51.419152: step 5274, loss 0.0137387, acc 1
2016-09-07T17:36:52.133814: step 5275, loss 0.0505188, acc 0.98
2016-09-07T17:36:52.825998: step 5276, loss 0.0294815, acc 0.98
2016-09-07T17:36:53.562097: step 5277, loss 0.00313723, acc 1
2016-09-07T17:36:54.259546: step 5278, loss 0.0270132, acc 0.98
2016-09-07T17:36:54.956301: step 5279, loss 0.0219974, acc 1
2016-09-07T17:36:55.680461: step 5280, loss 0.0383634, acc 0.98
2016-09-07T17:36:56.341547: step 5281, loss 0.0113985, acc 1
2016-09-07T17:36:57.054914: step 5282, loss 0.00735255, acc 1
2016-09-07T17:36:57.744632: step 5283, loss 0.110689, acc 0.98
2016-09-07T17:36:58.434771: step 5284, loss 0.153348, acc 0.96
2016-09-07T17:36:59.124184: step 5285, loss 0.0701772, acc 0.94
2016-09-07T17:36:59.818796: step 5286, loss 0.0219953, acc 1
2016-09-07T17:37:00.569065: step 5287, loss 0.027488, acc 0.98
2016-09-07T17:37:01.245934: step 5288, loss 0.0386896, acc 1
2016-09-07T17:37:01.936311: step 5289, loss 0.0254769, acc 0.98
2016-09-07T17:37:02.633106: step 5290, loss 0.0347967, acc 0.98
2016-09-07T17:37:03.319372: step 5291, loss 0.0357756, acc 0.98
2016-09-07T17:37:04.005752: step 5292, loss 0.0488787, acc 0.96
2016-09-07T17:37:04.685424: step 5293, loss 0.00777406, acc 1
2016-09-07T17:37:05.395100: step 5294, loss 0.025624, acc 0.98
2016-09-07T17:37:06.064265: step 5295, loss 0.0433256, acc 0.98
2016-09-07T17:37:06.760671: step 5296, loss 0.0061664, acc 1
2016-09-07T17:37:07.437983: step 5297, loss 0.0426866, acc 0.98
2016-09-07T17:37:08.134939: step 5298, loss 0.0449962, acc 0.98
2016-09-07T17:37:08.816722: step 5299, loss 0.0239187, acc 1
2016-09-07T17:37:09.469388: step 5300, loss 0.0304677, acc 0.98

Evaluation:
2016-09-07T17:37:12.480475: step 5300, loss 1.83324, acc 0.741

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-5300

2016-09-07T17:37:14.247532: step 5301, loss 0.0762438, acc 0.96
2016-09-07T17:37:14.941736: step 5302, loss 0.0252135, acc 0.98
2016-09-07T17:37:15.623893: step 5303, loss 0.0617577, acc 0.98
2016-09-07T17:37:16.306826: step 5304, loss 0.0191837, acc 0.98
2016-09-07T17:37:16.990589: step 5305, loss 0.0508837, acc 0.96
2016-09-07T17:37:17.688145: step 5306, loss 0.0111403, acc 1
2016-09-07T17:37:18.370409: step 5307, loss 0.041999, acc 0.98
2016-09-07T17:37:19.064418: step 5308, loss 0.0294455, acc 0.98
2016-09-07T17:37:19.787715: step 5309, loss 0.026366, acc 0.98
2016-09-07T17:37:20.461227: step 5310, loss 0.0112683, acc 1
2016-09-07T17:37:21.136856: step 5311, loss 0.00102163, acc 1
2016-09-07T17:37:21.822825: step 5312, loss 0.00501693, acc 1
2016-09-07T17:37:22.523097: step 5313, loss 0.022992, acc 0.98
2016-09-07T17:37:23.198825: step 5314, loss 0.0576603, acc 0.98
2016-09-07T17:37:23.888087: step 5315, loss 0.0502023, acc 0.98
2016-09-07T17:37:24.587974: step 5316, loss 0.00998295, acc 1
2016-09-07T17:37:25.277293: step 5317, loss 0.0946098, acc 0.98
2016-09-07T17:37:25.938988: step 5318, loss 0.0496249, acc 0.96
2016-09-07T17:37:26.626200: step 5319, loss 0.120164, acc 0.96
2016-09-07T17:37:27.321008: step 5320, loss 0.162922, acc 0.94
2016-09-07T17:37:28.020907: step 5321, loss 0.0350091, acc 0.98
2016-09-07T17:37:28.724136: step 5322, loss 0.00301443, acc 1
2016-09-07T17:37:29.421399: step 5323, loss 0.01359, acc 1
2016-09-07T17:37:30.116289: step 5324, loss 0.0035449, acc 1
2016-09-07T17:37:30.819194: step 5325, loss 0.00484094, acc 1
2016-09-07T17:37:31.523403: step 5326, loss 0.0137553, acc 1
2016-09-07T17:37:32.237954: step 5327, loss 0.0104629, acc 1
2016-09-07T17:37:32.955311: step 5328, loss 0.0561131, acc 0.98
2016-09-07T17:37:33.630004: step 5329, loss 0.0220622, acc 0.98
2016-09-07T17:37:34.339399: step 5330, loss 0.0247146, acc 1
2016-09-07T17:37:35.040405: step 5331, loss 0.0283972, acc 0.98
2016-09-07T17:37:35.733094: step 5332, loss 0.0910432, acc 0.98
2016-09-07T17:37:36.422795: step 5333, loss 0.0603613, acc 0.96
2016-09-07T17:37:37.094964: step 5334, loss 0.0506542, acc 0.96
2016-09-07T17:37:37.787835: step 5335, loss 0.0233125, acc 0.98
2016-09-07T17:37:38.447492: step 5336, loss 0.059587, acc 0.94
2016-09-07T17:37:39.136114: step 5337, loss 0.0343265, acc 0.96
2016-09-07T17:37:39.817188: step 5338, loss 0.0366416, acc 0.98
2016-09-07T17:37:40.541779: step 5339, loss 0.0147742, acc 1
2016-09-07T17:37:41.269343: step 5340, loss 0.0283504, acc 1
2016-09-07T17:37:41.930241: step 5341, loss 0.0525232, acc 0.98
2016-09-07T17:37:42.631929: step 5342, loss 0.0115857, acc 1
2016-09-07T17:37:43.314204: step 5343, loss 0.0403957, acc 0.98
2016-09-07T17:37:44.008448: step 5344, loss 0.00667786, acc 1
2016-09-07T17:37:44.699838: step 5345, loss 0.0308997, acc 0.98
2016-09-07T17:37:45.384936: step 5346, loss 0.0234088, acc 0.98
2016-09-07T17:37:46.083975: step 5347, loss 0.0175232, acc 0.98
2016-09-07T17:37:46.783124: step 5348, loss 0.0556185, acc 0.98
2016-09-07T17:37:47.519195: step 5349, loss 0.0867498, acc 0.96
2016-09-07T17:37:48.208755: step 5350, loss 0.0200434, acc 0.98
2016-09-07T17:37:48.908801: step 5351, loss 0.0251641, acc 0.98
2016-09-07T17:37:49.605641: step 5352, loss 0.0286409, acc 0.98
2016-09-07T17:37:50.287974: step 5353, loss 0.0378602, acc 0.98
2016-09-07T17:37:50.994383: step 5354, loss 0.0438643, acc 0.98
2016-09-07T17:37:51.664926: step 5355, loss 0.0101426, acc 1
2016-09-07T17:37:52.411957: step 5356, loss 0.050455, acc 0.98
2016-09-07T17:37:53.118656: step 5357, loss 0.103962, acc 0.98
2016-09-07T17:37:53.846504: step 5358, loss 0.0163491, acc 0.98
2016-09-07T17:37:54.542396: step 5359, loss 0.00531602, acc 1
2016-09-07T17:37:55.226397: step 5360, loss 0.0126311, acc 1
2016-09-07T17:37:55.943750: step 5361, loss 0.0919546, acc 0.98
2016-09-07T17:37:56.626099: step 5362, loss 0.000504971, acc 1
2016-09-07T17:37:57.304411: step 5363, loss 0.00433819, acc 1
2016-09-07T17:37:58.001917: step 5364, loss 0.00422627, acc 1
2016-09-07T17:37:58.719909: step 5365, loss 0.0847943, acc 0.96
2016-09-07T17:37:59.429653: step 5366, loss 0.000348407, acc 1
2016-09-07T17:38:00.099778: step 5367, loss 0.00201241, acc 1
2016-09-07T17:38:00.833853: step 5368, loss 0.0487424, acc 0.96
2016-09-07T17:38:01.522433: step 5369, loss 0.140902, acc 0.94
2016-09-07T17:38:02.217442: step 5370, loss 0.0166393, acc 0.98
2016-09-07T17:38:02.919411: step 5371, loss 0.00208245, acc 1
2016-09-07T17:38:03.617325: step 5372, loss 0.0479059, acc 0.98
2016-09-07T17:38:04.310993: step 5373, loss 0.0251273, acc 1
2016-09-07T17:38:05.023649: step 5374, loss 0.00471156, acc 1
2016-09-07T17:38:05.708944: step 5375, loss 0.000207175, acc 1
2016-09-07T17:38:06.399097: step 5376, loss 0.00643411, acc 1
2016-09-07T17:38:07.074039: step 5377, loss 6.12874e-05, acc 1
2016-09-07T17:38:07.759821: step 5378, loss 0.0394473, acc 0.98
2016-09-07T17:38:08.411422: step 5379, loss 0.0544125, acc 0.96
2016-09-07T17:38:09.112712: step 5380, loss 0.205916, acc 0.94
2016-09-07T17:38:09.819116: step 5381, loss 0.0168926, acc 0.98
2016-09-07T17:38:10.506491: step 5382, loss 0.0212287, acc 0.98
2016-09-07T17:38:11.176437: step 5383, loss 0.0166967, acc 0.98
2016-09-07T17:38:11.856295: step 5384, loss 0.00357843, acc 1
2016-09-07T17:38:12.526901: step 5385, loss 0.0280199, acc 0.98
2016-09-07T17:38:13.193055: step 5386, loss 0.154303, acc 0.98
2016-09-07T17:38:13.889063: step 5387, loss 0.029725, acc 1
2016-09-07T17:38:14.560977: step 5388, loss 0.149456, acc 0.98
2016-09-07T17:38:15.245291: step 5389, loss 0.0318744, acc 0.98
2016-09-07T17:38:15.947721: step 5390, loss 0.0236743, acc 1
2016-09-07T17:38:16.638976: step 5391, loss 0.0250008, acc 0.98
2016-09-07T17:38:17.338062: step 5392, loss 0.129654, acc 0.96
2016-09-07T17:38:18.004295: step 5393, loss 0.0359485, acc 0.96
2016-09-07T17:38:18.706838: step 5394, loss 0.0234778, acc 0.98
2016-09-07T17:38:19.410738: step 5395, loss 0.00305422, acc 1
2016-09-07T17:38:20.094209: step 5396, loss 0.014846, acc 1
2016-09-07T17:38:20.782029: step 5397, loss 0.00254306, acc 1
2016-09-07T17:38:21.463225: step 5398, loss 0.0266744, acc 0.98
2016-09-07T17:38:22.161395: step 5399, loss 0.035038, acc 0.98
2016-09-07T17:38:22.853309: step 5400, loss 0.0148834, acc 1

Evaluation:
2016-09-07T17:38:25.850414: step 5400, loss 1.70083, acc 0.744

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-5400

2016-09-07T17:38:27.463378: step 5401, loss 0.000433613, acc 1
2016-09-07T17:38:28.181151: step 5402, loss 0.0315121, acc 1
2016-09-07T17:38:28.836560: step 5403, loss 0.0171629, acc 1
2016-09-07T17:38:29.537636: step 5404, loss 0.0292008, acc 0.98
2016-09-07T17:38:30.220101: step 5405, loss 0.0300457, acc 0.98
2016-09-07T17:38:30.906508: step 5406, loss 0.0669069, acc 0.96
2016-09-07T17:38:31.598027: step 5407, loss 0.0440145, acc 0.98
2016-09-07T17:38:32.289600: step 5408, loss 0.0804926, acc 0.98
2016-09-07T17:38:32.990529: step 5409, loss 0.0286754, acc 0.98
2016-09-07T17:38:33.691051: step 5410, loss 0.0407452, acc 0.98
2016-09-07T17:38:34.384276: step 5411, loss 0.0559835, acc 0.96
2016-09-07T17:38:35.080390: step 5412, loss 0.000285401, acc 1
2016-09-07T17:38:35.768163: step 5413, loss 0.0947841, acc 0.98
2016-09-07T17:38:36.446565: step 5414, loss 0.00306934, acc 1
2016-09-07T17:38:37.129857: step 5415, loss 0.00966839, acc 1
2016-09-07T17:38:37.846156: step 5416, loss 0.0128176, acc 1
2016-09-07T17:38:38.517074: step 5417, loss 0.0099446, acc 1
2016-09-07T17:38:39.220421: step 5418, loss 0.0268954, acc 1
2016-09-07T17:38:39.895655: step 5419, loss 0.0413834, acc 0.96
2016-09-07T17:38:40.585722: step 5420, loss 0.0415328, acc 0.98
2016-09-07T17:38:41.287652: step 5421, loss 0.00824888, acc 1
2016-09-07T17:38:42.060295: step 5422, loss 0.0193535, acc 0.98
2016-09-07T17:38:42.754648: step 5423, loss 0.0452948, acc 0.98
2016-09-07T17:38:43.419942: step 5424, loss 0.0348785, acc 0.98
2016-09-07T17:38:44.095202: step 5425, loss 0.0266363, acc 1
2016-09-07T17:38:44.781881: step 5426, loss 0.000850509, acc 1
2016-09-07T17:38:45.449800: step 5427, loss 0.00795656, acc 1
2016-09-07T17:38:46.138267: step 5428, loss 0.120663, acc 0.92
2016-09-07T17:38:46.840411: step 5429, loss 0.0394687, acc 0.98
2016-09-07T17:38:47.558294: step 5430, loss 0.0284225, acc 0.98
2016-09-07T17:38:48.241450: step 5431, loss 0.00237439, acc 1
2016-09-07T17:38:48.611947: step 5432, loss 0.0412052, acc 1
2016-09-07T17:38:49.275337: step 5433, loss 0.0345628, acc 0.96
2016-09-07T17:38:49.975635: step 5434, loss 0.0321401, acc 0.98
2016-09-07T17:38:50.641060: step 5435, loss 0.0724912, acc 0.96
2016-09-07T17:38:51.318918: step 5436, loss 0.0147924, acc 1
2016-09-07T17:38:51.996257: step 5437, loss 0.0160323, acc 0.98
2016-09-07T17:38:52.683652: step 5438, loss 0.0224949, acc 1
2016-09-07T17:38:53.353296: step 5439, loss 0.0153534, acc 0.98
2016-09-07T17:38:54.047928: step 5440, loss 0.00874374, acc 1
2016-09-07T17:38:54.757993: step 5441, loss 0.00481597, acc 1
2016-09-07T17:38:55.435697: step 5442, loss 0.0142891, acc 0.98
2016-09-07T17:38:56.116429: step 5443, loss 0.0233135, acc 0.98
2016-09-07T17:38:56.818685: step 5444, loss 0.00599103, acc 1
2016-09-07T17:38:57.508005: step 5445, loss 0.0505174, acc 0.96
2016-09-07T17:38:58.191461: step 5446, loss 0.0672769, acc 0.96
2016-09-07T17:38:58.871867: step 5447, loss 0.0125006, acc 1
2016-09-07T17:38:59.582273: step 5448, loss 0.072414, acc 0.96
2016-09-07T17:39:00.271416: step 5449, loss 0.0253395, acc 0.98
2016-09-07T17:39:00.948625: step 5450, loss 0.033307, acc 0.96
2016-09-07T17:39:01.632237: step 5451, loss 0.0205864, acc 0.98
2016-09-07T17:39:02.314162: step 5452, loss 0.0203442, acc 0.98
2016-09-07T17:39:03.015713: step 5453, loss 0.160076, acc 0.94
2016-09-07T17:39:03.690493: step 5454, loss 0.0195502, acc 1
2016-09-07T17:39:04.378753: step 5455, loss 0.00736915, acc 1
2016-09-07T17:39:05.058775: step 5456, loss 0.000552891, acc 1
2016-09-07T17:39:05.736824: step 5457, loss 0.0135358, acc 1
2016-09-07T17:39:06.407419: step 5458, loss 0.0466151, acc 0.96
2016-09-07T17:39:07.094635: step 5459, loss 0.00165592, acc 1
2016-09-07T17:39:07.769802: step 5460, loss 0.0221524, acc 0.98
2016-09-07T17:39:08.461447: step 5461, loss 0.0563852, acc 0.96
2016-09-07T17:39:09.178376: step 5462, loss 0.0138913, acc 1
2016-09-07T17:39:09.851225: step 5463, loss 0.0355871, acc 0.98
2016-09-07T17:39:10.537910: step 5464, loss 0.0552048, acc 0.98
2016-09-07T17:39:11.226087: step 5465, loss 0.0245295, acc 0.98
2016-09-07T17:39:11.895812: step 5466, loss 0.00935126, acc 1
2016-09-07T17:39:12.634606: step 5467, loss 0.000287957, acc 1
2016-09-07T17:39:13.299556: step 5468, loss 0.0400222, acc 0.98
2016-09-07T17:39:13.997130: step 5469, loss 0.016891, acc 0.98
2016-09-07T17:39:14.678883: step 5470, loss 0.00910137, acc 1
2016-09-07T17:39:15.337739: step 5471, loss 0.0415636, acc 0.98
2016-09-07T17:39:16.017878: step 5472, loss 0.000527376, acc 1
2016-09-07T17:39:16.721885: step 5473, loss 0.0280622, acc 1
2016-09-07T17:39:17.393437: step 5474, loss 0.00123785, acc 1
2016-09-07T17:39:18.069986: step 5475, loss 0.0278067, acc 0.98
2016-09-07T17:39:18.779801: step 5476, loss 0.0295777, acc 0.98
2016-09-07T17:39:19.461055: step 5477, loss 0.0353868, acc 0.98
2016-09-07T17:39:20.169049: step 5478, loss 0.0936883, acc 0.96
2016-09-07T17:39:20.855709: step 5479, loss 0.0335513, acc 0.98
2016-09-07T17:39:21.543456: step 5480, loss 0.0153513, acc 1
2016-09-07T17:39:22.255684: step 5481, loss 0.0153326, acc 1
2016-09-07T17:39:22.929976: step 5482, loss 0.0119737, acc 1
2016-09-07T17:39:23.652917: step 5483, loss 0.0024912, acc 1
2016-09-07T17:39:24.311811: step 5484, loss 0.0381027, acc 0.98
2016-09-07T17:39:25.003398: step 5485, loss 0.00285866, acc 1
2016-09-07T17:39:25.692911: step 5486, loss 0.0721442, acc 0.98
2016-09-07T17:39:26.370681: step 5487, loss 0.000274656, acc 1
2016-09-07T17:39:27.066258: step 5488, loss 0.0243506, acc 0.98
2016-09-07T17:39:27.744061: step 5489, loss 0.0144236, acc 0.98
2016-09-07T17:39:28.443008: step 5490, loss 0.000247372, acc 1
2016-09-07T17:39:29.123435: step 5491, loss 0.0274876, acc 0.98
2016-09-07T17:39:29.819786: step 5492, loss 0.00969218, acc 1
2016-09-07T17:39:30.529186: step 5493, loss 0.0360533, acc 1
2016-09-07T17:39:31.225304: step 5494, loss 0.00563024, acc 1
2016-09-07T17:39:31.916361: step 5495, loss 0.0242794, acc 1
2016-09-07T17:39:32.569979: step 5496, loss 0.0557492, acc 0.98
2016-09-07T17:39:33.275332: step 5497, loss 0.0225029, acc 1
2016-09-07T17:39:33.953827: step 5498, loss 0.0272965, acc 0.98
2016-09-07T17:39:34.630153: step 5499, loss 0.00490398, acc 1
2016-09-07T17:39:35.322785: step 5500, loss 0.0944312, acc 0.96

Evaluation:
2016-09-07T17:39:38.301762: step 5500, loss 2.48023, acc 0.74

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-5500

2016-09-07T17:39:40.125931: step 5501, loss 0.0193118, acc 0.98
2016-09-07T17:39:40.806838: step 5502, loss 0.137715, acc 0.94
2016-09-07T17:39:41.498608: step 5503, loss 0.0893649, acc 0.96
2016-09-07T17:39:42.175186: step 5504, loss 0.0148711, acc 1
2016-09-07T17:39:42.866397: step 5505, loss 0.00136566, acc 1
2016-09-07T17:39:43.533083: step 5506, loss 0.00117007, acc 1
2016-09-07T17:39:44.201536: step 5507, loss 0.0206024, acc 0.98
2016-09-07T17:39:44.900485: step 5508, loss 0.0190525, acc 0.98
2016-09-07T17:39:45.576358: step 5509, loss 0.131734, acc 0.94
2016-09-07T17:39:46.262548: step 5510, loss 0.000126626, acc 1
2016-09-07T17:39:46.952636: step 5511, loss 0.0213341, acc 0.98
2016-09-07T17:39:47.654693: step 5512, loss 0.082138, acc 0.98
2016-09-07T17:39:48.321646: step 5513, loss 0.0454581, acc 0.98
2016-09-07T17:39:49.033706: step 5514, loss 0.0184111, acc 1
2016-09-07T17:39:49.727592: step 5515, loss 0.0601686, acc 0.96
2016-09-07T17:39:50.414674: step 5516, loss 0.00963106, acc 1
2016-09-07T17:39:51.118120: step 5517, loss 0.0134697, acc 1
2016-09-07T17:39:51.787541: step 5518, loss 0.0134646, acc 1
2016-09-07T17:39:52.520227: step 5519, loss 0.00528098, acc 1
2016-09-07T17:39:53.208822: step 5520, loss 0.0461219, acc 0.98
2016-09-07T17:39:53.868498: step 5521, loss 0.00746755, acc 1
2016-09-07T17:39:54.559244: step 5522, loss 0.0180928, acc 1
2016-09-07T17:39:55.230106: step 5523, loss 0.0584486, acc 0.94
2016-09-07T17:39:55.920488: step 5524, loss 0.0416678, acc 0.98
2016-09-07T17:39:56.602166: step 5525, loss 0.0014735, acc 1
2016-09-07T17:39:57.311160: step 5526, loss 0.038264, acc 0.98
2016-09-07T17:39:57.983258: step 5527, loss 0.0300593, acc 0.98
2016-09-07T17:39:58.663769: step 5528, loss 0.033872, acc 0.98
2016-09-07T17:39:59.360794: step 5529, loss 0.000773576, acc 1
2016-09-07T17:40:00.050077: step 5530, loss 0.0762012, acc 0.94
2016-09-07T17:40:00.772176: step 5531, loss 0.0483002, acc 0.98
2016-09-07T17:40:01.430816: step 5532, loss 0.0338539, acc 0.98
2016-09-07T17:40:02.125248: step 5533, loss 0.00309994, acc 1
2016-09-07T17:40:02.797775: step 5534, loss 0.038111, acc 0.98
2016-09-07T17:40:03.492219: step 5535, loss 0.033629, acc 0.98
2016-09-07T17:40:04.174337: step 5536, loss 0.00547347, acc 1
2016-09-07T17:40:04.862556: step 5537, loss 0.000410221, acc 1
2016-09-07T17:40:05.551609: step 5538, loss 0.0254396, acc 0.98
2016-09-07T17:40:06.241197: step 5539, loss 0.00348511, acc 1
2016-09-07T17:40:06.935856: step 5540, loss 0.0133494, acc 1
2016-09-07T17:40:07.612284: step 5541, loss 0.0170164, acc 1
2016-09-07T17:40:08.282429: step 5542, loss 0.0178613, acc 0.98
2016-09-07T17:40:08.957961: step 5543, loss 0.0277034, acc 0.98
2016-09-07T17:40:09.637952: step 5544, loss 0.00983533, acc 1
2016-09-07T17:40:10.313784: step 5545, loss 0.0127546, acc 1
2016-09-07T17:40:11.012146: step 5546, loss 0.0270464, acc 0.98
2016-09-07T17:40:11.701890: step 5547, loss 0.0279658, acc 0.98
2016-09-07T17:40:12.390651: step 5548, loss 0.0184625, acc 1
2016-09-07T17:40:13.092627: step 5549, loss 0.0430432, acc 0.98
2016-09-07T17:40:13.771162: step 5550, loss 0.0311817, acc 0.98
2016-09-07T17:40:14.475190: step 5551, loss 0.00936998, acc 1
2016-09-07T17:40:15.159194: step 5552, loss 0.0409132, acc 1
2016-09-07T17:40:15.837261: step 5553, loss 0.0148156, acc 0.98
2016-09-07T17:40:16.535786: step 5554, loss 0.0116625, acc 1
2016-09-07T17:40:17.214624: step 5555, loss 0.0022042, acc 1
2016-09-07T17:40:17.897494: step 5556, loss 0.00310183, acc 1
2016-09-07T17:40:18.585114: step 5557, loss 0.0343229, acc 0.96
2016-09-07T17:40:19.291999: step 5558, loss 0.00905541, acc 1
2016-09-07T17:40:20.001697: step 5559, loss 0.0776646, acc 0.98
2016-09-07T17:40:20.665255: step 5560, loss 0.016777, acc 0.98
2016-09-07T17:40:21.357818: step 5561, loss 0.0384337, acc 0.98
2016-09-07T17:40:22.038408: step 5562, loss 0.00764374, acc 1
2016-09-07T17:40:22.737543: step 5563, loss 0.0276993, acc 1
2016-09-07T17:40:23.419105: step 5564, loss 0.04681, acc 0.98
2016-09-07T17:40:24.101659: step 5565, loss 5.63275e-05, acc 1
2016-09-07T17:40:24.786494: step 5566, loss 0.00565266, acc 1
2016-09-07T17:40:25.504539: step 5567, loss 0.0480444, acc 0.98
2016-09-07T17:40:26.228718: step 5568, loss 0.119705, acc 0.98
2016-09-07T17:40:26.912940: step 5569, loss 0.0192331, acc 1
2016-09-07T17:40:27.597545: step 5570, loss 1.91981e-05, acc 1
2016-09-07T17:40:28.284613: step 5571, loss 0.000487416, acc 1
2016-09-07T17:40:28.961806: step 5572, loss 0.0502465, acc 0.96
2016-09-07T17:40:29.653400: step 5573, loss 0.00422129, acc 1
2016-09-07T17:40:30.331550: step 5574, loss 0.00241107, acc 1
2016-09-07T17:40:31.029442: step 5575, loss 0.054017, acc 0.98
2016-09-07T17:40:31.730214: step 5576, loss 0.0314413, acc 0.98
2016-09-07T17:40:32.437652: step 5577, loss 0.0131234, acc 1
2016-09-07T17:40:33.128201: step 5578, loss 0.0651938, acc 0.98
2016-09-07T17:40:33.826490: step 5579, loss 0.0156893, acc 0.98
2016-09-07T17:40:34.539014: step 5580, loss 0.0277025, acc 0.98
2016-09-07T17:40:35.248554: step 5581, loss 0.0269315, acc 0.98
2016-09-07T17:40:35.926291: step 5582, loss 0.0160592, acc 1
2016-09-07T17:40:36.637336: step 5583, loss 0.0411479, acc 0.98
2016-09-07T17:40:37.337441: step 5584, loss 0.0186104, acc 0.98
2016-09-07T17:40:38.032980: step 5585, loss 0.0790773, acc 0.96
2016-09-07T17:40:38.698087: step 5586, loss 0.0518294, acc 0.96
2016-09-07T17:40:39.397470: step 5587, loss 0.0301297, acc 0.98
2016-09-07T17:40:40.060395: step 5588, loss 0.0201789, acc 0.98
2016-09-07T17:40:40.757612: step 5589, loss 0.00109656, acc 1
2016-09-07T17:40:41.468380: step 5590, loss 0.0469239, acc 0.96
2016-09-07T17:40:42.157331: step 5591, loss 0.0141754, acc 1
2016-09-07T17:40:42.853863: step 5592, loss 0.0592999, acc 0.98
2016-09-07T17:40:43.519413: step 5593, loss 0.0197912, acc 1
2016-09-07T17:40:44.243863: step 5594, loss 0.0198112, acc 1
2016-09-07T17:40:44.910435: step 5595, loss 0.00335529, acc 1
2016-09-07T17:40:45.604802: step 5596, loss 0.0623284, acc 0.98
2016-09-07T17:40:46.311402: step 5597, loss 0.044992, acc 0.98
2016-09-07T17:40:46.990002: step 5598, loss 0.0131731, acc 1
2016-09-07T17:40:47.675603: step 5599, loss 0.000398512, acc 1
2016-09-07T17:40:48.339529: step 5600, loss 0.0360002, acc 1

Evaluation:
2016-09-07T17:40:51.331672: step 5600, loss 2.47643, acc 0.74

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-5600

2016-09-07T17:40:53.000190: step 5601, loss 0.0287653, acc 1
2016-09-07T17:40:53.670746: step 5602, loss 0.0432606, acc 0.96
2016-09-07T17:40:54.327533: step 5603, loss 0.0277941, acc 0.98
2016-09-07T17:40:55.035920: step 5604, loss 0.0053412, acc 1
2016-09-07T17:40:55.723568: step 5605, loss 0.0329533, acc 0.98
2016-09-07T17:40:56.399051: step 5606, loss 0.0313535, acc 1
2016-09-07T17:40:57.098180: step 5607, loss 0.0328419, acc 0.98
2016-09-07T17:40:57.765939: step 5608, loss 0.0970792, acc 0.98
2016-09-07T17:40:58.449341: step 5609, loss 0.00365315, acc 1
2016-09-07T17:40:59.160530: step 5610, loss 0.0319489, acc 0.98
2016-09-07T17:40:59.864611: step 5611, loss 0.0290027, acc 1
2016-09-07T17:41:00.567223: step 5612, loss 0.0195465, acc 1
2016-09-07T17:41:01.252298: step 5613, loss 0.00864923, acc 1
2016-09-07T17:41:01.947320: step 5614, loss 0.0475306, acc 0.98
2016-09-07T17:41:02.628623: step 5615, loss 0.0303346, acc 0.96
2016-09-07T17:41:03.319990: step 5616, loss 0.0044713, acc 1
2016-09-07T17:41:03.995251: step 5617, loss 0.00517776, acc 1
2016-09-07T17:41:04.708974: step 5618, loss 0.00504225, acc 1
2016-09-07T17:41:05.391444: step 5619, loss 0.0137885, acc 1
2016-09-07T17:41:06.071692: step 5620, loss 0.00018411, acc 1
2016-09-07T17:41:06.753805: step 5621, loss 0.0609743, acc 0.96
2016-09-07T17:41:07.432235: step 5622, loss 0.0471492, acc 0.96
2016-09-07T17:41:08.118327: step 5623, loss 0.0518151, acc 0.96
2016-09-07T17:41:08.775910: step 5624, loss 0.00666593, acc 1
2016-09-07T17:41:09.472199: step 5625, loss 6.02302e-05, acc 1
2016-09-07T17:41:09.844178: step 5626, loss 0.0092042, acc 1
2016-09-07T17:41:10.547898: step 5627, loss 0.072091, acc 0.98
2016-09-07T17:41:11.230851: step 5628, loss 0.00898795, acc 1
2016-09-07T17:41:11.939036: step 5629, loss 0.0272364, acc 0.98
2016-09-07T17:41:12.632737: step 5630, loss 0.038052, acc 0.98
2016-09-07T17:41:13.303102: step 5631, loss 0.0517962, acc 0.98
2016-09-07T17:41:14.002632: step 5632, loss 0.00214318, acc 1
2016-09-07T17:41:14.697440: step 5633, loss 0.0327373, acc 0.98
2016-09-07T17:41:15.406048: step 5634, loss 0.0138761, acc 1
2016-09-07T17:41:16.094206: step 5635, loss 0.00315484, acc 1
2016-09-07T17:41:16.797332: step 5636, loss 0.00163259, acc 1
2016-09-07T17:41:17.479395: step 5637, loss 0.290354, acc 0.96
2016-09-07T17:41:18.165202: step 5638, loss 0.0177703, acc 1
2016-09-07T17:41:18.866961: step 5639, loss 0.0296359, acc 1
2016-09-07T17:41:19.543646: step 5640, loss 0.0648798, acc 0.96
2016-09-07T17:41:20.243378: step 5641, loss 0.00226242, acc 1
2016-09-07T17:41:20.921580: step 5642, loss 0.00324869, acc 1
2016-09-07T17:41:21.631149: step 5643, loss 0.0290999, acc 0.98
2016-09-07T17:41:22.340764: step 5644, loss 0.00409634, acc 1
2016-09-07T17:41:23.047452: step 5645, loss 0.0561034, acc 0.96
2016-09-07T17:41:23.731297: step 5646, loss 0.000314031, acc 1
2016-09-07T17:41:24.405295: step 5647, loss 0.164762, acc 0.96
2016-09-07T17:41:25.093920: step 5648, loss 0.000693326, acc 1
2016-09-07T17:41:25.766831: step 5649, loss 0.0289525, acc 0.98
2016-09-07T17:41:26.469116: step 5650, loss 0.013873, acc 1
2016-09-07T17:41:27.148635: step 5651, loss 0.0225345, acc 0.98
2016-09-07T17:41:27.825916: step 5652, loss 0.00126694, acc 1
2016-09-07T17:41:28.498048: step 5653, loss 0.0161073, acc 1
2016-09-07T17:41:29.173562: step 5654, loss 0.0143796, acc 1
2016-09-07T17:41:29.868255: step 5655, loss 0.0453372, acc 0.96
2016-09-07T17:41:30.546550: step 5656, loss 0.00426426, acc 1
2016-09-07T17:41:31.273091: step 5657, loss 0.0143086, acc 1
2016-09-07T17:41:31.987025: step 5658, loss 0.0766002, acc 0.98
2016-09-07T17:41:32.682598: step 5659, loss 0.0178662, acc 1
2016-09-07T17:41:33.374932: step 5660, loss 0.0789615, acc 0.94
2016-09-07T17:41:34.044495: step 5661, loss 0.00855475, acc 1
2016-09-07T17:41:34.724980: step 5662, loss 0.0285709, acc 0.98
2016-09-07T17:41:35.401015: step 5663, loss 0.0280671, acc 0.98
2016-09-07T17:41:36.074742: step 5664, loss 0.0246652, acc 0.98
2016-09-07T17:41:36.767690: step 5665, loss 0.0234109, acc 0.98
2016-09-07T17:41:37.459481: step 5666, loss 0.00816628, acc 1
2016-09-07T17:41:38.170936: step 5667, loss 0.00986846, acc 1
2016-09-07T17:41:38.851040: step 5668, loss 0.0154467, acc 1
2016-09-07T17:41:39.538316: step 5669, loss 0.0172366, acc 1
2016-09-07T17:41:40.207907: step 5670, loss 0.00176369, acc 1
2016-09-07T17:41:40.882692: step 5671, loss 0.0368203, acc 0.98
2016-09-07T17:41:41.558252: step 5672, loss 0.0829489, acc 0.96
2016-09-07T17:41:42.252814: step 5673, loss 0.0512576, acc 0.98
2016-09-07T17:41:42.949903: step 5674, loss 0.0511793, acc 0.96
2016-09-07T17:41:43.610015: step 5675, loss 0.020884, acc 0.98
2016-09-07T17:41:44.332492: step 5676, loss 0.0183516, acc 0.98
2016-09-07T17:41:45.011975: step 5677, loss 0.0247383, acc 1
2016-09-07T17:41:45.688989: step 5678, loss 0.00511991, acc 1
2016-09-07T17:41:46.382041: step 5679, loss 0.0256675, acc 0.98
2016-09-07T17:41:47.075248: step 5680, loss 0.0108612, acc 1
2016-09-07T17:41:47.758926: step 5681, loss 0.00948464, acc 1
2016-09-07T17:41:48.437778: step 5682, loss 0.0141803, acc 1
2016-09-07T17:41:49.131019: step 5683, loss 0.0161245, acc 1
2016-09-07T17:41:49.795999: step 5684, loss 0.00249604, acc 1
2016-09-07T17:41:50.474665: step 5685, loss 0.0550318, acc 0.98
2016-09-07T17:41:51.166480: step 5686, loss 0.000769524, acc 1
2016-09-07T17:41:51.849806: step 5687, loss 0.0502377, acc 0.98
2016-09-07T17:41:52.545193: step 5688, loss 0.00877299, acc 1
2016-09-07T17:41:53.241720: step 5689, loss 0.00497462, acc 1
2016-09-07T17:41:53.944956: step 5690, loss 0.0419016, acc 0.98
2016-09-07T17:41:54.615593: step 5691, loss 0.0169182, acc 1
2016-09-07T17:41:55.301467: step 5692, loss 0.0478936, acc 0.98
2016-09-07T17:41:56.000564: step 5693, loss 0.0929894, acc 0.98
2016-09-07T17:41:56.691104: step 5694, loss 0.00808418, acc 1
2016-09-07T17:41:57.389239: step 5695, loss 0.0475888, acc 0.96
2016-09-07T17:41:58.042257: step 5696, loss 0.00417333, acc 1
2016-09-07T17:41:58.753036: step 5697, loss 0.0343332, acc 0.98
2016-09-07T17:41:59.438291: step 5698, loss 0.0201763, acc 0.98
2016-09-07T17:42:00.122497: step 5699, loss 0.00044984, acc 1
2016-09-07T17:42:00.842440: step 5700, loss 0.00648761, acc 1

Evaluation:
2016-09-07T17:42:03.813411: step 5700, loss 2.69979, acc 0.74

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-5700

2016-09-07T17:42:05.525517: step 5701, loss 0.000820987, acc 1
2016-09-07T17:42:06.207375: step 5702, loss 0.0855783, acc 0.94
2016-09-07T17:42:06.902795: step 5703, loss 0.0281909, acc 0.98
2016-09-07T17:42:07.583556: step 5704, loss 0.000945076, acc 1
2016-09-07T17:42:08.290331: step 5705, loss 0.0172553, acc 0.98
2016-09-07T17:42:08.957824: step 5706, loss 0.0302697, acc 0.98
2016-09-07T17:42:09.672373: step 5707, loss 0.0134368, acc 1
2016-09-07T17:42:10.365049: step 5708, loss 0.0757949, acc 0.96
2016-09-07T17:42:11.052127: step 5709, loss 0.0269659, acc 0.98
2016-09-07T17:42:11.743763: step 5710, loss 0.00129642, acc 1
2016-09-07T17:42:12.426977: step 5711, loss 0.019475, acc 1
2016-09-07T17:42:13.107806: step 5712, loss 0.0305184, acc 0.98
2016-09-07T17:42:13.765640: step 5713, loss 0.00856394, acc 1
2016-09-07T17:42:14.462531: step 5714, loss 0.0231515, acc 0.98
2016-09-07T17:42:15.153595: step 5715, loss 0.0239364, acc 0.98
2016-09-07T17:42:15.863222: step 5716, loss 0.0255807, acc 0.98
2016-09-07T17:42:16.544939: step 5717, loss 0.00334059, acc 1
2016-09-07T17:42:17.257335: step 5718, loss 0.0152546, acc 1
2016-09-07T17:42:17.977780: step 5719, loss 0.00182577, acc 1
2016-09-07T17:42:18.687729: step 5720, loss 0.0480224, acc 0.96
2016-09-07T17:42:19.386031: step 5721, loss 0.114414, acc 0.96
2016-09-07T17:42:20.074876: step 5722, loss 0.0100279, acc 1
2016-09-07T17:42:20.769703: step 5723, loss 0.0223401, acc 0.98
2016-09-07T17:42:21.459687: step 5724, loss 0.0488168, acc 0.96
2016-09-07T17:42:22.124021: step 5725, loss 0.0170277, acc 0.98
2016-09-07T17:42:22.854299: step 5726, loss 0.00699768, acc 1
2016-09-07T17:42:23.551326: step 5727, loss 0.0163123, acc 0.98
2016-09-07T17:42:24.254588: step 5728, loss 0.0226454, acc 0.98
2016-09-07T17:42:24.919898: step 5729, loss 0.0259001, acc 0.98
2016-09-07T17:42:25.595527: step 5730, loss 0.0303589, acc 0.98
2016-09-07T17:42:26.285895: step 5731, loss 0.00762651, acc 1
2016-09-07T17:42:26.961469: step 5732, loss 0.00744987, acc 1
2016-09-07T17:42:27.665971: step 5733, loss 0.023818, acc 0.98
2016-09-07T17:42:28.341545: step 5734, loss 0.0459161, acc 0.98
2016-09-07T17:42:29.050598: step 5735, loss 0.0184029, acc 1
2016-09-07T17:42:29.728527: step 5736, loss 0.0223, acc 1
2016-09-07T17:42:30.404887: step 5737, loss 0.032655, acc 0.98
2016-09-07T17:42:31.090703: step 5738, loss 0.00850325, acc 1
2016-09-07T17:42:31.753377: step 5739, loss 0.015147, acc 1
2016-09-07T17:42:32.443661: step 5740, loss 0.0153439, acc 0.98
2016-09-07T17:42:33.107471: step 5741, loss 0.0133461, acc 1
2016-09-07T17:42:33.782625: step 5742, loss 0.0308683, acc 0.98
2016-09-07T17:42:34.495041: step 5743, loss 0.0236767, acc 1
2016-09-07T17:42:35.227434: step 5744, loss 0.0168431, acc 0.98
2016-09-07T17:42:35.927145: step 5745, loss 0.0140733, acc 1
2016-09-07T17:42:36.597128: step 5746, loss 0.0105295, acc 1
2016-09-07T17:42:37.288779: step 5747, loss 0.00992253, acc 1
2016-09-07T17:42:37.983383: step 5748, loss 0.0289283, acc 0.98
2016-09-07T17:42:38.665543: step 5749, loss 0.000318727, acc 1
2016-09-07T17:42:39.353398: step 5750, loss 0.0383193, acc 0.96
2016-09-07T17:42:40.073696: step 5751, loss 0.055953, acc 0.98
2016-09-07T17:42:40.777618: step 5752, loss 0.0500799, acc 0.96
2016-09-07T17:42:41.447342: step 5753, loss 0.0401803, acc 0.98
2016-09-07T17:42:42.127953: step 5754, loss 0.00852161, acc 1
2016-09-07T17:42:42.812778: step 5755, loss 0.0279018, acc 0.98
2016-09-07T17:42:43.518210: step 5756, loss 0.0352106, acc 1
2016-09-07T17:42:44.229985: step 5757, loss 0.0409386, acc 0.96
2016-09-07T17:42:44.953613: step 5758, loss 0.0178842, acc 1
2016-09-07T17:42:45.660165: step 5759, loss 0.00985731, acc 1
2016-09-07T17:42:46.354301: step 5760, loss 0.0065362, acc 1
2016-09-07T17:42:47.048735: step 5761, loss 0.0145641, acc 1
2016-09-07T17:42:47.725484: step 5762, loss 0.0144985, acc 1
2016-09-07T17:42:48.414111: step 5763, loss 0.0121369, acc 1
2016-09-07T17:42:49.085590: step 5764, loss 0.000430282, acc 1
2016-09-07T17:42:49.757127: step 5765, loss 0.0320817, acc 0.98
2016-09-07T17:42:50.456543: step 5766, loss 0.00280367, acc 1
2016-09-07T17:42:51.118702: step 5767, loss 0.00242014, acc 1
2016-09-07T17:42:51.817648: step 5768, loss 0.023258, acc 0.98
2016-09-07T17:42:52.499726: step 5769, loss 0.0148904, acc 1
2016-09-07T17:42:53.184643: step 5770, loss 0.021879, acc 1
2016-09-07T17:42:53.874905: step 5771, loss 0.0115441, acc 1
2016-09-07T17:42:54.564612: step 5772, loss 0.0250788, acc 0.98
2016-09-07T17:42:55.276883: step 5773, loss 0.0198304, acc 0.98
2016-09-07T17:42:55.958405: step 5774, loss 0.0752237, acc 0.96
2016-09-07T17:42:56.631142: step 5775, loss 0.00101878, acc 1
2016-09-07T17:42:57.323949: step 5776, loss 0.00882453, acc 1
2016-09-07T17:42:58.015521: step 5777, loss 0.0136922, acc 1
2016-09-07T17:42:58.700764: step 5778, loss 0.0322259, acc 0.98
2016-09-07T17:42:59.403618: step 5779, loss 0.0402363, acc 0.98
2016-09-07T17:43:00.102283: step 5780, loss 0.0403807, acc 0.98
2016-09-07T17:43:00.825227: step 5781, loss 0.0155511, acc 0.98
2016-09-07T17:43:01.526559: step 5782, loss 0.000349516, acc 1
2016-09-07T17:43:02.203246: step 5783, loss 0.00040416, acc 1
2016-09-07T17:43:02.902019: step 5784, loss 0.0090609, acc 1
2016-09-07T17:43:03.615843: step 5785, loss 0.0319295, acc 0.98
2016-09-07T17:43:04.288700: step 5786, loss 0.0133038, acc 1
2016-09-07T17:43:04.980027: step 5787, loss 0.0456581, acc 0.98
2016-09-07T17:43:05.666180: step 5788, loss 0.0139939, acc 1
2016-09-07T17:43:06.357979: step 5789, loss 0.000388556, acc 1
2016-09-07T17:43:07.043548: step 5790, loss 0.0331178, acc 0.98
2016-09-07T17:43:07.724840: step 5791, loss 0.000888401, acc 1
2016-09-07T17:43:08.449303: step 5792, loss 0.0147095, acc 1
2016-09-07T17:43:09.148532: step 5793, loss 0.0111785, acc 1
2016-09-07T17:43:09.828240: step 5794, loss 0.0176117, acc 1
2016-09-07T17:43:10.508141: step 5795, loss 0.0812327, acc 0.94
2016-09-07T17:43:11.208494: step 5796, loss 0.0546967, acc 0.98
2016-09-07T17:43:11.881040: step 5797, loss 0.0167562, acc 1
2016-09-07T17:43:12.560566: step 5798, loss 0.0401311, acc 0.98
2016-09-07T17:43:13.270942: step 5799, loss 0.0367863, acc 0.98
2016-09-07T17:43:13.953645: step 5800, loss 0.00796485, acc 1

Evaluation:
2016-09-07T17:43:16.936872: step 5800, loss 2.61083, acc 0.742

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473237087/checkpoints/model-5800

2016-09-07T17:43:18.665689: step 5801, loss 0.0257368, acc 0.98
2016-09-07T17:43:19.355450: step 5802, loss 0.0929664, acc 0.98
2016-09-07T17:43:20.039347: step 5803, loss 0.00836814, acc 1
2016-09-07T17:43:20.775832: step 5804, loss 0.0122106, acc 1
2016-09-07T17:43:21.465671: step 5805, loss 0.0290188, acc 0.98
2016-09-07T17:43:22.144983: step 5806, loss 0.0180852, acc 0.98
2016-09-07T17:43:22.857389: step 5807, loss 0.0129631, acc 1
2016-09-07T17:43:23.534888: step 5808, loss 0.0305268, acc 0.96
2016-09-07T17:43:24.231792: step 5809, loss 0.025073, acc 1
2016-09-07T17:43:24.937273: step 5810, loss 0.0207029, acc 0.98
2016-09-07T17:43:25.606567: step 5811, loss 0.0178148, acc 0.98
2016-09-07T17:43:26.314700: step 5812, loss 0.0346908, acc 0.98
2016-09-07T17:43:26.986477: step 5813, loss 0.0377434, acc 0.96
2016-09-07T17:43:27.711783: step 5814, loss 6.49223e-05, acc 1
2016-09-07T17:43:28.373723: step 5815, loss 0.0372413, acc 0.96
2016-09-07T17:43:29.061479: step 5816, loss 0.0951197, acc 0.96
2016-09-07T17:43:29.760387: step 5817, loss 0.0210679, acc 1
2016-09-07T17:43:30.449590: step 5818, loss 0.0292563, acc 0.98
2016-09-07T17:43:31.134311: step 5819, loss 5.75273e-06, acc 1
2016-09-07T17:43:31.515282: step 5820, loss 0.0144872, acc 1

WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7fde71e30190>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.
WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7fde6ac76f90>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.

Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=50
CHECKPOINT_EVERY=100
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=300
EVALUATE_EVERY=100
FILTER_SIZES=3,4,5
HIDDEN_DIM=300
L2_REG_LAMBDA=0.15
LOG_DEVICE_PLACEMENT=False
NUM_EPOCHS=100
NUM_FILTERS=100
WORD2VEC=GoogleNews-vectors-negative300.bin

Loading data...
Vocabulary Size: 18758
Train/Dev split: 9662/1000
Writing to /home/cil/lstm-context-embeddings/runs/1473073177

Load word2vec file GoogleNews-vectors-negative300.bin

2016-09-05T18:59:56.590586: step 1, loss 0.693147, acc 0.44
2016-09-05T18:59:57.382262: step 2, loss 0.719294, acc 0.44
2016-09-05T18:59:58.235900: step 3, loss 0.702603, acc 0.4
2016-09-05T18:59:59.001642: step 4, loss 0.691271, acc 0.54
2016-09-05T18:59:59.790891: step 5, loss 0.666698, acc 0.64
2016-09-05T19:00:00.620348: step 6, loss 0.708183, acc 0.56
2016-09-05T19:00:01.419121: step 7, loss 0.758425, acc 0.46
2016-09-05T19:00:02.216848: step 8, loss 0.742147, acc 0.42
2016-09-05T19:00:03.034949: step 9, loss 0.679688, acc 0.6
2016-09-05T19:00:03.829760: step 10, loss 0.694947, acc 0.48
2016-09-05T19:00:04.623373: step 11, loss 0.698024, acc 0.44
2016-09-05T19:00:05.477489: step 12, loss 0.687665, acc 0.46
2016-09-05T19:00:06.286012: step 13, loss 0.698993, acc 0.56
2016-09-05T19:00:07.085573: step 14, loss 0.68305, acc 0.54
2016-09-05T19:00:07.885550: step 15, loss 0.701135, acc 0.54
2016-09-05T19:00:08.657112: step 16, loss 0.693586, acc 0.52
2016-09-05T19:00:09.450699: step 17, loss 0.69632, acc 0.5
2016-09-05T19:00:10.268991: step 18, loss 0.710239, acc 0.4
2016-09-05T19:00:11.055388: step 19, loss 0.689667, acc 0.56
2016-09-05T19:00:11.852289: step 20, loss 0.688042, acc 0.58
2016-09-05T19:00:12.662966: step 21, loss 0.69374, acc 0.56
2016-09-05T19:00:13.473363: step 22, loss 0.683651, acc 0.54
2016-09-05T19:00:14.271573: step 23, loss 0.714694, acc 0.52
2016-09-05T19:00:15.097420: step 24, loss 0.706502, acc 0.48
2016-09-05T19:00:15.899111: step 25, loss 0.688715, acc 0.46
2016-09-05T19:00:16.697480: step 26, loss 0.674586, acc 0.58
2016-09-05T19:00:17.510062: step 27, loss 0.657966, acc 0.68
2016-09-05T19:00:18.301414: step 28, loss 0.677412, acc 0.58
2016-09-05T19:00:19.102766: step 29, loss 0.677769, acc 0.52
2016-09-05T19:00:19.924498: step 30, loss 0.699452, acc 0.5
2016-09-05T19:00:20.737383: step 31, loss 0.591399, acc 0.7
2016-09-05T19:00:21.538493: step 32, loss 0.609838, acc 0.6
2016-09-05T19:00:22.380043: step 33, loss 0.687399, acc 0.6
2016-09-05T19:00:23.153006: step 34, loss 0.642566, acc 0.62
2016-09-05T19:00:23.938557: step 35, loss 0.61907, acc 0.58
2016-09-05T19:00:24.755615: step 36, loss 0.818674, acc 0.52
2016-09-05T19:00:25.556176: step 37, loss 0.652653, acc 0.66
2016-09-05T19:00:26.353989: step 38, loss 0.659265, acc 0.56
2016-09-05T19:00:27.151881: step 39, loss 0.603039, acc 0.7
2016-09-05T19:00:27.947639: step 40, loss 0.623806, acc 0.64
2016-09-05T19:00:28.766707: step 41, loss 0.594466, acc 0.66
2016-09-05T19:00:29.609601: step 42, loss 0.630483, acc 0.72
2016-09-05T19:00:30.415800: step 43, loss 0.647424, acc 0.68
2016-09-05T19:00:31.228884: step 44, loss 0.620403, acc 0.72
2016-09-05T19:00:32.052978: step 45, loss 0.646241, acc 0.58
2016-09-05T19:00:32.823132: step 46, loss 0.604287, acc 0.68
2016-09-05T19:00:33.629576: step 47, loss 0.55736, acc 0.7
2016-09-05T19:00:34.424476: step 48, loss 0.720988, acc 0.52
2016-09-05T19:00:35.213916: step 49, loss 0.573139, acc 0.66
2016-09-05T19:00:36.045490: step 50, loss 0.793391, acc 0.64
2016-09-05T19:00:36.859141: step 51, loss 0.799146, acc 0.64
2016-09-05T19:00:37.645017: step 52, loss 0.614318, acc 0.7
2016-09-05T19:00:38.456816: step 53, loss 0.54942, acc 0.8
2016-09-05T19:00:39.261076: step 54, loss 0.665103, acc 0.58
2016-09-05T19:00:40.074644: step 55, loss 0.622099, acc 0.74
2016-09-05T19:00:40.884183: step 56, loss 0.605839, acc 0.66
2016-09-05T19:00:41.714283: step 57, loss 0.591908, acc 0.72
2016-09-05T19:00:42.500894: step 58, loss 0.583547, acc 0.66
2016-09-05T19:00:43.309593: step 59, loss 0.631059, acc 0.74
2016-09-05T19:00:44.126415: step 60, loss 0.640761, acc 0.66
2016-09-05T19:00:44.953674: step 61, loss 0.688492, acc 0.5
2016-09-05T19:00:45.807333: step 62, loss 0.598932, acc 0.7
2016-09-05T19:00:46.610461: step 63, loss 0.550802, acc 0.72
2016-09-05T19:00:47.410408: step 64, loss 0.529411, acc 0.76
2016-09-05T19:00:48.203379: step 65, loss 0.463565, acc 0.82
2016-09-05T19:00:48.996804: step 66, loss 0.580008, acc 0.7
2016-09-05T19:00:49.746357: step 67, loss 0.561673, acc 0.7
2016-09-05T19:00:50.566548: step 68, loss 0.669124, acc 0.58
2016-09-05T19:00:51.371696: step 69, loss 0.453211, acc 0.78
2016-09-05T19:00:52.147855: step 70, loss 0.606129, acc 0.7
2016-09-05T19:00:52.954177: step 71, loss 0.590606, acc 0.7
2016-09-05T19:00:53.752535: step 72, loss 0.582212, acc 0.76
2016-09-05T19:00:54.561889: step 73, loss 0.457678, acc 0.8
2016-09-05T19:00:55.362234: step 74, loss 0.46949, acc 0.82
2016-09-05T19:00:56.172170: step 75, loss 0.549881, acc 0.72
2016-09-05T19:00:56.972669: step 76, loss 0.721817, acc 0.64
2016-09-05T19:00:57.804703: step 77, loss 0.519435, acc 0.72
2016-09-05T19:00:58.628335: step 78, loss 0.581201, acc 0.74
2016-09-05T19:00:59.435986: step 79, loss 0.518992, acc 0.76
2016-09-05T19:01:00.277415: step 80, loss 0.590016, acc 0.72
2016-09-05T19:01:01.062627: step 81, loss 0.485136, acc 0.68
2016-09-05T19:01:01.858751: step 82, loss 0.529646, acc 0.74
2016-09-05T19:01:02.670898: step 83, loss 0.566929, acc 0.72
2016-09-05T19:01:03.482758: step 84, loss 0.446574, acc 0.82
2016-09-05T19:01:04.286832: step 85, loss 0.572, acc 0.74
2016-09-05T19:01:05.101626: step 86, loss 0.427586, acc 0.78
2016-09-05T19:01:05.918961: step 87, loss 0.57889, acc 0.74
2016-09-05T19:01:06.685016: step 88, loss 0.416327, acc 0.84
2016-09-05T19:01:07.473295: step 89, loss 0.43747, acc 0.78
2016-09-05T19:01:08.286513: step 90, loss 0.587598, acc 0.7
2016-09-05T19:01:09.066389: step 91, loss 0.589908, acc 0.7
2016-09-05T19:01:09.871543: step 92, loss 0.538862, acc 0.7
2016-09-05T19:01:10.711814: step 93, loss 0.58251, acc 0.66
2016-09-05T19:01:11.518336: step 94, loss 0.501877, acc 0.7
2016-09-05T19:01:12.325807: step 95, loss 0.38269, acc 0.8
2016-09-05T19:01:13.157837: step 96, loss 0.437135, acc 0.78
2016-09-05T19:01:13.941186: step 97, loss 0.631748, acc 0.7
2016-09-05T19:01:14.756060: step 98, loss 0.469236, acc 0.76
2016-09-05T19:01:15.564086: step 99, loss 0.585036, acc 0.66
2016-09-05T19:01:16.377394: step 100, loss 0.479008, acc 0.72

Evaluation:
2016-09-05T19:01:19.866419: step 100, loss 0.506756, acc 0.763

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-100

2016-09-05T19:01:21.827464: step 101, loss 0.522784, acc 0.76
2016-09-05T19:01:22.651863: step 102, loss 0.683653, acc 0.72
2016-09-05T19:01:23.455855: step 103, loss 0.490465, acc 0.76
2016-09-05T19:01:24.281809: step 104, loss 0.461227, acc 0.78
2016-09-05T19:01:25.092899: step 105, loss 0.487419, acc 0.74
2016-09-05T19:01:25.899738: step 106, loss 0.4917, acc 0.76
2016-09-05T19:01:26.749549: step 107, loss 0.501052, acc 0.76
2016-09-05T19:01:27.554397: step 108, loss 0.434979, acc 0.8
2016-09-05T19:01:28.395120: step 109, loss 0.462987, acc 0.8
2016-09-05T19:01:29.237569: step 110, loss 0.513866, acc 0.74
2016-09-05T19:01:30.042487: step 111, loss 0.525521, acc 0.74
2016-09-05T19:01:30.852980: step 112, loss 0.475844, acc 0.76
2016-09-05T19:01:31.675120: step 113, loss 0.512068, acc 0.74
2016-09-05T19:01:32.488651: step 114, loss 0.494932, acc 0.8
2016-09-05T19:01:33.283605: step 115, loss 0.40878, acc 0.8
2016-09-05T19:01:34.100526: step 116, loss 0.423763, acc 0.78
2016-09-05T19:01:34.897551: step 117, loss 0.749928, acc 0.64
2016-09-05T19:01:35.725058: step 118, loss 0.475307, acc 0.74
2016-09-05T19:01:36.565634: step 119, loss 0.581917, acc 0.76
2016-09-05T19:01:37.353759: step 120, loss 0.399494, acc 0.8
2016-09-05T19:01:38.142769: step 121, loss 0.595945, acc 0.74
2016-09-05T19:01:38.984659: step 122, loss 0.455427, acc 0.82
2016-09-05T19:01:39.791243: step 123, loss 0.578864, acc 0.72
2016-09-05T19:01:40.608578: step 124, loss 0.520379, acc 0.8
2016-09-05T19:01:41.440398: step 125, loss 0.645792, acc 0.68
2016-09-05T19:01:42.264820: step 126, loss 0.653221, acc 0.7
2016-09-05T19:01:43.047596: step 127, loss 0.634502, acc 0.64
2016-09-05T19:01:43.837990: step 128, loss 0.412134, acc 0.86
2016-09-05T19:01:44.656041: step 129, loss 0.453931, acc 0.72
2016-09-05T19:01:45.449890: step 130, loss 0.540734, acc 0.7
2016-09-05T19:01:46.261303: step 131, loss 0.487, acc 0.78
2016-09-05T19:01:47.068858: step 132, loss 0.489559, acc 0.84
2016-09-05T19:01:47.866599: step 133, loss 0.60473, acc 0.64
2016-09-05T19:01:48.671431: step 134, loss 0.421485, acc 0.86
2016-09-05T19:01:49.495649: step 135, loss 0.563045, acc 0.66
2016-09-05T19:01:50.318527: step 136, loss 0.507287, acc 0.74
2016-09-05T19:01:51.105144: step 137, loss 0.489812, acc 0.74
2016-09-05T19:01:51.907480: step 138, loss 0.554665, acc 0.72
2016-09-05T19:01:52.683563: step 139, loss 0.477727, acc 0.76
2016-09-05T19:01:53.490942: step 140, loss 0.400624, acc 0.86
2016-09-05T19:01:54.326681: step 141, loss 0.595941, acc 0.62
2016-09-05T19:01:55.098742: step 142, loss 0.523609, acc 0.72
2016-09-05T19:01:55.913855: step 143, loss 0.550572, acc 0.6
2016-09-05T19:01:56.707316: step 144, loss 0.517418, acc 0.8
2016-09-05T19:01:57.499771: step 145, loss 0.491587, acc 0.78
2016-09-05T19:01:58.321664: step 146, loss 0.561779, acc 0.68
2016-09-05T19:01:59.136547: step 147, loss 0.539572, acc 0.7
2016-09-05T19:01:59.916777: step 148, loss 0.384793, acc 0.86
2016-09-05T19:02:00.759369: step 149, loss 0.523972, acc 0.72
2016-09-05T19:02:01.577688: step 150, loss 0.57957, acc 0.72
2016-09-05T19:02:02.368133: step 151, loss 0.579914, acc 0.7
2016-09-05T19:02:03.197565: step 152, loss 0.48908, acc 0.76
2016-09-05T19:02:04.012212: step 153, loss 0.529433, acc 0.74
2016-09-05T19:02:04.805112: step 154, loss 0.588139, acc 0.66
2016-09-05T19:02:05.599409: step 155, loss 0.42681, acc 0.8
2016-09-05T19:02:06.414921: step 156, loss 0.417857, acc 0.78
2016-09-05T19:02:07.227132: step 157, loss 0.448749, acc 0.76
2016-09-05T19:02:08.021378: step 158, loss 0.595169, acc 0.66
2016-09-05T19:02:08.830293: step 159, loss 0.502945, acc 0.78
2016-09-05T19:02:09.619875: step 160, loss 0.512728, acc 0.72
2016-09-05T19:02:10.446925: step 161, loss 0.593738, acc 0.74
2016-09-05T19:02:11.288792: step 162, loss 0.606182, acc 0.66
2016-09-05T19:02:12.070959: step 163, loss 0.431856, acc 0.8
2016-09-05T19:02:12.901256: step 164, loss 0.402106, acc 0.76
2016-09-05T19:02:13.696684: step 165, loss 0.469855, acc 0.8
2016-09-05T19:02:14.477063: step 166, loss 0.47315, acc 0.76
2016-09-05T19:02:15.268643: step 167, loss 0.528601, acc 0.7
2016-09-05T19:02:16.097650: step 168, loss 0.556989, acc 0.7
2016-09-05T19:02:16.897551: step 169, loss 0.550414, acc 0.7
2016-09-05T19:02:17.684067: step 170, loss 0.5972, acc 0.7
2016-09-05T19:02:18.504348: step 171, loss 0.585735, acc 0.64
2016-09-05T19:02:19.299406: step 172, loss 0.515202, acc 0.76
2016-09-05T19:02:20.088605: step 173, loss 0.536153, acc 0.74
2016-09-05T19:02:20.907358: step 174, loss 0.481769, acc 0.7
2016-09-05T19:02:21.697091: step 175, loss 0.442993, acc 0.86
2016-09-05T19:02:22.485059: step 176, loss 0.475731, acc 0.82
2016-09-05T19:02:23.291476: step 177, loss 0.424894, acc 0.8
2016-09-05T19:02:24.089415: step 178, loss 0.45162, acc 0.78
2016-09-05T19:02:24.909127: step 179, loss 0.420943, acc 0.84
2016-09-05T19:02:25.727505: step 180, loss 0.308484, acc 0.88
2016-09-05T19:02:26.516870: step 181, loss 0.602505, acc 0.66
2016-09-05T19:02:27.320031: step 182, loss 0.535183, acc 0.72
2016-09-05T19:02:28.114031: step 183, loss 0.474347, acc 0.78
2016-09-05T19:02:28.937489: step 184, loss 0.661187, acc 0.6
2016-09-05T19:02:29.747850: step 185, loss 0.585711, acc 0.74
2016-09-05T19:02:30.549402: step 186, loss 0.466516, acc 0.8
2016-09-05T19:02:31.326452: step 187, loss 0.493238, acc 0.72
2016-09-05T19:02:32.138463: step 188, loss 0.511083, acc 0.74
2016-09-05T19:02:32.948196: step 189, loss 0.418667, acc 0.86
2016-09-05T19:02:33.726550: step 190, loss 0.47818, acc 0.78
2016-09-05T19:02:34.572778: step 191, loss 0.539687, acc 0.76
2016-09-05T19:02:35.398873: step 192, loss 0.538872, acc 0.7
2016-09-05T19:02:36.179071: step 193, loss 0.443106, acc 0.78
2016-09-05T19:02:36.609060: step 194, loss 0.277779, acc 0.916667
2016-09-05T19:02:37.387421: step 195, loss 0.407049, acc 0.82
2016-09-05T19:02:38.206788: step 196, loss 0.392445, acc 0.82
2016-09-05T19:02:39.033770: step 197, loss 0.304037, acc 0.84
2016-09-05T19:02:39.809322: step 198, loss 0.348695, acc 0.86
2016-09-05T19:02:40.615283: step 199, loss 0.333047, acc 0.88
2016-09-05T19:02:41.455313: step 200, loss 0.492622, acc 0.8

Evaluation:
2016-09-05T19:02:44.948423: step 200, loss 0.460262, acc 0.789

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-200

2016-09-05T19:02:46.789754: step 201, loss 0.421607, acc 0.8
2016-09-05T19:02:47.607413: step 202, loss 0.391016, acc 0.78
2016-09-05T19:02:48.389983: step 203, loss 0.275979, acc 0.84
2016-09-05T19:02:49.210192: step 204, loss 0.537671, acc 0.78
2016-09-05T19:02:50.005448: step 205, loss 0.366727, acc 0.84
2016-09-05T19:02:50.808409: step 206, loss 0.289516, acc 0.86
2016-09-05T19:02:51.618543: step 207, loss 0.676749, acc 0.76
2016-09-05T19:02:52.429554: step 208, loss 0.340248, acc 0.88
2016-09-05T19:02:53.230480: step 209, loss 0.256269, acc 0.94
2016-09-05T19:02:54.030032: step 210, loss 0.369194, acc 0.84
2016-09-05T19:02:54.858706: step 211, loss 0.25009, acc 0.92
2016-09-05T19:02:55.654052: step 212, loss 0.597724, acc 0.76
2016-09-05T19:02:56.460477: step 213, loss 0.284256, acc 0.86
2016-09-05T19:02:57.312249: step 214, loss 0.537533, acc 0.76
2016-09-05T19:02:58.093917: step 215, loss 0.232105, acc 0.92
2016-09-05T19:02:58.933407: step 216, loss 0.505916, acc 0.74
2016-09-05T19:02:59.752487: step 217, loss 0.336738, acc 0.84
2016-09-05T19:03:00.554159: step 218, loss 0.457753, acc 0.8
2016-09-05T19:03:01.345510: step 219, loss 0.332491, acc 0.86
2016-09-05T19:03:02.164410: step 220, loss 0.269689, acc 0.92
2016-09-05T19:03:02.968994: step 221, loss 0.261873, acc 0.9
2016-09-05T19:03:03.777350: step 222, loss 0.334358, acc 0.82
2016-09-05T19:03:04.579874: step 223, loss 0.428155, acc 0.78
2016-09-05T19:03:05.376630: step 224, loss 0.355367, acc 0.84
2016-09-05T19:03:06.175713: step 225, loss 0.475512, acc 0.78
2016-09-05T19:03:07.005673: step 226, loss 0.410455, acc 0.82
2016-09-05T19:03:07.794823: step 227, loss 0.220246, acc 0.96
2016-09-05T19:03:08.582957: step 228, loss 0.386776, acc 0.8
2016-09-05T19:03:09.418865: step 229, loss 0.257433, acc 0.88
2016-09-05T19:03:10.186492: step 230, loss 0.410915, acc 0.8
2016-09-05T19:03:10.982978: step 231, loss 0.32175, acc 0.86
2016-09-05T19:03:11.800650: step 232, loss 0.304732, acc 0.9
2016-09-05T19:03:12.578299: step 233, loss 0.477558, acc 0.76
2016-09-05T19:03:13.378025: step 234, loss 0.542954, acc 0.78
2016-09-05T19:03:14.165920: step 235, loss 0.287716, acc 0.92
2016-09-05T19:03:14.954926: step 236, loss 0.248009, acc 0.88
2016-09-05T19:03:15.788606: step 237, loss 0.502278, acc 0.76
2016-09-05T19:03:16.597629: step 238, loss 0.399028, acc 0.8
2016-09-05T19:03:17.387585: step 239, loss 0.474593, acc 0.76
2016-09-05T19:03:18.193619: step 240, loss 0.338879, acc 0.84
2016-09-05T19:03:19.006345: step 241, loss 0.332004, acc 0.88
2016-09-05T19:03:19.786986: step 242, loss 0.387702, acc 0.86
2016-09-05T19:03:20.599750: step 243, loss 0.451662, acc 0.8
2016-09-05T19:03:21.389476: step 244, loss 0.299138, acc 0.88
2016-09-05T19:03:22.176405: step 245, loss 0.258288, acc 0.86
2016-09-05T19:03:22.999428: step 246, loss 0.304894, acc 0.84
2016-09-05T19:03:23.812417: step 247, loss 0.31143, acc 0.9
2016-09-05T19:03:24.588472: step 248, loss 0.340903, acc 0.84
2016-09-05T19:03:25.423925: step 249, loss 0.421566, acc 0.8
2016-09-05T19:03:26.202889: step 250, loss 0.397918, acc 0.84
2016-09-05T19:03:27.008643: step 251, loss 0.492564, acc 0.7
2016-09-05T19:03:27.818669: step 252, loss 0.300711, acc 0.84
2016-09-05T19:03:28.643201: step 253, loss 0.398474, acc 0.78
2016-09-05T19:03:29.443045: step 254, loss 0.662054, acc 0.7
2016-09-05T19:03:30.275258: step 255, loss 0.421149, acc 0.78
2016-09-05T19:03:31.120363: step 256, loss 0.276934, acc 0.9
2016-09-05T19:03:31.889784: step 257, loss 0.327885, acc 0.86
2016-09-05T19:03:32.703393: step 258, loss 0.51179, acc 0.66
2016-09-05T19:03:33.517377: step 259, loss 0.439329, acc 0.76
2016-09-05T19:03:34.299410: step 260, loss 0.320641, acc 0.9
2016-09-05T19:03:35.121026: step 261, loss 0.30305, acc 0.9
2016-09-05T19:03:35.933405: step 262, loss 0.360637, acc 0.84
2016-09-05T19:03:36.721125: step 263, loss 0.416196, acc 0.82
2016-09-05T19:03:37.527608: step 264, loss 0.31071, acc 0.92
2016-09-05T19:03:38.345625: step 265, loss 0.333741, acc 0.9
2016-09-05T19:03:39.146506: step 266, loss 0.320273, acc 0.86
2016-09-05T19:03:39.945410: step 267, loss 0.235695, acc 0.9
2016-09-05T19:03:40.748693: step 268, loss 0.350562, acc 0.84
2016-09-05T19:03:41.576714: step 269, loss 0.284112, acc 0.9
2016-09-05T19:03:42.376676: step 270, loss 0.298718, acc 0.88
2016-09-05T19:03:43.183897: step 271, loss 0.351738, acc 0.86
2016-09-05T19:03:43.953244: step 272, loss 0.423211, acc 0.78
2016-09-05T19:03:44.776499: step 273, loss 0.295025, acc 0.84
2016-09-05T19:03:45.593414: step 274, loss 0.424692, acc 0.82
2016-09-05T19:03:46.371223: step 275, loss 0.373769, acc 0.8
2016-09-05T19:03:47.198116: step 276, loss 0.418729, acc 0.82
2016-09-05T19:03:47.993319: step 277, loss 0.386385, acc 0.8
2016-09-05T19:03:48.790705: step 278, loss 0.315571, acc 0.88
2016-09-05T19:03:49.599224: step 279, loss 0.417155, acc 0.84
2016-09-05T19:03:50.428540: step 280, loss 0.569199, acc 0.72
2016-09-05T19:03:51.205481: step 281, loss 0.38209, acc 0.84
2016-09-05T19:03:52.043016: step 282, loss 0.300592, acc 0.92
2016-09-05T19:03:52.888367: step 283, loss 0.424919, acc 0.82
2016-09-05T19:03:53.678768: step 284, loss 0.445645, acc 0.84
2016-09-05T19:03:54.478522: step 285, loss 0.382922, acc 0.82
2016-09-05T19:03:55.288823: step 286, loss 0.397009, acc 0.8
2016-09-05T19:03:56.079967: step 287, loss 0.335459, acc 0.86
2016-09-05T19:03:56.870744: step 288, loss 0.374218, acc 0.84
2016-09-05T19:03:57.670391: step 289, loss 0.392249, acc 0.78
2016-09-05T19:03:58.447429: step 290, loss 0.317028, acc 0.84
2016-09-05T19:03:59.265543: step 291, loss 0.264057, acc 0.92
2016-09-05T19:04:00.061788: step 292, loss 0.254341, acc 0.92
2016-09-05T19:04:00.888422: step 293, loss 0.455621, acc 0.76
2016-09-05T19:04:01.704780: step 294, loss 0.279778, acc 0.86
2016-09-05T19:04:02.526148: step 295, loss 0.371946, acc 0.82
2016-09-05T19:04:03.338443: step 296, loss 0.259405, acc 0.9
2016-09-05T19:04:04.162502: step 297, loss 0.309115, acc 0.88
2016-09-05T19:04:04.978822: step 298, loss 0.283215, acc 0.88
2016-09-05T19:04:05.758019: step 299, loss 0.55832, acc 0.72
2016-09-05T19:04:06.591566: step 300, loss 0.273246, acc 0.9

Evaluation:
2016-09-05T19:04:10.057743: step 300, loss 0.46808, acc 0.795

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-300

2016-09-05T19:04:11.910496: step 301, loss 0.393526, acc 0.84
2016-09-05T19:04:12.765917: step 302, loss 0.259673, acc 0.84
2016-09-05T19:04:13.592780: step 303, loss 0.433687, acc 0.86
2016-09-05T19:04:14.377382: step 304, loss 0.405704, acc 0.82
2016-09-05T19:04:15.173323: step 305, loss 0.492398, acc 0.8
2016-09-05T19:04:15.983968: step 306, loss 0.302159, acc 0.86
2016-09-05T19:04:16.763020: step 307, loss 0.363197, acc 0.86
2016-09-05T19:04:17.572347: step 308, loss 0.394601, acc 0.78
2016-09-05T19:04:18.381833: step 309, loss 0.440341, acc 0.84
2016-09-05T19:04:19.171219: step 310, loss 0.279835, acc 0.86
2016-09-05T19:04:19.965577: step 311, loss 0.274182, acc 0.84
2016-09-05T19:04:20.766336: step 312, loss 0.324921, acc 0.88
2016-09-05T19:04:21.555124: step 313, loss 0.348176, acc 0.86
2016-09-05T19:04:22.373096: step 314, loss 0.479189, acc 0.84
2016-09-05T19:04:23.165460: step 315, loss 0.510122, acc 0.7
2016-09-05T19:04:23.951639: step 316, loss 0.402341, acc 0.82
2016-09-05T19:04:24.768815: step 317, loss 0.37465, acc 0.82
2016-09-05T19:04:25.574218: step 318, loss 0.247591, acc 0.9
2016-09-05T19:04:26.391329: step 319, loss 0.255144, acc 0.98
2016-09-05T19:04:27.210310: step 320, loss 0.407225, acc 0.8
2016-09-05T19:04:27.996511: step 321, loss 0.263814, acc 0.9
2016-09-05T19:04:28.796191: step 322, loss 0.391141, acc 0.8
2016-09-05T19:04:29.615450: step 323, loss 0.385711, acc 0.8
2016-09-05T19:04:30.384435: step 324, loss 0.320243, acc 0.88
2016-09-05T19:04:31.178212: step 325, loss 0.503652, acc 0.72
2016-09-05T19:04:31.999731: step 326, loss 0.520296, acc 0.78
2016-09-05T19:04:32.788253: step 327, loss 0.39941, acc 0.78
2016-09-05T19:04:33.608707: step 328, loss 0.44357, acc 0.78
2016-09-05T19:04:34.455280: step 329, loss 0.417613, acc 0.8
2016-09-05T19:04:35.285682: step 330, loss 0.420599, acc 0.82
2016-09-05T19:04:36.056746: step 331, loss 0.241788, acc 0.9
2016-09-05T19:04:36.870143: step 332, loss 0.26723, acc 0.88
2016-09-05T19:04:37.666322: step 333, loss 0.336095, acc 0.88
2016-09-05T19:04:38.464706: step 334, loss 0.303508, acc 0.92
2016-09-05T19:04:39.299385: step 335, loss 0.333616, acc 0.86
2016-09-05T19:04:40.130746: step 336, loss 0.384357, acc 0.88
2016-09-05T19:04:40.909405: step 337, loss 0.346201, acc 0.88
2016-09-05T19:04:41.700379: step 338, loss 0.392451, acc 0.78
2016-09-05T19:04:42.534277: step 339, loss 0.297452, acc 0.86
2016-09-05T19:04:43.325509: step 340, loss 0.308567, acc 0.84
2016-09-05T19:04:44.125931: step 341, loss 0.309348, acc 0.84
2016-09-05T19:04:44.937997: step 342, loss 0.302354, acc 0.86
2016-09-05T19:04:45.765025: step 343, loss 0.282426, acc 0.88
2016-09-05T19:04:46.598444: step 344, loss 0.42687, acc 0.8
2016-09-05T19:04:47.396650: step 345, loss 0.379464, acc 0.88
2016-09-05T19:04:48.185280: step 346, loss 0.527607, acc 0.74
2016-09-05T19:04:48.980319: step 347, loss 0.256123, acc 0.9
2016-09-05T19:04:49.778339: step 348, loss 0.241769, acc 0.92
2016-09-05T19:04:50.604113: step 349, loss 0.43349, acc 0.84
2016-09-05T19:04:51.413898: step 350, loss 0.289095, acc 0.88
2016-09-05T19:04:52.222814: step 351, loss 0.557073, acc 0.74
2016-09-05T19:04:53.001677: step 352, loss 0.437774, acc 0.8
2016-09-05T19:04:53.830898: step 353, loss 0.370855, acc 0.8
2016-09-05T19:04:54.669473: step 354, loss 0.47436, acc 0.76
2016-09-05T19:04:55.450620: step 355, loss 0.34761, acc 0.86
2016-09-05T19:04:56.240203: step 356, loss 0.306133, acc 0.86
2016-09-05T19:04:57.055103: step 357, loss 0.425004, acc 0.8
2016-09-05T19:04:57.832740: step 358, loss 0.316519, acc 0.86
2016-09-05T19:04:58.643352: step 359, loss 0.446816, acc 0.8
2016-09-05T19:04:59.450924: step 360, loss 0.501233, acc 0.76
2016-09-05T19:05:00.270800: step 361, loss 0.448639, acc 0.76
2016-09-05T19:05:01.069190: step 362, loss 0.406323, acc 0.78
2016-09-05T19:05:01.880924: step 363, loss 0.304063, acc 0.88
2016-09-05T19:05:02.667454: step 364, loss 0.347999, acc 0.86
2016-09-05T19:05:03.481863: step 365, loss 0.47543, acc 0.74
2016-09-05T19:05:04.282133: step 366, loss 0.343752, acc 0.88
2016-09-05T19:05:05.067346: step 367, loss 0.345462, acc 0.9
2016-09-05T19:05:05.874277: step 368, loss 0.353847, acc 0.86
2016-09-05T19:05:06.716093: step 369, loss 0.425755, acc 0.8
2016-09-05T19:05:07.491563: step 370, loss 0.4264, acc 0.74
2016-09-05T19:05:08.298455: step 371, loss 0.443041, acc 0.8
2016-09-05T19:05:09.110289: step 372, loss 0.360876, acc 0.82
2016-09-05T19:05:09.897019: step 373, loss 0.482892, acc 0.76
2016-09-05T19:05:10.733541: step 374, loss 0.42088, acc 0.84
2016-09-05T19:05:11.555466: step 375, loss 0.360855, acc 0.86
2016-09-05T19:05:12.349801: step 376, loss 0.270686, acc 0.88
2016-09-05T19:05:13.137329: step 377, loss 0.492343, acc 0.76
2016-09-05T19:05:13.942175: step 378, loss 0.300184, acc 0.88
2016-09-05T19:05:14.697422: step 379, loss 0.353726, acc 0.84
2016-09-05T19:05:15.534407: step 380, loss 0.514915, acc 0.82
2016-09-05T19:05:16.360345: step 381, loss 0.389313, acc 0.8
2016-09-05T19:05:17.152596: step 382, loss 0.395684, acc 0.82
2016-09-05T19:05:18.040898: step 383, loss 0.393809, acc 0.8
2016-09-05T19:05:18.842518: step 384, loss 0.50049, acc 0.78
2016-09-05T19:05:19.617519: step 385, loss 0.365871, acc 0.86
2016-09-05T19:05:20.446954: step 386, loss 0.401789, acc 0.84
2016-09-05T19:05:21.241425: step 387, loss 0.404713, acc 0.82
2016-09-05T19:05:21.653018: step 388, loss 0.369681, acc 0.833333
2016-09-05T19:05:22.462198: step 389, loss 0.355674, acc 0.84
2016-09-05T19:05:23.246877: step 390, loss 0.259854, acc 0.86
2016-09-05T19:05:24.071503: step 391, loss 0.282922, acc 0.9
2016-09-05T19:05:24.882951: step 392, loss 0.215945, acc 0.92
2016-09-05T19:05:25.643830: step 393, loss 0.274792, acc 0.86
2016-09-05T19:05:26.446376: step 394, loss 0.260497, acc 0.9
2016-09-05T19:05:27.260348: step 395, loss 0.247019, acc 0.88
2016-09-05T19:05:28.054256: step 396, loss 0.250912, acc 0.88
2016-09-05T19:05:28.860481: step 397, loss 0.234051, acc 0.94
2016-09-05T19:05:29.719551: step 398, loss 0.224152, acc 0.94
2016-09-05T19:05:30.494608: step 399, loss 0.20948, acc 0.92
2016-09-05T19:05:31.283696: step 400, loss 0.136099, acc 0.94

Evaluation:
2016-09-05T19:05:34.773386: step 400, loss 0.524757, acc 0.8

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-400

2016-09-05T19:05:36.690984: step 401, loss 0.259489, acc 0.86
2016-09-05T19:05:37.474320: step 402, loss 0.262558, acc 0.9
2016-09-05T19:05:38.296413: step 403, loss 0.344955, acc 0.86
2016-09-05T19:05:39.122426: step 404, loss 0.237097, acc 0.92
2016-09-05T19:05:39.925268: step 405, loss 0.250318, acc 0.88
2016-09-05T19:05:40.736500: step 406, loss 0.235872, acc 0.92
2016-09-05T19:05:41.552146: step 407, loss 0.0839081, acc 0.98
2016-09-05T19:05:42.360093: step 408, loss 0.30363, acc 0.88
2016-09-05T19:05:43.171159: step 409, loss 0.455354, acc 0.84
2016-09-05T19:05:43.979617: step 410, loss 0.232451, acc 0.92
2016-09-05T19:05:44.787758: step 411, loss 0.22778, acc 0.88
2016-09-05T19:05:45.621880: step 412, loss 0.112105, acc 0.96
2016-09-05T19:05:46.445111: step 413, loss 0.292938, acc 0.92
2016-09-05T19:05:47.262102: step 414, loss 0.343022, acc 0.84
2016-09-05T19:05:48.086617: step 415, loss 0.31145, acc 0.88
2016-09-05T19:05:48.889270: step 416, loss 0.212301, acc 0.94
2016-09-05T19:05:49.695136: step 417, loss 0.185615, acc 0.92
2016-09-05T19:05:50.523897: step 418, loss 0.272058, acc 0.84
2016-09-05T19:05:51.351747: step 419, loss 0.124312, acc 0.96
2016-09-05T19:05:52.187993: step 420, loss 0.207056, acc 0.9
2016-09-05T19:05:53.025948: step 421, loss 0.426524, acc 0.84
2016-09-05T19:05:53.859786: step 422, loss 0.218903, acc 0.9
2016-09-05T19:05:54.640080: step 423, loss 0.287007, acc 0.9
2016-09-05T19:05:55.450315: step 424, loss 0.382965, acc 0.84
2016-09-05T19:05:56.243859: step 425, loss 0.182081, acc 0.94
2016-09-05T19:05:57.028919: step 426, loss 0.20701, acc 0.94
2016-09-05T19:05:57.826560: step 427, loss 0.374633, acc 0.82
2016-09-05T19:05:58.635349: step 428, loss 0.426586, acc 0.82
2016-09-05T19:05:59.440756: step 429, loss 0.187838, acc 0.94
2016-09-05T19:06:00.280699: step 430, loss 0.188523, acc 0.96
2016-09-05T19:06:01.088287: step 431, loss 0.367093, acc 0.84
2016-09-05T19:06:01.893159: step 432, loss 0.234963, acc 0.88
2016-09-05T19:06:02.702090: step 433, loss 0.22113, acc 0.92
2016-09-05T19:06:03.520030: step 434, loss 0.400808, acc 0.86
2016-09-05T19:06:04.336633: step 435, loss 0.275541, acc 0.82
2016-09-05T19:06:05.164908: step 436, loss 0.470795, acc 0.78
2016-09-05T19:06:05.987316: step 437, loss 0.209593, acc 0.92
2016-09-05T19:06:06.776767: step 438, loss 0.178206, acc 0.92
2016-09-05T19:06:07.559512: step 439, loss 0.105715, acc 0.96
2016-09-05T19:06:08.381290: step 440, loss 0.283567, acc 0.86
2016-09-05T19:06:09.162846: step 441, loss 0.245484, acc 0.94
2016-09-05T19:06:09.955300: step 442, loss 0.121558, acc 0.94
2016-09-05T19:06:10.777455: step 443, loss 0.273461, acc 0.88
2016-09-05T19:06:11.588526: step 444, loss 0.43223, acc 0.76
2016-09-05T19:06:12.395398: step 445, loss 0.235381, acc 0.92
2016-09-05T19:06:13.177813: step 446, loss 0.250842, acc 0.9
2016-09-05T19:06:13.972374: step 447, loss 0.288239, acc 0.84
2016-09-05T19:06:14.793731: step 448, loss 0.122876, acc 0.96
2016-09-05T19:06:15.618411: step 449, loss 0.12622, acc 0.96
2016-09-05T19:06:16.402280: step 450, loss 0.287588, acc 0.84
2016-09-05T19:06:17.235851: step 451, loss 0.259477, acc 0.9
2016-09-05T19:06:18.044225: step 452, loss 0.281014, acc 0.9
2016-09-05T19:06:18.817440: step 453, loss 0.241009, acc 0.9
2016-09-05T19:06:19.629883: step 454, loss 0.195667, acc 0.9
2016-09-05T19:06:20.437855: step 455, loss 0.332462, acc 0.86
2016-09-05T19:06:21.223148: step 456, loss 0.193951, acc 0.94
2016-09-05T19:06:22.037396: step 457, loss 0.190067, acc 0.9
2016-09-05T19:06:22.846436: step 458, loss 0.296616, acc 0.9
2016-09-05T19:06:23.631487: step 459, loss 0.275912, acc 0.88
2016-09-05T19:06:24.447735: step 460, loss 0.248984, acc 0.94
2016-09-05T19:06:25.256377: step 461, loss 0.262973, acc 0.9
2016-09-05T19:06:26.025132: step 462, loss 0.321862, acc 0.86
2016-09-05T19:06:26.855036: step 463, loss 0.270505, acc 0.92
2016-09-05T19:06:27.687623: step 464, loss 0.333973, acc 0.8
2016-09-05T19:06:28.518656: step 465, loss 0.308576, acc 0.88
2016-09-05T19:06:29.359545: step 466, loss 0.157377, acc 0.96
2016-09-05T19:06:30.172450: step 467, loss 0.210434, acc 0.92
2016-09-05T19:06:30.939492: step 468, loss 0.144876, acc 0.94
2016-09-05T19:06:31.775701: step 469, loss 0.205404, acc 0.9
2016-09-05T19:06:32.608420: step 470, loss 0.252003, acc 0.9
2016-09-05T19:06:33.391014: step 471, loss 0.287144, acc 0.9
2016-09-05T19:06:34.200010: step 472, loss 0.355814, acc 0.88
2016-09-05T19:06:35.018194: step 473, loss 0.22341, acc 0.84
2016-09-05T19:06:35.785061: step 474, loss 0.39983, acc 0.8
2016-09-05T19:06:36.565593: step 475, loss 0.172595, acc 0.96
2016-09-05T19:06:37.369796: step 476, loss 0.177581, acc 0.92
2016-09-05T19:06:38.174577: step 477, loss 0.307044, acc 0.86
2016-09-05T19:06:38.976614: step 478, loss 0.274021, acc 0.92
2016-09-05T19:06:39.783075: step 479, loss 0.356803, acc 0.84
2016-09-05T19:06:40.546663: step 480, loss 0.155911, acc 0.94
2016-09-05T19:06:41.361133: step 481, loss 0.154756, acc 0.92
2016-09-05T19:06:42.165278: step 482, loss 0.214285, acc 0.88
2016-09-05T19:06:42.970470: step 483, loss 0.265723, acc 0.88
2016-09-05T19:06:43.774589: step 484, loss 0.104011, acc 1
2016-09-05T19:06:44.582761: step 485, loss 0.325891, acc 0.78
2016-09-05T19:06:45.405771: step 486, loss 0.280239, acc 0.86
2016-09-05T19:06:46.218089: step 487, loss 0.177511, acc 0.92
2016-09-05T19:06:47.054741: step 488, loss 0.338495, acc 0.84
2016-09-05T19:06:47.851350: step 489, loss 0.396025, acc 0.86
2016-09-05T19:06:48.651105: step 490, loss 0.265749, acc 0.9
2016-09-05T19:06:49.456442: step 491, loss 0.210065, acc 0.9
2016-09-05T19:06:50.251551: step 492, loss 0.185711, acc 0.92
2016-09-05T19:06:51.032768: step 493, loss 0.151188, acc 0.94
2016-09-05T19:06:51.817088: step 494, loss 0.264402, acc 0.88
2016-09-05T19:06:52.610416: step 495, loss 0.324001, acc 0.88
2016-09-05T19:06:53.431459: step 496, loss 0.245914, acc 0.88
2016-09-05T19:06:54.233274: step 497, loss 0.213823, acc 0.9
2016-09-05T19:06:55.025492: step 498, loss 0.336826, acc 0.84
2016-09-05T19:06:55.860729: step 499, loss 0.280818, acc 0.88
2016-09-05T19:06:56.662006: step 500, loss 0.235365, acc 0.88

Evaluation:
2016-09-05T19:07:00.194289: step 500, loss 0.492673, acc 0.789

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-500

2016-09-05T19:07:02.111127: step 501, loss 0.312468, acc 0.86
2016-09-05T19:07:02.939814: step 502, loss 0.264509, acc 0.84
2016-09-05T19:07:03.736927: step 503, loss 0.230813, acc 0.86
2016-09-05T19:07:04.560574: step 504, loss 0.254849, acc 0.9
2016-09-05T19:07:05.385506: step 505, loss 0.287934, acc 0.86
2016-09-05T19:07:06.185043: step 506, loss 0.251459, acc 0.9
2016-09-05T19:07:06.993644: step 507, loss 0.211125, acc 0.94
2016-09-05T19:07:07.811073: step 508, loss 0.169543, acc 0.96
2016-09-05T19:07:08.644333: step 509, loss 0.189352, acc 0.94
2016-09-05T19:07:09.441416: step 510, loss 0.407845, acc 0.8
2016-09-05T19:07:10.283267: step 511, loss 0.175215, acc 0.88
2016-09-05T19:07:11.095687: step 512, loss 0.191286, acc 0.94
2016-09-05T19:07:11.914684: step 513, loss 0.210608, acc 0.94
2016-09-05T19:07:12.789357: step 514, loss 0.335366, acc 0.8
2016-09-05T19:07:13.669006: step 515, loss 0.182908, acc 0.92
2016-09-05T19:07:14.450323: step 516, loss 0.265532, acc 0.86
2016-09-05T19:07:15.273416: step 517, loss 0.191429, acc 0.92
2016-09-05T19:07:16.106000: step 518, loss 0.352104, acc 0.88
2016-09-05T19:07:16.895818: step 519, loss 0.374726, acc 0.82
2016-09-05T19:07:17.707761: step 520, loss 0.307782, acc 0.88
2016-09-05T19:07:18.517290: step 521, loss 0.239321, acc 0.9
2016-09-05T19:07:19.321125: step 522, loss 0.189047, acc 0.92
2016-09-05T19:07:20.108392: step 523, loss 0.321933, acc 0.84
2016-09-05T19:07:20.928146: step 524, loss 0.339054, acc 0.82
2016-09-05T19:07:21.693941: step 525, loss 0.347833, acc 0.88
2016-09-05T19:07:22.463577: step 526, loss 0.300101, acc 0.86
2016-09-05T19:07:23.285982: step 527, loss 0.254238, acc 0.9
2016-09-05T19:07:24.093142: step 528, loss 0.222598, acc 0.94
2016-09-05T19:07:24.904425: step 529, loss 0.349594, acc 0.84
2016-09-05T19:07:25.726140: step 530, loss 0.1949, acc 0.9
2016-09-05T19:07:26.516095: step 531, loss 0.252169, acc 0.92
2016-09-05T19:07:27.302697: step 532, loss 0.268399, acc 0.86
2016-09-05T19:07:28.106296: step 533, loss 0.297493, acc 0.84
2016-09-05T19:07:28.898392: step 534, loss 0.32768, acc 0.84
2016-09-05T19:07:29.781457: step 535, loss 0.260362, acc 0.82
2016-09-05T19:07:30.609293: step 536, loss 0.22039, acc 0.92
2016-09-05T19:07:31.399069: step 537, loss 0.199484, acc 0.94
2016-09-05T19:07:32.203078: step 538, loss 0.348684, acc 0.86
2016-09-05T19:07:33.028121: step 539, loss 0.243028, acc 0.86
2016-09-05T19:07:33.806922: step 540, loss 0.236379, acc 0.92
2016-09-05T19:07:34.577223: step 541, loss 0.412773, acc 0.86
2016-09-05T19:07:35.397521: step 542, loss 0.303212, acc 0.82
2016-09-05T19:07:36.227248: step 543, loss 0.276449, acc 0.86
2016-09-05T19:07:37.013832: step 544, loss 0.206735, acc 0.94
2016-09-05T19:07:37.821437: step 545, loss 0.191849, acc 0.94
2016-09-05T19:07:38.614374: step 546, loss 0.245288, acc 0.9
2016-09-05T19:07:39.414215: step 547, loss 0.242782, acc 0.9
2016-09-05T19:07:40.233474: step 548, loss 0.333751, acc 0.88
2016-09-05T19:07:41.020895: step 549, loss 0.273483, acc 0.86
2016-09-05T19:07:41.827408: step 550, loss 0.220253, acc 0.88
2016-09-05T19:07:42.649858: step 551, loss 0.2441, acc 0.86
2016-09-05T19:07:43.457360: step 552, loss 0.297881, acc 0.86
2016-09-05T19:07:44.241639: step 553, loss 0.228135, acc 0.9
2016-09-05T19:07:45.060414: step 554, loss 0.12492, acc 0.98
2016-09-05T19:07:45.831942: step 555, loss 0.2251, acc 0.92
2016-09-05T19:07:46.635484: step 556, loss 0.221262, acc 0.9
2016-09-05T19:07:47.469443: step 557, loss 0.332491, acc 0.84
2016-09-05T19:07:48.263167: step 558, loss 0.36092, acc 0.86
2016-09-05T19:07:49.088686: step 559, loss 0.234332, acc 0.9
2016-09-05T19:07:49.909929: step 560, loss 0.320414, acc 0.86
2016-09-05T19:07:50.708875: step 561, loss 0.213417, acc 0.94
2016-09-05T19:07:51.490561: step 562, loss 0.139069, acc 0.96
2016-09-05T19:07:52.328150: step 563, loss 0.357959, acc 0.86
2016-09-05T19:07:53.129727: step 564, loss 0.342514, acc 0.82
2016-09-05T19:07:53.921446: step 565, loss 0.170509, acc 0.92
2016-09-05T19:07:54.726318: step 566, loss 0.467303, acc 0.76
2016-09-05T19:07:55.504558: step 567, loss 0.294452, acc 0.9
2016-09-05T19:07:56.328451: step 568, loss 0.205299, acc 0.94
2016-09-05T19:07:57.164179: step 569, loss 0.318502, acc 0.88
2016-09-05T19:07:57.956181: step 570, loss 0.209107, acc 0.92
2016-09-05T19:07:58.745464: step 571, loss 0.182379, acc 0.96
2016-09-05T19:07:59.549924: step 572, loss 0.319091, acc 0.88
2016-09-05T19:08:00.335977: step 573, loss 0.324938, acc 0.9
2016-09-05T19:08:01.134544: step 574, loss 0.198129, acc 0.9
2016-09-05T19:08:01.952255: step 575, loss 0.257817, acc 0.86
2016-09-05T19:08:02.735255: step 576, loss 0.231762, acc 0.92
2016-09-05T19:08:03.528815: step 577, loss 0.303609, acc 0.86
2016-09-05T19:08:04.362156: step 578, loss 0.358884, acc 0.84
2016-09-05T19:08:05.147450: step 579, loss 0.228812, acc 0.92
2016-09-05T19:08:05.952546: step 580, loss 0.233177, acc 0.86
2016-09-05T19:08:06.764271: step 581, loss 0.223785, acc 0.9
2016-09-05T19:08:07.171410: step 582, loss 0.297666, acc 0.833333
2016-09-05T19:08:07.981582: step 583, loss 0.100492, acc 0.98
2016-09-05T19:08:08.771030: step 584, loss 0.179677, acc 0.9
2016-09-05T19:08:09.568746: step 585, loss 0.334223, acc 0.88
2016-09-05T19:08:10.363662: step 586, loss 0.142437, acc 0.92
2016-09-05T19:08:11.152857: step 587, loss 0.152457, acc 0.94
2016-09-05T19:08:11.979651: step 588, loss 0.228997, acc 0.9
2016-09-05T19:08:12.782902: step 589, loss 0.1246, acc 0.94
2016-09-05T19:08:13.582644: step 590, loss 0.16111, acc 0.94
2016-09-05T19:08:14.389447: step 591, loss 0.0868259, acc 0.96
2016-09-05T19:08:15.196468: step 592, loss 0.278383, acc 0.9
2016-09-05T19:08:16.009880: step 593, loss 0.111901, acc 0.98
2016-09-05T19:08:16.808660: step 594, loss 0.052951, acc 1
2016-09-05T19:08:17.624369: step 595, loss 0.286617, acc 0.9
2016-09-05T19:08:18.408355: step 596, loss 0.107343, acc 0.94
2016-09-05T19:08:19.215467: step 597, loss 0.210437, acc 0.9
2016-09-05T19:08:20.024006: step 598, loss 0.273114, acc 0.9
2016-09-05T19:08:20.825549: step 599, loss 0.240502, acc 0.86
2016-09-05T19:08:21.678097: step 600, loss 0.0745954, acc 0.96

Evaluation:
2016-09-05T19:08:25.165313: step 600, loss 0.575998, acc 0.787

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-600

2016-09-05T19:08:27.042113: step 601, loss 0.0980983, acc 0.94
2016-09-05T19:08:27.858953: step 602, loss 0.173937, acc 0.94
2016-09-05T19:08:28.683521: step 603, loss 0.106815, acc 0.98
2016-09-05T19:08:29.485043: step 604, loss 0.28421, acc 0.88
2016-09-05T19:08:30.309057: step 605, loss 0.283831, acc 0.92
2016-09-05T19:08:31.149040: step 606, loss 0.153703, acc 0.96
2016-09-05T19:08:31.970378: step 607, loss 0.153972, acc 0.92
2016-09-05T19:08:32.792913: step 608, loss 0.105593, acc 0.98
2016-09-05T19:08:33.633991: step 609, loss 0.180463, acc 0.92
2016-09-05T19:08:34.435713: step 610, loss 0.0829989, acc 0.98
2016-09-05T19:08:35.219277: step 611, loss 0.180688, acc 0.96
2016-09-05T19:08:36.059685: step 612, loss 0.062989, acc 1
2016-09-05T19:08:36.867255: step 613, loss 0.159764, acc 0.9
2016-09-05T19:08:37.653196: step 614, loss 0.255055, acc 0.9
2016-09-05T19:08:38.482724: step 615, loss 0.12838, acc 0.94
2016-09-05T19:08:39.289804: step 616, loss 0.181316, acc 0.94
2016-09-05T19:08:40.098594: step 617, loss 0.0700169, acc 0.98
2016-09-05T19:08:40.939906: step 618, loss 0.25421, acc 0.9
2016-09-05T19:08:41.753514: step 619, loss 0.124149, acc 0.92
2016-09-05T19:08:42.568041: step 620, loss 0.127227, acc 0.94
2016-09-05T19:08:43.405327: step 621, loss 0.0970107, acc 0.98
2016-09-05T19:08:44.223455: step 622, loss 0.0888429, acc 0.94
2016-09-05T19:08:45.037415: step 623, loss 0.173186, acc 0.96
2016-09-05T19:08:45.887428: step 624, loss 0.203175, acc 0.92
2016-09-05T19:08:46.728438: step 625, loss 0.0425042, acc 1
2016-09-05T19:08:47.492044: step 626, loss 0.352485, acc 0.88
2016-09-05T19:08:48.325479: step 627, loss 0.245572, acc 0.88
2016-09-05T19:08:49.147925: step 628, loss 0.158555, acc 0.92
2016-09-05T19:08:49.908342: step 629, loss 0.270763, acc 0.86
2016-09-05T19:08:50.702163: step 630, loss 0.164138, acc 0.96
2016-09-05T19:08:51.541386: step 631, loss 0.168907, acc 0.92
2016-09-05T19:08:52.290292: step 632, loss 0.192387, acc 0.9
2016-09-05T19:08:53.104878: step 633, loss 0.116668, acc 0.98
2016-09-05T19:08:53.919900: step 634, loss 0.18605, acc 0.92
2016-09-05T19:08:54.728778: step 635, loss 0.152896, acc 0.94
2016-09-05T19:08:55.528635: step 636, loss 0.355068, acc 0.84
2016-09-05T19:08:56.357904: step 637, loss 0.245887, acc 0.94
2016-09-05T19:08:57.159447: step 638, loss 0.177814, acc 0.92
2016-09-05T19:08:57.972966: step 639, loss 0.193156, acc 0.9
2016-09-05T19:08:58.808381: step 640, loss 0.203438, acc 0.96
2016-09-05T19:08:59.597481: step 641, loss 0.204596, acc 0.92
2016-09-05T19:09:00.439224: step 642, loss 0.183816, acc 0.94
2016-09-05T19:09:01.299834: step 643, loss 0.203025, acc 0.88
2016-09-05T19:09:02.136686: step 644, loss 0.143955, acc 0.92
2016-09-05T19:09:02.934908: step 645, loss 0.181687, acc 0.92
2016-09-05T19:09:03.756733: step 646, loss 0.328205, acc 0.92
2016-09-05T19:09:04.579923: step 647, loss 0.120104, acc 0.94
2016-09-05T19:09:05.390755: step 648, loss 0.224574, acc 0.9
2016-09-05T19:09:06.207197: step 649, loss 0.122885, acc 0.96
2016-09-05T19:09:07.025436: step 650, loss 0.136122, acc 0.94
2016-09-05T19:09:07.829221: step 651, loss 0.146308, acc 0.96
2016-09-05T19:09:08.667735: step 652, loss 0.116211, acc 0.96
2016-09-05T19:09:09.456793: step 653, loss 0.181598, acc 0.9
2016-09-05T19:09:10.277529: step 654, loss 0.260494, acc 0.88
2016-09-05T19:09:11.104156: step 655, loss 0.223372, acc 0.92
2016-09-05T19:09:11.914232: step 656, loss 0.221093, acc 0.92
2016-09-05T19:09:12.724801: step 657, loss 0.21395, acc 0.9
2016-09-05T19:09:13.554429: step 658, loss 0.267858, acc 0.9
2016-09-05T19:09:14.337629: step 659, loss 0.281877, acc 0.9
2016-09-05T19:09:15.132314: step 660, loss 0.138214, acc 0.98
2016-09-05T19:09:15.951343: step 661, loss 0.220359, acc 0.88
2016-09-05T19:09:16.774404: step 662, loss 0.233927, acc 0.88
2016-09-05T19:09:17.589129: step 663, loss 0.223051, acc 0.94
2016-09-05T19:09:18.394830: step 664, loss 0.21974, acc 0.86
2016-09-05T19:09:19.202662: step 665, loss 0.192579, acc 0.92
2016-09-05T19:09:20.021985: step 666, loss 0.136805, acc 0.96
2016-09-05T19:09:20.845034: step 667, loss 0.104134, acc 0.96
2016-09-05T19:09:21.678778: step 668, loss 0.217417, acc 0.9
2016-09-05T19:09:22.472039: step 669, loss 0.254607, acc 0.9
2016-09-05T19:09:23.295819: step 670, loss 0.067506, acc 1
2016-09-05T19:09:24.095670: step 671, loss 0.0679923, acc 0.98
2016-09-05T19:09:24.900524: step 672, loss 0.196816, acc 0.88
2016-09-05T19:09:25.708851: step 673, loss 0.14792, acc 0.96
2016-09-05T19:09:26.510265: step 674, loss 0.192298, acc 0.94
2016-09-05T19:09:27.328403: step 675, loss 0.218678, acc 0.9
2016-09-05T19:09:28.173565: step 676, loss 0.153474, acc 0.94
2016-09-05T19:09:28.995221: step 677, loss 0.146963, acc 0.88
2016-09-05T19:09:29.773419: step 678, loss 0.198973, acc 0.9
2016-09-05T19:09:30.565530: step 679, loss 0.116048, acc 0.96
2016-09-05T19:09:31.373394: step 680, loss 0.127676, acc 0.94
2016-09-05T19:09:32.165125: step 681, loss 0.0979209, acc 0.94
2016-09-05T19:09:32.983587: step 682, loss 0.204253, acc 0.92
2016-09-05T19:09:33.777645: step 683, loss 0.232595, acc 0.86
2016-09-05T19:09:34.572137: step 684, loss 0.260918, acc 0.9
2016-09-05T19:09:35.382942: step 685, loss 0.160987, acc 0.94
2016-09-05T19:09:36.184808: step 686, loss 0.209027, acc 0.92
2016-09-05T19:09:36.967858: step 687, loss 0.109306, acc 0.92
2016-09-05T19:09:37.778571: step 688, loss 0.135006, acc 0.96
2016-09-05T19:09:38.562993: step 689, loss 0.143781, acc 0.9
2016-09-05T19:09:39.387874: step 690, loss 0.227927, acc 0.88
2016-09-05T19:09:40.229091: step 691, loss 0.429321, acc 0.84
2016-09-05T19:09:41.033716: step 692, loss 0.165373, acc 0.92
2016-09-05T19:09:41.837889: step 693, loss 0.199126, acc 0.9
2016-09-05T19:09:42.668928: step 694, loss 0.0638189, acc 0.98
2016-09-05T19:09:43.489143: step 695, loss 0.150071, acc 0.94
2016-09-05T19:09:44.293378: step 696, loss 0.270941, acc 0.88
2016-09-05T19:09:45.090903: step 697, loss 0.151679, acc 0.9
2016-09-05T19:09:45.920028: step 698, loss 0.195815, acc 0.92
2016-09-05T19:09:46.713655: step 699, loss 0.201583, acc 0.9
2016-09-05T19:09:47.507673: step 700, loss 0.210553, acc 0.9

Evaluation:
2016-09-05T19:09:51.009394: step 700, loss 0.597094, acc 0.792

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-700

2016-09-05T19:09:52.821674: step 701, loss 0.237935, acc 0.9
2016-09-05T19:09:53.608742: step 702, loss 0.168208, acc 0.92
2016-09-05T19:09:54.428068: step 703, loss 0.085948, acc 0.96
2016-09-05T19:09:55.205657: step 704, loss 0.195482, acc 0.9
2016-09-05T19:09:56.021310: step 705, loss 0.170076, acc 0.94
2016-09-05T19:09:56.848992: step 706, loss 0.144909, acc 0.92
2016-09-05T19:09:57.612752: step 707, loss 0.147355, acc 0.94
2016-09-05T19:09:58.433004: step 708, loss 0.145619, acc 0.94
2016-09-05T19:09:59.223573: step 709, loss 0.211255, acc 0.92
2016-09-05T19:09:59.995619: step 710, loss 0.0838954, acc 0.96
2016-09-05T19:10:00.818227: step 711, loss 0.153137, acc 0.92
2016-09-05T19:10:01.639590: step 712, loss 0.109021, acc 0.94
2016-09-05T19:10:02.423030: step 713, loss 0.116399, acc 0.96
2016-09-05T19:10:03.256804: step 714, loss 0.162156, acc 0.94
2016-09-05T19:10:04.069835: step 715, loss 0.0971105, acc 0.96
2016-09-05T19:10:04.900388: step 716, loss 0.249599, acc 0.9
2016-09-05T19:10:05.724992: step 717, loss 0.22142, acc 0.88
2016-09-05T19:10:06.553215: step 718, loss 0.176163, acc 0.94
2016-09-05T19:10:07.348245: step 719, loss 0.203065, acc 0.88
2016-09-05T19:10:08.139463: step 720, loss 0.109847, acc 0.96
2016-09-05T19:10:08.948266: step 721, loss 0.161192, acc 0.88
2016-09-05T19:10:09.721269: step 722, loss 0.339017, acc 0.88
2016-09-05T19:10:10.542986: step 723, loss 0.138541, acc 0.94
2016-09-05T19:10:11.363028: step 724, loss 0.119086, acc 0.94
2016-09-05T19:10:12.146659: step 725, loss 0.146527, acc 0.94
2016-09-05T19:10:12.958123: step 726, loss 0.110823, acc 0.94
2016-09-05T19:10:13.788705: step 727, loss 0.142876, acc 0.96
2016-09-05T19:10:14.557617: step 728, loss 0.289375, acc 0.88
2016-09-05T19:10:15.338729: step 729, loss 0.176709, acc 0.92
2016-09-05T19:10:16.168444: step 730, loss 0.142734, acc 0.92
2016-09-05T19:10:16.959575: step 731, loss 0.0963426, acc 0.98
2016-09-05T19:10:17.756783: step 732, loss 0.286139, acc 0.92
2016-09-05T19:10:18.578570: step 733, loss 0.18055, acc 0.92
2016-09-05T19:10:19.341988: step 734, loss 0.186762, acc 0.92
2016-09-05T19:10:20.171139: step 735, loss 0.140898, acc 0.94
2016-09-05T19:10:20.988249: step 736, loss 0.425299, acc 0.78
2016-09-05T19:10:21.767122: step 737, loss 0.153405, acc 0.96
2016-09-05T19:10:22.566421: step 738, loss 0.146102, acc 0.96
2016-09-05T19:10:23.399459: step 739, loss 0.183914, acc 0.92
2016-09-05T19:10:24.185396: step 740, loss 0.195444, acc 0.92
2016-09-05T19:10:24.995227: step 741, loss 0.134522, acc 0.92
2016-09-05T19:10:25.793957: step 742, loss 0.15353, acc 0.92
2016-09-05T19:10:26.597421: step 743, loss 0.159621, acc 0.94
2016-09-05T19:10:27.410579: step 744, loss 0.243832, acc 0.9
2016-09-05T19:10:28.235082: step 745, loss 0.194548, acc 0.92
2016-09-05T19:10:29.004485: step 746, loss 0.140792, acc 0.94
2016-09-05T19:10:29.840442: step 747, loss 0.351046, acc 0.88
2016-09-05T19:10:30.650143: step 748, loss 0.16111, acc 0.9
2016-09-05T19:10:31.440617: step 749, loss 0.104006, acc 0.98
2016-09-05T19:10:32.227279: step 750, loss 0.164467, acc 0.94
2016-09-05T19:10:33.029306: step 751, loss 0.292452, acc 0.84
2016-09-05T19:10:33.817069: step 752, loss 0.210286, acc 0.92
2016-09-05T19:10:34.648130: step 753, loss 0.249157, acc 0.94
2016-09-05T19:10:35.470411: step 754, loss 0.293525, acc 0.9
2016-09-05T19:10:36.256136: step 755, loss 0.172999, acc 0.92
2016-09-05T19:10:37.066276: step 756, loss 0.141793, acc 0.94
2016-09-05T19:10:37.888860: step 757, loss 0.281347, acc 0.9
2016-09-05T19:10:38.683451: step 758, loss 0.123007, acc 0.96
2016-09-05T19:10:39.497663: step 759, loss 0.284351, acc 0.86
2016-09-05T19:10:40.320576: step 760, loss 0.184424, acc 0.9
2016-09-05T19:10:41.088260: step 761, loss 0.268158, acc 0.86
2016-09-05T19:10:41.931702: step 762, loss 0.273901, acc 0.86
2016-09-05T19:10:42.760545: step 763, loss 0.103956, acc 0.96
2016-09-05T19:10:43.549135: step 764, loss 0.0812008, acc 1
2016-09-05T19:10:44.344871: step 765, loss 0.137767, acc 0.98
2016-09-05T19:10:45.161821: step 766, loss 0.166214, acc 0.9
2016-09-05T19:10:45.979741: step 767, loss 0.390437, acc 0.84
2016-09-05T19:10:46.805911: step 768, loss 0.192448, acc 0.92
2016-09-05T19:10:47.639019: step 769, loss 0.296543, acc 0.86
2016-09-05T19:10:48.405394: step 770, loss 0.147323, acc 0.92
2016-09-05T19:10:49.182457: step 771, loss 0.226836, acc 0.94
2016-09-05T19:10:50.005002: step 772, loss 0.105129, acc 0.94
2016-09-05T19:10:50.794912: step 773, loss 0.197981, acc 0.94
2016-09-05T19:10:51.593689: step 774, loss 0.24051, acc 0.9
2016-09-05T19:10:52.411168: step 775, loss 0.184975, acc 0.92
2016-09-05T19:10:52.819914: step 776, loss 0.126804, acc 0.916667
2016-09-05T19:10:53.642157: step 777, loss 0.105761, acc 0.96
2016-09-05T19:10:54.440429: step 778, loss 0.16124, acc 0.9
2016-09-05T19:10:55.234378: step 779, loss 0.211435, acc 0.9
2016-09-05T19:10:56.058907: step 780, loss 0.080857, acc 1
2016-09-05T19:10:56.858624: step 781, loss 0.0910715, acc 0.98
2016-09-05T19:10:57.662707: step 782, loss 0.105944, acc 0.98
2016-09-05T19:10:58.491353: step 783, loss 0.149562, acc 0.96
2016-09-05T19:10:59.264619: step 784, loss 0.284132, acc 0.9
2016-09-05T19:11:00.070307: step 785, loss 0.210385, acc 0.94
2016-09-05T19:11:00.931615: step 786, loss 0.0881158, acc 0.98
2016-09-05T19:11:01.723410: step 787, loss 0.0749533, acc 0.98
2016-09-05T19:11:02.505806: step 788, loss 0.22443, acc 0.92
2016-09-05T19:11:03.297159: step 789, loss 0.039898, acc 0.98
2016-09-05T19:11:04.053333: step 790, loss 0.0673383, acc 0.98
2016-09-05T19:11:04.874763: step 791, loss 0.138356, acc 0.96
2016-09-05T19:11:05.686563: step 792, loss 0.0927828, acc 0.98
2016-09-05T19:11:06.486196: step 793, loss 0.0972884, acc 0.94
2016-09-05T19:11:07.312659: step 794, loss 0.119207, acc 0.96
2016-09-05T19:11:08.132428: step 795, loss 0.112219, acc 0.96
2016-09-05T19:11:08.924998: step 796, loss 0.140532, acc 0.94
2016-09-05T19:11:09.720532: step 797, loss 0.127373, acc 0.96
2016-09-05T19:11:10.554490: step 798, loss 0.0827299, acc 0.96
2016-09-05T19:11:11.342550: step 799, loss 0.127578, acc 0.96
2016-09-05T19:11:12.130419: step 800, loss 0.109408, acc 0.94

Evaluation:
2016-09-05T19:11:15.662380: step 800, loss 0.770487, acc 0.784

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-800

2016-09-05T19:11:17.533458: step 801, loss 0.0609062, acc 1
2016-09-05T19:11:18.342720: step 802, loss 0.052173, acc 0.96
2016-09-05T19:11:19.166633: step 803, loss 0.0892495, acc 0.98
2016-09-05T19:11:19.996779: step 804, loss 0.0689261, acc 0.96
2016-09-05T19:11:20.798133: step 805, loss 0.116551, acc 0.94
2016-09-05T19:11:21.634392: step 806, loss 0.124205, acc 0.92
2016-09-05T19:11:22.445273: step 807, loss 0.0826214, acc 0.96
2016-09-05T19:11:23.277890: step 808, loss 0.118202, acc 0.92
2016-09-05T19:11:24.097730: step 809, loss 0.112115, acc 0.96
2016-09-05T19:11:24.917684: step 810, loss 0.0663749, acc 0.98
2016-09-05T19:11:25.719841: step 811, loss 0.156373, acc 0.92
2016-09-05T19:11:26.597817: step 812, loss 0.138338, acc 0.96
2016-09-05T19:11:27.412749: step 813, loss 0.129037, acc 0.94
2016-09-05T19:11:28.229474: step 814, loss 0.100869, acc 0.96
2016-09-05T19:11:29.045297: step 815, loss 0.34561, acc 0.92
2016-09-05T19:11:29.883770: step 816, loss 0.0551546, acc 0.98
2016-09-05T19:11:30.682262: step 817, loss 0.33656, acc 0.84
2016-09-05T19:11:31.477890: step 818, loss 0.139545, acc 0.94
2016-09-05T19:11:32.318499: step 819, loss 0.101045, acc 0.96
2016-09-05T19:11:33.109944: step 820, loss 0.119118, acc 0.96
2016-09-05T19:11:33.903591: step 821, loss 0.0915828, acc 0.96
2016-09-05T19:11:34.734235: step 822, loss 0.169786, acc 0.9
2016-09-05T19:11:35.534268: step 823, loss 0.169856, acc 0.94
2016-09-05T19:11:36.341219: step 824, loss 0.127584, acc 0.94
2016-09-05T19:11:37.151853: step 825, loss 0.179041, acc 0.94
2016-09-05T19:11:37.942552: step 826, loss 0.200154, acc 0.92
2016-09-05T19:11:38.733045: step 827, loss 0.174561, acc 0.94
2016-09-05T19:11:39.533502: step 828, loss 0.115691, acc 0.98
2016-09-05T19:11:40.323819: step 829, loss 0.117854, acc 0.96
2016-09-05T19:11:41.129201: step 830, loss 0.0873046, acc 0.96
2016-09-05T19:11:41.928217: step 831, loss 0.159364, acc 0.92
2016-09-05T19:11:42.733372: step 832, loss 0.159664, acc 0.94
2016-09-05T19:11:43.540190: step 833, loss 0.200297, acc 0.92
2016-09-05T19:11:44.347742: step 834, loss 0.0951532, acc 0.98
2016-09-05T19:11:45.169516: step 835, loss 0.0704037, acc 0.98
2016-09-05T19:11:45.989809: step 836, loss 0.0291124, acc 1
2016-09-05T19:11:46.802611: step 837, loss 0.108925, acc 0.98
2016-09-05T19:11:47.588952: step 838, loss 0.0736766, acc 0.96
2016-09-05T19:11:48.392919: step 839, loss 0.129251, acc 0.94
2016-09-05T19:11:49.214949: step 840, loss 0.218648, acc 0.92
2016-09-05T19:11:49.995612: step 841, loss 0.149322, acc 0.94
2016-09-05T19:11:50.781859: step 842, loss 0.187632, acc 0.94
2016-09-05T19:11:51.579225: step 843, loss 0.0941577, acc 0.96
2016-09-05T19:11:52.375361: step 844, loss 0.197652, acc 0.94
2016-09-05T19:11:53.207766: step 845, loss 0.173117, acc 0.92
2016-09-05T19:11:54.029455: step 846, loss 0.180857, acc 0.92
2016-09-05T19:11:54.854106: step 847, loss 0.0810079, acc 1
2016-09-05T19:11:55.645180: step 848, loss 0.138801, acc 0.98
2016-09-05T19:11:56.473669: step 849, loss 0.123534, acc 0.94
2016-09-05T19:11:57.277690: step 850, loss 0.135793, acc 0.96
2016-09-05T19:11:58.091704: step 851, loss 0.142339, acc 0.96
2016-09-05T19:11:58.905404: step 852, loss 0.149338, acc 0.96
2016-09-05T19:11:59.698795: step 853, loss 0.139792, acc 0.96
2016-09-05T19:12:00.516198: step 854, loss 0.175617, acc 0.94
2016-09-05T19:12:01.325049: step 855, loss 0.157247, acc 0.94
2016-09-05T19:12:02.095115: step 856, loss 0.133258, acc 0.94
2016-09-05T19:12:02.897581: step 857, loss 0.0540984, acc 0.98
2016-09-05T19:12:03.714480: step 858, loss 0.100784, acc 0.96
2016-09-05T19:12:04.503389: step 859, loss 0.165781, acc 0.92
2016-09-05T19:12:05.300953: step 860, loss 0.271462, acc 0.9
2016-09-05T19:12:06.099461: step 861, loss 0.0785489, acc 0.94
2016-09-05T19:12:06.913471: step 862, loss 0.225703, acc 0.92
2016-09-05T19:12:07.736356: step 863, loss 0.0610755, acc 0.98
2016-09-05T19:12:08.529880: step 864, loss 0.0990467, acc 0.98
2016-09-05T19:12:09.310196: step 865, loss 0.111678, acc 0.94
2016-09-05T19:12:10.123542: step 866, loss 0.143758, acc 0.92
2016-09-05T19:12:10.927133: step 867, loss 0.165531, acc 0.92
2016-09-05T19:12:11.752290: step 868, loss 0.114981, acc 0.94
2016-09-05T19:12:12.557758: step 869, loss 0.107756, acc 0.96
2016-09-05T19:12:13.371072: step 870, loss 0.150506, acc 0.94
2016-09-05T19:12:14.159938: step 871, loss 0.184553, acc 0.92
2016-09-05T19:12:14.965117: step 872, loss 0.129589, acc 0.94
2016-09-05T19:12:15.777359: step 873, loss 0.111027, acc 0.98
2016-09-05T19:12:16.583229: step 874, loss 0.165869, acc 0.96
2016-09-05T19:12:17.392315: step 875, loss 0.116237, acc 0.96
2016-09-05T19:12:18.202502: step 876, loss 0.263409, acc 0.88
2016-09-05T19:12:18.985407: step 877, loss 0.124914, acc 0.96
2016-09-05T19:12:19.816592: step 878, loss 0.0753656, acc 0.96
2016-09-05T19:12:20.677242: step 879, loss 0.12161, acc 0.94
2016-09-05T19:12:21.488494: step 880, loss 0.203101, acc 0.88
2016-09-05T19:12:22.275922: step 881, loss 0.143948, acc 0.94
2016-09-05T19:12:23.089808: step 882, loss 0.0537808, acc 1
2016-09-05T19:12:23.867073: step 883, loss 0.14433, acc 0.96
2016-09-05T19:12:24.670360: step 884, loss 0.129231, acc 0.96
2016-09-05T19:12:25.502506: step 885, loss 0.150872, acc 0.92
2016-09-05T19:12:26.313666: step 886, loss 0.118698, acc 0.98
2016-09-05T19:12:27.107667: step 887, loss 0.179347, acc 0.88
2016-09-05T19:12:27.919704: step 888, loss 0.120864, acc 0.92
2016-09-05T19:12:28.697953: step 889, loss 0.153653, acc 0.94
2016-09-05T19:12:29.488641: step 890, loss 0.103165, acc 0.94
2016-09-05T19:12:30.309562: step 891, loss 0.187553, acc 0.96
2016-09-05T19:12:31.107430: step 892, loss 0.154521, acc 0.92
2016-09-05T19:12:31.933225: step 893, loss 0.0289933, acc 1
2016-09-05T19:12:32.781970: step 894, loss 0.13011, acc 0.94
2016-09-05T19:12:33.583254: step 895, loss 0.178574, acc 0.94
2016-09-05T19:12:34.378315: step 896, loss 0.147476, acc 0.98
2016-09-05T19:12:35.189993: step 897, loss 0.047044, acc 0.98
2016-09-05T19:12:35.982749: step 898, loss 0.119495, acc 0.94
2016-09-05T19:12:36.786718: step 899, loss 0.0809009, acc 0.98
2016-09-05T19:12:37.610534: step 900, loss 0.0992134, acc 0.94

Evaluation:
2016-09-05T19:12:41.125470: step 900, loss 0.780095, acc 0.771

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-900

2016-09-05T19:12:42.977585: step 901, loss 0.129835, acc 0.92
2016-09-05T19:12:43.806464: step 902, loss 0.217404, acc 0.92
2016-09-05T19:12:44.625200: step 903, loss 0.157621, acc 0.94
2016-09-05T19:12:45.474917: step 904, loss 0.0832465, acc 0.96
2016-09-05T19:12:46.305278: step 905, loss 0.0766701, acc 0.98
2016-09-05T19:12:47.108612: step 906, loss 0.187632, acc 0.94
2016-09-05T19:12:47.912280: step 907, loss 0.0776112, acc 0.96
2016-09-05T19:12:48.745752: step 908, loss 0.0593473, acc 0.96
2016-09-05T19:12:49.545122: step 909, loss 0.102426, acc 0.94
2016-09-05T19:12:50.374876: step 910, loss 0.149611, acc 0.96
2016-09-05T19:12:51.218502: step 911, loss 0.169333, acc 0.92
2016-09-05T19:12:52.010643: step 912, loss 0.116966, acc 0.96
2016-09-05T19:12:52.824478: step 913, loss 0.187762, acc 0.92
2016-09-05T19:12:53.655414: step 914, loss 0.0938462, acc 0.96
2016-09-05T19:12:54.484161: step 915, loss 0.178208, acc 0.92
2016-09-05T19:12:55.273630: step 916, loss 0.122452, acc 0.94
2016-09-05T19:12:56.072003: step 917, loss 0.0430493, acc 1
2016-09-05T19:12:56.893519: step 918, loss 0.237523, acc 0.92
2016-09-05T19:12:57.687442: step 919, loss 0.121583, acc 0.98
2016-09-05T19:12:58.485895: step 920, loss 0.192784, acc 0.9
2016-09-05T19:12:59.270871: step 921, loss 0.0655236, acc 0.96
2016-09-05T19:13:00.096093: step 922, loss 0.2456, acc 0.9
2016-09-05T19:13:00.978116: step 923, loss 0.1147, acc 0.94
2016-09-05T19:13:01.804676: step 924, loss 0.0874199, acc 0.96
2016-09-05T19:13:02.602518: step 925, loss 0.0937865, acc 0.96
2016-09-05T19:13:03.372547: step 926, loss 0.210403, acc 0.92
2016-09-05T19:13:04.181533: step 927, loss 0.175509, acc 0.92
2016-09-05T19:13:05.027590: step 928, loss 0.166165, acc 0.94
2016-09-05T19:13:05.830556: step 929, loss 0.181033, acc 0.9
2016-09-05T19:13:06.665529: step 930, loss 0.0982971, acc 0.96
2016-09-05T19:13:07.469358: step 931, loss 0.171533, acc 0.92
2016-09-05T19:13:08.276237: step 932, loss 0.151502, acc 0.94
2016-09-05T19:13:09.084560: step 933, loss 0.0439187, acc 1
2016-09-05T19:13:09.899737: step 934, loss 0.0971195, acc 0.96
2016-09-05T19:13:10.708690: step 935, loss 0.216345, acc 0.92
2016-09-05T19:13:11.518842: step 936, loss 0.0685937, acc 1
2016-09-05T19:13:12.328245: step 937, loss 0.162638, acc 0.88
2016-09-05T19:13:13.123342: step 938, loss 0.173127, acc 0.92
2016-09-05T19:13:13.954794: step 939, loss 0.136956, acc 0.94
2016-09-05T19:13:14.758880: step 940, loss 0.0960627, acc 0.94
2016-09-05T19:13:15.562096: step 941, loss 0.0907305, acc 0.96
2016-09-05T19:13:16.376064: step 942, loss 0.103522, acc 0.98
2016-09-05T19:13:17.162399: step 943, loss 0.201564, acc 0.88
2016-09-05T19:13:17.974895: step 944, loss 0.146223, acc 0.94
2016-09-05T19:13:18.816454: step 945, loss 0.370289, acc 0.78
2016-09-05T19:13:19.628196: step 946, loss 0.21599, acc 0.86
2016-09-05T19:13:20.436650: step 947, loss 0.096891, acc 0.98
2016-09-05T19:13:21.243545: step 948, loss 0.179784, acc 0.92
2016-09-05T19:13:22.054499: step 949, loss 0.104352, acc 0.98
2016-09-05T19:13:22.861618: step 950, loss 0.106428, acc 0.98
2016-09-05T19:13:23.668406: step 951, loss 0.0995227, acc 0.94
2016-09-05T19:13:24.484000: step 952, loss 0.120439, acc 0.94
2016-09-05T19:13:25.281424: step 953, loss 0.0718385, acc 0.96
2016-09-05T19:13:26.111218: step 954, loss 0.098875, acc 0.96
2016-09-05T19:13:26.929495: step 955, loss 0.23975, acc 0.88
2016-09-05T19:13:27.734032: step 956, loss 0.0652214, acc 1
2016-09-05T19:13:28.578465: step 957, loss 0.191232, acc 0.92
2016-09-05T19:13:29.369736: step 958, loss 0.0794639, acc 0.98
2016-09-05T19:13:30.175218: step 959, loss 0.129629, acc 0.92
2016-09-05T19:13:31.020882: step 960, loss 0.0681499, acc 0.96
2016-09-05T19:13:31.818081: step 961, loss 0.172511, acc 0.94
2016-09-05T19:13:32.624598: step 962, loss 0.127965, acc 0.96
2016-09-05T19:13:33.421533: step 963, loss 0.180081, acc 0.9
2016-09-05T19:13:34.200099: step 964, loss 0.0795945, acc 0.96
2016-09-05T19:13:35.053993: step 965, loss 0.0891039, acc 0.96
2016-09-05T19:13:35.868141: step 966, loss 0.169908, acc 0.92
2016-09-05T19:13:36.673219: step 967, loss 0.174727, acc 0.9
2016-09-05T19:13:37.471814: step 968, loss 0.0445325, acc 1
2016-09-05T19:13:38.295438: step 969, loss 0.169053, acc 0.9
2016-09-05T19:13:38.715063: step 970, loss 0.0942919, acc 1
2016-09-05T19:13:39.560669: step 971, loss 0.0699087, acc 0.98
2016-09-05T19:13:40.361262: step 972, loss 0.0794204, acc 0.96
2016-09-05T19:13:41.170278: step 973, loss 0.202531, acc 0.94
2016-09-05T19:13:41.981125: step 974, loss 0.127003, acc 0.98
2016-09-05T19:13:42.799786: step 975, loss 0.0840367, acc 0.96
2016-09-05T19:13:43.577912: step 976, loss 0.120045, acc 0.94
2016-09-05T19:13:44.413727: step 977, loss 0.096333, acc 0.98
2016-09-05T19:13:45.244374: step 978, loss 0.0823542, acc 0.98
2016-09-05T19:13:46.026295: step 979, loss 0.0330533, acc 1
2016-09-05T19:13:46.860768: step 980, loss 0.197079, acc 0.88
2016-09-05T19:13:47.705455: step 981, loss 0.213675, acc 0.9
2016-09-05T19:13:48.525785: step 982, loss 0.180448, acc 0.94
2016-09-05T19:13:49.331115: step 983, loss 0.210605, acc 0.86
2016-09-05T19:13:50.137204: step 984, loss 0.10045, acc 0.92
2016-09-05T19:13:50.937598: step 985, loss 0.0684264, acc 0.98
2016-09-05T19:13:51.711475: step 986, loss 0.162262, acc 0.94
2016-09-05T19:13:52.538520: step 987, loss 0.090843, acc 0.96
2016-09-05T19:13:53.353383: step 988, loss 0.033901, acc 1
2016-09-05T19:13:54.157459: step 989, loss 0.169012, acc 0.9
2016-09-05T19:13:54.972030: step 990, loss 0.0427083, acc 1
2016-09-05T19:13:55.753705: step 991, loss 0.140339, acc 0.94
2016-09-05T19:13:56.546206: step 992, loss 0.164614, acc 0.94
2016-09-05T19:13:57.379371: step 993, loss 0.0506785, acc 1
2016-09-05T19:13:58.146264: step 994, loss 0.189093, acc 0.96
2016-09-05T19:13:58.929779: step 995, loss 0.127958, acc 0.94
2016-09-05T19:13:59.732958: step 996, loss 0.139157, acc 0.94
2016-09-05T19:14:00.547189: step 997, loss 0.0752345, acc 0.94
2016-09-05T19:14:01.337184: step 998, loss 0.121242, acc 0.94
2016-09-05T19:14:02.138822: step 999, loss 0.108097, acc 0.96
2016-09-05T19:14:02.938353: step 1000, loss 0.111369, acc 0.98

Evaluation:
2016-09-05T19:14:06.478675: step 1000, loss 0.731773, acc 0.769

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-1000

2016-09-05T19:14:08.308418: step 1001, loss 0.0774235, acc 0.98
2016-09-05T19:14:09.131829: step 1002, loss 0.10372, acc 0.98
2016-09-05T19:14:09.952896: step 1003, loss 0.111916, acc 0.92
2016-09-05T19:14:10.796139: step 1004, loss 0.116199, acc 0.92
2016-09-05T19:14:11.570383: step 1005, loss 0.0929644, acc 0.96
2016-09-05T19:14:12.382302: step 1006, loss 0.0688774, acc 0.98
2016-09-05T19:14:13.202879: step 1007, loss 0.100001, acc 0.96
2016-09-05T19:14:13.975826: step 1008, loss 0.0772155, acc 0.96
2016-09-05T19:14:14.760097: step 1009, loss 0.0328844, acc 1
2016-09-05T19:14:15.574562: step 1010, loss 0.0608464, acc 0.98
2016-09-05T19:14:16.374182: step 1011, loss 0.0668614, acc 0.98
2016-09-05T19:14:17.167019: step 1012, loss 0.0631404, acc 0.98
2016-09-05T19:14:17.997398: step 1013, loss 0.0222323, acc 1
2016-09-05T19:14:18.795998: step 1014, loss 0.125495, acc 0.96
2016-09-05T19:14:19.604138: step 1015, loss 0.0566243, acc 0.98
2016-09-05T19:14:20.419807: step 1016, loss 0.100946, acc 0.94
2016-09-05T19:14:21.211420: step 1017, loss 0.0810826, acc 0.96
2016-09-05T19:14:22.029522: step 1018, loss 0.107282, acc 0.96
2016-09-05T19:14:22.843698: step 1019, loss 0.0965752, acc 0.98
2016-09-05T19:14:23.630172: step 1020, loss 0.0748765, acc 0.96
2016-09-05T19:14:24.409990: step 1021, loss 0.201721, acc 0.96
2016-09-05T19:14:25.204323: step 1022, loss 0.113048, acc 0.94
2016-09-05T19:14:26.013555: step 1023, loss 0.0805986, acc 0.96
2016-09-05T19:14:26.829301: step 1024, loss 0.104506, acc 0.96
2016-09-05T19:14:27.624015: step 1025, loss 0.202505, acc 0.9
2016-09-05T19:14:28.425537: step 1026, loss 0.176483, acc 0.9
2016-09-05T19:14:29.251212: step 1027, loss 0.0717601, acc 0.94
2016-09-05T19:14:30.080434: step 1028, loss 0.112644, acc 0.94
2016-09-05T19:14:30.889255: step 1029, loss 0.0734704, acc 0.98
2016-09-05T19:14:31.710808: step 1030, loss 0.0511404, acc 1
2016-09-05T19:14:32.537452: step 1031, loss 0.122526, acc 0.94
2016-09-05T19:14:33.328184: step 1032, loss 0.116929, acc 0.94
2016-09-05T19:14:34.149465: step 1033, loss 0.118925, acc 0.92
2016-09-05T19:14:34.947949: step 1034, loss 0.13721, acc 0.98
2016-09-05T19:14:35.707989: step 1035, loss 0.19524, acc 0.88
2016-09-05T19:14:36.512394: step 1036, loss 0.079306, acc 0.98
2016-09-05T19:14:37.330630: step 1037, loss 0.0490751, acc 0.98
2016-09-05T19:14:38.137269: step 1038, loss 0.120585, acc 0.96
2016-09-05T19:14:38.951721: step 1039, loss 0.113739, acc 0.98
2016-09-05T19:14:39.792503: step 1040, loss 0.0727937, acc 0.98
2016-09-05T19:14:40.563868: step 1041, loss 0.0538335, acc 0.98
2016-09-05T19:14:41.368373: step 1042, loss 0.0439094, acc 0.98
2016-09-05T19:14:42.189886: step 1043, loss 0.236922, acc 0.88
2016-09-05T19:14:42.969806: step 1044, loss 0.0154839, acc 1
2016-09-05T19:14:43.765301: step 1045, loss 0.0713404, acc 0.96
2016-09-05T19:14:44.569036: step 1046, loss 0.154666, acc 0.96
2016-09-05T19:14:45.363261: step 1047, loss 0.124312, acc 0.96
2016-09-05T19:14:46.166641: step 1048, loss 0.0874183, acc 0.98
2016-09-05T19:14:46.990440: step 1049, loss 0.113947, acc 0.96
2016-09-05T19:14:47.800356: step 1050, loss 0.128261, acc 0.96
2016-09-05T19:14:48.604517: step 1051, loss 0.0672127, acc 1
2016-09-05T19:14:49.417021: step 1052, loss 0.136272, acc 0.96
2016-09-05T19:14:50.189935: step 1053, loss 0.0480386, acc 1
2016-09-05T19:14:50.998754: step 1054, loss 0.127045, acc 0.94
2016-09-05T19:14:51.840016: step 1055, loss 0.0648083, acc 0.98
2016-09-05T19:14:52.650302: step 1056, loss 0.124283, acc 0.94
2016-09-05T19:14:53.463433: step 1057, loss 0.0947341, acc 0.98
2016-09-05T19:14:54.284364: step 1058, loss 0.0934435, acc 0.98
2016-09-05T19:14:55.070606: step 1059, loss 0.264513, acc 0.88
2016-09-05T19:14:55.917445: step 1060, loss 0.0837481, acc 1
2016-09-05T19:14:56.739295: step 1061, loss 0.065497, acc 0.96
2016-09-05T19:14:57.518481: step 1062, loss 0.077721, acc 0.98
2016-09-05T19:14:58.331044: step 1063, loss 0.163044, acc 0.94
2016-09-05T19:14:59.155290: step 1064, loss 0.0836063, acc 0.98
2016-09-05T19:14:59.987764: step 1065, loss 0.106736, acc 0.96
2016-09-05T19:15:00.820388: step 1066, loss 0.187404, acc 0.92
2016-09-05T19:15:01.667166: step 1067, loss 0.146455, acc 0.96
2016-09-05T19:15:02.476903: step 1068, loss 0.0401597, acc 1
2016-09-05T19:15:03.284567: step 1069, loss 0.0895428, acc 0.96
2016-09-05T19:15:04.123196: step 1070, loss 0.0564869, acc 0.98
2016-09-05T19:15:04.947739: step 1071, loss 0.0483286, acc 0.98
2016-09-05T19:15:05.752157: step 1072, loss 0.0591854, acc 0.98
2016-09-05T19:15:06.586594: step 1073, loss 0.22676, acc 0.92
2016-09-05T19:15:07.382792: step 1074, loss 0.0694896, acc 0.98
2016-09-05T19:15:08.202201: step 1075, loss 0.0753436, acc 0.98
2016-09-05T19:15:09.019635: step 1076, loss 0.12963, acc 0.92
2016-09-05T19:15:09.820041: step 1077, loss 0.111702, acc 0.94
2016-09-05T19:15:10.622919: step 1078, loss 0.245893, acc 0.94
2016-09-05T19:15:11.432339: step 1079, loss 0.209581, acc 0.9
2016-09-05T19:15:12.257973: step 1080, loss 0.118045, acc 0.94
2016-09-05T19:15:13.054157: step 1081, loss 0.0735148, acc 1
2016-09-05T19:15:13.933294: step 1082, loss 0.0261254, acc 1
2016-09-05T19:15:14.746806: step 1083, loss 0.140331, acc 0.92
2016-09-05T19:15:15.555548: step 1084, loss 0.0698376, acc 0.98
2016-09-05T19:15:16.380379: step 1085, loss 0.0936181, acc 0.96
2016-09-05T19:15:17.210133: step 1086, loss 0.110192, acc 0.96
2016-09-05T19:15:18.011312: step 1087, loss 0.0736074, acc 0.96
2016-09-05T19:15:18.804589: step 1088, loss 0.0642593, acc 0.96
2016-09-05T19:15:19.619338: step 1089, loss 0.159605, acc 0.94
2016-09-05T19:15:20.420899: step 1090, loss 0.229922, acc 0.9
2016-09-05T19:15:21.227365: step 1091, loss 0.13059, acc 0.92
2016-09-05T19:15:22.041201: step 1092, loss 0.107577, acc 0.94
2016-09-05T19:15:22.833400: step 1093, loss 0.179149, acc 0.92
2016-09-05T19:15:23.635326: step 1094, loss 0.123012, acc 0.96
2016-09-05T19:15:24.457915: step 1095, loss 0.0462697, acc 1
2016-09-05T19:15:25.225385: step 1096, loss 0.0869788, acc 0.94
2016-09-05T19:15:26.022988: step 1097, loss 0.200563, acc 0.92
2016-09-05T19:15:26.822657: step 1098, loss 0.0694164, acc 0.98
2016-09-05T19:15:27.632577: step 1099, loss 0.0853897, acc 0.98
2016-09-05T19:15:28.445321: step 1100, loss 0.0738048, acc 1

Evaluation:
2016-09-05T19:15:31.943968: step 1100, loss 0.719191, acc 0.771

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-1100

2016-09-05T19:15:33.915013: step 1101, loss 0.0845298, acc 1
2016-09-05T19:15:34.741275: step 1102, loss 0.194249, acc 0.92
2016-09-05T19:15:35.563526: step 1103, loss 0.0800561, acc 0.96
2016-09-05T19:15:36.364961: step 1104, loss 0.101938, acc 0.96
2016-09-05T19:15:37.152252: step 1105, loss 0.143828, acc 0.92
2016-09-05T19:15:37.984801: step 1106, loss 0.188998, acc 0.9
2016-09-05T19:15:38.784756: step 1107, loss 0.132028, acc 0.94
2016-09-05T19:15:39.601087: step 1108, loss 0.0411356, acc 0.98
2016-09-05T19:15:40.426663: step 1109, loss 0.110035, acc 0.96
2016-09-05T19:15:41.231550: step 1110, loss 0.151967, acc 0.92
2016-09-05T19:15:42.075573: step 1111, loss 0.118557, acc 0.92
2016-09-05T19:15:42.898598: step 1112, loss 0.13742, acc 0.9
2016-09-05T19:15:43.701248: step 1113, loss 0.0433115, acc 1
2016-09-05T19:15:44.525751: step 1114, loss 0.0875426, acc 0.96
2016-09-05T19:15:45.363632: step 1115, loss 0.0528895, acc 0.98
2016-09-05T19:15:46.202259: step 1116, loss 0.177803, acc 0.94
2016-09-05T19:15:47.006318: step 1117, loss 0.0811091, acc 0.96
2016-09-05T19:15:47.823041: step 1118, loss 0.0583591, acc 0.96
2016-09-05T19:15:48.657593: step 1119, loss 0.117224, acc 0.96
2016-09-05T19:15:49.437220: step 1120, loss 0.199138, acc 0.94
2016-09-05T19:15:50.245989: step 1121, loss 0.13661, acc 0.94
2016-09-05T19:15:51.043914: step 1122, loss 0.157125, acc 0.9
2016-09-05T19:15:51.826596: step 1123, loss 0.0770784, acc 0.98
2016-09-05T19:15:52.637001: step 1124, loss 0.120588, acc 0.94
2016-09-05T19:15:53.433408: step 1125, loss 0.192326, acc 0.88
2016-09-05T19:15:54.237461: step 1126, loss 0.110211, acc 0.98
2016-09-05T19:15:55.084553: step 1127, loss 0.0814819, acc 0.96
2016-09-05T19:15:55.899334: step 1128, loss 0.125301, acc 0.92
2016-09-05T19:15:56.707075: step 1129, loss 0.125217, acc 0.94
2016-09-05T19:15:57.512277: step 1130, loss 0.0658549, acc 0.98
2016-09-05T19:15:58.324637: step 1131, loss 0.0697016, acc 0.98
2016-09-05T19:15:59.102628: step 1132, loss 0.121574, acc 0.96
2016-09-05T19:15:59.932211: step 1133, loss 0.0396774, acc 0.98
2016-09-05T19:16:00.771024: step 1134, loss 0.0371468, acc 1
2016-09-05T19:16:01.585419: step 1135, loss 0.0947259, acc 0.98
2016-09-05T19:16:02.375084: step 1136, loss 0.0274677, acc 1
2016-09-05T19:16:03.183702: step 1137, loss 0.0867006, acc 0.98
2016-09-05T19:16:03.947629: step 1138, loss 0.077069, acc 0.96
2016-09-05T19:16:04.783589: step 1139, loss 0.0395251, acc 0.98
2016-09-05T19:16:05.624333: step 1140, loss 0.084162, acc 0.98
2016-09-05T19:16:06.411600: step 1141, loss 0.0687544, acc 0.96
2016-09-05T19:16:07.251649: step 1142, loss 0.0910267, acc 0.96
2016-09-05T19:16:08.049413: step 1143, loss 0.122165, acc 0.96
2016-09-05T19:16:08.817306: step 1144, loss 0.0227924, acc 1
2016-09-05T19:16:09.603930: step 1145, loss 0.138049, acc 0.96
2016-09-05T19:16:10.423541: step 1146, loss 0.194837, acc 0.94
2016-09-05T19:16:11.213339: step 1147, loss 0.157843, acc 0.94
2016-09-05T19:16:12.015583: step 1148, loss 0.0965021, acc 0.96
2016-09-05T19:16:12.827690: step 1149, loss 0.111651, acc 0.92
2016-09-05T19:16:13.608095: step 1150, loss 0.0952955, acc 0.96
2016-09-05T19:16:14.416039: step 1151, loss 0.105335, acc 0.94
2016-09-05T19:16:15.250767: step 1152, loss 0.0564076, acc 0.98
2016-09-05T19:16:16.073222: step 1153, loss 0.077685, acc 0.96
2016-09-05T19:16:16.901448: step 1154, loss 0.0168, acc 1
2016-09-05T19:16:17.764565: step 1155, loss 0.0615984, acc 0.98
2016-09-05T19:16:18.561987: step 1156, loss 0.0609557, acc 0.98
2016-09-05T19:16:19.377919: step 1157, loss 0.116733, acc 0.92
2016-09-05T19:16:20.190983: step 1158, loss 0.263926, acc 0.94
2016-09-05T19:16:20.966868: step 1159, loss 0.0580189, acc 0.98
2016-09-05T19:16:21.766608: step 1160, loss 0.0943137, acc 0.96
2016-09-05T19:16:22.597811: step 1161, loss 0.113844, acc 0.96
2016-09-05T19:16:23.389240: step 1162, loss 0.0676513, acc 0.96
2016-09-05T19:16:24.211354: step 1163, loss 0.0828056, acc 0.94
2016-09-05T19:16:24.661796: step 1164, loss 0.140123, acc 1
2016-09-05T19:16:25.473686: step 1165, loss 0.115876, acc 0.94
2016-09-05T19:16:26.311140: step 1166, loss 0.0417337, acc 1
2016-09-05T19:16:27.109672: step 1167, loss 0.0697012, acc 1
2016-09-05T19:16:27.932728: step 1168, loss 0.0653597, acc 0.96
2016-09-05T19:16:28.762441: step 1169, loss 0.175037, acc 0.94
2016-09-05T19:16:29.568151: step 1170, loss 0.290964, acc 0.94
2016-09-05T19:16:30.384746: step 1171, loss 0.089035, acc 0.94
2016-09-05T19:16:31.211064: step 1172, loss 0.0219948, acc 1
2016-09-05T19:16:31.983319: step 1173, loss 0.0506474, acc 0.96
2016-09-05T19:16:32.787110: step 1174, loss 0.0205956, acc 1
2016-09-05T19:16:33.629656: step 1175, loss 0.0661348, acc 0.98
2016-09-05T19:16:34.442103: step 1176, loss 0.181489, acc 0.92
2016-09-05T19:16:35.222856: step 1177, loss 0.0354594, acc 0.98
2016-09-05T19:16:36.036762: step 1178, loss 0.0208561, acc 1
2016-09-05T19:16:36.846475: step 1179, loss 0.0305417, acc 1
2016-09-05T19:16:37.671392: step 1180, loss 0.0722439, acc 0.96
2016-09-05T19:16:38.495904: step 1181, loss 0.0907759, acc 0.98
2016-09-05T19:16:39.329894: step 1182, loss 0.041579, acc 1
2016-09-05T19:16:40.139528: step 1183, loss 0.130531, acc 0.94
2016-09-05T19:16:40.974936: step 1184, loss 0.0593051, acc 0.98
2016-09-05T19:16:41.774204: step 1185, loss 0.0711327, acc 0.98
2016-09-05T19:16:42.591030: step 1186, loss 0.0812444, acc 0.96
2016-09-05T19:16:43.424237: step 1187, loss 0.0430629, acc 1
2016-09-05T19:16:44.253638: step 1188, loss 0.106527, acc 0.94
2016-09-05T19:16:45.037412: step 1189, loss 0.0659144, acc 0.96
2016-09-05T19:16:45.907505: step 1190, loss 0.0465148, acc 0.98
2016-09-05T19:16:46.739344: step 1191, loss 0.0873205, acc 0.96
2016-09-05T19:16:47.528573: step 1192, loss 0.0490824, acc 0.98
2016-09-05T19:16:48.325467: step 1193, loss 0.0572079, acc 0.96
2016-09-05T19:16:49.152023: step 1194, loss 0.0356763, acc 1
2016-09-05T19:16:49.943063: step 1195, loss 0.0568314, acc 0.98
2016-09-05T19:16:50.729698: step 1196, loss 0.0815566, acc 0.94
2016-09-05T19:16:51.543019: step 1197, loss 0.0272356, acc 0.98
2016-09-05T19:16:52.327154: step 1198, loss 0.0518475, acc 0.98
2016-09-05T19:16:53.130869: step 1199, loss 0.124529, acc 0.92
2016-09-05T19:16:53.932633: step 1200, loss 0.136449, acc 0.98

Evaluation:
2016-09-05T19:16:57.432968: step 1200, loss 1.09383, acc 0.746

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-1200

2016-09-05T19:16:59.291535: step 1201, loss 0.0543815, acc 0.96
2016-09-05T19:17:00.113725: step 1202, loss 0.0984459, acc 0.96
2016-09-05T19:17:00.955787: step 1203, loss 0.0546216, acc 0.98
2016-09-05T19:17:01.821938: step 1204, loss 0.0712498, acc 0.98
2016-09-05T19:17:02.669718: step 1205, loss 0.0780024, acc 0.98
2016-09-05T19:17:03.497705: step 1206, loss 0.174436, acc 0.9
2016-09-05T19:17:04.303052: step 1207, loss 0.125034, acc 0.98
2016-09-05T19:17:05.138378: step 1208, loss 0.0365059, acc 1
2016-09-05T19:17:05.971495: step 1209, loss 0.0528202, acc 0.98
2016-09-05T19:17:06.783246: step 1210, loss 0.262039, acc 0.9
2016-09-05T19:17:07.602655: step 1211, loss 0.11074, acc 0.94
2016-09-05T19:17:08.396121: step 1212, loss 0.0839407, acc 0.98
2016-09-05T19:17:09.190148: step 1213, loss 0.117158, acc 0.94
2016-09-05T19:17:10.034158: step 1214, loss 0.0600377, acc 0.98
2016-09-05T19:17:10.848448: step 1215, loss 0.200575, acc 0.94
2016-09-05T19:17:11.665611: step 1216, loss 0.0287481, acc 1
2016-09-05T19:17:12.507806: step 1217, loss 0.0507665, acc 0.98
2016-09-05T19:17:13.324354: step 1218, loss 0.0534794, acc 1
2016-09-05T19:17:14.101610: step 1219, loss 0.0850396, acc 0.96
2016-09-05T19:17:14.938431: step 1220, loss 0.124427, acc 0.94
2016-09-05T19:17:15.744187: step 1221, loss 0.140991, acc 0.94
2016-09-05T19:17:16.525410: step 1222, loss 0.0560265, acc 0.98
2016-09-05T19:17:17.301238: step 1223, loss 0.0871266, acc 0.98
2016-09-05T19:17:18.129376: step 1224, loss 0.066185, acc 1
2016-09-05T19:17:18.914347: step 1225, loss 0.0746881, acc 0.98
2016-09-05T19:17:19.701632: step 1226, loss 0.0593388, acc 0.96
2016-09-05T19:17:20.510838: step 1227, loss 0.0875545, acc 0.94
2016-09-05T19:17:21.299649: step 1228, loss 0.113579, acc 0.96
2016-09-05T19:17:22.141335: step 1229, loss 0.146842, acc 0.94
2016-09-05T19:17:22.967569: step 1230, loss 0.104006, acc 0.98
2016-09-05T19:17:23.771484: step 1231, loss 0.0635072, acc 0.98
2016-09-05T19:17:24.565935: step 1232, loss 0.0455008, acc 0.98
2016-09-05T19:17:25.381723: step 1233, loss 0.0293431, acc 1
2016-09-05T19:17:26.154725: step 1234, loss 0.0930706, acc 0.96
2016-09-05T19:17:26.941639: step 1235, loss 0.114373, acc 0.94
2016-09-05T19:17:27.763539: step 1236, loss 0.0668448, acc 0.98
2016-09-05T19:17:28.551852: step 1237, loss 0.0551681, acc 0.98
2016-09-05T19:17:29.356449: step 1238, loss 0.00923839, acc 1
2016-09-05T19:17:30.173413: step 1239, loss 0.0402727, acc 1
2016-09-05T19:17:30.944360: step 1240, loss 0.241189, acc 0.86
2016-09-05T19:17:31.763217: step 1241, loss 0.158544, acc 0.94
2016-09-05T19:17:32.561204: step 1242, loss 0.0748802, acc 0.98
2016-09-05T19:17:33.376531: step 1243, loss 0.0828161, acc 0.96
2016-09-05T19:17:34.207855: step 1244, loss 0.203189, acc 0.94
2016-09-05T19:17:35.027312: step 1245, loss 0.169205, acc 0.92
2016-09-05T19:17:35.837130: step 1246, loss 0.171843, acc 0.96
2016-09-05T19:17:36.635583: step 1247, loss 0.0796733, acc 0.96
2016-09-05T19:17:37.444037: step 1248, loss 0.044961, acc 1
2016-09-05T19:17:38.258467: step 1249, loss 0.179669, acc 0.92
2016-09-05T19:17:39.110301: step 1250, loss 0.0731431, acc 0.98
2016-09-05T19:17:39.919136: step 1251, loss 0.0459886, acc 1
2016-09-05T19:17:40.689296: step 1252, loss 0.0690117, acc 0.98
2016-09-05T19:17:41.487970: step 1253, loss 0.153908, acc 0.92
2016-09-05T19:17:42.319973: step 1254, loss 0.0685407, acc 0.96
2016-09-05T19:17:43.102114: step 1255, loss 0.119855, acc 0.94
2016-09-05T19:17:43.909406: step 1256, loss 0.133082, acc 0.94
2016-09-05T19:17:44.731130: step 1257, loss 0.0768481, acc 0.96
2016-09-05T19:17:45.513431: step 1258, loss 0.148843, acc 0.94
2016-09-05T19:17:46.318721: step 1259, loss 0.0566843, acc 0.98
2016-09-05T19:17:47.144128: step 1260, loss 0.112728, acc 0.96
2016-09-05T19:17:47.941445: step 1261, loss 0.0907965, acc 0.96
2016-09-05T19:17:48.743829: step 1262, loss 0.0848642, acc 0.98
2016-09-05T19:17:49.589220: step 1263, loss 0.0863802, acc 0.94
2016-09-05T19:17:50.423210: step 1264, loss 0.0658626, acc 0.98
2016-09-05T19:17:51.222575: step 1265, loss 0.0720582, acc 0.96
2016-09-05T19:17:52.050964: step 1266, loss 0.126035, acc 0.96
2016-09-05T19:17:52.840454: step 1267, loss 0.0642007, acc 0.96
2016-09-05T19:17:53.657509: step 1268, loss 0.161277, acc 0.92
2016-09-05T19:17:54.507258: step 1269, loss 0.068172, acc 0.98
2016-09-05T19:17:55.291835: step 1270, loss 0.0831436, acc 0.94
2016-09-05T19:17:56.104335: step 1271, loss 0.0715005, acc 0.98
2016-09-05T19:17:56.944723: step 1272, loss 0.0692678, acc 0.98
2016-09-05T19:17:57.753319: step 1273, loss 0.0749537, acc 0.98
2016-09-05T19:17:58.554223: step 1274, loss 0.0585742, acc 0.98
2016-09-05T19:17:59.364650: step 1275, loss 0.155678, acc 0.92
2016-09-05T19:18:00.202776: step 1276, loss 0.0525388, acc 0.98
2016-09-05T19:18:01.028587: step 1277, loss 0.202462, acc 0.96
2016-09-05T19:18:01.877417: step 1278, loss 0.0586318, acc 0.98
2016-09-05T19:18:02.700358: step 1279, loss 0.0841984, acc 0.96
2016-09-05T19:18:03.507701: step 1280, loss 0.0666816, acc 0.96
2016-09-05T19:18:04.328560: step 1281, loss 0.0618089, acc 0.98
2016-09-05T19:18:05.153287: step 1282, loss 0.0782994, acc 0.94
2016-09-05T19:18:05.936626: step 1283, loss 0.0382885, acc 0.98
2016-09-05T19:18:06.750971: step 1284, loss 0.0744562, acc 0.94
2016-09-05T19:18:07.555271: step 1285, loss 0.121142, acc 0.94
2016-09-05T19:18:08.354029: step 1286, loss 0.0885278, acc 0.96
2016-09-05T19:18:09.181877: step 1287, loss 0.201658, acc 0.94
2016-09-05T19:18:10.002200: step 1288, loss 0.12616, acc 0.96
2016-09-05T19:18:10.822524: step 1289, loss 0.171821, acc 0.9
2016-09-05T19:18:11.662735: step 1290, loss 0.0997823, acc 0.96
2016-09-05T19:18:12.489434: step 1291, loss 0.132614, acc 0.9
2016-09-05T19:18:13.323256: step 1292, loss 0.137813, acc 0.94
2016-09-05T19:18:14.121116: step 1293, loss 0.195285, acc 0.92
2016-09-05T19:18:14.932153: step 1294, loss 0.144081, acc 0.96
2016-09-05T19:18:15.713285: step 1295, loss 0.135573, acc 0.96
2016-09-05T19:18:16.536045: step 1296, loss 0.173824, acc 0.92
2016-09-05T19:18:17.394626: step 1297, loss 0.0303874, acc 1
2016-09-05T19:18:18.164918: step 1298, loss 0.0960135, acc 0.96
2016-09-05T19:18:18.990053: step 1299, loss 0.0918064, acc 0.96
2016-09-05T19:18:19.790052: step 1300, loss 0.166905, acc 0.96

Evaluation:
2016-09-05T19:18:23.276926: step 1300, loss 0.847147, acc 0.766

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-1300

2016-09-05T19:18:25.172831: step 1301, loss 0.135463, acc 0.94
2016-09-05T19:18:25.991966: step 1302, loss 0.0747272, acc 0.94
2016-09-05T19:18:26.774502: step 1303, loss 0.132259, acc 0.94
2016-09-05T19:18:27.559995: step 1304, loss 0.0634257, acc 0.96
2016-09-05T19:18:28.380416: step 1305, loss 0.103647, acc 0.98
2016-09-05T19:18:29.195904: step 1306, loss 0.107555, acc 0.96
2016-09-05T19:18:29.988341: step 1307, loss 0.118724, acc 0.94
2016-09-05T19:18:30.821451: step 1308, loss 0.149884, acc 0.96
2016-09-05T19:18:31.589706: step 1309, loss 0.142361, acc 0.94
2016-09-05T19:18:32.378067: step 1310, loss 0.0942606, acc 0.96
2016-09-05T19:18:33.172168: step 1311, loss 0.110493, acc 0.94
2016-09-05T19:18:33.976166: step 1312, loss 0.0487964, acc 0.98
2016-09-05T19:18:34.778311: step 1313, loss 0.132663, acc 0.96
2016-09-05T19:18:35.595229: step 1314, loss 0.105309, acc 0.96
2016-09-05T19:18:36.372213: step 1315, loss 0.215993, acc 0.94
2016-09-05T19:18:37.192661: step 1316, loss 0.0727939, acc 0.96
2016-09-05T19:18:38.004928: step 1317, loss 0.131008, acc 0.94
2016-09-05T19:18:38.790822: step 1318, loss 0.263954, acc 0.94
2016-09-05T19:18:39.637048: step 1319, loss 0.0319512, acc 1
2016-09-05T19:18:40.499890: step 1320, loss 0.0486688, acc 0.98
2016-09-05T19:18:41.282699: step 1321, loss 0.0541448, acc 0.98
2016-09-05T19:18:42.058281: step 1322, loss 0.154112, acc 0.88
2016-09-05T19:18:42.893057: step 1323, loss 0.0691954, acc 0.98
2016-09-05T19:18:43.685392: step 1324, loss 0.0678962, acc 0.96
2016-09-05T19:18:44.462731: step 1325, loss 0.114231, acc 0.94
2016-09-05T19:18:45.307558: step 1326, loss 0.0665369, acc 0.98
2016-09-05T19:18:46.092907: step 1327, loss 0.03373, acc 1
2016-09-05T19:18:46.880578: step 1328, loss 0.0245932, acc 1
2016-09-05T19:18:47.694402: step 1329, loss 0.0169062, acc 1
2016-09-05T19:18:48.493682: step 1330, loss 0.1023, acc 0.92
2016-09-05T19:18:49.283646: step 1331, loss 0.0572293, acc 0.98
2016-09-05T19:18:50.111163: step 1332, loss 0.0468986, acc 0.98
2016-09-05T19:18:50.891810: step 1333, loss 0.0233674, acc 1
2016-09-05T19:18:51.688356: step 1334, loss 0.0692106, acc 0.96
2016-09-05T19:18:52.517846: step 1335, loss 0.0807371, acc 0.96
2016-09-05T19:18:53.314356: step 1336, loss 0.12347, acc 0.96
2016-09-05T19:18:54.102454: step 1337, loss 0.127609, acc 0.96
2016-09-05T19:18:54.901481: step 1338, loss 0.0459072, acc 0.98
2016-09-05T19:18:55.689813: step 1339, loss 0.0760102, acc 0.94
2016-09-05T19:18:56.504919: step 1340, loss 0.0604486, acc 0.96
2016-09-05T19:18:57.303959: step 1341, loss 0.0387489, acc 0.98
2016-09-05T19:18:58.107358: step 1342, loss 0.0693254, acc 0.96
2016-09-05T19:18:58.943451: step 1343, loss 0.0663266, acc 0.98
2016-09-05T19:18:59.766337: step 1344, loss 0.0554634, acc 0.98
2016-09-05T19:19:00.598378: step 1345, loss 0.0551681, acc 0.98
2016-09-05T19:19:01.430038: step 1346, loss 0.0526026, acc 0.96
2016-09-05T19:19:02.230108: step 1347, loss 0.0365147, acc 0.98
2016-09-05T19:19:02.984475: step 1348, loss 0.0490527, acc 0.98
2016-09-05T19:19:03.781550: step 1349, loss 0.0525569, acc 0.98
2016-09-05T19:19:04.608116: step 1350, loss 0.0415457, acc 0.98
2016-09-05T19:19:05.401888: step 1351, loss 0.208486, acc 0.98
2016-09-05T19:19:06.213178: step 1352, loss 0.0889744, acc 0.98
2016-09-05T19:19:07.062686: step 1353, loss 0.0796851, acc 0.96
2016-09-05T19:19:07.864421: step 1354, loss 0.226488, acc 0.9
2016-09-05T19:19:08.657841: step 1355, loss 0.144736, acc 0.96
2016-09-05T19:19:09.465672: step 1356, loss 0.0596348, acc 0.96
2016-09-05T19:19:10.242643: step 1357, loss 0.030454, acc 1
2016-09-05T19:19:10.648695: step 1358, loss 0.207046, acc 0.833333
2016-09-05T19:19:11.440380: step 1359, loss 0.0644355, acc 0.96
2016-09-05T19:19:12.229359: step 1360, loss 0.166283, acc 0.94
2016-09-05T19:19:13.033221: step 1361, loss 0.163855, acc 0.96
2016-09-05T19:19:13.828681: step 1362, loss 0.0886574, acc 0.96
2016-09-05T19:19:14.620970: step 1363, loss 0.128766, acc 0.94
2016-09-05T19:19:15.423632: step 1364, loss 0.098592, acc 0.92
2016-09-05T19:19:16.230783: step 1365, loss 0.0700281, acc 1
2016-09-05T19:19:17.045996: step 1366, loss 0.0910774, acc 0.94
2016-09-05T19:19:17.857801: step 1367, loss 0.0637052, acc 0.98
2016-09-05T19:19:18.678223: step 1368, loss 0.134832, acc 0.96
2016-09-05T19:19:19.487776: step 1369, loss 0.0670532, acc 0.98
2016-09-05T19:19:20.294806: step 1370, loss 0.0344757, acc 0.98
2016-09-05T19:19:21.080814: step 1371, loss 0.164692, acc 0.94
2016-09-05T19:19:21.886906: step 1372, loss 0.143832, acc 0.92
2016-09-05T19:19:22.712090: step 1373, loss 0.0786872, acc 0.98
2016-09-05T19:19:23.488479: step 1374, loss 0.0605891, acc 0.98
2016-09-05T19:19:24.325569: step 1375, loss 0.0830335, acc 0.96
2016-09-05T19:19:25.152626: step 1376, loss 0.020998, acc 1
2016-09-05T19:19:25.937097: step 1377, loss 0.100627, acc 0.98
2016-09-05T19:19:26.736892: step 1378, loss 0.0415652, acc 1
2016-09-05T19:19:27.550552: step 1379, loss 0.144499, acc 0.94
2016-09-05T19:19:28.337086: step 1380, loss 0.0888401, acc 0.94
2016-09-05T19:19:29.147090: step 1381, loss 0.040193, acc 1
2016-09-05T19:19:29.964462: step 1382, loss 0.075393, acc 0.98
2016-09-05T19:19:30.751756: step 1383, loss 0.0339477, acc 1
2016-09-05T19:19:31.549238: step 1384, loss 0.0769892, acc 0.96
2016-09-05T19:19:32.382689: step 1385, loss 0.0504028, acc 0.98
2016-09-05T19:19:33.190185: step 1386, loss 0.178002, acc 0.92
2016-09-05T19:19:34.024742: step 1387, loss 0.0858255, acc 0.96
2016-09-05T19:19:34.858341: step 1388, loss 0.042567, acc 0.98
2016-09-05T19:19:35.660202: step 1389, loss 0.0589035, acc 0.98
2016-09-05T19:19:36.475409: step 1390, loss 0.0815, acc 0.98
2016-09-05T19:19:37.275610: step 1391, loss 0.0535328, acc 0.98
2016-09-05T19:19:38.055696: step 1392, loss 0.121848, acc 0.96
2016-09-05T19:19:38.836467: step 1393, loss 0.0234478, acc 1
2016-09-05T19:19:39.651962: step 1394, loss 0.105757, acc 0.96
2016-09-05T19:19:40.445132: step 1395, loss 0.0950044, acc 0.96
2016-09-05T19:19:41.263665: step 1396, loss 0.122789, acc 0.94
2016-09-05T19:19:42.076885: step 1397, loss 0.0366458, acc 1
2016-09-05T19:19:42.855671: step 1398, loss 0.0727116, acc 0.96
2016-09-05T19:19:43.673278: step 1399, loss 0.036686, acc 1
2016-09-05T19:19:44.505418: step 1400, loss 0.105689, acc 0.92

Evaluation:
2016-09-05T19:19:47.999584: step 1400, loss 1.23525, acc 0.757

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-1400

2016-09-05T19:19:49.871484: step 1401, loss 0.0363596, acc 1
2016-09-05T19:19:50.697186: step 1402, loss 0.100102, acc 0.96
2016-09-05T19:19:51.508677: step 1403, loss 0.0827657, acc 0.98
2016-09-05T19:19:52.297349: step 1404, loss 0.0220385, acc 1
2016-09-05T19:19:53.116752: step 1405, loss 0.0848208, acc 0.96
2016-09-05T19:19:53.949470: step 1406, loss 0.104025, acc 0.94
2016-09-05T19:19:54.777225: step 1407, loss 0.0243946, acc 1
2016-09-05T19:19:55.609653: step 1408, loss 0.0680307, acc 0.96
2016-09-05T19:19:56.415530: step 1409, loss 0.0936755, acc 0.96
2016-09-05T19:19:57.246569: step 1410, loss 0.0663237, acc 0.98
2016-09-05T19:19:58.054180: step 1411, loss 0.0323809, acc 0.98
2016-09-05T19:19:58.875329: step 1412, loss 0.18577, acc 0.92
2016-09-05T19:19:59.674809: step 1413, loss 0.0125372, acc 1
2016-09-05T19:20:00.551661: step 1414, loss 0.0725332, acc 0.98
2016-09-05T19:20:01.330239: step 1415, loss 0.110563, acc 0.96
2016-09-05T19:20:02.145459: step 1416, loss 0.0097562, acc 1
2016-09-05T19:20:02.970201: step 1417, loss 0.0374676, acc 0.98
2016-09-05T19:20:03.778412: step 1418, loss 0.053044, acc 0.96
2016-09-05T19:20:04.577294: step 1419, loss 0.113458, acc 0.94
2016-09-05T19:20:05.402254: step 1420, loss 0.110008, acc 0.96
2016-09-05T19:20:06.217197: step 1421, loss 0.0778653, acc 0.96
2016-09-05T19:20:07.019160: step 1422, loss 0.265105, acc 0.9
2016-09-05T19:20:07.820019: step 1423, loss 0.0852873, acc 0.96
2016-09-05T19:20:08.610356: step 1424, loss 0.281173, acc 0.9
2016-09-05T19:20:09.389345: step 1425, loss 0.0776166, acc 0.96
2016-09-05T19:20:10.241550: step 1426, loss 0.049243, acc 0.96
2016-09-05T19:20:11.132396: step 1427, loss 0.119715, acc 0.96
2016-09-05T19:20:11.945107: step 1428, loss 0.117227, acc 0.96
2016-09-05T19:20:12.752605: step 1429, loss 0.228962, acc 0.92
2016-09-05T19:20:13.551107: step 1430, loss 0.0927818, acc 0.96
2016-09-05T19:20:14.337432: step 1431, loss 0.227689, acc 0.92
2016-09-05T19:20:15.119144: step 1432, loss 0.128973, acc 0.98
2016-09-05T19:20:15.938096: step 1433, loss 0.0506871, acc 1
2016-09-05T19:20:16.719668: step 1434, loss 0.111054, acc 0.96
2016-09-05T19:20:17.536675: step 1435, loss 0.0671289, acc 0.98
2016-09-05T19:20:18.346370: step 1436, loss 0.107608, acc 0.94
2016-09-05T19:20:19.117393: step 1437, loss 0.104248, acc 0.96
2016-09-05T19:20:19.935084: step 1438, loss 0.0703482, acc 0.96
2016-09-05T19:20:20.765462: step 1439, loss 0.108037, acc 0.94
2016-09-05T19:20:21.568271: step 1440, loss 0.0932918, acc 0.96
2016-09-05T19:20:22.437346: step 1441, loss 0.111511, acc 0.98
2016-09-05T19:20:23.303961: step 1442, loss 0.063287, acc 0.96
2016-09-05T19:20:24.086962: step 1443, loss 0.0798779, acc 0.96
2016-09-05T19:20:24.887010: step 1444, loss 0.0576503, acc 0.98
2016-09-05T19:20:25.741488: step 1445, loss 0.0604686, acc 0.96
2016-09-05T19:20:26.579298: step 1446, loss 0.128038, acc 0.98
2016-09-05T19:20:27.388822: step 1447, loss 0.0538423, acc 1
2016-09-05T19:20:28.240747: step 1448, loss 0.13904, acc 0.94
2016-09-05T19:20:29.018313: step 1449, loss 0.130214, acc 0.94
2016-09-05T19:20:29.857224: step 1450, loss 0.160756, acc 0.92
2016-09-05T19:20:30.671784: step 1451, loss 0.0679571, acc 0.96
2016-09-05T19:20:31.488326: step 1452, loss 0.105793, acc 0.94
2016-09-05T19:20:32.308516: step 1453, loss 0.105643, acc 0.96
2016-09-05T19:20:33.145814: step 1454, loss 0.0705267, acc 0.98
2016-09-05T19:20:33.944167: step 1455, loss 0.138979, acc 0.98
2016-09-05T19:20:34.789688: step 1456, loss 0.126905, acc 0.96
2016-09-05T19:20:35.613276: step 1457, loss 0.0994092, acc 0.94
2016-09-05T19:20:36.441836: step 1458, loss 0.0507621, acc 0.98
2016-09-05T19:20:37.242710: step 1459, loss 0.0370242, acc 1
2016-09-05T19:20:38.042899: step 1460, loss 0.0981031, acc 0.94
2016-09-05T19:20:38.839013: step 1461, loss 0.0477183, acc 1
2016-09-05T19:20:39.620836: step 1462, loss 0.0775683, acc 0.96
2016-09-05T19:20:40.443141: step 1463, loss 0.03804, acc 1
2016-09-05T19:20:41.265253: step 1464, loss 0.208424, acc 0.94
2016-09-05T19:20:42.055830: step 1465, loss 0.072779, acc 0.98
2016-09-05T19:20:42.892532: step 1466, loss 0.0522773, acc 1
2016-09-05T19:20:43.720517: step 1467, loss 0.0689306, acc 0.98
2016-09-05T19:20:44.545755: step 1468, loss 0.11243, acc 0.98
2016-09-05T19:20:45.328684: step 1469, loss 0.0774183, acc 0.98
2016-09-05T19:20:46.139080: step 1470, loss 0.0508505, acc 0.98
2016-09-05T19:20:46.953407: step 1471, loss 0.0651829, acc 0.96
2016-09-05T19:20:47.742303: step 1472, loss 0.090923, acc 0.96
2016-09-05T19:20:48.585008: step 1473, loss 0.0122447, acc 1
2016-09-05T19:20:49.363589: step 1474, loss 0.0746283, acc 0.96
2016-09-05T19:20:50.177396: step 1475, loss 0.085836, acc 0.96
2016-09-05T19:20:50.971035: step 1476, loss 0.0738534, acc 0.96
2016-09-05T19:20:51.766177: step 1477, loss 0.0211496, acc 1
2016-09-05T19:20:52.559007: step 1478, loss 0.151731, acc 0.94
2016-09-05T19:20:53.387253: step 1479, loss 0.125506, acc 0.94
2016-09-05T19:20:54.177116: step 1480, loss 0.0521598, acc 0.98
2016-09-05T19:20:54.966928: step 1481, loss 0.0951397, acc 0.98
2016-09-05T19:20:55.788370: step 1482, loss 0.0156387, acc 1
2016-09-05T19:20:56.566606: step 1483, loss 0.150879, acc 0.94
2016-09-05T19:20:57.393200: step 1484, loss 0.0706698, acc 0.96
2016-09-05T19:20:58.201219: step 1485, loss 0.121821, acc 0.94
2016-09-05T19:20:59.001679: step 1486, loss 0.052836, acc 0.98
2016-09-05T19:20:59.840966: step 1487, loss 0.0491556, acc 0.98
2016-09-05T19:21:00.674437: step 1488, loss 0.0186068, acc 1
2016-09-05T19:21:01.489434: step 1489, loss 0.0945829, acc 0.96
2016-09-05T19:21:02.268888: step 1490, loss 0.103708, acc 0.94
2016-09-05T19:21:03.095071: step 1491, loss 0.0609571, acc 0.98
2016-09-05T19:21:03.868926: step 1492, loss 0.0474155, acc 0.98
2016-09-05T19:21:04.705035: step 1493, loss 0.119382, acc 0.94
2016-09-05T19:21:05.523136: step 1494, loss 0.0664542, acc 0.98
2016-09-05T19:21:06.278241: step 1495, loss 0.0492074, acc 0.98
2016-09-05T19:21:07.096547: step 1496, loss 0.153325, acc 0.94
2016-09-05T19:21:07.913979: step 1497, loss 0.0660796, acc 0.98
2016-09-05T19:21:08.712424: step 1498, loss 0.031707, acc 1
2016-09-05T19:21:09.535605: step 1499, loss 0.0779432, acc 0.96
2016-09-05T19:21:10.347742: step 1500, loss 0.0839062, acc 0.96

Evaluation:
2016-09-05T19:21:13.863035: step 1500, loss 1.01289, acc 0.772

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-1500

2016-09-05T19:21:15.776495: step 1501, loss 0.0733261, acc 0.96
2016-09-05T19:21:16.609361: step 1502, loss 0.0229607, acc 1
2016-09-05T19:21:17.419713: step 1503, loss 0.0578488, acc 0.96
2016-09-05T19:21:18.230900: step 1504, loss 0.137792, acc 0.94
2016-09-05T19:21:19.067999: step 1505, loss 0.0593568, acc 1
2016-09-05T19:21:19.891766: step 1506, loss 0.116904, acc 0.94
2016-09-05T19:21:20.746233: step 1507, loss 0.0323152, acc 1
2016-09-05T19:21:21.593546: step 1508, loss 0.00845545, acc 1
2016-09-05T19:21:22.420380: step 1509, loss 0.0987775, acc 0.96
2016-09-05T19:21:23.226750: step 1510, loss 0.0859206, acc 0.94
2016-09-05T19:21:24.029252: step 1511, loss 0.16444, acc 0.88
2016-09-05T19:21:24.836891: step 1512, loss 0.140742, acc 0.94
2016-09-05T19:21:25.635865: step 1513, loss 0.108706, acc 0.98
2016-09-05T19:21:26.455507: step 1514, loss 0.0399504, acc 0.96
2016-09-05T19:21:27.241055: step 1515, loss 0.0644687, acc 0.98
2016-09-05T19:21:28.039420: step 1516, loss 0.0714148, acc 0.98
2016-09-05T19:21:28.839968: step 1517, loss 0.0196948, acc 1
2016-09-05T19:21:29.665079: step 1518, loss 0.0882202, acc 0.94
2016-09-05T19:21:30.463773: step 1519, loss 0.207031, acc 0.92
2016-09-05T19:21:31.262375: step 1520, loss 0.0481174, acc 0.98
2016-09-05T19:21:32.050200: step 1521, loss 0.0524932, acc 0.98
2016-09-05T19:21:32.840399: step 1522, loss 0.198202, acc 0.94
2016-09-05T19:21:33.661083: step 1523, loss 0.0623572, acc 0.98
2016-09-05T19:21:34.471609: step 1524, loss 0.127163, acc 0.94
2016-09-05T19:21:35.250420: step 1525, loss 0.0674174, acc 0.98
2016-09-05T19:21:36.076456: step 1526, loss 0.0586676, acc 0.98
2016-09-05T19:21:36.872957: step 1527, loss 0.142134, acc 0.94
2016-09-05T19:21:37.656341: step 1528, loss 0.0754599, acc 0.98
2016-09-05T19:21:38.466341: step 1529, loss 0.159112, acc 0.94
2016-09-05T19:21:39.280549: step 1530, loss 0.0844474, acc 0.94
2016-09-05T19:21:40.076040: step 1531, loss 0.0772182, acc 0.96
2016-09-05T19:21:40.907601: step 1532, loss 0.0864538, acc 0.96
2016-09-05T19:21:41.745479: step 1533, loss 0.0706803, acc 0.98
2016-09-05T19:21:42.523259: step 1534, loss 0.118311, acc 0.96
2016-09-05T19:21:43.336096: step 1535, loss 0.0735619, acc 0.98
2016-09-05T19:21:44.158865: step 1536, loss 0.0795279, acc 0.98
2016-09-05T19:21:44.953691: step 1537, loss 0.0481488, acc 0.98
2016-09-05T19:21:45.757454: step 1538, loss 0.140523, acc 0.88
2016-09-05T19:21:46.589030: step 1539, loss 0.0495309, acc 0.96
2016-09-05T19:21:47.352135: step 1540, loss 0.0468516, acc 0.96
2016-09-05T19:21:48.176361: step 1541, loss 0.113568, acc 0.98
2016-09-05T19:21:48.997656: step 1542, loss 0.0611472, acc 0.98
2016-09-05T19:21:49.796364: step 1543, loss 0.0726353, acc 0.96
2016-09-05T19:21:50.641755: step 1544, loss 0.034066, acc 0.98
2016-09-05T19:21:51.427904: step 1545, loss 0.12904, acc 0.96
2016-09-05T19:21:52.223724: step 1546, loss 0.112059, acc 0.94
2016-09-05T19:21:53.040835: step 1547, loss 0.164074, acc 0.94
2016-09-05T19:21:53.824665: step 1548, loss 0.065404, acc 0.96
2016-09-05T19:21:54.595051: step 1549, loss 0.0319938, acc 1
2016-09-05T19:21:55.407358: step 1550, loss 0.0699375, acc 0.96
2016-09-05T19:21:56.231364: step 1551, loss 0.0840882, acc 0.94
2016-09-05T19:21:56.608669: step 1552, loss 0.00463468, acc 1
2016-09-05T19:21:57.411497: step 1553, loss 0.0351414, acc 1
2016-09-05T19:21:58.209952: step 1554, loss 0.0446949, acc 0.96
2016-09-05T19:21:59.055472: step 1555, loss 0.106688, acc 0.94
2016-09-05T19:21:59.873977: step 1556, loss 0.0180096, acc 1
2016-09-05T19:22:00.696883: step 1557, loss 0.115627, acc 0.96
2016-09-05T19:22:01.508048: step 1558, loss 0.252485, acc 0.96
2016-09-05T19:22:02.325396: step 1559, loss 0.0345871, acc 1
2016-09-05T19:22:03.116428: step 1560, loss 0.0726939, acc 0.94
2016-09-05T19:22:03.918124: step 1561, loss 0.0199019, acc 1
2016-09-05T19:22:04.741855: step 1562, loss 0.0639717, acc 0.94
2016-09-05T19:22:05.516799: step 1563, loss 0.106016, acc 0.96
2016-09-05T19:22:06.327764: step 1564, loss 0.10775, acc 0.96
2016-09-05T19:22:07.139325: step 1565, loss 0.116483, acc 0.92
2016-09-05T19:22:07.914194: step 1566, loss 0.0652164, acc 0.96
2016-09-05T19:22:08.749058: step 1567, loss 0.0253451, acc 1
2016-09-05T19:22:09.551724: step 1568, loss 0.111985, acc 0.96
2016-09-05T19:22:10.357222: step 1569, loss 0.0907737, acc 0.98
2016-09-05T19:22:11.183133: step 1570, loss 0.0311625, acc 0.98
2016-09-05T19:22:12.001489: step 1571, loss 0.0620626, acc 0.98
2016-09-05T19:22:12.766880: step 1572, loss 0.0908979, acc 0.98
2016-09-05T19:22:13.568281: step 1573, loss 0.0462288, acc 0.98
2016-09-05T19:22:14.389393: step 1574, loss 0.0955052, acc 0.94
2016-09-05T19:22:15.168316: step 1575, loss 0.029297, acc 1
2016-09-05T19:22:15.980539: step 1576, loss 0.209147, acc 0.92
2016-09-05T19:22:16.813449: step 1577, loss 0.0558745, acc 0.98
2016-09-05T19:22:17.603454: step 1578, loss 0.00769756, acc 1
2016-09-05T19:22:18.385778: step 1579, loss 0.061761, acc 0.98
2016-09-05T19:22:19.207258: step 1580, loss 0.111985, acc 0.96
2016-09-05T19:22:20.012570: step 1581, loss 0.0450263, acc 1
2016-09-05T19:22:20.815333: step 1582, loss 0.197305, acc 0.92
2016-09-05T19:22:21.614536: step 1583, loss 0.0458859, acc 0.98
2016-09-05T19:22:22.392707: step 1584, loss 0.0725809, acc 0.98
2016-09-05T19:22:23.188029: step 1585, loss 0.0862625, acc 0.98
2016-09-05T19:22:24.013432: step 1586, loss 0.109024, acc 0.94
2016-09-05T19:22:24.792806: step 1587, loss 0.0446692, acc 0.98
2016-09-05T19:22:25.603507: step 1588, loss 0.1175, acc 0.96
2016-09-05T19:22:26.418855: step 1589, loss 0.0403799, acc 1
2016-09-05T19:22:27.203593: step 1590, loss 0.072749, acc 0.98
2016-09-05T19:22:28.010287: step 1591, loss 0.0844285, acc 0.96
2016-09-05T19:22:28.848024: step 1592, loss 0.0814625, acc 0.96
2016-09-05T19:22:29.641455: step 1593, loss 0.119369, acc 0.94
2016-09-05T19:22:30.431399: step 1594, loss 0.103136, acc 0.94
2016-09-05T19:22:31.273531: step 1595, loss 0.0226323, acc 1
2016-09-05T19:22:32.064691: step 1596, loss 0.0269244, acc 1
2016-09-05T19:22:32.845731: step 1597, loss 0.131633, acc 0.92
2016-09-05T19:22:33.672624: step 1598, loss 0.0629306, acc 0.98
2016-09-05T19:22:34.499915: step 1599, loss 0.0228272, acc 1
2016-09-05T19:22:35.316993: step 1600, loss 0.0252404, acc 0.98

Evaluation:
2016-09-05T19:22:38.793383: step 1600, loss 1.07354, acc 0.753

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-1600

2016-09-05T19:22:40.620494: step 1601, loss 0.0211936, acc 1
2016-09-05T19:22:41.418067: step 1602, loss 0.0292288, acc 1
2016-09-05T19:22:42.236196: step 1603, loss 0.0737632, acc 0.96
2016-09-05T19:22:43.028959: step 1604, loss 0.0898094, acc 0.94
2016-09-05T19:22:43.829430: step 1605, loss 0.0274086, acc 1
2016-09-05T19:22:44.655740: step 1606, loss 0.0640798, acc 0.98
2016-09-05T19:22:45.470197: step 1607, loss 0.0484754, acc 1
2016-09-05T19:22:46.272054: step 1608, loss 0.12068, acc 0.94
2016-09-05T19:22:47.092124: step 1609, loss 0.153955, acc 0.94
2016-09-05T19:22:47.878677: step 1610, loss 0.00706176, acc 1
2016-09-05T19:22:48.687543: step 1611, loss 0.032643, acc 0.98
2016-09-05T19:22:49.483595: step 1612, loss 0.15248, acc 0.96
2016-09-05T19:22:50.285980: step 1613, loss 0.131657, acc 0.92
2016-09-05T19:22:51.085108: step 1614, loss 0.146194, acc 0.96
2016-09-05T19:22:51.898129: step 1615, loss 0.0575371, acc 0.98
2016-09-05T19:22:52.706863: step 1616, loss 0.122444, acc 0.96
2016-09-05T19:22:53.519873: step 1617, loss 0.0453953, acc 1
2016-09-05T19:22:54.346510: step 1618, loss 0.114046, acc 0.98
2016-09-05T19:22:55.147607: step 1619, loss 0.11873, acc 0.92
2016-09-05T19:22:55.948939: step 1620, loss 0.0683895, acc 0.98
2016-09-05T19:22:56.768149: step 1621, loss 0.0629551, acc 0.98
2016-09-05T19:22:57.558076: step 1622, loss 0.104386, acc 0.96
2016-09-05T19:22:58.360533: step 1623, loss 0.0840296, acc 0.96
2016-09-05T19:22:59.182976: step 1624, loss 0.0585222, acc 0.98
2016-09-05T19:22:59.995157: step 1625, loss 0.121124, acc 0.96
2016-09-05T19:23:00.806752: step 1626, loss 0.0316379, acc 1
2016-09-05T19:23:01.660373: step 1627, loss 0.0985898, acc 0.96
2016-09-05T19:23:02.488217: step 1628, loss 0.0561421, acc 1
2016-09-05T19:23:03.290102: step 1629, loss 0.10117, acc 0.98
2016-09-05T19:23:04.126653: step 1630, loss 0.0618772, acc 0.98
2016-09-05T19:23:04.929046: step 1631, loss 0.0487741, acc 1
2016-09-05T19:23:05.746001: step 1632, loss 0.0752476, acc 0.96
2016-09-05T19:23:06.612869: step 1633, loss 0.100868, acc 0.98
2016-09-05T19:23:07.435203: step 1634, loss 0.134599, acc 0.94
2016-09-05T19:23:08.250798: step 1635, loss 0.0566136, acc 0.96
2016-09-05T19:23:09.069692: step 1636, loss 0.0385418, acc 0.98
2016-09-05T19:23:09.914519: step 1637, loss 0.0615131, acc 0.96
2016-09-05T19:23:10.743271: step 1638, loss 0.0309658, acc 1
2016-09-05T19:23:11.564309: step 1639, loss 0.0368007, acc 1
2016-09-05T19:23:12.386372: step 1640, loss 0.0711708, acc 0.98
2016-09-05T19:23:13.143271: step 1641, loss 0.0413571, acc 0.98
2016-09-05T19:23:13.962880: step 1642, loss 0.0944173, acc 0.94
2016-09-05T19:23:14.768037: step 1643, loss 0.079944, acc 0.94
2016-09-05T19:23:15.580820: step 1644, loss 0.0583498, acc 0.98
2016-09-05T19:23:16.421777: step 1645, loss 0.0784203, acc 0.96
2016-09-05T19:23:17.231868: step 1646, loss 0.0846991, acc 0.96
2016-09-05T19:23:18.011626: step 1647, loss 0.0674694, acc 0.96
2016-09-05T19:23:18.817982: step 1648, loss 0.0426591, acc 0.98
2016-09-05T19:23:19.641201: step 1649, loss 0.0731362, acc 0.98
2016-09-05T19:23:20.415361: step 1650, loss 0.121419, acc 0.92
2016-09-05T19:23:21.224826: step 1651, loss 0.118893, acc 0.9
2016-09-05T19:23:22.046448: step 1652, loss 0.0295976, acc 1
2016-09-05T19:23:22.841582: step 1653, loss 0.00834209, acc 1
2016-09-05T19:23:23.662097: step 1654, loss 0.0216886, acc 1
2016-09-05T19:23:24.467232: step 1655, loss 0.0355164, acc 0.98
2016-09-05T19:23:25.263481: step 1656, loss 0.0257027, acc 1
2016-09-05T19:23:26.085480: step 1657, loss 0.0248653, acc 1
2016-09-05T19:23:26.915638: step 1658, loss 0.119671, acc 0.92
2016-09-05T19:23:27.698158: step 1659, loss 0.0404157, acc 0.98
2016-09-05T19:23:28.470270: step 1660, loss 0.017234, acc 1
2016-09-05T19:23:29.282318: step 1661, loss 0.0644336, acc 0.98
2016-09-05T19:23:30.087563: step 1662, loss 0.0371929, acc 0.98
2016-09-05T19:23:30.889196: step 1663, loss 0.0837274, acc 0.94
2016-09-05T19:23:31.735819: step 1664, loss 0.0276901, acc 1
2016-09-05T19:23:32.516225: step 1665, loss 0.052764, acc 0.98
2016-09-05T19:23:33.308455: step 1666, loss 0.0681749, acc 0.98
2016-09-05T19:23:34.114280: step 1667, loss 0.0974154, acc 0.96
2016-09-05T19:23:34.918434: step 1668, loss 0.0402622, acc 0.96
2016-09-05T19:23:35.717855: step 1669, loss 0.0780534, acc 0.94
2016-09-05T19:23:36.536768: step 1670, loss 0.0386877, acc 0.98
2016-09-05T19:23:37.344032: step 1671, loss 0.0723406, acc 0.98
2016-09-05T19:23:38.138931: step 1672, loss 0.0761065, acc 0.96
2016-09-05T19:23:38.948747: step 1673, loss 0.0328248, acc 1
2016-09-05T19:23:39.731616: step 1674, loss 0.161598, acc 0.92
2016-09-05T19:23:40.545086: step 1675, loss 0.0294012, acc 1
2016-09-05T19:23:41.341773: step 1676, loss 0.0333667, acc 0.98
2016-09-05T19:23:42.130686: step 1677, loss 0.0610505, acc 0.98
2016-09-05T19:23:42.952497: step 1678, loss 0.0831536, acc 0.96
2016-09-05T19:23:43.808890: step 1679, loss 0.234053, acc 0.94
2016-09-05T19:23:44.596019: step 1680, loss 0.0113602, acc 1
2016-09-05T19:23:45.394265: step 1681, loss 0.192634, acc 0.98
2016-09-05T19:23:46.201410: step 1682, loss 0.0600173, acc 0.96
2016-09-05T19:23:46.983921: step 1683, loss 0.0852147, acc 0.96
2016-09-05T19:23:47.821698: step 1684, loss 0.098143, acc 0.94
2016-09-05T19:23:48.644649: step 1685, loss 0.0631231, acc 0.96
2016-09-05T19:23:49.444170: step 1686, loss 0.0698367, acc 0.94
2016-09-05T19:23:50.271979: step 1687, loss 0.12231, acc 0.94
2016-09-05T19:23:51.107923: step 1688, loss 0.0288284, acc 0.98
2016-09-05T19:23:51.897832: step 1689, loss 0.199465, acc 0.94
2016-09-05T19:23:52.676213: step 1690, loss 0.0472441, acc 0.98
2016-09-05T19:23:53.511783: step 1691, loss 0.0150297, acc 1
2016-09-05T19:23:54.284559: step 1692, loss 0.101138, acc 0.96
2016-09-05T19:23:55.099211: step 1693, loss 0.161869, acc 0.92
2016-09-05T19:23:55.928096: step 1694, loss 0.126832, acc 0.96
2016-09-05T19:23:56.732146: step 1695, loss 0.027132, acc 1
2016-09-05T19:23:57.544176: step 1696, loss 0.0171151, acc 1
2016-09-05T19:23:58.358254: step 1697, loss 0.0270535, acc 1
2016-09-05T19:23:59.147011: step 1698, loss 0.0881978, acc 0.96
2016-09-05T19:23:59.941458: step 1699, loss 0.0462867, acc 0.96
2016-09-05T19:24:00.764193: step 1700, loss 0.0438633, acc 1

Evaluation:
2016-09-05T19:24:04.253956: step 1700, loss 1.01409, acc 0.759

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-1700

2016-09-05T19:24:06.036195: step 1701, loss 0.0737981, acc 0.94
2016-09-05T19:24:06.871020: step 1702, loss 0.0436074, acc 0.98
2016-09-05T19:24:07.680853: step 1703, loss 0.0432582, acc 1
2016-09-05T19:24:08.487393: step 1704, loss 0.159017, acc 0.92
2016-09-05T19:24:09.341701: step 1705, loss 0.0619241, acc 0.98
2016-09-05T19:24:10.133832: step 1706, loss 0.136866, acc 0.94
2016-09-05T19:24:10.943747: step 1707, loss 0.0997639, acc 0.92
2016-09-05T19:24:11.778200: step 1708, loss 0.0463076, acc 0.98
2016-09-05T19:24:12.597651: step 1709, loss 0.0635253, acc 0.98
2016-09-05T19:24:13.393803: step 1710, loss 0.138485, acc 0.94
2016-09-05T19:24:14.235342: step 1711, loss 0.073293, acc 1
2016-09-05T19:24:15.053363: step 1712, loss 0.0480268, acc 0.96
2016-09-05T19:24:15.857040: step 1713, loss 0.0636072, acc 0.94
2016-09-05T19:24:16.710714: step 1714, loss 0.0233282, acc 1
2016-09-05T19:24:17.518738: step 1715, loss 0.0453515, acc 0.98
2016-09-05T19:24:18.337084: step 1716, loss 0.0527913, acc 0.98
2016-09-05T19:24:19.187196: step 1717, loss 0.0501011, acc 0.98
2016-09-05T19:24:20.013441: step 1718, loss 0.0872826, acc 0.92
2016-09-05T19:24:20.821862: step 1719, loss 0.0379659, acc 0.98
2016-09-05T19:24:21.629802: step 1720, loss 0.0742992, acc 0.96
2016-09-05T19:24:22.438956: step 1721, loss 0.110621, acc 0.94
2016-09-05T19:24:23.235504: step 1722, loss 0.0632405, acc 0.96
2016-09-05T19:24:24.076472: step 1723, loss 0.169628, acc 0.92
2016-09-05T19:24:24.871122: step 1724, loss 0.0989366, acc 0.98
2016-09-05T19:24:25.709562: step 1725, loss 0.0872959, acc 0.92
2016-09-05T19:24:26.513435: step 1726, loss 0.0164215, acc 1
2016-09-05T19:24:27.309440: step 1727, loss 0.0596343, acc 0.96
2016-09-05T19:24:28.106118: step 1728, loss 0.0473185, acc 1
2016-09-05T19:24:28.923913: step 1729, loss 0.0408882, acc 0.98
2016-09-05T19:24:29.747090: step 1730, loss 0.108942, acc 0.94
2016-09-05T19:24:30.529401: step 1731, loss 0.0308735, acc 1
2016-09-05T19:24:31.321682: step 1732, loss 0.0774972, acc 0.96
2016-09-05T19:24:32.116065: step 1733, loss 0.0717405, acc 0.98
2016-09-05T19:24:32.895181: step 1734, loss 0.0704646, acc 0.98
2016-09-05T19:24:33.718520: step 1735, loss 0.0265667, acc 0.98
2016-09-05T19:24:34.521392: step 1736, loss 0.0340181, acc 0.98
2016-09-05T19:24:35.329149: step 1737, loss 0.105713, acc 0.96
2016-09-05T19:24:36.151005: step 1738, loss 0.0552522, acc 1
2016-09-05T19:24:36.946637: step 1739, loss 0.0817912, acc 0.96
2016-09-05T19:24:37.757023: step 1740, loss 0.0227697, acc 1
2016-09-05T19:24:38.575335: step 1741, loss 0.0642625, acc 0.98
2016-09-05T19:24:39.394075: step 1742, loss 0.0357529, acc 1
2016-09-05T19:24:40.216658: step 1743, loss 0.192624, acc 0.92
2016-09-05T19:24:41.022800: step 1744, loss 0.0561566, acc 0.96
2016-09-05T19:24:41.864902: step 1745, loss 0.0292722, acc 1
2016-09-05T19:24:42.269077: step 1746, loss 0.00715738, acc 1
2016-09-05T19:24:43.093039: step 1747, loss 0.0843728, acc 0.94
2016-09-05T19:24:43.885488: step 1748, loss 0.180411, acc 0.98
2016-09-05T19:24:44.655828: step 1749, loss 0.139122, acc 0.96
2016-09-05T19:24:45.456214: step 1750, loss 0.0154876, acc 1
2016-09-05T19:24:46.241447: step 1751, loss 0.0413263, acc 0.98
2016-09-05T19:24:47.058997: step 1752, loss 0.0190968, acc 1
2016-09-05T19:24:47.873124: step 1753, loss 0.0538135, acc 0.98
2016-09-05T19:24:48.663701: step 1754, loss 0.122255, acc 0.94
2016-09-05T19:24:49.457424: step 1755, loss 0.0432247, acc 0.98
2016-09-05T19:24:50.257406: step 1756, loss 0.0173285, acc 1
2016-09-05T19:24:51.042506: step 1757, loss 0.0596244, acc 0.96
2016-09-05T19:24:51.875783: step 1758, loss 0.100984, acc 0.98
2016-09-05T19:24:52.693008: step 1759, loss 0.0449456, acc 1
2016-09-05T19:24:53.496950: step 1760, loss 0.072592, acc 0.96
2016-09-05T19:24:54.285433: step 1761, loss 0.00644678, acc 1
2016-09-05T19:24:55.146512: step 1762, loss 0.0400997, acc 0.98
2016-09-05T19:24:55.949417: step 1763, loss 0.033355, acc 0.98
2016-09-05T19:24:56.747902: step 1764, loss 0.0893759, acc 0.94
2016-09-05T19:24:57.557560: step 1765, loss 0.0632386, acc 0.98
2016-09-05T19:24:58.342492: step 1766, loss 0.101355, acc 0.98
2016-09-05T19:24:59.147322: step 1767, loss 0.0820304, acc 0.94
2016-09-05T19:24:59.940360: step 1768, loss 0.0480306, acc 0.96
2016-09-05T19:25:00.753913: step 1769, loss 0.0116634, acc 1
2016-09-05T19:25:01.545410: step 1770, loss 0.0484216, acc 0.98
2016-09-05T19:25:02.367392: step 1771, loss 0.0337608, acc 1
2016-09-05T19:25:03.158486: step 1772, loss 0.0326106, acc 0.98
2016-09-05T19:25:03.965332: step 1773, loss 0.0170844, acc 1
2016-09-05T19:25:04.787642: step 1774, loss 0.0318807, acc 1
2016-09-05T19:25:05.561024: step 1775, loss 0.0999165, acc 0.98
2016-09-05T19:25:06.381810: step 1776, loss 0.0166946, acc 1
2016-09-05T19:25:07.205330: step 1777, loss 0.0464331, acc 0.96
2016-09-05T19:25:07.983548: step 1778, loss 0.0694368, acc 0.98
2016-09-05T19:25:08.836788: step 1779, loss 0.0411042, acc 1
2016-09-05T19:25:09.659062: step 1780, loss 0.0590775, acc 0.96
2016-09-05T19:25:10.432298: step 1781, loss 0.0767794, acc 0.96
2016-09-05T19:25:11.265900: step 1782, loss 0.0450318, acc 1
2016-09-05T19:25:12.085627: step 1783, loss 0.0118014, acc 1
2016-09-05T19:25:12.869068: step 1784, loss 0.172188, acc 0.96
2016-09-05T19:25:13.680936: step 1785, loss 0.0331616, acc 0.98
2016-09-05T19:25:14.480951: step 1786, loss 0.0863748, acc 0.98
2016-09-05T19:25:15.281859: step 1787, loss 0.113555, acc 0.96
2016-09-05T19:25:16.102632: step 1788, loss 0.0245599, acc 1
2016-09-05T19:25:16.936827: step 1789, loss 0.0207979, acc 1
2016-09-05T19:25:17.738190: step 1790, loss 0.00715275, acc 1
2016-09-05T19:25:18.520769: step 1791, loss 0.0173837, acc 1
2016-09-05T19:25:19.366224: step 1792, loss 0.135271, acc 0.96
2016-09-05T19:25:20.154565: step 1793, loss 0.0737542, acc 0.94
2016-09-05T19:25:20.935022: step 1794, loss 0.0739525, acc 0.98
2016-09-05T19:25:21.757389: step 1795, loss 0.0272022, acc 1
2016-09-05T19:25:22.545971: step 1796, loss 0.0198375, acc 1
2016-09-05T19:25:23.335821: step 1797, loss 0.0106615, acc 1
2016-09-05T19:25:24.152816: step 1798, loss 0.0506181, acc 0.98
2016-09-05T19:25:24.942593: step 1799, loss 0.0534531, acc 0.98
2016-09-05T19:25:25.745397: step 1800, loss 0.0450376, acc 1

Evaluation:
2016-09-05T19:25:29.297218: step 1800, loss 1.2019, acc 0.756

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-1800

2016-09-05T19:25:31.120383: step 1801, loss 0.0519088, acc 0.98
2016-09-05T19:25:31.921081: step 1802, loss 0.0865865, acc 0.94
2016-09-05T19:25:32.785150: step 1803, loss 0.0456182, acc 0.98
2016-09-05T19:25:33.556981: step 1804, loss 0.0199598, acc 1
2016-09-05T19:25:34.355938: step 1805, loss 0.0368902, acc 0.98
2016-09-05T19:25:35.162419: step 1806, loss 0.0460775, acc 0.96
2016-09-05T19:25:35.935953: step 1807, loss 0.0701342, acc 0.96
2016-09-05T19:25:36.744399: step 1808, loss 0.117831, acc 0.96
2016-09-05T19:25:37.554913: step 1809, loss 0.111573, acc 0.98
2016-09-05T19:25:38.351617: step 1810, loss 0.0277909, acc 1
2016-09-05T19:25:39.150002: step 1811, loss 0.0434826, acc 1
2016-09-05T19:25:39.989863: step 1812, loss 0.0899227, acc 0.96
2016-09-05T19:25:40.775341: step 1813, loss 0.0106866, acc 1
2016-09-05T19:25:41.565218: step 1814, loss 0.112317, acc 0.96
2016-09-05T19:25:42.358391: step 1815, loss 0.0397195, acc 0.98
2016-09-05T19:25:43.173158: step 1816, loss 0.0146635, acc 1
2016-09-05T19:25:43.966160: step 1817, loss 0.00663422, acc 1
2016-09-05T19:25:44.809179: step 1818, loss 0.0260686, acc 0.98
2016-09-05T19:25:45.598630: step 1819, loss 0.0586697, acc 0.96
2016-09-05T19:25:46.404170: step 1820, loss 0.0366756, acc 0.98
2016-09-05T19:25:47.216049: step 1821, loss 0.0208788, acc 0.98
2016-09-05T19:25:48.025053: step 1822, loss 0.0830893, acc 0.94
2016-09-05T19:25:48.819626: step 1823, loss 0.0908252, acc 0.98
2016-09-05T19:25:49.647471: step 1824, loss 0.0242576, acc 1
2016-09-05T19:25:50.450811: step 1825, loss 0.145609, acc 0.96
2016-09-05T19:25:51.266344: step 1826, loss 0.0541229, acc 0.98
2016-09-05T19:25:52.072833: step 1827, loss 0.0924105, acc 0.96
2016-09-05T19:25:52.841216: step 1828, loss 0.0242163, acc 1
2016-09-05T19:25:53.639826: step 1829, loss 0.115694, acc 0.96
2016-09-05T19:25:54.435691: step 1830, loss 0.0186043, acc 1
2016-09-05T19:25:55.246405: step 1831, loss 0.0731341, acc 0.98
2016-09-05T19:25:56.055010: step 1832, loss 0.0527251, acc 0.98
2016-09-05T19:25:56.897426: step 1833, loss 0.114867, acc 0.92
2016-09-05T19:25:57.672541: step 1834, loss 0.0873372, acc 0.98
2016-09-05T19:25:58.466256: step 1835, loss 0.0507508, acc 0.98
2016-09-05T19:25:59.289836: step 1836, loss 0.0124161, acc 1
2016-09-05T19:26:00.091296: step 1837, loss 0.0862206, acc 0.94
2016-09-05T19:26:00.909577: step 1838, loss 0.0787131, acc 0.96
2016-09-05T19:26:01.739825: step 1839, loss 0.133826, acc 0.9
2016-09-05T19:26:02.525075: step 1840, loss 0.047236, acc 0.98
2016-09-05T19:26:03.321299: step 1841, loss 0.0647174, acc 0.98
2016-09-05T19:26:04.131157: step 1842, loss 0.039539, acc 0.98
2016-09-05T19:26:04.915028: step 1843, loss 0.0477832, acc 1
2016-09-05T19:26:05.739095: step 1844, loss 0.0979508, acc 0.96
2016-09-05T19:26:06.589660: step 1845, loss 0.00871394, acc 1
2016-09-05T19:26:07.395046: step 1846, loss 0.101325, acc 0.96
2016-09-05T19:26:08.215393: step 1847, loss 0.0732888, acc 0.96
2016-09-05T19:26:09.053607: step 1848, loss 0.0227898, acc 1
2016-09-05T19:26:09.862374: step 1849, loss 0.0726831, acc 0.98
2016-09-05T19:26:10.716356: step 1850, loss 0.0237582, acc 1
2016-09-05T19:26:11.558670: step 1851, loss 0.0451201, acc 0.98
2016-09-05T19:26:12.366662: step 1852, loss 0.0573467, acc 0.98
2016-09-05T19:26:13.178015: step 1853, loss 0.153646, acc 0.94
2016-09-05T19:26:14.032392: step 1854, loss 0.074154, acc 0.98
2016-09-05T19:26:14.828516: step 1855, loss 0.0661809, acc 0.96
2016-09-05T19:26:15.627205: step 1856, loss 0.0536386, acc 0.96
2016-09-05T19:26:16.460995: step 1857, loss 0.160545, acc 0.92
2016-09-05T19:26:17.262377: step 1858, loss 0.0393988, acc 0.98
2016-09-05T19:26:18.056954: step 1859, loss 0.0307677, acc 0.98
2016-09-05T19:26:18.877560: step 1860, loss 0.109805, acc 0.96
2016-09-05T19:26:19.681257: step 1861, loss 0.106881, acc 0.98
2016-09-05T19:26:20.488437: step 1862, loss 0.0328984, acc 1
2016-09-05T19:26:21.308652: step 1863, loss 0.0607726, acc 0.98
2016-09-05T19:26:22.111825: step 1864, loss 0.0903065, acc 0.96
2016-09-05T19:26:22.924262: step 1865, loss 0.0652145, acc 0.98
2016-09-05T19:26:23.760990: step 1866, loss 0.0854161, acc 0.94
2016-09-05T19:26:24.573359: step 1867, loss 0.0714003, acc 0.98
2016-09-05T19:26:25.401187: step 1868, loss 0.11412, acc 0.96
2016-09-05T19:26:26.211364: step 1869, loss 0.0170667, acc 1
2016-09-05T19:26:27.055248: step 1870, loss 0.0552371, acc 0.96
2016-09-05T19:26:27.838452: step 1871, loss 0.177069, acc 0.92
2016-09-05T19:26:28.648249: step 1872, loss 0.0565707, acc 0.98
2016-09-05T19:26:29.478307: step 1873, loss 0.0601389, acc 0.98
2016-09-05T19:26:30.268361: step 1874, loss 0.0255261, acc 1
2016-09-05T19:26:31.056870: step 1875, loss 0.0274721, acc 0.98
2016-09-05T19:26:31.889314: step 1876, loss 0.0239941, acc 1
2016-09-05T19:26:32.684467: step 1877, loss 0.059702, acc 0.98
2016-09-05T19:26:33.478925: step 1878, loss 0.0614605, acc 0.96
2016-09-05T19:26:34.319534: step 1879, loss 0.0683326, acc 0.96
2016-09-05T19:26:35.099976: step 1880, loss 0.163629, acc 0.94
2016-09-05T19:26:35.904791: step 1881, loss 0.0414978, acc 1
2016-09-05T19:26:36.722157: step 1882, loss 0.0498105, acc 0.98
2016-09-05T19:26:37.517635: step 1883, loss 0.0744423, acc 0.96
2016-09-05T19:26:38.357686: step 1884, loss 0.0572078, acc 0.96
2016-09-05T19:26:39.171910: step 1885, loss 0.0486979, acc 0.96
2016-09-05T19:26:40.006289: step 1886, loss 0.043364, acc 0.98
2016-09-05T19:26:40.795271: step 1887, loss 0.0725697, acc 0.96
2016-09-05T19:26:41.613962: step 1888, loss 0.0841809, acc 0.94
2016-09-05T19:26:42.383779: step 1889, loss 0.0314863, acc 1
2016-09-05T19:26:43.221106: step 1890, loss 0.0208282, acc 1
2016-09-05T19:26:44.048218: step 1891, loss 0.0655888, acc 0.96
2016-09-05T19:26:44.835808: step 1892, loss 0.025025, acc 1
2016-09-05T19:26:45.647512: step 1893, loss 0.0630616, acc 0.98
2016-09-05T19:26:46.470637: step 1894, loss 0.119146, acc 0.94
2016-09-05T19:26:47.269795: step 1895, loss 0.0644897, acc 0.96
2016-09-05T19:26:48.089977: step 1896, loss 0.103175, acc 0.96
2016-09-05T19:26:48.913223: step 1897, loss 0.173414, acc 0.98
2016-09-05T19:26:49.701774: step 1898, loss 0.0603582, acc 0.98
2016-09-05T19:26:50.508941: step 1899, loss 0.154506, acc 0.94
2016-09-05T19:26:51.319781: step 1900, loss 0.094744, acc 0.96

Evaluation:
2016-09-05T19:26:54.819445: step 1900, loss 1.18132, acc 0.756

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-1900

2016-09-05T19:26:56.757313: step 1901, loss 0.0952588, acc 0.96
2016-09-05T19:26:57.570619: step 1902, loss 0.0352556, acc 1
2016-09-05T19:26:58.364386: step 1903, loss 0.0326205, acc 1
2016-09-05T19:26:59.167214: step 1904, loss 0.10922, acc 0.96
2016-09-05T19:26:59.989640: step 1905, loss 0.0284366, acc 0.98
2016-09-05T19:27:00.857415: step 1906, loss 0.0275651, acc 1
2016-09-05T19:27:01.672654: step 1907, loss 0.0829169, acc 0.98
2016-09-05T19:27:02.499662: step 1908, loss 0.0165536, acc 1
2016-09-05T19:27:03.330114: step 1909, loss 0.0360531, acc 0.98
2016-09-05T19:27:04.160883: step 1910, loss 0.0498225, acc 0.98
2016-09-05T19:27:04.977191: step 1911, loss 0.101428, acc 0.96
2016-09-05T19:27:05.822143: step 1912, loss 0.0521363, acc 0.96
2016-09-05T19:27:06.649780: step 1913, loss 0.010719, acc 1
2016-09-05T19:27:07.456102: step 1914, loss 0.144617, acc 0.94
2016-09-05T19:27:08.290889: step 1915, loss 0.10048, acc 0.94
2016-09-05T19:27:09.105531: step 1916, loss 0.125696, acc 0.92
2016-09-05T19:27:09.951230: step 1917, loss 0.0410153, acc 0.96
2016-09-05T19:27:10.864308: step 1918, loss 0.0851502, acc 0.96
2016-09-05T19:27:11.701393: step 1919, loss 0.0480019, acc 0.96
2016-09-05T19:27:12.545422: step 1920, loss 0.0106013, acc 1
2016-09-05T19:27:13.365914: step 1921, loss 0.0499487, acc 0.98
2016-09-05T19:27:14.153974: step 1922, loss 0.0215872, acc 1
2016-09-05T19:27:14.964427: step 1923, loss 0.0728052, acc 0.96
2016-09-05T19:27:15.792463: step 1924, loss 0.0275764, acc 1
2016-09-05T19:27:16.589247: step 1925, loss 0.0829155, acc 0.98
2016-09-05T19:27:17.401667: step 1926, loss 0.0321419, acc 1
2016-09-05T19:27:18.247349: step 1927, loss 0.112139, acc 0.96
2016-09-05T19:27:19.072517: step 1928, loss 0.0887799, acc 0.94
2016-09-05T19:27:19.853906: step 1929, loss 0.0347294, acc 0.98
2016-09-05T19:27:20.657818: step 1930, loss 0.0681835, acc 0.98
2016-09-05T19:27:21.474823: step 1931, loss 0.0190416, acc 1
2016-09-05T19:27:22.279917: step 1932, loss 0.0611722, acc 0.96
2016-09-05T19:27:23.070188: step 1933, loss 0.0521926, acc 0.98
2016-09-05T19:27:23.879515: step 1934, loss 0.0144884, acc 1
2016-09-05T19:27:24.649982: step 1935, loss 0.106298, acc 0.98
2016-09-05T19:27:25.458811: step 1936, loss 0.0473331, acc 0.98
2016-09-05T19:27:26.277125: step 1937, loss 0.034595, acc 1
2016-09-05T19:27:27.090684: step 1938, loss 0.0340981, acc 1
2016-09-05T19:27:27.890266: step 1939, loss 0.0994559, acc 0.94
2016-09-05T19:27:28.329552: step 1940, loss 0.369553, acc 0.833333
2016-09-05T19:27:29.156055: step 1941, loss 0.058431, acc 0.96
2016-09-05T19:27:29.956075: step 1942, loss 0.12868, acc 0.96
2016-09-05T19:27:30.728264: step 1943, loss 0.0578771, acc 0.98
2016-09-05T19:27:31.548431: step 1944, loss 0.146156, acc 0.94
2016-09-05T19:27:32.360219: step 1945, loss 0.207628, acc 0.94
2016-09-05T19:27:33.137298: step 1946, loss 0.137464, acc 0.94
2016-09-05T19:27:33.981552: step 1947, loss 0.0986082, acc 0.94
2016-09-05T19:27:34.783252: step 1948, loss 0.0361764, acc 1
2016-09-05T19:27:35.581451: step 1949, loss 0.0525462, acc 0.98
2016-09-05T19:27:36.368916: step 1950, loss 0.0429536, acc 0.98
2016-09-05T19:27:37.186635: step 1951, loss 0.0238996, acc 1
2016-09-05T19:27:38.001864: step 1952, loss 0.078972, acc 0.98
2016-09-05T19:27:38.773509: step 1953, loss 0.0576826, acc 0.98
2016-09-05T19:27:39.577005: step 1954, loss 0.267242, acc 0.92
2016-09-05T19:27:40.383080: step 1955, loss 0.0980553, acc 0.96
2016-09-05T19:27:41.194131: step 1956, loss 0.0213077, acc 1
2016-09-05T19:27:42.021441: step 1957, loss 0.160565, acc 0.94
2016-09-05T19:27:42.811313: step 1958, loss 0.0542571, acc 0.98
2016-09-05T19:27:43.650446: step 1959, loss 0.0940227, acc 0.92
2016-09-05T19:27:44.460823: step 1960, loss 0.0421986, acc 0.98
2016-09-05T19:27:45.216989: step 1961, loss 0.0180395, acc 1
2016-09-05T19:27:46.009307: step 1962, loss 0.061396, acc 0.96
2016-09-05T19:27:46.823526: step 1963, loss 0.0409407, acc 0.98
2016-09-05T19:27:47.600296: step 1964, loss 0.0175702, acc 1
2016-09-05T19:27:48.410811: step 1965, loss 0.176888, acc 0.96
2016-09-05T19:27:49.210179: step 1966, loss 0.0829008, acc 0.98
2016-09-05T19:27:50.004164: step 1967, loss 0.0216793, acc 1
2016-09-05T19:27:50.823342: step 1968, loss 0.0555636, acc 0.96
2016-09-05T19:27:51.609406: step 1969, loss 0.0823793, acc 0.98
2016-09-05T19:27:52.410434: step 1970, loss 0.094089, acc 0.96
2016-09-05T19:27:53.239392: step 1971, loss 0.0250469, acc 1
2016-09-05T19:27:54.031796: step 1972, loss 0.0857836, acc 0.96
2016-09-05T19:27:54.835437: step 1973, loss 0.0719592, acc 0.96
2016-09-05T19:27:55.648666: step 1974, loss 0.029048, acc 1
2016-09-05T19:27:56.491262: step 1975, loss 0.0643708, acc 0.96
2016-09-05T19:27:57.292647: step 1976, loss 0.118739, acc 0.96
2016-09-05T19:27:58.102546: step 1977, loss 0.0764122, acc 0.96
2016-09-05T19:27:58.904929: step 1978, loss 0.0938877, acc 0.94
2016-09-05T19:27:59.677263: step 1979, loss 0.0309618, acc 0.98
2016-09-05T19:28:00.500972: step 1980, loss 0.0846278, acc 0.98
2016-09-05T19:28:01.310704: step 1981, loss 0.0950068, acc 0.96
2016-09-05T19:28:02.087723: step 1982, loss 0.179233, acc 0.92
2016-09-05T19:28:02.915933: step 1983, loss 0.00910242, acc 1
2016-09-05T19:28:03.712423: step 1984, loss 0.0190773, acc 1
2016-09-05T19:28:04.506019: step 1985, loss 0.0548013, acc 0.98
2016-09-05T19:28:05.320849: step 1986, loss 0.194711, acc 0.92
2016-09-05T19:28:06.123640: step 1987, loss 0.0269793, acc 1
2016-09-05T19:28:06.926223: step 1988, loss 0.0179948, acc 1
2016-09-05T19:28:07.750653: step 1989, loss 0.125025, acc 0.96
2016-09-05T19:28:08.589316: step 1990, loss 0.0669773, acc 0.96
2016-09-05T19:28:09.397492: step 1991, loss 0.147842, acc 0.96
2016-09-05T19:28:10.199758: step 1992, loss 0.0734477, acc 0.96
2016-09-05T19:28:11.004046: step 1993, loss 0.0268368, acc 1
2016-09-05T19:28:11.800326: step 1994, loss 0.0945572, acc 0.98
2016-09-05T19:28:12.597135: step 1995, loss 0.0808968, acc 0.96
2016-09-05T19:28:13.421396: step 1996, loss 0.0993609, acc 0.96
2016-09-05T19:28:14.205483: step 1997, loss 0.107419, acc 0.92
2016-09-05T19:28:15.049193: step 1998, loss 0.0206839, acc 1
2016-09-05T19:28:15.894484: step 1999, loss 0.112303, acc 0.96
2016-09-05T19:28:16.664921: step 2000, loss 0.0861885, acc 0.96

Evaluation:
2016-09-05T19:28:20.167587: step 2000, loss 1.07717, acc 0.738

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-2000

2016-09-05T19:28:21.978473: step 2001, loss 0.0573771, acc 0.98
2016-09-05T19:28:22.757400: step 2002, loss 0.0510966, acc 0.98
2016-09-05T19:28:23.551116: step 2003, loss 0.0985116, acc 0.94
2016-09-05T19:28:24.355858: step 2004, loss 0.105932, acc 0.96
2016-09-05T19:28:25.138330: step 2005, loss 0.0709203, acc 1
2016-09-05T19:28:25.937893: step 2006, loss 0.0877688, acc 0.96
2016-09-05T19:28:26.769913: step 2007, loss 0.085031, acc 0.98
2016-09-05T19:28:27.556161: step 2008, loss 0.0549503, acc 0.98
2016-09-05T19:28:28.393839: step 2009, loss 0.059235, acc 0.96
2016-09-05T19:28:29.211260: step 2010, loss 0.052371, acc 0.98
2016-09-05T19:28:29.969941: step 2011, loss 0.0500039, acc 0.98
2016-09-05T19:28:30.759239: step 2012, loss 0.0605155, acc 0.96
2016-09-05T19:28:31.587594: step 2013, loss 0.0105468, acc 1
2016-09-05T19:28:32.373999: step 2014, loss 0.0709242, acc 0.98
2016-09-05T19:28:33.185837: step 2015, loss 0.0480438, acc 0.98
2016-09-05T19:28:34.010718: step 2016, loss 0.0605864, acc 0.98
2016-09-05T19:28:34.824185: step 2017, loss 0.0402478, acc 0.98
2016-09-05T19:28:35.621847: step 2018, loss 0.0646869, acc 0.98
2016-09-05T19:28:36.458769: step 2019, loss 0.0188943, acc 1
2016-09-05T19:28:37.227632: step 2020, loss 0.106243, acc 0.94
2016-09-05T19:28:38.027291: step 2021, loss 0.100072, acc 0.98
2016-09-05T19:28:38.875884: step 2022, loss 0.0176811, acc 1
2016-09-05T19:28:39.661437: step 2023, loss 0.0533059, acc 0.98
2016-09-05T19:28:40.461460: step 2024, loss 0.0468632, acc 0.98
2016-09-05T19:28:41.293187: step 2025, loss 0.0135666, acc 1
2016-09-05T19:28:42.083709: step 2026, loss 0.060003, acc 0.98
2016-09-05T19:28:42.861444: step 2027, loss 0.0399389, acc 0.98
2016-09-05T19:28:43.698281: step 2028, loss 0.195151, acc 0.96
2016-09-05T19:28:44.491858: step 2029, loss 0.00942166, acc 1
2016-09-05T19:28:45.268941: step 2030, loss 0.111726, acc 0.94
2016-09-05T19:28:46.083390: step 2031, loss 0.107355, acc 0.96
2016-09-05T19:28:46.897409: step 2032, loss 0.0337324, acc 0.98
2016-09-05T19:28:47.686897: step 2033, loss 0.0276456, acc 1
2016-09-05T19:28:48.531126: step 2034, loss 0.115199, acc 0.98
2016-09-05T19:28:49.332826: step 2035, loss 0.0552154, acc 0.98
2016-09-05T19:28:50.154534: step 2036, loss 0.0386639, acc 0.98
2016-09-05T19:28:50.956641: step 2037, loss 0.0599317, acc 0.98
2016-09-05T19:28:51.737656: step 2038, loss 0.041128, acc 1
2016-09-05T19:28:52.527145: step 2039, loss 0.0557638, acc 1
2016-09-05T19:28:53.376024: step 2040, loss 0.0581449, acc 0.98
2016-09-05T19:28:54.165038: step 2041, loss 0.0945961, acc 0.96
2016-09-05T19:28:54.971639: step 2042, loss 0.0288839, acc 1
2016-09-05T19:28:55.784008: step 2043, loss 0.0431949, acc 0.98
2016-09-05T19:28:56.550505: step 2044, loss 0.193988, acc 0.94
2016-09-05T19:28:57.340699: step 2045, loss 0.011129, acc 1
2016-09-05T19:28:58.149122: step 2046, loss 0.124616, acc 0.96
2016-09-05T19:28:58.929618: step 2047, loss 0.0694573, acc 0.96
2016-09-05T19:28:59.746562: step 2048, loss 0.0406627, acc 0.98
2016-09-05T19:29:00.589459: step 2049, loss 0.0794213, acc 0.98
2016-09-05T19:29:01.382947: step 2050, loss 0.0519911, acc 0.98
2016-09-05T19:29:02.190222: step 2051, loss 0.0749918, acc 0.98
2016-09-05T19:29:03.039320: step 2052, loss 0.0562271, acc 0.98
2016-09-05T19:29:03.802210: step 2053, loss 0.0423554, acc 1
2016-09-05T19:29:04.581944: step 2054, loss 0.0430156, acc 1
2016-09-05T19:29:05.390696: step 2055, loss 0.0415476, acc 0.98
2016-09-05T19:29:06.178662: step 2056, loss 0.0912278, acc 0.94
2016-09-05T19:29:06.974739: step 2057, loss 0.0084178, acc 1
2016-09-05T19:29:07.800110: step 2058, loss 0.0585791, acc 0.98
2016-09-05T19:29:08.578741: step 2059, loss 0.026788, acc 1
2016-09-05T19:29:09.398782: step 2060, loss 0.0679945, acc 0.98
2016-09-05T19:29:10.222657: step 2061, loss 0.0168036, acc 1
2016-09-05T19:29:11.051258: step 2062, loss 0.103644, acc 0.96
2016-09-05T19:29:11.858870: step 2063, loss 0.0712531, acc 0.96
2016-09-05T19:29:12.681045: step 2064, loss 0.0239422, acc 1
2016-09-05T19:29:13.481400: step 2065, loss 0.0918551, acc 0.98
2016-09-05T19:29:14.277442: step 2066, loss 0.0550006, acc 0.98
2016-09-05T19:29:15.119531: step 2067, loss 0.069124, acc 0.96
2016-09-05T19:29:15.926907: step 2068, loss 0.0622957, acc 0.98
2016-09-05T19:29:16.731054: step 2069, loss 0.0145339, acc 1
2016-09-05T19:29:17.557260: step 2070, loss 0.0534729, acc 0.96
2016-09-05T19:29:18.371231: step 2071, loss 0.0979, acc 0.92
2016-09-05T19:29:19.183907: step 2072, loss 0.0934875, acc 0.96
2016-09-05T19:29:20.017424: step 2073, loss 0.0182062, acc 1
2016-09-05T19:29:20.805951: step 2074, loss 0.0203703, acc 1
2016-09-05T19:29:21.627821: step 2075, loss 0.0731141, acc 0.96
2016-09-05T19:29:22.476634: step 2076, loss 0.0480208, acc 0.98
2016-09-05T19:29:23.293396: step 2077, loss 0.109789, acc 0.98
2016-09-05T19:29:24.110628: step 2078, loss 0.00700101, acc 1
2016-09-05T19:29:24.953424: step 2079, loss 0.040323, acc 1
2016-09-05T19:29:25.775919: step 2080, loss 0.0455268, acc 0.96
2016-09-05T19:29:26.574684: step 2081, loss 0.0198387, acc 1
2016-09-05T19:29:27.405604: step 2082, loss 0.035269, acc 1
2016-09-05T19:29:28.214285: step 2083, loss 0.0197909, acc 1
2016-09-05T19:29:29.043565: step 2084, loss 0.0383127, acc 0.98
2016-09-05T19:29:29.882746: step 2085, loss 0.0412391, acc 0.98
2016-09-05T19:29:30.693747: step 2086, loss 0.0371913, acc 0.98
2016-09-05T19:29:31.537438: step 2087, loss 0.0704739, acc 0.98
2016-09-05T19:29:32.346114: step 2088, loss 0.0869097, acc 0.96
2016-09-05T19:29:33.151505: step 2089, loss 0.0618936, acc 0.96
2016-09-05T19:29:33.966162: step 2090, loss 0.00674173, acc 1
2016-09-05T19:29:34.785503: step 2091, loss 0.179975, acc 0.94
2016-09-05T19:29:35.605532: step 2092, loss 0.0500968, acc 1
2016-09-05T19:29:36.402351: step 2093, loss 0.0477842, acc 0.98
2016-09-05T19:29:37.209516: step 2094, loss 0.160827, acc 0.92
2016-09-05T19:29:38.020720: step 2095, loss 0.0453715, acc 0.98
2016-09-05T19:29:38.828835: step 2096, loss 0.0354294, acc 0.98
2016-09-05T19:29:39.634292: step 2097, loss 0.0105104, acc 1
2016-09-05T19:29:40.455849: step 2098, loss 0.0360161, acc 0.98
2016-09-05T19:29:41.251657: step 2099, loss 0.127328, acc 0.96
2016-09-05T19:29:42.060371: step 2100, loss 0.0316564, acc 1

Evaluation:
2016-09-05T19:29:45.562992: step 2100, loss 1.1403, acc 0.758

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-2100

2016-09-05T19:29:47.438454: step 2101, loss 0.0380297, acc 0.96
2016-09-05T19:29:48.268217: step 2102, loss 0.0326881, acc 1
2016-09-05T19:29:49.094689: step 2103, loss 0.00567345, acc 1
2016-09-05T19:29:49.915174: step 2104, loss 0.0838078, acc 0.96
2016-09-05T19:29:50.732608: step 2105, loss 0.0319309, acc 0.98
2016-09-05T19:29:51.537261: step 2106, loss 0.0414007, acc 1
2016-09-05T19:29:52.343547: step 2107, loss 0.0624214, acc 0.96
2016-09-05T19:29:53.164111: step 2108, loss 0.0160181, acc 1
2016-09-05T19:29:53.979168: step 2109, loss 0.0776448, acc 0.96
2016-09-05T19:29:54.787901: step 2110, loss 0.043166, acc 0.96
2016-09-05T19:29:55.613427: step 2111, loss 0.114978, acc 0.94
2016-09-05T19:29:56.436471: step 2112, loss 0.101469, acc 0.96
2016-09-05T19:29:57.256305: step 2113, loss 0.0420166, acc 0.98
2016-09-05T19:29:58.083089: step 2114, loss 0.0803698, acc 0.96
2016-09-05T19:29:58.907223: step 2115, loss 0.113586, acc 0.94
2016-09-05T19:29:59.711022: step 2116, loss 0.0396658, acc 0.98
2016-09-05T19:30:00.546732: step 2117, loss 0.137521, acc 0.96
2016-09-05T19:30:01.368852: step 2118, loss 0.0186395, acc 1
2016-09-05T19:30:02.185649: step 2119, loss 0.0768389, acc 0.96
2016-09-05T19:30:02.986346: step 2120, loss 0.0378964, acc 1
2016-09-05T19:30:03.783910: step 2121, loss 0.0381187, acc 1
2016-09-05T19:30:04.591784: step 2122, loss 0.044386, acc 0.98
2016-09-05T19:30:05.405832: step 2123, loss 0.0710485, acc 0.98
2016-09-05T19:30:06.229986: step 2124, loss 0.0222546, acc 1
2016-09-05T19:30:07.068709: step 2125, loss 0.0187248, acc 1
2016-09-05T19:30:07.865551: step 2126, loss 0.0363296, acc 0.98
2016-09-05T19:30:08.657880: step 2127, loss 0.166109, acc 0.94
2016-09-05T19:30:09.471202: step 2128, loss 0.0382531, acc 0.98
2016-09-05T19:30:10.265038: step 2129, loss 0.077877, acc 0.96
2016-09-05T19:30:11.068737: step 2130, loss 0.0212782, acc 1
2016-09-05T19:30:11.881742: step 2131, loss 0.0506536, acc 1
2016-09-05T19:30:12.659991: step 2132, loss 0.0963203, acc 0.98
2016-09-05T19:30:13.447598: step 2133, loss 0.0193888, acc 1
2016-09-05T19:30:13.854911: step 2134, loss 0.00524759, acc 1
2016-09-05T19:30:14.664296: step 2135, loss 0.0343726, acc 0.98
2016-09-05T19:30:15.469072: step 2136, loss 0.0541326, acc 0.98
2016-09-05T19:30:16.264168: step 2137, loss 0.0383374, acc 0.98
2016-09-05T19:30:17.089253: step 2138, loss 0.064062, acc 0.96
2016-09-05T19:30:17.916271: step 2139, loss 0.0513553, acc 0.98
2016-09-05T19:30:18.713802: step 2140, loss 0.0438674, acc 1
2016-09-05T19:30:19.507702: step 2141, loss 0.0246346, acc 0.98
2016-09-05T19:30:20.299684: step 2142, loss 0.0181126, acc 1
2016-09-05T19:30:21.080176: step 2143, loss 0.0135208, acc 1
2016-09-05T19:30:21.914162: step 2144, loss 0.0319934, acc 0.98
2016-09-05T19:30:22.790936: step 2145, loss 0.0622443, acc 0.98
2016-09-05T19:30:23.569020: step 2146, loss 0.0474577, acc 0.98
2016-09-05T19:30:24.372999: step 2147, loss 0.0236879, acc 1
2016-09-05T19:30:25.206786: step 2148, loss 0.0888134, acc 0.96
2016-09-05T19:30:26.006817: step 2149, loss 0.0159243, acc 1
2016-09-05T19:30:26.818649: step 2150, loss 0.10082, acc 0.96
2016-09-05T19:30:27.603281: step 2151, loss 0.0570975, acc 0.98
2016-09-05T19:30:28.396299: step 2152, loss 0.0169748, acc 1
2016-09-05T19:30:29.202812: step 2153, loss 0.0443717, acc 0.98
2016-09-05T19:30:30.007175: step 2154, loss 0.040764, acc 0.96
2016-09-05T19:30:30.797939: step 2155, loss 0.0386951, acc 0.98
2016-09-05T19:30:31.613349: step 2156, loss 0.0276904, acc 1
2016-09-05T19:30:32.423618: step 2157, loss 0.127004, acc 0.96
2016-09-05T19:30:33.230132: step 2158, loss 0.143749, acc 0.94
2016-09-05T19:30:34.018275: step 2159, loss 0.0538712, acc 0.96
2016-09-05T19:30:34.836055: step 2160, loss 0.0540339, acc 0.98
2016-09-05T19:30:35.605667: step 2161, loss 0.0237421, acc 1
2016-09-05T19:30:36.415684: step 2162, loss 0.00799077, acc 1
2016-09-05T19:30:37.235418: step 2163, loss 0.0505541, acc 0.98
2016-09-05T19:30:38.046452: step 2164, loss 0.0180218, acc 1
2016-09-05T19:30:38.882233: step 2165, loss 0.114546, acc 0.92
2016-09-05T19:30:39.704818: step 2166, loss 0.110052, acc 0.94
2016-09-05T19:30:40.510239: step 2167, loss 0.0143489, acc 1
2016-09-05T19:30:41.305824: step 2168, loss 0.0756583, acc 0.98
2016-09-05T19:30:42.114675: step 2169, loss 0.0430303, acc 0.98
2016-09-05T19:30:42.888303: step 2170, loss 0.014468, acc 1
2016-09-05T19:30:43.684434: step 2171, loss 0.0420553, acc 0.98
2016-09-05T19:30:44.492771: step 2172, loss 0.0945949, acc 0.98
2016-09-05T19:30:45.268121: step 2173, loss 0.0436559, acc 0.98
2016-09-05T19:30:46.102904: step 2174, loss 0.0273259, acc 0.98
2016-09-05T19:30:46.921381: step 2175, loss 0.088227, acc 0.98
2016-09-05T19:30:47.703985: step 2176, loss 0.0493561, acc 0.98
2016-09-05T19:30:48.496708: step 2177, loss 0.0354644, acc 1
2016-09-05T19:30:49.335186: step 2178, loss 0.0236679, acc 1
2016-09-05T19:30:50.134042: step 2179, loss 0.0847344, acc 0.96
2016-09-05T19:30:50.944046: step 2180, loss 0.0443723, acc 0.96
2016-09-05T19:30:51.772115: step 2181, loss 0.0440943, acc 0.96
2016-09-05T19:30:52.570235: step 2182, loss 0.072761, acc 0.98
2016-09-05T19:30:53.359750: step 2183, loss 0.0497455, acc 0.98
2016-09-05T19:30:54.177508: step 2184, loss 0.09602, acc 0.98
2016-09-05T19:30:54.973193: step 2185, loss 0.027156, acc 1
2016-09-05T19:30:55.786473: step 2186, loss 0.0229676, acc 0.98
2016-09-05T19:30:56.621040: step 2187, loss 0.0262553, acc 0.98
2016-09-05T19:30:57.411244: step 2188, loss 0.0544854, acc 0.98
2016-09-05T19:30:58.209874: step 2189, loss 0.0456632, acc 0.98
2016-09-05T19:30:59.017639: step 2190, loss 0.0893067, acc 0.96
2016-09-05T19:30:59.811925: step 2191, loss 0.040541, acc 0.98
2016-09-05T19:31:00.619477: step 2192, loss 0.0412628, acc 0.98
2016-09-05T19:31:01.416870: step 2193, loss 0.0141025, acc 1
2016-09-05T19:31:02.198618: step 2194, loss 0.0345659, acc 1
2016-09-05T19:31:03.027342: step 2195, loss 0.0555516, acc 0.96
2016-09-05T19:31:03.825774: step 2196, loss 0.0297539, acc 0.98
2016-09-05T19:31:04.605980: step 2197, loss 0.153583, acc 0.94
2016-09-05T19:31:05.416157: step 2198, loss 0.0810968, acc 0.96
2016-09-05T19:31:06.218395: step 2199, loss 0.013208, acc 1
2016-09-05T19:31:07.027324: step 2200, loss 0.0698132, acc 0.98

Evaluation:
2016-09-05T19:31:10.553408: step 2200, loss 1.26144, acc 0.75

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-2200

2016-09-05T19:31:12.443543: step 2201, loss 0.057628, acc 0.98
2016-09-05T19:31:13.242947: step 2202, loss 0.0271755, acc 1
2016-09-05T19:31:14.059862: step 2203, loss 0.0319494, acc 0.98
2016-09-05T19:31:14.886966: step 2204, loss 0.0681926, acc 0.92
2016-09-05T19:31:15.703143: step 2205, loss 0.0265228, acc 1
2016-09-05T19:31:16.501116: step 2206, loss 0.0050457, acc 1
2016-09-05T19:31:17.331062: step 2207, loss 0.0157442, acc 1
2016-09-05T19:31:18.141424: step 2208, loss 0.0196979, acc 1
2016-09-05T19:31:18.945981: step 2209, loss 0.103073, acc 0.96
2016-09-05T19:31:19.758456: step 2210, loss 0.0469301, acc 0.96
2016-09-05T19:31:20.562098: step 2211, loss 0.0565697, acc 0.98
2016-09-05T19:31:21.364152: step 2212, loss 0.0265823, acc 1
2016-09-05T19:31:22.190178: step 2213, loss 0.040882, acc 1
2016-09-05T19:31:22.983525: step 2214, loss 0.0615391, acc 0.98
2016-09-05T19:31:23.805722: step 2215, loss 0.0490764, acc 0.98
2016-09-05T19:31:24.618336: step 2216, loss 0.0573182, acc 0.98
2016-09-05T19:31:25.425689: step 2217, loss 0.0465292, acc 0.98
2016-09-05T19:31:26.241795: step 2218, loss 0.0920571, acc 0.96
2016-09-05T19:31:27.050031: step 2219, loss 0.0306762, acc 1
2016-09-05T19:31:27.863280: step 2220, loss 0.0766353, acc 0.96
2016-09-05T19:31:28.668237: step 2221, loss 0.0753892, acc 0.98
2016-09-05T19:31:29.491022: step 2222, loss 0.028607, acc 1
2016-09-05T19:31:30.282938: step 2223, loss 0.0555606, acc 0.98
2016-09-05T19:31:31.087433: step 2224, loss 0.0484279, acc 0.98
2016-09-05T19:31:31.913503: step 2225, loss 0.0205176, acc 1
2016-09-05T19:31:32.723018: step 2226, loss 0.0399823, acc 1
2016-09-05T19:31:33.528859: step 2227, loss 0.0261867, acc 0.98
2016-09-05T19:31:34.369228: step 2228, loss 0.045681, acc 0.98
2016-09-05T19:31:35.188511: step 2229, loss 0.037593, acc 1
2016-09-05T19:31:36.025984: step 2230, loss 0.0616517, acc 0.96
2016-09-05T19:31:36.843825: step 2231, loss 0.0233662, acc 0.98
2016-09-05T19:31:37.639778: step 2232, loss 0.0502157, acc 0.96
2016-09-05T19:31:38.455924: step 2233, loss 0.0213917, acc 0.98
2016-09-05T19:31:39.307248: step 2234, loss 0.0177528, acc 1
2016-09-05T19:31:40.087957: step 2235, loss 0.0407928, acc 0.98
2016-09-05T19:31:40.900210: step 2236, loss 0.0460659, acc 0.96
2016-09-05T19:31:41.763150: step 2237, loss 0.0581151, acc 0.94
2016-09-05T19:31:42.571275: step 2238, loss 0.041339, acc 1
2016-09-05T19:31:43.393825: step 2239, loss 0.0417549, acc 0.96
2016-09-05T19:31:44.218512: step 2240, loss 0.0363265, acc 0.98
2016-09-05T19:31:45.030974: step 2241, loss 0.0329275, acc 0.98
2016-09-05T19:31:45.819249: step 2242, loss 0.0554311, acc 0.94
2016-09-05T19:31:46.619516: step 2243, loss 0.0339222, acc 0.98
2016-09-05T19:31:47.461547: step 2244, loss 0.18945, acc 0.96
2016-09-05T19:31:48.241391: step 2245, loss 0.0458158, acc 1
2016-09-05T19:31:49.049405: step 2246, loss 0.0127263, acc 1
2016-09-05T19:31:49.858082: step 2247, loss 0.0770755, acc 0.98
2016-09-05T19:31:50.643376: step 2248, loss 0.0310901, acc 1
2016-09-05T19:31:51.467647: step 2249, loss 0.0386488, acc 0.96
2016-09-05T19:31:52.266148: step 2250, loss 0.074304, acc 0.96
2016-09-05T19:31:53.067117: step 2251, loss 0.00649457, acc 1
2016-09-05T19:31:53.866311: step 2252, loss 0.0951835, acc 0.98
2016-09-05T19:31:54.663757: step 2253, loss 0.0263316, acc 1
2016-09-05T19:31:55.453225: step 2254, loss 0.126419, acc 0.96
2016-09-05T19:31:56.245698: step 2255, loss 0.129794, acc 0.96
2016-09-05T19:31:57.052502: step 2256, loss 0.0431971, acc 0.98
2016-09-05T19:31:57.829436: step 2257, loss 0.0187533, acc 1
2016-09-05T19:31:58.648300: step 2258, loss 0.0567946, acc 0.98
2016-09-05T19:31:59.453392: step 2259, loss 0.0514018, acc 0.96
2016-09-05T19:32:00.229946: step 2260, loss 0.0581667, acc 0.98
2016-09-05T19:32:01.059965: step 2261, loss 0.0312091, acc 0.98
2016-09-05T19:32:01.883263: step 2262, loss 0.0230061, acc 1
2016-09-05T19:32:02.708404: step 2263, loss 0.0197967, acc 1
2016-09-05T19:32:03.509719: step 2264, loss 0.18817, acc 0.96
2016-09-05T19:32:04.340005: step 2265, loss 0.0243148, acc 1
2016-09-05T19:32:05.136559: step 2266, loss 0.0738158, acc 0.96
2016-09-05T19:32:05.928411: step 2267, loss 0.122413, acc 0.96
2016-09-05T19:32:06.745864: step 2268, loss 0.0604532, acc 0.96
2016-09-05T19:32:07.576008: step 2269, loss 0.0215119, acc 1
2016-09-05T19:32:08.384767: step 2270, loss 0.0452655, acc 0.98
2016-09-05T19:32:09.202772: step 2271, loss 0.0561583, acc 0.98
2016-09-05T19:32:09.974303: step 2272, loss 0.0271078, acc 1
2016-09-05T19:32:10.786012: step 2273, loss 0.0405938, acc 1
2016-09-05T19:32:11.597455: step 2274, loss 0.212019, acc 0.94
2016-09-05T19:32:12.405647: step 2275, loss 0.0922928, acc 0.96
2016-09-05T19:32:13.213408: step 2276, loss 0.0782405, acc 0.96
2016-09-05T19:32:14.032054: step 2277, loss 0.0428487, acc 0.98
2016-09-05T19:32:14.831667: step 2278, loss 0.0291743, acc 0.98
2016-09-05T19:32:15.645549: step 2279, loss 0.0699058, acc 0.96
2016-09-05T19:32:16.478218: step 2280, loss 0.0466512, acc 1
2016-09-05T19:32:17.296468: step 2281, loss 0.224088, acc 0.9
2016-09-05T19:32:18.098806: step 2282, loss 0.0869832, acc 0.94
2016-09-05T19:32:18.913588: step 2283, loss 0.107149, acc 0.96
2016-09-05T19:32:19.698056: step 2284, loss 0.0335815, acc 1
2016-09-05T19:32:20.498538: step 2285, loss 0.0624537, acc 0.96
2016-09-05T19:32:21.295432: step 2286, loss 0.0505767, acc 0.98
2016-09-05T19:32:22.120733: step 2287, loss 0.0724543, acc 0.96
2016-09-05T19:32:22.937760: step 2288, loss 0.0590679, acc 0.96
2016-09-05T19:32:23.757935: step 2289, loss 0.107481, acc 0.92
2016-09-05T19:32:24.582413: step 2290, loss 0.0750871, acc 0.98
2016-09-05T19:32:25.391507: step 2291, loss 0.0517872, acc 1
2016-09-05T19:32:26.245640: step 2292, loss 0.0377193, acc 0.96
2016-09-05T19:32:27.049161: step 2293, loss 0.0968438, acc 0.98
2016-09-05T19:32:27.875971: step 2294, loss 0.0840044, acc 0.94
2016-09-05T19:32:28.738693: step 2295, loss 0.0175695, acc 1
2016-09-05T19:32:29.549207: step 2296, loss 0.0575764, acc 0.98
2016-09-05T19:32:30.366675: step 2297, loss 0.0390476, acc 1
2016-09-05T19:32:31.188362: step 2298, loss 0.0335922, acc 1
2016-09-05T19:32:31.996581: step 2299, loss 0.0460804, acc 0.98
2016-09-05T19:32:32.785378: step 2300, loss 0.158793, acc 0.92

Evaluation:
2016-09-05T19:32:36.272587: step 2300, loss 1.18842, acc 0.761

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-2300

2016-09-05T19:32:38.150055: step 2301, loss 0.036734, acc 0.98
2016-09-05T19:32:38.975634: step 2302, loss 0.0389979, acc 0.98
2016-09-05T19:32:39.818275: step 2303, loss 0.0687566, acc 0.98
2016-09-05T19:32:40.630336: step 2304, loss 0.180721, acc 0.96
2016-09-05T19:32:41.414229: step 2305, loss 0.0414541, acc 0.98
2016-09-05T19:32:42.227714: step 2306, loss 0.0283601, acc 1
2016-09-05T19:32:43.038456: step 2307, loss 0.0177615, acc 1
2016-09-05T19:32:43.819087: step 2308, loss 0.0630193, acc 0.98
2016-09-05T19:32:44.613966: step 2309, loss 0.0517331, acc 1
2016-09-05T19:32:45.432782: step 2310, loss 0.0560475, acc 0.98
2016-09-05T19:32:46.223645: step 2311, loss 0.0795005, acc 0.96
2016-09-05T19:32:47.030818: step 2312, loss 0.0595677, acc 0.98
2016-09-05T19:32:47.832228: step 2313, loss 0.0221035, acc 1
2016-09-05T19:32:48.619790: step 2314, loss 0.0473171, acc 0.98
2016-09-05T19:32:49.442836: step 2315, loss 0.0703614, acc 0.96
2016-09-05T19:32:50.247081: step 2316, loss 0.0545543, acc 0.96
2016-09-05T19:32:51.025884: step 2317, loss 0.01701, acc 1
2016-09-05T19:32:51.866198: step 2318, loss 0.0548676, acc 0.96
2016-09-05T19:32:52.684295: step 2319, loss 0.141065, acc 0.96
2016-09-05T19:32:53.469386: step 2320, loss 0.108892, acc 0.96
2016-09-05T19:32:54.275185: step 2321, loss 0.0627763, acc 0.96
2016-09-05T19:32:55.078894: step 2322, loss 0.0242679, acc 1
2016-09-05T19:32:55.889407: step 2323, loss 0.0441248, acc 0.98
2016-09-05T19:32:56.686861: step 2324, loss 0.060746, acc 0.98
2016-09-05T19:32:57.505788: step 2325, loss 0.174518, acc 0.94
2016-09-05T19:32:58.303520: step 2326, loss 0.0537522, acc 0.98
2016-09-05T19:32:59.099502: step 2327, loss 0.0262046, acc 1
2016-09-05T19:32:59.526265: step 2328, loss 0.0677813, acc 1
2016-09-05T19:33:00.361401: step 2329, loss 0.0371229, acc 0.98
2016-09-05T19:33:01.147086: step 2330, loss 0.0574351, acc 0.98
2016-09-05T19:33:01.924292: step 2331, loss 0.0786676, acc 0.96
2016-09-05T19:33:02.730584: step 2332, loss 0.038024, acc 0.98
2016-09-05T19:33:03.552425: step 2333, loss 0.116732, acc 0.98
2016-09-05T19:33:04.334902: step 2334, loss 0.04711, acc 0.98
2016-09-05T19:33:05.138616: step 2335, loss 0.033699, acc 0.98
2016-09-05T19:33:05.956578: step 2336, loss 0.0436099, acc 1
2016-09-05T19:33:06.735675: step 2337, loss 0.0701784, acc 0.96
2016-09-05T19:33:07.566628: step 2338, loss 0.0625199, acc 0.98
2016-09-05T19:33:08.409404: step 2339, loss 0.0766511, acc 0.98
2016-09-05T19:33:09.161872: step 2340, loss 0.0386616, acc 0.98
2016-09-05T19:33:09.967656: step 2341, loss 0.0187231, acc 1
2016-09-05T19:33:10.784915: step 2342, loss 0.04017, acc 0.98
2016-09-05T19:33:11.572619: step 2343, loss 0.0104972, acc 1
2016-09-05T19:33:12.382145: step 2344, loss 0.0152005, acc 1
2016-09-05T19:33:13.232045: step 2345, loss 0.0529384, acc 0.98
2016-09-05T19:33:14.035160: step 2346, loss 0.1527, acc 0.96
2016-09-05T19:33:14.851942: step 2347, loss 0.024556, acc 1
2016-09-05T19:33:15.701773: step 2348, loss 0.0133187, acc 1
2016-09-05T19:33:16.526508: step 2349, loss 0.0150009, acc 1
2016-09-05T19:33:17.318172: step 2350, loss 0.0812746, acc 0.96
2016-09-05T19:33:18.122470: step 2351, loss 0.0175555, acc 1
2016-09-05T19:33:18.905921: step 2352, loss 0.0194971, acc 1
2016-09-05T19:33:19.688248: step 2353, loss 0.0276517, acc 0.98
2016-09-05T19:33:20.512817: step 2354, loss 0.028331, acc 0.98
2016-09-05T19:33:21.278148: step 2355, loss 0.0245269, acc 0.98
2016-09-05T19:33:22.083673: step 2356, loss 0.00522861, acc 1
2016-09-05T19:33:22.935357: step 2357, loss 0.0500259, acc 0.98
2016-09-05T19:33:23.741521: step 2358, loss 0.0688869, acc 0.96
2016-09-05T19:33:24.549329: step 2359, loss 0.0994461, acc 0.96
2016-09-05T19:33:25.358955: step 2360, loss 0.0744319, acc 0.98
2016-09-05T19:33:26.141487: step 2361, loss 0.0725732, acc 0.98
2016-09-05T19:33:26.949417: step 2362, loss 0.118262, acc 0.94
2016-09-05T19:33:27.774719: step 2363, loss 0.00890193, acc 1
2016-09-05T19:33:28.561492: step 2364, loss 0.0595835, acc 0.98
2016-09-05T19:33:29.343372: step 2365, loss 0.0271048, acc 0.98
2016-09-05T19:33:30.144336: step 2366, loss 0.134496, acc 0.94
2016-09-05T19:33:30.936911: step 2367, loss 0.027185, acc 1
2016-09-05T19:33:31.753204: step 2368, loss 0.116706, acc 0.98
2016-09-05T19:33:32.570231: step 2369, loss 0.052839, acc 0.96
2016-09-05T19:33:33.355598: step 2370, loss 0.0800686, acc 0.96
2016-09-05T19:33:34.178633: step 2371, loss 0.0385475, acc 1
2016-09-05T19:33:35.016681: step 2372, loss 0.0602658, acc 0.96
2016-09-05T19:33:35.801397: step 2373, loss 0.0507449, acc 1
2016-09-05T19:33:36.598114: step 2374, loss 0.0315075, acc 0.98
2016-09-05T19:33:37.439681: step 2375, loss 0.0605019, acc 0.98
2016-09-05T19:33:38.210625: step 2376, loss 0.104928, acc 0.94
2016-09-05T19:33:38.992654: step 2377, loss 0.02119, acc 1
2016-09-05T19:33:39.817367: step 2378, loss 0.0150277, acc 1
2016-09-05T19:33:40.597324: step 2379, loss 0.0278849, acc 0.98
2016-09-05T19:33:41.403019: step 2380, loss 0.046125, acc 0.96
2016-09-05T19:33:42.249557: step 2381, loss 0.0224557, acc 1
2016-09-05T19:33:43.063295: step 2382, loss 0.00936043, acc 1
2016-09-05T19:33:43.884804: step 2383, loss 0.0446403, acc 0.98
2016-09-05T19:33:44.685351: step 2384, loss 0.031323, acc 1
2016-09-05T19:33:45.468292: step 2385, loss 0.0167844, acc 1
2016-09-05T19:33:46.264654: step 2386, loss 0.0603272, acc 0.96
2016-09-05T19:33:47.102786: step 2387, loss 0.0297542, acc 0.98
2016-09-05T19:33:47.906487: step 2388, loss 0.0335693, acc 0.98
2016-09-05T19:33:48.716202: step 2389, loss 0.0529815, acc 0.98
2016-09-05T19:33:49.491771: step 2390, loss 0.0354584, acc 0.98
2016-09-05T19:33:50.280154: step 2391, loss 0.0343401, acc 1
2016-09-05T19:33:51.118360: step 2392, loss 0.0415296, acc 1
2016-09-05T19:33:51.935372: step 2393, loss 0.147461, acc 0.94
2016-09-05T19:33:52.705909: step 2394, loss 0.0286338, acc 1
2016-09-05T19:33:53.493738: step 2395, loss 0.0613941, acc 0.96
2016-09-05T19:33:54.328439: step 2396, loss 0.024042, acc 1
2016-09-05T19:33:55.106875: step 2397, loss 0.0253422, acc 1
2016-09-05T19:33:55.899196: step 2398, loss 0.0751979, acc 0.94
2016-09-05T19:33:56.705329: step 2399, loss 0.067645, acc 0.98
2016-09-05T19:33:57.488498: step 2400, loss 0.0438952, acc 0.98

Evaluation:
2016-09-05T19:34:00.978357: step 2400, loss 1.7393, acc 0.741

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-2400

2016-09-05T19:34:02.940395: step 2401, loss 0.0238698, acc 1
2016-09-05T19:34:03.724008: step 2402, loss 0.0661624, acc 0.98
2016-09-05T19:34:04.516602: step 2403, loss 0.0382578, acc 0.98
2016-09-05T19:34:05.356510: step 2404, loss 0.104193, acc 0.96
2016-09-05T19:34:06.170490: step 2405, loss 0.166021, acc 0.96
2016-09-05T19:34:06.970815: step 2406, loss 0.0724534, acc 0.98
2016-09-05T19:34:07.763708: step 2407, loss 0.0645813, acc 0.96
2016-09-05T19:34:08.555422: step 2408, loss 0.07823, acc 0.96
2016-09-05T19:34:09.324581: step 2409, loss 0.0374448, acc 0.98
2016-09-05T19:34:10.136000: step 2410, loss 0.00507772, acc 1
2016-09-05T19:34:10.940631: step 2411, loss 0.0381575, acc 0.98
2016-09-05T19:34:11.769562: step 2412, loss 0.0178088, acc 1
2016-09-05T19:34:12.563402: step 2413, loss 0.0197587, acc 0.98
2016-09-05T19:34:13.371818: step 2414, loss 0.0260492, acc 1
2016-09-05T19:34:14.156798: step 2415, loss 0.00930538, acc 1
2016-09-05T19:34:14.971442: step 2416, loss 0.00808179, acc 1
2016-09-05T19:34:15.753618: step 2417, loss 0.0652113, acc 0.98
2016-09-05T19:34:16.547742: step 2418, loss 0.0393592, acc 1
2016-09-05T19:34:17.353808: step 2419, loss 0.137343, acc 0.96
2016-09-05T19:34:18.160512: step 2420, loss 0.0515788, acc 0.96
2016-09-05T19:34:18.977932: step 2421, loss 0.0236917, acc 1
2016-09-05T19:34:19.801479: step 2422, loss 0.016836, acc 1
2016-09-05T19:34:20.577860: step 2423, loss 0.0326896, acc 0.98
2016-09-05T19:34:21.413121: step 2424, loss 0.0760678, acc 0.98
2016-09-05T19:34:22.225060: step 2425, loss 0.010877, acc 1
2016-09-05T19:34:23.005770: step 2426, loss 0.0662199, acc 0.98
2016-09-05T19:34:23.806656: step 2427, loss 0.0365702, acc 0.98
2016-09-05T19:34:24.640442: step 2428, loss 0.032677, acc 1
2016-09-05T19:34:25.444727: step 2429, loss 0.025708, acc 1
2016-09-05T19:34:26.244007: step 2430, loss 0.0698964, acc 0.96
2016-09-05T19:34:27.066750: step 2431, loss 0.0649403, acc 0.98
2016-09-05T19:34:27.849414: step 2432, loss 0.0172873, acc 1
2016-09-05T19:34:28.644894: step 2433, loss 0.105428, acc 0.94
2016-09-05T19:34:29.442867: step 2434, loss 0.098653, acc 0.96
2016-09-05T19:34:30.242745: step 2435, loss 0.152177, acc 0.96
2016-09-05T19:34:31.081727: step 2436, loss 0.0245864, acc 1
2016-09-05T19:34:31.914642: step 2437, loss 0.0473954, acc 0.98
2016-09-05T19:34:32.720919: step 2438, loss 0.0197984, acc 1
2016-09-05T19:34:33.520386: step 2439, loss 0.119318, acc 0.96
2016-09-05T19:34:34.321152: step 2440, loss 0.0135243, acc 1
2016-09-05T19:34:35.102258: step 2441, loss 0.0652889, acc 0.98
2016-09-05T19:34:35.909141: step 2442, loss 0.111419, acc 0.94
2016-09-05T19:34:36.734196: step 2443, loss 0.153261, acc 0.96
2016-09-05T19:34:37.539976: step 2444, loss 0.0613757, acc 0.96
2016-09-05T19:34:38.384523: step 2445, loss 0.068524, acc 0.94
2016-09-05T19:34:39.210917: step 2446, loss 0.0448282, acc 1
2016-09-05T19:34:40.012277: step 2447, loss 0.0693137, acc 0.98
2016-09-05T19:34:40.823825: step 2448, loss 0.0675611, acc 0.98
2016-09-05T19:34:41.641500: step 2449, loss 0.0327486, acc 1
2016-09-05T19:34:42.434671: step 2450, loss 0.0656722, acc 0.98
2016-09-05T19:34:43.241061: step 2451, loss 0.0082954, acc 1
2016-09-05T19:34:44.057355: step 2452, loss 0.0316734, acc 1
2016-09-05T19:34:44.836241: step 2453, loss 0.0222144, acc 1
2016-09-05T19:34:45.638395: step 2454, loss 0.05363, acc 0.98
2016-09-05T19:34:46.452903: step 2455, loss 0.0475706, acc 0.98
2016-09-05T19:34:47.238644: step 2456, loss 0.0129192, acc 1
2016-09-05T19:34:48.029474: step 2457, loss 0.0939643, acc 0.94
2016-09-05T19:34:48.843882: step 2458, loss 0.0732114, acc 0.96
2016-09-05T19:34:49.615238: step 2459, loss 0.0117538, acc 1
2016-09-05T19:34:50.414306: step 2460, loss 0.0673398, acc 0.96
2016-09-05T19:34:51.224406: step 2461, loss 0.0222686, acc 0.98
2016-09-05T19:34:52.025842: step 2462, loss 0.0537307, acc 0.98
2016-09-05T19:34:52.836779: step 2463, loss 0.0387605, acc 0.98
2016-09-05T19:34:53.695525: step 2464, loss 0.038296, acc 0.98
2016-09-05T19:34:54.486301: step 2465, loss 0.0244853, acc 1
2016-09-05T19:34:55.273461: step 2466, loss 0.0265343, acc 1
2016-09-05T19:34:56.082638: step 2467, loss 0.0103514, acc 1
2016-09-05T19:34:56.859748: step 2468, loss 0.0506387, acc 1
2016-09-05T19:34:57.660346: step 2469, loss 0.0970099, acc 0.96
2016-09-05T19:34:58.480580: step 2470, loss 0.0518209, acc 0.98
2016-09-05T19:34:59.282123: step 2471, loss 0.051482, acc 0.98
2016-09-05T19:35:00.091162: step 2472, loss 0.0373164, acc 0.96
2016-09-05T19:35:00.928438: step 2473, loss 0.0378463, acc 0.98
2016-09-05T19:35:01.694067: step 2474, loss 0.0684414, acc 0.96
2016-09-05T19:35:02.489812: step 2475, loss 0.047703, acc 0.98
2016-09-05T19:35:03.293515: step 2476, loss 0.0223779, acc 1
2016-09-05T19:35:04.076545: step 2477, loss 0.105452, acc 0.98
2016-09-05T19:35:04.875997: step 2478, loss 0.116856, acc 0.94
2016-09-05T19:35:05.722749: step 2479, loss 0.0611005, acc 0.98
2016-09-05T19:35:06.524210: step 2480, loss 0.13542, acc 0.96
2016-09-05T19:35:07.363191: step 2481, loss 0.0587126, acc 0.96
2016-09-05T19:35:08.169896: step 2482, loss 0.097613, acc 0.98
2016-09-05T19:35:08.957033: step 2483, loss 0.126458, acc 0.98
2016-09-05T19:35:09.768000: step 2484, loss 0.00474872, acc 1
2016-09-05T19:35:10.601886: step 2485, loss 0.010275, acc 1
2016-09-05T19:35:11.415952: step 2486, loss 0.0514023, acc 1
2016-09-05T19:35:12.236183: step 2487, loss 0.0271618, acc 0.98
2016-09-05T19:35:13.064821: step 2488, loss 0.0792119, acc 0.96
2016-09-05T19:35:13.868312: step 2489, loss 0.144275, acc 0.96
2016-09-05T19:35:14.696977: step 2490, loss 0.0481055, acc 1
2016-09-05T19:35:15.516963: step 2491, loss 0.025711, acc 1
2016-09-05T19:35:16.307504: step 2492, loss 0.0425701, acc 1
2016-09-05T19:35:17.130709: step 2493, loss 0.0437256, acc 0.98
2016-09-05T19:35:17.955822: step 2494, loss 0.0537376, acc 0.96
2016-09-05T19:35:18.757501: step 2495, loss 0.0827289, acc 0.98
2016-09-05T19:35:19.530035: step 2496, loss 0.026608, acc 1
2016-09-05T19:35:20.399527: step 2497, loss 0.0363917, acc 0.98
2016-09-05T19:35:21.238655: step 2498, loss 0.081312, acc 0.94
2016-09-05T19:35:22.073445: step 2499, loss 0.051289, acc 0.96
2016-09-05T19:35:22.905715: step 2500, loss 0.103823, acc 0.92

Evaluation:
2016-09-05T19:35:26.428068: step 2500, loss 1.35775, acc 0.742

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-2500

2016-09-05T19:35:28.385979: step 2501, loss 0.0645439, acc 0.98
2016-09-05T19:35:29.192058: step 2502, loss 0.0344093, acc 0.98
2016-09-05T19:35:30.012900: step 2503, loss 0.0412967, acc 0.98
2016-09-05T19:35:30.805274: step 2504, loss 0.0349568, acc 0.98
2016-09-05T19:35:31.594777: step 2505, loss 0.0606342, acc 0.98
2016-09-05T19:35:32.397816: step 2506, loss 0.0340073, acc 0.98
2016-09-05T19:35:33.187123: step 2507, loss 0.0691814, acc 0.94
2016-09-05T19:35:34.012882: step 2508, loss 0.0468885, acc 0.98
2016-09-05T19:35:34.825183: step 2509, loss 0.0634622, acc 0.98
2016-09-05T19:35:35.595486: step 2510, loss 0.105253, acc 0.96
2016-09-05T19:35:36.412142: step 2511, loss 0.0147691, acc 1
2016-09-05T19:35:37.240979: step 2512, loss 0.0643901, acc 0.98
2016-09-05T19:35:38.044791: step 2513, loss 0.0833575, acc 0.96
2016-09-05T19:35:38.868496: step 2514, loss 0.0297184, acc 1
2016-09-05T19:35:39.701468: step 2515, loss 0.10525, acc 0.94
2016-09-05T19:35:40.491914: step 2516, loss 0.0352667, acc 1
2016-09-05T19:35:41.297500: step 2517, loss 0.0484353, acc 0.98
2016-09-05T19:35:42.135158: step 2518, loss 0.0202246, acc 0.98
2016-09-05T19:35:42.909068: step 2519, loss 0.0448056, acc 1
2016-09-05T19:35:43.689679: step 2520, loss 0.123735, acc 0.96
2016-09-05T19:35:44.493185: step 2521, loss 0.0131355, acc 1
2016-09-05T19:35:44.909659: step 2522, loss 0.0457676, acc 1
2016-09-05T19:35:45.731170: step 2523, loss 0.0151576, acc 1
2016-09-05T19:35:46.519101: step 2524, loss 0.0444489, acc 0.96
2016-09-05T19:35:47.354980: step 2525, loss 0.0483006, acc 0.98
2016-09-05T19:35:48.170429: step 2526, loss 0.0246022, acc 1
2016-09-05T19:35:48.970129: step 2527, loss 0.00806952, acc 1
2016-09-05T19:35:49.776769: step 2528, loss 0.190516, acc 0.92
2016-09-05T19:35:50.577563: step 2529, loss 0.0167042, acc 1
2016-09-05T19:35:51.361738: step 2530, loss 0.0359246, acc 1
2016-09-05T19:35:52.150544: step 2531, loss 0.0268055, acc 0.98
2016-09-05T19:35:52.973593: step 2532, loss 0.0212919, acc 1
2016-09-05T19:35:53.758016: step 2533, loss 0.0342402, acc 0.98
2016-09-05T19:35:54.547622: step 2534, loss 0.0153528, acc 1
2016-09-05T19:35:55.390847: step 2535, loss 0.0878392, acc 0.96
2016-09-05T19:35:56.180564: step 2536, loss 0.103798, acc 0.94
2016-09-05T19:35:56.992514: step 2537, loss 0.0112398, acc 1
2016-09-05T19:35:57.817462: step 2538, loss 0.0135912, acc 1
2016-09-05T19:35:58.599267: step 2539, loss 0.0694597, acc 0.96
2016-09-05T19:35:59.398762: step 2540, loss 0.00603371, acc 1
2016-09-05T19:36:00.230353: step 2541, loss 0.0202613, acc 1
2016-09-05T19:36:01.001567: step 2542, loss 0.0416566, acc 0.98
2016-09-05T19:36:01.849405: step 2543, loss 0.0349131, acc 1
2016-09-05T19:36:02.678740: step 2544, loss 0.0519999, acc 0.96
2016-09-05T19:36:03.480564: step 2545, loss 0.0234231, acc 1
2016-09-05T19:36:04.279585: step 2546, loss 0.0150747, acc 1
2016-09-05T19:36:05.123962: step 2547, loss 0.129633, acc 0.96
2016-09-05T19:36:05.947229: step 2548, loss 0.0241767, acc 1
2016-09-05T19:36:06.745948: step 2549, loss 0.0451624, acc 0.98
2016-09-05T19:36:07.613755: step 2550, loss 0.0833934, acc 0.96
2016-09-05T19:36:08.444830: step 2551, loss 0.081996, acc 0.98
2016-09-05T19:36:09.245147: step 2552, loss 0.0205124, acc 1
2016-09-05T19:36:10.068961: step 2553, loss 0.0655349, acc 0.96
2016-09-05T19:36:10.926532: step 2554, loss 0.0451015, acc 0.98
2016-09-05T19:36:11.755394: step 2555, loss 0.0505325, acc 0.96
2016-09-05T19:36:12.605276: step 2556, loss 0.00661881, acc 1
2016-09-05T19:36:13.425480: step 2557, loss 0.0194889, acc 1
2016-09-05T19:36:14.217492: step 2558, loss 0.120868, acc 0.98
2016-09-05T19:36:15.045544: step 2559, loss 0.0418823, acc 0.98
2016-09-05T19:36:15.850999: step 2560, loss 0.0690553, acc 0.96
2016-09-05T19:36:16.650046: step 2561, loss 0.0341793, acc 0.98
2016-09-05T19:36:17.469364: step 2562, loss 0.00825733, acc 1
2016-09-05T19:36:18.275230: step 2563, loss 0.101265, acc 0.96
2016-09-05T19:36:19.085515: step 2564, loss 0.0272261, acc 1
2016-09-05T19:36:19.897333: step 2565, loss 0.105537, acc 0.98
2016-09-05T19:36:20.700858: step 2566, loss 0.0211911, acc 1
2016-09-05T19:36:21.487932: step 2567, loss 0.0359555, acc 0.98
2016-09-05T19:36:22.325399: step 2568, loss 0.158096, acc 0.94
2016-09-05T19:36:23.160722: step 2569, loss 0.0578555, acc 0.98
2016-09-05T19:36:23.927845: step 2570, loss 0.0108733, acc 1
2016-09-05T19:36:24.737801: step 2571, loss 0.0130854, acc 1
2016-09-05T19:36:25.522158: step 2572, loss 0.0300704, acc 1
2016-09-05T19:36:26.304975: step 2573, loss 0.0276302, acc 1
2016-09-05T19:36:27.160802: step 2574, loss 0.0230492, acc 0.98
2016-09-05T19:36:27.963562: step 2575, loss 0.0260197, acc 1
2016-09-05T19:36:28.756350: step 2576, loss 0.00542949, acc 1
2016-09-05T19:36:29.578820: step 2577, loss 0.0082467, acc 1
2016-09-05T19:36:30.390054: step 2578, loss 0.00892, acc 1
2016-09-05T19:36:31.171884: step 2579, loss 0.0467905, acc 1
2016-09-05T19:36:31.978214: step 2580, loss 0.021938, acc 0.98
2016-09-05T19:36:32.810499: step 2581, loss 0.00817631, acc 1
2016-09-05T19:36:33.589776: step 2582, loss 0.0465469, acc 0.98
2016-09-05T19:36:34.455753: step 2583, loss 0.0320485, acc 0.98
2016-09-05T19:36:35.250267: step 2584, loss 0.031558, acc 0.98
2016-09-05T19:36:36.043200: step 2585, loss 0.0669754, acc 0.96
2016-09-05T19:36:36.855878: step 2586, loss 0.0619113, acc 0.98
2016-09-05T19:36:37.686545: step 2587, loss 0.0272839, acc 0.98
2016-09-05T19:36:38.466567: step 2588, loss 0.021275, acc 1
2016-09-05T19:36:39.255485: step 2589, loss 0.0761334, acc 0.96
2016-09-05T19:36:40.075372: step 2590, loss 0.00530495, acc 1
2016-09-05T19:36:40.874267: step 2591, loss 0.118213, acc 0.94
2016-09-05T19:36:41.661265: step 2592, loss 0.045136, acc 0.98
2016-09-05T19:36:42.491203: step 2593, loss 0.0551962, acc 0.98
2016-09-05T19:36:43.318895: step 2594, loss 0.0371959, acc 0.98
2016-09-05T19:36:44.096662: step 2595, loss 0.0151451, acc 1
2016-09-05T19:36:44.944143: step 2596, loss 0.0357828, acc 0.98
2016-09-05T19:36:45.726131: step 2597, loss 0.0193504, acc 1
2016-09-05T19:36:46.548024: step 2598, loss 0.0535315, acc 0.98
2016-09-05T19:36:47.389195: step 2599, loss 0.01616, acc 1
2016-09-05T19:36:48.164075: step 2600, loss 0.0106457, acc 1

Evaluation:
2016-09-05T19:36:51.672790: step 2600, loss 1.63689, acc 0.738

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-2600

2016-09-05T19:36:53.610915: step 2601, loss 0.0230314, acc 0.98
2016-09-05T19:36:54.421476: step 2602, loss 0.0961707, acc 0.96
2016-09-05T19:36:55.233506: step 2603, loss 0.123651, acc 0.94
2016-09-05T19:36:56.056785: step 2604, loss 0.0703821, acc 0.98
2016-09-05T19:36:56.874277: step 2605, loss 0.00570181, acc 1
2016-09-05T19:36:57.677110: step 2606, loss 0.00569884, acc 1
2016-09-05T19:36:58.499277: step 2607, loss 0.105329, acc 0.96
2016-09-05T19:36:59.306418: step 2608, loss 0.0296073, acc 1
2016-09-05T19:37:00.122159: step 2609, loss 0.0484194, acc 0.96
2016-09-05T19:37:01.005842: step 2610, loss 0.031309, acc 1
2016-09-05T19:37:01.829838: step 2611, loss 0.0338817, acc 0.98
2016-09-05T19:37:02.641423: step 2612, loss 0.0240078, acc 1
2016-09-05T19:37:03.461836: step 2613, loss 0.0470286, acc 0.98
2016-09-05T19:37:04.327248: step 2614, loss 0.0246564, acc 1
2016-09-05T19:37:05.122627: step 2615, loss 0.017754, acc 1
2016-09-05T19:37:05.944090: step 2616, loss 0.0186811, acc 1
2016-09-05T19:37:06.755190: step 2617, loss 0.111871, acc 0.94
2016-09-05T19:37:07.558547: step 2618, loss 0.0171892, acc 1
2016-09-05T19:37:08.366636: step 2619, loss 0.0748421, acc 0.98
2016-09-05T19:37:09.188640: step 2620, loss 0.0596478, acc 0.96
2016-09-05T19:37:10.009158: step 2621, loss 0.0507682, acc 0.98
2016-09-05T19:37:10.847026: step 2622, loss 0.0518448, acc 0.98
2016-09-05T19:37:11.666055: step 2623, loss 0.0265939, acc 0.98
2016-09-05T19:37:12.474603: step 2624, loss 0.0204808, acc 1
2016-09-05T19:37:13.282650: step 2625, loss 0.028939, acc 1
2016-09-05T19:37:14.119485: step 2626, loss 0.08857, acc 0.96
2016-09-05T19:37:14.927696: step 2627, loss 0.0255604, acc 1
2016-09-05T19:37:15.739268: step 2628, loss 0.06153, acc 0.96
2016-09-05T19:37:16.596635: step 2629, loss 0.0274128, acc 1
2016-09-05T19:37:17.405555: step 2630, loss 0.0251029, acc 0.98
2016-09-05T19:37:18.225403: step 2631, loss 0.106051, acc 0.96
2016-09-05T19:37:19.057772: step 2632, loss 0.0171254, acc 1
2016-09-05T19:37:19.858276: step 2633, loss 0.21999, acc 0.9
2016-09-05T19:37:20.685557: step 2634, loss 0.0203138, acc 1
2016-09-05T19:37:21.513418: step 2635, loss 0.0391899, acc 0.98
2016-09-05T19:37:22.325454: step 2636, loss 0.0152858, acc 1
2016-09-05T19:37:23.144673: step 2637, loss 0.0449294, acc 0.98
2016-09-05T19:37:23.963499: step 2638, loss 0.0655102, acc 0.98
2016-09-05T19:37:24.811597: step 2639, loss 0.0617565, acc 0.98
2016-09-05T19:37:25.612325: step 2640, loss 0.0539816, acc 0.96
2016-09-05T19:37:26.425443: step 2641, loss 0.0283424, acc 1
2016-09-05T19:37:27.236361: step 2642, loss 0.0543023, acc 0.94
2016-09-05T19:37:28.037924: step 2643, loss 0.0420252, acc 0.98
2016-09-05T19:37:28.837088: step 2644, loss 0.0616292, acc 0.98
2016-09-05T19:37:29.681950: step 2645, loss 0.0845421, acc 0.96
2016-09-05T19:37:30.476711: step 2646, loss 0.0201346, acc 1
2016-09-05T19:37:31.272771: step 2647, loss 0.0209185, acc 1
2016-09-05T19:37:32.103728: step 2648, loss 0.055231, acc 0.96
2016-09-05T19:37:32.912908: step 2649, loss 0.042292, acc 0.98
2016-09-05T19:37:33.707888: step 2650, loss 0.0916986, acc 0.98
2016-09-05T19:37:34.511260: step 2651, loss 0.0267074, acc 1
2016-09-05T19:37:35.274083: step 2652, loss 0.0844124, acc 0.98
2016-09-05T19:37:36.086418: step 2653, loss 0.0804002, acc 0.98
2016-09-05T19:37:36.913599: step 2654, loss 0.0215854, acc 0.98
2016-09-05T19:37:37.693108: step 2655, loss 0.0397845, acc 0.98
2016-09-05T19:37:38.538011: step 2656, loss 0.0518532, acc 0.98
2016-09-05T19:37:39.353803: step 2657, loss 0.0486235, acc 0.96
2016-09-05T19:37:40.135209: step 2658, loss 0.0203679, acc 1
2016-09-05T19:37:40.948963: step 2659, loss 0.0496926, acc 0.98
2016-09-05T19:37:41.767972: step 2660, loss 0.00359111, acc 1
2016-09-05T19:37:42.537836: step 2661, loss 0.0969901, acc 0.96
2016-09-05T19:37:43.323398: step 2662, loss 0.0473275, acc 0.98
2016-09-05T19:37:44.133153: step 2663, loss 0.0187867, acc 1
2016-09-05T19:37:44.909450: step 2664, loss 0.0337316, acc 0.98
2016-09-05T19:37:45.721870: step 2665, loss 0.077878, acc 0.96
2016-09-05T19:37:46.529123: step 2666, loss 0.015155, acc 1
2016-09-05T19:37:47.312469: step 2667, loss 0.0537786, acc 0.98
2016-09-05T19:37:48.126449: step 2668, loss 0.0540431, acc 0.98
2016-09-05T19:37:48.974227: step 2669, loss 0.0185254, acc 1
2016-09-05T19:37:49.772299: step 2670, loss 0.0598199, acc 0.96
2016-09-05T19:37:50.578304: step 2671, loss 0.0592535, acc 0.98
2016-09-05T19:37:51.414307: step 2672, loss 0.0419501, acc 0.98
2016-09-05T19:37:52.210961: step 2673, loss 0.0448586, acc 0.98
2016-09-05T19:37:53.021967: step 2674, loss 0.0474266, acc 0.98
2016-09-05T19:37:53.828946: step 2675, loss 0.0616141, acc 0.96
2016-09-05T19:37:54.634103: step 2676, loss 0.0427664, acc 1
2016-09-05T19:37:55.410739: step 2677, loss 0.0560851, acc 0.98
2016-09-05T19:37:56.257254: step 2678, loss 0.0232318, acc 0.98
2016-09-05T19:37:57.030538: step 2679, loss 0.0922634, acc 0.96
2016-09-05T19:37:57.818423: step 2680, loss 0.0103845, acc 1
2016-09-05T19:37:58.639592: step 2681, loss 0.00399273, acc 1
2016-09-05T19:37:59.428665: step 2682, loss 0.130075, acc 0.96
2016-09-05T19:38:00.255978: step 2683, loss 0.0312789, acc 1
2016-09-05T19:38:01.052900: step 2684, loss 0.0137626, acc 1
2016-09-05T19:38:01.860465: step 2685, loss 0.00479778, acc 1
2016-09-05T19:38:02.693832: step 2686, loss 0.100332, acc 0.98
2016-09-05T19:38:03.519626: step 2687, loss 0.0568647, acc 0.98
2016-09-05T19:38:04.308431: step 2688, loss 0.0597178, acc 0.96
2016-09-05T19:38:05.093482: step 2689, loss 0.0172584, acc 1
2016-09-05T19:38:05.931645: step 2690, loss 0.0535896, acc 0.98
2016-09-05T19:38:06.736252: step 2691, loss 0.0166419, acc 1
2016-09-05T19:38:07.550202: step 2692, loss 0.0516959, acc 1
2016-09-05T19:38:08.372089: step 2693, loss 0.0521933, acc 0.96
2016-09-05T19:38:09.175481: step 2694, loss 0.0204896, acc 1
2016-09-05T19:38:09.990189: step 2695, loss 0.026281, acc 1
2016-09-05T19:38:10.796808: step 2696, loss 0.0416931, acc 0.96
2016-09-05T19:38:11.641387: step 2697, loss 0.0918617, acc 0.96
2016-09-05T19:38:12.429776: step 2698, loss 0.0748642, acc 0.98
2016-09-05T19:38:13.256170: step 2699, loss 0.0961253, acc 0.96
2016-09-05T19:38:14.035643: step 2700, loss 0.026628, acc 1

Evaluation:
2016-09-05T19:38:17.558946: step 2700, loss 1.40484, acc 0.743

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-2700

2016-09-05T19:38:19.325452: step 2701, loss 0.0597685, acc 0.96
2016-09-05T19:38:20.125449: step 2702, loss 0.107083, acc 0.94
2016-09-05T19:38:20.908797: step 2703, loss 0.0648185, acc 0.96
2016-09-05T19:38:21.727042: step 2704, loss 0.0156899, acc 1
2016-09-05T19:38:22.518377: step 2705, loss 0.1313, acc 0.94
2016-09-05T19:38:23.320080: step 2706, loss 0.0240004, acc 0.98
2016-09-05T19:38:24.136166: step 2707, loss 0.0402042, acc 0.98
2016-09-05T19:38:24.933280: step 2708, loss 0.0692836, acc 0.98
2016-09-05T19:38:25.741585: step 2709, loss 0.0912472, acc 0.96
2016-09-05T19:38:26.563438: step 2710, loss 0.0112052, acc 1
2016-09-05T19:38:27.347052: step 2711, loss 0.0459375, acc 0.96
2016-09-05T19:38:28.156800: step 2712, loss 0.0832125, acc 0.94
2016-09-05T19:38:28.963822: step 2713, loss 0.113561, acc 0.94
2016-09-05T19:38:29.739458: step 2714, loss 0.0415931, acc 0.96
2016-09-05T19:38:30.546661: step 2715, loss 0.0551928, acc 0.98
2016-09-05T19:38:30.977367: step 2716, loss 0.0750536, acc 0.916667
2016-09-05T19:38:31.757638: step 2717, loss 0.0476818, acc 1
2016-09-05T19:38:32.573439: step 2718, loss 0.0261326, acc 1
2016-09-05T19:38:33.376916: step 2719, loss 0.101745, acc 0.92
2016-09-05T19:38:34.196158: step 2720, loss 0.0499847, acc 0.98
2016-09-05T19:38:35.005972: step 2721, loss 0.0427063, acc 1
2016-09-05T19:38:35.789139: step 2722, loss 0.0833751, acc 0.96
2016-09-05T19:38:36.600005: step 2723, loss 0.0641269, acc 0.96
2016-09-05T19:38:37.393850: step 2724, loss 0.0240938, acc 1
2016-09-05T19:38:38.196050: step 2725, loss 0.0162535, acc 1
2016-09-05T19:38:39.019907: step 2726, loss 0.00885137, acc 1
2016-09-05T19:38:39.802638: step 2727, loss 0.0328818, acc 1
2016-09-05T19:38:40.619330: step 2728, loss 0.0369375, acc 0.98
2016-09-05T19:38:41.445556: step 2729, loss 0.0641658, acc 0.96
2016-09-05T19:38:42.266814: step 2730, loss 0.0184849, acc 1
2016-09-05T19:38:43.072564: step 2731, loss 0.0289774, acc 1
2016-09-05T19:38:43.934406: step 2732, loss 0.0502939, acc 0.98
2016-09-05T19:38:44.768081: step 2733, loss 0.0212137, acc 1
2016-09-05T19:38:45.576321: step 2734, loss 0.0559596, acc 0.96
2016-09-05T19:38:46.400059: step 2735, loss 0.0425509, acc 0.98
2016-09-05T19:38:47.249129: step 2736, loss 0.178245, acc 0.96
2016-09-05T19:38:48.049721: step 2737, loss 0.0301204, acc 1
2016-09-05T19:38:48.883601: step 2738, loss 0.0426437, acc 0.98
2016-09-05T19:38:49.705486: step 2739, loss 0.0260896, acc 1
2016-09-05T19:38:50.520174: step 2740, loss 0.0507792, acc 0.98
2016-09-05T19:38:51.357262: step 2741, loss 0.036265, acc 0.98
2016-09-05T19:38:52.195104: step 2742, loss 0.0101273, acc 1
2016-09-05T19:38:53.023920: step 2743, loss 0.0347769, acc 0.98
2016-09-05T19:38:53.824133: step 2744, loss 0.0216897, acc 0.98
2016-09-05T19:38:54.644093: step 2745, loss 0.0199051, acc 1
2016-09-05T19:38:55.458570: step 2746, loss 0.0487559, acc 0.98
2016-09-05T19:38:56.252304: step 2747, loss 0.0306551, acc 1
2016-09-05T19:38:57.098343: step 2748, loss 0.106936, acc 0.92
2016-09-05T19:38:57.929216: step 2749, loss 0.0653537, acc 0.98
2016-09-05T19:38:58.768274: step 2750, loss 0.0869913, acc 0.98
2016-09-05T19:38:59.583698: step 2751, loss 0.064709, acc 0.98
2016-09-05T19:39:00.440318: step 2752, loss 0.0309237, acc 0.98
2016-09-05T19:39:01.227979: step 2753, loss 0.0509465, acc 0.98
2016-09-05T19:39:02.029805: step 2754, loss 0.0224584, acc 1
2016-09-05T19:39:02.814874: step 2755, loss 0.245443, acc 0.94
2016-09-05T19:39:03.592025: step 2756, loss 0.0042889, acc 1
2016-09-05T19:39:04.401858: step 2757, loss 0.0672072, acc 0.96
2016-09-05T19:39:05.184625: step 2758, loss 0.0765753, acc 0.98
2016-09-05T19:39:05.977961: step 2759, loss 0.175194, acc 0.98
2016-09-05T19:39:06.796497: step 2760, loss 0.0605081, acc 0.96
2016-09-05T19:39:07.595381: step 2761, loss 0.0143669, acc 1
2016-09-05T19:39:08.376556: step 2762, loss 0.0924196, acc 0.94
2016-09-05T19:39:09.205441: step 2763, loss 0.0381915, acc 0.98
2016-09-05T19:39:10.030177: step 2764, loss 0.0418493, acc 0.98
2016-09-05T19:39:10.818303: step 2765, loss 0.028422, acc 1
2016-09-05T19:39:11.607416: step 2766, loss 0.0396656, acc 0.98
2016-09-05T19:39:12.428771: step 2767, loss 0.0127851, acc 1
2016-09-05T19:39:13.275172: step 2768, loss 0.0343618, acc 1
2016-09-05T19:39:14.112227: step 2769, loss 0.0155346, acc 1
2016-09-05T19:39:14.905410: step 2770, loss 0.0267787, acc 0.98
2016-09-05T19:39:15.690082: step 2771, loss 0.0222903, acc 1
2016-09-05T19:39:16.508488: step 2772, loss 0.0335196, acc 1
2016-09-05T19:39:17.327337: step 2773, loss 0.0491203, acc 0.98
2016-09-05T19:39:18.131919: step 2774, loss 0.00548237, acc 1
2016-09-05T19:39:18.951914: step 2775, loss 0.05049, acc 0.98
2016-09-05T19:39:19.754686: step 2776, loss 0.0271585, acc 1
2016-09-05T19:39:20.526050: step 2777, loss 0.0319804, acc 1
2016-09-05T19:39:21.304347: step 2778, loss 0.0480259, acc 0.98
2016-09-05T19:39:22.149841: step 2779, loss 0.067958, acc 0.96
2016-09-05T19:39:22.943022: step 2780, loss 0.078125, acc 0.96
2016-09-05T19:39:23.749342: step 2781, loss 0.0967362, acc 0.94
2016-09-05T19:39:24.600595: step 2782, loss 0.0296095, acc 0.98
2016-09-05T19:39:25.375038: step 2783, loss 0.0172365, acc 1
2016-09-05T19:39:26.190320: step 2784, loss 0.118419, acc 0.94
2016-09-05T19:39:27.013411: step 2785, loss 0.00547071, acc 1
2016-09-05T19:39:27.785597: step 2786, loss 0.0614208, acc 0.96
2016-09-05T19:39:28.614230: step 2787, loss 0.0147134, acc 1
2016-09-05T19:39:29.425163: step 2788, loss 0.0171736, acc 1
2016-09-05T19:39:30.203902: step 2789, loss 0.0286829, acc 0.98
2016-09-05T19:39:30.981517: step 2790, loss 0.0266235, acc 0.98
2016-09-05T19:39:31.808918: step 2791, loss 0.0296695, acc 0.98
2016-09-05T19:39:32.607351: step 2792, loss 0.110413, acc 0.94
2016-09-05T19:39:33.457874: step 2793, loss 0.0357282, acc 1
2016-09-05T19:39:34.264745: step 2794, loss 0.00985042, acc 1
2016-09-05T19:39:35.079677: step 2795, loss 0.0792578, acc 0.94
2016-09-05T19:39:35.900076: step 2796, loss 0.0929932, acc 0.96
2016-09-05T19:39:36.712003: step 2797, loss 0.0130761, acc 1
2016-09-05T19:39:37.512453: step 2798, loss 0.0479238, acc 0.98
2016-09-05T19:39:38.321865: step 2799, loss 0.0966465, acc 0.94
2016-09-05T19:39:39.136050: step 2800, loss 0.0206483, acc 1

Evaluation:
2016-09-05T19:39:42.607918: step 2800, loss 1.49115, acc 0.741

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-2800

2016-09-05T19:39:44.521957: step 2801, loss 0.145901, acc 0.94
2016-09-05T19:39:45.365413: step 2802, loss 0.0183159, acc 1
2016-09-05T19:39:46.139792: step 2803, loss 0.0234381, acc 1
2016-09-05T19:39:46.965570: step 2804, loss 0.0303705, acc 1
2016-09-05T19:39:47.787769: step 2805, loss 0.0609961, acc 0.94
2016-09-05T19:39:48.569438: step 2806, loss 0.0537449, acc 0.98
2016-09-05T19:39:49.393987: step 2807, loss 0.0957993, acc 0.94
2016-09-05T19:39:50.217127: step 2808, loss 0.0313862, acc 0.98
2016-09-05T19:39:51.053008: step 2809, loss 0.0237748, acc 0.98
2016-09-05T19:39:51.866974: step 2810, loss 0.0307195, acc 1
2016-09-05T19:39:52.746076: step 2811, loss 0.0321239, acc 0.98
2016-09-05T19:39:53.535759: step 2812, loss 0.0329211, acc 0.98
2016-09-05T19:39:54.330939: step 2813, loss 0.0442402, acc 0.96
2016-09-05T19:39:55.183972: step 2814, loss 0.0300549, acc 1
2016-09-05T19:39:56.019241: step 2815, loss 0.0273794, acc 0.98
2016-09-05T19:39:56.863978: step 2816, loss 0.0562642, acc 0.96
2016-09-05T19:39:57.687294: step 2817, loss 0.0168622, acc 1
2016-09-05T19:39:58.508482: step 2818, loss 0.0713514, acc 0.98
2016-09-05T19:39:59.279000: step 2819, loss 0.0291337, acc 0.98
2016-09-05T19:40:00.089017: step 2820, loss 0.0235787, acc 1
2016-09-05T19:40:00.949771: step 2821, loss 0.0343731, acc 0.98
2016-09-05T19:40:01.753390: step 2822, loss 0.0750628, acc 0.98
2016-09-05T19:40:02.551365: step 2823, loss 0.0136599, acc 1
2016-09-05T19:40:03.368905: step 2824, loss 0.025636, acc 0.98
2016-09-05T19:40:04.198997: step 2825, loss 0.0237738, acc 1
2016-09-05T19:40:05.020207: step 2826, loss 0.0290198, acc 1
2016-09-05T19:40:05.812492: step 2827, loss 0.0319916, acc 1
2016-09-05T19:40:06.569915: step 2828, loss 0.0357316, acc 0.98
2016-09-05T19:40:07.361135: step 2829, loss 0.0461264, acc 0.96
2016-09-05T19:40:08.171795: step 2830, loss 0.042393, acc 1
2016-09-05T19:40:08.983220: step 2831, loss 0.107844, acc 0.98
2016-09-05T19:40:09.778852: step 2832, loss 0.0618048, acc 0.98
2016-09-05T19:40:10.603183: step 2833, loss 0.0178666, acc 1
2016-09-05T19:40:11.398992: step 2834, loss 0.0400604, acc 0.98
2016-09-05T19:40:12.185792: step 2835, loss 0.0290062, acc 0.98
2016-09-05T19:40:12.993485: step 2836, loss 0.164409, acc 0.94
2016-09-05T19:40:13.785878: step 2837, loss 0.0622942, acc 0.98
2016-09-05T19:40:14.596183: step 2838, loss 0.0325632, acc 0.98
2016-09-05T19:40:15.408711: step 2839, loss 0.00641194, acc 1
2016-09-05T19:40:16.184104: step 2840, loss 0.064292, acc 0.98
2016-09-05T19:40:17.026218: step 2841, loss 0.136412, acc 0.9
2016-09-05T19:40:17.815044: step 2842, loss 0.0219327, acc 0.98
2016-09-05T19:40:18.611592: step 2843, loss 0.0447681, acc 0.98
2016-09-05T19:40:19.418559: step 2844, loss 0.0126812, acc 1
2016-09-05T19:40:20.231437: step 2845, loss 0.0390168, acc 0.98
2016-09-05T19:40:21.067732: step 2846, loss 0.0123349, acc 1
2016-09-05T19:40:21.900132: step 2847, loss 0.0272904, acc 1
2016-09-05T19:40:22.708112: step 2848, loss 0.0377057, acc 0.98
2016-09-05T19:40:23.491622: step 2849, loss 0.00379111, acc 1
2016-09-05T19:40:24.294949: step 2850, loss 0.0269064, acc 0.98
2016-09-05T19:40:25.160868: step 2851, loss 0.04059, acc 0.98
2016-09-05T19:40:25.947964: step 2852, loss 0.0343367, acc 0.98
2016-09-05T19:40:26.808148: step 2853, loss 0.00543708, acc 1
2016-09-05T19:40:27.628170: step 2854, loss 0.00417924, acc 1
2016-09-05T19:40:28.443177: step 2855, loss 0.116139, acc 0.96
2016-09-05T19:40:29.259932: step 2856, loss 0.143429, acc 0.98
2016-09-05T19:40:30.126253: step 2857, loss 0.0510065, acc 0.98
2016-09-05T19:40:30.950586: step 2858, loss 0.0602797, acc 0.96
2016-09-05T19:40:31.797589: step 2859, loss 0.0319618, acc 1
2016-09-05T19:40:32.618435: step 2860, loss 0.012274, acc 1
2016-09-05T19:40:33.413485: step 2861, loss 0.0251842, acc 1
2016-09-05T19:40:34.233282: step 2862, loss 0.0283807, acc 1
2016-09-05T19:40:35.035289: step 2863, loss 0.10684, acc 0.98
2016-09-05T19:40:35.863045: step 2864, loss 0.0107514, acc 1
2016-09-05T19:40:36.691951: step 2865, loss 0.010743, acc 1
2016-09-05T19:40:37.513523: step 2866, loss 0.00825221, acc 1
2016-09-05T19:40:38.312224: step 2867, loss 0.0853171, acc 0.98
2016-09-05T19:40:39.137492: step 2868, loss 0.100105, acc 0.92
2016-09-05T19:40:39.984825: step 2869, loss 0.0136292, acc 1
2016-09-05T19:40:40.787840: step 2870, loss 0.14721, acc 0.96
2016-09-05T19:40:41.626136: step 2871, loss 0.0424429, acc 0.98
2016-09-05T19:40:42.415261: step 2872, loss 0.00614214, acc 1
2016-09-05T19:40:43.232541: step 2873, loss 0.0246882, acc 1
2016-09-05T19:40:44.045832: step 2874, loss 0.0295681, acc 1
2016-09-05T19:40:44.858373: step 2875, loss 0.0059522, acc 1
2016-09-05T19:40:45.672477: step 2876, loss 0.0524662, acc 0.96
2016-09-05T19:40:46.467513: step 2877, loss 0.0925537, acc 0.98
2016-09-05T19:40:47.279158: step 2878, loss 0.0593222, acc 0.98
2016-09-05T19:40:48.100396: step 2879, loss 0.0140796, acc 1
2016-09-05T19:40:48.890735: step 2880, loss 0.0694389, acc 0.96
2016-09-05T19:40:49.696115: step 2881, loss 0.00554723, acc 1
2016-09-05T19:40:50.500487: step 2882, loss 0.0308472, acc 1
2016-09-05T19:40:51.281452: step 2883, loss 0.0221548, acc 1
2016-09-05T19:40:52.082295: step 2884, loss 0.0184718, acc 1
2016-09-05T19:40:52.897031: step 2885, loss 0.0714905, acc 0.98
2016-09-05T19:40:53.702661: step 2886, loss 0.0717177, acc 0.98
2016-09-05T19:40:54.533556: step 2887, loss 0.0396536, acc 0.96
2016-09-05T19:40:55.374567: step 2888, loss 0.0306115, acc 1
2016-09-05T19:40:56.164521: step 2889, loss 0.047536, acc 0.98
2016-09-05T19:40:56.981543: step 2890, loss 0.0148909, acc 1
2016-09-05T19:40:57.775119: step 2891, loss 0.0253219, acc 1
2016-09-05T19:40:58.562134: step 2892, loss 0.0802941, acc 0.94
2016-09-05T19:40:59.371063: step 2893, loss 0.0116344, acc 1
2016-09-05T19:41:00.206497: step 2894, loss 0.0451878, acc 0.98
2016-09-05T19:41:01.020674: step 2895, loss 0.0918198, acc 0.96
2016-09-05T19:41:01.793512: step 2896, loss 0.0522808, acc 0.98
2016-09-05T19:41:02.636886: step 2897, loss 0.102842, acc 0.96
2016-09-05T19:41:03.400373: step 2898, loss 0.018659, acc 1
2016-09-05T19:41:04.215274: step 2899, loss 0.0124497, acc 1
2016-09-05T19:41:05.024671: step 2900, loss 0.0525541, acc 0.98

Evaluation:
2016-09-05T19:41:08.508138: step 2900, loss 1.45513, acc 0.745

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-2900

2016-09-05T19:41:10.352012: step 2901, loss 0.0166487, acc 1
2016-09-05T19:41:11.169450: step 2902, loss 0.098313, acc 0.96
2016-09-05T19:41:11.983817: step 2903, loss 0.0702573, acc 0.96
2016-09-05T19:41:12.797376: step 2904, loss 0.0187018, acc 1
2016-09-05T19:41:13.635970: step 2905, loss 0.0289391, acc 1
2016-09-05T19:41:14.425139: step 2906, loss 0.0344351, acc 1
2016-09-05T19:41:15.251382: step 2907, loss 0.0250831, acc 0.98
2016-09-05T19:41:16.054690: step 2908, loss 0.0325078, acc 0.98
2016-09-05T19:41:16.825911: step 2909, loss 0.0185401, acc 1
2016-09-05T19:41:17.246823: step 2910, loss 0.0833161, acc 1
2016-09-05T19:41:18.023101: step 2911, loss 0.0373238, acc 0.98
2016-09-05T19:41:18.827488: step 2912, loss 0.0219546, acc 0.98
2016-09-05T19:41:19.640920: step 2913, loss 0.0638852, acc 0.96
2016-09-05T19:41:20.429023: step 2914, loss 0.0310563, acc 0.98
2016-09-05T19:41:21.227173: step 2915, loss 0.0393477, acc 1
2016-09-05T19:41:22.031388: step 2916, loss 0.0701206, acc 0.96
2016-09-05T19:41:22.829011: step 2917, loss 0.0151933, acc 1
2016-09-05T19:41:23.625434: step 2918, loss 0.0607623, acc 0.94
2016-09-05T19:41:24.499675: step 2919, loss 0.154149, acc 0.98
2016-09-05T19:41:25.273083: step 2920, loss 0.0300395, acc 0.98
2016-09-05T19:41:26.094971: step 2921, loss 0.00922476, acc 1
2016-09-05T19:41:26.921133: step 2922, loss 0.0539024, acc 0.98
2016-09-05T19:41:27.681016: step 2923, loss 0.0573065, acc 0.96
2016-09-05T19:41:28.466952: step 2924, loss 0.0383456, acc 0.98
2016-09-05T19:41:29.271196: step 2925, loss 0.0135697, acc 1
2016-09-05T19:41:30.065926: step 2926, loss 0.0112589, acc 1
2016-09-05T19:41:30.921570: step 2927, loss 0.122133, acc 0.96
2016-09-05T19:41:31.734488: step 2928, loss 0.0529842, acc 0.98
2016-09-05T19:41:32.556784: step 2929, loss 0.0469148, acc 0.98
2016-09-05T19:41:33.331296: step 2930, loss 0.0382523, acc 0.96
2016-09-05T19:41:34.158704: step 2931, loss 0.0208691, acc 1
2016-09-05T19:41:34.927645: step 2932, loss 0.0240842, acc 1
2016-09-05T19:41:35.761642: step 2933, loss 0.0332408, acc 0.98
2016-09-05T19:41:36.592046: step 2934, loss 0.00700852, acc 1
2016-09-05T19:41:37.391212: step 2935, loss 0.0204468, acc 1
2016-09-05T19:41:38.192797: step 2936, loss 0.136328, acc 0.98
2016-09-05T19:41:39.015262: step 2937, loss 0.0753314, acc 0.96
2016-09-05T19:41:39.774345: step 2938, loss 0.0188616, acc 1
2016-09-05T19:41:40.580259: step 2939, loss 0.0238143, acc 0.98
2016-09-05T19:41:41.393399: step 2940, loss 0.0461821, acc 0.98
2016-09-05T19:41:42.184333: step 2941, loss 0.0251938, acc 0.98
2016-09-05T19:41:43.014123: step 2942, loss 0.049468, acc 0.98
2016-09-05T19:41:43.824188: step 2943, loss 0.0485001, acc 0.96
2016-09-05T19:41:44.592577: step 2944, loss 0.0168666, acc 1
2016-09-05T19:41:45.404926: step 2945, loss 0.0105169, acc 1
2016-09-05T19:41:46.220378: step 2946, loss 0.0615742, acc 0.98
2016-09-05T19:41:47.017885: step 2947, loss 0.0224426, acc 1
2016-09-05T19:41:47.842029: step 2948, loss 0.018862, acc 1
2016-09-05T19:41:48.665405: step 2949, loss 0.0533622, acc 0.96
2016-09-05T19:41:49.424669: step 2950, loss 0.0032594, acc 1
2016-09-05T19:41:50.236080: step 2951, loss 0.093956, acc 0.92
2016-09-05T19:41:51.068348: step 2952, loss 0.00960844, acc 1
2016-09-05T19:41:51.871873: step 2953, loss 0.017999, acc 1
2016-09-05T19:41:52.680141: step 2954, loss 0.128136, acc 0.92
2016-09-05T19:41:53.486587: step 2955, loss 0.0726092, acc 0.96
2016-09-05T19:41:54.265306: step 2956, loss 0.054686, acc 0.98
2016-09-05T19:41:55.053022: step 2957, loss 0.013448, acc 1
2016-09-05T19:41:55.874414: step 2958, loss 0.0493169, acc 0.98
2016-09-05T19:41:56.661326: step 2959, loss 0.098171, acc 0.96
2016-09-05T19:41:57.455954: step 2960, loss 0.0122665, acc 1
2016-09-05T19:41:58.301002: step 2961, loss 0.0233591, acc 1
2016-09-05T19:41:59.106003: step 2962, loss 0.00535369, acc 1
2016-09-05T19:41:59.933677: step 2963, loss 0.00587552, acc 1
2016-09-05T19:42:00.746070: step 2964, loss 0.0704097, acc 0.98
2016-09-05T19:42:01.526860: step 2965, loss 0.0238086, acc 1
2016-09-05T19:42:02.310932: step 2966, loss 0.0608773, acc 0.96
2016-09-05T19:42:03.144419: step 2967, loss 0.019583, acc 1
2016-09-05T19:42:03.964521: step 2968, loss 0.00554012, acc 1
2016-09-05T19:42:04.760535: step 2969, loss 0.0125452, acc 1
2016-09-05T19:42:05.570360: step 2970, loss 0.012948, acc 1
2016-09-05T19:42:06.337700: step 2971, loss 0.0377954, acc 1
2016-09-05T19:42:07.136321: step 2972, loss 0.0240282, acc 1
2016-09-05T19:42:07.950127: step 2973, loss 0.0240671, acc 1
2016-09-05T19:42:08.750758: step 2974, loss 0.0271413, acc 1
2016-09-05T19:42:09.551086: step 2975, loss 0.0920037, acc 0.98
2016-09-05T19:42:10.390509: step 2976, loss 0.0641289, acc 0.98
2016-09-05T19:42:11.170017: step 2977, loss 0.0171868, acc 1
2016-09-05T19:42:11.986232: step 2978, loss 0.0228839, acc 0.98
2016-09-05T19:42:12.802522: step 2979, loss 0.067847, acc 0.96
2016-09-05T19:42:13.622567: step 2980, loss 0.0536977, acc 0.96
2016-09-05T19:42:14.419190: step 2981, loss 0.0151569, acc 1
2016-09-05T19:42:15.225112: step 2982, loss 0.0935583, acc 0.94
2016-09-05T19:42:15.997438: step 2983, loss 0.0382182, acc 0.98
2016-09-05T19:42:16.797801: step 2984, loss 0.0528667, acc 0.96
2016-09-05T19:42:17.605594: step 2985, loss 0.0511125, acc 0.98
2016-09-05T19:42:18.407505: step 2986, loss 0.045346, acc 0.98
2016-09-05T19:42:19.251508: step 2987, loss 0.0225017, acc 0.98
2016-09-05T19:42:20.075771: step 2988, loss 0.0033124, acc 1
2016-09-05T19:42:20.884541: step 2989, loss 0.0538704, acc 0.98
2016-09-05T19:42:21.683460: step 2990, loss 0.0278045, acc 0.98
2016-09-05T19:42:22.497801: step 2991, loss 0.0156391, acc 1
2016-09-05T19:42:23.305527: step 2992, loss 0.0104791, acc 1
2016-09-05T19:42:24.104402: step 2993, loss 0.0154447, acc 1
2016-09-05T19:42:24.953003: step 2994, loss 0.0237148, acc 0.98
2016-09-05T19:42:25.767438: step 2995, loss 0.0113054, acc 1
2016-09-05T19:42:26.567526: step 2996, loss 0.0523365, acc 0.98
2016-09-05T19:42:27.397352: step 2997, loss 0.0271686, acc 1
2016-09-05T19:42:28.216197: step 2998, loss 0.00747572, acc 1
2016-09-05T19:42:29.036091: step 2999, loss 0.0376778, acc 0.98
2016-09-05T19:42:29.854322: step 3000, loss 0.0250026, acc 1

Evaluation:
2016-09-05T19:42:33.358471: step 3000, loss 1.66683, acc 0.76

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-3000

2016-09-05T19:42:35.225088: step 3001, loss 0.0339193, acc 0.98
2016-09-05T19:42:36.069990: step 3002, loss 0.044987, acc 0.96
2016-09-05T19:42:36.899908: step 3003, loss 0.00562066, acc 1
2016-09-05T19:42:37.683019: step 3004, loss 0.0477234, acc 0.98
2016-09-05T19:42:38.513007: step 3005, loss 0.0163971, acc 1
2016-09-05T19:42:39.323664: step 3006, loss 0.0545346, acc 0.98
2016-09-05T19:42:40.130425: step 3007, loss 0.0451183, acc 0.98
2016-09-05T19:42:40.946681: step 3008, loss 0.116935, acc 0.92
2016-09-05T19:42:41.773026: step 3009, loss 0.0876483, acc 0.96
2016-09-05T19:42:42.609418: step 3010, loss 0.043042, acc 0.98
2016-09-05T19:42:43.404977: step 3011, loss 0.016042, acc 1
2016-09-05T19:42:44.233732: step 3012, loss 0.129838, acc 0.94
2016-09-05T19:42:45.018205: step 3013, loss 0.0266392, acc 0.98
2016-09-05T19:42:45.830499: step 3014, loss 0.0155248, acc 1
2016-09-05T19:42:46.655749: step 3015, loss 0.124817, acc 0.98
2016-09-05T19:42:47.438014: step 3016, loss 0.0240846, acc 1
2016-09-05T19:42:48.247032: step 3017, loss 0.0378106, acc 0.98
2016-09-05T19:42:49.069726: step 3018, loss 0.0770963, acc 0.96
2016-09-05T19:42:49.863593: step 3019, loss 0.156014, acc 0.94
2016-09-05T19:42:50.671716: step 3020, loss 0.0200924, acc 1
2016-09-05T19:42:51.494171: step 3021, loss 0.028607, acc 1
2016-09-05T19:42:52.248417: step 3022, loss 0.0287627, acc 1
2016-09-05T19:42:53.052948: step 3023, loss 0.00293082, acc 1
2016-09-05T19:42:53.861315: step 3024, loss 0.0925195, acc 0.98
2016-09-05T19:42:54.642420: step 3025, loss 0.00591846, acc 1
2016-09-05T19:42:55.449040: step 3026, loss 0.0140696, acc 1
2016-09-05T19:42:56.277547: step 3027, loss 0.185142, acc 0.96
2016-09-05T19:42:57.048081: step 3028, loss 0.0717969, acc 0.98
2016-09-05T19:42:57.859784: step 3029, loss 0.0139889, acc 1
2016-09-05T19:42:58.658372: step 3030, loss 0.0346625, acc 0.96
2016-09-05T19:42:59.450673: step 3031, loss 0.0263177, acc 0.98
2016-09-05T19:43:00.283844: step 3032, loss 0.0166403, acc 1
2016-09-05T19:43:01.111607: step 3033, loss 0.00568495, acc 1
2016-09-05T19:43:01.907934: step 3034, loss 0.0727667, acc 0.98
2016-09-05T19:43:02.696544: step 3035, loss 0.0860197, acc 0.96
2016-09-05T19:43:03.511727: step 3036, loss 0.0466453, acc 0.96
2016-09-05T19:43:04.294218: step 3037, loss 0.027465, acc 0.98
2016-09-05T19:43:05.158262: step 3038, loss 0.0125477, acc 1
2016-09-05T19:43:06.015794: step 3039, loss 0.0118145, acc 1
2016-09-05T19:43:06.820090: step 3040, loss 0.0926214, acc 0.96
2016-09-05T19:43:07.633100: step 3041, loss 0.0360565, acc 1
2016-09-05T19:43:08.445756: step 3042, loss 0.0416577, acc 0.98
2016-09-05T19:43:09.240989: step 3043, loss 0.0160652, acc 1
2016-09-05T19:43:10.048475: step 3044, loss 0.022199, acc 1
2016-09-05T19:43:10.871911: step 3045, loss 0.100264, acc 0.98
2016-09-05T19:43:11.671238: step 3046, loss 0.025921, acc 0.98
2016-09-05T19:43:12.478409: step 3047, loss 0.0200641, acc 0.98
2016-09-05T19:43:13.293695: step 3048, loss 0.00810733, acc 1
2016-09-05T19:43:14.114150: step 3049, loss 0.0863743, acc 0.96
2016-09-05T19:43:14.928578: step 3050, loss 0.0578749, acc 1
2016-09-05T19:43:15.792403: step 3051, loss 0.0361845, acc 1
2016-09-05T19:43:16.596173: step 3052, loss 0.0327768, acc 0.98
2016-09-05T19:43:17.398934: step 3053, loss 0.0220681, acc 1
2016-09-05T19:43:18.213748: step 3054, loss 0.16245, acc 0.96
2016-09-05T19:43:19.028698: step 3055, loss 0.0235337, acc 0.98
2016-09-05T19:43:19.840045: step 3056, loss 0.0182383, acc 1
2016-09-05T19:43:20.707419: step 3057, loss 0.0528744, acc 0.96
2016-09-05T19:43:21.530376: step 3058, loss 0.0433805, acc 0.98
2016-09-05T19:43:22.348620: step 3059, loss 0.0502237, acc 0.98
2016-09-05T19:43:23.202943: step 3060, loss 0.0408923, acc 0.98
2016-09-05T19:43:24.038362: step 3061, loss 0.0424717, acc 0.98
2016-09-05T19:43:24.860955: step 3062, loss 0.0898021, acc 0.94
2016-09-05T19:43:25.687284: step 3063, loss 0.0757373, acc 0.96
2016-09-05T19:43:26.485465: step 3064, loss 0.0313321, acc 0.98
2016-09-05T19:43:27.297952: step 3065, loss 0.152041, acc 0.96
2016-09-05T19:43:28.177232: step 3066, loss 0.0237269, acc 1
2016-09-05T19:43:28.984364: step 3067, loss 0.0348781, acc 0.98
2016-09-05T19:43:29.775821: step 3068, loss 0.00418438, acc 1
2016-09-05T19:43:30.561432: step 3069, loss 0.0130412, acc 1
2016-09-05T19:43:31.364212: step 3070, loss 0.0647979, acc 0.98
2016-09-05T19:43:32.135421: step 3071, loss 0.0343283, acc 1
2016-09-05T19:43:32.936123: step 3072, loss 0.0295715, acc 0.98
2016-09-05T19:43:33.758367: step 3073, loss 0.00873345, acc 1
2016-09-05T19:43:34.523315: step 3074, loss 0.0201207, acc 1
2016-09-05T19:43:35.337295: step 3075, loss 0.021796, acc 1
2016-09-05T19:43:36.164341: step 3076, loss 0.0708412, acc 0.94
2016-09-05T19:43:36.953728: step 3077, loss 0.0179139, acc 1
2016-09-05T19:43:37.772201: step 3078, loss 0.0475044, acc 0.98
2016-09-05T19:43:38.591809: step 3079, loss 0.0330334, acc 0.98
2016-09-05T19:43:39.395312: step 3080, loss 0.053974, acc 0.96
2016-09-05T19:43:40.188247: step 3081, loss 0.045776, acc 0.98
2016-09-05T19:43:41.023110: step 3082, loss 0.0422252, acc 1
2016-09-05T19:43:41.830599: step 3083, loss 0.020385, acc 0.98
2016-09-05T19:43:42.671586: step 3084, loss 0.0222344, acc 0.98
2016-09-05T19:43:43.493684: step 3085, loss 0.0366712, acc 0.98
2016-09-05T19:43:44.300337: step 3086, loss 0.00394038, acc 1
2016-09-05T19:43:45.103079: step 3087, loss 0.00549191, acc 1
2016-09-05T19:43:45.924080: step 3088, loss 0.0213372, acc 1
2016-09-05T19:43:46.723668: step 3089, loss 0.0390191, acc 0.98
2016-09-05T19:43:47.534515: step 3090, loss 0.0241822, acc 0.98
2016-09-05T19:43:48.367773: step 3091, loss 0.00915238, acc 1
2016-09-05T19:43:49.166018: step 3092, loss 0.00519182, acc 1
2016-09-05T19:43:50.023776: step 3093, loss 0.0134106, acc 1
2016-09-05T19:43:50.861821: step 3094, loss 0.0127226, acc 1
2016-09-05T19:43:51.693878: step 3095, loss 0.0344578, acc 0.98
2016-09-05T19:43:52.496982: step 3096, loss 0.0542679, acc 0.96
2016-09-05T19:43:53.330325: step 3097, loss 0.0252992, acc 1
2016-09-05T19:43:54.187016: step 3098, loss 0.00441217, acc 1
2016-09-05T19:43:54.998457: step 3099, loss 0.017532, acc 1
2016-09-05T19:43:55.812433: step 3100, loss 0.136281, acc 0.98

Evaluation:
2016-09-05T19:43:59.275127: step 3100, loss 1.98936, acc 0.741

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-3100

2016-09-05T19:44:01.079626: step 3101, loss 0.0941661, acc 0.96
2016-09-05T19:44:01.933419: step 3102, loss 0.0176481, acc 0.98
2016-09-05T19:44:02.724697: step 3103, loss 0.00385095, acc 1
2016-09-05T19:44:03.177122: step 3104, loss 0.0036858, acc 1
2016-09-05T19:44:03.976861: step 3105, loss 0.0505231, acc 0.98
2016-09-05T19:44:04.757505: step 3106, loss 0.0611157, acc 0.98
2016-09-05T19:44:05.594245: step 3107, loss 0.0648174, acc 0.98
2016-09-05T19:44:06.403880: step 3108, loss 0.0310285, acc 1
2016-09-05T19:44:07.213599: step 3109, loss 0.0746232, acc 0.96
2016-09-05T19:44:08.023915: step 3110, loss 0.0294488, acc 0.98
2016-09-05T19:44:08.822638: step 3111, loss 0.00652028, acc 1
2016-09-05T19:44:09.639970: step 3112, loss 0.0811299, acc 0.94
2016-09-05T19:44:10.479337: step 3113, loss 0.0291961, acc 0.98
2016-09-05T19:44:11.305448: step 3114, loss 0.0994134, acc 0.96
2016-09-05T19:44:12.131199: step 3115, loss 0.0121845, acc 1
2016-09-05T19:44:12.958722: step 3116, loss 0.0218252, acc 1
2016-09-05T19:44:13.796685: step 3117, loss 0.0414641, acc 0.98
2016-09-05T19:44:14.595993: step 3118, loss 0.045635, acc 1
2016-09-05T19:44:15.421604: step 3119, loss 0.123858, acc 0.96
2016-09-05T19:44:16.225707: step 3120, loss 0.0539055, acc 0.96
2016-09-05T19:44:17.010365: step 3121, loss 0.0900064, acc 0.94
2016-09-05T19:44:17.819420: step 3122, loss 0.054843, acc 0.96
2016-09-05T19:44:18.630408: step 3123, loss 0.0669553, acc 0.96
2016-09-05T19:44:19.423076: step 3124, loss 0.00828813, acc 1
2016-09-05T19:44:20.224120: step 3125, loss 0.0692814, acc 0.96
2016-09-05T19:44:21.035025: step 3126, loss 0.0445589, acc 0.98
2016-09-05T19:44:21.822697: step 3127, loss 0.0400661, acc 0.98
2016-09-05T19:44:22.638084: step 3128, loss 0.0423732, acc 0.98
2016-09-05T19:44:23.480872: step 3129, loss 0.0614434, acc 0.96
2016-09-05T19:44:24.300474: step 3130, loss 0.00717114, acc 1
2016-09-05T19:44:25.095899: step 3131, loss 0.0180095, acc 1
2016-09-05T19:44:25.923851: step 3132, loss 0.012066, acc 1
2016-09-05T19:44:26.726731: step 3133, loss 0.0173159, acc 1
2016-09-05T19:44:27.524778: step 3134, loss 0.0310042, acc 0.98
2016-09-05T19:44:28.338950: step 3135, loss 0.0338528, acc 0.98
2016-09-05T19:44:29.110708: step 3136, loss 0.0804953, acc 0.96
2016-09-05T19:44:29.925397: step 3137, loss 0.0339504, acc 0.98
2016-09-05T19:44:30.748687: step 3138, loss 0.058678, acc 0.96
2016-09-05T19:44:31.505464: step 3139, loss 0.0578832, acc 0.98
2016-09-05T19:44:32.354523: step 3140, loss 0.122025, acc 0.96
2016-09-05T19:44:33.171054: step 3141, loss 0.104464, acc 0.96
2016-09-05T19:44:33.939819: step 3142, loss 0.0494128, acc 0.96
2016-09-05T19:44:34.726184: step 3143, loss 0.0124216, acc 1
2016-09-05T19:44:35.537630: step 3144, loss 0.00376793, acc 1
2016-09-05T19:44:36.317403: step 3145, loss 0.0747907, acc 0.96
2016-09-05T19:44:37.133327: step 3146, loss 0.0165806, acc 1
2016-09-05T19:44:37.944653: step 3147, loss 0.00399297, acc 1
2016-09-05T19:44:38.726126: step 3148, loss 0.0274281, acc 0.98
2016-09-05T19:44:39.542067: step 3149, loss 0.0316399, acc 0.98
2016-09-05T19:44:40.351202: step 3150, loss 0.0971879, acc 0.96
2016-09-05T19:44:41.133986: step 3151, loss 0.0827057, acc 0.98
2016-09-05T19:44:41.969530: step 3152, loss 0.0204816, acc 0.98
2016-09-05T19:44:42.783823: step 3153, loss 0.00968518, acc 1
2016-09-05T19:44:43.594585: step 3154, loss 0.0469279, acc 0.98
2016-09-05T19:44:44.407749: step 3155, loss 0.0081331, acc 1
2016-09-05T19:44:45.232959: step 3156, loss 0.0202227, acc 0.98
2016-09-05T19:44:46.021014: step 3157, loss 0.00538094, acc 1
2016-09-05T19:44:46.847132: step 3158, loss 0.0243674, acc 0.98
2016-09-05T19:44:47.670730: step 3159, loss 0.0595336, acc 0.98
2016-09-05T19:44:48.447060: step 3160, loss 0.0915172, acc 0.94
2016-09-05T19:44:49.235879: step 3161, loss 0.0284688, acc 0.98
2016-09-05T19:44:50.057164: step 3162, loss 0.0399299, acc 0.98
2016-09-05T19:44:50.836650: step 3163, loss 0.0191368, acc 1
2016-09-05T19:44:51.635422: step 3164, loss 0.0404433, acc 0.98
2016-09-05T19:44:52.450654: step 3165, loss 0.0203123, acc 1
2016-09-05T19:44:53.261007: step 3166, loss 0.014222, acc 1
2016-09-05T19:44:54.062287: step 3167, loss 0.111508, acc 0.96
2016-09-05T19:44:54.874562: step 3168, loss 0.0282255, acc 0.98
2016-09-05T19:44:55.685949: step 3169, loss 0.0476933, acc 0.96
2016-09-05T19:44:56.489192: step 3170, loss 0.105367, acc 0.96
2016-09-05T19:44:57.298864: step 3171, loss 0.00394168, acc 1
2016-09-05T19:44:58.090124: step 3172, loss 0.0188906, acc 1
2016-09-05T19:44:58.879709: step 3173, loss 0.0430947, acc 0.98
2016-09-05T19:44:59.701773: step 3174, loss 0.0356592, acc 0.96
2016-09-05T19:45:00.518928: step 3175, loss 0.00524309, acc 1
2016-09-05T19:45:01.369334: step 3176, loss 0.0353867, acc 0.98
2016-09-05T19:45:02.169316: step 3177, loss 0.0152436, acc 1
2016-09-05T19:45:02.948280: step 3178, loss 0.00552508, acc 1
2016-09-05T19:45:03.765800: step 3179, loss 0.0071984, acc 1
2016-09-05T19:45:04.591846: step 3180, loss 0.0365521, acc 0.98
2016-09-05T19:45:05.423146: step 3181, loss 0.0209294, acc 1
2016-09-05T19:45:06.231933: step 3182, loss 0.00724223, acc 1
2016-09-05T19:45:07.073488: step 3183, loss 0.00463019, acc 1
2016-09-05T19:45:07.894819: step 3184, loss 0.0140395, acc 1
2016-09-05T19:45:08.745138: step 3185, loss 0.0509323, acc 0.98
2016-09-05T19:45:09.567321: step 3186, loss 0.0919406, acc 0.94
2016-09-05T19:45:10.382126: step 3187, loss 0.0431172, acc 0.98
2016-09-05T19:45:11.202746: step 3188, loss 0.0523036, acc 0.98
2016-09-05T19:45:12.041871: step 3189, loss 0.0305386, acc 0.98
2016-09-05T19:45:12.841914: step 3190, loss 0.0688552, acc 0.94
2016-09-05T19:45:13.675257: step 3191, loss 0.0342154, acc 0.98
2016-09-05T19:45:14.494634: step 3192, loss 0.0305621, acc 0.98
2016-09-05T19:45:15.295219: step 3193, loss 0.00387274, acc 1
2016-09-05T19:45:16.124253: step 3194, loss 0.0219013, acc 1
2016-09-05T19:45:16.974696: step 3195, loss 0.0104162, acc 1
2016-09-05T19:45:17.771426: step 3196, loss 0.114391, acc 0.96
2016-09-05T19:45:18.577685: step 3197, loss 0.0294391, acc 1
2016-09-05T19:45:19.416582: step 3198, loss 0.00576651, acc 1
2016-09-05T19:45:20.257556: step 3199, loss 0.0384863, acc 0.98
2016-09-05T19:45:21.050675: step 3200, loss 0.0215282, acc 1

Evaluation:
2016-09-05T19:45:24.565951: step 3200, loss 1.5836, acc 0.758

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-3200

2016-09-05T19:45:26.499522: step 3201, loss 0.017653, acc 1
2016-09-05T19:45:27.302410: step 3202, loss 0.108179, acc 0.92
2016-09-05T19:45:28.092611: step 3203, loss 0.0217779, acc 1
2016-09-05T19:45:28.881203: step 3204, loss 0.0667713, acc 0.96
2016-09-05T19:45:29.663691: step 3205, loss 0.0189426, acc 1
2016-09-05T19:45:30.448251: step 3206, loss 0.0267691, acc 0.98
2016-09-05T19:45:31.251296: step 3207, loss 0.0742117, acc 0.98
2016-09-05T19:45:32.050294: step 3208, loss 0.0262892, acc 0.98
2016-09-05T19:45:32.853324: step 3209, loss 0.0402851, acc 0.98
2016-09-05T19:45:33.675030: step 3210, loss 0.0389268, acc 0.98
2016-09-05T19:45:34.450012: step 3211, loss 0.0390113, acc 0.96
2016-09-05T19:45:35.258484: step 3212, loss 0.0281459, acc 1
2016-09-05T19:45:36.070281: step 3213, loss 0.0223329, acc 1
2016-09-05T19:45:36.862235: step 3214, loss 0.0129487, acc 1
2016-09-05T19:45:37.677620: step 3215, loss 0.0753901, acc 0.96
2016-09-05T19:45:38.493792: step 3216, loss 0.0179278, acc 0.98
2016-09-05T19:45:39.269405: step 3217, loss 0.0399162, acc 1
2016-09-05T19:45:40.089532: step 3218, loss 0.0305269, acc 0.98
2016-09-05T19:45:40.916063: step 3219, loss 0.0528424, acc 0.96
2016-09-05T19:45:41.701059: step 3220, loss 0.0200008, acc 1
2016-09-05T19:45:42.499436: step 3221, loss 0.0680116, acc 0.96
2016-09-05T19:45:43.303696: step 3222, loss 0.0179035, acc 1
2016-09-05T19:45:44.067215: step 3223, loss 0.0652604, acc 0.98
2016-09-05T19:45:44.899158: step 3224, loss 0.0383785, acc 0.98
2016-09-05T19:45:45.728918: step 3225, loss 0.0552899, acc 0.96
2016-09-05T19:45:46.509401: step 3226, loss 0.0172988, acc 1
2016-09-05T19:45:47.330643: step 3227, loss 0.0112442, acc 1
2016-09-05T19:45:48.137731: step 3228, loss 0.198558, acc 0.96
2016-09-05T19:45:48.944451: step 3229, loss 0.110126, acc 0.96
2016-09-05T19:45:49.763945: step 3230, loss 0.087878, acc 0.98
2016-09-05T19:45:50.582054: step 3231, loss 0.0593372, acc 0.96
2016-09-05T19:45:51.383753: step 3232, loss 0.0121446, acc 1
2016-09-05T19:45:52.180562: step 3233, loss 0.0299432, acc 0.98
2016-09-05T19:45:52.970897: step 3234, loss 0.101615, acc 0.98
2016-09-05T19:45:53.744758: step 3235, loss 0.0268054, acc 1
2016-09-05T19:45:54.555673: step 3236, loss 0.0770918, acc 0.98
2016-09-05T19:45:55.363641: step 3237, loss 0.0363751, acc 0.98
2016-09-05T19:45:56.204656: step 3238, loss 0.0422486, acc 1
2016-09-05T19:45:57.045735: step 3239, loss 0.0324992, acc 0.98
2016-09-05T19:45:57.865439: step 3240, loss 0.0456986, acc 0.98
2016-09-05T19:45:58.658361: step 3241, loss 0.0561262, acc 0.98
2016-09-05T19:45:59.456187: step 3242, loss 0.00913558, acc 1
2016-09-05T19:46:00.298632: step 3243, loss 0.0211281, acc 1
2016-09-05T19:46:01.042683: step 3244, loss 0.0568385, acc 0.96
2016-09-05T19:46:01.847873: step 3245, loss 0.04914, acc 0.98
2016-09-05T19:46:02.622238: step 3246, loss 0.031811, acc 1
2016-09-05T19:46:03.444181: step 3247, loss 0.0187045, acc 1
2016-09-05T19:46:04.289476: step 3248, loss 0.0628561, acc 0.98
2016-09-05T19:46:05.097352: step 3249, loss 0.0430444, acc 0.98
2016-09-05T19:46:05.908955: step 3250, loss 0.0311662, acc 0.98
2016-09-05T19:46:06.724460: step 3251, loss 0.0236329, acc 0.98
2016-09-05T19:46:07.520936: step 3252, loss 0.0797283, acc 0.98
2016-09-05T19:46:08.290124: step 3253, loss 0.175786, acc 0.92
2016-09-05T19:46:09.115523: step 3254, loss 0.00665796, acc 1
2016-09-05T19:46:09.939818: step 3255, loss 0.0402275, acc 1
2016-09-05T19:46:10.738676: step 3256, loss 0.0383956, acc 0.98
2016-09-05T19:46:11.558022: step 3257, loss 0.0667468, acc 0.96
2016-09-05T19:46:12.359942: step 3258, loss 0.0406777, acc 0.96
2016-09-05T19:46:13.169613: step 3259, loss 0.0483175, acc 0.98
2016-09-05T19:46:13.966863: step 3260, loss 0.00591455, acc 1
2016-09-05T19:46:14.784321: step 3261, loss 0.0731121, acc 0.98
2016-09-05T19:46:15.589495: step 3262, loss 0.0207713, acc 0.98
2016-09-05T19:46:16.414258: step 3263, loss 0.0186512, acc 1
2016-09-05T19:46:17.226674: step 3264, loss 0.0281349, acc 1
2016-09-05T19:46:17.997589: step 3265, loss 0.023288, acc 1
2016-09-05T19:46:18.805874: step 3266, loss 0.0367419, acc 0.98
2016-09-05T19:46:19.615228: step 3267, loss 0.0262212, acc 0.98
2016-09-05T19:46:20.419160: step 3268, loss 0.10271, acc 0.98
2016-09-05T19:46:21.220988: step 3269, loss 0.00476395, acc 1
2016-09-05T19:46:22.067687: step 3270, loss 0.021063, acc 0.98
2016-09-05T19:46:22.861901: step 3271, loss 0.0112552, acc 1
2016-09-05T19:46:23.661480: step 3272, loss 0.0493104, acc 0.96
2016-09-05T19:46:24.481148: step 3273, loss 0.0401734, acc 0.98
2016-09-05T19:46:25.253202: step 3274, loss 0.0333305, acc 0.98
2016-09-05T19:46:26.043111: step 3275, loss 0.00819353, acc 1
2016-09-05T19:46:26.849330: step 3276, loss 0.0490722, acc 0.98
2016-09-05T19:46:27.653119: step 3277, loss 0.0142832, acc 1
2016-09-05T19:46:28.452444: step 3278, loss 0.00984904, acc 1
2016-09-05T19:46:29.297453: step 3279, loss 0.0341375, acc 0.98
2016-09-05T19:46:30.076940: step 3280, loss 0.112263, acc 0.98
2016-09-05T19:46:30.889883: step 3281, loss 0.00612667, acc 1
2016-09-05T19:46:31.741695: step 3282, loss 0.0119878, acc 1
2016-09-05T19:46:32.569396: step 3283, loss 0.0381676, acc 0.98
2016-09-05T19:46:33.362279: step 3284, loss 0.104679, acc 0.94
2016-09-05T19:46:34.180819: step 3285, loss 0.063694, acc 0.98
2016-09-05T19:46:34.982412: step 3286, loss 0.130403, acc 0.94
2016-09-05T19:46:35.787047: step 3287, loss 0.21971, acc 0.94
2016-09-05T19:46:36.590679: step 3288, loss 0.00763922, acc 1
2016-09-05T19:46:37.385359: step 3289, loss 0.00677053, acc 1
2016-09-05T19:46:38.196760: step 3290, loss 0.0959325, acc 0.98
2016-09-05T19:46:39.054895: step 3291, loss 0.0617938, acc 0.96
2016-09-05T19:46:39.893162: step 3292, loss 0.036533, acc 1
2016-09-05T19:46:40.711685: step 3293, loss 0.0464341, acc 0.98
2016-09-05T19:46:41.565188: step 3294, loss 0.0472765, acc 0.98
2016-09-05T19:46:42.374572: step 3295, loss 0.0598579, acc 0.96
2016-09-05T19:46:43.194051: step 3296, loss 0.0467202, acc 0.96
2016-09-05T19:46:44.020213: step 3297, loss 0.0197634, acc 1
2016-09-05T19:46:44.445845: step 3298, loss 0.0600172, acc 1
2016-09-05T19:46:45.258260: step 3299, loss 0.0797883, acc 0.94
2016-09-05T19:46:46.069107: step 3300, loss 0.0243815, acc 0.98

Evaluation:
2016-09-05T19:46:49.591938: step 3300, loss 1.36725, acc 0.742

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-3300

2016-09-05T19:46:51.519646: step 3301, loss 0.0411221, acc 0.96
2016-09-05T19:46:52.326776: step 3302, loss 0.0175665, acc 1
2016-09-05T19:46:53.148313: step 3303, loss 0.0303363, acc 1
2016-09-05T19:46:53.974206: step 3304, loss 0.0454914, acc 0.98
2016-09-05T19:46:54.786193: step 3305, loss 0.173696, acc 0.96
2016-09-05T19:46:55.570303: step 3306, loss 0.0507308, acc 0.98
2016-09-05T19:46:56.377589: step 3307, loss 0.0166509, acc 1
2016-09-05T19:46:57.204086: step 3308, loss 0.0194953, acc 1
2016-09-05T19:46:57.996347: step 3309, loss 0.00647672, acc 1
2016-09-05T19:46:58.798488: step 3310, loss 0.0295854, acc 1
2016-09-05T19:46:59.616695: step 3311, loss 0.148475, acc 0.94
2016-09-05T19:47:00.433795: step 3312, loss 0.0287074, acc 1
2016-09-05T19:47:01.232275: step 3313, loss 0.0774049, acc 0.96
2016-09-05T19:47:02.053043: step 3314, loss 0.0148303, acc 1
2016-09-05T19:47:02.828670: step 3315, loss 0.0368285, acc 0.98
2016-09-05T19:47:03.661459: step 3316, loss 0.0421745, acc 0.96
2016-09-05T19:47:04.480914: step 3317, loss 0.0178481, acc 1
2016-09-05T19:47:05.259197: step 3318, loss 0.079251, acc 0.94
2016-09-05T19:47:06.045113: step 3319, loss 0.0342719, acc 1
2016-09-05T19:47:06.843820: step 3320, loss 0.12236, acc 0.9
2016-09-05T19:47:07.621782: step 3321, loss 0.0444648, acc 0.98
2016-09-05T19:47:08.460702: step 3322, loss 0.0429828, acc 0.98
2016-09-05T19:47:09.265227: step 3323, loss 0.0539519, acc 0.96
2016-09-05T19:47:10.057245: step 3324, loss 0.049817, acc 0.98
2016-09-05T19:47:10.882619: step 3325, loss 0.0296924, acc 0.98
2016-09-05T19:47:11.703937: step 3326, loss 0.0174205, acc 1
2016-09-05T19:47:12.505303: step 3327, loss 0.00894668, acc 1
2016-09-05T19:47:13.315735: step 3328, loss 0.0376889, acc 0.98
2016-09-05T19:47:14.111581: step 3329, loss 0.023827, acc 0.98
2016-09-05T19:47:14.905591: step 3330, loss 0.0885595, acc 0.96
2016-09-05T19:47:15.706452: step 3331, loss 0.0172866, acc 1
2016-09-05T19:47:16.528121: step 3332, loss 0.0440788, acc 0.96
2016-09-05T19:47:17.325760: step 3333, loss 0.035839, acc 0.98
2016-09-05T19:47:18.121751: step 3334, loss 0.054077, acc 0.96
2016-09-05T19:47:18.967773: step 3335, loss 0.0527429, acc 0.98
2016-09-05T19:47:19.755431: step 3336, loss 0.0835743, acc 0.98
2016-09-05T19:47:20.557902: step 3337, loss 0.00741944, acc 1
2016-09-05T19:47:21.367111: step 3338, loss 0.0052383, acc 1
2016-09-05T19:47:22.150712: step 3339, loss 0.0263321, acc 1
2016-09-05T19:47:22.937781: step 3340, loss 0.0440252, acc 1
2016-09-05T19:47:23.761672: step 3341, loss 0.00961347, acc 1
2016-09-05T19:47:24.543016: step 3342, loss 0.014362, acc 1
2016-09-05T19:47:25.351125: step 3343, loss 0.0555249, acc 0.98
2016-09-05T19:47:26.174366: step 3344, loss 0.0510548, acc 0.98
2016-09-05T19:47:26.957882: step 3345, loss 0.0064242, acc 1
2016-09-05T19:47:27.756502: step 3346, loss 0.073539, acc 0.94
2016-09-05T19:47:28.566054: step 3347, loss 0.0218422, acc 0.98
2016-09-05T19:47:29.425020: step 3348, loss 0.0213775, acc 1
2016-09-05T19:47:30.213339: step 3349, loss 0.0127377, acc 1
2016-09-05T19:47:31.049238: step 3350, loss 0.0334366, acc 0.98
2016-09-05T19:47:31.837442: step 3351, loss 0.0271899, acc 0.98
2016-09-05T19:47:32.641705: step 3352, loss 0.0283592, acc 1
2016-09-05T19:47:33.465418: step 3353, loss 0.0320511, acc 1
2016-09-05T19:47:34.258193: step 3354, loss 0.124966, acc 0.96
2016-09-05T19:47:35.045785: step 3355, loss 0.0348436, acc 1
2016-09-05T19:47:35.865503: step 3356, loss 0.0393961, acc 0.98
2016-09-05T19:47:36.656382: step 3357, loss 0.0169024, acc 1
2016-09-05T19:47:37.459847: step 3358, loss 0.0147834, acc 1
2016-09-05T19:47:38.284984: step 3359, loss 0.0318951, acc 0.98
2016-09-05T19:47:39.087203: step 3360, loss 0.0223511, acc 1
2016-09-05T19:47:39.893862: step 3361, loss 0.0337358, acc 0.96
2016-09-05T19:47:40.726791: step 3362, loss 0.162329, acc 0.96
2016-09-05T19:47:41.501066: step 3363, loss 0.0888205, acc 0.96
2016-09-05T19:47:42.293301: step 3364, loss 0.0601006, acc 0.96
2016-09-05T19:47:43.114646: step 3365, loss 0.0229615, acc 1
2016-09-05T19:47:43.894640: step 3366, loss 0.00523637, acc 1
2016-09-05T19:47:44.691477: step 3367, loss 0.0148858, acc 1
2016-09-05T19:47:45.504255: step 3368, loss 0.0287242, acc 1
2016-09-05T19:47:46.293477: step 3369, loss 0.0231903, acc 1
2016-09-05T19:47:47.097505: step 3370, loss 0.0233453, acc 1
2016-09-05T19:47:47.893926: step 3371, loss 0.00664883, acc 1
2016-09-05T19:47:48.707618: step 3372, loss 0.0920162, acc 0.96
2016-09-05T19:47:49.542070: step 3373, loss 0.0324199, acc 0.98
2016-09-05T19:47:50.355413: step 3374, loss 0.0349317, acc 0.98
2016-09-05T19:47:51.155531: step 3375, loss 0.00591654, acc 1
2016-09-05T19:47:51.946352: step 3376, loss 0.0242832, acc 0.98
2016-09-05T19:47:52.773125: step 3377, loss 0.0291382, acc 1
2016-09-05T19:47:53.579649: step 3378, loss 0.0229853, acc 1
2016-09-05T19:47:54.390520: step 3379, loss 0.0563401, acc 0.98
2016-09-05T19:47:55.205164: step 3380, loss 0.0513777, acc 0.96
2016-09-05T19:47:55.993864: step 3381, loss 0.0192103, acc 0.98
2016-09-05T19:47:56.796442: step 3382, loss 0.0220812, acc 1
2016-09-05T19:47:57.606496: step 3383, loss 0.0320469, acc 0.98
2016-09-05T19:47:58.392606: step 3384, loss 0.0102667, acc 1
2016-09-05T19:47:59.203272: step 3385, loss 0.0392471, acc 0.98
2016-09-05T19:48:00.016529: step 3386, loss 0.0709803, acc 0.94
2016-09-05T19:48:00.845034: step 3387, loss 0.0485697, acc 0.98
2016-09-05T19:48:01.640047: step 3388, loss 0.0209193, acc 1
2016-09-05T19:48:02.448622: step 3389, loss 0.0188828, acc 1
2016-09-05T19:48:03.242321: step 3390, loss 0.101116, acc 0.96
2016-09-05T19:48:04.035663: step 3391, loss 0.0632743, acc 0.98
2016-09-05T19:48:04.864262: step 3392, loss 0.00701961, acc 1
2016-09-05T19:48:05.637164: step 3393, loss 0.0798054, acc 0.96
2016-09-05T19:48:06.448297: step 3394, loss 0.0699802, acc 0.96
2016-09-05T19:48:07.253250: step 3395, loss 0.0198632, acc 1
2016-09-05T19:48:08.045862: step 3396, loss 0.0981648, acc 0.94
2016-09-05T19:48:08.854877: step 3397, loss 0.0183725, acc 1
2016-09-05T19:48:09.681854: step 3398, loss 0.0245011, acc 1
2016-09-05T19:48:10.453670: step 3399, loss 0.0189754, acc 1
2016-09-05T19:48:11.271564: step 3400, loss 0.0246823, acc 1

Evaluation:
2016-09-05T19:48:14.824913: step 3400, loss 1.55592, acc 0.736

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-3400

2016-09-05T19:48:16.686052: step 3401, loss 0.0616193, acc 0.98
2016-09-05T19:48:17.489530: step 3402, loss 0.0619255, acc 0.96
2016-09-05T19:48:18.313421: step 3403, loss 0.0152866, acc 1
2016-09-05T19:48:19.067719: step 3404, loss 0.0219408, acc 1
2016-09-05T19:48:19.879298: step 3405, loss 0.0373841, acc 0.96
2016-09-05T19:48:20.681423: step 3406, loss 0.0425576, acc 0.98
2016-09-05T19:48:21.491042: step 3407, loss 0.0676687, acc 0.98
2016-09-05T19:48:22.294512: step 3408, loss 0.0125793, acc 1
2016-09-05T19:48:23.109494: step 3409, loss 0.0281901, acc 0.98
2016-09-05T19:48:23.903721: step 3410, loss 0.0253427, acc 0.98
2016-09-05T19:48:24.721633: step 3411, loss 0.0541705, acc 0.96
2016-09-05T19:48:25.555276: step 3412, loss 0.11039, acc 0.96
2016-09-05T19:48:26.358682: step 3413, loss 0.0946978, acc 0.96
2016-09-05T19:48:27.180938: step 3414, loss 0.0532922, acc 1
2016-09-05T19:48:27.967322: step 3415, loss 0.00521563, acc 1
2016-09-05T19:48:28.760606: step 3416, loss 0.00727392, acc 1
2016-09-05T19:48:29.554364: step 3417, loss 0.0121808, acc 1
2016-09-05T19:48:30.414112: step 3418, loss 0.0119315, acc 1
2016-09-05T19:48:31.211691: step 3419, loss 0.0131787, acc 1
2016-09-05T19:48:32.006740: step 3420, loss 0.0662188, acc 0.98
2016-09-05T19:48:32.826265: step 3421, loss 0.0207668, acc 1
2016-09-05T19:48:33.631877: step 3422, loss 0.0301731, acc 0.98
2016-09-05T19:48:34.434893: step 3423, loss 0.0720631, acc 0.98
2016-09-05T19:48:35.255455: step 3424, loss 0.0205203, acc 1
2016-09-05T19:48:36.067120: step 3425, loss 0.0387027, acc 1
2016-09-05T19:48:36.870193: step 3426, loss 0.020923, acc 1
2016-09-05T19:48:37.651745: step 3427, loss 0.00858915, acc 1
2016-09-05T19:48:38.426238: step 3428, loss 0.0692464, acc 0.98
2016-09-05T19:48:39.236181: step 3429, loss 0.0295056, acc 1
2016-09-05T19:48:40.081452: step 3430, loss 0.0835947, acc 0.98
2016-09-05T19:48:40.882879: step 3431, loss 0.0472905, acc 0.98
2016-09-05T19:48:41.685047: step 3432, loss 0.0501084, acc 0.98
2016-09-05T19:48:42.478579: step 3433, loss 0.0367533, acc 1
2016-09-05T19:48:43.253454: step 3434, loss 0.0230925, acc 1
2016-09-05T19:48:44.048106: step 3435, loss 0.0526371, acc 0.98
2016-09-05T19:48:44.864428: step 3436, loss 0.00427216, acc 1
2016-09-05T19:48:45.649690: step 3437, loss 0.0285806, acc 0.98
2016-09-05T19:48:46.458587: step 3438, loss 0.0292243, acc 1
2016-09-05T19:48:47.289484: step 3439, loss 0.0918513, acc 0.96
2016-09-05T19:48:48.085038: step 3440, loss 0.0343314, acc 0.98
2016-09-05T19:48:48.888747: step 3441, loss 0.0216265, acc 0.98
2016-09-05T19:48:49.707837: step 3442, loss 0.0123875, acc 1
2016-09-05T19:48:50.509377: step 3443, loss 0.0300413, acc 1
2016-09-05T19:48:51.310354: step 3444, loss 0.0159207, acc 1
2016-09-05T19:48:52.108807: step 3445, loss 0.0932626, acc 0.96
2016-09-05T19:48:52.905485: step 3446, loss 0.0251421, acc 1
2016-09-05T19:48:53.733309: step 3447, loss 0.0172655, acc 1
2016-09-05T19:48:54.540902: step 3448, loss 0.00506963, acc 1
2016-09-05T19:48:55.349147: step 3449, loss 0.0352944, acc 1
2016-09-05T19:48:56.164470: step 3450, loss 0.0335077, acc 0.98
2016-09-05T19:48:56.980277: step 3451, loss 0.0287677, acc 0.98
2016-09-05T19:48:57.776967: step 3452, loss 0.123588, acc 0.96
2016-09-05T19:48:58.561104: step 3453, loss 0.039852, acc 0.98
2016-09-05T19:48:59.367128: step 3454, loss 0.0438609, acc 0.98
2016-09-05T19:49:00.174184: step 3455, loss 0.0241523, acc 1
2016-09-05T19:49:01.007005: step 3456, loss 0.0104795, acc 1
2016-09-05T19:49:01.816522: step 3457, loss 0.0113357, acc 1
2016-09-05T19:49:02.607318: step 3458, loss 0.039873, acc 0.98
2016-09-05T19:49:03.452474: step 3459, loss 0.104033, acc 0.92
2016-09-05T19:49:04.284470: step 3460, loss 0.0967356, acc 0.98
2016-09-05T19:49:05.072189: step 3461, loss 0.0814828, acc 0.96
2016-09-05T19:49:05.861678: step 3462, loss 0.00811413, acc 1
2016-09-05T19:49:06.687944: step 3463, loss 0.0763548, acc 0.98
2016-09-05T19:49:07.481947: step 3464, loss 0.0310535, acc 1
2016-09-05T19:49:08.287655: step 3465, loss 0.00936091, acc 1
2016-09-05T19:49:09.094660: step 3466, loss 0.0538424, acc 0.98
2016-09-05T19:49:09.872494: step 3467, loss 0.0340018, acc 0.98
2016-09-05T19:49:10.681128: step 3468, loss 0.111662, acc 0.94
2016-09-05T19:49:11.482562: step 3469, loss 0.0539226, acc 0.96
2016-09-05T19:49:12.287145: step 3470, loss 0.00718587, acc 1
2016-09-05T19:49:13.102710: step 3471, loss 0.0454892, acc 0.98
2016-09-05T19:49:13.928063: step 3472, loss 0.030092, acc 0.98
2016-09-05T19:49:14.695714: step 3473, loss 0.0342597, acc 1
2016-09-05T19:49:15.502062: step 3474, loss 0.0454472, acc 1
2016-09-05T19:49:16.329934: step 3475, loss 0.00968259, acc 1
2016-09-05T19:49:17.102828: step 3476, loss 0.00544598, acc 1
2016-09-05T19:49:17.916107: step 3477, loss 0.0201445, acc 1
2016-09-05T19:49:18.715171: step 3478, loss 0.0190157, acc 0.98
2016-09-05T19:49:19.485611: step 3479, loss 0.0450134, acc 0.98
2016-09-05T19:49:20.315794: step 3480, loss 0.0199767, acc 1
2016-09-05T19:49:21.145451: step 3481, loss 0.0203514, acc 1
2016-09-05T19:49:21.925868: step 3482, loss 0.0200788, acc 0.98
2016-09-05T19:49:22.734462: step 3483, loss 0.0452408, acc 0.98
2016-09-05T19:49:23.555721: step 3484, loss 0.00454692, acc 1
2016-09-05T19:49:24.354162: step 3485, loss 0.0643403, acc 0.94
2016-09-05T19:49:25.154415: step 3486, loss 0.00861704, acc 1
2016-09-05T19:49:25.986807: step 3487, loss 0.0700452, acc 0.98
2016-09-05T19:49:26.758268: step 3488, loss 0.0962977, acc 0.96
2016-09-05T19:49:27.556985: step 3489, loss 0.0521734, acc 0.98
2016-09-05T19:49:28.370801: step 3490, loss 0.0540571, acc 0.98
2016-09-05T19:49:29.135222: step 3491, loss 0.0441821, acc 0.98
2016-09-05T19:49:29.585506: step 3492, loss 0.0575421, acc 1
2016-09-05T19:49:30.385615: step 3493, loss 0.0355729, acc 0.98
2016-09-05T19:49:31.188341: step 3494, loss 0.0529287, acc 0.96
2016-09-05T19:49:32.021528: step 3495, loss 0.0416724, acc 0.98
2016-09-05T19:49:32.820223: step 3496, loss 0.0170728, acc 1
2016-09-05T19:49:33.647061: step 3497, loss 0.00657123, acc 1
2016-09-05T19:49:34.477180: step 3498, loss 0.0628627, acc 0.96
2016-09-05T19:49:35.258376: step 3499, loss 0.0704236, acc 0.96
2016-09-05T19:49:36.034796: step 3500, loss 0.0240471, acc 0.98

Evaluation:
2016-09-05T19:49:39.550732: step 3500, loss 1.59551, acc 0.749

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-3500

2016-09-05T19:49:41.522477: step 3501, loss 0.0537246, acc 0.98
2016-09-05T19:49:42.325666: step 3502, loss 0.0130251, acc 1
2016-09-05T19:49:43.144315: step 3503, loss 0.0393743, acc 0.98
2016-09-05T19:49:43.991412: step 3504, loss 0.0469617, acc 0.98
2016-09-05T19:49:44.858895: step 3505, loss 0.106273, acc 0.98
2016-09-05T19:49:45.712663: step 3506, loss 0.103187, acc 0.94
2016-09-05T19:49:46.509700: step 3507, loss 0.0306289, acc 0.98
2016-09-05T19:49:47.309180: step 3508, loss 0.0200816, acc 1
2016-09-05T19:49:48.093710: step 3509, loss 0.0119281, acc 1
2016-09-05T19:49:48.894205: step 3510, loss 0.00622422, acc 1
2016-09-05T19:49:49.721156: step 3511, loss 0.0191009, acc 1
2016-09-05T19:49:50.518507: step 3512, loss 0.00620257, acc 1
2016-09-05T19:49:51.310871: step 3513, loss 0.0407034, acc 0.98
2016-09-05T19:49:52.108232: step 3514, loss 0.12591, acc 0.94
2016-09-05T19:49:52.955996: step 3515, loss 0.00416031, acc 1
2016-09-05T19:49:53.774960: step 3516, loss 0.0187637, acc 0.98
2016-09-05T19:49:54.578721: step 3517, loss 0.0465569, acc 0.96
2016-09-05T19:49:55.417384: step 3518, loss 0.0169649, acc 1
2016-09-05T19:49:56.216051: step 3519, loss 0.00496169, acc 1
2016-09-05T19:49:56.991902: step 3520, loss 0.0212104, acc 1
2016-09-05T19:49:57.819539: step 3521, loss 0.0471898, acc 0.98
2016-09-05T19:49:58.628730: step 3522, loss 0.0561938, acc 0.98
2016-09-05T19:49:59.416763: step 3523, loss 0.0623563, acc 0.98
2016-09-05T19:50:00.204426: step 3524, loss 0.0545728, acc 1
2016-09-05T19:50:01.048520: step 3525, loss 0.0545719, acc 0.98
2016-09-05T19:50:01.839976: step 3526, loss 0.0390544, acc 0.96
2016-09-05T19:50:02.626433: step 3527, loss 0.0183415, acc 1
2016-09-05T19:50:03.440545: step 3528, loss 0.0893384, acc 0.96
2016-09-05T19:50:04.236595: step 3529, loss 0.00320437, acc 1
2016-09-05T19:50:05.053666: step 3530, loss 0.0762423, acc 0.96
2016-09-05T19:50:05.885509: step 3531, loss 0.00526357, acc 1
2016-09-05T19:50:06.656735: step 3532, loss 0.00549032, acc 1
2016-09-05T19:50:07.490708: step 3533, loss 0.019849, acc 0.98
2016-09-05T19:50:08.290431: step 3534, loss 0.0259949, acc 1
2016-09-05T19:50:09.067979: step 3535, loss 0.0186992, acc 1
2016-09-05T19:50:09.860238: step 3536, loss 0.057538, acc 0.98
2016-09-05T19:50:10.685103: step 3537, loss 0.0275952, acc 1
2016-09-05T19:50:11.491794: step 3538, loss 0.0531557, acc 0.98
2016-09-05T19:50:12.272506: step 3539, loss 0.0694651, acc 0.98
2016-09-05T19:50:13.113439: step 3540, loss 0.00650131, acc 1
2016-09-05T19:50:13.933444: step 3541, loss 0.00716554, acc 1
2016-09-05T19:50:14.736955: step 3542, loss 0.0531362, acc 0.96
2016-09-05T19:50:15.553245: step 3543, loss 0.040894, acc 0.98
2016-09-05T19:50:16.356538: step 3544, loss 0.0898984, acc 0.96
2016-09-05T19:50:17.168190: step 3545, loss 0.0235742, acc 1
2016-09-05T19:50:17.986995: step 3546, loss 0.0536679, acc 0.98
2016-09-05T19:50:18.808668: step 3547, loss 0.0448541, acc 0.96
2016-09-05T19:50:19.608974: step 3548, loss 0.0165231, acc 1
2016-09-05T19:50:20.426179: step 3549, loss 0.0368822, acc 0.98
2016-09-05T19:50:21.239364: step 3550, loss 0.00756804, acc 1
2016-09-05T19:50:22.034995: step 3551, loss 0.0203613, acc 1
2016-09-05T19:50:22.859122: step 3552, loss 0.00586208, acc 1
2016-09-05T19:50:23.673460: step 3553, loss 0.0669984, acc 0.98
2016-09-05T19:50:24.471027: step 3554, loss 0.0166591, acc 1
2016-09-05T19:50:25.316528: step 3555, loss 0.043071, acc 0.98
2016-09-05T19:50:26.097568: step 3556, loss 0.047811, acc 0.96
2016-09-05T19:50:26.910669: step 3557, loss 0.0255573, acc 1
2016-09-05T19:50:27.781071: step 3558, loss 0.0299154, acc 1
2016-09-05T19:50:28.589823: step 3559, loss 0.0161538, acc 1
2016-09-05T19:50:29.380813: step 3560, loss 0.0149296, acc 1
2016-09-05T19:50:30.205383: step 3561, loss 0.00596667, acc 1
2016-09-05T19:50:31.010679: step 3562, loss 0.012553, acc 1
2016-09-05T19:50:31.834160: step 3563, loss 0.0174582, acc 1
2016-09-05T19:50:32.687662: step 3564, loss 0.0582376, acc 0.98
2016-09-05T19:50:33.488912: step 3565, loss 0.00344647, acc 1
2016-09-05T19:50:34.312851: step 3566, loss 0.0125257, acc 1
2016-09-05T19:50:35.152436: step 3567, loss 0.00732452, acc 1
2016-09-05T19:50:35.949253: step 3568, loss 0.0250282, acc 1
2016-09-05T19:50:36.742240: step 3569, loss 0.0103653, acc 1
2016-09-05T19:50:37.561881: step 3570, loss 0.0872129, acc 0.98
2016-09-05T19:50:38.374002: step 3571, loss 0.0148499, acc 1
2016-09-05T19:50:39.178361: step 3572, loss 0.0366431, acc 0.98
2016-09-05T19:50:40.025459: step 3573, loss 0.0272763, acc 1
2016-09-05T19:50:40.808349: step 3574, loss 0.0831366, acc 0.98
2016-09-05T19:50:41.668791: step 3575, loss 0.0220011, acc 0.98
2016-09-05T19:50:42.499298: step 3576, loss 0.00599744, acc 1
2016-09-05T19:50:43.316865: step 3577, loss 0.0106584, acc 1
2016-09-05T19:50:44.099896: step 3578, loss 0.111482, acc 0.94
2016-09-05T19:50:44.926381: step 3579, loss 0.0348706, acc 0.98
2016-09-05T19:50:45.741802: step 3580, loss 0.0142225, acc 1
2016-09-05T19:50:46.540615: step 3581, loss 0.0216765, acc 1
2016-09-05T19:50:47.335942: step 3582, loss 0.0174436, acc 1
2016-09-05T19:50:48.165097: step 3583, loss 0.0077256, acc 1
2016-09-05T19:50:48.957960: step 3584, loss 0.00533113, acc 1
2016-09-05T19:50:49.767626: step 3585, loss 0.0349029, acc 0.98
2016-09-05T19:50:50.614563: step 3586, loss 0.0572147, acc 0.98
2016-09-05T19:50:51.382757: step 3587, loss 0.0314294, acc 0.98
2016-09-05T19:50:52.210978: step 3588, loss 0.0401419, acc 0.98
2016-09-05T19:50:53.035416: step 3589, loss 0.0223377, acc 1
2016-09-05T19:50:53.827717: step 3590, loss 0.0293722, acc 1
2016-09-05T19:50:54.619527: step 3591, loss 0.0334669, acc 0.98
2016-09-05T19:50:55.425391: step 3592, loss 0.0464821, acc 0.98
2016-09-05T19:50:56.244705: step 3593, loss 0.0140095, acc 1
2016-09-05T19:50:57.054944: step 3594, loss 0.0361196, acc 0.98
2016-09-05T19:50:57.863615: step 3595, loss 0.0199448, acc 1
2016-09-05T19:50:58.656957: step 3596, loss 0.0196112, acc 1
2016-09-05T19:50:59.443926: step 3597, loss 0.0100878, acc 1
2016-09-05T19:51:00.285087: step 3598, loss 0.0975371, acc 0.96
2016-09-05T19:51:01.060704: step 3599, loss 0.019899, acc 1
2016-09-05T19:51:01.860025: step 3600, loss 0.0136847, acc 1

Evaluation:
2016-09-05T19:51:05.378980: step 3600, loss 1.85777, acc 0.733

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-3600

2016-09-05T19:51:07.263293: step 3601, loss 0.0135391, acc 1
2016-09-05T19:51:08.071635: step 3602, loss 0.0057841, acc 1
2016-09-05T19:51:08.910987: step 3603, loss 0.0934408, acc 0.96
2016-09-05T19:51:09.728794: step 3604, loss 0.00353806, acc 1
2016-09-05T19:51:10.531717: step 3605, loss 0.100662, acc 0.96
2016-09-05T19:51:11.366940: step 3606, loss 0.0646589, acc 0.98
2016-09-05T19:51:12.156350: step 3607, loss 0.0241391, acc 0.98
2016-09-05T19:51:12.923254: step 3608, loss 0.041158, acc 0.98
2016-09-05T19:51:13.767505: step 3609, loss 0.010227, acc 1
2016-09-05T19:51:14.537825: step 3610, loss 0.0857058, acc 0.94
2016-09-05T19:51:15.346355: step 3611, loss 0.0173338, acc 1
2016-09-05T19:51:16.168344: step 3612, loss 0.0910326, acc 0.96
2016-09-05T19:51:16.990154: step 3613, loss 0.0138537, acc 1
2016-09-05T19:51:17.797353: step 3614, loss 0.0268046, acc 0.98
2016-09-05T19:51:18.618713: step 3615, loss 0.00521246, acc 1
2016-09-05T19:51:19.420839: step 3616, loss 0.0159664, acc 1
2016-09-05T19:51:20.237396: step 3617, loss 0.11989, acc 0.96
2016-09-05T19:51:21.078430: step 3618, loss 0.0544691, acc 0.96
2016-09-05T19:51:21.893313: step 3619, loss 0.0441736, acc 0.98
2016-09-05T19:51:22.715044: step 3620, loss 0.108332, acc 0.96
2016-09-05T19:51:23.534003: step 3621, loss 0.0152146, acc 1
2016-09-05T19:51:24.342942: step 3622, loss 0.0533435, acc 0.98
2016-09-05T19:51:25.138755: step 3623, loss 0.029891, acc 0.98
2016-09-05T19:51:25.975682: step 3624, loss 0.0224158, acc 1
2016-09-05T19:51:26.806531: step 3625, loss 0.0429964, acc 0.98
2016-09-05T19:51:27.607246: step 3626, loss 0.01546, acc 1
2016-09-05T19:51:28.401208: step 3627, loss 0.0136628, acc 1
2016-09-05T19:51:29.239437: step 3628, loss 0.00715058, acc 1
2016-09-05T19:51:30.032375: step 3629, loss 0.058885, acc 0.96
2016-09-05T19:51:30.837937: step 3630, loss 0.00522816, acc 1
2016-09-05T19:51:31.664004: step 3631, loss 0.0219991, acc 1
2016-09-05T19:51:32.456607: step 3632, loss 0.0701061, acc 0.98
2016-09-05T19:51:33.258701: step 3633, loss 0.0636554, acc 0.96
2016-09-05T19:51:34.055205: step 3634, loss 0.0109057, acc 1
2016-09-05T19:51:34.857510: step 3635, loss 0.019937, acc 1
2016-09-05T19:51:35.653794: step 3636, loss 0.0212612, acc 0.98
2016-09-05T19:51:36.470454: step 3637, loss 0.0201071, acc 1
2016-09-05T19:51:37.256487: step 3638, loss 0.00833238, acc 1
2016-09-05T19:51:38.063124: step 3639, loss 0.118035, acc 0.96
2016-09-05T19:51:38.900440: step 3640, loss 0.0309437, acc 0.98
2016-09-05T19:51:39.687662: step 3641, loss 0.101819, acc 0.94
2016-09-05T19:51:40.493590: step 3642, loss 0.0667273, acc 0.94
2016-09-05T19:51:41.304565: step 3643, loss 0.0406079, acc 0.98
2016-09-05T19:51:42.084982: step 3644, loss 0.0584577, acc 0.98
2016-09-05T19:51:42.891386: step 3645, loss 0.031002, acc 0.98
2016-09-05T19:51:43.722976: step 3646, loss 0.023338, acc 1
2016-09-05T19:51:44.523582: step 3647, loss 0.100247, acc 0.94
2016-09-05T19:51:45.332750: step 3648, loss 0.0121882, acc 1
2016-09-05T19:51:46.142754: step 3649, loss 0.0981451, acc 0.94
2016-09-05T19:51:46.960630: step 3650, loss 0.0749508, acc 0.96
2016-09-05T19:51:47.800242: step 3651, loss 0.122978, acc 0.98
2016-09-05T19:51:48.589371: step 3652, loss 0.0949282, acc 0.96
2016-09-05T19:51:49.387267: step 3653, loss 0.0293931, acc 1
2016-09-05T19:51:50.178823: step 3654, loss 0.0385063, acc 0.98
2016-09-05T19:51:51.006769: step 3655, loss 0.00530892, acc 1
2016-09-05T19:51:51.800548: step 3656, loss 0.0148108, acc 1
2016-09-05T19:51:52.587914: step 3657, loss 0.0572133, acc 0.98
2016-09-05T19:51:53.439347: step 3658, loss 0.0612165, acc 0.96
2016-09-05T19:51:54.207499: step 3659, loss 0.0808363, acc 0.98
2016-09-05T19:51:55.019351: step 3660, loss 0.023513, acc 0.98
2016-09-05T19:51:55.816713: step 3661, loss 0.0688181, acc 0.98
2016-09-05T19:51:56.636734: step 3662, loss 0.104597, acc 0.94
2016-09-05T19:51:57.443646: step 3663, loss 0.029268, acc 1
2016-09-05T19:51:58.257666: step 3664, loss 0.0237222, acc 1
2016-09-05T19:51:59.048222: step 3665, loss 0.047838, acc 0.98
2016-09-05T19:51:59.846554: step 3666, loss 0.0852781, acc 0.98
2016-09-05T19:52:00.662445: step 3667, loss 0.0905148, acc 0.96
2016-09-05T19:52:01.451510: step 3668, loss 0.0616385, acc 0.96
2016-09-05T19:52:02.265198: step 3669, loss 0.020138, acc 1
2016-09-05T19:52:03.098444: step 3670, loss 0.0341242, acc 0.98
2016-09-05T19:52:03.885742: step 3671, loss 0.0562649, acc 0.98
2016-09-05T19:52:04.679802: step 3672, loss 0.0229502, acc 0.98
2016-09-05T19:52:05.511740: step 3673, loss 0.0408548, acc 1
2016-09-05T19:52:06.300828: step 3674, loss 0.0395897, acc 0.98
2016-09-05T19:52:07.097409: step 3675, loss 0.0038128, acc 1
2016-09-05T19:52:07.933052: step 3676, loss 0.0201071, acc 0.98
2016-09-05T19:52:08.700328: step 3677, loss 0.00442182, acc 1
2016-09-05T19:52:09.484131: step 3678, loss 0.16848, acc 0.94
2016-09-05T19:52:10.278614: step 3679, loss 0.111029, acc 0.96
2016-09-05T19:52:11.118891: step 3680, loss 0.0356789, acc 0.98
2016-09-05T19:52:11.905930: step 3681, loss 0.0250494, acc 0.98
2016-09-05T19:52:12.710087: step 3682, loss 0.0236762, acc 0.98
2016-09-05T19:52:13.488560: step 3683, loss 0.0199795, acc 1
2016-09-05T19:52:14.292293: step 3684, loss 0.032812, acc 0.98
2016-09-05T19:52:15.094357: step 3685, loss 0.114597, acc 0.94
2016-09-05T19:52:15.509881: step 3686, loss 0.0144593, acc 1
2016-09-05T19:52:16.325835: step 3687, loss 0.0215487, acc 1
2016-09-05T19:52:17.108894: step 3688, loss 0.0221711, acc 0.98
2016-09-05T19:52:17.931138: step 3689, loss 0.0642659, acc 0.96
2016-09-05T19:52:18.987459: step 3690, loss 0.0166701, acc 1
2016-09-05T19:52:19.789905: step 3691, loss 0.00420317, acc 1
2016-09-05T19:52:20.587248: step 3692, loss 0.0590262, acc 0.96
2016-09-05T19:52:21.446451: step 3693, loss 0.0105243, acc 1
2016-09-05T19:52:22.290863: step 3694, loss 0.129326, acc 0.96
2016-09-05T19:52:23.103946: step 3695, loss 0.0546534, acc 0.98
2016-09-05T19:52:23.926881: step 3696, loss 0.0392039, acc 0.98
2016-09-05T19:52:24.721322: step 3697, loss 0.0472946, acc 0.98
2016-09-05T19:52:25.525990: step 3698, loss 0.029218, acc 1
2016-09-05T19:52:26.327390: step 3699, loss 0.018384, acc 1
2016-09-05T19:52:27.129718: step 3700, loss 0.0453451, acc 0.98

Evaluation:
2016-09-05T19:52:30.667683: step 3700, loss 1.518, acc 0.74

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-3700

2016-09-05T19:52:32.563664: step 3701, loss 0.0118531, acc 1
2016-09-05T19:52:33.393936: step 3702, loss 0.00398773, acc 1
2016-09-05T19:52:34.184673: step 3703, loss 0.0217835, acc 1
2016-09-05T19:52:35.007017: step 3704, loss 0.0465709, acc 0.98
2016-09-05T19:52:35.823787: step 3705, loss 0.0511613, acc 0.98
2016-09-05T19:52:36.628471: step 3706, loss 0.0361795, acc 0.96
2016-09-05T19:52:37.432621: step 3707, loss 0.0456043, acc 0.98
2016-09-05T19:52:38.247419: step 3708, loss 0.0389714, acc 0.98
2016-09-05T19:52:39.049309: step 3709, loss 0.0809093, acc 0.98
2016-09-05T19:52:39.871435: step 3710, loss 0.0118203, acc 1
2016-09-05T19:52:40.721045: step 3711, loss 0.00430432, acc 1
2016-09-05T19:52:41.493229: step 3712, loss 0.0286419, acc 0.98
2016-09-05T19:52:42.282908: step 3713, loss 0.0193513, acc 0.98
2016-09-05T19:52:43.083937: step 3714, loss 0.0097534, acc 1
2016-09-05T19:52:43.880204: step 3715, loss 0.0178995, acc 1
2016-09-05T19:52:44.683988: step 3716, loss 0.00867063, acc 1
2016-09-05T19:52:45.494451: step 3717, loss 0.0326638, acc 0.98
2016-09-05T19:52:46.289398: step 3718, loss 0.0337244, acc 1
2016-09-05T19:52:47.083403: step 3719, loss 0.0284424, acc 1
2016-09-05T19:52:47.907845: step 3720, loss 0.0189473, acc 1
2016-09-05T19:52:48.679355: step 3721, loss 0.0270439, acc 1
2016-09-05T19:52:49.501608: step 3722, loss 0.039934, acc 1
2016-09-05T19:52:50.335027: step 3723, loss 0.0158937, acc 1
2016-09-05T19:52:51.128404: step 3724, loss 0.112411, acc 0.94
2016-09-05T19:52:51.916272: step 3725, loss 0.00551533, acc 1
2016-09-05T19:52:52.733901: step 3726, loss 0.00510765, acc 1
2016-09-05T19:52:53.524551: step 3727, loss 0.0686947, acc 0.96
2016-09-05T19:52:54.344936: step 3728, loss 0.0145151, acc 1
2016-09-05T19:52:55.160834: step 3729, loss 0.0257865, acc 0.98
2016-09-05T19:52:55.945427: step 3730, loss 0.0548588, acc 0.96
2016-09-05T19:52:56.742309: step 3731, loss 0.0204221, acc 0.98
2016-09-05T19:52:57.554103: step 3732, loss 0.0130345, acc 1
2016-09-05T19:52:58.387976: step 3733, loss 0.19992, acc 0.94
2016-09-05T19:52:59.183602: step 3734, loss 0.0440629, acc 0.98
2016-09-05T19:53:00.006602: step 3735, loss 0.0513669, acc 0.98
2016-09-05T19:53:00.803498: step 3736, loss 0.014235, acc 1
2016-09-05T19:53:01.607149: step 3737, loss 0.0370906, acc 0.98
2016-09-05T19:53:02.434485: step 3738, loss 0.0468073, acc 0.96
2016-09-05T19:53:03.229738: step 3739, loss 0.0689508, acc 0.98
2016-09-05T19:53:04.032452: step 3740, loss 0.0378964, acc 0.98
2016-09-05T19:53:04.850312: step 3741, loss 0.108686, acc 0.96
2016-09-05T19:53:05.630803: step 3742, loss 0.0570308, acc 0.98
2016-09-05T19:53:06.421961: step 3743, loss 0.0122663, acc 1
2016-09-05T19:53:07.263629: step 3744, loss 0.00449911, acc 1
2016-09-05T19:53:08.056796: step 3745, loss 0.00714434, acc 1
2016-09-05T19:53:08.854815: step 3746, loss 0.0126405, acc 1
2016-09-05T19:53:09.670742: step 3747, loss 0.0414036, acc 0.98
2016-09-05T19:53:10.458941: step 3748, loss 0.024392, acc 0.98
2016-09-05T19:53:11.279455: step 3749, loss 0.0108077, acc 1
2016-09-05T19:53:12.096871: step 3750, loss 0.0212568, acc 1
2016-09-05T19:53:12.864844: step 3751, loss 0.0674504, acc 0.98
2016-09-05T19:53:13.705411: step 3752, loss 0.0386444, acc 1
2016-09-05T19:53:14.522430: step 3753, loss 0.00932945, acc 1
2016-09-05T19:53:15.321393: step 3754, loss 0.0228601, acc 1
2016-09-05T19:53:16.106935: step 3755, loss 0.00913431, acc 1
2016-09-05T19:53:16.919308: step 3756, loss 0.0180035, acc 1
2016-09-05T19:53:17.724408: step 3757, loss 0.0193161, acc 1
2016-09-05T19:53:18.532590: step 3758, loss 0.0473185, acc 0.96
2016-09-05T19:53:19.369265: step 3759, loss 0.0230928, acc 0.98
2016-09-05T19:53:20.158185: step 3760, loss 0.0996697, acc 0.96
2016-09-05T19:53:20.977735: step 3761, loss 0.0371752, acc 0.98
2016-09-05T19:53:21.764357: step 3762, loss 0.0443228, acc 1
2016-09-05T19:53:22.532539: step 3763, loss 0.0367974, acc 0.98
2016-09-05T19:53:23.376242: step 3764, loss 0.0103934, acc 1
2016-09-05T19:53:24.216151: step 3765, loss 0.110836, acc 0.96
2016-09-05T19:53:24.977635: step 3766, loss 0.024909, acc 0.98
2016-09-05T19:53:25.790028: step 3767, loss 0.0732002, acc 0.96
2016-09-05T19:53:26.632369: step 3768, loss 0.0041756, acc 1
2016-09-05T19:53:27.403605: step 3769, loss 0.0620038, acc 0.96
2016-09-05T19:53:28.206092: step 3770, loss 0.00613696, acc 1
2016-09-05T19:53:29.020692: step 3771, loss 0.134416, acc 0.96
2016-09-05T19:53:29.818284: step 3772, loss 0.0103526, acc 1
2016-09-05T19:53:30.604057: step 3773, loss 0.00580211, acc 1
2016-09-05T19:53:31.417229: step 3774, loss 0.0074829, acc 1
2016-09-05T19:53:32.223585: step 3775, loss 0.187345, acc 0.94
2016-09-05T19:53:33.049132: step 3776, loss 0.0312409, acc 0.98
2016-09-05T19:53:33.854522: step 3777, loss 0.0549575, acc 0.98
2016-09-05T19:53:34.632383: step 3778, loss 0.0367406, acc 0.98
2016-09-05T19:53:35.423748: step 3779, loss 0.0370813, acc 0.96
2016-09-05T19:53:36.224730: step 3780, loss 0.0966498, acc 0.96
2016-09-05T19:53:37.020371: step 3781, loss 0.0414614, acc 0.96
2016-09-05T19:53:37.824896: step 3782, loss 0.0150433, acc 1
2016-09-05T19:53:38.631090: step 3783, loss 0.0976438, acc 0.92
2016-09-05T19:53:39.436539: step 3784, loss 0.0345015, acc 1
2016-09-05T19:53:40.238007: step 3785, loss 0.0212322, acc 1
2016-09-05T19:53:41.048354: step 3786, loss 0.0309758, acc 0.98
2016-09-05T19:53:41.886925: step 3787, loss 0.0538785, acc 0.98
2016-09-05T19:53:42.675011: step 3788, loss 0.0261027, acc 1
2016-09-05T19:53:43.496720: step 3789, loss 0.0291288, acc 0.98
2016-09-05T19:53:44.276627: step 3790, loss 0.0237948, acc 1
2016-09-05T19:53:45.094534: step 3791, loss 0.0264564, acc 0.98
2016-09-05T19:53:45.912083: step 3792, loss 0.0471985, acc 0.98
2016-09-05T19:53:46.712884: step 3793, loss 0.0312228, acc 0.98
2016-09-05T19:53:47.504360: step 3794, loss 0.0113337, acc 1
2016-09-05T19:53:48.290857: step 3795, loss 0.068282, acc 0.98
2016-09-05T19:53:49.085817: step 3796, loss 0.00629272, acc 1
2016-09-05T19:53:49.904850: step 3797, loss 0.0373989, acc 1
2016-09-05T19:53:50.705599: step 3798, loss 0.00926332, acc 1
2016-09-05T19:53:51.511070: step 3799, loss 0.0357164, acc 0.96
2016-09-05T19:53:52.336700: step 3800, loss 0.0233709, acc 0.98

Evaluation:
2016-09-05T19:53:55.881739: step 3800, loss 1.79026, acc 0.733

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-3800

2016-09-05T19:53:57.883705: step 3801, loss 0.0162064, acc 1
2016-09-05T19:53:58.685114: step 3802, loss 0.0898242, acc 0.96
2016-09-05T19:53:59.517841: step 3803, loss 0.0200441, acc 1
2016-09-05T19:54:00.395128: step 3804, loss 0.0606338, acc 0.96
2016-09-05T19:54:01.192800: step 3805, loss 0.00530006, acc 1
2016-09-05T19:54:02.024032: step 3806, loss 0.0520807, acc 0.98
2016-09-05T19:54:02.826191: step 3807, loss 0.00955133, acc 1
2016-09-05T19:54:03.625249: step 3808, loss 0.0984347, acc 0.96
2016-09-05T19:54:04.454398: step 3809, loss 0.0415125, acc 0.98
2016-09-05T19:54:05.285005: step 3810, loss 0.0249034, acc 0.98
2016-09-05T19:54:06.077211: step 3811, loss 0.0563684, acc 1
2016-09-05T19:54:06.891622: step 3812, loss 0.017695, acc 1
2016-09-05T19:54:07.711094: step 3813, loss 0.00508419, acc 1
2016-09-05T19:54:08.515094: step 3814, loss 0.0666157, acc 0.96
2016-09-05T19:54:09.322270: step 3815, loss 0.0247004, acc 0.98
2016-09-05T19:54:10.135473: step 3816, loss 0.0735371, acc 0.96
2016-09-05T19:54:10.926377: step 3817, loss 0.0391949, acc 0.98
2016-09-05T19:54:11.719062: step 3818, loss 0.0617477, acc 0.98
2016-09-05T19:54:12.563504: step 3819, loss 0.0125699, acc 1
2016-09-05T19:54:13.352462: step 3820, loss 0.0173637, acc 1
2016-09-05T19:54:14.158002: step 3821, loss 0.0679665, acc 0.98
2016-09-05T19:54:14.952118: step 3822, loss 0.0073755, acc 1
2016-09-05T19:54:15.739225: step 3823, loss 0.0280965, acc 0.98
2016-09-05T19:54:16.579856: step 3824, loss 0.0044183, acc 1
2016-09-05T19:54:17.401198: step 3825, loss 0.0176952, acc 1
2016-09-05T19:54:18.200470: step 3826, loss 0.062432, acc 0.96
2016-09-05T19:54:19.019577: step 3827, loss 0.0194544, acc 1
2016-09-05T19:54:19.830529: step 3828, loss 0.021663, acc 0.98
2016-09-05T19:54:20.620842: step 3829, loss 0.0240475, acc 0.98
2016-09-05T19:54:21.400298: step 3830, loss 0.0268514, acc 1
2016-09-05T19:54:22.220527: step 3831, loss 0.00384478, acc 1
2016-09-05T19:54:23.012838: step 3832, loss 0.147528, acc 0.94
2016-09-05T19:54:23.839671: step 3833, loss 0.00783389, acc 1
2016-09-05T19:54:24.670167: step 3834, loss 0.0292366, acc 0.98
2016-09-05T19:54:25.444907: step 3835, loss 0.0239515, acc 1
2016-09-05T19:54:26.270342: step 3836, loss 0.00537787, acc 1
2016-09-05T19:54:27.075888: step 3837, loss 0.0436106, acc 1
2016-09-05T19:54:27.843694: step 3838, loss 0.14261, acc 0.96
2016-09-05T19:54:28.652572: step 3839, loss 0.0181244, acc 1
2016-09-05T19:54:29.470420: step 3840, loss 0.0333735, acc 0.98
2016-09-05T19:54:30.264765: step 3841, loss 0.0331091, acc 0.98
2016-09-05T19:54:31.056155: step 3842, loss 0.0574097, acc 0.98
2016-09-05T19:54:31.867132: step 3843, loss 0.00367538, acc 1
2016-09-05T19:54:32.653415: step 3844, loss 0.0252313, acc 1
2016-09-05T19:54:33.464766: step 3845, loss 0.0282876, acc 0.98
2016-09-05T19:54:34.317286: step 3846, loss 0.0180409, acc 1
2016-09-05T19:54:35.096591: step 3847, loss 0.0334207, acc 1
2016-09-05T19:54:35.909499: step 3848, loss 0.0278981, acc 1
2016-09-05T19:54:36.751276: step 3849, loss 0.0171656, acc 1
2016-09-05T19:54:37.529404: step 3850, loss 0.00909013, acc 1
2016-09-05T19:54:38.304349: step 3851, loss 0.0331546, acc 0.98
2016-09-05T19:54:39.117293: step 3852, loss 0.0549126, acc 0.94
2016-09-05T19:54:39.896378: step 3853, loss 0.0236149, acc 0.98
2016-09-05T19:54:40.702549: step 3854, loss 0.0393055, acc 1
2016-09-05T19:54:41.490182: step 3855, loss 0.0227211, acc 1
2016-09-05T19:54:42.274765: step 3856, loss 0.12525, acc 0.94
2016-09-05T19:54:43.084097: step 3857, loss 0.0804689, acc 0.96
2016-09-05T19:54:43.894205: step 3858, loss 0.0432326, acc 0.98
2016-09-05T19:54:44.720048: step 3859, loss 0.0636209, acc 0.96
2016-09-05T19:54:45.554214: step 3860, loss 0.0890458, acc 0.98
2016-09-05T19:54:46.385450: step 3861, loss 0.0894715, acc 0.96
2016-09-05T19:54:47.168885: step 3862, loss 0.0815242, acc 0.98
2016-09-05T19:54:47.964776: step 3863, loss 0.0103906, acc 1
2016-09-05T19:54:48.799009: step 3864, loss 0.010154, acc 1
2016-09-05T19:54:49.591551: step 3865, loss 0.0281323, acc 0.98
2016-09-05T19:54:50.398887: step 3866, loss 0.119579, acc 0.94
2016-09-05T19:54:51.210912: step 3867, loss 0.0234005, acc 0.98
2016-09-05T19:54:52.004082: step 3868, loss 0.071947, acc 0.96
2016-09-05T19:54:52.808099: step 3869, loss 0.020246, acc 0.98
2016-09-05T19:54:53.641405: step 3870, loss 0.0204941, acc 0.98
2016-09-05T19:54:54.423953: step 3871, loss 0.0138602, acc 1
2016-09-05T19:54:55.235304: step 3872, loss 0.0144593, acc 1
2016-09-05T19:54:56.040733: step 3873, loss 0.0408302, acc 0.98
2016-09-05T19:54:56.839994: step 3874, loss 0.0202615, acc 1
2016-09-05T19:54:57.672967: step 3875, loss 0.0485905, acc 0.98
2016-09-05T19:54:58.486964: step 3876, loss 0.0283059, acc 0.98
2016-09-05T19:54:59.257747: step 3877, loss 0.0231539, acc 1
2016-09-05T19:55:00.045651: step 3878, loss 0.0240923, acc 0.98
2016-09-05T19:55:00.905968: step 3879, loss 0.0415167, acc 1
2016-09-05T19:55:01.315348: step 3880, loss 0.0866029, acc 0.916667
2016-09-05T19:55:02.130440: step 3881, loss 0.0227653, acc 1
2016-09-05T19:55:02.917770: step 3882, loss 0.0594222, acc 0.98
2016-09-05T19:55:03.717429: step 3883, loss 0.0261282, acc 1
2016-09-05T19:55:04.544543: step 3884, loss 0.1634, acc 0.94
2016-09-05T19:55:05.304496: step 3885, loss 0.0211884, acc 1
2016-09-05T19:55:06.104093: step 3886, loss 0.0177049, acc 1
2016-09-05T19:55:06.917569: step 3887, loss 0.0444132, acc 0.96
2016-09-05T19:55:07.700503: step 3888, loss 0.0674025, acc 0.96
2016-09-05T19:55:08.489014: step 3889, loss 0.0279705, acc 0.98
2016-09-05T19:55:09.298163: step 3890, loss 0.0176824, acc 1
2016-09-05T19:55:10.076195: step 3891, loss 0.00451316, acc 1
2016-09-05T19:55:10.882995: step 3892, loss 0.0567493, acc 0.98
2016-09-05T19:55:11.712836: step 3893, loss 0.0493619, acc 0.96
2016-09-05T19:55:12.488832: step 3894, loss 0.0327168, acc 0.98
2016-09-05T19:55:13.307888: step 3895, loss 0.0223499, acc 0.98
2016-09-05T19:55:14.121445: step 3896, loss 0.0130711, acc 1
2016-09-05T19:55:14.928769: step 3897, loss 0.031281, acc 0.98
2016-09-05T19:55:15.733741: step 3898, loss 0.00739698, acc 1
2016-09-05T19:55:16.561061: step 3899, loss 0.17556, acc 0.96
2016-09-05T19:55:17.386321: step 3900, loss 0.104178, acc 0.96

Evaluation:
2016-09-05T19:55:20.908191: step 3900, loss 1.7835, acc 0.732

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-3900

2016-09-05T19:55:22.801741: step 3901, loss 0.0196738, acc 1
2016-09-05T19:55:23.623123: step 3902, loss 0.0356303, acc 0.98
2016-09-05T19:55:24.455085: step 3903, loss 0.0537628, acc 0.98
2016-09-05T19:55:25.298471: step 3904, loss 0.0047058, acc 1
2016-09-05T19:55:26.118849: step 3905, loss 0.0346376, acc 0.98
2016-09-05T19:55:27.010901: step 3906, loss 0.0125194, acc 1
2016-09-05T19:55:27.856991: step 3907, loss 0.0105763, acc 1
2016-09-05T19:55:28.665412: step 3908, loss 0.0778423, acc 0.98
2016-09-05T19:55:29.452408: step 3909, loss 0.040455, acc 0.98
2016-09-05T19:55:30.267168: step 3910, loss 0.0680514, acc 0.98
2016-09-05T19:55:31.087793: step 3911, loss 0.0980256, acc 0.98
2016-09-05T19:55:31.883936: step 3912, loss 0.0357122, acc 1
2016-09-05T19:55:32.690793: step 3913, loss 0.0253062, acc 1
2016-09-05T19:55:33.501354: step 3914, loss 0.0383279, acc 0.98
2016-09-05T19:55:34.285254: step 3915, loss 0.0203974, acc 1
2016-09-05T19:55:35.085004: step 3916, loss 0.0588931, acc 0.98
2016-09-05T19:55:35.872488: step 3917, loss 0.0324294, acc 0.98
2016-09-05T19:55:36.663718: step 3918, loss 0.0370752, acc 1
2016-09-05T19:55:37.534276: step 3919, loss 0.00604459, acc 1
2016-09-05T19:55:38.388829: step 3920, loss 0.0077247, acc 1
2016-09-05T19:55:39.170411: step 3921, loss 0.0465428, acc 0.98
2016-09-05T19:55:39.949437: step 3922, loss 0.123087, acc 0.96
2016-09-05T19:55:40.767728: step 3923, loss 0.0132871, acc 1
2016-09-05T19:55:41.569694: step 3924, loss 0.0459814, acc 0.96
2016-09-05T19:55:42.370570: step 3925, loss 0.0359164, acc 1
2016-09-05T19:55:43.188107: step 3926, loss 0.0536376, acc 0.98
2016-09-05T19:55:43.974259: step 3927, loss 0.0440111, acc 0.96
2016-09-05T19:55:44.779004: step 3928, loss 0.00823007, acc 1
2016-09-05T19:55:45.590228: step 3929, loss 0.0137855, acc 1
2016-09-05T19:55:46.371877: step 3930, loss 0.086682, acc 0.96
2016-09-05T19:55:47.174872: step 3931, loss 0.0163237, acc 1
2016-09-05T19:55:47.977589: step 3932, loss 0.0491285, acc 0.98
2016-09-05T19:55:48.781400: step 3933, loss 0.00686347, acc 1
2016-09-05T19:55:49.579833: step 3934, loss 0.0272229, acc 0.98
2016-09-05T19:55:50.413344: step 3935, loss 0.0747192, acc 0.96
2016-09-05T19:55:51.205453: step 3936, loss 0.0991169, acc 0.98
2016-09-05T19:55:52.019226: step 3937, loss 0.0233824, acc 1
2016-09-05T19:55:52.856015: step 3938, loss 0.00514928, acc 1
2016-09-05T19:55:53.642999: step 3939, loss 0.0170839, acc 1
2016-09-05T19:55:54.444448: step 3940, loss 0.00560095, acc 1
2016-09-05T19:55:55.268096: step 3941, loss 0.0252339, acc 1
2016-09-05T19:55:56.077653: step 3942, loss 0.0192187, acc 1
2016-09-05T19:55:56.874559: step 3943, loss 0.0207123, acc 1
2016-09-05T19:55:57.683997: step 3944, loss 0.0919719, acc 0.96
2016-09-05T19:55:58.497036: step 3945, loss 0.0659726, acc 0.98
2016-09-05T19:55:59.302264: step 3946, loss 0.0386845, acc 0.96
2016-09-05T19:56:00.148023: step 3947, loss 0.0177811, acc 1
2016-09-05T19:56:00.978347: step 3948, loss 0.0273861, acc 0.98
2016-09-05T19:56:01.783968: step 3949, loss 0.0119465, acc 1
2016-09-05T19:56:02.567996: step 3950, loss 0.0638645, acc 0.98
2016-09-05T19:56:03.333864: step 3951, loss 0.0294313, acc 0.98
2016-09-05T19:56:04.150513: step 3952, loss 0.0398199, acc 0.98
2016-09-05T19:56:04.980021: step 3953, loss 0.0124476, acc 1
2016-09-05T19:56:05.753828: step 3954, loss 0.00641298, acc 1
2016-09-05T19:56:06.541466: step 3955, loss 0.0202511, acc 0.98
2016-09-05T19:56:07.329355: step 3956, loss 0.0588083, acc 0.94
2016-09-05T19:56:08.122983: step 3957, loss 0.0470908, acc 0.98
2016-09-05T19:56:08.938949: step 3958, loss 0.0468886, acc 0.98
2016-09-05T19:56:09.744415: step 3959, loss 0.0251433, acc 0.98
2016-09-05T19:56:10.526972: step 3960, loss 0.00645873, acc 1
2016-09-05T19:56:11.355261: step 3961, loss 0.018417, acc 1
2016-09-05T19:56:12.187412: step 3962, loss 0.00468044, acc 1
2016-09-05T19:56:12.984864: step 3963, loss 0.00456702, acc 1
2016-09-05T19:56:13.792537: step 3964, loss 0.0434856, acc 0.96
2016-09-05T19:56:14.600166: step 3965, loss 0.0291547, acc 1
2016-09-05T19:56:15.376700: step 3966, loss 0.0566717, acc 0.96
2016-09-05T19:56:16.206373: step 3967, loss 0.0174794, acc 1
2016-09-05T19:56:17.039599: step 3968, loss 0.011385, acc 1
2016-09-05T19:56:17.875953: step 3969, loss 0.0453276, acc 0.98
2016-09-05T19:56:18.677468: step 3970, loss 0.0928523, acc 0.94
2016-09-05T19:56:19.483458: step 3971, loss 0.0537532, acc 0.98
2016-09-05T19:56:20.264628: step 3972, loss 0.03756, acc 0.98
2016-09-05T19:56:21.065730: step 3973, loss 0.0138206, acc 1
2016-09-05T19:56:21.893896: step 3974, loss 0.0529592, acc 0.96
2016-09-05T19:56:22.681449: step 3975, loss 0.0474752, acc 0.98
2016-09-05T19:56:23.515597: step 3976, loss 0.016004, acc 1
2016-09-05T19:56:24.289841: step 3977, loss 0.0044776, acc 1
2016-09-05T19:56:25.069987: step 3978, loss 0.0865396, acc 0.98
2016-09-05T19:56:25.867777: step 3979, loss 0.0384112, acc 0.98
2016-09-05T19:56:26.687891: step 3980, loss 0.0459542, acc 0.98
2016-09-05T19:56:27.485843: step 3981, loss 0.016525, acc 1
2016-09-05T19:56:28.275678: step 3982, loss 0.0193744, acc 1
2016-09-05T19:56:29.093376: step 3983, loss 0.0250054, acc 0.98
2016-09-05T19:56:29.881612: step 3984, loss 0.0240037, acc 0.98
2016-09-05T19:56:30.704854: step 3985, loss 0.00911361, acc 1
2016-09-05T19:56:31.525706: step 3986, loss 0.0375287, acc 0.98
2016-09-05T19:56:32.317396: step 3987, loss 0.0297554, acc 1
2016-09-05T19:56:33.115907: step 3988, loss 0.0937666, acc 0.94
2016-09-05T19:56:33.950844: step 3989, loss 0.0319831, acc 1
2016-09-05T19:56:34.747391: step 3990, loss 0.0468977, acc 0.98
2016-09-05T19:56:35.551081: step 3991, loss 0.0972535, acc 0.94
2016-09-05T19:56:36.369723: step 3992, loss 0.00917536, acc 1
2016-09-05T19:56:37.157405: step 3993, loss 0.00733639, acc 1
2016-09-05T19:56:37.964645: step 3994, loss 0.0404341, acc 0.98
2016-09-05T19:56:38.778695: step 3995, loss 0.0104231, acc 1
2016-09-05T19:56:39.583842: step 3996, loss 0.039829, acc 0.98
2016-09-05T19:56:40.386415: step 3997, loss 0.0535747, acc 0.98
2016-09-05T19:56:41.217384: step 3998, loss 0.01181, acc 1
2016-09-05T19:56:42.005040: step 3999, loss 0.111974, acc 0.94
2016-09-05T19:56:42.789765: step 4000, loss 0.101673, acc 0.94

Evaluation:
2016-09-05T19:56:46.359573: step 4000, loss 1.87385, acc 0.737

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-4000

2016-09-05T19:56:48.160431: step 4001, loss 0.0173581, acc 1
2016-09-05T19:56:48.966900: step 4002, loss 0.105525, acc 0.92
2016-09-05T19:56:49.788448: step 4003, loss 0.0351386, acc 0.96
2016-09-05T19:56:50.571553: step 4004, loss 0.0120802, acc 1
2016-09-05T19:56:51.390331: step 4005, loss 0.0566497, acc 0.96
2016-09-05T19:56:52.199237: step 4006, loss 0.00828198, acc 1
2016-09-05T19:56:53.010962: step 4007, loss 0.0209579, acc 0.98
2016-09-05T19:56:53.842364: step 4008, loss 0.0190497, acc 1
2016-09-05T19:56:54.674165: step 4009, loss 0.0223156, acc 1
2016-09-05T19:56:55.512431: step 4010, loss 0.0197619, acc 1
2016-09-05T19:56:56.315246: step 4011, loss 0.0363625, acc 0.98
2016-09-05T19:56:57.131474: step 4012, loss 0.0684501, acc 0.98
2016-09-05T19:56:57.933060: step 4013, loss 0.00405894, acc 1
2016-09-05T19:56:58.735580: step 4014, loss 0.0709159, acc 0.98
2016-09-05T19:56:59.567390: step 4015, loss 0.0313692, acc 1
2016-09-05T19:57:00.422849: step 4016, loss 0.0672321, acc 0.96
2016-09-05T19:57:01.239815: step 4017, loss 0.00699725, acc 1
2016-09-05T19:57:02.083847: step 4018, loss 0.00350268, acc 1
2016-09-05T19:57:02.898093: step 4019, loss 0.0771019, acc 0.98
2016-09-05T19:57:03.713732: step 4020, loss 0.0367786, acc 0.98
2016-09-05T19:57:04.555923: step 4021, loss 0.0539849, acc 0.98
2016-09-05T19:57:05.345366: step 4022, loss 0.0674469, acc 0.98
2016-09-05T19:57:06.146809: step 4023, loss 0.024015, acc 1
2016-09-05T19:57:06.966882: step 4024, loss 0.0207539, acc 0.98
2016-09-05T19:57:07.779760: step 4025, loss 0.0223579, acc 0.98
2016-09-05T19:57:08.584422: step 4026, loss 0.0151784, acc 1
2016-09-05T19:57:09.417330: step 4027, loss 0.034889, acc 1
2016-09-05T19:57:10.209403: step 4028, loss 0.0312877, acc 1
2016-09-05T19:57:10.995857: step 4029, loss 0.0396407, acc 0.98
2016-09-05T19:57:11.807161: step 4030, loss 0.00893451, acc 1
2016-09-05T19:57:12.610859: step 4031, loss 0.0308235, acc 0.98
2016-09-05T19:57:13.384790: step 4032, loss 0.0424465, acc 0.98
2016-09-05T19:57:14.237451: step 4033, loss 0.143184, acc 0.96
2016-09-05T19:57:15.077260: step 4034, loss 0.0300083, acc 1
2016-09-05T19:57:15.859697: step 4035, loss 0.0354488, acc 0.98
2016-09-05T19:57:16.671952: step 4036, loss 0.0167176, acc 1
2016-09-05T19:57:17.483690: step 4037, loss 0.114939, acc 0.94
2016-09-05T19:57:18.269021: step 4038, loss 0.0341289, acc 0.98
2016-09-05T19:57:19.098219: step 4039, loss 0.0239917, acc 1
2016-09-05T19:57:19.908489: step 4040, loss 0.0227391, acc 0.98
2016-09-05T19:57:20.693390: step 4041, loss 0.0129259, acc 1
2016-09-05T19:57:21.497615: step 4042, loss 0.0776898, acc 0.94
2016-09-05T19:57:22.308548: step 4043, loss 0.0132436, acc 1
2016-09-05T19:57:23.102211: step 4044, loss 0.0169511, acc 1
2016-09-05T19:57:23.903100: step 4045, loss 0.0199696, acc 1
2016-09-05T19:57:24.719682: step 4046, loss 0.0323887, acc 0.98
2016-09-05T19:57:25.507645: step 4047, loss 0.0152516, acc 1
2016-09-05T19:57:26.343347: step 4048, loss 0.0182356, acc 0.98
2016-09-05T19:57:27.179843: step 4049, loss 0.0488262, acc 0.98
2016-09-05T19:57:27.986799: step 4050, loss 0.036931, acc 1
2016-09-05T19:57:28.794884: step 4051, loss 0.0299292, acc 0.98
2016-09-05T19:57:29.626763: step 4052, loss 0.0371256, acc 0.98
2016-09-05T19:57:30.412644: step 4053, loss 0.0576446, acc 0.98
2016-09-05T19:57:31.216423: step 4054, loss 0.00408428, acc 1
2016-09-05T19:57:32.053730: step 4055, loss 0.00790947, acc 1
2016-09-05T19:57:32.820527: step 4056, loss 0.0546551, acc 0.96
2016-09-05T19:57:33.621997: step 4057, loss 0.00586382, acc 1
2016-09-05T19:57:34.445448: step 4058, loss 0.0508144, acc 0.96
2016-09-05T19:57:35.249443: step 4059, loss 0.0187252, acc 1
2016-09-05T19:57:36.039000: step 4060, loss 0.0217068, acc 0.98
2016-09-05T19:57:36.851025: step 4061, loss 0.0589587, acc 0.96
2016-09-05T19:57:37.662189: step 4062, loss 0.0449265, acc 0.98
2016-09-05T19:57:38.482585: step 4063, loss 0.0402132, acc 0.98
2016-09-05T19:57:39.280560: step 4064, loss 0.00376656, acc 1
2016-09-05T19:57:40.068435: step 4065, loss 0.0113939, acc 1
2016-09-05T19:57:40.850536: step 4066, loss 0.00478247, acc 1
2016-09-05T19:57:41.667815: step 4067, loss 0.0392174, acc 0.98
2016-09-05T19:57:42.452810: step 4068, loss 0.015683, acc 1
2016-09-05T19:57:43.280393: step 4069, loss 0.00764293, acc 1
2016-09-05T19:57:44.086612: step 4070, loss 0.062161, acc 0.96
2016-09-05T19:57:44.923206: step 4071, loss 0.0607637, acc 0.96
2016-09-05T19:57:45.729540: step 4072, loss 0.0383172, acc 0.98
2016-09-05T19:57:46.558456: step 4073, loss 0.0398373, acc 0.98
2016-09-05T19:57:46.957119: step 4074, loss 0.0270531, acc 1
2016-09-05T19:57:47.788894: step 4075, loss 0.0289044, acc 1
2016-09-05T19:57:48.594308: step 4076, loss 0.0495004, acc 0.96
2016-09-05T19:57:49.387973: step 4077, loss 0.0231806, acc 0.98
2016-09-05T19:57:50.227859: step 4078, loss 0.0077859, acc 1
2016-09-05T19:57:51.034297: step 4079, loss 0.0617735, acc 0.98
2016-09-05T19:57:51.847530: step 4080, loss 0.0795632, acc 0.96
2016-09-05T19:57:52.684543: step 4081, loss 0.00511868, acc 1
2016-09-05T19:57:53.465669: step 4082, loss 0.0201069, acc 0.98
2016-09-05T19:57:54.283185: step 4083, loss 0.0135361, acc 1
2016-09-05T19:57:55.116437: step 4084, loss 0.0313534, acc 0.98
2016-09-05T19:57:55.949769: step 4085, loss 0.0324063, acc 0.98
2016-09-05T19:57:56.774948: step 4086, loss 0.0354351, acc 0.98
2016-09-05T19:57:57.616872: step 4087, loss 0.0298025, acc 1
2016-09-05T19:57:58.397407: step 4088, loss 0.0268938, acc 0.98
2016-09-05T19:57:59.192676: step 4089, loss 0.0331705, acc 0.98
2016-09-05T19:58:00.021493: step 4090, loss 0.00522651, acc 1
2016-09-05T19:58:00.846953: step 4091, loss 0.0225886, acc 1
2016-09-05T19:58:01.656390: step 4092, loss 0.0256351, acc 0.98
2016-09-05T19:58:02.485823: step 4093, loss 0.0138186, acc 1
2016-09-05T19:58:03.283143: step 4094, loss 0.0200598, acc 0.98
2016-09-05T19:58:04.096930: step 4095, loss 0.0368426, acc 0.98
2016-09-05T19:58:04.911280: step 4096, loss 0.0161787, acc 1
2016-09-05T19:58:05.725806: step 4097, loss 0.0686733, acc 0.98
2016-09-05T19:58:06.532981: step 4098, loss 0.0100504, acc 1
2016-09-05T19:58:07.384087: step 4099, loss 0.0202818, acc 1
2016-09-05T19:58:08.173405: step 4100, loss 0.01617, acc 1

Evaluation:
2016-09-05T19:58:11.690246: step 4100, loss 2.07706, acc 0.744

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-4100

2016-09-05T19:58:13.563377: step 4101, loss 0.0198998, acc 1
2016-09-05T19:58:14.409465: step 4102, loss 0.00410425, acc 1
2016-09-05T19:58:15.222945: step 4103, loss 0.0223534, acc 0.98
2016-09-05T19:58:16.065995: step 4104, loss 0.022717, acc 0.98
2016-09-05T19:58:16.896012: step 4105, loss 0.024167, acc 0.98
2016-09-05T19:58:17.710874: step 4106, loss 0.137244, acc 0.96
2016-09-05T19:58:18.524224: step 4107, loss 0.0141584, acc 1
2016-09-05T19:58:19.352534: step 4108, loss 0.00490566, acc 1
2016-09-05T19:58:20.164990: step 4109, loss 0.00736733, acc 1
2016-09-05T19:58:20.966317: step 4110, loss 0.0192357, acc 0.98
2016-09-05T19:58:21.815653: step 4111, loss 0.0581725, acc 0.98
2016-09-05T19:58:22.618074: step 4112, loss 0.00396297, acc 1
2016-09-05T19:58:23.417278: step 4113, loss 0.0288193, acc 0.98
2016-09-05T19:58:24.273404: step 4114, loss 0.0639756, acc 0.96
2016-09-05T19:58:25.075295: step 4115, loss 0.0244427, acc 1
2016-09-05T19:58:25.896445: step 4116, loss 0.0303302, acc 0.98
2016-09-05T19:58:26.743515: step 4117, loss 0.0031506, acc 1
2016-09-05T19:58:27.569374: step 4118, loss 0.0283162, acc 1
2016-09-05T19:58:28.395877: step 4119, loss 0.0239989, acc 1
2016-09-05T19:58:29.188622: step 4120, loss 0.0524599, acc 0.98
2016-09-05T19:58:30.015351: step 4121, loss 0.0166025, acc 1
2016-09-05T19:58:30.806797: step 4122, loss 0.0491428, acc 0.98
2016-09-05T19:58:31.604753: step 4123, loss 0.0370254, acc 0.98
2016-09-05T19:58:32.401457: step 4124, loss 0.0114231, acc 1
2016-09-05T19:58:33.233343: step 4125, loss 0.050912, acc 0.98
2016-09-05T19:58:34.046507: step 4126, loss 0.0110157, acc 1
2016-09-05T19:58:34.893524: step 4127, loss 0.00668722, acc 1
2016-09-05T19:58:35.676501: step 4128, loss 0.0162501, acc 1
2016-09-05T19:58:36.485853: step 4129, loss 0.0508352, acc 0.98
2016-09-05T19:58:37.304970: step 4130, loss 0.0851408, acc 0.96
2016-09-05T19:58:38.089560: step 4131, loss 0.0566918, acc 0.96
2016-09-05T19:58:38.890550: step 4132, loss 0.00559749, acc 1
2016-09-05T19:58:39.696566: step 4133, loss 0.0115732, acc 1
2016-09-05T19:58:40.478938: step 4134, loss 0.0243751, acc 0.98
2016-09-05T19:58:41.272028: step 4135, loss 0.0347872, acc 0.98
2016-09-05T19:58:42.076409: step 4136, loss 0.0302476, acc 0.98
2016-09-05T19:58:42.868871: step 4137, loss 0.042345, acc 0.96
2016-09-05T19:58:43.726764: step 4138, loss 0.0182701, acc 1
2016-09-05T19:58:44.546254: step 4139, loss 0.0393045, acc 0.98
2016-09-05T19:58:45.349016: step 4140, loss 0.00821858, acc 1
2016-09-05T19:58:46.177596: step 4141, loss 0.0256387, acc 0.98
2016-09-05T19:58:46.989546: step 4142, loss 0.0142502, acc 1
2016-09-05T19:58:47.788406: step 4143, loss 0.0192177, acc 1
2016-09-05T19:58:48.581613: step 4144, loss 0.0105923, acc 1
2016-09-05T19:58:49.361135: step 4145, loss 0.00630479, acc 1
2016-09-05T19:58:50.181388: step 4146, loss 0.028935, acc 0.98
2016-09-05T19:58:50.971979: step 4147, loss 0.0149026, acc 1
2016-09-05T19:58:51.807528: step 4148, loss 0.0121111, acc 1
2016-09-05T19:58:52.567794: step 4149, loss 0.0157982, acc 1
2016-09-05T19:58:53.371821: step 4150, loss 0.033476, acc 0.98
2016-09-05T19:58:54.230260: step 4151, loss 0.0331816, acc 1
2016-09-05T19:58:55.042142: step 4152, loss 0.0082929, acc 1
2016-09-05T19:58:55.822863: step 4153, loss 0.0365877, acc 0.96
2016-09-05T19:58:56.648633: step 4154, loss 0.0397291, acc 0.98
2016-09-05T19:58:57.450729: step 4155, loss 0.071656, acc 0.96
2016-09-05T19:58:58.241530: step 4156, loss 0.0188136, acc 0.98
2016-09-05T19:58:59.058115: step 4157, loss 0.0220237, acc 1
2016-09-05T19:58:59.833677: step 4158, loss 0.00649689, acc 1
2016-09-05T19:59:00.674588: step 4159, loss 0.00553856, acc 1
2016-09-05T19:59:01.480809: step 4160, loss 0.00716343, acc 1
2016-09-05T19:59:02.265424: step 4161, loss 0.0481355, acc 0.98
2016-09-05T19:59:03.051632: step 4162, loss 0.0277066, acc 0.98
2016-09-05T19:59:03.859490: step 4163, loss 0.00553204, acc 1
2016-09-05T19:59:04.651918: step 4164, loss 0.0768613, acc 0.96
2016-09-05T19:59:05.440139: step 4165, loss 0.0662038, acc 0.98
2016-09-05T19:59:06.264462: step 4166, loss 0.0445763, acc 0.98
2016-09-05T19:59:07.074463: step 4167, loss 0.0513544, acc 0.98
2016-09-05T19:59:07.857625: step 4168, loss 0.00536473, acc 1
2016-09-05T19:59:08.684951: step 4169, loss 0.0588106, acc 0.98
2016-09-05T19:59:09.508037: step 4170, loss 0.00717277, acc 1
2016-09-05T19:59:10.308148: step 4171, loss 0.0050048, acc 1
2016-09-05T19:59:11.125434: step 4172, loss 0.037359, acc 0.98
2016-09-05T19:59:11.899869: step 4173, loss 0.0485286, acc 0.98
2016-09-05T19:59:12.694140: step 4174, loss 0.00578137, acc 1
2016-09-05T19:59:13.519271: step 4175, loss 0.00704932, acc 1
2016-09-05T19:59:14.309180: step 4176, loss 0.0494896, acc 0.98
2016-09-05T19:59:15.095825: step 4177, loss 0.0241307, acc 1
2016-09-05T19:59:15.912955: step 4178, loss 0.0414957, acc 1
2016-09-05T19:59:16.698423: step 4179, loss 0.0222323, acc 1
2016-09-05T19:59:17.496102: step 4180, loss 0.0278896, acc 0.98
2016-09-05T19:59:18.296246: step 4181, loss 0.0422199, acc 0.98
2016-09-05T19:59:19.090945: step 4182, loss 0.0286414, acc 0.98
2016-09-05T19:59:19.915681: step 4183, loss 0.0172248, acc 1
2016-09-05T19:59:20.730875: step 4184, loss 0.00530266, acc 1
2016-09-05T19:59:21.540635: step 4185, loss 0.00867617, acc 1
2016-09-05T19:59:22.340541: step 4186, loss 0.00666563, acc 1
2016-09-05T19:59:23.189731: step 4187, loss 0.0383287, acc 1
2016-09-05T19:59:23.990193: step 4188, loss 0.0153843, acc 1
2016-09-05T19:59:24.800778: step 4189, loss 0.0295865, acc 0.98
2016-09-05T19:59:25.645442: step 4190, loss 0.0447888, acc 0.98
2016-09-05T19:59:26.437528: step 4191, loss 0.0656555, acc 0.96
2016-09-05T19:59:27.267110: step 4192, loss 0.0173945, acc 1
2016-09-05T19:59:28.080708: step 4193, loss 0.0170417, acc 1
2016-09-05T19:59:28.872242: step 4194, loss 0.038036, acc 1
2016-09-05T19:59:29.653237: step 4195, loss 0.0194403, acc 0.98
2016-09-05T19:59:30.482866: step 4196, loss 0.00981446, acc 1
2016-09-05T19:59:31.275105: step 4197, loss 0.0247538, acc 0.98
2016-09-05T19:59:32.067429: step 4198, loss 0.0276192, acc 1
2016-09-05T19:59:32.873814: step 4199, loss 0.0350663, acc 0.98
2016-09-05T19:59:33.659838: step 4200, loss 0.0669157, acc 0.96

Evaluation:
2016-09-05T19:59:37.168088: step 4200, loss 1.91452, acc 0.747

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-4200

2016-09-05T19:59:39.191720: step 4201, loss 0.0678384, acc 0.98
2016-09-05T19:59:40.001978: step 4202, loss 0.00740589, acc 1
2016-09-05T19:59:40.845641: step 4203, loss 0.0294604, acc 0.98
2016-09-05T19:59:41.689721: step 4204, loss 0.105164, acc 0.98
2016-09-05T19:59:42.496452: step 4205, loss 0.0632039, acc 0.96
2016-09-05T19:59:43.338379: step 4206, loss 0.023061, acc 0.98
2016-09-05T19:59:44.143291: step 4207, loss 0.0140283, acc 1
2016-09-05T19:59:44.949199: step 4208, loss 0.0315373, acc 0.98
2016-09-05T19:59:45.753730: step 4209, loss 0.0816531, acc 0.96
2016-09-05T19:59:46.572137: step 4210, loss 0.112714, acc 0.94
2016-09-05T19:59:47.411278: step 4211, loss 0.0263807, acc 1
2016-09-05T19:59:48.194740: step 4212, loss 0.00420647, acc 1
2016-09-05T19:59:49.018212: step 4213, loss 0.0130973, acc 1
2016-09-05T19:59:49.850767: step 4214, loss 0.0231555, acc 1
2016-09-05T19:59:50.648494: step 4215, loss 0.00373685, acc 1
2016-09-05T19:59:51.449219: step 4216, loss 0.0305106, acc 1
2016-09-05T19:59:52.250206: step 4217, loss 0.0577066, acc 0.98
2016-09-05T19:59:53.037404: step 4218, loss 0.0369871, acc 0.98
2016-09-05T19:59:53.841729: step 4219, loss 0.00996017, acc 1
2016-09-05T19:59:54.642780: step 4220, loss 0.00454455, acc 1
2016-09-05T19:59:55.418271: step 4221, loss 0.0356934, acc 1
2016-09-05T19:59:56.226747: step 4222, loss 0.0209804, acc 1
2016-09-05T19:59:57.046310: step 4223, loss 0.111214, acc 0.96
2016-09-05T19:59:57.829520: step 4224, loss 0.0496851, acc 0.96
2016-09-05T19:59:58.625714: step 4225, loss 0.115008, acc 0.96
2016-09-05T19:59:59.443803: step 4226, loss 0.0653616, acc 0.94
2016-09-05T20:00:00.244436: step 4227, loss 0.024461, acc 1
2016-09-05T20:00:01.039729: step 4228, loss 0.069566, acc 0.96
2016-09-05T20:00:01.828977: step 4229, loss 0.0268203, acc 0.98
2016-09-05T20:00:02.651086: step 4230, loss 0.00589996, acc 1
2016-09-05T20:00:03.449336: step 4231, loss 0.00594836, acc 1
2016-09-05T20:00:04.247027: step 4232, loss 0.0418732, acc 0.98
2016-09-05T20:00:05.058274: step 4233, loss 0.00681408, acc 1
2016-09-05T20:00:05.874951: step 4234, loss 0.00981191, acc 1
2016-09-05T20:00:06.676573: step 4235, loss 0.0262707, acc 1
2016-09-05T20:00:07.467312: step 4236, loss 0.0547995, acc 0.96
2016-09-05T20:00:08.283288: step 4237, loss 0.0179498, acc 1
2016-09-05T20:00:09.092013: step 4238, loss 0.00988631, acc 1
2016-09-05T20:00:09.913429: step 4239, loss 0.0128214, acc 1
2016-09-05T20:00:10.716557: step 4240, loss 0.0336103, acc 0.98
2016-09-05T20:00:11.533987: step 4241, loss 0.0162138, acc 1
2016-09-05T20:00:12.327466: step 4242, loss 0.0278769, acc 0.98
2016-09-05T20:00:13.117754: step 4243, loss 0.0442613, acc 0.98
2016-09-05T20:00:13.937427: step 4244, loss 0.248763, acc 0.98
2016-09-05T20:00:14.714337: step 4245, loss 0.0533683, acc 0.96
2016-09-05T20:00:15.502665: step 4246, loss 0.0243962, acc 1
2016-09-05T20:00:16.293394: step 4247, loss 0.00591597, acc 1
2016-09-05T20:00:17.092200: step 4248, loss 0.0170443, acc 1
2016-09-05T20:00:17.919704: step 4249, loss 0.0367482, acc 0.98
2016-09-05T20:00:18.735964: step 4250, loss 0.0477285, acc 0.98
2016-09-05T20:00:19.545470: step 4251, loss 0.0609001, acc 0.98
2016-09-05T20:00:20.365202: step 4252, loss 0.00469763, acc 1
2016-09-05T20:00:21.191942: step 4253, loss 0.00360794, acc 1
2016-09-05T20:00:22.003798: step 4254, loss 0.0185971, acc 0.98
2016-09-05T20:00:22.799693: step 4255, loss 0.022522, acc 1
2016-09-05T20:00:23.612880: step 4256, loss 0.00545583, acc 1
2016-09-05T20:00:24.398293: step 4257, loss 0.0332747, acc 0.98
2016-09-05T20:00:25.235689: step 4258, loss 0.0366159, acc 0.98
2016-09-05T20:00:26.038787: step 4259, loss 0.0233643, acc 0.98
2016-09-05T20:00:26.858487: step 4260, loss 0.0229368, acc 1
2016-09-05T20:00:27.666494: step 4261, loss 0.0260701, acc 0.98
2016-09-05T20:00:28.505543: step 4262, loss 0.00329187, acc 1
2016-09-05T20:00:29.283769: step 4263, loss 0.0198055, acc 0.98
2016-09-05T20:00:30.088284: step 4264, loss 0.0194402, acc 1
2016-09-05T20:00:30.935100: step 4265, loss 0.0608124, acc 0.98
2016-09-05T20:00:31.735980: step 4266, loss 0.00711462, acc 1
2016-09-05T20:00:32.532400: step 4267, loss 0.0034062, acc 1
2016-09-05T20:00:32.967562: step 4268, loss 0.0174823, acc 1
2016-09-05T20:00:33.784091: step 4269, loss 0.0224104, acc 1
2016-09-05T20:00:34.610250: step 4270, loss 0.00417899, acc 1
2016-09-05T20:00:35.433478: step 4271, loss 0.0701784, acc 0.96
2016-09-05T20:00:36.276738: step 4272, loss 0.0132195, acc 1
2016-09-05T20:00:37.100206: step 4273, loss 0.015622, acc 1
2016-09-05T20:00:37.881263: step 4274, loss 0.0730044, acc 0.96
2016-09-05T20:00:38.701850: step 4275, loss 0.0180528, acc 0.98
2016-09-05T20:00:39.526042: step 4276, loss 0.0471497, acc 0.98
2016-09-05T20:00:40.329141: step 4277, loss 0.117318, acc 0.98
2016-09-05T20:00:41.118708: step 4278, loss 0.0472104, acc 0.96
2016-09-05T20:00:41.960698: step 4279, loss 0.0330882, acc 0.98
2016-09-05T20:00:42.768323: step 4280, loss 0.0298672, acc 0.98
2016-09-05T20:00:43.559418: step 4281, loss 0.0353966, acc 0.98
2016-09-05T20:00:44.388181: step 4282, loss 0.0588918, acc 0.98
2016-09-05T20:00:45.192868: step 4283, loss 0.00698534, acc 1
2016-09-05T20:00:46.010458: step 4284, loss 0.0108404, acc 1
2016-09-05T20:00:46.852442: step 4285, loss 0.0179826, acc 1
2016-09-05T20:00:47.632414: step 4286, loss 0.0122691, acc 1
2016-09-05T20:00:48.420338: step 4287, loss 0.0471906, acc 0.98
2016-09-05T20:00:49.249428: step 4288, loss 0.0331166, acc 1
2016-09-05T20:00:50.040415: step 4289, loss 0.0455985, acc 0.98
2016-09-05T20:00:50.858181: step 4290, loss 0.0302033, acc 0.98
2016-09-05T20:00:51.704223: step 4291, loss 0.0246433, acc 1
2016-09-05T20:00:52.514805: step 4292, loss 0.0148466, acc 1
2016-09-05T20:00:53.300393: step 4293, loss 0.0461093, acc 0.98
2016-09-05T20:00:54.110806: step 4294, loss 0.0174599, acc 1
2016-09-05T20:00:54.932257: step 4295, loss 0.128184, acc 0.96
2016-09-05T20:00:55.748789: step 4296, loss 0.00708042, acc 1
2016-09-05T20:00:56.587864: step 4297, loss 0.0147698, acc 1
2016-09-05T20:00:57.423817: step 4298, loss 0.00440698, acc 1
2016-09-05T20:00:58.209363: step 4299, loss 0.0363828, acc 0.98
2016-09-05T20:00:59.041212: step 4300, loss 0.0709261, acc 0.96

Evaluation:
2016-09-05T20:01:02.576225: step 4300, loss 2.09673, acc 0.742

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-4300

2016-09-05T20:01:04.451438: step 4301, loss 0.0201913, acc 1
2016-09-05T20:01:05.274713: step 4302, loss 0.0378346, acc 1
2016-09-05T20:01:06.073650: step 4303, loss 0.00773237, acc 1
2016-09-05T20:01:06.867984: step 4304, loss 0.127551, acc 0.94
2016-09-05T20:01:07.686576: step 4305, loss 0.0123947, acc 1
2016-09-05T20:01:08.518238: step 4306, loss 0.0701535, acc 0.94
2016-09-05T20:01:09.309413: step 4307, loss 0.00683493, acc 1
2016-09-05T20:01:10.117490: step 4308, loss 0.0129461, acc 1
2016-09-05T20:01:10.901592: step 4309, loss 0.0419853, acc 0.98
2016-09-05T20:01:11.692096: step 4310, loss 0.0134259, acc 1
2016-09-05T20:01:12.485269: step 4311, loss 0.0742721, acc 0.98
2016-09-05T20:01:13.308894: step 4312, loss 0.0310315, acc 0.98
2016-09-05T20:01:14.104408: step 4313, loss 0.0100241, acc 1
2016-09-05T20:01:14.903169: step 4314, loss 0.102195, acc 0.94
2016-09-05T20:01:15.714370: step 4315, loss 0.0178678, acc 1
2016-09-05T20:01:16.503424: step 4316, loss 0.0215258, acc 1
2016-09-05T20:01:17.298256: step 4317, loss 0.0039933, acc 1
2016-09-05T20:01:18.097611: step 4318, loss 0.0204015, acc 1
2016-09-05T20:01:18.884913: step 4319, loss 0.027453, acc 0.98
2016-09-05T20:01:19.728056: step 4320, loss 0.0288707, acc 0.98
2016-09-05T20:01:20.523951: step 4321, loss 0.0280151, acc 1
2016-09-05T20:01:21.328243: step 4322, loss 0.00467868, acc 1
2016-09-05T20:01:22.155507: step 4323, loss 0.0347662, acc 0.98
2016-09-05T20:01:22.973574: step 4324, loss 0.0315116, acc 0.98
2016-09-05T20:01:23.771210: step 4325, loss 0.0115576, acc 1
2016-09-05T20:01:24.564075: step 4326, loss 0.0108753, acc 1
2016-09-05T20:01:25.363333: step 4327, loss 0.0216188, acc 1
2016-09-05T20:01:26.163636: step 4328, loss 0.0343874, acc 0.98
2016-09-05T20:01:27.029391: step 4329, loss 0.112599, acc 0.98
2016-09-05T20:01:27.816206: step 4330, loss 0.0661976, acc 0.96
2016-09-05T20:01:28.621384: step 4331, loss 0.048271, acc 0.98
2016-09-05T20:01:29.428584: step 4332, loss 0.0395965, acc 0.98
2016-09-05T20:01:30.256581: step 4333, loss 0.0405811, acc 0.98
2016-09-05T20:01:31.035645: step 4334, loss 0.111437, acc 0.96
2016-09-05T20:01:31.857819: step 4335, loss 0.0202626, acc 1
2016-09-05T20:01:32.675118: step 4336, loss 0.0331729, acc 1
2016-09-05T20:01:33.444012: step 4337, loss 0.00440302, acc 1
2016-09-05T20:01:34.230611: step 4338, loss 0.0557967, acc 0.96
2016-09-05T20:01:35.033233: step 4339, loss 0.0100066, acc 1
2016-09-05T20:01:35.843047: step 4340, loss 0.0297402, acc 1
2016-09-05T20:01:36.630172: step 4341, loss 0.005334, acc 1
2016-09-05T20:01:37.427586: step 4342, loss 0.0373059, acc 0.98
2016-09-05T20:01:38.216177: step 4343, loss 0.0934441, acc 0.98
2016-09-05T20:01:39.033782: step 4344, loss 0.0402304, acc 0.96
2016-09-05T20:01:39.849905: step 4345, loss 0.0192857, acc 1
2016-09-05T20:01:40.679420: step 4346, loss 0.0855471, acc 0.96
2016-09-05T20:01:41.479969: step 4347, loss 0.0200775, acc 1
2016-09-05T20:01:42.313811: step 4348, loss 0.0576615, acc 0.98
2016-09-05T20:01:43.092468: step 4349, loss 0.0256484, acc 0.98
2016-09-05T20:01:43.904663: step 4350, loss 0.0340454, acc 0.98
2016-09-05T20:01:44.728037: step 4351, loss 0.0756824, acc 0.96
2016-09-05T20:01:45.521341: step 4352, loss 0.00843928, acc 1
2016-09-05T20:01:46.335945: step 4353, loss 0.00494352, acc 1
2016-09-05T20:01:47.184005: step 4354, loss 0.0253587, acc 0.98
2016-09-05T20:01:47.981428: step 4355, loss 0.0261196, acc 0.98
2016-09-05T20:01:48.786084: step 4356, loss 0.0856614, acc 0.96
2016-09-05T20:01:49.603936: step 4357, loss 0.0250134, acc 1
2016-09-05T20:01:50.403586: step 4358, loss 0.0566344, acc 0.96
2016-09-05T20:01:51.207321: step 4359, loss 0.0893052, acc 0.98
2016-09-05T20:01:52.027045: step 4360, loss 0.00576236, acc 1
2016-09-05T20:01:52.841968: step 4361, loss 0.0254191, acc 1
2016-09-05T20:01:53.644956: step 4362, loss 0.0279055, acc 0.98
2016-09-05T20:01:54.488649: step 4363, loss 0.0265811, acc 1
2016-09-05T20:01:55.302180: step 4364, loss 0.0323823, acc 0.98
2016-09-05T20:01:56.110303: step 4365, loss 0.00462171, acc 1
2016-09-05T20:01:56.983122: step 4366, loss 0.0299737, acc 1
2016-09-05T20:01:57.855540: step 4367, loss 0.0446465, acc 0.98
2016-09-05T20:01:58.670499: step 4368, loss 0.0261732, acc 1
2016-09-05T20:01:59.512726: step 4369, loss 0.0084947, acc 1
2016-09-05T20:02:00.347126: step 4370, loss 0.0370163, acc 0.96
2016-09-05T20:02:01.142580: step 4371, loss 0.0516148, acc 0.98
2016-09-05T20:02:01.968996: step 4372, loss 0.15639, acc 0.94
2016-09-05T20:02:02.789420: step 4373, loss 0.00471672, acc 1
2016-09-05T20:02:03.583503: step 4374, loss 0.0100649, acc 1
2016-09-05T20:02:04.403342: step 4375, loss 0.0305872, acc 1
2016-09-05T20:02:05.247834: step 4376, loss 0.046864, acc 0.98
2016-09-05T20:02:06.039675: step 4377, loss 0.0723519, acc 0.94
2016-09-05T20:02:06.847487: step 4378, loss 0.0106341, acc 1
2016-09-05T20:02:07.668962: step 4379, loss 0.0389926, acc 0.98
2016-09-05T20:02:08.442925: step 4380, loss 0.00581811, acc 1
2016-09-05T20:02:09.265061: step 4381, loss 0.00679437, acc 1
2016-09-05T20:02:10.083958: step 4382, loss 0.0368056, acc 0.96
2016-09-05T20:02:10.884981: step 4383, loss 0.0149259, acc 1
2016-09-05T20:02:11.689532: step 4384, loss 0.0602548, acc 0.96
2016-09-05T20:02:12.513657: step 4385, loss 0.0429246, acc 0.96
2016-09-05T20:02:13.323261: step 4386, loss 0.098255, acc 0.96
2016-09-05T20:02:14.146234: step 4387, loss 0.0146125, acc 1
2016-09-05T20:02:14.986369: step 4388, loss 0.0511326, acc 0.98
2016-09-05T20:02:15.756825: step 4389, loss 0.0223562, acc 0.98
2016-09-05T20:02:16.542148: step 4390, loss 0.0160165, acc 1
2016-09-05T20:02:17.357749: step 4391, loss 0.0448186, acc 0.98
2016-09-05T20:02:18.155626: step 4392, loss 0.0494336, acc 0.96
2016-09-05T20:02:18.992721: step 4393, loss 0.0267261, acc 1
2016-09-05T20:02:19.832290: step 4394, loss 0.0377354, acc 1
2016-09-05T20:02:20.621967: step 4395, loss 0.0285743, acc 1
2016-09-05T20:02:21.403285: step 4396, loss 0.0438043, acc 0.98
2016-09-05T20:02:22.194451: step 4397, loss 0.00668744, acc 1
2016-09-05T20:02:22.988887: step 4398, loss 0.0415968, acc 0.98
2016-09-05T20:02:23.784403: step 4399, loss 0.0462915, acc 1
2016-09-05T20:02:24.588338: step 4400, loss 0.126806, acc 0.98

Evaluation:
2016-09-05T20:02:28.072090: step 4400, loss 1.76098, acc 0.743

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-4400

2016-09-05T20:02:29.989417: step 4401, loss 0.0362542, acc 1
2016-09-05T20:02:30.841481: step 4402, loss 0.0315815, acc 1
2016-09-05T20:02:31.687922: step 4403, loss 0.0609591, acc 0.98
2016-09-05T20:02:32.495182: step 4404, loss 0.0107193, acc 1
2016-09-05T20:02:33.304314: step 4405, loss 0.0290613, acc 0.98
2016-09-05T20:02:34.153002: step 4406, loss 0.012699, acc 1
2016-09-05T20:02:34.956324: step 4407, loss 0.00613046, acc 1
2016-09-05T20:02:35.766066: step 4408, loss 0.0400089, acc 0.98
2016-09-05T20:02:36.576670: step 4409, loss 0.0213742, acc 1
2016-09-05T20:02:37.377926: step 4410, loss 0.0205999, acc 0.98
2016-09-05T20:02:38.217566: step 4411, loss 0.0197391, acc 1
2016-09-05T20:02:39.023693: step 4412, loss 0.00372699, acc 1
2016-09-05T20:02:39.833565: step 4413, loss 0.0164684, acc 1
2016-09-05T20:02:40.645979: step 4414, loss 0.00386283, acc 1
2016-09-05T20:02:41.454549: step 4415, loss 0.0042372, acc 1
2016-09-05T20:02:42.247366: step 4416, loss 0.0601546, acc 0.96
2016-09-05T20:02:43.062808: step 4417, loss 0.0114286, acc 1
2016-09-05T20:02:43.887029: step 4418, loss 0.00963602, acc 1
2016-09-05T20:02:44.699287: step 4419, loss 0.0226598, acc 1
2016-09-05T20:02:45.544663: step 4420, loss 0.0782318, acc 0.96
2016-09-05T20:02:46.355197: step 4421, loss 0.0219556, acc 0.98
2016-09-05T20:02:47.164277: step 4422, loss 0.0317793, acc 1
2016-09-05T20:02:47.961770: step 4423, loss 0.0104126, acc 1
2016-09-05T20:02:48.736706: step 4424, loss 0.012741, acc 1
2016-09-05T20:02:49.562290: step 4425, loss 0.00381661, acc 1
2016-09-05T20:02:50.388601: step 4426, loss 0.0565018, acc 0.96
2016-09-05T20:02:51.219104: step 4427, loss 0.0384405, acc 0.98
2016-09-05T20:02:51.997389: step 4428, loss 0.0473804, acc 0.96
2016-09-05T20:02:52.808925: step 4429, loss 0.0631209, acc 0.96
2016-09-05T20:02:53.624030: step 4430, loss 0.0368001, acc 0.98
2016-09-05T20:02:54.435885: step 4431, loss 0.00477563, acc 1
2016-09-05T20:02:55.234150: step 4432, loss 0.113823, acc 0.96
2016-09-05T20:02:56.037040: step 4433, loss 0.00577602, acc 1
2016-09-05T20:02:56.847730: step 4434, loss 0.0285789, acc 0.98
2016-09-05T20:02:57.661448: step 4435, loss 0.0734869, acc 0.98
2016-09-05T20:02:58.486682: step 4436, loss 0.0238645, acc 0.98
2016-09-05T20:02:59.313398: step 4437, loss 0.0125283, acc 1
2016-09-05T20:03:00.099294: step 4438, loss 0.0780865, acc 0.96
2016-09-05T20:03:00.974329: step 4439, loss 0.0276818, acc 0.98
2016-09-05T20:03:01.780183: step 4440, loss 0.00414856, acc 1
2016-09-05T20:03:02.603876: step 4441, loss 0.0381248, acc 0.98
2016-09-05T20:03:03.417435: step 4442, loss 0.00272994, acc 1
2016-09-05T20:03:04.228507: step 4443, loss 0.0299791, acc 0.98
2016-09-05T20:03:05.030726: step 4444, loss 0.0228253, acc 0.98
2016-09-05T20:03:05.869470: step 4445, loss 0.0215161, acc 1
2016-09-05T20:03:06.673439: step 4446, loss 0.00909393, acc 1
2016-09-05T20:03:07.497630: step 4447, loss 0.0299359, acc 1
2016-09-05T20:03:08.314075: step 4448, loss 0.0167177, acc 1
2016-09-05T20:03:09.115866: step 4449, loss 0.0235711, acc 0.98
2016-09-05T20:03:09.927818: step 4450, loss 0.0157455, acc 1
2016-09-05T20:03:10.753777: step 4451, loss 0.0171811, acc 1
2016-09-05T20:03:11.556111: step 4452, loss 0.023936, acc 1
2016-09-05T20:03:12.349820: step 4453, loss 0.00316254, acc 1
2016-09-05T20:03:13.168428: step 4454, loss 0.00459957, acc 1
2016-09-05T20:03:13.959157: step 4455, loss 0.0144913, acc 1
2016-09-05T20:03:14.783906: step 4456, loss 0.0139526, acc 1
2016-09-05T20:03:15.652565: step 4457, loss 0.0193777, acc 1
2016-09-05T20:03:16.454046: step 4458, loss 0.0277083, acc 1
2016-09-05T20:03:17.259447: step 4459, loss 0.0823575, acc 0.94
2016-09-05T20:03:18.074182: step 4460, loss 0.00708832, acc 1
2016-09-05T20:03:18.878048: step 4461, loss 0.00329908, acc 1
2016-09-05T20:03:19.327458: step 4462, loss 0.0567965, acc 1
2016-09-05T20:03:20.193363: step 4463, loss 0.0254674, acc 0.98
2016-09-05T20:03:21.032664: step 4464, loss 0.0195291, acc 0.98
2016-09-05T20:03:21.838388: step 4465, loss 0.00539073, acc 1
2016-09-05T20:03:22.657469: step 4466, loss 0.117889, acc 0.98
2016-09-05T20:03:23.465525: step 4467, loss 0.0066859, acc 1
2016-09-05T20:03:24.254087: step 4468, loss 0.0168748, acc 1
2016-09-05T20:03:25.061430: step 4469, loss 0.0381744, acc 0.98
2016-09-05T20:03:25.836519: step 4470, loss 0.043214, acc 0.98
2016-09-05T20:03:26.643616: step 4471, loss 0.00780401, acc 1
2016-09-05T20:03:27.503558: step 4472, loss 0.0116136, acc 1
2016-09-05T20:03:28.280036: step 4473, loss 0.108173, acc 0.98
2016-09-05T20:03:29.083756: step 4474, loss 0.0290525, acc 0.98
2016-09-05T20:03:29.893723: step 4475, loss 0.0993511, acc 0.92
2016-09-05T20:03:30.670182: step 4476, loss 0.00410003, acc 1
2016-09-05T20:03:31.472433: step 4477, loss 0.019246, acc 0.98
2016-09-05T20:03:32.258501: step 4478, loss 0.00762749, acc 1
2016-09-05T20:03:33.072542: step 4479, loss 0.00366541, acc 1
2016-09-05T20:03:33.915682: step 4480, loss 0.0146048, acc 1
2016-09-05T20:03:34.733321: step 4481, loss 0.00380158, acc 1
2016-09-05T20:03:35.531416: step 4482, loss 0.0116937, acc 1
2016-09-05T20:03:36.336357: step 4483, loss 0.0356125, acc 0.96
2016-09-05T20:03:37.155300: step 4484, loss 0.036305, acc 0.98
2016-09-05T20:03:37.966926: step 4485, loss 0.0574102, acc 0.96
2016-09-05T20:03:38.770037: step 4486, loss 0.00636581, acc 1
2016-09-05T20:03:39.584420: step 4487, loss 0.0757041, acc 0.96
2016-09-05T20:03:40.357757: step 4488, loss 0.00399711, acc 1
2016-09-05T20:03:41.145505: step 4489, loss 0.00410252, acc 1
2016-09-05T20:03:41.997910: step 4490, loss 0.0113102, acc 1
2016-09-05T20:03:42.765030: step 4491, loss 0.0111354, acc 1
2016-09-05T20:03:43.552289: step 4492, loss 0.00850064, acc 1
2016-09-05T20:03:44.392516: step 4493, loss 0.0064177, acc 1
2016-09-05T20:03:45.201508: step 4494, loss 0.0555425, acc 0.96
2016-09-05T20:03:46.015461: step 4495, loss 0.0133796, acc 1
2016-09-05T20:03:46.815712: step 4496, loss 0.109374, acc 0.98
2016-09-05T20:03:47.611878: step 4497, loss 0.0309311, acc 0.98
2016-09-05T20:03:48.428917: step 4498, loss 0.137481, acc 0.98
2016-09-05T20:03:49.246841: step 4499, loss 0.0108762, acc 1
2016-09-05T20:03:50.059378: step 4500, loss 0.0558051, acc 0.98

Evaluation:
2016-09-05T20:03:53.594277: step 4500, loss 1.86846, acc 0.733

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-4500

2016-09-05T20:03:55.521476: step 4501, loss 0.0179499, acc 1
2016-09-05T20:03:56.317424: step 4502, loss 0.00323058, acc 1
2016-09-05T20:03:57.116830: step 4503, loss 0.0875628, acc 0.98
2016-09-05T20:03:57.971806: step 4504, loss 0.0138693, acc 1
2016-09-05T20:03:58.803422: step 4505, loss 0.0136283, acc 1
2016-09-05T20:03:59.613314: step 4506, loss 0.0198543, acc 1
2016-09-05T20:04:00.479861: step 4507, loss 0.0583108, acc 0.96
2016-09-05T20:04:01.273517: step 4508, loss 0.0356406, acc 0.98
2016-09-05T20:04:02.084324: step 4509, loss 0.0321935, acc 1
2016-09-05T20:04:02.910522: step 4510, loss 0.0199895, acc 1
2016-09-05T20:04:03.734992: step 4511, loss 0.0162277, acc 1
2016-09-05T20:04:04.570072: step 4512, loss 0.0352707, acc 0.98
2016-09-05T20:04:05.383027: step 4513, loss 0.132597, acc 0.96
2016-09-05T20:04:06.184382: step 4514, loss 0.0315099, acc 1
2016-09-05T20:04:07.046575: step 4515, loss 0.00639492, acc 1
2016-09-05T20:04:07.958940: step 4516, loss 0.0169512, acc 1
2016-09-05T20:04:08.773765: step 4517, loss 0.0317841, acc 0.98
2016-09-05T20:04:09.577216: step 4518, loss 0.00432726, acc 1
2016-09-05T20:04:10.360167: step 4519, loss 0.0360986, acc 0.98
2016-09-05T20:04:11.166741: step 4520, loss 0.00529647, acc 1
2016-09-05T20:04:11.981875: step 4521, loss 0.0704728, acc 0.96
2016-09-05T20:04:12.800603: step 4522, loss 0.0106558, acc 1
2016-09-05T20:04:13.629719: step 4523, loss 0.0187489, acc 1
2016-09-05T20:04:14.409238: step 4524, loss 0.00774775, acc 1
2016-09-05T20:04:15.207102: step 4525, loss 0.0333505, acc 0.98
2016-09-05T20:04:16.005135: step 4526, loss 0.156476, acc 0.96
2016-09-05T20:04:16.803659: step 4527, loss 0.0186289, acc 1
2016-09-05T20:04:17.650151: step 4528, loss 0.170693, acc 0.94
2016-09-05T20:04:18.466623: step 4529, loss 0.0224835, acc 1
2016-09-05T20:04:19.269923: step 4530, loss 0.00302956, acc 1
2016-09-05T20:04:20.058965: step 4531, loss 0.0656771, acc 0.96
2016-09-05T20:04:20.897012: step 4532, loss 0.0338951, acc 0.98
2016-09-05T20:04:21.704190: step 4533, loss 0.0723871, acc 0.96
2016-09-05T20:04:22.504355: step 4534, loss 0.035838, acc 0.98
2016-09-05T20:04:23.335049: step 4535, loss 0.00464357, acc 1
2016-09-05T20:04:24.111495: step 4536, loss 0.0216194, acc 1
2016-09-05T20:04:24.954428: step 4537, loss 0.012014, acc 1
2016-09-05T20:04:25.758032: step 4538, loss 0.0937866, acc 0.92
2016-09-05T20:04:26.529369: step 4539, loss 0.0916482, acc 0.96
2016-09-05T20:04:27.336662: step 4540, loss 0.0589699, acc 0.98
2016-09-05T20:04:28.173503: step 4541, loss 0.0547929, acc 0.98
2016-09-05T20:04:28.936017: step 4542, loss 0.0927115, acc 0.94
2016-09-05T20:04:29.748596: step 4543, loss 0.0232168, acc 1
2016-09-05T20:04:30.583774: step 4544, loss 0.0282477, acc 0.98
2016-09-05T20:04:31.357842: step 4545, loss 0.0053427, acc 1
2016-09-05T20:04:32.171873: step 4546, loss 0.00497211, acc 1
2016-09-05T20:04:33.016443: step 4547, loss 0.0172405, acc 1
2016-09-05T20:04:33.787444: step 4548, loss 0.00567784, acc 1
2016-09-05T20:04:34.592629: step 4549, loss 0.038757, acc 0.98
2016-09-05T20:04:35.425372: step 4550, loss 0.0320108, acc 1
2016-09-05T20:04:36.209401: step 4551, loss 0.114493, acc 0.96
2016-09-05T20:04:37.000546: step 4552, loss 0.0236421, acc 1
2016-09-05T20:04:37.817697: step 4553, loss 0.0118554, acc 1
2016-09-05T20:04:38.613663: step 4554, loss 0.0943777, acc 0.96
2016-09-05T20:04:39.400146: step 4555, loss 0.0726959, acc 0.98
2016-09-05T20:04:40.219999: step 4556, loss 0.0268338, acc 0.98
2016-09-05T20:04:40.996791: step 4557, loss 0.0479183, acc 1
2016-09-05T20:04:41.811685: step 4558, loss 0.0446118, acc 0.98
2016-09-05T20:04:42.598950: step 4559, loss 0.0238786, acc 0.98
2016-09-05T20:04:43.396367: step 4560, loss 0.0325286, acc 1
2016-09-05T20:04:44.206938: step 4561, loss 0.0250669, acc 1
2016-09-05T20:04:45.002746: step 4562, loss 0.0585202, acc 0.94
2016-09-05T20:04:45.784558: step 4563, loss 0.0379144, acc 0.98
2016-09-05T20:04:46.622004: step 4564, loss 0.0557076, acc 0.98
2016-09-05T20:04:47.412302: step 4565, loss 0.026007, acc 0.98
2016-09-05T20:04:48.219998: step 4566, loss 0.0350707, acc 0.98
2016-09-05T20:04:49.052972: step 4567, loss 0.0334184, acc 0.98
2016-09-05T20:04:49.865955: step 4568, loss 0.0412869, acc 0.98
2016-09-05T20:04:50.641786: step 4569, loss 0.0635938, acc 0.96
2016-09-05T20:04:51.464656: step 4570, loss 0.0286466, acc 0.98
2016-09-05T20:04:52.372426: step 4571, loss 0.0043054, acc 1
2016-09-05T20:04:53.170250: step 4572, loss 0.0204071, acc 1
2016-09-05T20:04:53.976962: step 4573, loss 0.042489, acc 0.98
2016-09-05T20:04:54.858613: step 4574, loss 0.0237178, acc 0.98
2016-09-05T20:04:55.682619: step 4575, loss 0.0630444, acc 0.98
2016-09-05T20:04:56.485237: step 4576, loss 0.0521776, acc 0.96
2016-09-05T20:04:57.302452: step 4577, loss 0.0123862, acc 1
2016-09-05T20:04:58.120597: step 4578, loss 0.00866974, acc 1
2016-09-05T20:04:58.930577: step 4579, loss 0.0277043, acc 1
2016-09-05T20:04:59.747487: step 4580, loss 0.0873009, acc 0.98
2016-09-05T20:05:00.584082: step 4581, loss 0.0443002, acc 0.98
2016-09-05T20:05:01.422921: step 4582, loss 0.0806648, acc 0.92
2016-09-05T20:05:02.225862: step 4583, loss 0.0296829, acc 0.98
2016-09-05T20:05:03.033083: step 4584, loss 0.0331065, acc 0.96
2016-09-05T20:05:03.863448: step 4585, loss 0.00385899, acc 1
2016-09-05T20:05:04.699221: step 4586, loss 0.0107681, acc 1
2016-09-05T20:05:05.521928: step 4587, loss 0.136266, acc 0.94
2016-09-05T20:05:06.330388: step 4588, loss 0.027053, acc 0.98
2016-09-05T20:05:07.169939: step 4589, loss 0.0363107, acc 0.98
2016-09-05T20:05:07.986601: step 4590, loss 0.00578963, acc 1
2016-09-05T20:05:08.782582: step 4591, loss 0.00729297, acc 1
2016-09-05T20:05:09.593859: step 4592, loss 0.0204283, acc 1
2016-09-05T20:05:10.389597: step 4593, loss 0.0304991, acc 1
2016-09-05T20:05:11.235462: step 4594, loss 0.0462566, acc 1
2016-09-05T20:05:12.060015: step 4595, loss 0.00377086, acc 1
2016-09-05T20:05:12.881451: step 4596, loss 0.0378876, acc 0.98
2016-09-05T20:05:13.669369: step 4597, loss 0.0387469, acc 0.98
2016-09-05T20:05:14.451255: step 4598, loss 0.030064, acc 1
2016-09-05T20:05:15.288766: step 4599, loss 0.010186, acc 1
2016-09-05T20:05:16.081150: step 4600, loss 0.00365938, acc 1

Evaluation:
2016-09-05T20:05:19.565293: step 4600, loss 1.88001, acc 0.737

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-4600

2016-09-05T20:05:21.469571: step 4601, loss 0.0522347, acc 0.98
2016-09-05T20:05:22.267644: step 4602, loss 0.0610109, acc 0.96
2016-09-05T20:05:23.088398: step 4603, loss 0.0086747, acc 1
2016-09-05T20:05:23.910542: step 4604, loss 0.00677509, acc 1
2016-09-05T20:05:24.694607: step 4605, loss 0.0206991, acc 1
2016-09-05T20:05:25.510717: step 4606, loss 0.0543541, acc 0.98
2016-09-05T20:05:26.344971: step 4607, loss 0.0966072, acc 0.96
2016-09-05T20:05:27.156985: step 4608, loss 0.0549776, acc 0.98
2016-09-05T20:05:27.959948: step 4609, loss 0.0197534, acc 1
2016-09-05T20:05:28.775886: step 4610, loss 0.0456003, acc 0.98
2016-09-05T20:05:29.584832: step 4611, loss 0.0134167, acc 1
2016-09-05T20:05:30.388741: step 4612, loss 0.0341788, acc 0.98
2016-09-05T20:05:31.203501: step 4613, loss 0.0261847, acc 1
2016-09-05T20:05:32.009881: step 4614, loss 0.0284068, acc 1
2016-09-05T20:05:32.827557: step 4615, loss 0.0564141, acc 0.96
2016-09-05T20:05:33.645736: step 4616, loss 0.0382697, acc 0.98
2016-09-05T20:05:34.478110: step 4617, loss 0.158743, acc 0.94
2016-09-05T20:05:35.291191: step 4618, loss 0.00781978, acc 1
2016-09-05T20:05:36.116240: step 4619, loss 0.00487933, acc 1
2016-09-05T20:05:36.896233: step 4620, loss 0.0472329, acc 0.96
2016-09-05T20:05:37.688793: step 4621, loss 0.00344209, acc 1
2016-09-05T20:05:38.533671: step 4622, loss 0.0312329, acc 1
2016-09-05T20:05:39.338865: step 4623, loss 0.070858, acc 0.96
2016-09-05T20:05:40.136594: step 4624, loss 0.0714832, acc 0.98
2016-09-05T20:05:40.952394: step 4625, loss 0.0167272, acc 1
2016-09-05T20:05:41.760438: step 4626, loss 0.0235123, acc 1
2016-09-05T20:05:42.561040: step 4627, loss 0.0120286, acc 1
2016-09-05T20:05:43.399618: step 4628, loss 0.0250742, acc 1
2016-09-05T20:05:44.213879: step 4629, loss 0.00538386, acc 1
2016-09-05T20:05:45.032968: step 4630, loss 0.0363194, acc 0.96
2016-09-05T20:05:45.874197: step 4631, loss 0.105278, acc 0.98
2016-09-05T20:05:46.662030: step 4632, loss 0.0334103, acc 1
2016-09-05T20:05:47.456090: step 4633, loss 0.00435618, acc 1
2016-09-05T20:05:48.289861: step 4634, loss 0.0284377, acc 0.98
2016-09-05T20:05:49.081530: step 4635, loss 0.0462738, acc 1
2016-09-05T20:05:49.893526: step 4636, loss 0.0678681, acc 0.96
2016-09-05T20:05:50.717418: step 4637, loss 0.0952092, acc 0.98
2016-09-05T20:05:51.573573: step 4638, loss 0.00655716, acc 1
2016-09-05T20:05:52.367933: step 4639, loss 0.0158382, acc 1
2016-09-05T20:05:53.161792: step 4640, loss 0.0304154, acc 0.98
2016-09-05T20:05:53.979077: step 4641, loss 0.107164, acc 0.98
2016-09-05T20:05:54.774525: step 4642, loss 0.0135837, acc 1
2016-09-05T20:05:55.570897: step 4643, loss 0.0561423, acc 0.96
2016-09-05T20:05:56.397888: step 4644, loss 0.0885533, acc 0.94
2016-09-05T20:05:57.226180: step 4645, loss 0.0118504, acc 1
2016-09-05T20:05:58.016289: step 4646, loss 0.0088702, acc 1
2016-09-05T20:05:58.826737: step 4647, loss 0.0102866, acc 1
2016-09-05T20:05:59.655977: step 4648, loss 0.0524952, acc 0.98
2016-09-05T20:06:00.481517: step 4649, loss 0.0269402, acc 1
2016-09-05T20:06:01.306499: step 4650, loss 0.0196962, acc 0.98
2016-09-05T20:06:02.113036: step 4651, loss 0.0278196, acc 0.98
2016-09-05T20:06:02.918717: step 4652, loss 0.0309659, acc 1
2016-09-05T20:06:03.743114: step 4653, loss 0.0254907, acc 1
2016-09-05T20:06:04.537156: step 4654, loss 0.0464643, acc 0.98
2016-09-05T20:06:05.341724: step 4655, loss 0.0265006, acc 1
2016-09-05T20:06:05.789496: step 4656, loss 0.0188127, acc 1
2016-09-05T20:06:06.624412: step 4657, loss 0.00920343, acc 1
2016-09-05T20:06:07.465196: step 4658, loss 0.0482914, acc 0.96
2016-09-05T20:06:08.298745: step 4659, loss 0.0146023, acc 1
2016-09-05T20:06:09.129349: step 4660, loss 0.0846009, acc 0.96
2016-09-05T20:06:09.956855: step 4661, loss 0.0794264, acc 0.98
2016-09-05T20:06:10.758232: step 4662, loss 0.0121816, acc 1
2016-09-05T20:06:11.549610: step 4663, loss 0.00883124, acc 1
2016-09-05T20:06:12.369110: step 4664, loss 0.0145302, acc 1
2016-09-05T20:06:13.182068: step 4665, loss 0.00429101, acc 1
2016-09-05T20:06:14.003380: step 4666, loss 0.0378975, acc 0.98
2016-09-05T20:06:14.869052: step 4667, loss 0.00824365, acc 1
2016-09-05T20:06:15.689548: step 4668, loss 0.0312992, acc 0.98
2016-09-05T20:06:16.493545: step 4669, loss 0.0470331, acc 0.98
2016-09-05T20:06:17.311868: step 4670, loss 0.0365418, acc 1
2016-09-05T20:06:18.142536: step 4671, loss 0.0155086, acc 1
2016-09-05T20:06:18.934067: step 4672, loss 0.0326991, acc 0.98
2016-09-05T20:06:19.735194: step 4673, loss 0.0347784, acc 0.98
2016-09-05T20:06:20.549468: step 4674, loss 0.01076, acc 1
2016-09-05T20:06:21.327889: step 4675, loss 0.0189021, acc 0.98
2016-09-05T20:06:22.141927: step 4676, loss 0.0277085, acc 1
2016-09-05T20:06:22.972782: step 4677, loss 0.0282952, acc 0.98
2016-09-05T20:06:23.778328: step 4678, loss 0.0134017, acc 1
2016-09-05T20:06:24.572514: step 4679, loss 0.00584978, acc 1
2016-09-05T20:06:25.397294: step 4680, loss 0.0282754, acc 0.98
2016-09-05T20:06:26.188740: step 4681, loss 0.00958759, acc 1
2016-09-05T20:06:26.997020: step 4682, loss 0.0207982, acc 0.98
2016-09-05T20:06:27.797398: step 4683, loss 0.00375632, acc 1
2016-09-05T20:06:28.603480: step 4684, loss 0.0041034, acc 1
2016-09-05T20:06:29.415026: step 4685, loss 0.0426204, acc 0.98
2016-09-05T20:06:30.234608: step 4686, loss 0.00365177, acc 1
2016-09-05T20:06:31.033189: step 4687, loss 0.070795, acc 0.96
2016-09-05T20:06:31.819782: step 4688, loss 0.059263, acc 0.96
2016-09-05T20:06:32.644198: step 4689, loss 0.0179791, acc 1
2016-09-05T20:06:33.444485: step 4690, loss 0.058091, acc 0.98
2016-09-05T20:06:34.255724: step 4691, loss 0.00400223, acc 1
2016-09-05T20:06:35.045443: step 4692, loss 0.00477868, acc 1
2016-09-05T20:06:35.847956: step 4693, loss 0.0272303, acc 0.98
2016-09-05T20:06:36.656089: step 4694, loss 0.00787378, acc 1
2016-09-05T20:06:37.458986: step 4695, loss 0.0304342, acc 0.98
2016-09-05T20:06:38.259640: step 4696, loss 0.00397828, acc 1
2016-09-05T20:06:39.076855: step 4697, loss 0.0364439, acc 1
2016-09-05T20:06:39.927176: step 4698, loss 0.00806587, acc 1
2016-09-05T20:06:40.720960: step 4699, loss 0.0285925, acc 1
2016-09-05T20:06:41.525054: step 4700, loss 0.0224578, acc 0.98

Evaluation:
2016-09-05T20:06:45.075532: step 4700, loss 2.09371, acc 0.747

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-4700

2016-09-05T20:06:46.965737: step 4701, loss 0.0444127, acc 0.98
2016-09-05T20:06:47.765583: step 4702, loss 0.0248668, acc 1
2016-09-05T20:06:48.574654: step 4703, loss 0.0500481, acc 0.98
2016-09-05T20:06:49.374127: step 4704, loss 0.0330652, acc 0.98
2016-09-05T20:06:50.166431: step 4705, loss 0.0039184, acc 1
2016-09-05T20:06:50.963646: step 4706, loss 0.0326557, acc 0.98
2016-09-05T20:06:51.742336: step 4707, loss 0.038476, acc 0.98
2016-09-05T20:06:52.528866: step 4708, loss 0.0268461, acc 1
2016-09-05T20:06:53.342784: step 4709, loss 0.0081755, acc 1
2016-09-05T20:06:54.168292: step 4710, loss 0.00562316, acc 1
2016-09-05T20:06:54.967943: step 4711, loss 0.0230849, acc 1
2016-09-05T20:06:55.781922: step 4712, loss 0.00729275, acc 1
2016-09-05T20:06:56.569551: step 4713, loss 0.1029, acc 0.94
2016-09-05T20:06:57.374340: step 4714, loss 0.00301743, acc 1
2016-09-05T20:06:58.186080: step 4715, loss 0.00310378, acc 1
2016-09-05T20:06:58.989256: step 4716, loss 0.0150706, acc 1
2016-09-05T20:06:59.793051: step 4717, loss 0.0522406, acc 0.96
2016-09-05T20:07:00.628066: step 4718, loss 0.0305311, acc 0.98
2016-09-05T20:07:01.408321: step 4719, loss 0.0457149, acc 0.98
2016-09-05T20:07:02.192491: step 4720, loss 0.010987, acc 1
2016-09-05T20:07:03.020985: step 4721, loss 0.0565675, acc 0.94
2016-09-05T20:07:03.804381: step 4722, loss 0.00505674, acc 1
2016-09-05T20:07:04.597596: step 4723, loss 0.103598, acc 0.96
2016-09-05T20:07:05.413221: step 4724, loss 0.0244071, acc 0.98
2016-09-05T20:07:06.187631: step 4725, loss 0.0221948, acc 0.98
2016-09-05T20:07:07.019118: step 4726, loss 0.05249, acc 0.94
2016-09-05T20:07:07.826307: step 4727, loss 0.00702458, acc 1
2016-09-05T20:07:08.600885: step 4728, loss 0.0292692, acc 0.98
2016-09-05T20:07:09.440162: step 4729, loss 0.00376862, acc 1
2016-09-05T20:07:10.217677: step 4730, loss 0.0215787, acc 0.98
2016-09-05T20:07:11.008848: step 4731, loss 0.0219529, acc 0.98
2016-09-05T20:07:11.825539: step 4732, loss 0.0476425, acc 0.98
2016-09-05T20:07:12.631536: step 4733, loss 0.0050299, acc 1
2016-09-05T20:07:13.435432: step 4734, loss 0.00803762, acc 1
2016-09-05T20:07:14.257782: step 4735, loss 0.04552, acc 0.98
2016-09-05T20:07:15.087668: step 4736, loss 0.11012, acc 0.98
2016-09-05T20:07:15.883027: step 4737, loss 0.0129755, acc 1
2016-09-05T20:07:16.694232: step 4738, loss 0.00604193, acc 1
2016-09-05T20:07:17.499838: step 4739, loss 0.0526829, acc 0.98
2016-09-05T20:07:18.273988: step 4740, loss 0.00364646, acc 1
2016-09-05T20:07:19.093685: step 4741, loss 0.00398351, acc 1
2016-09-05T20:07:19.922886: step 4742, loss 0.00224236, acc 1
2016-09-05T20:07:20.705896: step 4743, loss 0.00708398, acc 1
2016-09-05T20:07:21.500761: step 4744, loss 0.0155767, acc 1
2016-09-05T20:07:22.320111: step 4745, loss 0.0868369, acc 0.96
2016-09-05T20:07:23.117406: step 4746, loss 0.0166576, acc 1
2016-09-05T20:07:23.965450: step 4747, loss 0.0222687, acc 1
2016-09-05T20:07:24.811733: step 4748, loss 0.0502312, acc 0.98
2016-09-05T20:07:25.597421: step 4749, loss 0.0314408, acc 0.98
2016-09-05T20:07:26.396305: step 4750, loss 0.00245003, acc 1
2016-09-05T20:07:27.231629: step 4751, loss 0.0859668, acc 0.98
2016-09-05T20:07:28.028679: step 4752, loss 0.0155434, acc 1
2016-09-05T20:07:28.861690: step 4753, loss 0.037177, acc 0.96
2016-09-05T20:07:29.702029: step 4754, loss 0.0233884, acc 1
2016-09-05T20:07:30.511300: step 4755, loss 0.00804086, acc 1
2016-09-05T20:07:31.310705: step 4756, loss 0.0521601, acc 0.98
2016-09-05T20:07:32.146903: step 4757, loss 0.00453926, acc 1
2016-09-05T20:07:32.954934: step 4758, loss 0.0616276, acc 0.98
2016-09-05T20:07:33.785976: step 4759, loss 0.0355027, acc 0.98
2016-09-05T20:07:34.601341: step 4760, loss 0.024098, acc 0.98
2016-09-05T20:07:35.450290: step 4761, loss 0.0182615, acc 1
2016-09-05T20:07:36.259557: step 4762, loss 0.0150204, acc 1
2016-09-05T20:07:37.116308: step 4763, loss 0.0140811, acc 1
2016-09-05T20:07:37.926501: step 4764, loss 0.0364956, acc 0.98
2016-09-05T20:07:38.755966: step 4765, loss 0.0129923, acc 1
2016-09-05T20:07:39.607484: step 4766, loss 0.0111611, acc 1
2016-09-05T20:07:40.404722: step 4767, loss 0.0563929, acc 0.96
2016-09-05T20:07:41.227063: step 4768, loss 0.00824562, acc 1
2016-09-05T20:07:42.052688: step 4769, loss 0.0166818, acc 1
2016-09-05T20:07:42.864314: step 4770, loss 0.0576957, acc 0.98
2016-09-05T20:07:43.646319: step 4771, loss 0.0111411, acc 1
2016-09-05T20:07:44.448863: step 4772, loss 0.0444626, acc 0.98
2016-09-05T20:07:45.264683: step 4773, loss 0.034356, acc 0.98
2016-09-05T20:07:46.062915: step 4774, loss 0.0392071, acc 0.98
2016-09-05T20:07:46.851890: step 4775, loss 0.0180321, acc 1
2016-09-05T20:07:47.683649: step 4776, loss 0.0184303, acc 0.98
2016-09-05T20:07:48.458823: step 4777, loss 0.0415328, acc 0.96
2016-09-05T20:07:49.300999: step 4778, loss 0.00329524, acc 1
2016-09-05T20:07:50.110204: step 4779, loss 0.0288746, acc 1
2016-09-05T20:07:50.913663: step 4780, loss 0.0123453, acc 1
2016-09-05T20:07:51.753880: step 4781, loss 0.0160143, acc 1
2016-09-05T20:07:52.556858: step 4782, loss 0.0595488, acc 0.98
2016-09-05T20:07:53.356148: step 4783, loss 0.0726328, acc 0.98
2016-09-05T20:07:54.151216: step 4784, loss 0.0031239, acc 1
2016-09-05T20:07:54.944026: step 4785, loss 0.0491947, acc 0.98
2016-09-05T20:07:55.745520: step 4786, loss 0.0292181, acc 0.98
2016-09-05T20:07:56.566512: step 4787, loss 0.0370797, acc 0.98
2016-09-05T20:07:57.400129: step 4788, loss 0.168784, acc 0.92
2016-09-05T20:07:58.204435: step 4789, loss 0.0693993, acc 0.98
2016-09-05T20:07:59.018963: step 4790, loss 0.0787809, acc 0.94
2016-09-05T20:07:59.837684: step 4791, loss 0.118573, acc 0.94
2016-09-05T20:08:00.645437: step 4792, loss 0.0139016, acc 1
2016-09-05T20:08:01.417413: step 4793, loss 0.0745705, acc 0.96
2016-09-05T20:08:02.249420: step 4794, loss 0.0109126, acc 1
2016-09-05T20:08:03.040853: step 4795, loss 0.00529576, acc 1
2016-09-05T20:08:03.847507: step 4796, loss 0.0418939, acc 0.98
2016-09-05T20:08:04.703179: step 4797, loss 0.0472318, acc 0.98
2016-09-05T20:08:05.520918: step 4798, loss 0.0223977, acc 1
2016-09-05T20:08:06.327764: step 4799, loss 0.14439, acc 0.9
2016-09-05T20:08:07.189496: step 4800, loss 0.00306193, acc 1

Evaluation:
2016-09-05T20:08:10.687523: step 4800, loss 1.30733, acc 0.733

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-4800

2016-09-05T20:08:12.568658: step 4801, loss 0.0725713, acc 0.98
2016-09-05T20:08:13.383128: step 4802, loss 0.0328148, acc 1
2016-09-05T20:08:14.177320: step 4803, loss 0.0192025, acc 0.98
2016-09-05T20:08:14.973510: step 4804, loss 0.0319713, acc 1
2016-09-05T20:08:15.779266: step 4805, loss 0.00758606, acc 1
2016-09-05T20:08:16.577534: step 4806, loss 0.0256281, acc 0.98
2016-09-05T20:08:17.371010: step 4807, loss 0.0301486, acc 0.98
2016-09-05T20:08:18.195421: step 4808, loss 0.0561215, acc 0.98
2016-09-05T20:08:19.016167: step 4809, loss 0.0514566, acc 0.98
2016-09-05T20:08:19.816570: step 4810, loss 0.0900203, acc 0.94
2016-09-05T20:08:20.651327: step 4811, loss 0.0277261, acc 0.98
2016-09-05T20:08:21.464386: step 4812, loss 0.00356763, acc 1
2016-09-05T20:08:22.278764: step 4813, loss 0.0164098, acc 1
2016-09-05T20:08:23.108550: step 4814, loss 0.0601322, acc 0.98
2016-09-05T20:08:23.905300: step 4815, loss 0.0310177, acc 1
2016-09-05T20:08:24.725448: step 4816, loss 0.010246, acc 1
2016-09-05T20:08:25.552241: step 4817, loss 0.0168921, acc 1
2016-09-05T20:08:26.360740: step 4818, loss 0.0284118, acc 0.98
2016-09-05T20:08:27.166626: step 4819, loss 0.0419404, acc 0.98
2016-09-05T20:08:27.978671: step 4820, loss 0.0214734, acc 0.98
2016-09-05T20:08:28.838525: step 4821, loss 0.00330962, acc 1
2016-09-05T20:08:29.653411: step 4822, loss 0.0554632, acc 0.98
2016-09-05T20:08:30.444895: step 4823, loss 0.110955, acc 0.96
2016-09-05T20:08:31.244360: step 4824, loss 0.0261382, acc 1
2016-09-05T20:08:32.070747: step 4825, loss 0.0620714, acc 0.96
2016-09-05T20:08:32.865184: step 4826, loss 0.0574876, acc 0.96
2016-09-05T20:08:33.684838: step 4827, loss 0.0233747, acc 0.98
2016-09-05T20:08:34.475970: step 4828, loss 0.0110599, acc 1
2016-09-05T20:08:35.280746: step 4829, loss 0.0254848, acc 0.98
2016-09-05T20:08:36.082407: step 4830, loss 0.00651595, acc 1
2016-09-05T20:08:36.869965: step 4831, loss 0.0567412, acc 0.96
2016-09-05T20:08:37.679219: step 4832, loss 0.0508183, acc 0.98
2016-09-05T20:08:38.511666: step 4833, loss 0.00464771, acc 1
2016-09-05T20:08:39.329569: step 4834, loss 0.00530233, acc 1
2016-09-05T20:08:40.152042: step 4835, loss 0.0228463, acc 1
2016-09-05T20:08:40.960280: step 4836, loss 0.0338546, acc 0.96
2016-09-05T20:08:41.755667: step 4837, loss 0.0600689, acc 0.94
2016-09-05T20:08:42.581974: step 4838, loss 0.0163421, acc 1
2016-09-05T20:08:43.390483: step 4839, loss 0.0230005, acc 0.98
2016-09-05T20:08:44.177139: step 4840, loss 0.0170023, acc 1
2016-09-05T20:08:44.958144: step 4841, loss 0.0204313, acc 1
2016-09-05T20:08:45.770013: step 4842, loss 0.0234416, acc 0.98
2016-09-05T20:08:46.536025: step 4843, loss 0.0369749, acc 1
2016-09-05T20:08:47.355830: step 4844, loss 0.00300565, acc 1
2016-09-05T20:08:48.161980: step 4845, loss 0.0253271, acc 0.98
2016-09-05T20:08:48.957043: step 4846, loss 0.0946055, acc 0.98
2016-09-05T20:08:49.758967: step 4847, loss 0.0295564, acc 1
2016-09-05T20:08:50.568233: step 4848, loss 0.0179242, acc 0.98
2016-09-05T20:08:51.345462: step 4849, loss 0.0247771, acc 0.98
2016-09-05T20:08:51.747882: step 4850, loss 0.0616465, acc 0.916667
2016-09-05T20:08:52.528922: step 4851, loss 0.0668236, acc 0.94
2016-09-05T20:08:53.360932: step 4852, loss 0.0298558, acc 1
2016-09-05T20:08:54.161518: step 4853, loss 0.0554646, acc 0.96
2016-09-05T20:08:54.960612: step 4854, loss 0.0382882, acc 0.98
2016-09-05T20:08:55.771237: step 4855, loss 0.0163634, acc 1
2016-09-05T20:08:56.597051: step 4856, loss 0.052919, acc 0.98
2016-09-05T20:08:57.358493: step 4857, loss 0.00651048, acc 1
2016-09-05T20:08:58.163475: step 4858, loss 0.0254436, acc 1
2016-09-05T20:08:58.973638: step 4859, loss 0.00371854, acc 1
2016-09-05T20:08:59.807406: step 4860, loss 0.0186421, acc 1
2016-09-05T20:09:00.635772: step 4861, loss 0.00898072, acc 1
2016-09-05T20:09:01.441488: step 4862, loss 0.0203219, acc 0.98
2016-09-05T20:09:02.213288: step 4863, loss 0.0446059, acc 0.96
2016-09-05T20:09:03.018663: step 4864, loss 0.0357095, acc 1
2016-09-05T20:09:03.813574: step 4865, loss 0.00642136, acc 1
2016-09-05T20:09:04.605867: step 4866, loss 0.0457394, acc 0.98
2016-09-05T20:09:05.430120: step 4867, loss 0.00531401, acc 1
2016-09-05T20:09:06.225448: step 4868, loss 0.0129531, acc 1
2016-09-05T20:09:07.060760: step 4869, loss 0.0632503, acc 0.96
2016-09-05T20:09:07.857695: step 4870, loss 0.00795074, acc 1
2016-09-05T20:09:08.677427: step 4871, loss 0.0666895, acc 0.98
2016-09-05T20:09:09.456434: step 4872, loss 0.0359829, acc 0.98
2016-09-05T20:09:10.290624: step 4873, loss 0.0329725, acc 0.98
2016-09-05T20:09:11.109775: step 4874, loss 0.158245, acc 0.98
2016-09-05T20:09:11.897429: step 4875, loss 0.0484649, acc 0.98
2016-09-05T20:09:12.711294: step 4876, loss 0.00485773, acc 1
2016-09-05T20:09:13.528679: step 4877, loss 0.00297468, acc 1
2016-09-05T20:09:14.316016: step 4878, loss 0.168934, acc 0.92
2016-09-05T20:09:15.125860: step 4879, loss 0.0392181, acc 0.98
2016-09-05T20:09:15.974049: step 4880, loss 0.0346925, acc 0.98
2016-09-05T20:09:16.781445: step 4881, loss 0.0309623, acc 0.98
2016-09-05T20:09:17.579014: step 4882, loss 0.0371329, acc 0.98
2016-09-05T20:09:18.408817: step 4883, loss 0.0198365, acc 1
2016-09-05T20:09:19.218590: step 4884, loss 0.0115059, acc 1
2016-09-05T20:09:20.043327: step 4885, loss 0.0672979, acc 0.98
2016-09-05T20:09:20.899274: step 4886, loss 0.0226479, acc 1
2016-09-05T20:09:21.717456: step 4887, loss 0.0202488, acc 1
2016-09-05T20:09:22.546688: step 4888, loss 0.0394217, acc 0.98
2016-09-05T20:09:23.363205: step 4889, loss 0.0053382, acc 1
2016-09-05T20:09:24.169126: step 4890, loss 0.0289682, acc 0.98
2016-09-05T20:09:25.013261: step 4891, loss 0.0297197, acc 1
2016-09-05T20:09:25.867583: step 4892, loss 0.0210416, acc 0.98
2016-09-05T20:09:26.665253: step 4893, loss 0.0148529, acc 1
2016-09-05T20:09:27.468463: step 4894, loss 0.00563127, acc 1
2016-09-05T20:09:28.291837: step 4895, loss 0.0442521, acc 0.98
2016-09-05T20:09:29.100002: step 4896, loss 0.0203689, acc 1
2016-09-05T20:09:29.910448: step 4897, loss 0.0240412, acc 0.98
2016-09-05T20:09:30.745404: step 4898, loss 0.0990771, acc 0.96
2016-09-05T20:09:31.569389: step 4899, loss 0.0793296, acc 0.98
2016-09-05T20:09:32.390980: step 4900, loss 0.0464981, acc 0.98

Evaluation:
2016-09-05T20:09:35.904782: step 4900, loss 1.67512, acc 0.741

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-4900

2016-09-05T20:09:37.886853: step 4901, loss 0.0277527, acc 1
2016-09-05T20:09:38.649384: step 4902, loss 0.0204429, acc 0.98
2016-09-05T20:09:39.451001: step 4903, loss 0.0651986, acc 0.96
2016-09-05T20:09:40.279893: step 4904, loss 0.0154182, acc 1
2016-09-05T20:09:41.071015: step 4905, loss 0.0252014, acc 0.98
2016-09-05T20:09:41.851108: step 4906, loss 0.0834587, acc 0.96
2016-09-05T20:09:42.657766: step 4907, loss 0.0180969, acc 1
2016-09-05T20:09:43.451784: step 4908, loss 0.00270049, acc 1
2016-09-05T20:09:44.264016: step 4909, loss 0.0291371, acc 0.98
2016-09-05T20:09:45.060063: step 4910, loss 0.0195493, acc 1
2016-09-05T20:09:45.846045: step 4911, loss 0.0406084, acc 0.98
2016-09-05T20:09:46.655759: step 4912, loss 0.0267428, acc 0.98
2016-09-05T20:09:47.477247: step 4913, loss 0.0586264, acc 0.98
2016-09-05T20:09:48.264722: step 4914, loss 0.0700172, acc 0.96
2016-09-05T20:09:49.082736: step 4915, loss 0.0543583, acc 0.98
2016-09-05T20:09:49.877269: step 4916, loss 0.0282895, acc 1
2016-09-05T20:09:50.677408: step 4917, loss 0.0333814, acc 1
2016-09-05T20:09:51.507860: step 4918, loss 0.0556818, acc 0.98
2016-09-05T20:09:52.336187: step 4919, loss 0.0202589, acc 0.98
2016-09-05T20:09:53.142495: step 4920, loss 0.0486855, acc 0.98
2016-09-05T20:09:53.969417: step 4921, loss 0.0160336, acc 1
2016-09-05T20:09:54.777408: step 4922, loss 0.0125414, acc 1
2016-09-05T20:09:55.564062: step 4923, loss 0.00325316, acc 1
2016-09-05T20:09:56.370257: step 4924, loss 0.0337153, acc 1
2016-09-05T20:09:57.151527: step 4925, loss 0.00355961, acc 1
2016-09-05T20:09:57.935053: step 4926, loss 0.0340738, acc 0.96
2016-09-05T20:09:58.777744: step 4927, loss 0.0423532, acc 0.96
2016-09-05T20:09:59.595434: step 4928, loss 0.01974, acc 0.98
2016-09-05T20:10:00.403215: step 4929, loss 0.0338788, acc 0.98
2016-09-05T20:10:01.204737: step 4930, loss 0.0195143, acc 0.98
2016-09-05T20:10:02.056689: step 4931, loss 0.0401206, acc 0.98
2016-09-05T20:10:02.855446: step 4932, loss 0.0118668, acc 1
2016-09-05T20:10:03.653257: step 4933, loss 0.0111905, acc 1
2016-09-05T20:10:04.479385: step 4934, loss 0.0180473, acc 1
2016-09-05T20:10:05.288507: step 4935, loss 0.0217361, acc 1
2016-09-05T20:10:06.080334: step 4936, loss 0.00542607, acc 1
2016-09-05T20:10:06.872611: step 4937, loss 0.0466504, acc 0.98
2016-09-05T20:10:07.657436: step 4938, loss 0.0316778, acc 0.98
2016-09-05T20:10:08.445212: step 4939, loss 0.0141705, acc 1
2016-09-05T20:10:09.276545: step 4940, loss 0.039932, acc 0.98
2016-09-05T20:10:10.044638: step 4941, loss 0.00721236, acc 1
2016-09-05T20:10:10.847397: step 4942, loss 0.00412706, acc 1
2016-09-05T20:10:11.678512: step 4943, loss 0.00397639, acc 1
2016-09-05T20:10:12.488625: step 4944, loss 0.0227701, acc 0.98
2016-09-05T20:10:13.271360: step 4945, loss 0.0102386, acc 1
2016-09-05T20:10:14.097093: step 4946, loss 0.0195886, acc 0.98
2016-09-05T20:10:14.924965: step 4947, loss 0.0165198, acc 1
2016-09-05T20:10:15.709655: step 4948, loss 0.0614142, acc 0.98
2016-09-05T20:10:16.517398: step 4949, loss 0.0309763, acc 0.98
2016-09-05T20:10:17.294655: step 4950, loss 0.0293542, acc 0.98
2016-09-05T20:10:18.100937: step 4951, loss 0.00426044, acc 1
2016-09-05T20:10:18.913194: step 4952, loss 0.00417615, acc 1
2016-09-05T20:10:19.682255: step 4953, loss 0.0177248, acc 0.98
2016-09-05T20:10:20.483384: step 4954, loss 0.0164882, acc 1
2016-09-05T20:10:21.314152: step 4955, loss 0.0264799, acc 0.98
2016-09-05T20:10:22.118044: step 4956, loss 0.0409324, acc 0.98
2016-09-05T20:10:22.926531: step 4957, loss 0.0410593, acc 0.98
2016-09-05T20:10:23.752285: step 4958, loss 0.0177771, acc 1
2016-09-05T20:10:24.560052: step 4959, loss 0.015647, acc 1
2016-09-05T20:10:25.349336: step 4960, loss 0.0101337, acc 1
2016-09-05T20:10:26.172878: step 4961, loss 0.0772281, acc 0.96
2016-09-05T20:10:26.962450: step 4962, loss 0.00290108, acc 1
2016-09-05T20:10:27.759057: step 4963, loss 0.00496546, acc 1
2016-09-05T20:10:28.580633: step 4964, loss 0.0348339, acc 0.98
2016-09-05T20:10:29.359680: step 4965, loss 0.00445317, acc 1
2016-09-05T20:10:30.183400: step 4966, loss 0.014816, acc 1
2016-09-05T20:10:31.012269: step 4967, loss 0.0282648, acc 0.98
2016-09-05T20:10:31.795497: step 4968, loss 0.0418576, acc 0.98
2016-09-05T20:10:32.594758: step 4969, loss 0.00294649, acc 1
2016-09-05T20:10:33.413061: step 4970, loss 0.0543504, acc 0.94
2016-09-05T20:10:34.211847: step 4971, loss 0.00972096, acc 1
2016-09-05T20:10:35.013549: step 4972, loss 0.0368411, acc 0.96
2016-09-05T20:10:35.833036: step 4973, loss 0.0125626, acc 1
2016-09-05T20:10:36.623664: step 4974, loss 0.0423369, acc 0.98
2016-09-05T20:10:37.441303: step 4975, loss 0.0382338, acc 0.98
2016-09-05T20:10:38.238043: step 4976, loss 0.0732463, acc 0.98
2016-09-05T20:10:39.028229: step 4977, loss 0.0144593, acc 1
2016-09-05T20:10:39.844014: step 4978, loss 0.0128293, acc 1
2016-09-05T20:10:40.664474: step 4979, loss 0.00366558, acc 1
2016-09-05T20:10:41.481368: step 4980, loss 0.0612864, acc 0.98
2016-09-05T20:10:42.299970: step 4981, loss 0.0391489, acc 0.98
2016-09-05T20:10:43.104478: step 4982, loss 0.0574849, acc 0.96
2016-09-05T20:10:43.891920: step 4983, loss 0.0348388, acc 0.98
2016-09-05T20:10:44.686767: step 4984, loss 0.0145999, acc 1
2016-09-05T20:10:45.497184: step 4985, loss 0.00514176, acc 1
2016-09-05T20:10:46.295201: step 4986, loss 0.00348082, acc 1
2016-09-05T20:10:47.078356: step 4987, loss 0.0245025, acc 0.98
2016-09-05T20:10:47.877407: step 4988, loss 0.00602421, acc 1
2016-09-05T20:10:48.675535: step 4989, loss 0.0157928, acc 1
2016-09-05T20:10:49.484599: step 4990, loss 0.0368947, acc 0.98
2016-09-05T20:10:50.304179: step 4991, loss 0.0137563, acc 1
2016-09-05T20:10:51.095293: step 4992, loss 0.0237764, acc 0.98
2016-09-05T20:10:51.932717: step 4993, loss 0.01281, acc 1
2016-09-05T20:10:52.767383: step 4994, loss 0.0124861, acc 1
2016-09-05T20:10:53.579522: step 4995, loss 0.0181424, acc 1
2016-09-05T20:10:54.368259: step 4996, loss 0.0160339, acc 1
2016-09-05T20:10:55.175227: step 4997, loss 0.00776725, acc 1
2016-09-05T20:10:55.977670: step 4998, loss 0.074688, acc 0.94
2016-09-05T20:10:56.776132: step 4999, loss 0.0780623, acc 0.94
2016-09-05T20:10:57.587949: step 5000, loss 0.059372, acc 0.98

Evaluation:
2016-09-05T20:11:01.101989: step 5000, loss 1.98509, acc 0.737

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-5000

2016-09-05T20:11:02.968499: step 5001, loss 0.0275288, acc 0.98
2016-09-05T20:11:03.818425: step 5002, loss 0.0457722, acc 0.96
2016-09-05T20:11:04.631155: step 5003, loss 0.0325748, acc 0.98
2016-09-05T20:11:05.492362: step 5004, loss 0.0251234, acc 0.98
2016-09-05T20:11:06.337806: step 5005, loss 0.0208209, acc 0.98
2016-09-05T20:11:07.169998: step 5006, loss 0.00277566, acc 1
2016-09-05T20:11:07.983003: step 5007, loss 0.00383168, acc 1
2016-09-05T20:11:08.847347: step 5008, loss 0.067633, acc 0.96
2016-09-05T20:11:09.644256: step 5009, loss 0.0194442, acc 0.98
2016-09-05T20:11:10.436998: step 5010, loss 0.0194769, acc 1
2016-09-05T20:11:11.263948: step 5011, loss 0.0319208, acc 1
2016-09-05T20:11:12.070746: step 5012, loss 0.0469195, acc 0.98
2016-09-05T20:11:12.879153: step 5013, loss 0.0357759, acc 0.98
2016-09-05T20:11:13.704263: step 5014, loss 0.0965287, acc 0.98
2016-09-05T20:11:14.515168: step 5015, loss 0.0137028, acc 1
2016-09-05T20:11:15.319260: step 5016, loss 0.0522339, acc 0.98
2016-09-05T20:11:16.121799: step 5017, loss 0.0943678, acc 0.94
2016-09-05T20:11:16.909821: step 5018, loss 0.037628, acc 0.98
2016-09-05T20:11:17.697463: step 5019, loss 0.0571242, acc 0.98
2016-09-05T20:11:18.544836: step 5020, loss 0.0397174, acc 1
2016-09-05T20:11:19.379560: step 5021, loss 0.0585166, acc 0.96
2016-09-05T20:11:20.170366: step 5022, loss 0.03091, acc 0.98
2016-09-05T20:11:20.990615: step 5023, loss 0.00315343, acc 1
2016-09-05T20:11:21.800097: step 5024, loss 0.0444888, acc 0.98
2016-09-05T20:11:22.584118: step 5025, loss 0.0332746, acc 0.98
2016-09-05T20:11:23.395560: step 5026, loss 0.0473536, acc 0.98
2016-09-05T20:11:24.201287: step 5027, loss 0.0102712, acc 1
2016-09-05T20:11:24.993657: step 5028, loss 0.00509521, acc 1
2016-09-05T20:11:25.807897: step 5029, loss 0.0299722, acc 1
2016-09-05T20:11:26.625381: step 5030, loss 0.0476245, acc 0.98
2016-09-05T20:11:27.393716: step 5031, loss 0.0576132, acc 0.98
2016-09-05T20:11:28.203149: step 5032, loss 0.025796, acc 1
2016-09-05T20:11:29.020421: step 5033, loss 0.0453206, acc 0.96
2016-09-05T20:11:29.814938: step 5034, loss 0.0271275, acc 1
2016-09-05T20:11:30.636373: step 5035, loss 0.0186876, acc 1
2016-09-05T20:11:31.430197: step 5036, loss 0.0261187, acc 0.98
2016-09-05T20:11:32.205434: step 5037, loss 0.0241582, acc 1
2016-09-05T20:11:33.041929: step 5038, loss 0.0198979, acc 0.98
2016-09-05T20:11:33.845924: step 5039, loss 0.212192, acc 0.98
2016-09-05T20:11:34.704453: step 5040, loss 0.0311057, acc 1
2016-09-05T20:11:35.501384: step 5041, loss 0.0319907, acc 1
2016-09-05T20:11:36.329448: step 5042, loss 0.144353, acc 0.96
2016-09-05T20:11:37.139359: step 5043, loss 0.0492512, acc 0.98
2016-09-05T20:11:37.557763: step 5044, loss 0.0232376, acc 1
2016-09-05T20:11:38.386118: step 5045, loss 0.0321583, acc 0.98
2016-09-05T20:11:39.193140: step 5046, loss 0.0278125, acc 1
2016-09-05T20:11:40.004415: step 5047, loss 0.139242, acc 0.96
2016-09-05T20:11:40.827550: step 5048, loss 0.0356454, acc 0.98
2016-09-05T20:11:41.661480: step 5049, loss 0.01574, acc 1
2016-09-05T20:11:42.514261: step 5050, loss 0.112085, acc 0.94
2016-09-05T20:11:43.320922: step 5051, loss 0.0367948, acc 0.98
2016-09-05T20:11:44.143568: step 5052, loss 0.0279762, acc 1
2016-09-05T20:11:44.988087: step 5053, loss 0.0403039, acc 0.98
2016-09-05T20:11:45.789915: step 5054, loss 0.037334, acc 0.98
2016-09-05T20:11:46.583611: step 5055, loss 0.0688495, acc 0.98
2016-09-05T20:11:47.421137: step 5056, loss 0.0223263, acc 0.98
2016-09-05T20:11:48.235287: step 5057, loss 0.0433567, acc 0.98
2016-09-05T20:11:49.083395: step 5058, loss 0.0238292, acc 0.98
2016-09-05T20:11:49.931305: step 5059, loss 0.0414099, acc 0.98
2016-09-05T20:11:50.733412: step 5060, loss 0.0192547, acc 0.98
2016-09-05T20:11:51.537619: step 5061, loss 0.0265738, acc 0.98
2016-09-05T20:11:52.359666: step 5062, loss 0.0280514, acc 0.98
2016-09-05T20:11:53.172166: step 5063, loss 0.0105704, acc 1
2016-09-05T20:11:53.969876: step 5064, loss 0.00633469, acc 1
2016-09-05T20:11:54.797492: step 5065, loss 0.0163718, acc 1
2016-09-05T20:11:55.591640: step 5066, loss 0.0714551, acc 0.96
2016-09-05T20:11:56.400118: step 5067, loss 0.0194009, acc 1
2016-09-05T20:11:57.218602: step 5068, loss 0.01458, acc 1
2016-09-05T20:11:58.044570: step 5069, loss 0.00607719, acc 1
2016-09-05T20:11:58.829869: step 5070, loss 0.00419044, acc 1
2016-09-05T20:11:59.628467: step 5071, loss 0.0393649, acc 0.98
2016-09-05T20:12:00.476113: step 5072, loss 0.00517873, acc 1
2016-09-05T20:12:01.289522: step 5073, loss 0.00562173, acc 1
2016-09-05T20:12:02.094555: step 5074, loss 0.0280504, acc 0.98
2016-09-05T20:12:02.945783: step 5075, loss 0.00361525, acc 1
2016-09-05T20:12:03.738317: step 5076, loss 0.101311, acc 0.92
2016-09-05T20:12:04.541500: step 5077, loss 0.0315481, acc 0.98
2016-09-05T20:12:05.327354: step 5078, loss 0.0634914, acc 0.96
2016-09-05T20:12:06.146074: step 5079, loss 0.00430775, acc 1
2016-09-05T20:12:06.933749: step 5080, loss 0.0255269, acc 1
2016-09-05T20:12:07.757419: step 5081, loss 0.00901914, acc 1
2016-09-05T20:12:08.536779: step 5082, loss 0.0605984, acc 0.98
2016-09-05T20:12:09.359305: step 5083, loss 0.0102404, acc 1
2016-09-05T20:12:10.188820: step 5084, loss 0.00463042, acc 1
2016-09-05T20:12:10.969623: step 5085, loss 0.0180035, acc 0.98
2016-09-05T20:12:11.777409: step 5086, loss 0.0676745, acc 0.98
2016-09-05T20:12:12.581353: step 5087, loss 0.00913349, acc 1
2016-09-05T20:12:13.357986: step 5088, loss 0.0279553, acc 0.98
2016-09-05T20:12:14.163773: step 5089, loss 0.039679, acc 0.98
2016-09-05T20:12:14.970894: step 5090, loss 0.0160563, acc 1
2016-09-05T20:12:15.772362: step 5091, loss 0.0221454, acc 0.98
2016-09-05T20:12:16.593805: step 5092, loss 0.0962797, acc 0.94
2016-09-05T20:12:17.411358: step 5093, loss 0.0135132, acc 1
2016-09-05T20:12:18.232023: step 5094, loss 0.0345357, acc 1
2016-09-05T20:12:19.009739: step 5095, loss 0.0108624, acc 1
2016-09-05T20:12:19.847748: step 5096, loss 0.0777897, acc 0.96
2016-09-05T20:12:20.616063: step 5097, loss 0.0161979, acc 1
2016-09-05T20:12:21.414105: step 5098, loss 0.233643, acc 0.98
2016-09-05T20:12:22.222032: step 5099, loss 0.0121098, acc 1
2016-09-05T20:12:22.990971: step 5100, loss 0.0220268, acc 1

Evaluation:
2016-09-05T20:12:26.549473: step 5100, loss 1.75372, acc 0.737

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-5100

2016-09-05T20:12:28.421696: step 5101, loss 0.0997127, acc 0.98
2016-09-05T20:12:29.233697: step 5102, loss 0.0249526, acc 0.98
2016-09-05T20:12:30.034670: step 5103, loss 0.0350317, acc 0.98
2016-09-05T20:12:30.833831: step 5104, loss 0.0255698, acc 0.98
2016-09-05T20:12:31.663685: step 5105, loss 0.0284732, acc 0.98
2016-09-05T20:12:32.500537: step 5106, loss 0.0158214, acc 1
2016-09-05T20:12:33.327692: step 5107, loss 0.11802, acc 0.98
2016-09-05T20:12:34.150213: step 5108, loss 0.0183679, acc 1
2016-09-05T20:12:34.932317: step 5109, loss 0.084419, acc 0.96
2016-09-05T20:12:35.768110: step 5110, loss 0.0163417, acc 1
2016-09-05T20:12:36.565980: step 5111, loss 0.0410756, acc 0.98
2016-09-05T20:12:37.369249: step 5112, loss 0.0256132, acc 1
2016-09-05T20:12:38.187205: step 5113, loss 0.0238549, acc 0.98
2016-09-05T20:12:38.979874: step 5114, loss 0.0273272, acc 1
2016-09-05T20:12:39.785205: step 5115, loss 0.0149184, acc 1
2016-09-05T20:12:40.614438: step 5116, loss 0.0242313, acc 0.98
2016-09-05T20:12:41.428525: step 5117, loss 0.0114431, acc 1
2016-09-05T20:12:42.233272: step 5118, loss 0.0147564, acc 1
2016-09-05T20:12:43.062544: step 5119, loss 0.0125129, acc 1
2016-09-05T20:12:43.871781: step 5120, loss 0.0487736, acc 0.98
2016-09-05T20:12:44.672607: step 5121, loss 0.0351425, acc 1
2016-09-05T20:12:45.492496: step 5122, loss 0.0209697, acc 0.98
2016-09-05T20:12:46.322795: step 5123, loss 0.0075241, acc 1
2016-09-05T20:12:47.161380: step 5124, loss 0.0350605, acc 1
2016-09-05T20:12:47.981304: step 5125, loss 0.00535634, acc 1
2016-09-05T20:12:48.792500: step 5126, loss 0.00500018, acc 1
2016-09-05T20:12:49.578302: step 5127, loss 0.0378443, acc 0.96
2016-09-05T20:12:50.390873: step 5128, loss 0.0578643, acc 0.98
2016-09-05T20:12:51.206270: step 5129, loss 0.0308686, acc 0.98
2016-09-05T20:12:52.040749: step 5130, loss 0.0288117, acc 1
2016-09-05T20:12:52.853899: step 5131, loss 0.00330698, acc 1
2016-09-05T20:12:53.678267: step 5132, loss 0.0860767, acc 0.94
2016-09-05T20:12:54.466698: step 5133, loss 0.0305831, acc 0.98
2016-09-05T20:12:55.253322: step 5134, loss 0.0144575, acc 1
2016-09-05T20:12:56.068174: step 5135, loss 0.0393697, acc 0.98
2016-09-05T20:12:56.841420: step 5136, loss 0.0376405, acc 0.96
2016-09-05T20:12:57.667930: step 5137, loss 0.0347038, acc 0.98
2016-09-05T20:12:58.457086: step 5138, loss 0.00838226, acc 1
2016-09-05T20:12:59.276997: step 5139, loss 0.0683253, acc 0.96
2016-09-05T20:13:00.096840: step 5140, loss 0.0685911, acc 0.98
2016-09-05T20:13:00.928895: step 5141, loss 0.0249281, acc 0.98
2016-09-05T20:13:01.697500: step 5142, loss 0.0473389, acc 0.98
2016-09-05T20:13:02.510364: step 5143, loss 0.00706219, acc 1
2016-09-05T20:13:03.322018: step 5144, loss 0.033929, acc 0.98
2016-09-05T20:13:04.120688: step 5145, loss 0.028553, acc 0.98
2016-09-05T20:13:04.917794: step 5146, loss 0.00505107, acc 1
2016-09-05T20:13:05.731708: step 5147, loss 0.0170265, acc 1
2016-09-05T20:13:06.514398: step 5148, loss 0.0142278, acc 1
2016-09-05T20:13:07.320119: step 5149, loss 0.0521676, acc 0.98
2016-09-05T20:13:08.109788: step 5150, loss 0.0332973, acc 0.98
2016-09-05T20:13:08.925534: step 5151, loss 0.00562038, acc 1
2016-09-05T20:13:09.745186: step 5152, loss 0.0231259, acc 1
2016-09-05T20:13:10.575415: step 5153, loss 0.145747, acc 0.96
2016-09-05T20:13:11.365262: step 5154, loss 0.0890988, acc 0.98
2016-09-05T20:13:12.160222: step 5155, loss 0.0463703, acc 1
2016-09-05T20:13:12.989260: step 5156, loss 0.0089426, acc 1
2016-09-05T20:13:13.808284: step 5157, loss 0.0100759, acc 1
2016-09-05T20:13:14.589368: step 5158, loss 0.0301355, acc 0.98
2016-09-05T20:13:15.399348: step 5159, loss 0.040429, acc 0.98
2016-09-05T20:13:16.193499: step 5160, loss 0.00855288, acc 1
2016-09-05T20:13:16.986012: step 5161, loss 0.0240013, acc 1
2016-09-05T20:13:17.788793: step 5162, loss 0.014812, acc 1
2016-09-05T20:13:18.613342: step 5163, loss 0.0144734, acc 1
2016-09-05T20:13:19.418142: step 5164, loss 0.0941182, acc 0.96
2016-09-05T20:13:20.245937: step 5165, loss 0.059381, acc 0.96
2016-09-05T20:13:21.035043: step 5166, loss 0.151024, acc 0.94
2016-09-05T20:13:21.822133: step 5167, loss 0.0214252, acc 0.98
2016-09-05T20:13:22.646193: step 5168, loss 0.0212066, acc 1
2016-09-05T20:13:23.427849: step 5169, loss 0.0643081, acc 0.96
2016-09-05T20:13:24.245550: step 5170, loss 0.00662051, acc 1
2016-09-05T20:13:25.070337: step 5171, loss 0.0929524, acc 0.94
2016-09-05T20:13:25.889906: step 5172, loss 0.0344668, acc 1
2016-09-05T20:13:26.721875: step 5173, loss 0.0074868, acc 1
2016-09-05T20:13:27.529680: step 5174, loss 0.0262445, acc 0.98
2016-09-05T20:13:28.317475: step 5175, loss 0.0780678, acc 0.98
2016-09-05T20:13:29.125465: step 5176, loss 0.0254915, acc 1
2016-09-05T20:13:29.919593: step 5177, loss 0.0125084, acc 1
2016-09-05T20:13:30.711215: step 5178, loss 0.0125917, acc 1
2016-09-05T20:13:31.521722: step 5179, loss 0.0509349, acc 0.96
2016-09-05T20:13:32.338036: step 5180, loss 0.0361572, acc 0.98
2016-09-05T20:13:33.132983: step 5181, loss 0.00694056, acc 1
2016-09-05T20:13:33.944445: step 5182, loss 0.00541961, acc 1
2016-09-05T20:13:34.785422: step 5183, loss 0.0770837, acc 0.96
2016-09-05T20:13:35.561392: step 5184, loss 0.0336166, acc 1
2016-09-05T20:13:36.373410: step 5185, loss 0.0219816, acc 0.98
2016-09-05T20:13:37.203692: step 5186, loss 0.0186483, acc 1
2016-09-05T20:13:38.012308: step 5187, loss 0.0626599, acc 0.98
2016-09-05T20:13:38.810826: step 5188, loss 0.0569552, acc 0.98
2016-09-05T20:13:39.636029: step 5189, loss 0.046137, acc 0.98
2016-09-05T20:13:40.452006: step 5190, loss 0.155875, acc 0.98
2016-09-05T20:13:41.282714: step 5191, loss 0.0193368, acc 1
2016-09-05T20:13:42.133430: step 5192, loss 0.0786565, acc 0.98
2016-09-05T20:13:42.941451: step 5193, loss 0.0173652, acc 1
2016-09-05T20:13:43.739424: step 5194, loss 0.0135871, acc 1
2016-09-05T20:13:44.575825: step 5195, loss 0.028838, acc 0.98
2016-09-05T20:13:45.374911: step 5196, loss 0.044871, acc 0.98
2016-09-05T20:13:46.182843: step 5197, loss 0.0515966, acc 0.98
2016-09-05T20:13:47.013222: step 5198, loss 0.0302037, acc 0.98
2016-09-05T20:13:47.806928: step 5199, loss 0.0552948, acc 0.98
2016-09-05T20:13:48.641458: step 5200, loss 0.015582, acc 1

Evaluation:
2016-09-05T20:13:52.161574: step 5200, loss 1.67686, acc 0.734

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-5200

2016-09-05T20:13:54.121527: step 5201, loss 0.00701434, acc 1
2016-09-05T20:13:54.956673: step 5202, loss 0.0383042, acc 0.98
2016-09-05T20:13:55.789489: step 5203, loss 0.0499049, acc 0.98
2016-09-05T20:13:56.596824: step 5204, loss 0.00955925, acc 1
2016-09-05T20:13:57.384434: step 5205, loss 0.0476927, acc 0.98
2016-09-05T20:13:58.213002: step 5206, loss 0.0984351, acc 0.98
2016-09-05T20:13:59.026517: step 5207, loss 0.00851864, acc 1
2016-09-05T20:13:59.822436: step 5208, loss 0.0151296, acc 1
2016-09-05T20:14:00.656404: step 5209, loss 0.0214354, acc 0.98
2016-09-05T20:14:01.467975: step 5210, loss 0.015298, acc 1
2016-09-05T20:14:02.264145: step 5211, loss 0.0539306, acc 0.98
2016-09-05T20:14:03.062704: step 5212, loss 0.0236694, acc 1
2016-09-05T20:14:03.891904: step 5213, loss 0.00778897, acc 1
2016-09-05T20:14:04.674327: step 5214, loss 0.044484, acc 0.98
2016-09-05T20:14:05.485778: step 5215, loss 0.0277145, acc 0.98
2016-09-05T20:14:06.324771: step 5216, loss 0.0189148, acc 1
2016-09-05T20:14:07.140348: step 5217, loss 0.00926919, acc 1
2016-09-05T20:14:07.936303: step 5218, loss 0.017738, acc 1
2016-09-05T20:14:08.782396: step 5219, loss 0.00840265, acc 1
2016-09-05T20:14:09.574443: step 5220, loss 0.0137112, acc 1
2016-09-05T20:14:10.403415: step 5221, loss 0.0343808, acc 0.98
2016-09-05T20:14:11.225645: step 5222, loss 0.012102, acc 1
2016-09-05T20:14:12.040848: step 5223, loss 0.0106863, acc 1
2016-09-05T20:14:12.833572: step 5224, loss 0.0140533, acc 1
2016-09-05T20:14:13.677410: step 5225, loss 0.0395351, acc 0.98
2016-09-05T20:14:14.473411: step 5226, loss 0.0382192, acc 0.98
2016-09-05T20:14:15.307832: step 5227, loss 0.0199067, acc 1
2016-09-05T20:14:16.136114: step 5228, loss 0.00534558, acc 1
2016-09-05T20:14:16.970497: step 5229, loss 0.0231853, acc 1
2016-09-05T20:14:17.778879: step 5230, loss 0.0388994, acc 0.98
2016-09-05T20:14:18.631372: step 5231, loss 0.0297861, acc 1
2016-09-05T20:14:19.457508: step 5232, loss 0.0210364, acc 0.98
2016-09-05T20:14:20.282904: step 5233, loss 0.00679717, acc 1
2016-09-05T20:14:21.102484: step 5234, loss 0.0519071, acc 0.98
2016-09-05T20:14:21.927685: step 5235, loss 0.106583, acc 0.94
2016-09-05T20:14:22.771686: step 5236, loss 0.0203366, acc 0.98
2016-09-05T20:14:23.618428: step 5237, loss 0.0278055, acc 1
2016-09-05T20:14:24.044977: step 5238, loss 0.0108342, acc 1
2016-09-05T20:14:24.862925: step 5239, loss 0.00782442, acc 1
2016-09-05T20:14:25.683158: step 5240, loss 0.0268446, acc 1
2016-09-05T20:14:26.459412: step 5241, loss 0.040597, acc 0.98
2016-09-05T20:14:27.257639: step 5242, loss 0.0129997, acc 1
2016-09-05T20:14:28.081657: step 5243, loss 0.0850567, acc 0.96
2016-09-05T20:14:28.847599: step 5244, loss 0.0297534, acc 0.98
2016-09-05T20:14:29.650934: step 5245, loss 0.00860052, acc 1
2016-09-05T20:14:30.469520: step 5246, loss 0.0175395, acc 1
2016-09-05T20:14:31.268813: step 5247, loss 0.00972932, acc 1
2016-09-05T20:14:32.071441: step 5248, loss 0.00610851, acc 1
2016-09-05T20:14:32.900840: step 5249, loss 0.0249624, acc 0.98
2016-09-05T20:14:33.716046: step 5250, loss 0.0546836, acc 0.98
2016-09-05T20:14:34.529407: step 5251, loss 0.0806522, acc 0.98
2016-09-05T20:14:35.351623: step 5252, loss 0.0341792, acc 0.98
2016-09-05T20:14:36.125825: step 5253, loss 0.0215898, acc 1
2016-09-05T20:14:36.966899: step 5254, loss 0.00372598, acc 1
2016-09-05T20:14:37.784436: step 5255, loss 0.0118032, acc 1
2016-09-05T20:14:38.586882: step 5256, loss 0.00644723, acc 1
2016-09-05T20:14:39.381240: step 5257, loss 0.0357992, acc 0.98
2016-09-05T20:14:40.178848: step 5258, loss 0.0248644, acc 0.98
2016-09-05T20:14:40.956935: step 5259, loss 0.0211215, acc 0.98
2016-09-05T20:14:41.752157: step 5260, loss 0.0210121, acc 0.98
2016-09-05T20:14:42.555760: step 5261, loss 0.00920955, acc 1
2016-09-05T20:14:43.330419: step 5262, loss 0.00399493, acc 1
2016-09-05T20:14:44.130270: step 5263, loss 0.0200568, acc 0.98
2016-09-05T20:14:44.948822: step 5264, loss 0.0164518, acc 1
2016-09-05T20:14:45.742989: step 5265, loss 0.0333213, acc 1
2016-09-05T20:14:46.544501: step 5266, loss 0.0483148, acc 0.96
2016-09-05T20:14:47.373032: step 5267, loss 0.00815491, acc 1
2016-09-05T20:14:48.155763: step 5268, loss 0.0433752, acc 0.98
2016-09-05T20:14:48.980378: step 5269, loss 0.0149571, acc 1
2016-09-05T20:14:49.809269: step 5270, loss 0.0683323, acc 0.96
2016-09-05T20:14:50.608302: step 5271, loss 0.0247386, acc 0.98
2016-09-05T20:14:51.410629: step 5272, loss 0.107808, acc 0.96
2016-09-05T20:14:52.245432: step 5273, loss 0.0229796, acc 0.98
2016-09-05T20:14:53.021125: step 5274, loss 0.00815896, acc 1
2016-09-05T20:14:53.835319: step 5275, loss 0.0965595, acc 0.98
2016-09-05T20:14:54.650842: step 5276, loss 0.0374007, acc 0.98
2016-09-05T20:14:55.448862: step 5277, loss 0.00368347, acc 1
2016-09-05T20:14:56.259910: step 5278, loss 0.0207443, acc 0.98
2016-09-05T20:14:57.071979: step 5279, loss 0.028252, acc 0.98
2016-09-05T20:14:57.858793: step 5280, loss 0.0247973, acc 0.98
2016-09-05T20:14:58.666514: step 5281, loss 0.029163, acc 0.98
2016-09-05T20:14:59.493358: step 5282, loss 0.00343331, acc 1
2016-09-05T20:15:00.296905: step 5283, loss 0.00847726, acc 1
2016-09-05T20:15:01.065738: step 5284, loss 0.133414, acc 0.98
2016-09-05T20:15:01.917444: step 5285, loss 0.0715668, acc 0.94
2016-09-05T20:15:02.701576: step 5286, loss 0.0249507, acc 1
2016-09-05T20:15:03.503916: step 5287, loss 0.0163866, acc 1
2016-09-05T20:15:04.311794: step 5288, loss 0.039104, acc 0.98
2016-09-05T20:15:05.103733: step 5289, loss 0.0207998, acc 0.98
2016-09-05T20:15:05.896699: step 5290, loss 0.0335127, acc 0.98
2016-09-05T20:15:06.717649: step 5291, loss 0.0356074, acc 0.98
2016-09-05T20:15:07.494792: step 5292, loss 0.0604888, acc 0.96
2016-09-05T20:15:08.323422: step 5293, loss 0.0138079, acc 1
2016-09-05T20:15:09.133011: step 5294, loss 0.0104232, acc 1
2016-09-05T20:15:09.929454: step 5295, loss 0.0257877, acc 1
2016-09-05T20:15:10.729118: step 5296, loss 0.00294157, acc 1
2016-09-05T20:15:11.521794: step 5297, loss 0.0327105, acc 1
2016-09-05T20:15:12.315966: step 5298, loss 0.0348178, acc 0.98
2016-09-05T20:15:13.120619: step 5299, loss 0.0511617, acc 0.98
2016-09-05T20:15:13.932871: step 5300, loss 0.0174464, acc 1

Evaluation:
2016-09-05T20:15:17.416929: step 5300, loss 1.54665, acc 0.735

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-5300

2016-09-05T20:15:19.239381: step 5301, loss 0.0868032, acc 0.96
2016-09-05T20:15:20.043683: step 5302, loss 0.0185192, acc 1
2016-09-05T20:15:20.845385: step 5303, loss 0.0358598, acc 0.98
2016-09-05T20:15:21.671087: step 5304, loss 0.0083359, acc 1
2016-09-05T20:15:22.489112: step 5305, loss 0.0405392, acc 0.98
2016-09-05T20:15:23.278131: step 5306, loss 0.042243, acc 0.98
2016-09-05T20:15:24.114902: step 5307, loss 0.0737779, acc 0.98
2016-09-05T20:15:24.908735: step 5308, loss 0.0316433, acc 0.98
2016-09-05T20:15:25.704983: step 5309, loss 0.0278243, acc 0.98
2016-09-05T20:15:26.533450: step 5310, loss 0.0164907, acc 1
2016-09-05T20:15:27.362571: step 5311, loss 0.00443913, acc 1
2016-09-05T20:15:28.165454: step 5312, loss 0.0100092, acc 1
2016-09-05T20:15:28.991672: step 5313, loss 0.0231363, acc 0.98
2016-09-05T20:15:29.793053: step 5314, loss 0.107064, acc 0.92
2016-09-05T20:15:30.582986: step 5315, loss 0.0329554, acc 0.98
2016-09-05T20:15:31.397422: step 5316, loss 0.0178715, acc 0.98
2016-09-05T20:15:32.220543: step 5317, loss 0.010908, acc 1
2016-09-05T20:15:33.015976: step 5318, loss 0.120019, acc 0.98
2016-09-05T20:15:33.821147: step 5319, loss 0.0679078, acc 0.98
2016-09-05T20:15:34.647232: step 5320, loss 0.0450803, acc 0.98
2016-09-05T20:15:35.446039: step 5321, loss 0.01585, acc 1
2016-09-05T20:15:36.259370: step 5322, loss 0.0090554, acc 1
2016-09-05T20:15:37.070127: step 5323, loss 0.0143462, acc 1
2016-09-05T20:15:37.869380: step 5324, loss 0.00908433, acc 1
2016-09-05T20:15:38.679272: step 5325, loss 0.0229115, acc 0.98
2016-09-05T20:15:39.500441: step 5326, loss 0.0110557, acc 1
2016-09-05T20:15:40.293747: step 5327, loss 0.0134426, acc 1
2016-09-05T20:15:41.087305: step 5328, loss 0.0243666, acc 1
2016-09-05T20:15:41.915997: step 5329, loss 0.0180877, acc 0.98
2016-09-05T20:15:42.721527: step 5330, loss 0.00590864, acc 1
2016-09-05T20:15:43.508089: step 5331, loss 0.0407057, acc 0.98
2016-09-05T20:15:44.315272: step 5332, loss 0.128295, acc 0.98
2016-09-05T20:15:45.118611: step 5333, loss 0.0610022, acc 0.98
2016-09-05T20:15:45.912499: step 5334, loss 0.0455574, acc 1
2016-09-05T20:15:46.707959: step 5335, loss 0.0188573, acc 1
2016-09-05T20:15:47.490971: step 5336, loss 0.0578094, acc 0.96
2016-09-05T20:15:48.312321: step 5337, loss 0.0163547, acc 1
2016-09-05T20:15:49.139238: step 5338, loss 0.0344606, acc 0.98
2016-09-05T20:15:49.925691: step 5339, loss 0.0113864, acc 1
2016-09-05T20:15:50.714576: step 5340, loss 0.0272082, acc 0.98
2016-09-05T20:15:51.533251: step 5341, loss 0.0586782, acc 0.98
2016-09-05T20:15:52.340392: step 5342, loss 0.0112471, acc 1
2016-09-05T20:15:53.161573: step 5343, loss 0.0511058, acc 0.96
2016-09-05T20:15:53.980882: step 5344, loss 0.0200137, acc 0.98
2016-09-05T20:15:54.778927: step 5345, loss 0.0259142, acc 0.98
2016-09-05T20:15:55.600686: step 5346, loss 0.044904, acc 0.98
2016-09-05T20:15:56.464993: step 5347, loss 0.0165891, acc 1
2016-09-05T20:15:57.241392: step 5348, loss 0.0638853, acc 0.96
2016-09-05T20:15:58.015811: step 5349, loss 0.112948, acc 0.94
2016-09-05T20:15:58.835605: step 5350, loss 0.0130151, acc 1
2016-09-05T20:15:59.644916: step 5351, loss 0.0280392, acc 0.98
2016-09-05T20:16:00.493580: step 5352, loss 0.0232771, acc 0.98
2016-09-05T20:16:01.297111: step 5353, loss 0.0549236, acc 0.96
2016-09-05T20:16:02.059749: step 5354, loss 0.0874443, acc 0.96
2016-09-05T20:16:02.839845: step 5355, loss 0.0482074, acc 0.98
2016-09-05T20:16:03.640831: step 5356, loss 0.0162677, acc 1
2016-09-05T20:16:04.430574: step 5357, loss 0.0302064, acc 0.98
2016-09-05T20:16:05.255103: step 5358, loss 0.0338858, acc 0.98
2016-09-05T20:16:06.073731: step 5359, loss 0.0059879, acc 1
2016-09-05T20:16:06.858503: step 5360, loss 0.0141025, acc 1
2016-09-05T20:16:07.661276: step 5361, loss 0.0938682, acc 0.94
2016-09-05T20:16:08.481067: step 5362, loss 0.00826267, acc 1
2016-09-05T20:16:09.266695: step 5363, loss 0.0113566, acc 1
2016-09-05T20:16:10.065495: step 5364, loss 0.0255521, acc 0.98
2016-09-05T20:16:10.881813: step 5365, loss 0.0875093, acc 0.98
2016-09-05T20:16:11.712514: step 5366, loss 0.00870172, acc 1
2016-09-05T20:16:12.541653: step 5367, loss 0.0165592, acc 1
2016-09-05T20:16:13.369197: step 5368, loss 0.00459089, acc 1
2016-09-05T20:16:14.151057: step 5369, loss 0.0871626, acc 0.96
2016-09-05T20:16:14.961819: step 5370, loss 0.0205316, acc 0.98
2016-09-05T20:16:15.782253: step 5371, loss 0.00501547, acc 1
2016-09-05T20:16:16.609340: step 5372, loss 0.0214445, acc 0.98
2016-09-05T20:16:17.420736: step 5373, loss 0.0362151, acc 1
2016-09-05T20:16:18.278917: step 5374, loss 0.010654, acc 1
2016-09-05T20:16:19.088985: step 5375, loss 0.00505932, acc 1
2016-09-05T20:16:19.888836: step 5376, loss 0.0083668, acc 1
2016-09-05T20:16:20.723731: step 5377, loss 0.00490171, acc 1
2016-09-05T20:16:21.532693: step 5378, loss 0.0648338, acc 0.96
2016-09-05T20:16:22.320953: step 5379, loss 0.0584582, acc 0.98
2016-09-05T20:16:23.150833: step 5380, loss 0.0316964, acc 0.98
2016-09-05T20:16:23.954746: step 5381, loss 0.0555544, acc 0.98
2016-09-05T20:16:24.774367: step 5382, loss 0.00674387, acc 1
2016-09-05T20:16:25.606053: step 5383, loss 0.0176436, acc 0.98
2016-09-05T20:16:26.401223: step 5384, loss 0.00361779, acc 1
2016-09-05T20:16:27.232101: step 5385, loss 0.0564578, acc 0.96
2016-09-05T20:16:28.045488: step 5386, loss 0.070006, acc 0.98
2016-09-05T20:16:28.844874: step 5387, loss 0.0309949, acc 0.98
2016-09-05T20:16:29.658296: step 5388, loss 0.00328465, acc 1
2016-09-05T20:16:30.505356: step 5389, loss 0.0843751, acc 0.98
2016-09-05T20:16:31.298147: step 5390, loss 0.0390277, acc 0.98
2016-09-05T20:16:32.130134: step 5391, loss 0.0327607, acc 0.98
2016-09-05T20:16:32.964431: step 5392, loss 0.0554366, acc 0.96
2016-09-05T20:16:33.782232: step 5393, loss 0.0463493, acc 0.98
2016-09-05T20:16:34.589058: step 5394, loss 0.0711807, acc 0.98
2016-09-05T20:16:35.419794: step 5395, loss 0.0101996, acc 1
2016-09-05T20:16:36.236584: step 5396, loss 0.0242997, acc 1
2016-09-05T20:16:37.025201: step 5397, loss 0.005721, acc 1
2016-09-05T20:16:37.866133: step 5398, loss 0.0298112, acc 0.98
2016-09-05T20:16:38.662509: step 5399, loss 0.0162975, acc 1
2016-09-05T20:16:39.454018: step 5400, loss 0.0224538, acc 0.98

Evaluation:
2016-09-05T20:16:42.950332: step 5400, loss 1.54985, acc 0.747

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-5400

2016-09-05T20:16:44.836151: step 5401, loss 0.00395427, acc 1
2016-09-05T20:16:45.634476: step 5402, loss 0.0408821, acc 0.96
2016-09-05T20:16:46.470930: step 5403, loss 0.0127315, acc 1
2016-09-05T20:16:47.305348: step 5404, loss 0.0272636, acc 0.98
2016-09-05T20:16:48.098728: step 5405, loss 0.014766, acc 1
2016-09-05T20:16:48.897261: step 5406, loss 0.0278373, acc 0.98
2016-09-05T20:16:49.703204: step 5407, loss 0.0352425, acc 1
2016-09-05T20:16:50.532222: step 5408, loss 0.062561, acc 0.96
2016-09-05T20:16:51.335865: step 5409, loss 0.0282159, acc 0.98
2016-09-05T20:16:52.151206: step 5410, loss 0.0256721, acc 1
2016-09-05T20:16:52.954589: step 5411, loss 0.0239363, acc 0.98
2016-09-05T20:16:53.748102: step 5412, loss 0.00329634, acc 1
2016-09-05T20:16:54.552799: step 5413, loss 0.0275199, acc 0.98
2016-09-05T20:16:55.321362: step 5414, loss 0.00331929, acc 1
2016-09-05T20:16:56.135594: step 5415, loss 0.0255092, acc 1
2016-09-05T20:16:56.943986: step 5416, loss 0.0128739, acc 1
2016-09-05T20:16:57.716808: step 5417, loss 0.0113938, acc 1
2016-09-05T20:16:58.564360: step 5418, loss 0.0142433, acc 1
2016-09-05T20:16:59.367551: step 5419, loss 0.0283226, acc 0.98
2016-09-05T20:17:00.180765: step 5420, loss 0.0122867, acc 1
2016-09-05T20:17:01.076940: step 5421, loss 0.00463451, acc 1
2016-09-05T20:17:01.883706: step 5422, loss 0.0219675, acc 0.98
2016-09-05T20:17:02.675587: step 5423, loss 0.0282989, acc 0.98
2016-09-05T20:17:03.445589: step 5424, loss 0.00380081, acc 1
2016-09-05T20:17:04.270586: step 5425, loss 0.0269972, acc 1
2016-09-05T20:17:05.059718: step 5426, loss 0.00312619, acc 1
2016-09-05T20:17:05.866111: step 5427, loss 0.00828463, acc 1
2016-09-05T20:17:06.683862: step 5428, loss 0.115767, acc 0.94
2016-09-05T20:17:07.473876: step 5429, loss 0.0285121, acc 0.98
2016-09-05T20:17:08.272639: step 5430, loss 0.0308545, acc 0.98
2016-09-05T20:17:09.084541: step 5431, loss 0.00934181, acc 1
2016-09-05T20:17:09.507722: step 5432, loss 0.0224638, acc 1
2016-09-05T20:17:10.349985: step 5433, loss 0.0277208, acc 1
2016-09-05T20:17:11.138662: step 5434, loss 0.0502314, acc 0.98
2016-09-05T20:17:11.943840: step 5435, loss 0.0332571, acc 0.98
2016-09-05T20:17:12.770062: step 5436, loss 0.0104855, acc 1
2016-09-05T20:17:13.573268: step 5437, loss 0.0182101, acc 0.98
2016-09-05T20:17:14.387259: step 5438, loss 0.0174179, acc 1
2016-09-05T20:17:15.215375: step 5439, loss 0.026925, acc 1
2016-09-05T20:17:16.026358: step 5440, loss 0.00348905, acc 1
2016-09-05T20:17:16.852854: step 5441, loss 0.00367258, acc 1
2016-09-05T20:17:17.703888: step 5442, loss 0.00807133, acc 1
2016-09-05T20:17:18.553093: step 5443, loss 0.0338898, acc 0.98
2016-09-05T20:17:19.423694: step 5444, loss 0.00709849, acc 1
2016-09-05T20:17:20.243307: step 5445, loss 0.040351, acc 0.98
2016-09-05T20:17:21.075220: step 5446, loss 0.102419, acc 0.96
2016-09-05T20:17:21.879985: step 5447, loss 0.0304487, acc 0.98
2016-09-05T20:17:22.735896: step 5448, loss 0.124827, acc 0.96
2016-09-05T20:17:23.543373: step 5449, loss 0.0250014, acc 0.98
2016-09-05T20:17:24.348981: step 5450, loss 0.0192553, acc 1
2016-09-05T20:17:25.181748: step 5451, loss 0.0437935, acc 0.96
2016-09-05T20:17:25.998610: step 5452, loss 0.023927, acc 0.98
2016-09-05T20:17:26.797771: step 5453, loss 0.0785687, acc 0.94
2016-09-05T20:17:27.594941: step 5454, loss 0.0244242, acc 1
2016-09-05T20:17:28.407654: step 5455, loss 0.0209895, acc 0.98
2016-09-05T20:17:29.200299: step 5456, loss 0.0101803, acc 1
2016-09-05T20:17:30.017125: step 5457, loss 0.0180051, acc 1
2016-09-05T20:17:30.847478: step 5458, loss 0.0411449, acc 0.96
2016-09-05T20:17:31.650271: step 5459, loss 0.0167777, acc 1
2016-09-05T20:17:32.440605: step 5460, loss 0.0274705, acc 1
2016-09-05T20:17:33.261075: step 5461, loss 0.0605515, acc 0.98
2016-09-05T20:17:34.063017: step 5462, loss 0.0264828, acc 1
2016-09-05T20:17:34.853465: step 5463, loss 0.0244181, acc 0.98
2016-09-05T20:17:35.661532: step 5464, loss 0.0502073, acc 0.96
2016-09-05T20:17:36.454677: step 5465, loss 0.0474003, acc 0.96
2016-09-05T20:17:37.253861: step 5466, loss 0.00822534, acc 1
2016-09-05T20:17:38.064153: step 5467, loss 0.00448684, acc 1
2016-09-05T20:17:38.838640: step 5468, loss 0.0339141, acc 0.98
2016-09-05T20:17:39.647435: step 5469, loss 0.014206, acc 1
2016-09-05T20:17:40.454990: step 5470, loss 0.00514575, acc 1
2016-09-05T20:17:41.285460: step 5471, loss 0.0179314, acc 1
2016-09-05T20:17:42.130589: step 5472, loss 0.00243526, acc 1
2016-09-05T20:17:42.952746: step 5473, loss 0.0218125, acc 1
2016-09-05T20:17:43.721980: step 5474, loss 0.00247257, acc 1
2016-09-05T20:17:44.537882: step 5475, loss 0.035092, acc 0.98
2016-09-05T20:17:45.383790: step 5476, loss 0.00909889, acc 1
2016-09-05T20:17:46.163364: step 5477, loss 0.0181342, acc 0.98
2016-09-05T20:17:46.938340: step 5478, loss 0.0570376, acc 0.96
2016-09-05T20:17:47.764711: step 5479, loss 0.0314554, acc 0.98
2016-09-05T20:17:48.544840: step 5480, loss 0.0185765, acc 1
2016-09-05T20:17:49.357660: step 5481, loss 0.0065375, acc 1
2016-09-05T20:17:50.177252: step 5482, loss 0.0240908, acc 1
2016-09-05T20:17:50.965220: step 5483, loss 0.00868186, acc 1
2016-09-05T20:17:51.766472: step 5484, loss 0.0294607, acc 0.98
2016-09-05T20:17:52.591024: step 5485, loss 0.00395116, acc 1
2016-09-05T20:17:53.382683: step 5486, loss 0.0549956, acc 0.98
2016-09-05T20:17:54.188343: step 5487, loss 0.00260604, acc 1
2016-09-05T20:17:55.010944: step 5488, loss 0.031945, acc 0.98
2016-09-05T20:17:55.801424: step 5489, loss 0.0164647, acc 1
2016-09-05T20:17:56.604936: step 5490, loss 0.00291624, acc 1
2016-09-05T20:17:57.420936: step 5491, loss 0.0132825, acc 1
2016-09-05T20:17:58.209872: step 5492, loss 0.0327487, acc 0.98
2016-09-05T20:17:59.004031: step 5493, loss 0.03994, acc 0.96
2016-09-05T20:17:59.824804: step 5494, loss 0.00280995, acc 1
2016-09-05T20:18:00.641276: step 5495, loss 0.0291514, acc 0.98
2016-09-05T20:18:01.452133: step 5496, loss 0.0380808, acc 0.98
2016-09-05T20:18:02.252374: step 5497, loss 0.0196239, acc 1
2016-09-05T20:18:03.049419: step 5498, loss 0.0194475, acc 1
2016-09-05T20:18:03.833809: step 5499, loss 0.00458087, acc 1
2016-09-05T20:18:04.630279: step 5500, loss 0.0159867, acc 1

Evaluation:
2016-09-05T20:18:08.175036: step 5500, loss 2.10909, acc 0.73

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-5500

2016-09-05T20:18:10.132961: step 5501, loss 0.0192598, acc 0.98
2016-09-05T20:18:10.987538: step 5502, loss 0.0430879, acc 0.98
2016-09-05T20:18:11.796509: step 5503, loss 0.129927, acc 0.98
2016-09-05T20:18:12.591545: step 5504, loss 0.024776, acc 1
2016-09-05T20:18:13.404783: step 5505, loss 0.00306012, acc 1
2016-09-05T20:18:14.217777: step 5506, loss 0.00637898, acc 1
2016-09-05T20:18:15.036077: step 5507, loss 0.0307912, acc 0.98
2016-09-05T20:18:15.855303: step 5508, loss 0.0220177, acc 0.98
2016-09-05T20:18:16.683417: step 5509, loss 0.082788, acc 0.96
2016-09-05T20:18:17.492733: step 5510, loss 0.00247726, acc 1
2016-09-05T20:18:18.352486: step 5511, loss 0.0906257, acc 0.98
2016-09-05T20:18:19.193842: step 5512, loss 0.01759, acc 0.98
2016-09-05T20:18:20.001438: step 5513, loss 0.043384, acc 0.98
2016-09-05T20:18:20.800328: step 5514, loss 0.0280763, acc 1
2016-09-05T20:18:21.612123: step 5515, loss 0.0430021, acc 0.96
2016-09-05T20:18:22.436509: step 5516, loss 0.00803455, acc 1
2016-09-05T20:18:23.219874: step 5517, loss 0.0224734, acc 0.98
2016-09-05T20:18:24.036378: step 5518, loss 0.0199453, acc 1
2016-09-05T20:18:24.832932: step 5519, loss 0.00363693, acc 1
2016-09-05T20:18:25.641898: step 5520, loss 0.0330553, acc 0.98
2016-09-05T20:18:26.447296: step 5521, loss 0.00714806, acc 1
2016-09-05T20:18:27.253751: step 5522, loss 0.0188111, acc 1
2016-09-05T20:18:28.066603: step 5523, loss 0.0392681, acc 1
2016-09-05T20:18:28.920196: step 5524, loss 0.0240424, acc 1
2016-09-05T20:18:29.697395: step 5525, loss 0.00283364, acc 1
2016-09-05T20:18:30.481986: step 5526, loss 0.0226885, acc 1
2016-09-05T20:18:31.327586: step 5527, loss 0.0184076, acc 1
2016-09-05T20:18:32.123879: step 5528, loss 0.0165026, acc 1
2016-09-05T20:18:32.932633: step 5529, loss 0.00276315, acc 1
2016-09-05T20:18:33.751661: step 5530, loss 0.0646204, acc 0.94
2016-09-05T20:18:34.524185: step 5531, loss 0.0492382, acc 0.96
2016-09-05T20:18:35.332003: step 5532, loss 0.0335872, acc 0.98
2016-09-05T20:18:36.126821: step 5533, loss 0.0424115, acc 0.98
2016-09-05T20:18:36.929870: step 5534, loss 0.0221645, acc 0.98
2016-09-05T20:18:37.715855: step 5535, loss 0.00791311, acc 1
2016-09-05T20:18:38.537660: step 5536, loss 0.0146244, acc 1
2016-09-05T20:18:39.337613: step 5537, loss 0.00312951, acc 1
2016-09-05T20:18:40.157336: step 5538, loss 0.00333318, acc 1
2016-09-05T20:18:40.999278: step 5539, loss 0.00489372, acc 1
2016-09-05T20:18:41.770751: step 5540, loss 0.0304615, acc 0.98
2016-09-05T20:18:42.543134: step 5541, loss 0.0214102, acc 1
2016-09-05T20:18:43.363063: step 5542, loss 0.00539944, acc 1
2016-09-05T20:18:44.137890: step 5543, loss 0.0307198, acc 0.98
2016-09-05T20:18:44.945357: step 5544, loss 0.0118654, acc 1
2016-09-05T20:18:45.771841: step 5545, loss 0.00843249, acc 1
2016-09-05T20:18:46.575664: step 5546, loss 0.0216703, acc 1
2016-09-05T20:18:47.374326: step 5547, loss 0.0234276, acc 0.98
2016-09-05T20:18:48.191815: step 5548, loss 0.0125668, acc 1
2016-09-05T20:18:48.985409: step 5549, loss 0.0935822, acc 0.98
2016-09-05T20:18:49.771839: step 5550, loss 0.0351884, acc 0.96
2016-09-05T20:18:50.587532: step 5551, loss 0.00356278, acc 1
2016-09-05T20:18:51.373415: step 5552, loss 0.0363403, acc 1
2016-09-05T20:18:52.201869: step 5553, loss 0.0269823, acc 0.98
2016-09-05T20:18:53.023321: step 5554, loss 0.0151517, acc 1
2016-09-05T20:18:53.793470: step 5555, loss 0.00294007, acc 1
2016-09-05T20:18:54.599239: step 5556, loss 0.00363805, acc 1
2016-09-05T20:18:55.389100: step 5557, loss 0.0309973, acc 0.98
2016-09-05T20:18:56.173459: step 5558, loss 0.0236628, acc 1
2016-09-05T20:18:57.018461: step 5559, loss 0.0518272, acc 0.98
2016-09-05T20:18:57.841133: step 5560, loss 0.0160767, acc 1
2016-09-05T20:18:58.631538: step 5561, loss 0.025027, acc 1
2016-09-05T20:18:59.441882: step 5562, loss 0.00878418, acc 1
2016-09-05T20:19:00.275659: step 5563, loss 0.0157888, acc 1
2016-09-05T20:19:01.071341: step 5564, loss 0.0595417, acc 0.96
2016-09-05T20:19:01.850319: step 5565, loss 0.00316795, acc 1
2016-09-05T20:19:02.669955: step 5566, loss 0.0103808, acc 1
2016-09-05T20:19:03.459296: step 5567, loss 0.0846314, acc 0.98
2016-09-05T20:19:04.285847: step 5568, loss 0.0840994, acc 0.98
2016-09-05T20:19:05.102608: step 5569, loss 0.0321344, acc 0.98
2016-09-05T20:19:05.890193: step 5570, loss 0.00267942, acc 1
2016-09-05T20:19:06.688038: step 5571, loss 0.00707346, acc 1
2016-09-05T20:19:07.503681: step 5572, loss 0.0935633, acc 0.98
2016-09-05T20:19:08.289377: step 5573, loss 0.0055448, acc 1
2016-09-05T20:19:09.096481: step 5574, loss 0.00457913, acc 1
2016-09-05T20:19:09.906798: step 5575, loss 0.0263633, acc 0.98
2016-09-05T20:19:10.698580: step 5576, loss 0.0527827, acc 0.98
2016-09-05T20:19:11.504204: step 5577, loss 0.0241656, acc 1
2016-09-05T20:19:12.303337: step 5578, loss 0.015068, acc 1
2016-09-05T20:19:13.101476: step 5579, loss 0.0201151, acc 0.98
2016-09-05T20:19:13.938234: step 5580, loss 0.0216288, acc 1
2016-09-05T20:19:14.771111: step 5581, loss 0.0371484, acc 0.98
2016-09-05T20:19:15.544229: step 5582, loss 0.0284736, acc 0.98
2016-09-05T20:19:16.349639: step 5583, loss 0.0376695, acc 1
2016-09-05T20:19:17.156062: step 5584, loss 0.0248204, acc 0.98
2016-09-05T20:19:17.953264: step 5585, loss 0.0674494, acc 0.94
2016-09-05T20:19:18.767346: step 5586, loss 0.0408537, acc 0.98
2016-09-05T20:19:19.557419: step 5587, loss 0.0304507, acc 0.98
2016-09-05T20:19:20.378986: step 5588, loss 0.00277191, acc 1
2016-09-05T20:19:21.221697: step 5589, loss 0.00320971, acc 1
2016-09-05T20:19:22.035234: step 5590, loss 0.00433701, acc 1
2016-09-05T20:19:22.808535: step 5591, loss 0.0430439, acc 0.98
2016-09-05T20:19:23.623249: step 5592, loss 0.282798, acc 0.98
2016-09-05T20:19:24.434648: step 5593, loss 0.0282202, acc 0.98
2016-09-05T20:19:25.206002: step 5594, loss 0.0227101, acc 1
2016-09-05T20:19:26.019663: step 5595, loss 0.00310722, acc 1
2016-09-05T20:19:26.830635: step 5596, loss 0.0425637, acc 0.98
2016-09-05T20:19:27.652135: step 5597, loss 0.0396543, acc 0.98
2016-09-05T20:19:28.472313: step 5598, loss 0.0118505, acc 1
2016-09-05T20:19:29.286375: step 5599, loss 0.00775934, acc 1
2016-09-05T20:19:30.059382: step 5600, loss 0.0326839, acc 1

Evaluation:
2016-09-05T20:19:33.597577: step 5600, loss 1.47013, acc 0.735

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-5600

2016-09-05T20:19:35.566308: step 5601, loss 0.0246922, acc 1
2016-09-05T20:19:36.377225: step 5602, loss 0.051536, acc 0.98
2016-09-05T20:19:37.168438: step 5603, loss 0.0408237, acc 0.98
2016-09-05T20:19:38.036908: step 5604, loss 0.013591, acc 1
2016-09-05T20:19:38.851795: step 5605, loss 0.0415501, acc 0.98
2016-09-05T20:19:39.650883: step 5606, loss 0.0373748, acc 0.98
2016-09-05T20:19:40.466820: step 5607, loss 0.0594232, acc 0.98
2016-09-05T20:19:41.279797: step 5608, loss 0.084265, acc 0.98
2016-09-05T20:19:42.099317: step 5609, loss 0.0229337, acc 0.98
2016-09-05T20:19:42.913872: step 5610, loss 0.0866358, acc 0.98
2016-09-05T20:19:43.722703: step 5611, loss 0.0299781, acc 0.98
2016-09-05T20:19:44.524015: step 5612, loss 0.0283027, acc 0.98
2016-09-05T20:19:45.363326: step 5613, loss 0.00745967, acc 1
2016-09-05T20:19:46.185404: step 5614, loss 0.0393034, acc 0.98
2016-09-05T20:19:46.971541: step 5615, loss 0.022481, acc 0.98
2016-09-05T20:19:47.785132: step 5616, loss 0.00873063, acc 1
2016-09-05T20:19:48.626236: step 5617, loss 0.0119925, acc 1
2016-09-05T20:19:49.415645: step 5618, loss 0.00983542, acc 1
2016-09-05T20:19:50.223133: step 5619, loss 0.0235624, acc 1
2016-09-05T20:19:51.033791: step 5620, loss 0.0196147, acc 0.98
2016-09-05T20:19:51.838516: step 5621, loss 0.0788455, acc 0.98
2016-09-05T20:19:52.657433: step 5622, loss 0.0353663, acc 0.98
2016-09-05T20:19:53.468486: step 5623, loss 0.0405524, acc 0.98
2016-09-05T20:19:54.252830: step 5624, loss 0.0275582, acc 1
2016-09-05T20:19:55.056360: step 5625, loss 0.00286187, acc 1
2016-09-05T20:19:55.472190: step 5626, loss 0.0310366, acc 1
2016-09-05T20:19:56.275568: step 5627, loss 0.0212754, acc 1
2016-09-05T20:19:57.115298: step 5628, loss 0.0341147, acc 0.98
2016-09-05T20:19:57.910472: step 5629, loss 0.0300998, acc 1
2016-09-05T20:19:58.733002: step 5630, loss 0.0475375, acc 0.98
2016-09-05T20:19:59.571560: step 5631, loss 0.054892, acc 0.98
2016-09-05T20:20:00.377472: step 5632, loss 0.00350381, acc 1
2016-09-05T20:20:01.176856: step 5633, loss 0.0190816, acc 0.98
2016-09-05T20:20:01.989999: step 5634, loss 0.00886638, acc 1
2016-09-05T20:20:02.775592: step 5635, loss 0.00385147, acc 1
2016-09-05T20:20:03.565150: step 5636, loss 0.00419944, acc 1
2016-09-05T20:20:04.388573: step 5637, loss 0.142237, acc 0.98
2016-09-05T20:20:05.184050: step 5638, loss 0.0176568, acc 1
2016-09-05T20:20:05.965358: step 5639, loss 0.0324406, acc 1
2016-09-05T20:20:06.775968: step 5640, loss 0.0546816, acc 0.98
2016-09-05T20:20:07.599922: step 5641, loss 0.0092318, acc 1
2016-09-05T20:20:08.384045: step 5642, loss 0.00483747, acc 1
2016-09-05T20:20:09.201741: step 5643, loss 0.0258104, acc 0.98
2016-09-05T20:20:09.990072: step 5644, loss 0.0199846, acc 1
2016-09-05T20:20:10.799930: step 5645, loss 0.0449721, acc 0.96
2016-09-05T20:20:11.635689: step 5646, loss 0.00396084, acc 1
2016-09-05T20:20:12.413231: step 5647, loss 0.119507, acc 0.96
2016-09-05T20:20:13.217099: step 5648, loss 0.00349276, acc 1
2016-09-05T20:20:14.038911: step 5649, loss 0.0184694, acc 1
2016-09-05T20:20:14.823290: step 5650, loss 0.0136131, acc 1
2016-09-05T20:20:15.629820: step 5651, loss 0.0204366, acc 0.98
2016-09-05T20:20:16.448876: step 5652, loss 0.00454088, acc 1
2016-09-05T20:20:17.229804: step 5653, loss 0.0130757, acc 1
2016-09-05T20:20:18.048502: step 5654, loss 0.00988797, acc 1
2016-09-05T20:20:18.847717: step 5655, loss 0.0435139, acc 0.98
2016-09-05T20:20:19.626560: step 5656, loss 0.00846089, acc 1
2016-09-05T20:20:20.440429: step 5657, loss 0.0121595, acc 1
2016-09-05T20:20:21.292089: step 5658, loss 0.0514738, acc 0.98
2016-09-05T20:20:22.108739: step 5659, loss 0.0115497, acc 1
2016-09-05T20:20:22.910101: step 5660, loss 0.0565724, acc 0.96
2016-09-05T20:20:23.706725: step 5661, loss 0.00359134, acc 1
2016-09-05T20:20:24.466194: step 5662, loss 0.0170708, acc 0.98
2016-09-05T20:20:25.278852: step 5663, loss 0.0276096, acc 0.98
2016-09-05T20:20:26.105312: step 5664, loss 0.0193764, acc 1
2016-09-05T20:20:26.888275: step 5665, loss 0.0227047, acc 0.98
2016-09-05T20:20:27.698849: step 5666, loss 0.00803944, acc 1
2016-09-05T20:20:28.532183: step 5667, loss 0.0219753, acc 1
2016-09-05T20:20:29.289666: step 5668, loss 0.00620848, acc 1
2016-09-05T20:20:30.087903: step 5669, loss 0.0166205, acc 1
2016-09-05T20:20:30.914591: step 5670, loss 0.00403245, acc 1
2016-09-05T20:20:31.694253: step 5671, loss 0.0322974, acc 0.98
2016-09-05T20:20:32.522698: step 5672, loss 0.0764804, acc 0.98
2016-09-05T20:20:33.343803: step 5673, loss 0.117754, acc 0.98
2016-09-05T20:20:34.131981: step 5674, loss 0.0333492, acc 0.98
2016-09-05T20:20:34.931290: step 5675, loss 0.00593898, acc 1
2016-09-05T20:20:35.751070: step 5676, loss 0.0243012, acc 0.98
2016-09-05T20:20:36.566645: step 5677, loss 0.0126012, acc 1
2016-09-05T20:20:37.418125: step 5678, loss 0.00434343, acc 1
2016-09-05T20:20:38.267197: step 5679, loss 0.0329072, acc 0.98
2016-09-05T20:20:39.084984: step 5680, loss 0.00883837, acc 1
2016-09-05T20:20:39.894776: step 5681, loss 0.0115772, acc 1
2016-09-05T20:20:40.753098: step 5682, loss 0.03538, acc 0.98
2016-09-05T20:20:41.585870: step 5683, loss 0.00910369, acc 1
2016-09-05T20:20:42.406340: step 5684, loss 0.00412461, acc 1
2016-09-05T20:20:43.245851: step 5685, loss 0.0128233, acc 1
2016-09-05T20:20:44.044197: step 5686, loss 0.0105947, acc 1
2016-09-05T20:20:44.873569: step 5687, loss 0.0186704, acc 1
2016-09-05T20:20:45.706773: step 5688, loss 0.0190922, acc 1
2016-09-05T20:20:46.519340: step 5689, loss 0.00503519, acc 1
2016-09-05T20:20:47.334217: step 5690, loss 0.0301686, acc 1
2016-09-05T20:20:48.173358: step 5691, loss 0.0126134, acc 1
2016-09-05T20:20:48.980954: step 5692, loss 0.031889, acc 0.98
2016-09-05T20:20:49.799859: step 5693, loss 0.0750467, acc 0.98
2016-09-05T20:20:50.606204: step 5694, loss 0.0195202, acc 1
2016-09-05T20:20:51.416529: step 5695, loss 0.0517507, acc 0.98
2016-09-05T20:20:52.220059: step 5696, loss 0.0200604, acc 1
2016-09-05T20:20:53.042423: step 5697, loss 0.0056446, acc 1
2016-09-05T20:20:53.870868: step 5698, loss 0.00786955, acc 1
2016-09-05T20:20:54.661099: step 5699, loss 0.00288991, acc 1
2016-09-05T20:20:55.484063: step 5700, loss 0.0140931, acc 1

Evaluation:
2016-09-05T20:20:58.996666: step 5700, loss 2.33122, acc 0.728

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-5700

2016-09-05T20:21:00.852343: step 5701, loss 0.0146714, acc 1
2016-09-05T20:21:01.654332: step 5702, loss 0.0801434, acc 0.96
2016-09-05T20:21:02.489457: step 5703, loss 0.0157384, acc 1
2016-09-05T20:21:03.277494: step 5704, loss 0.00306763, acc 1
2016-09-05T20:21:04.091912: step 5705, loss 0.016673, acc 1
2016-09-05T20:21:04.913476: step 5706, loss 0.0400055, acc 0.98
2016-09-05T20:21:05.717796: step 5707, loss 0.0145362, acc 1
2016-09-05T20:21:06.546724: step 5708, loss 0.173121, acc 0.96
2016-09-05T20:21:07.367484: step 5709, loss 0.010724, acc 1
2016-09-05T20:21:08.177982: step 5710, loss 0.00275738, acc 1
2016-09-05T20:21:08.996000: step 5711, loss 0.00646043, acc 1
2016-09-05T20:21:09.852768: step 5712, loss 0.0424585, acc 0.98
2016-09-05T20:21:10.671055: step 5713, loss 0.00986091, acc 1
2016-09-05T20:21:11.484585: step 5714, loss 0.0261249, acc 0.98
2016-09-05T20:21:12.307694: step 5715, loss 0.0354237, acc 0.98
2016-09-05T20:21:13.116239: step 5716, loss 0.0378234, acc 0.98
2016-09-05T20:21:13.927471: step 5717, loss 0.00396356, acc 1
2016-09-05T20:21:14.762099: step 5718, loss 0.0458011, acc 0.98
2016-09-05T20:21:15.589779: step 5719, loss 0.00998006, acc 1
2016-09-05T20:21:16.374117: step 5720, loss 0.0558448, acc 0.98
2016-09-05T20:21:17.229911: step 5721, loss 0.111675, acc 0.94
2016-09-05T20:21:18.054228: step 5722, loss 0.00987844, acc 1
2016-09-05T20:21:18.834885: step 5723, loss 0.0134574, acc 1
2016-09-05T20:21:19.659798: step 5724, loss 0.0725992, acc 0.96
2016-09-05T20:21:20.471856: step 5725, loss 0.0256403, acc 0.98
2016-09-05T20:21:21.288887: step 5726, loss 0.0112938, acc 1
2016-09-05T20:21:22.098152: step 5727, loss 0.0157905, acc 1
2016-09-05T20:21:22.925108: step 5728, loss 0.0272216, acc 0.98
2016-09-05T20:21:23.706574: step 5729, loss 0.0324587, acc 0.98
2016-09-05T20:21:24.505343: step 5730, loss 0.0282764, acc 0.98
2016-09-05T20:21:25.312701: step 5731, loss 0.00697498, acc 1
2016-09-05T20:21:26.122376: step 5732, loss 0.0117089, acc 1
2016-09-05T20:21:26.915411: step 5733, loss 0.0242538, acc 0.98
2016-09-05T20:21:27.730439: step 5734, loss 0.0142912, acc 1
2016-09-05T20:21:28.509646: step 5735, loss 0.0211017, acc 1
2016-09-05T20:21:29.321683: step 5736, loss 0.0138923, acc 1
2016-09-05T20:21:30.113581: step 5737, loss 0.0667152, acc 0.98
2016-09-05T20:21:30.905456: step 5738, loss 0.0107387, acc 1
2016-09-05T20:21:31.735907: step 5739, loss 0.0109629, acc 1
2016-09-05T20:21:32.563825: step 5740, loss 0.0248849, acc 0.98
2016-09-05T20:21:33.365708: step 5741, loss 0.110862, acc 0.96
2016-09-05T20:21:34.166881: step 5742, loss 0.0424959, acc 0.98
2016-09-05T20:21:34.970757: step 5743, loss 0.0205317, acc 0.98
2016-09-05T20:21:35.754447: step 5744, loss 0.0242502, acc 0.98
2016-09-05T20:21:36.568066: step 5745, loss 0.00311324, acc 1
2016-09-05T20:21:37.353821: step 5746, loss 0.0142597, acc 1
2016-09-05T20:21:38.164390: step 5747, loss 0.0125333, acc 1
2016-09-05T20:21:39.004281: step 5748, loss 0.0381393, acc 0.96
2016-09-05T20:21:39.799174: step 5749, loss 0.00324249, acc 1
2016-09-05T20:21:40.553142: step 5750, loss 0.033932, acc 0.98
2016-09-05T20:21:41.378653: step 5751, loss 0.0963615, acc 0.94
2016-09-05T20:21:42.190735: step 5752, loss 0.0437554, acc 0.98
2016-09-05T20:21:42.981424: step 5753, loss 0.0422632, acc 0.98
2016-09-05T20:21:43.807532: step 5754, loss 0.0275853, acc 1
2016-09-05T20:21:44.618307: step 5755, loss 0.0289304, acc 0.98
2016-09-05T20:21:45.422678: step 5756, loss 0.0822603, acc 0.96
2016-09-05T20:21:46.229188: step 5757, loss 0.0402939, acc 0.96
2016-09-05T20:21:47.062648: step 5758, loss 0.0301173, acc 0.98
2016-09-05T20:21:47.889069: step 5759, loss 0.0292442, acc 0.98
2016-09-05T20:21:48.674434: step 5760, loss 0.027478, acc 0.98
2016-09-05T20:21:49.490659: step 5761, loss 0.0124497, acc 1
2016-09-05T20:21:50.292136: step 5762, loss 0.00635281, acc 1
2016-09-05T20:21:51.097833: step 5763, loss 0.031444, acc 1
2016-09-05T20:21:51.923109: step 5764, loss 0.00556099, acc 1
2016-09-05T20:21:52.741407: step 5765, loss 0.0362014, acc 0.98
2016-09-05T20:21:53.538762: step 5766, loss 0.00531335, acc 1
2016-09-05T20:21:54.367508: step 5767, loss 0.00600353, acc 1
2016-09-05T20:21:55.168911: step 5768, loss 0.00797601, acc 1
2016-09-05T20:21:55.954072: step 5769, loss 0.0288278, acc 1
2016-09-05T20:21:56.782400: step 5770, loss 0.020902, acc 1
2016-09-05T20:21:57.569394: step 5771, loss 0.0187293, acc 1
2016-09-05T20:21:58.370906: step 5772, loss 0.028779, acc 0.98
2016-09-05T20:21:59.194402: step 5773, loss 0.0269722, acc 0.98
2016-09-05T20:21:59.985382: step 5774, loss 0.0774922, acc 0.96
2016-09-05T20:22:00.831949: step 5775, loss 0.00280647, acc 1
2016-09-05T20:22:01.637119: step 5776, loss 0.014557, acc 1
2016-09-05T20:22:02.434269: step 5777, loss 0.0182296, acc 0.98
2016-09-05T20:22:03.234534: step 5778, loss 0.035139, acc 0.96
2016-09-05T20:22:04.046856: step 5779, loss 0.0608975, acc 0.96
2016-09-05T20:22:04.841403: step 5780, loss 0.0532248, acc 0.98
2016-09-05T20:22:05.648908: step 5781, loss 0.0194267, acc 0.98
2016-09-05T20:22:06.443512: step 5782, loss 0.00261641, acc 1
2016-09-05T20:22:07.218185: step 5783, loss 0.00288513, acc 1
2016-09-05T20:22:08.024413: step 5784, loss 0.0329816, acc 1
2016-09-05T20:22:08.842394: step 5785, loss 0.0343127, acc 0.98
2016-09-05T20:22:09.615070: step 5786, loss 0.0252687, acc 1
2016-09-05T20:22:10.423015: step 5787, loss 0.0558325, acc 0.98
2016-09-05T20:22:11.225535: step 5788, loss 0.0123747, acc 1
2016-09-05T20:22:12.010278: step 5789, loss 0.00332147, acc 1
2016-09-05T20:22:12.818303: step 5790, loss 0.0398619, acc 0.98
2016-09-05T20:22:13.626337: step 5791, loss 0.00725308, acc 1
2016-09-05T20:22:14.409895: step 5792, loss 0.0272975, acc 0.98
2016-09-05T20:22:15.230575: step 5793, loss 0.00537186, acc 1
2016-09-05T20:22:16.041250: step 5794, loss 0.0513202, acc 0.98
2016-09-05T20:22:16.888040: step 5795, loss 0.0288247, acc 1
2016-09-05T20:22:17.723793: step 5796, loss 0.0515597, acc 0.98
2016-09-05T20:22:18.539360: step 5797, loss 0.0261103, acc 0.98
2016-09-05T20:22:19.301087: step 5798, loss 0.0385973, acc 0.98
2016-09-05T20:22:20.107163: step 5799, loss 0.110597, acc 0.96
2016-09-05T20:22:20.921798: step 5800, loss 0.0519681, acc 0.98

Evaluation:
2016-09-05T20:22:24.419458: step 5800, loss 1.98223, acc 0.725

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-5800

2016-09-05T20:22:26.385460: step 5801, loss 0.0223235, acc 0.98
2016-09-05T20:22:27.230447: step 5802, loss 0.0162536, acc 1
2016-09-05T20:22:28.062954: step 5803, loss 0.00326158, acc 1
2016-09-05T20:22:28.882618: step 5804, loss 0.00766193, acc 1
2016-09-05T20:22:29.732663: step 5805, loss 0.0221895, acc 1
2016-09-05T20:22:30.568202: step 5806, loss 0.0205297, acc 1
2016-09-05T20:22:31.346620: step 5807, loss 0.01619, acc 1
2016-09-05T20:22:32.185015: step 5808, loss 0.0100629, acc 1
2016-09-05T20:22:33.010553: step 5809, loss 0.0258801, acc 1
2016-09-05T20:22:33.788349: step 5810, loss 0.0168922, acc 1
2016-09-05T20:22:34.597836: step 5811, loss 0.0112214, acc 1
2016-09-05T20:22:35.431466: step 5812, loss 0.0338343, acc 0.98
2016-09-05T20:22:36.224771: step 5813, loss 0.0524894, acc 0.94
2016-09-05T20:22:37.023725: step 5814, loss 0.00293673, acc 1
2016-09-05T20:22:37.855957: step 5815, loss 0.0356606, acc 0.96
2016-09-05T20:22:38.663875: step 5816, loss 0.0327837, acc 0.98
2016-09-05T20:22:39.467261: step 5817, loss 0.0286922, acc 0.98
2016-09-05T20:22:40.295380: step 5818, loss 0.0524071, acc 0.98
2016-09-05T20:22:41.063414: step 5819, loss 0.00287777, acc 1
2016-09-05T20:22:41.478460: step 5820, loss 0.00450865, acc 1
2016-09-05T20:22:42.263700: step 5821, loss 0.0162723, acc 1
2016-09-05T20:22:43.081377: step 5822, loss 0.00556697, acc 1
2016-09-05T20:22:43.902141: step 5823, loss 0.0290493, acc 0.98
2016-09-05T20:22:44.701656: step 5824, loss 0.00318177, acc 1
2016-09-05T20:22:45.500556: step 5825, loss 0.014011, acc 1
2016-09-05T20:22:46.330076: step 5826, loss 0.0548161, acc 0.96
2016-09-05T20:22:47.097005: step 5827, loss 0.00326645, acc 1
2016-09-05T20:22:47.888534: step 5828, loss 0.0298809, acc 0.98
2016-09-05T20:22:48.719116: step 5829, loss 0.0285884, acc 0.98
2016-09-05T20:22:49.510132: step 5830, loss 0.00255928, acc 1
2016-09-05T20:22:50.308836: step 5831, loss 0.02399, acc 0.98
2016-09-05T20:22:51.119846: step 5832, loss 0.035392, acc 1
2016-09-05T20:22:51.890711: step 5833, loss 0.0970716, acc 0.96
2016-09-05T20:22:52.698345: step 5834, loss 0.00716009, acc 1
2016-09-05T20:22:53.501526: step 5835, loss 0.00778963, acc 1
2016-09-05T20:22:54.296937: step 5836, loss 0.00251787, acc 1
2016-09-05T20:22:55.129807: step 5837, loss 0.0431846, acc 0.96
2016-09-05T20:22:55.953035: step 5838, loss 0.0434833, acc 0.98
2016-09-05T20:22:56.744545: step 5839, loss 0.00425092, acc 1
2016-09-05T20:22:57.547814: step 5840, loss 0.0829961, acc 0.98
2016-09-05T20:22:58.352701: step 5841, loss 0.0219433, acc 0.98
2016-09-05T20:22:59.190160: step 5842, loss 0.0794964, acc 0.96
2016-09-05T20:23:00.012346: step 5843, loss 0.0112238, acc 1
2016-09-05T20:23:00.875959: step 5844, loss 0.00282013, acc 1
2016-09-05T20:23:01.661312: step 5845, loss 0.00281879, acc 1
2016-09-05T20:23:02.481343: step 5846, loss 0.00392024, acc 1
2016-09-05T20:23:03.298386: step 5847, loss 0.00266929, acc 1
2016-09-05T20:23:04.097041: step 5848, loss 0.0243712, acc 0.98
2016-09-05T20:23:04.912941: step 5849, loss 0.0213481, acc 1
2016-09-05T20:23:05.751025: step 5850, loss 0.0147169, acc 1
2016-09-05T20:23:06.587340: step 5851, loss 0.0544791, acc 0.96
2016-09-05T20:23:07.379434: step 5852, loss 0.0177256, acc 1
2016-09-05T20:23:08.210405: step 5853, loss 0.0108132, acc 1
2016-09-05T20:23:09.031902: step 5854, loss 0.00336846, acc 1
2016-09-05T20:23:09.849484: step 5855, loss 0.0276174, acc 1
2016-09-05T20:23:10.676976: step 5856, loss 0.0990463, acc 0.98
2016-09-05T20:23:11.506867: step 5857, loss 0.00312357, acc 1
2016-09-05T20:23:12.374554: step 5858, loss 0.01172, acc 1
2016-09-05T20:23:13.221435: step 5859, loss 0.00516761, acc 1
2016-09-05T20:23:14.014705: step 5860, loss 0.00671685, acc 1
2016-09-05T20:23:14.861291: step 5861, loss 0.0419368, acc 0.96
2016-09-05T20:23:15.665433: step 5862, loss 0.00358986, acc 1
2016-09-05T20:23:16.471861: step 5863, loss 0.0162064, acc 1
2016-09-05T20:23:17.274373: step 5864, loss 0.018549, acc 0.98
2016-09-05T20:23:18.094135: step 5865, loss 0.0310332, acc 1
2016-09-05T20:23:18.925875: step 5866, loss 0.00816188, acc 1
2016-09-05T20:23:19.721907: step 5867, loss 0.00951694, acc 1
2016-09-05T20:23:20.527729: step 5868, loss 0.0174429, acc 1
2016-09-05T20:23:21.341587: step 5869, loss 0.00937682, acc 1
2016-09-05T20:23:22.145099: step 5870, loss 0.0279612, acc 0.98
2016-09-05T20:23:22.980335: step 5871, loss 0.016294, acc 1
2016-09-05T20:23:23.824888: step 5872, loss 0.00308169, acc 1
2016-09-05T20:23:24.611155: step 5873, loss 0.0223859, acc 0.98
2016-09-05T20:23:25.393085: step 5874, loss 0.0534897, acc 0.98
2016-09-05T20:23:26.193434: step 5875, loss 0.0394744, acc 0.98
2016-09-05T20:23:26.990853: step 5876, loss 0.0305888, acc 0.98
2016-09-05T20:23:27.786929: step 5877, loss 0.0597406, acc 0.98
2016-09-05T20:23:28.629400: step 5878, loss 0.0406164, acc 0.98
2016-09-05T20:23:29.428285: step 5879, loss 0.00280399, acc 1
2016-09-05T20:23:30.219155: step 5880, loss 0.0517986, acc 0.98
2016-09-05T20:23:31.060210: step 5881, loss 0.0468776, acc 0.98
2016-09-05T20:23:31.861657: step 5882, loss 0.00309454, acc 1
2016-09-05T20:23:32.665449: step 5883, loss 0.0265993, acc 1
2016-09-05T20:23:33.498344: step 5884, loss 0.0256154, acc 0.98
2016-09-05T20:23:34.295749: step 5885, loss 0.0308809, acc 0.98
2016-09-05T20:23:35.089845: step 5886, loss 0.0980387, acc 0.96
2016-09-05T20:23:35.918540: step 5887, loss 0.00524624, acc 1
2016-09-05T20:23:36.702558: step 5888, loss 0.00439226, acc 1
2016-09-05T20:23:37.498798: step 5889, loss 0.00238879, acc 1
2016-09-05T20:23:38.298644: step 5890, loss 0.00619412, acc 1
2016-09-05T20:23:39.079360: step 5891, loss 0.00380225, acc 1
2016-09-05T20:23:39.883399: step 5892, loss 0.00745472, acc 1
2016-09-05T20:23:40.697936: step 5893, loss 0.0248577, acc 0.98
2016-09-05T20:23:41.503122: step 5894, loss 0.00527606, acc 1
2016-09-05T20:23:42.293454: step 5895, loss 0.0171941, acc 0.98
2016-09-05T20:23:43.105466: step 5896, loss 0.0452157, acc 0.98
2016-09-05T20:23:43.898182: step 5897, loss 0.0143905, acc 1
2016-09-05T20:23:44.720540: step 5898, loss 0.0121781, acc 1
2016-09-05T20:23:45.542568: step 5899, loss 0.0250231, acc 0.98
2016-09-05T20:23:46.341032: step 5900, loss 0.0536353, acc 0.98

Evaluation:
2016-09-05T20:23:49.849550: step 5900, loss 2.04217, acc 0.742

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-5900

2016-09-05T20:23:51.794758: step 5901, loss 0.0701843, acc 0.98
2016-09-05T20:23:52.599293: step 5902, loss 0.00299728, acc 1
2016-09-05T20:23:53.392531: step 5903, loss 0.00530377, acc 1
2016-09-05T20:23:54.210840: step 5904, loss 0.0214363, acc 0.98
2016-09-05T20:23:55.034989: step 5905, loss 0.00933004, acc 1
2016-09-05T20:23:55.833621: step 5906, loss 0.00381935, acc 1
2016-09-05T20:23:56.663073: step 5907, loss 0.0166877, acc 1
2016-09-05T20:23:57.456959: step 5908, loss 0.0427862, acc 0.98
2016-09-05T20:23:58.269405: step 5909, loss 0.0744245, acc 0.96
2016-09-05T20:23:59.127013: step 5910, loss 0.0171032, acc 1
2016-09-05T20:23:59.935941: step 5911, loss 0.0184022, acc 1
2016-09-05T20:24:00.807159: step 5912, loss 0.0397749, acc 0.98
2016-09-05T20:24:01.618832: step 5913, loss 0.0187748, acc 0.98
2016-09-05T20:24:02.416779: step 5914, loss 0.00629772, acc 1
2016-09-05T20:24:03.227721: step 5915, loss 0.03965, acc 0.98
2016-09-05T20:24:04.045831: step 5916, loss 0.0200244, acc 0.98
2016-09-05T20:24:04.918151: step 5917, loss 0.0054648, acc 1
2016-09-05T20:24:05.685589: step 5918, loss 0.0520608, acc 0.94
2016-09-05T20:24:06.475310: step 5919, loss 0.00405218, acc 1
2016-09-05T20:24:07.286588: step 5920, loss 0.046177, acc 0.98
2016-09-05T20:24:08.077089: step 5921, loss 0.00255413, acc 1
2016-09-05T20:24:08.896085: step 5922, loss 0.0379778, acc 0.98
2016-09-05T20:24:09.749492: step 5923, loss 0.0190126, acc 0.98
2016-09-05T20:24:10.545184: step 5924, loss 0.0213225, acc 0.98
2016-09-05T20:24:11.346295: step 5925, loss 0.0535419, acc 0.98
2016-09-05T20:24:12.173394: step 5926, loss 0.0456082, acc 0.98
2016-09-05T20:24:12.952997: step 5927, loss 0.00677869, acc 1
2016-09-05T20:24:13.748640: step 5928, loss 0.00562279, acc 1
2016-09-05T20:24:14.560482: step 5929, loss 0.0110561, acc 1
2016-09-05T20:24:15.359964: step 5930, loss 0.067943, acc 0.98
2016-09-05T20:24:16.157755: step 5931, loss 0.0470808, acc 0.98
2016-09-05T20:24:16.962423: step 5932, loss 0.00842332, acc 1
2016-09-05T20:24:17.760672: step 5933, loss 0.0132082, acc 1
2016-09-05T20:24:18.588237: step 5934, loss 0.119953, acc 0.94
2016-09-05T20:24:19.408720: step 5935, loss 0.0323381, acc 0.96
2016-09-05T20:24:20.186853: step 5936, loss 0.0302249, acc 1
2016-09-05T20:24:20.984020: step 5937, loss 0.0176153, acc 0.98
2016-09-05T20:24:21.816760: step 5938, loss 0.0114743, acc 1
2016-09-05T20:24:22.602376: step 5939, loss 0.0249534, acc 0.98
2016-09-05T20:24:23.423150: step 5940, loss 0.0791847, acc 0.94
2016-09-05T20:24:24.233262: step 5941, loss 0.0205777, acc 1
2016-09-05T20:24:25.049022: step 5942, loss 0.0586173, acc 0.98
2016-09-05T20:24:25.849284: step 5943, loss 0.00237266, acc 1
2016-09-05T20:24:26.652783: step 5944, loss 0.00610832, acc 1
2016-09-05T20:24:27.449710: step 5945, loss 0.0431144, acc 0.96
2016-09-05T20:24:28.251137: step 5946, loss 0.0169428, acc 0.98
2016-09-05T20:24:29.113407: step 5947, loss 0.0381334, acc 0.98
2016-09-05T20:24:29.889906: step 5948, loss 0.00607281, acc 1
2016-09-05T20:24:30.677928: step 5949, loss 0.0184842, acc 0.98
2016-09-05T20:24:31.509797: step 5950, loss 0.0212391, acc 1
2016-09-05T20:24:32.286408: step 5951, loss 0.0397184, acc 0.98
2016-09-05T20:24:33.085637: step 5952, loss 0.0197578, acc 1
2016-09-05T20:24:33.904952: step 5953, loss 0.0197732, acc 0.98
2016-09-05T20:24:34.702939: step 5954, loss 0.011492, acc 1
2016-09-05T20:24:35.502711: step 5955, loss 0.00652548, acc 1
2016-09-05T20:24:36.313870: step 5956, loss 0.0166234, acc 1
2016-09-05T20:24:37.098853: step 5957, loss 0.020197, acc 0.98
2016-09-05T20:24:37.912844: step 5958, loss 0.00805133, acc 1
2016-09-05T20:24:38.705395: step 5959, loss 0.0229521, acc 0.98
2016-09-05T20:24:39.507481: step 5960, loss 0.0171461, acc 1
2016-09-05T20:24:40.356233: step 5961, loss 0.0274737, acc 1
2016-09-05T20:24:41.178832: step 5962, loss 0.0146062, acc 1
2016-09-05T20:24:41.964295: step 5963, loss 0.0269595, acc 1
2016-09-05T20:24:42.779292: step 5964, loss 0.0192539, acc 0.98
2016-09-05T20:24:43.623986: step 5965, loss 0.0247358, acc 0.98
2016-09-05T20:24:44.407333: step 5966, loss 0.00277042, acc 1
2016-09-05T20:24:45.200324: step 5967, loss 0.0140989, acc 1
2016-09-05T20:24:46.047786: step 5968, loss 0.0055712, acc 1
2016-09-05T20:24:46.820688: step 5969, loss 0.0364308, acc 1
2016-09-05T20:24:47.620631: step 5970, loss 0.0068073, acc 1
2016-09-05T20:24:48.429622: step 5971, loss 0.016608, acc 1
2016-09-05T20:24:49.236539: step 5972, loss 0.0471741, acc 0.98
2016-09-05T20:24:50.040263: step 5973, loss 0.0759504, acc 0.98
2016-09-05T20:24:50.848122: step 5974, loss 0.017205, acc 1
2016-09-05T20:24:51.633398: step 5975, loss 0.0631013, acc 0.96
2016-09-05T20:24:52.449564: step 5976, loss 0.00304967, acc 1
2016-09-05T20:24:53.281428: step 5977, loss 0.00268998, acc 1
2016-09-05T20:24:54.065452: step 5978, loss 0.0125908, acc 1
2016-09-05T20:24:54.872370: step 5979, loss 0.00302609, acc 1
2016-09-05T20:24:55.693400: step 5980, loss 0.00346174, acc 1
2016-09-05T20:24:56.485074: step 5981, loss 0.00308551, acc 1
2016-09-05T20:24:57.309552: step 5982, loss 0.0955191, acc 0.94
2016-09-05T20:24:58.126135: step 5983, loss 0.00710698, acc 1
2016-09-05T20:24:58.889326: step 5984, loss 0.0761197, acc 0.98
2016-09-05T20:24:59.686036: step 5985, loss 0.0278183, acc 0.98
2016-09-05T20:25:00.556136: step 5986, loss 0.0293989, acc 0.98
2016-09-05T20:25:01.346772: step 5987, loss 0.0182906, acc 1
2016-09-05T20:25:02.144964: step 5988, loss 0.0161843, acc 1
2016-09-05T20:25:02.960008: step 5989, loss 0.0174815, acc 0.98
2016-09-05T20:25:03.723097: step 5990, loss 0.0284924, acc 1
2016-09-05T20:25:04.512219: step 5991, loss 0.0160488, acc 1
2016-09-05T20:25:05.332883: step 5992, loss 0.00500383, acc 1
2016-09-05T20:25:06.117489: step 5993, loss 0.0331921, acc 0.98
2016-09-05T20:25:06.893208: step 5994, loss 0.00455844, acc 1
2016-09-05T20:25:07.704581: step 5995, loss 0.0425517, acc 0.98
2016-09-05T20:25:08.497455: step 5996, loss 0.0114721, acc 1
2016-09-05T20:25:09.330603: step 5997, loss 0.011475, acc 1
2016-09-05T20:25:10.127053: step 5998, loss 0.0103706, acc 1
2016-09-05T20:25:10.924521: step 5999, loss 0.0048567, acc 1
2016-09-05T20:25:11.748521: step 6000, loss 0.0973073, acc 0.94

Evaluation:
2016-09-05T20:25:15.290349: step 6000, loss 2.02609, acc 0.736

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-6000

2016-09-05T20:25:17.118852: step 6001, loss 0.0295331, acc 0.98
2016-09-05T20:25:17.933053: step 6002, loss 0.0635702, acc 0.98
2016-09-05T20:25:18.741969: step 6003, loss 0.0198375, acc 1
2016-09-05T20:25:19.563438: step 6004, loss 0.0175261, acc 0.98
2016-09-05T20:25:20.369456: step 6005, loss 0.00636827, acc 1
2016-09-05T20:25:21.209568: step 6006, loss 0.043728, acc 0.98
2016-09-05T20:25:21.987562: step 6007, loss 0.0780551, acc 0.98
2016-09-05T20:25:22.793050: step 6008, loss 0.00503854, acc 1
2016-09-05T20:25:23.659723: step 6009, loss 0.0772944, acc 0.94
2016-09-05T20:25:24.488013: step 6010, loss 0.0160765, acc 1
2016-09-05T20:25:25.289036: step 6011, loss 0.00713899, acc 1
2016-09-05T20:25:26.107637: step 6012, loss 0.0332803, acc 0.98
2016-09-05T20:25:26.910936: step 6013, loss 0.00302363, acc 1
2016-09-05T20:25:27.357897: step 6014, loss 0.0340975, acc 1
2016-09-05T20:25:28.175621: step 6015, loss 0.020468, acc 1
2016-09-05T20:25:28.981051: step 6016, loss 0.0722418, acc 0.96
2016-09-05T20:25:29.794974: step 6017, loss 0.0176263, acc 1
2016-09-05T20:25:30.624803: step 6018, loss 0.0130211, acc 1
2016-09-05T20:25:31.453031: step 6019, loss 0.0109745, acc 1
2016-09-05T20:25:32.303026: step 6020, loss 0.00924689, acc 1
2016-09-05T20:25:33.106873: step 6021, loss 0.0256828, acc 1
2016-09-05T20:25:33.914208: step 6022, loss 0.0339503, acc 0.98
2016-09-05T20:25:34.746033: step 6023, loss 0.0102214, acc 1
2016-09-05T20:25:35.613647: step 6024, loss 0.0266016, acc 1
2016-09-05T20:25:36.405556: step 6025, loss 0.0144422, acc 1
2016-09-05T20:25:37.186042: step 6026, loss 0.0228991, acc 1
2016-09-05T20:25:37.981575: step 6027, loss 0.0220195, acc 1
2016-09-05T20:25:38.767538: step 6028, loss 0.0802345, acc 0.98
2016-09-05T20:25:39.599270: step 6029, loss 0.0175546, acc 0.98
2016-09-05T20:25:40.453045: step 6030, loss 0.00292507, acc 1
2016-09-05T20:25:41.211871: step 6031, loss 0.00880291, acc 1
2016-09-05T20:25:42.001542: step 6032, loss 0.0200651, acc 1
2016-09-05T20:25:42.826858: step 6033, loss 0.0264365, acc 0.98
2016-09-05T20:25:43.614197: step 6034, loss 0.0202759, acc 1
2016-09-05T20:25:44.420047: step 6035, loss 0.0761723, acc 0.96
2016-09-05T20:25:45.227736: step 6036, loss 0.0111629, acc 1
2016-09-05T20:25:46.013757: step 6037, loss 0.00315046, acc 1
2016-09-05T20:25:46.806264: step 6038, loss 0.0349218, acc 0.98
2016-09-05T20:25:47.607866: step 6039, loss 0.0330574, acc 0.98
2016-09-05T20:25:48.410325: step 6040, loss 0.0167444, acc 0.98
2016-09-05T20:25:49.234798: step 6041, loss 0.00277215, acc 1
2016-09-05T20:25:50.075287: step 6042, loss 0.0108023, acc 1
2016-09-05T20:25:50.888907: step 6043, loss 0.00960926, acc 1
2016-09-05T20:25:51.691184: step 6044, loss 0.013203, acc 1
2016-09-05T20:25:52.513406: step 6045, loss 0.108018, acc 0.98
2016-09-05T20:25:53.284025: step 6046, loss 0.0583398, acc 0.96
2016-09-05T20:25:54.082521: step 6047, loss 0.0225541, acc 1
2016-09-05T20:25:54.915542: step 6048, loss 0.0735804, acc 0.96
2016-09-05T20:25:55.759648: step 6049, loss 0.00649849, acc 1
2016-09-05T20:25:56.573411: step 6050, loss 0.0209538, acc 0.98
2016-09-05T20:25:57.365519: step 6051, loss 0.0299727, acc 0.98
2016-09-05T20:25:58.138590: step 6052, loss 0.0405716, acc 0.96
2016-09-05T20:25:58.952846: step 6053, loss 0.0026771, acc 1
2016-09-05T20:25:59.800225: step 6054, loss 0.00256366, acc 1
2016-09-05T20:26:00.601155: step 6055, loss 0.0233193, acc 0.98
2016-09-05T20:26:01.400381: step 6056, loss 0.0199917, acc 1
2016-09-05T20:26:02.169576: step 6057, loss 0.051294, acc 0.98
2016-09-05T20:26:02.955223: step 6058, loss 0.0490118, acc 0.96
2016-09-05T20:26:03.761740: step 6059, loss 0.0237224, acc 0.98
2016-09-05T20:26:04.557471: step 6060, loss 0.0583461, acc 0.96
2016-09-05T20:26:05.345593: step 6061, loss 0.0329516, acc 0.98
2016-09-05T20:26:06.155569: step 6062, loss 0.0117412, acc 1
2016-09-05T20:26:06.995917: step 6063, loss 0.0045729, acc 1
2016-09-05T20:26:07.798902: step 6064, loss 0.131561, acc 0.96
2016-09-05T20:26:08.616794: step 6065, loss 0.0305523, acc 1
2016-09-05T20:26:09.424706: step 6066, loss 0.00411819, acc 1
2016-09-05T20:26:10.205012: step 6067, loss 0.00423025, acc 1
2016-09-05T20:26:10.989901: step 6068, loss 0.0137793, acc 1
2016-09-05T20:26:11.803777: step 6069, loss 0.0111787, acc 1
2016-09-05T20:26:12.592766: step 6070, loss 0.0108636, acc 1
2016-09-05T20:26:13.405810: step 6071, loss 0.0443332, acc 0.96
2016-09-05T20:26:14.227877: step 6072, loss 0.00475652, acc 1
2016-09-05T20:26:15.018977: step 6073, loss 0.00493148, acc 1
2016-09-05T20:26:15.840193: step 6074, loss 0.0168261, acc 1
2016-09-05T20:26:16.671758: step 6075, loss 0.012237, acc 1
2016-09-05T20:26:17.446852: step 6076, loss 0.00778514, acc 1
2016-09-05T20:26:18.250196: step 6077, loss 0.0265289, acc 0.98
2016-09-05T20:26:19.065887: step 6078, loss 0.0525066, acc 0.98
2016-09-05T20:26:19.848341: step 6079, loss 0.037157, acc 0.96
2016-09-05T20:26:20.684938: step 6080, loss 0.00513215, acc 1
2016-09-05T20:26:21.524856: step 6081, loss 0.0479104, acc 0.98
2016-09-05T20:26:22.306200: step 6082, loss 0.00471855, acc 1
2016-09-05T20:26:23.138050: step 6083, loss 0.00609756, acc 1
2016-09-05T20:26:23.946056: step 6084, loss 0.043853, acc 0.96
2016-09-05T20:26:24.712983: step 6085, loss 0.00342379, acc 1
2016-09-05T20:26:25.524380: step 6086, loss 0.0438157, acc 0.98
2016-09-05T20:26:26.345151: step 6087, loss 0.00314539, acc 1
2016-09-05T20:26:27.128292: step 6088, loss 0.00735948, acc 1
2016-09-05T20:26:27.922691: step 6089, loss 0.0232809, acc 0.98
2016-09-05T20:26:28.732191: step 6090, loss 0.0665218, acc 0.96
2016-09-05T20:26:29.539650: step 6091, loss 0.0459503, acc 0.96
2016-09-05T20:26:30.344568: step 6092, loss 0.00651922, acc 1
2016-09-05T20:26:31.148209: step 6093, loss 0.0101465, acc 1
2016-09-05T20:26:31.950035: step 6094, loss 0.00364226, acc 1
2016-09-05T20:26:32.750781: step 6095, loss 0.0626097, acc 0.96
2016-09-05T20:26:33.579040: step 6096, loss 0.00323788, acc 1
2016-09-05T20:26:34.346553: step 6097, loss 0.0057331, acc 1
2016-09-05T20:26:35.140488: step 6098, loss 0.0334696, acc 0.98
2016-09-05T20:26:35.939999: step 6099, loss 0.11206, acc 0.98
2016-09-05T20:26:36.754683: step 6100, loss 0.0382404, acc 1

Evaluation:
2016-09-05T20:26:40.272402: step 6100, loss 1.97376, acc 0.729

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-6100

2016-09-05T20:26:42.057443: step 6101, loss 0.0102197, acc 1
2016-09-05T20:26:42.866516: step 6102, loss 0.0392201, acc 0.96
2016-09-05T20:26:43.684944: step 6103, loss 0.0633732, acc 0.98
2016-09-05T20:26:44.494208: step 6104, loss 0.0719929, acc 0.96
2016-09-05T20:26:45.270350: step 6105, loss 0.0308385, acc 0.98
2016-09-05T20:26:46.105214: step 6106, loss 0.0420061, acc 0.98
2016-09-05T20:26:46.938071: step 6107, loss 0.00897163, acc 1
2016-09-05T20:26:47.728131: step 6108, loss 0.00499448, acc 1
2016-09-05T20:26:48.532853: step 6109, loss 0.0371779, acc 1
2016-09-05T20:26:49.367145: step 6110, loss 0.0138147, acc 1
2016-09-05T20:26:50.200760: step 6111, loss 0.0397751, acc 0.98
2016-09-05T20:26:51.029048: step 6112, loss 0.0315363, acc 1
2016-09-05T20:26:51.862882: step 6113, loss 0.02263, acc 1
2016-09-05T20:26:52.705070: step 6114, loss 0.0279006, acc 0.98
2016-09-05T20:26:53.506727: step 6115, loss 0.0146638, acc 1
2016-09-05T20:26:54.341327: step 6116, loss 0.0173134, acc 0.98
2016-09-05T20:26:55.132428: step 6117, loss 0.0240765, acc 0.98
2016-09-05T20:26:55.936237: step 6118, loss 0.0448566, acc 0.98
2016-09-05T20:26:56.779405: step 6119, loss 0.00322132, acc 1
2016-09-05T20:26:57.598922: step 6120, loss 0.0159887, acc 1
2016-09-05T20:26:58.412463: step 6121, loss 0.0418349, acc 0.96
2016-09-05T20:26:59.223377: step 6122, loss 0.015854, acc 1
2016-09-05T20:27:00.041537: step 6123, loss 0.0472326, acc 0.96
2016-09-05T20:27:00.862026: step 6124, loss 0.035477, acc 0.98
2016-09-05T20:27:01.686518: step 6125, loss 0.0439112, acc 0.98
2016-09-05T20:27:02.485310: step 6126, loss 0.00284009, acc 1
2016-09-05T20:27:03.309702: step 6127, loss 0.0453886, acc 0.98
2016-09-05T20:27:04.123562: step 6128, loss 0.00838405, acc 1
2016-09-05T20:27:04.904330: step 6129, loss 0.00422837, acc 1
2016-09-05T20:27:05.709608: step 6130, loss 0.0356332, acc 1
2016-09-05T20:27:06.528006: step 6131, loss 0.00435912, acc 1
2016-09-05T20:27:07.337887: step 6132, loss 0.0341044, acc 1
2016-09-05T20:27:08.157262: step 6133, loss 0.0247505, acc 0.98
2016-09-05T20:27:08.985926: step 6134, loss 0.0166808, acc 1
2016-09-05T20:27:09.799046: step 6135, loss 0.0710074, acc 0.96
2016-09-05T20:27:10.615960: step 6136, loss 0.0500323, acc 0.96
2016-09-05T20:27:11.430448: step 6137, loss 0.019392, acc 1
2016-09-05T20:27:12.229513: step 6138, loss 0.0455755, acc 0.98
2016-09-05T20:27:13.022006: step 6139, loss 0.00844957, acc 1
2016-09-05T20:27:13.861185: step 6140, loss 0.00308199, acc 1
2016-09-05T20:27:14.667979: step 6141, loss 0.0120013, acc 1
2016-09-05T20:27:15.469832: step 6142, loss 0.0180003, acc 1
2016-09-05T20:27:16.284432: step 6143, loss 0.00334029, acc 1
2016-09-05T20:27:17.093437: step 6144, loss 0.0207728, acc 0.98
2016-09-05T20:27:17.881213: step 6145, loss 0.0206522, acc 0.98
2016-09-05T20:27:18.697759: step 6146, loss 0.0103819, acc 1
2016-09-05T20:27:19.485444: step 6147, loss 0.00924475, acc 1
2016-09-05T20:27:20.329409: step 6148, loss 0.00373503, acc 1
2016-09-05T20:27:21.150270: step 6149, loss 0.0197025, acc 0.98
2016-09-05T20:27:21.984586: step 6150, loss 0.0121545, acc 1
2016-09-05T20:27:22.779208: step 6151, loss 0.0718119, acc 0.98
2016-09-05T20:27:23.582809: step 6152, loss 0.00298893, acc 1
2016-09-05T20:27:24.399504: step 6153, loss 0.010447, acc 1
2016-09-05T20:27:25.195988: step 6154, loss 0.0489495, acc 0.96
2016-09-05T20:27:25.986513: step 6155, loss 0.0907987, acc 0.98
2016-09-05T20:27:26.783971: step 6156, loss 0.0163272, acc 1
2016-09-05T20:27:27.593434: step 6157, loss 0.145563, acc 0.94
2016-09-05T20:27:28.385115: step 6158, loss 0.0155339, acc 1
2016-09-05T20:27:29.189532: step 6159, loss 0.0302798, acc 0.98
2016-09-05T20:27:29.942409: step 6160, loss 0.00301752, acc 1
2016-09-05T20:27:30.767395: step 6161, loss 0.0143533, acc 1
2016-09-05T20:27:31.585199: step 6162, loss 0.0944471, acc 0.98
2016-09-05T20:27:32.398948: step 6163, loss 0.030398, acc 0.98
2016-09-05T20:27:33.215373: step 6164, loss 0.00255045, acc 1
2016-09-05T20:27:34.053882: step 6165, loss 0.0361115, acc 0.98
2016-09-05T20:27:34.853781: step 6166, loss 0.0559113, acc 0.96
2016-09-05T20:27:35.663088: step 6167, loss 0.10165, acc 0.96
2016-09-05T20:27:36.491236: step 6168, loss 0.0640474, acc 0.96
2016-09-05T20:27:37.282672: step 6169, loss 0.0228627, acc 1
2016-09-05T20:27:38.093917: step 6170, loss 0.00975238, acc 1
2016-09-05T20:27:38.899912: step 6171, loss 0.00626721, acc 1
2016-09-05T20:27:39.682405: step 6172, loss 0.0411506, acc 0.98
2016-09-05T20:27:40.475648: step 6173, loss 0.0606745, acc 0.98
2016-09-05T20:27:41.266506: step 6174, loss 0.00436548, acc 1
2016-09-05T20:27:42.121732: step 6175, loss 0.0330918, acc 0.98
2016-09-05T20:27:42.949436: step 6176, loss 0.0554786, acc 0.96
2016-09-05T20:27:43.795824: step 6177, loss 0.0248526, acc 0.98
2016-09-05T20:27:44.607595: step 6178, loss 0.0258485, acc 0.98
2016-09-05T20:27:45.408724: step 6179, loss 0.00440862, acc 1
2016-09-05T20:27:46.228660: step 6180, loss 0.0186165, acc 1
2016-09-05T20:27:47.022978: step 6181, loss 0.00253426, acc 1
2016-09-05T20:27:47.844103: step 6182, loss 0.0422457, acc 0.98
2016-09-05T20:27:48.656514: step 6183, loss 0.0124703, acc 1
2016-09-05T20:27:49.441104: step 6184, loss 0.104769, acc 0.94
2016-09-05T20:27:50.241203: step 6185, loss 0.0135061, acc 1
2016-09-05T20:27:51.062035: step 6186, loss 0.0154113, acc 1
2016-09-05T20:27:51.876166: step 6187, loss 0.0155525, acc 1
2016-09-05T20:27:52.678387: step 6188, loss 0.0533396, acc 0.96
2016-09-05T20:27:53.538917: step 6189, loss 0.00488759, acc 1
2016-09-05T20:27:54.332060: step 6190, loss 0.0270925, acc 1
2016-09-05T20:27:55.165017: step 6191, loss 0.00746278, acc 1
2016-09-05T20:27:55.998928: step 6192, loss 0.00322807, acc 1
2016-09-05T20:27:56.807952: step 6193, loss 0.0342031, acc 0.96
2016-09-05T20:27:57.618994: step 6194, loss 0.0315338, acc 0.98
2016-09-05T20:27:58.469357: step 6195, loss 0.0479647, acc 0.96
2016-09-05T20:27:59.296461: step 6196, loss 0.00289087, acc 1
2016-09-05T20:28:00.084311: step 6197, loss 0.023227, acc 0.98
2016-09-05T20:28:00.927970: step 6198, loss 0.011468, acc 1
2016-09-05T20:28:01.737117: step 6199, loss 0.0107037, acc 1
2016-09-05T20:28:02.547072: step 6200, loss 0.015705, acc 1

Evaluation:
2016-09-05T20:28:06.047836: step 6200, loss 2.21136, acc 0.722

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-6200

2016-09-05T20:28:08.007103: step 6201, loss 0.0350665, acc 1
2016-09-05T20:28:08.804594: step 6202, loss 0.00378141, acc 1
2016-09-05T20:28:09.640509: step 6203, loss 0.0147268, acc 1
2016-09-05T20:28:10.490375: step 6204, loss 0.0170908, acc 0.98
2016-09-05T20:28:11.308165: step 6205, loss 0.00687571, acc 1
2016-09-05T20:28:12.116426: step 6206, loss 0.0411248, acc 0.98
2016-09-05T20:28:12.925695: step 6207, loss 0.00501133, acc 1
2016-09-05T20:28:13.342052: step 6208, loss 0.00282649, acc 1
2016-09-05T20:28:14.169744: step 6209, loss 0.0401062, acc 0.98
2016-09-05T20:28:14.976843: step 6210, loss 0.0892909, acc 0.96
2016-09-05T20:28:15.780982: step 6211, loss 0.0577769, acc 0.98
2016-09-05T20:28:16.610990: step 6212, loss 0.0157213, acc 1
2016-09-05T20:28:17.444343: step 6213, loss 0.0291911, acc 1
2016-09-05T20:28:18.250603: step 6214, loss 0.0346001, acc 0.98
2016-09-05T20:28:19.072106: step 6215, loss 0.0072247, acc 1
2016-09-05T20:28:19.873309: step 6216, loss 0.0305381, acc 0.98
2016-09-05T20:28:20.705571: step 6217, loss 0.00764036, acc 1
2016-09-05T20:28:21.553101: step 6218, loss 0.0492184, acc 0.98
2016-09-05T20:28:22.391970: step 6219, loss 0.00315319, acc 1
2016-09-05T20:28:23.200611: step 6220, loss 0.018389, acc 0.98
2016-09-05T20:28:24.015341: step 6221, loss 0.036587, acc 1
2016-09-05T20:28:24.831881: step 6222, loss 0.0262819, acc 0.98
2016-09-05T20:28:25.616417: step 6223, loss 0.00455364, acc 1
2016-09-05T20:28:26.453587: step 6224, loss 0.0828784, acc 0.98
2016-09-05T20:28:27.314843: step 6225, loss 0.0253564, acc 0.98
2016-09-05T20:28:28.073970: step 6226, loss 0.0284746, acc 0.98
2016-09-05T20:28:28.922073: step 6227, loss 0.0329418, acc 0.98
2016-09-05T20:28:29.697028: step 6228, loss 0.00669421, acc 1
2016-09-05T20:28:30.465381: step 6229, loss 0.0347379, acc 0.98
2016-09-05T20:28:31.273741: step 6230, loss 0.005756, acc 1
2016-09-05T20:28:32.091821: step 6231, loss 0.0325221, acc 1
2016-09-05T20:28:32.887381: step 6232, loss 0.0031558, acc 1
2016-09-05T20:28:33.675894: step 6233, loss 0.0195821, acc 1
2016-09-05T20:28:34.502730: step 6234, loss 0.00442617, acc 1
2016-09-05T20:28:35.289313: step 6235, loss 0.0112058, acc 1
2016-09-05T20:28:36.097483: step 6236, loss 0.0235225, acc 1
2016-09-05T20:28:36.917023: step 6237, loss 0.0560015, acc 0.98
2016-09-05T20:28:37.689680: step 6238, loss 0.018463, acc 1
2016-09-05T20:28:38.495805: step 6239, loss 0.0311469, acc 0.96
2016-09-05T20:28:39.331486: step 6240, loss 0.0344996, acc 0.98
2016-09-05T20:28:40.128925: step 6241, loss 0.0976532, acc 0.96
2016-09-05T20:28:40.939865: step 6242, loss 0.0203515, acc 0.98
2016-09-05T20:28:41.718184: step 6243, loss 0.00286125, acc 1
2016-09-05T20:28:42.506522: step 6244, loss 0.0267636, acc 1
2016-09-05T20:28:43.331489: step 6245, loss 0.00293926, acc 1
2016-09-05T20:28:44.175135: step 6246, loss 0.0124898, acc 1
2016-09-05T20:28:44.977693: step 6247, loss 0.00320747, acc 1
2016-09-05T20:28:45.780642: step 6248, loss 0.00256789, acc 1
2016-09-05T20:28:46.600324: step 6249, loss 0.0613035, acc 0.98
2016-09-05T20:28:47.384094: step 6250, loss 0.0195139, acc 0.98
2016-09-05T20:28:48.169339: step 6251, loss 0.0496958, acc 0.96
2016-09-05T20:28:48.979540: step 6252, loss 0.00380081, acc 1
2016-09-05T20:28:49.807909: step 6253, loss 0.141142, acc 0.98
2016-09-05T20:28:50.619782: step 6254, loss 0.0939517, acc 0.96
2016-09-05T20:28:51.431510: step 6255, loss 0.00469748, acc 1
2016-09-05T20:28:52.231849: step 6256, loss 0.0229392, acc 0.98
2016-09-05T20:28:53.034223: step 6257, loss 0.0506064, acc 0.98
2016-09-05T20:28:53.852303: step 6258, loss 0.0388686, acc 0.98
2016-09-05T20:28:54.656825: step 6259, loss 0.0273718, acc 0.98
2016-09-05T20:28:55.473514: step 6260, loss 0.0038681, acc 1
2016-09-05T20:28:56.275622: step 6261, loss 0.00940572, acc 1
2016-09-05T20:28:57.066976: step 6262, loss 0.0677374, acc 0.98
2016-09-05T20:28:57.862458: step 6263, loss 0.0128898, acc 1
2016-09-05T20:28:58.691627: step 6264, loss 0.0394894, acc 1
2016-09-05T20:28:59.474998: step 6265, loss 0.0404895, acc 1
2016-09-05T20:29:00.271848: step 6266, loss 0.0136356, acc 1
2016-09-05T20:29:01.091394: step 6267, loss 0.00404744, acc 1
2016-09-05T20:29:01.865836: step 6268, loss 0.0359792, acc 0.98
2016-09-05T20:29:02.685497: step 6269, loss 0.0325333, acc 1
2016-09-05T20:29:03.514421: step 6270, loss 0.0293553, acc 1
2016-09-05T20:29:04.306788: step 6271, loss 0.0326939, acc 1
2016-09-05T20:29:05.094709: step 6272, loss 0.0602884, acc 0.96
2016-09-05T20:29:05.897921: step 6273, loss 0.0161081, acc 1
2016-09-05T20:29:06.719883: step 6274, loss 0.0662856, acc 0.98
2016-09-05T20:29:07.519279: step 6275, loss 0.105549, acc 0.98
2016-09-05T20:29:08.319619: step 6276, loss 0.00612804, acc 1
2016-09-05T20:29:09.133953: step 6277, loss 0.0167055, acc 1
2016-09-05T20:29:09.936303: step 6278, loss 0.0429191, acc 0.98
2016-09-05T20:29:10.746326: step 6279, loss 0.0038757, acc 1
2016-09-05T20:29:11.536302: step 6280, loss 0.0163424, acc 1
2016-09-05T20:29:12.379841: step 6281, loss 0.00356641, acc 1
2016-09-05T20:29:13.186257: step 6282, loss 0.0514574, acc 0.98
2016-09-05T20:29:13.972413: step 6283, loss 0.0318883, acc 0.98
2016-09-05T20:29:14.774395: step 6284, loss 0.049644, acc 0.96
2016-09-05T20:29:15.607502: step 6285, loss 0.0136061, acc 1
2016-09-05T20:29:16.398572: step 6286, loss 0.00888838, acc 1
2016-09-05T20:29:17.204597: step 6287, loss 0.0154883, acc 1
2016-09-05T20:29:18.007586: step 6288, loss 0.0494731, acc 0.96
2016-09-05T20:29:18.782941: step 6289, loss 0.0281408, acc 1
2016-09-05T20:29:19.600882: step 6290, loss 0.0161635, acc 1
2016-09-05T20:29:20.403516: step 6291, loss 0.0571998, acc 0.96
2016-09-05T20:29:21.237031: step 6292, loss 0.0202005, acc 0.98
2016-09-05T20:29:22.053391: step 6293, loss 0.0236474, acc 0.98
2016-09-05T20:29:22.887013: step 6294, loss 0.0246142, acc 1
2016-09-05T20:29:23.691505: step 6295, loss 0.0089395, acc 1
2016-09-05T20:29:24.477393: step 6296, loss 0.0423218, acc 0.96
2016-09-05T20:29:25.280176: step 6297, loss 0.011139, acc 1
2016-09-05T20:29:26.084356: step 6298, loss 0.108001, acc 0.96
2016-09-05T20:29:26.881498: step 6299, loss 0.0324683, acc 0.98
2016-09-05T20:29:27.690938: step 6300, loss 0.0717896, acc 0.94

Evaluation:
2016-09-05T20:29:31.207374: step 6300, loss 2.16781, acc 0.724

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-6300

2016-09-05T20:29:33.136803: step 6301, loss 0.00279011, acc 1
2016-09-05T20:29:34.024125: step 6302, loss 0.0133932, acc 1
2016-09-05T20:29:34.833792: step 6303, loss 0.0281581, acc 0.98
2016-09-05T20:29:35.642045: step 6304, loss 0.00304539, acc 1
2016-09-05T20:29:36.449031: step 6305, loss 0.0147219, acc 1
2016-09-05T20:29:37.249837: step 6306, loss 0.0457414, acc 1
2016-09-05T20:29:38.060310: step 6307, loss 0.137382, acc 0.98
2016-09-05T20:29:38.882359: step 6308, loss 0.0215323, acc 1
2016-09-05T20:29:39.709417: step 6309, loss 0.013276, acc 1
2016-09-05T20:29:40.525767: step 6310, loss 0.0316165, acc 0.98
2016-09-05T20:29:41.343316: step 6311, loss 0.0062572, acc 1
2016-09-05T20:29:42.192099: step 6312, loss 0.00764633, acc 1
2016-09-05T20:29:42.984601: step 6313, loss 0.00478763, acc 1
2016-09-05T20:29:43.832928: step 6314, loss 0.0158754, acc 1
2016-09-05T20:29:44.655489: step 6315, loss 0.0298911, acc 0.98
2016-09-05T20:29:45.449946: step 6316, loss 0.0517497, acc 0.98
2016-09-05T20:29:46.255568: step 6317, loss 0.0571571, acc 0.98
2016-09-05T20:29:47.063655: step 6318, loss 0.0379586, acc 0.98
2016-09-05T20:29:47.852847: step 6319, loss 0.0381418, acc 0.98
2016-09-05T20:29:48.692804: step 6320, loss 0.0201108, acc 0.98
2016-09-05T20:29:49.553345: step 6321, loss 0.0284738, acc 0.98
2016-09-05T20:29:50.323916: step 6322, loss 0.015233, acc 1
2016-09-05T20:29:51.148531: step 6323, loss 0.0168139, acc 1
2016-09-05T20:29:51.965259: step 6324, loss 0.0521478, acc 0.98
2016-09-05T20:29:52.734618: step 6325, loss 0.0402726, acc 0.96
2016-09-05T20:29:53.539857: step 6326, loss 0.0290982, acc 0.98
2016-09-05T20:29:54.344810: step 6327, loss 0.0142333, acc 1
2016-09-05T20:29:55.132246: step 6328, loss 0.0783079, acc 0.94
2016-09-05T20:29:55.937677: step 6329, loss 0.0183898, acc 0.98
2016-09-05T20:29:56.730831: step 6330, loss 0.008583, acc 1
2016-09-05T20:29:57.523657: step 6331, loss 0.00354215, acc 1
2016-09-05T20:29:58.338926: step 6332, loss 0.0302251, acc 0.98
2016-09-05T20:29:59.174726: step 6333, loss 0.0377276, acc 0.98
2016-09-05T20:29:59.982129: step 6334, loss 0.0185144, acc 1
2016-09-05T20:30:00.791078: step 6335, loss 0.0238813, acc 1
2016-09-05T20:30:01.605102: step 6336, loss 0.0183993, acc 1
2016-09-05T20:30:02.373930: step 6337, loss 0.0392769, acc 0.98
2016-09-05T20:30:03.193027: step 6338, loss 0.0341345, acc 0.98
2016-09-05T20:30:04.033404: step 6339, loss 0.0420313, acc 0.98
2016-09-05T20:30:04.799221: step 6340, loss 0.0205184, acc 1
2016-09-05T20:30:05.620668: step 6341, loss 0.0152748, acc 1
2016-09-05T20:30:06.429530: step 6342, loss 0.0280859, acc 0.98
2016-09-05T20:30:07.209556: step 6343, loss 0.0087758, acc 1
2016-09-05T20:30:08.009668: step 6344, loss 0.0157901, acc 1
2016-09-05T20:30:08.828661: step 6345, loss 0.0248953, acc 1
2016-09-05T20:30:09.621732: step 6346, loss 0.00304668, acc 1
2016-09-05T20:30:10.443047: step 6347, loss 0.0273212, acc 0.98
2016-09-05T20:30:11.258101: step 6348, loss 0.0268713, acc 0.98
2016-09-05T20:30:12.055981: step 6349, loss 0.00831679, acc 1
2016-09-05T20:30:12.851600: step 6350, loss 0.01708, acc 0.98
2016-09-05T20:30:13.664184: step 6351, loss 0.00400802, acc 1
2016-09-05T20:30:14.443892: step 6352, loss 0.0664584, acc 0.98
2016-09-05T20:30:15.260690: step 6353, loss 0.00497633, acc 1
2016-09-05T20:30:16.076812: step 6354, loss 0.00515553, acc 1
2016-09-05T20:30:16.874008: step 6355, loss 0.0302511, acc 0.98
2016-09-05T20:30:17.685729: step 6356, loss 0.021379, acc 1
2016-09-05T20:30:18.508189: step 6357, loss 0.0281488, acc 1
2016-09-05T20:30:19.285281: step 6358, loss 0.0278067, acc 0.98
2016-09-05T20:30:20.097106: step 6359, loss 0.0168055, acc 1
2016-09-05T20:30:20.888203: step 6360, loss 0.0774399, acc 0.96
2016-09-05T20:30:21.697856: step 6361, loss 0.0538093, acc 0.98
2016-09-05T20:30:22.513444: step 6362, loss 0.0125836, acc 1
2016-09-05T20:30:23.321142: step 6363, loss 0.0211737, acc 0.98
2016-09-05T20:30:24.117867: step 6364, loss 0.00293006, acc 1
2016-09-05T20:30:24.920088: step 6365, loss 0.00354609, acc 1
2016-09-05T20:30:25.748227: step 6366, loss 0.0534754, acc 0.98
2016-09-05T20:30:26.557011: step 6367, loss 0.0146771, acc 1
2016-09-05T20:30:27.358691: step 6368, loss 0.00866068, acc 1
2016-09-05T20:30:28.161017: step 6369, loss 0.00344111, acc 1
2016-09-05T20:30:28.932885: step 6370, loss 0.0226177, acc 1
2016-09-05T20:30:29.747841: step 6371, loss 0.0109865, acc 1
2016-09-05T20:30:30.584953: step 6372, loss 0.0788042, acc 0.98
2016-09-05T20:30:31.365718: step 6373, loss 0.0048675, acc 1
2016-09-05T20:30:32.190611: step 6374, loss 0.0416452, acc 0.98
2016-09-05T20:30:33.018724: step 6375, loss 0.00684786, acc 1
2016-09-05T20:30:33.796464: step 6376, loss 0.00430513, acc 1
2016-09-05T20:30:34.597609: step 6377, loss 0.00324232, acc 1
2016-09-05T20:30:35.414222: step 6378, loss 0.0364768, acc 1
2016-09-05T20:30:36.201421: step 6379, loss 0.0409156, acc 0.98
2016-09-05T20:30:37.003559: step 6380, loss 0.00926916, acc 1
2016-09-05T20:30:37.822851: step 6381, loss 0.0307471, acc 0.98
2016-09-05T20:30:38.607108: step 6382, loss 0.101731, acc 0.98
2016-09-05T20:30:39.423848: step 6383, loss 0.00929318, acc 1
2016-09-05T20:30:40.222027: step 6384, loss 0.0124066, acc 1
2016-09-05T20:30:41.017018: step 6385, loss 0.00396485, acc 1
2016-09-05T20:30:41.840887: step 6386, loss 0.00279369, acc 1
2016-09-05T20:30:42.642729: step 6387, loss 0.00345082, acc 1
2016-09-05T20:30:43.439457: step 6388, loss 0.00959215, acc 1
2016-09-05T20:30:44.241049: step 6389, loss 0.0176715, acc 1
2016-09-05T20:30:45.077474: step 6390, loss 0.0119368, acc 1
2016-09-05T20:30:45.868081: step 6391, loss 0.0161169, acc 1
2016-09-05T20:30:46.672048: step 6392, loss 0.00376671, acc 1
2016-09-05T20:30:47.504941: step 6393, loss 0.0557501, acc 0.98
2016-09-05T20:30:48.300179: step 6394, loss 0.16248, acc 0.94
2016-09-05T20:30:49.126006: step 6395, loss 0.0207934, acc 1
2016-09-05T20:30:49.922607: step 6396, loss 0.0053219, acc 1
2016-09-05T20:30:50.715677: step 6397, loss 0.0213252, acc 1
2016-09-05T20:30:51.510779: step 6398, loss 0.0521178, acc 0.98
2016-09-05T20:30:52.310216: step 6399, loss 0.0417404, acc 0.96
2016-09-05T20:30:53.087533: step 6400, loss 0.0417611, acc 0.96

Evaluation:
2016-09-05T20:30:56.578653: step 6400, loss 2.56993, acc 0.726

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-6400

2016-09-05T20:30:58.433402: step 6401, loss 0.0707474, acc 0.98
2016-09-05T20:30:58.884393: step 6402, loss 0.240707, acc 0.916667
2016-09-05T20:30:59.717050: step 6403, loss 0.0330679, acc 0.98
2016-09-05T20:31:00.557429: step 6404, loss 0.0843507, acc 0.96
2016-09-05T20:31:01.361330: step 6405, loss 0.0236937, acc 1
2016-09-05T20:31:02.173942: step 6406, loss 0.0247513, acc 1
2016-09-05T20:31:02.970774: step 6407, loss 0.0117498, acc 1
2016-09-05T20:31:03.761117: step 6408, loss 0.115706, acc 0.94
2016-09-05T20:31:04.571368: step 6409, loss 0.00776784, acc 1
2016-09-05T20:31:05.381002: step 6410, loss 0.0370861, acc 0.96
2016-09-05T20:31:06.211905: step 6411, loss 0.0348801, acc 0.98
2016-09-05T20:31:07.032763: step 6412, loss 0.00416258, acc 1
2016-09-05T20:31:07.857857: step 6413, loss 0.0785497, acc 0.96
2016-09-05T20:31:08.656709: step 6414, loss 0.0286557, acc 1
2016-09-05T20:31:09.468035: step 6415, loss 0.102532, acc 0.92
2016-09-05T20:31:10.294946: step 6416, loss 0.0315719, acc 0.98
2016-09-05T20:31:11.113370: step 6417, loss 0.0254449, acc 0.98
2016-09-05T20:31:11.957080: step 6418, loss 0.0064976, acc 1
2016-09-05T20:31:12.792956: step 6419, loss 0.0257548, acc 1
2016-09-05T20:31:13.610207: step 6420, loss 0.0165134, acc 1
2016-09-05T20:31:14.445688: step 6421, loss 0.0313042, acc 1
2016-09-05T20:31:15.267084: step 6422, loss 0.0411724, acc 0.98
2016-09-05T20:31:16.045618: step 6423, loss 0.06649, acc 0.96
2016-09-05T20:31:16.859818: step 6424, loss 0.0426863, acc 0.98
2016-09-05T20:31:17.668475: step 6425, loss 0.0094329, acc 1
2016-09-05T20:31:18.480636: step 6426, loss 0.00461973, acc 1
2016-09-05T20:31:19.315117: step 6427, loss 0.10945, acc 0.96
2016-09-05T20:31:20.120694: step 6428, loss 0.0355665, acc 0.98
2016-09-05T20:31:20.958545: step 6429, loss 0.00828377, acc 1
2016-09-05T20:31:21.770631: step 6430, loss 0.00624673, acc 1
2016-09-05T20:31:22.626821: step 6431, loss 0.0364743, acc 0.98
2016-09-05T20:31:23.442001: step 6432, loss 0.0176779, acc 1
2016-09-05T20:31:24.239347: step 6433, loss 0.022368, acc 1
2016-09-05T20:31:25.043572: step 6434, loss 0.0719196, acc 0.94
2016-09-05T20:31:25.821616: step 6435, loss 0.022849, acc 0.98
2016-09-05T20:31:26.619166: step 6436, loss 0.0911115, acc 0.96
2016-09-05T20:31:27.461682: step 6437, loss 0.021006, acc 0.98
2016-09-05T20:31:28.256378: step 6438, loss 0.0176175, acc 1
2016-09-05T20:31:29.033669: step 6439, loss 0.0163853, acc 1
2016-09-05T20:31:29.857326: step 6440, loss 0.189386, acc 0.96
2016-09-05T20:31:30.626423: step 6441, loss 0.00551263, acc 1
2016-09-05T20:31:31.414210: step 6442, loss 0.0100953, acc 1
2016-09-05T20:31:32.229835: step 6443, loss 0.0459672, acc 0.98
2016-09-05T20:31:33.020831: step 6444, loss 0.00780325, acc 1
2016-09-05T20:31:33.814043: step 6445, loss 0.00769383, acc 1
2016-09-05T20:31:34.595296: step 6446, loss 0.0277234, acc 0.98
2016-09-05T20:31:35.417248: step 6447, loss 0.0296457, acc 0.98
2016-09-05T20:31:36.224834: step 6448, loss 0.0290878, acc 1
2016-09-05T20:31:37.066094: step 6449, loss 0.00884144, acc 1
2016-09-05T20:31:37.867242: step 6450, loss 0.0407852, acc 0.98
2016-09-05T20:31:38.666065: step 6451, loss 0.0265257, acc 0.98
2016-09-05T20:31:39.477339: step 6452, loss 0.0269747, acc 1
2016-09-05T20:31:40.264868: step 6453, loss 0.0809925, acc 0.98
2016-09-05T20:31:41.087777: step 6454, loss 0.0112323, acc 1
2016-09-05T20:31:41.965542: step 6455, loss 0.0364904, acc 0.98
2016-09-05T20:31:42.731782: step 6456, loss 0.0179957, acc 1
2016-09-05T20:31:43.523948: step 6457, loss 0.0158565, acc 1
2016-09-05T20:31:44.353767: step 6458, loss 0.022004, acc 1
2016-09-05T20:31:45.124164: step 6459, loss 0.0580735, acc 0.98
2016-09-05T20:31:45.936441: step 6460, loss 0.00938085, acc 1
2016-09-05T20:31:46.760566: step 6461, loss 0.0186599, acc 1
2016-09-05T20:31:47.543676: step 6462, loss 0.0194008, acc 0.98
2016-09-05T20:31:48.330690: step 6463, loss 0.0400729, acc 0.98
2016-09-05T20:31:49.152173: step 6464, loss 0.0167442, acc 1
2016-09-05T20:31:49.939982: step 6465, loss 0.0154786, acc 1
2016-09-05T20:31:50.780609: step 6466, loss 0.0371313, acc 0.98
2016-09-05T20:31:51.616667: step 6467, loss 0.0202129, acc 0.98
2016-09-05T20:31:52.408315: step 6468, loss 0.0461471, acc 0.98
2016-09-05T20:31:53.222176: step 6469, loss 0.0570729, acc 0.96
2016-09-05T20:31:54.030670: step 6470, loss 0.0132092, acc 1
2016-09-05T20:31:54.802436: step 6471, loss 0.0488891, acc 0.98
2016-09-05T20:31:55.613996: step 6472, loss 0.0507227, acc 0.98
2016-09-05T20:31:56.428448: step 6473, loss 0.00980316, acc 1
2016-09-05T20:31:57.192694: step 6474, loss 0.0227636, acc 0.98
2016-09-05T20:31:58.008125: step 6475, loss 0.00668147, acc 1
2016-09-05T20:31:58.815987: step 6476, loss 0.00485208, acc 1
2016-09-05T20:31:59.572855: step 6477, loss 0.012912, acc 1
2016-09-05T20:32:00.417391: step 6478, loss 0.0260351, acc 0.98
2016-09-05T20:32:01.215448: step 6479, loss 0.0399188, acc 0.98
2016-09-05T20:32:02.015756: step 6480, loss 0.0686275, acc 0.98
2016-09-05T20:32:02.833937: step 6481, loss 0.012497, acc 1
2016-09-05T20:32:03.640250: step 6482, loss 0.020211, acc 1
2016-09-05T20:32:04.417845: step 6483, loss 0.0440674, acc 0.96
2016-09-05T20:32:05.252057: step 6484, loss 0.0178914, acc 1
2016-09-05T20:32:06.065654: step 6485, loss 0.0449017, acc 0.96
2016-09-05T20:32:06.852655: step 6486, loss 0.0131048, acc 1
2016-09-05T20:32:07.636048: step 6487, loss 0.041366, acc 0.98
2016-09-05T20:32:08.433475: step 6488, loss 0.00394461, acc 1
2016-09-05T20:32:09.269541: step 6489, loss 0.00470917, acc 1
2016-09-05T20:32:10.084640: step 6490, loss 0.0204079, acc 0.98
2016-09-05T20:32:10.894461: step 6491, loss 0.0130087, acc 1
2016-09-05T20:32:11.672230: step 6492, loss 0.0457558, acc 1
2016-09-05T20:32:12.487974: step 6493, loss 0.0403229, acc 1
2016-09-05T20:32:13.342822: step 6494, loss 0.0231543, acc 0.98
2016-09-05T20:32:14.132281: step 6495, loss 0.00541502, acc 1
2016-09-05T20:32:14.956116: step 6496, loss 0.0215784, acc 1
2016-09-05T20:32:15.795124: step 6497, loss 0.0417119, acc 0.98
2016-09-05T20:32:16.593048: step 6498, loss 0.0050247, acc 1
2016-09-05T20:32:17.367445: step 6499, loss 0.0200975, acc 1
2016-09-05T20:32:18.152422: step 6500, loss 0.0236435, acc 0.98

Evaluation:
2016-09-05T20:32:21.635657: step 6500, loss 2.47308, acc 0.741

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-6500

2016-09-05T20:32:23.518976: step 6501, loss 0.0127132, acc 1
2016-09-05T20:32:24.319594: step 6502, loss 0.0195411, acc 1
2016-09-05T20:32:25.106832: step 6503, loss 0.0233724, acc 0.98
2016-09-05T20:32:25.909642: step 6504, loss 0.00484885, acc 1
2016-09-05T20:32:26.708866: step 6505, loss 0.141591, acc 0.98
2016-09-05T20:32:27.522909: step 6506, loss 0.00829868, acc 1
2016-09-05T20:32:28.342009: step 6507, loss 0.0174019, acc 1
2016-09-05T20:32:29.168964: step 6508, loss 0.0662024, acc 0.98
2016-09-05T20:32:29.973893: step 6509, loss 0.0369618, acc 0.96
2016-09-05T20:32:30.778049: step 6510, loss 0.0155978, acc 1
2016-09-05T20:32:31.616278: step 6511, loss 0.0490169, acc 0.98
2016-09-05T20:32:32.406805: step 6512, loss 0.037064, acc 0.98
2016-09-05T20:32:33.203445: step 6513, loss 0.0702387, acc 0.96
2016-09-05T20:32:34.013212: step 6514, loss 0.00598862, acc 1
2016-09-05T20:32:34.799782: step 6515, loss 0.0353096, acc 0.98
2016-09-05T20:32:35.601894: step 6516, loss 0.144355, acc 0.96
2016-09-05T20:32:36.435553: step 6517, loss 0.00683399, acc 1
2016-09-05T20:32:37.239993: step 6518, loss 0.0731266, acc 0.96
2016-09-05T20:32:38.035661: step 6519, loss 0.0157268, acc 1
2016-09-05T20:32:38.853911: step 6520, loss 0.0424658, acc 0.98
2016-09-05T20:32:39.644735: step 6521, loss 0.0233197, acc 1
2016-09-05T20:32:40.470801: step 6522, loss 0.0434364, acc 0.96
2016-09-05T20:32:41.285416: step 6523, loss 0.00894853, acc 1
2016-09-05T20:32:42.058012: step 6524, loss 0.0387797, acc 0.98
2016-09-05T20:32:42.869806: step 6525, loss 0.0108848, acc 1
2016-09-05T20:32:43.693996: step 6526, loss 0.00721852, acc 1
2016-09-05T20:32:44.479726: step 6527, loss 0.0108733, acc 1
2016-09-05T20:32:45.283743: step 6528, loss 0.0144826, acc 1
2016-09-05T20:32:46.117466: step 6529, loss 0.0246377, acc 1
2016-09-05T20:32:46.899120: step 6530, loss 0.0747586, acc 0.98
2016-09-05T20:32:47.711955: step 6531, loss 0.00938069, acc 1
2016-09-05T20:32:48.554222: step 6532, loss 0.0161424, acc 1
2016-09-05T20:32:49.335782: step 6533, loss 0.0297938, acc 0.98
2016-09-05T20:32:50.123145: step 6534, loss 0.0295039, acc 0.98
2016-09-05T20:32:50.927876: step 6535, loss 0.00801587, acc 1
2016-09-05T20:32:51.739290: step 6536, loss 0.0278214, acc 0.98
2016-09-05T20:32:52.540730: step 6537, loss 0.170638, acc 0.96
2016-09-05T20:32:53.379013: step 6538, loss 0.039936, acc 0.98
2016-09-05T20:32:54.152077: step 6539, loss 0.0130175, acc 1
2016-09-05T20:32:54.958316: step 6540, loss 0.0475553, acc 0.98
2016-09-05T20:32:55.777536: step 6541, loss 0.0255683, acc 0.98
2016-09-05T20:32:56.573561: step 6542, loss 0.05903, acc 0.98
2016-09-05T20:32:57.377061: step 6543, loss 0.0322216, acc 1
2016-09-05T20:32:58.166572: step 6544, loss 0.0403457, acc 1
2016-09-05T20:32:58.962063: step 6545, loss 0.0340095, acc 0.98
2016-09-05T20:32:59.768965: step 6546, loss 0.0162859, acc 1
2016-09-05T20:33:00.596536: step 6547, loss 0.0445083, acc 0.96
2016-09-05T20:33:01.399961: step 6548, loss 0.0249503, acc 1
2016-09-05T20:33:02.251739: step 6549, loss 0.0280344, acc 1
2016-09-05T20:33:03.085050: step 6550, loss 0.0207837, acc 1
2016-09-05T20:33:03.892549: step 6551, loss 0.053556, acc 0.96
2016-09-05T20:33:04.708336: step 6552, loss 0.0582531, acc 0.96
2016-09-05T20:33:05.522070: step 6553, loss 0.0520785, acc 0.98
2016-09-05T20:33:06.353003: step 6554, loss 0.0396281, acc 1
2016-09-05T20:33:07.186240: step 6555, loss 0.0444387, acc 0.96
2016-09-05T20:33:08.004357: step 6556, loss 0.0649274, acc 0.96
2016-09-05T20:33:08.808021: step 6557, loss 0.071597, acc 0.96
2016-09-05T20:33:09.614613: step 6558, loss 0.0125247, acc 1
2016-09-05T20:33:10.447937: step 6559, loss 0.00367413, acc 1
2016-09-05T20:33:11.267728: step 6560, loss 0.0521609, acc 0.98
2016-09-05T20:33:12.083098: step 6561, loss 0.0393561, acc 0.98
2016-09-05T20:33:12.927563: step 6562, loss 0.0226171, acc 0.98
2016-09-05T20:33:13.745193: step 6563, loss 0.00425879, acc 1
2016-09-05T20:33:14.533420: step 6564, loss 0.00275898, acc 1
2016-09-05T20:33:15.359502: step 6565, loss 0.0202474, acc 1
2016-09-05T20:33:16.190757: step 6566, loss 0.0496408, acc 0.98
2016-09-05T20:33:17.018121: step 6567, loss 0.0316896, acc 0.98
2016-09-05T20:33:17.862228: step 6568, loss 0.0189999, acc 0.98
2016-09-05T20:33:18.705402: step 6569, loss 0.0304001, acc 0.98
2016-09-05T20:33:19.519590: step 6570, loss 0.0397228, acc 0.96
2016-09-05T20:33:20.340685: step 6571, loss 0.0160446, acc 1
2016-09-05T20:33:21.138752: step 6572, loss 0.153007, acc 0.94
2016-09-05T20:33:21.902058: step 6573, loss 0.021072, acc 0.98
2016-09-05T20:33:22.722929: step 6574, loss 0.0219689, acc 0.98
2016-09-05T20:33:23.518829: step 6575, loss 0.0132847, acc 1
2016-09-05T20:33:24.330317: step 6576, loss 0.0176243, acc 1
2016-09-05T20:33:25.175776: step 6577, loss 0.102454, acc 0.96
2016-09-05T20:33:25.989722: step 6578, loss 0.108839, acc 0.98
2016-09-05T20:33:26.779279: step 6579, loss 0.00741237, acc 1
2016-09-05T20:33:27.580015: step 6580, loss 0.0428308, acc 0.98
2016-09-05T20:33:28.389548: step 6581, loss 0.0179767, acc 0.98
2016-09-05T20:33:29.193640: step 6582, loss 0.0284606, acc 0.98
2016-09-05T20:33:30.006491: step 6583, loss 0.049769, acc 0.96
2016-09-05T20:33:30.815893: step 6584, loss 0.0169749, acc 1
2016-09-05T20:33:31.622137: step 6585, loss 0.0109642, acc 1
2016-09-05T20:33:32.439398: step 6586, loss 0.0230501, acc 0.98
2016-09-05T20:33:33.240605: step 6587, loss 0.0274112, acc 0.98
2016-09-05T20:33:34.018739: step 6588, loss 0.0157819, acc 1
2016-09-05T20:33:34.807849: step 6589, loss 0.0638817, acc 0.96
2016-09-05T20:33:35.614809: step 6590, loss 0.0177045, acc 1
2016-09-05T20:33:36.410773: step 6591, loss 0.0741165, acc 0.96
2016-09-05T20:33:37.227726: step 6592, loss 0.0865133, acc 0.96
2016-09-05T20:33:38.039213: step 6593, loss 0.0247374, acc 1
2016-09-05T20:33:38.827059: step 6594, loss 0.0287341, acc 0.98
2016-09-05T20:33:39.660610: step 6595, loss 0.0127378, acc 1
2016-09-05T20:33:40.090521: step 6596, loss 0.00305252, acc 1
2016-09-05T20:33:40.888094: step 6597, loss 0.0201399, acc 1
2016-09-05T20:33:41.723471: step 6598, loss 0.0355996, acc 0.98
2016-09-05T20:33:42.511593: step 6599, loss 0.101601, acc 0.96
2016-09-05T20:33:43.329834: step 6600, loss 0.0801354, acc 0.98

Evaluation:
2016-09-05T20:33:46.849018: step 6600, loss 1.6264, acc 0.731

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-6600

2016-09-05T20:33:48.847272: step 6601, loss 0.00528582, acc 1
2016-09-05T20:33:49.665633: step 6602, loss 0.0232703, acc 1
2016-09-05T20:33:50.477614: step 6603, loss 0.0167016, acc 1
2016-09-05T20:33:51.332895: step 6604, loss 0.107768, acc 0.98
2016-09-05T20:33:52.171120: step 6605, loss 0.0112153, acc 1
2016-09-05T20:33:53.026151: step 6606, loss 0.0111292, acc 1
2016-09-05T20:33:53.840720: step 6607, loss 0.0571282, acc 0.96
2016-09-05T20:33:54.633474: step 6608, loss 0.0409782, acc 0.98
2016-09-05T20:33:55.450242: step 6609, loss 0.0264101, acc 0.98
2016-09-05T20:33:56.241626: step 6610, loss 0.00790689, acc 1
2016-09-05T20:33:57.053076: step 6611, loss 0.00494782, acc 1
2016-09-05T20:33:57.876341: step 6612, loss 0.00419708, acc 1
2016-09-05T20:33:58.685245: step 6613, loss 0.0566288, acc 0.98
2016-09-05T20:33:59.478541: step 6614, loss 0.0240577, acc 0.98
2016-09-05T20:34:00.327604: step 6615, loss 0.0164028, acc 1
2016-09-05T20:34:01.142228: step 6616, loss 0.0265374, acc 0.98
2016-09-05T20:34:01.934978: step 6617, loss 0.0204034, acc 0.98
2016-09-05T20:34:02.738907: step 6618, loss 0.021805, acc 0.98
2016-09-05T20:34:03.540878: step 6619, loss 0.0639895, acc 0.96
2016-09-05T20:34:04.332430: step 6620, loss 0.00540204, acc 1
2016-09-05T20:34:05.153529: step 6621, loss 0.0753902, acc 0.98
2016-09-05T20:34:05.979986: step 6622, loss 0.0200171, acc 0.98
2016-09-05T20:34:06.764376: step 6623, loss 0.0646792, acc 0.98
2016-09-05T20:34:07.576207: step 6624, loss 0.0724999, acc 0.98
2016-09-05T20:34:08.377654: step 6625, loss 0.023026, acc 0.98
2016-09-05T20:34:09.196144: step 6626, loss 0.0154412, acc 1
2016-09-05T20:34:09.998899: step 6627, loss 0.00645753, acc 1
2016-09-05T20:34:10.813612: step 6628, loss 0.0577273, acc 0.98
2016-09-05T20:34:11.588562: step 6629, loss 0.0234141, acc 1
2016-09-05T20:34:12.435604: step 6630, loss 0.0273573, acc 0.98
2016-09-05T20:34:13.256369: step 6631, loss 0.0319627, acc 0.98
2016-09-05T20:34:14.038853: step 6632, loss 0.01276, acc 1
2016-09-05T20:34:14.860452: step 6633, loss 0.0312025, acc 0.98
2016-09-05T20:34:15.695875: step 6634, loss 0.0392782, acc 0.98
2016-09-05T20:34:16.487872: step 6635, loss 0.0277285, acc 0.98
2016-09-05T20:34:17.301446: step 6636, loss 0.0394573, acc 0.98
2016-09-05T20:34:18.119539: step 6637, loss 0.0871503, acc 0.96
2016-09-05T20:34:18.890603: step 6638, loss 0.025611, acc 0.98
2016-09-05T20:34:19.713969: step 6639, loss 0.0308795, acc 0.98
2016-09-05T20:34:20.525400: step 6640, loss 0.0484394, acc 0.96
2016-09-05T20:34:21.312277: step 6641, loss 0.0196186, acc 0.98
2016-09-05T20:34:22.111776: step 6642, loss 0.0170641, acc 1
2016-09-05T20:34:22.926071: step 6643, loss 0.0115257, acc 1
2016-09-05T20:34:23.707666: step 6644, loss 0.0178721, acc 1
2016-09-05T20:34:24.503345: step 6645, loss 0.00344645, acc 1
2016-09-05T20:34:25.308225: step 6646, loss 0.0259578, acc 1
2016-09-05T20:34:26.088190: step 6647, loss 0.0143115, acc 1
2016-09-05T20:34:26.897366: step 6648, loss 0.0173027, acc 1
2016-09-05T20:34:27.713619: step 6649, loss 0.0035415, acc 1
2016-09-05T20:34:28.529401: step 6650, loss 0.00343443, acc 1
2016-09-05T20:34:29.349184: step 6651, loss 0.01465, acc 1
2016-09-05T20:34:30.176569: step 6652, loss 0.0272893, acc 1
2016-09-05T20:34:30.967045: step 6653, loss 0.0262837, acc 0.98
2016-09-05T20:34:31.749662: step 6654, loss 0.00898978, acc 1
2016-09-05T20:34:32.541852: step 6655, loss 0.0170191, acc 1
2016-09-05T20:34:33.349816: step 6656, loss 0.00534274, acc 1
2016-09-05T20:34:34.151869: step 6657, loss 0.0168778, acc 1
2016-09-05T20:34:34.957817: step 6658, loss 0.0224536, acc 1
2016-09-05T20:34:35.747472: step 6659, loss 0.0209538, acc 0.98
2016-09-05T20:34:36.593869: step 6660, loss 0.0182544, acc 1
2016-09-05T20:34:37.430484: step 6661, loss 0.0196322, acc 1
2016-09-05T20:34:38.237102: step 6662, loss 0.0647129, acc 0.96
2016-09-05T20:34:39.035625: step 6663, loss 0.0423376, acc 0.98
2016-09-05T20:34:39.849182: step 6664, loss 0.00337083, acc 1
2016-09-05T20:34:40.642711: step 6665, loss 0.0183151, acc 0.98
2016-09-05T20:34:41.444904: step 6666, loss 0.0475315, acc 0.98
2016-09-05T20:34:42.260812: step 6667, loss 0.00585783, acc 1
2016-09-05T20:34:43.062397: step 6668, loss 0.0441823, acc 0.98
2016-09-05T20:34:43.867306: step 6669, loss 0.00318555, acc 1
2016-09-05T20:34:44.676543: step 6670, loss 0.0041059, acc 1
2016-09-05T20:34:45.478821: step 6671, loss 0.00507392, acc 1
2016-09-05T20:34:46.277069: step 6672, loss 0.064869, acc 0.96
2016-09-05T20:34:47.089608: step 6673, loss 0.0249736, acc 0.98
2016-09-05T20:34:47.877089: step 6674, loss 0.00519369, acc 1
2016-09-05T20:34:48.691032: step 6675, loss 0.0178129, acc 0.98
2016-09-05T20:34:49.511564: step 6676, loss 0.00676506, acc 1
2016-09-05T20:34:50.309631: step 6677, loss 0.0304832, acc 0.98
2016-09-05T20:34:51.114674: step 6678, loss 0.0238598, acc 1
2016-09-05T20:34:51.933606: step 6679, loss 0.0178629, acc 1
2016-09-05T20:34:52.734866: step 6680, loss 0.00348701, acc 1
2016-09-05T20:34:53.534101: step 6681, loss 0.00760013, acc 1
2016-09-05T20:34:54.351352: step 6682, loss 0.031831, acc 0.98
2016-09-05T20:34:55.153114: step 6683, loss 0.0051254, acc 1
2016-09-05T20:34:56.024747: step 6684, loss 0.0155312, acc 1
2016-09-05T20:34:56.843593: step 6685, loss 0.00300743, acc 1
2016-09-05T20:34:57.630318: step 6686, loss 0.056598, acc 0.96
2016-09-05T20:34:58.477351: step 6687, loss 0.0150867, acc 1
2016-09-05T20:34:59.293884: step 6688, loss 0.0256483, acc 1
2016-09-05T20:35:00.097815: step 6689, loss 0.0114363, acc 1
2016-09-05T20:35:00.919806: step 6690, loss 0.0119198, acc 1
2016-09-05T20:35:01.754859: step 6691, loss 0.0334417, acc 0.98
2016-09-05T20:35:02.569360: step 6692, loss 0.0110195, acc 1
2016-09-05T20:35:03.389554: step 6693, loss 0.00462777, acc 1
2016-09-05T20:35:04.248284: step 6694, loss 0.0390155, acc 0.98
2016-09-05T20:35:05.090463: step 6695, loss 0.00960862, acc 1
2016-09-05T20:35:05.897112: step 6696, loss 0.00736986, acc 1
2016-09-05T20:35:06.712059: step 6697, loss 0.0106247, acc 1
2016-09-05T20:35:07.524308: step 6698, loss 0.012566, acc 1
2016-09-05T20:35:08.324140: step 6699, loss 0.023845, acc 1
2016-09-05T20:35:09.135731: step 6700, loss 0.0158497, acc 1

Evaluation:
2016-09-05T20:35:12.653995: step 6700, loss 2.38097, acc 0.741

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-6700

2016-09-05T20:35:14.578296: step 6701, loss 0.0686251, acc 0.96
2016-09-05T20:35:15.372612: step 6702, loss 0.00369084, acc 1
2016-09-05T20:35:16.198781: step 6703, loss 0.0403968, acc 0.98
2016-09-05T20:35:17.007025: step 6704, loss 0.0176233, acc 1
2016-09-05T20:35:17.809133: step 6705, loss 0.00534389, acc 1
2016-09-05T20:35:18.618958: step 6706, loss 0.0524836, acc 0.98
2016-09-05T20:35:19.422223: step 6707, loss 0.0175838, acc 1
2016-09-05T20:35:20.234310: step 6708, loss 0.00309335, acc 1
2016-09-05T20:35:21.060255: step 6709, loss 0.0354632, acc 0.98
2016-09-05T20:35:21.864330: step 6710, loss 0.0156365, acc 1
2016-09-05T20:35:22.658953: step 6711, loss 0.011574, acc 1
2016-09-05T20:35:23.482772: step 6712, loss 0.0559339, acc 0.94
2016-09-05T20:35:24.292262: step 6713, loss 0.00321164, acc 1
2016-09-05T20:35:25.100237: step 6714, loss 0.0274249, acc 1
2016-09-05T20:35:25.952537: step 6715, loss 0.0068906, acc 1
2016-09-05T20:35:26.754616: step 6716, loss 0.014282, acc 1
2016-09-05T20:35:27.532111: step 6717, loss 0.0283512, acc 0.98
2016-09-05T20:35:28.367834: step 6718, loss 0.00603522, acc 1
2016-09-05T20:35:29.189162: step 6719, loss 0.00476557, acc 1
2016-09-05T20:35:30.003059: step 6720, loss 0.0136544, acc 1
2016-09-05T20:35:30.825643: step 6721, loss 0.0405439, acc 0.98
2016-09-05T20:35:31.620890: step 6722, loss 0.00308953, acc 1
2016-09-05T20:35:32.437149: step 6723, loss 0.00920824, acc 1
2016-09-05T20:35:33.276418: step 6724, loss 0.00436105, acc 1
2016-09-05T20:35:34.105135: step 6725, loss 0.00311734, acc 1
2016-09-05T20:35:34.927194: step 6726, loss 0.0491097, acc 0.96
2016-09-05T20:35:35.820010: step 6727, loss 0.0284549, acc 0.98
2016-09-05T20:35:36.665479: step 6728, loss 0.014763, acc 1
2016-09-05T20:35:37.493437: step 6729, loss 0.00433988, acc 1
2016-09-05T20:35:38.307141: step 6730, loss 0.0391468, acc 0.96
2016-09-05T20:35:39.140193: step 6731, loss 0.145354, acc 0.96
2016-09-05T20:35:39.947906: step 6732, loss 0.0222714, acc 0.98
2016-09-05T20:35:40.742767: step 6733, loss 0.0355482, acc 0.98
2016-09-05T20:35:41.552220: step 6734, loss 0.00270701, acc 1
2016-09-05T20:35:42.325987: step 6735, loss 0.0363494, acc 0.98
2016-09-05T20:35:43.113918: step 6736, loss 0.152905, acc 0.96
2016-09-05T20:35:43.931662: step 6737, loss 0.0424598, acc 0.98
2016-09-05T20:35:44.717160: step 6738, loss 0.00981344, acc 1
2016-09-05T20:35:45.520814: step 6739, loss 0.00790103, acc 1
2016-09-05T20:35:46.337320: step 6740, loss 0.00274313, acc 1
2016-09-05T20:35:47.122934: step 6741, loss 0.0250106, acc 0.98
2016-09-05T20:35:47.931972: step 6742, loss 0.0251171, acc 0.98
2016-09-05T20:35:48.753289: step 6743, loss 0.034172, acc 0.98
2016-09-05T20:35:49.529449: step 6744, loss 0.0334382, acc 0.98
2016-09-05T20:35:50.349826: step 6745, loss 0.00783595, acc 1
2016-09-05T20:35:51.168423: step 6746, loss 0.0114279, acc 1
2016-09-05T20:35:51.943559: step 6747, loss 0.0485232, acc 0.98
2016-09-05T20:35:52.785760: step 6748, loss 0.0201793, acc 0.98
2016-09-05T20:35:53.626575: step 6749, loss 0.0530724, acc 0.96
2016-09-05T20:35:54.444737: step 6750, loss 0.037943, acc 0.98
2016-09-05T20:35:55.246220: step 6751, loss 0.0337923, acc 1
2016-09-05T20:35:56.068770: step 6752, loss 0.00737363, acc 1
2016-09-05T20:35:56.863021: step 6753, loss 0.00670494, acc 1
2016-09-05T20:35:57.669194: step 6754, loss 0.00600894, acc 1
2016-09-05T20:35:58.483213: step 6755, loss 0.00708722, acc 1
2016-09-05T20:35:59.267019: step 6756, loss 0.0178705, acc 1
2016-09-05T20:36:00.041770: step 6757, loss 0.0480724, acc 0.96
2016-09-05T20:36:00.917995: step 6758, loss 0.0387129, acc 0.98
2016-09-05T20:36:01.735607: step 6759, loss 0.06479, acc 0.96
2016-09-05T20:36:02.529787: step 6760, loss 0.00958756, acc 1
2016-09-05T20:36:03.345557: step 6761, loss 0.0117064, acc 1
2016-09-05T20:36:04.146692: step 6762, loss 0.100761, acc 0.98
2016-09-05T20:36:04.952404: step 6763, loss 0.0554295, acc 0.96
2016-09-05T20:36:05.776055: step 6764, loss 0.0227836, acc 1
2016-09-05T20:36:06.581391: step 6765, loss 0.0373346, acc 0.98
2016-09-05T20:36:07.385411: step 6766, loss 0.0764136, acc 0.96
2016-09-05T20:36:08.204905: step 6767, loss 0.0292511, acc 1
2016-09-05T20:36:09.033499: step 6768, loss 0.0271572, acc 1
2016-09-05T20:36:09.857284: step 6769, loss 0.010853, acc 1
2016-09-05T20:36:10.674481: step 6770, loss 0.0171517, acc 1
2016-09-05T20:36:11.478515: step 6771, loss 0.0289576, acc 0.98
2016-09-05T20:36:12.288435: step 6772, loss 0.00368268, acc 1
2016-09-05T20:36:13.123327: step 6773, loss 0.00310987, acc 1
2016-09-05T20:36:13.955886: step 6774, loss 0.00380246, acc 1
2016-09-05T20:36:14.761062: step 6775, loss 0.040154, acc 0.98
2016-09-05T20:36:15.594953: step 6776, loss 0.0305061, acc 1
2016-09-05T20:36:16.426676: step 6777, loss 0.0261278, acc 0.98
2016-09-05T20:36:17.248211: step 6778, loss 0.0208533, acc 0.98
2016-09-05T20:36:18.065180: step 6779, loss 0.0108903, acc 1
2016-09-05T20:36:18.879395: step 6780, loss 0.00777956, acc 1
2016-09-05T20:36:19.722736: step 6781, loss 0.0454207, acc 0.98
2016-09-05T20:36:20.531081: step 6782, loss 0.0311338, acc 0.98
2016-09-05T20:36:21.312866: step 6783, loss 0.0412403, acc 0.96
2016-09-05T20:36:22.137355: step 6784, loss 0.085833, acc 0.98
2016-09-05T20:36:22.994599: step 6785, loss 0.0119467, acc 1
2016-09-05T20:36:23.814243: step 6786, loss 0.0324275, acc 0.96
2016-09-05T20:36:24.629205: step 6787, loss 0.0333709, acc 1
2016-09-05T20:36:25.441747: step 6788, loss 0.0111156, acc 1
2016-09-05T20:36:26.268652: step 6789, loss 0.00345993, acc 1
2016-09-05T20:36:26.675076: step 6790, loss 0.0879041, acc 0.916667
2016-09-05T20:36:27.469219: step 6791, loss 0.0129571, acc 1
2016-09-05T20:36:28.256704: step 6792, loss 0.0146013, acc 1
2016-09-05T20:36:29.076015: step 6793, loss 0.031687, acc 0.98
2016-09-05T20:36:29.889558: step 6794, loss 0.0373944, acc 0.98
2016-09-05T20:36:30.672500: step 6795, loss 0.0217994, acc 1
2016-09-05T20:36:31.507485: step 6796, loss 0.0288968, acc 1
2016-09-05T20:36:32.318421: step 6797, loss 0.0113093, acc 1
2016-09-05T20:36:33.120181: step 6798, loss 0.0310613, acc 0.98
2016-09-05T20:36:33.903567: step 6799, loss 0.0304661, acc 0.98
2016-09-05T20:36:34.705407: step 6800, loss 0.00447901, acc 1

Evaluation:
2016-09-05T20:36:38.209444: step 6800, loss 2.67425, acc 0.741

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-6800

2016-09-05T20:36:40.018054: step 6801, loss 0.013534, acc 1
2016-09-05T20:36:40.852242: step 6802, loss 0.00681408, acc 1
2016-09-05T20:36:41.677364: step 6803, loss 0.0401359, acc 1
2016-09-05T20:36:42.492454: step 6804, loss 0.0493797, acc 0.98
2016-09-05T20:36:43.334826: step 6805, loss 0.0178121, acc 1
2016-09-05T20:36:44.179972: step 6806, loss 0.019788, acc 0.98
2016-09-05T20:36:44.991845: step 6807, loss 0.00952813, acc 1
2016-09-05T20:36:45.838279: step 6808, loss 0.0193711, acc 0.98
2016-09-05T20:36:46.635242: step 6809, loss 0.0292327, acc 0.98
2016-09-05T20:36:47.432392: step 6810, loss 0.0251307, acc 0.98
2016-09-05T20:36:48.249257: step 6811, loss 0.004113, acc 1
2016-09-05T20:36:49.056475: step 6812, loss 0.00805474, acc 1
2016-09-05T20:36:49.889837: step 6813, loss 0.0263876, acc 0.98
2016-09-05T20:36:50.712597: step 6814, loss 0.0988829, acc 0.96
2016-09-05T20:36:51.514928: step 6815, loss 0.00408668, acc 1
2016-09-05T20:36:52.315528: step 6816, loss 0.00448759, acc 1
2016-09-05T20:36:53.159437: step 6817, loss 0.00346743, acc 1
2016-09-05T20:36:53.987968: step 6818, loss 0.00365581, acc 1
2016-09-05T20:36:54.781857: step 6819, loss 0.145277, acc 0.98
2016-09-05T20:36:55.604440: step 6820, loss 0.00871693, acc 1
2016-09-05T20:36:56.401958: step 6821, loss 0.0153061, acc 1
2016-09-05T20:36:57.215301: step 6822, loss 0.0409861, acc 0.98
2016-09-05T20:36:58.045346: step 6823, loss 0.00825542, acc 1
2016-09-05T20:36:58.845393: step 6824, loss 0.0493372, acc 0.98
2016-09-05T20:36:59.646085: step 6825, loss 0.0335804, acc 0.98
2016-09-05T20:37:00.467682: step 6826, loss 0.0343235, acc 0.98
2016-09-05T20:37:01.294240: step 6827, loss 0.00927005, acc 1
2016-09-05T20:37:02.115053: step 6828, loss 0.0172103, acc 1
2016-09-05T20:37:02.943971: step 6829, loss 0.181113, acc 0.94
2016-09-05T20:37:03.784588: step 6830, loss 0.0190712, acc 0.98
2016-09-05T20:37:04.584296: step 6831, loss 0.0523908, acc 0.96
2016-09-05T20:37:05.376144: step 6832, loss 0.0238704, acc 0.98
2016-09-05T20:37:06.174460: step 6833, loss 0.0186226, acc 1
2016-09-05T20:37:06.940782: step 6834, loss 0.00541728, acc 1
2016-09-05T20:37:07.729221: step 6835, loss 0.00542125, acc 1
2016-09-05T20:37:08.538111: step 6836, loss 0.0226761, acc 1
2016-09-05T20:37:09.324862: step 6837, loss 0.0974276, acc 0.96
2016-09-05T20:37:10.133764: step 6838, loss 0.0726115, acc 0.98
2016-09-05T20:37:10.952069: step 6839, loss 0.017364, acc 1
2016-09-05T20:37:11.742546: step 6840, loss 0.095228, acc 0.96
2016-09-05T20:37:12.536466: step 6841, loss 0.01072, acc 1
2016-09-05T20:37:13.367587: step 6842, loss 0.0214807, acc 1
2016-09-05T20:37:14.176392: step 6843, loss 0.0186509, acc 1
2016-09-05T20:37:14.978894: step 6844, loss 0.0199408, acc 1
2016-09-05T20:37:15.826858: step 6845, loss 0.0414163, acc 0.98
2016-09-05T20:37:16.623429: step 6846, loss 0.0601661, acc 0.96
2016-09-05T20:37:17.447478: step 6847, loss 0.0873892, acc 0.98
2016-09-05T20:37:18.274928: step 6848, loss 0.00401934, acc 1
2016-09-05T20:37:19.068688: step 6849, loss 0.0209889, acc 1
2016-09-05T20:37:19.873333: step 6850, loss 0.00890771, acc 1
2016-09-05T20:37:20.684782: step 6851, loss 0.0188943, acc 1
2016-09-05T20:37:21.465400: step 6852, loss 0.0379223, acc 1
2016-09-05T20:37:22.269083: step 6853, loss 0.0933537, acc 0.98
2016-09-05T20:37:23.064101: step 6854, loss 0.0347626, acc 0.98
2016-09-05T20:37:23.864194: step 6855, loss 0.0880869, acc 0.98
2016-09-05T20:37:24.653981: step 6856, loss 0.0178615, acc 1
2016-09-05T20:37:25.468350: step 6857, loss 0.0130147, acc 1
2016-09-05T20:37:26.253822: step 6858, loss 0.0308998, acc 0.98
2016-09-05T20:37:27.058472: step 6859, loss 0.00328276, acc 1
2016-09-05T20:37:27.873276: step 6860, loss 0.0511063, acc 0.96
2016-09-05T20:37:28.657466: step 6861, loss 0.0175568, acc 0.98
2016-09-05T20:37:29.487375: step 6862, loss 0.00454119, acc 1
2016-09-05T20:37:30.300794: step 6863, loss 0.00689229, acc 1
2016-09-05T20:37:31.106045: step 6864, loss 0.00752847, acc 1
2016-09-05T20:37:31.926764: step 6865, loss 0.00940952, acc 1
2016-09-05T20:37:32.726088: step 6866, loss 0.0254809, acc 1
2016-09-05T20:37:33.528047: step 6867, loss 0.0383342, acc 0.96
2016-09-05T20:37:34.364761: step 6868, loss 0.00317458, acc 1
2016-09-05T20:37:35.148864: step 6869, loss 0.0345284, acc 0.98
2016-09-05T20:37:35.939917: step 6870, loss 0.0734974, acc 0.96
2016-09-05T20:37:36.750279: step 6871, loss 0.0253214, acc 0.98
2016-09-05T20:37:37.553284: step 6872, loss 0.00571827, acc 1
2016-09-05T20:37:38.348789: step 6873, loss 0.021277, acc 0.98
2016-09-05T20:37:39.162603: step 6874, loss 0.0031326, acc 1
2016-09-05T20:37:39.994960: step 6875, loss 0.0113058, acc 1
2016-09-05T20:37:40.774886: step 6876, loss 0.0102689, acc 1
2016-09-05T20:37:41.572963: step 6877, loss 0.0219957, acc 1
2016-09-05T20:37:42.393342: step 6878, loss 0.00485218, acc 1
2016-09-05T20:37:43.182089: step 6879, loss 0.00724208, acc 1
2016-09-05T20:37:43.994341: step 6880, loss 0.074913, acc 0.96
2016-09-05T20:37:44.804944: step 6881, loss 0.0238368, acc 0.98
2016-09-05T20:37:45.592834: step 6882, loss 0.011018, acc 1
2016-09-05T20:37:46.391201: step 6883, loss 0.0346683, acc 0.98
2016-09-05T20:37:47.204068: step 6884, loss 0.0100518, acc 1
2016-09-05T20:37:48.002988: step 6885, loss 0.0303227, acc 0.98
2016-09-05T20:37:48.834374: step 6886, loss 0.00509545, acc 1
2016-09-05T20:37:49.649822: step 6887, loss 0.0181312, acc 1
2016-09-05T20:37:50.438226: step 6888, loss 0.0438724, acc 1
2016-09-05T20:37:51.262754: step 6889, loss 0.0306243, acc 1
2016-09-05T20:37:52.062779: step 6890, loss 0.0191529, acc 1
2016-09-05T20:37:52.840625: step 6891, loss 0.303044, acc 0.94
2016-09-05T20:37:53.642379: step 6892, loss 0.0269163, acc 0.98
2016-09-05T20:37:54.445867: step 6893, loss 0.0228132, acc 0.98
2016-09-05T20:37:55.234900: step 6894, loss 0.0181981, acc 1
2016-09-05T20:37:56.056542: step 6895, loss 0.00490904, acc 1
2016-09-05T20:37:56.859903: step 6896, loss 0.0296088, acc 0.98
2016-09-05T20:37:57.646446: step 6897, loss 0.122227, acc 0.98
2016-09-05T20:37:58.458367: step 6898, loss 0.0449852, acc 0.96
2016-09-05T20:37:59.251088: step 6899, loss 0.00780813, acc 1
2016-09-05T20:38:00.099607: step 6900, loss 0.0672536, acc 0.96

Evaluation:
2016-09-05T20:38:03.637391: step 6900, loss 1.89629, acc 0.717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-6900

2016-09-05T20:38:05.577963: step 6901, loss 0.0880161, acc 0.92
2016-09-05T20:38:06.379694: step 6902, loss 0.00523286, acc 1
2016-09-05T20:38:07.197139: step 6903, loss 0.0224227, acc 0.98
2016-09-05T20:38:08.036842: step 6904, loss 0.0936849, acc 0.96
2016-09-05T20:38:08.847445: step 6905, loss 0.0161375, acc 1
2016-09-05T20:38:09.681637: step 6906, loss 0.0620742, acc 0.98
2016-09-05T20:38:10.527591: step 6907, loss 0.0096951, acc 1
2016-09-05T20:38:11.354744: step 6908, loss 0.0163081, acc 1
2016-09-05T20:38:12.173972: step 6909, loss 0.00908994, acc 1
2016-09-05T20:38:12.995855: step 6910, loss 0.0320608, acc 0.98
2016-09-05T20:38:13.813844: step 6911, loss 0.0534679, acc 0.96
2016-09-05T20:38:14.626637: step 6912, loss 0.0148123, acc 1
2016-09-05T20:38:15.460614: step 6913, loss 0.00530718, acc 1
2016-09-05T20:38:16.272190: step 6914, loss 0.015182, acc 1
2016-09-05T20:38:17.100845: step 6915, loss 0.0266198, acc 1
2016-09-05T20:38:17.929669: step 6916, loss 0.0744014, acc 0.96
2016-09-05T20:38:18.758377: step 6917, loss 0.0404788, acc 0.98
2016-09-05T20:38:19.536122: step 6918, loss 0.003751, acc 1
2016-09-05T20:38:20.361662: step 6919, loss 0.0246237, acc 0.98
2016-09-05T20:38:21.183879: step 6920, loss 0.00799084, acc 1
2016-09-05T20:38:21.956243: step 6921, loss 0.0272404, acc 0.98
2016-09-05T20:38:22.784373: step 6922, loss 0.00382852, acc 1
2016-09-05T20:38:23.567216: step 6923, loss 0.0251655, acc 1
2016-09-05T20:38:24.352516: step 6924, loss 0.0181386, acc 1
2016-09-05T20:38:25.186142: step 6925, loss 0.017557, acc 1
2016-09-05T20:38:26.017762: step 6926, loss 0.0336423, acc 0.98
2016-09-05T20:38:26.834053: step 6927, loss 0.0150589, acc 1
2016-09-05T20:38:27.663885: step 6928, loss 0.00392918, acc 1
2016-09-05T20:38:28.474616: step 6929, loss 0.0271606, acc 0.98
2016-09-05T20:38:29.291885: step 6930, loss 0.0276551, acc 1
2016-09-05T20:38:30.131506: step 6931, loss 0.0842063, acc 0.96
2016-09-05T20:38:30.956110: step 6932, loss 0.0293274, acc 0.98
2016-09-05T20:38:31.788689: step 6933, loss 0.0669475, acc 0.96
2016-09-05T20:38:32.616613: step 6934, loss 0.0181955, acc 1
2016-09-05T20:38:33.466198: step 6935, loss 0.0454165, acc 0.96
2016-09-05T20:38:34.267113: step 6936, loss 0.0155308, acc 1
2016-09-05T20:38:35.066238: step 6937, loss 0.0389637, acc 0.98
2016-09-05T20:38:35.894624: step 6938, loss 0.0524, acc 0.98
2016-09-05T20:38:36.698577: step 6939, loss 0.0309801, acc 0.98
2016-09-05T20:38:37.505624: step 6940, loss 0.0133279, acc 1
2016-09-05T20:38:38.372100: step 6941, loss 0.018479, acc 1
2016-09-05T20:38:39.203273: step 6942, loss 0.0399924, acc 0.98
2016-09-05T20:38:40.039181: step 6943, loss 0.0230205, acc 1
2016-09-05T20:38:40.827216: step 6944, loss 0.111591, acc 0.94
2016-09-05T20:38:41.644042: step 6945, loss 0.00693723, acc 1
2016-09-05T20:38:42.430034: step 6946, loss 0.00794738, acc 1
2016-09-05T20:38:43.243217: step 6947, loss 0.013042, acc 1
2016-09-05T20:38:44.064602: step 6948, loss 0.0268339, acc 0.98
2016-09-05T20:38:44.841466: step 6949, loss 0.00325696, acc 1
2016-09-05T20:38:45.660474: step 6950, loss 0.00506446, acc 1
2016-09-05T20:38:46.490665: step 6951, loss 0.0265372, acc 0.98
2016-09-05T20:38:47.265678: step 6952, loss 0.0279278, acc 1
2016-09-05T20:38:48.077809: step 6953, loss 0.0251092, acc 0.98
2016-09-05T20:38:48.900104: step 6954, loss 0.00664862, acc 1
2016-09-05T20:38:49.671811: step 6955, loss 0.0219945, acc 0.98
2016-09-05T20:38:50.499156: step 6956, loss 0.0483716, acc 0.96
2016-09-05T20:38:51.290450: step 6957, loss 0.0140105, acc 1
2016-09-05T20:38:52.075990: step 6958, loss 0.00586626, acc 1
2016-09-05T20:38:52.894347: step 6959, loss 0.108616, acc 0.98
2016-09-05T20:38:53.696263: step 6960, loss 0.0731938, acc 0.98
2016-09-05T20:38:54.511977: step 6961, loss 0.0408758, acc 0.96
2016-09-05T20:38:55.345698: step 6962, loss 0.0411865, acc 0.98
2016-09-05T20:38:56.167729: step 6963, loss 0.0252618, acc 0.98
2016-09-05T20:38:56.954637: step 6964, loss 0.00745012, acc 1
2016-09-05T20:38:57.751270: step 6965, loss 0.0330439, acc 1
2016-09-05T20:38:58.579536: step 6966, loss 0.062162, acc 0.96
2016-09-05T20:38:59.365412: step 6967, loss 0.00696797, acc 1
2016-09-05T20:39:00.158157: step 6968, loss 0.0131116, acc 1
2016-09-05T20:39:01.042033: step 6969, loss 0.0553914, acc 0.98
2016-09-05T20:39:01.845192: step 6970, loss 0.00961362, acc 1
2016-09-05T20:39:02.661441: step 6971, loss 0.0635845, acc 0.96
2016-09-05T20:39:03.481239: step 6972, loss 0.0106508, acc 1
2016-09-05T20:39:04.316342: step 6973, loss 0.0201177, acc 0.98
2016-09-05T20:39:05.177199: step 6974, loss 0.0145558, acc 1
2016-09-05T20:39:05.994929: step 6975, loss 0.0176725, acc 1
2016-09-05T20:39:06.803934: step 6976, loss 0.0108014, acc 1
2016-09-05T20:39:07.608742: step 6977, loss 0.0275994, acc 0.98
2016-09-05T20:39:08.432335: step 6978, loss 0.00733726, acc 1
2016-09-05T20:39:09.244115: step 6979, loss 0.0161929, acc 1
2016-09-05T20:39:10.039775: step 6980, loss 0.0185022, acc 0.98
2016-09-05T20:39:10.862050: step 6981, loss 0.0231492, acc 0.98
2016-09-05T20:39:11.676052: step 6982, loss 0.039063, acc 0.98
2016-09-05T20:39:12.483769: step 6983, loss 0.00304611, acc 1
2016-09-05T20:39:12.904513: step 6984, loss 0.00627884, acc 1
2016-09-05T20:39:13.700829: step 6985, loss 0.0302217, acc 0.98
2016-09-05T20:39:14.553998: step 6986, loss 0.00640085, acc 1
2016-09-05T20:39:15.397679: step 6987, loss 0.0049437, acc 1
2016-09-05T20:39:16.199062: step 6988, loss 0.0085849, acc 1
2016-09-05T20:39:17.041377: step 6989, loss 0.0125492, acc 1
2016-09-05T20:39:17.856079: step 6990, loss 0.0111131, acc 1
2016-09-05T20:39:18.653514: step 6991, loss 0.0154352, acc 1
2016-09-05T20:39:19.475581: step 6992, loss 0.0292805, acc 0.98
2016-09-05T20:39:20.284766: step 6993, loss 0.0142436, acc 1
2016-09-05T20:39:21.087220: step 6994, loss 0.0159977, acc 1
2016-09-05T20:39:21.912038: step 6995, loss 0.0191419, acc 1
2016-09-05T20:39:22.734204: step 6996, loss 0.0302532, acc 1
2016-09-05T20:39:23.521607: step 6997, loss 0.0195536, acc 1
2016-09-05T20:39:24.346113: step 6998, loss 0.0771661, acc 0.96
2016-09-05T20:39:25.136891: step 6999, loss 0.00357799, acc 1
2016-09-05T20:39:25.950469: step 7000, loss 0.00569248, acc 1

Evaluation:
2016-09-05T20:39:29.447567: step 7000, loss 2.46944, acc 0.732

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-7000

2016-09-05T20:39:31.286349: step 7001, loss 0.142607, acc 0.96
2016-09-05T20:39:32.073431: step 7002, loss 0.00354577, acc 1
2016-09-05T20:39:32.871384: step 7003, loss 0.0174168, acc 1
2016-09-05T20:39:33.734311: step 7004, loss 0.116079, acc 0.98
2016-09-05T20:39:34.534759: step 7005, loss 0.0322736, acc 0.96
2016-09-05T20:39:35.329924: step 7006, loss 0.00799853, acc 1
2016-09-05T20:39:36.144831: step 7007, loss 0.048867, acc 0.98
2016-09-05T20:39:36.957599: step 7008, loss 0.0194476, acc 0.98
2016-09-05T20:39:37.785936: step 7009, loss 0.0550076, acc 0.98
2016-09-05T20:39:38.617915: step 7010, loss 0.00448884, acc 1
2016-09-05T20:39:39.434137: step 7011, loss 0.00448089, acc 1
2016-09-05T20:39:40.223127: step 7012, loss 0.00889235, acc 1
2016-09-05T20:39:41.040394: step 7013, loss 0.0162019, acc 1
2016-09-05T20:39:41.866685: step 7014, loss 0.0168403, acc 1
2016-09-05T20:39:42.689789: step 7015, loss 0.0324844, acc 0.98
2016-09-05T20:39:43.544285: step 7016, loss 0.032129, acc 1
2016-09-05T20:39:44.376420: step 7017, loss 0.029744, acc 0.98
2016-09-05T20:39:45.179338: step 7018, loss 0.0977544, acc 0.98
2016-09-05T20:39:46.009215: step 7019, loss 0.0391531, acc 0.98
2016-09-05T20:39:46.818668: step 7020, loss 0.00276394, acc 1
2016-09-05T20:39:47.610780: step 7021, loss 0.0343552, acc 0.98
2016-09-05T20:39:48.423414: step 7022, loss 0.00487958, acc 1
2016-09-05T20:39:49.238970: step 7023, loss 0.00326971, acc 1
2016-09-05T20:39:50.058396: step 7024, loss 0.00431248, acc 1
2016-09-05T20:39:50.909093: step 7025, loss 0.0188255, acc 0.98
2016-09-05T20:39:51.727841: step 7026, loss 0.0113739, acc 1
2016-09-05T20:39:52.536209: step 7027, loss 0.00462064, acc 1
2016-09-05T20:39:53.343589: step 7028, loss 0.0190001, acc 1
2016-09-05T20:39:54.195322: step 7029, loss 0.0355356, acc 0.96
2016-09-05T20:39:54.993026: step 7030, loss 0.0584345, acc 0.98
2016-09-05T20:39:55.825193: step 7031, loss 0.00366639, acc 1
2016-09-05T20:39:56.645408: step 7032, loss 0.0039824, acc 1
2016-09-05T20:39:57.430481: step 7033, loss 0.0299309, acc 0.98
2016-09-05T20:39:58.241501: step 7034, loss 0.048101, acc 0.96
2016-09-05T20:39:59.067425: step 7035, loss 0.0500297, acc 0.98
2016-09-05T20:39:59.861891: step 7036, loss 0.00616122, acc 1
2016-09-05T20:40:00.677803: step 7037, loss 0.0135581, acc 1
2016-09-05T20:40:01.484882: step 7038, loss 0.0084572, acc 1
2016-09-05T20:40:02.295260: step 7039, loss 0.00319913, acc 1
2016-09-05T20:40:03.128262: step 7040, loss 0.00767648, acc 1
2016-09-05T20:40:03.971756: step 7041, loss 0.0598809, acc 0.98
2016-09-05T20:40:04.761077: step 7042, loss 0.00317041, acc 1
2016-09-05T20:40:05.561013: step 7043, loss 0.0145435, acc 1
2016-09-05T20:40:06.355022: step 7044, loss 0.00343911, acc 1
2016-09-05T20:40:07.140299: step 7045, loss 0.0159519, acc 1
2016-09-05T20:40:07.956462: step 7046, loss 0.0506209, acc 0.98
2016-09-05T20:40:08.769794: step 7047, loss 0.0479182, acc 0.94
2016-09-05T20:40:09.569199: step 7048, loss 0.0445563, acc 1
2016-09-05T20:40:10.382525: step 7049, loss 0.0481882, acc 0.96
2016-09-05T20:40:11.190485: step 7050, loss 0.00775438, acc 1
2016-09-05T20:40:11.984499: step 7051, loss 0.246993, acc 0.96
2016-09-05T20:40:12.794565: step 7052, loss 0.0188958, acc 1
2016-09-05T20:40:13.595458: step 7053, loss 0.00462611, acc 1
2016-09-05T20:40:14.394215: step 7054, loss 0.0245222, acc 1
2016-09-05T20:40:15.204038: step 7055, loss 0.00678677, acc 1
2016-09-05T20:40:16.025428: step 7056, loss 0.0182268, acc 1
2016-09-05T20:40:16.861262: step 7057, loss 0.00403579, acc 1
2016-09-05T20:40:17.649400: step 7058, loss 0.00583965, acc 1
2016-09-05T20:40:18.469182: step 7059, loss 0.0517439, acc 0.98
2016-09-05T20:40:19.280075: step 7060, loss 0.0376128, acc 0.98
2016-09-05T20:40:20.094837: step 7061, loss 0.0262769, acc 0.98
2016-09-05T20:40:20.898621: step 7062, loss 0.01401, acc 1
2016-09-05T20:40:21.687609: step 7063, loss 0.00595558, acc 1
2016-09-05T20:40:22.487421: step 7064, loss 0.0434346, acc 0.98
2016-09-05T20:40:23.301933: step 7065, loss 0.0222112, acc 0.98
2016-09-05T20:40:24.069997: step 7066, loss 0.017465, acc 1
2016-09-05T20:40:24.878512: step 7067, loss 0.0315758, acc 0.98
2016-09-05T20:40:25.687136: step 7068, loss 0.0031106, acc 1
2016-09-05T20:40:26.523346: step 7069, loss 0.0235614, acc 0.98
2016-09-05T20:40:27.318378: step 7070, loss 0.0234996, acc 1
2016-09-05T20:40:28.156220: step 7071, loss 0.015196, acc 1
2016-09-05T20:40:28.944843: step 7072, loss 0.0471142, acc 0.98
2016-09-05T20:40:29.756586: step 7073, loss 0.00962693, acc 1
2016-09-05T20:40:30.574489: step 7074, loss 0.0613677, acc 0.98
2016-09-05T20:40:31.392755: step 7075, loss 0.0324598, acc 0.98
2016-09-05T20:40:32.211201: step 7076, loss 0.013999, acc 1
2016-09-05T20:40:33.029923: step 7077, loss 0.00319095, acc 1
2016-09-05T20:40:33.825875: step 7078, loss 0.00335692, acc 1
2016-09-05T20:40:34.633593: step 7079, loss 0.00430772, acc 1
2016-09-05T20:40:35.440035: step 7080, loss 0.0587273, acc 0.96
2016-09-05T20:40:36.241983: step 7081, loss 0.0455285, acc 0.96
2016-09-05T20:40:37.037419: step 7082, loss 0.0750166, acc 0.96
2016-09-05T20:40:37.838358: step 7083, loss 0.0216365, acc 0.98
2016-09-05T20:40:38.612836: step 7084, loss 0.0282829, acc 0.98
2016-09-05T20:40:39.403375: step 7085, loss 0.0108923, acc 1
2016-09-05T20:40:40.240071: step 7086, loss 0.0387805, acc 0.98
2016-09-05T20:40:41.026280: step 7087, loss 0.0387099, acc 0.98
2016-09-05T20:40:41.821832: step 7088, loss 0.0360757, acc 0.98
2016-09-05T20:40:42.640868: step 7089, loss 0.0149887, acc 1
2016-09-05T20:40:43.442283: step 7090, loss 0.0142267, acc 1
2016-09-05T20:40:44.253378: step 7091, loss 0.0412047, acc 0.98
2016-09-05T20:40:45.056060: step 7092, loss 0.0556716, acc 0.98
2016-09-05T20:40:45.856895: step 7093, loss 0.0677556, acc 0.98
2016-09-05T20:40:46.672813: step 7094, loss 0.00852251, acc 1
2016-09-05T20:40:47.504347: step 7095, loss 0.0141981, acc 1
2016-09-05T20:40:48.291820: step 7096, loss 0.0031103, acc 1
2016-09-05T20:40:49.067476: step 7097, loss 0.0300129, acc 0.98
2016-09-05T20:40:49.884982: step 7098, loss 0.0615924, acc 0.98
2016-09-05T20:40:50.687308: step 7099, loss 0.0136617, acc 1
2016-09-05T20:40:51.468409: step 7100, loss 0.0492075, acc 0.98

Evaluation:
2016-09-05T20:40:54.960876: step 7100, loss 1.80026, acc 0.737

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-7100

2016-09-05T20:40:56.748007: step 7101, loss 0.110665, acc 0.96
2016-09-05T20:40:57.577592: step 7102, loss 0.0294902, acc 1
2016-09-05T20:40:58.413594: step 7103, loss 0.0100388, acc 1
2016-09-05T20:40:59.220115: step 7104, loss 0.0410303, acc 0.98
2016-09-05T20:41:00.035798: step 7105, loss 0.0340177, acc 0.98
2016-09-05T20:41:00.869393: step 7106, loss 0.0589102, acc 0.98
2016-09-05T20:41:01.649373: step 7107, loss 0.0363412, acc 0.96
2016-09-05T20:41:02.461706: step 7108, loss 0.0146625, acc 1
2016-09-05T20:41:03.283793: step 7109, loss 0.00751072, acc 1
2016-09-05T20:41:04.063262: step 7110, loss 0.0867842, acc 0.98
2016-09-05T20:41:04.884931: step 7111, loss 0.0200278, acc 1
2016-09-05T20:41:05.728559: step 7112, loss 0.021041, acc 1
2016-09-05T20:41:06.545503: step 7113, loss 0.0059766, acc 1
2016-09-05T20:41:07.359316: step 7114, loss 0.0292431, acc 0.98
2016-09-05T20:41:08.206935: step 7115, loss 0.0251346, acc 0.98
2016-09-05T20:41:08.992024: step 7116, loss 0.00671799, acc 1
2016-09-05T20:41:09.801711: step 7117, loss 0.00439552, acc 1
2016-09-05T20:41:10.601985: step 7118, loss 0.00439208, acc 1
2016-09-05T20:41:11.385387: step 7119, loss 0.0307847, acc 0.98
2016-09-05T20:41:12.178380: step 7120, loss 0.0653148, acc 0.96
2016-09-05T20:41:12.981288: step 7121, loss 0.00609406, acc 1
2016-09-05T20:41:13.744505: step 7122, loss 0.00461973, acc 1
2016-09-05T20:41:14.558660: step 7123, loss 0.0066978, acc 1
2016-09-05T20:41:15.392545: step 7124, loss 0.0254361, acc 0.98
2016-09-05T20:41:16.201021: step 7125, loss 0.00700863, acc 1
2016-09-05T20:41:16.999230: step 7126, loss 0.0141607, acc 1
2016-09-05T20:41:17.814903: step 7127, loss 0.0165611, acc 1
2016-09-05T20:41:18.611215: step 7128, loss 0.016975, acc 1
2016-09-05T20:41:19.401616: step 7129, loss 0.0453497, acc 0.98
2016-09-05T20:41:20.213261: step 7130, loss 0.0189731, acc 0.98
2016-09-05T20:41:21.007375: step 7131, loss 0.0567943, acc 0.96
2016-09-05T20:41:21.815971: step 7132, loss 0.00530462, acc 1
2016-09-05T20:41:22.650370: step 7133, loss 0.0541347, acc 0.96
2016-09-05T20:41:23.449111: step 7134, loss 0.00473326, acc 1
2016-09-05T20:41:24.292711: step 7135, loss 0.0330021, acc 0.98
2016-09-05T20:41:25.099096: step 7136, loss 0.107892, acc 0.96
2016-09-05T20:41:25.859334: step 7137, loss 0.0402696, acc 0.98
2016-09-05T20:41:26.670332: step 7138, loss 0.0325527, acc 0.98
2016-09-05T20:41:27.524926: step 7139, loss 0.0259705, acc 0.98
2016-09-05T20:41:28.343728: step 7140, loss 0.0302235, acc 0.98
2016-09-05T20:41:29.147485: step 7141, loss 0.00578326, acc 1
2016-09-05T20:41:29.938067: step 7142, loss 0.00773121, acc 1
2016-09-05T20:41:30.731119: step 7143, loss 0.0328345, acc 0.98
2016-09-05T20:41:31.547453: step 7144, loss 0.0263071, acc 0.98
2016-09-05T20:41:32.342067: step 7145, loss 0.0261521, acc 0.98
2016-09-05T20:41:33.114748: step 7146, loss 0.0835601, acc 0.98
2016-09-05T20:41:33.901269: step 7147, loss 0.0554428, acc 0.98
2016-09-05T20:41:34.732185: step 7148, loss 0.00876246, acc 1
2016-09-05T20:41:35.529769: step 7149, loss 0.0197273, acc 0.98
2016-09-05T20:41:36.349597: step 7150, loss 0.040405, acc 0.96
2016-09-05T20:41:37.153667: step 7151, loss 0.00443176, acc 1
2016-09-05T20:41:37.934085: step 7152, loss 0.00384651, acc 1
2016-09-05T20:41:38.736895: step 7153, loss 0.019187, acc 1
2016-09-05T20:41:39.552030: step 7154, loss 0.0409466, acc 1
2016-09-05T20:41:40.341450: step 7155, loss 0.0259586, acc 0.98
2016-09-05T20:41:41.152773: step 7156, loss 0.0049135, acc 1
2016-09-05T20:41:41.983309: step 7157, loss 0.0199896, acc 1
2016-09-05T20:41:42.769099: step 7158, loss 0.00427019, acc 1
2016-09-05T20:41:43.587889: step 7159, loss 0.0408234, acc 0.98
2016-09-05T20:41:44.432864: step 7160, loss 0.0509028, acc 0.98
2016-09-05T20:41:45.221377: step 7161, loss 0.00876368, acc 1
2016-09-05T20:41:46.031462: step 7162, loss 0.00325149, acc 1
2016-09-05T20:41:46.815113: step 7163, loss 0.016233, acc 1
2016-09-05T20:41:47.601395: step 7164, loss 0.00984758, acc 1
2016-09-05T20:41:48.406461: step 7165, loss 0.0370137, acc 0.98
2016-09-05T20:41:49.216665: step 7166, loss 0.00596076, acc 1
2016-09-05T20:41:50.018369: step 7167, loss 0.00364465, acc 1
2016-09-05T20:41:50.814162: step 7168, loss 0.00355135, acc 1
2016-09-05T20:41:51.624124: step 7169, loss 0.124423, acc 0.96
2016-09-05T20:41:52.402424: step 7170, loss 0.00635775, acc 1
2016-09-05T20:41:53.217530: step 7171, loss 0.0390245, acc 0.98
2016-09-05T20:41:54.070738: step 7172, loss 0.00704332, acc 1
2016-09-05T20:41:54.866670: step 7173, loss 0.0178026, acc 0.98
2016-09-05T20:41:55.676322: step 7174, loss 0.0231273, acc 1
2016-09-05T20:41:56.511156: step 7175, loss 0.0158679, acc 1
2016-09-05T20:41:57.288275: step 7176, loss 0.011084, acc 1
2016-09-05T20:41:58.083071: step 7177, loss 0.00844177, acc 1
2016-09-05T20:41:58.510283: step 7178, loss 0.00301236, acc 1
2016-09-05T20:41:59.326106: step 7179, loss 0.0131226, acc 1
2016-09-05T20:42:00.139320: step 7180, loss 0.0236759, acc 1
2016-09-05T20:42:00.953363: step 7181, loss 0.116848, acc 0.98
2016-09-05T20:42:01.755072: step 7182, loss 0.017253, acc 1
2016-09-05T20:42:02.581461: step 7183, loss 0.00637108, acc 1
2016-09-05T20:42:03.373680: step 7184, loss 0.0733128, acc 0.94
2016-09-05T20:42:04.187775: step 7185, loss 0.0296235, acc 1
2016-09-05T20:42:05.010200: step 7186, loss 0.0587465, acc 0.96
2016-09-05T20:42:05.816827: step 7187, loss 0.00363134, acc 1
2016-09-05T20:42:06.620127: step 7188, loss 0.0196186, acc 1
2016-09-05T20:42:07.427034: step 7189, loss 0.00534799, acc 1
2016-09-05T20:42:08.242104: step 7190, loss 0.0187915, acc 1
2016-09-05T20:42:09.087583: step 7191, loss 0.0586191, acc 0.98
2016-09-05T20:42:09.928990: step 7192, loss 0.00795558, acc 1
2016-09-05T20:42:10.752549: step 7193, loss 0.00265036, acc 1
2016-09-05T20:42:11.555099: step 7194, loss 0.00845872, acc 1
2016-09-05T20:42:12.366317: step 7195, loss 0.0261113, acc 0.98
2016-09-05T20:42:13.166533: step 7196, loss 0.00991188, acc 1
2016-09-05T20:42:13.994578: step 7197, loss 0.0229343, acc 0.98
2016-09-05T20:42:14.848271: step 7198, loss 0.00820409, acc 1
2016-09-05T20:42:15.636492: step 7199, loss 0.0141, acc 1
2016-09-05T20:42:16.427591: step 7200, loss 0.0912396, acc 0.96

Evaluation:
2016-09-05T20:42:19.961513: step 7200, loss 1.93761, acc 0.721

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-7200

2016-09-05T20:42:21.968033: step 7201, loss 0.00379019, acc 1
2016-09-05T20:42:22.769694: step 7202, loss 0.00315464, acc 1
2016-09-05T20:42:23.567485: step 7203, loss 0.0200406, acc 0.98
2016-09-05T20:42:24.405784: step 7204, loss 0.0128271, acc 1
2016-09-05T20:42:25.195304: step 7205, loss 0.00297954, acc 1
2016-09-05T20:42:26.005845: step 7206, loss 0.0383114, acc 0.98
2016-09-05T20:42:26.799305: step 7207, loss 0.0247463, acc 1
2016-09-05T20:42:27.599702: step 7208, loss 0.0125268, acc 1
2016-09-05T20:42:28.395329: step 7209, loss 0.0310213, acc 0.98
2016-09-05T20:42:29.189029: step 7210, loss 0.0616823, acc 0.96
2016-09-05T20:42:29.993203: step 7211, loss 0.0312611, acc 0.98
2016-09-05T20:42:30.848804: step 7212, loss 0.00910484, acc 1
2016-09-05T20:42:31.664022: step 7213, loss 0.00361531, acc 1
2016-09-05T20:42:32.454470: step 7214, loss 0.199441, acc 0.96
2016-09-05T20:42:33.263170: step 7215, loss 0.00802132, acc 1
2016-09-05T20:42:34.091096: step 7216, loss 0.0128892, acc 1
2016-09-05T20:42:34.877342: step 7217, loss 0.00390812, acc 1
2016-09-05T20:42:35.679367: step 7218, loss 0.00750494, acc 1
2016-09-05T20:42:36.537802: step 7219, loss 0.0393004, acc 0.98
2016-09-05T20:42:37.333629: step 7220, loss 0.0263485, acc 1
2016-09-05T20:42:38.145349: step 7221, loss 0.062791, acc 0.96
2016-09-05T20:42:38.946208: step 7222, loss 0.0582, acc 0.98
2016-09-05T20:42:39.725351: step 7223, loss 0.017441, acc 1
2016-09-05T20:42:40.522233: step 7224, loss 0.0909606, acc 0.96
2016-09-05T20:42:41.305847: step 7225, loss 0.0068092, acc 1
2016-09-05T20:42:42.105934: step 7226, loss 0.0245605, acc 0.98
2016-09-05T20:42:42.907673: step 7227, loss 0.0100627, acc 1
2016-09-05T20:42:43.713812: step 7228, loss 0.0323933, acc 0.98
2016-09-05T20:42:44.495116: step 7229, loss 0.0169462, acc 1
2016-09-05T20:42:45.318208: step 7230, loss 0.00748201, acc 1
2016-09-05T20:42:46.116098: step 7231, loss 0.00424103, acc 1
2016-09-05T20:42:46.916791: step 7232, loss 0.0167267, acc 1
2016-09-05T20:42:47.736188: step 7233, loss 0.0445166, acc 0.98
2016-09-05T20:42:48.540891: step 7234, loss 0.0196834, acc 1
2016-09-05T20:42:49.329101: step 7235, loss 0.038775, acc 1
2016-09-05T20:42:50.149037: step 7236, loss 0.00722936, acc 1
2016-09-05T20:42:50.975010: step 7237, loss 0.00390327, acc 1
2016-09-05T20:42:51.790709: step 7238, loss 0.0114902, acc 1
2016-09-05T20:42:52.610644: step 7239, loss 0.0384706, acc 0.98
2016-09-05T20:42:53.441842: step 7240, loss 0.00307622, acc 1
2016-09-05T20:42:54.240294: step 7241, loss 0.0454206, acc 0.98
2016-09-05T20:42:55.033974: step 7242, loss 0.0803789, acc 0.98
2016-09-05T20:42:55.828312: step 7243, loss 0.0495915, acc 0.98
2016-09-05T20:42:56.610575: step 7244, loss 0.00567097, acc 1
2016-09-05T20:42:57.407184: step 7245, loss 0.00692327, acc 1
2016-09-05T20:42:58.225331: step 7246, loss 0.0849841, acc 0.98
2016-09-05T20:42:59.018431: step 7247, loss 0.022114, acc 0.98
2016-09-05T20:42:59.815950: step 7248, loss 0.0312186, acc 0.98
2016-09-05T20:43:00.656714: step 7249, loss 0.00359283, acc 1
2016-09-05T20:43:01.434454: step 7250, loss 0.00988902, acc 1
2016-09-05T20:43:02.259141: step 7251, loss 0.00256006, acc 1
2016-09-05T20:43:03.074515: step 7252, loss 0.00653858, acc 1
2016-09-05T20:43:03.856659: step 7253, loss 0.0032457, acc 1
2016-09-05T20:43:04.697850: step 7254, loss 0.0442306, acc 0.96
2016-09-05T20:43:05.522198: step 7255, loss 0.0114807, acc 1
2016-09-05T20:43:06.293037: step 7256, loss 0.00315269, acc 1
2016-09-05T20:43:07.088086: step 7257, loss 0.0252639, acc 0.98
2016-09-05T20:43:07.913948: step 7258, loss 0.0574963, acc 0.96
2016-09-05T20:43:08.711496: step 7259, loss 0.00357665, acc 1
2016-09-05T20:43:09.521242: step 7260, loss 0.00446353, acc 1
2016-09-05T20:43:10.338107: step 7261, loss 0.00983869, acc 1
2016-09-05T20:43:11.132952: step 7262, loss 0.00305248, acc 1
2016-09-05T20:43:11.926944: step 7263, loss 0.0137505, acc 1
2016-09-05T20:43:12.787147: step 7264, loss 0.0491891, acc 0.98
2016-09-05T20:43:13.580743: step 7265, loss 0.0428583, acc 0.98
2016-09-05T20:43:14.382445: step 7266, loss 0.0355182, acc 0.98
2016-09-05T20:43:15.181002: step 7267, loss 0.0652957, acc 0.94
2016-09-05T20:43:15.967472: step 7268, loss 0.0282025, acc 1
2016-09-05T20:43:16.770061: step 7269, loss 0.007949, acc 1
2016-09-05T20:43:17.573050: step 7270, loss 0.0280457, acc 0.98
2016-09-05T20:43:18.348941: step 7271, loss 0.00303448, acc 1
2016-09-05T20:43:19.239940: step 7272, loss 0.00374861, acc 1
2016-09-05T20:43:20.050018: step 7273, loss 0.129473, acc 0.96
2016-09-05T20:43:20.867138: step 7274, loss 0.0226867, acc 0.98
2016-09-05T20:43:21.680638: step 7275, loss 0.0157632, acc 1
2016-09-05T20:43:22.493369: step 7276, loss 0.0468582, acc 0.98
2016-09-05T20:43:23.299491: step 7277, loss 0.0181077, acc 0.98
2016-09-05T20:43:24.095936: step 7278, loss 0.00575921, acc 1
2016-09-05T20:43:24.965568: step 7279, loss 0.0122925, acc 1
2016-09-05T20:43:25.773411: step 7280, loss 0.0336715, acc 0.98
2016-09-05T20:43:26.597301: step 7281, loss 0.00471872, acc 1
2016-09-05T20:43:27.418762: step 7282, loss 0.0172237, acc 1
2016-09-05T20:43:28.245959: step 7283, loss 0.0257097, acc 1
2016-09-05T20:43:29.075334: step 7284, loss 0.0329324, acc 0.98
2016-09-05T20:43:29.922452: step 7285, loss 0.0177686, acc 0.98
2016-09-05T20:43:30.718749: step 7286, loss 0.0160072, acc 1
2016-09-05T20:43:31.530354: step 7287, loss 0.0658967, acc 0.98
2016-09-05T20:43:32.335618: step 7288, loss 0.0184338, acc 0.98
2016-09-05T20:43:33.137336: step 7289, loss 0.0106973, acc 1
2016-09-05T20:43:33.938919: step 7290, loss 0.0579132, acc 0.98
2016-09-05T20:43:34.745833: step 7291, loss 0.00664556, acc 1
2016-09-05T20:43:35.575227: step 7292, loss 0.0842469, acc 0.94
2016-09-05T20:43:36.422569: step 7293, loss 0.0372811, acc 0.98
2016-09-05T20:43:37.246807: step 7294, loss 0.0118784, acc 1
2016-09-05T20:43:38.086927: step 7295, loss 0.0157525, acc 1
2016-09-05T20:43:38.873460: step 7296, loss 0.0347817, acc 0.98
2016-09-05T20:43:39.702081: step 7297, loss 0.00328992, acc 1
2016-09-05T20:43:40.544084: step 7298, loss 0.0552487, acc 0.98
2016-09-05T20:43:41.346476: step 7299, loss 0.0287162, acc 0.98
2016-09-05T20:43:42.142442: step 7300, loss 0.0040944, acc 1

Evaluation:
2016-09-05T20:43:45.636721: step 7300, loss 1.97639, acc 0.72

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-7300

2016-09-05T20:43:47.497332: step 7301, loss 0.00982517, acc 1
2016-09-05T20:43:48.315759: step 7302, loss 0.00510231, acc 1
2016-09-05T20:43:49.138346: step 7303, loss 0.0256385, acc 1
2016-09-05T20:43:49.947688: step 7304, loss 0.022614, acc 1
2016-09-05T20:43:50.759574: step 7305, loss 0.0403029, acc 0.98
2016-09-05T20:43:51.568883: step 7306, loss 0.018529, acc 0.98
2016-09-05T20:43:52.382972: step 7307, loss 0.00420578, acc 1
2016-09-05T20:43:53.207329: step 7308, loss 0.0339195, acc 0.98
2016-09-05T20:43:54.026542: step 7309, loss 0.0678959, acc 0.96
2016-09-05T20:43:54.813414: step 7310, loss 0.0221949, acc 0.98
2016-09-05T20:43:55.636125: step 7311, loss 0.00655142, acc 1
2016-09-05T20:43:56.455490: step 7312, loss 0.0709845, acc 0.96
2016-09-05T20:43:57.256404: step 7313, loss 0.00895877, acc 1
2016-09-05T20:43:58.073479: step 7314, loss 0.0106352, acc 1
2016-09-05T20:43:58.911268: step 7315, loss 0.00560128, acc 1
2016-09-05T20:43:59.728412: step 7316, loss 0.0129268, acc 1
2016-09-05T20:44:00.587444: step 7317, loss 0.00395519, acc 1
2016-09-05T20:44:01.434862: step 7318, loss 0.0331416, acc 0.98
2016-09-05T20:44:02.245193: step 7319, loss 0.00740876, acc 1
2016-09-05T20:44:03.065038: step 7320, loss 0.0183769, acc 1
2016-09-05T20:44:03.893214: step 7321, loss 0.00574578, acc 1
2016-09-05T20:44:04.691938: step 7322, loss 0.00650719, acc 1
2016-09-05T20:44:05.505467: step 7323, loss 0.00704847, acc 1
2016-09-05T20:44:06.336477: step 7324, loss 0.0184943, acc 1
2016-09-05T20:44:07.137559: step 7325, loss 0.00362879, acc 1
2016-09-05T20:44:07.951918: step 7326, loss 0.0041111, acc 1
2016-09-05T20:44:08.760059: step 7327, loss 0.00589396, acc 1
2016-09-05T20:44:09.590467: step 7328, loss 0.00299121, acc 1
2016-09-05T20:44:10.382470: step 7329, loss 0.00754776, acc 1
2016-09-05T20:44:11.174243: step 7330, loss 0.101593, acc 0.98
2016-09-05T20:44:11.985890: step 7331, loss 0.034361, acc 0.98
2016-09-05T20:44:12.777440: step 7332, loss 0.0856695, acc 0.98
2016-09-05T20:44:13.585389: step 7333, loss 0.0418322, acc 0.98
2016-09-05T20:44:14.400374: step 7334, loss 0.0108394, acc 1
2016-09-05T20:44:15.177551: step 7335, loss 0.00349526, acc 1
2016-09-05T20:44:15.986415: step 7336, loss 0.049576, acc 0.96
2016-09-05T20:44:16.787016: step 7337, loss 0.00781204, acc 1
2016-09-05T20:44:17.614733: step 7338, loss 0.019955, acc 0.98
2016-09-05T20:44:18.412802: step 7339, loss 0.00361429, acc 1
2016-09-05T20:44:19.216322: step 7340, loss 0.0269356, acc 1
2016-09-05T20:44:20.039065: step 7341, loss 0.03127, acc 0.98
2016-09-05T20:44:20.868000: step 7342, loss 0.0480138, acc 0.96
2016-09-05T20:44:21.667396: step 7343, loss 0.00383947, acc 1
2016-09-05T20:44:22.491822: step 7344, loss 0.01513, acc 1
2016-09-05T20:44:23.309182: step 7345, loss 0.00260521, acc 1
2016-09-05T20:44:24.123399: step 7346, loss 0.00458642, acc 1
2016-09-05T20:44:24.907156: step 7347, loss 0.021489, acc 1
2016-09-05T20:44:25.688506: step 7348, loss 0.0332433, acc 1
2016-09-05T20:44:26.495133: step 7349, loss 0.031385, acc 0.98
2016-09-05T20:44:27.296866: step 7350, loss 0.0214091, acc 0.98
2016-09-05T20:44:28.120252: step 7351, loss 0.017699, acc 1
2016-09-05T20:44:28.945330: step 7352, loss 0.00283091, acc 1
2016-09-05T20:44:29.707184: step 7353, loss 0.00611485, acc 1
2016-09-05T20:44:30.510595: step 7354, loss 0.0396178, acc 0.96
2016-09-05T20:44:31.344324: step 7355, loss 0.0132272, acc 1
2016-09-05T20:44:32.111930: step 7356, loss 0.00250997, acc 1
2016-09-05T20:44:32.911836: step 7357, loss 0.0332029, acc 0.98
2016-09-05T20:44:33.731442: step 7358, loss 0.0192606, acc 0.98
2016-09-05T20:44:34.520707: step 7359, loss 0.0179495, acc 1
2016-09-05T20:44:35.318362: step 7360, loss 0.00357446, acc 1
2016-09-05T20:44:36.152643: step 7361, loss 0.0117249, acc 1
2016-09-05T20:44:36.997423: step 7362, loss 0.0036607, acc 1
2016-09-05T20:44:37.792407: step 7363, loss 0.00486334, acc 1
2016-09-05T20:44:38.598220: step 7364, loss 0.0059045, acc 1
2016-09-05T20:44:39.389577: step 7365, loss 0.0355492, acc 0.98
2016-09-05T20:44:40.210423: step 7366, loss 0.00308685, acc 1
2016-09-05T20:44:41.018246: step 7367, loss 0.00316158, acc 1
2016-09-05T20:44:41.796104: step 7368, loss 0.0185013, acc 0.98
2016-09-05T20:44:42.586908: step 7369, loss 0.0780458, acc 0.98
2016-09-05T20:44:43.413576: step 7370, loss 0.0255297, acc 0.98
2016-09-05T20:44:44.240123: step 7371, loss 0.0141683, acc 1
2016-09-05T20:44:44.648342: step 7372, loss 0.00412331, acc 1
2016-09-05T20:44:45.420121: step 7373, loss 0.0635651, acc 0.98
2016-09-05T20:44:46.220509: step 7374, loss 0.0169099, acc 1
2016-09-05T20:44:47.032109: step 7375, loss 0.0168035, acc 1
2016-09-05T20:44:47.814419: step 7376, loss 0.0624654, acc 0.98
2016-09-05T20:44:48.612187: step 7377, loss 0.00746635, acc 1
2016-09-05T20:44:49.432824: step 7378, loss 0.0439347, acc 0.98
2016-09-05T20:44:50.223051: step 7379, loss 0.00896159, acc 1
2016-09-05T20:44:51.011685: step 7380, loss 0.00980821, acc 1
2016-09-05T20:44:51.825393: step 7381, loss 0.016146, acc 1
2016-09-05T20:44:52.599385: step 7382, loss 0.0315373, acc 0.98
2016-09-05T20:44:53.416458: step 7383, loss 0.014643, acc 1
2016-09-05T20:44:54.240733: step 7384, loss 0.0258187, acc 0.98
2016-09-05T20:44:55.044792: step 7385, loss 0.0304705, acc 1
2016-09-05T20:44:55.850860: step 7386, loss 0.00332435, acc 1
2016-09-05T20:44:56.663145: step 7387, loss 0.00283166, acc 1
2016-09-05T20:44:57.456737: step 7388, loss 0.00318688, acc 1
2016-09-05T20:44:58.275773: step 7389, loss 0.00979781, acc 1
2016-09-05T20:44:59.076163: step 7390, loss 0.0281255, acc 0.98
2016-09-05T20:44:59.855125: step 7391, loss 0.0149669, acc 1
2016-09-05T20:45:00.700951: step 7392, loss 0.00573342, acc 1
2016-09-05T20:45:01.531287: step 7393, loss 0.0191528, acc 0.98
2016-09-05T20:45:02.329628: step 7394, loss 0.00949509, acc 1
2016-09-05T20:45:03.157023: step 7395, loss 0.00255679, acc 1
2016-09-05T20:45:03.972952: step 7396, loss 0.00346975, acc 1
2016-09-05T20:45:04.742709: step 7397, loss 0.0216561, acc 0.98
2016-09-05T20:45:05.540851: step 7398, loss 0.106197, acc 0.96
2016-09-05T20:45:06.347593: step 7399, loss 0.0348277, acc 0.98
2016-09-05T20:45:07.126967: step 7400, loss 0.00417412, acc 1

Evaluation:
2016-09-05T20:45:10.620282: step 7400, loss 1.99042, acc 0.732

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-7400

2016-09-05T20:45:12.463167: step 7401, loss 0.00942391, acc 1
2016-09-05T20:45:13.243774: step 7402, loss 0.0135728, acc 1
2016-09-05T20:45:14.065408: step 7403, loss 0.00282857, acc 1
2016-09-05T20:45:14.874241: step 7404, loss 0.0363678, acc 0.98
2016-09-05T20:45:15.653652: step 7405, loss 0.00267235, acc 1
2016-09-05T20:45:16.489475: step 7406, loss 0.0119654, acc 1
2016-09-05T20:45:17.297426: step 7407, loss 0.0235537, acc 0.98
2016-09-05T20:45:18.102603: step 7408, loss 0.00539929, acc 1
2016-09-05T20:45:18.887053: step 7409, loss 0.0255173, acc 0.98
2016-09-05T20:45:19.678911: step 7410, loss 0.0162717, acc 1
2016-09-05T20:45:20.493502: step 7411, loss 0.00409874, acc 1
2016-09-05T20:45:21.300568: step 7412, loss 0.00194007, acc 1
2016-09-05T20:45:22.131009: step 7413, loss 0.0367992, acc 0.98
2016-09-05T20:45:22.905548: step 7414, loss 0.0458634, acc 0.96
2016-09-05T20:45:23.708990: step 7415, loss 0.0643328, acc 0.98
2016-09-05T20:45:24.528816: step 7416, loss 0.00240037, acc 1
2016-09-05T20:45:25.308979: step 7417, loss 0.0186671, acc 1
2016-09-05T20:45:26.094664: step 7418, loss 0.0121834, acc 1
2016-09-05T20:45:26.915487: step 7419, loss 0.033975, acc 0.98
2016-09-05T20:45:27.686980: step 7420, loss 0.0405375, acc 0.98
2016-09-05T20:45:28.500592: step 7421, loss 0.0122202, acc 1
2016-09-05T20:45:29.317947: step 7422, loss 0.0298021, acc 0.98
2016-09-05T20:45:30.128246: step 7423, loss 0.0105725, acc 1
2016-09-05T20:45:30.918812: step 7424, loss 0.0225293, acc 0.98
2016-09-05T20:45:31.730910: step 7425, loss 0.021625, acc 0.98
2016-09-05T20:45:32.528637: step 7426, loss 0.00768833, acc 1
2016-09-05T20:45:33.359550: step 7427, loss 0.00378207, acc 1
2016-09-05T20:45:34.196009: step 7428, loss 0.00612666, acc 1
2016-09-05T20:45:34.981848: step 7429, loss 0.0181631, acc 1
2016-09-05T20:45:35.801101: step 7430, loss 0.0077726, acc 1
2016-09-05T20:45:36.589728: step 7431, loss 0.0200146, acc 1
2016-09-05T20:45:37.380743: step 7432, loss 0.00906771, acc 1
2016-09-05T20:45:38.166917: step 7433, loss 0.0271871, acc 0.98
2016-09-05T20:45:38.981730: step 7434, loss 0.00731321, acc 1
2016-09-05T20:45:39.777464: step 7435, loss 0.0395028, acc 0.98
2016-09-05T20:45:40.586854: step 7436, loss 0.043521, acc 0.98
2016-09-05T20:45:41.403024: step 7437, loss 0.0372907, acc 0.98
2016-09-05T20:45:42.187785: step 7438, loss 0.0974791, acc 0.96
2016-09-05T20:45:42.991323: step 7439, loss 0.0244448, acc 1
2016-09-05T20:45:43.802991: step 7440, loss 0.0189547, acc 1
2016-09-05T20:45:44.608118: step 7441, loss 0.0302478, acc 0.98
2016-09-05T20:45:45.437534: step 7442, loss 0.0425659, acc 0.98
2016-09-05T20:45:46.248814: step 7443, loss 0.004149, acc 1
2016-09-05T20:45:47.025410: step 7444, loss 0.0172047, acc 0.98
2016-09-05T20:45:47.828567: step 7445, loss 0.00233208, acc 1
2016-09-05T20:45:48.638183: step 7446, loss 0.00241492, acc 1
2016-09-05T20:45:49.443650: step 7447, loss 0.0196392, acc 0.98
2016-09-05T20:45:50.253945: step 7448, loss 0.0328467, acc 0.98
2016-09-05T20:45:51.073494: step 7449, loss 0.0464837, acc 0.98
2016-09-05T20:45:51.864140: step 7450, loss 0.00331335, acc 1
2016-09-05T20:45:52.661948: step 7451, loss 0.00223585, acc 1
2016-09-05T20:45:53.535499: step 7452, loss 0.0180959, acc 0.98
2016-09-05T20:45:54.321096: step 7453, loss 0.0161637, acc 1
2016-09-05T20:45:55.090843: step 7454, loss 0.0244698, acc 0.98
2016-09-05T20:45:55.918579: step 7455, loss 0.0255636, acc 0.98
2016-09-05T20:45:56.716519: step 7456, loss 0.021703, acc 0.98
2016-09-05T20:45:57.513128: step 7457, loss 0.0231492, acc 0.98
2016-09-05T20:45:58.344834: step 7458, loss 0.0680169, acc 0.98
2016-09-05T20:45:59.145292: step 7459, loss 0.00248302, acc 1
2016-09-05T20:45:59.943969: step 7460, loss 0.0121508, acc 1
2016-09-05T20:46:00.787302: step 7461, loss 0.0218735, acc 1
2016-09-05T20:46:01.567647: step 7462, loss 0.0238475, acc 1
2016-09-05T20:46:02.372646: step 7463, loss 0.0292829, acc 0.98
2016-09-05T20:46:03.193661: step 7464, loss 0.02206, acc 0.98
2016-09-05T20:46:03.986592: step 7465, loss 0.0202717, acc 1
2016-09-05T20:46:04.792039: step 7466, loss 0.0124478, acc 1
2016-09-05T20:46:05.636313: step 7467, loss 0.0102096, acc 1
2016-09-05T20:46:06.398541: step 7468, loss 0.0201901, acc 0.98
2016-09-05T20:46:07.184206: step 7469, loss 0.0242268, acc 0.98
2016-09-05T20:46:07.991880: step 7470, loss 0.0647451, acc 0.98
2016-09-05T20:46:08.779369: step 7471, loss 0.00981664, acc 1
2016-09-05T20:46:09.613140: step 7472, loss 0.0274999, acc 0.98
2016-09-05T20:46:10.445792: step 7473, loss 0.0128394, acc 1
2016-09-05T20:46:11.248215: step 7474, loss 0.0237833, acc 1
2016-09-05T20:46:12.049120: step 7475, loss 0.0310753, acc 0.98
2016-09-05T20:46:12.888941: step 7476, loss 0.0274196, acc 0.98
2016-09-05T20:46:13.657239: step 7477, loss 0.0577007, acc 0.98
2016-09-05T20:46:14.448340: step 7478, loss 0.0455166, acc 0.98
2016-09-05T20:46:15.273097: step 7479, loss 0.00960802, acc 1
2016-09-05T20:46:16.060332: step 7480, loss 0.0059945, acc 1
2016-09-05T20:46:16.869080: step 7481, loss 0.010307, acc 1
2016-09-05T20:46:17.712220: step 7482, loss 0.0164865, acc 1
2016-09-05T20:46:18.477702: step 7483, loss 0.00634141, acc 1
2016-09-05T20:46:19.277867: step 7484, loss 0.00239924, acc 1
2016-09-05T20:46:20.121445: step 7485, loss 0.0050659, acc 1
2016-09-05T20:46:20.932940: step 7486, loss 0.0778132, acc 0.92
2016-09-05T20:46:21.725575: step 7487, loss 0.0115363, acc 1
2016-09-05T20:46:22.504093: step 7488, loss 0.0374862, acc 1
2016-09-05T20:46:23.296196: step 7489, loss 0.0155852, acc 1
2016-09-05T20:46:24.116234: step 7490, loss 0.0279108, acc 0.98
2016-09-05T20:46:24.936439: step 7491, loss 0.00239564, acc 1
2016-09-05T20:46:25.727554: step 7492, loss 0.0173664, acc 1
2016-09-05T20:46:26.531793: step 7493, loss 0.0256875, acc 1
2016-09-05T20:46:27.338232: step 7494, loss 0.030166, acc 1
2016-09-05T20:46:28.123277: step 7495, loss 0.00210941, acc 1
2016-09-05T20:46:28.922119: step 7496, loss 0.00233713, acc 1
2016-09-05T20:46:29.723790: step 7497, loss 0.0489431, acc 0.98
2016-09-05T20:46:30.520144: step 7498, loss 0.00245781, acc 1
2016-09-05T20:46:31.322517: step 7499, loss 0.0046131, acc 1
2016-09-05T20:46:32.133478: step 7500, loss 0.0140765, acc 1

Evaluation:
2016-09-05T20:46:35.608217: step 7500, loss 2.37902, acc 0.733

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-7500

2016-09-05T20:46:37.520451: step 7501, loss 0.0146845, acc 1
2016-09-05T20:46:38.354261: step 7502, loss 0.0767503, acc 0.96
2016-09-05T20:46:39.161532: step 7503, loss 0.0210075, acc 0.98
2016-09-05T20:46:39.945802: step 7504, loss 0.0165189, acc 0.98
2016-09-05T20:46:40.789406: step 7505, loss 0.0331123, acc 0.98
2016-09-05T20:46:41.577637: step 7506, loss 0.0100127, acc 1
2016-09-05T20:46:42.381798: step 7507, loss 0.00769709, acc 1
2016-09-05T20:46:43.187415: step 7508, loss 0.173268, acc 0.98
2016-09-05T20:46:43.988072: step 7509, loss 0.0180731, acc 1
2016-09-05T20:46:44.822487: step 7510, loss 0.00244816, acc 1
2016-09-05T20:46:45.631771: step 7511, loss 0.00198837, acc 1
2016-09-05T20:46:46.391344: step 7512, loss 0.0110563, acc 1
2016-09-05T20:46:47.188865: step 7513, loss 0.00495658, acc 1
2016-09-05T20:46:48.021116: step 7514, loss 0.0207273, acc 0.98
2016-09-05T20:46:48.835988: step 7515, loss 0.021189, acc 0.98
2016-09-05T20:46:49.635093: step 7516, loss 0.00691926, acc 1
2016-09-05T20:46:50.434791: step 7517, loss 0.0237843, acc 1
2016-09-05T20:46:51.225990: step 7518, loss 0.00967076, acc 1
2016-09-05T20:46:52.041103: step 7519, loss 0.0079058, acc 1
2016-09-05T20:46:52.865113: step 7520, loss 0.00431966, acc 1
2016-09-05T20:46:53.654438: step 7521, loss 0.0543809, acc 0.96
2016-09-05T20:46:54.451487: step 7522, loss 0.0161792, acc 1
2016-09-05T20:46:55.261391: step 7523, loss 0.0662523, acc 0.96
2016-09-05T20:46:56.045454: step 7524, loss 0.0926477, acc 0.96
2016-09-05T20:46:56.866026: step 7525, loss 0.0360163, acc 0.98
2016-09-05T20:46:57.686228: step 7526, loss 0.0184939, acc 1
2016-09-05T20:46:58.493632: step 7527, loss 0.0235776, acc 1
2016-09-05T20:46:59.313335: step 7528, loss 0.00508362, acc 1
2016-09-05T20:47:00.133509: step 7529, loss 0.00286024, acc 1
2016-09-05T20:47:00.959200: step 7530, loss 0.0464006, acc 0.96
2016-09-05T20:47:01.777443: step 7531, loss 0.0167663, acc 1
2016-09-05T20:47:02.581910: step 7532, loss 0.00726356, acc 1
2016-09-05T20:47:03.391399: step 7533, loss 0.00246197, acc 1
2016-09-05T20:47:04.191783: step 7534, loss 0.0561552, acc 0.98
2016-09-05T20:47:05.010451: step 7535, loss 0.0181308, acc 1
2016-09-05T20:47:05.827490: step 7536, loss 0.0103311, acc 1
2016-09-05T20:47:06.663153: step 7537, loss 0.00569352, acc 1
2016-09-05T20:47:07.498610: step 7538, loss 0.00416433, acc 1
2016-09-05T20:47:08.327106: step 7539, loss 0.0349014, acc 0.98
2016-09-05T20:47:09.193426: step 7540, loss 0.0168795, acc 1
2016-09-05T20:47:10.008792: step 7541, loss 0.0316528, acc 0.98
2016-09-05T20:47:10.816789: step 7542, loss 0.0546507, acc 0.96
2016-09-05T20:47:11.622352: step 7543, loss 0.109772, acc 0.94
2016-09-05T20:47:12.442234: step 7544, loss 0.00371213, acc 1
2016-09-05T20:47:13.251339: step 7545, loss 0.00660441, acc 1
2016-09-05T20:47:14.058621: step 7546, loss 0.0364785, acc 0.98
2016-09-05T20:47:14.887365: step 7547, loss 0.0140754, acc 1
2016-09-05T20:47:15.700694: step 7548, loss 0.0201189, acc 0.98
2016-09-05T20:47:16.509161: step 7549, loss 0.038748, acc 0.98
2016-09-05T20:47:17.321112: step 7550, loss 0.0192866, acc 0.98
2016-09-05T20:47:18.118047: step 7551, loss 0.00619381, acc 1
2016-09-05T20:47:18.939459: step 7552, loss 0.0207597, acc 0.98
2016-09-05T20:47:19.797129: step 7553, loss 0.02704, acc 1
2016-09-05T20:47:20.596232: step 7554, loss 0.0133428, acc 1
2016-09-05T20:47:21.404970: step 7555, loss 0.0123831, acc 1
2016-09-05T20:47:22.224022: step 7556, loss 0.0195345, acc 0.98
2016-09-05T20:47:23.037044: step 7557, loss 0.0197302, acc 1
2016-09-05T20:47:23.839304: step 7558, loss 0.0467384, acc 0.98
2016-09-05T20:47:24.644319: step 7559, loss 0.100145, acc 0.96
2016-09-05T20:47:25.477770: step 7560, loss 0.00237885, acc 1
2016-09-05T20:47:26.257980: step 7561, loss 0.00301138, acc 1
2016-09-05T20:47:27.090757: step 7562, loss 0.0490484, acc 0.96
2016-09-05T20:47:27.942650: step 7563, loss 0.0327946, acc 0.96
2016-09-05T20:47:28.721605: step 7564, loss 0.0068591, acc 1
2016-09-05T20:47:29.501424: step 7565, loss 0.0164472, acc 1
2016-09-05T20:47:29.935271: step 7566, loss 0.0969746, acc 0.916667
2016-09-05T20:47:30.719132: step 7567, loss 0.0277742, acc 0.98
2016-09-05T20:47:31.527293: step 7568, loss 0.00899971, acc 1
2016-09-05T20:47:32.320388: step 7569, loss 0.0344507, acc 1
2016-09-05T20:47:33.114837: step 7570, loss 0.0086597, acc 1
2016-09-05T20:47:33.939520: step 7571, loss 0.0212455, acc 0.98
2016-09-05T20:47:34.730904: step 7572, loss 0.0652382, acc 0.98
2016-09-05T20:47:35.577414: step 7573, loss 0.0299441, acc 0.98
2016-09-05T20:47:36.399493: step 7574, loss 0.0505707, acc 0.98
2016-09-05T20:47:37.206730: step 7575, loss 0.0169031, acc 1
2016-09-05T20:47:38.023094: step 7576, loss 0.0270072, acc 1
2016-09-05T20:47:38.852555: step 7577, loss 0.026927, acc 0.98
2016-09-05T20:47:39.636080: step 7578, loss 0.0313698, acc 0.98
2016-09-05T20:47:40.437712: step 7579, loss 0.0519829, acc 0.98
2016-09-05T20:47:41.305572: step 7580, loss 0.00888903, acc 1
2016-09-05T20:47:42.113415: step 7581, loss 0.0024976, acc 1
2016-09-05T20:47:42.943192: step 7582, loss 0.0196142, acc 0.98
2016-09-05T20:47:43.764331: step 7583, loss 0.0283884, acc 0.98
2016-09-05T20:47:44.599035: step 7584, loss 0.0741922, acc 0.98
2016-09-05T20:47:45.408516: step 7585, loss 0.0261267, acc 0.98
2016-09-05T20:47:46.244878: step 7586, loss 0.0172254, acc 1
2016-09-05T20:47:47.072021: step 7587, loss 0.0224868, acc 0.98
2016-09-05T20:47:47.883663: step 7588, loss 0.00524675, acc 1
2016-09-05T20:47:48.752656: step 7589, loss 0.00216199, acc 1
2016-09-05T20:47:49.532270: step 7590, loss 0.00231292, acc 1
2016-09-05T20:47:50.350624: step 7591, loss 0.0566533, acc 0.98
2016-09-05T20:47:51.181881: step 7592, loss 0.0245377, acc 0.98
2016-09-05T20:47:51.985369: step 7593, loss 0.0020282, acc 1
2016-09-05T20:47:52.814767: step 7594, loss 0.0718296, acc 0.96
2016-09-05T20:47:53.670889: step 7595, loss 0.0536481, acc 0.96
2016-09-05T20:47:54.486578: step 7596, loss 0.0271933, acc 1
2016-09-05T20:47:55.283875: step 7597, loss 0.0186473, acc 1
2016-09-05T20:47:56.150432: step 7598, loss 0.0274258, acc 1
2016-09-05T20:47:56.945789: step 7599, loss 0.029282, acc 0.98
2016-09-05T20:47:57.731400: step 7600, loss 0.0326388, acc 0.98

Evaluation:
2016-09-05T20:48:01.219982: step 7600, loss 1.68041, acc 0.726

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-7600

2016-09-05T20:48:03.157162: step 7601, loss 0.00580715, acc 1
2016-09-05T20:48:03.985937: step 7602, loss 0.0222295, acc 1
2016-09-05T20:48:04.810810: step 7603, loss 0.0660744, acc 0.98
2016-09-05T20:48:05.647046: step 7604, loss 0.0335495, acc 0.98
2016-09-05T20:48:06.452662: step 7605, loss 0.010594, acc 1
2016-09-05T20:48:07.314700: step 7606, loss 0.00281882, acc 1
2016-09-05T20:48:08.134047: step 7607, loss 0.0278487, acc 1
2016-09-05T20:48:08.963346: step 7608, loss 0.0609885, acc 0.96
2016-09-05T20:48:09.854604: step 7609, loss 0.0149205, acc 1
2016-09-05T20:48:10.684386: step 7610, loss 0.00241732, acc 1
2016-09-05T20:48:11.506534: step 7611, loss 0.0218221, acc 0.98
2016-09-05T20:48:12.314992: step 7612, loss 0.0280808, acc 0.98
2016-09-05T20:48:13.137501: step 7613, loss 0.0665223, acc 0.98
2016-09-05T20:48:13.951726: step 7614, loss 0.0296987, acc 1
2016-09-05T20:48:14.736003: step 7615, loss 0.00518522, acc 1
2016-09-05T20:48:15.545154: step 7616, loss 0.0301743, acc 0.98
2016-09-05T20:48:16.339212: step 7617, loss 0.0311801, acc 0.98
2016-09-05T20:48:17.114184: step 7618, loss 0.0213103, acc 0.98
2016-09-05T20:48:17.919873: step 7619, loss 0.0188127, acc 0.98
2016-09-05T20:48:18.745892: step 7620, loss 0.00891122, acc 1
2016-09-05T20:48:19.553439: step 7621, loss 0.00549611, acc 1
2016-09-05T20:48:20.382456: step 7622, loss 0.0433696, acc 0.96
2016-09-05T20:48:21.197593: step 7623, loss 0.0228663, acc 1
2016-09-05T20:48:21.994458: step 7624, loss 0.00335309, acc 1
2016-09-05T20:48:22.798929: step 7625, loss 0.0194804, acc 1
2016-09-05T20:48:23.608428: step 7626, loss 0.00711974, acc 1
2016-09-05T20:48:24.400463: step 7627, loss 0.00780252, acc 1
2016-09-05T20:48:25.209751: step 7628, loss 0.0297205, acc 0.98
2016-09-05T20:48:26.041817: step 7629, loss 0.0112335, acc 1
2016-09-05T20:48:26.851637: step 7630, loss 0.0109465, acc 1
2016-09-05T20:48:27.636752: step 7631, loss 0.00646878, acc 1
2016-09-05T20:48:28.463178: step 7632, loss 0.0615487, acc 0.96
2016-09-05T20:48:29.233701: step 7633, loss 0.00612269, acc 1
2016-09-05T20:48:30.044863: step 7634, loss 0.00927344, acc 1
2016-09-05T20:48:30.855052: step 7635, loss 0.012573, acc 1
2016-09-05T20:48:31.643278: step 7636, loss 0.00460553, acc 1
2016-09-05T20:48:32.455546: step 7637, loss 0.0027512, acc 1
2016-09-05T20:48:33.263162: step 7638, loss 0.0316398, acc 0.98
2016-09-05T20:48:34.070410: step 7639, loss 0.00266723, acc 1
2016-09-05T20:48:34.892906: step 7640, loss 0.0662576, acc 0.96
2016-09-05T20:48:35.727448: step 7641, loss 0.0271593, acc 0.98
2016-09-05T20:48:36.488669: step 7642, loss 0.0389626, acc 1
2016-09-05T20:48:37.324274: step 7643, loss 0.0355626, acc 0.98
2016-09-05T20:48:38.128038: step 7644, loss 0.00645555, acc 1
2016-09-05T20:48:38.921710: step 7645, loss 0.00608467, acc 1
2016-09-05T20:48:39.739485: step 7646, loss 0.0178268, acc 0.98
2016-09-05T20:48:40.543845: step 7647, loss 0.011547, acc 1
2016-09-05T20:48:41.328201: step 7648, loss 0.132219, acc 0.98
2016-09-05T20:48:42.124168: step 7649, loss 0.00980976, acc 1
2016-09-05T20:48:42.922187: step 7650, loss 0.024015, acc 0.98
2016-09-05T20:48:43.713887: step 7651, loss 0.00339326, acc 1
2016-09-05T20:48:44.507596: step 7652, loss 0.00386285, acc 1
2016-09-05T20:48:45.338031: step 7653, loss 0.00267975, acc 1
2016-09-05T20:48:46.144159: step 7654, loss 0.00308978, acc 1
2016-09-05T20:48:46.948640: step 7655, loss 0.0204443, acc 1
2016-09-05T20:48:47.777701: step 7656, loss 0.0261342, acc 0.98
2016-09-05T20:48:48.558561: step 7657, loss 0.0189904, acc 0.98
2016-09-05T20:48:49.366638: step 7658, loss 0.00279963, acc 1
2016-09-05T20:48:50.170859: step 7659, loss 0.00537038, acc 1
2016-09-05T20:48:50.959799: step 7660, loss 0.00623809, acc 1
2016-09-05T20:48:51.773037: step 7661, loss 0.0306123, acc 0.98
2016-09-05T20:48:52.572430: step 7662, loss 0.0260181, acc 0.98
2016-09-05T20:48:53.365723: step 7663, loss 0.0389539, acc 0.98
2016-09-05T20:48:54.189690: step 7664, loss 0.0548704, acc 0.96
2016-09-05T20:48:55.013542: step 7665, loss 0.0147219, acc 1
2016-09-05T20:48:55.799313: step 7666, loss 0.0340815, acc 0.98
2016-09-05T20:48:56.593925: step 7667, loss 0.0330362, acc 0.98
2016-09-05T20:48:57.410692: step 7668, loss 0.0039558, acc 1
2016-09-05T20:48:58.197063: step 7669, loss 0.0289399, acc 1
2016-09-05T20:48:59.001079: step 7670, loss 0.0622877, acc 0.98
2016-09-05T20:48:59.792317: step 7671, loss 0.0176945, acc 0.98
2016-09-05T20:49:00.606240: step 7672, loss 0.0283013, acc 1
2016-09-05T20:49:01.419790: step 7673, loss 0.0609868, acc 0.96
2016-09-05T20:49:02.239166: step 7674, loss 0.0116694, acc 1
2016-09-05T20:49:03.025872: step 7675, loss 0.0245811, acc 1
2016-09-05T20:49:03.849824: step 7676, loss 0.0204614, acc 1
2016-09-05T20:49:04.678349: step 7677, loss 0.0217053, acc 0.98
2016-09-05T20:49:05.470032: step 7678, loss 0.00714327, acc 1
2016-09-05T20:49:06.336045: step 7679, loss 0.0175333, acc 1
2016-09-05T20:49:07.232177: step 7680, loss 0.0786099, acc 0.98
2016-09-05T20:49:08.043380: step 7681, loss 0.0432483, acc 0.96
2016-09-05T20:49:08.862461: step 7682, loss 0.022292, acc 1
2016-09-05T20:49:09.704281: step 7683, loss 0.0233326, acc 1
2016-09-05T20:49:10.505514: step 7684, loss 0.032944, acc 0.98
2016-09-05T20:49:11.360562: step 7685, loss 0.00253757, acc 1
2016-09-05T20:49:12.195113: step 7686, loss 0.0422912, acc 0.98
2016-09-05T20:49:13.011547: step 7687, loss 0.0117173, acc 1
2016-09-05T20:49:13.815896: step 7688, loss 0.00732445, acc 1
2016-09-05T20:49:14.633521: step 7689, loss 0.0389016, acc 0.98
2016-09-05T20:49:15.454303: step 7690, loss 0.00315563, acc 1
2016-09-05T20:49:16.269413: step 7691, loss 0.0434845, acc 0.96
2016-09-05T20:49:17.105431: step 7692, loss 0.0207749, acc 1
2016-09-05T20:49:17.889467: step 7693, loss 0.0214915, acc 0.98
2016-09-05T20:49:18.708153: step 7694, loss 0.0167052, acc 1
2016-09-05T20:49:19.526029: step 7695, loss 0.00433428, acc 1
2016-09-05T20:49:20.333198: step 7696, loss 0.0416418, acc 0.98
2016-09-05T20:49:21.142268: step 7697, loss 0.0512993, acc 0.98
2016-09-05T20:49:21.963687: step 7698, loss 0.0214312, acc 1
2016-09-05T20:49:22.775446: step 7699, loss 0.00613015, acc 1
2016-09-05T20:49:23.562477: step 7700, loss 0.00453986, acc 1

Evaluation:
2016-09-05T20:49:27.140072: step 7700, loss 2.36531, acc 0.736

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-7700

2016-09-05T20:49:29.030957: step 7701, loss 0.0604505, acc 0.98
2016-09-05T20:49:29.795316: step 7702, loss 0.0355576, acc 0.98
2016-09-05T20:49:30.573354: step 7703, loss 0.00318593, acc 1
2016-09-05T20:49:31.380414: step 7704, loss 0.00355556, acc 1
2016-09-05T20:49:32.176327: step 7705, loss 0.011307, acc 1
2016-09-05T20:49:32.984650: step 7706, loss 0.0771911, acc 0.98
2016-09-05T20:49:33.804325: step 7707, loss 0.0394054, acc 0.98
2016-09-05T20:49:34.583572: step 7708, loss 0.030928, acc 1
2016-09-05T20:49:35.397299: step 7709, loss 0.0766564, acc 0.96
2016-09-05T20:49:36.199577: step 7710, loss 0.0160843, acc 1
2016-09-05T20:49:37.018002: step 7711, loss 0.049352, acc 0.98
2016-09-05T20:49:37.874468: step 7712, loss 0.00588369, acc 1
2016-09-05T20:49:38.670061: step 7713, loss 0.0318421, acc 1
2016-09-05T20:49:39.451123: step 7714, loss 0.0509842, acc 0.96
2016-09-05T20:49:40.255546: step 7715, loss 0.028646, acc 0.98
2016-09-05T20:49:41.073779: step 7716, loss 0.0237128, acc 0.98
2016-09-05T20:49:41.884065: step 7717, loss 0.0165759, acc 1
2016-09-05T20:49:42.677277: step 7718, loss 0.0687453, acc 0.98
2016-09-05T20:49:43.487762: step 7719, loss 0.0372628, acc 1
2016-09-05T20:49:44.287762: step 7720, loss 0.0268848, acc 1
2016-09-05T20:49:45.093833: step 7721, loss 0.0326629, acc 0.98
2016-09-05T20:49:45.907960: step 7722, loss 0.0110982, acc 1
2016-09-05T20:49:46.716200: step 7723, loss 0.0051473, acc 1
2016-09-05T20:49:47.530236: step 7724, loss 0.0212603, acc 0.98
2016-09-05T20:49:48.370743: step 7725, loss 0.00331516, acc 1
2016-09-05T20:49:49.136194: step 7726, loss 0.0206037, acc 1
2016-09-05T20:49:49.952244: step 7727, loss 0.00296116, acc 1
2016-09-05T20:49:50.748687: step 7728, loss 0.00716128, acc 1
2016-09-05T20:49:51.555501: step 7729, loss 0.00370627, acc 1
2016-09-05T20:49:52.364584: step 7730, loss 0.0217409, acc 0.98
2016-09-05T20:49:53.194596: step 7731, loss 0.0243682, acc 1
2016-09-05T20:49:53.966317: step 7732, loss 0.00334105, acc 1
2016-09-05T20:49:54.779402: step 7733, loss 0.012644, acc 1
2016-09-05T20:49:55.585400: step 7734, loss 0.0221, acc 1
2016-09-05T20:49:56.380166: step 7735, loss 0.00450009, acc 1
2016-09-05T20:49:57.213957: step 7736, loss 0.0134575, acc 1
2016-09-05T20:49:58.023840: step 7737, loss 0.0472403, acc 0.98
2016-09-05T20:49:58.795285: step 7738, loss 0.0133609, acc 1
2016-09-05T20:49:59.595733: step 7739, loss 0.038055, acc 0.96
2016-09-05T20:50:00.443074: step 7740, loss 0.0510009, acc 0.98
2016-09-05T20:50:01.242615: step 7741, loss 0.0142514, acc 1
2016-09-05T20:50:02.027820: step 7742, loss 0.00634368, acc 1
2016-09-05T20:50:02.842351: step 7743, loss 0.130272, acc 0.98
2016-09-05T20:50:03.621835: step 7744, loss 0.00648914, acc 1
2016-09-05T20:50:04.424099: step 7745, loss 0.0203996, acc 1
2016-09-05T20:50:05.261406: step 7746, loss 0.0157609, acc 1
2016-09-05T20:50:06.016868: step 7747, loss 0.00691003, acc 1
2016-09-05T20:50:06.836349: step 7748, loss 0.0170744, acc 1
2016-09-05T20:50:07.652506: step 7749, loss 0.00395394, acc 1
2016-09-05T20:50:08.491361: step 7750, loss 0.00341875, acc 1
2016-09-05T20:50:09.283890: step 7751, loss 0.0408032, acc 0.98
2016-09-05T20:50:10.106247: step 7752, loss 0.00296943, acc 1
2016-09-05T20:50:10.889537: step 7753, loss 0.0588677, acc 0.96
2016-09-05T20:50:11.695433: step 7754, loss 0.00614319, acc 1
2016-09-05T20:50:12.525286: step 7755, loss 0.0342166, acc 0.98
2016-09-05T20:50:13.327744: step 7756, loss 0.0497405, acc 0.96
2016-09-05T20:50:14.112198: step 7757, loss 0.0174362, acc 0.98
2016-09-05T20:50:14.921406: step 7758, loss 0.0316205, acc 0.98
2016-09-05T20:50:15.697941: step 7759, loss 0.00283071, acc 1
2016-09-05T20:50:16.144296: step 7760, loss 0.0576753, acc 1
2016-09-05T20:50:16.915956: step 7761, loss 0.0594801, acc 0.96
2016-09-05T20:50:17.710227: step 7762, loss 0.0078789, acc 1
2016-09-05T20:50:18.522113: step 7763, loss 0.00438871, acc 1
2016-09-05T20:50:19.316734: step 7764, loss 0.00960113, acc 1
2016-09-05T20:50:20.133474: step 7765, loss 0.00614719, acc 1
2016-09-05T20:50:20.988772: step 7766, loss 0.00741318, acc 1
2016-09-05T20:50:21.807372: step 7767, loss 0.0248896, acc 0.98
2016-09-05T20:50:22.622847: step 7768, loss 0.0069456, acc 1
2016-09-05T20:50:23.432001: step 7769, loss 0.0331778, acc 0.98
2016-09-05T20:50:24.239241: step 7770, loss 0.00746316, acc 1
2016-09-05T20:50:25.110980: step 7771, loss 0.00443875, acc 1
2016-09-05T20:50:25.955224: step 7772, loss 0.0849107, acc 0.96
2016-09-05T20:50:26.765724: step 7773, loss 0.0369414, acc 1
2016-09-05T20:50:27.586982: step 7774, loss 0.0258786, acc 0.98
2016-09-05T20:50:28.417400: step 7775, loss 0.00262336, acc 1
2016-09-05T20:50:29.217432: step 7776, loss 0.0468118, acc 0.98
2016-09-05T20:50:30.053134: step 7777, loss 0.0150777, acc 1
2016-09-05T20:50:30.877518: step 7778, loss 0.00951728, acc 1
2016-09-05T20:50:31.688885: step 7779, loss 0.0142893, acc 1
2016-09-05T20:50:32.502286: step 7780, loss 0.0164387, acc 1
2016-09-05T20:50:33.324117: step 7781, loss 0.018599, acc 1
2016-09-05T20:50:34.124749: step 7782, loss 0.0427638, acc 0.98
2016-09-05T20:50:34.934330: step 7783, loss 0.00508786, acc 1
2016-09-05T20:50:35.774886: step 7784, loss 0.0115691, acc 1
2016-09-05T20:50:36.564779: step 7785, loss 0.221438, acc 0.98
2016-09-05T20:50:37.382577: step 7786, loss 0.0101695, acc 1
2016-09-05T20:50:38.200371: step 7787, loss 0.00240369, acc 1
2016-09-05T20:50:38.974506: step 7788, loss 0.00641386, acc 1
2016-09-05T20:50:39.799238: step 7789, loss 0.0641043, acc 0.98
2016-09-05T20:50:40.631076: step 7790, loss 0.0479609, acc 0.98
2016-09-05T20:50:41.450366: step 7791, loss 0.0188581, acc 1
2016-09-05T20:50:42.234872: step 7792, loss 0.134807, acc 0.96
2016-09-05T20:50:43.066935: step 7793, loss 0.0189827, acc 1
2016-09-05T20:50:43.892468: step 7794, loss 0.00842412, acc 1
2016-09-05T20:50:44.679256: step 7795, loss 0.0189492, acc 1
2016-09-05T20:50:45.497562: step 7796, loss 0.0223943, acc 1
2016-09-05T20:50:46.318775: step 7797, loss 0.0778494, acc 0.98
2016-09-05T20:50:47.109434: step 7798, loss 0.0233466, acc 1
2016-09-05T20:50:47.913125: step 7799, loss 0.013103, acc 1
2016-09-05T20:50:48.739587: step 7800, loss 0.0211486, acc 1

Evaluation:
2016-09-05T20:50:52.224790: step 7800, loss 1.41689, acc 0.732

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-7800

2016-09-05T20:50:54.069437: step 7801, loss 0.0769193, acc 0.98
2016-09-05T20:50:54.901130: step 7802, loss 0.00684017, acc 1
2016-09-05T20:50:55.685917: step 7803, loss 0.00450072, acc 1
2016-09-05T20:50:56.482556: step 7804, loss 0.0163336, acc 1
2016-09-05T20:50:57.305666: step 7805, loss 0.0199668, acc 0.98
2016-09-05T20:50:58.111423: step 7806, loss 0.0331768, acc 0.98
2016-09-05T20:50:58.910231: step 7807, loss 0.00631527, acc 1
2016-09-05T20:50:59.738693: step 7808, loss 0.0341928, acc 0.98
2016-09-05T20:51:00.583282: step 7809, loss 0.0186642, acc 0.98
2016-09-05T20:51:01.382648: step 7810, loss 0.00855104, acc 1
2016-09-05T20:51:02.221132: step 7811, loss 0.0602274, acc 0.98
2016-09-05T20:51:03.047102: step 7812, loss 0.00552665, acc 1
2016-09-05T20:51:03.850667: step 7813, loss 0.0219818, acc 0.98
2016-09-05T20:51:04.679258: step 7814, loss 0.00679713, acc 1
2016-09-05T20:51:05.501825: step 7815, loss 0.0232012, acc 0.98
2016-09-05T20:51:06.331749: step 7816, loss 0.0340957, acc 1
2016-09-05T20:51:07.212373: step 7817, loss 0.0487181, acc 0.98
2016-09-05T20:51:08.051364: step 7818, loss 0.012608, acc 1
2016-09-05T20:51:08.877428: step 7819, loss 0.0646865, acc 0.98
2016-09-05T20:51:09.687938: step 7820, loss 0.0448571, acc 0.98
2016-09-05T20:51:10.545435: step 7821, loss 0.0194146, acc 1
2016-09-05T20:51:11.314242: step 7822, loss 0.00624634, acc 1
2016-09-05T20:51:12.102747: step 7823, loss 0.0242524, acc 0.98
2016-09-05T20:51:12.908050: step 7824, loss 0.0332913, acc 1
2016-09-05T20:51:13.689797: step 7825, loss 0.0449278, acc 0.96
2016-09-05T20:51:14.500925: step 7826, loss 0.0424261, acc 0.98
2016-09-05T20:51:15.296424: step 7827, loss 0.00458429, acc 1
2016-09-05T20:51:16.095877: step 7828, loss 0.0500532, acc 0.98
2016-09-05T20:51:16.907353: step 7829, loss 0.00509391, acc 1
2016-09-05T20:51:17.744234: step 7830, loss 0.0273186, acc 1
2016-09-05T20:51:18.520603: step 7831, loss 0.00287829, acc 1
2016-09-05T20:51:19.336755: step 7832, loss 0.0237133, acc 0.98
2016-09-05T20:51:20.153982: step 7833, loss 0.00271471, acc 1
2016-09-05T20:51:21.006523: step 7834, loss 0.0119474, acc 1
2016-09-05T20:51:21.844336: step 7835, loss 0.016951, acc 0.98
2016-09-05T20:51:22.651625: step 7836, loss 0.0537428, acc 0.98
2016-09-05T20:51:23.441084: step 7837, loss 0.0150189, acc 1
2016-09-05T20:51:24.237472: step 7838, loss 0.0500871, acc 0.98
2016-09-05T20:51:25.073501: step 7839, loss 0.0127355, acc 1
2016-09-05T20:51:25.870530: step 7840, loss 0.0173339, acc 1
2016-09-05T20:51:26.662655: step 7841, loss 0.030258, acc 1
2016-09-05T20:51:27.431918: step 7842, loss 0.00296835, acc 1
2016-09-05T20:51:28.226910: step 7843, loss 0.0174957, acc 1
2016-09-05T20:51:29.021733: step 7844, loss 0.0416423, acc 0.98
2016-09-05T20:51:29.847551: step 7845, loss 0.0212632, acc 0.98
2016-09-05T20:51:30.637262: step 7846, loss 0.0170914, acc 0.98
2016-09-05T20:51:31.447220: step 7847, loss 0.00892542, acc 1
2016-09-05T20:51:32.261024: step 7848, loss 0.00795151, acc 1
2016-09-05T20:51:33.048202: step 7849, loss 0.137197, acc 0.98
2016-09-05T20:51:33.841868: step 7850, loss 0.0198538, acc 0.98
2016-09-05T20:51:34.641694: step 7851, loss 0.0249089, acc 1
2016-09-05T20:51:35.437272: step 7852, loss 0.0677969, acc 0.98
2016-09-05T20:51:36.260229: step 7853, loss 0.00447119, acc 1
2016-09-05T20:51:37.084550: step 7854, loss 0.0160837, acc 1
2016-09-05T20:51:37.879982: step 7855, loss 0.0133664, acc 1
2016-09-05T20:51:38.671682: step 7856, loss 0.00914758, acc 1
2016-09-05T20:51:39.496893: step 7857, loss 0.00891174, acc 1
2016-09-05T20:51:40.281399: step 7858, loss 0.033032, acc 1
2016-09-05T20:51:41.087835: step 7859, loss 0.0399206, acc 0.98
2016-09-05T20:51:41.947145: step 7860, loss 0.0320988, acc 0.96
2016-09-05T20:51:42.742247: step 7861, loss 0.0106733, acc 1
2016-09-05T20:51:43.540634: step 7862, loss 0.0117629, acc 1
2016-09-05T20:51:44.327122: step 7863, loss 0.0245088, acc 1
2016-09-05T20:51:45.136283: step 7864, loss 0.0047934, acc 1
2016-09-05T20:51:45.955300: step 7865, loss 0.0750402, acc 0.94
2016-09-05T20:51:46.797440: step 7866, loss 0.0113959, acc 1
2016-09-05T20:51:47.582506: step 7867, loss 0.00461151, acc 1
2016-09-05T20:51:48.380447: step 7868, loss 0.0194032, acc 0.98
2016-09-05T20:51:49.213409: step 7869, loss 0.0480679, acc 0.98
2016-09-05T20:51:50.002293: step 7870, loss 0.0335455, acc 0.98
2016-09-05T20:51:50.815212: step 7871, loss 0.0454924, acc 0.96
2016-09-05T20:51:51.657162: step 7872, loss 0.00358999, acc 1
2016-09-05T20:51:52.459725: step 7873, loss 0.0380476, acc 0.98
2016-09-05T20:51:53.263250: step 7874, loss 0.00414896, acc 1
2016-09-05T20:51:54.087542: step 7875, loss 0.0177488, acc 0.98
2016-09-05T20:51:54.887915: step 7876, loss 0.0661205, acc 0.96
2016-09-05T20:51:55.678843: step 7877, loss 0.0254386, acc 1
2016-09-05T20:51:56.511312: step 7878, loss 0.00975528, acc 1
2016-09-05T20:51:57.317627: step 7879, loss 0.0198436, acc 1
2016-09-05T20:51:58.132924: step 7880, loss 0.0704991, acc 0.98
2016-09-05T20:51:58.968289: step 7881, loss 0.00402033, acc 1
2016-09-05T20:51:59.787333: step 7882, loss 0.0143729, acc 1
2016-09-05T20:52:00.641615: step 7883, loss 0.00371301, acc 1
2016-09-05T20:52:01.457131: step 7884, loss 0.00249592, acc 1
2016-09-05T20:52:02.249782: step 7885, loss 0.0276447, acc 1
2016-09-05T20:52:03.060496: step 7886, loss 0.00431914, acc 1
2016-09-05T20:52:03.887349: step 7887, loss 0.0514216, acc 0.98
2016-09-05T20:52:04.703296: step 7888, loss 0.00799363, acc 1
2016-09-05T20:52:05.510215: step 7889, loss 0.00382017, acc 1
2016-09-05T20:52:06.344399: step 7890, loss 0.0249964, acc 0.98
2016-09-05T20:52:07.159172: step 7891, loss 0.0202761, acc 1
2016-09-05T20:52:07.959863: step 7892, loss 0.0457352, acc 0.98
2016-09-05T20:52:08.789014: step 7893, loss 0.00300221, acc 1
2016-09-05T20:52:09.635929: step 7894, loss 0.00612019, acc 1
2016-09-05T20:52:10.438232: step 7895, loss 0.0726717, acc 0.94
2016-09-05T20:52:11.292367: step 7896, loss 0.0233849, acc 0.98
2016-09-05T20:52:12.085409: step 7897, loss 0.00355641, acc 1
2016-09-05T20:52:12.897309: step 7898, loss 0.00398166, acc 1
2016-09-05T20:52:13.722088: step 7899, loss 0.00277247, acc 1
2016-09-05T20:52:14.540582: step 7900, loss 0.0440434, acc 0.96

Evaluation:
2016-09-05T20:52:18.083072: step 7900, loss 2.33648, acc 0.732

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-7900

2016-09-05T20:52:19.975241: step 7901, loss 0.0067433, acc 1
2016-09-05T20:52:20.808873: step 7902, loss 0.0304373, acc 1
2016-09-05T20:52:21.612884: step 7903, loss 0.00363055, acc 1
2016-09-05T20:52:22.435172: step 7904, loss 0.00273012, acc 1
2016-09-05T20:52:23.259198: step 7905, loss 0.020456, acc 1
2016-09-05T20:52:24.066493: step 7906, loss 0.00474833, acc 1
2016-09-05T20:52:24.877746: step 7907, loss 0.00408667, acc 1
2016-09-05T20:52:25.724505: step 7908, loss 0.00292806, acc 1
2016-09-05T20:52:26.521946: step 7909, loss 0.00293267, acc 1
2016-09-05T20:52:27.343530: step 7910, loss 0.0131285, acc 1
2016-09-05T20:52:28.187758: step 7911, loss 0.0166926, acc 1
2016-09-05T20:52:29.026943: step 7912, loss 0.0572034, acc 0.96
2016-09-05T20:52:29.819697: step 7913, loss 0.0848295, acc 0.98
2016-09-05T20:52:30.654040: step 7914, loss 0.0558688, acc 0.96
2016-09-05T20:52:31.461358: step 7915, loss 0.17988, acc 0.96
2016-09-05T20:52:32.281398: step 7916, loss 0.00262326, acc 1
2016-09-05T20:52:33.092862: step 7917, loss 0.00289289, acc 1
2016-09-05T20:52:33.907990: step 7918, loss 0.0267037, acc 1
2016-09-05T20:52:34.701574: step 7919, loss 0.00251825, acc 1
2016-09-05T20:52:35.527979: step 7920, loss 0.0474306, acc 0.96
2016-09-05T20:52:36.355380: step 7921, loss 0.0217973, acc 1
2016-09-05T20:52:37.148030: step 7922, loss 0.0782874, acc 0.98
2016-09-05T20:52:37.954281: step 7923, loss 0.0085502, acc 1
2016-09-05T20:52:38.781694: step 7924, loss 0.00823359, acc 1
2016-09-05T20:52:39.580994: step 7925, loss 0.00799577, acc 1
2016-09-05T20:52:40.405791: step 7926, loss 0.0429541, acc 0.98
2016-09-05T20:52:41.205436: step 7927, loss 0.0135063, acc 1
2016-09-05T20:52:42.005087: step 7928, loss 0.00760401, acc 1
2016-09-05T20:52:42.785612: step 7929, loss 0.00791888, acc 1
2016-09-05T20:52:43.629340: step 7930, loss 0.0809494, acc 0.98
2016-09-05T20:52:44.399809: step 7931, loss 0.0213143, acc 0.98
2016-09-05T20:52:45.203383: step 7932, loss 0.0261155, acc 0.98
2016-09-05T20:52:45.998134: step 7933, loss 0.024419, acc 0.98
2016-09-05T20:52:46.788502: step 7934, loss 0.0131479, acc 1
2016-09-05T20:52:47.601018: step 7935, loss 0.0124887, acc 1
2016-09-05T20:52:48.414476: step 7936, loss 0.00890384, acc 1
2016-09-05T20:52:49.208957: step 7937, loss 0.00450864, acc 1
2016-09-05T20:52:50.012825: step 7938, loss 0.00381026, acc 1
2016-09-05T20:52:50.830801: step 7939, loss 0.00392956, acc 1
2016-09-05T20:52:51.615504: step 7940, loss 0.0183249, acc 1
2016-09-05T20:52:52.460036: step 7941, loss 0.0324396, acc 0.98
2016-09-05T20:52:53.276721: step 7942, loss 0.0263012, acc 0.98
2016-09-05T20:52:54.058167: step 7943, loss 0.01687, acc 1
2016-09-05T20:52:54.863420: step 7944, loss 0.0133365, acc 1
2016-09-05T20:52:55.682925: step 7945, loss 0.00492775, acc 1
2016-09-05T20:52:56.473230: step 7946, loss 0.0243995, acc 1
2016-09-05T20:52:57.264115: step 7947, loss 0.0053361, acc 1
2016-09-05T20:52:58.061410: step 7948, loss 0.0624016, acc 0.98
2016-09-05T20:52:58.854692: step 7949, loss 0.0137889, acc 1
2016-09-05T20:52:59.730235: step 7950, loss 0.02847, acc 1
2016-09-05T20:53:00.566257: step 7951, loss 0.0124908, acc 1
2016-09-05T20:53:01.337303: step 7952, loss 0.0477407, acc 0.96
2016-09-05T20:53:02.174582: step 7953, loss 0.00420457, acc 1
2016-09-05T20:53:02.615870: step 7954, loss 0.0648008, acc 0.916667
2016-09-05T20:53:03.420424: step 7955, loss 0.0426806, acc 0.98
2016-09-05T20:53:04.274981: step 7956, loss 0.0379849, acc 0.96
2016-09-05T20:53:05.088392: step 7957, loss 0.0295654, acc 0.98
2016-09-05T20:53:05.910013: step 7958, loss 0.022544, acc 0.98
2016-09-05T20:53:06.761494: step 7959, loss 0.0543459, acc 0.98
2016-09-05T20:53:07.573444: step 7960, loss 0.0274496, acc 1
2016-09-05T20:53:08.365424: step 7961, loss 0.0209279, acc 0.98
2016-09-05T20:53:09.169929: step 7962, loss 0.0167939, acc 1
2016-09-05T20:53:09.999300: step 7963, loss 0.00619361, acc 1
2016-09-05T20:53:10.811093: step 7964, loss 0.0647128, acc 0.96
2016-09-05T20:53:11.647436: step 7965, loss 0.0381345, acc 0.98
2016-09-05T20:53:12.458968: step 7966, loss 0.0371712, acc 0.98
2016-09-05T20:53:13.279169: step 7967, loss 0.0874466, acc 0.94
2016-09-05T20:53:14.121356: step 7968, loss 0.00661218, acc 1
2016-09-05T20:53:14.957136: step 7969, loss 0.00810486, acc 1
2016-09-05T20:53:15.759477: step 7970, loss 0.00944483, acc 1
2016-09-05T20:53:16.568889: step 7971, loss 0.0253258, acc 1
2016-09-05T20:53:17.373456: step 7972, loss 0.00528387, acc 1
2016-09-05T20:53:18.187173: step 7973, loss 0.0483129, acc 0.98
2016-09-05T20:53:19.034171: step 7974, loss 0.0438482, acc 0.98
2016-09-05T20:53:19.850412: step 7975, loss 0.0152351, acc 1
2016-09-05T20:53:20.617690: step 7976, loss 0.040282, acc 0.98
2016-09-05T20:53:21.415179: step 7977, loss 0.0256125, acc 1
2016-09-05T20:53:22.235781: step 7978, loss 0.0111476, acc 1
2016-09-05T20:53:23.057455: step 7979, loss 0.0290559, acc 1
2016-09-05T20:53:23.858320: step 7980, loss 0.00499896, acc 1
2016-09-05T20:53:24.685450: step 7981, loss 0.00482715, acc 1
2016-09-05T20:53:25.505164: step 7982, loss 0.137905, acc 0.98
2016-09-05T20:53:26.309316: step 7983, loss 0.00466388, acc 1
2016-09-05T20:53:27.150042: step 7984, loss 0.00720724, acc 1
2016-09-05T20:53:27.925736: step 7985, loss 0.0104721, acc 1
2016-09-05T20:53:28.726466: step 7986, loss 0.0948988, acc 0.94
2016-09-05T20:53:29.528432: step 7987, loss 0.0204145, acc 1
2016-09-05T20:53:30.306570: step 7988, loss 0.071786, acc 0.98
2016-09-05T20:53:31.097849: step 7989, loss 0.00708342, acc 1
2016-09-05T20:53:31.897752: step 7990, loss 0.0879297, acc 0.94
2016-09-05T20:53:32.697520: step 7991, loss 0.0120728, acc 1
2016-09-05T20:53:33.521575: step 7992, loss 0.00941615, acc 1
2016-09-05T20:53:34.331164: step 7993, loss 0.00406847, acc 1
2016-09-05T20:53:35.107434: step 7994, loss 0.00432992, acc 1
2016-09-05T20:53:35.930804: step 7995, loss 0.00926196, acc 1
2016-09-05T20:53:36.752209: step 7996, loss 0.0229281, acc 1
2016-09-05T20:53:37.527687: step 7997, loss 0.0482715, acc 0.98
2016-09-05T20:53:38.336811: step 7998, loss 0.00665903, acc 1
2016-09-05T20:53:39.141533: step 7999, loss 0.0288637, acc 0.98
2016-09-05T20:53:39.956323: step 8000, loss 0.011432, acc 1

Evaluation:
2016-09-05T20:53:43.443193: step 8000, loss 2.04448, acc 0.741

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-8000

2016-09-05T20:53:45.345451: step 8001, loss 0.0120626, acc 1
2016-09-05T20:53:46.141788: step 8002, loss 0.0187251, acc 1
2016-09-05T20:53:46.950976: step 8003, loss 0.00630075, acc 1
2016-09-05T20:53:47.780549: step 8004, loss 0.0181868, acc 1
2016-09-05T20:53:48.565578: step 8005, loss 0.0120982, acc 1
2016-09-05T20:53:49.399638: step 8006, loss 0.00383174, acc 1
2016-09-05T20:53:50.197656: step 8007, loss 0.013129, acc 1
2016-09-05T20:53:50.963351: step 8008, loss 0.0474367, acc 0.98
2016-09-05T20:53:51.774632: step 8009, loss 0.0246174, acc 0.98
2016-09-05T20:53:52.614307: step 8010, loss 0.0259393, acc 0.98
2016-09-05T20:53:53.412452: step 8011, loss 0.0259513, acc 0.98
2016-09-05T20:53:54.233165: step 8012, loss 0.0163178, acc 1
2016-09-05T20:53:55.085521: step 8013, loss 0.0147355, acc 1
2016-09-05T20:53:55.870545: step 8014, loss 0.0205991, acc 0.98
2016-09-05T20:53:56.675474: step 8015, loss 0.0382574, acc 0.96
2016-09-05T20:53:57.501023: step 8016, loss 0.00369285, acc 1
2016-09-05T20:53:58.296231: step 8017, loss 0.0132472, acc 1
2016-09-05T20:53:59.094219: step 8018, loss 0.031539, acc 0.98
2016-09-05T20:53:59.919995: step 8019, loss 0.119943, acc 0.98
2016-09-05T20:54:00.740050: step 8020, loss 0.0088436, acc 1
2016-09-05T20:54:01.537324: step 8021, loss 0.00344287, acc 1
2016-09-05T20:54:02.353452: step 8022, loss 0.0191269, acc 1
2016-09-05T20:54:03.186404: step 8023, loss 0.0205252, acc 0.98
2016-09-05T20:54:03.996142: step 8024, loss 0.0965534, acc 0.98
2016-09-05T20:54:04.856119: step 8025, loss 0.0634516, acc 0.96
2016-09-05T20:54:05.626956: step 8026, loss 0.0827129, acc 0.98
2016-09-05T20:54:06.429265: step 8027, loss 0.0289038, acc 0.98
2016-09-05T20:54:07.263939: step 8028, loss 0.0130289, acc 1
2016-09-05T20:54:08.051009: step 8029, loss 0.0100604, acc 1
2016-09-05T20:54:08.867812: step 8030, loss 0.0426322, acc 0.98
2016-09-05T20:54:09.692735: step 8031, loss 0.0557472, acc 0.98
2016-09-05T20:54:10.476468: step 8032, loss 0.0258694, acc 0.98
2016-09-05T20:54:11.272884: step 8033, loss 0.0119902, acc 1
2016-09-05T20:54:12.121029: step 8034, loss 0.00570969, acc 1
2016-09-05T20:54:12.901783: step 8035, loss 0.0169119, acc 1
2016-09-05T20:54:13.688215: step 8036, loss 0.00530817, acc 1
2016-09-05T20:54:14.530199: step 8037, loss 0.00572387, acc 1
2016-09-05T20:54:15.341426: step 8038, loss 0.00302532, acc 1
2016-09-05T20:54:16.145117: step 8039, loss 0.0163865, acc 1
2016-09-05T20:54:16.937413: step 8040, loss 0.0144215, acc 1
2016-09-05T20:54:17.776560: step 8041, loss 0.0893517, acc 0.96
2016-09-05T20:54:18.595792: step 8042, loss 0.0336222, acc 1
2016-09-05T20:54:19.421276: step 8043, loss 0.0109096, acc 1
2016-09-05T20:54:20.221283: step 8044, loss 0.020144, acc 1
2016-09-05T20:54:21.029572: step 8045, loss 0.0493878, acc 0.96
2016-09-05T20:54:21.869427: step 8046, loss 0.00787314, acc 1
2016-09-05T20:54:22.661027: step 8047, loss 0.00924304, acc 1
2016-09-05T20:54:23.490022: step 8048, loss 0.0227742, acc 1
2016-09-05T20:54:24.312895: step 8049, loss 0.0973088, acc 0.94
2016-09-05T20:54:25.136927: step 8050, loss 0.0286303, acc 0.98
2016-09-05T20:54:25.934691: step 8051, loss 0.00641264, acc 1
2016-09-05T20:54:26.787177: step 8052, loss 0.0515271, acc 0.98
2016-09-05T20:54:27.594328: step 8053, loss 0.00503447, acc 1
2016-09-05T20:54:28.393575: step 8054, loss 0.00542881, acc 1
2016-09-05T20:54:29.248266: step 8055, loss 0.00448077, acc 1
2016-09-05T20:54:30.109711: step 8056, loss 0.00985446, acc 1
2016-09-05T20:54:30.902003: step 8057, loss 0.0352902, acc 0.98
2016-09-05T20:54:31.697992: step 8058, loss 0.0316248, acc 1
2016-09-05T20:54:32.519824: step 8059, loss 0.012883, acc 1
2016-09-05T20:54:33.310768: step 8060, loss 0.00706804, acc 1
2016-09-05T20:54:34.111333: step 8061, loss 0.00441404, acc 1
2016-09-05T20:54:34.907510: step 8062, loss 0.0437337, acc 0.98
2016-09-05T20:54:35.672353: step 8063, loss 0.00550851, acc 1
2016-09-05T20:54:36.491157: step 8064, loss 0.00403363, acc 1
2016-09-05T20:54:37.298337: step 8065, loss 0.0262883, acc 0.98
2016-09-05T20:54:38.163921: step 8066, loss 0.0234424, acc 1
2016-09-05T20:54:39.000800: step 8067, loss 0.0571778, acc 0.96
2016-09-05T20:54:39.834826: step 8068, loss 0.0560427, acc 0.96
2016-09-05T20:54:40.631528: step 8069, loss 0.00913471, acc 1
2016-09-05T20:54:41.440978: step 8070, loss 0.0227765, acc 1
2016-09-05T20:54:42.288106: step 8071, loss 0.0416451, acc 0.98
2016-09-05T20:54:43.114084: step 8072, loss 0.0261811, acc 0.98
2016-09-05T20:54:43.923742: step 8073, loss 0.0116599, acc 1
2016-09-05T20:54:44.746652: step 8074, loss 0.0203103, acc 1
2016-09-05T20:54:45.548938: step 8075, loss 0.0280572, acc 1
2016-09-05T20:54:46.369087: step 8076, loss 0.0416677, acc 0.98
2016-09-05T20:54:47.176489: step 8077, loss 0.00334898, acc 1
2016-09-05T20:54:47.973260: step 8078, loss 0.00364058, acc 1
2016-09-05T20:54:48.787492: step 8079, loss 0.0466876, acc 0.98
2016-09-05T20:54:49.606653: step 8080, loss 0.00351129, acc 1
2016-09-05T20:54:50.417404: step 8081, loss 0.015497, acc 1
2016-09-05T20:54:51.224222: step 8082, loss 0.0112828, acc 1
2016-09-05T20:54:52.048978: step 8083, loss 0.00403935, acc 1
2016-09-05T20:54:52.846601: step 8084, loss 0.0345689, acc 0.98
2016-09-05T20:54:53.660343: step 8085, loss 0.019354, acc 1
2016-09-05T20:54:54.490193: step 8086, loss 0.0110853, acc 1
2016-09-05T20:54:55.286758: step 8087, loss 0.00411898, acc 1
2016-09-05T20:54:56.101939: step 8088, loss 0.00412072, acc 1
2016-09-05T20:54:56.954644: step 8089, loss 0.0220368, acc 0.98
2016-09-05T20:54:57.802459: step 8090, loss 0.0184637, acc 0.98
2016-09-05T20:54:58.623859: step 8091, loss 0.0477403, acc 0.98
2016-09-05T20:54:59.482991: step 8092, loss 0.00325866, acc 1
2016-09-05T20:55:00.298283: step 8093, loss 0.0198429, acc 0.98
2016-09-05T20:55:01.104754: step 8094, loss 0.0373118, acc 0.98
2016-09-05T20:55:01.931125: step 8095, loss 0.00413177, acc 1
2016-09-05T20:55:02.751377: step 8096, loss 0.0543906, acc 0.96
2016-09-05T20:55:03.549778: step 8097, loss 0.00828077, acc 1
2016-09-05T20:55:04.363618: step 8098, loss 0.0371727, acc 0.98
2016-09-05T20:55:05.165119: step 8099, loss 0.00618569, acc 1
2016-09-05T20:55:05.973753: step 8100, loss 0.00517865, acc 1

Evaluation:
2016-09-05T20:55:09.499825: step 8100, loss 2.54836, acc 0.722

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-8100

2016-09-05T20:55:11.454460: step 8101, loss 0.0349035, acc 0.98
2016-09-05T20:55:12.283808: step 8102, loss 0.0182329, acc 0.98
2016-09-05T20:55:13.102752: step 8103, loss 0.0277242, acc 0.98
2016-09-05T20:55:13.950902: step 8104, loss 0.00298163, acc 1
2016-09-05T20:55:14.783884: step 8105, loss 0.0286835, acc 0.98
2016-09-05T20:55:15.617153: step 8106, loss 0.0535248, acc 0.98
2016-09-05T20:55:16.423197: step 8107, loss 0.017148, acc 1
2016-09-05T20:55:17.213996: step 8108, loss 0.15487, acc 0.92
2016-09-05T20:55:17.993708: step 8109, loss 0.0296837, acc 0.98
2016-09-05T20:55:18.848658: step 8110, loss 0.00298604, acc 1
2016-09-05T20:55:19.661721: step 8111, loss 0.00257679, acc 1
2016-09-05T20:55:20.502035: step 8112, loss 0.00448964, acc 1
2016-09-05T20:55:21.303748: step 8113, loss 0.0105333, acc 1
2016-09-05T20:55:22.119668: step 8114, loss 0.0175865, acc 1
2016-09-05T20:55:22.918863: step 8115, loss 0.0124743, acc 1
2016-09-05T20:55:23.720552: step 8116, loss 0.0198651, acc 1
2016-09-05T20:55:24.538812: step 8117, loss 0.00630704, acc 1
2016-09-05T20:55:25.325537: step 8118, loss 0.0952562, acc 0.96
2016-09-05T20:55:26.125457: step 8119, loss 0.00762861, acc 1
2016-09-05T20:55:26.933629: step 8120, loss 0.0537209, acc 0.96
2016-09-05T20:55:27.704881: step 8121, loss 0.0055805, acc 1
2016-09-05T20:55:28.526540: step 8122, loss 0.015905, acc 1
2016-09-05T20:55:29.338750: step 8123, loss 0.00594419, acc 1
2016-09-05T20:55:30.141062: step 8124, loss 0.0416234, acc 0.98
2016-09-05T20:55:30.957858: step 8125, loss 0.0185564, acc 0.98
2016-09-05T20:55:31.781672: step 8126, loss 0.0207834, acc 0.98
2016-09-05T20:55:32.569005: step 8127, loss 0.00818747, acc 1
2016-09-05T20:55:33.361580: step 8128, loss 0.00503433, acc 1
2016-09-05T20:55:34.174551: step 8129, loss 0.0794435, acc 0.94
2016-09-05T20:55:34.961307: step 8130, loss 0.00863247, acc 1
2016-09-05T20:55:35.758956: step 8131, loss 0.00960732, acc 1
2016-09-05T20:55:36.586291: step 8132, loss 0.0153107, acc 1
2016-09-05T20:55:37.384847: step 8133, loss 0.00623958, acc 1
2016-09-05T20:55:38.179411: step 8134, loss 0.0248684, acc 0.98
2016-09-05T20:55:38.998427: step 8135, loss 0.047153, acc 0.98
2016-09-05T20:55:39.797634: step 8136, loss 0.00397441, acc 1
2016-09-05T20:55:40.607789: step 8137, loss 0.00662953, acc 1
2016-09-05T20:55:41.442161: step 8138, loss 0.00774277, acc 1
2016-09-05T20:55:42.244897: step 8139, loss 0.0161616, acc 1
2016-09-05T20:55:43.051778: step 8140, loss 0.0204654, acc 0.98
2016-09-05T20:55:43.864745: step 8141, loss 0.0376918, acc 0.98
2016-09-05T20:55:44.642182: step 8142, loss 0.00950247, acc 1
2016-09-05T20:55:45.444364: step 8143, loss 0.00267405, acc 1
2016-09-05T20:55:46.263011: step 8144, loss 0.0100967, acc 1
2016-09-05T20:55:47.046979: step 8145, loss 0.0287559, acc 0.98
2016-09-05T20:55:47.842327: step 8146, loss 0.0413111, acc 0.96
2016-09-05T20:55:48.630793: step 8147, loss 0.00317577, acc 1
2016-09-05T20:55:49.065744: step 8148, loss 0.0971385, acc 0.916667
2016-09-05T20:55:49.906622: step 8149, loss 0.03688, acc 0.96
2016-09-05T20:55:50.687468: step 8150, loss 0.0276447, acc 1
2016-09-05T20:55:51.491843: step 8151, loss 0.0308826, acc 0.98
2016-09-05T20:55:52.293614: step 8152, loss 0.0034595, acc 1
2016-09-05T20:55:53.077019: step 8153, loss 0.0330376, acc 0.98
2016-09-05T20:55:53.897960: step 8154, loss 0.0428012, acc 0.96
2016-09-05T20:55:54.730698: step 8155, loss 0.0026106, acc 1
2016-09-05T20:55:55.511812: step 8156, loss 0.0251063, acc 1
2016-09-05T20:55:56.304668: step 8157, loss 0.00262036, acc 1
2016-09-05T20:55:57.099253: step 8158, loss 0.00273132, acc 1
2016-09-05T20:55:57.927417: step 8159, loss 0.0218088, acc 0.98
2016-09-05T20:55:58.779786: step 8160, loss 0.0171455, acc 1
2016-09-05T20:55:59.605707: step 8161, loss 0.0368838, acc 0.98
2016-09-05T20:56:00.409901: step 8162, loss 0.0497542, acc 0.96
2016-09-05T20:56:01.204799: step 8163, loss 0.00268332, acc 1
2016-09-05T20:56:02.016262: step 8164, loss 0.00272534, acc 1
2016-09-05T20:56:02.796990: step 8165, loss 0.00825576, acc 1
2016-09-05T20:56:03.595303: step 8166, loss 0.00327352, acc 1
2016-09-05T20:56:04.414293: step 8167, loss 0.00717278, acc 1
2016-09-05T20:56:05.199435: step 8168, loss 0.177097, acc 0.98
2016-09-05T20:56:06.004213: step 8169, loss 0.0231403, acc 1
2016-09-05T20:56:06.837715: step 8170, loss 0.0415738, acc 0.96
2016-09-05T20:56:07.611441: step 8171, loss 0.00241571, acc 1
2016-09-05T20:56:08.451136: step 8172, loss 0.00992789, acc 1
2016-09-05T20:56:09.261832: step 8173, loss 0.0123971, acc 1
2016-09-05T20:56:10.024727: step 8174, loss 0.00248928, acc 1
2016-09-05T20:56:10.814955: step 8175, loss 0.0206007, acc 0.98
2016-09-05T20:56:11.643959: step 8176, loss 0.00240019, acc 1
2016-09-05T20:56:12.448126: step 8177, loss 0.00410249, acc 1
2016-09-05T20:56:13.281493: step 8178, loss 0.0345811, acc 1
2016-09-05T20:56:14.089822: step 8179, loss 0.023347, acc 1
2016-09-05T20:56:14.860009: step 8180, loss 0.0219894, acc 1
2016-09-05T20:56:15.651196: step 8181, loss 0.0682805, acc 0.98
2016-09-05T20:56:16.487797: step 8182, loss 0.0497209, acc 0.98
2016-09-05T20:56:17.272200: step 8183, loss 0.034182, acc 0.98
2016-09-05T20:56:18.091354: step 8184, loss 0.0283056, acc 1
2016-09-05T20:56:18.901808: step 8185, loss 0.00323126, acc 1
2016-09-05T20:56:19.682001: step 8186, loss 0.00294194, acc 1
2016-09-05T20:56:20.493663: step 8187, loss 0.00262185, acc 1
2016-09-05T20:56:21.317672: step 8188, loss 0.00279278, acc 1
2016-09-05T20:56:22.105074: step 8189, loss 0.0178077, acc 0.98
2016-09-05T20:56:22.897234: step 8190, loss 0.0245343, acc 1
2016-09-05T20:56:23.734094: step 8191, loss 0.00381914, acc 1
2016-09-05T20:56:24.531118: step 8192, loss 0.0172849, acc 1
2016-09-05T20:56:25.359693: step 8193, loss 0.0211068, acc 1
2016-09-05T20:56:26.176082: step 8194, loss 0.0183487, acc 1
2016-09-05T20:56:26.969926: step 8195, loss 0.0150133, acc 1
2016-09-05T20:56:27.750129: step 8196, loss 0.0179588, acc 0.98
2016-09-05T20:56:28.559304: step 8197, loss 0.00344712, acc 1
2016-09-05T20:56:29.373808: step 8198, loss 0.0391259, acc 1
2016-09-05T20:56:30.173083: step 8199, loss 0.0344155, acc 0.98
2016-09-05T20:56:30.988805: step 8200, loss 0.00564358, acc 1

Evaluation:
2016-09-05T20:56:34.491143: step 8200, loss 2.33179, acc 0.724

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-8200

2016-09-05T20:56:36.387664: step 8201, loss 0.00256378, acc 1
2016-09-05T20:56:37.225062: step 8202, loss 0.00612645, acc 1
2016-09-05T20:56:38.043771: step 8203, loss 0.0562273, acc 0.96
2016-09-05T20:56:38.858267: step 8204, loss 0.0701064, acc 0.98
2016-09-05T20:56:39.707416: step 8205, loss 0.139824, acc 0.98
2016-09-05T20:56:40.505426: step 8206, loss 0.0105566, acc 1
2016-09-05T20:56:41.315137: step 8207, loss 0.0112338, acc 1
2016-09-05T20:56:42.142732: step 8208, loss 0.00295309, acc 1
2016-09-05T20:56:42.946396: step 8209, loss 0.021845, acc 1
2016-09-05T20:56:43.780666: step 8210, loss 0.00353234, acc 1
2016-09-05T20:56:44.650554: step 8211, loss 0.0173457, acc 0.98
2016-09-05T20:56:45.482401: step 8212, loss 0.0281534, acc 0.98
2016-09-05T20:56:46.277993: step 8213, loss 0.0303708, acc 1
2016-09-05T20:56:47.071465: step 8214, loss 0.0149738, acc 1
2016-09-05T20:56:47.878427: step 8215, loss 0.029332, acc 0.98
2016-09-05T20:56:48.690314: step 8216, loss 0.0963913, acc 0.96
2016-09-05T20:56:49.540781: step 8217, loss 0.0235478, acc 0.98
2016-09-05T20:56:50.377713: step 8218, loss 0.0411848, acc 0.98
2016-09-05T20:56:51.179123: step 8219, loss 0.0028982, acc 1
2016-09-05T20:56:51.962124: step 8220, loss 0.00368973, acc 1
2016-09-05T20:56:52.793288: step 8221, loss 0.00490517, acc 1
2016-09-05T20:56:53.571784: step 8222, loss 0.0170815, acc 1
2016-09-05T20:56:54.372932: step 8223, loss 0.0267425, acc 0.98
2016-09-05T20:56:55.180294: step 8224, loss 0.117063, acc 0.96
2016-09-05T20:56:55.970655: step 8225, loss 0.0154151, acc 1
2016-09-05T20:56:56.764032: step 8226, loss 0.00251217, acc 1
2016-09-05T20:56:57.569474: step 8227, loss 0.0189931, acc 1
2016-09-05T20:56:58.354289: step 8228, loss 0.0580865, acc 0.98
2016-09-05T20:56:59.162894: step 8229, loss 0.00589091, acc 1
2016-09-05T20:56:59.960423: step 8230, loss 0.0179153, acc 1
2016-09-05T20:57:00.778404: step 8231, loss 0.0296813, acc 1
2016-09-05T20:57:01.604117: step 8232, loss 0.00365544, acc 1
2016-09-05T20:57:02.439589: step 8233, loss 0.0222603, acc 0.98
2016-09-05T20:57:03.271512: step 8234, loss 0.0466671, acc 0.98
2016-09-05T20:57:04.069393: step 8235, loss 0.00373596, acc 1
2016-09-05T20:57:04.865965: step 8236, loss 0.0442057, acc 0.98
2016-09-05T20:57:05.647574: step 8237, loss 0.00229089, acc 1
2016-09-05T20:57:06.456666: step 8238, loss 0.0538328, acc 0.96
2016-09-05T20:57:07.302873: step 8239, loss 0.0366236, acc 0.98
2016-09-05T20:57:08.100558: step 8240, loss 0.015551, acc 1
2016-09-05T20:57:08.879708: step 8241, loss 0.00363076, acc 1
2016-09-05T20:57:09.724307: step 8242, loss 0.00380501, acc 1
2016-09-05T20:57:10.541000: step 8243, loss 0.0290444, acc 0.98
2016-09-05T20:57:11.339796: step 8244, loss 0.0312366, acc 1
2016-09-05T20:57:12.196934: step 8245, loss 0.0172504, acc 0.98
2016-09-05T20:57:12.991303: step 8246, loss 0.0597408, acc 0.98
2016-09-05T20:57:13.802887: step 8247, loss 0.0145046, acc 1
2016-09-05T20:57:14.663344: step 8248, loss 0.042327, acc 0.98
2016-09-05T20:57:15.502194: step 8249, loss 0.00231337, acc 1
2016-09-05T20:57:16.333410: step 8250, loss 0.00409957, acc 1
2016-09-05T20:57:17.173829: step 8251, loss 0.0374525, acc 0.98
2016-09-05T20:57:17.978502: step 8252, loss 0.0380161, acc 0.98
2016-09-05T20:57:18.807114: step 8253, loss 0.00461274, acc 1
2016-09-05T20:57:19.660978: step 8254, loss 0.00390073, acc 1
2016-09-05T20:57:20.450049: step 8255, loss 0.00283537, acc 1
2016-09-05T20:57:21.247695: step 8256, loss 0.00403879, acc 1
2016-09-05T20:57:22.094332: step 8257, loss 0.155175, acc 0.96
2016-09-05T20:57:22.938870: step 8258, loss 0.0142803, acc 1
2016-09-05T20:57:23.742978: step 8259, loss 0.0229452, acc 0.98
2016-09-05T20:57:24.539342: step 8260, loss 0.0211468, acc 0.98
2016-09-05T20:57:25.346026: step 8261, loss 0.00667184, acc 1
2016-09-05T20:57:26.120356: step 8262, loss 0.018721, acc 1
2016-09-05T20:57:26.940683: step 8263, loss 0.00548769, acc 1
2016-09-05T20:57:27.778376: step 8264, loss 0.0124654, acc 1
2016-09-05T20:57:28.545535: step 8265, loss 0.030181, acc 0.98
2016-09-05T20:57:29.357500: step 8266, loss 0.0185286, acc 0.98
2016-09-05T20:57:30.213130: step 8267, loss 0.00367771, acc 1
2016-09-05T20:57:31.017758: step 8268, loss 0.0529522, acc 0.96
2016-09-05T20:57:31.852482: step 8269, loss 0.0171558, acc 0.98
2016-09-05T20:57:32.666555: step 8270, loss 0.0107835, acc 1
2016-09-05T20:57:33.450881: step 8271, loss 0.00282271, acc 1
2016-09-05T20:57:34.218889: step 8272, loss 0.0146853, acc 1
2016-09-05T20:57:35.052784: step 8273, loss 0.0161255, acc 1
2016-09-05T20:57:35.848281: step 8274, loss 0.00694789, acc 1
2016-09-05T20:57:36.660630: step 8275, loss 0.0108272, acc 1
2016-09-05T20:57:37.451853: step 8276, loss 0.0148255, acc 1
2016-09-05T20:57:38.260389: step 8277, loss 0.0867255, acc 0.96
2016-09-05T20:57:39.062091: step 8278, loss 0.026759, acc 1
2016-09-05T20:57:39.897124: step 8279, loss 0.0813429, acc 0.96
2016-09-05T20:57:40.657808: step 8280, loss 0.0177173, acc 0.98
2016-09-05T20:57:41.459982: step 8281, loss 0.0118481, acc 1
2016-09-05T20:57:42.283964: step 8282, loss 0.0348026, acc 0.96
2016-09-05T20:57:43.084415: step 8283, loss 0.0117226, acc 1
2016-09-05T20:57:43.885653: step 8284, loss 0.00264959, acc 1
2016-09-05T20:57:44.703278: step 8285, loss 0.0507173, acc 0.98
2016-09-05T20:57:45.506894: step 8286, loss 0.00242578, acc 1
2016-09-05T20:57:46.322950: step 8287, loss 0.0042635, acc 1
2016-09-05T20:57:47.137417: step 8288, loss 0.00287882, acc 1
2016-09-05T20:57:47.951095: step 8289, loss 0.00559013, acc 1
2016-09-05T20:57:48.776144: step 8290, loss 0.0672588, acc 0.98
2016-09-05T20:57:49.609664: step 8291, loss 0.0200138, acc 1
2016-09-05T20:57:50.394716: step 8292, loss 0.00789363, acc 1
2016-09-05T20:57:51.204871: step 8293, loss 0.0261164, acc 0.98
2016-09-05T20:57:52.010760: step 8294, loss 0.00400416, acc 1
2016-09-05T20:57:52.798859: step 8295, loss 0.0257767, acc 1
2016-09-05T20:57:53.609721: step 8296, loss 0.00416484, acc 1
2016-09-05T20:57:54.416484: step 8297, loss 0.00410073, acc 1
2016-09-05T20:57:55.216241: step 8298, loss 0.0160778, acc 1
2016-09-05T20:57:56.009525: step 8299, loss 0.0854601, acc 0.98
2016-09-05T20:57:56.815510: step 8300, loss 0.0371223, acc 0.98

Evaluation:
2016-09-05T20:58:00.272766: step 8300, loss 1.97391, acc 0.724

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-8300

2016-09-05T20:58:02.193665: step 8301, loss 0.0245914, acc 0.98
2016-09-05T20:58:03.007073: step 8302, loss 0.0574774, acc 0.96
2016-09-05T20:58:03.791105: step 8303, loss 0.017956, acc 1
2016-09-05T20:58:04.592489: step 8304, loss 0.00555809, acc 1
2016-09-05T20:58:05.404418: step 8305, loss 0.0084488, acc 1
2016-09-05T20:58:06.183588: step 8306, loss 0.0244469, acc 1
2016-09-05T20:58:06.968752: step 8307, loss 0.00643613, acc 1
2016-09-05T20:58:07.796224: step 8308, loss 0.0265229, acc 1
2016-09-05T20:58:08.578350: step 8309, loss 0.0524999, acc 0.98
2016-09-05T20:58:09.402331: step 8310, loss 0.023768, acc 0.98
2016-09-05T20:58:10.202081: step 8311, loss 0.0184337, acc 1
2016-09-05T20:58:10.991374: step 8312, loss 0.0265915, acc 0.98
2016-09-05T20:58:11.800236: step 8313, loss 0.0254716, acc 0.98
2016-09-05T20:58:12.604739: step 8314, loss 0.0496031, acc 0.96
2016-09-05T20:58:13.395941: step 8315, loss 0.00613138, acc 1
2016-09-05T20:58:14.193327: step 8316, loss 0.0156854, acc 1
2016-09-05T20:58:15.013189: step 8317, loss 0.0804145, acc 0.98
2016-09-05T20:58:15.803681: step 8318, loss 0.0431163, acc 1
2016-09-05T20:58:16.597305: step 8319, loss 0.0172089, acc 1
2016-09-05T20:58:17.456790: step 8320, loss 0.00342649, acc 1
2016-09-05T20:58:18.245310: step 8321, loss 0.0228144, acc 1
2016-09-05T20:58:19.042852: step 8322, loss 0.00331654, acc 1
2016-09-05T20:58:19.895348: step 8323, loss 0.0147846, acc 1
2016-09-05T20:58:20.677319: step 8324, loss 0.0143296, acc 1
2016-09-05T20:58:21.468543: step 8325, loss 0.00295053, acc 1
2016-09-05T20:58:22.301849: step 8326, loss 0.0368109, acc 0.98
2016-09-05T20:58:23.106578: step 8327, loss 0.0062777, acc 1
2016-09-05T20:58:23.917046: step 8328, loss 0.0240496, acc 0.98
2016-09-05T20:58:24.735750: step 8329, loss 0.00394906, acc 1
2016-09-05T20:58:25.499484: step 8330, loss 0.0104294, acc 1
2016-09-05T20:58:26.301287: step 8331, loss 0.00293703, acc 1
2016-09-05T20:58:27.091644: step 8332, loss 0.00414282, acc 1
2016-09-05T20:58:27.881802: step 8333, loss 0.00711808, acc 1
2016-09-05T20:58:28.734602: step 8334, loss 0.0324739, acc 0.98
2016-09-05T20:58:29.560498: step 8335, loss 0.036385, acc 0.98
2016-09-05T20:58:30.330977: step 8336, loss 0.0159876, acc 1
2016-09-05T20:58:31.147101: step 8337, loss 0.00965561, acc 1
2016-09-05T20:58:31.964886: step 8338, loss 0.028264, acc 0.98
2016-09-05T20:58:32.753675: step 8339, loss 0.0278122, acc 0.98
2016-09-05T20:58:33.585429: step 8340, loss 0.0416395, acc 0.98
2016-09-05T20:58:34.403938: step 8341, loss 0.00285626, acc 1
2016-09-05T20:58:34.821140: step 8342, loss 0.00449895, acc 1
2016-09-05T20:58:35.655372: step 8343, loss 0.0306824, acc 0.98
2016-09-05T20:58:36.448395: step 8344, loss 0.0338948, acc 0.98
2016-09-05T20:58:37.270542: step 8345, loss 0.0146515, acc 1
2016-09-05T20:58:38.111701: step 8346, loss 0.0500044, acc 0.96
2016-09-05T20:58:38.912376: step 8347, loss 0.00437733, acc 1
2016-09-05T20:58:39.692339: step 8348, loss 0.0283145, acc 1
2016-09-05T20:58:40.531777: step 8349, loss 0.0244233, acc 0.98
2016-09-05T20:58:41.342657: step 8350, loss 0.034512, acc 1
2016-09-05T20:58:42.161305: step 8351, loss 0.0194472, acc 1
2016-09-05T20:58:42.984581: step 8352, loss 0.0211828, acc 0.98
2016-09-05T20:58:43.804117: step 8353, loss 0.0025569, acc 1
2016-09-05T20:58:44.609113: step 8354, loss 0.0420718, acc 0.98
2016-09-05T20:58:45.465454: step 8355, loss 0.0168344, acc 1
2016-09-05T20:58:46.289230: step 8356, loss 0.0228891, acc 0.98
2016-09-05T20:58:47.079051: step 8357, loss 0.0115402, acc 1
2016-09-05T20:58:47.925898: step 8358, loss 0.00286367, acc 1
2016-09-05T20:58:48.745397: step 8359, loss 0.0170712, acc 0.98
2016-09-05T20:58:49.556977: step 8360, loss 0.039327, acc 0.98
2016-09-05T20:58:50.377432: step 8361, loss 0.0304619, acc 0.98
2016-09-05T20:58:51.185219: step 8362, loss 0.0025505, acc 1
2016-09-05T20:58:51.986199: step 8363, loss 0.00342074, acc 1
2016-09-05T20:58:52.833324: step 8364, loss 0.0410966, acc 0.98
2016-09-05T20:58:53.625593: step 8365, loss 0.0257534, acc 0.98
2016-09-05T20:58:54.414879: step 8366, loss 0.01664, acc 0.98
2016-09-05T20:58:55.282172: step 8367, loss 0.016795, acc 0.98
2016-09-05T20:58:56.152527: step 8368, loss 0.0025386, acc 1
2016-09-05T20:58:56.915932: step 8369, loss 0.0253885, acc 0.98
2016-09-05T20:58:57.705550: step 8370, loss 0.0314243, acc 0.98
2016-09-05T20:58:58.521366: step 8371, loss 0.00581315, acc 1
2016-09-05T20:58:59.306478: step 8372, loss 0.0317503, acc 0.98
2016-09-05T20:59:00.108051: step 8373, loss 0.0220691, acc 1
2016-09-05T20:59:00.942009: step 8374, loss 0.0375256, acc 0.98
2016-09-05T20:59:01.753410: step 8375, loss 0.0262501, acc 0.98
2016-09-05T20:59:02.571487: step 8376, loss 0.0175998, acc 0.98
2016-09-05T20:59:03.423554: step 8377, loss 0.0185534, acc 0.98
2016-09-05T20:59:04.239592: step 8378, loss 0.0122469, acc 1
2016-09-05T20:59:05.058076: step 8379, loss 0.0021529, acc 1
2016-09-05T20:59:05.871517: step 8380, loss 0.00855414, acc 1
2016-09-05T20:59:06.679188: step 8381, loss 0.00582684, acc 1
2016-09-05T20:59:07.503832: step 8382, loss 0.00500064, acc 1
2016-09-05T20:59:08.325438: step 8383, loss 0.019845, acc 0.98
2016-09-05T20:59:09.132237: step 8384, loss 0.0396618, acc 0.98
2016-09-05T20:59:09.949408: step 8385, loss 0.0152714, acc 1
2016-09-05T20:59:10.762894: step 8386, loss 0.01042, acc 1
2016-09-05T20:59:11.575676: step 8387, loss 0.0373985, acc 0.98
2016-09-05T20:59:12.374736: step 8388, loss 0.00552919, acc 1
2016-09-05T20:59:13.208712: step 8389, loss 0.00268519, acc 1
2016-09-05T20:59:14.018582: step 8390, loss 0.0361076, acc 0.98
2016-09-05T20:59:14.858473: step 8391, loss 0.0409652, acc 0.98
2016-09-05T20:59:15.685029: step 8392, loss 0.0520932, acc 0.96
2016-09-05T20:59:16.505923: step 8393, loss 0.00863787, acc 1
2016-09-05T20:59:17.311535: step 8394, loss 0.00197687, acc 1
2016-09-05T20:59:18.149455: step 8395, loss 0.00292542, acc 1
2016-09-05T20:59:18.953950: step 8396, loss 0.0188988, acc 0.98
2016-09-05T20:59:19.782882: step 8397, loss 0.00435965, acc 1
2016-09-05T20:59:20.616057: step 8398, loss 0.0141286, acc 1
2016-09-05T20:59:21.453959: step 8399, loss 0.00484991, acc 1
2016-09-05T20:59:22.281431: step 8400, loss 0.00403583, acc 1

Evaluation:
2016-09-05T20:59:25.789514: step 8400, loss 2.37319, acc 0.736

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-8400

2016-09-05T20:59:27.756680: step 8401, loss 0.00296599, acc 1
2016-09-05T20:59:28.534321: step 8402, loss 0.00863877, acc 1
2016-09-05T20:59:29.340621: step 8403, loss 0.0597805, acc 0.96
2016-09-05T20:59:30.166779: step 8404, loss 0.00590584, acc 1
2016-09-05T20:59:30.980002: step 8405, loss 0.0329912, acc 1
2016-09-05T20:59:31.787547: step 8406, loss 0.00576982, acc 1
2016-09-05T20:59:32.608267: step 8407, loss 0.0738265, acc 0.98
2016-09-05T20:59:33.445798: step 8408, loss 0.0200817, acc 0.98
2016-09-05T20:59:34.258751: step 8409, loss 0.00692184, acc 1
2016-09-05T20:59:35.065839: step 8410, loss 0.0688161, acc 0.94
2016-09-05T20:59:35.878281: step 8411, loss 0.0021101, acc 1
2016-09-05T20:59:36.658937: step 8412, loss 0.0332739, acc 0.98
2016-09-05T20:59:37.511967: step 8413, loss 0.0141117, acc 1
2016-09-05T20:59:38.336255: step 8414, loss 0.165377, acc 0.98
2016-09-05T20:59:39.128640: step 8415, loss 0.00317171, acc 1
2016-09-05T20:59:39.947267: step 8416, loss 0.0093138, acc 1
2016-09-05T20:59:40.787288: step 8417, loss 0.00249461, acc 1
2016-09-05T20:59:41.609414: step 8418, loss 0.145032, acc 0.94
2016-09-05T20:59:42.427953: step 8419, loss 0.0193181, acc 1
2016-09-05T20:59:43.238926: step 8420, loss 0.00292061, acc 1
2016-09-05T20:59:44.037071: step 8421, loss 0.0282015, acc 0.98
2016-09-05T20:59:44.861596: step 8422, loss 0.0212303, acc 1
2016-09-05T20:59:45.688199: step 8423, loss 0.0209399, acc 0.98
2016-09-05T20:59:46.495470: step 8424, loss 0.001827, acc 1
2016-09-05T20:59:47.320497: step 8425, loss 0.020947, acc 1
2016-09-05T20:59:48.124665: step 8426, loss 0.030813, acc 0.98
2016-09-05T20:59:48.933442: step 8427, loss 0.0377846, acc 0.98
2016-09-05T20:59:49.777361: step 8428, loss 0.0325214, acc 0.98
2016-09-05T20:59:50.569392: step 8429, loss 0.0270268, acc 0.98
2016-09-05T20:59:51.382330: step 8430, loss 0.0132224, acc 1
2016-09-05T20:59:52.185439: step 8431, loss 0.013512, acc 1
2016-09-05T20:59:53.000146: step 8432, loss 0.0496098, acc 0.98
2016-09-05T20:59:53.819643: step 8433, loss 0.0203742, acc 1
2016-09-05T20:59:54.646931: step 8434, loss 0.0124371, acc 1
2016-09-05T20:59:55.468407: step 8435, loss 0.0276823, acc 0.98
2016-09-05T20:59:56.250754: step 8436, loss 0.0348652, acc 0.98
2016-09-05T20:59:57.087205: step 8437, loss 0.0306545, acc 0.98
2016-09-05T20:59:57.902353: step 8438, loss 0.0550762, acc 0.98
2016-09-05T20:59:58.734247: step 8439, loss 0.0176916, acc 1
2016-09-05T20:59:59.514882: step 8440, loss 0.031698, acc 0.98
2016-09-05T21:00:00.355221: step 8441, loss 0.00419006, acc 1
2016-09-05T21:00:01.125297: step 8442, loss 0.00621426, acc 1
2016-09-05T21:00:01.934902: step 8443, loss 0.021161, acc 0.98
2016-09-05T21:00:02.753014: step 8444, loss 0.0183037, acc 1
2016-09-05T21:00:03.542456: step 8445, loss 0.0171228, acc 1
2016-09-05T21:00:04.343698: step 8446, loss 0.014656, acc 1
2016-09-05T21:00:05.168655: step 8447, loss 0.0053354, acc 1
2016-09-05T21:00:05.954674: step 8448, loss 0.00799562, acc 1
2016-09-05T21:00:06.778967: step 8449, loss 0.00327678, acc 1
2016-09-05T21:00:07.602013: step 8450, loss 0.0119305, acc 1
2016-09-05T21:00:08.391266: step 8451, loss 0.00307955, acc 1
2016-09-05T21:00:09.200569: step 8452, loss 0.0206244, acc 0.98
2016-09-05T21:00:10.043096: step 8453, loss 0.0919352, acc 0.94
2016-09-05T21:00:10.855443: step 8454, loss 0.0524225, acc 0.98
2016-09-05T21:00:11.692824: step 8455, loss 0.0171089, acc 1
2016-09-05T21:00:12.534609: step 8456, loss 0.0104172, acc 1
2016-09-05T21:00:13.345388: step 8457, loss 0.00524051, acc 1
2016-09-05T21:00:14.163430: step 8458, loss 0.0889331, acc 0.98
2016-09-05T21:00:15.018866: step 8459, loss 0.0240323, acc 1
2016-09-05T21:00:15.848997: step 8460, loss 0.0295208, acc 0.98
2016-09-05T21:00:16.664617: step 8461, loss 0.0226658, acc 0.98
2016-09-05T21:00:17.480813: step 8462, loss 0.0629723, acc 0.96
2016-09-05T21:00:18.287820: step 8463, loss 0.0320027, acc 0.98
2016-09-05T21:00:19.082797: step 8464, loss 0.0041629, acc 1
2016-09-05T21:00:19.905856: step 8465, loss 0.00279342, acc 1
2016-09-05T21:00:20.720751: step 8466, loss 0.00412181, acc 1
2016-09-05T21:00:21.527285: step 8467, loss 0.0514176, acc 0.98
2016-09-05T21:00:22.342725: step 8468, loss 0.00571496, acc 1
2016-09-05T21:00:23.150984: step 8469, loss 0.0926849, acc 0.98
2016-09-05T21:00:23.958015: step 8470, loss 0.0202962, acc 0.98
2016-09-05T21:00:24.781789: step 8471, loss 0.00516593, acc 1
2016-09-05T21:00:25.605399: step 8472, loss 0.0355865, acc 0.98
2016-09-05T21:00:26.435866: step 8473, loss 0.0476971, acc 0.96
2016-09-05T21:00:27.267157: step 8474, loss 0.00775107, acc 1
2016-09-05T21:00:28.068149: step 8475, loss 0.00238386, acc 1
2016-09-05T21:00:28.874369: step 8476, loss 0.0104499, acc 1
2016-09-05T21:00:29.707097: step 8477, loss 0.179384, acc 0.98
2016-09-05T21:00:30.545402: step 8478, loss 0.00992081, acc 1
2016-09-05T21:00:31.300996: step 8479, loss 0.034382, acc 0.98
2016-09-05T21:00:32.100106: step 8480, loss 0.00440508, acc 1
2016-09-05T21:00:32.932729: step 8481, loss 0.00708329, acc 1
2016-09-05T21:00:33.725067: step 8482, loss 0.00611734, acc 1
2016-09-05T21:00:34.543085: step 8483, loss 0.0129708, acc 1
2016-09-05T21:00:35.353279: step 8484, loss 0.0193769, acc 1
2016-09-05T21:00:36.171286: step 8485, loss 0.0248315, acc 0.98
2016-09-05T21:00:36.969825: step 8486, loss 0.0300589, acc 0.98
2016-09-05T21:00:37.807622: step 8487, loss 0.0234302, acc 1
2016-09-05T21:00:38.583126: step 8488, loss 0.0933548, acc 0.98
2016-09-05T21:00:39.430467: step 8489, loss 0.0283017, acc 0.98
2016-09-05T21:00:40.273566: step 8490, loss 0.00685699, acc 1
2016-09-05T21:00:41.072274: step 8491, loss 0.0717307, acc 0.96
2016-09-05T21:00:41.909144: step 8492, loss 0.0127524, acc 1
2016-09-05T21:00:42.754940: step 8493, loss 0.0342726, acc 0.98
2016-09-05T21:00:43.557380: step 8494, loss 0.0396938, acc 1
2016-09-05T21:00:44.363240: step 8495, loss 0.0214529, acc 1
2016-09-05T21:00:45.208201: step 8496, loss 0.017659, acc 1
2016-09-05T21:00:46.016053: step 8497, loss 0.0638492, acc 0.98
2016-09-05T21:00:46.830692: step 8498, loss 0.00235048, acc 1
2016-09-05T21:00:47.682774: step 8499, loss 0.0031954, acc 1
2016-09-05T21:00:48.493434: step 8500, loss 0.0198212, acc 1

Evaluation:
2016-09-05T21:00:52.018001: step 8500, loss 2.04529, acc 0.723

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-8500

2016-09-05T21:00:53.883886: step 8501, loss 0.00347126, acc 1
2016-09-05T21:00:54.718921: step 8502, loss 0.00732944, acc 1
2016-09-05T21:00:55.541828: step 8503, loss 0.00402217, acc 1
2016-09-05T21:00:56.368465: step 8504, loss 0.0289966, acc 1
2016-09-05T21:00:57.172418: step 8505, loss 0.00239687, acc 1
2016-09-05T21:00:57.951159: step 8506, loss 0.0183025, acc 0.98
2016-09-05T21:00:58.787199: step 8507, loss 0.00627128, acc 1
2016-09-05T21:00:59.585252: step 8508, loss 0.0393936, acc 0.98
2016-09-05T21:01:00.421076: step 8509, loss 0.0042347, acc 1
2016-09-05T21:01:01.229101: step 8510, loss 0.0375705, acc 0.98
2016-09-05T21:01:02.030722: step 8511, loss 0.0532745, acc 0.98
2016-09-05T21:01:02.872677: step 8512, loss 0.0346377, acc 0.98
2016-09-05T21:01:03.664801: step 8513, loss 0.0318723, acc 1
2016-09-05T21:01:04.515614: step 8514, loss 0.0144799, acc 1
2016-09-05T21:01:05.277711: step 8515, loss 0.0367371, acc 0.98
2016-09-05T21:01:06.062821: step 8516, loss 0.00450128, acc 1
2016-09-05T21:01:06.868900: step 8517, loss 0.00284162, acc 1
2016-09-05T21:01:07.647371: step 8518, loss 0.0176706, acc 0.98
2016-09-05T21:01:08.453829: step 8519, loss 0.00259701, acc 1
2016-09-05T21:01:09.265433: step 8520, loss 0.00487212, acc 1
2016-09-05T21:01:10.051785: step 8521, loss 0.00394722, acc 1
2016-09-05T21:01:10.861513: step 8522, loss 0.0245276, acc 1
2016-09-05T21:01:11.661094: step 8523, loss 0.0304464, acc 0.98
2016-09-05T21:01:12.459465: step 8524, loss 0.00770886, acc 1
2016-09-05T21:01:13.282235: step 8525, loss 0.0189987, acc 0.98
2016-09-05T21:01:14.096667: step 8526, loss 0.00406414, acc 1
2016-09-05T21:01:14.895029: step 8527, loss 0.00708375, acc 1
2016-09-05T21:01:15.718437: step 8528, loss 0.0157987, acc 1
2016-09-05T21:01:16.540257: step 8529, loss 0.0186363, acc 1
2016-09-05T21:01:17.343914: step 8530, loss 0.00284989, acc 1
2016-09-05T21:01:18.158774: step 8531, loss 0.0191484, acc 0.98
2016-09-05T21:01:18.984998: step 8532, loss 0.00865527, acc 1
2016-09-05T21:01:19.769062: step 8533, loss 0.0421665, acc 0.96
2016-09-05T21:01:20.571017: step 8534, loss 0.00280032, acc 1
2016-09-05T21:01:21.405995: step 8535, loss 0.0025808, acc 1
2016-09-05T21:01:21.855748: step 8536, loss 0.00245725, acc 1
2016-09-05T21:01:22.683357: step 8537, loss 0.0230489, acc 0.98
2016-09-05T21:01:23.483901: step 8538, loss 0.0229721, acc 0.98
2016-09-05T21:01:24.308956: step 8539, loss 0.116854, acc 0.98
2016-09-05T21:01:25.142504: step 8540, loss 0.0174923, acc 0.98
2016-09-05T21:01:25.972836: step 8541, loss 0.0512395, acc 0.98
2016-09-05T21:01:26.801770: step 8542, loss 0.0195439, acc 0.98
2016-09-05T21:01:27.624941: step 8543, loss 0.0129543, acc 1
2016-09-05T21:01:28.446841: step 8544, loss 0.0246295, acc 0.98
2016-09-05T21:01:29.259338: step 8545, loss 0.0192501, acc 0.98
2016-09-05T21:01:30.106364: step 8546, loss 0.0239371, acc 1
2016-09-05T21:01:30.905728: step 8547, loss 0.00806655, acc 1
2016-09-05T21:01:31.723425: step 8548, loss 0.0324635, acc 0.98
2016-09-05T21:01:32.562341: step 8549, loss 0.0370742, acc 1
2016-09-05T21:01:33.367066: step 8550, loss 0.0668809, acc 0.98
2016-09-05T21:01:34.169339: step 8551, loss 0.0203291, acc 0.98
2016-09-05T21:01:35.002966: step 8552, loss 0.00718849, acc 1
2016-09-05T21:01:35.812227: step 8553, loss 0.00339771, acc 1
2016-09-05T21:01:36.644820: step 8554, loss 0.0111781, acc 1
2016-09-05T21:01:37.474698: step 8555, loss 0.0226715, acc 1
2016-09-05T21:01:38.301869: step 8556, loss 0.00246041, acc 1
2016-09-05T21:01:39.104971: step 8557, loss 0.0238556, acc 1
2016-09-05T21:01:39.906037: step 8558, loss 0.0342985, acc 1
2016-09-05T21:01:40.724788: step 8559, loss 0.0103047, acc 1
2016-09-05T21:01:41.495495: step 8560, loss 0.00390573, acc 1
2016-09-05T21:01:42.310844: step 8561, loss 0.0148291, acc 1
2016-09-05T21:01:43.126646: step 8562, loss 0.00233692, acc 1
2016-09-05T21:01:43.912826: step 8563, loss 0.0146132, acc 1
2016-09-05T21:01:44.710508: step 8564, loss 0.0125114, acc 1
2016-09-05T21:01:45.512849: step 8565, loss 0.0227968, acc 0.98
2016-09-05T21:01:46.284114: step 8566, loss 0.0146969, acc 1
2016-09-05T21:01:47.150279: step 8567, loss 0.00406282, acc 1
2016-09-05T21:01:47.969699: step 8568, loss 0.0193308, acc 0.98
2016-09-05T21:01:48.765406: step 8569, loss 0.0757829, acc 0.98
2016-09-05T21:01:49.563645: step 8570, loss 0.0233004, acc 0.98
2016-09-05T21:01:50.373596: step 8571, loss 0.0270154, acc 0.98
2016-09-05T21:01:51.182232: step 8572, loss 0.0297499, acc 1
2016-09-05T21:01:51.963357: step 8573, loss 0.00223277, acc 1
2016-09-05T21:01:52.764392: step 8574, loss 0.0119973, acc 1
2016-09-05T21:01:53.557926: step 8575, loss 0.00421139, acc 1
2016-09-05T21:01:54.391801: step 8576, loss 0.00229406, acc 1
2016-09-05T21:01:55.233848: step 8577, loss 0.0309436, acc 0.98
2016-09-05T21:01:56.021626: step 8578, loss 0.00252146, acc 1
2016-09-05T21:01:56.820366: step 8579, loss 0.00235341, acc 1
2016-09-05T21:01:57.619133: step 8580, loss 0.0220464, acc 0.98
2016-09-05T21:01:58.372866: step 8581, loss 0.0280881, acc 0.98
2016-09-05T21:01:59.204319: step 8582, loss 0.0454219, acc 0.98
2016-09-05T21:02:00.007003: step 8583, loss 0.015675, acc 1
2016-09-05T21:02:00.866436: step 8584, loss 0.0347151, acc 1
2016-09-05T21:02:01.667536: step 8585, loss 0.0181268, acc 0.98
2016-09-05T21:02:02.488284: step 8586, loss 0.0313202, acc 0.98
2016-09-05T21:02:03.287687: step 8587, loss 0.0286643, acc 0.98
2016-09-05T21:02:04.115915: step 8588, loss 0.00377995, acc 1
2016-09-05T21:02:04.960559: step 8589, loss 0.00465579, acc 1
2016-09-05T21:02:05.779095: step 8590, loss 0.0114727, acc 1
2016-09-05T21:02:06.616113: step 8591, loss 0.0184012, acc 0.98
2016-09-05T21:02:07.475539: step 8592, loss 0.0280763, acc 0.98
2016-09-05T21:02:08.298418: step 8593, loss 0.0289841, acc 1
2016-09-05T21:02:09.108873: step 8594, loss 0.00252877, acc 1
2016-09-05T21:02:09.934835: step 8595, loss 0.05225, acc 0.96
2016-09-05T21:02:10.752476: step 8596, loss 0.00235931, acc 1
2016-09-05T21:02:11.553148: step 8597, loss 0.0476943, acc 0.96
2016-09-05T21:02:12.401231: step 8598, loss 0.00234553, acc 1
2016-09-05T21:02:13.206955: step 8599, loss 0.0162868, acc 1
2016-09-05T21:02:14.014498: step 8600, loss 0.0182837, acc 1

Evaluation:
2016-09-05T21:02:17.515216: step 8600, loss 2.49467, acc 0.726

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-8600

2016-09-05T21:02:19.479614: step 8601, loss 0.0101933, acc 1
2016-09-05T21:02:20.274237: step 8602, loss 0.0804353, acc 0.98
2016-09-05T21:02:21.045747: step 8603, loss 0.0181587, acc 1
2016-09-05T21:02:21.865463: step 8604, loss 0.00272008, acc 1
2016-09-05T21:02:22.655159: step 8605, loss 0.0254866, acc 1
2016-09-05T21:02:23.464474: step 8606, loss 0.145867, acc 0.94
2016-09-05T21:02:24.277399: step 8607, loss 0.00259569, acc 1
2016-09-05T21:02:25.041077: step 8608, loss 0.00843197, acc 1
2016-09-05T21:02:25.858823: step 8609, loss 0.00527384, acc 1
2016-09-05T21:02:26.676697: step 8610, loss 0.00200937, acc 1
2016-09-05T21:02:27.468730: step 8611, loss 0.0189586, acc 0.98
2016-09-05T21:02:28.260738: step 8612, loss 0.0398629, acc 1
2016-09-05T21:02:29.071177: step 8613, loss 0.0481084, acc 0.98
2016-09-05T21:02:29.854446: step 8614, loss 0.00183086, acc 1
2016-09-05T21:02:30.658137: step 8615, loss 0.0109635, acc 1
2016-09-05T21:02:31.488075: step 8616, loss 0.015199, acc 1
2016-09-05T21:02:32.273920: step 8617, loss 0.00220274, acc 1
2016-09-05T21:02:33.090216: step 8618, loss 0.00249488, acc 1
2016-09-05T21:02:33.933206: step 8619, loss 0.0112043, acc 1
2016-09-05T21:02:34.735839: step 8620, loss 0.00198835, acc 1
2016-09-05T21:02:35.532627: step 8621, loss 0.0174667, acc 0.98
2016-09-05T21:02:36.383613: step 8622, loss 0.0180288, acc 1
2016-09-05T21:02:37.165377: step 8623, loss 0.00442713, acc 1
2016-09-05T21:02:37.986233: step 8624, loss 0.0032961, acc 1
2016-09-05T21:02:38.797654: step 8625, loss 0.00310154, acc 1
2016-09-05T21:02:39.579539: step 8626, loss 0.0535365, acc 0.98
2016-09-05T21:02:40.369782: step 8627, loss 0.0100195, acc 1
2016-09-05T21:02:41.196131: step 8628, loss 0.0269636, acc 0.98
2016-09-05T21:02:41.981191: step 8629, loss 0.0819232, acc 0.98
2016-09-05T21:02:42.813467: step 8630, loss 0.00259921, acc 1
2016-09-05T21:02:43.627251: step 8631, loss 0.038089, acc 0.98
2016-09-05T21:02:44.395968: step 8632, loss 0.0623079, acc 0.98
2016-09-05T21:02:45.185246: step 8633, loss 0.0305876, acc 0.98
2016-09-05T21:02:45.997283: step 8634, loss 0.0358192, acc 0.98
2016-09-05T21:02:46.780632: step 8635, loss 0.03328, acc 0.98
2016-09-05T21:02:47.605581: step 8636, loss 0.0129468, acc 1
2016-09-05T21:02:48.431761: step 8637, loss 0.022256, acc 0.98
2016-09-05T21:02:49.229106: step 8638, loss 0.0371026, acc 0.98
2016-09-05T21:02:50.021955: step 8639, loss 0.01366, acc 1
2016-09-05T21:02:50.855672: step 8640, loss 0.00361475, acc 1
2016-09-05T21:02:51.636761: step 8641, loss 0.0332668, acc 0.98
2016-09-05T21:02:52.433579: step 8642, loss 0.00894052, acc 1
2016-09-05T21:02:53.262490: step 8643, loss 0.0442576, acc 0.98
2016-09-05T21:02:54.056065: step 8644, loss 0.0232982, acc 1
2016-09-05T21:02:54.864798: step 8645, loss 0.00714544, acc 1
2016-09-05T21:02:55.672661: step 8646, loss 0.00452984, acc 1
2016-09-05T21:02:56.467609: step 8647, loss 0.0505759, acc 0.98
2016-09-05T21:02:57.264091: step 8648, loss 0.00287396, acc 1
2016-09-05T21:02:58.082484: step 8649, loss 0.0191128, acc 1
2016-09-05T21:02:58.864759: step 8650, loss 0.0169501, acc 0.98
2016-09-05T21:02:59.693730: step 8651, loss 0.00707164, acc 1
2016-09-05T21:03:00.526733: step 8652, loss 0.0171269, acc 1
2016-09-05T21:03:01.315667: step 8653, loss 0.00381471, acc 1
2016-09-05T21:03:02.129273: step 8654, loss 0.0291349, acc 0.98
2016-09-05T21:03:02.929530: step 8655, loss 0.0125268, acc 1
2016-09-05T21:03:03.768713: step 8656, loss 0.0942332, acc 0.98
2016-09-05T21:03:04.565380: step 8657, loss 0.0179939, acc 1
2016-09-05T21:03:05.406628: step 8658, loss 0.0166584, acc 1
2016-09-05T21:03:06.183798: step 8659, loss 0.0144805, acc 1
2016-09-05T21:03:07.002921: step 8660, loss 0.00740107, acc 1
2016-09-05T21:03:07.817825: step 8661, loss 0.0860887, acc 0.96
2016-09-05T21:03:08.582225: step 8662, loss 0.0123621, acc 1
2016-09-05T21:03:09.394241: step 8663, loss 0.00741805, acc 1
2016-09-05T21:03:10.231846: step 8664, loss 0.00251974, acc 1
2016-09-05T21:03:11.035093: step 8665, loss 0.00277257, acc 1
2016-09-05T21:03:11.831195: step 8666, loss 0.00689581, acc 1
2016-09-05T21:03:12.627143: step 8667, loss 0.0309068, acc 0.98
2016-09-05T21:03:13.422528: step 8668, loss 0.010639, acc 1
2016-09-05T21:03:14.218419: step 8669, loss 0.00291059, acc 1
2016-09-05T21:03:15.063615: step 8670, loss 0.0116448, acc 1
2016-09-05T21:03:15.825280: step 8671, loss 0.0212435, acc 1
2016-09-05T21:03:16.633549: step 8672, loss 0.0354193, acc 0.98
2016-09-05T21:03:17.462332: step 8673, loss 0.0186912, acc 0.98
2016-09-05T21:03:18.224041: step 8674, loss 0.00251633, acc 1
2016-09-05T21:03:19.026269: step 8675, loss 0.0207491, acc 0.98
2016-09-05T21:03:19.856891: step 8676, loss 0.0172769, acc 0.98
2016-09-05T21:03:20.674200: step 8677, loss 0.0270031, acc 0.98
2016-09-05T21:03:21.500116: step 8678, loss 0.0459321, acc 0.98
2016-09-05T21:03:22.293445: step 8679, loss 0.0308831, acc 0.98
2016-09-05T21:03:23.080631: step 8680, loss 0.00256569, acc 1
2016-09-05T21:03:23.872329: step 8681, loss 0.0139827, acc 1
2016-09-05T21:03:24.677409: step 8682, loss 0.0225936, acc 0.98
2016-09-05T21:03:25.457858: step 8683, loss 0.00234755, acc 1
2016-09-05T21:03:26.253593: step 8684, loss 0.028428, acc 0.98
2016-09-05T21:03:27.095717: step 8685, loss 0.00343376, acc 1
2016-09-05T21:03:27.878718: step 8686, loss 0.0067504, acc 1
2016-09-05T21:03:28.690381: step 8687, loss 0.0109523, acc 1
2016-09-05T21:03:29.507641: step 8688, loss 0.0179777, acc 1
2016-09-05T21:03:30.310555: step 8689, loss 0.100856, acc 0.96
2016-09-05T21:03:31.096241: step 8690, loss 0.0329418, acc 0.98
2016-09-05T21:03:31.932797: step 8691, loss 0.0179712, acc 0.98
2016-09-05T21:03:32.731899: step 8692, loss 0.00217644, acc 1
2016-09-05T21:03:33.557237: step 8693, loss 0.00313984, acc 1
2016-09-05T21:03:34.379251: step 8694, loss 0.0429971, acc 0.98
2016-09-05T21:03:35.169831: step 8695, loss 0.0134593, acc 1
2016-09-05T21:03:35.984863: step 8696, loss 0.0156989, acc 1
2016-09-05T21:03:36.819626: step 8697, loss 0.0243163, acc 1
2016-09-05T21:03:37.594179: step 8698, loss 0.0163705, acc 1
2016-09-05T21:03:38.401030: step 8699, loss 0.014497, acc 1
2016-09-05T21:03:39.209415: step 8700, loss 0.0120916, acc 1

Evaluation:
2016-09-05T21:03:42.729634: step 8700, loss 2.19785, acc 0.726

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-8700

2016-09-05T21:03:44.595527: step 8701, loss 0.0253543, acc 0.98
2016-09-05T21:03:45.390012: step 8702, loss 0.0309619, acc 1
2016-09-05T21:03:46.161363: step 8703, loss 0.00255508, acc 1
2016-09-05T21:03:46.964788: step 8704, loss 0.0208552, acc 0.98
2016-09-05T21:03:47.813221: step 8705, loss 0.0173903, acc 0.98
2016-09-05T21:03:48.610995: step 8706, loss 0.0306439, acc 1
2016-09-05T21:03:49.434449: step 8707, loss 0.0467522, acc 0.98
2016-09-05T21:03:50.287309: step 8708, loss 0.00280112, acc 1
2016-09-05T21:03:51.099823: step 8709, loss 0.0130347, acc 1
2016-09-05T21:03:51.933568: step 8710, loss 0.00247817, acc 1
2016-09-05T21:03:52.769038: step 8711, loss 0.0148982, acc 1
2016-09-05T21:03:53.577271: step 8712, loss 0.0328782, acc 0.98
2016-09-05T21:03:54.362402: step 8713, loss 0.00548097, acc 1
2016-09-05T21:03:55.197767: step 8714, loss 0.00431232, acc 1
2016-09-05T21:03:56.009845: step 8715, loss 0.0147739, acc 1
2016-09-05T21:03:56.811770: step 8716, loss 0.0321418, acc 0.96
2016-09-05T21:03:57.631526: step 8717, loss 0.0410022, acc 0.98
2016-09-05T21:03:58.466258: step 8718, loss 0.00251326, acc 1
2016-09-05T21:03:59.299458: step 8719, loss 0.0147873, acc 1
2016-09-05T21:04:00.126992: step 8720, loss 0.00264408, acc 1
2016-09-05T21:04:00.957801: step 8721, loss 0.024943, acc 0.98
2016-09-05T21:04:01.763789: step 8722, loss 0.0125326, acc 1
2016-09-05T21:04:02.558597: step 8723, loss 0.00741255, acc 1
2016-09-05T21:04:03.377483: step 8724, loss 0.0169387, acc 1
2016-09-05T21:04:04.170385: step 8725, loss 0.0192278, acc 1
2016-09-05T21:04:05.002612: step 8726, loss 0.0836698, acc 0.98
2016-09-05T21:04:05.800719: step 8727, loss 0.0597878, acc 0.98
2016-09-05T21:04:06.617192: step 8728, loss 0.0161696, acc 1
2016-09-05T21:04:07.409440: step 8729, loss 0.00229092, acc 1
2016-09-05T21:04:07.837476: step 8730, loss 0.0507941, acc 1
2016-09-05T21:04:08.667354: step 8731, loss 0.0225094, acc 1
2016-09-05T21:04:09.493365: step 8732, loss 0.0392097, acc 0.96
2016-09-05T21:04:10.278398: step 8733, loss 0.0588238, acc 0.98
2016-09-05T21:04:11.078380: step 8734, loss 0.0133426, acc 1
2016-09-05T21:04:11.883771: step 8735, loss 0.0348992, acc 0.98
2016-09-05T21:04:12.662829: step 8736, loss 0.0180842, acc 0.98
2016-09-05T21:04:13.489378: step 8737, loss 0.0959098, acc 0.98
2016-09-05T21:04:14.302028: step 8738, loss 0.0544773, acc 0.96
2016-09-05T21:04:15.117256: step 8739, loss 0.00239235, acc 1
2016-09-05T21:04:15.920854: step 8740, loss 0.0212436, acc 0.98
2016-09-05T21:04:16.736975: step 8741, loss 0.0311365, acc 0.98
2016-09-05T21:04:17.514086: step 8742, loss 0.0115178, acc 1
2016-09-05T21:04:18.287038: step 8743, loss 0.0301712, acc 0.96
2016-09-05T21:04:19.112632: step 8744, loss 0.0675769, acc 0.98
2016-09-05T21:04:19.897457: step 8745, loss 0.0208074, acc 0.98
2016-09-05T21:04:20.691478: step 8746, loss 0.00576659, acc 1
2016-09-05T21:04:21.529573: step 8747, loss 0.0142658, acc 1
2016-09-05T21:04:22.291328: step 8748, loss 0.0250632, acc 0.98
2016-09-05T21:04:23.076724: step 8749, loss 0.0322256, acc 0.98
2016-09-05T21:04:23.851535: step 8750, loss 0.00433204, acc 1
2016-09-05T21:04:24.664273: step 8751, loss 0.00592233, acc 1
2016-09-05T21:04:25.481146: step 8752, loss 0.0228757, acc 1
2016-09-05T21:04:26.287468: step 8753, loss 0.0091841, acc 1
2016-09-05T21:04:27.100740: step 8754, loss 0.0209394, acc 0.98
2016-09-05T21:04:27.951875: step 8755, loss 0.0150031, acc 1
2016-09-05T21:04:28.794569: step 8756, loss 0.00310643, acc 1
2016-09-05T21:04:29.562749: step 8757, loss 0.00417091, acc 1
2016-09-05T21:04:30.353772: step 8758, loss 0.0248819, acc 0.98
2016-09-05T21:04:31.186545: step 8759, loss 0.0250971, acc 1
2016-09-05T21:04:31.982304: step 8760, loss 0.00473579, acc 1
2016-09-05T21:04:32.795145: step 8761, loss 0.0168375, acc 1
2016-09-05T21:04:33.627764: step 8762, loss 0.0961504, acc 0.96
2016-09-05T21:04:34.414037: step 8763, loss 0.00269883, acc 1
2016-09-05T21:04:35.201828: step 8764, loss 0.0336037, acc 0.98
2016-09-05T21:04:36.009621: step 8765, loss 0.00239689, acc 1
2016-09-05T21:04:36.782374: step 8766, loss 0.0144924, acc 1
2016-09-05T21:04:37.577025: step 8767, loss 0.0165035, acc 0.98
2016-09-05T21:04:38.397941: step 8768, loss 0.0182745, acc 0.98
2016-09-05T21:04:39.197902: step 8769, loss 0.00287472, acc 1
2016-09-05T21:04:40.013342: step 8770, loss 0.00333788, acc 1
2016-09-05T21:04:40.796395: step 8771, loss 0.0245608, acc 1
2016-09-05T21:04:41.618334: step 8772, loss 0.00261889, acc 1
2016-09-05T21:04:42.497339: step 8773, loss 0.0046874, acc 1
2016-09-05T21:04:43.306349: step 8774, loss 0.00915669, acc 1
2016-09-05T21:04:44.100743: step 8775, loss 0.0251246, acc 0.98
2016-09-05T21:04:44.918696: step 8776, loss 0.00450166, acc 1
2016-09-05T21:04:45.729037: step 8777, loss 0.0122564, acc 1
2016-09-05T21:04:46.523933: step 8778, loss 0.0304429, acc 0.98
2016-09-05T21:04:47.315561: step 8779, loss 0.00764467, acc 1
2016-09-05T21:04:48.096302: step 8780, loss 0.0274085, acc 0.98
2016-09-05T21:04:48.876130: step 8781, loss 0.0121674, acc 1
2016-09-05T21:04:49.672752: step 8782, loss 0.0118888, acc 1
2016-09-05T21:04:50.497852: step 8783, loss 0.00357047, acc 1
2016-09-05T21:04:51.257470: step 8784, loss 0.00667613, acc 1
2016-09-05T21:04:52.089543: step 8785, loss 0.00240787, acc 1
2016-09-05T21:04:52.933472: step 8786, loss 0.0148822, acc 1
2016-09-05T21:04:53.778382: step 8787, loss 0.0385979, acc 0.98
2016-09-05T21:04:54.592676: step 8788, loss 0.00690223, acc 1
2016-09-05T21:04:55.427655: step 8789, loss 0.00326167, acc 1
2016-09-05T21:04:56.231351: step 8790, loss 0.00285322, acc 1
2016-09-05T21:04:57.038353: step 8791, loss 0.0166825, acc 1
2016-09-05T21:04:57.848497: step 8792, loss 0.0029256, acc 1
2016-09-05T21:04:58.678954: step 8793, loss 0.108552, acc 0.98
2016-09-05T21:04:59.485478: step 8794, loss 0.00239595, acc 1
2016-09-05T21:05:00.329425: step 8795, loss 0.00564369, acc 1
2016-09-05T21:05:01.110901: step 8796, loss 0.0125804, acc 1
2016-09-05T21:05:01.928464: step 8797, loss 0.00349635, acc 1
2016-09-05T21:05:02.788379: step 8798, loss 0.0259192, acc 1
2016-09-05T21:05:03.615276: step 8799, loss 0.00425457, acc 1
2016-09-05T21:05:04.438632: step 8800, loss 0.0322594, acc 0.98

Evaluation:
2016-09-05T21:05:07.982128: step 8800, loss 2.58588, acc 0.728

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-8800

2016-09-05T21:05:09.814673: step 8801, loss 0.00241159, acc 1
2016-09-05T21:05:10.658707: step 8802, loss 0.0194513, acc 0.98
2016-09-05T21:05:11.469513: step 8803, loss 0.0226324, acc 0.98
2016-09-05T21:05:12.286231: step 8804, loss 0.00382833, acc 1
2016-09-05T21:05:13.138169: step 8805, loss 0.0134857, acc 1
2016-09-05T21:05:13.972719: step 8806, loss 0.0182231, acc 1
2016-09-05T21:05:14.788992: step 8807, loss 0.00224105, acc 1
2016-09-05T21:05:15.580708: step 8808, loss 0.0104552, acc 1
2016-09-05T21:05:16.357665: step 8809, loss 0.0094723, acc 1
2016-09-05T21:05:17.169769: step 8810, loss 0.0790413, acc 0.96
2016-09-05T21:05:17.954373: step 8811, loss 0.0252542, acc 0.98
2016-09-05T21:05:18.756063: step 8812, loss 0.0365736, acc 0.98
2016-09-05T21:05:19.596023: step 8813, loss 0.00903774, acc 1
2016-09-05T21:05:20.382805: step 8814, loss 0.011396, acc 1
2016-09-05T21:05:21.178256: step 8815, loss 0.0174036, acc 0.98
2016-09-05T21:05:21.986395: step 8816, loss 0.128655, acc 0.98
2016-09-05T21:05:22.783314: step 8817, loss 0.00426793, acc 1
2016-09-05T21:05:23.591293: step 8818, loss 0.0091551, acc 1
2016-09-05T21:05:24.408014: step 8819, loss 0.00307856, acc 1
2016-09-05T21:05:25.206104: step 8820, loss 0.0135991, acc 1
2016-09-05T21:05:26.013763: step 8821, loss 0.00409135, acc 1
2016-09-05T21:05:26.867333: step 8822, loss 0.00951594, acc 1
2016-09-05T21:05:27.671453: step 8823, loss 0.00465752, acc 1
2016-09-05T21:05:28.487112: step 8824, loss 0.00845602, acc 1
2016-09-05T21:05:29.336528: step 8825, loss 0.00743719, acc 1
2016-09-05T21:05:30.153426: step 8826, loss 0.075102, acc 0.96
2016-09-05T21:05:30.986387: step 8827, loss 0.0027675, acc 1
2016-09-05T21:05:31.828428: step 8828, loss 0.039157, acc 0.98
2016-09-05T21:05:32.640884: step 8829, loss 0.0124376, acc 1
2016-09-05T21:05:33.463986: step 8830, loss 0.0176856, acc 0.98
2016-09-05T21:05:34.306368: step 8831, loss 0.017223, acc 0.98
2016-09-05T21:05:35.116002: step 8832, loss 0.0308125, acc 0.98
2016-09-05T21:05:35.908513: step 8833, loss 0.0204418, acc 1
2016-09-05T21:05:36.712555: step 8834, loss 0.01422, acc 1
2016-09-05T21:05:37.531223: step 8835, loss 0.00641021, acc 1
2016-09-05T21:05:38.336727: step 8836, loss 0.0136578, acc 1
2016-09-05T21:05:39.195798: step 8837, loss 0.031142, acc 0.98
2016-09-05T21:05:40.029970: step 8838, loss 0.0468303, acc 0.98
2016-09-05T21:05:40.833408: step 8839, loss 0.0293818, acc 0.98
2016-09-05T21:05:41.678700: step 8840, loss 0.00757167, acc 1
2016-09-05T21:05:42.491939: step 8841, loss 0.0428597, acc 0.98
2016-09-05T21:05:43.283016: step 8842, loss 0.00228225, acc 1
2016-09-05T21:05:44.094679: step 8843, loss 0.0186619, acc 0.98
2016-09-05T21:05:44.905735: step 8844, loss 0.0113818, acc 1
2016-09-05T21:05:45.687973: step 8845, loss 0.053911, acc 0.98
2016-09-05T21:05:46.512013: step 8846, loss 0.0274149, acc 1
2016-09-05T21:05:47.345344: step 8847, loss 0.0131961, acc 1
2016-09-05T21:05:48.123595: step 8848, loss 0.0271289, acc 0.98
2016-09-05T21:05:48.943697: step 8849, loss 0.0122538, acc 1
2016-09-05T21:05:49.752651: step 8850, loss 0.0307732, acc 0.98
2016-09-05T21:05:50.532111: step 8851, loss 0.0144663, acc 1
2016-09-05T21:05:51.341226: step 8852, loss 0.0171595, acc 0.98
2016-09-05T21:05:52.149757: step 8853, loss 0.00234941, acc 1
2016-09-05T21:05:52.961442: step 8854, loss 0.00228634, acc 1
2016-09-05T21:05:53.777756: step 8855, loss 0.00472605, acc 1
2016-09-05T21:05:54.625433: step 8856, loss 0.0168821, acc 0.98
2016-09-05T21:05:55.400679: step 8857, loss 0.0169196, acc 1
2016-09-05T21:05:56.200960: step 8858, loss 0.0287314, acc 0.98
2016-09-05T21:05:57.033450: step 8859, loss 0.010801, acc 1
2016-09-05T21:05:57.814216: step 8860, loss 0.0652382, acc 0.98
2016-09-05T21:05:58.606917: step 8861, loss 0.0051801, acc 1
2016-09-05T21:05:59.403376: step 8862, loss 0.00345804, acc 1
2016-09-05T21:06:00.177391: step 8863, loss 0.0145181, acc 1
2016-09-05T21:06:01.015609: step 8864, loss 0.0165062, acc 1
2016-09-05T21:06:01.816618: step 8865, loss 0.0139084, acc 1
2016-09-05T21:06:02.632539: step 8866, loss 0.0256568, acc 0.98
2016-09-05T21:06:03.445930: step 8867, loss 0.00432525, acc 1
2016-09-05T21:06:04.267569: step 8868, loss 0.00213661, acc 1
2016-09-05T21:06:05.086148: step 8869, loss 0.00252532, acc 1
2016-09-05T21:06:05.933911: step 8870, loss 0.0211742, acc 0.98
2016-09-05T21:06:06.831293: step 8871, loss 0.00392697, acc 1
2016-09-05T21:06:07.633922: step 8872, loss 0.0256374, acc 0.98
2016-09-05T21:06:08.438212: step 8873, loss 0.0410099, acc 0.98
2016-09-05T21:06:09.289910: step 8874, loss 0.00906067, acc 1
2016-09-05T21:06:10.113819: step 8875, loss 0.00467445, acc 1
2016-09-05T21:06:10.920830: step 8876, loss 0.0150692, acc 1
2016-09-05T21:06:11.760399: step 8877, loss 0.016625, acc 1
2016-09-05T21:06:12.542158: step 8878, loss 0.0171207, acc 1
2016-09-05T21:06:13.346790: step 8879, loss 0.00961006, acc 1
2016-09-05T21:06:14.183794: step 8880, loss 0.00235767, acc 1
2016-09-05T21:06:15.000895: step 8881, loss 0.0269869, acc 0.98
2016-09-05T21:06:15.838193: step 8882, loss 0.03215, acc 1
2016-09-05T21:06:16.698296: step 8883, loss 0.0218944, acc 0.98
2016-09-05T21:06:17.507743: step 8884, loss 0.0270277, acc 0.98
2016-09-05T21:06:18.312118: step 8885, loss 0.0282686, acc 0.98
2016-09-05T21:06:19.143155: step 8886, loss 0.00221279, acc 1
2016-09-05T21:06:19.966366: step 8887, loss 0.00232605, acc 1
2016-09-05T21:06:20.782892: step 8888, loss 0.00839314, acc 1
2016-09-05T21:06:21.625422: step 8889, loss 0.00248624, acc 1
2016-09-05T21:06:22.406941: step 8890, loss 0.0186705, acc 0.98
2016-09-05T21:06:23.224464: step 8891, loss 0.00217663, acc 1
2016-09-05T21:06:24.052828: step 8892, loss 0.0152727, acc 1
2016-09-05T21:06:24.841483: step 8893, loss 0.00581968, acc 1
2016-09-05T21:06:25.666174: step 8894, loss 0.00275538, acc 1
2016-09-05T21:06:26.480664: step 8895, loss 0.0427821, acc 0.96
2016-09-05T21:06:27.317312: step 8896, loss 0.0533483, acc 0.94
2016-09-05T21:06:28.083700: step 8897, loss 0.017388, acc 1
2016-09-05T21:06:28.916752: step 8898, loss 0.0187032, acc 1
2016-09-05T21:06:29.733069: step 8899, loss 0.0147469, acc 1
2016-09-05T21:06:30.527552: step 8900, loss 0.0402695, acc 0.98

Evaluation:
2016-09-05T21:06:34.046078: step 8900, loss 2.7858, acc 0.726

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-8900

2016-09-05T21:06:36.088342: step 8901, loss 0.0457087, acc 0.96
2016-09-05T21:06:36.884219: step 8902, loss 0.00486853, acc 1
2016-09-05T21:06:37.694822: step 8903, loss 0.00372572, acc 1
2016-09-05T21:06:38.501830: step 8904, loss 0.00219621, acc 1
2016-09-05T21:06:39.314179: step 8905, loss 0.0470024, acc 0.96
2016-09-05T21:06:40.102702: step 8906, loss 0.0186076, acc 1
2016-09-05T21:06:40.916375: step 8907, loss 0.00412094, acc 1
2016-09-05T21:06:41.721955: step 8908, loss 0.00233126, acc 1
2016-09-05T21:06:42.523019: step 8909, loss 0.100774, acc 0.96
2016-09-05T21:06:43.338082: step 8910, loss 0.021953, acc 1
2016-09-05T21:06:44.141402: step 8911, loss 0.0811561, acc 0.94
2016-09-05T21:06:44.947815: step 8912, loss 0.00217296, acc 1
2016-09-05T21:06:45.758093: step 8913, loss 0.0162464, acc 0.98
2016-09-05T21:06:46.565229: step 8914, loss 0.0161464, acc 0.98
2016-09-05T21:06:47.388502: step 8915, loss 0.00783763, acc 1
2016-09-05T21:06:48.243589: step 8916, loss 0.0354093, acc 0.98
2016-09-05T21:06:49.069969: step 8917, loss 0.0161107, acc 1
2016-09-05T21:06:49.900779: step 8918, loss 0.0113752, acc 1
2016-09-05T21:06:50.729968: step 8919, loss 0.0411101, acc 0.96
2016-09-05T21:06:51.561365: step 8920, loss 0.0173634, acc 0.98
2016-09-05T21:06:52.342294: step 8921, loss 0.0228722, acc 0.98
2016-09-05T21:06:53.149313: step 8922, loss 0.0126825, acc 1
2016-09-05T21:06:53.973131: step 8923, loss 0.00503325, acc 1
2016-09-05T21:06:54.389311: step 8924, loss 0.0400438, acc 1
2016-09-05T21:06:55.207587: step 8925, loss 0.0198154, acc 1
2016-09-05T21:06:56.009732: step 8926, loss 0.0365191, acc 0.98
2016-09-05T21:06:56.831387: step 8927, loss 0.0461893, acc 0.98
2016-09-05T21:06:57.649088: step 8928, loss 0.0147581, acc 1
2016-09-05T21:06:58.451915: step 8929, loss 0.0170385, acc 1
2016-09-05T21:06:59.256700: step 8930, loss 0.0309668, acc 1
2016-09-05T21:07:00.082695: step 8931, loss 0.00239247, acc 1
2016-09-05T21:07:00.944810: step 8932, loss 0.0225985, acc 0.98
2016-09-05T21:07:01.753978: step 8933, loss 0.0753214, acc 0.98
2016-09-05T21:07:02.576197: step 8934, loss 0.0020924, acc 1
2016-09-05T21:07:03.397561: step 8935, loss 0.03704, acc 0.98
2016-09-05T21:07:04.207595: step 8936, loss 0.0358235, acc 0.96
2016-09-05T21:07:05.064833: step 8937, loss 0.0444564, acc 0.98
2016-09-05T21:07:05.869379: step 8938, loss 0.0321643, acc 0.98
2016-09-05T21:07:06.661942: step 8939, loss 0.00447002, acc 1
2016-09-05T21:07:07.500719: step 8940, loss 0.00233147, acc 1
2016-09-05T21:07:08.310823: step 8941, loss 0.00182306, acc 1
2016-09-05T21:07:09.128518: step 8942, loss 0.00303474, acc 1
2016-09-05T21:07:09.993081: step 8943, loss 0.00374633, acc 1
2016-09-05T21:07:10.809018: step 8944, loss 0.00478869, acc 1
2016-09-05T21:07:11.613135: step 8945, loss 0.0303717, acc 0.98
2016-09-05T21:07:12.445693: step 8946, loss 0.0251791, acc 0.98
2016-09-05T21:07:13.302798: step 8947, loss 0.0255283, acc 0.98
2016-09-05T21:07:14.081687: step 8948, loss 0.00266164, acc 1
2016-09-05T21:07:14.893651: step 8949, loss 0.00643923, acc 1
2016-09-05T21:07:15.698283: step 8950, loss 0.00297181, acc 1
2016-09-05T21:07:16.475954: step 8951, loss 0.00282384, acc 1
2016-09-05T21:07:17.326353: step 8952, loss 0.0133619, acc 1
2016-09-05T21:07:18.153386: step 8953, loss 0.0129856, acc 1
2016-09-05T21:07:18.945085: step 8954, loss 0.00435006, acc 1
2016-09-05T21:07:19.748348: step 8955, loss 0.0408632, acc 0.96
2016-09-05T21:07:20.580455: step 8956, loss 0.0272996, acc 0.98
2016-09-05T21:07:21.339135: step 8957, loss 0.0322456, acc 0.98
2016-09-05T21:07:22.172331: step 8958, loss 0.0374421, acc 1
2016-09-05T21:07:22.979120: step 8959, loss 0.0307707, acc 0.98
2016-09-05T21:07:23.747410: step 8960, loss 0.0108776, acc 1
2016-09-05T21:07:24.563052: step 8961, loss 0.0179421, acc 0.98
2016-09-05T21:07:25.381951: step 8962, loss 0.00777416, acc 1
2016-09-05T21:07:26.170133: step 8963, loss 0.00180138, acc 1
2016-09-05T21:07:26.973689: step 8964, loss 0.00176641, acc 1
2016-09-05T21:07:27.796285: step 8965, loss 0.0378176, acc 0.98
2016-09-05T21:07:28.609269: step 8966, loss 0.0130699, acc 1
2016-09-05T21:07:29.388758: step 8967, loss 0.00237992, acc 1
2016-09-05T21:07:30.212595: step 8968, loss 0.00695032, acc 1
2016-09-05T21:07:31.018046: step 8969, loss 0.0185495, acc 0.98
2016-09-05T21:07:31.807590: step 8970, loss 0.00454391, acc 1
2016-09-05T21:07:32.629651: step 8971, loss 0.00326698, acc 1
2016-09-05T21:07:33.412947: step 8972, loss 0.0566233, acc 0.98
2016-09-05T21:07:34.222653: step 8973, loss 0.0245807, acc 0.98
2016-09-05T21:07:35.033408: step 8974, loss 0.00640456, acc 1
2016-09-05T21:07:35.822513: step 8975, loss 0.00752378, acc 1
2016-09-05T21:07:36.633066: step 8976, loss 0.00257189, acc 1
2016-09-05T21:07:37.415245: step 8977, loss 0.00272252, acc 1
2016-09-05T21:07:38.211508: step 8978, loss 0.0434879, acc 0.98
2016-09-05T21:07:39.029685: step 8979, loss 0.0166996, acc 1
2016-09-05T21:07:39.857626: step 8980, loss 0.00654897, acc 1
2016-09-05T21:07:40.666095: step 8981, loss 0.011434, acc 1
2016-09-05T21:07:41.496137: step 8982, loss 0.00325368, acc 1
2016-09-05T21:07:42.334087: step 8983, loss 0.0099253, acc 1
2016-09-05T21:07:43.144591: step 8984, loss 0.0132016, acc 1
2016-09-05T21:07:43.978804: step 8985, loss 0.00439995, acc 1
2016-09-05T21:07:44.818841: step 8986, loss 0.00227449, acc 1
2016-09-05T21:07:45.676085: step 8987, loss 0.0282866, acc 0.98
2016-09-05T21:07:46.498362: step 8988, loss 0.0120469, acc 1
2016-09-05T21:07:47.350461: step 8989, loss 0.0144537, acc 1
2016-09-05T21:07:48.160205: step 8990, loss 0.00853775, acc 1
2016-09-05T21:07:48.952862: step 8991, loss 0.0114059, acc 1
2016-09-05T21:07:49.797163: step 8992, loss 0.00229178, acc 1
2016-09-05T21:07:50.617658: step 8993, loss 0.016173, acc 1
2016-09-05T21:07:51.429556: step 8994, loss 0.0165138, acc 0.98
2016-09-05T21:07:52.271601: step 8995, loss 0.0121993, acc 1
2016-09-05T21:07:53.107323: step 8996, loss 0.00848338, acc 1
2016-09-05T21:07:53.901909: step 8997, loss 0.00257271, acc 1
2016-09-05T21:07:54.715788: step 8998, loss 0.00394395, acc 1
2016-09-05T21:07:55.533952: step 8999, loss 0.00675884, acc 1
2016-09-05T21:07:56.325189: step 9000, loss 0.021322, acc 0.98

Evaluation:
2016-09-05T21:07:59.849034: step 9000, loss 3.1236, acc 0.726

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-9000

2016-09-05T21:08:01.807735: step 9001, loss 0.0232078, acc 0.98
2016-09-05T21:08:02.606452: step 9002, loss 0.00228122, acc 1
2016-09-05T21:08:03.418068: step 9003, loss 0.0119471, acc 1
2016-09-05T21:08:04.236466: step 9004, loss 0.0273261, acc 1
2016-09-05T21:08:05.021897: step 9005, loss 0.00241775, acc 1
2016-09-05T21:08:05.818343: step 9006, loss 0.0181499, acc 0.98
2016-09-05T21:08:06.623778: step 9007, loss 0.0859076, acc 0.98
2016-09-05T21:08:07.397305: step 9008, loss 0.0341251, acc 0.98
2016-09-05T21:08:08.223655: step 9009, loss 0.00304363, acc 1
2016-09-05T21:08:09.042356: step 9010, loss 0.00263232, acc 1
2016-09-05T21:08:09.838975: step 9011, loss 0.00811285, acc 1
2016-09-05T21:08:10.619195: step 9012, loss 0.00416141, acc 1
2016-09-05T21:08:11.470475: step 9013, loss 0.0449012, acc 0.98
2016-09-05T21:08:12.242715: step 9014, loss 0.0960463, acc 0.98
2016-09-05T21:08:13.029506: step 9015, loss 0.00311369, acc 1
2016-09-05T21:08:13.863554: step 9016, loss 0.0990331, acc 0.96
2016-09-05T21:08:14.656853: step 9017, loss 0.00568353, acc 1
2016-09-05T21:08:15.428988: step 9018, loss 0.00366322, acc 1
2016-09-05T21:08:16.251366: step 9019, loss 0.0446012, acc 0.98
2016-09-05T21:08:17.033691: step 9020, loss 0.0284429, acc 0.98
2016-09-05T21:08:17.825315: step 9021, loss 0.00388394, acc 1
2016-09-05T21:08:18.641276: step 9022, loss 0.0385463, acc 0.98
2016-09-05T21:08:19.445537: step 9023, loss 0.0657232, acc 0.98
2016-09-05T21:08:20.240881: step 9024, loss 0.0079026, acc 1
2016-09-05T21:08:21.033584: step 9025, loss 0.0198211, acc 1
2016-09-05T21:08:21.828181: step 9026, loss 0.022036, acc 0.98
2016-09-05T21:08:22.643297: step 9027, loss 0.02805, acc 0.98
2016-09-05T21:08:23.449435: step 9028, loss 0.0116286, acc 1
2016-09-05T21:08:24.249402: step 9029, loss 0.0145209, acc 1
2016-09-05T21:08:25.062924: step 9030, loss 0.0388444, acc 0.98
2016-09-05T21:08:25.894614: step 9031, loss 0.0260848, acc 1
2016-09-05T21:08:26.704455: step 9032, loss 0.0172802, acc 1
2016-09-05T21:08:27.500965: step 9033, loss 0.0293105, acc 0.98
2016-09-05T21:08:28.324104: step 9034, loss 0.0201262, acc 1
2016-09-05T21:08:29.120209: step 9035, loss 0.0314873, acc 0.98
2016-09-05T21:08:29.924364: step 9036, loss 0.00268099, acc 1
2016-09-05T21:08:30.750561: step 9037, loss 0.0159111, acc 1
2016-09-05T21:08:31.538095: step 9038, loss 0.00939565, acc 1
2016-09-05T21:08:32.332479: step 9039, loss 0.0130486, acc 1
2016-09-05T21:08:33.135616: step 9040, loss 0.00347525, acc 1
2016-09-05T21:08:33.925030: step 9041, loss 0.00280932, acc 1
2016-09-05T21:08:34.739034: step 9042, loss 0.123772, acc 0.96
2016-09-05T21:08:35.552527: step 9043, loss 0.00351068, acc 1
2016-09-05T21:08:36.351383: step 9044, loss 0.0624216, acc 0.98
2016-09-05T21:08:37.143749: step 9045, loss 0.0237926, acc 1
2016-09-05T21:08:37.957247: step 9046, loss 0.0294021, acc 0.98
2016-09-05T21:08:38.741354: step 9047, loss 0.0481341, acc 0.96
2016-09-05T21:08:39.551444: step 9048, loss 0.0168098, acc 0.98
2016-09-05T21:08:40.373912: step 9049, loss 0.013026, acc 1
2016-09-05T21:08:41.177405: step 9050, loss 0.0258369, acc 0.98
2016-09-05T21:08:41.990056: step 9051, loss 0.0136306, acc 1
2016-09-05T21:08:42.793814: step 9052, loss 0.00697514, acc 1
2016-09-05T21:08:43.585423: step 9053, loss 0.023208, acc 0.98
2016-09-05T21:08:44.396493: step 9054, loss 0.0271785, acc 1
2016-09-05T21:08:45.211272: step 9055, loss 0.0258627, acc 1
2016-09-05T21:08:46.010057: step 9056, loss 0.0262603, acc 0.98
2016-09-05T21:08:46.824263: step 9057, loss 0.00391163, acc 1
2016-09-05T21:08:47.657506: step 9058, loss 0.0506906, acc 0.96
2016-09-05T21:08:48.458712: step 9059, loss 0.0831367, acc 0.96
2016-09-05T21:08:49.272224: step 9060, loss 0.0122884, acc 1
2016-09-05T21:08:50.087021: step 9061, loss 0.131555, acc 0.94
2016-09-05T21:08:50.875118: step 9062, loss 0.017031, acc 1
2016-09-05T21:08:51.688406: step 9063, loss 0.0116255, acc 1
2016-09-05T21:08:52.516431: step 9064, loss 0.0597446, acc 0.98
2016-09-05T21:08:53.287454: step 9065, loss 0.095785, acc 0.96
2016-09-05T21:08:54.083199: step 9066, loss 0.0265872, acc 0.98
2016-09-05T21:08:54.900546: step 9067, loss 0.0109393, acc 1
2016-09-05T21:08:55.704720: step 9068, loss 0.00399147, acc 1
2016-09-05T21:08:56.492956: step 9069, loss 0.0307986, acc 0.98
2016-09-05T21:08:57.300691: step 9070, loss 0.0119962, acc 1
2016-09-05T21:08:58.129765: step 9071, loss 0.0161194, acc 1
2016-09-05T21:08:58.941405: step 9072, loss 0.00423444, acc 1
2016-09-05T21:08:59.756275: step 9073, loss 0.0200345, acc 0.98
2016-09-05T21:09:00.573734: step 9074, loss 0.00552583, acc 1
2016-09-05T21:09:01.368285: step 9075, loss 0.0295551, acc 1
2016-09-05T21:09:02.172645: step 9076, loss 0.0319763, acc 1
2016-09-05T21:09:02.964434: step 9077, loss 0.0833618, acc 0.96
2016-09-05T21:09:03.756894: step 9078, loss 0.0395122, acc 0.96
2016-09-05T21:09:04.546156: step 9079, loss 0.0208533, acc 0.98
2016-09-05T21:09:05.358609: step 9080, loss 0.00279317, acc 1
2016-09-05T21:09:06.168663: step 9081, loss 0.0174115, acc 1
2016-09-05T21:09:06.969415: step 9082, loss 0.0111997, acc 1
2016-09-05T21:09:07.771572: step 9083, loss 0.035187, acc 0.98
2016-09-05T21:09:08.570136: step 9084, loss 0.0243557, acc 1
2016-09-05T21:09:09.386629: step 9085, loss 0.00304387, acc 1
2016-09-05T21:09:10.179716: step 9086, loss 0.0232362, acc 1
2016-09-05T21:09:10.969434: step 9087, loss 0.0179299, acc 0.98
2016-09-05T21:09:11.762496: step 9088, loss 0.120487, acc 0.98
2016-09-05T21:09:12.554747: step 9089, loss 0.00274751, acc 1
2016-09-05T21:09:13.395139: step 9090, loss 0.0437066, acc 0.98
2016-09-05T21:09:14.223114: step 9091, loss 0.0131277, acc 1
2016-09-05T21:09:15.016071: step 9092, loss 0.0111785, acc 1
2016-09-05T21:09:15.823170: step 9093, loss 0.02081, acc 1
2016-09-05T21:09:16.629682: step 9094, loss 0.00926711, acc 1
2016-09-05T21:09:17.409606: step 9095, loss 0.00872175, acc 1
2016-09-05T21:09:18.212679: step 9096, loss 0.0118719, acc 1
2016-09-05T21:09:19.023333: step 9097, loss 0.0161818, acc 1
2016-09-05T21:09:19.813113: step 9098, loss 0.00256772, acc 1
2016-09-05T21:09:20.625110: step 9099, loss 0.0190023, acc 1
2016-09-05T21:09:21.433402: step 9100, loss 0.0077867, acc 1

Evaluation:
2016-09-05T21:09:24.950982: step 9100, loss 2.19709, acc 0.725

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-9100

2016-09-05T21:09:26.816793: step 9101, loss 0.00287548, acc 1
2016-09-05T21:09:27.617094: step 9102, loss 0.00507354, acc 1
2016-09-05T21:09:28.456915: step 9103, loss 0.0521554, acc 0.96
2016-09-05T21:09:29.263185: step 9104, loss 0.0556879, acc 0.96
2016-09-05T21:09:30.079130: step 9105, loss 0.0447075, acc 0.98
2016-09-05T21:09:30.847266: step 9106, loss 0.00384899, acc 1
2016-09-05T21:09:31.668186: step 9107, loss 0.00388317, acc 1
2016-09-05T21:09:32.478969: step 9108, loss 0.0101945, acc 1
2016-09-05T21:09:33.267712: step 9109, loss 0.104299, acc 0.96
2016-09-05T21:09:34.081613: step 9110, loss 0.112374, acc 0.98
2016-09-05T21:09:34.923713: step 9111, loss 0.0194685, acc 0.98
2016-09-05T21:09:35.723962: step 9112, loss 0.0638248, acc 0.98
2016-09-05T21:09:36.541482: step 9113, loss 0.0118053, acc 1
2016-09-05T21:09:37.365166: step 9114, loss 0.00744098, acc 1
2016-09-05T21:09:38.153936: step 9115, loss 0.0971571, acc 0.98
2016-09-05T21:09:38.943388: step 9116, loss 0.0234999, acc 0.98
2016-09-05T21:09:39.775952: step 9117, loss 0.0918047, acc 0.98
2016-09-05T21:09:40.185981: step 9118, loss 0.111772, acc 0.916667
2016-09-05T21:09:41.006133: step 9119, loss 0.0286697, acc 0.98
2016-09-05T21:09:41.799795: step 9120, loss 0.00346132, acc 1
2016-09-05T21:09:42.602395: step 9121, loss 0.0424846, acc 0.98
2016-09-05T21:09:43.407530: step 9122, loss 0.004423, acc 1
2016-09-05T21:09:44.209262: step 9123, loss 0.0119092, acc 1
2016-09-05T21:09:44.991485: step 9124, loss 0.0729731, acc 0.98
2016-09-05T21:09:45.819043: step 9125, loss 0.0257905, acc 1
2016-09-05T21:09:46.591431: step 9126, loss 0.0171018, acc 1
2016-09-05T21:09:47.388205: step 9127, loss 0.0362149, acc 0.98
2016-09-05T21:09:48.245367: step 9128, loss 0.00960484, acc 1
2016-09-05T21:09:49.057651: step 9129, loss 0.00646131, acc 1
2016-09-05T21:09:49.996344: step 9130, loss 0.0433436, acc 0.98
2016-09-05T21:09:50.844963: step 9131, loss 0.0157061, acc 1
2016-09-05T21:09:51.666071: step 9132, loss 0.121256, acc 0.96
2016-09-05T21:09:52.636694: step 9133, loss 0.0122653, acc 1
2016-09-05T21:09:53.514186: step 9134, loss 0.019411, acc 1
2016-09-05T21:09:54.330979: step 9135, loss 0.024891, acc 0.98
2016-09-05T21:09:55.142332: step 9136, loss 0.00730108, acc 1
2016-09-05T21:09:55.955676: step 9137, loss 0.0352395, acc 0.98
2016-09-05T21:09:56.807078: step 9138, loss 0.0120604, acc 1
2016-09-05T21:09:57.617748: step 9139, loss 0.0538888, acc 0.98
2016-09-05T21:09:58.429571: step 9140, loss 0.0235235, acc 0.98
2016-09-05T21:09:59.254775: step 9141, loss 0.0321361, acc 0.98
2016-09-05T21:10:00.077601: step 9142, loss 0.0120871, acc 1
2016-09-05T21:10:00.937438: step 9143, loss 0.0683194, acc 0.98
2016-09-05T21:10:01.772604: step 9144, loss 0.0120039, acc 1
2016-09-05T21:10:02.583206: step 9145, loss 0.00735712, acc 1
2016-09-05T21:10:03.369036: step 9146, loss 0.00952506, acc 1
2016-09-05T21:10:04.177500: step 9147, loss 0.00735469, acc 1
2016-09-05T21:10:04.976135: step 9148, loss 0.008982, acc 1
2016-09-05T21:10:05.792038: step 9149, loss 0.0678546, acc 0.98
2016-09-05T21:10:06.621546: step 9150, loss 0.0434354, acc 1
2016-09-05T21:10:07.427041: step 9151, loss 0.0285622, acc 1
2016-09-05T21:10:08.235330: step 9152, loss 0.0318695, acc 0.98
2016-09-05T21:10:09.060765: step 9153, loss 0.00927972, acc 1
2016-09-05T21:10:09.858540: step 9154, loss 0.0213263, acc 1
2016-09-05T21:10:10.663566: step 9155, loss 0.0162278, acc 1
2016-09-05T21:10:11.484741: step 9156, loss 0.115739, acc 0.96
2016-09-05T21:10:12.298564: step 9157, loss 0.00863506, acc 1
2016-09-05T21:10:13.121091: step 9158, loss 0.00675417, acc 1
2016-09-05T21:10:13.942862: step 9159, loss 0.0231081, acc 1
2016-09-05T21:10:14.749941: step 9160, loss 0.011544, acc 1
2016-09-05T21:10:15.576108: step 9161, loss 0.0135511, acc 1
2016-09-05T21:10:16.392952: step 9162, loss 0.0067627, acc 1
2016-09-05T21:10:17.207037: step 9163, loss 0.00652526, acc 1
2016-09-05T21:10:17.982381: step 9164, loss 0.0214365, acc 0.98
2016-09-05T21:10:18.790980: step 9165, loss 0.00761917, acc 1
2016-09-05T21:10:19.605531: step 9166, loss 0.00771319, acc 1
2016-09-05T21:10:20.384241: step 9167, loss 0.00626549, acc 1
2016-09-05T21:10:21.198022: step 9168, loss 0.00809849, acc 1
2016-09-05T21:10:22.002916: step 9169, loss 0.0112252, acc 1
2016-09-05T21:10:22.806920: step 9170, loss 0.00832023, acc 1
2016-09-05T21:10:23.601272: step 9171, loss 0.00598399, acc 1
2016-09-05T21:10:24.416654: step 9172, loss 0.00600332, acc 1
2016-09-05T21:10:25.214755: step 9173, loss 0.0294421, acc 1
2016-09-05T21:10:26.005489: step 9174, loss 0.0898621, acc 0.96
2016-09-05T21:10:26.845602: step 9175, loss 0.0187507, acc 1
2016-09-05T21:10:27.706534: step 9176, loss 0.00587813, acc 1
2016-09-05T21:10:28.541944: step 9177, loss 0.0145814, acc 1
2016-09-05T21:10:29.348737: step 9178, loss 0.00564311, acc 1
2016-09-05T21:10:30.118938: step 9179, loss 0.0312747, acc 0.98
2016-09-05T21:10:30.906223: step 9180, loss 0.0218948, acc 1
2016-09-05T21:10:31.703475: step 9181, loss 0.0105529, acc 1
2016-09-05T21:10:32.480995: step 9182, loss 0.00535643, acc 1
2016-09-05T21:10:33.336314: step 9183, loss 0.0131718, acc 1
2016-09-05T21:10:34.171116: step 9184, loss 0.0110959, acc 1
2016-09-05T21:10:34.961511: step 9185, loss 0.0163592, acc 1
2016-09-05T21:10:35.748698: step 9186, loss 0.0106294, acc 1
2016-09-05T21:10:36.553697: step 9187, loss 0.0400416, acc 1
2016-09-05T21:10:37.356106: step 9188, loss 0.0301586, acc 1
2016-09-05T21:10:38.139804: step 9189, loss 0.0116586, acc 1
2016-09-05T21:10:38.943292: step 9190, loss 0.0259483, acc 1
2016-09-05T21:10:39.715700: step 9191, loss 0.00635116, acc 1
2016-09-05T21:10:40.539846: step 9192, loss 0.00539762, acc 1
2016-09-05T21:10:41.368164: step 9193, loss 0.0109037, acc 1
2016-09-05T21:10:42.167266: step 9194, loss 0.0586647, acc 0.94
2016-09-05T21:10:42.979879: step 9195, loss 0.0224623, acc 0.98
2016-09-05T21:10:43.809589: step 9196, loss 0.00526582, acc 1
2016-09-05T21:10:44.612573: step 9197, loss 0.0239517, acc 1
2016-09-05T21:10:45.401367: step 9198, loss 0.0221023, acc 0.98
2016-09-05T21:10:46.249808: step 9199, loss 0.00954456, acc 1
2016-09-05T21:10:47.025455: step 9200, loss 0.0288901, acc 0.98

Evaluation:
2016-09-05T21:10:50.540998: step 9200, loss 2.79661, acc 0.72

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-9200

2016-09-05T21:10:52.425791: step 9201, loss 0.0203915, acc 1
2016-09-05T21:10:53.212260: step 9202, loss 0.013428, acc 1
2016-09-05T21:10:54.010055: step 9203, loss 0.017675, acc 1
2016-09-05T21:10:54.824089: step 9204, loss 0.00989127, acc 1
2016-09-05T21:10:55.605878: step 9205, loss 0.0047175, acc 1
2016-09-05T21:10:56.409867: step 9206, loss 0.0555291, acc 0.98
2016-09-05T21:10:57.225506: step 9207, loss 0.00891877, acc 1
2016-09-05T21:10:58.013875: step 9208, loss 0.019058, acc 1
2016-09-05T21:10:58.808345: step 9209, loss 0.0214299, acc 0.98
2016-09-05T21:10:59.607308: step 9210, loss 0.00466443, acc 1
2016-09-05T21:11:00.397399: step 9211, loss 0.0187937, acc 1
2016-09-05T21:11:01.210609: step 9212, loss 0.0182014, acc 1
2016-09-05T21:11:02.002669: step 9213, loss 0.0382233, acc 0.96
2016-09-05T21:11:02.771077: step 9214, loss 0.00445283, acc 1
2016-09-05T21:11:03.578393: step 9215, loss 0.0179198, acc 1
2016-09-05T21:11:04.383742: step 9216, loss 0.107859, acc 0.96
2016-09-05T21:11:05.159625: step 9217, loss 0.0352969, acc 0.98
2016-09-05T21:11:05.983356: step 9218, loss 0.0064549, acc 1
2016-09-05T21:11:06.797399: step 9219, loss 0.00417979, acc 1
2016-09-05T21:11:07.619118: step 9220, loss 0.00825895, acc 1
2016-09-05T21:11:08.417616: step 9221, loss 0.0121098, acc 1
2016-09-05T21:11:09.218081: step 9222, loss 0.0173465, acc 1
2016-09-05T21:11:09.986402: step 9223, loss 0.0256824, acc 0.98
2016-09-05T21:11:10.817320: step 9224, loss 0.0687565, acc 0.98
2016-09-05T21:11:11.616944: step 9225, loss 0.0053731, acc 1
2016-09-05T21:11:12.436686: step 9226, loss 0.0446028, acc 0.98
2016-09-05T21:11:13.248419: step 9227, loss 0.00581045, acc 1
2016-09-05T21:11:14.080463: step 9228, loss 0.0817173, acc 0.98
2016-09-05T21:11:14.908824: step 9229, loss 0.0583722, acc 0.96
2016-09-05T21:11:15.726683: step 9230, loss 0.00537203, acc 1
2016-09-05T21:11:16.529976: step 9231, loss 0.00576166, acc 1
2016-09-05T21:11:17.322912: step 9232, loss 0.018106, acc 0.98
2016-09-05T21:11:18.127355: step 9233, loss 0.0395166, acc 0.98
2016-09-05T21:11:18.965750: step 9234, loss 0.0224253, acc 1
2016-09-05T21:11:19.721961: step 9235, loss 0.0124374, acc 1
2016-09-05T21:11:20.500749: step 9236, loss 0.0138324, acc 1
2016-09-05T21:11:21.297056: step 9237, loss 0.00464537, acc 1
2016-09-05T21:11:22.089432: step 9238, loss 0.0306768, acc 1
2016-09-05T21:11:22.945889: step 9239, loss 0.0032259, acc 1
2016-09-05T21:11:23.792499: step 9240, loss 0.0032306, acc 1
2016-09-05T21:11:24.577870: step 9241, loss 0.0624126, acc 0.98
2016-09-05T21:11:25.372215: step 9242, loss 0.00372183, acc 1
2016-09-05T21:11:26.197473: step 9243, loss 0.0290442, acc 0.98
2016-09-05T21:11:27.022027: step 9244, loss 0.049753, acc 0.96
2016-09-05T21:11:27.827607: step 9245, loss 0.00959932, acc 1
2016-09-05T21:11:28.638152: step 9246, loss 0.00849546, acc 1
2016-09-05T21:11:29.416341: step 9247, loss 0.0266419, acc 0.98
2016-09-05T21:11:30.212340: step 9248, loss 0.0812468, acc 0.98
2016-09-05T21:11:31.029333: step 9249, loss 0.011407, acc 1
2016-09-05T21:11:31.796875: step 9250, loss 0.0280341, acc 1
2016-09-05T21:11:32.633374: step 9251, loss 0.00284202, acc 1
2016-09-05T21:11:33.448519: step 9252, loss 0.0088094, acc 1
2016-09-05T21:11:34.217848: step 9253, loss 0.0321529, acc 1
2016-09-05T21:11:35.010471: step 9254, loss 0.0522081, acc 0.98
2016-09-05T21:11:35.837095: step 9255, loss 0.00474482, acc 1
2016-09-05T21:11:36.625590: step 9256, loss 0.00800023, acc 1
2016-09-05T21:11:37.427641: step 9257, loss 0.0165763, acc 1
2016-09-05T21:11:38.233662: step 9258, loss 0.0219374, acc 0.98
2016-09-05T21:11:39.034349: step 9259, loss 0.0126545, acc 1
2016-09-05T21:11:39.846841: step 9260, loss 0.0164362, acc 1
2016-09-05T21:11:40.645733: step 9261, loss 0.0193884, acc 1
2016-09-05T21:11:41.460043: step 9262, loss 0.00611993, acc 1
2016-09-05T21:11:42.268978: step 9263, loss 0.0116597, acc 1
2016-09-05T21:11:43.098599: step 9264, loss 0.0097745, acc 1
2016-09-05T21:11:43.885462: step 9265, loss 0.0503766, acc 0.98
2016-09-05T21:11:44.679806: step 9266, loss 0.0115235, acc 1
2016-09-05T21:11:45.502672: step 9267, loss 0.0192266, acc 0.98
2016-09-05T21:11:46.313203: step 9268, loss 0.0308025, acc 0.98
2016-09-05T21:11:47.143931: step 9269, loss 0.0263629, acc 0.98
2016-09-05T21:11:47.957668: step 9270, loss 0.046498, acc 0.98
2016-09-05T21:11:48.734000: step 9271, loss 0.0791979, acc 0.94
2016-09-05T21:11:49.528718: step 9272, loss 0.0361491, acc 0.98
2016-09-05T21:11:50.386798: step 9273, loss 0.00300075, acc 1
2016-09-05T21:11:51.155772: step 9274, loss 0.0115135, acc 1
2016-09-05T21:11:51.938611: step 9275, loss 0.00304377, acc 1
2016-09-05T21:11:52.756456: step 9276, loss 0.0682003, acc 0.98
2016-09-05T21:11:53.553855: step 9277, loss 0.00977191, acc 1
2016-09-05T21:11:54.401410: step 9278, loss 0.00325656, acc 1
2016-09-05T21:11:55.224140: step 9279, loss 0.00357253, acc 1
2016-09-05T21:11:56.022062: step 9280, loss 0.0212678, acc 0.98
2016-09-05T21:11:56.824366: step 9281, loss 0.00416403, acc 1
2016-09-05T21:11:57.631842: step 9282, loss 0.00605906, acc 1
2016-09-05T21:11:58.433396: step 9283, loss 0.00425971, acc 1
2016-09-05T21:11:59.229821: step 9284, loss 0.041882, acc 0.98
2016-09-05T21:12:00.063955: step 9285, loss 0.00492354, acc 1
2016-09-05T21:12:00.866209: step 9286, loss 0.051838, acc 0.94
2016-09-05T21:12:01.669668: step 9287, loss 0.00356833, acc 1
2016-09-05T21:12:02.495794: step 9288, loss 0.0133659, acc 1
2016-09-05T21:12:03.287164: step 9289, loss 0.00419094, acc 1
2016-09-05T21:12:04.082777: step 9290, loss 0.00283826, acc 1
2016-09-05T21:12:04.895184: step 9291, loss 0.0175444, acc 0.98
2016-09-05T21:12:05.671755: step 9292, loss 0.022089, acc 0.98
2016-09-05T21:12:06.461090: step 9293, loss 0.0592098, acc 0.94
2016-09-05T21:12:07.306176: step 9294, loss 0.0253146, acc 0.98
2016-09-05T21:12:08.099376: step 9295, loss 0.0497193, acc 0.98
2016-09-05T21:12:08.925330: step 9296, loss 0.00807593, acc 1
2016-09-05T21:12:09.736821: step 9297, loss 0.0155432, acc 1
2016-09-05T21:12:10.517458: step 9298, loss 0.00416525, acc 1
2016-09-05T21:12:11.296369: step 9299, loss 0.0284101, acc 0.98
2016-09-05T21:12:12.107328: step 9300, loss 0.00304573, acc 1

Evaluation:
2016-09-05T21:12:15.622322: step 9300, loss 2.38484, acc 0.735

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-9300

2016-09-05T21:12:17.556739: step 9301, loss 0.00305836, acc 1
2016-09-05T21:12:18.423276: step 9302, loss 0.0223423, acc 1
2016-09-05T21:12:19.196876: step 9303, loss 0.0100963, acc 1
2016-09-05T21:12:19.992026: step 9304, loss 0.0146864, acc 1
2016-09-05T21:12:20.821411: step 9305, loss 0.0189224, acc 1
2016-09-05T21:12:21.608881: step 9306, loss 0.0119625, acc 1
2016-09-05T21:12:22.442797: step 9307, loss 0.00923054, acc 1
2016-09-05T21:12:23.258828: step 9308, loss 0.0400471, acc 0.96
2016-09-05T21:12:24.041829: step 9309, loss 0.0326357, acc 0.98
2016-09-05T21:12:24.851943: step 9310, loss 0.0241408, acc 1
2016-09-05T21:12:25.673662: step 9311, loss 0.159652, acc 0.98
2016-09-05T21:12:26.110279: step 9312, loss 0.0816886, acc 1
2016-09-05T21:12:26.944554: step 9313, loss 0.00424345, acc 1
2016-09-05T21:12:27.757460: step 9314, loss 0.00969905, acc 1
2016-09-05T21:12:28.574012: step 9315, loss 0.0439975, acc 0.98
2016-09-05T21:12:29.411519: step 9316, loss 0.0173607, acc 0.98
2016-09-05T21:12:30.225608: step 9317, loss 0.00339657, acc 1
2016-09-05T21:12:31.017865: step 9318, loss 0.0202053, acc 1
2016-09-05T21:12:31.831607: step 9319, loss 0.00597903, acc 1
2016-09-05T21:12:32.655515: step 9320, loss 0.0133158, acc 1
2016-09-05T21:12:33.434196: step 9321, loss 0.0189201, acc 1
2016-09-05T21:12:34.280175: step 9322, loss 0.00584167, acc 1
2016-09-05T21:12:35.094437: step 9323, loss 0.00327757, acc 1
2016-09-05T21:12:35.884229: step 9324, loss 0.00389005, acc 1
2016-09-05T21:12:36.689928: step 9325, loss 0.00835724, acc 1
2016-09-05T21:12:37.497939: step 9326, loss 0.0260588, acc 1
2016-09-05T21:12:38.290588: step 9327, loss 0.0383847, acc 0.98
2016-09-05T21:12:39.118840: step 9328, loss 0.00338362, acc 1
2016-09-05T21:12:39.937710: step 9329, loss 0.00966045, acc 1
2016-09-05T21:12:40.745496: step 9330, loss 0.0125182, acc 1
2016-09-05T21:12:41.578429: step 9331, loss 0.00372004, acc 1
2016-09-05T21:12:42.377362: step 9332, loss 0.00344178, acc 1
2016-09-05T21:12:43.136480: step 9333, loss 0.00784841, acc 1
2016-09-05T21:12:43.928981: step 9334, loss 0.348973, acc 0.96
2016-09-05T21:12:44.763316: step 9335, loss 0.0335883, acc 0.98
2016-09-05T21:12:45.542664: step 9336, loss 0.00711787, acc 1
2016-09-05T21:12:46.361737: step 9337, loss 0.0104756, acc 1
2016-09-05T21:12:47.177687: step 9338, loss 0.0101124, acc 1
2016-09-05T21:12:47.968563: step 9339, loss 0.00287267, acc 1
2016-09-05T21:12:48.778675: step 9340, loss 0.0253892, acc 0.98
2016-09-05T21:12:49.589953: step 9341, loss 0.00381162, acc 1
2016-09-05T21:12:50.366881: step 9342, loss 0.0245645, acc 0.98
2016-09-05T21:12:51.173004: step 9343, loss 0.00390112, acc 1
2016-09-05T21:12:52.008138: step 9344, loss 0.0407487, acc 0.98
2016-09-05T21:12:52.808733: step 9345, loss 0.0169653, acc 0.98
2016-09-05T21:12:53.614597: step 9346, loss 0.0397797, acc 0.96
2016-09-05T21:12:54.436203: step 9347, loss 0.00290313, acc 1
2016-09-05T21:12:55.235894: step 9348, loss 0.02497, acc 1
2016-09-05T21:12:56.067576: step 9349, loss 0.00974285, acc 1
2016-09-05T21:12:56.894415: step 9350, loss 0.0107443, acc 1
2016-09-05T21:12:57.676757: step 9351, loss 0.00353119, acc 1
2016-09-05T21:12:58.507316: step 9352, loss 0.00285673, acc 1
2016-09-05T21:12:59.319240: step 9353, loss 0.0251918, acc 0.98
2016-09-05T21:13:00.099905: step 9354, loss 0.0269224, acc 0.98
2016-09-05T21:13:00.922262: step 9355, loss 0.00352828, acc 1
2016-09-05T21:13:01.742161: step 9356, loss 0.100251, acc 0.98
2016-09-05T21:13:02.522033: step 9357, loss 0.0409265, acc 0.98
2016-09-05T21:13:03.310999: step 9358, loss 0.0206894, acc 1
2016-09-05T21:13:04.112434: step 9359, loss 0.0119907, acc 1
2016-09-05T21:13:04.903980: step 9360, loss 0.0575535, acc 0.98
2016-09-05T21:13:05.699439: step 9361, loss 0.00761778, acc 1
2016-09-05T21:13:06.519123: step 9362, loss 0.0114051, acc 1
2016-09-05T21:13:07.297483: step 9363, loss 0.0110631, acc 1
2016-09-05T21:13:08.107330: step 9364, loss 0.00375652, acc 1
2016-09-05T21:13:08.923985: step 9365, loss 0.021178, acc 0.98
2016-09-05T21:13:09.720864: step 9366, loss 0.0157868, acc 1
2016-09-05T21:13:10.527657: step 9367, loss 0.0236095, acc 0.98
2016-09-05T21:13:11.361387: step 9368, loss 0.0962471, acc 0.98
2016-09-05T21:13:12.161693: step 9369, loss 0.0532572, acc 0.96
2016-09-05T21:13:12.965243: step 9370, loss 0.00294049, acc 1
2016-09-05T21:13:13.805410: step 9371, loss 0.0202009, acc 1
2016-09-05T21:13:14.593526: step 9372, loss 0.0208308, acc 0.98
2016-09-05T21:13:15.440849: step 9373, loss 0.0230692, acc 1
2016-09-05T21:13:16.260993: step 9374, loss 0.00598454, acc 1
2016-09-05T21:13:17.030526: step 9375, loss 0.0820034, acc 0.96
2016-09-05T21:13:17.825850: step 9376, loss 0.0349773, acc 0.98
2016-09-05T21:13:18.626835: step 9377, loss 0.0544441, acc 0.98
2016-09-05T21:13:19.414512: step 9378, loss 0.0368523, acc 0.98
2016-09-05T21:13:20.212976: step 9379, loss 0.00425131, acc 1
2016-09-05T21:13:21.035443: step 9380, loss 0.0150677, acc 1
2016-09-05T21:13:21.800342: step 9381, loss 0.0755812, acc 0.96
2016-09-05T21:13:22.612267: step 9382, loss 0.0179794, acc 1
2016-09-05T21:13:23.433247: step 9383, loss 0.0030837, acc 1
2016-09-05T21:13:24.224258: step 9384, loss 0.0185684, acc 0.98
2016-09-05T21:13:25.011909: step 9385, loss 0.00551038, acc 1
2016-09-05T21:13:25.815571: step 9386, loss 0.00623699, acc 1
2016-09-05T21:13:26.614366: step 9387, loss 0.0198999, acc 1
2016-09-05T21:13:27.421115: step 9388, loss 0.0422789, acc 0.98
2016-09-05T21:13:28.219213: step 9389, loss 0.00518584, acc 1
2016-09-05T21:13:29.027774: step 9390, loss 0.0057062, acc 1
2016-09-05T21:13:29.833377: step 9391, loss 0.00718677, acc 1
2016-09-05T21:13:30.636635: step 9392, loss 0.0168407, acc 0.98
2016-09-05T21:13:31.447724: step 9393, loss 0.00347385, acc 1
2016-09-05T21:13:32.265239: step 9394, loss 0.0182479, acc 0.98
2016-09-05T21:13:33.085139: step 9395, loss 0.0274528, acc 1
2016-09-05T21:13:33.877615: step 9396, loss 0.0551455, acc 0.98
2016-09-05T21:13:34.714756: step 9397, loss 0.0101973, acc 1
2016-09-05T21:13:35.504535: step 9398, loss 0.023157, acc 0.98
2016-09-05T21:13:36.285154: step 9399, loss 0.0026233, acc 1
2016-09-05T21:13:37.100170: step 9400, loss 0.00594246, acc 1

Evaluation:
2016-09-05T21:13:40.584168: step 9400, loss 1.99597, acc 0.733

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-9400

2016-09-05T21:13:42.475862: step 9401, loss 0.0352296, acc 0.98
2016-09-05T21:13:43.314292: step 9402, loss 0.019824, acc 1
2016-09-05T21:13:44.111151: step 9403, loss 0.00524664, acc 1
2016-09-05T21:13:44.902771: step 9404, loss 0.0250534, acc 0.98
2016-09-05T21:13:45.698728: step 9405, loss 0.0180348, acc 1
2016-09-05T21:13:46.506486: step 9406, loss 0.016236, acc 1
2016-09-05T21:13:47.323957: step 9407, loss 0.019039, acc 1
2016-09-05T21:13:48.149637: step 9408, loss 0.0111928, acc 1
2016-09-05T21:13:48.965404: step 9409, loss 0.0174302, acc 1
2016-09-05T21:13:49.773427: step 9410, loss 0.0502941, acc 0.96
2016-09-05T21:13:50.550488: step 9411, loss 0.00891468, acc 1
2016-09-05T21:13:51.357604: step 9412, loss 0.0164578, acc 1
2016-09-05T21:13:52.168549: step 9413, loss 0.00421223, acc 1
2016-09-05T21:13:52.948661: step 9414, loss 0.0080789, acc 1
2016-09-05T21:13:53.756506: step 9415, loss 0.0249723, acc 0.98
2016-09-05T21:13:54.541845: step 9416, loss 0.0384823, acc 0.96
2016-09-05T21:13:55.376306: step 9417, loss 0.0264268, acc 0.98
2016-09-05T21:13:56.201985: step 9418, loss 0.054418, acc 0.98
2016-09-05T21:13:56.993725: step 9419, loss 0.00308951, acc 1
2016-09-05T21:13:57.803382: step 9420, loss 0.020697, acc 0.98
2016-09-05T21:13:58.627321: step 9421, loss 0.00676284, acc 1
2016-09-05T21:13:59.409230: step 9422, loss 0.0163677, acc 1
2016-09-05T21:14:00.218230: step 9423, loss 0.0543295, acc 0.96
2016-09-05T21:14:01.038095: step 9424, loss 0.0418128, acc 0.98
2016-09-05T21:14:01.832990: step 9425, loss 0.0156383, acc 1
2016-09-05T21:14:02.636159: step 9426, loss 0.0213978, acc 1
2016-09-05T21:14:03.450687: step 9427, loss 0.00413885, acc 1
2016-09-05T21:14:04.256050: step 9428, loss 0.0208422, acc 1
2016-09-05T21:14:05.081296: step 9429, loss 0.0686715, acc 0.96
2016-09-05T21:14:05.883394: step 9430, loss 0.00807027, acc 1
2016-09-05T21:14:06.714465: step 9431, loss 0.00269783, acc 1
2016-09-05T21:14:07.519653: step 9432, loss 0.0214505, acc 0.98
2016-09-05T21:14:08.316964: step 9433, loss 0.00375851, acc 1
2016-09-05T21:14:09.090658: step 9434, loss 0.0386791, acc 0.98
2016-09-05T21:14:09.928567: step 9435, loss 0.0345025, acc 0.98
2016-09-05T21:14:10.740357: step 9436, loss 0.00630979, acc 1
2016-09-05T21:14:11.498630: step 9437, loss 0.0135372, acc 1
2016-09-05T21:14:12.301333: step 9438, loss 0.0199203, acc 1
2016-09-05T21:14:13.119699: step 9439, loss 0.0467123, acc 0.96
2016-09-05T21:14:13.906512: step 9440, loss 0.0325499, acc 0.98
2016-09-05T21:14:14.713415: step 9441, loss 0.00291457, acc 1
2016-09-05T21:14:15.531181: step 9442, loss 0.0263781, acc 0.98
2016-09-05T21:14:16.351511: step 9443, loss 0.0323927, acc 0.98
2016-09-05T21:14:17.172284: step 9444, loss 0.00903487, acc 1
2016-09-05T21:14:17.976375: step 9445, loss 0.0660268, acc 0.98
2016-09-05T21:14:18.761620: step 9446, loss 0.0333507, acc 0.98
2016-09-05T21:14:19.570855: step 9447, loss 0.0459037, acc 0.98
2016-09-05T21:14:20.402114: step 9448, loss 0.00355481, acc 1
2016-09-05T21:14:21.147736: step 9449, loss 0.00722812, acc 1
2016-09-05T21:14:21.945043: step 9450, loss 0.00283603, acc 1
2016-09-05T21:14:22.766374: step 9451, loss 0.0234827, acc 0.98
2016-09-05T21:14:23.570749: step 9452, loss 0.0297791, acc 1
2016-09-05T21:14:24.399229: step 9453, loss 0.0201389, acc 0.98
2016-09-05T21:14:25.227281: step 9454, loss 0.0423215, acc 0.96
2016-09-05T21:14:25.990040: step 9455, loss 0.0139152, acc 1
2016-09-05T21:14:26.793400: step 9456, loss 0.0322085, acc 0.98
2016-09-05T21:14:27.598888: step 9457, loss 0.00341612, acc 1
2016-09-05T21:14:28.394041: step 9458, loss 0.00828597, acc 1
2016-09-05T21:14:29.213276: step 9459, loss 0.016182, acc 1
2016-09-05T21:14:30.031173: step 9460, loss 0.012964, acc 1
2016-09-05T21:14:30.826386: step 9461, loss 0.0046033, acc 1
2016-09-05T21:14:31.611546: step 9462, loss 0.00237646, acc 1
2016-09-05T21:14:32.410479: step 9463, loss 0.0320608, acc 0.98
2016-09-05T21:14:33.197581: step 9464, loss 0.0485768, acc 0.96
2016-09-05T21:14:34.017547: step 9465, loss 0.0170002, acc 0.98
2016-09-05T21:14:34.833443: step 9466, loss 0.125745, acc 0.98
2016-09-05T21:14:35.639369: step 9467, loss 0.0254858, acc 0.98
2016-09-05T21:14:36.446870: step 9468, loss 0.00252895, acc 1
2016-09-05T21:14:37.241240: step 9469, loss 0.00358942, acc 1
2016-09-05T21:14:38.049427: step 9470, loss 0.00520968, acc 1
2016-09-05T21:14:38.858277: step 9471, loss 0.00353493, acc 1
2016-09-05T21:14:39.671642: step 9472, loss 0.00291879, acc 1
2016-09-05T21:14:40.453085: step 9473, loss 0.00239888, acc 1
2016-09-05T21:14:41.274582: step 9474, loss 0.0178595, acc 0.98
2016-09-05T21:14:42.068161: step 9475, loss 0.0159488, acc 1
2016-09-05T21:14:42.908115: step 9476, loss 0.0135141, acc 1
2016-09-05T21:14:43.705213: step 9477, loss 0.00245579, acc 1
2016-09-05T21:14:44.536159: step 9478, loss 0.0510394, acc 0.96
2016-09-05T21:14:45.326015: step 9479, loss 0.0481007, acc 0.98
2016-09-05T21:14:46.130694: step 9480, loss 0.00474571, acc 1
2016-09-05T21:14:46.961896: step 9481, loss 0.0275268, acc 0.98
2016-09-05T21:14:47.770651: step 9482, loss 0.00896561, acc 1
2016-09-05T21:14:48.585465: step 9483, loss 0.0274655, acc 1
2016-09-05T21:14:49.377002: step 9484, loss 0.00337047, acc 1
2016-09-05T21:14:50.173981: step 9485, loss 0.0214307, acc 0.98
2016-09-05T21:14:50.962595: step 9486, loss 0.0235603, acc 1
2016-09-05T21:14:51.757370: step 9487, loss 0.0274433, acc 0.98
2016-09-05T21:14:52.601985: step 9488, loss 0.0137016, acc 1
2016-09-05T21:14:53.424194: step 9489, loss 0.00216811, acc 1
2016-09-05T21:14:54.214449: step 9490, loss 0.00647888, acc 1
2016-09-05T21:14:55.035073: step 9491, loss 0.0177957, acc 0.98
2016-09-05T21:14:55.834286: step 9492, loss 0.0467292, acc 0.98
2016-09-05T21:14:56.646467: step 9493, loss 0.0620303, acc 0.98
2016-09-05T21:14:57.403745: step 9494, loss 0.00337314, acc 1
2016-09-05T21:14:58.211358: step 9495, loss 0.0484322, acc 0.98
2016-09-05T21:14:59.056288: step 9496, loss 0.00508369, acc 1
2016-09-05T21:14:59.872271: step 9497, loss 0.0415192, acc 0.98
2016-09-05T21:15:00.702042: step 9498, loss 0.00647051, acc 1
2016-09-05T21:15:01.499923: step 9499, loss 0.0907574, acc 0.94
2016-09-05T21:15:02.259828: step 9500, loss 0.00338209, acc 1

Evaluation:
2016-09-05T21:15:05.753216: step 9500, loss 2.16226, acc 0.733

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-9500

2016-09-05T21:15:07.594574: step 9501, loss 0.00249848, acc 1
2016-09-05T21:15:08.370231: step 9502, loss 0.0202286, acc 1
2016-09-05T21:15:09.198863: step 9503, loss 0.0286316, acc 0.98
2016-09-05T21:15:10.038273: step 9504, loss 0.0190417, acc 0.98
2016-09-05T21:15:10.847289: step 9505, loss 0.01479, acc 1
2016-09-05T21:15:11.271981: step 9506, loss 0.0345858, acc 1
2016-09-05T21:15:12.050022: step 9507, loss 0.0416736, acc 0.98
2016-09-05T21:15:12.847633: step 9508, loss 0.0378248, acc 0.98
2016-09-05T21:15:13.640575: step 9509, loss 0.0562488, acc 0.98
2016-09-05T21:15:14.418076: step 9510, loss 0.00414871, acc 1
2016-09-05T21:15:15.235361: step 9511, loss 0.014161, acc 1
2016-09-05T21:15:16.024087: step 9512, loss 0.00423238, acc 1
2016-09-05T21:15:16.816673: step 9513, loss 0.00339814, acc 1
2016-09-05T21:15:17.647695: step 9514, loss 0.00603709, acc 1
2016-09-05T21:15:18.475148: step 9515, loss 0.0027818, acc 1
2016-09-05T21:15:19.267627: step 9516, loss 0.00688325, acc 1
2016-09-05T21:15:20.089643: step 9517, loss 0.00303779, acc 1
2016-09-05T21:15:20.921331: step 9518, loss 0.0180497, acc 0.98
2016-09-05T21:15:21.717711: step 9519, loss 0.052418, acc 0.98
2016-09-05T21:15:22.517220: step 9520, loss 0.00808171, acc 1
2016-09-05T21:15:23.313849: step 9521, loss 0.00598493, acc 1
2016-09-05T21:15:24.106721: step 9522, loss 0.00341875, acc 1
2016-09-05T21:15:24.911232: step 9523, loss 0.00315317, acc 1
2016-09-05T21:15:25.712755: step 9524, loss 0.00302838, acc 1
2016-09-05T21:15:26.520868: step 9525, loss 0.00951913, acc 1
2016-09-05T21:15:27.341353: step 9526, loss 0.016487, acc 1
2016-09-05T21:15:28.169708: step 9527, loss 0.00919493, acc 1
2016-09-05T21:15:28.967714: step 9528, loss 0.00832223, acc 1
2016-09-05T21:15:29.772870: step 9529, loss 0.013158, acc 1
2016-09-05T21:15:30.613404: step 9530, loss 0.00310988, acc 1
2016-09-05T21:15:31.385230: step 9531, loss 0.00774055, acc 1
2016-09-05T21:15:32.217931: step 9532, loss 0.244516, acc 0.98
2016-09-05T21:15:33.055174: step 9533, loss 0.0134924, acc 1
2016-09-05T21:15:33.856814: step 9534, loss 0.0203076, acc 0.98
2016-09-05T21:15:34.633483: step 9535, loss 0.027366, acc 0.98
2016-09-05T21:15:35.449420: step 9536, loss 0.0132569, acc 1
2016-09-05T21:15:36.232001: step 9537, loss 0.00992679, acc 1
2016-09-05T21:15:37.072999: step 9538, loss 0.41418, acc 0.96
2016-09-05T21:15:37.899038: step 9539, loss 0.0559457, acc 0.98
2016-09-05T21:15:38.693683: step 9540, loss 0.0303424, acc 0.98
2016-09-05T21:15:39.495485: step 9541, loss 0.00411102, acc 1
2016-09-05T21:15:40.296612: step 9542, loss 0.0207388, acc 0.98
2016-09-05T21:15:41.105914: step 9543, loss 0.0161665, acc 0.98
2016-09-05T21:15:41.898797: step 9544, loss 0.0444421, acc 0.96
2016-09-05T21:15:42.717984: step 9545, loss 0.00526518, acc 1
2016-09-05T21:15:43.507078: step 9546, loss 0.0527726, acc 0.98
2016-09-05T21:15:44.296683: step 9547, loss 0.0392765, acc 0.96
2016-09-05T21:15:45.117964: step 9548, loss 0.00480233, acc 1
2016-09-05T21:15:45.901496: step 9549, loss 0.00403926, acc 1
2016-09-05T21:15:46.725226: step 9550, loss 0.0226026, acc 0.98
2016-09-05T21:15:47.521406: step 9551, loss 0.0241503, acc 0.98
2016-09-05T21:15:48.305162: step 9552, loss 0.0148312, acc 1
2016-09-05T21:15:49.110765: step 9553, loss 0.00906502, acc 1
2016-09-05T21:15:49.930825: step 9554, loss 0.0549987, acc 0.96
2016-09-05T21:15:50.705332: step 9555, loss 0.00359424, acc 1
2016-09-05T21:15:51.510979: step 9556, loss 0.0200647, acc 1
2016-09-05T21:15:52.348291: step 9557, loss 0.0739574, acc 0.98
2016-09-05T21:15:53.151050: step 9558, loss 0.0289293, acc 0.98
2016-09-05T21:15:53.987349: step 9559, loss 0.00384249, acc 1
2016-09-05T21:15:54.806784: step 9560, loss 0.00370863, acc 1
2016-09-05T21:15:55.575190: step 9561, loss 0.0767134, acc 0.98
2016-09-05T21:15:56.358218: step 9562, loss 0.0201412, acc 1
2016-09-05T21:15:57.191839: step 9563, loss 0.0212151, acc 1
2016-09-05T21:15:57.954971: step 9564, loss 0.00657356, acc 1
2016-09-05T21:15:58.750960: step 9565, loss 0.0394515, acc 0.96
2016-09-05T21:15:59.563282: step 9566, loss 0.0318946, acc 0.98
2016-09-05T21:16:00.371666: step 9567, loss 0.00979313, acc 1
2016-09-05T21:16:01.174263: step 9568, loss 0.00691213, acc 1
2016-09-05T21:16:01.983881: step 9569, loss 0.00866944, acc 1
2016-09-05T21:16:02.799920: step 9570, loss 0.0374694, acc 0.98
2016-09-05T21:16:03.606779: step 9571, loss 0.0186852, acc 0.98
2016-09-05T21:16:04.449370: step 9572, loss 0.00818464, acc 1
2016-09-05T21:16:05.240366: step 9573, loss 0.0202209, acc 1
2016-09-05T21:16:06.073000: step 9574, loss 0.0127292, acc 1
2016-09-05T21:16:06.881251: step 9575, loss 0.0160813, acc 1
2016-09-05T21:16:07.677392: step 9576, loss 0.00911493, acc 1
2016-09-05T21:16:08.469673: step 9577, loss 0.00541365, acc 1
2016-09-05T21:16:09.298058: step 9578, loss 0.0112882, acc 1
2016-09-05T21:16:10.102109: step 9579, loss 0.00411594, acc 1
2016-09-05T21:16:10.903360: step 9580, loss 0.0167895, acc 1
2016-09-05T21:16:11.721369: step 9581, loss 0.00677445, acc 1
2016-09-05T21:16:12.497409: step 9582, loss 0.0314064, acc 0.98
2016-09-05T21:16:13.298125: step 9583, loss 0.0634175, acc 0.98
2016-09-05T21:16:14.116762: step 9584, loss 0.00657337, acc 1
2016-09-05T21:16:14.913583: step 9585, loss 0.00996654, acc 1
2016-09-05T21:16:15.706595: step 9586, loss 0.0279444, acc 0.98
2016-09-05T21:16:16.512916: step 9587, loss 0.0168277, acc 1
2016-09-05T21:16:17.298878: step 9588, loss 0.00665545, acc 1
2016-09-05T21:16:18.106937: step 9589, loss 0.0580321, acc 0.96
2016-09-05T21:16:18.931088: step 9590, loss 0.0102905, acc 1
2016-09-05T21:16:19.741416: step 9591, loss 0.00812405, acc 1
2016-09-05T21:16:20.544086: step 9592, loss 0.00970243, acc 1
2016-09-05T21:16:21.366396: step 9593, loss 0.00396426, acc 1
2016-09-05T21:16:22.131454: step 9594, loss 0.119036, acc 0.96
2016-09-05T21:16:22.945406: step 9595, loss 0.0106286, acc 1
2016-09-05T21:16:23.765054: step 9596, loss 0.0109033, acc 1
2016-09-05T21:16:24.561778: step 9597, loss 0.0104219, acc 1
2016-09-05T21:16:25.358390: step 9598, loss 0.0106809, acc 1
2016-09-05T21:16:26.189447: step 9599, loss 0.0659411, acc 0.96
2016-09-05T21:16:26.977110: step 9600, loss 0.00795765, acc 1

Evaluation:
2016-09-05T21:16:30.465362: step 9600, loss 2.50937, acc 0.73

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-9600

2016-09-05T21:16:32.510679: step 9601, loss 0.0157265, acc 1
2016-09-05T21:16:33.347717: step 9602, loss 0.0206727, acc 0.98
2016-09-05T21:16:34.179086: step 9603, loss 0.0176141, acc 1
2016-09-05T21:16:34.983853: step 9604, loss 0.0722985, acc 0.94
2016-09-05T21:16:35.805007: step 9605, loss 0.0064536, acc 1
2016-09-05T21:16:36.627243: step 9606, loss 0.0600645, acc 0.98
2016-09-05T21:16:37.464176: step 9607, loss 0.0274809, acc 0.98
2016-09-05T21:16:38.290447: step 9608, loss 0.00502686, acc 1
2016-09-05T21:16:39.090615: step 9609, loss 0.00434987, acc 1
2016-09-05T21:16:39.928750: step 9610, loss 0.0218065, acc 1
2016-09-05T21:16:40.731479: step 9611, loss 0.0137976, acc 1
2016-09-05T21:16:41.522457: step 9612, loss 0.0174796, acc 1
2016-09-05T21:16:42.355040: step 9613, loss 0.0142729, acc 1
2016-09-05T21:16:43.161400: step 9614, loss 0.00496061, acc 1
2016-09-05T21:16:43.989418: step 9615, loss 0.0184168, acc 1
2016-09-05T21:16:44.794207: step 9616, loss 0.00387275, acc 1
2016-09-05T21:16:45.626772: step 9617, loss 0.0296258, acc 0.98
2016-09-05T21:16:46.415858: step 9618, loss 0.00603664, acc 1
2016-09-05T21:16:47.229451: step 9619, loss 0.0614132, acc 0.98
2016-09-05T21:16:48.099274: step 9620, loss 0.0924502, acc 0.96
2016-09-05T21:16:48.889282: step 9621, loss 0.0125971, acc 1
2016-09-05T21:16:49.716999: step 9622, loss 0.0703295, acc 0.96
2016-09-05T21:16:50.581436: step 9623, loss 0.00741711, acc 1
2016-09-05T21:16:51.405436: step 9624, loss 0.0475167, acc 0.98
2016-09-05T21:16:52.209120: step 9625, loss 0.00440484, acc 1
2016-09-05T21:16:53.030486: step 9626, loss 0.053733, acc 0.98
2016-09-05T21:16:53.844462: step 9627, loss 0.0562082, acc 0.98
2016-09-05T21:16:54.664824: step 9628, loss 0.0304669, acc 0.98
2016-09-05T21:16:55.485252: step 9629, loss 0.00582423, acc 1
2016-09-05T21:16:56.289915: step 9630, loss 0.0626733, acc 0.98
2016-09-05T21:16:57.082685: step 9631, loss 0.0454203, acc 0.98
2016-09-05T21:16:57.896215: step 9632, loss 0.0174823, acc 1
2016-09-05T21:16:58.714958: step 9633, loss 0.0102633, acc 1
2016-09-05T21:16:59.527561: step 9634, loss 0.00675921, acc 1
2016-09-05T21:17:00.422508: step 9635, loss 0.0137321, acc 1
2016-09-05T21:17:01.232100: step 9636, loss 0.100531, acc 0.96
2016-09-05T21:17:02.021975: step 9637, loss 0.0209038, acc 1
2016-09-05T21:17:02.847779: step 9638, loss 0.00899152, acc 1
2016-09-05T21:17:03.667012: step 9639, loss 0.00399767, acc 1
2016-09-05T21:17:04.464663: step 9640, loss 0.00969881, acc 1
2016-09-05T21:17:05.301784: step 9641, loss 0.0384953, acc 0.98
2016-09-05T21:17:06.097084: step 9642, loss 0.0180218, acc 1
2016-09-05T21:17:06.921447: step 9643, loss 0.0193417, acc 0.98
2016-09-05T21:17:07.747141: step 9644, loss 0.00523116, acc 1
2016-09-05T21:17:08.543183: step 9645, loss 0.00538946, acc 1
2016-09-05T21:17:09.347220: step 9646, loss 0.0230183, acc 0.98
2016-09-05T21:17:10.177407: step 9647, loss 0.0181885, acc 1
2016-09-05T21:17:10.975474: step 9648, loss 0.0160249, acc 1
2016-09-05T21:17:11.766392: step 9649, loss 0.024829, acc 0.98
2016-09-05T21:17:12.590927: step 9650, loss 0.00539551, acc 1
2016-09-05T21:17:13.401416: step 9651, loss 0.0278344, acc 1
2016-09-05T21:17:14.209422: step 9652, loss 0.0239317, acc 0.98
2016-09-05T21:17:15.038675: step 9653, loss 0.00414436, acc 1
2016-09-05T21:17:15.855829: step 9654, loss 0.0137194, acc 1
2016-09-05T21:17:16.653733: step 9655, loss 0.0180048, acc 1
2016-09-05T21:17:17.462876: step 9656, loss 0.00572167, acc 1
2016-09-05T21:17:18.288152: step 9657, loss 0.0163615, acc 1
2016-09-05T21:17:19.088724: step 9658, loss 0.0282122, acc 1
2016-09-05T21:17:19.891600: step 9659, loss 0.0369326, acc 0.98
2016-09-05T21:17:20.717405: step 9660, loss 0.0199046, acc 1
2016-09-05T21:17:21.497890: step 9661, loss 0.024898, acc 0.98
2016-09-05T21:17:22.337979: step 9662, loss 0.00412603, acc 1
2016-09-05T21:17:23.135954: step 9663, loss 0.00474479, acc 1
2016-09-05T21:17:23.947973: step 9664, loss 0.00457018, acc 1
2016-09-05T21:17:24.719923: step 9665, loss 0.00395581, acc 1
2016-09-05T21:17:25.571111: step 9666, loss 0.00392528, acc 1
2016-09-05T21:17:26.365211: step 9667, loss 0.00779933, acc 1
2016-09-05T21:17:27.156883: step 9668, loss 0.0163369, acc 1
2016-09-05T21:17:27.968922: step 9669, loss 0.0124656, acc 1
2016-09-05T21:17:28.739652: step 9670, loss 0.00441916, acc 1
2016-09-05T21:17:29.538881: step 9671, loss 0.0379394, acc 0.98
2016-09-05T21:17:30.365465: step 9672, loss 0.0237757, acc 0.98
2016-09-05T21:17:31.143854: step 9673, loss 0.00381735, acc 1
2016-09-05T21:17:31.941426: step 9674, loss 0.00387398, acc 1
2016-09-05T21:17:32.747177: step 9675, loss 0.0413302, acc 0.98
2016-09-05T21:17:33.514680: step 9676, loss 0.0475322, acc 0.96
2016-09-05T21:17:34.345556: step 9677, loss 0.010975, acc 1
2016-09-05T21:17:35.189435: step 9678, loss 0.00391556, acc 1
2016-09-05T21:17:35.974521: step 9679, loss 0.0304347, acc 0.98
2016-09-05T21:17:36.815646: step 9680, loss 0.0182563, acc 0.98
2016-09-05T21:17:37.633848: step 9681, loss 0.0116312, acc 1
2016-09-05T21:17:38.425025: step 9682, loss 0.0687225, acc 0.96
2016-09-05T21:17:39.189823: step 9683, loss 0.00366175, acc 1
2016-09-05T21:17:39.999676: step 9684, loss 0.0166493, acc 1
2016-09-05T21:17:40.800113: step 9685, loss 0.00686115, acc 1
2016-09-05T21:17:41.612544: step 9686, loss 0.0198996, acc 1
2016-09-05T21:17:42.437250: step 9687, loss 0.00361188, acc 1
2016-09-05T21:17:43.262468: step 9688, loss 0.0178795, acc 0.98
2016-09-05T21:17:44.067283: step 9689, loss 0.017169, acc 1
2016-09-05T21:17:44.885696: step 9690, loss 0.00989578, acc 1
2016-09-05T21:17:45.671641: step 9691, loss 0.110109, acc 0.96
2016-09-05T21:17:46.483083: step 9692, loss 0.0225332, acc 1
2016-09-05T21:17:47.310734: step 9693, loss 0.0627351, acc 0.96
2016-09-05T21:17:48.087619: step 9694, loss 0.00310603, acc 1
2016-09-05T21:17:48.875855: step 9695, loss 0.0160739, acc 1
2016-09-05T21:17:49.690786: step 9696, loss 0.021216, acc 1
2016-09-05T21:17:50.456571: step 9697, loss 0.0364517, acc 0.96
2016-09-05T21:17:51.268373: step 9698, loss 0.00496515, acc 1
2016-09-05T21:17:52.087560: step 9699, loss 0.00307966, acc 1
2016-09-05T21:17:52.487207: step 9700, loss 0.0571545, acc 1

Evaluation:
2016-09-05T21:17:56.000027: step 9700, loss 2.4934, acc 0.744

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-9700

2016-09-05T21:17:57.918380: step 9701, loss 0.00408556, acc 1
2016-09-05T21:17:58.712805: step 9702, loss 0.00746863, acc 1
2016-09-05T21:17:59.539080: step 9703, loss 0.00622704, acc 1
2016-09-05T21:18:00.351087: step 9704, loss 0.00894618, acc 1
2016-09-05T21:18:01.175857: step 9705, loss 0.0206539, acc 0.98
2016-09-05T21:18:02.033090: step 9706, loss 0.024944, acc 0.98
2016-09-05T21:18:02.845564: step 9707, loss 0.00451936, acc 1
2016-09-05T21:18:03.656133: step 9708, loss 0.0303148, acc 0.98
2016-09-05T21:18:04.477694: step 9709, loss 0.0162195, acc 1
2016-09-05T21:18:05.294704: step 9710, loss 0.00283225, acc 1
2016-09-05T21:18:06.105402: step 9711, loss 0.0137328, acc 1
2016-09-05T21:18:06.959486: step 9712, loss 0.0325749, acc 0.98
2016-09-05T21:18:07.769396: step 9713, loss 0.0353366, acc 1
2016-09-05T21:18:08.589309: step 9714, loss 0.00329521, acc 1
2016-09-05T21:18:09.400918: step 9715, loss 0.0188574, acc 0.98
2016-09-05T21:18:10.216295: step 9716, loss 0.0135036, acc 1
2016-09-05T21:18:11.015935: step 9717, loss 0.0686645, acc 0.98
2016-09-05T21:18:11.820728: step 9718, loss 0.0298286, acc 0.98
2016-09-05T21:18:12.628378: step 9719, loss 0.0059839, acc 1
2016-09-05T21:18:13.470749: step 9720, loss 0.0643143, acc 0.98
2016-09-05T21:18:14.261515: step 9721, loss 0.0117598, acc 1
2016-09-05T21:18:15.102953: step 9722, loss 0.00813452, acc 1
2016-09-05T21:18:15.898864: step 9723, loss 0.0027348, acc 1
2016-09-05T21:18:16.709707: step 9724, loss 0.00260181, acc 1
2016-09-05T21:18:17.534357: step 9725, loss 0.00468896, acc 1
2016-09-05T21:18:18.351936: step 9726, loss 0.00792513, acc 1
2016-09-05T21:18:19.152357: step 9727, loss 0.00241018, acc 1
2016-09-05T21:18:19.958341: step 9728, loss 0.00538479, acc 1
2016-09-05T21:18:20.755151: step 9729, loss 0.032096, acc 0.98
2016-09-05T21:18:21.570301: step 9730, loss 0.00304772, acc 1
2016-09-05T21:18:22.393749: step 9731, loss 0.00262665, acc 1
2016-09-05T21:18:23.195842: step 9732, loss 0.0171264, acc 1
2016-09-05T21:18:24.005642: step 9733, loss 0.0144055, acc 1
2016-09-05T21:18:24.855673: step 9734, loss 0.00633084, acc 1
2016-09-05T21:18:25.682598: step 9735, loss 0.00644814, acc 1
2016-09-05T21:18:26.478635: step 9736, loss 0.00334061, acc 1
2016-09-05T21:18:27.311644: step 9737, loss 0.0156819, acc 1
2016-09-05T21:18:28.112336: step 9738, loss 0.0176712, acc 0.98
2016-09-05T21:18:28.915720: step 9739, loss 0.0022567, acc 1
2016-09-05T21:18:29.732861: step 9740, loss 0.0030791, acc 1
2016-09-05T21:18:30.532327: step 9741, loss 0.0297393, acc 0.98
2016-09-05T21:18:31.366337: step 9742, loss 0.0181317, acc 1
2016-09-05T21:18:32.205476: step 9743, loss 0.00755274, acc 1
2016-09-05T21:18:33.035024: step 9744, loss 0.0022455, acc 1
2016-09-05T21:18:33.840178: step 9745, loss 0.00295578, acc 1
2016-09-05T21:18:34.659364: step 9746, loss 0.0130301, acc 1
2016-09-05T21:18:35.448060: step 9747, loss 0.0672191, acc 0.96
2016-09-05T21:18:36.257005: step 9748, loss 0.0193639, acc 1
2016-09-05T21:18:37.075294: step 9749, loss 0.00557781, acc 1
2016-09-05T21:18:37.881300: step 9750, loss 0.0296787, acc 0.98
2016-09-05T21:18:38.695466: step 9751, loss 0.00333989, acc 1
2016-09-05T21:18:39.511449: step 9752, loss 0.0234601, acc 0.98
2016-09-05T21:18:40.319230: step 9753, loss 0.00249532, acc 1
2016-09-05T21:18:41.123977: step 9754, loss 0.00454796, acc 1
2016-09-05T21:18:41.956425: step 9755, loss 0.00507171, acc 1
2016-09-05T21:18:42.754949: step 9756, loss 0.0385148, acc 0.98
2016-09-05T21:18:43.564394: step 9757, loss 0.0257864, acc 1
2016-09-05T21:18:44.378348: step 9758, loss 0.0066073, acc 1
2016-09-05T21:18:45.183351: step 9759, loss 0.0118821, acc 1
2016-09-05T21:18:45.996842: step 9760, loss 0.00990023, acc 1
2016-09-05T21:18:46.835030: step 9761, loss 0.00557502, acc 1
2016-09-05T21:18:47.615661: step 9762, loss 0.0112057, acc 1
2016-09-05T21:18:48.421993: step 9763, loss 0.0501826, acc 0.98
2016-09-05T21:18:49.266636: step 9764, loss 0.0452173, acc 0.98
2016-09-05T21:18:50.082582: step 9765, loss 0.0108978, acc 1
2016-09-05T21:18:50.903640: step 9766, loss 0.00232178, acc 1
2016-09-05T21:18:51.697449: step 9767, loss 0.0386049, acc 0.98
2016-09-05T21:18:52.517698: step 9768, loss 0.00553886, acc 1
2016-09-05T21:18:53.301435: step 9769, loss 0.00237666, acc 1
2016-09-05T21:18:54.131574: step 9770, loss 0.0303005, acc 0.98
2016-09-05T21:18:54.970165: step 9771, loss 0.00274913, acc 1
2016-09-05T21:18:55.755121: step 9772, loss 0.0149768, acc 1
2016-09-05T21:18:56.536156: step 9773, loss 0.00208435, acc 1
2016-09-05T21:18:57.348768: step 9774, loss 0.0610269, acc 0.98
2016-09-05T21:18:58.159618: step 9775, loss 0.0139555, acc 1
2016-09-05T21:18:58.996895: step 9776, loss 0.0617502, acc 0.96
2016-09-05T21:18:59.802599: step 9777, loss 0.0271378, acc 1
2016-09-05T21:19:00.604654: step 9778, loss 0.00279681, acc 1
2016-09-05T21:19:01.408862: step 9779, loss 0.00252684, acc 1
2016-09-05T21:19:02.267047: step 9780, loss 0.0196122, acc 1
2016-09-05T21:19:03.052475: step 9781, loss 0.0234156, acc 1
2016-09-05T21:19:03.862326: step 9782, loss 0.00194195, acc 1
2016-09-05T21:19:04.682738: step 9783, loss 0.0188571, acc 1
2016-09-05T21:19:05.479442: step 9784, loss 0.00228639, acc 1
2016-09-05T21:19:06.286125: step 9785, loss 0.00236362, acc 1
2016-09-05T21:19:07.101504: step 9786, loss 0.165922, acc 0.96
2016-09-05T21:19:07.865880: step 9787, loss 0.00184298, acc 1
2016-09-05T21:19:08.655422: step 9788, loss 0.0267297, acc 0.98
2016-09-05T21:19:09.499361: step 9789, loss 0.0181734, acc 0.98
2016-09-05T21:19:10.303130: step 9790, loss 0.0304448, acc 0.98
2016-09-05T21:19:11.079827: step 9791, loss 0.0120505, acc 1
2016-09-05T21:19:11.899664: step 9792, loss 0.0349133, acc 0.96
2016-09-05T21:19:12.681087: step 9793, loss 0.0032702, acc 1
2016-09-05T21:19:13.478002: step 9794, loss 0.0171249, acc 0.98
2016-09-05T21:19:14.284034: step 9795, loss 0.0111474, acc 1
2016-09-05T21:19:15.060479: step 9796, loss 0.005034, acc 1
2016-09-05T21:19:15.889297: step 9797, loss 0.00741792, acc 1
2016-09-05T21:19:16.706929: step 9798, loss 0.0896748, acc 0.96
2016-09-05T21:19:17.492435: step 9799, loss 0.0134015, acc 1
2016-09-05T21:19:18.300244: step 9800, loss 0.0237654, acc 1

Evaluation:
2016-09-05T21:19:21.811106: step 9800, loss 1.82108, acc 0.749

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-9800

2016-09-05T21:19:23.673952: step 9801, loss 0.0146933, acc 1
2016-09-05T21:19:24.480668: step 9802, loss 0.0150691, acc 1
2016-09-05T21:19:25.322943: step 9803, loss 0.00879073, acc 1
2016-09-05T21:19:26.115212: step 9804, loss 0.0411133, acc 0.98
2016-09-05T21:19:26.952677: step 9805, loss 0.0130095, acc 1
2016-09-05T21:19:27.775432: step 9806, loss 0.00252318, acc 1
2016-09-05T21:19:28.587984: step 9807, loss 0.0300727, acc 1
2016-09-05T21:19:29.428337: step 9808, loss 0.0202712, acc 1
2016-09-05T21:19:30.278712: step 9809, loss 0.0222268, acc 1
2016-09-05T21:19:31.094860: step 9810, loss 0.0333932, acc 0.98
2016-09-05T21:19:31.901591: step 9811, loss 0.0194375, acc 0.98
2016-09-05T21:19:32.736352: step 9812, loss 0.00211148, acc 1
2016-09-05T21:19:33.522227: step 9813, loss 0.0359944, acc 1
2016-09-05T21:19:34.358977: step 9814, loss 0.00260669, acc 1
2016-09-05T21:19:35.199797: step 9815, loss 0.00207771, acc 1
2016-09-05T21:19:35.990226: step 9816, loss 0.0170605, acc 0.98
2016-09-05T21:19:36.776772: step 9817, loss 0.0184587, acc 0.98
2016-09-05T21:19:37.636341: step 9818, loss 0.0316972, acc 0.98
2016-09-05T21:19:38.455314: step 9819, loss 0.00666704, acc 1
2016-09-05T21:19:39.305909: step 9820, loss 0.00339601, acc 1
2016-09-05T21:19:40.111270: step 9821, loss 0.00247245, acc 1
2016-09-05T21:19:40.924814: step 9822, loss 0.0281112, acc 0.98
2016-09-05T21:19:41.730231: step 9823, loss 0.00223603, acc 1
2016-09-05T21:19:42.553594: step 9824, loss 0.0156076, acc 1
2016-09-05T21:19:43.360010: step 9825, loss 0.0105431, acc 1
2016-09-05T21:19:44.139747: step 9826, loss 0.0163304, acc 1
2016-09-05T21:19:44.947702: step 9827, loss 0.0378337, acc 0.98
2016-09-05T21:19:45.762759: step 9828, loss 0.00322021, acc 1
2016-09-05T21:19:46.560857: step 9829, loss 0.00827616, acc 1
2016-09-05T21:19:47.369700: step 9830, loss 0.0394804, acc 0.98
2016-09-05T21:19:48.171017: step 9831, loss 0.0172663, acc 0.98
2016-09-05T21:19:48.961961: step 9832, loss 0.00761459, acc 1
2016-09-05T21:19:49.790969: step 9833, loss 0.00222138, acc 1
2016-09-05T21:19:50.604924: step 9834, loss 0.0269536, acc 0.98
2016-09-05T21:19:51.372171: step 9835, loss 0.0245202, acc 0.98
2016-09-05T21:19:52.175334: step 9836, loss 0.0187568, acc 0.98
2016-09-05T21:19:53.018351: step 9837, loss 0.0105873, acc 1
2016-09-05T21:19:53.806886: step 9838, loss 0.00221942, acc 1
2016-09-05T21:19:54.603852: step 9839, loss 0.00228175, acc 1
2016-09-05T21:19:55.432674: step 9840, loss 0.00422824, acc 1
2016-09-05T21:19:56.207054: step 9841, loss 0.0437722, acc 0.96
2016-09-05T21:19:57.014159: step 9842, loss 0.0183006, acc 0.98
2016-09-05T21:19:57.842955: step 9843, loss 0.002753, acc 1
2016-09-05T21:19:58.618585: step 9844, loss 0.0026596, acc 1
2016-09-05T21:19:59.435433: step 9845, loss 0.0206754, acc 1
2016-09-05T21:20:00.272095: step 9846, loss 0.00463365, acc 1
2016-09-05T21:20:01.039389: step 9847, loss 0.0169719, acc 0.98
2016-09-05T21:20:01.840891: step 9848, loss 0.0269227, acc 1
2016-09-05T21:20:02.654730: step 9849, loss 0.048825, acc 0.98
2016-09-05T21:20:03.430675: step 9850, loss 0.0269902, acc 0.98
2016-09-05T21:20:04.232650: step 9851, loss 0.0571284, acc 0.98
2016-09-05T21:20:05.041036: step 9852, loss 0.125774, acc 0.96
2016-09-05T21:20:05.814877: step 9853, loss 0.0240708, acc 1
2016-09-05T21:20:06.645371: step 9854, loss 0.0138625, acc 1
2016-09-05T21:20:07.460637: step 9855, loss 0.014382, acc 1
2016-09-05T21:20:08.260508: step 9856, loss 0.0137188, acc 1
2016-09-05T21:20:09.060144: step 9857, loss 0.00653931, acc 1
2016-09-05T21:20:09.840039: step 9858, loss 0.0124141, acc 1
2016-09-05T21:20:10.635572: step 9859, loss 0.010012, acc 1
2016-09-05T21:20:11.461329: step 9860, loss 0.0231081, acc 0.98
2016-09-05T21:20:12.270932: step 9861, loss 0.00242053, acc 1
2016-09-05T21:20:13.057326: step 9862, loss 0.00568351, acc 1
2016-09-05T21:20:13.864272: step 9863, loss 0.00669959, acc 1
2016-09-05T21:20:14.671520: step 9864, loss 0.00462555, acc 1
2016-09-05T21:20:15.468415: step 9865, loss 0.00240684, acc 1
2016-09-05T21:20:16.309776: step 9866, loss 0.039065, acc 0.98
2016-09-05T21:20:17.106244: step 9867, loss 0.00489755, acc 1
2016-09-05T21:20:17.915798: step 9868, loss 0.0136552, acc 1
2016-09-05T21:20:18.733541: step 9869, loss 0.00974965, acc 1
2016-09-05T21:20:19.553277: step 9870, loss 0.0184476, acc 0.98
2016-09-05T21:20:20.330174: step 9871, loss 0.121065, acc 0.98
2016-09-05T21:20:21.148312: step 9872, loss 0.0298651, acc 1
2016-09-05T21:20:22.018581: step 9873, loss 0.00401364, acc 1
2016-09-05T21:20:22.813310: step 9874, loss 0.00545044, acc 1
2016-09-05T21:20:23.621116: step 9875, loss 0.0383297, acc 0.96
2016-09-05T21:20:24.452309: step 9876, loss 0.019348, acc 1
2016-09-05T21:20:25.243326: step 9877, loss 0.00536706, acc 1
2016-09-05T21:20:26.046107: step 9878, loss 0.00280052, acc 1
2016-09-05T21:20:26.834614: step 9879, loss 0.00600297, acc 1
2016-09-05T21:20:27.655138: step 9880, loss 0.0300093, acc 0.98
2016-09-05T21:20:28.449854: step 9881, loss 0.0300928, acc 0.98
2016-09-05T21:20:29.238568: step 9882, loss 0.00914006, acc 1
2016-09-05T21:20:30.036959: step 9883, loss 0.0217108, acc 1
2016-09-05T21:20:30.820160: step 9884, loss 0.0162728, acc 1
2016-09-05T21:20:31.645247: step 9885, loss 0.173664, acc 0.98
2016-09-05T21:20:32.449211: step 9886, loss 0.00317877, acc 1
2016-09-05T21:20:33.255967: step 9887, loss 0.00996404, acc 1
2016-09-05T21:20:34.081444: step 9888, loss 0.00876692, acc 1
2016-09-05T21:20:34.889437: step 9889, loss 0.00310134, acc 1
2016-09-05T21:20:35.697517: step 9890, loss 0.0218577, acc 1
2016-09-05T21:20:36.498430: step 9891, loss 0.00311672, acc 1
2016-09-05T21:20:37.257448: step 9892, loss 0.0222353, acc 0.98
2016-09-05T21:20:38.078924: step 9893, loss 0.032055, acc 0.98
2016-09-05T21:20:38.505816: step 9894, loss 0.00767636, acc 1
2016-09-05T21:20:39.308326: step 9895, loss 0.0473745, acc 0.98
2016-09-05T21:20:40.140893: step 9896, loss 0.00506922, acc 1
2016-09-05T21:20:40.946409: step 9897, loss 0.0307056, acc 0.98
2016-09-05T21:20:41.767448: step 9898, loss 0.0051778, acc 1
2016-09-05T21:20:42.589155: step 9899, loss 0.00408873, acc 1
2016-09-05T21:20:43.374447: step 9900, loss 0.0877865, acc 0.96

Evaluation:
2016-09-05T21:20:46.881110: step 9900, loss 1.98801, acc 0.738

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-9900

2016-09-05T21:20:48.799987: step 9901, loss 0.0174294, acc 1
2016-09-05T21:20:49.604965: step 9902, loss 0.00781574, acc 1
2016-09-05T21:20:50.431353: step 9903, loss 0.00666118, acc 1
2016-09-05T21:20:51.249454: step 9904, loss 0.0262783, acc 0.98
2016-09-05T21:20:52.029414: step 9905, loss 0.0187392, acc 0.98
2016-09-05T21:20:52.840656: step 9906, loss 0.0146787, acc 1
2016-09-05T21:20:53.654761: step 9907, loss 0.0256396, acc 0.98
2016-09-05T21:20:54.463632: step 9908, loss 0.00680378, acc 1
2016-09-05T21:20:55.252587: step 9909, loss 0.00241, acc 1
2016-09-05T21:20:56.083956: step 9910, loss 0.00562883, acc 1
2016-09-05T21:20:56.891187: step 9911, loss 0.00504305, acc 1
2016-09-05T21:20:57.666068: step 9912, loss 0.00245328, acc 1
2016-09-05T21:20:58.489848: step 9913, loss 0.0108388, acc 1
2016-09-05T21:20:59.285402: step 9914, loss 0.00365651, acc 1
2016-09-05T21:21:00.069601: step 9915, loss 0.0166972, acc 1
2016-09-05T21:21:00.948541: step 9916, loss 0.128681, acc 0.98
2016-09-05T21:21:01.765452: step 9917, loss 0.00252126, acc 1
2016-09-05T21:21:02.575217: step 9918, loss 0.0121224, acc 1
2016-09-05T21:21:03.394929: step 9919, loss 0.00256976, acc 1
2016-09-05T21:21:04.193969: step 9920, loss 0.00316931, acc 1
2016-09-05T21:21:04.992631: step 9921, loss 0.00316448, acc 1
2016-09-05T21:21:05.844017: step 9922, loss 0.0197117, acc 0.98
2016-09-05T21:21:06.625627: step 9923, loss 0.00433039, acc 1
2016-09-05T21:21:07.440254: step 9924, loss 0.0702943, acc 0.98
2016-09-05T21:21:08.279033: step 9925, loss 0.00832711, acc 1
2016-09-05T21:21:09.101417: step 9926, loss 0.0398482, acc 0.98
2016-09-05T21:21:09.893145: step 9927, loss 0.0111822, acc 1
2016-09-05T21:21:10.694776: step 9928, loss 0.0206619, acc 0.98
2016-09-05T21:21:11.501441: step 9929, loss 0.00394842, acc 1
2016-09-05T21:21:12.292807: step 9930, loss 0.0205088, acc 1
2016-09-05T21:21:13.116427: step 9931, loss 0.0118782, acc 1
2016-09-05T21:21:13.923467: step 9932, loss 0.0139521, acc 1
2016-09-05T21:21:14.728108: step 9933, loss 0.00268256, acc 1
2016-09-05T21:21:15.541805: step 9934, loss 0.00274038, acc 1
2016-09-05T21:21:16.362250: step 9935, loss 0.030972, acc 0.98
2016-09-05T21:21:17.152938: step 9936, loss 0.0257346, acc 0.98
2016-09-05T21:21:17.938982: step 9937, loss 0.00254175, acc 1
2016-09-05T21:21:18.732783: step 9938, loss 0.0409297, acc 0.98
2016-09-05T21:21:19.545447: step 9939, loss 0.00490819, acc 1
2016-09-05T21:21:20.374007: step 9940, loss 0.0562321, acc 0.98
2016-09-05T21:21:21.219086: step 9941, loss 0.0238149, acc 0.98
2016-09-05T21:21:22.003391: step 9942, loss 0.0356002, acc 0.98
2016-09-05T21:21:22.826909: step 9943, loss 0.00259619, acc 1
2016-09-05T21:21:23.660293: step 9944, loss 0.0928442, acc 0.96
2016-09-05T21:21:24.438542: step 9945, loss 0.0146841, acc 1
2016-09-05T21:21:25.261719: step 9946, loss 0.00565833, acc 1
2016-09-05T21:21:26.099415: step 9947, loss 0.0294569, acc 0.98
2016-09-05T21:21:26.871776: step 9948, loss 0.00778745, acc 1
2016-09-05T21:21:27.690831: step 9949, loss 0.0023552, acc 1
2016-09-05T21:21:28.508900: step 9950, loss 0.0309688, acc 1
2016-09-05T21:21:29.286423: step 9951, loss 0.0175653, acc 1
2016-09-05T21:21:30.079224: step 9952, loss 0.0080656, acc 1
2016-09-05T21:21:30.916688: step 9953, loss 0.0055713, acc 1
2016-09-05T21:21:31.675414: step 9954, loss 0.00326379, acc 1
2016-09-05T21:21:32.484920: step 9955, loss 0.0943025, acc 0.98
2016-09-05T21:21:33.314522: step 9956, loss 0.0487764, acc 0.98
2016-09-05T21:21:34.109490: step 9957, loss 0.0378086, acc 0.98
2016-09-05T21:21:34.897911: step 9958, loss 0.0196061, acc 0.98
2016-09-05T21:21:35.720460: step 9959, loss 0.00502649, acc 1
2016-09-05T21:21:36.526774: step 9960, loss 0.0681766, acc 0.98
2016-09-05T21:21:37.357194: step 9961, loss 0.0206879, acc 0.98
2016-09-05T21:21:38.168249: step 9962, loss 0.00634482, acc 1
2016-09-05T21:21:38.970151: step 9963, loss 0.00285364, acc 1
2016-09-05T21:21:39.781275: step 9964, loss 0.0324634, acc 0.98
2016-09-05T21:21:40.567224: step 9965, loss 0.0237955, acc 0.98
2016-09-05T21:21:41.351283: step 9966, loss 0.0845678, acc 0.94
2016-09-05T21:21:42.181362: step 9967, loss 0.0350032, acc 0.98
2016-09-05T21:21:43.010612: step 9968, loss 0.00213902, acc 1
2016-09-05T21:21:43.801601: step 9969, loss 0.0248954, acc 0.98
2016-09-05T21:21:44.608419: step 9970, loss 0.0385639, acc 0.96
2016-09-05T21:21:45.424574: step 9971, loss 0.00436136, acc 1
2016-09-05T21:21:46.221917: step 9972, loss 0.00699443, acc 1
2016-09-05T21:21:47.003885: step 9973, loss 0.00530904, acc 1
2016-09-05T21:21:47.805265: step 9974, loss 0.0326857, acc 0.98
2016-09-05T21:21:48.593878: step 9975, loss 0.0243261, acc 0.98
2016-09-05T21:21:49.443216: step 9976, loss 0.0067641, acc 1
2016-09-05T21:21:50.258411: step 9977, loss 0.0331466, acc 0.96
2016-09-05T21:21:51.035504: step 9978, loss 0.0165863, acc 1
2016-09-05T21:21:51.857561: step 9979, loss 0.030581, acc 0.98
2016-09-05T21:21:52.687735: step 9980, loss 0.0589043, acc 0.98
2016-09-05T21:21:53.474591: step 9981, loss 0.0105905, acc 1
2016-09-05T21:21:54.278813: step 9982, loss 0.00801784, acc 1
2016-09-05T21:21:55.087832: step 9983, loss 0.00385249, acc 1
2016-09-05T21:21:55.851341: step 9984, loss 0.0118317, acc 1
2016-09-05T21:21:56.651200: step 9985, loss 0.0181425, acc 0.98
2016-09-05T21:21:57.472815: step 9986, loss 0.00814879, acc 1
2016-09-05T21:21:58.271725: step 9987, loss 0.0142803, acc 1
2016-09-05T21:21:59.073963: step 9988, loss 0.0429922, acc 0.98
2016-09-05T21:21:59.923338: step 9989, loss 0.0459764, acc 0.98
2016-09-05T21:22:00.738971: step 9990, loss 0.0156058, acc 1
2016-09-05T21:22:01.545849: step 9991, loss 0.0178889, acc 0.98
2016-09-05T21:22:02.394025: step 9992, loss 0.032098, acc 0.98
2016-09-05T21:22:03.195801: step 9993, loss 0.00329284, acc 1
2016-09-05T21:22:04.020358: step 9994, loss 0.0164415, acc 1
2016-09-05T21:22:04.861734: step 9995, loss 0.019388, acc 1
2016-09-05T21:22:05.684179: step 9996, loss 0.0150092, acc 1
2016-09-05T21:22:06.498222: step 9997, loss 0.029718, acc 0.98
2016-09-05T21:22:07.367913: step 9998, loss 0.0199278, acc 1
2016-09-05T21:22:08.199195: step 9999, loss 0.00257662, acc 1
2016-09-05T21:22:09.012326: step 10000, loss 0.00340543, acc 1

Evaluation:
2016-09-05T21:22:12.536726: step 10000, loss 2.09442, acc 0.737

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-10000

2016-09-05T21:22:14.504058: step 10001, loss 0.0162658, acc 1
2016-09-05T21:22:15.326646: step 10002, loss 0.0400076, acc 0.98
2016-09-05T21:22:16.136959: step 10003, loss 0.00791916, acc 1
2016-09-05T21:22:16.985306: step 10004, loss 0.0332378, acc 0.96
2016-09-05T21:22:17.831567: step 10005, loss 0.0493512, acc 0.96
2016-09-05T21:22:18.631717: step 10006, loss 0.00228139, acc 1
2016-09-05T21:22:19.501687: step 10007, loss 0.0618713, acc 0.96
2016-09-05T21:22:20.325019: step 10008, loss 0.0171859, acc 1
2016-09-05T21:22:21.148617: step 10009, loss 0.00235598, acc 1
2016-09-05T21:22:21.983075: step 10010, loss 0.0318255, acc 0.98
2016-09-05T21:22:22.820496: step 10011, loss 0.025818, acc 0.98
2016-09-05T21:22:23.624961: step 10012, loss 0.041529, acc 0.96
2016-09-05T21:22:24.444358: step 10013, loss 0.00501297, acc 1
2016-09-05T21:22:25.235849: step 10014, loss 0.0164441, acc 0.98
2016-09-05T21:22:26.040656: step 10015, loss 0.0207112, acc 0.98
2016-09-05T21:22:26.867689: step 10016, loss 0.0230961, acc 1
2016-09-05T21:22:27.677418: step 10017, loss 0.00741636, acc 1
2016-09-05T21:22:28.489663: step 10018, loss 0.0328477, acc 0.98
2016-09-05T21:22:29.295673: step 10019, loss 0.0165677, acc 1
2016-09-05T21:22:30.101454: step 10020, loss 0.0163966, acc 1
2016-09-05T21:22:30.883810: step 10021, loss 0.017961, acc 0.98
2016-09-05T21:22:31.706993: step 10022, loss 0.0144821, acc 1
2016-09-05T21:22:32.535212: step 10023, loss 0.0126367, acc 1
2016-09-05T21:22:33.330807: step 10024, loss 0.0257412, acc 0.98
2016-09-05T21:22:34.127712: step 10025, loss 0.022936, acc 1
2016-09-05T21:22:34.923446: step 10026, loss 0.00640519, acc 1
2016-09-05T21:22:35.721797: step 10027, loss 0.00235231, acc 1
2016-09-05T21:22:36.534865: step 10028, loss 0.00248381, acc 1
2016-09-05T21:22:37.342792: step 10029, loss 0.0318252, acc 0.98
2016-09-05T21:22:38.127506: step 10030, loss 0.00239285, acc 1
2016-09-05T21:22:38.958541: step 10031, loss 0.0666967, acc 0.96
2016-09-05T21:22:39.791664: step 10032, loss 0.0111635, acc 1
2016-09-05T21:22:40.585379: step 10033, loss 0.0159719, acc 1
2016-09-05T21:22:41.386189: step 10034, loss 0.0349258, acc 0.98
2016-09-05T21:22:42.200505: step 10035, loss 0.00581678, acc 1
2016-09-05T21:22:42.969276: step 10036, loss 0.0177305, acc 0.98
2016-09-05T21:22:43.811540: step 10037, loss 0.00289857, acc 1
2016-09-05T21:22:44.653407: step 10038, loss 0.0100208, acc 1
2016-09-05T21:22:45.419038: step 10039, loss 0.0082804, acc 1
2016-09-05T21:22:46.224813: step 10040, loss 0.00256497, acc 1
2016-09-05T21:22:47.073387: step 10041, loss 0.0624096, acc 0.98
2016-09-05T21:22:47.855539: step 10042, loss 0.019692, acc 0.98
2016-09-05T21:22:48.662809: step 10043, loss 0.010258, acc 1
2016-09-05T21:22:49.507193: step 10044, loss 0.00246689, acc 1
2016-09-05T21:22:50.297883: step 10045, loss 0.0287402, acc 1
2016-09-05T21:22:51.101478: step 10046, loss 0.0348784, acc 0.98
2016-09-05T21:22:51.910371: step 10047, loss 0.0343705, acc 0.96
2016-09-05T21:22:52.680145: step 10048, loss 0.0345591, acc 0.98
2016-09-05T21:22:53.477306: step 10049, loss 0.00824883, acc 1
2016-09-05T21:22:54.305477: step 10050, loss 0.00454697, acc 1
2016-09-05T21:22:55.130030: step 10051, loss 0.00222818, acc 1
2016-09-05T21:22:55.918630: step 10052, loss 0.0648599, acc 0.98
2016-09-05T21:22:56.740443: step 10053, loss 0.00228693, acc 1
2016-09-05T21:22:57.513484: step 10054, loss 0.013672, acc 1
2016-09-05T21:22:58.329646: step 10055, loss 0.00918809, acc 1
2016-09-05T21:22:59.110930: step 10056, loss 0.0113416, acc 1
2016-09-05T21:22:59.880704: step 10057, loss 0.00217345, acc 1
2016-09-05T21:23:00.707703: step 10058, loss 0.0226757, acc 0.98
2016-09-05T21:23:01.496769: step 10059, loss 0.00220412, acc 1
2016-09-05T21:23:02.312396: step 10060, loss 0.0159495, acc 1
2016-09-05T21:23:03.136633: step 10061, loss 0.0147813, acc 1
2016-09-05T21:23:03.967288: step 10062, loss 0.0222908, acc 0.98
2016-09-05T21:23:04.769922: step 10063, loss 0.0337067, acc 1
2016-09-05T21:23:05.578591: step 10064, loss 0.00521798, acc 1
2016-09-05T21:23:06.406925: step 10065, loss 0.0123604, acc 1
2016-09-05T21:23:07.186079: step 10066, loss 0.00372828, acc 1
2016-09-05T21:23:07.999113: step 10067, loss 0.00218197, acc 1
2016-09-05T21:23:08.800184: step 10068, loss 0.0150641, acc 1
2016-09-05T21:23:09.590721: step 10069, loss 0.03114, acc 0.98
2016-09-05T21:23:10.440296: step 10070, loss 0.00210887, acc 1
2016-09-05T21:23:11.241326: step 10071, loss 0.00223383, acc 1
2016-09-05T21:23:12.019609: step 10072, loss 0.00225494, acc 1
2016-09-05T21:23:12.811947: step 10073, loss 0.0032755, acc 1
2016-09-05T21:23:13.630930: step 10074, loss 0.0166551, acc 1
2016-09-05T21:23:14.441965: step 10075, loss 0.0182915, acc 0.98
2016-09-05T21:23:15.247983: step 10076, loss 0.00281621, acc 1
2016-09-05T21:23:16.070437: step 10077, loss 0.00204364, acc 1
2016-09-05T21:23:16.864955: step 10078, loss 0.00356642, acc 1
2016-09-05T21:23:17.650584: step 10079, loss 0.00207289, acc 1
2016-09-05T21:23:18.458617: step 10080, loss 0.0571535, acc 0.98
2016-09-05T21:23:19.245878: step 10081, loss 0.0202019, acc 1
2016-09-05T21:23:20.074764: step 10082, loss 0.00195403, acc 1
2016-09-05T21:23:20.869928: step 10083, loss 0.0165789, acc 1
2016-09-05T21:23:21.659411: step 10084, loss 0.00524706, acc 1
2016-09-05T21:23:22.498983: step 10085, loss 0.0118392, acc 1
2016-09-05T21:23:23.333784: step 10086, loss 0.013599, acc 1
2016-09-05T21:23:24.111599: step 10087, loss 0.00228663, acc 1
2016-09-05T21:23:24.533612: step 10088, loss 0.077364, acc 0.916667
2016-09-05T21:23:25.321892: step 10089, loss 0.016159, acc 1
2016-09-05T21:23:26.122619: step 10090, loss 0.00278978, acc 1
2016-09-05T21:23:26.915618: step 10091, loss 0.0844473, acc 0.98
2016-09-05T21:23:27.715265: step 10092, loss 0.00205502, acc 1
2016-09-05T21:23:28.518107: step 10093, loss 0.00202942, acc 1
2016-09-05T21:23:29.361008: step 10094, loss 0.00872706, acc 1
2016-09-05T21:23:30.120145: step 10095, loss 0.0158058, acc 1
2016-09-05T21:23:30.901079: step 10096, loss 0.0184319, acc 1
2016-09-05T21:23:31.689020: step 10097, loss 0.00227577, acc 1
2016-09-05T21:23:32.473386: step 10098, loss 0.030235, acc 0.98
2016-09-05T21:23:33.311076: step 10099, loss 0.00197842, acc 1
2016-09-05T21:23:34.132106: step 10100, loss 0.0442798, acc 0.98

Evaluation:
2016-09-05T21:23:37.620842: step 10100, loss 2.24043, acc 0.742

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-10100

2016-09-05T21:23:39.477436: step 10101, loss 0.0632023, acc 0.96
2016-09-05T21:23:40.290245: step 10102, loss 0.0377233, acc 0.98
2016-09-05T21:23:41.074329: step 10103, loss 0.00205166, acc 1
2016-09-05T21:23:41.884213: step 10104, loss 0.0053908, acc 1
2016-09-05T21:23:42.679965: step 10105, loss 0.0222767, acc 1
2016-09-05T21:23:43.471633: step 10106, loss 0.00536289, acc 1
2016-09-05T21:23:44.277637: step 10107, loss 0.0542376, acc 0.98
2016-09-05T21:23:45.113060: step 10108, loss 0.0153618, acc 1
2016-09-05T21:23:45.913777: step 10109, loss 0.0788009, acc 0.98
2016-09-05T21:23:46.744467: step 10110, loss 0.0150118, acc 1
2016-09-05T21:23:47.565390: step 10111, loss 0.00294367, acc 1
2016-09-05T21:23:48.370771: step 10112, loss 0.00389684, acc 1
2016-09-05T21:23:49.184290: step 10113, loss 0.0202639, acc 0.98
2016-09-05T21:23:50.050940: step 10114, loss 0.00225083, acc 1
2016-09-05T21:23:50.876276: step 10115, loss 0.00164958, acc 1
2016-09-05T21:23:51.723621: step 10116, loss 0.00380412, acc 1
2016-09-05T21:23:52.561348: step 10117, loss 0.0743028, acc 0.98
2016-09-05T21:23:53.336018: step 10118, loss 0.0227021, acc 1
2016-09-05T21:23:54.175966: step 10119, loss 0.0107123, acc 1
2016-09-05T21:23:55.023173: step 10120, loss 0.0461196, acc 0.96
2016-09-05T21:23:55.879627: step 10121, loss 0.00608898, acc 1
2016-09-05T21:23:56.682976: step 10122, loss 0.021806, acc 1
2016-09-05T21:23:57.528861: step 10123, loss 0.0494964, acc 0.98
2016-09-05T21:23:58.336135: step 10124, loss 0.0211668, acc 0.98
2016-09-05T21:23:59.144006: step 10125, loss 0.0129143, acc 1
2016-09-05T21:23:59.943183: step 10126, loss 0.00600528, acc 1
2016-09-05T21:24:00.758055: step 10127, loss 0.0335485, acc 0.98
2016-09-05T21:24:01.566645: step 10128, loss 0.00299766, acc 1
2016-09-05T21:24:02.366173: step 10129, loss 0.0192588, acc 0.98
2016-09-05T21:24:03.180171: step 10130, loss 0.0371832, acc 0.98
2016-09-05T21:24:03.983280: step 10131, loss 0.00201635, acc 1
2016-09-05T21:24:04.793301: step 10132, loss 0.028867, acc 0.98
2016-09-05T21:24:05.612977: step 10133, loss 0.026063, acc 0.98
2016-09-05T21:24:06.426232: step 10134, loss 0.00691319, acc 1
2016-09-05T21:24:07.258624: step 10135, loss 0.020634, acc 1
2016-09-05T21:24:08.051942: step 10136, loss 0.00244956, acc 1
2016-09-05T21:24:08.849340: step 10137, loss 0.0183236, acc 0.98
2016-09-05T21:24:09.613206: step 10138, loss 0.0393873, acc 0.98
2016-09-05T21:24:10.411888: step 10139, loss 0.0286586, acc 0.98
2016-09-05T21:24:11.211832: step 10140, loss 0.00288696, acc 1
2016-09-05T21:24:12.000669: step 10141, loss 0.00428867, acc 1
2016-09-05T21:24:12.789422: step 10142, loss 0.0295014, acc 0.98
2016-09-05T21:24:13.612358: step 10143, loss 0.0194607, acc 0.98
2016-09-05T21:24:14.431219: step 10144, loss 0.00311083, acc 1
2016-09-05T21:24:15.239155: step 10145, loss 0.0991161, acc 0.96
2016-09-05T21:24:16.018839: step 10146, loss 0.00234137, acc 1
2016-09-05T21:24:16.835041: step 10147, loss 0.00188328, acc 1
2016-09-05T21:24:17.633666: step 10148, loss 0.0235834, acc 0.98
2016-09-05T21:24:18.459073: step 10149, loss 0.0100096, acc 1
2016-09-05T21:24:19.275460: step 10150, loss 0.00599253, acc 1
2016-09-05T21:24:20.095803: step 10151, loss 0.0609682, acc 0.94
2016-09-05T21:24:20.874164: step 10152, loss 0.0370431, acc 0.96
2016-09-05T21:24:21.674524: step 10153, loss 0.00223203, acc 1
2016-09-05T21:24:22.551566: step 10154, loss 0.0564577, acc 0.98
2016-09-05T21:24:23.333442: step 10155, loss 0.00350682, acc 1
2016-09-05T21:24:24.141719: step 10156, loss 0.00535716, acc 1
2016-09-05T21:24:24.937409: step 10157, loss 0.0150628, acc 1
2016-09-05T21:24:25.728099: step 10158, loss 0.0474627, acc 0.98
2016-09-05T21:24:26.535279: step 10159, loss 0.00238982, acc 1
2016-09-05T21:24:27.352962: step 10160, loss 0.0239183, acc 0.98
2016-09-05T21:24:28.147303: step 10161, loss 0.00595857, acc 1
2016-09-05T21:24:28.933034: step 10162, loss 0.0310523, acc 0.98
2016-09-05T21:24:29.820052: step 10163, loss 0.00236666, acc 1
2016-09-05T21:24:30.661443: step 10164, loss 0.0477041, acc 0.96
2016-09-05T21:24:31.460635: step 10165, loss 0.0218784, acc 0.98
2016-09-05T21:24:32.269096: step 10166, loss 0.00828909, acc 1
2016-09-05T21:24:33.058800: step 10167, loss 0.00670275, acc 1
2016-09-05T21:24:33.851291: step 10168, loss 0.0330666, acc 0.98
2016-09-05T21:24:34.659822: step 10169, loss 0.0423659, acc 0.98
2016-09-05T21:24:35.441721: step 10170, loss 0.00351194, acc 1
2016-09-05T21:24:36.220291: step 10171, loss 0.0182645, acc 1
2016-09-05T21:24:37.023506: step 10172, loss 0.00353529, acc 1
2016-09-05T21:24:37.798359: step 10173, loss 0.00248902, acc 1
2016-09-05T21:24:38.622912: step 10174, loss 0.0415904, acc 0.98
2016-09-05T21:24:39.450164: step 10175, loss 0.00648304, acc 1
2016-09-05T21:24:40.230602: step 10176, loss 0.00908683, acc 1
2016-09-05T21:24:41.065998: step 10177, loss 0.0148982, acc 1
2016-09-05T21:24:41.904832: step 10178, loss 0.0366122, acc 0.96
2016-09-05T21:24:42.691552: step 10179, loss 0.0181056, acc 0.98
2016-09-05T21:24:43.475361: step 10180, loss 0.0253457, acc 1
2016-09-05T21:24:44.283626: step 10181, loss 0.00255456, acc 1
2016-09-05T21:24:45.084667: step 10182, loss 0.0147635, acc 1
2016-09-05T21:24:45.889265: step 10183, loss 0.0417428, acc 0.98
2016-09-05T21:24:46.734684: step 10184, loss 0.0522778, acc 0.98
2016-09-05T21:24:47.509009: step 10185, loss 0.00335327, acc 1
2016-09-05T21:24:48.298134: step 10186, loss 0.0199766, acc 0.98
2016-09-05T21:24:49.109963: step 10187, loss 0.0126346, acc 1
2016-09-05T21:24:49.924636: step 10188, loss 0.0460117, acc 0.98
2016-09-05T21:24:50.711433: step 10189, loss 0.00856973, acc 1
2016-09-05T21:24:51.521213: step 10190, loss 0.035176, acc 0.98
2016-09-05T21:24:52.320486: step 10191, loss 0.0947882, acc 0.96
2016-09-05T21:24:53.120528: step 10192, loss 0.00486242, acc 1
2016-09-05T21:24:53.937616: step 10193, loss 0.0139414, acc 1
2016-09-05T21:24:54.712128: step 10194, loss 0.0393695, acc 0.98
2016-09-05T21:24:55.559029: step 10195, loss 0.00347627, acc 1
2016-09-05T21:24:56.416575: step 10196, loss 0.0249583, acc 0.98
2016-09-05T21:24:57.170805: step 10197, loss 0.0127091, acc 1
2016-09-05T21:24:57.980632: step 10198, loss 0.0152658, acc 1
2016-09-05T21:24:58.837310: step 10199, loss 0.0673383, acc 0.96
2016-09-05T21:24:59.634771: step 10200, loss 0.00368858, acc 1

Evaluation:
2016-09-05T21:25:03.100783: step 10200, loss 1.95567, acc 0.741

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-10200

2016-09-05T21:25:05.019774: step 10201, loss 0.0217529, acc 1
2016-09-05T21:25:05.885866: step 10202, loss 0.00219858, acc 1
2016-09-05T21:25:06.689487: step 10203, loss 0.0136839, acc 1
2016-09-05T21:25:07.534641: step 10204, loss 0.066561, acc 0.98
2016-09-05T21:25:08.360417: step 10205, loss 0.0216338, acc 0.98
2016-09-05T21:25:09.180296: step 10206, loss 0.00247167, acc 1
2016-09-05T21:25:10.021417: step 10207, loss 0.00645396, acc 1
2016-09-05T21:25:10.844638: step 10208, loss 0.0205639, acc 0.98
2016-09-05T21:25:11.639938: step 10209, loss 0.0027993, acc 1
2016-09-05T21:25:12.463622: step 10210, loss 0.0400959, acc 0.98
2016-09-05T21:25:13.305187: step 10211, loss 0.0110278, acc 1
2016-09-05T21:25:14.114089: step 10212, loss 0.00452562, acc 1
2016-09-05T21:25:14.934732: step 10213, loss 0.0145214, acc 1
2016-09-05T21:25:15.730498: step 10214, loss 0.0385898, acc 0.98
2016-09-05T21:25:16.528308: step 10215, loss 0.00446036, acc 1
2016-09-05T21:25:17.335974: step 10216, loss 0.0033848, acc 1
2016-09-05T21:25:18.152640: step 10217, loss 0.00642433, acc 1
2016-09-05T21:25:18.917342: step 10218, loss 0.00243591, acc 1
2016-09-05T21:25:19.730449: step 10219, loss 0.00663949, acc 1
2016-09-05T21:25:20.519536: step 10220, loss 0.00348809, acc 1
2016-09-05T21:25:21.321726: step 10221, loss 0.00337231, acc 1
2016-09-05T21:25:22.152187: step 10222, loss 0.0161053, acc 1
2016-09-05T21:25:22.986075: step 10223, loss 0.0423558, acc 0.98
2016-09-05T21:25:23.776895: step 10224, loss 0.00622967, acc 1
2016-09-05T21:25:24.568840: step 10225, loss 0.0184691, acc 1
2016-09-05T21:25:25.394129: step 10226, loss 0.00285093, acc 1
2016-09-05T21:25:26.206338: step 10227, loss 0.00581108, acc 1
2016-09-05T21:25:27.020239: step 10228, loss 0.00848848, acc 1
2016-09-05T21:25:27.850013: step 10229, loss 0.0138031, acc 1
2016-09-05T21:25:28.633208: step 10230, loss 0.0511498, acc 0.98
2016-09-05T21:25:29.449581: step 10231, loss 0.00500969, acc 1
2016-09-05T21:25:30.260377: step 10232, loss 0.0190521, acc 0.98
2016-09-05T21:25:31.037091: step 10233, loss 0.0036323, acc 1
2016-09-05T21:25:31.836253: step 10234, loss 0.0422685, acc 0.98
2016-09-05T21:25:32.653470: step 10235, loss 0.00359247, acc 1
2016-09-05T21:25:33.451981: step 10236, loss 0.0403602, acc 0.98
2016-09-05T21:25:34.285409: step 10237, loss 0.00327949, acc 1
2016-09-05T21:25:35.116693: step 10238, loss 0.00232667, acc 1
2016-09-05T21:25:35.917285: step 10239, loss 0.0126079, acc 1
2016-09-05T21:25:36.722412: step 10240, loss 0.0476335, acc 1
2016-09-05T21:25:37.538411: step 10241, loss 0.0234902, acc 0.98
2016-09-05T21:25:38.329757: step 10242, loss 0.0316137, acc 0.98
2016-09-05T21:25:39.147493: step 10243, loss 0.0357357, acc 0.98
2016-09-05T21:25:39.995870: step 10244, loss 0.00252948, acc 1
2016-09-05T21:25:40.802181: step 10245, loss 0.0628272, acc 0.98
2016-09-05T21:25:41.617159: step 10246, loss 0.00301638, acc 1
2016-09-05T21:25:42.432942: step 10247, loss 0.008795, acc 1
2016-09-05T21:25:43.275068: step 10248, loss 0.0179869, acc 1
2016-09-05T21:25:44.075527: step 10249, loss 0.00259239, acc 1
2016-09-05T21:25:44.915387: step 10250, loss 0.022014, acc 1
2016-09-05T21:25:45.733115: step 10251, loss 0.00300665, acc 1
2016-09-05T21:25:46.563467: step 10252, loss 0.0220683, acc 1
2016-09-05T21:25:47.416819: step 10253, loss 0.00925606, acc 1
2016-09-05T21:25:48.242096: step 10254, loss 0.0237253, acc 0.98
2016-09-05T21:25:49.049746: step 10255, loss 0.00437092, acc 1
2016-09-05T21:25:49.858565: step 10256, loss 0.0250394, acc 1
2016-09-05T21:25:50.704123: step 10257, loss 0.0217887, acc 0.98
2016-09-05T21:25:51.523362: step 10258, loss 0.146444, acc 0.98
2016-09-05T21:25:52.342437: step 10259, loss 0.0283902, acc 0.98
2016-09-05T21:25:53.150526: step 10260, loss 0.0331121, acc 0.98
2016-09-05T21:25:53.954812: step 10261, loss 0.00292831, acc 1
2016-09-05T21:25:54.826788: step 10262, loss 0.011071, acc 1
2016-09-05T21:25:55.655814: step 10263, loss 0.0215431, acc 0.98
2016-09-05T21:25:56.434368: step 10264, loss 0.0663404, acc 0.94
2016-09-05T21:25:57.253467: step 10265, loss 0.00278083, acc 1
2016-09-05T21:25:58.087165: step 10266, loss 0.0030295, acc 1
2016-09-05T21:25:58.885149: step 10267, loss 0.0443286, acc 0.96
2016-09-05T21:25:59.692327: step 10268, loss 0.0210185, acc 0.98
2016-09-05T21:26:00.613411: step 10269, loss 0.0666493, acc 0.96
2016-09-05T21:26:01.431397: step 10270, loss 0.00283756, acc 1
2016-09-05T21:26:02.251586: step 10271, loss 0.0203427, acc 0.98
2016-09-05T21:26:03.104522: step 10272, loss 0.0146449, acc 1
2016-09-05T21:26:03.900731: step 10273, loss 0.00454037, acc 1
2016-09-05T21:26:04.710416: step 10274, loss 0.00346335, acc 1
2016-09-05T21:26:05.543683: step 10275, loss 0.0370871, acc 0.98
2016-09-05T21:26:06.379359: step 10276, loss 0.125539, acc 0.96
2016-09-05T21:26:07.206983: step 10277, loss 0.045438, acc 0.98
2016-09-05T21:26:08.039275: step 10278, loss 0.045533, acc 0.98
2016-09-05T21:26:08.833552: step 10279, loss 0.0514432, acc 0.98
2016-09-05T21:26:09.679245: step 10280, loss 0.00916195, acc 1
2016-09-05T21:26:10.497963: step 10281, loss 0.0427612, acc 0.98
2016-09-05T21:26:10.907820: step 10282, loss 0.0029243, acc 1
2016-09-05T21:26:11.700525: step 10283, loss 0.0313042, acc 0.98
2016-09-05T21:26:12.520434: step 10284, loss 0.0131202, acc 1
2016-09-05T21:26:13.298268: step 10285, loss 0.0191191, acc 1
2016-09-05T21:26:14.104616: step 10286, loss 0.00846879, acc 1
2016-09-05T21:26:14.939562: step 10287, loss 0.0110996, acc 1
2016-09-05T21:26:15.731687: step 10288, loss 0.0430429, acc 0.98
2016-09-05T21:26:16.544948: step 10289, loss 0.0299009, acc 1
2016-09-05T21:26:17.374374: step 10290, loss 0.0304979, acc 0.98
2016-09-05T21:26:18.164209: step 10291, loss 0.00399414, acc 1
2016-09-05T21:26:18.954524: step 10292, loss 0.00684652, acc 1
2016-09-05T21:26:19.778548: step 10293, loss 0.0111895, acc 1
2016-09-05T21:26:20.563577: step 10294, loss 0.01771, acc 1
2016-09-05T21:26:21.402827: step 10295, loss 0.0135369, acc 1
2016-09-05T21:26:22.235034: step 10296, loss 0.0522934, acc 0.96
2016-09-05T21:26:23.045465: step 10297, loss 0.00401191, acc 1
2016-09-05T21:26:23.857433: step 10298, loss 0.0191925, acc 0.98
2016-09-05T21:26:24.674714: step 10299, loss 0.00507352, acc 1
2016-09-05T21:26:25.431891: step 10300, loss 0.0129565, acc 1

Evaluation:
2016-09-05T21:26:28.927435: step 10300, loss 3.02535, acc 0.739

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-10300

2016-09-05T21:26:30.856308: step 10301, loss 0.0102379, acc 1
2016-09-05T21:26:31.646565: step 10302, loss 0.00509565, acc 1
2016-09-05T21:26:32.441306: step 10303, loss 0.00470443, acc 1
2016-09-05T21:26:33.254395: step 10304, loss 0.01653, acc 1
2016-09-05T21:26:34.093878: step 10305, loss 0.00482699, acc 1
2016-09-05T21:26:34.896774: step 10306, loss 0.00681816, acc 1
2016-09-05T21:26:35.842427: step 10307, loss 0.0910818, acc 0.98
2016-09-05T21:26:36.724881: step 10308, loss 0.048818, acc 0.98
2016-09-05T21:26:37.568333: step 10309, loss 0.00507361, acc 1
2016-09-05T21:26:38.446061: step 10310, loss 0.00637112, acc 1
2016-09-05T21:26:39.275357: step 10311, loss 0.00567562, acc 1
2016-09-05T21:26:40.177387: step 10312, loss 0.0705139, acc 0.96
2016-09-05T21:26:41.028635: step 10313, loss 0.0121042, acc 1
2016-09-05T21:26:41.965261: step 10314, loss 0.0494367, acc 0.96
2016-09-05T21:26:42.786743: step 10315, loss 0.00477127, acc 1
2016-09-05T21:26:43.621433: step 10316, loss 0.0899188, acc 0.96
2016-09-05T21:26:44.468697: step 10317, loss 0.00557949, acc 1
2016-09-05T21:26:45.291243: step 10318, loss 0.0629307, acc 0.98
2016-09-05T21:26:46.096450: step 10319, loss 0.0340785, acc 0.98
2016-09-05T21:26:46.914500: step 10320, loss 0.0517798, acc 0.98
2016-09-05T21:26:47.730826: step 10321, loss 0.00754864, acc 1
2016-09-05T21:26:48.523327: step 10322, loss 0.00439382, acc 1
2016-09-05T21:26:49.374972: step 10323, loss 0.0531767, acc 0.98
2016-09-05T21:26:50.196541: step 10324, loss 0.00879099, acc 1
2016-09-05T21:26:50.964602: step 10325, loss 0.0220336, acc 1
2016-09-05T21:26:51.756116: step 10326, loss 0.00590899, acc 1
2016-09-05T21:26:52.578112: step 10327, loss 0.0408217, acc 0.98
2016-09-05T21:26:53.374666: step 10328, loss 0.0318787, acc 0.98
2016-09-05T21:26:54.205402: step 10329, loss 0.0041474, acc 1
2016-09-05T21:26:54.988562: step 10330, loss 0.037844, acc 0.98
2016-09-05T21:26:55.809408: step 10331, loss 0.0426333, acc 0.98
2016-09-05T21:26:56.584629: step 10332, loss 0.135492, acc 0.96
2016-09-05T21:26:57.401461: step 10333, loss 0.027209, acc 1
2016-09-05T21:26:58.192111: step 10334, loss 0.00369516, acc 1
2016-09-05T21:26:58.998717: step 10335, loss 0.003256, acc 1
2016-09-05T21:26:59.852308: step 10336, loss 0.00334485, acc 1
2016-09-05T21:27:00.669064: step 10337, loss 0.00314894, acc 1
2016-09-05T21:27:01.468625: step 10338, loss 0.0180454, acc 1
2016-09-05T21:27:02.276812: step 10339, loss 0.0386185, acc 0.98
2016-09-05T21:27:03.035292: step 10340, loss 0.14493, acc 0.98
2016-09-05T21:27:03.841680: step 10341, loss 0.00791487, acc 1
2016-09-05T21:27:04.656494: step 10342, loss 0.0335344, acc 0.98
2016-09-05T21:27:05.455080: step 10343, loss 0.0185013, acc 1
2016-09-05T21:27:06.302186: step 10344, loss 0.0218055, acc 0.98
2016-09-05T21:27:07.115444: step 10345, loss 0.0772766, acc 0.96
2016-09-05T21:27:07.918536: step 10346, loss 0.106404, acc 0.98
2016-09-05T21:27:08.721362: step 10347, loss 0.00257614, acc 1
2016-09-05T21:27:09.538346: step 10348, loss 0.0159849, acc 1
2016-09-05T21:27:10.327934: step 10349, loss 0.0374704, acc 0.98
2016-09-05T21:27:11.122428: step 10350, loss 0.00719187, acc 1
2016-09-05T21:27:11.940210: step 10351, loss 0.0208194, acc 1
2016-09-05T21:27:12.707133: step 10352, loss 0.0311385, acc 1
2016-09-05T21:27:13.503669: step 10353, loss 0.0299727, acc 0.98
2016-09-05T21:27:14.319710: step 10354, loss 0.0135111, acc 1
2016-09-05T21:27:15.115892: step 10355, loss 0.0108631, acc 1
2016-09-05T21:27:15.913363: step 10356, loss 0.0044986, acc 1
2016-09-05T21:27:16.727804: step 10357, loss 0.00500732, acc 1
2016-09-05T21:27:17.518808: step 10358, loss 0.0611997, acc 0.96
2016-09-05T21:27:18.341552: step 10359, loss 0.00900169, acc 1
2016-09-05T21:27:19.198658: step 10360, loss 0.00273973, acc 1
2016-09-05T21:27:19.986652: step 10361, loss 0.0194751, acc 0.98
2016-09-05T21:27:20.787930: step 10362, loss 0.0157542, acc 1
2016-09-05T21:27:21.601878: step 10363, loss 0.00404981, acc 1
2016-09-05T21:27:22.377613: step 10364, loss 0.0202064, acc 0.98
2016-09-05T21:27:23.172872: step 10365, loss 0.0304991, acc 1
2016-09-05T21:27:23.998000: step 10366, loss 0.0181851, acc 1
2016-09-05T21:27:24.780876: step 10367, loss 0.0280994, acc 0.98
2016-09-05T21:27:25.575806: step 10368, loss 0.0337318, acc 0.98
2016-09-05T21:27:26.389580: step 10369, loss 0.0045113, acc 1
2016-09-05T21:27:27.168992: step 10370, loss 0.00302398, acc 1
2016-09-05T21:27:27.974462: step 10371, loss 0.0883484, acc 0.96
2016-09-05T21:27:28.806521: step 10372, loss 0.0298271, acc 1
2016-09-05T21:27:29.588313: step 10373, loss 0.018572, acc 0.98
2016-09-05T21:27:30.396617: step 10374, loss 0.365426, acc 0.96
2016-09-05T21:27:31.220951: step 10375, loss 0.0366528, acc 0.98
2016-09-05T21:27:32.010643: step 10376, loss 0.0109185, acc 1
2016-09-05T21:27:32.805338: step 10377, loss 0.00912821, acc 1
2016-09-05T21:27:33.630381: step 10378, loss 0.00592792, acc 1
2016-09-05T21:27:34.405439: step 10379, loss 0.0200489, acc 1
2016-09-05T21:27:35.217660: step 10380, loss 0.0517042, acc 1
2016-09-05T21:27:36.002956: step 10381, loss 0.04063, acc 0.98
2016-09-05T21:27:36.813879: step 10382, loss 0.0306809, acc 1
2016-09-05T21:27:37.623657: step 10383, loss 0.0235929, acc 1
2016-09-05T21:27:38.437926: step 10384, loss 0.0487053, acc 0.98
2016-09-05T21:27:39.275796: step 10385, loss 0.0389669, acc 0.98
2016-09-05T21:27:40.075751: step 10386, loss 0.0123109, acc 1
2016-09-05T21:27:40.913411: step 10387, loss 0.110518, acc 0.98
2016-09-05T21:27:41.694617: step 10388, loss 0.0214644, acc 1
2016-09-05T21:27:42.503081: step 10389, loss 0.018377, acc 0.98
2016-09-05T21:27:43.352081: step 10390, loss 0.0143458, acc 1
2016-09-05T21:27:44.112678: step 10391, loss 0.0155454, acc 1
2016-09-05T21:27:44.911723: step 10392, loss 0.0298515, acc 0.98
2016-09-05T21:27:45.727254: step 10393, loss 0.0483314, acc 0.96
2016-09-05T21:27:46.531981: step 10394, loss 0.0205661, acc 0.98
2016-09-05T21:27:47.348157: step 10395, loss 0.0070694, acc 1
2016-09-05T21:27:48.148476: step 10396, loss 0.0105951, acc 1
2016-09-05T21:27:48.951410: step 10397, loss 0.00357171, acc 1
2016-09-05T21:27:49.758850: step 10398, loss 0.0208753, acc 0.98
2016-09-05T21:27:50.595831: step 10399, loss 0.0226451, acc 1
2016-09-05T21:27:51.363515: step 10400, loss 0.00669157, acc 1

Evaluation:
2016-09-05T21:27:54.867198: step 10400, loss 2.02594, acc 0.735

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-10400

2016-09-05T21:27:56.758063: step 10401, loss 0.00465076, acc 1
2016-09-05T21:27:57.568716: step 10402, loss 0.0508471, acc 0.98
2016-09-05T21:27:58.353045: step 10403, loss 0.0174245, acc 1
2016-09-05T21:27:59.149970: step 10404, loss 0.00363427, acc 1
2016-09-05T21:27:59.901852: step 10405, loss 0.00356749, acc 1
2016-09-05T21:28:00.727605: step 10406, loss 0.00802347, acc 1
2016-09-05T21:28:01.562548: step 10407, loss 0.082768, acc 0.96
2016-09-05T21:28:02.366748: step 10408, loss 0.0219289, acc 0.98
2016-09-05T21:28:03.203326: step 10409, loss 0.00849717, acc 1
2016-09-05T21:28:04.008668: step 10410, loss 0.00310052, acc 1
2016-09-05T21:28:04.808887: step 10411, loss 0.00382805, acc 1
2016-09-05T21:28:05.613794: step 10412, loss 0.0538462, acc 0.98
2016-09-05T21:28:06.446181: step 10413, loss 0.0186089, acc 0.98
2016-09-05T21:28:07.249804: step 10414, loss 0.0162191, acc 1
2016-09-05T21:28:08.080760: step 10415, loss 0.0584226, acc 0.98
2016-09-05T21:28:08.890489: step 10416, loss 0.0375829, acc 0.98
2016-09-05T21:28:09.721793: step 10417, loss 0.0410705, acc 0.98
2016-09-05T21:28:10.543658: step 10418, loss 0.015953, acc 1
2016-09-05T21:28:11.359949: step 10419, loss 0.0311492, acc 0.98
2016-09-05T21:28:12.171389: step 10420, loss 0.0114318, acc 1
2016-09-05T21:28:12.983751: step 10421, loss 0.026832, acc 0.98
2016-09-05T21:28:13.807281: step 10422, loss 0.005803, acc 1
2016-09-05T21:28:14.630004: step 10423, loss 0.0186604, acc 1
2016-09-05T21:28:15.446745: step 10424, loss 0.00844362, acc 1
2016-09-05T21:28:16.262513: step 10425, loss 0.0407878, acc 0.98
2016-09-05T21:28:17.079629: step 10426, loss 0.0167307, acc 1
2016-09-05T21:28:17.875856: step 10427, loss 0.039529, acc 0.98
2016-09-05T21:28:18.708516: step 10428, loss 0.0140438, acc 1
2016-09-05T21:28:19.527073: step 10429, loss 0.0286052, acc 1
2016-09-05T21:28:20.339081: step 10430, loss 0.00275919, acc 1
2016-09-05T21:28:21.189397: step 10431, loss 0.0213153, acc 0.98
2016-09-05T21:28:21.986090: step 10432, loss 0.00583612, acc 1
2016-09-05T21:28:22.793397: step 10433, loss 0.0179899, acc 1
2016-09-05T21:28:23.626194: step 10434, loss 0.0321032, acc 0.98
2016-09-05T21:28:24.432797: step 10435, loss 0.0310036, acc 0.98
2016-09-05T21:28:25.266909: step 10436, loss 0.03477, acc 0.98
2016-09-05T21:28:26.080259: step 10437, loss 0.0297091, acc 0.98
2016-09-05T21:28:26.913513: step 10438, loss 0.0028791, acc 1
2016-09-05T21:28:27.709303: step 10439, loss 0.00299827, acc 1
2016-09-05T21:28:28.497256: step 10440, loss 0.0216521, acc 1
2016-09-05T21:28:29.314764: step 10441, loss 0.0321271, acc 0.98
2016-09-05T21:28:30.115639: step 10442, loss 0.0145579, acc 1
2016-09-05T21:28:30.903216: step 10443, loss 0.00333338, acc 1
2016-09-05T21:28:31.736934: step 10444, loss 0.00408105, acc 1
2016-09-05T21:28:32.537766: step 10445, loss 0.00273019, acc 1
2016-09-05T21:28:33.335705: step 10446, loss 0.00500339, acc 1
2016-09-05T21:28:34.198021: step 10447, loss 0.0276409, acc 0.98
2016-09-05T21:28:34.968005: step 10448, loss 0.00943818, acc 1
2016-09-05T21:28:35.754839: step 10449, loss 0.00271328, acc 1
2016-09-05T21:28:36.576493: step 10450, loss 0.00938373, acc 1
2016-09-05T21:28:37.361398: step 10451, loss 0.0137164, acc 1
2016-09-05T21:28:38.161573: step 10452, loss 0.00582487, acc 1
2016-09-05T21:28:38.969314: step 10453, loss 0.0898051, acc 0.98
2016-09-05T21:28:39.759763: step 10454, loss 0.00669695, acc 1
2016-09-05T21:28:40.580110: step 10455, loss 0.00539243, acc 1
2016-09-05T21:28:41.388751: step 10456, loss 0.00453236, acc 1
2016-09-05T21:28:42.181902: step 10457, loss 0.0583605, acc 0.96
2016-09-05T21:28:42.986524: step 10458, loss 0.0195155, acc 1
2016-09-05T21:28:43.820188: step 10459, loss 0.0229862, acc 0.98
2016-09-05T21:28:44.604852: step 10460, loss 0.00261493, acc 1
2016-09-05T21:28:45.419142: step 10461, loss 0.016393, acc 1
2016-09-05T21:28:46.223679: step 10462, loss 0.0165961, acc 0.98
2016-09-05T21:28:47.017720: step 10463, loss 0.0347384, acc 0.98
2016-09-05T21:28:47.835643: step 10464, loss 0.00454238, acc 1
2016-09-05T21:28:48.663819: step 10465, loss 0.00749984, acc 1
2016-09-05T21:28:49.465249: step 10466, loss 0.0148883, acc 1
2016-09-05T21:28:50.265679: step 10467, loss 0.00671968, acc 1
2016-09-05T21:28:51.099016: step 10468, loss 0.0131172, acc 1
2016-09-05T21:28:51.916522: step 10469, loss 0.0371213, acc 0.98
2016-09-05T21:28:52.741776: step 10470, loss 0.00318134, acc 1
2016-09-05T21:28:53.545568: step 10471, loss 0.0155842, acc 1
2016-09-05T21:28:54.313004: step 10472, loss 0.0166623, acc 1
2016-09-05T21:28:55.132362: step 10473, loss 0.0402932, acc 0.98
2016-09-05T21:28:55.976708: step 10474, loss 0.017013, acc 1
2016-09-05T21:28:56.778156: step 10475, loss 0.00643696, acc 1
2016-09-05T21:28:57.200985: step 10476, loss 0.00264421, acc 1
2016-09-05T21:28:58.026065: step 10477, loss 0.0205887, acc 1
2016-09-05T21:28:58.815252: step 10478, loss 0.00920593, acc 1
2016-09-05T21:28:59.616106: step 10479, loss 0.0428693, acc 0.96
2016-09-05T21:29:00.459852: step 10480, loss 0.00291384, acc 1
2016-09-05T21:29:01.255467: step 10481, loss 0.0119997, acc 1
2016-09-05T21:29:02.075454: step 10482, loss 0.0190286, acc 1
2016-09-05T21:29:02.884032: step 10483, loss 0.0068828, acc 1
2016-09-05T21:29:03.716671: step 10484, loss 0.00478865, acc 1
2016-09-05T21:29:04.560922: step 10485, loss 0.00415392, acc 1
2016-09-05T21:29:05.363275: step 10486, loss 0.00319366, acc 1
2016-09-05T21:29:06.187721: step 10487, loss 0.00871352, acc 1
2016-09-05T21:29:07.058048: step 10488, loss 0.0131094, acc 1
2016-09-05T21:29:07.863020: step 10489, loss 0.0170529, acc 0.98
2016-09-05T21:29:08.658213: step 10490, loss 0.0563474, acc 0.98
2016-09-05T21:29:09.480724: step 10491, loss 0.00772344, acc 1
2016-09-05T21:29:10.306955: step 10492, loss 0.0241056, acc 0.98
2016-09-05T21:29:11.119034: step 10493, loss 0.0972187, acc 0.98
2016-09-05T21:29:11.983372: step 10494, loss 0.00379229, acc 1
2016-09-05T21:29:12.813030: step 10495, loss 0.0171912, acc 1
2016-09-05T21:29:13.573302: step 10496, loss 0.0195211, acc 0.98
2016-09-05T21:29:14.405022: step 10497, loss 0.0262034, acc 0.98
2016-09-05T21:29:15.195151: step 10498, loss 0.0398458, acc 0.96
2016-09-05T21:29:16.022005: step 10499, loss 0.00316589, acc 1
2016-09-05T21:29:16.861180: step 10500, loss 0.00683046, acc 1

Evaluation:
2016-09-05T21:29:20.363191: step 10500, loss 2.18605, acc 0.744

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-10500

2016-09-05T21:29:22.205459: step 10501, loss 0.0140222, acc 1
2016-09-05T21:29:23.022107: step 10502, loss 0.00396027, acc 1
2016-09-05T21:29:23.863981: step 10503, loss 0.00727828, acc 1
2016-09-05T21:29:24.683400: step 10504, loss 0.00349472, acc 1
2016-09-05T21:29:25.499650: step 10505, loss 0.00364121, acc 1
2016-09-05T21:29:26.341917: step 10506, loss 0.00466685, acc 1
2016-09-05T21:29:27.160329: step 10507, loss 0.0250507, acc 0.98
2016-09-05T21:29:28.013603: step 10508, loss 0.0810062, acc 0.96
2016-09-05T21:29:28.840206: step 10509, loss 0.0548392, acc 0.96
2016-09-05T21:29:29.665757: step 10510, loss 0.0265657, acc 1
2016-09-05T21:29:30.458332: step 10511, loss 0.0427736, acc 0.98
2016-09-05T21:29:31.297639: step 10512, loss 0.0112543, acc 1
2016-09-05T21:29:32.121937: step 10513, loss 0.0123195, acc 1
2016-09-05T21:29:32.942164: step 10514, loss 0.0494876, acc 0.98
2016-09-05T21:29:33.767936: step 10515, loss 0.00277418, acc 1
2016-09-05T21:29:34.564777: step 10516, loss 0.00221026, acc 1
2016-09-05T21:29:35.363184: step 10517, loss 0.015596, acc 1
2016-09-05T21:29:36.185446: step 10518, loss 0.0653665, acc 0.98
2016-09-05T21:29:36.996394: step 10519, loss 0.00536063, acc 1
2016-09-05T21:29:37.836524: step 10520, loss 0.0168759, acc 0.98
2016-09-05T21:29:38.656354: step 10521, loss 0.0255064, acc 0.98
2016-09-05T21:29:39.495720: step 10522, loss 0.0359234, acc 0.96
2016-09-05T21:29:40.255326: step 10523, loss 0.0112987, acc 1
2016-09-05T21:29:41.050618: step 10524, loss 0.0348017, acc 0.98
2016-09-05T21:29:41.868716: step 10525, loss 0.00272424, acc 1
2016-09-05T21:29:42.657001: step 10526, loss 0.021241, acc 0.98
2016-09-05T21:29:43.472881: step 10527, loss 0.0211362, acc 0.98
2016-09-05T21:29:44.312838: step 10528, loss 0.0032817, acc 1
2016-09-05T21:29:45.121068: step 10529, loss 0.00243137, acc 1
2016-09-05T21:29:45.911411: step 10530, loss 0.0248756, acc 1
2016-09-05T21:29:46.724387: step 10531, loss 0.0335441, acc 0.98
2016-09-05T21:29:47.527806: step 10532, loss 0.0205931, acc 0.98
2016-09-05T21:29:48.321200: step 10533, loss 0.0349977, acc 0.98
2016-09-05T21:29:49.142698: step 10534, loss 0.00341482, acc 1
2016-09-05T21:29:49.932862: step 10535, loss 0.0141915, acc 1
2016-09-05T21:29:50.723178: step 10536, loss 0.0226039, acc 0.98
2016-09-05T21:29:51.532822: step 10537, loss 0.0166996, acc 1
2016-09-05T21:29:52.314546: step 10538, loss 0.0235093, acc 0.98
2016-09-05T21:29:53.125126: step 10539, loss 0.0320417, acc 0.98
2016-09-05T21:29:53.933805: step 10540, loss 0.0197607, acc 1
2016-09-05T21:29:54.730374: step 10541, loss 0.0459355, acc 0.96
2016-09-05T21:29:55.553344: step 10542, loss 0.038252, acc 0.98
2016-09-05T21:29:56.365422: step 10543, loss 0.0159531, acc 1
2016-09-05T21:29:57.135613: step 10544, loss 0.139495, acc 0.96
2016-09-05T21:29:57.948659: step 10545, loss 0.0249081, acc 0.98
2016-09-05T21:29:58.794852: step 10546, loss 0.0326983, acc 1
2016-09-05T21:29:59.584327: step 10547, loss 0.00179121, acc 1
2016-09-05T21:30:00.415966: step 10548, loss 0.00430462, acc 1
2016-09-05T21:30:01.217880: step 10549, loss 0.00325411, acc 1
2016-09-05T21:30:01.982199: step 10550, loss 0.0309006, acc 0.98
2016-09-05T21:30:02.781713: step 10551, loss 0.0213064, acc 1
2016-09-05T21:30:03.593310: step 10552, loss 0.0430753, acc 0.96
2016-09-05T21:30:04.429551: step 10553, loss 0.04652, acc 0.98
2016-09-05T21:30:05.237384: step 10554, loss 0.00418301, acc 1
2016-09-05T21:30:06.052665: step 10555, loss 0.0174652, acc 1
2016-09-05T21:30:06.826434: step 10556, loss 0.0062211, acc 1
2016-09-05T21:30:07.617691: step 10557, loss 0.00287554, acc 1
2016-09-05T21:30:08.433834: step 10558, loss 0.00521922, acc 1
2016-09-05T21:30:09.260367: step 10559, loss 0.0189427, acc 0.98
2016-09-05T21:30:10.048893: step 10560, loss 0.0176479, acc 0.98
2016-09-05T21:30:10.850682: step 10561, loss 0.00198095, acc 1
2016-09-05T21:30:11.633742: step 10562, loss 0.0225822, acc 0.98
2016-09-05T21:30:12.473317: step 10563, loss 0.00450007, acc 1
2016-09-05T21:30:13.285446: step 10564, loss 0.0469339, acc 0.98
2016-09-05T21:30:14.037951: step 10565, loss 0.0128595, acc 1
2016-09-05T21:30:14.849568: step 10566, loss 0.0274592, acc 0.98
2016-09-05T21:30:15.678946: step 10567, loss 0.0314805, acc 0.96
2016-09-05T21:30:16.468043: step 10568, loss 0.00719035, acc 1
2016-09-05T21:30:17.272608: step 10569, loss 0.00468277, acc 1
2016-09-05T21:30:18.087831: step 10570, loss 0.0218442, acc 0.98
2016-09-05T21:30:18.881178: step 10571, loss 0.0255982, acc 0.98
2016-09-05T21:30:19.683267: step 10572, loss 0.0310701, acc 0.98
2016-09-05T21:30:20.507188: step 10573, loss 0.0186832, acc 0.98
2016-09-05T21:30:21.298818: step 10574, loss 0.0326406, acc 1
2016-09-05T21:30:22.087827: step 10575, loss 0.0180292, acc 1
2016-09-05T21:30:22.915579: step 10576, loss 0.0528343, acc 0.98
2016-09-05T21:30:23.723466: step 10577, loss 0.14942, acc 0.98
2016-09-05T21:30:24.517164: step 10578, loss 0.0280334, acc 1
2016-09-05T21:30:25.344159: step 10579, loss 0.0583439, acc 0.94
2016-09-05T21:30:26.140646: step 10580, loss 0.00236172, acc 1
2016-09-05T21:30:26.954496: step 10581, loss 0.0175711, acc 0.98
2016-09-05T21:30:27.763493: step 10582, loss 0.0360974, acc 0.98
2016-09-05T21:30:28.572236: step 10583, loss 0.00906136, acc 1
2016-09-05T21:30:29.380477: step 10584, loss 0.00917217, acc 1
2016-09-05T21:30:30.230926: step 10585, loss 0.0158623, acc 1
2016-09-05T21:30:31.009790: step 10586, loss 0.0241707, acc 1
2016-09-05T21:30:31.820570: step 10587, loss 0.0303187, acc 0.98
2016-09-05T21:30:32.636997: step 10588, loss 0.0071992, acc 1
2016-09-05T21:30:33.425987: step 10589, loss 0.0212627, acc 1
2016-09-05T21:30:34.228092: step 10590, loss 0.0283949, acc 0.98
2016-09-05T21:30:35.045185: step 10591, loss 0.00483759, acc 1
2016-09-05T21:30:35.827737: step 10592, loss 0.0148214, acc 1
2016-09-05T21:30:36.632793: step 10593, loss 0.181405, acc 0.98
2016-09-05T21:30:37.459155: step 10594, loss 0.021444, acc 0.98
2016-09-05T21:30:38.242945: step 10595, loss 0.0209154, acc 0.98
2016-09-05T21:30:39.047442: step 10596, loss 0.038959, acc 0.98
2016-09-05T21:30:39.897027: step 10597, loss 0.0212061, acc 1
2016-09-05T21:30:40.694234: step 10598, loss 0.0234958, acc 1
2016-09-05T21:30:41.483245: step 10599, loss 0.00248049, acc 1
2016-09-05T21:30:42.311943: step 10600, loss 0.00330666, acc 1

Evaluation:
2016-09-05T21:30:45.780026: step 10600, loss 2.38788, acc 0.727

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-10600

2016-09-05T21:30:47.747219: step 10601, loss 0.0156737, acc 1
2016-09-05T21:30:48.559571: step 10602, loss 0.0213107, acc 0.98
2016-09-05T21:30:49.344588: step 10603, loss 0.00501693, acc 1
2016-09-05T21:30:50.169744: step 10604, loss 0.0171124, acc 1
2016-09-05T21:30:51.004613: step 10605, loss 0.00257358, acc 1
2016-09-05T21:30:51.830606: step 10606, loss 0.0272583, acc 1
2016-09-05T21:30:52.644614: step 10607, loss 0.0175229, acc 1
2016-09-05T21:30:53.497149: step 10608, loss 0.0272136, acc 0.98
2016-09-05T21:30:54.317426: step 10609, loss 0.00302313, acc 1
2016-09-05T21:30:55.152468: step 10610, loss 0.0185753, acc 0.98
2016-09-05T21:30:56.002577: step 10611, loss 0.0142217, acc 1
2016-09-05T21:30:56.800401: step 10612, loss 0.00764258, acc 1
2016-09-05T21:30:57.613413: step 10613, loss 0.0216853, acc 0.98
2016-09-05T21:30:58.442582: step 10614, loss 0.00270293, acc 1
2016-09-05T21:30:59.259623: step 10615, loss 0.00265641, acc 1
2016-09-05T21:31:00.084813: step 10616, loss 0.0254444, acc 1
2016-09-05T21:31:00.946852: step 10617, loss 0.00309845, acc 1
2016-09-05T21:31:01.753213: step 10618, loss 0.0277503, acc 1
2016-09-05T21:31:02.525810: step 10619, loss 0.00283511, acc 1
2016-09-05T21:31:03.335286: step 10620, loss 0.00851179, acc 1
2016-09-05T21:31:04.172931: step 10621, loss 0.00406938, acc 1
2016-09-05T21:31:04.943889: step 10622, loss 0.0171945, acc 1
2016-09-05T21:31:05.766215: step 10623, loss 0.0249217, acc 0.98
2016-09-05T21:31:06.583072: step 10624, loss 0.00274273, acc 1
2016-09-05T21:31:07.355118: step 10625, loss 0.0176438, acc 1
2016-09-05T21:31:08.154532: step 10626, loss 0.0149195, acc 1
2016-09-05T21:31:08.977811: step 10627, loss 0.00642882, acc 1
2016-09-05T21:31:09.768571: step 10628, loss 0.043847, acc 0.96
2016-09-05T21:31:10.562268: step 10629, loss 0.0168525, acc 1
2016-09-05T21:31:11.394860: step 10630, loss 0.00367448, acc 1
2016-09-05T21:31:12.168007: step 10631, loss 0.00306979, acc 1
2016-09-05T21:31:12.983064: step 10632, loss 0.00261814, acc 1
2016-09-05T21:31:13.825650: step 10633, loss 0.00261743, acc 1
2016-09-05T21:31:14.573128: step 10634, loss 0.00326001, acc 1
2016-09-05T21:31:15.397345: step 10635, loss 0.0192968, acc 0.98
2016-09-05T21:31:16.232920: step 10636, loss 0.00262736, acc 1
2016-09-05T21:31:17.016199: step 10637, loss 0.0262224, acc 0.98
2016-09-05T21:31:17.818352: step 10638, loss 0.00736771, acc 1
2016-09-05T21:31:18.640685: step 10639, loss 0.00933599, acc 1
2016-09-05T21:31:19.426578: step 10640, loss 0.00529618, acc 1
2016-09-05T21:31:20.253261: step 10641, loss 0.00246538, acc 1
2016-09-05T21:31:21.068342: step 10642, loss 0.014637, acc 1
2016-09-05T21:31:21.853788: step 10643, loss 0.0025241, acc 1
2016-09-05T21:31:22.651725: step 10644, loss 0.0122911, acc 1
2016-09-05T21:31:23.460957: step 10645, loss 0.0280582, acc 1
2016-09-05T21:31:24.244806: step 10646, loss 0.0132316, acc 1
2016-09-05T21:31:25.058646: step 10647, loss 0.0130566, acc 1
2016-09-05T21:31:25.869807: step 10648, loss 0.00460238, acc 1
2016-09-05T21:31:26.675284: step 10649, loss 0.0262211, acc 1
2016-09-05T21:31:27.508522: step 10650, loss 0.0126131, acc 1
2016-09-05T21:31:28.329967: step 10651, loss 0.0189378, acc 1
2016-09-05T21:31:29.118695: step 10652, loss 0.00388436, acc 1
2016-09-05T21:31:29.915016: step 10653, loss 0.00255164, acc 1
2016-09-05T21:31:30.754509: step 10654, loss 0.00244552, acc 1
2016-09-05T21:31:31.545841: step 10655, loss 0.00597455, acc 1
2016-09-05T21:31:32.328062: step 10656, loss 0.0263495, acc 0.98
2016-09-05T21:31:33.126762: step 10657, loss 0.0273263, acc 0.98
2016-09-05T21:31:33.902856: step 10658, loss 0.0024659, acc 1
2016-09-05T21:31:34.695973: step 10659, loss 0.0282903, acc 0.98
2016-09-05T21:31:35.528077: step 10660, loss 0.00250564, acc 1
2016-09-05T21:31:36.343272: step 10661, loss 0.00275416, acc 1
2016-09-05T21:31:37.156770: step 10662, loss 0.00268139, acc 1
2016-09-05T21:31:37.999218: step 10663, loss 0.0197602, acc 1
2016-09-05T21:31:38.799283: step 10664, loss 0.00239198, acc 1
2016-09-05T21:31:39.608340: step 10665, loss 0.0311812, acc 0.98
2016-09-05T21:31:40.415544: step 10666, loss 0.0165, acc 1
2016-09-05T21:31:41.173600: step 10667, loss 0.0332902, acc 0.98
2016-09-05T21:31:42.009436: step 10668, loss 0.0196125, acc 1
2016-09-05T21:31:42.830962: step 10669, loss 0.0023994, acc 1
2016-09-05T21:31:43.228881: step 10670, loss 0.00234533, acc 1
2016-09-05T21:31:44.049373: step 10671, loss 0.00446382, acc 1
2016-09-05T21:31:44.842407: step 10672, loss 0.0331004, acc 0.96
2016-09-05T21:31:45.644786: step 10673, loss 0.0264529, acc 1
2016-09-05T21:31:46.464538: step 10674, loss 0.0320472, acc 0.98
2016-09-05T21:31:47.241654: step 10675, loss 0.00239084, acc 1
2016-09-05T21:31:48.026931: step 10676, loss 0.0271563, acc 1
2016-09-05T21:31:48.828012: step 10677, loss 0.00281771, acc 1
2016-09-05T21:31:49.602212: step 10678, loss 0.0367908, acc 0.96
2016-09-05T21:31:50.458361: step 10679, loss 0.00613147, acc 1
2016-09-05T21:31:51.281410: step 10680, loss 0.00447574, acc 1
2016-09-05T21:31:52.073929: step 10681, loss 0.0025171, acc 1
2016-09-05T21:31:52.861553: step 10682, loss 0.00995459, acc 1
2016-09-05T21:31:53.680518: step 10683, loss 0.0371956, acc 1
2016-09-05T21:31:54.505938: step 10684, loss 0.0295448, acc 0.98
2016-09-05T21:31:55.331297: step 10685, loss 0.0060682, acc 1
2016-09-05T21:31:56.140744: step 10686, loss 0.0162616, acc 1
2016-09-05T21:31:56.907644: step 10687, loss 0.00345852, acc 1
2016-09-05T21:31:57.709377: step 10688, loss 0.0268076, acc 1
2016-09-05T21:31:58.525717: step 10689, loss 0.0208889, acc 1
2016-09-05T21:31:59.333921: step 10690, loss 0.00272138, acc 1
2016-09-05T21:32:00.154474: step 10691, loss 0.00388478, acc 1
2016-09-05T21:32:01.027419: step 10692, loss 0.00744695, acc 1
2016-09-05T21:32:01.833802: step 10693, loss 0.00612685, acc 1
2016-09-05T21:32:02.635372: step 10694, loss 0.00333353, acc 1
2016-09-05T21:32:03.481598: step 10695, loss 0.0118683, acc 1
2016-09-05T21:32:04.291219: step 10696, loss 0.00261443, acc 1
2016-09-05T21:32:05.073998: step 10697, loss 0.00239841, acc 1
2016-09-05T21:32:05.874063: step 10698, loss 0.00327007, acc 1
2016-09-05T21:32:06.696062: step 10699, loss 0.0213515, acc 0.98
2016-09-05T21:32:07.534412: step 10700, loss 0.0191817, acc 0.98

Evaluation:
2016-09-05T21:32:11.070031: step 10700, loss 2.6926, acc 0.737

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-10700

2016-09-05T21:32:12.926978: step 10701, loss 0.0455392, acc 0.98
2016-09-05T21:32:13.727727: step 10702, loss 0.0211107, acc 0.98
2016-09-05T21:32:14.578484: step 10703, loss 0.0462133, acc 0.98
2016-09-05T21:32:15.369312: step 10704, loss 0.010928, acc 1
2016-09-05T21:32:16.150426: step 10705, loss 0.0146574, acc 1
2016-09-05T21:32:16.998604: step 10706, loss 0.00232121, acc 1
2016-09-05T21:32:17.809561: step 10707, loss 0.0180894, acc 1
2016-09-05T21:32:18.598723: step 10708, loss 0.0519642, acc 0.98
2016-09-05T21:32:19.432785: step 10709, loss 0.0104571, acc 1
2016-09-05T21:32:20.225267: step 10710, loss 0.00225401, acc 1
2016-09-05T21:32:21.034013: step 10711, loss 0.0378565, acc 0.98
2016-09-05T21:32:21.854139: step 10712, loss 0.00260223, acc 1
2016-09-05T21:32:22.673863: step 10713, loss 0.0232108, acc 0.98
2016-09-05T21:32:23.448673: step 10714, loss 0.00631687, acc 1
2016-09-05T21:32:24.246722: step 10715, loss 0.0226742, acc 0.98
2016-09-05T21:32:25.075706: step 10716, loss 0.00887735, acc 1
2016-09-05T21:32:25.856442: step 10717, loss 0.00243859, acc 1
2016-09-05T21:32:26.682793: step 10718, loss 0.0244196, acc 1
2016-09-05T21:32:27.506512: step 10719, loss 0.00553659, acc 1
2016-09-05T21:32:28.297922: step 10720, loss 0.00234917, acc 1
2016-09-05T21:32:29.109319: step 10721, loss 0.015597, acc 1
2016-09-05T21:32:29.918299: step 10722, loss 0.00594827, acc 1
2016-09-05T21:32:30.725375: step 10723, loss 0.00442563, acc 1
2016-09-05T21:32:31.529780: step 10724, loss 0.0229884, acc 0.98
2016-09-05T21:32:32.372062: step 10725, loss 0.0069195, acc 1
2016-09-05T21:32:33.172239: step 10726, loss 0.00552395, acc 1
2016-09-05T21:32:33.963082: step 10727, loss 0.0115874, acc 1
2016-09-05T21:32:34.811641: step 10728, loss 0.00285181, acc 1
2016-09-05T21:32:35.579554: step 10729, loss 0.00270369, acc 1
2016-09-05T21:32:36.400539: step 10730, loss 0.0421768, acc 0.98
2016-09-05T21:32:37.245200: step 10731, loss 0.0238833, acc 0.98
2016-09-05T21:32:38.053000: step 10732, loss 0.00250349, acc 1
2016-09-05T21:32:38.854170: step 10733, loss 0.0399718, acc 0.96
2016-09-05T21:32:39.654563: step 10734, loss 0.100438, acc 0.96
2016-09-05T21:32:40.456381: step 10735, loss 0.0202373, acc 1
2016-09-05T21:32:41.272323: step 10736, loss 0.0199772, acc 1
2016-09-05T21:32:42.078998: step 10737, loss 0.00899171, acc 1
2016-09-05T21:32:42.875024: step 10738, loss 0.00188431, acc 1
2016-09-05T21:32:43.678170: step 10739, loss 0.048444, acc 0.98
2016-09-05T21:32:44.525278: step 10740, loss 0.0965803, acc 0.98
2016-09-05T21:32:45.356716: step 10741, loss 0.00435538, acc 1
2016-09-05T21:32:46.171510: step 10742, loss 0.00668965, acc 1
2016-09-05T21:32:46.987380: step 10743, loss 0.00180436, acc 1
2016-09-05T21:32:47.786452: step 10744, loss 0.00172408, acc 1
2016-09-05T21:32:48.586677: step 10745, loss 0.0477307, acc 0.96
2016-09-05T21:32:49.406885: step 10746, loss 0.0279538, acc 0.98
2016-09-05T21:32:50.201984: step 10747, loss 0.0190976, acc 1
2016-09-05T21:32:50.984836: step 10748, loss 0.00920712, acc 1
2016-09-05T21:32:51.817537: step 10749, loss 0.0227136, acc 0.98
2016-09-05T21:32:52.618879: step 10750, loss 0.00270705, acc 1
2016-09-05T21:32:53.415614: step 10751, loss 0.00458984, acc 1
2016-09-05T21:32:54.253527: step 10752, loss 0.00172286, acc 1
2016-09-05T21:32:55.039250: step 10753, loss 0.0463016, acc 0.98
2016-09-05T21:32:55.874134: step 10754, loss 0.0363903, acc 0.98
2016-09-05T21:32:56.690332: step 10755, loss 0.00755205, acc 1
2016-09-05T21:32:57.502282: step 10756, loss 0.00346067, acc 1
2016-09-05T21:32:58.312863: step 10757, loss 0.0125843, acc 1
2016-09-05T21:32:59.149182: step 10758, loss 0.0161658, acc 1
2016-09-05T21:32:59.939629: step 10759, loss 0.00710827, acc 1
2016-09-05T21:33:00.769314: step 10760, loss 0.0230239, acc 0.98
2016-09-05T21:33:01.629813: step 10761, loss 0.123358, acc 0.98
2016-09-05T21:33:02.438071: step 10762, loss 0.043413, acc 0.98
2016-09-05T21:33:03.232563: step 10763, loss 0.00197804, acc 1
2016-09-05T21:33:04.054262: step 10764, loss 0.00201312, acc 1
2016-09-05T21:33:04.876141: step 10765, loss 0.0255935, acc 1
2016-09-05T21:33:05.675637: step 10766, loss 0.00604869, acc 1
2016-09-05T21:33:06.525504: step 10767, loss 0.0111658, acc 1
2016-09-05T21:33:07.338543: step 10768, loss 0.0186126, acc 1
2016-09-05T21:33:08.170009: step 10769, loss 0.00621921, acc 1
2016-09-05T21:33:08.991007: step 10770, loss 0.0135089, acc 1
2016-09-05T21:33:09.798334: step 10771, loss 0.0154339, acc 1
2016-09-05T21:33:10.604725: step 10772, loss 0.0124599, acc 1
2016-09-05T21:33:11.403471: step 10773, loss 0.017204, acc 0.98
2016-09-05T21:33:12.216099: step 10774, loss 0.00904432, acc 1
2016-09-05T21:33:13.000490: step 10775, loss 0.00367462, acc 1
2016-09-05T21:33:13.805234: step 10776, loss 0.0399093, acc 0.98
2016-09-05T21:33:14.639947: step 10777, loss 0.0132861, acc 1
2016-09-05T21:33:15.450091: step 10778, loss 0.00319961, acc 1
2016-09-05T21:33:16.304976: step 10779, loss 0.0149338, acc 1
2016-09-05T21:33:17.093343: step 10780, loss 0.0022215, acc 1
2016-09-05T21:33:17.875512: step 10781, loss 0.00213123, acc 1
2016-09-05T21:33:18.683040: step 10782, loss 0.0266792, acc 0.98
2016-09-05T21:33:19.515084: step 10783, loss 0.0145015, acc 1
2016-09-05T21:33:20.286818: step 10784, loss 0.0111666, acc 1
2016-09-05T21:33:21.081231: step 10785, loss 0.00333902, acc 1
2016-09-05T21:33:21.921404: step 10786, loss 0.0518026, acc 0.98
2016-09-05T21:33:22.714800: step 10787, loss 0.0187002, acc 1
2016-09-05T21:33:23.518691: step 10788, loss 0.0438178, acc 0.96
2016-09-05T21:33:24.333071: step 10789, loss 0.00219657, acc 1
2016-09-05T21:33:25.115443: step 10790, loss 0.0421518, acc 0.98
2016-09-05T21:33:25.906132: step 10791, loss 0.0184555, acc 0.98
2016-09-05T21:33:26.719827: step 10792, loss 0.0185983, acc 1
2016-09-05T21:33:27.508142: step 10793, loss 0.00422927, acc 1
2016-09-05T21:33:28.300623: step 10794, loss 0.00886713, acc 1
2016-09-05T21:33:29.116858: step 10795, loss 0.0164159, acc 1
2016-09-05T21:33:29.905408: step 10796, loss 0.0165519, acc 1
2016-09-05T21:33:30.692289: step 10797, loss 0.0396481, acc 0.98
2016-09-05T21:33:31.496704: step 10798, loss 0.012104, acc 1
2016-09-05T21:33:32.290147: step 10799, loss 0.00411514, acc 1
2016-09-05T21:33:33.121227: step 10800, loss 0.0275721, acc 0.98

Evaluation:
2016-09-05T21:33:36.657340: step 10800, loss 2.54698, acc 0.741

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-10800

2016-09-05T21:33:38.498110: step 10801, loss 0.0447586, acc 0.98
2016-09-05T21:33:39.315201: step 10802, loss 0.0117276, acc 1
2016-09-05T21:33:40.138615: step 10803, loss 0.00492334, acc 1
2016-09-05T21:33:40.961048: step 10804, loss 0.0365596, acc 0.98
2016-09-05T21:33:41.781642: step 10805, loss 0.0336264, acc 0.98
2016-09-05T21:33:42.617194: step 10806, loss 0.0196758, acc 0.98
2016-09-05T21:33:43.399160: step 10807, loss 0.0133786, acc 1
2016-09-05T21:33:44.190371: step 10808, loss 0.0066889, acc 1
2016-09-05T21:33:45.030124: step 10809, loss 0.0133429, acc 1
2016-09-05T21:33:45.834285: step 10810, loss 0.0142551, acc 1
2016-09-05T21:33:46.639466: step 10811, loss 0.0120353, acc 1
2016-09-05T21:33:47.468436: step 10812, loss 0.00351341, acc 1
2016-09-05T21:33:48.262058: step 10813, loss 0.0150841, acc 1
2016-09-05T21:33:49.063507: step 10814, loss 0.00832445, acc 1
2016-09-05T21:33:49.879538: step 10815, loss 0.00430507, acc 1
2016-09-05T21:33:50.671111: step 10816, loss 0.0117628, acc 1
2016-09-05T21:33:51.470920: step 10817, loss 0.0463342, acc 0.98
2016-09-05T21:33:52.299938: step 10818, loss 0.00506052, acc 1
2016-09-05T21:33:53.108478: step 10819, loss 0.00274407, acc 1
2016-09-05T21:33:53.912547: step 10820, loss 0.0060817, acc 1
2016-09-05T21:33:54.776454: step 10821, loss 0.0266251, acc 1
2016-09-05T21:33:55.577458: step 10822, loss 0.0183147, acc 0.98
2016-09-05T21:33:56.373801: step 10823, loss 0.0694012, acc 0.96
2016-09-05T21:33:57.222592: step 10824, loss 0.418836, acc 0.96
2016-09-05T21:33:58.027291: step 10825, loss 0.00587073, acc 1
2016-09-05T21:33:58.849135: step 10826, loss 0.00231759, acc 1
2016-09-05T21:33:59.687490: step 10827, loss 0.00226616, acc 1
2016-09-05T21:34:00.495987: step 10828, loss 0.00786198, acc 1
2016-09-05T21:34:01.327321: step 10829, loss 0.00220237, acc 1
2016-09-05T21:34:02.158653: step 10830, loss 0.0323675, acc 0.98
2016-09-05T21:34:02.959433: step 10831, loss 0.00268252, acc 1
2016-09-05T21:34:03.759635: step 10832, loss 0.0373998, acc 0.98
2016-09-05T21:34:04.588816: step 10833, loss 0.0208598, acc 0.98
2016-09-05T21:34:05.421712: step 10834, loss 0.00222058, acc 1
2016-09-05T21:34:06.241902: step 10835, loss 0.0385396, acc 0.98
2016-09-05T21:34:07.087158: step 10836, loss 0.0259032, acc 1
2016-09-05T21:34:07.889338: step 10837, loss 0.009516, acc 1
2016-09-05T21:34:08.665443: step 10838, loss 0.00538218, acc 1
2016-09-05T21:34:09.491627: step 10839, loss 0.0201843, acc 1
2016-09-05T21:34:10.312093: step 10840, loss 0.0290667, acc 0.98
2016-09-05T21:34:11.120519: step 10841, loss 0.00503673, acc 1
2016-09-05T21:34:11.944446: step 10842, loss 0.0333012, acc 0.98
2016-09-05T21:34:12.750444: step 10843, loss 0.0314841, acc 0.98
2016-09-05T21:34:13.549545: step 10844, loss 0.00596988, acc 1
2016-09-05T21:34:14.376477: step 10845, loss 0.121584, acc 0.98
2016-09-05T21:34:15.175683: step 10846, loss 0.0077928, acc 1
2016-09-05T21:34:15.985739: step 10847, loss 0.00637443, acc 1
2016-09-05T21:34:16.782950: step 10848, loss 0.00248355, acc 1
2016-09-05T21:34:17.595235: step 10849, loss 0.035338, acc 0.98
2016-09-05T21:34:18.387696: step 10850, loss 0.0274195, acc 0.98
2016-09-05T21:34:19.225683: step 10851, loss 0.0334611, acc 0.98
2016-09-05T21:34:20.040624: step 10852, loss 0.0121767, acc 1
2016-09-05T21:34:20.836904: step 10853, loss 0.00532424, acc 1
2016-09-05T21:34:21.699528: step 10854, loss 0.00270072, acc 1
2016-09-05T21:34:22.517191: step 10855, loss 0.00334712, acc 1
2016-09-05T21:34:23.309304: step 10856, loss 0.0054246, acc 1
2016-09-05T21:34:24.120969: step 10857, loss 0.0230854, acc 0.98
2016-09-05T21:34:24.962730: step 10858, loss 0.0182182, acc 0.98
2016-09-05T21:34:25.738792: step 10859, loss 0.0585438, acc 0.98
2016-09-05T21:34:26.565263: step 10860, loss 0.033361, acc 0.98
2016-09-05T21:34:27.398714: step 10861, loss 0.0213479, acc 1
2016-09-05T21:34:28.205647: step 10862, loss 0.00965509, acc 1
2016-09-05T21:34:29.030869: step 10863, loss 0.0113293, acc 1
2016-09-05T21:34:29.459460: step 10864, loss 0.0392083, acc 1
2016-09-05T21:34:30.275044: step 10865, loss 0.0251048, acc 1
2016-09-05T21:34:31.151951: step 10866, loss 0.0168422, acc 0.98
2016-09-05T21:34:31.957429: step 10867, loss 0.0096891, acc 1
2016-09-05T21:34:32.761903: step 10868, loss 0.011553, acc 1
2016-09-05T21:34:33.585880: step 10869, loss 0.0036153, acc 1
2016-09-05T21:34:34.395510: step 10870, loss 0.00814305, acc 1
2016-09-05T21:34:35.215124: step 10871, loss 0.0208092, acc 0.98
2016-09-05T21:34:36.038142: step 10872, loss 0.0654617, acc 0.98
2016-09-05T21:34:36.863872: step 10873, loss 0.0262899, acc 0.98
2016-09-05T21:34:37.694792: step 10874, loss 0.0114101, acc 1
2016-09-05T21:34:38.508834: step 10875, loss 0.0179495, acc 0.98
2016-09-05T21:34:39.327167: step 10876, loss 0.00419168, acc 1
2016-09-05T21:34:40.130139: step 10877, loss 0.0380091, acc 0.98
2016-09-05T21:34:40.992086: step 10878, loss 0.059629, acc 0.98
2016-09-05T21:34:41.800929: step 10879, loss 0.00961359, acc 1
2016-09-05T21:34:42.605828: step 10880, loss 0.00262613, acc 1
2016-09-05T21:34:43.459653: step 10881, loss 0.0232655, acc 0.98
2016-09-05T21:34:44.290675: step 10882, loss 0.025435, acc 0.98
2016-09-05T21:34:45.069139: step 10883, loss 0.0221345, acc 0.98
2016-09-05T21:34:45.897146: step 10884, loss 0.00361712, acc 1
2016-09-05T21:34:46.718741: step 10885, loss 0.00951412, acc 1
2016-09-05T21:34:47.526702: step 10886, loss 0.0125878, acc 1
2016-09-05T21:34:48.346578: step 10887, loss 0.00576667, acc 1
2016-09-05T21:34:49.170091: step 10888, loss 0.00264162, acc 1
2016-09-05T21:34:49.964918: step 10889, loss 0.0138984, acc 1
2016-09-05T21:34:50.790623: step 10890, loss 0.057686, acc 0.98
2016-09-05T21:34:51.624980: step 10891, loss 0.00259458, acc 1
2016-09-05T21:34:52.419592: step 10892, loss 0.00889073, acc 1
2016-09-05T21:34:53.220734: step 10893, loss 0.0202784, acc 1
2016-09-05T21:34:54.024474: step 10894, loss 0.0210673, acc 0.98
2016-09-05T21:34:54.854511: step 10895, loss 0.0213762, acc 0.98
2016-09-05T21:34:55.651788: step 10896, loss 0.0385954, acc 0.98
2016-09-05T21:34:56.466801: step 10897, loss 0.0200255, acc 0.98
2016-09-05T21:34:57.258966: step 10898, loss 0.0192496, acc 0.98
2016-09-05T21:34:58.035827: step 10899, loss 0.00297993, acc 1
2016-09-05T21:34:58.834638: step 10900, loss 0.013539, acc 1

Evaluation:
2016-09-05T21:35:02.363520: step 10900, loss 2.48296, acc 0.73

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-10900

2016-09-05T21:35:04.231046: step 10901, loss 0.0154285, acc 1
2016-09-05T21:35:05.065947: step 10902, loss 0.0139884, acc 1
2016-09-05T21:35:05.880302: step 10903, loss 0.00285416, acc 1
2016-09-05T21:35:06.727562: step 10904, loss 0.00297845, acc 1
2016-09-05T21:35:07.533808: step 10905, loss 0.0248984, acc 0.98
2016-09-05T21:35:08.334032: step 10906, loss 0.0173763, acc 1
2016-09-05T21:35:09.142467: step 10907, loss 0.083105, acc 0.98
2016-09-05T21:35:09.943320: step 10908, loss 0.0117402, acc 1
2016-09-05T21:35:10.771722: step 10909, loss 0.0126923, acc 1
2016-09-05T21:35:11.567317: step 10910, loss 0.00984936, acc 1
2016-09-05T21:35:12.386292: step 10911, loss 0.0221898, acc 1
2016-09-05T21:35:13.189741: step 10912, loss 0.01919, acc 0.98
2016-09-05T21:35:13.977402: step 10913, loss 0.0028216, acc 1
2016-09-05T21:35:14.825320: step 10914, loss 0.0132465, acc 1
2016-09-05T21:35:15.648763: step 10915, loss 0.0321179, acc 0.98
2016-09-05T21:35:16.459688: step 10916, loss 0.00278982, acc 1
2016-09-05T21:35:17.283859: step 10917, loss 0.00445361, acc 1
2016-09-05T21:35:18.102830: step 10918, loss 0.0261047, acc 1
2016-09-05T21:35:18.902796: step 10919, loss 0.0137553, acc 1
2016-09-05T21:35:19.748028: step 10920, loss 0.00706224, acc 1
2016-09-05T21:35:20.557368: step 10921, loss 0.0129248, acc 1
2016-09-05T21:35:21.377574: step 10922, loss 0.0135115, acc 1
2016-09-05T21:35:22.213512: step 10923, loss 0.0595741, acc 0.98
2016-09-05T21:35:23.048890: step 10924, loss 0.00654461, acc 1
2016-09-05T21:35:23.835749: step 10925, loss 0.0271715, acc 0.98
2016-09-05T21:35:24.642799: step 10926, loss 0.0214959, acc 0.98
2016-09-05T21:35:25.490508: step 10927, loss 0.0247526, acc 0.98
2016-09-05T21:35:26.300967: step 10928, loss 0.0108758, acc 1
2016-09-05T21:35:27.105040: step 10929, loss 0.015362, acc 1
2016-09-05T21:35:27.953844: step 10930, loss 0.0132829, acc 1
2016-09-05T21:35:28.764946: step 10931, loss 0.00406673, acc 1
2016-09-05T21:35:29.590210: step 10932, loss 0.0043441, acc 1
2016-09-05T21:35:30.439282: step 10933, loss 0.00655955, acc 1
2016-09-05T21:35:31.232887: step 10934, loss 0.0141601, acc 1
2016-09-05T21:35:32.037307: step 10935, loss 0.00271235, acc 1
2016-09-05T21:35:32.850933: step 10936, loss 0.00360616, acc 1
2016-09-05T21:35:33.655959: step 10937, loss 0.00274059, acc 1
2016-09-05T21:35:34.460613: step 10938, loss 0.00274561, acc 1
2016-09-05T21:35:35.317757: step 10939, loss 0.0091931, acc 1
2016-09-05T21:35:36.142020: step 10940, loss 0.024443, acc 1
2016-09-05T21:35:36.930816: step 10941, loss 0.00296716, acc 1
2016-09-05T21:35:37.778680: step 10942, loss 0.00480685, acc 1
2016-09-05T21:35:38.578084: step 10943, loss 0.0412529, acc 0.98
2016-09-05T21:35:39.383949: step 10944, loss 0.006888, acc 1
2016-09-05T21:35:40.201293: step 10945, loss 0.0198825, acc 1
2016-09-05T21:35:41.008030: step 10946, loss 0.00259829, acc 1
2016-09-05T21:35:41.832567: step 10947, loss 0.0715294, acc 0.98
2016-09-05T21:35:42.646736: step 10948, loss 0.00296866, acc 1
2016-09-05T21:35:43.447850: step 10949, loss 0.00876901, acc 1
2016-09-05T21:35:44.281093: step 10950, loss 0.0204759, acc 1
2016-09-05T21:35:45.116368: step 10951, loss 0.00235703, acc 1
2016-09-05T21:35:45.919602: step 10952, loss 0.0281655, acc 0.98
2016-09-05T21:35:46.731613: step 10953, loss 0.0400447, acc 0.98
2016-09-05T21:35:47.548395: step 10954, loss 0.0430147, acc 0.98
2016-09-05T21:35:48.352837: step 10955, loss 0.0130905, acc 1
2016-09-05T21:35:49.172777: step 10956, loss 0.00451666, acc 1
2016-09-05T21:35:50.003693: step 10957, loss 0.00528185, acc 1
2016-09-05T21:35:50.818403: step 10958, loss 0.145531, acc 0.96
2016-09-05T21:35:51.622845: step 10959, loss 0.00567123, acc 1
2016-09-05T21:35:52.426724: step 10960, loss 0.00254097, acc 1
2016-09-05T21:35:53.245276: step 10961, loss 0.0176419, acc 0.98
2016-09-05T21:35:54.051320: step 10962, loss 0.0314472, acc 1
2016-09-05T21:35:54.871049: step 10963, loss 0.0345284, acc 0.98
2016-09-05T21:35:55.667354: step 10964, loss 0.0458655, acc 0.98
2016-09-05T21:35:56.452540: step 10965, loss 0.0160876, acc 1
2016-09-05T21:35:57.273904: step 10966, loss 0.0294119, acc 0.98
2016-09-05T21:35:58.071605: step 10967, loss 0.0238873, acc 0.98
2016-09-05T21:35:58.861753: step 10968, loss 0.0104117, acc 1
2016-09-05T21:35:59.668808: step 10969, loss 0.0078938, acc 1
2016-09-05T21:36:00.539827: step 10970, loss 0.0770782, acc 0.98
2016-09-05T21:36:01.352653: step 10971, loss 0.00414283, acc 1
2016-09-05T21:36:02.151297: step 10972, loss 0.0356058, acc 0.98
2016-09-05T21:36:02.983262: step 10973, loss 0.0190376, acc 1
2016-09-05T21:36:03.790132: step 10974, loss 0.00986763, acc 1
2016-09-05T21:36:04.590445: step 10975, loss 0.08847, acc 0.96
2016-09-05T21:36:05.403717: step 10976, loss 0.00360431, acc 1
2016-09-05T21:36:06.201490: step 10977, loss 0.0059914, acc 1
2016-09-05T21:36:07.007815: step 10978, loss 0.0332871, acc 0.98
2016-09-05T21:36:07.829773: step 10979, loss 0.00283028, acc 1
2016-09-05T21:36:08.609052: step 10980, loss 0.0139566, acc 1
2016-09-05T21:36:09.395592: step 10981, loss 0.0670843, acc 0.96
2016-09-05T21:36:10.201015: step 10982, loss 0.0192959, acc 1
2016-09-05T21:36:10.953548: step 10983, loss 0.0065776, acc 1
2016-09-05T21:36:11.772173: step 10984, loss 0.0346237, acc 0.98
2016-09-05T21:36:12.594187: step 10985, loss 0.0185975, acc 1
2016-09-05T21:36:13.383515: step 10986, loss 0.0304049, acc 0.98
2016-09-05T21:36:14.193937: step 10987, loss 0.0337134, acc 0.98
2016-09-05T21:36:15.013402: step 10988, loss 0.00270907, acc 1
2016-09-05T21:36:15.793503: step 10989, loss 0.0263186, acc 0.98
2016-09-05T21:36:16.602009: step 10990, loss 0.0127029, acc 1
2016-09-05T21:36:17.424369: step 10991, loss 0.0212803, acc 0.98
2016-09-05T21:36:18.211160: step 10992, loss 0.00875324, acc 1
2016-09-05T21:36:19.024274: step 10993, loss 0.00474786, acc 1
2016-09-05T21:36:19.863774: step 10994, loss 0.0308922, acc 0.96
2016-09-05T21:36:20.641756: step 10995, loss 0.0142238, acc 1
2016-09-05T21:36:21.480016: step 10996, loss 0.0286079, acc 0.98
2016-09-05T21:36:22.283332: step 10997, loss 0.00293869, acc 1
2016-09-05T21:36:23.056048: step 10998, loss 0.0159217, acc 1
2016-09-05T21:36:23.850985: step 10999, loss 0.0422077, acc 0.98
2016-09-05T21:36:24.660180: step 11000, loss 0.010081, acc 1

Evaluation:
2016-09-05T21:36:28.112838: step 11000, loss 2.07514, acc 0.735

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-11000

2016-09-05T21:36:30.038656: step 11001, loss 0.00879292, acc 1
2016-09-05T21:36:30.873883: step 11002, loss 0.00238201, acc 1
2016-09-05T21:36:31.657744: step 11003, loss 0.00250128, acc 1
2016-09-05T21:36:32.468997: step 11004, loss 0.0617286, acc 0.98
2016-09-05T21:36:33.290714: step 11005, loss 0.00496455, acc 1
2016-09-05T21:36:34.120967: step 11006, loss 0.0204787, acc 0.98
2016-09-05T21:36:34.920555: step 11007, loss 0.0144818, acc 1
2016-09-05T21:36:35.745286: step 11008, loss 0.00422035, acc 1
2016-09-05T21:36:36.562905: step 11009, loss 0.011709, acc 1
2016-09-05T21:36:37.397453: step 11010, loss 0.00642037, acc 1
2016-09-05T21:36:38.245443: step 11011, loss 0.0672541, acc 0.98
2016-09-05T21:36:39.094536: step 11012, loss 0.00423423, acc 1
2016-09-05T21:36:39.909750: step 11013, loss 0.0858971, acc 0.98
2016-09-05T21:36:40.749510: step 11014, loss 0.231638, acc 0.96
2016-09-05T21:36:41.557473: step 11015, loss 0.0075853, acc 1
2016-09-05T21:36:42.385363: step 11016, loss 0.0187734, acc 1
2016-09-05T21:36:43.197189: step 11017, loss 0.0372619, acc 1
2016-09-05T21:36:43.997126: step 11018, loss 0.0351594, acc 1
2016-09-05T21:36:44.822799: step 11019, loss 0.031313, acc 0.98
2016-09-05T21:36:45.669443: step 11020, loss 0.00238242, acc 1
2016-09-05T21:36:46.496574: step 11021, loss 0.00223362, acc 1
2016-09-05T21:36:47.271295: step 11022, loss 0.0115185, acc 1
2016-09-05T21:36:48.071322: step 11023, loss 0.00357076, acc 1
2016-09-05T21:36:48.885159: step 11024, loss 0.0154383, acc 1
2016-09-05T21:36:49.673130: step 11025, loss 0.0454663, acc 0.98
2016-09-05T21:36:50.465438: step 11026, loss 0.00823266, acc 1
2016-09-05T21:36:51.269398: step 11027, loss 0.00514228, acc 1
2016-09-05T21:36:52.081827: step 11028, loss 0.0529294, acc 0.98
2016-09-05T21:36:52.884198: step 11029, loss 0.0326402, acc 0.98
2016-09-05T21:36:53.686163: step 11030, loss 0.0465966, acc 0.98
2016-09-05T21:36:54.474567: step 11031, loss 0.00327388, acc 1
2016-09-05T21:36:55.302490: step 11032, loss 0.0350896, acc 0.98
2016-09-05T21:36:56.140542: step 11033, loss 0.0111195, acc 1
2016-09-05T21:36:56.957406: step 11034, loss 0.0332283, acc 1
2016-09-05T21:36:57.773289: step 11035, loss 0.02418, acc 0.98
2016-09-05T21:36:58.581946: step 11036, loss 0.0249867, acc 0.98
2016-09-05T21:36:59.365884: step 11037, loss 0.0286684, acc 0.98
2016-09-05T21:37:00.187561: step 11038, loss 0.0184461, acc 1
2016-09-05T21:37:01.009322: step 11039, loss 0.0221843, acc 1
2016-09-05T21:37:01.776139: step 11040, loss 0.010268, acc 1
2016-09-05T21:37:02.609206: step 11041, loss 0.0168445, acc 0.98
2016-09-05T21:37:03.391940: step 11042, loss 0.00213956, acc 1
2016-09-05T21:37:04.176038: step 11043, loss 0.00222469, acc 1
2016-09-05T21:37:04.983808: step 11044, loss 0.0203238, acc 1
2016-09-05T21:37:05.820416: step 11045, loss 0.0252309, acc 1
2016-09-05T21:37:06.606186: step 11046, loss 0.0251683, acc 0.98
2016-09-05T21:37:07.411734: step 11047, loss 0.0165246, acc 0.98
2016-09-05T21:37:08.269439: step 11048, loss 0.0172458, acc 1
2016-09-05T21:37:09.050436: step 11049, loss 0.0111993, acc 1
2016-09-05T21:37:09.820820: step 11050, loss 0.0150087, acc 1
2016-09-05T21:37:10.657125: step 11051, loss 0.068557, acc 0.96
2016-09-05T21:37:11.430819: step 11052, loss 0.00253803, acc 1
2016-09-05T21:37:12.224308: step 11053, loss 0.0179079, acc 1
2016-09-05T21:37:13.032675: step 11054, loss 0.0412014, acc 0.98
2016-09-05T21:37:13.812213: step 11055, loss 0.0239684, acc 1
2016-09-05T21:37:14.640229: step 11056, loss 0.031424, acc 0.98
2016-09-05T21:37:15.455072: step 11057, loss 0.0222847, acc 0.98
2016-09-05T21:37:15.854393: step 11058, loss 0.0167232, acc 1
2016-09-05T21:37:16.689404: step 11059, loss 0.0344522, acc 0.96
2016-09-05T21:37:17.499825: step 11060, loss 0.0610059, acc 0.98
2016-09-05T21:37:18.298276: step 11061, loss 0.00914647, acc 1
2016-09-05T21:37:19.142145: step 11062, loss 0.0568306, acc 0.98
2016-09-05T21:37:19.897721: step 11063, loss 0.0436442, acc 0.98
2016-09-05T21:37:20.696841: step 11064, loss 0.0159442, acc 1
2016-09-05T21:37:21.512008: step 11065, loss 0.0152842, acc 1
2016-09-05T21:37:22.313099: step 11066, loss 0.043554, acc 0.98
2016-09-05T21:37:23.122978: step 11067, loss 0.0155067, acc 1
2016-09-05T21:37:23.932198: step 11068, loss 0.034203, acc 0.98
2016-09-05T21:37:24.719936: step 11069, loss 0.0521724, acc 0.98
2016-09-05T21:37:25.528697: step 11070, loss 0.0216438, acc 0.98
2016-09-05T21:37:26.352461: step 11071, loss 0.0231943, acc 1
2016-09-05T21:37:27.135161: step 11072, loss 0.00448999, acc 1
2016-09-05T21:37:27.949591: step 11073, loss 0.00378189, acc 1
2016-09-05T21:37:28.797870: step 11074, loss 0.0180185, acc 1
2016-09-05T21:37:29.613372: step 11075, loss 0.0135337, acc 1
2016-09-05T21:37:30.422262: step 11076, loss 0.0145364, acc 1
2016-09-05T21:37:31.270762: step 11077, loss 0.0384748, acc 0.98
2016-09-05T21:37:32.080300: step 11078, loss 0.0132438, acc 1
2016-09-05T21:37:32.867020: step 11079, loss 0.0085999, acc 1
2016-09-05T21:37:33.678288: step 11080, loss 0.0257412, acc 1
2016-09-05T21:37:34.484967: step 11081, loss 0.0219707, acc 0.98
2016-09-05T21:37:35.288626: step 11082, loss 0.00389647, acc 1
2016-09-05T21:37:36.112673: step 11083, loss 0.00473452, acc 1
2016-09-05T21:37:36.924612: step 11084, loss 0.00946163, acc 1
2016-09-05T21:37:37.755961: step 11085, loss 0.00406664, acc 1
2016-09-05T21:37:38.575735: step 11086, loss 0.00398307, acc 1
2016-09-05T21:37:39.384958: step 11087, loss 0.00400176, acc 1
2016-09-05T21:37:40.183747: step 11088, loss 0.0107009, acc 1
2016-09-05T21:37:41.010048: step 11089, loss 0.271206, acc 0.96
2016-09-05T21:37:41.822156: step 11090, loss 0.0872062, acc 0.92
2016-09-05T21:37:42.641206: step 11091, loss 0.0173072, acc 1
2016-09-05T21:37:43.465200: step 11092, loss 0.0500001, acc 0.98
2016-09-05T21:37:44.296798: step 11093, loss 0.00483113, acc 1
2016-09-05T21:37:45.106733: step 11094, loss 0.0112938, acc 1
2016-09-05T21:37:45.941829: step 11095, loss 0.0129024, acc 1
2016-09-05T21:37:46.829875: step 11096, loss 0.0193761, acc 1
2016-09-05T21:37:47.663412: step 11097, loss 0.00421453, acc 1
2016-09-05T21:37:48.479580: step 11098, loss 0.00362588, acc 1
2016-09-05T21:37:49.294450: step 11099, loss 0.0435485, acc 0.98
2016-09-05T21:37:50.085704: step 11100, loss 0.0088504, acc 1

Evaluation:
2016-09-05T21:37:53.589385: step 11100, loss 1.88606, acc 0.72

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-11100

2016-09-05T21:37:55.434229: step 11101, loss 0.0262536, acc 0.98
2016-09-05T21:37:56.252203: step 11102, loss 0.0117405, acc 1
2016-09-05T21:37:57.067540: step 11103, loss 0.0225249, acc 1
2016-09-05T21:37:57.898727: step 11104, loss 0.0333783, acc 0.98
2016-09-05T21:37:58.712268: step 11105, loss 0.0227295, acc 0.98
2016-09-05T21:37:59.524794: step 11106, loss 0.0672764, acc 0.98
2016-09-05T21:38:00.352204: step 11107, loss 0.0030427, acc 1
2016-09-05T21:38:01.129497: step 11108, loss 0.023106, acc 0.98
2016-09-05T21:38:01.958446: step 11109, loss 0.00434097, acc 1
2016-09-05T21:38:02.790571: step 11110, loss 0.00848157, acc 1
2016-09-05T21:38:03.586649: step 11111, loss 0.0408351, acc 0.98
2016-09-05T21:38:04.391470: step 11112, loss 0.0353114, acc 0.98
2016-09-05T21:38:05.202984: step 11113, loss 0.00925311, acc 1
2016-09-05T21:38:06.008165: step 11114, loss 0.00774445, acc 1
2016-09-05T21:38:06.849515: step 11115, loss 0.0208023, acc 0.98
2016-09-05T21:38:07.704692: step 11116, loss 0.0204132, acc 0.98
2016-09-05T21:38:08.489131: step 11117, loss 0.00372184, acc 1
2016-09-05T21:38:09.286991: step 11118, loss 0.0656818, acc 0.96
2016-09-05T21:38:10.140338: step 11119, loss 0.00986102, acc 1
2016-09-05T21:38:10.939516: step 11120, loss 0.00300247, acc 1
2016-09-05T21:38:11.739210: step 11121, loss 0.0135614, acc 1
2016-09-05T21:38:12.561832: step 11122, loss 0.0123469, acc 1
2016-09-05T21:38:13.363599: step 11123, loss 0.0354678, acc 0.98
2016-09-05T21:38:14.173811: step 11124, loss 0.0135059, acc 1
2016-09-05T21:38:15.021141: step 11125, loss 0.0242714, acc 1
2016-09-05T21:38:15.861473: step 11126, loss 0.026978, acc 0.98
2016-09-05T21:38:16.682484: step 11127, loss 0.00650087, acc 1
2016-09-05T21:38:17.534407: step 11128, loss 0.0126758, acc 1
2016-09-05T21:38:18.369409: step 11129, loss 0.0189195, acc 0.98
2016-09-05T21:38:19.162367: step 11130, loss 0.0244971, acc 0.98
2016-09-05T21:38:19.954855: step 11131, loss 0.00457301, acc 1
2016-09-05T21:38:20.793296: step 11132, loss 0.0202821, acc 0.98
2016-09-05T21:38:21.568209: step 11133, loss 0.0446196, acc 0.98
2016-09-05T21:38:22.383718: step 11134, loss 0.0437542, acc 0.98
2016-09-05T21:38:23.173224: step 11135, loss 0.0565251, acc 0.96
2016-09-05T21:38:23.965900: step 11136, loss 0.00252876, acc 1
2016-09-05T21:38:24.791489: step 11137, loss 0.0434655, acc 0.98
2016-09-05T21:38:25.637842: step 11138, loss 0.0297688, acc 0.98
2016-09-05T21:38:26.467886: step 11139, loss 0.0149142, acc 1
2016-09-05T21:38:27.277867: step 11140, loss 0.00579667, acc 1
2016-09-05T21:38:28.091052: step 11141, loss 0.0326938, acc 0.98
2016-09-05T21:38:28.897563: step 11142, loss 0.014295, acc 1
2016-09-05T21:38:29.712096: step 11143, loss 0.00501401, acc 1
2016-09-05T21:38:30.606197: step 11144, loss 0.00551717, acc 1
2016-09-05T21:38:31.413405: step 11145, loss 0.0183779, acc 0.98
2016-09-05T21:38:32.209406: step 11146, loss 0.0275323, acc 0.98
2016-09-05T21:38:33.045206: step 11147, loss 0.00836541, acc 1
2016-09-05T21:38:33.877972: step 11148, loss 0.0603436, acc 0.96
2016-09-05T21:38:34.705585: step 11149, loss 0.0606885, acc 0.96
2016-09-05T21:38:35.546523: step 11150, loss 0.0245031, acc 0.98
2016-09-05T21:38:36.373604: step 11151, loss 0.0181891, acc 0.98
2016-09-05T21:38:37.201457: step 11152, loss 0.00646877, acc 1
2016-09-05T21:38:38.033155: step 11153, loss 0.128236, acc 0.98
2016-09-05T21:38:38.811813: step 11154, loss 0.0216089, acc 0.98
2016-09-05T21:38:39.635807: step 11155, loss 0.0166767, acc 0.98
2016-09-05T21:38:40.430985: step 11156, loss 0.065957, acc 0.98
2016-09-05T21:38:41.249317: step 11157, loss 0.017061, acc 1
2016-09-05T21:38:42.074588: step 11158, loss 0.0151498, acc 1
2016-09-05T21:38:42.923441: step 11159, loss 0.0263596, acc 1
2016-09-05T21:38:43.757470: step 11160, loss 0.00744928, acc 1
2016-09-05T21:38:44.539484: step 11161, loss 0.00808339, acc 1
2016-09-05T21:38:45.353007: step 11162, loss 0.0572233, acc 0.98
2016-09-05T21:38:46.134702: step 11163, loss 0.0121085, acc 1
2016-09-05T21:38:46.937497: step 11164, loss 0.00353784, acc 1
2016-09-05T21:38:47.750080: step 11165, loss 0.0445378, acc 0.96
2016-09-05T21:38:48.569594: step 11166, loss 0.0192184, acc 0.98
2016-09-05T21:38:49.331544: step 11167, loss 0.0478381, acc 0.98
2016-09-05T21:38:50.168568: step 11168, loss 0.00596782, acc 1
2016-09-05T21:38:50.986669: step 11169, loss 0.0519744, acc 0.96
2016-09-05T21:38:51.793511: step 11170, loss 0.0114435, acc 1
2016-09-05T21:38:52.596431: step 11171, loss 0.0202187, acc 1
2016-09-05T21:38:53.418456: step 11172, loss 0.00885334, acc 1
2016-09-05T21:38:54.227277: step 11173, loss 0.0249928, acc 0.98
2016-09-05T21:38:55.037569: step 11174, loss 0.0724859, acc 0.96
2016-09-05T21:38:55.843742: step 11175, loss 0.0221513, acc 1
2016-09-05T21:38:56.645390: step 11176, loss 0.0182921, acc 1
2016-09-05T21:38:57.476683: step 11177, loss 0.00242229, acc 1
2016-09-05T21:38:58.265160: step 11178, loss 0.0199842, acc 1
2016-09-05T21:38:59.035518: step 11179, loss 0.0132252, acc 1
2016-09-05T21:38:59.849642: step 11180, loss 0.0251798, acc 0.98
2016-09-05T21:39:00.682739: step 11181, loss 0.00863493, acc 1
2016-09-05T21:39:01.488777: step 11182, loss 0.0268379, acc 0.98
2016-09-05T21:39:02.322562: step 11183, loss 0.0115195, acc 1
2016-09-05T21:39:03.109748: step 11184, loss 0.004008, acc 1
2016-09-05T21:39:03.919849: step 11185, loss 0.00306958, acc 1
2016-09-05T21:39:04.741408: step 11186, loss 0.00440948, acc 1
2016-09-05T21:39:05.563231: step 11187, loss 0.00286115, acc 1
2016-09-05T21:39:06.350579: step 11188, loss 0.0525819, acc 0.98
2016-09-05T21:39:07.155655: step 11189, loss 0.0195903, acc 1
2016-09-05T21:39:07.945716: step 11190, loss 0.0185001, acc 0.98
2016-09-05T21:39:08.744700: step 11191, loss 0.0176141, acc 0.98
2016-09-05T21:39:09.567608: step 11192, loss 0.0118259, acc 1
2016-09-05T21:39:10.383325: step 11193, loss 0.0116489, acc 1
2016-09-05T21:39:11.166327: step 11194, loss 0.0419155, acc 0.98
2016-09-05T21:39:11.983162: step 11195, loss 0.00829314, acc 1
2016-09-05T21:39:12.809441: step 11196, loss 0.00258465, acc 1
2016-09-05T21:39:13.614379: step 11197, loss 0.0514751, acc 0.98
2016-09-05T21:39:14.436571: step 11198, loss 0.0179296, acc 0.98
2016-09-05T21:39:15.234006: step 11199, loss 0.0249089, acc 0.98
2016-09-05T21:39:16.018228: step 11200, loss 0.00347404, acc 1

Evaluation:
2016-09-05T21:39:19.578586: step 11200, loss 2.34213, acc 0.726

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-11200

2016-09-05T21:39:21.553109: step 11201, loss 0.0045041, acc 1
2016-09-05T21:39:22.330888: step 11202, loss 0.00256902, acc 1
2016-09-05T21:39:23.151176: step 11203, loss 0.0553488, acc 0.94
2016-09-05T21:39:23.988648: step 11204, loss 0.012139, acc 1
2016-09-05T21:39:24.800808: step 11205, loss 0.0114256, acc 1
2016-09-05T21:39:25.605573: step 11206, loss 0.0278249, acc 0.98
2016-09-05T21:39:26.432416: step 11207, loss 0.00245096, acc 1
2016-09-05T21:39:27.237575: step 11208, loss 0.00241561, acc 1
2016-09-05T21:39:28.046307: step 11209, loss 0.0401511, acc 0.98
2016-09-05T21:39:28.866045: step 11210, loss 0.0448507, acc 0.98
2016-09-05T21:39:29.696422: step 11211, loss 0.0168021, acc 1
2016-09-05T21:39:30.505610: step 11212, loss 0.0253946, acc 0.98
2016-09-05T21:39:31.320280: step 11213, loss 0.01012, acc 1
2016-09-05T21:39:32.136372: step 11214, loss 0.00240538, acc 1
2016-09-05T21:39:32.935491: step 11215, loss 0.00293167, acc 1
2016-09-05T21:39:33.748269: step 11216, loss 0.00245109, acc 1
2016-09-05T21:39:34.562968: step 11217, loss 0.00864488, acc 1
2016-09-05T21:39:35.335762: step 11218, loss 0.00319544, acc 1
2016-09-05T21:39:36.161400: step 11219, loss 0.0145924, acc 1
2016-09-05T21:39:36.965456: step 11220, loss 0.0219935, acc 0.98
2016-09-05T21:39:37.780610: step 11221, loss 0.00508829, acc 1
2016-09-05T21:39:38.658404: step 11222, loss 0.165352, acc 0.98
2016-09-05T21:39:39.489103: step 11223, loss 0.0174837, acc 0.98
2016-09-05T21:39:40.268392: step 11224, loss 0.0294214, acc 0.98
2016-09-05T21:39:41.072188: step 11225, loss 0.00228236, acc 1
2016-09-05T21:39:41.897767: step 11226, loss 0.0239892, acc 0.98
2016-09-05T21:39:42.658524: step 11227, loss 0.0166526, acc 1
2016-09-05T21:39:43.460456: step 11228, loss 0.0337666, acc 0.98
2016-09-05T21:39:44.313882: step 11229, loss 0.0126904, acc 1
2016-09-05T21:39:45.093397: step 11230, loss 0.0036897, acc 1
2016-09-05T21:39:45.908665: step 11231, loss 0.00427068, acc 1
2016-09-05T21:39:46.738790: step 11232, loss 0.0133711, acc 1
2016-09-05T21:39:47.533430: step 11233, loss 0.00339983, acc 1
2016-09-05T21:39:48.316659: step 11234, loss 0.00295674, acc 1
2016-09-05T21:39:49.161635: step 11235, loss 0.00779415, acc 1
2016-09-05T21:39:49.965878: step 11236, loss 0.00349681, acc 1
2016-09-05T21:39:50.736964: step 11237, loss 0.0403697, acc 0.98
2016-09-05T21:39:51.546505: step 11238, loss 0.017607, acc 1
2016-09-05T21:39:52.341216: step 11239, loss 0.0153482, acc 1
2016-09-05T21:39:53.151103: step 11240, loss 0.00656097, acc 1
2016-09-05T21:39:53.990484: step 11241, loss 0.00316868, acc 1
2016-09-05T21:39:54.783006: step 11242, loss 0.002258, acc 1
2016-09-05T21:39:55.590847: step 11243, loss 0.0301693, acc 0.98
2016-09-05T21:39:56.425567: step 11244, loss 0.00222232, acc 1
2016-09-05T21:39:57.193824: step 11245, loss 0.0331331, acc 0.98
2016-09-05T21:39:57.958479: step 11246, loss 0.00229389, acc 1
2016-09-05T21:39:58.759542: step 11247, loss 0.0544182, acc 0.98
2016-09-05T21:39:59.563813: step 11248, loss 0.0200242, acc 1
2016-09-05T21:40:00.378826: step 11249, loss 0.0338634, acc 0.98
2016-09-05T21:40:01.202562: step 11250, loss 0.0168125, acc 0.98
2016-09-05T21:40:01.997667: step 11251, loss 0.00587559, acc 1
2016-09-05T21:40:02.415902: step 11252, loss 0.00881723, acc 1
2016-09-05T21:40:03.198671: step 11253, loss 0.0412403, acc 0.96
2016-09-05T21:40:04.001956: step 11254, loss 0.0120703, acc 1
2016-09-05T21:40:04.820920: step 11255, loss 0.153627, acc 0.96
2016-09-05T21:40:05.613981: step 11256, loss 0.0032054, acc 1
2016-09-05T21:40:06.406147: step 11257, loss 0.00259225, acc 1
2016-09-05T21:40:07.224832: step 11258, loss 0.0100516, acc 1
2016-09-05T21:40:08.036534: step 11259, loss 0.0169752, acc 1
2016-09-05T21:40:08.823576: step 11260, loss 0.00411028, acc 1
2016-09-05T21:40:09.633293: step 11261, loss 0.00224548, acc 1
2016-09-05T21:40:10.430109: step 11262, loss 0.0111145, acc 1
2016-09-05T21:40:11.211389: step 11263, loss 0.00865576, acc 1
2016-09-05T21:40:12.025360: step 11264, loss 0.031147, acc 0.98
2016-09-05T21:40:12.851976: step 11265, loss 0.0565257, acc 0.96
2016-09-05T21:40:13.639280: step 11266, loss 0.0342243, acc 0.98
2016-09-05T21:40:14.445190: step 11267, loss 0.0184942, acc 0.98
2016-09-05T21:40:15.268265: step 11268, loss 0.00263212, acc 1
2016-09-05T21:40:16.067935: step 11269, loss 0.00275477, acc 1
2016-09-05T21:40:16.871534: step 11270, loss 0.0104883, acc 1
2016-09-05T21:40:17.694580: step 11271, loss 0.0195164, acc 1
2016-09-05T21:40:18.521073: step 11272, loss 0.00617562, acc 1
2016-09-05T21:40:19.349773: step 11273, loss 0.00498192, acc 1
2016-09-05T21:40:20.149832: step 11274, loss 0.00417078, acc 1
2016-09-05T21:40:20.953401: step 11275, loss 0.00249954, acc 1
2016-09-05T21:40:21.768993: step 11276, loss 0.00373941, acc 1
2016-09-05T21:40:22.550427: step 11277, loss 0.0225683, acc 0.98
2016-09-05T21:40:23.386731: step 11278, loss 0.00224943, acc 1
2016-09-05T21:40:24.181402: step 11279, loss 0.0033552, acc 1
2016-09-05T21:40:24.978029: step 11280, loss 0.00237405, acc 1
2016-09-05T21:40:25.775105: step 11281, loss 0.0143105, acc 1
2016-09-05T21:40:26.622881: step 11282, loss 0.019624, acc 1
2016-09-05T21:40:27.413225: step 11283, loss 0.0173426, acc 1
2016-09-05T21:40:28.248968: step 11284, loss 0.0708821, acc 0.94
2016-09-05T21:40:29.084259: step 11285, loss 0.028439, acc 0.98
2016-09-05T21:40:29.893431: step 11286, loss 0.0155777, acc 1
2016-09-05T21:40:30.703122: step 11287, loss 0.0151434, acc 1
2016-09-05T21:40:31.550221: step 11288, loss 0.016696, acc 0.98
2016-09-05T21:40:32.356566: step 11289, loss 0.0286468, acc 0.98
2016-09-05T21:40:33.169298: step 11290, loss 0.0134829, acc 1
2016-09-05T21:40:34.004870: step 11291, loss 0.00384888, acc 1
2016-09-05T21:40:34.805419: step 11292, loss 0.00264742, acc 1
2016-09-05T21:40:35.609999: step 11293, loss 0.0151398, acc 1
2016-09-05T21:40:36.437080: step 11294, loss 0.00947463, acc 1
2016-09-05T21:40:37.252876: step 11295, loss 0.0155259, acc 1
2016-09-05T21:40:38.048616: step 11296, loss 0.0120598, acc 1
2016-09-05T21:40:38.890074: step 11297, loss 0.00236731, acc 1
2016-09-05T21:40:39.667605: step 11298, loss 0.0650382, acc 0.98
2016-09-05T21:40:40.490440: step 11299, loss 0.00208366, acc 1
2016-09-05T21:40:41.323501: step 11300, loss 0.0147795, acc 1

Evaluation:
2016-09-05T21:40:44.804239: step 11300, loss 2.16913, acc 0.733

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-11300

2016-09-05T21:40:46.847234: step 11301, loss 0.00487992, acc 1
2016-09-05T21:40:47.695957: step 11302, loss 0.0153312, acc 1
2016-09-05T21:40:48.516880: step 11303, loss 0.00230648, acc 1
2016-09-05T21:40:49.309378: step 11304, loss 0.00759928, acc 1
2016-09-05T21:40:50.118055: step 11305, loss 0.00483324, acc 1
2016-09-05T21:40:50.931823: step 11306, loss 0.00545552, acc 1
2016-09-05T21:40:51.757026: step 11307, loss 0.0532722, acc 0.96
2016-09-05T21:40:52.609837: step 11308, loss 0.00324071, acc 1
2016-09-05T21:40:53.457955: step 11309, loss 0.00214514, acc 1
2016-09-05T21:40:54.259397: step 11310, loss 0.00405538, acc 1
2016-09-05T21:40:55.083695: step 11311, loss 0.0173535, acc 0.98
2016-09-05T21:40:55.927633: step 11312, loss 0.00648853, acc 1
2016-09-05T21:40:56.757249: step 11313, loss 0.00253614, acc 1
2016-09-05T21:40:57.557899: step 11314, loss 0.00229399, acc 1
2016-09-05T21:40:58.353936: step 11315, loss 0.021312, acc 0.98
2016-09-05T21:40:59.150049: step 11316, loss 0.139887, acc 0.98
2016-09-05T21:40:59.949401: step 11317, loss 0.0362886, acc 0.98
2016-09-05T21:41:00.786887: step 11318, loss 0.0213024, acc 0.98
2016-09-05T21:41:01.591966: step 11319, loss 0.0226564, acc 0.98
2016-09-05T21:41:02.401461: step 11320, loss 0.0151658, acc 1
2016-09-05T21:41:03.224099: step 11321, loss 0.0205095, acc 1
2016-09-05T21:41:04.050051: step 11322, loss 0.0109369, acc 1
2016-09-05T21:41:04.840060: step 11323, loss 0.00183406, acc 1
2016-09-05T21:41:05.671083: step 11324, loss 0.0027501, acc 1
2016-09-05T21:41:06.497531: step 11325, loss 0.00205163, acc 1
2016-09-05T21:41:07.279173: step 11326, loss 0.0024362, acc 1
2016-09-05T21:41:08.106787: step 11327, loss 0.0111254, acc 1
2016-09-05T21:41:08.911005: step 11328, loss 0.0276808, acc 1
2016-09-05T21:41:09.704998: step 11329, loss 0.109021, acc 0.98
2016-09-05T21:41:10.522357: step 11330, loss 0.00320273, acc 1
2016-09-05T21:41:11.330951: step 11331, loss 0.0169402, acc 1
2016-09-05T21:41:12.131280: step 11332, loss 0.0136466, acc 1
2016-09-05T21:41:12.930140: step 11333, loss 0.0264849, acc 1
2016-09-05T21:41:13.754302: step 11334, loss 0.00428931, acc 1
2016-09-05T21:41:14.531189: step 11335, loss 0.0225537, acc 0.98
2016-09-05T21:41:15.339456: step 11336, loss 0.00896929, acc 1
2016-09-05T21:41:16.146052: step 11337, loss 0.00599522, acc 1
2016-09-05T21:41:16.946679: step 11338, loss 0.0232493, acc 1
2016-09-05T21:41:17.751980: step 11339, loss 0.0137707, acc 1
2016-09-05T21:41:18.605110: step 11340, loss 0.0394914, acc 0.98
2016-09-05T21:41:19.393383: step 11341, loss 0.00802793, acc 1
2016-09-05T21:41:20.192189: step 11342, loss 0.0410867, acc 0.98
2016-09-05T21:41:21.013069: step 11343, loss 0.00281358, acc 1
2016-09-05T21:41:21.817193: step 11344, loss 0.0157399, acc 1
2016-09-05T21:41:22.626301: step 11345, loss 0.00337525, acc 1
2016-09-05T21:41:23.450412: step 11346, loss 0.00468831, acc 1
2016-09-05T21:41:24.221938: step 11347, loss 0.0284007, acc 0.98
2016-09-05T21:41:25.020407: step 11348, loss 0.0520936, acc 0.98
2016-09-05T21:41:25.849446: step 11349, loss 0.190933, acc 0.98
2016-09-05T21:41:26.638337: step 11350, loss 0.0468416, acc 0.96
2016-09-05T21:41:27.439932: step 11351, loss 0.0155672, acc 1
2016-09-05T21:41:28.286722: step 11352, loss 0.0146976, acc 1
2016-09-05T21:41:29.081481: step 11353, loss 0.00600356, acc 1
2016-09-05T21:41:29.891185: step 11354, loss 0.00909054, acc 1
2016-09-05T21:41:30.707305: step 11355, loss 0.0345121, acc 0.98
2016-09-05T21:41:31.499035: step 11356, loss 0.00365469, acc 1
2016-09-05T21:41:32.292004: step 11357, loss 0.002078, acc 1
2016-09-05T21:41:33.090794: step 11358, loss 0.0283987, acc 0.98
2016-09-05T21:41:33.865691: step 11359, loss 0.0301203, acc 1
2016-09-05T21:41:34.680338: step 11360, loss 0.0323039, acc 0.98
2016-09-05T21:41:35.503755: step 11361, loss 0.0490865, acc 0.96
2016-09-05T21:41:36.287256: step 11362, loss 0.0143556, acc 1
2016-09-05T21:41:37.073687: step 11363, loss 0.088906, acc 0.96
2016-09-05T21:41:37.898494: step 11364, loss 0.0027808, acc 1
2016-09-05T21:41:38.703274: step 11365, loss 0.0244431, acc 0.98
2016-09-05T21:41:39.491024: step 11366, loss 0.0279481, acc 1
2016-09-05T21:41:40.312499: step 11367, loss 0.00230056, acc 1
2016-09-05T21:41:41.105407: step 11368, loss 0.0227738, acc 0.98
2016-09-05T21:41:41.945407: step 11369, loss 0.00774432, acc 1
2016-09-05T21:41:42.759413: step 11370, loss 0.0821437, acc 0.98
2016-09-05T21:41:43.591798: step 11371, loss 0.00258156, acc 1
2016-09-05T21:41:44.397897: step 11372, loss 0.0310765, acc 0.98
2016-09-05T21:41:45.210980: step 11373, loss 0.00306504, acc 1
2016-09-05T21:41:46.011865: step 11374, loss 0.0110633, acc 1
2016-09-05T21:41:46.789636: step 11375, loss 0.00980387, acc 1
2016-09-05T21:41:47.584539: step 11376, loss 0.0528154, acc 0.96
2016-09-05T21:41:48.385869: step 11377, loss 0.00798329, acc 1
2016-09-05T21:41:49.229608: step 11378, loss 0.0227173, acc 0.98
2016-09-05T21:41:50.040646: step 11379, loss 0.0261925, acc 1
2016-09-05T21:41:50.818055: step 11380, loss 0.00476564, acc 1
2016-09-05T21:41:51.586535: step 11381, loss 0.00347366, acc 1
2016-09-05T21:41:52.433403: step 11382, loss 0.00835428, acc 1
2016-09-05T21:41:53.228287: step 11383, loss 0.0292488, acc 1
2016-09-05T21:41:54.049532: step 11384, loss 0.0761758, acc 0.94
2016-09-05T21:41:54.867032: step 11385, loss 0.00266357, acc 1
2016-09-05T21:41:55.653392: step 11386, loss 0.016037, acc 1
2016-09-05T21:41:56.457493: step 11387, loss 0.0523329, acc 0.96
2016-09-05T21:41:57.267631: step 11388, loss 0.0475585, acc 0.98
2016-09-05T21:41:58.065812: step 11389, loss 0.00927155, acc 1
2016-09-05T21:41:58.853947: step 11390, loss 0.00260359, acc 1
2016-09-05T21:41:59.677432: step 11391, loss 0.0144165, acc 1
2016-09-05T21:42:00.511270: step 11392, loss 0.0416814, acc 0.96
2016-09-05T21:42:01.311994: step 11393, loss 0.0654336, acc 0.98
2016-09-05T21:42:02.128488: step 11394, loss 0.0406774, acc 0.98
2016-09-05T21:42:02.911864: step 11395, loss 0.0259473, acc 0.98
2016-09-05T21:42:03.722896: step 11396, loss 0.0105407, acc 1
2016-09-05T21:42:04.535153: step 11397, loss 0.00936576, acc 1
2016-09-05T21:42:05.293300: step 11398, loss 0.0255171, acc 0.98
2016-09-05T21:42:06.098973: step 11399, loss 0.0123294, acc 1
2016-09-05T21:42:06.940195: step 11400, loss 0.00482495, acc 1

Evaluation:
2016-09-05T21:42:10.451400: step 11400, loss 1.88016, acc 0.733

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-11400

2016-09-05T21:42:12.313014: step 11401, loss 0.0817387, acc 0.96
2016-09-05T21:42:13.142126: step 11402, loss 0.122686, acc 0.96
2016-09-05T21:42:13.956324: step 11403, loss 0.0106151, acc 1
2016-09-05T21:42:14.756850: step 11404, loss 0.0296969, acc 1
2016-09-05T21:42:15.572777: step 11405, loss 0.0170612, acc 0.98
2016-09-05T21:42:16.369739: step 11406, loss 0.0479172, acc 0.98
2016-09-05T21:42:17.181924: step 11407, loss 0.0379554, acc 0.98
2016-09-05T21:42:18.036486: step 11408, loss 0.00398127, acc 1
2016-09-05T21:42:18.857037: step 11409, loss 0.00302762, acc 1
2016-09-05T21:42:19.676266: step 11410, loss 0.0105889, acc 1
2016-09-05T21:42:20.553428: step 11411, loss 0.00249919, acc 1
2016-09-05T21:42:21.376935: step 11412, loss 0.0173084, acc 1
2016-09-05T21:42:22.176988: step 11413, loss 0.00544285, acc 1
2016-09-05T21:42:22.995501: step 11414, loss 0.0177981, acc 0.98
2016-09-05T21:42:23.802012: step 11415, loss 0.0180923, acc 1
2016-09-05T21:42:24.611009: step 11416, loss 0.00299962, acc 1
2016-09-05T21:42:25.464008: step 11417, loss 0.00258744, acc 1
2016-09-05T21:42:26.303214: step 11418, loss 0.0970832, acc 0.94
2016-09-05T21:42:27.089388: step 11419, loss 0.0468167, acc 0.98
2016-09-05T21:42:27.915918: step 11420, loss 0.0190867, acc 0.98
2016-09-05T21:42:28.756623: step 11421, loss 0.0151197, acc 1
2016-09-05T21:42:29.569990: step 11422, loss 0.0310003, acc 1
2016-09-05T21:42:30.355904: step 11423, loss 0.0253642, acc 0.98
2016-09-05T21:42:31.157893: step 11424, loss 0.0102528, acc 1
2016-09-05T21:42:31.951167: step 11425, loss 0.0201353, acc 0.98
2016-09-05T21:42:32.760558: step 11426, loss 0.00373623, acc 1
2016-09-05T21:42:33.561942: step 11427, loss 0.0239663, acc 1
2016-09-05T21:42:34.340360: step 11428, loss 0.0583536, acc 0.98
2016-09-05T21:42:35.111685: step 11429, loss 0.00809329, acc 1
2016-09-05T21:42:35.945426: step 11430, loss 0.00514067, acc 1
2016-09-05T21:42:36.727316: step 11431, loss 0.0190675, acc 0.98
2016-09-05T21:42:37.557417: step 11432, loss 0.0324178, acc 0.98
2016-09-05T21:42:38.352667: step 11433, loss 0.023975, acc 1
2016-09-05T21:42:39.155957: step 11434, loss 0.00312695, acc 1
2016-09-05T21:42:39.980203: step 11435, loss 0.0164579, acc 0.98
2016-09-05T21:42:40.802992: step 11436, loss 0.0150628, acc 1
2016-09-05T21:42:41.597565: step 11437, loss 0.0260694, acc 0.98
2016-09-05T21:42:42.396602: step 11438, loss 0.0102594, acc 1
2016-09-05T21:42:43.226216: step 11439, loss 0.0148987, acc 1
2016-09-05T21:42:44.019859: step 11440, loss 0.00227113, acc 1
2016-09-05T21:42:44.819620: step 11441, loss 0.00759926, acc 1
2016-09-05T21:42:45.641290: step 11442, loss 0.0151849, acc 1
2016-09-05T21:42:46.413575: step 11443, loss 0.00266574, acc 1
2016-09-05T21:42:47.198941: step 11444, loss 0.00816847, acc 1
2016-09-05T21:42:48.003489: step 11445, loss 0.00600917, acc 1
2016-09-05T21:42:48.414270: step 11446, loss 0.0116142, acc 1
2016-09-05T21:42:49.223897: step 11447, loss 0.0236565, acc 1
2016-09-05T21:42:50.004415: step 11448, loss 0.0205395, acc 0.98
2016-09-05T21:42:50.798063: step 11449, loss 0.0208597, acc 1
2016-09-05T21:42:51.625647: step 11450, loss 0.00273317, acc 1
2016-09-05T21:42:52.412165: step 11451, loss 0.0312321, acc 0.98
2016-09-05T21:42:53.215249: step 11452, loss 0.00350363, acc 1
2016-09-05T21:42:54.030970: step 11453, loss 0.0227928, acc 1
2016-09-05T21:42:54.837194: step 11454, loss 0.00252787, acc 1
2016-09-05T21:42:55.632599: step 11455, loss 0.0023888, acc 1
2016-09-05T21:42:56.442311: step 11456, loss 0.0128326, acc 1
2016-09-05T21:42:57.214130: step 11457, loss 0.00882868, acc 1
2016-09-05T21:42:58.033079: step 11458, loss 0.0283997, acc 0.98
2016-09-05T21:42:58.881142: step 11459, loss 0.0448794, acc 0.98
2016-09-05T21:42:59.664994: step 11460, loss 0.0208852, acc 1
2016-09-05T21:43:00.484315: step 11461, loss 0.0235011, acc 0.98
2016-09-05T21:43:01.308381: step 11462, loss 0.00247337, acc 1
2016-09-05T21:43:02.090262: step 11463, loss 0.00239267, acc 1
2016-09-05T21:43:02.938084: step 11464, loss 0.0111012, acc 1
2016-09-05T21:43:03.763710: step 11465, loss 0.00840447, acc 1
2016-09-05T21:43:04.535349: step 11466, loss 0.00303334, acc 1
2016-09-05T21:43:05.324030: step 11467, loss 0.0172309, acc 1
2016-09-05T21:43:06.130261: step 11468, loss 0.00656066, acc 1
2016-09-05T21:43:06.930349: step 11469, loss 0.0344034, acc 0.98
2016-09-05T21:43:07.716954: step 11470, loss 0.00535635, acc 1
2016-09-05T21:43:08.535454: step 11471, loss 0.00413879, acc 1
2016-09-05T21:43:09.337314: step 11472, loss 0.0028325, acc 1
2016-09-05T21:43:10.144660: step 11473, loss 0.00255944, acc 1
2016-09-05T21:43:10.999727: step 11474, loss 0.019453, acc 0.98
2016-09-05T21:43:11.818968: step 11475, loss 0.00880592, acc 1
2016-09-05T21:43:12.627272: step 11476, loss 0.0160279, acc 1
2016-09-05T21:43:13.447647: step 11477, loss 0.00245909, acc 1
2016-09-05T21:43:14.225213: step 11478, loss 0.0801235, acc 0.96
2016-09-05T21:43:15.034369: step 11479, loss 0.0327476, acc 0.98
2016-09-05T21:43:15.888810: step 11480, loss 0.00254834, acc 1
2016-09-05T21:43:16.689992: step 11481, loss 0.00221933, acc 1
2016-09-05T21:43:17.513291: step 11482, loss 0.072187, acc 0.98
2016-09-05T21:43:18.363290: step 11483, loss 0.00530792, acc 1
2016-09-05T21:43:19.194298: step 11484, loss 0.067565, acc 0.98
2016-09-05T21:43:19.986668: step 11485, loss 0.00605229, acc 1
2016-09-05T21:43:20.806067: step 11486, loss 0.00230306, acc 1
2016-09-05T21:43:21.621464: step 11487, loss 0.0218662, acc 1
2016-09-05T21:43:22.437665: step 11488, loss 0.0169559, acc 1
2016-09-05T21:43:23.271048: step 11489, loss 0.00548219, acc 1
2016-09-05T21:43:24.091778: step 11490, loss 0.0044287, acc 1
2016-09-05T21:43:24.877989: step 11491, loss 0.0133339, acc 1
2016-09-05T21:43:25.694647: step 11492, loss 0.0109522, acc 1
2016-09-05T21:43:26.505420: step 11493, loss 0.0086522, acc 1
2016-09-05T21:43:27.328128: step 11494, loss 0.0149111, acc 1
2016-09-05T21:43:28.141460: step 11495, loss 0.0081213, acc 1
2016-09-05T21:43:28.960581: step 11496, loss 0.0183709, acc 0.98
2016-09-05T21:43:29.808433: step 11497, loss 0.00366996, acc 1
2016-09-05T21:43:30.651753: step 11498, loss 0.00265625, acc 1
2016-09-05T21:43:31.459888: step 11499, loss 0.0171915, acc 1
2016-09-05T21:43:32.280377: step 11500, loss 0.0028318, acc 1

Evaluation:
2016-09-05T21:43:35.770109: step 11500, loss 2.3485, acc 0.731

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-11500

2016-09-05T21:43:37.658765: step 11501, loss 0.00521411, acc 1
2016-09-05T21:43:38.473775: step 11502, loss 0.0124561, acc 1
2016-09-05T21:43:39.295114: step 11503, loss 0.0229783, acc 0.98
2016-09-05T21:43:40.149999: step 11504, loss 0.00280342, acc 1
2016-09-05T21:43:40.959841: step 11505, loss 0.0236006, acc 0.98
2016-09-05T21:43:41.799644: step 11506, loss 0.0116785, acc 1
2016-09-05T21:43:42.638719: step 11507, loss 0.00825001, acc 1
2016-09-05T21:43:43.453453: step 11508, loss 0.0337906, acc 0.98
2016-09-05T21:43:44.268171: step 11509, loss 0.0242921, acc 1
2016-09-05T21:43:45.086308: step 11510, loss 0.0197525, acc 0.98
2016-09-05T21:43:45.927379: step 11511, loss 0.0126943, acc 1
2016-09-05T21:43:46.752573: step 11512, loss 0.00593442, acc 1
2016-09-05T21:43:47.599440: step 11513, loss 0.00328479, acc 1
2016-09-05T21:43:48.393589: step 11514, loss 0.0198165, acc 1
2016-09-05T21:43:49.215935: step 11515, loss 0.00490727, acc 1
2016-09-05T21:43:50.067283: step 11516, loss 0.0224403, acc 1
2016-09-05T21:43:50.872798: step 11517, loss 0.0123214, acc 1
2016-09-05T21:43:51.646344: step 11518, loss 0.0134589, acc 1
2016-09-05T21:43:52.467898: step 11519, loss 0.00289242, acc 1
2016-09-05T21:43:53.284957: step 11520, loss 0.00287795, acc 1
2016-09-05T21:43:54.070010: step 11521, loss 0.0184309, acc 0.98
2016-09-05T21:43:54.900818: step 11522, loss 0.0206011, acc 1
2016-09-05T21:43:55.708962: step 11523, loss 0.0171894, acc 0.98
2016-09-05T21:43:56.536257: step 11524, loss 0.0252746, acc 0.98
2016-09-05T21:43:57.334943: step 11525, loss 0.0163941, acc 1
2016-09-05T21:43:58.144921: step 11526, loss 0.00901123, acc 1
2016-09-05T21:43:58.949617: step 11527, loss 0.00286243, acc 1
2016-09-05T21:43:59.742450: step 11528, loss 0.00544212, acc 1
2016-09-05T21:44:00.589886: step 11529, loss 0.0784012, acc 0.98
2016-09-05T21:44:01.403573: step 11530, loss 0.00270534, acc 1
2016-09-05T21:44:02.205597: step 11531, loss 0.00549993, acc 1
2016-09-05T21:44:03.018372: step 11532, loss 0.00326676, acc 1
2016-09-05T21:44:03.809221: step 11533, loss 0.0179967, acc 1
2016-09-05T21:44:04.611505: step 11534, loss 0.0584493, acc 0.98
2016-09-05T21:44:05.424618: step 11535, loss 0.104275, acc 0.98
2016-09-05T21:44:06.211363: step 11536, loss 0.0518721, acc 0.98
2016-09-05T21:44:07.041199: step 11537, loss 0.0296458, acc 1
2016-09-05T21:44:07.884818: step 11538, loss 0.0384418, acc 0.96
2016-09-05T21:44:08.694510: step 11539, loss 0.00245214, acc 1
2016-09-05T21:44:09.491387: step 11540, loss 0.0139019, acc 1
2016-09-05T21:44:10.317988: step 11541, loss 0.0550428, acc 0.98
2016-09-05T21:44:11.116993: step 11542, loss 0.0171694, acc 0.98
2016-09-05T21:44:11.920874: step 11543, loss 0.0405935, acc 0.98
2016-09-05T21:44:12.745065: step 11544, loss 0.0268998, acc 0.98
2016-09-05T21:44:13.577429: step 11545, loss 0.00510845, acc 1
2016-09-05T21:44:14.402782: step 11546, loss 0.0566035, acc 0.96
2016-09-05T21:44:15.214279: step 11547, loss 0.00296689, acc 1
2016-09-05T21:44:16.018779: step 11548, loss 0.0105893, acc 1
2016-09-05T21:44:16.827349: step 11549, loss 0.032011, acc 1
2016-09-05T21:44:17.653821: step 11550, loss 0.015671, acc 1
2016-09-05T21:44:18.467638: step 11551, loss 0.00358508, acc 1
2016-09-05T21:44:19.279998: step 11552, loss 0.0105236, acc 1
2016-09-05T21:44:20.116913: step 11553, loss 0.0206832, acc 1
2016-09-05T21:44:20.946529: step 11554, loss 0.0235067, acc 1
2016-09-05T21:44:21.738818: step 11555, loss 0.0290537, acc 0.98
2016-09-05T21:44:22.555465: step 11556, loss 0.00215893, acc 1
2016-09-05T21:44:23.363957: step 11557, loss 0.0454189, acc 0.98
2016-09-05T21:44:24.158999: step 11558, loss 0.00199362, acc 1
2016-09-05T21:44:24.981369: step 11559, loss 0.0237689, acc 1
2016-09-05T21:44:25.761919: step 11560, loss 0.0200716, acc 0.98
2016-09-05T21:44:26.594420: step 11561, loss 0.00204948, acc 1
2016-09-05T21:44:27.414333: step 11562, loss 0.0126528, acc 1
2016-09-05T21:44:28.254993: step 11563, loss 0.0130673, acc 1
2016-09-05T21:44:29.090977: step 11564, loss 0.0281793, acc 1
2016-09-05T21:44:29.914106: step 11565, loss 0.0159807, acc 1
2016-09-05T21:44:30.727704: step 11566, loss 0.061573, acc 0.96
2016-09-05T21:44:31.533957: step 11567, loss 0.00809267, acc 1
2016-09-05T21:44:32.364678: step 11568, loss 0.0127447, acc 1
2016-09-05T21:44:33.182029: step 11569, loss 0.00221684, acc 1
2016-09-05T21:44:33.965712: step 11570, loss 0.00212191, acc 1
2016-09-05T21:44:34.799927: step 11571, loss 0.00974621, acc 1
2016-09-05T21:44:35.617731: step 11572, loss 0.00417767, acc 1
2016-09-05T21:44:36.433600: step 11573, loss 0.0197947, acc 0.98
2016-09-05T21:44:37.245271: step 11574, loss 0.0495659, acc 0.96
2016-09-05T21:44:38.048896: step 11575, loss 0.0272781, acc 0.98
2016-09-05T21:44:38.828402: step 11576, loss 0.0174204, acc 1
2016-09-05T21:44:39.618244: step 11577, loss 0.0446061, acc 0.98
2016-09-05T21:44:40.430081: step 11578, loss 0.0798203, acc 0.98
2016-09-05T21:44:41.232778: step 11579, loss 0.00221366, acc 1
2016-09-05T21:44:42.025079: step 11580, loss 0.0228303, acc 0.98
2016-09-05T21:44:42.855622: step 11581, loss 0.0645439, acc 0.94
2016-09-05T21:44:43.654761: step 11582, loss 0.018618, acc 1
2016-09-05T21:44:44.458122: step 11583, loss 0.138149, acc 0.98
2016-09-05T21:44:45.249032: step 11584, loss 0.0021316, acc 1
2016-09-05T21:44:46.053377: step 11585, loss 0.0290002, acc 0.98
2016-09-05T21:44:46.858457: step 11586, loss 0.0076836, acc 1
2016-09-05T21:44:47.691374: step 11587, loss 0.00511532, acc 1
2016-09-05T21:44:48.491365: step 11588, loss 0.0137434, acc 1
2016-09-05T21:44:49.292414: step 11589, loss 0.00819255, acc 1
2016-09-05T21:44:50.103879: step 11590, loss 0.0131712, acc 1
2016-09-05T21:44:50.897543: step 11591, loss 0.00986131, acc 1
2016-09-05T21:44:51.723310: step 11592, loss 0.0698802, acc 0.98
2016-09-05T21:44:52.543780: step 11593, loss 0.0286608, acc 0.98
2016-09-05T21:44:53.336031: step 11594, loss 0.0361026, acc 0.98
2016-09-05T21:44:54.129610: step 11595, loss 0.00754612, acc 1
2016-09-05T21:44:54.965426: step 11596, loss 0.00514266, acc 1
2016-09-05T21:44:55.792125: step 11597, loss 0.0392537, acc 0.98
2016-09-05T21:44:56.574562: step 11598, loss 0.0140694, acc 1
2016-09-05T21:44:57.374040: step 11599, loss 0.0227598, acc 1
2016-09-05T21:44:58.173790: step 11600, loss 0.0148829, acc 1

Evaluation:
2016-09-05T21:45:01.718148: step 11600, loss 2.05427, acc 0.732

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-11600

2016-09-05T21:45:03.622408: step 11601, loss 0.00373836, acc 1
2016-09-05T21:45:04.441968: step 11602, loss 0.00334872, acc 1
2016-09-05T21:45:05.250038: step 11603, loss 0.00398954, acc 1
2016-09-05T21:45:06.071522: step 11604, loss 0.0109804, acc 1
2016-09-05T21:45:06.883043: step 11605, loss 0.00233327, acc 1
2016-09-05T21:45:07.690488: step 11606, loss 0.0164178, acc 1
2016-09-05T21:45:08.543555: step 11607, loss 0.00269108, acc 1
2016-09-05T21:45:09.361983: step 11608, loss 0.0163168, acc 1
2016-09-05T21:45:10.166208: step 11609, loss 0.0164212, acc 0.98
2016-09-05T21:45:10.993771: step 11610, loss 0.0257901, acc 0.98
2016-09-05T21:45:11.837469: step 11611, loss 0.0261341, acc 0.98
2016-09-05T21:45:12.621631: step 11612, loss 0.0178771, acc 0.98
2016-09-05T21:45:13.464803: step 11613, loss 0.00250616, acc 1
2016-09-05T21:45:14.261833: step 11614, loss 0.00319113, acc 1
2016-09-05T21:45:15.084676: step 11615, loss 0.0193014, acc 1
2016-09-05T21:45:15.917554: step 11616, loss 0.0900344, acc 0.98
2016-09-05T21:45:16.758936: step 11617, loss 0.0241179, acc 0.98
2016-09-05T21:45:17.529683: step 11618, loss 0.0108861, acc 1
2016-09-05T21:45:18.360792: step 11619, loss 0.0173295, acc 1
2016-09-05T21:45:19.206054: step 11620, loss 0.0368101, acc 0.98
2016-09-05T21:45:19.965795: step 11621, loss 0.0237719, acc 0.98
2016-09-05T21:45:20.773948: step 11622, loss 0.0134112, acc 1
2016-09-05T21:45:21.578633: step 11623, loss 0.00247656, acc 1
2016-09-05T21:45:22.355926: step 11624, loss 0.00287392, acc 1
2016-09-05T21:45:23.170528: step 11625, loss 0.00983308, acc 1
2016-09-05T21:45:24.009402: step 11626, loss 0.0201926, acc 1
2016-09-05T21:45:24.801970: step 11627, loss 0.0320857, acc 1
2016-09-05T21:45:25.627043: step 11628, loss 0.00333967, acc 1
2016-09-05T21:45:26.438210: step 11629, loss 0.00371861, acc 1
2016-09-05T21:45:27.214132: step 11630, loss 0.0110165, acc 1
2016-09-05T21:45:28.009114: step 11631, loss 0.00673803, acc 1
2016-09-05T21:45:28.812945: step 11632, loss 0.00753264, acc 1
2016-09-05T21:45:29.600316: step 11633, loss 0.00709456, acc 1
2016-09-05T21:45:30.413055: step 11634, loss 0.00235532, acc 1
2016-09-05T21:45:31.240441: step 11635, loss 0.0135874, acc 1
2016-09-05T21:45:32.033419: step 11636, loss 0.015544, acc 1
2016-09-05T21:45:32.844853: step 11637, loss 0.0239223, acc 0.98
2016-09-05T21:45:33.693146: step 11638, loss 0.0845304, acc 0.94
2016-09-05T21:45:34.495561: step 11639, loss 0.00238089, acc 1
2016-09-05T21:45:34.888506: step 11640, loss 0.00550122, acc 1
2016-09-05T21:45:35.662843: step 11641, loss 0.01651, acc 1
2016-09-05T21:45:36.511335: step 11642, loss 0.00247996, acc 1
2016-09-05T21:45:37.374573: step 11643, loss 0.0294801, acc 0.98
2016-09-05T21:45:38.167584: step 11644, loss 0.00241802, acc 1
2016-09-05T21:45:38.972441: step 11645, loss 0.00852371, acc 1
2016-09-05T21:45:39.792610: step 11646, loss 0.0200947, acc 0.98
2016-09-05T21:45:40.606927: step 11647, loss 0.0232815, acc 1
2016-09-05T21:45:41.407795: step 11648, loss 0.00227502, acc 1
2016-09-05T21:45:42.203926: step 11649, loss 0.0241561, acc 0.98
2016-09-05T21:45:43.018778: step 11650, loss 0.00289685, acc 1
2016-09-05T21:45:43.810418: step 11651, loss 0.0072082, acc 1
2016-09-05T21:45:44.634742: step 11652, loss 0.00589538, acc 1
2016-09-05T21:45:45.428506: step 11653, loss 0.0222031, acc 0.98
2016-09-05T21:45:46.240821: step 11654, loss 0.021597, acc 0.98
2016-09-05T21:45:47.051520: step 11655, loss 0.00223935, acc 1
2016-09-05T21:45:47.896870: step 11656, loss 0.0022769, acc 1
2016-09-05T21:45:48.707325: step 11657, loss 0.0022335, acc 1
2016-09-05T21:45:49.552013: step 11658, loss 0.00289121, acc 1
2016-09-05T21:45:50.364355: step 11659, loss 0.0144383, acc 1
2016-09-05T21:45:51.178530: step 11660, loss 0.0062272, acc 1
2016-09-05T21:45:52.011962: step 11661, loss 0.0313559, acc 0.98
2016-09-05T21:45:52.817044: step 11662, loss 0.00531713, acc 1
2016-09-05T21:45:53.645197: step 11663, loss 0.00224906, acc 1
2016-09-05T21:45:54.465918: step 11664, loss 0.00219059, acc 1
2016-09-05T21:45:55.291211: step 11665, loss 0.106117, acc 0.98
2016-09-05T21:45:56.105336: step 11666, loss 0.00597054, acc 1
2016-09-05T21:45:56.940412: step 11667, loss 0.00290196, acc 1
2016-09-05T21:45:57.746459: step 11668, loss 0.00204192, acc 1
2016-09-05T21:45:58.545425: step 11669, loss 0.00257877, acc 1
2016-09-05T21:45:59.373973: step 11670, loss 0.0395359, acc 0.98
2016-09-05T21:46:00.184239: step 11671, loss 0.00647661, acc 1
2016-09-05T21:46:01.029597: step 11672, loss 0.00434732, acc 1
2016-09-05T21:46:01.830570: step 11673, loss 0.0363937, acc 0.98
2016-09-05T21:46:02.678118: step 11674, loss 0.0531202, acc 0.98
2016-09-05T21:46:03.465533: step 11675, loss 0.00180184, acc 1
2016-09-05T21:46:04.308701: step 11676, loss 0.0258105, acc 0.98
2016-09-05T21:46:05.117463: step 11677, loss 0.00262973, acc 1
2016-09-05T21:46:05.896181: step 11678, loss 0.0212056, acc 1
2016-09-05T21:46:06.687398: step 11679, loss 0.00174574, acc 1
2016-09-05T21:46:07.487960: step 11680, loss 0.00307037, acc 1
2016-09-05T21:46:08.264321: step 11681, loss 0.0318089, acc 0.96
2016-09-05T21:46:09.076899: step 11682, loss 0.0199972, acc 1
2016-09-05T21:46:09.898827: step 11683, loss 0.00614829, acc 1
2016-09-05T21:46:10.679758: step 11684, loss 0.112501, acc 0.98
2016-09-05T21:46:11.483553: step 11685, loss 0.0167972, acc 1
2016-09-05T21:46:12.294103: step 11686, loss 0.0361187, acc 0.98
2016-09-05T21:46:13.087643: step 11687, loss 0.0113295, acc 1
2016-09-05T21:46:13.866622: step 11688, loss 0.00607608, acc 1
2016-09-05T21:46:14.659294: step 11689, loss 0.00297128, acc 1
2016-09-05T21:46:15.467503: step 11690, loss 0.016614, acc 1
2016-09-05T21:46:16.295409: step 11691, loss 0.00879346, acc 1
2016-09-05T21:46:17.116758: step 11692, loss 0.0110223, acc 1
2016-09-05T21:46:17.903556: step 11693, loss 0.00565619, acc 1
2016-09-05T21:46:18.749160: step 11694, loss 0.00373127, acc 1
2016-09-05T21:46:19.563570: step 11695, loss 0.0608843, acc 0.96
2016-09-05T21:46:20.378145: step 11696, loss 0.0890397, acc 0.96
2016-09-05T21:46:21.218213: step 11697, loss 0.0144216, acc 1
2016-09-05T21:46:22.016193: step 11698, loss 0.0027676, acc 1
2016-09-05T21:46:22.785630: step 11699, loss 0.0031932, acc 1
2016-09-05T21:46:23.592982: step 11700, loss 0.00738477, acc 1

Evaluation:
2016-09-05T21:46:27.109348: step 11700, loss 2.21626, acc 0.729

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-11700

2016-09-05T21:46:29.086459: step 11701, loss 0.0209772, acc 0.98
2016-09-05T21:46:29.889636: step 11702, loss 0.0254778, acc 0.98
2016-09-05T21:46:30.714365: step 11703, loss 0.037993, acc 1
2016-09-05T21:46:31.545325: step 11704, loss 0.0333618, acc 0.98
2016-09-05T21:46:32.342389: step 11705, loss 0.0485826, acc 0.98
2016-09-05T21:46:33.164477: step 11706, loss 0.0247906, acc 0.98
2016-09-05T21:46:33.980986: step 11707, loss 0.0172294, acc 0.98
2016-09-05T21:46:34.790215: step 11708, loss 0.00288358, acc 1
2016-09-05T21:46:35.611077: step 11709, loss 0.00471385, acc 1
2016-09-05T21:46:36.429425: step 11710, loss 0.0165516, acc 0.98
2016-09-05T21:46:37.232170: step 11711, loss 0.00859843, acc 1
2016-09-05T21:46:38.076764: step 11712, loss 0.0644593, acc 0.98
2016-09-05T21:46:38.872645: step 11713, loss 0.00214969, acc 1
2016-09-05T21:46:39.680532: step 11714, loss 0.00452272, acc 1
2016-09-05T21:46:40.520867: step 11715, loss 0.0140251, acc 1
2016-09-05T21:46:41.335504: step 11716, loss 0.0048006, acc 1
2016-09-05T21:46:42.146801: step 11717, loss 0.00211807, acc 1
2016-09-05T21:46:42.978498: step 11718, loss 0.00221335, acc 1
2016-09-05T21:46:43.775331: step 11719, loss 0.0178069, acc 1
2016-09-05T21:46:44.594706: step 11720, loss 0.0099404, acc 1
2016-09-05T21:46:45.407325: step 11721, loss 0.00502053, acc 1
2016-09-05T21:46:46.218335: step 11722, loss 0.00302025, acc 1
2016-09-05T21:46:47.028533: step 11723, loss 0.0451865, acc 0.98
2016-09-05T21:46:47.845529: step 11724, loss 0.00219759, acc 1
2016-09-05T21:46:48.696423: step 11725, loss 0.00391189, acc 1
2016-09-05T21:46:49.470423: step 11726, loss 0.0285852, acc 0.98
2016-09-05T21:46:50.274878: step 11727, loss 0.0018362, acc 1
2016-09-05T21:46:51.093096: step 11728, loss 0.0328367, acc 0.98
2016-09-05T21:46:51.905231: step 11729, loss 0.00468879, acc 1
2016-09-05T21:46:52.699031: step 11730, loss 0.0249865, acc 1
2016-09-05T21:46:53.553454: step 11731, loss 0.0332176, acc 0.98
2016-09-05T21:46:54.356208: step 11732, loss 0.0325232, acc 0.98
2016-09-05T21:46:55.152077: step 11733, loss 0.00225881, acc 1
2016-09-05T21:46:55.961532: step 11734, loss 0.00592633, acc 1
2016-09-05T21:46:56.735628: step 11735, loss 0.0367347, acc 0.98
2016-09-05T21:46:57.568807: step 11736, loss 0.00276377, acc 1
2016-09-05T21:46:58.419434: step 11737, loss 0.0390348, acc 0.98
2016-09-05T21:46:59.182508: step 11738, loss 0.0449303, acc 0.96
2016-09-05T21:47:00.039116: step 11739, loss 0.00483064, acc 1
2016-09-05T21:47:00.888017: step 11740, loss 0.0225945, acc 0.98
2016-09-05T21:47:01.705488: step 11741, loss 0.0194321, acc 0.98
2016-09-05T21:47:02.507787: step 11742, loss 0.0137803, acc 1
2016-09-05T21:47:03.337645: step 11743, loss 0.0380617, acc 1
2016-09-05T21:47:04.135077: step 11744, loss 0.0393575, acc 0.98
2016-09-05T21:47:04.965688: step 11745, loss 0.00164348, acc 1
2016-09-05T21:47:05.797584: step 11746, loss 0.0159203, acc 1
2016-09-05T21:47:06.642538: step 11747, loss 0.0522514, acc 0.98
2016-09-05T21:47:07.455672: step 11748, loss 0.00513542, acc 1
2016-09-05T21:47:08.310880: step 11749, loss 0.0140718, acc 1
2016-09-05T21:47:09.139676: step 11750, loss 0.0354295, acc 0.98
2016-09-05T21:47:09.932728: step 11751, loss 0.00488727, acc 1
2016-09-05T21:47:10.756356: step 11752, loss 0.00168272, acc 1
2016-09-05T21:47:11.569843: step 11753, loss 0.0475895, acc 0.98
2016-09-05T21:47:12.403149: step 11754, loss 0.0116167, acc 1
2016-09-05T21:47:13.239178: step 11755, loss 0.00475329, acc 1
2016-09-05T21:47:14.031527: step 11756, loss 0.00940087, acc 1
2016-09-05T21:47:14.840268: step 11757, loss 0.0155948, acc 0.98
2016-09-05T21:47:15.663802: step 11758, loss 0.00963701, acc 1
2016-09-05T21:47:16.447530: step 11759, loss 0.0211777, acc 0.98
2016-09-05T21:47:17.233319: step 11760, loss 0.0127728, acc 1
2016-09-05T21:47:18.041138: step 11761, loss 0.0100145, acc 1
2016-09-05T21:47:18.856277: step 11762, loss 0.0210581, acc 0.98
2016-09-05T21:47:19.631760: step 11763, loss 0.0243053, acc 0.98
2016-09-05T21:47:20.481226: step 11764, loss 0.0147252, acc 1
2016-09-05T21:47:21.285627: step 11765, loss 0.0226958, acc 1
2016-09-05T21:47:22.107537: step 11766, loss 0.033216, acc 0.96
2016-09-05T21:47:22.972993: step 11767, loss 0.0218986, acc 1
2016-09-05T21:47:23.783015: step 11768, loss 0.014101, acc 1
2016-09-05T21:47:24.600261: step 11769, loss 0.00466887, acc 1
2016-09-05T21:47:25.382381: step 11770, loss 0.0235622, acc 1
2016-09-05T21:47:26.211385: step 11771, loss 0.0151037, acc 1
2016-09-05T21:47:26.984541: step 11772, loss 0.00850766, acc 1
2016-09-05T21:47:27.772158: step 11773, loss 0.00349731, acc 1
2016-09-05T21:47:28.581481: step 11774, loss 0.00542667, acc 1
2016-09-05T21:47:29.398536: step 11775, loss 0.0187418, acc 0.98
2016-09-05T21:47:30.189821: step 11776, loss 0.0759841, acc 0.98
2016-09-05T21:47:31.002335: step 11777, loss 0.0022035, acc 1
2016-09-05T21:47:31.800227: step 11778, loss 0.00208061, acc 1
2016-09-05T21:47:32.614125: step 11779, loss 0.0200361, acc 0.98
2016-09-05T21:47:33.436382: step 11780, loss 0.00248173, acc 1
2016-09-05T21:47:34.223086: step 11781, loss 0.0047186, acc 1
2016-09-05T21:47:35.039231: step 11782, loss 0.00446269, acc 1
2016-09-05T21:47:35.859907: step 11783, loss 0.00719023, acc 1
2016-09-05T21:47:36.642385: step 11784, loss 0.00692048, acc 1
2016-09-05T21:47:37.436301: step 11785, loss 0.00550713, acc 1
2016-09-05T21:47:38.239471: step 11786, loss 0.00310536, acc 1
2016-09-05T21:47:39.018506: step 11787, loss 0.00449781, acc 1
2016-09-05T21:47:39.837641: step 11788, loss 0.0112052, acc 1
2016-09-05T21:47:40.681982: step 11789, loss 0.0148368, acc 1
2016-09-05T21:47:41.483118: step 11790, loss 0.00436432, acc 1
2016-09-05T21:47:42.288442: step 11791, loss 0.103515, acc 0.98
2016-09-05T21:47:43.096648: step 11792, loss 0.0371926, acc 0.98
2016-09-05T21:47:43.881334: step 11793, loss 0.0316816, acc 0.98
2016-09-05T21:47:44.659400: step 11794, loss 0.0168964, acc 1
2016-09-05T21:47:45.485458: step 11795, loss 0.0451132, acc 0.98
2016-09-05T21:47:46.262923: step 11796, loss 0.00205755, acc 1
2016-09-05T21:47:47.073564: step 11797, loss 0.0020351, acc 1
2016-09-05T21:47:47.877311: step 11798, loss 0.00584298, acc 1
2016-09-05T21:47:48.687349: step 11799, loss 0.00698996, acc 1
2016-09-05T21:47:49.498797: step 11800, loss 0.00380102, acc 1

Evaluation:
2016-09-05T21:47:53.024911: step 11800, loss 2.51696, acc 0.724

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-11800

2016-09-05T21:47:54.912661: step 11801, loss 0.00455861, acc 1
2016-09-05T21:47:55.710586: step 11802, loss 0.0179509, acc 0.98
2016-09-05T21:47:56.516098: step 11803, loss 0.00325215, acc 1
2016-09-05T21:47:57.301526: step 11804, loss 0.00379119, acc 1
2016-09-05T21:47:58.137462: step 11805, loss 0.0020625, acc 1
2016-09-05T21:47:58.947227: step 11806, loss 0.0597392, acc 0.94
2016-09-05T21:47:59.718468: step 11807, loss 0.0159408, acc 0.98
2016-09-05T21:48:00.570920: step 11808, loss 0.00217254, acc 1
2016-09-05T21:48:01.382714: step 11809, loss 0.0104353, acc 1
2016-09-05T21:48:02.172492: step 11810, loss 0.0177067, acc 0.98
2016-09-05T21:48:02.974630: step 11811, loss 0.00509749, acc 1
2016-09-05T21:48:03.788066: step 11812, loss 0.0272671, acc 0.98
2016-09-05T21:48:04.542821: step 11813, loss 0.00224613, acc 1
2016-09-05T21:48:05.344104: step 11814, loss 0.0238142, acc 0.98
2016-09-05T21:48:06.201356: step 11815, loss 0.0154958, acc 1
2016-09-05T21:48:06.980231: step 11816, loss 0.0115613, acc 1
2016-09-05T21:48:07.768727: step 11817, loss 0.0031895, acc 1
2016-09-05T21:48:08.582213: step 11818, loss 0.00377086, acc 1
2016-09-05T21:48:09.345417: step 11819, loss 0.00793417, acc 1
2016-09-05T21:48:10.179019: step 11820, loss 0.00531683, acc 1
2016-09-05T21:48:10.992114: step 11821, loss 0.0385814, acc 0.98
2016-09-05T21:48:11.774668: step 11822, loss 0.00594423, acc 1
2016-09-05T21:48:12.578076: step 11823, loss 0.013635, acc 1
2016-09-05T21:48:13.389211: step 11824, loss 0.0276517, acc 1
2016-09-05T21:48:14.177978: step 11825, loss 0.0130442, acc 1
2016-09-05T21:48:15.063552: step 11826, loss 0.00208717, acc 1
2016-09-05T21:48:15.881291: step 11827, loss 0.0722314, acc 0.96
2016-09-05T21:48:16.697593: step 11828, loss 0.0132964, acc 1
2016-09-05T21:48:17.525085: step 11829, loss 0.0607386, acc 0.98
2016-09-05T21:48:18.337859: step 11830, loss 0.0217162, acc 0.98
2016-09-05T21:48:19.160459: step 11831, loss 0.0190062, acc 1
2016-09-05T21:48:19.961374: step 11832, loss 0.0280631, acc 0.98
2016-09-05T21:48:20.784572: step 11833, loss 0.00218425, acc 1
2016-09-05T21:48:21.206970: step 11834, loss 0.00417318, acc 1
2016-09-05T21:48:22.042070: step 11835, loss 0.00194035, acc 1
2016-09-05T21:48:22.839410: step 11836, loss 0.0159264, acc 0.98
2016-09-05T21:48:23.638831: step 11837, loss 0.0762852, acc 0.96
2016-09-05T21:48:24.452582: step 11838, loss 0.00320547, acc 1
2016-09-05T21:48:25.255388: step 11839, loss 0.0293486, acc 0.98
2016-09-05T21:48:26.065639: step 11840, loss 0.0096642, acc 1
2016-09-05T21:48:26.900513: step 11841, loss 0.0315979, acc 0.98
2016-09-05T21:48:27.701036: step 11842, loss 0.0224291, acc 1
2016-09-05T21:48:28.525962: step 11843, loss 0.00568058, acc 1
2016-09-05T21:48:29.356117: step 11844, loss 0.0290087, acc 0.98
2016-09-05T21:48:30.148973: step 11845, loss 0.0128681, acc 1
2016-09-05T21:48:30.952733: step 11846, loss 0.0220581, acc 0.98
2016-09-05T21:48:31.773729: step 11847, loss 0.0648413, acc 0.96
2016-09-05T21:48:32.591847: step 11848, loss 0.0183491, acc 1
2016-09-05T21:48:33.434620: step 11849, loss 0.00221445, acc 1
2016-09-05T21:48:34.265403: step 11850, loss 0.00189278, acc 1
2016-09-05T21:48:35.075793: step 11851, loss 0.0129296, acc 1
2016-09-05T21:48:35.864386: step 11852, loss 0.0102789, acc 1
2016-09-05T21:48:36.697996: step 11853, loss 0.0440211, acc 0.98
2016-09-05T21:48:37.512553: step 11854, loss 0.00418139, acc 1
2016-09-05T21:48:38.325910: step 11855, loss 0.0095923, acc 1
2016-09-05T21:48:39.183672: step 11856, loss 0.00181911, acc 1
2016-09-05T21:48:40.027711: step 11857, loss 0.00257147, acc 1
2016-09-05T21:48:40.829163: step 11858, loss 0.00272491, acc 1
2016-09-05T21:48:41.620690: step 11859, loss 0.00392977, acc 1
2016-09-05T21:48:42.433716: step 11860, loss 0.00176169, acc 1
2016-09-05T21:48:43.206827: step 11861, loss 0.00242686, acc 1
2016-09-05T21:48:43.995925: step 11862, loss 0.00187022, acc 1
2016-09-05T21:48:44.807331: step 11863, loss 0.00178068, acc 1
2016-09-05T21:48:45.598407: step 11864, loss 0.053885, acc 0.98
2016-09-05T21:48:46.389760: step 11865, loss 0.0050559, acc 1
2016-09-05T21:48:47.232258: step 11866, loss 0.019625, acc 1
2016-09-05T21:48:48.040510: step 11867, loss 0.0511005, acc 0.96
2016-09-05T21:48:48.834145: step 11868, loss 0.0164602, acc 0.98
2016-09-05T21:48:49.661939: step 11869, loss 0.0186165, acc 1
2016-09-05T21:48:50.473079: step 11870, loss 0.0180301, acc 0.98
2016-09-05T21:48:51.277229: step 11871, loss 0.00225572, acc 1
2016-09-05T21:48:52.087604: step 11872, loss 0.0137854, acc 1
2016-09-05T21:48:52.873522: step 11873, loss 0.00313647, acc 1
2016-09-05T21:48:53.674662: step 11874, loss 0.00191914, acc 1
2016-09-05T21:48:54.483928: step 11875, loss 0.0237622, acc 0.98
2016-09-05T21:48:55.280934: step 11876, loss 0.0100659, acc 1
2016-09-05T21:48:56.079160: step 11877, loss 0.00319966, acc 1
2016-09-05T21:48:56.884861: step 11878, loss 0.00339168, acc 1
2016-09-05T21:48:57.677076: step 11879, loss 0.0127794, acc 1
2016-09-05T21:48:58.473641: step 11880, loss 0.00178681, acc 1
2016-09-05T21:48:59.294660: step 11881, loss 0.0149255, acc 1
2016-09-05T21:49:00.079917: step 11882, loss 0.00204196, acc 1
2016-09-05T21:49:00.882510: step 11883, loss 0.013133, acc 1
2016-09-05T21:49:01.704165: step 11884, loss 0.0543277, acc 0.96
2016-09-05T21:49:02.484482: step 11885, loss 0.00273288, acc 1
2016-09-05T21:49:03.293078: step 11886, loss 0.00165426, acc 1
2016-09-05T21:49:04.132100: step 11887, loss 0.0064851, acc 1
2016-09-05T21:49:04.936232: step 11888, loss 0.0143652, acc 1
2016-09-05T21:49:05.752451: step 11889, loss 0.0702284, acc 0.98
2016-09-05T21:49:06.576065: step 11890, loss 0.00330712, acc 1
2016-09-05T21:49:07.365970: step 11891, loss 0.025621, acc 1
2016-09-05T21:49:08.166668: step 11892, loss 0.0022874, acc 1
2016-09-05T21:49:08.969939: step 11893, loss 0.0175562, acc 0.98
2016-09-05T21:49:09.761273: step 11894, loss 0.0339701, acc 0.98
2016-09-05T21:49:10.552545: step 11895, loss 0.0245607, acc 1
2016-09-05T21:49:11.399245: step 11896, loss 0.0171108, acc 1
2016-09-05T21:49:12.169843: step 11897, loss 0.0498533, acc 0.98
2016-09-05T21:49:12.991942: step 11898, loss 0.0119306, acc 1
2016-09-05T21:49:13.815932: step 11899, loss 0.0137088, acc 1
2016-09-05T21:49:14.625395: step 11900, loss 0.0045162, acc 1

Evaluation:
2016-09-05T21:49:18.137650: step 11900, loss 2.24398, acc 0.736

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-11900

2016-09-05T21:49:19.946895: step 11901, loss 0.00254515, acc 1
2016-09-05T21:49:20.735255: step 11902, loss 0.0201971, acc 0.98
2016-09-05T21:49:21.551511: step 11903, loss 0.00203914, acc 1
2016-09-05T21:49:22.402034: step 11904, loss 0.00262872, acc 1
2016-09-05T21:49:23.194687: step 11905, loss 0.0133717, acc 1
2016-09-05T21:49:24.003316: step 11906, loss 0.0328245, acc 0.98
2016-09-05T21:49:24.848519: step 11907, loss 0.00204818, acc 1
2016-09-05T21:49:25.645415: step 11908, loss 0.00388154, acc 1
2016-09-05T21:49:26.433091: step 11909, loss 0.00271742, acc 1
2016-09-05T21:49:27.298819: step 11910, loss 0.108249, acc 0.96
2016-09-05T21:49:28.128808: step 11911, loss 0.0633168, acc 0.98
2016-09-05T21:49:28.973416: step 11912, loss 0.00205459, acc 1
2016-09-05T21:49:29.766757: step 11913, loss 0.0159614, acc 1
2016-09-05T21:49:30.574087: step 11914, loss 0.0291497, acc 0.98
2016-09-05T21:49:31.398196: step 11915, loss 0.0212958, acc 0.98
2016-09-05T21:49:32.225466: step 11916, loss 0.0110305, acc 1
2016-09-05T21:49:33.023944: step 11917, loss 0.0043999, acc 1
2016-09-05T21:49:33.846407: step 11918, loss 0.00459256, acc 1
2016-09-05T21:49:34.673032: step 11919, loss 0.0499473, acc 0.98
2016-09-05T21:49:35.483413: step 11920, loss 0.00676911, acc 1
2016-09-05T21:49:36.282771: step 11921, loss 0.00493064, acc 1
2016-09-05T21:49:37.108515: step 11922, loss 0.00968663, acc 1
2016-09-05T21:49:37.916876: step 11923, loss 0.00690146, acc 1
2016-09-05T21:49:38.702730: step 11924, loss 0.0358276, acc 0.98
2016-09-05T21:49:39.545384: step 11925, loss 0.0103809, acc 1
2016-09-05T21:49:40.371613: step 11926, loss 0.0330622, acc 1
2016-09-05T21:49:41.165649: step 11927, loss 0.0363817, acc 0.98
2016-09-05T21:49:41.970761: step 11928, loss 0.00523382, acc 1
2016-09-05T21:49:42.791769: step 11929, loss 0.014067, acc 1
2016-09-05T21:49:43.595084: step 11930, loss 0.00230772, acc 1
2016-09-05T21:49:44.395862: step 11931, loss 0.00574043, acc 1
2016-09-05T21:49:45.174557: step 11932, loss 0.0217047, acc 0.98
2016-09-05T21:49:45.998362: step 11933, loss 0.0335248, acc 0.98
2016-09-05T21:49:46.824634: step 11934, loss 0.0239846, acc 0.98
2016-09-05T21:49:47.636263: step 11935, loss 0.00661319, acc 1
2016-09-05T21:49:48.440688: step 11936, loss 0.00246376, acc 1
2016-09-05T21:49:49.259963: step 11937, loss 0.0245984, acc 1
2016-09-05T21:49:50.094954: step 11938, loss 0.0158758, acc 1
2016-09-05T21:49:50.937555: step 11939, loss 0.0141955, acc 1
2016-09-05T21:49:51.740257: step 11940, loss 0.00527991, acc 1
2016-09-05T21:49:52.529074: step 11941, loss 0.00232256, acc 1
2016-09-05T21:49:53.304055: step 11942, loss 0.0139965, acc 1
2016-09-05T21:49:54.105494: step 11943, loss 0.00407045, acc 1
2016-09-05T21:49:54.910339: step 11944, loss 0.0185313, acc 0.98
2016-09-05T21:49:55.682345: step 11945, loss 0.0038469, acc 1
2016-09-05T21:49:56.516348: step 11946, loss 0.00224586, acc 1
2016-09-05T21:49:57.329537: step 11947, loss 0.075559, acc 0.98
2016-09-05T21:49:58.156711: step 11948, loss 0.0493992, acc 0.98
2016-09-05T21:49:58.980903: step 11949, loss 0.00216044, acc 1
2016-09-05T21:49:59.797420: step 11950, loss 0.0543991, acc 0.98
2016-09-05T21:50:00.606438: step 11951, loss 0.00558725, acc 1
2016-09-05T21:50:01.405770: step 11952, loss 0.00480052, acc 1
2016-09-05T21:50:02.222999: step 11953, loss 0.0020149, acc 1
2016-09-05T21:50:03.018157: step 11954, loss 0.0486972, acc 0.96
2016-09-05T21:50:03.821136: step 11955, loss 0.0026491, acc 1
2016-09-05T21:50:04.628064: step 11956, loss 0.0160245, acc 1
2016-09-05T21:50:05.414946: step 11957, loss 0.00669203, acc 1
2016-09-05T21:50:06.243428: step 11958, loss 0.00204275, acc 1
2016-09-05T21:50:07.062805: step 11959, loss 0.0259344, acc 0.98
2016-09-05T21:50:07.850816: step 11960, loss 0.0212604, acc 1
2016-09-05T21:50:08.671948: step 11961, loss 0.0019326, acc 1
2016-09-05T21:50:09.515731: step 11962, loss 0.0135644, acc 1
2016-09-05T21:50:10.363816: step 11963, loss 0.00190401, acc 1
2016-09-05T21:50:11.169912: step 11964, loss 0.0188859, acc 0.98
2016-09-05T21:50:11.996975: step 11965, loss 0.0404474, acc 0.96
2016-09-05T21:50:12.814377: step 11966, loss 0.0674594, acc 0.98
2016-09-05T21:50:13.617226: step 11967, loss 0.00188221, acc 1
2016-09-05T21:50:14.445476: step 11968, loss 0.0175274, acc 0.98
2016-09-05T21:50:15.258967: step 11969, loss 0.00792485, acc 1
2016-09-05T21:50:16.093589: step 11970, loss 0.00238585, acc 1
2016-09-05T21:50:16.946822: step 11971, loss 0.00230116, acc 1
2016-09-05T21:50:17.773378: step 11972, loss 0.00172886, acc 1
2016-09-05T21:50:18.580147: step 11973, loss 0.032088, acc 0.98
2016-09-05T21:50:19.446606: step 11974, loss 0.012143, acc 1
2016-09-05T21:50:20.253354: step 11975, loss 0.00662861, acc 1
2016-09-05T21:50:21.072930: step 11976, loss 0.0265499, acc 1
2016-09-05T21:50:21.864994: step 11977, loss 0.00549279, acc 1
2016-09-05T21:50:22.689728: step 11978, loss 0.00547101, acc 1
2016-09-05T21:50:23.459993: step 11979, loss 0.0271251, acc 0.98
2016-09-05T21:50:24.279430: step 11980, loss 0.0267332, acc 0.98
2016-09-05T21:50:25.102036: step 11981, loss 0.00222408, acc 1
2016-09-05T21:50:25.885648: step 11982, loss 0.00422173, acc 1
2016-09-05T21:50:26.715385: step 11983, loss 0.0201716, acc 1
2016-09-05T21:50:27.539193: step 11984, loss 0.00326463, acc 1
2016-09-05T21:50:28.333238: step 11985, loss 0.0236246, acc 0.98
2016-09-05T21:50:29.124245: step 11986, loss 0.0188764, acc 0.98
2016-09-05T21:50:29.923266: step 11987, loss 0.0237412, acc 1
2016-09-05T21:50:30.736905: step 11988, loss 0.100885, acc 0.98
2016-09-05T21:50:31.520456: step 11989, loss 0.0403995, acc 0.98
2016-09-05T21:50:32.353933: step 11990, loss 0.00282498, acc 1
2016-09-05T21:50:33.149522: step 11991, loss 0.00714467, acc 1
2016-09-05T21:50:33.935641: step 11992, loss 0.0539533, acc 0.96
2016-09-05T21:50:34.728937: step 11993, loss 0.00322723, acc 1
2016-09-05T21:50:35.551170: step 11994, loss 0.0061412, acc 1
2016-09-05T21:50:36.407369: step 11995, loss 0.00706495, acc 1
2016-09-05T21:50:37.215118: step 11996, loss 0.00932847, acc 1
2016-09-05T21:50:38.008209: step 11997, loss 0.00488309, acc 1
2016-09-05T21:50:38.825225: step 11998, loss 0.0111612, acc 1
2016-09-05T21:50:39.659481: step 11999, loss 0.0211373, acc 1
2016-09-05T21:50:40.476929: step 12000, loss 0.0387564, acc 0.98

Evaluation:
2016-09-05T21:50:43.996610: step 12000, loss 2.51145, acc 0.72

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-12000

2016-09-05T21:50:45.983171: step 12001, loss 0.00249743, acc 1
2016-09-05T21:50:46.790432: step 12002, loss 0.00247637, acc 1
2016-09-05T21:50:47.605548: step 12003, loss 0.0234613, acc 1
2016-09-05T21:50:48.466033: step 12004, loss 0.0232542, acc 0.98
2016-09-05T21:50:49.296580: step 12005, loss 0.0062202, acc 1
2016-09-05T21:50:50.090953: step 12006, loss 0.0126419, acc 1
2016-09-05T21:50:50.908225: step 12007, loss 0.0149073, acc 1
2016-09-05T21:50:51.740530: step 12008, loss 0.00221019, acc 1
2016-09-05T21:50:52.565969: step 12009, loss 0.0180512, acc 0.98
2016-09-05T21:50:53.389695: step 12010, loss 0.00415672, acc 1
2016-09-05T21:50:54.194910: step 12011, loss 0.00223326, acc 1
2016-09-05T21:50:54.977062: step 12012, loss 0.00226745, acc 1
2016-09-05T21:50:55.835070: step 12013, loss 0.0147201, acc 1
2016-09-05T21:50:56.649324: step 12014, loss 0.0166573, acc 1
2016-09-05T21:50:57.435093: step 12015, loss 0.0344449, acc 0.98
2016-09-05T21:50:58.237204: step 12016, loss 0.0133843, acc 1
2016-09-05T21:50:59.052768: step 12017, loss 0.00425647, acc 1
2016-09-05T21:50:59.845549: step 12018, loss 0.00235528, acc 1
2016-09-05T21:51:00.693262: step 12019, loss 0.0698494, acc 0.98
2016-09-05T21:51:01.566356: step 12020, loss 0.00747119, acc 1
2016-09-05T21:51:02.364017: step 12021, loss 0.0319675, acc 0.96
2016-09-05T21:51:03.178381: step 12022, loss 0.00223848, acc 1
2016-09-05T21:51:03.963619: step 12023, loss 0.00570756, acc 1
2016-09-05T21:51:04.731668: step 12024, loss 0.0286157, acc 0.98
2016-09-05T21:51:05.570437: step 12025, loss 0.04789, acc 0.96
2016-09-05T21:51:06.380573: step 12026, loss 0.00240066, acc 1
2016-09-05T21:51:07.150060: step 12027, loss 0.00221339, acc 1
2016-09-05T21:51:07.586991: step 12028, loss 0.00223527, acc 1
2016-09-05T21:51:08.363524: step 12029, loss 0.00254385, acc 1
2016-09-05T21:51:09.161555: step 12030, loss 0.0116586, acc 1
2016-09-05T21:51:09.996568: step 12031, loss 0.043667, acc 0.98
2016-09-05T21:51:10.785401: step 12032, loss 0.00814175, acc 1
2016-09-05T21:51:11.610499: step 12033, loss 0.00530043, acc 1
2016-09-05T21:51:12.443088: step 12034, loss 0.00824184, acc 1
2016-09-05T21:51:13.246463: step 12035, loss 0.00284267, acc 1
2016-09-05T21:51:14.043390: step 12036, loss 0.00449569, acc 1
2016-09-05T21:51:14.873610: step 12037, loss 0.00283393, acc 1
2016-09-05T21:51:15.679218: step 12038, loss 0.0101633, acc 1
2016-09-05T21:51:16.491231: step 12039, loss 0.002893, acc 1
2016-09-05T21:51:17.305099: step 12040, loss 0.019547, acc 0.98
2016-09-05T21:51:18.112210: step 12041, loss 0.0384336, acc 0.98
2016-09-05T21:51:18.925745: step 12042, loss 0.0258854, acc 0.98
2016-09-05T21:51:19.766745: step 12043, loss 0.00416799, acc 1
2016-09-05T21:51:20.577417: step 12044, loss 0.00295064, acc 1
2016-09-05T21:51:21.382844: step 12045, loss 0.0146391, acc 1
2016-09-05T21:51:22.185400: step 12046, loss 0.00685982, acc 1
2016-09-05T21:51:22.986154: step 12047, loss 0.00285912, acc 1
2016-09-05T21:51:23.770314: step 12048, loss 0.0145331, acc 1
2016-09-05T21:51:24.621350: step 12049, loss 0.0108152, acc 1
2016-09-05T21:51:25.421302: step 12050, loss 0.00344674, acc 1
2016-09-05T21:51:26.224121: step 12051, loss 0.00322088, acc 1
2016-09-05T21:51:27.058379: step 12052, loss 0.00453841, acc 1
2016-09-05T21:51:27.873951: step 12053, loss 0.0030491, acc 1
2016-09-05T21:51:28.686899: step 12054, loss 0.00285445, acc 1
2016-09-05T21:51:29.507607: step 12055, loss 0.00557099, acc 1
2016-09-05T21:51:30.359963: step 12056, loss 0.00287986, acc 1
2016-09-05T21:51:31.183630: step 12057, loss 0.00273445, acc 1
2016-09-05T21:51:31.991253: step 12058, loss 0.0534099, acc 0.96
2016-09-05T21:51:32.797638: step 12059, loss 0.014066, acc 1
2016-09-05T21:51:33.589563: step 12060, loss 0.0216424, acc 1
2016-09-05T21:51:34.452269: step 12061, loss 0.00714197, acc 1
2016-09-05T21:51:35.246065: step 12062, loss 0.00302918, acc 1
2016-09-05T21:51:36.042589: step 12063, loss 0.00262952, acc 1
2016-09-05T21:51:36.857972: step 12064, loss 0.00399372, acc 1
2016-09-05T21:51:37.673279: step 12065, loss 0.0381283, acc 0.98
2016-09-05T21:51:38.495831: step 12066, loss 0.0177578, acc 0.98
2016-09-05T21:51:39.325421: step 12067, loss 0.00671746, acc 1
2016-09-05T21:51:40.129661: step 12068, loss 0.00245729, acc 1
2016-09-05T21:51:40.924772: step 12069, loss 0.0075402, acc 1
2016-09-05T21:51:41.735850: step 12070, loss 0.124387, acc 0.98
2016-09-05T21:51:42.533483: step 12071, loss 0.00522829, acc 1
2016-09-05T21:51:43.332262: step 12072, loss 0.0030973, acc 1
2016-09-05T21:51:44.147021: step 12073, loss 0.0173733, acc 0.98
2016-09-05T21:51:44.958269: step 12074, loss 0.00255122, acc 1
2016-09-05T21:51:45.806383: step 12075, loss 0.00233329, acc 1
2016-09-05T21:51:46.625836: step 12076, loss 0.0114496, acc 1
2016-09-05T21:51:47.438624: step 12077, loss 0.00205509, acc 1
2016-09-05T21:51:48.222759: step 12078, loss 0.0186444, acc 0.98
2016-09-05T21:51:49.016341: step 12079, loss 0.0217153, acc 0.98
2016-09-05T21:51:49.853437: step 12080, loss 0.0193767, acc 1
2016-09-05T21:51:50.637727: step 12081, loss 0.00303983, acc 1
2016-09-05T21:51:51.450954: step 12082, loss 0.00262831, acc 1
2016-09-05T21:51:52.270814: step 12083, loss 0.0148847, acc 1
2016-09-05T21:51:53.075107: step 12084, loss 0.0257686, acc 0.98
2016-09-05T21:51:53.898195: step 12085, loss 0.00747283, acc 1
2016-09-05T21:51:54.700270: step 12086, loss 0.0114523, acc 1
2016-09-05T21:51:55.481407: step 12087, loss 0.0375947, acc 0.98
2016-09-05T21:51:56.307001: step 12088, loss 0.0193417, acc 0.98
2016-09-05T21:51:57.121403: step 12089, loss 0.00756268, acc 1
2016-09-05T21:51:57.906564: step 12090, loss 0.0187533, acc 0.98
2016-09-05T21:51:58.700640: step 12091, loss 0.0338951, acc 0.96
2016-09-05T21:51:59.556554: step 12092, loss 0.0266161, acc 0.98
2016-09-05T21:52:00.363345: step 12093, loss 0.0271309, acc 0.98
2016-09-05T21:52:01.166460: step 12094, loss 0.00204511, acc 1
2016-09-05T21:52:02.011785: step 12095, loss 0.00970923, acc 1
2016-09-05T21:52:02.799126: step 12096, loss 0.00403515, acc 1
2016-09-05T21:52:03.610281: step 12097, loss 0.00198658, acc 1
2016-09-05T21:52:04.422230: step 12098, loss 0.00393414, acc 1
2016-09-05T21:52:05.219319: step 12099, loss 0.00212074, acc 1
2016-09-05T21:52:06.039649: step 12100, loss 0.00320001, acc 1

Evaluation:
2016-09-05T21:52:09.556223: step 12100, loss 2.46554, acc 0.728

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-12100

2016-09-05T21:52:11.455845: step 12101, loss 0.00202648, acc 1
2016-09-05T21:52:12.252519: step 12102, loss 0.00207345, acc 1
2016-09-05T21:52:13.081974: step 12103, loss 0.0103167, acc 1
2016-09-05T21:52:13.896190: step 12104, loss 0.0374427, acc 1
2016-09-05T21:52:14.710337: step 12105, loss 0.0151412, acc 1
2016-09-05T21:52:15.531277: step 12106, loss 0.002131, acc 1
2016-09-05T21:52:16.341607: step 12107, loss 0.031445, acc 0.98
2016-09-05T21:52:17.148572: step 12108, loss 0.0561944, acc 0.96
2016-09-05T21:52:17.981635: step 12109, loss 0.00232067, acc 1
2016-09-05T21:52:18.785101: step 12110, loss 0.0109228, acc 1
2016-09-05T21:52:19.603782: step 12111, loss 0.0123763, acc 1
2016-09-05T21:52:20.416305: step 12112, loss 0.00269925, acc 1
2016-09-05T21:52:21.212143: step 12113, loss 0.00615873, acc 1
2016-09-05T21:52:22.020536: step 12114, loss 0.0261613, acc 1
2016-09-05T21:52:22.854102: step 12115, loss 0.0533969, acc 0.98
2016-09-05T21:52:23.662564: step 12116, loss 0.00690587, acc 1
2016-09-05T21:52:24.454666: step 12117, loss 0.0124366, acc 1
2016-09-05T21:52:25.251166: step 12118, loss 0.0106286, acc 1
2016-09-05T21:52:26.054658: step 12119, loss 0.0210311, acc 0.98
2016-09-05T21:52:26.858347: step 12120, loss 0.0166303, acc 0.98
2016-09-05T21:52:27.664651: step 12121, loss 0.049928, acc 0.98
2016-09-05T21:52:28.483105: step 12122, loss 0.00802544, acc 1
2016-09-05T21:52:29.368939: step 12123, loss 0.0330767, acc 0.98
2016-09-05T21:52:30.178771: step 12124, loss 0.153237, acc 0.96
2016-09-05T21:52:30.994561: step 12125, loss 0.00217911, acc 1
2016-09-05T21:52:31.786060: step 12126, loss 0.0542598, acc 0.96
2016-09-05T21:52:32.575454: step 12127, loss 0.00976212, acc 1
2016-09-05T21:52:33.390169: step 12128, loss 0.0133121, acc 1
2016-09-05T21:52:34.153976: step 12129, loss 0.0219272, acc 0.98
2016-09-05T21:52:34.950942: step 12130, loss 0.00326841, acc 1
2016-09-05T21:52:35.756877: step 12131, loss 0.0102267, acc 1
2016-09-05T21:52:36.527411: step 12132, loss 0.0296804, acc 0.98
2016-09-05T21:52:37.336813: step 12133, loss 0.00763895, acc 1
2016-09-05T21:52:38.106230: step 12134, loss 0.0644511, acc 0.96
2016-09-05T21:52:38.921137: step 12135, loss 0.00201747, acc 1
2016-09-05T21:52:39.749630: step 12136, loss 0.0170465, acc 0.98
2016-09-05T21:52:40.568598: step 12137, loss 0.00208943, acc 1
2016-09-05T21:52:41.374731: step 12138, loss 0.144095, acc 0.98
2016-09-05T21:52:42.232510: step 12139, loss 0.0190637, acc 0.98
2016-09-05T21:52:43.059484: step 12140, loss 0.00237993, acc 1
2016-09-05T21:52:43.828748: step 12141, loss 0.0101341, acc 1
2016-09-05T21:52:44.683506: step 12142, loss 0.0246622, acc 1
2016-09-05T21:52:45.492365: step 12143, loss 0.00655075, acc 1
2016-09-05T21:52:46.285498: step 12144, loss 0.0433159, acc 0.98
2016-09-05T21:52:47.091837: step 12145, loss 0.00751602, acc 1
2016-09-05T21:52:47.936176: step 12146, loss 0.0307863, acc 0.98
2016-09-05T21:52:48.743892: step 12147, loss 0.00202985, acc 1
2016-09-05T21:52:49.553793: step 12148, loss 0.0500953, acc 0.98
2016-09-05T21:52:50.400712: step 12149, loss 0.0249474, acc 1
2016-09-05T21:52:51.219678: step 12150, loss 0.0250759, acc 1
2016-09-05T21:52:52.034039: step 12151, loss 0.0221173, acc 0.98
2016-09-05T21:52:52.846095: step 12152, loss 0.00238401, acc 1
2016-09-05T21:52:53.636252: step 12153, loss 0.0143598, acc 1
2016-09-05T21:52:54.445553: step 12154, loss 0.0160466, acc 1
2016-09-05T21:52:55.285135: step 12155, loss 0.0177186, acc 1
2016-09-05T21:52:56.100796: step 12156, loss 0.030489, acc 0.98
2016-09-05T21:52:56.898757: step 12157, loss 0.0504977, acc 0.98
2016-09-05T21:52:57.749560: step 12158, loss 0.0396424, acc 0.98
2016-09-05T21:52:58.593735: step 12159, loss 0.0320854, acc 1
2016-09-05T21:52:59.406713: step 12160, loss 0.0198254, acc 1
2016-09-05T21:53:00.255698: step 12161, loss 0.0025432, acc 1
2016-09-05T21:53:01.048056: step 12162, loss 0.00786764, acc 1
2016-09-05T21:53:01.864505: step 12163, loss 0.113009, acc 0.94
2016-09-05T21:53:02.676956: step 12164, loss 0.0252626, acc 1
2016-09-05T21:53:03.470391: step 12165, loss 0.021934, acc 0.98
2016-09-05T21:53:04.292247: step 12166, loss 0.00275942, acc 1
2016-09-05T21:53:05.106274: step 12167, loss 0.0131891, acc 1
2016-09-05T21:53:05.930946: step 12168, loss 0.0146846, acc 1
2016-09-05T21:53:06.749336: step 12169, loss 0.0179074, acc 1
2016-09-05T21:53:07.559382: step 12170, loss 0.0760922, acc 0.98
2016-09-05T21:53:08.372687: step 12171, loss 0.016384, acc 1
2016-09-05T21:53:09.182611: step 12172, loss 0.00355318, acc 1
2016-09-05T21:53:10.000687: step 12173, loss 0.0258257, acc 0.98
2016-09-05T21:53:10.951350: step 12174, loss 0.00745033, acc 1
2016-09-05T21:53:11.783416: step 12175, loss 0.0116138, acc 1
2016-09-05T21:53:12.560642: step 12176, loss 0.0217547, acc 0.98
2016-09-05T21:53:13.376905: step 12177, loss 0.0327664, acc 0.98
2016-09-05T21:53:14.197733: step 12178, loss 0.0162646, acc 1
2016-09-05T21:53:15.021981: step 12179, loss 0.0330503, acc 0.98
2016-09-05T21:53:15.869385: step 12180, loss 0.0718617, acc 0.96
2016-09-05T21:53:16.675381: step 12181, loss 0.0174834, acc 1
2016-09-05T21:53:17.475213: step 12182, loss 0.115552, acc 0.96
2016-09-05T21:53:18.303211: step 12183, loss 0.0144471, acc 1
2016-09-05T21:53:19.118287: step 12184, loss 0.0129655, acc 1
2016-09-05T21:53:19.950163: step 12185, loss 0.00810429, acc 1
2016-09-05T21:53:20.771739: step 12186, loss 0.00568936, acc 1
2016-09-05T21:53:21.574088: step 12187, loss 0.00323094, acc 1
2016-09-05T21:53:22.399547: step 12188, loss 0.00362289, acc 1
2016-09-05T21:53:23.209833: step 12189, loss 0.00671706, acc 1
2016-09-05T21:53:24.016728: step 12190, loss 0.0615695, acc 0.96
2016-09-05T21:53:24.815830: step 12191, loss 0.00310999, acc 1
2016-09-05T21:53:25.650727: step 12192, loss 0.0762366, acc 0.98
2016-09-05T21:53:26.463749: step 12193, loss 0.0405331, acc 0.98
2016-09-05T21:53:27.267166: step 12194, loss 0.0103417, acc 1
2016-09-05T21:53:28.097409: step 12195, loss 0.00320062, acc 1
2016-09-05T21:53:28.911405: step 12196, loss 0.0105917, acc 1
2016-09-05T21:53:29.728148: step 12197, loss 0.00241681, acc 1
2016-09-05T21:53:30.543608: step 12198, loss 0.0275643, acc 0.98
2016-09-05T21:53:31.377193: step 12199, loss 0.0286027, acc 0.98
2016-09-05T21:53:32.190656: step 12200, loss 0.053003, acc 0.96

Evaluation:
2016-09-05T21:53:35.707545: step 12200, loss 1.84325, acc 0.717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-12200

2016-09-05T21:53:37.630379: step 12201, loss 0.0035637, acc 1
2016-09-05T21:53:38.487272: step 12202, loss 0.0104424, acc 1
2016-09-05T21:53:39.321183: step 12203, loss 0.0113259, acc 1
2016-09-05T21:53:40.150571: step 12204, loss 0.00830709, acc 1
2016-09-05T21:53:40.966536: step 12205, loss 0.0120342, acc 1
2016-09-05T21:53:41.806028: step 12206, loss 0.00345203, acc 1
2016-09-05T21:53:42.661808: step 12207, loss 0.00536202, acc 1
2016-09-05T21:53:43.475116: step 12208, loss 0.0111604, acc 1
2016-09-05T21:53:44.279844: step 12209, loss 0.0264195, acc 1
2016-09-05T21:53:45.112525: step 12210, loss 0.00487172, acc 1
2016-09-05T21:53:45.910683: step 12211, loss 0.00299299, acc 1
2016-09-05T21:53:46.709664: step 12212, loss 0.00526844, acc 1
2016-09-05T21:53:47.559802: step 12213, loss 0.0196394, acc 0.98
2016-09-05T21:53:48.372837: step 12214, loss 0.0583864, acc 0.98
2016-09-05T21:53:49.199715: step 12215, loss 0.024422, acc 0.98
2016-09-05T21:53:50.021107: step 12216, loss 0.00218339, acc 1
2016-09-05T21:53:50.847904: step 12217, loss 0.0121449, acc 1
2016-09-05T21:53:51.617398: step 12218, loss 0.0173262, acc 0.98
2016-09-05T21:53:52.427165: step 12219, loss 0.00921663, acc 1
2016-09-05T21:53:53.245585: step 12220, loss 0.017076, acc 1
2016-09-05T21:53:54.043183: step 12221, loss 0.00211021, acc 1
2016-09-05T21:53:54.492375: step 12222, loss 0.0569073, acc 1
2016-09-05T21:53:55.333187: step 12223, loss 0.0628683, acc 0.98
2016-09-05T21:53:56.136444: step 12224, loss 0.0179763, acc 0.98
2016-09-05T21:53:56.937603: step 12225, loss 0.0252395, acc 1
2016-09-05T21:53:57.711897: step 12226, loss 0.00291499, acc 1
2016-09-05T21:53:58.516011: step 12227, loss 0.0250335, acc 0.98
2016-09-05T21:53:59.332940: step 12228, loss 0.0488316, acc 0.96
2016-09-05T21:54:00.137621: step 12229, loss 0.023449, acc 0.98
2016-09-05T21:54:00.961416: step 12230, loss 0.0551898, acc 0.98
2016-09-05T21:54:01.783883: step 12231, loss 0.0349226, acc 0.98
2016-09-05T21:54:02.584719: step 12232, loss 0.0121539, acc 1
2016-09-05T21:54:03.384017: step 12233, loss 0.00579796, acc 1
2016-09-05T21:54:04.198383: step 12234, loss 0.00638666, acc 1
2016-09-05T21:54:04.967074: step 12235, loss 0.0362828, acc 0.98
2016-09-05T21:54:05.766587: step 12236, loss 0.0295145, acc 0.98
2016-09-05T21:54:06.581524: step 12237, loss 0.0105152, acc 1
2016-09-05T21:54:07.392485: step 12238, loss 0.0136829, acc 1
2016-09-05T21:54:08.176423: step 12239, loss 0.0302916, acc 0.98
2016-09-05T21:54:09.022194: step 12240, loss 0.0574281, acc 0.98
2016-09-05T21:54:09.801806: step 12241, loss 0.00523872, acc 1
2016-09-05T21:54:10.608047: step 12242, loss 0.0022434, acc 1
2016-09-05T21:54:11.452516: step 12243, loss 0.00956956, acc 1
2016-09-05T21:54:12.292908: step 12244, loss 0.0831952, acc 0.96
2016-09-05T21:54:13.102834: step 12245, loss 0.00316117, acc 1
2016-09-05T21:54:13.916209: step 12246, loss 0.0363832, acc 0.98
2016-09-05T21:54:14.737250: step 12247, loss 0.00393512, acc 1
2016-09-05T21:54:15.544103: step 12248, loss 0.00853251, acc 1
2016-09-05T21:54:16.391849: step 12249, loss 0.00541132, acc 1
2016-09-05T21:54:17.236338: step 12250, loss 0.0291936, acc 1
2016-09-05T21:54:18.063717: step 12251, loss 0.00411759, acc 1
2016-09-05T21:54:18.896928: step 12252, loss 0.0184562, acc 0.98
2016-09-05T21:54:19.700076: step 12253, loss 0.0148024, acc 1
2016-09-05T21:54:20.519366: step 12254, loss 0.0529404, acc 0.96
2016-09-05T21:54:21.339713: step 12255, loss 0.0278398, acc 1
2016-09-05T21:54:22.143460: step 12256, loss 0.023103, acc 1
2016-09-05T21:54:22.955801: step 12257, loss 0.00481483, acc 1
2016-09-05T21:54:23.778116: step 12258, loss 0.00821348, acc 1
2016-09-05T21:54:24.601522: step 12259, loss 0.0112022, acc 1
2016-09-05T21:54:25.446955: step 12260, loss 0.005877, acc 1
2016-09-05T21:54:26.257447: step 12261, loss 0.00307626, acc 1
2016-09-05T21:54:27.065761: step 12262, loss 0.00254393, acc 1
2016-09-05T21:54:27.864667: step 12263, loss 0.030857, acc 0.96
2016-09-05T21:54:28.693799: step 12264, loss 0.0440032, acc 0.98
2016-09-05T21:54:29.504865: step 12265, loss 0.00418445, acc 1
2016-09-05T21:54:30.299906: step 12266, loss 0.0026483, acc 1
2016-09-05T21:54:31.106206: step 12267, loss 0.00795373, acc 1
2016-09-05T21:54:31.921041: step 12268, loss 0.0301354, acc 1
2016-09-05T21:54:32.698460: step 12269, loss 0.00370061, acc 1
2016-09-05T21:54:33.548672: step 12270, loss 0.010569, acc 1
2016-09-05T21:54:34.393316: step 12271, loss 0.00537148, acc 1
2016-09-05T21:54:35.171372: step 12272, loss 0.0701309, acc 0.98
2016-09-05T21:54:35.975428: step 12273, loss 0.00497074, acc 1
2016-09-05T21:54:36.783802: step 12274, loss 0.0226583, acc 0.98
2016-09-05T21:54:37.578957: step 12275, loss 0.00342095, acc 1
2016-09-05T21:54:38.397194: step 12276, loss 0.00647255, acc 1
2016-09-05T21:54:39.201029: step 12277, loss 0.00612738, acc 1
2016-09-05T21:54:40.030171: step 12278, loss 0.0226543, acc 0.98
2016-09-05T21:54:40.864341: step 12279, loss 0.0119714, acc 1
2016-09-05T21:54:41.721565: step 12280, loss 0.00266379, acc 1
2016-09-05T21:54:42.530529: step 12281, loss 0.056426, acc 0.96
2016-09-05T21:54:43.321813: step 12282, loss 0.00469992, acc 1
2016-09-05T21:54:44.129910: step 12283, loss 0.0360992, acc 1
2016-09-05T21:54:44.937330: step 12284, loss 0.0217404, acc 0.98
2016-09-05T21:54:45.751404: step 12285, loss 0.0173795, acc 1
2016-09-05T21:54:46.569740: step 12286, loss 0.0179875, acc 1
2016-09-05T21:54:47.392170: step 12287, loss 0.0228952, acc 0.98
2016-09-05T21:54:48.209506: step 12288, loss 0.0651168, acc 0.94
2016-09-05T21:54:49.022069: step 12289, loss 0.00657161, acc 1
2016-09-05T21:54:49.821947: step 12290, loss 0.00304824, acc 1
2016-09-05T21:54:50.605387: step 12291, loss 0.0461987, acc 0.96
2016-09-05T21:54:51.429742: step 12292, loss 0.0078619, acc 1
2016-09-05T21:54:52.247181: step 12293, loss 0.00467922, acc 1
2016-09-05T21:54:53.057787: step 12294, loss 0.00765827, acc 1
2016-09-05T21:54:53.885548: step 12295, loss 0.00524205, acc 1
2016-09-05T21:54:54.710382: step 12296, loss 0.0108208, acc 1
2016-09-05T21:54:55.508124: step 12297, loss 0.00338004, acc 1
2016-09-05T21:54:56.353582: step 12298, loss 0.0564135, acc 0.98
2016-09-05T21:54:57.152341: step 12299, loss 0.0215205, acc 1
2016-09-05T21:54:57.987782: step 12300, loss 0.0220956, acc 0.98

Evaluation:
2016-09-05T21:55:01.510876: step 12300, loss 2.531, acc 0.718

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-12300

2016-09-05T21:55:03.457407: step 12301, loss 0.0120916, acc 1
2016-09-05T21:55:04.263704: step 12302, loss 0.0739889, acc 0.96
2016-09-05T21:55:05.033395: step 12303, loss 0.00764875, acc 1
2016-09-05T21:55:05.832531: step 12304, loss 0.0155198, acc 1
2016-09-05T21:55:06.631347: step 12305, loss 0.00794781, acc 1
2016-09-05T21:55:07.451033: step 12306, loss 0.00645616, acc 1
2016-09-05T21:55:08.289376: step 12307, loss 0.00902677, acc 1
2016-09-05T21:55:09.094823: step 12308, loss 0.00332078, acc 1
2016-09-05T21:55:09.882812: step 12309, loss 0.00303596, acc 1
2016-09-05T21:55:10.721120: step 12310, loss 0.00348491, acc 1
2016-09-05T21:55:11.520532: step 12311, loss 0.00688729, acc 1
2016-09-05T21:55:12.305441: step 12312, loss 0.0248284, acc 0.98
2016-09-05T21:55:13.129868: step 12313, loss 0.00324992, acc 1
2016-09-05T21:55:13.906710: step 12314, loss 0.00722213, acc 1
2016-09-05T21:55:14.729391: step 12315, loss 0.00781017, acc 1
2016-09-05T21:55:15.556998: step 12316, loss 0.00387389, acc 1
2016-09-05T21:55:16.348379: step 12317, loss 0.0338852, acc 0.98
2016-09-05T21:55:17.126249: step 12318, loss 0.0186596, acc 1
2016-09-05T21:55:17.945401: step 12319, loss 0.00987242, acc 1
2016-09-05T21:55:18.740406: step 12320, loss 0.138719, acc 0.98
2016-09-05T21:55:19.529030: step 12321, loss 0.0478495, acc 0.98
2016-09-05T21:55:20.367207: step 12322, loss 0.0316457, acc 0.98
2016-09-05T21:55:21.131224: step 12323, loss 0.00510341, acc 1
2016-09-05T21:55:21.934409: step 12324, loss 0.00996209, acc 1
2016-09-05T21:55:22.742859: step 12325, loss 0.0476572, acc 0.98
2016-09-05T21:55:23.528526: step 12326, loss 0.00262929, acc 1
2016-09-05T21:55:24.357474: step 12327, loss 0.0222229, acc 1
2016-09-05T21:55:25.152139: step 12328, loss 0.00273184, acc 1
2016-09-05T21:55:25.927902: step 12329, loss 0.0225815, acc 0.98
2016-09-05T21:55:26.747835: step 12330, loss 0.0214048, acc 0.98
2016-09-05T21:55:27.541961: step 12331, loss 0.016335, acc 1
2016-09-05T21:55:28.350098: step 12332, loss 0.0204829, acc 0.98
2016-09-05T21:55:29.174243: step 12333, loss 0.0128444, acc 1
2016-09-05T21:55:29.989083: step 12334, loss 0.00260097, acc 1
2016-09-05T21:55:30.808291: step 12335, loss 0.0168792, acc 0.98
2016-09-05T21:55:31.595537: step 12336, loss 0.0100716, acc 1
2016-09-05T21:55:32.408189: step 12337, loss 0.037862, acc 0.98
2016-09-05T21:55:33.205334: step 12338, loss 0.0161484, acc 1
2016-09-05T21:55:34.030297: step 12339, loss 0.0278848, acc 0.98
2016-09-05T21:55:34.833094: step 12340, loss 0.0106356, acc 1
2016-09-05T21:55:35.600034: step 12341, loss 0.00276755, acc 1
2016-09-05T21:55:36.456767: step 12342, loss 0.0112318, acc 1
2016-09-05T21:55:37.309760: step 12343, loss 0.0181013, acc 0.98
2016-09-05T21:55:38.131735: step 12344, loss 0.0275338, acc 0.98
2016-09-05T21:55:38.940145: step 12345, loss 0.00268275, acc 1
2016-09-05T21:55:39.774667: step 12346, loss 0.00281528, acc 1
2016-09-05T21:55:40.572490: step 12347, loss 0.00329609, acc 1
2016-09-05T21:55:41.386343: step 12348, loss 0.00369836, acc 1
2016-09-05T21:55:42.227210: step 12349, loss 0.00506897, acc 1
2016-09-05T21:55:43.033504: step 12350, loss 0.0320452, acc 1
2016-09-05T21:55:43.848457: step 12351, loss 0.00323426, acc 1
2016-09-05T21:55:44.683922: step 12352, loss 0.00808048, acc 1
2016-09-05T21:55:45.531132: step 12353, loss 0.0119948, acc 1
2016-09-05T21:55:46.362965: step 12354, loss 0.0133846, acc 1
2016-09-05T21:55:47.193022: step 12355, loss 0.00369306, acc 1
2016-09-05T21:55:48.033663: step 12356, loss 0.0108582, acc 1
2016-09-05T21:55:48.864238: step 12357, loss 0.041069, acc 0.96
2016-09-05T21:55:49.768131: step 12358, loss 0.00670779, acc 1
2016-09-05T21:55:50.634907: step 12359, loss 0.0176078, acc 1
2016-09-05T21:55:51.536254: step 12360, loss 0.00389888, acc 1
2016-09-05T21:55:52.590857: step 12361, loss 0.385699, acc 0.94
2016-09-05T21:55:53.390138: step 12362, loss 0.0037647, acc 1
2016-09-05T21:55:54.214254: step 12363, loss 0.0109656, acc 1
2016-09-05T21:55:55.009179: step 12364, loss 0.0145463, acc 1
2016-09-05T21:55:55.846222: step 12365, loss 0.00953463, acc 1
2016-09-05T21:55:56.675771: step 12366, loss 0.0134179, acc 1
2016-09-05T21:55:57.578224: step 12367, loss 0.0280902, acc 0.98
2016-09-05T21:55:58.391097: step 12368, loss 0.0101408, acc 1
2016-09-05T21:55:59.407193: step 12369, loss 0.010187, acc 1
2016-09-05T21:56:00.253229: step 12370, loss 0.00507576, acc 1
2016-09-05T21:56:01.110336: step 12371, loss 0.00766067, acc 1
2016-09-05T21:56:02.084725: step 12372, loss 0.00727438, acc 1
2016-09-05T21:56:03.089452: step 12373, loss 0.0280011, acc 1
2016-09-05T21:56:03.931516: step 12374, loss 0.0214531, acc 1
2016-09-05T21:56:04.827601: step 12375, loss 0.296553, acc 0.96
2016-09-05T21:56:05.648977: step 12376, loss 0.0247894, acc 0.98
2016-09-05T21:56:06.522683: step 12377, loss 0.11901, acc 0.96
2016-09-05T21:56:07.401700: step 12378, loss 0.0218475, acc 0.98
2016-09-05T21:56:08.227089: step 12379, loss 0.00565006, acc 1
2016-09-05T21:56:09.046377: step 12380, loss 0.0197518, acc 0.98
2016-09-05T21:56:09.859315: step 12381, loss 0.00608962, acc 1
2016-09-05T21:56:10.673411: step 12382, loss 0.00595681, acc 1
2016-09-05T21:56:11.487598: step 12383, loss 0.0081728, acc 1
2016-09-05T21:56:12.480229: step 12384, loss 0.0206592, acc 0.98
2016-09-05T21:56:13.297551: step 12385, loss 0.00640548, acc 1
2016-09-05T21:56:14.132074: step 12386, loss 0.0322379, acc 0.98
2016-09-05T21:56:15.021334: step 12387, loss 0.0232219, acc 0.98
2016-09-05T21:56:15.835733: step 12388, loss 0.0132184, acc 1
2016-09-05T21:56:16.729411: step 12389, loss 0.0110728, acc 1
2016-09-05T21:56:17.567568: step 12390, loss 0.0122009, acc 1
2016-09-05T21:56:18.501503: step 12391, loss 0.0352703, acc 0.98
2016-09-05T21:56:19.355525: step 12392, loss 0.0747344, acc 0.98
2016-09-05T21:56:20.186884: step 12393, loss 0.0321319, acc 0.98
2016-09-05T21:56:21.204334: step 12394, loss 0.0132917, acc 1
2016-09-05T21:56:22.090881: step 12395, loss 0.0197836, acc 1
2016-09-05T21:56:22.999258: step 12396, loss 0.0088911, acc 1
2016-09-05T21:56:24.090537: step 12397, loss 0.0261332, acc 1
2016-09-05T21:56:24.895505: step 12398, loss 0.00751659, acc 1
2016-09-05T21:56:25.726962: step 12399, loss 0.00695825, acc 1
2016-09-05T21:56:26.614947: step 12400, loss 0.00690496, acc 1

Evaluation:
2016-09-05T21:56:30.097512: step 12400, loss 3.73172, acc 0.732

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-12400

2016-09-05T21:56:32.072241: step 12401, loss 0.00708929, acc 1
2016-09-05T21:56:33.050898: step 12402, loss 0.0307248, acc 0.98
2016-09-05T21:56:33.952390: step 12403, loss 0.203762, acc 0.96
2016-09-05T21:56:34.788207: step 12404, loss 0.00665674, acc 1
2016-09-05T21:56:35.689874: step 12405, loss 0.00782108, acc 1
2016-09-05T21:56:36.580642: step 12406, loss 0.00848201, acc 1
2016-09-05T21:56:37.353010: step 12407, loss 0.00631566, acc 1
2016-09-05T21:56:38.169421: step 12408, loss 0.069732, acc 0.98
2016-09-05T21:56:39.010733: step 12409, loss 0.0252156, acc 0.98
2016-09-05T21:56:39.881458: step 12410, loss 0.0219024, acc 0.98
2016-09-05T21:56:40.856422: step 12411, loss 0.0107226, acc 1
2016-09-05T21:56:41.820209: step 12412, loss 0.183359, acc 0.92
2016-09-05T21:56:42.639134: step 12413, loss 0.00577282, acc 1
2016-09-05T21:56:43.447260: step 12414, loss 0.0169864, acc 1
2016-09-05T21:56:44.250319: step 12415, loss 0.00562213, acc 1
2016-09-05T21:56:44.667359: step 12416, loss 0.0177225, acc 1
2016-09-05T21:56:45.564897: step 12417, loss 0.0203284, acc 1
2016-09-05T21:56:46.378477: step 12418, loss 0.00632473, acc 1
2016-09-05T21:56:47.176729: step 12419, loss 0.0302841, acc 0.98
2016-09-05T21:56:48.043882: step 12420, loss 0.0174413, acc 1
2016-09-05T21:56:48.853735: step 12421, loss 0.00543807, acc 1
2016-09-05T21:56:49.656615: step 12422, loss 0.0352752, acc 0.98
2016-09-05T21:56:50.449419: step 12423, loss 0.0191153, acc 0.98
2016-09-05T21:56:51.263626: step 12424, loss 0.0114129, acc 1
2016-09-05T21:56:52.036914: step 12425, loss 0.00987709, acc 1
2016-09-05T21:56:52.897078: step 12426, loss 0.0198336, acc 0.98
2016-09-05T21:56:53.713380: step 12427, loss 0.0201157, acc 0.98
2016-09-05T21:56:54.538242: step 12428, loss 0.00514828, acc 1
2016-09-05T21:56:55.378488: step 12429, loss 0.0385571, acc 0.98
2016-09-05T21:56:56.209615: step 12430, loss 0.0249213, acc 1
2016-09-05T21:56:57.060971: step 12431, loss 0.00504324, acc 1
2016-09-05T21:56:57.852015: step 12432, loss 0.00462561, acc 1
2016-09-05T21:56:58.662096: step 12433, loss 0.0044532, acc 1
2016-09-05T21:56:59.476529: step 12434, loss 0.05164, acc 0.98
2016-09-05T21:57:00.273151: step 12435, loss 0.0509526, acc 0.98
2016-09-05T21:57:01.069714: step 12436, loss 0.0375881, acc 0.98
2016-09-05T21:57:01.915447: step 12437, loss 0.0125244, acc 1
2016-09-05T21:57:02.701196: step 12438, loss 0.0144387, acc 1
2016-09-05T21:57:03.504048: step 12439, loss 0.0393816, acc 0.98
2016-09-05T21:57:04.329393: step 12440, loss 0.00406871, acc 1
2016-09-05T21:57:05.120535: step 12441, loss 0.0190064, acc 1
2016-09-05T21:57:05.925684: step 12442, loss 0.00979486, acc 1
2016-09-05T21:57:06.761335: step 12443, loss 0.00393551, acc 1
2016-09-05T21:57:07.519462: step 12444, loss 0.0137502, acc 1
2016-09-05T21:57:08.331447: step 12445, loss 0.0171325, acc 1
2016-09-05T21:57:09.182527: step 12446, loss 0.00418812, acc 1
2016-09-05T21:57:09.965022: step 12447, loss 0.0458104, acc 0.98
2016-09-05T21:57:10.765696: step 12448, loss 0.123186, acc 0.98
2016-09-05T21:57:11.595742: step 12449, loss 0.014512, acc 1
2016-09-05T21:57:12.379853: step 12450, loss 0.0223313, acc 1
2016-09-05T21:57:13.188534: step 12451, loss 0.016175, acc 1
2016-09-05T21:57:14.012191: step 12452, loss 0.0329227, acc 0.98
2016-09-05T21:57:14.792007: step 12453, loss 0.0039366, acc 1
2016-09-05T21:57:15.619968: step 12454, loss 0.131611, acc 0.98
2016-09-05T21:57:16.417628: step 12455, loss 0.00353521, acc 1
2016-09-05T21:57:17.215955: step 12456, loss 0.00322051, acc 1
2016-09-05T21:57:18.031939: step 12457, loss 0.0151427, acc 1
2016-09-05T21:57:18.854860: step 12458, loss 0.0104679, acc 1
2016-09-05T21:57:19.613130: step 12459, loss 0.0169179, acc 1
2016-09-05T21:57:20.409334: step 12460, loss 0.00909225, acc 1
2016-09-05T21:57:21.207748: step 12461, loss 0.0202185, acc 1
2016-09-05T21:57:22.040616: step 12462, loss 0.0122902, acc 1
2016-09-05T21:57:22.828573: step 12463, loss 0.00798606, acc 1
2016-09-05T21:57:23.630281: step 12464, loss 0.0230543, acc 1
2016-09-05T21:57:24.422208: step 12465, loss 0.0170709, acc 1
2016-09-05T21:57:25.218542: step 12466, loss 0.0246669, acc 1
2016-09-05T21:57:26.035144: step 12467, loss 0.00382887, acc 1
2016-09-05T21:57:26.850820: step 12468, loss 0.00465719, acc 1
2016-09-05T21:57:27.647091: step 12469, loss 0.0239938, acc 0.98
2016-09-05T21:57:28.472382: step 12470, loss 0.0211576, acc 1
2016-09-05T21:57:29.274706: step 12471, loss 0.0387543, acc 0.98
2016-09-05T21:57:30.091151: step 12472, loss 0.0553936, acc 0.96
2016-09-05T21:57:30.932268: step 12473, loss 0.0400208, acc 0.98
2016-09-05T21:57:31.741385: step 12474, loss 0.00274145, acc 1
2016-09-05T21:57:32.516189: step 12475, loss 0.00594242, acc 1
2016-09-05T21:57:33.359342: step 12476, loss 0.0180779, acc 1
2016-09-05T21:57:34.144370: step 12477, loss 0.0534449, acc 0.96
2016-09-05T21:57:34.923476: step 12478, loss 0.00524831, acc 1
2016-09-05T21:57:35.741803: step 12479, loss 0.072501, acc 0.98
2016-09-05T21:57:36.558739: step 12480, loss 0.0198723, acc 0.98
2016-09-05T21:57:37.342267: step 12481, loss 0.0160717, acc 1
2016-09-05T21:57:38.169554: step 12482, loss 0.0103585, acc 1
2016-09-05T21:57:38.940050: step 12483, loss 0.00374805, acc 1
2016-09-05T21:57:39.786292: step 12484, loss 0.0588011, acc 0.96
2016-09-05T21:57:40.592990: step 12485, loss 0.0163625, acc 1
2016-09-05T21:57:41.370915: step 12486, loss 0.0146268, acc 1
2016-09-05T21:57:42.207686: step 12487, loss 0.00672496, acc 1
2016-09-05T21:57:43.026217: step 12488, loss 0.0290199, acc 0.98
2016-09-05T21:57:43.852808: step 12489, loss 0.00274709, acc 1
2016-09-05T21:57:44.664784: step 12490, loss 0.00351548, acc 1
2016-09-05T21:57:45.531867: step 12491, loss 0.00595417, acc 1
2016-09-05T21:57:46.338580: step 12492, loss 0.017902, acc 0.98
2016-09-05T21:57:47.149988: step 12493, loss 0.0122073, acc 1
2016-09-05T21:57:47.997327: step 12494, loss 0.00443018, acc 1
2016-09-05T21:57:48.845568: step 12495, loss 0.11044, acc 0.98
2016-09-05T21:57:49.641988: step 12496, loss 0.068642, acc 0.98
2016-09-05T21:57:50.476043: step 12497, loss 0.0033351, acc 1
2016-09-05T21:57:51.275923: step 12498, loss 0.00431648, acc 1
2016-09-05T21:57:52.080157: step 12499, loss 0.0267791, acc 0.98
2016-09-05T21:57:52.903631: step 12500, loss 0.00347085, acc 1

Evaluation:
2016-09-05T21:57:56.434403: step 12500, loss 2.66382, acc 0.726

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-12500

2016-09-05T21:57:58.287331: step 12501, loss 0.00743363, acc 1
2016-09-05T21:57:59.130461: step 12502, loss 0.00289965, acc 1
2016-09-05T21:57:59.980153: step 12503, loss 0.0127917, acc 1
2016-09-05T21:58:00.787211: step 12504, loss 0.00815062, acc 1
2016-09-05T21:58:01.583347: step 12505, loss 0.0208606, acc 1
2016-09-05T21:58:02.397434: step 12506, loss 0.0293144, acc 1
2016-09-05T21:58:03.189332: step 12507, loss 0.015661, acc 1
2016-09-05T21:58:03.982793: step 12508, loss 0.015566, acc 1
2016-09-05T21:58:04.779680: step 12509, loss 0.0240795, acc 0.98
2016-09-05T21:58:05.564536: step 12510, loss 0.00293859, acc 1
2016-09-05T21:58:06.373589: step 12511, loss 0.0298048, acc 1
2016-09-05T21:58:07.185850: step 12512, loss 0.0130517, acc 1
2016-09-05T21:58:07.971157: step 12513, loss 0.0145558, acc 1
2016-09-05T21:58:08.805537: step 12514, loss 0.040067, acc 0.98
2016-09-05T21:58:09.626985: step 12515, loss 0.0499774, acc 0.98
2016-09-05T21:58:10.395397: step 12516, loss 0.0145076, acc 1
2016-09-05T21:58:11.186772: step 12517, loss 0.0123764, acc 1
2016-09-05T21:58:12.005962: step 12518, loss 0.020649, acc 1
2016-09-05T21:58:12.822533: step 12519, loss 0.0206108, acc 1
2016-09-05T21:58:13.622607: step 12520, loss 0.00344614, acc 1
2016-09-05T21:58:14.449407: step 12521, loss 0.0120954, acc 1
2016-09-05T21:58:15.210192: step 12522, loss 0.00271774, acc 1
2016-09-05T21:58:15.997891: step 12523, loss 0.00452016, acc 1
2016-09-05T21:58:16.834034: step 12524, loss 0.0219171, acc 0.98
2016-09-05T21:58:17.637151: step 12525, loss 0.016178, acc 1
2016-09-05T21:58:18.434672: step 12526, loss 0.00264061, acc 1
2016-09-05T21:58:19.240344: step 12527, loss 0.0481329, acc 0.96
2016-09-05T21:58:20.032855: step 12528, loss 0.00327482, acc 1
2016-09-05T21:58:20.846401: step 12529, loss 0.0367522, acc 0.98
2016-09-05T21:58:21.640817: step 12530, loss 0.0519605, acc 0.98
2016-09-05T21:58:22.459425: step 12531, loss 0.0386173, acc 0.98
2016-09-05T21:58:23.259440: step 12532, loss 0.0321483, acc 0.98
2016-09-05T21:58:24.057779: step 12533, loss 0.0294705, acc 0.98
2016-09-05T21:58:24.866145: step 12534, loss 0.0338831, acc 0.96
2016-09-05T21:58:25.673292: step 12535, loss 0.00291804, acc 1
2016-09-05T21:58:26.477352: step 12536, loss 0.0753427, acc 0.98
2016-09-05T21:58:27.256871: step 12537, loss 0.0288872, acc 0.98
2016-09-05T21:58:28.077497: step 12538, loss 0.0332916, acc 0.98
2016-09-05T21:58:28.922565: step 12539, loss 0.00308956, acc 1
2016-09-05T21:58:29.708857: step 12540, loss 0.00396922, acc 1
2016-09-05T21:58:30.507116: step 12541, loss 0.0257564, acc 0.98
2016-09-05T21:58:31.323848: step 12542, loss 0.0139197, acc 1
2016-09-05T21:58:32.125498: step 12543, loss 0.0269253, acc 1
2016-09-05T21:58:32.940679: step 12544, loss 0.00230957, acc 1
2016-09-05T21:58:33.750241: step 12545, loss 0.00228828, acc 1
2016-09-05T21:58:34.542029: step 12546, loss 0.00218119, acc 1
2016-09-05T21:58:35.348086: step 12547, loss 0.0115185, acc 1
2016-09-05T21:58:36.191715: step 12548, loss 0.00411351, acc 1
2016-09-05T21:58:36.997410: step 12549, loss 0.00308924, acc 1
2016-09-05T21:58:37.802160: step 12550, loss 0.0131217, acc 1
2016-09-05T21:58:38.627801: step 12551, loss 0.00327279, acc 1
2016-09-05T21:58:39.427418: step 12552, loss 0.0219111, acc 0.98
2016-09-05T21:58:40.225068: step 12553, loss 0.00779046, acc 1
2016-09-05T21:58:41.042020: step 12554, loss 0.00456598, acc 1
2016-09-05T21:58:41.813523: step 12555, loss 0.0116873, acc 1
2016-09-05T21:58:42.617243: step 12556, loss 0.0237089, acc 1
2016-09-05T21:58:43.464283: step 12557, loss 0.0431002, acc 0.98
2016-09-05T21:58:44.229131: step 12558, loss 0.0152102, acc 1
2016-09-05T21:58:45.013679: step 12559, loss 0.0209887, acc 0.98
2016-09-05T21:58:45.840352: step 12560, loss 0.0023097, acc 1
2016-09-05T21:58:46.647319: step 12561, loss 0.00908155, acc 1
2016-09-05T21:58:47.439660: step 12562, loss 0.0162978, acc 1
2016-09-05T21:58:48.260797: step 12563, loss 0.00747106, acc 1
2016-09-05T21:58:49.040819: step 12564, loss 0.00798583, acc 1
2016-09-05T21:58:49.871254: step 12565, loss 0.00242446, acc 1
2016-09-05T21:58:50.685953: step 12566, loss 0.00234946, acc 1
2016-09-05T21:58:51.456681: step 12567, loss 0.00745589, acc 1
2016-09-05T21:58:52.247639: step 12568, loss 0.0396521, acc 0.98
2016-09-05T21:58:53.056291: step 12569, loss 0.0401133, acc 0.98
2016-09-05T21:58:53.854120: step 12570, loss 0.0770416, acc 0.94
2016-09-05T21:58:54.654230: step 12571, loss 0.0161784, acc 0.98
2016-09-05T21:58:55.481155: step 12572, loss 0.00292294, acc 1
2016-09-05T21:58:56.252292: step 12573, loss 0.00188638, acc 1
2016-09-05T21:58:57.056734: step 12574, loss 0.0115538, acc 1
2016-09-05T21:58:57.848541: step 12575, loss 0.00767806, acc 1
2016-09-05T21:58:58.661164: step 12576, loss 0.0164594, acc 1
2016-09-05T21:58:59.481926: step 12577, loss 0.0064383, acc 1
2016-09-05T21:59:00.300854: step 12578, loss 0.0356771, acc 0.96
2016-09-05T21:59:01.087489: step 12579, loss 0.0113861, acc 1
2016-09-05T21:59:01.917404: step 12580, loss 0.0112669, acc 1
2016-09-05T21:59:02.742457: step 12581, loss 0.00200742, acc 1
2016-09-05T21:59:03.542816: step 12582, loss 0.0501457, acc 0.98
2016-09-05T21:59:04.336587: step 12583, loss 0.0129034, acc 1
2016-09-05T21:59:05.129398: step 12584, loss 0.0245235, acc 0.98
2016-09-05T21:59:05.914352: step 12585, loss 0.0785288, acc 0.98
2016-09-05T21:59:06.742704: step 12586, loss 0.0145375, acc 1
2016-09-05T21:59:07.577984: step 12587, loss 0.0281045, acc 0.98
2016-09-05T21:59:08.393849: step 12588, loss 0.00254188, acc 1
2016-09-05T21:59:09.211324: step 12589, loss 0.0171848, acc 1
2016-09-05T21:59:10.029367: step 12590, loss 0.0113754, acc 1
2016-09-05T21:59:10.819849: step 12591, loss 0.0110946, acc 1
2016-09-05T21:59:11.628240: step 12592, loss 0.0315757, acc 0.98
2016-09-05T21:59:12.437053: step 12593, loss 0.0039242, acc 1
2016-09-05T21:59:13.257275: step 12594, loss 0.00306695, acc 1
2016-09-05T21:59:14.071935: step 12595, loss 0.0227614, acc 0.98
2016-09-05T21:59:14.866134: step 12596, loss 0.0491866, acc 0.96
2016-09-05T21:59:15.637871: step 12597, loss 0.115577, acc 0.96
2016-09-05T21:59:16.447818: step 12598, loss 0.00685821, acc 1
2016-09-05T21:59:17.254146: step 12599, loss 0.0103926, acc 1
2016-09-05T21:59:18.049747: step 12600, loss 0.0138358, acc 1

Evaluation:
2016-09-05T21:59:21.579025: step 12600, loss 2.38164, acc 0.715

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-12600

2016-09-05T21:59:23.641434: step 12601, loss 0.00398049, acc 1
2016-09-05T21:59:24.444856: step 12602, loss 0.00239324, acc 1
2016-09-05T21:59:25.267999: step 12603, loss 0.00545136, acc 1
2016-09-05T21:59:26.115003: step 12604, loss 0.00361829, acc 1
2016-09-05T21:59:26.946531: step 12605, loss 0.00486447, acc 1
2016-09-05T21:59:27.730987: step 12606, loss 0.0134139, acc 1
2016-09-05T21:59:28.536163: step 12607, loss 0.0148591, acc 1
2016-09-05T21:59:29.354464: step 12608, loss 0.0201232, acc 1
2016-09-05T21:59:30.149611: step 12609, loss 0.0443249, acc 0.98
2016-09-05T21:59:30.592082: step 12610, loss 0.0830943, acc 0.916667
2016-09-05T21:59:31.394385: step 12611, loss 0.0190365, acc 0.98
2016-09-05T21:59:32.185044: step 12612, loss 0.00446425, acc 1
2016-09-05T21:59:32.993231: step 12613, loss 0.0330843, acc 0.98
2016-09-05T21:59:33.785795: step 12614, loss 0.00229912, acc 1
2016-09-05T21:59:34.580923: step 12615, loss 0.00307801, acc 1
2016-09-05T21:59:35.397561: step 12616, loss 0.01585, acc 1
2016-09-05T21:59:36.189669: step 12617, loss 0.00459273, acc 1
2016-09-05T21:59:36.994667: step 12618, loss 0.022217, acc 0.98
2016-09-05T21:59:37.822632: step 12619, loss 0.00217319, acc 1
2016-09-05T21:59:38.632522: step 12620, loss 0.0165902, acc 1
2016-09-05T21:59:39.425494: step 12621, loss 0.00633067, acc 1
2016-09-05T21:59:40.235347: step 12622, loss 0.0224384, acc 0.98
2016-09-05T21:59:41.046707: step 12623, loss 0.0163603, acc 0.98
2016-09-05T21:59:41.848143: step 12624, loss 0.00357157, acc 1
2016-09-05T21:59:42.652500: step 12625, loss 0.00556351, acc 1
2016-09-05T21:59:43.455402: step 12626, loss 0.00373344, acc 1
2016-09-05T21:59:44.250726: step 12627, loss 0.0730337, acc 0.94
2016-09-05T21:59:45.064275: step 12628, loss 0.0165884, acc 0.98
2016-09-05T21:59:45.837706: step 12629, loss 0.0211688, acc 0.98
2016-09-05T21:59:46.650295: step 12630, loss 0.00195568, acc 1
2016-09-05T21:59:47.447791: step 12631, loss 0.00851996, acc 1
2016-09-05T21:59:48.268792: step 12632, loss 0.0226894, acc 0.98
2016-09-05T21:59:49.072381: step 12633, loss 0.00359911, acc 1
2016-09-05T21:59:49.889706: step 12634, loss 0.00298982, acc 1
2016-09-05T21:59:50.686959: step 12635, loss 0.00222664, acc 1
2016-09-05T21:59:51.495411: step 12636, loss 0.0113122, acc 1
2016-09-05T21:59:52.335328: step 12637, loss 0.0020149, acc 1
2016-09-05T21:59:53.134438: step 12638, loss 0.00215179, acc 1
2016-09-05T21:59:53.959249: step 12639, loss 0.0223338, acc 1
2016-09-05T21:59:54.785932: step 12640, loss 0.0370289, acc 1
2016-09-05T21:59:55.556288: step 12641, loss 0.0168986, acc 0.98
2016-09-05T21:59:56.372126: step 12642, loss 0.0696288, acc 0.94
2016-09-05T21:59:57.199810: step 12643, loss 0.00459229, acc 1
2016-09-05T21:59:57.960753: step 12644, loss 0.00488475, acc 1
2016-09-05T21:59:58.783695: step 12645, loss 0.00196924, acc 1
2016-09-05T21:59:59.608054: step 12646, loss 0.0047735, acc 1
2016-09-05T22:00:00.409613: step 12647, loss 0.00198607, acc 1
2016-09-05T22:00:01.217820: step 12648, loss 0.0205938, acc 1
2016-09-05T22:00:02.008884: step 12649, loss 0.00205898, acc 1
2016-09-05T22:00:02.819492: step 12650, loss 0.00182241, acc 1
2016-09-05T22:00:03.601202: step 12651, loss 0.0353136, acc 0.96
2016-09-05T22:00:04.405886: step 12652, loss 0.0185429, acc 1
2016-09-05T22:00:05.212046: step 12653, loss 0.00207603, acc 1
2016-09-05T22:00:06.019014: step 12654, loss 0.00235849, acc 1
2016-09-05T22:00:06.875848: step 12655, loss 0.00213312, acc 1
2016-09-05T22:00:07.648889: step 12656, loss 0.0165244, acc 1
2016-09-05T22:00:08.446766: step 12657, loss 0.00347998, acc 1
2016-09-05T22:00:09.264286: step 12658, loss 0.00204944, acc 1
2016-09-05T22:00:10.051785: step 12659, loss 0.00207729, acc 1
2016-09-05T22:00:10.841445: step 12660, loss 0.0285779, acc 0.98
2016-09-05T22:00:11.643153: step 12661, loss 0.00931573, acc 1
2016-09-05T22:00:12.430509: step 12662, loss 0.00492623, acc 1
2016-09-05T22:00:13.254728: step 12663, loss 0.00502574, acc 1
2016-09-05T22:00:14.076867: step 12664, loss 0.00286443, acc 1
2016-09-05T22:00:14.874129: step 12665, loss 0.00200912, acc 1
2016-09-05T22:00:15.691074: step 12666, loss 0.0840809, acc 0.98
2016-09-05T22:00:16.505037: step 12667, loss 0.00882198, acc 1
2016-09-05T22:00:17.323088: step 12668, loss 0.0018117, acc 1
2016-09-05T22:00:18.127107: step 12669, loss 0.00179039, acc 1
2016-09-05T22:00:18.951918: step 12670, loss 0.00255367, acc 1
2016-09-05T22:00:19.726487: step 12671, loss 0.0054169, acc 1
2016-09-05T22:00:20.536062: step 12672, loss 0.0578255, acc 0.96
2016-09-05T22:00:21.341710: step 12673, loss 0.00331678, acc 1
2016-09-05T22:00:22.117677: step 12674, loss 0.0426654, acc 0.98
2016-09-05T22:00:22.930976: step 12675, loss 0.025431, acc 0.98
2016-09-05T22:00:23.721587: step 12676, loss 0.0188383, acc 0.98
2016-09-05T22:00:24.523347: step 12677, loss 0.0151062, acc 1
2016-09-05T22:00:25.324915: step 12678, loss 0.00174479, acc 1
2016-09-05T22:00:26.129360: step 12679, loss 0.0171915, acc 0.98
2016-09-05T22:00:26.934970: step 12680, loss 0.0170561, acc 0.98
2016-09-05T22:00:27.768325: step 12681, loss 0.00405264, acc 1
2016-09-05T22:00:28.580385: step 12682, loss 0.0151241, acc 1
2016-09-05T22:00:29.382941: step 12683, loss 0.00155857, acc 1
2016-09-05T22:00:30.179165: step 12684, loss 0.005846, acc 1
2016-09-05T22:00:31.016537: step 12685, loss 0.00153164, acc 1
2016-09-05T22:00:31.801001: step 12686, loss 0.0352023, acc 0.96
2016-09-05T22:00:32.588606: step 12687, loss 0.0206565, acc 1
2016-09-05T22:00:33.380999: step 12688, loss 0.0104547, acc 1
2016-09-05T22:00:34.180196: step 12689, loss 0.0156733, acc 1
2016-09-05T22:00:35.001955: step 12690, loss 0.0170959, acc 1
2016-09-05T22:00:35.820656: step 12691, loss 0.0451039, acc 0.98
2016-09-05T22:00:36.599606: step 12692, loss 0.00175995, acc 1
2016-09-05T22:00:37.405573: step 12693, loss 0.00517005, acc 1
2016-09-05T22:00:38.206881: step 12694, loss 0.00211908, acc 1
2016-09-05T22:00:39.006545: step 12695, loss 0.00347105, acc 1
2016-09-05T22:00:39.841373: step 12696, loss 0.0485873, acc 0.98
2016-09-05T22:00:40.688989: step 12697, loss 0.00187364, acc 1
2016-09-05T22:00:41.506045: step 12698, loss 0.0085191, acc 1
2016-09-05T22:00:42.296547: step 12699, loss 0.0187026, acc 0.98
2016-09-05T22:00:43.114777: step 12700, loss 0.0155712, acc 1

Evaluation:
2016-09-05T22:00:46.608283: step 12700, loss 2.44034, acc 0.719

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-12700

2016-09-05T22:00:48.475574: step 12701, loss 0.0121981, acc 1
2016-09-05T22:00:49.292726: step 12702, loss 0.013104, acc 1
2016-09-05T22:00:50.064974: step 12703, loss 0.00167051, acc 1
2016-09-05T22:00:50.919556: step 12704, loss 0.0369644, acc 0.98
2016-09-05T22:00:51.750698: step 12705, loss 0.0140881, acc 1
2016-09-05T22:00:52.555570: step 12706, loss 0.0240289, acc 0.98
2016-09-05T22:00:53.402641: step 12707, loss 0.0156937, acc 1
2016-09-05T22:00:54.203668: step 12708, loss 0.0408666, acc 0.98
2016-09-05T22:00:55.035535: step 12709, loss 0.00312164, acc 1
2016-09-05T22:00:55.816575: step 12710, loss 0.0394145, acc 0.96
2016-09-05T22:00:56.653720: step 12711, loss 0.00247662, acc 1
2016-09-05T22:00:57.480871: step 12712, loss 0.0126502, acc 1
2016-09-05T22:00:58.303715: step 12713, loss 0.00459721, acc 1
2016-09-05T22:00:59.136767: step 12714, loss 0.00506109, acc 1
2016-09-05T22:00:59.929827: step 12715, loss 0.00194775, acc 1
2016-09-05T22:01:00.775626: step 12716, loss 0.0381465, acc 0.98
2016-09-05T22:01:01.596022: step 12717, loss 0.00264808, acc 1
2016-09-05T22:01:02.408787: step 12718, loss 0.0289233, acc 1
2016-09-05T22:01:03.214596: step 12719, loss 0.0650924, acc 0.98
2016-09-05T22:01:04.039113: step 12720, loss 0.00179813, acc 1
2016-09-05T22:01:04.872833: step 12721, loss 0.0229916, acc 0.98
2016-09-05T22:01:05.701473: step 12722, loss 0.00210185, acc 1
2016-09-05T22:01:06.500789: step 12723, loss 0.0409924, acc 0.98
2016-09-05T22:01:07.300168: step 12724, loss 0.0240681, acc 0.98
2016-09-05T22:01:08.077944: step 12725, loss 0.00170404, acc 1
2016-09-05T22:01:08.877996: step 12726, loss 0.0505416, acc 0.98
2016-09-05T22:01:09.706869: step 12727, loss 0.0164239, acc 0.98
2016-09-05T22:01:10.501672: step 12728, loss 0.0517692, acc 0.98
2016-09-05T22:01:11.313565: step 12729, loss 0.0118576, acc 1
2016-09-05T22:01:12.154849: step 12730, loss 0.0132296, acc 1
2016-09-05T22:01:12.952078: step 12731, loss 0.0179334, acc 0.98
2016-09-05T22:01:13.737670: step 12732, loss 0.00271676, acc 1
2016-09-05T22:01:14.561407: step 12733, loss 0.0271246, acc 0.98
2016-09-05T22:01:15.348686: step 12734, loss 0.0026612, acc 1
2016-09-05T22:01:16.139058: step 12735, loss 0.0580862, acc 0.98
2016-09-05T22:01:16.959853: step 12736, loss 0.0210563, acc 0.98
2016-09-05T22:01:17.769267: step 12737, loss 0.043882, acc 0.98
2016-09-05T22:01:18.595569: step 12738, loss 0.0231328, acc 0.98
2016-09-05T22:01:19.410517: step 12739, loss 0.0184422, acc 0.98
2016-09-05T22:01:20.177952: step 12740, loss 0.0608006, acc 0.98
2016-09-05T22:01:20.994030: step 12741, loss 0.016195, acc 1
2016-09-05T22:01:21.822926: step 12742, loss 0.00443135, acc 1
2016-09-05T22:01:22.622857: step 12743, loss 0.0145447, acc 1
2016-09-05T22:01:23.403649: step 12744, loss 0.0258453, acc 1
2016-09-05T22:01:24.203030: step 12745, loss 0.0337621, acc 0.98
2016-09-05T22:01:25.005693: step 12746, loss 0.0483405, acc 0.98
2016-09-05T22:01:25.800062: step 12747, loss 0.0221756, acc 1
2016-09-05T22:01:26.655324: step 12748, loss 0.00386292, acc 1
2016-09-05T22:01:27.441408: step 12749, loss 0.0134631, acc 1
2016-09-05T22:01:28.241719: step 12750, loss 0.0324332, acc 1
2016-09-05T22:01:29.063477: step 12751, loss 0.00472693, acc 1
2016-09-05T22:01:29.833990: step 12752, loss 0.0185349, acc 1
2016-09-05T22:01:30.640690: step 12753, loss 0.0188685, acc 0.98
2016-09-05T22:01:31.465397: step 12754, loss 0.00284747, acc 1
2016-09-05T22:01:32.239320: step 12755, loss 0.00708909, acc 1
2016-09-05T22:01:33.047231: step 12756, loss 0.0372363, acc 0.98
2016-09-05T22:01:33.853001: step 12757, loss 0.00495462, acc 1
2016-09-05T22:01:34.660467: step 12758, loss 0.00420424, acc 1
2016-09-05T22:01:35.469511: step 12759, loss 0.0431673, acc 0.98
2016-09-05T22:01:36.301491: step 12760, loss 0.00390237, acc 1
2016-09-05T22:01:37.085163: step 12761, loss 0.0220988, acc 0.98
2016-09-05T22:01:37.904050: step 12762, loss 0.0252683, acc 1
2016-09-05T22:01:38.745606: step 12763, loss 0.0401221, acc 0.98
2016-09-05T22:01:39.549936: step 12764, loss 0.0146188, acc 1
2016-09-05T22:01:40.367883: step 12765, loss 0.0387775, acc 0.98
2016-09-05T22:01:41.181948: step 12766, loss 0.00832195, acc 1
2016-09-05T22:01:42.001150: step 12767, loss 0.00310144, acc 1
2016-09-05T22:01:42.805709: step 12768, loss 0.00345998, acc 1
2016-09-05T22:01:43.633384: step 12769, loss 0.0035519, acc 1
2016-09-05T22:01:44.436500: step 12770, loss 0.0422092, acc 0.98
2016-09-05T22:01:45.261767: step 12771, loss 0.00308138, acc 1
2016-09-05T22:01:46.096502: step 12772, loss 0.101238, acc 0.98
2016-09-05T22:01:46.884794: step 12773, loss 0.00297718, acc 1
2016-09-05T22:01:47.676935: step 12774, loss 0.00299525, acc 1
2016-09-05T22:01:48.514738: step 12775, loss 0.00286124, acc 1
2016-09-05T22:01:49.311367: step 12776, loss 0.0034763, acc 1
2016-09-05T22:01:50.140475: step 12777, loss 0.00900964, acc 1
2016-09-05T22:01:50.966018: step 12778, loss 0.00284121, acc 1
2016-09-05T22:01:51.803517: step 12779, loss 0.0206957, acc 0.98
2016-09-05T22:01:52.628828: step 12780, loss 0.0392148, acc 1
2016-09-05T22:01:53.457905: step 12781, loss 0.00766939, acc 1
2016-09-05T22:01:54.279353: step 12782, loss 0.00280086, acc 1
2016-09-05T22:01:55.106723: step 12783, loss 0.0198565, acc 0.98
2016-09-05T22:01:55.953559: step 12784, loss 0.00370517, acc 1
2016-09-05T22:01:56.776294: step 12785, loss 0.0361956, acc 0.98
2016-09-05T22:01:57.565597: step 12786, loss 0.00482585, acc 1
2016-09-05T22:01:58.420488: step 12787, loss 0.0040743, acc 1
2016-09-05T22:01:59.232741: step 12788, loss 0.00256732, acc 1
2016-09-05T22:02:00.020614: step 12789, loss 0.0176432, acc 0.98
2016-09-05T22:02:00.915986: step 12790, loss 0.0165542, acc 0.98
2016-09-05T22:02:01.722893: step 12791, loss 0.0209746, acc 1
2016-09-05T22:02:02.500795: step 12792, loss 0.00290718, acc 1
2016-09-05T22:02:03.298860: step 12793, loss 0.0141448, acc 1
2016-09-05T22:02:04.158167: step 12794, loss 0.0194487, acc 0.98
2016-09-05T22:02:04.945368: step 12795, loss 0.0160082, acc 1
2016-09-05T22:02:05.741436: step 12796, loss 0.00247805, acc 1
2016-09-05T22:02:06.539409: step 12797, loss 0.0292636, acc 0.98
2016-09-05T22:02:07.325368: step 12798, loss 0.00239152, acc 1
2016-09-05T22:02:08.174385: step 12799, loss 0.0587159, acc 0.96
2016-09-05T22:02:08.983296: step 12800, loss 0.0207768, acc 0.98

Evaluation:
2016-09-05T22:02:12.477861: step 12800, loss 2.82143, acc 0.703

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-12800

2016-09-05T22:02:14.403009: step 12801, loss 0.069259, acc 0.94
2016-09-05T22:02:15.257834: step 12802, loss 0.021527, acc 0.98
2016-09-05T22:02:16.097892: step 12803, loss 0.00220349, acc 1
2016-09-05T22:02:16.520876: step 12804, loss 0.00215466, acc 1
2016-09-05T22:02:17.332952: step 12805, loss 0.025507, acc 1
2016-09-05T22:02:18.156817: step 12806, loss 0.0216122, acc 0.98
2016-09-05T22:02:18.966351: step 12807, loss 0.0335714, acc 1
2016-09-05T22:02:19.780332: step 12808, loss 0.00226088, acc 1
2016-09-05T22:02:20.579976: step 12809, loss 0.023098, acc 0.98
2016-09-05T22:02:21.410266: step 12810, loss 0.0521599, acc 0.96
2016-09-05T22:02:22.204875: step 12811, loss 0.0164102, acc 1
2016-09-05T22:02:23.015249: step 12812, loss 0.00384697, acc 1
2016-09-05T22:02:23.844274: step 12813, loss 0.0196868, acc 0.98
2016-09-05T22:02:24.670312: step 12814, loss 0.0330685, acc 1
2016-09-05T22:02:25.445491: step 12815, loss 0.00879069, acc 1
2016-09-05T22:02:26.249236: step 12816, loss 0.0038837, acc 1
2016-09-05T22:02:27.078587: step 12817, loss 0.0305803, acc 0.98
2016-09-05T22:02:27.859565: step 12818, loss 0.0319382, acc 0.98
2016-09-05T22:02:28.651697: step 12819, loss 0.00610985, acc 1
2016-09-05T22:02:29.475837: step 12820, loss 0.00800423, acc 1
2016-09-05T22:02:30.263596: step 12821, loss 0.0027954, acc 1
2016-09-05T22:02:31.086324: step 12822, loss 0.00188415, acc 1
2016-09-05T22:02:31.899982: step 12823, loss 0.0220397, acc 0.98
2016-09-05T22:02:32.706449: step 12824, loss 0.00241124, acc 1
2016-09-05T22:02:33.513394: step 12825, loss 0.0026251, acc 1
2016-09-05T22:02:34.341031: step 12826, loss 0.0449496, acc 0.98
2016-09-05T22:02:35.151221: step 12827, loss 0.0027789, acc 1
2016-09-05T22:02:35.932878: step 12828, loss 0.00178825, acc 1
2016-09-05T22:02:36.754180: step 12829, loss 0.00213586, acc 1
2016-09-05T22:02:37.538504: step 12830, loss 0.00179276, acc 1
2016-09-05T22:02:38.333227: step 12831, loss 0.00177263, acc 1
2016-09-05T22:02:39.156191: step 12832, loss 0.0256055, acc 0.98
2016-09-05T22:02:39.945402: step 12833, loss 0.0182474, acc 0.98
2016-09-05T22:02:40.747753: step 12834, loss 0.00703943, acc 1
2016-09-05T22:02:41.558409: step 12835, loss 0.00634435, acc 1
2016-09-05T22:02:42.345251: step 12836, loss 0.0150915, acc 1
2016-09-05T22:02:43.142536: step 12837, loss 0.00186658, acc 1
2016-09-05T22:02:43.953904: step 12838, loss 0.00945991, acc 1
2016-09-05T22:02:44.784192: step 12839, loss 0.00200181, acc 1
2016-09-05T22:02:45.595916: step 12840, loss 0.00585183, acc 1
2016-09-05T22:02:46.401260: step 12841, loss 0.010249, acc 1
2016-09-05T22:02:47.209348: step 12842, loss 0.00971861, acc 1
2016-09-05T22:02:48.017002: step 12843, loss 0.00228181, acc 1
2016-09-05T22:02:48.835335: step 12844, loss 0.0018756, acc 1
2016-09-05T22:02:49.639172: step 12845, loss 0.0137911, acc 1
2016-09-05T22:02:50.438676: step 12846, loss 0.0172844, acc 1
2016-09-05T22:02:51.253811: step 12847, loss 0.00187601, acc 1
2016-09-05T22:02:52.057943: step 12848, loss 0.00216895, acc 1
2016-09-05T22:02:52.870156: step 12849, loss 0.0242595, acc 0.98
2016-09-05T22:02:53.702387: step 12850, loss 0.006959, acc 1
2016-09-05T22:02:54.479024: step 12851, loss 0.00204418, acc 1
2016-09-05T22:02:55.301993: step 12852, loss 0.0672582, acc 0.98
2016-09-05T22:02:56.111539: step 12853, loss 0.00270745, acc 1
2016-09-05T22:02:56.895786: step 12854, loss 0.00474447, acc 1
2016-09-05T22:02:57.713596: step 12855, loss 0.00982289, acc 1
2016-09-05T22:02:58.529323: step 12856, loss 0.0216637, acc 1
2016-09-05T22:02:59.317629: step 12857, loss 0.0418253, acc 0.98
2016-09-05T22:03:00.114560: step 12858, loss 0.00443327, acc 1
2016-09-05T22:03:00.945905: step 12859, loss 0.0154871, acc 1
2016-09-05T22:03:01.740242: step 12860, loss 0.0164832, acc 1
2016-09-05T22:03:02.541991: step 12861, loss 0.0139551, acc 1
2016-09-05T22:03:03.369209: step 12862, loss 0.00283757, acc 1
2016-09-05T22:03:04.153422: step 12863, loss 0.0545788, acc 0.98
2016-09-05T22:03:04.940056: step 12864, loss 0.00222978, acc 1
2016-09-05T22:03:05.781587: step 12865, loss 0.0240883, acc 1
2016-09-05T22:03:06.587992: step 12866, loss 0.0275752, acc 0.98
2016-09-05T22:03:07.351335: step 12867, loss 0.030898, acc 0.98
2016-09-05T22:03:08.175474: step 12868, loss 0.0208433, acc 0.98
2016-09-05T22:03:08.963270: step 12869, loss 0.00382637, acc 1
2016-09-05T22:03:09.750528: step 12870, loss 0.0280254, acc 0.98
2016-09-05T22:03:10.577182: step 12871, loss 0.0020103, acc 1
2016-09-05T22:03:11.391638: step 12872, loss 0.00185792, acc 1
2016-09-05T22:03:12.196433: step 12873, loss 0.00341852, acc 1
2016-09-05T22:03:13.008027: step 12874, loss 0.0130212, acc 1
2016-09-05T22:03:13.835838: step 12875, loss 0.00393645, acc 1
2016-09-05T22:03:14.604409: step 12876, loss 0.0604693, acc 0.96
2016-09-05T22:03:15.424309: step 12877, loss 0.00174737, acc 1
2016-09-05T22:03:16.206619: step 12878, loss 0.00199032, acc 1
2016-09-05T22:03:17.018154: step 12879, loss 0.013915, acc 1
2016-09-05T22:03:17.837146: step 12880, loss 0.0452496, acc 0.98
2016-09-05T22:03:18.630021: step 12881, loss 0.047515, acc 0.98
2016-09-05T22:03:19.446225: step 12882, loss 0.00169912, acc 1
2016-09-05T22:03:20.275675: step 12883, loss 0.013706, acc 1
2016-09-05T22:03:21.046344: step 12884, loss 0.0163444, acc 0.98
2016-09-05T22:03:21.826426: step 12885, loss 0.0198369, acc 1
2016-09-05T22:03:22.645348: step 12886, loss 0.0107472, acc 1
2016-09-05T22:03:23.431001: step 12887, loss 0.0553031, acc 0.98
2016-09-05T22:03:24.258184: step 12888, loss 0.00263893, acc 1
2016-09-05T22:03:25.084752: step 12889, loss 0.00275367, acc 1
2016-09-05T22:03:25.888820: step 12890, loss 0.0140287, acc 1
2016-09-05T22:03:26.696957: step 12891, loss 0.00544476, acc 1
2016-09-05T22:03:27.507385: step 12892, loss 0.0206815, acc 0.98
2016-09-05T22:03:28.295972: step 12893, loss 0.0195678, acc 0.98
2016-09-05T22:03:29.102926: step 12894, loss 0.0381863, acc 1
2016-09-05T22:03:29.936242: step 12895, loss 0.00483117, acc 1
2016-09-05T22:03:30.720645: step 12896, loss 0.00349172, acc 1
2016-09-05T22:03:31.533753: step 12897, loss 0.0432655, acc 0.98
2016-09-05T22:03:32.333965: step 12898, loss 0.00351806, acc 1
2016-09-05T22:03:33.133937: step 12899, loss 0.0305196, acc 0.98
2016-09-05T22:03:33.931488: step 12900, loss 0.0364692, acc 0.98

Evaluation:
2016-09-05T22:03:37.459989: step 12900, loss 2.08884, acc 0.703

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-12900

2016-09-05T22:03:39.264166: step 12901, loss 0.0251543, acc 0.98
2016-09-05T22:03:40.058115: step 12902, loss 0.0181376, acc 1
2016-09-05T22:03:40.879491: step 12903, loss 0.0132723, acc 1
2016-09-05T22:03:41.657744: step 12904, loss 0.0193955, acc 1
2016-09-05T22:03:42.443652: step 12905, loss 0.00388941, acc 1
2016-09-05T22:03:43.270066: step 12906, loss 0.0286618, acc 0.98
2016-09-05T22:03:44.032173: step 12907, loss 0.0931092, acc 0.96
2016-09-05T22:03:44.851877: step 12908, loss 0.00183694, acc 1
2016-09-05T22:03:45.678739: step 12909, loss 0.0172796, acc 1
2016-09-05T22:03:46.483123: step 12910, loss 0.016415, acc 0.98
2016-09-05T22:03:47.273107: step 12911, loss 0.0132575, acc 1
2016-09-05T22:03:48.073356: step 12912, loss 0.00411549, acc 1
2016-09-05T22:03:48.871652: step 12913, loss 0.0131986, acc 1
2016-09-05T22:03:49.666443: step 12914, loss 0.00467228, acc 1
2016-09-05T22:03:50.487126: step 12915, loss 0.0433144, acc 0.98
2016-09-05T22:03:51.282922: step 12916, loss 0.00187809, acc 1
2016-09-05T22:03:52.122529: step 12917, loss 0.0142847, acc 1
2016-09-05T22:03:52.919358: step 12918, loss 0.0173233, acc 0.98
2016-09-05T22:03:53.701224: step 12919, loss 0.00530408, acc 1
2016-09-05T22:03:54.491420: step 12920, loss 0.0286206, acc 1
2016-09-05T22:03:55.300256: step 12921, loss 0.0176882, acc 1
2016-09-05T22:03:56.100941: step 12922, loss 0.0169637, acc 0.98
2016-09-05T22:03:56.907523: step 12923, loss 0.00190977, acc 1
2016-09-05T22:03:57.759886: step 12924, loss 0.00944573, acc 1
2016-09-05T22:03:58.561549: step 12925, loss 0.0219077, acc 0.98
2016-09-05T22:03:59.384619: step 12926, loss 0.00426061, acc 1
2016-09-05T22:04:00.190700: step 12927, loss 0.00193438, acc 1
2016-09-05T22:04:00.990483: step 12928, loss 0.0241043, acc 0.98
2016-09-05T22:04:01.809772: step 12929, loss 0.0303468, acc 0.98
2016-09-05T22:04:02.663295: step 12930, loss 0.00294003, acc 1
2016-09-05T22:04:03.496625: step 12931, loss 0.0297236, acc 0.98
2016-09-05T22:04:04.309586: step 12932, loss 0.00291592, acc 1
2016-09-05T22:04:05.114442: step 12933, loss 0.00197453, acc 1
2016-09-05T22:04:05.922043: step 12934, loss 0.0110271, acc 1
2016-09-05T22:04:06.707380: step 12935, loss 0.00267586, acc 1
2016-09-05T22:04:07.534721: step 12936, loss 0.0019845, acc 1
2016-09-05T22:04:08.345351: step 12937, loss 0.00947782, acc 1
2016-09-05T22:04:09.153891: step 12938, loss 0.00636986, acc 1
2016-09-05T22:04:09.976664: step 12939, loss 0.0507264, acc 0.96
2016-09-05T22:04:10.770701: step 12940, loss 0.0167018, acc 1
2016-09-05T22:04:11.572688: step 12941, loss 0.00205566, acc 1
2016-09-05T22:04:12.401348: step 12942, loss 0.00223356, acc 1
2016-09-05T22:04:13.224000: step 12943, loss 0.0124384, acc 1
2016-09-05T22:04:14.069072: step 12944, loss 0.00952827, acc 1
2016-09-05T22:04:14.898338: step 12945, loss 0.0448886, acc 0.96
2016-09-05T22:04:15.707539: step 12946, loss 0.00555978, acc 1
2016-09-05T22:04:16.523630: step 12947, loss 0.01034, acc 1
2016-09-05T22:04:17.345755: step 12948, loss 0.00470646, acc 1
2016-09-05T22:04:18.170676: step 12949, loss 0.0165993, acc 1
2016-09-05T22:04:19.005915: step 12950, loss 0.0176005, acc 0.98
2016-09-05T22:04:19.811110: step 12951, loss 0.0164668, acc 0.98
2016-09-05T22:04:20.609468: step 12952, loss 0.0446339, acc 0.98
2016-09-05T22:04:21.414027: step 12953, loss 0.00416874, acc 1
2016-09-05T22:04:22.243926: step 12954, loss 0.00228731, acc 1
2016-09-05T22:04:23.064728: step 12955, loss 0.00266522, acc 1
2016-09-05T22:04:23.832860: step 12956, loss 0.0287581, acc 0.98
2016-09-05T22:04:24.663673: step 12957, loss 0.0243891, acc 0.98
2016-09-05T22:04:25.496663: step 12958, loss 0.158481, acc 0.92
2016-09-05T22:04:26.285743: step 12959, loss 0.00576658, acc 1
2016-09-05T22:04:27.101165: step 12960, loss 0.00217578, acc 1
2016-09-05T22:04:27.929051: step 12961, loss 0.00195581, acc 1
2016-09-05T22:04:28.715236: step 12962, loss 0.0725932, acc 0.96
2016-09-05T22:04:29.505336: step 12963, loss 0.00279024, acc 1
2016-09-05T22:04:30.333865: step 12964, loss 0.0065698, acc 1
2016-09-05T22:04:31.162652: step 12965, loss 0.00855656, acc 1
2016-09-05T22:04:31.967466: step 12966, loss 0.0158176, acc 0.98
2016-09-05T22:04:32.797268: step 12967, loss 0.0158964, acc 1
2016-09-05T22:04:33.569674: step 12968, loss 0.004498, acc 1
2016-09-05T22:04:34.374067: step 12969, loss 0.0167904, acc 0.98
2016-09-05T22:04:35.215643: step 12970, loss 0.0133112, acc 1
2016-09-05T22:04:36.000088: step 12971, loss 0.0356492, acc 0.98
2016-09-05T22:04:36.820676: step 12972, loss 0.00397507, acc 1
2016-09-05T22:04:37.672891: step 12973, loss 0.0142852, acc 1
2016-09-05T22:04:38.493619: step 12974, loss 0.0334293, acc 1
2016-09-05T22:04:39.307515: step 12975, loss 0.0265256, acc 0.98
2016-09-05T22:04:40.140789: step 12976, loss 0.00312666, acc 1
2016-09-05T22:04:40.940181: step 12977, loss 0.0177423, acc 1
2016-09-05T22:04:41.751465: step 12978, loss 0.00161569, acc 1
2016-09-05T22:04:42.590571: step 12979, loss 0.00245525, acc 1
2016-09-05T22:04:43.412213: step 12980, loss 0.00437206, acc 1
2016-09-05T22:04:44.234099: step 12981, loss 0.00166876, acc 1
2016-09-05T22:04:45.070488: step 12982, loss 0.00167079, acc 1
2016-09-05T22:04:45.865373: step 12983, loss 0.0358046, acc 0.98
2016-09-05T22:04:46.705421: step 12984, loss 0.0261223, acc 1
2016-09-05T22:04:47.528910: step 12985, loss 0.0598186, acc 0.96
2016-09-05T22:04:48.343794: step 12986, loss 0.00177621, acc 1
2016-09-05T22:04:49.161685: step 12987, loss 0.0279222, acc 0.98
2016-09-05T22:04:49.980816: step 12988, loss 0.0109954, acc 1
2016-09-05T22:04:50.788691: step 12989, loss 0.00352004, acc 1
2016-09-05T22:04:51.597886: step 12990, loss 0.00198493, acc 1
2016-09-05T22:04:52.419629: step 12991, loss 0.0254762, acc 0.98
2016-09-05T22:04:53.237988: step 12992, loss 0.0547453, acc 0.98
2016-09-05T22:04:54.027947: step 12993, loss 0.0344136, acc 0.98
2016-09-05T22:04:54.859823: step 12994, loss 0.0370477, acc 1
2016-09-05T22:04:55.699685: step 12995, loss 0.0349245, acc 0.98
2016-09-05T22:04:56.509442: step 12996, loss 0.0189811, acc 0.98
2016-09-05T22:04:57.325684: step 12997, loss 0.00189155, acc 1
2016-09-05T22:04:57.739575: step 12998, loss 0.0547359, acc 1
2016-09-05T22:04:58.536319: step 12999, loss 0.0397626, acc 0.98
2016-09-05T22:04:59.401624: step 13000, loss 0.00265528, acc 1

Evaluation:
2016-09-05T22:05:02.909494: step 13000, loss 2.25785, acc 0.712

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-13000

2016-09-05T22:05:04.890209: step 13001, loss 0.0325902, acc 0.96
2016-09-05T22:05:05.777003: step 13002, loss 0.0053512, acc 1
2016-09-05T22:05:06.593976: step 13003, loss 0.0135598, acc 1
2016-09-05T22:05:07.397477: step 13004, loss 0.0349619, acc 0.98
2016-09-05T22:05:08.217868: step 13005, loss 0.00768943, acc 1
2016-09-05T22:05:09.054446: step 13006, loss 0.0118585, acc 1
2016-09-05T22:05:09.849440: step 13007, loss 0.00162962, acc 1
2016-09-05T22:05:10.655804: step 13008, loss 0.0186558, acc 0.98
2016-09-05T22:05:11.443405: step 13009, loss 0.00461874, acc 1
2016-09-05T22:05:12.223696: step 13010, loss 0.0255001, acc 0.98
2016-09-05T22:05:13.039781: step 13011, loss 0.0431873, acc 0.96
2016-09-05T22:05:13.850928: step 13012, loss 0.0285124, acc 0.98
2016-09-05T22:05:14.674479: step 13013, loss 0.0165818, acc 0.98
2016-09-05T22:05:15.492083: step 13014, loss 0.00943906, acc 1
2016-09-05T22:05:16.304785: step 13015, loss 0.0252122, acc 0.98
2016-09-05T22:05:17.076966: step 13016, loss 0.0557393, acc 0.96
2016-09-05T22:05:17.901941: step 13017, loss 0.0234627, acc 1
2016-09-05T22:05:18.731231: step 13018, loss 0.00171609, acc 1
2016-09-05T22:05:19.515454: step 13019, loss 0.0178324, acc 1
2016-09-05T22:05:20.344166: step 13020, loss 0.0163508, acc 0.98
2016-09-05T22:05:21.144565: step 13021, loss 0.00811317, acc 1
2016-09-05T22:05:21.942249: step 13022, loss 0.00161426, acc 1
2016-09-05T22:05:22.743265: step 13023, loss 0.0158132, acc 1
2016-09-05T22:05:23.568990: step 13024, loss 0.00384096, acc 1
2016-09-05T22:05:24.369677: step 13025, loss 0.00200044, acc 1
2016-09-05T22:05:25.208605: step 13026, loss 0.0117885, acc 1
2016-09-05T22:05:26.056305: step 13027, loss 0.0222334, acc 0.98
2016-09-05T22:05:26.897716: step 13028, loss 0.00161226, acc 1
2016-09-05T22:05:27.751642: step 13029, loss 0.00218821, acc 1
2016-09-05T22:05:28.584997: step 13030, loss 0.00308453, acc 1
2016-09-05T22:05:29.420622: step 13031, loss 0.0298864, acc 0.98
2016-09-05T22:05:30.245459: step 13032, loss 0.010734, acc 1
2016-09-05T22:05:31.066600: step 13033, loss 0.0149881, acc 1
2016-09-05T22:05:31.894465: step 13034, loss 0.00591202, acc 1
2016-09-05T22:05:32.713426: step 13035, loss 0.0100081, acc 1
2016-09-05T22:05:33.552596: step 13036, loss 0.0179211, acc 0.98
2016-09-05T22:05:34.338356: step 13037, loss 0.00173702, acc 1
2016-09-05T22:05:35.174744: step 13038, loss 0.00175347, acc 1
2016-09-05T22:05:36.000882: step 13039, loss 0.0160101, acc 0.98
2016-09-05T22:05:36.793076: step 13040, loss 0.010704, acc 1
2016-09-05T22:05:37.577408: step 13041, loss 0.00255479, acc 1
2016-09-05T22:05:38.403118: step 13042, loss 0.0044822, acc 1
2016-09-05T22:05:39.207190: step 13043, loss 0.0276765, acc 0.98
2016-09-05T22:05:40.011330: step 13044, loss 0.0158908, acc 1
2016-09-05T22:05:40.822693: step 13045, loss 0.00222026, acc 1
2016-09-05T22:05:41.602378: step 13046, loss 0.00196313, acc 1
2016-09-05T22:05:42.420207: step 13047, loss 0.00491173, acc 1
2016-09-05T22:05:43.257585: step 13048, loss 0.0234567, acc 0.98
2016-09-05T22:05:44.105095: step 13049, loss 0.0583848, acc 0.98
2016-09-05T22:05:44.903018: step 13050, loss 0.00204767, acc 1
2016-09-05T22:05:45.716899: step 13051, loss 0.0129038, acc 1
2016-09-05T22:05:46.541394: step 13052, loss 0.0174332, acc 1
2016-09-05T22:05:47.326067: step 13053, loss 0.00184991, acc 1
2016-09-05T22:05:48.138490: step 13054, loss 0.00772352, acc 1
2016-09-05T22:05:48.943072: step 13055, loss 0.0165114, acc 0.98
2016-09-05T22:05:49.728455: step 13056, loss 0.022598, acc 0.98
2016-09-05T22:05:50.526421: step 13057, loss 0.00175233, acc 1
2016-09-05T22:05:51.343568: step 13058, loss 0.00243749, acc 1
2016-09-05T22:05:52.117441: step 13059, loss 0.0331344, acc 0.96
2016-09-05T22:05:52.933268: step 13060, loss 0.00168956, acc 1
2016-09-05T22:05:53.736267: step 13061, loss 0.0197312, acc 1
2016-09-05T22:05:54.522374: step 13062, loss 0.0168788, acc 0.98
2016-09-05T22:05:55.332066: step 13063, loss 0.033639, acc 0.98
2016-09-05T22:05:56.149626: step 13064, loss 0.0146839, acc 1
2016-09-05T22:05:56.937748: step 13065, loss 0.00917784, acc 1
2016-09-05T22:05:57.793686: step 13066, loss 0.00182464, acc 1
2016-09-05T22:05:58.613148: step 13067, loss 0.0025598, acc 1
2016-09-05T22:05:59.388466: step 13068, loss 0.0615994, acc 0.98
2016-09-05T22:06:00.212158: step 13069, loss 0.00271604, acc 1
2016-09-05T22:06:01.053646: step 13070, loss 0.0241031, acc 0.98
2016-09-05T22:06:01.851879: step 13071, loss 0.00150285, acc 1
2016-09-05T22:06:02.621788: step 13072, loss 0.0196459, acc 1
2016-09-05T22:06:03.417249: step 13073, loss 0.00810516, acc 1
2016-09-05T22:06:04.260100: step 13074, loss 0.0357092, acc 0.96
2016-09-05T22:06:05.069535: step 13075, loss 0.0148751, acc 1
2016-09-05T22:06:05.853266: step 13076, loss 0.0195937, acc 0.98
2016-09-05T22:06:06.650419: step 13077, loss 0.0179566, acc 0.98
2016-09-05T22:06:07.446771: step 13078, loss 0.0136578, acc 1
2016-09-05T22:06:08.249699: step 13079, loss 0.0137399, acc 1
2016-09-05T22:06:09.039653: step 13080, loss 0.00258592, acc 1
2016-09-05T22:06:09.832776: step 13081, loss 0.00908003, acc 1
2016-09-05T22:06:10.656979: step 13082, loss 0.0125897, acc 1
2016-09-05T22:06:11.424793: step 13083, loss 0.0129826, acc 1
2016-09-05T22:06:12.244630: step 13084, loss 0.0366015, acc 0.96
2016-09-05T22:06:13.081968: step 13085, loss 0.00188038, acc 1
2016-09-05T22:06:13.863086: step 13086, loss 0.0248187, acc 0.98
2016-09-05T22:06:14.715953: step 13087, loss 0.00324875, acc 1
2016-09-05T22:06:15.518286: step 13088, loss 0.00428624, acc 1
2016-09-05T22:06:16.308445: step 13089, loss 0.00162005, acc 1
2016-09-05T22:06:17.171877: step 13090, loss 0.0189344, acc 1
2016-09-05T22:06:18.011789: step 13091, loss 0.00159226, acc 1
2016-09-05T22:06:18.848927: step 13092, loss 0.0790694, acc 0.98
2016-09-05T22:06:19.649578: step 13093, loss 0.0238958, acc 0.98
2016-09-05T22:06:20.457528: step 13094, loss 0.00151857, acc 1
2016-09-05T22:06:21.267644: step 13095, loss 0.0709593, acc 0.98
2016-09-05T22:06:22.093630: step 13096, loss 0.0388192, acc 0.96
2016-09-05T22:06:22.917051: step 13097, loss 0.0026621, acc 1
2016-09-05T22:06:23.731099: step 13098, loss 0.0387475, acc 0.98
2016-09-05T22:06:24.537652: step 13099, loss 0.00892541, acc 1
2016-09-05T22:06:25.367123: step 13100, loss 0.0103452, acc 1

Evaluation:
2016-09-05T22:06:28.900108: step 13100, loss 2.14608, acc 0.714

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-13100

2016-09-05T22:06:30.744314: step 13101, loss 0.0253883, acc 0.98
2016-09-05T22:06:31.594546: step 13102, loss 0.0231684, acc 0.98
2016-09-05T22:06:32.406316: step 13103, loss 0.00233006, acc 1
2016-09-05T22:06:33.215677: step 13104, loss 0.0241637, acc 0.98
2016-09-05T22:06:34.019613: step 13105, loss 0.0502207, acc 0.96
2016-09-05T22:06:34.851893: step 13106, loss 0.0185783, acc 0.98
2016-09-05T22:06:35.650822: step 13107, loss 0.0182688, acc 1
2016-09-05T22:06:36.472171: step 13108, loss 0.0193595, acc 0.98
2016-09-05T22:06:37.315065: step 13109, loss 0.0327799, acc 0.96
2016-09-05T22:06:38.100534: step 13110, loss 0.00197164, acc 1
2016-09-05T22:06:38.917711: step 13111, loss 0.0283382, acc 1
2016-09-05T22:06:39.740804: step 13112, loss 0.0219313, acc 1
2016-09-05T22:06:40.511903: step 13113, loss 0.00170419, acc 1
2016-09-05T22:06:41.319670: step 13114, loss 0.0377664, acc 0.98
2016-09-05T22:06:42.163134: step 13115, loss 0.10767, acc 0.98
2016-09-05T22:06:42.968051: step 13116, loss 0.0102489, acc 1
2016-09-05T22:06:43.765798: step 13117, loss 0.00344083, acc 1
2016-09-05T22:06:44.566814: step 13118, loss 0.0159876, acc 1
2016-09-05T22:06:45.337250: step 13119, loss 0.0150059, acc 1
2016-09-05T22:06:46.148558: step 13120, loss 0.0205374, acc 1
2016-09-05T22:06:46.953879: step 13121, loss 0.019606, acc 0.98
2016-09-05T22:06:47.744142: step 13122, loss 0.0108203, acc 1
2016-09-05T22:06:48.522522: step 13123, loss 0.00753346, acc 1
2016-09-05T22:06:49.325432: step 13124, loss 0.0203562, acc 1
2016-09-05T22:06:50.115142: step 13125, loss 0.016541, acc 1
2016-09-05T22:06:50.933521: step 13126, loss 0.00280917, acc 1
2016-09-05T22:06:51.743137: step 13127, loss 0.00523668, acc 1
2016-09-05T22:06:52.525859: step 13128, loss 0.0137862, acc 1
2016-09-05T22:06:53.334607: step 13129, loss 0.0591267, acc 0.96
2016-09-05T22:06:54.165863: step 13130, loss 0.01692, acc 1
2016-09-05T22:06:54.989703: step 13131, loss 0.00345233, acc 1
2016-09-05T22:06:55.833403: step 13132, loss 0.00358009, acc 1
2016-09-05T22:06:56.661970: step 13133, loss 0.0805904, acc 0.98
2016-09-05T22:06:57.434603: step 13134, loss 0.00615681, acc 1
2016-09-05T22:06:58.233917: step 13135, loss 0.00458974, acc 1
2016-09-05T22:06:59.149109: step 13136, loss 0.00862744, acc 1
2016-09-05T22:06:59.982298: step 13137, loss 0.00403277, acc 1
2016-09-05T22:07:00.821596: step 13138, loss 0.00904309, acc 1
2016-09-05T22:07:01.643839: step 13139, loss 0.00521276, acc 1
2016-09-05T22:07:02.463571: step 13140, loss 0.0292963, acc 0.98
2016-09-05T22:07:03.298485: step 13141, loss 0.00974815, acc 1
2016-09-05T22:07:04.132549: step 13142, loss 0.00471806, acc 1
2016-09-05T22:07:04.936612: step 13143, loss 0.0203856, acc 1
2016-09-05T22:07:05.772934: step 13144, loss 0.00743205, acc 1
2016-09-05T22:07:06.624320: step 13145, loss 0.00397024, acc 1
2016-09-05T22:07:07.455830: step 13146, loss 0.00402022, acc 1
2016-09-05T22:07:08.265101: step 13147, loss 0.0225463, acc 0.98
2016-09-05T22:07:09.125284: step 13148, loss 0.00416626, acc 1
2016-09-05T22:07:09.983990: step 13149, loss 0.0368442, acc 0.98
2016-09-05T22:07:10.825752: step 13150, loss 0.0273506, acc 1
2016-09-05T22:07:11.638179: step 13151, loss 0.0192843, acc 0.98
2016-09-05T22:07:12.485626: step 13152, loss 0.0211658, acc 0.98
2016-09-05T22:07:13.323689: step 13153, loss 0.0278458, acc 1
2016-09-05T22:07:14.229946: step 13154, loss 0.00980163, acc 1
2016-09-05T22:07:15.074538: step 13155, loss 0.00376431, acc 1
2016-09-05T22:07:15.886004: step 13156, loss 0.00580205, acc 1
2016-09-05T22:07:16.739430: step 13157, loss 0.00370634, acc 1
2016-09-05T22:07:17.624615: step 13158, loss 0.0281368, acc 0.98
2016-09-05T22:07:18.465211: step 13159, loss 0.00664227, acc 1
2016-09-05T22:07:19.314272: step 13160, loss 0.00371983, acc 1
2016-09-05T22:07:20.134482: step 13161, loss 0.0101109, acc 1
2016-09-05T22:07:20.975649: step 13162, loss 0.00359505, acc 1
2016-09-05T22:07:21.853165: step 13163, loss 0.0035073, acc 1
2016-09-05T22:07:22.691936: step 13164, loss 0.0442277, acc 0.98
2016-09-05T22:07:23.540164: step 13165, loss 0.00342182, acc 1
2016-09-05T22:07:24.405339: step 13166, loss 0.0746612, acc 0.98
2016-09-05T22:07:25.221660: step 13167, loss 0.0136597, acc 1
2016-09-05T22:07:26.055220: step 13168, loss 0.0174228, acc 0.98
2016-09-05T22:07:26.906444: step 13169, loss 0.0198455, acc 0.98
2016-09-05T22:07:27.721393: step 13170, loss 0.0202444, acc 0.98
2016-09-05T22:07:28.556308: step 13171, loss 0.00327472, acc 1
2016-09-05T22:07:29.396963: step 13172, loss 0.0030735, acc 1
2016-09-05T22:07:30.177027: step 13173, loss 0.00295302, acc 1
2016-09-05T22:07:30.998291: step 13174, loss 0.0314346, acc 0.98
2016-09-05T22:07:31.799869: step 13175, loss 0.00433152, acc 1
2016-09-05T22:07:32.632264: step 13176, loss 0.0159777, acc 1
2016-09-05T22:07:33.443681: step 13177, loss 0.00459248, acc 1
2016-09-05T22:07:34.317413: step 13178, loss 0.0244118, acc 0.98
2016-09-05T22:07:35.109254: step 13179, loss 0.00282609, acc 1
2016-09-05T22:07:35.900539: step 13180, loss 0.00269346, acc 1
2016-09-05T22:07:36.723453: step 13181, loss 0.00670709, acc 1
2016-09-05T22:07:37.541000: step 13182, loss 0.0160537, acc 1
2016-09-05T22:07:38.378035: step 13183, loss 0.0178699, acc 0.98
2016-09-05T22:07:39.180076: step 13184, loss 0.00259393, acc 1
2016-09-05T22:07:39.964797: step 13185, loss 0.0214646, acc 0.98
2016-09-05T22:07:40.772512: step 13186, loss 0.006864, acc 1
2016-09-05T22:07:41.614584: step 13187, loss 0.0424739, acc 0.98
2016-09-05T22:07:42.425263: step 13188, loss 0.0150313, acc 1
2016-09-05T22:07:43.250468: step 13189, loss 0.00254277, acc 1
2016-09-05T22:07:44.074037: step 13190, loss 0.0208141, acc 0.98
2016-09-05T22:07:44.879175: step 13191, loss 0.00238878, acc 1
2016-09-05T22:07:45.309040: step 13192, loss 0.0040607, acc 1
2016-09-05T22:07:46.111250: step 13193, loss 0.0167495, acc 1
2016-09-05T22:07:46.923615: step 13194, loss 0.0406259, acc 0.98
2016-09-05T22:07:47.746855: step 13195, loss 0.033298, acc 0.98
2016-09-05T22:07:48.533862: step 13196, loss 0.0710796, acc 0.98
2016-09-05T22:07:49.345571: step 13197, loss 0.0148905, acc 1
2016-09-05T22:07:50.177180: step 13198, loss 0.0235509, acc 1
2016-09-05T22:07:50.979720: step 13199, loss 0.00684075, acc 1
2016-09-05T22:07:51.782012: step 13200, loss 0.00748442, acc 1

Evaluation:
2016-09-05T22:07:55.311330: step 13200, loss 3.24019, acc 0.712

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-13200

2016-09-05T22:07:57.261640: step 13201, loss 0.0028986, acc 1
2016-09-05T22:07:58.036308: step 13202, loss 0.0104878, acc 1
2016-09-05T22:07:58.847793: step 13203, loss 0.029585, acc 0.98
2016-09-05T22:07:59.652600: step 13204, loss 0.00226839, acc 1
2016-09-05T22:08:00.478005: step 13205, loss 0.0172039, acc 1
2016-09-05T22:08:01.284968: step 13206, loss 0.0242812, acc 0.98
2016-09-05T22:08:02.085266: step 13207, loss 0.0021272, acc 1
2016-09-05T22:08:02.862442: step 13208, loss 0.00211591, acc 1
2016-09-05T22:08:03.654131: step 13209, loss 0.020132, acc 0.98
2016-09-05T22:08:04.456379: step 13210, loss 0.00622156, acc 1
2016-09-05T22:08:05.254286: step 13211, loss 0.0153833, acc 1
2016-09-05T22:08:06.056800: step 13212, loss 0.0117545, acc 1
2016-09-05T22:08:06.867621: step 13213, loss 0.0231958, acc 1
2016-09-05T22:08:07.668273: step 13214, loss 0.0227478, acc 0.98
2016-09-05T22:08:08.486179: step 13215, loss 0.00208793, acc 1
2016-09-05T22:08:09.332692: step 13216, loss 0.00208371, acc 1
2016-09-05T22:08:10.098291: step 13217, loss 0.00437107, acc 1
2016-09-05T22:08:10.871098: step 13218, loss 0.00275994, acc 1
2016-09-05T22:08:11.673408: step 13219, loss 0.00207398, acc 1
2016-09-05T22:08:12.468515: step 13220, loss 0.0126976, acc 1
2016-09-05T22:08:13.278734: step 13221, loss 0.0156173, acc 1
2016-09-05T22:08:14.104830: step 13222, loss 0.00363092, acc 1
2016-09-05T22:08:14.893944: step 13223, loss 0.00487078, acc 1
2016-09-05T22:08:15.701057: step 13224, loss 0.0222454, acc 1
2016-09-05T22:08:16.503829: step 13225, loss 0.0036899, acc 1
2016-09-05T22:08:17.300877: step 13226, loss 0.181081, acc 0.98
2016-09-05T22:08:18.121288: step 13227, loss 0.00908253, acc 1
2016-09-05T22:08:18.947438: step 13228, loss 0.016178, acc 0.98
2016-09-05T22:08:19.742153: step 13229, loss 0.00185289, acc 1
2016-09-05T22:08:20.588148: step 13230, loss 0.0248137, acc 0.98
2016-09-05T22:08:21.404561: step 13231, loss 0.00202128, acc 1
2016-09-05T22:08:22.179513: step 13232, loss 0.00422497, acc 1
2016-09-05T22:08:22.953214: step 13233, loss 0.0669214, acc 0.98
2016-09-05T22:08:23.754063: step 13234, loss 0.0158511, acc 0.98
2016-09-05T22:08:24.541493: step 13235, loss 0.0122157, acc 1
2016-09-05T22:08:25.355360: step 13236, loss 0.0123939, acc 1
2016-09-05T22:08:26.159173: step 13237, loss 0.0189414, acc 0.98
2016-09-05T22:08:26.993631: step 13238, loss 0.0141198, acc 1
2016-09-05T22:08:27.801785: step 13239, loss 0.0227979, acc 1
2016-09-05T22:08:28.622221: step 13240, loss 0.0356164, acc 0.98
2016-09-05T22:08:29.412844: step 13241, loss 0.0138259, acc 1
2016-09-05T22:08:30.219105: step 13242, loss 0.029724, acc 1
2016-09-05T22:08:31.039999: step 13243, loss 0.00186158, acc 1
2016-09-05T22:08:31.839312: step 13244, loss 0.0149257, acc 1
2016-09-05T22:08:32.635559: step 13245, loss 0.0117946, acc 1
2016-09-05T22:08:33.424324: step 13246, loss 0.0445562, acc 0.98
2016-09-05T22:08:34.221547: step 13247, loss 0.0194742, acc 1
2016-09-05T22:08:35.050357: step 13248, loss 0.0349253, acc 0.98
2016-09-05T22:08:35.863604: step 13249, loss 0.00972242, acc 1
2016-09-05T22:08:36.658365: step 13250, loss 0.00214902, acc 1
2016-09-05T22:08:37.507619: step 13251, loss 0.0101315, acc 1
2016-09-05T22:08:38.351432: step 13252, loss 0.00210484, acc 1
2016-09-05T22:08:39.129404: step 13253, loss 0.0706909, acc 0.98
2016-09-05T22:08:39.921150: step 13254, loss 0.00260893, acc 1
2016-09-05T22:08:40.759704: step 13255, loss 0.0952407, acc 0.96
2016-09-05T22:08:41.527012: step 13256, loss 0.0221117, acc 1
2016-09-05T22:08:42.349106: step 13257, loss 0.00780361, acc 1
2016-09-05T22:08:43.150943: step 13258, loss 0.0176897, acc 1
2016-09-05T22:08:43.962317: step 13259, loss 0.0136358, acc 1
2016-09-05T22:08:44.756034: step 13260, loss 0.00482892, acc 1
2016-09-05T22:08:45.596284: step 13261, loss 0.00378227, acc 1
2016-09-05T22:08:46.381409: step 13262, loss 0.0114513, acc 1
2016-09-05T22:08:47.169957: step 13263, loss 0.0130152, acc 1
2016-09-05T22:08:47.974435: step 13264, loss 0.0190818, acc 0.98
2016-09-05T22:08:48.773334: step 13265, loss 0.00207305, acc 1
2016-09-05T22:08:49.575723: step 13266, loss 0.092677, acc 0.96
2016-09-05T22:08:50.383412: step 13267, loss 0.00200045, acc 1
2016-09-05T22:08:51.170202: step 13268, loss 0.04651, acc 0.98
2016-09-05T22:08:51.955385: step 13269, loss 0.00834158, acc 1
2016-09-05T22:08:52.747909: step 13270, loss 0.00183461, acc 1
2016-09-05T22:08:53.540559: step 13271, loss 0.00191379, acc 1
2016-09-05T22:08:54.363829: step 13272, loss 0.0023845, acc 1
2016-09-05T22:08:55.160303: step 13273, loss 0.00191918, acc 1
2016-09-05T22:08:55.956158: step 13274, loss 0.0416487, acc 0.98
2016-09-05T22:08:56.778452: step 13275, loss 0.0419169, acc 0.98
2016-09-05T22:08:57.583114: step 13276, loss 0.00175092, acc 1
2016-09-05T22:08:58.393534: step 13277, loss 0.0166596, acc 0.98
2016-09-05T22:08:59.212532: step 13278, loss 0.0140644, acc 1
2016-09-05T22:09:00.037103: step 13279, loss 0.0297313, acc 0.98
2016-09-05T22:09:00.871448: step 13280, loss 0.0156736, acc 1
2016-09-05T22:09:01.682913: step 13281, loss 0.0479656, acc 0.96
2016-09-05T22:09:02.511719: step 13282, loss 0.0298901, acc 1
2016-09-05T22:09:03.308163: step 13283, loss 0.0420633, acc 0.96
2016-09-05T22:09:04.122808: step 13284, loss 0.0110686, acc 1
2016-09-05T22:09:04.951187: step 13285, loss 0.0081064, acc 1
2016-09-05T22:09:05.766546: step 13286, loss 0.0272172, acc 1
2016-09-05T22:09:06.565839: step 13287, loss 0.0559481, acc 0.98
2016-09-05T22:09:07.409029: step 13288, loss 0.00520339, acc 1
2016-09-05T22:09:08.225254: step 13289, loss 0.0133962, acc 1
2016-09-05T22:09:09.033036: step 13290, loss 0.0656354, acc 0.96
2016-09-05T22:09:09.853785: step 13291, loss 0.00502015, acc 1
2016-09-05T22:09:10.683842: step 13292, loss 0.0510334, acc 0.98
2016-09-05T22:09:11.486614: step 13293, loss 0.019367, acc 0.98
2016-09-05T22:09:12.315499: step 13294, loss 0.00787964, acc 1
2016-09-05T22:09:13.172187: step 13295, loss 0.0113012, acc 1
2016-09-05T22:09:13.982227: step 13296, loss 0.0183637, acc 1
2016-09-05T22:09:14.790900: step 13297, loss 0.0183847, acc 0.98
2016-09-05T22:09:15.586183: step 13298, loss 0.0121648, acc 1
2016-09-05T22:09:16.397095: step 13299, loss 0.00787914, acc 1
2016-09-05T22:09:17.235755: step 13300, loss 0.0923886, acc 0.96

Evaluation:
2016-09-05T22:09:20.737496: step 13300, loss 2.84366, acc 0.721

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-13300

2016-09-05T22:09:22.615605: step 13301, loss 0.0236421, acc 1
2016-09-05T22:09:23.445155: step 13302, loss 0.00356637, acc 1
2016-09-05T22:09:24.289479: step 13303, loss 0.0424374, acc 0.98
2016-09-05T22:09:25.065124: step 13304, loss 0.00275783, acc 1
2016-09-05T22:09:25.876809: step 13305, loss 0.0224332, acc 0.98
2016-09-05T22:09:26.693477: step 13306, loss 0.00294876, acc 1
2016-09-05T22:09:27.485071: step 13307, loss 0.00272374, acc 1
2016-09-05T22:09:28.296302: step 13308, loss 0.0165161, acc 1
2016-09-05T22:09:29.125612: step 13309, loss 0.00344107, acc 1
2016-09-05T22:09:29.935709: step 13310, loss 0.0161471, acc 1
2016-09-05T22:09:30.748819: step 13311, loss 0.00285342, acc 1
2016-09-05T22:09:31.595424: step 13312, loss 0.0146112, acc 1
2016-09-05T22:09:32.410755: step 13313, loss 0.0126112, acc 1
2016-09-05T22:09:33.225541: step 13314, loss 0.0205738, acc 0.98
2016-09-05T22:09:34.101320: step 13315, loss 0.0032598, acc 1
2016-09-05T22:09:34.915487: step 13316, loss 0.00367298, acc 1
2016-09-05T22:09:35.951332: step 13317, loss 0.00846923, acc 1
2016-09-05T22:09:36.769813: step 13318, loss 0.003095, acc 1
2016-09-05T22:09:37.601054: step 13319, loss 0.088446, acc 0.96
2016-09-05T22:09:38.402797: step 13320, loss 0.00292263, acc 1
2016-09-05T22:09:39.201926: step 13321, loss 0.00984391, acc 1
2016-09-05T22:09:40.030806: step 13322, loss 0.0266149, acc 0.98
2016-09-05T22:09:40.850371: step 13323, loss 0.0402656, acc 0.98
2016-09-05T22:09:41.838968: step 13324, loss 0.0141211, acc 1
2016-09-05T22:09:42.652331: step 13325, loss 0.00383261, acc 1
2016-09-05T22:09:43.484528: step 13326, loss 0.0239278, acc 0.98
2016-09-05T22:09:44.300521: step 13327, loss 0.0229732, acc 0.98
2016-09-05T22:09:45.306432: step 13328, loss 0.0192135, acc 0.98
2016-09-05T22:09:46.170663: step 13329, loss 0.0263638, acc 0.98
2016-09-05T22:09:47.003781: step 13330, loss 0.00262307, acc 1
2016-09-05T22:09:47.818211: step 13331, loss 0.0137095, acc 1
2016-09-05T22:09:48.651234: step 13332, loss 0.020131, acc 1
2016-09-05T22:09:49.488156: step 13333, loss 0.0195246, acc 0.98
2016-09-05T22:09:50.318077: step 13334, loss 0.0243838, acc 1
2016-09-05T22:09:51.224393: step 13335, loss 0.00762512, acc 1
2016-09-05T22:09:52.081447: step 13336, loss 0.00267329, acc 1
2016-09-05T22:09:52.884402: step 13337, loss 0.0291931, acc 0.98
2016-09-05T22:09:53.849869: step 13338, loss 0.00592999, acc 1
2016-09-05T22:09:54.709469: step 13339, loss 0.0141555, acc 1
2016-09-05T22:09:55.537122: step 13340, loss 0.00861227, acc 1
2016-09-05T22:09:56.336737: step 13341, loss 0.0278133, acc 0.98
2016-09-05T22:09:57.210866: step 13342, loss 0.0130533, acc 1
2016-09-05T22:09:58.048625: step 13343, loss 0.0123582, acc 1
2016-09-05T22:09:58.866020: step 13344, loss 0.0183027, acc 0.98
2016-09-05T22:09:59.697990: step 13345, loss 0.064142, acc 0.98
2016-09-05T22:10:00.565162: step 13346, loss 0.055823, acc 0.98
2016-09-05T22:10:01.357445: step 13347, loss 0.00572832, acc 1
2016-09-05T22:10:02.175065: step 13348, loss 0.0124642, acc 1
2016-09-05T22:10:02.982921: step 13349, loss 0.00319847, acc 1
2016-09-05T22:10:03.752558: step 13350, loss 0.00882148, acc 1
2016-09-05T22:10:04.554721: step 13351, loss 0.00229574, acc 1
2016-09-05T22:10:05.459444: step 13352, loss 0.00268675, acc 1
2016-09-05T22:10:06.260550: step 13353, loss 0.0783679, acc 0.98
2016-09-05T22:10:07.064466: step 13354, loss 0.0148311, acc 1
2016-09-05T22:10:07.899405: step 13355, loss 0.00407293, acc 1
2016-09-05T22:10:08.675683: step 13356, loss 0.00413217, acc 1
2016-09-05T22:10:09.490942: step 13357, loss 0.0155816, acc 1
2016-09-05T22:10:10.331507: step 13358, loss 0.0420063, acc 0.98
2016-09-05T22:10:11.108355: step 13359, loss 0.0339532, acc 0.98
2016-09-05T22:10:11.921329: step 13360, loss 0.0208493, acc 0.98
2016-09-05T22:10:12.780870: step 13361, loss 0.0131844, acc 1
2016-09-05T22:10:13.594043: step 13362, loss 0.0443169, acc 0.98
2016-09-05T22:10:14.399371: step 13363, loss 0.0152052, acc 1
2016-09-05T22:10:15.227785: step 13364, loss 0.0474633, acc 0.98
2016-09-05T22:10:16.037398: step 13365, loss 0.00227551, acc 1
2016-09-05T22:10:16.823708: step 13366, loss 0.00217509, acc 1
2016-09-05T22:10:17.644228: step 13367, loss 0.0157148, acc 1
2016-09-05T22:10:18.433921: step 13368, loss 0.0198678, acc 0.98
2016-09-05T22:10:19.241378: step 13369, loss 0.0184026, acc 0.98
2016-09-05T22:10:20.070529: step 13370, loss 0.00207869, acc 1
2016-09-05T22:10:20.889948: step 13371, loss 0.00664, acc 1
2016-09-05T22:10:21.694291: step 13372, loss 0.0246683, acc 1
2016-09-05T22:10:22.535499: step 13373, loss 0.051094, acc 0.98
2016-09-05T22:10:23.333194: step 13374, loss 0.00319157, acc 1
2016-09-05T22:10:24.140971: step 13375, loss 0.00217274, acc 1
2016-09-05T22:10:24.964266: step 13376, loss 0.0126745, acc 1
2016-09-05T22:10:25.765519: step 13377, loss 0.00308724, acc 1
2016-09-05T22:10:26.583225: step 13378, loss 0.00583077, acc 1
2016-09-05T22:10:27.424096: step 13379, loss 0.026716, acc 1
2016-09-05T22:10:28.279141: step 13380, loss 0.00211888, acc 1
2016-09-05T22:10:29.105432: step 13381, loss 0.0178042, acc 1
2016-09-05T22:10:29.913798: step 13382, loss 0.0312417, acc 0.98
2016-09-05T22:10:30.737034: step 13383, loss 0.0320086, acc 0.98
2016-09-05T22:10:31.550207: step 13384, loss 0.0223794, acc 1
2016-09-05T22:10:32.342226: step 13385, loss 0.00264214, acc 1
2016-09-05T22:10:32.776403: step 13386, loss 0.00223759, acc 1
2016-09-05T22:10:33.561976: step 13387, loss 0.0138643, acc 1
2016-09-05T22:10:34.396246: step 13388, loss 0.0200701, acc 1
2016-09-05T22:10:35.190086: step 13389, loss 0.0348513, acc 0.98
2016-09-05T22:10:36.032165: step 13390, loss 0.0647311, acc 0.98
2016-09-05T22:10:36.844694: step 13391, loss 0.0317614, acc 0.98
2016-09-05T22:10:37.617405: step 13392, loss 0.0201952, acc 0.98
2016-09-05T22:10:38.413957: step 13393, loss 0.0181921, acc 1
2016-09-05T22:10:39.240687: step 13394, loss 0.0147708, acc 1
2016-09-05T22:10:40.015920: step 13395, loss 0.00324243, acc 1
2016-09-05T22:10:40.819742: step 13396, loss 0.0133844, acc 1
2016-09-05T22:10:41.656302: step 13397, loss 0.0138939, acc 1
2016-09-05T22:10:42.434131: step 13398, loss 0.00255651, acc 1
2016-09-05T22:10:43.242157: step 13399, loss 0.0276696, acc 1
2016-09-05T22:10:44.082585: step 13400, loss 0.00278907, acc 1

Evaluation:
2016-09-05T22:10:47.562265: step 13400, loss 3.0204, acc 0.723

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-13400

2016-09-05T22:10:49.485470: step 13401, loss 0.00222092, acc 1
2016-09-05T22:10:50.355706: step 13402, loss 0.00227159, acc 1
2016-09-05T22:10:51.171845: step 13403, loss 0.00385701, acc 1
2016-09-05T22:10:52.017374: step 13404, loss 0.0255029, acc 0.98
2016-09-05T22:10:52.877282: step 13405, loss 0.00475507, acc 1
2016-09-05T22:10:53.692413: step 13406, loss 0.00942159, acc 1
2016-09-05T22:10:54.502633: step 13407, loss 0.0495857, acc 0.98
2016-09-05T22:10:55.347063: step 13408, loss 0.041331, acc 0.98
2016-09-05T22:10:56.163468: step 13409, loss 0.00220346, acc 1
2016-09-05T22:10:56.951733: step 13410, loss 0.0020876, acc 1
2016-09-05T22:10:57.745946: step 13411, loss 0.0122386, acc 1
2016-09-05T22:10:58.566946: step 13412, loss 0.00894419, acc 1
2016-09-05T22:10:59.392563: step 13413, loss 0.0116505, acc 1
2016-09-05T22:11:00.211572: step 13414, loss 0.00205177, acc 1
2016-09-05T22:11:01.047178: step 13415, loss 0.0792589, acc 0.98
2016-09-05T22:11:01.841942: step 13416, loss 0.00993933, acc 1
2016-09-05T22:11:02.651212: step 13417, loss 0.00215653, acc 1
2016-09-05T22:11:03.468551: step 13418, loss 0.0239924, acc 0.98
2016-09-05T22:11:04.257620: step 13419, loss 0.0350646, acc 0.98
2016-09-05T22:11:05.059123: step 13420, loss 0.0165942, acc 0.98
2016-09-05T22:11:05.871909: step 13421, loss 0.0227808, acc 0.98
2016-09-05T22:11:06.635675: step 13422, loss 0.0074631, acc 1
2016-09-05T22:11:07.435479: step 13423, loss 0.00175889, acc 1
2016-09-05T22:11:08.238023: step 13424, loss 0.0442922, acc 0.98
2016-09-05T22:11:09.025410: step 13425, loss 0.0021153, acc 1
2016-09-05T22:11:09.817773: step 13426, loss 0.00216359, acc 1
2016-09-05T22:11:10.643784: step 13427, loss 0.00251067, acc 1
2016-09-05T22:11:11.471268: step 13428, loss 0.0046424, acc 1
2016-09-05T22:11:12.270723: step 13429, loss 0.0589937, acc 0.98
2016-09-05T22:11:13.107599: step 13430, loss 0.00728602, acc 1
2016-09-05T22:11:13.927523: step 13431, loss 0.0020763, acc 1
2016-09-05T22:11:14.727692: step 13432, loss 0.0335316, acc 0.98
2016-09-05T22:11:15.551486: step 13433, loss 0.0216738, acc 0.98
2016-09-05T22:11:16.375442: step 13434, loss 0.00195381, acc 1
2016-09-05T22:11:17.171662: step 13435, loss 0.00154522, acc 1
2016-09-05T22:11:17.973123: step 13436, loss 0.0235362, acc 0.98
2016-09-05T22:11:18.779879: step 13437, loss 0.0363396, acc 0.98
2016-09-05T22:11:19.593349: step 13438, loss 0.0238011, acc 0.98
2016-09-05T22:11:20.431159: step 13439, loss 0.00170348, acc 1
2016-09-05T22:11:21.239387: step 13440, loss 0.00749671, acc 1
2016-09-05T22:11:22.062555: step 13441, loss 0.00419998, acc 1
2016-09-05T22:11:22.890318: step 13442, loss 0.00706624, acc 1
2016-09-05T22:11:23.693772: step 13443, loss 0.0345872, acc 0.96
2016-09-05T22:11:24.495937: step 13444, loss 0.00220035, acc 1
2016-09-05T22:11:25.300106: step 13445, loss 0.00995935, acc 1
2016-09-05T22:11:26.097377: step 13446, loss 0.0110835, acc 1
2016-09-05T22:11:26.882505: step 13447, loss 0.0283793, acc 0.98
2016-09-05T22:11:27.709800: step 13448, loss 0.00891695, acc 1
2016-09-05T22:11:28.508070: step 13449, loss 0.0147677, acc 1
2016-09-05T22:11:29.328659: step 13450, loss 0.0259602, acc 0.98
2016-09-05T22:11:30.163273: step 13451, loss 0.00178964, acc 1
2016-09-05T22:11:30.970437: step 13452, loss 0.00528701, acc 1
2016-09-05T22:11:31.787277: step 13453, loss 0.0177733, acc 0.98
2016-09-05T22:11:32.625785: step 13454, loss 0.00188923, acc 1
2016-09-05T22:11:33.438124: step 13455, loss 0.00294944, acc 1
2016-09-05T22:11:34.242700: step 13456, loss 0.0262117, acc 1
2016-09-05T22:11:35.076137: step 13457, loss 0.00273729, acc 1
2016-09-05T22:11:35.878348: step 13458, loss 0.0140834, acc 1
2016-09-05T22:11:36.699837: step 13459, loss 0.00190562, acc 1
2016-09-05T22:11:37.516662: step 13460, loss 0.0018399, acc 1
2016-09-05T22:11:38.318506: step 13461, loss 0.0131119, acc 1
2016-09-05T22:11:39.146756: step 13462, loss 0.0266941, acc 1
2016-09-05T22:11:39.968181: step 13463, loss 0.0136501, acc 1
2016-09-05T22:11:40.776189: step 13464, loss 0.0617023, acc 0.98
2016-09-05T22:11:41.586210: step 13465, loss 0.00520363, acc 1
2016-09-05T22:11:42.426732: step 13466, loss 0.00472344, acc 1
2016-09-05T22:11:43.206270: step 13467, loss 0.0130031, acc 1
2016-09-05T22:11:44.033575: step 13468, loss 0.00279152, acc 1
2016-09-05T22:11:44.868741: step 13469, loss 0.00181181, acc 1
2016-09-05T22:11:45.667190: step 13470, loss 0.0146849, acc 1
2016-09-05T22:11:46.440305: step 13471, loss 0.0126651, acc 1
2016-09-05T22:11:47.269071: step 13472, loss 0.0132451, acc 1
2016-09-05T22:11:48.105303: step 13473, loss 0.00251799, acc 1
2016-09-05T22:11:48.911704: step 13474, loss 0.00188129, acc 1
2016-09-05T22:11:49.718113: step 13475, loss 0.0221472, acc 0.98
2016-09-05T22:11:50.523149: step 13476, loss 0.0150333, acc 1
2016-09-05T22:11:51.327744: step 13477, loss 0.0316605, acc 0.98
2016-09-05T22:11:52.129455: step 13478, loss 0.0426868, acc 0.96
2016-09-05T22:11:52.941453: step 13479, loss 0.00163338, acc 1
2016-09-05T22:11:53.747958: step 13480, loss 0.0169181, acc 0.98
2016-09-05T22:11:54.543654: step 13481, loss 0.0190382, acc 0.98
2016-09-05T22:11:55.380860: step 13482, loss 0.00179614, acc 1
2016-09-05T22:11:56.167429: step 13483, loss 0.00229994, acc 1
2016-09-05T22:11:56.951526: step 13484, loss 0.0290437, acc 0.98
2016-09-05T22:11:57.764268: step 13485, loss 0.00443956, acc 1
2016-09-05T22:11:58.542666: step 13486, loss 0.0156902, acc 1
2016-09-05T22:11:59.356801: step 13487, loss 0.0313, acc 0.98
2016-09-05T22:12:00.152363: step 13488, loss 0.00661961, acc 1
2016-09-05T22:12:00.994906: step 13489, loss 0.0105887, acc 1
2016-09-05T22:12:01.790868: step 13490, loss 0.00219591, acc 1
2016-09-05T22:12:02.603015: step 13491, loss 0.00275836, acc 1
2016-09-05T22:12:03.410945: step 13492, loss 0.00320756, acc 1
2016-09-05T22:12:04.209026: step 13493, loss 0.00390799, acc 1
2016-09-05T22:12:05.014605: step 13494, loss 0.00400492, acc 1
2016-09-05T22:12:05.835805: step 13495, loss 0.00282133, acc 1
2016-09-05T22:12:06.644116: step 13496, loss 0.00258461, acc 1
2016-09-05T22:12:07.465539: step 13497, loss 0.00917993, acc 1
2016-09-05T22:12:08.267515: step 13498, loss 0.00153931, acc 1
2016-09-05T22:12:09.057725: step 13499, loss 0.0088508, acc 1
2016-09-05T22:12:09.880429: step 13500, loss 0.0151407, acc 1

Evaluation:
2016-09-05T22:12:13.377376: step 13500, loss 2.96252, acc 0.725

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-13500

2016-09-05T22:12:15.267689: step 13501, loss 0.0691178, acc 0.98
2016-09-05T22:12:16.113141: step 13502, loss 0.0139788, acc 1
2016-09-05T22:12:16.902034: step 13503, loss 0.00401341, acc 1
2016-09-05T22:12:17.672181: step 13504, loss 0.0109297, acc 1
2016-09-05T22:12:18.521466: step 13505, loss 0.00161969, acc 1
2016-09-05T22:12:19.313128: step 13506, loss 0.0192397, acc 1
2016-09-05T22:12:20.122849: step 13507, loss 0.00233173, acc 1
2016-09-05T22:12:20.959700: step 13508, loss 0.0123056, acc 1
2016-09-05T22:12:21.769057: step 13509, loss 0.00160116, acc 1
2016-09-05T22:12:22.576257: step 13510, loss 0.00159614, acc 1
2016-09-05T22:12:23.409471: step 13511, loss 0.0151534, acc 1
2016-09-05T22:12:24.216694: step 13512, loss 0.0262552, acc 0.98
2016-09-05T22:12:25.026651: step 13513, loss 0.12621, acc 0.96
2016-09-05T22:12:25.853447: step 13514, loss 0.00342624, acc 1
2016-09-05T22:12:26.670391: step 13515, loss 0.00161411, acc 1
2016-09-05T22:12:27.451279: step 13516, loss 0.0017668, acc 1
2016-09-05T22:12:28.294108: step 13517, loss 0.0124412, acc 1
2016-09-05T22:12:29.128725: step 13518, loss 0.0123163, acc 1
2016-09-05T22:12:29.927051: step 13519, loss 0.00485595, acc 1
2016-09-05T22:12:30.746166: step 13520, loss 0.0345131, acc 0.96
2016-09-05T22:12:31.567327: step 13521, loss 0.0268518, acc 0.98
2016-09-05T22:12:32.374244: step 13522, loss 0.203225, acc 0.96
2016-09-05T22:12:33.205193: step 13523, loss 0.00286294, acc 1
2016-09-05T22:12:34.021035: step 13524, loss 0.00238533, acc 1
2016-09-05T22:12:34.831360: step 13525, loss 0.0159485, acc 1
2016-09-05T22:12:35.659168: step 13526, loss 0.0438664, acc 1
2016-09-05T22:12:36.483205: step 13527, loss 0.10721, acc 0.94
2016-09-05T22:12:37.281039: step 13528, loss 0.00672242, acc 1
2016-09-05T22:12:38.081554: step 13529, loss 0.0393836, acc 0.98
2016-09-05T22:12:38.990126: step 13530, loss 0.0211287, acc 0.98
2016-09-05T22:12:39.853125: step 13531, loss 0.0139845, acc 1
2016-09-05T22:12:40.772825: step 13532, loss 0.274011, acc 0.98
2016-09-05T22:12:41.922752: step 13533, loss 0.0207252, acc 0.98
2016-09-05T22:12:42.986007: step 13534, loss 0.0120727, acc 1
2016-09-05T22:12:44.188930: step 13535, loss 0.207549, acc 0.98
2016-09-05T22:12:45.485606: step 13536, loss 0.00808802, acc 1
2016-09-05T22:12:46.950272: step 13537, loss 0.0383348, acc 0.98
2016-09-05T22:12:48.024706: step 13538, loss 0.0226282, acc 1
2016-09-05T22:12:49.073647: step 13539, loss 0.0623347, acc 0.96
2016-09-05T22:12:50.439451: step 13540, loss 0.00972662, acc 1
2016-09-05T22:12:51.875662: step 13541, loss 0.0294414, acc 0.98
2016-09-05T22:12:53.141254: step 13542, loss 0.010123, acc 1
2016-09-05T22:12:54.314599: step 13543, loss 0.0103384, acc 1
2016-09-05T22:12:55.562983: step 13544, loss 0.0118865, acc 1
2016-09-05T22:12:56.781213: step 13545, loss 0.0185999, acc 1
2016-09-05T22:12:57.925350: step 13546, loss 0.0799166, acc 0.98
2016-09-05T22:12:59.184192: step 13547, loss 0.0160614, acc 1
2016-09-05T22:13:00.770638: step 13548, loss 0.0109409, acc 1
2016-09-05T22:13:02.032913: step 13549, loss 0.0113307, acc 1
2016-09-05T22:13:03.384283: step 13550, loss 0.0442241, acc 0.98
2016-09-05T22:13:04.943643: step 13551, loss 0.0951016, acc 0.98
2016-09-05T22:13:06.252204: step 13552, loss 0.0121718, acc 1
2016-09-05T22:13:07.444046: step 13553, loss 0.0116084, acc 1
2016-09-05T22:13:08.671286: step 13554, loss 0.0127796, acc 1
2016-09-05T22:13:09.975568: step 13555, loss 0.0278429, acc 0.98
2016-09-05T22:13:11.083318: step 13556, loss 0.0302136, acc 1
2016-09-05T22:13:12.236191: step 13557, loss 0.0108204, acc 1
2016-09-05T22:13:13.437377: step 13558, loss 0.0104441, acc 1
2016-09-05T22:13:14.786684: step 13559, loss 0.0103321, acc 1
2016-09-05T22:13:15.984644: step 13560, loss 0.0103063, acc 1
2016-09-05T22:13:16.888706: step 13561, loss 0.0102431, acc 1
2016-09-05T22:13:17.891616: step 13562, loss 0.0100555, acc 1
2016-09-05T22:13:19.012987: step 13563, loss 0.0100588, acc 1
2016-09-05T22:13:20.268916: step 13564, loss 0.012918, acc 1
2016-09-05T22:13:21.341960: step 13565, loss 0.0218253, acc 1
2016-09-05T22:13:22.742971: step 13566, loss 0.0333739, acc 1
2016-09-05T22:13:24.162655: step 13567, loss 0.00989166, acc 1
2016-09-05T22:13:25.442227: step 13568, loss 0.00981943, acc 1
2016-09-05T22:13:26.621684: step 13569, loss 0.0111, acc 1
2016-09-05T22:13:27.844742: step 13570, loss 0.0507552, acc 0.96
2016-09-05T22:13:29.344392: step 13571, loss 0.0260076, acc 0.98
2016-09-05T22:13:30.446710: step 13572, loss 0.0282867, acc 0.98
2016-09-05T22:13:31.548809: step 13573, loss 0.0102398, acc 1
2016-09-05T22:13:32.704308: step 13574, loss 0.0089878, acc 1
2016-09-05T22:13:33.759605: step 13575, loss 0.0666797, acc 0.98
2016-09-05T22:13:34.747402: step 13576, loss 0.0925788, acc 0.98
2016-09-05T22:13:35.786609: step 13577, loss 0.0107069, acc 1
2016-09-05T22:13:36.847039: step 13578, loss 0.0253481, acc 0.98
2016-09-05T22:13:37.863190: step 13579, loss 0.0453967, acc 0.98
2016-09-05T22:13:38.344314: step 13580, loss 0.0258096, acc 1
2016-09-05T22:13:39.435010: step 13581, loss 0.0218375, acc 1
2016-09-05T22:13:40.683298: step 13582, loss 0.0484765, acc 0.98
2016-09-05T22:13:41.670351: step 13583, loss 0.0134789, acc 1
2016-09-05T22:13:42.838930: step 13584, loss 0.0220246, acc 1
2016-09-05T22:13:43.843114: step 13585, loss 0.00781429, acc 1
2016-09-05T22:13:45.191722: step 13586, loss 0.00832214, acc 1
2016-09-05T22:13:46.496359: step 13587, loss 0.00766403, acc 1
2016-09-05T22:13:47.597083: step 13588, loss 0.0104501, acc 1
2016-09-05T22:13:48.730982: step 13589, loss 0.00772239, acc 1
2016-09-05T22:13:50.146216: step 13590, loss 0.0102048, acc 1
2016-09-05T22:13:51.593679: step 13591, loss 0.00757784, acc 1
2016-09-05T22:13:53.137762: step 13592, loss 0.00786829, acc 1
2016-09-05T22:13:54.501594: step 13593, loss 0.0458573, acc 1
2016-09-05T22:13:55.674691: step 13594, loss 0.0161866, acc 1
2016-09-05T22:13:57.065262: step 13595, loss 0.0607525, acc 0.96
2016-09-05T22:13:58.437527: step 13596, loss 0.00882177, acc 1
2016-09-05T22:13:59.572251: step 13597, loss 0.00726746, acc 1
2016-09-05T22:14:00.724064: step 13598, loss 0.00725522, acc 1
2016-09-05T22:14:01.630303: step 13599, loss 0.153186, acc 0.98
2016-09-05T22:14:02.491940: step 13600, loss 0.0073814, acc 1

Evaluation:
2016-09-05T22:14:05.989303: step 13600, loss 4.46275, acc 0.72

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-13600

2016-09-05T22:14:08.024826: step 13601, loss 0.00756132, acc 1
2016-09-05T22:14:09.192352: step 13602, loss 0.0195888, acc 1
2016-09-05T22:14:10.116949: step 13603, loss 0.0267073, acc 0.98
2016-09-05T22:14:11.191110: step 13604, loss 0.0127158, acc 1
2016-09-05T22:14:12.281106: step 13605, loss 0.0934012, acc 0.96
2016-09-05T22:14:13.267649: step 13606, loss 0.00755822, acc 1
2016-09-05T22:14:14.213915: step 13607, loss 0.0151223, acc 1
2016-09-05T22:14:15.050493: step 13608, loss 0.00835441, acc 1
2016-09-05T22:14:15.913548: step 13609, loss 0.00823227, acc 1
2016-09-05T22:14:16.762161: step 13610, loss 0.0133552, acc 1
2016-09-05T22:14:17.781767: step 13611, loss 0.0157279, acc 1
2016-09-05T22:14:18.759138: step 13612, loss 0.0539424, acc 0.98
2016-09-05T22:14:19.557551: step 13613, loss 0.0167474, acc 1
2016-09-05T22:14:20.553736: step 13614, loss 0.0396726, acc 0.98
2016-09-05T22:14:21.380379: step 13615, loss 0.074991, acc 0.98
2016-09-05T22:14:22.346818: step 13616, loss 0.0217738, acc 0.98
2016-09-05T22:14:23.173945: step 13617, loss 0.00724046, acc 1
2016-09-05T22:14:24.101808: step 13618, loss 0.0129056, acc 1
2016-09-05T22:14:25.096546: step 13619, loss 0.00830234, acc 1
2016-09-05T22:14:25.907177: step 13620, loss 0.00689341, acc 1
2016-09-05T22:14:26.812604: step 13621, loss 0.0733199, acc 0.98
2016-09-05T22:14:27.660082: step 13622, loss 0.109965, acc 0.98
2016-09-05T22:14:28.549495: step 13623, loss 0.00647028, acc 1
2016-09-05T22:14:29.427430: step 13624, loss 0.0464854, acc 0.96
2016-09-05T22:14:30.233578: step 13625, loss 0.0263044, acc 1
2016-09-05T22:14:31.038837: step 13626, loss 0.00854987, acc 1
2016-09-05T22:14:31.842775: step 13627, loss 0.0274056, acc 0.98
2016-09-05T22:14:32.703143: step 13628, loss 0.00712213, acc 1
2016-09-05T22:14:33.574641: step 13629, loss 0.0490737, acc 0.98
2016-09-05T22:14:34.409339: step 13630, loss 0.00619753, acc 1
2016-09-05T22:14:35.285451: step 13631, loss 0.0275622, acc 1
2016-09-05T22:14:36.111452: step 13632, loss 0.0245405, acc 1
2016-09-05T22:14:36.961426: step 13633, loss 0.030098, acc 0.98
2016-09-05T22:14:37.768391: step 13634, loss 0.0119657, acc 1
2016-09-05T22:14:38.590363: step 13635, loss 0.0257446, acc 0.98
2016-09-05T22:14:39.494862: step 13636, loss 0.00611246, acc 1
2016-09-05T22:14:40.348256: step 13637, loss 0.00562207, acc 1
2016-09-05T22:14:41.234755: step 13638, loss 0.00676797, acc 1
2016-09-05T22:14:42.037359: step 13639, loss 0.0182287, acc 1
2016-09-05T22:14:42.860877: step 13640, loss 0.0314753, acc 0.98
2016-09-05T22:14:43.697157: step 13641, loss 0.00603712, acc 1
2016-09-05T22:14:44.515836: step 13642, loss 0.00497957, acc 1
2016-09-05T22:14:45.361739: step 13643, loss 0.0351841, acc 0.96
2016-09-05T22:14:46.168278: step 13644, loss 0.0401972, acc 0.98
2016-09-05T22:14:47.008906: step 13645, loss 0.061115, acc 0.96
2016-09-05T22:14:47.841072: step 13646, loss 0.0135812, acc 1
2016-09-05T22:14:48.659782: step 13647, loss 0.0123912, acc 1
2016-09-05T22:14:49.483993: step 13648, loss 0.0342513, acc 0.98
2016-09-05T22:14:50.299919: step 13649, loss 0.0052384, acc 1
2016-09-05T22:14:51.126145: step 13650, loss 0.022446, acc 1
2016-09-05T22:14:51.927326: step 13651, loss 0.00553359, acc 1
2016-09-05T22:14:52.778729: step 13652, loss 0.0280959, acc 1
2016-09-05T22:14:53.683319: step 13653, loss 0.00454885, acc 1
2016-09-05T22:14:54.494044: step 13654, loss 0.00690814, acc 1
2016-09-05T22:14:55.324860: step 13655, loss 0.0230532, acc 1
2016-09-05T22:14:56.214611: step 13656, loss 0.0434278, acc 0.96
2016-09-05T22:14:57.108582: step 13657, loss 0.0147442, acc 1
2016-09-05T22:14:57.954534: step 13658, loss 0.160236, acc 0.96
2016-09-05T22:14:58.748559: step 13659, loss 0.00432369, acc 1
2016-09-05T22:14:59.568280: step 13660, loss 0.0354785, acc 0.98
2016-09-05T22:15:00.422470: step 13661, loss 0.0119066, acc 1
2016-09-05T22:15:01.200688: step 13662, loss 0.0210264, acc 1
2016-09-05T22:15:02.009340: step 13663, loss 0.0196878, acc 1
2016-09-05T22:15:02.811192: step 13664, loss 0.00412028, acc 1
2016-09-05T22:15:03.613181: step 13665, loss 0.0259469, acc 0.98
2016-09-05T22:15:04.450622: step 13666, loss 0.0329548, acc 0.98
2016-09-05T22:15:05.220624: step 13667, loss 0.0109918, acc 1
2016-09-05T22:15:06.019414: step 13668, loss 0.011674, acc 1
2016-09-05T22:15:06.840693: step 13669, loss 0.00668655, acc 1
2016-09-05T22:15:07.626612: step 13670, loss 0.0494777, acc 0.98
2016-09-05T22:15:08.433838: step 13671, loss 0.018445, acc 1
2016-09-05T22:15:09.245998: step 13672, loss 0.0183197, acc 1
2016-09-05T22:15:10.049044: step 13673, loss 0.0176776, acc 1
2016-09-05T22:15:10.840401: step 13674, loss 0.0120678, acc 1
2016-09-05T22:15:11.659256: step 13675, loss 0.0231099, acc 1
2016-09-05T22:15:12.503460: step 13676, loss 0.0159152, acc 1
2016-09-05T22:15:13.310326: step 13677, loss 0.0255721, acc 1
2016-09-05T22:15:14.144630: step 13678, loss 0.0393528, acc 0.98
2016-09-05T22:15:15.007454: step 13679, loss 0.00814105, acc 1
2016-09-05T22:15:15.858842: step 13680, loss 0.0292492, acc 1
2016-09-05T22:15:16.769163: step 13681, loss 0.0237322, acc 0.98
2016-09-05T22:15:17.592483: step 13682, loss 0.0275539, acc 0.98
2016-09-05T22:15:18.441456: step 13683, loss 0.0300932, acc 0.98
2016-09-05T22:15:19.336519: step 13684, loss 0.00376163, acc 1
2016-09-05T22:15:20.175077: step 13685, loss 0.00517626, acc 1
2016-09-05T22:15:20.958779: step 13686, loss 0.00375259, acc 1
2016-09-05T22:15:21.778074: step 13687, loss 0.00381561, acc 1
2016-09-05T22:15:22.562259: step 13688, loss 0.0196845, acc 0.98
2016-09-05T22:15:23.398416: step 13689, loss 0.0265274, acc 1
2016-09-05T22:15:24.198329: step 13690, loss 0.0180217, acc 1
2016-09-05T22:15:25.185271: step 13691, loss 0.05221, acc 0.96
2016-09-05T22:15:26.009002: step 13692, loss 0.00404209, acc 1
2016-09-05T22:15:26.798850: step 13693, loss 0.0459774, acc 0.96
2016-09-05T22:15:27.696946: step 13694, loss 0.0299797, acc 1
2016-09-05T22:15:28.516995: step 13695, loss 0.045184, acc 0.98
2016-09-05T22:15:29.304241: step 13696, loss 0.0185506, acc 1
2016-09-05T22:15:30.155294: step 13697, loss 0.00865593, acc 1
2016-09-05T22:15:30.985580: step 13698, loss 0.0281392, acc 1
2016-09-05T22:15:31.802934: step 13699, loss 0.0181441, acc 0.98
2016-09-05T22:15:32.608531: step 13700, loss 0.0237988, acc 1

Evaluation:
2016-09-05T22:15:36.128548: step 13700, loss 3.33693, acc 0.716

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-13700

2016-09-05T22:15:38.098483: step 13701, loss 0.0150122, acc 1
2016-09-05T22:15:38.907881: step 13702, loss 0.00460729, acc 1
2016-09-05T22:15:39.755691: step 13703, loss 0.00409647, acc 1
2016-09-05T22:15:40.574486: step 13704, loss 0.0878937, acc 0.98
2016-09-05T22:15:41.369417: step 13705, loss 0.0571403, acc 0.96
2016-09-05T22:15:42.189227: step 13706, loss 0.0207438, acc 0.98
2016-09-05T22:15:43.007052: step 13707, loss 0.0109351, acc 1
2016-09-05T22:15:43.817108: step 13708, loss 0.0134802, acc 1
2016-09-05T22:15:44.629927: step 13709, loss 0.00355677, acc 1
2016-09-05T22:15:45.451414: step 13710, loss 0.0149576, acc 1
2016-09-05T22:15:46.288487: step 13711, loss 0.0152629, acc 1
2016-09-05T22:15:47.097684: step 13712, loss 0.0106359, acc 1
2016-09-05T22:15:47.889432: step 13713, loss 0.00440651, acc 1
2016-09-05T22:15:48.697009: step 13714, loss 0.0141573, acc 1
2016-09-05T22:15:49.515257: step 13715, loss 0.0116845, acc 1
2016-09-05T22:15:50.323861: step 13716, loss 0.00365014, acc 1
2016-09-05T22:15:51.122972: step 13717, loss 0.00332955, acc 1
2016-09-05T22:15:52.021296: step 13718, loss 0.00331039, acc 1
2016-09-05T22:15:52.800485: step 13719, loss 0.00562263, acc 1
2016-09-05T22:15:53.594522: step 13720, loss 0.055483, acc 0.96
2016-09-05T22:15:54.430463: step 13721, loss 0.002985, acc 1
2016-09-05T22:15:55.249626: step 13722, loss 0.0464395, acc 0.98
2016-09-05T22:15:56.021300: step 13723, loss 0.0177622, acc 0.98
2016-09-05T22:15:56.831774: step 13724, loss 0.0330146, acc 0.98
2016-09-05T22:15:57.665414: step 13725, loss 0.0139502, acc 1
2016-09-05T22:15:58.447151: step 13726, loss 0.00352803, acc 1
2016-09-05T22:15:59.264703: step 13727, loss 0.00477966, acc 1
2016-09-05T22:16:00.050655: step 13728, loss 0.0385882, acc 0.98
2016-09-05T22:16:00.890585: step 13729, loss 0.0030638, acc 1
2016-09-05T22:16:01.689414: step 13730, loss 0.00602631, acc 1
2016-09-05T22:16:02.503038: step 13731, loss 0.102022, acc 0.96
2016-09-05T22:16:03.306312: step 13732, loss 0.0148465, acc 1
2016-09-05T22:16:04.133553: step 13733, loss 0.0357802, acc 0.98
2016-09-05T22:16:04.937758: step 13734, loss 0.00687585, acc 1
2016-09-05T22:16:05.732404: step 13735, loss 0.0189032, acc 0.98
2016-09-05T22:16:06.543029: step 13736, loss 0.00398012, acc 1
2016-09-05T22:16:07.366695: step 13737, loss 0.00642763, acc 1
2016-09-05T22:16:08.168075: step 13738, loss 0.0188816, acc 1
2016-09-05T22:16:08.976511: step 13739, loss 0.00287568, acc 1
2016-09-05T22:16:09.802028: step 13740, loss 0.0247061, acc 0.98
2016-09-05T22:16:10.627709: step 13741, loss 0.0350383, acc 0.98
2016-09-05T22:16:11.445364: step 13742, loss 0.0230297, acc 1
2016-09-05T22:16:12.287557: step 13743, loss 0.0186752, acc 1
2016-09-05T22:16:13.096124: step 13744, loss 0.00404641, acc 1
2016-09-05T22:16:13.913887: step 13745, loss 0.00574709, acc 1
2016-09-05T22:16:14.740085: step 13746, loss 0.0725696, acc 0.94
2016-09-05T22:16:15.554424: step 13747, loss 0.00248413, acc 1
2016-09-05T22:16:16.374928: step 13748, loss 0.0200686, acc 1
2016-09-05T22:16:17.200174: step 13749, loss 0.0183001, acc 0.98
2016-09-05T22:16:17.971167: step 13750, loss 0.0342465, acc 0.98
2016-09-05T22:16:18.801736: step 13751, loss 0.0285013, acc 0.98
2016-09-05T22:16:19.621181: step 13752, loss 0.00736511, acc 1
2016-09-05T22:16:20.424762: step 13753, loss 0.00920326, acc 1
2016-09-05T22:16:21.229430: step 13754, loss 0.00587555, acc 1
2016-09-05T22:16:22.047406: step 13755, loss 0.00254995, acc 1
2016-09-05T22:16:22.848715: step 13756, loss 0.0114231, acc 1
2016-09-05T22:16:23.678845: step 13757, loss 0.00747385, acc 1
2016-09-05T22:16:24.530917: step 13758, loss 0.00311643, acc 1
2016-09-05T22:16:25.337479: step 13759, loss 0.0756134, acc 0.96
2016-09-05T22:16:26.150056: step 13760, loss 0.0218751, acc 0.98
2016-09-05T22:16:26.977802: step 13761, loss 0.0271196, acc 1
2016-09-05T22:16:27.890435: step 13762, loss 0.00513466, acc 1
2016-09-05T22:16:28.707663: step 13763, loss 0.00692947, acc 1
2016-09-05T22:16:29.515370: step 13764, loss 0.108428, acc 0.98
2016-09-05T22:16:30.334241: step 13765, loss 0.0754391, acc 0.98
2016-09-05T22:16:31.165897: step 13766, loss 0.0380384, acc 0.98
2016-09-05T22:16:31.993126: step 13767, loss 0.0183461, acc 0.98
2016-09-05T22:16:32.840792: step 13768, loss 0.0475704, acc 0.98
2016-09-05T22:16:33.655306: step 13769, loss 0.00242092, acc 1
2016-09-05T22:16:34.457513: step 13770, loss 0.0228685, acc 0.98
2016-09-05T22:16:35.274251: step 13771, loss 0.0300918, acc 0.98
2016-09-05T22:16:36.081509: step 13772, loss 0.0146495, acc 1
2016-09-05T22:16:36.901064: step 13773, loss 0.00205225, acc 1
2016-09-05T22:16:37.329216: step 13774, loss 0.0107176, acc 1
2016-09-05T22:16:38.154233: step 13775, loss 0.0326838, acc 0.98
2016-09-05T22:16:38.987219: step 13776, loss 0.0164733, acc 1
2016-09-05T22:16:39.796692: step 13777, loss 0.0408622, acc 0.98
2016-09-05T22:16:40.642701: step 13778, loss 0.0108754, acc 1
2016-09-05T22:16:41.468080: step 13779, loss 0.024923, acc 0.98
2016-09-05T22:16:42.297428: step 13780, loss 0.0205503, acc 0.98
2016-09-05T22:16:43.105226: step 13781, loss 0.00295566, acc 1
2016-09-05T22:16:43.933438: step 13782, loss 0.0174329, acc 1
2016-09-05T22:16:44.791466: step 13783, loss 0.00279092, acc 1
2016-09-05T22:16:45.572039: step 13784, loss 0.00291502, acc 1
2016-09-05T22:16:46.374291: step 13785, loss 0.0257622, acc 0.98
2016-09-05T22:16:47.211550: step 13786, loss 0.0860256, acc 0.98
2016-09-05T22:16:47.988298: step 13787, loss 0.019216, acc 1
2016-09-05T22:16:48.785727: step 13788, loss 0.0304097, acc 0.98
2016-09-05T22:16:49.603893: step 13789, loss 0.0024522, acc 1
2016-09-05T22:16:50.371674: step 13790, loss 0.0020347, acc 1
2016-09-05T22:16:51.166910: step 13791, loss 0.0183986, acc 1
2016-09-05T22:16:52.007227: step 13792, loss 0.0119631, acc 1
2016-09-05T22:16:52.792490: step 13793, loss 0.0082746, acc 1
2016-09-05T22:16:53.623295: step 13794, loss 0.00249542, acc 1
2016-09-05T22:16:54.416267: step 13795, loss 0.0203724, acc 1
2016-09-05T22:16:55.228637: step 13796, loss 0.00568657, acc 1
2016-09-05T22:16:56.037863: step 13797, loss 0.00330023, acc 1
2016-09-05T22:16:56.856209: step 13798, loss 0.00435317, acc 1
2016-09-05T22:16:57.644952: step 13799, loss 0.00537282, acc 1
2016-09-05T22:16:58.485420: step 13800, loss 0.0022592, acc 1

Evaluation:
2016-09-05T22:17:02.044486: step 13800, loss 2.38679, acc 0.708

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-13800

2016-09-05T22:17:03.957066: step 13801, loss 0.00474322, acc 1
2016-09-05T22:17:04.766981: step 13802, loss 0.00299373, acc 1
2016-09-05T22:17:05.624279: step 13803, loss 0.0244396, acc 0.98
2016-09-05T22:17:06.421712: step 13804, loss 0.02113, acc 0.98
2016-09-05T22:17:07.223008: step 13805, loss 0.0253785, acc 0.98
2016-09-05T22:17:08.050502: step 13806, loss 0.0300327, acc 1
2016-09-05T22:17:08.877749: step 13807, loss 0.00347642, acc 1
2016-09-05T22:17:09.682497: step 13808, loss 0.0189892, acc 0.98
2016-09-05T22:17:10.508974: step 13809, loss 0.00223612, acc 1
2016-09-05T22:17:11.310658: step 13810, loss 0.0200366, acc 0.98
2016-09-05T22:17:12.093110: step 13811, loss 0.0186358, acc 0.98
2016-09-05T22:17:12.911050: step 13812, loss 0.0170612, acc 1
2016-09-05T22:17:13.725653: step 13813, loss 0.00688079, acc 1
2016-09-05T22:17:14.542152: step 13814, loss 0.00208414, acc 1
2016-09-05T22:17:15.344465: step 13815, loss 0.0285582, acc 1
2016-09-05T22:17:16.184427: step 13816, loss 0.0153576, acc 1
2016-09-05T22:17:16.962627: step 13817, loss 0.0140618, acc 1
2016-09-05T22:17:17.762030: step 13818, loss 0.0147541, acc 1
2016-09-05T22:17:18.571858: step 13819, loss 0.00748008, acc 1
2016-09-05T22:17:19.365351: step 13820, loss 0.0354475, acc 0.98
2016-09-05T22:17:20.178022: step 13821, loss 0.0278095, acc 0.98
2016-09-05T22:17:21.011047: step 13822, loss 0.0056691, acc 1
2016-09-05T22:17:21.810879: step 13823, loss 0.00205404, acc 1
2016-09-05T22:17:22.629603: step 13824, loss 0.033605, acc 0.98
2016-09-05T22:17:23.474083: step 13825, loss 0.0152458, acc 1
2016-09-05T22:17:24.250947: step 13826, loss 0.0050359, acc 1
2016-09-05T22:17:25.059731: step 13827, loss 0.00708814, acc 1
2016-09-05T22:17:25.851869: step 13828, loss 0.00533503, acc 1
2016-09-05T22:17:26.631150: step 13829, loss 0.00328859, acc 1
2016-09-05T22:17:27.430824: step 13830, loss 0.00501387, acc 1
2016-09-05T22:17:28.255323: step 13831, loss 0.0777103, acc 0.98
2016-09-05T22:17:29.046984: step 13832, loss 0.00202758, acc 1
2016-09-05T22:17:29.851789: step 13833, loss 0.0050817, acc 1
2016-09-05T22:17:30.678343: step 13834, loss 0.0096432, acc 1
2016-09-05T22:17:31.486804: step 13835, loss 0.0634339, acc 0.96
2016-09-05T22:17:32.281072: step 13836, loss 0.00254164, acc 1
2016-09-05T22:17:33.106739: step 13837, loss 0.0210788, acc 1
2016-09-05T22:17:33.886833: step 13838, loss 0.0444223, acc 0.98
2016-09-05T22:17:34.702415: step 13839, loss 0.0565389, acc 0.96
2016-09-05T22:17:35.562696: step 13840, loss 0.026671, acc 0.98
2016-09-05T22:17:36.368308: step 13841, loss 0.0277622, acc 0.98
2016-09-05T22:17:37.160342: step 13842, loss 0.00195638, acc 1
2016-09-05T22:17:37.976535: step 13843, loss 0.0668398, acc 0.98
2016-09-05T22:17:38.747381: step 13844, loss 0.0120051, acc 1
2016-09-05T22:17:39.535916: step 13845, loss 0.0139723, acc 1
2016-09-05T22:17:40.393073: step 13846, loss 0.0390356, acc 0.98
2016-09-05T22:17:41.208439: step 13847, loss 0.00246018, acc 1
2016-09-05T22:17:42.016145: step 13848, loss 0.00262551, acc 1
2016-09-05T22:17:42.848134: step 13849, loss 0.00283039, acc 1
2016-09-05T22:17:43.672908: step 13850, loss 0.0307166, acc 0.98
2016-09-05T22:17:44.497461: step 13851, loss 0.0176254, acc 1
2016-09-05T22:17:45.336513: step 13852, loss 0.0376202, acc 0.98
2016-09-05T22:17:46.155812: step 13853, loss 0.0171299, acc 1
2016-09-05T22:17:46.980328: step 13854, loss 0.0836172, acc 0.98
2016-09-05T22:17:47.832641: step 13855, loss 0.040273, acc 0.98
2016-09-05T22:17:48.621560: step 13856, loss 0.00392209, acc 1
2016-09-05T22:17:49.413599: step 13857, loss 0.0203442, acc 0.98
2016-09-05T22:17:50.265027: step 13858, loss 0.0441755, acc 0.98
2016-09-05T22:17:51.066433: step 13859, loss 0.00829588, acc 1
2016-09-05T22:17:51.858572: step 13860, loss 0.00533302, acc 1
2016-09-05T22:17:52.683773: step 13861, loss 0.0144376, acc 1
2016-09-05T22:17:53.502199: step 13862, loss 0.0456517, acc 0.98
2016-09-05T22:17:54.312620: step 13863, loss 0.00177125, acc 1
2016-09-05T22:17:55.119803: step 13864, loss 0.015188, acc 1
2016-09-05T22:17:55.915063: step 13865, loss 0.019972, acc 0.98
2016-09-05T22:17:56.726357: step 13866, loss 0.0261029, acc 0.98
2016-09-05T22:17:57.564690: step 13867, loss 0.00161554, acc 1
2016-09-05T22:17:58.378406: step 13868, loss 0.00229176, acc 1
2016-09-05T22:17:59.182855: step 13869, loss 0.0266982, acc 1
2016-09-05T22:18:00.020735: step 13870, loss 0.0130552, acc 1
2016-09-05T22:18:00.835581: step 13871, loss 0.0519928, acc 0.98
2016-09-05T22:18:01.627052: step 13872, loss 0.0438792, acc 0.98
2016-09-05T22:18:02.461942: step 13873, loss 0.00623049, acc 1
2016-09-05T22:18:03.309425: step 13874, loss 0.0447947, acc 0.96
2016-09-05T22:18:04.099860: step 13875, loss 0.0194912, acc 0.98
2016-09-05T22:18:04.900892: step 13876, loss 0.0207308, acc 1
2016-09-05T22:18:05.711658: step 13877, loss 0.00378603, acc 1
2016-09-05T22:18:06.501371: step 13878, loss 0.0320476, acc 0.98
2016-09-05T22:18:07.374636: step 13879, loss 0.0354341, acc 0.98
2016-09-05T22:18:08.196602: step 13880, loss 0.019074, acc 0.98
2016-09-05T22:18:08.996695: step 13881, loss 0.00235136, acc 1
2016-09-05T22:18:09.798870: step 13882, loss 0.00850886, acc 1
2016-09-05T22:18:10.621706: step 13883, loss 0.0349555, acc 0.98
2016-09-05T22:18:11.417457: step 13884, loss 0.00942777, acc 1
2016-09-05T22:18:12.226535: step 13885, loss 0.0411254, acc 0.96
2016-09-05T22:18:13.040862: step 13886, loss 0.00287993, acc 1
2016-09-05T22:18:13.829971: step 13887, loss 0.0233033, acc 1
2016-09-05T22:18:14.624581: step 13888, loss 0.0478681, acc 0.98
2016-09-05T22:18:15.429052: step 13889, loss 0.0033993, acc 1
2016-09-05T22:18:16.216269: step 13890, loss 0.0283437, acc 1
2016-09-05T22:18:17.054229: step 13891, loss 0.0297437, acc 0.98
2016-09-05T22:18:17.885002: step 13892, loss 0.00755908, acc 1
2016-09-05T22:18:18.682717: step 13893, loss 0.0023896, acc 1
2016-09-05T22:18:19.489890: step 13894, loss 0.0382714, acc 0.98
2016-09-05T22:18:20.311228: step 13895, loss 0.00646636, acc 1
2016-09-05T22:18:21.121030: step 13896, loss 0.011134, acc 1
2016-09-05T22:18:21.907215: step 13897, loss 0.00255353, acc 1
2016-09-05T22:18:22.743872: step 13898, loss 0.0586622, acc 0.98
2016-09-05T22:18:23.598869: step 13899, loss 0.0342154, acc 0.98
2016-09-05T22:18:24.418576: step 13900, loss 0.038679, acc 0.98

Evaluation:
2016-09-05T22:18:27.932280: step 13900, loss 2.55457, acc 0.698

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-13900

2016-09-05T22:18:29.761473: step 13901, loss 0.00301844, acc 1
2016-09-05T22:18:30.603332: step 13902, loss 0.00933741, acc 1
2016-09-05T22:18:31.443003: step 13903, loss 0.00269521, acc 1
2016-09-05T22:18:32.293033: step 13904, loss 0.0288058, acc 0.98
2016-09-05T22:18:33.118201: step 13905, loss 0.00980806, acc 1
2016-09-05T22:18:33.947184: step 13906, loss 0.0229568, acc 1
2016-09-05T22:18:34.758340: step 13907, loss 0.0103508, acc 1
2016-09-05T22:18:35.566342: step 13908, loss 0.06338, acc 0.96
2016-09-05T22:18:36.397524: step 13909, loss 0.0368248, acc 0.98
2016-09-05T22:18:37.243190: step 13910, loss 0.016851, acc 0.98
2016-09-05T22:18:38.002411: step 13911, loss 0.0273354, acc 0.98
2016-09-05T22:18:38.794572: step 13912, loss 0.00321043, acc 1
2016-09-05T22:18:39.633419: step 13913, loss 0.0139791, acc 1
2016-09-05T22:18:40.420444: step 13914, loss 0.00879888, acc 1
2016-09-05T22:18:41.230585: step 13915, loss 0.00820575, acc 1
2016-09-05T22:18:42.037339: step 13916, loss 0.015936, acc 1
2016-09-05T22:18:42.839360: step 13917, loss 0.00763472, acc 1
2016-09-05T22:18:43.625528: step 13918, loss 0.0169221, acc 0.98
2016-09-05T22:18:44.441047: step 13919, loss 0.00629207, acc 1
2016-09-05T22:18:45.243345: step 13920, loss 0.0250441, acc 0.98
2016-09-05T22:18:46.047736: step 13921, loss 0.0354473, acc 0.98
2016-09-05T22:18:46.867294: step 13922, loss 0.00509007, acc 1
2016-09-05T22:18:47.657011: step 13923, loss 0.0155769, acc 1
2016-09-05T22:18:48.465000: step 13924, loss 0.00261046, acc 1
2016-09-05T22:18:49.292221: step 13925, loss 0.0044382, acc 1
2016-09-05T22:18:50.090513: step 13926, loss 0.0237793, acc 1
2016-09-05T22:18:50.887056: step 13927, loss 0.0490864, acc 0.98
2016-09-05T22:18:51.704045: step 13928, loss 0.0231071, acc 1
2016-09-05T22:18:52.483809: step 13929, loss 0.0129301, acc 1
2016-09-05T22:18:53.297145: step 13930, loss 0.00209951, acc 1
2016-09-05T22:18:54.115498: step 13931, loss 0.00261794, acc 1
2016-09-05T22:18:54.919750: step 13932, loss 0.00532836, acc 1
2016-09-05T22:18:55.721026: step 13933, loss 0.00253487, acc 1
2016-09-05T22:18:56.549396: step 13934, loss 0.00307, acc 1
2016-09-05T22:18:57.339347: step 13935, loss 0.0170523, acc 0.98
2016-09-05T22:18:58.189858: step 13936, loss 0.0197344, acc 0.98
2016-09-05T22:18:59.013770: step 13937, loss 0.0252846, acc 1
2016-09-05T22:18:59.807405: step 13938, loss 0.00233691, acc 1
2016-09-05T22:19:00.637316: step 13939, loss 0.00244804, acc 1
2016-09-05T22:19:01.475850: step 13940, loss 0.0154972, acc 1
2016-09-05T22:19:02.287373: step 13941, loss 0.00212066, acc 1
2016-09-05T22:19:03.117044: step 13942, loss 0.0228614, acc 0.98
2016-09-05T22:19:03.935773: step 13943, loss 0.0126376, acc 1
2016-09-05T22:19:04.759040: step 13944, loss 0.0147622, acc 1
2016-09-05T22:19:05.575987: step 13945, loss 0.0293902, acc 0.98
2016-09-05T22:19:06.408298: step 13946, loss 0.00242411, acc 1
2016-09-05T22:19:07.225154: step 13947, loss 0.00213558, acc 1
2016-09-05T22:19:08.050335: step 13948, loss 0.00762144, acc 1
2016-09-05T22:19:08.899989: step 13949, loss 0.0562535, acc 0.96
2016-09-05T22:19:09.773495: step 13950, loss 0.00233357, acc 1
2016-09-05T22:19:10.591515: step 13951, loss 0.00214454, acc 1
2016-09-05T22:19:11.420313: step 13952, loss 0.00228528, acc 1
2016-09-05T22:19:12.217519: step 13953, loss 0.016484, acc 0.98
2016-09-05T22:19:13.017565: step 13954, loss 0.0159942, acc 0.98
2016-09-05T22:19:13.828619: step 13955, loss 0.0148725, acc 1
2016-09-05T22:19:14.649138: step 13956, loss 0.00207482, acc 1
2016-09-05T22:19:15.430661: step 13957, loss 0.015595, acc 1
2016-09-05T22:19:16.234701: step 13958, loss 0.0375633, acc 0.98
2016-09-05T22:19:17.070375: step 13959, loss 0.0058045, acc 1
2016-09-05T22:19:17.864450: step 13960, loss 0.0199426, acc 0.98
2016-09-05T22:19:18.649935: step 13961, loss 0.0172962, acc 0.98
2016-09-05T22:19:19.458553: step 13962, loss 0.0031139, acc 1
2016-09-05T22:19:20.285625: step 13963, loss 0.00280278, acc 1
2016-09-05T22:19:21.146915: step 13964, loss 0.0227731, acc 0.98
2016-09-05T22:19:21.959468: step 13965, loss 0.00603781, acc 1
2016-09-05T22:19:22.747514: step 13966, loss 0.0469725, acc 0.98
2016-09-05T22:19:23.532019: step 13967, loss 0.0334451, acc 0.98
2016-09-05T22:19:23.982365: step 13968, loss 0.108601, acc 0.916667
2016-09-05T22:19:24.798167: step 13969, loss 0.00309669, acc 1
2016-09-05T22:19:25.638403: step 13970, loss 0.00182395, acc 1
2016-09-05T22:19:26.452712: step 13971, loss 0.00861206, acc 1
2016-09-05T22:19:27.298793: step 13972, loss 0.024923, acc 0.98
2016-09-05T22:19:28.188985: step 13973, loss 0.00182029, acc 1
2016-09-05T22:19:29.006123: step 13974, loss 0.00700392, acc 1
2016-09-05T22:19:29.809424: step 13975, loss 0.0122863, acc 1
2016-09-05T22:19:30.647242: step 13976, loss 0.00193274, acc 1
2016-09-05T22:19:31.466977: step 13977, loss 0.0571178, acc 0.98
2016-09-05T22:19:32.263574: step 13978, loss 0.00575752, acc 1
2016-09-05T22:19:33.102129: step 13979, loss 0.00179639, acc 1
2016-09-05T22:19:33.924311: step 13980, loss 0.0329819, acc 0.96
2016-09-05T22:19:34.733218: step 13981, loss 0.0325559, acc 0.98
2016-09-05T22:19:35.549424: step 13982, loss 0.00291539, acc 1
2016-09-05T22:19:36.354769: step 13983, loss 0.00194465, acc 1
2016-09-05T22:19:37.150869: step 13984, loss 0.00439438, acc 1
2016-09-05T22:19:37.979582: step 13985, loss 0.0187092, acc 1
2016-09-05T22:19:38.753787: step 13986, loss 0.0161414, acc 1
2016-09-05T22:19:39.553401: step 13987, loss 0.0113838, acc 1
2016-09-05T22:19:40.382692: step 13988, loss 0.0166248, acc 0.98
2016-09-05T22:19:41.191311: step 13989, loss 0.00307345, acc 1
2016-09-05T22:19:41.985236: step 13990, loss 0.0190649, acc 1
2016-09-05T22:19:42.778143: step 13991, loss 0.0188801, acc 1
2016-09-05T22:19:43.595982: step 13992, loss 0.00224422, acc 1
2016-09-05T22:19:44.409135: step 13993, loss 0.0197456, acc 1
2016-09-05T22:19:45.236861: step 13994, loss 0.00355292, acc 1
2016-09-05T22:19:46.064839: step 13995, loss 0.00207179, acc 1
2016-09-05T22:19:46.876744: step 13996, loss 0.0137078, acc 1
2016-09-05T22:19:47.668220: step 13997, loss 0.00208087, acc 1
2016-09-05T22:19:48.459831: step 13998, loss 0.0229264, acc 0.98
2016-09-05T22:19:49.253990: step 13999, loss 0.0200688, acc 0.98
2016-09-05T22:19:50.064317: step 14000, loss 0.0492273, acc 0.94

Evaluation:
2016-09-05T22:19:53.605837: step 14000, loss 2.52027, acc 0.712

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-14000

2016-09-05T22:19:55.563577: step 14001, loss 0.00322252, acc 1
2016-09-05T22:19:56.395979: step 14002, loss 0.0190008, acc 1
2016-09-05T22:19:57.210595: step 14003, loss 0.0158288, acc 0.98
2016-09-05T22:19:58.032422: step 14004, loss 0.00919432, acc 1
2016-09-05T22:19:58.840385: step 14005, loss 0.00412632, acc 1
2016-09-05T22:19:59.679491: step 14006, loss 0.010188, acc 1
2016-09-05T22:20:00.519471: step 14007, loss 0.0019821, acc 1
2016-09-05T22:20:01.326403: step 14008, loss 0.00209113, acc 1
2016-09-05T22:20:02.170800: step 14009, loss 0.0172478, acc 0.98
2016-09-05T22:20:02.963322: step 14010, loss 0.00860613, acc 1
2016-09-05T22:20:03.757975: step 14011, loss 0.0403531, acc 0.98
2016-09-05T22:20:04.586586: step 14012, loss 0.016761, acc 1
2016-09-05T22:20:05.395897: step 14013, loss 0.0128125, acc 1
2016-09-05T22:20:06.214977: step 14014, loss 0.0458999, acc 0.98
2016-09-05T22:20:07.015485: step 14015, loss 0.00186608, acc 1
2016-09-05T22:20:07.857480: step 14016, loss 0.0118833, acc 1
2016-09-05T22:20:08.652493: step 14017, loss 0.00351473, acc 1
2016-09-05T22:20:09.472250: step 14018, loss 0.0264017, acc 0.98
2016-09-05T22:20:10.301360: step 14019, loss 0.00735394, acc 1
2016-09-05T22:20:11.107064: step 14020, loss 0.0121153, acc 1
2016-09-05T22:20:11.920056: step 14021, loss 0.00374814, acc 1
2016-09-05T22:20:12.744907: step 14022, loss 0.00376459, acc 1
2016-09-05T22:20:13.544841: step 14023, loss 0.00836318, acc 1
2016-09-05T22:20:14.336186: step 14024, loss 0.00343882, acc 1
2016-09-05T22:20:15.166273: step 14025, loss 0.00207184, acc 1
2016-09-05T22:20:15.925388: step 14026, loss 0.00788385, acc 1
2016-09-05T22:20:16.741359: step 14027, loss 0.0101631, acc 1
2016-09-05T22:20:17.544421: step 14028, loss 0.0023461, acc 1
2016-09-05T22:20:18.322108: step 14029, loss 0.00186824, acc 1
2016-09-05T22:20:19.121984: step 14030, loss 0.00496856, acc 1
2016-09-05T22:20:19.956386: step 14031, loss 0.0200733, acc 0.98
2016-09-05T22:20:20.745450: step 14032, loss 0.027756, acc 0.98
2016-09-05T22:20:21.555881: step 14033, loss 0.0174799, acc 0.98
2016-09-05T22:20:22.378279: step 14034, loss 0.0041967, acc 1
2016-09-05T22:20:23.208190: step 14035, loss 0.0630252, acc 0.96
2016-09-05T22:20:24.006227: step 14036, loss 0.0411537, acc 0.96
2016-09-05T22:20:24.825765: step 14037, loss 0.00679174, acc 1
2016-09-05T22:20:25.607031: step 14038, loss 0.00235997, acc 1
2016-09-05T22:20:26.397763: step 14039, loss 0.00233129, acc 1
2016-09-05T22:20:27.218406: step 14040, loss 0.0177627, acc 0.98
2016-09-05T22:20:28.001345: step 14041, loss 0.00191653, acc 1
2016-09-05T22:20:28.800343: step 14042, loss 0.00459848, acc 1
2016-09-05T22:20:29.620493: step 14043, loss 0.00291393, acc 1
2016-09-05T22:20:30.413128: step 14044, loss 0.0465125, acc 0.98
2016-09-05T22:20:31.234789: step 14045, loss 0.00262424, acc 1
2016-09-05T22:20:32.030167: step 14046, loss 0.0191208, acc 0.98
2016-09-05T22:20:32.817689: step 14047, loss 0.0199, acc 1
2016-09-05T22:20:33.639203: step 14048, loss 0.0017604, acc 1
2016-09-05T22:20:34.473732: step 14049, loss 0.00231141, acc 1
2016-09-05T22:20:35.246561: step 14050, loss 0.0204781, acc 0.98
2016-09-05T22:20:36.037334: step 14051, loss 0.0288742, acc 0.98
2016-09-05T22:20:36.851774: step 14052, loss 0.00221086, acc 1
2016-09-05T22:20:37.644594: step 14053, loss 0.00725394, acc 1
2016-09-05T22:20:38.442441: step 14054, loss 0.00841979, acc 1
2016-09-05T22:20:39.272382: step 14055, loss 0.00168742, acc 1
2016-09-05T22:20:40.066002: step 14056, loss 0.0232196, acc 1
2016-09-05T22:20:40.869221: step 14057, loss 0.00233366, acc 1
2016-09-05T22:20:41.690729: step 14058, loss 0.0496473, acc 0.98
2016-09-05T22:20:42.491569: step 14059, loss 0.0171159, acc 1
2016-09-05T22:20:43.306774: step 14060, loss 0.0268332, acc 0.98
2016-09-05T22:20:44.137390: step 14061, loss 0.00172667, acc 1
2016-09-05T22:20:44.941396: step 14062, loss 0.00290076, acc 1
2016-09-05T22:20:45.717642: step 14063, loss 0.0269582, acc 0.98
2016-09-05T22:20:46.528096: step 14064, loss 0.0142686, acc 1
2016-09-05T22:20:47.347783: step 14065, loss 0.0100964, acc 1
2016-09-05T22:20:48.167498: step 14066, loss 0.0200072, acc 0.98
2016-09-05T22:20:48.952237: step 14067, loss 0.00328441, acc 1
2016-09-05T22:20:49.743231: step 14068, loss 0.016402, acc 1
2016-09-05T22:20:50.541222: step 14069, loss 0.00187941, acc 1
2016-09-05T22:20:51.376082: step 14070, loss 0.00202829, acc 1
2016-09-05T22:20:52.175101: step 14071, loss 0.0165756, acc 0.98
2016-09-05T22:20:53.033998: step 14072, loss 0.00347876, acc 1
2016-09-05T22:20:53.921363: step 14073, loss 0.00306866, acc 1
2016-09-05T22:20:54.731713: step 14074, loss 0.00690477, acc 1
2016-09-05T22:20:55.556216: step 14075, loss 0.00632079, acc 1
2016-09-05T22:20:56.401367: step 14076, loss 0.00574255, acc 1
2016-09-05T22:20:57.242860: step 14077, loss 0.0342402, acc 0.98
2016-09-05T22:20:58.042498: step 14078, loss 0.00228312, acc 1
2016-09-05T22:20:58.860836: step 14079, loss 0.0354816, acc 0.98
2016-09-05T22:20:59.679523: step 14080, loss 0.00313571, acc 1
2016-09-05T22:21:00.541329: step 14081, loss 0.0269329, acc 0.98
2016-09-05T22:21:01.411624: step 14082, loss 0.041416, acc 0.98
2016-09-05T22:21:02.228313: step 14083, loss 0.00187424, acc 1
2016-09-05T22:21:03.013921: step 14084, loss 0.0180049, acc 1
2016-09-05T22:21:03.826900: step 14085, loss 0.00277507, acc 1
2016-09-05T22:21:04.655981: step 14086, loss 0.0162381, acc 0.98
2016-09-05T22:21:05.450011: step 14087, loss 0.00207149, acc 1
2016-09-05T22:21:06.235708: step 14088, loss 0.0424826, acc 0.96
2016-09-05T22:21:07.104831: step 14089, loss 0.00728665, acc 1
2016-09-05T22:21:07.908438: step 14090, loss 0.00269055, acc 1
2016-09-05T22:21:08.691675: step 14091, loss 0.0020757, acc 1
2016-09-05T22:21:09.517916: step 14092, loss 0.00213747, acc 1
2016-09-05T22:21:10.328911: step 14093, loss 0.0077654, acc 1
2016-09-05T22:21:11.118266: step 14094, loss 0.023913, acc 0.98
2016-09-05T22:21:11.933106: step 14095, loss 0.0174489, acc 0.98
2016-09-05T22:21:12.733457: step 14096, loss 0.00266685, acc 1
2016-09-05T22:21:13.546010: step 14097, loss 0.00197813, acc 1
2016-09-05T22:21:14.385600: step 14098, loss 0.021658, acc 0.98
2016-09-05T22:21:15.236021: step 14099, loss 0.020101, acc 0.98
2016-09-05T22:21:16.083525: step 14100, loss 0.0165668, acc 1

Evaluation:
2016-09-05T22:21:19.590608: step 14100, loss 2.95806, acc 0.704

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-14100

2016-09-05T22:21:21.451445: step 14101, loss 0.00645021, acc 1
2016-09-05T22:21:22.272751: step 14102, loss 0.00226563, acc 1
2016-09-05T22:21:23.108354: step 14103, loss 0.00647097, acc 1
2016-09-05T22:21:23.922510: step 14104, loss 0.00251024, acc 1
2016-09-05T22:21:24.743097: step 14105, loss 0.0184352, acc 0.98
2016-09-05T22:21:25.536731: step 14106, loss 0.00193405, acc 1
2016-09-05T22:21:26.353834: step 14107, loss 0.00802132, acc 1
2016-09-05T22:21:27.154379: step 14108, loss 0.0101855, acc 1
2016-09-05T22:21:27.987271: step 14109, loss 0.00254093, acc 1
2016-09-05T22:21:28.810332: step 14110, loss 0.0046367, acc 1
2016-09-05T22:21:29.639142: step 14111, loss 0.016594, acc 1
2016-09-05T22:21:30.450591: step 14112, loss 0.00971228, acc 1
2016-09-05T22:21:31.290687: step 14113, loss 0.00982428, acc 1
2016-09-05T22:21:32.093030: step 14114, loss 0.0023344, acc 1
2016-09-05T22:21:32.909873: step 14115, loss 0.0157219, acc 1
2016-09-05T22:21:33.721286: step 14116, loss 0.00209277, acc 1
2016-09-05T22:21:34.532485: step 14117, loss 0.172841, acc 0.98
2016-09-05T22:21:35.386625: step 14118, loss 0.00464043, acc 1
2016-09-05T22:21:36.204777: step 14119, loss 0.010213, acc 1
2016-09-05T22:21:36.983810: step 14120, loss 0.0156611, acc 1
2016-09-05T22:21:37.825084: step 14121, loss 0.016067, acc 0.98
2016-09-05T22:21:38.644486: step 14122, loss 0.0293366, acc 1
2016-09-05T22:21:39.436554: step 14123, loss 0.0301335, acc 0.98
2016-09-05T22:21:40.229423: step 14124, loss 0.00717971, acc 1
2016-09-05T22:21:41.027983: step 14125, loss 0.00205992, acc 1
2016-09-05T22:21:41.811842: step 14126, loss 0.00225727, acc 1
2016-09-05T22:21:42.653880: step 14127, loss 0.0485332, acc 0.98
2016-09-05T22:21:43.462717: step 14128, loss 0.00366296, acc 1
2016-09-05T22:21:44.268601: step 14129, loss 0.00234214, acc 1
2016-09-05T22:21:45.077549: step 14130, loss 0.0104225, acc 1
2016-09-05T22:21:45.881990: step 14131, loss 0.0151723, acc 1
2016-09-05T22:21:46.679646: step 14132, loss 0.0747041, acc 0.96
2016-09-05T22:21:47.477460: step 14133, loss 0.0157585, acc 1
2016-09-05T22:21:48.277466: step 14134, loss 0.0466202, acc 0.96
2016-09-05T22:21:49.067536: step 14135, loss 0.00546837, acc 1
2016-09-05T22:21:49.883894: step 14136, loss 0.013611, acc 1
2016-09-05T22:21:50.700466: step 14137, loss 0.00738854, acc 1
2016-09-05T22:21:51.470848: step 14138, loss 0.0445005, acc 0.98
2016-09-05T22:21:52.281893: step 14139, loss 0.006211, acc 1
2016-09-05T22:21:53.090216: step 14140, loss 0.0201211, acc 0.98
2016-09-05T22:21:53.880391: step 14141, loss 0.0234495, acc 0.98
2016-09-05T22:21:54.700452: step 14142, loss 0.0144361, acc 1
2016-09-05T22:21:55.499019: step 14143, loss 0.0217984, acc 0.98
2016-09-05T22:21:56.308656: step 14144, loss 0.0144565, acc 1
2016-09-05T22:21:57.159181: step 14145, loss 0.00220704, acc 1
2016-09-05T22:21:57.966253: step 14146, loss 0.00224537, acc 1
2016-09-05T22:21:58.734082: step 14147, loss 0.00219188, acc 1
2016-09-05T22:21:59.509394: step 14148, loss 0.025643, acc 1
2016-09-05T22:22:00.379222: step 14149, loss 0.0034074, acc 1
2016-09-05T22:22:01.190240: step 14150, loss 0.0404956, acc 0.98
2016-09-05T22:22:02.016940: step 14151, loss 0.016286, acc 0.98
2016-09-05T22:22:02.849300: step 14152, loss 0.00450248, acc 1
2016-09-05T22:22:03.685451: step 14153, loss 0.00246317, acc 1
2016-09-05T22:22:04.508876: step 14154, loss 0.00909023, acc 1
2016-09-05T22:22:05.376122: step 14155, loss 0.0256931, acc 1
2016-09-05T22:22:06.197943: step 14156, loss 0.00315978, acc 1
2016-09-05T22:22:07.007450: step 14157, loss 0.0144821, acc 1
2016-09-05T22:22:07.850054: step 14158, loss 0.0261949, acc 0.98
2016-09-05T22:22:08.661303: step 14159, loss 0.0207671, acc 1
2016-09-05T22:22:09.469092: step 14160, loss 0.0265028, acc 1
2016-09-05T22:22:10.305820: step 14161, loss 0.00702779, acc 1
2016-09-05T22:22:10.752603: step 14162, loss 0.0191657, acc 1
2016-09-05T22:22:11.586077: step 14163, loss 0.0504891, acc 0.96
2016-09-05T22:22:12.421263: step 14164, loss 0.00934838, acc 1
2016-09-05T22:22:13.220373: step 14165, loss 0.00386535, acc 1
2016-09-05T22:22:14.013928: step 14166, loss 0.00361022, acc 1
2016-09-05T22:22:14.832477: step 14167, loss 0.00316803, acc 1
2016-09-05T22:22:15.637986: step 14168, loss 0.0241945, acc 1
2016-09-05T22:22:16.439668: step 14169, loss 0.0079884, acc 1
2016-09-05T22:22:17.256055: step 14170, loss 0.00690938, acc 1
2016-09-05T22:22:18.052094: step 14171, loss 0.0832885, acc 0.98
2016-09-05T22:22:18.857702: step 14172, loss 0.0258627, acc 0.98
2016-09-05T22:22:19.676659: step 14173, loss 0.0030407, acc 1
2016-09-05T22:22:20.484352: step 14174, loss 0.00295245, acc 1
2016-09-05T22:22:21.319045: step 14175, loss 0.023375, acc 1
2016-09-05T22:22:22.130813: step 14176, loss 0.00477625, acc 1
2016-09-05T22:22:22.925527: step 14177, loss 0.00242046, acc 1
2016-09-05T22:22:23.723086: step 14178, loss 0.00246376, acc 1
2016-09-05T22:22:24.540772: step 14179, loss 0.00506068, acc 1
2016-09-05T22:22:25.384038: step 14180, loss 0.018949, acc 0.98
2016-09-05T22:22:26.193925: step 14181, loss 0.0120472, acc 1
2016-09-05T22:22:27.027084: step 14182, loss 0.013644, acc 1
2016-09-05T22:22:27.849078: step 14183, loss 0.00322527, acc 1
2016-09-05T22:22:28.675014: step 14184, loss 0.00797627, acc 1
2016-09-05T22:22:29.509366: step 14185, loss 0.0176617, acc 0.98
2016-09-05T22:22:30.327664: step 14186, loss 0.0799125, acc 0.98
2016-09-05T22:22:31.130802: step 14187, loss 0.00487455, acc 1
2016-09-05T22:22:31.955042: step 14188, loss 0.00276174, acc 1
2016-09-05T22:22:32.793223: step 14189, loss 0.0202012, acc 0.98
2016-09-05T22:22:33.588308: step 14190, loss 0.00242918, acc 1
2016-09-05T22:22:34.419555: step 14191, loss 0.00224648, acc 1
2016-09-05T22:22:35.207260: step 14192, loss 0.00759062, acc 1
2016-09-05T22:22:36.019296: step 14193, loss 0.0128107, acc 1
2016-09-05T22:22:36.843972: step 14194, loss 0.0685909, acc 0.96
2016-09-05T22:22:37.681485: step 14195, loss 0.0145244, acc 1
2016-09-05T22:22:38.520295: step 14196, loss 0.0156911, acc 1
2016-09-05T22:22:39.341524: step 14197, loss 0.00280118, acc 1
2016-09-05T22:22:40.145721: step 14198, loss 0.00238208, acc 1
2016-09-05T22:22:40.947119: step 14199, loss 0.0308126, acc 0.98
2016-09-05T22:22:41.763281: step 14200, loss 0.00350647, acc 1

Evaluation:
2016-09-05T22:22:45.264029: step 14200, loss 2.8297, acc 0.704

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-14200

2016-09-05T22:22:47.199432: step 14201, loss 0.00252252, acc 1
2016-09-05T22:22:47.979037: step 14202, loss 0.0288052, acc 0.98
2016-09-05T22:22:48.803531: step 14203, loss 0.0602103, acc 0.96
2016-09-05T22:22:49.605475: step 14204, loss 0.00894387, acc 1
2016-09-05T22:22:50.388191: step 14205, loss 0.019293, acc 0.98
2016-09-05T22:22:51.192208: step 14206, loss 0.0213038, acc 0.98
2016-09-05T22:22:51.969208: step 14207, loss 0.0271232, acc 0.98
2016-09-05T22:22:52.780065: step 14208, loss 0.0403318, acc 0.96
2016-09-05T22:22:53.607185: step 14209, loss 0.0164449, acc 0.98
2016-09-05T22:22:54.412901: step 14210, loss 0.047155, acc 0.96
2016-09-05T22:22:55.207892: step 14211, loss 0.00226823, acc 1
2016-09-05T22:22:56.049437: step 14212, loss 0.00565118, acc 1
2016-09-05T22:22:56.822761: step 14213, loss 0.00440629, acc 1
2016-09-05T22:22:57.620737: step 14214, loss 0.00201558, acc 1
2016-09-05T22:22:58.431339: step 14215, loss 0.00765687, acc 1
2016-09-05T22:22:59.234967: step 14216, loss 0.0142229, acc 1
2016-09-05T22:23:00.054845: step 14217, loss 0.00955587, acc 1
2016-09-05T22:23:00.929839: step 14218, loss 0.0132961, acc 1
2016-09-05T22:23:01.718440: step 14219, loss 0.0151413, acc 1
2016-09-05T22:23:02.516995: step 14220, loss 0.00175619, acc 1
2016-09-05T22:23:03.328073: step 14221, loss 0.00402712, acc 1
2016-09-05T22:23:04.112177: step 14222, loss 0.00758822, acc 1
2016-09-05T22:23:04.919382: step 14223, loss 0.00386881, acc 1
2016-09-05T22:23:05.773303: step 14224, loss 0.0511053, acc 0.98
2016-09-05T22:23:06.565608: step 14225, loss 0.0441658, acc 0.96
2016-09-05T22:23:07.362172: step 14226, loss 0.0139032, acc 1
2016-09-05T22:23:08.181673: step 14227, loss 0.0028036, acc 1
2016-09-05T22:23:08.982861: step 14228, loss 0.00242224, acc 1
2016-09-05T22:23:09.781262: step 14229, loss 0.00194317, acc 1
2016-09-05T22:23:10.610282: step 14230, loss 0.0165629, acc 1
2016-09-05T22:23:11.427334: step 14231, loss 0.021106, acc 0.98
2016-09-05T22:23:12.243326: step 14232, loss 0.0246913, acc 0.98
2016-09-05T22:23:13.076094: step 14233, loss 0.00271817, acc 1
2016-09-05T22:23:13.904397: step 14234, loss 0.0166776, acc 0.98
2016-09-05T22:23:14.716545: step 14235, loss 0.00192568, acc 1
2016-09-05T22:23:15.530376: step 14236, loss 0.00266117, acc 1
2016-09-05T22:23:16.344437: step 14237, loss 0.0018527, acc 1
2016-09-05T22:23:17.144687: step 14238, loss 0.0372599, acc 0.96
2016-09-05T22:23:17.984560: step 14239, loss 0.00269399, acc 1
2016-09-05T22:23:18.788399: step 14240, loss 0.00181136, acc 1
2016-09-05T22:23:19.573651: step 14241, loss 0.00507869, acc 1
2016-09-05T22:23:20.407407: step 14242, loss 0.0597094, acc 0.98
2016-09-05T22:23:21.221355: step 14243, loss 0.119344, acc 0.94
2016-09-05T22:23:22.054595: step 14244, loss 0.00449127, acc 1
2016-09-05T22:23:22.881484: step 14245, loss 0.067306, acc 0.98
2016-09-05T22:23:23.688119: step 14246, loss 0.00230707, acc 1
2016-09-05T22:23:24.507079: step 14247, loss 0.00392367, acc 1
2016-09-05T22:23:25.351272: step 14248, loss 0.00652406, acc 1
2016-09-05T22:23:26.160534: step 14249, loss 0.00726351, acc 1
2016-09-05T22:23:26.969846: step 14250, loss 0.0284945, acc 0.98
2016-09-05T22:23:27.799627: step 14251, loss 0.00673832, acc 1
2016-09-05T22:23:28.590670: step 14252, loss 0.0364119, acc 0.98
2016-09-05T22:23:29.413686: step 14253, loss 0.0093411, acc 1
2016-09-05T22:23:30.238189: step 14254, loss 0.00288614, acc 1
2016-09-05T22:23:31.074895: step 14255, loss 0.0203234, acc 0.98
2016-09-05T22:23:31.885487: step 14256, loss 0.0149915, acc 1
2016-09-05T22:23:32.676375: step 14257, loss 0.0376709, acc 1
2016-09-05T22:23:33.490001: step 14258, loss 0.0236233, acc 0.98
2016-09-05T22:23:34.302543: step 14259, loss 0.00894532, acc 1
2016-09-05T22:23:35.110511: step 14260, loss 0.0400002, acc 1
2016-09-05T22:23:35.937671: step 14261, loss 0.133112, acc 0.94
2016-09-05T22:23:36.740807: step 14262, loss 0.0216494, acc 0.98
2016-09-05T22:23:37.536192: step 14263, loss 0.0107735, acc 1
2016-09-05T22:23:38.371753: step 14264, loss 0.0128996, acc 1
2016-09-05T22:23:39.161716: step 14265, loss 0.0153454, acc 1
2016-09-05T22:23:39.943469: step 14266, loss 0.0257056, acc 0.98
2016-09-05T22:23:40.798594: step 14267, loss 0.00287734, acc 1
2016-09-05T22:23:41.598347: step 14268, loss 0.00649752, acc 1
2016-09-05T22:23:42.383025: step 14269, loss 0.0397978, acc 0.98
2016-09-05T22:23:43.190793: step 14270, loss 0.00378672, acc 1
2016-09-05T22:23:44.016440: step 14271, loss 0.018001, acc 0.98
2016-09-05T22:23:44.824705: step 14272, loss 0.00167014, acc 1
2016-09-05T22:23:45.657838: step 14273, loss 0.0976859, acc 0.96
2016-09-05T22:23:46.453469: step 14274, loss 0.00152788, acc 1
2016-09-05T22:23:47.259700: step 14275, loss 0.0223936, acc 0.98
2016-09-05T22:23:48.091070: step 14276, loss 0.0249098, acc 1
2016-09-05T22:23:48.910888: step 14277, loss 0.00225455, acc 1
2016-09-05T22:23:49.726046: step 14278, loss 0.0439011, acc 0.98
2016-09-05T22:23:50.538404: step 14279, loss 0.0852247, acc 0.98
2016-09-05T22:23:51.369564: step 14280, loss 0.01328, acc 1
2016-09-05T22:23:52.174488: step 14281, loss 0.00167528, acc 1
2016-09-05T22:23:52.997627: step 14282, loss 0.0368617, acc 0.98
2016-09-05T22:23:53.803472: step 14283, loss 0.0750634, acc 0.96
2016-09-05T22:23:54.606362: step 14284, loss 0.0419397, acc 0.98
2016-09-05T22:23:55.452891: step 14285, loss 0.0317408, acc 0.98
2016-09-05T22:23:56.248875: step 14286, loss 0.0636953, acc 0.96
2016-09-05T22:23:57.038043: step 14287, loss 0.0325425, acc 0.98
2016-09-05T22:23:57.883946: step 14288, loss 0.00650784, acc 1
2016-09-05T22:23:58.747466: step 14289, loss 0.00843633, acc 1
2016-09-05T22:23:59.563408: step 14290, loss 0.00897153, acc 1
2016-09-05T22:24:00.429414: step 14291, loss 0.00352546, acc 1
2016-09-05T22:24:01.265472: step 14292, loss 0.0654765, acc 0.98
2016-09-05T22:24:02.181253: step 14293, loss 0.0141396, acc 1
2016-09-05T22:24:02.991322: step 14294, loss 0.0173563, acc 1
2016-09-05T22:24:03.827644: step 14295, loss 0.00544857, acc 1
2016-09-05T22:24:04.710491: step 14296, loss 0.00517351, acc 1
2016-09-05T22:24:05.530845: step 14297, loss 0.0177178, acc 1
2016-09-05T22:24:06.461417: step 14298, loss 0.0104596, acc 1
2016-09-05T22:24:07.294240: step 14299, loss 0.0149926, acc 1
2016-09-05T22:24:08.142490: step 14300, loss 0.00461444, acc 1

Evaluation:
2016-09-05T22:24:11.650760: step 14300, loss 3.36323, acc 0.702

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-14300

2016-09-05T22:24:13.529831: step 14301, loss 0.0273003, acc 0.98
2016-09-05T22:24:14.404103: step 14302, loss 0.0143166, acc 1
2016-09-05T22:24:15.210293: step 14303, loss 0.00485076, acc 1
2016-09-05T22:24:16.062036: step 14304, loss 0.0160831, acc 1
2016-09-05T22:24:16.930526: step 14305, loss 0.0244806, acc 0.98
2016-09-05T22:24:17.836633: step 14306, loss 0.00497101, acc 1
2016-09-05T22:24:18.658681: step 14307, loss 0.0141676, acc 1
2016-09-05T22:24:19.480586: step 14308, loss 0.0121345, acc 1
2016-09-05T22:24:20.303520: step 14309, loss 0.01028, acc 1
2016-09-05T22:24:21.133243: step 14310, loss 0.0398204, acc 0.98
2016-09-05T22:24:21.961339: step 14311, loss 0.023388, acc 0.98
2016-09-05T22:24:22.768412: step 14312, loss 0.00614032, acc 1
2016-09-05T22:24:23.655061: step 14313, loss 0.0218393, acc 0.98
2016-09-05T22:24:24.521668: step 14314, loss 0.033875, acc 0.98
2016-09-05T22:24:25.358495: step 14315, loss 0.0179743, acc 1
2016-09-05T22:24:26.197350: step 14316, loss 0.00764448, acc 1
2016-09-05T22:24:27.014244: step 14317, loss 0.0356688, acc 1
2016-09-05T22:24:27.809641: step 14318, loss 0.00462378, acc 1
2016-09-05T22:24:28.642518: step 14319, loss 0.00470053, acc 1
2016-09-05T22:24:29.484336: step 14320, loss 0.020495, acc 0.98
2016-09-05T22:24:30.304958: step 14321, loss 0.00452138, acc 1
2016-09-05T22:24:31.113776: step 14322, loss 0.00674796, acc 1
2016-09-05T22:24:31.948899: step 14323, loss 0.031605, acc 0.98
2016-09-05T22:24:32.769930: step 14324, loss 0.0170081, acc 1
2016-09-05T22:24:33.586734: step 14325, loss 0.0220592, acc 0.98
2016-09-05T22:24:34.422692: step 14326, loss 0.0298298, acc 0.98
2016-09-05T22:24:35.235849: step 14327, loss 0.017102, acc 1
2016-09-05T22:24:36.052395: step 14328, loss 0.00430442, acc 1
2016-09-05T22:24:36.897898: step 14329, loss 0.00419765, acc 1
2016-09-05T22:24:37.751655: step 14330, loss 0.00456119, acc 1
2016-09-05T22:24:38.558099: step 14331, loss 0.0112782, acc 1
2016-09-05T22:24:39.367380: step 14332, loss 0.0410665, acc 0.98
2016-09-05T22:24:40.349381: step 14333, loss 0.0338779, acc 0.98
2016-09-05T22:24:41.168286: step 14334, loss 0.00415268, acc 1
2016-09-05T22:24:41.996082: step 14335, loss 0.00484408, acc 1
2016-09-05T22:24:42.803668: step 14336, loss 0.00400384, acc 1
2016-09-05T22:24:43.607185: step 14337, loss 0.00797531, acc 1
2016-09-05T22:24:44.422463: step 14338, loss 0.00399951, acc 1
2016-09-05T22:24:45.247156: step 14339, loss 0.00388441, acc 1
2016-09-05T22:24:46.073297: step 14340, loss 0.0180009, acc 0.98
2016-09-05T22:24:46.867517: step 14341, loss 0.0206449, acc 0.98
2016-09-05T22:24:47.674944: step 14342, loss 0.0204062, acc 0.98
2016-09-05T22:24:48.486891: step 14343, loss 0.00418515, acc 1
2016-09-05T22:24:49.289915: step 14344, loss 0.00372566, acc 1
2016-09-05T22:24:50.092021: step 14345, loss 0.0184671, acc 0.98
2016-09-05T22:24:50.947910: step 14346, loss 0.00384085, acc 1
2016-09-05T22:24:51.720142: step 14347, loss 0.00375158, acc 1
2016-09-05T22:24:52.522848: step 14348, loss 0.00359376, acc 1
2016-09-05T22:24:53.321661: step 14349, loss 0.013992, acc 1
2016-09-05T22:24:54.153407: step 14350, loss 0.0034762, acc 1
2016-09-05T22:24:54.957399: step 14351, loss 0.0222444, acc 0.98
2016-09-05T22:24:55.795149: step 14352, loss 0.0173784, acc 0.98
2016-09-05T22:24:56.606392: step 14353, loss 0.00966663, acc 1
2016-09-05T22:24:57.407855: step 14354, loss 0.00608922, acc 1
2016-09-05T22:24:58.269975: step 14355, loss 0.0307765, acc 0.98
2016-09-05T22:24:58.696169: step 14356, loss 0.00326262, acc 1
2016-09-05T22:24:59.517142: step 14357, loss 0.0256384, acc 1
2016-09-05T22:25:00.322655: step 14358, loss 0.107449, acc 0.96
2016-09-05T22:25:01.128058: step 14359, loss 0.0031507, acc 1
2016-09-05T22:25:01.945219: step 14360, loss 0.0103833, acc 1
2016-09-05T22:25:02.756816: step 14361, loss 0.00315576, acc 1
2016-09-05T22:25:03.574767: step 14362, loss 0.0454428, acc 0.98
2016-09-05T22:25:04.409921: step 14363, loss 0.00899572, acc 1
2016-09-05T22:25:05.226126: step 14364, loss 0.0354433, acc 0.98
2016-09-05T22:25:06.075298: step 14365, loss 0.0027039, acc 1
2016-09-05T22:25:06.891570: step 14366, loss 0.0129625, acc 1
2016-09-05T22:25:07.716142: step 14367, loss 0.0307434, acc 1
2016-09-05T22:25:08.515916: step 14368, loss 0.0255457, acc 0.98
2016-09-05T22:25:09.327026: step 14369, loss 0.0290135, acc 0.98
2016-09-05T22:25:10.147433: step 14370, loss 0.0104866, acc 1
2016-09-05T22:25:10.943418: step 14371, loss 0.00254177, acc 1
2016-09-05T22:25:11.774844: step 14372, loss 0.00565566, acc 1
2016-09-05T22:25:12.605397: step 14373, loss 0.00287908, acc 1
2016-09-05T22:25:13.392471: step 14374, loss 0.00313197, acc 1
2016-09-05T22:25:14.239285: step 14375, loss 0.0689345, acc 0.98
2016-09-05T22:25:15.081863: step 14376, loss 0.0107748, acc 1
2016-09-05T22:25:15.857278: step 14377, loss 0.0252803, acc 0.98
2016-09-05T22:25:16.659493: step 14378, loss 0.0028341, acc 1
2016-09-05T22:25:17.484469: step 14379, loss 0.00234353, acc 1
2016-09-05T22:25:18.265811: step 14380, loss 0.0100584, acc 1
2016-09-05T22:25:19.086394: step 14381, loss 0.0230211, acc 0.98
2016-09-05T22:25:19.895364: step 14382, loss 0.00242228, acc 1
2016-09-05T22:25:20.751993: step 14383, loss 0.00239027, acc 1
2016-09-05T22:25:21.550258: step 14384, loss 0.00457183, acc 1
2016-09-05T22:25:22.356284: step 14385, loss 0.0024776, acc 1
2016-09-05T22:25:23.138884: step 14386, loss 0.00727064, acc 1
2016-09-05T22:25:23.938607: step 14387, loss 0.0346772, acc 0.96
2016-09-05T22:25:24.753592: step 14388, loss 0.0529118, acc 0.98
2016-09-05T22:25:25.550847: step 14389, loss 0.0196227, acc 1
2016-09-05T22:25:26.350685: step 14390, loss 0.0128789, acc 1
2016-09-05T22:25:27.165931: step 14391, loss 0.00266399, acc 1
2016-09-05T22:25:27.940616: step 14392, loss 0.0177802, acc 1
2016-09-05T22:25:28.745346: step 14393, loss 0.00848265, acc 1
2016-09-05T22:25:29.574222: step 14394, loss 0.00220378, acc 1
2016-09-05T22:25:30.373946: step 14395, loss 0.00219236, acc 1
2016-09-05T22:25:31.203871: step 14396, loss 0.00221794, acc 1
2016-09-05T22:25:32.015150: step 14397, loss 0.0214467, acc 0.98
2016-09-05T22:25:32.845766: step 14398, loss 0.00244907, acc 1
2016-09-05T22:25:33.661861: step 14399, loss 0.00928658, acc 1
2016-09-05T22:25:34.483595: step 14400, loss 0.0103363, acc 1

Evaluation:
2016-09-05T22:25:37.970340: step 14400, loss 2.65119, acc 0.715

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-14400

2016-09-05T22:25:40.017460: step 14401, loss 0.0178231, acc 0.98
2016-09-05T22:25:40.869457: step 14402, loss 0.0372292, acc 0.98
2016-09-05T22:25:41.679726: step 14403, loss 0.0114268, acc 1
2016-09-05T22:25:42.501009: step 14404, loss 0.0355999, acc 0.98
2016-09-05T22:25:43.345678: step 14405, loss 0.0156372, acc 1
2016-09-05T22:25:44.136695: step 14406, loss 0.0192061, acc 0.98
2016-09-05T22:25:44.920008: step 14407, loss 0.034719, acc 0.96
2016-09-05T22:25:45.717930: step 14408, loss 0.0317983, acc 0.98
2016-09-05T22:25:46.529808: step 14409, loss 0.00219423, acc 1
2016-09-05T22:25:47.295785: step 14410, loss 0.0123435, acc 1
2016-09-05T22:25:48.112477: step 14411, loss 0.00496461, acc 1
2016-09-05T22:25:48.948287: step 14412, loss 0.0119104, acc 1
2016-09-05T22:25:49.735856: step 14413, loss 0.0279453, acc 0.98
2016-09-05T22:25:50.532998: step 14414, loss 0.00216376, acc 1
2016-09-05T22:25:51.337504: step 14415, loss 0.0130286, acc 1
2016-09-05T22:25:52.174721: step 14416, loss 0.00210523, acc 1
2016-09-05T22:25:52.979325: step 14417, loss 0.0554963, acc 0.98
2016-09-05T22:25:53.784500: step 14418, loss 0.037787, acc 0.98
2016-09-05T22:25:54.567095: step 14419, loss 0.0284291, acc 0.98
2016-09-05T22:25:55.390880: step 14420, loss 0.0149207, acc 1
2016-09-05T22:25:56.231999: step 14421, loss 0.0151986, acc 1
2016-09-05T22:25:57.012177: step 14422, loss 0.0141488, acc 1
2016-09-05T22:25:57.807867: step 14423, loss 0.0473042, acc 0.98
2016-09-05T22:25:58.622089: step 14424, loss 0.00261071, acc 1
2016-09-05T22:25:59.402901: step 14425, loss 0.00184301, acc 1
2016-09-05T22:26:00.224448: step 14426, loss 0.0322964, acc 0.96
2016-09-05T22:26:01.025315: step 14427, loss 0.0466615, acc 0.98
2016-09-05T22:26:01.834803: step 14428, loss 0.00196607, acc 1
2016-09-05T22:26:02.657219: step 14429, loss 0.00189325, acc 1
2016-09-05T22:26:03.465209: step 14430, loss 0.00940776, acc 1
2016-09-05T22:26:04.268456: step 14431, loss 0.0100176, acc 1
2016-09-05T22:26:05.070684: step 14432, loss 0.0776388, acc 0.96
2016-09-05T22:26:05.914410: step 14433, loss 0.0211126, acc 1
2016-09-05T22:26:06.697483: step 14434, loss 0.00281133, acc 1
2016-09-05T22:26:07.501631: step 14435, loss 0.00220895, acc 1
2016-09-05T22:26:08.321429: step 14436, loss 0.0210983, acc 0.98
2016-09-05T22:26:09.099839: step 14437, loss 0.00199295, acc 1
2016-09-05T22:26:09.889693: step 14438, loss 0.00936469, acc 1
2016-09-05T22:26:10.691586: step 14439, loss 0.0286981, acc 0.98
2016-09-05T22:26:11.482582: step 14440, loss 0.00701538, acc 1
2016-09-05T22:26:12.351662: step 14441, loss 0.00271902, acc 1
2016-09-05T22:26:13.172629: step 14442, loss 0.0153211, acc 1
2016-09-05T22:26:13.925480: step 14443, loss 0.0053719, acc 1
2016-09-05T22:26:14.723751: step 14444, loss 0.00492555, acc 1
2016-09-05T22:26:15.535616: step 14445, loss 0.00334443, acc 1
2016-09-05T22:26:16.318037: step 14446, loss 0.0430375, acc 0.96
2016-09-05T22:26:17.134973: step 14447, loss 0.0159155, acc 0.98
2016-09-05T22:26:17.951492: step 14448, loss 0.00458416, acc 1
2016-09-05T22:26:18.754935: step 14449, loss 0.110635, acc 0.98
2016-09-05T22:26:19.565599: step 14450, loss 0.0430394, acc 0.98
2016-09-05T22:26:20.391032: step 14451, loss 0.0151369, acc 1
2016-09-05T22:26:21.175958: step 14452, loss 0.0123342, acc 1
2016-09-05T22:26:21.970848: step 14453, loss 0.00332714, acc 1
2016-09-05T22:26:22.788348: step 14454, loss 0.0898241, acc 0.96
2016-09-05T22:26:23.621425: step 14455, loss 0.0536026, acc 0.98
2016-09-05T22:26:24.430869: step 14456, loss 0.012198, acc 1
2016-09-05T22:26:25.245010: step 14457, loss 0.00478617, acc 1
2016-09-05T22:26:26.023346: step 14458, loss 0.0428281, acc 0.98
2016-09-05T22:26:26.825477: step 14459, loss 0.0194448, acc 1
2016-09-05T22:26:27.659468: step 14460, loss 0.0208559, acc 1
2016-09-05T22:26:28.411232: step 14461, loss 0.0456258, acc 0.98
2016-09-05T22:26:29.201060: step 14462, loss 0.0226126, acc 0.98
2016-09-05T22:26:30.026638: step 14463, loss 0.00549803, acc 1
2016-09-05T22:26:30.827472: step 14464, loss 0.00987918, acc 1
2016-09-05T22:26:31.646706: step 14465, loss 0.0175646, acc 1
2016-09-05T22:26:32.466911: step 14466, loss 0.0128704, acc 1
2016-09-05T22:26:33.255475: step 14467, loss 0.0254571, acc 0.98
2016-09-05T22:26:34.098702: step 14468, loss 0.00237353, acc 1
2016-09-05T22:26:34.916072: step 14469, loss 0.013836, acc 1
2016-09-05T22:26:35.677938: step 14470, loss 0.0363689, acc 0.98
2016-09-05T22:26:36.478940: step 14471, loss 0.00229363, acc 1
2016-09-05T22:26:37.328099: step 14472, loss 0.015951, acc 1
2016-09-05T22:26:38.121689: step 14473, loss 0.00204021, acc 1
2016-09-05T22:26:38.902561: step 14474, loss 0.0191321, acc 1
2016-09-05T22:26:39.724216: step 14475, loss 0.0025312, acc 1
2016-09-05T22:26:40.512077: step 14476, loss 0.0286076, acc 1
2016-09-05T22:26:41.350552: step 14477, loss 0.00951627, acc 1
2016-09-05T22:26:42.149323: step 14478, loss 0.00433048, acc 1
2016-09-05T22:26:42.918153: step 14479, loss 0.00241847, acc 1
2016-09-05T22:26:43.704930: step 14480, loss 0.00231365, acc 1
2016-09-05T22:26:44.523166: step 14481, loss 0.00267571, acc 1
2016-09-05T22:26:45.318670: step 14482, loss 0.00259707, acc 1
2016-09-05T22:26:46.133264: step 14483, loss 0.00257089, acc 1
2016-09-05T22:26:46.945483: step 14484, loss 0.0399747, acc 0.98
2016-09-05T22:26:47.751479: step 14485, loss 0.00286195, acc 1
2016-09-05T22:26:48.563730: step 14486, loss 0.009175, acc 1
2016-09-05T22:26:49.381464: step 14487, loss 0.046571, acc 0.98
2016-09-05T22:26:50.174271: step 14488, loss 0.0240253, acc 0.98
2016-09-05T22:26:50.971458: step 14489, loss 0.00499164, acc 1
2016-09-05T22:26:51.773226: step 14490, loss 0.00256693, acc 1
2016-09-05T22:26:52.562589: step 14491, loss 0.0374611, acc 0.98
2016-09-05T22:26:53.355702: step 14492, loss 0.00242676, acc 1
2016-09-05T22:26:54.177929: step 14493, loss 0.00625966, acc 1
2016-09-05T22:26:54.942236: step 14494, loss 0.0040215, acc 1
2016-09-05T22:26:55.756918: step 14495, loss 0.00266734, acc 1
2016-09-05T22:26:56.555673: step 14496, loss 0.0261201, acc 0.98
2016-09-05T22:26:57.331863: step 14497, loss 0.0116066, acc 1
2016-09-05T22:26:58.164259: step 14498, loss 0.0194829, acc 0.98
2016-09-05T22:26:58.977840: step 14499, loss 0.0395777, acc 0.98
2016-09-05T22:26:59.787405: step 14500, loss 0.00217215, acc 1

Evaluation:
2016-09-05T22:27:03.313571: step 14500, loss 2.77559, acc 0.716

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-14500

2016-09-05T22:27:05.277921: step 14501, loss 0.0045958, acc 1
2016-09-05T22:27:06.103066: step 14502, loss 0.0021081, acc 1
2016-09-05T22:27:06.914622: step 14503, loss 0.00244786, acc 1
2016-09-05T22:27:07.745397: step 14504, loss 0.0619026, acc 0.98
2016-09-05T22:27:08.566376: step 14505, loss 0.0166738, acc 1
2016-09-05T22:27:09.377746: step 14506, loss 0.00206465, acc 1
2016-09-05T22:27:10.193053: step 14507, loss 0.0100651, acc 1
2016-09-05T22:27:10.973848: step 14508, loss 0.0335046, acc 0.98
2016-09-05T22:27:11.795123: step 14509, loss 0.0371887, acc 0.96
2016-09-05T22:27:12.599384: step 14510, loss 0.0234077, acc 0.98
2016-09-05T22:27:13.416042: step 14511, loss 0.0309477, acc 0.98
2016-09-05T22:27:14.242808: step 14512, loss 0.00206472, acc 1
2016-09-05T22:27:15.074801: step 14513, loss 0.00189346, acc 1
2016-09-05T22:27:15.893714: step 14514, loss 0.00197213, acc 1
2016-09-05T22:27:16.738836: step 14515, loss 0.0187564, acc 0.98
2016-09-05T22:27:17.548883: step 14516, loss 0.0116141, acc 1
2016-09-05T22:27:18.363894: step 14517, loss 0.00475848, acc 1
2016-09-05T22:27:19.155401: step 14518, loss 0.00453203, acc 1
2016-09-05T22:27:19.955478: step 14519, loss 0.00204647, acc 1
2016-09-05T22:27:20.779791: step 14520, loss 0.017467, acc 1
2016-09-05T22:27:21.585012: step 14521, loss 0.00185979, acc 1
2016-09-05T22:27:22.384640: step 14522, loss 0.0193916, acc 1
2016-09-05T22:27:23.197622: step 14523, loss 0.0169099, acc 0.98
2016-09-05T22:27:23.976882: step 14524, loss 0.00202207, acc 1
2016-09-05T22:27:24.781948: step 14525, loss 0.02582, acc 0.98
2016-09-05T22:27:25.583286: step 14526, loss 0.0185376, acc 1
2016-09-05T22:27:26.385990: step 14527, loss 0.00184574, acc 1
2016-09-05T22:27:27.209230: step 14528, loss 0.0161313, acc 0.98
2016-09-05T22:27:28.039171: step 14529, loss 0.00267846, acc 1
2016-09-05T22:27:28.819016: step 14530, loss 0.0101519, acc 1
2016-09-05T22:27:29.631360: step 14531, loss 0.00534392, acc 1
2016-09-05T22:27:30.432137: step 14532, loss 0.00649438, acc 1
2016-09-05T22:27:31.236649: step 14533, loss 0.00199487, acc 1
2016-09-05T22:27:32.062392: step 14534, loss 0.00217116, acc 1
2016-09-05T22:27:32.913428: step 14535, loss 0.0173398, acc 0.98
2016-09-05T22:27:33.701698: step 14536, loss 0.0194579, acc 1
2016-09-05T22:27:34.538498: step 14537, loss 0.0997129, acc 0.98
2016-09-05T22:27:35.346208: step 14538, loss 0.00791438, acc 1
2016-09-05T22:27:36.159956: step 14539, loss 0.0114357, acc 1
2016-09-05T22:27:36.962795: step 14540, loss 0.0161112, acc 0.98
2016-09-05T22:27:37.796742: step 14541, loss 0.00175804, acc 1
2016-09-05T22:27:38.582593: step 14542, loss 0.0362767, acc 0.98
2016-09-05T22:27:39.384829: step 14543, loss 0.0272538, acc 0.98
2016-09-05T22:27:40.188352: step 14544, loss 0.00187654, acc 1
2016-09-05T22:27:40.982697: step 14545, loss 0.0111323, acc 1
2016-09-05T22:27:41.759709: step 14546, loss 0.0310148, acc 0.98
2016-09-05T22:27:42.581502: step 14547, loss 0.0752925, acc 0.98
2016-09-05T22:27:43.359629: step 14548, loss 0.00595107, acc 1
2016-09-05T22:27:44.172869: step 14549, loss 0.0031591, acc 1
2016-09-05T22:27:44.629448: step 14550, loss 0.047794, acc 1
2016-09-05T22:27:45.463845: step 14551, loss 0.00493851, acc 1
2016-09-05T22:27:46.323092: step 14552, loss 0.0888991, acc 0.96
2016-09-05T22:27:47.093739: step 14553, loss 0.0139356, acc 1
2016-09-05T22:27:47.891932: step 14554, loss 0.0020189, acc 1
2016-09-05T22:27:48.701021: step 14555, loss 0.0381332, acc 0.98
2016-09-05T22:27:49.528491: step 14556, loss 0.047546, acc 0.96
2016-09-05T22:27:50.329785: step 14557, loss 0.0128184, acc 1
2016-09-05T22:27:51.156655: step 14558, loss 0.0123947, acc 1
2016-09-05T22:27:51.972664: step 14559, loss 0.00825759, acc 1
2016-09-05T22:27:52.787375: step 14560, loss 0.00162932, acc 1
2016-09-05T22:27:53.638391: step 14561, loss 0.0224796, acc 0.98
2016-09-05T22:27:54.468700: step 14562, loss 0.0342185, acc 0.98
2016-09-05T22:27:55.306154: step 14563, loss 0.0170144, acc 1
2016-09-05T22:27:56.140834: step 14564, loss 0.00222014, acc 1
2016-09-05T22:27:56.948489: step 14565, loss 0.00717798, acc 1
2016-09-05T22:27:57.752415: step 14566, loss 0.00160636, acc 1
2016-09-05T22:27:58.609104: step 14567, loss 0.0168337, acc 0.98
2016-09-05T22:27:59.423912: step 14568, loss 0.00541542, acc 1
2016-09-05T22:28:00.250817: step 14569, loss 0.0029735, acc 1
2016-09-05T22:28:01.055682: step 14570, loss 0.0137853, acc 1
2016-09-05T22:28:01.905108: step 14571, loss 0.00209787, acc 1
2016-09-05T22:28:02.708337: step 14572, loss 0.0375285, acc 0.98
2016-09-05T22:28:03.497186: step 14573, loss 0.0024838, acc 1
2016-09-05T22:28:04.322250: step 14574, loss 0.00176697, acc 1
2016-09-05T22:28:05.118428: step 14575, loss 0.0193561, acc 1
2016-09-05T22:28:05.925966: step 14576, loss 0.00310975, acc 1
2016-09-05T22:28:06.723359: step 14577, loss 0.00165131, acc 1
2016-09-05T22:28:07.508015: step 14578, loss 0.0017476, acc 1
2016-09-05T22:28:08.332390: step 14579, loss 0.0673408, acc 0.96
2016-09-05T22:28:09.118264: step 14580, loss 0.014673, acc 1
2016-09-05T22:28:09.918635: step 14581, loss 0.0160911, acc 1
2016-09-05T22:28:10.754729: step 14582, loss 0.0288275, acc 0.98
2016-09-05T22:28:11.555957: step 14583, loss 0.00255586, acc 1
2016-09-05T22:28:12.345541: step 14584, loss 0.00168743, acc 1
2016-09-05T22:28:13.194352: step 14585, loss 0.023925, acc 0.98
2016-09-05T22:28:13.990667: step 14586, loss 0.00194181, acc 1
2016-09-05T22:28:14.794941: step 14587, loss 0.0155161, acc 1
2016-09-05T22:28:15.587031: step 14588, loss 0.0261012, acc 0.98
2016-09-05T22:28:16.411729: step 14589, loss 0.00177851, acc 1
2016-09-05T22:28:17.196878: step 14590, loss 0.00893254, acc 1
2016-09-05T22:28:18.000325: step 14591, loss 0.019951, acc 1
2016-09-05T22:28:18.847074: step 14592, loss 0.00163432, acc 1
2016-09-05T22:28:19.626296: step 14593, loss 0.0123086, acc 1
2016-09-05T22:28:20.414834: step 14594, loss 0.0153048, acc 1
2016-09-05T22:28:21.242603: step 14595, loss 0.00254883, acc 1
2016-09-05T22:28:22.040983: step 14596, loss 0.0218137, acc 0.98
2016-09-05T22:28:22.924783: step 14597, loss 0.0240926, acc 0.98
2016-09-05T22:28:23.757397: step 14598, loss 0.00389233, acc 1
2016-09-05T22:28:24.560136: step 14599, loss 0.00166179, acc 1
2016-09-05T22:28:25.351246: step 14600, loss 0.00479974, acc 1

Evaluation:
2016-09-05T22:28:28.874360: step 14600, loss 2.5639, acc 0.71

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-14600

2016-09-05T22:28:30.770759: step 14601, loss 0.00167945, acc 1
2016-09-05T22:28:31.556473: step 14602, loss 0.00694826, acc 1
2016-09-05T22:28:32.413252: step 14603, loss 0.0158679, acc 0.98
2016-09-05T22:28:33.240271: step 14604, loss 0.00237363, acc 1
2016-09-05T22:28:34.080864: step 14605, loss 0.00202064, acc 1
2016-09-05T22:28:34.911580: step 14606, loss 0.00256427, acc 1
2016-09-05T22:28:35.724270: step 14607, loss 0.0471358, acc 0.98
2016-09-05T22:28:36.518811: step 14608, loss 0.00204589, acc 1
2016-09-05T22:28:37.350132: step 14609, loss 0.0020959, acc 1
2016-09-05T22:28:38.173418: step 14610, loss 0.00228513, acc 1
2016-09-05T22:28:38.967290: step 14611, loss 0.0168466, acc 1
2016-09-05T22:28:39.781028: step 14612, loss 0.00159893, acc 1
2016-09-05T22:28:40.601220: step 14613, loss 0.0361254, acc 0.98
2016-09-05T22:28:41.405636: step 14614, loss 0.0252338, acc 0.98
2016-09-05T22:28:42.230834: step 14615, loss 0.00528187, acc 1
2016-09-05T22:28:43.027630: step 14616, loss 0.0158261, acc 1
2016-09-05T22:28:43.831543: step 14617, loss 0.00193152, acc 1
2016-09-05T22:28:44.624555: step 14618, loss 0.00153364, acc 1
2016-09-05T22:28:45.433281: step 14619, loss 0.00434142, acc 1
2016-09-05T22:28:46.259648: step 14620, loss 0.00458795, acc 1
2016-09-05T22:28:47.057124: step 14621, loss 0.00530838, acc 1
2016-09-05T22:28:47.856734: step 14622, loss 0.0270254, acc 0.98
2016-09-05T22:28:48.639642: step 14623, loss 0.00151748, acc 1
2016-09-05T22:28:49.453795: step 14624, loss 0.00152176, acc 1
2016-09-05T22:28:50.268626: step 14625, loss 0.0091241, acc 1
2016-09-05T22:28:51.057812: step 14626, loss 0.0197376, acc 1
2016-09-05T22:28:51.889729: step 14627, loss 0.00245761, acc 1
2016-09-05T22:28:52.712289: step 14628, loss 0.00156182, acc 1
2016-09-05T22:28:53.517574: step 14629, loss 0.0130912, acc 1
2016-09-05T22:28:54.345722: step 14630, loss 0.0376884, acc 0.96
2016-09-05T22:28:55.130075: step 14631, loss 0.0250847, acc 0.98
2016-09-05T22:28:55.921839: step 14632, loss 0.0018703, acc 1
2016-09-05T22:28:56.718552: step 14633, loss 0.00168087, acc 1
2016-09-05T22:28:57.525568: step 14634, loss 0.00171249, acc 1
2016-09-05T22:28:58.313422: step 14635, loss 0.00444635, acc 1
2016-09-05T22:28:59.133491: step 14636, loss 0.019478, acc 1
2016-09-05T22:28:59.962949: step 14637, loss 0.00158329, acc 1
2016-09-05T22:29:00.761100: step 14638, loss 0.00844386, acc 1
2016-09-05T22:29:01.547979: step 14639, loss 0.0290098, acc 0.98
2016-09-05T22:29:02.347212: step 14640, loss 0.031756, acc 0.98
2016-09-05T22:29:03.142627: step 14641, loss 0.00226525, acc 1
2016-09-05T22:29:03.963633: step 14642, loss 0.00287917, acc 1
2016-09-05T22:29:04.782901: step 14643, loss 0.00715872, acc 1
2016-09-05T22:29:05.556963: step 14644, loss 0.00279433, acc 1
2016-09-05T22:29:06.358386: step 14645, loss 0.0107146, acc 1
2016-09-05T22:29:07.163393: step 14646, loss 0.0018336, acc 1
2016-09-05T22:29:07.997588: step 14647, loss 0.0138827, acc 1
2016-09-05T22:29:08.832942: step 14648, loss 0.0179342, acc 0.98
2016-09-05T22:29:09.649419: step 14649, loss 0.0141522, acc 1
2016-09-05T22:29:10.454564: step 14650, loss 0.0177764, acc 0.98
2016-09-05T22:29:11.265499: step 14651, loss 0.00161582, acc 1
2016-09-05T22:29:12.102639: step 14652, loss 0.00163398, acc 1
2016-09-05T22:29:12.899011: step 14653, loss 0.00190741, acc 1
2016-09-05T22:29:13.697317: step 14654, loss 0.0141404, acc 1
2016-09-05T22:29:14.509326: step 14655, loss 0.0544405, acc 0.98
2016-09-05T22:29:15.312945: step 14656, loss 0.0277301, acc 0.98
2016-09-05T22:29:16.108427: step 14657, loss 0.00326246, acc 1
2016-09-05T22:29:16.924081: step 14658, loss 0.00386548, acc 1
2016-09-05T22:29:17.730341: step 14659, loss 0.00239664, acc 1
2016-09-05T22:29:18.531571: step 14660, loss 0.00481685, acc 1
2016-09-05T22:29:19.336420: step 14661, loss 0.0209267, acc 0.98
2016-09-05T22:29:20.132800: step 14662, loss 0.00151141, acc 1
2016-09-05T22:29:20.898065: step 14663, loss 0.00204282, acc 1
2016-09-05T22:29:21.708640: step 14664, loss 0.0280293, acc 0.98
2016-09-05T22:29:22.496427: step 14665, loss 0.00980132, acc 1
2016-09-05T22:29:23.342739: step 14666, loss 0.0296794, acc 1
2016-09-05T22:29:24.169812: step 14667, loss 0.0547696, acc 0.98
2016-09-05T22:29:24.949405: step 14668, loss 0.00347813, acc 1
2016-09-05T22:29:25.738841: step 14669, loss 0.00192101, acc 1
2016-09-05T22:29:26.582405: step 14670, loss 0.0474222, acc 0.98
2016-09-05T22:29:27.368589: step 14671, loss 0.0473117, acc 0.96
2016-09-05T22:29:28.150910: step 14672, loss 0.013626, acc 1
2016-09-05T22:29:28.995362: step 14673, loss 0.00142048, acc 1
2016-09-05T22:29:29.801602: step 14674, loss 0.00404771, acc 1
2016-09-05T22:29:30.584772: step 14675, loss 0.0354327, acc 0.98
2016-09-05T22:29:31.383203: step 14676, loss 0.00146465, acc 1
2016-09-05T22:29:32.183929: step 14677, loss 0.00148037, acc 1
2016-09-05T22:29:32.989598: step 14678, loss 0.0147324, acc 1
2016-09-05T22:29:33.790971: step 14679, loss 0.00147778, acc 1
2016-09-05T22:29:34.589338: step 14680, loss 0.00788577, acc 1
2016-09-05T22:29:35.398589: step 14681, loss 0.028835, acc 1
2016-09-05T22:29:36.233443: step 14682, loss 0.0110127, acc 1
2016-09-05T22:29:37.033787: step 14683, loss 0.00138277, acc 1
2016-09-05T22:29:37.803407: step 14684, loss 0.0184306, acc 0.98
2016-09-05T22:29:38.616574: step 14685, loss 0.00419425, acc 1
2016-09-05T22:29:39.424114: step 14686, loss 0.0216885, acc 0.98
2016-09-05T22:29:40.232842: step 14687, loss 0.046942, acc 0.98
2016-09-05T22:29:41.038736: step 14688, loss 0.00138982, acc 1
2016-09-05T22:29:41.833001: step 14689, loss 0.00276449, acc 1
2016-09-05T22:29:42.623524: step 14690, loss 0.0367702, acc 0.98
2016-09-05T22:29:43.424701: step 14691, loss 0.00137773, acc 1
2016-09-05T22:29:44.229218: step 14692, loss 0.0132118, acc 1
2016-09-05T22:29:45.039278: step 14693, loss 0.00205496, acc 1
2016-09-05T22:29:45.851425: step 14694, loss 0.0118526, acc 1
2016-09-05T22:29:46.634495: step 14695, loss 0.0228326, acc 0.98
2016-09-05T22:29:47.440996: step 14696, loss 0.0079316, acc 1
2016-09-05T22:29:48.241101: step 14697, loss 0.00142216, acc 1
2016-09-05T22:29:49.039504: step 14698, loss 0.0105517, acc 1
2016-09-05T22:29:49.851308: step 14699, loss 0.0147494, acc 1
2016-09-05T22:29:50.651975: step 14700, loss 0.0017202, acc 1

Evaluation:
2016-09-05T22:29:54.159570: step 14700, loss 2.50248, acc 0.707

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-14700

2016-09-05T22:29:56.101473: step 14701, loss 0.0128777, acc 1
2016-09-05T22:29:56.936302: step 14702, loss 0.0131251, acc 1
2016-09-05T22:29:57.732783: step 14703, loss 0.00261027, acc 1
2016-09-05T22:29:58.540128: step 14704, loss 0.0126592, acc 1
2016-09-05T22:29:59.371889: step 14705, loss 0.0311261, acc 1
2016-09-05T22:30:00.176069: step 14706, loss 0.00155894, acc 1
2016-09-05T22:30:01.018492: step 14707, loss 0.00156876, acc 1
2016-09-05T22:30:01.826537: step 14708, loss 0.00159931, acc 1
2016-09-05T22:30:02.655430: step 14709, loss 0.00161806, acc 1
2016-09-05T22:30:03.440888: step 14710, loss 0.0482976, acc 0.98
2016-09-05T22:30:04.249552: step 14711, loss 0.0230712, acc 0.98
2016-09-05T22:30:05.050353: step 14712, loss 0.0155688, acc 0.98
2016-09-05T22:30:05.851856: step 14713, loss 0.0228934, acc 0.98
2016-09-05T22:30:06.711639: step 14714, loss 0.00435134, acc 1
2016-09-05T22:30:07.530738: step 14715, loss 0.00163645, acc 1
2016-09-05T22:30:08.369706: step 14716, loss 0.0426659, acc 0.98
2016-09-05T22:30:09.223705: step 14717, loss 0.00269837, acc 1
2016-09-05T22:30:10.046709: step 14718, loss 0.0147543, acc 1
2016-09-05T22:30:10.851204: step 14719, loss 0.0120619, acc 1
2016-09-05T22:30:11.693770: step 14720, loss 0.0456597, acc 0.98
2016-09-05T22:30:12.514330: step 14721, loss 0.0143918, acc 1
2016-09-05T22:30:13.335498: step 14722, loss 0.00193722, acc 1
2016-09-05T22:30:14.171916: step 14723, loss 0.0418715, acc 0.96
2016-09-05T22:30:14.963582: step 14724, loss 0.00158865, acc 1
2016-09-05T22:30:15.769046: step 14725, loss 0.0141377, acc 1
2016-09-05T22:30:16.601607: step 14726, loss 0.0224759, acc 0.98
2016-09-05T22:30:17.415726: step 14727, loss 0.0305037, acc 0.98
2016-09-05T22:30:18.231370: step 14728, loss 0.00153431, acc 1
2016-09-05T22:30:19.046078: step 14729, loss 0.0876557, acc 0.98
2016-09-05T22:30:19.873694: step 14730, loss 0.0312214, acc 0.96
2016-09-05T22:30:20.657865: step 14731, loss 0.0522157, acc 0.98
2016-09-05T22:30:21.473413: step 14732, loss 0.00466399, acc 1
2016-09-05T22:30:22.330845: step 14733, loss 0.00838478, acc 1
2016-09-05T22:30:23.111856: step 14734, loss 0.00431197, acc 1
2016-09-05T22:30:23.919641: step 14735, loss 0.00453179, acc 1
2016-09-05T22:30:24.734613: step 14736, loss 0.0128856, acc 1
2016-09-05T22:30:25.496480: step 14737, loss 0.0372946, acc 1
2016-09-05T22:30:26.321398: step 14738, loss 0.00135715, acc 1
2016-09-05T22:30:27.150108: step 14739, loss 0.00609356, acc 1
2016-09-05T22:30:27.950344: step 14740, loss 0.02999, acc 1
2016-09-05T22:30:28.737929: step 14741, loss 0.0444535, acc 0.98
2016-09-05T22:30:29.548641: step 14742, loss 0.0238233, acc 1
2016-09-05T22:30:30.309388: step 14743, loss 0.00632232, acc 1
2016-09-05T22:30:30.753224: step 14744, loss 0.00516804, acc 1
2016-09-05T22:30:31.550532: step 14745, loss 0.0297513, acc 0.98
2016-09-05T22:30:32.367881: step 14746, loss 0.00969882, acc 1
2016-09-05T22:30:33.171670: step 14747, loss 0.00913883, acc 1
2016-09-05T22:30:33.979647: step 14748, loss 0.00669676, acc 1
2016-09-05T22:30:34.786083: step 14749, loss 0.0136482, acc 1
2016-09-05T22:30:35.609635: step 14750, loss 0.0973057, acc 0.98
2016-09-05T22:30:36.367540: step 14751, loss 0.0148827, acc 1
2016-09-05T22:30:37.163393: step 14752, loss 0.00229131, acc 1
2016-09-05T22:30:38.003985: step 14753, loss 0.00599395, acc 1
2016-09-05T22:30:38.778333: step 14754, loss 0.00217617, acc 1
2016-09-05T22:30:39.575245: step 14755, loss 0.00309681, acc 1
2016-09-05T22:30:40.404424: step 14756, loss 0.00331215, acc 1
2016-09-05T22:30:41.172854: step 14757, loss 0.0149291, acc 1
2016-09-05T22:30:41.983143: step 14758, loss 0.0563814, acc 0.98
2016-09-05T22:30:42.783762: step 14759, loss 0.00164935, acc 1
2016-09-05T22:30:43.562892: step 14760, loss 0.00164135, acc 1
2016-09-05T22:30:44.356166: step 14761, loss 0.0284627, acc 0.98
2016-09-05T22:30:45.163703: step 14762, loss 0.012461, acc 1
2016-09-05T22:30:45.971694: step 14763, loss 0.0187381, acc 0.98
2016-09-05T22:30:46.778049: step 14764, loss 0.00179243, acc 1
2016-09-05T22:30:47.586391: step 14765, loss 0.00290222, acc 1
2016-09-05T22:30:48.413405: step 14766, loss 0.0194496, acc 1
2016-09-05T22:30:49.219521: step 14767, loss 0.00568477, acc 1
2016-09-05T22:30:50.010217: step 14768, loss 0.0180667, acc 0.98
2016-09-05T22:30:50.785365: step 14769, loss 0.00213363, acc 1
2016-09-05T22:30:51.634217: step 14770, loss 0.00236553, acc 1
2016-09-05T22:30:52.502086: step 14771, loss 0.0180683, acc 0.98
2016-09-05T22:30:53.293012: step 14772, loss 0.00254592, acc 1
2016-09-05T22:30:54.140060: step 14773, loss 0.01347, acc 1
2016-09-05T22:30:54.972728: step 14774, loss 0.00785929, acc 1
2016-09-05T22:30:55.797829: step 14775, loss 0.0483262, acc 0.98
2016-09-05T22:30:56.591156: step 14776, loss 0.0646014, acc 0.96
2016-09-05T22:30:57.440472: step 14777, loss 0.00188084, acc 1
2016-09-05T22:30:58.253848: step 14778, loss 0.0122881, acc 1
2016-09-05T22:30:59.048284: step 14779, loss 0.00659553, acc 1
2016-09-05T22:30:59.858346: step 14780, loss 0.0549989, acc 0.98
2016-09-05T22:31:00.675711: step 14781, loss 0.00270984, acc 1
2016-09-05T22:31:01.487881: step 14782, loss 0.00592455, acc 1
2016-09-05T22:31:02.339250: step 14783, loss 0.00209946, acc 1
2016-09-05T22:31:03.142978: step 14784, loss 0.00260668, acc 1
2016-09-05T22:31:03.940103: step 14785, loss 0.0531339, acc 0.98
2016-09-05T22:31:04.778494: step 14786, loss 0.02968, acc 0.98
2016-09-05T22:31:05.572773: step 14787, loss 0.0141356, acc 1
2016-09-05T22:31:06.374821: step 14788, loss 0.00247857, acc 1
2016-09-05T22:31:07.207292: step 14789, loss 0.0043242, acc 1
2016-09-05T22:31:08.009417: step 14790, loss 0.0284535, acc 0.98
2016-09-05T22:31:08.825118: step 14791, loss 0.00347989, acc 1
2016-09-05T22:31:09.650030: step 14792, loss 0.0212475, acc 0.98
2016-09-05T22:31:10.497335: step 14793, loss 0.00201665, acc 1
2016-09-05T22:31:11.306106: step 14794, loss 0.0308549, acc 0.98
2016-09-05T22:31:12.180843: step 14795, loss 0.00226983, acc 1
2016-09-05T22:31:12.993182: step 14796, loss 0.00189104, acc 1
2016-09-05T22:31:13.786686: step 14797, loss 0.0111091, acc 1
2016-09-05T22:31:14.576601: step 14798, loss 0.00597464, acc 1
2016-09-05T22:31:15.374153: step 14799, loss 0.00192091, acc 1
2016-09-05T22:31:16.179042: step 14800, loss 0.0138637, acc 1

Evaluation:
2016-09-05T22:31:19.687179: step 14800, loss 2.86312, acc 0.716

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-14800

2016-09-05T22:31:21.566896: step 14801, loss 0.0255681, acc 0.98
2016-09-05T22:31:22.384393: step 14802, loss 0.00197546, acc 1
2016-09-05T22:31:23.187460: step 14803, loss 0.00197485, acc 1
2016-09-05T22:31:23.998796: step 14804, loss 0.00195092, acc 1
2016-09-05T22:31:24.820734: step 14805, loss 0.023147, acc 1
2016-09-05T22:31:25.638545: step 14806, loss 0.00919424, acc 1
2016-09-05T22:31:26.464137: step 14807, loss 0.0415696, acc 0.98
2016-09-05T22:31:27.290990: step 14808, loss 0.00342663, acc 1
2016-09-05T22:31:28.141773: step 14809, loss 0.00596826, acc 1
2016-09-05T22:31:28.997219: step 14810, loss 0.0028465, acc 1
2016-09-05T22:31:29.806078: step 14811, loss 0.00310307, acc 1
2016-09-05T22:31:30.610460: step 14812, loss 0.00286506, acc 1
2016-09-05T22:31:31.415270: step 14813, loss 0.00544595, acc 1
2016-09-05T22:31:32.215420: step 14814, loss 0.0160298, acc 1
2016-09-05T22:31:33.016806: step 14815, loss 0.00190681, acc 1
2016-09-05T22:31:33.852431: step 14816, loss 0.0250308, acc 0.98
2016-09-05T22:31:34.681008: step 14817, loss 0.00189424, acc 1
2016-09-05T22:31:35.474276: step 14818, loss 0.00248769, acc 1
2016-09-05T22:31:36.303941: step 14819, loss 0.00286929, acc 1
2016-09-05T22:31:37.149702: step 14820, loss 0.169735, acc 0.96
2016-09-05T22:31:37.961087: step 14821, loss 0.00538991, acc 1
2016-09-05T22:31:38.804226: step 14822, loss 0.0179162, acc 0.98
2016-09-05T22:31:39.621358: step 14823, loss 0.0235369, acc 0.98
2016-09-05T22:31:40.415777: step 14824, loss 0.0113227, acc 1
2016-09-05T22:31:41.201315: step 14825, loss 0.00366532, acc 1
2016-09-05T22:31:42.004594: step 14826, loss 0.00159448, acc 1
2016-09-05T22:31:42.829465: step 14827, loss 0.0237369, acc 1
2016-09-05T22:31:43.671410: step 14828, loss 0.00155965, acc 1
2016-09-05T22:31:44.478781: step 14829, loss 0.00814149, acc 1
2016-09-05T22:31:45.270644: step 14830, loss 0.0698679, acc 0.96
2016-09-05T22:31:46.123289: step 14831, loss 0.0138153, acc 1
2016-09-05T22:31:46.940718: step 14832, loss 0.00332723, acc 1
2016-09-05T22:31:47.723910: step 14833, loss 0.00603913, acc 1
2016-09-05T22:31:48.525578: step 14834, loss 0.0508846, acc 0.98
2016-09-05T22:31:49.340024: step 14835, loss 0.00361175, acc 1
2016-09-05T22:31:50.135445: step 14836, loss 0.00955512, acc 1
2016-09-05T22:31:50.933313: step 14837, loss 0.00162194, acc 1
2016-09-05T22:31:51.721263: step 14838, loss 0.00520208, acc 1
2016-09-05T22:31:52.510087: step 14839, loss 0.01911, acc 0.98
2016-09-05T22:31:53.314060: step 14840, loss 0.00175577, acc 1
2016-09-05T22:31:54.144105: step 14841, loss 0.00685787, acc 1
2016-09-05T22:31:54.942490: step 14842, loss 0.0273635, acc 1
2016-09-05T22:31:55.728960: step 14843, loss 0.00646711, acc 1
2016-09-05T22:31:56.533327: step 14844, loss 0.0367247, acc 0.98
2016-09-05T22:31:57.326568: step 14845, loss 0.00210602, acc 1
2016-09-05T22:31:58.118950: step 14846, loss 0.0144993, acc 1
2016-09-05T22:31:58.942143: step 14847, loss 0.0160057, acc 1
2016-09-05T22:31:59.749316: step 14848, loss 0.022055, acc 0.98
2016-09-05T22:32:00.599440: step 14849, loss 0.00355048, acc 1
2016-09-05T22:32:01.434058: step 14850, loss 0.00254796, acc 1
2016-09-05T22:32:02.238654: step 14851, loss 0.0485435, acc 0.98
2016-09-05T22:32:03.053189: step 14852, loss 0.00547669, acc 1
2016-09-05T22:32:03.856424: step 14853, loss 0.0342987, acc 1
2016-09-05T22:32:04.645942: step 14854, loss 0.00181353, acc 1
2016-09-05T22:32:05.477383: step 14855, loss 0.161674, acc 0.92
2016-09-05T22:32:06.301965: step 14856, loss 0.00416258, acc 1
2016-09-05T22:32:07.107647: step 14857, loss 0.0298894, acc 0.98
2016-09-05T22:32:07.955713: step 14858, loss 0.0215935, acc 0.98
2016-09-05T22:32:08.791722: step 14859, loss 0.00253231, acc 1
2016-09-05T22:32:09.600050: step 14860, loss 0.0257892, acc 1
2016-09-05T22:32:10.388537: step 14861, loss 0.0163244, acc 1
2016-09-05T22:32:11.197656: step 14862, loss 0.0229565, acc 0.98
2016-09-05T22:32:12.001991: step 14863, loss 0.00228658, acc 1
2016-09-05T22:32:12.809694: step 14864, loss 0.0324579, acc 0.98
2016-09-05T22:32:13.664833: step 14865, loss 0.00551767, acc 1
2016-09-05T22:32:14.461124: step 14866, loss 0.0147387, acc 1
2016-09-05T22:32:15.263967: step 14867, loss 0.00799481, acc 1
2016-09-05T22:32:16.110272: step 14868, loss 0.103821, acc 0.98
2016-09-05T22:32:16.924258: step 14869, loss 0.00334583, acc 1
2016-09-05T22:32:17.719022: step 14870, loss 0.00228927, acc 1
2016-09-05T22:32:18.540036: step 14871, loss 0.0109104, acc 1
2016-09-05T22:32:19.354521: step 14872, loss 0.00201947, acc 1
2016-09-05T22:32:20.163075: step 14873, loss 0.00357972, acc 1
2016-09-05T22:32:21.041429: step 14874, loss 0.0316395, acc 0.98
2016-09-05T22:32:21.830953: step 14875, loss 0.0263124, acc 0.98
2016-09-05T22:32:22.636548: step 14876, loss 0.0200187, acc 1
2016-09-05T22:32:23.482324: step 14877, loss 0.0101433, acc 1
2016-09-05T22:32:24.338449: step 14878, loss 0.0230466, acc 1
2016-09-05T22:32:25.137049: step 14879, loss 0.0408131, acc 0.98
2016-09-05T22:32:25.933580: step 14880, loss 0.025955, acc 1
2016-09-05T22:32:26.752010: step 14881, loss 0.00681066, acc 1
2016-09-05T22:32:27.550409: step 14882, loss 0.00245378, acc 1
2016-09-05T22:32:28.348715: step 14883, loss 0.00426838, acc 1
2016-09-05T22:32:29.177648: step 14884, loss 0.0211288, acc 0.98
2016-09-05T22:32:29.965974: step 14885, loss 0.00952766, acc 1
2016-09-05T22:32:30.762731: step 14886, loss 0.0689262, acc 0.98
2016-09-05T22:32:31.594991: step 14887, loss 0.00632695, acc 1
2016-09-05T22:32:32.381359: step 14888, loss 0.0290472, acc 0.98
2016-09-05T22:32:33.185337: step 14889, loss 0.0248399, acc 1
2016-09-05T22:32:34.013850: step 14890, loss 0.00332078, acc 1
2016-09-05T22:32:34.805579: step 14891, loss 0.00239449, acc 1
2016-09-05T22:32:35.610282: step 14892, loss 0.0098863, acc 1
2016-09-05T22:32:36.419918: step 14893, loss 0.00204567, acc 1
2016-09-05T22:32:37.222656: step 14894, loss 0.036452, acc 0.98
2016-09-05T22:32:38.005369: step 14895, loss 0.0205409, acc 1
2016-09-05T22:32:38.825961: step 14896, loss 0.104472, acc 0.96
2016-09-05T22:32:39.632465: step 14897, loss 0.0152199, acc 1
2016-09-05T22:32:40.434376: step 14898, loss 0.0138404, acc 1
2016-09-05T22:32:41.227998: step 14899, loss 0.0448829, acc 0.98
2016-09-05T22:32:42.023848: step 14900, loss 0.00201207, acc 1

Evaluation:
2016-09-05T22:32:45.538140: step 14900, loss 2.41012, acc 0.701

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-14900

2016-09-05T22:32:47.324588: step 14901, loss 0.0028475, acc 1
2016-09-05T22:32:48.162584: step 14902, loss 0.00675938, acc 1
2016-09-05T22:32:49.009814: step 14903, loss 0.00195458, acc 1
2016-09-05T22:32:49.833015: step 14904, loss 0.0051408, acc 1
2016-09-05T22:32:50.650601: step 14905, loss 0.031822, acc 0.98
2016-09-05T22:32:51.448840: step 14906, loss 0.00626442, acc 1
2016-09-05T22:32:52.277732: step 14907, loss 0.00192879, acc 1
2016-09-05T22:32:53.054389: step 14908, loss 0.010563, acc 1
2016-09-05T22:32:53.870961: step 14909, loss 0.00188301, acc 1
2016-09-05T22:32:54.686019: step 14910, loss 0.0320389, acc 0.98
2016-09-05T22:32:55.475595: step 14911, loss 0.0213091, acc 0.98
2016-09-05T22:32:56.291071: step 14912, loss 0.00738342, acc 1
2016-09-05T22:32:57.132389: step 14913, loss 0.00747154, acc 1
2016-09-05T22:32:57.928920: step 14914, loss 0.0311878, acc 0.98
2016-09-05T22:32:58.711553: step 14915, loss 0.00548972, acc 1
2016-09-05T22:32:59.567130: step 14916, loss 0.01047, acc 1
2016-09-05T22:33:00.375399: step 14917, loss 0.00767875, acc 1
2016-09-05T22:33:01.173397: step 14918, loss 0.0154364, acc 1
2016-09-05T22:33:01.979180: step 14919, loss 0.0740572, acc 0.98
2016-09-05T22:33:02.756436: step 14920, loss 0.0124843, acc 1
2016-09-05T22:33:03.568442: step 14921, loss 0.0018928, acc 1
2016-09-05T22:33:04.392378: step 14922, loss 0.00217367, acc 1
2016-09-05T22:33:05.172549: step 14923, loss 0.0295584, acc 0.98
2016-09-05T22:33:05.957330: step 14924, loss 0.0237741, acc 0.98
2016-09-05T22:33:06.778254: step 14925, loss 0.0242111, acc 0.98
2016-09-05T22:33:07.556765: step 14926, loss 0.00763624, acc 1
2016-09-05T22:33:08.356403: step 14927, loss 0.023029, acc 0.98
2016-09-05T22:33:09.163399: step 14928, loss 0.00199375, acc 1
2016-09-05T22:33:09.949465: step 14929, loss 0.0258733, acc 0.98
2016-09-05T22:33:10.774322: step 14930, loss 0.0169148, acc 1
2016-09-05T22:33:11.618489: step 14931, loss 0.0173155, acc 1
2016-09-05T22:33:12.442282: step 14932, loss 0.0020404, acc 1
2016-09-05T22:33:13.241620: step 14933, loss 0.00296817, acc 1
2016-09-05T22:33:14.038218: step 14934, loss 0.0209389, acc 0.98
2016-09-05T22:33:14.818235: step 14935, loss 0.0143794, acc 1
2016-09-05T22:33:15.613429: step 14936, loss 0.00375399, acc 1
2016-09-05T22:33:16.488786: step 14937, loss 0.0301432, acc 0.98
2016-09-05T22:33:16.886682: step 14938, loss 0.209426, acc 0.916667
2016-09-05T22:33:17.719179: step 14939, loss 0.042186, acc 0.98
2016-09-05T22:33:18.549694: step 14940, loss 0.0191158, acc 1
2016-09-05T22:33:19.396353: step 14941, loss 0.0154018, acc 1
2016-09-05T22:33:20.239986: step 14942, loss 0.00654087, acc 1
2016-09-05T22:33:21.070870: step 14943, loss 0.0146056, acc 1
2016-09-05T22:33:21.871886: step 14944, loss 0.0536012, acc 0.98
2016-09-05T22:33:22.712895: step 14945, loss 0.0107032, acc 1
2016-09-05T22:33:23.513310: step 14946, loss 0.00250597, acc 1
2016-09-05T22:33:24.323882: step 14947, loss 0.00303594, acc 1
2016-09-05T22:33:25.154105: step 14948, loss 0.0124849, acc 1
2016-09-05T22:33:25.973338: step 14949, loss 0.00494078, acc 1
2016-09-05T22:33:26.791057: step 14950, loss 0.0205598, acc 0.98
2016-09-05T22:33:27.617181: step 14951, loss 0.0184108, acc 0.98
2016-09-05T22:33:28.447788: step 14952, loss 0.0118289, acc 1
2016-09-05T22:33:29.241396: step 14953, loss 0.00259345, acc 1
2016-09-05T22:33:30.091900: step 14954, loss 0.0027338, acc 1
2016-09-05T22:33:30.904840: step 14955, loss 0.00924512, acc 1
2016-09-05T22:33:31.686797: step 14956, loss 0.0325436, acc 0.98
2016-09-05T22:33:32.506421: step 14957, loss 0.0466239, acc 0.98
2016-09-05T22:33:33.263201: step 14958, loss 0.0161025, acc 1
2016-09-05T22:33:34.065541: step 14959, loss 0.0183814, acc 0.98
2016-09-05T22:33:34.904350: step 14960, loss 0.0121688, acc 1
2016-09-05T22:33:35.696014: step 14961, loss 0.00297884, acc 1
2016-09-05T22:33:36.491673: step 14962, loss 0.0292069, acc 0.98
2016-09-05T22:33:37.353679: step 14963, loss 0.00648973, acc 1
2016-09-05T22:33:38.211870: step 14964, loss 0.00298725, acc 1
2016-09-05T22:33:38.964197: step 14965, loss 0.0029905, acc 1
2016-09-05T22:33:39.773087: step 14966, loss 0.0030862, acc 1
2016-09-05T22:33:40.648481: step 14967, loss 0.0152143, acc 1
2016-09-05T22:33:41.419914: step 14968, loss 0.012404, acc 1
2016-09-05T22:33:42.203209: step 14969, loss 0.0195669, acc 0.98
2016-09-05T22:33:43.052779: step 14970, loss 0.0194444, acc 1
2016-09-05T22:33:43.904389: step 14971, loss 0.0293243, acc 1
2016-09-05T22:33:44.694743: step 14972, loss 0.0450252, acc 0.98
2016-09-05T22:33:45.532182: step 14973, loss 0.0247265, acc 0.98
2016-09-05T22:33:46.360334: step 14974, loss 0.0173737, acc 0.98
2016-09-05T22:33:47.189453: step 14975, loss 0.0415159, acc 0.98
2016-09-05T22:33:48.015192: step 14976, loss 0.00300856, acc 1
2016-09-05T22:33:48.836546: step 14977, loss 0.00315985, acc 1
2016-09-05T22:33:49.666318: step 14978, loss 0.0199241, acc 0.98
2016-09-05T22:33:50.522398: step 14979, loss 0.0264222, acc 0.98
2016-09-05T22:33:51.337710: step 14980, loss 0.00426989, acc 1
2016-09-05T22:33:52.185022: step 14981, loss 0.00423756, acc 1
2016-09-05T22:33:53.013277: step 14982, loss 0.0110959, acc 1
2016-09-05T22:33:53.838890: step 14983, loss 0.00406917, acc 1
2016-09-05T22:33:54.669648: step 14984, loss 0.156557, acc 0.98
2016-09-05T22:33:55.600038: step 14985, loss 0.0475831, acc 0.98
2016-09-05T22:33:56.446160: step 14986, loss 0.00391758, acc 1
2016-09-05T22:33:57.250150: step 14987, loss 0.0030313, acc 1
2016-09-05T22:33:58.054224: step 14988, loss 0.0366207, acc 0.96
2016-09-05T22:33:58.889098: step 14989, loss 0.00431895, acc 1
2016-09-05T22:33:59.689108: step 14990, loss 0.007709, acc 1
2016-09-05T22:34:00.522211: step 14991, loss 0.00428241, acc 1
2016-09-05T22:34:01.385446: step 14992, loss 0.0125139, acc 1
2016-09-05T22:34:02.175982: step 14993, loss 0.0028801, acc 1
2016-09-05T22:34:02.975699: step 14994, loss 0.0125832, acc 1
2016-09-05T22:34:03.813399: step 14995, loss 0.0227472, acc 0.98
2016-09-05T22:34:04.638578: step 14996, loss 0.0117583, acc 1
2016-09-05T22:34:05.449209: step 14997, loss 0.0180516, acc 0.98
2016-09-05T22:34:06.260814: step 14998, loss 0.00257792, acc 1
2016-09-05T22:34:07.069896: step 14999, loss 0.0147963, acc 1
2016-09-05T22:34:07.909799: step 15000, loss 0.0163904, acc 1

Evaluation:
2016-09-05T22:34:11.452193: step 15000, loss 2.48292, acc 0.717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-15000

2016-09-05T22:34:13.333489: step 15001, loss 0.0361701, acc 0.98
2016-09-05T22:34:14.150433: step 15002, loss 0.036166, acc 0.96
2016-09-05T22:34:14.987304: step 15003, loss 0.0298141, acc 0.98
2016-09-05T22:34:15.823467: step 15004, loss 0.0401423, acc 0.98
2016-09-05T22:34:16.623144: step 15005, loss 0.0404593, acc 0.98
2016-09-05T22:34:17.437207: step 15006, loss 0.00262932, acc 1
2016-09-05T22:34:18.269352: step 15007, loss 0.016633, acc 1
2016-09-05T22:34:19.054650: step 15008, loss 0.0358971, acc 0.96
2016-09-05T22:34:19.848336: step 15009, loss 0.00282052, acc 1
2016-09-05T22:34:20.661889: step 15010, loss 0.00542849, acc 1
2016-09-05T22:34:21.454624: step 15011, loss 0.00298375, acc 1
2016-09-05T22:34:22.264981: step 15012, loss 0.0213082, acc 0.98
2016-09-05T22:34:23.106003: step 15013, loss 0.0187963, acc 0.98
2016-09-05T22:34:23.869524: step 15014, loss 0.0509687, acc 0.96
2016-09-05T22:34:24.686637: step 15015, loss 0.0213246, acc 1
2016-09-05T22:34:25.523455: step 15016, loss 0.00436472, acc 1
2016-09-05T22:34:26.316131: step 15017, loss 0.0177187, acc 0.98
2016-09-05T22:34:27.103309: step 15018, loss 0.0107463, acc 1
2016-09-05T22:34:27.900695: step 15019, loss 0.00242076, acc 1
2016-09-05T22:34:28.691650: step 15020, loss 0.00883554, acc 1
2016-09-05T22:34:29.506841: step 15021, loss 0.00246555, acc 1
2016-09-05T22:34:30.352253: step 15022, loss 0.00552513, acc 1
2016-09-05T22:34:31.148882: step 15023, loss 0.0256285, acc 0.98
2016-09-05T22:34:31.950171: step 15024, loss 0.0133706, acc 1
2016-09-05T22:34:32.775425: step 15025, loss 0.00259625, acc 1
2016-09-05T22:34:33.568410: step 15026, loss 0.0516082, acc 0.98
2016-09-05T22:34:34.355292: step 15027, loss 0.0158868, acc 1
2016-09-05T22:34:35.184302: step 15028, loss 0.0327026, acc 0.98
2016-09-05T22:34:35.967030: step 15029, loss 0.0764121, acc 0.96
2016-09-05T22:34:36.757700: step 15030, loss 0.00777496, acc 1
2016-09-05T22:34:37.575508: step 15031, loss 0.00710903, acc 1
2016-09-05T22:34:38.374506: step 15032, loss 0.0169819, acc 1
2016-09-05T22:34:39.198349: step 15033, loss 0.0301675, acc 1
2016-09-05T22:34:40.006076: step 15034, loss 0.00260606, acc 1
2016-09-05T22:34:40.782047: step 15035, loss 0.0284921, acc 0.98
2016-09-05T22:34:41.593534: step 15036, loss 0.0207739, acc 1
2016-09-05T22:34:42.405523: step 15037, loss 0.00195086, acc 1
2016-09-05T22:34:43.183554: step 15038, loss 0.016411, acc 1
2016-09-05T22:34:43.983928: step 15039, loss 0.012559, acc 1
2016-09-05T22:34:44.766528: step 15040, loss 0.00293989, acc 1
2016-09-05T22:34:45.601081: step 15041, loss 0.0312967, acc 0.98
2016-09-05T22:34:46.416287: step 15042, loss 0.0159515, acc 1
2016-09-05T22:34:47.254099: step 15043, loss 0.00200133, acc 1
2016-09-05T22:34:48.032650: step 15044, loss 0.0100847, acc 1
2016-09-05T22:34:48.828393: step 15045, loss 0.00619013, acc 1
2016-09-05T22:34:49.641213: step 15046, loss 0.00437498, acc 1
2016-09-05T22:34:50.423525: step 15047, loss 0.00972477, acc 1
2016-09-05T22:34:51.253416: step 15048, loss 0.0285615, acc 0.98
2016-09-05T22:34:52.093187: step 15049, loss 0.00220798, acc 1
2016-09-05T22:34:52.863499: step 15050, loss 0.00198306, acc 1
2016-09-05T22:34:53.658837: step 15051, loss 0.0366198, acc 0.96
2016-09-05T22:34:54.473979: step 15052, loss 0.0049987, acc 1
2016-09-05T22:34:55.268917: step 15053, loss 0.00900694, acc 1
2016-09-05T22:34:56.062509: step 15054, loss 0.0263954, acc 1
2016-09-05T22:34:56.869512: step 15055, loss 0.0242102, acc 0.98
2016-09-05T22:34:57.661926: step 15056, loss 0.0253425, acc 0.98
2016-09-05T22:34:58.469964: step 15057, loss 0.00268517, acc 1
2016-09-05T22:34:59.280411: step 15058, loss 0.0342199, acc 0.96
2016-09-05T22:35:00.081836: step 15059, loss 0.00350396, acc 1
2016-09-05T22:35:00.916774: step 15060, loss 0.0116111, acc 1
2016-09-05T22:35:01.715551: step 15061, loss 0.0025606, acc 1
2016-09-05T22:35:02.504777: step 15062, loss 0.00435509, acc 1
2016-09-05T22:35:03.292509: step 15063, loss 0.00234748, acc 1
2016-09-05T22:35:04.090916: step 15064, loss 0.00221345, acc 1
2016-09-05T22:35:04.916032: step 15065, loss 0.0182132, acc 1
2016-09-05T22:35:05.704274: step 15066, loss 0.00977871, acc 1
2016-09-05T22:35:06.492589: step 15067, loss 0.00206203, acc 1
2016-09-05T22:35:07.290574: step 15068, loss 0.0332884, acc 0.98
2016-09-05T22:35:08.113049: step 15069, loss 0.0584086, acc 0.96
2016-09-05T22:35:08.935379: step 15070, loss 0.00748722, acc 1
2016-09-05T22:35:09.726651: step 15071, loss 0.0218366, acc 1
2016-09-05T22:35:10.540886: step 15072, loss 0.010457, acc 1
2016-09-05T22:35:11.355099: step 15073, loss 0.00950158, acc 1
2016-09-05T22:35:12.161466: step 15074, loss 0.00699592, acc 1
2016-09-05T22:35:12.961560: step 15075, loss 0.0657234, acc 0.98
2016-09-05T22:35:13.780560: step 15076, loss 0.00208935, acc 1
2016-09-05T22:35:14.567020: step 15077, loss 0.0677228, acc 0.98
2016-09-05T22:35:15.371486: step 15078, loss 0.00790684, acc 1
2016-09-05T22:35:16.188714: step 15079, loss 0.00859352, acc 1
2016-09-05T22:35:16.986894: step 15080, loss 0.0165081, acc 1
2016-09-05T22:35:17.784659: step 15081, loss 0.00617085, acc 1
2016-09-05T22:35:18.594949: step 15082, loss 0.0395196, acc 0.96
2016-09-05T22:35:19.419967: step 15083, loss 0.0039945, acc 1
2016-09-05T22:35:20.217463: step 15084, loss 0.00403232, acc 1
2016-09-05T22:35:21.099818: step 15085, loss 0.0163344, acc 1
2016-09-05T22:35:21.887486: step 15086, loss 0.0139193, acc 1
2016-09-05T22:35:22.684003: step 15087, loss 0.00372675, acc 1
2016-09-05T22:35:23.521114: step 15088, loss 0.0159514, acc 1
2016-09-05T22:35:24.385477: step 15089, loss 0.0514376, acc 0.96
2016-09-05T22:35:25.202431: step 15090, loss 0.03723, acc 0.98
2016-09-05T22:35:26.009018: step 15091, loss 0.0406811, acc 0.98
2016-09-05T22:35:26.840677: step 15092, loss 0.0419207, acc 0.98
2016-09-05T22:35:27.661900: step 15093, loss 0.00410627, acc 1
2016-09-05T22:35:28.507444: step 15094, loss 0.00419598, acc 1
2016-09-05T22:35:29.325753: step 15095, loss 0.00408191, acc 1
2016-09-05T22:35:30.158986: step 15096, loss 0.0247817, acc 0.98
2016-09-05T22:35:30.997179: step 15097, loss 0.0041267, acc 1
2016-09-05T22:35:31.808456: step 15098, loss 0.0223138, acc 0.98
2016-09-05T22:35:32.627829: step 15099, loss 0.00937828, acc 1
2016-09-05T22:35:33.518465: step 15100, loss 0.0110104, acc 1

Evaluation:
2016-09-05T22:35:37.004951: step 15100, loss 3.21828, acc 0.718

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-15100

2016-09-05T22:35:38.951194: step 15101, loss 0.0090284, acc 1
2016-09-05T22:35:39.854141: step 15102, loss 0.00413719, acc 1
2016-09-05T22:35:40.687383: step 15103, loss 0.00424173, acc 1
2016-09-05T22:35:41.488880: step 15104, loss 0.0370488, acc 0.98
2016-09-05T22:35:42.305990: step 15105, loss 0.0174042, acc 1
2016-09-05T22:35:43.169660: step 15106, loss 0.0191727, acc 0.98
2016-09-05T22:35:44.034303: step 15107, loss 0.00415946, acc 1
2016-09-05T22:35:44.825825: step 15108, loss 0.00598315, acc 1
2016-09-05T22:35:45.664424: step 15109, loss 0.00405375, acc 1
2016-09-05T22:35:46.497560: step 15110, loss 0.00458367, acc 1
2016-09-05T22:35:47.317575: step 15111, loss 0.0105141, acc 1
2016-09-05T22:35:48.129782: step 15112, loss 0.0440527, acc 0.98
2016-09-05T22:35:48.953797: step 15113, loss 0.00994977, acc 1
2016-09-05T22:35:49.775331: step 15114, loss 0.0982991, acc 0.96
2016-09-05T22:35:50.595725: step 15115, loss 0.00373681, acc 1
2016-09-05T22:35:51.438888: step 15116, loss 0.00368319, acc 1
2016-09-05T22:35:52.245767: step 15117, loss 0.00480648, acc 1
2016-09-05T22:35:53.086479: step 15118, loss 0.00459451, acc 1
2016-09-05T22:35:53.958905: step 15119, loss 0.00487892, acc 1
2016-09-05T22:35:54.787923: step 15120, loss 0.00548221, acc 1
2016-09-05T22:35:55.598416: step 15121, loss 0.003655, acc 1
2016-09-05T22:35:56.470429: step 15122, loss 0.0189319, acc 0.98
2016-09-05T22:35:57.312519: step 15123, loss 0.013408, acc 1
2016-09-05T22:35:58.118092: step 15124, loss 0.00384002, acc 1
2016-09-05T22:35:58.953459: step 15125, loss 0.0215523, acc 1
2016-09-05T22:35:59.796177: step 15126, loss 0.00327372, acc 1
2016-09-05T22:36:00.643601: step 15127, loss 0.0140191, acc 1
2016-09-05T22:36:01.458859: step 15128, loss 0.0161309, acc 1
2016-09-05T22:36:02.303063: step 15129, loss 0.0644766, acc 0.96
2016-09-05T22:36:03.117423: step 15130, loss 0.0256423, acc 0.98
2016-09-05T22:36:03.928840: step 15131, loss 0.00311501, acc 1
2016-09-05T22:36:04.373896: step 15132, loss 0.0030817, acc 1
2016-09-05T22:36:05.200035: step 15133, loss 0.0302595, acc 0.98
2016-09-05T22:36:06.041011: step 15134, loss 0.0208837, acc 1
2016-09-05T22:36:06.863652: step 15135, loss 0.030628, acc 0.98
2016-09-05T22:36:07.687836: step 15136, loss 0.0310523, acc 0.98
2016-09-05T22:36:08.501211: step 15137, loss 0.0180129, acc 0.98
2016-09-05T22:36:09.316470: step 15138, loss 0.0154029, acc 1
2016-09-05T22:36:10.157743: step 15139, loss 0.0113145, acc 1
2016-09-05T22:36:10.993987: step 15140, loss 0.00393254, acc 1
2016-09-05T22:36:11.785373: step 15141, loss 0.0477621, acc 0.98
2016-09-05T22:36:12.585087: step 15142, loss 0.00280882, acc 1
2016-09-05T22:36:13.415985: step 15143, loss 0.0106196, acc 1
2016-09-05T22:36:14.250177: step 15144, loss 0.00499269, acc 1
2016-09-05T22:36:15.053084: step 15145, loss 0.0258816, acc 0.98
2016-09-05T22:36:15.865144: step 15146, loss 0.00585004, acc 1
2016-09-05T22:36:16.705059: step 15147, loss 0.00265324, acc 1
2016-09-05T22:36:17.493353: step 15148, loss 0.00269362, acc 1
2016-09-05T22:36:18.282191: step 15149, loss 0.00260916, acc 1
2016-09-05T22:36:19.145108: step 15150, loss 0.0165337, acc 0.98
2016-09-05T22:36:19.946011: step 15151, loss 0.0295912, acc 1
2016-09-05T22:36:20.762182: step 15152, loss 0.00273581, acc 1
2016-09-05T22:36:21.593640: step 15153, loss 0.0310475, acc 0.98
2016-09-05T22:36:22.408836: step 15154, loss 0.00272863, acc 1
2016-09-05T22:36:23.263786: step 15155, loss 0.0439933, acc 0.98
2016-09-05T22:36:24.137399: step 15156, loss 0.00239997, acc 1
2016-09-05T22:36:24.928802: step 15157, loss 0.00348408, acc 1
2016-09-05T22:36:25.738254: step 15158, loss 0.00537717, acc 1
2016-09-05T22:36:26.580302: step 15159, loss 0.0145746, acc 1
2016-09-05T22:36:27.387946: step 15160, loss 0.0131682, acc 1
2016-09-05T22:36:28.200050: step 15161, loss 0.00445711, acc 1
2016-09-05T22:36:29.025560: step 15162, loss 0.018793, acc 0.98
2016-09-05T22:36:29.823802: step 15163, loss 0.0125062, acc 1
2016-09-05T22:36:30.616934: step 15164, loss 0.085479, acc 0.94
2016-09-05T22:36:31.428219: step 15165, loss 0.0563993, acc 0.98
2016-09-05T22:36:32.237130: step 15166, loss 0.00607585, acc 1
2016-09-05T22:36:33.032703: step 15167, loss 0.0119192, acc 1
2016-09-05T22:36:33.857064: step 15168, loss 0.0149057, acc 1
2016-09-05T22:36:34.653888: step 15169, loss 0.00753246, acc 1
2016-09-05T22:36:35.475252: step 15170, loss 0.0822219, acc 0.98
2016-09-05T22:36:36.281543: step 15171, loss 0.00223002, acc 1
2016-09-05T22:36:37.083720: step 15172, loss 0.00253577, acc 1
2016-09-05T22:36:37.900263: step 15173, loss 0.0170687, acc 1
2016-09-05T22:36:38.714313: step 15174, loss 0.00475496, acc 1
2016-09-05T22:36:39.556786: step 15175, loss 0.0296642, acc 0.98
2016-09-05T22:36:40.361888: step 15176, loss 0.00477106, acc 1
2016-09-05T22:36:41.183478: step 15177, loss 0.00813237, acc 1
2016-09-05T22:36:42.011558: step 15178, loss 0.0252433, acc 1
2016-09-05T22:36:42.801369: step 15179, loss 0.00254657, acc 1
2016-09-05T22:36:43.604365: step 15180, loss 0.0101766, acc 1
2016-09-05T22:36:44.397552: step 15181, loss 0.0335557, acc 0.98
2016-09-05T22:36:45.213680: step 15182, loss 0.0232317, acc 0.98
2016-09-05T22:36:46.035514: step 15183, loss 0.0156138, acc 1
2016-09-05T22:36:46.840745: step 15184, loss 0.00918083, acc 1
2016-09-05T22:36:47.637146: step 15185, loss 0.0321645, acc 0.98
2016-09-05T22:36:48.452880: step 15186, loss 0.00311703, acc 1
2016-09-05T22:36:49.284235: step 15187, loss 0.019053, acc 0.98
2016-09-05T22:36:50.072340: step 15188, loss 0.0148585, acc 1
2016-09-05T22:36:50.861733: step 15189, loss 0.00226556, acc 1
2016-09-05T22:36:51.672613: step 15190, loss 0.00189271, acc 1
2016-09-05T22:36:52.445825: step 15191, loss 0.00216291, acc 1
2016-09-05T22:36:53.254798: step 15192, loss 0.00213074, acc 1
2016-09-05T22:36:54.057956: step 15193, loss 0.0021455, acc 1
2016-09-05T22:36:54.868065: step 15194, loss 0.0179328, acc 1
2016-09-05T22:36:55.690746: step 15195, loss 0.037981, acc 0.98
2016-09-05T22:36:56.572119: step 15196, loss 0.0174097, acc 0.98
2016-09-05T22:36:57.376511: step 15197, loss 0.00280536, acc 1
2016-09-05T22:36:58.189243: step 15198, loss 0.00509207, acc 1
2016-09-05T22:36:59.023273: step 15199, loss 0.00511541, acc 1
2016-09-05T22:36:59.811028: step 15200, loss 0.0105737, acc 1

Evaluation:
2016-09-05T22:37:03.324116: step 15200, loss 2.4012, acc 0.727

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-15200

2016-09-05T22:37:05.178420: step 15201, loss 0.00989977, acc 1
2016-09-05T22:37:05.977083: step 15202, loss 0.0350617, acc 0.98
2016-09-05T22:37:06.779671: step 15203, loss 0.00249937, acc 1
2016-09-05T22:37:07.591468: step 15204, loss 0.0141409, acc 1
2016-09-05T22:37:08.359457: step 15205, loss 0.00186933, acc 1
2016-09-05T22:37:09.151614: step 15206, loss 0.00190922, acc 1
2016-09-05T22:37:09.996696: step 15207, loss 0.00938584, acc 1
2016-09-05T22:37:10.780610: step 15208, loss 0.023113, acc 1
2016-09-05T22:37:11.557232: step 15209, loss 0.00189062, acc 1
2016-09-05T22:37:12.355807: step 15210, loss 0.0130963, acc 1
2016-09-05T22:37:13.158182: step 15211, loss 0.00273833, acc 1
2016-09-05T22:37:13.964982: step 15212, loss 0.00731046, acc 1
2016-09-05T22:37:14.764973: step 15213, loss 0.00808718, acc 1
2016-09-05T22:37:15.584751: step 15214, loss 0.0125073, acc 1
2016-09-05T22:37:16.382446: step 15215, loss 0.0124351, acc 1
2016-09-05T22:37:17.197925: step 15216, loss 0.00551701, acc 1
2016-09-05T22:37:17.997378: step 15217, loss 0.00484507, acc 1
2016-09-05T22:37:18.801189: step 15218, loss 0.043453, acc 0.98
2016-09-05T22:37:19.609786: step 15219, loss 0.00670041, acc 1
2016-09-05T22:37:20.389071: step 15220, loss 0.0659728, acc 0.98
2016-09-05T22:37:21.213233: step 15221, loss 0.00486115, acc 1
2016-09-05T22:37:21.999493: step 15222, loss 0.00354466, acc 1
2016-09-05T22:37:22.788001: step 15223, loss 0.0371525, acc 0.96
2016-09-05T22:37:23.630519: step 15224, loss 0.0424531, acc 0.98
2016-09-05T22:37:24.454379: step 15225, loss 0.0425772, acc 0.98
2016-09-05T22:37:25.243847: step 15226, loss 0.0019446, acc 1
2016-09-05T22:37:26.046202: step 15227, loss 0.0261126, acc 0.98
2016-09-05T22:37:26.866749: step 15228, loss 0.00818713, acc 1
2016-09-05T22:37:27.641628: step 15229, loss 0.0169528, acc 0.98
2016-09-05T22:37:28.479741: step 15230, loss 0.0682674, acc 0.98
2016-09-05T22:37:29.297901: step 15231, loss 0.0307331, acc 0.98
2016-09-05T22:37:30.121486: step 15232, loss 0.00403768, acc 1
2016-09-05T22:37:30.890888: step 15233, loss 0.00502153, acc 1
2016-09-05T22:37:31.715135: step 15234, loss 0.0068899, acc 1
2016-09-05T22:37:32.504212: step 15235, loss 0.0227978, acc 0.98
2016-09-05T22:37:33.337018: step 15236, loss 0.0173652, acc 1
2016-09-05T22:37:34.149594: step 15237, loss 0.0022353, acc 1
2016-09-05T22:37:34.933753: step 15238, loss 0.00254093, acc 1
2016-09-05T22:37:35.725682: step 15239, loss 0.00249798, acc 1
2016-09-05T22:37:36.530923: step 15240, loss 0.0951634, acc 0.98
2016-09-05T22:37:37.344212: step 15241, loss 0.0020641, acc 1
2016-09-05T22:37:38.137807: step 15242, loss 0.00639509, acc 1
2016-09-05T22:37:38.947548: step 15243, loss 0.0333205, acc 0.98
2016-09-05T22:37:39.733653: step 15244, loss 0.00183549, acc 1
2016-09-05T22:37:40.558247: step 15245, loss 0.0167057, acc 0.98
2016-09-05T22:37:41.374405: step 15246, loss 0.0390558, acc 0.98
2016-09-05T22:37:42.160939: step 15247, loss 0.00176539, acc 1
2016-09-05T22:37:42.975640: step 15248, loss 0.0421939, acc 0.98
2016-09-05T22:37:43.795556: step 15249, loss 0.0189654, acc 1
2016-09-05T22:37:44.596102: step 15250, loss 0.0513633, acc 0.96
2016-09-05T22:37:45.397675: step 15251, loss 0.0119034, acc 1
2016-09-05T22:37:46.206170: step 15252, loss 0.0179278, acc 0.98
2016-09-05T22:37:46.992452: step 15253, loss 0.0168493, acc 0.98
2016-09-05T22:37:47.808658: step 15254, loss 0.0121012, acc 1
2016-09-05T22:37:48.615025: step 15255, loss 0.00176509, acc 1
2016-09-05T22:37:49.398587: step 15256, loss 0.00260626, acc 1
2016-09-05T22:37:50.190509: step 15257, loss 0.0209247, acc 1
2016-09-05T22:37:51.009631: step 15258, loss 0.00591514, acc 1
2016-09-05T22:37:51.826235: step 15259, loss 0.0431545, acc 0.96
2016-09-05T22:37:52.631713: step 15260, loss 0.0102494, acc 1
2016-09-05T22:37:53.441145: step 15261, loss 0.0132058, acc 1
2016-09-05T22:37:54.220397: step 15262, loss 0.0177829, acc 0.98
2016-09-05T22:37:55.053689: step 15263, loss 0.0506183, acc 0.98
2016-09-05T22:37:55.860665: step 15264, loss 0.104996, acc 0.94
2016-09-05T22:37:56.639107: step 15265, loss 0.0341472, acc 0.98
2016-09-05T22:37:57.451783: step 15266, loss 0.0172963, acc 1
2016-09-05T22:37:58.279472: step 15267, loss 0.0170621, acc 1
2016-09-05T22:37:59.051671: step 15268, loss 0.0211334, acc 1
2016-09-05T22:37:59.862372: step 15269, loss 0.0018796, acc 1
2016-09-05T22:38:00.665164: step 15270, loss 0.00210335, acc 1
2016-09-05T22:38:01.468815: step 15271, loss 0.0057861, acc 1
2016-09-05T22:38:02.289635: step 15272, loss 0.0140342, acc 1
2016-09-05T22:38:03.090793: step 15273, loss 0.00292023, acc 1
2016-09-05T22:38:03.913457: step 15274, loss 0.0103132, acc 1
2016-09-05T22:38:04.729848: step 15275, loss 0.0187458, acc 0.98
2016-09-05T22:38:05.544369: step 15276, loss 0.00478979, acc 1
2016-09-05T22:38:06.315917: step 15277, loss 0.0329511, acc 0.98
2016-09-05T22:38:07.133501: step 15278, loss 0.00335124, acc 1
2016-09-05T22:38:07.980811: step 15279, loss 0.0376096, acc 0.98
2016-09-05T22:38:08.786485: step 15280, loss 0.0404924, acc 0.98
2016-09-05T22:38:09.588345: step 15281, loss 0.0194983, acc 0.98
2016-09-05T22:38:10.432342: step 15282, loss 0.00265468, acc 1
2016-09-05T22:38:11.177258: step 15283, loss 0.0274269, acc 1
2016-09-05T22:38:11.961827: step 15284, loss 0.0529734, acc 0.98
2016-09-05T22:38:12.776336: step 15285, loss 0.088526, acc 0.96
2016-09-05T22:38:13.581405: step 15286, loss 0.0453621, acc 0.98
2016-09-05T22:38:14.429093: step 15287, loss 0.0264313, acc 0.98
2016-09-05T22:38:15.277732: step 15288, loss 0.00241411, acc 1
2016-09-05T22:38:16.074480: step 15289, loss 0.00229156, acc 1
2016-09-05T22:38:16.864602: step 15290, loss 0.00227386, acc 1
2016-09-05T22:38:17.670387: step 15291, loss 0.00271195, acc 1
2016-09-05T22:38:18.454839: step 15292, loss 0.0137884, acc 1
2016-09-05T22:38:19.251278: step 15293, loss 0.00342014, acc 1
2016-09-05T22:38:20.066287: step 15294, loss 0.00293971, acc 1
2016-09-05T22:38:20.860948: step 15295, loss 0.00230401, acc 1
2016-09-05T22:38:21.697452: step 15296, loss 0.0712812, acc 0.98
2016-09-05T22:38:22.518259: step 15297, loss 0.00227755, acc 1
2016-09-05T22:38:23.297398: step 15298, loss 0.0343341, acc 0.96
2016-09-05T22:38:24.085755: step 15299, loss 0.00256998, acc 1
2016-09-05T22:38:24.901257: step 15300, loss 0.0521742, acc 0.98

Evaluation:
2016-09-05T22:38:28.407482: step 15300, loss 2.63428, acc 0.713

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-15300

2016-09-05T22:38:30.245509: step 15301, loss 0.00221265, acc 1
2016-09-05T22:38:31.075377: step 15302, loss 0.0344312, acc 0.98
2016-09-05T22:38:31.902704: step 15303, loss 0.00366907, acc 1
2016-09-05T22:38:32.715768: step 15304, loss 0.0335792, acc 0.96
2016-09-05T22:38:33.537078: step 15305, loss 0.00434926, acc 1
2016-09-05T22:38:34.360827: step 15306, loss 0.0147757, acc 1
2016-09-05T22:38:35.147474: step 15307, loss 0.00793431, acc 1
2016-09-05T22:38:35.983153: step 15308, loss 0.0389137, acc 0.98
2016-09-05T22:38:36.806177: step 15309, loss 0.00933805, acc 1
2016-09-05T22:38:37.623480: step 15310, loss 0.0123507, acc 1
2016-09-05T22:38:38.473215: step 15311, loss 0.0169889, acc 0.98
2016-09-05T22:38:39.309592: step 15312, loss 0.0161261, acc 0.98
2016-09-05T22:38:40.117204: step 15313, loss 0.0164338, acc 1
2016-09-05T22:38:40.925314: step 15314, loss 0.00200877, acc 1
2016-09-05T22:38:41.724326: step 15315, loss 0.0327974, acc 0.98
2016-09-05T22:38:42.527422: step 15316, loss 0.0142114, acc 1
2016-09-05T22:38:43.347500: step 15317, loss 0.0403669, acc 0.98
2016-09-05T22:38:44.161247: step 15318, loss 0.00630045, acc 1
2016-09-05T22:38:44.971071: step 15319, loss 0.00949018, acc 1
2016-09-05T22:38:45.831989: step 15320, loss 0.00183632, acc 1
2016-09-05T22:38:46.652431: step 15321, loss 0.0164118, acc 0.98
2016-09-05T22:38:47.469374: step 15322, loss 0.0227792, acc 0.98
2016-09-05T22:38:48.310036: step 15323, loss 0.0174374, acc 0.98
2016-09-05T22:38:49.139853: step 15324, loss 0.0317574, acc 0.98
2016-09-05T22:38:49.968958: step 15325, loss 0.00210807, acc 1
2016-09-05T22:38:50.395118: step 15326, loss 0.00255057, acc 1
2016-09-05T22:38:51.157967: step 15327, loss 0.00191117, acc 1
2016-09-05T22:38:51.970378: step 15328, loss 0.00641476, acc 1
2016-09-05T22:38:52.819348: step 15329, loss 0.017159, acc 1
2016-09-05T22:38:53.623143: step 15330, loss 0.00872088, acc 1
2016-09-05T22:38:54.440516: step 15331, loss 0.00791448, acc 1
2016-09-05T22:38:55.290159: step 15332, loss 0.0833713, acc 0.96
2016-09-05T22:38:56.119770: step 15333, loss 0.00424257, acc 1
2016-09-05T22:38:56.943100: step 15334, loss 0.0187741, acc 0.98
2016-09-05T22:38:57.784013: step 15335, loss 0.00319138, acc 1
2016-09-05T22:38:58.587902: step 15336, loss 0.0140744, acc 1
2016-09-05T22:38:59.393989: step 15337, loss 0.0212204, acc 0.98
2016-09-05T22:39:00.184749: step 15338, loss 0.0143825, acc 1
2016-09-05T22:39:01.047986: step 15339, loss 0.0289181, acc 0.98
2016-09-05T22:39:01.875312: step 15340, loss 0.0249342, acc 1
2016-09-05T22:39:02.729481: step 15341, loss 0.00206427, acc 1
2016-09-05T22:39:03.538823: step 15342, loss 0.00196523, acc 1
2016-09-05T22:39:04.337565: step 15343, loss 0.0111059, acc 1
2016-09-05T22:39:05.154492: step 15344, loss 0.0378345, acc 0.98
2016-09-05T22:39:05.956986: step 15345, loss 0.0124486, acc 1
2016-09-05T22:39:06.749460: step 15346, loss 0.0144767, acc 1
2016-09-05T22:39:07.572784: step 15347, loss 0.0119034, acc 1
2016-09-05T22:39:08.394532: step 15348, loss 0.0549951, acc 0.94
2016-09-05T22:39:09.194943: step 15349, loss 0.00195067, acc 1
2016-09-05T22:39:10.016932: step 15350, loss 0.00204583, acc 1
2016-09-05T22:39:10.854862: step 15351, loss 0.00264918, acc 1
2016-09-05T22:39:11.668286: step 15352, loss 0.0022341, acc 1
2016-09-05T22:39:12.511576: step 15353, loss 0.00200863, acc 1
2016-09-05T22:39:13.310874: step 15354, loss 0.0271254, acc 0.98
2016-09-05T22:39:14.105825: step 15355, loss 0.00914186, acc 1
2016-09-05T22:39:14.925908: step 15356, loss 0.00511094, acc 1
2016-09-05T22:39:15.728075: step 15357, loss 0.0263286, acc 1
2016-09-05T22:39:16.559727: step 15358, loss 0.0451561, acc 0.98
2016-09-05T22:39:17.352346: step 15359, loss 0.0372961, acc 0.98
2016-09-05T22:39:18.179472: step 15360, loss 0.00183615, acc 1
2016-09-05T22:39:18.957423: step 15361, loss 0.0180807, acc 1
2016-09-05T22:39:19.784469: step 15362, loss 0.00261471, acc 1
2016-09-05T22:39:20.616610: step 15363, loss 0.00572827, acc 1
2016-09-05T22:39:21.404702: step 15364, loss 0.0768028, acc 0.96
2016-09-05T22:39:22.204519: step 15365, loss 0.00192229, acc 1
2016-09-05T22:39:23.001444: step 15366, loss 0.00163288, acc 1
2016-09-05T22:39:23.786495: step 15367, loss 0.0271788, acc 1
2016-09-05T22:39:24.586034: step 15368, loss 0.00749762, acc 1
2016-09-05T22:39:25.369397: step 15369, loss 0.00600666, acc 1
2016-09-05T22:39:26.167886: step 15370, loss 0.00454357, acc 1
2016-09-05T22:39:27.005845: step 15371, loss 0.00233073, acc 1
2016-09-05T22:39:27.778577: step 15372, loss 0.0121874, acc 1
2016-09-05T22:39:28.606945: step 15373, loss 0.0127888, acc 1
2016-09-05T22:39:29.436405: step 15374, loss 0.00850012, acc 1
2016-09-05T22:39:30.238618: step 15375, loss 0.00147964, acc 1
2016-09-05T22:39:31.037624: step 15376, loss 0.0349375, acc 0.98
2016-09-05T22:39:31.863347: step 15377, loss 0.00394094, acc 1
2016-09-05T22:39:32.696000: step 15378, loss 0.0014942, acc 1
2016-09-05T22:39:33.491917: step 15379, loss 0.00162437, acc 1
2016-09-05T22:39:34.282096: step 15380, loss 0.00811004, acc 1
2016-09-05T22:39:35.112799: step 15381, loss 0.00211848, acc 1
2016-09-05T22:39:35.884097: step 15382, loss 0.0155228, acc 1
2016-09-05T22:39:36.695506: step 15383, loss 0.00178076, acc 1
2016-09-05T22:39:37.498782: step 15384, loss 0.0015021, acc 1
2016-09-05T22:39:38.296832: step 15385, loss 0.00971657, acc 1
2016-09-05T22:39:39.096176: step 15386, loss 0.00149586, acc 1
2016-09-05T22:39:39.902158: step 15387, loss 0.00523686, acc 1
2016-09-05T22:39:40.711590: step 15388, loss 0.0169844, acc 0.98
2016-09-05T22:39:41.540046: step 15389, loss 0.0165678, acc 0.98
2016-09-05T22:39:42.360691: step 15390, loss 0.00294553, acc 1
2016-09-05T22:39:43.136633: step 15391, loss 0.011556, acc 1
2016-09-05T22:39:43.943790: step 15392, loss 0.0415729, acc 0.98
2016-09-05T22:39:44.764161: step 15393, loss 0.0547196, acc 0.98
2016-09-05T22:39:45.535095: step 15394, loss 0.00666987, acc 1
2016-09-05T22:39:46.372764: step 15395, loss 0.00152395, acc 1
2016-09-05T22:39:47.232384: step 15396, loss 0.00647317, acc 1
2016-09-05T22:39:48.023237: step 15397, loss 0.00149742, acc 1
2016-09-05T22:39:48.819598: step 15398, loss 0.019148, acc 1
2016-09-05T22:39:49.646594: step 15399, loss 0.00152794, acc 1
2016-09-05T22:39:50.457085: step 15400, loss 0.123252, acc 0.98

Evaluation:
2016-09-05T22:39:53.970149: step 15400, loss 2.65394, acc 0.719

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-15400

2016-09-05T22:39:55.957481: step 15401, loss 0.00393945, acc 1
2016-09-05T22:39:56.775563: step 15402, loss 0.0164862, acc 0.98
2016-09-05T22:39:57.573486: step 15403, loss 0.0250246, acc 1
2016-09-05T22:39:58.397507: step 15404, loss 0.00418178, acc 1
2016-09-05T22:39:59.218716: step 15405, loss 0.00912503, acc 1
2016-09-05T22:40:00.025162: step 15406, loss 0.0997909, acc 0.98
2016-09-05T22:40:00.862961: step 15407, loss 0.00490243, acc 1
2016-09-05T22:40:01.677416: step 15408, loss 0.00255388, acc 1
2016-09-05T22:40:02.511233: step 15409, loss 0.0444545, acc 0.98
2016-09-05T22:40:03.341718: step 15410, loss 0.00359993, acc 1
2016-09-05T22:40:04.161415: step 15411, loss 0.0145482, acc 1
2016-09-05T22:40:04.952796: step 15412, loss 0.00576154, acc 1
2016-09-05T22:40:05.758330: step 15413, loss 0.00928342, acc 1
2016-09-05T22:40:06.583789: step 15414, loss 0.0421065, acc 0.98
2016-09-05T22:40:07.391620: step 15415, loss 0.00182281, acc 1
2016-09-05T22:40:08.199286: step 15416, loss 0.0172571, acc 1
2016-09-05T22:40:09.046938: step 15417, loss 0.0226499, acc 0.98
2016-09-05T22:40:09.830985: step 15418, loss 0.0224243, acc 0.98
2016-09-05T22:40:10.678428: step 15419, loss 0.00216903, acc 1
2016-09-05T22:40:11.507454: step 15420, loss 0.00463045, acc 1
2016-09-05T22:40:12.304477: step 15421, loss 0.0251991, acc 0.98
2016-09-05T22:40:13.103685: step 15422, loss 0.00798243, acc 1
2016-09-05T22:40:13.927039: step 15423, loss 0.01816, acc 1
2016-09-05T22:40:14.736417: step 15424, loss 0.0547684, acc 0.98
2016-09-05T22:40:15.540283: step 15425, loss 0.0039402, acc 1
2016-09-05T22:40:16.341307: step 15426, loss 0.014349, acc 1
2016-09-05T22:40:17.125023: step 15427, loss 0.0114238, acc 1
2016-09-05T22:40:17.924074: step 15428, loss 0.00468248, acc 1
2016-09-05T22:40:18.720014: step 15429, loss 0.0394918, acc 0.96
2016-09-05T22:40:19.480962: step 15430, loss 0.0216525, acc 0.98
2016-09-05T22:40:20.308315: step 15431, loss 0.00589451, acc 1
2016-09-05T22:40:21.147751: step 15432, loss 0.00802469, acc 1
2016-09-05T22:40:21.897891: step 15433, loss 0.00969757, acc 1
2016-09-05T22:40:22.690361: step 15434, loss 0.0145901, acc 1
2016-09-05T22:40:23.501816: step 15435, loss 0.0124383, acc 1
2016-09-05T22:40:24.282826: step 15436, loss 0.00280562, acc 1
2016-09-05T22:40:25.098675: step 15437, loss 0.0364867, acc 0.98
2016-09-05T22:40:25.909420: step 15438, loss 0.00247669, acc 1
2016-09-05T22:40:26.701227: step 15439, loss 0.0252092, acc 0.98
2016-09-05T22:40:27.543308: step 15440, loss 0.00392328, acc 1
2016-09-05T22:40:28.344436: step 15441, loss 0.00251216, acc 1
2016-09-05T22:40:29.136357: step 15442, loss 0.0290057, acc 0.98
2016-09-05T22:40:29.950104: step 15443, loss 0.0103296, acc 1
2016-09-05T22:40:30.749465: step 15444, loss 0.0202382, acc 0.98
2016-09-05T22:40:31.530793: step 15445, loss 0.00248288, acc 1
2016-09-05T22:40:32.349678: step 15446, loss 0.0272613, acc 0.98
2016-09-05T22:40:33.163143: step 15447, loss 0.0109691, acc 1
2016-09-05T22:40:33.949195: step 15448, loss 0.0025234, acc 1
2016-09-05T22:40:34.774278: step 15449, loss 0.016657, acc 0.98
2016-09-05T22:40:35.665550: step 15450, loss 0.00251195, acc 1
2016-09-05T22:40:36.456817: step 15451, loss 0.0217338, acc 0.98
2016-09-05T22:40:37.256299: step 15452, loss 0.0396634, acc 0.98
2016-09-05T22:40:38.038440: step 15453, loss 0.00248642, acc 1
2016-09-05T22:40:38.813376: step 15454, loss 0.00280201, acc 1
2016-09-05T22:40:39.643603: step 15455, loss 0.00243269, acc 1
2016-09-05T22:40:40.469363: step 15456, loss 0.00248091, acc 1
2016-09-05T22:40:41.269628: step 15457, loss 0.0331321, acc 0.98
2016-09-05T22:40:42.113040: step 15458, loss 0.00382243, acc 1
2016-09-05T22:40:42.957158: step 15459, loss 0.00236749, acc 1
2016-09-05T22:40:43.763578: step 15460, loss 0.00455108, acc 1
2016-09-05T22:40:44.583085: step 15461, loss 0.00410657, acc 1
2016-09-05T22:40:45.400188: step 15462, loss 0.0108577, acc 1
2016-09-05T22:40:46.237158: step 15463, loss 0.0241232, acc 1
2016-09-05T22:40:47.042955: step 15464, loss 0.00230596, acc 1
2016-09-05T22:40:47.857018: step 15465, loss 0.0044988, acc 1
2016-09-05T22:40:48.654728: step 15466, loss 0.0201662, acc 1
2016-09-05T22:40:49.445438: step 15467, loss 0.00539571, acc 1
2016-09-05T22:40:50.233859: step 15468, loss 0.00828932, acc 1
2016-09-05T22:40:51.054007: step 15469, loss 0.022123, acc 0.98
2016-09-05T22:40:51.859888: step 15470, loss 0.0022884, acc 1
2016-09-05T22:40:52.676178: step 15471, loss 0.0121002, acc 1
2016-09-05T22:40:53.482133: step 15472, loss 0.0034014, acc 1
2016-09-05T22:40:54.266840: step 15473, loss 0.0344938, acc 0.96
2016-09-05T22:40:55.116268: step 15474, loss 0.0174595, acc 1
2016-09-05T22:40:55.912470: step 15475, loss 0.0108288, acc 1
2016-09-05T22:40:56.724359: step 15476, loss 0.00285487, acc 1
2016-09-05T22:40:57.536095: step 15477, loss 0.0102472, acc 1
2016-09-05T22:40:58.348068: step 15478, loss 0.0785258, acc 0.98
2016-09-05T22:40:59.152978: step 15479, loss 0.0124165, acc 1
2016-09-05T22:40:59.970552: step 15480, loss 0.0347827, acc 0.98
2016-09-05T22:41:00.845384: step 15481, loss 0.0425874, acc 0.98
2016-09-05T22:41:01.656978: step 15482, loss 0.00212848, acc 1
2016-09-05T22:41:02.476928: step 15483, loss 0.00211332, acc 1
2016-09-05T22:41:03.272297: step 15484, loss 0.0120874, acc 1
2016-09-05T22:41:04.101407: step 15485, loss 0.0024345, acc 1
2016-09-05T22:41:04.940221: step 15486, loss 0.0211365, acc 0.98
2016-09-05T22:41:05.769151: step 15487, loss 0.00282798, acc 1
2016-09-05T22:41:06.599203: step 15488, loss 0.00501758, acc 1
2016-09-05T22:41:07.415738: step 15489, loss 0.00217868, acc 1
2016-09-05T22:41:08.223010: step 15490, loss 0.00201327, acc 1
2016-09-05T22:41:09.023401: step 15491, loss 0.00662523, acc 1
2016-09-05T22:41:09.857112: step 15492, loss 0.0145693, acc 1
2016-09-05T22:41:10.685770: step 15493, loss 0.00302198, acc 1
2016-09-05T22:41:11.523156: step 15494, loss 0.00242829, acc 1
2016-09-05T22:41:12.307217: step 15495, loss 0.01509, acc 1
2016-09-05T22:41:13.111248: step 15496, loss 0.0145516, acc 1
2016-09-05T22:41:13.929298: step 15497, loss 0.00221709, acc 1
2016-09-05T22:41:14.754321: step 15498, loss 0.00311501, acc 1
2016-09-05T22:41:15.557052: step 15499, loss 0.0142689, acc 1
2016-09-05T22:41:16.325332: step 15500, loss 0.00199622, acc 1

Evaluation:
2016-09-05T22:41:19.856638: step 15500, loss 2.9059, acc 0.717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-15500

2016-09-05T22:41:21.781971: step 15501, loss 0.0102789, acc 1
2016-09-05T22:41:22.593371: step 15502, loss 0.0241036, acc 0.98
2016-09-05T22:41:23.391131: step 15503, loss 0.00213856, acc 1
2016-09-05T22:41:24.210670: step 15504, loss 0.0174145, acc 0.98
2016-09-05T22:41:25.017797: step 15505, loss 0.00271328, acc 1
2016-09-05T22:41:25.838885: step 15506, loss 0.0156375, acc 1
2016-09-05T22:41:26.696749: step 15507, loss 0.032607, acc 0.96
2016-09-05T22:41:27.510344: step 15508, loss 0.00184277, acc 1
2016-09-05T22:41:28.324675: step 15509, loss 0.00391584, acc 1
2016-09-05T22:41:29.134053: step 15510, loss 0.0656786, acc 0.98
2016-09-05T22:41:29.946948: step 15511, loss 0.0160619, acc 1
2016-09-05T22:41:30.757002: step 15512, loss 0.0018007, acc 1
2016-09-05T22:41:31.569022: step 15513, loss 0.0161648, acc 1
2016-09-05T22:41:32.391849: step 15514, loss 0.00579927, acc 1
2016-09-05T22:41:33.199847: step 15515, loss 0.0135276, acc 1
2016-09-05T22:41:34.060412: step 15516, loss 0.0207162, acc 0.98
2016-09-05T22:41:34.887829: step 15517, loss 0.0133567, acc 1
2016-09-05T22:41:35.719481: step 15518, loss 0.0118509, acc 1
2016-09-05T22:41:36.565980: step 15519, loss 0.0344415, acc 0.98
2016-09-05T22:41:36.975337: step 15520, loss 0.00807477, acc 1
2016-09-05T22:41:37.790549: step 15521, loss 0.0793346, acc 0.96
2016-09-05T22:41:38.616204: step 15522, loss 0.00166549, acc 1
2016-09-05T22:41:39.399513: step 15523, loss 0.0912938, acc 0.96
2016-09-05T22:41:40.205357: step 15524, loss 0.0192732, acc 1
2016-09-05T22:41:41.064420: step 15525, loss 0.0249027, acc 0.98
2016-09-05T22:41:41.852978: step 15526, loss 0.0119355, acc 1
2016-09-05T22:41:42.697386: step 15527, loss 0.00690735, acc 1
2016-09-05T22:41:43.514350: step 15528, loss 0.0242686, acc 0.98
2016-09-05T22:41:44.324874: step 15529, loss 0.00151272, acc 1
2016-09-05T22:41:45.129608: step 15530, loss 0.0143291, acc 1
2016-09-05T22:41:45.972546: step 15531, loss 0.0299045, acc 0.98
2016-09-05T22:41:46.767784: step 15532, loss 0.00450195, acc 1
2016-09-05T22:41:47.579245: step 15533, loss 0.0521091, acc 0.98
2016-09-05T22:41:48.404764: step 15534, loss 0.0145875, acc 1
2016-09-05T22:41:49.220383: step 15535, loss 0.00294696, acc 1
2016-09-05T22:41:50.011606: step 15536, loss 0.00242827, acc 1
2016-09-05T22:41:50.840923: step 15537, loss 0.0451007, acc 0.98
2016-09-05T22:41:51.671875: step 15538, loss 0.0088368, acc 1
2016-09-05T22:41:52.475986: step 15539, loss 0.0277712, acc 1
2016-09-05T22:41:53.304516: step 15540, loss 0.00869755, acc 1
2016-09-05T22:41:54.120109: step 15541, loss 0.00758757, acc 1
2016-09-05T22:41:54.930201: step 15542, loss 0.0112837, acc 1
2016-09-05T22:41:55.768316: step 15543, loss 0.00442085, acc 1
2016-09-05T22:41:56.597380: step 15544, loss 0.00166708, acc 1
2016-09-05T22:41:57.417828: step 15545, loss 0.0354324, acc 0.98
2016-09-05T22:41:58.240475: step 15546, loss 0.00207714, acc 1
2016-09-05T22:41:59.093435: step 15547, loss 0.00151048, acc 1
2016-09-05T22:41:59.905626: step 15548, loss 0.0223299, acc 0.98
2016-09-05T22:42:00.792136: step 15549, loss 0.00213482, acc 1
2016-09-05T22:42:01.621959: step 15550, loss 0.0673716, acc 0.94
2016-09-05T22:42:02.441402: step 15551, loss 0.0192627, acc 1
2016-09-05T22:42:03.230810: step 15552, loss 0.0325792, acc 0.98
2016-09-05T22:42:04.025441: step 15553, loss 0.0026902, acc 1
2016-09-05T22:42:04.826162: step 15554, loss 0.00404629, acc 1
2016-09-05T22:42:05.629596: step 15555, loss 0.00165568, acc 1
2016-09-05T22:42:06.469403: step 15556, loss 0.0161429, acc 1
2016-09-05T22:42:07.229459: step 15557, loss 0.0134034, acc 1
2016-09-05T22:42:08.042562: step 15558, loss 0.00303178, acc 1
2016-09-05T22:42:08.858391: step 15559, loss 0.00140755, acc 1
2016-09-05T22:42:09.690130: step 15560, loss 0.0622723, acc 0.98
2016-09-05T22:42:10.485220: step 15561, loss 0.00553303, acc 1
2016-09-05T22:42:11.318947: step 15562, loss 0.00190474, acc 1
2016-09-05T22:42:12.078885: step 15563, loss 0.00177685, acc 1
2016-09-05T22:42:12.873010: step 15564, loss 0.00396288, acc 1
2016-09-05T22:42:13.702881: step 15565, loss 0.0161824, acc 1
2016-09-05T22:42:14.517950: step 15566, loss 0.0296292, acc 0.98
2016-09-05T22:42:15.336000: step 15567, loss 0.00259554, acc 1
2016-09-05T22:42:16.158355: step 15568, loss 0.0193746, acc 1
2016-09-05T22:42:16.965942: step 15569, loss 0.00470848, acc 1
2016-09-05T22:42:17.798211: step 15570, loss 0.0188102, acc 0.98
2016-09-05T22:42:18.685873: step 15571, loss 0.00349465, acc 1
2016-09-05T22:42:19.594585: step 15572, loss 0.00416345, acc 1
2016-09-05T22:42:20.572657: step 15573, loss 0.0122477, acc 1
2016-09-05T22:42:21.414072: step 15574, loss 0.00445958, acc 1
2016-09-05T22:42:22.403046: step 15575, loss 0.00480114, acc 1
2016-09-05T22:42:23.254381: step 15576, loss 0.00445223, acc 1
2016-09-05T22:42:24.200103: step 15577, loss 0.00630328, acc 1
2016-09-05T22:42:25.051978: step 15578, loss 0.00493535, acc 1
2016-09-05T22:42:25.985609: step 15579, loss 0.00490769, acc 1
2016-09-05T22:42:26.849331: step 15580, loss 0.0542514, acc 0.98
2016-09-05T22:42:27.791299: step 15581, loss 0.0187579, acc 1
2016-09-05T22:42:28.721879: step 15582, loss 0.00537043, acc 1
2016-09-05T22:42:29.563533: step 15583, loss 0.108479, acc 0.98
2016-09-05T22:42:30.519737: step 15584, loss 0.0199918, acc 1
2016-09-05T22:42:31.403536: step 15585, loss 0.0126959, acc 1
2016-09-05T22:42:32.308467: step 15586, loss 0.0304141, acc 0.98
2016-09-05T22:42:33.236868: step 15587, loss 0.0204667, acc 0.98
2016-09-05T22:42:34.134640: step 15588, loss 0.00526153, acc 1
2016-09-05T22:42:35.033261: step 15589, loss 0.00717094, acc 1
2016-09-05T22:42:35.856815: step 15590, loss 0.132173, acc 0.98
2016-09-05T22:42:36.686030: step 15591, loss 0.00720154, acc 1
2016-09-05T22:42:37.474960: step 15592, loss 0.00579923, acc 1
2016-09-05T22:42:38.302725: step 15593, loss 0.00511088, acc 1
2016-09-05T22:42:39.256169: step 15594, loss 0.00627753, acc 1
2016-09-05T22:42:40.075023: step 15595, loss 0.0194087, acc 0.98
2016-09-05T22:42:40.881572: step 15596, loss 0.0337713, acc 1
2016-09-05T22:42:41.741617: step 15597, loss 0.00498799, acc 1
2016-09-05T22:42:42.552682: step 15598, loss 0.014867, acc 1
2016-09-05T22:42:43.409703: step 15599, loss 0.00483988, acc 1
2016-09-05T22:42:44.285204: step 15600, loss 0.0166943, acc 1

Evaluation:
2016-09-05T22:42:47.835513: step 15600, loss 3.95939, acc 0.705

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-15600

2016-09-05T22:42:49.715582: step 15601, loss 0.0212618, acc 1
2016-09-05T22:42:50.509440: step 15602, loss 0.0159727, acc 1
2016-09-05T22:42:51.340156: step 15603, loss 0.133662, acc 0.94
2016-09-05T22:42:52.165446: step 15604, loss 0.00494526, acc 1
2016-09-05T22:42:53.008818: step 15605, loss 0.0045421, acc 1
2016-09-05T22:42:53.823212: step 15606, loss 0.0911519, acc 0.96
2016-09-05T22:42:54.647575: step 15607, loss 0.00508231, acc 1
2016-09-05T22:42:55.484414: step 15608, loss 0.00688173, acc 1
2016-09-05T22:42:56.343318: step 15609, loss 0.00433755, acc 1
2016-09-05T22:42:57.154236: step 15610, loss 0.0278996, acc 0.98
2016-09-05T22:42:57.965134: step 15611, loss 0.0109147, acc 1
2016-09-05T22:42:58.791417: step 15612, loss 0.0316266, acc 0.98
2016-09-05T22:42:59.623958: step 15613, loss 0.0161914, acc 1
2016-09-05T22:43:00.509143: step 15614, loss 0.0174892, acc 1
2016-09-05T22:43:01.332519: step 15615, loss 0.01267, acc 1
2016-09-05T22:43:02.141775: step 15616, loss 0.0716247, acc 0.98
2016-09-05T22:43:02.929098: step 15617, loss 0.018887, acc 0.98
2016-09-05T22:43:03.723642: step 15618, loss 0.0249236, acc 1
2016-09-05T22:43:04.517860: step 15619, loss 0.00880005, acc 1
2016-09-05T22:43:05.319765: step 15620, loss 0.028601, acc 0.98
2016-09-05T22:43:06.142419: step 15621, loss 0.0444036, acc 0.98
2016-09-05T22:43:06.986908: step 15622, loss 0.00557201, acc 1
2016-09-05T22:43:07.786620: step 15623, loss 0.0130684, acc 1
2016-09-05T22:43:08.580607: step 15624, loss 0.0932562, acc 0.96
2016-09-05T22:43:09.398230: step 15625, loss 0.00739715, acc 1
2016-09-05T22:43:10.177401: step 15626, loss 0.0135934, acc 1
2016-09-05T22:43:10.978920: step 15627, loss 0.0217518, acc 1
2016-09-05T22:43:11.783755: step 15628, loss 0.0178208, acc 0.98
2016-09-05T22:43:12.577487: step 15629, loss 0.0378813, acc 0.98
2016-09-05T22:43:13.379213: step 15630, loss 0.0212756, acc 0.98
2016-09-05T22:43:14.192535: step 15631, loss 0.0211142, acc 1
2016-09-05T22:43:15.000517: step 15632, loss 0.00282144, acc 1
2016-09-05T22:43:15.806447: step 15633, loss 0.0141705, acc 1
2016-09-05T22:43:16.655683: step 15634, loss 0.0537196, acc 0.98
2016-09-05T22:43:17.452786: step 15635, loss 0.00370093, acc 1
2016-09-05T22:43:18.255233: step 15636, loss 0.00330895, acc 1
2016-09-05T22:43:19.062299: step 15637, loss 0.0316427, acc 0.98
2016-09-05T22:43:19.862814: step 15638, loss 0.00496393, acc 1
2016-09-05T22:43:20.657250: step 15639, loss 0.00731647, acc 1
2016-09-05T22:43:21.448160: step 15640, loss 0.0428029, acc 0.96
2016-09-05T22:43:22.281383: step 15641, loss 0.0126997, acc 1
2016-09-05T22:43:23.072453: step 15642, loss 0.0214244, acc 0.98
2016-09-05T22:43:23.875466: step 15643, loss 0.00265155, acc 1
2016-09-05T22:43:24.666429: step 15644, loss 0.00278519, acc 1
2016-09-05T22:43:25.487920: step 15645, loss 0.0446674, acc 0.98
2016-09-05T22:43:26.286546: step 15646, loss 0.00451159, acc 1
2016-09-05T22:43:27.099154: step 15647, loss 0.0293602, acc 0.98
2016-09-05T22:43:27.925345: step 15648, loss 0.00258252, acc 1
2016-09-05T22:43:28.779548: step 15649, loss 0.00272838, acc 1
2016-09-05T22:43:29.597870: step 15650, loss 0.0029584, acc 1
2016-09-05T22:43:30.415358: step 15651, loss 0.0362066, acc 0.98
2016-09-05T22:43:31.233062: step 15652, loss 0.00524201, acc 1
2016-09-05T22:43:32.043301: step 15653, loss 0.0226411, acc 0.98
2016-09-05T22:43:32.871232: step 15654, loss 0.0163046, acc 1
2016-09-05T22:43:33.675087: step 15655, loss 0.038529, acc 0.98
2016-09-05T22:43:34.473581: step 15656, loss 0.0274273, acc 0.98
2016-09-05T22:43:35.271439: step 15657, loss 0.023871, acc 0.98
2016-09-05T22:43:36.092135: step 15658, loss 0.00250518, acc 1
2016-09-05T22:43:36.877831: step 15659, loss 0.00312523, acc 1
2016-09-05T22:43:37.676206: step 15660, loss 0.0361217, acc 0.96
2016-09-05T22:43:38.505941: step 15661, loss 0.00259116, acc 1
2016-09-05T22:43:39.328879: step 15662, loss 0.0237912, acc 0.98
2016-09-05T22:43:40.157076: step 15663, loss 0.0116222, acc 1
2016-09-05T22:43:40.992539: step 15664, loss 0.00578126, acc 1
2016-09-05T22:43:41.813417: step 15665, loss 0.0159895, acc 1
2016-09-05T22:43:42.627230: step 15666, loss 0.00302586, acc 1
2016-09-05T22:43:43.472375: step 15667, loss 0.0395114, acc 0.98
2016-09-05T22:43:44.282273: step 15668, loss 0.0565753, acc 0.98
2016-09-05T22:43:45.099570: step 15669, loss 0.0212087, acc 0.98
2016-09-05T22:43:45.919787: step 15670, loss 0.00897845, acc 1
2016-09-05T22:43:46.725126: step 15671, loss 0.0337245, acc 0.98
2016-09-05T22:43:47.537195: step 15672, loss 0.0407816, acc 0.96
2016-09-05T22:43:48.389797: step 15673, loss 0.0116395, acc 1
2016-09-05T22:43:49.171966: step 15674, loss 0.00996486, acc 1
2016-09-05T22:43:49.993334: step 15675, loss 0.0185736, acc 0.98
2016-09-05T22:43:50.871514: step 15676, loss 0.00298977, acc 1
2016-09-05T22:43:51.729688: step 15677, loss 0.00361068, acc 1
2016-09-05T22:43:52.499018: step 15678, loss 0.110598, acc 0.98
2016-09-05T22:43:53.293925: step 15679, loss 0.00359527, acc 1
2016-09-05T22:43:54.110726: step 15680, loss 0.021694, acc 0.98
2016-09-05T22:43:54.899984: step 15681, loss 0.00224002, acc 1
2016-09-05T22:43:55.700624: step 15682, loss 0.0178375, acc 0.98
2016-09-05T22:43:56.512670: step 15683, loss 0.0174066, acc 0.98
2016-09-05T22:43:57.286354: step 15684, loss 0.0137883, acc 1
2016-09-05T22:43:58.093910: step 15685, loss 0.0166917, acc 1
2016-09-05T22:43:58.899576: step 15686, loss 0.0408305, acc 0.98
2016-09-05T22:43:59.692421: step 15687, loss 0.0162381, acc 1
2016-09-05T22:44:00.549252: step 15688, loss 0.0251984, acc 0.98
2016-09-05T22:44:01.348510: step 15689, loss 0.00271832, acc 1
2016-09-05T22:44:02.126009: step 15690, loss 0.0363961, acc 0.98
2016-09-05T22:44:02.958045: step 15691, loss 0.00486943, acc 1
2016-09-05T22:44:03.770679: step 15692, loss 0.0105483, acc 1
2016-09-05T22:44:04.553451: step 15693, loss 0.0101121, acc 1
2016-09-05T22:44:05.368084: step 15694, loss 0.00214192, acc 1
2016-09-05T22:44:06.173979: step 15695, loss 0.00555547, acc 1
2016-09-05T22:44:06.977544: step 15696, loss 0.0124305, acc 1
2016-09-05T22:44:07.806238: step 15697, loss 0.020203, acc 0.98
2016-09-05T22:44:08.625934: step 15698, loss 0.00697091, acc 1
2016-09-05T22:44:09.426617: step 15699, loss 0.00655589, acc 1
2016-09-05T22:44:10.231630: step 15700, loss 0.0242351, acc 1

Evaluation:
2016-09-05T22:44:13.759434: step 15700, loss 2.81033, acc 0.726

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-15700

2016-09-05T22:44:15.739355: step 15701, loss 0.00604972, acc 1
2016-09-05T22:44:16.542447: step 15702, loss 0.00235997, acc 1
2016-09-05T22:44:17.375110: step 15703, loss 0.00254224, acc 1
2016-09-05T22:44:18.187766: step 15704, loss 0.0048098, acc 1
2016-09-05T22:44:19.015615: step 15705, loss 0.00248283, acc 1
2016-09-05T22:44:19.857524: step 15706, loss 0.00416689, acc 1
2016-09-05T22:44:20.689438: step 15707, loss 0.0251786, acc 0.98
2016-09-05T22:44:21.484670: step 15708, loss 0.00261933, acc 1
2016-09-05T22:44:22.344852: step 15709, loss 0.00881344, acc 1
2016-09-05T22:44:23.190693: step 15710, loss 0.0162608, acc 1
2016-09-05T22:44:24.017137: step 15711, loss 0.0164118, acc 1
2016-09-05T22:44:24.824388: step 15712, loss 0.0145277, acc 1
2016-09-05T22:44:25.689420: step 15713, loss 0.00306151, acc 1
2016-09-05T22:44:26.107529: step 15714, loss 0.00265804, acc 1
2016-09-05T22:44:26.946386: step 15715, loss 0.00507126, acc 1
2016-09-05T22:44:27.821300: step 15716, loss 0.0351887, acc 0.98
2016-09-05T22:44:28.624188: step 15717, loss 0.0106523, acc 1
2016-09-05T22:44:29.478927: step 15718, loss 0.0100567, acc 1
2016-09-05T22:44:30.361683: step 15719, loss 0.00261505, acc 1
2016-09-05T22:44:31.149928: step 15720, loss 0.00397834, acc 1
2016-09-05T22:44:31.963938: step 15721, loss 0.0401252, acc 0.98
2016-09-05T22:44:32.779163: step 15722, loss 0.0235144, acc 0.98
2016-09-05T22:44:33.583457: step 15723, loss 0.00370238, acc 1
2016-09-05T22:44:34.461886: step 15724, loss 0.0212067, acc 1
2016-09-05T22:44:35.324285: step 15725, loss 0.00280162, acc 1
2016-09-05T22:44:36.149140: step 15726, loss 0.11417, acc 0.98
2016-09-05T22:44:36.965010: step 15727, loss 0.0154523, acc 1
2016-09-05T22:44:37.795042: step 15728, loss 0.0047081, acc 1
2016-09-05T22:44:38.601633: step 15729, loss 0.00326825, acc 1
2016-09-05T22:44:39.413692: step 15730, loss 0.0164963, acc 0.98
2016-09-05T22:44:40.255405: step 15731, loss 0.0079264, acc 1
2016-09-05T22:44:41.062881: step 15732, loss 0.0165757, acc 1
2016-09-05T22:44:41.886050: step 15733, loss 0.0162739, acc 0.98
2016-09-05T22:44:42.704015: step 15734, loss 0.00700551, acc 1
2016-09-05T22:44:43.538868: step 15735, loss 0.00222635, acc 1
2016-09-05T22:44:44.355033: step 15736, loss 0.00240349, acc 1
2016-09-05T22:44:45.160408: step 15737, loss 0.00219558, acc 1
2016-09-05T22:44:45.959155: step 15738, loss 0.0161475, acc 0.98
2016-09-05T22:44:46.785729: step 15739, loss 0.0143432, acc 1
2016-09-05T22:44:47.657421: step 15740, loss 0.00215623, acc 1
2016-09-05T22:44:48.463351: step 15741, loss 0.00214674, acc 1
2016-09-05T22:44:49.296663: step 15742, loss 0.00264649, acc 1
2016-09-05T22:44:50.118133: step 15743, loss 0.0957918, acc 0.98
2016-09-05T22:44:50.959600: step 15744, loss 0.00219748, acc 1
2016-09-05T22:44:51.732415: step 15745, loss 0.0161518, acc 1
2016-09-05T22:44:52.529449: step 15746, loss 0.0216917, acc 1
2016-09-05T22:44:53.353334: step 15747, loss 0.00279316, acc 1
2016-09-05T22:44:54.153458: step 15748, loss 0.00286699, acc 1
2016-09-05T22:44:54.946552: step 15749, loss 0.00201846, acc 1
2016-09-05T22:44:55.772506: step 15750, loss 0.0155817, acc 1
2016-09-05T22:44:56.567110: step 15751, loss 0.00475828, acc 1
2016-09-05T22:44:57.354210: step 15752, loss 0.00220866, acc 1
2016-09-05T22:44:58.211163: step 15753, loss 0.0018268, acc 1
2016-09-05T22:44:58.993391: step 15754, loss 0.00181078, acc 1
2016-09-05T22:44:59.771430: step 15755, loss 0.0237041, acc 0.98
2016-09-05T22:45:00.599472: step 15756, loss 0.0101038, acc 1
2016-09-05T22:45:01.375374: step 15757, loss 0.00179661, acc 1
2016-09-05T22:45:02.175703: step 15758, loss 0.00346713, acc 1
2016-09-05T22:45:02.980042: step 15759, loss 0.00712057, acc 1
2016-09-05T22:45:03.753193: step 15760, loss 0.00911698, acc 1
2016-09-05T22:45:04.605697: step 15761, loss 0.00256802, acc 1
2016-09-05T22:45:05.428232: step 15762, loss 0.00832471, acc 1
2016-09-05T22:45:06.235451: step 15763, loss 0.00576002, acc 1
2016-09-05T22:45:07.026648: step 15764, loss 0.00794352, acc 1
2016-09-05T22:45:07.845049: step 15765, loss 0.00184671, acc 1
2016-09-05T22:45:08.646646: step 15766, loss 0.00177754, acc 1
2016-09-05T22:45:09.435509: step 15767, loss 0.00176184, acc 1
2016-09-05T22:45:10.252605: step 15768, loss 0.0105002, acc 1
2016-09-05T22:45:11.045226: step 15769, loss 0.00345114, acc 1
2016-09-05T22:45:11.840545: step 15770, loss 0.00347401, acc 1
2016-09-05T22:45:12.653119: step 15771, loss 0.0228357, acc 0.98
2016-09-05T22:45:13.421378: step 15772, loss 0.00268029, acc 1
2016-09-05T22:45:14.253419: step 15773, loss 0.00761097, acc 1
2016-09-05T22:45:15.053361: step 15774, loss 0.0153471, acc 1
2016-09-05T22:45:15.841433: step 15775, loss 0.00398424, acc 1
2016-09-05T22:45:16.657236: step 15776, loss 0.0259351, acc 0.98
2016-09-05T22:45:17.478638: step 15777, loss 0.0492349, acc 0.98
2016-09-05T22:45:18.276122: step 15778, loss 0.0370742, acc 0.98
2016-09-05T22:45:19.064392: step 15779, loss 0.0344438, acc 0.98
2016-09-05T22:45:19.885401: step 15780, loss 0.0104075, acc 1
2016-09-05T22:45:20.664117: step 15781, loss 0.00171891, acc 1
2016-09-05T22:45:21.491620: step 15782, loss 0.00207638, acc 1
2016-09-05T22:45:22.306044: step 15783, loss 0.00175786, acc 1
2016-09-05T22:45:23.087367: step 15784, loss 0.0587578, acc 0.98
2016-09-05T22:45:23.892439: step 15785, loss 0.00213971, acc 1
2016-09-05T22:45:24.696967: step 15786, loss 0.00768982, acc 1
2016-09-05T22:45:25.502292: step 15787, loss 0.00172649, acc 1
2016-09-05T22:45:26.328271: step 15788, loss 0.00564922, acc 1
2016-09-05T22:45:27.159129: step 15789, loss 0.00167042, acc 1
2016-09-05T22:45:27.944009: step 15790, loss 0.0171873, acc 0.98
2016-09-05T22:45:28.747416: step 15791, loss 0.0193632, acc 0.98
2016-09-05T22:45:29.558170: step 15792, loss 0.0165283, acc 0.98
2016-09-05T22:45:30.342478: step 15793, loss 0.0016812, acc 1
2016-09-05T22:45:31.174690: step 15794, loss 0.011626, acc 1
2016-09-05T22:45:31.996111: step 15795, loss 0.00287547, acc 1
2016-09-05T22:45:32.799435: step 15796, loss 0.00974653, acc 1
2016-09-05T22:45:33.599498: step 15797, loss 0.00170569, acc 1
2016-09-05T22:45:34.428893: step 15798, loss 0.00249208, acc 1
2016-09-05T22:45:35.223799: step 15799, loss 0.0128243, acc 1
2016-09-05T22:45:36.057412: step 15800, loss 0.062348, acc 0.94

Evaluation:
2016-09-05T22:45:39.576490: step 15800, loss 2.6839, acc 0.73

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-15800

2016-09-05T22:45:41.458884: step 15801, loss 0.0017797, acc 1
2016-09-05T22:45:42.283543: step 15802, loss 0.00170224, acc 1
2016-09-05T22:45:43.121626: step 15803, loss 0.0181755, acc 0.98
2016-09-05T22:45:43.944662: step 15804, loss 0.00811467, acc 1
2016-09-05T22:45:44.774865: step 15805, loss 0.0119701, acc 1
2016-09-05T22:45:45.597209: step 15806, loss 0.00355543, acc 1
2016-09-05T22:45:46.436695: step 15807, loss 0.00332196, acc 1
2016-09-05T22:45:47.275024: step 15808, loss 0.0120395, acc 1
2016-09-05T22:45:48.098096: step 15809, loss 0.0117254, acc 1
2016-09-05T22:45:48.937201: step 15810, loss 0.00240043, acc 1
2016-09-05T22:45:49.740081: step 15811, loss 0.00894875, acc 1
2016-09-05T22:45:50.573089: step 15812, loss 0.0186438, acc 1
2016-09-05T22:45:51.377857: step 15813, loss 0.00164899, acc 1
2016-09-05T22:45:52.193562: step 15814, loss 0.0425396, acc 0.98
2016-09-05T22:45:53.006671: step 15815, loss 0.00166702, acc 1
2016-09-05T22:45:53.822183: step 15816, loss 0.026327, acc 0.98
2016-09-05T22:45:54.615637: step 15817, loss 0.00906186, acc 1
2016-09-05T22:45:55.404946: step 15818, loss 0.011345, acc 1
2016-09-05T22:45:56.239179: step 15819, loss 0.00856291, acc 1
2016-09-05T22:45:57.055951: step 15820, loss 0.00161664, acc 1
2016-09-05T22:45:57.869998: step 15821, loss 0.001614, acc 1
2016-09-05T22:45:58.691329: step 15822, loss 0.00217486, acc 1
2016-09-05T22:45:59.478304: step 15823, loss 0.00237536, acc 1
2016-09-05T22:46:00.262615: step 15824, loss 0.0256955, acc 1
2016-09-05T22:46:01.075894: step 15825, loss 0.0662194, acc 0.98
2016-09-05T22:46:01.854004: step 15826, loss 0.00493885, acc 1
2016-09-05T22:46:02.644991: step 15827, loss 0.0103375, acc 1
2016-09-05T22:46:03.449090: step 15828, loss 0.00178706, acc 1
2016-09-05T22:46:04.254545: step 15829, loss 0.114537, acc 0.96
2016-09-05T22:46:05.110917: step 15830, loss 0.026847, acc 0.98
2016-09-05T22:46:05.911067: step 15831, loss 0.00309253, acc 1
2016-09-05T22:46:06.721503: step 15832, loss 0.010909, acc 1
2016-09-05T22:46:07.523230: step 15833, loss 0.00180009, acc 1
2016-09-05T22:46:08.316579: step 15834, loss 0.0271127, acc 0.98
2016-09-05T22:46:09.106269: step 15835, loss 0.00169546, acc 1
2016-09-05T22:46:09.931965: step 15836, loss 0.00803836, acc 1
2016-09-05T22:46:10.721015: step 15837, loss 0.00124304, acc 1
2016-09-05T22:46:11.533840: step 15838, loss 0.0059352, acc 1
2016-09-05T22:46:12.349633: step 15839, loss 0.00894664, acc 1
2016-09-05T22:46:13.189867: step 15840, loss 0.02828, acc 0.98
2016-09-05T22:46:13.969912: step 15841, loss 0.0117292, acc 1
2016-09-05T22:46:14.786108: step 15842, loss 0.0180842, acc 1
2016-09-05T22:46:15.587677: step 15843, loss 0.00131095, acc 1
2016-09-05T22:46:16.365601: step 15844, loss 0.0187412, acc 1
2016-09-05T22:46:17.187605: step 15845, loss 0.0076592, acc 1
2016-09-05T22:46:17.981724: step 15846, loss 0.00129688, acc 1
2016-09-05T22:46:18.784593: step 15847, loss 0.00127971, acc 1
2016-09-05T22:46:19.604549: step 15848, loss 0.051744, acc 0.98
2016-09-05T22:46:20.393474: step 15849, loss 0.0253267, acc 0.98
2016-09-05T22:46:21.191873: step 15850, loss 0.00541807, acc 1
2016-09-05T22:46:22.013760: step 15851, loss 0.0153037, acc 1
2016-09-05T22:46:22.813759: step 15852, loss 0.00166798, acc 1
2016-09-05T22:46:23.626304: step 15853, loss 0.0839313, acc 0.98
2016-09-05T22:46:24.447667: step 15854, loss 0.0175138, acc 0.98
2016-09-05T22:46:25.257320: step 15855, loss 0.00379493, acc 1
2016-09-05T22:46:26.041992: step 15856, loss 0.0262046, acc 0.98
2016-09-05T22:46:26.867050: step 15857, loss 0.0298645, acc 0.98
2016-09-05T22:46:27.725588: step 15858, loss 0.00364195, acc 1
2016-09-05T22:46:28.509332: step 15859, loss 0.0371495, acc 0.98
2016-09-05T22:46:29.318239: step 15860, loss 0.00165699, acc 1
2016-09-05T22:46:30.128309: step 15861, loss 0.00678775, acc 1
2016-09-05T22:46:30.981242: step 15862, loss 0.0426915, acc 0.96
2016-09-05T22:46:31.781015: step 15863, loss 0.00388221, acc 1
2016-09-05T22:46:32.606639: step 15864, loss 0.0032656, acc 1
2016-09-05T22:46:33.413882: step 15865, loss 0.0192701, acc 0.98
2016-09-05T22:46:34.215687: step 15866, loss 0.10721, acc 0.98
2016-09-05T22:46:35.031119: step 15867, loss 0.0531542, acc 0.96
2016-09-05T22:46:35.828684: step 15868, loss 0.0484674, acc 0.96
2016-09-05T22:46:36.666672: step 15869, loss 0.00940712, acc 1
2016-09-05T22:46:37.518943: step 15870, loss 0.00208037, acc 1
2016-09-05T22:46:38.312782: step 15871, loss 0.0106055, acc 1
2016-09-05T22:46:39.131539: step 15872, loss 0.00578759, acc 1
2016-09-05T22:46:39.959695: step 15873, loss 0.00303041, acc 1
2016-09-05T22:46:40.760079: step 15874, loss 0.0199026, acc 0.98
2016-09-05T22:46:41.552956: step 15875, loss 0.00659082, acc 1
2016-09-05T22:46:42.372205: step 15876, loss 0.0178526, acc 1
2016-09-05T22:46:43.200004: step 15877, loss 0.00777537, acc 1
2016-09-05T22:46:44.037803: step 15878, loss 0.0131193, acc 1
2016-09-05T22:46:44.888747: step 15879, loss 0.0117137, acc 1
2016-09-05T22:46:45.725493: step 15880, loss 0.00454701, acc 1
2016-09-05T22:46:46.573224: step 15881, loss 0.0167402, acc 1
2016-09-05T22:46:47.412148: step 15882, loss 0.00443347, acc 1
2016-09-05T22:46:48.216994: step 15883, loss 0.0214255, acc 0.98
2016-09-05T22:46:49.046510: step 15884, loss 0.030138, acc 1
2016-09-05T22:46:49.885430: step 15885, loss 0.00544166, acc 1
2016-09-05T22:46:50.712662: step 15886, loss 0.00374775, acc 1
2016-09-05T22:46:51.522523: step 15887, loss 0.00280921, acc 1
2016-09-05T22:46:52.321602: step 15888, loss 0.0133014, acc 1
2016-09-05T22:46:53.151478: step 15889, loss 0.016285, acc 0.98
2016-09-05T22:46:53.924988: step 15890, loss 0.00757792, acc 1
2016-09-05T22:46:54.749406: step 15891, loss 0.00207742, acc 1
2016-09-05T22:46:55.546680: step 15892, loss 0.00250424, acc 1
2016-09-05T22:46:56.335033: step 15893, loss 0.00201892, acc 1
2016-09-05T22:46:57.136662: step 15894, loss 0.0195116, acc 0.98
2016-09-05T22:46:57.956508: step 15895, loss 0.0131497, acc 1
2016-09-05T22:46:58.746667: step 15896, loss 0.00571659, acc 1
2016-09-05T22:46:59.543992: step 15897, loss 0.00638846, acc 1
2016-09-05T22:47:00.356995: step 15898, loss 0.0216162, acc 0.98
2016-09-05T22:47:01.163931: step 15899, loss 0.00555316, acc 1
2016-09-05T22:47:01.962437: step 15900, loss 0.00359654, acc 1

Evaluation:
2016-09-05T22:47:05.491121: step 15900, loss 3.23282, acc 0.726

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-15900

2016-09-05T22:47:07.310855: step 15901, loss 0.0208676, acc 1
2016-09-05T22:47:08.110998: step 15902, loss 0.00210816, acc 1
2016-09-05T22:47:08.906766: step 15903, loss 0.0271867, acc 0.98
2016-09-05T22:47:09.685608: step 15904, loss 0.0178516, acc 1
2016-09-05T22:47:10.474988: step 15905, loss 0.0643937, acc 0.98
2016-09-05T22:47:11.285973: step 15906, loss 0.0729383, acc 0.98
2016-09-05T22:47:12.065281: step 15907, loss 0.00199551, acc 1
2016-09-05T22:47:12.482119: step 15908, loss 0.0586, acc 1
2016-09-05T22:47:13.257843: step 15909, loss 0.0319082, acc 0.98
2016-09-05T22:47:14.067916: step 15910, loss 0.00411992, acc 1
2016-09-05T22:47:14.885931: step 15911, loss 0.00763705, acc 1
2016-09-05T22:47:15.661712: step 15912, loss 0.0144253, acc 1
2016-09-05T22:47:16.472385: step 15913, loss 0.002826, acc 1
2016-09-05T22:47:17.288141: step 15914, loss 0.007616, acc 1
2016-09-05T22:47:18.070517: step 15915, loss 0.0152822, acc 1
2016-09-05T22:47:18.910678: step 15916, loss 0.0139842, acc 1
2016-09-05T22:47:19.740305: step 15917, loss 0.00191293, acc 1
2016-09-05T22:47:20.547816: step 15918, loss 0.00193112, acc 1
2016-09-05T22:47:21.347111: step 15919, loss 0.00560123, acc 1
2016-09-05T22:47:22.167136: step 15920, loss 0.0199441, acc 0.98
2016-09-05T22:47:22.959141: step 15921, loss 0.0204781, acc 0.98
2016-09-05T22:47:23.793097: step 15922, loss 0.00374488, acc 1
2016-09-05T22:47:24.603775: step 15923, loss 0.00188052, acc 1
2016-09-05T22:47:25.391593: step 15924, loss 0.00188637, acc 1
2016-09-05T22:47:26.221817: step 15925, loss 0.0176545, acc 1
2016-09-05T22:47:27.032094: step 15926, loss 0.00573255, acc 1
2016-09-05T22:47:27.810050: step 15927, loss 0.0264608, acc 0.98
2016-09-05T22:47:28.621909: step 15928, loss 0.00233568, acc 1
2016-09-05T22:47:29.467745: step 15929, loss 0.012301, acc 1
2016-09-05T22:47:30.274271: step 15930, loss 0.00209894, acc 1
2016-09-05T22:47:31.061913: step 15931, loss 0.00195792, acc 1
2016-09-05T22:47:31.891126: step 15932, loss 0.00214157, acc 1
2016-09-05T22:47:32.717328: step 15933, loss 0.0107957, acc 1
2016-09-05T22:47:33.531316: step 15934, loss 0.00189965, acc 1
2016-09-05T22:47:34.338302: step 15935, loss 0.00189377, acc 1
2016-09-05T22:47:35.157707: step 15936, loss 0.0063434, acc 1
2016-09-05T22:47:35.972556: step 15937, loss 0.00810072, acc 1
2016-09-05T22:47:36.787539: step 15938, loss 0.0177741, acc 0.98
2016-09-05T22:47:37.594151: step 15939, loss 0.0186749, acc 0.98
2016-09-05T22:47:38.409220: step 15940, loss 0.0481779, acc 0.96
2016-09-05T22:47:39.266930: step 15941, loss 0.00189476, acc 1
2016-09-05T22:47:40.080851: step 15942, loss 0.0506554, acc 0.96
2016-09-05T22:47:40.917538: step 15943, loss 0.0202249, acc 0.98
2016-09-05T22:47:41.747344: step 15944, loss 0.0325567, acc 0.98
2016-09-05T22:47:42.567471: step 15945, loss 0.0026977, acc 1
2016-09-05T22:47:43.391978: step 15946, loss 0.00682928, acc 1
2016-09-05T22:47:44.223261: step 15947, loss 0.00206752, acc 1
2016-09-05T22:47:45.044978: step 15948, loss 0.00188541, acc 1
2016-09-05T22:47:45.843611: step 15949, loss 0.0388736, acc 0.96
2016-09-05T22:47:46.666562: step 15950, loss 0.0163491, acc 0.98
2016-09-05T22:47:47.450586: step 15951, loss 0.00270817, acc 1
2016-09-05T22:47:48.262252: step 15952, loss 0.00479011, acc 1
2016-09-05T22:47:49.087212: step 15953, loss 0.0144825, acc 1
2016-09-05T22:47:49.886703: step 15954, loss 0.0642411, acc 0.98
2016-09-05T22:47:50.706174: step 15955, loss 0.0427891, acc 0.98
2016-09-05T22:47:51.524713: step 15956, loss 0.0156278, acc 1
2016-09-05T22:47:52.351984: step 15957, loss 0.00176602, acc 1
2016-09-05T22:47:53.133147: step 15958, loss 0.016628, acc 1
2016-09-05T22:47:53.947502: step 15959, loss 0.00273565, acc 1
2016-09-05T22:47:54.773968: step 15960, loss 0.00192394, acc 1
2016-09-05T22:47:55.612569: step 15961, loss 0.00158073, acc 1
2016-09-05T22:47:56.413408: step 15962, loss 0.00157318, acc 1
2016-09-05T22:47:57.221896: step 15963, loss 0.0292189, acc 0.98
2016-09-05T22:47:58.008993: step 15964, loss 0.0217688, acc 0.98
2016-09-05T22:47:58.805984: step 15965, loss 0.0172903, acc 0.98
2016-09-05T22:47:59.631295: step 15966, loss 0.00153257, acc 1
2016-09-05T22:48:00.448579: step 15967, loss 0.00205953, acc 1
2016-09-05T22:48:01.262739: step 15968, loss 0.00146509, acc 1
2016-09-05T22:48:02.079523: step 15969, loss 0.0028362, acc 1
2016-09-05T22:48:02.876632: step 15970, loss 0.00463837, acc 1
2016-09-05T22:48:03.683211: step 15971, loss 0.0354483, acc 0.98
2016-09-05T22:48:04.537913: step 15972, loss 0.0366986, acc 0.98
2016-09-05T22:48:05.379121: step 15973, loss 0.0305898, acc 0.96
2016-09-05T22:48:06.162715: step 15974, loss 0.0153967, acc 1
2016-09-05T22:48:06.998867: step 15975, loss 0.00393867, acc 1
2016-09-05T22:48:07.789371: step 15976, loss 0.00170265, acc 1
2016-09-05T22:48:08.576089: step 15977, loss 0.00652494, acc 1
2016-09-05T22:48:09.424426: step 15978, loss 0.0156635, acc 1
2016-09-05T22:48:10.224076: step 15979, loss 0.0111039, acc 1
2016-09-05T22:48:11.057253: step 15980, loss 0.0193136, acc 1
2016-09-05T22:48:11.909397: step 15981, loss 0.00205383, acc 1
2016-09-05T22:48:12.734923: step 15982, loss 0.00208155, acc 1
2016-09-05T22:48:13.500146: step 15983, loss 0.0247719, acc 0.98
2016-09-05T22:48:14.343515: step 15984, loss 0.0393095, acc 0.98
2016-09-05T22:48:15.146139: step 15985, loss 0.0176672, acc 1
2016-09-05T22:48:15.962756: step 15986, loss 0.00158497, acc 1
2016-09-05T22:48:16.776612: step 15987, loss 0.0239483, acc 0.98
2016-09-05T22:48:17.562635: step 15988, loss 0.0151526, acc 1
2016-09-05T22:48:18.404438: step 15989, loss 0.0892309, acc 0.98
2016-09-05T22:48:19.248924: step 15990, loss 0.00224259, acc 1
2016-09-05T22:48:20.089493: step 15991, loss 0.0197009, acc 0.98
2016-09-05T22:48:20.888876: step 15992, loss 0.00155194, acc 1
2016-09-05T22:48:21.701599: step 15993, loss 0.00356317, acc 1
2016-09-05T22:48:22.507939: step 15994, loss 0.012988, acc 1
2016-09-05T22:48:23.304713: step 15995, loss 0.00152764, acc 1
2016-09-05T22:48:24.135338: step 15996, loss 0.00154478, acc 1
2016-09-05T22:48:24.947322: step 15997, loss 0.0583569, acc 0.98
2016-09-05T22:48:25.727562: step 15998, loss 0.0357905, acc 0.96
2016-09-05T22:48:26.535419: step 15999, loss 0.00337126, acc 1
2016-09-05T22:48:27.339921: step 16000, loss 0.0548284, acc 0.98

Evaluation:
2016-09-05T22:48:30.820793: step 16000, loss 2.68248, acc 0.732

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-16000

2016-09-05T22:48:32.801748: step 16001, loss 0.00453272, acc 1
2016-09-05T22:48:33.610172: step 16002, loss 0.0347225, acc 0.96
2016-09-05T22:48:34.430611: step 16003, loss 0.0332396, acc 0.98
2016-09-05T22:48:35.229866: step 16004, loss 0.00767666, acc 1
2016-09-05T22:48:36.058855: step 16005, loss 0.0143292, acc 1
2016-09-05T22:48:36.850820: step 16006, loss 0.0396756, acc 0.98
2016-09-05T22:48:37.667759: step 16007, loss 0.0075141, acc 1
2016-09-05T22:48:38.502885: step 16008, loss 0.019273, acc 1
2016-09-05T22:48:39.314388: step 16009, loss 0.0213182, acc 0.98
2016-09-05T22:48:40.150563: step 16010, loss 0.0364714, acc 0.96
2016-09-05T22:48:40.972377: step 16011, loss 0.0149592, acc 1
2016-09-05T22:48:41.777500: step 16012, loss 0.0509237, acc 0.96
2016-09-05T22:48:42.611131: step 16013, loss 0.0018624, acc 1
2016-09-05T22:48:43.444849: step 16014, loss 0.0964153, acc 0.96
2016-09-05T22:48:44.280006: step 16015, loss 0.00281038, acc 1
2016-09-05T22:48:45.104097: step 16016, loss 0.00671967, acc 1
2016-09-05T22:48:45.945589: step 16017, loss 0.0125089, acc 1
2016-09-05T22:48:46.795421: step 16018, loss 0.0272104, acc 0.98
2016-09-05T22:48:47.573992: step 16019, loss 0.020094, acc 1
2016-09-05T22:48:48.365869: step 16020, loss 0.0239803, acc 0.98
2016-09-05T22:48:49.188944: step 16021, loss 0.0157798, acc 1
2016-09-05T22:48:49.988902: step 16022, loss 0.0201362, acc 0.98
2016-09-05T22:48:50.817419: step 16023, loss 0.00185418, acc 1
2016-09-05T22:48:51.621509: step 16024, loss 0.0287299, acc 1
2016-09-05T22:48:52.421401: step 16025, loss 0.015138, acc 1
2016-09-05T22:48:53.195285: step 16026, loss 0.0103897, acc 1
2016-09-05T22:48:54.051644: step 16027, loss 0.0277485, acc 0.98
2016-09-05T22:48:54.865145: step 16028, loss 0.0703867, acc 0.96
2016-09-05T22:48:55.666388: step 16029, loss 0.00372323, acc 1
2016-09-05T22:48:56.480920: step 16030, loss 0.0206727, acc 1
2016-09-05T22:48:57.282796: step 16031, loss 0.00208899, acc 1
2016-09-05T22:48:58.067693: step 16032, loss 0.0296, acc 0.98
2016-09-05T22:48:58.883031: step 16033, loss 0.073759, acc 0.98
2016-09-05T22:48:59.647973: step 16034, loss 0.020775, acc 1
2016-09-05T22:49:00.496429: step 16035, loss 0.0156916, acc 1
2016-09-05T22:49:01.322314: step 16036, loss 0.0216966, acc 1
2016-09-05T22:49:02.107769: step 16037, loss 0.00253735, acc 1
2016-09-05T22:49:02.892767: step 16038, loss 0.0223519, acc 0.98
2016-09-05T22:49:03.731605: step 16039, loss 0.0106626, acc 1
2016-09-05T22:49:04.539243: step 16040, loss 0.0115282, acc 1
2016-09-05T22:49:05.351555: step 16041, loss 0.0047004, acc 1
2016-09-05T22:49:06.238011: step 16042, loss 0.00230056, acc 1
2016-09-05T22:49:07.029399: step 16043, loss 0.0179491, acc 1
2016-09-05T22:49:07.823521: step 16044, loss 0.0135234, acc 1
2016-09-05T22:49:08.664711: step 16045, loss 0.0231188, acc 0.98
2016-09-05T22:49:09.493180: step 16046, loss 0.00236524, acc 1
2016-09-05T22:49:10.290498: step 16047, loss 0.00283126, acc 1
2016-09-05T22:49:11.134987: step 16048, loss 0.00363386, acc 1
2016-09-05T22:49:11.988065: step 16049, loss 0.00307232, acc 1
2016-09-05T22:49:12.771203: step 16050, loss 0.118321, acc 0.96
2016-09-05T22:49:13.633362: step 16051, loss 0.0382614, acc 0.98
2016-09-05T22:49:14.445190: step 16052, loss 0.0356693, acc 0.98
2016-09-05T22:49:15.243732: step 16053, loss 0.0312576, acc 1
2016-09-05T22:49:16.091416: step 16054, loss 0.00218695, acc 1
2016-09-05T22:49:16.901370: step 16055, loss 0.00249289, acc 1
2016-09-05T22:49:17.713883: step 16056, loss 0.0194975, acc 1
2016-09-05T22:49:18.537098: step 16057, loss 0.0139585, acc 1
2016-09-05T22:49:19.350681: step 16058, loss 0.00672177, acc 1
2016-09-05T22:49:20.142319: step 16059, loss 0.00565032, acc 1
2016-09-05T22:49:20.966247: step 16060, loss 0.0274552, acc 1
2016-09-05T22:49:21.808907: step 16061, loss 0.0187801, acc 1
2016-09-05T22:49:22.631178: step 16062, loss 0.0137189, acc 1
2016-09-05T22:49:23.461463: step 16063, loss 0.00450339, acc 1
2016-09-05T22:49:24.292619: step 16064, loss 0.00358632, acc 1
2016-09-05T22:49:25.073065: step 16065, loss 0.00213095, acc 1
2016-09-05T22:49:25.900676: step 16066, loss 0.0423456, acc 0.98
2016-09-05T22:49:26.703765: step 16067, loss 0.00218147, acc 1
2016-09-05T22:49:27.505161: step 16068, loss 0.00259623, acc 1
2016-09-05T22:49:28.306226: step 16069, loss 0.00228814, acc 1
2016-09-05T22:49:29.150926: step 16070, loss 0.00449516, acc 1
2016-09-05T22:49:29.939922: step 16071, loss 0.00214329, acc 1
2016-09-05T22:49:30.744800: step 16072, loss 0.00678119, acc 1
2016-09-05T22:49:31.584448: step 16073, loss 0.00216834, acc 1
2016-09-05T22:49:32.471935: step 16074, loss 0.00374358, acc 1
2016-09-05T22:49:33.287562: step 16075, loss 0.00224427, acc 1
2016-09-05T22:49:34.142483: step 16076, loss 0.00215552, acc 1
2016-09-05T22:49:34.944701: step 16077, loss 0.00252989, acc 1
2016-09-05T22:49:35.766031: step 16078, loss 0.03925, acc 0.98
2016-09-05T22:49:36.612671: step 16079, loss 0.00535965, acc 1
2016-09-05T22:49:37.432844: step 16080, loss 0.00742956, acc 1
2016-09-05T22:49:38.239832: step 16081, loss 0.0080047, acc 1
2016-09-05T22:49:39.088312: step 16082, loss 0.00204302, acc 1
2016-09-05T22:49:39.895337: step 16083, loss 0.0195817, acc 1
2016-09-05T22:49:40.707462: step 16084, loss 0.022759, acc 0.98
2016-09-05T22:49:41.514713: step 16085, loss 0.0168031, acc 0.98
2016-09-05T22:49:42.338170: step 16086, loss 0.00577969, acc 1
2016-09-05T22:49:43.135591: step 16087, loss 0.00205654, acc 1
2016-09-05T22:49:43.962374: step 16088, loss 0.00354504, acc 1
2016-09-05T22:49:44.787972: step 16089, loss 0.0451154, acc 0.98
2016-09-05T22:49:45.578058: step 16090, loss 0.00238276, acc 1
2016-09-05T22:49:46.372876: step 16091, loss 0.0211809, acc 0.98
2016-09-05T22:49:47.183940: step 16092, loss 0.00202197, acc 1
2016-09-05T22:49:47.978407: step 16093, loss 0.00200619, acc 1
2016-09-05T22:49:48.792680: step 16094, loss 0.0421274, acc 0.98
2016-09-05T22:49:49.615234: step 16095, loss 0.0192145, acc 0.98
2016-09-05T22:49:50.419477: step 16096, loss 0.00198512, acc 1
2016-09-05T22:49:51.251153: step 16097, loss 0.00477416, acc 1
2016-09-05T22:49:52.082164: step 16098, loss 0.017828, acc 0.98
2016-09-05T22:49:52.879080: step 16099, loss 0.0313251, acc 0.98
2016-09-05T22:49:53.700223: step 16100, loss 0.240333, acc 0.96

Evaluation:
2016-09-05T22:49:57.180481: step 16100, loss 3.13107, acc 0.723

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-16100

2016-09-05T22:49:59.041446: step 16101, loss 0.0018316, acc 1
2016-09-05T22:49:59.476817: step 16102, loss 0.00173206, acc 1
2016-09-05T22:50:00.310687: step 16103, loss 0.0280754, acc 0.98
2016-09-05T22:50:01.116560: step 16104, loss 0.0114024, acc 1
2016-09-05T22:50:01.977509: step 16105, loss 0.10681, acc 0.96
2016-09-05T22:50:02.792455: step 16106, loss 0.0562186, acc 0.98
2016-09-05T22:50:03.595609: step 16107, loss 0.00914255, acc 1
2016-09-05T22:50:04.419660: step 16108, loss 0.0314429, acc 1
2016-09-05T22:50:05.229419: step 16109, loss 0.00184638, acc 1
2016-09-05T22:50:06.059488: step 16110, loss 0.0091583, acc 1
2016-09-05T22:50:06.891813: step 16111, loss 0.00184038, acc 1
2016-09-05T22:50:07.701784: step 16112, loss 0.00255466, acc 1
2016-09-05T22:50:08.518219: step 16113, loss 0.00308613, acc 1
2016-09-05T22:50:09.348268: step 16114, loss 0.0401534, acc 0.98
2016-09-05T22:50:10.162873: step 16115, loss 0.0212354, acc 0.98
2016-09-05T22:50:11.001477: step 16116, loss 0.00425323, acc 1
2016-09-05T22:50:11.801713: step 16117, loss 0.00201857, acc 1
2016-09-05T22:50:12.656502: step 16118, loss 0.0019223, acc 1
2016-09-05T22:50:13.456302: step 16119, loss 0.037588, acc 0.98
2016-09-05T22:50:14.239674: step 16120, loss 0.00500064, acc 1
2016-09-05T22:50:15.085393: step 16121, loss 0.0034734, acc 1
2016-09-05T22:50:15.929818: step 16122, loss 0.0119098, acc 1
2016-09-05T22:50:16.744908: step 16123, loss 0.00724837, acc 1
2016-09-05T22:50:17.613957: step 16124, loss 0.0133432, acc 1
2016-09-05T22:50:18.451308: step 16125, loss 0.0169871, acc 0.98
2016-09-05T22:50:19.271555: step 16126, loss 0.00611132, acc 1
2016-09-05T22:50:20.120934: step 16127, loss 0.00908, acc 1
2016-09-05T22:50:20.939211: step 16128, loss 0.00198519, acc 1
2016-09-05T22:50:21.798250: step 16129, loss 0.00238682, acc 1
2016-09-05T22:50:22.658939: step 16130, loss 0.00233257, acc 1
2016-09-05T22:50:23.491401: step 16131, loss 0.00284514, acc 1
2016-09-05T22:50:24.281905: step 16132, loss 0.011959, acc 1
2016-09-05T22:50:25.109010: step 16133, loss 0.00940631, acc 1
2016-09-05T22:50:25.974260: step 16134, loss 0.0131331, acc 1
2016-09-05T22:50:26.777968: step 16135, loss 0.00207698, acc 1
2016-09-05T22:50:27.573867: step 16136, loss 0.010642, acc 1
2016-09-05T22:50:28.402677: step 16137, loss 0.00209813, acc 1
2016-09-05T22:50:29.196986: step 16138, loss 0.0126763, acc 1
2016-09-05T22:50:29.980481: step 16139, loss 0.0309163, acc 0.98
2016-09-05T22:50:30.780067: step 16140, loss 0.00863611, acc 1
2016-09-05T22:50:31.565236: step 16141, loss 0.00747985, acc 1
2016-09-05T22:50:32.364672: step 16142, loss 0.00219927, acc 1
2016-09-05T22:50:33.173963: step 16143, loss 0.0281664, acc 1
2016-09-05T22:50:33.963839: step 16144, loss 0.0481241, acc 0.98
2016-09-05T22:50:34.762412: step 16145, loss 0.00444876, acc 1
2016-09-05T22:50:35.595342: step 16146, loss 0.00219658, acc 1
2016-09-05T22:50:36.375252: step 16147, loss 0.0338842, acc 0.98
2016-09-05T22:50:37.187825: step 16148, loss 0.0470459, acc 0.98
2016-09-05T22:50:38.002706: step 16149, loss 0.00216825, acc 1
2016-09-05T22:50:38.799390: step 16150, loss 0.016277, acc 1
2016-09-05T22:50:39.610897: step 16151, loss 0.00206531, acc 1
2016-09-05T22:50:40.427486: step 16152, loss 0.0257884, acc 1
2016-09-05T22:50:41.221234: step 16153, loss 0.041764, acc 0.96
2016-09-05T22:50:42.051252: step 16154, loss 0.00620895, acc 1
2016-09-05T22:50:42.870265: step 16155, loss 0.00230518, acc 1
2016-09-05T22:50:43.634403: step 16156, loss 0.0465423, acc 0.98
2016-09-05T22:50:44.475692: step 16157, loss 0.0181992, acc 0.98
2016-09-05T22:50:45.251044: step 16158, loss 0.00212361, acc 1
2016-09-05T22:50:46.050214: step 16159, loss 0.0238227, acc 0.98
2016-09-05T22:50:46.869430: step 16160, loss 0.00193921, acc 1
2016-09-05T22:50:47.677277: step 16161, loss 0.00214628, acc 1
2016-09-05T22:50:48.462470: step 16162, loss 0.0184304, acc 0.98
2016-09-05T22:50:49.298160: step 16163, loss 0.00446261, acc 1
2016-09-05T22:50:50.122853: step 16164, loss 0.00202718, acc 1
2016-09-05T22:50:50.914723: step 16165, loss 0.00877444, acc 1
2016-09-05T22:50:51.738651: step 16166, loss 0.0075644, acc 1
2016-09-05T22:50:52.563227: step 16167, loss 0.0238296, acc 0.98
2016-09-05T22:50:53.349734: step 16168, loss 0.0169517, acc 1
2016-09-05T22:50:54.164124: step 16169, loss 0.00445758, acc 1
2016-09-05T22:50:54.978194: step 16170, loss 0.00237416, acc 1
2016-09-05T22:50:55.775948: step 16171, loss 0.0080352, acc 1
2016-09-05T22:50:56.587158: step 16172, loss 0.0389819, acc 0.96
2016-09-05T22:50:57.399867: step 16173, loss 0.0045018, acc 1
2016-09-05T22:50:58.223219: step 16174, loss 0.0018342, acc 1
2016-09-05T22:50:59.040144: step 16175, loss 0.00190888, acc 1
2016-09-05T22:50:59.878223: step 16176, loss 0.00189002, acc 1
2016-09-05T22:51:00.713703: step 16177, loss 0.00862528, acc 1
2016-09-05T22:51:01.529186: step 16178, loss 0.0453392, acc 0.96
2016-09-05T22:51:02.346400: step 16179, loss 0.00945585, acc 1
2016-09-05T22:51:03.159491: step 16180, loss 0.00200102, acc 1
2016-09-05T22:51:04.002519: step 16181, loss 0.00247576, acc 1
2016-09-05T22:51:04.842946: step 16182, loss 0.0213059, acc 0.98
2016-09-05T22:51:05.661990: step 16183, loss 0.00913785, acc 1
2016-09-05T22:51:06.468406: step 16184, loss 0.00186333, acc 1
2016-09-05T22:51:07.295821: step 16185, loss 0.00856724, acc 1
2016-09-05T22:51:08.105922: step 16186, loss 0.00197675, acc 1
2016-09-05T22:51:08.920351: step 16187, loss 0.00185983, acc 1
2016-09-05T22:51:09.761802: step 16188, loss 0.00582752, acc 1
2016-09-05T22:51:10.568408: step 16189, loss 0.00287547, acc 1
2016-09-05T22:51:11.348899: step 16190, loss 0.00194428, acc 1
2016-09-05T22:51:12.179298: step 16191, loss 0.00568665, acc 1
2016-09-05T22:51:12.990015: step 16192, loss 0.0551821, acc 0.96
2016-09-05T22:51:13.863206: step 16193, loss 0.0155534, acc 1
2016-09-05T22:51:14.652913: step 16194, loss 0.0113079, acc 1
2016-09-05T22:51:15.460069: step 16195, loss 0.00186691, acc 1
2016-09-05T22:51:16.259608: step 16196, loss 0.0114525, acc 1
2016-09-05T22:51:17.103362: step 16197, loss 0.00987327, acc 1
2016-09-05T22:51:17.901463: step 16198, loss 0.0924563, acc 0.98
2016-09-05T22:51:18.722187: step 16199, loss 0.0162291, acc 0.98
2016-09-05T22:51:19.539380: step 16200, loss 0.0235973, acc 0.98

Evaluation:
2016-09-05T22:51:23.023701: step 16200, loss 3.44349, acc 0.703

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-16200

2016-09-05T22:51:24.891586: step 16201, loss 0.00469619, acc 1
2016-09-05T22:51:25.783860: step 16202, loss 0.0292961, acc 0.98
2016-09-05T22:51:26.813125: step 16203, loss 0.00287431, acc 1
2016-09-05T22:51:27.864858: step 16204, loss 0.0048793, acc 1
2016-09-05T22:51:28.860161: step 16205, loss 0.0092675, acc 1
2016-09-05T22:51:30.039226: step 16206, loss 0.00770475, acc 1
2016-09-05T22:51:30.951353: step 16207, loss 0.0184585, acc 1
2016-09-05T22:51:32.075145: step 16208, loss 0.0054549, acc 1
2016-09-05T22:51:33.336102: step 16209, loss 0.0224708, acc 0.98
2016-09-05T22:51:34.336862: step 16210, loss 0.0262811, acc 0.98
2016-09-05T22:51:35.370236: step 16211, loss 0.0201011, acc 0.98
2016-09-05T22:51:36.541971: step 16212, loss 0.00974616, acc 1
2016-09-05T22:51:37.479240: step 16213, loss 0.021719, acc 0.98
2016-09-05T22:51:38.777912: step 16214, loss 0.00574599, acc 1
2016-09-05T22:51:39.708798: step 16215, loss 0.0324463, acc 0.98
2016-09-05T22:51:41.209645: step 16216, loss 0.00442396, acc 1
2016-09-05T22:51:42.494378: step 16217, loss 0.0897201, acc 0.98
2016-09-05T22:51:43.676838: step 16218, loss 0.00813908, acc 1
2016-09-05T22:51:44.770448: step 16219, loss 0.00452554, acc 1
2016-09-05T22:51:45.814354: step 16220, loss 0.0549108, acc 0.98
2016-09-05T22:51:46.832961: step 16221, loss 0.00476244, acc 1
2016-09-05T22:51:47.684887: step 16222, loss 0.00494712, acc 1
2016-09-05T22:51:48.576553: step 16223, loss 0.0499757, acc 0.98
2016-09-05T22:51:49.520308: step 16224, loss 0.0163788, acc 1
2016-09-05T22:51:50.403204: step 16225, loss 0.00516348, acc 1
2016-09-05T22:51:51.321019: step 16226, loss 0.00491351, acc 1
2016-09-05T22:51:52.220768: step 16227, loss 0.0217089, acc 0.98
2016-09-05T22:51:53.160015: step 16228, loss 0.00553549, acc 1
2016-09-05T22:51:54.002259: step 16229, loss 0.00734638, acc 1
2016-09-05T22:51:55.025938: step 16230, loss 0.0049441, acc 1
2016-09-05T22:51:55.955532: step 16231, loss 0.0263232, acc 0.98
2016-09-05T22:51:56.996877: step 16232, loss 0.00786558, acc 1
2016-09-05T22:51:58.014710: step 16233, loss 0.00603435, acc 1
2016-09-05T22:51:59.083702: step 16234, loss 0.0117527, acc 1
2016-09-05T22:52:00.118540: step 16235, loss 0.0048903, acc 1
2016-09-05T22:52:01.056886: step 16236, loss 0.0348266, acc 0.98
2016-09-05T22:52:01.944541: step 16237, loss 0.00834121, acc 1
2016-09-05T22:52:03.013524: step 16238, loss 0.113561, acc 0.98
2016-09-05T22:52:04.121782: step 16239, loss 0.226707, acc 0.98
2016-09-05T22:52:05.085429: step 16240, loss 0.00454914, acc 1
2016-09-05T22:52:05.920841: step 16241, loss 0.00431118, acc 1
2016-09-05T22:52:06.816164: step 16242, loss 0.00982422, acc 1
2016-09-05T22:52:07.735078: step 16243, loss 0.0169336, acc 1
2016-09-05T22:52:08.566810: step 16244, loss 0.168149, acc 0.96
2016-09-05T22:52:09.377980: step 16245, loss 0.0131359, acc 1
2016-09-05T22:52:10.226060: step 16246, loss 0.0147896, acc 1
2016-09-05T22:52:11.038720: step 16247, loss 0.00604302, acc 1
2016-09-05T22:52:11.856484: step 16248, loss 0.0122506, acc 1
2016-09-05T22:52:12.670257: step 16249, loss 0.0194647, acc 0.98
2016-09-05T22:52:13.485014: step 16250, loss 0.0337879, acc 0.98
2016-09-05T22:52:14.304199: step 16251, loss 0.0459116, acc 0.96
2016-09-05T22:52:15.129729: step 16252, loss 0.0294592, acc 0.98
2016-09-05T22:52:15.961067: step 16253, loss 0.0161797, acc 1
2016-09-05T22:52:16.797902: step 16254, loss 0.0240782, acc 1
2016-09-05T22:52:17.612660: step 16255, loss 0.00473937, acc 1
2016-09-05T22:52:18.449953: step 16256, loss 0.0322589, acc 0.98
2016-09-05T22:52:19.229356: step 16257, loss 0.00682165, acc 1
2016-09-05T22:52:20.038742: step 16258, loss 0.00333082, acc 1
2016-09-05T22:52:20.869344: step 16259, loss 0.00316068, acc 1
2016-09-05T22:52:21.670516: step 16260, loss 0.00658021, acc 1
2016-09-05T22:52:22.477393: step 16261, loss 0.00638603, acc 1
2016-09-05T22:52:23.303459: step 16262, loss 0.0173296, acc 0.98
2016-09-05T22:52:24.085565: step 16263, loss 0.00304535, acc 1
2016-09-05T22:52:24.888089: step 16264, loss 0.00304197, acc 1
2016-09-05T22:52:25.717716: step 16265, loss 0.00310823, acc 1
2016-09-05T22:52:26.522728: step 16266, loss 0.00749941, acc 1
2016-09-05T22:52:27.326433: step 16267, loss 0.00283661, acc 1
2016-09-05T22:52:28.163771: step 16268, loss 0.00677834, acc 1
2016-09-05T22:52:28.937230: step 16269, loss 0.00933951, acc 1
2016-09-05T22:52:29.742816: step 16270, loss 0.00764646, acc 1
2016-09-05T22:52:30.592546: step 16271, loss 0.00325419, acc 1
2016-09-05T22:52:31.396497: step 16272, loss 0.012441, acc 1
2016-09-05T22:52:32.193405: step 16273, loss 0.0179578, acc 0.98
2016-09-05T22:52:33.001906: step 16274, loss 0.00284062, acc 1
2016-09-05T22:52:33.779331: step 16275, loss 0.00719589, acc 1
2016-09-05T22:52:34.587818: step 16276, loss 0.00295374, acc 1
2016-09-05T22:52:35.394833: step 16277, loss 0.0236694, acc 0.98
2016-09-05T22:52:36.223550: step 16278, loss 0.00425406, acc 1
2016-09-05T22:52:37.007675: step 16279, loss 0.00388493, acc 1
2016-09-05T22:52:37.849699: step 16280, loss 0.00305646, acc 1
2016-09-05T22:52:38.626021: step 16281, loss 0.0980742, acc 0.96
2016-09-05T22:52:39.406164: step 16282, loss 0.0271715, acc 1
2016-09-05T22:52:40.220274: step 16283, loss 0.0221498, acc 0.98
2016-09-05T22:52:41.005443: step 16284, loss 0.00255672, acc 1
2016-09-05T22:52:41.822380: step 16285, loss 0.00411677, acc 1
2016-09-05T22:52:42.653448: step 16286, loss 0.00785097, acc 1
2016-09-05T22:52:43.436103: step 16287, loss 0.0302292, acc 0.98
2016-09-05T22:52:44.247955: step 16288, loss 0.00582916, acc 1
2016-09-05T22:52:45.079326: step 16289, loss 0.0167374, acc 1
2016-09-05T22:52:45.845018: step 16290, loss 0.00241833, acc 1
2016-09-05T22:52:46.651432: step 16291, loss 0.00501949, acc 1
2016-09-05T22:52:47.481664: step 16292, loss 0.0150648, acc 1
2016-09-05T22:52:48.282056: step 16293, loss 0.0261693, acc 0.98
2016-09-05T22:52:49.107689: step 16294, loss 0.0237892, acc 0.98
2016-09-05T22:52:49.928366: step 16295, loss 0.00661667, acc 1
2016-09-05T22:52:50.368829: step 16296, loss 0.00230188, acc 1
2016-09-05T22:52:51.192432: step 16297, loss 0.01306, acc 1
2016-09-05T22:52:52.030312: step 16298, loss 0.00849045, acc 1
2016-09-05T22:52:52.843250: step 16299, loss 0.0313196, acc 1
2016-09-05T22:52:53.669121: step 16300, loss 0.0258457, acc 1

Evaluation:
2016-09-05T22:52:57.132013: step 16300, loss 2.37498, acc 0.726

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-16300

2016-09-05T22:52:59.043465: step 16301, loss 0.00294953, acc 1
2016-09-05T22:52:59.925613: step 16302, loss 0.0262424, acc 0.98
2016-09-05T22:53:00.757786: step 16303, loss 0.0166739, acc 0.98
2016-09-05T22:53:01.579538: step 16304, loss 0.0112415, acc 1
2016-09-05T22:53:02.409958: step 16305, loss 0.0168689, acc 1
2016-09-05T22:53:03.244944: step 16306, loss 0.00917602, acc 1
2016-09-05T22:53:04.031964: step 16307, loss 0.00348241, acc 1
2016-09-05T22:53:04.836567: step 16308, loss 0.0137989, acc 1
2016-09-05T22:53:05.680610: step 16309, loss 0.0319626, acc 0.98
2016-09-05T22:53:06.498782: step 16310, loss 0.0350004, acc 0.98
2016-09-05T22:53:07.309877: step 16311, loss 0.00223504, acc 1
2016-09-05T22:53:08.126722: step 16312, loss 0.00845192, acc 1
2016-09-05T22:53:08.955602: step 16313, loss 0.00789756, acc 1
2016-09-05T22:53:09.776206: step 16314, loss 0.00226134, acc 1
2016-09-05T22:53:10.616758: step 16315, loss 0.00249873, acc 1
2016-09-05T22:53:11.407898: step 16316, loss 0.0278238, acc 0.98
2016-09-05T22:53:12.220617: step 16317, loss 0.0582309, acc 0.98
2016-09-05T22:53:13.047726: step 16318, loss 0.012359, acc 1
2016-09-05T22:53:13.862902: step 16319, loss 0.00672177, acc 1
2016-09-05T22:53:14.731222: step 16320, loss 0.010969, acc 1
2016-09-05T22:53:15.575345: step 16321, loss 0.0117594, acc 1
2016-09-05T22:53:16.385458: step 16322, loss 0.0107974, acc 1
2016-09-05T22:53:17.205095: step 16323, loss 0.0100902, acc 1
2016-09-05T22:53:18.065322: step 16324, loss 0.00599301, acc 1
2016-09-05T22:53:18.886831: step 16325, loss 0.00524566, acc 1
2016-09-05T22:53:19.739196: step 16326, loss 0.0179282, acc 0.98
2016-09-05T22:53:20.561962: step 16327, loss 0.035133, acc 0.98
2016-09-05T22:53:21.375762: step 16328, loss 0.0968658, acc 0.98
2016-09-05T22:53:22.159206: step 16329, loss 0.00778158, acc 1
2016-09-05T22:53:22.945209: step 16330, loss 0.0147247, acc 1
2016-09-05T22:53:23.768914: step 16331, loss 0.00212017, acc 1
2016-09-05T22:53:24.563074: step 16332, loss 0.00261761, acc 1
2016-09-05T22:53:25.365751: step 16333, loss 0.0127962, acc 1
2016-09-05T22:53:26.167902: step 16334, loss 0.00864758, acc 1
2016-09-05T22:53:26.943756: step 16335, loss 0.00352474, acc 1
2016-09-05T22:53:27.765024: step 16336, loss 0.00264983, acc 1
2016-09-05T22:53:28.578440: step 16337, loss 0.0429521, acc 0.96
2016-09-05T22:53:29.365454: step 16338, loss 0.00227014, acc 1
2016-09-05T22:53:30.190874: step 16339, loss 0.0149246, acc 1
2016-09-05T22:53:31.024621: step 16340, loss 0.00495481, acc 1
2016-09-05T22:53:31.808738: step 16341, loss 0.0141032, acc 1
2016-09-05T22:53:32.609657: step 16342, loss 0.00446018, acc 1
2016-09-05T22:53:33.419896: step 16343, loss 0.0116751, acc 1
2016-09-05T22:53:34.217480: step 16344, loss 0.0151012, acc 1
2016-09-05T22:53:35.030163: step 16345, loss 0.00230466, acc 1
2016-09-05T22:53:35.847979: step 16346, loss 0.042617, acc 0.98
2016-09-05T22:53:36.668123: step 16347, loss 0.0361932, acc 0.98
2016-09-05T22:53:37.467298: step 16348, loss 0.011979, acc 1
2016-09-05T22:53:38.277728: step 16349, loss 0.0151321, acc 1
2016-09-05T22:53:39.074758: step 16350, loss 0.146265, acc 0.98
2016-09-05T22:53:39.884459: step 16351, loss 0.00253046, acc 1
2016-09-05T22:53:40.697828: step 16352, loss 0.00569083, acc 1
2016-09-05T22:53:41.493325: step 16353, loss 0.0115725, acc 1
2016-09-05T22:53:42.330867: step 16354, loss 0.00388075, acc 1
2016-09-05T22:53:43.141433: step 16355, loss 0.00678532, acc 1
2016-09-05T22:53:43.915185: step 16356, loss 0.0124295, acc 1
2016-09-05T22:53:44.736552: step 16357, loss 0.00424663, acc 1
2016-09-05T22:53:45.566745: step 16358, loss 0.00749606, acc 1
2016-09-05T22:53:46.376765: step 16359, loss 0.00996239, acc 1
2016-09-05T22:53:47.181409: step 16360, loss 0.015162, acc 1
2016-09-05T22:53:47.965391: step 16361, loss 0.0168452, acc 1
2016-09-05T22:53:48.746099: step 16362, loss 0.0367916, acc 1
2016-09-05T22:53:49.534269: step 16363, loss 0.0504302, acc 0.96
2016-09-05T22:53:50.357497: step 16364, loss 0.00544313, acc 1
2016-09-05T22:53:51.137390: step 16365, loss 0.00368337, acc 1
2016-09-05T22:53:51.930441: step 16366, loss 0.0351792, acc 0.98
2016-09-05T22:53:52.768877: step 16367, loss 0.0067586, acc 1
2016-09-05T22:53:53.564525: step 16368, loss 0.0285457, acc 0.98
2016-09-05T22:53:54.367370: step 16369, loss 0.00320453, acc 1
2016-09-05T22:53:55.184306: step 16370, loss 0.00403684, acc 1
2016-09-05T22:53:55.989404: step 16371, loss 0.00427955, acc 1
2016-09-05T22:53:56.844234: step 16372, loss 0.0261666, acc 0.98
2016-09-05T22:53:57.650571: step 16373, loss 0.00559126, acc 1
2016-09-05T22:53:58.433340: step 16374, loss 0.00293474, acc 1
2016-09-05T22:53:59.228844: step 16375, loss 0.00536219, acc 1
2016-09-05T22:54:00.080416: step 16376, loss 0.00416369, acc 1
2016-09-05T22:54:00.943140: step 16377, loss 0.00672601, acc 1
2016-09-05T22:54:01.785244: step 16378, loss 0.0278854, acc 0.98
2016-09-05T22:54:02.703737: step 16379, loss 0.00620183, acc 1
2016-09-05T22:54:03.575380: step 16380, loss 0.00306286, acc 1
2016-09-05T22:54:04.402590: step 16381, loss 0.0030804, acc 1
2016-09-05T22:54:05.241065: step 16382, loss 0.00530335, acc 1
2016-09-05T22:54:06.160357: step 16383, loss 0.00321991, acc 1
2016-09-05T22:54:06.957882: step 16384, loss 0.00377927, acc 1
2016-09-05T22:54:07.795697: step 16385, loss 0.00307912, acc 1
2016-09-05T22:54:08.687220: step 16386, loss 0.0139081, acc 1
2016-09-05T22:54:09.579926: step 16387, loss 0.023483, acc 1
2016-09-05T22:54:10.518019: step 16388, loss 0.0169861, acc 0.98
2016-09-05T22:54:11.370230: step 16389, loss 0.00306772, acc 1
2016-09-05T22:54:12.256572: step 16390, loss 0.00376592, acc 1
2016-09-05T22:54:13.088624: step 16391, loss 0.0030507, acc 1
2016-09-05T22:54:13.941571: step 16392, loss 0.00729257, acc 1
2016-09-05T22:54:14.806001: step 16393, loss 0.0108296, acc 1
2016-09-05T22:54:15.672147: step 16394, loss 0.0167565, acc 1
2016-09-05T22:54:16.494528: step 16395, loss 0.025264, acc 1
2016-09-05T22:54:17.393895: step 16396, loss 0.00344337, acc 1
2016-09-05T22:54:18.224092: step 16397, loss 0.0137263, acc 1
2016-09-05T22:54:19.050223: step 16398, loss 0.00296973, acc 1
2016-09-05T22:54:20.005365: step 16399, loss 0.00587437, acc 1
2016-09-05T22:54:20.842308: step 16400, loss 0.025054, acc 0.98

Evaluation:
2016-09-05T22:54:24.326928: step 16400, loss 3.5746, acc 0.723

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-16400

2016-09-05T22:54:26.323186: step 16401, loss 0.00289463, acc 1
2016-09-05T22:54:27.181822: step 16402, loss 0.00516337, acc 1
2016-09-05T22:54:27.997053: step 16403, loss 0.0570665, acc 0.98
2016-09-05T22:54:28.788648: step 16404, loss 0.00282573, acc 1
2016-09-05T22:54:29.616153: step 16405, loss 0.0335817, acc 0.98
2016-09-05T22:54:30.491084: step 16406, loss 0.00378297, acc 1
2016-09-05T22:54:31.309141: step 16407, loss 0.0445668, acc 0.98
2016-09-05T22:54:32.239025: step 16408, loss 0.00686796, acc 1
2016-09-05T22:54:33.092915: step 16409, loss 0.0191422, acc 0.98
2016-09-05T22:54:33.922569: step 16410, loss 0.00325542, acc 1
2016-09-05T22:54:34.725666: step 16411, loss 0.00255239, acc 1
2016-09-05T22:54:35.557339: step 16412, loss 0.15877, acc 0.98
2016-09-05T22:54:36.367736: step 16413, loss 0.00248805, acc 1
2016-09-05T22:54:37.169410: step 16414, loss 0.0243058, acc 0.98
2016-09-05T22:54:38.000116: step 16415, loss 0.0124838, acc 1
2016-09-05T22:54:38.845109: step 16416, loss 0.0148235, acc 1
2016-09-05T22:54:39.649762: step 16417, loss 0.00864046, acc 1
2016-09-05T22:54:40.441533: step 16418, loss 0.0590566, acc 0.96
2016-09-05T22:54:41.263180: step 16419, loss 0.0209008, acc 0.98
2016-09-05T22:54:42.052891: step 16420, loss 0.0026185, acc 1
2016-09-05T22:54:42.845712: step 16421, loss 0.084438, acc 0.96
2016-09-05T22:54:43.640004: step 16422, loss 0.0227499, acc 1
2016-09-05T22:54:44.450612: step 16423, loss 0.00211386, acc 1
2016-09-05T22:54:45.276448: step 16424, loss 0.00591455, acc 1
2016-09-05T22:54:46.099869: step 16425, loss 0.00185503, acc 1
2016-09-05T22:54:46.898353: step 16426, loss 0.0451049, acc 0.98
2016-09-05T22:54:47.664311: step 16427, loss 0.00717655, acc 1
2016-09-05T22:54:48.502229: step 16428, loss 0.00213835, acc 1
2016-09-05T22:54:49.316211: step 16429, loss 0.00697008, acc 1
2016-09-05T22:54:50.119609: step 16430, loss 0.0420804, acc 0.98
2016-09-05T22:54:50.952558: step 16431, loss 0.00493478, acc 1
2016-09-05T22:54:51.753039: step 16432, loss 0.00623139, acc 1
2016-09-05T22:54:52.564970: step 16433, loss 0.00262608, acc 1
2016-09-05T22:54:53.397016: step 16434, loss 0.00168515, acc 1
2016-09-05T22:54:54.197164: step 16435, loss 0.00160533, acc 1
2016-09-05T22:54:55.009811: step 16436, loss 0.00990787, acc 1
2016-09-05T22:54:55.865588: step 16437, loss 0.00636657, acc 1
2016-09-05T22:54:56.690749: step 16438, loss 0.0501992, acc 0.96
2016-09-05T22:54:57.500402: step 16439, loss 0.0229393, acc 1
2016-09-05T22:54:58.312140: step 16440, loss 0.0180168, acc 0.98
2016-09-05T22:54:59.113512: step 16441, loss 0.0121141, acc 1
2016-09-05T22:54:59.940041: step 16442, loss 0.0130311, acc 1
2016-09-05T22:55:00.832369: step 16443, loss 0.0839088, acc 0.96
2016-09-05T22:55:01.666776: step 16444, loss 0.0076747, acc 1
2016-09-05T22:55:02.503546: step 16445, loss 0.00512102, acc 1
2016-09-05T22:55:03.350980: step 16446, loss 0.00410999, acc 1
2016-09-05T22:55:04.189063: step 16447, loss 0.029015, acc 0.98
2016-09-05T22:55:04.982364: step 16448, loss 0.0376639, acc 0.98
2016-09-05T22:55:05.834627: step 16449, loss 0.027044, acc 0.98
2016-09-05T22:55:06.646737: step 16450, loss 0.0332425, acc 1
2016-09-05T22:55:07.435179: step 16451, loss 0.0144694, acc 1
2016-09-05T22:55:08.247319: step 16452, loss 0.0277523, acc 0.98
2016-09-05T22:55:09.064913: step 16453, loss 0.00233664, acc 1
2016-09-05T22:55:09.867714: step 16454, loss 0.00799351, acc 1
2016-09-05T22:55:10.671206: step 16455, loss 0.00353187, acc 1
2016-09-05T22:55:11.472496: step 16456, loss 0.0161133, acc 1
2016-09-05T22:55:12.272608: step 16457, loss 0.00950431, acc 1
2016-09-05T22:55:13.041012: step 16458, loss 0.0248836, acc 0.98
2016-09-05T22:55:13.869199: step 16459, loss 0.00204489, acc 1
2016-09-05T22:55:14.651311: step 16460, loss 0.00605628, acc 1
2016-09-05T22:55:15.503997: step 16461, loss 0.00364647, acc 1
2016-09-05T22:55:16.353181: step 16462, loss 0.022308, acc 1
2016-09-05T22:55:17.195423: step 16463, loss 0.00264483, acc 1
2016-09-05T22:55:18.000507: step 16464, loss 0.0250058, acc 1
2016-09-05T22:55:18.850894: step 16465, loss 0.0171589, acc 1
2016-09-05T22:55:19.662588: step 16466, loss 0.0158507, acc 1
2016-09-05T22:55:20.484647: step 16467, loss 0.0264221, acc 0.98
2016-09-05T22:55:21.339015: step 16468, loss 0.028947, acc 0.98
2016-09-05T22:55:22.156673: step 16469, loss 0.0228454, acc 0.98
2016-09-05T22:55:23.015764: step 16470, loss 0.00319291, acc 1
2016-09-05T22:55:23.852343: step 16471, loss 0.0147514, acc 1
2016-09-05T22:55:24.695603: step 16472, loss 0.00716524, acc 1
2016-09-05T22:55:25.483957: step 16473, loss 0.00249994, acc 1
2016-09-05T22:55:26.291674: step 16474, loss 0.00235568, acc 1
2016-09-05T22:55:27.098170: step 16475, loss 0.0038923, acc 1
2016-09-05T22:55:27.898448: step 16476, loss 0.0313128, acc 0.98
2016-09-05T22:55:28.729301: step 16477, loss 0.0100524, acc 1
2016-09-05T22:55:29.564344: step 16478, loss 0.00249581, acc 1
2016-09-05T22:55:30.376651: step 16479, loss 0.0144236, acc 1
2016-09-05T22:55:31.191399: step 16480, loss 0.00309845, acc 1
2016-09-05T22:55:31.992675: step 16481, loss 0.00230635, acc 1
2016-09-05T22:55:32.774269: step 16482, loss 0.0504071, acc 0.98
2016-09-05T22:55:33.578031: step 16483, loss 0.0373666, acc 0.96
2016-09-05T22:55:34.367637: step 16484, loss 0.00225537, acc 1
2016-09-05T22:55:35.147183: step 16485, loss 0.0072754, acc 1
2016-09-05T22:55:36.027228: step 16486, loss 0.0439391, acc 0.98
2016-09-05T22:55:36.850741: step 16487, loss 0.00283415, acc 1
2016-09-05T22:55:37.660579: step 16488, loss 0.00241988, acc 1
2016-09-05T22:55:38.495051: step 16489, loss 0.00210495, acc 1
2016-09-05T22:55:38.936052: step 16490, loss 0.00207524, acc 1
2016-09-05T22:55:39.751516: step 16491, loss 0.017239, acc 1
2016-09-05T22:55:40.571783: step 16492, loss 0.0171884, acc 1
2016-09-05T22:55:41.384396: step 16493, loss 0.00833366, acc 1
2016-09-05T22:55:42.186044: step 16494, loss 0.00224155, acc 1
2016-09-05T22:55:42.971067: step 16495, loss 0.00387253, acc 1
2016-09-05T22:55:43.792648: step 16496, loss 0.0197636, acc 1
2016-09-05T22:55:44.580494: step 16497, loss 0.0032747, acc 1
2016-09-05T22:55:45.417193: step 16498, loss 0.0102796, acc 1
2016-09-05T22:55:46.234668: step 16499, loss 0.0134847, acc 1
2016-09-05T22:55:47.044219: step 16500, loss 0.0022608, acc 1

Evaluation:
2016-09-05T22:55:50.548824: step 16500, loss 2.9794, acc 0.724

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-16500

2016-09-05T22:55:52.385118: step 16501, loss 0.00422502, acc 1
2016-09-05T22:55:53.197551: step 16502, loss 0.00203059, acc 1
2016-09-05T22:55:54.035154: step 16503, loss 0.0228888, acc 1
2016-09-05T22:55:54.857421: step 16504, loss 0.00329691, acc 1
2016-09-05T22:55:55.666182: step 16505, loss 0.00201892, acc 1
2016-09-05T22:55:56.527355: step 16506, loss 0.00200977, acc 1
2016-09-05T22:55:57.372340: step 16507, loss 0.0229738, acc 0.98
2016-09-05T22:55:58.181622: step 16508, loss 0.0446908, acc 0.98
2016-09-05T22:55:59.022721: step 16509, loss 0.0376932, acc 0.98
2016-09-05T22:55:59.839844: step 16510, loss 0.0159547, acc 1
2016-09-05T22:56:00.679577: step 16511, loss 0.0101175, acc 1
2016-09-05T22:56:01.488489: step 16512, loss 0.0100786, acc 1
2016-09-05T22:56:02.322143: step 16513, loss 0.00358375, acc 1
2016-09-05T22:56:03.242318: step 16514, loss 0.008016, acc 1
2016-09-05T22:56:04.077031: step 16515, loss 0.0184572, acc 0.98
2016-09-05T22:56:04.921672: step 16516, loss 0.00207607, acc 1
2016-09-05T22:56:05.780358: step 16517, loss 0.00217596, acc 1
2016-09-05T22:56:06.593425: step 16518, loss 0.00203437, acc 1
2016-09-05T22:56:07.419805: step 16519, loss 0.0112247, acc 1
2016-09-05T22:56:08.322202: step 16520, loss 0.00502613, acc 1
2016-09-05T22:56:09.243639: step 16521, loss 0.0141256, acc 1
2016-09-05T22:56:10.089442: step 16522, loss 0.108221, acc 0.96
2016-09-05T22:56:10.923665: step 16523, loss 0.00290955, acc 1
2016-09-05T22:56:11.733276: step 16524, loss 0.00193884, acc 1
2016-09-05T22:56:12.551800: step 16525, loss 0.00426688, acc 1
2016-09-05T22:56:13.377068: step 16526, loss 0.00793919, acc 1
2016-09-05T22:56:14.182367: step 16527, loss 0.00191172, acc 1
2016-09-05T22:56:14.975070: step 16528, loss 0.0149823, acc 1
2016-09-05T22:56:15.785785: step 16529, loss 0.00183924, acc 1
2016-09-05T22:56:16.596624: step 16530, loss 0.0018146, acc 1
2016-09-05T22:56:17.458163: step 16531, loss 0.0162035, acc 1
2016-09-05T22:56:18.267753: step 16532, loss 0.0113113, acc 1
2016-09-05T22:56:19.102195: step 16533, loss 0.00180294, acc 1
2016-09-05T22:56:19.905446: step 16534, loss 0.00179026, acc 1
2016-09-05T22:56:20.718218: step 16535, loss 0.00690854, acc 1
2016-09-05T22:56:21.535377: step 16536, loss 0.00299097, acc 1
2016-09-05T22:56:22.357366: step 16537, loss 0.0290052, acc 0.98
2016-09-05T22:56:23.160292: step 16538, loss 0.0114086, acc 1
2016-09-05T22:56:23.983267: step 16539, loss 0.00175836, acc 1
2016-09-05T22:56:24.781401: step 16540, loss 0.00191521, acc 1
2016-09-05T22:56:25.605725: step 16541, loss 0.018962, acc 0.98
2016-09-05T22:56:26.425822: step 16542, loss 0.00173088, acc 1
2016-09-05T22:56:27.237490: step 16543, loss 0.0304188, acc 0.98
2016-09-05T22:56:28.049616: step 16544, loss 0.0049739, acc 1
2016-09-05T22:56:28.916366: step 16545, loss 0.00267725, acc 1
2016-09-05T22:56:29.750244: step 16546, loss 0.0135659, acc 1
2016-09-05T22:56:30.579921: step 16547, loss 0.123099, acc 0.98
2016-09-05T22:56:31.422881: step 16548, loss 0.00163346, acc 1
2016-09-05T22:56:32.257493: step 16549, loss 0.00153608, acc 1
2016-09-05T22:56:33.083256: step 16550, loss 0.00369442, acc 1
2016-09-05T22:56:33.979893: step 16551, loss 0.0382927, acc 0.98
2016-09-05T22:56:34.800247: step 16552, loss 0.00144453, acc 1
2016-09-05T22:56:35.588970: step 16553, loss 0.0155411, acc 0.98
2016-09-05T22:56:36.394873: step 16554, loss 0.0137311, acc 1
2016-09-05T22:56:37.205130: step 16555, loss 0.0410724, acc 0.98
2016-09-05T22:56:38.053376: step 16556, loss 0.0530813, acc 0.98
2016-09-05T22:56:38.876137: step 16557, loss 0.047964, acc 0.98
2016-09-05T22:56:39.725291: step 16558, loss 0.00973186, acc 1
2016-09-05T22:56:40.527270: step 16559, loss 0.00245557, acc 1
2016-09-05T22:56:41.327851: step 16560, loss 0.0371908, acc 0.98
2016-09-05T22:56:42.151030: step 16561, loss 0.00232789, acc 1
2016-09-05T22:56:42.948466: step 16562, loss 0.00292171, acc 1
2016-09-05T22:56:43.754086: step 16563, loss 0.00195244, acc 1
2016-09-05T22:56:44.596223: step 16564, loss 0.00228969, acc 1
2016-09-05T22:56:45.414020: step 16565, loss 0.00969277, acc 1
2016-09-05T22:56:46.241442: step 16566, loss 0.0156785, acc 1
2016-09-05T22:56:47.077639: step 16567, loss 0.0219171, acc 0.98
2016-09-05T22:56:47.890568: step 16568, loss 0.00944044, acc 1
2016-09-05T22:56:48.720299: step 16569, loss 0.0169804, acc 0.98
2016-09-05T22:56:49.569476: step 16570, loss 0.0108562, acc 1
2016-09-05T22:56:50.373416: step 16571, loss 0.00264864, acc 1
2016-09-05T22:56:51.167230: step 16572, loss 0.0018693, acc 1
2016-09-05T22:56:52.032381: step 16573, loss 0.00362235, acc 1
2016-09-05T22:56:52.885443: step 16574, loss 0.00264241, acc 1
2016-09-05T22:56:53.699902: step 16575, loss 0.0164054, acc 0.98
2016-09-05T22:56:54.514419: step 16576, loss 0.0120799, acc 1
2016-09-05T22:56:55.322541: step 16577, loss 0.00164018, acc 1
2016-09-05T22:56:56.110669: step 16578, loss 0.00165272, acc 1
2016-09-05T22:56:56.949093: step 16579, loss 0.0192779, acc 1
2016-09-05T22:56:57.751105: step 16580, loss 0.0372936, acc 0.98
2016-09-05T22:56:58.532814: step 16581, loss 0.00807436, acc 1
2016-09-05T22:56:59.332400: step 16582, loss 0.0243655, acc 0.98
2016-09-05T22:57:00.124039: step 16583, loss 0.00177199, acc 1
2016-09-05T22:57:00.965863: step 16584, loss 0.00174508, acc 1
2016-09-05T22:57:01.789025: step 16585, loss 0.0329169, acc 0.98
2016-09-05T22:57:02.613939: step 16586, loss 0.00184997, acc 1
2016-09-05T22:57:03.418662: step 16587, loss 0.0023625, acc 1
2016-09-05T22:57:04.210282: step 16588, loss 0.0442873, acc 0.98
2016-09-05T22:57:05.041834: step 16589, loss 0.115919, acc 0.94
2016-09-05T22:57:05.823789: step 16590, loss 0.0266289, acc 0.98
2016-09-05T22:57:06.623780: step 16591, loss 0.00197092, acc 1
2016-09-05T22:57:07.441420: step 16592, loss 0.0233418, acc 1
2016-09-05T22:57:08.262253: step 16593, loss 0.0720498, acc 0.96
2016-09-05T22:57:09.084113: step 16594, loss 0.019413, acc 0.98
2016-09-05T22:57:09.884356: step 16595, loss 0.00216632, acc 1
2016-09-05T22:57:10.682844: step 16596, loss 0.016344, acc 1
2016-09-05T22:57:11.490917: step 16597, loss 0.016465, acc 0.98
2016-09-05T22:57:12.306470: step 16598, loss 0.017256, acc 0.98
2016-09-05T22:57:13.097692: step 16599, loss 0.0141648, acc 1
2016-09-05T22:57:13.890983: step 16600, loss 0.0355678, acc 0.96

Evaluation:
2016-09-05T22:57:17.389473: step 16600, loss 2.33379, acc 0.719

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-16600

2016-09-05T22:57:19.343738: step 16601, loss 0.0023412, acc 1
2016-09-05T22:57:20.164881: step 16602, loss 0.0113022, acc 1
2016-09-05T22:57:21.013618: step 16603, loss 0.00653725, acc 1
2016-09-05T22:57:21.836165: step 16604, loss 0.0380531, acc 0.96
2016-09-05T22:57:22.635461: step 16605, loss 0.00762057, acc 1
2016-09-05T22:57:23.455354: step 16606, loss 0.00428062, acc 1
2016-09-05T22:57:24.281006: step 16607, loss 0.016808, acc 0.98
2016-09-05T22:57:25.086701: step 16608, loss 0.029292, acc 0.98
2016-09-05T22:57:25.926177: step 16609, loss 0.0151707, acc 1
2016-09-05T22:57:26.727971: step 16610, loss 0.00525941, acc 1
2016-09-05T22:57:27.528219: step 16611, loss 0.0170102, acc 0.98
2016-09-05T22:57:28.413058: step 16612, loss 0.00602441, acc 1
2016-09-05T22:57:29.245278: step 16613, loss 0.0192232, acc 0.98
2016-09-05T22:57:30.017475: step 16614, loss 0.00211834, acc 1
2016-09-05T22:57:30.826761: step 16615, loss 0.020833, acc 0.98
2016-09-05T22:57:31.661978: step 16616, loss 0.0166755, acc 0.98
2016-09-05T22:57:32.476629: step 16617, loss 0.00189543, acc 1
2016-09-05T22:57:33.300914: step 16618, loss 0.00195583, acc 1
2016-09-05T22:57:34.128689: step 16619, loss 0.00178211, acc 1
2016-09-05T22:57:34.929236: step 16620, loss 0.00286362, acc 1
2016-09-05T22:57:35.731978: step 16621, loss 0.0234417, acc 0.98
2016-09-05T22:57:36.541406: step 16622, loss 0.146408, acc 0.98
2016-09-05T22:57:37.325348: step 16623, loss 0.0030704, acc 1
2016-09-05T22:57:38.117582: step 16624, loss 0.00210995, acc 1
2016-09-05T22:57:38.952550: step 16625, loss 0.0479388, acc 0.98
2016-09-05T22:57:39.723592: step 16626, loss 0.0365429, acc 1
2016-09-05T22:57:40.536369: step 16627, loss 0.0157165, acc 1
2016-09-05T22:57:41.336934: step 16628, loss 0.00162606, acc 1
2016-09-05T22:57:42.122227: step 16629, loss 0.00271298, acc 1
2016-09-05T22:57:42.931156: step 16630, loss 0.0178154, acc 0.98
2016-09-05T22:57:43.747682: step 16631, loss 0.00766641, acc 1
2016-09-05T22:57:44.546992: step 16632, loss 0.00266107, acc 1
2016-09-05T22:57:45.362216: step 16633, loss 0.00377794, acc 1
2016-09-05T22:57:46.204223: step 16634, loss 0.00277302, acc 1
2016-09-05T22:57:46.973663: step 16635, loss 0.03633, acc 0.96
2016-09-05T22:57:47.770840: step 16636, loss 0.00185407, acc 1
2016-09-05T22:57:48.618320: step 16637, loss 0.00165058, acc 1
2016-09-05T22:57:49.390836: step 16638, loss 0.00282567, acc 1
2016-09-05T22:57:50.216380: step 16639, loss 0.0183984, acc 0.98
2016-09-05T22:57:51.033976: step 16640, loss 0.00180887, acc 1
2016-09-05T22:57:51.799667: step 16641, loss 0.038934, acc 0.98
2016-09-05T22:57:52.619812: step 16642, loss 0.038478, acc 0.98
2016-09-05T22:57:53.464485: step 16643, loss 0.0145399, acc 1
2016-09-05T22:57:54.243038: step 16644, loss 0.0746779, acc 0.96
2016-09-05T22:57:55.030179: step 16645, loss 0.00168416, acc 1
2016-09-05T22:57:55.857405: step 16646, loss 0.00174738, acc 1
2016-09-05T22:57:56.643645: step 16647, loss 0.00187968, acc 1
2016-09-05T22:57:57.456086: step 16648, loss 0.0121478, acc 1
2016-09-05T22:57:58.326833: step 16649, loss 0.00231004, acc 1
2016-09-05T22:57:59.156031: step 16650, loss 0.00598973, acc 1
2016-09-05T22:58:00.007018: step 16651, loss 0.0157017, acc 1
2016-09-05T22:58:00.852968: step 16652, loss 0.0161403, acc 1
2016-09-05T22:58:01.671924: step 16653, loss 0.00218113, acc 1
2016-09-05T22:58:02.529951: step 16654, loss 0.00436765, acc 1
2016-09-05T22:58:03.411164: step 16655, loss 0.0110246, acc 1
2016-09-05T22:58:04.247352: step 16656, loss 0.0465473, acc 0.96
2016-09-05T22:58:05.063087: step 16657, loss 0.00230723, acc 1
2016-09-05T22:58:05.885576: step 16658, loss 0.0026347, acc 1
2016-09-05T22:58:06.709095: step 16659, loss 0.0287997, acc 0.98
2016-09-05T22:58:07.498531: step 16660, loss 0.0688356, acc 0.98
2016-09-05T22:58:08.328212: step 16661, loss 0.0307622, acc 0.98
2016-09-05T22:58:09.184373: step 16662, loss 0.00316376, acc 1
2016-09-05T22:58:09.985802: step 16663, loss 0.0182903, acc 1
2016-09-05T22:58:10.791251: step 16664, loss 0.002671, acc 1
2016-09-05T22:58:11.615620: step 16665, loss 0.00270794, acc 1
2016-09-05T22:58:12.422469: step 16666, loss 0.0105481, acc 1
2016-09-05T22:58:13.304522: step 16667, loss 0.0100903, acc 1
2016-09-05T22:58:14.224414: step 16668, loss 0.00281552, acc 1
2016-09-05T22:58:15.031933: step 16669, loss 0.002864, acc 1
2016-09-05T22:58:15.891886: step 16670, loss 0.0147495, acc 1
2016-09-05T22:58:16.737710: step 16671, loss 0.0168054, acc 0.98
2016-09-05T22:58:17.597960: step 16672, loss 0.00316545, acc 1
2016-09-05T22:58:18.560371: step 16673, loss 0.0301209, acc 0.98
2016-09-05T22:58:19.377587: step 16674, loss 0.00415526, acc 1
2016-09-05T22:58:20.213856: step 16675, loss 0.00288513, acc 1
2016-09-05T22:58:21.080572: step 16676, loss 0.00418336, acc 1
2016-09-05T22:58:21.908135: step 16677, loss 0.0171096, acc 0.98
2016-09-05T22:58:22.793747: step 16678, loss 0.00285112, acc 1
2016-09-05T22:58:23.623298: step 16679, loss 0.0042041, acc 1
2016-09-05T22:58:24.475079: step 16680, loss 0.0252921, acc 0.98
2016-09-05T22:58:25.304794: step 16681, loss 0.077809, acc 0.98
2016-09-05T22:58:26.195071: step 16682, loss 0.0165855, acc 1
2016-09-05T22:58:27.016938: step 16683, loss 0.00267119, acc 1
2016-09-05T22:58:27.507298: step 16684, loss 0.0321485, acc 1
2016-09-05T22:58:28.348578: step 16685, loss 0.0394124, acc 0.98
2016-09-05T22:58:29.177782: step 16686, loss 0.0025948, acc 1
2016-09-05T22:58:30.042963: step 16687, loss 0.0122869, acc 1
2016-09-05T22:58:30.888422: step 16688, loss 0.0141774, acc 1
2016-09-05T22:58:31.676365: step 16689, loss 0.0416903, acc 0.98
2016-09-05T22:58:32.477361: step 16690, loss 0.0180202, acc 0.98
2016-09-05T22:58:33.305686: step 16691, loss 0.0833701, acc 0.98
2016-09-05T22:58:34.101577: step 16692, loss 0.00273288, acc 1
2016-09-05T22:58:34.911980: step 16693, loss 0.032713, acc 0.98
2016-09-05T22:58:35.712101: step 16694, loss 0.00231248, acc 1
2016-09-05T22:58:36.511344: step 16695, loss 0.0032087, acc 1
2016-09-05T22:58:37.292328: step 16696, loss 0.0353381, acc 0.98
2016-09-05T22:58:38.089579: step 16697, loss 0.0386884, acc 0.98
2016-09-05T22:58:38.881918: step 16698, loss 0.0301288, acc 0.98
2016-09-05T22:58:39.707664: step 16699, loss 0.00195027, acc 1
2016-09-05T22:58:40.507681: step 16700, loss 0.011555, acc 1

Evaluation:
2016-09-05T22:58:43.999363: step 16700, loss 2.48445, acc 0.729

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-16700

2016-09-05T22:58:45.885055: step 16701, loss 0.00872073, acc 1
2016-09-05T22:58:46.706692: step 16702, loss 0.00188866, acc 1
2016-09-05T22:58:47.507322: step 16703, loss 0.0158782, acc 0.98
2016-09-05T22:58:48.304438: step 16704, loss 0.00183557, acc 1
2016-09-05T22:58:49.100747: step 16705, loss 0.0017603, acc 1
2016-09-05T22:58:49.889206: step 16706, loss 0.00667266, acc 1
2016-09-05T22:58:50.690484: step 16707, loss 0.00798921, acc 1
2016-09-05T22:58:51.511457: step 16708, loss 0.00496907, acc 1
2016-09-05T22:58:52.298434: step 16709, loss 0.0198404, acc 0.98
2016-09-05T22:58:53.070760: step 16710, loss 0.00266418, acc 1
2016-09-05T22:58:53.918014: step 16711, loss 0.00172555, acc 1
2016-09-05T22:58:54.677098: step 16712, loss 0.00367272, acc 1
2016-09-05T22:58:55.475704: step 16713, loss 0.00171177, acc 1
2016-09-05T22:58:56.317413: step 16714, loss 0.00484798, acc 1
2016-09-05T22:58:57.099514: step 16715, loss 0.038459, acc 0.96
2016-09-05T22:58:57.941163: step 16716, loss 0.0375307, acc 0.98
2016-09-05T22:58:58.761726: step 16717, loss 0.0072385, acc 1
2016-09-05T22:58:59.561529: step 16718, loss 0.0018426, acc 1
2016-09-05T22:59:00.392825: step 16719, loss 0.0129457, acc 1
2016-09-05T22:59:01.202192: step 16720, loss 0.0657945, acc 0.98
2016-09-05T22:59:02.021480: step 16721, loss 0.0190546, acc 0.98
2016-09-05T22:59:02.838201: step 16722, loss 0.0159256, acc 1
2016-09-05T22:59:03.689207: step 16723, loss 0.00195286, acc 1
2016-09-05T22:59:04.498981: step 16724, loss 0.00154622, acc 1
2016-09-05T22:59:05.306819: step 16725, loss 0.0444285, acc 0.96
2016-09-05T22:59:06.127710: step 16726, loss 0.00157115, acc 1
2016-09-05T22:59:06.959030: step 16727, loss 0.0197388, acc 0.98
2016-09-05T22:59:07.764096: step 16728, loss 0.0134953, acc 1
2016-09-05T22:59:08.615998: step 16729, loss 0.00534996, acc 1
2016-09-05T22:59:09.419649: step 16730, loss 0.0205757, acc 0.98
2016-09-05T22:59:10.221917: step 16731, loss 0.0162782, acc 1
2016-09-05T22:59:11.069696: step 16732, loss 0.0426612, acc 0.98
2016-09-05T22:59:11.904689: step 16733, loss 0.013009, acc 1
2016-09-05T22:59:12.717407: step 16734, loss 0.037225, acc 0.96
2016-09-05T22:59:13.550234: step 16735, loss 0.0247383, acc 0.98
2016-09-05T22:59:14.361288: step 16736, loss 0.00189506, acc 1
2016-09-05T22:59:15.161345: step 16737, loss 0.00154649, acc 1
2016-09-05T22:59:15.987609: step 16738, loss 0.00215814, acc 1
2016-09-05T22:59:16.821994: step 16739, loss 0.00174922, acc 1
2016-09-05T22:59:17.624296: step 16740, loss 0.00199171, acc 1
2016-09-05T22:59:18.432635: step 16741, loss 0.0354219, acc 0.98
2016-09-05T22:59:19.266200: step 16742, loss 0.00179546, acc 1
2016-09-05T22:59:20.052474: step 16743, loss 0.00194817, acc 1
2016-09-05T22:59:20.881439: step 16744, loss 0.00604429, acc 1
2016-09-05T22:59:21.692362: step 16745, loss 0.00450821, acc 1
2016-09-05T22:59:22.480199: step 16746, loss 0.0167765, acc 1
2016-09-05T22:59:23.305775: step 16747, loss 0.0404188, acc 0.96
2016-09-05T22:59:24.138488: step 16748, loss 0.0242532, acc 0.98
2016-09-05T22:59:24.953338: step 16749, loss 0.138006, acc 0.98
2016-09-05T22:59:25.757460: step 16750, loss 0.00809307, acc 1
2016-09-05T22:59:26.594842: step 16751, loss 0.00325391, acc 1
2016-09-05T22:59:27.420833: step 16752, loss 0.00237908, acc 1
2016-09-05T22:59:28.225344: step 16753, loss 0.00341544, acc 1
2016-09-05T22:59:29.069397: step 16754, loss 0.00260601, acc 1
2016-09-05T22:59:29.889968: step 16755, loss 0.00195534, acc 1
2016-09-05T22:59:30.713924: step 16756, loss 0.0122386, acc 1
2016-09-05T22:59:31.560994: step 16757, loss 0.00187765, acc 1
2016-09-05T22:59:32.373073: step 16758, loss 0.00234043, acc 1
2016-09-05T22:59:33.180634: step 16759, loss 0.0019732, acc 1
2016-09-05T22:59:34.008205: step 16760, loss 0.018027, acc 1
2016-09-05T22:59:34.809121: step 16761, loss 0.0402794, acc 0.98
2016-09-05T22:59:35.631758: step 16762, loss 0.00283883, acc 1
2016-09-05T22:59:36.460563: step 16763, loss 0.0303335, acc 0.98
2016-09-05T22:59:37.259407: step 16764, loss 0.0327024, acc 0.98
2016-09-05T22:59:38.063241: step 16765, loss 0.0110094, acc 1
2016-09-05T22:59:38.913396: step 16766, loss 0.00266109, acc 1
2016-09-05T22:59:39.700437: step 16767, loss 0.014719, acc 1
2016-09-05T22:59:40.514415: step 16768, loss 0.00247933, acc 1
2016-09-05T22:59:41.362676: step 16769, loss 0.00474284, acc 1
2016-09-05T22:59:42.210559: step 16770, loss 0.0244804, acc 0.98
2016-09-05T22:59:43.013984: step 16771, loss 0.00547267, acc 1
2016-09-05T22:59:43.837635: step 16772, loss 0.00530339, acc 1
2016-09-05T22:59:44.646497: step 16773, loss 0.00590043, acc 1
2016-09-05T22:59:45.440506: step 16774, loss 0.0128727, acc 1
2016-09-05T22:59:46.235875: step 16775, loss 0.00946538, acc 1
2016-09-05T22:59:47.065392: step 16776, loss 0.0101865, acc 1
2016-09-05T22:59:47.843147: step 16777, loss 0.00193354, acc 1
2016-09-05T22:59:48.642023: step 16778, loss 0.00473197, acc 1
2016-09-05T22:59:49.489537: step 16779, loss 0.0336307, acc 0.98
2016-09-05T22:59:50.266219: step 16780, loss 0.00946965, acc 1
2016-09-05T22:59:51.061690: step 16781, loss 0.0125461, acc 1
2016-09-05T22:59:51.887263: step 16782, loss 0.023536, acc 1
2016-09-05T22:59:52.676434: step 16783, loss 0.0190619, acc 1
2016-09-05T22:59:53.477681: step 16784, loss 0.0253034, acc 0.98
2016-09-05T22:59:54.299884: step 16785, loss 0.00405791, acc 1
2016-09-05T22:59:55.095970: step 16786, loss 0.0339786, acc 0.98
2016-09-05T22:59:55.900129: step 16787, loss 0.0506531, acc 0.96
2016-09-05T22:59:56.744995: step 16788, loss 0.0116063, acc 1
2016-09-05T22:59:57.533652: step 16789, loss 0.00641905, acc 1
2016-09-05T22:59:58.324054: step 16790, loss 0.00241591, acc 1
2016-09-05T22:59:59.134766: step 16791, loss 0.00388328, acc 1
2016-09-05T22:59:59.937068: step 16792, loss 0.00236083, acc 1
2016-09-05T23:00:00.773902: step 16793, loss 0.00234418, acc 1
2016-09-05T23:00:01.583228: step 16794, loss 0.0113085, acc 1
2016-09-05T23:00:02.357265: step 16795, loss 0.0102669, acc 1
2016-09-05T23:00:03.146578: step 16796, loss 0.00206395, acc 1
2016-09-05T23:00:03.990568: step 16797, loss 0.0153023, acc 1
2016-09-05T23:00:04.797741: step 16798, loss 0.0149421, acc 1
2016-09-05T23:00:05.576206: step 16799, loss 0.0165601, acc 0.98
2016-09-05T23:00:06.417320: step 16800, loss 0.116978, acc 0.98

Evaluation:
2016-09-05T23:00:09.896854: step 16800, loss 2.85461, acc 0.719

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-16800

2016-09-05T23:00:11.780043: step 16801, loss 0.0181882, acc 0.98
2016-09-05T23:00:12.613285: step 16802, loss 0.0155193, acc 1
2016-09-05T23:00:13.472859: step 16803, loss 0.00269008, acc 1
2016-09-05T23:00:14.304220: step 16804, loss 0.0178286, acc 1
2016-09-05T23:00:15.119587: step 16805, loss 0.0023839, acc 1
2016-09-05T23:00:15.937507: step 16806, loss 0.0169376, acc 0.98
2016-09-05T23:00:16.741262: step 16807, loss 0.00200481, acc 1
2016-09-05T23:00:17.585788: step 16808, loss 0.00284901, acc 1
2016-09-05T23:00:18.447628: step 16809, loss 0.00645144, acc 1
2016-09-05T23:00:19.278162: step 16810, loss 0.0244046, acc 0.98
2016-09-05T23:00:20.101447: step 16811, loss 0.00465212, acc 1
2016-09-05T23:00:20.905420: step 16812, loss 0.00218849, acc 1
2016-09-05T23:00:21.712778: step 16813, loss 0.00234685, acc 1
2016-09-05T23:00:22.518833: step 16814, loss 0.0210769, acc 0.98
2016-09-05T23:00:23.358831: step 16815, loss 0.0265899, acc 1
2016-09-05T23:00:24.141407: step 16816, loss 0.0155738, acc 1
2016-09-05T23:00:24.960800: step 16817, loss 0.00723951, acc 1
2016-09-05T23:00:25.780217: step 16818, loss 0.00290146, acc 1
2016-09-05T23:00:26.565670: step 16819, loss 0.00644255, acc 1
2016-09-05T23:00:27.337943: step 16820, loss 0.0282486, acc 0.98
2016-09-05T23:00:28.143971: step 16821, loss 0.0144776, acc 1
2016-09-05T23:00:28.932349: step 16822, loss 0.0025383, acc 1
2016-09-05T23:00:29.772169: step 16823, loss 0.0131181, acc 1
2016-09-05T23:00:30.612825: step 16824, loss 0.0791543, acc 0.96
2016-09-05T23:00:31.418109: step 16825, loss 0.0130968, acc 1
2016-09-05T23:00:32.190389: step 16826, loss 0.00230436, acc 1
2016-09-05T23:00:33.027342: step 16827, loss 0.00903398, acc 1
2016-09-05T23:00:33.793814: step 16828, loss 0.00204422, acc 1
2016-09-05T23:00:34.621626: step 16829, loss 0.0334314, acc 0.98
2016-09-05T23:00:35.450868: step 16830, loss 0.0288842, acc 0.98
2016-09-05T23:00:36.243916: step 16831, loss 0.00512307, acc 1
2016-09-05T23:00:37.041417: step 16832, loss 0.0322572, acc 0.98
2016-09-05T23:00:37.863901: step 16833, loss 0.0520956, acc 0.96
2016-09-05T23:00:38.671559: step 16834, loss 0.00217011, acc 1
2016-09-05T23:00:39.441988: step 16835, loss 0.00401397, acc 1
2016-09-05T23:00:40.285785: step 16836, loss 0.00910376, acc 1
2016-09-05T23:00:41.049691: step 16837, loss 0.0196375, acc 1
2016-09-05T23:00:41.901720: step 16838, loss 0.00527644, acc 1
2016-09-05T23:00:42.755019: step 16839, loss 0.0337721, acc 1
2016-09-05T23:00:43.554752: step 16840, loss 0.00189439, acc 1
2016-09-05T23:00:44.356282: step 16841, loss 0.00182221, acc 1
2016-09-05T23:00:45.206477: step 16842, loss 0.00581898, acc 1
2016-09-05T23:00:46.012631: step 16843, loss 0.00185697, acc 1
2016-09-05T23:00:46.838578: step 16844, loss 0.022664, acc 1
2016-09-05T23:00:47.706662: step 16845, loss 0.00684085, acc 1
2016-09-05T23:00:48.553724: step 16846, loss 0.00195765, acc 1
2016-09-05T23:00:49.367429: step 16847, loss 0.00706938, acc 1
2016-09-05T23:00:50.202577: step 16848, loss 0.0131079, acc 1
2016-09-05T23:00:51.032228: step 16849, loss 0.00205722, acc 1
2016-09-05T23:00:51.874267: step 16850, loss 0.0638163, acc 0.94
2016-09-05T23:00:52.702212: step 16851, loss 0.00189757, acc 1
2016-09-05T23:00:53.525315: step 16852, loss 0.00617721, acc 1
2016-09-05T23:00:54.310872: step 16853, loss 0.00196802, acc 1
2016-09-05T23:00:55.125084: step 16854, loss 0.00667807, acc 1
2016-09-05T23:00:55.959726: step 16855, loss 0.0155218, acc 1
2016-09-05T23:00:56.757102: step 16856, loss 0.00192839, acc 1
2016-09-05T23:00:57.565115: step 16857, loss 0.017462, acc 0.98
2016-09-05T23:00:58.360802: step 16858, loss 0.126599, acc 0.98
2016-09-05T23:00:59.178065: step 16859, loss 0.00216736, acc 1
2016-09-05T23:00:59.988018: step 16860, loss 0.0118606, acc 1
2016-09-05T23:01:00.841131: step 16861, loss 0.00185873, acc 1
2016-09-05T23:01:01.650964: step 16862, loss 0.00201092, acc 1
2016-09-05T23:01:02.485637: step 16863, loss 0.0199614, acc 1
2016-09-05T23:01:03.342467: step 16864, loss 0.0207273, acc 0.98
2016-09-05T23:01:04.139716: step 16865, loss 0.0202523, acc 1
2016-09-05T23:01:04.966507: step 16866, loss 0.0181268, acc 0.98
2016-09-05T23:01:05.828651: step 16867, loss 0.0108185, acc 1
2016-09-05T23:01:06.661459: step 16868, loss 0.00341559, acc 1
2016-09-05T23:01:07.493234: step 16869, loss 0.0129998, acc 1
2016-09-05T23:01:08.348931: step 16870, loss 0.00425355, acc 1
2016-09-05T23:01:09.181327: step 16871, loss 0.0178549, acc 1
2016-09-05T23:01:09.986676: step 16872, loss 0.0255684, acc 0.98
2016-09-05T23:01:10.825885: step 16873, loss 0.00333627, acc 1
2016-09-05T23:01:11.622151: step 16874, loss 0.0344802, acc 0.98
2016-09-05T23:01:12.444491: step 16875, loss 0.00216884, acc 1
2016-09-05T23:01:13.270981: step 16876, loss 0.00262808, acc 1
2016-09-05T23:01:14.121690: step 16877, loss 0.0196463, acc 0.98
2016-09-05T23:01:14.537061: step 16878, loss 0.105218, acc 0.916667
2016-09-05T23:01:15.345983: step 16879, loss 0.0293663, acc 0.98
2016-09-05T23:01:16.145952: step 16880, loss 0.0370234, acc 0.98
2016-09-05T23:01:16.947346: step 16881, loss 0.00552567, acc 1
2016-09-05T23:01:17.759865: step 16882, loss 0.00751431, acc 1
2016-09-05T23:01:18.547429: step 16883, loss 0.0351795, acc 0.98
2016-09-05T23:01:19.327506: step 16884, loss 0.0723474, acc 0.96
2016-09-05T23:01:20.146955: step 16885, loss 0.00381385, acc 1
2016-09-05T23:01:20.925960: step 16886, loss 0.00321801, acc 1
2016-09-05T23:01:21.751843: step 16887, loss 0.00207665, acc 1
2016-09-05T23:01:22.553270: step 16888, loss 0.00943244, acc 1
2016-09-05T23:01:23.370915: step 16889, loss 0.00487385, acc 1
2016-09-05T23:01:24.188153: step 16890, loss 0.0153124, acc 1
2016-09-05T23:01:25.012592: step 16891, loss 0.0205312, acc 1
2016-09-05T23:01:25.812254: step 16892, loss 0.0120994, acc 1
2016-09-05T23:01:26.634513: step 16893, loss 0.0547267, acc 0.98
2016-09-05T23:01:27.473587: step 16894, loss 0.00195178, acc 1
2016-09-05T23:01:28.313038: step 16895, loss 0.00524897, acc 1
2016-09-05T23:01:29.110895: step 16896, loss 0.00204873, acc 1
2016-09-05T23:01:29.954492: step 16897, loss 0.0175167, acc 0.98
2016-09-05T23:01:30.802575: step 16898, loss 0.00263784, acc 1
2016-09-05T23:01:31.642465: step 16899, loss 0.00358892, acc 1
2016-09-05T23:01:32.482835: step 16900, loss 0.0172624, acc 0.98

Evaluation:
2016-09-05T23:01:35.965292: step 16900, loss 2.89457, acc 0.713

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-16900

2016-09-05T23:01:37.912451: step 16901, loss 0.00221397, acc 1
2016-09-05T23:01:38.745780: step 16902, loss 0.102697, acc 0.98
2016-09-05T23:01:39.561403: step 16903, loss 0.00757887, acc 1
2016-09-05T23:01:40.341963: step 16904, loss 0.00218948, acc 1
2016-09-05T23:01:41.168167: step 16905, loss 0.00215167, acc 1
2016-09-05T23:01:41.969726: step 16906, loss 0.00240752, acc 1
2016-09-05T23:01:42.844027: step 16907, loss 0.00341263, acc 1
2016-09-05T23:01:43.633708: step 16908, loss 0.00232916, acc 1
2016-09-05T23:01:44.460195: step 16909, loss 0.00631506, acc 1
2016-09-05T23:01:45.231444: step 16910, loss 0.0354779, acc 1
2016-09-05T23:01:46.057473: step 16911, loss 0.00769533, acc 1
2016-09-05T23:01:46.877506: step 16912, loss 0.0148661, acc 1
2016-09-05T23:01:47.677811: step 16913, loss 0.00361338, acc 1
2016-09-05T23:01:48.503157: step 16914, loss 0.0140717, acc 1
2016-09-05T23:01:49.323012: step 16915, loss 0.00633469, acc 1
2016-09-05T23:01:50.133449: step 16916, loss 0.0195343, acc 1
2016-09-05T23:01:50.967957: step 16917, loss 0.00277052, acc 1
2016-09-05T23:01:51.809696: step 16918, loss 0.00250546, acc 1
2016-09-05T23:01:52.586290: step 16919, loss 0.0240213, acc 1
2016-09-05T23:01:53.382231: step 16920, loss 0.00363581, acc 1
2016-09-05T23:01:54.209452: step 16921, loss 0.00323187, acc 1
2016-09-05T23:01:55.027814: step 16922, loss 0.0158262, acc 1
2016-09-05T23:01:55.813874: step 16923, loss 0.00224098, acc 1
2016-09-05T23:01:56.648489: step 16924, loss 0.045976, acc 0.96
2016-09-05T23:01:57.409073: step 16925, loss 0.0121109, acc 1
2016-09-05T23:01:58.237415: step 16926, loss 0.0100066, acc 1
2016-09-05T23:01:59.059498: step 16927, loss 0.00228721, acc 1
2016-09-05T23:01:59.820573: step 16928, loss 0.0334582, acc 0.98
2016-09-05T23:02:00.634986: step 16929, loss 0.00256701, acc 1
2016-09-05T23:02:01.447413: step 16930, loss 0.00224473, acc 1
2016-09-05T23:02:02.242652: step 16931, loss 0.00223949, acc 1
2016-09-05T23:02:03.051236: step 16932, loss 0.0021932, acc 1
2016-09-05T23:02:03.884753: step 16933, loss 0.0164057, acc 0.98
2016-09-05T23:02:04.681009: step 16934, loss 0.0102545, acc 1
2016-09-05T23:02:05.470930: step 16935, loss 0.0097861, acc 1
2016-09-05T23:02:06.301289: step 16936, loss 0.0021349, acc 1
2016-09-05T23:02:07.075877: step 16937, loss 0.010281, acc 1
2016-09-05T23:02:07.885721: step 16938, loss 0.0059333, acc 1
2016-09-05T23:02:08.691338: step 16939, loss 0.00317684, acc 1
2016-09-05T23:02:09.483310: step 16940, loss 0.0146677, acc 1
2016-09-05T23:02:10.289635: step 16941, loss 0.0930478, acc 0.96
2016-09-05T23:02:11.104232: step 16942, loss 0.0256076, acc 1
2016-09-05T23:02:11.889249: step 16943, loss 0.0155378, acc 1
2016-09-05T23:02:12.698007: step 16944, loss 0.00345992, acc 1
2016-09-05T23:02:13.537443: step 16945, loss 0.00761318, acc 1
2016-09-05T23:02:14.363604: step 16946, loss 0.00421348, acc 1
2016-09-05T23:02:15.157619: step 16947, loss 0.00219902, acc 1
2016-09-05T23:02:15.962292: step 16948, loss 0.100813, acc 0.98
2016-09-05T23:02:16.742145: step 16949, loss 0.0194791, acc 0.98
2016-09-05T23:02:17.536073: step 16950, loss 0.0169426, acc 1
2016-09-05T23:02:18.361056: step 16951, loss 0.00198848, acc 1
2016-09-05T23:02:19.146311: step 16952, loss 0.00252249, acc 1
2016-09-05T23:02:19.953425: step 16953, loss 0.00184791, acc 1
2016-09-05T23:02:20.840503: step 16954, loss 0.0533122, acc 0.96
2016-09-05T23:02:21.663138: step 16955, loss 0.00201985, acc 1
2016-09-05T23:02:22.466368: step 16956, loss 0.0318547, acc 0.98
2016-09-05T23:02:23.315716: step 16957, loss 0.0440107, acc 0.98
2016-09-05T23:02:24.136830: step 16958, loss 0.0198642, acc 0.98
2016-09-05T23:02:24.944420: step 16959, loss 0.022751, acc 0.98
2016-09-05T23:02:25.774460: step 16960, loss 0.0114936, acc 1
2016-09-05T23:02:26.591573: step 16961, loss 0.019854, acc 0.98
2016-09-05T23:02:27.400851: step 16962, loss 0.00171105, acc 1
2016-09-05T23:02:28.237102: step 16963, loss 0.00603283, acc 1
2016-09-05T23:02:29.040816: step 16964, loss 0.00193666, acc 1
2016-09-05T23:02:29.852505: step 16965, loss 0.00174101, acc 1
2016-09-05T23:02:30.667764: step 16966, loss 0.0125435, acc 1
2016-09-05T23:02:31.471946: step 16967, loss 0.0154361, acc 1
2016-09-05T23:02:32.268048: step 16968, loss 0.0278539, acc 0.98
2016-09-05T23:02:33.093723: step 16969, loss 0.0508206, acc 0.98
2016-09-05T23:02:33.885573: step 16970, loss 0.0140347, acc 1
2016-09-05T23:02:34.682069: step 16971, loss 0.002934, acc 1
2016-09-05T23:02:35.499203: step 16972, loss 0.00186955, acc 1
2016-09-05T23:02:36.312674: step 16973, loss 0.0317791, acc 0.96
2016-09-05T23:02:37.131565: step 16974, loss 0.00997957, acc 1
2016-09-05T23:02:37.984470: step 16975, loss 0.00169775, acc 1
2016-09-05T23:02:38.793717: step 16976, loss 0.0493575, acc 0.98
2016-09-05T23:02:39.617425: step 16977, loss 0.00224267, acc 1
2016-09-05T23:02:40.451399: step 16978, loss 0.0245345, acc 0.98
2016-09-05T23:02:41.253185: step 16979, loss 0.00489805, acc 1
2016-09-05T23:02:42.077218: step 16980, loss 0.0175399, acc 1
2016-09-05T23:02:42.876711: step 16981, loss 0.00836642, acc 1
2016-09-05T23:02:43.710991: step 16982, loss 0.00241497, acc 1
2016-09-05T23:02:44.543928: step 16983, loss 0.00578042, acc 1
2016-09-05T23:02:45.365233: step 16984, loss 0.00182189, acc 1
2016-09-05T23:02:46.221626: step 16985, loss 0.0163166, acc 1
2016-09-05T23:02:47.044970: step 16986, loss 0.00180346, acc 1
2016-09-05T23:02:47.837499: step 16987, loss 0.00261671, acc 1
2016-09-05T23:02:48.685562: step 16988, loss 0.00189161, acc 1
2016-09-05T23:02:49.506361: step 16989, loss 0.0461166, acc 0.96
2016-09-05T23:02:50.330022: step 16990, loss 0.0018347, acc 1
2016-09-05T23:02:51.172538: step 16991, loss 0.0410741, acc 0.98
2016-09-05T23:02:51.983778: step 16992, loss 0.021084, acc 1
2016-09-05T23:02:52.794967: step 16993, loss 0.00188641, acc 1
2016-09-05T23:02:53.637497: step 16994, loss 0.0546695, acc 0.98
2016-09-05T23:02:54.458564: step 16995, loss 0.00196538, acc 1
2016-09-05T23:02:55.268560: step 16996, loss 0.032536, acc 0.96
2016-09-05T23:02:56.107493: step 16997, loss 0.00191859, acc 1
2016-09-05T23:02:56.940406: step 16998, loss 0.0263, acc 1
2016-09-05T23:02:57.733990: step 16999, loss 0.0332518, acc 0.98
2016-09-05T23:02:58.571513: step 17000, loss 0.0192704, acc 0.98

Evaluation:
2016-09-05T23:03:02.097669: step 17000, loss 2.78235, acc 0.706

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-17000

2016-09-05T23:03:03.917435: step 17001, loss 0.00174761, acc 1
2016-09-05T23:03:04.762412: step 17002, loss 0.00174619, acc 1
2016-09-05T23:03:05.614500: step 17003, loss 0.0246568, acc 0.98
2016-09-05T23:03:06.433478: step 17004, loss 0.00985719, acc 1
2016-09-05T23:03:07.239056: step 17005, loss 0.00281279, acc 1
2016-09-05T23:03:08.032533: step 17006, loss 0.00189384, acc 1
2016-09-05T23:03:08.835614: step 17007, loss 0.00193798, acc 1
2016-09-05T23:03:09.683132: step 17008, loss 0.0634205, acc 0.96
2016-09-05T23:03:10.499479: step 17009, loss 0.0428052, acc 0.96
2016-09-05T23:03:11.294834: step 17010, loss 0.0513725, acc 0.98
2016-09-05T23:03:12.112614: step 17011, loss 0.00161223, acc 1
2016-09-05T23:03:12.930862: step 17012, loss 0.0287508, acc 0.98
2016-09-05T23:03:13.735298: step 17013, loss 0.103995, acc 0.96
2016-09-05T23:03:14.560762: step 17014, loss 0.00880955, acc 1
2016-09-05T23:03:15.388820: step 17015, loss 0.154224, acc 0.94
2016-09-05T23:03:16.215319: step 17016, loss 0.00142876, acc 1
2016-09-05T23:03:17.021064: step 17017, loss 0.0264406, acc 0.98
2016-09-05T23:03:17.842244: step 17018, loss 0.0162493, acc 0.98
2016-09-05T23:03:18.639043: step 17019, loss 0.0059729, acc 1
2016-09-05T23:03:19.431305: step 17020, loss 0.0174475, acc 1
2016-09-05T23:03:20.245410: step 17021, loss 0.0256693, acc 0.98
2016-09-05T23:03:21.024256: step 17022, loss 0.0239624, acc 1
2016-09-05T23:03:21.857610: step 17023, loss 0.017211, acc 1
2016-09-05T23:03:22.685968: step 17024, loss 0.00696473, acc 1
2016-09-05T23:03:23.499967: step 17025, loss 0.0150494, acc 1
2016-09-05T23:03:24.314537: step 17026, loss 0.00499241, acc 1
2016-09-05T23:03:25.130983: step 17027, loss 0.0187417, acc 1
2016-09-05T23:03:25.962176: step 17028, loss 0.00441687, acc 1
2016-09-05T23:03:26.785435: step 17029, loss 0.0170946, acc 0.98
2016-09-05T23:03:27.613723: step 17030, loss 0.0222664, acc 1
2016-09-05T23:03:28.426067: step 17031, loss 0.010767, acc 1
2016-09-05T23:03:29.256785: step 17032, loss 0.0172648, acc 1
2016-09-05T23:03:30.080064: step 17033, loss 0.0143455, acc 1
2016-09-05T23:03:30.926476: step 17034, loss 0.00733617, acc 1
2016-09-05T23:03:31.725425: step 17035, loss 0.00269408, acc 1
2016-09-05T23:03:32.571795: step 17036, loss 0.0226889, acc 1
2016-09-05T23:03:33.500262: step 17037, loss 0.00332449, acc 1
2016-09-05T23:03:34.471171: step 17038, loss 0.00293479, acc 1
2016-09-05T23:03:35.311279: step 17039, loss 0.0111028, acc 1
2016-09-05T23:03:36.199929: step 17040, loss 0.0169803, acc 1
2016-09-05T23:03:37.140651: step 17041, loss 0.00621184, acc 1
2016-09-05T23:03:37.965211: step 17042, loss 0.00344454, acc 1
2016-09-05T23:03:39.337004: step 17043, loss 0.0704725, acc 0.98
2016-09-05T23:03:40.247603: step 17044, loss 0.060297, acc 0.98
2016-09-05T23:03:41.142598: step 17045, loss 0.00313424, acc 1
2016-09-05T23:03:42.117949: step 17046, loss 0.01818, acc 0.98
2016-09-05T23:03:42.930580: step 17047, loss 0.00360905, acc 1
2016-09-05T23:03:43.755031: step 17048, loss 0.0410592, acc 0.96
2016-09-05T23:03:44.676917: step 17049, loss 0.0433877, acc 0.98
2016-09-05T23:03:45.570158: step 17050, loss 0.0164288, acc 1
2016-09-05T23:03:46.433886: step 17051, loss 0.00276711, acc 1
2016-09-05T23:03:47.250755: step 17052, loss 0.0769509, acc 0.98
2016-09-05T23:03:48.245941: step 17053, loss 0.0166, acc 0.98
2016-09-05T23:03:49.052999: step 17054, loss 0.0359578, acc 0.96
2016-09-05T23:03:50.084533: step 17055, loss 0.00270638, acc 1
2016-09-05T23:03:50.998905: step 17056, loss 0.0026216, acc 1
2016-09-05T23:03:51.882433: step 17057, loss 0.00301679, acc 1
2016-09-05T23:03:52.780557: step 17058, loss 0.0285705, acc 0.98
2016-09-05T23:03:53.692746: step 17059, loss 0.0220943, acc 0.98
2016-09-05T23:03:54.758400: step 17060, loss 0.00557571, acc 1
2016-09-05T23:03:55.573136: step 17061, loss 0.0341048, acc 0.98
2016-09-05T23:03:56.485926: step 17062, loss 0.00571799, acc 1
2016-09-05T23:03:57.345753: step 17063, loss 0.0199775, acc 1
2016-09-05T23:03:58.238431: step 17064, loss 0.015314, acc 1
2016-09-05T23:03:59.070906: step 17065, loss 0.0369774, acc 0.98
2016-09-05T23:03:59.943251: step 17066, loss 0.00832385, acc 1
2016-09-05T23:04:00.799534: step 17067, loss 0.00480614, acc 1
2016-09-05T23:04:01.759691: step 17068, loss 0.0152234, acc 1
2016-09-05T23:04:02.619007: step 17069, loss 0.0540766, acc 0.98
2016-09-05T23:04:03.441764: step 17070, loss 0.0162361, acc 1
2016-09-05T23:04:04.272529: step 17071, loss 0.00318856, acc 1
2016-09-05T23:04:04.707250: step 17072, loss 0.0516808, acc 1
2016-09-05T23:04:05.513245: step 17073, loss 0.00741685, acc 1
2016-09-05T23:04:06.493396: step 17074, loss 0.0171895, acc 0.98
2016-09-05T23:04:07.320781: step 17075, loss 0.0213915, acc 0.98
2016-09-05T23:04:08.218462: step 17076, loss 0.00267299, acc 1
2016-09-05T23:04:09.051804: step 17077, loss 0.0154797, acc 1
2016-09-05T23:04:09.921924: step 17078, loss 0.0268344, acc 0.98
2016-09-05T23:04:10.734562: step 17079, loss 0.00273904, acc 1
2016-09-05T23:04:11.637094: step 17080, loss 0.0129454, acc 1
2016-09-05T23:04:12.571748: step 17081, loss 0.0027953, acc 1
2016-09-05T23:04:13.459570: step 17082, loss 0.00392999, acc 1
2016-09-05T23:04:14.312367: step 17083, loss 0.00999753, acc 1
2016-09-05T23:04:15.149649: step 17084, loss 0.0561488, acc 0.96
2016-09-05T23:04:15.979333: step 17085, loss 0.0268532, acc 1
2016-09-05T23:04:16.831657: step 17086, loss 0.0145777, acc 1
2016-09-05T23:04:17.668776: step 17087, loss 0.00273455, acc 1
2016-09-05T23:04:18.491245: step 17088, loss 0.0225277, acc 0.98
2016-09-05T23:04:19.361420: step 17089, loss 0.0244416, acc 0.98
2016-09-05T23:04:20.195939: step 17090, loss 0.003288, acc 1
2016-09-05T23:04:20.986952: step 17091, loss 0.0314878, acc 0.98
2016-09-05T23:04:21.901669: step 17092, loss 0.0906988, acc 0.96
2016-09-05T23:04:22.804168: step 17093, loss 0.00251723, acc 1
2016-09-05T23:04:23.615908: step 17094, loss 0.00320504, acc 1
2016-09-05T23:04:24.480463: step 17095, loss 0.00251481, acc 1
2016-09-05T23:04:25.309348: step 17096, loss 0.0036053, acc 1
2016-09-05T23:04:26.173209: step 17097, loss 0.00275724, acc 1
2016-09-05T23:04:26.990190: step 17098, loss 0.00232417, acc 1
2016-09-05T23:04:27.848940: step 17099, loss 0.00782519, acc 1
2016-09-05T23:04:28.645213: step 17100, loss 0.00240019, acc 1

Evaluation:
2016-09-05T23:04:32.141131: step 17100, loss 3.06007, acc 0.728

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-17100

2016-09-05T23:04:34.213582: step 17101, loss 0.0120009, acc 1
2016-09-05T23:04:35.013709: step 17102, loss 0.00633943, acc 1
2016-09-05T23:04:35.926593: step 17103, loss 0.00236511, acc 1
2016-09-05T23:04:36.724395: step 17104, loss 0.0177177, acc 1
2016-09-05T23:04:37.612370: step 17105, loss 0.022368, acc 0.98
2016-09-05T23:04:38.429549: step 17106, loss 0.00228977, acc 1
2016-09-05T23:04:39.254446: step 17107, loss 0.0149829, acc 1
2016-09-05T23:04:40.100941: step 17108, loss 0.0197872, acc 0.98
2016-09-05T23:04:40.981636: step 17109, loss 0.0071481, acc 1
2016-09-05T23:04:41.795594: step 17110, loss 0.00688723, acc 1
2016-09-05T23:04:42.677001: step 17111, loss 0.00222594, acc 1
2016-09-05T23:04:43.512760: step 17112, loss 0.00251193, acc 1
2016-09-05T23:04:44.307726: step 17113, loss 0.0598581, acc 0.98
2016-09-05T23:04:45.107493: step 17114, loss 0.0021191, acc 1
2016-09-05T23:04:45.934456: step 17115, loss 0.0251317, acc 0.98
2016-09-05T23:04:46.722898: step 17116, loss 0.00216307, acc 1
2016-09-05T23:04:47.536426: step 17117, loss 0.0151263, acc 1
2016-09-05T23:04:48.358234: step 17118, loss 0.0862051, acc 0.96
2016-09-05T23:04:49.158493: step 17119, loss 0.00287768, acc 1
2016-09-05T23:04:49.963425: step 17120, loss 0.00423396, acc 1
2016-09-05T23:04:50.787202: step 17121, loss 0.00226379, acc 1
2016-09-05T23:04:51.601909: step 17122, loss 0.147352, acc 0.92
2016-09-05T23:04:52.396855: step 17123, loss 0.0242656, acc 0.98
2016-09-05T23:04:53.187918: step 17124, loss 0.00228256, acc 1
2016-09-05T23:04:54.053084: step 17125, loss 0.00230342, acc 1
2016-09-05T23:04:54.897600: step 17126, loss 0.00237766, acc 1
2016-09-05T23:04:55.812475: step 17127, loss 0.00264542, acc 1
2016-09-05T23:04:56.668145: step 17128, loss 0.00487047, acc 1
2016-09-05T23:04:57.477188: step 17129, loss 0.0314182, acc 0.98
2016-09-05T23:04:58.333736: step 17130, loss 0.00670314, acc 1
2016-09-05T23:04:59.224128: step 17131, loss 0.00243073, acc 1
2016-09-05T23:05:00.020588: step 17132, loss 0.0232636, acc 0.98
2016-09-05T23:05:00.872785: step 17133, loss 0.103459, acc 0.96
2016-09-05T23:05:01.767097: step 17134, loss 0.00229959, acc 1
2016-09-05T23:05:02.562833: step 17135, loss 0.00757494, acc 1
2016-09-05T23:05:03.366149: step 17136, loss 0.00239851, acc 1
2016-09-05T23:05:04.174915: step 17137, loss 0.0162808, acc 1
2016-09-05T23:05:04.990005: step 17138, loss 0.00248206, acc 1
2016-09-05T23:05:05.829491: step 17139, loss 0.0261314, acc 0.98
2016-09-05T23:05:06.651173: step 17140, loss 0.00295224, acc 1
2016-09-05T23:05:07.456554: step 17141, loss 0.00340951, acc 1
2016-09-05T23:05:08.262685: step 17142, loss 0.0156419, acc 1
2016-09-05T23:05:09.101608: step 17143, loss 0.00652317, acc 1
2016-09-05T23:05:09.915427: step 17144, loss 0.0167227, acc 0.98
2016-09-05T23:05:10.721133: step 17145, loss 0.00280902, acc 1
2016-09-05T23:05:11.558998: step 17146, loss 0.0217994, acc 0.98
2016-09-05T23:05:12.365759: step 17147, loss 0.00242384, acc 1
2016-09-05T23:05:13.169069: step 17148, loss 0.0213922, acc 1
2016-09-05T23:05:14.011598: step 17149, loss 0.0283587, acc 0.98
2016-09-05T23:05:14.838417: step 17150, loss 0.0020757, acc 1
2016-09-05T23:05:15.655008: step 17151, loss 0.038437, acc 0.98
2016-09-05T23:05:16.483496: step 17152, loss 0.044992, acc 0.96
2016-09-05T23:05:17.302493: step 17153, loss 0.00581628, acc 1
2016-09-05T23:05:18.133493: step 17154, loss 0.00210316, acc 1
2016-09-05T23:05:18.958395: step 17155, loss 0.0134564, acc 1
2016-09-05T23:05:19.770501: step 17156, loss 0.00344662, acc 1
2016-09-05T23:05:20.576845: step 17157, loss 0.0128226, acc 1
2016-09-05T23:05:21.391673: step 17158, loss 0.0484828, acc 0.98
2016-09-05T23:05:22.214439: step 17159, loss 0.00227679, acc 1
2016-09-05T23:05:23.011780: step 17160, loss 0.00282997, acc 1
2016-09-05T23:05:23.837638: step 17161, loss 0.00196715, acc 1
2016-09-05T23:05:24.664318: step 17162, loss 0.0302907, acc 0.98
2016-09-05T23:05:25.434135: step 17163, loss 0.0171525, acc 0.98
2016-09-05T23:05:26.238451: step 17164, loss 0.00808756, acc 1
2016-09-05T23:05:27.042476: step 17165, loss 0.0295059, acc 0.98
2016-09-05T23:05:27.818226: step 17166, loss 0.0221203, acc 0.98
2016-09-05T23:05:28.640004: step 17167, loss 0.00846036, acc 1
2016-09-05T23:05:29.453100: step 17168, loss 0.112637, acc 0.94
2016-09-05T23:05:30.248594: step 17169, loss 0.0212908, acc 0.98
2016-09-05T23:05:31.064890: step 17170, loss 0.0393755, acc 0.98
2016-09-05T23:05:31.859246: step 17171, loss 0.00971225, acc 1
2016-09-05T23:05:32.646660: step 17172, loss 0.019367, acc 0.98
2016-09-05T23:05:33.476617: step 17173, loss 0.0199094, acc 1
2016-09-05T23:05:34.290339: step 17174, loss 0.00654316, acc 1
2016-09-05T23:05:35.080207: step 17175, loss 0.00828156, acc 1
2016-09-05T23:05:35.880176: step 17176, loss 0.0191174, acc 1
2016-09-05T23:05:36.700535: step 17177, loss 0.00288122, acc 1
2016-09-05T23:05:37.486261: step 17178, loss 0.0133583, acc 1
2016-09-05T23:05:38.289807: step 17179, loss 0.024195, acc 1
2016-09-05T23:05:39.104319: step 17180, loss 0.0116308, acc 1
2016-09-05T23:05:39.873283: step 17181, loss 0.0187102, acc 0.98
2016-09-05T23:05:40.686597: step 17182, loss 0.0120204, acc 1
2016-09-05T23:05:41.463160: step 17183, loss 0.0154457, acc 1
2016-09-05T23:05:42.272140: step 17184, loss 0.00184914, acc 1
2016-09-05T23:05:43.100318: step 17185, loss 0.0115753, acc 1
2016-09-05T23:05:43.912258: step 17186, loss 0.0208171, acc 0.98
2016-09-05T23:05:44.733689: step 17187, loss 0.0116624, acc 1
2016-09-05T23:05:45.556540: step 17188, loss 0.00998845, acc 1
2016-09-05T23:05:46.403256: step 17189, loss 0.00564427, acc 1
2016-09-05T23:05:47.228564: step 17190, loss 0.00902785, acc 1
2016-09-05T23:05:48.040499: step 17191, loss 0.0708527, acc 0.98
2016-09-05T23:05:48.844966: step 17192, loss 0.0205151, acc 0.98
2016-09-05T23:05:49.643680: step 17193, loss 0.0156628, acc 1
2016-09-05T23:05:50.441461: step 17194, loss 0.00781567, acc 1
2016-09-05T23:05:51.248822: step 17195, loss 0.0177479, acc 0.98
2016-09-05T23:05:52.031860: step 17196, loss 0.0153292, acc 1
2016-09-05T23:05:52.830511: step 17197, loss 0.00216749, acc 1
2016-09-05T23:05:53.657664: step 17198, loss 0.00558059, acc 1
2016-09-05T23:05:54.447725: step 17199, loss 0.0158936, acc 1
2016-09-05T23:05:55.246859: step 17200, loss 0.00568229, acc 1

Evaluation:
2016-09-05T23:05:58.745956: step 17200, loss 3.16278, acc 0.714

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-17200

2016-09-05T23:06:00.599048: step 17201, loss 0.00327091, acc 1
2016-09-05T23:06:01.430852: step 17202, loss 0.0378092, acc 0.98
2016-09-05T23:06:02.263344: step 17203, loss 0.0212699, acc 1
2016-09-05T23:06:03.069961: step 17204, loss 0.00547562, acc 1
2016-09-05T23:06:03.873674: step 17205, loss 0.00228223, acc 1
2016-09-05T23:06:04.730956: step 17206, loss 0.0167165, acc 1
2016-09-05T23:06:05.559578: step 17207, loss 0.058666, acc 0.96
2016-09-05T23:06:06.373955: step 17208, loss 0.00239161, acc 1
2016-09-05T23:06:07.206724: step 17209, loss 0.00333946, acc 1
2016-09-05T23:06:07.994733: step 17210, loss 0.0280712, acc 0.98
2016-09-05T23:06:08.817787: step 17211, loss 0.00312028, acc 1
2016-09-05T23:06:09.648009: step 17212, loss 0.0164926, acc 0.98
2016-09-05T23:06:10.508279: step 17213, loss 0.00233497, acc 1
2016-09-05T23:06:11.309814: step 17214, loss 0.0187334, acc 0.98
2016-09-05T23:06:12.136042: step 17215, loss 0.00254489, acc 1
2016-09-05T23:06:12.956977: step 17216, loss 0.0179225, acc 0.98
2016-09-05T23:06:13.783185: step 17217, loss 0.0195748, acc 0.98
2016-09-05T23:06:14.629450: step 17218, loss 0.00234724, acc 1
2016-09-05T23:06:15.429650: step 17219, loss 0.00234156, acc 1
2016-09-05T23:06:16.247867: step 17220, loss 0.0458924, acc 0.96
2016-09-05T23:06:17.063755: step 17221, loss 0.00954744, acc 1
2016-09-05T23:06:17.883026: step 17222, loss 0.00810512, acc 1
2016-09-05T23:06:18.670949: step 17223, loss 0.0294898, acc 1
2016-09-05T23:06:19.512317: step 17224, loss 0.080157, acc 0.98
2016-09-05T23:06:20.384967: step 17225, loss 0.0522823, acc 0.98
2016-09-05T23:06:21.176897: step 17226, loss 0.0365855, acc 0.98
2016-09-05T23:06:21.990929: step 17227, loss 0.02787, acc 0.98
2016-09-05T23:06:22.797315: step 17228, loss 0.00232305, acc 1
2016-09-05T23:06:23.640036: step 17229, loss 0.00228534, acc 1
2016-09-05T23:06:24.445754: step 17230, loss 0.0174195, acc 0.98
2016-09-05T23:06:25.306782: step 17231, loss 0.00226767, acc 1
2016-09-05T23:06:26.160736: step 17232, loss 0.0134025, acc 1
2016-09-05T23:06:27.011651: step 17233, loss 0.00221946, acc 1
2016-09-05T23:06:27.844576: step 17234, loss 0.00725962, acc 1
2016-09-05T23:06:28.674711: step 17235, loss 0.0126627, acc 1
2016-09-05T23:06:29.464551: step 17236, loss 0.00218197, acc 1
2016-09-05T23:06:30.275814: step 17237, loss 0.00235667, acc 1
2016-09-05T23:06:31.164453: step 17238, loss 0.0274251, acc 1
2016-09-05T23:06:31.976680: step 17239, loss 0.00213117, acc 1
2016-09-05T23:06:32.915664: step 17240, loss 0.0117342, acc 1
2016-09-05T23:06:33.805537: step 17241, loss 0.0178422, acc 1
2016-09-05T23:06:34.610031: step 17242, loss 0.100648, acc 0.96
2016-09-05T23:06:35.423749: step 17243, loss 0.0201747, acc 0.98
2016-09-05T23:06:36.264731: step 17244, loss 0.00304467, acc 1
2016-09-05T23:06:37.106534: step 17245, loss 0.0195858, acc 0.98
2016-09-05T23:06:37.943277: step 17246, loss 0.0234607, acc 0.98
2016-09-05T23:06:38.782546: step 17247, loss 0.00381832, acc 1
2016-09-05T23:06:39.627940: step 17248, loss 0.029592, acc 0.98
2016-09-05T23:06:40.451790: step 17249, loss 0.0018821, acc 1
2016-09-05T23:06:41.266618: step 17250, loss 0.00190639, acc 1
2016-09-05T23:06:42.092534: step 17251, loss 0.00205448, acc 1
2016-09-05T23:06:42.861571: step 17252, loss 0.0115473, acc 1
2016-09-05T23:06:43.678066: step 17253, loss 0.00214857, acc 1
2016-09-05T23:06:44.513560: step 17254, loss 0.00405143, acc 1
2016-09-05T23:06:45.301512: step 17255, loss 0.00190453, acc 1
2016-09-05T23:06:46.082608: step 17256, loss 0.010082, acc 1
2016-09-05T23:06:46.881458: step 17257, loss 0.0017716, acc 1
2016-09-05T23:06:47.669465: step 17258, loss 0.00358496, acc 1
2016-09-05T23:06:48.495900: step 17259, loss 0.00876167, acc 1
2016-09-05T23:06:49.315959: step 17260, loss 0.00184827, acc 1
2016-09-05T23:06:50.120635: step 17261, loss 0.0148699, acc 1
2016-09-05T23:06:50.916839: step 17262, loss 0.0121623, acc 1
2016-09-05T23:06:51.734853: step 17263, loss 0.00712981, acc 1
2016-09-05T23:06:52.520381: step 17264, loss 0.00842716, acc 1
2016-09-05T23:06:53.324002: step 17265, loss 0.00344952, acc 1
2016-09-05T23:06:53.735757: step 17266, loss 0.00182311, acc 1
2016-09-05T23:06:54.555741: step 17267, loss 0.00371433, acc 1
2016-09-05T23:06:55.375735: step 17268, loss 0.00216619, acc 1
2016-09-05T23:06:56.174578: step 17269, loss 0.00775169, acc 1
2016-09-05T23:06:56.981727: step 17270, loss 0.00663401, acc 1
2016-09-05T23:06:57.794165: step 17271, loss 0.0151908, acc 1
2016-09-05T23:06:58.592839: step 17272, loss 0.100017, acc 0.96
2016-09-05T23:06:59.405608: step 17273, loss 0.0122444, acc 1
2016-09-05T23:07:00.249590: step 17274, loss 0.001929, acc 1
2016-09-05T23:07:01.014585: step 17275, loss 0.012151, acc 1
2016-09-05T23:07:01.813610: step 17276, loss 0.00802346, acc 1
2016-09-05T23:07:02.646550: step 17277, loss 0.00186592, acc 1
2016-09-05T23:07:03.432452: step 17278, loss 0.00276321, acc 1
2016-09-05T23:07:04.255446: step 17279, loss 0.016412, acc 0.98
2016-09-05T23:07:05.102418: step 17280, loss 0.0311395, acc 0.98
2016-09-05T23:07:05.879283: step 17281, loss 0.0044965, acc 1
2016-09-05T23:07:06.691054: step 17282, loss 0.00202654, acc 1
2016-09-05T23:07:07.535279: step 17283, loss 0.0141454, acc 1
2016-09-05T23:07:08.329949: step 17284, loss 0.0148241, acc 1
2016-09-05T23:07:09.128827: step 17285, loss 0.032777, acc 0.98
2016-09-05T23:07:09.946309: step 17286, loss 0.0180879, acc 0.98
2016-09-05T23:07:10.733880: step 17287, loss 0.00168832, acc 1
2016-09-05T23:07:11.509548: step 17288, loss 0.0100685, acc 1
2016-09-05T23:07:12.317427: step 17289, loss 0.00157384, acc 1
2016-09-05T23:07:13.098815: step 17290, loss 0.00165418, acc 1
2016-09-05T23:07:13.933408: step 17291, loss 0.00243945, acc 1
2016-09-05T23:07:14.751231: step 17292, loss 0.00171496, acc 1
2016-09-05T23:07:15.549199: step 17293, loss 0.00162801, acc 1
2016-09-05T23:07:16.346189: step 17294, loss 0.0216437, acc 0.98
2016-09-05T23:07:17.173414: step 17295, loss 0.00329887, acc 1
2016-09-05T23:07:17.961346: step 17296, loss 0.00201329, acc 1
2016-09-05T23:07:18.781665: step 17297, loss 0.0164424, acc 1
2016-09-05T23:07:19.588009: step 17298, loss 0.0534097, acc 0.94
2016-09-05T23:07:20.360680: step 17299, loss 0.0058126, acc 1
2016-09-05T23:07:21.158665: step 17300, loss 0.026957, acc 1

Evaluation:
2016-09-05T23:07:24.661161: step 17300, loss 2.64085, acc 0.726

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-17300

2016-09-05T23:07:26.536357: step 17301, loss 0.0410802, acc 0.98
2016-09-05T23:07:27.349676: step 17302, loss 0.0400242, acc 0.98
2016-09-05T23:07:28.175193: step 17303, loss 0.0128352, acc 1
2016-09-05T23:07:28.982691: step 17304, loss 0.00173673, acc 1
2016-09-05T23:07:29.819797: step 17305, loss 0.00211729, acc 1
2016-09-05T23:07:30.638130: step 17306, loss 0.00168501, acc 1
2016-09-05T23:07:31.469682: step 17307, loss 0.0280697, acc 1
2016-09-05T23:07:32.296168: step 17308, loss 0.01304, acc 1
2016-09-05T23:07:33.148738: step 17309, loss 0.00172753, acc 1
2016-09-05T23:07:33.958594: step 17310, loss 0.00885477, acc 1
2016-09-05T23:07:34.792611: step 17311, loss 0.00209199, acc 1
2016-09-05T23:07:35.610573: step 17312, loss 0.00413925, acc 1
2016-09-05T23:07:36.417632: step 17313, loss 0.0520163, acc 0.98
2016-09-05T23:07:37.230107: step 17314, loss 0.0170683, acc 1
2016-09-05T23:07:38.046873: step 17315, loss 0.00231933, acc 1
2016-09-05T23:07:38.891921: step 17316, loss 0.0267014, acc 0.98
2016-09-05T23:07:39.697514: step 17317, loss 0.00172126, acc 1
2016-09-05T23:07:40.531485: step 17318, loss 0.0106841, acc 1
2016-09-05T23:07:41.365995: step 17319, loss 0.00275848, acc 1
2016-09-05T23:07:42.167082: step 17320, loss 0.00217284, acc 1
2016-09-05T23:07:42.987127: step 17321, loss 0.016112, acc 1
2016-09-05T23:07:43.791023: step 17322, loss 0.00267245, acc 1
2016-09-05T23:07:44.592628: step 17323, loss 0.00504837, acc 1
2016-09-05T23:07:45.386314: step 17324, loss 0.00178855, acc 1
2016-09-05T23:07:46.193163: step 17325, loss 0.0104656, acc 1
2016-09-05T23:07:46.997810: step 17326, loss 0.00179954, acc 1
2016-09-05T23:07:47.805938: step 17327, loss 0.0041791, acc 1
2016-09-05T23:07:48.615445: step 17328, loss 0.00226662, acc 1
2016-09-05T23:07:49.412954: step 17329, loss 0.00426221, acc 1
2016-09-05T23:07:50.207953: step 17330, loss 0.00854965, acc 1
2016-09-05T23:07:51.014042: step 17331, loss 0.0167649, acc 0.98
2016-09-05T23:07:51.814978: step 17332, loss 0.0162988, acc 0.98
2016-09-05T23:07:52.638847: step 17333, loss 0.00565893, acc 1
2016-09-05T23:07:53.431121: step 17334, loss 0.0107491, acc 1
2016-09-05T23:07:54.242794: step 17335, loss 0.0019063, acc 1
2016-09-05T23:07:55.026428: step 17336, loss 0.00348544, acc 1
2016-09-05T23:07:55.836631: step 17337, loss 0.0018518, acc 1
2016-09-05T23:07:56.603652: step 17338, loss 0.00566743, acc 1
2016-09-05T23:07:57.462907: step 17339, loss 0.00184676, acc 1
2016-09-05T23:07:58.297389: step 17340, loss 0.00451444, acc 1
2016-09-05T23:07:59.069341: step 17341, loss 0.00191931, acc 1
2016-09-05T23:07:59.853880: step 17342, loss 0.0397989, acc 0.98
2016-09-05T23:08:00.701074: step 17343, loss 0.0223178, acc 1
2016-09-05T23:08:01.462139: step 17344, loss 0.00182517, acc 1
2016-09-05T23:08:02.275336: step 17345, loss 0.00415655, acc 1
2016-09-05T23:08:03.091352: step 17346, loss 0.0573406, acc 0.98
2016-09-05T23:08:03.885436: step 17347, loss 0.00211373, acc 1
2016-09-05T23:08:04.676585: step 17348, loss 0.00246013, acc 1
2016-09-05T23:08:05.522873: step 17349, loss 0.0199911, acc 0.98
2016-09-05T23:08:06.336677: step 17350, loss 0.00298397, acc 1
2016-09-05T23:08:07.141329: step 17351, loss 0.0577425, acc 0.98
2016-09-05T23:08:07.972125: step 17352, loss 0.00282237, acc 1
2016-09-05T23:08:08.755833: step 17353, loss 0.00216219, acc 1
2016-09-05T23:08:09.669436: step 17354, loss 0.00532667, acc 1
2016-09-05T23:08:10.562005: step 17355, loss 0.00693761, acc 1
2016-09-05T23:08:11.407742: step 17356, loss 0.0208535, acc 0.98
2016-09-05T23:08:12.211418: step 17357, loss 0.0355314, acc 0.98
2016-09-05T23:08:13.048456: step 17358, loss 0.0124328, acc 1
2016-09-05T23:08:13.967523: step 17359, loss 0.00468936, acc 1
2016-09-05T23:08:14.885429: step 17360, loss 0.0121431, acc 1
2016-09-05T23:08:15.687894: step 17361, loss 0.0959248, acc 0.96
2016-09-05T23:08:16.531451: step 17362, loss 0.00599105, acc 1
2016-09-05T23:08:17.345914: step 17363, loss 0.0159528, acc 1
2016-09-05T23:08:18.159078: step 17364, loss 0.0215341, acc 1
2016-09-05T23:08:18.992692: step 17365, loss 0.0149212, acc 1
2016-09-05T23:08:19.809395: step 17366, loss 0.0117846, acc 1
2016-09-05T23:08:20.620194: step 17367, loss 0.00235458, acc 1
2016-09-05T23:08:21.433419: step 17368, loss 0.0145324, acc 1
2016-09-05T23:08:22.237425: step 17369, loss 0.027343, acc 1
2016-09-05T23:08:23.129610: step 17370, loss 0.00248634, acc 1
2016-09-05T23:08:23.966542: step 17371, loss 0.00252002, acc 1
2016-09-05T23:08:24.758413: step 17372, loss 0.00600034, acc 1
2016-09-05T23:08:25.573430: step 17373, loss 0.109793, acc 0.96
2016-09-05T23:08:26.399784: step 17374, loss 0.00291095, acc 1
2016-09-05T23:08:27.223111: step 17375, loss 0.0223845, acc 1
2016-09-05T23:08:28.053325: step 17376, loss 0.051053, acc 0.98
2016-09-05T23:08:28.874252: step 17377, loss 0.0171452, acc 1
2016-09-05T23:08:29.726744: step 17378, loss 0.00225844, acc 1
2016-09-05T23:08:30.529345: step 17379, loss 0.0870141, acc 0.96
2016-09-05T23:08:31.340258: step 17380, loss 0.013159, acc 1
2016-09-05T23:08:32.140869: step 17381, loss 0.025724, acc 0.98
2016-09-05T23:08:32.908847: step 17382, loss 0.0671649, acc 0.98
2016-09-05T23:08:33.727763: step 17383, loss 0.00603427, acc 1
2016-09-05T23:08:34.577539: step 17384, loss 0.0132548, acc 1
2016-09-05T23:08:35.391852: step 17385, loss 0.0128041, acc 1
2016-09-05T23:08:36.191440: step 17386, loss 0.0201107, acc 1
2016-09-05T23:08:37.028335: step 17387, loss 0.0158105, acc 1
2016-09-05T23:08:37.859097: step 17388, loss 0.0299609, acc 0.98
2016-09-05T23:08:38.687703: step 17389, loss 0.00255142, acc 1
2016-09-05T23:08:39.530479: step 17390, loss 0.00449064, acc 1
2016-09-05T23:08:40.383083: step 17391, loss 0.0140599, acc 1
2016-09-05T23:08:41.213578: step 17392, loss 0.00832648, acc 1
2016-09-05T23:08:42.041084: step 17393, loss 0.0180645, acc 1
2016-09-05T23:08:42.839922: step 17394, loss 0.00246207, acc 1
2016-09-05T23:08:43.658445: step 17395, loss 0.00533376, acc 1
2016-09-05T23:08:44.483414: step 17396, loss 0.0254356, acc 0.98
2016-09-05T23:08:45.278460: step 17397, loss 0.0212295, acc 1
2016-09-05T23:08:46.068695: step 17398, loss 0.0824526, acc 0.96
2016-09-05T23:08:46.882975: step 17399, loss 0.00272794, acc 1
2016-09-05T23:08:47.699232: step 17400, loss 0.00322317, acc 1

Evaluation:
2016-09-05T23:08:51.179814: step 17400, loss 2.81323, acc 0.72

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-17400

2016-09-05T23:08:53.134126: step 17401, loss 0.0115448, acc 1
2016-09-05T23:08:53.946883: step 17402, loss 0.0031474, acc 1
2016-09-05T23:08:54.746789: step 17403, loss 0.00237548, acc 1
2016-09-05T23:08:55.538776: step 17404, loss 0.00235057, acc 1
2016-09-05T23:08:56.359021: step 17405, loss 0.0038721, acc 1
2016-09-05T23:08:57.146149: step 17406, loss 0.0455093, acc 0.96
2016-09-05T23:08:57.923586: step 17407, loss 0.011966, acc 1
2016-09-05T23:08:58.756109: step 17408, loss 0.00286462, acc 1
2016-09-05T23:08:59.522300: step 17409, loss 0.00267164, acc 1
2016-09-05T23:09:00.345539: step 17410, loss 0.016686, acc 0.98
2016-09-05T23:09:01.154789: step 17411, loss 0.025074, acc 0.98
2016-09-05T23:09:01.939027: step 17412, loss 0.00216802, acc 1
2016-09-05T23:09:02.750149: step 17413, loss 0.0149183, acc 1
2016-09-05T23:09:03.556925: step 17414, loss 0.0146862, acc 1
2016-09-05T23:09:04.340223: step 17415, loss 0.0171211, acc 0.98
2016-09-05T23:09:05.144937: step 17416, loss 0.00208639, acc 1
2016-09-05T23:09:05.967564: step 17417, loss 0.0327599, acc 0.98
2016-09-05T23:09:06.780914: step 17418, loss 0.0317777, acc 1
2016-09-05T23:09:07.581043: step 17419, loss 0.0455203, acc 0.96
2016-09-05T23:09:08.386973: step 17420, loss 0.0174037, acc 0.98
2016-09-05T23:09:09.179630: step 17421, loss 0.00297785, acc 1
2016-09-05T23:09:09.990042: step 17422, loss 0.0020869, acc 1
2016-09-05T23:09:10.801237: step 17423, loss 0.00498627, acc 1
2016-09-05T23:09:11.593045: step 17424, loss 0.00297326, acc 1
2016-09-05T23:09:12.390522: step 17425, loss 0.00200248, acc 1
2016-09-05T23:09:13.197720: step 17426, loss 0.0202538, acc 1
2016-09-05T23:09:13.969290: step 17427, loss 0.0564449, acc 0.98
2016-09-05T23:09:14.800246: step 17428, loss 0.0210758, acc 1
2016-09-05T23:09:15.626725: step 17429, loss 0.00206837, acc 1
2016-09-05T23:09:16.416382: step 17430, loss 0.0483052, acc 0.98
2016-09-05T23:09:17.221245: step 17431, loss 0.0424989, acc 0.96
2016-09-05T23:09:18.003892: step 17432, loss 0.0366582, acc 0.98
2016-09-05T23:09:18.796144: step 17433, loss 0.00177204, acc 1
2016-09-05T23:09:19.615595: step 17434, loss 0.0017633, acc 1
2016-09-05T23:09:20.416231: step 17435, loss 0.0132643, acc 1
2016-09-05T23:09:21.225693: step 17436, loss 0.00423339, acc 1
2016-09-05T23:09:22.034317: step 17437, loss 0.00176387, acc 1
2016-09-05T23:09:22.872688: step 17438, loss 0.0238542, acc 0.98
2016-09-05T23:09:23.662444: step 17439, loss 0.00164999, acc 1
2016-09-05T23:09:24.482960: step 17440, loss 0.00317025, acc 1
2016-09-05T23:09:25.298542: step 17441, loss 0.00164326, acc 1
2016-09-05T23:09:26.074720: step 17442, loss 0.00193287, acc 1
2016-09-05T23:09:26.890877: step 17443, loss 0.00155319, acc 1
2016-09-05T23:09:27.707505: step 17444, loss 0.00166907, acc 1
2016-09-05T23:09:28.501404: step 17445, loss 0.0157745, acc 1
2016-09-05T23:09:29.306092: step 17446, loss 0.0191045, acc 0.98
2016-09-05T23:09:30.110828: step 17447, loss 0.0208815, acc 0.98
2016-09-05T23:09:30.890608: step 17448, loss 0.00154392, acc 1
2016-09-05T23:09:31.703194: step 17449, loss 0.00168491, acc 1
2016-09-05T23:09:32.536562: step 17450, loss 0.00344143, acc 1
2016-09-05T23:09:33.312526: step 17451, loss 0.0246302, acc 0.98
2016-09-05T23:09:34.124346: step 17452, loss 0.0014519, acc 1
2016-09-05T23:09:34.929446: step 17453, loss 0.0257181, acc 0.98
2016-09-05T23:09:35.733776: step 17454, loss 0.00145028, acc 1
2016-09-05T23:09:36.536519: step 17455, loss 0.0255902, acc 0.98
2016-09-05T23:09:37.349396: step 17456, loss 0.0338481, acc 0.98
2016-09-05T23:09:38.121972: step 17457, loss 0.0176422, acc 1
2016-09-05T23:09:38.941947: step 17458, loss 0.0188123, acc 0.98
2016-09-05T23:09:39.753039: step 17459, loss 0.00146681, acc 1
2016-09-05T23:09:40.208892: step 17460, loss 0.00145001, acc 1
2016-09-05T23:09:41.037402: step 17461, loss 0.00795983, acc 1
2016-09-05T23:09:41.830924: step 17462, loss 0.00483595, acc 1
2016-09-05T23:09:42.653097: step 17463, loss 0.0101094, acc 1
2016-09-05T23:09:43.488336: step 17464, loss 0.00283214, acc 1
2016-09-05T23:09:44.269520: step 17465, loss 0.00553118, acc 1
2016-09-05T23:09:45.069268: step 17466, loss 0.00230751, acc 1
2016-09-05T23:09:45.891702: step 17467, loss 0.015054, acc 1
2016-09-05T23:09:46.675341: step 17468, loss 0.00395124, acc 1
2016-09-05T23:09:47.457860: step 17469, loss 0.00154245, acc 1
2016-09-05T23:09:48.294067: step 17470, loss 0.00242802, acc 1
2016-09-05T23:09:49.057674: step 17471, loss 0.00574606, acc 1
2016-09-05T23:09:49.851795: step 17472, loss 0.0185024, acc 0.98
2016-09-05T23:09:50.665677: step 17473, loss 0.0382595, acc 0.96
2016-09-05T23:09:51.462767: step 17474, loss 0.0114936, acc 1
2016-09-05T23:09:52.272566: step 17475, loss 0.00665205, acc 1
2016-09-05T23:09:53.097764: step 17476, loss 0.0103424, acc 1
2016-09-05T23:09:53.884547: step 17477, loss 0.00889303, acc 1
2016-09-05T23:09:54.686412: step 17478, loss 0.0174189, acc 0.98
2016-09-05T23:09:55.533411: step 17479, loss 0.00167952, acc 1
2016-09-05T23:09:56.303199: step 17480, loss 0.0146679, acc 1
2016-09-05T23:09:57.110280: step 17481, loss 0.0121905, acc 1
2016-09-05T23:09:57.949486: step 17482, loss 0.0438368, acc 0.98
2016-09-05T23:09:58.752417: step 17483, loss 0.00757076, acc 1
2016-09-05T23:09:59.562570: step 17484, loss 0.00349077, acc 1
2016-09-05T23:10:00.382538: step 17485, loss 0.00170695, acc 1
2016-09-05T23:10:01.199880: step 17486, loss 0.00170427, acc 1
2016-09-05T23:10:01.994173: step 17487, loss 0.00231001, acc 1
2016-09-05T23:10:02.806606: step 17488, loss 0.00176242, acc 1
2016-09-05T23:10:03.630731: step 17489, loss 0.019075, acc 0.98
2016-09-05T23:10:04.432781: step 17490, loss 0.00171289, acc 1
2016-09-05T23:10:05.245074: step 17491, loss 0.0615423, acc 0.98
2016-09-05T23:10:06.060480: step 17492, loss 0.0354857, acc 0.96
2016-09-05T23:10:06.896039: step 17493, loss 0.0660518, acc 0.98
2016-09-05T23:10:07.746688: step 17494, loss 0.0017032, acc 1
2016-09-05T23:10:08.567990: step 17495, loss 0.0681729, acc 0.98
2016-09-05T23:10:09.402820: step 17496, loss 0.00687006, acc 1
2016-09-05T23:10:10.267358: step 17497, loss 0.00213201, acc 1
2016-09-05T23:10:11.068768: step 17498, loss 0.0236873, acc 0.98
2016-09-05T23:10:11.860565: step 17499, loss 0.00430831, acc 1
2016-09-05T23:10:12.692954: step 17500, loss 0.0104187, acc 1

Evaluation:
2016-09-05T23:10:16.185379: step 17500, loss 2.24009, acc 0.709

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-17500

2016-09-05T23:10:18.106999: step 17501, loss 0.021031, acc 0.98
2016-09-05T23:10:18.924950: step 17502, loss 0.0263713, acc 1
2016-09-05T23:10:19.743768: step 17503, loss 0.017886, acc 0.98
2016-09-05T23:10:20.532779: step 17504, loss 0.00247704, acc 1
2016-09-05T23:10:21.292557: step 17505, loss 0.0149714, acc 1
2016-09-05T23:10:22.115199: step 17506, loss 0.00564087, acc 1
2016-09-05T23:10:22.921357: step 17507, loss 0.00219838, acc 1
2016-09-05T23:10:23.903642: step 17508, loss 0.00654812, acc 1
2016-09-05T23:10:24.735476: step 17509, loss 0.00235118, acc 1
2016-09-05T23:10:25.526044: step 17510, loss 0.0101903, acc 1
2016-09-05T23:10:26.376781: step 17511, loss 0.00248117, acc 1
2016-09-05T23:10:27.192086: step 17512, loss 0.0693478, acc 0.98
2016-09-05T23:10:27.996385: step 17513, loss 0.00270136, acc 1
2016-09-05T23:10:28.826686: step 17514, loss 0.00273798, acc 1
2016-09-05T23:10:29.667617: step 17515, loss 0.00650034, acc 1
2016-09-05T23:10:30.482155: step 17516, loss 0.0025618, acc 1
2016-09-05T23:10:31.266223: step 17517, loss 0.0119466, acc 1
2016-09-05T23:10:32.139059: step 17518, loss 0.00256649, acc 1
2016-09-05T23:10:32.979623: step 17519, loss 0.0061128, acc 1
2016-09-05T23:10:33.779168: step 17520, loss 0.0287764, acc 0.98
2016-09-05T23:10:34.597781: step 17521, loss 0.00293911, acc 1
2016-09-05T23:10:35.435944: step 17522, loss 0.00268263, acc 1
2016-09-05T23:10:36.211568: step 17523, loss 0.0394972, acc 0.98
2016-09-05T23:10:37.003409: step 17524, loss 0.0316995, acc 0.98
2016-09-05T23:10:37.838462: step 17525, loss 0.00533131, acc 1
2016-09-05T23:10:38.615471: step 17526, loss 0.00292698, acc 1
2016-09-05T23:10:39.414508: step 17527, loss 0.00513653, acc 1
2016-09-05T23:10:40.235930: step 17528, loss 0.0214147, acc 0.98
2016-09-05T23:10:41.055149: step 17529, loss 0.0039886, acc 1
2016-09-05T23:10:41.917047: step 17530, loss 0.0149229, acc 1
2016-09-05T23:10:42.725180: step 17531, loss 0.0102869, acc 1
2016-09-05T23:10:43.524030: step 17532, loss 0.0273282, acc 0.98
2016-09-05T23:10:44.324829: step 17533, loss 0.00243679, acc 1
2016-09-05T23:10:45.143400: step 17534, loss 0.00464545, acc 1
2016-09-05T23:10:45.940158: step 17535, loss 0.012723, acc 1
2016-09-05T23:10:46.746545: step 17536, loss 0.00304258, acc 1
2016-09-05T23:10:47.549598: step 17537, loss 0.00303142, acc 1
2016-09-05T23:10:48.350862: step 17538, loss 0.00677984, acc 1
2016-09-05T23:10:49.139958: step 17539, loss 0.00835626, acc 1
2016-09-05T23:10:49.929947: step 17540, loss 0.0107637, acc 1
2016-09-05T23:10:50.688578: step 17541, loss 0.0164598, acc 1
2016-09-05T23:10:51.557536: step 17542, loss 0.00924565, acc 1
2016-09-05T23:10:52.370349: step 17543, loss 0.0398751, acc 0.98
2016-09-05T23:10:53.172978: step 17544, loss 0.00237029, acc 1
2016-09-05T23:10:53.987244: step 17545, loss 0.00656469, acc 1
2016-09-05T23:10:54.856245: step 17546, loss 0.0235928, acc 1
2016-09-05T23:10:55.706103: step 17547, loss 0.00236033, acc 1
2016-09-05T23:10:56.507372: step 17548, loss 0.0232438, acc 0.98
2016-09-05T23:10:57.327189: step 17549, loss 0.00821938, acc 1
2016-09-05T23:10:58.139037: step 17550, loss 0.0235204, acc 1
2016-09-05T23:10:58.965152: step 17551, loss 0.0173271, acc 0.98
2016-09-05T23:10:59.805774: step 17552, loss 0.00510907, acc 1
2016-09-05T23:11:00.650075: step 17553, loss 0.00233449, acc 1
2016-09-05T23:11:01.448080: step 17554, loss 0.0105974, acc 1
2016-09-05T23:11:02.278942: step 17555, loss 0.0316627, acc 1
2016-09-05T23:11:03.079103: step 17556, loss 0.00250517, acc 1
2016-09-05T23:11:03.863095: step 17557, loss 0.00247208, acc 1
2016-09-05T23:11:04.684596: step 17558, loss 0.0172284, acc 1
2016-09-05T23:11:05.476501: step 17559, loss 0.0165474, acc 0.98
2016-09-05T23:11:06.316795: step 17560, loss 0.0101071, acc 1
2016-09-05T23:11:07.164835: step 17561, loss 0.0121946, acc 1
2016-09-05T23:11:07.996076: step 17562, loss 0.00255883, acc 1
2016-09-05T23:11:08.782719: step 17563, loss 0.00243605, acc 1
2016-09-05T23:11:09.643043: step 17564, loss 0.00339135, acc 1
2016-09-05T23:11:10.518091: step 17565, loss 0.00230302, acc 1
2016-09-05T23:11:11.309812: step 17566, loss 0.00248701, acc 1
2016-09-05T23:11:12.138417: step 17567, loss 0.0167941, acc 0.98
2016-09-05T23:11:12.963834: step 17568, loss 0.021885, acc 0.98
2016-09-05T23:11:13.818798: step 17569, loss 0.037896, acc 0.98
2016-09-05T23:11:14.634237: step 17570, loss 0.00256596, acc 1
2016-09-05T23:11:15.462942: step 17571, loss 0.0340358, acc 0.98
2016-09-05T23:11:16.334172: step 17572, loss 0.00215886, acc 1
2016-09-05T23:11:17.140562: step 17573, loss 0.00439257, acc 1
2016-09-05T23:11:17.976834: step 17574, loss 0.00230202, acc 1
2016-09-05T23:11:18.806999: step 17575, loss 0.00208633, acc 1
2016-09-05T23:11:19.601909: step 17576, loss 0.0301149, acc 1
2016-09-05T23:11:20.463692: step 17577, loss 0.0370235, acc 0.98
2016-09-05T23:11:21.264924: step 17578, loss 0.00199763, acc 1
2016-09-05T23:11:22.039681: step 17579, loss 0.00195368, acc 1
2016-09-05T23:11:22.881371: step 17580, loss 0.0288918, acc 0.98
2016-09-05T23:11:23.692266: step 17581, loss 0.00656007, acc 1
2016-09-05T23:11:24.518449: step 17582, loss 0.0273979, acc 0.98
2016-09-05T23:11:25.328387: step 17583, loss 0.0238072, acc 0.98
2016-09-05T23:11:26.141818: step 17584, loss 0.00470438, acc 1
2016-09-05T23:11:26.917087: step 17585, loss 0.0294024, acc 0.98
2016-09-05T23:11:27.733103: step 17586, loss 0.00197409, acc 1
2016-09-05T23:11:28.559498: step 17587, loss 0.0203386, acc 0.98
2016-09-05T23:11:29.336671: step 17588, loss 0.00176058, acc 1
2016-09-05T23:11:30.163694: step 17589, loss 0.00243504, acc 1
2016-09-05T23:11:30.979836: step 17590, loss 0.00381508, acc 1
2016-09-05T23:11:31.758096: step 17591, loss 0.0102487, acc 1
2016-09-05T23:11:32.556508: step 17592, loss 0.050687, acc 0.98
2016-09-05T23:11:33.354303: step 17593, loss 0.00170127, acc 1
2016-09-05T23:11:34.122013: step 17594, loss 0.0233515, acc 0.98
2016-09-05T23:11:34.952861: step 17595, loss 0.0147691, acc 1
2016-09-05T23:11:35.748079: step 17596, loss 0.0163846, acc 1
2016-09-05T23:11:36.548263: step 17597, loss 0.0193743, acc 0.98
2016-09-05T23:11:37.396309: step 17598, loss 0.00158843, acc 1
2016-09-05T23:11:38.196658: step 17599, loss 0.00174683, acc 1
2016-09-05T23:11:38.986914: step 17600, loss 0.0285046, acc 0.98

Evaluation:
2016-09-05T23:11:42.502354: step 17600, loss 2.57092, acc 0.721

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-17600

2016-09-05T23:11:44.390918: step 17601, loss 0.0352477, acc 0.98
2016-09-05T23:11:45.192834: step 17602, loss 0.0171057, acc 0.98
2016-09-05T23:11:45.977935: step 17603, loss 0.00530688, acc 1
2016-09-05T23:11:46.775366: step 17604, loss 0.00319177, acc 1
2016-09-05T23:11:47.551973: step 17605, loss 0.0175636, acc 0.98
2016-09-05T23:11:48.340875: step 17606, loss 0.00217062, acc 1
2016-09-05T23:11:49.168055: step 17607, loss 0.00644637, acc 1
2016-09-05T23:11:49.957333: step 17608, loss 0.0184026, acc 0.98
2016-09-05T23:11:50.781287: step 17609, loss 0.00807541, acc 1
2016-09-05T23:11:51.617039: step 17610, loss 0.00572199, acc 1
2016-09-05T23:11:52.418276: step 17611, loss 0.00497535, acc 1
2016-09-05T23:11:53.178065: step 17612, loss 0.0255479, acc 0.98
2016-09-05T23:11:54.004323: step 17613, loss 0.0139162, acc 1
2016-09-05T23:11:54.773445: step 17614, loss 0.0418135, acc 0.98
2016-09-05T23:11:55.600651: step 17615, loss 0.0186257, acc 1
2016-09-05T23:11:56.397188: step 17616, loss 0.00339725, acc 1
2016-09-05T23:11:57.175089: step 17617, loss 0.00246817, acc 1
2016-09-05T23:11:57.961521: step 17618, loss 0.00162565, acc 1
2016-09-05T23:11:58.768053: step 17619, loss 0.00166875, acc 1
2016-09-05T23:11:59.574451: step 17620, loss 0.0018384, acc 1
2016-09-05T23:12:00.419341: step 17621, loss 0.00169052, acc 1
2016-09-05T23:12:01.259554: step 17622, loss 0.00180523, acc 1
2016-09-05T23:12:02.055293: step 17623, loss 0.00174416, acc 1
2016-09-05T23:12:02.839363: step 17624, loss 0.012232, acc 1
2016-09-05T23:12:03.678889: step 17625, loss 0.00745946, acc 1
2016-09-05T23:12:04.497201: step 17626, loss 0.0170168, acc 1
2016-09-05T23:12:05.306025: step 17627, loss 0.001777, acc 1
2016-09-05T23:12:06.164964: step 17628, loss 0.0163789, acc 0.98
2016-09-05T23:12:06.985417: step 17629, loss 0.0991143, acc 0.98
2016-09-05T23:12:07.804442: step 17630, loss 0.026245, acc 1
2016-09-05T23:12:08.639550: step 17631, loss 0.0555786, acc 0.98
2016-09-05T23:12:09.447403: step 17632, loss 0.00167959, acc 1
2016-09-05T23:12:10.262580: step 17633, loss 0.00197002, acc 1
2016-09-05T23:12:11.089472: step 17634, loss 0.0113446, acc 1
2016-09-05T23:12:11.892608: step 17635, loss 0.0151952, acc 1
2016-09-05T23:12:12.712900: step 17636, loss 0.00563948, acc 1
2016-09-05T23:12:13.569813: step 17637, loss 0.00181122, acc 1
2016-09-05T23:12:14.364012: step 17638, loss 0.00152298, acc 1
2016-09-05T23:12:15.156082: step 17639, loss 0.0156721, acc 0.98
2016-09-05T23:12:15.976558: step 17640, loss 0.0143315, acc 1
2016-09-05T23:12:16.793555: step 17641, loss 0.0874018, acc 0.98
2016-09-05T23:12:17.606033: step 17642, loss 0.0163757, acc 0.98
2016-09-05T23:12:18.438630: step 17643, loss 0.0130181, acc 1
2016-09-05T23:12:19.224492: step 17644, loss 0.00822643, acc 1
2016-09-05T23:12:20.033212: step 17645, loss 0.0014253, acc 1
2016-09-05T23:12:20.873201: step 17646, loss 0.00222143, acc 1
2016-09-05T23:12:21.670222: step 17647, loss 0.0450198, acc 0.98
2016-09-05T23:12:22.512927: step 17648, loss 0.0259487, acc 0.98
2016-09-05T23:12:23.346988: step 17649, loss 0.0160795, acc 1
2016-09-05T23:12:24.185870: step 17650, loss 0.00537211, acc 1
2016-09-05T23:12:24.992612: step 17651, loss 0.032576, acc 0.98
2016-09-05T23:12:25.833329: step 17652, loss 0.00316262, acc 1
2016-09-05T23:12:26.658639: step 17653, loss 0.0116097, acc 1
2016-09-05T23:12:27.066825: step 17654, loss 0.0547671, acc 1
2016-09-05T23:12:27.871395: step 17655, loss 0.0166524, acc 1
2016-09-05T23:12:28.678730: step 17656, loss 0.0365214, acc 0.98
2016-09-05T23:12:29.481134: step 17657, loss 0.0165727, acc 0.98
2016-09-05T23:12:30.346925: step 17658, loss 0.00419707, acc 1
2016-09-05T23:12:31.152842: step 17659, loss 0.00220445, acc 1
2016-09-05T23:12:31.990646: step 17660, loss 0.009169, acc 1
2016-09-05T23:12:32.824948: step 17661, loss 0.0206037, acc 1
2016-09-05T23:12:33.635740: step 17662, loss 0.0168996, acc 0.98
2016-09-05T23:12:34.430418: step 17663, loss 0.00438053, acc 1
2016-09-05T23:12:35.268960: step 17664, loss 0.00229896, acc 1
2016-09-05T23:12:36.074593: step 17665, loss 0.00281459, acc 1
2016-09-05T23:12:36.916168: step 17666, loss 0.00473537, acc 1
2016-09-05T23:12:37.744180: step 17667, loss 0.0151147, acc 1
2016-09-05T23:12:38.585183: step 17668, loss 0.00848567, acc 1
2016-09-05T23:12:39.392885: step 17669, loss 0.00219906, acc 1
2016-09-05T23:12:40.244106: step 17670, loss 0.00234514, acc 1
2016-09-05T23:12:41.050333: step 17671, loss 0.0478498, acc 0.98
2016-09-05T23:12:41.848546: step 17672, loss 0.00319168, acc 1
2016-09-05T23:12:42.680691: step 17673, loss 0.0194677, acc 0.98
2016-09-05T23:12:43.497583: step 17674, loss 0.00552428, acc 1
2016-09-05T23:12:44.300029: step 17675, loss 0.00223657, acc 1
2016-09-05T23:12:45.093386: step 17676, loss 0.00233324, acc 1
2016-09-05T23:12:45.895558: step 17677, loss 0.00268647, acc 1
2016-09-05T23:12:46.706482: step 17678, loss 0.00269676, acc 1
2016-09-05T23:12:47.546828: step 17679, loss 0.00232286, acc 1
2016-09-05T23:12:48.353850: step 17680, loss 0.00220639, acc 1
2016-09-05T23:12:49.149038: step 17681, loss 0.0021961, acc 1
2016-09-05T23:12:49.943527: step 17682, loss 0.00218614, acc 1
2016-09-05T23:12:50.767088: step 17683, loss 0.00253158, acc 1
2016-09-05T23:12:51.573102: step 17684, loss 0.00230477, acc 1
2016-09-05T23:12:52.376260: step 17685, loss 0.00215892, acc 1
2016-09-05T23:12:53.203096: step 17686, loss 0.0494302, acc 0.98
2016-09-05T23:12:54.074169: step 17687, loss 0.0317357, acc 0.98
2016-09-05T23:12:54.882505: step 17688, loss 0.00422189, acc 1
2016-09-05T23:12:55.698768: step 17689, loss 0.0124659, acc 1
2016-09-05T23:12:56.517420: step 17690, loss 0.00239648, acc 1
2016-09-05T23:12:57.369097: step 17691, loss 0.00215836, acc 1
2016-09-05T23:12:58.216877: step 17692, loss 0.0190518, acc 0.98
2016-09-05T23:12:59.049140: step 17693, loss 0.00208738, acc 1
2016-09-05T23:12:59.848111: step 17694, loss 0.00216334, acc 1
2016-09-05T23:13:00.674310: step 17695, loss 0.0177478, acc 0.98
2016-09-05T23:13:01.479571: step 17696, loss 0.00203271, acc 1
2016-09-05T23:13:02.280633: step 17697, loss 0.00205519, acc 1
2016-09-05T23:13:03.121535: step 17698, loss 0.0021498, acc 1
2016-09-05T23:13:03.921231: step 17699, loss 0.00210044, acc 1
2016-09-05T23:13:04.717082: step 17700, loss 0.0636707, acc 0.98

Evaluation:
2016-09-05T23:13:08.213489: step 17700, loss 3.15581, acc 0.717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-17700

2016-09-05T23:13:10.005260: step 17701, loss 0.00648736, acc 1
2016-09-05T23:13:10.821327: step 17702, loss 0.00203089, acc 1
2016-09-05T23:13:11.649124: step 17703, loss 0.00187176, acc 1
2016-09-05T23:13:12.448964: step 17704, loss 0.0116487, acc 1
2016-09-05T23:13:13.249928: step 17705, loss 0.035258, acc 0.98
2016-09-05T23:13:14.111359: step 17706, loss 0.0150092, acc 1
2016-09-05T23:13:14.929490: step 17707, loss 0.00180684, acc 1
2016-09-05T23:13:15.733194: step 17708, loss 0.00173983, acc 1
2016-09-05T23:13:16.555970: step 17709, loss 0.00994815, acc 1
2016-09-05T23:13:17.371864: step 17710, loss 0.0167087, acc 0.98
2016-09-05T23:13:18.176094: step 17711, loss 0.00817102, acc 1
2016-09-05T23:13:19.044410: step 17712, loss 0.00166128, acc 1
2016-09-05T23:13:19.857044: step 17713, loss 0.00266108, acc 1
2016-09-05T23:13:20.668422: step 17714, loss 0.0548053, acc 0.98
2016-09-05T23:13:21.502071: step 17715, loss 0.0101194, acc 1
2016-09-05T23:13:22.337388: step 17716, loss 0.0213746, acc 0.98
2016-09-05T23:13:23.130935: step 17717, loss 0.0151054, acc 1
2016-09-05T23:13:23.943073: step 17718, loss 0.0251396, acc 0.98
2016-09-05T23:13:24.804951: step 17719, loss 0.0207702, acc 1
2016-09-05T23:13:25.590628: step 17720, loss 0.00416904, acc 1
2016-09-05T23:13:26.376742: step 17721, loss 0.0086992, acc 1
2016-09-05T23:13:27.184192: step 17722, loss 0.0129921, acc 1
2016-09-05T23:13:27.957504: step 17723, loss 0.029495, acc 0.98
2016-09-05T23:13:28.767429: step 17724, loss 0.0164422, acc 1
2016-09-05T23:13:29.599525: step 17725, loss 0.00300947, acc 1
2016-09-05T23:13:30.397137: step 17726, loss 0.0164629, acc 0.98
2016-09-05T23:13:31.204863: step 17727, loss 0.0016152, acc 1
2016-09-05T23:13:32.027188: step 17728, loss 0.00165214, acc 1
2016-09-05T23:13:32.840022: step 17729, loss 0.00812535, acc 1
2016-09-05T23:13:33.629906: step 17730, loss 0.0385779, acc 0.96
2016-09-05T23:13:34.458595: step 17731, loss 0.00433895, acc 1
2016-09-05T23:13:35.252668: step 17732, loss 0.00253826, acc 1
2016-09-05T23:13:36.044070: step 17733, loss 0.0168959, acc 1
2016-09-05T23:13:36.869441: step 17734, loss 0.0120177, acc 1
2016-09-05T23:13:37.663259: step 17735, loss 0.00679233, acc 1
2016-09-05T23:13:38.495499: step 17736, loss 0.0164788, acc 0.98
2016-09-05T23:13:39.308416: step 17737, loss 0.00568711, acc 1
2016-09-05T23:13:40.098687: step 17738, loss 0.0113806, acc 1
2016-09-05T23:13:40.880863: step 17739, loss 0.0177771, acc 0.98
2016-09-05T23:13:41.703222: step 17740, loss 0.0037766, acc 1
2016-09-05T23:13:42.454353: step 17741, loss 0.00187517, acc 1
2016-09-05T23:13:43.279657: step 17742, loss 0.00188307, acc 1
2016-09-05T23:13:44.099309: step 17743, loss 0.00191753, acc 1
2016-09-05T23:13:44.940776: step 17744, loss 0.0301504, acc 1
2016-09-05T23:13:45.758011: step 17745, loss 0.0221966, acc 0.98
2016-09-05T23:13:46.629396: step 17746, loss 0.00981452, acc 1
2016-09-05T23:13:47.473871: step 17747, loss 0.00417735, acc 1
2016-09-05T23:13:48.368338: step 17748, loss 0.129456, acc 0.96
2016-09-05T23:13:49.266447: step 17749, loss 0.0158123, acc 1
2016-09-05T23:13:50.076602: step 17750, loss 0.0296242, acc 0.98
2016-09-05T23:13:50.902259: step 17751, loss 0.0887878, acc 0.98
2016-09-05T23:13:51.737947: step 17752, loss 0.0218453, acc 0.98
2016-09-05T23:13:52.604447: step 17753, loss 0.0017327, acc 1
2016-09-05T23:13:53.402867: step 17754, loss 0.0472412, acc 0.96
2016-09-05T23:13:54.215266: step 17755, loss 0.00184112, acc 1
2016-09-05T23:13:55.026563: step 17756, loss 0.00186634, acc 1
2016-09-05T23:13:55.812906: step 17757, loss 0.0236101, acc 1
2016-09-05T23:13:56.617412: step 17758, loss 0.0174747, acc 0.98
2016-09-05T23:13:57.457067: step 17759, loss 0.00847915, acc 1
2016-09-05T23:13:58.240911: step 17760, loss 0.0016626, acc 1
2016-09-05T23:13:59.022883: step 17761, loss 0.00359216, acc 1
2016-09-05T23:13:59.848136: step 17762, loss 0.0293749, acc 0.98
2016-09-05T23:14:00.660021: step 17763, loss 0.00205421, acc 1
2016-09-05T23:14:01.453417: step 17764, loss 0.00525979, acc 1
2016-09-05T23:14:02.281866: step 17765, loss 0.0359191, acc 0.98
2016-09-05T23:14:03.077189: step 17766, loss 0.00353321, acc 1
2016-09-05T23:14:03.880760: step 17767, loss 0.00635162, acc 1
2016-09-05T23:14:04.689005: step 17768, loss 0.0128807, acc 1
2016-09-05T23:14:05.485108: step 17769, loss 0.00628161, acc 1
2016-09-05T23:14:06.280488: step 17770, loss 0.0298504, acc 0.98
2016-09-05T23:14:07.103506: step 17771, loss 0.010563, acc 1
2016-09-05T23:14:07.908124: step 17772, loss 0.00689331, acc 1
2016-09-05T23:14:08.725320: step 17773, loss 0.00602598, acc 1
2016-09-05T23:14:09.578472: step 17774, loss 0.0260633, acc 0.98
2016-09-05T23:14:10.396365: step 17775, loss 0.0123943, acc 1
2016-09-05T23:14:11.204535: step 17776, loss 0.00415214, acc 1
2016-09-05T23:14:12.034070: step 17777, loss 0.00189622, acc 1
2016-09-05T23:14:12.825763: step 17778, loss 0.00187718, acc 1
2016-09-05T23:14:13.635539: step 17779, loss 0.00454764, acc 1
2016-09-05T23:14:14.437456: step 17780, loss 0.0185603, acc 0.98
2016-09-05T23:14:15.237831: step 17781, loss 0.00872409, acc 1
2016-09-05T23:14:16.043643: step 17782, loss 0.00592394, acc 1
2016-09-05T23:14:16.861392: step 17783, loss 0.022643, acc 0.98
2016-09-05T23:14:17.642147: step 17784, loss 0.0527999, acc 0.98
2016-09-05T23:14:18.444122: step 17785, loss 0.0161767, acc 1
2016-09-05T23:14:19.289460: step 17786, loss 0.0181687, acc 0.98
2016-09-05T23:14:20.112026: step 17787, loss 0.00203859, acc 1
2016-09-05T23:14:20.925215: step 17788, loss 0.0247258, acc 0.98
2016-09-05T23:14:21.750084: step 17789, loss 0.0155326, acc 1
2016-09-05T23:14:22.583156: step 17790, loss 0.00906223, acc 1
2016-09-05T23:14:23.376589: step 17791, loss 0.00348445, acc 1
2016-09-05T23:14:24.231101: step 17792, loss 0.00193938, acc 1
2016-09-05T23:14:25.052324: step 17793, loss 0.0175267, acc 1
2016-09-05T23:14:25.869156: step 17794, loss 0.0230763, acc 0.98
2016-09-05T23:14:26.699863: step 17795, loss 0.00272431, acc 1
2016-09-05T23:14:27.520793: step 17796, loss 0.0221895, acc 0.98
2016-09-05T23:14:28.321018: step 17797, loss 0.00318544, acc 1
2016-09-05T23:14:29.192263: step 17798, loss 0.00196987, acc 1
2016-09-05T23:14:30.016018: step 17799, loss 0.0459056, acc 0.98
2016-09-05T23:14:30.822914: step 17800, loss 0.00218285, acc 1

Evaluation:
2016-09-05T23:14:34.329455: step 17800, loss 3.10974, acc 0.716

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-17800

2016-09-05T23:14:36.204661: step 17801, loss 0.00206616, acc 1
2016-09-05T23:14:37.011443: step 17802, loss 0.0406219, acc 0.98
2016-09-05T23:14:37.805978: step 17803, loss 0.0368651, acc 0.98
2016-09-05T23:14:38.641358: step 17804, loss 0.00193583, acc 1
2016-09-05T23:14:39.456187: step 17805, loss 0.00274737, acc 1
2016-09-05T23:14:40.263549: step 17806, loss 0.0276182, acc 1
2016-09-05T23:14:41.078343: step 17807, loss 0.0298955, acc 0.98
2016-09-05T23:14:41.874531: step 17808, loss 0.0435055, acc 0.96
2016-09-05T23:14:42.654464: step 17809, loss 0.0022223, acc 1
2016-09-05T23:14:43.483430: step 17810, loss 0.0019211, acc 1
2016-09-05T23:14:44.273401: step 17811, loss 0.00219408, acc 1
2016-09-05T23:14:45.051598: step 17812, loss 0.0281658, acc 0.98
2016-09-05T23:14:45.892837: step 17813, loss 0.00193641, acc 1
2016-09-05T23:14:46.691856: step 17814, loss 0.0143421, acc 1
2016-09-05T23:14:47.527492: step 17815, loss 0.0608732, acc 0.96
2016-09-05T23:14:48.364772: step 17816, loss 0.0116099, acc 1
2016-09-05T23:14:49.163662: step 17817, loss 0.00424204, acc 1
2016-09-05T23:14:49.967804: step 17818, loss 0.00253898, acc 1
2016-09-05T23:14:50.779960: step 17819, loss 0.0162136, acc 0.98
2016-09-05T23:14:51.601035: step 17820, loss 0.00547314, acc 1
2016-09-05T23:14:52.399660: step 17821, loss 0.00222999, acc 1
2016-09-05T23:14:53.229966: step 17822, loss 0.00549031, acc 1
2016-09-05T23:14:54.034320: step 17823, loss 0.0169039, acc 0.98
2016-09-05T23:14:54.855366: step 17824, loss 0.0321151, acc 1
2016-09-05T23:14:55.685937: step 17825, loss 0.0173189, acc 1
2016-09-05T23:14:56.477478: step 17826, loss 0.0164386, acc 1
2016-09-05T23:14:57.289479: step 17827, loss 0.0146731, acc 1
2016-09-05T23:14:58.115446: step 17828, loss 0.00217657, acc 1
2016-09-05T23:14:58.923699: step 17829, loss 0.00221242, acc 1
2016-09-05T23:14:59.721737: step 17830, loss 0.00837671, acc 1
2016-09-05T23:15:00.578125: step 17831, loss 0.00212493, acc 1
2016-09-05T23:15:01.401367: step 17832, loss 0.00686625, acc 1
2016-09-05T23:15:02.216334: step 17833, loss 0.0206739, acc 1
2016-09-05T23:15:03.055433: step 17834, loss 0.0168218, acc 1
2016-09-05T23:15:03.878380: step 17835, loss 0.0162203, acc 0.98
2016-09-05T23:15:04.697558: step 17836, loss 0.00225089, acc 1
2016-09-05T23:15:05.558673: step 17837, loss 0.0215223, acc 0.98
2016-09-05T23:15:06.357375: step 17838, loss 0.00260114, acc 1
2016-09-05T23:15:07.164033: step 17839, loss 0.00263423, acc 1
2016-09-05T23:15:07.983082: step 17840, loss 0.00247554, acc 1
2016-09-05T23:15:08.808066: step 17841, loss 0.0163524, acc 0.98
2016-09-05T23:15:09.589183: step 17842, loss 0.00670024, acc 1
2016-09-05T23:15:10.429294: step 17843, loss 0.0164633, acc 0.98
2016-09-05T23:15:11.287191: step 17844, loss 0.0402231, acc 0.98
2016-09-05T23:15:12.075757: step 17845, loss 0.016654, acc 1
2016-09-05T23:15:12.902403: step 17846, loss 0.0145464, acc 1
2016-09-05T23:15:13.748750: step 17847, loss 0.00257146, acc 1
2016-09-05T23:15:14.160695: step 17848, loss 0.00221273, acc 1
2016-09-05T23:15:14.975215: step 17849, loss 0.0145281, acc 1
2016-09-05T23:15:15.783031: step 17850, loss 0.0177437, acc 0.98
2016-09-05T23:15:16.637729: step 17851, loss 0.021084, acc 1
2016-09-05T23:15:17.458052: step 17852, loss 0.0199166, acc 1
2016-09-05T23:15:18.271404: step 17853, loss 0.00239315, acc 1
2016-09-05T23:15:19.069745: step 17854, loss 0.12753, acc 0.98
2016-09-05T23:15:19.894691: step 17855, loss 0.00210231, acc 1
2016-09-05T23:15:20.707740: step 17856, loss 0.169263, acc 0.98
2016-09-05T23:15:21.495733: step 17857, loss 0.002054, acc 1
2016-09-05T23:15:22.316834: step 17858, loss 0.00483126, acc 1
2016-09-05T23:15:23.115294: step 17859, loss 0.0318048, acc 0.98
2016-09-05T23:15:23.941022: step 17860, loss 0.00883845, acc 1
2016-09-05T23:15:24.794542: step 17861, loss 0.0539752, acc 0.96
2016-09-05T23:15:25.578003: step 17862, loss 0.030689, acc 1
2016-09-05T23:15:26.414454: step 17863, loss 0.00541898, acc 1
2016-09-05T23:15:27.235930: step 17864, loss 0.00680143, acc 1
2016-09-05T23:15:28.033815: step 17865, loss 0.00230593, acc 1
2016-09-05T23:15:28.837322: step 17866, loss 0.00382612, acc 1
2016-09-05T23:15:29.646703: step 17867, loss 0.039691, acc 0.98
2016-09-05T23:15:30.472583: step 17868, loss 0.0194595, acc 0.98
2016-09-05T23:15:31.283302: step 17869, loss 0.00901439, acc 1
2016-09-05T23:15:32.121745: step 17870, loss 0.00706997, acc 1
2016-09-05T23:15:32.930393: step 17871, loss 0.00306755, acc 1
2016-09-05T23:15:33.761129: step 17872, loss 0.00384115, acc 1
2016-09-05T23:15:34.597588: step 17873, loss 0.00265213, acc 1
2016-09-05T23:15:35.390042: step 17874, loss 0.0026636, acc 1
2016-09-05T23:15:36.190671: step 17875, loss 0.00285718, acc 1
2016-09-05T23:15:37.008394: step 17876, loss 0.00300097, acc 1
2016-09-05T23:15:37.842913: step 17877, loss 0.00271427, acc 1
2016-09-05T23:15:38.648228: step 17878, loss 0.00277016, acc 1
2016-09-05T23:15:39.459580: step 17879, loss 0.0223627, acc 1
2016-09-05T23:15:40.297451: step 17880, loss 0.0085704, acc 1
2016-09-05T23:15:41.080165: step 17881, loss 0.00879742, acc 1
2016-09-05T23:15:41.882052: step 17882, loss 0.0156269, acc 1
2016-09-05T23:15:42.728075: step 17883, loss 0.00619018, acc 1
2016-09-05T23:15:43.489872: step 17884, loss 0.0152939, acc 1
2016-09-05T23:15:44.344733: step 17885, loss 0.00698234, acc 1
2016-09-05T23:15:45.209790: step 17886, loss 0.00917202, acc 1
2016-09-05T23:15:46.030321: step 17887, loss 0.00255572, acc 1
2016-09-05T23:15:46.860621: step 17888, loss 0.00254006, acc 1
2016-09-05T23:15:47.657430: step 17889, loss 0.0038815, acc 1
2016-09-05T23:15:48.475033: step 17890, loss 0.0118253, acc 1
2016-09-05T23:15:49.291467: step 17891, loss 0.0111556, acc 1
2016-09-05T23:15:50.118604: step 17892, loss 0.00309619, acc 1
2016-09-05T23:15:50.929876: step 17893, loss 0.0157093, acc 1
2016-09-05T23:15:51.736393: step 17894, loss 0.0047623, acc 1
2016-09-05T23:15:52.543020: step 17895, loss 0.00955126, acc 1
2016-09-05T23:15:53.363279: step 17896, loss 0.00677015, acc 1
2016-09-05T23:15:54.184634: step 17897, loss 0.00245597, acc 1
2016-09-05T23:15:55.047241: step 17898, loss 0.0281182, acc 0.98
2016-09-05T23:15:55.856046: step 17899, loss 0.0038598, acc 1
2016-09-05T23:15:56.695979: step 17900, loss 0.00275625, acc 1

Evaluation:
2016-09-05T23:16:00.208256: step 17900, loss 3.56683, acc 0.716

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-17900

2016-09-05T23:16:02.125562: step 17901, loss 0.0024685, acc 1
2016-09-05T23:16:02.928138: step 17902, loss 0.00238579, acc 1
2016-09-05T23:16:03.744376: step 17903, loss 0.00392948, acc 1
2016-09-05T23:16:04.559304: step 17904, loss 0.0098869, acc 1
2016-09-05T23:16:05.371853: step 17905, loss 0.00704795, acc 1
2016-09-05T23:16:06.187026: step 17906, loss 0.00248243, acc 1
2016-09-05T23:16:07.047107: step 17907, loss 0.00237141, acc 1
2016-09-05T23:16:07.854873: step 17908, loss 0.00231862, acc 1
2016-09-05T23:16:08.652010: step 17909, loss 0.00735759, acc 1
2016-09-05T23:16:09.472528: step 17910, loss 0.00229557, acc 1
2016-09-05T23:16:10.284930: step 17911, loss 0.0581598, acc 0.96
2016-09-05T23:16:11.122990: step 17912, loss 0.0384548, acc 0.96
2016-09-05T23:16:11.995522: step 17913, loss 0.0251097, acc 1
2016-09-05T23:16:12.823023: step 17914, loss 0.0106682, acc 1
2016-09-05T23:16:13.635473: step 17915, loss 0.00216558, acc 1
2016-09-05T23:16:14.482650: step 17916, loss 0.00294766, acc 1
2016-09-05T23:16:15.339242: step 17917, loss 0.00267733, acc 1
2016-09-05T23:16:16.137224: step 17918, loss 0.0315039, acc 0.98
2016-09-05T23:16:16.937136: step 17919, loss 0.0213709, acc 0.98
2016-09-05T23:16:17.776855: step 17920, loss 0.00764192, acc 1
2016-09-05T23:16:18.572528: step 17921, loss 0.00209037, acc 1
2016-09-05T23:16:19.381218: step 17922, loss 0.00512849, acc 1
2016-09-05T23:16:20.244137: step 17923, loss 0.00655341, acc 1
2016-09-05T23:16:21.040875: step 17924, loss 0.0266845, acc 0.98
2016-09-05T23:16:21.841761: step 17925, loss 0.0020233, acc 1
2016-09-05T23:16:22.647916: step 17926, loss 0.00198273, acc 1
2016-09-05T23:16:23.465199: step 17927, loss 0.0484321, acc 0.98
2016-09-05T23:16:24.279799: step 17928, loss 0.0152466, acc 1
2016-09-05T23:16:25.104136: step 17929, loss 0.00461933, acc 1
2016-09-05T23:16:25.931708: step 17930, loss 0.0163638, acc 0.98
2016-09-05T23:16:26.721620: step 17931, loss 0.00332119, acc 1
2016-09-05T23:16:27.586394: step 17932, loss 0.00603927, acc 1
2016-09-05T23:16:28.386142: step 17933, loss 0.0282791, acc 0.98
2016-09-05T23:16:29.203725: step 17934, loss 0.01726, acc 1
2016-09-05T23:16:30.046057: step 17935, loss 0.00188637, acc 1
2016-09-05T23:16:30.892954: step 17936, loss 0.00205457, acc 1
2016-09-05T23:16:31.707857: step 17937, loss 0.00184947, acc 1
2016-09-05T23:16:32.530104: step 17938, loss 0.0163621, acc 0.98
2016-09-05T23:16:33.341521: step 17939, loss 0.0190054, acc 0.98
2016-09-05T23:16:34.163759: step 17940, loss 0.0314852, acc 0.98
2016-09-05T23:16:35.002165: step 17941, loss 0.00193906, acc 1
2016-09-05T23:16:35.837566: step 17942, loss 0.00389127, acc 1
2016-09-05T23:16:36.649884: step 17943, loss 0.0164682, acc 1
2016-09-05T23:16:37.486094: step 17944, loss 0.0563086, acc 0.96
2016-09-05T23:16:38.300148: step 17945, loss 0.0141154, acc 1
2016-09-05T23:16:39.131202: step 17946, loss 0.0328407, acc 1
2016-09-05T23:16:39.929798: step 17947, loss 0.00418716, acc 1
2016-09-05T23:16:40.769527: step 17948, loss 0.0326174, acc 0.98
2016-09-05T23:16:41.553447: step 17949, loss 0.0197009, acc 0.98
2016-09-05T23:16:42.359445: step 17950, loss 0.00318722, acc 1
2016-09-05T23:16:43.199012: step 17951, loss 0.0236899, acc 1
2016-09-05T23:16:44.006266: step 17952, loss 0.0432227, acc 0.98
2016-09-05T23:16:44.825352: step 17953, loss 0.00802539, acc 1
2016-09-05T23:16:45.654861: step 17954, loss 0.0255066, acc 0.98
2016-09-05T23:16:46.540002: step 17955, loss 0.0483184, acc 0.96
2016-09-05T23:16:47.367241: step 17956, loss 0.0161595, acc 1
2016-09-05T23:16:48.233279: step 17957, loss 0.00439635, acc 1
2016-09-05T23:16:49.094263: step 17958, loss 0.0133774, acc 1
2016-09-05T23:16:49.935255: step 17959, loss 0.00919281, acc 1
2016-09-05T23:16:50.782704: step 17960, loss 0.00441937, acc 1
2016-09-05T23:16:51.656144: step 17961, loss 0.00460807, acc 1
2016-09-05T23:16:52.454405: step 17962, loss 0.0154087, acc 1
2016-09-05T23:16:53.276601: step 17963, loss 0.0066947, acc 1
2016-09-05T23:16:54.161866: step 17964, loss 0.0289986, acc 0.98
2016-09-05T23:16:55.074399: step 17965, loss 0.00848425, acc 1
2016-09-05T23:16:55.905906: step 17966, loss 0.00796534, acc 1
2016-09-05T23:16:56.832383: step 17967, loss 0.00512734, acc 1
2016-09-05T23:16:57.881474: step 17968, loss 0.0231202, acc 1
2016-09-05T23:16:58.888325: step 17969, loss 0.0251216, acc 0.98
2016-09-05T23:16:59.963468: step 17970, loss 0.0238366, acc 0.98
2016-09-05T23:17:00.988887: step 17971, loss 0.00521286, acc 1
2016-09-05T23:17:01.940710: step 17972, loss 0.00521865, acc 1
2016-09-05T23:17:02.861054: step 17973, loss 0.0171329, acc 1
2016-09-05T23:17:03.767474: step 17974, loss 0.0197347, acc 0.98
2016-09-05T23:17:04.699578: step 17975, loss 0.00519077, acc 1
2016-09-05T23:17:05.816507: step 17976, loss 0.0752519, acc 0.98
2016-09-05T23:17:06.734645: step 17977, loss 0.00511969, acc 1
2016-09-05T23:17:07.572582: step 17978, loss 0.0404261, acc 0.98
2016-09-05T23:17:08.692252: step 17979, loss 0.0279008, acc 0.98
2016-09-05T23:17:09.569089: step 17980, loss 0.0181703, acc 1
2016-09-05T23:17:10.549443: step 17981, loss 0.00488344, acc 1
2016-09-05T23:17:11.445224: step 17982, loss 0.00485559, acc 1
2016-09-05T23:17:12.418732: step 17983, loss 0.0442734, acc 0.98
2016-09-05T23:17:13.328060: step 17984, loss 0.0059139, acc 1
2016-09-05T23:17:14.177038: step 17985, loss 0.00766909, acc 1
2016-09-05T23:17:15.162004: step 17986, loss 0.00455545, acc 1
2016-09-05T23:17:16.075067: step 17987, loss 0.00449464, acc 1
2016-09-05T23:17:17.009375: step 17988, loss 0.00457705, acc 1
2016-09-05T23:17:17.834744: step 17989, loss 0.011749, acc 1
2016-09-05T23:17:18.664810: step 17990, loss 0.0191655, acc 0.98
2016-09-05T23:17:19.480738: step 17991, loss 0.00504296, acc 1
2016-09-05T23:17:20.294967: step 17992, loss 0.00418686, acc 1
2016-09-05T23:17:21.116637: step 17993, loss 0.015491, acc 1
2016-09-05T23:17:21.998171: step 17994, loss 0.00435135, acc 1
2016-09-05T23:17:22.873811: step 17995, loss 0.0197217, acc 1
2016-09-05T23:17:23.715255: step 17996, loss 0.00521476, acc 1
2016-09-05T23:17:24.596805: step 17997, loss 0.00577511, acc 1
2016-09-05T23:17:25.430304: step 17998, loss 0.164467, acc 0.98
2016-09-05T23:17:26.309341: step 17999, loss 0.00373582, acc 1
2016-09-05T23:17:27.100879: step 18000, loss 0.0324473, acc 1

Evaluation:
2016-09-05T23:17:30.609769: step 18000, loss 3.66277, acc 0.711

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-18000

2016-09-05T23:17:32.485399: step 18001, loss 0.017607, acc 1
2016-09-05T23:17:33.289204: step 18002, loss 0.00522862, acc 1
2016-09-05T23:17:34.136770: step 18003, loss 0.00402157, acc 1
2016-09-05T23:17:34.987273: step 18004, loss 0.00374405, acc 1
2016-09-05T23:17:35.782504: step 18005, loss 0.00328363, acc 1
2016-09-05T23:17:36.612879: step 18006, loss 0.00370015, acc 1
2016-09-05T23:17:37.418815: step 18007, loss 0.00331218, acc 1
2016-09-05T23:17:38.233680: step 18008, loss 0.0340631, acc 0.98
2016-09-05T23:17:39.086174: step 18009, loss 0.00313942, acc 1
2016-09-05T23:17:39.905472: step 18010, loss 0.0030981, acc 1
2016-09-05T23:17:40.718413: step 18011, loss 0.00533898, acc 1
2016-09-05T23:17:41.552763: step 18012, loss 0.00973117, acc 1
2016-09-05T23:17:42.396133: step 18013, loss 0.00531333, acc 1
2016-09-05T23:17:43.229427: step 18014, loss 0.00437787, acc 1
2016-09-05T23:17:44.084983: step 18015, loss 0.00861274, acc 1
2016-09-05T23:17:44.907234: step 18016, loss 0.0035324, acc 1
2016-09-05T23:17:45.810884: step 18017, loss 0.00361144, acc 1
2016-09-05T23:17:46.619580: step 18018, loss 0.0340586, acc 0.98
2016-09-05T23:17:47.501268: step 18019, loss 0.0601012, acc 0.98
2016-09-05T23:17:48.318714: step 18020, loss 0.0306676, acc 0.98
2016-09-05T23:17:49.129544: step 18021, loss 0.0186265, acc 0.98
2016-09-05T23:17:49.965192: step 18022, loss 0.0127068, acc 1
2016-09-05T23:17:50.748454: step 18023, loss 0.0143874, acc 1
2016-09-05T23:17:51.537684: step 18024, loss 0.00769039, acc 1
2016-09-05T23:17:52.382728: step 18025, loss 0.00585477, acc 1
2016-09-05T23:17:53.193631: step 18026, loss 0.0104402, acc 1
2016-09-05T23:17:53.987263: step 18027, loss 0.0028288, acc 1
2016-09-05T23:17:54.819131: step 18028, loss 0.0176018, acc 0.98
2016-09-05T23:17:55.635587: step 18029, loss 0.0271075, acc 0.98
2016-09-05T23:17:56.451106: step 18030, loss 0.00515752, acc 1
2016-09-05T23:17:57.336959: step 18031, loss 0.00259654, acc 1
2016-09-05T23:17:58.157556: step 18032, loss 0.0153028, acc 1
2016-09-05T23:17:58.965485: step 18033, loss 0.00623069, acc 1
2016-09-05T23:17:59.781715: step 18034, loss 0.00263885, acc 1
2016-09-05T23:18:00.615516: step 18035, loss 0.022999, acc 1
2016-09-05T23:18:01.419958: step 18036, loss 0.00248681, acc 1
2016-09-05T23:18:02.240604: step 18037, loss 0.0024983, acc 1
2016-09-05T23:18:03.087772: step 18038, loss 0.0636286, acc 0.96
2016-09-05T23:18:03.862844: step 18039, loss 0.0267003, acc 0.98
2016-09-05T23:18:04.666631: step 18040, loss 0.00608446, acc 1
2016-09-05T23:18:05.502365: step 18041, loss 0.00309411, acc 1
2016-09-05T23:18:05.908164: step 18042, loss 0.00237341, acc 1
2016-09-05T23:18:06.745252: step 18043, loss 0.00269757, acc 1
2016-09-05T23:18:07.551524: step 18044, loss 0.00240294, acc 1
2016-09-05T23:18:08.359517: step 18045, loss 0.0336353, acc 1
2016-09-05T23:18:09.204296: step 18046, loss 0.0204973, acc 0.98
2016-09-05T23:18:10.064161: step 18047, loss 0.0171085, acc 0.98
2016-09-05T23:18:10.897483: step 18048, loss 0.00376797, acc 1
2016-09-05T23:18:11.747637: step 18049, loss 0.0305269, acc 0.98
2016-09-05T23:18:12.531327: step 18050, loss 0.00623692, acc 1
2016-09-05T23:18:13.331782: step 18051, loss 0.00281883, acc 1
2016-09-05T23:18:14.164111: step 18052, loss 0.00521933, acc 1
2016-09-05T23:18:14.977367: step 18053, loss 0.00269534, acc 1
2016-09-05T23:18:15.793267: step 18054, loss 0.00352813, acc 1
2016-09-05T23:18:16.622230: step 18055, loss 0.0472121, acc 0.96
2016-09-05T23:18:17.439775: step 18056, loss 0.015113, acc 1
2016-09-05T23:18:18.306124: step 18057, loss 0.00234095, acc 1
2016-09-05T23:18:19.131466: step 18058, loss 0.0030061, acc 1
2016-09-05T23:18:19.955363: step 18059, loss 0.053687, acc 0.96
2016-09-05T23:18:20.752712: step 18060, loss 0.00280245, acc 1
2016-09-05T23:18:21.550031: step 18061, loss 0.00340039, acc 1
2016-09-05T23:18:22.374603: step 18062, loss 0.00233085, acc 1
2016-09-05T23:18:23.150094: step 18063, loss 0.00350532, acc 1
2016-09-05T23:18:23.970266: step 18064, loss 0.00828744, acc 1
2016-09-05T23:18:24.793128: step 18065, loss 0.0207986, acc 0.98
2016-09-05T23:18:25.593924: step 18066, loss 0.00232698, acc 1
2016-09-05T23:18:26.386308: step 18067, loss 0.00322358, acc 1
2016-09-05T23:18:27.226836: step 18068, loss 0.0173681, acc 0.98
2016-09-05T23:18:28.061052: step 18069, loss 0.00223628, acc 1
2016-09-05T23:18:28.882818: step 18070, loss 0.140125, acc 0.98
2016-09-05T23:18:29.690366: step 18071, loss 0.0108101, acc 1
2016-09-05T23:18:30.441090: step 18072, loss 0.002125, acc 1
2016-09-05T23:18:31.253991: step 18073, loss 0.00470493, acc 1
2016-09-05T23:18:32.079638: step 18074, loss 0.0357489, acc 1
2016-09-05T23:18:32.881847: step 18075, loss 0.00766893, acc 1
2016-09-05T23:18:33.737512: step 18076, loss 0.00296506, acc 1
2016-09-05T23:18:34.557643: step 18077, loss 0.00226101, acc 1
2016-09-05T23:18:35.342683: step 18078, loss 0.0256768, acc 1
2016-09-05T23:18:36.178506: step 18079, loss 0.00894908, acc 1
2016-09-05T23:18:37.004465: step 18080, loss 0.00580059, acc 1
2016-09-05T23:18:37.834792: step 18081, loss 0.00242769, acc 1
2016-09-05T23:18:38.625353: step 18082, loss 0.00260936, acc 1
2016-09-05T23:18:39.466968: step 18083, loss 0.12429, acc 0.98
2016-09-05T23:18:40.286797: step 18084, loss 0.00916322, acc 1
2016-09-05T23:18:41.110187: step 18085, loss 0.00831494, acc 1
2016-09-05T23:18:41.971157: step 18086, loss 0.0159277, acc 1
2016-09-05T23:18:42.791076: step 18087, loss 0.0523199, acc 0.96
2016-09-05T23:18:43.616194: step 18088, loss 0.035615, acc 0.98
2016-09-05T23:18:44.443550: step 18089, loss 0.0185358, acc 0.98
2016-09-05T23:18:45.255196: step 18090, loss 0.0410042, acc 0.98
2016-09-05T23:18:46.059015: step 18091, loss 0.00209273, acc 1
2016-09-05T23:18:46.894469: step 18092, loss 0.010933, acc 1
2016-09-05T23:18:47.700601: step 18093, loss 0.00292203, acc 1
2016-09-05T23:18:48.505053: step 18094, loss 0.00208011, acc 1
2016-09-05T23:18:49.326359: step 18095, loss 0.0122554, acc 1
2016-09-05T23:18:50.117446: step 18096, loss 0.0699375, acc 0.94
2016-09-05T23:18:50.897466: step 18097, loss 0.00566565, acc 1
2016-09-05T23:18:51.736969: step 18098, loss 0.00365662, acc 1
2016-09-05T23:18:52.547599: step 18099, loss 0.00310157, acc 1
2016-09-05T23:18:53.350936: step 18100, loss 0.0143985, acc 1

Evaluation:
2016-09-05T23:18:56.873881: step 18100, loss 2.25344, acc 0.714

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-18100

2016-09-05T23:18:58.875563: step 18101, loss 0.00196384, acc 1
2016-09-05T23:18:59.699647: step 18102, loss 0.00204186, acc 1
2016-09-05T23:19:00.541812: step 18103, loss 0.0286666, acc 0.98
2016-09-05T23:19:01.354984: step 18104, loss 0.0298984, acc 0.98
2016-09-05T23:19:02.156871: step 18105, loss 0.00988481, acc 1
2016-09-05T23:19:02.954039: step 18106, loss 0.0101441, acc 1
2016-09-05T23:19:03.773244: step 18107, loss 0.00308799, acc 1
2016-09-05T23:19:04.578636: step 18108, loss 0.00214973, acc 1
2016-09-05T23:19:05.379625: step 18109, loss 0.00512439, acc 1
2016-09-05T23:19:06.195869: step 18110, loss 0.00806295, acc 1
2016-09-05T23:19:07.005249: step 18111, loss 0.00228697, acc 1
2016-09-05T23:19:07.804248: step 18112, loss 0.0463451, acc 0.98
2016-09-05T23:19:08.673123: step 18113, loss 0.00349016, acc 1
2016-09-05T23:19:09.470739: step 18114, loss 0.00209979, acc 1
2016-09-05T23:19:10.293449: step 18115, loss 0.00212802, acc 1
2016-09-05T23:19:11.116319: step 18116, loss 0.011031, acc 1
2016-09-05T23:19:11.920372: step 18117, loss 0.00303558, acc 1
2016-09-05T23:19:12.733480: step 18118, loss 0.0239239, acc 0.98
2016-09-05T23:19:13.565328: step 18119, loss 0.0069467, acc 1
2016-09-05T23:19:14.382729: step 18120, loss 0.00284706, acc 1
2016-09-05T23:19:15.151687: step 18121, loss 0.0169063, acc 0.98
2016-09-05T23:19:15.959582: step 18122, loss 0.0022966, acc 1
2016-09-05T23:19:16.793362: step 18123, loss 0.0315282, acc 0.98
2016-09-05T23:19:17.592390: step 18124, loss 0.00226711, acc 1
2016-09-05T23:19:18.405618: step 18125, loss 0.0464469, acc 0.98
2016-09-05T23:19:19.216250: step 18126, loss 0.00859465, acc 1
2016-09-05T23:19:20.078267: step 18127, loss 0.0166192, acc 1
2016-09-05T23:19:20.871026: step 18128, loss 0.0250663, acc 1
2016-09-05T23:19:21.695630: step 18129, loss 0.0204692, acc 0.98
2016-09-05T23:19:22.501343: step 18130, loss 0.0100879, acc 1
2016-09-05T23:19:23.348982: step 18131, loss 0.00530961, acc 1
2016-09-05T23:19:24.194910: step 18132, loss 0.0106691, acc 1
2016-09-05T23:19:25.002487: step 18133, loss 0.00538039, acc 1
2016-09-05T23:19:25.813189: step 18134, loss 0.0145275, acc 1
2016-09-05T23:19:26.641383: step 18135, loss 0.0132618, acc 1
2016-09-05T23:19:27.451809: step 18136, loss 0.00720517, acc 1
2016-09-05T23:19:28.258185: step 18137, loss 0.0146817, acc 1
2016-09-05T23:19:29.074061: step 18138, loss 0.00236552, acc 1
2016-09-05T23:19:29.901613: step 18139, loss 0.00823396, acc 1
2016-09-05T23:19:30.753931: step 18140, loss 0.00828945, acc 1
2016-09-05T23:19:31.572208: step 18141, loss 0.00383259, acc 1
2016-09-05T23:19:32.395351: step 18142, loss 0.0172582, acc 0.98
2016-09-05T23:19:33.194042: step 18143, loss 0.00326113, acc 1
2016-09-05T23:19:34.071756: step 18144, loss 0.00456445, acc 1
2016-09-05T23:19:34.880516: step 18145, loss 0.0592219, acc 0.94
2016-09-05T23:19:35.700254: step 18146, loss 0.0467277, acc 0.98
2016-09-05T23:19:36.536131: step 18147, loss 0.0111258, acc 1
2016-09-05T23:19:37.371436: step 18148, loss 0.0234916, acc 0.98
2016-09-05T23:19:38.146111: step 18149, loss 0.0022007, acc 1
2016-09-05T23:19:38.947100: step 18150, loss 0.00215642, acc 1
2016-09-05T23:19:39.763991: step 18151, loss 0.0465928, acc 0.98
2016-09-05T23:19:40.537975: step 18152, loss 0.0171867, acc 0.98
2016-09-05T23:19:41.354866: step 18153, loss 0.044131, acc 0.96
2016-09-05T23:19:42.176868: step 18154, loss 0.00203724, acc 1
2016-09-05T23:19:42.957759: step 18155, loss 0.0020081, acc 1
2016-09-05T23:19:43.739246: step 18156, loss 0.0197943, acc 0.98
2016-09-05T23:19:44.547360: step 18157, loss 0.0127602, acc 1
2016-09-05T23:19:45.337449: step 18158, loss 0.113183, acc 0.96
2016-09-05T23:19:46.160360: step 18159, loss 0.00198614, acc 1
2016-09-05T23:19:46.975026: step 18160, loss 0.00345513, acc 1
2016-09-05T23:19:47.777617: step 18161, loss 0.00185132, acc 1
2016-09-05T23:19:48.591123: step 18162, loss 0.00461273, acc 1
2016-09-05T23:19:49.442650: step 18163, loss 0.00329836, acc 1
2016-09-05T23:19:50.253832: step 18164, loss 0.00234912, acc 1
2016-09-05T23:19:51.060040: step 18165, loss 0.00257704, acc 1
2016-09-05T23:19:51.875048: step 18166, loss 0.00192958, acc 1
2016-09-05T23:19:52.677249: step 18167, loss 0.0126756, acc 1
2016-09-05T23:19:53.482287: step 18168, loss 0.0296372, acc 0.98
2016-09-05T23:19:54.326058: step 18169, loss 0.0205456, acc 0.98
2016-09-05T23:19:55.123306: step 18170, loss 0.00333571, acc 1
2016-09-05T23:19:55.936249: step 18171, loss 0.00199004, acc 1
2016-09-05T23:19:56.729202: step 18172, loss 0.00825843, acc 1
2016-09-05T23:19:57.505492: step 18173, loss 0.00625807, acc 1
2016-09-05T23:19:58.297152: step 18174, loss 0.0996306, acc 0.96
2016-09-05T23:19:59.132707: step 18175, loss 0.0035994, acc 1
2016-09-05T23:19:59.899104: step 18176, loss 0.0226946, acc 1
2016-09-05T23:20:00.712026: step 18177, loss 0.00404444, acc 1
2016-09-05T23:20:01.527659: step 18178, loss 0.0179541, acc 1
2016-09-05T23:20:02.326262: step 18179, loss 0.00294216, acc 1
2016-09-05T23:20:03.143325: step 18180, loss 0.00172149, acc 1
2016-09-05T23:20:03.954795: step 18181, loss 0.00408827, acc 1
2016-09-05T23:20:04.727922: step 18182, loss 0.0130497, acc 1
2016-09-05T23:20:05.521363: step 18183, loss 0.00388751, acc 1
2016-09-05T23:20:06.337659: step 18184, loss 0.00456412, acc 1
2016-09-05T23:20:07.117608: step 18185, loss 0.00175859, acc 1
2016-09-05T23:20:07.916404: step 18186, loss 0.0024619, acc 1
2016-09-05T23:20:08.711679: step 18187, loss 0.00793439, acc 1
2016-09-05T23:20:09.503429: step 18188, loss 0.00211937, acc 1
2016-09-05T23:20:10.320248: step 18189, loss 0.00290897, acc 1
2016-09-05T23:20:11.142601: step 18190, loss 0.00158784, acc 1
2016-09-05T23:20:11.971142: step 18191, loss 0.00236527, acc 1
2016-09-05T23:20:12.765942: step 18192, loss 0.0016092, acc 1
2016-09-05T23:20:13.585443: step 18193, loss 0.024634, acc 0.98
2016-09-05T23:20:14.411527: step 18194, loss 0.0961529, acc 0.96
2016-09-05T23:20:15.223132: step 18195, loss 0.0127644, acc 1
2016-09-05T23:20:16.032636: step 18196, loss 0.0371756, acc 0.98
2016-09-05T23:20:16.819883: step 18197, loss 0.0115879, acc 1
2016-09-05T23:20:17.628475: step 18198, loss 0.00276775, acc 1
2016-09-05T23:20:18.449852: step 18199, loss 0.00156893, acc 1
2016-09-05T23:20:19.248973: step 18200, loss 0.0114779, acc 1

Evaluation:
2016-09-05T23:20:22.756395: step 18200, loss 2.21617, acc 0.723

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-18200

2016-09-05T23:20:24.655705: step 18201, loss 0.00149948, acc 1
2016-09-05T23:20:25.456978: step 18202, loss 0.00148844, acc 1
2016-09-05T23:20:26.256346: step 18203, loss 0.00341909, acc 1
2016-09-05T23:20:27.083170: step 18204, loss 0.0025109, acc 1
2016-09-05T23:20:27.891540: step 18205, loss 0.0179845, acc 1
2016-09-05T23:20:28.724057: step 18206, loss 0.0031027, acc 1
2016-09-05T23:20:29.549867: step 18207, loss 0.00698202, acc 1
2016-09-05T23:20:30.366950: step 18208, loss 0.0207656, acc 1
2016-09-05T23:20:31.178217: step 18209, loss 0.0390985, acc 0.98
2016-09-05T23:20:32.005499: step 18210, loss 0.0123162, acc 1
2016-09-05T23:20:32.801913: step 18211, loss 0.0225869, acc 0.98
2016-09-05T23:20:33.603202: step 18212, loss 0.0331888, acc 1
2016-09-05T23:20:34.437689: step 18213, loss 0.030627, acc 1
2016-09-05T23:20:35.248435: step 18214, loss 0.0162564, acc 1
2016-09-05T23:20:36.061085: step 18215, loss 0.0257956, acc 0.98
2016-09-05T23:20:36.901193: step 18216, loss 0.00187322, acc 1
2016-09-05T23:20:37.730625: step 18217, loss 0.10419, acc 0.98
2016-09-05T23:20:38.532557: step 18218, loss 0.00894989, acc 1
2016-09-05T23:20:39.356302: step 18219, loss 0.00867028, acc 1
2016-09-05T23:20:40.150510: step 18220, loss 0.00180557, acc 1
2016-09-05T23:20:40.962724: step 18221, loss 0.00188182, acc 1
2016-09-05T23:20:41.827949: step 18222, loss 0.0141962, acc 1
2016-09-05T23:20:42.653882: step 18223, loss 0.0182881, acc 0.98
2016-09-05T23:20:43.470002: step 18224, loss 0.00268088, acc 1
2016-09-05T23:20:44.298355: step 18225, loss 0.0312219, acc 0.96
2016-09-05T23:20:45.113929: step 18226, loss 0.0135019, acc 1
2016-09-05T23:20:45.908812: step 18227, loss 0.00198683, acc 1
2016-09-05T23:20:46.717714: step 18228, loss 0.0135969, acc 1
2016-09-05T23:20:47.530042: step 18229, loss 0.0172298, acc 0.98
2016-09-05T23:20:48.362661: step 18230, loss 0.00274118, acc 1
2016-09-05T23:20:49.170869: step 18231, loss 0.00426227, acc 1
2016-09-05T23:20:49.987340: step 18232, loss 0.028922, acc 0.98
2016-09-05T23:20:50.779150: step 18233, loss 0.0172227, acc 0.98
2016-09-05T23:20:51.591667: step 18234, loss 0.00312319, acc 1
2016-09-05T23:20:52.420109: step 18235, loss 0.00578178, acc 1
2016-09-05T23:20:52.839274: step 18236, loss 0.212658, acc 0.916667
2016-09-05T23:20:53.662558: step 18237, loss 0.0270204, acc 1
2016-09-05T23:20:54.462363: step 18238, loss 0.00700994, acc 1
2016-09-05T23:20:55.270466: step 18239, loss 0.0248273, acc 1
2016-09-05T23:20:56.078210: step 18240, loss 0.00314777, acc 1
2016-09-05T23:20:56.895772: step 18241, loss 0.00219838, acc 1
2016-09-05T23:20:57.701816: step 18242, loss 0.0140732, acc 1
2016-09-05T23:20:58.506984: step 18243, loss 0.00207672, acc 1
2016-09-05T23:20:59.296582: step 18244, loss 0.031292, acc 0.98
2016-09-05T23:21:00.102054: step 18245, loss 0.00263203, acc 1
2016-09-05T23:21:00.932628: step 18246, loss 0.00274255, acc 1
2016-09-05T23:21:01.719410: step 18247, loss 0.027126, acc 1
2016-09-05T23:21:02.505475: step 18248, loss 0.00646792, acc 1
2016-09-05T23:21:03.348428: step 18249, loss 0.0362067, acc 0.98
2016-09-05T23:21:04.154761: step 18250, loss 0.0122604, acc 1
2016-09-05T23:21:04.965356: step 18251, loss 0.00230261, acc 1
2016-09-05T23:21:05.841340: step 18252, loss 0.00272738, acc 1
2016-09-05T23:21:06.646401: step 18253, loss 0.0062018, acc 1
2016-09-05T23:21:07.469987: step 18254, loss 0.0025412, acc 1
2016-09-05T23:21:08.291908: step 18255, loss 0.0165048, acc 1
2016-09-05T23:21:09.095479: step 18256, loss 0.00706344, acc 1
2016-09-05T23:21:09.911935: step 18257, loss 0.0176232, acc 0.98
2016-09-05T23:21:10.770185: step 18258, loss 0.0174906, acc 1
2016-09-05T23:21:11.577498: step 18259, loss 0.0170613, acc 1
2016-09-05T23:21:12.366098: step 18260, loss 0.00268524, acc 1
2016-09-05T23:21:13.201071: step 18261, loss 0.0181263, acc 0.98
2016-09-05T23:21:14.007041: step 18262, loss 0.00265054, acc 1
2016-09-05T23:21:14.798989: step 18263, loss 0.00253952, acc 1
2016-09-05T23:21:15.631081: step 18264, loss 0.00961208, acc 1
2016-09-05T23:21:16.462023: step 18265, loss 0.0881351, acc 0.98
2016-09-05T23:21:17.283249: step 18266, loss 0.0222003, acc 0.98
2016-09-05T23:21:18.131745: step 18267, loss 0.0110578, acc 1
2016-09-05T23:21:18.941201: step 18268, loss 0.0164201, acc 1
2016-09-05T23:21:19.742208: step 18269, loss 0.0233592, acc 0.98
2016-09-05T23:21:20.553323: step 18270, loss 0.00256292, acc 1
2016-09-05T23:21:21.344796: step 18271, loss 0.00256742, acc 1
2016-09-05T23:21:22.161477: step 18272, loss 0.00937927, acc 1
2016-09-05T23:21:22.977907: step 18273, loss 0.00240258, acc 1
2016-09-05T23:21:23.771966: step 18274, loss 0.00852577, acc 1
2016-09-05T23:21:24.597425: step 18275, loss 0.00238636, acc 1
2016-09-05T23:21:25.407048: step 18276, loss 0.0023756, acc 1
2016-09-05T23:21:26.197738: step 18277, loss 0.0127818, acc 1
2016-09-05T23:21:26.998538: step 18278, loss 0.0346094, acc 1
2016-09-05T23:21:27.860965: step 18279, loss 0.00238744, acc 1
2016-09-05T23:21:28.661608: step 18280, loss 0.00239185, acc 1
2016-09-05T23:21:29.438894: step 18281, loss 0.0146379, acc 1
2016-09-05T23:21:30.244069: step 18282, loss 0.0520464, acc 0.96
2016-09-05T23:21:31.077578: step 18283, loss 0.004556, acc 1
2016-09-05T23:21:31.867039: step 18284, loss 0.032925, acc 0.98
2016-09-05T23:21:32.671748: step 18285, loss 0.00304624, acc 1
2016-09-05T23:21:33.483919: step 18286, loss 0.0324948, acc 0.98
2016-09-05T23:21:34.300412: step 18287, loss 0.0119523, acc 1
2016-09-05T23:21:35.127276: step 18288, loss 0.00224551, acc 1
2016-09-05T23:21:35.954439: step 18289, loss 0.00222179, acc 1
2016-09-05T23:21:36.733277: step 18290, loss 0.00261621, acc 1
2016-09-05T23:21:37.538830: step 18291, loss 0.00219181, acc 1
2016-09-05T23:21:38.350881: step 18292, loss 0.0293056, acc 0.98
2016-09-05T23:21:39.130894: step 18293, loss 0.0062363, acc 1
2016-09-05T23:21:39.931620: step 18294, loss 0.00222323, acc 1
2016-09-05T23:21:40.722439: step 18295, loss 0.0391293, acc 0.98
2016-09-05T23:21:41.506563: step 18296, loss 0.0203334, acc 1
2016-09-05T23:21:42.309797: step 18297, loss 0.014603, acc 1
2016-09-05T23:21:43.129418: step 18298, loss 0.00243822, acc 1
2016-09-05T23:21:43.922048: step 18299, loss 0.0144448, acc 1
2016-09-05T23:21:44.762941: step 18300, loss 0.0217658, acc 0.98

Evaluation:
2016-09-05T23:21:48.275878: step 18300, loss 2.94421, acc 0.704

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-18300

2016-09-05T23:21:50.160328: step 18301, loss 0.0103759, acc 1
2016-09-05T23:21:50.974778: step 18302, loss 0.0163037, acc 1
2016-09-05T23:21:51.776490: step 18303, loss 0.0239905, acc 1
2016-09-05T23:21:52.594244: step 18304, loss 0.00774224, acc 1
2016-09-05T23:21:53.371897: step 18305, loss 0.00249892, acc 1
2016-09-05T23:21:54.229487: step 18306, loss 0.00721656, acc 1
2016-09-05T23:21:55.044953: step 18307, loss 0.0028993, acc 1
2016-09-05T23:21:55.862408: step 18308, loss 0.018027, acc 0.98
2016-09-05T23:21:56.704011: step 18309, loss 0.00203673, acc 1
2016-09-05T23:21:57.497770: step 18310, loss 0.00206439, acc 1
2016-09-05T23:21:58.329298: step 18311, loss 0.00344104, acc 1
2016-09-05T23:21:59.161462: step 18312, loss 0.0238089, acc 1
2016-09-05T23:21:59.982910: step 18313, loss 0.0132456, acc 1
2016-09-05T23:22:00.798622: step 18314, loss 0.0753129, acc 0.98
2016-09-05T23:22:01.626178: step 18315, loss 0.0615208, acc 0.96
2016-09-05T23:22:02.460102: step 18316, loss 0.0178004, acc 1
2016-09-05T23:22:03.291164: step 18317, loss 0.00190805, acc 1
2016-09-05T23:22:04.129869: step 18318, loss 0.0374571, acc 0.98
2016-09-05T23:22:04.955477: step 18319, loss 0.0734128, acc 0.96
2016-09-05T23:22:05.757650: step 18320, loss 0.0198772, acc 0.98
2016-09-05T23:22:06.576549: step 18321, loss 0.00395915, acc 1
2016-09-05T23:22:07.423431: step 18322, loss 0.00442929, acc 1
2016-09-05T23:22:08.222316: step 18323, loss 0.0678494, acc 0.98
2016-09-05T23:22:09.023754: step 18324, loss 0.00382938, acc 1
2016-09-05T23:22:09.821401: step 18325, loss 0.00395275, acc 1
2016-09-05T23:22:10.594859: step 18326, loss 0.010998, acc 1
2016-09-05T23:22:11.401480: step 18327, loss 0.0345818, acc 0.98
2016-09-05T23:22:12.208710: step 18328, loss 0.0191426, acc 1
2016-09-05T23:22:13.014173: step 18329, loss 0.00633284, acc 1
2016-09-05T23:22:13.825714: step 18330, loss 0.00500335, acc 1
2016-09-05T23:22:14.646395: step 18331, loss 0.0226029, acc 1
2016-09-05T23:22:15.421758: step 18332, loss 0.0160652, acc 1
2016-09-05T23:22:16.238709: step 18333, loss 0.0232315, acc 1
2016-09-05T23:22:17.059263: step 18334, loss 0.00931914, acc 1
2016-09-05T23:22:17.886136: step 18335, loss 0.0158893, acc 1
2016-09-05T23:22:18.708199: step 18336, loss 0.0294036, acc 1
2016-09-05T23:22:19.517413: step 18337, loss 0.108185, acc 0.98
2016-09-05T23:22:20.307200: step 18338, loss 0.00294418, acc 1
2016-09-05T23:22:21.097521: step 18339, loss 0.00812099, acc 1
2016-09-05T23:22:21.932200: step 18340, loss 0.00341408, acc 1
2016-09-05T23:22:22.724629: step 18341, loss 0.00296656, acc 1
2016-09-05T23:22:23.516784: step 18342, loss 0.0924185, acc 0.96
2016-09-05T23:22:24.340794: step 18343, loss 0.0177556, acc 0.98
2016-09-05T23:22:25.106561: step 18344, loss 0.00846731, acc 1
2016-09-05T23:22:25.907781: step 18345, loss 0.0113879, acc 1
2016-09-05T23:22:26.753283: step 18346, loss 0.0230954, acc 1
2016-09-05T23:22:27.518948: step 18347, loss 0.0154437, acc 1
2016-09-05T23:22:28.313586: step 18348, loss 0.00335749, acc 1
2016-09-05T23:22:29.130541: step 18349, loss 0.0171484, acc 0.98
2016-09-05T23:22:29.933994: step 18350, loss 0.0165392, acc 1
2016-09-05T23:22:30.743644: step 18351, loss 0.00717489, acc 1
2016-09-05T23:22:31.549740: step 18352, loss 0.0170225, acc 0.98
2016-09-05T23:22:32.351186: step 18353, loss 0.00304718, acc 1
2016-09-05T23:22:33.148609: step 18354, loss 0.0170338, acc 0.98
2016-09-05T23:22:33.960411: step 18355, loss 0.00291558, acc 1
2016-09-05T23:22:34.743189: step 18356, loss 0.0115533, acc 1
2016-09-05T23:22:35.554223: step 18357, loss 0.00298879, acc 1
2016-09-05T23:22:36.358519: step 18358, loss 0.0288808, acc 0.98
2016-09-05T23:22:37.145611: step 18359, loss 0.00302419, acc 1
2016-09-05T23:22:37.960975: step 18360, loss 0.00273762, acc 1
2016-09-05T23:22:38.772653: step 18361, loss 0.0519307, acc 0.98
2016-09-05T23:22:39.556275: step 18362, loss 0.0137952, acc 1
2016-09-05T23:22:40.346790: step 18363, loss 0.0219976, acc 0.98
2016-09-05T23:22:41.154249: step 18364, loss 0.00443936, acc 1
2016-09-05T23:22:41.985718: step 18365, loss 0.00843142, acc 1
2016-09-05T23:22:42.797950: step 18366, loss 0.0164883, acc 1
2016-09-05T23:22:43.599578: step 18367, loss 0.00806438, acc 1
2016-09-05T23:22:44.410121: step 18368, loss 0.00282907, acc 1
2016-09-05T23:22:45.206685: step 18369, loss 0.0111308, acc 1
2016-09-05T23:22:46.022812: step 18370, loss 0.00648091, acc 1
2016-09-05T23:22:46.821494: step 18371, loss 0.00458561, acc 1
2016-09-05T23:22:47.628347: step 18372, loss 0.0296214, acc 0.98
2016-09-05T23:22:48.475008: step 18373, loss 0.0207528, acc 0.98
2016-09-05T23:22:49.274634: step 18374, loss 0.00254672, acc 1
2016-09-05T23:22:50.071274: step 18375, loss 0.00834413, acc 1
2016-09-05T23:22:50.888709: step 18376, loss 0.0855564, acc 0.96
2016-09-05T23:22:51.671403: step 18377, loss 0.00806727, acc 1
2016-09-05T23:22:52.472790: step 18378, loss 0.0541847, acc 0.98
2016-09-05T23:22:53.299306: step 18379, loss 0.0219892, acc 0.98
2016-09-05T23:22:54.090715: step 18380, loss 0.00256171, acc 1
2016-09-05T23:22:54.899461: step 18381, loss 0.0343535, acc 1
2016-09-05T23:22:55.706221: step 18382, loss 0.00243, acc 1
2016-09-05T23:22:56.553718: step 18383, loss 0.00583293, acc 1
2016-09-05T23:22:57.368987: step 18384, loss 0.0224884, acc 0.98
2016-09-05T23:22:58.213526: step 18385, loss 0.0157543, acc 1
2016-09-05T23:22:59.024230: step 18386, loss 0.017725, acc 0.98
2016-09-05T23:22:59.838452: step 18387, loss 0.0302838, acc 0.98
2016-09-05T23:23:00.695787: step 18388, loss 0.036894, acc 0.98
2016-09-05T23:23:01.515658: step 18389, loss 0.0268199, acc 0.98
2016-09-05T23:23:02.327176: step 18390, loss 0.0197452, acc 1
2016-09-05T23:23:03.159808: step 18391, loss 0.0268671, acc 0.98
2016-09-05T23:23:03.982158: step 18392, loss 0.0022931, acc 1
2016-09-05T23:23:04.806138: step 18393, loss 0.00230654, acc 1
2016-09-05T23:23:05.624464: step 18394, loss 0.0118906, acc 1
2016-09-05T23:23:06.457827: step 18395, loss 0.0025854, acc 1
2016-09-05T23:23:07.262137: step 18396, loss 0.0225849, acc 0.98
2016-09-05T23:23:08.093286: step 18397, loss 0.00261189, acc 1
2016-09-05T23:23:08.906052: step 18398, loss 0.0040812, acc 1
2016-09-05T23:23:09.705670: step 18399, loss 0.0142986, acc 1
2016-09-05T23:23:10.525493: step 18400, loss 0.00681405, acc 1

Evaluation:
2016-09-05T23:23:14.023220: step 18400, loss 2.74763, acc 0.707

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-18400

2016-09-05T23:23:15.888720: step 18401, loss 0.0350455, acc 0.98
2016-09-05T23:23:16.746894: step 18402, loss 0.0396088, acc 0.98
2016-09-05T23:23:17.574631: step 18403, loss 0.0022512, acc 1
2016-09-05T23:23:18.355262: step 18404, loss 0.0284919, acc 0.98
2016-09-05T23:23:19.156690: step 18405, loss 0.0158833, acc 1
2016-09-05T23:23:19.966760: step 18406, loss 0.0400767, acc 0.98
2016-09-05T23:23:20.734101: step 18407, loss 0.00697373, acc 1
2016-09-05T23:23:21.564689: step 18408, loss 0.0188183, acc 0.98
2016-09-05T23:23:22.367068: step 18409, loss 0.0119912, acc 1
2016-09-05T23:23:23.172605: step 18410, loss 0.00234023, acc 1
2016-09-05T23:23:23.982578: step 18411, loss 0.0131284, acc 1
2016-09-05T23:23:24.808982: step 18412, loss 0.00752138, acc 1
2016-09-05T23:23:25.596816: step 18413, loss 0.00246413, acc 1
2016-09-05T23:23:26.486265: step 18414, loss 0.00544012, acc 1
2016-09-05T23:23:27.305026: step 18415, loss 0.00687467, acc 1
2016-09-05T23:23:28.114517: step 18416, loss 0.0187877, acc 0.98
2016-09-05T23:23:28.914760: step 18417, loss 0.00577653, acc 1
2016-09-05T23:23:29.725654: step 18418, loss 0.00284602, acc 1
2016-09-05T23:23:30.525537: step 18419, loss 0.015714, acc 1
2016-09-05T23:23:31.320762: step 18420, loss 0.0106879, acc 1
2016-09-05T23:23:32.141949: step 18421, loss 0.0429646, acc 0.98
2016-09-05T23:23:32.942084: step 18422, loss 0.00269825, acc 1
2016-09-05T23:23:33.752728: step 18423, loss 0.0168314, acc 0.98
2016-09-05T23:23:34.584825: step 18424, loss 0.00228687, acc 1
2016-09-05T23:23:35.377767: step 18425, loss 0.0134561, acc 1
2016-09-05T23:23:36.176135: step 18426, loss 0.00991328, acc 1
2016-09-05T23:23:36.984814: step 18427, loss 0.0151582, acc 1
2016-09-05T23:23:37.760218: step 18428, loss 0.0178282, acc 0.98
2016-09-05T23:23:38.533429: step 18429, loss 0.00247002, acc 1
2016-09-05T23:23:38.968762: step 18430, loss 0.00222296, acc 1
2016-09-05T23:23:39.783348: step 18431, loss 0.0776798, acc 0.94
2016-09-05T23:23:40.597860: step 18432, loss 0.002382, acc 1
2016-09-05T23:23:41.386016: step 18433, loss 0.024064, acc 0.98
2016-09-05T23:23:42.171954: step 18434, loss 0.0091342, acc 1
2016-09-05T23:23:43.003338: step 18435, loss 0.00263209, acc 1
2016-09-05T23:23:43.782358: step 18436, loss 0.0176656, acc 1
2016-09-05T23:23:44.585290: step 18437, loss 0.0208031, acc 0.98
2016-09-05T23:23:45.395176: step 18438, loss 0.0173088, acc 0.98
2016-09-05T23:23:46.171566: step 18439, loss 0.0120905, acc 1
2016-09-05T23:23:46.983528: step 18440, loss 0.00457996, acc 1
2016-09-05T23:23:47.803535: step 18441, loss 0.0130627, acc 1
2016-09-05T23:23:48.655849: step 18442, loss 0.0196545, acc 0.98
2016-09-05T23:23:49.457494: step 18443, loss 0.0217362, acc 0.98
2016-09-05T23:23:50.256110: step 18444, loss 0.00217433, acc 1
2016-09-05T23:23:51.033910: step 18445, loss 0.00676072, acc 1
2016-09-05T23:23:51.893471: step 18446, loss 0.00203575, acc 1
2016-09-05T23:23:52.704484: step 18447, loss 0.0037704, acc 1
2016-09-05T23:23:53.505637: step 18448, loss 0.00214595, acc 1
2016-09-05T23:23:54.321233: step 18449, loss 0.00235007, acc 1
2016-09-05T23:23:55.159916: step 18450, loss 0.00201863, acc 1
2016-09-05T23:23:55.973780: step 18451, loss 0.00348374, acc 1
2016-09-05T23:23:56.776322: step 18452, loss 0.0288105, acc 0.98
2016-09-05T23:23:57.615902: step 18453, loss 0.0211686, acc 0.98
2016-09-05T23:23:58.426951: step 18454, loss 0.00523231, acc 1
2016-09-05T23:23:59.237582: step 18455, loss 0.00205177, acc 1
2016-09-05T23:24:00.073818: step 18456, loss 0.00252869, acc 1
2016-09-05T23:24:00.900601: step 18457, loss 0.00794035, acc 1
2016-09-05T23:24:01.748337: step 18458, loss 0.00204571, acc 1
2016-09-05T23:24:02.584088: step 18459, loss 0.00332492, acc 1
2016-09-05T23:24:03.439841: step 18460, loss 0.0125878, acc 1
2016-09-05T23:24:04.271819: step 18461, loss 0.0248257, acc 0.98
2016-09-05T23:24:05.094884: step 18462, loss 0.0450867, acc 0.96
2016-09-05T23:24:05.941586: step 18463, loss 0.0112081, acc 1
2016-09-05T23:24:06.797682: step 18464, loss 0.00180358, acc 1
2016-09-05T23:24:07.586258: step 18465, loss 0.0141486, acc 1
2016-09-05T23:24:08.429694: step 18466, loss 0.00180012, acc 1
2016-09-05T23:24:09.232164: step 18467, loss 0.00180237, acc 1
2016-09-05T23:24:10.022858: step 18468, loss 0.0292709, acc 0.98
2016-09-05T23:24:10.869161: step 18469, loss 0.00173704, acc 1
2016-09-05T23:24:11.697832: step 18470, loss 0.00561745, acc 1
2016-09-05T23:24:12.515458: step 18471, loss 0.0216024, acc 1
2016-09-05T23:24:13.351268: step 18472, loss 0.00292743, acc 1
2016-09-05T23:24:14.154403: step 18473, loss 0.0124129, acc 1
2016-09-05T23:24:14.993574: step 18474, loss 0.00456503, acc 1
2016-09-05T23:24:15.818045: step 18475, loss 0.0161759, acc 0.98
2016-09-05T23:24:16.655359: step 18476, loss 0.00183518, acc 1
2016-09-05T23:24:17.454419: step 18477, loss 0.0214832, acc 0.98
2016-09-05T23:24:18.289280: step 18478, loss 0.00724571, acc 1
2016-09-05T23:24:19.104940: step 18479, loss 0.0016307, acc 1
2016-09-05T23:24:19.908426: step 18480, loss 0.0189116, acc 0.98
2016-09-05T23:24:20.731723: step 18481, loss 0.0205324, acc 1
2016-09-05T23:24:21.552101: step 18482, loss 0.00185641, acc 1
2016-09-05T23:24:22.365117: step 18483, loss 0.00854699, acc 1
2016-09-05T23:24:23.203338: step 18484, loss 0.00252206, acc 1
2016-09-05T23:24:24.029720: step 18485, loss 0.0131077, acc 1
2016-09-05T23:24:24.825162: step 18486, loss 0.00159447, acc 1
2016-09-05T23:24:25.652348: step 18487, loss 0.0123686, acc 1
2016-09-05T23:24:26.474393: step 18488, loss 0.0130508, acc 1
2016-09-05T23:24:27.241194: step 18489, loss 0.0321363, acc 0.98
2016-09-05T23:24:28.082762: step 18490, loss 0.00163811, acc 1
2016-09-05T23:24:28.900807: step 18491, loss 0.00184617, acc 1
2016-09-05T23:24:29.684433: step 18492, loss 0.0131852, acc 1
2016-09-05T23:24:30.488962: step 18493, loss 0.0846605, acc 0.96
2016-09-05T23:24:31.297086: step 18494, loss 0.00864331, acc 1
2016-09-05T23:24:32.087557: step 18495, loss 0.0586027, acc 0.96
2016-09-05T23:24:32.907008: step 18496, loss 0.0643531, acc 0.98
2016-09-05T23:24:33.715401: step 18497, loss 0.00246835, acc 1
2016-09-05T23:24:34.501040: step 18498, loss 0.0056006, acc 1
2016-09-05T23:24:35.333123: step 18499, loss 0.00144124, acc 1
2016-09-05T23:24:36.149490: step 18500, loss 0.0394305, acc 0.98

Evaluation:
2016-09-05T23:24:39.630801: step 18500, loss 2.8474, acc 0.702

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-18500

2016-09-05T23:24:41.647883: step 18501, loss 0.00183216, acc 1
2016-09-05T23:24:42.476005: step 18502, loss 0.00134782, acc 1
2016-09-05T23:24:43.283656: step 18503, loss 0.00131444, acc 1
2016-09-05T23:24:44.099377: step 18504, loss 0.00418321, acc 1
2016-09-05T23:24:44.941036: step 18505, loss 0.00168301, acc 1
2016-09-05T23:24:45.761429: step 18506, loss 0.016642, acc 1
2016-09-05T23:24:46.595601: step 18507, loss 0.0671899, acc 0.98
2016-09-05T23:24:47.434495: step 18508, loss 0.0295811, acc 0.98
2016-09-05T23:24:48.269127: step 18509, loss 0.0168259, acc 0.98
2016-09-05T23:24:49.073889: step 18510, loss 0.00127161, acc 1
2016-09-05T23:24:49.884220: step 18511, loss 0.00398318, acc 1
2016-09-05T23:24:50.733107: step 18512, loss 0.00425754, acc 1
2016-09-05T23:24:51.506232: step 18513, loss 0.025632, acc 0.98
2016-09-05T23:24:52.296366: step 18514, loss 0.0043233, acc 1
2016-09-05T23:24:53.110309: step 18515, loss 0.00219933, acc 1
2016-09-05T23:24:53.894880: step 18516, loss 0.00686416, acc 1
2016-09-05T23:24:54.675579: step 18517, loss 0.00824062, acc 1
2016-09-05T23:24:55.507655: step 18518, loss 0.00181425, acc 1
2016-09-05T23:24:56.286098: step 18519, loss 0.00136753, acc 1
2016-09-05T23:24:57.100342: step 18520, loss 0.0418175, acc 0.98
2016-09-05T23:24:57.921958: step 18521, loss 0.0322964, acc 0.98
2016-09-05T23:24:58.701404: step 18522, loss 0.0190467, acc 0.98
2016-09-05T23:24:59.523915: step 18523, loss 0.00371165, acc 1
2016-09-05T23:25:00.381609: step 18524, loss 0.00355609, acc 1
2016-09-05T23:25:01.164245: step 18525, loss 0.0631058, acc 0.98
2016-09-05T23:25:01.969064: step 18526, loss 0.014107, acc 1
2016-09-05T23:25:02.807800: step 18527, loss 0.0112686, acc 1
2016-09-05T23:25:03.589412: step 18528, loss 0.0332329, acc 0.98
2016-09-05T23:25:04.393362: step 18529, loss 0.102111, acc 0.96
2016-09-05T23:25:05.211362: step 18530, loss 0.0272484, acc 0.98
2016-09-05T23:25:05.993220: step 18531, loss 0.00343188, acc 1
2016-09-05T23:25:06.792675: step 18532, loss 0.00428093, acc 1
2016-09-05T23:25:07.597394: step 18533, loss 0.0153465, acc 1
2016-09-05T23:25:08.395333: step 18534, loss 0.00200588, acc 1
2016-09-05T23:25:09.194842: step 18535, loss 0.00157343, acc 1
2016-09-05T23:25:10.005385: step 18536, loss 0.0122782, acc 1
2016-09-05T23:25:10.798977: step 18537, loss 0.0202508, acc 1
2016-09-05T23:25:11.612553: step 18538, loss 0.00915177, acc 1
2016-09-05T23:25:12.443258: step 18539, loss 0.00752981, acc 1
2016-09-05T23:25:13.248956: step 18540, loss 0.0448127, acc 0.98
2016-09-05T23:25:14.046232: step 18541, loss 0.0397038, acc 0.96
2016-09-05T23:25:14.848651: step 18542, loss 0.00167897, acc 1
2016-09-05T23:25:15.637218: step 18543, loss 0.0104064, acc 1
2016-09-05T23:25:16.428337: step 18544, loss 0.00150815, acc 1
2016-09-05T23:25:17.261583: step 18545, loss 0.00155348, acc 1
2016-09-05T23:25:18.122708: step 18546, loss 0.0335012, acc 1
2016-09-05T23:25:18.927704: step 18547, loss 0.0158645, acc 1
2016-09-05T23:25:19.765032: step 18548, loss 0.00800195, acc 1
2016-09-05T23:25:20.552730: step 18549, loss 0.00334655, acc 1
2016-09-05T23:25:21.359664: step 18550, loss 0.00508451, acc 1
2016-09-05T23:25:22.178185: step 18551, loss 0.0160747, acc 0.98
2016-09-05T23:25:22.955123: step 18552, loss 0.0299313, acc 0.98
2016-09-05T23:25:23.755825: step 18553, loss 0.00408797, acc 1
2016-09-05T23:25:24.546086: step 18554, loss 0.0196461, acc 0.98
2016-09-05T23:25:25.332516: step 18555, loss 0.0116351, acc 1
2016-09-05T23:25:26.143857: step 18556, loss 0.00438602, acc 1
2016-09-05T23:25:26.970525: step 18557, loss 0.00924545, acc 1
2016-09-05T23:25:27.736852: step 18558, loss 0.00987508, acc 1
2016-09-05T23:25:28.539518: step 18559, loss 0.00184572, acc 1
2016-09-05T23:25:29.341578: step 18560, loss 0.00223258, acc 1
2016-09-05T23:25:30.124044: step 18561, loss 0.0353686, acc 0.98
2016-09-05T23:25:30.952576: step 18562, loss 0.00247981, acc 1
2016-09-05T23:25:31.775768: step 18563, loss 0.0157658, acc 0.98
2016-09-05T23:25:32.572081: step 18564, loss 0.0213804, acc 0.98
2016-09-05T23:25:33.391137: step 18565, loss 0.00245739, acc 1
2016-09-05T23:25:34.198628: step 18566, loss 0.0439198, acc 0.98
2016-09-05T23:25:34.988494: step 18567, loss 0.00266804, acc 1
2016-09-05T23:25:35.786638: step 18568, loss 0.00185635, acc 1
2016-09-05T23:25:36.583550: step 18569, loss 0.00280688, acc 1
2016-09-05T23:25:37.361723: step 18570, loss 0.00747196, acc 1
2016-09-05T23:25:38.177154: step 18571, loss 0.00456262, acc 1
2016-09-05T23:25:38.955047: step 18572, loss 0.00187248, acc 1
2016-09-05T23:25:39.766814: step 18573, loss 0.026571, acc 0.98
2016-09-05T23:25:40.582331: step 18574, loss 0.0157626, acc 0.98
2016-09-05T23:25:41.411747: step 18575, loss 0.0446015, acc 0.98
2016-09-05T23:25:42.212931: step 18576, loss 0.00180035, acc 1
2016-09-05T23:25:43.020823: step 18577, loss 0.00168781, acc 1
2016-09-05T23:25:43.819989: step 18578, loss 0.0113013, acc 1
2016-09-05T23:25:44.629509: step 18579, loss 0.0299639, acc 0.98
2016-09-05T23:25:45.455954: step 18580, loss 0.00176791, acc 1
2016-09-05T23:25:46.280167: step 18581, loss 0.0481043, acc 0.98
2016-09-05T23:25:47.073454: step 18582, loss 0.0240233, acc 0.98
2016-09-05T23:25:47.887139: step 18583, loss 0.00634309, acc 1
2016-09-05T23:25:48.704228: step 18584, loss 0.00851088, acc 1
2016-09-05T23:25:49.481064: step 18585, loss 0.0168377, acc 1
2016-09-05T23:25:50.373931: step 18586, loss 0.00265516, acc 1
2016-09-05T23:25:51.173977: step 18587, loss 0.00259855, acc 1
2016-09-05T23:25:51.955556: step 18588, loss 0.0441689, acc 0.96
2016-09-05T23:25:52.729232: step 18589, loss 0.00276745, acc 1
2016-09-05T23:25:53.573776: step 18590, loss 0.0156353, acc 1
2016-09-05T23:25:54.346732: step 18591, loss 0.0488477, acc 0.98
2016-09-05T23:25:55.149801: step 18592, loss 0.00976893, acc 1
2016-09-05T23:25:56.004798: step 18593, loss 0.00313063, acc 1
2016-09-05T23:25:56.789773: step 18594, loss 0.00321845, acc 1
2016-09-05T23:25:57.602633: step 18595, loss 0.0103925, acc 1
2016-09-05T23:25:58.403321: step 18596, loss 0.0196404, acc 0.98
2016-09-05T23:25:59.232815: step 18597, loss 0.0403064, acc 0.98
2016-09-05T23:26:00.039920: step 18598, loss 0.00335925, acc 1
2016-09-05T23:26:00.925249: step 18599, loss 0.033894, acc 0.98
2016-09-05T23:26:01.745022: step 18600, loss 0.0236202, acc 1

Evaluation:
2016-09-05T23:26:05.223889: step 18600, loss 3.40488, acc 0.708

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-18600

2016-09-05T23:26:07.213470: step 18601, loss 0.0155016, acc 1
2016-09-05T23:26:08.061596: step 18602, loss 0.021599, acc 0.98
2016-09-05T23:26:08.888970: step 18603, loss 0.00344314, acc 1
2016-09-05T23:26:09.705424: step 18604, loss 0.00342036, acc 1
2016-09-05T23:26:10.528485: step 18605, loss 0.00449317, acc 1
2016-09-05T23:26:11.352320: step 18606, loss 0.00498163, acc 1
2016-09-05T23:26:12.171499: step 18607, loss 0.00341053, acc 1
2016-09-05T23:26:13.047647: step 18608, loss 0.00339993, acc 1
2016-09-05T23:26:13.834212: step 18609, loss 0.00359429, acc 1
2016-09-05T23:26:14.680410: step 18610, loss 0.0183903, acc 0.98
2016-09-05T23:26:15.529394: step 18611, loss 0.00437268, acc 1
2016-09-05T23:26:16.330661: step 18612, loss 0.0238411, acc 0.98
2016-09-05T23:26:17.165310: step 18613, loss 0.00484796, acc 1
2016-09-05T23:26:17.992470: step 18614, loss 0.00343416, acc 1
2016-09-05T23:26:18.835030: step 18615, loss 0.00326913, acc 1
2016-09-05T23:26:19.749910: step 18616, loss 0.0195821, acc 0.98
2016-09-05T23:26:20.555285: step 18617, loss 0.0111005, acc 1
2016-09-05T23:26:21.415087: step 18618, loss 0.00373213, acc 1
2016-09-05T23:26:22.215292: step 18619, loss 0.0814059, acc 0.98
2016-09-05T23:26:23.021877: step 18620, loss 0.0339493, acc 0.98
2016-09-05T23:26:23.859845: step 18621, loss 0.00796097, acc 1
2016-09-05T23:26:24.670116: step 18622, loss 0.021246, acc 0.98
2016-09-05T23:26:25.494096: step 18623, loss 0.00285886, acc 1
2016-09-05T23:26:25.939401: step 18624, loss 0.00286809, acc 1
2016-09-05T23:26:26.758868: step 18625, loss 0.00360194, acc 1
2016-09-05T23:26:27.588700: step 18626, loss 0.0271374, acc 1
2016-09-05T23:26:28.397336: step 18627, loss 0.0233732, acc 1
2016-09-05T23:26:29.217995: step 18628, loss 0.00442024, acc 1
2016-09-05T23:26:30.059528: step 18629, loss 0.00314999, acc 1
2016-09-05T23:26:30.912050: step 18630, loss 0.00338401, acc 1
2016-09-05T23:26:31.745744: step 18631, loss 0.0150287, acc 1
2016-09-05T23:26:32.590865: step 18632, loss 0.0158538, acc 1
2016-09-05T23:26:33.404225: step 18633, loss 0.0233643, acc 0.98
2016-09-05T23:26:34.209055: step 18634, loss 0.00566678, acc 1
2016-09-05T23:26:35.042078: step 18635, loss 0.00486095, acc 1
2016-09-05T23:26:35.863394: step 18636, loss 0.0114531, acc 1
2016-09-05T23:26:36.660539: step 18637, loss 0.0517603, acc 0.96
2016-09-05T23:26:37.452268: step 18638, loss 0.0441062, acc 0.96
2016-09-05T23:26:38.270455: step 18639, loss 0.00271712, acc 1
2016-09-05T23:26:39.045675: step 18640, loss 0.00271252, acc 1
2016-09-05T23:26:39.857105: step 18641, loss 0.00959172, acc 1
2016-09-05T23:26:40.704791: step 18642, loss 0.00312211, acc 1
2016-09-05T23:26:41.500780: step 18643, loss 0.00869919, acc 1
2016-09-05T23:26:42.310537: step 18644, loss 0.0035612, acc 1
2016-09-05T23:26:43.116111: step 18645, loss 0.0157602, acc 1
2016-09-05T23:26:43.901561: step 18646, loss 0.0171292, acc 1
2016-09-05T23:26:44.707897: step 18647, loss 0.0169587, acc 0.98
2016-09-05T23:26:45.517624: step 18648, loss 0.00296523, acc 1
2016-09-05T23:26:46.293426: step 18649, loss 0.0149675, acc 1
2016-09-05T23:26:47.118424: step 18650, loss 0.00264992, acc 1
2016-09-05T23:26:47.929420: step 18651, loss 0.00265803, acc 1
2016-09-05T23:26:48.753193: step 18652, loss 0.00404061, acc 1
2016-09-05T23:26:49.553132: step 18653, loss 0.00315647, acc 1
2016-09-05T23:26:50.376183: step 18654, loss 0.0131494, acc 1
2016-09-05T23:26:51.173221: step 18655, loss 0.00258917, acc 1
2016-09-05T23:26:52.004347: step 18656, loss 0.027436, acc 0.98
2016-09-05T23:26:52.829926: step 18657, loss 0.0123936, acc 1
2016-09-05T23:26:53.637697: step 18658, loss 0.00259401, acc 1
2016-09-05T23:26:54.438704: step 18659, loss 0.00272074, acc 1
2016-09-05T23:26:55.240710: step 18660, loss 0.0625207, acc 0.94
2016-09-05T23:26:56.014662: step 18661, loss 0.00310423, acc 1
2016-09-05T23:26:56.812135: step 18662, loss 0.0134165, acc 1
2016-09-05T23:26:57.627692: step 18663, loss 0.00245171, acc 1
2016-09-05T23:26:58.418599: step 18664, loss 0.00242582, acc 1
2016-09-05T23:26:59.241405: step 18665, loss 0.00449521, acc 1
2016-09-05T23:27:00.092079: step 18666, loss 0.021221, acc 1
2016-09-05T23:27:00.958612: step 18667, loss 0.00236959, acc 1
2016-09-05T23:27:01.812917: step 18668, loss 0.00882015, acc 1
2016-09-05T23:27:02.635809: step 18669, loss 0.0354186, acc 0.98
2016-09-05T23:27:03.440385: step 18670, loss 0.0033969, acc 1
2016-09-05T23:27:04.276611: step 18671, loss 0.00229539, acc 1
2016-09-05T23:27:05.111116: step 18672, loss 0.0256763, acc 1
2016-09-05T23:27:05.915197: step 18673, loss 0.00225715, acc 1
2016-09-05T23:27:06.742216: step 18674, loss 0.0198422, acc 0.98
2016-09-05T23:27:07.603364: step 18675, loss 0.00251591, acc 1
2016-09-05T23:27:08.446386: step 18676, loss 0.00729572, acc 1
2016-09-05T23:27:09.246069: step 18677, loss 0.00244046, acc 1
2016-09-05T23:27:10.057268: step 18678, loss 0.00596607, acc 1
2016-09-05T23:27:10.878112: step 18679, loss 0.0249885, acc 0.98
2016-09-05T23:27:11.676995: step 18680, loss 0.00212523, acc 1
2016-09-05T23:27:12.482226: step 18681, loss 0.00228015, acc 1
2016-09-05T23:27:13.296304: step 18682, loss 0.00206239, acc 1
2016-09-05T23:27:14.106129: step 18683, loss 0.00208237, acc 1
2016-09-05T23:27:14.904868: step 18684, loss 0.0112886, acc 1
2016-09-05T23:27:15.687661: step 18685, loss 0.0107706, acc 1
2016-09-05T23:27:16.485749: step 18686, loss 0.0172741, acc 0.98
2016-09-05T23:27:17.322222: step 18687, loss 0.00335856, acc 1
2016-09-05T23:27:18.111544: step 18688, loss 0.0059299, acc 1
2016-09-05T23:27:18.920409: step 18689, loss 0.00264918, acc 1
2016-09-05T23:27:19.751211: step 18690, loss 0.026371, acc 0.98
2016-09-05T23:27:20.537413: step 18691, loss 0.00200094, acc 1
2016-09-05T23:27:21.327066: step 18692, loss 0.00883866, acc 1
2016-09-05T23:27:22.151097: step 18693, loss 0.00569076, acc 1
2016-09-05T23:27:22.957676: step 18694, loss 0.00615461, acc 1
2016-09-05T23:27:23.758540: step 18695, loss 0.0289627, acc 0.98
2016-09-05T23:27:24.573420: step 18696, loss 0.00880485, acc 1
2016-09-05T23:27:25.361096: step 18697, loss 0.00229514, acc 1
2016-09-05T23:27:26.169092: step 18698, loss 0.00271701, acc 1
2016-09-05T23:27:26.995701: step 18699, loss 0.00459258, acc 1
2016-09-05T23:27:27.820369: step 18700, loss 0.0545824, acc 0.96

Evaluation:
2016-09-05T23:27:31.304514: step 18700, loss 2.94926, acc 0.711

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-18700

2016-09-05T23:27:33.180782: step 18701, loss 0.0210171, acc 0.98
2016-09-05T23:27:34.002554: step 18702, loss 0.0158072, acc 0.98
2016-09-05T23:27:34.790451: step 18703, loss 0.0472634, acc 0.98
2016-09-05T23:27:35.592490: step 18704, loss 0.0154816, acc 1
2016-09-05T23:27:36.399974: step 18705, loss 0.0127859, acc 1
2016-09-05T23:27:37.192089: step 18706, loss 0.00178578, acc 1
2016-09-05T23:27:37.993802: step 18707, loss 0.00198461, acc 1
2016-09-05T23:27:38.786754: step 18708, loss 0.0185064, acc 0.98
2016-09-05T23:27:39.595909: step 18709, loss 0.00176323, acc 1
2016-09-05T23:27:40.376137: step 18710, loss 0.0180131, acc 0.98
2016-09-05T23:27:41.225781: step 18711, loss 0.00165978, acc 1
2016-09-05T23:27:42.023565: step 18712, loss 0.00819253, acc 1
2016-09-05T23:27:42.827200: step 18713, loss 0.00183469, acc 1
2016-09-05T23:27:43.639833: step 18714, loss 0.0239027, acc 1
2016-09-05T23:27:44.443984: step 18715, loss 0.0132771, acc 1
2016-09-05T23:27:45.239295: step 18716, loss 0.0751974, acc 0.94
2016-09-05T23:27:46.052134: step 18717, loss 0.00300154, acc 1
2016-09-05T23:27:46.830696: step 18718, loss 0.00627673, acc 1
2016-09-05T23:27:47.620615: step 18719, loss 0.0327404, acc 0.98
2016-09-05T23:27:48.423909: step 18720, loss 0.00160286, acc 1
2016-09-05T23:27:49.216284: step 18721, loss 0.00178601, acc 1
2016-09-05T23:27:50.024317: step 18722, loss 0.0165626, acc 1
2016-09-05T23:27:50.856274: step 18723, loss 0.0210666, acc 0.98
2016-09-05T23:27:51.633106: step 18724, loss 0.0317417, acc 1
2016-09-05T23:27:52.434095: step 18725, loss 0.020033, acc 0.98
2016-09-05T23:27:53.269730: step 18726, loss 0.100997, acc 0.98
2016-09-05T23:27:54.054017: step 18727, loss 0.00387449, acc 1
2016-09-05T23:27:54.851575: step 18728, loss 0.00698626, acc 1
2016-09-05T23:27:55.691713: step 18729, loss 0.00297268, acc 1
2016-09-05T23:27:56.521933: step 18730, loss 0.0165773, acc 1
2016-09-05T23:27:57.332581: step 18731, loss 0.0245134, acc 0.98
2016-09-05T23:27:58.139905: step 18732, loss 0.00345394, acc 1
2016-09-05T23:27:58.952453: step 18733, loss 0.00364524, acc 1
2016-09-05T23:27:59.756525: step 18734, loss 0.00269604, acc 1
2016-09-05T23:28:00.634977: step 18735, loss 0.00228872, acc 1
2016-09-05T23:28:01.453015: step 18736, loss 0.00149973, acc 1
2016-09-05T23:28:02.316399: step 18737, loss 0.00491756, acc 1
2016-09-05T23:28:03.146200: step 18738, loss 0.0890645, acc 0.94
2016-09-05T23:28:03.962105: step 18739, loss 0.00168207, acc 1
2016-09-05T23:28:04.795037: step 18740, loss 0.0319568, acc 0.98
2016-09-05T23:28:05.623800: step 18741, loss 0.00422896, acc 1
2016-09-05T23:28:06.434577: step 18742, loss 0.0246892, acc 0.98
2016-09-05T23:28:07.245978: step 18743, loss 0.0109914, acc 1
2016-09-05T23:28:08.083405: step 18744, loss 0.0804296, acc 0.98
2016-09-05T23:28:08.889483: step 18745, loss 0.00205127, acc 1
2016-09-05T23:28:09.689254: step 18746, loss 0.0191273, acc 0.98
2016-09-05T23:28:10.509249: step 18747, loss 0.00156493, acc 1
2016-09-05T23:28:11.316120: step 18748, loss 0.00146442, acc 1
2016-09-05T23:28:12.117572: step 18749, loss 0.0257357, acc 1
2016-09-05T23:28:12.949087: step 18750, loss 0.0167667, acc 1
2016-09-05T23:28:13.798972: step 18751, loss 0.0514534, acc 0.98
2016-09-05T23:28:14.592620: step 18752, loss 0.00707415, acc 1
2016-09-05T23:28:15.406319: step 18753, loss 0.00198077, acc 1
2016-09-05T23:28:16.231090: step 18754, loss 0.0157433, acc 1
2016-09-05T23:28:17.021422: step 18755, loss 0.0317876, acc 0.96
2016-09-05T23:28:17.796588: step 18756, loss 0.0178593, acc 0.98
2016-09-05T23:28:18.592219: step 18757, loss 0.00160355, acc 1
2016-09-05T23:28:19.405824: step 18758, loss 0.0222656, acc 1
2016-09-05T23:28:20.221666: step 18759, loss 0.00244622, acc 1
2016-09-05T23:28:21.078082: step 18760, loss 0.00317314, acc 1
2016-09-05T23:28:21.925541: step 18761, loss 0.0239619, acc 1
2016-09-05T23:28:22.799969: step 18762, loss 0.00203542, acc 1
2016-09-05T23:28:23.651664: step 18763, loss 0.00357294, acc 1
2016-09-05T23:28:24.447817: step 18764, loss 0.00900679, acc 1
2016-09-05T23:28:25.257243: step 18765, loss 0.00218773, acc 1
2016-09-05T23:28:26.118843: step 18766, loss 0.0125166, acc 1
2016-09-05T23:28:26.941523: step 18767, loss 0.00923233, acc 1
2016-09-05T23:28:27.731769: step 18768, loss 0.00300102, acc 1
2016-09-05T23:28:28.555967: step 18769, loss 0.0288825, acc 0.98
2016-09-05T23:28:29.387877: step 18770, loss 0.00400621, acc 1
2016-09-05T23:28:30.201898: step 18771, loss 0.024111, acc 0.98
2016-09-05T23:28:31.044795: step 18772, loss 0.00399717, acc 1
2016-09-05T23:28:31.856386: step 18773, loss 0.0023406, acc 1
2016-09-05T23:28:32.685950: step 18774, loss 0.0158855, acc 1
2016-09-05T23:28:33.491124: step 18775, loss 0.0119993, acc 1
2016-09-05T23:28:34.325555: step 18776, loss 0.0543289, acc 0.96
2016-09-05T23:28:35.141705: step 18777, loss 0.0477076, acc 0.98
2016-09-05T23:28:35.960045: step 18778, loss 0.0248241, acc 1
2016-09-05T23:28:36.751179: step 18779, loss 0.030651, acc 0.98
2016-09-05T23:28:37.576460: step 18780, loss 0.00210303, acc 1
2016-09-05T23:28:38.377484: step 18781, loss 0.00209628, acc 1
2016-09-05T23:28:39.303098: step 18782, loss 0.0106104, acc 1
2016-09-05T23:28:40.099620: step 18783, loss 0.00208593, acc 1
2016-09-05T23:28:40.898101: step 18784, loss 0.0167278, acc 0.98
2016-09-05T23:28:41.731947: step 18785, loss 0.00206353, acc 1
2016-09-05T23:28:42.530565: step 18786, loss 0.00923806, acc 1
2016-09-05T23:28:43.328639: step 18787, loss 0.00319527, acc 1
2016-09-05T23:28:44.142403: step 18788, loss 0.0153451, acc 1
2016-09-05T23:28:44.934092: step 18789, loss 0.0175661, acc 0.98
2016-09-05T23:28:45.743579: step 18790, loss 0.0119464, acc 1
2016-09-05T23:28:46.569986: step 18791, loss 0.00349844, acc 1
2016-09-05T23:28:47.367145: step 18792, loss 0.018233, acc 0.98
2016-09-05T23:28:48.195229: step 18793, loss 0.00231803, acc 1
2016-09-05T23:28:49.078902: step 18794, loss 0.0306528, acc 0.96
2016-09-05T23:28:49.888914: step 18795, loss 0.00814646, acc 1
2016-09-05T23:28:50.693620: step 18796, loss 0.00195584, acc 1
2016-09-05T23:28:51.511837: step 18797, loss 0.0983401, acc 0.98
2016-09-05T23:28:52.335995: step 18798, loss 0.00833878, acc 1
2016-09-05T23:28:53.145858: step 18799, loss 0.00818923, acc 1
2016-09-05T23:28:53.963467: step 18800, loss 0.00183497, acc 1

Evaluation:
2016-09-05T23:28:57.508422: step 18800, loss 2.92593, acc 0.706

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-18800

2016-09-05T23:28:59.470626: step 18801, loss 0.00608494, acc 1
2016-09-05T23:29:00.293446: step 18802, loss 0.00211608, acc 1
2016-09-05T23:29:01.117881: step 18803, loss 0.0019468, acc 1
2016-09-05T23:29:01.960974: step 18804, loss 0.00792407, acc 1
2016-09-05T23:29:02.781964: step 18805, loss 0.0118127, acc 1
2016-09-05T23:29:03.609116: step 18806, loss 0.00325442, acc 1
2016-09-05T23:29:04.424978: step 18807, loss 0.00239516, acc 1
2016-09-05T23:29:05.232741: step 18808, loss 0.00169794, acc 1
2016-09-05T23:29:06.046545: step 18809, loss 0.00595046, acc 1
2016-09-05T23:29:06.863454: step 18810, loss 0.0019528, acc 1
2016-09-05T23:29:07.675417: step 18811, loss 0.0209176, acc 0.98
2016-09-05T23:29:08.502025: step 18812, loss 0.00215682, acc 1
2016-09-05T23:29:09.336561: step 18813, loss 0.0297466, acc 0.98
2016-09-05T23:29:10.159599: step 18814, loss 0.0164958, acc 1
2016-09-05T23:29:10.990153: step 18815, loss 0.0283322, acc 0.98
2016-09-05T23:29:11.826534: step 18816, loss 0.0155172, acc 1
2016-09-05T23:29:12.623659: step 18817, loss 0.00155057, acc 1
2016-09-05T23:29:13.058938: step 18818, loss 0.00156347, acc 1
2016-09-05T23:29:13.842963: step 18819, loss 0.027759, acc 0.98
2016-09-05T23:29:14.671528: step 18820, loss 0.00337889, acc 1
2016-09-05T23:29:15.482480: step 18821, loss 0.0181494, acc 1
2016-09-05T23:29:16.281342: step 18822, loss 0.00325529, acc 1
2016-09-05T23:29:17.113185: step 18823, loss 0.00658122, acc 1
2016-09-05T23:29:17.927597: step 18824, loss 0.00915889, acc 1
2016-09-05T23:29:18.739953: step 18825, loss 0.00502329, acc 1
2016-09-05T23:29:19.529817: step 18826, loss 0.0143525, acc 1
2016-09-05T23:29:20.336063: step 18827, loss 0.00172862, acc 1
2016-09-05T23:29:21.144650: step 18828, loss 0.00161627, acc 1
2016-09-05T23:29:21.932840: step 18829, loss 0.0038046, acc 1
2016-09-05T23:29:22.744078: step 18830, loss 0.0993888, acc 0.98
2016-09-05T23:29:23.539063: step 18831, loss 0.0152257, acc 1
2016-09-05T23:29:24.343244: step 18832, loss 0.00156748, acc 1
2016-09-05T23:29:25.114975: step 18833, loss 0.00171044, acc 1
2016-09-05T23:29:25.918018: step 18834, loss 0.00159471, acc 1
2016-09-05T23:29:26.758295: step 18835, loss 0.0205928, acc 1
2016-09-05T23:29:27.565017: step 18836, loss 0.0015132, acc 1
2016-09-05T23:29:28.374314: step 18837, loss 0.0140799, acc 1
2016-09-05T23:29:29.210897: step 18838, loss 0.0167088, acc 0.98
2016-09-05T23:29:30.027718: step 18839, loss 0.00206822, acc 1
2016-09-05T23:29:30.823343: step 18840, loss 0.0311411, acc 1
2016-09-05T23:29:31.622659: step 18841, loss 0.00155551, acc 1
2016-09-05T23:29:32.443478: step 18842, loss 0.0171674, acc 1
2016-09-05T23:29:33.217771: step 18843, loss 0.00217308, acc 1
2016-09-05T23:29:34.035409: step 18844, loss 0.00224617, acc 1
2016-09-05T23:29:34.852338: step 18845, loss 0.00251951, acc 1
2016-09-05T23:29:35.634626: step 18846, loss 0.0210681, acc 0.98
2016-09-05T23:29:36.431777: step 18847, loss 0.0141149, acc 1
2016-09-05T23:29:37.238810: step 18848, loss 0.0435418, acc 0.98
2016-09-05T23:29:38.054876: step 18849, loss 0.0178801, acc 1
2016-09-05T23:29:38.870700: step 18850, loss 0.0344915, acc 0.98
2016-09-05T23:29:39.677561: step 18851, loss 0.00789673, acc 1
2016-09-05T23:29:40.462466: step 18852, loss 0.0038893, acc 1
2016-09-05T23:29:41.285433: step 18853, loss 0.00740112, acc 1
2016-09-05T23:29:42.114679: step 18854, loss 0.0121661, acc 1
2016-09-05T23:29:42.903239: step 18855, loss 0.0190457, acc 0.98
2016-09-05T23:29:43.721966: step 18856, loss 0.0119192, acc 1
2016-09-05T23:29:44.538890: step 18857, loss 0.00164412, acc 1
2016-09-05T23:29:45.338634: step 18858, loss 0.00158295, acc 1
2016-09-05T23:29:46.139175: step 18859, loss 0.0375772, acc 0.98
2016-09-05T23:29:46.955673: step 18860, loss 0.0146099, acc 1
2016-09-05T23:29:47.750789: step 18861, loss 0.0343368, acc 0.98
2016-09-05T23:29:48.554647: step 18862, loss 0.00160043, acc 1
2016-09-05T23:29:49.363614: step 18863, loss 0.00563908, acc 1
2016-09-05T23:29:50.180519: step 18864, loss 0.0142064, acc 1
2016-09-05T23:29:51.010094: step 18865, loss 0.00497719, acc 1
2016-09-05T23:29:51.818159: step 18866, loss 0.00185757, acc 1
2016-09-05T23:29:52.613659: step 18867, loss 0.00159719, acc 1
2016-09-05T23:29:53.414279: step 18868, loss 0.0219737, acc 0.98
2016-09-05T23:29:54.220263: step 18869, loss 0.0175172, acc 0.98
2016-09-05T23:29:54.997690: step 18870, loss 0.00170978, acc 1
2016-09-05T23:29:55.807694: step 18871, loss 0.00183232, acc 1
2016-09-05T23:29:56.635239: step 18872, loss 0.00161359, acc 1
2016-09-05T23:29:57.444954: step 18873, loss 0.00197196, acc 1
2016-09-05T23:29:58.254829: step 18874, loss 0.013751, acc 1
2016-09-05T23:29:59.085475: step 18875, loss 0.0159191, acc 1
2016-09-05T23:29:59.869644: step 18876, loss 0.00161102, acc 1
2016-09-05T23:30:00.704630: step 18877, loss 0.001634, acc 1
2016-09-05T23:30:01.527546: step 18878, loss 0.00209291, acc 1
2016-09-05T23:30:02.344069: step 18879, loss 0.00395249, acc 1
2016-09-05T23:30:03.169120: step 18880, loss 0.00165817, acc 1
2016-09-05T23:30:03.998681: step 18881, loss 0.00465584, acc 1
2016-09-05T23:30:04.817402: step 18882, loss 0.0319833, acc 0.98
2016-09-05T23:30:05.627917: step 18883, loss 0.00168338, acc 1
2016-09-05T23:30:06.453882: step 18884, loss 0.00276429, acc 1
2016-09-05T23:30:07.253144: step 18885, loss 0.0872232, acc 0.96
2016-09-05T23:30:08.023842: step 18886, loss 0.00242515, acc 1
2016-09-05T23:30:08.867707: step 18887, loss 0.00158671, acc 1
2016-09-05T23:30:09.695567: step 18888, loss 0.00813906, acc 1
2016-09-05T23:30:10.502391: step 18889, loss 0.00149498, acc 1
2016-09-05T23:30:11.343220: step 18890, loss 0.0127442, acc 1
2016-09-05T23:30:12.173786: step 18891, loss 0.00146953, acc 1
2016-09-05T23:30:12.989822: step 18892, loss 0.039554, acc 0.98
2016-09-05T23:30:13.840868: step 18893, loss 0.0140279, acc 1
2016-09-05T23:30:14.654029: step 18894, loss 0.0174217, acc 1
2016-09-05T23:30:15.452018: step 18895, loss 0.0037672, acc 1
2016-09-05T23:30:16.300163: step 18896, loss 0.00141118, acc 1
2016-09-05T23:30:17.100835: step 18897, loss 0.121915, acc 0.98
2016-09-05T23:30:17.904238: step 18898, loss 0.0275748, acc 1
2016-09-05T23:30:18.755994: step 18899, loss 0.0196303, acc 0.98
2016-09-05T23:30:19.575020: step 18900, loss 0.00154209, acc 1

Evaluation:
2016-09-05T23:30:23.097334: step 18900, loss 2.12766, acc 0.71

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-18900

2016-09-05T23:30:25.042331: step 18901, loss 0.0169969, acc 0.98
2016-09-05T23:30:25.905054: step 18902, loss 0.0012419, acc 1
2016-09-05T23:30:26.712963: step 18903, loss 0.00510267, acc 1
2016-09-05T23:30:27.506070: step 18904, loss 0.00946004, acc 1
2016-09-05T23:30:28.350954: step 18905, loss 0.00191635, acc 1
2016-09-05T23:30:29.179228: step 18906, loss 0.00676844, acc 1
2016-09-05T23:30:29.992901: step 18907, loss 0.0100947, acc 1
2016-09-05T23:30:30.843059: step 18908, loss 0.0249674, acc 1
2016-09-05T23:30:31.630607: step 18909, loss 0.0193771, acc 1
2016-09-05T23:30:32.447159: step 18910, loss 0.0155519, acc 1
2016-09-05T23:30:33.277127: step 18911, loss 0.00162073, acc 1
2016-09-05T23:30:34.112553: step 18912, loss 0.00140181, acc 1
2016-09-05T23:30:34.901496: step 18913, loss 0.018187, acc 1
2016-09-05T23:30:35.701355: step 18914, loss 0.00164131, acc 1
2016-09-05T23:30:36.545739: step 18915, loss 0.00233567, acc 1
2016-09-05T23:30:37.334251: step 18916, loss 0.0380422, acc 0.98
2016-09-05T23:30:38.123256: step 18917, loss 0.017188, acc 0.98
2016-09-05T23:30:38.945471: step 18918, loss 0.059084, acc 0.96
2016-09-05T23:30:39.791078: step 18919, loss 0.00202374, acc 1
2016-09-05T23:30:40.596414: step 18920, loss 0.0044294, acc 1
2016-09-05T23:30:41.440960: step 18921, loss 0.0225307, acc 0.98
2016-09-05T23:30:42.237528: step 18922, loss 0.116931, acc 0.98
2016-09-05T23:30:43.041079: step 18923, loss 0.00412805, acc 1
2016-09-05T23:30:43.875079: step 18924, loss 0.0151621, acc 1
2016-09-05T23:30:44.713771: step 18925, loss 0.002176, acc 1
2016-09-05T23:30:45.516410: step 18926, loss 0.028545, acc 0.98
2016-09-05T23:30:46.337439: step 18927, loss 0.00251993, acc 1
2016-09-05T23:30:47.135386: step 18928, loss 0.00254937, acc 1
2016-09-05T23:30:47.932184: step 18929, loss 0.0240815, acc 0.98
2016-09-05T23:30:48.797122: step 18930, loss 0.00534135, acc 1
2016-09-05T23:30:49.629017: step 18931, loss 0.118312, acc 0.98
2016-09-05T23:30:50.428380: step 18932, loss 0.00278193, acc 1
2016-09-05T23:30:51.283912: step 18933, loss 0.157859, acc 0.96
2016-09-05T23:30:52.108683: step 18934, loss 0.0247709, acc 0.98
2016-09-05T23:30:52.955207: step 18935, loss 0.00598747, acc 1
2016-09-05T23:30:53.770727: step 18936, loss 0.0434033, acc 0.96
2016-09-05T23:30:54.584720: step 18937, loss 0.017494, acc 1
2016-09-05T23:30:55.419417: step 18938, loss 0.0788442, acc 0.96
2016-09-05T23:30:56.232157: step 18939, loss 0.00988862, acc 1
2016-09-05T23:30:57.034885: step 18940, loss 0.03708, acc 0.96
2016-09-05T23:30:57.831039: step 18941, loss 0.00236402, acc 1
2016-09-05T23:30:58.656961: step 18942, loss 0.00646405, acc 1
2016-09-05T23:30:59.496167: step 18943, loss 0.0147175, acc 1
2016-09-05T23:31:00.325424: step 18944, loss 0.0133532, acc 1
2016-09-05T23:31:01.130601: step 18945, loss 0.00588633, acc 1
2016-09-05T23:31:01.957890: step 18946, loss 0.00913995, acc 1
2016-09-05T23:31:02.745925: step 18947, loss 0.00307005, acc 1
2016-09-05T23:31:03.555767: step 18948, loss 0.0328236, acc 0.98
2016-09-05T23:31:04.345567: step 18949, loss 0.0137994, acc 1
2016-09-05T23:31:05.143745: step 18950, loss 0.0223674, acc 0.98
2016-09-05T23:31:05.977407: step 18951, loss 0.00312418, acc 1
2016-09-05T23:31:06.774388: step 18952, loss 0.00604819, acc 1
2016-09-05T23:31:07.566322: step 18953, loss 0.0095938, acc 1
2016-09-05T23:31:08.402721: step 18954, loss 0.0266179, acc 0.98
2016-09-05T23:31:09.218120: step 18955, loss 0.00406635, acc 1
2016-09-05T23:31:10.033352: step 18956, loss 0.0264091, acc 1
2016-09-05T23:31:10.819551: step 18957, loss 0.0063891, acc 1
2016-09-05T23:31:11.624899: step 18958, loss 0.0269219, acc 0.98
2016-09-05T23:31:12.395939: step 18959, loss 0.0123865, acc 1
2016-09-05T23:31:13.203697: step 18960, loss 0.00320734, acc 1
2016-09-05T23:31:14.055178: step 18961, loss 0.00612057, acc 1
2016-09-05T23:31:14.866526: step 18962, loss 0.00400095, acc 1
2016-09-05T23:31:15.677659: step 18963, loss 0.00347963, acc 1
2016-09-05T23:31:16.521520: step 18964, loss 0.00769845, acc 1
2016-09-05T23:31:17.362942: step 18965, loss 0.00323685, acc 1
2016-09-05T23:31:18.180044: step 18966, loss 0.0907445, acc 0.98
2016-09-05T23:31:19.024908: step 18967, loss 0.00351234, acc 1
2016-09-05T23:31:19.899281: step 18968, loss 0.0032056, acc 1
2016-09-05T23:31:20.729984: step 18969, loss 0.0188064, acc 1
2016-09-05T23:31:21.584785: step 18970, loss 0.0546292, acc 0.94
2016-09-05T23:31:22.407886: step 18971, loss 0.0564686, acc 0.98
2016-09-05T23:31:23.191418: step 18972, loss 0.019076, acc 1
2016-09-05T23:31:24.052842: step 18973, loss 0.0176627, acc 1
2016-09-05T23:31:24.990456: step 18974, loss 0.0110225, acc 1
2016-09-05T23:31:25.839608: step 18975, loss 0.00452865, acc 1
2016-09-05T23:31:26.737894: step 18976, loss 0.00534905, acc 1
2016-09-05T23:31:27.600702: step 18977, loss 0.00351837, acc 1
2016-09-05T23:31:28.445547: step 18978, loss 0.0191574, acc 0.98
2016-09-05T23:31:29.255667: step 18979, loss 0.00358634, acc 1
2016-09-05T23:31:30.321533: step 18980, loss 0.00457754, acc 1
2016-09-05T23:31:31.173885: step 18981, loss 0.00360586, acc 1
2016-09-05T23:31:31.991476: step 18982, loss 0.00377959, acc 1
2016-09-05T23:31:32.904014: step 18983, loss 0.00364171, acc 1
2016-09-05T23:31:33.788751: step 18984, loss 0.0166691, acc 1
2016-09-05T23:31:34.607316: step 18985, loss 0.00350487, acc 1
2016-09-05T23:31:35.548467: step 18986, loss 0.00351349, acc 1
2016-09-05T23:31:36.381417: step 18987, loss 0.0564542, acc 0.98
2016-09-05T23:31:37.226453: step 18988, loss 0.0274034, acc 0.98
2016-09-05T23:31:38.033973: step 18989, loss 0.0414243, acc 0.98
2016-09-05T23:31:38.887762: step 18990, loss 0.00383591, acc 1
2016-09-05T23:31:39.761915: step 18991, loss 0.00937899, acc 1
2016-09-05T23:31:40.688503: step 18992, loss 0.00523129, acc 1
2016-09-05T23:31:41.513500: step 18993, loss 0.00392921, acc 1
2016-09-05T23:31:42.474074: step 18994, loss 0.0215842, acc 1
2016-09-05T23:31:43.321098: step 18995, loss 0.003189, acc 1
2016-09-05T23:31:44.161154: step 18996, loss 0.00313533, acc 1
2016-09-05T23:31:45.011585: step 18997, loss 0.0196936, acc 0.98
2016-09-05T23:31:45.845553: step 18998, loss 0.0182472, acc 1
2016-09-05T23:31:46.686813: step 18999, loss 0.0103794, acc 1
2016-09-05T23:31:47.519466: step 19000, loss 0.00307671, acc 1

Evaluation:
2016-09-05T23:31:51.016893: step 19000, loss 3.41998, acc 0.731

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-19000

2016-09-05T23:31:52.925207: step 19001, loss 0.0125174, acc 1
2016-09-05T23:31:53.767494: step 19002, loss 0.023864, acc 0.98
2016-09-05T23:31:54.584915: step 19003, loss 0.0035263, acc 1
2016-09-05T23:31:55.441438: step 19004, loss 0.00299262, acc 1
2016-09-05T23:31:56.285130: step 19005, loss 0.0223857, acc 0.98
2016-09-05T23:31:57.108960: step 19006, loss 0.00295185, acc 1
2016-09-05T23:31:57.969681: step 19007, loss 0.15395, acc 0.98
2016-09-05T23:31:58.765736: step 19008, loss 0.0117133, acc 1
2016-09-05T23:31:59.572866: step 19009, loss 0.0138798, acc 1
2016-09-05T23:32:00.421066: step 19010, loss 0.0315327, acc 0.98
2016-09-05T23:32:01.240858: step 19011, loss 0.00297826, acc 1
2016-09-05T23:32:01.674306: step 19012, loss 0.00573594, acc 1
2016-09-05T23:32:02.505936: step 19013, loss 0.0177886, acc 0.98
2016-09-05T23:32:03.325434: step 19014, loss 0.00285366, acc 1
2016-09-05T23:32:04.109049: step 19015, loss 0.0373848, acc 0.98
2016-09-05T23:32:04.933395: step 19016, loss 0.00616745, acc 1
2016-09-05T23:32:05.724513: step 19017, loss 0.00267098, acc 1
2016-09-05T23:32:06.537871: step 19018, loss 0.0213482, acc 0.98
2016-09-05T23:32:07.365654: step 19019, loss 0.0171797, acc 0.98
2016-09-05T23:32:08.129566: step 19020, loss 0.0140745, acc 1
2016-09-05T23:32:08.929659: step 19021, loss 0.0678229, acc 0.96
2016-09-05T23:32:09.767643: step 19022, loss 0.00645942, acc 1
2016-09-05T23:32:10.572822: step 19023, loss 0.0185846, acc 1
2016-09-05T23:32:11.373062: step 19024, loss 0.0140615, acc 1
2016-09-05T23:32:12.179862: step 19025, loss 0.0728102, acc 0.96
2016-09-05T23:32:12.951928: step 19026, loss 0.0426155, acc 0.98
2016-09-05T23:32:13.771492: step 19027, loss 0.0186068, acc 0.98
2016-09-05T23:32:14.577580: step 19028, loss 0.0023069, acc 1
2016-09-05T23:32:15.368306: step 19029, loss 0.0391635, acc 0.98
2016-09-05T23:32:16.201867: step 19030, loss 0.00977104, acc 1
2016-09-05T23:32:17.019863: step 19031, loss 0.00857838, acc 1
2016-09-05T23:32:17.786644: step 19032, loss 0.00699479, acc 1
2016-09-05T23:32:18.597976: step 19033, loss 0.00430627, acc 1
2016-09-05T23:32:19.433268: step 19034, loss 0.0197273, acc 1
2016-09-05T23:32:20.200348: step 19035, loss 0.00447694, acc 1
2016-09-05T23:32:20.993924: step 19036, loss 0.00235782, acc 1
2016-09-05T23:32:21.815991: step 19037, loss 0.00278829, acc 1
2016-09-05T23:32:22.615220: step 19038, loss 0.00239347, acc 1
2016-09-05T23:32:23.406901: step 19039, loss 0.002401, acc 1
2016-09-05T23:32:24.220328: step 19040, loss 0.00263488, acc 1
2016-09-05T23:32:25.008209: step 19041, loss 0.0172116, acc 0.98
2016-09-05T23:32:25.832772: step 19042, loss 0.0109235, acc 1
2016-09-05T23:32:26.646537: step 19043, loss 0.01167, acc 1
2016-09-05T23:32:27.435431: step 19044, loss 0.0232162, acc 0.98
2016-09-05T23:32:28.238406: step 19045, loss 0.0219035, acc 0.98
2016-09-05T23:32:29.102640: step 19046, loss 0.00311475, acc 1
2016-09-05T23:32:29.897398: step 19047, loss 0.00260149, acc 1
2016-09-05T23:32:30.686346: step 19048, loss 0.00255508, acc 1
2016-09-05T23:32:31.490575: step 19049, loss 0.00328076, acc 1
2016-09-05T23:32:32.333499: step 19050, loss 0.00558326, acc 1
2016-09-05T23:32:33.139430: step 19051, loss 0.00244871, acc 1
2016-09-05T23:32:33.970366: step 19052, loss 0.00251349, acc 1
2016-09-05T23:32:34.773982: step 19053, loss 0.0171065, acc 0.98
2016-09-05T23:32:35.580548: step 19054, loss 0.0184371, acc 1
2016-09-05T23:32:36.411577: step 19055, loss 0.0156368, acc 1
2016-09-05T23:32:37.241073: step 19056, loss 0.0178096, acc 0.98
2016-09-05T23:32:38.026178: step 19057, loss 0.0147456, acc 1
2016-09-05T23:32:38.869035: step 19058, loss 0.00261842, acc 1
2016-09-05T23:32:39.729597: step 19059, loss 0.00282691, acc 1
2016-09-05T23:32:40.556499: step 19060, loss 0.00734958, acc 1
2016-09-05T23:32:41.406251: step 19061, loss 0.00231062, acc 1
2016-09-05T23:32:42.229056: step 19062, loss 0.0160536, acc 1
2016-09-05T23:32:43.029498: step 19063, loss 0.00227426, acc 1
2016-09-05T23:32:43.867180: step 19064, loss 0.00281548, acc 1
2016-09-05T23:32:44.699855: step 19065, loss 0.010373, acc 1
2016-09-05T23:32:45.509379: step 19066, loss 0.00432865, acc 1
2016-09-05T23:32:46.349801: step 19067, loss 0.0495793, acc 0.98
2016-09-05T23:32:47.151708: step 19068, loss 0.0265812, acc 1
2016-09-05T23:32:47.982280: step 19069, loss 0.00961524, acc 1
2016-09-05T23:32:48.778957: step 19070, loss 0.00216876, acc 1
2016-09-05T23:32:49.598701: step 19071, loss 0.00255835, acc 1
2016-09-05T23:32:50.397095: step 19072, loss 0.0023635, acc 1
2016-09-05T23:32:51.187257: step 19073, loss 0.055807, acc 0.98
2016-09-05T23:32:51.993628: step 19074, loss 0.00230408, acc 1
2016-09-05T23:32:52.782357: step 19075, loss 0.0151563, acc 1
2016-09-05T23:32:53.626242: step 19076, loss 0.0378861, acc 0.98
2016-09-05T23:32:54.480373: step 19077, loss 0.00208948, acc 1
2016-09-05T23:32:55.252755: step 19078, loss 0.0271589, acc 0.98
2016-09-05T23:32:56.070197: step 19079, loss 0.00385005, acc 1
2016-09-05T23:32:56.916218: step 19080, loss 0.013711, acc 1
2016-09-05T23:32:57.697558: step 19081, loss 0.00407502, acc 1
2016-09-05T23:32:58.509451: step 19082, loss 0.0273071, acc 0.98
2016-09-05T23:32:59.385407: step 19083, loss 0.004656, acc 1
2016-09-05T23:33:00.191888: step 19084, loss 0.00235073, acc 1
2016-09-05T23:33:01.039189: step 19085, loss 0.00234095, acc 1
2016-09-05T23:33:01.842680: step 19086, loss 0.0253355, acc 0.98
2016-09-05T23:33:02.659732: step 19087, loss 0.00233157, acc 1
2016-09-05T23:33:03.462352: step 19088, loss 0.0111807, acc 1
2016-09-05T23:33:04.266523: step 19089, loss 0.00295196, acc 1
2016-09-05T23:33:05.080200: step 19090, loss 0.00380167, acc 1
2016-09-05T23:33:05.875678: step 19091, loss 0.00337669, acc 1
2016-09-05T23:33:06.688835: step 19092, loss 0.0112241, acc 1
2016-09-05T23:33:07.505638: step 19093, loss 0.0402536, acc 0.98
2016-09-05T23:33:08.323857: step 19094, loss 0.00235628, acc 1
2016-09-05T23:33:09.152464: step 19095, loss 0.0112476, acc 1
2016-09-05T23:33:09.956924: step 19096, loss 0.00239706, acc 1
2016-09-05T23:33:10.793272: step 19097, loss 0.00238938, acc 1
2016-09-05T23:33:11.621637: step 19098, loss 0.00244362, acc 1
2016-09-05T23:33:12.454518: step 19099, loss 0.00249033, acc 1
2016-09-05T23:33:13.270384: step 19100, loss 0.0107863, acc 1

Evaluation:
2016-09-05T23:33:16.782169: step 19100, loss 3.09926, acc 0.732

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-19100

2016-09-05T23:33:18.665566: step 19101, loss 0.025355, acc 0.98
2016-09-05T23:33:19.469573: step 19102, loss 0.0485172, acc 0.96
2016-09-05T23:33:20.320109: step 19103, loss 0.00346226, acc 1
2016-09-05T23:33:21.169927: step 19104, loss 0.0545549, acc 0.96
2016-09-05T23:33:22.027211: step 19105, loss 0.00258071, acc 1
2016-09-05T23:33:22.839698: step 19106, loss 0.00230457, acc 1
2016-09-05T23:33:23.651097: step 19107, loss 0.00472802, acc 1
2016-09-05T23:33:24.532978: step 19108, loss 0.00233384, acc 1
2016-09-05T23:33:25.356773: step 19109, loss 0.00762806, acc 1
2016-09-05T23:33:26.216462: step 19110, loss 0.0400256, acc 0.98
2016-09-05T23:33:27.089984: step 19111, loss 0.0410263, acc 0.96
2016-09-05T23:33:27.901443: step 19112, loss 0.0371012, acc 0.98
2016-09-05T23:33:28.706252: step 19113, loss 0.00236885, acc 1
2016-09-05T23:33:29.531345: step 19114, loss 0.00287098, acc 1
2016-09-05T23:33:30.326843: step 19115, loss 0.0411728, acc 0.98
2016-09-05T23:33:31.144128: step 19116, loss 0.00241611, acc 1
2016-09-05T23:33:31.951927: step 19117, loss 0.00307528, acc 1
2016-09-05T23:33:32.744952: step 19118, loss 0.0208618, acc 0.98
2016-09-05T23:33:33.603592: step 19119, loss 0.00606337, acc 1
2016-09-05T23:33:34.397560: step 19120, loss 0.0535121, acc 0.98
2016-09-05T23:33:35.160976: step 19121, loss 0.0164049, acc 1
2016-09-05T23:33:35.964954: step 19122, loss 0.00239745, acc 1
2016-09-05T23:33:36.819447: step 19123, loss 0.00809255, acc 1
2016-09-05T23:33:37.632314: step 19124, loss 0.00391789, acc 1
2016-09-05T23:33:38.430770: step 19125, loss 0.0193108, acc 0.98
2016-09-05T23:33:39.260132: step 19126, loss 0.0202583, acc 0.98
2016-09-05T23:33:40.062450: step 19127, loss 0.00218274, acc 1
2016-09-05T23:33:40.873442: step 19128, loss 0.0193011, acc 1
2016-09-05T23:33:41.711965: step 19129, loss 0.0163095, acc 1
2016-09-05T23:33:42.537426: step 19130, loss 0.0181198, acc 0.98
2016-09-05T23:33:43.332113: step 19131, loss 0.00216257, acc 1
2016-09-05T23:33:44.174737: step 19132, loss 0.0246836, acc 1
2016-09-05T23:33:44.943766: step 19133, loss 0.00887062, acc 1
2016-09-05T23:33:45.744261: step 19134, loss 0.0151145, acc 1
2016-09-05T23:33:46.584164: step 19135, loss 0.00217658, acc 1
2016-09-05T23:33:47.479573: step 19136, loss 0.00220919, acc 1
2016-09-05T23:33:48.271690: step 19137, loss 0.0259773, acc 0.98
2016-09-05T23:33:49.114893: step 19138, loss 0.0277584, acc 0.98
2016-09-05T23:33:49.941258: step 19139, loss 0.00498189, acc 1
2016-09-05T23:33:50.744722: step 19140, loss 0.00219175, acc 1
2016-09-05T23:33:51.566919: step 19141, loss 0.0181061, acc 0.98
2016-09-05T23:33:52.396372: step 19142, loss 0.0153537, acc 1
2016-09-05T23:33:53.220290: step 19143, loss 0.0373081, acc 0.98
2016-09-05T23:33:54.053760: step 19144, loss 0.00222561, acc 1
2016-09-05T23:33:54.893534: step 19145, loss 0.00393435, acc 1
2016-09-05T23:33:55.673611: step 19146, loss 0.015835, acc 1
2016-09-05T23:33:56.469631: step 19147, loss 0.0242881, acc 0.98
2016-09-05T23:33:57.281155: step 19148, loss 0.00360133, acc 1
2016-09-05T23:33:58.104551: step 19149, loss 0.007892, acc 1
2016-09-05T23:33:58.884718: step 19150, loss 0.00206502, acc 1
2016-09-05T23:33:59.718329: step 19151, loss 0.00212907, acc 1
2016-09-05T23:34:00.544303: step 19152, loss 0.0424135, acc 0.98
2016-09-05T23:34:01.348579: step 19153, loss 0.00205842, acc 1
2016-09-05T23:34:02.173652: step 19154, loss 0.0507673, acc 0.98
2016-09-05T23:34:02.962255: step 19155, loss 0.0352433, acc 0.98
2016-09-05T23:34:03.807137: step 19156, loss 0.00197985, acc 1
2016-09-05T23:34:04.619069: step 19157, loss 0.0108034, acc 1
2016-09-05T23:34:05.420582: step 19158, loss 0.00185928, acc 1
2016-09-05T23:34:06.218574: step 19159, loss 0.0129508, acc 1
2016-09-05T23:34:07.038853: step 19160, loss 0.0191278, acc 0.98
2016-09-05T23:34:07.867598: step 19161, loss 0.00320793, acc 1
2016-09-05T23:34:08.671164: step 19162, loss 0.00286845, acc 1
2016-09-05T23:34:09.494204: step 19163, loss 0.0217349, acc 0.98
2016-09-05T23:34:10.298771: step 19164, loss 0.039669, acc 0.96
2016-09-05T23:34:11.113486: step 19165, loss 0.0205186, acc 0.98
2016-09-05T23:34:11.961843: step 19166, loss 0.0539646, acc 0.98
2016-09-05T23:34:12.768837: step 19167, loss 0.0248287, acc 0.98
2016-09-05T23:34:13.573544: step 19168, loss 0.00168258, acc 1
2016-09-05T23:34:14.386688: step 19169, loss 0.00160511, acc 1
2016-09-05T23:34:15.156950: step 19170, loss 0.00198968, acc 1
2016-09-05T23:34:15.985603: step 19171, loss 0.016991, acc 0.98
2016-09-05T23:34:16.790544: step 19172, loss 0.00303537, acc 1
2016-09-05T23:34:17.593688: step 19173, loss 0.0134662, acc 1
2016-09-05T23:34:18.404506: step 19174, loss 0.0197722, acc 0.98
2016-09-05T23:34:19.234947: step 19175, loss 0.0216763, acc 0.98
2016-09-05T23:34:20.042796: step 19176, loss 0.00205725, acc 1
2016-09-05T23:34:20.856321: step 19177, loss 0.00150061, acc 1
2016-09-05T23:34:21.683501: step 19178, loss 0.0214948, acc 0.98
2016-09-05T23:34:22.489494: step 19179, loss 0.00159184, acc 1
2016-09-05T23:34:23.312303: step 19180, loss 0.00236951, acc 1
2016-09-05T23:34:24.141486: step 19181, loss 0.0193043, acc 0.98
2016-09-05T23:34:24.943540: step 19182, loss 0.0180164, acc 1
2016-09-05T23:34:25.750852: step 19183, loss 0.0104018, acc 1
2016-09-05T23:34:26.619948: step 19184, loss 0.027823, acc 0.98
2016-09-05T23:34:27.425822: step 19185, loss 0.00203877, acc 1
2016-09-05T23:34:28.214319: step 19186, loss 0.0062263, acc 1
2016-09-05T23:34:29.057678: step 19187, loss 0.00823836, acc 1
2016-09-05T23:34:29.878221: step 19188, loss 0.017067, acc 0.98
2016-09-05T23:34:30.700912: step 19189, loss 0.00890369, acc 1
2016-09-05T23:34:31.529332: step 19190, loss 0.0620366, acc 0.98
2016-09-05T23:34:32.328617: step 19191, loss 0.0199146, acc 0.98
2016-09-05T23:34:33.122471: step 19192, loss 0.0310901, acc 0.98
2016-09-05T23:34:33.953228: step 19193, loss 0.0163714, acc 1
2016-09-05T23:34:34.754928: step 19194, loss 0.00259183, acc 1
2016-09-05T23:34:35.545239: step 19195, loss 0.00916522, acc 1
2016-09-05T23:34:36.384380: step 19196, loss 0.00960888, acc 1
2016-09-05T23:34:37.210904: step 19197, loss 0.00260788, acc 1
2016-09-05T23:34:37.990025: step 19198, loss 0.0230275, acc 1
2016-09-05T23:34:38.803221: step 19199, loss 0.0263144, acc 0.98
2016-09-05T23:34:39.779011: step 19200, loss 0.00222171, acc 1

Evaluation:
2016-09-05T23:34:43.289040: step 19200, loss 3.14597, acc 0.714

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-19200

2016-09-05T23:34:45.117595: step 19201, loss 0.0413446, acc 0.98
2016-09-05T23:34:45.990713: step 19202, loss 0.0171242, acc 0.98
2016-09-05T23:34:46.913202: step 19203, loss 0.0102503, acc 1
2016-09-05T23:34:47.718213: step 19204, loss 0.0146918, acc 1
2016-09-05T23:34:48.554797: step 19205, loss 0.043507, acc 0.98
2016-09-05T23:34:48.979649: step 19206, loss 0.00239937, acc 1
2016-09-05T23:34:49.804527: step 19207, loss 0.0302505, acc 0.98
2016-09-05T23:34:50.656159: step 19208, loss 0.055317, acc 0.98
2016-09-05T23:34:51.450444: step 19209, loss 0.00317082, acc 1
2016-09-05T23:34:52.265413: step 19210, loss 0.00278327, acc 1
2016-09-05T23:34:53.109314: step 19211, loss 0.0026111, acc 1
2016-09-05T23:34:53.909761: step 19212, loss 0.0207087, acc 0.98
2016-09-05T23:34:54.728548: step 19213, loss 0.0226706, acc 0.98
2016-09-05T23:34:55.587828: step 19214, loss 0.00870052, acc 1
2016-09-05T23:34:56.445380: step 19215, loss 0.00261224, acc 1
2016-09-05T23:34:57.232795: step 19216, loss 0.0172114, acc 1
2016-09-05T23:34:58.081097: step 19217, loss 0.0144028, acc 1
2016-09-05T23:34:58.887680: step 19218, loss 0.018331, acc 0.98
2016-09-05T23:34:59.700881: step 19219, loss 0.0163494, acc 1
2016-09-05T23:35:00.505165: step 19220, loss 0.00530297, acc 1
2016-09-05T23:35:01.324057: step 19221, loss 0.00264214, acc 1
2016-09-05T23:35:02.114323: step 19222, loss 0.00265095, acc 1
2016-09-05T23:35:02.964836: step 19223, loss 0.0297345, acc 0.98
2016-09-05T23:35:03.829466: step 19224, loss 0.00269511, acc 1
2016-09-05T23:35:04.656392: step 19225, loss 0.0112654, acc 1
2016-09-05T23:35:05.480470: step 19226, loss 0.0767867, acc 0.98
2016-09-05T23:35:06.297431: step 19227, loss 0.00339946, acc 1
2016-09-05T23:35:07.114637: step 19228, loss 0.0119114, acc 1
2016-09-05T23:35:07.972921: step 19229, loss 0.0032048, acc 1
2016-09-05T23:35:08.802592: step 19230, loss 0.00266819, acc 1
2016-09-05T23:35:09.610836: step 19231, loss 0.01186, acc 1
2016-09-05T23:35:10.418518: step 19232, loss 0.00422259, acc 1
2016-09-05T23:35:11.257779: step 19233, loss 0.00299705, acc 1
2016-09-05T23:35:12.035142: step 19234, loss 0.022216, acc 1
2016-09-05T23:35:12.837161: step 19235, loss 0.0436806, acc 0.98
2016-09-05T23:35:13.672465: step 19236, loss 0.00308449, acc 1
2016-09-05T23:35:14.507608: step 19237, loss 0.00478861, acc 1
2016-09-05T23:35:15.320917: step 19238, loss 0.0179798, acc 1
2016-09-05T23:35:16.143624: step 19239, loss 0.00276051, acc 1
2016-09-05T23:35:16.957784: step 19240, loss 0.0089736, acc 1
2016-09-05T23:35:17.756439: step 19241, loss 0.00277308, acc 1
2016-09-05T23:35:18.569879: step 19242, loss 0.00288228, acc 1
2016-09-05T23:35:19.457645: step 19243, loss 0.00487198, acc 1
2016-09-05T23:35:20.248550: step 19244, loss 0.0312486, acc 0.98
2016-09-05T23:35:21.086731: step 19245, loss 0.00294315, acc 1
2016-09-05T23:35:21.955936: step 19246, loss 0.00402165, acc 1
2016-09-05T23:35:22.791548: step 19247, loss 0.0248231, acc 1
2016-09-05T23:35:23.621012: step 19248, loss 0.029473, acc 0.98
2016-09-05T23:35:24.451743: step 19249, loss 0.0181696, acc 0.98
2016-09-05T23:35:25.348877: step 19250, loss 0.00299117, acc 1
2016-09-05T23:35:26.204921: step 19251, loss 0.00526991, acc 1
2016-09-05T23:35:27.154128: step 19252, loss 0.123844, acc 0.96
2016-09-05T23:35:27.968225: step 19253, loss 0.00721947, acc 1
2016-09-05T23:35:28.908241: step 19254, loss 0.00297268, acc 1
2016-09-05T23:35:29.863743: step 19255, loss 0.00289774, acc 1
2016-09-05T23:35:30.913256: step 19256, loss 0.0402846, acc 0.98
2016-09-05T23:35:31.765442: step 19257, loss 0.00307202, acc 1
2016-09-05T23:35:32.701153: step 19258, loss 0.00875469, acc 1
2016-09-05T23:35:33.538005: step 19259, loss 0.0115832, acc 1
2016-09-05T23:35:34.414868: step 19260, loss 0.00426487, acc 1
2016-09-05T23:35:35.252710: step 19261, loss 0.023087, acc 0.98
2016-09-05T23:35:36.107049: step 19262, loss 0.00307513, acc 1
2016-09-05T23:35:36.964613: step 19263, loss 0.018646, acc 0.98
2016-09-05T23:35:37.875301: step 19264, loss 0.00305385, acc 1
2016-09-05T23:35:38.688595: step 19265, loss 0.00262747, acc 1
2016-09-05T23:35:39.584215: step 19266, loss 0.00573, acc 1
2016-09-05T23:35:40.480849: step 19267, loss 0.0169179, acc 0.98
2016-09-05T23:35:41.365368: step 19268, loss 0.00324435, acc 1
2016-09-05T23:35:42.257968: step 19269, loss 0.0142592, acc 1
2016-09-05T23:35:43.127314: step 19270, loss 0.0161829, acc 1
2016-09-05T23:35:43.954887: step 19271, loss 0.0255232, acc 0.98
2016-09-05T23:35:44.815028: step 19272, loss 0.0125817, acc 1
2016-09-05T23:35:45.637746: step 19273, loss 0.0147633, acc 1
2016-09-05T23:35:46.484068: step 19274, loss 0.00247832, acc 1
2016-09-05T23:35:47.307923: step 19275, loss 0.00249964, acc 1
2016-09-05T23:35:48.149108: step 19276, loss 0.0172241, acc 1
2016-09-05T23:35:48.986207: step 19277, loss 0.00380123, acc 1
2016-09-05T23:35:49.961025: step 19278, loss 0.0223772, acc 0.98
2016-09-05T23:35:50.805525: step 19279, loss 0.00242138, acc 1
2016-09-05T23:35:51.662716: step 19280, loss 0.00266677, acc 1
2016-09-05T23:35:52.493425: step 19281, loss 0.0855925, acc 0.98
2016-09-05T23:35:53.317655: step 19282, loss 0.0323123, acc 1
2016-09-05T23:35:54.300448: step 19283, loss 0.0241323, acc 0.98
2016-09-05T23:35:55.132670: step 19284, loss 0.0347179, acc 0.98
2016-09-05T23:35:56.009784: step 19285, loss 0.0139268, acc 1
2016-09-05T23:35:57.026826: step 19286, loss 0.0273746, acc 0.98
2016-09-05T23:35:57.938253: step 19287, loss 0.00258534, acc 1
2016-09-05T23:35:58.786520: step 19288, loss 0.00289273, acc 1
2016-09-05T23:35:59.639132: step 19289, loss 0.0154393, acc 1
2016-09-05T23:36:00.564796: step 19290, loss 0.00311966, acc 1
2016-09-05T23:36:01.448623: step 19291, loss 0.00274693, acc 1
2016-09-05T23:36:02.341670: step 19292, loss 0.00767914, acc 1
2016-09-05T23:36:03.265734: step 19293, loss 0.0268446, acc 0.98
2016-09-05T23:36:04.077848: step 19294, loss 0.0131135, acc 1
2016-09-05T23:36:04.963167: step 19295, loss 0.124147, acc 0.98
2016-09-05T23:36:05.810645: step 19296, loss 0.100079, acc 0.98
2016-09-05T23:36:06.619168: step 19297, loss 0.0189047, acc 0.98
2016-09-05T23:36:07.444647: step 19298, loss 0.00814571, acc 1
2016-09-05T23:36:08.273432: step 19299, loss 0.00488401, acc 1
2016-09-05T23:36:09.106486: step 19300, loss 0.0118126, acc 1

Evaluation:
2016-09-05T23:36:12.593233: step 19300, loss 3.03337, acc 0.708

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-19300

2016-09-05T23:36:14.486707: step 19301, loss 0.0219113, acc 1
2016-09-05T23:36:15.296047: step 19302, loss 0.00256848, acc 1
2016-09-05T23:36:16.100545: step 19303, loss 0.00245511, acc 1
2016-09-05T23:36:16.927606: step 19304, loss 0.0359284, acc 0.96
2016-09-05T23:36:17.742402: step 19305, loss 0.00261487, acc 1
2016-09-05T23:36:18.516597: step 19306, loss 0.00283841, acc 1
2016-09-05T23:36:19.336256: step 19307, loss 0.0190836, acc 1
2016-09-05T23:36:20.160402: step 19308, loss 0.00388774, acc 1
2016-09-05T23:36:20.950171: step 19309, loss 0.00543536, acc 1
2016-09-05T23:36:21.754101: step 19310, loss 0.016026, acc 1
2016-09-05T23:36:22.604869: step 19311, loss 0.00565682, acc 1
2016-09-05T23:36:23.376222: step 19312, loss 0.00317439, acc 1
2016-09-05T23:36:24.178428: step 19313, loss 0.0282731, acc 1
2016-09-05T23:36:24.988732: step 19314, loss 0.012758, acc 1
2016-09-05T23:36:25.771045: step 19315, loss 0.00280807, acc 1
2016-09-05T23:36:26.562027: step 19316, loss 0.0091033, acc 1
2016-09-05T23:36:27.386620: step 19317, loss 0.00335701, acc 1
2016-09-05T23:36:28.161076: step 19318, loss 0.0028054, acc 1
2016-09-05T23:36:28.994356: step 19319, loss 0.0901762, acc 0.98
2016-09-05T23:36:29.909267: step 19320, loss 0.00805464, acc 1
2016-09-05T23:36:30.738645: step 19321, loss 0.00267391, acc 1
2016-09-05T23:36:31.535114: step 19322, loss 0.00593306, acc 1
2016-09-05T23:36:32.388514: step 19323, loss 0.016808, acc 0.98
2016-09-05T23:36:33.223607: step 19324, loss 0.0164669, acc 1
2016-09-05T23:36:34.059375: step 19325, loss 0.00286073, acc 1
2016-09-05T23:36:34.894360: step 19326, loss 0.00359599, acc 1
2016-09-05T23:36:35.687151: step 19327, loss 0.00261535, acc 1
2016-09-05T23:36:36.525332: step 19328, loss 0.00987033, acc 1
2016-09-05T23:36:37.376798: step 19329, loss 0.0193803, acc 0.98
2016-09-05T23:36:38.208703: step 19330, loss 0.00246387, acc 1
2016-09-05T23:36:39.022516: step 19331, loss 0.0103955, acc 1
2016-09-05T23:36:39.836490: step 19332, loss 0.0159524, acc 1
2016-09-05T23:36:40.626740: step 19333, loss 0.00257196, acc 1
2016-09-05T23:36:41.407472: step 19334, loss 0.00391646, acc 1
2016-09-05T23:36:42.233925: step 19335, loss 0.00277062, acc 1
2016-09-05T23:36:43.037334: step 19336, loss 0.0184625, acc 0.98
2016-09-05T23:36:43.850013: step 19337, loss 0.0123377, acc 1
2016-09-05T23:36:44.679588: step 19338, loss 0.00393747, acc 1
2016-09-05T23:36:45.568079: step 19339, loss 0.00678518, acc 1
2016-09-05T23:36:46.362880: step 19340, loss 0.00235081, acc 1
2016-09-05T23:36:47.173018: step 19341, loss 0.00989484, acc 1
2016-09-05T23:36:47.993678: step 19342, loss 0.00271388, acc 1
2016-09-05T23:36:48.784188: step 19343, loss 0.0257023, acc 0.98
2016-09-05T23:36:49.550896: step 19344, loss 0.00226757, acc 1
2016-09-05T23:36:50.394348: step 19345, loss 0.00223772, acc 1
2016-09-05T23:36:51.160573: step 19346, loss 0.0150832, acc 1
2016-09-05T23:36:51.954751: step 19347, loss 0.00258204, acc 1
2016-09-05T23:36:52.765921: step 19348, loss 0.00281421, acc 1
2016-09-05T23:36:53.548789: step 19349, loss 0.00375761, acc 1
2016-09-05T23:36:54.354898: step 19350, loss 0.00792421, acc 1
2016-09-05T23:36:55.180543: step 19351, loss 0.00762276, acc 1
2016-09-05T23:36:55.963322: step 19352, loss 0.00210028, acc 1
2016-09-05T23:36:56.767746: step 19353, loss 0.0352326, acc 0.98
2016-09-05T23:36:57.585874: step 19354, loss 0.0139879, acc 1
2016-09-05T23:36:58.373266: step 19355, loss 0.0130906, acc 1
2016-09-05T23:36:59.178607: step 19356, loss 0.00347596, acc 1
2016-09-05T23:36:59.983410: step 19357, loss 0.0131133, acc 1
2016-09-05T23:37:00.802962: step 19358, loss 0.0206282, acc 1
2016-09-05T23:37:01.596162: step 19359, loss 0.0632855, acc 0.98
2016-09-05T23:37:02.418193: step 19360, loss 0.0150787, acc 1
2016-09-05T23:37:03.190864: step 19361, loss 0.00207534, acc 1
2016-09-05T23:37:04.002651: step 19362, loss 0.00188785, acc 1
2016-09-05T23:37:04.831245: step 19363, loss 0.00187283, acc 1
2016-09-05T23:37:05.606657: step 19364, loss 0.00564887, acc 1
2016-09-05T23:37:06.456543: step 19365, loss 0.00182958, acc 1
2016-09-05T23:37:07.293433: step 19366, loss 0.00444995, acc 1
2016-09-05T23:37:08.061566: step 19367, loss 0.00195311, acc 1
2016-09-05T23:37:08.852211: step 19368, loss 0.00932905, acc 1
2016-09-05T23:37:09.648903: step 19369, loss 0.0336953, acc 0.98
2016-09-05T23:37:10.434466: step 19370, loss 0.00255159, acc 1
2016-09-05T23:37:11.249049: step 19371, loss 0.00177387, acc 1
2016-09-05T23:37:12.052854: step 19372, loss 0.154729, acc 0.98
2016-09-05T23:37:12.821353: step 19373, loss 0.00165257, acc 1
2016-09-05T23:37:13.667422: step 19374, loss 0.0145703, acc 1
2016-09-05T23:37:14.488169: step 19375, loss 0.0405588, acc 0.98
2016-09-05T23:37:15.288257: step 19376, loss 0.0429759, acc 0.98
2016-09-05T23:37:16.094477: step 19377, loss 0.00828568, acc 1
2016-09-05T23:37:16.922924: step 19378, loss 0.0178188, acc 0.98
2016-09-05T23:37:17.724727: step 19379, loss 0.00523257, acc 1
2016-09-05T23:37:18.523842: step 19380, loss 0.00344583, acc 1
2016-09-05T23:37:19.332163: step 19381, loss 0.0514401, acc 0.98
2016-09-05T23:37:20.103029: step 19382, loss 0.00252616, acc 1
2016-09-05T23:37:20.905964: step 19383, loss 0.00176772, acc 1
2016-09-05T23:37:21.736849: step 19384, loss 0.003843, acc 1
2016-09-05T23:37:22.548431: step 19385, loss 0.00963319, acc 1
2016-09-05T23:37:23.341562: step 19386, loss 0.0351683, acc 0.98
2016-09-05T23:37:24.150425: step 19387, loss 0.0257345, acc 1
2016-09-05T23:37:24.922280: step 19388, loss 0.0017302, acc 1
2016-09-05T23:37:25.717435: step 19389, loss 0.00337972, acc 1
2016-09-05T23:37:26.558400: step 19390, loss 0.0165579, acc 0.98
2016-09-05T23:37:27.391107: step 19391, loss 0.0136732, acc 1
2016-09-05T23:37:28.198332: step 19392, loss 0.00211199, acc 1
2016-09-05T23:37:29.037634: step 19393, loss 0.0195615, acc 1
2016-09-05T23:37:29.837399: step 19394, loss 0.00211571, acc 1
2016-09-05T23:37:30.646566: step 19395, loss 0.00861881, acc 1
2016-09-05T23:37:31.453459: step 19396, loss 0.0202839, acc 1
2016-09-05T23:37:32.234295: step 19397, loss 0.00893267, acc 1
2016-09-05T23:37:33.020787: step 19398, loss 0.0363677, acc 0.98
2016-09-05T23:37:33.866810: step 19399, loss 0.00180055, acc 1
2016-09-05T23:37:34.290506: step 19400, loss 0.0117504, acc 1

Evaluation:
2016-09-05T23:37:37.851496: step 19400, loss 2.63854, acc 0.713

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473073177/checkpoints/model-19400


WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7ff3c59fee90>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.
WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7ff3c59fee50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.

Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=50
CHECKPOINT_EVERY=100
CV_INDEX=2
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=300
EVALUATE_EVERY=100
FILTER_SIZES=3,4,5
HIDDEN_DIM=300
L2_REG_LAMBDA=0.15
LOG_DEVICE_PLACEMENT=False
NUM_EPOCHS=100
NUM_FILTERS=100
WORD2VEC=GoogleNews-vectors-negative300.bin

Loading data...
Vocabulary Size: 18758
Train/Dev split: 9594/1066
Writing to /home/cil/lstm-context-embeddings/runs/1473123033

Load word2vec file GoogleNews-vectors-negative300.bin

2016-09-06T08:50:53.450136: step 1, loss 0.693147, acc 0.66
2016-09-06T08:50:54.276885: step 2, loss 0.687412, acc 0.52
2016-09-06T08:50:55.090350: step 3, loss 0.702082, acc 0.52
2016-09-06T08:50:55.917330: step 4, loss 0.701284, acc 0.54
2016-09-06T08:50:56.777942: step 5, loss 0.697171, acc 0.56
2016-09-06T08:50:57.588029: step 6, loss 0.727124, acc 0.46
2016-09-06T08:50:58.409446: step 7, loss 0.687124, acc 0.6
2016-09-06T08:50:59.242849: step 8, loss 0.696429, acc 0.4
2016-09-06T08:51:00.050076: step 9, loss 0.69554, acc 0.42
2016-09-06T08:51:00.901312: step 10, loss 0.708034, acc 0.48
2016-09-06T08:51:01.715003: step 11, loss 0.693711, acc 0.5
2016-09-06T08:51:02.515356: step 12, loss 0.69789, acc 0.58
2016-09-06T08:51:03.320954: step 13, loss 0.711175, acc 0.34
2016-09-06T08:51:04.142771: step 14, loss 0.691738, acc 0.54
2016-09-06T08:51:04.990099: step 15, loss 0.685571, acc 0.52
2016-09-06T08:51:05.806975: step 16, loss 0.683477, acc 0.52
2016-09-06T08:51:06.595036: step 17, loss 0.705968, acc 0.46
2016-09-06T08:51:07.411935: step 18, loss 0.695516, acc 0.5
2016-09-06T08:51:08.217039: step 19, loss 0.702778, acc 0.44
2016-09-06T08:51:09.061333: step 20, loss 0.694585, acc 0.48
2016-09-06T08:51:09.872050: step 21, loss 0.695547, acc 0.6
2016-09-06T08:51:10.671347: step 22, loss 0.705295, acc 0.46
2016-09-06T08:51:11.474183: step 23, loss 0.701348, acc 0.5
2016-09-06T08:51:12.296186: step 24, loss 0.680112, acc 0.6
2016-09-06T08:51:13.070317: step 25, loss 0.664996, acc 0.68
2016-09-06T08:51:13.841708: step 26, loss 0.700717, acc 0.52
2016-09-06T08:51:14.665794: step 27, loss 0.673526, acc 0.58
2016-09-06T08:51:15.450665: step 28, loss 0.663089, acc 0.64
2016-09-06T08:51:16.267695: step 29, loss 0.677801, acc 0.62
2016-09-06T08:51:17.114902: step 30, loss 0.643573, acc 0.66
2016-09-06T08:51:17.917061: step 31, loss 0.605796, acc 0.64
2016-09-06T08:51:18.722261: step 32, loss 0.776501, acc 0.58
2016-09-06T08:51:19.535952: step 33, loss 0.611399, acc 0.66
2016-09-06T08:51:20.309821: step 34, loss 0.715083, acc 0.64
2016-09-06T08:51:21.083649: step 35, loss 0.703838, acc 0.48
2016-09-06T08:51:21.880673: step 36, loss 0.643514, acc 0.7
2016-09-06T08:51:22.684226: step 37, loss 0.587629, acc 0.74
2016-09-06T08:51:23.485893: step 38, loss 0.970013, acc 0.44
2016-09-06T08:51:24.300645: step 39, loss 0.716967, acc 0.62
2016-09-06T08:51:25.077654: step 40, loss 0.715672, acc 0.48
2016-09-06T08:51:25.920427: step 41, loss 0.689601, acc 0.6
2016-09-06T08:51:26.721301: step 42, loss 0.645278, acc 0.66
2016-09-06T08:51:27.500707: step 43, loss 0.67423, acc 0.56
2016-09-06T08:51:28.335952: step 44, loss 0.733572, acc 0.52
2016-09-06T08:51:29.154707: step 45, loss 0.635365, acc 0.68
2016-09-06T08:51:29.945341: step 46, loss 0.748437, acc 0.5
2016-09-06T08:51:30.730417: step 47, loss 0.671202, acc 0.54
2016-09-06T08:51:31.523638: step 48, loss 0.661487, acc 0.68
2016-09-06T08:51:32.319100: step 49, loss 0.666239, acc 0.58
2016-09-06T08:51:33.143990: step 50, loss 0.65725, acc 0.62
2016-09-06T08:51:33.974007: step 51, loss 0.667975, acc 0.64
2016-09-06T08:51:34.773566: step 52, loss 0.665292, acc 0.6
2016-09-06T08:51:35.571234: step 53, loss 0.665993, acc 0.6
2016-09-06T08:51:36.403716: step 54, loss 0.62899, acc 0.7
2016-09-06T08:51:37.197522: step 55, loss 0.638477, acc 0.62
2016-09-06T08:51:38.007088: step 56, loss 0.687382, acc 0.58
2016-09-06T08:51:38.817909: step 57, loss 0.659693, acc 0.56
2016-09-06T08:51:39.615210: step 58, loss 0.601457, acc 0.78
2016-09-06T08:51:40.436761: step 59, loss 0.599564, acc 0.66
2016-09-06T08:51:41.254416: step 60, loss 0.612576, acc 0.72
2016-09-06T08:51:42.074695: step 61, loss 0.596251, acc 0.78
2016-09-06T08:51:42.850271: step 62, loss 0.628369, acc 0.64
2016-09-06T08:51:43.669588: step 63, loss 0.525279, acc 0.78
2016-09-06T08:51:44.458502: step 64, loss 0.603273, acc 0.68
2016-09-06T08:51:45.286356: step 65, loss 0.601955, acc 0.7
2016-09-06T08:51:46.096576: step 66, loss 0.501326, acc 0.76
2016-09-06T08:51:46.879418: step 67, loss 0.670693, acc 0.7
2016-09-06T08:51:47.717130: step 68, loss 0.770121, acc 0.64
2016-09-06T08:51:48.535460: step 69, loss 0.584847, acc 0.62
2016-09-06T08:51:49.323126: step 70, loss 0.678634, acc 0.7
2016-09-06T08:51:50.104648: step 71, loss 0.596469, acc 0.66
2016-09-06T08:51:50.925134: step 72, loss 0.584665, acc 0.72
2016-09-06T08:51:51.693428: step 73, loss 0.556244, acc 0.74
2016-09-06T08:51:52.515891: step 74, loss 0.646891, acc 0.66
2016-09-06T08:51:53.338872: step 75, loss 0.647506, acc 0.58
2016-09-06T08:51:54.119523: step 76, loss 0.658899, acc 0.6
2016-09-06T08:51:54.934664: step 77, loss 0.585767, acc 0.74
2016-09-06T08:51:55.766919: step 78, loss 0.616542, acc 0.6
2016-09-06T08:51:56.535490: step 79, loss 0.645647, acc 0.62
2016-09-06T08:51:57.329303: step 80, loss 0.618789, acc 0.66
2016-09-06T08:51:58.156246: step 81, loss 0.570658, acc 0.74
2016-09-06T08:51:58.924861: step 82, loss 0.608764, acc 0.64
2016-09-06T08:51:59.723663: step 83, loss 0.639913, acc 0.56
2016-09-06T08:52:00.580470: step 84, loss 0.608479, acc 0.62
2016-09-06T08:52:01.338018: step 85, loss 0.529437, acc 0.72
2016-09-06T08:52:02.127797: step 86, loss 0.721877, acc 0.58
2016-09-06T08:52:02.933688: step 87, loss 0.545926, acc 0.66
2016-09-06T08:52:03.731354: step 88, loss 0.524219, acc 0.72
2016-09-06T08:52:04.547968: step 89, loss 0.676346, acc 0.66
2016-09-06T08:52:05.380705: step 90, loss 0.583798, acc 0.72
2016-09-06T08:52:06.176049: step 91, loss 0.694601, acc 0.66
2016-09-06T08:52:06.995393: step 92, loss 0.65779, acc 0.62
2016-09-06T08:52:07.816129: step 93, loss 0.61739, acc 0.72
2016-09-06T08:52:08.610138: step 94, loss 0.510441, acc 0.82
2016-09-06T08:52:09.424105: step 95, loss 0.549222, acc 0.78
2016-09-06T08:52:10.239059: step 96, loss 0.581441, acc 0.66
2016-09-06T08:52:11.017917: step 97, loss 0.569554, acc 0.68
2016-09-06T08:52:11.818845: step 98, loss 0.700214, acc 0.58
2016-09-06T08:52:12.628839: step 99, loss 0.52666, acc 0.78
2016-09-06T08:52:13.403876: step 100, loss 0.564646, acc 0.68

Evaluation:
2016-09-06T08:52:17.149479: step 100, loss 0.517591, acc 0.765478

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-100

2016-09-06T08:52:19.220986: step 101, loss 0.609932, acc 0.56
2016-09-06T08:52:20.035346: step 102, loss 0.54628, acc 0.64
2016-09-06T08:52:20.838292: step 103, loss 0.584274, acc 0.74
2016-09-06T08:52:21.669062: step 104, loss 0.587628, acc 0.68
2016-09-06T08:52:22.522454: step 105, loss 0.595626, acc 0.64
2016-09-06T08:52:23.328738: step 106, loss 0.558603, acc 0.74
2016-09-06T08:52:24.125921: step 107, loss 0.629511, acc 0.62
2016-09-06T08:52:24.957986: step 108, loss 0.451226, acc 0.78
2016-09-06T08:52:25.762054: step 109, loss 0.546957, acc 0.7
2016-09-06T08:52:26.595958: step 110, loss 0.625985, acc 0.64
2016-09-06T08:52:27.439807: step 111, loss 0.538238, acc 0.7
2016-09-06T08:52:28.238961: step 112, loss 0.535157, acc 0.7
2016-09-06T08:52:29.048157: step 113, loss 0.469827, acc 0.78
2016-09-06T08:52:29.880173: step 114, loss 0.674592, acc 0.54
2016-09-06T08:52:30.666377: step 115, loss 0.551644, acc 0.72
2016-09-06T08:52:31.479935: step 116, loss 0.419147, acc 0.8
2016-09-06T08:52:32.290899: step 117, loss 0.477477, acc 0.8
2016-09-06T08:52:33.107471: step 118, loss 0.461658, acc 0.76
2016-09-06T08:52:33.928319: step 119, loss 0.464555, acc 0.74
2016-09-06T08:52:34.759859: step 120, loss 0.591358, acc 0.72
2016-09-06T08:52:35.590159: step 121, loss 0.512117, acc 0.76
2016-09-06T08:52:36.400737: step 122, loss 0.538574, acc 0.76
2016-09-06T08:52:37.235348: step 123, loss 0.533791, acc 0.72
2016-09-06T08:52:38.057206: step 124, loss 0.608748, acc 0.64
2016-09-06T08:52:38.905519: step 125, loss 0.640783, acc 0.62
2016-09-06T08:52:39.689628: step 126, loss 0.577374, acc 0.72
2016-09-06T08:52:40.522777: step 127, loss 0.526091, acc 0.76
2016-09-06T08:52:41.301989: step 128, loss 0.490454, acc 0.78
2016-09-06T08:52:42.112622: step 129, loss 0.602948, acc 0.7
2016-09-06T08:52:42.903001: step 130, loss 0.601699, acc 0.7
2016-09-06T08:52:43.686612: step 131, loss 0.506594, acc 0.74
2016-09-06T08:52:44.478111: step 132, loss 0.471855, acc 0.72
2016-09-06T08:52:45.291549: step 133, loss 0.490528, acc 0.8
2016-09-06T08:52:46.078591: step 134, loss 0.514277, acc 0.8
2016-09-06T08:52:46.889916: step 135, loss 0.563426, acc 0.74
2016-09-06T08:52:47.699650: step 136, loss 0.496409, acc 0.76
2016-09-06T08:52:48.513767: step 137, loss 0.339538, acc 0.9
2016-09-06T08:52:49.312594: step 138, loss 0.471698, acc 0.82
2016-09-06T08:52:50.127383: step 139, loss 0.56561, acc 0.66
2016-09-06T08:52:50.907858: step 140, loss 0.41863, acc 0.78
2016-09-06T08:52:51.722318: step 141, loss 0.366724, acc 0.84
2016-09-06T08:52:52.522586: step 142, loss 0.496774, acc 0.8
2016-09-06T08:52:53.317479: step 143, loss 0.443112, acc 0.8
2016-09-06T08:52:54.132603: step 144, loss 0.587456, acc 0.72
2016-09-06T08:52:54.962273: step 145, loss 0.4827, acc 0.72
2016-09-06T08:52:55.761291: step 146, loss 0.384343, acc 0.86
2016-09-06T08:52:56.561908: step 147, loss 0.556108, acc 0.72
2016-09-06T08:52:57.349159: step 148, loss 0.621543, acc 0.72
2016-09-06T08:52:58.150080: step 149, loss 0.627062, acc 0.66
2016-09-06T08:52:58.952972: step 150, loss 0.539874, acc 0.78
2016-09-06T08:52:59.755908: step 151, loss 0.552354, acc 0.74
2016-09-06T08:53:00.585431: step 152, loss 0.527465, acc 0.78
2016-09-06T08:53:01.390805: step 153, loss 0.556949, acc 0.74
2016-09-06T08:53:02.201476: step 154, loss 0.605593, acc 0.66
2016-09-06T08:53:03.032408: step 155, loss 0.434019, acc 0.8
2016-09-06T08:53:03.822303: step 156, loss 0.532703, acc 0.78
2016-09-06T08:53:04.642278: step 157, loss 0.46624, acc 0.8
2016-09-06T08:53:05.445377: step 158, loss 0.561882, acc 0.68
2016-09-06T08:53:06.257221: step 159, loss 0.51035, acc 0.78
2016-09-06T08:53:07.072817: step 160, loss 0.47312, acc 0.78
2016-09-06T08:53:07.845285: step 161, loss 0.477599, acc 0.8
2016-09-06T08:53:08.672799: step 162, loss 0.570437, acc 0.72
2016-09-06T08:53:09.496857: step 163, loss 0.541279, acc 0.68
2016-09-06T08:53:10.293684: step 164, loss 0.51199, acc 0.82
2016-09-06T08:53:11.104185: step 165, loss 0.48402, acc 0.82
2016-09-06T08:53:11.929444: step 166, loss 0.494245, acc 0.72
2016-09-06T08:53:12.721887: step 167, loss 0.445611, acc 0.76
2016-09-06T08:53:13.528834: step 168, loss 0.439789, acc 0.82
2016-09-06T08:53:14.350075: step 169, loss 0.580413, acc 0.64
2016-09-06T08:53:15.140205: step 170, loss 0.453724, acc 0.72
2016-09-06T08:53:15.937007: step 171, loss 0.538888, acc 0.74
2016-09-06T08:53:16.754104: step 172, loss 0.386762, acc 0.8
2016-09-06T08:53:17.540383: step 173, loss 0.426146, acc 0.76
2016-09-06T08:53:18.334087: step 174, loss 0.477986, acc 0.74
2016-09-06T08:53:19.159102: step 175, loss 0.58799, acc 0.76
2016-09-06T08:53:19.950183: step 176, loss 0.476038, acc 0.76
2016-09-06T08:53:20.764508: step 177, loss 0.651487, acc 0.66
2016-09-06T08:53:21.584549: step 178, loss 0.502081, acc 0.76
2016-09-06T08:53:22.382503: step 179, loss 0.39544, acc 0.86
2016-09-06T08:53:23.180953: step 180, loss 0.471159, acc 0.72
2016-09-06T08:53:24.009158: step 181, loss 0.585946, acc 0.72
2016-09-06T08:53:24.777406: step 182, loss 0.495231, acc 0.78
2016-09-06T08:53:25.577758: step 183, loss 0.432705, acc 0.76
2016-09-06T08:53:26.393219: step 184, loss 0.513077, acc 0.74
2016-09-06T08:53:27.199616: step 185, loss 0.452296, acc 0.8
2016-09-06T08:53:28.003519: step 186, loss 0.434921, acc 0.7
2016-09-06T08:53:28.830443: step 187, loss 0.476738, acc 0.78
2016-09-06T08:53:29.621352: step 188, loss 0.594477, acc 0.66
2016-09-06T08:53:30.435367: step 189, loss 0.609532, acc 0.72
2016-09-06T08:53:31.271165: step 190, loss 0.458042, acc 0.76
2016-09-06T08:53:32.047882: step 191, loss 0.576635, acc 0.66
2016-09-06T08:53:32.820715: step 192, loss 0.505448, acc 0.772727
2016-09-06T08:53:33.642646: step 193, loss 0.403736, acc 0.84
2016-09-06T08:53:34.440022: step 194, loss 0.354996, acc 0.84
2016-09-06T08:53:35.253535: step 195, loss 0.4338, acc 0.82
2016-09-06T08:53:36.069393: step 196, loss 0.420732, acc 0.78
2016-09-06T08:53:36.871493: step 197, loss 0.434428, acc 0.84
2016-09-06T08:53:37.707417: step 198, loss 0.369218, acc 0.94
2016-09-06T08:53:38.527248: step 199, loss 0.414541, acc 0.82
2016-09-06T08:53:39.318199: step 200, loss 0.578779, acc 0.74

Evaluation:
2016-09-06T08:53:43.025267: step 200, loss 0.4734, acc 0.772983

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-200

2016-09-06T08:53:44.842735: step 201, loss 0.411454, acc 0.76
2016-09-06T08:53:45.677451: step 202, loss 0.469787, acc 0.86
2016-09-06T08:53:46.493593: step 203, loss 0.320735, acc 0.86
2016-09-06T08:53:47.313225: step 204, loss 0.249519, acc 0.92
2016-09-06T08:53:48.121334: step 205, loss 0.337411, acc 0.84
2016-09-06T08:53:48.946183: step 206, loss 0.407291, acc 0.84
2016-09-06T08:53:49.789381: step 207, loss 0.55604, acc 0.66
2016-09-06T08:53:50.614801: step 208, loss 0.510818, acc 0.72
2016-09-06T08:53:51.408136: step 209, loss 0.287771, acc 0.88
2016-09-06T08:53:52.222776: step 210, loss 0.29815, acc 0.92
2016-09-06T08:53:53.039293: step 211, loss 0.387518, acc 0.76
2016-09-06T08:53:53.850042: step 212, loss 0.432102, acc 0.8
2016-09-06T08:53:54.674290: step 213, loss 0.473816, acc 0.82
2016-09-06T08:53:55.501289: step 214, loss 0.46209, acc 0.8
2016-09-06T08:53:56.295016: step 215, loss 0.273152, acc 0.88
2016-09-06T08:53:57.111627: step 216, loss 0.45224, acc 0.8
2016-09-06T08:53:57.948062: step 217, loss 0.424866, acc 0.8
2016-09-06T08:53:58.770689: step 218, loss 0.434805, acc 0.78
2016-09-06T08:53:59.567735: step 219, loss 0.37344, acc 0.8
2016-09-06T08:54:00.419596: step 220, loss 0.368171, acc 0.84
2016-09-06T08:54:01.217434: step 221, loss 0.364622, acc 0.84
2016-09-06T08:54:02.041374: step 222, loss 0.364906, acc 0.8
2016-09-06T08:54:02.843908: step 223, loss 0.320508, acc 0.88
2016-09-06T08:54:03.620605: step 224, loss 0.571777, acc 0.72
2016-09-06T08:54:04.415570: step 225, loss 0.311555, acc 0.84
2016-09-06T08:54:05.273719: step 226, loss 0.408394, acc 0.82
2016-09-06T08:54:06.104267: step 227, loss 0.338133, acc 0.84
2016-09-06T08:54:06.917719: step 228, loss 0.333244, acc 0.84
2016-09-06T08:54:07.747952: step 229, loss 0.501027, acc 0.82
2016-09-06T08:54:08.568599: step 230, loss 0.282744, acc 0.86
2016-09-06T08:54:09.362165: step 231, loss 0.400129, acc 0.8
2016-09-06T08:54:10.222242: step 232, loss 0.254364, acc 0.96
2016-09-06T08:54:11.058563: step 233, loss 0.431109, acc 0.82
2016-09-06T08:54:11.856358: step 234, loss 0.308748, acc 0.88
2016-09-06T08:54:12.685953: step 235, loss 0.354148, acc 0.86
2016-09-06T08:54:13.489186: step 236, loss 0.3426, acc 0.9
2016-09-06T08:54:14.300355: step 237, loss 0.407084, acc 0.78
2016-09-06T08:54:15.159205: step 238, loss 0.332645, acc 0.82
2016-09-06T08:54:15.957293: step 239, loss 0.351777, acc 0.9
2016-09-06T08:54:16.774384: step 240, loss 0.428253, acc 0.82
2016-09-06T08:54:17.582027: step 241, loss 0.301741, acc 0.86
2016-09-06T08:54:18.374052: step 242, loss 0.189225, acc 0.94
2016-09-06T08:54:19.152419: step 243, loss 0.362837, acc 0.86
2016-09-06T08:54:19.975197: step 244, loss 0.578534, acc 0.78
2016-09-06T08:54:20.757462: step 245, loss 0.261981, acc 0.9
2016-09-06T08:54:21.569486: step 246, loss 0.315615, acc 0.9
2016-09-06T08:54:22.394751: step 247, loss 0.527168, acc 0.72
2016-09-06T08:54:23.191868: step 248, loss 0.405198, acc 0.8
2016-09-06T08:54:23.963998: step 249, loss 0.25631, acc 0.86
2016-09-06T08:54:24.793808: step 250, loss 0.28826, acc 0.88
2016-09-06T08:54:25.610791: step 251, loss 0.339055, acc 0.88
2016-09-06T08:54:26.439439: step 252, loss 0.413109, acc 0.82
2016-09-06T08:54:27.258886: step 253, loss 0.417194, acc 0.84
2016-09-06T08:54:28.077083: step 254, loss 0.332573, acc 0.86
2016-09-06T08:54:28.894887: step 255, loss 0.331926, acc 0.84
2016-09-06T08:54:29.705752: step 256, loss 0.422624, acc 0.76
2016-09-06T08:54:30.553423: step 257, loss 0.386827, acc 0.84
2016-09-06T08:54:31.345287: step 258, loss 0.611344, acc 0.74
2016-09-06T08:54:32.162096: step 259, loss 0.41749, acc 0.84
2016-09-06T08:54:32.991774: step 260, loss 0.481004, acc 0.76
2016-09-06T08:54:33.793796: step 261, loss 0.35889, acc 0.82
2016-09-06T08:54:34.585605: step 262, loss 0.266731, acc 0.92
2016-09-06T08:54:35.401733: step 263, loss 0.390169, acc 0.84
2016-09-06T08:54:36.217842: step 264, loss 0.524074, acc 0.74
2016-09-06T08:54:37.040387: step 265, loss 0.460658, acc 0.84
2016-09-06T08:54:37.869872: step 266, loss 0.362444, acc 0.86
2016-09-06T08:54:38.664579: step 267, loss 0.5771, acc 0.7
2016-09-06T08:54:39.461916: step 268, loss 0.370818, acc 0.84
2016-09-06T08:54:40.297906: step 269, loss 0.313642, acc 0.84
2016-09-06T08:54:41.083669: step 270, loss 0.499452, acc 0.74
2016-09-06T08:54:41.890891: step 271, loss 0.296375, acc 0.86
2016-09-06T08:54:42.732615: step 272, loss 0.389037, acc 0.8
2016-09-06T08:54:43.549923: step 273, loss 0.354411, acc 0.88
2016-09-06T08:54:44.409398: step 274, loss 0.341905, acc 0.86
2016-09-06T08:54:45.240748: step 275, loss 0.31342, acc 0.88
2016-09-06T08:54:46.067380: step 276, loss 0.313025, acc 0.88
2016-09-06T08:54:46.906170: step 277, loss 0.430427, acc 0.78
2016-09-06T08:54:47.774753: step 278, loss 0.402455, acc 0.84
2016-09-06T08:54:48.583995: step 279, loss 0.426743, acc 0.8
2016-09-06T08:54:49.381068: step 280, loss 0.301353, acc 0.88
2016-09-06T08:54:50.199345: step 281, loss 0.385883, acc 0.84
2016-09-06T08:54:50.996879: step 282, loss 0.41772, acc 0.82
2016-09-06T08:54:51.824968: step 283, loss 0.467872, acc 0.8
2016-09-06T08:54:52.618547: step 284, loss 0.294378, acc 0.88
2016-09-06T08:54:53.457963: step 285, loss 0.486549, acc 0.72
2016-09-06T08:54:54.280105: step 286, loss 0.289813, acc 0.92
2016-09-06T08:54:55.071851: step 287, loss 0.497206, acc 0.78
2016-09-06T08:54:55.886491: step 288, loss 0.405512, acc 0.82
2016-09-06T08:54:56.686619: step 289, loss 0.356477, acc 0.76
2016-09-06T08:54:57.506854: step 290, loss 0.31688, acc 0.88
2016-09-06T08:54:58.349386: step 291, loss 0.435356, acc 0.8
2016-09-06T08:54:59.127039: step 292, loss 0.366561, acc 0.82
2016-09-06T08:54:59.946041: step 293, loss 0.422498, acc 0.86
2016-09-06T08:55:00.761993: step 294, loss 0.468874, acc 0.82
2016-09-06T08:55:01.545574: step 295, loss 0.402108, acc 0.78
2016-09-06T08:55:02.399664: step 296, loss 0.336368, acc 0.88
2016-09-06T08:55:03.209458: step 297, loss 0.321179, acc 0.86
2016-09-06T08:55:04.002342: step 298, loss 0.338418, acc 0.84
2016-09-06T08:55:04.840433: step 299, loss 0.377851, acc 0.82
2016-09-06T08:55:05.645319: step 300, loss 0.344812, acc 0.84

Evaluation:
2016-09-06T08:55:09.371595: step 300, loss 0.455209, acc 0.786116

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-300

2016-09-06T08:55:11.235857: step 301, loss 0.462979, acc 0.82
2016-09-06T08:55:12.081261: step 302, loss 0.498084, acc 0.78
2016-09-06T08:55:12.904573: step 303, loss 0.431614, acc 0.78
2016-09-06T08:55:13.760358: step 304, loss 0.38725, acc 0.84
2016-09-06T08:55:14.596183: step 305, loss 0.464417, acc 0.82
2016-09-06T08:55:15.422853: step 306, loss 0.394185, acc 0.82
2016-09-06T08:55:16.225164: step 307, loss 0.397972, acc 0.76
2016-09-06T08:55:17.047470: step 308, loss 0.348099, acc 0.88
2016-09-06T08:55:17.884772: step 309, loss 0.276516, acc 0.88
2016-09-06T08:55:18.706902: step 310, loss 0.539469, acc 0.74
2016-09-06T08:55:19.539844: step 311, loss 0.416027, acc 0.88
2016-09-06T08:55:20.360924: step 312, loss 0.424462, acc 0.8
2016-09-06T08:55:21.157248: step 313, loss 0.29109, acc 0.86
2016-09-06T08:55:21.990754: step 314, loss 0.322749, acc 0.8
2016-09-06T08:55:22.836127: step 315, loss 0.326464, acc 0.86
2016-09-06T08:55:23.640789: step 316, loss 0.438079, acc 0.74
2016-09-06T08:55:24.447722: step 317, loss 0.289823, acc 0.9
2016-09-06T08:55:25.273083: step 318, loss 0.392749, acc 0.78
2016-09-06T08:55:26.116250: step 319, loss 0.453142, acc 0.8
2016-09-06T08:55:26.939467: step 320, loss 0.421215, acc 0.8
2016-09-06T08:55:27.773300: step 321, loss 0.326409, acc 0.82
2016-09-06T08:55:28.558508: step 322, loss 0.429207, acc 0.78
2016-09-06T08:55:29.353312: step 323, loss 0.322093, acc 0.88
2016-09-06T08:55:30.197143: step 324, loss 0.334374, acc 0.86
2016-09-06T08:55:30.998006: step 325, loss 0.466922, acc 0.8
2016-09-06T08:55:31.791410: step 326, loss 0.294417, acc 0.84
2016-09-06T08:55:32.616413: step 327, loss 0.376971, acc 0.82
2016-09-06T08:55:33.419777: step 328, loss 0.404709, acc 0.86
2016-09-06T08:55:34.231093: step 329, loss 0.361495, acc 0.82
2016-09-06T08:55:35.076729: step 330, loss 0.447679, acc 0.84
2016-09-06T08:55:35.878088: step 331, loss 0.493937, acc 0.82
2016-09-06T08:55:36.648894: step 332, loss 0.353543, acc 0.84
2016-09-06T08:55:37.450570: step 333, loss 0.424769, acc 0.72
2016-09-06T08:55:38.250144: step 334, loss 0.382797, acc 0.86
2016-09-06T08:55:39.057973: step 335, loss 0.329933, acc 0.84
2016-09-06T08:55:39.894712: step 336, loss 0.470474, acc 0.78
2016-09-06T08:55:40.701463: step 337, loss 0.286657, acc 0.86
2016-09-06T08:55:41.505065: step 338, loss 0.271831, acc 0.92
2016-09-06T08:55:42.314456: step 339, loss 0.362652, acc 0.82
2016-09-06T08:55:43.118922: step 340, loss 0.390456, acc 0.78
2016-09-06T08:55:43.913353: step 341, loss 0.315509, acc 0.86
2016-09-06T08:55:44.739606: step 342, loss 0.391869, acc 0.78
2016-09-06T08:55:45.574358: step 343, loss 0.381516, acc 0.86
2016-09-06T08:55:46.357679: step 344, loss 0.431184, acc 0.78
2016-09-06T08:55:47.158104: step 345, loss 0.541672, acc 0.78
2016-09-06T08:55:47.994196: step 346, loss 0.262704, acc 0.9
2016-09-06T08:55:48.791849: step 347, loss 0.348275, acc 0.86
2016-09-06T08:55:49.588253: step 348, loss 0.64487, acc 0.7
2016-09-06T08:55:50.390596: step 349, loss 0.245881, acc 0.88
2016-09-06T08:55:51.178415: step 350, loss 0.326792, acc 0.9
2016-09-06T08:55:51.976051: step 351, loss 0.254693, acc 0.88
2016-09-06T08:55:52.812958: step 352, loss 0.520022, acc 0.74
2016-09-06T08:55:53.616187: step 353, loss 0.368763, acc 0.82
2016-09-06T08:55:54.411343: step 354, loss 0.213901, acc 0.92
2016-09-06T08:55:55.218663: step 355, loss 0.375426, acc 0.84
2016-09-06T08:55:56.058155: step 356, loss 0.281566, acc 0.86
2016-09-06T08:55:56.864806: step 357, loss 0.453419, acc 0.78
2016-09-06T08:55:57.661354: step 358, loss 0.448978, acc 0.78
2016-09-06T08:55:58.451372: step 359, loss 0.285235, acc 0.86
2016-09-06T08:55:59.272824: step 360, loss 0.404297, acc 0.82
2016-09-06T08:56:00.086863: step 361, loss 0.307511, acc 0.94
2016-09-06T08:56:00.918227: step 362, loss 0.179243, acc 0.94
2016-09-06T08:56:01.721768: step 363, loss 0.345116, acc 0.84
2016-09-06T08:56:02.533516: step 364, loss 0.404738, acc 0.82
2016-09-06T08:56:03.351177: step 365, loss 0.421553, acc 0.78
2016-09-06T08:56:04.163691: step 366, loss 0.318324, acc 0.86
2016-09-06T08:56:04.989700: step 367, loss 0.474471, acc 0.7
2016-09-06T08:56:05.801108: step 368, loss 0.449859, acc 0.76
2016-09-06T08:56:06.576749: step 369, loss 0.274019, acc 0.86
2016-09-06T08:56:07.395844: step 370, loss 0.466076, acc 0.76
2016-09-06T08:56:08.191521: step 371, loss 0.364637, acc 0.88
2016-09-06T08:56:09.016697: step 372, loss 0.437047, acc 0.8
2016-09-06T08:56:09.827742: step 373, loss 0.363547, acc 0.82
2016-09-06T08:56:10.687496: step 374, loss 0.276466, acc 0.94
2016-09-06T08:56:11.511759: step 375, loss 0.510504, acc 0.8
2016-09-06T08:56:12.370386: step 376, loss 0.423582, acc 0.82
2016-09-06T08:56:13.160333: step 377, loss 0.367223, acc 0.84
2016-09-06T08:56:13.942616: step 378, loss 0.35005, acc 0.84
2016-09-06T08:56:14.778573: step 379, loss 0.365722, acc 0.86
2016-09-06T08:56:15.583669: step 380, loss 0.347736, acc 0.84
2016-09-06T08:56:16.387086: step 381, loss 0.365989, acc 0.84
2016-09-06T08:56:17.228651: step 382, loss 0.325769, acc 0.82
2016-09-06T08:56:18.024256: step 383, loss 0.301247, acc 0.84
2016-09-06T08:56:18.768370: step 384, loss 0.360084, acc 0.863636
2016-09-06T08:56:19.594923: step 385, loss 0.266676, acc 0.9
2016-09-06T08:56:20.433471: step 386, loss 0.192367, acc 0.9
2016-09-06T08:56:21.262156: step 387, loss 0.302109, acc 0.86
2016-09-06T08:56:22.133268: step 388, loss 0.243434, acc 0.86
2016-09-06T08:56:22.942765: step 389, loss 0.26938, acc 0.92
2016-09-06T08:56:23.736517: step 390, loss 0.309446, acc 0.9
2016-09-06T08:56:24.586259: step 391, loss 0.21104, acc 0.9
2016-09-06T08:56:25.383154: step 392, loss 0.34856, acc 0.9
2016-09-06T08:56:26.185560: step 393, loss 0.260904, acc 0.92
2016-09-06T08:56:27.000016: step 394, loss 0.214119, acc 0.92
2016-09-06T08:56:27.814456: step 395, loss 0.298187, acc 0.9
2016-09-06T08:56:28.656875: step 396, loss 0.205041, acc 0.9
2016-09-06T08:56:29.481828: step 397, loss 0.370666, acc 0.86
2016-09-06T08:56:30.321337: step 398, loss 0.18399, acc 0.96
2016-09-06T08:56:31.131853: step 399, loss 0.104706, acc 0.96
2016-09-06T08:56:31.921502: step 400, loss 0.268813, acc 0.88

Evaluation:
2016-09-06T08:56:35.659594: step 400, loss 0.444497, acc 0.80863

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-400

2016-09-06T08:56:37.498920: step 401, loss 0.133655, acc 0.94
2016-09-06T08:56:38.328643: step 402, loss 0.152019, acc 0.96
2016-09-06T08:56:39.154504: step 403, loss 0.277689, acc 0.9
2016-09-06T08:56:39.966401: step 404, loss 0.224666, acc 0.88
2016-09-06T08:56:40.761513: step 405, loss 0.310744, acc 0.88
2016-09-06T08:56:41.589964: step 406, loss 0.31356, acc 0.86
2016-09-06T08:56:42.409683: step 407, loss 0.266843, acc 0.92
2016-09-06T08:56:43.198347: step 408, loss 0.179669, acc 0.92
2016-09-06T08:56:43.998779: step 409, loss 0.247654, acc 0.92
2016-09-06T08:56:44.831156: step 410, loss 0.193661, acc 0.94
2016-09-06T08:56:45.615191: step 411, loss 0.216165, acc 0.88
2016-09-06T08:56:46.415194: step 412, loss 0.153083, acc 0.96
2016-09-06T08:56:47.236705: step 413, loss 0.253007, acc 0.86
2016-09-06T08:56:48.031025: step 414, loss 0.133875, acc 0.96
2016-09-06T08:56:48.833005: step 415, loss 0.184359, acc 0.9
2016-09-06T08:56:49.687209: step 416, loss 0.249463, acc 0.92
2016-09-06T08:56:50.457433: step 417, loss 0.194391, acc 0.92
2016-09-06T08:56:51.256255: step 418, loss 0.358894, acc 0.78
2016-09-06T08:56:52.076885: step 419, loss 0.321647, acc 0.9
2016-09-06T08:56:52.878707: step 420, loss 0.179877, acc 0.96
2016-09-06T08:56:53.705427: step 421, loss 0.184883, acc 0.94
2016-09-06T08:56:54.529457: step 422, loss 0.179944, acc 0.92
2016-09-06T08:56:55.320388: step 423, loss 0.404227, acc 0.82
2016-09-06T08:56:56.109287: step 424, loss 0.13361, acc 0.96
2016-09-06T08:56:56.920101: step 425, loss 0.22145, acc 0.92
2016-09-06T08:56:57.700708: step 426, loss 0.257838, acc 0.92
2016-09-06T08:56:58.497343: step 427, loss 0.309651, acc 0.9
2016-09-06T08:56:59.321003: step 428, loss 0.298871, acc 0.86
2016-09-06T08:57:00.101789: step 429, loss 0.290627, acc 0.86
2016-09-06T08:57:00.916238: step 430, loss 0.170204, acc 0.92
2016-09-06T08:57:01.728448: step 431, loss 0.189151, acc 0.92
2016-09-06T08:57:02.501316: step 432, loss 0.248837, acc 0.88
2016-09-06T08:57:03.342153: step 433, loss 0.247835, acc 0.92
2016-09-06T08:57:04.167567: step 434, loss 0.272798, acc 0.9
2016-09-06T08:57:04.965066: step 435, loss 0.295033, acc 0.88
2016-09-06T08:57:05.777755: step 436, loss 0.245162, acc 0.9
2016-09-06T08:57:06.616748: step 437, loss 0.425312, acc 0.78
2016-09-06T08:57:07.391695: step 438, loss 0.176401, acc 0.92
2016-09-06T08:57:08.190582: step 439, loss 0.269081, acc 0.84
2016-09-06T08:57:09.035972: step 440, loss 0.293911, acc 0.84
2016-09-06T08:57:09.838086: step 441, loss 0.339722, acc 0.86
2016-09-06T08:57:10.613675: step 442, loss 0.301686, acc 0.84
2016-09-06T08:57:11.450940: step 443, loss 0.223274, acc 0.88
2016-09-06T08:57:12.249511: step 444, loss 0.284833, acc 0.88
2016-09-06T08:57:13.033421: step 445, loss 0.33446, acc 0.86
2016-09-06T08:57:13.837551: step 446, loss 0.241085, acc 0.88
2016-09-06T08:57:14.635256: step 447, loss 0.188974, acc 0.9
2016-09-06T08:57:15.476658: step 448, loss 0.157757, acc 0.96
2016-09-06T08:57:16.277363: step 449, loss 0.306148, acc 0.9
2016-09-06T08:57:17.065448: step 450, loss 0.237918, acc 0.9
2016-09-06T08:57:17.873783: step 451, loss 0.420519, acc 0.9
2016-09-06T08:57:18.715883: step 452, loss 0.317657, acc 0.88
2016-09-06T08:57:19.505524: step 453, loss 0.349856, acc 0.86
2016-09-06T08:57:20.302259: step 454, loss 0.178842, acc 0.94
2016-09-06T08:57:21.120685: step 455, loss 0.300205, acc 0.86
2016-09-06T08:57:21.926485: step 456, loss 0.252696, acc 0.88
2016-09-06T08:57:22.726578: step 457, loss 0.195, acc 0.94
2016-09-06T08:57:23.533452: step 458, loss 0.214197, acc 0.9
2016-09-06T08:57:24.305184: step 459, loss 0.261463, acc 0.9
2016-09-06T08:57:25.104068: step 460, loss 0.15705, acc 0.92
2016-09-06T08:57:25.925403: step 461, loss 0.273688, acc 0.84
2016-09-06T08:57:26.690831: step 462, loss 0.268961, acc 0.88
2016-09-06T08:57:27.491553: step 463, loss 0.327928, acc 0.88
2016-09-06T08:57:28.292045: step 464, loss 0.26307, acc 0.88
2016-09-06T08:57:29.089407: step 465, loss 0.190413, acc 0.92
2016-09-06T08:57:29.934152: step 466, loss 0.31022, acc 0.88
2016-09-06T08:57:30.764254: step 467, loss 0.296148, acc 0.88
2016-09-06T08:57:31.534780: step 468, loss 0.50877, acc 0.8
2016-09-06T08:57:32.334679: step 469, loss 0.190502, acc 0.96
2016-09-06T08:57:33.189858: step 470, loss 0.283149, acc 0.88
2016-09-06T08:57:33.963044: step 471, loss 0.210373, acc 0.9
2016-09-06T08:57:34.765358: step 472, loss 0.279537, acc 0.88
2016-09-06T08:57:35.600274: step 473, loss 0.26432, acc 0.9
2016-09-06T08:57:36.375901: step 474, loss 0.166383, acc 0.94
2016-09-06T08:57:37.185199: step 475, loss 0.262218, acc 0.92
2016-09-06T08:57:38.019857: step 476, loss 0.183023, acc 0.92
2016-09-06T08:57:38.779859: step 477, loss 0.326129, acc 0.86
2016-09-06T08:57:39.597042: step 478, loss 0.172373, acc 0.94
2016-09-06T08:57:40.412310: step 479, loss 0.199506, acc 0.94
2016-09-06T08:57:41.196724: step 480, loss 0.315247, acc 0.9
2016-09-06T08:57:42.008935: step 481, loss 0.30629, acc 0.86
2016-09-06T08:57:42.826277: step 482, loss 0.361367, acc 0.82
2016-09-06T08:57:43.617637: step 483, loss 0.296373, acc 0.84
2016-09-06T08:57:44.408463: step 484, loss 0.289239, acc 0.88
2016-09-06T08:57:45.226531: step 485, loss 0.283309, acc 0.92
2016-09-06T08:57:46.031191: step 486, loss 0.432421, acc 0.78
2016-09-06T08:57:46.853019: step 487, loss 0.123427, acc 0.96
2016-09-06T08:57:47.672937: step 488, loss 0.141168, acc 0.9
2016-09-06T08:57:48.450522: step 489, loss 0.284267, acc 0.9
2016-09-06T08:57:49.241095: step 490, loss 0.352165, acc 0.8
2016-09-06T08:57:50.047984: step 491, loss 0.305269, acc 0.9
2016-09-06T08:57:50.808575: step 492, loss 0.333282, acc 0.84
2016-09-06T08:57:51.638302: step 493, loss 0.21822, acc 0.92
2016-09-06T08:57:52.475291: step 494, loss 0.1909, acc 0.92
2016-09-06T08:57:53.273064: step 495, loss 0.214817, acc 0.94
2016-09-06T08:57:54.078728: step 496, loss 0.312719, acc 0.9
2016-09-06T08:57:54.894026: step 497, loss 0.262616, acc 0.88
2016-09-06T08:57:55.678637: step 498, loss 0.210834, acc 0.92
2016-09-06T08:57:56.474957: step 499, loss 0.273946, acc 0.86
2016-09-06T08:57:57.318214: step 500, loss 0.271012, acc 0.92

Evaluation:
2016-09-06T08:58:01.015112: step 500, loss 0.453145, acc 0.806754

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-500

2016-09-06T08:58:03.043577: step 501, loss 0.201158, acc 0.92
2016-09-06T08:58:03.841119: step 502, loss 0.348919, acc 0.76
2016-09-06T08:58:04.626228: step 503, loss 0.148174, acc 0.96
2016-09-06T08:58:05.411045: step 504, loss 0.209895, acc 0.92
2016-09-06T08:58:06.221645: step 505, loss 0.219328, acc 0.92
2016-09-06T08:58:07.045432: step 506, loss 0.358243, acc 0.84
2016-09-06T08:58:07.833479: step 507, loss 0.24859, acc 0.9
2016-09-06T08:58:08.639748: step 508, loss 0.504029, acc 0.86
2016-09-06T08:58:09.444377: step 509, loss 0.277525, acc 0.82
2016-09-06T08:58:10.235449: step 510, loss 0.362321, acc 0.82
2016-09-06T08:58:11.073570: step 511, loss 0.416986, acc 0.8
2016-09-06T08:58:11.925234: step 512, loss 0.220731, acc 0.92
2016-09-06T08:58:12.723913: step 513, loss 0.314534, acc 0.9
2016-09-06T08:58:13.514729: step 514, loss 0.335266, acc 0.88
2016-09-06T08:58:14.358472: step 515, loss 0.227907, acc 0.94
2016-09-06T08:58:15.167619: step 516, loss 0.32396, acc 0.86
2016-09-06T08:58:15.973993: step 517, loss 0.328363, acc 0.86
2016-09-06T08:58:16.821522: step 518, loss 0.248888, acc 0.84
2016-09-06T08:58:17.628619: step 519, loss 0.336321, acc 0.88
2016-09-06T08:58:18.457339: step 520, loss 0.285944, acc 0.9
2016-09-06T08:58:19.281908: step 521, loss 0.293663, acc 0.88
2016-09-06T08:58:20.084415: step 522, loss 0.226748, acc 0.88
2016-09-06T08:58:20.879518: step 523, loss 0.162116, acc 0.92
2016-09-06T08:58:21.714053: step 524, loss 0.227566, acc 0.94
2016-09-06T08:58:22.533162: step 525, loss 0.259942, acc 0.94
2016-09-06T08:58:23.363080: step 526, loss 0.274747, acc 0.9
2016-09-06T08:58:24.184904: step 527, loss 0.321876, acc 0.84
2016-09-06T08:58:24.967619: step 528, loss 0.328692, acc 0.88
2016-09-06T08:58:25.785559: step 529, loss 0.27655, acc 0.9
2016-09-06T08:58:26.607050: step 530, loss 0.174275, acc 0.94
2016-09-06T08:58:27.416958: step 531, loss 0.380416, acc 0.84
2016-09-06T08:58:28.227145: step 532, loss 0.396101, acc 0.84
2016-09-06T08:58:29.083663: step 533, loss 0.232818, acc 0.94
2016-09-06T08:58:29.908737: step 534, loss 0.212011, acc 0.88
2016-09-06T08:58:30.691719: step 535, loss 0.200922, acc 0.96
2016-09-06T08:58:31.513150: step 536, loss 0.333907, acc 0.86
2016-09-06T08:58:32.316077: step 537, loss 0.208106, acc 0.88
2016-09-06T08:58:33.147017: step 538, loss 0.271321, acc 0.84
2016-09-06T08:58:33.985236: step 539, loss 0.36634, acc 0.86
2016-09-06T08:58:34.808959: step 540, loss 0.173503, acc 0.92
2016-09-06T08:58:35.612908: step 541, loss 0.305636, acc 0.86
2016-09-06T08:58:36.413129: step 542, loss 0.169611, acc 0.92
2016-09-06T08:58:37.206526: step 543, loss 0.364332, acc 0.8
2016-09-06T08:58:37.979268: step 544, loss 0.294357, acc 0.86
2016-09-06T08:58:38.816120: step 545, loss 0.236974, acc 0.94
2016-09-06T08:58:39.645695: step 546, loss 0.263845, acc 0.88
2016-09-06T08:58:40.429639: step 547, loss 0.323995, acc 0.84
2016-09-06T08:58:41.221830: step 548, loss 0.316092, acc 0.86
2016-09-06T08:58:42.037136: step 549, loss 0.257649, acc 0.94
2016-09-06T08:58:42.838885: step 550, loss 0.245007, acc 0.92
2016-09-06T08:58:43.659460: step 551, loss 0.228919, acc 0.94
2016-09-06T08:58:44.488637: step 552, loss 0.283737, acc 0.92
2016-09-06T08:58:45.279962: step 553, loss 0.200352, acc 0.94
2016-09-06T08:58:46.086025: step 554, loss 0.362252, acc 0.84
2016-09-06T08:58:46.909814: step 555, loss 0.344186, acc 0.84
2016-09-06T08:58:47.711750: step 556, loss 0.392766, acc 0.8
2016-09-06T08:58:48.502482: step 557, loss 0.183659, acc 0.9
2016-09-06T08:58:49.306471: step 558, loss 0.228471, acc 0.88
2016-09-06T08:58:50.095749: step 559, loss 0.38461, acc 0.78
2016-09-06T08:58:50.903950: step 560, loss 0.18168, acc 0.92
2016-09-06T08:58:51.727680: step 561, loss 0.220447, acc 0.88
2016-09-06T08:58:52.521522: step 562, loss 0.242328, acc 0.88
2016-09-06T08:58:53.334987: step 563, loss 0.222068, acc 0.9
2016-09-06T08:58:54.143891: step 564, loss 0.264325, acc 0.92
2016-09-06T08:58:54.933528: step 565, loss 0.278495, acc 0.9
2016-09-06T08:58:55.766132: step 566, loss 0.332836, acc 0.9
2016-09-06T08:58:56.584464: step 567, loss 0.413431, acc 0.88
2016-09-06T08:58:57.365516: step 568, loss 0.490965, acc 0.8
2016-09-06T08:58:58.166735: step 569, loss 0.0994821, acc 0.96
2016-09-06T08:58:58.985717: step 570, loss 0.383337, acc 0.86
2016-09-06T08:58:59.782057: step 571, loss 0.311892, acc 0.86
2016-09-06T08:59:00.641394: step 572, loss 0.240091, acc 0.92
2016-09-06T08:59:01.457001: step 573, loss 0.21394, acc 0.9
2016-09-06T08:59:02.254805: step 574, loss 0.307477, acc 0.88
2016-09-06T08:59:03.056243: step 575, loss 0.253183, acc 0.9
2016-09-06T08:59:03.794581: step 576, loss 0.162366, acc 0.931818
2016-09-06T08:59:04.585923: step 577, loss 0.193809, acc 0.92
2016-09-06T08:59:05.414215: step 578, loss 0.169178, acc 0.96
2016-09-06T08:59:06.247708: step 579, loss 0.174261, acc 0.96
2016-09-06T08:59:07.023121: step 580, loss 0.201042, acc 0.88
2016-09-06T08:59:07.817678: step 581, loss 0.216764, acc 0.92
2016-09-06T08:59:08.609690: step 582, loss 0.257113, acc 0.96
2016-09-06T08:59:09.390799: step 583, loss 0.116165, acc 0.94
2016-09-06T08:59:10.245874: step 584, loss 0.115284, acc 0.94
2016-09-06T08:59:11.066994: step 585, loss 0.173831, acc 0.94
2016-09-06T08:59:11.864640: step 586, loss 0.396361, acc 0.88
2016-09-06T08:59:12.658766: step 587, loss 0.267779, acc 0.88
2016-09-06T08:59:13.448574: step 588, loss 0.0821014, acc 0.96
2016-09-06T08:59:14.276150: step 589, loss 0.197323, acc 0.94
2016-09-06T08:59:15.099231: step 590, loss 0.167858, acc 0.96
2016-09-06T08:59:15.937949: step 591, loss 0.186549, acc 0.92
2016-09-06T08:59:16.720868: step 592, loss 0.150014, acc 0.92
2016-09-06T08:59:17.512897: step 593, loss 0.126805, acc 0.96
2016-09-06T08:59:18.330966: step 594, loss 0.205861, acc 0.88
2016-09-06T08:59:19.124207: step 595, loss 0.232352, acc 0.92
2016-09-06T08:59:19.946918: step 596, loss 0.125591, acc 0.94
2016-09-06T08:59:20.788571: step 597, loss 0.10741, acc 1
2016-09-06T08:59:21.566083: step 598, loss 0.212555, acc 0.9
2016-09-06T08:59:22.407810: step 599, loss 0.121751, acc 0.96
2016-09-06T08:59:23.230884: step 600, loss 0.0808461, acc 0.98

Evaluation:
2016-09-06T08:59:26.967048: step 600, loss 0.486866, acc 0.812383

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-600

2016-09-06T08:59:28.975554: step 601, loss 0.161212, acc 0.94
2016-09-06T08:59:29.803154: step 602, loss 0.111734, acc 0.96
2016-09-06T08:59:30.607138: step 603, loss 0.186401, acc 0.92
2016-09-06T08:59:31.407913: step 604, loss 0.0835624, acc 0.98
2016-09-06T08:59:32.231789: step 605, loss 0.116388, acc 0.96
2016-09-06T08:59:33.065347: step 606, loss 0.3661, acc 0.86
2016-09-06T08:59:33.885528: step 607, loss 0.188213, acc 0.94
2016-09-06T08:59:34.701464: step 608, loss 0.229922, acc 0.92
2016-09-06T08:59:35.518262: step 609, loss 0.107007, acc 0.96
2016-09-06T08:59:36.322556: step 610, loss 0.145777, acc 0.92
2016-09-06T08:59:37.110467: step 611, loss 0.134727, acc 0.94
2016-09-06T08:59:37.931640: step 612, loss 0.263035, acc 0.9
2016-09-06T08:59:38.727992: step 613, loss 0.181589, acc 0.94
2016-09-06T08:59:39.540493: step 614, loss 0.121198, acc 0.96
2016-09-06T08:59:40.360916: step 615, loss 0.260101, acc 0.88
2016-09-06T08:59:41.174949: step 616, loss 0.142646, acc 0.94
2016-09-06T08:59:41.978770: step 617, loss 0.179621, acc 0.94
2016-09-06T08:59:42.811569: step 618, loss 0.194794, acc 0.92
2016-09-06T08:59:43.602109: step 619, loss 0.224797, acc 0.88
2016-09-06T08:59:44.418034: step 620, loss 0.109247, acc 0.98
2016-09-06T08:59:45.242841: step 621, loss 0.135643, acc 0.96
2016-09-06T08:59:46.036064: step 622, loss 0.187749, acc 0.92
2016-09-06T08:59:46.845839: step 623, loss 0.157786, acc 0.9
2016-09-06T08:59:47.661686: step 624, loss 0.195616, acc 0.9
2016-09-06T08:59:48.468298: step 625, loss 0.106568, acc 0.96
2016-09-06T08:59:49.288180: step 626, loss 0.237368, acc 0.9
2016-09-06T08:59:50.124767: step 627, loss 0.172813, acc 0.94
2016-09-06T08:59:50.921469: step 628, loss 0.211854, acc 0.86
2016-09-06T08:59:51.755782: step 629, loss 0.219873, acc 0.9
2016-09-06T08:59:52.558002: step 630, loss 0.213316, acc 0.94
2016-09-06T08:59:53.392092: step 631, loss 0.231664, acc 0.92
2016-09-06T08:59:54.183448: step 632, loss 0.247161, acc 0.92
2016-09-06T08:59:54.979545: step 633, loss 0.105678, acc 0.94
2016-09-06T08:59:55.805668: step 634, loss 0.17497, acc 0.92
2016-09-06T08:59:56.596341: step 635, loss 0.187407, acc 0.92
2016-09-06T08:59:57.400212: step 636, loss 0.173181, acc 0.92
2016-09-06T08:59:58.227457: step 637, loss 0.131419, acc 0.92
2016-09-06T08:59:59.008145: step 638, loss 0.117064, acc 0.94
2016-09-06T08:59:59.840855: step 639, loss 0.22729, acc 0.88
2016-09-06T09:00:00.688357: step 640, loss 0.25224, acc 0.88
2016-09-06T09:00:01.459004: step 641, loss 0.197153, acc 0.92
2016-09-06T09:00:02.286482: step 642, loss 0.180249, acc 0.94
2016-09-06T09:00:03.126803: step 643, loss 0.116941, acc 0.92
2016-09-06T09:00:03.915923: step 644, loss 0.19399, acc 0.88
2016-09-06T09:00:04.735117: step 645, loss 0.339237, acc 0.9
2016-09-06T09:00:05.552050: step 646, loss 0.204558, acc 0.88
2016-09-06T09:00:06.339467: step 647, loss 0.144531, acc 0.96
2016-09-06T09:00:07.146805: step 648, loss 0.0582028, acc 0.98
2016-09-06T09:00:07.948538: step 649, loss 0.131775, acc 0.98
2016-09-06T09:00:08.739496: step 650, loss 0.198092, acc 0.9
2016-09-06T09:00:09.549682: step 651, loss 0.174615, acc 0.9
2016-09-06T09:00:10.371590: step 652, loss 0.104352, acc 0.96
2016-09-06T09:00:11.157924: step 653, loss 0.167679, acc 0.9
2016-09-06T09:00:11.951696: step 654, loss 0.427519, acc 0.86
2016-09-06T09:00:12.751729: step 655, loss 0.191768, acc 0.92
2016-09-06T09:00:13.536252: step 656, loss 0.0620816, acc 0.98
2016-09-06T09:00:14.352743: step 657, loss 0.0851585, acc 0.98
2016-09-06T09:00:15.180310: step 658, loss 0.128663, acc 0.96
2016-09-06T09:00:15.954551: step 659, loss 0.14307, acc 0.92
2016-09-06T09:00:16.761965: step 660, loss 0.0840915, acc 1
2016-09-06T09:00:17.590784: step 661, loss 0.176983, acc 0.92
2016-09-06T09:00:18.367681: step 662, loss 0.255873, acc 0.88
2016-09-06T09:00:19.213101: step 663, loss 0.228139, acc 0.94
2016-09-06T09:00:20.027914: step 664, loss 0.146179, acc 0.96
2016-09-06T09:00:20.850938: step 665, loss 0.168894, acc 0.92
2016-09-06T09:00:21.659154: step 666, loss 0.138301, acc 0.92
2016-09-06T09:00:22.467992: step 667, loss 0.167218, acc 0.94
2016-09-06T09:00:23.264037: step 668, loss 0.197229, acc 0.94
2016-09-06T09:00:24.062035: step 669, loss 0.151957, acc 0.92
2016-09-06T09:00:24.874972: step 670, loss 0.146883, acc 0.92
2016-09-06T09:00:25.697267: step 671, loss 0.241333, acc 0.92
2016-09-06T09:00:26.505121: step 672, loss 0.128573, acc 0.96
2016-09-06T09:00:27.329206: step 673, loss 0.274543, acc 0.88
2016-09-06T09:00:28.149844: step 674, loss 0.21119, acc 0.88
2016-09-06T09:00:28.951377: step 675, loss 0.249923, acc 0.86
2016-09-06T09:00:29.774388: step 676, loss 0.149567, acc 0.94
2016-09-06T09:00:30.577567: step 677, loss 0.146238, acc 0.96
2016-09-06T09:00:31.415011: step 678, loss 0.182578, acc 0.92
2016-09-06T09:00:32.248756: step 679, loss 0.189015, acc 0.9
2016-09-06T09:00:33.065147: step 680, loss 0.162718, acc 0.92
2016-09-06T09:00:33.878106: step 681, loss 0.111754, acc 0.94
2016-09-06T09:00:34.692868: step 682, loss 0.238326, acc 0.9
2016-09-06T09:00:35.555166: step 683, loss 0.182146, acc 0.92
2016-09-06T09:00:36.352860: step 684, loss 0.10604, acc 0.98
2016-09-06T09:00:37.171886: step 685, loss 0.271088, acc 0.88
2016-09-06T09:00:37.978688: step 686, loss 0.236277, acc 0.9
2016-09-06T09:00:38.796577: step 687, loss 0.247827, acc 0.9
2016-09-06T09:00:39.634196: step 688, loss 0.176398, acc 0.92
2016-09-06T09:00:40.470877: step 689, loss 0.337131, acc 0.82
2016-09-06T09:00:41.276105: step 690, loss 0.251306, acc 0.9
2016-09-06T09:00:42.109438: step 691, loss 0.214992, acc 0.92
2016-09-06T09:00:42.912596: step 692, loss 0.144758, acc 0.92
2016-09-06T09:00:43.704983: step 693, loss 0.218636, acc 0.86
2016-09-06T09:00:44.533120: step 694, loss 0.133309, acc 0.94
2016-09-06T09:00:45.337629: step 695, loss 0.273417, acc 0.86
2016-09-06T09:00:46.165190: step 696, loss 0.146835, acc 0.92
2016-09-06T09:00:46.973463: step 697, loss 0.180442, acc 0.92
2016-09-06T09:00:47.809458: step 698, loss 0.178744, acc 0.92
2016-09-06T09:00:48.598917: step 699, loss 0.143665, acc 0.96
2016-09-06T09:00:49.405921: step 700, loss 0.278045, acc 0.8

Evaluation:
2016-09-06T09:00:53.142037: step 700, loss 0.494019, acc 0.79925

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-700

2016-09-06T09:00:55.007262: step 701, loss 0.217183, acc 0.84
2016-09-06T09:00:55.818409: step 702, loss 0.13508, acc 0.94
2016-09-06T09:00:56.658708: step 703, loss 0.143608, acc 0.96
2016-09-06T09:00:57.496763: step 704, loss 0.104178, acc 0.94
2016-09-06T09:00:58.289733: step 705, loss 0.224482, acc 0.84
2016-09-06T09:00:59.124359: step 706, loss 0.198302, acc 0.92
2016-09-06T09:00:59.932480: step 707, loss 0.180165, acc 0.9
2016-09-06T09:01:00.748964: step 708, loss 0.109036, acc 0.94
2016-09-06T09:01:01.573458: step 709, loss 0.175284, acc 0.92
2016-09-06T09:01:02.413719: step 710, loss 0.300588, acc 0.92
2016-09-06T09:01:03.195533: step 711, loss 0.0480416, acc 0.98
2016-09-06T09:01:03.993509: step 712, loss 0.12257, acc 0.94
2016-09-06T09:01:04.795106: step 713, loss 0.252296, acc 0.88
2016-09-06T09:01:05.595486: step 714, loss 0.281456, acc 0.9
2016-09-06T09:01:06.429146: step 715, loss 0.27131, acc 0.86
2016-09-06T09:01:07.265401: step 716, loss 0.31007, acc 0.94
2016-09-06T09:01:08.039141: step 717, loss 0.123824, acc 0.98
2016-09-06T09:01:08.842178: step 718, loss 0.147086, acc 0.94
2016-09-06T09:01:09.663619: step 719, loss 0.250871, acc 0.9
2016-09-06T09:01:10.455431: step 720, loss 0.200761, acc 0.92
2016-09-06T09:01:11.256732: step 721, loss 0.323243, acc 0.82
2016-09-06T09:01:12.047451: step 722, loss 0.199056, acc 0.86
2016-09-06T09:01:12.857911: step 723, loss 0.126771, acc 0.96
2016-09-06T09:01:13.696928: step 724, loss 0.136432, acc 0.96
2016-09-06T09:01:14.524826: step 725, loss 0.285197, acc 0.88
2016-09-06T09:01:15.302703: step 726, loss 0.160796, acc 0.94
2016-09-06T09:01:16.103786: step 727, loss 0.221711, acc 0.94
2016-09-06T09:01:16.933514: step 728, loss 0.117915, acc 0.98
2016-09-06T09:01:17.742942: step 729, loss 0.450141, acc 0.86
2016-09-06T09:01:18.525062: step 730, loss 0.178286, acc 0.94
2016-09-06T09:01:19.344124: step 731, loss 0.118915, acc 0.96
2016-09-06T09:01:20.145021: step 732, loss 0.167715, acc 0.94
2016-09-06T09:01:20.947096: step 733, loss 0.201156, acc 0.92
2016-09-06T09:01:21.773943: step 734, loss 0.272376, acc 0.84
2016-09-06T09:01:22.554599: step 735, loss 0.129901, acc 0.96
2016-09-06T09:01:23.368736: step 736, loss 0.272499, acc 0.88
2016-09-06T09:01:24.193615: step 737, loss 0.25174, acc 0.88
2016-09-06T09:01:24.981545: step 738, loss 0.219563, acc 0.88
2016-09-06T09:01:25.779212: step 739, loss 0.254801, acc 0.92
2016-09-06T09:01:26.601860: step 740, loss 0.126859, acc 0.96
2016-09-06T09:01:27.378196: step 741, loss 0.149935, acc 0.94
2016-09-06T09:01:28.204031: step 742, loss 0.13062, acc 0.98
2016-09-06T09:01:29.031113: step 743, loss 0.20023, acc 0.9
2016-09-06T09:01:29.818636: step 744, loss 0.182498, acc 0.92
2016-09-06T09:01:30.623395: step 745, loss 0.244771, acc 0.9
2016-09-06T09:01:31.438865: step 746, loss 0.258224, acc 0.82
2016-09-06T09:01:32.244658: step 747, loss 0.150414, acc 0.94
2016-09-06T09:01:33.050635: step 748, loss 0.142019, acc 0.94
2016-09-06T09:01:33.858136: step 749, loss 0.280264, acc 0.9
2016-09-06T09:01:34.634702: step 750, loss 0.134811, acc 0.9
2016-09-06T09:01:35.433261: step 751, loss 0.299744, acc 0.88
2016-09-06T09:01:36.270476: step 752, loss 0.150026, acc 0.94
2016-09-06T09:01:37.050629: step 753, loss 0.103447, acc 0.96
2016-09-06T09:01:37.857323: step 754, loss 0.321722, acc 0.86
2016-09-06T09:01:38.657209: step 755, loss 0.340059, acc 0.84
2016-09-06T09:01:39.449915: step 756, loss 0.130929, acc 0.94
2016-09-06T09:01:40.257390: step 757, loss 0.109908, acc 0.94
2016-09-06T09:01:41.075260: step 758, loss 0.259743, acc 0.9
2016-09-06T09:01:41.858743: step 759, loss 0.194409, acc 0.92
2016-09-06T09:01:42.670652: step 760, loss 0.126653, acc 0.94
2016-09-06T09:01:43.499275: step 761, loss 0.391399, acc 0.82
2016-09-06T09:01:44.288761: step 762, loss 0.140931, acc 0.94
2016-09-06T09:01:45.094678: step 763, loss 0.156027, acc 0.94
2016-09-06T09:01:45.921783: step 764, loss 0.140665, acc 0.98
2016-09-06T09:01:46.711583: step 765, loss 0.19868, acc 0.9
2016-09-06T09:01:47.503379: step 766, loss 0.138802, acc 0.94
2016-09-06T09:01:48.330442: step 767, loss 0.340073, acc 0.88
2016-09-06T09:01:49.060311: step 768, loss 0.0619679, acc 1
2016-09-06T09:01:49.873913: step 769, loss 0.0914991, acc 0.98
2016-09-06T09:01:50.681438: step 770, loss 0.138352, acc 0.94
2016-09-06T09:01:51.515576: step 771, loss 0.189953, acc 0.94
2016-09-06T09:01:52.372685: step 772, loss 0.104112, acc 0.96
2016-09-06T09:01:53.209182: step 773, loss 0.0749685, acc 0.98
2016-09-06T09:01:53.985769: step 774, loss 0.072841, acc 1
2016-09-06T09:01:54.815276: step 775, loss 0.158545, acc 0.94
2016-09-06T09:01:55.639144: step 776, loss 0.0828321, acc 0.92
2016-09-06T09:01:56.425440: step 777, loss 0.180897, acc 0.94
2016-09-06T09:01:57.216189: step 778, loss 0.069291, acc 0.96
2016-09-06T09:01:58.068194: step 779, loss 0.201609, acc 0.94
2016-09-06T09:01:58.844910: step 780, loss 0.101112, acc 0.96
2016-09-06T09:01:59.633764: step 781, loss 0.0924383, acc 0.98
2016-09-06T09:02:00.495297: step 782, loss 0.0491122, acc 0.98
2016-09-06T09:02:01.290959: step 783, loss 0.119058, acc 0.96
2016-09-06T09:02:02.088966: step 784, loss 0.187896, acc 0.92
2016-09-06T09:02:02.896662: step 785, loss 0.148433, acc 0.94
2016-09-06T09:02:03.693528: step 786, loss 0.0522025, acc 0.98
2016-09-06T09:02:04.521761: step 787, loss 0.174177, acc 0.92
2016-09-06T09:02:05.356399: step 788, loss 0.0839432, acc 0.96
2016-09-06T09:02:06.190086: step 789, loss 0.0599897, acc 0.96
2016-09-06T09:02:07.026264: step 790, loss 0.0431869, acc 0.98
2016-09-06T09:02:07.857565: step 791, loss 0.11921, acc 0.96
2016-09-06T09:02:08.678711: step 792, loss 0.154349, acc 0.92
2016-09-06T09:02:09.478968: step 793, loss 0.315865, acc 0.96
2016-09-06T09:02:10.294762: step 794, loss 0.105339, acc 0.96
2016-09-06T09:02:11.105573: step 795, loss 0.0375204, acc 0.98
2016-09-06T09:02:11.912860: step 796, loss 0.0719032, acc 0.96
2016-09-06T09:02:12.750533: step 797, loss 0.0713924, acc 0.96
2016-09-06T09:02:13.561094: step 798, loss 0.0997418, acc 0.96
2016-09-06T09:02:14.399868: step 799, loss 0.0325253, acc 1
2016-09-06T09:02:15.250912: step 800, loss 0.0760635, acc 0.96

Evaluation:
2016-09-06T09:02:18.965729: step 800, loss 0.654412, acc 0.804878

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-800

2016-09-06T09:02:20.855280: step 801, loss 0.1301, acc 0.94
2016-09-06T09:02:21.663291: step 802, loss 0.0741556, acc 0.96
2016-09-06T09:02:22.502497: step 803, loss 0.170888, acc 0.9
2016-09-06T09:02:23.336438: step 804, loss 0.0487159, acc 0.98
2016-09-06T09:02:24.149700: step 805, loss 0.176022, acc 0.96
2016-09-06T09:02:24.992532: step 806, loss 0.0603673, acc 0.98
2016-09-06T09:02:25.811501: step 807, loss 0.202382, acc 0.92
2016-09-06T09:02:26.614613: step 808, loss 0.137157, acc 0.9
2016-09-06T09:02:27.470886: step 809, loss 0.0607086, acc 0.98
2016-09-06T09:02:28.272919: step 810, loss 0.119242, acc 0.94
2016-09-06T09:02:29.081819: step 811, loss 0.0967835, acc 0.96
2016-09-06T09:02:29.895935: step 812, loss 0.0744151, acc 0.98
2016-09-06T09:02:30.740779: step 813, loss 0.0197755, acc 1
2016-09-06T09:02:31.533215: step 814, loss 0.112977, acc 0.94
2016-09-06T09:02:32.332741: step 815, loss 0.157399, acc 0.94
2016-09-06T09:02:33.143647: step 816, loss 0.104323, acc 0.96
2016-09-06T09:02:33.937418: step 817, loss 0.200943, acc 0.92
2016-09-06T09:02:34.751376: step 818, loss 0.168121, acc 0.9
2016-09-06T09:02:35.561289: step 819, loss 0.104763, acc 0.96
2016-09-06T09:02:36.357424: step 820, loss 0.125397, acc 0.96
2016-09-06T09:02:37.175339: step 821, loss 0.212877, acc 0.92
2016-09-06T09:02:37.999788: step 822, loss 0.0565885, acc 1
2016-09-06T09:02:38.783653: step 823, loss 0.159443, acc 0.92
2016-09-06T09:02:39.584850: step 824, loss 0.21468, acc 0.9
2016-09-06T09:02:40.408023: step 825, loss 0.150729, acc 0.94
2016-09-06T09:02:41.206271: step 826, loss 0.162782, acc 0.92
2016-09-06T09:02:42.003721: step 827, loss 0.129815, acc 0.92
2016-09-06T09:02:42.825693: step 828, loss 0.0812611, acc 0.98
2016-09-06T09:02:43.625519: step 829, loss 0.0581788, acc 0.96
2016-09-06T09:02:44.424611: step 830, loss 0.225401, acc 0.92
2016-09-06T09:02:45.248611: step 831, loss 0.157601, acc 0.94
2016-09-06T09:02:46.033355: step 832, loss 0.0925241, acc 0.98
2016-09-06T09:02:46.838538: step 833, loss 0.0746708, acc 0.98
2016-09-06T09:02:47.666111: step 834, loss 0.177707, acc 0.92
2016-09-06T09:02:48.462092: step 835, loss 0.0726953, acc 0.98
2016-09-06T09:02:49.255273: step 836, loss 0.230684, acc 0.88
2016-09-06T09:02:50.061427: step 837, loss 0.101725, acc 0.98
2016-09-06T09:02:50.839652: step 838, loss 0.0441604, acc 1
2016-09-06T09:02:51.694282: step 839, loss 0.130152, acc 0.98
2016-09-06T09:02:52.483607: step 840, loss 0.222749, acc 0.88
2016-09-06T09:02:53.274111: step 841, loss 0.113952, acc 0.94
2016-09-06T09:02:54.081947: step 842, loss 0.096906, acc 0.92
2016-09-06T09:02:54.891209: step 843, loss 0.191222, acc 0.94
2016-09-06T09:02:55.669961: step 844, loss 0.118585, acc 0.94
2016-09-06T09:02:56.491987: step 845, loss 0.0782025, acc 0.98
2016-09-06T09:02:57.337436: step 846, loss 0.213067, acc 0.86
2016-09-06T09:02:58.181402: step 847, loss 0.121868, acc 0.94
2016-09-06T09:02:58.977456: step 848, loss 0.102845, acc 0.96
2016-09-06T09:02:59.794392: step 849, loss 0.0762729, acc 0.96
2016-09-06T09:03:00.588004: step 850, loss 0.0925442, acc 0.94
2016-09-06T09:03:01.394525: step 851, loss 0.0744377, acc 0.98
2016-09-06T09:03:02.182171: step 852, loss 0.140794, acc 0.94
2016-09-06T09:03:02.967021: step 853, loss 0.161141, acc 0.94
2016-09-06T09:03:03.830678: step 854, loss 0.237283, acc 0.86
2016-09-06T09:03:04.676761: step 855, loss 0.0721055, acc 0.98
2016-09-06T09:03:05.473199: step 856, loss 0.0815504, acc 0.96
2016-09-06T09:03:06.267802: step 857, loss 0.235479, acc 0.92
2016-09-06T09:03:07.100357: step 858, loss 0.153761, acc 0.94
2016-09-06T09:03:07.925454: step 859, loss 0.15554, acc 0.92
2016-09-06T09:03:08.756018: step 860, loss 0.105533, acc 0.96
2016-09-06T09:03:09.572815: step 861, loss 0.117618, acc 0.96
2016-09-06T09:03:10.353460: step 862, loss 0.111435, acc 0.96
2016-09-06T09:03:11.174219: step 863, loss 0.102601, acc 0.96
2016-09-06T09:03:12.003124: step 864, loss 0.180243, acc 0.92
2016-09-06T09:03:12.803157: step 865, loss 0.140418, acc 0.92
2016-09-06T09:03:13.637482: step 866, loss 0.144177, acc 0.94
2016-09-06T09:03:14.468957: step 867, loss 0.0568546, acc 0.96
2016-09-06T09:03:15.287439: step 868, loss 0.152435, acc 0.92
2016-09-06T09:03:16.107221: step 869, loss 0.0975926, acc 0.96
2016-09-06T09:03:16.928466: step 870, loss 0.191501, acc 0.88
2016-09-06T09:03:17.769371: step 871, loss 0.140946, acc 0.92
2016-09-06T09:03:18.587736: step 872, loss 0.11383, acc 0.96
2016-09-06T09:03:19.445252: step 873, loss 0.118299, acc 0.96
2016-09-06T09:03:20.270185: step 874, loss 0.103139, acc 0.96
2016-09-06T09:03:21.044289: step 875, loss 0.0936337, acc 0.98
2016-09-06T09:03:21.854220: step 876, loss 0.101512, acc 0.98
2016-09-06T09:03:22.699672: step 877, loss 0.233267, acc 0.9
2016-09-06T09:03:23.497471: step 878, loss 0.197208, acc 0.9
2016-09-06T09:03:24.288645: step 879, loss 0.184685, acc 0.92
2016-09-06T09:03:25.072822: step 880, loss 0.133029, acc 0.92
2016-09-06T09:03:25.888920: step 881, loss 0.0959553, acc 0.96
2016-09-06T09:03:26.699571: step 882, loss 0.160091, acc 0.92
2016-09-06T09:03:27.538050: step 883, loss 0.140158, acc 0.94
2016-09-06T09:03:28.341819: step 884, loss 0.106326, acc 0.96
2016-09-06T09:03:29.151119: step 885, loss 0.116549, acc 0.94
2016-09-06T09:03:29.966705: step 886, loss 0.11771, acc 0.98
2016-09-06T09:03:30.768807: step 887, loss 0.0983379, acc 0.92
2016-09-06T09:03:31.568156: step 888, loss 0.121958, acc 0.94
2016-09-06T09:03:32.392585: step 889, loss 0.0718746, acc 0.96
2016-09-06T09:03:33.187202: step 890, loss 0.172017, acc 0.94
2016-09-06T09:03:33.982296: step 891, loss 0.0884985, acc 0.96
2016-09-06T09:03:34.799635: step 892, loss 0.306654, acc 0.92
2016-09-06T09:03:35.608893: step 893, loss 0.143612, acc 0.94
2016-09-06T09:03:36.434222: step 894, loss 0.127085, acc 0.94
2016-09-06T09:03:37.263308: step 895, loss 0.160838, acc 0.9
2016-09-06T09:03:38.041494: step 896, loss 0.156854, acc 0.9
2016-09-06T09:03:38.812027: step 897, loss 0.144061, acc 0.96
2016-09-06T09:03:39.625104: step 898, loss 0.218451, acc 0.92
2016-09-06T09:03:40.422269: step 899, loss 0.120479, acc 0.94
2016-09-06T09:03:41.235349: step 900, loss 0.120282, acc 0.92

Evaluation:
2016-09-06T09:03:44.977002: step 900, loss 0.56565, acc 0.789869

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-900

2016-09-06T09:03:46.754993: step 901, loss 0.167707, acc 0.9
2016-09-06T09:03:47.584496: step 902, loss 0.134943, acc 0.94
2016-09-06T09:03:48.411223: step 903, loss 0.0998356, acc 0.96
2016-09-06T09:03:49.249648: step 904, loss 0.175763, acc 0.9
2016-09-06T09:03:50.058945: step 905, loss 0.161372, acc 0.9
2016-09-06T09:03:50.891981: step 906, loss 0.150872, acc 0.94
2016-09-06T09:03:51.725155: step 907, loss 0.117133, acc 0.94
2016-09-06T09:03:52.563331: step 908, loss 0.120782, acc 0.94
2016-09-06T09:03:53.397940: step 909, loss 0.111861, acc 0.96
2016-09-06T09:03:54.195455: step 910, loss 0.0935158, acc 0.98
2016-09-06T09:03:55.003269: step 911, loss 0.0933999, acc 0.94
2016-09-06T09:03:55.857037: step 912, loss 0.105743, acc 0.98
2016-09-06T09:03:56.678485: step 913, loss 0.129061, acc 0.92
2016-09-06T09:03:57.480219: step 914, loss 0.158359, acc 0.92
2016-09-06T09:03:58.287278: step 915, loss 0.154928, acc 0.94
2016-09-06T09:03:59.121183: step 916, loss 0.0527066, acc 0.96
2016-09-06T09:03:59.898580: step 917, loss 0.130949, acc 0.96
2016-09-06T09:04:00.714679: step 918, loss 0.158819, acc 0.92
2016-09-06T09:04:01.559450: step 919, loss 0.150512, acc 0.94
2016-09-06T09:04:02.378468: step 920, loss 0.0469022, acc 1
2016-09-06T09:04:03.183864: step 921, loss 0.058054, acc 0.98
2016-09-06T09:04:04.015114: step 922, loss 0.0765592, acc 0.98
2016-09-06T09:04:04.838861: step 923, loss 0.113614, acc 0.96
2016-09-06T09:04:05.672920: step 924, loss 0.177417, acc 0.94
2016-09-06T09:04:06.499930: step 925, loss 0.100193, acc 0.96
2016-09-06T09:04:07.328828: step 926, loss 0.153503, acc 0.96
2016-09-06T09:04:08.149590: step 927, loss 0.287673, acc 0.94
2016-09-06T09:04:09.011788: step 928, loss 0.0816412, acc 0.94
2016-09-06T09:04:09.813994: step 929, loss 0.103352, acc 0.94
2016-09-06T09:04:10.631590: step 930, loss 0.207193, acc 0.92
2016-09-06T09:04:11.478533: step 931, loss 0.266669, acc 0.94
2016-09-06T09:04:12.294906: step 932, loss 0.219208, acc 0.9
2016-09-06T09:04:13.131113: step 933, loss 0.0960231, acc 0.96
2016-09-06T09:04:13.964882: step 934, loss 0.07647, acc 0.96
2016-09-06T09:04:14.801455: step 935, loss 0.20064, acc 0.96
2016-09-06T09:04:15.619952: step 936, loss 0.150078, acc 0.96
2016-09-06T09:04:16.435333: step 937, loss 0.160311, acc 0.94
2016-09-06T09:04:17.270413: step 938, loss 0.0761183, acc 0.98
2016-09-06T09:04:18.070939: step 939, loss 0.169137, acc 0.9
2016-09-06T09:04:18.870997: step 940, loss 0.105346, acc 0.96
2016-09-06T09:04:19.672608: step 941, loss 0.298084, acc 0.84
2016-09-06T09:04:20.470769: step 942, loss 0.0818655, acc 0.98
2016-09-06T09:04:21.300927: step 943, loss 0.174862, acc 0.92
2016-09-06T09:04:22.160744: step 944, loss 0.19576, acc 0.94
2016-09-06T09:04:22.979650: step 945, loss 0.0892588, acc 0.98
2016-09-06T09:04:23.843477: step 946, loss 0.0946835, acc 0.98
2016-09-06T09:04:24.675052: step 947, loss 0.359196, acc 0.86
2016-09-06T09:04:25.514637: step 948, loss 0.106478, acc 0.94
2016-09-06T09:04:26.313624: step 949, loss 0.126288, acc 0.96
2016-09-06T09:04:27.146485: step 950, loss 0.140648, acc 0.96
2016-09-06T09:04:28.006231: step 951, loss 0.0605162, acc 0.98
2016-09-06T09:04:28.822297: step 952, loss 0.187328, acc 0.92
2016-09-06T09:04:29.662619: step 953, loss 0.179737, acc 0.92
2016-09-06T09:04:30.490333: step 954, loss 0.192254, acc 0.9
2016-09-06T09:04:31.298550: step 955, loss 0.121968, acc 0.96
2016-09-06T09:04:32.131027: step 956, loss 0.0769392, acc 0.98
2016-09-06T09:04:32.963143: step 957, loss 0.11582, acc 0.96
2016-09-06T09:04:33.725577: step 958, loss 0.107672, acc 0.96
2016-09-06T09:04:34.515564: step 959, loss 0.113203, acc 0.96
2016-09-06T09:04:35.256296: step 960, loss 0.165198, acc 0.909091
2016-09-06T09:04:36.084272: step 961, loss 0.207963, acc 0.9
2016-09-06T09:04:36.933208: step 962, loss 0.165505, acc 0.9
2016-09-06T09:04:37.764492: step 963, loss 0.10946, acc 0.98
2016-09-06T09:04:38.575959: step 964, loss 0.134566, acc 0.94
2016-09-06T09:04:39.423664: step 965, loss 0.171652, acc 0.94
2016-09-06T09:04:40.218545: step 966, loss 0.0696579, acc 0.96
2016-09-06T09:04:40.989694: step 967, loss 0.109874, acc 0.96
2016-09-06T09:04:41.809619: step 968, loss 0.126648, acc 0.94
2016-09-06T09:04:42.654363: step 969, loss 0.0299944, acc 1
2016-09-06T09:04:43.457072: step 970, loss 0.0755253, acc 0.98
2016-09-06T09:04:44.283309: step 971, loss 0.125302, acc 0.94
2016-09-06T09:04:45.148488: step 972, loss 0.0908759, acc 0.96
2016-09-06T09:04:45.960079: step 973, loss 0.136141, acc 0.9
2016-09-06T09:04:46.779928: step 974, loss 0.0465577, acc 1
2016-09-06T09:04:47.587473: step 975, loss 0.220986, acc 0.9
2016-09-06T09:04:48.396871: step 976, loss 0.113084, acc 0.96
2016-09-06T09:04:49.205054: step 977, loss 0.0858911, acc 0.96
2016-09-06T09:04:50.050419: step 978, loss 0.0328312, acc 1
2016-09-06T09:04:50.860477: step 979, loss 0.0732631, acc 0.98
2016-09-06T09:04:51.680817: step 980, loss 0.115582, acc 0.92
2016-09-06T09:04:52.491695: step 981, loss 0.0870972, acc 0.98
2016-09-06T09:04:53.279932: step 982, loss 0.064111, acc 0.96
2016-09-06T09:04:54.088626: step 983, loss 0.0403782, acc 1
2016-09-06T09:04:54.943919: step 984, loss 0.254699, acc 0.94
2016-09-06T09:04:55.764770: step 985, loss 0.0718766, acc 0.98
2016-09-06T09:04:56.565156: step 986, loss 0.135917, acc 0.92
2016-09-06T09:04:57.394064: step 987, loss 0.0193596, acc 1
2016-09-06T09:04:58.213471: step 988, loss 0.0419608, acc 1
2016-09-06T09:04:59.020851: step 989, loss 0.05516, acc 0.98
2016-09-06T09:04:59.864047: step 990, loss 0.150797, acc 0.94
2016-09-06T09:05:00.687064: step 991, loss 0.050981, acc 0.98
2016-09-06T09:05:01.505852: step 992, loss 0.0894861, acc 0.94
2016-09-06T09:05:02.323164: step 993, loss 0.0837325, acc 0.96
2016-09-06T09:05:03.147884: step 994, loss 0.14762, acc 0.96
2016-09-06T09:05:03.951752: step 995, loss 0.124159, acc 0.94
2016-09-06T09:05:04.775185: step 996, loss 0.0721774, acc 0.98
2016-09-06T09:05:05.590939: step 997, loss 0.0899894, acc 0.94
2016-09-06T09:05:06.356842: step 998, loss 0.0972727, acc 0.96
2016-09-06T09:05:07.162637: step 999, loss 0.109643, acc 0.94
2016-09-06T09:05:08.005800: step 1000, loss 0.0550244, acc 1

Evaluation:
2016-09-06T09:05:11.726689: step 1000, loss 0.717452, acc 0.797373

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-1000

2016-09-06T09:05:13.596395: step 1001, loss 0.0876876, acc 0.96
2016-09-06T09:05:14.415546: step 1002, loss 0.165814, acc 0.94
2016-09-06T09:05:15.231560: step 1003, loss 0.0735524, acc 0.96
2016-09-06T09:05:16.068717: step 1004, loss 0.065717, acc 0.96
2016-09-06T09:05:16.880315: step 1005, loss 0.0597031, acc 0.98
2016-09-06T09:05:17.699236: step 1006, loss 0.133893, acc 0.9
2016-09-06T09:05:18.484526: step 1007, loss 0.0831561, acc 0.98
2016-09-06T09:05:19.324127: step 1008, loss 0.0782968, acc 0.96
2016-09-06T09:05:20.145646: step 1009, loss 0.167138, acc 0.92
2016-09-06T09:05:20.914654: step 1010, loss 0.065038, acc 0.98
2016-09-06T09:05:21.737192: step 1011, loss 0.0306631, acc 1
2016-09-06T09:05:22.557958: step 1012, loss 0.165843, acc 0.9
2016-09-06T09:05:23.356614: step 1013, loss 0.035707, acc 0.98
2016-09-06T09:05:24.133670: step 1014, loss 0.104386, acc 0.98
2016-09-06T09:05:24.967847: step 1015, loss 0.0893385, acc 0.94
2016-09-06T09:05:25.765503: step 1016, loss 0.050471, acc 0.98
2016-09-06T09:05:26.551124: step 1017, loss 0.0990826, acc 0.96
2016-09-06T09:05:27.380742: step 1018, loss 0.139716, acc 0.94
2016-09-06T09:05:28.163342: step 1019, loss 0.0499659, acc 0.98
2016-09-06T09:05:28.959242: step 1020, loss 0.0578451, acc 1
2016-09-06T09:05:29.787916: step 1021, loss 0.10807, acc 0.92
2016-09-06T09:05:30.551095: step 1022, loss 0.0705736, acc 0.98
2016-09-06T09:05:31.357114: step 1023, loss 0.0259165, acc 1
2016-09-06T09:05:32.214720: step 1024, loss 0.17017, acc 0.94
2016-09-06T09:05:32.986344: step 1025, loss 0.0938897, acc 0.96
2016-09-06T09:05:33.838078: step 1026, loss 0.140849, acc 0.92
2016-09-06T09:05:34.673105: step 1027, loss 0.021287, acc 1
2016-09-06T09:05:35.454782: step 1028, loss 0.20765, acc 0.9
2016-09-06T09:05:36.255175: step 1029, loss 0.0629783, acc 0.96
2016-09-06T09:05:37.060556: step 1030, loss 0.146313, acc 0.94
2016-09-06T09:05:37.875355: step 1031, loss 0.0312029, acc 1
2016-09-06T09:05:38.674564: step 1032, loss 0.0289328, acc 1
2016-09-06T09:05:39.507214: step 1033, loss 0.0468274, acc 1
2016-09-06T09:05:40.304782: step 1034, loss 0.133029, acc 0.94
2016-09-06T09:05:41.121354: step 1035, loss 0.0495809, acc 0.98
2016-09-06T09:05:41.969413: step 1036, loss 0.137288, acc 0.96
2016-09-06T09:05:42.786009: step 1037, loss 0.031856, acc 1
2016-09-06T09:05:43.621333: step 1038, loss 0.0789584, acc 0.96
2016-09-06T09:05:44.446494: step 1039, loss 0.00870729, acc 1
2016-09-06T09:05:45.244786: step 1040, loss 0.161183, acc 0.92
2016-09-06T09:05:46.064508: step 1041, loss 0.0967343, acc 0.96
2016-09-06T09:05:46.913436: step 1042, loss 0.0542936, acc 0.98
2016-09-06T09:05:47.716749: step 1043, loss 0.109739, acc 0.94
2016-09-06T09:05:48.509491: step 1044, loss 0.0976476, acc 0.94
2016-09-06T09:05:49.356767: step 1045, loss 0.0587299, acc 0.98
2016-09-06T09:05:50.172395: step 1046, loss 0.201742, acc 0.92
2016-09-06T09:05:50.995282: step 1047, loss 0.0292672, acc 1
2016-09-06T09:05:51.827177: step 1048, loss 0.113661, acc 0.96
2016-09-06T09:05:52.664357: step 1049, loss 0.115631, acc 0.98
2016-09-06T09:05:53.497244: step 1050, loss 0.111684, acc 0.98
2016-09-06T09:05:54.286421: step 1051, loss 0.073164, acc 0.98
2016-09-06T09:05:55.102755: step 1052, loss 0.0706202, acc 0.98
2016-09-06T09:05:55.890375: step 1053, loss 0.13725, acc 0.98
2016-09-06T09:05:56.725482: step 1054, loss 0.155586, acc 0.92
2016-09-06T09:05:57.546352: step 1055, loss 0.115103, acc 0.96
2016-09-06T09:05:58.353708: step 1056, loss 0.0360903, acc 0.98
2016-09-06T09:05:59.214962: step 1057, loss 0.161464, acc 0.92
2016-09-06T09:06:00.011377: step 1058, loss 0.157574, acc 0.96
2016-09-06T09:06:00.782357: step 1059, loss 0.0846683, acc 0.94
2016-09-06T09:06:01.585255: step 1060, loss 0.170832, acc 0.9
2016-09-06T09:06:02.394057: step 1061, loss 0.0781618, acc 0.96
2016-09-06T09:06:03.174232: step 1062, loss 0.124323, acc 0.94
2016-09-06T09:06:04.001391: step 1063, loss 0.0561992, acc 1
2016-09-06T09:06:04.839863: step 1064, loss 0.203563, acc 0.94
2016-09-06T09:06:05.606344: step 1065, loss 0.0938383, acc 0.96
2016-09-06T09:06:06.461589: step 1066, loss 0.080534, acc 0.98
2016-09-06T09:06:07.266120: step 1067, loss 0.219714, acc 0.9
2016-09-06T09:06:08.035956: step 1068, loss 0.115902, acc 0.92
2016-09-06T09:06:08.865837: step 1069, loss 0.0599043, acc 0.98
2016-09-06T09:06:09.686711: step 1070, loss 0.139583, acc 0.92
2016-09-06T09:06:10.500191: step 1071, loss 0.148543, acc 0.96
2016-09-06T09:06:11.302641: step 1072, loss 0.125911, acc 0.94
2016-09-06T09:06:12.159678: step 1073, loss 0.185657, acc 0.9
2016-09-06T09:06:12.978137: step 1074, loss 0.0806432, acc 0.98
2016-09-06T09:06:13.799767: step 1075, loss 0.119179, acc 0.94
2016-09-06T09:06:14.622015: step 1076, loss 0.285011, acc 0.92
2016-09-06T09:06:15.440207: step 1077, loss 0.101812, acc 0.94
2016-09-06T09:06:16.251968: step 1078, loss 0.180051, acc 0.94
2016-09-06T09:06:17.076462: step 1079, loss 0.0701441, acc 0.96
2016-09-06T09:06:17.887291: step 1080, loss 0.0732761, acc 0.98
2016-09-06T09:06:18.708404: step 1081, loss 0.0543015, acc 0.98
2016-09-06T09:06:19.563368: step 1082, loss 0.0604439, acc 0.98
2016-09-06T09:06:20.402605: step 1083, loss 0.0953756, acc 0.96
2016-09-06T09:06:21.229570: step 1084, loss 0.112272, acc 0.98
2016-09-06T09:06:22.050304: step 1085, loss 0.0518005, acc 0.98
2016-09-06T09:06:22.840673: step 1086, loss 0.110344, acc 0.94
2016-09-06T09:06:23.656175: step 1087, loss 0.0641692, acc 1
2016-09-06T09:06:24.482972: step 1088, loss 0.0926175, acc 0.98
2016-09-06T09:06:25.285615: step 1089, loss 0.287581, acc 0.86
2016-09-06T09:06:26.091232: step 1090, loss 0.146075, acc 0.9
2016-09-06T09:06:26.922647: step 1091, loss 0.0989353, acc 0.98
2016-09-06T09:06:27.753468: step 1092, loss 0.133773, acc 0.92
2016-09-06T09:06:28.553928: step 1093, loss 0.0629478, acc 0.96
2016-09-06T09:06:29.343420: step 1094, loss 0.326544, acc 0.9
2016-09-06T09:06:30.151636: step 1095, loss 0.0997743, acc 0.94
2016-09-06T09:06:30.925080: step 1096, loss 0.147264, acc 0.9
2016-09-06T09:06:31.735887: step 1097, loss 0.10006, acc 0.96
2016-09-06T09:06:32.538226: step 1098, loss 0.108995, acc 0.96
2016-09-06T09:06:33.317392: step 1099, loss 0.0699803, acc 0.96
2016-09-06T09:06:34.133406: step 1100, loss 0.0712144, acc 0.98

Evaluation:
2016-09-06T09:06:37.861055: step 1100, loss 0.71784, acc 0.786116

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-1100

2016-09-06T09:06:39.800291: step 1101, loss 0.127924, acc 0.94
2016-09-06T09:06:40.610814: step 1102, loss 0.0290491, acc 1
2016-09-06T09:06:41.421983: step 1103, loss 0.0518289, acc 1
2016-09-06T09:06:42.229248: step 1104, loss 0.107603, acc 0.96
2016-09-06T09:06:43.027320: step 1105, loss 0.0676756, acc 0.98
2016-09-06T09:06:43.860768: step 1106, loss 0.0418532, acc 1
2016-09-06T09:06:44.659706: step 1107, loss 0.189898, acc 0.9
2016-09-06T09:06:45.459725: step 1108, loss 0.0778703, acc 0.94
2016-09-06T09:06:46.295856: step 1109, loss 0.0611774, acc 0.96
2016-09-06T09:06:47.096987: step 1110, loss 0.105351, acc 0.94
2016-09-06T09:06:47.908282: step 1111, loss 0.0504022, acc 1
2016-09-06T09:06:48.738261: step 1112, loss 0.076564, acc 0.94
2016-09-06T09:06:49.547278: step 1113, loss 0.0955755, acc 0.96
2016-09-06T09:06:50.341552: step 1114, loss 0.0631178, acc 0.96
2016-09-06T09:06:51.191139: step 1115, loss 0.150584, acc 0.94
2016-09-06T09:06:52.003679: step 1116, loss 0.189372, acc 0.96
2016-09-06T09:06:52.833250: step 1117, loss 0.0672619, acc 0.98
2016-09-06T09:06:53.642303: step 1118, loss 0.033856, acc 1
2016-09-06T09:06:54.448908: step 1119, loss 0.178483, acc 0.94
2016-09-06T09:06:55.226130: step 1120, loss 0.169106, acc 0.94
2016-09-06T09:06:56.040209: step 1121, loss 0.0834381, acc 0.98
2016-09-06T09:06:56.854243: step 1122, loss 0.133117, acc 0.96
2016-09-06T09:06:57.643251: step 1123, loss 0.199808, acc 0.92
2016-09-06T09:06:58.465885: step 1124, loss 0.123842, acc 0.96
2016-09-06T09:06:59.295267: step 1125, loss 0.0601396, acc 0.98
2016-09-06T09:07:00.088717: step 1126, loss 0.0459627, acc 0.98
2016-09-06T09:07:00.919490: step 1127, loss 0.104367, acc 0.96
2016-09-06T09:07:01.752449: step 1128, loss 0.195991, acc 0.94
2016-09-06T09:07:02.523978: step 1129, loss 0.183249, acc 0.94
2016-09-06T09:07:03.325553: step 1130, loss 0.118363, acc 0.92
2016-09-06T09:07:04.161251: step 1131, loss 0.134887, acc 0.92
2016-09-06T09:07:04.934785: step 1132, loss 0.16869, acc 0.92
2016-09-06T09:07:05.731017: step 1133, loss 0.123161, acc 0.9
2016-09-06T09:07:06.551967: step 1134, loss 0.0492571, acc 1
2016-09-06T09:07:07.341866: step 1135, loss 0.206864, acc 0.92
2016-09-06T09:07:08.184980: step 1136, loss 0.0944864, acc 0.96
2016-09-06T09:07:08.994146: step 1137, loss 0.105651, acc 0.92
2016-09-06T09:07:09.787197: step 1138, loss 0.166944, acc 0.9
2016-09-06T09:07:10.585411: step 1139, loss 0.143916, acc 0.96
2016-09-06T09:07:11.389428: step 1140, loss 0.134817, acc 0.96
2016-09-06T09:07:12.157032: step 1141, loss 0.0817374, acc 0.98
2016-09-06T09:07:12.990879: step 1142, loss 0.131315, acc 0.94
2016-09-06T09:07:13.831675: step 1143, loss 0.195848, acc 0.94
2016-09-06T09:07:14.600113: step 1144, loss 0.0984434, acc 0.96
2016-09-06T09:07:15.390233: step 1145, loss 0.105879, acc 0.96
2016-09-06T09:07:16.209534: step 1146, loss 0.144643, acc 0.94
2016-09-06T09:07:16.999249: step 1147, loss 0.136532, acc 0.96
2016-09-06T09:07:17.799189: step 1148, loss 0.0397788, acc 0.98
2016-09-06T09:07:18.615091: step 1149, loss 0.121815, acc 0.96
2016-09-06T09:07:19.404933: step 1150, loss 0.0614751, acc 0.98
2016-09-06T09:07:20.199610: step 1151, loss 0.149466, acc 0.94
2016-09-06T09:07:20.961453: step 1152, loss 0.0931687, acc 0.954545
2016-09-06T09:07:21.766926: step 1153, loss 0.08575, acc 0.96
2016-09-06T09:07:22.615976: step 1154, loss 0.133745, acc 0.88
2016-09-06T09:07:23.441565: step 1155, loss 0.0438464, acc 0.98
2016-09-06T09:07:24.261406: step 1156, loss 0.079961, acc 0.96
2016-09-06T09:07:25.103835: step 1157, loss 0.13781, acc 0.9
2016-09-06T09:07:25.928266: step 1158, loss 0.0262404, acc 1
2016-09-06T09:07:26.734709: step 1159, loss 0.0689991, acc 0.98
2016-09-06T09:07:27.542275: step 1160, loss 0.0351776, acc 1
2016-09-06T09:07:28.347845: step 1161, loss 0.105, acc 0.94
2016-09-06T09:07:29.151387: step 1162, loss 0.0809958, acc 0.94
2016-09-06T09:07:29.954471: step 1163, loss 0.0264695, acc 0.98
2016-09-06T09:07:30.790112: step 1164, loss 0.0843029, acc 0.98
2016-09-06T09:07:31.588745: step 1165, loss 0.0737293, acc 0.96
2016-09-06T09:07:32.397739: step 1166, loss 0.0537405, acc 0.98
2016-09-06T09:07:33.218736: step 1167, loss 0.0453463, acc 0.98
2016-09-06T09:07:34.028156: step 1168, loss 0.0586329, acc 0.98
2016-09-06T09:07:34.830341: step 1169, loss 0.0554432, acc 0.98
2016-09-06T09:07:35.681389: step 1170, loss 0.00956618, acc 1
2016-09-06T09:07:36.491887: step 1171, loss 0.153682, acc 0.94
2016-09-06T09:07:37.307245: step 1172, loss 0.094425, acc 0.94
2016-09-06T09:07:38.165936: step 1173, loss 0.12531, acc 0.96
2016-09-06T09:07:38.975809: step 1174, loss 0.0859923, acc 0.98
2016-09-06T09:07:39.790111: step 1175, loss 0.040693, acc 0.98
2016-09-06T09:07:40.622046: step 1176, loss 0.199511, acc 0.94
2016-09-06T09:07:41.429659: step 1177, loss 0.0194765, acc 1
2016-09-06T09:07:42.244768: step 1178, loss 0.042196, acc 0.98
2016-09-06T09:07:43.068811: step 1179, loss 0.0257311, acc 1
2016-09-06T09:07:43.882447: step 1180, loss 0.0511413, acc 0.98
2016-09-06T09:07:44.708001: step 1181, loss 0.0620638, acc 0.96
2016-09-06T09:07:45.530031: step 1182, loss 0.0844862, acc 0.96
2016-09-06T09:07:46.346710: step 1183, loss 0.112319, acc 0.96
2016-09-06T09:07:47.163663: step 1184, loss 0.024536, acc 1
2016-09-06T09:07:47.979860: step 1185, loss 0.030626, acc 1
2016-09-06T09:07:48.791043: step 1186, loss 0.0584478, acc 0.96
2016-09-06T09:07:49.628133: step 1187, loss 0.0716403, acc 0.96
2016-09-06T09:07:50.447603: step 1188, loss 0.0779694, acc 0.98
2016-09-06T09:07:51.283134: step 1189, loss 0.180798, acc 0.9
2016-09-06T09:07:52.067572: step 1190, loss 0.0777463, acc 0.96
2016-09-06T09:07:52.867657: step 1191, loss 0.128719, acc 0.94
2016-09-06T09:07:53.672116: step 1192, loss 0.062905, acc 0.96
2016-09-06T09:07:54.453441: step 1193, loss 0.0995874, acc 0.98
2016-09-06T09:07:55.269825: step 1194, loss 0.076845, acc 0.98
2016-09-06T09:07:56.080850: step 1195, loss 0.0821894, acc 0.96
2016-09-06T09:07:56.869441: step 1196, loss 0.0537216, acc 1
2016-09-06T09:07:57.679159: step 1197, loss 0.0317207, acc 1
2016-09-06T09:07:58.515659: step 1198, loss 0.0363892, acc 1
2016-09-06T09:07:59.345720: step 1199, loss 0.109417, acc 0.94
2016-09-06T09:08:00.145457: step 1200, loss 0.162028, acc 0.94

Evaluation:
2016-09-06T09:08:03.863224: step 1200, loss 0.932433, acc 0.785178

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-1200

2016-09-06T09:08:05.680660: step 1201, loss 0.0649022, acc 0.98
2016-09-06T09:08:06.499324: step 1202, loss 0.0489963, acc 0.98
2016-09-06T09:08:07.345960: step 1203, loss 0.174712, acc 0.96
2016-09-06T09:08:08.165957: step 1204, loss 0.0383412, acc 1
2016-09-06T09:08:08.977326: step 1205, loss 0.0635701, acc 0.96
2016-09-06T09:08:09.818622: step 1206, loss 0.0487567, acc 0.98
2016-09-06T09:08:10.660018: step 1207, loss 0.0909161, acc 0.94
2016-09-06T09:08:11.464977: step 1208, loss 0.131985, acc 0.94
2016-09-06T09:08:12.284964: step 1209, loss 0.0426527, acc 0.98
2016-09-06T09:08:13.078050: step 1210, loss 0.0640082, acc 0.96
2016-09-06T09:08:13.888696: step 1211, loss 0.127532, acc 0.96
2016-09-06T09:08:14.722763: step 1212, loss 0.192611, acc 0.92
2016-09-06T09:08:15.563605: step 1213, loss 0.0831897, acc 0.94
2016-09-06T09:08:16.364004: step 1214, loss 0.036372, acc 1
2016-09-06T09:08:17.164657: step 1215, loss 0.0292561, acc 1
2016-09-06T09:08:18.030413: step 1216, loss 0.0678373, acc 1
2016-09-06T09:08:18.829085: step 1217, loss 0.0257057, acc 1
2016-09-06T09:08:19.627916: step 1218, loss 0.0949116, acc 0.96
2016-09-06T09:08:20.448879: step 1219, loss 0.177374, acc 0.94
2016-09-06T09:08:21.240186: step 1220, loss 0.0689179, acc 0.98
2016-09-06T09:08:22.039259: step 1221, loss 0.0253612, acc 1
2016-09-06T09:08:22.851353: step 1222, loss 0.191688, acc 0.92
2016-09-06T09:08:23.642103: step 1223, loss 0.138201, acc 0.92
2016-09-06T09:08:24.467700: step 1224, loss 0.0416736, acc 1
2016-09-06T09:08:25.295055: step 1225, loss 0.0972266, acc 0.96
2016-09-06T09:08:26.109349: step 1226, loss 0.107875, acc 0.94
2016-09-06T09:08:26.904889: step 1227, loss 0.117928, acc 0.96
2016-09-06T09:08:27.737161: step 1228, loss 0.0646676, acc 0.98
2016-09-06T09:08:28.569452: step 1229, loss 0.0937501, acc 0.94
2016-09-06T09:08:29.401522: step 1230, loss 0.196872, acc 0.92
2016-09-06T09:08:30.265087: step 1231, loss 0.0706951, acc 0.96
2016-09-06T09:08:31.093394: step 1232, loss 0.168601, acc 0.94
2016-09-06T09:08:31.922187: step 1233, loss 0.0580161, acc 0.98
2016-09-06T09:08:32.769503: step 1234, loss 0.1066, acc 0.94
2016-09-06T09:08:33.586375: step 1235, loss 0.120542, acc 0.96
2016-09-06T09:08:34.424109: step 1236, loss 0.0612622, acc 0.98
2016-09-06T09:08:35.256157: step 1237, loss 0.100502, acc 0.92
2016-09-06T09:08:36.066811: step 1238, loss 0.0603581, acc 1
2016-09-06T09:08:36.849710: step 1239, loss 0.0813535, acc 0.98
2016-09-06T09:08:37.675570: step 1240, loss 0.114957, acc 0.94
2016-09-06T09:08:38.566129: step 1241, loss 0.09362, acc 0.98
2016-09-06T09:08:39.345453: step 1242, loss 0.0519529, acc 0.96
2016-09-06T09:08:40.133145: step 1243, loss 0.0556477, acc 1
2016-09-06T09:08:40.955574: step 1244, loss 0.073089, acc 0.98
2016-09-06T09:08:41.745087: step 1245, loss 0.105903, acc 0.98
2016-09-06T09:08:42.546056: step 1246, loss 0.0588097, acc 0.98
2016-09-06T09:08:43.379784: step 1247, loss 0.064292, acc 0.98
2016-09-06T09:08:44.168546: step 1248, loss 0.116433, acc 0.94
2016-09-06T09:08:44.981411: step 1249, loss 0.0580312, acc 0.98
2016-09-06T09:08:45.806260: step 1250, loss 0.059778, acc 1
2016-09-06T09:08:46.610090: step 1251, loss 0.076434, acc 0.94
2016-09-06T09:08:47.414216: step 1252, loss 0.0257328, acc 1
2016-09-06T09:08:48.220917: step 1253, loss 0.102631, acc 0.96
2016-09-06T09:08:48.992234: step 1254, loss 0.113788, acc 0.98
2016-09-06T09:08:49.796304: step 1255, loss 0.0144358, acc 1
2016-09-06T09:08:50.590798: step 1256, loss 0.0518899, acc 0.98
2016-09-06T09:08:51.389003: step 1257, loss 0.255886, acc 0.94
2016-09-06T09:08:52.186061: step 1258, loss 0.0845358, acc 0.98
2016-09-06T09:08:53.009806: step 1259, loss 0.0469529, acc 0.98
2016-09-06T09:08:53.803032: step 1260, loss 0.178269, acc 0.9
2016-09-06T09:08:54.596730: step 1261, loss 0.0267145, acc 1
2016-09-06T09:08:55.401405: step 1262, loss 0.0554622, acc 0.98
2016-09-06T09:08:56.170984: step 1263, loss 0.0812448, acc 0.94
2016-09-06T09:08:56.993156: step 1264, loss 0.0688664, acc 0.98
2016-09-06T09:08:57.792306: step 1265, loss 0.0925721, acc 0.96
2016-09-06T09:08:58.625460: step 1266, loss 0.0556726, acc 0.98
2016-09-06T09:08:59.438721: step 1267, loss 0.0898679, acc 0.96
2016-09-06T09:09:00.260746: step 1268, loss 0.128833, acc 0.94
2016-09-06T09:09:01.044790: step 1269, loss 0.164626, acc 0.92
2016-09-06T09:09:01.869664: step 1270, loss 0.0906695, acc 0.94
2016-09-06T09:09:02.714251: step 1271, loss 0.0996745, acc 0.96
2016-09-06T09:09:03.501458: step 1272, loss 0.0999987, acc 0.96
2016-09-06T09:09:04.286631: step 1273, loss 0.079234, acc 0.98
2016-09-06T09:09:05.116657: step 1274, loss 0.0773184, acc 0.98
2016-09-06T09:09:05.901048: step 1275, loss 0.160765, acc 0.94
2016-09-06T09:09:06.705784: step 1276, loss 0.121142, acc 0.98
2016-09-06T09:09:07.545866: step 1277, loss 0.0637304, acc 0.96
2016-09-06T09:09:08.337384: step 1278, loss 0.11244, acc 0.94
2016-09-06T09:09:09.114374: step 1279, loss 0.108081, acc 0.98
2016-09-06T09:09:09.936998: step 1280, loss 0.0866489, acc 0.96
2016-09-06T09:09:10.735153: step 1281, loss 0.155163, acc 0.94
2016-09-06T09:09:11.530077: step 1282, loss 0.194674, acc 0.94
2016-09-06T09:09:12.334882: step 1283, loss 0.0525973, acc 0.96
2016-09-06T09:09:13.134053: step 1284, loss 0.0651751, acc 0.98
2016-09-06T09:09:13.933840: step 1285, loss 0.131215, acc 0.9
2016-09-06T09:09:14.739396: step 1286, loss 0.0591887, acc 0.98
2016-09-06T09:09:15.517327: step 1287, loss 0.0730659, acc 0.96
2016-09-06T09:09:16.341357: step 1288, loss 0.101653, acc 0.98
2016-09-06T09:09:17.149154: step 1289, loss 0.121992, acc 0.92
2016-09-06T09:09:17.999162: step 1290, loss 0.0435812, acc 1
2016-09-06T09:09:18.842924: step 1291, loss 0.0848827, acc 0.96
2016-09-06T09:09:19.677334: step 1292, loss 0.113749, acc 0.96
2016-09-06T09:09:20.482322: step 1293, loss 0.0893505, acc 0.94
2016-09-06T09:09:21.283211: step 1294, loss 0.0766302, acc 0.94
2016-09-06T09:09:22.095974: step 1295, loss 0.0292905, acc 1
2016-09-06T09:09:22.878806: step 1296, loss 0.0739507, acc 0.96
2016-09-06T09:09:23.701368: step 1297, loss 0.0474421, acc 0.98
2016-09-06T09:09:24.531725: step 1298, loss 0.221568, acc 0.9
2016-09-06T09:09:25.339017: step 1299, loss 0.028386, acc 1
2016-09-06T09:09:26.172955: step 1300, loss 0.0287302, acc 1

Evaluation:
2016-09-06T09:09:29.930494: step 1300, loss 0.987429, acc 0.773921

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-1300

2016-09-06T09:09:31.825740: step 1301, loss 0.163688, acc 0.96
2016-09-06T09:09:32.609425: step 1302, loss 0.0735836, acc 0.98
2016-09-06T09:09:33.407310: step 1303, loss 0.036883, acc 1
2016-09-06T09:09:34.235875: step 1304, loss 0.109677, acc 0.96
2016-09-06T09:09:35.034677: step 1305, loss 0.0610988, acc 0.98
2016-09-06T09:09:35.825605: step 1306, loss 0.085702, acc 0.96
2016-09-06T09:09:36.651771: step 1307, loss 0.127294, acc 0.94
2016-09-06T09:09:37.459550: step 1308, loss 0.123369, acc 0.94
2016-09-06T09:09:38.290392: step 1309, loss 0.117479, acc 0.94
2016-09-06T09:09:39.137999: step 1310, loss 0.100984, acc 0.96
2016-09-06T09:09:39.945717: step 1311, loss 0.0717638, acc 0.98
2016-09-06T09:09:40.754306: step 1312, loss 0.114641, acc 0.96
2016-09-06T09:09:41.584233: step 1313, loss 0.0850782, acc 0.98
2016-09-06T09:09:42.413714: step 1314, loss 0.295835, acc 0.88
2016-09-06T09:09:43.232457: step 1315, loss 0.0329364, acc 1
2016-09-06T09:09:44.062864: step 1316, loss 0.0491711, acc 0.98
2016-09-06T09:09:44.877285: step 1317, loss 0.0644772, acc 0.98
2016-09-06T09:09:45.683778: step 1318, loss 0.0954152, acc 0.96
2016-09-06T09:09:46.538574: step 1319, loss 0.0880509, acc 0.96
2016-09-06T09:09:47.359067: step 1320, loss 0.198692, acc 0.92
2016-09-06T09:09:48.181585: step 1321, loss 0.196459, acc 0.9
2016-09-06T09:09:49.006115: step 1322, loss 0.0873805, acc 0.98
2016-09-06T09:09:49.802858: step 1323, loss 0.107922, acc 0.96
2016-09-06T09:09:50.625326: step 1324, loss 0.142036, acc 0.96
2016-09-06T09:09:51.496708: step 1325, loss 0.0917693, acc 0.96
2016-09-06T09:09:52.276933: step 1326, loss 0.20437, acc 0.94
2016-09-06T09:09:53.066620: step 1327, loss 0.150762, acc 0.92
2016-09-06T09:09:53.881245: step 1328, loss 0.0461944, acc 1
2016-09-06T09:09:54.686981: step 1329, loss 0.092111, acc 0.96
2016-09-06T09:09:55.478274: step 1330, loss 0.140779, acc 0.94
2016-09-06T09:09:56.278885: step 1331, loss 0.155434, acc 0.92
2016-09-06T09:09:57.083343: step 1332, loss 0.107933, acc 0.94
2016-09-06T09:09:57.877484: step 1333, loss 0.189482, acc 0.94
2016-09-06T09:09:58.686878: step 1334, loss 0.0363348, acc 0.98
2016-09-06T09:09:59.505322: step 1335, loss 0.0543283, acc 0.98
2016-09-06T09:10:00.287880: step 1336, loss 0.0750449, acc 0.96
2016-09-06T09:10:01.117738: step 1337, loss 0.0566681, acc 1
2016-09-06T09:10:01.949818: step 1338, loss 0.0368031, acc 1
2016-09-06T09:10:02.768003: step 1339, loss 0.273107, acc 0.86
2016-09-06T09:10:03.581851: step 1340, loss 0.0928396, acc 0.94
2016-09-06T09:10:04.364737: step 1341, loss 0.172653, acc 0.94
2016-09-06T09:10:05.154376: step 1342, loss 0.0991067, acc 0.96
2016-09-06T09:10:06.004822: step 1343, loss 0.073108, acc 0.96
2016-09-06T09:10:06.754103: step 1344, loss 0.0326774, acc 0.977273
2016-09-06T09:10:07.561718: step 1345, loss 0.121623, acc 0.96
2016-09-06T09:10:08.392421: step 1346, loss 0.107477, acc 0.94
2016-09-06T09:10:09.183319: step 1347, loss 0.039724, acc 0.98
2016-09-06T09:10:09.975032: step 1348, loss 0.0499659, acc 0.98
2016-09-06T09:10:10.797808: step 1349, loss 0.067572, acc 0.98
2016-09-06T09:10:11.609799: step 1350, loss 0.115416, acc 0.94
2016-09-06T09:10:12.403966: step 1351, loss 0.0960833, acc 0.94
2016-09-06T09:10:13.229326: step 1352, loss 0.0578796, acc 0.98
2016-09-06T09:10:14.078568: step 1353, loss 0.0694266, acc 0.98
2016-09-06T09:10:14.888564: step 1354, loss 0.0304106, acc 0.98
2016-09-06T09:10:15.684785: step 1355, loss 0.0379744, acc 0.98
2016-09-06T09:10:16.486507: step 1356, loss 0.0524781, acc 0.98
2016-09-06T09:10:17.268139: step 1357, loss 0.13261, acc 0.94
2016-09-06T09:10:18.061680: step 1358, loss 0.0417462, acc 0.96
2016-09-06T09:10:18.870691: step 1359, loss 0.083867, acc 0.96
2016-09-06T09:10:19.651957: step 1360, loss 0.0465491, acc 0.96
2016-09-06T09:10:20.464459: step 1361, loss 0.202, acc 0.94
2016-09-06T09:10:21.304498: step 1362, loss 0.0846448, acc 0.96
2016-09-06T09:10:22.085683: step 1363, loss 0.0161516, acc 1
2016-09-06T09:10:22.875122: step 1364, loss 0.068727, acc 0.96
2016-09-06T09:10:23.701155: step 1365, loss 0.048724, acc 0.98
2016-09-06T09:10:24.499326: step 1366, loss 0.102481, acc 0.94
2016-09-06T09:10:25.288274: step 1367, loss 0.0509376, acc 0.96
2016-09-06T09:10:26.107829: step 1368, loss 0.127479, acc 0.94
2016-09-06T09:10:26.915877: step 1369, loss 0.0259848, acc 1
2016-09-06T09:10:27.751206: step 1370, loss 0.130152, acc 0.94
2016-09-06T09:10:28.564696: step 1371, loss 0.112976, acc 0.94
2016-09-06T09:10:29.347424: step 1372, loss 0.0557094, acc 0.98
2016-09-06T09:10:30.141347: step 1373, loss 0.0581727, acc 0.96
2016-09-06T09:10:30.983344: step 1374, loss 0.157014, acc 0.88
2016-09-06T09:10:31.755220: step 1375, loss 0.0976528, acc 0.94
2016-09-06T09:10:32.550800: step 1376, loss 0.0889274, acc 0.98
2016-09-06T09:10:33.395065: step 1377, loss 0.050723, acc 0.98
2016-09-06T09:10:34.188574: step 1378, loss 0.0488402, acc 1
2016-09-06T09:10:34.990032: step 1379, loss 0.0630829, acc 0.98
2016-09-06T09:10:35.774480: step 1380, loss 0.0403641, acc 0.98
2016-09-06T09:10:36.571132: step 1381, loss 0.102656, acc 0.94
2016-09-06T09:10:37.392366: step 1382, loss 0.0768262, acc 0.98
2016-09-06T09:10:38.228652: step 1383, loss 0.0554991, acc 1
2016-09-06T09:10:39.027436: step 1384, loss 0.0304662, acc 1
2016-09-06T09:10:39.842776: step 1385, loss 0.0696092, acc 0.96
2016-09-06T09:10:40.690433: step 1386, loss 0.014255, acc 1
2016-09-06T09:10:41.542842: step 1387, loss 0.0883048, acc 0.96
2016-09-06T09:10:42.349322: step 1388, loss 0.22025, acc 0.94
2016-09-06T09:10:43.175583: step 1389, loss 0.0447261, acc 0.98
2016-09-06T09:10:44.033810: step 1390, loss 0.150558, acc 0.92
2016-09-06T09:10:44.854679: step 1391, loss 0.0167241, acc 1
2016-09-06T09:10:45.693836: step 1392, loss 0.075089, acc 0.98
2016-09-06T09:10:46.547395: step 1393, loss 0.123805, acc 0.94
2016-09-06T09:10:47.360482: step 1394, loss 0.144015, acc 0.98
2016-09-06T09:10:48.199232: step 1395, loss 0.0568827, acc 0.96
2016-09-06T09:10:48.971174: step 1396, loss 0.0683141, acc 0.94
2016-09-06T09:10:49.772118: step 1397, loss 0.0629886, acc 0.98
2016-09-06T09:10:50.626766: step 1398, loss 0.0529281, acc 1
2016-09-06T09:10:51.494342: step 1399, loss 0.0960114, acc 0.96
2016-09-06T09:10:52.282261: step 1400, loss 0.0530995, acc 0.98

Evaluation:
2016-09-06T09:10:56.012826: step 1400, loss 0.82604, acc 0.778612

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-1400

2016-09-06T09:10:57.891274: step 1401, loss 0.0536636, acc 0.96
2016-09-06T09:10:58.710411: step 1402, loss 0.110475, acc 0.98
2016-09-06T09:10:59.517197: step 1403, loss 0.0278959, acc 1
2016-09-06T09:11:00.393641: step 1404, loss 0.12854, acc 0.94
2016-09-06T09:11:01.236255: step 1405, loss 0.0576932, acc 0.98
2016-09-06T09:11:02.051126: step 1406, loss 0.0358, acc 1
2016-09-06T09:11:02.871330: step 1407, loss 0.0655732, acc 0.96
2016-09-06T09:11:03.690020: step 1408, loss 0.126039, acc 0.96
2016-09-06T09:11:04.513999: step 1409, loss 0.0897047, acc 0.96
2016-09-06T09:11:05.332409: step 1410, loss 0.0535507, acc 0.96
2016-09-06T09:11:06.160734: step 1411, loss 0.0369067, acc 0.98
2016-09-06T09:11:06.965878: step 1412, loss 0.189771, acc 0.94
2016-09-06T09:11:07.767389: step 1413, loss 0.0896369, acc 0.98
2016-09-06T09:11:08.594020: step 1414, loss 0.0307705, acc 1
2016-09-06T09:11:09.422868: step 1415, loss 0.0448738, acc 1
2016-09-06T09:11:10.255393: step 1416, loss 0.0421952, acc 1
2016-09-06T09:11:11.084954: step 1417, loss 0.10089, acc 0.96
2016-09-06T09:11:11.902441: step 1418, loss 0.0460846, acc 1
2016-09-06T09:11:12.720398: step 1419, loss 0.172842, acc 0.94
2016-09-06T09:11:13.556771: step 1420, loss 0.0717444, acc 0.98
2016-09-06T09:11:14.372150: step 1421, loss 0.0625208, acc 0.96
2016-09-06T09:11:15.185988: step 1422, loss 0.0689551, acc 0.98
2016-09-06T09:11:16.044204: step 1423, loss 0.0602323, acc 0.98
2016-09-06T09:11:16.871964: step 1424, loss 0.0544893, acc 0.98
2016-09-06T09:11:17.677833: step 1425, loss 0.0492803, acc 0.98
2016-09-06T09:11:18.472536: step 1426, loss 0.0334925, acc 0.98
2016-09-06T09:11:19.297287: step 1427, loss 0.0829573, acc 0.94
2016-09-06T09:11:20.084298: step 1428, loss 0.0249144, acc 0.98
2016-09-06T09:11:20.889783: step 1429, loss 0.0552505, acc 0.98
2016-09-06T09:11:21.741441: step 1430, loss 0.0584771, acc 0.98
2016-09-06T09:11:22.529236: step 1431, loss 0.0281639, acc 0.98
2016-09-06T09:11:23.309776: step 1432, loss 0.0748998, acc 0.98
2016-09-06T09:11:24.155437: step 1433, loss 0.0451361, acc 0.98
2016-09-06T09:11:24.942967: step 1434, loss 0.0870821, acc 0.94
2016-09-06T09:11:25.730945: step 1435, loss 0.0385614, acc 0.98
2016-09-06T09:11:26.564271: step 1436, loss 0.0196295, acc 1
2016-09-06T09:11:27.365310: step 1437, loss 0.144045, acc 0.96
2016-09-06T09:11:28.179079: step 1438, loss 0.242183, acc 0.96
2016-09-06T09:11:28.983000: step 1439, loss 0.349186, acc 0.88
2016-09-06T09:11:29.750188: step 1440, loss 0.0496213, acc 0.98
2016-09-06T09:11:30.554162: step 1441, loss 0.0745621, acc 0.96
2016-09-06T09:11:31.405578: step 1442, loss 0.188077, acc 0.94
2016-09-06T09:11:32.210993: step 1443, loss 0.109206, acc 0.96
2016-09-06T09:11:33.014601: step 1444, loss 0.063035, acc 0.98
2016-09-06T09:11:33.829420: step 1445, loss 0.0362785, acc 1
2016-09-06T09:11:34.633539: step 1446, loss 0.0898102, acc 0.96
2016-09-06T09:11:35.429056: step 1447, loss 0.0414907, acc 0.98
2016-09-06T09:11:36.247059: step 1448, loss 0.108606, acc 0.96
2016-09-06T09:11:37.041936: step 1449, loss 0.0888068, acc 0.96
2016-09-06T09:11:37.854372: step 1450, loss 0.0891256, acc 0.94
2016-09-06T09:11:38.645801: step 1451, loss 0.072516, acc 0.98
2016-09-06T09:11:39.452267: step 1452, loss 0.0860463, acc 0.96
2016-09-06T09:11:40.272183: step 1453, loss 0.129735, acc 0.94
2016-09-06T09:11:41.092003: step 1454, loss 0.107857, acc 0.96
2016-09-06T09:11:41.912808: step 1455, loss 0.0447225, acc 1
2016-09-06T09:11:42.745277: step 1456, loss 0.0826224, acc 0.96
2016-09-06T09:11:43.587671: step 1457, loss 0.0867146, acc 0.94
2016-09-06T09:11:44.396328: step 1458, loss 0.042765, acc 0.98
2016-09-06T09:11:45.222641: step 1459, loss 0.0743097, acc 0.94
2016-09-06T09:11:46.088695: step 1460, loss 0.0847805, acc 0.96
2016-09-06T09:11:46.910942: step 1461, loss 0.0258076, acc 1
2016-09-06T09:11:47.725997: step 1462, loss 0.0408474, acc 0.98
2016-09-06T09:11:48.585382: step 1463, loss 0.0402408, acc 1
2016-09-06T09:11:49.381655: step 1464, loss 0.192131, acc 0.96
2016-09-06T09:11:50.195024: step 1465, loss 0.0963133, acc 0.92
2016-09-06T09:11:51.045105: step 1466, loss 0.0148171, acc 1
2016-09-06T09:11:51.884198: step 1467, loss 0.0259031, acc 1
2016-09-06T09:11:52.656928: step 1468, loss 0.031707, acc 1
2016-09-06T09:11:53.470210: step 1469, loss 0.116684, acc 0.96
2016-09-06T09:11:54.284735: step 1470, loss 0.0572632, acc 0.96
2016-09-06T09:11:55.073998: step 1471, loss 0.0500535, acc 0.98
2016-09-06T09:11:55.902346: step 1472, loss 0.0518384, acc 0.98
2016-09-06T09:11:56.733061: step 1473, loss 0.14389, acc 0.94
2016-09-06T09:11:57.541464: step 1474, loss 0.138772, acc 0.96
2016-09-06T09:11:58.357662: step 1475, loss 0.0929502, acc 0.96
2016-09-06T09:11:59.205392: step 1476, loss 0.0962773, acc 0.94
2016-09-06T09:11:59.991046: step 1477, loss 0.0720025, acc 0.94
2016-09-06T09:12:00.815018: step 1478, loss 0.119395, acc 0.96
2016-09-06T09:12:01.639483: step 1479, loss 0.0739258, acc 0.96
2016-09-06T09:12:02.418899: step 1480, loss 0.178752, acc 0.94
2016-09-06T09:12:03.249704: step 1481, loss 0.138114, acc 0.94
2016-09-06T09:12:04.079891: step 1482, loss 0.163227, acc 0.92
2016-09-06T09:12:04.880086: step 1483, loss 0.0729174, acc 0.96
2016-09-06T09:12:05.686418: step 1484, loss 0.0183672, acc 1
2016-09-06T09:12:06.504560: step 1485, loss 0.0989034, acc 0.92
2016-09-06T09:12:07.302874: step 1486, loss 0.112287, acc 0.94
2016-09-06T09:12:08.115516: step 1487, loss 0.110537, acc 0.92
2016-09-06T09:12:08.944743: step 1488, loss 0.0557253, acc 1
2016-09-06T09:12:09.769487: step 1489, loss 0.0833219, acc 0.96
2016-09-06T09:12:10.578524: step 1490, loss 0.0412823, acc 0.98
2016-09-06T09:12:11.419728: step 1491, loss 0.107917, acc 0.96
2016-09-06T09:12:12.262167: step 1492, loss 0.108397, acc 0.98
2016-09-06T09:12:13.074391: step 1493, loss 0.171264, acc 0.96
2016-09-06T09:12:13.922194: step 1494, loss 0.0443719, acc 0.98
2016-09-06T09:12:14.730918: step 1495, loss 0.0305418, acc 1
2016-09-06T09:12:15.563388: step 1496, loss 0.0423901, acc 1
2016-09-06T09:12:16.416919: step 1497, loss 0.0790015, acc 0.98
2016-09-06T09:12:17.197808: step 1498, loss 0.140797, acc 0.92
2016-09-06T09:12:18.028578: step 1499, loss 0.0930861, acc 0.98
2016-09-06T09:12:18.843991: step 1500, loss 0.150487, acc 0.92

Evaluation:
2016-09-06T09:12:22.556865: step 1500, loss 0.887561, acc 0.777674

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-1500

2016-09-06T09:12:24.537278: step 1501, loss 0.0408234, acc 0.98
2016-09-06T09:12:25.350676: step 1502, loss 0.111846, acc 0.92
2016-09-06T09:12:26.176351: step 1503, loss 0.0407407, acc 1
2016-09-06T09:12:26.999941: step 1504, loss 0.117782, acc 0.96
2016-09-06T09:12:27.824910: step 1505, loss 0.117947, acc 0.94
2016-09-06T09:12:28.652416: step 1506, loss 0.0943661, acc 0.98
2016-09-06T09:12:29.458205: step 1507, loss 0.211812, acc 0.88
2016-09-06T09:12:30.273482: step 1508, loss 0.0251829, acc 1
2016-09-06T09:12:31.093503: step 1509, loss 0.0534981, acc 0.98
2016-09-06T09:12:31.916759: step 1510, loss 0.0231466, acc 1
2016-09-06T09:12:32.710416: step 1511, loss 0.0561555, acc 0.98
2016-09-06T09:12:33.517735: step 1512, loss 0.0424576, acc 0.98
2016-09-06T09:12:34.330629: step 1513, loss 0.0958376, acc 0.98
2016-09-06T09:12:35.109985: step 1514, loss 0.0923908, acc 0.96
2016-09-06T09:12:35.913642: step 1515, loss 0.113888, acc 0.96
2016-09-06T09:12:36.728291: step 1516, loss 0.0929735, acc 0.96
2016-09-06T09:12:37.512859: step 1517, loss 0.0783747, acc 0.96
2016-09-06T09:12:38.337820: step 1518, loss 0.0687339, acc 0.96
2016-09-06T09:12:39.151182: step 1519, loss 0.117247, acc 0.94
2016-09-06T09:12:39.929318: step 1520, loss 0.0556668, acc 0.98
2016-09-06T09:12:40.744599: step 1521, loss 0.0751847, acc 0.94
2016-09-06T09:12:41.562256: step 1522, loss 0.0958913, acc 0.96
2016-09-06T09:12:42.345000: step 1523, loss 0.108861, acc 0.96
2016-09-06T09:12:43.173189: step 1524, loss 0.0611406, acc 0.98
2016-09-06T09:12:43.992685: step 1525, loss 0.0987461, acc 0.94
2016-09-06T09:12:44.801574: step 1526, loss 0.0924448, acc 0.96
2016-09-06T09:12:45.601577: step 1527, loss 0.059701, acc 0.98
2016-09-06T09:12:46.408542: step 1528, loss 0.0480862, acc 1
2016-09-06T09:12:47.219148: step 1529, loss 0.103909, acc 0.94
2016-09-06T09:12:48.012099: step 1530, loss 0.0402406, acc 0.98
2016-09-06T09:12:48.836995: step 1531, loss 0.0489589, acc 0.98
2016-09-06T09:12:49.651092: step 1532, loss 0.0842916, acc 0.94
2016-09-06T09:12:50.462342: step 1533, loss 0.160387, acc 0.94
2016-09-06T09:12:51.295743: step 1534, loss 0.0678293, acc 0.96
2016-09-06T09:12:52.088602: step 1535, loss 0.0384571, acc 0.98
2016-09-06T09:12:52.840963: step 1536, loss 0.0276667, acc 1
2016-09-06T09:12:53.653947: step 1537, loss 0.0479794, acc 0.98
2016-09-06T09:12:54.478395: step 1538, loss 0.0900089, acc 0.94
2016-09-06T09:12:55.306662: step 1539, loss 0.121476, acc 0.96
2016-09-06T09:12:56.121043: step 1540, loss 0.0472214, acc 0.98
2016-09-06T09:12:56.922303: step 1541, loss 0.0510409, acc 1
2016-09-06T09:12:57.756737: step 1542, loss 0.0368836, acc 0.98
2016-09-06T09:12:58.583085: step 1543, loss 0.124492, acc 0.92
2016-09-06T09:12:59.381479: step 1544, loss 0.0285778, acc 1
2016-09-06T09:13:00.201301: step 1545, loss 0.174006, acc 0.92
2016-09-06T09:13:01.010079: step 1546, loss 0.125155, acc 0.94
2016-09-06T09:13:01.801330: step 1547, loss 0.0423358, acc 0.98
2016-09-06T09:13:02.629282: step 1548, loss 0.0539895, acc 0.98
2016-09-06T09:13:03.476109: step 1549, loss 0.0749775, acc 0.96
2016-09-06T09:13:04.299960: step 1550, loss 0.0203862, acc 1
2016-09-06T09:13:05.131858: step 1551, loss 0.031023, acc 1
2016-09-06T09:13:05.990058: step 1552, loss 0.0206712, acc 0.98
2016-09-06T09:13:06.798619: step 1553, loss 0.116434, acc 0.94
2016-09-06T09:13:07.616220: step 1554, loss 0.022384, acc 1
2016-09-06T09:13:08.445845: step 1555, loss 0.0933309, acc 0.98
2016-09-06T09:13:09.280199: step 1556, loss 0.0160883, acc 1
2016-09-06T09:13:10.087320: step 1557, loss 0.0132001, acc 1
2016-09-06T09:13:10.906731: step 1558, loss 0.0921063, acc 0.98
2016-09-06T09:13:11.730351: step 1559, loss 0.0224143, acc 1
2016-09-06T09:13:12.559747: step 1560, loss 0.049462, acc 0.98
2016-09-06T09:13:13.379736: step 1561, loss 0.0221578, acc 1
2016-09-06T09:13:14.201067: step 1562, loss 0.129134, acc 0.92
2016-09-06T09:13:15.011440: step 1563, loss 0.0992877, acc 0.98
2016-09-06T09:13:15.811885: step 1564, loss 0.122107, acc 0.92
2016-09-06T09:13:16.627019: step 1565, loss 0.0831236, acc 0.96
2016-09-06T09:13:17.437831: step 1566, loss 0.0299874, acc 0.98
2016-09-06T09:13:18.230165: step 1567, loss 0.018051, acc 1
2016-09-06T09:13:19.067076: step 1568, loss 0.0432359, acc 0.98
2016-09-06T09:13:19.859861: step 1569, loss 0.111553, acc 0.98
2016-09-06T09:13:20.679646: step 1570, loss 0.079187, acc 0.98
2016-09-06T09:13:21.492850: step 1571, loss 0.0449757, acc 0.98
2016-09-06T09:13:22.287674: step 1572, loss 0.0453378, acc 0.98
2016-09-06T09:13:23.097259: step 1573, loss 0.0507337, acc 0.96
2016-09-06T09:13:23.938774: step 1574, loss 0.0644969, acc 0.96
2016-09-06T09:13:24.727499: step 1575, loss 0.0277793, acc 1
2016-09-06T09:13:25.544962: step 1576, loss 0.139116, acc 0.96
2016-09-06T09:13:26.340881: step 1577, loss 0.0976283, acc 0.98
2016-09-06T09:13:27.124374: step 1578, loss 0.0820024, acc 0.96
2016-09-06T09:13:27.929853: step 1579, loss 0.0553427, acc 0.98
2016-09-06T09:13:28.787583: step 1580, loss 0.0114768, acc 1
2016-09-06T09:13:29.606108: step 1581, loss 0.0721993, acc 0.98
2016-09-06T09:13:30.415473: step 1582, loss 0.124001, acc 0.94
2016-09-06T09:13:31.212390: step 1583, loss 0.10184, acc 0.96
2016-09-06T09:13:32.014711: step 1584, loss 0.045781, acc 0.98
2016-09-06T09:13:32.813002: step 1585, loss 0.0449392, acc 0.96
2016-09-06T09:13:33.600597: step 1586, loss 0.0306639, acc 0.98
2016-09-06T09:13:34.370915: step 1587, loss 0.09236, acc 0.96
2016-09-06T09:13:35.184168: step 1588, loss 0.0712518, acc 0.96
2016-09-06T09:13:36.026519: step 1589, loss 0.0351175, acc 1
2016-09-06T09:13:36.813471: step 1590, loss 0.0247142, acc 1
2016-09-06T09:13:37.593052: step 1591, loss 0.212762, acc 0.92
2016-09-06T09:13:38.407266: step 1592, loss 0.0832105, acc 0.98
2016-09-06T09:13:39.180334: step 1593, loss 0.0508248, acc 0.98
2016-09-06T09:13:39.995508: step 1594, loss 0.105975, acc 0.94
2016-09-06T09:13:40.850314: step 1595, loss 0.117388, acc 0.94
2016-09-06T09:13:41.625790: step 1596, loss 0.116325, acc 0.96
2016-09-06T09:13:42.472531: step 1597, loss 0.0957531, acc 0.96
2016-09-06T09:13:43.296044: step 1598, loss 0.00898558, acc 1
2016-09-06T09:13:44.067400: step 1599, loss 0.0233826, acc 1
2016-09-06T09:13:44.870190: step 1600, loss 0.0429325, acc 0.98

Evaluation:
2016-09-06T09:13:48.589934: step 1600, loss 0.912217, acc 0.787992

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-1600

2016-09-06T09:13:50.563325: step 1601, loss 0.0493732, acc 0.98
2016-09-06T09:13:51.353614: step 1602, loss 0.159288, acc 0.94
2016-09-06T09:13:52.167452: step 1603, loss 0.0597274, acc 0.98
2016-09-06T09:13:52.968651: step 1604, loss 0.0534557, acc 0.98
2016-09-06T09:13:53.740549: step 1605, loss 0.104587, acc 0.96
2016-09-06T09:13:54.568540: step 1606, loss 0.0565307, acc 0.96
2016-09-06T09:13:55.373669: step 1607, loss 0.100106, acc 0.98
2016-09-06T09:13:56.169901: step 1608, loss 0.0229626, acc 1
2016-09-06T09:13:56.972509: step 1609, loss 0.0576863, acc 0.98
2016-09-06T09:13:57.797941: step 1610, loss 0.232372, acc 0.92
2016-09-06T09:13:58.615865: step 1611, loss 0.051001, acc 0.98
2016-09-06T09:13:59.420085: step 1612, loss 0.0248131, acc 1
2016-09-06T09:14:00.282675: step 1613, loss 0.0473455, acc 1
2016-09-06T09:14:01.066106: step 1614, loss 0.0363403, acc 1
2016-09-06T09:14:01.875307: step 1615, loss 0.104705, acc 0.96
2016-09-06T09:14:02.700927: step 1616, loss 0.118773, acc 0.94
2016-09-06T09:14:03.536466: step 1617, loss 0.206385, acc 0.92
2016-09-06T09:14:04.324523: step 1618, loss 0.0549732, acc 0.98
2016-09-06T09:14:05.156810: step 1619, loss 0.06052, acc 0.98
2016-09-06T09:14:05.969058: step 1620, loss 0.33168, acc 0.92
2016-09-06T09:14:06.781355: step 1621, loss 0.0529813, acc 0.96
2016-09-06T09:14:07.588992: step 1622, loss 0.0939321, acc 0.96
2016-09-06T09:14:08.399002: step 1623, loss 0.0355488, acc 1
2016-09-06T09:14:09.196124: step 1624, loss 0.0704858, acc 0.98
2016-09-06T09:14:10.048799: step 1625, loss 0.0702092, acc 0.96
2016-09-06T09:14:10.858269: step 1626, loss 0.0984921, acc 0.96
2016-09-06T09:14:11.670256: step 1627, loss 0.0480382, acc 1
2016-09-06T09:14:12.500027: step 1628, loss 0.0360798, acc 0.98
2016-09-06T09:14:13.299538: step 1629, loss 0.0763372, acc 0.94
2016-09-06T09:14:14.102397: step 1630, loss 0.0153579, acc 1
2016-09-06T09:14:14.924663: step 1631, loss 0.0310525, acc 0.98
2016-09-06T09:14:15.731015: step 1632, loss 0.0328546, acc 1
2016-09-06T09:14:16.560077: step 1633, loss 0.048756, acc 0.98
2016-09-06T09:14:17.444043: step 1634, loss 0.102322, acc 0.96
2016-09-06T09:14:18.283074: step 1635, loss 0.047459, acc 1
2016-09-06T09:14:19.061540: step 1636, loss 0.0894626, acc 0.94
2016-09-06T09:14:19.892441: step 1637, loss 0.0380842, acc 1
2016-09-06T09:14:20.695639: step 1638, loss 0.0856388, acc 0.96
2016-09-06T09:14:21.469711: step 1639, loss 0.0979574, acc 0.96
2016-09-06T09:14:22.270330: step 1640, loss 0.0825681, acc 0.96
2016-09-06T09:14:23.079279: step 1641, loss 0.0660638, acc 0.94
2016-09-06T09:14:23.898709: step 1642, loss 0.0953825, acc 0.94
2016-09-06T09:14:24.724858: step 1643, loss 0.0553005, acc 0.98
2016-09-06T09:14:25.560030: step 1644, loss 0.0730011, acc 0.94
2016-09-06T09:14:26.354669: step 1645, loss 0.0897413, acc 0.92
2016-09-06T09:14:27.159109: step 1646, loss 0.227799, acc 0.92
2016-09-06T09:14:28.007610: step 1647, loss 0.0849586, acc 0.96
2016-09-06T09:14:28.791010: step 1648, loss 0.035406, acc 1
2016-09-06T09:14:29.563366: step 1649, loss 0.0836119, acc 0.96
2016-09-06T09:14:30.391144: step 1650, loss 0.0923023, acc 0.96
2016-09-06T09:14:31.180418: step 1651, loss 0.22492, acc 0.9
2016-09-06T09:14:31.983480: step 1652, loss 0.0399284, acc 0.98
2016-09-06T09:14:32.800828: step 1653, loss 0.103092, acc 0.96
2016-09-06T09:14:33.606996: step 1654, loss 0.0303909, acc 1
2016-09-06T09:14:34.453363: step 1655, loss 0.0850458, acc 0.98
2016-09-06T09:14:35.285192: step 1656, loss 0.0759881, acc 0.98
2016-09-06T09:14:36.089582: step 1657, loss 0.0396823, acc 0.98
2016-09-06T09:14:36.907279: step 1658, loss 0.221766, acc 0.94
2016-09-06T09:14:37.745438: step 1659, loss 0.157653, acc 0.92
2016-09-06T09:14:38.568206: step 1660, loss 0.0666125, acc 0.98
2016-09-06T09:14:39.379059: step 1661, loss 0.0734301, acc 0.98
2016-09-06T09:14:40.208975: step 1662, loss 0.0123725, acc 1
2016-09-06T09:14:41.070127: step 1663, loss 0.114251, acc 0.96
2016-09-06T09:14:41.903518: step 1664, loss 0.137304, acc 0.96
2016-09-06T09:14:42.720418: step 1665, loss 0.0718772, acc 0.96
2016-09-06T09:14:43.526224: step 1666, loss 0.0856374, acc 0.96
2016-09-06T09:14:44.337374: step 1667, loss 0.072185, acc 0.96
2016-09-06T09:14:45.166988: step 1668, loss 0.0650559, acc 0.98
2016-09-06T09:14:45.966868: step 1669, loss 0.0858512, acc 0.96
2016-09-06T09:14:46.784761: step 1670, loss 0.123143, acc 0.96
2016-09-06T09:14:47.638667: step 1671, loss 0.0882528, acc 0.96
2016-09-06T09:14:48.449372: step 1672, loss 0.0422793, acc 1
2016-09-06T09:14:49.251916: step 1673, loss 0.0342536, acc 1
2016-09-06T09:14:50.081502: step 1674, loss 0.13087, acc 0.92
2016-09-06T09:14:50.894681: step 1675, loss 0.111133, acc 0.94
2016-09-06T09:14:51.705577: step 1676, loss 0.092801, acc 0.94
2016-09-06T09:14:52.513950: step 1677, loss 0.128783, acc 0.92
2016-09-06T09:14:53.320072: step 1678, loss 0.111052, acc 0.98
2016-09-06T09:14:54.110174: step 1679, loss 0.0867217, acc 0.96
2016-09-06T09:14:54.937731: step 1680, loss 0.12159, acc 0.94
2016-09-06T09:14:55.762070: step 1681, loss 0.119376, acc 0.96
2016-09-06T09:14:56.551495: step 1682, loss 0.049097, acc 0.98
2016-09-06T09:14:57.336295: step 1683, loss 0.0913449, acc 0.96
2016-09-06T09:14:58.166206: step 1684, loss 0.0603345, acc 0.96
2016-09-06T09:14:58.965462: step 1685, loss 0.06881, acc 0.96
2016-09-06T09:14:59.753254: step 1686, loss 0.0997033, acc 0.98
2016-09-06T09:15:00.586157: step 1687, loss 0.0244536, acc 1
2016-09-06T09:15:01.413362: step 1688, loss 0.0838002, acc 0.96
2016-09-06T09:15:02.193409: step 1689, loss 0.0798905, acc 0.96
2016-09-06T09:15:03.021180: step 1690, loss 0.0497526, acc 0.98
2016-09-06T09:15:03.817406: step 1691, loss 0.0783932, acc 0.96
2016-09-06T09:15:04.653413: step 1692, loss 0.0818631, acc 0.98
2016-09-06T09:15:05.462209: step 1693, loss 0.0294032, acc 1
2016-09-06T09:15:06.272656: step 1694, loss 0.0277214, acc 1
2016-09-06T09:15:07.082421: step 1695, loss 0.0614624, acc 0.98
2016-09-06T09:15:07.898983: step 1696, loss 0.0612319, acc 1
2016-09-06T09:15:08.693666: step 1697, loss 0.0569238, acc 0.94
2016-09-06T09:15:09.497345: step 1698, loss 0.0899832, acc 0.96
2016-09-06T09:15:10.327100: step 1699, loss 0.0289112, acc 1
2016-09-06T09:15:11.144238: step 1700, loss 0.0435075, acc 1

Evaluation:
2016-09-06T09:15:14.858053: step 1700, loss 1.09563, acc 0.777674

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-1700

2016-09-06T09:15:16.801203: step 1701, loss 0.0622226, acc 0.96
2016-09-06T09:15:17.607383: step 1702, loss 0.0950153, acc 0.96
2016-09-06T09:15:18.415075: step 1703, loss 0.0400316, acc 0.98
2016-09-06T09:15:19.205391: step 1704, loss 0.0751634, acc 0.96
2016-09-06T09:15:20.013324: step 1705, loss 0.118855, acc 0.96
2016-09-06T09:15:20.811480: step 1706, loss 0.0764271, acc 0.94
2016-09-06T09:15:21.615202: step 1707, loss 0.07175, acc 0.96
2016-09-06T09:15:22.428805: step 1708, loss 0.0484295, acc 1
2016-09-06T09:15:23.228841: step 1709, loss 0.0710302, acc 0.96
2016-09-06T09:15:24.007303: step 1710, loss 0.160875, acc 0.92
2016-09-06T09:15:24.839452: step 1711, loss 0.118323, acc 0.94
2016-09-06T09:15:25.622030: step 1712, loss 0.0201296, acc 1
2016-09-06T09:15:26.450705: step 1713, loss 0.0587594, acc 0.96
2016-09-06T09:15:27.264247: step 1714, loss 0.0203841, acc 1
2016-09-06T09:15:28.023969: step 1715, loss 0.173042, acc 0.94
2016-09-06T09:15:28.814184: step 1716, loss 0.0703873, acc 0.96
2016-09-06T09:15:29.643732: step 1717, loss 0.0354682, acc 0.98
2016-09-06T09:15:30.443038: step 1718, loss 0.0419757, acc 1
2016-09-06T09:15:31.239390: step 1719, loss 0.0224687, acc 0.98
2016-09-06T09:15:32.061533: step 1720, loss 0.0356856, acc 0.98
2016-09-06T09:15:32.916404: step 1721, loss 0.0383979, acc 0.98
2016-09-06T09:15:33.734677: step 1722, loss 0.109674, acc 0.94
2016-09-06T09:15:34.545604: step 1723, loss 0.104796, acc 0.96
2016-09-06T09:15:35.340363: step 1724, loss 0.101891, acc 0.94
2016-09-06T09:15:36.157138: step 1725, loss 0.0667627, acc 0.96
2016-09-06T09:15:36.988166: step 1726, loss 0.0219528, acc 1
2016-09-06T09:15:37.828373: step 1727, loss 0.105448, acc 0.96
2016-09-06T09:15:38.573370: step 1728, loss 0.00831156, acc 1
2016-09-06T09:15:39.405267: step 1729, loss 0.0481759, acc 0.98
2016-09-06T09:15:40.260298: step 1730, loss 0.037229, acc 1
2016-09-06T09:15:41.119424: step 1731, loss 0.0598534, acc 0.98
2016-09-06T09:15:41.970002: step 1732, loss 0.0313169, acc 1
2016-09-06T09:15:42.793756: step 1733, loss 0.0354516, acc 1
2016-09-06T09:15:43.627722: step 1734, loss 0.032923, acc 0.98
2016-09-06T09:15:44.465956: step 1735, loss 0.0539646, acc 0.98
2016-09-06T09:15:45.279218: step 1736, loss 0.112632, acc 0.96
2016-09-06T09:15:46.087431: step 1737, loss 0.0318025, acc 0.98
2016-09-06T09:15:46.931972: step 1738, loss 0.0153693, acc 1
2016-09-06T09:15:47.743366: step 1739, loss 0.0181073, acc 1
2016-09-06T09:15:48.541608: step 1740, loss 0.0471958, acc 0.98
2016-09-06T09:15:49.359232: step 1741, loss 0.027701, acc 1
2016-09-06T09:15:50.211142: step 1742, loss 0.0307051, acc 0.98
2016-09-06T09:15:51.035866: step 1743, loss 0.132631, acc 0.98
2016-09-06T09:15:51.860723: step 1744, loss 0.0593565, acc 0.96
2016-09-06T09:15:52.687927: step 1745, loss 0.0598754, acc 0.98
2016-09-06T09:15:53.531631: step 1746, loss 0.0183491, acc 1
2016-09-06T09:15:54.360605: step 1747, loss 0.049772, acc 0.98
2016-09-06T09:15:55.217140: step 1748, loss 0.00567317, acc 1
2016-09-06T09:15:56.005447: step 1749, loss 0.0418998, acc 0.98
2016-09-06T09:15:56.846107: step 1750, loss 0.0223355, acc 1
2016-09-06T09:15:57.663244: step 1751, loss 0.0255256, acc 1
2016-09-06T09:15:58.471114: step 1752, loss 0.0278032, acc 1
2016-09-06T09:15:59.256771: step 1753, loss 0.0641013, acc 0.96
2016-09-06T09:16:00.063915: step 1754, loss 0.0575281, acc 0.96
2016-09-06T09:16:00.895037: step 1755, loss 0.027927, acc 0.98
2016-09-06T09:16:01.716819: step 1756, loss 0.0125633, acc 1
2016-09-06T09:16:02.554243: step 1757, loss 0.031088, acc 0.98
2016-09-06T09:16:03.387472: step 1758, loss 0.0962786, acc 0.96
2016-09-06T09:16:04.242822: step 1759, loss 0.004729, acc 1
2016-09-06T09:16:05.086144: step 1760, loss 0.0799376, acc 0.98
2016-09-06T09:16:05.931366: step 1761, loss 0.0706863, acc 0.98
2016-09-06T09:16:06.740443: step 1762, loss 0.0962137, acc 0.96
2016-09-06T09:16:07.564868: step 1763, loss 0.162255, acc 0.98
2016-09-06T09:16:08.382591: step 1764, loss 0.102298, acc 0.96
2016-09-06T09:16:09.193985: step 1765, loss 0.0327119, acc 0.98
2016-09-06T09:16:09.996518: step 1766, loss 0.0375556, acc 0.98
2016-09-06T09:16:10.832214: step 1767, loss 0.0643023, acc 0.96
2016-09-06T09:16:11.618208: step 1768, loss 0.0696577, acc 0.98
2016-09-06T09:16:12.449446: step 1769, loss 0.0608191, acc 0.98
2016-09-06T09:16:13.277743: step 1770, loss 0.0952628, acc 0.98
2016-09-06T09:16:14.100498: step 1771, loss 0.0162026, acc 1
2016-09-06T09:16:14.939469: step 1772, loss 0.0671716, acc 0.98
2016-09-06T09:16:15.780601: step 1773, loss 0.0160452, acc 1
2016-09-06T09:16:16.561835: step 1774, loss 0.0303121, acc 1
2016-09-06T09:16:17.381948: step 1775, loss 0.0933859, acc 0.98
2016-09-06T09:16:18.205996: step 1776, loss 0.0269761, acc 1
2016-09-06T09:16:19.027618: step 1777, loss 0.048883, acc 0.98
2016-09-06T09:16:19.876863: step 1778, loss 0.0373162, acc 0.98
2016-09-06T09:16:20.695398: step 1779, loss 0.107468, acc 0.94
2016-09-06T09:16:21.500058: step 1780, loss 0.154309, acc 0.9
2016-09-06T09:16:22.341285: step 1781, loss 0.0439529, acc 1
2016-09-06T09:16:23.163903: step 1782, loss 0.0406073, acc 0.98
2016-09-06T09:16:23.967565: step 1783, loss 0.0302984, acc 1
2016-09-06T09:16:24.775593: step 1784, loss 0.0281504, acc 1
2016-09-06T09:16:25.593710: step 1785, loss 0.0814761, acc 0.98
2016-09-06T09:16:26.404215: step 1786, loss 0.0578748, acc 1
2016-09-06T09:16:27.231463: step 1787, loss 0.0870121, acc 0.98
2016-09-06T09:16:28.044629: step 1788, loss 0.113178, acc 0.96
2016-09-06T09:16:28.866580: step 1789, loss 0.093536, acc 0.94
2016-09-06T09:16:29.678953: step 1790, loss 0.0219883, acc 0.98
2016-09-06T09:16:30.496185: step 1791, loss 0.00774583, acc 1
2016-09-06T09:16:31.310207: step 1792, loss 0.0887248, acc 0.94
2016-09-06T09:16:32.117826: step 1793, loss 0.0371879, acc 0.98
2016-09-06T09:16:32.942243: step 1794, loss 0.102074, acc 0.94
2016-09-06T09:16:33.760366: step 1795, loss 0.0435411, acc 0.96
2016-09-06T09:16:34.549130: step 1796, loss 0.076995, acc 0.98
2016-09-06T09:16:35.356170: step 1797, loss 0.0379086, acc 0.98
2016-09-06T09:16:36.179933: step 1798, loss 0.0416123, acc 0.98
2016-09-06T09:16:36.977699: step 1799, loss 0.0337462, acc 1
2016-09-06T09:16:37.781829: step 1800, loss 0.0937236, acc 0.98

Evaluation:
2016-09-06T09:16:41.532270: step 1800, loss 1.10067, acc 0.782364

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-1800

2016-09-06T09:16:43.420022: step 1801, loss 0.0317204, acc 1
2016-09-06T09:16:44.249381: step 1802, loss 0.10218, acc 0.94
2016-09-06T09:16:45.065278: step 1803, loss 0.121804, acc 0.94
2016-09-06T09:16:45.905417: step 1804, loss 0.117571, acc 0.94
2016-09-06T09:16:46.752770: step 1805, loss 0.106404, acc 0.96
2016-09-06T09:16:47.573186: step 1806, loss 0.113713, acc 0.98
2016-09-06T09:16:48.424646: step 1807, loss 0.0443795, acc 0.98
2016-09-06T09:16:49.212428: step 1808, loss 0.0796794, acc 0.98
2016-09-06T09:16:50.005133: step 1809, loss 0.0584561, acc 0.96
2016-09-06T09:16:50.827272: step 1810, loss 0.00783886, acc 1
2016-09-06T09:16:51.649573: step 1811, loss 0.0594496, acc 0.96
2016-09-06T09:16:52.458566: step 1812, loss 0.0402797, acc 0.98
2016-09-06T09:16:53.282956: step 1813, loss 0.0944831, acc 0.98
2016-09-06T09:16:54.045409: step 1814, loss 0.0437263, acc 0.98
2016-09-06T09:16:54.857354: step 1815, loss 0.0221094, acc 0.98
2016-09-06T09:16:55.655885: step 1816, loss 0.109282, acc 0.98
2016-09-06T09:16:56.464298: step 1817, loss 0.0201731, acc 1
2016-09-06T09:16:57.251572: step 1818, loss 0.0303063, acc 1
2016-09-06T09:16:58.070093: step 1819, loss 0.0681499, acc 0.96
2016-09-06T09:16:58.855864: step 1820, loss 0.00831097, acc 1
2016-09-06T09:16:59.646737: step 1821, loss 0.146944, acc 0.94
2016-09-06T09:17:00.492944: step 1822, loss 0.0982505, acc 0.96
2016-09-06T09:17:01.266388: step 1823, loss 0.10881, acc 0.94
2016-09-06T09:17:02.055588: step 1824, loss 0.0567771, acc 0.94
2016-09-06T09:17:02.881794: step 1825, loss 0.0330171, acc 1
2016-09-06T09:17:03.686594: step 1826, loss 0.17646, acc 0.9
2016-09-06T09:17:04.482264: step 1827, loss 0.0433038, acc 1
2016-09-06T09:17:05.303442: step 1828, loss 0.130261, acc 0.92
2016-09-06T09:17:06.096016: step 1829, loss 0.0472481, acc 0.98
2016-09-06T09:17:06.893262: step 1830, loss 0.0147678, acc 1
2016-09-06T09:17:07.711025: step 1831, loss 0.0320412, acc 1
2016-09-06T09:17:08.506013: step 1832, loss 0.0177341, acc 1
2016-09-06T09:17:09.310146: step 1833, loss 0.0545849, acc 0.98
2016-09-06T09:17:10.111092: step 1834, loss 0.0788007, acc 0.98
2016-09-06T09:17:10.890349: step 1835, loss 0.125343, acc 0.96
2016-09-06T09:17:11.722731: step 1836, loss 0.133145, acc 0.94
2016-09-06T09:17:12.507898: step 1837, loss 0.0942028, acc 0.98
2016-09-06T09:17:13.292561: step 1838, loss 0.0970388, acc 0.96
2016-09-06T09:17:14.120853: step 1839, loss 0.0690069, acc 0.96
2016-09-06T09:17:14.941076: step 1840, loss 0.0681345, acc 0.96
2016-09-06T09:17:15.749433: step 1841, loss 0.14254, acc 0.96
2016-09-06T09:17:16.538232: step 1842, loss 0.0598648, acc 1
2016-09-06T09:17:17.310726: step 1843, loss 0.0905593, acc 0.98
2016-09-06T09:17:18.129499: step 1844, loss 0.0719929, acc 0.98
2016-09-06T09:17:19.017313: step 1845, loss 0.0279151, acc 1
2016-09-06T09:17:19.876540: step 1846, loss 0.111496, acc 0.96
2016-09-06T09:17:20.676301: step 1847, loss 0.0800675, acc 0.98
2016-09-06T09:17:21.468201: step 1848, loss 0.136908, acc 0.96
2016-09-06T09:17:22.316532: step 1849, loss 0.0395273, acc 0.98
2016-09-06T09:17:23.124927: step 1850, loss 0.170185, acc 0.92
2016-09-06T09:17:23.936817: step 1851, loss 0.0337902, acc 1
2016-09-06T09:17:24.759310: step 1852, loss 0.0501002, acc 0.96
2016-09-06T09:17:25.550589: step 1853, loss 0.0565926, acc 0.98
2016-09-06T09:17:26.382201: step 1854, loss 0.0251074, acc 1
2016-09-06T09:17:27.215941: step 1855, loss 0.0982051, acc 0.96
2016-09-06T09:17:28.003604: step 1856, loss 0.179229, acc 0.92
2016-09-06T09:17:28.832503: step 1857, loss 0.140371, acc 0.96
2016-09-06T09:17:29.648104: step 1858, loss 0.0645629, acc 0.96
2016-09-06T09:17:30.469365: step 1859, loss 0.0958893, acc 0.94
2016-09-06T09:17:31.313582: step 1860, loss 0.0835057, acc 0.96
2016-09-06T09:17:32.180738: step 1861, loss 0.0605517, acc 0.98
2016-09-06T09:17:32.995304: step 1862, loss 0.0984395, acc 0.94
2016-09-06T09:17:33.818896: step 1863, loss 0.00941816, acc 1
2016-09-06T09:17:34.641902: step 1864, loss 0.0569486, acc 0.96
2016-09-06T09:17:35.469761: step 1865, loss 0.0412993, acc 0.98
2016-09-06T09:17:36.285786: step 1866, loss 0.10706, acc 0.98
2016-09-06T09:17:37.113402: step 1867, loss 0.0108217, acc 1
2016-09-06T09:17:37.951184: step 1868, loss 0.121676, acc 0.92
2016-09-06T09:17:38.749601: step 1869, loss 0.192485, acc 0.84
2016-09-06T09:17:39.550518: step 1870, loss 0.106027, acc 0.96
2016-09-06T09:17:40.369721: step 1871, loss 0.0902041, acc 0.96
2016-09-06T09:17:41.164505: step 1872, loss 0.0782874, acc 0.96
2016-09-06T09:17:41.966624: step 1873, loss 0.0459571, acc 1
2016-09-06T09:17:42.797201: step 1874, loss 0.0374786, acc 0.98
2016-09-06T09:17:43.620081: step 1875, loss 0.0625522, acc 0.96
2016-09-06T09:17:44.413208: step 1876, loss 0.0997275, acc 0.94
2016-09-06T09:17:45.221903: step 1877, loss 0.0217055, acc 1
2016-09-06T09:17:46.014758: step 1878, loss 0.140357, acc 0.94
2016-09-06T09:17:46.831261: step 1879, loss 0.0569604, acc 0.96
2016-09-06T09:17:47.634434: step 1880, loss 0.0348722, acc 0.98
2016-09-06T09:17:48.425967: step 1881, loss 0.064157, acc 0.98
2016-09-06T09:17:49.231922: step 1882, loss 0.15417, acc 0.94
2016-09-06T09:17:50.052551: step 1883, loss 0.0558876, acc 0.94
2016-09-06T09:17:50.857827: step 1884, loss 0.165995, acc 0.92
2016-09-06T09:17:51.653718: step 1885, loss 0.119201, acc 0.94
2016-09-06T09:17:52.480386: step 1886, loss 0.0634437, acc 0.98
2016-09-06T09:17:53.256055: step 1887, loss 0.0330589, acc 0.98
2016-09-06T09:17:54.075850: step 1888, loss 0.0373959, acc 1
2016-09-06T09:17:54.912183: step 1889, loss 0.173707, acc 0.94
2016-09-06T09:17:55.712850: step 1890, loss 0.0969213, acc 0.96
2016-09-06T09:17:56.520957: step 1891, loss 0.00875512, acc 1
2016-09-06T09:17:57.323622: step 1892, loss 0.120814, acc 0.96
2016-09-06T09:17:58.103287: step 1893, loss 0.110087, acc 0.96
2016-09-06T09:17:58.887673: step 1894, loss 0.0377518, acc 0.98
2016-09-06T09:17:59.701403: step 1895, loss 0.0610813, acc 0.94
2016-09-06T09:18:00.541645: step 1896, loss 0.060694, acc 0.98
2016-09-06T09:18:01.322540: step 1897, loss 0.0909114, acc 0.94
2016-09-06T09:18:02.134332: step 1898, loss 0.138238, acc 0.96
2016-09-06T09:18:02.941448: step 1899, loss 0.0349857, acc 0.98
2016-09-06T09:18:03.743062: step 1900, loss 0.114485, acc 0.96

Evaluation:
2016-09-06T09:18:07.556846: step 1900, loss 1.06262, acc 0.768293

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-1900

2016-09-06T09:18:09.553230: step 1901, loss 0.0211059, acc 1
2016-09-06T09:18:10.359666: step 1902, loss 0.0884534, acc 0.96
2016-09-06T09:18:11.158471: step 1903, loss 0.145535, acc 0.92
2016-09-06T09:18:11.953323: step 1904, loss 0.0442647, acc 0.98
2016-09-06T09:18:12.734962: step 1905, loss 0.227685, acc 0.9
2016-09-06T09:18:13.540149: step 1906, loss 0.0905538, acc 0.96
2016-09-06T09:18:14.370773: step 1907, loss 0.0717012, acc 0.96
2016-09-06T09:18:15.157397: step 1908, loss 0.120844, acc 0.96
2016-09-06T09:18:15.969854: step 1909, loss 0.0655918, acc 0.96
2016-09-06T09:18:16.765451: step 1910, loss 0.0380806, acc 0.98
2016-09-06T09:18:17.568399: step 1911, loss 0.0700514, acc 0.96
2016-09-06T09:18:18.362567: step 1912, loss 0.0659879, acc 0.96
2016-09-06T09:18:19.170184: step 1913, loss 0.0266128, acc 1
2016-09-06T09:18:19.973356: step 1914, loss 0.047724, acc 0.98
2016-09-06T09:18:20.815785: step 1915, loss 0.0182757, acc 1
2016-09-06T09:18:21.646501: step 1916, loss 0.0607472, acc 0.96
2016-09-06T09:18:22.443410: step 1917, loss 0.0574986, acc 1
2016-09-06T09:18:23.236323: step 1918, loss 0.184183, acc 0.9
2016-09-06T09:18:24.054838: step 1919, loss 0.0496113, acc 0.98
2016-09-06T09:18:24.784227: step 1920, loss 0.123522, acc 0.977273
2016-09-06T09:18:25.582666: step 1921, loss 0.0801039, acc 0.98
2016-09-06T09:18:26.411789: step 1922, loss 0.0727609, acc 0.94
2016-09-06T09:18:27.214843: step 1923, loss 0.0348589, acc 0.98
2016-09-06T09:18:28.002873: step 1924, loss 0.0206445, acc 1
2016-09-06T09:18:28.813109: step 1925, loss 0.0704782, acc 0.98
2016-09-06T09:18:29.606403: step 1926, loss 0.0841317, acc 0.98
2016-09-06T09:18:30.406959: step 1927, loss 0.148087, acc 0.94
2016-09-06T09:18:31.205264: step 1928, loss 0.0545508, acc 0.96
2016-09-06T09:18:32.028847: step 1929, loss 0.0274818, acc 1
2016-09-06T09:18:32.853179: step 1930, loss 0.0668913, acc 0.94
2016-09-06T09:18:33.678927: step 1931, loss 0.00457339, acc 1
2016-09-06T09:18:34.483001: step 1932, loss 0.033491, acc 0.98
2016-09-06T09:18:35.271245: step 1933, loss 0.0906852, acc 0.96
2016-09-06T09:18:36.124337: step 1934, loss 0.0577179, acc 0.98
2016-09-06T09:18:36.933040: step 1935, loss 0.1047, acc 0.98
2016-09-06T09:18:37.717406: step 1936, loss 0.0150297, acc 1
2016-09-06T09:18:38.568334: step 1937, loss 0.037675, acc 1
2016-09-06T09:18:39.355795: step 1938, loss 0.0235225, acc 1
2016-09-06T09:18:40.156437: step 1939, loss 0.0747795, acc 0.94
2016-09-06T09:18:40.964057: step 1940, loss 0.00776208, acc 1
2016-09-06T09:18:41.749467: step 1941, loss 0.0712254, acc 0.98
2016-09-06T09:18:42.553456: step 1942, loss 0.0653027, acc 0.96
2016-09-06T09:18:43.365989: step 1943, loss 0.105023, acc 0.98
2016-09-06T09:18:44.138245: step 1944, loss 0.11426, acc 0.92
2016-09-06T09:18:44.948644: step 1945, loss 0.0966137, acc 0.94
2016-09-06T09:18:45.752963: step 1946, loss 0.0295851, acc 1
2016-09-06T09:18:46.541784: step 1947, loss 0.0955109, acc 0.94
2016-09-06T09:18:47.333601: step 1948, loss 0.021717, acc 1
2016-09-06T09:18:48.148198: step 1949, loss 0.0390437, acc 1
2016-09-06T09:18:48.950925: step 1950, loss 0.0517316, acc 0.98
2016-09-06T09:18:49.752079: step 1951, loss 0.0158972, acc 1
2016-09-06T09:18:50.545441: step 1952, loss 0.0446409, acc 0.98
2016-09-06T09:18:51.360652: step 1953, loss 0.0114165, acc 1
2016-09-06T09:18:52.207253: step 1954, loss 0.0146238, acc 1
2016-09-06T09:18:53.042812: step 1955, loss 0.00508793, acc 1
2016-09-06T09:18:53.829435: step 1956, loss 0.044717, acc 0.96
2016-09-06T09:18:54.633854: step 1957, loss 0.0405926, acc 0.96
2016-09-06T09:18:55.453416: step 1958, loss 0.0407024, acc 0.98
2016-09-06T09:18:56.276093: step 1959, loss 0.051453, acc 0.98
2016-09-06T09:18:57.080682: step 1960, loss 0.0146856, acc 1
2016-09-06T09:18:57.880812: step 1961, loss 0.0407191, acc 0.98
2016-09-06T09:18:58.665510: step 1962, loss 0.00983745, acc 1
2016-09-06T09:18:59.471705: step 1963, loss 0.0879458, acc 0.94
2016-09-06T09:19:00.296324: step 1964, loss 0.0458736, acc 0.98
2016-09-06T09:19:01.075384: step 1965, loss 0.018057, acc 1
2016-09-06T09:19:01.854420: step 1966, loss 0.0562153, acc 0.98
2016-09-06T09:19:02.674990: step 1967, loss 0.0235648, acc 1
2016-09-06T09:19:03.449398: step 1968, loss 0.0843699, acc 0.98
2016-09-06T09:19:04.276998: step 1969, loss 0.0765167, acc 0.96
2016-09-06T09:19:05.105977: step 1970, loss 0.0258632, acc 0.98
2016-09-06T09:19:05.895124: step 1971, loss 0.0224231, acc 1
2016-09-06T09:19:06.707242: step 1972, loss 0.0209739, acc 1
2016-09-06T09:19:07.543832: step 1973, loss 0.0772783, acc 0.94
2016-09-06T09:19:08.320844: step 1974, loss 0.0759817, acc 0.98
2016-09-06T09:19:09.177663: step 1975, loss 0.040048, acc 0.98
2016-09-06T09:19:10.055106: step 1976, loss 0.22472, acc 0.88
2016-09-06T09:19:10.882030: step 1977, loss 0.0482392, acc 0.98
2016-09-06T09:19:11.685299: step 1978, loss 0.0834139, acc 0.96
2016-09-06T09:19:12.504855: step 1979, loss 0.06854, acc 0.96
2016-09-06T09:19:13.316390: step 1980, loss 0.122366, acc 0.94
2016-09-06T09:19:14.126981: step 1981, loss 0.0267761, acc 1
2016-09-06T09:19:14.963812: step 1982, loss 0.0768379, acc 0.98
2016-09-06T09:19:15.758148: step 1983, loss 0.11501, acc 0.96
2016-09-06T09:19:16.571311: step 1984, loss 0.144939, acc 0.94
2016-09-06T09:19:17.401253: step 1985, loss 0.0683145, acc 0.96
2016-09-06T09:19:18.208251: step 1986, loss 0.0786783, acc 0.96
2016-09-06T09:19:19.027421: step 1987, loss 0.0285088, acc 1
2016-09-06T09:19:19.851117: step 1988, loss 0.0863241, acc 0.98
2016-09-06T09:19:20.690863: step 1989, loss 0.0407712, acc 1
2016-09-06T09:19:21.501306: step 1990, loss 0.0705378, acc 0.96
2016-09-06T09:19:22.351664: step 1991, loss 0.0522455, acc 1
2016-09-06T09:19:23.176223: step 1992, loss 0.0731461, acc 1
2016-09-06T09:19:23.968392: step 1993, loss 0.0841842, acc 0.96
2016-09-06T09:19:24.776733: step 1994, loss 0.0847864, acc 0.96
2016-09-06T09:19:25.649459: step 1995, loss 0.101035, acc 0.94
2016-09-06T09:19:26.442070: step 1996, loss 0.0187205, acc 1
2016-09-06T09:19:27.244526: step 1997, loss 0.0765359, acc 0.98
2016-09-06T09:19:28.059913: step 1998, loss 0.081287, acc 0.96
2016-09-06T09:19:28.883977: step 1999, loss 0.0318583, acc 0.98
2016-09-06T09:19:29.697282: step 2000, loss 0.0967536, acc 0.96

Evaluation:
2016-09-06T09:19:33.438763: step 2000, loss 1.22634, acc 0.768293

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-2000

2016-09-06T09:19:35.310315: step 2001, loss 0.0231122, acc 1
2016-09-06T09:19:36.109996: step 2002, loss 0.007441, acc 1
2016-09-06T09:19:36.938047: step 2003, loss 0.0493213, acc 1
2016-09-06T09:19:37.753265: step 2004, loss 0.0134999, acc 1
2016-09-06T09:19:38.564751: step 2005, loss 0.0615771, acc 0.98
2016-09-06T09:19:39.408258: step 2006, loss 0.0843651, acc 0.92
2016-09-06T09:19:40.209130: step 2007, loss 0.0506928, acc 0.98
2016-09-06T09:19:41.025641: step 2008, loss 0.0197432, acc 1
2016-09-06T09:19:41.844456: step 2009, loss 0.0772436, acc 0.98
2016-09-06T09:19:42.673339: step 2010, loss 0.0501821, acc 0.96
2016-09-06T09:19:43.469146: step 2011, loss 0.0273647, acc 1
2016-09-06T09:19:44.257016: step 2012, loss 0.0304583, acc 0.98
2016-09-06T09:19:45.080784: step 2013, loss 0.178086, acc 0.94
2016-09-06T09:19:45.875953: step 2014, loss 0.0146022, acc 1
2016-09-06T09:19:46.672272: step 2015, loss 0.0281401, acc 1
2016-09-06T09:19:47.479861: step 2016, loss 0.0391732, acc 0.96
2016-09-06T09:19:48.272395: step 2017, loss 0.0634291, acc 0.96
2016-09-06T09:19:49.153504: step 2018, loss 0.0408763, acc 0.98
2016-09-06T09:19:49.971945: step 2019, loss 0.0671534, acc 0.96
2016-09-06T09:19:50.777417: step 2020, loss 0.0627816, acc 0.98
2016-09-06T09:19:51.574652: step 2021, loss 0.0668945, acc 0.94
2016-09-06T09:19:52.361453: step 2022, loss 0.00838157, acc 1
2016-09-06T09:19:53.146602: step 2023, loss 0.0582534, acc 0.96
2016-09-06T09:19:53.968210: step 2024, loss 0.09663, acc 0.98
2016-09-06T09:19:54.782278: step 2025, loss 0.020018, acc 1
2016-09-06T09:19:55.563200: step 2026, loss 0.03285, acc 1
2016-09-06T09:19:56.358018: step 2027, loss 0.0986364, acc 0.94
2016-09-06T09:19:57.155978: step 2028, loss 0.144244, acc 0.96
2016-09-06T09:19:57.942217: step 2029, loss 0.0630891, acc 0.98
2016-09-06T09:19:58.747999: step 2030, loss 0.0663313, acc 0.98
2016-09-06T09:19:59.564664: step 2031, loss 0.0327379, acc 0.98
2016-09-06T09:20:00.393450: step 2032, loss 0.0859405, acc 0.94
2016-09-06T09:20:01.222109: step 2033, loss 0.0266045, acc 1
2016-09-06T09:20:02.047800: step 2034, loss 0.0423975, acc 1
2016-09-06T09:20:02.872570: step 2035, loss 0.1444, acc 0.96
2016-09-06T09:20:03.679720: step 2036, loss 0.0321528, acc 1
2016-09-06T09:20:04.497914: step 2037, loss 0.0313317, acc 0.98
2016-09-06T09:20:05.297890: step 2038, loss 0.0453271, acc 0.96
2016-09-06T09:20:06.116895: step 2039, loss 0.0773704, acc 0.96
2016-09-06T09:20:06.913010: step 2040, loss 0.0451141, acc 0.98
2016-09-06T09:20:07.716400: step 2041, loss 0.0100038, acc 1
2016-09-06T09:20:08.525274: step 2042, loss 0.016979, acc 1
2016-09-06T09:20:09.331505: step 2043, loss 0.027627, acc 0.98
2016-09-06T09:20:10.114688: step 2044, loss 0.0186696, acc 1
2016-09-06T09:20:10.928847: step 2045, loss 0.0195601, acc 1
2016-09-06T09:20:11.732731: step 2046, loss 0.131049, acc 0.94
2016-09-06T09:20:12.509482: step 2047, loss 0.0401777, acc 0.98
2016-09-06T09:20:13.303523: step 2048, loss 0.0212728, acc 1
2016-09-06T09:20:14.150972: step 2049, loss 0.0877648, acc 0.98
2016-09-06T09:20:14.945460: step 2050, loss 0.0536107, acc 0.98
2016-09-06T09:20:15.758737: step 2051, loss 0.040606, acc 0.98
2016-09-06T09:20:16.593354: step 2052, loss 0.136911, acc 0.94
2016-09-06T09:20:17.384961: step 2053, loss 0.215496, acc 0.96
2016-09-06T09:20:18.191649: step 2054, loss 0.0692423, acc 0.98
2016-09-06T09:20:19.021262: step 2055, loss 0.0802977, acc 0.98
2016-09-06T09:20:19.818280: step 2056, loss 0.082409, acc 0.96
2016-09-06T09:20:20.681879: step 2057, loss 0.0348665, acc 0.98
2016-09-06T09:20:21.501372: step 2058, loss 0.0823732, acc 0.98
2016-09-06T09:20:22.302174: step 2059, loss 0.0340405, acc 0.98
2016-09-06T09:20:23.104290: step 2060, loss 0.0281398, acc 1
2016-09-06T09:20:23.919217: step 2061, loss 0.03723, acc 0.98
2016-09-06T09:20:24.737052: step 2062, loss 0.0230582, acc 0.98
2016-09-06T09:20:25.565280: step 2063, loss 0.0354076, acc 0.98
2016-09-06T09:20:26.391807: step 2064, loss 0.0182171, acc 1
2016-09-06T09:20:27.183253: step 2065, loss 0.110536, acc 0.92
2016-09-06T09:20:27.992587: step 2066, loss 0.0357626, acc 1
2016-09-06T09:20:28.807002: step 2067, loss 0.0118059, acc 1
2016-09-06T09:20:29.609378: step 2068, loss 0.0345593, acc 1
2016-09-06T09:20:30.431455: step 2069, loss 0.138058, acc 0.94
2016-09-06T09:20:31.273493: step 2070, loss 0.0269775, acc 1
2016-09-06T09:20:32.102623: step 2071, loss 0.0217023, acc 1
2016-09-06T09:20:32.917221: step 2072, loss 0.061155, acc 0.98
2016-09-06T09:20:33.774578: step 2073, loss 0.111735, acc 0.94
2016-09-06T09:20:34.589145: step 2074, loss 0.0457777, acc 0.98
2016-09-06T09:20:35.375293: step 2075, loss 0.0966564, acc 0.96
2016-09-06T09:20:36.196836: step 2076, loss 0.0459919, acc 0.98
2016-09-06T09:20:36.989412: step 2077, loss 0.0882224, acc 0.94
2016-09-06T09:20:37.794153: step 2078, loss 0.160473, acc 0.92
2016-09-06T09:20:38.631568: step 2079, loss 0.156179, acc 0.94
2016-09-06T09:20:39.472895: step 2080, loss 0.0452085, acc 1
2016-09-06T09:20:40.282084: step 2081, loss 0.0248747, acc 1
2016-09-06T09:20:41.086883: step 2082, loss 0.0312487, acc 1
2016-09-06T09:20:41.939635: step 2083, loss 0.0432285, acc 1
2016-09-06T09:20:42.706802: step 2084, loss 0.0123913, acc 1
2016-09-06T09:20:43.506637: step 2085, loss 0.0433447, acc 0.98
2016-09-06T09:20:44.325689: step 2086, loss 0.0416518, acc 0.96
2016-09-06T09:20:45.121574: step 2087, loss 0.0491666, acc 0.98
2016-09-06T09:20:45.933872: step 2088, loss 0.0645791, acc 0.96
2016-09-06T09:20:46.765090: step 2089, loss 0.0741114, acc 0.96
2016-09-06T09:20:47.546765: step 2090, loss 0.171468, acc 0.9
2016-09-06T09:20:48.353551: step 2091, loss 0.0417654, acc 0.98
2016-09-06T09:20:49.201921: step 2092, loss 0.0302791, acc 1
2016-09-06T09:20:50.004366: step 2093, loss 0.0299476, acc 1
2016-09-06T09:20:50.789035: step 2094, loss 0.0377561, acc 0.98
2016-09-06T09:20:51.596722: step 2095, loss 0.166139, acc 0.92
2016-09-06T09:20:52.368243: step 2096, loss 0.0364996, acc 0.98
2016-09-06T09:20:53.182051: step 2097, loss 0.04111, acc 0.98
2016-09-06T09:20:54.024058: step 2098, loss 0.0669046, acc 0.98
2016-09-06T09:20:54.843355: step 2099, loss 0.0289064, acc 0.98
2016-09-06T09:20:55.650172: step 2100, loss 0.0488852, acc 0.98

Evaluation:
2016-09-06T09:20:59.360722: step 2100, loss 1.15602, acc 0.78424

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-2100

2016-09-06T09:21:01.214044: step 2101, loss 0.0385457, acc 1
2016-09-06T09:21:02.023112: step 2102, loss 0.0555305, acc 0.98
2016-09-06T09:21:02.838823: step 2103, loss 0.00920001, acc 1
2016-09-06T09:21:03.708832: step 2104, loss 0.0783095, acc 0.94
2016-09-06T09:21:04.508664: step 2105, loss 0.0213899, acc 1
2016-09-06T09:21:05.325718: step 2106, loss 0.0582437, acc 0.96
2016-09-06T09:21:06.153536: step 2107, loss 0.100733, acc 0.94
2016-09-06T09:21:06.985549: step 2108, loss 0.01498, acc 1
2016-09-06T09:21:07.811581: step 2109, loss 0.0799889, acc 0.94
2016-09-06T09:21:08.661499: step 2110, loss 0.0465578, acc 0.98
2016-09-06T09:21:09.464679: step 2111, loss 0.0579427, acc 0.98
2016-09-06T09:21:10.204531: step 2112, loss 0.0621045, acc 0.977273
2016-09-06T09:21:11.033135: step 2113, loss 0.0377392, acc 0.98
2016-09-06T09:21:11.847448: step 2114, loss 0.115997, acc 0.96
2016-09-06T09:21:12.655791: step 2115, loss 0.0389207, acc 0.98
2016-09-06T09:21:13.490445: step 2116, loss 0.0423196, acc 0.98
2016-09-06T09:21:14.310881: step 2117, loss 0.0494468, acc 0.96
2016-09-06T09:21:15.140293: step 2118, loss 0.014436, acc 1
2016-09-06T09:21:16.022381: step 2119, loss 0.0226515, acc 0.98
2016-09-06T09:21:16.815204: step 2120, loss 0.0677081, acc 0.96
2016-09-06T09:21:17.619041: step 2121, loss 0.0562151, acc 0.98
2016-09-06T09:21:18.456155: step 2122, loss 0.0552738, acc 0.98
2016-09-06T09:21:19.251661: step 2123, loss 0.0400981, acc 0.98
2016-09-06T09:21:20.056607: step 2124, loss 0.0466321, acc 0.98
2016-09-06T09:21:20.877613: step 2125, loss 0.0573046, acc 0.98
2016-09-06T09:21:21.681209: step 2126, loss 0.0283041, acc 0.98
2016-09-06T09:21:22.473358: step 2127, loss 0.1592, acc 0.92
2016-09-06T09:21:23.310597: step 2128, loss 0.00755712, acc 1
2016-09-06T09:21:24.128411: step 2129, loss 0.0311318, acc 0.98
2016-09-06T09:21:24.945486: step 2130, loss 0.0339686, acc 0.98
2016-09-06T09:21:25.756799: step 2131, loss 0.170748, acc 0.96
2016-09-06T09:21:26.565577: step 2132, loss 0.0140335, acc 1
2016-09-06T09:21:27.384862: step 2133, loss 0.0366915, acc 0.98
2016-09-06T09:21:28.204531: step 2134, loss 0.0462322, acc 1
2016-09-06T09:21:29.024688: step 2135, loss 0.0134324, acc 1
2016-09-06T09:21:29.831455: step 2136, loss 0.0383235, acc 1
2016-09-06T09:21:30.644542: step 2137, loss 0.017168, acc 1
2016-09-06T09:21:31.474720: step 2138, loss 0.115855, acc 0.94
2016-09-06T09:21:32.272244: step 2139, loss 0.0205051, acc 1
2016-09-06T09:21:33.074293: step 2140, loss 0.0992716, acc 0.98
2016-09-06T09:21:33.869381: step 2141, loss 0.0400967, acc 0.98
2016-09-06T09:21:34.668476: step 2142, loss 0.0561831, acc 0.98
2016-09-06T09:21:35.482034: step 2143, loss 0.0593295, acc 0.96
2016-09-06T09:21:36.288439: step 2144, loss 0.0933243, acc 0.94
2016-09-06T09:21:37.125290: step 2145, loss 0.0727203, acc 0.98
2016-09-06T09:21:37.952161: step 2146, loss 0.0450699, acc 0.98
2016-09-06T09:21:38.761659: step 2147, loss 0.0225077, acc 1
2016-09-06T09:21:39.554447: step 2148, loss 0.0331215, acc 0.98
2016-09-06T09:21:40.357422: step 2149, loss 0.146402, acc 0.94
2016-09-06T09:21:41.180147: step 2150, loss 0.00907176, acc 1
2016-09-06T09:21:41.971679: step 2151, loss 0.0964678, acc 0.94
2016-09-06T09:21:42.767640: step 2152, loss 0.0104184, acc 1
2016-09-06T09:21:43.578883: step 2153, loss 0.0960984, acc 0.96
2016-09-06T09:21:44.383114: step 2154, loss 0.020412, acc 1
2016-09-06T09:21:45.184594: step 2155, loss 0.246081, acc 0.94
2016-09-06T09:21:46.004397: step 2156, loss 0.046911, acc 0.98
2016-09-06T09:21:46.783152: step 2157, loss 0.00599026, acc 1
2016-09-06T09:21:47.579647: step 2158, loss 0.0597474, acc 0.98
2016-09-06T09:21:48.392450: step 2159, loss 0.0862958, acc 0.96
2016-09-06T09:21:49.173132: step 2160, loss 0.0908213, acc 0.94
2016-09-06T09:21:49.979358: step 2161, loss 0.0409582, acc 0.96
2016-09-06T09:21:50.793860: step 2162, loss 0.111206, acc 0.98
2016-09-06T09:21:51.586920: step 2163, loss 0.0158177, acc 1
2016-09-06T09:21:52.405249: step 2164, loss 0.088231, acc 0.94
2016-09-06T09:21:53.232421: step 2165, loss 0.0307581, acc 0.98
2016-09-06T09:21:54.073760: step 2166, loss 0.029574, acc 1
2016-09-06T09:21:54.859798: step 2167, loss 0.113431, acc 0.96
2016-09-06T09:21:55.685680: step 2168, loss 0.029129, acc 1
2016-09-06T09:21:56.464203: step 2169, loss 0.0423139, acc 0.98
2016-09-06T09:21:57.267455: step 2170, loss 0.0829321, acc 0.96
2016-09-06T09:21:58.087976: step 2171, loss 0.0291248, acc 1
2016-09-06T09:21:58.878446: step 2172, loss 0.033378, acc 0.98
2016-09-06T09:21:59.708551: step 2173, loss 0.297269, acc 0.92
2016-09-06T09:22:00.539907: step 2174, loss 0.0674116, acc 0.96
2016-09-06T09:22:01.335787: step 2175, loss 0.0294808, acc 1
2016-09-06T09:22:02.120086: step 2176, loss 0.0948947, acc 0.96
2016-09-06T09:22:02.944564: step 2177, loss 0.0894023, acc 0.96
2016-09-06T09:22:03.742808: step 2178, loss 0.0393698, acc 0.98
2016-09-06T09:22:04.552481: step 2179, loss 0.0332136, acc 0.98
2016-09-06T09:22:05.374458: step 2180, loss 0.0836074, acc 0.98
2016-09-06T09:22:06.203181: step 2181, loss 0.129729, acc 0.94
2016-09-06T09:22:07.004850: step 2182, loss 0.0269569, acc 1
2016-09-06T09:22:07.808327: step 2183, loss 0.0447205, acc 0.98
2016-09-06T09:22:08.601872: step 2184, loss 0.0620903, acc 0.96
2016-09-06T09:22:09.400422: step 2185, loss 0.0479297, acc 0.96
2016-09-06T09:22:10.201481: step 2186, loss 0.0768262, acc 0.96
2016-09-06T09:22:10.989792: step 2187, loss 0.0668916, acc 0.98
2016-09-06T09:22:11.785296: step 2188, loss 0.0703121, acc 0.98
2016-09-06T09:22:12.617327: step 2189, loss 0.0766224, acc 0.96
2016-09-06T09:22:13.401073: step 2190, loss 0.0880888, acc 0.96
2016-09-06T09:22:14.239892: step 2191, loss 0.0132908, acc 1
2016-09-06T09:22:15.044727: step 2192, loss 0.107767, acc 0.94
2016-09-06T09:22:15.846495: step 2193, loss 0.047548, acc 1
2016-09-06T09:22:16.639657: step 2194, loss 0.070979, acc 0.96
2016-09-06T09:22:17.457204: step 2195, loss 0.0177994, acc 1
2016-09-06T09:22:18.247583: step 2196, loss 0.0136287, acc 1
2016-09-06T09:22:19.059698: step 2197, loss 0.0943377, acc 0.98
2016-09-06T09:22:19.870102: step 2198, loss 0.0395681, acc 0.98
2016-09-06T09:22:20.678842: step 2199, loss 0.0166447, acc 1
2016-09-06T09:22:21.483491: step 2200, loss 0.0635724, acc 0.98

Evaluation:
2016-09-06T09:22:25.186635: step 2200, loss 1.20867, acc 0.774859

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-2200

2016-09-06T09:22:27.039510: step 2201, loss 0.0331885, acc 1
2016-09-06T09:22:27.864473: step 2202, loss 0.0139469, acc 1
2016-09-06T09:22:28.686174: step 2203, loss 0.16635, acc 0.9
2016-09-06T09:22:29.513472: step 2204, loss 0.0289973, acc 1
2016-09-06T09:22:30.362707: step 2205, loss 0.0670508, acc 0.96
2016-09-06T09:22:31.148463: step 2206, loss 0.0533598, acc 0.98
2016-09-06T09:22:31.967161: step 2207, loss 0.102389, acc 0.94
2016-09-06T09:22:32.736233: step 2208, loss 0.0309249, acc 0.98
2016-09-06T09:22:33.525663: step 2209, loss 0.0157654, acc 1
2016-09-06T09:22:34.329974: step 2210, loss 0.07496, acc 0.98
2016-09-06T09:22:35.126413: step 2211, loss 0.0792487, acc 0.96
2016-09-06T09:22:35.975704: step 2212, loss 0.100812, acc 0.94
2016-09-06T09:22:36.819157: step 2213, loss 0.0934309, acc 0.96
2016-09-06T09:22:37.623105: step 2214, loss 0.0588506, acc 0.96
2016-09-06T09:22:38.451322: step 2215, loss 0.0680744, acc 0.96
2016-09-06T09:22:39.247944: step 2216, loss 0.0204617, acc 1
2016-09-06T09:22:40.044243: step 2217, loss 0.0488113, acc 0.96
2016-09-06T09:22:40.847998: step 2218, loss 0.0296274, acc 1
2016-09-06T09:22:41.666093: step 2219, loss 0.0691495, acc 0.96
2016-09-06T09:22:42.481571: step 2220, loss 0.0478826, acc 0.98
2016-09-06T09:22:43.271360: step 2221, loss 0.0847522, acc 0.98
2016-09-06T09:22:44.083837: step 2222, loss 0.0499029, acc 0.98
2016-09-06T09:22:44.891836: step 2223, loss 0.0383296, acc 0.98
2016-09-06T09:22:45.692403: step 2224, loss 0.111876, acc 0.94
2016-09-06T09:22:46.497310: step 2225, loss 0.0292768, acc 0.98
2016-09-06T09:22:47.322523: step 2226, loss 0.0380842, acc 1
2016-09-06T09:22:48.128119: step 2227, loss 0.0561196, acc 0.96
2016-09-06T09:22:48.977606: step 2228, loss 0.0875598, acc 0.98
2016-09-06T09:22:49.805486: step 2229, loss 0.154324, acc 0.96
2016-09-06T09:22:50.614900: step 2230, loss 0.026581, acc 1
2016-09-06T09:22:51.454932: step 2231, loss 0.0155192, acc 1
2016-09-06T09:22:52.252813: step 2232, loss 0.0343559, acc 0.98
2016-09-06T09:22:53.060137: step 2233, loss 0.0497863, acc 0.98
2016-09-06T09:22:53.901262: step 2234, loss 0.0584255, acc 0.98
2016-09-06T09:22:54.718289: step 2235, loss 0.0238277, acc 0.98
2016-09-06T09:22:55.512674: step 2236, loss 0.0687539, acc 0.96
2016-09-06T09:22:56.365414: step 2237, loss 0.104752, acc 0.94
2016-09-06T09:22:57.189375: step 2238, loss 0.0244837, acc 1
2016-09-06T09:22:58.005586: step 2239, loss 0.199782, acc 0.9
2016-09-06T09:22:58.830220: step 2240, loss 0.0737517, acc 0.94
2016-09-06T09:22:59.626152: step 2241, loss 0.0280122, acc 0.98
2016-09-06T09:23:00.445109: step 2242, loss 0.0630023, acc 0.96
2016-09-06T09:23:01.265692: step 2243, loss 0.0134957, acc 1
2016-09-06T09:23:02.072248: step 2244, loss 0.0484883, acc 0.98
2016-09-06T09:23:02.925259: step 2245, loss 0.037627, acc 1
2016-09-06T09:23:03.791326: step 2246, loss 0.0996401, acc 0.94
2016-09-06T09:23:04.610809: step 2247, loss 0.0711638, acc 0.96
2016-09-06T09:23:05.377127: step 2248, loss 0.0320155, acc 1
2016-09-06T09:23:06.182286: step 2249, loss 0.0443422, acc 0.98
2016-09-06T09:23:06.978325: step 2250, loss 0.111832, acc 0.98
2016-09-06T09:23:07.791184: step 2251, loss 0.0169806, acc 1
2016-09-06T09:23:08.599992: step 2252, loss 0.0164312, acc 1
2016-09-06T09:23:09.438769: step 2253, loss 0.0577863, acc 0.98
2016-09-06T09:23:10.207651: step 2254, loss 0.00968798, acc 1
2016-09-06T09:23:11.003475: step 2255, loss 0.0153989, acc 1
2016-09-06T09:23:11.820814: step 2256, loss 0.0449003, acc 1
2016-09-06T09:23:12.617889: step 2257, loss 0.0654917, acc 0.98
2016-09-06T09:23:13.421933: step 2258, loss 0.0520365, acc 0.96
2016-09-06T09:23:14.224792: step 2259, loss 0.0836943, acc 0.96
2016-09-06T09:23:15.026653: step 2260, loss 0.0159051, acc 1
2016-09-06T09:23:15.820440: step 2261, loss 0.0228803, acc 1
2016-09-06T09:23:16.620329: step 2262, loss 0.0186672, acc 1
2016-09-06T09:23:17.380873: step 2263, loss 0.0326669, acc 0.98
2016-09-06T09:23:18.232391: step 2264, loss 0.0367051, acc 1
2016-09-06T09:23:19.088016: step 2265, loss 0.0103154, acc 1
2016-09-06T09:23:19.872629: step 2266, loss 0.0596466, acc 0.96
2016-09-06T09:23:20.674849: step 2267, loss 0.0262732, acc 0.98
2016-09-06T09:23:21.495159: step 2268, loss 0.0217713, acc 1
2016-09-06T09:23:22.287841: step 2269, loss 0.0955056, acc 0.96
2016-09-06T09:23:23.093324: step 2270, loss 0.0599201, acc 0.96
2016-09-06T09:23:23.916069: step 2271, loss 0.139732, acc 0.94
2016-09-06T09:23:24.714020: step 2272, loss 0.106181, acc 0.94
2016-09-06T09:23:25.521794: step 2273, loss 0.075901, acc 0.94
2016-09-06T09:23:26.353432: step 2274, loss 0.033734, acc 1
2016-09-06T09:23:27.146227: step 2275, loss 0.02363, acc 0.98
2016-09-06T09:23:27.934152: step 2276, loss 0.0965442, acc 0.96
2016-09-06T09:23:28.767786: step 2277, loss 0.0372209, acc 0.98
2016-09-06T09:23:29.553581: step 2278, loss 0.0307981, acc 1
2016-09-06T09:23:30.352876: step 2279, loss 0.0331206, acc 1
2016-09-06T09:23:31.197323: step 2280, loss 0.127778, acc 0.92
2016-09-06T09:23:31.982486: step 2281, loss 0.109571, acc 0.96
2016-09-06T09:23:32.808999: step 2282, loss 0.0545633, acc 1
2016-09-06T09:23:33.613424: step 2283, loss 0.0187252, acc 1
2016-09-06T09:23:34.377055: step 2284, loss 0.0726261, acc 0.96
2016-09-06T09:23:35.201996: step 2285, loss 0.0254758, acc 1
2016-09-06T09:23:36.028818: step 2286, loss 0.0338233, acc 0.98
2016-09-06T09:23:36.819181: step 2287, loss 0.0607348, acc 0.98
2016-09-06T09:23:37.659023: step 2288, loss 0.0309469, acc 1
2016-09-06T09:23:38.478367: step 2289, loss 0.0384793, acc 0.98
2016-09-06T09:23:39.274092: step 2290, loss 0.0585424, acc 0.96
2016-09-06T09:23:40.079483: step 2291, loss 0.0631834, acc 0.96
2016-09-06T09:23:40.890071: step 2292, loss 0.0717294, acc 0.96
2016-09-06T09:23:41.656739: step 2293, loss 0.0496991, acc 0.98
2016-09-06T09:23:42.499247: step 2294, loss 0.0875743, acc 0.96
2016-09-06T09:23:43.312905: step 2295, loss 0.0143272, acc 1
2016-09-06T09:23:44.111421: step 2296, loss 0.0865033, acc 0.96
2016-09-06T09:23:44.911062: step 2297, loss 0.0172287, acc 1
2016-09-06T09:23:45.703982: step 2298, loss 0.0637439, acc 0.98
2016-09-06T09:23:46.478468: step 2299, loss 0.0306713, acc 0.98
2016-09-06T09:23:47.275610: step 2300, loss 0.0138909, acc 1

Evaluation:
2016-09-06T09:23:51.010074: step 2300, loss 1.35674, acc 0.780488

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-2300

2016-09-06T09:23:52.891626: step 2301, loss 0.0518482, acc 0.98
2016-09-06T09:23:53.728612: step 2302, loss 0.0272182, acc 1
2016-09-06T09:23:54.540900: step 2303, loss 0.0298865, acc 0.98
2016-09-06T09:23:55.315638: step 2304, loss 0.0140208, acc 1
2016-09-06T09:23:56.122964: step 2305, loss 0.0605039, acc 0.98
2016-09-06T09:23:56.931379: step 2306, loss 0.0624729, acc 0.96
2016-09-06T09:23:57.722474: step 2307, loss 0.0397449, acc 0.98
2016-09-06T09:23:58.517245: step 2308, loss 0.0136937, acc 1
2016-09-06T09:23:59.336769: step 2309, loss 0.0445116, acc 0.98
2016-09-06T09:24:00.127916: step 2310, loss 0.0365732, acc 1
2016-09-06T09:24:00.953347: step 2311, loss 0.0970575, acc 0.94
2016-09-06T09:24:01.777873: step 2312, loss 0.0467845, acc 0.98
2016-09-06T09:24:02.591331: step 2313, loss 0.142665, acc 0.94
2016-09-06T09:24:03.388805: step 2314, loss 0.048353, acc 0.98
2016-09-06T09:24:04.243843: step 2315, loss 0.00490489, acc 1
2016-09-06T09:24:05.065372: step 2316, loss 0.0193192, acc 0.98
2016-09-06T09:24:05.870068: step 2317, loss 0.0404982, acc 1
2016-09-06T09:24:06.686293: step 2318, loss 0.0421205, acc 0.96
2016-09-06T09:24:07.502082: step 2319, loss 0.120447, acc 0.94
2016-09-06T09:24:08.298500: step 2320, loss 0.0538795, acc 0.98
2016-09-06T09:24:09.126902: step 2321, loss 0.0205661, acc 1
2016-09-06T09:24:09.948117: step 2322, loss 0.00516954, acc 1
2016-09-06T09:24:10.741436: step 2323, loss 0.0735889, acc 0.98
2016-09-06T09:24:11.578171: step 2324, loss 0.0597533, acc 0.98
2016-09-06T09:24:12.402111: step 2325, loss 0.0337435, acc 0.98
2016-09-06T09:24:13.175960: step 2326, loss 0.046276, acc 1
2016-09-06T09:24:13.997461: step 2327, loss 0.041089, acc 0.98
2016-09-06T09:24:14.811582: step 2328, loss 0.0851266, acc 0.96
2016-09-06T09:24:15.592999: step 2329, loss 0.0295013, acc 0.98
2016-09-06T09:24:16.416920: step 2330, loss 0.0116951, acc 1
2016-09-06T09:24:17.235898: step 2331, loss 0.0234187, acc 1
2016-09-06T09:24:18.012370: step 2332, loss 0.0365806, acc 0.98
2016-09-06T09:24:18.825950: step 2333, loss 0.054286, acc 0.96
2016-09-06T09:24:19.637521: step 2334, loss 0.121321, acc 0.98
2016-09-06T09:24:20.428510: step 2335, loss 0.00389471, acc 1
2016-09-06T09:24:21.252365: step 2336, loss 0.0471913, acc 0.96
2016-09-06T09:24:22.081698: step 2337, loss 0.033517, acc 0.98
2016-09-06T09:24:22.850748: step 2338, loss 0.0252462, acc 1
2016-09-06T09:24:23.687204: step 2339, loss 0.0687406, acc 0.98
2016-09-06T09:24:24.506256: step 2340, loss 0.0486355, acc 0.98
2016-09-06T09:24:25.285398: step 2341, loss 0.093546, acc 0.96
2016-09-06T09:24:26.103552: step 2342, loss 0.0094521, acc 1
2016-09-06T09:24:26.923821: step 2343, loss 0.0890942, acc 0.98
2016-09-06T09:24:27.721946: step 2344, loss 0.0442373, acc 0.96
2016-09-06T09:24:28.509409: step 2345, loss 0.0238489, acc 0.98
2016-09-06T09:24:29.286844: step 2346, loss 0.0446592, acc 0.96
2016-09-06T09:24:30.080942: step 2347, loss 0.136503, acc 0.96
2016-09-06T09:24:30.938713: step 2348, loss 0.0470427, acc 0.98
2016-09-06T09:24:31.745930: step 2349, loss 0.00604275, acc 1
2016-09-06T09:24:32.537496: step 2350, loss 0.0174227, acc 1
2016-09-06T09:24:33.327727: step 2351, loss 0.0606814, acc 0.98
2016-09-06T09:24:34.145444: step 2352, loss 0.0694028, acc 0.96
2016-09-06T09:24:34.932219: step 2353, loss 0.0492486, acc 0.98
2016-09-06T09:24:35.724698: step 2354, loss 0.0120599, acc 1
2016-09-06T09:24:36.555598: step 2355, loss 0.0175548, acc 1
2016-09-06T09:24:37.366075: step 2356, loss 0.0457823, acc 0.98
2016-09-06T09:24:38.206984: step 2357, loss 0.0466107, acc 0.98
2016-09-06T09:24:39.031399: step 2358, loss 0.0416316, acc 0.98
2016-09-06T09:24:39.827579: step 2359, loss 0.00734358, acc 1
2016-09-06T09:24:40.614457: step 2360, loss 0.0356883, acc 0.98
2016-09-06T09:24:41.424895: step 2361, loss 0.00832698, acc 1
2016-09-06T09:24:42.229219: step 2362, loss 0.0385338, acc 0.98
2016-09-06T09:24:43.029222: step 2363, loss 0.0148027, acc 1
2016-09-06T09:24:43.847831: step 2364, loss 0.0167561, acc 1
2016-09-06T09:24:44.651622: step 2365, loss 0.0694809, acc 0.96
2016-09-06T09:24:45.465649: step 2366, loss 0.0663223, acc 0.98
2016-09-06T09:24:46.298908: step 2367, loss 0.00742721, acc 1
2016-09-06T09:24:47.114314: step 2368, loss 0.0561034, acc 0.96
2016-09-06T09:24:47.943277: step 2369, loss 0.0274789, acc 0.98
2016-09-06T09:24:48.784252: step 2370, loss 0.0657386, acc 0.96
2016-09-06T09:24:49.606483: step 2371, loss 0.0570397, acc 0.98
2016-09-06T09:24:50.423348: step 2372, loss 0.0460512, acc 0.98
2016-09-06T09:24:51.250029: step 2373, loss 0.0128929, acc 1
2016-09-06T09:24:52.059420: step 2374, loss 0.0192812, acc 1
2016-09-06T09:24:52.868171: step 2375, loss 0.0860955, acc 0.98
2016-09-06T09:24:53.705859: step 2376, loss 0.107724, acc 0.96
2016-09-06T09:24:54.517440: step 2377, loss 0.0192869, acc 1
2016-09-06T09:24:55.336563: step 2378, loss 0.0621895, acc 0.96
2016-09-06T09:24:56.196452: step 2379, loss 0.0556024, acc 1
2016-09-06T09:24:57.021046: step 2380, loss 0.0399194, acc 1
2016-09-06T09:24:57.821987: step 2381, loss 0.0347074, acc 0.98
2016-09-06T09:24:58.647554: step 2382, loss 0.0663074, acc 0.96
2016-09-06T09:24:59.425125: step 2383, loss 0.0302764, acc 0.98
2016-09-06T09:25:00.253577: step 2384, loss 0.00578688, acc 1
2016-09-06T09:25:01.078434: step 2385, loss 0.0363745, acc 0.98
2016-09-06T09:25:01.939447: step 2386, loss 0.107855, acc 0.92
2016-09-06T09:25:02.740919: step 2387, loss 0.0647867, acc 0.96
2016-09-06T09:25:03.585469: step 2388, loss 0.0411519, acc 0.98
2016-09-06T09:25:04.426394: step 2389, loss 0.0373887, acc 0.98
2016-09-06T09:25:05.242486: step 2390, loss 0.0427564, acc 1
2016-09-06T09:25:06.060090: step 2391, loss 0.0345092, acc 0.98
2016-09-06T09:25:06.923765: step 2392, loss 0.153389, acc 0.94
2016-09-06T09:25:07.751465: step 2393, loss 0.0803513, acc 0.96
2016-09-06T09:25:08.560704: step 2394, loss 0.0365359, acc 0.98
2016-09-06T09:25:09.413448: step 2395, loss 0.126657, acc 0.96
2016-09-06T09:25:10.232615: step 2396, loss 0.108231, acc 0.98
2016-09-06T09:25:11.019606: step 2397, loss 0.0818436, acc 0.98
2016-09-06T09:25:11.854490: step 2398, loss 0.094937, acc 0.96
2016-09-06T09:25:12.669661: step 2399, loss 0.0148065, acc 1
2016-09-06T09:25:13.476201: step 2400, loss 0.0462598, acc 0.96

Evaluation:
2016-09-06T09:25:17.173387: step 2400, loss 1.15288, acc 0.770169

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-2400

2016-09-06T09:25:19.132432: step 2401, loss 0.0297743, acc 0.98
2016-09-06T09:25:20.007996: step 2402, loss 0.062488, acc 1
2016-09-06T09:25:20.838155: step 2403, loss 0.0486735, acc 0.98
2016-09-06T09:25:21.670299: step 2404, loss 0.0532539, acc 0.96
2016-09-06T09:25:22.483461: step 2405, loss 0.0375117, acc 1
2016-09-06T09:25:23.313396: step 2406, loss 0.0261617, acc 0.98
2016-09-06T09:25:24.168934: step 2407, loss 0.0365554, acc 1
2016-09-06T09:25:25.036828: step 2408, loss 0.0528937, acc 0.98
2016-09-06T09:25:25.813427: step 2409, loss 0.108763, acc 0.94
2016-09-06T09:25:26.597799: step 2410, loss 0.0383321, acc 1
2016-09-06T09:25:27.412400: step 2411, loss 0.0525173, acc 0.94
2016-09-06T09:25:28.213819: step 2412, loss 0.0230051, acc 1
2016-09-06T09:25:29.009683: step 2413, loss 0.0214264, acc 1
2016-09-06T09:25:29.827950: step 2414, loss 0.0469084, acc 0.98
2016-09-06T09:25:30.617481: step 2415, loss 0.0060477, acc 1
2016-09-06T09:25:31.418262: step 2416, loss 0.0909066, acc 0.96
2016-09-06T09:25:32.246033: step 2417, loss 0.0275269, acc 0.98
2016-09-06T09:25:33.048092: step 2418, loss 0.0461282, acc 0.98
2016-09-06T09:25:33.858935: step 2419, loss 0.0403726, acc 0.98
2016-09-06T09:25:34.699503: step 2420, loss 0.0101546, acc 1
2016-09-06T09:25:35.485565: step 2421, loss 0.0181954, acc 0.98
2016-09-06T09:25:36.285508: step 2422, loss 0.034347, acc 0.98
2016-09-06T09:25:37.129434: step 2423, loss 0.0340553, acc 0.98
2016-09-06T09:25:37.897002: step 2424, loss 0.0473173, acc 0.96
2016-09-06T09:25:38.699315: step 2425, loss 0.0143471, acc 1
2016-09-06T09:25:39.525429: step 2426, loss 0.0253637, acc 0.98
2016-09-06T09:25:40.296321: step 2427, loss 0.0213883, acc 1
2016-09-06T09:25:41.095122: step 2428, loss 0.0644944, acc 0.96
2016-09-06T09:25:41.918036: step 2429, loss 0.0560524, acc 0.98
2016-09-06T09:25:42.708248: step 2430, loss 0.0184058, acc 1
2016-09-06T09:25:43.525154: step 2431, loss 0.0395931, acc 0.98
2016-09-06T09:25:44.346726: step 2432, loss 0.0302222, acc 1
2016-09-06T09:25:45.165652: step 2433, loss 0.0859141, acc 0.94
2016-09-06T09:25:45.977555: step 2434, loss 0.054849, acc 0.98
2016-09-06T09:25:46.797358: step 2435, loss 0.0303771, acc 1
2016-09-06T09:25:47.597984: step 2436, loss 0.0427748, acc 0.98
2016-09-06T09:25:48.408488: step 2437, loss 0.0196799, acc 1
2016-09-06T09:25:49.229060: step 2438, loss 0.104073, acc 0.96
2016-09-06T09:25:50.015502: step 2439, loss 0.0883236, acc 0.98
2016-09-06T09:25:50.800095: step 2440, loss 0.0734995, acc 0.96
2016-09-06T09:25:51.596077: step 2441, loss 0.0227893, acc 1
2016-09-06T09:25:52.390655: step 2442, loss 0.0863539, acc 0.96
2016-09-06T09:25:53.183012: step 2443, loss 0.0187748, acc 1
2016-09-06T09:25:54.037771: step 2444, loss 0.0509728, acc 0.96
2016-09-06T09:25:54.825526: step 2445, loss 0.07379, acc 0.96
2016-09-06T09:25:55.636024: step 2446, loss 0.0449773, acc 0.96
2016-09-06T09:25:56.460612: step 2447, loss 0.091411, acc 0.96
2016-09-06T09:25:57.227630: step 2448, loss 0.137936, acc 0.92
2016-09-06T09:25:58.013715: step 2449, loss 0.0103281, acc 1
2016-09-06T09:25:58.824226: step 2450, loss 0.0338167, acc 0.98
2016-09-06T09:25:59.616300: step 2451, loss 0.160932, acc 0.96
2016-09-06T09:26:00.446281: step 2452, loss 0.0686834, acc 0.96
2016-09-06T09:26:01.273821: step 2453, loss 0.0564669, acc 0.98
2016-09-06T09:26:02.055051: step 2454, loss 0.0834343, acc 0.98
2016-09-06T09:26:02.865089: step 2455, loss 0.247606, acc 0.92
2016-09-06T09:26:03.673621: step 2456, loss 0.0186678, acc 1
2016-09-06T09:26:04.470271: step 2457, loss 0.0651336, acc 0.98
2016-09-06T09:26:05.274331: step 2458, loss 0.0427963, acc 1
2016-09-06T09:26:06.107485: step 2459, loss 0.0548262, acc 0.98
2016-09-06T09:26:06.893696: step 2460, loss 0.0432712, acc 1
2016-09-06T09:26:07.712531: step 2461, loss 0.045751, acc 0.98
2016-09-06T09:26:08.537915: step 2462, loss 0.102357, acc 0.94
2016-09-06T09:26:09.309081: step 2463, loss 0.0308893, acc 0.98
2016-09-06T09:26:10.118197: step 2464, loss 0.105952, acc 0.94
2016-09-06T09:26:10.947346: step 2465, loss 0.113771, acc 0.96
2016-09-06T09:26:11.737390: step 2466, loss 0.0518927, acc 0.98
2016-09-06T09:26:12.520537: step 2467, loss 0.167694, acc 0.96
2016-09-06T09:26:13.351731: step 2468, loss 0.123308, acc 0.98
2016-09-06T09:26:14.137255: step 2469, loss 0.034908, acc 1
2016-09-06T09:26:14.939112: step 2470, loss 0.0522761, acc 1
2016-09-06T09:26:15.764392: step 2471, loss 0.0633496, acc 0.96
2016-09-06T09:26:16.566642: step 2472, loss 0.132496, acc 0.92
2016-09-06T09:26:17.363567: step 2473, loss 0.0872593, acc 0.98
2016-09-06T09:26:18.192912: step 2474, loss 0.04104, acc 0.98
2016-09-06T09:26:18.990874: step 2475, loss 0.131752, acc 0.98
2016-09-06T09:26:19.789346: step 2476, loss 0.0439729, acc 0.98
2016-09-06T09:26:20.622416: step 2477, loss 0.0575821, acc 0.98
2016-09-06T09:26:21.416250: step 2478, loss 0.0467808, acc 0.96
2016-09-06T09:26:22.212294: step 2479, loss 0.117298, acc 0.98
2016-09-06T09:26:23.031480: step 2480, loss 0.0195204, acc 1
2016-09-06T09:26:23.817398: step 2481, loss 0.141834, acc 0.96
2016-09-06T09:26:24.629019: step 2482, loss 0.028475, acc 1
2016-09-06T09:26:25.476608: step 2483, loss 0.0932125, acc 0.96
2016-09-06T09:26:26.273851: step 2484, loss 0.0594218, acc 0.96
2016-09-06T09:26:27.067364: step 2485, loss 0.0378147, acc 0.98
2016-09-06T09:26:27.863599: step 2486, loss 0.0847407, acc 0.96
2016-09-06T09:26:28.657423: step 2487, loss 0.0348151, acc 0.98
2016-09-06T09:26:29.436990: step 2488, loss 0.120681, acc 0.96
2016-09-06T09:26:30.265920: step 2489, loss 0.0337344, acc 0.98
2016-09-06T09:26:31.060864: step 2490, loss 0.0373631, acc 0.98
2016-09-06T09:26:31.884946: step 2491, loss 0.0692624, acc 0.96
2016-09-06T09:26:32.724613: step 2492, loss 0.023777, acc 1
2016-09-06T09:26:33.532576: step 2493, loss 0.0810087, acc 0.94
2016-09-06T09:26:34.358773: step 2494, loss 0.0654576, acc 0.98
2016-09-06T09:26:35.186509: step 2495, loss 0.0311045, acc 0.98
2016-09-06T09:26:35.943206: step 2496, loss 0.0129511, acc 1
2016-09-06T09:26:36.748931: step 2497, loss 0.0927366, acc 0.96
2016-09-06T09:26:37.557345: step 2498, loss 0.0944035, acc 0.94
2016-09-06T09:26:38.351308: step 2499, loss 0.0889294, acc 0.96
2016-09-06T09:26:39.155130: step 2500, loss 0.0558711, acc 0.96

Evaluation:
2016-09-06T09:26:42.941258: step 2500, loss 1.18649, acc 0.768293

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-2500

2016-09-06T09:26:44.848877: step 2501, loss 0.149591, acc 0.94
2016-09-06T09:26:45.670436: step 2502, loss 0.0518266, acc 1
2016-09-06T09:26:46.505747: step 2503, loss 0.0445146, acc 0.98
2016-09-06T09:26:47.348838: step 2504, loss 0.0454967, acc 0.96
2016-09-06T09:26:48.155655: step 2505, loss 0.0931011, acc 0.98
2016-09-06T09:26:48.987768: step 2506, loss 0.141408, acc 0.96
2016-09-06T09:26:49.805070: step 2507, loss 0.00659038, acc 1
2016-09-06T09:26:50.624549: step 2508, loss 0.0293714, acc 0.98
2016-09-06T09:26:51.440995: step 2509, loss 0.0802589, acc 0.96
2016-09-06T09:26:52.283614: step 2510, loss 0.0763496, acc 0.96
2016-09-06T09:26:53.081407: step 2511, loss 0.112374, acc 0.94
2016-09-06T09:26:53.901343: step 2512, loss 0.0303683, acc 0.98
2016-09-06T09:26:54.754313: step 2513, loss 0.0529828, acc 0.96
2016-09-06T09:26:55.557057: step 2514, loss 0.0240554, acc 1
2016-09-06T09:26:56.364531: step 2515, loss 0.0199095, acc 1
2016-09-06T09:26:57.189722: step 2516, loss 0.0221836, acc 0.98
2016-09-06T09:26:58.032367: step 2517, loss 0.0137696, acc 1
2016-09-06T09:26:58.851129: step 2518, loss 0.021906, acc 1
2016-09-06T09:26:59.699972: step 2519, loss 0.0237874, acc 1
2016-09-06T09:27:00.554475: step 2520, loss 0.110037, acc 0.96
2016-09-06T09:27:01.321942: step 2521, loss 0.0250892, acc 1
2016-09-06T09:27:02.120551: step 2522, loss 0.0510031, acc 0.98
2016-09-06T09:27:02.945804: step 2523, loss 0.0519324, acc 0.98
2016-09-06T09:27:03.756594: step 2524, loss 0.0667859, acc 0.96
2016-09-06T09:27:04.556895: step 2525, loss 0.0106711, acc 1
2016-09-06T09:27:05.371244: step 2526, loss 0.0576621, acc 0.96
2016-09-06T09:27:06.150796: step 2527, loss 0.0178949, acc 1
2016-09-06T09:27:06.957179: step 2528, loss 0.0614759, acc 0.98
2016-09-06T09:27:07.767161: step 2529, loss 0.0775169, acc 0.98
2016-09-06T09:27:08.567316: step 2530, loss 0.0361805, acc 1
2016-09-06T09:27:09.397453: step 2531, loss 0.024635, acc 1
2016-09-06T09:27:10.213034: step 2532, loss 0.00987375, acc 1
2016-09-06T09:27:10.973246: step 2533, loss 0.0428256, acc 0.98
2016-09-06T09:27:11.765798: step 2534, loss 0.0477508, acc 0.96
2016-09-06T09:27:12.576204: step 2535, loss 0.0304698, acc 0.98
2016-09-06T09:27:13.384139: step 2536, loss 0.0451788, acc 0.98
2016-09-06T09:27:14.165584: step 2537, loss 0.0049362, acc 1
2016-09-06T09:27:14.965581: step 2538, loss 0.123891, acc 0.98
2016-09-06T09:27:15.802843: step 2539, loss 0.105314, acc 0.96
2016-09-06T09:27:16.608696: step 2540, loss 0.0196765, acc 1
2016-09-06T09:27:17.417539: step 2541, loss 0.00698667, acc 1
2016-09-06T09:27:18.243494: step 2542, loss 0.0799188, acc 0.94
2016-09-06T09:27:19.029765: step 2543, loss 0.0703616, acc 0.98
2016-09-06T09:27:19.879681: step 2544, loss 0.0373785, acc 0.98
2016-09-06T09:27:20.699480: step 2545, loss 0.0354677, acc 0.98
2016-09-06T09:27:21.488031: step 2546, loss 0.0051721, acc 1
2016-09-06T09:27:22.298423: step 2547, loss 0.0342052, acc 0.98
2016-09-06T09:27:23.069215: step 2548, loss 0.0712219, acc 0.98
2016-09-06T09:27:23.889642: step 2549, loss 0.0387037, acc 1
2016-09-06T09:27:24.696285: step 2550, loss 0.0420459, acc 0.96
2016-09-06T09:27:25.465943: step 2551, loss 0.0900721, acc 0.96
2016-09-06T09:27:26.266732: step 2552, loss 0.0209011, acc 1
2016-09-06T09:27:27.088662: step 2553, loss 0.0151338, acc 1
2016-09-06T09:27:27.880409: step 2554, loss 0.0478234, acc 0.98
2016-09-06T09:27:28.680439: step 2555, loss 0.0265629, acc 0.98
2016-09-06T09:27:29.488412: step 2556, loss 0.0421342, acc 0.98
2016-09-06T09:27:30.293747: step 2557, loss 0.0530062, acc 0.98
2016-09-06T09:27:31.097049: step 2558, loss 0.0293541, acc 0.98
2016-09-06T09:27:31.916603: step 2559, loss 0.00581813, acc 1
2016-09-06T09:27:32.693565: step 2560, loss 0.0322937, acc 0.98
2016-09-06T09:27:33.517550: step 2561, loss 0.0525943, acc 0.96
2016-09-06T09:27:34.345290: step 2562, loss 0.0905187, acc 0.94
2016-09-06T09:27:35.146665: step 2563, loss 0.0573535, acc 1
2016-09-06T09:27:35.974805: step 2564, loss 0.0301724, acc 1
2016-09-06T09:27:36.790841: step 2565, loss 0.0183148, acc 1
2016-09-06T09:27:37.561929: step 2566, loss 0.0722271, acc 0.94
2016-09-06T09:27:38.354477: step 2567, loss 0.0137079, acc 1
2016-09-06T09:27:39.157192: step 2568, loss 0.029303, acc 0.98
2016-09-06T09:27:39.955240: step 2569, loss 0.0268414, acc 1
2016-09-06T09:27:40.768564: step 2570, loss 0.00856785, acc 1
2016-09-06T09:27:41.605512: step 2571, loss 0.043511, acc 0.98
2016-09-06T09:27:42.403399: step 2572, loss 0.0158238, acc 1
2016-09-06T09:27:43.212329: step 2573, loss 0.0168826, acc 1
2016-09-06T09:27:44.029426: step 2574, loss 0.052128, acc 0.98
2016-09-06T09:27:44.817013: step 2575, loss 0.0329282, acc 0.98
2016-09-06T09:27:45.606365: step 2576, loss 0.065553, acc 0.98
2016-09-06T09:27:46.434197: step 2577, loss 0.0245593, acc 0.98
2016-09-06T09:27:47.233273: step 2578, loss 0.024989, acc 0.98
2016-09-06T09:27:48.031153: step 2579, loss 0.0405853, acc 1
2016-09-06T09:27:48.871853: step 2580, loss 0.0115844, acc 1
2016-09-06T09:27:49.662832: step 2581, loss 0.0340022, acc 0.98
2016-09-06T09:27:50.447498: step 2582, loss 0.0477307, acc 0.98
2016-09-06T09:27:51.250498: step 2583, loss 0.00403966, acc 1
2016-09-06T09:27:52.046236: step 2584, loss 0.0493083, acc 0.98
2016-09-06T09:27:52.841959: step 2585, loss 0.101872, acc 0.96
2016-09-06T09:27:53.640542: step 2586, loss 0.0314296, acc 1
2016-09-06T09:27:54.431102: step 2587, loss 0.0484047, acc 0.98
2016-09-06T09:27:55.238946: step 2588, loss 0.0400344, acc 0.98
2016-09-06T09:27:56.064849: step 2589, loss 0.0450506, acc 0.98
2016-09-06T09:27:56.876249: step 2590, loss 0.0182469, acc 1
2016-09-06T09:27:57.666754: step 2591, loss 0.0112853, acc 1
2016-09-06T09:27:58.481782: step 2592, loss 0.0783161, acc 0.96
2016-09-06T09:27:59.289882: step 2593, loss 0.0654505, acc 0.98
2016-09-06T09:28:00.113776: step 2594, loss 0.0221736, acc 1
2016-09-06T09:28:00.959881: step 2595, loss 0.00907699, acc 1
2016-09-06T09:28:01.728941: step 2596, loss 0.0237674, acc 1
2016-09-06T09:28:02.535350: step 2597, loss 0.0186902, acc 1
2016-09-06T09:28:03.375602: step 2598, loss 0.00572837, acc 1
2016-09-06T09:28:04.156276: step 2599, loss 0.024587, acc 0.98
2016-09-06T09:28:04.955423: step 2600, loss 0.0224669, acc 1

Evaluation:
2016-09-06T09:28:08.696333: step 2600, loss 1.43639, acc 0.76454

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-2600

2016-09-06T09:28:10.687287: step 2601, loss 0.121395, acc 0.94
2016-09-06T09:28:11.488665: step 2602, loss 0.0184762, acc 1
2016-09-06T09:28:12.289998: step 2603, loss 0.0641228, acc 0.94
2016-09-06T09:28:13.113151: step 2604, loss 0.0888153, acc 0.98
2016-09-06T09:28:13.922033: step 2605, loss 0.044001, acc 1
2016-09-06T09:28:14.703728: step 2606, loss 0.0859342, acc 0.94
2016-09-06T09:28:15.535197: step 2607, loss 0.13415, acc 0.94
2016-09-06T09:28:16.348178: step 2608, loss 0.0522323, acc 0.96
2016-09-06T09:28:17.160177: step 2609, loss 0.0282642, acc 1
2016-09-06T09:28:17.992512: step 2610, loss 0.0302481, acc 0.98
2016-09-06T09:28:18.744889: step 2611, loss 0.0762326, acc 0.94
2016-09-06T09:28:19.566054: step 2612, loss 0.022281, acc 1
2016-09-06T09:28:20.373785: step 2613, loss 0.0696435, acc 0.98
2016-09-06T09:28:21.144040: step 2614, loss 0.0102859, acc 1
2016-09-06T09:28:21.984154: step 2615, loss 0.0343297, acc 0.98
2016-09-06T09:28:22.826038: step 2616, loss 0.114508, acc 0.96
2016-09-06T09:28:23.595807: step 2617, loss 0.00472022, acc 1
2016-09-06T09:28:24.405413: step 2618, loss 0.106482, acc 0.98
2016-09-06T09:28:25.226565: step 2619, loss 0.0314046, acc 0.98
2016-09-06T09:28:26.010785: step 2620, loss 0.0332633, acc 1
2016-09-06T09:28:26.811098: step 2621, loss 0.0601258, acc 0.98
2016-09-06T09:28:27.625161: step 2622, loss 0.0235854, acc 0.98
2016-09-06T09:28:28.389532: step 2623, loss 0.088869, acc 0.98
2016-09-06T09:28:29.205695: step 2624, loss 0.0738257, acc 0.96
2016-09-06T09:28:30.025579: step 2625, loss 0.136703, acc 0.94
2016-09-06T09:28:30.799274: step 2626, loss 0.025849, acc 0.98
2016-09-06T09:28:31.618912: step 2627, loss 0.00806323, acc 1
2016-09-06T09:28:32.470704: step 2628, loss 0.0351062, acc 1
2016-09-06T09:28:33.254346: step 2629, loss 0.0565384, acc 0.98
2016-09-06T09:28:34.069709: step 2630, loss 0.0661777, acc 0.94
2016-09-06T09:28:34.896876: step 2631, loss 0.0339947, acc 0.98
2016-09-06T09:28:35.687965: step 2632, loss 0.0165913, acc 1
2016-09-06T09:28:36.472003: step 2633, loss 0.101275, acc 0.96
2016-09-06T09:28:37.271406: step 2634, loss 0.0218008, acc 1
2016-09-06T09:28:38.075695: step 2635, loss 0.0181642, acc 1
2016-09-06T09:28:38.894691: step 2636, loss 0.0338506, acc 0.98
2016-09-06T09:28:39.711335: step 2637, loss 0.112824, acc 0.98
2016-09-06T09:28:40.515794: step 2638, loss 0.0161957, acc 1
2016-09-06T09:28:41.305363: step 2639, loss 0.0303844, acc 1
2016-09-06T09:28:42.097194: step 2640, loss 0.0329002, acc 0.98
2016-09-06T09:28:42.901916: step 2641, loss 0.0911632, acc 0.96
2016-09-06T09:28:43.724661: step 2642, loss 0.0217055, acc 1
2016-09-06T09:28:44.565083: step 2643, loss 0.090622, acc 0.96
2016-09-06T09:28:45.371217: step 2644, loss 0.0914357, acc 0.96
2016-09-06T09:28:46.185330: step 2645, loss 0.0429155, acc 0.96
2016-09-06T09:28:47.023994: step 2646, loss 0.0427664, acc 1
2016-09-06T09:28:47.816865: step 2647, loss 0.0180986, acc 1
2016-09-06T09:28:48.596397: step 2648, loss 0.105926, acc 0.96
2016-09-06T09:28:49.444657: step 2649, loss 0.0799658, acc 0.96
2016-09-06T09:28:50.285746: step 2650, loss 0.164586, acc 0.96
2016-09-06T09:28:51.120530: step 2651, loss 0.0673367, acc 0.94
2016-09-06T09:28:51.966547: step 2652, loss 0.096227, acc 0.96
2016-09-06T09:28:52.764269: step 2653, loss 0.0499179, acc 0.98
2016-09-06T09:28:53.563673: step 2654, loss 0.0593854, acc 0.98
2016-09-06T09:28:54.405594: step 2655, loss 0.0350447, acc 1
2016-09-06T09:28:55.229388: step 2656, loss 0.084505, acc 0.96
2016-09-06T09:28:56.039232: step 2657, loss 0.0611286, acc 0.98
2016-09-06T09:28:56.891972: step 2658, loss 0.0384537, acc 1
2016-09-06T09:28:57.698230: step 2659, loss 0.0718168, acc 0.94
2016-09-06T09:28:58.531673: step 2660, loss 0.0156577, acc 1
2016-09-06T09:28:59.395836: step 2661, loss 0.0230432, acc 0.98
2016-09-06T09:29:00.208512: step 2662, loss 0.0376529, acc 0.96
2016-09-06T09:29:01.024608: step 2663, loss 0.0327749, acc 0.98
2016-09-06T09:29:01.838759: step 2664, loss 0.056818, acc 0.98
2016-09-06T09:29:02.664817: step 2665, loss 0.0459674, acc 1
2016-09-06T09:29:03.456494: step 2666, loss 0.00983101, acc 1
2016-09-06T09:29:04.276069: step 2667, loss 0.0976952, acc 0.98
2016-09-06T09:29:05.089375: step 2668, loss 0.127147, acc 0.94
2016-09-06T09:29:05.909466: step 2669, loss 0.0157472, acc 1
2016-09-06T09:29:06.749412: step 2670, loss 0.0917105, acc 0.92
2016-09-06T09:29:07.554584: step 2671, loss 0.134076, acc 0.92
2016-09-06T09:29:08.334063: step 2672, loss 0.0598094, acc 0.98
2016-09-06T09:29:09.194653: step 2673, loss 0.0269104, acc 0.98
2016-09-06T09:29:10.035543: step 2674, loss 0.0862983, acc 0.96
2016-09-06T09:29:10.835431: step 2675, loss 0.093155, acc 0.94
2016-09-06T09:29:11.644239: step 2676, loss 0.0500763, acc 0.98
2016-09-06T09:29:12.457898: step 2677, loss 0.0481766, acc 0.96
2016-09-06T09:29:13.244716: step 2678, loss 0.071949, acc 0.94
2016-09-06T09:29:14.045136: step 2679, loss 0.0273476, acc 1
2016-09-06T09:29:14.859643: step 2680, loss 0.0320628, acc 1
2016-09-06T09:29:15.655649: step 2681, loss 0.0253119, acc 0.98
2016-09-06T09:29:16.458894: step 2682, loss 0.0762279, acc 0.96
2016-09-06T09:29:17.288228: step 2683, loss 0.0819935, acc 0.94
2016-09-06T09:29:18.152291: step 2684, loss 0.0243014, acc 0.98
2016-09-06T09:29:18.983015: step 2685, loss 0.0729052, acc 0.98
2016-09-06T09:29:19.789151: step 2686, loss 0.0229148, acc 0.98
2016-09-06T09:29:20.617873: step 2687, loss 0.0358374, acc 1
2016-09-06T09:29:21.389064: step 2688, loss 0.0792633, acc 0.977273
2016-09-06T09:29:22.211328: step 2689, loss 0.0634558, acc 0.96
2016-09-06T09:29:23.002830: step 2690, loss 0.155365, acc 0.98
2016-09-06T09:29:23.809385: step 2691, loss 0.0289719, acc 0.98
2016-09-06T09:29:24.641571: step 2692, loss 0.0626275, acc 0.94
2016-09-06T09:29:25.431091: step 2693, loss 0.0304863, acc 1
2016-09-06T09:29:26.233207: step 2694, loss 0.0174468, acc 1
2016-09-06T09:29:27.058513: step 2695, loss 0.168077, acc 0.92
2016-09-06T09:29:27.867686: step 2696, loss 0.0111655, acc 1
2016-09-06T09:29:28.656775: step 2697, loss 0.076107, acc 0.94
2016-09-06T09:29:29.495749: step 2698, loss 0.102832, acc 0.98
2016-09-06T09:29:30.305189: step 2699, loss 0.0468592, acc 0.98
2016-09-06T09:29:31.141482: step 2700, loss 0.0256118, acc 0.98

Evaluation:
2016-09-06T09:29:34.906309: step 2700, loss 1.40515, acc 0.762664

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-2700

2016-09-06T09:29:36.784145: step 2701, loss 0.0367937, acc 1
2016-09-06T09:29:37.565693: step 2702, loss 0.0298277, acc 1
2016-09-06T09:29:38.378667: step 2703, loss 0.0599245, acc 0.96
2016-09-06T09:29:39.201987: step 2704, loss 0.136569, acc 0.94
2016-09-06T09:29:39.996886: step 2705, loss 0.0153091, acc 1
2016-09-06T09:29:40.789965: step 2706, loss 0.0157525, acc 1
2016-09-06T09:29:41.607896: step 2707, loss 0.0200452, acc 1
2016-09-06T09:29:42.399664: step 2708, loss 0.0174463, acc 1
2016-09-06T09:29:43.203745: step 2709, loss 0.0129602, acc 1
2016-09-06T09:29:44.056849: step 2710, loss 0.00956454, acc 1
2016-09-06T09:29:44.836276: step 2711, loss 0.0204053, acc 0.98
2016-09-06T09:29:45.625702: step 2712, loss 0.00616801, acc 1
2016-09-06T09:29:46.431393: step 2713, loss 0.0353265, acc 0.98
2016-09-06T09:29:47.215224: step 2714, loss 0.0591472, acc 0.98
2016-09-06T09:29:48.031214: step 2715, loss 0.0268404, acc 0.98
2016-09-06T09:29:48.833447: step 2716, loss 0.01733, acc 1
2016-09-06T09:29:49.633815: step 2717, loss 0.0763338, acc 0.94
2016-09-06T09:29:50.437376: step 2718, loss 0.0352969, acc 0.98
2016-09-06T09:29:51.270354: step 2719, loss 0.0286686, acc 1
2016-09-06T09:29:52.077050: step 2720, loss 0.0427904, acc 0.98
2016-09-06T09:29:52.877631: step 2721, loss 0.0618912, acc 0.96
2016-09-06T09:29:53.716136: step 2722, loss 0.0583655, acc 0.94
2016-09-06T09:29:54.476485: step 2723, loss 0.0243043, acc 1
2016-09-06T09:29:55.272499: step 2724, loss 0.0120926, acc 1
2016-09-06T09:29:56.075998: step 2725, loss 0.0661202, acc 0.96
2016-09-06T09:29:56.889123: step 2726, loss 0.067107, acc 0.98
2016-09-06T09:29:57.692061: step 2727, loss 0.032294, acc 0.98
2016-09-06T09:29:58.514115: step 2728, loss 0.0157294, acc 1
2016-09-06T09:29:59.341652: step 2729, loss 0.0045066, acc 1
2016-09-06T09:30:00.145426: step 2730, loss 0.00951795, acc 1
2016-09-06T09:30:00.967487: step 2731, loss 0.101786, acc 0.94
2016-09-06T09:30:01.748322: step 2732, loss 0.0527237, acc 0.98
2016-09-06T09:30:02.575334: step 2733, loss 0.00861664, acc 1
2016-09-06T09:30:03.392931: step 2734, loss 0.086979, acc 0.94
2016-09-06T09:30:04.221527: step 2735, loss 0.0321508, acc 1
2016-09-06T09:30:05.032496: step 2736, loss 0.0352865, acc 1
2016-09-06T09:30:05.852993: step 2737, loss 0.0342212, acc 0.98
2016-09-06T09:30:06.656834: step 2738, loss 0.00502342, acc 1
2016-09-06T09:30:07.479636: step 2739, loss 0.134717, acc 0.98
2016-09-06T09:30:08.299817: step 2740, loss 0.0316144, acc 1
2016-09-06T09:30:09.124338: step 2741, loss 0.0864175, acc 0.98
2016-09-06T09:30:09.957182: step 2742, loss 0.0129033, acc 1
2016-09-06T09:30:10.822375: step 2743, loss 0.043943, acc 0.98
2016-09-06T09:30:11.619469: step 2744, loss 0.028828, acc 0.98
2016-09-06T09:30:12.455096: step 2745, loss 0.00825872, acc 1
2016-09-06T09:30:13.265618: step 2746, loss 0.0945966, acc 0.96
2016-09-06T09:30:14.071275: step 2747, loss 0.0425574, acc 0.98
2016-09-06T09:30:14.876064: step 2748, loss 0.0399608, acc 0.98
2016-09-06T09:30:15.707765: step 2749, loss 0.0706497, acc 0.96
2016-09-06T09:30:16.511484: step 2750, loss 0.0106332, acc 1
2016-09-06T09:30:17.319218: step 2751, loss 0.0258717, acc 1
2016-09-06T09:30:18.166761: step 2752, loss 0.0262804, acc 0.98
2016-09-06T09:30:19.011783: step 2753, loss 0.00621207, acc 1
2016-09-06T09:30:19.837682: step 2754, loss 0.052108, acc 0.98
2016-09-06T09:30:20.640414: step 2755, loss 0.013484, acc 1
2016-09-06T09:30:21.468495: step 2756, loss 0.0822299, acc 0.96
2016-09-06T09:30:22.262048: step 2757, loss 0.0391389, acc 1
2016-09-06T09:30:23.068237: step 2758, loss 0.0550755, acc 0.96
2016-09-06T09:30:23.884410: step 2759, loss 0.0341449, acc 0.98
2016-09-06T09:30:24.681517: step 2760, loss 0.0817383, acc 0.98
2016-09-06T09:30:25.500210: step 2761, loss 0.0773832, acc 0.96
2016-09-06T09:30:26.333432: step 2762, loss 0.0458287, acc 0.96
2016-09-06T09:30:27.165979: step 2763, loss 0.0363308, acc 0.98
2016-09-06T09:30:27.990029: step 2764, loss 0.00946486, acc 1
2016-09-06T09:30:28.813336: step 2765, loss 0.0208509, acc 1
2016-09-06T09:30:29.615277: step 2766, loss 0.164589, acc 0.96
2016-09-06T09:30:30.430004: step 2767, loss 0.0554103, acc 0.96
2016-09-06T09:30:31.252518: step 2768, loss 0.0508263, acc 0.98
2016-09-06T09:30:32.048112: step 2769, loss 0.00973347, acc 1
2016-09-06T09:30:32.861193: step 2770, loss 0.0743049, acc 0.98
2016-09-06T09:30:33.709730: step 2771, loss 0.0292303, acc 0.98
2016-09-06T09:30:34.541060: step 2772, loss 0.0067837, acc 1
2016-09-06T09:30:35.375644: step 2773, loss 0.06968, acc 0.98
2016-09-06T09:30:36.242422: step 2774, loss 0.0271795, acc 1
2016-09-06T09:30:37.071352: step 2775, loss 0.0237123, acc 0.98
2016-09-06T09:30:37.901734: step 2776, loss 0.0140419, acc 1
2016-09-06T09:30:38.749028: step 2777, loss 0.0969029, acc 0.94
2016-09-06T09:30:39.545441: step 2778, loss 0.0322285, acc 0.98
2016-09-06T09:30:40.345388: step 2779, loss 0.0495048, acc 0.98
2016-09-06T09:30:41.160581: step 2780, loss 0.0294183, acc 0.98
2016-09-06T09:30:41.979425: step 2781, loss 0.11332, acc 0.94
2016-09-06T09:30:42.793419: step 2782, loss 0.0145781, acc 1
2016-09-06T09:30:43.606945: step 2783, loss 0.0447306, acc 0.98
2016-09-06T09:30:44.432279: step 2784, loss 0.00846264, acc 1
2016-09-06T09:30:45.237280: step 2785, loss 0.0416365, acc 0.98
2016-09-06T09:30:46.031657: step 2786, loss 0.068448, acc 0.94
2016-09-06T09:30:46.843082: step 2787, loss 0.0482552, acc 0.98
2016-09-06T09:30:47.627782: step 2788, loss 0.0357115, acc 0.98
2016-09-06T09:30:48.432620: step 2789, loss 0.0355997, acc 0.98
2016-09-06T09:30:49.243926: step 2790, loss 0.0433839, acc 0.98
2016-09-06T09:30:50.023749: step 2791, loss 0.0393145, acc 0.98
2016-09-06T09:30:50.840223: step 2792, loss 0.0308462, acc 1
2016-09-06T09:30:51.657045: step 2793, loss 0.0246338, acc 0.98
2016-09-06T09:30:52.463511: step 2794, loss 0.244059, acc 0.94
2016-09-06T09:30:53.306711: step 2795, loss 0.0289259, acc 0.98
2016-09-06T09:30:54.108421: step 2796, loss 0.0326333, acc 1
2016-09-06T09:30:54.888780: step 2797, loss 0.0981278, acc 0.96
2016-09-06T09:30:55.735713: step 2798, loss 0.0574116, acc 0.96
2016-09-06T09:30:56.579570: step 2799, loss 0.0264308, acc 0.98
2016-09-06T09:30:57.411570: step 2800, loss 0.102089, acc 0.96

Evaluation:
2016-09-06T09:31:01.142835: step 2800, loss 1.19924, acc 0.760788

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-2800

2016-09-06T09:31:02.983523: step 2801, loss 0.0736596, acc 0.98
2016-09-06T09:31:03.803177: step 2802, loss 0.0641556, acc 0.96
2016-09-06T09:31:04.610020: step 2803, loss 0.0656609, acc 0.96
2016-09-06T09:31:05.441800: step 2804, loss 0.052534, acc 0.98
2016-09-06T09:31:06.281109: step 2805, loss 0.0651945, acc 0.98
2016-09-06T09:31:07.079168: step 2806, loss 0.0177891, acc 1
2016-09-06T09:31:07.887856: step 2807, loss 0.0387805, acc 0.98
2016-09-06T09:31:08.701217: step 2808, loss 0.044396, acc 0.96
2016-09-06T09:31:09.498220: step 2809, loss 0.0638076, acc 0.96
2016-09-06T09:31:10.268245: step 2810, loss 0.0432311, acc 0.96
2016-09-06T09:31:11.108119: step 2811, loss 0.0517759, acc 0.98
2016-09-06T09:31:11.892392: step 2812, loss 0.0102507, acc 1
2016-09-06T09:31:12.680066: step 2813, loss 0.0485876, acc 1
2016-09-06T09:31:13.498719: step 2814, loss 0.0138722, acc 1
2016-09-06T09:31:14.284260: step 2815, loss 0.0320655, acc 0.98
2016-09-06T09:31:15.103396: step 2816, loss 0.0441426, acc 0.98
2016-09-06T09:31:15.931237: step 2817, loss 0.0959294, acc 0.96
2016-09-06T09:31:16.734811: step 2818, loss 0.0375015, acc 0.98
2016-09-06T09:31:17.537473: step 2819, loss 0.0868229, acc 0.98
2016-09-06T09:31:18.376742: step 2820, loss 0.0196572, acc 1
2016-09-06T09:31:19.178396: step 2821, loss 0.0415393, acc 0.96
2016-09-06T09:31:19.997453: step 2822, loss 0.0258504, acc 1
2016-09-06T09:31:20.808949: step 2823, loss 0.0450389, acc 0.98
2016-09-06T09:31:21.603414: step 2824, loss 0.0150669, acc 1
2016-09-06T09:31:22.408107: step 2825, loss 0.0353977, acc 1
2016-09-06T09:31:23.248924: step 2826, loss 0.151446, acc 0.96
2016-09-06T09:31:24.056940: step 2827, loss 0.0182341, acc 0.98
2016-09-06T09:31:24.865196: step 2828, loss 0.0355182, acc 0.98
2016-09-06T09:31:25.712394: step 2829, loss 0.119957, acc 0.96
2016-09-06T09:31:26.521500: step 2830, loss 0.0326088, acc 1
2016-09-06T09:31:27.334874: step 2831, loss 0.141352, acc 0.94
2016-09-06T09:31:28.177798: step 2832, loss 0.120137, acc 0.9
2016-09-06T09:31:29.001507: step 2833, loss 0.17967, acc 0.96
2016-09-06T09:31:29.790694: step 2834, loss 0.0634346, acc 0.96
2016-09-06T09:31:30.620082: step 2835, loss 0.0474655, acc 0.98
2016-09-06T09:31:31.446471: step 2836, loss 0.0439434, acc 0.96
2016-09-06T09:31:32.257590: step 2837, loss 0.0556256, acc 0.98
2016-09-06T09:31:33.074730: step 2838, loss 0.0619117, acc 0.96
2016-09-06T09:31:33.885860: step 2839, loss 0.135617, acc 0.96
2016-09-06T09:31:34.720926: step 2840, loss 0.0250369, acc 1
2016-09-06T09:31:35.578356: step 2841, loss 0.0572385, acc 0.96
2016-09-06T09:31:36.397613: step 2842, loss 0.0183036, acc 1
2016-09-06T09:31:37.196217: step 2843, loss 0.0695109, acc 0.94
2016-09-06T09:31:38.035182: step 2844, loss 0.0514014, acc 0.96
2016-09-06T09:31:38.860348: step 2845, loss 0.0424698, acc 1
2016-09-06T09:31:39.646273: step 2846, loss 0.070602, acc 0.98
2016-09-06T09:31:40.452861: step 2847, loss 0.0112712, acc 1
2016-09-06T09:31:41.264796: step 2848, loss 0.0750667, acc 0.96
2016-09-06T09:31:42.055550: step 2849, loss 0.151542, acc 0.92
2016-09-06T09:31:42.867257: step 2850, loss 0.0346135, acc 1
2016-09-06T09:31:43.695422: step 2851, loss 0.023259, acc 1
2016-09-06T09:31:44.521554: step 2852, loss 0.0262595, acc 1
2016-09-06T09:31:45.317426: step 2853, loss 0.0808197, acc 0.96
2016-09-06T09:31:46.148519: step 2854, loss 0.0297062, acc 1
2016-09-06T09:31:46.934257: step 2855, loss 0.0316237, acc 1
2016-09-06T09:31:47.738711: step 2856, loss 0.0296836, acc 1
2016-09-06T09:31:48.528790: step 2857, loss 0.0561354, acc 0.98
2016-09-06T09:31:49.324092: step 2858, loss 0.0735328, acc 0.98
2016-09-06T09:31:50.122090: step 2859, loss 0.0962329, acc 0.94
2016-09-06T09:31:50.926501: step 2860, loss 0.0385438, acc 0.98
2016-09-06T09:31:51.718873: step 2861, loss 0.0450647, acc 0.98
2016-09-06T09:31:52.521778: step 2862, loss 0.0411574, acc 1
2016-09-06T09:31:53.342680: step 2863, loss 0.0615997, acc 0.94
2016-09-06T09:31:54.130920: step 2864, loss 0.0857804, acc 0.96
2016-09-06T09:31:54.939291: step 2865, loss 0.0378007, acc 0.98
2016-09-06T09:31:55.755234: step 2866, loss 0.124805, acc 0.96
2016-09-06T09:31:56.566434: step 2867, loss 0.125124, acc 0.94
2016-09-06T09:31:57.367684: step 2868, loss 0.0302857, acc 1
2016-09-06T09:31:58.197941: step 2869, loss 0.0130583, acc 1
2016-09-06T09:31:58.980252: step 2870, loss 0.0326866, acc 1
2016-09-06T09:31:59.797085: step 2871, loss 0.0226734, acc 1
2016-09-06T09:32:00.663308: step 2872, loss 0.0794478, acc 0.98
2016-09-06T09:32:01.466572: step 2873, loss 0.0425469, acc 0.98
2016-09-06T09:32:02.277994: step 2874, loss 0.0194434, acc 0.98
2016-09-06T09:32:03.111381: step 2875, loss 0.130902, acc 0.94
2016-09-06T09:32:03.912396: step 2876, loss 0.136517, acc 0.94
2016-09-06T09:32:04.715983: step 2877, loss 0.0633831, acc 0.98
2016-09-06T09:32:05.526623: step 2878, loss 0.031645, acc 0.98
2016-09-06T09:32:06.323923: step 2879, loss 0.0332549, acc 0.98
2016-09-06T09:32:07.079829: step 2880, loss 0.0307864, acc 1
2016-09-06T09:32:07.922452: step 2881, loss 0.033557, acc 0.98
2016-09-06T09:32:08.746615: step 2882, loss 0.106277, acc 0.94
2016-09-06T09:32:09.566984: step 2883, loss 0.0268966, acc 0.98
2016-09-06T09:32:10.395317: step 2884, loss 0.0240399, acc 1
2016-09-06T09:32:11.196059: step 2885, loss 0.0274273, acc 0.98
2016-09-06T09:32:12.033886: step 2886, loss 0.0393421, acc 0.96
2016-09-06T09:32:12.854210: step 2887, loss 0.0176927, acc 1
2016-09-06T09:32:13.687903: step 2888, loss 0.092818, acc 0.98
2016-09-06T09:32:14.493613: step 2889, loss 0.0559685, acc 0.98
2016-09-06T09:32:15.345399: step 2890, loss 0.170137, acc 0.96
2016-09-06T09:32:16.153409: step 2891, loss 0.00842776, acc 1
2016-09-06T09:32:16.942533: step 2892, loss 0.0203589, acc 1
2016-09-06T09:32:17.789847: step 2893, loss 0.117434, acc 0.96
2016-09-06T09:32:18.624800: step 2894, loss 0.07513, acc 0.94
2016-09-06T09:32:19.437441: step 2895, loss 0.101353, acc 0.94
2016-09-06T09:32:20.278197: step 2896, loss 0.0267222, acc 1
2016-09-06T09:32:21.102789: step 2897, loss 0.0407257, acc 0.98
2016-09-06T09:32:21.895750: step 2898, loss 0.0295211, acc 0.98
2016-09-06T09:32:22.717883: step 2899, loss 0.0564974, acc 0.98
2016-09-06T09:32:23.521366: step 2900, loss 0.0935111, acc 0.96

Evaluation:
2016-09-06T09:32:27.279735: step 2900, loss 1.388, acc 0.757974

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-2900

2016-09-06T09:32:29.274849: step 2901, loss 0.0119551, acc 1
2016-09-06T09:32:30.097830: step 2902, loss 0.00876272, acc 1
2016-09-06T09:32:30.883917: step 2903, loss 0.059374, acc 0.96
2016-09-06T09:32:31.691133: step 2904, loss 0.0573628, acc 0.96
2016-09-06T09:32:32.532443: step 2905, loss 0.052298, acc 0.96
2016-09-06T09:32:33.335348: step 2906, loss 0.0782115, acc 0.96
2016-09-06T09:32:34.134215: step 2907, loss 0.010986, acc 1
2016-09-06T09:32:34.957339: step 2908, loss 0.0212479, acc 1
2016-09-06T09:32:35.763530: step 2909, loss 0.0425699, acc 0.98
2016-09-06T09:32:36.577432: step 2910, loss 0.0583805, acc 0.96
2016-09-06T09:32:37.376355: step 2911, loss 0.0141667, acc 1
2016-09-06T09:32:38.201484: step 2912, loss 0.0353675, acc 0.98
2016-09-06T09:32:38.969465: step 2913, loss 0.088938, acc 0.94
2016-09-06T09:32:39.785332: step 2914, loss 0.0268166, acc 1
2016-09-06T09:32:40.626145: step 2915, loss 0.00733065, acc 1
2016-09-06T09:32:41.419630: step 2916, loss 0.00868977, acc 1
2016-09-06T09:32:42.226095: step 2917, loss 0.0215402, acc 1
2016-09-06T09:32:43.029884: step 2918, loss 0.0621357, acc 0.98
2016-09-06T09:32:43.824346: step 2919, loss 0.0362938, acc 0.98
2016-09-06T09:32:44.640538: step 2920, loss 0.0582095, acc 0.94
2016-09-06T09:32:45.463656: step 2921, loss 0.0452682, acc 0.98
2016-09-06T09:32:46.291855: step 2922, loss 0.0573379, acc 0.96
2016-09-06T09:32:47.123289: step 2923, loss 0.0267048, acc 1
2016-09-06T09:32:47.928622: step 2924, loss 0.0238316, acc 0.98
2016-09-06T09:32:48.741525: step 2925, loss 0.0399067, acc 0.98
2016-09-06T09:32:49.540172: step 2926, loss 0.0592487, acc 0.96
2016-09-06T09:32:50.362588: step 2927, loss 0.0788077, acc 0.98
2016-09-06T09:32:51.127249: step 2928, loss 0.0629433, acc 0.98
2016-09-06T09:32:51.932021: step 2929, loss 0.029589, acc 0.98
2016-09-06T09:32:52.736595: step 2930, loss 0.0100869, acc 1
2016-09-06T09:32:53.524901: step 2931, loss 0.0346154, acc 0.98
2016-09-06T09:32:54.332259: step 2932, loss 0.121085, acc 0.92
2016-09-06T09:32:55.154892: step 2933, loss 0.0497335, acc 1
2016-09-06T09:32:55.944982: step 2934, loss 0.102038, acc 0.96
2016-09-06T09:32:56.744371: step 2935, loss 0.0697698, acc 0.96
2016-09-06T09:32:57.574320: step 2936, loss 0.0633146, acc 0.96
2016-09-06T09:32:58.366402: step 2937, loss 0.0194046, acc 1
2016-09-06T09:32:59.167513: step 2938, loss 0.0431953, acc 0.96
2016-09-06T09:32:59.989139: step 2939, loss 0.0349806, acc 0.98
2016-09-06T09:33:00.800814: step 2940, loss 0.033964, acc 0.98
2016-09-06T09:33:01.607366: step 2941, loss 0.0431545, acc 0.98
2016-09-06T09:33:02.443864: step 2942, loss 0.059971, acc 0.96
2016-09-06T09:33:03.212608: step 2943, loss 0.0178435, acc 0.98
2016-09-06T09:33:04.049253: step 2944, loss 0.0845471, acc 0.94
2016-09-06T09:33:04.865523: step 2945, loss 0.074559, acc 0.98
2016-09-06T09:33:05.675837: step 2946, loss 0.0412707, acc 0.98
2016-09-06T09:33:06.494107: step 2947, loss 0.0442214, acc 0.98
2016-09-06T09:33:07.321501: step 2948, loss 0.0362053, acc 0.98
2016-09-06T09:33:08.119535: step 2949, loss 0.0676522, acc 1
2016-09-06T09:33:08.916175: step 2950, loss 0.0451849, acc 0.98
2016-09-06T09:33:09.727336: step 2951, loss 0.0380684, acc 0.98
2016-09-06T09:33:10.531561: step 2952, loss 0.101665, acc 0.98
2016-09-06T09:33:11.350308: step 2953, loss 0.0283072, acc 0.98
2016-09-06T09:33:12.157233: step 2954, loss 0.00841757, acc 1
2016-09-06T09:33:12.955827: step 2955, loss 0.0675181, acc 0.96
2016-09-06T09:33:13.755538: step 2956, loss 0.069282, acc 0.98
2016-09-06T09:33:14.574088: step 2957, loss 0.0102376, acc 1
2016-09-06T09:33:15.385619: step 2958, loss 0.0351823, acc 0.98
2016-09-06T09:33:16.205678: step 2959, loss 0.0551583, acc 0.94
2016-09-06T09:33:17.062969: step 2960, loss 0.0134835, acc 1
2016-09-06T09:33:17.870832: step 2961, loss 0.0301326, acc 0.98
2016-09-06T09:33:18.710576: step 2962, loss 0.0188412, acc 1
2016-09-06T09:33:19.564360: step 2963, loss 0.0633096, acc 0.98
2016-09-06T09:33:20.344469: step 2964, loss 0.0492485, acc 0.96
2016-09-06T09:33:21.157211: step 2965, loss 0.120474, acc 0.92
2016-09-06T09:33:21.965989: step 2966, loss 0.068553, acc 0.96
2016-09-06T09:33:22.755872: step 2967, loss 0.0223405, acc 0.98
2016-09-06T09:33:23.562888: step 2968, loss 0.0342077, acc 1
2016-09-06T09:33:24.388720: step 2969, loss 0.0544128, acc 0.96
2016-09-06T09:33:25.219295: step 2970, loss 0.0855841, acc 0.98
2016-09-06T09:33:26.022152: step 2971, loss 0.0627654, acc 0.94
2016-09-06T09:33:26.830658: step 2972, loss 0.00733525, acc 1
2016-09-06T09:33:27.650412: step 2973, loss 0.0773598, acc 0.96
2016-09-06T09:33:28.464058: step 2974, loss 0.0126382, acc 1
2016-09-06T09:33:29.293442: step 2975, loss 0.0235971, acc 0.98
2016-09-06T09:33:30.097055: step 2976, loss 0.0225557, acc 0.98
2016-09-06T09:33:30.906343: step 2977, loss 0.0786521, acc 0.98
2016-09-06T09:33:31.767007: step 2978, loss 0.0588962, acc 0.96
2016-09-06T09:33:32.573227: step 2979, loss 0.0263176, acc 0.98
2016-09-06T09:33:33.343924: step 2980, loss 0.0547951, acc 0.98
2016-09-06T09:33:34.167221: step 2981, loss 0.0157339, acc 1
2016-09-06T09:33:34.972624: step 2982, loss 0.0289909, acc 0.98
2016-09-06T09:33:35.786889: step 2983, loss 0.0401941, acc 0.96
2016-09-06T09:33:36.615001: step 2984, loss 0.0338013, acc 1
2016-09-06T09:33:37.449735: step 2985, loss 0.0480931, acc 0.98
2016-09-06T09:33:38.282684: step 2986, loss 0.0808335, acc 0.98
2016-09-06T09:33:39.126470: step 2987, loss 0.04857, acc 0.96
2016-09-06T09:33:39.946202: step 2988, loss 0.0411483, acc 0.98
2016-09-06T09:33:40.755633: step 2989, loss 0.0259418, acc 1
2016-09-06T09:33:41.589204: step 2990, loss 0.0776758, acc 0.98
2016-09-06T09:33:42.429478: step 2991, loss 0.0158306, acc 1
2016-09-06T09:33:43.232750: step 2992, loss 0.0264179, acc 0.98
2016-09-06T09:33:44.039981: step 2993, loss 0.0188842, acc 1
2016-09-06T09:33:44.833545: step 2994, loss 0.0357031, acc 0.98
2016-09-06T09:33:45.670749: step 2995, loss 0.102451, acc 0.96
2016-09-06T09:33:46.479326: step 2996, loss 0.0126678, acc 1
2016-09-06T09:33:47.307220: step 2997, loss 0.119642, acc 0.96
2016-09-06T09:33:48.111496: step 2998, loss 0.0325819, acc 0.98
2016-09-06T09:33:48.912345: step 2999, loss 0.0262537, acc 1
2016-09-06T09:33:49.750468: step 3000, loss 0.0346708, acc 0.98

Evaluation:
2016-09-06T09:33:53.498874: step 3000, loss 1.46337, acc 0.767355

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-3000

2016-09-06T09:33:55.398468: step 3001, loss 0.0110518, acc 1
2016-09-06T09:33:56.211273: step 3002, loss 0.0819488, acc 0.98
2016-09-06T09:33:57.043624: step 3003, loss 0.0294445, acc 0.98
2016-09-06T09:33:57.880323: step 3004, loss 0.0462635, acc 1
2016-09-06T09:33:58.694130: step 3005, loss 0.0724691, acc 0.94
2016-09-06T09:33:59.518714: step 3006, loss 0.0219072, acc 1
2016-09-06T09:34:00.346169: step 3007, loss 0.0698361, acc 0.94
2016-09-06T09:34:01.151136: step 3008, loss 0.0190511, acc 0.98
2016-09-06T09:34:01.984589: step 3009, loss 0.0725253, acc 0.96
2016-09-06T09:34:02.784070: step 3010, loss 0.159374, acc 0.96
2016-09-06T09:34:03.630048: step 3011, loss 0.0337207, acc 0.98
2016-09-06T09:34:04.500363: step 3012, loss 0.0233196, acc 1
2016-09-06T09:34:05.317460: step 3013, loss 0.0738428, acc 0.98
2016-09-06T09:34:06.118002: step 3014, loss 0.0509114, acc 0.96
2016-09-06T09:34:06.921403: step 3015, loss 0.0607945, acc 0.96
2016-09-06T09:34:07.741280: step 3016, loss 0.024893, acc 1
2016-09-06T09:34:08.513008: step 3017, loss 0.0487126, acc 0.96
2016-09-06T09:34:09.317886: step 3018, loss 0.0707667, acc 0.96
2016-09-06T09:34:10.158879: step 3019, loss 0.0607383, acc 0.98
2016-09-06T09:34:10.940796: step 3020, loss 0.0074114, acc 1
2016-09-06T09:34:11.757658: step 3021, loss 0.0907362, acc 0.94
2016-09-06T09:34:12.584738: step 3022, loss 0.00830391, acc 1
2016-09-06T09:34:13.355330: step 3023, loss 0.00343494, acc 1
2016-09-06T09:34:14.138087: step 3024, loss 0.0404562, acc 0.98
2016-09-06T09:34:14.960072: step 3025, loss 0.0532258, acc 0.96
2016-09-06T09:34:15.757772: step 3026, loss 0.0252487, acc 0.98
2016-09-06T09:34:16.555388: step 3027, loss 0.0368063, acc 0.98
2016-09-06T09:34:17.376066: step 3028, loss 0.0688257, acc 0.98
2016-09-06T09:34:18.227355: step 3029, loss 0.0126933, acc 1
2016-09-06T09:34:19.034442: step 3030, loss 0.0195641, acc 0.98
2016-09-06T09:34:19.866978: step 3031, loss 0.0732065, acc 0.94
2016-09-06T09:34:20.667858: step 3032, loss 0.0262136, acc 0.98
2016-09-06T09:34:21.472561: step 3033, loss 0.0566585, acc 0.96
2016-09-06T09:34:22.300185: step 3034, loss 0.0822617, acc 0.96
2016-09-06T09:34:23.091997: step 3035, loss 0.0953769, acc 0.94
2016-09-06T09:34:23.899614: step 3036, loss 0.0638746, acc 0.96
2016-09-06T09:34:24.709067: step 3037, loss 0.00882894, acc 1
2016-09-06T09:34:25.491853: step 3038, loss 0.014832, acc 1
2016-09-06T09:34:26.304422: step 3039, loss 0.0216543, acc 0.98
2016-09-06T09:34:27.107401: step 3040, loss 0.147838, acc 0.96
2016-09-06T09:34:27.884308: step 3041, loss 0.0678468, acc 0.96
2016-09-06T09:34:28.673844: step 3042, loss 0.0250138, acc 0.98
2016-09-06T09:34:29.491331: step 3043, loss 0.00730133, acc 1
2016-09-06T09:34:30.285449: step 3044, loss 0.0862153, acc 0.96
2016-09-06T09:34:31.110621: step 3045, loss 0.00469276, acc 1
2016-09-06T09:34:31.939956: step 3046, loss 0.0251807, acc 0.98
2016-09-06T09:34:32.739390: step 3047, loss 0.0308375, acc 0.98
2016-09-06T09:34:33.556531: step 3048, loss 0.00908782, acc 1
2016-09-06T09:34:34.379984: step 3049, loss 0.066788, acc 0.94
2016-09-06T09:34:35.172414: step 3050, loss 0.0491554, acc 0.98
2016-09-06T09:34:35.985529: step 3051, loss 0.0445822, acc 0.98
2016-09-06T09:34:36.859953: step 3052, loss 0.0272041, acc 1
2016-09-06T09:34:37.703526: step 3053, loss 0.111423, acc 0.96
2016-09-06T09:34:38.528723: step 3054, loss 0.0325421, acc 0.98
2016-09-06T09:34:39.374635: step 3055, loss 0.106553, acc 0.96
2016-09-06T09:34:40.180290: step 3056, loss 0.109951, acc 0.98
2016-09-06T09:34:40.991443: step 3057, loss 0.0226076, acc 1
2016-09-06T09:34:41.826886: step 3058, loss 0.0728464, acc 0.96
2016-09-06T09:34:42.640741: step 3059, loss 0.0142247, acc 1
2016-09-06T09:34:43.459256: step 3060, loss 0.123182, acc 0.94
2016-09-06T09:34:44.308261: step 3061, loss 0.0174199, acc 1
2016-09-06T09:34:45.168897: step 3062, loss 0.08301, acc 0.94
2016-09-06T09:34:45.966584: step 3063, loss 0.0371402, acc 1
2016-09-06T09:34:46.783455: step 3064, loss 0.0490451, acc 0.98
2016-09-06T09:34:47.582252: step 3065, loss 0.0302266, acc 0.98
2016-09-06T09:34:48.386870: step 3066, loss 0.060356, acc 0.98
2016-09-06T09:34:49.191182: step 3067, loss 0.0382906, acc 1
2016-09-06T09:34:50.033558: step 3068, loss 0.0317365, acc 1
2016-09-06T09:34:50.811963: step 3069, loss 0.0589461, acc 0.98
2016-09-06T09:34:51.618508: step 3070, loss 0.101721, acc 0.96
2016-09-06T09:34:52.415781: step 3071, loss 0.0679569, acc 0.98
2016-09-06T09:34:53.159410: step 3072, loss 0.0107034, acc 1
2016-09-06T09:34:53.976853: step 3073, loss 0.0366766, acc 1
2016-09-06T09:34:54.816059: step 3074, loss 0.113425, acc 0.94
2016-09-06T09:34:55.599550: step 3075, loss 0.0416662, acc 0.98
2016-09-06T09:34:56.414759: step 3076, loss 0.0389241, acc 1
2016-09-06T09:34:57.236349: step 3077, loss 0.0469699, acc 0.96
2016-09-06T09:34:58.029678: step 3078, loss 0.0335023, acc 1
2016-09-06T09:34:58.819749: step 3079, loss 0.0494425, acc 0.98
2016-09-06T09:34:59.639409: step 3080, loss 0.0164667, acc 1
2016-09-06T09:35:00.460302: step 3081, loss 0.041119, acc 0.98
2016-09-06T09:35:01.238651: step 3082, loss 0.019785, acc 0.98
2016-09-06T09:35:02.032878: step 3083, loss 0.00416287, acc 1
2016-09-06T09:35:02.835012: step 3084, loss 0.0313162, acc 0.98
2016-09-06T09:35:03.664896: step 3085, loss 0.0603197, acc 0.94
2016-09-06T09:35:04.509047: step 3086, loss 0.0214024, acc 1
2016-09-06T09:35:05.313740: step 3087, loss 0.0603192, acc 0.98
2016-09-06T09:35:06.114231: step 3088, loss 0.00329776, acc 1
2016-09-06T09:35:06.923683: step 3089, loss 0.0532995, acc 0.98
2016-09-06T09:35:07.758898: step 3090, loss 0.0141682, acc 1
2016-09-06T09:35:08.565918: step 3091, loss 0.165137, acc 0.96
2016-09-06T09:35:09.373066: step 3092, loss 0.0220017, acc 0.98
2016-09-06T09:35:10.150800: step 3093, loss 0.0626856, acc 0.96
2016-09-06T09:35:10.952716: step 3094, loss 0.0290625, acc 0.98
2016-09-06T09:35:11.781366: step 3095, loss 0.0229497, acc 1
2016-09-06T09:35:12.558569: step 3096, loss 0.0420645, acc 0.98
2016-09-06T09:35:13.394146: step 3097, loss 0.0316587, acc 0.98
2016-09-06T09:35:14.189404: step 3098, loss 0.11901, acc 0.92
2016-09-06T09:35:14.976228: step 3099, loss 0.0206857, acc 0.98
2016-09-06T09:35:15.780873: step 3100, loss 0.0226098, acc 1

Evaluation:
2016-09-06T09:35:19.517995: step 3100, loss 1.65507, acc 0.753283

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-3100

2016-09-06T09:35:21.388502: step 3101, loss 0.00721573, acc 1
2016-09-06T09:35:22.231625: step 3102, loss 0.062459, acc 0.96
2016-09-06T09:35:23.087234: step 3103, loss 0.138039, acc 0.98
2016-09-06T09:35:23.901185: step 3104, loss 0.0389359, acc 0.98
2016-09-06T09:35:24.703800: step 3105, loss 0.0278742, acc 1
2016-09-06T09:35:25.499373: step 3106, loss 0.0220038, acc 1
2016-09-06T09:35:26.303614: step 3107, loss 0.0284831, acc 1
2016-09-06T09:35:27.085596: step 3108, loss 0.0307294, acc 1
2016-09-06T09:35:27.885807: step 3109, loss 0.0371143, acc 0.98
2016-09-06T09:35:28.707582: step 3110, loss 0.0304725, acc 0.98
2016-09-06T09:35:29.458948: step 3111, loss 0.070517, acc 0.98
2016-09-06T09:35:30.270031: step 3112, loss 0.0160007, acc 1
2016-09-06T09:35:31.095726: step 3113, loss 0.0455058, acc 0.98
2016-09-06T09:35:31.887778: step 3114, loss 0.0195313, acc 0.98
2016-09-06T09:35:32.715472: step 3115, loss 0.128564, acc 0.98
2016-09-06T09:35:33.553783: step 3116, loss 0.103521, acc 0.94
2016-09-06T09:35:34.353414: step 3117, loss 0.0222198, acc 1
2016-09-06T09:35:35.129630: step 3118, loss 0.0302747, acc 1
2016-09-06T09:35:35.957068: step 3119, loss 0.0411099, acc 0.98
2016-09-06T09:35:36.752817: step 3120, loss 0.0321075, acc 1
2016-09-06T09:35:37.536355: step 3121, loss 0.0464906, acc 0.96
2016-09-06T09:35:38.389049: step 3122, loss 0.0123111, acc 1
2016-09-06T09:35:39.182569: step 3123, loss 0.0424862, acc 0.98
2016-09-06T09:35:39.979271: step 3124, loss 0.0541267, acc 0.98
2016-09-06T09:35:40.807568: step 3125, loss 0.0308823, acc 1
2016-09-06T09:35:41.637383: step 3126, loss 0.0549745, acc 0.96
2016-09-06T09:35:42.443982: step 3127, loss 0.0411687, acc 0.98
2016-09-06T09:35:43.284997: step 3128, loss 0.0254996, acc 1
2016-09-06T09:35:44.088525: step 3129, loss 0.0129855, acc 1
2016-09-06T09:35:44.898159: step 3130, loss 0.019364, acc 1
2016-09-06T09:35:45.721012: step 3131, loss 0.0553176, acc 0.98
2016-09-06T09:35:46.545891: step 3132, loss 0.0343127, acc 0.98
2016-09-06T09:35:47.352478: step 3133, loss 0.0697498, acc 0.98
2016-09-06T09:35:48.157705: step 3134, loss 0.0196988, acc 1
2016-09-06T09:35:48.964929: step 3135, loss 0.0278134, acc 1
2016-09-06T09:35:49.804541: step 3136, loss 0.00582249, acc 1
2016-09-06T09:35:50.634781: step 3137, loss 0.188944, acc 0.96
2016-09-06T09:35:51.477786: step 3138, loss 0.00939948, acc 1
2016-09-06T09:35:52.268155: step 3139, loss 0.00711384, acc 1
2016-09-06T09:35:53.096493: step 3140, loss 0.0427747, acc 0.98
2016-09-06T09:35:53.905848: step 3141, loss 0.0191887, acc 1
2016-09-06T09:35:54.898969: step 3142, loss 0.0469055, acc 0.98
2016-09-06T09:35:55.701445: step 3143, loss 0.109609, acc 0.96
2016-09-06T09:35:56.511803: step 3144, loss 0.0376792, acc 0.98
2016-09-06T09:35:57.322021: step 3145, loss 0.0451422, acc 0.98
2016-09-06T09:35:58.148886: step 3146, loss 0.109422, acc 0.94
2016-09-06T09:35:58.944077: step 3147, loss 0.0249084, acc 1
2016-09-06T09:35:59.739263: step 3148, loss 0.0175531, acc 1
2016-09-06T09:36:00.555191: step 3149, loss 0.0140857, acc 1
2016-09-06T09:36:01.404944: step 3150, loss 0.021447, acc 0.98
2016-09-06T09:36:02.217352: step 3151, loss 0.0172545, acc 1
2016-09-06T09:36:03.046188: step 3152, loss 0.0480397, acc 0.98
2016-09-06T09:36:03.868353: step 3153, loss 0.0175078, acc 1
2016-09-06T09:36:04.686507: step 3154, loss 0.0294659, acc 0.98
2016-09-06T09:36:05.529763: step 3155, loss 0.0292623, acc 0.98
2016-09-06T09:36:06.387024: step 3156, loss 0.044796, acc 0.98
2016-09-06T09:36:07.200465: step 3157, loss 0.0615558, acc 0.96
2016-09-06T09:36:08.025287: step 3158, loss 0.049604, acc 0.96
2016-09-06T09:36:08.855572: step 3159, loss 0.0312821, acc 0.98
2016-09-06T09:36:09.664626: step 3160, loss 0.132818, acc 0.94
2016-09-06T09:36:10.500291: step 3161, loss 0.0250716, acc 0.98
2016-09-06T09:36:11.338039: step 3162, loss 0.0121717, acc 1
2016-09-06T09:36:12.182914: step 3163, loss 0.0516885, acc 0.98
2016-09-06T09:36:12.961999: step 3164, loss 0.014158, acc 1
2016-09-06T09:36:13.774747: step 3165, loss 0.0230795, acc 1
2016-09-06T09:36:14.634851: step 3166, loss 0.0357042, acc 0.98
2016-09-06T09:36:15.471618: step 3167, loss 0.0221107, acc 1
2016-09-06T09:36:16.264166: step 3168, loss 0.0211841, acc 1
2016-09-06T09:36:17.098674: step 3169, loss 0.0406696, acc 1
2016-09-06T09:36:17.946580: step 3170, loss 0.0893462, acc 0.96
2016-09-06T09:36:18.753138: step 3171, loss 0.0312997, acc 0.98
2016-09-06T09:36:19.576705: step 3172, loss 0.0436375, acc 1
2016-09-06T09:36:20.370403: step 3173, loss 0.0956906, acc 0.96
2016-09-06T09:36:21.175725: step 3174, loss 0.0198014, acc 1
2016-09-06T09:36:22.011439: step 3175, loss 0.00793173, acc 1
2016-09-06T09:36:22.835726: step 3176, loss 0.0173921, acc 1
2016-09-06T09:36:23.644421: step 3177, loss 0.0589222, acc 0.96
2016-09-06T09:36:24.468509: step 3178, loss 0.0296278, acc 0.98
2016-09-06T09:36:25.269496: step 3179, loss 0.0275497, acc 1
2016-09-06T09:36:26.142100: step 3180, loss 0.036563, acc 0.98
2016-09-06T09:36:26.980883: step 3181, loss 0.0137349, acc 1
2016-09-06T09:36:27.826541: step 3182, loss 0.0998132, acc 0.94
2016-09-06T09:36:28.634929: step 3183, loss 0.0209315, acc 0.98
2016-09-06T09:36:29.447324: step 3184, loss 0.112467, acc 0.98
2016-09-06T09:36:30.255353: step 3185, loss 0.0104898, acc 1
2016-09-06T09:36:31.093121: step 3186, loss 0.103934, acc 0.96
2016-09-06T09:36:31.886238: step 3187, loss 0.0315228, acc 0.98
2016-09-06T09:36:32.707062: step 3188, loss 0.127776, acc 0.94
2016-09-06T09:36:33.534345: step 3189, loss 0.0138571, acc 1
2016-09-06T09:36:34.359880: step 3190, loss 0.00517853, acc 1
2016-09-06T09:36:35.179993: step 3191, loss 0.0334519, acc 1
2016-09-06T09:36:35.947303: step 3192, loss 0.017862, acc 1
2016-09-06T09:36:36.746548: step 3193, loss 0.0346969, acc 0.98
2016-09-06T09:36:37.548629: step 3194, loss 0.0222297, acc 1
2016-09-06T09:36:38.340974: step 3195, loss 0.00672724, acc 1
2016-09-06T09:36:39.126619: step 3196, loss 0.0356875, acc 1
2016-09-06T09:36:39.943808: step 3197, loss 0.0492275, acc 0.96
2016-09-06T09:36:40.776967: step 3198, loss 0.0458792, acc 0.98
2016-09-06T09:36:41.612187: step 3199, loss 0.0346686, acc 0.98
2016-09-06T09:36:42.435158: step 3200, loss 0.0775603, acc 0.98

Evaluation:
2016-09-06T09:36:46.196641: step 3200, loss 1.4114, acc 0.767355

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-3200

2016-09-06T09:36:48.096774: step 3201, loss 0.0350378, acc 0.98
2016-09-06T09:36:48.885109: step 3202, loss 0.0395752, acc 0.98
2016-09-06T09:36:49.668625: step 3203, loss 0.0284408, acc 0.98
2016-09-06T09:36:50.460928: step 3204, loss 0.0102236, acc 1
2016-09-06T09:36:51.280580: step 3205, loss 0.034847, acc 0.98
2016-09-06T09:36:52.113764: step 3206, loss 0.0265046, acc 1
2016-09-06T09:36:52.894371: step 3207, loss 0.0891402, acc 0.96
2016-09-06T09:36:53.690776: step 3208, loss 0.0173786, acc 1
2016-09-06T09:36:54.521422: step 3209, loss 0.0485395, acc 0.98
2016-09-06T09:36:55.362751: step 3210, loss 0.0809263, acc 0.96
2016-09-06T09:36:56.179999: step 3211, loss 0.0444608, acc 0.98
2016-09-06T09:36:56.986155: step 3212, loss 0.0357648, acc 0.98
2016-09-06T09:36:57.798082: step 3213, loss 0.0737302, acc 0.96
2016-09-06T09:36:58.590184: step 3214, loss 0.0732789, acc 0.96
2016-09-06T09:36:59.422530: step 3215, loss 0.0263743, acc 1
2016-09-06T09:37:00.277183: step 3216, loss 0.109377, acc 0.96
2016-09-06T09:37:01.067024: step 3217, loss 0.0806986, acc 0.98
2016-09-06T09:37:01.909859: step 3218, loss 0.124761, acc 0.98
2016-09-06T09:37:02.752225: step 3219, loss 0.0420908, acc 0.96
2016-09-06T09:37:03.593645: step 3220, loss 0.0146422, acc 1
2016-09-06T09:37:04.413271: step 3221, loss 0.0329776, acc 0.98
2016-09-06T09:37:05.226071: step 3222, loss 0.0490648, acc 1
2016-09-06T09:37:06.049760: step 3223, loss 0.00341922, acc 1
2016-09-06T09:37:06.868672: step 3224, loss 0.00339473, acc 1
2016-09-06T09:37:07.683687: step 3225, loss 0.0903248, acc 0.98
2016-09-06T09:37:08.490787: step 3226, loss 0.0743847, acc 0.98
2016-09-06T09:37:09.349786: step 3227, loss 0.0219014, acc 1
2016-09-06T09:37:10.200074: step 3228, loss 0.0664508, acc 0.98
2016-09-06T09:37:11.013307: step 3229, loss 0.0624252, acc 0.96
2016-09-06T09:37:11.860471: step 3230, loss 0.0726794, acc 0.96
2016-09-06T09:37:12.669251: step 3231, loss 0.0367517, acc 0.98
2016-09-06T09:37:13.442319: step 3232, loss 0.0236937, acc 0.98
2016-09-06T09:37:14.241214: step 3233, loss 0.029543, acc 1
2016-09-06T09:37:15.076989: step 3234, loss 0.0334661, acc 1
2016-09-06T09:37:15.869441: step 3235, loss 0.0393997, acc 0.98
2016-09-06T09:37:16.667993: step 3236, loss 0.0331228, acc 1
2016-09-06T09:37:17.487383: step 3237, loss 0.0380986, acc 1
2016-09-06T09:37:18.249903: step 3238, loss 0.057816, acc 1
2016-09-06T09:37:19.040971: step 3239, loss 0.0452119, acc 0.98
2016-09-06T09:37:19.883152: step 3240, loss 0.0643127, acc 0.96
2016-09-06T09:37:20.689345: step 3241, loss 0.0617187, acc 0.96
2016-09-06T09:37:21.541137: step 3242, loss 0.0310169, acc 0.98
2016-09-06T09:37:22.359337: step 3243, loss 0.0592762, acc 0.98
2016-09-06T09:37:23.145465: step 3244, loss 0.0288747, acc 0.98
2016-09-06T09:37:23.950137: step 3245, loss 0.0357282, acc 0.98
2016-09-06T09:37:24.753345: step 3246, loss 0.0324551, acc 0.98
2016-09-06T09:37:25.557871: step 3247, loss 0.123044, acc 0.92
2016-09-06T09:37:26.361442: step 3248, loss 0.0115991, acc 1
2016-09-06T09:37:27.159614: step 3249, loss 0.0305118, acc 0.98
2016-09-06T09:37:27.937958: step 3250, loss 0.0675579, acc 0.94
2016-09-06T09:37:28.791393: step 3251, loss 0.0422105, acc 0.98
2016-09-06T09:37:29.613797: step 3252, loss 0.0673834, acc 0.96
2016-09-06T09:37:30.433832: step 3253, loss 0.0181392, acc 1
2016-09-06T09:37:31.226782: step 3254, loss 0.0529977, acc 0.98
2016-09-06T09:37:32.070605: step 3255, loss 0.168228, acc 0.96
2016-09-06T09:37:32.882408: step 3256, loss 0.0800891, acc 0.96
2016-09-06T09:37:33.693416: step 3257, loss 0.0805974, acc 0.98
2016-09-06T09:37:34.538125: step 3258, loss 0.0915124, acc 0.96
2016-09-06T09:37:35.378898: step 3259, loss 0.0405856, acc 0.98
2016-09-06T09:37:36.197842: step 3260, loss 0.0235818, acc 0.98
2016-09-06T09:37:37.024981: step 3261, loss 0.0511183, acc 0.98
2016-09-06T09:37:37.840989: step 3262, loss 0.0502269, acc 0.98
2016-09-06T09:37:38.620007: step 3263, loss 0.0409162, acc 0.98
2016-09-06T09:37:39.399505: step 3264, loss 0.00522209, acc 1
2016-09-06T09:37:40.254034: step 3265, loss 0.0569581, acc 0.96
2016-09-06T09:37:41.049360: step 3266, loss 0.068093, acc 0.96
2016-09-06T09:37:41.870392: step 3267, loss 0.0373742, acc 0.98
2016-09-06T09:37:42.683116: step 3268, loss 0.0419498, acc 1
2016-09-06T09:37:43.521310: step 3269, loss 0.0197368, acc 1
2016-09-06T09:37:44.356540: step 3270, loss 0.080323, acc 0.96
2016-09-06T09:37:45.181631: step 3271, loss 0.0200575, acc 1
2016-09-06T09:37:45.987435: step 3272, loss 0.0400774, acc 0.98
2016-09-06T09:37:46.808335: step 3273, loss 0.0890548, acc 0.94
2016-09-06T09:37:47.609321: step 3274, loss 0.0178, acc 1
2016-09-06T09:37:48.407326: step 3275, loss 0.0246969, acc 0.98
2016-09-06T09:37:49.235820: step 3276, loss 0.0111313, acc 1
2016-09-06T09:37:50.034608: step 3277, loss 0.0420336, acc 0.98
2016-09-06T09:37:50.841759: step 3278, loss 0.0473603, acc 0.98
2016-09-06T09:37:51.651728: step 3279, loss 0.101634, acc 0.96
2016-09-06T09:37:52.530194: step 3280, loss 0.0564451, acc 0.98
2016-09-06T09:37:53.299319: step 3281, loss 0.0705882, acc 0.98
2016-09-06T09:37:54.129430: step 3282, loss 0.00718463, acc 1
2016-09-06T09:37:54.950621: step 3283, loss 0.0220333, acc 1
2016-09-06T09:37:55.741460: step 3284, loss 0.00701262, acc 1
2016-09-06T09:37:56.547917: step 3285, loss 0.0093629, acc 1
2016-09-06T09:37:57.365984: step 3286, loss 0.0845061, acc 0.98
2016-09-06T09:37:58.133759: step 3287, loss 0.0550947, acc 0.98
2016-09-06T09:37:58.935323: step 3288, loss 0.0294543, acc 1
2016-09-06T09:37:59.746953: step 3289, loss 0.0170433, acc 1
2016-09-06T09:38:00.561880: step 3290, loss 0.0197295, acc 1
2016-09-06T09:38:01.365404: step 3291, loss 0.0105344, acc 1
2016-09-06T09:38:02.195903: step 3292, loss 0.0207757, acc 0.98
2016-09-06T09:38:02.972969: step 3293, loss 0.00631294, acc 1
2016-09-06T09:38:03.776278: step 3294, loss 0.0497841, acc 0.98
2016-09-06T09:38:04.623363: step 3295, loss 0.015668, acc 1
2016-09-06T09:38:05.411891: step 3296, loss 0.0314939, acc 1
2016-09-06T09:38:06.242202: step 3297, loss 0.0356898, acc 0.98
2016-09-06T09:38:07.035498: step 3298, loss 0.0348022, acc 1
2016-09-06T09:38:07.846218: step 3299, loss 0.253556, acc 0.98
2016-09-06T09:38:08.654596: step 3300, loss 0.032728, acc 1

Evaluation:
2016-09-06T09:38:12.384947: step 3300, loss 1.35041, acc 0.75985

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-3300

2016-09-06T09:38:14.393821: step 3301, loss 0.093303, acc 0.94
2016-09-06T09:38:15.186401: step 3302, loss 0.026444, acc 1
2016-09-06T09:38:16.001981: step 3303, loss 0.0271996, acc 1
2016-09-06T09:38:16.849187: step 3304, loss 0.0208761, acc 1
2016-09-06T09:38:17.675475: step 3305, loss 0.0324943, acc 0.98
2016-09-06T09:38:18.481919: step 3306, loss 0.0166526, acc 1
2016-09-06T09:38:19.309913: step 3307, loss 0.0217688, acc 1
2016-09-06T09:38:20.143286: step 3308, loss 0.0333262, acc 0.98
2016-09-06T09:38:20.964209: step 3309, loss 0.0141198, acc 1
2016-09-06T09:38:21.786927: step 3310, loss 0.0911095, acc 0.96
2016-09-06T09:38:22.593882: step 3311, loss 0.0565903, acc 0.96
2016-09-06T09:38:23.381638: step 3312, loss 0.0256793, acc 1
2016-09-06T09:38:24.214440: step 3313, loss 0.0201271, acc 1
2016-09-06T09:38:25.064470: step 3314, loss 0.00425936, acc 1
2016-09-06T09:38:25.914113: step 3315, loss 0.0284321, acc 1
2016-09-06T09:38:26.726786: step 3316, loss 0.114827, acc 0.94
2016-09-06T09:38:27.529074: step 3317, loss 0.062114, acc 0.96
2016-09-06T09:38:28.366638: step 3318, loss 0.0251072, acc 0.98
2016-09-06T09:38:29.209348: step 3319, loss 0.0350124, acc 0.98
2016-09-06T09:38:30.024483: step 3320, loss 0.0344709, acc 0.98
2016-09-06T09:38:30.792648: step 3321, loss 0.00955906, acc 1
2016-09-06T09:38:31.599656: step 3322, loss 0.0575727, acc 0.98
2016-09-06T09:38:32.420852: step 3323, loss 0.0234644, acc 1
2016-09-06T09:38:33.205942: step 3324, loss 0.00654897, acc 1
2016-09-06T09:38:34.000565: step 3325, loss 0.086608, acc 0.94
2016-09-06T09:38:34.816585: step 3326, loss 0.00369336, acc 1
2016-09-06T09:38:35.608391: step 3327, loss 0.0094947, acc 1
2016-09-06T09:38:36.402307: step 3328, loss 0.0467365, acc 0.96
2016-09-06T09:38:37.222561: step 3329, loss 0.0116032, acc 1
2016-09-06T09:38:38.016070: step 3330, loss 0.0426889, acc 1
2016-09-06T09:38:38.805910: step 3331, loss 0.00682171, acc 1
2016-09-06T09:38:39.646023: step 3332, loss 0.0768764, acc 0.98
2016-09-06T09:38:40.436820: step 3333, loss 0.0899858, acc 0.94
2016-09-06T09:38:41.247983: step 3334, loss 0.0128799, acc 1
2016-09-06T09:38:42.065106: step 3335, loss 0.0305242, acc 0.98
2016-09-06T09:38:42.846007: step 3336, loss 0.0490655, acc 0.98
2016-09-06T09:38:43.632836: step 3337, loss 0.0196368, acc 1
2016-09-06T09:38:44.435241: step 3338, loss 0.0105015, acc 1
2016-09-06T09:38:45.271064: step 3339, loss 0.0409512, acc 1
2016-09-06T09:38:46.072359: step 3340, loss 0.00565612, acc 1
2016-09-06T09:38:46.883805: step 3341, loss 0.0287365, acc 0.98
2016-09-06T09:38:47.661476: step 3342, loss 0.069882, acc 0.94
2016-09-06T09:38:48.481842: step 3343, loss 0.0508383, acc 0.96
2016-09-06T09:38:49.294487: step 3344, loss 0.0247952, acc 0.98
2016-09-06T09:38:50.058582: step 3345, loss 0.0690999, acc 0.98
2016-09-06T09:38:50.875182: step 3346, loss 0.0896257, acc 0.96
2016-09-06T09:38:51.734633: step 3347, loss 0.110283, acc 0.94
2016-09-06T09:38:52.534621: step 3348, loss 0.0108915, acc 1
2016-09-06T09:38:53.353010: step 3349, loss 0.00489506, acc 1
2016-09-06T09:38:54.185916: step 3350, loss 0.0725273, acc 0.96
2016-09-06T09:38:54.986069: step 3351, loss 0.0339746, acc 0.98
2016-09-06T09:38:55.799033: step 3352, loss 0.0509965, acc 0.96
2016-09-06T09:38:56.599893: step 3353, loss 0.00679179, acc 1
2016-09-06T09:38:57.410876: step 3354, loss 0.0525503, acc 0.98
2016-09-06T09:38:58.210372: step 3355, loss 0.0388643, acc 1
2016-09-06T09:38:59.038623: step 3356, loss 0.00363498, acc 1
2016-09-06T09:38:59.860407: step 3357, loss 0.212478, acc 0.92
2016-09-06T09:39:00.690154: step 3358, loss 0.0277315, acc 0.98
2016-09-06T09:39:01.542816: step 3359, loss 0.0276341, acc 1
2016-09-06T09:39:02.347041: step 3360, loss 0.0244325, acc 1
2016-09-06T09:39:03.148780: step 3361, loss 0.0756609, acc 0.98
2016-09-06T09:39:03.981310: step 3362, loss 0.0955121, acc 0.96
2016-09-06T09:39:04.783376: step 3363, loss 0.0471434, acc 0.98
2016-09-06T09:39:05.607241: step 3364, loss 0.0496154, acc 0.98
2016-09-06T09:39:06.421975: step 3365, loss 0.0607581, acc 0.98
2016-09-06T09:39:07.235356: step 3366, loss 0.039621, acc 1
2016-09-06T09:39:08.033229: step 3367, loss 0.0261276, acc 0.98
2016-09-06T09:39:08.851646: step 3368, loss 0.0159237, acc 1
2016-09-06T09:39:09.640520: step 3369, loss 0.135542, acc 0.96
2016-09-06T09:39:10.495616: step 3370, loss 0.0242056, acc 0.98
2016-09-06T09:39:11.311686: step 3371, loss 0.120304, acc 0.94
2016-09-06T09:39:12.120437: step 3372, loss 0.0457882, acc 0.98
2016-09-06T09:39:12.927662: step 3373, loss 0.0423408, acc 1
2016-09-06T09:39:13.740031: step 3374, loss 0.0758901, acc 0.96
2016-09-06T09:39:14.556256: step 3375, loss 0.069589, acc 0.98
2016-09-06T09:39:15.380102: step 3376, loss 0.0824612, acc 0.96
2016-09-06T09:39:16.210706: step 3377, loss 0.0565616, acc 0.96
2016-09-06T09:39:17.025137: step 3378, loss 0.120388, acc 0.96
2016-09-06T09:39:17.848321: step 3379, loss 0.114646, acc 0.96
2016-09-06T09:39:18.669783: step 3380, loss 0.0287292, acc 1
2016-09-06T09:39:19.465154: step 3381, loss 0.0365931, acc 1
2016-09-06T09:39:20.253606: step 3382, loss 0.0427044, acc 0.98
2016-09-06T09:39:21.100707: step 3383, loss 0.084488, acc 0.96
2016-09-06T09:39:21.901004: step 3384, loss 0.0838765, acc 0.94
2016-09-06T09:39:22.701724: step 3385, loss 0.164241, acc 0.94
2016-09-06T09:39:23.526836: step 3386, loss 0.00652762, acc 1
2016-09-06T09:39:24.330923: step 3387, loss 0.0217105, acc 1
2016-09-06T09:39:25.181374: step 3388, loss 0.0252683, acc 0.98
2016-09-06T09:39:25.995412: step 3389, loss 0.06158, acc 0.96
2016-09-06T09:39:26.823190: step 3390, loss 0.0406874, acc 0.98
2016-09-06T09:39:27.612207: step 3391, loss 0.0564177, acc 0.98
2016-09-06T09:39:28.429733: step 3392, loss 0.0289318, acc 0.98
2016-09-06T09:39:29.256142: step 3393, loss 0.0368209, acc 0.98
2016-09-06T09:39:30.047897: step 3394, loss 0.117481, acc 0.96
2016-09-06T09:39:30.854335: step 3395, loss 0.0821889, acc 0.96
2016-09-06T09:39:31.664189: step 3396, loss 0.0347964, acc 1
2016-09-06T09:39:32.476559: step 3397, loss 0.0599266, acc 0.96
2016-09-06T09:39:33.278078: step 3398, loss 0.0944087, acc 0.94
2016-09-06T09:39:34.085580: step 3399, loss 0.079893, acc 0.98
2016-09-06T09:39:34.850107: step 3400, loss 0.0150342, acc 1

Evaluation:
2016-09-06T09:39:38.583725: step 3400, loss 1.49438, acc 0.756098

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-3400

2016-09-06T09:39:40.498058: step 3401, loss 0.0370276, acc 0.98
2016-09-06T09:39:41.306255: step 3402, loss 0.0497875, acc 0.98
2016-09-06T09:39:42.094547: step 3403, loss 0.0100959, acc 1
2016-09-06T09:39:42.910690: step 3404, loss 0.0333542, acc 0.98
2016-09-06T09:39:43.731569: step 3405, loss 0.0472616, acc 0.96
2016-09-06T09:39:44.539276: step 3406, loss 0.00768768, acc 1
2016-09-06T09:39:45.350448: step 3407, loss 0.0199161, acc 1
2016-09-06T09:39:46.167448: step 3408, loss 0.0367513, acc 0.98
2016-09-06T09:39:46.966642: step 3409, loss 0.0123765, acc 1
2016-09-06T09:39:47.772584: step 3410, loss 0.0432799, acc 1
2016-09-06T09:39:48.598202: step 3411, loss 0.0667116, acc 0.96
2016-09-06T09:39:49.393476: step 3412, loss 0.0221127, acc 1
2016-09-06T09:39:50.180716: step 3413, loss 0.0366898, acc 0.98
2016-09-06T09:39:51.009787: step 3414, loss 0.0218438, acc 1
2016-09-06T09:39:51.811453: step 3415, loss 0.00885768, acc 1
2016-09-06T09:39:52.591557: step 3416, loss 0.0253443, acc 0.98
2016-09-06T09:39:53.426541: step 3417, loss 0.0187073, acc 1
2016-09-06T09:39:54.221969: step 3418, loss 0.0695251, acc 0.98
2016-09-06T09:39:55.058954: step 3419, loss 0.0126932, acc 1
2016-09-06T09:39:55.875101: step 3420, loss 0.0382351, acc 0.98
2016-09-06T09:39:56.663826: step 3421, loss 0.0704702, acc 0.96
2016-09-06T09:39:57.451177: step 3422, loss 0.0131315, acc 1
2016-09-06T09:39:58.259037: step 3423, loss 0.0478714, acc 0.98
2016-09-06T09:39:59.060378: step 3424, loss 0.0367311, acc 1
2016-09-06T09:39:59.878219: step 3425, loss 0.0314851, acc 1
2016-09-06T09:40:00.746184: step 3426, loss 0.0217521, acc 1
2016-09-06T09:40:01.555243: step 3427, loss 0.0170738, acc 1
2016-09-06T09:40:02.415348: step 3428, loss 0.00510887, acc 1
2016-09-06T09:40:03.241580: step 3429, loss 0.0154847, acc 1
2016-09-06T09:40:04.098966: step 3430, loss 0.026529, acc 1
2016-09-06T09:40:04.896843: step 3431, loss 0.0841904, acc 0.94
2016-09-06T09:40:05.743318: step 3432, loss 0.0343376, acc 0.98
2016-09-06T09:40:06.591120: step 3433, loss 0.0521572, acc 0.96
2016-09-06T09:40:07.398521: step 3434, loss 0.0205724, acc 1
2016-09-06T09:40:08.236668: step 3435, loss 0.00605615, acc 1
2016-09-06T09:40:09.054973: step 3436, loss 0.0412758, acc 0.98
2016-09-06T09:40:09.862135: step 3437, loss 0.0565166, acc 0.96
2016-09-06T09:40:10.669485: step 3438, loss 0.00581575, acc 1
2016-09-06T09:40:11.473692: step 3439, loss 0.0759293, acc 0.96
2016-09-06T09:40:12.282248: step 3440, loss 0.0100489, acc 1
2016-09-06T09:40:13.109063: step 3441, loss 0.0173935, acc 1
2016-09-06T09:40:13.909060: step 3442, loss 0.0229134, acc 0.98
2016-09-06T09:40:14.686480: step 3443, loss 0.0573859, acc 0.96
2016-09-06T09:40:15.543447: step 3444, loss 0.114295, acc 0.94
2016-09-06T09:40:16.393692: step 3445, loss 0.0371742, acc 0.98
2016-09-06T09:40:17.176664: step 3446, loss 0.0278951, acc 0.98
2016-09-06T09:40:17.980657: step 3447, loss 0.0111382, acc 1
2016-09-06T09:40:18.810765: step 3448, loss 0.0252238, acc 1
2016-09-06T09:40:19.594041: step 3449, loss 0.0746397, acc 0.96
2016-09-06T09:40:20.445419: step 3450, loss 0.0484788, acc 0.98
2016-09-06T09:40:21.233645: step 3451, loss 0.0506286, acc 0.98
2016-09-06T09:40:22.009096: step 3452, loss 0.109093, acc 0.98
2016-09-06T09:40:22.800546: step 3453, loss 0.0362082, acc 0.98
2016-09-06T09:40:23.607381: step 3454, loss 0.0599279, acc 0.98
2016-09-06T09:40:24.372942: step 3455, loss 0.0582554, acc 0.94
2016-09-06T09:40:25.125615: step 3456, loss 0.0271463, acc 0.977273
2016-09-06T09:40:25.969721: step 3457, loss 0.016684, acc 1
2016-09-06T09:40:26.794917: step 3458, loss 0.0773654, acc 0.98
2016-09-06T09:40:27.622611: step 3459, loss 0.0522124, acc 0.96
2016-09-06T09:40:28.441956: step 3460, loss 0.0252192, acc 1
2016-09-06T09:40:29.236502: step 3461, loss 0.0382289, acc 0.98
2016-09-06T09:40:30.037423: step 3462, loss 0.0610466, acc 0.98
2016-09-06T09:40:30.847762: step 3463, loss 0.0415223, acc 1
2016-09-06T09:40:31.665453: step 3464, loss 0.0209601, acc 0.98
2016-09-06T09:40:32.483187: step 3465, loss 0.11144, acc 0.92
2016-09-06T09:40:33.299197: step 3466, loss 0.186004, acc 0.96
2016-09-06T09:40:34.110641: step 3467, loss 0.00524359, acc 1
2016-09-06T09:40:34.927620: step 3468, loss 0.00608186, acc 1
2016-09-06T09:40:35.760242: step 3469, loss 0.0539892, acc 0.98
2016-09-06T09:40:36.554669: step 3470, loss 0.0432495, acc 0.98
2016-09-06T09:40:37.344271: step 3471, loss 0.114354, acc 0.94
2016-09-06T09:40:38.181758: step 3472, loss 0.0140492, acc 1
2016-09-06T09:40:38.952497: step 3473, loss 0.0108175, acc 1
2016-09-06T09:40:39.750006: step 3474, loss 0.00624777, acc 1
2016-09-06T09:40:40.579904: step 3475, loss 0.0351609, acc 0.98
2016-09-06T09:40:41.357357: step 3476, loss 0.00621015, acc 1
2016-09-06T09:40:42.170587: step 3477, loss 0.0259678, acc 0.98
2016-09-06T09:40:43.000917: step 3478, loss 0.0593785, acc 0.96
2016-09-06T09:40:43.774046: step 3479, loss 0.043956, acc 0.96
2016-09-06T09:40:44.587906: step 3480, loss 0.0306886, acc 0.98
2016-09-06T09:40:45.420649: step 3481, loss 0.0388156, acc 0.98
2016-09-06T09:40:46.186305: step 3482, loss 0.0326259, acc 0.98
2016-09-06T09:40:46.981319: step 3483, loss 0.0314553, acc 0.98
2016-09-06T09:40:47.807226: step 3484, loss 0.0320699, acc 1
2016-09-06T09:40:48.587660: step 3485, loss 0.037011, acc 0.98
2016-09-06T09:40:49.397871: step 3486, loss 0.0638306, acc 0.98
2016-09-06T09:40:50.235326: step 3487, loss 0.00371311, acc 1
2016-09-06T09:40:51.024056: step 3488, loss 0.00752812, acc 1
2016-09-06T09:40:51.820921: step 3489, loss 0.0738425, acc 0.94
2016-09-06T09:40:52.661353: step 3490, loss 0.0989084, acc 0.98
2016-09-06T09:40:53.433234: step 3491, loss 0.00690456, acc 1
2016-09-06T09:40:54.239028: step 3492, loss 0.0228364, acc 0.98
2016-09-06T09:40:55.060743: step 3493, loss 0.0672418, acc 0.96
2016-09-06T09:40:55.843891: step 3494, loss 0.0831111, acc 0.98
2016-09-06T09:40:56.647169: step 3495, loss 0.014691, acc 1
2016-09-06T09:40:57.460886: step 3496, loss 0.00448578, acc 1
2016-09-06T09:40:58.271766: step 3497, loss 0.0638029, acc 0.96
2016-09-06T09:40:59.110444: step 3498, loss 0.00881357, acc 1
2016-09-06T09:40:59.912811: step 3499, loss 0.0876411, acc 0.94
2016-09-06T09:41:00.721738: step 3500, loss 0.00708153, acc 1

Evaluation:
2016-09-06T09:41:04.440495: step 3500, loss 1.41985, acc 0.763602

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-3500

2016-09-06T09:41:06.373319: step 3501, loss 0.0249209, acc 0.98
2016-09-06T09:41:07.223750: step 3502, loss 0.0175161, acc 1
2016-09-06T09:41:08.031917: step 3503, loss 0.0273857, acc 0.98
2016-09-06T09:41:08.836216: step 3504, loss 0.0306789, acc 0.98
2016-09-06T09:41:09.675143: step 3505, loss 0.0596702, acc 0.98
2016-09-06T09:41:10.520793: step 3506, loss 0.0183516, acc 1
2016-09-06T09:41:11.321424: step 3507, loss 0.0247766, acc 0.98
2016-09-06T09:41:12.166767: step 3508, loss 0.0878985, acc 0.94
2016-09-06T09:41:12.970156: step 3509, loss 0.0518904, acc 0.96
2016-09-06T09:41:13.805970: step 3510, loss 0.020936, acc 0.98
2016-09-06T09:41:14.645020: step 3511, loss 0.0254425, acc 1
2016-09-06T09:41:15.451691: step 3512, loss 0.051201, acc 0.98
2016-09-06T09:41:16.269701: step 3513, loss 0.0182459, acc 1
2016-09-06T09:41:17.082137: step 3514, loss 0.119176, acc 0.9
2016-09-06T09:41:17.887722: step 3515, loss 0.015825, acc 1
2016-09-06T09:41:18.698655: step 3516, loss 0.0130126, acc 1
2016-09-06T09:41:19.533290: step 3517, loss 0.0607817, acc 0.96
2016-09-06T09:41:20.367565: step 3518, loss 0.0192936, acc 1
2016-09-06T09:41:21.212482: step 3519, loss 0.0072464, acc 1
2016-09-06T09:41:22.023562: step 3520, loss 0.073335, acc 0.96
2016-09-06T09:41:22.845281: step 3521, loss 0.198169, acc 0.94
2016-09-06T09:41:23.637578: step 3522, loss 0.0158822, acc 1
2016-09-06T09:41:24.449770: step 3523, loss 0.0614984, acc 0.96
2016-09-06T09:41:25.280215: step 3524, loss 0.123157, acc 0.92
2016-09-06T09:41:26.060701: step 3525, loss 0.10628, acc 0.92
2016-09-06T09:41:26.935244: step 3526, loss 0.0204716, acc 1
2016-09-06T09:41:27.722255: step 3527, loss 0.0825577, acc 0.96
2016-09-06T09:41:28.524363: step 3528, loss 0.0390688, acc 1
2016-09-06T09:41:29.339399: step 3529, loss 0.0407846, acc 0.98
2016-09-06T09:41:30.157083: step 3530, loss 0.0182823, acc 0.98
2016-09-06T09:41:30.973686: step 3531, loss 0.157348, acc 0.96
2016-09-06T09:41:31.767641: step 3532, loss 0.160331, acc 0.94
2016-09-06T09:41:32.583114: step 3533, loss 0.0265041, acc 0.98
2016-09-06T09:41:33.361746: step 3534, loss 0.11715, acc 0.98
2016-09-06T09:41:34.169305: step 3535, loss 0.0972754, acc 0.94
2016-09-06T09:41:35.007927: step 3536, loss 0.051451, acc 0.96
2016-09-06T09:41:35.780482: step 3537, loss 0.0208299, acc 1
2016-09-06T09:41:36.566714: step 3538, loss 0.0428256, acc 0.98
2016-09-06T09:41:37.385567: step 3539, loss 0.0218333, acc 1
2016-09-06T09:41:38.214872: step 3540, loss 0.0303059, acc 0.98
2016-09-06T09:41:38.998390: step 3541, loss 0.0965886, acc 0.96
2016-09-06T09:41:39.847215: step 3542, loss 0.116029, acc 0.98
2016-09-06T09:41:40.645238: step 3543, loss 0.0485196, acc 0.98
2016-09-06T09:41:41.437872: step 3544, loss 0.0485927, acc 0.98
2016-09-06T09:41:42.256477: step 3545, loss 0.0616993, acc 0.96
2016-09-06T09:41:43.042741: step 3546, loss 0.056752, acc 0.98
2016-09-06T09:41:43.838712: step 3547, loss 0.0616045, acc 0.98
2016-09-06T09:41:44.655747: step 3548, loss 0.0381103, acc 1
2016-09-06T09:41:45.427375: step 3549, loss 0.0744321, acc 0.96
2016-09-06T09:41:46.261710: step 3550, loss 0.033228, acc 0.98
2016-09-06T09:41:47.107843: step 3551, loss 0.0565911, acc 0.98
2016-09-06T09:41:47.936699: step 3552, loss 0.00589848, acc 1
2016-09-06T09:41:48.730300: step 3553, loss 0.00772229, acc 1
2016-09-06T09:41:49.537262: step 3554, loss 0.100748, acc 0.96
2016-09-06T09:41:50.341994: step 3555, loss 0.0289106, acc 0.98
2016-09-06T09:41:51.134666: step 3556, loss 0.0312843, acc 1
2016-09-06T09:41:51.978638: step 3557, loss 0.0727043, acc 0.94
2016-09-06T09:41:52.783691: step 3558, loss 0.0188146, acc 1
2016-09-06T09:41:53.606985: step 3559, loss 0.0543475, acc 0.96
2016-09-06T09:41:54.475272: step 3560, loss 0.00930719, acc 1
2016-09-06T09:41:55.270554: step 3561, loss 0.0365161, acc 0.98
2016-09-06T09:41:56.079538: step 3562, loss 0.0372145, acc 0.98
2016-09-06T09:41:56.905443: step 3563, loss 0.0317829, acc 0.98
2016-09-06T09:41:57.738755: step 3564, loss 0.0307586, acc 1
2016-09-06T09:41:58.529655: step 3565, loss 0.0280387, acc 1
2016-09-06T09:41:59.376197: step 3566, loss 0.0162837, acc 1
2016-09-06T09:42:00.173566: step 3567, loss 0.0511197, acc 0.98
2016-09-06T09:42:00.992723: step 3568, loss 0.0989836, acc 0.96
2016-09-06T09:42:01.808171: step 3569, loss 0.0197257, acc 1
2016-09-06T09:42:02.605246: step 3570, loss 0.0583419, acc 0.96
2016-09-06T09:42:03.414394: step 3571, loss 0.0344487, acc 1
2016-09-06T09:42:04.236997: step 3572, loss 0.0402359, acc 1
2016-09-06T09:42:05.034862: step 3573, loss 0.0911786, acc 0.94
2016-09-06T09:42:05.860981: step 3574, loss 0.0169273, acc 1
2016-09-06T09:42:06.684974: step 3575, loss 0.124432, acc 0.98
2016-09-06T09:42:07.507408: step 3576, loss 0.0388175, acc 0.98
2016-09-06T09:42:08.337715: step 3577, loss 0.0213142, acc 1
2016-09-06T09:42:09.159768: step 3578, loss 0.0202691, acc 0.98
2016-09-06T09:42:09.948648: step 3579, loss 0.0225597, acc 1
2016-09-06T09:42:10.766553: step 3580, loss 0.0307205, acc 1
2016-09-06T09:42:11.585570: step 3581, loss 0.0250187, acc 0.98
2016-09-06T09:42:12.376853: step 3582, loss 0.0143179, acc 1
2016-09-06T09:42:13.154225: step 3583, loss 0.0179188, acc 1
2016-09-06T09:42:14.003718: step 3584, loss 0.0262871, acc 1
2016-09-06T09:42:14.807398: step 3585, loss 0.0519609, acc 0.98
2016-09-06T09:42:15.626890: step 3586, loss 0.0448409, acc 0.98
2016-09-06T09:42:16.459292: step 3587, loss 0.0976625, acc 0.94
2016-09-06T09:42:17.283371: step 3588, loss 0.0520793, acc 0.96
2016-09-06T09:42:18.086513: step 3589, loss 0.0286714, acc 0.98
2016-09-06T09:42:18.895963: step 3590, loss 0.124648, acc 0.96
2016-09-06T09:42:19.714388: step 3591, loss 0.0904262, acc 0.98
2016-09-06T09:42:20.483279: step 3592, loss 0.0668779, acc 0.96
2016-09-06T09:42:21.277975: step 3593, loss 0.0739154, acc 0.96
2016-09-06T09:42:22.078432: step 3594, loss 0.0694941, acc 0.96
2016-09-06T09:42:22.871565: step 3595, loss 0.0227429, acc 0.98
2016-09-06T09:42:23.680418: step 3596, loss 0.00403692, acc 1
2016-09-06T09:42:24.479893: step 3597, loss 0.0865377, acc 0.96
2016-09-06T09:42:25.278757: step 3598, loss 0.00435309, acc 1
2016-09-06T09:42:26.084922: step 3599, loss 0.0474703, acc 0.98
2016-09-06T09:42:26.899309: step 3600, loss 0.0277941, acc 1

Evaluation:
2016-09-06T09:42:30.653298: step 3600, loss 1.64937, acc 0.753283

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-3600

2016-09-06T09:42:32.582095: step 3601, loss 0.0303458, acc 0.98
2016-09-06T09:42:33.415935: step 3602, loss 0.0516544, acc 0.96
2016-09-06T09:42:34.240926: step 3603, loss 0.129492, acc 0.96
2016-09-06T09:42:35.086582: step 3604, loss 0.0734685, acc 0.94
2016-09-06T09:42:35.904391: step 3605, loss 0.0855537, acc 0.96
2016-09-06T09:42:36.706989: step 3606, loss 0.0396957, acc 0.98
2016-09-06T09:42:37.541332: step 3607, loss 0.0298013, acc 1
2016-09-06T09:42:38.342921: step 3608, loss 0.00415075, acc 1
2016-09-06T09:42:39.154751: step 3609, loss 0.00344857, acc 1
2016-09-06T09:42:39.957606: step 3610, loss 0.0275575, acc 1
2016-09-06T09:42:40.756812: step 3611, loss 0.0896, acc 0.94
2016-09-06T09:42:41.571637: step 3612, loss 0.192247, acc 0.94
2016-09-06T09:42:42.358604: step 3613, loss 0.04626, acc 0.98
2016-09-06T09:42:43.171782: step 3614, loss 0.0215776, acc 1
2016-09-06T09:42:43.989486: step 3615, loss 0.0290398, acc 0.98
2016-09-06T09:42:44.795701: step 3616, loss 0.049249, acc 1
2016-09-06T09:42:45.633266: step 3617, loss 0.0149603, acc 1
2016-09-06T09:42:46.431515: step 3618, loss 0.0310933, acc 0.98
2016-09-06T09:42:47.231364: step 3619, loss 0.0148787, acc 1
2016-09-06T09:42:48.035469: step 3620, loss 0.00711172, acc 1
2016-09-06T09:42:48.870474: step 3621, loss 0.006838, acc 1
2016-09-06T09:42:49.649468: step 3622, loss 0.0343146, acc 0.98
2016-09-06T09:42:50.480661: step 3623, loss 0.0899336, acc 0.96
2016-09-06T09:42:51.303195: step 3624, loss 0.0602846, acc 0.98
2016-09-06T09:42:52.075297: step 3625, loss 0.0619224, acc 0.98
2016-09-06T09:42:52.855491: step 3626, loss 0.0196301, acc 1
2016-09-06T09:42:53.684390: step 3627, loss 0.065111, acc 0.96
2016-09-06T09:42:54.497400: step 3628, loss 0.086273, acc 0.96
2016-09-06T09:42:55.297763: step 3629, loss 0.0299604, acc 1
2016-09-06T09:42:56.096939: step 3630, loss 0.034531, acc 0.98
2016-09-06T09:42:56.889979: step 3631, loss 0.0365936, acc 1
2016-09-06T09:42:57.685122: step 3632, loss 0.0632707, acc 0.98
2016-09-06T09:42:58.518861: step 3633, loss 0.0735481, acc 0.94
2016-09-06T09:42:59.316350: step 3634, loss 0.156826, acc 0.94
2016-09-06T09:43:00.183163: step 3635, loss 0.0297246, acc 1
2016-09-06T09:43:01.007227: step 3636, loss 0.0466022, acc 0.98
2016-09-06T09:43:01.804139: step 3637, loss 0.011909, acc 1
2016-09-06T09:43:02.616799: step 3638, loss 0.15387, acc 0.96
2016-09-06T09:43:03.439372: step 3639, loss 0.0123425, acc 1
2016-09-06T09:43:04.267560: step 3640, loss 0.0103491, acc 1
2016-09-06T09:43:05.065527: step 3641, loss 0.0435825, acc 0.98
2016-09-06T09:43:05.886065: step 3642, loss 0.0340008, acc 0.98
2016-09-06T09:43:06.705400: step 3643, loss 0.0082002, acc 1
2016-09-06T09:43:07.508905: step 3644, loss 0.00587817, acc 1
2016-09-06T09:43:08.334727: step 3645, loss 0.0529291, acc 0.98
2016-09-06T09:43:09.141872: step 3646, loss 0.00447524, acc 1
2016-09-06T09:43:09.936863: step 3647, loss 0.0268002, acc 1
2016-09-06T09:43:10.708122: step 3648, loss 0.00752587, acc 1
2016-09-06T09:43:11.551716: step 3649, loss 0.0469708, acc 0.98
2016-09-06T09:43:12.364120: step 3650, loss 0.0745309, acc 0.94
2016-09-06T09:43:13.177441: step 3651, loss 0.0201617, acc 0.98
2016-09-06T09:43:14.009832: step 3652, loss 0.025525, acc 0.98
2016-09-06T09:43:14.812056: step 3653, loss 0.0127069, acc 1
2016-09-06T09:43:15.632148: step 3654, loss 0.0305732, acc 0.98
2016-09-06T09:43:16.445675: step 3655, loss 0.0322021, acc 0.98
2016-09-06T09:43:17.263370: step 3656, loss 0.0423227, acc 1
2016-09-06T09:43:18.117634: step 3657, loss 0.0238922, acc 1
2016-09-06T09:43:18.932429: step 3658, loss 0.0487542, acc 0.98
2016-09-06T09:43:19.749701: step 3659, loss 0.029451, acc 0.98
2016-09-06T09:43:20.602267: step 3660, loss 0.0187398, acc 0.98
2016-09-06T09:43:21.391530: step 3661, loss 0.0701094, acc 0.96
2016-09-06T09:43:22.193449: step 3662, loss 0.0289489, acc 0.98
2016-09-06T09:43:23.044644: step 3663, loss 0.0995372, acc 0.96
2016-09-06T09:43:23.869330: step 3664, loss 0.0438367, acc 0.98
2016-09-06T09:43:24.679316: step 3665, loss 0.173193, acc 0.96
2016-09-06T09:43:25.475912: step 3666, loss 0.0132767, acc 1
2016-09-06T09:43:26.296083: step 3667, loss 0.0264395, acc 1
2016-09-06T09:43:27.093467: step 3668, loss 0.0329556, acc 0.98
2016-09-06T09:43:27.885585: step 3669, loss 0.0231175, acc 1
2016-09-06T09:43:28.712159: step 3670, loss 0.023609, acc 0.98
2016-09-06T09:43:29.504236: step 3671, loss 0.0117204, acc 1
2016-09-06T09:43:30.310935: step 3672, loss 0.107521, acc 0.96
2016-09-06T09:43:31.119198: step 3673, loss 0.056996, acc 0.96
2016-09-06T09:43:31.915514: step 3674, loss 0.0266668, acc 0.98
2016-09-06T09:43:32.736868: step 3675, loss 0.0446659, acc 1
2016-09-06T09:43:33.556116: step 3676, loss 0.0197755, acc 0.98
2016-09-06T09:43:34.351825: step 3677, loss 0.00932005, acc 1
2016-09-06T09:43:35.155089: step 3678, loss 0.017902, acc 1
2016-09-06T09:43:35.979735: step 3679, loss 0.0140687, acc 1
2016-09-06T09:43:36.760525: step 3680, loss 0.058014, acc 0.98
2016-09-06T09:43:37.563512: step 3681, loss 0.019822, acc 1
2016-09-06T09:43:38.381557: step 3682, loss 0.0164599, acc 1
2016-09-06T09:43:39.187853: step 3683, loss 0.0048644, acc 1
2016-09-06T09:43:40.006422: step 3684, loss 0.00983448, acc 1
2016-09-06T09:43:40.823025: step 3685, loss 0.0481084, acc 0.96
2016-09-06T09:43:41.615679: step 3686, loss 0.0160829, acc 1
2016-09-06T09:43:42.413388: step 3687, loss 0.0401365, acc 0.98
2016-09-06T09:43:43.250222: step 3688, loss 0.0136357, acc 1
2016-09-06T09:43:44.072258: step 3689, loss 0.0227593, acc 0.98
2016-09-06T09:43:44.856582: step 3690, loss 0.0266636, acc 0.98
2016-09-06T09:43:45.682736: step 3691, loss 0.0613951, acc 0.98
2016-09-06T09:43:46.450015: step 3692, loss 0.0201, acc 0.98
2016-09-06T09:43:47.239782: step 3693, loss 0.00456578, acc 1
2016-09-06T09:43:48.059358: step 3694, loss 0.0209014, acc 1
2016-09-06T09:43:48.855623: step 3695, loss 0.0191229, acc 0.98
2016-09-06T09:43:49.658275: step 3696, loss 0.0497807, acc 0.98
2016-09-06T09:43:50.485111: step 3697, loss 0.0451551, acc 0.96
2016-09-06T09:43:51.272023: step 3698, loss 0.0138242, acc 1
2016-09-06T09:43:52.089281: step 3699, loss 0.0101966, acc 1
2016-09-06T09:43:52.902055: step 3700, loss 0.107786, acc 0.96

Evaluation:
2016-09-06T09:43:56.626937: step 3700, loss 1.70122, acc 0.771107

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-3700

2016-09-06T09:43:58.502608: step 3701, loss 0.0257037, acc 0.98
2016-09-06T09:43:59.344942: step 3702, loss 0.0175708, acc 0.98
2016-09-06T09:44:00.150834: step 3703, loss 0.0613752, acc 0.96
2016-09-06T09:44:00.963273: step 3704, loss 0.0444143, acc 0.98
2016-09-06T09:44:01.788155: step 3705, loss 0.0261782, acc 1
2016-09-06T09:44:02.612417: step 3706, loss 0.0547146, acc 0.98
2016-09-06T09:44:03.402449: step 3707, loss 0.0187859, acc 1
2016-09-06T09:44:04.239259: step 3708, loss 0.00414119, acc 1
2016-09-06T09:44:05.044785: step 3709, loss 0.0179227, acc 1
2016-09-06T09:44:05.849566: step 3710, loss 0.0388314, acc 0.98
2016-09-06T09:44:06.711535: step 3711, loss 0.0292013, acc 0.98
2016-09-06T09:44:07.528742: step 3712, loss 0.0660333, acc 0.96
2016-09-06T09:44:08.328479: step 3713, loss 0.0609211, acc 0.96
2016-09-06T09:44:09.173090: step 3714, loss 0.0362723, acc 0.98
2016-09-06T09:44:10.007428: step 3715, loss 0.0331521, acc 1
2016-09-06T09:44:10.834014: step 3716, loss 0.104276, acc 0.96
2016-09-06T09:44:11.645809: step 3717, loss 0.091776, acc 0.98
2016-09-06T09:44:12.492903: step 3718, loss 0.0512721, acc 0.96
2016-09-06T09:44:13.308915: step 3719, loss 0.0468084, acc 0.98
2016-09-06T09:44:14.121992: step 3720, loss 0.0117974, acc 1
2016-09-06T09:44:14.958644: step 3721, loss 0.0753661, acc 0.94
2016-09-06T09:44:15.763788: step 3722, loss 0.0403585, acc 0.98
2016-09-06T09:44:16.582190: step 3723, loss 0.0298699, acc 1
2016-09-06T09:44:17.403761: step 3724, loss 0.0344214, acc 0.98
2016-09-06T09:44:18.212435: step 3725, loss 0.0199441, acc 0.98
2016-09-06T09:44:19.037311: step 3726, loss 0.0350981, acc 0.96
2016-09-06T09:44:19.888754: step 3727, loss 0.163851, acc 0.96
2016-09-06T09:44:20.724813: step 3728, loss 0.0304794, acc 1
2016-09-06T09:44:21.529864: step 3729, loss 0.00381983, acc 1
2016-09-06T09:44:22.397753: step 3730, loss 0.0069242, acc 1
2016-09-06T09:44:23.227755: step 3731, loss 0.079669, acc 0.96
2016-09-06T09:44:24.037448: step 3732, loss 0.0334882, acc 0.98
2016-09-06T09:44:24.873906: step 3733, loss 0.0319787, acc 0.98
2016-09-06T09:44:25.686904: step 3734, loss 0.0348204, acc 0.98
2016-09-06T09:44:26.451742: step 3735, loss 0.00789358, acc 1
2016-09-06T09:44:27.257523: step 3736, loss 0.0347288, acc 0.96
2016-09-06T09:44:28.093950: step 3737, loss 0.0640528, acc 0.96
2016-09-06T09:44:28.897665: step 3738, loss 0.0133885, acc 1
2016-09-06T09:44:29.709537: step 3739, loss 0.0106791, acc 1
2016-09-06T09:44:30.525032: step 3740, loss 0.0489782, acc 0.98
2016-09-06T09:44:31.301873: step 3741, loss 0.0079665, acc 1
2016-09-06T09:44:32.105019: step 3742, loss 0.0225935, acc 1
2016-09-06T09:44:32.933291: step 3743, loss 0.00795198, acc 1
2016-09-06T09:44:33.697780: step 3744, loss 0.0115541, acc 1
2016-09-06T09:44:34.527739: step 3745, loss 0.0476566, acc 0.98
2016-09-06T09:44:35.370646: step 3746, loss 0.0453105, acc 0.96
2016-09-06T09:44:36.145861: step 3747, loss 0.0331106, acc 1
2016-09-06T09:44:36.947390: step 3748, loss 0.022489, acc 1
2016-09-06T09:44:37.750652: step 3749, loss 0.0472354, acc 0.96
2016-09-06T09:44:38.604948: step 3750, loss 0.00930777, acc 1
2016-09-06T09:44:39.407208: step 3751, loss 0.0199331, acc 1
2016-09-06T09:44:40.226270: step 3752, loss 0.0246788, acc 1
2016-09-06T09:44:40.999709: step 3753, loss 0.0154176, acc 1
2016-09-06T09:44:41.811983: step 3754, loss 0.00615108, acc 1
2016-09-06T09:44:42.646746: step 3755, loss 0.0856254, acc 0.96
2016-09-06T09:44:43.439685: step 3756, loss 0.0488623, acc 1
2016-09-06T09:44:44.254729: step 3757, loss 0.0298488, acc 1
2016-09-06T09:44:45.079654: step 3758, loss 0.0487592, acc 0.96
2016-09-06T09:44:45.872888: step 3759, loss 0.0153949, acc 1
2016-09-06T09:44:46.687073: step 3760, loss 0.0402873, acc 0.96
2016-09-06T09:44:47.497399: step 3761, loss 0.0225361, acc 1
2016-09-06T09:44:48.317208: step 3762, loss 0.00397819, acc 1
2016-09-06T09:44:49.117668: step 3763, loss 0.0305593, acc 1
2016-09-06T09:44:49.929518: step 3764, loss 0.128093, acc 0.94
2016-09-06T09:44:50.738930: step 3765, loss 0.0383817, acc 0.96
2016-09-06T09:44:51.539271: step 3766, loss 0.0129741, acc 1
2016-09-06T09:44:52.364377: step 3767, loss 0.0487575, acc 0.98
2016-09-06T09:44:53.197423: step 3768, loss 0.0588286, acc 0.96
2016-09-06T09:44:54.001669: step 3769, loss 0.0202155, acc 1
2016-09-06T09:44:54.846399: step 3770, loss 0.052432, acc 0.98
2016-09-06T09:44:55.674632: step 3771, loss 0.116023, acc 0.98
2016-09-06T09:44:56.484280: step 3772, loss 0.0248949, acc 0.98
2016-09-06T09:44:57.315071: step 3773, loss 0.0527225, acc 0.98
2016-09-06T09:44:58.125505: step 3774, loss 0.0141141, acc 1
2016-09-06T09:44:58.945411: step 3775, loss 0.022207, acc 1
2016-09-06T09:44:59.763391: step 3776, loss 0.0162026, acc 1
2016-09-06T09:45:00.582536: step 3777, loss 0.0240125, acc 1
2016-09-06T09:45:01.395699: step 3778, loss 0.0444785, acc 0.96
2016-09-06T09:45:02.218980: step 3779, loss 0.00962511, acc 1
2016-09-06T09:45:03.035118: step 3780, loss 0.0306169, acc 0.98
2016-09-06T09:45:03.847952: step 3781, loss 0.0229526, acc 1
2016-09-06T09:45:04.691145: step 3782, loss 0.0855405, acc 0.96
2016-09-06T09:45:05.513251: step 3783, loss 0.0408229, acc 1
2016-09-06T09:45:06.333590: step 3784, loss 0.0443905, acc 0.98
2016-09-06T09:45:07.183650: step 3785, loss 0.043785, acc 0.98
2016-09-06T09:45:08.016351: step 3786, loss 0.0321957, acc 0.98
2016-09-06T09:45:08.793246: step 3787, loss 0.00773634, acc 1
2016-09-06T09:45:09.624565: step 3788, loss 0.0165723, acc 1
2016-09-06T09:45:10.444597: step 3789, loss 0.013215, acc 1
2016-09-06T09:45:11.214132: step 3790, loss 0.023921, acc 1
2016-09-06T09:45:12.047574: step 3791, loss 0.0104907, acc 1
2016-09-06T09:45:12.853713: step 3792, loss 0.0381995, acc 0.98
2016-09-06T09:45:13.632654: step 3793, loss 0.0127991, acc 1
2016-09-06T09:45:14.431195: step 3794, loss 0.0471782, acc 0.96
2016-09-06T09:45:15.289094: step 3795, loss 0.0426525, acc 0.98
2016-09-06T09:45:16.122614: step 3796, loss 0.025379, acc 1
2016-09-06T09:45:16.932836: step 3797, loss 0.0684429, acc 0.98
2016-09-06T09:45:17.784583: step 3798, loss 0.0866455, acc 0.94
2016-09-06T09:45:18.562369: step 3799, loss 0.00879426, acc 1
2016-09-06T09:45:19.358834: step 3800, loss 0.00466182, acc 1

Evaluation:
2016-09-06T09:45:23.096283: step 3800, loss 2.30028, acc 0.751407

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-3800

2016-09-06T09:45:24.997216: step 3801, loss 0.0714426, acc 0.98
2016-09-06T09:45:25.801669: step 3802, loss 0.111418, acc 0.96
2016-09-06T09:45:26.627252: step 3803, loss 0.0186578, acc 1
2016-09-06T09:45:27.474827: step 3804, loss 0.0509056, acc 0.98
2016-09-06T09:45:28.305904: step 3805, loss 0.0743977, acc 0.98
2016-09-06T09:45:29.121320: step 3806, loss 0.0490269, acc 0.96
2016-09-06T09:45:29.951990: step 3807, loss 0.0858569, acc 0.96
2016-09-06T09:45:30.766319: step 3808, loss 0.118073, acc 0.94
2016-09-06T09:45:31.563618: step 3809, loss 0.101292, acc 0.96
2016-09-06T09:45:32.373872: step 3810, loss 0.0360398, acc 0.98
2016-09-06T09:45:33.167630: step 3811, loss 0.0213336, acc 0.98
2016-09-06T09:45:33.976986: step 3812, loss 0.0258466, acc 1
2016-09-06T09:45:34.869717: step 3813, loss 0.0222763, acc 0.98
2016-09-06T09:45:35.700446: step 3814, loss 0.0315886, acc 0.98
2016-09-06T09:45:36.510564: step 3815, loss 0.00762527, acc 1
2016-09-06T09:45:37.334750: step 3816, loss 0.0483293, acc 0.98
2016-09-06T09:45:38.173900: step 3817, loss 0.0444788, acc 0.98
2016-09-06T09:45:38.992767: step 3818, loss 0.0236112, acc 1
2016-09-06T09:45:39.816716: step 3819, loss 0.0832076, acc 0.98
2016-09-06T09:45:40.690538: step 3820, loss 0.0618933, acc 0.96
2016-09-06T09:45:41.496674: step 3821, loss 0.132255, acc 0.96
2016-09-06T09:45:42.304337: step 3822, loss 0.0274273, acc 0.98
2016-09-06T09:45:43.115724: step 3823, loss 0.109382, acc 0.92
2016-09-06T09:45:43.889796: step 3824, loss 0.043911, acc 0.98
2016-09-06T09:45:44.698488: step 3825, loss 0.0246954, acc 0.98
2016-09-06T09:45:45.535354: step 3826, loss 0.114113, acc 0.92
2016-09-06T09:45:46.318576: step 3827, loss 0.0141714, acc 1
2016-09-06T09:45:47.114459: step 3828, loss 0.114818, acc 0.96
2016-09-06T09:45:47.960327: step 3829, loss 0.0151386, acc 1
2016-09-06T09:45:48.788474: step 3830, loss 0.0418676, acc 0.98
2016-09-06T09:45:49.608140: step 3831, loss 0.196305, acc 0.94
2016-09-06T09:45:50.457092: step 3832, loss 0.040584, acc 0.96
2016-09-06T09:45:51.244351: step 3833, loss 0.0650559, acc 0.98
2016-09-06T09:45:52.044318: step 3834, loss 0.134811, acc 0.98
2016-09-06T09:45:52.906468: step 3835, loss 0.0306497, acc 1
2016-09-06T09:45:53.706354: step 3836, loss 0.0424583, acc 0.98
2016-09-06T09:45:54.513754: step 3837, loss 0.0958793, acc 0.98
2016-09-06T09:45:55.337809: step 3838, loss 0.0534508, acc 1
2016-09-06T09:45:56.147999: step 3839, loss 0.0340663, acc 1
2016-09-06T09:45:56.897194: step 3840, loss 0.0479453, acc 0.977273
2016-09-06T09:45:57.727506: step 3841, loss 0.107976, acc 0.98
2016-09-06T09:45:58.545451: step 3842, loss 0.0829393, acc 0.96
2016-09-06T09:45:59.360011: step 3843, loss 0.0534178, acc 0.96
2016-09-06T09:46:00.180232: step 3844, loss 0.0651625, acc 0.96
2016-09-06T09:46:01.009424: step 3845, loss 0.0654274, acc 0.98
2016-09-06T09:46:01.808145: step 3846, loss 0.0114496, acc 1
2016-09-06T09:46:02.643592: step 3847, loss 0.0310926, acc 0.98
2016-09-06T09:46:03.498106: step 3848, loss 0.0511492, acc 0.96
2016-09-06T09:46:04.303617: step 3849, loss 0.0534789, acc 0.96
2016-09-06T09:46:05.130081: step 3850, loss 0.0564639, acc 0.96
2016-09-06T09:46:05.925061: step 3851, loss 0.0123974, acc 1
2016-09-06T09:46:06.728915: step 3852, loss 0.0411733, acc 0.98
2016-09-06T09:46:07.576234: step 3853, loss 0.0746806, acc 0.96
2016-09-06T09:46:08.388432: step 3854, loss 0.0383551, acc 0.98
2016-09-06T09:46:09.220307: step 3855, loss 0.0644288, acc 0.96
2016-09-06T09:46:10.039465: step 3856, loss 0.0442166, acc 0.98
2016-09-06T09:46:10.875476: step 3857, loss 0.033626, acc 1
2016-09-06T09:46:11.676488: step 3858, loss 0.021316, acc 1
2016-09-06T09:46:12.504100: step 3859, loss 0.0472167, acc 0.96
2016-09-06T09:46:13.346459: step 3860, loss 0.00588751, acc 1
2016-09-06T09:46:14.147881: step 3861, loss 0.0197216, acc 1
2016-09-06T09:46:14.961623: step 3862, loss 0.0144512, acc 1
2016-09-06T09:46:15.770389: step 3863, loss 0.0125883, acc 1
2016-09-06T09:46:16.549429: step 3864, loss 0.0211557, acc 1
2016-09-06T09:46:17.334871: step 3865, loss 0.0148766, acc 1
2016-09-06T09:46:18.150493: step 3866, loss 0.0457258, acc 0.98
2016-09-06T09:46:18.932531: step 3867, loss 0.0111686, acc 1
2016-09-06T09:46:19.725093: step 3868, loss 0.0054408, acc 1
2016-09-06T09:46:20.547298: step 3869, loss 0.00514448, acc 1
2016-09-06T09:46:21.339905: step 3870, loss 0.0888828, acc 0.96
2016-09-06T09:46:22.151823: step 3871, loss 0.00508259, acc 1
2016-09-06T09:46:22.980397: step 3872, loss 0.215773, acc 0.96
2016-09-06T09:46:23.742452: step 3873, loss 0.0505796, acc 0.96
2016-09-06T09:46:24.552434: step 3874, loss 0.0257418, acc 1
2016-09-06T09:46:25.377584: step 3875, loss 0.0143361, acc 1
2016-09-06T09:46:26.169550: step 3876, loss 0.0298356, acc 1
2016-09-06T09:46:26.971906: step 3877, loss 0.0362267, acc 0.96
2016-09-06T09:46:27.812144: step 3878, loss 0.0235161, acc 0.98
2016-09-06T09:46:28.615718: step 3879, loss 0.0211655, acc 0.98
2016-09-06T09:46:29.416630: step 3880, loss 0.00715632, acc 1
2016-09-06T09:46:30.222216: step 3881, loss 0.0216819, acc 0.98
2016-09-06T09:46:30.998674: step 3882, loss 0.00943473, acc 1
2016-09-06T09:46:31.825727: step 3883, loss 0.101835, acc 0.98
2016-09-06T09:46:32.650752: step 3884, loss 0.0070863, acc 1
2016-09-06T09:46:33.445708: step 3885, loss 0.0858366, acc 0.98
2016-09-06T09:46:34.252942: step 3886, loss 0.040418, acc 1
2016-09-06T09:46:35.101000: step 3887, loss 0.00810247, acc 1
2016-09-06T09:46:35.915814: step 3888, loss 0.173198, acc 0.94
2016-09-06T09:46:36.710718: step 3889, loss 0.100062, acc 0.94
2016-09-06T09:46:37.535702: step 3890, loss 0.099247, acc 0.98
2016-09-06T09:46:38.324608: step 3891, loss 0.0177113, acc 1
2016-09-06T09:46:39.125092: step 3892, loss 0.164436, acc 0.92
2016-09-06T09:46:39.931521: step 3893, loss 0.0463752, acc 0.96
2016-09-06T09:46:40.706777: step 3894, loss 0.0825972, acc 0.96
2016-09-06T09:46:41.510844: step 3895, loss 0.0198736, acc 1
2016-09-06T09:46:42.348116: step 3896, loss 0.0369929, acc 0.98
2016-09-06T09:46:43.120714: step 3897, loss 0.0386647, acc 0.98
2016-09-06T09:46:43.930617: step 3898, loss 0.0370063, acc 1
2016-09-06T09:46:44.764644: step 3899, loss 0.0564164, acc 0.96
2016-09-06T09:46:45.572257: step 3900, loss 0.0699988, acc 0.98

Evaluation:
2016-09-06T09:46:49.300848: step 3900, loss 1.28805, acc 0.749531

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-3900

2016-09-06T09:46:51.157707: step 3901, loss 0.065808, acc 0.94
2016-09-06T09:46:51.987800: step 3902, loss 0.0242154, acc 1
2016-09-06T09:46:52.812492: step 3903, loss 0.0145665, acc 1
2016-09-06T09:46:53.652340: step 3904, loss 0.0859674, acc 0.96
2016-09-06T09:46:54.473131: step 3905, loss 0.0449369, acc 0.98
2016-09-06T09:46:55.292519: step 3906, loss 0.0620572, acc 0.96
2016-09-06T09:46:56.082155: step 3907, loss 0.0377317, acc 0.96
2016-09-06T09:46:56.902791: step 3908, loss 0.0505211, acc 1
2016-09-06T09:46:57.678177: step 3909, loss 0.0930887, acc 0.94
2016-09-06T09:46:58.489517: step 3910, loss 0.0363693, acc 0.98
2016-09-06T09:46:59.326138: step 3911, loss 0.0292247, acc 1
2016-09-06T09:47:00.153211: step 3912, loss 0.0327852, acc 0.98
2016-09-06T09:47:00.998581: step 3913, loss 0.0576314, acc 0.98
2016-09-06T09:47:01.816094: step 3914, loss 0.0627392, acc 0.98
2016-09-06T09:47:02.642113: step 3915, loss 0.0382463, acc 0.98
2016-09-06T09:47:03.461309: step 3916, loss 0.0191556, acc 0.98
2016-09-06T09:47:04.287009: step 3917, loss 0.0425012, acc 0.98
2016-09-06T09:47:05.111277: step 3918, loss 0.0382182, acc 1
2016-09-06T09:47:05.944241: step 3919, loss 0.0416984, acc 0.98
2016-09-06T09:47:06.770364: step 3920, loss 0.0121581, acc 1
2016-09-06T09:47:07.567847: step 3921, loss 0.0264844, acc 1
2016-09-06T09:47:08.400383: step 3922, loss 0.0278828, acc 0.98
2016-09-06T09:47:09.234930: step 3923, loss 0.0183021, acc 1
2016-09-06T09:47:10.051846: step 3924, loss 0.00620421, acc 1
2016-09-06T09:47:10.883908: step 3925, loss 0.0573897, acc 0.98
2016-09-06T09:47:11.697071: step 3926, loss 0.03387, acc 0.96
2016-09-06T09:47:12.515507: step 3927, loss 0.0121062, acc 1
2016-09-06T09:47:13.318205: step 3928, loss 0.0346986, acc 1
2016-09-06T09:47:14.130193: step 3929, loss 0.038973, acc 0.98
2016-09-06T09:47:14.940411: step 3930, loss 0.015461, acc 1
2016-09-06T09:47:15.749367: step 3931, loss 0.0309989, acc 1
2016-09-06T09:47:16.562887: step 3932, loss 0.00568218, acc 1
2016-09-06T09:47:17.385283: step 3933, loss 0.0535662, acc 0.98
2016-09-06T09:47:18.164559: step 3934, loss 0.0225822, acc 1
2016-09-06T09:47:18.955746: step 3935, loss 0.0535228, acc 0.96
2016-09-06T09:47:19.769292: step 3936, loss 0.0152732, acc 1
2016-09-06T09:47:20.580461: step 3937, loss 0.0072864, acc 1
2016-09-06T09:47:21.388535: step 3938, loss 0.0654825, acc 0.98
2016-09-06T09:47:22.225401: step 3939, loss 0.0586414, acc 0.96
2016-09-06T09:47:23.018747: step 3940, loss 0.0152433, acc 1
2016-09-06T09:47:23.882901: step 3941, loss 0.0336433, acc 0.98
2016-09-06T09:47:24.775917: step 3942, loss 0.0121079, acc 1
2016-09-06T09:47:25.625001: step 3943, loss 0.026774, acc 0.98
2016-09-06T09:47:26.461497: step 3944, loss 0.0602355, acc 0.96
2016-09-06T09:47:27.280093: step 3945, loss 0.0548062, acc 0.96
2016-09-06T09:47:28.077359: step 3946, loss 0.00816735, acc 1
2016-09-06T09:47:28.900516: step 3947, loss 0.0330163, acc 1
2016-09-06T09:47:29.737931: step 3948, loss 0.0103961, acc 1
2016-09-06T09:47:30.534699: step 3949, loss 0.0101224, acc 1
2016-09-06T09:47:31.356476: step 3950, loss 0.0307501, acc 0.98
2016-09-06T09:47:32.192251: step 3951, loss 0.0107084, acc 1
2016-09-06T09:47:32.992292: step 3952, loss 0.0437525, acc 0.98
2016-09-06T09:47:33.789605: step 3953, loss 0.0383041, acc 0.98
2016-09-06T09:47:34.625395: step 3954, loss 0.0308864, acc 0.98
2016-09-06T09:47:35.413851: step 3955, loss 0.0153288, acc 1
2016-09-06T09:47:36.251181: step 3956, loss 0.0300902, acc 0.98
2016-09-06T09:47:37.074196: step 3957, loss 0.0571572, acc 0.98
2016-09-06T09:47:37.889890: step 3958, loss 0.0459668, acc 0.98
2016-09-06T09:47:38.681506: step 3959, loss 0.0266946, acc 0.98
2016-09-06T09:47:39.486744: step 3960, loss 0.0240994, acc 1
2016-09-06T09:47:40.285828: step 3961, loss 0.0590018, acc 0.98
2016-09-06T09:47:41.060388: step 3962, loss 0.190881, acc 0.94
2016-09-06T09:47:41.886015: step 3963, loss 0.0137031, acc 1
2016-09-06T09:47:42.690474: step 3964, loss 0.0435223, acc 0.98
2016-09-06T09:47:43.492178: step 3965, loss 0.0419525, acc 0.98
2016-09-06T09:47:44.304977: step 3966, loss 0.0160714, acc 1
2016-09-06T09:47:45.103782: step 3967, loss 0.0683441, acc 0.98
2016-09-06T09:47:45.901601: step 3968, loss 0.0392698, acc 0.98
2016-09-06T09:47:46.734418: step 3969, loss 0.0223015, acc 0.98
2016-09-06T09:47:47.530344: step 3970, loss 0.0921394, acc 0.96
2016-09-06T09:47:48.338416: step 3971, loss 0.0260885, acc 0.98
2016-09-06T09:47:49.161239: step 3972, loss 0.0204081, acc 0.98
2016-09-06T09:47:49.987293: step 3973, loss 0.0396548, acc 0.96
2016-09-06T09:47:50.782774: step 3974, loss 0.0598027, acc 0.98
2016-09-06T09:47:51.614232: step 3975, loss 0.0616167, acc 0.96
2016-09-06T09:47:52.441522: step 3976, loss 0.0272423, acc 0.98
2016-09-06T09:47:53.236556: step 3977, loss 0.066077, acc 0.98
2016-09-06T09:47:54.025433: step 3978, loss 0.0465444, acc 0.98
2016-09-06T09:47:54.871337: step 3979, loss 0.0135805, acc 1
2016-09-06T09:47:55.674831: step 3980, loss 0.128834, acc 0.96
2016-09-06T09:47:56.497265: step 3981, loss 0.0616212, acc 0.96
2016-09-06T09:47:57.325429: step 3982, loss 0.0197852, acc 1
2016-09-06T09:47:58.117325: step 3983, loss 0.0326875, acc 1
2016-09-06T09:47:58.910922: step 3984, loss 0.107289, acc 0.94
2016-09-06T09:47:59.727601: step 3985, loss 0.0157847, acc 1
2016-09-06T09:48:00.550655: step 3986, loss 0.0504598, acc 0.98
2016-09-06T09:48:01.351493: step 3987, loss 0.0667704, acc 0.98
2016-09-06T09:48:02.165935: step 3988, loss 0.0370145, acc 0.98
2016-09-06T09:48:02.977812: step 3989, loss 0.00458593, acc 1
2016-09-06T09:48:03.796062: step 3990, loss 0.0221216, acc 0.98
2016-09-06T09:48:04.621783: step 3991, loss 0.00593563, acc 1
2016-09-06T09:48:05.451156: step 3992, loss 0.0141331, acc 1
2016-09-06T09:48:06.273359: step 3993, loss 0.071573, acc 0.96
2016-09-06T09:48:07.140346: step 3994, loss 0.0392561, acc 0.98
2016-09-06T09:48:07.967360: step 3995, loss 0.0468158, acc 0.96
2016-09-06T09:48:08.777429: step 3996, loss 0.0691944, acc 0.94
2016-09-06T09:48:09.600350: step 3997, loss 0.0221162, acc 1
2016-09-06T09:48:10.407039: step 3998, loss 0.131179, acc 0.96
2016-09-06T09:48:11.233912: step 3999, loss 0.085903, acc 0.98
2016-09-06T09:48:12.059935: step 4000, loss 0.0772482, acc 0.94

Evaluation:
2016-09-06T09:48:15.784597: step 4000, loss 1.35095, acc 0.763602

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-4000

2016-09-06T09:48:17.734642: step 4001, loss 0.0482515, acc 0.98
2016-09-06T09:48:18.546888: step 4002, loss 0.0614613, acc 0.96
2016-09-06T09:48:19.399721: step 4003, loss 0.00621945, acc 1
2016-09-06T09:48:20.202868: step 4004, loss 0.0457914, acc 0.98
2016-09-06T09:48:21.033858: step 4005, loss 0.00897573, acc 1
2016-09-06T09:48:21.831246: step 4006, loss 0.0257959, acc 0.98
2016-09-06T09:48:22.619037: step 4007, loss 0.0192419, acc 0.98
2016-09-06T09:48:23.429645: step 4008, loss 0.0393424, acc 0.98
2016-09-06T09:48:24.284433: step 4009, loss 0.159066, acc 0.94
2016-09-06T09:48:25.143606: step 4010, loss 0.0380159, acc 0.98
2016-09-06T09:48:25.938028: step 4011, loss 0.0615249, acc 0.96
2016-09-06T09:48:26.736037: step 4012, loss 0.0162272, acc 1
2016-09-06T09:48:27.540984: step 4013, loss 0.0136323, acc 1
2016-09-06T09:48:28.312125: step 4014, loss 0.0450006, acc 0.98
2016-09-06T09:48:29.114844: step 4015, loss 0.116618, acc 0.96
2016-09-06T09:48:29.962365: step 4016, loss 0.0397347, acc 1
2016-09-06T09:48:30.707190: step 4017, loss 0.0282177, acc 0.98
2016-09-06T09:48:31.489617: step 4018, loss 0.042864, acc 0.98
2016-09-06T09:48:32.289410: step 4019, loss 0.0379467, acc 0.98
2016-09-06T09:48:33.093761: step 4020, loss 0.085932, acc 0.96
2016-09-06T09:48:33.904367: step 4021, loss 0.0252791, acc 1
2016-09-06T09:48:34.721345: step 4022, loss 0.0520913, acc 0.98
2016-09-06T09:48:35.534920: step 4023, loss 0.00947117, acc 1
2016-09-06T09:48:36.327009: step 4024, loss 0.0353849, acc 0.98
2016-09-06T09:48:37.142770: step 4025, loss 0.0400689, acc 0.98
2016-09-06T09:48:37.929437: step 4026, loss 0.051014, acc 0.98
2016-09-06T09:48:38.763108: step 4027, loss 0.0420771, acc 0.98
2016-09-06T09:48:39.573037: step 4028, loss 0.0418129, acc 0.98
2016-09-06T09:48:40.371531: step 4029, loss 0.0904294, acc 0.98
2016-09-06T09:48:41.189967: step 4030, loss 0.0712329, acc 0.96
2016-09-06T09:48:42.020162: step 4031, loss 0.0139465, acc 1
2016-09-06T09:48:42.769090: step 4032, loss 0.0072931, acc 1
2016-09-06T09:48:43.590230: step 4033, loss 0.00813275, acc 1
2016-09-06T09:48:44.410051: step 4034, loss 0.00953919, acc 1
2016-09-06T09:48:45.227991: step 4035, loss 0.0611211, acc 0.96
2016-09-06T09:48:46.028784: step 4036, loss 0.0425222, acc 0.96
2016-09-06T09:48:46.868406: step 4037, loss 0.0214642, acc 0.98
2016-09-06T09:48:47.634883: step 4038, loss 0.0258087, acc 1
2016-09-06T09:48:48.432187: step 4039, loss 0.0445389, acc 0.98
2016-09-06T09:48:49.272383: step 4040, loss 0.00844686, acc 1
2016-09-06T09:48:50.058643: step 4041, loss 0.105892, acc 0.92
2016-09-06T09:48:50.869159: step 4042, loss 0.11263, acc 0.94
2016-09-06T09:48:51.678295: step 4043, loss 0.0224903, acc 1
2016-09-06T09:48:52.481956: step 4044, loss 0.00546456, acc 1
2016-09-06T09:48:53.345563: step 4045, loss 0.0307599, acc 0.98
2016-09-06T09:48:54.197048: step 4046, loss 0.0488964, acc 0.98
2016-09-06T09:48:55.025447: step 4047, loss 0.0536545, acc 0.96
2016-09-06T09:48:55.838343: step 4048, loss 0.0327317, acc 0.98
2016-09-06T09:48:56.674776: step 4049, loss 0.0368159, acc 0.98
2016-09-06T09:48:57.496324: step 4050, loss 0.0041108, acc 1
2016-09-06T09:48:58.305843: step 4051, loss 0.0183269, acc 1
2016-09-06T09:48:59.121286: step 4052, loss 0.033138, acc 0.98
2016-09-06T09:48:59.976282: step 4053, loss 0.0391429, acc 0.98
2016-09-06T09:49:00.827056: step 4054, loss 0.021676, acc 1
2016-09-06T09:49:01.686576: step 4055, loss 0.0631355, acc 0.96
2016-09-06T09:49:02.488380: step 4056, loss 0.0349005, acc 1
2016-09-06T09:49:03.266981: step 4057, loss 0.013799, acc 1
2016-09-06T09:49:04.074351: step 4058, loss 0.00880631, acc 1
2016-09-06T09:49:04.898351: step 4059, loss 0.00576603, acc 1
2016-09-06T09:49:05.686253: step 4060, loss 0.0206899, acc 1
2016-09-06T09:49:06.511455: step 4061, loss 0.0158014, acc 1
2016-09-06T09:49:07.336397: step 4062, loss 0.053954, acc 0.98
2016-09-06T09:49:08.153209: step 4063, loss 0.00574396, acc 1
2016-09-06T09:49:08.987124: step 4064, loss 0.0265247, acc 1
2016-09-06T09:49:09.812911: step 4065, loss 0.0351354, acc 1
2016-09-06T09:49:10.608464: step 4066, loss 0.0217671, acc 1
2016-09-06T09:49:11.422044: step 4067, loss 0.0453259, acc 0.96
2016-09-06T09:49:12.260954: step 4068, loss 0.0374939, acc 0.98
2016-09-06T09:49:13.024398: step 4069, loss 0.0421627, acc 1
2016-09-06T09:49:13.812506: step 4070, loss 0.0346062, acc 0.98
2016-09-06T09:49:14.622917: step 4071, loss 0.0167418, acc 1
2016-09-06T09:49:15.432833: step 4072, loss 0.0147052, acc 1
2016-09-06T09:49:16.213980: step 4073, loss 0.00552896, acc 1
2016-09-06T09:49:17.021450: step 4074, loss 0.0098537, acc 1
2016-09-06T09:49:17.853430: step 4075, loss 0.0619433, acc 0.98
2016-09-06T09:49:18.673723: step 4076, loss 0.0388915, acc 0.96
2016-09-06T09:49:19.513583: step 4077, loss 0.0083337, acc 1
2016-09-06T09:49:20.285572: step 4078, loss 0.0202884, acc 1
2016-09-06T09:49:21.081710: step 4079, loss 0.0170495, acc 1
2016-09-06T09:49:21.902510: step 4080, loss 0.00579466, acc 1
2016-09-06T09:49:22.699143: step 4081, loss 0.020485, acc 1
2016-09-06T09:49:23.521260: step 4082, loss 0.015904, acc 1
2016-09-06T09:49:24.343388: step 4083, loss 0.0208678, acc 1
2016-09-06T09:49:25.122795: step 4084, loss 0.104819, acc 0.96
2016-09-06T09:49:25.922149: step 4085, loss 0.0164917, acc 1
2016-09-06T09:49:26.736934: step 4086, loss 0.00433129, acc 1
2016-09-06T09:49:27.509065: step 4087, loss 0.0500389, acc 0.98
2016-09-06T09:49:28.314496: step 4088, loss 0.0170278, acc 1
2016-09-06T09:49:29.113477: step 4089, loss 0.0328548, acc 1
2016-09-06T09:49:29.909819: step 4090, loss 0.0753823, acc 0.96
2016-09-06T09:49:30.735103: step 4091, loss 0.0150697, acc 1
2016-09-06T09:49:31.536770: step 4092, loss 0.0344706, acc 0.98
2016-09-06T09:49:32.322058: step 4093, loss 0.0414192, acc 0.96
2016-09-06T09:49:33.137245: step 4094, loss 0.0630809, acc 0.98
2016-09-06T09:49:33.962487: step 4095, loss 0.00376032, acc 1
2016-09-06T09:49:34.736407: step 4096, loss 0.0039778, acc 1
2016-09-06T09:49:35.541313: step 4097, loss 0.0634483, acc 0.96
2016-09-06T09:49:36.380968: step 4098, loss 0.0813396, acc 0.96
2016-09-06T09:49:37.180263: step 4099, loss 0.104511, acc 0.96
2016-09-06T09:49:37.988587: step 4100, loss 0.0340262, acc 1

Evaluation:
2016-09-06T09:49:41.700038: step 4100, loss 1.7948, acc 0.757974

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-4100

2016-09-06T09:49:43.600558: step 4101, loss 0.0216742, acc 0.98
2016-09-06T09:49:44.411517: step 4102, loss 0.0323668, acc 0.98
2016-09-06T09:49:45.238986: step 4103, loss 0.0727339, acc 0.96
2016-09-06T09:49:46.044700: step 4104, loss 0.0602404, acc 0.98
2016-09-06T09:49:46.902545: step 4105, loss 0.0134457, acc 1
2016-09-06T09:49:47.708042: step 4106, loss 0.0557902, acc 0.96
2016-09-06T09:49:48.562718: step 4107, loss 0.00694935, acc 1
2016-09-06T09:49:49.375046: step 4108, loss 0.0299478, acc 0.98
2016-09-06T09:49:50.197605: step 4109, loss 0.00416023, acc 1
2016-09-06T09:49:51.037965: step 4110, loss 0.0652822, acc 1
2016-09-06T09:49:51.831708: step 4111, loss 0.012339, acc 1
2016-09-06T09:49:52.654521: step 4112, loss 0.0296241, acc 0.98
2016-09-06T09:49:53.499284: step 4113, loss 0.0360398, acc 0.98
2016-09-06T09:49:54.312714: step 4114, loss 0.120909, acc 0.96
2016-09-06T09:49:55.139481: step 4115, loss 0.080371, acc 0.96
2016-09-06T09:49:55.996788: step 4116, loss 0.00438185, acc 1
2016-09-06T09:49:56.798164: step 4117, loss 0.0922095, acc 0.96
2016-09-06T09:49:57.595855: step 4118, loss 0.025947, acc 1
2016-09-06T09:49:58.423282: step 4119, loss 0.00684091, acc 1
2016-09-06T09:49:59.247541: step 4120, loss 0.0652671, acc 0.96
2016-09-06T09:50:00.071237: step 4121, loss 0.0224083, acc 0.98
2016-09-06T09:50:00.899316: step 4122, loss 0.00565389, acc 1
2016-09-06T09:50:01.718649: step 4123, loss 0.0199852, acc 1
2016-09-06T09:50:02.528676: step 4124, loss 0.00508425, acc 1
2016-09-06T09:50:03.381043: step 4125, loss 0.0215314, acc 1
2016-09-06T09:50:04.220042: step 4126, loss 0.00854989, acc 1
2016-09-06T09:50:05.027851: step 4127, loss 0.0143693, acc 1
2016-09-06T09:50:05.837740: step 4128, loss 0.0102309, acc 1
2016-09-06T09:50:06.681819: step 4129, loss 0.0161613, acc 1
2016-09-06T09:50:07.452301: step 4130, loss 0.0230475, acc 1
2016-09-06T09:50:08.285435: step 4131, loss 0.0419391, acc 0.98
2016-09-06T09:50:09.100350: step 4132, loss 0.0399227, acc 0.96
2016-09-06T09:50:09.885900: step 4133, loss 0.0456344, acc 0.98
2016-09-06T09:50:10.671695: step 4134, loss 0.0219215, acc 0.98
2016-09-06T09:50:11.528741: step 4135, loss 0.0122275, acc 1
2016-09-06T09:50:12.327821: step 4136, loss 0.0332213, acc 1
2016-09-06T09:50:13.139450: step 4137, loss 0.0253924, acc 0.98
2016-09-06T09:50:13.940932: step 4138, loss 0.0370829, acc 1
2016-09-06T09:50:14.736958: step 4139, loss 0.0673203, acc 0.98
2016-09-06T09:50:15.545981: step 4140, loss 0.0227505, acc 0.98
2016-09-06T09:50:16.370484: step 4141, loss 0.0339134, acc 1
2016-09-06T09:50:17.158831: step 4142, loss 0.0950814, acc 0.96
2016-09-06T09:50:17.948570: step 4143, loss 0.0351303, acc 0.98
2016-09-06T09:50:18.754300: step 4144, loss 0.100036, acc 0.94
2016-09-06T09:50:19.541769: step 4145, loss 0.0525997, acc 0.98
2016-09-06T09:50:20.345640: step 4146, loss 0.0829115, acc 0.98
2016-09-06T09:50:21.158003: step 4147, loss 0.0768814, acc 0.98
2016-09-06T09:50:21.944769: step 4148, loss 0.0780757, acc 0.96
2016-09-06T09:50:22.757793: step 4149, loss 0.0497631, acc 0.98
2016-09-06T09:50:23.560977: step 4150, loss 0.0130415, acc 1
2016-09-06T09:50:24.346550: step 4151, loss 0.0406661, acc 0.98
2016-09-06T09:50:25.141021: step 4152, loss 0.040262, acc 0.98
2016-09-06T09:50:25.956362: step 4153, loss 0.0416886, acc 0.98
2016-09-06T09:50:26.788069: step 4154, loss 0.0471836, acc 0.98
2016-09-06T09:50:27.609455: step 4155, loss 0.00376486, acc 1
2016-09-06T09:50:28.418304: step 4156, loss 0.0223848, acc 1
2016-09-06T09:50:29.210281: step 4157, loss 0.0309168, acc 1
2016-09-06T09:50:30.016350: step 4158, loss 0.0178529, acc 1
2016-09-06T09:50:30.846558: step 4159, loss 0.0991505, acc 0.98
2016-09-06T09:50:31.661594: step 4160, loss 0.021864, acc 0.98
2016-09-06T09:50:32.478157: step 4161, loss 0.0481999, acc 0.96
2016-09-06T09:50:33.271678: step 4162, loss 0.04954, acc 1
2016-09-06T09:50:34.077757: step 4163, loss 0.0378084, acc 1
2016-09-06T09:50:34.883460: step 4164, loss 0.0504328, acc 0.96
2016-09-06T09:50:35.703230: step 4165, loss 0.0495182, acc 0.98
2016-09-06T09:50:36.492446: step 4166, loss 0.0560192, acc 0.98
2016-09-06T09:50:37.295095: step 4167, loss 0.0557112, acc 0.98
2016-09-06T09:50:38.096625: step 4168, loss 0.00903057, acc 1
2016-09-06T09:50:38.884308: step 4169, loss 0.00518891, acc 1
2016-09-06T09:50:39.665464: step 4170, loss 0.0673831, acc 0.94
2016-09-06T09:50:40.518675: step 4171, loss 0.0629617, acc 0.98
2016-09-06T09:50:41.357269: step 4172, loss 0.035262, acc 0.98
2016-09-06T09:50:42.169248: step 4173, loss 0.0368254, acc 1
2016-09-06T09:50:42.998796: step 4174, loss 0.0094521, acc 1
2016-09-06T09:50:43.803670: step 4175, loss 0.0296438, acc 0.98
2016-09-06T09:50:44.615477: step 4176, loss 0.0258374, acc 1
2016-09-06T09:50:45.462999: step 4177, loss 0.0373001, acc 0.98
2016-09-06T09:50:46.269750: step 4178, loss 0.154724, acc 0.96
2016-09-06T09:50:47.086862: step 4179, loss 0.0290677, acc 1
2016-09-06T09:50:47.934386: step 4180, loss 0.0260247, acc 1
2016-09-06T09:50:48.777458: step 4181, loss 0.0222427, acc 0.98
2016-09-06T09:50:49.588978: step 4182, loss 0.0281434, acc 1
2016-09-06T09:50:50.398536: step 4183, loss 0.00354259, acc 1
2016-09-06T09:50:51.226198: step 4184, loss 0.00777336, acc 1
2016-09-06T09:50:52.027643: step 4185, loss 0.0176171, acc 1
2016-09-06T09:50:52.862343: step 4186, loss 0.00480009, acc 1
2016-09-06T09:50:53.669250: step 4187, loss 0.0328213, acc 0.98
2016-09-06T09:50:54.500642: step 4188, loss 0.0199428, acc 0.98
2016-09-06T09:50:55.325576: step 4189, loss 0.0407146, acc 0.98
2016-09-06T09:50:56.140796: step 4190, loss 0.0614766, acc 0.96
2016-09-06T09:50:56.971982: step 4191, loss 0.00438813, acc 1
2016-09-06T09:50:57.796977: step 4192, loss 0.0402413, acc 1
2016-09-06T09:50:58.643026: step 4193, loss 0.0266928, acc 1
2016-09-06T09:50:59.421430: step 4194, loss 0.0411114, acc 0.98
2016-09-06T09:51:00.248907: step 4195, loss 0.0284465, acc 0.98
2016-09-06T09:51:01.100725: step 4196, loss 0.0117046, acc 1
2016-09-06T09:51:01.915422: step 4197, loss 0.0115953, acc 1
2016-09-06T09:51:02.718789: step 4198, loss 0.0227645, acc 1
2016-09-06T09:51:03.547512: step 4199, loss 0.0366358, acc 0.96
2016-09-06T09:51:04.366831: step 4200, loss 0.00596753, acc 1

Evaluation:
2016-09-06T09:51:08.116033: step 4200, loss 1.94161, acc 0.761726

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-4200

2016-09-06T09:51:10.049533: step 4201, loss 0.0996284, acc 0.96
2016-09-06T09:51:10.867536: step 4202, loss 0.0134794, acc 1
2016-09-06T09:51:11.695501: step 4203, loss 0.0995255, acc 0.96
2016-09-06T09:51:12.493256: step 4204, loss 0.00871748, acc 1
2016-09-06T09:51:13.319165: step 4205, loss 0.04777, acc 0.98
2016-09-06T09:51:14.128778: step 4206, loss 0.0270067, acc 1
2016-09-06T09:51:14.973412: step 4207, loss 0.0686373, acc 0.96
2016-09-06T09:51:15.813796: step 4208, loss 0.040489, acc 0.98
2016-09-06T09:51:16.656197: step 4209, loss 0.0340246, acc 0.98
2016-09-06T09:51:17.461017: step 4210, loss 0.193619, acc 0.92
2016-09-06T09:51:18.277586: step 4211, loss 0.0201362, acc 0.98
2016-09-06T09:51:19.091453: step 4212, loss 0.0624167, acc 0.96
2016-09-06T09:51:19.933375: step 4213, loss 0.0431075, acc 0.98
2016-09-06T09:51:20.752200: step 4214, loss 0.038724, acc 0.98
2016-09-06T09:51:21.537145: step 4215, loss 0.0449652, acc 0.98
2016-09-06T09:51:22.333388: step 4216, loss 0.0216438, acc 0.98
2016-09-06T09:51:23.153888: step 4217, loss 0.0248916, acc 1
2016-09-06T09:51:23.975288: step 4218, loss 0.0313777, acc 1
2016-09-06T09:51:24.781844: step 4219, loss 0.0204485, acc 0.98
2016-09-06T09:51:25.607584: step 4220, loss 0.0151104, acc 1
2016-09-06T09:51:26.432287: step 4221, loss 0.0354738, acc 1
2016-09-06T09:51:27.253353: step 4222, loss 0.029747, acc 0.98
2016-09-06T09:51:28.058664: step 4223, loss 0.0405683, acc 1
2016-09-06T09:51:28.843107: step 4224, loss 0.00833953, acc 1
2016-09-06T09:51:29.645586: step 4225, loss 0.0704744, acc 0.98
2016-09-06T09:51:30.465961: step 4226, loss 0.0507436, acc 0.96
2016-09-06T09:51:31.256098: step 4227, loss 0.025443, acc 0.98
2016-09-06T09:51:32.046792: step 4228, loss 0.0462055, acc 0.98
2016-09-06T09:51:32.892994: step 4229, loss 0.0612447, acc 0.98
2016-09-06T09:51:33.748165: step 4230, loss 0.0412445, acc 1
2016-09-06T09:51:34.554686: step 4231, loss 0.081578, acc 0.96
2016-09-06T09:51:35.353039: step 4232, loss 0.0188527, acc 1
2016-09-06T09:51:36.190536: step 4233, loss 0.0510627, acc 0.96
2016-09-06T09:51:37.010720: step 4234, loss 0.0572823, acc 0.98
2016-09-06T09:51:37.831987: step 4235, loss 0.00621888, acc 1
2016-09-06T09:51:38.670110: step 4236, loss 0.0114262, acc 1
2016-09-06T09:51:39.461448: step 4237, loss 0.120064, acc 0.96
2016-09-06T09:51:40.289568: step 4238, loss 0.0373406, acc 0.98
2016-09-06T09:51:41.111584: step 4239, loss 0.0322324, acc 0.98
2016-09-06T09:51:41.947794: step 4240, loss 0.022569, acc 1
2016-09-06T09:51:42.764795: step 4241, loss 0.0341522, acc 0.98
2016-09-06T09:51:43.577726: step 4242, loss 0.0188264, acc 1
2016-09-06T09:51:44.382792: step 4243, loss 0.0264273, acc 1
2016-09-06T09:51:45.188066: step 4244, loss 0.0214983, acc 1
2016-09-06T09:51:46.028793: step 4245, loss 0.011397, acc 1
2016-09-06T09:51:46.836680: step 4246, loss 0.0788674, acc 0.96
2016-09-06T09:51:47.658514: step 4247, loss 0.0327838, acc 0.98
2016-09-06T09:51:48.506367: step 4248, loss 0.0736728, acc 0.96
2016-09-06T09:51:49.333093: step 4249, loss 0.0338795, acc 0.98
2016-09-06T09:51:50.149242: step 4250, loss 0.0216872, acc 1
2016-09-06T09:51:50.981436: step 4251, loss 0.0805719, acc 0.98
2016-09-06T09:51:51.797821: step 4252, loss 0.0406051, acc 0.98
2016-09-06T09:51:52.589447: step 4253, loss 0.00553139, acc 1
2016-09-06T09:51:53.411057: step 4254, loss 0.0372815, acc 1
2016-09-06T09:51:54.235889: step 4255, loss 0.00448659, acc 1
2016-09-06T09:51:55.032725: step 4256, loss 0.047346, acc 0.98
2016-09-06T09:51:55.818685: step 4257, loss 0.0456782, acc 0.98
2016-09-06T09:51:56.650242: step 4258, loss 0.00578579, acc 1
2016-09-06T09:51:57.473907: step 4259, loss 0.00489259, acc 1
2016-09-06T09:51:58.272727: step 4260, loss 0.030119, acc 0.98
2016-09-06T09:51:59.105463: step 4261, loss 0.0188276, acc 0.98
2016-09-06T09:51:59.917926: step 4262, loss 0.0284754, acc 1
2016-09-06T09:52:00.713403: step 4263, loss 0.0419958, acc 0.98
2016-09-06T09:52:01.528671: step 4264, loss 0.0428051, acc 1
2016-09-06T09:52:02.316725: step 4265, loss 0.0212728, acc 1
2016-09-06T09:52:03.109628: step 4266, loss 0.0592852, acc 0.98
2016-09-06T09:52:03.926311: step 4267, loss 0.0426061, acc 0.96
2016-09-06T09:52:04.738861: step 4268, loss 0.00339103, acc 1
2016-09-06T09:52:05.548482: step 4269, loss 0.0153265, acc 1
2016-09-06T09:52:06.368607: step 4270, loss 0.0544428, acc 0.98
2016-09-06T09:52:07.165683: step 4271, loss 0.0327747, acc 0.98
2016-09-06T09:52:07.985537: step 4272, loss 0.0192081, acc 1
2016-09-06T09:52:08.804113: step 4273, loss 0.0239153, acc 1
2016-09-06T09:52:09.611481: step 4274, loss 0.00400493, acc 1
2016-09-06T09:52:10.414923: step 4275, loss 0.0711757, acc 0.98
2016-09-06T09:52:11.244652: step 4276, loss 0.0295643, acc 1
2016-09-06T09:52:12.068647: step 4277, loss 0.0207476, acc 1
2016-09-06T09:52:12.872185: step 4278, loss 0.0306107, acc 0.98
2016-09-06T09:52:13.689514: step 4279, loss 0.0317939, acc 0.98
2016-09-06T09:52:14.490725: step 4280, loss 0.00342094, acc 1
2016-09-06T09:52:15.322349: step 4281, loss 0.0808279, acc 0.98
2016-09-06T09:52:16.164602: step 4282, loss 0.0531011, acc 0.96
2016-09-06T09:52:16.981849: step 4283, loss 0.0207041, acc 1
2016-09-06T09:52:17.801367: step 4284, loss 0.00360722, acc 1
2016-09-06T09:52:18.599968: step 4285, loss 0.0835445, acc 0.94
2016-09-06T09:52:19.421681: step 4286, loss 0.0619253, acc 0.98
2016-09-06T09:52:20.215432: step 4287, loss 0.00437756, acc 1
2016-09-06T09:52:21.078997: step 4288, loss 0.0443207, acc 0.96
2016-09-06T09:52:21.884176: step 4289, loss 0.0309167, acc 0.98
2016-09-06T09:52:22.686864: step 4290, loss 0.082781, acc 0.94
2016-09-06T09:52:23.513968: step 4291, loss 0.00476211, acc 1
2016-09-06T09:52:24.392212: step 4292, loss 0.198693, acc 0.92
2016-09-06T09:52:25.203869: step 4293, loss 0.0529727, acc 0.96
2016-09-06T09:52:26.018894: step 4294, loss 0.0307486, acc 0.98
2016-09-06T09:52:26.861127: step 4295, loss 0.0242232, acc 1
2016-09-06T09:52:27.666093: step 4296, loss 0.0090468, acc 1
2016-09-06T09:52:28.532695: step 4297, loss 0.0148849, acc 1
2016-09-06T09:52:29.399748: step 4298, loss 0.00837384, acc 1
2016-09-06T09:52:30.225651: step 4299, loss 0.0167193, acc 1
2016-09-06T09:52:31.051307: step 4300, loss 0.0469084, acc 1

Evaluation:
2016-09-06T09:52:34.843371: step 4300, loss 1.54136, acc 0.752345

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-4300

2016-09-06T09:52:36.800900: step 4301, loss 0.0543742, acc 0.98
2016-09-06T09:52:37.618462: step 4302, loss 0.0559393, acc 0.96
2016-09-06T09:52:38.411421: step 4303, loss 0.0176534, acc 0.98
2016-09-06T09:52:39.241118: step 4304, loss 0.0157093, acc 1
2016-09-06T09:52:40.039336: step 4305, loss 0.0287844, acc 1
2016-09-06T09:52:40.835541: step 4306, loss 0.00334806, acc 1
2016-09-06T09:52:41.662126: step 4307, loss 0.0247031, acc 1
2016-09-06T09:52:42.486396: step 4308, loss 0.0359474, acc 0.98
2016-09-06T09:52:43.276662: step 4309, loss 0.0357256, acc 0.96
2016-09-06T09:52:44.107726: step 4310, loss 0.0426244, acc 0.98
2016-09-06T09:52:44.931976: step 4311, loss 0.0147676, acc 1
2016-09-06T09:52:45.738216: step 4312, loss 0.102245, acc 0.96
2016-09-06T09:52:46.581629: step 4313, loss 0.0389314, acc 1
2016-09-06T09:52:47.406789: step 4314, loss 0.00366539, acc 1
2016-09-06T09:52:48.225592: step 4315, loss 0.0129015, acc 1
2016-09-06T09:52:49.107936: step 4316, loss 0.00508285, acc 1
2016-09-06T09:52:49.915603: step 4317, loss 0.0237106, acc 1
2016-09-06T09:52:50.709481: step 4318, loss 0.0108962, acc 1
2016-09-06T09:52:51.518173: step 4319, loss 0.04033, acc 1
2016-09-06T09:52:52.329384: step 4320, loss 0.0318769, acc 1
2016-09-06T09:52:53.125901: step 4321, loss 0.0128262, acc 1
2016-09-06T09:52:53.957583: step 4322, loss 0.0382716, acc 0.98
2016-09-06T09:52:54.785707: step 4323, loss 0.01784, acc 0.98
2016-09-06T09:52:55.574073: step 4324, loss 0.0439944, acc 0.98
2016-09-06T09:52:56.379967: step 4325, loss 0.101723, acc 0.94
2016-09-06T09:52:57.206872: step 4326, loss 0.00474188, acc 1
2016-09-06T09:52:58.018544: step 4327, loss 0.00281863, acc 1
2016-09-06T09:52:58.862485: step 4328, loss 0.011872, acc 1
2016-09-06T09:52:59.696182: step 4329, loss 0.0506794, acc 0.98
2016-09-06T09:53:00.548309: step 4330, loss 0.0362687, acc 1
2016-09-06T09:53:01.353150: step 4331, loss 0.0291288, acc 0.98
2016-09-06T09:53:02.211315: step 4332, loss 0.00865166, acc 1
2016-09-06T09:53:03.012968: step 4333, loss 0.0287142, acc 1
2016-09-06T09:53:03.811740: step 4334, loss 0.113452, acc 0.94
2016-09-06T09:53:04.650941: step 4335, loss 0.0295415, acc 1
2016-09-06T09:53:05.459333: step 4336, loss 0.0190947, acc 1
2016-09-06T09:53:06.277515: step 4337, loss 0.0257297, acc 1
2016-09-06T09:53:07.097864: step 4338, loss 0.00834554, acc 1
2016-09-06T09:53:07.903381: step 4339, loss 0.0396699, acc 0.98
2016-09-06T09:53:08.719955: step 4340, loss 0.0169331, acc 0.98
2016-09-06T09:53:09.563869: step 4341, loss 0.0378659, acc 0.98
2016-09-06T09:53:10.400870: step 4342, loss 0.0169779, acc 1
2016-09-06T09:53:11.208058: step 4343, loss 0.0242271, acc 1
2016-09-06T09:53:12.014768: step 4344, loss 0.0137683, acc 1
2016-09-06T09:53:12.837276: step 4345, loss 0.00310161, acc 1
2016-09-06T09:53:13.631831: step 4346, loss 0.053153, acc 0.98
2016-09-06T09:53:14.419092: step 4347, loss 0.029777, acc 0.98
2016-09-06T09:53:15.241900: step 4348, loss 0.00779539, acc 1
2016-09-06T09:53:16.029398: step 4349, loss 0.057569, acc 0.96
2016-09-06T09:53:16.834284: step 4350, loss 0.00577231, acc 1
2016-09-06T09:53:17.654271: step 4351, loss 0.023571, acc 1
2016-09-06T09:53:18.451782: step 4352, loss 0.0417408, acc 0.98
2016-09-06T09:53:19.258275: step 4353, loss 0.0868277, acc 0.96
2016-09-06T09:53:20.076484: step 4354, loss 0.0611664, acc 0.96
2016-09-06T09:53:20.857339: step 4355, loss 0.0185143, acc 1
2016-09-06T09:53:21.657457: step 4356, loss 0.0418656, acc 0.98
2016-09-06T09:53:22.456546: step 4357, loss 0.00466659, acc 1
2016-09-06T09:53:23.266172: step 4358, loss 0.0431818, acc 0.96
2016-09-06T09:53:24.085282: step 4359, loss 0.0137603, acc 1
2016-09-06T09:53:24.882203: step 4360, loss 0.0113569, acc 1
2016-09-06T09:53:25.709820: step 4361, loss 0.021372, acc 1
2016-09-06T09:53:26.510261: step 4362, loss 0.0349608, acc 0.98
2016-09-06T09:53:27.324318: step 4363, loss 0.00971865, acc 1
2016-09-06T09:53:28.104004: step 4364, loss 0.00538155, acc 1
2016-09-06T09:53:28.932010: step 4365, loss 0.101954, acc 0.94
2016-09-06T09:53:29.765344: step 4366, loss 0.0150711, acc 1
2016-09-06T09:53:30.563268: step 4367, loss 0.0273171, acc 0.98
2016-09-06T09:53:31.374252: step 4368, loss 0.0764185, acc 0.96
2016-09-06T09:53:32.211479: step 4369, loss 0.029657, acc 0.98
2016-09-06T09:53:33.023011: step 4370, loss 0.0457014, acc 0.96
2016-09-06T09:53:33.823565: step 4371, loss 0.0240938, acc 0.98
2016-09-06T09:53:34.626726: step 4372, loss 0.0406927, acc 0.96
2016-09-06T09:53:35.418651: step 4373, loss 0.0184604, acc 0.98
2016-09-06T09:53:36.217413: step 4374, loss 0.119735, acc 0.96
2016-09-06T09:53:37.014715: step 4375, loss 0.0795921, acc 0.98
2016-09-06T09:53:37.804665: step 4376, loss 0.0169525, acc 0.98
2016-09-06T09:53:38.613845: step 4377, loss 0.0215141, acc 0.98
2016-09-06T09:53:39.447728: step 4378, loss 0.00423005, acc 1
2016-09-06T09:53:40.210922: step 4379, loss 0.191801, acc 0.96
2016-09-06T09:53:41.029162: step 4380, loss 0.0453583, acc 0.96
2016-09-06T09:53:41.837245: step 4381, loss 0.0144735, acc 1
2016-09-06T09:53:42.639732: step 4382, loss 0.0341067, acc 0.98
2016-09-06T09:53:43.441469: step 4383, loss 0.0249248, acc 0.98
2016-09-06T09:53:44.262599: step 4384, loss 0.0568018, acc 0.98
2016-09-06T09:53:45.081411: step 4385, loss 0.0352589, acc 1
2016-09-06T09:53:45.890922: step 4386, loss 0.0327496, acc 1
2016-09-06T09:53:46.741938: step 4387, loss 0.0658391, acc 0.96
2016-09-06T09:53:47.545581: step 4388, loss 0.0185824, acc 1
2016-09-06T09:53:48.359557: step 4389, loss 0.0204807, acc 1
2016-09-06T09:53:49.193617: step 4390, loss 0.0543998, acc 0.98
2016-09-06T09:53:49.996441: step 4391, loss 0.0568126, acc 0.98
2016-09-06T09:53:50.827257: step 4392, loss 0.0343146, acc 1
2016-09-06T09:53:51.661898: step 4393, loss 0.0562975, acc 0.96
2016-09-06T09:53:52.510002: step 4394, loss 0.0494326, acc 0.98
2016-09-06T09:53:53.319014: step 4395, loss 0.0290532, acc 1
2016-09-06T09:53:54.166157: step 4396, loss 0.0127932, acc 1
2016-09-06T09:53:55.011180: step 4397, loss 0.00650237, acc 1
2016-09-06T09:53:55.859907: step 4398, loss 0.0302568, acc 0.98
2016-09-06T09:53:56.707259: step 4399, loss 0.0967615, acc 0.94
2016-09-06T09:53:57.515909: step 4400, loss 0.0183731, acc 1

Evaluation:
2016-09-06T09:54:01.271452: step 4400, loss 1.55865, acc 0.757974

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-4400

2016-09-06T09:54:03.221102: step 4401, loss 0.0479345, acc 0.98
2016-09-06T09:54:04.063028: step 4402, loss 0.0371001, acc 0.98
2016-09-06T09:54:04.875448: step 4403, loss 0.016775, acc 1
2016-09-06T09:54:05.733407: step 4404, loss 0.0555991, acc 0.96
2016-09-06T09:54:06.553402: step 4405, loss 0.0318318, acc 0.98
2016-09-06T09:54:07.375568: step 4406, loss 0.0239034, acc 0.98
2016-09-06T09:54:08.165398: step 4407, loss 0.0187211, acc 1
2016-09-06T09:54:08.975967: step 4408, loss 0.0204968, acc 1
2016-09-06T09:54:09.782933: step 4409, loss 0.0464554, acc 1
2016-09-06T09:54:10.544940: step 4410, loss 0.0723578, acc 0.98
2016-09-06T09:54:11.350799: step 4411, loss 0.030224, acc 0.98
2016-09-06T09:54:12.180371: step 4412, loss 0.00784761, acc 1
2016-09-06T09:54:12.962163: step 4413, loss 0.0684978, acc 0.98
2016-09-06T09:54:13.759932: step 4414, loss 0.0186596, acc 0.98
2016-09-06T09:54:14.575076: step 4415, loss 0.0289545, acc 0.98
2016-09-06T09:54:15.338308: step 4416, loss 0.100981, acc 0.954545
2016-09-06T09:54:16.177077: step 4417, loss 0.05703, acc 0.98
2016-09-06T09:54:17.014007: step 4418, loss 0.027871, acc 1
2016-09-06T09:54:17.805948: step 4419, loss 0.0336524, acc 0.98
2016-09-06T09:54:18.643958: step 4420, loss 0.0150374, acc 1
2016-09-06T09:54:19.458997: step 4421, loss 0.0611508, acc 0.96
2016-09-06T09:54:20.239728: step 4422, loss 0.0100568, acc 1
2016-09-06T09:54:21.034668: step 4423, loss 0.0339524, acc 0.98
2016-09-06T09:54:21.867516: step 4424, loss 0.0331781, acc 0.98
2016-09-06T09:54:22.620850: step 4425, loss 0.0400017, acc 1
2016-09-06T09:54:23.429847: step 4426, loss 0.0470324, acc 0.98
2016-09-06T09:54:24.256138: step 4427, loss 0.0101676, acc 1
2016-09-06T09:54:25.046944: step 4428, loss 0.0319474, acc 0.98
2016-09-06T09:54:25.842051: step 4429, loss 0.0489951, acc 0.96
2016-09-06T09:54:26.678197: step 4430, loss 0.053123, acc 0.98
2016-09-06T09:54:27.468990: step 4431, loss 0.0861798, acc 0.98
2016-09-06T09:54:28.309338: step 4432, loss 0.00529717, acc 1
2016-09-06T09:54:29.123156: step 4433, loss 0.00809808, acc 1
2016-09-06T09:54:29.905617: step 4434, loss 0.031493, acc 1
2016-09-06T09:54:30.696170: step 4435, loss 0.0419077, acc 0.98
2016-09-06T09:54:31.551950: step 4436, loss 0.0175109, acc 1
2016-09-06T09:54:32.348022: step 4437, loss 0.00457183, acc 1
2016-09-06T09:54:33.138952: step 4438, loss 0.0264377, acc 0.98
2016-09-06T09:54:33.963188: step 4439, loss 0.0271134, acc 0.98
2016-09-06T09:54:34.758093: step 4440, loss 0.0238467, acc 0.98
2016-09-06T09:54:35.562575: step 4441, loss 0.0678414, acc 0.98
2016-09-06T09:54:36.623497: step 4442, loss 0.14623, acc 0.94
2016-09-06T09:54:37.440647: step 4443, loss 0.0171181, acc 1
2016-09-06T09:54:38.263300: step 4444, loss 0.0277388, acc 0.98
2016-09-06T09:54:39.102726: step 4445, loss 0.0295976, acc 0.98
2016-09-06T09:54:39.908817: step 4446, loss 0.0455299, acc 0.98
2016-09-06T09:54:40.731841: step 4447, loss 0.00449213, acc 1
2016-09-06T09:54:41.545688: step 4448, loss 0.0291298, acc 1
2016-09-06T09:54:42.401445: step 4449, loss 0.0572492, acc 0.96
2016-09-06T09:54:43.169158: step 4450, loss 0.0352481, acc 0.98
2016-09-06T09:54:43.992131: step 4451, loss 0.00960313, acc 1
2016-09-06T09:54:44.835277: step 4452, loss 0.00289589, acc 1
2016-09-06T09:54:45.602857: step 4453, loss 0.0287319, acc 0.98
2016-09-06T09:54:46.384901: step 4454, loss 0.021574, acc 1
2016-09-06T09:54:47.237045: step 4455, loss 0.0362061, acc 0.98
2016-09-06T09:54:48.015268: step 4456, loss 0.100934, acc 0.94
2016-09-06T09:54:48.849656: step 4457, loss 0.0529403, acc 0.98
2016-09-06T09:54:49.653443: step 4458, loss 0.0194631, acc 1
2016-09-06T09:54:50.431762: step 4459, loss 0.0573594, acc 0.98
2016-09-06T09:54:51.225407: step 4460, loss 0.00482345, acc 1
2016-09-06T09:54:52.044048: step 4461, loss 0.00397663, acc 1
2016-09-06T09:54:52.826618: step 4462, loss 0.0316567, acc 0.98
2016-09-06T09:54:53.670669: step 4463, loss 0.0128935, acc 1
2016-09-06T09:54:54.488496: step 4464, loss 0.0262681, acc 0.98
2016-09-06T09:54:55.268963: step 4465, loss 0.0297334, acc 0.98
2016-09-06T09:54:56.062370: step 4466, loss 0.0386773, acc 0.98
2016-09-06T09:54:56.905954: step 4467, loss 0.0271698, acc 0.98
2016-09-06T09:54:57.709458: step 4468, loss 0.0912517, acc 0.92
2016-09-06T09:54:58.490257: step 4469, loss 0.0413686, acc 0.98
2016-09-06T09:54:59.305159: step 4470, loss 0.0192921, acc 1
2016-09-06T09:55:00.098855: step 4471, loss 0.180732, acc 0.92
2016-09-06T09:55:00.925797: step 4472, loss 0.0148311, acc 1
2016-09-06T09:55:01.740038: step 4473, loss 0.0325088, acc 0.98
2016-09-06T09:55:02.509853: step 4474, loss 0.0516565, acc 1
2016-09-06T09:55:03.298275: step 4475, loss 0.0181799, acc 1
2016-09-06T09:55:04.127278: step 4476, loss 0.02065, acc 1
2016-09-06T09:55:04.925041: step 4477, loss 0.0890301, acc 0.96
2016-09-06T09:55:05.745790: step 4478, loss 0.106434, acc 0.96
2016-09-06T09:55:06.567866: step 4479, loss 0.005292, acc 1
2016-09-06T09:55:07.347393: step 4480, loss 0.0327265, acc 0.98
2016-09-06T09:55:08.141952: step 4481, loss 0.00894637, acc 1
2016-09-06T09:55:08.986964: step 4482, loss 0.0374245, acc 1
2016-09-06T09:55:09.787170: step 4483, loss 0.0199551, acc 0.98
2016-09-06T09:55:10.564653: step 4484, loss 0.088406, acc 0.96
2016-09-06T09:55:11.371810: step 4485, loss 0.0393926, acc 0.98
2016-09-06T09:55:12.163808: step 4486, loss 0.0526543, acc 0.96
2016-09-06T09:55:12.969839: step 4487, loss 0.0309742, acc 0.98
2016-09-06T09:55:13.789347: step 4488, loss 0.014551, acc 1
2016-09-06T09:55:14.635670: step 4489, loss 0.0277538, acc 0.98
2016-09-06T09:55:15.444287: step 4490, loss 0.0957941, acc 0.98
2016-09-06T09:55:16.232171: step 4491, loss 0.0373525, acc 1
2016-09-06T09:55:16.986885: step 4492, loss 0.0182139, acc 1
2016-09-06T09:55:17.805540: step 4493, loss 0.023878, acc 1
2016-09-06T09:55:18.661354: step 4494, loss 0.0232296, acc 1
2016-09-06T09:55:19.475076: step 4495, loss 0.0055914, acc 1
2016-09-06T09:55:20.291704: step 4496, loss 0.00673174, acc 1
2016-09-06T09:55:21.098856: step 4497, loss 0.0280056, acc 0.98
2016-09-06T09:55:21.897887: step 4498, loss 0.0122631, acc 1
2016-09-06T09:55:22.698963: step 4499, loss 0.0206896, acc 1
2016-09-06T09:55:23.527598: step 4500, loss 0.0282989, acc 0.98

Evaluation:
2016-09-06T09:55:27.293958: step 4500, loss 2.46697, acc 0.744841

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-4500

2016-09-06T09:55:29.202435: step 4501, loss 0.00573101, acc 1
2016-09-06T09:55:30.036602: step 4502, loss 0.0231327, acc 1
2016-09-06T09:55:30.862492: step 4503, loss 0.0158911, acc 1
2016-09-06T09:55:31.690361: step 4504, loss 0.0979659, acc 0.98
2016-09-06T09:55:32.517552: step 4505, loss 0.0265458, acc 0.98
2016-09-06T09:55:33.366356: step 4506, loss 0.0538515, acc 0.96
2016-09-06T09:55:34.172285: step 4507, loss 0.0846251, acc 0.98
2016-09-06T09:55:34.986361: step 4508, loss 0.0533911, acc 0.98
2016-09-06T09:55:35.806156: step 4509, loss 0.0161621, acc 1
2016-09-06T09:55:36.650577: step 4510, loss 0.00524868, acc 1
2016-09-06T09:55:37.455601: step 4511, loss 0.105621, acc 0.98
2016-09-06T09:55:38.311060: step 4512, loss 0.0240342, acc 0.98
2016-09-06T09:55:39.111883: step 4513, loss 0.034729, acc 0.98
2016-09-06T09:55:39.923668: step 4514, loss 0.0284999, acc 0.98
2016-09-06T09:55:40.753749: step 4515, loss 0.0730777, acc 0.96
2016-09-06T09:55:41.554907: step 4516, loss 0.0389461, acc 1
2016-09-06T09:55:42.323444: step 4517, loss 0.0316151, acc 1
2016-09-06T09:55:43.137436: step 4518, loss 0.00554856, acc 1
2016-09-06T09:55:43.968671: step 4519, loss 0.00768707, acc 1
2016-09-06T09:55:44.773609: step 4520, loss 0.016315, acc 1
2016-09-06T09:55:45.582972: step 4521, loss 0.0386124, acc 0.98
2016-09-06T09:55:46.375801: step 4522, loss 0.00542386, acc 1
2016-09-06T09:55:47.185681: step 4523, loss 0.0449267, acc 0.98
2016-09-06T09:55:48.041801: step 4524, loss 0.0361062, acc 0.96
2016-09-06T09:55:48.861393: step 4525, loss 0.0269487, acc 0.98
2016-09-06T09:55:49.645026: step 4526, loss 0.0275743, acc 1
2016-09-06T09:55:50.465442: step 4527, loss 0.0171749, acc 1
2016-09-06T09:55:51.287777: step 4528, loss 0.0177019, acc 1
2016-09-06T09:55:52.097243: step 4529, loss 0.0480699, acc 0.98
2016-09-06T09:55:52.902335: step 4530, loss 0.0187867, acc 1
2016-09-06T09:55:53.716778: step 4531, loss 0.0297523, acc 1
2016-09-06T09:55:54.514750: step 4532, loss 0.0261541, acc 1
2016-09-06T09:55:55.319496: step 4533, loss 0.0375267, acc 0.96
2016-09-06T09:55:56.130745: step 4534, loss 0.00908381, acc 1
2016-09-06T09:55:56.913402: step 4535, loss 0.0266069, acc 0.98
2016-09-06T09:55:57.695038: step 4536, loss 0.0968022, acc 0.96
2016-09-06T09:55:58.536554: step 4537, loss 0.00682256, acc 1
2016-09-06T09:55:59.314169: step 4538, loss 0.0312593, acc 0.98
2016-09-06T09:56:00.118229: step 4539, loss 0.0459824, acc 0.98
2016-09-06T09:56:00.973493: step 4540, loss 0.047498, acc 0.98
2016-09-06T09:56:01.751816: step 4541, loss 0.0986847, acc 0.94
2016-09-06T09:56:02.534574: step 4542, loss 0.125764, acc 0.96
2016-09-06T09:56:03.357516: step 4543, loss 0.0340246, acc 0.98
2016-09-06T09:56:04.138179: step 4544, loss 0.0263766, acc 0.98
2016-09-06T09:56:04.928704: step 4545, loss 0.0313144, acc 0.98
2016-09-06T09:56:05.736669: step 4546, loss 0.00510588, acc 1
2016-09-06T09:56:06.543360: step 4547, loss 0.0437547, acc 1
2016-09-06T09:56:07.340435: step 4548, loss 0.0278709, acc 1
2016-09-06T09:56:08.161725: step 4549, loss 0.0478178, acc 0.96
2016-09-06T09:56:08.953366: step 4550, loss 0.0356774, acc 1
2016-09-06T09:56:09.753998: step 4551, loss 0.0633925, acc 0.98
2016-09-06T09:56:10.578892: step 4552, loss 0.0421603, acc 0.98
2016-09-06T09:56:11.388827: step 4553, loss 0.0169508, acc 1
2016-09-06T09:56:12.206739: step 4554, loss 0.0913354, acc 0.92
2016-09-06T09:56:13.033566: step 4555, loss 0.0146226, acc 1
2016-09-06T09:56:13.807042: step 4556, loss 0.0442613, acc 0.98
2016-09-06T09:56:14.617148: step 4557, loss 0.0270028, acc 0.98
2016-09-06T09:56:15.441493: step 4558, loss 0.115361, acc 0.96
2016-09-06T09:56:16.261545: step 4559, loss 0.0150265, acc 1
2016-09-06T09:56:17.078340: step 4560, loss 0.0346685, acc 0.98
2016-09-06T09:56:17.932145: step 4561, loss 0.00899254, acc 1
2016-09-06T09:56:18.742212: step 4562, loss 0.00691218, acc 1
2016-09-06T09:56:19.555798: step 4563, loss 0.100048, acc 0.92
2016-09-06T09:56:20.399318: step 4564, loss 0.0168272, acc 1
2016-09-06T09:56:21.188418: step 4565, loss 0.0158094, acc 1
2016-09-06T09:56:21.992515: step 4566, loss 0.0359623, acc 1
2016-09-06T09:56:22.831417: step 4567, loss 0.00466168, acc 1
2016-09-06T09:56:23.623650: step 4568, loss 0.0475503, acc 0.98
2016-09-06T09:56:24.433448: step 4569, loss 0.0638361, acc 0.98
2016-09-06T09:56:25.241497: step 4570, loss 0.0590654, acc 0.98
2016-09-06T09:56:26.049461: step 4571, loss 0.027677, acc 1
2016-09-06T09:56:26.865759: step 4572, loss 0.0272572, acc 0.98
2016-09-06T09:56:27.691964: step 4573, loss 0.0166723, acc 1
2016-09-06T09:56:28.513456: step 4574, loss 0.0200506, acc 1
2016-09-06T09:56:29.342078: step 4575, loss 0.0363711, acc 0.98
2016-09-06T09:56:30.134523: step 4576, loss 0.277948, acc 0.96
2016-09-06T09:56:30.976851: step 4577, loss 0.0798159, acc 0.94
2016-09-06T09:56:31.782987: step 4578, loss 0.0465019, acc 0.96
2016-09-06T09:56:32.644276: step 4579, loss 0.0421571, acc 0.98
2016-09-06T09:56:33.436593: step 4580, loss 0.0615799, acc 0.98
2016-09-06T09:56:34.253019: step 4581, loss 0.00995234, acc 1
2016-09-06T09:56:35.118681: step 4582, loss 0.0265315, acc 1
2016-09-06T09:56:35.942212: step 4583, loss 0.0154766, acc 1
2016-09-06T09:56:36.723374: step 4584, loss 0.0803433, acc 0.96
2016-09-06T09:56:37.531668: step 4585, loss 0.0660849, acc 0.96
2016-09-06T09:56:38.336857: step 4586, loss 0.017322, acc 1
2016-09-06T09:56:39.140380: step 4587, loss 0.0218007, acc 0.98
2016-09-06T09:56:39.994026: step 4588, loss 0.0331645, acc 1
2016-09-06T09:56:40.841546: step 4589, loss 0.00405064, acc 1
2016-09-06T09:56:41.614499: step 4590, loss 0.00867764, acc 1
2016-09-06T09:56:42.400752: step 4591, loss 0.0723364, acc 0.98
2016-09-06T09:56:43.213937: step 4592, loss 0.00584279, acc 1
2016-09-06T09:56:44.007567: step 4593, loss 0.0239911, acc 0.98
2016-09-06T09:56:44.812726: step 4594, loss 0.0479987, acc 0.98
2016-09-06T09:56:45.650662: step 4595, loss 0.0347282, acc 1
2016-09-06T09:56:46.428338: step 4596, loss 0.0198268, acc 0.98
2016-09-06T09:56:47.207446: step 4597, loss 0.0199913, acc 0.98
2016-09-06T09:56:48.031117: step 4598, loss 0.0963421, acc 0.96
2016-09-06T09:56:48.839369: step 4599, loss 0.0205185, acc 1
2016-09-06T09:56:49.654556: step 4600, loss 0.0350079, acc 1

Evaluation:
2016-09-06T09:56:53.426833: step 4600, loss 1.64928, acc 0.743902

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-4600

2016-09-06T09:56:55.306936: step 4601, loss 0.0368249, acc 0.96
2016-09-06T09:56:56.105870: step 4602, loss 0.0247931, acc 1
2016-09-06T09:56:56.917940: step 4603, loss 0.0404708, acc 1
2016-09-06T09:56:57.716805: step 4604, loss 0.113274, acc 0.98
2016-09-06T09:56:58.533915: step 4605, loss 0.0487845, acc 0.98
2016-09-06T09:56:59.356055: step 4606, loss 0.0305154, acc 1
2016-09-06T09:57:00.189097: step 4607, loss 0.0294361, acc 0.98
2016-09-06T09:57:00.950794: step 4608, loss 0.189882, acc 0.954545
2016-09-06T09:57:01.784962: step 4609, loss 0.0440669, acc 0.98
2016-09-06T09:57:02.621445: step 4610, loss 0.0697408, acc 0.94
2016-09-06T09:57:03.413465: step 4611, loss 0.0050716, acc 1
2016-09-06T09:57:04.238554: step 4612, loss 0.015586, acc 1
2016-09-06T09:57:05.032675: step 4613, loss 0.0519878, acc 0.98
2016-09-06T09:57:05.820364: step 4614, loss 0.0356979, acc 0.98
2016-09-06T09:57:06.630316: step 4615, loss 0.0325578, acc 1
2016-09-06T09:57:07.472488: step 4616, loss 0.0332288, acc 0.98
2016-09-06T09:57:08.268342: step 4617, loss 0.0471229, acc 0.98
2016-09-06T09:57:09.050599: step 4618, loss 0.0389224, acc 0.96
2016-09-06T09:57:09.876956: step 4619, loss 0.020516, acc 0.98
2016-09-06T09:57:10.662331: step 4620, loss 0.0164968, acc 1
2016-09-06T09:57:11.464839: step 4621, loss 0.0200351, acc 1
2016-09-06T09:57:12.295354: step 4622, loss 0.068182, acc 0.96
2016-09-06T09:57:13.099494: step 4623, loss 0.0463952, acc 0.96
2016-09-06T09:57:13.903255: step 4624, loss 0.0541613, acc 0.98
2016-09-06T09:57:14.702312: step 4625, loss 0.0402384, acc 0.98
2016-09-06T09:57:15.505120: step 4626, loss 0.0442771, acc 0.98
2016-09-06T09:57:16.324852: step 4627, loss 0.0491957, acc 0.98
2016-09-06T09:57:17.132623: step 4628, loss 0.0919182, acc 0.98
2016-09-06T09:57:17.935188: step 4629, loss 0.00611799, acc 1
2016-09-06T09:57:18.772121: step 4630, loss 0.0180465, acc 1
2016-09-06T09:57:19.651082: step 4631, loss 0.00589755, acc 1
2016-09-06T09:57:20.465582: step 4632, loss 0.0462806, acc 0.96
2016-09-06T09:57:21.282226: step 4633, loss 0.0420195, acc 0.98
2016-09-06T09:57:22.124921: step 4634, loss 0.0452109, acc 0.98
2016-09-06T09:57:22.941246: step 4635, loss 0.0540531, acc 0.98
2016-09-06T09:57:23.758422: step 4636, loss 0.00787177, acc 1
2016-09-06T09:57:24.597564: step 4637, loss 0.00423031, acc 1
2016-09-06T09:57:25.411280: step 4638, loss 0.0834517, acc 0.94
2016-09-06T09:57:26.213420: step 4639, loss 0.0162975, acc 1
2016-09-06T09:57:27.103783: step 4640, loss 0.0338578, acc 0.98
2016-09-06T09:57:27.919124: step 4641, loss 0.0461454, acc 0.96
2016-09-06T09:57:28.724941: step 4642, loss 0.0543009, acc 0.98
2016-09-06T09:57:29.547410: step 4643, loss 0.0442289, acc 0.98
2016-09-06T09:57:30.346327: step 4644, loss 0.0293614, acc 0.98
2016-09-06T09:57:31.153893: step 4645, loss 0.10788, acc 0.94
2016-09-06T09:57:31.998698: step 4646, loss 0.0441415, acc 0.98
2016-09-06T09:57:32.825868: step 4647, loss 0.044759, acc 0.96
2016-09-06T09:57:33.621883: step 4648, loss 0.00612303, acc 1
2016-09-06T09:57:34.437422: step 4649, loss 0.0248697, acc 0.98
2016-09-06T09:57:35.274354: step 4650, loss 0.0694201, acc 0.98
2016-09-06T09:57:36.053626: step 4651, loss 0.0701299, acc 0.96
2016-09-06T09:57:36.854213: step 4652, loss 0.0433842, acc 0.96
2016-09-06T09:57:37.700125: step 4653, loss 0.0150549, acc 1
2016-09-06T09:57:38.490740: step 4654, loss 0.0457783, acc 0.96
2016-09-06T09:57:39.308571: step 4655, loss 0.0351258, acc 0.98
2016-09-06T09:57:40.132203: step 4656, loss 0.0231283, acc 0.98
2016-09-06T09:57:40.953800: step 4657, loss 0.0689168, acc 0.98
2016-09-06T09:57:41.784531: step 4658, loss 0.0195775, acc 1
2016-09-06T09:57:42.634310: step 4659, loss 0.104857, acc 0.96
2016-09-06T09:57:43.455522: step 4660, loss 0.0424339, acc 0.98
2016-09-06T09:57:44.258851: step 4661, loss 0.0327661, acc 1
2016-09-06T09:57:45.059992: step 4662, loss 0.0142247, acc 1
2016-09-06T09:57:45.859128: step 4663, loss 0.0130808, acc 1
2016-09-06T09:57:46.682517: step 4664, loss 0.0614796, acc 0.96
2016-09-06T09:57:47.517191: step 4665, loss 0.0383086, acc 0.98
2016-09-06T09:57:48.346230: step 4666, loss 0.0428041, acc 1
2016-09-06T09:57:49.157064: step 4667, loss 0.0627391, acc 0.98
2016-09-06T09:57:49.969060: step 4668, loss 0.0946198, acc 0.94
2016-09-06T09:57:50.789255: step 4669, loss 0.0357904, acc 0.98
2016-09-06T09:57:51.599325: step 4670, loss 0.00721954, acc 1
2016-09-06T09:57:52.425610: step 4671, loss 0.00462262, acc 1
2016-09-06T09:57:53.243416: step 4672, loss 0.024295, acc 0.98
2016-09-06T09:57:54.045308: step 4673, loss 0.0242841, acc 0.98
2016-09-06T09:57:54.889415: step 4674, loss 0.0666934, acc 0.96
2016-09-06T09:57:55.733453: step 4675, loss 0.0421811, acc 0.98
2016-09-06T09:57:56.555659: step 4676, loss 0.0499543, acc 0.98
2016-09-06T09:57:57.392547: step 4677, loss 0.0448582, acc 0.98
2016-09-06T09:57:58.205355: step 4678, loss 0.0168224, acc 1
2016-09-06T09:57:58.990522: step 4679, loss 0.0425676, acc 0.98
2016-09-06T09:57:59.797973: step 4680, loss 0.0266294, acc 1
2016-09-06T09:58:00.653286: step 4681, loss 0.0295573, acc 0.98
2016-09-06T09:58:01.449247: step 4682, loss 0.0275439, acc 1
2016-09-06T09:58:02.254512: step 4683, loss 0.0809346, acc 0.98
2016-09-06T09:58:03.074828: step 4684, loss 0.0763684, acc 0.96
2016-09-06T09:58:03.879112: step 4685, loss 0.0181414, acc 0.98
2016-09-06T09:58:04.680758: step 4686, loss 0.0187094, acc 1
2016-09-06T09:58:05.506403: step 4687, loss 0.0332689, acc 0.98
2016-09-06T09:58:06.321186: step 4688, loss 0.0530923, acc 0.98
2016-09-06T09:58:07.136365: step 4689, loss 0.00431107, acc 1
2016-09-06T09:58:07.972633: step 4690, loss 0.0313178, acc 0.98
2016-09-06T09:58:08.793376: step 4691, loss 0.0299521, acc 0.98
2016-09-06T09:58:09.597130: step 4692, loss 0.00372062, acc 1
2016-09-06T09:58:10.429309: step 4693, loss 0.0562488, acc 0.94
2016-09-06T09:58:11.237697: step 4694, loss 0.0298321, acc 1
2016-09-06T09:58:12.043327: step 4695, loss 0.00665517, acc 1
2016-09-06T09:58:12.861046: step 4696, loss 0.0313776, acc 1
2016-09-06T09:58:13.675645: step 4697, loss 0.0412966, acc 1
2016-09-06T09:58:14.496039: step 4698, loss 0.0052528, acc 1
2016-09-06T09:58:15.327195: step 4699, loss 0.043044, acc 0.98
2016-09-06T09:58:16.149621: step 4700, loss 0.00428856, acc 1

Evaluation:
2016-09-06T09:58:19.873078: step 4700, loss 1.95268, acc 0.757036

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-4700

2016-09-06T09:58:21.775539: step 4701, loss 0.0876212, acc 0.96
2016-09-06T09:58:22.635074: step 4702, loss 0.00417897, acc 1
2016-09-06T09:58:23.451117: step 4703, loss 0.0312752, acc 0.98
2016-09-06T09:58:24.263053: step 4704, loss 0.0198207, acc 1
2016-09-06T09:58:25.083663: step 4705, loss 0.0203196, acc 0.98
2016-09-06T09:58:25.914481: step 4706, loss 0.0885486, acc 0.94
2016-09-06T09:58:26.726243: step 4707, loss 0.00970852, acc 1
2016-09-06T09:58:27.558655: step 4708, loss 0.0410689, acc 0.98
2016-09-06T09:58:28.384796: step 4709, loss 0.0449491, acc 0.96
2016-09-06T09:58:29.183884: step 4710, loss 0.00608122, acc 1
2016-09-06T09:58:29.998219: step 4711, loss 0.018021, acc 1
2016-09-06T09:58:30.821429: step 4712, loss 0.117033, acc 0.98
2016-09-06T09:58:31.600456: step 4713, loss 0.094474, acc 0.98
2016-09-06T09:58:32.413204: step 4714, loss 0.0639153, acc 0.98
2016-09-06T09:58:33.225398: step 4715, loss 0.0478507, acc 0.98
2016-09-06T09:58:34.056133: step 4716, loss 0.0876352, acc 0.96
2016-09-06T09:58:34.864704: step 4717, loss 0.0284421, acc 1
2016-09-06T09:58:35.694449: step 4718, loss 0.0225051, acc 1
2016-09-06T09:58:36.476890: step 4719, loss 0.00823873, acc 1
2016-09-06T09:58:37.272200: step 4720, loss 0.0477148, acc 0.98
2016-09-06T09:58:38.077847: step 4721, loss 0.0359853, acc 0.98
2016-09-06T09:58:38.852463: step 4722, loss 0.00903002, acc 1
2016-09-06T09:58:39.648511: step 4723, loss 0.0909112, acc 0.94
2016-09-06T09:58:40.451478: step 4724, loss 0.0422324, acc 0.98
2016-09-06T09:58:41.257858: step 4725, loss 0.00758781, acc 1
2016-09-06T09:58:42.070677: step 4726, loss 0.0240992, acc 1
2016-09-06T09:58:42.869146: step 4727, loss 0.0177997, acc 1
2016-09-06T09:58:43.688189: step 4728, loss 0.021406, acc 1
2016-09-06T09:58:44.533448: step 4729, loss 0.026014, acc 1
2016-09-06T09:58:45.366013: step 4730, loss 0.0201215, acc 1
2016-09-06T09:58:46.175059: step 4731, loss 0.0103257, acc 1
2016-09-06T09:58:46.995940: step 4732, loss 0.0526835, acc 0.96
2016-09-06T09:58:47.817812: step 4733, loss 0.113607, acc 0.98
2016-09-06T09:58:48.605411: step 4734, loss 0.0547987, acc 0.98
2016-09-06T09:58:49.380217: step 4735, loss 0.0482404, acc 0.94
2016-09-06T09:58:50.217899: step 4736, loss 0.0411162, acc 0.98
2016-09-06T09:58:51.011606: step 4737, loss 0.0533405, acc 0.96
2016-09-06T09:58:51.824422: step 4738, loss 0.120454, acc 0.94
2016-09-06T09:58:52.634207: step 4739, loss 0.0274546, acc 0.98
2016-09-06T09:58:53.399874: step 4740, loss 0.0394978, acc 0.98
2016-09-06T09:58:54.225456: step 4741, loss 0.0273625, acc 1
2016-09-06T09:58:55.034345: step 4742, loss 0.0454184, acc 0.98
2016-09-06T09:58:55.842805: step 4743, loss 0.0255374, acc 0.98
2016-09-06T09:58:56.610713: step 4744, loss 0.0397883, acc 0.98
2016-09-06T09:58:57.452587: step 4745, loss 0.010428, acc 1
2016-09-06T09:58:58.244794: step 4746, loss 0.0451698, acc 0.96
2016-09-06T09:58:59.057455: step 4747, loss 0.020432, acc 0.98
2016-09-06T09:58:59.888655: step 4748, loss 0.0102031, acc 1
2016-09-06T09:59:00.710245: step 4749, loss 0.0627214, acc 0.96
2016-09-06T09:59:01.512562: step 4750, loss 0.0410814, acc 0.98
2016-09-06T09:59:02.305899: step 4751, loss 0.0588013, acc 0.96
2016-09-06T09:59:03.078343: step 4752, loss 0.0360038, acc 0.96
2016-09-06T09:59:03.893559: step 4753, loss 0.0175466, acc 1
2016-09-06T09:59:04.686589: step 4754, loss 0.0226431, acc 0.98
2016-09-06T09:59:05.461372: step 4755, loss 0.0246805, acc 0.98
2016-09-06T09:59:06.262267: step 4756, loss 0.0169466, acc 1
2016-09-06T09:59:07.088603: step 4757, loss 0.0557871, acc 0.98
2016-09-06T09:59:07.889124: step 4758, loss 0.0392218, acc 0.98
2016-09-06T09:59:08.711025: step 4759, loss 0.029177, acc 0.98
2016-09-06T09:59:09.517383: step 4760, loss 0.010291, acc 1
2016-09-06T09:59:10.315048: step 4761, loss 0.0110711, acc 1
2016-09-06T09:59:11.118088: step 4762, loss 0.0146042, acc 1
2016-09-06T09:59:11.944228: step 4763, loss 0.00641606, acc 1
2016-09-06T09:59:12.728158: step 4764, loss 0.0738048, acc 0.96
2016-09-06T09:59:13.534436: step 4765, loss 0.0259976, acc 1
2016-09-06T09:59:14.383811: step 4766, loss 0.0156332, acc 1
2016-09-06T09:59:15.200611: step 4767, loss 0.0103887, acc 1
2016-09-06T09:59:16.030673: step 4768, loss 0.0834456, acc 0.94
2016-09-06T09:59:16.882103: step 4769, loss 0.0259521, acc 1
2016-09-06T09:59:17.662631: step 4770, loss 0.0321406, acc 1
2016-09-06T09:59:18.478938: step 4771, loss 0.124363, acc 0.98
2016-09-06T09:59:19.341392: step 4772, loss 0.0269996, acc 0.98
2016-09-06T09:59:20.156255: step 4773, loss 0.024188, acc 0.98
2016-09-06T09:59:21.025569: step 4774, loss 0.0490074, acc 0.98
2016-09-06T09:59:21.862811: step 4775, loss 0.00443716, acc 1
2016-09-06T09:59:22.718894: step 4776, loss 0.0193678, acc 0.98
2016-09-06T09:59:23.540787: step 4777, loss 0.0442759, acc 0.98
2016-09-06T09:59:24.366151: step 4778, loss 0.00775738, acc 1
2016-09-06T09:59:25.180988: step 4779, loss 0.0400007, acc 0.98
2016-09-06T09:59:25.985315: step 4780, loss 0.0295824, acc 1
2016-09-06T09:59:26.806425: step 4781, loss 0.0686399, acc 0.98
2016-09-06T09:59:27.637396: step 4782, loss 0.0372601, acc 0.98
2016-09-06T09:59:28.448473: step 4783, loss 0.0137475, acc 1
2016-09-06T09:59:29.258819: step 4784, loss 0.099783, acc 0.92
2016-09-06T09:59:30.081607: step 4785, loss 0.0487345, acc 0.98
2016-09-06T09:59:30.867198: step 4786, loss 0.107309, acc 0.96
2016-09-06T09:59:31.677767: step 4787, loss 0.00480206, acc 1
2016-09-06T09:59:32.502470: step 4788, loss 0.0592726, acc 0.98
2016-09-06T09:59:33.290284: step 4789, loss 0.00705984, acc 1
2016-09-06T09:59:34.128065: step 4790, loss 0.174241, acc 0.96
2016-09-06T09:59:34.951061: step 4791, loss 0.038015, acc 0.98
2016-09-06T09:59:35.769307: step 4792, loss 0.0096748, acc 1
2016-09-06T09:59:36.592394: step 4793, loss 0.038302, acc 0.98
2016-09-06T09:59:37.406031: step 4794, loss 0.0523004, acc 0.96
2016-09-06T09:59:38.226473: step 4795, loss 0.0305087, acc 0.98
2016-09-06T09:59:39.067462: step 4796, loss 0.0494108, acc 0.98
2016-09-06T09:59:39.910502: step 4797, loss 0.111302, acc 0.92
2016-09-06T09:59:40.720841: step 4798, loss 0.0549954, acc 0.96
2016-09-06T09:59:41.528225: step 4799, loss 0.0298206, acc 0.98
2016-09-06T09:59:42.311108: step 4800, loss 0.0275486, acc 0.977273

Evaluation:
2016-09-06T09:59:46.019497: step 4800, loss 1.32093, acc 0.755159

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-4800

2016-09-06T09:59:47.854919: step 4801, loss 0.0589983, acc 0.98
2016-09-06T09:59:48.676740: step 4802, loss 0.0316021, acc 1
2016-09-06T09:59:49.494103: step 4803, loss 0.007807, acc 1
2016-09-06T09:59:50.292996: step 4804, loss 0.00942613, acc 1
2016-09-06T09:59:51.109008: step 4805, loss 0.0388671, acc 0.98
2016-09-06T09:59:51.929079: step 4806, loss 0.0822808, acc 0.98
2016-09-06T09:59:52.731173: step 4807, loss 0.0752666, acc 0.96
2016-09-06T09:59:53.558940: step 4808, loss 0.00786631, acc 1
2016-09-06T09:59:54.380315: step 4809, loss 0.0507552, acc 0.98
2016-09-06T09:59:55.151913: step 4810, loss 0.0157649, acc 1
2016-09-06T09:59:55.953592: step 4811, loss 0.0428706, acc 0.96
2016-09-06T09:59:56.750268: step 4812, loss 0.00506284, acc 1
2016-09-06T09:59:57.546579: step 4813, loss 0.080994, acc 0.96
2016-09-06T09:59:58.352342: step 4814, loss 0.00341316, acc 1
2016-09-06T09:59:59.212520: step 4815, loss 0.0665334, acc 0.94
2016-09-06T09:59:59.997422: step 4816, loss 0.0110853, acc 1
2016-09-06T10:00:00.835109: step 4817, loss 0.137914, acc 0.94
2016-09-06T10:00:01.673316: step 4818, loss 0.0238679, acc 0.98
2016-09-06T10:00:02.468298: step 4819, loss 0.0110314, acc 1
2016-09-06T10:00:03.262246: step 4820, loss 0.102282, acc 0.98
2016-09-06T10:00:04.079089: step 4821, loss 0.0200267, acc 1
2016-09-06T10:00:04.867369: step 4822, loss 0.0199103, acc 0.98
2016-09-06T10:00:05.664219: step 4823, loss 0.0436976, acc 0.98
2016-09-06T10:00:06.479053: step 4824, loss 0.0243944, acc 1
2016-09-06T10:00:07.284599: step 4825, loss 0.0353636, acc 0.98
2016-09-06T10:00:08.079423: step 4826, loss 0.0334481, acc 0.98
2016-09-06T10:00:08.870122: step 4827, loss 0.0159522, acc 1
2016-09-06T10:00:09.635489: step 4828, loss 0.0250849, acc 0.98
2016-09-06T10:00:10.452046: step 4829, loss 0.0328117, acc 0.98
2016-09-06T10:00:11.288872: step 4830, loss 0.0102788, acc 1
2016-09-06T10:00:12.069689: step 4831, loss 0.0265875, acc 1
2016-09-06T10:00:12.867088: step 4832, loss 0.0297719, acc 1
2016-09-06T10:00:13.709373: step 4833, loss 0.0370733, acc 0.98
2016-09-06T10:00:14.473420: step 4834, loss 0.041788, acc 0.98
2016-09-06T10:00:15.270743: step 4835, loss 0.00440613, acc 1
2016-09-06T10:00:16.090352: step 4836, loss 0.0108574, acc 1
2016-09-06T10:00:16.894234: step 4837, loss 0.0505622, acc 0.96
2016-09-06T10:00:17.727378: step 4838, loss 0.0403827, acc 1
2016-09-06T10:00:18.554263: step 4839, loss 0.0296281, acc 0.98
2016-09-06T10:00:19.350032: step 4840, loss 0.00330824, acc 1
2016-09-06T10:00:20.134144: step 4841, loss 0.0263355, acc 1
2016-09-06T10:00:20.942874: step 4842, loss 0.0244718, acc 1
2016-09-06T10:00:21.736510: step 4843, loss 0.0202952, acc 1
2016-09-06T10:00:22.532572: step 4844, loss 0.0504681, acc 0.96
2016-09-06T10:00:23.372594: step 4845, loss 0.0158574, acc 1
2016-09-06T10:00:24.159070: step 4846, loss 0.0373675, acc 1
2016-09-06T10:00:24.948525: step 4847, loss 0.013838, acc 1
2016-09-06T10:00:25.783872: step 4848, loss 0.0159974, acc 1
2016-09-06T10:00:26.551211: step 4849, loss 0.0619347, acc 0.98
2016-09-06T10:00:27.342251: step 4850, loss 0.00390205, acc 1
2016-09-06T10:00:28.150202: step 4851, loss 0.0206922, acc 1
2016-09-06T10:00:28.963100: step 4852, loss 0.0334311, acc 0.98
2016-09-06T10:00:29.795882: step 4853, loss 0.0466865, acc 0.98
2016-09-06T10:00:30.642892: step 4854, loss 0.0106146, acc 1
2016-09-06T10:00:31.441814: step 4855, loss 0.053902, acc 0.98
2016-09-06T10:00:32.206286: step 4856, loss 0.0357222, acc 0.98
2016-09-06T10:00:33.016711: step 4857, loss 0.00394782, acc 1
2016-09-06T10:00:33.801856: step 4858, loss 0.0604278, acc 0.96
2016-09-06T10:00:34.608553: step 4859, loss 0.0184774, acc 1
2016-09-06T10:00:35.426591: step 4860, loss 0.0179145, acc 1
2016-09-06T10:00:36.224299: step 4861, loss 0.0408601, acc 1
2016-09-06T10:00:37.035273: step 4862, loss 0.0380827, acc 0.96
2016-09-06T10:00:37.841450: step 4863, loss 0.0658241, acc 0.98
2016-09-06T10:00:38.648367: step 4864, loss 0.0322942, acc 1
2016-09-06T10:00:39.428463: step 4865, loss 0.00691204, acc 1
2016-09-06T10:00:40.232249: step 4866, loss 0.110725, acc 0.96
2016-09-06T10:00:41.056395: step 4867, loss 0.0555441, acc 0.96
2016-09-06T10:00:41.910188: step 4868, loss 0.0948957, acc 0.96
2016-09-06T10:00:42.734861: step 4869, loss 0.00565387, acc 1
2016-09-06T10:00:43.508928: step 4870, loss 0.00638762, acc 1
2016-09-06T10:00:44.337690: step 4871, loss 0.0299253, acc 1
2016-09-06T10:00:45.134028: step 4872, loss 0.160149, acc 0.96
2016-09-06T10:00:45.912377: step 4873, loss 0.0091468, acc 1
2016-09-06T10:00:46.730672: step 4874, loss 0.0270678, acc 1
2016-09-06T10:00:47.548173: step 4875, loss 0.0332316, acc 0.98
2016-09-06T10:00:48.348092: step 4876, loss 0.0229267, acc 1
2016-09-06T10:00:49.155667: step 4877, loss 0.021487, acc 0.98
2016-09-06T10:00:49.962062: step 4878, loss 0.0926744, acc 0.96
2016-09-06T10:00:50.774821: step 4879, loss 0.0361283, acc 1
2016-09-06T10:00:51.594643: step 4880, loss 0.0418559, acc 0.96
2016-09-06T10:00:52.423485: step 4881, loss 0.00758483, acc 1
2016-09-06T10:00:53.217832: step 4882, loss 0.0223682, acc 0.98
2016-09-06T10:00:54.025215: step 4883, loss 0.0461887, acc 0.98
2016-09-06T10:00:54.832482: step 4884, loss 0.0140902, acc 1
2016-09-06T10:00:55.627574: step 4885, loss 0.0164925, acc 1
2016-09-06T10:00:56.442412: step 4886, loss 0.0756206, acc 0.98
2016-09-06T10:00:57.253138: step 4887, loss 0.00587355, acc 1
2016-09-06T10:00:58.049201: step 4888, loss 0.0114013, acc 1
2016-09-06T10:00:58.854390: step 4889, loss 0.0656808, acc 0.98
2016-09-06T10:00:59.662473: step 4890, loss 0.0167173, acc 1
2016-09-06T10:01:00.508631: step 4891, loss 0.0409499, acc 1
2016-09-06T10:01:01.321372: step 4892, loss 0.0033683, acc 1
2016-09-06T10:01:02.155438: step 4893, loss 0.020675, acc 1
2016-09-06T10:01:02.962924: step 4894, loss 0.00409999, acc 1
2016-09-06T10:01:03.825480: step 4895, loss 0.0339286, acc 0.98
2016-09-06T10:01:04.692533: step 4896, loss 0.0218536, acc 1
2016-09-06T10:01:05.509629: step 4897, loss 0.0430389, acc 0.98
2016-09-06T10:01:06.304533: step 4898, loss 0.0486028, acc 1
2016-09-06T10:01:07.127427: step 4899, loss 0.148611, acc 0.96
2016-09-06T10:01:07.939523: step 4900, loss 0.0179889, acc 1

Evaluation:
2016-09-06T10:01:11.687883: step 4900, loss 1.43915, acc 0.757974

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-4900

2016-09-06T10:01:13.524944: step 4901, loss 0.0753887, acc 0.98
2016-09-06T10:01:14.362893: step 4902, loss 0.0273907, acc 0.98
2016-09-06T10:01:15.196659: step 4903, loss 0.0894535, acc 0.98
2016-09-06T10:01:15.993488: step 4904, loss 0.0465974, acc 0.98
2016-09-06T10:01:16.817799: step 4905, loss 0.0235626, acc 1
2016-09-06T10:01:17.634360: step 4906, loss 0.0137825, acc 1
2016-09-06T10:01:18.464461: step 4907, loss 0.0546178, acc 0.98
2016-09-06T10:01:19.321258: step 4908, loss 0.0276716, acc 1
2016-09-06T10:01:20.125961: step 4909, loss 0.071441, acc 0.98
2016-09-06T10:01:20.935947: step 4910, loss 0.0699812, acc 0.94
2016-09-06T10:01:21.759795: step 4911, loss 0.0217855, acc 1
2016-09-06T10:01:22.557861: step 4912, loss 0.0915643, acc 0.92
2016-09-06T10:01:23.371892: step 4913, loss 0.0269841, acc 1
2016-09-06T10:01:24.200304: step 4914, loss 0.0300608, acc 0.98
2016-09-06T10:01:25.052026: step 4915, loss 0.0814639, acc 0.96
2016-09-06T10:01:25.851562: step 4916, loss 0.0402347, acc 0.98
2016-09-06T10:01:26.646529: step 4917, loss 0.0688536, acc 0.96
2016-09-06T10:01:27.458935: step 4918, loss 0.0206772, acc 1
2016-09-06T10:01:28.257180: step 4919, loss 0.0390806, acc 1
2016-09-06T10:01:29.065781: step 4920, loss 0.0177247, acc 1
2016-09-06T10:01:29.868384: step 4921, loss 0.0175415, acc 1
2016-09-06T10:01:30.674349: step 4922, loss 0.0187529, acc 1
2016-09-06T10:01:31.510689: step 4923, loss 0.0272362, acc 0.98
2016-09-06T10:01:32.325108: step 4924, loss 0.189426, acc 0.98
2016-09-06T10:01:33.103445: step 4925, loss 0.0261411, acc 1
2016-09-06T10:01:33.906566: step 4926, loss 0.0142559, acc 1
2016-09-06T10:01:34.753063: step 4927, loss 0.0519377, acc 0.96
2016-09-06T10:01:35.529471: step 4928, loss 0.0302518, acc 0.98
2016-09-06T10:01:36.339717: step 4929, loss 0.0364823, acc 0.98
2016-09-06T10:01:37.151418: step 4930, loss 0.00704208, acc 1
2016-09-06T10:01:37.930384: step 4931, loss 0.0116606, acc 1
2016-09-06T10:01:38.731682: step 4932, loss 0.045896, acc 0.98
2016-09-06T10:01:39.566635: step 4933, loss 0.161239, acc 0.98
2016-09-06T10:01:40.331809: step 4934, loss 0.0414446, acc 0.98
2016-09-06T10:01:41.125003: step 4935, loss 0.0563869, acc 0.96
2016-09-06T10:01:41.937360: step 4936, loss 0.0473485, acc 0.98
2016-09-06T10:01:42.737999: step 4937, loss 0.0272658, acc 0.98
2016-09-06T10:01:43.573385: step 4938, loss 0.0311655, acc 1
2016-09-06T10:01:44.400402: step 4939, loss 0.0455781, acc 0.96
2016-09-06T10:01:45.194421: step 4940, loss 0.0345398, acc 0.98
2016-09-06T10:01:45.982747: step 4941, loss 0.037166, acc 0.98
2016-09-06T10:01:46.810993: step 4942, loss 0.00444811, acc 1
2016-09-06T10:01:47.609480: step 4943, loss 0.0143493, acc 1
2016-09-06T10:01:48.395434: step 4944, loss 0.0187033, acc 1
2016-09-06T10:01:49.228339: step 4945, loss 0.00466787, acc 1
2016-09-06T10:01:50.008113: step 4946, loss 0.0166394, acc 1
2016-09-06T10:01:50.832411: step 4947, loss 0.0113484, acc 1
2016-09-06T10:01:51.634235: step 4948, loss 0.0479544, acc 0.98
2016-09-06T10:01:52.405160: step 4949, loss 0.0249795, acc 0.98
2016-09-06T10:01:53.209094: step 4950, loss 0.0889425, acc 0.98
2016-09-06T10:01:54.058692: step 4951, loss 0.0983689, acc 0.96
2016-09-06T10:01:54.865952: step 4952, loss 0.0927718, acc 0.98
2016-09-06T10:01:55.665027: step 4953, loss 0.0269436, acc 0.98
2016-09-06T10:01:56.475878: step 4954, loss 0.0223918, acc 0.98
2016-09-06T10:01:57.275761: step 4955, loss 0.0177845, acc 1
2016-09-06T10:01:58.096966: step 4956, loss 0.0279569, acc 0.98
2016-09-06T10:01:58.960742: step 4957, loss 0.0132384, acc 1
2016-09-06T10:01:59.816694: step 4958, loss 0.0249178, acc 0.98
2016-09-06T10:02:00.663429: step 4959, loss 0.059249, acc 0.96
2016-09-06T10:02:01.503252: step 4960, loss 0.0802394, acc 0.96
2016-09-06T10:02:02.279272: step 4961, loss 0.0613099, acc 0.96
2016-09-06T10:02:03.083879: step 4962, loss 0.0336295, acc 1
2016-09-06T10:02:03.921866: step 4963, loss 0.0127815, acc 1
2016-09-06T10:02:04.730065: step 4964, loss 0.0407633, acc 0.98
2016-09-06T10:02:05.533694: step 4965, loss 0.00760395, acc 1
2016-09-06T10:02:06.374219: step 4966, loss 0.0309118, acc 1
2016-09-06T10:02:07.174293: step 4967, loss 0.0281212, acc 1
2016-09-06T10:02:07.982612: step 4968, loss 0.0493245, acc 1
2016-09-06T10:02:08.803347: step 4969, loss 0.0464879, acc 0.96
2016-09-06T10:02:09.626300: step 4970, loss 0.0235318, acc 1
2016-09-06T10:02:10.473580: step 4971, loss 0.0726265, acc 0.98
2016-09-06T10:02:11.314921: step 4972, loss 0.00464911, acc 1
2016-09-06T10:02:12.148123: step 4973, loss 0.0820336, acc 0.98
2016-09-06T10:02:12.948517: step 4974, loss 0.00979275, acc 1
2016-09-06T10:02:13.777599: step 4975, loss 0.0610967, acc 0.96
2016-09-06T10:02:14.596706: step 4976, loss 0.00530815, acc 1
2016-09-06T10:02:15.389281: step 4977, loss 0.0573555, acc 0.98
2016-09-06T10:02:16.167986: step 4978, loss 0.0218735, acc 0.98
2016-09-06T10:02:16.994266: step 4979, loss 0.037167, acc 0.98
2016-09-06T10:02:17.775512: step 4980, loss 0.0269662, acc 1
2016-09-06T10:02:18.581530: step 4981, loss 0.0113553, acc 1
2016-09-06T10:02:19.397513: step 4982, loss 0.0264385, acc 0.98
2016-09-06T10:02:20.190710: step 4983, loss 0.052134, acc 0.96
2016-09-06T10:02:20.988211: step 4984, loss 0.0930796, acc 0.98
2016-09-06T10:02:21.812473: step 4985, loss 0.0132572, acc 1
2016-09-06T10:02:22.589615: step 4986, loss 0.0217631, acc 1
2016-09-06T10:02:23.397841: step 4987, loss 0.0200892, acc 0.98
2016-09-06T10:02:24.201379: step 4988, loss 0.00321021, acc 1
2016-09-06T10:02:25.016968: step 4989, loss 0.0501569, acc 0.98
2016-09-06T10:02:25.825846: step 4990, loss 0.0205381, acc 0.98
2016-09-06T10:02:26.664472: step 4991, loss 0.0195082, acc 0.98
2016-09-06T10:02:27.439543: step 4992, loss 0.0732884, acc 0.954545
2016-09-06T10:02:28.245036: step 4993, loss 0.0076673, acc 1
2016-09-06T10:02:29.077082: step 4994, loss 0.0504119, acc 0.96
2016-09-06T10:02:29.857417: step 4995, loss 0.0523074, acc 0.94
2016-09-06T10:02:30.667995: step 4996, loss 0.046026, acc 0.98
2016-09-06T10:02:31.530739: step 4997, loss 0.0336374, acc 0.96
2016-09-06T10:02:32.320362: step 4998, loss 0.0304351, acc 0.98
2016-09-06T10:02:33.110300: step 4999, loss 0.0189795, acc 1
2016-09-06T10:02:33.942600: step 5000, loss 0.0182677, acc 1

Evaluation:
2016-09-06T10:02:37.665687: step 5000, loss 1.62169, acc 0.757036

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-5000

2016-09-06T10:02:39.612073: step 5001, loss 0.0459551, acc 0.98
2016-09-06T10:02:40.463165: step 5002, loss 0.0138035, acc 1
2016-09-06T10:02:41.297413: step 5003, loss 0.0151589, acc 1
2016-09-06T10:02:42.090070: step 5004, loss 0.012382, acc 1
2016-09-06T10:02:42.912614: step 5005, loss 0.06961, acc 0.98
2016-09-06T10:02:43.725324: step 5006, loss 0.0141129, acc 1
2016-09-06T10:02:44.515508: step 5007, loss 0.0566296, acc 0.98
2016-09-06T10:02:45.293088: step 5008, loss 0.0178157, acc 1
2016-09-06T10:02:46.112842: step 5009, loss 0.0180809, acc 0.98
2016-09-06T10:02:46.911285: step 5010, loss 0.00518417, acc 1
2016-09-06T10:02:47.721452: step 5011, loss 0.0204946, acc 0.98
2016-09-06T10:02:48.562578: step 5012, loss 0.0199326, acc 1
2016-09-06T10:02:49.354185: step 5013, loss 0.00308514, acc 1
2016-09-06T10:02:50.126953: step 5014, loss 0.00931504, acc 1
2016-09-06T10:02:50.940606: step 5015, loss 0.00740996, acc 1
2016-09-06T10:02:51.735429: step 5016, loss 0.0192573, acc 0.98
2016-09-06T10:02:52.536605: step 5017, loss 0.00531496, acc 1
2016-09-06T10:02:53.396182: step 5018, loss 0.0066822, acc 1
2016-09-06T10:02:54.177451: step 5019, loss 0.0186769, acc 1
2016-09-06T10:02:55.011623: step 5020, loss 0.00526731, acc 1
2016-09-06T10:02:55.872084: step 5021, loss 0.0178725, acc 0.98
2016-09-06T10:02:56.674769: step 5022, loss 0.0444969, acc 0.98
2016-09-06T10:02:57.508390: step 5023, loss 0.00392219, acc 1
2016-09-06T10:02:58.330976: step 5024, loss 0.0219614, acc 0.98
2016-09-06T10:02:59.150000: step 5025, loss 0.00536146, acc 1
2016-09-06T10:02:59.962741: step 5026, loss 0.0134582, acc 1
2016-09-06T10:03:00.849437: step 5027, loss 0.00417694, acc 1
2016-09-06T10:03:01.645691: step 5028, loss 0.0102043, acc 1
2016-09-06T10:03:02.474613: step 5029, loss 0.0219899, acc 1
2016-09-06T10:03:03.323648: step 5030, loss 0.00914799, acc 1
2016-09-06T10:03:04.131764: step 5031, loss 0.0184516, acc 1
2016-09-06T10:03:04.959963: step 5032, loss 0.00921615, acc 1
2016-09-06T10:03:05.805429: step 5033, loss 0.00773197, acc 1
2016-09-06T10:03:06.617850: step 5034, loss 0.00662809, acc 1
2016-09-06T10:03:07.449730: step 5035, loss 0.0164853, acc 1
2016-09-06T10:03:08.299786: step 5036, loss 0.00454436, acc 1
2016-09-06T10:03:09.121034: step 5037, loss 0.0123257, acc 1
2016-09-06T10:03:09.891128: step 5038, loss 0.0344591, acc 1
2016-09-06T10:03:10.708580: step 5039, loss 0.0138701, acc 1
2016-09-06T10:03:11.539089: step 5040, loss 0.0126961, acc 1
2016-09-06T10:03:12.326968: step 5041, loss 0.0286562, acc 0.98
2016-09-06T10:03:13.141915: step 5042, loss 0.00324137, acc 1
2016-09-06T10:03:13.948689: step 5043, loss 0.0179382, acc 0.98
2016-09-06T10:03:14.716380: step 5044, loss 0.0788851, acc 0.96
2016-09-06T10:03:15.550059: step 5045, loss 0.0157549, acc 1
2016-09-06T10:03:16.360659: step 5046, loss 0.100937, acc 0.96
2016-09-06T10:03:17.141787: step 5047, loss 0.0307494, acc 0.98
2016-09-06T10:03:17.929624: step 5048, loss 0.00344547, acc 1
2016-09-06T10:03:18.777129: step 5049, loss 0.0558614, acc 0.98
2016-09-06T10:03:19.550750: step 5050, loss 0.0412148, acc 0.96
2016-09-06T10:03:20.355361: step 5051, loss 0.00512739, acc 1
2016-09-06T10:03:21.174555: step 5052, loss 0.018093, acc 1
2016-09-06T10:03:21.981548: step 5053, loss 0.0219022, acc 0.98
2016-09-06T10:03:22.790411: step 5054, loss 0.0553643, acc 0.98
2016-09-06T10:03:23.595825: step 5055, loss 0.00293741, acc 1
2016-09-06T10:03:24.393398: step 5056, loss 0.00923139, acc 1
2016-09-06T10:03:25.197611: step 5057, loss 0.0238021, acc 0.98
2016-09-06T10:03:26.014065: step 5058, loss 0.0222815, acc 1
2016-09-06T10:03:26.782635: step 5059, loss 0.0142446, acc 1
2016-09-06T10:03:27.607961: step 5060, loss 0.0518115, acc 0.96
2016-09-06T10:03:28.432966: step 5061, loss 0.0168614, acc 1
2016-09-06T10:03:29.229842: step 5062, loss 0.0253675, acc 1
2016-09-06T10:03:30.041492: step 5063, loss 0.00280349, acc 1
2016-09-06T10:03:30.849389: step 5064, loss 0.00399111, acc 1
2016-09-06T10:03:31.619173: step 5065, loss 0.0237051, acc 0.98
2016-09-06T10:03:32.418138: step 5066, loss 0.00405056, acc 1
2016-09-06T10:03:33.231713: step 5067, loss 0.0377202, acc 0.98
2016-09-06T10:03:34.027143: step 5068, loss 0.0381062, acc 0.98
2016-09-06T10:03:34.859782: step 5069, loss 0.0187815, acc 1
2016-09-06T10:03:35.676122: step 5070, loss 0.0485841, acc 0.98
2016-09-06T10:03:36.462313: step 5071, loss 0.0612127, acc 0.96
2016-09-06T10:03:37.262985: step 5072, loss 0.00448879, acc 1
2016-09-06T10:03:38.102946: step 5073, loss 0.00253514, acc 1
2016-09-06T10:03:38.928128: step 5074, loss 0.018723, acc 0.98
2016-09-06T10:03:39.744722: step 5075, loss 0.0260729, acc 0.98
2016-09-06T10:03:40.556335: step 5076, loss 0.00288805, acc 1
2016-09-06T10:03:41.365654: step 5077, loss 0.026729, acc 0.98
2016-09-06T10:03:42.196623: step 5078, loss 0.0404241, acc 0.98
2016-09-06T10:03:43.022780: step 5079, loss 0.0102864, acc 1
2016-09-06T10:03:43.846124: step 5080, loss 0.0253169, acc 0.98
2016-09-06T10:03:44.643618: step 5081, loss 0.0101672, acc 1
2016-09-06T10:03:45.467631: step 5082, loss 0.00292224, acc 1
2016-09-06T10:03:46.282765: step 5083, loss 0.0215824, acc 0.98
2016-09-06T10:03:47.104488: step 5084, loss 0.00267096, acc 1
2016-09-06T10:03:47.933005: step 5085, loss 0.0488689, acc 0.98
2016-09-06T10:03:48.777715: step 5086, loss 0.0453635, acc 0.98
2016-09-06T10:03:49.581089: step 5087, loss 0.0208855, acc 0.98
2016-09-06T10:03:50.413922: step 5088, loss 0.0147069, acc 1
2016-09-06T10:03:51.272307: step 5089, loss 0.0201612, acc 1
2016-09-06T10:03:52.113619: step 5090, loss 0.0398601, acc 0.98
2016-09-06T10:03:52.941822: step 5091, loss 0.0583681, acc 0.98
2016-09-06T10:03:53.744257: step 5092, loss 0.0522712, acc 0.98
2016-09-06T10:03:54.523441: step 5093, loss 0.0558588, acc 0.98
2016-09-06T10:03:55.344827: step 5094, loss 0.0075469, acc 1
2016-09-06T10:03:56.178556: step 5095, loss 0.0234008, acc 0.98
2016-09-06T10:03:56.991844: step 5096, loss 0.00679494, acc 1
2016-09-06T10:03:57.798330: step 5097, loss 0.0452923, acc 0.96
2016-09-06T10:03:58.630194: step 5098, loss 0.00399467, acc 1
2016-09-06T10:03:59.421933: step 5099, loss 0.0596159, acc 0.96
2016-09-06T10:04:00.245184: step 5100, loss 0.00829141, acc 1

Evaluation:
2016-09-06T10:04:03.951603: step 5100, loss 1.7163, acc 0.766416

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-5100

2016-09-06T10:04:05.914163: step 5101, loss 0.0370933, acc 0.98
2016-09-06T10:04:06.753680: step 5102, loss 0.0377655, acc 0.98
2016-09-06T10:04:07.578059: step 5103, loss 0.00251321, acc 1
2016-09-06T10:04:08.400083: step 5104, loss 0.062106, acc 0.96
2016-09-06T10:04:09.176762: step 5105, loss 0.0557081, acc 0.94
2016-09-06T10:04:09.994715: step 5106, loss 0.024018, acc 1
2016-09-06T10:04:10.841235: step 5107, loss 0.0626566, acc 0.98
2016-09-06T10:04:11.625457: step 5108, loss 0.00310926, acc 1
2016-09-06T10:04:12.420285: step 5109, loss 0.153762, acc 0.98
2016-09-06T10:04:13.268776: step 5110, loss 0.0127101, acc 1
2016-09-06T10:04:14.068323: step 5111, loss 0.0816285, acc 0.96
2016-09-06T10:04:14.855077: step 5112, loss 0.0500498, acc 0.98
2016-09-06T10:04:15.650053: step 5113, loss 0.00230106, acc 1
2016-09-06T10:04:16.439022: step 5114, loss 0.0771617, acc 0.98
2016-09-06T10:04:17.228229: step 5115, loss 0.00391322, acc 1
2016-09-06T10:04:18.040716: step 5116, loss 0.044504, acc 0.98
2016-09-06T10:04:18.821845: step 5117, loss 0.0454655, acc 0.96
2016-09-06T10:04:19.627541: step 5118, loss 0.0110217, acc 1
2016-09-06T10:04:20.418645: step 5119, loss 0.0355502, acc 1
2016-09-06T10:04:21.242580: step 5120, loss 0.0261306, acc 0.98
2016-09-06T10:04:22.112499: step 5121, loss 0.0141751, acc 1
2016-09-06T10:04:22.941854: step 5122, loss 0.0425355, acc 0.98
2016-09-06T10:04:23.741295: step 5123, loss 0.0488762, acc 0.96
2016-09-06T10:04:24.548049: step 5124, loss 0.00919809, acc 1
2016-09-06T10:04:25.357191: step 5125, loss 0.032036, acc 0.98
2016-09-06T10:04:26.130774: step 5126, loss 0.151621, acc 0.92
2016-09-06T10:04:26.946080: step 5127, loss 0.00808999, acc 1
2016-09-06T10:04:27.757490: step 5128, loss 0.0215815, acc 1
2016-09-06T10:04:28.536830: step 5129, loss 0.0227344, acc 1
2016-09-06T10:04:29.334418: step 5130, loss 0.0866047, acc 0.94
2016-09-06T10:04:30.128808: step 5131, loss 0.0074132, acc 1
2016-09-06T10:04:30.926560: step 5132, loss 0.0117082, acc 1
2016-09-06T10:04:31.718360: step 5133, loss 0.0387794, acc 0.96
2016-09-06T10:04:32.551102: step 5134, loss 0.033912, acc 0.98
2016-09-06T10:04:33.335324: step 5135, loss 0.00431606, acc 1
2016-09-06T10:04:34.118415: step 5136, loss 0.0729769, acc 0.98
2016-09-06T10:04:34.909413: step 5137, loss 0.0289521, acc 1
2016-09-06T10:04:35.720148: step 5138, loss 0.0269133, acc 1
2016-09-06T10:04:36.520200: step 5139, loss 0.0120608, acc 1
2016-09-06T10:04:37.317734: step 5140, loss 0.0373289, acc 0.98
2016-09-06T10:04:38.147430: step 5141, loss 0.00618237, acc 1
2016-09-06T10:04:38.976364: step 5142, loss 0.0824347, acc 0.96
2016-09-06T10:04:39.792361: step 5143, loss 0.0429346, acc 1
2016-09-06T10:04:40.599169: step 5144, loss 0.0861425, acc 0.96
2016-09-06T10:04:41.404960: step 5145, loss 0.0120932, acc 1
2016-09-06T10:04:42.228012: step 5146, loss 0.00921647, acc 1
2016-09-06T10:04:43.027142: step 5147, loss 0.00439894, acc 1
2016-09-06T10:04:43.874127: step 5148, loss 0.0398885, acc 1
2016-09-06T10:04:44.694716: step 5149, loss 0.0844121, acc 0.98
2016-09-06T10:04:45.484209: step 5150, loss 0.102494, acc 0.96
2016-09-06T10:04:46.260138: step 5151, loss 0.0121732, acc 1
2016-09-06T10:04:47.072981: step 5152, loss 0.034057, acc 0.98
2016-09-06T10:04:47.869288: step 5153, loss 0.0146351, acc 1
2016-09-06T10:04:48.668916: step 5154, loss 0.0759457, acc 0.98
2016-09-06T10:04:49.485478: step 5155, loss 0.0168047, acc 1
2016-09-06T10:04:50.250035: step 5156, loss 0.0134225, acc 1
2016-09-06T10:04:51.085685: step 5157, loss 0.00414225, acc 1
2016-09-06T10:04:51.898757: step 5158, loss 0.0293089, acc 0.98
2016-09-06T10:04:52.681456: step 5159, loss 0.0124369, acc 1
2016-09-06T10:04:53.498774: step 5160, loss 0.00859865, acc 1
2016-09-06T10:04:54.339914: step 5161, loss 0.0338498, acc 1
2016-09-06T10:04:55.132223: step 5162, loss 0.0191956, acc 1
2016-09-06T10:04:55.966112: step 5163, loss 0.00433833, acc 1
2016-09-06T10:04:56.761398: step 5164, loss 0.0731859, acc 0.98
2016-09-06T10:04:57.540889: step 5165, loss 0.0406656, acc 0.98
2016-09-06T10:04:58.350001: step 5166, loss 0.0295276, acc 0.98
2016-09-06T10:04:59.187399: step 5167, loss 0.0513251, acc 0.98
2016-09-06T10:04:59.955381: step 5168, loss 0.0037455, acc 1
2016-09-06T10:05:00.777356: step 5169, loss 0.0265226, acc 1
2016-09-06T10:05:01.626942: step 5170, loss 0.0419024, acc 0.98
2016-09-06T10:05:02.421058: step 5171, loss 0.0365482, acc 0.98
2016-09-06T10:05:03.224666: step 5172, loss 0.0207535, acc 1
2016-09-06T10:05:04.034474: step 5173, loss 0.00431438, acc 1
2016-09-06T10:05:04.805491: step 5174, loss 0.0160767, acc 1
2016-09-06T10:05:05.607623: step 5175, loss 0.00387016, acc 1
2016-09-06T10:05:06.442564: step 5176, loss 0.116028, acc 0.98
2016-09-06T10:05:07.222833: step 5177, loss 0.0143266, acc 1
2016-09-06T10:05:08.029788: step 5178, loss 0.00377828, acc 1
2016-09-06T10:05:08.862114: step 5179, loss 0.05389, acc 0.98
2016-09-06T10:05:09.653412: step 5180, loss 0.0543469, acc 0.98
2016-09-06T10:05:10.444534: step 5181, loss 0.0096991, acc 1
2016-09-06T10:05:11.263901: step 5182, loss 0.0746411, acc 0.98
2016-09-06T10:05:12.039041: step 5183, loss 0.0343897, acc 0.98
2016-09-06T10:05:12.791602: step 5184, loss 0.00931776, acc 1
2016-09-06T10:05:13.594155: step 5185, loss 0.0134323, acc 1
2016-09-06T10:05:14.396959: step 5186, loss 0.0321347, acc 1
2016-09-06T10:05:15.236229: step 5187, loss 0.00773409, acc 1
2016-09-06T10:05:16.061893: step 5188, loss 0.0257192, acc 1
2016-09-06T10:05:16.869614: step 5189, loss 0.0258748, acc 0.98
2016-09-06T10:05:17.676424: step 5190, loss 0.0353414, acc 0.98
2016-09-06T10:05:18.512633: step 5191, loss 0.0130073, acc 1
2016-09-06T10:05:19.296581: step 5192, loss 0.00340731, acc 1
2016-09-06T10:05:20.124353: step 5193, loss 0.0523942, acc 0.94
2016-09-06T10:05:20.939309: step 5194, loss 0.0152817, acc 1
2016-09-06T10:05:21.725523: step 5195, loss 0.0227828, acc 0.98
2016-09-06T10:05:22.516310: step 5196, loss 0.0197953, acc 0.98
2016-09-06T10:05:23.356446: step 5197, loss 0.0650018, acc 0.94
2016-09-06T10:05:24.139547: step 5198, loss 0.0634738, acc 0.96
2016-09-06T10:05:24.930276: step 5199, loss 0.0210781, acc 1
2016-09-06T10:05:25.738026: step 5200, loss 0.00322414, acc 1

Evaluation:
2016-09-06T10:05:29.468324: step 5200, loss 1.85483, acc 0.757974

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-5200

2016-09-06T10:05:31.391979: step 5201, loss 0.0127095, acc 1
2016-09-06T10:05:32.249598: step 5202, loss 0.00415701, acc 1
2016-09-06T10:05:33.081340: step 5203, loss 0.0742405, acc 0.98
2016-09-06T10:05:33.885337: step 5204, loss 0.0191203, acc 0.98
2016-09-06T10:05:34.702287: step 5205, loss 0.00482854, acc 1
2016-09-06T10:05:35.523139: step 5206, loss 0.0303819, acc 0.98
2016-09-06T10:05:36.322171: step 5207, loss 0.00310379, acc 1
2016-09-06T10:05:37.107101: step 5208, loss 0.0320046, acc 0.98
2016-09-06T10:05:37.924972: step 5209, loss 0.0104809, acc 1
2016-09-06T10:05:38.739025: step 5210, loss 0.0203184, acc 0.98
2016-09-06T10:05:39.559792: step 5211, loss 0.0103174, acc 1
2016-09-06T10:05:40.368508: step 5212, loss 0.0296939, acc 0.98
2016-09-06T10:05:41.176098: step 5213, loss 0.00439899, acc 1
2016-09-06T10:05:41.983572: step 5214, loss 0.00427843, acc 1
2016-09-06T10:05:42.815679: step 5215, loss 0.0034377, acc 1
2016-09-06T10:05:43.602270: step 5216, loss 0.0575725, acc 0.96
2016-09-06T10:05:44.423867: step 5217, loss 0.0222973, acc 1
2016-09-06T10:05:45.238741: step 5218, loss 0.028757, acc 0.98
2016-09-06T10:05:46.011680: step 5219, loss 0.00730743, acc 1
2016-09-06T10:05:46.828974: step 5220, loss 0.138401, acc 0.96
2016-09-06T10:05:47.663455: step 5221, loss 0.131476, acc 0.92
2016-09-06T10:05:48.452628: step 5222, loss 0.0237391, acc 0.98
2016-09-06T10:05:49.256102: step 5223, loss 0.0491096, acc 0.98
2016-09-06T10:05:50.061735: step 5224, loss 0.00345535, acc 1
2016-09-06T10:05:50.836551: step 5225, loss 0.0617131, acc 0.96
2016-09-06T10:05:51.630740: step 5226, loss 0.0229394, acc 1
2016-09-06T10:05:52.422883: step 5227, loss 0.0359959, acc 0.98
2016-09-06T10:05:53.225641: step 5228, loss 0.0295616, acc 1
2016-09-06T10:05:54.026938: step 5229, loss 0.00492651, acc 1
2016-09-06T10:05:54.844910: step 5230, loss 0.0321514, acc 0.98
2016-09-06T10:05:55.638919: step 5231, loss 0.00433648, acc 1
2016-09-06T10:05:56.451349: step 5232, loss 0.0890743, acc 0.98
2016-09-06T10:05:57.281246: step 5233, loss 0.0262905, acc 1
2016-09-06T10:05:58.089723: step 5234, loss 0.00453542, acc 1
2016-09-06T10:05:58.910350: step 5235, loss 0.0309076, acc 0.98
2016-09-06T10:05:59.703928: step 5236, loss 0.0530832, acc 0.96
2016-09-06T10:06:00.513791: step 5237, loss 0.0552445, acc 0.98
2016-09-06T10:06:01.337437: step 5238, loss 0.00957673, acc 1
2016-09-06T10:06:02.174687: step 5239, loss 0.0461842, acc 0.98
2016-09-06T10:06:02.977244: step 5240, loss 0.00750942, acc 1
2016-09-06T10:06:03.779687: step 5241, loss 0.00275493, acc 1
2016-09-06T10:06:04.619920: step 5242, loss 0.09287, acc 0.94
2016-09-06T10:06:05.421116: step 5243, loss 0.0327695, acc 0.98
2016-09-06T10:06:06.227750: step 5244, loss 0.00635491, acc 1
2016-09-06T10:06:07.042309: step 5245, loss 0.0781554, acc 0.96
2016-09-06T10:06:07.861444: step 5246, loss 0.0333964, acc 0.98
2016-09-06T10:06:08.680080: step 5247, loss 0.0150822, acc 1
2016-09-06T10:06:09.534442: step 5248, loss 0.0268959, acc 0.98
2016-09-06T10:06:10.397684: step 5249, loss 0.0182027, acc 1
2016-09-06T10:06:11.209450: step 5250, loss 0.0609038, acc 0.94
2016-09-06T10:06:12.057819: step 5251, loss 0.0203257, acc 1
2016-09-06T10:06:12.860336: step 5252, loss 0.059804, acc 0.98
2016-09-06T10:06:13.672043: step 5253, loss 0.0457057, acc 0.98
2016-09-06T10:06:14.510172: step 5254, loss 0.0125722, acc 1
2016-09-06T10:06:15.354404: step 5255, loss 0.00859425, acc 1
2016-09-06T10:06:16.152829: step 5256, loss 0.0187819, acc 0.98
2016-09-06T10:06:16.982882: step 5257, loss 0.00391091, acc 1
2016-09-06T10:06:17.791034: step 5258, loss 0.0172152, acc 1
2016-09-06T10:06:18.601199: step 5259, loss 0.0210819, acc 1
2016-09-06T10:06:19.448012: step 5260, loss 0.0275523, acc 0.98
2016-09-06T10:06:20.262133: step 5261, loss 0.0219763, acc 1
2016-09-06T10:06:21.052111: step 5262, loss 0.0181048, acc 0.98
2016-09-06T10:06:21.843284: step 5263, loss 0.0522605, acc 0.98
2016-09-06T10:06:22.685808: step 5264, loss 0.0303847, acc 0.98
2016-09-06T10:06:23.475038: step 5265, loss 0.0170954, acc 1
2016-09-06T10:06:24.317644: step 5266, loss 0.0062072, acc 1
2016-09-06T10:06:25.137975: step 5267, loss 0.0516763, acc 0.96
2016-09-06T10:06:25.913391: step 5268, loss 0.0166824, acc 1
2016-09-06T10:06:26.729021: step 5269, loss 0.10836, acc 0.96
2016-09-06T10:06:27.525402: step 5270, loss 0.0210523, acc 1
2016-09-06T10:06:28.306712: step 5271, loss 0.00264734, acc 1
2016-09-06T10:06:29.109841: step 5272, loss 0.0765203, acc 0.96
2016-09-06T10:06:29.913621: step 5273, loss 0.00799349, acc 1
2016-09-06T10:06:30.693402: step 5274, loss 0.00246622, acc 1
2016-09-06T10:06:31.525805: step 5275, loss 0.00605721, acc 1
2016-09-06T10:06:32.371329: step 5276, loss 0.101991, acc 0.98
2016-09-06T10:06:33.159271: step 5277, loss 0.0132739, acc 1
2016-09-06T10:06:33.977722: step 5278, loss 0.0236871, acc 1
2016-09-06T10:06:34.789341: step 5279, loss 0.0239908, acc 0.98
2016-09-06T10:06:35.575033: step 5280, loss 0.00284222, acc 1
2016-09-06T10:06:36.346963: step 5281, loss 0.0025851, acc 1
2016-09-06T10:06:37.159655: step 5282, loss 0.054352, acc 0.98
2016-09-06T10:06:37.953630: step 5283, loss 0.0154777, acc 1
2016-09-06T10:06:38.760887: step 5284, loss 0.00372441, acc 1
2016-09-06T10:06:39.565194: step 5285, loss 0.0133714, acc 1
2016-09-06T10:06:40.346116: step 5286, loss 0.00444612, acc 1
2016-09-06T10:06:41.153959: step 5287, loss 0.0186652, acc 1
2016-09-06T10:06:41.977085: step 5288, loss 0.00883066, acc 1
2016-09-06T10:06:42.762611: step 5289, loss 0.0277429, acc 1
2016-09-06T10:06:43.588554: step 5290, loss 0.013242, acc 1
2016-09-06T10:06:44.417595: step 5291, loss 0.0228512, acc 0.98
2016-09-06T10:06:45.226276: step 5292, loss 0.0223117, acc 0.98
2016-09-06T10:06:46.029108: step 5293, loss 0.0418553, acc 0.98
2016-09-06T10:06:46.857357: step 5294, loss 0.0455353, acc 0.98
2016-09-06T10:06:47.645626: step 5295, loss 0.00329367, acc 1
2016-09-06T10:06:48.458524: step 5296, loss 0.108701, acc 0.96
2016-09-06T10:06:49.255979: step 5297, loss 0.00563761, acc 1
2016-09-06T10:06:50.056297: step 5298, loss 0.0778645, acc 0.98
2016-09-06T10:06:50.858904: step 5299, loss 0.128154, acc 0.98
2016-09-06T10:06:51.656676: step 5300, loss 0.0307939, acc 1

Evaluation:
2016-09-06T10:06:55.372168: step 5300, loss 1.67775, acc 0.758912

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-5300

2016-09-06T10:06:57.334735: step 5301, loss 0.00927438, acc 1
2016-09-06T10:06:58.179641: step 5302, loss 0.00548694, acc 1
2016-09-06T10:06:58.981497: step 5303, loss 0.00836654, acc 1
2016-09-06T10:06:59.790069: step 5304, loss 0.0232035, acc 1
2016-09-06T10:07:00.632952: step 5305, loss 0.0105131, acc 1
2016-09-06T10:07:01.441883: step 5306, loss 0.0108839, acc 1
2016-09-06T10:07:02.247476: step 5307, loss 0.00325644, acc 1
2016-09-06T10:07:03.101116: step 5308, loss 0.0261015, acc 1
2016-09-06T10:07:03.939036: step 5309, loss 0.0243111, acc 1
2016-09-06T10:07:04.724260: step 5310, loss 0.0620901, acc 0.96
2016-09-06T10:07:05.521864: step 5311, loss 0.0357115, acc 0.98
2016-09-06T10:07:06.325753: step 5312, loss 0.014853, acc 1
2016-09-06T10:07:07.111134: step 5313, loss 0.0171121, acc 0.98
2016-09-06T10:07:07.919232: step 5314, loss 0.0091572, acc 1
2016-09-06T10:07:08.765889: step 5315, loss 0.0298758, acc 0.98
2016-09-06T10:07:09.566907: step 5316, loss 0.0473445, acc 0.96
2016-09-06T10:07:10.358617: step 5317, loss 0.0448156, acc 0.96
2016-09-06T10:07:11.177408: step 5318, loss 0.12905, acc 0.94
2016-09-06T10:07:11.972471: step 5319, loss 0.0234231, acc 1
2016-09-06T10:07:12.782137: step 5320, loss 0.0381302, acc 0.98
2016-09-06T10:07:13.616583: step 5321, loss 0.0270017, acc 0.98
2016-09-06T10:07:14.425455: step 5322, loss 0.0306639, acc 0.98
2016-09-06T10:07:15.247620: step 5323, loss 0.00412164, acc 1
2016-09-06T10:07:16.108274: step 5324, loss 0.00641709, acc 1
2016-09-06T10:07:16.916063: step 5325, loss 0.0388201, acc 0.98
2016-09-06T10:07:17.766295: step 5326, loss 0.0600593, acc 0.98
2016-09-06T10:07:18.620176: step 5327, loss 0.0311393, acc 0.98
2016-09-06T10:07:19.425418: step 5328, loss 0.0456932, acc 0.98
2016-09-06T10:07:20.252203: step 5329, loss 0.0441794, acc 0.98
2016-09-06T10:07:21.096241: step 5330, loss 0.0529013, acc 0.98
2016-09-06T10:07:21.929424: step 5331, loss 0.0298092, acc 1
2016-09-06T10:07:22.745885: step 5332, loss 0.00646103, acc 1
2016-09-06T10:07:23.570096: step 5333, loss 0.0187999, acc 1
2016-09-06T10:07:24.395695: step 5334, loss 0.0883942, acc 0.98
2016-09-06T10:07:25.187821: step 5335, loss 0.0631456, acc 0.96
2016-09-06T10:07:26.000193: step 5336, loss 0.00436466, acc 1
2016-09-06T10:07:26.810380: step 5337, loss 0.0148205, acc 1
2016-09-06T10:07:27.598302: step 5338, loss 0.0357153, acc 0.98
2016-09-06T10:07:28.423814: step 5339, loss 0.0378212, acc 0.98
2016-09-06T10:07:29.238720: step 5340, loss 0.0371779, acc 0.98
2016-09-06T10:07:30.029458: step 5341, loss 0.0384623, acc 0.98
2016-09-06T10:07:30.848758: step 5342, loss 0.0895688, acc 0.96
2016-09-06T10:07:31.661250: step 5343, loss 0.0456892, acc 0.98
2016-09-06T10:07:32.450072: step 5344, loss 0.0910462, acc 0.98
2016-09-06T10:07:33.255192: step 5345, loss 0.0520157, acc 0.96
2016-09-06T10:07:34.075520: step 5346, loss 0.0673016, acc 0.96
2016-09-06T10:07:34.875398: step 5347, loss 0.046709, acc 0.98
2016-09-06T10:07:35.678450: step 5348, loss 0.0143429, acc 1
2016-09-06T10:07:36.505377: step 5349, loss 0.0167181, acc 1
2016-09-06T10:07:37.293082: step 5350, loss 0.0329005, acc 0.96
2016-09-06T10:07:38.094876: step 5351, loss 0.0500467, acc 0.98
2016-09-06T10:07:38.891033: step 5352, loss 0.0474116, acc 0.98
2016-09-06T10:07:39.672343: step 5353, loss 0.0782266, acc 0.96
2016-09-06T10:07:40.495523: step 5354, loss 0.00807715, acc 1
2016-09-06T10:07:41.308358: step 5355, loss 0.0408621, acc 0.98
2016-09-06T10:07:42.109695: step 5356, loss 0.0153593, acc 1
2016-09-06T10:07:42.915779: step 5357, loss 0.0260671, acc 1
2016-09-06T10:07:43.746268: step 5358, loss 0.0177421, acc 1
2016-09-06T10:07:44.563826: step 5359, loss 0.0467729, acc 0.98
2016-09-06T10:07:45.393220: step 5360, loss 0.00607687, acc 1
2016-09-06T10:07:46.181311: step 5361, loss 0.135376, acc 0.96
2016-09-06T10:07:46.988432: step 5362, loss 0.0391857, acc 0.98
2016-09-06T10:07:47.786742: step 5363, loss 0.0157298, acc 1
2016-09-06T10:07:48.583962: step 5364, loss 0.0600547, acc 0.96
2016-09-06T10:07:49.368185: step 5365, loss 0.0254159, acc 0.98
2016-09-06T10:07:50.195215: step 5366, loss 0.0565207, acc 0.98
2016-09-06T10:07:51.053924: step 5367, loss 0.00332849, acc 1
2016-09-06T10:07:51.850028: step 5368, loss 0.0140871, acc 1
2016-09-06T10:07:52.628302: step 5369, loss 0.0321208, acc 0.98
2016-09-06T10:07:53.447801: step 5370, loss 0.0195476, acc 0.98
2016-09-06T10:07:54.221840: step 5371, loss 0.0674495, acc 0.96
2016-09-06T10:07:55.028868: step 5372, loss 0.0218606, acc 0.98
2016-09-06T10:07:55.852740: step 5373, loss 0.103359, acc 0.94
2016-09-06T10:07:56.634112: step 5374, loss 0.0220833, acc 1
2016-09-06T10:07:57.439266: step 5375, loss 0.0130039, acc 1
2016-09-06T10:07:58.200041: step 5376, loss 0.0234952, acc 1
2016-09-06T10:07:59.001551: step 5377, loss 0.0199059, acc 1
2016-09-06T10:07:59.853125: step 5378, loss 0.0579463, acc 0.98
2016-09-06T10:08:00.659831: step 5379, loss 0.0459528, acc 0.96
2016-09-06T10:08:01.472000: step 5380, loss 0.0288742, acc 0.98
2016-09-06T10:08:02.298044: step 5381, loss 0.0487, acc 0.98
2016-09-06T10:08:03.105559: step 5382, loss 0.106628, acc 0.96
2016-09-06T10:08:03.936914: step 5383, loss 0.0455353, acc 0.98
2016-09-06T10:08:04.771632: step 5384, loss 0.0163351, acc 1
2016-09-06T10:08:05.567976: step 5385, loss 0.0312025, acc 1
2016-09-06T10:08:06.343742: step 5386, loss 0.019294, acc 1
2016-09-06T10:08:07.153599: step 5387, loss 0.0274288, acc 0.98
2016-09-06T10:08:07.976461: step 5388, loss 0.0399544, acc 0.98
2016-09-06T10:08:08.820645: step 5389, loss 0.0247793, acc 1
2016-09-06T10:08:09.609291: step 5390, loss 0.0258841, acc 0.98
2016-09-06T10:08:10.437461: step 5391, loss 0.049221, acc 0.98
2016-09-06T10:08:11.246184: step 5392, loss 0.0191225, acc 0.98
2016-09-06T10:08:12.055461: step 5393, loss 0.00618211, acc 1
2016-09-06T10:08:12.876664: step 5394, loss 0.00310652, acc 1
2016-09-06T10:08:13.662144: step 5395, loss 0.0179479, acc 0.98
2016-09-06T10:08:14.462337: step 5396, loss 0.00563429, acc 1
2016-09-06T10:08:15.277218: step 5397, loss 0.0271629, acc 0.98
2016-09-06T10:08:16.060374: step 5398, loss 0.0248355, acc 1
2016-09-06T10:08:16.876418: step 5399, loss 0.00469887, acc 1
2016-09-06T10:08:17.705863: step 5400, loss 0.00958206, acc 1

Evaluation:
2016-09-06T10:08:21.413230: step 5400, loss 1.7511, acc 0.749531

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-5400

2016-09-06T10:08:23.293507: step 5401, loss 0.00335306, acc 1
2016-09-06T10:08:24.112913: step 5402, loss 0.0500598, acc 0.98
2016-09-06T10:08:24.938966: step 5403, loss 0.0276602, acc 1
2016-09-06T10:08:25.728154: step 5404, loss 0.00567566, acc 1
2016-09-06T10:08:26.559286: step 5405, loss 0.0070349, acc 1
2016-09-06T10:08:27.368437: step 5406, loss 0.0292496, acc 0.98
2016-09-06T10:08:28.156354: step 5407, loss 0.00845388, acc 1
2016-09-06T10:08:28.989664: step 5408, loss 0.0430537, acc 0.98
2016-09-06T10:08:29.803816: step 5409, loss 0.00287247, acc 1
2016-09-06T10:08:30.607263: step 5410, loss 0.0119358, acc 1
2016-09-06T10:08:31.416752: step 5411, loss 0.00385212, acc 1
2016-09-06T10:08:32.237448: step 5412, loss 0.00529748, acc 1
2016-09-06T10:08:33.045448: step 5413, loss 0.0339371, acc 1
2016-09-06T10:08:33.861352: step 5414, loss 0.0107235, acc 1
2016-09-06T10:08:34.680261: step 5415, loss 0.0227572, acc 1
2016-09-06T10:08:35.490317: step 5416, loss 0.0281069, acc 1
2016-09-06T10:08:36.282278: step 5417, loss 0.0186206, acc 0.98
2016-09-06T10:08:37.095243: step 5418, loss 0.0216187, acc 1
2016-09-06T10:08:37.872393: step 5419, loss 0.01567, acc 1
2016-09-06T10:08:38.690795: step 5420, loss 0.0209912, acc 0.98
2016-09-06T10:08:39.515970: step 5421, loss 0.00292122, acc 1
2016-09-06T10:08:40.310027: step 5422, loss 0.00307289, acc 1
2016-09-06T10:08:41.091107: step 5423, loss 0.0089572, acc 1
2016-09-06T10:08:41.916296: step 5424, loss 0.0396315, acc 0.98
2016-09-06T10:08:42.707242: step 5425, loss 0.0191234, acc 1
2016-09-06T10:08:43.551440: step 5426, loss 0.00333504, acc 1
2016-09-06T10:08:44.397435: step 5427, loss 0.0422479, acc 0.96
2016-09-06T10:08:45.199624: step 5428, loss 0.0495014, acc 0.98
2016-09-06T10:08:46.005123: step 5429, loss 0.0360815, acc 0.98
2016-09-06T10:08:46.843928: step 5430, loss 0.018994, acc 0.98
2016-09-06T10:08:47.686676: step 5431, loss 0.02734, acc 1
2016-09-06T10:08:48.497431: step 5432, loss 0.0143604, acc 1
2016-09-06T10:08:49.323967: step 5433, loss 0.0255504, acc 0.98
2016-09-06T10:08:50.140554: step 5434, loss 0.0220079, acc 0.98
2016-09-06T10:08:50.961682: step 5435, loss 0.00718542, acc 1
2016-09-06T10:08:51.801563: step 5436, loss 0.00305106, acc 1
2016-09-06T10:08:52.610181: step 5437, loss 0.025375, acc 1
2016-09-06T10:08:53.416123: step 5438, loss 0.00382682, acc 1
2016-09-06T10:08:54.247290: step 5439, loss 0.00301012, acc 1
2016-09-06T10:08:55.056341: step 5440, loss 0.0503814, acc 0.96
2016-09-06T10:08:55.882634: step 5441, loss 0.0596354, acc 0.98
2016-09-06T10:08:56.701460: step 5442, loss 0.00507444, acc 1
2016-09-06T10:08:57.515333: step 5443, loss 0.0392736, acc 0.96
2016-09-06T10:08:58.300392: step 5444, loss 0.0533774, acc 0.96
2016-09-06T10:08:59.170050: step 5445, loss 0.0226382, acc 0.98
2016-09-06T10:08:59.967592: step 5446, loss 0.0174984, acc 1
2016-09-06T10:09:00.810569: step 5447, loss 0.0211705, acc 1
2016-09-06T10:09:01.627369: step 5448, loss 0.0584962, acc 0.96
2016-09-06T10:09:02.438512: step 5449, loss 0.017833, acc 1
2016-09-06T10:09:03.225420: step 5450, loss 0.0308347, acc 0.98
2016-09-06T10:09:04.033490: step 5451, loss 0.0610517, acc 0.98
2016-09-06T10:09:04.852927: step 5452, loss 0.0243553, acc 0.98
2016-09-06T10:09:05.624799: step 5453, loss 0.0049412, acc 1
2016-09-06T10:09:06.461920: step 5454, loss 0.0465526, acc 0.98
2016-09-06T10:09:07.263588: step 5455, loss 0.0345123, acc 0.98
2016-09-06T10:09:08.074269: step 5456, loss 0.00320161, acc 1
2016-09-06T10:09:08.871781: step 5457, loss 0.0460023, acc 0.98
2016-09-06T10:09:09.696054: step 5458, loss 0.00387139, acc 1
2016-09-06T10:09:10.488835: step 5459, loss 0.0483522, acc 0.98
2016-09-06T10:09:11.308515: step 5460, loss 0.0225843, acc 0.98
2016-09-06T10:09:12.136683: step 5461, loss 0.00701896, acc 1
2016-09-06T10:09:12.903024: step 5462, loss 0.0242556, acc 1
2016-09-06T10:09:13.696655: step 5463, loss 0.0248219, acc 0.98
2016-09-06T10:09:14.515391: step 5464, loss 0.0251853, acc 0.98
2016-09-06T10:09:15.286796: step 5465, loss 0.107625, acc 0.98
2016-09-06T10:09:16.092329: step 5466, loss 0.0192658, acc 0.98
2016-09-06T10:09:16.913908: step 5467, loss 0.0132575, acc 1
2016-09-06T10:09:17.717059: step 5468, loss 0.00428777, acc 1
2016-09-06T10:09:18.530976: step 5469, loss 0.0290237, acc 1
2016-09-06T10:09:19.358194: step 5470, loss 0.00903763, acc 1
2016-09-06T10:09:20.145456: step 5471, loss 0.0555744, acc 0.96
2016-09-06T10:09:20.960138: step 5472, loss 0.0726076, acc 0.96
2016-09-06T10:09:21.767153: step 5473, loss 0.0247583, acc 0.98
2016-09-06T10:09:22.564866: step 5474, loss 0.0326581, acc 1
2016-09-06T10:09:23.369204: step 5475, loss 0.0174039, acc 1
2016-09-06T10:09:24.181035: step 5476, loss 0.024373, acc 1
2016-09-06T10:09:24.976793: step 5477, loss 0.0389008, acc 0.98
2016-09-06T10:09:25.805383: step 5478, loss 0.0243865, acc 0.98
2016-09-06T10:09:26.612417: step 5479, loss 0.00991876, acc 1
2016-09-06T10:09:27.419392: step 5480, loss 0.00761474, acc 1
2016-09-06T10:09:28.267139: step 5481, loss 0.0375915, acc 0.96
2016-09-06T10:09:29.081979: step 5482, loss 0.0323677, acc 1
2016-09-06T10:09:29.851656: step 5483, loss 0.0357377, acc 0.98
2016-09-06T10:09:30.653389: step 5484, loss 0.0305525, acc 1
2016-09-06T10:09:31.465220: step 5485, loss 0.0140516, acc 1
2016-09-06T10:09:32.281980: step 5486, loss 0.135772, acc 0.94
2016-09-06T10:09:33.083070: step 5487, loss 0.0217727, acc 0.98
2016-09-06T10:09:33.886448: step 5488, loss 0.0575995, acc 0.98
2016-09-06T10:09:34.709033: step 5489, loss 0.0381842, acc 0.96
2016-09-06T10:09:35.511275: step 5490, loss 0.104465, acc 0.98
2016-09-06T10:09:36.297034: step 5491, loss 0.104053, acc 0.96
2016-09-06T10:09:37.081778: step 5492, loss 0.0773034, acc 0.96
2016-09-06T10:09:37.884681: step 5493, loss 0.027559, acc 0.98
2016-09-06T10:09:38.714771: step 5494, loss 0.00800491, acc 1
2016-09-06T10:09:39.532551: step 5495, loss 0.0448944, acc 0.98
2016-09-06T10:09:40.333872: step 5496, loss 0.0348805, acc 1
2016-09-06T10:09:41.142286: step 5497, loss 0.010281, acc 1
2016-09-06T10:09:41.930290: step 5498, loss 0.0122686, acc 1
2016-09-06T10:09:42.744226: step 5499, loss 0.0354011, acc 0.98
2016-09-06T10:09:43.571729: step 5500, loss 0.0279905, acc 1

Evaluation:
2016-09-06T10:09:47.252032: step 5500, loss 1.41277, acc 0.754221

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-5500

2016-09-06T10:09:49.073081: step 5501, loss 0.0597141, acc 0.98
2016-09-06T10:09:49.936514: step 5502, loss 0.077513, acc 0.98
2016-09-06T10:09:50.746639: step 5503, loss 0.0181943, acc 1
2016-09-06T10:09:51.565822: step 5504, loss 0.108089, acc 0.94
2016-09-06T10:09:52.382701: step 5505, loss 0.0873272, acc 0.92
2016-09-06T10:09:53.172599: step 5506, loss 0.0546915, acc 0.96
2016-09-06T10:09:53.986085: step 5507, loss 0.0222279, acc 1
2016-09-06T10:09:54.806697: step 5508, loss 0.0357711, acc 0.98
2016-09-06T10:09:55.623540: step 5509, loss 0.0459222, acc 0.96
2016-09-06T10:09:56.427492: step 5510, loss 0.027752, acc 1
2016-09-06T10:09:57.229802: step 5511, loss 0.0461456, acc 0.98
2016-09-06T10:09:58.041579: step 5512, loss 0.0199518, acc 1
2016-09-06T10:09:58.834387: step 5513, loss 0.0193944, acc 1
2016-09-06T10:09:59.655399: step 5514, loss 0.0668523, acc 0.96
2016-09-06T10:10:00.497504: step 5515, loss 0.00978445, acc 1
2016-09-06T10:10:01.283625: step 5516, loss 0.0110541, acc 1
2016-09-06T10:10:02.079142: step 5517, loss 0.048216, acc 0.98
2016-09-06T10:10:02.919337: step 5518, loss 0.0125958, acc 1
2016-09-06T10:10:03.690268: step 5519, loss 0.0128173, acc 1
2016-09-06T10:10:04.497997: step 5520, loss 0.0482216, acc 0.98
2016-09-06T10:10:05.327745: step 5521, loss 0.0300313, acc 0.98
2016-09-06T10:10:06.153006: step 5522, loss 0.0216296, acc 0.98
2016-09-06T10:10:06.967037: step 5523, loss 0.0131861, acc 1
2016-09-06T10:10:07.768841: step 5524, loss 0.0103285, acc 1
2016-09-06T10:10:08.563136: step 5525, loss 0.00449654, acc 1
2016-09-06T10:10:09.331687: step 5526, loss 0.0340307, acc 1
2016-09-06T10:10:10.128818: step 5527, loss 0.0281815, acc 0.98
2016-09-06T10:10:10.950845: step 5528, loss 0.0253434, acc 0.98
2016-09-06T10:10:11.801253: step 5529, loss 0.00634537, acc 1
2016-09-06T10:10:12.660683: step 5530, loss 0.00426239, acc 1
2016-09-06T10:10:13.463870: step 5531, loss 0.00452712, acc 1
2016-09-06T10:10:14.279382: step 5532, loss 0.0466937, acc 0.98
2016-09-06T10:10:15.147030: step 5533, loss 0.00502747, acc 1
2016-09-06T10:10:15.979172: step 5534, loss 0.00848381, acc 1
2016-09-06T10:10:16.802514: step 5535, loss 0.0100806, acc 1
2016-09-06T10:10:17.645490: step 5536, loss 0.105704, acc 0.98
2016-09-06T10:10:18.481857: step 5537, loss 0.131007, acc 0.96
2016-09-06T10:10:19.300081: step 5538, loss 0.0196872, acc 0.98
2016-09-06T10:10:20.148767: step 5539, loss 0.0132159, acc 1
2016-09-06T10:10:20.957065: step 5540, loss 0.0180479, acc 0.98
2016-09-06T10:10:21.783754: step 5541, loss 0.00845779, acc 1
2016-09-06T10:10:22.674978: step 5542, loss 0.0255751, acc 1
2016-09-06T10:10:23.490746: step 5543, loss 0.00537015, acc 1
2016-09-06T10:10:24.269398: step 5544, loss 0.0870076, acc 0.96
2016-09-06T10:10:25.040271: step 5545, loss 0.0319567, acc 0.98
2016-09-06T10:10:25.868078: step 5546, loss 0.0202496, acc 1
2016-09-06T10:10:26.665895: step 5547, loss 0.0653543, acc 0.96
2016-09-06T10:10:27.474406: step 5548, loss 0.0171516, acc 1
2016-09-06T10:10:28.297455: step 5549, loss 0.0194285, acc 0.98
2016-09-06T10:10:29.073377: step 5550, loss 0.0297452, acc 1
2016-09-06T10:10:29.881061: step 5551, loss 0.150156, acc 0.96
2016-09-06T10:10:30.703281: step 5552, loss 0.0226694, acc 1
2016-09-06T10:10:31.498733: step 5553, loss 0.0277497, acc 1
2016-09-06T10:10:32.343299: step 5554, loss 0.0564059, acc 0.98
2016-09-06T10:10:33.145303: step 5555, loss 0.0369618, acc 0.96
2016-09-06T10:10:33.942674: step 5556, loss 0.0421737, acc 0.98
2016-09-06T10:10:34.739470: step 5557, loss 0.0109863, acc 1
2016-09-06T10:10:35.557447: step 5558, loss 0.0200841, acc 0.98
2016-09-06T10:10:36.329925: step 5559, loss 0.0283391, acc 0.98
2016-09-06T10:10:37.141915: step 5560, loss 0.0035832, acc 1
2016-09-06T10:10:37.980906: step 5561, loss 0.0201054, acc 0.98
2016-09-06T10:10:38.777576: step 5562, loss 0.0322028, acc 1
2016-09-06T10:10:39.569402: step 5563, loss 0.0995739, acc 0.94
2016-09-06T10:10:40.399383: step 5564, loss 0.0267559, acc 0.98
2016-09-06T10:10:41.196150: step 5565, loss 0.0446678, acc 0.98
2016-09-06T10:10:42.011380: step 5566, loss 0.0224263, acc 0.98
2016-09-06T10:10:42.842591: step 5567, loss 0.0200562, acc 0.98
2016-09-06T10:10:43.605352: step 5568, loss 0.0602394, acc 0.977273
2016-09-06T10:10:44.403824: step 5569, loss 0.00852959, acc 1
2016-09-06T10:10:45.222519: step 5570, loss 0.0340049, acc 1
2016-09-06T10:10:46.007506: step 5571, loss 0.0533432, acc 0.98
2016-09-06T10:10:46.792815: step 5572, loss 0.0035195, acc 1
2016-09-06T10:10:47.623171: step 5573, loss 0.0110088, acc 1
2016-09-06T10:10:48.394488: step 5574, loss 0.00894721, acc 1
2016-09-06T10:10:49.199267: step 5575, loss 0.0144384, acc 1
2016-09-06T10:10:50.028821: step 5576, loss 0.0237565, acc 1
2016-09-06T10:10:50.809305: step 5577, loss 0.025558, acc 0.98
2016-09-06T10:10:51.614906: step 5578, loss 0.0392197, acc 0.98
2016-09-06T10:10:52.463577: step 5579, loss 0.00379965, acc 1
2016-09-06T10:10:53.249665: step 5580, loss 0.0174102, acc 1
2016-09-06T10:10:54.030677: step 5581, loss 0.0420406, acc 0.96
2016-09-06T10:10:54.848806: step 5582, loss 0.0381569, acc 1
2016-09-06T10:10:55.658020: step 5583, loss 0.0189508, acc 1
2016-09-06T10:10:56.455682: step 5584, loss 0.00380968, acc 1
2016-09-06T10:10:57.305972: step 5585, loss 0.136174, acc 0.96
2016-09-06T10:10:58.094386: step 5586, loss 0.0046306, acc 1
2016-09-06T10:10:58.882019: step 5587, loss 0.0189867, acc 0.98
2016-09-06T10:10:59.686849: step 5588, loss 0.003417, acc 1
2016-09-06T10:11:00.516773: step 5589, loss 0.00707528, acc 1
2016-09-06T10:11:01.334774: step 5590, loss 0.0148575, acc 1
2016-09-06T10:11:02.170491: step 5591, loss 0.0225786, acc 1
2016-09-06T10:11:02.959590: step 5592, loss 0.0383834, acc 0.98
2016-09-06T10:11:03.761668: step 5593, loss 0.00322105, acc 1
2016-09-06T10:11:04.579793: step 5594, loss 0.00803567, acc 1
2016-09-06T10:11:05.400618: step 5595, loss 0.0249325, acc 1
2016-09-06T10:11:06.208295: step 5596, loss 0.00534399, acc 1
2016-09-06T10:11:07.041549: step 5597, loss 0.0302085, acc 0.98
2016-09-06T10:11:07.865558: step 5598, loss 0.0140616, acc 1
2016-09-06T10:11:08.661412: step 5599, loss 0.00456044, acc 1
2016-09-06T10:11:09.492272: step 5600, loss 0.0142021, acc 1

Evaluation:
2016-09-06T10:11:13.234280: step 5600, loss 1.77638, acc 0.763602

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-5600

2016-09-06T10:11:15.136078: step 5601, loss 0.0875495, acc 0.98
2016-09-06T10:11:15.963814: step 5602, loss 0.0130582, acc 1
2016-09-06T10:11:16.801855: step 5603, loss 0.00337438, acc 1
2016-09-06T10:11:17.639223: step 5604, loss 0.0191361, acc 0.98
2016-09-06T10:11:18.451327: step 5605, loss 0.0328066, acc 0.96
2016-09-06T10:11:19.289940: step 5606, loss 0.0176435, acc 0.98
2016-09-06T10:11:20.144898: step 5607, loss 0.0252496, acc 1
2016-09-06T10:11:20.982296: step 5608, loss 0.0121181, acc 1
2016-09-06T10:11:21.815506: step 5609, loss 0.0216909, acc 0.98
2016-09-06T10:11:22.656360: step 5610, loss 0.0105429, acc 1
2016-09-06T10:11:23.480793: step 5611, loss 0.00430756, acc 1
2016-09-06T10:11:24.287405: step 5612, loss 0.00390148, acc 1
2016-09-06T10:11:25.098859: step 5613, loss 0.0183917, acc 1
2016-09-06T10:11:25.953103: step 5614, loss 0.0510838, acc 0.98
2016-09-06T10:11:26.779219: step 5615, loss 0.00918326, acc 1
2016-09-06T10:11:27.598398: step 5616, loss 0.00998737, acc 1
2016-09-06T10:11:28.405431: step 5617, loss 0.0277181, acc 1
2016-09-06T10:11:29.230201: step 5618, loss 0.00481519, acc 1
2016-09-06T10:11:30.051355: step 5619, loss 0.00286058, acc 1
2016-09-06T10:11:30.845877: step 5620, loss 0.0440671, acc 0.98
2016-09-06T10:11:31.692335: step 5621, loss 0.0535425, acc 0.96
2016-09-06T10:11:32.519507: step 5622, loss 0.067096, acc 0.96
2016-09-06T10:11:33.328609: step 5623, loss 0.0164336, acc 1
2016-09-06T10:11:34.141637: step 5624, loss 0.00290987, acc 1
2016-09-06T10:11:34.958626: step 5625, loss 0.00445766, acc 1
2016-09-06T10:11:35.787853: step 5626, loss 0.018155, acc 1
2016-09-06T10:11:36.605378: step 5627, loss 0.0172304, acc 0.98
2016-09-06T10:11:37.432946: step 5628, loss 0.104976, acc 0.96
2016-09-06T10:11:38.270475: step 5629, loss 0.0190053, acc 0.98
2016-09-06T10:11:39.088339: step 5630, loss 0.00418735, acc 1
2016-09-06T10:11:39.935637: step 5631, loss 0.00454683, acc 1
2016-09-06T10:11:40.709692: step 5632, loss 0.0370518, acc 0.96
2016-09-06T10:11:41.535329: step 5633, loss 0.0203649, acc 1
2016-09-06T10:11:42.378034: step 5634, loss 0.067071, acc 0.96
2016-09-06T10:11:43.229591: step 5635, loss 0.0215516, acc 1
2016-09-06T10:11:44.071518: step 5636, loss 0.0304643, acc 0.98
2016-09-06T10:11:44.882095: step 5637, loss 0.0566373, acc 0.98
2016-09-06T10:11:45.679646: step 5638, loss 0.0274766, acc 0.98
2016-09-06T10:11:46.509809: step 5639, loss 0.00783533, acc 1
2016-09-06T10:11:47.308329: step 5640, loss 0.0217779, acc 1
2016-09-06T10:11:48.114326: step 5641, loss 0.0228465, acc 1
2016-09-06T10:11:48.907243: step 5642, loss 0.00583916, acc 1
2016-09-06T10:11:49.753441: step 5643, loss 0.0241742, acc 0.98
2016-09-06T10:11:50.606099: step 5644, loss 0.0134408, acc 1
2016-09-06T10:11:51.391553: step 5645, loss 0.0206021, acc 0.98
2016-09-06T10:11:52.180956: step 5646, loss 0.0427376, acc 0.98
2016-09-06T10:11:52.998689: step 5647, loss 0.0161792, acc 1
2016-09-06T10:11:53.796661: step 5648, loss 0.0358717, acc 0.98
2016-09-06T10:11:54.599141: step 5649, loss 0.00256413, acc 1
2016-09-06T10:11:55.448720: step 5650, loss 0.0560744, acc 0.98
2016-09-06T10:11:56.232658: step 5651, loss 0.0311229, acc 1
2016-09-06T10:11:57.010652: step 5652, loss 0.0067646, acc 1
2016-09-06T10:11:57.823952: step 5653, loss 0.0127072, acc 1
2016-09-06T10:11:58.618214: step 5654, loss 0.0447369, acc 1
2016-09-06T10:11:59.437527: step 5655, loss 0.0337532, acc 0.98
2016-09-06T10:12:00.251402: step 5656, loss 0.0298305, acc 0.98
2016-09-06T10:12:01.066201: step 5657, loss 0.0145735, acc 1
2016-09-06T10:12:01.900361: step 5658, loss 0.00325962, acc 1
2016-09-06T10:12:02.727277: step 5659, loss 0.0660551, acc 0.96
2016-09-06T10:12:03.547247: step 5660, loss 0.00601938, acc 1
2016-09-06T10:12:04.349117: step 5661, loss 0.0346139, acc 0.98
2016-09-06T10:12:05.142522: step 5662, loss 0.00730674, acc 1
2016-09-06T10:12:05.957236: step 5663, loss 0.0148664, acc 1
2016-09-06T10:12:06.770768: step 5664, loss 0.0121423, acc 1
2016-09-06T10:12:07.591983: step 5665, loss 0.0081418, acc 1
2016-09-06T10:12:08.407059: step 5666, loss 0.0160715, acc 1
2016-09-06T10:12:09.207565: step 5667, loss 0.0197879, acc 0.98
2016-09-06T10:12:10.046647: step 5668, loss 0.0212872, acc 0.98
2016-09-06T10:12:10.857883: step 5669, loss 0.020295, acc 0.98
2016-09-06T10:12:11.664472: step 5670, loss 0.0710463, acc 0.96
2016-09-06T10:12:12.498580: step 5671, loss 0.00298199, acc 1
2016-09-06T10:12:13.301808: step 5672, loss 0.00382796, acc 1
2016-09-06T10:12:14.100960: step 5673, loss 0.0177641, acc 0.98
2016-09-06T10:12:14.906019: step 5674, loss 0.0222888, acc 0.98
2016-09-06T10:12:15.701818: step 5675, loss 0.0161605, acc 1
2016-09-06T10:12:16.509893: step 5676, loss 0.0321515, acc 0.98
2016-09-06T10:12:17.344703: step 5677, loss 0.0292156, acc 1
2016-09-06T10:12:18.158471: step 5678, loss 0.0919683, acc 0.98
2016-09-06T10:12:18.962415: step 5679, loss 0.0193318, acc 0.98
2016-09-06T10:12:19.777879: step 5680, loss 0.0181535, acc 1
2016-09-06T10:12:20.597973: step 5681, loss 0.0383579, acc 0.98
2016-09-06T10:12:21.394074: step 5682, loss 0.0635966, acc 0.98
2016-09-06T10:12:22.258541: step 5683, loss 0.0676721, acc 0.98
2016-09-06T10:12:23.075093: step 5684, loss 0.0293433, acc 0.98
2016-09-06T10:12:23.909259: step 5685, loss 0.0210954, acc 0.98
2016-09-06T10:12:24.785876: step 5686, loss 0.00402612, acc 1
2016-09-06T10:12:25.605402: step 5687, loss 0.038114, acc 0.98
2016-09-06T10:12:26.399256: step 5688, loss 0.0162282, acc 1
2016-09-06T10:12:27.261623: step 5689, loss 0.0241244, acc 1
2016-09-06T10:12:28.111232: step 5690, loss 0.00250219, acc 1
2016-09-06T10:12:28.936301: step 5691, loss 0.00528917, acc 1
2016-09-06T10:12:29.752343: step 5692, loss 0.0488875, acc 1
2016-09-06T10:12:30.565048: step 5693, loss 0.0365253, acc 0.98
2016-09-06T10:12:31.354469: step 5694, loss 0.0261414, acc 0.98
2016-09-06T10:12:32.162734: step 5695, loss 0.0153785, acc 1
2016-09-06T10:12:32.978881: step 5696, loss 0.0206495, acc 1
2016-09-06T10:12:33.790341: step 5697, loss 0.0282213, acc 1
2016-09-06T10:12:34.602930: step 5698, loss 0.141326, acc 0.98
2016-09-06T10:12:35.437456: step 5699, loss 0.0171249, acc 0.98
2016-09-06T10:12:36.238581: step 5700, loss 0.0437592, acc 0.98

Evaluation:
2016-09-06T10:12:39.992634: step 5700, loss 1.7145, acc 0.760788

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-5700

2016-09-06T10:12:41.862381: step 5701, loss 0.0426449, acc 1
2016-09-06T10:12:42.689263: step 5702, loss 0.0343373, acc 0.98
2016-09-06T10:12:43.513431: step 5703, loss 0.0258751, acc 0.98
2016-09-06T10:12:44.329975: step 5704, loss 0.00426224, acc 1
2016-09-06T10:12:45.167536: step 5705, loss 0.119718, acc 0.96
2016-09-06T10:12:45.973702: step 5706, loss 0.0299767, acc 0.98
2016-09-06T10:12:46.796894: step 5707, loss 0.00891458, acc 1
2016-09-06T10:12:47.626067: step 5708, loss 0.00996597, acc 1
2016-09-06T10:12:48.436104: step 5709, loss 0.0565774, acc 0.98
2016-09-06T10:12:49.277409: step 5710, loss 0.0619844, acc 0.98
2016-09-06T10:12:50.102824: step 5711, loss 0.0235469, acc 0.98
2016-09-06T10:12:50.920566: step 5712, loss 0.0598716, acc 0.96
2016-09-06T10:12:51.748817: step 5713, loss 0.00522583, acc 1
2016-09-06T10:12:52.610914: step 5714, loss 0.0137272, acc 1
2016-09-06T10:12:53.422948: step 5715, loss 0.0111105, acc 1
2016-09-06T10:12:54.206036: step 5716, loss 0.00720041, acc 1
2016-09-06T10:12:55.043189: step 5717, loss 0.107221, acc 0.98
2016-09-06T10:12:55.893469: step 5718, loss 0.0438157, acc 0.98
2016-09-06T10:12:56.705667: step 5719, loss 0.00558823, acc 1
2016-09-06T10:12:57.498871: step 5720, loss 0.0165004, acc 1
2016-09-06T10:12:58.333424: step 5721, loss 0.0165969, acc 1
2016-09-06T10:12:59.106522: step 5722, loss 0.00726565, acc 1
2016-09-06T10:12:59.894492: step 5723, loss 0.0398721, acc 0.98
2016-09-06T10:13:00.728143: step 5724, loss 0.085633, acc 0.96
2016-09-06T10:13:01.527677: step 5725, loss 0.0181754, acc 1
2016-09-06T10:13:02.324878: step 5726, loss 0.0399042, acc 0.98
2016-09-06T10:13:03.170075: step 5727, loss 0.0274478, acc 0.98
2016-09-06T10:13:03.931836: step 5728, loss 0.021736, acc 1
2016-09-06T10:13:04.734816: step 5729, loss 0.0202052, acc 1
2016-09-06T10:13:05.536889: step 5730, loss 0.0334253, acc 1
2016-09-06T10:13:06.318606: step 5731, loss 0.0266699, acc 0.98
2016-09-06T10:13:07.195963: step 5732, loss 0.0223956, acc 0.98
2016-09-06T10:13:08.004858: step 5733, loss 0.0107225, acc 1
2016-09-06T10:13:08.795020: step 5734, loss 0.0140569, acc 1
2016-09-06T10:13:09.601460: step 5735, loss 0.0117014, acc 1
2016-09-06T10:13:10.429291: step 5736, loss 0.0187401, acc 1
2016-09-06T10:13:11.213392: step 5737, loss 0.0392787, acc 0.98
2016-09-06T10:13:12.015669: step 5738, loss 0.0291557, acc 1
2016-09-06T10:13:12.839901: step 5739, loss 0.0539499, acc 0.98
2016-09-06T10:13:13.614978: step 5740, loss 0.00801656, acc 1
2016-09-06T10:13:14.430864: step 5741, loss 0.0172667, acc 0.98
2016-09-06T10:13:15.280430: step 5742, loss 0.0512028, acc 0.96
2016-09-06T10:13:16.075102: step 5743, loss 0.0703666, acc 0.94
2016-09-06T10:13:16.872960: step 5744, loss 0.113835, acc 0.98
2016-09-06T10:13:17.681836: step 5745, loss 0.0284649, acc 0.98
2016-09-06T10:13:18.468652: step 5746, loss 0.0516416, acc 1
2016-09-06T10:13:19.252721: step 5747, loss 0.00437238, acc 1
2016-09-06T10:13:20.086623: step 5748, loss 0.0235819, acc 1
2016-09-06T10:13:20.877413: step 5749, loss 0.0402967, acc 0.96
2016-09-06T10:13:21.671636: step 5750, loss 0.0540575, acc 0.98
2016-09-06T10:13:22.495285: step 5751, loss 0.00365394, acc 1
2016-09-06T10:13:23.274281: step 5752, loss 0.00504618, acc 1
2016-09-06T10:13:24.070305: step 5753, loss 0.00851924, acc 1
2016-09-06T10:13:24.900747: step 5754, loss 0.0198951, acc 1
2016-09-06T10:13:25.686985: step 5755, loss 0.0115884, acc 1
2016-09-06T10:13:26.490285: step 5756, loss 0.00907087, acc 1
2016-09-06T10:13:27.314754: step 5757, loss 0.0492612, acc 0.98
2016-09-06T10:13:28.123664: step 5758, loss 0.0186807, acc 0.98
2016-09-06T10:13:28.925449: step 5759, loss 0.00944873, acc 1
2016-09-06T10:13:29.681247: step 5760, loss 0.112376, acc 0.977273
2016-09-06T10:13:30.505396: step 5761, loss 0.0399757, acc 0.98
2016-09-06T10:13:31.349666: step 5762, loss 0.0340753, acc 0.98
2016-09-06T10:13:32.182712: step 5763, loss 0.0967562, acc 0.96
2016-09-06T10:13:33.020300: step 5764, loss 0.0597305, acc 0.94
2016-09-06T10:13:33.830887: step 5765, loss 0.0181735, acc 1
2016-09-06T10:13:34.684503: step 5766, loss 0.0462153, acc 0.96
2016-09-06T10:13:35.496970: step 5767, loss 0.0241141, acc 0.98
2016-09-06T10:13:36.303084: step 5768, loss 0.0389341, acc 0.96
2016-09-06T10:13:37.148723: step 5769, loss 0.0206914, acc 1
2016-09-06T10:13:37.974764: step 5770, loss 0.061709, acc 0.98
2016-09-06T10:13:38.785884: step 5771, loss 0.0112992, acc 1
2016-09-06T10:13:39.615743: step 5772, loss 0.0134016, acc 1
2016-09-06T10:13:40.430769: step 5773, loss 0.0444479, acc 0.96
2016-09-06T10:13:41.239342: step 5774, loss 0.0148114, acc 1
2016-09-06T10:13:42.047026: step 5775, loss 0.037551, acc 0.98
2016-09-06T10:13:42.868158: step 5776, loss 0.0320827, acc 0.98
2016-09-06T10:13:43.684156: step 5777, loss 0.0793897, acc 0.94
2016-09-06T10:13:44.543605: step 5778, loss 0.00671211, acc 1
2016-09-06T10:13:45.360292: step 5779, loss 0.0128793, acc 1
2016-09-06T10:13:46.170608: step 5780, loss 0.0161512, acc 1
2016-09-06T10:13:46.995085: step 5781, loss 0.00237109, acc 1
2016-09-06T10:13:47.819990: step 5782, loss 0.0116021, acc 1
2016-09-06T10:13:48.599567: step 5783, loss 0.00761138, acc 1
2016-09-06T10:13:49.424255: step 5784, loss 0.0545913, acc 0.98
2016-09-06T10:13:50.233456: step 5785, loss 0.0144537, acc 1
2016-09-06T10:13:51.042234: step 5786, loss 0.0115886, acc 1
2016-09-06T10:13:51.827491: step 5787, loss 0.0444537, acc 1
2016-09-06T10:13:52.647477: step 5788, loss 0.00906848, acc 1
2016-09-06T10:13:53.440958: step 5789, loss 0.0160406, acc 1
2016-09-06T10:13:54.231335: step 5790, loss 0.0158137, acc 1
2016-09-06T10:13:55.054723: step 5791, loss 0.0101021, acc 1
2016-09-06T10:13:55.844567: step 5792, loss 0.0401568, acc 0.98
2016-09-06T10:13:56.681717: step 5793, loss 0.0276652, acc 0.98
2016-09-06T10:13:57.488918: step 5794, loss 0.0117503, acc 1
2016-09-06T10:13:58.281944: step 5795, loss 0.0344541, acc 1
2016-09-06T10:13:59.097803: step 5796, loss 0.0151254, acc 1
2016-09-06T10:13:59.905076: step 5797, loss 0.0210982, acc 1
2016-09-06T10:14:00.710774: step 5798, loss 0.0456582, acc 0.98
2016-09-06T10:14:01.502281: step 5799, loss 0.00880151, acc 1
2016-09-06T10:14:02.326737: step 5800, loss 0.0423589, acc 0.96

Evaluation:
2016-09-06T10:14:06.057943: step 5800, loss 1.97395, acc 0.752345

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-5800

2016-09-06T10:14:07.980710: step 5801, loss 0.028215, acc 1
2016-09-06T10:14:08.819944: step 5802, loss 0.00997645, acc 1
2016-09-06T10:14:09.684666: step 5803, loss 0.026565, acc 0.98
2016-09-06T10:14:10.496481: step 5804, loss 0.0116332, acc 1
2016-09-06T10:14:11.300636: step 5805, loss 0.00310724, acc 1
2016-09-06T10:14:12.169405: step 5806, loss 0.0183917, acc 1
2016-09-06T10:14:12.992081: step 5807, loss 0.0152912, acc 1
2016-09-06T10:14:13.796807: step 5808, loss 0.0115317, acc 1
2016-09-06T10:14:14.621978: step 5809, loss 0.00396575, acc 1
2016-09-06T10:14:15.431921: step 5810, loss 0.00374846, acc 1
2016-09-06T10:14:16.237985: step 5811, loss 0.0148561, acc 1
2016-09-06T10:14:17.051737: step 5812, loss 0.0426447, acc 0.98
2016-09-06T10:14:17.860960: step 5813, loss 0.0481989, acc 0.96
2016-09-06T10:14:18.680425: step 5814, loss 0.00465248, acc 1
2016-09-06T10:14:19.537567: step 5815, loss 0.0234487, acc 0.98
2016-09-06T10:14:20.356373: step 5816, loss 0.0886557, acc 0.98
2016-09-06T10:14:21.160145: step 5817, loss 0.00383116, acc 1
2016-09-06T10:14:21.970665: step 5818, loss 0.0703689, acc 0.92
2016-09-06T10:14:22.808570: step 5819, loss 0.0266636, acc 0.98
2016-09-06T10:14:23.622548: step 5820, loss 0.00418899, acc 1
2016-09-06T10:14:24.482605: step 5821, loss 0.0622322, acc 0.98
2016-09-06T10:14:25.285491: step 5822, loss 0.00544825, acc 1
2016-09-06T10:14:26.090558: step 5823, loss 0.00610428, acc 1
2016-09-06T10:14:26.908917: step 5824, loss 0.0255312, acc 0.98
2016-09-06T10:14:27.725068: step 5825, loss 0.0486003, acc 0.98
2016-09-06T10:14:28.562725: step 5826, loss 0.00672205, acc 1
2016-09-06T10:14:29.381065: step 5827, loss 0.0215393, acc 0.98
2016-09-06T10:14:30.174905: step 5828, loss 0.0142475, acc 1
2016-09-06T10:14:30.951682: step 5829, loss 0.00924562, acc 1
2016-09-06T10:14:31.758910: step 5830, loss 0.05069, acc 0.96
2016-09-06T10:14:32.575207: step 5831, loss 0.048343, acc 0.96
2016-09-06T10:14:33.363756: step 5832, loss 0.146932, acc 0.96
2016-09-06T10:14:34.199040: step 5833, loss 0.0234528, acc 0.98
2016-09-06T10:14:35.036127: step 5834, loss 0.0135254, acc 1
2016-09-06T10:14:35.828456: step 5835, loss 0.0174298, acc 1
2016-09-06T10:14:36.619078: step 5836, loss 0.0193761, acc 0.98
2016-09-06T10:14:37.427506: step 5837, loss 0.0830829, acc 0.98
2016-09-06T10:14:38.212058: step 5838, loss 0.0190879, acc 0.98
2016-09-06T10:14:39.024400: step 5839, loss 0.0605727, acc 0.98
2016-09-06T10:14:39.843375: step 5840, loss 0.0321782, acc 0.98
2016-09-06T10:14:40.884788: step 5841, loss 0.0891398, acc 0.96
2016-09-06T10:14:41.694592: step 5842, loss 0.0257656, acc 0.98
2016-09-06T10:14:42.547595: step 5843, loss 0.0167555, acc 1
2016-09-06T10:14:43.402799: step 5844, loss 0.00377451, acc 1
2016-09-06T10:14:44.211990: step 5845, loss 0.023746, acc 1
2016-09-06T10:14:45.033043: step 5846, loss 0.0162123, acc 1
2016-09-06T10:14:45.817669: step 5847, loss 0.00329731, acc 1
2016-09-06T10:14:46.623932: step 5848, loss 0.0190566, acc 0.98
2016-09-06T10:14:47.460240: step 5849, loss 0.0388009, acc 0.98
2016-09-06T10:14:48.290495: step 5850, loss 0.0200144, acc 0.98
2016-09-06T10:14:49.107924: step 5851, loss 0.00425608, acc 1
2016-09-06T10:14:49.925089: step 5852, loss 0.00489197, acc 1
2016-09-06T10:14:50.776909: step 5853, loss 0.076513, acc 0.98
2016-09-06T10:14:51.591465: step 5854, loss 0.0348418, acc 0.98
2016-09-06T10:14:52.388304: step 5855, loss 0.00427219, acc 1
2016-09-06T10:14:53.205646: step 5856, loss 0.0122706, acc 1
2016-09-06T10:14:53.976970: step 5857, loss 0.00820431, acc 1
2016-09-06T10:14:54.778920: step 5858, loss 0.0405114, acc 0.98
2016-09-06T10:14:55.602324: step 5859, loss 0.0422195, acc 1
2016-09-06T10:14:56.387472: step 5860, loss 0.0197484, acc 1
2016-09-06T10:14:57.160992: step 5861, loss 0.0546796, acc 0.96
2016-09-06T10:14:57.975131: step 5862, loss 0.00313936, acc 1
2016-09-06T10:14:58.776696: step 5863, loss 0.0528053, acc 0.98
2016-09-06T10:14:59.568351: step 5864, loss 0.00374449, acc 1
2016-09-06T10:15:00.397551: step 5865, loss 0.0548451, acc 0.98
2016-09-06T10:15:01.198222: step 5866, loss 0.0086443, acc 1
2016-09-06T10:15:01.993778: step 5867, loss 0.0606445, acc 0.98
2016-09-06T10:15:02.818120: step 5868, loss 0.0794807, acc 0.98
2016-09-06T10:15:03.582383: step 5869, loss 0.0334044, acc 0.96
2016-09-06T10:15:04.380052: step 5870, loss 0.169113, acc 0.94
2016-09-06T10:15:05.201273: step 5871, loss 0.00991714, acc 1
2016-09-06T10:15:05.993085: step 5872, loss 0.0163535, acc 1
2016-09-06T10:15:06.808012: step 5873, loss 0.0598458, acc 0.98
2016-09-06T10:15:07.628391: step 5874, loss 0.00310125, acc 1
2016-09-06T10:15:08.417098: step 5875, loss 0.0134695, acc 1
2016-09-06T10:15:09.228283: step 5876, loss 0.105739, acc 0.98
2016-09-06T10:15:10.050093: step 5877, loss 0.0287287, acc 1
2016-09-06T10:15:10.845650: step 5878, loss 0.013659, acc 1
2016-09-06T10:15:11.633488: step 5879, loss 0.00901352, acc 1
2016-09-06T10:15:12.445768: step 5880, loss 0.00388508, acc 1
2016-09-06T10:15:13.243884: step 5881, loss 0.00509599, acc 1
2016-09-06T10:15:14.041795: step 5882, loss 0.0252205, acc 0.98
2016-09-06T10:15:14.866666: step 5883, loss 0.0231174, acc 0.98
2016-09-06T10:15:15.646043: step 5884, loss 0.00961399, acc 1
2016-09-06T10:15:16.466281: step 5885, loss 0.0467294, acc 0.96
2016-09-06T10:15:17.289703: step 5886, loss 0.0140782, acc 1
2016-09-06T10:15:18.076304: step 5887, loss 0.0246023, acc 0.98
2016-09-06T10:15:18.883381: step 5888, loss 0.0509372, acc 0.96
2016-09-06T10:15:19.707473: step 5889, loss 0.0037194, acc 1
2016-09-06T10:15:20.515420: step 5890, loss 0.0501477, acc 0.98
2016-09-06T10:15:21.329755: step 5891, loss 0.0107872, acc 1
2016-09-06T10:15:22.153415: step 5892, loss 0.0168935, acc 1
2016-09-06T10:15:22.940760: step 5893, loss 0.0124926, acc 1
2016-09-06T10:15:23.791430: step 5894, loss 0.0510285, acc 0.98
2016-09-06T10:15:24.628612: step 5895, loss 0.0283648, acc 1
2016-09-06T10:15:25.446120: step 5896, loss 0.0265454, acc 0.98
2016-09-06T10:15:26.269665: step 5897, loss 0.00507558, acc 1
2016-09-06T10:15:27.131856: step 5898, loss 0.0491596, acc 0.98
2016-09-06T10:15:27.956374: step 5899, loss 0.0266798, acc 0.98
2016-09-06T10:15:28.781758: step 5900, loss 0.0123404, acc 1

Evaluation:
2016-09-06T10:15:32.536359: step 5900, loss 2.10155, acc 0.767355

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-5900

2016-09-06T10:15:34.424144: step 5901, loss 0.0245238, acc 1
2016-09-06T10:15:35.208152: step 5902, loss 0.00396059, acc 1
2016-09-06T10:15:36.018630: step 5903, loss 0.00367571, acc 1
2016-09-06T10:15:36.848825: step 5904, loss 0.0371665, acc 0.98
2016-09-06T10:15:37.650685: step 5905, loss 0.00539543, acc 1
2016-09-06T10:15:38.457425: step 5906, loss 0.024943, acc 0.98
2016-09-06T10:15:39.280456: step 5907, loss 0.0489891, acc 0.96
2016-09-06T10:15:40.116656: step 5908, loss 0.0120328, acc 1
2016-09-06T10:15:40.930604: step 5909, loss 0.0256217, acc 0.98
2016-09-06T10:15:41.778632: step 5910, loss 0.0293513, acc 0.98
2016-09-06T10:15:42.610117: step 5911, loss 0.00759776, acc 1
2016-09-06T10:15:43.408441: step 5912, loss 0.0358065, acc 0.98
2016-09-06T10:15:44.254076: step 5913, loss 0.0400424, acc 0.98
2016-09-06T10:15:45.050680: step 5914, loss 0.00391351, acc 1
2016-09-06T10:15:45.870048: step 5915, loss 0.0106876, acc 1
2016-09-06T10:15:46.687661: step 5916, loss 0.0322149, acc 1
2016-09-06T10:15:47.498502: step 5917, loss 0.0241538, acc 0.98
2016-09-06T10:15:48.332386: step 5918, loss 0.0172657, acc 1
2016-09-06T10:15:49.145781: step 5919, loss 0.010998, acc 1
2016-09-06T10:15:49.962873: step 5920, loss 0.0436697, acc 0.98
2016-09-06T10:15:50.760446: step 5921, loss 0.0366464, acc 1
2016-09-06T10:15:51.559708: step 5922, loss 0.0776571, acc 0.98
2016-09-06T10:15:52.380738: step 5923, loss 0.00906688, acc 1
2016-09-06T10:15:53.168345: step 5924, loss 0.00604298, acc 1
2016-09-06T10:15:53.954328: step 5925, loss 0.00360754, acc 1
2016-09-06T10:15:54.763836: step 5926, loss 0.0213853, acc 0.98
2016-09-06T10:15:55.584008: step 5927, loss 0.0288038, acc 0.98
2016-09-06T10:15:56.403968: step 5928, loss 0.0436478, acc 0.96
2016-09-06T10:15:57.231076: step 5929, loss 0.0615402, acc 0.98
2016-09-06T10:15:58.033122: step 5930, loss 0.0325516, acc 0.98
2016-09-06T10:15:58.869885: step 5931, loss 0.00890821, acc 1
2016-09-06T10:15:59.687795: step 5932, loss 0.0163068, acc 1
2016-09-06T10:16:00.483016: step 5933, loss 0.022775, acc 0.98
2016-09-06T10:16:01.285074: step 5934, loss 0.0175588, acc 1
2016-09-06T10:16:02.099926: step 5935, loss 0.0281487, acc 1
2016-09-06T10:16:02.890949: step 5936, loss 0.00373871, acc 1
2016-09-06T10:16:03.718479: step 5937, loss 0.0201332, acc 1
2016-09-06T10:16:04.543579: step 5938, loss 0.0618071, acc 0.96
2016-09-06T10:16:05.354477: step 5939, loss 0.0721949, acc 0.96
2016-09-06T10:16:06.168192: step 5940, loss 0.0590946, acc 0.98
2016-09-06T10:16:07.000851: step 5941, loss 0.0186044, acc 1
2016-09-06T10:16:07.812667: step 5942, loss 0.00408773, acc 1
2016-09-06T10:16:08.634361: step 5943, loss 0.0113495, acc 1
2016-09-06T10:16:09.474940: step 5944, loss 0.0195748, acc 0.98
2016-09-06T10:16:10.276472: step 5945, loss 0.02723, acc 0.98
2016-09-06T10:16:11.096658: step 5946, loss 0.0493209, acc 0.96
2016-09-06T10:16:11.917691: step 5947, loss 0.100501, acc 0.96
2016-09-06T10:16:12.732955: step 5948, loss 0.0311745, acc 0.98
2016-09-06T10:16:13.538823: step 5949, loss 0.0296062, acc 1
2016-09-06T10:16:14.372160: step 5950, loss 0.122967, acc 0.98
2016-09-06T10:16:15.185034: step 5951, loss 0.023902, acc 0.98
2016-09-06T10:16:15.938860: step 5952, loss 0.00383639, acc 1
2016-09-06T10:16:16.776406: step 5953, loss 0.0177621, acc 1
2016-09-06T10:16:17.611386: step 5954, loss 0.0989811, acc 0.92
2016-09-06T10:16:18.417233: step 5955, loss 0.0417475, acc 0.96
2016-09-06T10:16:19.247194: step 5956, loss 0.053908, acc 0.98
2016-09-06T10:16:20.090306: step 5957, loss 0.0056322, acc 1
2016-09-06T10:16:20.894589: step 5958, loss 0.00966853, acc 1
2016-09-06T10:16:21.705041: step 5959, loss 0.00745972, acc 1
2016-09-06T10:16:22.505747: step 5960, loss 0.0400491, acc 0.98
2016-09-06T10:16:23.312129: step 5961, loss 0.0625099, acc 0.96
2016-09-06T10:16:24.167391: step 5962, loss 0.0721358, acc 0.96
2016-09-06T10:16:24.975002: step 5963, loss 0.0159006, acc 1
2016-09-06T10:16:25.778800: step 5964, loss 0.0155371, acc 1
2016-09-06T10:16:26.595365: step 5965, loss 0.0392214, acc 0.98
2016-09-06T10:16:27.425775: step 5966, loss 0.032892, acc 0.98
2016-09-06T10:16:28.212751: step 5967, loss 0.0287279, acc 0.98
2016-09-06T10:16:29.010915: step 5968, loss 0.0218837, acc 0.98
2016-09-06T10:16:29.804487: step 5969, loss 0.169011, acc 0.98
2016-09-06T10:16:30.585975: step 5970, loss 0.00238214, acc 1
2016-09-06T10:16:31.425894: step 5971, loss 0.00757234, acc 1
2016-09-06T10:16:32.253409: step 5972, loss 0.00350714, acc 1
2016-09-06T10:16:33.046395: step 5973, loss 0.0394921, acc 0.96
2016-09-06T10:16:33.870521: step 5974, loss 0.0191947, acc 0.98
2016-09-06T10:16:34.680988: step 5975, loss 0.0216402, acc 0.98
2016-09-06T10:16:35.495486: step 5976, loss 0.0459608, acc 0.98
2016-09-06T10:16:36.318658: step 5977, loss 0.00471967, acc 1
2016-09-06T10:16:37.129367: step 5978, loss 0.0248174, acc 0.98
2016-09-06T10:16:37.960238: step 5979, loss 0.0355776, acc 0.96
2016-09-06T10:16:38.758073: step 5980, loss 0.0322926, acc 0.98
2016-09-06T10:16:39.573635: step 5981, loss 0.0315569, acc 0.98
2016-09-06T10:16:40.336603: step 5982, loss 0.0358087, acc 1
2016-09-06T10:16:41.144944: step 5983, loss 0.00372216, acc 1
2016-09-06T10:16:41.977888: step 5984, loss 0.0170775, acc 0.98
2016-09-06T10:16:42.765551: step 5985, loss 0.0243331, acc 1
2016-09-06T10:16:43.561108: step 5986, loss 0.0617559, acc 0.98
2016-09-06T10:16:44.384987: step 5987, loss 0.0226263, acc 0.98
2016-09-06T10:16:45.186606: step 5988, loss 0.0455295, acc 0.98
2016-09-06T10:16:45.985367: step 5989, loss 0.0378466, acc 0.98
2016-09-06T10:16:46.798954: step 5990, loss 0.0112505, acc 1
2016-09-06T10:16:47.591292: step 5991, loss 0.0247839, acc 1
2016-09-06T10:16:48.381274: step 5992, loss 0.031187, acc 0.98
2016-09-06T10:16:49.184784: step 5993, loss 0.0203099, acc 0.98
2016-09-06T10:16:49.997271: step 5994, loss 0.0255322, acc 1
2016-09-06T10:16:50.848042: step 5995, loss 0.0255572, acc 1
2016-09-06T10:16:51.696508: step 5996, loss 0.00341548, acc 1
2016-09-06T10:16:52.525496: step 5997, loss 0.00913502, acc 1
2016-09-06T10:16:53.330981: step 5998, loss 0.0308294, acc 0.98
2016-09-06T10:16:54.117717: step 5999, loss 0.0356601, acc 1
2016-09-06T10:16:54.907226: step 6000, loss 0.0196435, acc 1

Evaluation:
2016-09-06T10:16:58.632149: step 6000, loss 2.02891, acc 0.761726

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-6000

2016-09-06T10:17:00.619102: step 6001, loss 0.0115928, acc 1
2016-09-06T10:17:01.463561: step 6002, loss 0.00764076, acc 1
2016-09-06T10:17:02.294385: step 6003, loss 0.00345478, acc 1
2016-09-06T10:17:03.103335: step 6004, loss 0.0293823, acc 1
2016-09-06T10:17:03.915233: step 6005, loss 0.0217388, acc 0.98
2016-09-06T10:17:04.747757: step 6006, loss 0.00549967, acc 1
2016-09-06T10:17:05.572829: step 6007, loss 0.0183319, acc 1
2016-09-06T10:17:06.425674: step 6008, loss 0.0619425, acc 0.94
2016-09-06T10:17:07.227854: step 6009, loss 0.00431926, acc 1
2016-09-06T10:17:08.030465: step 6010, loss 0.0681686, acc 0.96
2016-09-06T10:17:08.853907: step 6011, loss 0.00430868, acc 1
2016-09-06T10:17:09.672068: step 6012, loss 0.0368961, acc 0.98
2016-09-06T10:17:10.474665: step 6013, loss 0.0474155, acc 0.98
2016-09-06T10:17:11.293584: step 6014, loss 0.0181481, acc 1
2016-09-06T10:17:12.085908: step 6015, loss 0.00335691, acc 1
2016-09-06T10:17:12.888551: step 6016, loss 0.0183397, acc 0.98
2016-09-06T10:17:13.713722: step 6017, loss 0.00302385, acc 1
2016-09-06T10:17:14.537403: step 6018, loss 0.056015, acc 0.98
2016-09-06T10:17:15.358380: step 6019, loss 0.0141872, acc 1
2016-09-06T10:17:16.206817: step 6020, loss 0.0433901, acc 0.98
2016-09-06T10:17:16.997111: step 6021, loss 0.00984026, acc 1
2016-09-06T10:17:17.809842: step 6022, loss 0.0254081, acc 0.98
2016-09-06T10:17:18.643442: step 6023, loss 0.0646227, acc 0.96
2016-09-06T10:17:19.473420: step 6024, loss 0.0408773, acc 0.96
2016-09-06T10:17:20.267455: step 6025, loss 0.0802096, acc 0.96
2016-09-06T10:17:21.118525: step 6026, loss 0.0211102, acc 1
2016-09-06T10:17:21.938140: step 6027, loss 0.0274006, acc 1
2016-09-06T10:17:22.736346: step 6028, loss 0.0238347, acc 1
2016-09-06T10:17:23.545338: step 6029, loss 0.0191118, acc 0.98
2016-09-06T10:17:24.367730: step 6030, loss 0.0337245, acc 0.98
2016-09-06T10:17:25.151365: step 6031, loss 0.0034614, acc 1
2016-09-06T10:17:25.955809: step 6032, loss 0.00308365, acc 1
2016-09-06T10:17:26.767089: step 6033, loss 0.0139722, acc 1
2016-09-06T10:17:27.559266: step 6034, loss 0.0196901, acc 1
2016-09-06T10:17:28.350523: step 6035, loss 0.0218352, acc 1
2016-09-06T10:17:29.164970: step 6036, loss 0.0113526, acc 1
2016-09-06T10:17:29.965404: step 6037, loss 0.053474, acc 0.98
2016-09-06T10:17:30.771535: step 6038, loss 0.0482002, acc 0.96
2016-09-06T10:17:31.614484: step 6039, loss 0.00468946, acc 1
2016-09-06T10:17:32.406911: step 6040, loss 0.0353943, acc 0.98
2016-09-06T10:17:33.221037: step 6041, loss 0.00469689, acc 1
2016-09-06T10:17:34.032716: step 6042, loss 0.0384805, acc 0.98
2016-09-06T10:17:34.837568: step 6043, loss 0.0727434, acc 0.98
2016-09-06T10:17:35.640890: step 6044, loss 0.02532, acc 0.98
2016-09-06T10:17:36.521422: step 6045, loss 0.0100866, acc 1
2016-09-06T10:17:37.349498: step 6046, loss 0.0304293, acc 0.98
2016-09-06T10:17:38.181232: step 6047, loss 0.054428, acc 0.98
2016-09-06T10:17:39.003494: step 6048, loss 0.0362498, acc 0.98
2016-09-06T10:17:39.823376: step 6049, loss 0.0958103, acc 0.96
2016-09-06T10:17:40.659520: step 6050, loss 0.0296695, acc 0.98
2016-09-06T10:17:41.477698: step 6051, loss 0.10077, acc 0.96
2016-09-06T10:17:42.284759: step 6052, loss 0.0352535, acc 0.98
2016-09-06T10:17:43.096008: step 6053, loss 0.0100188, acc 1
2016-09-06T10:17:43.935605: step 6054, loss 0.00854994, acc 1
2016-09-06T10:17:44.748792: step 6055, loss 0.0135219, acc 1
2016-09-06T10:17:45.549962: step 6056, loss 0.0103105, acc 1
2016-09-06T10:17:46.380642: step 6057, loss 0.0204193, acc 1
2016-09-06T10:17:47.200769: step 6058, loss 0.0374134, acc 0.98
2016-09-06T10:17:48.022764: step 6059, loss 0.0454715, acc 0.98
2016-09-06T10:17:48.852196: step 6060, loss 0.0555874, acc 0.96
2016-09-06T10:17:49.667780: step 6061, loss 0.0232809, acc 0.98
2016-09-06T10:17:50.454580: step 6062, loss 0.0333795, acc 0.96
2016-09-06T10:17:51.273706: step 6063, loss 0.0291437, acc 0.98
2016-09-06T10:17:52.084726: step 6064, loss 0.0669783, acc 0.98
2016-09-06T10:17:52.863100: step 6065, loss 0.0491427, acc 0.96
2016-09-06T10:17:53.675507: step 6066, loss 0.0874841, acc 0.94
2016-09-06T10:17:54.537903: step 6067, loss 0.0321795, acc 0.98
2016-09-06T10:17:55.305440: step 6068, loss 0.0237343, acc 1
2016-09-06T10:17:56.132524: step 6069, loss 0.0227207, acc 1
2016-09-06T10:17:56.945284: step 6070, loss 0.0320756, acc 0.98
2016-09-06T10:17:57.728918: step 6071, loss 0.0321674, acc 1
2016-09-06T10:17:58.536213: step 6072, loss 0.0210668, acc 1
2016-09-06T10:17:59.345538: step 6073, loss 0.0333419, acc 0.98
2016-09-06T10:18:00.134572: step 6074, loss 0.00555474, acc 1
2016-09-06T10:18:00.973228: step 6075, loss 0.0059654, acc 1
2016-09-06T10:18:01.776038: step 6076, loss 0.0194366, acc 1
2016-09-06T10:18:02.573866: step 6077, loss 0.0508442, acc 0.98
2016-09-06T10:18:03.391419: step 6078, loss 0.0416973, acc 0.98
2016-09-06T10:18:04.217229: step 6079, loss 0.0326493, acc 0.96
2016-09-06T10:18:05.007443: step 6080, loss 0.0185081, acc 1
2016-09-06T10:18:05.829890: step 6081, loss 0.0386993, acc 0.98
2016-09-06T10:18:06.675090: step 6082, loss 0.0267686, acc 0.98
2016-09-06T10:18:07.471378: step 6083, loss 0.00444299, acc 1
2016-09-06T10:18:08.278083: step 6084, loss 0.0339739, acc 0.98
2016-09-06T10:18:09.116009: step 6085, loss 0.0407257, acc 0.98
2016-09-06T10:18:09.926812: step 6086, loss 0.111777, acc 0.98
2016-09-06T10:18:10.740282: step 6087, loss 0.024918, acc 0.98
2016-09-06T10:18:11.571048: step 6088, loss 0.0107535, acc 1
2016-09-06T10:18:12.397566: step 6089, loss 0.0322999, acc 1
2016-09-06T10:18:13.188043: step 6090, loss 0.0240755, acc 1
2016-09-06T10:18:14.039729: step 6091, loss 0.00621485, acc 1
2016-09-06T10:18:14.828796: step 6092, loss 0.00351355, acc 1
2016-09-06T10:18:15.654516: step 6093, loss 0.0607575, acc 0.96
2016-09-06T10:18:16.498301: step 6094, loss 0.0110623, acc 1
2016-09-06T10:18:17.304546: step 6095, loss 0.00433996, acc 1
2016-09-06T10:18:18.094826: step 6096, loss 0.0433217, acc 0.98
2016-09-06T10:18:18.931341: step 6097, loss 0.0192928, acc 1
2016-09-06T10:18:19.749317: step 6098, loss 0.00576174, acc 1
2016-09-06T10:18:20.554162: step 6099, loss 0.0793965, acc 0.94
2016-09-06T10:18:21.378344: step 6100, loss 0.0506764, acc 0.96

Evaluation:
2016-09-06T10:18:25.150142: step 6100, loss 2.31843, acc 0.746717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-6100

2016-09-06T10:18:26.989066: step 6101, loss 0.016615, acc 1
2016-09-06T10:18:27.855368: step 6102, loss 0.0237455, acc 1
2016-09-06T10:18:28.704321: step 6103, loss 0.0952788, acc 0.96
2016-09-06T10:18:29.513518: step 6104, loss 0.0373026, acc 0.98
2016-09-06T10:18:30.324156: step 6105, loss 0.0513199, acc 0.96
2016-09-06T10:18:31.176017: step 6106, loss 0.0410853, acc 0.98
2016-09-06T10:18:31.983640: step 6107, loss 0.0174347, acc 1
2016-09-06T10:18:32.775591: step 6108, loss 0.0549762, acc 0.96
2016-09-06T10:18:33.590697: step 6109, loss 0.00377836, acc 1
2016-09-06T10:18:34.395553: step 6110, loss 0.0184334, acc 1
2016-09-06T10:18:35.204865: step 6111, loss 0.12162, acc 0.96
2016-09-06T10:18:36.014823: step 6112, loss 0.0351688, acc 1
2016-09-06T10:18:36.845214: step 6113, loss 0.0431051, acc 0.98
2016-09-06T10:18:37.621438: step 6114, loss 0.0248997, acc 1
2016-09-06T10:18:38.454692: step 6115, loss 0.00941937, acc 1
2016-09-06T10:18:39.245217: step 6116, loss 0.0343435, acc 0.98
2016-09-06T10:18:40.026589: step 6117, loss 0.00707335, acc 1
2016-09-06T10:18:40.856034: step 6118, loss 0.0266606, acc 0.98
2016-09-06T10:18:41.690386: step 6119, loss 0.0212032, acc 1
2016-09-06T10:18:42.465142: step 6120, loss 0.123043, acc 0.98
2016-09-06T10:18:43.284084: step 6121, loss 0.0551867, acc 0.98
2016-09-06T10:18:44.084591: step 6122, loss 0.0154259, acc 1
2016-09-06T10:18:44.848527: step 6123, loss 0.018806, acc 1
2016-09-06T10:18:45.687784: step 6124, loss 0.0168667, acc 1
2016-09-06T10:18:46.485860: step 6125, loss 0.0190236, acc 0.98
2016-09-06T10:18:47.291302: step 6126, loss 0.0182131, acc 0.98
2016-09-06T10:18:48.119088: step 6127, loss 0.0984909, acc 0.94
2016-09-06T10:18:48.953939: step 6128, loss 0.00485631, acc 1
2016-09-06T10:18:49.784601: step 6129, loss 0.0670898, acc 0.96
2016-09-06T10:18:50.588458: step 6130, loss 0.0622148, acc 0.98
2016-09-06T10:18:51.411047: step 6131, loss 0.0361143, acc 0.98
2016-09-06T10:18:52.196682: step 6132, loss 0.048757, acc 0.98
2016-09-06T10:18:52.973166: step 6133, loss 0.00669517, acc 1
2016-09-06T10:18:53.792586: step 6134, loss 0.0248185, acc 1
2016-09-06T10:18:54.592738: step 6135, loss 0.0250633, acc 0.98
2016-09-06T10:18:55.377989: step 6136, loss 0.00698976, acc 1
2016-09-06T10:18:56.207623: step 6137, loss 0.0073834, acc 1
2016-09-06T10:18:56.985020: step 6138, loss 0.00982974, acc 1
2016-09-06T10:18:57.783058: step 6139, loss 0.0594504, acc 0.98
2016-09-06T10:18:58.596292: step 6140, loss 0.0090016, acc 1
2016-09-06T10:18:59.392726: step 6141, loss 0.0205255, acc 1
2016-09-06T10:19:00.193427: step 6142, loss 0.0267794, acc 1
2016-09-06T10:19:01.040481: step 6143, loss 0.0336528, acc 1
2016-09-06T10:19:01.778053: step 6144, loss 0.0265625, acc 0.977273
2016-09-06T10:19:02.604532: step 6145, loss 0.0118406, acc 1
2016-09-06T10:19:03.409921: step 6146, loss 0.0276693, acc 0.98
2016-09-06T10:19:04.222974: step 6147, loss 0.0332515, acc 1
2016-09-06T10:19:05.046574: step 6148, loss 0.0863102, acc 0.94
2016-09-06T10:19:05.866314: step 6149, loss 0.0344513, acc 1
2016-09-06T10:19:06.663533: step 6150, loss 0.0402299, acc 0.98
2016-09-06T10:19:07.453409: step 6151, loss 0.0102342, acc 1
2016-09-06T10:19:08.257655: step 6152, loss 0.0047448, acc 1
2016-09-06T10:19:09.033790: step 6153, loss 0.04993, acc 0.98
2016-09-06T10:19:09.841078: step 6154, loss 0.0472673, acc 0.96
2016-09-06T10:19:10.646051: step 6155, loss 0.0114895, acc 1
2016-09-06T10:19:11.467744: step 6156, loss 0.00630199, acc 1
2016-09-06T10:19:12.287238: step 6157, loss 0.0642501, acc 0.96
2016-09-06T10:19:13.092723: step 6158, loss 0.0237259, acc 0.98
2016-09-06T10:19:13.863811: step 6159, loss 0.0184249, acc 0.98
2016-09-06T10:19:14.681615: step 6160, loss 0.0269195, acc 1
2016-09-06T10:19:15.518239: step 6161, loss 0.0459931, acc 0.96
2016-09-06T10:19:16.306567: step 6162, loss 0.0147396, acc 1
2016-09-06T10:19:17.114031: step 6163, loss 0.0162271, acc 1
2016-09-06T10:19:17.928925: step 6164, loss 0.00447522, acc 1
2016-09-06T10:19:18.733486: step 6165, loss 0.00425514, acc 1
2016-09-06T10:19:19.541407: step 6166, loss 0.00904387, acc 1
2016-09-06T10:19:20.391620: step 6167, loss 0.0201619, acc 0.98
2016-09-06T10:19:21.185656: step 6168, loss 0.0555446, acc 0.98
2016-09-06T10:19:21.990173: step 6169, loss 0.00425345, acc 1
2016-09-06T10:19:22.817713: step 6170, loss 0.012347, acc 1
2016-09-06T10:19:23.594062: step 6171, loss 0.00746412, acc 1
2016-09-06T10:19:24.392816: step 6172, loss 0.0854439, acc 0.98
2016-09-06T10:19:25.229564: step 6173, loss 0.0101833, acc 1
2016-09-06T10:19:26.018473: step 6174, loss 0.0211505, acc 1
2016-09-06T10:19:26.795913: step 6175, loss 0.00405792, acc 1
2016-09-06T10:19:27.596876: step 6176, loss 0.0502201, acc 0.98
2016-09-06T10:19:28.377968: step 6177, loss 0.0277587, acc 0.98
2016-09-06T10:19:29.183239: step 6178, loss 0.00506866, acc 1
2016-09-06T10:19:30.022054: step 6179, loss 0.00795163, acc 1
2016-09-06T10:19:30.825841: step 6180, loss 0.00385985, acc 1
2016-09-06T10:19:31.633337: step 6181, loss 0.0116092, acc 1
2016-09-06T10:19:32.463828: step 6182, loss 0.0138594, acc 1
2016-09-06T10:19:33.276382: step 6183, loss 0.00790508, acc 1
2016-09-06T10:19:34.062454: step 6184, loss 0.00422779, acc 1
2016-09-06T10:19:34.887852: step 6185, loss 0.0129357, acc 1
2016-09-06T10:19:35.662942: step 6186, loss 0.0135513, acc 1
2016-09-06T10:19:36.503566: step 6187, loss 0.0613181, acc 0.98
2016-09-06T10:19:37.314989: step 6188, loss 0.00408861, acc 1
2016-09-06T10:19:38.116612: step 6189, loss 0.0043533, acc 1
2016-09-06T10:19:38.928081: step 6190, loss 0.0259908, acc 1
2016-09-06T10:19:39.764894: step 6191, loss 0.0746648, acc 0.96
2016-09-06T10:19:40.566885: step 6192, loss 0.0172761, acc 1
2016-09-06T10:19:41.369710: step 6193, loss 0.0408303, acc 0.96
2016-09-06T10:19:42.170785: step 6194, loss 0.0149096, acc 1
2016-09-06T10:19:42.988266: step 6195, loss 0.0247699, acc 0.98
2016-09-06T10:19:43.809040: step 6196, loss 0.0160569, acc 1
2016-09-06T10:19:44.668185: step 6197, loss 0.0275851, acc 0.98
2016-09-06T10:19:45.475634: step 6198, loss 0.0222482, acc 0.98
2016-09-06T10:19:46.286974: step 6199, loss 0.0310039, acc 1
2016-09-06T10:19:47.102103: step 6200, loss 0.0089841, acc 1

Evaluation:
2016-09-06T10:19:50.870982: step 6200, loss 2.10929, acc 0.74015

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-6200

2016-09-06T10:19:52.742557: step 6201, loss 0.00654608, acc 1
2016-09-06T10:19:53.531435: step 6202, loss 0.0263959, acc 0.98
2016-09-06T10:19:54.363739: step 6203, loss 0.0200016, acc 1
2016-09-06T10:19:55.152627: step 6204, loss 0.024837, acc 0.98
2016-09-06T10:19:55.940349: step 6205, loss 0.0479616, acc 0.98
2016-09-06T10:19:56.785390: step 6206, loss 0.0125111, acc 1
2016-09-06T10:19:57.553486: step 6207, loss 0.00363071, acc 1
2016-09-06T10:19:58.357842: step 6208, loss 0.0414573, acc 0.98
2016-09-06T10:19:59.215805: step 6209, loss 0.0194662, acc 0.98
2016-09-06T10:20:00.015909: step 6210, loss 0.0419098, acc 0.98
2016-09-06T10:20:00.841456: step 6211, loss 0.00435504, acc 1
2016-09-06T10:20:01.656390: step 6212, loss 0.047793, acc 0.96
2016-09-06T10:20:02.490483: step 6213, loss 0.0627378, acc 0.98
2016-09-06T10:20:03.287144: step 6214, loss 0.0145989, acc 1
2016-09-06T10:20:04.141733: step 6215, loss 0.00363209, acc 1
2016-09-06T10:20:04.943518: step 6216, loss 0.00511744, acc 1
2016-09-06T10:20:05.781665: step 6217, loss 0.0242377, acc 0.98
2016-09-06T10:20:06.592505: step 6218, loss 0.0438426, acc 0.96
2016-09-06T10:20:07.411865: step 6219, loss 0.00499162, acc 1
2016-09-06T10:20:08.257693: step 6220, loss 0.00517362, acc 1
2016-09-06T10:20:09.086934: step 6221, loss 0.00492516, acc 1
2016-09-06T10:20:09.970875: step 6222, loss 0.034744, acc 0.96
2016-09-06T10:20:10.762467: step 6223, loss 0.0588864, acc 0.96
2016-09-06T10:20:11.545443: step 6224, loss 0.00401444, acc 1
2016-09-06T10:20:12.373743: step 6225, loss 0.0050527, acc 1
2016-09-06T10:20:13.159831: step 6226, loss 0.00306907, acc 1
2016-09-06T10:20:13.967911: step 6227, loss 0.0501833, acc 0.98
2016-09-06T10:20:14.796166: step 6228, loss 0.00966023, acc 1
2016-09-06T10:20:15.572868: step 6229, loss 0.0462863, acc 0.98
2016-09-06T10:20:16.398958: step 6230, loss 0.0113143, acc 1
2016-09-06T10:20:17.220838: step 6231, loss 0.00392431, acc 1
2016-09-06T10:20:18.000758: step 6232, loss 0.00959165, acc 1
2016-09-06T10:20:18.804857: step 6233, loss 0.0316788, acc 1
2016-09-06T10:20:19.621714: step 6234, loss 0.0228051, acc 0.98
2016-09-06T10:20:20.395284: step 6235, loss 0.0361509, acc 0.98
2016-09-06T10:20:21.187725: step 6236, loss 0.0117159, acc 1
2016-09-06T10:20:22.014451: step 6237, loss 0.0223792, acc 0.98
2016-09-06T10:20:22.798486: step 6238, loss 0.0095108, acc 1
2016-09-06T10:20:23.605087: step 6239, loss 0.00370038, acc 1
2016-09-06T10:20:24.422160: step 6240, loss 0.00671877, acc 1
2016-09-06T10:20:25.223959: step 6241, loss 0.00434622, acc 1
2016-09-06T10:20:26.031119: step 6242, loss 0.0325492, acc 0.98
2016-09-06T10:20:26.839302: step 6243, loss 0.0331991, acc 0.98
2016-09-06T10:20:27.631955: step 6244, loss 0.0405627, acc 0.98
2016-09-06T10:20:28.412272: step 6245, loss 0.0334342, acc 0.98
2016-09-06T10:20:29.234872: step 6246, loss 0.0358315, acc 0.98
2016-09-06T10:20:30.044275: step 6247, loss 0.00915614, acc 1
2016-09-06T10:20:30.833274: step 6248, loss 0.0039537, acc 1
2016-09-06T10:20:31.648410: step 6249, loss 0.041752, acc 0.96
2016-09-06T10:20:32.423602: step 6250, loss 0.0307072, acc 0.98
2016-09-06T10:20:33.231724: step 6251, loss 0.0520359, acc 0.98
2016-09-06T10:20:34.041417: step 6252, loss 0.01858, acc 0.98
2016-09-06T10:20:34.834318: step 6253, loss 0.0382162, acc 0.98
2016-09-06T10:20:35.651173: step 6254, loss 0.025467, acc 1
2016-09-06T10:20:36.468708: step 6255, loss 0.0107431, acc 1
2016-09-06T10:20:37.275639: step 6256, loss 0.0213618, acc 1
2016-09-06T10:20:38.060079: step 6257, loss 0.034459, acc 0.98
2016-09-06T10:20:38.870626: step 6258, loss 0.0184178, acc 0.98
2016-09-06T10:20:39.673495: step 6259, loss 0.0211414, acc 0.98
2016-09-06T10:20:40.473924: step 6260, loss 0.151914, acc 0.96
2016-09-06T10:20:41.279707: step 6261, loss 0.0176931, acc 0.98
2016-09-06T10:20:42.084973: step 6262, loss 0.00248846, acc 1
2016-09-06T10:20:42.911635: step 6263, loss 0.0816617, acc 0.98
2016-09-06T10:20:43.733234: step 6264, loss 0.0156373, acc 1
2016-09-06T10:20:44.535891: step 6265, loss 0.00461944, acc 1
2016-09-06T10:20:45.358172: step 6266, loss 0.00286758, acc 1
2016-09-06T10:20:46.172219: step 6267, loss 0.0171969, acc 0.98
2016-09-06T10:20:46.973037: step 6268, loss 0.00687256, acc 1
2016-09-06T10:20:47.788833: step 6269, loss 0.0395444, acc 0.98
2016-09-06T10:20:48.609271: step 6270, loss 0.00565317, acc 1
2016-09-06T10:20:49.401780: step 6271, loss 0.0864192, acc 0.96
2016-09-06T10:20:50.206403: step 6272, loss 0.0343837, acc 0.96
2016-09-06T10:20:50.987013: step 6273, loss 0.0605039, acc 0.96
2016-09-06T10:20:51.784417: step 6274, loss 0.0194486, acc 0.98
2016-09-06T10:20:52.622193: step 6275, loss 0.00850514, acc 1
2016-09-06T10:20:53.444254: step 6276, loss 0.00374322, acc 1
2016-09-06T10:20:54.243131: step 6277, loss 0.019503, acc 1
2016-09-06T10:20:55.045467: step 6278, loss 0.0159447, acc 1
2016-09-06T10:20:55.873463: step 6279, loss 0.0474844, acc 0.98
2016-09-06T10:20:56.679883: step 6280, loss 0.00413882, acc 1
2016-09-06T10:20:57.480200: step 6281, loss 0.0221547, acc 1
2016-09-06T10:20:58.298485: step 6282, loss 0.00632193, acc 1
2016-09-06T10:20:59.059528: step 6283, loss 0.0139305, acc 1
2016-09-06T10:20:59.837452: step 6284, loss 0.0100074, acc 1
2016-09-06T10:21:00.651715: step 6285, loss 0.0141903, acc 1
2016-09-06T10:21:01.469807: step 6286, loss 0.0120811, acc 1
2016-09-06T10:21:02.312124: step 6287, loss 0.00677981, acc 1
2016-09-06T10:21:03.094862: step 6288, loss 0.0341541, acc 0.98
2016-09-06T10:21:03.892188: step 6289, loss 0.00390051, acc 1
2016-09-06T10:21:04.727447: step 6290, loss 0.0148101, acc 1
2016-09-06T10:21:05.542614: step 6291, loss 0.00526586, acc 1
2016-09-06T10:21:06.332075: step 6292, loss 0.0175648, acc 0.98
2016-09-06T10:21:07.149340: step 6293, loss 0.0107925, acc 1
2016-09-06T10:21:07.949697: step 6294, loss 0.0924488, acc 0.98
2016-09-06T10:21:08.770751: step 6295, loss 0.0425533, acc 0.98
2016-09-06T10:21:09.602529: step 6296, loss 0.0306574, acc 0.98
2016-09-06T10:21:10.433834: step 6297, loss 0.0431249, acc 0.98
2016-09-06T10:21:11.223273: step 6298, loss 0.00589688, acc 1
2016-09-06T10:21:12.039355: step 6299, loss 0.00349343, acc 1
2016-09-06T10:21:12.882429: step 6300, loss 0.018801, acc 0.98

Evaluation:
2016-09-06T10:21:16.591456: step 6300, loss 1.87105, acc 0.750469

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-6300

2016-09-06T10:21:18.463672: step 6301, loss 0.0189297, acc 0.98
2016-09-06T10:21:19.288780: step 6302, loss 0.0168016, acc 1
2016-09-06T10:21:20.119285: step 6303, loss 0.0206517, acc 0.98
2016-09-06T10:21:20.936633: step 6304, loss 0.0791055, acc 0.96
2016-09-06T10:21:21.782384: step 6305, loss 0.0619507, acc 0.94
2016-09-06T10:21:22.664957: step 6306, loss 0.0446597, acc 0.98
2016-09-06T10:21:23.475847: step 6307, loss 0.0154334, acc 1
2016-09-06T10:21:24.261848: step 6308, loss 0.0033382, acc 1
2016-09-06T10:21:25.092356: step 6309, loss 0.00239749, acc 1
2016-09-06T10:21:25.859512: step 6310, loss 0.0188247, acc 0.98
2016-09-06T10:21:26.674009: step 6311, loss 0.0613234, acc 0.98
2016-09-06T10:21:27.506535: step 6312, loss 0.0221086, acc 0.98
2016-09-06T10:21:28.272657: step 6313, loss 0.127835, acc 0.94
2016-09-06T10:21:29.062529: step 6314, loss 0.0436411, acc 0.96
2016-09-06T10:21:29.890732: step 6315, loss 0.0316054, acc 1
2016-09-06T10:21:30.713409: step 6316, loss 0.00227527, acc 1
2016-09-06T10:21:31.515734: step 6317, loss 0.0160979, acc 1
2016-09-06T10:21:32.356581: step 6318, loss 0.00231632, acc 1
2016-09-06T10:21:33.182604: step 6319, loss 0.0401889, acc 0.98
2016-09-06T10:21:33.981789: step 6320, loss 0.0223255, acc 0.98
2016-09-06T10:21:34.806003: step 6321, loss 0.00561007, acc 1
2016-09-06T10:21:35.601352: step 6322, loss 0.0759796, acc 0.96
2016-09-06T10:21:36.405700: step 6323, loss 0.0185077, acc 0.98
2016-09-06T10:21:37.223732: step 6324, loss 0.0419935, acc 0.96
2016-09-06T10:21:38.030581: step 6325, loss 0.010976, acc 1
2016-09-06T10:21:38.846119: step 6326, loss 0.0642206, acc 0.94
2016-09-06T10:21:39.714778: step 6327, loss 0.00601279, acc 1
2016-09-06T10:21:40.524428: step 6328, loss 0.0166424, acc 1
2016-09-06T10:21:41.385189: step 6329, loss 0.0275134, acc 0.98
2016-09-06T10:21:42.233425: step 6330, loss 0.0177581, acc 1
2016-09-06T10:21:43.067549: step 6331, loss 0.0528138, acc 0.94
2016-09-06T10:21:43.872236: step 6332, loss 0.0227036, acc 1
2016-09-06T10:21:44.684439: step 6333, loss 0.0562225, acc 0.94
2016-09-06T10:21:45.493134: step 6334, loss 0.0148786, acc 1
2016-09-06T10:21:46.295138: step 6335, loss 0.0326502, acc 1
2016-09-06T10:21:47.066151: step 6336, loss 0.0115785, acc 1
2016-09-06T10:21:47.887588: step 6337, loss 0.00522966, acc 1
2016-09-06T10:21:48.708100: step 6338, loss 0.0258554, acc 0.98
2016-09-06T10:21:49.538938: step 6339, loss 0.0150221, acc 1
2016-09-06T10:21:50.353311: step 6340, loss 0.0267359, acc 0.98
2016-09-06T10:21:51.191280: step 6341, loss 0.0255844, acc 0.98
2016-09-06T10:21:52.029460: step 6342, loss 0.0427635, acc 0.96
2016-09-06T10:21:52.855949: step 6343, loss 0.0391198, acc 0.98
2016-09-06T10:21:53.648928: step 6344, loss 0.0243797, acc 0.98
2016-09-06T10:21:54.459525: step 6345, loss 0.0399806, acc 1
2016-09-06T10:21:55.276164: step 6346, loss 0.0222809, acc 0.98
2016-09-06T10:21:56.067669: step 6347, loss 0.00251987, acc 1
2016-09-06T10:21:56.873125: step 6348, loss 0.0241252, acc 0.98
2016-09-06T10:21:57.662394: step 6349, loss 0.0615453, acc 0.98
2016-09-06T10:21:58.490182: step 6350, loss 0.00577971, acc 1
2016-09-06T10:21:59.321062: step 6351, loss 0.0555558, acc 0.96
2016-09-06T10:22:00.146808: step 6352, loss 0.0171022, acc 1
2016-09-06T10:22:00.972594: step 6353, loss 0.0083348, acc 1
2016-09-06T10:22:01.808506: step 6354, loss 0.00427565, acc 1
2016-09-06T10:22:02.647045: step 6355, loss 0.00499042, acc 1
2016-09-06T10:22:03.458287: step 6356, loss 0.0200017, acc 0.98
2016-09-06T10:22:04.245525: step 6357, loss 0.0197173, acc 1
2016-09-06T10:22:05.094277: step 6358, loss 0.0668295, acc 0.96
2016-09-06T10:22:05.896886: step 6359, loss 0.0359598, acc 0.98
2016-09-06T10:22:06.721408: step 6360, loss 0.0303128, acc 0.98
2016-09-06T10:22:07.526872: step 6361, loss 0.00723938, acc 1
2016-09-06T10:22:08.328661: step 6362, loss 0.1016, acc 0.98
2016-09-06T10:22:09.132727: step 6363, loss 0.00475907, acc 1
2016-09-06T10:22:09.947174: step 6364, loss 0.00452983, acc 1
2016-09-06T10:22:10.767925: step 6365, loss 0.0300833, acc 0.98
2016-09-06T10:22:11.578287: step 6366, loss 0.0302081, acc 1
2016-09-06T10:22:12.399900: step 6367, loss 0.00341622, acc 1
2016-09-06T10:22:13.218220: step 6368, loss 0.019448, acc 1
2016-09-06T10:22:14.024651: step 6369, loss 0.047744, acc 0.96
2016-09-06T10:22:14.875535: step 6370, loss 0.0492692, acc 0.96
2016-09-06T10:22:15.680283: step 6371, loss 0.0423697, acc 0.98
2016-09-06T10:22:16.490915: step 6372, loss 0.00891232, acc 1
2016-09-06T10:22:17.313260: step 6373, loss 0.0316607, acc 0.98
2016-09-06T10:22:18.142442: step 6374, loss 0.0126723, acc 1
2016-09-06T10:22:18.954112: step 6375, loss 0.0139544, acc 1
2016-09-06T10:22:19.806247: step 6376, loss 0.00510715, acc 1
2016-09-06T10:22:20.618600: step 6377, loss 0.00715752, acc 1
2016-09-06T10:22:21.448954: step 6378, loss 0.0799681, acc 0.98
2016-09-06T10:22:22.244729: step 6379, loss 0.0169524, acc 1
2016-09-06T10:22:23.055847: step 6380, loss 0.0433447, acc 0.98
2016-09-06T10:22:23.848369: step 6381, loss 0.00523445, acc 1
2016-09-06T10:22:24.667198: step 6382, loss 0.0218203, acc 0.98
2016-09-06T10:22:25.489349: step 6383, loss 0.0205653, acc 0.98
2016-09-06T10:22:26.297239: step 6384, loss 0.0184325, acc 1
2016-09-06T10:22:27.094838: step 6385, loss 0.0114346, acc 1
2016-09-06T10:22:27.907524: step 6386, loss 0.00595461, acc 1
2016-09-06T10:22:28.677522: step 6387, loss 0.0330059, acc 0.98
2016-09-06T10:22:29.503791: step 6388, loss 0.0389275, acc 0.98
2016-09-06T10:22:30.321444: step 6389, loss 0.037083, acc 0.98
2016-09-06T10:22:31.115703: step 6390, loss 0.019407, acc 1
2016-09-06T10:22:31.940138: step 6391, loss 0.00297573, acc 1
2016-09-06T10:22:32.757780: step 6392, loss 0.0329393, acc 0.98
2016-09-06T10:22:33.575429: step 6393, loss 0.0305228, acc 0.98
2016-09-06T10:22:34.398143: step 6394, loss 0.0313862, acc 0.98
2016-09-06T10:22:35.212275: step 6395, loss 0.063671, acc 0.96
2016-09-06T10:22:36.014980: step 6396, loss 0.0276057, acc 0.98
2016-09-06T10:22:36.826104: step 6397, loss 0.0455375, acc 0.96
2016-09-06T10:22:37.662775: step 6398, loss 0.0157889, acc 1
2016-09-06T10:22:38.446432: step 6399, loss 0.00325284, acc 1
2016-09-06T10:22:39.231780: step 6400, loss 0.0597279, acc 0.98

Evaluation:
2016-09-06T10:22:42.951955: step 6400, loss 2.02216, acc 0.739212

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-6400

2016-09-06T10:22:44.934465: step 6401, loss 0.0224778, acc 0.98
2016-09-06T10:22:45.715288: step 6402, loss 0.0173213, acc 1
2016-09-06T10:22:46.535461: step 6403, loss 0.0127889, acc 1
2016-09-06T10:22:47.359968: step 6404, loss 0.0838474, acc 0.94
2016-09-06T10:22:48.142402: step 6405, loss 0.0243259, acc 0.98
2016-09-06T10:22:48.949276: step 6406, loss 0.0721216, acc 0.94
2016-09-06T10:22:49.782254: step 6407, loss 0.0540611, acc 0.96
2016-09-06T10:22:50.597536: step 6408, loss 0.017758, acc 0.98
2016-09-06T10:22:51.406446: step 6409, loss 0.0185245, acc 0.98
2016-09-06T10:22:52.227701: step 6410, loss 0.0133119, acc 1
2016-09-06T10:22:53.057405: step 6411, loss 0.0162901, acc 1
2016-09-06T10:22:53.844157: step 6412, loss 0.0203642, acc 0.98
2016-09-06T10:22:54.696987: step 6413, loss 0.00353852, acc 1
2016-09-06T10:22:55.507290: step 6414, loss 0.0284303, acc 0.98
2016-09-06T10:22:56.321791: step 6415, loss 0.00556468, acc 1
2016-09-06T10:22:57.195134: step 6416, loss 0.109195, acc 0.98
2016-09-06T10:22:58.007383: step 6417, loss 0.00348976, acc 1
2016-09-06T10:22:58.813381: step 6418, loss 0.0124333, acc 1
2016-09-06T10:22:59.640539: step 6419, loss 0.038716, acc 1
2016-09-06T10:23:00.496141: step 6420, loss 0.0624207, acc 0.98
2016-09-06T10:23:01.302400: step 6421, loss 0.0203022, acc 1
2016-09-06T10:23:02.145699: step 6422, loss 0.00877694, acc 1
2016-09-06T10:23:02.981148: step 6423, loss 0.00356058, acc 1
2016-09-06T10:23:03.761570: step 6424, loss 0.0688913, acc 0.94
2016-09-06T10:23:04.611991: step 6425, loss 0.0476358, acc 0.96
2016-09-06T10:23:05.413544: step 6426, loss 0.0219106, acc 0.98
2016-09-06T10:23:06.222942: step 6427, loss 0.0339289, acc 0.98
2016-09-06T10:23:07.024838: step 6428, loss 0.00272131, acc 1
2016-09-06T10:23:07.854803: step 6429, loss 0.0303846, acc 0.98
2016-09-06T10:23:08.654394: step 6430, loss 0.00920988, acc 1
2016-09-06T10:23:09.459958: step 6431, loss 0.0183542, acc 1
2016-09-06T10:23:10.265146: step 6432, loss 0.003244, acc 1
2016-09-06T10:23:11.052066: step 6433, loss 0.0137707, acc 1
2016-09-06T10:23:11.855335: step 6434, loss 0.0412798, acc 0.96
2016-09-06T10:23:12.669844: step 6435, loss 0.00955828, acc 1
2016-09-06T10:23:13.457412: step 6436, loss 0.0151883, acc 1
2016-09-06T10:23:14.241328: step 6437, loss 0.0447354, acc 0.98
2016-09-06T10:23:15.028415: step 6438, loss 0.00315695, acc 1
2016-09-06T10:23:15.844832: step 6439, loss 0.0175474, acc 0.98
2016-09-06T10:23:16.661346: step 6440, loss 0.00454687, acc 1
2016-09-06T10:23:17.489741: step 6441, loss 0.0337497, acc 0.98
2016-09-06T10:23:18.299742: step 6442, loss 0.0197265, acc 1
2016-09-06T10:23:19.101860: step 6443, loss 0.0911699, acc 0.96
2016-09-06T10:23:19.932633: step 6444, loss 0.0336268, acc 0.98
2016-09-06T10:23:20.733314: step 6445, loss 0.0183814, acc 0.98
2016-09-06T10:23:21.532089: step 6446, loss 0.0736214, acc 0.98
2016-09-06T10:23:22.336658: step 6447, loss 0.00532464, acc 1
2016-09-06T10:23:23.137452: step 6448, loss 0.0709847, acc 0.98
2016-09-06T10:23:23.938349: step 6449, loss 0.00795831, acc 1
2016-09-06T10:23:24.775460: step 6450, loss 0.028756, acc 0.98
2016-09-06T10:23:25.572935: step 6451, loss 0.0882207, acc 0.94
2016-09-06T10:23:26.409028: step 6452, loss 0.0765696, acc 0.96
2016-09-06T10:23:27.233894: step 6453, loss 0.00841596, acc 1
2016-09-06T10:23:28.037201: step 6454, loss 0.0131992, acc 1
2016-09-06T10:23:28.845968: step 6455, loss 0.0404352, acc 1
2016-09-06T10:23:29.651199: step 6456, loss 0.0196799, acc 1
2016-09-06T10:23:30.437242: step 6457, loss 0.00337094, acc 1
2016-09-06T10:23:31.242058: step 6458, loss 0.00482775, acc 1
2016-09-06T10:23:32.041406: step 6459, loss 0.0128347, acc 1
2016-09-06T10:23:32.827772: step 6460, loss 0.0445173, acc 0.98
2016-09-06T10:23:33.644599: step 6461, loss 0.0361071, acc 1
2016-09-06T10:23:34.451811: step 6462, loss 0.0360542, acc 1
2016-09-06T10:23:35.248077: step 6463, loss 0.0312985, acc 0.98
2016-09-06T10:23:36.055171: step 6464, loss 0.0229241, acc 1
2016-09-06T10:23:36.900401: step 6465, loss 0.0406714, acc 0.96
2016-09-06T10:23:37.700102: step 6466, loss 0.0152648, acc 1
2016-09-06T10:23:38.462428: step 6467, loss 0.0264059, acc 0.98
2016-09-06T10:23:39.281384: step 6468, loss 0.0266304, acc 0.98
2016-09-06T10:23:40.075482: step 6469, loss 0.0243581, acc 0.98
2016-09-06T10:23:40.888796: step 6470, loss 0.0369582, acc 0.98
2016-09-06T10:23:41.713626: step 6471, loss 0.03788, acc 1
2016-09-06T10:23:42.531639: step 6472, loss 0.0230904, acc 0.98
2016-09-06T10:23:43.328841: step 6473, loss 0.0336207, acc 0.98
2016-09-06T10:23:44.147598: step 6474, loss 0.0200191, acc 1
2016-09-06T10:23:44.933397: step 6475, loss 0.0400988, acc 0.98
2016-09-06T10:23:45.726077: step 6476, loss 0.00678428, acc 1
2016-09-06T10:23:46.551348: step 6477, loss 0.040583, acc 0.96
2016-09-06T10:23:47.342340: step 6478, loss 0.00274732, acc 1
2016-09-06T10:23:48.135103: step 6479, loss 0.00512157, acc 1
2016-09-06T10:23:48.966227: step 6480, loss 0.0642815, acc 0.94
2016-09-06T10:23:49.743059: step 6481, loss 0.0228758, acc 1
2016-09-06T10:23:50.527311: step 6482, loss 0.010286, acc 1
2016-09-06T10:23:51.331169: step 6483, loss 0.0121141, acc 1
2016-09-06T10:23:52.113041: step 6484, loss 0.0666565, acc 0.98
2016-09-06T10:23:52.945187: step 6485, loss 0.0467704, acc 0.98
2016-09-06T10:23:53.799831: step 6486, loss 0.0144554, acc 1
2016-09-06T10:23:54.587639: step 6487, loss 0.00291443, acc 1
2016-09-06T10:23:55.410193: step 6488, loss 0.00548192, acc 1
2016-09-06T10:23:56.239351: step 6489, loss 0.0172589, acc 1
2016-09-06T10:23:57.037885: step 6490, loss 0.00289524, acc 1
2016-09-06T10:23:57.840190: step 6491, loss 0.038547, acc 0.98
2016-09-06T10:23:58.666780: step 6492, loss 0.0201808, acc 1
2016-09-06T10:23:59.471031: step 6493, loss 0.0106141, acc 1
2016-09-06T10:24:00.278935: step 6494, loss 0.0156242, acc 1
2016-09-06T10:24:01.119673: step 6495, loss 0.0057047, acc 1
2016-09-06T10:24:01.911643: step 6496, loss 0.00578561, acc 1
2016-09-06T10:24:02.705704: step 6497, loss 0.167788, acc 0.96
2016-09-06T10:24:03.516690: step 6498, loss 0.0134489, acc 1
2016-09-06T10:24:04.337535: step 6499, loss 0.0872008, acc 0.96
2016-09-06T10:24:05.133236: step 6500, loss 0.00428377, acc 1

Evaluation:
2016-09-06T10:24:08.852138: step 6500, loss 2.27562, acc 0.732645

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-6500

2016-09-06T10:24:10.764169: step 6501, loss 0.0084815, acc 1
2016-09-06T10:24:11.544265: step 6502, loss 0.0166015, acc 1
2016-09-06T10:24:12.369001: step 6503, loss 0.0274347, acc 1
2016-09-06T10:24:13.190706: step 6504, loss 0.0268075, acc 0.98
2016-09-06T10:24:13.985529: step 6505, loss 0.106208, acc 0.96
2016-09-06T10:24:14.771638: step 6506, loss 0.0973775, acc 0.98
2016-09-06T10:24:15.611734: step 6507, loss 0.0138681, acc 1
2016-09-06T10:24:16.383327: step 6508, loss 0.0250911, acc 1
2016-09-06T10:24:17.197407: step 6509, loss 0.10576, acc 0.94
2016-09-06T10:24:17.997280: step 6510, loss 0.0147391, acc 1
2016-09-06T10:24:18.772172: step 6511, loss 0.114295, acc 0.96
2016-09-06T10:24:19.592920: step 6512, loss 0.0161947, acc 1
2016-09-06T10:24:20.389453: step 6513, loss 0.0194267, acc 1
2016-09-06T10:24:21.194690: step 6514, loss 0.122557, acc 0.98
2016-09-06T10:24:22.003308: step 6515, loss 0.00468455, acc 1
2016-09-06T10:24:22.811066: step 6516, loss 0.046892, acc 0.98
2016-09-06T10:24:23.604876: step 6517, loss 0.00605173, acc 1
2016-09-06T10:24:24.426213: step 6518, loss 0.0932327, acc 0.96
2016-09-06T10:24:25.230976: step 6519, loss 0.0802869, acc 0.98
2016-09-06T10:24:26.043077: step 6520, loss 0.0179767, acc 1
2016-09-06T10:24:26.851384: step 6521, loss 0.00339136, acc 1
2016-09-06T10:24:27.687206: step 6522, loss 0.224919, acc 0.98
2016-09-06T10:24:28.478225: step 6523, loss 0.0472798, acc 0.96
2016-09-06T10:24:29.274808: step 6524, loss 0.0465266, acc 0.96
2016-09-06T10:24:30.103814: step 6525, loss 0.0759164, acc 0.96
2016-09-06T10:24:30.898735: step 6526, loss 0.0300453, acc 0.98
2016-09-06T10:24:31.704771: step 6527, loss 0.0068507, acc 1
2016-09-06T10:24:32.463541: step 6528, loss 0.0348152, acc 0.977273
2016-09-06T10:24:33.256632: step 6529, loss 0.0256077, acc 1
2016-09-06T10:24:34.068854: step 6530, loss 0.0257632, acc 1
2016-09-06T10:24:34.874637: step 6531, loss 0.0207696, acc 0.98
2016-09-06T10:24:35.661289: step 6532, loss 0.0267331, acc 0.98
2016-09-06T10:24:36.479272: step 6533, loss 0.0549701, acc 0.98
2016-09-06T10:24:37.300255: step 6534, loss 0.0337359, acc 0.98
2016-09-06T10:24:38.091560: step 6535, loss 0.0576812, acc 0.98
2016-09-06T10:24:38.917327: step 6536, loss 0.0434835, acc 1
2016-09-06T10:24:39.774852: step 6537, loss 0.0289359, acc 1
2016-09-06T10:24:40.605070: step 6538, loss 0.0344471, acc 0.98
2016-09-06T10:24:41.428997: step 6539, loss 0.00759067, acc 1
2016-09-06T10:24:42.289638: step 6540, loss 0.0148133, acc 1
2016-09-06T10:24:43.065703: step 6541, loss 0.0217118, acc 0.98
2016-09-06T10:24:43.892817: step 6542, loss 0.0202333, acc 1
2016-09-06T10:24:44.718818: step 6543, loss 0.0245745, acc 1
2016-09-06T10:24:45.514602: step 6544, loss 0.0113744, acc 1
2016-09-06T10:24:46.302683: step 6545, loss 0.0322157, acc 0.98
2016-09-06T10:24:47.147697: step 6546, loss 0.00479875, acc 1
2016-09-06T10:24:48.009714: step 6547, loss 0.0614829, acc 0.96
2016-09-06T10:24:48.823666: step 6548, loss 0.00499505, acc 1
2016-09-06T10:24:49.671334: step 6549, loss 0.0252893, acc 1
2016-09-06T10:24:50.476290: step 6550, loss 0.0185989, acc 1
2016-09-06T10:24:51.274065: step 6551, loss 0.0042156, acc 1
2016-09-06T10:24:52.097849: step 6552, loss 0.0344679, acc 0.98
2016-09-06T10:24:52.891911: step 6553, loss 0.0186448, acc 0.98
2016-09-06T10:24:53.717078: step 6554, loss 0.00857251, acc 1
2016-09-06T10:24:54.586144: step 6555, loss 0.0145853, acc 1
2016-09-06T10:24:55.413975: step 6556, loss 0.00884789, acc 1
2016-09-06T10:24:56.212537: step 6557, loss 0.0165798, acc 1
2016-09-06T10:24:57.005128: step 6558, loss 0.0488653, acc 0.96
2016-09-06T10:24:57.820701: step 6559, loss 0.00423421, acc 1
2016-09-06T10:24:58.624797: step 6560, loss 0.0281571, acc 0.98
2016-09-06T10:24:59.424226: step 6561, loss 0.0195571, acc 0.98
2016-09-06T10:25:00.256691: step 6562, loss 0.0908787, acc 0.96
2016-09-06T10:25:01.045885: step 6563, loss 0.00437368, acc 1
2016-09-06T10:25:01.857257: step 6564, loss 0.0164695, acc 1
2016-09-06T10:25:02.677439: step 6565, loss 0.00718668, acc 1
2016-09-06T10:25:03.462362: step 6566, loss 0.0297514, acc 0.98
2016-09-06T10:25:04.274481: step 6567, loss 0.0159399, acc 1
2016-09-06T10:25:05.078950: step 6568, loss 0.0419021, acc 0.98
2016-09-06T10:25:05.864808: step 6569, loss 0.0069272, acc 1
2016-09-06T10:25:06.679838: step 6570, loss 0.0110025, acc 1
2016-09-06T10:25:07.500508: step 6571, loss 0.0821329, acc 0.98
2016-09-06T10:25:08.288078: step 6572, loss 0.0133237, acc 1
2016-09-06T10:25:09.103707: step 6573, loss 0.0036697, acc 1
2016-09-06T10:25:09.917695: step 6574, loss 0.00801604, acc 1
2016-09-06T10:25:10.715657: step 6575, loss 0.010991, acc 1
2016-09-06T10:25:11.506487: step 6576, loss 0.0139658, acc 1
2016-09-06T10:25:12.339701: step 6577, loss 0.00823036, acc 1
2016-09-06T10:25:13.135680: step 6578, loss 0.0170818, acc 1
2016-09-06T10:25:13.953056: step 6579, loss 0.0101879, acc 1
2016-09-06T10:25:14.817317: step 6580, loss 0.0550495, acc 0.96
2016-09-06T10:25:15.618004: step 6581, loss 0.0693113, acc 0.96
2016-09-06T10:25:16.384114: step 6582, loss 0.00366535, acc 1
2016-09-06T10:25:17.234178: step 6583, loss 0.0335444, acc 0.98
2016-09-06T10:25:18.007897: step 6584, loss 0.0293709, acc 0.98
2016-09-06T10:25:18.844239: step 6585, loss 0.0351236, acc 0.98
2016-09-06T10:25:19.653455: step 6586, loss 0.0155854, acc 1
2016-09-06T10:25:20.442302: step 6587, loss 0.0058669, acc 1
2016-09-06T10:25:21.233017: step 6588, loss 0.00368527, acc 1
2016-09-06T10:25:22.061440: step 6589, loss 0.0861581, acc 0.94
2016-09-06T10:25:22.841148: step 6590, loss 0.0141838, acc 1
2016-09-06T10:25:23.657402: step 6591, loss 0.00398012, acc 1
2016-09-06T10:25:24.466611: step 6592, loss 0.0254022, acc 0.98
2016-09-06T10:25:25.272334: step 6593, loss 0.0241169, acc 0.98
2016-09-06T10:25:26.063601: step 6594, loss 0.0173986, acc 1
2016-09-06T10:25:26.894249: step 6595, loss 0.0378039, acc 0.98
2016-09-06T10:25:27.669347: step 6596, loss 0.0666956, acc 0.96
2016-09-06T10:25:28.478961: step 6597, loss 0.0489743, acc 0.98
2016-09-06T10:25:29.287515: step 6598, loss 0.0397327, acc 1
2016-09-06T10:25:30.117757: step 6599, loss 0.0124156, acc 1
2016-09-06T10:25:30.916251: step 6600, loss 0.00299465, acc 1

Evaluation:
2016-09-06T10:25:34.651366: step 6600, loss 1.90424, acc 0.756098

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-6600

2016-09-06T10:25:36.480213: step 6601, loss 0.0373502, acc 0.98
2016-09-06T10:25:37.282245: step 6602, loss 0.0111785, acc 1
2016-09-06T10:25:38.118606: step 6603, loss 0.0105029, acc 1
2016-09-06T10:25:38.981473: step 6604, loss 0.0913963, acc 0.98
2016-09-06T10:25:39.793999: step 6605, loss 0.00657012, acc 1
2016-09-06T10:25:40.565263: step 6606, loss 0.00755324, acc 1
2016-09-06T10:25:41.377434: step 6607, loss 0.003117, acc 1
2016-09-06T10:25:42.172474: step 6608, loss 0.0582192, acc 0.96
2016-09-06T10:25:42.979307: step 6609, loss 0.156505, acc 0.96
2016-09-06T10:25:43.793599: step 6610, loss 0.0292706, acc 1
2016-09-06T10:25:44.621276: step 6611, loss 0.110575, acc 0.96
2016-09-06T10:25:45.414095: step 6612, loss 0.00382209, acc 1
2016-09-06T10:25:46.221122: step 6613, loss 0.0379979, acc 0.96
2016-09-06T10:25:47.018219: step 6614, loss 0.00560237, acc 1
2016-09-06T10:25:47.832004: step 6615, loss 0.0031962, acc 1
2016-09-06T10:25:48.682443: step 6616, loss 0.0203012, acc 1
2016-09-06T10:25:49.490018: step 6617, loss 0.0133705, acc 1
2016-09-06T10:25:50.295950: step 6618, loss 0.00881405, acc 1
2016-09-06T10:25:51.141930: step 6619, loss 0.0664807, acc 0.96
2016-09-06T10:25:51.974728: step 6620, loss 0.00282455, acc 1
2016-09-06T10:25:52.775843: step 6621, loss 0.0242484, acc 1
2016-09-06T10:25:53.631810: step 6622, loss 0.00375755, acc 1
2016-09-06T10:25:54.458335: step 6623, loss 0.0175779, acc 1
2016-09-06T10:25:55.303903: step 6624, loss 0.0195411, acc 0.98
2016-09-06T10:25:56.160657: step 6625, loss 0.00553044, acc 1
2016-09-06T10:25:56.964185: step 6626, loss 0.0884817, acc 0.98
2016-09-06T10:25:57.780673: step 6627, loss 0.0202671, acc 0.98
2016-09-06T10:25:58.624982: step 6628, loss 0.0492597, acc 0.96
2016-09-06T10:25:59.463696: step 6629, loss 0.008091, acc 1
2016-09-06T10:26:00.271213: step 6630, loss 0.0118479, acc 1
2016-09-06T10:26:01.080680: step 6631, loss 0.00633803, acc 1
2016-09-06T10:26:01.875055: step 6632, loss 0.00370322, acc 1
2016-09-06T10:26:02.697097: step 6633, loss 0.0181647, acc 1
2016-09-06T10:26:03.512857: step 6634, loss 0.0389795, acc 0.96
2016-09-06T10:26:04.337326: step 6635, loss 0.0244469, acc 1
2016-09-06T10:26:05.123584: step 6636, loss 0.030643, acc 1
2016-09-06T10:26:05.928886: step 6637, loss 0.0423661, acc 0.98
2016-09-06T10:26:06.730350: step 6638, loss 0.028966, acc 1
2016-09-06T10:26:07.535888: step 6639, loss 0.00281687, acc 1
2016-09-06T10:26:08.356186: step 6640, loss 0.0722613, acc 0.98
2016-09-06T10:26:09.185069: step 6641, loss 0.017812, acc 0.98
2016-09-06T10:26:09.968659: step 6642, loss 0.00660161, acc 1
2016-09-06T10:26:10.771839: step 6643, loss 0.0351958, acc 1
2016-09-06T10:26:11.597835: step 6644, loss 0.00413322, acc 1
2016-09-06T10:26:12.397511: step 6645, loss 0.0222128, acc 0.98
2016-09-06T10:26:13.182741: step 6646, loss 0.0176221, acc 1
2016-09-06T10:26:13.991585: step 6647, loss 0.0185147, acc 1
2016-09-06T10:26:14.812966: step 6648, loss 0.00396749, acc 1
2016-09-06T10:26:15.602241: step 6649, loss 0.00384186, acc 1
2016-09-06T10:26:16.431956: step 6650, loss 0.00416821, acc 1
2016-09-06T10:26:17.218781: step 6651, loss 0.0148215, acc 1
2016-09-06T10:26:18.024337: step 6652, loss 0.0305086, acc 0.98
2016-09-06T10:26:18.847748: step 6653, loss 0.0179935, acc 1
2016-09-06T10:26:19.640608: step 6654, loss 0.00352131, acc 1
2016-09-06T10:26:20.424255: step 6655, loss 0.00767398, acc 1
2016-09-06T10:26:21.275278: step 6656, loss 0.0285341, acc 1
2016-09-06T10:26:22.068632: step 6657, loss 0.0601239, acc 0.98
2016-09-06T10:26:22.881866: step 6658, loss 0.00939366, acc 1
2016-09-06T10:26:23.701647: step 6659, loss 0.0628401, acc 0.98
2016-09-06T10:26:24.494741: step 6660, loss 0.144548, acc 0.94
2016-09-06T10:26:25.303154: step 6661, loss 0.00504137, acc 1
2016-09-06T10:26:26.115115: step 6662, loss 0.0981364, acc 0.98
2016-09-06T10:26:26.929906: step 6663, loss 0.0197226, acc 1
2016-09-06T10:26:27.720446: step 6664, loss 0.0298671, acc 0.98
2016-09-06T10:26:28.545391: step 6665, loss 0.00458102, acc 1
2016-09-06T10:26:29.327771: step 6666, loss 0.00802268, acc 1
2016-09-06T10:26:30.172709: step 6667, loss 0.00970998, acc 1
2016-09-06T10:26:30.983423: step 6668, loss 0.0517715, acc 0.98
2016-09-06T10:26:31.750121: step 6669, loss 0.0301696, acc 1
2016-09-06T10:26:32.590550: step 6670, loss 0.0260984, acc 1
2016-09-06T10:26:33.412428: step 6671, loss 0.0308212, acc 0.98
2016-09-06T10:26:34.196420: step 6672, loss 0.0313128, acc 1
2016-09-06T10:26:34.989312: step 6673, loss 0.0427174, acc 0.98
2016-09-06T10:26:35.807698: step 6674, loss 0.0727461, acc 0.96
2016-09-06T10:26:36.577406: step 6675, loss 0.0268062, acc 0.98
2016-09-06T10:26:37.428336: step 6676, loss 0.00470551, acc 1
2016-09-06T10:26:38.274019: step 6677, loss 0.0288423, acc 0.98
2016-09-06T10:26:39.095332: step 6678, loss 0.0372951, acc 0.98
2016-09-06T10:26:39.895467: step 6679, loss 0.0136065, acc 1
2016-09-06T10:26:40.734097: step 6680, loss 0.0171089, acc 1
2016-09-06T10:26:41.548416: step 6681, loss 0.147161, acc 0.96
2016-09-06T10:26:42.371210: step 6682, loss 0.00802307, acc 1
2016-09-06T10:26:43.203933: step 6683, loss 0.00877121, acc 1
2016-09-06T10:26:44.005353: step 6684, loss 0.0668781, acc 0.96
2016-09-06T10:26:44.819630: step 6685, loss 0.038799, acc 0.96
2016-09-06T10:26:45.669465: step 6686, loss 0.0205987, acc 1
2016-09-06T10:26:46.477248: step 6687, loss 0.0139942, acc 1
2016-09-06T10:26:47.299699: step 6688, loss 0.0425395, acc 0.98
2016-09-06T10:26:48.129727: step 6689, loss 0.0236613, acc 1
2016-09-06T10:26:48.931416: step 6690, loss 0.0329258, acc 1
2016-09-06T10:26:49.737825: step 6691, loss 0.0448112, acc 0.98
2016-09-06T10:26:50.564296: step 6692, loss 0.00693383, acc 1
2016-09-06T10:26:51.411463: step 6693, loss 0.0123354, acc 1
2016-09-06T10:26:52.190754: step 6694, loss 0.0212111, acc 0.98
2016-09-06T10:26:53.035827: step 6695, loss 0.0430504, acc 0.98
2016-09-06T10:26:53.886285: step 6696, loss 0.0661547, acc 0.98
2016-09-06T10:26:54.699410: step 6697, loss 0.0603976, acc 0.96
2016-09-06T10:26:55.561783: step 6698, loss 0.0435479, acc 0.96
2016-09-06T10:26:56.417843: step 6699, loss 0.0238524, acc 1
2016-09-06T10:26:57.228588: step 6700, loss 0.0562761, acc 0.98

Evaluation:
2016-09-06T10:27:00.982265: step 6700, loss 2.31471, acc 0.744841

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-6700

2016-09-06T10:27:03.013331: step 6701, loss 0.00632762, acc 1
2016-09-06T10:27:03.828413: step 6702, loss 0.0294826, acc 1
2016-09-06T10:27:04.632909: step 6703, loss 0.0356554, acc 0.96
2016-09-06T10:27:05.443012: step 6704, loss 0.0271586, acc 0.98
2016-09-06T10:27:06.253180: step 6705, loss 0.0105355, acc 1
2016-09-06T10:27:07.057153: step 6706, loss 0.0209179, acc 1
2016-09-06T10:27:07.871160: step 6707, loss 0.0305197, acc 1
2016-09-06T10:27:08.707401: step 6708, loss 0.107201, acc 0.96
2016-09-06T10:27:09.531849: step 6709, loss 0.00673362, acc 1
2016-09-06T10:27:10.341152: step 6710, loss 0.00560168, acc 1
2016-09-06T10:27:11.178851: step 6711, loss 0.026188, acc 0.98
2016-09-06T10:27:11.984887: step 6712, loss 0.0158432, acc 1
2016-09-06T10:27:12.787612: step 6713, loss 0.0130476, acc 1
2016-09-06T10:27:13.592596: step 6714, loss 0.0172061, acc 1
2016-09-06T10:27:14.408896: step 6715, loss 0.0301995, acc 1
2016-09-06T10:27:15.222046: step 6716, loss 0.00933827, acc 1
2016-09-06T10:27:16.047183: step 6717, loss 0.14966, acc 0.96
2016-09-06T10:27:16.837124: step 6718, loss 0.0050081, acc 1
2016-09-06T10:27:17.626301: step 6719, loss 0.0281445, acc 0.98
2016-09-06T10:27:18.375676: step 6720, loss 0.0215661, acc 1
2016-09-06T10:27:19.190354: step 6721, loss 0.0366638, acc 0.98
2016-09-06T10:27:20.022332: step 6722, loss 0.0258576, acc 0.98
2016-09-06T10:27:20.833822: step 6723, loss 0.00280521, acc 1
2016-09-06T10:27:21.621996: step 6724, loss 0.0130178, acc 1
2016-09-06T10:27:22.446398: step 6725, loss 0.0181016, acc 1
2016-09-06T10:27:23.289930: step 6726, loss 0.0172606, acc 1
2016-09-06T10:27:24.123792: step 6727, loss 0.0345363, acc 1
2016-09-06T10:27:24.926511: step 6728, loss 0.0359235, acc 0.98
2016-09-06T10:27:25.784684: step 6729, loss 0.0315744, acc 0.98
2016-09-06T10:27:26.623964: step 6730, loss 0.0567903, acc 0.96
2016-09-06T10:27:27.414345: step 6731, loss 0.00265366, acc 1
2016-09-06T10:27:28.231605: step 6732, loss 0.00278185, acc 1
2016-09-06T10:27:29.055438: step 6733, loss 0.0677098, acc 0.98
2016-09-06T10:27:29.874440: step 6734, loss 0.00570003, acc 1
2016-09-06T10:27:30.698269: step 6735, loss 0.0431877, acc 0.96
2016-09-06T10:27:31.520832: step 6736, loss 0.0191091, acc 0.98
2016-09-06T10:27:32.353470: step 6737, loss 0.00724833, acc 1
2016-09-06T10:27:33.138269: step 6738, loss 0.0102604, acc 1
2016-09-06T10:27:33.964389: step 6739, loss 0.0276752, acc 1
2016-09-06T10:27:34.796798: step 6740, loss 0.00619439, acc 1
2016-09-06T10:27:35.636221: step 6741, loss 0.0458101, acc 0.96
2016-09-06T10:27:36.463263: step 6742, loss 0.0723812, acc 0.98
2016-09-06T10:27:37.292387: step 6743, loss 0.0295813, acc 1
2016-09-06T10:27:38.088815: step 6744, loss 0.081864, acc 0.98
2016-09-06T10:27:38.909984: step 6745, loss 0.0418266, acc 0.98
2016-09-06T10:27:39.719430: step 6746, loss 0.0268395, acc 0.98
2016-09-06T10:27:40.520884: step 6747, loss 0.0151714, acc 1
2016-09-06T10:27:41.344086: step 6748, loss 0.0216809, acc 0.98
2016-09-06T10:27:42.160000: step 6749, loss 0.00746569, acc 1
2016-09-06T10:27:42.990225: step 6750, loss 0.0141951, acc 1
2016-09-06T10:27:43.828773: step 6751, loss 0.00284544, acc 1
2016-09-06T10:27:44.646617: step 6752, loss 0.0609297, acc 0.98
2016-09-06T10:27:45.451497: step 6753, loss 0.00900365, acc 1
2016-09-06T10:27:46.317477: step 6754, loss 0.0317089, acc 0.98
2016-09-06T10:27:47.131593: step 6755, loss 0.00881659, acc 1
2016-09-06T10:27:47.932913: step 6756, loss 0.00649541, acc 1
2016-09-06T10:27:48.763679: step 6757, loss 0.0200784, acc 0.98
2016-09-06T10:27:49.594168: step 6758, loss 0.0583249, acc 0.96
2016-09-06T10:27:50.386339: step 6759, loss 0.0214739, acc 1
2016-09-06T10:27:51.188080: step 6760, loss 0.00538908, acc 1
2016-09-06T10:27:52.005306: step 6761, loss 0.0157125, acc 1
2016-09-06T10:27:52.794358: step 6762, loss 0.00317033, acc 1
2016-09-06T10:27:53.609659: step 6763, loss 0.0489959, acc 0.96
2016-09-06T10:27:54.437862: step 6764, loss 0.00264924, acc 1
2016-09-06T10:27:55.295678: step 6765, loss 0.0152516, acc 1
2016-09-06T10:27:56.107669: step 6766, loss 0.0878476, acc 0.94
2016-09-06T10:27:56.997346: step 6767, loss 0.0357958, acc 1
2016-09-06T10:27:57.818499: step 6768, loss 0.0192929, acc 1
2016-09-06T10:27:58.613160: step 6769, loss 0.0313885, acc 0.98
2016-09-06T10:27:59.427485: step 6770, loss 0.00278663, acc 1
2016-09-06T10:28:00.259238: step 6771, loss 0.0162974, acc 1
2016-09-06T10:28:01.049205: step 6772, loss 0.0142606, acc 1
2016-09-06T10:28:01.864284: step 6773, loss 0.0257391, acc 1
2016-09-06T10:28:02.674314: step 6774, loss 0.0424076, acc 0.98
2016-09-06T10:28:03.465523: step 6775, loss 0.0137701, acc 1
2016-09-06T10:28:04.272465: step 6776, loss 0.00329091, acc 1
2016-09-06T10:28:05.085502: step 6777, loss 0.00557007, acc 1
2016-09-06T10:28:05.886286: step 6778, loss 0.0234007, acc 1
2016-09-06T10:28:06.695221: step 6779, loss 0.00241096, acc 1
2016-09-06T10:28:07.488660: step 6780, loss 0.0104959, acc 1
2016-09-06T10:28:08.292401: step 6781, loss 0.0391317, acc 1
2016-09-06T10:28:09.109557: step 6782, loss 0.0151225, acc 1
2016-09-06T10:28:09.948333: step 6783, loss 0.00450148, acc 1
2016-09-06T10:28:10.756879: step 6784, loss 0.0158538, acc 1
2016-09-06T10:28:11.614367: step 6785, loss 0.0263689, acc 0.98
2016-09-06T10:28:12.413918: step 6786, loss 0.146237, acc 0.96
2016-09-06T10:28:13.202121: step 6787, loss 0.0113168, acc 1
2016-09-06T10:28:14.051879: step 6788, loss 0.0134136, acc 1
2016-09-06T10:28:14.905499: step 6789, loss 0.0178276, acc 0.98
2016-09-06T10:28:15.739931: step 6790, loss 0.00385545, acc 1
2016-09-06T10:28:16.571495: step 6791, loss 0.02333, acc 0.98
2016-09-06T10:28:17.399398: step 6792, loss 0.0406749, acc 0.98
2016-09-06T10:28:18.204223: step 6793, loss 0.018642, acc 0.98
2016-09-06T10:28:19.007848: step 6794, loss 0.00545161, acc 1
2016-09-06T10:28:19.859312: step 6795, loss 0.0485082, acc 0.96
2016-09-06T10:28:20.621137: step 6796, loss 0.0422459, acc 0.96
2016-09-06T10:28:21.417624: step 6797, loss 0.0294604, acc 0.98
2016-09-06T10:28:22.241936: step 6798, loss 0.14009, acc 0.98
2016-09-06T10:28:23.029416: step 6799, loss 0.0460416, acc 0.96
2016-09-06T10:28:23.861780: step 6800, loss 0.0199265, acc 0.98

Evaluation:
2016-09-06T10:28:27.618985: step 6800, loss 2.20406, acc 0.751407

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-6800

2016-09-06T10:28:29.582377: step 6801, loss 0.00258244, acc 1
2016-09-06T10:28:30.387452: step 6802, loss 0.00244484, acc 1
2016-09-06T10:28:31.246957: step 6803, loss 0.0311341, acc 0.98
2016-09-06T10:28:32.103503: step 6804, loss 0.00270096, acc 1
2016-09-06T10:28:32.909868: step 6805, loss 0.0607668, acc 0.96
2016-09-06T10:28:33.715993: step 6806, loss 0.0158043, acc 1
2016-09-06T10:28:34.561233: step 6807, loss 0.0038272, acc 1
2016-09-06T10:28:35.391540: step 6808, loss 0.00390383, acc 1
2016-09-06T10:28:36.218184: step 6809, loss 0.0358315, acc 0.98
2016-09-06T10:28:37.050143: step 6810, loss 0.0207068, acc 0.98
2016-09-06T10:28:37.877938: step 6811, loss 0.0175867, acc 1
2016-09-06T10:28:38.714536: step 6812, loss 0.00290518, acc 1
2016-09-06T10:28:39.535911: step 6813, loss 0.0125876, acc 1
2016-09-06T10:28:40.328713: step 6814, loss 0.0217331, acc 0.98
2016-09-06T10:28:41.129521: step 6815, loss 0.0189073, acc 1
2016-09-06T10:28:41.961268: step 6816, loss 0.0103936, acc 1
2016-09-06T10:28:42.807785: step 6817, loss 0.0113247, acc 1
2016-09-06T10:28:43.607153: step 6818, loss 0.0209502, acc 1
2016-09-06T10:28:44.429370: step 6819, loss 0.0397695, acc 0.96
2016-09-06T10:28:45.226632: step 6820, loss 0.00254048, acc 1
2016-09-06T10:28:45.989397: step 6821, loss 0.0312911, acc 0.98
2016-09-06T10:28:46.790717: step 6822, loss 0.0196603, acc 1
2016-09-06T10:28:47.620954: step 6823, loss 0.0505476, acc 0.96
2016-09-06T10:28:48.408877: step 6824, loss 0.0312191, acc 0.98
2016-09-06T10:28:49.227942: step 6825, loss 0.00534805, acc 1
2016-09-06T10:28:50.046055: step 6826, loss 0.0297373, acc 0.98
2016-09-06T10:28:50.831530: step 6827, loss 0.0349673, acc 0.96
2016-09-06T10:28:51.669234: step 6828, loss 0.0354805, acc 0.96
2016-09-06T10:28:52.508858: step 6829, loss 0.0479172, acc 0.98
2016-09-06T10:28:53.297613: step 6830, loss 0.0248991, acc 1
2016-09-06T10:28:54.123694: step 6831, loss 0.00261337, acc 1
2016-09-06T10:28:54.943366: step 6832, loss 0.0652265, acc 0.96
2016-09-06T10:28:55.754829: step 6833, loss 0.0197144, acc 0.98
2016-09-06T10:28:56.561207: step 6834, loss 0.00292502, acc 1
2016-09-06T10:28:57.421684: step 6835, loss 0.190197, acc 0.94
2016-09-06T10:28:58.253385: step 6836, loss 0.181917, acc 0.96
2016-09-06T10:28:59.066406: step 6837, loss 0.0524553, acc 0.98
2016-09-06T10:28:59.885943: step 6838, loss 0.00352033, acc 1
2016-09-06T10:29:00.713982: step 6839, loss 0.00906603, acc 1
2016-09-06T10:29:01.525065: step 6840, loss 0.0355732, acc 0.96
2016-09-06T10:29:02.339003: step 6841, loss 0.0356027, acc 0.98
2016-09-06T10:29:03.160831: step 6842, loss 0.00728596, acc 1
2016-09-06T10:29:03.999930: step 6843, loss 0.00763745, acc 1
2016-09-06T10:29:04.823364: step 6844, loss 0.0342727, acc 1
2016-09-06T10:29:05.648032: step 6845, loss 0.0453926, acc 0.98
2016-09-06T10:29:06.452875: step 6846, loss 0.0528831, acc 0.98
2016-09-06T10:29:07.276236: step 6847, loss 0.0244386, acc 1
2016-09-06T10:29:08.095116: step 6848, loss 0.0315413, acc 0.98
2016-09-06T10:29:08.913466: step 6849, loss 0.04166, acc 0.98
2016-09-06T10:29:09.746301: step 6850, loss 0.0437401, acc 0.98
2016-09-06T10:29:10.555559: step 6851, loss 0.0087617, acc 1
2016-09-06T10:29:11.364374: step 6852, loss 0.0185691, acc 1
2016-09-06T10:29:12.186912: step 6853, loss 0.00304878, acc 1
2016-09-06T10:29:13.005406: step 6854, loss 0.0511024, acc 0.98
2016-09-06T10:29:13.818109: step 6855, loss 0.0194567, acc 0.98
2016-09-06T10:29:14.612944: step 6856, loss 0.00413494, acc 1
2016-09-06T10:29:15.444228: step 6857, loss 0.0232613, acc 0.98
2016-09-06T10:29:16.226486: step 6858, loss 0.0394963, acc 0.96
2016-09-06T10:29:17.018943: step 6859, loss 0.0100849, acc 1
2016-09-06T10:29:17.835429: step 6860, loss 0.0168663, acc 1
2016-09-06T10:29:18.629362: step 6861, loss 0.0251169, acc 0.98
2016-09-06T10:29:19.428438: step 6862, loss 0.00313783, acc 1
2016-09-06T10:29:20.251568: step 6863, loss 0.0202969, acc 0.98
2016-09-06T10:29:21.044727: step 6864, loss 0.0311662, acc 0.98
2016-09-06T10:29:21.844481: step 6865, loss 0.0239416, acc 0.98
2016-09-06T10:29:22.659802: step 6866, loss 0.0245562, acc 0.98
2016-09-06T10:29:23.441797: step 6867, loss 0.0155165, acc 1
2016-09-06T10:29:24.291039: step 6868, loss 0.0196623, acc 1
2016-09-06T10:29:25.140575: step 6869, loss 0.0265652, acc 0.98
2016-09-06T10:29:25.915158: step 6870, loss 0.018928, acc 1
2016-09-06T10:29:26.718446: step 6871, loss 0.00448462, acc 1
2016-09-06T10:29:27.528270: step 6872, loss 0.00702459, acc 1
2016-09-06T10:29:28.326240: step 6873, loss 0.00919636, acc 1
2016-09-06T10:29:29.114259: step 6874, loss 0.00372362, acc 1
2016-09-06T10:29:29.924936: step 6875, loss 0.0230172, acc 1
2016-09-06T10:29:30.689870: step 6876, loss 0.00691304, acc 1
2016-09-06T10:29:31.537867: step 6877, loss 0.063689, acc 0.98
2016-09-06T10:29:32.355283: step 6878, loss 0.0131208, acc 1
2016-09-06T10:29:33.196037: step 6879, loss 0.0207495, acc 0.98
2016-09-06T10:29:34.005099: step 6880, loss 0.110869, acc 0.98
2016-09-06T10:29:34.816009: step 6881, loss 0.035548, acc 0.98
2016-09-06T10:29:35.615742: step 6882, loss 0.010991, acc 1
2016-09-06T10:29:36.423714: step 6883, loss 0.0163483, acc 1
2016-09-06T10:29:37.234845: step 6884, loss 0.0109505, acc 1
2016-09-06T10:29:38.006019: step 6885, loss 0.00928865, acc 1
2016-09-06T10:29:38.798861: step 6886, loss 0.0175433, acc 1
2016-09-06T10:29:39.609877: step 6887, loss 0.0140396, acc 1
2016-09-06T10:29:40.402172: step 6888, loss 0.00319511, acc 1
2016-09-06T10:29:41.204457: step 6889, loss 0.0710827, acc 0.96
2016-09-06T10:29:42.031486: step 6890, loss 0.0159206, acc 1
2016-09-06T10:29:42.831938: step 6891, loss 0.0813684, acc 0.94
2016-09-06T10:29:43.641911: step 6892, loss 0.0393924, acc 0.98
2016-09-06T10:29:44.500186: step 6893, loss 0.0384302, acc 0.98
2016-09-06T10:29:45.304176: step 6894, loss 0.0930302, acc 0.96
2016-09-06T10:29:46.113130: step 6895, loss 0.0270152, acc 0.98
2016-09-06T10:29:46.937303: step 6896, loss 0.0109948, acc 1
2016-09-06T10:29:47.740080: step 6897, loss 0.0310575, acc 0.98
2016-09-06T10:29:48.539701: step 6898, loss 0.0626441, acc 0.96
2016-09-06T10:29:49.375269: step 6899, loss 0.0160672, acc 1
2016-09-06T10:29:50.180011: step 6900, loss 0.0362308, acc 0.96

Evaluation:
2016-09-06T10:29:53.921386: step 6900, loss 2.16091, acc 0.746717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-6900

2016-09-06T10:29:55.841888: step 6901, loss 0.00972061, acc 1
2016-09-06T10:29:56.687366: step 6902, loss 0.0345335, acc 0.98
2016-09-06T10:29:57.489457: step 6903, loss 0.00758646, acc 1
2016-09-06T10:29:58.302606: step 6904, loss 0.026708, acc 1
2016-09-06T10:29:59.142287: step 6905, loss 0.0262762, acc 0.98
2016-09-06T10:29:59.944244: step 6906, loss 0.0268483, acc 0.98
2016-09-06T10:30:00.764617: step 6907, loss 0.0203695, acc 0.98
2016-09-06T10:30:01.602261: step 6908, loss 0.0224733, acc 1
2016-09-06T10:30:02.424868: step 6909, loss 0.0912787, acc 0.94
2016-09-06T10:30:03.223753: step 6910, loss 0.0296996, acc 0.98
2016-09-06T10:30:04.064441: step 6911, loss 0.0331711, acc 1
2016-09-06T10:30:04.832149: step 6912, loss 0.0194802, acc 1
2016-09-06T10:30:05.661105: step 6913, loss 0.00892369, acc 1
2016-09-06T10:30:06.480140: step 6914, loss 0.052468, acc 0.98
2016-09-06T10:30:07.288130: step 6915, loss 0.0210945, acc 0.98
2016-09-06T10:30:08.089475: step 6916, loss 0.00525253, acc 1
2016-09-06T10:30:08.913104: step 6917, loss 0.0226591, acc 1
2016-09-06T10:30:09.725643: step 6918, loss 0.0150894, acc 1
2016-09-06T10:30:10.534672: step 6919, loss 0.0103088, acc 1
2016-09-06T10:30:11.352167: step 6920, loss 0.00382156, acc 1
2016-09-06T10:30:12.156697: step 6921, loss 0.0214027, acc 1
2016-09-06T10:30:12.970852: step 6922, loss 0.0229236, acc 1
2016-09-06T10:30:13.778398: step 6923, loss 0.00973796, acc 1
2016-09-06T10:30:14.580697: step 6924, loss 0.00294408, acc 1
2016-09-06T10:30:15.382532: step 6925, loss 0.0273844, acc 0.98
2016-09-06T10:30:16.217192: step 6926, loss 0.0405397, acc 0.96
2016-09-06T10:30:17.007003: step 6927, loss 0.011882, acc 1
2016-09-06T10:30:17.800593: step 6928, loss 0.00346542, acc 1
2016-09-06T10:30:18.651071: step 6929, loss 0.018096, acc 0.98
2016-09-06T10:30:19.492908: step 6930, loss 0.098722, acc 0.98
2016-09-06T10:30:20.304545: step 6931, loss 0.0214854, acc 1
2016-09-06T10:30:21.118587: step 6932, loss 0.0148694, acc 1
2016-09-06T10:30:21.938175: step 6933, loss 0.00335205, acc 1
2016-09-06T10:30:22.726158: step 6934, loss 0.014993, acc 1
2016-09-06T10:30:23.539452: step 6935, loss 0.017164, acc 1
2016-09-06T10:30:24.382241: step 6936, loss 0.0279922, acc 1
2016-09-06T10:30:25.183742: step 6937, loss 0.00703338, acc 1
2016-09-06T10:30:25.994179: step 6938, loss 0.0128839, acc 1
2016-09-06T10:30:26.879544: step 6939, loss 0.00457779, acc 1
2016-09-06T10:30:27.686651: step 6940, loss 0.0100199, acc 1
2016-09-06T10:30:28.518215: step 6941, loss 0.040417, acc 0.98
2016-09-06T10:30:29.377733: step 6942, loss 0.0665598, acc 0.96
2016-09-06T10:30:30.187426: step 6943, loss 0.0160673, acc 1
2016-09-06T10:30:30.989224: step 6944, loss 0.043537, acc 0.96
2016-09-06T10:30:31.841124: step 6945, loss 0.0121685, acc 1
2016-09-06T10:30:32.637313: step 6946, loss 0.0155735, acc 1
2016-09-06T10:30:33.445583: step 6947, loss 0.017461, acc 1
2016-09-06T10:30:34.277790: step 6948, loss 0.0222878, acc 0.98
2016-09-06T10:30:35.073580: step 6949, loss 0.0194999, acc 1
2016-09-06T10:30:35.907150: step 6950, loss 0.0317478, acc 0.98
2016-09-06T10:30:36.758017: step 6951, loss 0.0394266, acc 0.96
2016-09-06T10:30:37.558946: step 6952, loss 0.0348306, acc 0.98
2016-09-06T10:30:38.369214: step 6953, loss 0.00532428, acc 1
2016-09-06T10:30:39.202636: step 6954, loss 0.014638, acc 1
2016-09-06T10:30:40.000769: step 6955, loss 0.00427337, acc 1
2016-09-06T10:30:40.800351: step 6956, loss 0.00563403, acc 1
2016-09-06T10:30:41.640308: step 6957, loss 0.00364114, acc 1
2016-09-06T10:30:42.454597: step 6958, loss 0.0178686, acc 1
2016-09-06T10:30:43.245410: step 6959, loss 0.0120711, acc 1
2016-09-06T10:30:44.045505: step 6960, loss 0.0177121, acc 1
2016-09-06T10:30:44.855815: step 6961, loss 0.0237002, acc 0.98
2016-09-06T10:30:45.654189: step 6962, loss 0.00359072, acc 1
2016-09-06T10:30:46.461704: step 6963, loss 0.0163726, acc 1
2016-09-06T10:30:47.268622: step 6964, loss 0.00575536, acc 1
2016-09-06T10:30:48.051925: step 6965, loss 0.0374401, acc 0.96
2016-09-06T10:30:48.860228: step 6966, loss 0.0304032, acc 0.98
2016-09-06T10:30:49.674312: step 6967, loss 0.00555343, acc 1
2016-09-06T10:30:50.449586: step 6968, loss 0.00592246, acc 1
2016-09-06T10:30:51.284965: step 6969, loss 0.00368097, acc 1
2016-09-06T10:30:52.119364: step 6970, loss 0.0278697, acc 1
2016-09-06T10:30:52.921852: step 6971, loss 0.0037633, acc 1
2016-09-06T10:30:53.734660: step 6972, loss 0.00465664, acc 1
2016-09-06T10:30:54.579373: step 6973, loss 0.0297704, acc 0.98
2016-09-06T10:30:55.356937: step 6974, loss 0.0035011, acc 1
2016-09-06T10:30:56.159888: step 6975, loss 0.00441077, acc 1
2016-09-06T10:30:57.003595: step 6976, loss 0.0423181, acc 0.98
2016-09-06T10:30:57.838733: step 6977, loss 0.0409908, acc 0.98
2016-09-06T10:30:58.661395: step 6978, loss 0.0589691, acc 0.96
2016-09-06T10:30:59.529016: step 6979, loss 0.0191106, acc 0.98
2016-09-06T10:31:00.354295: step 6980, loss 0.154027, acc 0.96
2016-09-06T10:31:01.152283: step 6981, loss 0.0591745, acc 0.98
2016-09-06T10:31:01.985100: step 6982, loss 0.0122607, acc 1
2016-09-06T10:31:02.796863: step 6983, loss 0.0470085, acc 0.98
2016-09-06T10:31:03.633490: step 6984, loss 0.00713254, acc 1
2016-09-06T10:31:04.459230: step 6985, loss 0.0169846, acc 1
2016-09-06T10:31:05.285232: step 6986, loss 0.00436022, acc 1
2016-09-06T10:31:06.155194: step 6987, loss 0.0701959, acc 0.96
2016-09-06T10:31:06.988692: step 6988, loss 0.0165723, acc 1
2016-09-06T10:31:07.793421: step 6989, loss 0.0348855, acc 0.98
2016-09-06T10:31:08.585360: step 6990, loss 0.0659661, acc 0.94
2016-09-06T10:31:09.399211: step 6991, loss 0.00569925, acc 1
2016-09-06T10:31:10.241452: step 6992, loss 0.0481215, acc 0.98
2016-09-06T10:31:11.098812: step 6993, loss 0.00601357, acc 1
2016-09-06T10:31:11.915980: step 6994, loss 0.0506866, acc 0.98
2016-09-06T10:31:12.748678: step 6995, loss 0.0333409, acc 0.98
2016-09-06T10:31:13.573238: step 6996, loss 0.0204754, acc 1
2016-09-06T10:31:14.385693: step 6997, loss 0.0228335, acc 1
2016-09-06T10:31:15.225794: step 6998, loss 0.0196734, acc 1
2016-09-06T10:31:16.030739: step 6999, loss 0.00669542, acc 1
2016-09-06T10:31:16.844896: step 7000, loss 0.00812808, acc 1

Evaluation:
2016-09-06T10:31:20.576230: step 7000, loss 1.62522, acc 0.756098

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-7000

2016-09-06T10:31:22.537369: step 7001, loss 0.0622032, acc 0.96
2016-09-06T10:31:23.354421: step 7002, loss 0.00794497, acc 1
2016-09-06T10:31:24.172143: step 7003, loss 0.0443193, acc 0.98
2016-09-06T10:31:25.037374: step 7004, loss 0.0114592, acc 1
2016-09-06T10:31:25.857940: step 7005, loss 0.0387077, acc 0.98
2016-09-06T10:31:26.718339: step 7006, loss 0.00737083, acc 1
2016-09-06T10:31:27.565222: step 7007, loss 0.0197258, acc 1
2016-09-06T10:31:28.401629: step 7008, loss 0.0400862, acc 0.98
2016-09-06T10:31:29.228781: step 7009, loss 0.0688444, acc 0.98
2016-09-06T10:31:30.092883: step 7010, loss 0.0158532, acc 1
2016-09-06T10:31:30.914498: step 7011, loss 0.00504092, acc 1
2016-09-06T10:31:31.740004: step 7012, loss 0.0164919, acc 1
2016-09-06T10:31:32.541423: step 7013, loss 0.00414456, acc 1
2016-09-06T10:31:33.364890: step 7014, loss 0.0108205, acc 1
2016-09-06T10:31:34.152722: step 7015, loss 0.00459581, acc 1
2016-09-06T10:31:35.001744: step 7016, loss 0.00366623, acc 1
2016-09-06T10:31:35.836774: step 7017, loss 0.0728713, acc 0.98
2016-09-06T10:31:36.629882: step 7018, loss 0.0240042, acc 0.98
2016-09-06T10:31:37.451930: step 7019, loss 0.0123995, acc 1
2016-09-06T10:31:38.254399: step 7020, loss 0.0613236, acc 0.94
2016-09-06T10:31:39.065226: step 7021, loss 0.020665, acc 1
2016-09-06T10:31:39.894677: step 7022, loss 0.112363, acc 0.92
2016-09-06T10:31:40.718610: step 7023, loss 0.003664, acc 1
2016-09-06T10:31:41.545492: step 7024, loss 0.0592073, acc 0.96
2016-09-06T10:31:42.375375: step 7025, loss 0.0170267, acc 1
2016-09-06T10:31:43.227059: step 7026, loss 0.0373611, acc 0.98
2016-09-06T10:31:44.068193: step 7027, loss 0.0508146, acc 0.96
2016-09-06T10:31:44.892245: step 7028, loss 0.0183745, acc 1
2016-09-06T10:31:45.723181: step 7029, loss 0.0156528, acc 1
2016-09-06T10:31:46.523580: step 7030, loss 0.00268026, acc 1
2016-09-06T10:31:47.320712: step 7031, loss 0.0462396, acc 0.98
2016-09-06T10:31:48.150128: step 7032, loss 0.0785324, acc 0.98
2016-09-06T10:31:48.964142: step 7033, loss 0.0359231, acc 0.98
2016-09-06T10:31:49.775018: step 7034, loss 0.0041452, acc 1
2016-09-06T10:31:50.614775: step 7035, loss 0.0169106, acc 1
2016-09-06T10:31:51.423877: step 7036, loss 0.0275549, acc 0.98
2016-09-06T10:31:52.235744: step 7037, loss 0.0339108, acc 0.98
2016-09-06T10:31:53.050436: step 7038, loss 0.0174536, acc 1
2016-09-06T10:31:53.891900: step 7039, loss 0.0215948, acc 1
2016-09-06T10:31:54.715094: step 7040, loss 0.0371624, acc 0.96
2016-09-06T10:31:55.549059: step 7041, loss 0.0658609, acc 0.96
2016-09-06T10:31:56.357220: step 7042, loss 0.0588606, acc 0.96
2016-09-06T10:31:57.169985: step 7043, loss 0.0177479, acc 0.98
2016-09-06T10:31:57.979521: step 7044, loss 0.0210296, acc 1
2016-09-06T10:31:58.791767: step 7045, loss 0.0250282, acc 1
2016-09-06T10:31:59.589985: step 7046, loss 0.0122407, acc 1
2016-09-06T10:32:00.411138: step 7047, loss 0.0620618, acc 0.98
2016-09-06T10:32:01.235178: step 7048, loss 0.00289776, acc 1
2016-09-06T10:32:02.035679: step 7049, loss 0.0390293, acc 0.96
2016-09-06T10:32:02.843341: step 7050, loss 0.0150022, acc 1
2016-09-06T10:32:03.691613: step 7051, loss 0.00260907, acc 1
2016-09-06T10:32:04.511216: step 7052, loss 0.00285036, acc 1
2016-09-06T10:32:05.315375: step 7053, loss 0.0349364, acc 0.96
2016-09-06T10:32:06.136247: step 7054, loss 0.0239408, acc 0.98
2016-09-06T10:32:06.941557: step 7055, loss 0.00518699, acc 1
2016-09-06T10:32:07.746837: step 7056, loss 0.0233278, acc 1
2016-09-06T10:32:08.580221: step 7057, loss 0.0255754, acc 0.98
2016-09-06T10:32:09.394111: step 7058, loss 0.0299918, acc 1
2016-09-06T10:32:10.196494: step 7059, loss 0.0357169, acc 0.96
2016-09-06T10:32:11.039462: step 7060, loss 0.0107211, acc 1
2016-09-06T10:32:11.861398: step 7061, loss 0.02238, acc 0.98
2016-09-06T10:32:12.686033: step 7062, loss 0.0265381, acc 0.98
2016-09-06T10:32:13.509938: step 7063, loss 0.00781609, acc 1
2016-09-06T10:32:14.329706: step 7064, loss 0.00557408, acc 1
2016-09-06T10:32:15.139518: step 7065, loss 0.0256711, acc 0.98
2016-09-06T10:32:15.954285: step 7066, loss 0.00574738, acc 1
2016-09-06T10:32:16.775462: step 7067, loss 0.0255361, acc 0.98
2016-09-06T10:32:17.592415: step 7068, loss 0.00858186, acc 1
2016-09-06T10:32:18.404186: step 7069, loss 0.034388, acc 0.96
2016-09-06T10:32:19.201743: step 7070, loss 0.0133888, acc 1
2016-09-06T10:32:20.013143: step 7071, loss 0.0095768, acc 1
2016-09-06T10:32:20.838668: step 7072, loss 0.0548451, acc 0.96
2016-09-06T10:32:21.679506: step 7073, loss 0.0449213, acc 0.98
2016-09-06T10:32:22.487437: step 7074, loss 0.0060131, acc 1
2016-09-06T10:32:23.302584: step 7075, loss 0.0198497, acc 0.98
2016-09-06T10:32:24.128564: step 7076, loss 0.103534, acc 0.98
2016-09-06T10:32:24.944936: step 7077, loss 0.0334887, acc 0.98
2016-09-06T10:32:25.803538: step 7078, loss 0.0457501, acc 0.98
2016-09-06T10:32:26.676324: step 7079, loss 0.104776, acc 0.98
2016-09-06T10:32:27.507998: step 7080, loss 0.0994282, acc 0.98
2016-09-06T10:32:28.358920: step 7081, loss 0.0394742, acc 0.98
2016-09-06T10:32:29.186325: step 7082, loss 0.00629627, acc 1
2016-09-06T10:32:29.970616: step 7083, loss 0.00308098, acc 1
2016-09-06T10:32:30.814413: step 7084, loss 0.0175445, acc 0.98
2016-09-06T10:32:31.660375: step 7085, loss 0.00360637, acc 1
2016-09-06T10:32:32.463338: step 7086, loss 0.0175774, acc 1
2016-09-06T10:32:33.268585: step 7087, loss 0.0159018, acc 1
2016-09-06T10:32:34.102102: step 7088, loss 0.0163952, acc 1
2016-09-06T10:32:34.935367: step 7089, loss 0.0142527, acc 1
2016-09-06T10:32:35.762244: step 7090, loss 0.0580361, acc 0.96
2016-09-06T10:32:36.560755: step 7091, loss 0.0183056, acc 1
2016-09-06T10:32:37.406997: step 7092, loss 0.0350541, acc 1
2016-09-06T10:32:38.201182: step 7093, loss 0.0056386, acc 1
2016-09-06T10:32:39.009353: step 7094, loss 0.088912, acc 0.96
2016-09-06T10:32:39.841696: step 7095, loss 0.00428861, acc 1
2016-09-06T10:32:40.621410: step 7096, loss 0.056431, acc 0.98
2016-09-06T10:32:41.418367: step 7097, loss 0.0478407, acc 0.98
2016-09-06T10:32:42.233095: step 7098, loss 0.0535807, acc 0.98
2016-09-06T10:32:43.011814: step 7099, loss 0.0139988, acc 1
2016-09-06T10:32:43.833425: step 7100, loss 0.0173738, acc 1

Evaluation:
2016-09-06T10:32:47.591384: step 7100, loss 1.86906, acc 0.757036

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-7100

2016-09-06T10:32:49.500899: step 7101, loss 0.0359806, acc 0.98
2016-09-06T10:32:50.318826: step 7102, loss 0.0263956, acc 0.98
2016-09-06T10:32:51.192795: step 7103, loss 0.0347461, acc 0.98
2016-09-06T10:32:51.940710: step 7104, loss 0.163637, acc 0.954545
2016-09-06T10:32:52.748153: step 7105, loss 0.111225, acc 0.98
2016-09-06T10:32:53.555139: step 7106, loss 0.118475, acc 0.94
2016-09-06T10:32:54.396352: step 7107, loss 0.0237927, acc 0.98
2016-09-06T10:32:55.161002: step 7108, loss 0.0299889, acc 1
2016-09-06T10:32:55.975192: step 7109, loss 0.00960952, acc 1
2016-09-06T10:32:56.809461: step 7110, loss 0.0303206, acc 1
2016-09-06T10:32:57.587756: step 7111, loss 0.0185612, acc 1
2016-09-06T10:32:58.390948: step 7112, loss 0.0110468, acc 1
2016-09-06T10:32:59.239485: step 7113, loss 0.0217161, acc 0.98
2016-09-06T10:33:00.025906: step 7114, loss 0.0506665, acc 0.96
2016-09-06T10:33:00.853125: step 7115, loss 0.0196189, acc 1
2016-09-06T10:33:01.665692: step 7116, loss 0.0285386, acc 1
2016-09-06T10:33:02.429986: step 7117, loss 0.0563598, acc 0.94
2016-09-06T10:33:03.266597: step 7118, loss 0.0174302, acc 0.98
2016-09-06T10:33:04.083800: step 7119, loss 0.041017, acc 0.98
2016-09-06T10:33:04.873099: step 7120, loss 0.00476862, acc 1
2016-09-06T10:33:05.665844: step 7121, loss 0.0154671, acc 1
2016-09-06T10:33:06.498085: step 7122, loss 0.0183556, acc 0.98
2016-09-06T10:33:07.308166: step 7123, loss 0.0194045, acc 1
2016-09-06T10:33:08.105796: step 7124, loss 0.00780477, acc 1
2016-09-06T10:33:08.919670: step 7125, loss 0.0192063, acc 0.98
2016-09-06T10:33:09.713587: step 7126, loss 0.00558984, acc 1
2016-09-06T10:33:10.502005: step 7127, loss 0.00307292, acc 1
2016-09-06T10:33:11.324910: step 7128, loss 0.065665, acc 0.96
2016-09-06T10:33:12.140535: step 7129, loss 0.0237898, acc 0.98
2016-09-06T10:33:12.949110: step 7130, loss 0.0184267, acc 1
2016-09-06T10:33:13.794295: step 7131, loss 0.0404136, acc 0.96
2016-09-06T10:33:14.643331: step 7132, loss 0.00337922, acc 1
2016-09-06T10:33:15.463202: step 7133, loss 0.0281758, acc 1
2016-09-06T10:33:16.297345: step 7134, loss 0.051562, acc 0.98
2016-09-06T10:33:17.099539: step 7135, loss 0.00654367, acc 1
2016-09-06T10:33:17.922936: step 7136, loss 0.0231282, acc 0.98
2016-09-06T10:33:18.769904: step 7137, loss 0.00880186, acc 1
2016-09-06T10:33:19.600107: step 7138, loss 0.034037, acc 0.98
2016-09-06T10:33:20.442170: step 7139, loss 0.011789, acc 1
2016-09-06T10:33:21.278801: step 7140, loss 0.0115157, acc 1
2016-09-06T10:33:22.067407: step 7141, loss 0.0302677, acc 0.98
2016-09-06T10:33:22.865470: step 7142, loss 0.024116, acc 0.98
2016-09-06T10:33:23.701272: step 7143, loss 0.0203331, acc 0.98
2016-09-06T10:33:24.564933: step 7144, loss 0.0160273, acc 1
2016-09-06T10:33:25.371647: step 7145, loss 0.0236265, acc 0.98
2016-09-06T10:33:26.170538: step 7146, loss 0.0224395, acc 1
2016-09-06T10:33:26.987097: step 7147, loss 0.0193168, acc 0.98
2016-09-06T10:33:27.760215: step 7148, loss 0.0748611, acc 0.98
2016-09-06T10:33:28.562187: step 7149, loss 0.0115482, acc 1
2016-09-06T10:33:29.395975: step 7150, loss 0.027051, acc 0.98
2016-09-06T10:33:30.187647: step 7151, loss 0.00625757, acc 1
2016-09-06T10:33:31.024998: step 7152, loss 0.0357693, acc 0.98
2016-09-06T10:33:31.846338: step 7153, loss 0.0664681, acc 0.98
2016-09-06T10:33:32.636090: step 7154, loss 0.00273907, acc 1
2016-09-06T10:33:33.446304: step 7155, loss 0.0123933, acc 1
2016-09-06T10:33:34.250475: step 7156, loss 0.04437, acc 0.96
2016-09-06T10:33:35.019123: step 7157, loss 0.0173576, acc 1
2016-09-06T10:33:35.810900: step 7158, loss 0.0122888, acc 1
2016-09-06T10:33:36.634755: step 7159, loss 0.0441013, acc 0.98
2016-09-06T10:33:37.421849: step 7160, loss 0.0158276, acc 1
2016-09-06T10:33:38.228677: step 7161, loss 0.0130449, acc 1
2016-09-06T10:33:39.040814: step 7162, loss 0.0127353, acc 1
2016-09-06T10:33:39.835510: step 7163, loss 0.0157167, acc 1
2016-09-06T10:33:40.627727: step 7164, loss 0.0149462, acc 1
2016-09-06T10:33:41.459021: step 7165, loss 0.029602, acc 1
2016-09-06T10:33:42.252455: step 7166, loss 0.0115836, acc 1
2016-09-06T10:33:43.072705: step 7167, loss 0.00894079, acc 1
2016-09-06T10:33:43.905538: step 7168, loss 0.00503713, acc 1
2016-09-06T10:33:44.710648: step 7169, loss 0.0187532, acc 0.98
2016-09-06T10:33:45.503813: step 7170, loss 0.0588984, acc 0.96
2016-09-06T10:33:46.296998: step 7171, loss 0.0249427, acc 0.98
2016-09-06T10:33:47.087114: step 7172, loss 0.0816017, acc 0.94
2016-09-06T10:33:47.900466: step 7173, loss 0.024986, acc 1
2016-09-06T10:33:48.728490: step 7174, loss 0.0229388, acc 1
2016-09-06T10:33:49.505778: step 7175, loss 0.0199069, acc 0.98
2016-09-06T10:33:50.316477: step 7176, loss 0.0124354, acc 1
2016-09-06T10:33:51.147489: step 7177, loss 0.0343754, acc 0.98
2016-09-06T10:33:51.932294: step 7178, loss 0.0146302, acc 1
2016-09-06T10:33:52.728139: step 7179, loss 0.0239233, acc 1
2016-09-06T10:33:53.550129: step 7180, loss 0.0165885, acc 1
2016-09-06T10:33:54.357030: step 7181, loss 0.0168577, acc 0.98
2016-09-06T10:33:55.146379: step 7182, loss 0.0206235, acc 0.98
2016-09-06T10:33:55.969862: step 7183, loss 0.0370898, acc 0.96
2016-09-06T10:33:56.758041: step 7184, loss 0.0276001, acc 1
2016-09-06T10:33:57.583382: step 7185, loss 0.00699395, acc 1
2016-09-06T10:33:58.443712: step 7186, loss 0.0036876, acc 1
2016-09-06T10:33:59.241335: step 7187, loss 0.0118315, acc 1
2016-09-06T10:34:00.064219: step 7188, loss 0.00860628, acc 1
2016-09-06T10:34:00.945446: step 7189, loss 0.0168681, acc 1
2016-09-06T10:34:01.727618: step 7190, loss 0.0307233, acc 0.98
2016-09-06T10:34:02.529346: step 7191, loss 0.00254167, acc 1
2016-09-06T10:34:03.362032: step 7192, loss 0.0105985, acc 1
2016-09-06T10:34:04.186974: step 7193, loss 0.0148942, acc 1
2016-09-06T10:34:05.009529: step 7194, loss 0.0169699, acc 1
2016-09-06T10:34:05.876146: step 7195, loss 0.00377344, acc 1
2016-09-06T10:34:06.726415: step 7196, loss 0.00264186, acc 1
2016-09-06T10:34:07.518983: step 7197, loss 0.0170908, acc 1
2016-09-06T10:34:08.337876: step 7198, loss 0.00364077, acc 1
2016-09-06T10:34:09.166437: step 7199, loss 0.00981811, acc 1
2016-09-06T10:34:09.992736: step 7200, loss 0.00469491, acc 1

Evaluation:
2016-09-06T10:34:13.773384: step 7200, loss 2.33511, acc 0.757036

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-7200

2016-09-06T10:34:15.735977: step 7201, loss 0.00765552, acc 1
2016-09-06T10:34:16.556855: step 7202, loss 0.112342, acc 0.98
2016-09-06T10:34:17.358036: step 7203, loss 0.0279457, acc 0.98
2016-09-06T10:34:18.195195: step 7204, loss 0.013863, acc 1
2016-09-06T10:34:19.028549: step 7205, loss 0.0273153, acc 0.98
2016-09-06T10:34:19.822785: step 7206, loss 0.00844754, acc 1
2016-09-06T10:34:20.632290: step 7207, loss 0.00305854, acc 1
2016-09-06T10:34:21.434805: step 7208, loss 0.00269342, acc 1
2016-09-06T10:34:22.214745: step 7209, loss 0.11709, acc 0.92
2016-09-06T10:34:23.010761: step 7210, loss 0.0508084, acc 0.96
2016-09-06T10:34:23.848714: step 7211, loss 0.0686819, acc 0.96
2016-09-06T10:34:24.641740: step 7212, loss 0.00236338, acc 1
2016-09-06T10:34:25.445417: step 7213, loss 0.0297657, acc 1
2016-09-06T10:34:26.275773: step 7214, loss 0.0376445, acc 0.96
2016-09-06T10:34:27.042024: step 7215, loss 0.0053035, acc 1
2016-09-06T10:34:27.838550: step 7216, loss 0.0403225, acc 0.98
2016-09-06T10:34:28.661573: step 7217, loss 0.0141014, acc 1
2016-09-06T10:34:29.428268: step 7218, loss 0.0446626, acc 0.98
2016-09-06T10:34:30.265430: step 7219, loss 0.00948894, acc 1
2016-09-06T10:34:31.091822: step 7220, loss 0.0182465, acc 1
2016-09-06T10:34:31.871179: step 7221, loss 0.00887386, acc 1
2016-09-06T10:34:32.705597: step 7222, loss 0.00234243, acc 1
2016-09-06T10:34:33.523372: step 7223, loss 0.0187418, acc 0.98
2016-09-06T10:34:34.308592: step 7224, loss 0.0421816, acc 0.98
2016-09-06T10:34:35.088072: step 7225, loss 0.0144554, acc 1
2016-09-06T10:34:35.869467: step 7226, loss 0.00769525, acc 1
2016-09-06T10:34:36.677453: step 7227, loss 0.0021874, acc 1
2016-09-06T10:34:37.509152: step 7228, loss 0.0253515, acc 1
2016-09-06T10:34:38.338034: step 7229, loss 0.0536373, acc 0.98
2016-09-06T10:34:39.114653: step 7230, loss 0.00304528, acc 1
2016-09-06T10:34:39.910034: step 7231, loss 0.0223835, acc 1
2016-09-06T10:34:40.710627: step 7232, loss 0.0126838, acc 1
2016-09-06T10:34:41.495689: step 7233, loss 0.0373377, acc 0.98
2016-09-06T10:34:42.302680: step 7234, loss 0.0139614, acc 1
2016-09-06T10:34:43.094054: step 7235, loss 0.0193553, acc 1
2016-09-06T10:34:43.885413: step 7236, loss 0.0601616, acc 0.96
2016-09-06T10:34:44.710289: step 7237, loss 0.00227214, acc 1
2016-09-06T10:34:45.517513: step 7238, loss 0.0905774, acc 0.98
2016-09-06T10:34:46.345254: step 7239, loss 0.0272295, acc 1
2016-09-06T10:34:47.150433: step 7240, loss 0.0029476, acc 1
2016-09-06T10:34:47.978173: step 7241, loss 0.00868064, acc 1
2016-09-06T10:34:48.775145: step 7242, loss 0.0382563, acc 1
2016-09-06T10:34:49.571636: step 7243, loss 0.0340697, acc 0.98
2016-09-06T10:34:50.395166: step 7244, loss 0.00226991, acc 1
2016-09-06T10:34:51.205692: step 7245, loss 0.0936973, acc 0.92
2016-09-06T10:34:52.009652: step 7246, loss 0.00820751, acc 1
2016-09-06T10:34:52.856403: step 7247, loss 0.0299385, acc 0.98
2016-09-06T10:34:53.628871: step 7248, loss 0.055417, acc 0.98
2016-09-06T10:34:54.415511: step 7249, loss 0.00594496, acc 1
2016-09-06T10:34:55.267129: step 7250, loss 0.0247416, acc 0.98
2016-09-06T10:34:56.102941: step 7251, loss 0.0390338, acc 0.98
2016-09-06T10:34:56.908145: step 7252, loss 0.00570173, acc 1
2016-09-06T10:34:57.757901: step 7253, loss 0.00606291, acc 1
2016-09-06T10:34:58.601400: step 7254, loss 0.015021, acc 1
2016-09-06T10:34:59.417039: step 7255, loss 0.0267111, acc 1
2016-09-06T10:35:00.266937: step 7256, loss 0.00238197, acc 1
2016-09-06T10:35:01.094617: step 7257, loss 0.0236096, acc 0.98
2016-09-06T10:35:01.925102: step 7258, loss 0.0228633, acc 1
2016-09-06T10:35:02.735638: step 7259, loss 0.00296833, acc 1
2016-09-06T10:35:03.539678: step 7260, loss 0.0111313, acc 1
2016-09-06T10:35:04.341158: step 7261, loss 0.0122978, acc 1
2016-09-06T10:35:05.174593: step 7262, loss 0.0218773, acc 1
2016-09-06T10:35:05.985698: step 7263, loss 0.0052052, acc 1
2016-09-06T10:35:06.796342: step 7264, loss 0.0852574, acc 0.92
2016-09-06T10:35:07.654631: step 7265, loss 0.0249286, acc 1
2016-09-06T10:35:08.470291: step 7266, loss 0.0985414, acc 0.98
2016-09-06T10:35:09.270578: step 7267, loss 0.00630763, acc 1
2016-09-06T10:35:10.083775: step 7268, loss 0.0121063, acc 1
2016-09-06T10:35:10.928765: step 7269, loss 0.00853929, acc 1
2016-09-06T10:35:11.745650: step 7270, loss 0.0397509, acc 0.96
2016-09-06T10:35:12.577614: step 7271, loss 0.0122581, acc 1
2016-09-06T10:35:13.389781: step 7272, loss 0.0428468, acc 0.98
2016-09-06T10:35:14.182178: step 7273, loss 0.150594, acc 0.94
2016-09-06T10:35:14.985346: step 7274, loss 0.0184062, acc 1
2016-09-06T10:35:15.817088: step 7275, loss 0.0226957, acc 1
2016-09-06T10:35:16.601872: step 7276, loss 0.0225407, acc 0.98
2016-09-06T10:35:17.415885: step 7277, loss 0.00670858, acc 1
2016-09-06T10:35:18.214896: step 7278, loss 0.0038067, acc 1
2016-09-06T10:35:18.999585: step 7279, loss 0.0469848, acc 0.98
2016-09-06T10:35:19.826978: step 7280, loss 0.0060013, acc 1
2016-09-06T10:35:20.631173: step 7281, loss 0.0323039, acc 0.98
2016-09-06T10:35:21.414410: step 7282, loss 0.0685041, acc 0.96
2016-09-06T10:35:22.257845: step 7283, loss 0.00949851, acc 1
2016-09-06T10:35:23.033164: step 7284, loss 0.020804, acc 0.98
2016-09-06T10:35:23.825806: step 7285, loss 0.0477353, acc 0.98
2016-09-06T10:35:24.641336: step 7286, loss 0.0442842, acc 0.98
2016-09-06T10:35:25.445616: step 7287, loss 0.00440754, acc 1
2016-09-06T10:35:26.237800: step 7288, loss 0.0120532, acc 1
2016-09-06T10:35:27.045796: step 7289, loss 0.0133753, acc 1
2016-09-06T10:35:27.907274: step 7290, loss 0.183585, acc 0.98
2016-09-06T10:35:28.700295: step 7291, loss 0.0396329, acc 1
2016-09-06T10:35:29.502263: step 7292, loss 0.0166996, acc 1
2016-09-06T10:35:30.317953: step 7293, loss 0.0290683, acc 0.98
2016-09-06T10:35:31.103298: step 7294, loss 0.0250387, acc 0.98
2016-09-06T10:35:31.888819: step 7295, loss 0.0132038, acc 1
2016-09-06T10:35:32.651401: step 7296, loss 0.0115022, acc 1
2016-09-06T10:35:33.459476: step 7297, loss 0.0371403, acc 0.98
2016-09-06T10:35:34.258667: step 7298, loss 0.0332122, acc 0.98
2016-09-06T10:35:35.095265: step 7299, loss 0.0194397, acc 0.98
2016-09-06T10:35:35.899283: step 7300, loss 0.117192, acc 0.98

Evaluation:
2016-09-06T10:35:39.645466: step 7300, loss 2.74312, acc 0.751407

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-7300

2016-09-06T10:35:41.522603: step 7301, loss 0.00659545, acc 1
2016-09-06T10:35:42.328458: step 7302, loss 0.0603235, acc 0.96
2016-09-06T10:35:43.145753: step 7303, loss 0.00842698, acc 1
2016-09-06T10:35:43.959964: step 7304, loss 0.0388212, acc 0.98
2016-09-06T10:35:44.785957: step 7305, loss 0.0639435, acc 0.96
2016-09-06T10:35:45.573187: step 7306, loss 0.0421225, acc 0.98
2016-09-06T10:35:46.397135: step 7307, loss 0.0308376, acc 0.98
2016-09-06T10:35:47.197953: step 7308, loss 0.0127537, acc 1
2016-09-06T10:35:47.995277: step 7309, loss 0.0221042, acc 0.98
2016-09-06T10:35:48.814606: step 7310, loss 0.0173371, acc 1
2016-09-06T10:35:49.660037: step 7311, loss 0.0322978, acc 0.98
2016-09-06T10:35:50.435221: step 7312, loss 0.0420849, acc 0.98
2016-09-06T10:35:51.295040: step 7313, loss 0.0173794, acc 1
2016-09-06T10:35:52.166023: step 7314, loss 0.00537623, acc 1
2016-09-06T10:35:52.995707: step 7315, loss 0.0601373, acc 0.98
2016-09-06T10:35:53.814702: step 7316, loss 0.00454182, acc 1
2016-09-06T10:35:54.639961: step 7317, loss 0.00708837, acc 1
2016-09-06T10:35:55.489592: step 7318, loss 0.0257971, acc 0.98
2016-09-06T10:35:56.312416: step 7319, loss 0.00591937, acc 1
2016-09-06T10:35:57.170003: step 7320, loss 0.00527845, acc 1
2016-09-06T10:35:58.006038: step 7321, loss 0.0044788, acc 1
2016-09-06T10:35:58.827853: step 7322, loss 0.0237823, acc 1
2016-09-06T10:35:59.636651: step 7323, loss 0.0357915, acc 0.98
2016-09-06T10:36:00.465080: step 7324, loss 0.0182292, acc 1
2016-09-06T10:36:01.298623: step 7325, loss 0.0111651, acc 1
2016-09-06T10:36:02.100604: step 7326, loss 0.0115955, acc 1
2016-09-06T10:36:02.929635: step 7327, loss 0.00626411, acc 1
2016-09-06T10:36:03.733806: step 7328, loss 0.0405517, acc 0.96
2016-09-06T10:36:04.570952: step 7329, loss 0.207065, acc 0.98
2016-09-06T10:36:05.388025: step 7330, loss 0.0182549, acc 0.98
2016-09-06T10:36:06.169142: step 7331, loss 0.0062115, acc 1
2016-09-06T10:36:06.982915: step 7332, loss 0.00434207, acc 1
2016-09-06T10:36:07.781420: step 7333, loss 0.0210637, acc 0.98
2016-09-06T10:36:08.593016: step 7334, loss 0.0172097, acc 1
2016-09-06T10:36:09.389453: step 7335, loss 0.0171885, acc 1
2016-09-06T10:36:10.221885: step 7336, loss 0.0245774, acc 0.98
2016-09-06T10:36:11.025091: step 7337, loss 0.00387443, acc 1
2016-09-06T10:36:11.824974: step 7338, loss 0.0105887, acc 1
2016-09-06T10:36:12.645091: step 7339, loss 0.00761938, acc 1
2016-09-06T10:36:13.433475: step 7340, loss 0.00549445, acc 1
2016-09-06T10:36:14.239485: step 7341, loss 0.00350481, acc 1
2016-09-06T10:36:15.059470: step 7342, loss 0.00369474, acc 1
2016-09-06T10:36:15.858506: step 7343, loss 0.0248921, acc 0.98
2016-09-06T10:36:16.654878: step 7344, loss 0.00724127, acc 1
2016-09-06T10:36:17.491861: step 7345, loss 0.0133076, acc 1
2016-09-06T10:36:18.305331: step 7346, loss 0.0365462, acc 0.98
2016-09-06T10:36:19.120006: step 7347, loss 0.0188962, acc 1
2016-09-06T10:36:19.949776: step 7348, loss 0.120538, acc 0.94
2016-09-06T10:36:20.746411: step 7349, loss 0.00451113, acc 1
2016-09-06T10:36:21.595158: step 7350, loss 0.0160928, acc 1
2016-09-06T10:36:22.419105: step 7351, loss 0.06551, acc 0.98
2016-09-06T10:36:23.215268: step 7352, loss 0.00812909, acc 1
2016-09-06T10:36:24.025590: step 7353, loss 0.00434498, acc 1
2016-09-06T10:36:24.845675: step 7354, loss 0.0221291, acc 1
2016-09-06T10:36:25.642581: step 7355, loss 0.021079, acc 0.98
2016-09-06T10:36:26.487586: step 7356, loss 0.0114238, acc 1
2016-09-06T10:36:27.344736: step 7357, loss 0.031932, acc 1
2016-09-06T10:36:28.184648: step 7358, loss 0.0366195, acc 0.98
2016-09-06T10:36:29.011022: step 7359, loss 0.0994154, acc 0.98
2016-09-06T10:36:29.864959: step 7360, loss 0.0097766, acc 1
2016-09-06T10:36:30.660651: step 7361, loss 0.00589877, acc 1
2016-09-06T10:36:31.480315: step 7362, loss 0.018759, acc 0.98
2016-09-06T10:36:32.300779: step 7363, loss 0.0429156, acc 0.96
2016-09-06T10:36:33.122558: step 7364, loss 0.076868, acc 0.96
2016-09-06T10:36:33.955022: step 7365, loss 0.0220956, acc 0.98
2016-09-06T10:36:34.740868: step 7366, loss 0.0296802, acc 0.98
2016-09-06T10:36:35.578502: step 7367, loss 0.00520062, acc 1
2016-09-06T10:36:36.360000: step 7368, loss 0.0231221, acc 1
2016-09-06T10:36:37.158305: step 7369, loss 0.00390847, acc 1
2016-09-06T10:36:37.981932: step 7370, loss 0.0309857, acc 1
2016-09-06T10:36:38.769899: step 7371, loss 0.0703837, acc 0.98
2016-09-06T10:36:39.582207: step 7372, loss 0.0397424, acc 0.96
2016-09-06T10:36:40.419669: step 7373, loss 0.0128103, acc 1
2016-09-06T10:36:41.189316: step 7374, loss 0.0326512, acc 0.98
2016-09-06T10:36:42.032518: step 7375, loss 0.0406592, acc 0.98
2016-09-06T10:36:42.841632: step 7376, loss 0.00287654, acc 1
2016-09-06T10:36:43.636963: step 7377, loss 0.0146327, acc 1
2016-09-06T10:36:44.432686: step 7378, loss 0.00620718, acc 1
2016-09-06T10:36:45.279195: step 7379, loss 0.0276512, acc 0.98
2016-09-06T10:36:46.074848: step 7380, loss 0.00315676, acc 1
2016-09-06T10:36:46.890328: step 7381, loss 0.00420785, acc 1
2016-09-06T10:36:47.712522: step 7382, loss 0.0239147, acc 1
2016-09-06T10:36:48.520713: step 7383, loss 0.00329773, acc 1
2016-09-06T10:36:49.320367: step 7384, loss 0.0149181, acc 1
2016-09-06T10:36:50.144262: step 7385, loss 0.00857791, acc 1
2016-09-06T10:36:50.966987: step 7386, loss 0.0140402, acc 1
2016-09-06T10:36:51.780624: step 7387, loss 0.0228923, acc 1
2016-09-06T10:36:52.633837: step 7388, loss 0.00855473, acc 1
2016-09-06T10:36:53.438478: step 7389, loss 0.0411923, acc 0.98
2016-09-06T10:36:54.256926: step 7390, loss 0.0360171, acc 0.98
2016-09-06T10:36:55.090327: step 7391, loss 0.00930976, acc 1
2016-09-06T10:36:55.908021: step 7392, loss 0.0171389, acc 0.98
2016-09-06T10:36:56.726010: step 7393, loss 0.00544238, acc 1
2016-09-06T10:36:57.588455: step 7394, loss 0.0651149, acc 0.96
2016-09-06T10:36:58.404280: step 7395, loss 0.0155555, acc 1
2016-09-06T10:36:59.199836: step 7396, loss 0.0284157, acc 1
2016-09-06T10:37:00.043882: step 7397, loss 0.0137647, acc 1
2016-09-06T10:37:00.897278: step 7398, loss 0.01358, acc 1
2016-09-06T10:37:01.712125: step 7399, loss 0.0212422, acc 0.98
2016-09-06T10:37:02.533077: step 7400, loss 0.00629031, acc 1

Evaluation:
2016-09-06T10:37:06.332501: step 7400, loss 1.79844, acc 0.762664

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-7400

2016-09-06T10:37:08.283152: step 7401, loss 0.0307692, acc 0.98
2016-09-06T10:37:09.110251: step 7402, loss 0.11492, acc 0.96
2016-09-06T10:37:09.944723: step 7403, loss 0.0388568, acc 0.96
2016-09-06T10:37:10.769442: step 7404, loss 0.159381, acc 0.98
2016-09-06T10:37:11.569572: step 7405, loss 0.00897553, acc 1
2016-09-06T10:37:12.340873: step 7406, loss 0.0109013, acc 1
2016-09-06T10:37:13.132397: step 7407, loss 0.0185039, acc 1
2016-09-06T10:37:13.919637: step 7408, loss 0.0694277, acc 0.94
2016-09-06T10:37:14.734964: step 7409, loss 0.00708758, acc 1
2016-09-06T10:37:15.537136: step 7410, loss 0.00357545, acc 1
2016-09-06T10:37:16.346067: step 7411, loss 0.0299046, acc 0.98
2016-09-06T10:37:17.152575: step 7412, loss 0.00480082, acc 1
2016-09-06T10:37:17.965994: step 7413, loss 0.0205571, acc 1
2016-09-06T10:37:18.743127: step 7414, loss 0.00314405, acc 1
2016-09-06T10:37:19.563777: step 7415, loss 0.129835, acc 0.94
2016-09-06T10:37:20.387767: step 7416, loss 0.00913169, acc 1
2016-09-06T10:37:21.171926: step 7417, loss 0.00804262, acc 1
2016-09-06T10:37:22.021441: step 7418, loss 0.0142339, acc 1
2016-09-06T10:37:22.841930: step 7419, loss 0.0174713, acc 1
2016-09-06T10:37:23.641578: step 7420, loss 0.0328588, acc 0.98
2016-09-06T10:37:24.477795: step 7421, loss 0.0487056, acc 0.96
2016-09-06T10:37:25.279749: step 7422, loss 0.0780832, acc 0.98
2016-09-06T10:37:26.044447: step 7423, loss 0.0246827, acc 1
2016-09-06T10:37:26.846524: step 7424, loss 0.0222179, acc 1
2016-09-06T10:37:27.693395: step 7425, loss 0.00297223, acc 1
2016-09-06T10:37:28.470603: step 7426, loss 0.0228252, acc 0.98
2016-09-06T10:37:29.275240: step 7427, loss 0.0172729, acc 1
2016-09-06T10:37:30.125794: step 7428, loss 0.00583853, acc 1
2016-09-06T10:37:30.915828: step 7429, loss 0.0454852, acc 0.98
2016-09-06T10:37:31.729267: step 7430, loss 0.0360682, acc 1
2016-09-06T10:37:32.528714: step 7431, loss 0.0509569, acc 0.96
2016-09-06T10:37:33.309404: step 7432, loss 0.0329266, acc 0.98
2016-09-06T10:37:34.093188: step 7433, loss 0.0215425, acc 1
2016-09-06T10:37:34.898946: step 7434, loss 0.107272, acc 0.96
2016-09-06T10:37:35.693662: step 7435, loss 0.00434281, acc 1
2016-09-06T10:37:36.521396: step 7436, loss 0.0166736, acc 0.98
2016-09-06T10:37:37.337424: step 7437, loss 0.039355, acc 0.98
2016-09-06T10:37:38.122913: step 7438, loss 0.00597908, acc 1
2016-09-06T10:37:38.933660: step 7439, loss 0.0178714, acc 1
2016-09-06T10:37:39.740938: step 7440, loss 0.0121278, acc 1
2016-09-06T10:37:40.538673: step 7441, loss 0.00372846, acc 1
2016-09-06T10:37:41.356090: step 7442, loss 0.0255095, acc 0.98
2016-09-06T10:37:42.156574: step 7443, loss 0.0393016, acc 0.98
2016-09-06T10:37:42.951821: step 7444, loss 0.0228838, acc 0.98
2016-09-06T10:37:43.759885: step 7445, loss 0.0191445, acc 1
2016-09-06T10:37:44.606025: step 7446, loss 0.0346572, acc 0.96
2016-09-06T10:37:45.426389: step 7447, loss 0.0268131, acc 0.98
2016-09-06T10:37:46.224936: step 7448, loss 0.0130967, acc 1
2016-09-06T10:37:47.035303: step 7449, loss 0.00321827, acc 1
2016-09-06T10:37:47.813540: step 7450, loss 0.00774868, acc 1
2016-09-06T10:37:48.587517: step 7451, loss 0.00349967, acc 1
2016-09-06T10:37:49.390862: step 7452, loss 0.037005, acc 1
2016-09-06T10:37:50.177425: step 7453, loss 0.0138114, acc 1
2016-09-06T10:37:50.979861: step 7454, loss 0.0114824, acc 1
2016-09-06T10:37:51.787508: step 7455, loss 0.0241355, acc 1
2016-09-06T10:37:52.569957: step 7456, loss 0.0371171, acc 1
2016-09-06T10:37:53.395873: step 7457, loss 0.0488534, acc 0.96
2016-09-06T10:37:54.210828: step 7458, loss 0.0569584, acc 0.94
2016-09-06T10:37:55.053983: step 7459, loss 0.00739452, acc 1
2016-09-06T10:37:55.853687: step 7460, loss 0.0166323, acc 1
2016-09-06T10:37:56.670145: step 7461, loss 0.00519153, acc 1
2016-09-06T10:37:57.470281: step 7462, loss 0.0160863, acc 1
2016-09-06T10:37:58.279859: step 7463, loss 0.00346172, acc 1
2016-09-06T10:37:59.075945: step 7464, loss 0.00968948, acc 1
2016-09-06T10:37:59.852157: step 7465, loss 0.0323146, acc 0.98
2016-09-06T10:38:00.670856: step 7466, loss 0.00768391, acc 1
2016-09-06T10:38:01.501534: step 7467, loss 0.0775807, acc 0.98
2016-09-06T10:38:02.289521: step 7468, loss 0.102771, acc 0.96
2016-09-06T10:38:03.119176: step 7469, loss 0.100641, acc 0.98
2016-09-06T10:38:03.941612: step 7470, loss 0.0357721, acc 0.96
2016-09-06T10:38:04.731887: step 7471, loss 0.0281544, acc 1
2016-09-06T10:38:05.531123: step 7472, loss 0.00312505, acc 1
2016-09-06T10:38:06.369421: step 7473, loss 0.049155, acc 0.96
2016-09-06T10:38:07.148231: step 7474, loss 0.101529, acc 0.96
2016-09-06T10:38:07.973413: step 7475, loss 0.0129477, acc 1
2016-09-06T10:38:08.800382: step 7476, loss 0.0397364, acc 1
2016-09-06T10:38:09.564246: step 7477, loss 0.017305, acc 1
2016-09-06T10:38:10.355344: step 7478, loss 0.0371312, acc 0.98
2016-09-06T10:38:11.153422: step 7479, loss 0.00354221, acc 1
2016-09-06T10:38:11.962563: step 7480, loss 0.00600823, acc 1
2016-09-06T10:38:12.767616: step 7481, loss 0.0507178, acc 0.98
2016-09-06T10:38:13.591321: step 7482, loss 0.0294941, acc 0.98
2016-09-06T10:38:14.397686: step 7483, loss 0.0457098, acc 0.98
2016-09-06T10:38:15.212807: step 7484, loss 0.201094, acc 0.98
2016-09-06T10:38:16.048246: step 7485, loss 0.0682959, acc 0.96
2016-09-06T10:38:16.830752: step 7486, loss 0.0472359, acc 0.98
2016-09-06T10:38:17.677709: step 7487, loss 0.016659, acc 1
2016-09-06T10:38:18.423339: step 7488, loss 0.00903021, acc 1
2016-09-06T10:38:19.213444: step 7489, loss 0.0188182, acc 1
2016-09-06T10:38:20.036196: step 7490, loss 0.0379006, acc 0.98
2016-09-06T10:38:20.839518: step 7491, loss 0.0140358, acc 1
2016-09-06T10:38:21.631757: step 7492, loss 0.0398783, acc 0.98
2016-09-06T10:38:22.428156: step 7493, loss 0.0357486, acc 0.98
2016-09-06T10:38:23.247407: step 7494, loss 0.0210671, acc 1
2016-09-06T10:38:24.041810: step 7495, loss 0.0273749, acc 1
2016-09-06T10:38:24.840461: step 7496, loss 0.0377481, acc 0.98
2016-09-06T10:38:25.668240: step 7497, loss 0.0284041, acc 1
2016-09-06T10:38:26.456245: step 7498, loss 0.0236848, acc 0.98
2016-09-06T10:38:27.256941: step 7499, loss 0.0145005, acc 1
2016-09-06T10:38:28.106910: step 7500, loss 0.0132061, acc 1

Evaluation:
2016-09-06T10:38:31.852497: step 7500, loss 1.46202, acc 0.763602

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-7500

2016-09-06T10:38:33.704785: step 7501, loss 0.0136541, acc 1
2016-09-06T10:38:34.511099: step 7502, loss 0.0478524, acc 0.96
2016-09-06T10:38:35.311816: step 7503, loss 0.0533788, acc 0.98
2016-09-06T10:38:36.128045: step 7504, loss 0.0783128, acc 0.94
2016-09-06T10:38:36.980911: step 7505, loss 0.0205898, acc 1
2016-09-06T10:38:37.820169: step 7506, loss 0.0259767, acc 0.98
2016-09-06T10:38:38.613400: step 7507, loss 0.00449431, acc 1
2016-09-06T10:38:39.419287: step 7508, loss 0.00354465, acc 1
2016-09-06T10:38:40.236495: step 7509, loss 0.0110969, acc 1
2016-09-06T10:38:41.051114: step 7510, loss 0.00309265, acc 1
2016-09-06T10:38:41.879826: step 7511, loss 0.00309976, acc 1
2016-09-06T10:38:42.707497: step 7512, loss 0.0223985, acc 0.98
2016-09-06T10:38:43.474114: step 7513, loss 0.0032491, acc 1
2016-09-06T10:38:44.289593: step 7514, loss 0.00602582, acc 1
2016-09-06T10:38:45.110780: step 7515, loss 0.0286076, acc 0.98
2016-09-06T10:38:45.902193: step 7516, loss 0.0214209, acc 1
2016-09-06T10:38:46.699858: step 7517, loss 0.00324336, acc 1
2016-09-06T10:38:47.509132: step 7518, loss 0.0545482, acc 0.98
2016-09-06T10:38:48.290074: step 7519, loss 0.00318968, acc 1
2016-09-06T10:38:49.111015: step 7520, loss 0.0179933, acc 1
2016-09-06T10:38:49.917693: step 7521, loss 0.00368178, acc 1
2016-09-06T10:38:50.717292: step 7522, loss 0.00388161, acc 1
2016-09-06T10:38:51.509318: step 7523, loss 0.0377916, acc 0.98
2016-09-06T10:38:52.321444: step 7524, loss 0.00764937, acc 1
2016-09-06T10:38:53.122518: step 7525, loss 0.00456567, acc 1
2016-09-06T10:38:53.943856: step 7526, loss 0.00470871, acc 1
2016-09-06T10:38:54.762202: step 7527, loss 0.0408298, acc 0.98
2016-09-06T10:38:55.561410: step 7528, loss 0.00337945, acc 1
2016-09-06T10:38:56.369963: step 7529, loss 0.0315869, acc 0.98
2016-09-06T10:38:57.197185: step 7530, loss 0.00314105, acc 1
2016-09-06T10:38:57.988169: step 7531, loss 0.0163667, acc 1
2016-09-06T10:38:58.800151: step 7532, loss 0.0130153, acc 1
2016-09-06T10:38:59.606106: step 7533, loss 0.0128504, acc 1
2016-09-06T10:39:00.418267: step 7534, loss 0.0112828, acc 1
2016-09-06T10:39:01.214217: step 7535, loss 0.0749485, acc 0.98
2016-09-06T10:39:02.032033: step 7536, loss 0.0188037, acc 1
2016-09-06T10:39:02.810404: step 7537, loss 0.0340277, acc 0.98
2016-09-06T10:39:03.625415: step 7538, loss 0.0160605, acc 1
2016-09-06T10:39:04.924510: step 7539, loss 0.0173795, acc 1
2016-09-06T10:39:05.738636: step 7540, loss 0.00684506, acc 1
2016-09-06T10:39:06.516862: step 7541, loss 0.031664, acc 0.96
2016-09-06T10:39:07.318418: step 7542, loss 0.00340289, acc 1
2016-09-06T10:39:08.138833: step 7543, loss 0.0178427, acc 1
2016-09-06T10:39:08.944779: step 7544, loss 0.0251509, acc 0.98
2016-09-06T10:39:09.754383: step 7545, loss 0.0252329, acc 1
2016-09-06T10:39:10.564121: step 7546, loss 0.0179129, acc 1
2016-09-06T10:39:11.367093: step 7547, loss 0.00712846, acc 1
2016-09-06T10:39:12.163022: step 7548, loss 0.00272756, acc 1
2016-09-06T10:39:12.981926: step 7549, loss 0.0315363, acc 1
2016-09-06T10:39:13.772881: step 7550, loss 0.00676749, acc 1
2016-09-06T10:39:14.562804: step 7551, loss 0.0037161, acc 1
2016-09-06T10:39:15.366572: step 7552, loss 0.0178498, acc 0.98
2016-09-06T10:39:16.142367: step 7553, loss 0.012118, acc 1
2016-09-06T10:39:16.945466: step 7554, loss 0.0217562, acc 0.98
2016-09-06T10:39:17.744809: step 7555, loss 0.0583336, acc 0.96
2016-09-06T10:39:18.569562: step 7556, loss 0.020194, acc 1
2016-09-06T10:39:19.396381: step 7557, loss 0.0158004, acc 1
2016-09-06T10:39:20.239157: step 7558, loss 0.0470918, acc 0.98
2016-09-06T10:39:21.035416: step 7559, loss 0.0433324, acc 0.96
2016-09-06T10:39:21.844080: step 7560, loss 0.0176525, acc 1
2016-09-06T10:39:22.713442: step 7561, loss 0.0204559, acc 0.98
2016-09-06T10:39:23.521227: step 7562, loss 0.0056101, acc 1
2016-09-06T10:39:24.325734: step 7563, loss 0.0344406, acc 0.98
2016-09-06T10:39:25.186340: step 7564, loss 0.023525, acc 0.98
2016-09-06T10:39:26.003008: step 7565, loss 0.00908824, acc 1
2016-09-06T10:39:26.819683: step 7566, loss 0.00783898, acc 1
2016-09-06T10:39:27.672133: step 7567, loss 0.0107029, acc 1
2016-09-06T10:39:28.486744: step 7568, loss 0.0208302, acc 1
2016-09-06T10:39:29.298025: step 7569, loss 0.00274566, acc 1
2016-09-06T10:39:30.135068: step 7570, loss 0.00613229, acc 1
2016-09-06T10:39:30.946822: step 7571, loss 0.00783419, acc 1
2016-09-06T10:39:31.750722: step 7572, loss 0.0350155, acc 0.98
2016-09-06T10:39:32.578840: step 7573, loss 0.0380364, acc 0.98
2016-09-06T10:39:33.387408: step 7574, loss 0.175439, acc 0.96
2016-09-06T10:39:34.195326: step 7575, loss 0.0172285, acc 1
2016-09-06T10:39:35.045362: step 7576, loss 0.00451176, acc 1
2016-09-06T10:39:35.872488: step 7577, loss 0.0169207, acc 0.98
2016-09-06T10:39:36.664231: step 7578, loss 0.00355885, acc 1
2016-09-06T10:39:37.460676: step 7579, loss 0.00748052, acc 1
2016-09-06T10:39:38.270306: step 7580, loss 0.00253278, acc 1
2016-09-06T10:39:39.081735: step 7581, loss 0.0832534, acc 0.96
2016-09-06T10:39:39.879898: step 7582, loss 0.00264117, acc 1
2016-09-06T10:39:40.698426: step 7583, loss 0.0389689, acc 0.96
2016-09-06T10:39:41.493438: step 7584, loss 0.0146045, acc 1
2016-09-06T10:39:42.295637: step 7585, loss 0.0179288, acc 0.98
2016-09-06T10:39:43.138386: step 7586, loss 0.0121965, acc 1
2016-09-06T10:39:43.920362: step 7587, loss 0.0221693, acc 0.98
2016-09-06T10:39:44.739333: step 7588, loss 0.0225026, acc 1
2016-09-06T10:39:45.573513: step 7589, loss 0.0126082, acc 1
2016-09-06T10:39:46.363518: step 7590, loss 0.00507034, acc 1
2016-09-06T10:39:47.197266: step 7591, loss 0.0237039, acc 0.98
2016-09-06T10:39:48.009905: step 7592, loss 0.00259382, acc 1
2016-09-06T10:39:48.747590: step 7593, loss 0.0159494, acc 1
2016-09-06T10:39:49.565378: step 7594, loss 0.0280722, acc 0.98
2016-09-06T10:39:50.377623: step 7595, loss 0.109747, acc 0.96
2016-09-06T10:39:51.184347: step 7596, loss 0.0538021, acc 0.98
2016-09-06T10:39:51.998508: step 7597, loss 0.0819426, acc 0.96
2016-09-06T10:39:52.811790: step 7598, loss 0.0295349, acc 0.98
2016-09-06T10:39:53.629465: step 7599, loss 0.00304515, acc 1
2016-09-06T10:39:54.442138: step 7600, loss 0.0347394, acc 1

Evaluation:
2016-09-06T10:39:58.161891: step 7600, loss 1.75586, acc 0.75985

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-7600

2016-09-06T10:40:00.144102: step 7601, loss 0.0151021, acc 1
2016-09-06T10:40:00.958359: step 7602, loss 0.00371952, acc 1
2016-09-06T10:40:01.753904: step 7603, loss 0.0916977, acc 0.98
2016-09-06T10:40:02.585546: step 7604, loss 0.00306117, acc 1
2016-09-06T10:40:03.397642: step 7605, loss 0.069101, acc 0.96
2016-09-06T10:40:04.211115: step 7606, loss 0.00366239, acc 1
2016-09-06T10:40:05.051411: step 7607, loss 0.0520393, acc 0.96
2016-09-06T10:40:05.841798: step 7608, loss 0.0241266, acc 0.98
2016-09-06T10:40:06.658882: step 7609, loss 0.00262308, acc 1
2016-09-06T10:40:07.495508: step 7610, loss 0.0294617, acc 0.98
2016-09-06T10:40:08.307647: step 7611, loss 0.0134082, acc 1
2016-09-06T10:40:09.121230: step 7612, loss 0.0306055, acc 1
2016-09-06T10:40:09.956916: step 7613, loss 0.0345296, acc 0.98
2016-09-06T10:40:10.800831: step 7614, loss 0.0497606, acc 0.96
2016-09-06T10:40:11.650966: step 7615, loss 0.00866969, acc 1
2016-09-06T10:40:12.476435: step 7616, loss 0.00344626, acc 1
2016-09-06T10:40:13.268872: step 7617, loss 0.0198067, acc 1
2016-09-06T10:40:14.075558: step 7618, loss 0.0232181, acc 0.98
2016-09-06T10:40:14.893830: step 7619, loss 0.00324827, acc 1
2016-09-06T10:40:15.721787: step 7620, loss 0.0114071, acc 1
2016-09-06T10:40:16.534350: step 7621, loss 0.00871619, acc 1
2016-09-06T10:40:17.350864: step 7622, loss 0.0455504, acc 0.98
2016-09-06T10:40:18.171635: step 7623, loss 0.076319, acc 0.94
2016-09-06T10:40:18.974075: step 7624, loss 0.0329839, acc 0.98
2016-09-06T10:40:19.791461: step 7625, loss 0.0133478, acc 1
2016-09-06T10:40:20.587805: step 7626, loss 0.0364553, acc 0.98
2016-09-06T10:40:21.401980: step 7627, loss 0.00280187, acc 1
2016-09-06T10:40:22.211104: step 7628, loss 0.00223149, acc 1
2016-09-06T10:40:23.004204: step 7629, loss 0.029241, acc 0.98
2016-09-06T10:40:23.787993: step 7630, loss 0.0180217, acc 1
2016-09-06T10:40:24.594275: step 7631, loss 0.0257773, acc 0.98
2016-09-06T10:40:25.415275: step 7632, loss 0.020751, acc 1
2016-09-06T10:40:26.221423: step 7633, loss 0.00456204, acc 1
2016-09-06T10:40:27.035561: step 7634, loss 0.00916395, acc 1
2016-09-06T10:40:27.881185: step 7635, loss 0.0244925, acc 1
2016-09-06T10:40:28.665438: step 7636, loss 0.0363363, acc 0.98
2016-09-06T10:40:29.450636: step 7637, loss 0.00283215, acc 1
2016-09-06T10:40:30.266918: step 7638, loss 0.0179296, acc 1
2016-09-06T10:40:31.068138: step 7639, loss 0.0028862, acc 1
2016-09-06T10:40:31.868312: step 7640, loss 0.0136161, acc 1
2016-09-06T10:40:32.697875: step 7641, loss 0.00317803, acc 1
2016-09-06T10:40:33.492129: step 7642, loss 0.0222659, acc 1
2016-09-06T10:40:34.289947: step 7643, loss 0.00272838, acc 1
2016-09-06T10:40:35.132064: step 7644, loss 0.0499035, acc 0.96
2016-09-06T10:40:35.931919: step 7645, loss 0.00284957, acc 1
2016-09-06T10:40:36.761410: step 7646, loss 0.0083287, acc 1
2016-09-06T10:40:37.553400: step 7647, loss 0.0100335, acc 1
2016-09-06T10:40:38.329135: step 7648, loss 0.0343574, acc 1
2016-09-06T10:40:39.127487: step 7649, loss 0.0151587, acc 1
2016-09-06T10:40:39.925297: step 7650, loss 0.0124297, acc 1
2016-09-06T10:40:40.734322: step 7651, loss 0.0097623, acc 1
2016-09-06T10:40:41.563664: step 7652, loss 0.00281909, acc 1
2016-09-06T10:40:42.386665: step 7653, loss 0.00264124, acc 1
2016-09-06T10:40:43.173914: step 7654, loss 0.0724932, acc 0.96
2016-09-06T10:40:43.981764: step 7655, loss 0.00685778, acc 1
2016-09-06T10:40:44.817772: step 7656, loss 0.0107547, acc 1
2016-09-06T10:40:45.615863: step 7657, loss 0.129918, acc 0.92
2016-09-06T10:40:46.416866: step 7658, loss 0.00354915, acc 1
2016-09-06T10:40:47.251625: step 7659, loss 0.0168323, acc 0.98
2016-09-06T10:40:48.020091: step 7660, loss 0.0643457, acc 0.98
2016-09-06T10:40:48.829430: step 7661, loss 0.0362338, acc 0.98
2016-09-06T10:40:49.681333: step 7662, loss 0.0287049, acc 0.98
2016-09-06T10:40:50.482206: step 7663, loss 0.0322525, acc 1
2016-09-06T10:40:51.300188: step 7664, loss 0.0544484, acc 0.98
2016-09-06T10:40:52.188454: step 7665, loss 0.00287713, acc 1
2016-09-06T10:40:52.981408: step 7666, loss 0.0218131, acc 0.98
2016-09-06T10:40:53.809708: step 7667, loss 0.0246188, acc 1
2016-09-06T10:40:54.637588: step 7668, loss 0.0335922, acc 0.98
2016-09-06T10:40:55.442758: step 7669, loss 0.0238173, acc 0.98
2016-09-06T10:40:56.261682: step 7670, loss 0.0164419, acc 1
2016-09-06T10:40:57.120592: step 7671, loss 0.00998671, acc 1
2016-09-06T10:40:57.965986: step 7672, loss 0.00221986, acc 1
2016-09-06T10:40:58.793463: step 7673, loss 0.0126211, acc 1
2016-09-06T10:40:59.659166: step 7674, loss 0.0148253, acc 1
2016-09-06T10:41:00.496470: step 7675, loss 0.00677433, acc 1
2016-09-06T10:41:01.278546: step 7676, loss 0.0170347, acc 1
2016-09-06T10:41:02.106937: step 7677, loss 0.0384825, acc 0.98
2016-09-06T10:41:02.929478: step 7678, loss 0.014028, acc 1
2016-09-06T10:41:03.731861: step 7679, loss 0.0393791, acc 0.98
2016-09-06T10:41:04.497371: step 7680, loss 0.0264315, acc 0.977273
2016-09-06T10:41:05.295019: step 7681, loss 0.00232409, acc 1
2016-09-06T10:41:06.107954: step 7682, loss 0.0223163, acc 0.98
2016-09-06T10:41:06.935777: step 7683, loss 0.0218462, acc 1
2016-09-06T10:41:07.744954: step 7684, loss 0.0202205, acc 0.98
2016-09-06T10:41:08.548533: step 7685, loss 0.0291192, acc 0.98
2016-09-06T10:41:09.364180: step 7686, loss 0.0121198, acc 1
2016-09-06T10:41:10.191093: step 7687, loss 0.00305859, acc 1
2016-09-06T10:41:10.978928: step 7688, loss 0.013617, acc 1
2016-09-06T10:41:11.822314: step 7689, loss 0.0077295, acc 1
2016-09-06T10:41:12.631660: step 7690, loss 0.0362688, acc 0.96
2016-09-06T10:41:13.408789: step 7691, loss 0.0114849, acc 1
2016-09-06T10:41:14.223759: step 7692, loss 0.00234922, acc 1
2016-09-06T10:41:15.046420: step 7693, loss 0.0321807, acc 0.96
2016-09-06T10:41:15.820329: step 7694, loss 0.0313681, acc 0.98
2016-09-06T10:41:16.633021: step 7695, loss 0.0247311, acc 0.98
2016-09-06T10:41:17.494009: step 7696, loss 0.0163496, acc 1
2016-09-06T10:41:18.299503: step 7697, loss 0.0765348, acc 0.96
2016-09-06T10:41:19.100582: step 7698, loss 0.0131999, acc 1
2016-09-06T10:41:19.928439: step 7699, loss 0.00634002, acc 1
2016-09-06T10:41:20.723080: step 7700, loss 0.00215466, acc 1

Evaluation:
2016-09-06T10:41:24.438225: step 7700, loss 1.89069, acc 0.754221

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-7700

2016-09-06T10:41:26.364815: step 7701, loss 0.00686492, acc 1
2016-09-06T10:41:27.176047: step 7702, loss 0.0326619, acc 1
2016-09-06T10:41:27.971130: step 7703, loss 0.0731853, acc 0.98
2016-09-06T10:41:28.774506: step 7704, loss 0.0343254, acc 0.98
2016-09-06T10:41:29.578979: step 7705, loss 0.0201069, acc 0.98
2016-09-06T10:41:30.367092: step 7706, loss 0.0157145, acc 1
2016-09-06T10:41:31.161402: step 7707, loss 0.0148527, acc 1
2016-09-06T10:41:31.990054: step 7708, loss 0.0050785, acc 1
2016-09-06T10:41:32.766415: step 7709, loss 0.00222021, acc 1
2016-09-06T10:41:33.558452: step 7710, loss 0.01068, acc 1
2016-09-06T10:41:34.408422: step 7711, loss 0.00526172, acc 1
2016-09-06T10:41:35.178030: step 7712, loss 0.0137557, acc 1
2016-09-06T10:41:35.964927: step 7713, loss 0.0340252, acc 1
2016-09-06T10:41:36.776403: step 7714, loss 0.0214447, acc 1
2016-09-06T10:41:37.559312: step 7715, loss 0.0680498, acc 0.96
2016-09-06T10:41:38.365727: step 7716, loss 0.00279074, acc 1
2016-09-06T10:41:39.205659: step 7717, loss 0.0222894, acc 0.98
2016-09-06T10:41:39.986699: step 7718, loss 0.0254752, acc 0.98
2016-09-06T10:41:40.776127: step 7719, loss 0.0129237, acc 1
2016-09-06T10:41:41.595378: step 7720, loss 0.00977593, acc 1
2016-09-06T10:41:42.388304: step 7721, loss 0.00502991, acc 1
2016-09-06T10:41:43.194360: step 7722, loss 0.107553, acc 0.98
2016-09-06T10:41:44.050832: step 7723, loss 0.0122444, acc 1
2016-09-06T10:41:44.831433: step 7724, loss 0.021973, acc 1
2016-09-06T10:41:45.657013: step 7725, loss 0.0145703, acc 1
2016-09-06T10:41:46.480507: step 7726, loss 0.030931, acc 0.98
2016-09-06T10:41:47.249352: step 7727, loss 0.0104781, acc 1
2016-09-06T10:41:48.050105: step 7728, loss 0.0268298, acc 0.98
2016-09-06T10:41:48.873647: step 7729, loss 0.0313035, acc 0.98
2016-09-06T10:41:49.667133: step 7730, loss 0.00285537, acc 1
2016-09-06T10:41:50.497582: step 7731, loss 0.0236952, acc 0.98
2016-09-06T10:41:51.312455: step 7732, loss 0.00868577, acc 1
2016-09-06T10:41:52.105521: step 7733, loss 0.0477532, acc 0.98
2016-09-06T10:41:52.884772: step 7734, loss 0.00675174, acc 1
2016-09-06T10:41:53.720950: step 7735, loss 0.0282344, acc 1
2016-09-06T10:41:54.488369: step 7736, loss 0.0311642, acc 0.98
2016-09-06T10:41:55.287203: step 7737, loss 0.00359954, acc 1
2016-09-06T10:41:56.099146: step 7738, loss 0.0435824, acc 0.98
2016-09-06T10:41:56.889286: step 7739, loss 0.00386163, acc 1
2016-09-06T10:41:57.691562: step 7740, loss 0.021218, acc 0.98
2016-09-06T10:41:58.511298: step 7741, loss 0.0230533, acc 1
2016-09-06T10:41:59.307178: step 7742, loss 0.0207472, acc 0.98
2016-09-06T10:42:00.108871: step 7743, loss 0.00445093, acc 1
2016-09-06T10:42:00.971144: step 7744, loss 0.0424031, acc 0.98
2016-09-06T10:42:01.745751: step 7745, loss 0.0178599, acc 0.98
2016-09-06T10:42:02.546559: step 7746, loss 0.00700717, acc 1
2016-09-06T10:42:03.382364: step 7747, loss 0.0123993, acc 1
2016-09-06T10:42:04.163341: step 7748, loss 0.0313235, acc 0.98
2016-09-06T10:42:04.975581: step 7749, loss 0.0029799, acc 1
2016-09-06T10:42:05.782278: step 7750, loss 0.0199516, acc 1
2016-09-06T10:42:06.600297: step 7751, loss 0.00364938, acc 1
2016-09-06T10:42:07.402127: step 7752, loss 0.0139903, acc 1
2016-09-06T10:42:08.213955: step 7753, loss 0.0147341, acc 1
2016-09-06T10:42:08.989047: step 7754, loss 0.00469819, acc 1
2016-09-06T10:42:09.849734: step 7755, loss 0.0571904, acc 0.96
2016-09-06T10:42:10.740294: step 7756, loss 0.0700544, acc 0.96
2016-09-06T10:42:11.548136: step 7757, loss 0.0270386, acc 0.98
2016-09-06T10:42:12.341099: step 7758, loss 0.0211345, acc 0.98
2016-09-06T10:42:13.173887: step 7759, loss 0.0311779, acc 0.98
2016-09-06T10:42:13.991687: step 7760, loss 0.0121009, acc 1
2016-09-06T10:42:14.820587: step 7761, loss 0.00222824, acc 1
2016-09-06T10:42:15.688260: step 7762, loss 0.0021233, acc 1
2016-09-06T10:42:16.500142: step 7763, loss 0.0768505, acc 0.98
2016-09-06T10:42:17.317853: step 7764, loss 0.0396565, acc 0.98
2016-09-06T10:42:18.171541: step 7765, loss 0.0489647, acc 0.98
2016-09-06T10:42:19.009025: step 7766, loss 0.0223962, acc 1
2016-09-06T10:42:19.816603: step 7767, loss 0.00206458, acc 1
2016-09-06T10:42:20.630232: step 7768, loss 0.0369103, acc 0.96
2016-09-06T10:42:21.431547: step 7769, loss 0.0290411, acc 1
2016-09-06T10:42:22.232582: step 7770, loss 0.00191033, acc 1
2016-09-06T10:42:23.060039: step 7771, loss 0.0321808, acc 1
2016-09-06T10:42:23.891819: step 7772, loss 0.00218535, acc 1
2016-09-06T10:42:24.683180: step 7773, loss 0.00794561, acc 1
2016-09-06T10:42:25.478415: step 7774, loss 0.00178998, acc 1
2016-09-06T10:42:26.289729: step 7775, loss 0.0256125, acc 1
2016-09-06T10:42:27.095659: step 7776, loss 0.016029, acc 0.98
2016-09-06T10:42:27.900528: step 7777, loss 0.0111375, acc 1
2016-09-06T10:42:28.714823: step 7778, loss 0.0373512, acc 0.98
2016-09-06T10:42:29.491267: step 7779, loss 0.0100794, acc 1
2016-09-06T10:42:30.310927: step 7780, loss 0.0275002, acc 0.98
2016-09-06T10:42:31.135712: step 7781, loss 0.0793569, acc 0.98
2016-09-06T10:42:31.918054: step 7782, loss 0.057577, acc 0.98
2016-09-06T10:42:32.727225: step 7783, loss 0.03814, acc 0.98
2016-09-06T10:42:33.542395: step 7784, loss 0.00240619, acc 1
2016-09-06T10:42:34.317646: step 7785, loss 0.0220643, acc 1
2016-09-06T10:42:35.144293: step 7786, loss 0.0152459, acc 1
2016-09-06T10:42:35.966163: step 7787, loss 0.0307342, acc 1
2016-09-06T10:42:36.743491: step 7788, loss 0.0311127, acc 0.98
2016-09-06T10:42:37.579214: step 7789, loss 0.0288098, acc 1
2016-09-06T10:42:38.381756: step 7790, loss 0.0253015, acc 1
2016-09-06T10:42:39.192999: step 7791, loss 0.0037422, acc 1
2016-09-06T10:42:40.016987: step 7792, loss 0.056037, acc 0.96
2016-09-06T10:42:40.863391: step 7793, loss 0.100413, acc 0.96
2016-09-06T10:42:41.675267: step 7794, loss 0.0214759, acc 0.98
2016-09-06T10:42:42.469431: step 7795, loss 0.0526977, acc 0.96
2016-09-06T10:42:43.254707: step 7796, loss 0.00322664, acc 1
2016-09-06T10:42:44.048908: step 7797, loss 0.0766313, acc 0.94
2016-09-06T10:42:44.862608: step 7798, loss 0.0246841, acc 1
2016-09-06T10:42:45.708771: step 7799, loss 0.0210453, acc 1
2016-09-06T10:42:46.493387: step 7800, loss 0.0049124, acc 1

Evaluation:
2016-09-06T10:42:50.184650: step 7800, loss 1.49824, acc 0.762664

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-7800

2016-09-06T10:42:52.186396: step 7801, loss 0.0038769, acc 1
2016-09-06T10:42:53.027303: step 7802, loss 0.0158009, acc 1
2016-09-06T10:42:53.801207: step 7803, loss 0.0145479, acc 1
2016-09-06T10:42:54.632431: step 7804, loss 0.024744, acc 1
2016-09-06T10:42:55.435453: step 7805, loss 0.0354611, acc 1
2016-09-06T10:42:56.214912: step 7806, loss 0.0429571, acc 0.96
2016-09-06T10:42:57.025920: step 7807, loss 0.00761202, acc 1
2016-09-06T10:42:57.841435: step 7808, loss 0.0178776, acc 1
2016-09-06T10:42:58.613898: step 7809, loss 0.0222899, acc 0.98
2016-09-06T10:42:59.433035: step 7810, loss 0.0872092, acc 0.98
2016-09-06T10:43:00.294969: step 7811, loss 0.0125992, acc 1
2016-09-06T10:43:01.087336: step 7812, loss 0.00200687, acc 1
2016-09-06T10:43:01.891191: step 7813, loss 0.0652415, acc 0.98
2016-09-06T10:43:02.699794: step 7814, loss 0.0214204, acc 1
2016-09-06T10:43:03.500174: step 7815, loss 0.143071, acc 0.96
2016-09-06T10:43:04.303361: step 7816, loss 0.0117119, acc 1
2016-09-06T10:43:05.166816: step 7817, loss 0.00737522, acc 1
2016-09-06T10:43:05.966173: step 7818, loss 0.0242021, acc 1
2016-09-06T10:43:06.742034: step 7819, loss 0.00991041, acc 1
2016-09-06T10:43:07.556484: step 7820, loss 0.0516084, acc 0.98
2016-09-06T10:43:08.347440: step 7821, loss 0.0522015, acc 0.98
2016-09-06T10:43:09.152867: step 7822, loss 0.00727772, acc 1
2016-09-06T10:43:09.977398: step 7823, loss 0.0123236, acc 1
2016-09-06T10:43:10.758939: step 7824, loss 0.0534363, acc 0.98
2016-09-06T10:43:11.569379: step 7825, loss 0.00469051, acc 1
2016-09-06T10:43:12.375214: step 7826, loss 0.0222729, acc 0.98
2016-09-06T10:43:13.182162: step 7827, loss 0.0807242, acc 0.96
2016-09-06T10:43:14.006475: step 7828, loss 0.0163688, acc 1
2016-09-06T10:43:14.812374: step 7829, loss 0.0523904, acc 0.96
2016-09-06T10:43:15.592596: step 7830, loss 0.0432338, acc 0.96
2016-09-06T10:43:16.382175: step 7831, loss 0.0176971, acc 0.98
2016-09-06T10:43:17.196683: step 7832, loss 0.00793863, acc 1
2016-09-06T10:43:17.999430: step 7833, loss 0.0111854, acc 1
2016-09-06T10:43:18.794819: step 7834, loss 0.0169006, acc 0.98
2016-09-06T10:43:19.620587: step 7835, loss 0.0287174, acc 1
2016-09-06T10:43:20.410769: step 7836, loss 0.0353746, acc 0.98
2016-09-06T10:43:21.211304: step 7837, loss 0.0313628, acc 0.98
2016-09-06T10:43:22.036220: step 7838, loss 0.0686891, acc 0.96
2016-09-06T10:43:22.828992: step 7839, loss 0.0129221, acc 1
2016-09-06T10:43:23.637408: step 7840, loss 0.0560762, acc 0.98
2016-09-06T10:43:24.471181: step 7841, loss 0.00322204, acc 1
2016-09-06T10:43:25.639688: step 7842, loss 0.0267816, acc 1
2016-09-06T10:43:26.456221: step 7843, loss 0.00527174, acc 1
2016-09-06T10:43:27.297425: step 7844, loss 0.0242139, acc 0.98
2016-09-06T10:43:28.107227: step 7845, loss 0.0026102, acc 1
2016-09-06T10:43:28.885494: step 7846, loss 0.0208029, acc 1
2016-09-06T10:43:29.707216: step 7847, loss 0.0991384, acc 0.96
2016-09-06T10:43:30.559439: step 7848, loss 0.00422195, acc 1
2016-09-06T10:43:31.356859: step 7849, loss 0.0276415, acc 1
2016-09-06T10:43:32.153399: step 7850, loss 0.056971, acc 0.98
2016-09-06T10:43:32.973207: step 7851, loss 0.0485424, acc 0.98
2016-09-06T10:43:33.762235: step 7852, loss 0.00689111, acc 1
2016-09-06T10:43:34.591349: step 7853, loss 0.00639588, acc 1
2016-09-06T10:43:35.403606: step 7854, loss 0.0140667, acc 1
2016-09-06T10:43:36.169038: step 7855, loss 0.0155563, acc 1
2016-09-06T10:43:36.962650: step 7856, loss 0.00218791, acc 1
2016-09-06T10:43:37.770355: step 7857, loss 0.0267053, acc 1
2016-09-06T10:43:38.545842: step 7858, loss 0.0528704, acc 0.96
2016-09-06T10:43:39.361247: step 7859, loss 0.0312406, acc 0.98
2016-09-06T10:43:40.188470: step 7860, loss 0.0477167, acc 0.98
2016-09-06T10:43:40.974362: step 7861, loss 0.00535912, acc 1
2016-09-06T10:43:41.776022: step 7862, loss 0.0181453, acc 1
2016-09-06T10:43:42.592388: step 7863, loss 0.00221765, acc 1
2016-09-06T10:43:43.383741: step 7864, loss 0.00344603, acc 1
2016-09-06T10:43:44.196216: step 7865, loss 0.00492278, acc 1
2016-09-06T10:43:45.021856: step 7866, loss 0.0231061, acc 1
2016-09-06T10:43:45.843126: step 7867, loss 0.00908103, acc 1
2016-09-06T10:43:46.663089: step 7868, loss 0.00321362, acc 1
2016-09-06T10:43:47.469557: step 7869, loss 0.0311348, acc 0.98
2016-09-06T10:43:48.250787: step 7870, loss 0.00735515, acc 1
2016-09-06T10:43:49.052245: step 7871, loss 0.0167188, acc 1
2016-09-06T10:43:49.814421: step 7872, loss 0.0795397, acc 0.977273
2016-09-06T10:43:50.596918: step 7873, loss 0.0287383, acc 0.98
2016-09-06T10:43:51.411326: step 7874, loss 0.0771402, acc 0.94
2016-09-06T10:43:52.210388: step 7875, loss 0.00436256, acc 1
2016-09-06T10:43:53.038950: step 7876, loss 0.0238183, acc 0.98
2016-09-06T10:43:53.857202: step 7877, loss 0.0174056, acc 1
2016-09-06T10:43:54.714516: step 7878, loss 0.0340294, acc 0.96
2016-09-06T10:43:55.478634: step 7879, loss 0.00715589, acc 1
2016-09-06T10:43:56.267000: step 7880, loss 0.0206481, acc 1
2016-09-06T10:43:57.081636: step 7881, loss 0.0545567, acc 0.98
2016-09-06T10:43:57.906211: step 7882, loss 0.0260672, acc 0.98
2016-09-06T10:43:58.725854: step 7883, loss 0.00221204, acc 1
2016-09-06T10:43:59.548260: step 7884, loss 0.00877429, acc 1
2016-09-06T10:44:00.323732: step 7885, loss 0.0125359, acc 1
2016-09-06T10:44:01.121185: step 7886, loss 0.0307838, acc 0.98
2016-09-06T10:44:01.956234: step 7887, loss 0.0471393, acc 0.98
2016-09-06T10:44:02.722357: step 7888, loss 0.0248571, acc 1
2016-09-06T10:44:03.506024: step 7889, loss 0.0417308, acc 0.96
2016-09-06T10:44:04.336269: step 7890, loss 0.0023918, acc 1
2016-09-06T10:44:05.147009: step 7891, loss 0.00977747, acc 1
2016-09-06T10:44:05.951480: step 7892, loss 0.00354168, acc 1
2016-09-06T10:44:06.757631: step 7893, loss 0.00247814, acc 1
2016-09-06T10:44:07.551412: step 7894, loss 0.0728686, acc 0.96
2016-09-06T10:44:08.348750: step 7895, loss 0.00403997, acc 1
2016-09-06T10:44:09.177758: step 7896, loss 0.00622159, acc 1
2016-09-06T10:44:09.978669: step 7897, loss 0.00807386, acc 1
2016-09-06T10:44:10.792021: step 7898, loss 0.0236965, acc 0.98
2016-09-06T10:44:11.628109: step 7899, loss 0.0189807, acc 1
2016-09-06T10:44:12.401778: step 7900, loss 0.0755437, acc 0.96

Evaluation:
2016-09-06T10:44:16.129503: step 7900, loss 1.95929, acc 0.750469

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-7900

2016-09-06T10:44:18.112307: step 7901, loss 0.0178174, acc 1
2016-09-06T10:44:18.925438: step 7902, loss 0.0330672, acc 0.98
2016-09-06T10:44:19.705933: step 7903, loss 0.00240017, acc 1
2016-09-06T10:44:20.517988: step 7904, loss 0.0220036, acc 1
2016-09-06T10:44:21.343280: step 7905, loss 0.00813651, acc 1
2016-09-06T10:44:22.115983: step 7906, loss 0.0106246, acc 1
2016-09-06T10:44:22.923534: step 7907, loss 0.0051014, acc 1
2016-09-06T10:44:23.755245: step 7908, loss 0.0026245, acc 1
2016-09-06T10:44:24.530355: step 7909, loss 0.0105161, acc 1
2016-09-06T10:44:25.330199: step 7910, loss 0.0252805, acc 0.98
2016-09-06T10:44:26.164866: step 7911, loss 0.0132401, acc 1
2016-09-06T10:44:26.946611: step 7912, loss 0.014972, acc 1
2016-09-06T10:44:27.734953: step 7913, loss 0.0359102, acc 0.98
2016-09-06T10:44:28.545214: step 7914, loss 0.00385811, acc 1
2016-09-06T10:44:29.322851: step 7915, loss 0.021281, acc 0.98
2016-09-06T10:44:30.141186: step 7916, loss 0.00758477, acc 1
2016-09-06T10:44:30.976164: step 7917, loss 0.00294768, acc 1
2016-09-06T10:44:31.761697: step 7918, loss 0.0456778, acc 0.96
2016-09-06T10:44:32.580562: step 7919, loss 0.0159578, acc 1
2016-09-06T10:44:33.379933: step 7920, loss 0.0450584, acc 0.96
2016-09-06T10:44:34.173970: step 7921, loss 0.0286564, acc 0.98
2016-09-06T10:44:34.986931: step 7922, loss 0.0025487, acc 1
2016-09-06T10:44:35.817792: step 7923, loss 0.0437029, acc 0.98
2016-09-06T10:44:36.627435: step 7924, loss 0.0297271, acc 0.98
2016-09-06T10:44:37.400061: step 7925, loss 0.00564046, acc 1
2016-09-06T10:44:38.229448: step 7926, loss 0.030536, acc 0.98
2016-09-06T10:44:39.017899: step 7927, loss 0.0348449, acc 0.98
2016-09-06T10:44:39.822415: step 7928, loss 0.00491179, acc 1
2016-09-06T10:44:40.674317: step 7929, loss 0.00257927, acc 1
2016-09-06T10:44:41.460895: step 7930, loss 0.0213646, acc 0.98
2016-09-06T10:44:42.251333: step 7931, loss 0.0100751, acc 1
2016-09-06T10:44:43.056304: step 7932, loss 0.00315928, acc 1
2016-09-06T10:44:43.858980: step 7933, loss 0.0431416, acc 0.98
2016-09-06T10:44:44.684861: step 7934, loss 0.0137353, acc 1
2016-09-06T10:44:45.503386: step 7935, loss 0.00374084, acc 1
2016-09-06T10:44:46.303353: step 7936, loss 0.0167791, acc 0.98
2016-09-06T10:44:47.102063: step 7937, loss 0.102039, acc 0.98
2016-09-06T10:44:47.919128: step 7938, loss 0.0267701, acc 0.98
2016-09-06T10:44:48.706546: step 7939, loss 0.00687412, acc 1
2016-09-06T10:44:49.495141: step 7940, loss 0.0484672, acc 0.98
2016-09-06T10:44:50.327631: step 7941, loss 0.0118931, acc 1
2016-09-06T10:44:51.097461: step 7942, loss 0.0470408, acc 0.96
2016-09-06T10:44:51.883613: step 7943, loss 0.0286967, acc 1
2016-09-06T10:44:52.693258: step 7944, loss 0.00552428, acc 1
2016-09-06T10:44:53.473403: step 7945, loss 0.0123585, acc 1
2016-09-06T10:44:54.279543: step 7946, loss 0.00594509, acc 1
2016-09-06T10:44:55.089945: step 7947, loss 0.0254122, acc 0.98
2016-09-06T10:44:55.866988: step 7948, loss 0.0164265, acc 0.98
2016-09-06T10:44:56.694130: step 7949, loss 0.0111694, acc 1
2016-09-06T10:44:57.517713: step 7950, loss 0.0117655, acc 1
2016-09-06T10:44:58.312538: step 7951, loss 0.0326506, acc 0.98
2016-09-06T10:44:59.125632: step 7952, loss 0.0118061, acc 1
2016-09-06T10:44:59.958425: step 7953, loss 0.00317911, acc 1
2016-09-06T10:45:00.765541: step 7954, loss 0.0111426, acc 1
2016-09-06T10:45:01.618605: step 7955, loss 0.0263626, acc 0.98
2016-09-06T10:45:02.467443: step 7956, loss 0.013381, acc 1
2016-09-06T10:45:03.265283: step 7957, loss 0.00353667, acc 1
2016-09-06T10:45:04.071474: step 7958, loss 0.0108909, acc 1
2016-09-06T10:45:04.897089: step 7959, loss 0.0224427, acc 1
2016-09-06T10:45:05.704110: step 7960, loss 0.00297837, acc 1
2016-09-06T10:45:06.519875: step 7961, loss 0.00554797, acc 1
2016-09-06T10:45:07.355256: step 7962, loss 0.191138, acc 0.98
2016-09-06T10:45:08.181286: step 7963, loss 0.0151647, acc 1
2016-09-06T10:45:09.028365: step 7964, loss 0.00252035, acc 1
2016-09-06T10:45:09.863712: step 7965, loss 0.00975534, acc 1
2016-09-06T10:45:10.669468: step 7966, loss 0.0244831, acc 0.98
2016-09-06T10:45:11.472676: step 7967, loss 0.0301429, acc 0.98
2016-09-06T10:45:12.298129: step 7968, loss 0.0132118, acc 1
2016-09-06T10:45:13.107107: step 7969, loss 0.00322897, acc 1
2016-09-06T10:45:13.919737: step 7970, loss 0.0240314, acc 1
2016-09-06T10:45:14.765501: step 7971, loss 0.0173424, acc 0.98
2016-09-06T10:45:15.583701: step 7972, loss 0.00655814, acc 1
2016-09-06T10:45:16.380204: step 7973, loss 0.027866, acc 0.98
2016-09-06T10:45:17.209265: step 7974, loss 0.00387623, acc 1
2016-09-06T10:45:18.025327: step 7975, loss 0.00388764, acc 1
2016-09-06T10:45:18.802400: step 7976, loss 0.00251586, acc 1
2016-09-06T10:45:19.638244: step 7977, loss 0.0168809, acc 1
2016-09-06T10:45:20.449974: step 7978, loss 0.0159638, acc 1
2016-09-06T10:45:21.254397: step 7979, loss 0.0236522, acc 0.98
2016-09-06T10:45:22.086909: step 7980, loss 0.0389534, acc 0.96
2016-09-06T10:45:22.912028: step 7981, loss 0.0110443, acc 1
2016-09-06T10:45:23.730931: step 7982, loss 0.0224074, acc 1
2016-09-06T10:45:24.529921: step 7983, loss 0.00237786, acc 1
2016-09-06T10:45:25.337690: step 7984, loss 0.0515124, acc 0.96
2016-09-06T10:45:26.128187: step 7985, loss 0.0160208, acc 1
2016-09-06T10:45:26.952882: step 7986, loss 0.00510547, acc 1
2016-09-06T10:45:27.780668: step 7987, loss 0.0102668, acc 1
2016-09-06T10:45:28.536774: step 7988, loss 0.124558, acc 0.96
2016-09-06T10:45:29.364267: step 7989, loss 0.0244283, acc 1
2016-09-06T10:45:30.164539: step 7990, loss 0.0277867, acc 0.98
2016-09-06T10:45:30.929489: step 7991, loss 0.00948534, acc 1
2016-09-06T10:45:31.753862: step 7992, loss 0.070359, acc 0.98
2016-09-06T10:45:32.577137: step 7993, loss 0.00263066, acc 1
2016-09-06T10:45:33.362932: step 7994, loss 0.00664763, acc 1
2016-09-06T10:45:34.174222: step 7995, loss 0.0134959, acc 1
2016-09-06T10:45:34.981460: step 7996, loss 0.084483, acc 0.96
2016-09-06T10:45:35.784959: step 7997, loss 0.0335616, acc 1
2016-09-06T10:45:36.578744: step 7998, loss 0.0342215, acc 0.98
2016-09-06T10:45:37.373905: step 7999, loss 0.0313117, acc 1
2016-09-06T10:45:38.186620: step 8000, loss 0.0519414, acc 0.98

Evaluation:
2016-09-06T10:45:41.906069: step 8000, loss 1.61813, acc 0.746717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-8000

2016-09-06T10:45:43.864390: step 8001, loss 0.0371327, acc 0.98
2016-09-06T10:45:44.680146: step 8002, loss 0.045633, acc 0.98
2016-09-06T10:45:45.495104: step 8003, loss 0.0729263, acc 0.94
2016-09-06T10:45:46.315742: step 8004, loss 0.0392853, acc 0.98
2016-09-06T10:45:47.132136: step 8005, loss 0.0433672, acc 0.98
2016-09-06T10:45:47.981971: step 8006, loss 0.0128592, acc 1
2016-09-06T10:45:48.801098: step 8007, loss 0.0387339, acc 0.96
2016-09-06T10:45:49.630377: step 8008, loss 0.0164042, acc 1
2016-09-06T10:45:50.419653: step 8009, loss 0.00524391, acc 1
2016-09-06T10:45:51.219229: step 8010, loss 0.0288497, acc 0.98
2016-09-06T10:45:52.049023: step 8011, loss 0.00905428, acc 1
2016-09-06T10:45:52.817995: step 8012, loss 0.00436007, acc 1
2016-09-06T10:45:53.650152: step 8013, loss 0.0594479, acc 0.96
2016-09-06T10:45:54.486882: step 8014, loss 0.0234254, acc 1
2016-09-06T10:45:55.340320: step 8015, loss 0.0588522, acc 0.94
2016-09-06T10:45:56.152828: step 8016, loss 0.0194617, acc 1
2016-09-06T10:45:56.982674: step 8017, loss 0.0108125, acc 1
2016-09-06T10:45:57.897498: step 8018, loss 0.0181703, acc 1
2016-09-06T10:45:58.752845: step 8019, loss 0.054129, acc 0.98
2016-09-06T10:45:59.588341: step 8020, loss 0.0314447, acc 0.98
2016-09-06T10:46:00.483136: step 8021, loss 0.0113006, acc 1
2016-09-06T10:46:01.306248: step 8022, loss 0.0121388, acc 1
2016-09-06T10:46:02.107265: step 8023, loss 0.00741888, acc 1
2016-09-06T10:46:02.925855: step 8024, loss 0.00544648, acc 1
2016-09-06T10:46:03.724454: step 8025, loss 0.0118977, acc 1
2016-09-06T10:46:04.526711: step 8026, loss 0.00532946, acc 1
2016-09-06T10:46:05.374675: step 8027, loss 0.00556174, acc 1
2016-09-06T10:46:06.171668: step 8028, loss 0.00655326, acc 1
2016-09-06T10:46:06.994227: step 8029, loss 0.0277167, acc 0.98
2016-09-06T10:46:07.864393: step 8030, loss 0.00524216, acc 1
2016-09-06T10:46:08.665074: step 8031, loss 0.022172, acc 1
2016-09-06T10:46:09.485524: step 8032, loss 0.0168209, acc 1
2016-09-06T10:46:10.338373: step 8033, loss 0.0186952, acc 1
2016-09-06T10:46:11.146184: step 8034, loss 0.0445331, acc 0.98
2016-09-06T10:46:11.941989: step 8035, loss 0.0341019, acc 0.98
2016-09-06T10:46:12.754621: step 8036, loss 0.0331133, acc 0.98
2016-09-06T10:46:13.582758: step 8037, loss 0.00482468, acc 1
2016-09-06T10:46:14.429496: step 8038, loss 0.0382906, acc 1
2016-09-06T10:46:15.241247: step 8039, loss 0.00638905, acc 1
2016-09-06T10:46:16.075994: step 8040, loss 0.0207345, acc 0.98
2016-09-06T10:46:17.036533: step 8041, loss 0.0582145, acc 0.98
2016-09-06T10:46:17.858472: step 8042, loss 0.182997, acc 0.98
2016-09-06T10:46:18.667225: step 8043, loss 0.0278864, acc 0.98
2016-09-06T10:46:19.566347: step 8044, loss 0.0502144, acc 0.98
2016-09-06T10:46:20.370928: step 8045, loss 0.00992778, acc 1
2016-09-06T10:46:21.200920: step 8046, loss 0.021546, acc 1
2016-09-06T10:46:22.033422: step 8047, loss 0.0277291, acc 0.98
2016-09-06T10:46:22.876284: step 8048, loss 0.0040167, acc 1
2016-09-06T10:46:23.712019: step 8049, loss 0.0170372, acc 1
2016-09-06T10:46:24.490290: step 8050, loss 0.204076, acc 0.92
2016-09-06T10:46:25.297519: step 8051, loss 0.183975, acc 0.96
2016-09-06T10:46:26.129463: step 8052, loss 0.0617858, acc 0.98
2016-09-06T10:46:26.941435: step 8053, loss 0.0034333, acc 1
2016-09-06T10:46:27.762400: step 8054, loss 0.00787225, acc 1
2016-09-06T10:46:28.600243: step 8055, loss 0.05318, acc 0.98
2016-09-06T10:46:29.421293: step 8056, loss 0.00449015, acc 1
2016-09-06T10:46:30.222843: step 8057, loss 0.0356696, acc 1
2016-09-06T10:46:31.051443: step 8058, loss 0.0542299, acc 0.94
2016-09-06T10:46:31.864983: step 8059, loss 0.072066, acc 0.98
2016-09-06T10:46:32.689476: step 8060, loss 0.00372256, acc 1
2016-09-06T10:46:33.485404: step 8061, loss 0.138405, acc 0.94
2016-09-06T10:46:34.278852: step 8062, loss 0.00393532, acc 1
2016-09-06T10:46:35.079180: step 8063, loss 0.0100514, acc 1
2016-09-06T10:46:35.848819: step 8064, loss 0.0214566, acc 1
2016-09-06T10:46:36.664887: step 8065, loss 0.0136763, acc 1
2016-09-06T10:46:37.462425: step 8066, loss 0.104791, acc 0.96
2016-09-06T10:46:38.277370: step 8067, loss 0.0345693, acc 1
2016-09-06T10:46:39.100955: step 8068, loss 0.0244787, acc 1
2016-09-06T10:46:39.930540: step 8069, loss 0.0461175, acc 0.98
2016-09-06T10:46:40.759387: step 8070, loss 0.018795, acc 1
2016-09-06T10:46:41.584223: step 8071, loss 0.0200004, acc 0.98
2016-09-06T10:46:42.377614: step 8072, loss 0.0183547, acc 1
2016-09-06T10:46:43.191286: step 8073, loss 0.0236678, acc 1
2016-09-06T10:46:43.997557: step 8074, loss 0.044596, acc 0.98
2016-09-06T10:46:44.795999: step 8075, loss 0.0174463, acc 1
2016-09-06T10:46:45.620869: step 8076, loss 0.0278276, acc 0.98
2016-09-06T10:46:46.445910: step 8077, loss 0.00673697, acc 1
2016-09-06T10:46:47.239098: step 8078, loss 0.0237127, acc 1
2016-09-06T10:46:48.043661: step 8079, loss 0.0172681, acc 1
2016-09-06T10:46:48.819987: step 8080, loss 0.017906, acc 1
2016-09-06T10:46:49.648062: step 8081, loss 0.0184725, acc 0.98
2016-09-06T10:46:50.492323: step 8082, loss 0.00380906, acc 1
2016-09-06T10:46:51.344654: step 8083, loss 0.0118724, acc 1
2016-09-06T10:46:52.139407: step 8084, loss 0.0685027, acc 0.98
2016-09-06T10:46:52.932327: step 8085, loss 0.0798631, acc 0.98
2016-09-06T10:46:53.749935: step 8086, loss 0.0187488, acc 1
2016-09-06T10:46:54.527609: step 8087, loss 0.0238722, acc 1
2016-09-06T10:46:55.351208: step 8088, loss 0.0248988, acc 1
2016-09-06T10:46:56.172992: step 8089, loss 0.0137703, acc 1
2016-09-06T10:46:56.947336: step 8090, loss 0.0741035, acc 0.94
2016-09-06T10:46:57.747416: step 8091, loss 0.00362833, acc 1
2016-09-06T10:46:58.562725: step 8092, loss 0.00398793, acc 1
2016-09-06T10:46:59.359244: step 8093, loss 0.0223446, acc 0.98
2016-09-06T10:47:00.162205: step 8094, loss 0.0222944, acc 1
2016-09-06T10:47:01.001927: step 8095, loss 0.0212518, acc 1
2016-09-06T10:47:01.784713: step 8096, loss 0.0087329, acc 1
2016-09-06T10:47:02.619410: step 8097, loss 0.0335476, acc 0.98
2016-09-06T10:47:03.432036: step 8098, loss 0.0260131, acc 0.98
2016-09-06T10:47:04.234613: step 8099, loss 0.00338238, acc 1
2016-09-06T10:47:05.043941: step 8100, loss 0.00326817, acc 1

Evaluation:
2016-09-06T10:47:08.776920: step 8100, loss 1.93037, acc 0.762664

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-8100

2016-09-06T10:47:10.811086: step 8101, loss 0.0684561, acc 0.94
2016-09-06T10:47:11.613749: step 8102, loss 0.00786036, acc 1
2016-09-06T10:47:12.454345: step 8103, loss 0.146737, acc 0.96
2016-09-06T10:47:13.298262: step 8104, loss 0.00483763, acc 1
2016-09-06T10:47:14.085873: step 8105, loss 0.0191968, acc 1
2016-09-06T10:47:14.929149: step 8106, loss 0.0160239, acc 1
2016-09-06T10:47:15.749038: step 8107, loss 0.0202471, acc 0.98
2016-09-06T10:47:16.553377: step 8108, loss 0.00380017, acc 1
2016-09-06T10:47:17.358514: step 8109, loss 0.00392026, acc 1
2016-09-06T10:47:18.183313: step 8110, loss 0.0209498, acc 0.98
2016-09-06T10:47:19.001591: step 8111, loss 0.0225448, acc 0.98
2016-09-06T10:47:19.817616: step 8112, loss 0.0982575, acc 0.98
2016-09-06T10:47:20.662270: step 8113, loss 0.0102218, acc 1
2016-09-06T10:47:21.487387: step 8114, loss 0.00291899, acc 1
2016-09-06T10:47:22.288163: step 8115, loss 0.0140883, acc 1
2016-09-06T10:47:23.079366: step 8116, loss 0.0609819, acc 0.94
2016-09-06T10:47:23.885775: step 8117, loss 0.0167971, acc 1
2016-09-06T10:47:24.702934: step 8118, loss 0.0119033, acc 1
2016-09-06T10:47:25.530677: step 8119, loss 0.00462348, acc 1
2016-09-06T10:47:26.338856: step 8120, loss 0.0101398, acc 1
2016-09-06T10:47:27.134446: step 8121, loss 0.00531964, acc 1
2016-09-06T10:47:27.929190: step 8122, loss 0.0289949, acc 1
2016-09-06T10:47:28.772461: step 8123, loss 0.00318641, acc 1
2016-09-06T10:47:29.573359: step 8124, loss 0.00347066, acc 1
2016-09-06T10:47:30.386621: step 8125, loss 0.0560476, acc 0.98
2016-09-06T10:47:31.206523: step 8126, loss 0.00326335, acc 1
2016-09-06T10:47:32.002727: step 8127, loss 0.00336363, acc 1
2016-09-06T10:47:32.802992: step 8128, loss 0.0479815, acc 0.96
2016-09-06T10:47:33.649699: step 8129, loss 0.0251394, acc 0.98
2016-09-06T10:47:34.432777: step 8130, loss 0.00401594, acc 1
2016-09-06T10:47:35.241658: step 8131, loss 0.0118546, acc 1
2016-09-06T10:47:36.066445: step 8132, loss 0.0416211, acc 1
2016-09-06T10:47:36.836941: step 8133, loss 0.0145726, acc 1
2016-09-06T10:47:37.622955: step 8134, loss 0.0154608, acc 1
2016-09-06T10:47:38.436946: step 8135, loss 0.0205167, acc 1
2016-09-06T10:47:39.249590: step 8136, loss 0.0265007, acc 1
2016-09-06T10:47:40.088025: step 8137, loss 0.00422098, acc 1
2016-09-06T10:47:40.899874: step 8138, loss 0.00853384, acc 1
2016-09-06T10:47:41.666323: step 8139, loss 0.00710556, acc 1
2016-09-06T10:47:42.475569: step 8140, loss 0.00556967, acc 1
2016-09-06T10:47:43.298570: step 8141, loss 0.033512, acc 0.98
2016-09-06T10:47:44.065706: step 8142, loss 0.0238315, acc 0.98
2016-09-06T10:47:44.869425: step 8143, loss 0.0517618, acc 0.98
2016-09-06T10:47:45.685498: step 8144, loss 0.00341859, acc 1
2016-09-06T10:47:46.477073: step 8145, loss 0.00808966, acc 1
2016-09-06T10:47:47.286029: step 8146, loss 0.0153268, acc 1
2016-09-06T10:47:48.106026: step 8147, loss 0.045678, acc 0.98
2016-09-06T10:47:48.914814: step 8148, loss 0.0127805, acc 1
2016-09-06T10:47:49.738098: step 8149, loss 0.00295069, acc 1
2016-09-06T10:47:50.568914: step 8150, loss 0.0313677, acc 0.98
2016-09-06T10:47:51.368224: step 8151, loss 0.00267412, acc 1
2016-09-06T10:47:52.163702: step 8152, loss 0.0153896, acc 1
2016-09-06T10:47:52.974189: step 8153, loss 0.0382035, acc 0.98
2016-09-06T10:47:53.747908: step 8154, loss 0.0187423, acc 1
2016-09-06T10:47:54.556253: step 8155, loss 0.0152025, acc 1
2016-09-06T10:47:55.392003: step 8156, loss 0.00397477, acc 1
2016-09-06T10:47:56.183989: step 8157, loss 0.0274952, acc 1
2016-09-06T10:47:56.949599: step 8158, loss 0.0219032, acc 0.98
2016-09-06T10:47:57.779818: step 8159, loss 0.00685447, acc 1
2016-09-06T10:47:58.558145: step 8160, loss 0.00255439, acc 1
2016-09-06T10:47:59.356445: step 8161, loss 0.00429629, acc 1
2016-09-06T10:48:00.144193: step 8162, loss 0.0185381, acc 1
2016-09-06T10:48:00.960863: step 8163, loss 0.0235283, acc 1
2016-09-06T10:48:01.780898: step 8164, loss 0.0223419, acc 0.98
2016-09-06T10:48:02.595078: step 8165, loss 0.00784696, acc 1
2016-09-06T10:48:03.393308: step 8166, loss 0.0123358, acc 1
2016-09-06T10:48:04.171978: step 8167, loss 0.00339419, acc 1
2016-09-06T10:48:05.014767: step 8168, loss 0.0119295, acc 1
2016-09-06T10:48:05.801447: step 8169, loss 0.0350981, acc 0.98
2016-09-06T10:48:06.650870: step 8170, loss 0.0184654, acc 0.98
2016-09-06T10:48:07.485106: step 8171, loss 0.0390044, acc 0.98
2016-09-06T10:48:08.266274: step 8172, loss 0.0341312, acc 0.98
2016-09-06T10:48:09.069080: step 8173, loss 0.0228806, acc 1
2016-09-06T10:48:09.912142: step 8174, loss 0.0138532, acc 1
2016-09-06T10:48:10.710681: step 8175, loss 0.0120996, acc 1
2016-09-06T10:48:11.497390: step 8176, loss 0.0283935, acc 1
2016-09-06T10:48:12.292242: step 8177, loss 0.0563735, acc 0.98
2016-09-06T10:48:13.082603: step 8178, loss 0.00293813, acc 1
2016-09-06T10:48:13.902793: step 8179, loss 0.0169686, acc 1
2016-09-06T10:48:14.713369: step 8180, loss 0.0103013, acc 1
2016-09-06T10:48:15.488068: step 8181, loss 0.0318355, acc 0.98
2016-09-06T10:48:16.289344: step 8182, loss 0.00343275, acc 1
2016-09-06T10:48:17.110360: step 8183, loss 0.00450058, acc 1
2016-09-06T10:48:17.905610: step 8184, loss 0.0339064, acc 0.98
2016-09-06T10:48:18.718850: step 8185, loss 0.0197515, acc 0.98
2016-09-06T10:48:19.533372: step 8186, loss 0.0208342, acc 0.98
2016-09-06T10:48:20.333384: step 8187, loss 0.0221049, acc 0.98
2016-09-06T10:48:21.136757: step 8188, loss 0.00477701, acc 1
2016-09-06T10:48:21.972290: step 8189, loss 0.0329757, acc 0.98
2016-09-06T10:48:22.741720: step 8190, loss 0.0162938, acc 1
2016-09-06T10:48:23.548928: step 8191, loss 0.02861, acc 0.98
2016-09-06T10:48:24.359291: step 8192, loss 0.0321789, acc 0.98
2016-09-06T10:48:25.147203: step 8193, loss 0.00808191, acc 1
2016-09-06T10:48:25.941994: step 8194, loss 0.0171112, acc 1
2016-09-06T10:48:26.764231: step 8195, loss 0.0208239, acc 0.98
2016-09-06T10:48:27.555463: step 8196, loss 0.00640993, acc 1
2016-09-06T10:48:28.362202: step 8197, loss 0.0153976, acc 1
2016-09-06T10:48:29.188453: step 8198, loss 0.0150682, acc 1
2016-09-06T10:48:29.998615: step 8199, loss 0.0200486, acc 0.98
2016-09-06T10:48:30.798161: step 8200, loss 0.018349, acc 0.98

Evaluation:
2016-09-06T10:48:34.542293: step 8200, loss 2.10217, acc 0.757974

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-8200

2016-09-06T10:48:36.435483: step 8201, loss 0.0121771, acc 1
2016-09-06T10:48:37.244485: step 8202, loss 0.133315, acc 0.96
2016-09-06T10:48:38.071356: step 8203, loss 0.00672438, acc 1
2016-09-06T10:48:38.873824: step 8204, loss 0.0110011, acc 1
2016-09-06T10:48:39.671066: step 8205, loss 0.0312958, acc 0.98
2016-09-06T10:48:40.506952: step 8206, loss 0.0109683, acc 1
2016-09-06T10:48:41.318129: step 8207, loss 0.0245067, acc 0.98
2016-09-06T10:48:42.124922: step 8208, loss 0.0326442, acc 0.98
2016-09-06T10:48:42.958943: step 8209, loss 0.0556701, acc 0.96
2016-09-06T10:48:43.798363: step 8210, loss 0.0514794, acc 0.96
2016-09-06T10:48:44.623841: step 8211, loss 0.0359349, acc 0.98
2016-09-06T10:48:45.433974: step 8212, loss 0.0729264, acc 0.96
2016-09-06T10:48:46.256113: step 8213, loss 0.0537367, acc 0.98
2016-09-06T10:48:47.081480: step 8214, loss 0.0230616, acc 1
2016-09-06T10:48:47.891825: step 8215, loss 0.00645351, acc 1
2016-09-06T10:48:48.692558: step 8216, loss 0.00309861, acc 1
2016-09-06T10:48:49.459475: step 8217, loss 0.0119982, acc 1
2016-09-06T10:48:50.252901: step 8218, loss 0.00336079, acc 1
2016-09-06T10:48:51.070327: step 8219, loss 0.021548, acc 0.98
2016-09-06T10:48:51.893243: step 8220, loss 0.0108698, acc 1
2016-09-06T10:48:52.706229: step 8221, loss 0.0253564, acc 0.98
2016-09-06T10:48:53.526024: step 8222, loss 0.0153199, acc 1
2016-09-06T10:48:54.295630: step 8223, loss 0.00384208, acc 1
2016-09-06T10:48:55.100906: step 8224, loss 0.0362456, acc 0.98
2016-09-06T10:48:55.914376: step 8225, loss 0.0450345, acc 0.98
2016-09-06T10:48:56.725288: step 8226, loss 0.0345175, acc 0.98
2016-09-06T10:48:57.573645: step 8227, loss 0.0152122, acc 1
2016-09-06T10:48:58.398127: step 8228, loss 0.00877338, acc 1
2016-09-06T10:48:59.210343: step 8229, loss 0.102402, acc 0.98
2016-09-06T10:49:00.025373: step 8230, loss 0.00760645, acc 1
2016-09-06T10:49:00.883733: step 8231, loss 0.0101422, acc 1
2016-09-06T10:49:01.664853: step 8232, loss 0.0722541, acc 0.98
2016-09-06T10:49:02.502935: step 8233, loss 0.116523, acc 0.92
2016-09-06T10:49:03.332762: step 8234, loss 0.00605458, acc 1
2016-09-06T10:49:04.154182: step 8235, loss 0.0145237, acc 1
2016-09-06T10:49:05.009471: step 8236, loss 0.0176705, acc 1
2016-09-06T10:49:05.840115: step 8237, loss 0.00748717, acc 1
2016-09-06T10:49:06.657800: step 8238, loss 0.0047071, acc 1
2016-09-06T10:49:07.497296: step 8239, loss 0.141732, acc 0.96
2016-09-06T10:49:08.337000: step 8240, loss 0.00232741, acc 1
2016-09-06T10:49:09.161733: step 8241, loss 0.0286354, acc 0.98
2016-09-06T10:49:10.045420: step 8242, loss 0.0343686, acc 0.98
2016-09-06T10:49:10.845405: step 8243, loss 0.0224674, acc 1
2016-09-06T10:49:11.663335: step 8244, loss 0.0204374, acc 0.98
2016-09-06T10:49:12.441880: step 8245, loss 0.00409335, acc 1
2016-09-06T10:49:13.253559: step 8246, loss 0.0169343, acc 1
2016-09-06T10:49:14.078708: step 8247, loss 0.0257863, acc 0.98
2016-09-06T10:49:14.871347: step 8248, loss 0.00728588, acc 1
2016-09-06T10:49:15.683064: step 8249, loss 0.0195202, acc 0.98
2016-09-06T10:49:16.494964: step 8250, loss 0.0323773, acc 0.98
2016-09-06T10:49:17.281664: step 8251, loss 0.0388791, acc 0.96
2016-09-06T10:49:18.069498: step 8252, loss 0.00426482, acc 1
2016-09-06T10:49:18.892639: step 8253, loss 0.104527, acc 0.98
2016-09-06T10:49:19.692596: step 8254, loss 0.00208438, acc 1
2016-09-06T10:49:20.471698: step 8255, loss 0.0150697, acc 1
2016-09-06T10:49:21.229145: step 8256, loss 0.00403876, acc 1
2016-09-06T10:49:22.045858: step 8257, loss 0.028709, acc 0.98
2016-09-06T10:49:22.909053: step 8258, loss 0.0136356, acc 1
2016-09-06T10:49:23.713237: step 8259, loss 0.0394564, acc 0.98
2016-09-06T10:49:24.495614: step 8260, loss 0.00406791, acc 1
2016-09-06T10:49:25.299268: step 8261, loss 0.0616191, acc 0.98
2016-09-06T10:49:26.127560: step 8262, loss 0.0305839, acc 1
2016-09-06T10:49:26.943211: step 8263, loss 0.00912717, acc 1
2016-09-06T10:49:27.756682: step 8264, loss 0.0472275, acc 0.98
2016-09-06T10:49:28.547914: step 8265, loss 0.0166224, acc 1
2016-09-06T10:49:29.348029: step 8266, loss 0.0165569, acc 1
2016-09-06T10:49:30.143821: step 8267, loss 0.0139871, acc 1
2016-09-06T10:49:30.954642: step 8268, loss 0.00326708, acc 1
2016-09-06T10:49:31.753084: step 8269, loss 0.0263884, acc 0.98
2016-09-06T10:49:32.550375: step 8270, loss 0.016207, acc 1
2016-09-06T10:49:33.346323: step 8271, loss 0.0470559, acc 0.98
2016-09-06T10:49:34.146152: step 8272, loss 0.00333859, acc 1
2016-09-06T10:49:34.985230: step 8273, loss 0.0225199, acc 0.98
2016-09-06T10:49:35.772694: step 8274, loss 0.00560756, acc 1
2016-09-06T10:49:36.590207: step 8275, loss 0.0440398, acc 0.98
2016-09-06T10:49:37.390871: step 8276, loss 0.00227731, acc 1
2016-09-06T10:49:38.204755: step 8277, loss 0.00540202, acc 1
2016-09-06T10:49:39.013665: step 8278, loss 0.0845996, acc 0.96
2016-09-06T10:49:39.822343: step 8279, loss 0.00285443, acc 1
2016-09-06T10:49:40.637103: step 8280, loss 0.0168086, acc 0.98
2016-09-06T10:49:41.442925: step 8281, loss 0.00481759, acc 1
2016-09-06T10:49:42.292132: step 8282, loss 0.0151593, acc 1
2016-09-06T10:49:43.098972: step 8283, loss 0.0116479, acc 1
2016-09-06T10:49:43.881044: step 8284, loss 0.00528975, acc 1
2016-09-06T10:49:44.675375: step 8285, loss 0.0217407, acc 1
2016-09-06T10:49:45.509805: step 8286, loss 0.0107568, acc 1
2016-09-06T10:49:46.307721: step 8287, loss 0.00240333, acc 1
2016-09-06T10:49:47.111960: step 8288, loss 0.0580666, acc 0.98
2016-09-06T10:49:47.920860: step 8289, loss 0.0385411, acc 0.98
2016-09-06T10:49:48.697488: step 8290, loss 0.036433, acc 0.96
2016-09-06T10:49:49.498613: step 8291, loss 0.00240761, acc 1
2016-09-06T10:49:50.345811: step 8292, loss 0.00208927, acc 1
2016-09-06T10:49:51.146398: step 8293, loss 0.0134474, acc 1
2016-09-06T10:49:51.918145: step 8294, loss 0.0193098, acc 1
2016-09-06T10:49:52.752096: step 8295, loss 0.03772, acc 0.98
2016-09-06T10:49:53.527946: step 8296, loss 0.0042791, acc 1
2016-09-06T10:49:54.356332: step 8297, loss 0.0187306, acc 0.98
2016-09-06T10:49:55.160571: step 8298, loss 0.00796647, acc 1
2016-09-06T10:49:55.980246: step 8299, loss 0.0388573, acc 0.98
2016-09-06T10:49:56.791724: step 8300, loss 0.00861761, acc 1

Evaluation:
2016-09-06T10:50:00.517577: step 8300, loss 1.77997, acc 0.76454

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-8300

2016-09-06T10:50:02.455692: step 8301, loss 0.00241025, acc 1
2016-09-06T10:50:03.301885: step 8302, loss 0.0144194, acc 1
2016-09-06T10:50:04.154651: step 8303, loss 0.00465569, acc 1
2016-09-06T10:50:04.972509: step 8304, loss 0.0370509, acc 0.98
2016-09-06T10:50:05.801529: step 8305, loss 0.0128494, acc 1
2016-09-06T10:50:06.594215: step 8306, loss 0.00348977, acc 1
2016-09-06T10:50:07.386578: step 8307, loss 0.0186568, acc 1
2016-09-06T10:50:08.175159: step 8308, loss 0.0786695, acc 0.96
2016-09-06T10:50:08.990238: step 8309, loss 0.0177027, acc 0.98
2016-09-06T10:50:09.794577: step 8310, loss 0.024004, acc 0.98
2016-09-06T10:50:10.621557: step 8311, loss 0.00684561, acc 1
2016-09-06T10:50:11.448048: step 8312, loss 0.0154437, acc 1
2016-09-06T10:50:12.267044: step 8313, loss 0.00814921, acc 1
2016-09-06T10:50:13.068613: step 8314, loss 0.0456724, acc 0.98
2016-09-06T10:50:13.877861: step 8315, loss 0.00681893, acc 1
2016-09-06T10:50:14.691772: step 8316, loss 0.0247823, acc 0.98
2016-09-06T10:50:15.481460: step 8317, loss 0.0145025, acc 1
2016-09-06T10:50:16.292725: step 8318, loss 0.00523, acc 1
2016-09-06T10:50:17.111777: step 8319, loss 0.00209769, acc 1
2016-09-06T10:50:17.908610: step 8320, loss 0.0274951, acc 0.98
2016-09-06T10:50:18.728603: step 8321, loss 0.0163774, acc 0.98
2016-09-06T10:50:19.533719: step 8322, loss 0.0164843, acc 1
2016-09-06T10:50:20.318716: step 8323, loss 0.027649, acc 0.98
2016-09-06T10:50:21.140018: step 8324, loss 0.0776307, acc 0.94
2016-09-06T10:50:21.953528: step 8325, loss 0.00777037, acc 1
2016-09-06T10:50:22.739499: step 8326, loss 0.0760229, acc 0.98
2016-09-06T10:50:23.551074: step 8327, loss 0.0538141, acc 0.98
2016-09-06T10:50:24.366713: step 8328, loss 0.00299155, acc 1
2016-09-06T10:50:25.146007: step 8329, loss 0.00980333, acc 1
2016-09-06T10:50:25.963156: step 8330, loss 0.0202092, acc 0.98
2016-09-06T10:50:26.787835: step 8331, loss 0.00557408, acc 1
2016-09-06T10:50:27.574279: step 8332, loss 0.0191712, acc 0.98
2016-09-06T10:50:28.369235: step 8333, loss 0.016011, acc 0.98
2016-09-06T10:50:29.192270: step 8334, loss 0.00806877, acc 1
2016-09-06T10:50:29.975615: step 8335, loss 0.015933, acc 1
2016-09-06T10:50:30.798483: step 8336, loss 0.0841409, acc 0.98
2016-09-06T10:50:31.618795: step 8337, loss 0.0133055, acc 1
2016-09-06T10:50:32.413692: step 8338, loss 0.00241801, acc 1
2016-09-06T10:50:33.212129: step 8339, loss 0.0136537, acc 1
2016-09-06T10:50:34.046349: step 8340, loss 0.0041735, acc 1
2016-09-06T10:50:34.870576: step 8341, loss 0.0630413, acc 0.98
2016-09-06T10:50:35.668223: step 8342, loss 0.0166689, acc 0.98
2016-09-06T10:50:36.461599: step 8343, loss 0.0123292, acc 1
2016-09-06T10:50:37.235765: step 8344, loss 0.0140613, acc 1
2016-09-06T10:50:38.028507: step 8345, loss 0.0442796, acc 0.96
2016-09-06T10:50:38.854590: step 8346, loss 0.00258284, acc 1
2016-09-06T10:50:39.693459: step 8347, loss 0.015484, acc 1
2016-09-06T10:50:40.497090: step 8348, loss 0.00277343, acc 1
2016-09-06T10:50:41.328661: step 8349, loss 0.00459422, acc 1
2016-09-06T10:50:42.128827: step 8350, loss 0.0342096, acc 0.96
2016-09-06T10:50:42.937017: step 8351, loss 0.00673021, acc 1
2016-09-06T10:50:43.776452: step 8352, loss 0.00212133, acc 1
2016-09-06T10:50:44.612887: step 8353, loss 0.0116407, acc 1
2016-09-06T10:50:45.440355: step 8354, loss 0.0218999, acc 1
2016-09-06T10:50:46.268428: step 8355, loss 0.00186453, acc 1
2016-09-06T10:50:47.075074: step 8356, loss 0.017069, acc 1
2016-09-06T10:50:47.896516: step 8357, loss 0.0289201, acc 0.98
2016-09-06T10:50:48.724158: step 8358, loss 0.0269151, acc 0.98
2016-09-06T10:50:49.520539: step 8359, loss 0.0163789, acc 0.98
2016-09-06T10:50:50.326590: step 8360, loss 0.00309162, acc 1
2016-09-06T10:50:51.181263: step 8361, loss 0.0264363, acc 0.98
2016-09-06T10:50:51.993267: step 8362, loss 0.00470075, acc 1
2016-09-06T10:50:52.802369: step 8363, loss 0.0276278, acc 1
2016-09-06T10:50:53.648959: step 8364, loss 0.0131364, acc 1
2016-09-06T10:50:54.466277: step 8365, loss 0.0869068, acc 0.98
2016-09-06T10:50:55.315115: step 8366, loss 0.0475231, acc 0.98
2016-09-06T10:50:56.174844: step 8367, loss 0.0070191, acc 1
2016-09-06T10:50:57.013418: step 8368, loss 0.110263, acc 0.94
2016-09-06T10:50:57.790738: step 8369, loss 0.0151489, acc 1
2016-09-06T10:50:58.572990: step 8370, loss 0.0183611, acc 1
2016-09-06T10:50:59.403227: step 8371, loss 0.0225211, acc 1
2016-09-06T10:51:00.187653: step 8372, loss 0.0166641, acc 0.98
2016-09-06T10:51:01.048414: step 8373, loss 0.00851772, acc 1
2016-09-06T10:51:01.854962: step 8374, loss 0.0109092, acc 1
2016-09-06T10:51:02.655291: step 8375, loss 0.153507, acc 0.96
2016-09-06T10:51:03.460220: step 8376, loss 0.00319124, acc 1
2016-09-06T10:51:04.258993: step 8377, loss 0.00179043, acc 1
2016-09-06T10:51:05.039606: step 8378, loss 0.0122855, acc 1
2016-09-06T10:51:05.837682: step 8379, loss 0.00358897, acc 1
2016-09-06T10:51:06.665036: step 8380, loss 0.014977, acc 1
2016-09-06T10:51:07.446335: step 8381, loss 0.0435841, acc 0.96
2016-09-06T10:51:08.251925: step 8382, loss 0.00793716, acc 1
2016-09-06T10:51:09.087000: step 8383, loss 0.0278799, acc 1
2016-09-06T10:51:09.870017: step 8384, loss 0.0297086, acc 1
2016-09-06T10:51:10.677131: step 8385, loss 0.0658925, acc 0.98
2016-09-06T10:51:11.520388: step 8386, loss 0.0612646, acc 0.98
2016-09-06T10:51:12.295074: step 8387, loss 0.0127248, acc 1
2016-09-06T10:51:13.083093: step 8388, loss 0.00865587, acc 1
2016-09-06T10:51:13.930889: step 8389, loss 0.00352724, acc 1
2016-09-06T10:51:14.736463: step 8390, loss 0.00196654, acc 1
2016-09-06T10:51:15.551793: step 8391, loss 0.0434015, acc 1
2016-09-06T10:51:16.408672: step 8392, loss 0.00322365, acc 1
2016-09-06T10:51:17.261041: step 8393, loss 0.00213213, acc 1
2016-09-06T10:51:18.088733: step 8394, loss 0.0226815, acc 0.98
2016-09-06T10:51:18.971604: step 8395, loss 0.0156412, acc 1
2016-09-06T10:51:19.774572: step 8396, loss 0.00229719, acc 1
2016-09-06T10:51:20.549559: step 8397, loss 0.022409, acc 1
2016-09-06T10:51:21.422747: step 8398, loss 0.0020147, acc 1
2016-09-06T10:51:22.240754: step 8399, loss 0.0170543, acc 1
2016-09-06T10:51:23.053330: step 8400, loss 0.0243474, acc 1

Evaluation:
2016-09-06T10:51:26.827758: step 8400, loss 1.9877, acc 0.755159

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-8400

2016-09-06T10:51:28.695907: step 8401, loss 0.0303987, acc 0.98
2016-09-06T10:51:29.507643: step 8402, loss 0.0192344, acc 0.98
2016-09-06T10:51:30.308537: step 8403, loss 0.0572265, acc 0.96
2016-09-06T10:51:31.152435: step 8404, loss 0.0371292, acc 0.96
2016-09-06T10:51:31.979251: step 8405, loss 0.0164001, acc 0.98
2016-09-06T10:51:32.782937: step 8406, loss 0.0109063, acc 1
2016-09-06T10:51:33.614363: step 8407, loss 0.114814, acc 0.98
2016-09-06T10:51:34.453472: step 8408, loss 0.00231559, acc 1
2016-09-06T10:51:35.260158: step 8409, loss 0.0139524, acc 1
2016-09-06T10:51:36.077644: step 8410, loss 0.0178052, acc 0.98
2016-09-06T10:51:36.920860: step 8411, loss 0.0993327, acc 0.96
2016-09-06T10:51:37.683361: step 8412, loss 0.0143233, acc 1
2016-09-06T10:51:38.474590: step 8413, loss 0.0212338, acc 1
2016-09-06T10:51:39.286114: step 8414, loss 0.0250613, acc 1
2016-09-06T10:51:40.079174: step 8415, loss 0.0315417, acc 0.98
2016-09-06T10:51:40.911860: step 8416, loss 0.0682903, acc 0.98
2016-09-06T10:51:41.714637: step 8417, loss 0.0341961, acc 0.98
2016-09-06T10:51:42.516523: step 8418, loss 0.00810762, acc 1
2016-09-06T10:51:43.374465: step 8419, loss 0.0103171, acc 1
2016-09-06T10:51:44.179804: step 8420, loss 0.0704128, acc 0.98
2016-09-06T10:51:44.974567: step 8421, loss 0.00311035, acc 1
2016-09-06T10:51:45.743961: step 8422, loss 0.0390787, acc 0.96
2016-09-06T10:51:46.570517: step 8423, loss 0.0311676, acc 1
2016-09-06T10:51:47.339156: step 8424, loss 0.00791103, acc 1
2016-09-06T10:51:48.167163: step 8425, loss 0.059164, acc 0.96
2016-09-06T10:51:48.991437: step 8426, loss 0.00508784, acc 1
2016-09-06T10:51:49.812845: step 8427, loss 0.0331771, acc 1
2016-09-06T10:51:50.623213: step 8428, loss 0.0508753, acc 0.96
2016-09-06T10:51:51.505419: step 8429, loss 0.0130889, acc 1
2016-09-06T10:51:52.334758: step 8430, loss 0.0176906, acc 1
2016-09-06T10:51:53.173145: step 8431, loss 0.0240925, acc 0.98
2016-09-06T10:51:54.039417: step 8432, loss 0.0359603, acc 0.98
2016-09-06T10:51:54.845195: step 8433, loss 0.00981554, acc 1
2016-09-06T10:51:55.653094: step 8434, loss 0.0220674, acc 1
2016-09-06T10:51:56.481361: step 8435, loss 0.0179565, acc 1
2016-09-06T10:51:57.306379: step 8436, loss 0.0283698, acc 1
2016-09-06T10:51:58.121939: step 8437, loss 0.0302425, acc 0.98
2016-09-06T10:51:58.957477: step 8438, loss 0.0440514, acc 0.98
2016-09-06T10:51:59.771283: step 8439, loss 0.0150359, acc 1
2016-09-06T10:52:00.613690: step 8440, loss 0.0215039, acc 1
2016-09-06T10:52:01.441039: step 8441, loss 0.0134772, acc 1
2016-09-06T10:52:02.271704: step 8442, loss 0.0412632, acc 0.98
2016-09-06T10:52:03.050593: step 8443, loss 0.119971, acc 0.94
2016-09-06T10:52:03.850438: step 8444, loss 0.00504203, acc 1
2016-09-06T10:52:04.664672: step 8445, loss 0.0107143, acc 1
2016-09-06T10:52:05.468514: step 8446, loss 0.0162738, acc 1
2016-09-06T10:52:06.302370: step 8447, loss 0.00364851, acc 1
2016-09-06T10:52:07.121930: step 8448, loss 0.00348834, acc 1
2016-09-06T10:52:07.901701: step 8449, loss 0.0791644, acc 0.96
2016-09-06T10:52:08.696177: step 8450, loss 0.0383151, acc 0.98
2016-09-06T10:52:09.508615: step 8451, loss 0.0250037, acc 1
2016-09-06T10:52:10.289426: step 8452, loss 0.0268566, acc 1
2016-09-06T10:52:11.094454: step 8453, loss 0.0144009, acc 1
2016-09-06T10:52:11.912553: step 8454, loss 0.0269408, acc 0.98
2016-09-06T10:52:12.678169: step 8455, loss 0.0608055, acc 0.94
2016-09-06T10:52:13.496013: step 8456, loss 0.0136665, acc 1
2016-09-06T10:52:14.307753: step 8457, loss 0.113366, acc 0.96
2016-09-06T10:52:15.117628: step 8458, loss 0.047391, acc 0.96
2016-09-06T10:52:15.914061: step 8459, loss 0.00321145, acc 1
2016-09-06T10:52:16.717570: step 8460, loss 0.00356556, acc 1
2016-09-06T10:52:17.509203: step 8461, loss 0.0165045, acc 1
2016-09-06T10:52:18.347789: step 8462, loss 0.0335761, acc 0.98
2016-09-06T10:52:19.168398: step 8463, loss 0.0375793, acc 0.98
2016-09-06T10:52:20.025134: step 8464, loss 0.0165508, acc 1
2016-09-06T10:52:20.827134: step 8465, loss 0.00404944, acc 1
2016-09-06T10:52:21.636518: step 8466, loss 0.0079545, acc 1
2016-09-06T10:52:22.442636: step 8467, loss 0.0353848, acc 0.98
2016-09-06T10:52:23.203942: step 8468, loss 0.0029462, acc 1
2016-09-06T10:52:24.050223: step 8469, loss 0.00991847, acc 1
2016-09-06T10:52:24.823357: step 8470, loss 0.00480711, acc 1
2016-09-06T10:52:25.634932: step 8471, loss 0.00306446, acc 1
2016-09-06T10:52:26.439028: step 8472, loss 0.00472303, acc 1
2016-09-06T10:52:27.235129: step 8473, loss 0.00392073, acc 1
2016-09-06T10:52:28.054664: step 8474, loss 0.0235778, acc 0.98
2016-09-06T10:52:28.882136: step 8475, loss 0.00522758, acc 1
2016-09-06T10:52:29.650042: step 8476, loss 0.00303589, acc 1
2016-09-06T10:52:30.464977: step 8477, loss 0.00407597, acc 1
2016-09-06T10:52:31.273583: step 8478, loss 0.0442276, acc 0.98
2016-09-06T10:52:32.061460: step 8479, loss 0.00291419, acc 1
2016-09-06T10:52:32.858235: step 8480, loss 0.00869497, acc 1
2016-09-06T10:52:33.713682: step 8481, loss 0.0212482, acc 0.98
2016-09-06T10:52:34.516103: step 8482, loss 0.00333545, acc 1
2016-09-06T10:52:35.323891: step 8483, loss 0.0108537, acc 1
2016-09-06T10:52:36.146280: step 8484, loss 0.00965642, acc 1
2016-09-06T10:52:36.921294: step 8485, loss 0.0189044, acc 0.98
2016-09-06T10:52:37.723804: step 8486, loss 0.0594489, acc 0.96
2016-09-06T10:52:38.553916: step 8487, loss 0.0183057, acc 0.98
2016-09-06T10:52:39.351497: step 8488, loss 0.0272543, acc 1
2016-09-06T10:52:40.159407: step 8489, loss 0.0250031, acc 1
2016-09-06T10:52:40.963686: step 8490, loss 0.00336193, acc 1
2016-09-06T10:52:41.738934: step 8491, loss 0.0169927, acc 1
2016-09-06T10:52:42.537184: step 8492, loss 0.00439139, acc 1
2016-09-06T10:52:43.378821: step 8493, loss 0.00263789, acc 1
2016-09-06T10:52:44.154479: step 8494, loss 0.0164752, acc 1
2016-09-06T10:52:44.962641: step 8495, loss 0.0166498, acc 1
2016-09-06T10:52:45.769236: step 8496, loss 0.0172683, acc 1
2016-09-06T10:52:46.570649: step 8497, loss 0.032491, acc 0.98
2016-09-06T10:52:47.402546: step 8498, loss 0.0034565, acc 1
2016-09-06T10:52:48.223110: step 8499, loss 0.0243479, acc 1
2016-09-06T10:52:49.015840: step 8500, loss 0.0528462, acc 0.98

Evaluation:
2016-09-06T10:52:52.744951: step 8500, loss 2.32999, acc 0.758912

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-8500

2016-09-06T10:52:54.696442: step 8501, loss 0.0167021, acc 0.98
2016-09-06T10:52:55.516126: step 8502, loss 0.0580528, acc 0.98
2016-09-06T10:52:56.334772: step 8503, loss 0.0246238, acc 0.98
2016-09-06T10:52:57.140742: step 8504, loss 0.114042, acc 0.98
2016-09-06T10:52:57.964037: step 8505, loss 0.0152331, acc 1
2016-09-06T10:52:58.759536: step 8506, loss 0.0503379, acc 0.98
2016-09-06T10:52:59.565755: step 8507, loss 0.0215299, acc 0.98
2016-09-06T10:53:00.419991: step 8508, loss 0.00356498, acc 1
2016-09-06T10:53:01.174213: step 8509, loss 0.0812197, acc 0.96
2016-09-06T10:53:01.987019: step 8510, loss 0.00835096, acc 1
2016-09-06T10:53:02.806411: step 8511, loss 0.013901, acc 1
2016-09-06T10:53:03.597359: step 8512, loss 0.00972171, acc 1
2016-09-06T10:53:04.398385: step 8513, loss 0.0165998, acc 0.98
2016-09-06T10:53:05.211912: step 8514, loss 0.00318073, acc 1
2016-09-06T10:53:06.003832: step 8515, loss 0.0232294, acc 1
2016-09-06T10:53:06.803661: step 8516, loss 0.10524, acc 0.94
2016-09-06T10:53:07.633124: step 8517, loss 0.034732, acc 0.98
2016-09-06T10:53:08.422473: step 8518, loss 0.0395934, acc 0.98
2016-09-06T10:53:09.244638: step 8519, loss 0.0175423, acc 0.98
2016-09-06T10:53:10.090650: step 8520, loss 0.00791415, acc 1
2016-09-06T10:53:10.906771: step 8521, loss 0.026284, acc 0.98
2016-09-06T10:53:11.703312: step 8522, loss 0.0032331, acc 1
2016-09-06T10:53:12.496108: step 8523, loss 0.0171101, acc 1
2016-09-06T10:53:13.270869: step 8524, loss 0.0249474, acc 0.98
2016-09-06T10:53:14.072702: step 8525, loss 0.0334529, acc 0.98
2016-09-06T10:53:14.890481: step 8526, loss 0.0443973, acc 0.96
2016-09-06T10:53:15.699547: step 8527, loss 0.0242049, acc 1
2016-09-06T10:53:16.504206: step 8528, loss 0.00780569, acc 1
2016-09-06T10:53:17.327985: step 8529, loss 0.00230306, acc 1
2016-09-06T10:53:18.105122: step 8530, loss 0.048189, acc 0.98
2016-09-06T10:53:18.937892: step 8531, loss 0.0141217, acc 1
2016-09-06T10:53:19.728128: step 8532, loss 0.00539145, acc 1
2016-09-06T10:53:20.530482: step 8533, loss 0.00674615, acc 1
2016-09-06T10:53:21.353021: step 8534, loss 0.0426528, acc 1
2016-09-06T10:53:22.153239: step 8535, loss 0.00507808, acc 1
2016-09-06T10:53:22.939034: step 8536, loss 0.00496175, acc 1
2016-09-06T10:53:23.743408: step 8537, loss 0.00630269, acc 1
2016-09-06T10:53:24.555182: step 8538, loss 0.0163634, acc 1
2016-09-06T10:53:25.363148: step 8539, loss 0.00719292, acc 1
2016-09-06T10:53:26.180315: step 8540, loss 0.0124441, acc 1
2016-09-06T10:53:26.985905: step 8541, loss 0.00307273, acc 1
2016-09-06T10:53:27.782149: step 8542, loss 0.00262154, acc 1
2016-09-06T10:53:28.597933: step 8543, loss 0.0325417, acc 0.98
2016-09-06T10:53:29.414384: step 8544, loss 0.00400943, acc 1
2016-09-06T10:53:30.205426: step 8545, loss 0.0026641, acc 1
2016-09-06T10:53:30.998015: step 8546, loss 0.0469605, acc 0.98
2016-09-06T10:53:31.802652: step 8547, loss 0.0220789, acc 1
2016-09-06T10:53:32.584144: step 8548, loss 0.00254045, acc 1
2016-09-06T10:53:33.373868: step 8549, loss 0.0324269, acc 0.98
2016-09-06T10:53:34.188956: step 8550, loss 0.0195374, acc 0.98
2016-09-06T10:53:34.972459: step 8551, loss 0.0114456, acc 1
2016-09-06T10:53:35.775733: step 8552, loss 0.00257713, acc 1
2016-09-06T10:53:36.591551: step 8553, loss 0.0339847, acc 0.96
2016-09-06T10:53:37.382730: step 8554, loss 0.0355652, acc 0.96
2016-09-06T10:53:38.208691: step 8555, loss 0.0239119, acc 0.98
2016-09-06T10:53:39.027651: step 8556, loss 0.0263215, acc 1
2016-09-06T10:53:39.800418: step 8557, loss 0.0264216, acc 1
2016-09-06T10:53:40.600903: step 8558, loss 0.0421889, acc 1
2016-09-06T10:53:41.397637: step 8559, loss 0.0026061, acc 1
2016-09-06T10:53:42.214124: step 8560, loss 0.0145052, acc 1
2016-09-06T10:53:43.069887: step 8561, loss 0.009102, acc 1
2016-09-06T10:53:43.902377: step 8562, loss 0.0124082, acc 1
2016-09-06T10:53:44.665063: step 8563, loss 0.00372331, acc 1
2016-09-06T10:53:45.511061: step 8564, loss 0.0099214, acc 1
2016-09-06T10:53:46.336387: step 8565, loss 0.0174035, acc 1
2016-09-06T10:53:47.116642: step 8566, loss 0.00557526, acc 1
2016-09-06T10:53:47.900382: step 8567, loss 0.0104703, acc 1
2016-09-06T10:53:48.713441: step 8568, loss 0.0436555, acc 0.98
2016-09-06T10:53:49.501416: step 8569, loss 0.00269424, acc 1
2016-09-06T10:53:50.304146: step 8570, loss 0.0174075, acc 0.98
2016-09-06T10:53:51.128600: step 8571, loss 0.00329906, acc 1
2016-09-06T10:53:51.913423: step 8572, loss 0.00587911, acc 1
2016-09-06T10:53:52.726592: step 8573, loss 0.0127366, acc 1
2016-09-06T10:53:53.536449: step 8574, loss 0.00293007, acc 1
2016-09-06T10:53:54.318876: step 8575, loss 0.0166476, acc 1
2016-09-06T10:53:55.119929: step 8576, loss 0.0126879, acc 1
2016-09-06T10:53:55.951733: step 8577, loss 0.00321374, acc 1
2016-09-06T10:53:56.760078: step 8578, loss 0.0563563, acc 0.98
2016-09-06T10:53:57.572577: step 8579, loss 0.0552463, acc 0.98
2016-09-06T10:53:58.398301: step 8580, loss 0.0189908, acc 0.98
2016-09-06T10:53:59.197439: step 8581, loss 0.0411389, acc 0.96
2016-09-06T10:53:59.992871: step 8582, loss 0.004172, acc 1
2016-09-06T10:54:00.863020: step 8583, loss 0.013535, acc 1
2016-09-06T10:54:01.668374: step 8584, loss 0.0413862, acc 0.98
2016-09-06T10:54:02.478997: step 8585, loss 0.00383056, acc 1
2016-09-06T10:54:03.292370: step 8586, loss 0.0519429, acc 0.98
2016-09-06T10:54:04.112187: step 8587, loss 0.00792459, acc 1
2016-09-06T10:54:04.937782: step 8588, loss 0.00305155, acc 1
2016-09-06T10:54:05.781779: step 8589, loss 0.0156633, acc 1
2016-09-06T10:54:06.589573: step 8590, loss 0.0330237, acc 0.98
2016-09-06T10:54:07.441782: step 8591, loss 0.00392392, acc 1
2016-09-06T10:54:08.292105: step 8592, loss 0.0510228, acc 0.96
2016-09-06T10:54:09.121739: step 8593, loss 0.0117972, acc 1
2016-09-06T10:54:09.952467: step 8594, loss 0.00556263, acc 1
2016-09-06T10:54:10.790300: step 8595, loss 0.0436736, acc 0.96
2016-09-06T10:54:11.618104: step 8596, loss 0.00415109, acc 1
2016-09-06T10:54:12.416236: step 8597, loss 0.0684211, acc 0.98
2016-09-06T10:54:13.258718: step 8598, loss 0.0231224, acc 1
2016-09-06T10:54:14.061402: step 8599, loss 0.00262193, acc 1
2016-09-06T10:54:14.880393: step 8600, loss 0.00594276, acc 1

Evaluation:
2016-09-06T10:54:18.654679: step 8600, loss 2.12186, acc 0.754221

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-8600

2016-09-06T10:54:20.577415: step 8601, loss 0.00980437, acc 1
2016-09-06T10:54:21.379518: step 8602, loss 0.00366338, acc 1
2016-09-06T10:54:22.183865: step 8603, loss 0.00528941, acc 1
2016-09-06T10:54:23.019442: step 8604, loss 0.0294863, acc 1
2016-09-06T10:54:23.827583: step 8605, loss 0.00889932, acc 1
2016-09-06T10:54:24.626126: step 8606, loss 0.0139252, acc 1
2016-09-06T10:54:25.431827: step 8607, loss 0.0195948, acc 0.98
2016-09-06T10:54:26.213128: step 8608, loss 0.0145943, acc 1
2016-09-06T10:54:27.042130: step 8609, loss 0.0172236, acc 1
2016-09-06T10:54:27.863475: step 8610, loss 0.00807617, acc 1
2016-09-06T10:54:28.695272: step 8611, loss 0.0028477, acc 1
2016-09-06T10:54:29.508781: step 8612, loss 0.00234482, acc 1
2016-09-06T10:54:30.324643: step 8613, loss 0.0231794, acc 1
2016-09-06T10:54:31.143492: step 8614, loss 0.0285377, acc 0.98
2016-09-06T10:54:31.934935: step 8615, loss 0.0166519, acc 1
2016-09-06T10:54:32.726008: step 8616, loss 0.0140737, acc 1
2016-09-06T10:54:33.547802: step 8617, loss 0.0485734, acc 0.98
2016-09-06T10:54:34.373665: step 8618, loss 0.0189703, acc 1
2016-09-06T10:54:35.211991: step 8619, loss 0.00278379, acc 1
2016-09-06T10:54:36.027035: step 8620, loss 0.00951791, acc 1
2016-09-06T10:54:36.816854: step 8621, loss 0.00264653, acc 1
2016-09-06T10:54:37.595360: step 8622, loss 0.0386246, acc 0.98
2016-09-06T10:54:38.422927: step 8623, loss 0.0244951, acc 1
2016-09-06T10:54:39.198950: step 8624, loss 0.0102878, acc 1
2016-09-06T10:54:40.001862: step 8625, loss 0.00314995, acc 1
2016-09-06T10:54:40.812577: step 8626, loss 0.0274047, acc 1
2016-09-06T10:54:41.596687: step 8627, loss 0.0436325, acc 0.98
2016-09-06T10:54:42.398411: step 8628, loss 0.0268741, acc 1
2016-09-06T10:54:43.213777: step 8629, loss 0.0159924, acc 1
2016-09-06T10:54:43.978120: step 8630, loss 0.0707335, acc 0.96
2016-09-06T10:54:44.784454: step 8631, loss 0.00306829, acc 1
2016-09-06T10:54:45.606893: step 8632, loss 0.00313923, acc 1
2016-09-06T10:54:46.405497: step 8633, loss 0.0379274, acc 0.98
2016-09-06T10:54:47.226769: step 8634, loss 0.0146152, acc 1
2016-09-06T10:54:48.044463: step 8635, loss 0.0375757, acc 0.98
2016-09-06T10:54:48.853164: step 8636, loss 0.0123897, acc 1
2016-09-06T10:54:49.643421: step 8637, loss 0.0729127, acc 0.96
2016-09-06T10:54:50.451399: step 8638, loss 0.0103716, acc 1
2016-09-06T10:54:51.239011: step 8639, loss 0.00381464, acc 1
2016-09-06T10:54:51.983222: step 8640, loss 0.00855296, acc 1
2016-09-06T10:54:52.782472: step 8641, loss 0.0147109, acc 1
2016-09-06T10:54:53.585437: step 8642, loss 0.168559, acc 0.96
2016-09-06T10:54:54.409695: step 8643, loss 0.00212887, acc 1
2016-09-06T10:54:55.231264: step 8644, loss 0.019878, acc 1
2016-09-06T10:54:56.039117: step 8645, loss 0.0203492, acc 1
2016-09-06T10:54:56.853920: step 8646, loss 0.0190853, acc 1
2016-09-06T10:54:57.656707: step 8647, loss 0.00709998, acc 1
2016-09-06T10:54:58.457426: step 8648, loss 0.0281419, acc 0.98
2016-09-06T10:54:59.273414: step 8649, loss 0.0550893, acc 0.96
2016-09-06T10:55:00.069772: step 8650, loss 0.0711107, acc 0.96
2016-09-06T10:55:00.917164: step 8651, loss 0.0033383, acc 1
2016-09-06T10:55:01.731475: step 8652, loss 0.0209959, acc 1
2016-09-06T10:55:02.592123: step 8653, loss 0.0462927, acc 0.96
2016-09-06T10:55:03.365540: step 8654, loss 0.0179173, acc 1
2016-09-06T10:55:04.163142: step 8655, loss 0.0402322, acc 0.98
2016-09-06T10:55:05.014354: step 8656, loss 0.00434901, acc 1
2016-09-06T10:55:05.810641: step 8657, loss 0.0184879, acc 1
2016-09-06T10:55:06.591101: step 8658, loss 0.00434814, acc 1
2016-09-06T10:55:07.435515: step 8659, loss 0.0111925, acc 1
2016-09-06T10:55:08.224755: step 8660, loss 0.00204108, acc 1
2016-09-06T10:55:08.999441: step 8661, loss 0.0123048, acc 1
2016-09-06T10:55:09.826731: step 8662, loss 0.00238767, acc 1
2016-09-06T10:55:10.633472: step 8663, loss 0.0283568, acc 1
2016-09-06T10:55:11.448191: step 8664, loss 0.0266789, acc 1
2016-09-06T10:55:12.255833: step 8665, loss 0.0117453, acc 1
2016-09-06T10:55:13.027460: step 8666, loss 0.00286757, acc 1
2016-09-06T10:55:13.819544: step 8667, loss 0.00399681, acc 1
2016-09-06T10:55:14.629637: step 8668, loss 0.0020929, acc 1
2016-09-06T10:55:15.434624: step 8669, loss 0.0763215, acc 0.98
2016-09-06T10:55:16.256388: step 8670, loss 0.0321684, acc 0.98
2016-09-06T10:55:17.084174: step 8671, loss 0.0198123, acc 0.98
2016-09-06T10:55:17.871588: step 8672, loss 0.00219815, acc 1
2016-09-06T10:55:18.668048: step 8673, loss 0.054566, acc 0.96
2016-09-06T10:55:19.478123: step 8674, loss 0.0403313, acc 0.98
2016-09-06T10:55:20.291049: step 8675, loss 0.00247198, acc 1
2016-09-06T10:55:21.113415: step 8676, loss 0.00823965, acc 1
2016-09-06T10:55:21.944240: step 8677, loss 0.020712, acc 1
2016-09-06T10:55:22.764363: step 8678, loss 0.0119884, acc 1
2016-09-06T10:55:23.561257: step 8679, loss 0.00639877, acc 1
2016-09-06T10:55:24.369304: step 8680, loss 0.0100638, acc 1
2016-09-06T10:55:25.160091: step 8681, loss 0.00537192, acc 1
2016-09-06T10:55:25.933513: step 8682, loss 0.0596775, acc 0.98
2016-09-06T10:55:26.767053: step 8683, loss 0.044943, acc 0.96
2016-09-06T10:55:27.512148: step 8684, loss 0.00235846, acc 1
2016-09-06T10:55:28.337829: step 8685, loss 0.00339656, acc 1
2016-09-06T10:55:29.148345: step 8686, loss 0.182397, acc 0.94
2016-09-06T10:55:29.928483: step 8687, loss 0.00232748, acc 1
2016-09-06T10:55:30.740379: step 8688, loss 0.0894592, acc 0.96
2016-09-06T10:55:31.565335: step 8689, loss 0.0177296, acc 1
2016-09-06T10:55:32.349452: step 8690, loss 0.0217742, acc 0.98
2016-09-06T10:55:33.168325: step 8691, loss 0.00210216, acc 1
2016-09-06T10:55:34.003598: step 8692, loss 0.0930395, acc 0.98
2016-09-06T10:55:34.806288: step 8693, loss 0.0300884, acc 0.98
2016-09-06T10:55:35.638397: step 8694, loss 0.0385654, acc 1
2016-09-06T10:55:36.451974: step 8695, loss 0.0971324, acc 0.96
2016-09-06T10:55:37.226880: step 8696, loss 0.0201469, acc 1
2016-09-06T10:55:38.049586: step 8697, loss 0.0147083, acc 1
2016-09-06T10:55:38.862904: step 8698, loss 0.0353284, acc 0.98
2016-09-06T10:55:39.624598: step 8699, loss 0.00364351, acc 1
2016-09-06T10:55:40.438910: step 8700, loss 0.0112254, acc 1

Evaluation:
2016-09-06T10:55:44.191156: step 8700, loss 1.39671, acc 0.751407

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-8700

2016-09-06T10:55:46.062975: step 8701, loss 0.0621946, acc 0.98
2016-09-06T10:55:46.924961: step 8702, loss 0.0200218, acc 1
2016-09-06T10:55:47.766796: step 8703, loss 0.0392426, acc 0.98
2016-09-06T10:55:48.606102: step 8704, loss 0.0256957, acc 1
2016-09-06T10:55:49.415590: step 8705, loss 0.0513625, acc 0.98
2016-09-06T10:55:50.231147: step 8706, loss 0.0456712, acc 0.98
2016-09-06T10:55:51.086464: step 8707, loss 0.0208126, acc 1
2016-09-06T10:55:51.865810: step 8708, loss 0.0539069, acc 0.98
2016-09-06T10:55:52.675136: step 8709, loss 0.0194735, acc 1
2016-09-06T10:55:53.480077: step 8710, loss 0.00871281, acc 1
2016-09-06T10:55:54.300558: step 8711, loss 0.0383382, acc 0.98
2016-09-06T10:55:55.107520: step 8712, loss 0.00550792, acc 1
2016-09-06T10:55:55.929319: step 8713, loss 0.0210002, acc 0.98
2016-09-06T10:55:56.740483: step 8714, loss 0.00415913, acc 1
2016-09-06T10:55:57.537287: step 8715, loss 0.00323595, acc 1
2016-09-06T10:55:58.360310: step 8716, loss 0.0206169, acc 0.98
2016-09-06T10:55:59.156032: step 8717, loss 0.00347572, acc 1
2016-09-06T10:55:59.971600: step 8718, loss 0.0209334, acc 1
2016-09-06T10:56:00.847734: step 8719, loss 0.00352966, acc 1
2016-09-06T10:56:01.665398: step 8720, loss 0.0146551, acc 1
2016-09-06T10:56:02.470303: step 8721, loss 0.00491417, acc 1
2016-09-06T10:56:03.302399: step 8722, loss 0.0180821, acc 0.98
2016-09-06T10:56:04.123417: step 8723, loss 0.0391098, acc 0.96
2016-09-06T10:56:04.921671: step 8724, loss 0.0100025, acc 1
2016-09-06T10:56:05.768503: step 8725, loss 0.0121379, acc 1
2016-09-06T10:56:06.594857: step 8726, loss 0.0357193, acc 0.96
2016-09-06T10:56:07.390811: step 8727, loss 0.00676851, acc 1
2016-09-06T10:56:08.232131: step 8728, loss 0.00338843, acc 1
2016-09-06T10:56:09.069815: step 8729, loss 0.0112433, acc 1
2016-09-06T10:56:09.909394: step 8730, loss 0.00371392, acc 1
2016-09-06T10:56:10.706566: step 8731, loss 0.009996, acc 1
2016-09-06T10:56:11.551627: step 8732, loss 0.00343357, acc 1
2016-09-06T10:56:12.353236: step 8733, loss 0.0039851, acc 1
2016-09-06T10:56:13.155958: step 8734, loss 0.00334093, acc 1
2016-09-06T10:56:13.967181: step 8735, loss 0.00758187, acc 1
2016-09-06T10:56:14.785007: step 8736, loss 0.00331404, acc 1
2016-09-06T10:56:15.599463: step 8737, loss 0.00330969, acc 1
2016-09-06T10:56:16.406176: step 8738, loss 0.0588775, acc 0.96
2016-09-06T10:56:17.220031: step 8739, loss 0.0047079, acc 1
2016-09-06T10:56:18.041114: step 8740, loss 0.00599984, acc 1
2016-09-06T10:56:18.899975: step 8741, loss 0.00374248, acc 1
2016-09-06T10:56:19.695576: step 8742, loss 0.0765498, acc 0.98
2016-09-06T10:56:20.480712: step 8743, loss 0.00312201, acc 1
2016-09-06T10:56:21.322733: step 8744, loss 0.105451, acc 0.98
2016-09-06T10:56:22.127268: step 8745, loss 0.0280948, acc 0.98
2016-09-06T10:56:22.942834: step 8746, loss 0.0030806, acc 1
2016-09-06T10:56:23.758185: step 8747, loss 0.0391786, acc 1
2016-09-06T10:56:24.577260: step 8748, loss 0.0254728, acc 0.98
2016-09-06T10:56:25.389752: step 8749, loss 0.0354991, acc 0.98
2016-09-06T10:56:26.221072: step 8750, loss 0.0991796, acc 0.96
2016-09-06T10:56:27.016755: step 8751, loss 0.0340852, acc 0.98
2016-09-06T10:56:27.820440: step 8752, loss 0.0342257, acc 0.98
2016-09-06T10:56:28.643707: step 8753, loss 0.0197646, acc 1
2016-09-06T10:56:29.438664: step 8754, loss 0.0293004, acc 1
2016-09-06T10:56:30.241355: step 8755, loss 0.0164358, acc 1
2016-09-06T10:56:31.094364: step 8756, loss 0.00764238, acc 1
2016-09-06T10:56:31.905303: step 8757, loss 0.00711298, acc 1
2016-09-06T10:56:32.721929: step 8758, loss 0.0115681, acc 1
2016-09-06T10:56:33.566960: step 8759, loss 0.0173965, acc 1
2016-09-06T10:56:34.415030: step 8760, loss 0.00436429, acc 1
2016-09-06T10:56:35.210207: step 8761, loss 0.0300553, acc 0.98
2016-09-06T10:56:36.038592: step 8762, loss 0.0126348, acc 1
2016-09-06T10:56:36.853429: step 8763, loss 0.0328996, acc 0.96
2016-09-06T10:56:37.659813: step 8764, loss 0.0325585, acc 0.98
2016-09-06T10:56:38.495291: step 8765, loss 0.00832707, acc 1
2016-09-06T10:56:39.301233: step 8766, loss 0.0208615, acc 1
2016-09-06T10:56:40.107266: step 8767, loss 0.0128991, acc 1
2016-09-06T10:56:40.926594: step 8768, loss 0.0347411, acc 0.98
2016-09-06T10:56:41.783094: step 8769, loss 0.00554671, acc 1
2016-09-06T10:56:42.569488: step 8770, loss 0.00909081, acc 1
2016-09-06T10:56:43.386600: step 8771, loss 0.00453999, acc 1
2016-09-06T10:56:44.208186: step 8772, loss 0.0127648, acc 1
2016-09-06T10:56:45.005676: step 8773, loss 0.0225283, acc 0.98
2016-09-06T10:56:45.800630: step 8774, loss 0.108437, acc 0.96
2016-09-06T10:56:46.622749: step 8775, loss 0.0461134, acc 0.98
2016-09-06T10:56:47.431794: step 8776, loss 0.0290666, acc 0.98
2016-09-06T10:56:48.244906: step 8777, loss 0.0174811, acc 1
2016-09-06T10:56:49.074750: step 8778, loss 0.106912, acc 0.96
2016-09-06T10:56:49.872914: step 8779, loss 0.00630385, acc 1
2016-09-06T10:56:50.687993: step 8780, loss 0.00383958, acc 1
2016-09-06T10:56:51.557276: step 8781, loss 0.0538808, acc 0.98
2016-09-06T10:56:52.372604: step 8782, loss 0.00295646, acc 1
2016-09-06T10:56:53.173921: step 8783, loss 0.0107496, acc 1
2016-09-06T10:56:54.003104: step 8784, loss 0.022539, acc 1
2016-09-06T10:56:54.813538: step 8785, loss 0.0336488, acc 0.98
2016-09-06T10:56:55.631609: step 8786, loss 0.0216271, acc 0.98
2016-09-06T10:56:56.491795: step 8787, loss 0.0293762, acc 0.98
2016-09-06T10:56:57.289563: step 8788, loss 0.0182464, acc 1
2016-09-06T10:56:58.084893: step 8789, loss 0.0133389, acc 1
2016-09-06T10:56:58.921254: step 8790, loss 0.0150752, acc 1
2016-09-06T10:56:59.753288: step 8791, loss 0.00722654, acc 1
2016-09-06T10:57:00.585386: step 8792, loss 0.00379002, acc 1
2016-09-06T10:57:01.408448: step 8793, loss 0.0139252, acc 1
2016-09-06T10:57:02.191238: step 8794, loss 0.0470587, acc 0.96
2016-09-06T10:57:02.999342: step 8795, loss 0.0298168, acc 0.98
2016-09-06T10:57:03.839237: step 8796, loss 0.0307701, acc 1
2016-09-06T10:57:04.681497: step 8797, loss 0.0217281, acc 0.98
2016-09-06T10:57:05.509971: step 8798, loss 0.0488319, acc 0.96
2016-09-06T10:57:06.317623: step 8799, loss 0.0073345, acc 1
2016-09-06T10:57:07.124166: step 8800, loss 0.0392044, acc 0.96

Evaluation:
2016-09-06T10:57:10.825941: step 8800, loss 2.21892, acc 0.746717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-8800

2016-09-06T10:57:12.725280: step 8801, loss 0.0237331, acc 1
2016-09-06T10:57:13.544233: step 8802, loss 0.0104632, acc 1
2016-09-06T10:57:14.341992: step 8803, loss 0.014331, acc 1
2016-09-06T10:57:15.140653: step 8804, loss 0.0287322, acc 0.98
2016-09-06T10:57:15.968583: step 8805, loss 0.0100921, acc 1
2016-09-06T10:57:16.771276: step 8806, loss 0.062545, acc 0.94
2016-09-06T10:57:17.594634: step 8807, loss 0.00289543, acc 1
2016-09-06T10:57:18.429490: step 8808, loss 0.013607, acc 1
2016-09-06T10:57:19.271168: step 8809, loss 0.030392, acc 1
2016-09-06T10:57:20.097757: step 8810, loss 0.0159416, acc 1
2016-09-06T10:57:20.949407: step 8811, loss 0.00448312, acc 1
2016-09-06T10:57:21.778090: step 8812, loss 0.00312286, acc 1
2016-09-06T10:57:22.650009: step 8813, loss 0.0157015, acc 1
2016-09-06T10:57:23.481083: step 8814, loss 0.0174564, acc 0.98
2016-09-06T10:57:24.308244: step 8815, loss 0.026443, acc 0.98
2016-09-06T10:57:25.131262: step 8816, loss 0.00308346, acc 1
2016-09-06T10:57:25.950793: step 8817, loss 0.0313109, acc 0.98
2016-09-06T10:57:26.790686: step 8818, loss 0.0508074, acc 0.96
2016-09-06T10:57:27.574826: step 8819, loss 0.021988, acc 0.98
2016-09-06T10:57:28.393599: step 8820, loss 0.0326305, acc 0.98
2016-09-06T10:57:29.231226: step 8821, loss 0.00302471, acc 1
2016-09-06T10:57:30.063801: step 8822, loss 0.0190665, acc 0.98
2016-09-06T10:57:30.842006: step 8823, loss 0.0029056, acc 1
2016-09-06T10:57:31.698425: step 8824, loss 0.00284889, acc 1
2016-09-06T10:57:32.538461: step 8825, loss 0.0595256, acc 0.96
2016-09-06T10:57:33.303476: step 8826, loss 0.0323413, acc 0.98
2016-09-06T10:57:34.117868: step 8827, loss 0.0190579, acc 1
2016-09-06T10:57:34.963971: step 8828, loss 0.00605544, acc 1
2016-09-06T10:57:35.768551: step 8829, loss 0.0168934, acc 0.98
2016-09-06T10:57:36.558612: step 8830, loss 0.0101288, acc 1
2016-09-06T10:57:37.367380: step 8831, loss 0.0359318, acc 0.98
2016-09-06T10:57:38.090671: step 8832, loss 0.0144326, acc 1
2016-09-06T10:57:38.905717: step 8833, loss 0.0127351, acc 1
2016-09-06T10:57:39.725357: step 8834, loss 0.00935726, acc 1
2016-09-06T10:57:40.538248: step 8835, loss 0.00499991, acc 1
2016-09-06T10:57:41.340500: step 8836, loss 0.0653617, acc 0.98
2016-09-06T10:57:42.154044: step 8837, loss 0.0134809, acc 1
2016-09-06T10:57:42.919335: step 8838, loss 0.0662779, acc 0.98
2016-09-06T10:57:43.729294: step 8839, loss 0.0155838, acc 1
2016-09-06T10:57:44.580857: step 8840, loss 0.00449828, acc 1
2016-09-06T10:57:45.366172: step 8841, loss 0.0745426, acc 0.96
2016-09-06T10:57:46.164632: step 8842, loss 0.0136098, acc 1
2016-09-06T10:57:46.967977: step 8843, loss 0.0128483, acc 1
2016-09-06T10:57:47.792360: step 8844, loss 0.0122419, acc 1
2016-09-06T10:57:48.602245: step 8845, loss 0.0617552, acc 0.94
2016-09-06T10:57:49.441475: step 8846, loss 0.0236932, acc 1
2016-09-06T10:57:50.286170: step 8847, loss 0.0213711, acc 1
2016-09-06T10:57:51.091152: step 8848, loss 0.00204574, acc 1
2016-09-06T10:57:51.923799: step 8849, loss 0.0281365, acc 0.98
2016-09-06T10:57:52.751993: step 8850, loss 0.00210554, acc 1
2016-09-06T10:57:53.589616: step 8851, loss 0.0135244, acc 1
2016-09-06T10:57:54.428310: step 8852, loss 0.0138666, acc 1
2016-09-06T10:57:55.278262: step 8853, loss 0.0020439, acc 1
2016-09-06T10:57:56.110678: step 8854, loss 0.0181606, acc 1
2016-09-06T10:57:56.958356: step 8855, loss 0.00223025, acc 1
2016-09-06T10:57:57.784354: step 8856, loss 0.0407981, acc 0.96
2016-09-06T10:57:58.613412: step 8857, loss 0.00768627, acc 1
2016-09-06T10:57:59.446176: step 8858, loss 0.0113032, acc 1
2016-09-06T10:58:00.276409: step 8859, loss 0.0161604, acc 1
2016-09-06T10:58:01.054388: step 8860, loss 0.0046621, acc 1
2016-09-06T10:58:01.871668: step 8861, loss 0.00587544, acc 1
2016-09-06T10:58:02.663800: step 8862, loss 0.0528538, acc 0.98
2016-09-06T10:58:03.474989: step 8863, loss 0.00233516, acc 1
2016-09-06T10:58:04.311297: step 8864, loss 0.0102021, acc 1
2016-09-06T10:58:05.128111: step 8865, loss 0.00570516, acc 1
2016-09-06T10:58:05.934344: step 8866, loss 0.0468163, acc 0.98
2016-09-06T10:58:06.757437: step 8867, loss 0.0727113, acc 0.96
2016-09-06T10:58:07.584931: step 8868, loss 0.00216087, acc 1
2016-09-06T10:58:08.348103: step 8869, loss 0.26848, acc 0.98
2016-09-06T10:58:09.200789: step 8870, loss 0.0281521, acc 1
2016-09-06T10:58:10.008352: step 8871, loss 0.0491382, acc 0.98
2016-09-06T10:58:10.799520: step 8872, loss 0.00270909, acc 1
2016-09-06T10:58:11.586110: step 8873, loss 0.0080611, acc 1
2016-09-06T10:58:12.414700: step 8874, loss 0.0156601, acc 1
2016-09-06T10:58:13.220787: step 8875, loss 0.0139362, acc 1
2016-09-06T10:58:14.023714: step 8876, loss 0.0161391, acc 1
2016-09-06T10:58:14.851094: step 8877, loss 0.00815974, acc 1
2016-09-06T10:58:15.637356: step 8878, loss 0.0180479, acc 1
2016-09-06T10:58:16.440793: step 8879, loss 0.0121341, acc 1
2016-09-06T10:58:17.270819: step 8880, loss 0.00581971, acc 1
2016-09-06T10:58:18.096594: step 8881, loss 0.0648339, acc 0.96
2016-09-06T10:58:18.922875: step 8882, loss 0.00387649, acc 1
2016-09-06T10:58:19.766660: step 8883, loss 0.0315091, acc 0.96
2016-09-06T10:58:20.575634: step 8884, loss 0.0230319, acc 0.98
2016-09-06T10:58:21.369564: step 8885, loss 0.0125601, acc 1
2016-09-06T10:58:22.178902: step 8886, loss 0.00396151, acc 1
2016-09-06T10:58:22.993614: step 8887, loss 0.00702739, acc 1
2016-09-06T10:58:23.824545: step 8888, loss 0.0145097, acc 1
2016-09-06T10:58:24.660732: step 8889, loss 0.0382793, acc 0.96
2016-09-06T10:58:25.469488: step 8890, loss 0.0415083, acc 0.98
2016-09-06T10:58:26.282406: step 8891, loss 0.0026967, acc 1
2016-09-06T10:58:27.153819: step 8892, loss 0.00371661, acc 1
2016-09-06T10:58:27.964710: step 8893, loss 0.0113993, acc 1
2016-09-06T10:58:28.767110: step 8894, loss 0.0344154, acc 1
2016-09-06T10:58:29.615342: step 8895, loss 0.00535434, acc 1
2016-09-06T10:58:30.421044: step 8896, loss 0.0156554, acc 1
2016-09-06T10:58:31.225585: step 8897, loss 0.0220186, acc 0.98
2016-09-06T10:58:32.070954: step 8898, loss 0.0479135, acc 0.96
2016-09-06T10:58:32.876643: step 8899, loss 0.00560342, acc 1
2016-09-06T10:58:33.671111: step 8900, loss 0.0457419, acc 0.96

Evaluation:
2016-09-06T10:58:37.385639: step 8900, loss 2.12153, acc 0.736398

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-8900

2016-09-06T10:58:39.292450: step 8901, loss 0.0481452, acc 1
2016-09-06T10:58:40.102836: step 8902, loss 0.00415521, acc 1
2016-09-06T10:58:40.914436: step 8903, loss 0.00359243, acc 1
2016-09-06T10:58:41.761950: step 8904, loss 0.011615, acc 1
2016-09-06T10:58:42.565610: step 8905, loss 0.00609431, acc 1
2016-09-06T10:58:43.375380: step 8906, loss 0.0142495, acc 1
2016-09-06T10:58:44.199239: step 8907, loss 0.0254049, acc 0.98
2016-09-06T10:58:45.011381: step 8908, loss 0.0444259, acc 0.98
2016-09-06T10:58:45.847619: step 8909, loss 0.00567621, acc 1
2016-09-06T10:58:46.651304: step 8910, loss 0.0237995, acc 1
2016-09-06T10:58:47.505688: step 8911, loss 0.0641337, acc 0.96
2016-09-06T10:58:48.319217: step 8912, loss 0.00376313, acc 1
2016-09-06T10:58:49.134457: step 8913, loss 0.00320227, acc 1
2016-09-06T10:58:49.991008: step 8914, loss 0.00561216, acc 1
2016-09-06T10:58:50.777411: step 8915, loss 0.094121, acc 0.96
2016-09-06T10:58:51.580193: step 8916, loss 0.00360622, acc 1
2016-09-06T10:58:52.437013: step 8917, loss 0.0178497, acc 1
2016-09-06T10:58:53.249362: step 8918, loss 0.00346372, acc 1
2016-09-06T10:58:54.067221: step 8919, loss 0.00311208, acc 1
2016-09-06T10:58:54.902741: step 8920, loss 0.0328492, acc 0.98
2016-09-06T10:58:55.720011: step 8921, loss 0.0172077, acc 1
2016-09-06T10:58:56.545464: step 8922, loss 0.00302845, acc 1
2016-09-06T10:58:57.373485: step 8923, loss 0.0403091, acc 0.98
2016-09-06T10:58:58.185885: step 8924, loss 0.0219016, acc 0.98
2016-09-06T10:58:59.024511: step 8925, loss 0.00496432, acc 1
2016-09-06T10:58:59.865408: step 8926, loss 0.00330125, acc 1
2016-09-06T10:59:00.728137: step 8927, loss 0.0148155, acc 1
2016-09-06T10:59:01.562635: step 8928, loss 0.0215906, acc 0.98
2016-09-06T10:59:02.374773: step 8929, loss 0.0250943, acc 0.98
2016-09-06T10:59:03.196984: step 8930, loss 0.0809449, acc 0.98
2016-09-06T10:59:03.982573: step 8931, loss 0.0186777, acc 0.98
2016-09-06T10:59:04.817230: step 8932, loss 0.00280754, acc 1
2016-09-06T10:59:05.630717: step 8933, loss 0.0604247, acc 0.98
2016-09-06T10:59:06.429164: step 8934, loss 0.00284398, acc 1
2016-09-06T10:59:07.235833: step 8935, loss 0.0155823, acc 1
2016-09-06T10:59:08.046309: step 8936, loss 0.00387412, acc 1
2016-09-06T10:59:08.840543: step 8937, loss 0.0299393, acc 1
2016-09-06T10:59:09.645242: step 8938, loss 0.00312916, acc 1
2016-09-06T10:59:10.466608: step 8939, loss 0.017368, acc 1
2016-09-06T10:59:11.248774: step 8940, loss 0.0185211, acc 1
2016-09-06T10:59:12.042941: step 8941, loss 0.206667, acc 0.94
2016-09-06T10:59:12.869944: step 8942, loss 0.0356385, acc 1
2016-09-06T10:59:13.651634: step 8943, loss 0.00287018, acc 1
2016-09-06T10:59:14.458099: step 8944, loss 0.118376, acc 0.96
2016-09-06T10:59:15.265405: step 8945, loss 0.0156314, acc 1
2016-09-06T10:59:16.031632: step 8946, loss 0.00270885, acc 1
2016-09-06T10:59:16.841142: step 8947, loss 0.0285067, acc 1
2016-09-06T10:59:17.655397: step 8948, loss 0.0226003, acc 0.98
2016-09-06T10:59:18.436772: step 8949, loss 0.00246428, acc 1
2016-09-06T10:59:19.234286: step 8950, loss 0.00285504, acc 1
2016-09-06T10:59:20.057099: step 8951, loss 0.0072216, acc 1
2016-09-06T10:59:20.855502: step 8952, loss 0.00834237, acc 1
2016-09-06T10:59:21.656108: step 8953, loss 0.0107737, acc 1
2016-09-06T10:59:22.474733: step 8954, loss 0.00234111, acc 1
2016-09-06T10:59:23.257502: step 8955, loss 0.00446331, acc 1
2016-09-06T10:59:24.043693: step 8956, loss 0.0147176, acc 1
2016-09-06T10:59:24.839575: step 8957, loss 0.0258834, acc 1
2016-09-06T10:59:25.711827: step 8958, loss 0.00466221, acc 1
2016-09-06T10:59:26.528584: step 8959, loss 0.00780562, acc 1
2016-09-06T10:59:27.352707: step 8960, loss 0.00805926, acc 1
2016-09-06T10:59:28.149549: step 8961, loss 0.0141422, acc 1
2016-09-06T10:59:28.931774: step 8962, loss 0.0216595, acc 1
2016-09-06T10:59:29.771252: step 8963, loss 0.0134945, acc 1
2016-09-06T10:59:30.564323: step 8964, loss 0.00720956, acc 1
2016-09-06T10:59:31.381240: step 8965, loss 0.00549261, acc 1
2016-09-06T10:59:32.160951: step 8966, loss 0.0269664, acc 1
2016-09-06T10:59:32.943534: step 8967, loss 0.0246514, acc 1
2016-09-06T10:59:33.803097: step 8968, loss 0.0106659, acc 1
2016-09-06T10:59:34.619486: step 8969, loss 0.0277601, acc 0.98
2016-09-06T10:59:35.391228: step 8970, loss 0.0292347, acc 1
2016-09-06T10:59:36.186875: step 8971, loss 0.0438017, acc 0.98
2016-09-06T10:59:37.017271: step 8972, loss 0.00657388, acc 1
2016-09-06T10:59:37.805396: step 8973, loss 0.0328853, acc 0.98
2016-09-06T10:59:38.582564: step 8974, loss 0.0303312, acc 0.98
2016-09-06T10:59:39.427581: step 8975, loss 0.017324, acc 1
2016-09-06T10:59:40.225411: step 8976, loss 0.0409483, acc 1
2016-09-06T10:59:41.083519: step 8977, loss 0.00320907, acc 1
2016-09-06T10:59:41.910522: step 8978, loss 0.0146206, acc 1
2016-09-06T10:59:42.745408: step 8979, loss 0.00392593, acc 1
2016-09-06T10:59:43.560674: step 8980, loss 0.00397037, acc 1
2016-09-06T10:59:44.399271: step 8981, loss 0.0200166, acc 1
2016-09-06T10:59:45.207997: step 8982, loss 0.0179231, acc 0.98
2016-09-06T10:59:46.017783: step 8983, loss 0.0064146, acc 1
2016-09-06T10:59:46.843908: step 8984, loss 0.00381641, acc 1
2016-09-06T10:59:47.677735: step 8985, loss 0.017875, acc 1
2016-09-06T10:59:48.501718: step 8986, loss 0.00417248, acc 1
2016-09-06T10:59:49.334662: step 8987, loss 0.00407642, acc 1
2016-09-06T10:59:50.154449: step 8988, loss 0.0081981, acc 1
2016-09-06T10:59:50.972923: step 8989, loss 0.00307395, acc 1
2016-09-06T10:59:51.824194: step 8990, loss 0.0384276, acc 0.98
2016-09-06T10:59:52.607106: step 8991, loss 0.00283218, acc 1
2016-09-06T10:59:53.418509: step 8992, loss 0.0958217, acc 0.96
2016-09-06T10:59:54.250745: step 8993, loss 0.0183706, acc 1
2016-09-06T10:59:55.033401: step 8994, loss 0.0467084, acc 0.96
2016-09-06T10:59:55.863759: step 8995, loss 0.235174, acc 0.98
2016-09-06T10:59:56.685808: step 8996, loss 0.0026211, acc 1
2016-09-06T10:59:57.501094: step 8997, loss 0.0346568, acc 0.98
2016-09-06T10:59:58.309393: step 8998, loss 0.0293259, acc 1
2016-09-06T10:59:59.139790: step 8999, loss 0.0204818, acc 0.98
2016-09-06T10:59:59.968690: step 9000, loss 0.0602147, acc 0.98

Evaluation:
2016-09-06T11:00:03.678723: step 9000, loss 1.56942, acc 0.743902

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-9000

2016-09-06T11:00:05.581007: step 9001, loss 0.0821018, acc 0.96
2016-09-06T11:00:06.419174: step 9002, loss 0.0472545, acc 0.98
2016-09-06T11:00:07.247497: step 9003, loss 0.0555367, acc 0.98
2016-09-06T11:00:08.076119: step 9004, loss 0.00936789, acc 1
2016-09-06T11:00:08.909994: step 9005, loss 0.0105649, acc 1
2016-09-06T11:00:09.732173: step 9006, loss 0.0179553, acc 1
2016-09-06T11:00:10.576040: step 9007, loss 0.0571627, acc 0.96
2016-09-06T11:00:11.384513: step 9008, loss 0.00880191, acc 1
2016-09-06T11:00:12.177409: step 9009, loss 0.0205783, acc 0.98
2016-09-06T11:00:12.960000: step 9010, loss 0.0451863, acc 0.98
2016-09-06T11:00:13.775881: step 9011, loss 0.0356425, acc 0.98
2016-09-06T11:00:14.620747: step 9012, loss 0.0485958, acc 0.98
2016-09-06T11:00:15.385104: step 9013, loss 0.0134902, acc 1
2016-09-06T11:00:16.220338: step 9014, loss 0.0418882, acc 0.98
2016-09-06T11:00:17.031323: step 9015, loss 0.00409594, acc 1
2016-09-06T11:00:17.848104: step 9016, loss 0.0455015, acc 0.96
2016-09-06T11:00:18.656651: step 9017, loss 0.0730327, acc 0.96
2016-09-06T11:00:19.471862: step 9018, loss 0.0190079, acc 0.98
2016-09-06T11:00:20.283043: step 9019, loss 0.0497466, acc 0.96
2016-09-06T11:00:21.096188: step 9020, loss 0.00394931, acc 1
2016-09-06T11:00:21.937020: step 9021, loss 0.0644681, acc 0.96
2016-09-06T11:00:22.775465: step 9022, loss 0.0586477, acc 0.94
2016-09-06T11:00:23.556453: step 9023, loss 0.00587154, acc 1
2016-09-06T11:00:24.335010: step 9024, loss 0.00653678, acc 1
2016-09-06T11:00:25.132844: step 9025, loss 0.0167189, acc 1
2016-09-06T11:00:25.936547: step 9026, loss 0.0287998, acc 1
2016-09-06T11:00:26.751283: step 9027, loss 0.00362938, acc 1
2016-09-06T11:00:27.549494: step 9028, loss 0.00734844, acc 1
2016-09-06T11:00:28.377539: step 9029, loss 0.0149221, acc 1
2016-09-06T11:00:29.205354: step 9030, loss 0.00745656, acc 1
2016-09-06T11:00:30.013891: step 9031, loss 0.0258634, acc 1
2016-09-06T11:00:30.827753: step 9032, loss 0.0175516, acc 1
2016-09-06T11:00:31.662774: step 9033, loss 0.0191257, acc 0.98
2016-09-06T11:00:32.464280: step 9034, loss 0.0465144, acc 0.98
2016-09-06T11:00:33.279048: step 9035, loss 0.00578267, acc 1
2016-09-06T11:00:34.104252: step 9036, loss 0.00448427, acc 1
2016-09-06T11:00:34.903029: step 9037, loss 0.0144155, acc 1
2016-09-06T11:00:35.707077: step 9038, loss 0.0385376, acc 0.96
2016-09-06T11:00:36.557897: step 9039, loss 0.0645051, acc 0.98
2016-09-06T11:00:37.348127: step 9040, loss 0.0319783, acc 0.98
2016-09-06T11:00:38.158283: step 9041, loss 0.0208946, acc 0.98
2016-09-06T11:00:39.005774: step 9042, loss 0.00344073, acc 1
2016-09-06T11:00:39.812030: step 9043, loss 0.01778, acc 0.98
2016-09-06T11:00:40.614398: step 9044, loss 0.00336645, acc 1
2016-09-06T11:00:41.447018: step 9045, loss 0.00345373, acc 1
2016-09-06T11:00:42.262559: step 9046, loss 0.0829964, acc 0.98
2016-09-06T11:00:43.056447: step 9047, loss 0.00325852, acc 1
2016-09-06T11:00:43.886057: step 9048, loss 0.0204239, acc 1
2016-09-06T11:00:44.696677: step 9049, loss 0.0181719, acc 1
2016-09-06T11:00:45.507453: step 9050, loss 0.0163092, acc 1
2016-09-06T11:00:46.353386: step 9051, loss 0.0122067, acc 1
2016-09-06T11:00:47.130904: step 9052, loss 0.00309562, acc 1
2016-09-06T11:00:47.931316: step 9053, loss 0.0268365, acc 0.98
2016-09-06T11:00:48.794150: step 9054, loss 0.0928637, acc 0.98
2016-09-06T11:00:49.595695: step 9055, loss 0.00298676, acc 1
2016-09-06T11:00:50.386784: step 9056, loss 0.0194784, acc 1
2016-09-06T11:00:51.209896: step 9057, loss 0.0186631, acc 1
2016-09-06T11:00:52.027514: step 9058, loss 0.0276265, acc 1
2016-09-06T11:00:52.835110: step 9059, loss 0.00944163, acc 1
2016-09-06T11:00:53.654803: step 9060, loss 0.00571413, acc 1
2016-09-06T11:00:54.496453: step 9061, loss 0.0185895, acc 1
2016-09-06T11:00:55.302390: step 9062, loss 0.0177703, acc 0.98
2016-09-06T11:00:56.094427: step 9063, loss 0.0508586, acc 0.98
2016-09-06T11:00:56.937940: step 9064, loss 0.00631711, acc 1
2016-09-06T11:00:57.717351: step 9065, loss 0.00818771, acc 1
2016-09-06T11:00:58.523918: step 9066, loss 0.0249046, acc 1
2016-09-06T11:00:59.364021: step 9067, loss 0.0189561, acc 0.98
2016-09-06T11:01:00.162711: step 9068, loss 0.0116641, acc 1
2016-09-06T11:01:00.975263: step 9069, loss 0.00381663, acc 1
2016-09-06T11:01:01.774845: step 9070, loss 0.00331954, acc 1
2016-09-06T11:01:02.564366: step 9071, loss 0.00811483, acc 1
2016-09-06T11:01:03.373737: step 9072, loss 0.0388013, acc 0.98
2016-09-06T11:01:04.196680: step 9073, loss 0.00794101, acc 1
2016-09-06T11:01:04.977050: step 9074, loss 0.00571322, acc 1
2016-09-06T11:01:05.776929: step 9075, loss 0.00329422, acc 1
2016-09-06T11:01:06.597925: step 9076, loss 0.0321604, acc 0.98
2016-09-06T11:01:07.387804: step 9077, loss 0.00351254, acc 1
2016-09-06T11:01:08.184651: step 9078, loss 0.0256377, acc 1
2016-09-06T11:01:09.009361: step 9079, loss 0.0199211, acc 1
2016-09-06T11:01:09.786997: step 9080, loss 0.00937914, acc 1
2016-09-06T11:01:10.591449: step 9081, loss 0.00613192, acc 1
2016-09-06T11:01:11.409410: step 9082, loss 0.0692664, acc 0.98
2016-09-06T11:01:12.216273: step 9083, loss 0.00750904, acc 1
2016-09-06T11:01:13.047768: step 9084, loss 0.00335369, acc 1
2016-09-06T11:01:13.849501: step 9085, loss 0.021774, acc 1
2016-09-06T11:01:14.634190: step 9086, loss 0.0166462, acc 1
2016-09-06T11:01:15.436475: step 9087, loss 0.00689149, acc 1
2016-09-06T11:01:16.259341: step 9088, loss 0.00416783, acc 1
2016-09-06T11:01:17.014623: step 9089, loss 0.00363051, acc 1
2016-09-06T11:01:17.825872: step 9090, loss 0.0198382, acc 0.98
2016-09-06T11:01:18.659805: step 9091, loss 0.0163563, acc 1
2016-09-06T11:01:19.449371: step 9092, loss 0.0161691, acc 1
2016-09-06T11:01:20.253917: step 9093, loss 0.0186133, acc 1
2016-09-06T11:01:21.077793: step 9094, loss 0.00549019, acc 1
2016-09-06T11:01:21.868490: step 9095, loss 0.058199, acc 0.96
2016-09-06T11:01:22.678223: step 9096, loss 0.0053497, acc 1
2016-09-06T11:01:23.490052: step 9097, loss 0.0424361, acc 0.98
2016-09-06T11:01:24.287001: step 9098, loss 0.00348652, acc 1
2016-09-06T11:01:25.094035: step 9099, loss 0.0109384, acc 1
2016-09-06T11:01:25.914370: step 9100, loss 0.0118363, acc 1

Evaluation:
2016-09-06T11:01:29.655024: step 9100, loss 2.24376, acc 0.750469

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-9100

2016-09-06T11:01:31.517414: step 9101, loss 0.0029546, acc 1
2016-09-06T11:01:32.338227: step 9102, loss 0.0255287, acc 0.98
2016-09-06T11:01:33.145027: step 9103, loss 0.00592095, acc 1
2016-09-06T11:01:33.976658: step 9104, loss 0.017841, acc 1
2016-09-06T11:01:34.831089: step 9105, loss 0.00783157, acc 1
2016-09-06T11:01:35.633147: step 9106, loss 0.00549003, acc 1
2016-09-06T11:01:36.429917: step 9107, loss 0.0246419, acc 1
2016-09-06T11:01:37.251815: step 9108, loss 0.0038382, acc 1
2016-09-06T11:01:38.051237: step 9109, loss 0.10563, acc 0.98
2016-09-06T11:01:38.842374: step 9110, loss 0.0689793, acc 0.96
2016-09-06T11:01:39.658117: step 9111, loss 0.00256793, acc 1
2016-09-06T11:01:40.463733: step 9112, loss 0.0113367, acc 1
2016-09-06T11:01:41.282539: step 9113, loss 0.00920201, acc 1
2016-09-06T11:01:42.120716: step 9114, loss 0.0412258, acc 0.98
2016-09-06T11:01:42.938484: step 9115, loss 0.0184852, acc 0.98
2016-09-06T11:01:43.707427: step 9116, loss 0.0102239, acc 1
2016-09-06T11:01:44.525957: step 9117, loss 0.00701351, acc 1
2016-09-06T11:01:45.340766: step 9118, loss 0.00771678, acc 1
2016-09-06T11:01:46.132354: step 9119, loss 0.0141431, acc 1
2016-09-06T11:01:46.932285: step 9120, loss 0.00497538, acc 1
2016-09-06T11:01:47.758566: step 9121, loss 0.0186468, acc 0.98
2016-09-06T11:01:48.553686: step 9122, loss 0.0098887, acc 1
2016-09-06T11:01:49.344897: step 9123, loss 0.00289172, acc 1
2016-09-06T11:01:50.142572: step 9124, loss 0.053578, acc 0.98
2016-09-06T11:01:50.940450: step 9125, loss 0.0806366, acc 0.96
2016-09-06T11:01:51.753664: step 9126, loss 0.00286947, acc 1
2016-09-06T11:01:52.575213: step 9127, loss 0.00397725, acc 1
2016-09-06T11:01:53.368240: step 9128, loss 0.00769806, acc 1
2016-09-06T11:01:54.169848: step 9129, loss 0.0132678, acc 1
2016-09-06T11:01:54.998466: step 9130, loss 0.023475, acc 0.98
2016-09-06T11:01:55.810553: step 9131, loss 0.0139784, acc 1
2016-09-06T11:01:56.659506: step 9132, loss 0.01864, acc 0.98
2016-09-06T11:01:57.479310: step 9133, loss 0.0201116, acc 1
2016-09-06T11:01:58.297334: step 9134, loss 0.0497255, acc 0.98
2016-09-06T11:01:59.151003: step 9135, loss 0.0458212, acc 0.98
2016-09-06T11:02:00.024428: step 9136, loss 0.0464303, acc 0.98
2016-09-06T11:02:00.872360: step 9137, loss 0.10152, acc 0.98
2016-09-06T11:02:01.703971: step 9138, loss 0.0225547, acc 0.98
2016-09-06T11:02:02.527991: step 9139, loss 0.0339771, acc 0.98
2016-09-06T11:02:03.327861: step 9140, loss 0.00806915, acc 1
2016-09-06T11:02:04.135983: step 9141, loss 0.0224071, acc 0.98
2016-09-06T11:02:04.959988: step 9142, loss 0.00431003, acc 1
2016-09-06T11:02:05.776525: step 9143, loss 0.0046533, acc 1
2016-09-06T11:02:06.617561: step 9144, loss 0.0139418, acc 1
2016-09-06T11:02:07.471471: step 9145, loss 0.068697, acc 0.98
2016-09-06T11:02:08.292990: step 9146, loss 0.0115154, acc 1
2016-09-06T11:02:09.133279: step 9147, loss 0.028552, acc 0.98
2016-09-06T11:02:09.975986: step 9148, loss 0.0148352, acc 1
2016-09-06T11:02:10.806649: step 9149, loss 0.0879792, acc 0.96
2016-09-06T11:02:11.607462: step 9150, loss 0.0170924, acc 1
2016-09-06T11:02:12.414151: step 9151, loss 0.00977943, acc 1
2016-09-06T11:02:13.222033: step 9152, loss 0.0176358, acc 1
2016-09-06T11:02:14.009626: step 9153, loss 0.00297729, acc 1
2016-09-06T11:02:14.799962: step 9154, loss 0.0338434, acc 0.98
2016-09-06T11:02:15.628885: step 9155, loss 0.00736602, acc 1
2016-09-06T11:02:16.429607: step 9156, loss 0.0536612, acc 0.98
2016-09-06T11:02:17.217572: step 9157, loss 0.0187426, acc 1
2016-09-06T11:02:18.050052: step 9158, loss 0.04093, acc 0.96
2016-09-06T11:02:18.851619: step 9159, loss 0.0354703, acc 0.98
2016-09-06T11:02:19.648077: step 9160, loss 0.088195, acc 0.98
2016-09-06T11:02:20.439628: step 9161, loss 0.0224665, acc 0.98
2016-09-06T11:02:21.233044: step 9162, loss 0.0480554, acc 0.98
2016-09-06T11:02:22.069871: step 9163, loss 0.00557162, acc 1
2016-09-06T11:02:22.895121: step 9164, loss 0.00413417, acc 1
2016-09-06T11:02:23.689695: step 9165, loss 0.0617684, acc 0.96
2016-09-06T11:02:24.506853: step 9166, loss 0.011838, acc 1
2016-09-06T11:02:25.314132: step 9167, loss 0.0075752, acc 1
2016-09-06T11:02:26.136067: step 9168, loss 0.0128257, acc 1
2016-09-06T11:02:26.952842: step 9169, loss 0.0302556, acc 1
2016-09-06T11:02:27.781656: step 9170, loss 0.0671176, acc 0.98
2016-09-06T11:02:28.613448: step 9171, loss 0.0100357, acc 1
2016-09-06T11:02:29.426525: step 9172, loss 0.0106545, acc 1
2016-09-06T11:02:30.248595: step 9173, loss 0.064032, acc 0.98
2016-09-06T11:02:31.037782: step 9174, loss 0.0219279, acc 1
2016-09-06T11:02:31.837257: step 9175, loss 0.0779535, acc 0.98
2016-09-06T11:02:32.676925: step 9176, loss 0.00320752, acc 1
2016-09-06T11:02:33.494040: step 9177, loss 0.0264002, acc 0.98
2016-09-06T11:02:34.315807: step 9178, loss 0.0404316, acc 1
2016-09-06T11:02:35.170075: step 9179, loss 0.0192176, acc 1
2016-09-06T11:02:35.977399: step 9180, loss 0.0279195, acc 1
2016-09-06T11:02:36.816401: step 9181, loss 0.0221204, acc 1
2016-09-06T11:02:37.644402: step 9182, loss 0.00665668, acc 1
2016-09-06T11:02:38.470324: step 9183, loss 0.00468306, acc 1
2016-09-06T11:02:39.287455: step 9184, loss 0.0342907, acc 1
2016-09-06T11:02:40.109679: step 9185, loss 0.0209708, acc 0.98
2016-09-06T11:02:40.902875: step 9186, loss 0.0294429, acc 0.98
2016-09-06T11:02:41.689414: step 9187, loss 0.0238622, acc 0.98
2016-09-06T11:02:42.501395: step 9188, loss 0.0125985, acc 1
2016-09-06T11:02:43.313096: step 9189, loss 0.0112218, acc 1
2016-09-06T11:02:44.093839: step 9190, loss 0.0665275, acc 0.98
2016-09-06T11:02:44.929877: step 9191, loss 0.00835464, acc 1
2016-09-06T11:02:45.753255: step 9192, loss 0.00321303, acc 1
2016-09-06T11:02:46.544316: step 9193, loss 0.0451741, acc 0.96
2016-09-06T11:02:47.372859: step 9194, loss 0.0380182, acc 0.98
2016-09-06T11:02:48.216046: step 9195, loss 0.031883, acc 1
2016-09-06T11:02:49.005888: step 9196, loss 0.0770707, acc 0.96
2016-09-06T11:02:49.795013: step 9197, loss 0.00698637, acc 1
2016-09-06T11:02:50.632535: step 9198, loss 0.0337868, acc 0.98
2016-09-06T11:02:51.430760: step 9199, loss 0.108393, acc 0.98
2016-09-06T11:02:52.211450: step 9200, loss 0.00534884, acc 1

Evaluation:
2016-09-06T11:02:55.948471: step 9200, loss 1.83076, acc 0.754221

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-9200

2016-09-06T11:02:57.981821: step 9201, loss 0.0152674, acc 1
2016-09-06T11:02:58.764867: step 9202, loss 0.0190181, acc 1
2016-09-06T11:02:59.572206: step 9203, loss 0.037152, acc 0.98
2016-09-06T11:03:00.388434: step 9204, loss 0.0366547, acc 0.98
2016-09-06T11:03:01.168661: step 9205, loss 0.0331886, acc 0.98
2016-09-06T11:03:01.965474: step 9206, loss 0.0105043, acc 1
2016-09-06T11:03:02.769389: step 9207, loss 0.00477267, acc 1
2016-09-06T11:03:03.522265: step 9208, loss 0.00334521, acc 1
2016-09-06T11:03:04.352098: step 9209, loss 0.00483698, acc 1
2016-09-06T11:03:05.190413: step 9210, loss 0.0166556, acc 1
2016-09-06T11:03:05.967588: step 9211, loss 0.103624, acc 0.96
2016-09-06T11:03:06.781050: step 9212, loss 0.00321119, acc 1
2016-09-06T11:03:07.583479: step 9213, loss 0.0764925, acc 0.98
2016-09-06T11:03:08.385261: step 9214, loss 0.00731744, acc 1
2016-09-06T11:03:09.213425: step 9215, loss 0.0256752, acc 1
2016-09-06T11:03:09.977191: step 9216, loss 0.022518, acc 1
2016-09-06T11:03:10.760459: step 9217, loss 0.00447368, acc 1
2016-09-06T11:03:11.560373: step 9218, loss 0.081339, acc 0.94
2016-09-06T11:03:12.369833: step 9219, loss 0.00318498, acc 1
2016-09-06T11:03:13.184015: step 9220, loss 0.0194421, acc 1
2016-09-06T11:03:14.012536: step 9221, loss 0.0338464, acc 0.98
2016-09-06T11:03:14.865596: step 9222, loss 0.0113283, acc 1
2016-09-06T11:03:15.668027: step 9223, loss 0.04279, acc 0.98
2016-09-06T11:03:16.465446: step 9224, loss 0.0272394, acc 0.98
2016-09-06T11:03:17.292059: step 9225, loss 0.0631687, acc 0.96
2016-09-06T11:03:18.120942: step 9226, loss 0.0288918, acc 0.98
2016-09-06T11:03:18.913782: step 9227, loss 0.00964092, acc 1
2016-09-06T11:03:19.741773: step 9228, loss 0.0284639, acc 0.98
2016-09-06T11:03:20.532927: step 9229, loss 0.00633292, acc 1
2016-09-06T11:03:21.360188: step 9230, loss 0.0500881, acc 0.96
2016-09-06T11:03:22.215402: step 9231, loss 0.00754052, acc 1
2016-09-06T11:03:23.017875: step 9232, loss 0.0114403, acc 1
2016-09-06T11:03:23.824169: step 9233, loss 0.0178896, acc 1
2016-09-06T11:03:24.657861: step 9234, loss 0.00277456, acc 1
2016-09-06T11:03:25.489825: step 9235, loss 0.026873, acc 0.98
2016-09-06T11:03:26.305410: step 9236, loss 0.0438837, acc 0.98
2016-09-06T11:03:27.158585: step 9237, loss 0.0041379, acc 1
2016-09-06T11:03:27.967628: step 9238, loss 0.0295466, acc 1
2016-09-06T11:03:28.774037: step 9239, loss 0.00574, acc 1
2016-09-06T11:03:29.584008: step 9240, loss 0.0259067, acc 0.98
2016-09-06T11:03:30.406544: step 9241, loss 0.0130885, acc 1
2016-09-06T11:03:31.224609: step 9242, loss 0.0334568, acc 0.98
2016-09-06T11:03:32.042098: step 9243, loss 0.00526861, acc 1
2016-09-06T11:03:32.867706: step 9244, loss 0.0269295, acc 1
2016-09-06T11:03:33.705642: step 9245, loss 0.0028621, acc 1
2016-09-06T11:03:34.514734: step 9246, loss 0.00279349, acc 1
2016-09-06T11:03:35.328242: step 9247, loss 0.0102126, acc 1
2016-09-06T11:03:36.156176: step 9248, loss 0.00585826, acc 1
2016-09-06T11:03:36.963967: step 9249, loss 0.0160561, acc 1
2016-09-06T11:03:37.780567: step 9250, loss 0.00539712, acc 1
2016-09-06T11:03:38.562947: step 9251, loss 0.00272917, acc 1
2016-09-06T11:03:39.375581: step 9252, loss 0.00332011, acc 1
2016-09-06T11:03:40.203734: step 9253, loss 0.0277699, acc 1
2016-09-06T11:03:41.005886: step 9254, loss 0.0286968, acc 1
2016-09-06T11:03:41.797990: step 9255, loss 0.058274, acc 0.96
2016-09-06T11:03:42.589930: step 9256, loss 0.00248709, acc 1
2016-09-06T11:03:43.411760: step 9257, loss 0.018081, acc 1
2016-09-06T11:03:44.217287: step 9258, loss 0.0294563, acc 1
2016-09-06T11:03:45.072288: step 9259, loss 0.0342188, acc 0.98
2016-09-06T11:03:45.869876: step 9260, loss 0.0223341, acc 1
2016-09-06T11:03:46.663594: step 9261, loss 0.00239263, acc 1
2016-09-06T11:03:47.485425: step 9262, loss 0.0045282, acc 1
2016-09-06T11:03:48.254774: step 9263, loss 0.00249443, acc 1
2016-09-06T11:03:49.041758: step 9264, loss 0.0315927, acc 0.98
2016-09-06T11:03:49.854839: step 9265, loss 0.0271703, acc 0.98
2016-09-06T11:03:50.657593: step 9266, loss 0.0342186, acc 0.98
2016-09-06T11:03:51.509737: step 9267, loss 0.0139197, acc 1
2016-09-06T11:03:52.333606: step 9268, loss 0.0187604, acc 1
2016-09-06T11:03:53.098128: step 9269, loss 0.0188738, acc 1
2016-09-06T11:03:53.889769: step 9270, loss 0.019547, acc 0.98
2016-09-06T11:03:54.726908: step 9271, loss 0.0165788, acc 1
2016-09-06T11:03:55.494280: step 9272, loss 0.0226255, acc 1
2016-09-06T11:03:56.307036: step 9273, loss 0.00241443, acc 1
2016-09-06T11:03:57.135320: step 9274, loss 0.025156, acc 0.98
2016-09-06T11:03:57.936420: step 9275, loss 0.00937022, acc 1
2016-09-06T11:03:58.739541: step 9276, loss 0.00318562, acc 1
2016-09-06T11:03:59.561996: step 9277, loss 0.0733179, acc 0.98
2016-09-06T11:04:00.386973: step 9278, loss 0.0613152, acc 0.98
2016-09-06T11:04:01.196737: step 9279, loss 0.00225956, acc 1
2016-09-06T11:04:02.010170: step 9280, loss 0.0234318, acc 0.98
2016-09-06T11:04:02.798811: step 9281, loss 0.0120031, acc 1
2016-09-06T11:04:03.610428: step 9282, loss 0.0140718, acc 1
2016-09-06T11:04:04.445976: step 9283, loss 0.0163676, acc 1
2016-09-06T11:04:05.242978: step 9284, loss 0.0093239, acc 1
2016-09-06T11:04:06.039896: step 9285, loss 0.0333539, acc 0.98
2016-09-06T11:04:06.881296: step 9286, loss 0.00743259, acc 1
2016-09-06T11:04:07.682741: step 9287, loss 0.00716527, acc 1
2016-09-06T11:04:08.495633: step 9288, loss 0.0222207, acc 1
2016-09-06T11:04:09.322789: step 9289, loss 0.0180213, acc 1
2016-09-06T11:04:10.156182: step 9290, loss 0.027145, acc 0.98
2016-09-06T11:04:10.971618: step 9291, loss 0.00413508, acc 1
2016-09-06T11:04:11.793271: step 9292, loss 0.0118641, acc 1
2016-09-06T11:04:12.607295: step 9293, loss 0.0253441, acc 0.98
2016-09-06T11:04:13.443994: step 9294, loss 0.0392118, acc 0.98
2016-09-06T11:04:14.314982: step 9295, loss 0.0397829, acc 0.98
2016-09-06T11:04:15.135263: step 9296, loss 0.00937125, acc 1
2016-09-06T11:04:15.956622: step 9297, loss 0.00769582, acc 1
2016-09-06T11:04:16.777600: step 9298, loss 0.0147192, acc 1
2016-09-06T11:04:17.571224: step 9299, loss 0.0451451, acc 0.98
2016-09-06T11:04:18.395256: step 9300, loss 0.0297403, acc 0.98

Evaluation:
2016-09-06T11:04:22.115399: step 9300, loss 2.14093, acc 0.757036

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-9300

2016-09-06T11:04:24.012793: step 9301, loss 0.0157802, acc 1
2016-09-06T11:04:24.839778: step 9302, loss 0.00748416, acc 1
2016-09-06T11:04:25.683129: step 9303, loss 0.0179028, acc 0.98
2016-09-06T11:04:26.529316: step 9304, loss 0.0449043, acc 0.98
2016-09-06T11:04:27.359139: step 9305, loss 0.00445658, acc 1
2016-09-06T11:04:28.168031: step 9306, loss 0.00233188, acc 1
2016-09-06T11:04:29.004056: step 9307, loss 0.00337172, acc 1
2016-09-06T11:04:29.839918: step 9308, loss 0.00226787, acc 1
2016-09-06T11:04:30.645726: step 9309, loss 0.0277402, acc 0.98
2016-09-06T11:04:31.458829: step 9310, loss 0.00671497, acc 1
2016-09-06T11:04:32.263713: step 9311, loss 0.0208281, acc 0.98
2016-09-06T11:04:33.056854: step 9312, loss 0.0283843, acc 0.98
2016-09-06T11:04:33.879019: step 9313, loss 0.00236636, acc 1
2016-09-06T11:04:34.737738: step 9314, loss 0.0251978, acc 1
2016-09-06T11:04:35.517026: step 9315, loss 0.00395084, acc 1
2016-09-06T11:04:36.325747: step 9316, loss 0.0339381, acc 0.98
2016-09-06T11:04:37.169599: step 9317, loss 0.0280339, acc 0.98
2016-09-06T11:04:37.945301: step 9318, loss 0.00785825, acc 1
2016-09-06T11:04:38.744160: step 9319, loss 0.00241698, acc 1
2016-09-06T11:04:39.553308: step 9320, loss 0.00299543, acc 1
2016-09-06T11:04:40.332750: step 9321, loss 0.00751852, acc 1
2016-09-06T11:04:41.145642: step 9322, loss 0.0176887, acc 1
2016-09-06T11:04:41.966224: step 9323, loss 0.0192103, acc 0.98
2016-09-06T11:04:42.775944: step 9324, loss 0.00451382, acc 1
2016-09-06T11:04:43.577201: step 9325, loss 0.0473036, acc 0.96
2016-09-06T11:04:44.407957: step 9326, loss 0.0899488, acc 0.96
2016-09-06T11:04:45.190989: step 9327, loss 0.00502001, acc 1
2016-09-06T11:04:45.987698: step 9328, loss 0.0205431, acc 0.98
2016-09-06T11:04:46.806888: step 9329, loss 0.0337429, acc 0.98
2016-09-06T11:04:47.591706: step 9330, loss 0.0263522, acc 1
2016-09-06T11:04:48.390086: step 9331, loss 0.0658173, acc 0.96
2016-09-06T11:04:49.207785: step 9332, loss 0.0833684, acc 0.98
2016-09-06T11:04:49.993353: step 9333, loss 0.0182667, acc 0.98
2016-09-06T11:04:50.807937: step 9334, loss 0.00269699, acc 1
2016-09-06T11:04:51.631335: step 9335, loss 0.0180404, acc 1
2016-09-06T11:04:52.435135: step 9336, loss 0.0708671, acc 0.96
2016-09-06T11:04:53.233460: step 9337, loss 0.002096, acc 1
2016-09-06T11:04:54.052873: step 9338, loss 0.00377339, acc 1
2016-09-06T11:04:54.861928: step 9339, loss 0.00216525, acc 1
2016-09-06T11:04:55.698881: step 9340, loss 0.0219467, acc 1
2016-09-06T11:04:56.478341: step 9341, loss 0.0131338, acc 1
2016-09-06T11:04:57.294624: step 9342, loss 0.00380854, acc 1
2016-09-06T11:04:58.088826: step 9343, loss 0.0240102, acc 0.98
2016-09-06T11:04:58.932837: step 9344, loss 0.0170897, acc 1
2016-09-06T11:04:59.730975: step 9345, loss 0.00336533, acc 1
2016-09-06T11:05:00.552716: step 9346, loss 0.0175827, acc 1
2016-09-06T11:05:01.342065: step 9347, loss 0.0326474, acc 1
2016-09-06T11:05:02.109965: step 9348, loss 0.00583127, acc 1
2016-09-06T11:05:02.923446: step 9349, loss 0.0191065, acc 1
2016-09-06T11:05:03.753441: step 9350, loss 0.0370804, acc 0.98
2016-09-06T11:05:04.527044: step 9351, loss 0.00517758, acc 1
2016-09-06T11:05:05.331225: step 9352, loss 0.00193168, acc 1
2016-09-06T11:05:06.150442: step 9353, loss 0.0022227, acc 1
2016-09-06T11:05:06.936267: step 9354, loss 0.0247483, acc 0.98
2016-09-06T11:05:07.753925: step 9355, loss 0.00288519, acc 1
2016-09-06T11:05:08.568482: step 9356, loss 0.00208488, acc 1
2016-09-06T11:05:09.380183: step 9357, loss 0.0182645, acc 1
2016-09-06T11:05:10.180223: step 9358, loss 0.0177684, acc 0.98
2016-09-06T11:05:11.024760: step 9359, loss 0.00337592, acc 1
2016-09-06T11:05:11.835451: step 9360, loss 0.0127215, acc 1
2016-09-06T11:05:12.634939: step 9361, loss 0.00216066, acc 1
2016-09-06T11:05:13.432215: step 9362, loss 0.0190954, acc 1
2016-09-06T11:05:14.199491: step 9363, loss 0.176535, acc 0.94
2016-09-06T11:05:15.005211: step 9364, loss 0.00235906, acc 1
2016-09-06T11:05:15.852579: step 9365, loss 0.0232555, acc 1
2016-09-06T11:05:16.664135: step 9366, loss 0.0185363, acc 0.98
2016-09-06T11:05:17.467710: step 9367, loss 0.00612908, acc 1
2016-09-06T11:05:18.275578: step 9368, loss 0.00309663, acc 1
2016-09-06T11:05:19.083594: step 9369, loss 0.00427593, acc 1
2016-09-06T11:05:19.857987: step 9370, loss 0.00433576, acc 1
2016-09-06T11:05:20.685074: step 9371, loss 0.0152598, acc 1
2016-09-06T11:05:21.490016: step 9372, loss 0.0523441, acc 0.96
2016-09-06T11:05:22.314021: step 9373, loss 0.0169726, acc 0.98
2016-09-06T11:05:23.136116: step 9374, loss 0.013707, acc 1
2016-09-06T11:05:23.930247: step 9375, loss 0.0520766, acc 0.96
2016-09-06T11:05:24.736151: step 9376, loss 0.0109365, acc 1
2016-09-06T11:05:25.551700: step 9377, loss 0.0170226, acc 1
2016-09-06T11:05:26.364246: step 9378, loss 0.00601578, acc 1
2016-09-06T11:05:27.158412: step 9379, loss 0.00245569, acc 1
2016-09-06T11:05:27.977635: step 9380, loss 0.0132756, acc 1
2016-09-06T11:05:28.769344: step 9381, loss 0.0115548, acc 1
2016-09-06T11:05:29.582170: step 9382, loss 0.0170679, acc 1
2016-09-06T11:05:30.433377: step 9383, loss 0.0112666, acc 1
2016-09-06T11:05:31.239118: step 9384, loss 0.0116632, acc 1
2016-09-06T11:05:32.066937: step 9385, loss 0.0276234, acc 0.98
2016-09-06T11:05:32.894174: step 9386, loss 0.00241652, acc 1
2016-09-06T11:05:33.727049: step 9387, loss 0.0100081, acc 1
2016-09-06T11:05:34.579719: step 9388, loss 0.020595, acc 1
2016-09-06T11:05:35.415973: step 9389, loss 0.00541381, acc 1
2016-09-06T11:05:36.211275: step 9390, loss 0.00200205, acc 1
2016-09-06T11:05:37.038784: step 9391, loss 0.0296909, acc 0.98
2016-09-06T11:05:37.878682: step 9392, loss 0.00929363, acc 1
2016-09-06T11:05:38.730463: step 9393, loss 0.00702483, acc 1
2016-09-06T11:05:39.542621: step 9394, loss 0.0131903, acc 1
2016-09-06T11:05:40.399719: step 9395, loss 0.014393, acc 1
2016-09-06T11:05:41.236623: step 9396, loss 0.0414276, acc 0.96
2016-09-06T11:05:42.014287: step 9397, loss 0.00228329, acc 1
2016-09-06T11:05:42.813515: step 9398, loss 0.0202644, acc 0.98
2016-09-06T11:05:43.659370: step 9399, loss 0.0130934, acc 1
2016-09-06T11:05:44.458030: step 9400, loss 0.00505733, acc 1

Evaluation:
2016-09-06T11:05:48.140209: step 9400, loss 2.34215, acc 0.742026

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-9400

2016-09-06T11:05:50.083962: step 9401, loss 0.00287129, acc 1
2016-09-06T11:05:50.894858: step 9402, loss 0.0158415, acc 1
2016-09-06T11:05:51.728716: step 9403, loss 0.00226665, acc 1
2016-09-06T11:05:52.545708: step 9404, loss 0.00219575, acc 1
2016-09-06T11:05:53.346776: step 9405, loss 0.0266353, acc 1
2016-09-06T11:05:54.114757: step 9406, loss 0.00525928, acc 1
2016-09-06T11:05:54.951796: step 9407, loss 0.0338248, acc 0.98
2016-09-06T11:05:55.731266: step 9408, loss 0.0934345, acc 0.977273
2016-09-06T11:05:56.573281: step 9409, loss 0.0507092, acc 0.98
2016-09-06T11:05:57.399119: step 9410, loss 0.0157323, acc 1
2016-09-06T11:05:58.196062: step 9411, loss 0.0171624, acc 0.98
2016-09-06T11:05:58.967037: step 9412, loss 0.0116579, acc 1
2016-09-06T11:05:59.782744: step 9413, loss 0.0386633, acc 0.98
2016-09-06T11:06:00.616596: step 9414, loss 0.00345634, acc 1
2016-09-06T11:06:01.444349: step 9415, loss 0.00290942, acc 1
2016-09-06T11:06:02.249998: step 9416, loss 0.0252419, acc 1
2016-09-06T11:06:03.104109: step 9417, loss 0.0463855, acc 0.98
2016-09-06T11:06:03.920955: step 9418, loss 0.0389289, acc 0.98
2016-09-06T11:06:04.732091: step 9419, loss 0.00199258, acc 1
2016-09-06T11:06:05.550125: step 9420, loss 0.00231961, acc 1
2016-09-06T11:06:06.350986: step 9421, loss 0.00698748, acc 1
2016-09-06T11:06:07.190954: step 9422, loss 0.0270077, acc 1
2016-09-06T11:06:08.022193: step 9423, loss 0.0234847, acc 0.98
2016-09-06T11:06:08.812983: step 9424, loss 0.00394934, acc 1
2016-09-06T11:06:09.672313: step 9425, loss 0.0245043, acc 1
2016-09-06T11:06:10.496362: step 9426, loss 0.00252472, acc 1
2016-09-06T11:06:11.320737: step 9427, loss 0.00607873, acc 1
2016-09-06T11:06:12.114925: step 9428, loss 0.00198823, acc 1
2016-09-06T11:06:12.941647: step 9429, loss 0.00538305, acc 1
2016-09-06T11:06:13.762732: step 9430, loss 0.00242627, acc 1
2016-09-06T11:06:14.592704: step 9431, loss 0.00214499, acc 1
2016-09-06T11:06:15.412075: step 9432, loss 0.0642052, acc 0.96
2016-09-06T11:06:16.230970: step 9433, loss 0.00553994, acc 1
2016-09-06T11:06:17.018665: step 9434, loss 0.0259823, acc 0.98
2016-09-06T11:06:17.835194: step 9435, loss 0.0118566, acc 1
2016-09-06T11:06:18.634889: step 9436, loss 0.0228888, acc 0.98
2016-09-06T11:06:19.454606: step 9437, loss 0.00441209, acc 1
2016-09-06T11:06:20.312549: step 9438, loss 0.0580966, acc 0.96
2016-09-06T11:06:21.137408: step 9439, loss 0.0148494, acc 1
2016-09-06T11:06:21.897645: step 9440, loss 0.00657107, acc 1
2016-09-06T11:06:22.699585: step 9441, loss 0.124695, acc 0.98
2016-09-06T11:06:23.504296: step 9442, loss 0.0687413, acc 0.98
2016-09-06T11:06:24.275667: step 9443, loss 0.00173572, acc 1
2016-09-06T11:06:25.095827: step 9444, loss 0.0215605, acc 1
2016-09-06T11:06:25.901340: step 9445, loss 0.0317348, acc 0.98
2016-09-06T11:06:26.709266: step 9446, loss 0.0119965, acc 1
2016-09-06T11:06:27.571682: step 9447, loss 0.012146, acc 1
2016-09-06T11:06:28.413314: step 9448, loss 0.00257584, acc 1
2016-09-06T11:06:29.222566: step 9449, loss 0.0262289, acc 1
2016-09-06T11:06:30.021237: step 9450, loss 0.0176765, acc 1
2016-09-06T11:06:30.828371: step 9451, loss 0.00799871, acc 1
2016-09-06T11:06:31.627649: step 9452, loss 0.00470048, acc 1
2016-09-06T11:06:32.430907: step 9453, loss 0.0263853, acc 0.98
2016-09-06T11:06:33.241204: step 9454, loss 0.0179549, acc 1
2016-09-06T11:06:34.037583: step 9455, loss 0.0101852, acc 1
2016-09-06T11:06:34.824749: step 9456, loss 0.0149574, acc 1
2016-09-06T11:06:35.626566: step 9457, loss 0.0151055, acc 1
2016-09-06T11:06:36.421500: step 9458, loss 0.00235899, acc 1
2016-09-06T11:06:37.226208: step 9459, loss 0.00329215, acc 1
2016-09-06T11:06:38.027831: step 9460, loss 0.0138813, acc 1
2016-09-06T11:06:38.815693: step 9461, loss 0.073438, acc 0.94
2016-09-06T11:06:39.627085: step 9462, loss 0.0207408, acc 1
2016-09-06T11:06:40.464750: step 9463, loss 0.0264987, acc 0.98
2016-09-06T11:06:41.226410: step 9464, loss 0.00219972, acc 1
2016-09-06T11:06:42.022034: step 9465, loss 0.00489574, acc 1
2016-09-06T11:06:42.834462: step 9466, loss 0.0407213, acc 0.98
2016-09-06T11:06:43.622717: step 9467, loss 0.00588284, acc 1
2016-09-06T11:06:44.438538: step 9468, loss 0.0160761, acc 1
2016-09-06T11:06:45.256429: step 9469, loss 0.0528488, acc 0.94
2016-09-06T11:06:46.061759: step 9470, loss 0.0198539, acc 1
2016-09-06T11:06:46.875346: step 9471, loss 0.00666964, acc 1
2016-09-06T11:06:47.695671: step 9472, loss 0.0687693, acc 0.96
2016-09-06T11:06:48.472553: step 9473, loss 0.00507994, acc 1
2016-09-06T11:06:49.272996: step 9474, loss 0.0116538, acc 1
2016-09-06T11:06:50.075583: step 9475, loss 0.00587724, acc 1
2016-09-06T11:06:50.865442: step 9476, loss 0.0379919, acc 0.98
2016-09-06T11:06:51.709811: step 9477, loss 0.0205929, acc 0.98
2016-09-06T11:06:52.525985: step 9478, loss 0.0199954, acc 0.98
2016-09-06T11:06:53.332847: step 9479, loss 0.00490931, acc 1
2016-09-06T11:06:54.129860: step 9480, loss 0.0433804, acc 0.98
2016-09-06T11:06:54.949630: step 9481, loss 0.00325782, acc 1
2016-09-06T11:06:55.775219: step 9482, loss 0.00851839, acc 1
2016-09-06T11:06:56.598303: step 9483, loss 0.0214253, acc 0.98
2016-09-06T11:06:57.433206: step 9484, loss 0.0252318, acc 1
2016-09-06T11:06:58.242961: step 9485, loss 0.0335669, acc 0.98
2016-09-06T11:06:59.060087: step 9486, loss 0.00447075, acc 1
2016-09-06T11:06:59.907966: step 9487, loss 0.00228871, acc 1
2016-09-06T11:07:00.750329: step 9488, loss 0.00818797, acc 1
2016-09-06T11:07:01.568470: step 9489, loss 0.00237501, acc 1
2016-09-06T11:07:02.430786: step 9490, loss 0.00850302, acc 1
2016-09-06T11:07:03.256111: step 9491, loss 0.0491007, acc 0.96
2016-09-06T11:07:04.066516: step 9492, loss 0.00381734, acc 1
2016-09-06T11:07:04.884513: step 9493, loss 0.0203456, acc 1
2016-09-06T11:07:05.679710: step 9494, loss 0.0150795, acc 1
2016-09-06T11:07:06.477474: step 9495, loss 0.0036038, acc 1
2016-09-06T11:07:07.306212: step 9496, loss 0.0119886, acc 1
2016-09-06T11:07:08.141648: step 9497, loss 0.00422434, acc 1
2016-09-06T11:07:08.943187: step 9498, loss 0.00222465, acc 1
2016-09-06T11:07:09.754547: step 9499, loss 0.00272285, acc 1
2016-09-06T11:07:10.565466: step 9500, loss 0.00555955, acc 1

Evaluation:
2016-09-06T11:07:14.328546: step 9500, loss 2.30804, acc 0.754221

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-9500

2016-09-06T11:07:16.243366: step 9501, loss 0.0925649, acc 0.98
2016-09-06T11:07:17.098490: step 9502, loss 0.00255332, acc 1
2016-09-06T11:07:17.933442: step 9503, loss 0.0126026, acc 1
2016-09-06T11:07:18.725116: step 9504, loss 0.00217643, acc 1
2016-09-06T11:07:19.541441: step 9505, loss 0.00264339, acc 1
2016-09-06T11:07:20.350088: step 9506, loss 0.00524692, acc 1
2016-09-06T11:07:21.119718: step 9507, loss 0.0179197, acc 0.98
2016-09-06T11:07:21.914477: step 9508, loss 0.0187785, acc 0.98
2016-09-06T11:07:22.740202: step 9509, loss 0.0191177, acc 0.98
2016-09-06T11:07:23.533578: step 9510, loss 0.00686197, acc 1
2016-09-06T11:07:24.351117: step 9511, loss 0.0143752, acc 1
2016-09-06T11:07:25.163109: step 9512, loss 0.00324028, acc 1
2016-09-06T11:07:25.939923: step 9513, loss 0.021031, acc 0.98
2016-09-06T11:07:26.727991: step 9514, loss 0.00264519, acc 1
2016-09-06T11:07:27.517573: step 9515, loss 0.0528843, acc 0.96
2016-09-06T11:07:28.322305: step 9516, loss 0.00936706, acc 1
2016-09-06T11:07:29.144945: step 9517, loss 0.0436307, acc 0.98
2016-09-06T11:07:29.940770: step 9518, loss 0.0347018, acc 0.98
2016-09-06T11:07:30.738013: step 9519, loss 0.00273574, acc 1
2016-09-06T11:07:31.551085: step 9520, loss 0.0642423, acc 0.98
2016-09-06T11:07:32.381440: step 9521, loss 0.00688953, acc 1
2016-09-06T11:07:33.181879: step 9522, loss 0.0161806, acc 1
2016-09-06T11:07:33.994197: step 9523, loss 0.147628, acc 0.94
2016-09-06T11:07:34.810544: step 9524, loss 0.00256575, acc 1
2016-09-06T11:07:35.608022: step 9525, loss 0.0496472, acc 0.98
2016-09-06T11:07:36.406103: step 9526, loss 0.00216003, acc 1
2016-09-06T11:07:37.224582: step 9527, loss 0.0313805, acc 0.98
2016-09-06T11:07:38.020153: step 9528, loss 0.0223027, acc 0.98
2016-09-06T11:07:38.825328: step 9529, loss 0.00298357, acc 1
2016-09-06T11:07:39.633153: step 9530, loss 0.00185387, acc 1
2016-09-06T11:07:40.415644: step 9531, loss 0.0051237, acc 1
2016-09-06T11:07:41.220135: step 9532, loss 0.0262325, acc 0.98
2016-09-06T11:07:42.026663: step 9533, loss 0.01589, acc 1
2016-09-06T11:07:42.817997: step 9534, loss 0.027656, acc 0.98
2016-09-06T11:07:43.632144: step 9535, loss 0.0328399, acc 0.96
2016-09-06T11:07:44.433693: step 9536, loss 0.0197844, acc 1
2016-09-06T11:07:45.256342: step 9537, loss 0.0264054, acc 0.98
2016-09-06T11:07:46.091299: step 9538, loss 0.0412667, acc 0.98
2016-09-06T11:07:46.924996: step 9539, loss 0.0244916, acc 1
2016-09-06T11:07:47.730470: step 9540, loss 0.0321273, acc 0.98
2016-09-06T11:07:48.524830: step 9541, loss 0.042618, acc 0.98
2016-09-06T11:07:49.364225: step 9542, loss 0.0192068, acc 1
2016-09-06T11:07:50.141147: step 9543, loss 0.0254123, acc 0.98
2016-09-06T11:07:50.945903: step 9544, loss 0.0106991, acc 1
2016-09-06T11:07:51.757712: step 9545, loss 0.0123071, acc 1
2016-09-06T11:07:52.553916: step 9546, loss 0.00772785, acc 1
2016-09-06T11:07:53.356611: step 9547, loss 0.001988, acc 1
2016-09-06T11:07:54.207726: step 9548, loss 0.00263163, acc 1
2016-09-06T11:07:55.013470: step 9549, loss 0.0208416, acc 0.98
2016-09-06T11:07:55.815160: step 9550, loss 0.107042, acc 0.96
2016-09-06T11:07:56.649879: step 9551, loss 0.00222448, acc 1
2016-09-06T11:07:57.444622: step 9552, loss 0.0102251, acc 1
2016-09-06T11:07:58.262381: step 9553, loss 0.00376129, acc 1
2016-09-06T11:07:59.074820: step 9554, loss 0.0326017, acc 0.98
2016-09-06T11:07:59.873515: step 9555, loss 0.020435, acc 1
2016-09-06T11:08:00.700030: step 9556, loss 0.00499556, acc 1
2016-09-06T11:08:01.530794: step 9557, loss 0.0135441, acc 1
2016-09-06T11:08:02.354844: step 9558, loss 0.268291, acc 0.98
2016-09-06T11:08:03.165207: step 9559, loss 0.0029028, acc 1
2016-09-06T11:08:03.981187: step 9560, loss 0.0022929, acc 1
2016-09-06T11:08:04.795297: step 9561, loss 0.0555662, acc 0.96
2016-09-06T11:08:05.596438: step 9562, loss 0.0032203, acc 1
2016-09-06T11:08:06.418915: step 9563, loss 0.0192118, acc 1
2016-09-06T11:08:07.232320: step 9564, loss 0.017043, acc 1
2016-09-06T11:08:08.067494: step 9565, loss 0.00577808, acc 1
2016-09-06T11:08:08.901000: step 9566, loss 0.00294127, acc 1
2016-09-06T11:08:09.735908: step 9567, loss 0.0678826, acc 0.98
2016-09-06T11:08:10.558406: step 9568, loss 0.0432841, acc 0.96
2016-09-06T11:08:11.392030: step 9569, loss 0.17844, acc 0.94
2016-09-06T11:08:12.207759: step 9570, loss 0.0033429, acc 1
2016-09-06T11:08:13.030385: step 9571, loss 0.0256837, acc 0.98
2016-09-06T11:08:13.862577: step 9572, loss 0.00755371, acc 1
2016-09-06T11:08:14.687617: step 9573, loss 0.00386983, acc 1
2016-09-06T11:08:15.496621: step 9574, loss 0.0439798, acc 0.98
2016-09-06T11:08:16.295252: step 9575, loss 0.0336542, acc 1
2016-09-06T11:08:17.107949: step 9576, loss 0.00819696, acc 1
2016-09-06T11:08:17.891829: step 9577, loss 0.0766961, acc 0.96
2016-09-06T11:08:18.701966: step 9578, loss 0.0459123, acc 0.98
2016-09-06T11:08:19.513284: step 9579, loss 0.0260846, acc 0.98
2016-09-06T11:08:20.298270: step 9580, loss 0.0128235, acc 1
2016-09-06T11:08:21.107153: step 9581, loss 0.147242, acc 0.94
2016-09-06T11:08:21.916977: step 9582, loss 0.0310651, acc 0.98
2016-09-06T11:08:22.717067: step 9583, loss 0.10013, acc 0.96
2016-09-06T11:08:23.537940: step 9584, loss 0.00492912, acc 1
2016-09-06T11:08:24.335991: step 9585, loss 0.0295136, acc 0.98
2016-09-06T11:08:25.148846: step 9586, loss 0.0425579, acc 0.98
2016-09-06T11:08:25.960873: step 9587, loss 0.0125904, acc 1
2016-09-06T11:08:26.875393: step 9588, loss 0.028328, acc 0.98
2016-09-06T11:08:27.675937: step 9589, loss 0.00497067, acc 1
2016-09-06T11:08:28.505430: step 9590, loss 0.00535666, acc 1
2016-09-06T11:08:29.357834: step 9591, loss 0.00363926, acc 1
2016-09-06T11:08:30.153652: step 9592, loss 0.00666261, acc 1
2016-09-06T11:08:30.967912: step 9593, loss 0.0261327, acc 0.98
2016-09-06T11:08:31.786207: step 9594, loss 0.0395849, acc 0.96
2016-09-06T11:08:32.697768: step 9595, loss 0.0575514, acc 0.98
2016-09-06T11:08:33.527463: step 9596, loss 0.0123028, acc 1
2016-09-06T11:08:34.423676: step 9597, loss 0.0592637, acc 0.98
2016-09-06T11:08:35.280458: step 9598, loss 0.111598, acc 0.98
2016-09-06T11:08:36.231489: step 9599, loss 0.00377174, acc 1
2016-09-06T11:08:36.986040: step 9600, loss 0.00825605, acc 1

Evaluation:
2016-09-06T11:08:40.686850: step 9600, loss 2.54377, acc 0.762664

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-9600

2016-09-06T11:08:42.666557: step 9601, loss 0.00864834, acc 1
2016-09-06T11:08:43.488252: step 9602, loss 0.045245, acc 0.98
2016-09-06T11:08:44.295923: step 9603, loss 0.0172361, acc 1
2016-09-06T11:08:45.152484: step 9604, loss 0.0044282, acc 1
2016-09-06T11:08:45.990416: step 9605, loss 0.0326673, acc 0.98
2016-09-06T11:08:46.787808: step 9606, loss 0.0335892, acc 0.98
2016-09-06T11:08:47.626300: step 9607, loss 0.0321234, acc 0.98
2016-09-06T11:08:48.466614: step 9608, loss 0.00770683, acc 1
2016-09-06T11:08:49.287651: step 9609, loss 0.0232269, acc 1
2016-09-06T11:08:50.131707: step 9610, loss 0.0214947, acc 1
2016-09-06T11:08:50.961945: step 9611, loss 0.00344085, acc 1
2016-09-06T11:08:51.740723: step 9612, loss 0.00347303, acc 1
2016-09-06T11:08:52.563413: step 9613, loss 0.0214296, acc 1
2016-09-06T11:08:53.401304: step 9614, loss 0.0199873, acc 1
2016-09-06T11:08:54.219507: step 9615, loss 0.0171456, acc 1
2016-09-06T11:08:55.049562: step 9616, loss 0.0114098, acc 1
2016-09-06T11:08:55.859961: step 9617, loss 0.0113173, acc 1
2016-09-06T11:08:56.663625: step 9618, loss 0.00593776, acc 1
2016-09-06T11:08:57.465205: step 9619, loss 0.00510808, acc 1
2016-09-06T11:08:58.285898: step 9620, loss 0.00368471, acc 1
2016-09-06T11:08:59.089129: step 9621, loss 0.00922132, acc 1
2016-09-06T11:08:59.878429: step 9622, loss 0.00370517, acc 1
2016-09-06T11:09:00.890720: step 9623, loss 0.00593668, acc 1
2016-09-06T11:09:01.701353: step 9624, loss 0.018276, acc 1
2016-09-06T11:09:02.500873: step 9625, loss 0.00587628, acc 1
2016-09-06T11:09:03.373446: step 9626, loss 0.0240931, acc 1
2016-09-06T11:09:04.214509: step 9627, loss 0.0296934, acc 1
2016-09-06T11:09:05.028161: step 9628, loss 0.0100808, acc 1
2016-09-06T11:09:05.879386: step 9629, loss 0.00554906, acc 1
2016-09-06T11:09:06.724657: step 9630, loss 0.00347912, acc 1
2016-09-06T11:09:07.518780: step 9631, loss 0.00919035, acc 1
2016-09-06T11:09:08.320881: step 9632, loss 0.00519203, acc 1
2016-09-06T11:09:09.151556: step 9633, loss 0.183007, acc 0.98
2016-09-06T11:09:09.945505: step 9634, loss 0.0415321, acc 0.98
2016-09-06T11:09:10.737472: step 9635, loss 0.00410679, acc 1
2016-09-06T11:09:11.559122: step 9636, loss 0.00316619, acc 1
2016-09-06T11:09:12.355382: step 9637, loss 0.00864296, acc 1
2016-09-06T11:09:13.145924: step 9638, loss 0.0139848, acc 1
2016-09-06T11:09:13.964528: step 9639, loss 0.0125373, acc 1
2016-09-06T11:09:14.744660: step 9640, loss 0.0110186, acc 1
2016-09-06T11:09:15.544693: step 9641, loss 0.00398987, acc 1
2016-09-06T11:09:16.351930: step 9642, loss 0.0172576, acc 1
2016-09-06T11:09:17.115237: step 9643, loss 0.0335031, acc 0.98
2016-09-06T11:09:17.927549: step 9644, loss 0.00326848, acc 1
2016-09-06T11:09:18.759528: step 9645, loss 0.00336845, acc 1
2016-09-06T11:09:19.531554: step 9646, loss 0.0127876, acc 1
2016-09-06T11:09:20.420778: step 9647, loss 0.00474996, acc 1
2016-09-06T11:09:21.222901: step 9648, loss 0.0434003, acc 0.96
2016-09-06T11:09:21.987569: step 9649, loss 0.0329981, acc 0.98
2016-09-06T11:09:22.802606: step 9650, loss 0.00714306, acc 1
2016-09-06T11:09:23.641369: step 9651, loss 0.0285862, acc 0.98
2016-09-06T11:09:24.411713: step 9652, loss 0.0708401, acc 0.98
2016-09-06T11:09:25.203016: step 9653, loss 0.0171115, acc 1
2016-09-06T11:09:26.029624: step 9654, loss 0.0313167, acc 0.98
2016-09-06T11:09:26.825111: step 9655, loss 0.0457386, acc 0.98
2016-09-06T11:09:27.641660: step 9656, loss 0.00902705, acc 1
2016-09-06T11:09:28.459400: step 9657, loss 0.0107325, acc 1
2016-09-06T11:09:29.235764: step 9658, loss 0.0439782, acc 0.98
2016-09-06T11:09:30.067075: step 9659, loss 0.00538055, acc 1
2016-09-06T11:09:30.886598: step 9660, loss 0.00431013, acc 1
2016-09-06T11:09:31.662046: step 9661, loss 0.0264827, acc 0.98
2016-09-06T11:09:32.452256: step 9662, loss 0.00305585, acc 1
2016-09-06T11:09:33.305754: step 9663, loss 0.0272785, acc 0.98
2016-09-06T11:09:34.094827: step 9664, loss 0.0937806, acc 0.96
2016-09-06T11:09:34.883772: step 9665, loss 0.00567232, acc 1
2016-09-06T11:09:35.695472: step 9666, loss 0.00903793, acc 1
2016-09-06T11:09:36.480986: step 9667, loss 0.0043776, acc 1
2016-09-06T11:09:37.292993: step 9668, loss 0.0192048, acc 1
2016-09-06T11:09:38.127695: step 9669, loss 0.0155475, acc 1
2016-09-06T11:09:38.905673: step 9670, loss 0.00511719, acc 1
2016-09-06T11:09:39.696212: step 9671, loss 0.0122383, acc 1
2016-09-06T11:09:40.508883: step 9672, loss 0.0194012, acc 1
2016-09-06T11:09:41.279637: step 9673, loss 0.0247495, acc 1
2016-09-06T11:09:42.103060: step 9674, loss 0.0200683, acc 1
2016-09-06T11:09:42.910139: step 9675, loss 0.0299674, acc 0.98
2016-09-06T11:09:43.689028: step 9676, loss 0.0217984, acc 0.98
2016-09-06T11:09:44.495220: step 9677, loss 0.00525459, acc 1
2016-09-06T11:09:45.306744: step 9678, loss 0.0372517, acc 0.98
2016-09-06T11:09:46.095705: step 9679, loss 0.0443573, acc 0.96
2016-09-06T11:09:46.925891: step 9680, loss 0.00378039, acc 1
2016-09-06T11:09:47.793380: step 9681, loss 0.00676753, acc 1
2016-09-06T11:09:48.599899: step 9682, loss 0.0070987, acc 1
2016-09-06T11:09:49.381685: step 9683, loss 0.0698354, acc 0.96
2016-09-06T11:09:50.213982: step 9684, loss 0.0046128, acc 1
2016-09-06T11:09:51.010876: step 9685, loss 0.00593557, acc 1
2016-09-06T11:09:51.832956: step 9686, loss 0.0106087, acc 1
2016-09-06T11:09:52.647644: step 9687, loss 0.00286145, acc 1
2016-09-06T11:09:53.410162: step 9688, loss 0.0444192, acc 0.98
2016-09-06T11:09:54.213631: step 9689, loss 0.00604161, acc 1
2016-09-06T11:09:55.020443: step 9690, loss 0.00285214, acc 1
2016-09-06T11:09:55.809793: step 9691, loss 0.0550048, acc 0.98
2016-09-06T11:09:56.612687: step 9692, loss 0.0149369, acc 1
2016-09-06T11:09:57.444024: step 9693, loss 0.00341194, acc 1
2016-09-06T11:09:58.233376: step 9694, loss 0.0034682, acc 1
2016-09-06T11:09:59.045081: step 9695, loss 0.00283654, acc 1
2016-09-06T11:09:59.888694: step 9696, loss 0.00264352, acc 1
2016-09-06T11:10:00.682128: step 9697, loss 0.0031419, acc 1
2016-09-06T11:10:01.466103: step 9698, loss 0.0162718, acc 1
2016-09-06T11:10:02.283149: step 9699, loss 0.158281, acc 0.98
2016-09-06T11:10:03.077919: step 9700, loss 0.0224905, acc 1

Evaluation:
2016-09-06T11:10:06.825781: step 9700, loss 2.26601, acc 0.761726

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-9700

2016-09-06T11:10:08.709810: step 9701, loss 0.011498, acc 1
2016-09-06T11:10:09.514629: step 9702, loss 0.00272153, acc 1
2016-09-06T11:10:10.306921: step 9703, loss 0.00230947, acc 1
2016-09-06T11:10:11.111922: step 9704, loss 0.00226037, acc 1
2016-09-06T11:10:11.891012: step 9705, loss 0.0177505, acc 0.98
2016-09-06T11:10:12.701735: step 9706, loss 0.0229902, acc 0.98
2016-09-06T11:10:13.534987: step 9707, loss 0.0416022, acc 0.96
2016-09-06T11:10:14.362131: step 9708, loss 0.0348543, acc 0.96
2016-09-06T11:10:15.175919: step 9709, loss 0.0193853, acc 1
2016-09-06T11:10:16.006579: step 9710, loss 0.0225356, acc 0.98
2016-09-06T11:10:16.816944: step 9711, loss 0.00379374, acc 1
2016-09-06T11:10:17.602284: step 9712, loss 0.0115414, acc 1
2016-09-06T11:10:18.441447: step 9713, loss 0.0232189, acc 0.98
2016-09-06T11:10:19.264180: step 9714, loss 0.0025498, acc 1
2016-09-06T11:10:20.081402: step 9715, loss 0.0108728, acc 1
2016-09-06T11:10:20.888210: step 9716, loss 0.00818945, acc 1
2016-09-06T11:10:21.716120: step 9717, loss 0.0411566, acc 0.96
2016-09-06T11:10:22.527108: step 9718, loss 0.00247138, acc 1
2016-09-06T11:10:23.340897: step 9719, loss 0.0160583, acc 1
2016-09-06T11:10:24.187935: step 9720, loss 0.0399494, acc 0.98
2016-09-06T11:10:24.963340: step 9721, loss 0.0029175, acc 1
2016-09-06T11:10:25.763746: step 9722, loss 0.0103112, acc 1
2016-09-06T11:10:26.600254: step 9723, loss 0.00279488, acc 1
2016-09-06T11:10:27.372341: step 9724, loss 0.024026, acc 1
2016-09-06T11:10:28.198884: step 9725, loss 0.033597, acc 0.98
2016-09-06T11:10:29.009399: step 9726, loss 0.00443068, acc 1
2016-09-06T11:10:29.786752: step 9727, loss 0.0170153, acc 0.98
2016-09-06T11:10:30.628139: step 9728, loss 0.054348, acc 0.96
2016-09-06T11:10:31.455024: step 9729, loss 0.00265054, acc 1
2016-09-06T11:10:32.258361: step 9730, loss 0.0138245, acc 1
2016-09-06T11:10:33.057047: step 9731, loss 0.029167, acc 1
2016-09-06T11:10:33.937039: step 9732, loss 0.0398154, acc 0.98
2016-09-06T11:10:34.743597: step 9733, loss 0.0104614, acc 1
2016-09-06T11:10:35.572567: step 9734, loss 0.0459268, acc 0.98
2016-09-06T11:10:36.403912: step 9735, loss 0.0111319, acc 1
2016-09-06T11:10:37.206402: step 9736, loss 0.0179547, acc 0.98
2016-09-06T11:10:38.003242: step 9737, loss 0.00610203, acc 1
2016-09-06T11:10:38.838237: step 9738, loss 0.0134496, acc 1
2016-09-06T11:10:39.664208: step 9739, loss 0.00256704, acc 1
2016-09-06T11:10:40.484377: step 9740, loss 0.00234168, acc 1
2016-09-06T11:10:41.309314: step 9741, loss 0.0468751, acc 0.98
2016-09-06T11:10:42.126084: step 9742, loss 0.0250632, acc 0.98
2016-09-06T11:10:42.984681: step 9743, loss 0.0231613, acc 0.98
2016-09-06T11:10:43.845016: step 9744, loss 0.0215969, acc 0.98
2016-09-06T11:10:44.660984: step 9745, loss 0.00715598, acc 1
2016-09-06T11:10:45.456230: step 9746, loss 0.0310346, acc 1
2016-09-06T11:10:46.278441: step 9747, loss 0.0255193, acc 0.98
2016-09-06T11:10:47.090108: step 9748, loss 0.0384292, acc 0.98
2016-09-06T11:10:47.895288: step 9749, loss 0.0241407, acc 0.98
2016-09-06T11:10:48.707840: step 9750, loss 0.0302084, acc 1
2016-09-06T11:10:49.537120: step 9751, loss 0.00450639, acc 1
2016-09-06T11:10:50.332197: step 9752, loss 0.00210997, acc 1
2016-09-06T11:10:51.123374: step 9753, loss 0.0093721, acc 1
2016-09-06T11:10:51.937789: step 9754, loss 0.0144704, acc 1
2016-09-06T11:10:52.735383: step 9755, loss 0.0330684, acc 0.98
2016-09-06T11:10:53.552478: step 9756, loss 0.0115059, acc 1
2016-09-06T11:10:54.397362: step 9757, loss 0.0279562, acc 0.98
2016-09-06T11:10:55.186706: step 9758, loss 0.019181, acc 0.98
2016-09-06T11:10:55.984208: step 9759, loss 0.0135539, acc 1
2016-09-06T11:10:56.784233: step 9760, loss 0.0228523, acc 0.98
2016-09-06T11:10:57.561896: step 9761, loss 0.00471877, acc 1
2016-09-06T11:10:58.372951: step 9762, loss 0.0711515, acc 0.94
2016-09-06T11:10:59.203604: step 9763, loss 0.104453, acc 0.98
2016-09-06T11:10:59.974351: step 9764, loss 0.00373358, acc 1
2016-09-06T11:11:00.777348: step 9765, loss 0.0271489, acc 0.98
2016-09-06T11:11:01.604498: step 9766, loss 0.0264808, acc 1
2016-09-06T11:11:02.392946: step 9767, loss 0.0200679, acc 1
2016-09-06T11:11:03.207783: step 9768, loss 0.0018367, acc 1
2016-09-06T11:11:04.015202: step 9769, loss 0.0243342, acc 1
2016-09-06T11:11:04.794417: step 9770, loss 0.015979, acc 0.98
2016-09-06T11:11:05.592762: step 9771, loss 0.008094, acc 1
2016-09-06T11:11:06.380426: step 9772, loss 0.017806, acc 1
2016-09-06T11:11:07.202832: step 9773, loss 0.0035433, acc 1
2016-09-06T11:11:08.044847: step 9774, loss 0.031252, acc 0.98
2016-09-06T11:11:08.880405: step 9775, loss 0.0184152, acc 1
2016-09-06T11:11:09.666773: step 9776, loss 0.00287927, acc 1
2016-09-06T11:11:10.466525: step 9777, loss 0.0141951, acc 1
2016-09-06T11:11:11.260685: step 9778, loss 0.0202227, acc 1
2016-09-06T11:11:12.041012: step 9779, loss 0.0167905, acc 0.98
2016-09-06T11:11:12.847257: step 9780, loss 0.0941569, acc 0.98
2016-09-06T11:11:13.662625: step 9781, loss 0.0657175, acc 0.96
2016-09-06T11:11:14.432003: step 9782, loss 0.00826869, acc 1
2016-09-06T11:11:15.261062: step 9783, loss 0.00185419, acc 1
2016-09-06T11:11:16.058280: step 9784, loss 0.00189664, acc 1
2016-09-06T11:11:16.855231: step 9785, loss 0.0059292, acc 1
2016-09-06T11:11:17.680320: step 9786, loss 0.0290553, acc 0.98
2016-09-06T11:11:18.505410: step 9787, loss 0.0242074, acc 1
2016-09-06T11:11:19.308280: step 9788, loss 0.0640473, acc 0.98
2016-09-06T11:11:20.125889: step 9789, loss 0.0358166, acc 0.98
2016-09-06T11:11:20.940480: step 9790, loss 0.00784313, acc 1
2016-09-06T11:11:21.740337: step 9791, loss 0.0122599, acc 1
2016-09-06T11:11:22.497166: step 9792, loss 0.0367477, acc 0.977273
2016-09-06T11:11:23.306276: step 9793, loss 0.0165548, acc 1
2016-09-06T11:11:24.100019: step 9794, loss 0.0753041, acc 0.98
2016-09-06T11:11:24.945823: step 9795, loss 0.00347229, acc 1
2016-09-06T11:11:25.749455: step 9796, loss 0.0177903, acc 1
2016-09-06T11:11:26.567323: step 9797, loss 0.0781486, acc 0.98
2016-09-06T11:11:27.379007: step 9798, loss 0.0109521, acc 1
2016-09-06T11:11:28.201582: step 9799, loss 0.00802863, acc 1
2016-09-06T11:11:29.008568: step 9800, loss 0.00309851, acc 1

Evaluation:
2016-09-06T11:11:32.721564: step 9800, loss 1.55675, acc 0.754221

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-9800

2016-09-06T11:11:34.638064: step 9801, loss 0.0234267, acc 1
2016-09-06T11:11:35.450553: step 9802, loss 0.033139, acc 0.98
2016-09-06T11:11:36.278543: step 9803, loss 0.00235648, acc 1
2016-09-06T11:11:37.094778: step 9804, loss 0.0143292, acc 1
2016-09-06T11:11:37.960011: step 9805, loss 0.0111676, acc 1
2016-09-06T11:11:38.787145: step 9806, loss 0.0520802, acc 0.96
2016-09-06T11:11:39.603284: step 9807, loss 0.032466, acc 0.98
2016-09-06T11:11:40.451400: step 9808, loss 0.0061127, acc 1
2016-09-06T11:11:41.216531: step 9809, loss 0.00490199, acc 1
2016-09-06T11:11:42.036367: step 9810, loss 0.0735488, acc 0.98
2016-09-06T11:11:42.870315: step 9811, loss 0.00477106, acc 1
2016-09-06T11:11:43.678790: step 9812, loss 0.00615575, acc 1
2016-09-06T11:11:44.490705: step 9813, loss 0.0146631, acc 1
2016-09-06T11:11:45.307742: step 9814, loss 0.112662, acc 0.96
2016-09-06T11:11:46.117549: step 9815, loss 0.00477677, acc 1
2016-09-06T11:11:46.936450: step 9816, loss 0.0418105, acc 0.98
2016-09-06T11:11:47.748207: step 9817, loss 0.00320618, acc 1
2016-09-06T11:11:48.561471: step 9818, loss 0.0167747, acc 1
2016-09-06T11:11:49.397403: step 9819, loss 0.0311663, acc 0.98
2016-09-06T11:11:50.235214: step 9820, loss 0.00211521, acc 1
2016-09-06T11:11:51.053922: step 9821, loss 0.00407264, acc 1
2016-09-06T11:11:51.864088: step 9822, loss 0.0227332, acc 0.98
2016-09-06T11:11:52.666877: step 9823, loss 0.00395864, acc 1
2016-09-06T11:11:53.466257: step 9824, loss 0.00256761, acc 1
2016-09-06T11:11:54.264052: step 9825, loss 0.0149897, acc 1
2016-09-06T11:11:55.110792: step 9826, loss 0.00664796, acc 1
2016-09-06T11:11:55.934829: step 9827, loss 0.0103023, acc 1
2016-09-06T11:11:56.735097: step 9828, loss 0.00227063, acc 1
2016-09-06T11:11:57.592750: step 9829, loss 0.0229767, acc 1
2016-09-06T11:11:58.415659: step 9830, loss 0.0364278, acc 0.98
2016-09-06T11:11:59.204512: step 9831, loss 0.0370693, acc 0.98
2016-09-06T11:12:00.015729: step 9832, loss 0.0024035, acc 1
2016-09-06T11:12:00.858242: step 9833, loss 0.0454588, acc 0.98
2016-09-06T11:12:01.636546: step 9834, loss 0.0060318, acc 1
2016-09-06T11:12:02.455154: step 9835, loss 0.0111712, acc 1
2016-09-06T11:12:03.270996: step 9836, loss 0.0042752, acc 1
2016-09-06T11:12:04.068710: step 9837, loss 0.0247448, acc 0.98
2016-09-06T11:12:04.854006: step 9838, loss 0.0357405, acc 1
2016-09-06T11:12:05.682144: step 9839, loss 0.00254587, acc 1
2016-09-06T11:12:06.487652: step 9840, loss 0.00313288, acc 1
2016-09-06T11:12:07.281830: step 9841, loss 0.109968, acc 0.96
2016-09-06T11:12:08.094861: step 9842, loss 0.00322989, acc 1
2016-09-06T11:12:08.876338: step 9843, loss 0.00512588, acc 1
2016-09-06T11:12:09.663484: step 9844, loss 0.0302474, acc 0.98
2016-09-06T11:12:10.480825: step 9845, loss 0.0233964, acc 1
2016-09-06T11:12:11.260222: step 9846, loss 0.00293189, acc 1
2016-09-06T11:12:12.094508: step 9847, loss 0.0245572, acc 0.98
2016-09-06T11:12:12.916999: step 9848, loss 0.00369044, acc 1
2016-09-06T11:12:13.700973: step 9849, loss 0.00462022, acc 1
2016-09-06T11:12:14.514080: step 9850, loss 0.0362574, acc 0.96
2016-09-06T11:12:15.332855: step 9851, loss 0.155749, acc 0.98
2016-09-06T11:12:16.128300: step 9852, loss 0.0148213, acc 1
2016-09-06T11:12:16.925956: step 9853, loss 0.0196812, acc 1
2016-09-06T11:12:17.747040: step 9854, loss 0.027624, acc 0.98
2016-09-06T11:12:18.550636: step 9855, loss 0.00216745, acc 1
2016-09-06T11:12:19.360730: step 9856, loss 0.022863, acc 1
2016-09-06T11:12:20.217416: step 9857, loss 0.00904531, acc 1
2016-09-06T11:12:21.013401: step 9858, loss 0.00508142, acc 1
2016-09-06T11:12:21.848106: step 9859, loss 0.0111553, acc 1
2016-09-06T11:12:22.677011: step 9860, loss 0.03859, acc 0.98
2016-09-06T11:12:23.493328: step 9861, loss 0.0205781, acc 0.98
2016-09-06T11:12:24.308893: step 9862, loss 0.0269776, acc 0.98
2016-09-06T11:12:25.135480: step 9863, loss 0.00378193, acc 1
2016-09-06T11:12:25.974107: step 9864, loss 0.0235919, acc 0.98
2016-09-06T11:12:26.784384: step 9865, loss 0.00487412, acc 1
2016-09-06T11:12:27.609550: step 9866, loss 0.00219048, acc 1
2016-09-06T11:12:28.418525: step 9867, loss 0.0184967, acc 1
2016-09-06T11:12:29.241127: step 9868, loss 0.0292624, acc 0.98
2016-09-06T11:12:30.077298: step 9869, loss 0.0206788, acc 0.98
2016-09-06T11:12:30.910452: step 9870, loss 0.0679602, acc 0.98
2016-09-06T11:12:31.745270: step 9871, loss 0.0297023, acc 0.98
2016-09-06T11:12:32.575968: step 9872, loss 0.0202885, acc 1
2016-09-06T11:12:33.402857: step 9873, loss 0.0023359, acc 1
2016-09-06T11:12:34.246236: step 9874, loss 0.0305504, acc 0.98
2016-09-06T11:12:35.076950: step 9875, loss 0.0630974, acc 0.94
2016-09-06T11:12:35.891198: step 9876, loss 0.00404885, acc 1
2016-09-06T11:12:36.691440: step 9877, loss 0.0842214, acc 0.98
2016-09-06T11:12:37.496036: step 9878, loss 0.0143951, acc 1
2016-09-06T11:12:38.307454: step 9879, loss 0.00265639, acc 1
2016-09-06T11:12:39.111347: step 9880, loss 0.00318519, acc 1
2016-09-06T11:12:39.908767: step 9881, loss 0.00839247, acc 1
2016-09-06T11:12:40.750277: step 9882, loss 0.0116696, acc 1
2016-09-06T11:12:41.566490: step 9883, loss 0.0144945, acc 1
2016-09-06T11:12:42.387083: step 9884, loss 0.00217295, acc 1
2016-09-06T11:12:43.185974: step 9885, loss 0.0419533, acc 0.98
2016-09-06T11:12:43.966869: step 9886, loss 0.00272394, acc 1
2016-09-06T11:12:44.758919: step 9887, loss 0.0262729, acc 0.98
2016-09-06T11:12:45.583121: step 9888, loss 0.0361055, acc 0.96
2016-09-06T11:12:46.379300: step 9889, loss 0.0171875, acc 0.98
2016-09-06T11:12:47.216036: step 9890, loss 0.0471572, acc 0.96
2016-09-06T11:12:48.034738: step 9891, loss 0.00319629, acc 1
2016-09-06T11:12:48.833875: step 9892, loss 0.00317426, acc 1
2016-09-06T11:12:49.602902: step 9893, loss 0.0154477, acc 1
2016-09-06T11:12:50.439825: step 9894, loss 0.0816566, acc 0.98
2016-09-06T11:12:51.202998: step 9895, loss 0.0230487, acc 0.98
2016-09-06T11:12:52.005048: step 9896, loss 0.00934167, acc 1
2016-09-06T11:12:52.828159: step 9897, loss 0.040819, acc 0.98
2016-09-06T11:12:53.609692: step 9898, loss 0.00377691, acc 1
2016-09-06T11:12:54.396378: step 9899, loss 0.0161456, acc 0.98
2016-09-06T11:12:55.230268: step 9900, loss 0.00766301, acc 1

Evaluation:
2016-09-06T11:12:58.944781: step 9900, loss 1.90842, acc 0.757036

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-9900

2016-09-06T11:13:00.949432: step 9901, loss 0.0445621, acc 0.96
2016-09-06T11:13:01.747653: step 9902, loss 0.0276747, acc 1
2016-09-06T11:13:02.549059: step 9903, loss 0.00221717, acc 1
2016-09-06T11:13:03.332483: step 9904, loss 0.0636177, acc 0.96
2016-09-06T11:13:04.160791: step 9905, loss 0.0178863, acc 0.98
2016-09-06T11:13:04.971229: step 9906, loss 0.00917713, acc 1
2016-09-06T11:13:05.794742: step 9907, loss 0.0320226, acc 1
2016-09-06T11:13:06.597421: step 9908, loss 0.00882523, acc 1
2016-09-06T11:13:07.401587: step 9909, loss 0.0176511, acc 1
2016-09-06T11:13:08.222141: step 9910, loss 0.00246905, acc 1
2016-09-06T11:13:09.034434: step 9911, loss 0.00680977, acc 1
2016-09-06T11:13:09.886264: step 9912, loss 0.0150907, acc 1
2016-09-06T11:13:10.686038: step 9913, loss 0.0181354, acc 0.98
2016-09-06T11:13:11.502745: step 9914, loss 0.00266312, acc 1
2016-09-06T11:13:12.332480: step 9915, loss 0.0153505, acc 1
2016-09-06T11:13:13.125120: step 9916, loss 0.0223165, acc 0.98
2016-09-06T11:13:13.942917: step 9917, loss 0.0024863, acc 1
2016-09-06T11:13:14.761598: step 9918, loss 0.00246292, acc 1
2016-09-06T11:13:15.572078: step 9919, loss 0.0370629, acc 0.98
2016-09-06T11:13:16.385662: step 9920, loss 0.00248967, acc 1
2016-09-06T11:13:17.200757: step 9921, loss 0.00435425, acc 1
2016-09-06T11:13:17.994629: step 9922, loss 0.0434465, acc 0.98
2016-09-06T11:13:18.811475: step 9923, loss 0.0131036, acc 1
2016-09-06T11:13:19.628370: step 9924, loss 0.0330221, acc 0.98
2016-09-06T11:13:20.466250: step 9925, loss 0.0271607, acc 0.98
2016-09-06T11:13:21.273709: step 9926, loss 0.0125272, acc 1
2016-09-06T11:13:22.104272: step 9927, loss 0.0772465, acc 0.96
2016-09-06T11:13:22.916712: step 9928, loss 0.00258506, acc 1
2016-09-06T11:13:23.739177: step 9929, loss 0.00346946, acc 1
2016-09-06T11:13:24.570380: step 9930, loss 0.0190197, acc 1
2016-09-06T11:13:25.375884: step 9931, loss 0.00219492, acc 1
2016-09-06T11:13:26.193393: step 9932, loss 0.00323241, acc 1
2016-09-06T11:13:27.022093: step 9933, loss 0.0199342, acc 1
2016-09-06T11:13:27.883814: step 9934, loss 0.00212743, acc 1
2016-09-06T11:13:28.684725: step 9935, loss 0.0187316, acc 0.98
2016-09-06T11:13:29.550732: step 9936, loss 0.00255341, acc 1
2016-09-06T11:13:30.373285: step 9937, loss 0.00210621, acc 1
2016-09-06T11:13:31.152101: step 9938, loss 0.00512161, acc 1
2016-09-06T11:13:32.004124: step 9939, loss 0.0314958, acc 0.98
2016-09-06T11:13:32.829452: step 9940, loss 0.00438294, acc 1
2016-09-06T11:13:33.617643: step 9941, loss 0.00256942, acc 1
2016-09-06T11:13:34.450881: step 9942, loss 0.0156076, acc 1
2016-09-06T11:13:35.254289: step 9943, loss 0.0021344, acc 1
2016-09-06T11:13:36.018461: step 9944, loss 0.00195829, acc 1
2016-09-06T11:13:36.832429: step 9945, loss 0.00217318, acc 1
2016-09-06T11:13:37.682674: step 9946, loss 0.0158129, acc 1
2016-09-06T11:13:38.468705: step 9947, loss 0.0121318, acc 1
2016-09-06T11:13:39.272923: step 9948, loss 0.0208763, acc 1
2016-09-06T11:13:40.099571: step 9949, loss 0.010013, acc 1
2016-09-06T11:13:40.862320: step 9950, loss 0.0111484, acc 1
2016-09-06T11:13:41.655074: step 9951, loss 0.00218343, acc 1
2016-09-06T11:13:42.485829: step 9952, loss 0.0484921, acc 0.98
2016-09-06T11:13:43.274834: step 9953, loss 0.00601736, acc 1
2016-09-06T11:13:44.080304: step 9954, loss 0.0134021, acc 1
2016-09-06T11:13:44.928696: step 9955, loss 0.0129453, acc 1
2016-09-06T11:13:45.723019: step 9956, loss 0.00258398, acc 1
2016-09-06T11:13:46.525253: step 9957, loss 0.00207795, acc 1
2016-09-06T11:13:47.355870: step 9958, loss 0.0315547, acc 0.96
2016-09-06T11:13:48.158813: step 9959, loss 0.00257596, acc 1
2016-09-06T11:13:48.978539: step 9960, loss 0.00908377, acc 1
2016-09-06T11:13:49.813468: step 9961, loss 0.0324797, acc 1
2016-09-06T11:13:50.636760: step 9962, loss 0.00634678, acc 1
2016-09-06T11:13:51.464153: step 9963, loss 0.0388181, acc 0.98
2016-09-06T11:13:52.296586: step 9964, loss 0.0194388, acc 1
2016-09-06T11:13:53.097337: step 9965, loss 0.00239526, acc 1
2016-09-06T11:13:53.913594: step 9966, loss 0.0264207, acc 0.98
2016-09-06T11:13:54.776412: step 9967, loss 0.0197642, acc 1
2016-09-06T11:13:55.608057: step 9968, loss 0.00443162, acc 1
2016-09-06T11:13:56.430479: step 9969, loss 0.00356997, acc 1
2016-09-06T11:13:57.241449: step 9970, loss 0.0276897, acc 1
2016-09-06T11:13:58.033024: step 9971, loss 0.00212315, acc 1
2016-09-06T11:13:58.839914: step 9972, loss 0.0527448, acc 0.96
2016-09-06T11:13:59.657681: step 9973, loss 0.0164096, acc 0.98
2016-09-06T11:14:00.461446: step 9974, loss 0.00255257, acc 1
2016-09-06T11:14:01.258813: step 9975, loss 0.00443752, acc 1
2016-09-06T11:14:02.103983: step 9976, loss 0.00383229, acc 1
2016-09-06T11:14:02.910926: step 9977, loss 0.00801357, acc 1
2016-09-06T11:14:03.741306: step 9978, loss 0.0179866, acc 0.98
2016-09-06T11:14:04.588670: step 9979, loss 0.00704516, acc 1
2016-09-06T11:14:05.396913: step 9980, loss 0.00321968, acc 1
2016-09-06T11:14:06.199605: step 9981, loss 0.00391039, acc 1
2016-09-06T11:14:07.012477: step 9982, loss 0.045754, acc 0.98
2016-09-06T11:14:07.815244: step 9983, loss 0.0159702, acc 1
2016-09-06T11:14:08.544032: step 9984, loss 0.00269507, acc 1
2016-09-06T11:14:09.365956: step 9985, loss 0.00307614, acc 1
2016-09-06T11:14:10.166882: step 9986, loss 0.008535, acc 1
2016-09-06T11:14:10.983043: step 9987, loss 0.0023113, acc 1
2016-09-06T11:14:11.841641: step 9988, loss 0.0193699, acc 0.98
2016-09-06T11:14:12.712883: step 9989, loss 0.0303126, acc 0.98
2016-09-06T11:14:13.512840: step 9990, loss 0.0377045, acc 0.98
2016-09-06T11:14:14.333411: step 9991, loss 0.0253652, acc 1
2016-09-06T11:14:15.155710: step 9992, loss 0.00277986, acc 1
2016-09-06T11:14:15.939570: step 9993, loss 0.0104775, acc 1
2016-09-06T11:14:16.770498: step 9994, loss 0.0107222, acc 1
2016-09-06T11:14:17.575690: step 9995, loss 0.00211431, acc 1
2016-09-06T11:14:18.352068: step 9996, loss 0.00230642, acc 1
2016-09-06T11:14:19.160967: step 9997, loss 0.0178591, acc 1
2016-09-06T11:14:19.987430: step 9998, loss 0.0157553, acc 1
2016-09-06T11:14:20.776163: step 9999, loss 0.00382018, acc 1
2016-09-06T11:14:21.601590: step 10000, loss 0.00209831, acc 1

Evaluation:
2016-09-06T11:14:25.330624: step 10000, loss 2.55015, acc 0.757036

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-10000

2016-09-06T11:14:27.282842: step 10001, loss 0.0389171, acc 0.98
2016-09-06T11:14:28.096010: step 10002, loss 0.0030842, acc 1
2016-09-06T11:14:28.903858: step 10003, loss 0.0164118, acc 0.98
2016-09-06T11:14:29.717756: step 10004, loss 0.00823754, acc 1
2016-09-06T11:14:30.517402: step 10005, loss 0.00247735, acc 1
2016-09-06T11:14:31.318268: step 10006, loss 0.00577427, acc 1
2016-09-06T11:14:32.123141: step 10007, loss 0.0290243, acc 0.98
2016-09-06T11:14:32.933877: step 10008, loss 0.0100932, acc 1
2016-09-06T11:14:33.735620: step 10009, loss 0.0022216, acc 1
2016-09-06T11:14:34.565011: step 10010, loss 0.0360917, acc 0.98
2016-09-06T11:14:35.372833: step 10011, loss 0.0364221, acc 0.98
2016-09-06T11:14:36.185979: step 10012, loss 0.00256772, acc 1
2016-09-06T11:14:37.005747: step 10013, loss 0.00227671, acc 1
2016-09-06T11:14:37.819341: step 10014, loss 0.0314934, acc 1
2016-09-06T11:14:38.611334: step 10015, loss 0.00194232, acc 1
2016-09-06T11:14:39.448316: step 10016, loss 0.003167, acc 1
2016-09-06T11:14:40.252536: step 10017, loss 0.00330368, acc 1
2016-09-06T11:14:41.042235: step 10018, loss 0.0419558, acc 0.98
2016-09-06T11:14:41.878629: step 10019, loss 0.00192194, acc 1
2016-09-06T11:14:42.682490: step 10020, loss 0.00190569, acc 1
2016-09-06T11:14:43.483016: step 10021, loss 0.0109021, acc 1
2016-09-06T11:14:44.302047: step 10022, loss 0.00363982, acc 1
2016-09-06T11:14:45.099181: step 10023, loss 0.00498743, acc 1
2016-09-06T11:14:45.914787: step 10024, loss 0.00246387, acc 1
2016-09-06T11:14:46.722658: step 10025, loss 0.0195134, acc 0.98
2016-09-06T11:14:47.507130: step 10026, loss 0.0133794, acc 1
2016-09-06T11:14:48.301082: step 10027, loss 0.0273048, acc 1
2016-09-06T11:14:49.108693: step 10028, loss 0.0636319, acc 0.98
2016-09-06T11:14:49.919379: step 10029, loss 0.00179913, acc 1
2016-09-06T11:14:50.749007: step 10030, loss 0.0231672, acc 1
2016-09-06T11:14:51.571489: step 10031, loss 0.0125565, acc 1
2016-09-06T11:14:52.392117: step 10032, loss 0.00186865, acc 1
2016-09-06T11:14:53.204616: step 10033, loss 0.00748032, acc 1
2016-09-06T11:14:54.025562: step 10034, loss 0.00173089, acc 1
2016-09-06T11:14:54.820208: step 10035, loss 0.00630375, acc 1
2016-09-06T11:14:55.636871: step 10036, loss 0.0921497, acc 0.96
2016-09-06T11:14:56.456129: step 10037, loss 0.028705, acc 0.98
2016-09-06T11:14:57.294174: step 10038, loss 0.0303854, acc 0.98
2016-09-06T11:14:58.094880: step 10039, loss 0.00237461, acc 1
2016-09-06T11:14:58.935077: step 10040, loss 0.00375173, acc 1
2016-09-06T11:14:59.786738: step 10041, loss 0.0235731, acc 0.98
2016-09-06T11:15:00.643733: step 10042, loss 0.0542614, acc 0.96
2016-09-06T11:15:01.473029: step 10043, loss 0.00754135, acc 1
2016-09-06T11:15:02.268644: step 10044, loss 0.001695, acc 1
2016-09-06T11:15:03.065553: step 10045, loss 0.030258, acc 0.98
2016-09-06T11:15:03.892476: step 10046, loss 0.00303928, acc 1
2016-09-06T11:15:04.690990: step 10047, loss 0.00172082, acc 1
2016-09-06T11:15:05.466935: step 10048, loss 0.0202031, acc 0.98
2016-09-06T11:15:06.291826: step 10049, loss 0.00763705, acc 1
2016-09-06T11:15:07.113843: step 10050, loss 0.00720243, acc 1
2016-09-06T11:15:07.911202: step 10051, loss 0.014361, acc 1
2016-09-06T11:15:08.713470: step 10052, loss 0.055373, acc 0.96
2016-09-06T11:15:09.514168: step 10053, loss 0.0192988, acc 0.98
2016-09-06T11:15:10.315437: step 10054, loss 0.0201431, acc 0.98
2016-09-06T11:15:11.133704: step 10055, loss 0.00206332, acc 1
2016-09-06T11:15:11.959155: step 10056, loss 0.00227111, acc 1
2016-09-06T11:15:12.756142: step 10057, loss 0.0021956, acc 1
2016-09-06T11:15:13.590219: step 10058, loss 0.0519437, acc 0.98
2016-09-06T11:15:14.433774: step 10059, loss 0.00747868, acc 1
2016-09-06T11:15:15.261935: step 10060, loss 0.0196104, acc 0.98
2016-09-06T11:15:16.064330: step 10061, loss 0.0118668, acc 1
2016-09-06T11:15:16.925532: step 10062, loss 0.0167367, acc 1
2016-09-06T11:15:17.756275: step 10063, loss 0.0283402, acc 0.98
2016-09-06T11:15:18.557140: step 10064, loss 0.00692835, acc 1
2016-09-06T11:15:19.390583: step 10065, loss 0.0230827, acc 0.98
2016-09-06T11:15:20.207461: step 10066, loss 0.00164557, acc 1
2016-09-06T11:15:21.047361: step 10067, loss 0.0102145, acc 1
2016-09-06T11:15:21.896339: step 10068, loss 0.00167079, acc 1
2016-09-06T11:15:22.708927: step 10069, loss 0.00823371, acc 1
2016-09-06T11:15:23.524585: step 10070, loss 0.0137777, acc 1
2016-09-06T11:15:24.385693: step 10071, loss 0.00268963, acc 1
2016-09-06T11:15:25.202429: step 10072, loss 0.00763575, acc 1
2016-09-06T11:15:26.010027: step 10073, loss 0.00861993, acc 1
2016-09-06T11:15:26.816942: step 10074, loss 0.00190204, acc 1
2016-09-06T11:15:27.622760: step 10075, loss 0.0045528, acc 1
2016-09-06T11:15:28.418997: step 10076, loss 0.00261159, acc 1
2016-09-06T11:15:29.276742: step 10077, loss 0.0469975, acc 0.96
2016-09-06T11:15:30.103589: step 10078, loss 0.0242352, acc 0.98
2016-09-06T11:15:30.904463: step 10079, loss 0.139294, acc 0.94
2016-09-06T11:15:31.742918: step 10080, loss 0.020399, acc 0.98
2016-09-06T11:15:32.557254: step 10081, loss 0.00218364, acc 1
2016-09-06T11:15:33.357545: step 10082, loss 0.0216893, acc 1
2016-09-06T11:15:34.158851: step 10083, loss 0.0266225, acc 0.98
2016-09-06T11:15:34.971297: step 10084, loss 0.0154931, acc 1
2016-09-06T11:15:35.728409: step 10085, loss 0.045929, acc 0.98
2016-09-06T11:15:36.538091: step 10086, loss 0.00404388, acc 1
2016-09-06T11:15:37.353271: step 10087, loss 0.00341442, acc 1
2016-09-06T11:15:38.133487: step 10088, loss 0.0024502, acc 1
2016-09-06T11:15:38.935662: step 10089, loss 0.0338456, acc 0.96
2016-09-06T11:15:39.782374: step 10090, loss 0.00238923, acc 1
2016-09-06T11:15:40.546606: step 10091, loss 0.0289166, acc 1
2016-09-06T11:15:41.345788: step 10092, loss 0.006284, acc 1
2016-09-06T11:15:42.157991: step 10093, loss 0.0142282, acc 1
2016-09-06T11:15:42.964229: step 10094, loss 0.0405119, acc 0.98
2016-09-06T11:15:43.764870: step 10095, loss 0.00234014, acc 1
2016-09-06T11:15:44.594645: step 10096, loss 0.0405343, acc 0.98
2016-09-06T11:15:45.375185: step 10097, loss 0.00687918, acc 1
2016-09-06T11:15:46.180292: step 10098, loss 0.00599544, acc 1
2016-09-06T11:15:46.980953: step 10099, loss 0.0613696, acc 0.96
2016-09-06T11:15:47.774691: step 10100, loss 0.0108143, acc 1

Evaluation:
2016-09-06T11:15:51.487713: step 10100, loss 2.00021, acc 0.75985

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-10100

2016-09-06T11:15:53.322332: step 10101, loss 0.00219762, acc 1
2016-09-06T11:15:54.119615: step 10102, loss 0.00332626, acc 1
2016-09-06T11:15:54.918089: step 10103, loss 0.0031329, acc 1
2016-09-06T11:15:55.722039: step 10104, loss 0.0424101, acc 0.96
2016-09-06T11:15:56.534226: step 10105, loss 0.017365, acc 1
2016-09-06T11:15:57.340396: step 10106, loss 0.0045769, acc 1
2016-09-06T11:15:58.167283: step 10107, loss 0.0556512, acc 0.96
2016-09-06T11:15:58.991548: step 10108, loss 0.00676516, acc 1
2016-09-06T11:15:59.779213: step 10109, loss 0.0315857, acc 0.98
2016-09-06T11:16:00.629915: step 10110, loss 0.0148641, acc 1
2016-09-06T11:16:01.434899: step 10111, loss 0.0209807, acc 0.98
2016-09-06T11:16:02.253730: step 10112, loss 0.0106284, acc 1
2016-09-06T11:16:03.080525: step 10113, loss 0.00295355, acc 1
2016-09-06T11:16:03.948717: step 10114, loss 0.0191417, acc 1
2016-09-06T11:16:04.757410: step 10115, loss 0.0284889, acc 0.98
2016-09-06T11:16:05.547985: step 10116, loss 0.00533736, acc 1
2016-09-06T11:16:06.374475: step 10117, loss 0.00292061, acc 1
2016-09-06T11:16:07.145673: step 10118, loss 0.0558642, acc 0.96
2016-09-06T11:16:07.985480: step 10119, loss 0.0104485, acc 1
2016-09-06T11:16:08.811396: step 10120, loss 0.0507794, acc 0.98
2016-09-06T11:16:09.620544: step 10121, loss 0.0774488, acc 0.98
2016-09-06T11:16:10.454336: step 10122, loss 0.0443649, acc 0.98
2016-09-06T11:16:11.323353: step 10123, loss 0.00619497, acc 1
2016-09-06T11:16:12.157233: step 10124, loss 0.00439112, acc 1
2016-09-06T11:16:13.013884: step 10125, loss 0.0687353, acc 0.96
2016-09-06T11:16:13.837166: step 10126, loss 0.00450879, acc 1
2016-09-06T11:16:14.636727: step 10127, loss 0.0402796, acc 0.98
2016-09-06T11:16:15.460164: step 10128, loss 0.0276064, acc 1
2016-09-06T11:16:16.291907: step 10129, loss 0.0152663, acc 1
2016-09-06T11:16:17.108624: step 10130, loss 0.0133633, acc 1
2016-09-06T11:16:17.930101: step 10131, loss 0.0171133, acc 1
2016-09-06T11:16:18.754544: step 10132, loss 0.00395496, acc 1
2016-09-06T11:16:19.584940: step 10133, loss 0.0261003, acc 0.98
2016-09-06T11:16:20.389385: step 10134, loss 0.0770342, acc 0.98
2016-09-06T11:16:21.208276: step 10135, loss 0.00503132, acc 1
2016-09-06T11:16:22.036810: step 10136, loss 0.00702716, acc 1
2016-09-06T11:16:22.837956: step 10137, loss 0.00498372, acc 1
2016-09-06T11:16:23.647974: step 10138, loss 0.00326446, acc 1
2016-09-06T11:16:24.459364: step 10139, loss 0.0168932, acc 1
2016-09-06T11:16:25.256814: step 10140, loss 0.0084759, acc 1
2016-09-06T11:16:26.045605: step 10141, loss 0.00373732, acc 1
2016-09-06T11:16:26.882601: step 10142, loss 0.0235805, acc 0.98
2016-09-06T11:16:27.741061: step 10143, loss 0.00457669, acc 1
2016-09-06T11:16:28.552368: step 10144, loss 0.0231303, acc 0.98
2016-09-06T11:16:29.399380: step 10145, loss 0.0430466, acc 0.98
2016-09-06T11:16:30.202790: step 10146, loss 0.0169891, acc 1
2016-09-06T11:16:31.013348: step 10147, loss 0.0176673, acc 0.98
2016-09-06T11:16:31.841434: step 10148, loss 0.0077642, acc 1
2016-09-06T11:16:32.660881: step 10149, loss 0.0177461, acc 0.98
2016-09-06T11:16:33.484740: step 10150, loss 0.0204642, acc 1
2016-09-06T11:16:34.346530: step 10151, loss 0.00993034, acc 1
2016-09-06T11:16:35.164255: step 10152, loss 0.0338779, acc 0.98
2016-09-06T11:16:35.947387: step 10153, loss 0.0459806, acc 0.98
2016-09-06T11:16:36.810673: step 10154, loss 0.00628798, acc 1
2016-09-06T11:16:37.607646: step 10155, loss 0.00841964, acc 1
2016-09-06T11:16:38.422879: step 10156, loss 0.0121276, acc 1
2016-09-06T11:16:39.257672: step 10157, loss 0.0148302, acc 1
2016-09-06T11:16:40.065703: step 10158, loss 0.03741, acc 0.96
2016-09-06T11:16:40.860390: step 10159, loss 0.0409533, acc 0.96
2016-09-06T11:16:41.701367: step 10160, loss 0.00333131, acc 1
2016-09-06T11:16:42.504629: step 10161, loss 0.0195979, acc 0.98
2016-09-06T11:16:43.311208: step 10162, loss 0.0712202, acc 0.98
2016-09-06T11:16:44.146702: step 10163, loss 0.0182751, acc 1
2016-09-06T11:16:44.959965: step 10164, loss 0.0177404, acc 0.98
2016-09-06T11:16:45.779106: step 10165, loss 0.0321097, acc 0.98
2016-09-06T11:16:46.578484: step 10166, loss 0.0234051, acc 0.98
2016-09-06T11:16:47.387508: step 10167, loss 0.00295505, acc 1
2016-09-06T11:16:48.228131: step 10168, loss 0.00293769, acc 1
2016-09-06T11:16:49.058427: step 10169, loss 0.0259303, acc 1
2016-09-06T11:16:49.883325: step 10170, loss 0.0153556, acc 1
2016-09-06T11:16:50.667328: step 10171, loss 0.0290283, acc 1
2016-09-06T11:16:51.482535: step 10172, loss 0.052293, acc 0.98
2016-09-06T11:16:52.285451: step 10173, loss 0.0216885, acc 0.98
2016-09-06T11:16:53.105406: step 10174, loss 0.00512208, acc 1
2016-09-06T11:16:53.905619: step 10175, loss 0.0150761, acc 1
2016-09-06T11:16:54.626949: step 10176, loss 0.00268613, acc 1
2016-09-06T11:16:55.450541: step 10177, loss 0.0384211, acc 0.98
2016-09-06T11:16:56.268361: step 10178, loss 0.0048644, acc 1
2016-09-06T11:16:57.084535: step 10179, loss 0.00284163, acc 1
2016-09-06T11:16:57.882814: step 10180, loss 0.0295194, acc 0.98
2016-09-06T11:16:58.708719: step 10181, loss 0.00360106, acc 1
2016-09-06T11:16:59.559750: step 10182, loss 0.0274749, acc 0.98
2016-09-06T11:17:00.388639: step 10183, loss 0.0322119, acc 0.98
2016-09-06T11:17:01.223356: step 10184, loss 0.0362137, acc 0.98
2016-09-06T11:17:02.074932: step 10185, loss 0.0311424, acc 0.98
2016-09-06T11:17:02.889632: step 10186, loss 0.0979082, acc 0.94
2016-09-06T11:17:03.709489: step 10187, loss 0.00267712, acc 1
2016-09-06T11:17:04.585682: step 10188, loss 0.00261327, acc 1
2016-09-06T11:17:05.393207: step 10189, loss 0.0289885, acc 1
2016-09-06T11:17:06.201865: step 10190, loss 0.04064, acc 0.98
2016-09-06T11:17:07.026007: step 10191, loss 0.0359565, acc 0.98
2016-09-06T11:17:07.841244: step 10192, loss 0.043376, acc 0.98
2016-09-06T11:17:08.644698: step 10193, loss 0.0150382, acc 1
2016-09-06T11:17:09.512880: step 10194, loss 0.00611463, acc 1
2016-09-06T11:17:10.331406: step 10195, loss 0.00496591, acc 1
2016-09-06T11:17:11.149437: step 10196, loss 0.00252821, acc 1
2016-09-06T11:17:11.982392: step 10197, loss 0.0107996, acc 1
2016-09-06T11:17:12.801304: step 10198, loss 0.0116342, acc 1
2016-09-06T11:17:13.634982: step 10199, loss 0.0101349, acc 1
2016-09-06T11:17:14.445893: step 10200, loss 0.120049, acc 0.98

Evaluation:
2016-09-06T11:17:18.185806: step 10200, loss 2.33656, acc 0.755159

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-10200

2016-09-06T11:17:20.157742: step 10201, loss 0.0275117, acc 0.98
2016-09-06T11:17:20.947628: step 10202, loss 0.0108635, acc 1
2016-09-06T11:17:21.758593: step 10203, loss 0.0341954, acc 0.98
2016-09-06T11:17:22.567419: step 10204, loss 0.00226225, acc 1
2016-09-06T11:17:23.361669: step 10205, loss 0.0249231, acc 1
2016-09-06T11:17:24.173073: step 10206, loss 0.0157409, acc 1
2016-09-06T11:17:24.999582: step 10207, loss 0.012802, acc 1
2016-09-06T11:17:25.811596: step 10208, loss 0.0208426, acc 0.98
2016-09-06T11:17:26.668658: step 10209, loss 0.00296185, acc 1
2016-09-06T11:17:27.472465: step 10210, loss 0.0206171, acc 0.98
2016-09-06T11:17:28.298723: step 10211, loss 0.00331903, acc 1
2016-09-06T11:17:29.113710: step 10212, loss 0.00540793, acc 1
2016-09-06T11:17:29.995730: step 10213, loss 0.0160811, acc 1
2016-09-06T11:17:30.803181: step 10214, loss 0.0731468, acc 0.98
2016-09-06T11:17:31.618987: step 10215, loss 0.0168925, acc 0.98
2016-09-06T11:17:32.429011: step 10216, loss 0.00576663, acc 1
2016-09-06T11:17:33.238950: step 10217, loss 0.0105118, acc 1
2016-09-06T11:17:34.057151: step 10218, loss 0.0141952, acc 1
2016-09-06T11:17:34.897841: step 10219, loss 0.0512982, acc 0.98
2016-09-06T11:17:35.738828: step 10220, loss 0.0030788, acc 1
2016-09-06T11:17:36.542015: step 10221, loss 0.0170833, acc 1
2016-09-06T11:17:37.366228: step 10222, loss 0.0721227, acc 0.96
2016-09-06T11:17:38.199423: step 10223, loss 0.0119549, acc 1
2016-09-06T11:17:39.008357: step 10224, loss 0.0319803, acc 0.98
2016-09-06T11:17:39.806715: step 10225, loss 0.025093, acc 1
2016-09-06T11:17:40.621582: step 10226, loss 0.0024167, acc 1
2016-09-06T11:17:41.405361: step 10227, loss 0.0415722, acc 0.98
2016-09-06T11:17:42.205950: step 10228, loss 0.0407627, acc 0.98
2016-09-06T11:17:43.017787: step 10229, loss 0.0422601, acc 0.98
2016-09-06T11:17:43.818106: step 10230, loss 0.0734786, acc 0.96
2016-09-06T11:17:44.663415: step 10231, loss 0.0315867, acc 0.98
2016-09-06T11:17:45.489875: step 10232, loss 0.00420087, acc 1
2016-09-06T11:17:46.287679: step 10233, loss 0.00992155, acc 1
2016-09-06T11:17:47.094555: step 10234, loss 0.0324841, acc 0.98
2016-09-06T11:17:47.940998: step 10235, loss 0.0580698, acc 0.98
2016-09-06T11:17:48.717681: step 10236, loss 0.00234029, acc 1
2016-09-06T11:17:49.511812: step 10237, loss 0.0177017, acc 1
2016-09-06T11:17:50.302105: step 10238, loss 0.0113809, acc 1
2016-09-06T11:17:51.105286: step 10239, loss 0.00656806, acc 1
2016-09-06T11:17:51.929401: step 10240, loss 0.0607697, acc 0.98
2016-09-06T11:17:52.744067: step 10241, loss 0.0237555, acc 0.98
2016-09-06T11:17:53.527414: step 10242, loss 0.0340738, acc 0.98
2016-09-06T11:17:54.318057: step 10243, loss 0.0307394, acc 0.98
2016-09-06T11:17:55.140970: step 10244, loss 0.0349244, acc 0.98
2016-09-06T11:17:55.939942: step 10245, loss 0.00376214, acc 1
2016-09-06T11:17:56.755821: step 10246, loss 0.0259489, acc 0.98
2016-09-06T11:17:57.590436: step 10247, loss 0.0476078, acc 0.96
2016-09-06T11:17:58.380567: step 10248, loss 0.00270242, acc 1
2016-09-06T11:17:59.206066: step 10249, loss 0.039438, acc 0.98
2016-09-06T11:18:00.067202: step 10250, loss 0.0193132, acc 0.98
2016-09-06T11:18:00.925450: step 10251, loss 0.0137409, acc 1
2016-09-06T11:18:01.751191: step 10252, loss 0.0169831, acc 0.98
2016-09-06T11:18:02.575962: step 10253, loss 0.0178059, acc 1
2016-09-06T11:18:03.378835: step 10254, loss 0.0310841, acc 1
2016-09-06T11:18:04.204065: step 10255, loss 0.0272168, acc 1
2016-09-06T11:18:05.055345: step 10256, loss 0.00314919, acc 1
2016-09-06T11:18:05.872106: step 10257, loss 0.00346419, acc 1
2016-09-06T11:18:06.682657: step 10258, loss 0.00462035, acc 1
2016-09-06T11:18:07.520786: step 10259, loss 0.00357879, acc 1
2016-09-06T11:18:08.331937: step 10260, loss 0.01543, acc 1
2016-09-06T11:18:09.137105: step 10261, loss 0.0140144, acc 1
2016-09-06T11:18:09.969917: step 10262, loss 0.0238287, acc 0.98
2016-09-06T11:18:10.767765: step 10263, loss 0.00469417, acc 1
2016-09-06T11:18:11.589041: step 10264, loss 0.00379584, acc 1
2016-09-06T11:18:12.458228: step 10265, loss 0.00390462, acc 1
2016-09-06T11:18:13.262630: step 10266, loss 0.0224661, acc 0.98
2016-09-06T11:18:14.042575: step 10267, loss 0.0471068, acc 0.96
2016-09-06T11:18:14.851485: step 10268, loss 0.00335726, acc 1
2016-09-06T11:18:15.661685: step 10269, loss 0.0197436, acc 1
2016-09-06T11:18:16.502886: step 10270, loss 0.00402268, acc 1
2016-09-06T11:18:17.317938: step 10271, loss 0.0170314, acc 1
2016-09-06T11:18:18.139492: step 10272, loss 0.00321767, acc 1
2016-09-06T11:18:18.918467: step 10273, loss 0.00356375, acc 1
2016-09-06T11:18:19.745480: step 10274, loss 0.0391795, acc 0.98
2016-09-06T11:18:20.563533: step 10275, loss 0.0167232, acc 1
2016-09-06T11:18:21.340230: step 10276, loss 0.00939932, acc 1
2016-09-06T11:18:22.150846: step 10277, loss 0.004262, acc 1
2016-09-06T11:18:22.981255: step 10278, loss 0.00469749, acc 1
2016-09-06T11:18:23.750391: step 10279, loss 0.0262969, acc 0.98
2016-09-06T11:18:24.546450: step 10280, loss 0.0030581, acc 1
2016-09-06T11:18:25.366281: step 10281, loss 0.0161665, acc 1
2016-09-06T11:18:26.151057: step 10282, loss 0.0189077, acc 0.98
2016-09-06T11:18:26.983737: step 10283, loss 0.0461059, acc 0.98
2016-09-06T11:18:27.789556: step 10284, loss 0.0911836, acc 0.96
2016-09-06T11:18:28.591515: step 10285, loss 0.00503364, acc 1
2016-09-06T11:18:29.385249: step 10286, loss 0.0664074, acc 0.96
2016-09-06T11:18:30.192395: step 10287, loss 0.0028152, acc 1
2016-09-06T11:18:30.977618: step 10288, loss 0.0466449, acc 0.98
2016-09-06T11:18:31.786981: step 10289, loss 0.0041383, acc 1
2016-09-06T11:18:32.592638: step 10290, loss 0.0493254, acc 0.98
2016-09-06T11:18:33.385283: step 10291, loss 0.03082, acc 0.98
2016-09-06T11:18:34.192749: step 10292, loss 0.00622501, acc 1
2016-09-06T11:18:35.027036: step 10293, loss 0.0143937, acc 1
2016-09-06T11:18:35.816940: step 10294, loss 0.00265758, acc 1
2016-09-06T11:18:36.603140: step 10295, loss 0.0141294, acc 1
2016-09-06T11:18:37.416677: step 10296, loss 0.00640079, acc 1
2016-09-06T11:18:38.229958: step 10297, loss 0.00752457, acc 1
2016-09-06T11:18:39.043345: step 10298, loss 0.00262615, acc 1
2016-09-06T11:18:39.850465: step 10299, loss 0.00259262, acc 1
2016-09-06T11:18:40.631045: step 10300, loss 0.0297604, acc 0.98

Evaluation:
2016-09-06T11:18:44.364459: step 10300, loss 2.50101, acc 0.752345

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-10300

2016-09-06T11:18:46.304129: step 10301, loss 0.0535109, acc 0.98
2016-09-06T11:18:47.169411: step 10302, loss 0.0048751, acc 1
2016-09-06T11:18:47.963624: step 10303, loss 0.0110952, acc 1
2016-09-06T11:18:48.766079: step 10304, loss 0.190827, acc 0.98
2016-09-06T11:18:49.591218: step 10305, loss 0.00669651, acc 1
2016-09-06T11:18:50.357288: step 10306, loss 0.010834, acc 1
2016-09-06T11:18:51.167551: step 10307, loss 0.0297426, acc 0.98
2016-09-06T11:18:51.997704: step 10308, loss 0.052135, acc 0.98
2016-09-06T11:18:52.779173: step 10309, loss 0.015975, acc 1
2016-09-06T11:18:53.599681: step 10310, loss 0.0635603, acc 0.96
2016-09-06T11:18:54.431017: step 10311, loss 0.0128543, acc 1
2016-09-06T11:18:55.273193: step 10312, loss 0.0259973, acc 1
2016-09-06T11:18:56.073497: step 10313, loss 0.0209209, acc 0.98
2016-09-06T11:18:56.887523: step 10314, loss 0.0302244, acc 1
2016-09-06T11:18:57.715454: step 10315, loss 0.00222752, acc 1
2016-09-06T11:18:58.532346: step 10316, loss 0.00964911, acc 1
2016-09-06T11:18:59.372514: step 10317, loss 0.0234668, acc 1
2016-09-06T11:19:00.181312: step 10318, loss 0.0031129, acc 1
2016-09-06T11:19:00.997592: step 10319, loss 0.00905757, acc 1
2016-09-06T11:19:01.835745: step 10320, loss 0.0192138, acc 1
2016-09-06T11:19:02.653216: step 10321, loss 0.00572822, acc 1
2016-09-06T11:19:03.504260: step 10322, loss 0.0163937, acc 1
2016-09-06T11:19:04.343524: step 10323, loss 0.00261077, acc 1
2016-09-06T11:19:05.167595: step 10324, loss 0.0138561, acc 1
2016-09-06T11:19:05.980238: step 10325, loss 0.0407446, acc 0.98
2016-09-06T11:19:06.817518: step 10326, loss 0.0175876, acc 1
2016-09-06T11:19:07.645316: step 10327, loss 0.0024916, acc 1
2016-09-06T11:19:08.447163: step 10328, loss 0.00342231, acc 1
2016-09-06T11:19:09.275807: step 10329, loss 0.00270226, acc 1
2016-09-06T11:19:10.081370: step 10330, loss 0.0270847, acc 0.98
2016-09-06T11:19:10.865939: step 10331, loss 0.0447645, acc 0.96
2016-09-06T11:19:11.637288: step 10332, loss 0.0102314, acc 1
2016-09-06T11:19:12.462825: step 10333, loss 0.00306941, acc 1
2016-09-06T11:19:13.257450: step 10334, loss 0.00361518, acc 1
2016-09-06T11:19:14.087804: step 10335, loss 0.00297794, acc 1
2016-09-06T11:19:14.888805: step 10336, loss 0.0498583, acc 0.98
2016-09-06T11:19:15.715136: step 10337, loss 0.00830897, acc 1
2016-09-06T11:19:16.595267: step 10338, loss 0.0219928, acc 0.98
2016-09-06T11:19:17.417575: step 10339, loss 0.0276374, acc 1
2016-09-06T11:19:18.228687: step 10340, loss 0.0480108, acc 0.98
2016-09-06T11:19:19.033690: step 10341, loss 0.00268586, acc 1
2016-09-06T11:19:19.835788: step 10342, loss 0.0304976, acc 0.98
2016-09-06T11:19:20.624131: step 10343, loss 0.0454501, acc 0.98
2016-09-06T11:19:21.430269: step 10344, loss 0.00485763, acc 1
2016-09-06T11:19:22.254593: step 10345, loss 0.0267855, acc 0.98
2016-09-06T11:19:23.039731: step 10346, loss 0.0459769, acc 0.98
2016-09-06T11:19:23.837452: step 10347, loss 0.0661289, acc 0.96
2016-09-06T11:19:24.676005: step 10348, loss 0.0512942, acc 0.98
2016-09-06T11:19:25.488257: step 10349, loss 0.00370463, acc 1
2016-09-06T11:19:26.274504: step 10350, loss 0.00930472, acc 1
2016-09-06T11:19:27.083794: step 10351, loss 0.0283123, acc 1
2016-09-06T11:19:27.862005: step 10352, loss 0.0102408, acc 1
2016-09-06T11:19:28.630372: step 10353, loss 0.0206725, acc 0.98
2016-09-06T11:19:29.458249: step 10354, loss 0.0333739, acc 0.98
2016-09-06T11:19:30.254962: step 10355, loss 0.0182401, acc 0.98
2016-09-06T11:19:31.068887: step 10356, loss 0.0203953, acc 1
2016-09-06T11:19:31.868149: step 10357, loss 0.0224993, acc 0.98
2016-09-06T11:19:32.667949: step 10358, loss 0.0330809, acc 1
2016-09-06T11:19:33.457306: step 10359, loss 0.00245784, acc 1
2016-09-06T11:19:34.277442: step 10360, loss 0.00494548, acc 1
2016-09-06T11:19:35.077612: step 10361, loss 0.0130498, acc 1
2016-09-06T11:19:35.882315: step 10362, loss 0.0154844, acc 1
2016-09-06T11:19:36.727117: step 10363, loss 0.0306977, acc 0.98
2016-09-06T11:19:37.508220: step 10364, loss 0.00310096, acc 1
2016-09-06T11:19:38.314617: step 10365, loss 0.00848525, acc 1
2016-09-06T11:19:39.167924: step 10366, loss 0.0074489, acc 1
2016-09-06T11:19:39.968381: step 10367, loss 0.00462886, acc 1
2016-09-06T11:19:40.707907: step 10368, loss 0.039506, acc 0.977273
2016-09-06T11:19:41.505784: step 10369, loss 0.0436774, acc 0.96
2016-09-06T11:19:42.307509: step 10370, loss 0.0414566, acc 0.98
2016-09-06T11:19:43.123124: step 10371, loss 0.0149699, acc 1
2016-09-06T11:19:43.940894: step 10372, loss 0.0145369, acc 1
2016-09-06T11:19:44.757822: step 10373, loss 0.0231955, acc 0.98
2016-09-06T11:19:45.600297: step 10374, loss 0.0192728, acc 1
2016-09-06T11:19:46.425816: step 10375, loss 0.0166792, acc 1
2016-09-06T11:19:47.237719: step 10376, loss 0.0167889, acc 1
2016-09-06T11:19:48.035700: step 10377, loss 0.0351328, acc 1
2016-09-06T11:19:48.837558: step 10378, loss 0.00659618, acc 1
2016-09-06T11:19:49.654015: step 10379, loss 0.00247462, acc 1
2016-09-06T11:19:50.477443: step 10380, loss 0.0028775, acc 1
2016-09-06T11:19:51.304704: step 10381, loss 0.0791285, acc 0.94
2016-09-06T11:19:52.104768: step 10382, loss 0.0379756, acc 0.98
2016-09-06T11:19:52.929443: step 10383, loss 0.0119512, acc 1
2016-09-06T11:19:53.778654: step 10384, loss 0.00255832, acc 1
2016-09-06T11:19:54.583005: step 10385, loss 0.00351405, acc 1
2016-09-06T11:19:55.403570: step 10386, loss 0.00364421, acc 1
2016-09-06T11:19:56.221919: step 10387, loss 0.00307611, acc 1
2016-09-06T11:19:57.034706: step 10388, loss 0.00250846, acc 1
2016-09-06T11:19:57.834538: step 10389, loss 0.00270494, acc 1
2016-09-06T11:19:58.639096: step 10390, loss 0.00370336, acc 1
2016-09-06T11:19:59.437911: step 10391, loss 0.00317644, acc 1
2016-09-06T11:20:00.244952: step 10392, loss 0.0117697, acc 1
2016-09-06T11:20:01.078113: step 10393, loss 0.00259624, acc 1
2016-09-06T11:20:01.909440: step 10394, loss 0.00299659, acc 1
2016-09-06T11:20:02.701791: step 10395, loss 0.0337971, acc 0.98
2016-09-06T11:20:03.520417: step 10396, loss 0.00332353, acc 1
2016-09-06T11:20:04.342607: step 10397, loss 0.0136788, acc 1
2016-09-06T11:20:05.165590: step 10398, loss 0.0165112, acc 1
2016-09-06T11:20:06.000069: step 10399, loss 0.0350756, acc 0.98
2016-09-06T11:20:06.790260: step 10400, loss 0.0239523, acc 1

Evaluation:
2016-09-06T11:20:10.533195: step 10400, loss 2.61206, acc 0.749531

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-10400

2016-09-06T11:20:12.343631: step 10401, loss 0.0203513, acc 0.98
2016-09-06T11:20:13.237161: step 10402, loss 0.0335772, acc 0.96
2016-09-06T11:20:14.060217: step 10403, loss 0.00250327, acc 1
2016-09-06T11:20:14.865997: step 10404, loss 0.00433571, acc 1
2016-09-06T11:20:15.699943: step 10405, loss 0.0217647, acc 0.98
2016-09-06T11:20:16.516554: step 10406, loss 0.0232936, acc 0.98
2016-09-06T11:20:17.341982: step 10407, loss 0.072733, acc 0.98
2016-09-06T11:20:18.182324: step 10408, loss 0.105023, acc 0.98
2016-09-06T11:20:19.005805: step 10409, loss 0.0358129, acc 0.98
2016-09-06T11:20:19.796847: step 10410, loss 0.00322491, acc 1
2016-09-06T11:20:20.621367: step 10411, loss 0.00498574, acc 1
2016-09-06T11:20:21.440748: step 10412, loss 0.00792452, acc 1
2016-09-06T11:20:22.252864: step 10413, loss 0.00225964, acc 1
2016-09-06T11:20:23.071816: step 10414, loss 0.00287866, acc 1
2016-09-06T11:20:23.906960: step 10415, loss 0.0177096, acc 1
2016-09-06T11:20:24.704906: step 10416, loss 0.00279724, acc 1
2016-09-06T11:20:25.494245: step 10417, loss 0.132685, acc 0.96
2016-09-06T11:20:26.318422: step 10418, loss 0.0105185, acc 1
2016-09-06T11:20:27.103449: step 10419, loss 0.00577558, acc 1
2016-09-06T11:20:27.892535: step 10420, loss 0.025314, acc 1
2016-09-06T11:20:28.709612: step 10421, loss 0.152404, acc 0.96
2016-09-06T11:20:29.487576: step 10422, loss 0.0207204, acc 1
2016-09-06T11:20:30.300748: step 10423, loss 0.0176006, acc 1
2016-09-06T11:20:31.145204: step 10424, loss 0.00888527, acc 1
2016-09-06T11:20:31.922383: step 10425, loss 0.00584451, acc 1
2016-09-06T11:20:32.734323: step 10426, loss 0.0871709, acc 0.98
2016-09-06T11:20:33.565835: step 10427, loss 0.00732932, acc 1
2016-09-06T11:20:34.349111: step 10428, loss 0.037026, acc 0.98
2016-09-06T11:20:35.180484: step 10429, loss 0.0232441, acc 0.98
2016-09-06T11:20:35.996029: step 10430, loss 0.00849412, acc 1
2016-09-06T11:20:36.785341: step 10431, loss 0.00858736, acc 1
2016-09-06T11:20:37.556356: step 10432, loss 0.0341052, acc 1
2016-09-06T11:20:38.367060: step 10433, loss 0.0095816, acc 1
2016-09-06T11:20:39.154868: step 10434, loss 0.0391209, acc 0.98
2016-09-06T11:20:39.995949: step 10435, loss 0.0138742, acc 1
2016-09-06T11:20:40.813595: step 10436, loss 0.060769, acc 0.98
2016-09-06T11:20:41.661951: step 10437, loss 0.0192112, acc 1
2016-09-06T11:20:42.509423: step 10438, loss 0.0546779, acc 0.96
2016-09-06T11:20:43.345629: step 10439, loss 0.00315353, acc 1
2016-09-06T11:20:44.160077: step 10440, loss 0.0126558, acc 1
2016-09-06T11:20:44.967122: step 10441, loss 0.0122161, acc 1
2016-09-06T11:20:45.795695: step 10442, loss 0.00394977, acc 1
2016-09-06T11:20:46.605944: step 10443, loss 0.0793445, acc 0.98
2016-09-06T11:20:47.421321: step 10444, loss 0.0258923, acc 0.98
2016-09-06T11:20:48.284286: step 10445, loss 0.0187711, acc 0.98
2016-09-06T11:20:49.126031: step 10446, loss 0.00317989, acc 1
2016-09-06T11:20:49.956121: step 10447, loss 0.0110095, acc 1
2016-09-06T11:20:50.778504: step 10448, loss 0.0172301, acc 1
2016-09-06T11:20:51.600112: step 10449, loss 0.00317006, acc 1
2016-09-06T11:20:52.441832: step 10450, loss 0.00553459, acc 1
2016-09-06T11:20:53.268212: step 10451, loss 0.0338023, acc 1
2016-09-06T11:20:54.119235: step 10452, loss 0.0593147, acc 0.96
2016-09-06T11:20:54.905392: step 10453, loss 0.0190385, acc 1
2016-09-06T11:20:55.722987: step 10454, loss 0.0195441, acc 0.98
2016-09-06T11:20:56.543248: step 10455, loss 0.00312557, acc 1
2016-09-06T11:20:57.377271: step 10456, loss 0.0369196, acc 0.98
2016-09-06T11:20:58.190316: step 10457, loss 0.00740629, acc 1
2016-09-06T11:20:59.002885: step 10458, loss 0.0136092, acc 1
2016-09-06T11:20:59.824193: step 10459, loss 0.0151616, acc 1
2016-09-06T11:21:00.668666: step 10460, loss 0.00337799, acc 1
2016-09-06T11:21:01.496809: step 10461, loss 0.00895029, acc 1
2016-09-06T11:21:02.347405: step 10462, loss 0.00398206, acc 1
2016-09-06T11:21:03.177124: step 10463, loss 0.0168877, acc 1
2016-09-06T11:21:04.132016: step 10464, loss 0.00640611, acc 1
2016-09-06T11:21:04.978070: step 10465, loss 0.015268, acc 1
2016-09-06T11:21:05.809670: step 10466, loss 0.0484533, acc 0.96
2016-09-06T11:21:06.623035: step 10467, loss 0.0240788, acc 1
2016-09-06T11:21:07.456360: step 10468, loss 0.0415458, acc 0.98
2016-09-06T11:21:08.271375: step 10469, loss 0.0183529, acc 0.98
2016-09-06T11:21:09.064783: step 10470, loss 0.0100037, acc 1
2016-09-06T11:21:09.917462: step 10471, loss 0.00639544, acc 1
2016-09-06T11:21:10.745694: step 10472, loss 0.00768513, acc 1
2016-09-06T11:21:11.589119: step 10473, loss 0.0324754, acc 0.98
2016-09-06T11:21:12.452427: step 10474, loss 0.00371434, acc 1
2016-09-06T11:21:13.259363: step 10475, loss 0.0295036, acc 0.98
2016-09-06T11:21:14.071235: step 10476, loss 0.0126109, acc 1
2016-09-06T11:21:15.108647: step 10477, loss 0.0161666, acc 1
2016-09-06T11:21:16.009950: step 10478, loss 0.0466772, acc 0.96
2016-09-06T11:21:16.891657: step 10479, loss 0.00352714, acc 1
2016-09-06T11:21:17.681682: step 10480, loss 0.00788191, acc 1
2016-09-06T11:21:18.529266: step 10481, loss 0.00339532, acc 1
2016-09-06T11:21:19.335664: step 10482, loss 0.00602987, acc 1
2016-09-06T11:21:20.145469: step 10483, loss 0.0740928, acc 0.96
2016-09-06T11:21:20.985215: step 10484, loss 0.0179639, acc 0.98
2016-09-06T11:21:21.813441: step 10485, loss 0.026669, acc 0.98
2016-09-06T11:21:22.669542: step 10486, loss 0.00324685, acc 1
2016-09-06T11:21:23.477437: step 10487, loss 0.024509, acc 1
2016-09-06T11:21:24.301280: step 10488, loss 0.050111, acc 0.96
2016-09-06T11:21:25.087217: step 10489, loss 0.00983162, acc 1
2016-09-06T11:21:25.885500: step 10490, loss 0.00315815, acc 1
2016-09-06T11:21:26.702246: step 10491, loss 0.00577958, acc 1
2016-09-06T11:21:27.516120: step 10492, loss 0.00323157, acc 1
2016-09-06T11:21:28.342812: step 10493, loss 0.0501397, acc 0.98
2016-09-06T11:21:29.150748: step 10494, loss 0.0200737, acc 1
2016-09-06T11:21:29.916860: step 10495, loss 0.0336655, acc 0.96
2016-09-06T11:21:30.723849: step 10496, loss 0.0697937, acc 0.96
2016-09-06T11:21:31.555685: step 10497, loss 0.0139644, acc 1
2016-09-06T11:21:32.352830: step 10498, loss 0.0172641, acc 1
2016-09-06T11:21:33.160012: step 10499, loss 0.0354829, acc 0.98
2016-09-06T11:21:33.972812: step 10500, loss 0.01723, acc 1

Evaluation:
2016-09-06T11:21:37.698045: step 10500, loss 2.84933, acc 0.754221

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-10500

2016-09-06T11:21:39.502941: step 10501, loss 0.00707116, acc 1
2016-09-06T11:21:40.338745: step 10502, loss 0.0348573, acc 1
2016-09-06T11:21:41.143338: step 10503, loss 0.0486175, acc 0.98
2016-09-06T11:21:41.968156: step 10504, loss 0.0167831, acc 0.98
2016-09-06T11:21:42.798974: step 10505, loss 0.0145932, acc 1
2016-09-06T11:21:43.597921: step 10506, loss 0.0246786, acc 0.98
2016-09-06T11:21:44.412209: step 10507, loss 0.0153973, acc 1
2016-09-06T11:21:45.235040: step 10508, loss 0.00429482, acc 1
2016-09-06T11:21:46.033860: step 10509, loss 0.0526132, acc 0.98
2016-09-06T11:21:46.839638: step 10510, loss 0.00273087, acc 1
2016-09-06T11:21:47.666711: step 10511, loss 0.00423344, acc 1
2016-09-06T11:21:48.464936: step 10512, loss 0.0130766, acc 1
2016-09-06T11:21:49.278918: step 10513, loss 0.033743, acc 0.98
2016-09-06T11:21:50.096730: step 10514, loss 0.104077, acc 0.98
2016-09-06T11:21:50.945194: step 10515, loss 0.0195108, acc 1
2016-09-06T11:21:51.786619: step 10516, loss 0.0189704, acc 0.98
2016-09-06T11:21:52.581553: step 10517, loss 0.0172044, acc 0.98
2016-09-06T11:21:53.401393: step 10518, loss 0.00369652, acc 1
2016-09-06T11:21:54.216289: step 10519, loss 0.00243951, acc 1
2016-09-06T11:21:55.003653: step 10520, loss 0.00255433, acc 1
2016-09-06T11:21:55.830215: step 10521, loss 0.0023067, acc 1
2016-09-06T11:21:56.622757: step 10522, loss 0.0278272, acc 0.98
2016-09-06T11:21:57.422112: step 10523, loss 0.0313522, acc 0.98
2016-09-06T11:21:58.249462: step 10524, loss 0.0546151, acc 0.98
2016-09-06T11:21:59.057102: step 10525, loss 0.00977498, acc 1
2016-09-06T11:21:59.883628: step 10526, loss 0.00897166, acc 1
2016-09-06T11:22:00.722196: step 10527, loss 0.00588251, acc 1
2016-09-06T11:22:01.489412: step 10528, loss 0.0476522, acc 0.98
2016-09-06T11:22:02.294909: step 10529, loss 0.00681857, acc 1
2016-09-06T11:22:03.121451: step 10530, loss 0.0152291, acc 1
2016-09-06T11:22:03.906316: step 10531, loss 0.00799211, acc 1
2016-09-06T11:22:04.703091: step 10532, loss 0.0226468, acc 0.98
2016-09-06T11:22:05.520445: step 10533, loss 0.00454236, acc 1
2016-09-06T11:22:06.322074: step 10534, loss 0.0818573, acc 0.98
2016-09-06T11:22:07.169370: step 10535, loss 0.00457505, acc 1
2016-09-06T11:22:08.018957: step 10536, loss 0.00729076, acc 1
2016-09-06T11:22:08.854127: step 10537, loss 0.0556023, acc 0.96
2016-09-06T11:22:09.660072: step 10538, loss 0.00208203, acc 1
2016-09-06T11:22:10.508195: step 10539, loss 0.0427872, acc 0.98
2016-09-06T11:22:11.394088: step 10540, loss 0.0219295, acc 1
2016-09-06T11:22:12.211817: step 10541, loss 0.0165178, acc 1
2016-09-06T11:22:13.028657: step 10542, loss 0.0261522, acc 1
2016-09-06T11:22:13.836462: step 10543, loss 0.0453038, acc 0.96
2016-09-06T11:22:14.625355: step 10544, loss 0.0208431, acc 0.98
2016-09-06T11:22:15.448796: step 10545, loss 0.0257957, acc 0.98
2016-09-06T11:22:16.260565: step 10546, loss 0.0412669, acc 1
2016-09-06T11:22:17.078115: step 10547, loss 0.0367358, acc 0.98
2016-09-06T11:22:17.892526: step 10548, loss 0.0171228, acc 1
2016-09-06T11:22:18.687557: step 10549, loss 0.00623324, acc 1
2016-09-06T11:22:19.504807: step 10550, loss 0.011735, acc 1
2016-09-06T11:22:20.340933: step 10551, loss 0.00373877, acc 1
2016-09-06T11:22:21.173089: step 10552, loss 0.00206195, acc 1
2016-09-06T11:22:21.975248: step 10553, loss 0.00216988, acc 1
2016-09-06T11:22:22.855684: step 10554, loss 0.0425879, acc 0.98
2016-09-06T11:22:23.689570: step 10555, loss 0.0174684, acc 1
2016-09-06T11:22:24.479468: step 10556, loss 0.0399448, acc 0.98
2016-09-06T11:22:25.278313: step 10557, loss 0.0106888, acc 1
2016-09-06T11:22:26.090269: step 10558, loss 0.0106316, acc 1
2016-09-06T11:22:26.853984: step 10559, loss 0.00459193, acc 1
2016-09-06T11:22:27.615059: step 10560, loss 0.00526068, acc 1
2016-09-06T11:22:28.433108: step 10561, loss 0.0147894, acc 1
2016-09-06T11:22:29.246011: step 10562, loss 0.0761594, acc 0.96
2016-09-06T11:22:30.061753: step 10563, loss 0.00692512, acc 1
2016-09-06T11:22:30.878303: step 10564, loss 0.0240224, acc 0.98
2016-09-06T11:22:31.684830: step 10565, loss 0.00279096, acc 1
2016-09-06T11:22:32.494742: step 10566, loss 0.0169775, acc 0.98
2016-09-06T11:22:33.341540: step 10567, loss 0.00546851, acc 1
2016-09-06T11:22:34.147600: step 10568, loss 0.026939, acc 0.98
2016-09-06T11:22:34.948365: step 10569, loss 0.0213267, acc 1
2016-09-06T11:22:35.776843: step 10570, loss 0.00554133, acc 1
2016-09-06T11:22:36.563933: step 10571, loss 0.0133768, acc 1
2016-09-06T11:22:37.380455: step 10572, loss 0.00270989, acc 1
2016-09-06T11:22:38.214260: step 10573, loss 0.00406557, acc 1
2016-09-06T11:22:39.022920: step 10574, loss 0.0245841, acc 1
2016-09-06T11:22:39.817241: step 10575, loss 0.0191214, acc 0.98
2016-09-06T11:22:40.632368: step 10576, loss 0.00400324, acc 1
2016-09-06T11:22:41.427915: step 10577, loss 0.00398487, acc 1
2016-09-06T11:22:42.233404: step 10578, loss 0.00269332, acc 1
2016-09-06T11:22:43.066756: step 10579, loss 0.0350704, acc 0.98
2016-09-06T11:22:43.855157: step 10580, loss 0.0024521, acc 1
2016-09-06T11:22:44.666851: step 10581, loss 0.0103988, acc 1
2016-09-06T11:22:45.488329: step 10582, loss 0.0195201, acc 0.98
2016-09-06T11:22:46.317440: step 10583, loss 0.0677307, acc 0.96
2016-09-06T11:22:47.117405: step 10584, loss 0.00750477, acc 1
2016-09-06T11:22:47.950739: step 10585, loss 0.0165463, acc 0.98
2016-09-06T11:22:48.782334: step 10586, loss 0.0108358, acc 1
2016-09-06T11:22:49.604681: step 10587, loss 0.00433805, acc 1
2016-09-06T11:22:50.435719: step 10588, loss 0.0151853, acc 1
2016-09-06T11:22:51.286377: step 10589, loss 0.00444963, acc 1
2016-09-06T11:22:52.097612: step 10590, loss 0.00635282, acc 1
2016-09-06T11:22:52.931204: step 10591, loss 0.00307369, acc 1
2016-09-06T11:22:53.749509: step 10592, loss 0.0387049, acc 0.98
2016-09-06T11:22:54.590388: step 10593, loss 0.00330587, acc 1
2016-09-06T11:22:55.416962: step 10594, loss 0.018426, acc 0.98
2016-09-06T11:22:56.242086: step 10595, loss 0.00310471, acc 1
2016-09-06T11:22:57.034937: step 10596, loss 0.00300427, acc 1
2016-09-06T11:22:57.826970: step 10597, loss 0.0261144, acc 0.98
2016-09-06T11:22:58.638573: step 10598, loss 0.0322876, acc 0.98
2016-09-06T11:22:59.445400: step 10599, loss 0.00395273, acc 1
2016-09-06T11:23:00.230383: step 10600, loss 0.00425717, acc 1

Evaluation:
2016-09-06T11:23:03.972880: step 10600, loss 2.70106, acc 0.736398

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-10600

2016-09-06T11:23:05.851116: step 10601, loss 0.00659295, acc 1
2016-09-06T11:23:06.670976: step 10602, loss 0.00302874, acc 1
2016-09-06T11:23:07.531995: step 10603, loss 0.00601759, acc 1
2016-09-06T11:23:08.354964: step 10604, loss 0.00296214, acc 1
2016-09-06T11:23:09.184752: step 10605, loss 0.00375092, acc 1
2016-09-06T11:23:10.036037: step 10606, loss 0.0491227, acc 0.98
2016-09-06T11:23:10.872645: step 10607, loss 0.0144849, acc 1
2016-09-06T11:23:11.665287: step 10608, loss 0.0125828, acc 1
2016-09-06T11:23:12.466101: step 10609, loss 0.05001, acc 0.98
2016-09-06T11:23:13.296605: step 10610, loss 0.0188428, acc 0.98
2016-09-06T11:23:14.088128: step 10611, loss 0.0143187, acc 1
2016-09-06T11:23:14.871537: step 10612, loss 0.0566023, acc 0.98
2016-09-06T11:23:15.684169: step 10613, loss 0.0160379, acc 1
2016-09-06T11:23:16.505764: step 10614, loss 0.0301328, acc 0.98
2016-09-06T11:23:17.327090: step 10615, loss 0.0156031, acc 1
2016-09-06T11:23:18.133145: step 10616, loss 0.00580454, acc 1
2016-09-06T11:23:18.930141: step 10617, loss 0.00450225, acc 1
2016-09-06T11:23:19.746175: step 10618, loss 0.0347487, acc 0.98
2016-09-06T11:23:20.562654: step 10619, loss 0.00954507, acc 1
2016-09-06T11:23:21.366073: step 10620, loss 0.00244849, acc 1
2016-09-06T11:23:22.175588: step 10621, loss 0.0908856, acc 0.98
2016-09-06T11:23:23.002776: step 10622, loss 0.0123976, acc 1
2016-09-06T11:23:23.812104: step 10623, loss 0.00240277, acc 1
2016-09-06T11:23:24.639625: step 10624, loss 0.0181487, acc 1
2016-09-06T11:23:25.449459: step 10625, loss 0.0311409, acc 0.98
2016-09-06T11:23:26.272003: step 10626, loss 0.0113667, acc 1
2016-09-06T11:23:27.082049: step 10627, loss 0.0182724, acc 0.98
2016-09-06T11:23:27.957807: step 10628, loss 0.0827419, acc 0.98
2016-09-06T11:23:28.743141: step 10629, loss 0.0301386, acc 1
2016-09-06T11:23:29.586646: step 10630, loss 0.010752, acc 1
2016-09-06T11:23:30.420435: step 10631, loss 0.0147703, acc 1
2016-09-06T11:23:31.269461: step 10632, loss 0.0146151, acc 1
2016-09-06T11:23:32.099151: step 10633, loss 0.0709103, acc 0.98
2016-09-06T11:23:32.932361: step 10634, loss 0.0152484, acc 1
2016-09-06T11:23:33.757128: step 10635, loss 0.023973, acc 1
2016-09-06T11:23:34.559401: step 10636, loss 0.0192516, acc 0.98
2016-09-06T11:23:35.399274: step 10637, loss 0.048856, acc 0.98
2016-09-06T11:23:36.196734: step 10638, loss 0.00349827, acc 1
2016-09-06T11:23:37.014813: step 10639, loss 0.0246216, acc 0.98
2016-09-06T11:23:37.836054: step 10640, loss 0.15933, acc 0.96
2016-09-06T11:23:38.657581: step 10641, loss 0.00324262, acc 1
2016-09-06T11:23:39.480483: step 10642, loss 0.0371204, acc 0.98
2016-09-06T11:23:40.264995: step 10643, loss 0.0291295, acc 1
2016-09-06T11:23:41.060238: step 10644, loss 0.019411, acc 0.98
2016-09-06T11:23:41.860449: step 10645, loss 0.0170188, acc 1
2016-09-06T11:23:42.673716: step 10646, loss 0.0147097, acc 1
2016-09-06T11:23:43.507146: step 10647, loss 0.00375587, acc 1
2016-09-06T11:23:44.313671: step 10648, loss 0.0175025, acc 1
2016-09-06T11:23:45.113799: step 10649, loss 0.0262128, acc 0.98
2016-09-06T11:23:45.927068: step 10650, loss 0.00646261, acc 1
2016-09-06T11:23:46.709432: step 10651, loss 0.044134, acc 0.96
2016-09-06T11:23:47.511656: step 10652, loss 0.00299436, acc 1
2016-09-06T11:23:48.313730: step 10653, loss 0.00298282, acc 1
2016-09-06T11:23:49.104319: step 10654, loss 0.0240712, acc 1
2016-09-06T11:23:49.922580: step 10655, loss 0.0122293, acc 1
2016-09-06T11:23:50.734921: step 10656, loss 0.0140775, acc 1
2016-09-06T11:23:51.520171: step 10657, loss 0.00511546, acc 1
2016-09-06T11:23:52.326362: step 10658, loss 0.0468216, acc 0.96
2016-09-06T11:23:53.128664: step 10659, loss 0.014735, acc 1
2016-09-06T11:23:53.962980: step 10660, loss 0.0105213, acc 1
2016-09-06T11:23:54.769586: step 10661, loss 0.0394556, acc 0.98
2016-09-06T11:23:55.570224: step 10662, loss 0.00759849, acc 1
2016-09-06T11:23:56.377495: step 10663, loss 0.00318839, acc 1
2016-09-06T11:23:57.213935: step 10664, loss 0.00320206, acc 1
2016-09-06T11:23:58.034368: step 10665, loss 0.0131847, acc 1
2016-09-06T11:23:58.826122: step 10666, loss 0.0880454, acc 0.98
2016-09-06T11:23:59.626341: step 10667, loss 0.0493845, acc 1
2016-09-06T11:24:00.483488: step 10668, loss 0.0436505, acc 0.98
2016-09-06T11:24:01.261939: step 10669, loss 0.0223806, acc 1
2016-09-06T11:24:02.086568: step 10670, loss 0.00400936, acc 1
2016-09-06T11:24:02.857898: step 10671, loss 0.00850897, acc 1
2016-09-06T11:24:03.661243: step 10672, loss 0.113354, acc 0.98
2016-09-06T11:24:04.473207: step 10673, loss 0.00407322, acc 1
2016-09-06T11:24:05.312487: step 10674, loss 0.0248072, acc 1
2016-09-06T11:24:06.084858: step 10675, loss 0.00854774, acc 1
2016-09-06T11:24:06.903154: step 10676, loss 0.0245984, acc 1
2016-09-06T11:24:07.754733: step 10677, loss 0.00851209, acc 1
2016-09-06T11:24:08.569618: step 10678, loss 0.0031795, acc 1
2016-09-06T11:24:09.397477: step 10679, loss 0.00397731, acc 1
2016-09-06T11:24:10.267980: step 10680, loss 0.0254337, acc 1
2016-09-06T11:24:11.118221: step 10681, loss 0.00323885, acc 1
2016-09-06T11:24:11.922518: step 10682, loss 0.00306558, acc 1
2016-09-06T11:24:12.778586: step 10683, loss 0.014485, acc 1
2016-09-06T11:24:13.612342: step 10684, loss 0.0220244, acc 1
2016-09-06T11:24:14.420929: step 10685, loss 0.0320364, acc 0.98
2016-09-06T11:24:15.243530: step 10686, loss 0.00427143, acc 1
2016-09-06T11:24:16.042762: step 10687, loss 0.0184354, acc 1
2016-09-06T11:24:16.865036: step 10688, loss 0.0360719, acc 0.98
2016-09-06T11:24:17.703749: step 10689, loss 0.00312494, acc 1
2016-09-06T11:24:18.508100: step 10690, loss 0.0386824, acc 0.98
2016-09-06T11:24:19.326107: step 10691, loss 0.0110528, acc 1
2016-09-06T11:24:20.165898: step 10692, loss 0.0034621, acc 1
2016-09-06T11:24:21.026874: step 10693, loss 0.0366397, acc 0.98
2016-09-06T11:24:21.817435: step 10694, loss 0.0323943, acc 0.96
2016-09-06T11:24:22.633850: step 10695, loss 0.0686799, acc 0.98
2016-09-06T11:24:23.463040: step 10696, loss 0.0355975, acc 0.98
2016-09-06T11:24:24.259674: step 10697, loss 0.0167973, acc 1
2016-09-06T11:24:25.067114: step 10698, loss 0.0707055, acc 0.96
2016-09-06T11:24:25.891346: step 10699, loss 0.00304543, acc 1
2016-09-06T11:24:26.708796: step 10700, loss 0.00277269, acc 1

Evaluation:
2016-09-06T11:24:30.460506: step 10700, loss 2.21812, acc 0.747655

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-10700

2016-09-06T11:24:32.340314: step 10701, loss 0.0341782, acc 0.98
2016-09-06T11:24:33.171851: step 10702, loss 0.00363577, acc 1
2016-09-06T11:24:33.975047: step 10703, loss 0.00520096, acc 1
2016-09-06T11:24:34.782866: step 10704, loss 0.0333608, acc 1
2016-09-06T11:24:35.583884: step 10705, loss 0.0133626, acc 1
2016-09-06T11:24:36.390478: step 10706, loss 0.0470314, acc 0.98
2016-09-06T11:24:37.224765: step 10707, loss 0.00751175, acc 1
2016-09-06T11:24:38.078279: step 10708, loss 0.0200447, acc 1
2016-09-06T11:24:38.854054: step 10709, loss 0.0128016, acc 1
2016-09-06T11:24:39.690071: step 10710, loss 0.0173638, acc 1
2016-09-06T11:24:40.521325: step 10711, loss 0.00327943, acc 1
2016-09-06T11:24:41.329916: step 10712, loss 0.00260572, acc 1
2016-09-06T11:24:42.140219: step 10713, loss 0.0834785, acc 0.98
2016-09-06T11:24:42.974894: step 10714, loss 0.010739, acc 1
2016-09-06T11:24:43.788433: step 10715, loss 0.0147614, acc 1
2016-09-06T11:24:44.601258: step 10716, loss 0.0227348, acc 0.98
2016-09-06T11:24:45.443935: step 10717, loss 0.016769, acc 0.98
2016-09-06T11:24:46.247282: step 10718, loss 0.0426521, acc 0.96
2016-09-06T11:24:47.081942: step 10719, loss 0.00422942, acc 1
2016-09-06T11:24:47.897258: step 10720, loss 0.0378466, acc 0.98
2016-09-06T11:24:48.695127: step 10721, loss 0.050076, acc 0.98
2016-09-06T11:24:49.507211: step 10722, loss 0.0106172, acc 1
2016-09-06T11:24:50.336786: step 10723, loss 0.0222609, acc 1
2016-09-06T11:24:51.176777: step 10724, loss 0.0587837, acc 0.98
2016-09-06T11:24:51.994917: step 10725, loss 0.00318944, acc 1
2016-09-06T11:24:52.799663: step 10726, loss 0.0163804, acc 1
2016-09-06T11:24:53.622401: step 10727, loss 0.00574544, acc 1
2016-09-06T11:24:54.427483: step 10728, loss 0.0231789, acc 0.98
2016-09-06T11:24:55.253166: step 10729, loss 0.0342381, acc 0.98
2016-09-06T11:24:56.061450: step 10730, loss 0.0247497, acc 0.98
2016-09-06T11:24:56.858446: step 10731, loss 0.0197026, acc 1
2016-09-06T11:24:57.725127: step 10732, loss 0.0100341, acc 1
2016-09-06T11:24:58.550409: step 10733, loss 0.0363094, acc 0.98
2016-09-06T11:24:59.392997: step 10734, loss 0.0131692, acc 1
2016-09-06T11:25:00.232652: step 10735, loss 0.036854, acc 0.98
2016-09-06T11:25:01.076860: step 10736, loss 0.00383869, acc 1
2016-09-06T11:25:01.893983: step 10737, loss 0.00902205, acc 1
2016-09-06T11:25:02.725500: step 10738, loss 0.0385689, acc 0.98
2016-09-06T11:25:03.619934: step 10739, loss 0.0283423, acc 0.98
2016-09-06T11:25:04.563784: step 10740, loss 0.0406684, acc 0.98
2016-09-06T11:25:05.381992: step 10741, loss 0.0305514, acc 0.98
2016-09-06T11:25:06.196085: step 10742, loss 0.00428885, acc 1
2016-09-06T11:25:06.998811: step 10743, loss 0.61548, acc 0.98
2016-09-06T11:25:07.799872: step 10744, loss 0.00935872, acc 1
2016-09-06T11:25:08.586932: step 10745, loss 0.00722506, acc 1
2016-09-06T11:25:09.384783: step 10746, loss 0.00520963, acc 1
2016-09-06T11:25:10.211902: step 10747, loss 0.00856688, acc 1
2016-09-06T11:25:11.030365: step 10748, loss 0.00443077, acc 1
2016-09-06T11:25:11.915745: step 10749, loss 0.00633248, acc 1
2016-09-06T11:25:12.733706: step 10750, loss 0.0708052, acc 0.96
2016-09-06T11:25:13.546136: step 10751, loss 0.0401028, acc 0.98
2016-09-06T11:25:14.356271: step 10752, loss 0.0150414, acc 1
2016-09-06T11:25:15.197676: step 10753, loss 0.0489289, acc 0.96
2016-09-06T11:25:16.005552: step 10754, loss 0.0273157, acc 1
2016-09-06T11:25:16.833455: step 10755, loss 0.0172873, acc 1
2016-09-06T11:25:17.637401: step 10756, loss 0.00695415, acc 1
2016-09-06T11:25:18.477552: step 10757, loss 0.111434, acc 0.96
2016-09-06T11:25:19.298151: step 10758, loss 0.0266097, acc 0.98
2016-09-06T11:25:20.127885: step 10759, loss 0.0186221, acc 1
2016-09-06T11:25:20.944561: step 10760, loss 0.0397875, acc 0.98
2016-09-06T11:25:21.794874: step 10761, loss 0.030364, acc 0.98
2016-09-06T11:25:22.625192: step 10762, loss 0.00821013, acc 1
2016-09-06T11:25:23.406184: step 10763, loss 0.00546556, acc 1
2016-09-06T11:25:24.230469: step 10764, loss 0.00344218, acc 1
2016-09-06T11:25:25.061317: step 10765, loss 0.0310035, acc 0.98
2016-09-06T11:25:25.870208: step 10766, loss 0.0034923, acc 1
2016-09-06T11:25:26.665945: step 10767, loss 0.0726641, acc 0.94
2016-09-06T11:25:27.484787: step 10768, loss 0.0351977, acc 0.98
2016-09-06T11:25:28.288661: step 10769, loss 0.00364007, acc 1
2016-09-06T11:25:29.108355: step 10770, loss 0.00371736, acc 1
2016-09-06T11:25:29.946709: step 10771, loss 0.022188, acc 0.98
2016-09-06T11:25:30.730040: step 10772, loss 0.0103232, acc 1
2016-09-06T11:25:31.564727: step 10773, loss 0.00372508, acc 1
2016-09-06T11:25:32.355968: step 10774, loss 0.00664576, acc 1
2016-09-06T11:25:33.137739: step 10775, loss 0.00516899, acc 1
2016-09-06T11:25:33.957404: step 10776, loss 0.137233, acc 0.98
2016-09-06T11:25:34.798953: step 10777, loss 0.00760399, acc 1
2016-09-06T11:25:35.587410: step 10778, loss 0.0346916, acc 0.98
2016-09-06T11:25:36.391156: step 10779, loss 0.00668254, acc 1
2016-09-06T11:25:37.215258: step 10780, loss 0.00400126, acc 1
2016-09-06T11:25:38.026183: step 10781, loss 0.028343, acc 1
2016-09-06T11:25:38.834556: step 10782, loss 0.0102945, acc 1
2016-09-06T11:25:39.641464: step 10783, loss 0.00325372, acc 1
2016-09-06T11:25:40.461565: step 10784, loss 0.012203, acc 1
2016-09-06T11:25:41.293505: step 10785, loss 0.0245337, acc 1
2016-09-06T11:25:42.137462: step 10786, loss 0.0216801, acc 1
2016-09-06T11:25:42.948594: step 10787, loss 0.00391585, acc 1
2016-09-06T11:25:43.752963: step 10788, loss 0.00294036, acc 1
2016-09-06T11:25:44.600170: step 10789, loss 0.061615, acc 0.98
2016-09-06T11:25:45.429642: step 10790, loss 0.0218962, acc 0.98
2016-09-06T11:25:46.250777: step 10791, loss 0.00674527, acc 1
2016-09-06T11:25:47.086490: step 10792, loss 0.015652, acc 1
2016-09-06T11:25:47.915882: step 10793, loss 0.0252667, acc 1
2016-09-06T11:25:48.738803: step 10794, loss 0.00437797, acc 1
2016-09-06T11:25:49.582644: step 10795, loss 0.0372603, acc 0.98
2016-09-06T11:25:50.427366: step 10796, loss 0.00804079, acc 1
2016-09-06T11:25:51.219341: step 10797, loss 0.0203037, acc 0.98
2016-09-06T11:25:52.024407: step 10798, loss 0.00321931, acc 1
2016-09-06T11:25:52.856040: step 10799, loss 0.0243656, acc 0.98
2016-09-06T11:25:53.654335: step 10800, loss 0.0161955, acc 1

Evaluation:
2016-09-06T11:25:57.396684: step 10800, loss 2.08823, acc 0.751407

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-10800

2016-09-06T11:25:59.283831: step 10801, loss 0.0266271, acc 1
2016-09-06T11:26:00.123369: step 10802, loss 0.00575583, acc 1
2016-09-06T11:26:00.977733: step 10803, loss 0.0207961, acc 0.98
2016-09-06T11:26:01.785582: step 10804, loss 0.0376378, acc 0.98
2016-09-06T11:26:02.583571: step 10805, loss 0.0195816, acc 0.98
2016-09-06T11:26:03.379106: step 10806, loss 0.0220884, acc 0.98
2016-09-06T11:26:04.178168: step 10807, loss 0.0335839, acc 0.98
2016-09-06T11:26:04.997452: step 10808, loss 0.00285024, acc 1
2016-09-06T11:26:05.793914: step 10809, loss 0.005876, acc 1
2016-09-06T11:26:06.590502: step 10810, loss 0.0453866, acc 0.96
2016-09-06T11:26:07.392573: step 10811, loss 0.0181606, acc 0.98
2016-09-06T11:26:08.192057: step 10812, loss 0.00284708, acc 1
2016-09-06T11:26:08.998436: step 10813, loss 0.183106, acc 0.94
2016-09-06T11:26:09.828255: step 10814, loss 0.00439096, acc 1
2016-09-06T11:26:10.664508: step 10815, loss 0.00249516, acc 1
2016-09-06T11:26:11.473827: step 10816, loss 0.0319642, acc 0.98
2016-09-06T11:26:12.296473: step 10817, loss 0.00481859, acc 1
2016-09-06T11:26:13.118335: step 10818, loss 0.0104085, acc 1
2016-09-06T11:26:13.908375: step 10819, loss 0.0104284, acc 1
2016-09-06T11:26:14.741777: step 10820, loss 0.0347932, acc 1
2016-09-06T11:26:15.593646: step 10821, loss 0.144417, acc 0.96
2016-09-06T11:26:16.423255: step 10822, loss 0.00376908, acc 1
2016-09-06T11:26:17.283094: step 10823, loss 0.00273234, acc 1
2016-09-06T11:26:18.102604: step 10824, loss 0.0243234, acc 0.98
2016-09-06T11:26:18.902492: step 10825, loss 0.0420859, acc 0.98
2016-09-06T11:26:19.743398: step 10826, loss 0.00597178, acc 1
2016-09-06T11:26:20.545977: step 10827, loss 0.0345065, acc 0.98
2016-09-06T11:26:21.361020: step 10828, loss 0.0176546, acc 1
2016-09-06T11:26:22.183306: step 10829, loss 0.0464831, acc 0.96
2016-09-06T11:26:23.003165: step 10830, loss 0.0344995, acc 1
2016-09-06T11:26:23.838592: step 10831, loss 0.0193473, acc 0.98
2016-09-06T11:26:24.672644: step 10832, loss 0.0141502, acc 1
2016-09-06T11:26:25.493448: step 10833, loss 0.0140899, acc 1
2016-09-06T11:26:26.301368: step 10834, loss 0.00561911, acc 1
2016-09-06T11:26:27.138277: step 10835, loss 0.108053, acc 0.96
2016-09-06T11:26:27.947682: step 10836, loss 0.00453197, acc 1
2016-09-06T11:26:28.772485: step 10837, loss 0.013641, acc 1
2016-09-06T11:26:29.554006: step 10838, loss 0.0285009, acc 1
2016-09-06T11:26:30.373402: step 10839, loss 0.010561, acc 1
2016-09-06T11:26:31.158510: step 10840, loss 0.0250546, acc 0.98
2016-09-06T11:26:31.994367: step 10841, loss 0.0317805, acc 0.98
2016-09-06T11:26:32.814950: step 10842, loss 0.0585891, acc 0.98
2016-09-06T11:26:33.600956: step 10843, loss 0.008142, acc 1
2016-09-06T11:26:34.413275: step 10844, loss 0.0153971, acc 1
2016-09-06T11:26:35.226775: step 10845, loss 0.00425676, acc 1
2016-09-06T11:26:36.005398: step 10846, loss 0.0133807, acc 1
2016-09-06T11:26:36.809430: step 10847, loss 0.0168598, acc 0.98
2016-09-06T11:26:37.625458: step 10848, loss 0.00363261, acc 1
2016-09-06T11:26:38.392079: step 10849, loss 0.00795711, acc 1
2016-09-06T11:26:39.201397: step 10850, loss 0.0158473, acc 1
2016-09-06T11:26:40.050742: step 10851, loss 0.0057786, acc 1
2016-09-06T11:26:40.819528: step 10852, loss 0.00249585, acc 1
2016-09-06T11:26:41.636888: step 10853, loss 0.0789418, acc 0.98
2016-09-06T11:26:42.461039: step 10854, loss 0.00521637, acc 1
2016-09-06T11:26:43.260936: step 10855, loss 0.0581943, acc 0.98
2016-09-06T11:26:44.080630: step 10856, loss 0.0437638, acc 0.98
2016-09-06T11:26:44.912579: step 10857, loss 0.0140013, acc 1
2016-09-06T11:26:45.697635: step 10858, loss 0.00459444, acc 1
2016-09-06T11:26:46.498006: step 10859, loss 0.0298378, acc 1
2016-09-06T11:26:47.309275: step 10860, loss 0.0176135, acc 0.98
2016-09-06T11:26:48.085409: step 10861, loss 0.0275446, acc 0.98
2016-09-06T11:26:48.895593: step 10862, loss 0.00746173, acc 1
2016-09-06T11:26:49.709639: step 10863, loss 0.010673, acc 1
2016-09-06T11:26:50.483615: step 10864, loss 0.0664532, acc 0.94
2016-09-06T11:26:51.265012: step 10865, loss 0.00307596, acc 1
2016-09-06T11:26:52.096033: step 10866, loss 0.0230253, acc 1
2016-09-06T11:26:52.886292: step 10867, loss 0.0505092, acc 1
2016-09-06T11:26:53.682705: step 10868, loss 0.0255266, acc 0.98
2016-09-06T11:26:54.501861: step 10869, loss 0.0204268, acc 1
2016-09-06T11:26:55.289454: step 10870, loss 0.0173397, acc 1
2016-09-06T11:26:56.080160: step 10871, loss 0.0224911, acc 0.98
2016-09-06T11:26:56.881147: step 10872, loss 0.0130505, acc 1
2016-09-06T11:26:57.673434: step 10873, loss 0.00705288, acc 1
2016-09-06T11:26:58.487612: step 10874, loss 0.0280072, acc 0.98
2016-09-06T11:26:59.282627: step 10875, loss 0.00469258, acc 1
2016-09-06T11:27:00.077786: step 10876, loss 0.0128729, acc 1
2016-09-06T11:27:00.920793: step 10877, loss 0.0133114, acc 1
2016-09-06T11:27:01.734661: step 10878, loss 0.00264483, acc 1
2016-09-06T11:27:02.530591: step 10879, loss 0.0138782, acc 1
2016-09-06T11:27:03.353198: step 10880, loss 0.0393181, acc 0.98
2016-09-06T11:27:04.193449: step 10881, loss 0.0102534, acc 1
2016-09-06T11:27:04.978790: step 10882, loss 0.0282569, acc 0.98
2016-09-06T11:27:05.792347: step 10883, loss 0.018788, acc 1
2016-09-06T11:27:06.592878: step 10884, loss 0.0841558, acc 0.98
2016-09-06T11:27:07.401052: step 10885, loss 0.0123311, acc 1
2016-09-06T11:27:08.230249: step 10886, loss 0.00716899, acc 1
2016-09-06T11:27:09.042486: step 10887, loss 0.0722937, acc 0.98
2016-09-06T11:27:09.826873: step 10888, loss 0.160057, acc 0.98
2016-09-06T11:27:10.637413: step 10889, loss 0.0228, acc 1
2016-09-06T11:27:11.452166: step 10890, loss 0.0114536, acc 1
2016-09-06T11:27:12.239871: step 10891, loss 0.00669596, acc 1
2016-09-06T11:27:13.041953: step 10892, loss 0.0134917, acc 1
2016-09-06T11:27:13.864630: step 10893, loss 0.0317751, acc 1
2016-09-06T11:27:14.668516: step 10894, loss 0.0139674, acc 1
2016-09-06T11:27:15.479503: step 10895, loss 0.00238724, acc 1
2016-09-06T11:27:16.312666: step 10896, loss 0.0178053, acc 1
2016-09-06T11:27:17.121017: step 10897, loss 0.00374953, acc 1
2016-09-06T11:27:17.931140: step 10898, loss 0.0417988, acc 0.98
2016-09-06T11:27:18.760775: step 10899, loss 0.0252772, acc 1
2016-09-06T11:27:19.558522: step 10900, loss 0.103308, acc 0.98

Evaluation:
2016-09-06T11:27:23.304397: step 10900, loss 1.79922, acc 0.749531

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-10900

2016-09-06T11:27:25.311297: step 10901, loss 0.00574627, acc 1
2016-09-06T11:27:26.122347: step 10902, loss 0.022815, acc 1
2016-09-06T11:27:26.923970: step 10903, loss 0.00491209, acc 1
2016-09-06T11:27:27.756932: step 10904, loss 0.00404371, acc 1
2016-09-06T11:27:28.585399: step 10905, loss 0.0189367, acc 1
2016-09-06T11:27:29.387071: step 10906, loss 0.00267069, acc 1
2016-09-06T11:27:30.180924: step 10907, loss 0.00409525, acc 1
2016-09-06T11:27:31.034240: step 10908, loss 0.0267031, acc 1
2016-09-06T11:27:31.844891: step 10909, loss 0.00369026, acc 1
2016-09-06T11:27:32.662348: step 10910, loss 0.0108422, acc 1
2016-09-06T11:27:33.503882: step 10911, loss 0.00556896, acc 1
2016-09-06T11:27:34.333451: step 10912, loss 0.0422442, acc 1
2016-09-06T11:27:35.162936: step 10913, loss 0.0665459, acc 0.96
2016-09-06T11:27:35.989955: step 10914, loss 0.00733956, acc 1
2016-09-06T11:27:36.815551: step 10915, loss 0.0248915, acc 1
2016-09-06T11:27:37.609635: step 10916, loss 0.0102283, acc 1
2016-09-06T11:27:38.400216: step 10917, loss 0.0089784, acc 1
2016-09-06T11:27:39.211933: step 10918, loss 0.0304857, acc 1
2016-09-06T11:27:39.984692: step 10919, loss 0.00375058, acc 1
2016-09-06T11:27:40.817615: step 10920, loss 0.0332501, acc 0.98
2016-09-06T11:27:41.637781: step 10921, loss 0.0147686, acc 1
2016-09-06T11:27:42.436189: step 10922, loss 0.00385087, acc 1
2016-09-06T11:27:43.268515: step 10923, loss 0.0641831, acc 0.96
2016-09-06T11:27:44.090666: step 10924, loss 0.00358866, acc 1
2016-09-06T11:27:44.874667: step 10925, loss 0.00615308, acc 1
2016-09-06T11:27:45.665258: step 10926, loss 0.0435462, acc 0.98
2016-09-06T11:27:46.489953: step 10927, loss 0.0149541, acc 1
2016-09-06T11:27:47.297662: step 10928, loss 0.00300464, acc 1
2016-09-06T11:27:48.126541: step 10929, loss 0.0713208, acc 0.98
2016-09-06T11:27:48.926274: step 10930, loss 0.0507456, acc 0.96
2016-09-06T11:27:49.713709: step 10931, loss 0.0351554, acc 0.98
2016-09-06T11:27:50.541314: step 10932, loss 0.029676, acc 1
2016-09-06T11:27:51.356583: step 10933, loss 0.0267294, acc 0.98
2016-09-06T11:27:52.157165: step 10934, loss 0.00909967, acc 1
2016-09-06T11:27:52.961028: step 10935, loss 0.00243856, acc 1
2016-09-06T11:27:53.768655: step 10936, loss 0.00376366, acc 1
2016-09-06T11:27:54.531998: step 10937, loss 0.0193681, acc 1
2016-09-06T11:27:55.346220: step 10938, loss 0.0644338, acc 0.98
2016-09-06T11:27:56.144824: step 10939, loss 0.0284028, acc 1
2016-09-06T11:27:56.917933: step 10940, loss 0.00241592, acc 1
2016-09-06T11:27:57.737628: step 10941, loss 0.084225, acc 0.98
2016-09-06T11:27:58.588582: step 10942, loss 0.0347907, acc 0.98
2016-09-06T11:27:59.353269: step 10943, loss 0.0139653, acc 1
2016-09-06T11:28:00.109860: step 10944, loss 0.00253075, acc 1
2016-09-06T11:28:00.938994: step 10945, loss 0.034641, acc 0.98
2016-09-06T11:28:01.711470: step 10946, loss 0.0306432, acc 0.98
2016-09-06T11:28:02.550499: step 10947, loss 0.0155709, acc 1
2016-09-06T11:28:03.387180: step 10948, loss 0.00620539, acc 1
2016-09-06T11:28:04.203179: step 10949, loss 0.0373625, acc 0.98
2016-09-06T11:28:05.032462: step 10950, loss 0.0478249, acc 1
2016-09-06T11:28:05.860682: step 10951, loss 0.0350839, acc 0.98
2016-09-06T11:28:06.657307: step 10952, loss 0.0269254, acc 0.98
2016-09-06T11:28:07.462134: step 10953, loss 0.0969895, acc 0.96
2016-09-06T11:28:08.268062: step 10954, loss 0.0276584, acc 0.98
2016-09-06T11:28:09.060361: step 10955, loss 0.00225518, acc 1
2016-09-06T11:28:09.876092: step 10956, loss 0.00332892, acc 1
2016-09-06T11:28:10.699307: step 10957, loss 0.00662143, acc 1
2016-09-06T11:28:11.488145: step 10958, loss 0.0199082, acc 1
2016-09-06T11:28:12.296014: step 10959, loss 0.0226295, acc 0.98
2016-09-06T11:28:13.102136: step 10960, loss 0.00229197, acc 1
2016-09-06T11:28:13.895508: step 10961, loss 0.00282707, acc 1
2016-09-06T11:28:14.692286: step 10962, loss 0.00823683, acc 1
2016-09-06T11:28:15.519489: step 10963, loss 0.00251045, acc 1
2016-09-06T11:28:16.311215: step 10964, loss 0.00950866, acc 1
2016-09-06T11:28:17.091630: step 10965, loss 0.00313712, acc 1
2016-09-06T11:28:17.916315: step 10966, loss 0.00699179, acc 1
2016-09-06T11:28:18.720548: step 10967, loss 0.00220856, acc 1
2016-09-06T11:28:19.530395: step 10968, loss 0.105364, acc 0.98
2016-09-06T11:28:20.361831: step 10969, loss 0.0053083, acc 1
2016-09-06T11:28:21.151333: step 10970, loss 0.0311151, acc 0.98
2016-09-06T11:28:21.952811: step 10971, loss 0.00262581, acc 1
2016-09-06T11:28:22.760241: step 10972, loss 0.00252524, acc 1
2016-09-06T11:28:23.527059: step 10973, loss 0.0197091, acc 0.98
2016-09-06T11:28:24.322142: step 10974, loss 0.0277423, acc 0.98
2016-09-06T11:28:25.125342: step 10975, loss 0.00331646, acc 1
2016-09-06T11:28:25.935228: step 10976, loss 0.0212871, acc 1
2016-09-06T11:28:26.758909: step 10977, loss 0.0476275, acc 0.98
2016-09-06T11:28:27.577755: step 10978, loss 0.0244959, acc 1
2016-09-06T11:28:28.352014: step 10979, loss 0.00234324, acc 1
2016-09-06T11:28:29.219276: step 10980, loss 0.00288646, acc 1
2016-09-06T11:28:30.036054: step 10981, loss 0.0188485, acc 1
2016-09-06T11:28:30.841125: step 10982, loss 0.0468032, acc 0.98
2016-09-06T11:28:31.671360: step 10983, loss 0.0250536, acc 0.98
2016-09-06T11:28:32.507946: step 10984, loss 0.00921623, acc 1
2016-09-06T11:28:33.340898: step 10985, loss 0.0103731, acc 1
2016-09-06T11:28:34.143910: step 10986, loss 0.00230142, acc 1
2016-09-06T11:28:34.961459: step 10987, loss 0.0346014, acc 0.98
2016-09-06T11:28:35.844947: step 10988, loss 0.0050607, acc 1
2016-09-06T11:28:36.676679: step 10989, loss 0.019478, acc 0.98
2016-09-06T11:28:37.518869: step 10990, loss 0.0231562, acc 1
2016-09-06T11:28:38.318604: step 10991, loss 0.0212874, acc 1
2016-09-06T11:28:39.149928: step 10992, loss 0.0168964, acc 0.98
2016-09-06T11:28:39.987000: step 10993, loss 0.0337932, acc 0.98
2016-09-06T11:28:40.825426: step 10994, loss 0.00516767, acc 1
2016-09-06T11:28:41.630681: step 10995, loss 0.0262754, acc 0.98
2016-09-06T11:28:42.455261: step 10996, loss 0.0222137, acc 1
2016-09-06T11:28:43.266432: step 10997, loss 0.0648252, acc 0.96
2016-09-06T11:28:44.043016: step 10998, loss 0.0272562, acc 0.98
2016-09-06T11:28:44.866482: step 10999, loss 0.0167931, acc 1
2016-09-06T11:28:45.690460: step 11000, loss 0.0220451, acc 0.98

Evaluation:
2016-09-06T11:28:49.375376: step 11000, loss 2.01905, acc 0.746717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-11000

2016-09-06T11:28:51.202460: step 11001, loss 0.0236959, acc 0.98
2016-09-06T11:28:52.029658: step 11002, loss 0.0130256, acc 1
2016-09-06T11:28:52.907401: step 11003, loss 0.00270245, acc 1
2016-09-06T11:28:53.736192: step 11004, loss 0.00420162, acc 1
2016-09-06T11:28:54.609699: step 11005, loss 0.0178671, acc 0.98
2016-09-06T11:28:55.403288: step 11006, loss 0.0414664, acc 0.98
2016-09-06T11:28:56.217399: step 11007, loss 0.00497658, acc 1
2016-09-06T11:28:57.014015: step 11008, loss 0.0385408, acc 0.98
2016-09-06T11:28:57.843319: step 11009, loss 0.0126888, acc 1
2016-09-06T11:28:58.627865: step 11010, loss 0.00369557, acc 1
2016-09-06T11:28:59.484060: step 11011, loss 0.0174772, acc 0.98
2016-09-06T11:29:00.320213: step 11012, loss 0.0337725, acc 0.98
2016-09-06T11:29:01.118850: step 11013, loss 0.0232102, acc 0.98
2016-09-06T11:29:01.960686: step 11014, loss 0.00216087, acc 1
2016-09-06T11:29:02.790099: step 11015, loss 0.0421313, acc 0.98
2016-09-06T11:29:03.618667: step 11016, loss 0.00404801, acc 1
2016-09-06T11:29:04.424916: step 11017, loss 0.0428938, acc 0.98
2016-09-06T11:29:05.256322: step 11018, loss 0.0050997, acc 1
2016-09-06T11:29:06.093412: step 11019, loss 0.00637517, acc 1
2016-09-06T11:29:06.912717: step 11020, loss 0.00324993, acc 1
2016-09-06T11:29:07.742410: step 11021, loss 0.00209771, acc 1
2016-09-06T11:29:08.558534: step 11022, loss 0.0111104, acc 1
2016-09-06T11:29:09.376889: step 11023, loss 0.0269706, acc 1
2016-09-06T11:29:10.220880: step 11024, loss 0.0136564, acc 1
2016-09-06T11:29:11.028086: step 11025, loss 0.00839339, acc 1
2016-09-06T11:29:11.861266: step 11026, loss 0.00210602, acc 1
2016-09-06T11:29:12.737440: step 11027, loss 0.00222728, acc 1
2016-09-06T11:29:13.577273: step 11028, loss 0.00221329, acc 1
2016-09-06T11:29:14.354510: step 11029, loss 0.0231866, acc 0.98
2016-09-06T11:29:15.136165: step 11030, loss 0.0416138, acc 0.96
2016-09-06T11:29:15.966632: step 11031, loss 0.00200214, acc 1
2016-09-06T11:29:16.757568: step 11032, loss 0.0294274, acc 0.98
2016-09-06T11:29:17.576279: step 11033, loss 0.293415, acc 0.94
2016-09-06T11:29:18.393776: step 11034, loss 0.00581403, acc 1
2016-09-06T11:29:19.172672: step 11035, loss 0.0155506, acc 1
2016-09-06T11:29:19.961974: step 11036, loss 0.0088622, acc 1
2016-09-06T11:29:20.809975: step 11037, loss 0.0032569, acc 1
2016-09-06T11:29:21.622721: step 11038, loss 0.00193789, acc 1
2016-09-06T11:29:22.416862: step 11039, loss 0.00914837, acc 1
2016-09-06T11:29:23.217622: step 11040, loss 0.00716038, acc 1
2016-09-06T11:29:23.987840: step 11041, loss 0.00268754, acc 1
2016-09-06T11:29:24.819053: step 11042, loss 0.0351434, acc 1
2016-09-06T11:29:25.632807: step 11043, loss 0.00333307, acc 1
2016-09-06T11:29:26.407524: step 11044, loss 0.0278086, acc 0.98
2016-09-06T11:29:27.204752: step 11045, loss 0.00331741, acc 1
2016-09-06T11:29:28.019150: step 11046, loss 0.00335763, acc 1
2016-09-06T11:29:28.817717: step 11047, loss 0.00222118, acc 1
2016-09-06T11:29:29.625156: step 11048, loss 0.0136158, acc 1
2016-09-06T11:29:30.438148: step 11049, loss 0.0185057, acc 0.98
2016-09-06T11:29:31.231806: step 11050, loss 0.00193334, acc 1
2016-09-06T11:29:32.057751: step 11051, loss 0.0294299, acc 1
2016-09-06T11:29:32.849561: step 11052, loss 0.0328255, acc 0.98
2016-09-06T11:29:33.680696: step 11053, loss 0.0362895, acc 0.98
2016-09-06T11:29:34.503916: step 11054, loss 0.0189628, acc 1
2016-09-06T11:29:35.307207: step 11055, loss 0.00297857, acc 1
2016-09-06T11:29:36.105845: step 11056, loss 0.0138929, acc 1
2016-09-06T11:29:36.922840: step 11057, loss 0.0182135, acc 1
2016-09-06T11:29:37.724677: step 11058, loss 0.0415517, acc 0.96
2016-09-06T11:29:38.532469: step 11059, loss 0.0197733, acc 1
2016-09-06T11:29:39.335156: step 11060, loss 0.00392055, acc 1
2016-09-06T11:29:40.180948: step 11061, loss 0.00974455, acc 1
2016-09-06T11:29:41.008298: step 11062, loss 0.0235765, acc 1
2016-09-06T11:29:41.845558: step 11063, loss 0.0301294, acc 1
2016-09-06T11:29:42.684869: step 11064, loss 0.0368921, acc 1
2016-09-06T11:29:43.503062: step 11065, loss 0.0198591, acc 1
2016-09-06T11:29:44.349413: step 11066, loss 0.0306073, acc 0.98
2016-09-06T11:29:45.176620: step 11067, loss 0.00313406, acc 1
2016-09-06T11:29:46.108370: step 11068, loss 0.00362012, acc 1
2016-09-06T11:29:46.904027: step 11069, loss 0.0212413, acc 1
2016-09-06T11:29:47.721240: step 11070, loss 0.00930242, acc 1
2016-09-06T11:29:48.535966: step 11071, loss 0.0160608, acc 1
2016-09-06T11:29:49.322610: step 11072, loss 0.00604539, acc 1
2016-09-06T11:29:50.126679: step 11073, loss 0.0142338, acc 1
2016-09-06T11:29:50.956687: step 11074, loss 0.0044031, acc 1
2016-09-06T11:29:51.731536: step 11075, loss 0.00967412, acc 1
2016-09-06T11:29:52.585791: step 11076, loss 0.0369093, acc 0.98
2016-09-06T11:29:53.400939: step 11077, loss 0.0100718, acc 1
2016-09-06T11:29:54.193232: step 11078, loss 0.023873, acc 1
2016-09-06T11:29:54.984894: step 11079, loss 0.0128225, acc 1
2016-09-06T11:29:55.799129: step 11080, loss 0.0161065, acc 1
2016-09-06T11:29:56.592025: step 11081, loss 0.0116363, acc 1
2016-09-06T11:29:57.388519: step 11082, loss 0.0210922, acc 1
2016-09-06T11:29:58.226265: step 11083, loss 0.00409758, acc 1
2016-09-06T11:29:59.012681: step 11084, loss 0.0388075, acc 0.98
2016-09-06T11:29:59.809706: step 11085, loss 0.0571485, acc 0.98
2016-09-06T11:30:00.634244: step 11086, loss 0.00285518, acc 1
2016-09-06T11:30:01.425389: step 11087, loss 0.002905, acc 1
2016-09-06T11:30:02.226816: step 11088, loss 0.030382, acc 0.98
2016-09-06T11:30:03.059568: step 11089, loss 0.00660558, acc 1
2016-09-06T11:30:03.855589: step 11090, loss 0.00637736, acc 1
2016-09-06T11:30:04.681996: step 11091, loss 0.00278572, acc 1
2016-09-06T11:30:05.512491: step 11092, loss 0.0317218, acc 0.98
2016-09-06T11:30:06.347990: step 11093, loss 0.00307093, acc 1
2016-09-06T11:30:07.155124: step 11094, loss 0.0215682, acc 1
2016-09-06T11:30:07.974170: step 11095, loss 0.00264749, acc 1
2016-09-06T11:30:08.780426: step 11096, loss 0.00391808, acc 1
2016-09-06T11:30:09.600026: step 11097, loss 0.0397544, acc 0.96
2016-09-06T11:30:10.470495: step 11098, loss 0.00802919, acc 1
2016-09-06T11:30:11.293474: step 11099, loss 0.00632324, acc 1
2016-09-06T11:30:12.149303: step 11100, loss 0.0277462, acc 0.98

Evaluation:
2016-09-06T11:30:15.878541: step 11100, loss 2.97413, acc 0.737336

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-11100

2016-09-06T11:30:17.824624: step 11101, loss 0.0401342, acc 0.96
2016-09-06T11:30:18.635473: step 11102, loss 0.0301864, acc 0.98
2016-09-06T11:30:19.442848: step 11103, loss 0.00452526, acc 1
2016-09-06T11:30:20.265235: step 11104, loss 0.0186671, acc 1
2016-09-06T11:30:21.068260: step 11105, loss 0.0999659, acc 0.94
2016-09-06T11:30:21.886045: step 11106, loss 0.0173171, acc 0.98
2016-09-06T11:30:22.740216: step 11107, loss 0.0153131, acc 1
2016-09-06T11:30:23.513685: step 11108, loss 0.00790483, acc 1
2016-09-06T11:30:24.348207: step 11109, loss 0.00258487, acc 1
2016-09-06T11:30:25.167107: step 11110, loss 0.0180049, acc 0.98
2016-09-06T11:30:25.977617: step 11111, loss 0.00263159, acc 1
2016-09-06T11:30:26.779294: step 11112, loss 0.00273904, acc 1
2016-09-06T11:30:27.599888: step 11113, loss 0.0380156, acc 0.96
2016-09-06T11:30:28.425355: step 11114, loss 0.0318295, acc 0.98
2016-09-06T11:30:29.208614: step 11115, loss 0.00263104, acc 1
2016-09-06T11:30:30.014471: step 11116, loss 0.0118978, acc 1
2016-09-06T11:30:30.828592: step 11117, loss 0.00809678, acc 1
2016-09-06T11:30:31.648130: step 11118, loss 0.0185485, acc 0.98
2016-09-06T11:30:32.451289: step 11119, loss 0.0159278, acc 1
2016-09-06T11:30:33.285023: step 11120, loss 0.00263274, acc 1
2016-09-06T11:30:34.071193: step 11121, loss 0.0247904, acc 1
2016-09-06T11:30:34.866395: step 11122, loss 0.115313, acc 0.96
2016-09-06T11:30:35.688566: step 11123, loss 0.0306084, acc 0.98
2016-09-06T11:30:36.482131: step 11124, loss 0.0281668, acc 0.98
2016-09-06T11:30:37.277495: step 11125, loss 0.00996197, acc 1
2016-09-06T11:30:38.085321: step 11126, loss 0.128019, acc 0.98
2016-09-06T11:30:38.868618: step 11127, loss 0.00253852, acc 1
2016-09-06T11:30:39.701449: step 11128, loss 0.00278914, acc 1
2016-09-06T11:30:40.536250: step 11129, loss 0.0356341, acc 0.98
2016-09-06T11:30:41.388242: step 11130, loss 0.0126384, acc 1
2016-09-06T11:30:42.191243: step 11131, loss 0.0142466, acc 1
2016-09-06T11:30:43.003586: step 11132, loss 0.0243477, acc 0.98
2016-09-06T11:30:43.784525: step 11133, loss 0.0135975, acc 1
2016-09-06T11:30:44.586479: step 11134, loss 0.0506736, acc 0.98
2016-09-06T11:30:45.405287: step 11135, loss 0.0211214, acc 0.98
2016-09-06T11:30:46.128974: step 11136, loss 0.00412016, acc 1
2016-09-06T11:30:46.952297: step 11137, loss 0.0211111, acc 0.98
2016-09-06T11:30:47.811305: step 11138, loss 0.0248497, acc 0.98
2016-09-06T11:30:48.613724: step 11139, loss 0.036076, acc 0.98
2016-09-06T11:30:49.405580: step 11140, loss 0.0846094, acc 0.98
2016-09-06T11:30:50.232312: step 11141, loss 0.0230846, acc 1
2016-09-06T11:30:51.047593: step 11142, loss 0.00835522, acc 1
2016-09-06T11:30:51.848130: step 11143, loss 0.00448537, acc 1
2016-09-06T11:30:52.698780: step 11144, loss 0.00659118, acc 1
2016-09-06T11:30:53.516157: step 11145, loss 0.0900546, acc 0.94
2016-09-06T11:30:54.341455: step 11146, loss 0.00992111, acc 1
2016-09-06T11:30:55.186808: step 11147, loss 0.00230252, acc 1
2016-09-06T11:30:56.003147: step 11148, loss 0.00341376, acc 1
2016-09-06T11:30:56.827125: step 11149, loss 0.0248151, acc 0.98
2016-09-06T11:30:57.660919: step 11150, loss 0.0300572, acc 0.98
2016-09-06T11:30:58.488508: step 11151, loss 0.0737365, acc 0.98
2016-09-06T11:30:59.326183: step 11152, loss 0.014871, acc 1
2016-09-06T11:31:00.181389: step 11153, loss 0.0568694, acc 0.98
2016-09-06T11:31:01.031478: step 11154, loss 0.00314667, acc 1
2016-09-06T11:31:01.837113: step 11155, loss 0.0102293, acc 1
2016-09-06T11:31:02.691572: step 11156, loss 0.00299751, acc 1
2016-09-06T11:31:03.515666: step 11157, loss 0.00214961, acc 1
2016-09-06T11:31:04.330697: step 11158, loss 0.0192159, acc 0.98
2016-09-06T11:31:05.191469: step 11159, loss 0.0034296, acc 1
2016-09-06T11:31:06.012454: step 11160, loss 0.0248612, acc 0.98
2016-09-06T11:31:06.797505: step 11161, loss 0.0198008, acc 1
2016-09-06T11:31:07.639873: step 11162, loss 0.0343278, acc 0.98
2016-09-06T11:31:08.476205: step 11163, loss 0.00657556, acc 1
2016-09-06T11:31:09.291230: step 11164, loss 0.0024041, acc 1
2016-09-06T11:31:10.119358: step 11165, loss 0.00426375, acc 1
2016-09-06T11:31:10.958272: step 11166, loss 0.0211156, acc 1
2016-09-06T11:31:11.738574: step 11167, loss 0.00369256, acc 1
2016-09-06T11:31:12.549465: step 11168, loss 0.0174739, acc 0.98
2016-09-06T11:31:13.369592: step 11169, loss 0.00338096, acc 1
2016-09-06T11:31:14.164330: step 11170, loss 0.00486248, acc 1
2016-09-06T11:31:14.985464: step 11171, loss 0.00873752, acc 1
2016-09-06T11:31:15.802483: step 11172, loss 0.00387003, acc 1
2016-09-06T11:31:16.571378: step 11173, loss 0.0238651, acc 0.98
2016-09-06T11:31:17.403900: step 11174, loss 0.0225722, acc 1
2016-09-06T11:31:18.224819: step 11175, loss 0.0544858, acc 0.98
2016-09-06T11:31:19.039281: step 11176, loss 0.00365723, acc 1
2016-09-06T11:31:19.878170: step 11177, loss 0.0149555, acc 1
2016-09-06T11:31:20.700546: step 11178, loss 0.00302255, acc 1
2016-09-06T11:31:21.509728: step 11179, loss 0.0648117, acc 0.98
2016-09-06T11:31:22.351963: step 11180, loss 0.025894, acc 0.98
2016-09-06T11:31:23.169957: step 11181, loss 0.00290636, acc 1
2016-09-06T11:31:23.970192: step 11182, loss 0.00280011, acc 1
2016-09-06T11:31:24.794369: step 11183, loss 0.0159622, acc 1
2016-09-06T11:31:25.618499: step 11184, loss 0.00268437, acc 1
2016-09-06T11:31:26.442157: step 11185, loss 0.00737632, acc 1
2016-09-06T11:31:27.279336: step 11186, loss 0.0118819, acc 1
2016-09-06T11:31:28.097196: step 11187, loss 0.00435271, acc 1
2016-09-06T11:31:28.906498: step 11188, loss 0.00735771, acc 1
2016-09-06T11:31:29.730472: step 11189, loss 0.0424416, acc 0.98
2016-09-06T11:31:30.533402: step 11190, loss 0.011661, acc 1
2016-09-06T11:31:31.374369: step 11191, loss 0.00675051, acc 1
2016-09-06T11:31:32.171066: step 11192, loss 0.00259778, acc 1
2016-09-06T11:31:32.995123: step 11193, loss 0.00369731, acc 1
2016-09-06T11:31:33.788181: step 11194, loss 0.0109694, acc 1
2016-09-06T11:31:34.580511: step 11195, loss 0.00874146, acc 1
2016-09-06T11:31:35.383352: step 11196, loss 0.0031521, acc 1
2016-09-06T11:31:36.217138: step 11197, loss 0.0190264, acc 1
2016-09-06T11:31:36.999132: step 11198, loss 0.00366991, acc 1
2016-09-06T11:31:37.824665: step 11199, loss 0.00251884, acc 1
2016-09-06T11:31:38.631105: step 11200, loss 0.0210923, acc 1

Evaluation:
2016-09-06T11:31:42.363676: step 11200, loss 2.48202, acc 0.750469

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-11200

2016-09-06T11:31:44.335642: step 11201, loss 0.00878944, acc 1
2016-09-06T11:31:45.170126: step 11202, loss 0.0104404, acc 1
2016-09-06T11:31:45.976075: step 11203, loss 0.00329563, acc 1
2016-09-06T11:31:46.773838: step 11204, loss 0.0166488, acc 1
2016-09-06T11:31:47.637248: step 11205, loss 0.0144321, acc 1
2016-09-06T11:31:48.479128: step 11206, loss 0.219112, acc 0.96
2016-09-06T11:31:49.246387: step 11207, loss 0.0133072, acc 1
2016-09-06T11:31:50.050676: step 11208, loss 0.00431765, acc 1
2016-09-06T11:31:50.882767: step 11209, loss 0.00271277, acc 1
2016-09-06T11:31:51.659848: step 11210, loss 0.00250305, acc 1
2016-09-06T11:31:52.443726: step 11211, loss 0.0164542, acc 1
2016-09-06T11:31:53.258363: step 11212, loss 0.00715939, acc 1
2016-09-06T11:31:54.061585: step 11213, loss 0.00379154, acc 1
2016-09-06T11:31:54.905997: step 11214, loss 0.133677, acc 0.98
2016-09-06T11:31:55.715494: step 11215, loss 0.00753092, acc 1
2016-09-06T11:31:56.482588: step 11216, loss 0.0045232, acc 1
2016-09-06T11:31:57.313431: step 11217, loss 0.00241728, acc 1
2016-09-06T11:31:58.134520: step 11218, loss 0.00246281, acc 1
2016-09-06T11:31:58.942419: step 11219, loss 0.00979355, acc 1
2016-09-06T11:31:59.727187: step 11220, loss 0.0051215, acc 1
2016-09-06T11:32:00.559526: step 11221, loss 0.0348945, acc 0.98
2016-09-06T11:32:01.362103: step 11222, loss 0.0142733, acc 1
2016-09-06T11:32:02.182834: step 11223, loss 0.0036227, acc 1
2016-09-06T11:32:02.995884: step 11224, loss 0.0131629, acc 1
2016-09-06T11:32:03.801099: step 11225, loss 0.003364, acc 1
2016-09-06T11:32:04.614842: step 11226, loss 0.00954747, acc 1
2016-09-06T11:32:05.451633: step 11227, loss 0.0177182, acc 0.98
2016-09-06T11:32:06.269395: step 11228, loss 0.00463037, acc 1
2016-09-06T11:32:07.069857: step 11229, loss 0.0187985, acc 1
2016-09-06T11:32:07.910239: step 11230, loss 0.00319374, acc 1
2016-09-06T11:32:08.717021: step 11231, loss 0.063925, acc 0.98
2016-09-06T11:32:09.521106: step 11232, loss 0.00727371, acc 1
2016-09-06T11:32:10.340993: step 11233, loss 0.00278225, acc 1
2016-09-06T11:32:11.160220: step 11234, loss 0.0168491, acc 1
2016-09-06T11:32:11.971281: step 11235, loss 0.00320696, acc 1
2016-09-06T11:32:12.795022: step 11236, loss 0.00610458, acc 1
2016-09-06T11:32:13.622515: step 11237, loss 0.135097, acc 0.96
2016-09-06T11:32:14.454348: step 11238, loss 0.00315518, acc 1
2016-09-06T11:32:15.268908: step 11239, loss 0.0306092, acc 0.98
2016-09-06T11:32:16.055250: step 11240, loss 0.00270566, acc 1
2016-09-06T11:32:16.857908: step 11241, loss 0.0171596, acc 1
2016-09-06T11:32:17.682900: step 11242, loss 0.00298101, acc 1
2016-09-06T11:32:18.477153: step 11243, loss 0.0440441, acc 0.98
2016-09-06T11:32:19.289934: step 11244, loss 0.0502737, acc 0.96
2016-09-06T11:32:20.124536: step 11245, loss 0.0165337, acc 1
2016-09-06T11:32:20.940077: step 11246, loss 0.0452194, acc 0.98
2016-09-06T11:32:21.747792: step 11247, loss 0.0102316, acc 1
2016-09-06T11:32:22.607588: step 11248, loss 0.0214151, acc 0.98
2016-09-06T11:32:23.422938: step 11249, loss 0.0328429, acc 0.98
2016-09-06T11:32:24.222942: step 11250, loss 0.0508031, acc 0.98
2016-09-06T11:32:25.030876: step 11251, loss 0.0266342, acc 0.98
2016-09-06T11:32:25.843859: step 11252, loss 0.0428636, acc 0.98
2016-09-06T11:32:26.640700: step 11253, loss 0.0045862, acc 1
2016-09-06T11:32:27.460084: step 11254, loss 0.00309461, acc 1
2016-09-06T11:32:28.285410: step 11255, loss 0.00309073, acc 1
2016-09-06T11:32:29.077426: step 11256, loss 0.0175747, acc 1
2016-09-06T11:32:29.887877: step 11257, loss 0.00337386, acc 1
2016-09-06T11:32:30.678880: step 11258, loss 0.00948115, acc 1
2016-09-06T11:32:31.522060: step 11259, loss 0.00776611, acc 1
2016-09-06T11:32:32.338524: step 11260, loss 0.00382967, acc 1
2016-09-06T11:32:33.191728: step 11261, loss 0.02815, acc 0.98
2016-09-06T11:32:34.008192: step 11262, loss 0.033809, acc 0.98
2016-09-06T11:32:34.811362: step 11263, loss 0.00512476, acc 1
2016-09-06T11:32:35.624564: step 11264, loss 0.00416778, acc 1
2016-09-06T11:32:36.423539: step 11265, loss 0.00372928, acc 1
2016-09-06T11:32:37.228847: step 11266, loss 0.0281761, acc 0.98
2016-09-06T11:32:38.029160: step 11267, loss 0.0124149, acc 1
2016-09-06T11:32:38.829810: step 11268, loss 0.0115236, acc 1
2016-09-06T11:32:39.650450: step 11269, loss 0.0046915, acc 1
2016-09-06T11:32:40.461653: step 11270, loss 0.0269616, acc 1
2016-09-06T11:32:41.269816: step 11271, loss 0.0375294, acc 0.98
2016-09-06T11:32:42.075139: step 11272, loss 0.0169009, acc 1
2016-09-06T11:32:42.899904: step 11273, loss 0.00406696, acc 1
2016-09-06T11:32:43.676792: step 11274, loss 0.0237096, acc 1
2016-09-06T11:32:44.472038: step 11275, loss 0.00391443, acc 1
2016-09-06T11:32:45.311653: step 11276, loss 0.00391629, acc 1
2016-09-06T11:32:46.115085: step 11277, loss 0.00690876, acc 1
2016-09-06T11:32:46.917912: step 11278, loss 0.00809872, acc 1
2016-09-06T11:32:47.732587: step 11279, loss 0.0112789, acc 1
2016-09-06T11:32:48.528086: step 11280, loss 0.0153243, acc 1
2016-09-06T11:32:49.352767: step 11281, loss 0.00424655, acc 1
2016-09-06T11:32:50.193384: step 11282, loss 0.0279744, acc 0.98
2016-09-06T11:32:51.004845: step 11283, loss 0.00996442, acc 1
2016-09-06T11:32:51.802635: step 11284, loss 0.00765354, acc 1
2016-09-06T11:32:52.661724: step 11285, loss 0.0208051, acc 0.98
2016-09-06T11:32:53.496875: step 11286, loss 0.0356792, acc 0.98
2016-09-06T11:32:54.310575: step 11287, loss 0.00512387, acc 1
2016-09-06T11:32:55.155384: step 11288, loss 0.00498172, acc 1
2016-09-06T11:32:55.959095: step 11289, loss 0.0104673, acc 1
2016-09-06T11:32:56.779836: step 11290, loss 0.013458, acc 1
2016-09-06T11:32:57.600266: step 11291, loss 0.0038604, acc 1
2016-09-06T11:32:58.401054: step 11292, loss 0.0687174, acc 0.98
2016-09-06T11:32:59.200864: step 11293, loss 0.00402532, acc 1
2016-09-06T11:33:00.033698: step 11294, loss 0.00385703, acc 1
2016-09-06T11:33:00.909076: step 11295, loss 0.00363102, acc 1
2016-09-06T11:33:01.706504: step 11296, loss 0.0140038, acc 1
2016-09-06T11:33:02.539854: step 11297, loss 0.0159342, acc 1
2016-09-06T11:33:03.342280: step 11298, loss 0.0528386, acc 0.96
2016-09-06T11:33:04.143513: step 11299, loss 0.00382819, acc 1
2016-09-06T11:33:04.970625: step 11300, loss 0.0039487, acc 1

Evaluation:
2016-09-06T11:33:08.703666: step 11300, loss 2.64971, acc 0.762664

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-11300

2016-09-06T11:33:10.662926: step 11301, loss 0.00339435, acc 1
2016-09-06T11:33:11.483936: step 11302, loss 0.0170959, acc 1
2016-09-06T11:33:12.285322: step 11303, loss 0.00333316, acc 1
2016-09-06T11:33:13.105391: step 11304, loss 0.00444947, acc 1
2016-09-06T11:33:13.879867: step 11305, loss 0.108152, acc 0.94
2016-09-06T11:33:14.682851: step 11306, loss 0.0152582, acc 1
2016-09-06T11:33:15.523034: step 11307, loss 0.00937118, acc 1
2016-09-06T11:33:16.328940: step 11308, loss 0.00345606, acc 1
2016-09-06T11:33:17.138105: step 11309, loss 0.029683, acc 0.98
2016-09-06T11:33:17.947825: step 11310, loss 0.0227217, acc 1
2016-09-06T11:33:18.713406: step 11311, loss 0.0469726, acc 1
2016-09-06T11:33:19.506816: step 11312, loss 0.0971277, acc 0.98
2016-09-06T11:33:20.337392: step 11313, loss 0.0225099, acc 0.98
2016-09-06T11:33:21.116609: step 11314, loss 0.143757, acc 0.98
2016-09-06T11:33:21.913126: step 11315, loss 0.00395316, acc 1
2016-09-06T11:33:22.723629: step 11316, loss 0.030417, acc 0.98
2016-09-06T11:33:23.518880: step 11317, loss 0.00319905, acc 1
2016-09-06T11:33:24.331564: step 11318, loss 0.00307383, acc 1
2016-09-06T11:33:25.159639: step 11319, loss 0.00259568, acc 1
2016-09-06T11:33:25.968971: step 11320, loss 0.00461371, acc 1
2016-09-06T11:33:26.788873: step 11321, loss 0.012473, acc 1
2016-09-06T11:33:27.606806: step 11322, loss 0.0335487, acc 0.96
2016-09-06T11:33:28.434129: step 11323, loss 0.0145326, acc 1
2016-09-06T11:33:29.236681: step 11324, loss 0.00418265, acc 1
2016-09-06T11:33:30.054280: step 11325, loss 0.0105995, acc 1
2016-09-06T11:33:30.909464: step 11326, loss 0.00578066, acc 1
2016-09-06T11:33:31.724402: step 11327, loss 0.00654784, acc 1
2016-09-06T11:33:32.476556: step 11328, loss 0.00390926, acc 1
2016-09-06T11:33:33.280051: step 11329, loss 0.0083256, acc 1
2016-09-06T11:33:34.095296: step 11330, loss 0.0202846, acc 1
2016-09-06T11:33:34.933398: step 11331, loss 0.00301638, acc 1
2016-09-06T11:33:35.750694: step 11332, loss 0.00596997, acc 1
2016-09-06T11:33:36.556458: step 11333, loss 0.0368251, acc 1
2016-09-06T11:33:37.370812: step 11334, loss 0.0329814, acc 0.98
2016-09-06T11:33:38.176162: step 11335, loss 0.0680848, acc 0.96
2016-09-06T11:33:38.979512: step 11336, loss 0.0186246, acc 0.98
2016-09-06T11:33:39.812439: step 11337, loss 0.0304212, acc 1
2016-09-06T11:33:40.621610: step 11338, loss 0.0513595, acc 0.96
2016-09-06T11:33:41.451855: step 11339, loss 0.00361806, acc 1
2016-09-06T11:33:42.290024: step 11340, loss 0.00690946, acc 1
2016-09-06T11:33:43.118239: step 11341, loss 0.0175365, acc 1
2016-09-06T11:33:43.935968: step 11342, loss 0.0152231, acc 1
2016-09-06T11:33:44.747582: step 11343, loss 0.0519075, acc 0.98
2016-09-06T11:33:45.561808: step 11344, loss 0.00281883, acc 1
2016-09-06T11:33:46.359936: step 11345, loss 0.0177572, acc 1
2016-09-06T11:33:47.193917: step 11346, loss 0.00300499, acc 1
2016-09-06T11:33:48.007113: step 11347, loss 0.00457203, acc 1
2016-09-06T11:33:48.822820: step 11348, loss 0.00219414, acc 1
2016-09-06T11:33:49.669719: step 11349, loss 0.00262477, acc 1
2016-09-06T11:33:50.511338: step 11350, loss 0.0304399, acc 0.98
2016-09-06T11:33:51.283002: step 11351, loss 0.00220162, acc 1
2016-09-06T11:33:52.096329: step 11352, loss 0.0203196, acc 1
2016-09-06T11:33:52.934614: step 11353, loss 0.00669901, acc 1
2016-09-06T11:33:53.708874: step 11354, loss 0.0250396, acc 1
2016-09-06T11:33:54.528468: step 11355, loss 0.00264371, acc 1
2016-09-06T11:33:55.350010: step 11356, loss 0.00549902, acc 1
2016-09-06T11:33:56.141438: step 11357, loss 0.00306141, acc 1
2016-09-06T11:33:56.924586: step 11358, loss 0.00372161, acc 1
2016-09-06T11:33:57.739168: step 11359, loss 0.016348, acc 1
2016-09-06T11:33:58.541886: step 11360, loss 0.0163184, acc 1
2016-09-06T11:33:59.346935: step 11361, loss 0.0454655, acc 0.96
2016-09-06T11:34:00.144739: step 11362, loss 0.0797051, acc 0.94
2016-09-06T11:34:00.989811: step 11363, loss 0.0307835, acc 0.98
2016-09-06T11:34:01.786028: step 11364, loss 0.00662165, acc 1
2016-09-06T11:34:02.621293: step 11365, loss 0.0486434, acc 0.98
2016-09-06T11:34:03.405446: step 11366, loss 0.016667, acc 1
2016-09-06T11:34:04.198802: step 11367, loss 0.00591141, acc 1
2016-09-06T11:34:05.019177: step 11368, loss 0.00222368, acc 1
2016-09-06T11:34:05.822493: step 11369, loss 0.101027, acc 0.98
2016-09-06T11:34:06.642531: step 11370, loss 0.00202164, acc 1
2016-09-06T11:34:07.471865: step 11371, loss 0.00319092, acc 1
2016-09-06T11:34:08.259117: step 11372, loss 0.00217994, acc 1
2016-09-06T11:34:09.038903: step 11373, loss 0.00189005, acc 1
2016-09-06T11:34:09.856739: step 11374, loss 0.00278286, acc 1
2016-09-06T11:34:10.664824: step 11375, loss 0.0104333, acc 1
2016-09-06T11:34:11.484702: step 11376, loss 0.0121309, acc 1
2016-09-06T11:34:12.276642: step 11377, loss 0.00819609, acc 1
2016-09-06T11:34:13.073935: step 11378, loss 0.0019301, acc 1
2016-09-06T11:34:13.872302: step 11379, loss 0.0244235, acc 0.98
2016-09-06T11:34:14.689264: step 11380, loss 0.0311787, acc 1
2016-09-06T11:34:15.473244: step 11381, loss 0.0168526, acc 1
2016-09-06T11:34:16.267798: step 11382, loss 0.00366262, acc 1
2016-09-06T11:34:17.094109: step 11383, loss 0.00242471, acc 1
2016-09-06T11:34:17.903892: step 11384, loss 0.00190401, acc 1
2016-09-06T11:34:18.697469: step 11385, loss 0.00221412, acc 1
2016-09-06T11:34:19.494915: step 11386, loss 0.00276528, acc 1
2016-09-06T11:34:20.303553: step 11387, loss 0.00239254, acc 1
2016-09-06T11:34:21.104485: step 11388, loss 0.00200802, acc 1
2016-09-06T11:34:21.954047: step 11389, loss 0.0715497, acc 0.96
2016-09-06T11:34:22.775744: step 11390, loss 0.00612002, acc 1
2016-09-06T11:34:23.585724: step 11391, loss 0.00186188, acc 1
2016-09-06T11:34:24.366498: step 11392, loss 0.00233428, acc 1
2016-09-06T11:34:25.172741: step 11393, loss 0.0177781, acc 1
2016-09-06T11:34:25.968863: step 11394, loss 0.0308303, acc 0.98
2016-09-06T11:34:26.791884: step 11395, loss 0.00459373, acc 1
2016-09-06T11:34:27.580012: step 11396, loss 0.0128848, acc 1
2016-09-06T11:34:28.386482: step 11397, loss 0.068704, acc 0.98
2016-09-06T11:34:29.201699: step 11398, loss 0.0307215, acc 0.98
2016-09-06T11:34:29.985494: step 11399, loss 0.053519, acc 0.98
2016-09-06T11:34:30.784043: step 11400, loss 0.00361483, acc 1

Evaluation:
2016-09-06T11:34:34.495590: step 11400, loss 1.79345, acc 0.755159

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-11400

2016-09-06T11:34:36.326040: step 11401, loss 0.0335639, acc 0.98
2016-09-06T11:34:37.141607: step 11402, loss 0.00277391, acc 1
2016-09-06T11:34:37.974482: step 11403, loss 0.0128832, acc 1
2016-09-06T11:34:38.802278: step 11404, loss 0.0182861, acc 1
2016-09-06T11:34:39.593912: step 11405, loss 0.02357, acc 1
2016-09-06T11:34:40.429429: step 11406, loss 0.0133397, acc 1
2016-09-06T11:34:41.260650: step 11407, loss 0.00274464, acc 1
2016-09-06T11:34:42.062980: step 11408, loss 0.0167102, acc 1
2016-09-06T11:34:42.896252: step 11409, loss 0.0370206, acc 0.98
2016-09-06T11:34:43.711910: step 11410, loss 0.038441, acc 0.98
2016-09-06T11:34:44.513683: step 11411, loss 0.0351214, acc 0.98
2016-09-06T11:34:45.324010: step 11412, loss 0.00609814, acc 1
2016-09-06T11:34:46.158761: step 11413, loss 0.00198913, acc 1
2016-09-06T11:34:46.974499: step 11414, loss 0.0345576, acc 0.98
2016-09-06T11:34:47.813432: step 11415, loss 0.00257062, acc 1
2016-09-06T11:34:48.682535: step 11416, loss 0.00293427, acc 1
2016-09-06T11:34:49.445399: step 11417, loss 0.00370745, acc 1
2016-09-06T11:34:50.258447: step 11418, loss 0.00202418, acc 1
2016-09-06T11:34:51.084947: step 11419, loss 0.0315837, acc 0.98
2016-09-06T11:34:51.894991: step 11420, loss 0.00291098, acc 1
2016-09-06T11:34:52.697943: step 11421, loss 0.0310991, acc 0.98
2016-09-06T11:34:53.517396: step 11422, loss 0.0125531, acc 1
2016-09-06T11:34:54.304853: step 11423, loss 0.0141738, acc 1
2016-09-06T11:34:55.110787: step 11424, loss 0.00215712, acc 1
2016-09-06T11:34:55.936708: step 11425, loss 0.0163556, acc 0.98
2016-09-06T11:34:56.725450: step 11426, loss 0.049722, acc 0.96
2016-09-06T11:34:57.545467: step 11427, loss 0.0175712, acc 0.98
2016-09-06T11:34:58.368047: step 11428, loss 0.015364, acc 1
2016-09-06T11:34:59.132482: step 11429, loss 0.117371, acc 0.96
2016-09-06T11:34:59.936331: step 11430, loss 0.0021596, acc 1
2016-09-06T11:35:00.791487: step 11431, loss 0.0086381, acc 1
2016-09-06T11:35:01.582906: step 11432, loss 0.00495579, acc 1
2016-09-06T11:35:02.386306: step 11433, loss 0.0302935, acc 0.96
2016-09-06T11:35:03.192740: step 11434, loss 0.00183577, acc 1
2016-09-06T11:35:03.975700: step 11435, loss 0.0319479, acc 0.98
2016-09-06T11:35:04.767592: step 11436, loss 0.0174307, acc 0.98
2016-09-06T11:35:05.569993: step 11437, loss 0.0091947, acc 1
2016-09-06T11:35:06.352460: step 11438, loss 0.0289414, acc 0.98
2016-09-06T11:35:07.176018: step 11439, loss 0.00447332, acc 1
2016-09-06T11:35:08.005553: step 11440, loss 0.00863582, acc 1
2016-09-06T11:35:08.814184: step 11441, loss 0.0440942, acc 0.98
2016-09-06T11:35:09.641953: step 11442, loss 0.0117258, acc 1
2016-09-06T11:35:10.452043: step 11443, loss 0.0268233, acc 1
2016-09-06T11:35:11.255473: step 11444, loss 0.00242785, acc 1
2016-09-06T11:35:12.038296: step 11445, loss 0.01294, acc 1
2016-09-06T11:35:12.865337: step 11446, loss 0.00217971, acc 1
2016-09-06T11:35:13.653031: step 11447, loss 0.00333514, acc 1
2016-09-06T11:35:14.445451: step 11448, loss 0.0124201, acc 1
2016-09-06T11:35:15.265961: step 11449, loss 0.0030499, acc 1
2016-09-06T11:35:16.074522: step 11450, loss 0.00212316, acc 1
2016-09-06T11:35:16.858379: step 11451, loss 0.00242655, acc 1
2016-09-06T11:35:17.704858: step 11452, loss 0.00477437, acc 1
2016-09-06T11:35:18.484878: step 11453, loss 0.0222382, acc 0.98
2016-09-06T11:35:19.270199: step 11454, loss 0.0148601, acc 1
2016-09-06T11:35:20.072598: step 11455, loss 0.01326, acc 1
2016-09-06T11:35:20.855604: step 11456, loss 0.031875, acc 0.98
2016-09-06T11:35:21.680568: step 11457, loss 0.00924336, acc 1
2016-09-06T11:35:22.496692: step 11458, loss 0.0194925, acc 1
2016-09-06T11:35:23.275288: step 11459, loss 0.0234593, acc 0.98
2016-09-06T11:35:24.068796: step 11460, loss 0.00212034, acc 1
2016-09-06T11:35:24.870745: step 11461, loss 0.00666464, acc 1
2016-09-06T11:35:25.659453: step 11462, loss 0.00409483, acc 1
2016-09-06T11:35:26.489679: step 11463, loss 0.0130813, acc 1
2016-09-06T11:35:27.297297: step 11464, loss 0.0439869, acc 0.98
2016-09-06T11:35:28.114503: step 11465, loss 0.00232991, acc 1
2016-09-06T11:35:28.920140: step 11466, loss 0.0146435, acc 1
2016-09-06T11:35:29.729555: step 11467, loss 0.00249404, acc 1
2016-09-06T11:35:30.513876: step 11468, loss 0.00359649, acc 1
2016-09-06T11:35:31.318092: step 11469, loss 0.0208942, acc 1
2016-09-06T11:35:32.150599: step 11470, loss 0.0021226, acc 1
2016-09-06T11:35:32.936947: step 11471, loss 0.00871269, acc 1
2016-09-06T11:35:33.712409: step 11472, loss 0.0110029, acc 1
2016-09-06T11:35:34.489857: step 11473, loss 0.00214336, acc 1
2016-09-06T11:35:35.318557: step 11474, loss 0.0054524, acc 1
2016-09-06T11:35:36.139565: step 11475, loss 0.0191946, acc 1
2016-09-06T11:35:36.962389: step 11476, loss 0.00445464, acc 1
2016-09-06T11:35:37.746902: step 11477, loss 0.0082431, acc 1
2016-09-06T11:35:38.571324: step 11478, loss 0.0188617, acc 0.98
2016-09-06T11:35:39.390463: step 11479, loss 0.00291804, acc 1
2016-09-06T11:35:40.204129: step 11480, loss 0.00404549, acc 1
2016-09-06T11:35:41.008418: step 11481, loss 0.0297598, acc 0.98
2016-09-06T11:35:41.808742: step 11482, loss 0.00216457, acc 1
2016-09-06T11:35:42.579666: step 11483, loss 0.00254163, acc 1
2016-09-06T11:35:43.389695: step 11484, loss 0.0164899, acc 1
2016-09-06T11:35:44.201201: step 11485, loss 0.0321789, acc 0.98
2016-09-06T11:35:45.004422: step 11486, loss 0.00217634, acc 1
2016-09-06T11:35:45.814359: step 11487, loss 0.00234272, acc 1
2016-09-06T11:35:46.643120: step 11488, loss 0.0274155, acc 1
2016-09-06T11:35:47.438046: step 11489, loss 0.0169979, acc 1
2016-09-06T11:35:48.251911: step 11490, loss 0.0338147, acc 0.96
2016-09-06T11:35:49.142736: step 11491, loss 0.0127432, acc 1
2016-09-06T11:35:50.006966: step 11492, loss 0.00220901, acc 1
2016-09-06T11:35:50.789316: step 11493, loss 0.00224136, acc 1
2016-09-06T11:35:51.663081: step 11494, loss 0.045695, acc 0.96
2016-09-06T11:35:52.480858: step 11495, loss 0.00422301, acc 1
2016-09-06T11:35:53.290249: step 11496, loss 0.0252158, acc 0.98
2016-09-06T11:35:54.125684: step 11497, loss 0.028479, acc 0.98
2016-09-06T11:35:54.950124: step 11498, loss 0.00299124, acc 1
2016-09-06T11:35:55.749461: step 11499, loss 0.0517489, acc 0.98
2016-09-06T11:35:56.582829: step 11500, loss 0.0292945, acc 0.98

Evaluation:
2016-09-06T11:36:00.333517: step 11500, loss 2.7391, acc 0.757036

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-11500

2016-09-06T11:36:02.264512: step 11501, loss 0.00724268, acc 1
2016-09-06T11:36:03.079967: step 11502, loss 0.0374792, acc 0.98
2016-09-06T11:36:03.890086: step 11503, loss 0.0199654, acc 0.98
2016-09-06T11:36:04.687931: step 11504, loss 0.00243216, acc 1
2016-09-06T11:36:05.499353: step 11505, loss 0.00253362, acc 1
2016-09-06T11:36:06.302172: step 11506, loss 0.0272646, acc 1
2016-09-06T11:36:07.120517: step 11507, loss 0.00512994, acc 1
2016-09-06T11:36:07.965423: step 11508, loss 0.0643014, acc 0.96
2016-09-06T11:36:08.809633: step 11509, loss 0.0280598, acc 0.98
2016-09-06T11:36:09.624775: step 11510, loss 0.00245741, acc 1
2016-09-06T11:36:10.423218: step 11511, loss 0.00277313, acc 1
2016-09-06T11:36:11.246830: step 11512, loss 0.00238861, acc 1
2016-09-06T11:36:12.070893: step 11513, loss 0.00262345, acc 1
2016-09-06T11:36:12.845401: step 11514, loss 0.0278397, acc 1
2016-09-06T11:36:13.668937: step 11515, loss 0.0162098, acc 1
2016-09-06T11:36:14.490831: step 11516, loss 0.00514477, acc 1
2016-09-06T11:36:15.270404: step 11517, loss 0.00378794, acc 1
2016-09-06T11:36:16.078274: step 11518, loss 0.0350576, acc 0.98
2016-09-06T11:36:16.893624: step 11519, loss 0.0197079, acc 1
2016-09-06T11:36:17.621533: step 11520, loss 0.00747333, acc 1
2016-09-06T11:36:18.455945: step 11521, loss 0.0304192, acc 0.98
2016-09-06T11:36:19.278197: step 11522, loss 0.134869, acc 0.96
2016-09-06T11:36:20.077748: step 11523, loss 0.0252723, acc 0.98
2016-09-06T11:36:20.921841: step 11524, loss 0.0108567, acc 1
2016-09-06T11:36:21.743722: step 11525, loss 0.0367902, acc 0.98
2016-09-06T11:36:22.545599: step 11526, loss 0.0400595, acc 0.98
2016-09-06T11:36:23.353754: step 11527, loss 0.00846504, acc 1
2016-09-06T11:36:24.172602: step 11528, loss 0.0598179, acc 0.98
2016-09-06T11:36:24.964421: step 11529, loss 0.00815663, acc 1
2016-09-06T11:36:25.778500: step 11530, loss 0.100282, acc 0.98
2016-09-06T11:36:26.604452: step 11531, loss 0.00789879, acc 1
2016-09-06T11:36:27.401464: step 11532, loss 0.00337637, acc 1
2016-09-06T11:36:28.202452: step 11533, loss 0.0148133, acc 1
2016-09-06T11:36:28.995096: step 11534, loss 0.038344, acc 0.98
2016-09-06T11:36:29.801205: step 11535, loss 0.0320139, acc 1
2016-09-06T11:36:30.638199: step 11536, loss 0.00646652, acc 1
2016-09-06T11:36:31.440649: step 11537, loss 0.0245293, acc 0.98
2016-09-06T11:36:32.242224: step 11538, loss 0.00201981, acc 1
2016-09-06T11:36:33.044810: step 11539, loss 0.021182, acc 0.98
2016-09-06T11:36:33.845108: step 11540, loss 0.0025555, acc 1
2016-09-06T11:36:34.611252: step 11541, loss 0.00206745, acc 1
2016-09-06T11:36:35.421873: step 11542, loss 0.00262471, acc 1
2016-09-06T11:36:36.284686: step 11543, loss 0.0020425, acc 1
2016-09-06T11:36:37.093111: step 11544, loss 0.0206536, acc 1
2016-09-06T11:36:37.909101: step 11545, loss 0.00206667, acc 1
2016-09-06T11:36:38.725646: step 11546, loss 0.00293296, acc 1
2016-09-06T11:36:39.531065: step 11547, loss 0.00828689, acc 1
2016-09-06T11:36:40.331527: step 11548, loss 0.0192459, acc 0.98
2016-09-06T11:36:41.147006: step 11549, loss 0.00764153, acc 1
2016-09-06T11:36:41.937193: step 11550, loss 0.0514173, acc 0.98
2016-09-06T11:36:42.727176: step 11551, loss 0.00266097, acc 1
2016-09-06T11:36:43.529920: step 11552, loss 0.00415598, acc 1
2016-09-06T11:36:44.327138: step 11553, loss 0.0835259, acc 0.98
2016-09-06T11:36:45.161021: step 11554, loss 0.0109557, acc 1
2016-09-06T11:36:45.976781: step 11555, loss 0.00310516, acc 1
2016-09-06T11:36:46.767056: step 11556, loss 0.00611022, acc 1
2016-09-06T11:36:47.578457: step 11557, loss 0.0378875, acc 0.96
2016-09-06T11:36:48.398721: step 11558, loss 0.0121873, acc 1
2016-09-06T11:36:49.222322: step 11559, loss 0.00225285, acc 1
2016-09-06T11:36:50.053419: step 11560, loss 0.00254895, acc 1
2016-09-06T11:36:50.877008: step 11561, loss 0.0521583, acc 0.98
2016-09-06T11:36:51.689143: step 11562, loss 0.00250996, acc 1
2016-09-06T11:36:52.506417: step 11563, loss 0.0256363, acc 0.98
2016-09-06T11:36:53.327349: step 11564, loss 0.00266861, acc 1
2016-09-06T11:36:54.128654: step 11565, loss 0.00783655, acc 1
2016-09-06T11:36:54.963400: step 11566, loss 0.00486489, acc 1
2016-09-06T11:36:55.803151: step 11567, loss 0.00424342, acc 1
2016-09-06T11:36:56.610411: step 11568, loss 0.0137044, acc 1
2016-09-06T11:36:57.405642: step 11569, loss 0.0195694, acc 1
2016-09-06T11:36:58.219077: step 11570, loss 0.00204391, acc 1
2016-09-06T11:36:59.014795: step 11571, loss 0.0338915, acc 0.98
2016-09-06T11:36:59.866636: step 11572, loss 0.0337849, acc 0.98
2016-09-06T11:37:00.728503: step 11573, loss 0.0326878, acc 1
2016-09-06T11:37:01.556530: step 11574, loss 0.00222129, acc 1
2016-09-06T11:37:02.386990: step 11575, loss 0.0123853, acc 1
2016-09-06T11:37:03.276194: step 11576, loss 0.00345459, acc 1
2016-09-06T11:37:04.105240: step 11577, loss 0.014887, acc 1
2016-09-06T11:37:04.901082: step 11578, loss 0.0271817, acc 0.98
2016-09-06T11:37:05.697960: step 11579, loss 0.0328824, acc 0.98
2016-09-06T11:37:06.504781: step 11580, loss 0.00276903, acc 1
2016-09-06T11:37:07.313079: step 11581, loss 0.00685847, acc 1
2016-09-06T11:37:08.127498: step 11582, loss 0.00277539, acc 1
2016-09-06T11:37:08.941276: step 11583, loss 0.0160836, acc 0.98
2016-09-06T11:37:09.732995: step 11584, loss 0.0156216, acc 1
2016-09-06T11:37:10.529056: step 11585, loss 0.0461791, acc 0.96
2016-09-06T11:37:11.346565: step 11586, loss 0.042356, acc 0.98
2016-09-06T11:37:12.196343: step 11587, loss 0.00641611, acc 1
2016-09-06T11:37:12.998231: step 11588, loss 0.0250585, acc 1
2016-09-06T11:37:13.836838: step 11589, loss 0.0626733, acc 0.98
2016-09-06T11:37:14.639414: step 11590, loss 0.0903191, acc 0.96
2016-09-06T11:37:15.454062: step 11591, loss 0.00423332, acc 1
2016-09-06T11:37:16.302113: step 11592, loss 0.0143828, acc 1
2016-09-06T11:37:17.128996: step 11593, loss 0.0240954, acc 1
2016-09-06T11:37:17.937525: step 11594, loss 0.00364234, acc 1
2016-09-06T11:37:18.778321: step 11595, loss 0.0442526, acc 0.98
2016-09-06T11:37:19.586048: step 11596, loss 0.0350546, acc 0.98
2016-09-06T11:37:20.409815: step 11597, loss 0.0186964, acc 0.98
2016-09-06T11:37:21.238063: step 11598, loss 0.00516468, acc 1
2016-09-06T11:37:22.042867: step 11599, loss 0.0127474, acc 1
2016-09-06T11:37:22.861261: step 11600, loss 0.016113, acc 1

Evaluation:
2016-09-06T11:37:26.610052: step 11600, loss 2.33724, acc 0.753283

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-11600

2016-09-06T11:37:28.490787: step 11601, loss 0.00643279, acc 1
2016-09-06T11:37:29.288786: step 11602, loss 0.0207565, acc 0.98
2016-09-06T11:37:30.108094: step 11603, loss 0.00986078, acc 1
2016-09-06T11:37:30.932520: step 11604, loss 0.00358062, acc 1
2016-09-06T11:37:31.744669: step 11605, loss 0.0249562, acc 0.98
2016-09-06T11:37:32.535602: step 11606, loss 0.00674107, acc 1
2016-09-06T11:37:33.355552: step 11607, loss 0.00342015, acc 1
2016-09-06T11:37:34.190844: step 11608, loss 0.0733828, acc 0.96
2016-09-06T11:37:35.013019: step 11609, loss 0.00475442, acc 1
2016-09-06T11:37:35.862922: step 11610, loss 0.00712255, acc 1
2016-09-06T11:37:36.682969: step 11611, loss 0.0187713, acc 0.98
2016-09-06T11:37:37.500641: step 11612, loss 0.00511865, acc 1
2016-09-06T11:37:38.338500: step 11613, loss 0.0127003, acc 1
2016-09-06T11:37:39.125949: step 11614, loss 0.159404, acc 0.98
2016-09-06T11:37:39.930700: step 11615, loss 0.0177239, acc 1
2016-09-06T11:37:40.776464: step 11616, loss 0.00477581, acc 1
2016-09-06T11:37:41.603496: step 11617, loss 0.00342, acc 1
2016-09-06T11:37:42.400807: step 11618, loss 0.0939074, acc 0.96
2016-09-06T11:37:43.239573: step 11619, loss 0.00416182, acc 1
2016-09-06T11:37:44.046967: step 11620, loss 0.0203025, acc 0.98
2016-09-06T11:37:44.853812: step 11621, loss 0.0379622, acc 1
2016-09-06T11:37:45.688164: step 11622, loss 0.0271615, acc 0.98
2016-09-06T11:37:46.510349: step 11623, loss 0.0112302, acc 1
2016-09-06T11:37:47.319950: step 11624, loss 0.00390589, acc 1
2016-09-06T11:37:48.180789: step 11625, loss 0.0475933, acc 0.98
2016-09-06T11:37:49.039206: step 11626, loss 0.0272911, acc 0.98
2016-09-06T11:37:49.846622: step 11627, loss 0.0172317, acc 1
2016-09-06T11:37:50.659635: step 11628, loss 0.0236892, acc 0.98
2016-09-06T11:37:51.484919: step 11629, loss 0.0218463, acc 0.98
2016-09-06T11:37:52.308128: step 11630, loss 0.136857, acc 0.94
2016-09-06T11:37:53.145505: step 11631, loss 0.0166075, acc 1
2016-09-06T11:37:53.962361: step 11632, loss 0.191282, acc 0.92
2016-09-06T11:37:54.754301: step 11633, loss 0.0273983, acc 1
2016-09-06T11:37:55.594952: step 11634, loss 0.00303807, acc 1
2016-09-06T11:37:56.422404: step 11635, loss 0.0132737, acc 1
2016-09-06T11:37:57.261944: step 11636, loss 0.0102637, acc 1
2016-09-06T11:37:58.096071: step 11637, loss 0.0081101, acc 1
2016-09-06T11:37:58.969410: step 11638, loss 0.00267945, acc 1
2016-09-06T11:37:59.797695: step 11639, loss 0.0083703, acc 1
2016-09-06T11:38:00.625441: step 11640, loss 0.0197616, acc 1
2016-09-06T11:38:01.441536: step 11641, loss 0.00369283, acc 1
2016-09-06T11:38:02.267567: step 11642, loss 0.00261528, acc 1
2016-09-06T11:38:03.068620: step 11643, loss 0.0241514, acc 0.98
2016-09-06T11:38:03.861741: step 11644, loss 0.0234886, acc 0.98
2016-09-06T11:38:04.669905: step 11645, loss 0.0166612, acc 1
2016-09-06T11:38:05.474356: step 11646, loss 0.0028072, acc 1
2016-09-06T11:38:06.299719: step 11647, loss 0.0185237, acc 0.98
2016-09-06T11:38:07.127085: step 11648, loss 0.0158767, acc 1
2016-09-06T11:38:07.917556: step 11649, loss 0.0239293, acc 0.98
2016-09-06T11:38:08.724258: step 11650, loss 0.0494552, acc 0.96
2016-09-06T11:38:09.553345: step 11651, loss 0.00845445, acc 1
2016-09-06T11:38:10.351673: step 11652, loss 0.0580984, acc 0.96
2016-09-06T11:38:11.146555: step 11653, loss 0.0362851, acc 0.98
2016-09-06T11:38:11.992130: step 11654, loss 0.0126811, acc 1
2016-09-06T11:38:12.769323: step 11655, loss 0.00527842, acc 1
2016-09-06T11:38:13.563991: step 11656, loss 0.0289638, acc 0.98
2016-09-06T11:38:14.383971: step 11657, loss 0.0137623, acc 1
2016-09-06T11:38:15.163270: step 11658, loss 0.0240946, acc 0.98
2016-09-06T11:38:15.982965: step 11659, loss 0.00669067, acc 1
2016-09-06T11:38:16.832902: step 11660, loss 0.00969856, acc 1
2016-09-06T11:38:17.622227: step 11661, loss 0.0247366, acc 0.98
2016-09-06T11:38:18.418853: step 11662, loss 0.0911783, acc 0.96
2016-09-06T11:38:19.224352: step 11663, loss 0.0225445, acc 1
2016-09-06T11:38:20.006019: step 11664, loss 0.00702591, acc 1
2016-09-06T11:38:20.843766: step 11665, loss 0.00321109, acc 1
2016-09-06T11:38:21.662621: step 11666, loss 0.00339676, acc 1
2016-09-06T11:38:22.454908: step 11667, loss 0.014782, acc 1
2016-09-06T11:38:23.259711: step 11668, loss 0.00309247, acc 1
2016-09-06T11:38:24.067731: step 11669, loss 0.00819078, acc 1
2016-09-06T11:38:24.841507: step 11670, loss 0.00945275, acc 1
2016-09-06T11:38:25.642830: step 11671, loss 0.00333319, acc 1
2016-09-06T11:38:26.457251: step 11672, loss 0.00250712, acc 1
2016-09-06T11:38:27.235684: step 11673, loss 0.00256091, acc 1
2016-09-06T11:38:28.042715: step 11674, loss 0.0808142, acc 0.98
2016-09-06T11:38:28.841475: step 11675, loss 0.0196529, acc 0.98
2016-09-06T11:38:29.641950: step 11676, loss 0.0313015, acc 0.98
2016-09-06T11:38:30.454157: step 11677, loss 0.0293898, acc 0.98
2016-09-06T11:38:31.288924: step 11678, loss 0.0176423, acc 0.98
2016-09-06T11:38:32.092637: step 11679, loss 0.0037018, acc 1
2016-09-06T11:38:32.891762: step 11680, loss 0.00529816, acc 1
2016-09-06T11:38:33.701982: step 11681, loss 0.026117, acc 1
2016-09-06T11:38:34.515141: step 11682, loss 0.00266211, acc 1
2016-09-06T11:38:35.344402: step 11683, loss 0.00901066, acc 1
2016-09-06T11:38:36.173266: step 11684, loss 0.0103531, acc 1
2016-09-06T11:38:36.965896: step 11685, loss 0.0219854, acc 0.98
2016-09-06T11:38:37.773528: step 11686, loss 0.0319651, acc 0.98
2016-09-06T11:38:38.583003: step 11687, loss 0.0318471, acc 1
2016-09-06T11:38:39.373894: step 11688, loss 0.108747, acc 0.98
2016-09-06T11:38:40.192162: step 11689, loss 0.0760268, acc 0.92
2016-09-06T11:38:41.081979: step 11690, loss 0.0643071, acc 0.98
2016-09-06T11:38:41.897515: step 11691, loss 0.0031089, acc 1
2016-09-06T11:38:42.700505: step 11692, loss 0.0111623, acc 1
2016-09-06T11:38:43.516030: step 11693, loss 0.00334731, acc 1
2016-09-06T11:38:44.316580: step 11694, loss 0.0202792, acc 0.98
2016-09-06T11:38:45.122131: step 11695, loss 0.0447664, acc 0.96
2016-09-06T11:38:45.932855: step 11696, loss 0.0732662, acc 0.98
2016-09-06T11:38:46.738496: step 11697, loss 0.0186872, acc 1
2016-09-06T11:38:47.550578: step 11698, loss 0.0432592, acc 0.96
2016-09-06T11:38:48.404625: step 11699, loss 0.0276069, acc 0.98
2016-09-06T11:38:49.235849: step 11700, loss 0.0176669, acc 1

Evaluation:
2016-09-06T11:38:52.971218: step 11700, loss 1.88036, acc 0.752345

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-11700

2016-09-06T11:38:54.846003: step 11701, loss 0.039552, acc 0.98
2016-09-06T11:38:55.706020: step 11702, loss 0.0190573, acc 1
2016-09-06T11:38:56.509021: step 11703, loss 0.0068691, acc 1
2016-09-06T11:38:57.307968: step 11704, loss 0.00841001, acc 1
2016-09-06T11:38:58.128558: step 11705, loss 0.00389373, acc 1
2016-09-06T11:38:58.961608: step 11706, loss 0.0170016, acc 0.98
2016-09-06T11:38:59.762558: step 11707, loss 0.0247618, acc 1
2016-09-06T11:39:00.601511: step 11708, loss 0.00295167, acc 1
2016-09-06T11:39:01.415932: step 11709, loss 0.0425266, acc 1
2016-09-06T11:39:02.255721: step 11710, loss 0.00475219, acc 1
2016-09-06T11:39:03.099300: step 11711, loss 0.00285221, acc 1
2016-09-06T11:39:03.863233: step 11712, loss 0.00367431, acc 1
2016-09-06T11:39:04.668917: step 11713, loss 0.00375679, acc 1
2016-09-06T11:39:05.509131: step 11714, loss 0.0157697, acc 1
2016-09-06T11:39:06.321216: step 11715, loss 0.0168259, acc 0.98
2016-09-06T11:39:07.123300: step 11716, loss 0.00486376, acc 1
2016-09-06T11:39:07.946588: step 11717, loss 0.0432049, acc 0.98
2016-09-06T11:39:08.772808: step 11718, loss 0.0474607, acc 0.98
2016-09-06T11:39:09.592900: step 11719, loss 0.00947741, acc 1
2016-09-06T11:39:10.420420: step 11720, loss 0.0163536, acc 1
2016-09-06T11:39:11.246620: step 11721, loss 0.0586645, acc 0.96
2016-09-06T11:39:12.020023: step 11722, loss 0.0258925, acc 0.98
2016-09-06T11:39:12.830930: step 11723, loss 0.0401384, acc 0.98
2016-09-06T11:39:13.641370: step 11724, loss 0.00450305, acc 1
2016-09-06T11:39:14.459674: step 11725, loss 0.0237936, acc 0.98
2016-09-06T11:39:15.267240: step 11726, loss 0.0291925, acc 0.98
2016-09-06T11:39:16.103701: step 11727, loss 0.0299626, acc 0.98
2016-09-06T11:39:16.901946: step 11728, loss 0.00795865, acc 1
2016-09-06T11:39:17.707469: step 11729, loss 0.012719, acc 1
2016-09-06T11:39:18.524480: step 11730, loss 0.0217108, acc 0.98
2016-09-06T11:39:19.335549: step 11731, loss 0.0138384, acc 1
2016-09-06T11:39:20.142100: step 11732, loss 0.00246128, acc 1
2016-09-06T11:39:20.947133: step 11733, loss 0.00491739, acc 1
2016-09-06T11:39:21.740549: step 11734, loss 0.0232638, acc 0.98
2016-09-06T11:39:22.545578: step 11735, loss 0.018224, acc 1
2016-09-06T11:39:23.354992: step 11736, loss 0.00293879, acc 1
2016-09-06T11:39:24.148528: step 11737, loss 0.00955638, acc 1
2016-09-06T11:39:24.951051: step 11738, loss 0.0256022, acc 0.98
2016-09-06T11:39:25.756878: step 11739, loss 0.00550778, acc 1
2016-09-06T11:39:26.559063: step 11740, loss 0.00241866, acc 1
2016-09-06T11:39:27.350579: step 11741, loss 0.00259654, acc 1
2016-09-06T11:39:28.159194: step 11742, loss 0.041164, acc 0.98
2016-09-06T11:39:28.985783: step 11743, loss 0.00234647, acc 1
2016-09-06T11:39:29.805492: step 11744, loss 0.0156685, acc 1
2016-09-06T11:39:30.607199: step 11745, loss 0.0220552, acc 1
2016-09-06T11:39:31.395309: step 11746, loss 0.0215835, acc 0.98
2016-09-06T11:39:32.199188: step 11747, loss 0.00445442, acc 1
2016-09-06T11:39:33.016465: step 11748, loss 0.00241699, acc 1
2016-09-06T11:39:33.797589: step 11749, loss 0.0163351, acc 1
2016-09-06T11:39:34.630852: step 11750, loss 0.0251729, acc 0.98
2016-09-06T11:39:35.447255: step 11751, loss 0.0196151, acc 1
2016-09-06T11:39:36.232302: step 11752, loss 0.00266498, acc 1
2016-09-06T11:39:37.049154: step 11753, loss 0.0135913, acc 1
2016-09-06T11:39:37.889433: step 11754, loss 0.00239797, acc 1
2016-09-06T11:39:38.663051: step 11755, loss 0.00241477, acc 1
2016-09-06T11:39:39.466872: step 11756, loss 0.0046428, acc 1
2016-09-06T11:39:40.302377: step 11757, loss 0.0039133, acc 1
2016-09-06T11:39:41.081397: step 11758, loss 0.00323458, acc 1
2016-09-06T11:39:41.877002: step 11759, loss 0.00360155, acc 1
2016-09-06T11:39:42.706543: step 11760, loss 0.0207428, acc 1
2016-09-06T11:39:43.530117: step 11761, loss 0.0196537, acc 0.98
2016-09-06T11:39:44.304280: step 11762, loss 0.00834355, acc 1
2016-09-06T11:39:45.142790: step 11763, loss 0.0025358, acc 1
2016-09-06T11:39:45.931089: step 11764, loss 0.0164075, acc 1
2016-09-06T11:39:46.753368: step 11765, loss 0.0178888, acc 0.98
2016-09-06T11:39:47.581909: step 11766, loss 0.0316138, acc 0.98
2016-09-06T11:39:48.350591: step 11767, loss 0.0499368, acc 0.98
2016-09-06T11:39:49.182866: step 11768, loss 0.0392797, acc 0.98
2016-09-06T11:39:49.992999: step 11769, loss 0.00294438, acc 1
2016-09-06T11:39:50.775483: step 11770, loss 0.0241154, acc 0.98
2016-09-06T11:39:51.568735: step 11771, loss 0.0335276, acc 0.98
2016-09-06T11:39:52.425931: step 11772, loss 0.0209574, acc 0.98
2016-09-06T11:39:53.216170: step 11773, loss 0.00775914, acc 1
2016-09-06T11:39:54.027668: step 11774, loss 0.00394373, acc 1
2016-09-06T11:39:54.866482: step 11775, loss 0.00226951, acc 1
2016-09-06T11:39:55.655878: step 11776, loss 0.0146491, acc 1
2016-09-06T11:39:56.455563: step 11777, loss 0.0178655, acc 0.98
2016-09-06T11:39:57.279586: step 11778, loss 0.00339436, acc 1
2016-09-06T11:39:58.079759: step 11779, loss 0.0155037, acc 1
2016-09-06T11:39:58.923427: step 11780, loss 0.0225068, acc 0.98
2016-09-06T11:39:59.765577: step 11781, loss 0.0866387, acc 0.98
2016-09-06T11:40:00.601364: step 11782, loss 0.014014, acc 1
2016-09-06T11:40:01.400628: step 11783, loss 0.00398819, acc 1
2016-09-06T11:40:02.211886: step 11784, loss 0.0202906, acc 0.98
2016-09-06T11:40:03.018085: step 11785, loss 0.00422801, acc 1
2016-09-06T11:40:03.822035: step 11786, loss 0.00226156, acc 1
2016-09-06T11:40:04.651675: step 11787, loss 0.0299673, acc 0.98
2016-09-06T11:40:05.469273: step 11788, loss 0.00563713, acc 1
2016-09-06T11:40:06.281670: step 11789, loss 0.00218356, acc 1
2016-09-06T11:40:07.133514: step 11790, loss 0.00516668, acc 1
2016-09-06T11:40:07.943937: step 11791, loss 0.0124184, acc 1
2016-09-06T11:40:08.781099: step 11792, loss 0.0175867, acc 1
2016-09-06T11:40:09.613727: step 11793, loss 0.002138, acc 1
2016-09-06T11:40:10.455408: step 11794, loss 0.00275396, acc 1
2016-09-06T11:40:11.277507: step 11795, loss 0.0195148, acc 1
2016-09-06T11:40:12.098389: step 11796, loss 0.00213684, acc 1
2016-09-06T11:40:12.908702: step 11797, loss 0.00333967, acc 1
2016-09-06T11:40:13.710838: step 11798, loss 0.0128311, acc 1
2016-09-06T11:40:14.520878: step 11799, loss 0.0184452, acc 0.98
2016-09-06T11:40:15.339093: step 11800, loss 0.00737088, acc 1

Evaluation:
2016-09-06T11:40:19.048178: step 11800, loss 2.41733, acc 0.752345

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-11800

2016-09-06T11:40:20.891751: step 11801, loss 0.0156364, acc 1
2016-09-06T11:40:21.704160: step 11802, loss 0.00208791, acc 1
2016-09-06T11:40:22.522083: step 11803, loss 0.0258584, acc 0.98
2016-09-06T11:40:23.338872: step 11804, loss 0.00206652, acc 1
2016-09-06T11:40:24.184777: step 11805, loss 0.025264, acc 1
2016-09-06T11:40:24.979117: step 11806, loss 0.0020697, acc 1
2016-09-06T11:40:25.772286: step 11807, loss 0.0161017, acc 0.98
2016-09-06T11:40:26.580670: step 11808, loss 0.012694, acc 1
2016-09-06T11:40:27.399479: step 11809, loss 0.00231169, acc 1
2016-09-06T11:40:28.205668: step 11810, loss 0.0184158, acc 0.98
2016-09-06T11:40:29.049081: step 11811, loss 0.0148648, acc 1
2016-09-06T11:40:29.844394: step 11812, loss 0.0265815, acc 0.98
2016-09-06T11:40:30.677408: step 11813, loss 0.0154265, acc 1
2016-09-06T11:40:31.525070: step 11814, loss 0.00395885, acc 1
2016-09-06T11:40:32.317482: step 11815, loss 0.0027496, acc 1
2016-09-06T11:40:33.136962: step 11816, loss 0.00224453, acc 1
2016-09-06T11:40:33.983209: step 11817, loss 0.0201656, acc 0.98
2016-09-06T11:40:34.801644: step 11818, loss 0.00197285, acc 1
2016-09-06T11:40:35.594947: step 11819, loss 0.0359809, acc 0.98
2016-09-06T11:40:36.427254: step 11820, loss 0.0219715, acc 1
2016-09-06T11:40:37.238761: step 11821, loss 0.0206684, acc 1
2016-09-06T11:40:38.057439: step 11822, loss 0.0639523, acc 0.96
2016-09-06T11:40:38.839674: step 11823, loss 0.00210294, acc 1
2016-09-06T11:40:39.683090: step 11824, loss 0.0185635, acc 1
2016-09-06T11:40:40.445558: step 11825, loss 0.0256564, acc 1
2016-09-06T11:40:41.231832: step 11826, loss 0.0236605, acc 0.98
2016-09-06T11:40:42.059884: step 11827, loss 0.076129, acc 0.98
2016-09-06T11:40:42.852614: step 11828, loss 0.00196493, acc 1
2016-09-06T11:40:43.653612: step 11829, loss 0.00508418, acc 1
2016-09-06T11:40:44.504521: step 11830, loss 0.00584159, acc 1
2016-09-06T11:40:45.296150: step 11831, loss 0.0300924, acc 0.98
2016-09-06T11:40:46.110584: step 11832, loss 0.00400323, acc 1
2016-09-06T11:40:46.932016: step 11833, loss 0.00200679, acc 1
2016-09-06T11:40:47.702814: step 11834, loss 0.00456875, acc 1
2016-09-06T11:40:48.504379: step 11835, loss 0.00305665, acc 1
2016-09-06T11:40:49.388718: step 11836, loss 0.0179727, acc 0.98
2016-09-06T11:40:50.193763: step 11837, loss 0.0968497, acc 0.98
2016-09-06T11:40:50.999834: step 11838, loss 0.0043734, acc 1
2016-09-06T11:40:51.808587: step 11839, loss 0.016179, acc 1
2016-09-06T11:40:52.618379: step 11840, loss 0.00394795, acc 1
2016-09-06T11:40:53.445845: step 11841, loss 0.00206268, acc 1
2016-09-06T11:40:54.298141: step 11842, loss 0.0137283, acc 1
2016-09-06T11:40:55.122964: step 11843, loss 0.0246415, acc 0.98
2016-09-06T11:40:55.954213: step 11844, loss 0.00817109, acc 1
2016-09-06T11:40:56.795689: step 11845, loss 0.00458116, acc 1
2016-09-06T11:40:57.589442: step 11846, loss 0.0340787, acc 0.98
2016-09-06T11:40:58.395141: step 11847, loss 0.0587814, acc 0.96
2016-09-06T11:40:59.223815: step 11848, loss 0.00414879, acc 1
2016-09-06T11:41:00.032598: step 11849, loss 0.00178556, acc 1
2016-09-06T11:41:00.863264: step 11850, loss 0.0160183, acc 1
2016-09-06T11:41:01.697114: step 11851, loss 0.00191558, acc 1
2016-09-06T11:41:02.498720: step 11852, loss 0.00218587, acc 1
2016-09-06T11:41:03.318585: step 11853, loss 0.00834394, acc 1
2016-09-06T11:41:04.127848: step 11854, loss 0.00201072, acc 1
2016-09-06T11:41:04.950274: step 11855, loss 0.00412469, acc 1
2016-09-06T11:41:05.753716: step 11856, loss 0.037196, acc 0.98
2016-09-06T11:41:06.589179: step 11857, loss 0.00338998, acc 1
2016-09-06T11:41:07.387047: step 11858, loss 0.00181043, acc 1
2016-09-06T11:41:08.188356: step 11859, loss 0.0550655, acc 0.98
2016-09-06T11:41:09.006780: step 11860, loss 0.0176559, acc 0.98
2016-09-06T11:41:09.832460: step 11861, loss 0.00186755, acc 1
2016-09-06T11:41:10.654820: step 11862, loss 0.00909583, acc 1
2016-09-06T11:41:11.446175: step 11863, loss 0.00164114, acc 1
2016-09-06T11:41:12.262939: step 11864, loss 0.0042145, acc 1
2016-09-06T11:41:13.048709: step 11865, loss 0.0211039, acc 0.98
2016-09-06T11:41:13.845851: step 11866, loss 0.0221112, acc 0.98
2016-09-06T11:41:14.652332: step 11867, loss 0.0124915, acc 1
2016-09-06T11:41:15.453449: step 11868, loss 0.0483651, acc 0.98
2016-09-06T11:41:16.266378: step 11869, loss 0.0226397, acc 0.98
2016-09-06T11:41:17.064729: step 11870, loss 0.0315632, acc 0.98
2016-09-06T11:41:17.858676: step 11871, loss 0.00392845, acc 1
2016-09-06T11:41:18.688911: step 11872, loss 0.0226402, acc 0.98
2016-09-06T11:41:19.510182: step 11873, loss 0.020743, acc 0.98
2016-09-06T11:41:20.301796: step 11874, loss 0.00398442, acc 1
2016-09-06T11:41:21.111744: step 11875, loss 0.0755145, acc 0.98
2016-09-06T11:41:21.913433: step 11876, loss 0.0126953, acc 1
2016-09-06T11:41:22.727684: step 11877, loss 0.0150664, acc 1
2016-09-06T11:41:23.537160: step 11878, loss 0.0266891, acc 1
2016-09-06T11:41:24.378898: step 11879, loss 0.00701881, acc 1
2016-09-06T11:41:25.181704: step 11880, loss 0.00375073, acc 1
2016-09-06T11:41:25.977680: step 11881, loss 0.0244705, acc 1
2016-09-06T11:41:26.814632: step 11882, loss 0.00474844, acc 1
2016-09-06T11:41:27.593433: step 11883, loss 0.0189134, acc 1
2016-09-06T11:41:28.406843: step 11884, loss 0.0125037, acc 1
2016-09-06T11:41:29.223186: step 11885, loss 0.0179012, acc 1
2016-09-06T11:41:30.004727: step 11886, loss 0.00521449, acc 1
2016-09-06T11:41:30.869258: step 11887, loss 0.024521, acc 1
2016-09-06T11:41:31.697002: step 11888, loss 0.00241169, acc 1
2016-09-06T11:41:32.477699: step 11889, loss 0.00340973, acc 1
2016-09-06T11:41:33.275616: step 11890, loss 0.0430483, acc 1
2016-09-06T11:41:34.083368: step 11891, loss 0.0181121, acc 0.98
2016-09-06T11:41:34.874722: step 11892, loss 0.0456139, acc 0.98
2016-09-06T11:41:35.688001: step 11893, loss 0.0140853, acc 1
2016-09-06T11:41:36.522448: step 11894, loss 0.00183128, acc 1
2016-09-06T11:41:37.341764: step 11895, loss 0.00183246, acc 1
2016-09-06T11:41:38.147922: step 11896, loss 0.00741165, acc 1
2016-09-06T11:41:39.005688: step 11897, loss 0.00223866, acc 1
2016-09-06T11:41:39.819687: step 11898, loss 0.0129652, acc 1
2016-09-06T11:41:40.634288: step 11899, loss 0.0317427, acc 0.98
2016-09-06T11:41:41.457284: step 11900, loss 0.00191257, acc 1

Evaluation:
2016-09-06T11:41:45.212794: step 11900, loss 2.55872, acc 0.746717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-11900

2016-09-06T11:41:47.173359: step 11901, loss 0.105401, acc 0.98
2016-09-06T11:41:47.982566: step 11902, loss 0.00706226, acc 1
2016-09-06T11:41:48.815381: step 11903, loss 0.00248929, acc 1
2016-09-06T11:41:49.597393: step 11904, loss 0.00180405, acc 1
2016-09-06T11:41:50.419029: step 11905, loss 0.0662045, acc 0.98
2016-09-06T11:41:51.243649: step 11906, loss 0.0420384, acc 0.98
2016-09-06T11:41:52.069339: step 11907, loss 0.00187356, acc 1
2016-09-06T11:41:52.913917: step 11908, loss 0.0146711, acc 1
2016-09-06T11:41:53.734251: step 11909, loss 0.0349159, acc 0.98
2016-09-06T11:41:54.536533: step 11910, loss 0.0213056, acc 0.98
2016-09-06T11:41:55.341874: step 11911, loss 0.0725889, acc 0.98
2016-09-06T11:41:56.156565: step 11912, loss 0.00176583, acc 1
2016-09-06T11:41:56.965219: step 11913, loss 0.0208654, acc 0.98
2016-09-06T11:41:57.810921: step 11914, loss 0.0321871, acc 0.98
2016-09-06T11:41:58.620974: step 11915, loss 0.00554973, acc 1
2016-09-06T11:41:59.431112: step 11916, loss 0.00289489, acc 1
2016-09-06T11:42:00.214926: step 11917, loss 0.00921161, acc 1
2016-09-06T11:42:01.038972: step 11918, loss 0.0256551, acc 0.98
2016-09-06T11:42:01.852170: step 11919, loss 0.0328825, acc 0.98
2016-09-06T11:42:02.657794: step 11920, loss 0.00607856, acc 1
2016-09-06T11:42:03.493610: step 11921, loss 0.00776115, acc 1
2016-09-06T11:42:04.317073: step 11922, loss 0.019599, acc 0.98
2016-09-06T11:42:05.107274: step 11923, loss 0.0146422, acc 1
2016-09-06T11:42:05.901504: step 11924, loss 0.00325807, acc 1
2016-09-06T11:42:06.737982: step 11925, loss 0.00166816, acc 1
2016-09-06T11:42:07.534605: step 11926, loss 0.00348809, acc 1
2016-09-06T11:42:08.336001: step 11927, loss 0.00170671, acc 1
2016-09-06T11:42:09.141760: step 11928, loss 0.0182707, acc 1
2016-09-06T11:42:09.954648: step 11929, loss 0.0143517, acc 1
2016-09-06T11:42:10.754132: step 11930, loss 0.0467638, acc 0.98
2016-09-06T11:42:11.571390: step 11931, loss 0.00342033, acc 1
2016-09-06T11:42:12.324796: step 11932, loss 0.00164517, acc 1
2016-09-06T11:42:13.130420: step 11933, loss 0.00508878, acc 1
2016-09-06T11:42:13.942537: step 11934, loss 0.015911, acc 1
2016-09-06T11:42:14.711637: step 11935, loss 0.0017156, acc 1
2016-09-06T11:42:15.524839: step 11936, loss 0.0060478, acc 1
2016-09-06T11:42:16.356462: step 11937, loss 0.00178654, acc 1
2016-09-06T11:42:17.177401: step 11938, loss 0.0123269, acc 1
2016-09-06T11:42:17.950914: step 11939, loss 0.0020301, acc 1
2016-09-06T11:42:18.789432: step 11940, loss 0.00207066, acc 1
2016-09-06T11:42:19.582673: step 11941, loss 0.00936284, acc 1
2016-09-06T11:42:20.381857: step 11942, loss 0.00409738, acc 1
2016-09-06T11:42:21.223329: step 11943, loss 0.00441155, acc 1
2016-09-06T11:42:21.986240: step 11944, loss 0.00189318, acc 1
2016-09-06T11:42:22.805461: step 11945, loss 0.0278122, acc 0.98
2016-09-06T11:42:23.621433: step 11946, loss 0.00203721, acc 1
2016-09-06T11:42:24.391339: step 11947, loss 0.0414826, acc 0.98
2016-09-06T11:42:25.201449: step 11948, loss 0.00188919, acc 1
2016-09-06T11:42:26.048514: step 11949, loss 0.00188927, acc 1
2016-09-06T11:42:26.830280: step 11950, loss 0.0498473, acc 0.98
2016-09-06T11:42:27.639572: step 11951, loss 0.00792064, acc 1
2016-09-06T11:42:28.464864: step 11952, loss 0.0371062, acc 0.98
2016-09-06T11:42:29.236513: step 11953, loss 0.0305546, acc 0.98
2016-09-06T11:42:30.048607: step 11954, loss 0.00169024, acc 1
2016-09-06T11:42:30.876916: step 11955, loss 0.101394, acc 0.94
2016-09-06T11:42:31.648190: step 11956, loss 0.0353082, acc 1
2016-09-06T11:42:32.490371: step 11957, loss 0.00891052, acc 1
2016-09-06T11:42:33.287077: step 11958, loss 0.00730471, acc 1
2016-09-06T11:42:34.064888: step 11959, loss 0.00309713, acc 1
2016-09-06T11:42:34.862440: step 11960, loss 0.00182393, acc 1
2016-09-06T11:42:35.674815: step 11961, loss 0.00532243, acc 1
2016-09-06T11:42:36.456137: step 11962, loss 0.0154977, acc 1
2016-09-06T11:42:37.265032: step 11963, loss 0.0253649, acc 0.98
2016-09-06T11:42:38.085646: step 11964, loss 0.00900966, acc 1
2016-09-06T11:42:38.890087: step 11965, loss 0.0192002, acc 1
2016-09-06T11:42:39.702866: step 11966, loss 0.00452316, acc 1
2016-09-06T11:42:40.504035: step 11967, loss 0.00237697, acc 1
2016-09-06T11:42:41.305824: step 11968, loss 0.0089466, acc 1
2016-09-06T11:42:42.113509: step 11969, loss 0.00704453, acc 1
2016-09-06T11:42:42.970554: step 11970, loss 0.0428708, acc 0.98
2016-09-06T11:42:43.747638: step 11971, loss 0.0225478, acc 0.98
2016-09-06T11:42:44.573660: step 11972, loss 0.0121729, acc 1
2016-09-06T11:42:45.404687: step 11973, loss 0.0183068, acc 0.98
2016-09-06T11:42:46.203349: step 11974, loss 0.0315493, acc 0.98
2016-09-06T11:42:46.996221: step 11975, loss 0.00501632, acc 1
2016-09-06T11:42:47.809129: step 11976, loss 0.00246291, acc 1
2016-09-06T11:42:48.574704: step 11977, loss 0.00624428, acc 1
2016-09-06T11:42:49.366632: step 11978, loss 0.0061742, acc 1
2016-09-06T11:42:50.173404: step 11979, loss 0.0274621, acc 0.98
2016-09-06T11:42:50.974165: step 11980, loss 0.0156669, acc 1
2016-09-06T11:42:51.779303: step 11981, loss 0.00226055, acc 1
2016-09-06T11:42:52.621634: step 11982, loss 0.0188557, acc 1
2016-09-06T11:42:53.411037: step 11983, loss 0.00226093, acc 1
2016-09-06T11:42:54.243192: step 11984, loss 0.00333713, acc 1
2016-09-06T11:42:55.049031: step 11985, loss 0.00342245, acc 1
2016-09-06T11:42:55.845612: step 11986, loss 0.00247342, acc 1
2016-09-06T11:42:56.632116: step 11987, loss 0.0398136, acc 0.96
2016-09-06T11:42:57.468773: step 11988, loss 0.0962386, acc 0.98
2016-09-06T11:42:58.235896: step 11989, loss 0.0222203, acc 0.98
2016-09-06T11:42:59.027778: step 11990, loss 0.0115305, acc 1
2016-09-06T11:42:59.838556: step 11991, loss 0.00218421, acc 1
2016-09-06T11:43:00.626937: step 11992, loss 0.0248935, acc 0.98
2016-09-06T11:43:01.421412: step 11993, loss 0.0200265, acc 1
2016-09-06T11:43:02.229647: step 11994, loss 0.00483718, acc 1
2016-09-06T11:43:03.097109: step 11995, loss 0.0134503, acc 1
2016-09-06T11:43:03.907728: step 11996, loss 0.00217112, acc 1
2016-09-06T11:43:04.701654: step 11997, loss 0.0127221, acc 1
2016-09-06T11:43:05.480792: step 11998, loss 0.00241465, acc 1
2016-09-06T11:43:06.272671: step 11999, loss 0.0201381, acc 0.98
2016-09-06T11:43:07.134821: step 12000, loss 0.00243556, acc 1

Evaluation:
2016-09-06T11:43:10.885377: step 12000, loss 2.79405, acc 0.742964

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-12000

2016-09-06T11:43:12.796234: step 12001, loss 0.00233212, acc 1
2016-09-06T11:43:13.643087: step 12002, loss 0.105733, acc 0.96
2016-09-06T11:43:14.475438: step 12003, loss 0.0147296, acc 1
2016-09-06T11:43:15.271239: step 12004, loss 0.012117, acc 1
2016-09-06T11:43:16.078493: step 12005, loss 0.00885399, acc 1
2016-09-06T11:43:16.897476: step 12006, loss 0.00248574, acc 1
2016-09-06T11:43:17.685601: step 12007, loss 0.020155, acc 1
2016-09-06T11:43:18.468711: step 12008, loss 0.00230402, acc 1
2016-09-06T11:43:19.273334: step 12009, loss 0.0936011, acc 0.96
2016-09-06T11:43:20.073799: step 12010, loss 0.00349689, acc 1
2016-09-06T11:43:20.885923: step 12011, loss 0.0219944, acc 0.98
2016-09-06T11:43:21.716521: step 12012, loss 0.0204885, acc 0.98
2016-09-06T11:43:22.523099: step 12013, loss 0.0213511, acc 1
2016-09-06T11:43:23.364166: step 12014, loss 0.00998415, acc 1
2016-09-06T11:43:24.189763: step 12015, loss 0.00326648, acc 1
2016-09-06T11:43:25.017958: step 12016, loss 0.0240808, acc 0.98
2016-09-06T11:43:25.821457: step 12017, loss 0.00868308, acc 1
2016-09-06T11:43:26.659021: step 12018, loss 0.00206792, acc 1
2016-09-06T11:43:27.478874: step 12019, loss 0.0530948, acc 0.96
2016-09-06T11:43:28.284992: step 12020, loss 0.00976667, acc 1
2016-09-06T11:43:29.143861: step 12021, loss 0.00627379, acc 1
2016-09-06T11:43:29.933439: step 12022, loss 0.00214646, acc 1
2016-09-06T11:43:30.745383: step 12023, loss 0.00475684, acc 1
2016-09-06T11:43:31.550831: step 12024, loss 0.00891602, acc 1
2016-09-06T11:43:32.351994: step 12025, loss 0.00326858, acc 1
2016-09-06T11:43:33.166419: step 12026, loss 0.00453383, acc 1
2016-09-06T11:43:34.012040: step 12027, loss 0.0025899, acc 1
2016-09-06T11:43:34.808852: step 12028, loss 0.0203563, acc 0.98
2016-09-06T11:43:35.622951: step 12029, loss 0.0203002, acc 0.98
2016-09-06T11:43:36.465411: step 12030, loss 0.121505, acc 0.98
2016-09-06T11:43:37.247243: step 12031, loss 0.0253958, acc 1
2016-09-06T11:43:38.091038: step 12032, loss 0.00378283, acc 1
2016-09-06T11:43:38.922155: step 12033, loss 0.00831945, acc 1
2016-09-06T11:43:39.736572: step 12034, loss 0.0175219, acc 1
2016-09-06T11:43:40.541795: step 12035, loss 0.0345984, acc 0.98
2016-09-06T11:43:41.526038: step 12036, loss 0.0052366, acc 1
2016-09-06T11:43:42.316169: step 12037, loss 0.0258085, acc 0.98
2016-09-06T11:43:43.083029: step 12038, loss 0.00404269, acc 1
2016-09-06T11:43:43.923219: step 12039, loss 0.0147707, acc 1
2016-09-06T11:43:44.744050: step 12040, loss 0.0493489, acc 0.98
2016-09-06T11:43:45.546100: step 12041, loss 0.0138128, acc 1
2016-09-06T11:43:46.341632: step 12042, loss 0.0173436, acc 1
2016-09-06T11:43:47.155839: step 12043, loss 0.0116471, acc 1
2016-09-06T11:43:47.920707: step 12044, loss 0.00236588, acc 1
2016-09-06T11:43:48.715571: step 12045, loss 0.0395562, acc 0.96
2016-09-06T11:43:49.517401: step 12046, loss 0.018017, acc 0.98
2016-09-06T11:43:50.331814: step 12047, loss 0.0563826, acc 0.96
2016-09-06T11:43:51.130169: step 12048, loss 0.0105106, acc 1
2016-09-06T11:43:51.961464: step 12049, loss 0.00514405, acc 1
2016-09-06T11:43:52.745353: step 12050, loss 0.0301346, acc 1
2016-09-06T11:43:53.530903: step 12051, loss 0.00575191, acc 1
2016-09-06T11:43:54.350761: step 12052, loss 0.0446133, acc 0.98
2016-09-06T11:43:55.153226: step 12053, loss 0.0268825, acc 0.98
2016-09-06T11:43:55.965590: step 12054, loss 0.0179393, acc 0.98
2016-09-06T11:43:56.786790: step 12055, loss 0.00237881, acc 1
2016-09-06T11:43:57.584587: step 12056, loss 0.00653816, acc 1
2016-09-06T11:43:58.408699: step 12057, loss 0.00906863, acc 1
2016-09-06T11:43:59.233042: step 12058, loss 0.00246671, acc 1
2016-09-06T11:44:00.005622: step 12059, loss 0.037054, acc 0.96
2016-09-06T11:44:00.848224: step 12060, loss 0.00321342, acc 1
2016-09-06T11:44:01.665641: step 12061, loss 0.0257296, acc 0.98
2016-09-06T11:44:02.453317: step 12062, loss 0.0126741, acc 1
2016-09-06T11:44:03.246179: step 12063, loss 0.00932364, acc 1
2016-09-06T11:44:04.044384: step 12064, loss 0.0208509, acc 1
2016-09-06T11:44:04.849402: step 12065, loss 0.0609498, acc 0.96
2016-09-06T11:44:05.645490: step 12066, loss 0.00251579, acc 1
2016-09-06T11:44:06.487718: step 12067, loss 0.00275713, acc 1
2016-09-06T11:44:07.265952: step 12068, loss 0.0025099, acc 1
2016-09-06T11:44:08.042190: step 12069, loss 0.00323507, acc 1
2016-09-06T11:44:08.869034: step 12070, loss 0.0298752, acc 0.98
2016-09-06T11:44:09.643474: step 12071, loss 0.0144856, acc 1
2016-09-06T11:44:10.463228: step 12072, loss 0.0148163, acc 1
2016-09-06T11:44:11.327059: step 12073, loss 0.0545828, acc 0.96
2016-09-06T11:44:12.126317: step 12074, loss 0.00235486, acc 1
2016-09-06T11:44:12.951967: step 12075, loss 0.131632, acc 0.98
2016-09-06T11:44:13.797146: step 12076, loss 0.00264395, acc 1
2016-09-06T11:44:14.593151: step 12077, loss 0.0766671, acc 0.98
2016-09-06T11:44:15.411750: step 12078, loss 0.0194788, acc 1
2016-09-06T11:44:16.250081: step 12079, loss 0.0132787, acc 1
2016-09-06T11:44:17.067546: step 12080, loss 0.00595241, acc 1
2016-09-06T11:44:17.902053: step 12081, loss 0.0148639, acc 1
2016-09-06T11:44:18.746454: step 12082, loss 0.0431485, acc 1
2016-09-06T11:44:19.580752: step 12083, loss 0.0060785, acc 1
2016-09-06T11:44:20.397314: step 12084, loss 0.0429405, acc 0.98
2016-09-06T11:44:21.232222: step 12085, loss 0.0173397, acc 0.98
2016-09-06T11:44:22.043035: step 12086, loss 0.033933, acc 0.98
2016-09-06T11:44:22.878052: step 12087, loss 0.00272344, acc 1
2016-09-06T11:44:23.697756: step 12088, loss 0.00294503, acc 1
2016-09-06T11:44:24.500439: step 12089, loss 0.00286338, acc 1
2016-09-06T11:44:25.314362: step 12090, loss 0.0188596, acc 0.98
2016-09-06T11:44:26.132809: step 12091, loss 0.02837, acc 1
2016-09-06T11:44:26.981080: step 12092, loss 0.00249603, acc 1
2016-09-06T11:44:27.772827: step 12093, loss 0.0427701, acc 0.96
2016-09-06T11:44:28.560536: step 12094, loss 0.0321803, acc 0.96
2016-09-06T11:44:29.357225: step 12095, loss 0.00473896, acc 1
2016-09-06T11:44:30.071642: step 12096, loss 0.0023288, acc 1
2016-09-06T11:44:30.917651: step 12097, loss 0.0279413, acc 0.98
2016-09-06T11:44:31.753994: step 12098, loss 0.0133781, acc 1
2016-09-06T11:44:32.570199: step 12099, loss 0.036171, acc 0.96
2016-09-06T11:44:33.407631: step 12100, loss 0.0163923, acc 0.98

Evaluation:
2016-09-06T11:44:37.145921: step 12100, loss 2.23737, acc 0.741088

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-12100

2016-09-06T11:44:39.107270: step 12101, loss 0.0310119, acc 0.98
2016-09-06T11:44:39.944472: step 12102, loss 0.0207734, acc 1
2016-09-06T11:44:40.764670: step 12103, loss 0.00303512, acc 1
2016-09-06T11:44:41.583807: step 12104, loss 0.0225613, acc 0.98
2016-09-06T11:44:42.375781: step 12105, loss 0.0187289, acc 0.98
2016-09-06T11:44:43.183298: step 12106, loss 0.0390985, acc 0.96
2016-09-06T11:44:44.017984: step 12107, loss 0.00305993, acc 1
2016-09-06T11:44:44.830124: step 12108, loss 0.00206453, acc 1
2016-09-06T11:44:45.634334: step 12109, loss 0.0167998, acc 1
2016-09-06T11:44:46.474882: step 12110, loss 0.0611912, acc 0.98
2016-09-06T11:44:47.236973: step 12111, loss 0.016936, acc 0.98
2016-09-06T11:44:48.035906: step 12112, loss 0.00421858, acc 1
2016-09-06T11:44:48.849555: step 12113, loss 0.0457324, acc 0.98
2016-09-06T11:44:49.618334: step 12114, loss 0.00211217, acc 1
2016-09-06T11:44:50.476615: step 12115, loss 0.00440094, acc 1
2016-09-06T11:44:51.303670: step 12116, loss 0.0022075, acc 1
2016-09-06T11:44:52.100428: step 12117, loss 0.0217064, acc 0.98
2016-09-06T11:44:52.870185: step 12118, loss 0.00254673, acc 1
2016-09-06T11:44:53.706148: step 12119, loss 0.00530913, acc 1
2016-09-06T11:44:54.484541: step 12120, loss 0.00253751, acc 1
2016-09-06T11:44:55.301869: step 12121, loss 0.00981379, acc 1
2016-09-06T11:44:56.145214: step 12122, loss 0.0266957, acc 0.98
2016-09-06T11:44:56.919118: step 12123, loss 0.0093059, acc 1
2016-09-06T11:44:57.717461: step 12124, loss 0.0167233, acc 0.98
2016-09-06T11:44:58.553614: step 12125, loss 0.0117334, acc 1
2016-09-06T11:44:59.321216: step 12126, loss 0.00213924, acc 1
2016-09-06T11:45:00.101346: step 12127, loss 0.00249268, acc 1
2016-09-06T11:45:00.955453: step 12128, loss 0.00753024, acc 1
2016-09-06T11:45:01.736603: step 12129, loss 0.0248434, acc 0.98
2016-09-06T11:45:02.531545: step 12130, loss 0.00756067, acc 1
2016-09-06T11:45:03.329494: step 12131, loss 0.00219879, acc 1
2016-09-06T11:45:04.116094: step 12132, loss 0.00184683, acc 1
2016-09-06T11:45:04.926725: step 12133, loss 0.0190895, acc 0.98
2016-09-06T11:45:05.734970: step 12134, loss 0.0435431, acc 0.98
2016-09-06T11:45:06.558449: step 12135, loss 0.00343967, acc 1
2016-09-06T11:45:07.362283: step 12136, loss 0.00225049, acc 1
2016-09-06T11:45:08.192673: step 12137, loss 0.0159382, acc 0.98
2016-09-06T11:45:08.980250: step 12138, loss 0.00452662, acc 1
2016-09-06T11:45:09.805407: step 12139, loss 0.0364669, acc 0.98
2016-09-06T11:45:10.634124: step 12140, loss 0.00265944, acc 1
2016-09-06T11:45:11.434298: step 12141, loss 0.00316902, acc 1
2016-09-06T11:45:12.259474: step 12142, loss 0.0186548, acc 0.98
2016-09-06T11:45:13.084580: step 12143, loss 0.0127101, acc 1
2016-09-06T11:45:13.885628: step 12144, loss 0.00185986, acc 1
2016-09-06T11:45:14.722365: step 12145, loss 0.0278239, acc 0.98
2016-09-06T11:45:15.524382: step 12146, loss 0.00179083, acc 1
2016-09-06T11:45:16.334515: step 12147, loss 0.00240113, acc 1
2016-09-06T11:45:17.150598: step 12148, loss 0.0405169, acc 0.98
2016-09-06T11:45:17.997391: step 12149, loss 0.00230913, acc 1
2016-09-06T11:45:18.772153: step 12150, loss 0.00436969, acc 1
2016-09-06T11:45:19.607860: step 12151, loss 0.00677884, acc 1
2016-09-06T11:45:20.441273: step 12152, loss 0.0032539, acc 1
2016-09-06T11:45:21.257252: step 12153, loss 0.00188416, acc 1
2016-09-06T11:45:22.059534: step 12154, loss 0.0450418, acc 0.98
2016-09-06T11:45:22.884456: step 12155, loss 0.00209896, acc 1
2016-09-06T11:45:23.694861: step 12156, loss 0.00668705, acc 1
2016-09-06T11:45:24.497934: step 12157, loss 0.0632828, acc 0.96
2016-09-06T11:45:25.333396: step 12158, loss 0.00182901, acc 1
2016-09-06T11:45:26.136946: step 12159, loss 0.00185104, acc 1
2016-09-06T11:45:26.923502: step 12160, loss 0.00471772, acc 1
2016-09-06T11:45:27.756245: step 12161, loss 0.00256124, acc 1
2016-09-06T11:45:28.569598: step 12162, loss 0.00241873, acc 1
2016-09-06T11:45:29.388691: step 12163, loss 0.00412635, acc 1
2016-09-06T11:45:30.216071: step 12164, loss 0.0291734, acc 0.98
2016-09-06T11:45:31.015054: step 12165, loss 0.00821283, acc 1
2016-09-06T11:45:31.816450: step 12166, loss 0.018803, acc 0.98
2016-09-06T11:45:32.647615: step 12167, loss 0.00188256, acc 1
2016-09-06T11:45:33.453210: step 12168, loss 0.0107487, acc 1
2016-09-06T11:45:34.243247: step 12169, loss 0.0097377, acc 1
2016-09-06T11:45:35.056077: step 12170, loss 0.0022144, acc 1
2016-09-06T11:45:35.863665: step 12171, loss 0.0307447, acc 0.98
2016-09-06T11:45:36.678725: step 12172, loss 0.0155104, acc 1
2016-09-06T11:45:37.536576: step 12173, loss 0.00224374, acc 1
2016-09-06T11:45:38.340876: step 12174, loss 0.0734733, acc 0.98
2016-09-06T11:45:39.160699: step 12175, loss 0.00224903, acc 1
2016-09-06T11:45:40.002521: step 12176, loss 0.00547298, acc 1
2016-09-06T11:45:40.804356: step 12177, loss 0.00190208, acc 1
2016-09-06T11:45:41.597879: step 12178, loss 0.011388, acc 1
2016-09-06T11:45:42.427342: step 12179, loss 0.042798, acc 0.98
2016-09-06T11:45:43.254286: step 12180, loss 0.0138491, acc 1
2016-09-06T11:45:44.042391: step 12181, loss 0.0038984, acc 1
2016-09-06T11:45:44.852538: step 12182, loss 0.08747, acc 0.98
2016-09-06T11:45:45.673399: step 12183, loss 0.00636028, acc 1
2016-09-06T11:45:46.453232: step 12184, loss 0.0577468, acc 0.98
2016-09-06T11:45:47.265858: step 12185, loss 0.0459688, acc 0.98
2016-09-06T11:45:48.075215: step 12186, loss 0.00702391, acc 1
2016-09-06T11:45:48.882915: step 12187, loss 0.0254778, acc 1
2016-09-06T11:45:49.712381: step 12188, loss 0.00535238, acc 1
2016-09-06T11:45:50.530103: step 12189, loss 0.0500574, acc 0.98
2016-09-06T11:45:51.327976: step 12190, loss 0.060313, acc 0.98
2016-09-06T11:45:52.123548: step 12191, loss 0.0195688, acc 1
2016-09-06T11:45:52.947924: step 12192, loss 0.0145884, acc 1
2016-09-06T11:45:53.756032: step 12193, loss 0.023706, acc 0.98
2016-09-06T11:45:54.557495: step 12194, loss 0.0215144, acc 0.98
2016-09-06T11:45:55.369346: step 12195, loss 0.00235925, acc 1
2016-09-06T11:45:56.141852: step 12196, loss 0.00505118, acc 1
2016-09-06T11:45:56.955554: step 12197, loss 0.0194365, acc 1
2016-09-06T11:45:57.767437: step 12198, loss 0.012884, acc 1
2016-09-06T11:45:58.537234: step 12199, loss 0.00312146, acc 1
2016-09-06T11:45:59.343113: step 12200, loss 0.00286289, acc 1

Evaluation:
2016-09-06T11:46:03.117352: step 12200, loss 2.49404, acc 0.742026

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-12200

2016-09-06T11:46:04.985683: step 12201, loss 0.0997941, acc 0.94
2016-09-06T11:46:05.801748: step 12202, loss 0.0249727, acc 1
2016-09-06T11:46:06.622067: step 12203, loss 0.0270707, acc 1
2016-09-06T11:46:07.444944: step 12204, loss 0.018024, acc 0.98
2016-09-06T11:46:08.264778: step 12205, loss 0.0225777, acc 1
2016-09-06T11:46:09.079694: step 12206, loss 0.0360972, acc 0.98
2016-09-06T11:46:09.921533: step 12207, loss 0.00521089, acc 1
2016-09-06T11:46:10.777791: step 12208, loss 0.0137164, acc 1
2016-09-06T11:46:11.604293: step 12209, loss 0.103152, acc 0.98
2016-09-06T11:46:12.450810: step 12210, loss 0.0176253, acc 1
2016-09-06T11:46:13.284149: step 12211, loss 0.0289412, acc 0.98
2016-09-06T11:46:14.092666: step 12212, loss 0.00404991, acc 1
2016-09-06T11:46:14.947506: step 12213, loss 0.043622, acc 0.98
2016-09-06T11:46:15.760351: step 12214, loss 0.00498032, acc 1
2016-09-06T11:46:16.563208: step 12215, loss 0.00538002, acc 1
2016-09-06T11:46:17.459604: step 12216, loss 0.0181028, acc 0.98
2016-09-06T11:46:18.304596: step 12217, loss 0.00382817, acc 1
2016-09-06T11:46:19.104981: step 12218, loss 0.00361075, acc 1
2016-09-06T11:46:20.011842: step 12219, loss 0.027907, acc 1
2016-09-06T11:46:20.881223: step 12220, loss 0.0043344, acc 1
2016-09-06T11:46:21.680846: step 12221, loss 0.0220526, acc 1
2016-09-06T11:46:22.486059: step 12222, loss 0.00723855, acc 1
2016-09-06T11:46:23.329653: step 12223, loss 0.0186599, acc 0.98
2016-09-06T11:46:24.165584: step 12224, loss 0.017394, acc 1
2016-09-06T11:46:24.999127: step 12225, loss 0.00997747, acc 1
2016-09-06T11:46:25.824466: step 12226, loss 0.0142499, acc 1
2016-09-06T11:46:26.641102: step 12227, loss 0.00349714, acc 1
2016-09-06T11:46:27.457275: step 12228, loss 0.0102501, acc 1
2016-09-06T11:46:28.296701: step 12229, loss 0.027278, acc 0.98
2016-09-06T11:46:29.110490: step 12230, loss 0.0129303, acc 1
2016-09-06T11:46:29.913726: step 12231, loss 0.0178826, acc 0.98
2016-09-06T11:46:30.737391: step 12232, loss 0.0145685, acc 1
2016-09-06T11:46:31.588332: step 12233, loss 0.0043483, acc 1
2016-09-06T11:46:32.394372: step 12234, loss 0.0110633, acc 1
2016-09-06T11:46:33.246029: step 12235, loss 0.00337316, acc 1
2016-09-06T11:46:34.170922: step 12236, loss 0.00335847, acc 1
2016-09-06T11:46:35.025415: step 12237, loss 0.00851198, acc 1
2016-09-06T11:46:35.812441: step 12238, loss 0.00700999, acc 1
2016-09-06T11:46:36.645452: step 12239, loss 0.0169167, acc 1
2016-09-06T11:46:37.457635: step 12240, loss 0.0362005, acc 0.98
2016-09-06T11:46:38.259087: step 12241, loss 0.00329669, acc 1
2016-09-06T11:46:39.081339: step 12242, loss 0.00329552, acc 1
2016-09-06T11:46:39.895061: step 12243, loss 0.0137026, acc 1
2016-09-06T11:46:40.701272: step 12244, loss 0.0044505, acc 1
2016-09-06T11:46:41.520382: step 12245, loss 0.0163564, acc 1
2016-09-06T11:46:42.324820: step 12246, loss 0.0155403, acc 1
2016-09-06T11:46:43.156866: step 12247, loss 0.00347292, acc 1
2016-09-06T11:46:44.012206: step 12248, loss 0.00316437, acc 1
2016-09-06T11:46:44.830388: step 12249, loss 0.00312887, acc 1
2016-09-06T11:46:45.614432: step 12250, loss 0.00310259, acc 1
2016-09-06T11:46:46.435732: step 12251, loss 0.00353934, acc 1
2016-09-06T11:46:47.250478: step 12252, loss 0.00378932, acc 1
2016-09-06T11:46:48.058430: step 12253, loss 0.0140103, acc 1
2016-09-06T11:46:48.891616: step 12254, loss 0.019572, acc 0.98
2016-09-06T11:46:49.706655: step 12255, loss 0.0029671, acc 1
2016-09-06T11:46:50.504962: step 12256, loss 0.0291798, acc 0.98
2016-09-06T11:46:51.311030: step 12257, loss 0.0114467, acc 1
2016-09-06T11:46:52.132078: step 12258, loss 0.0143284, acc 1
2016-09-06T11:46:52.939805: step 12259, loss 0.00301091, acc 1
2016-09-06T11:46:53.746308: step 12260, loss 0.00290919, acc 1
2016-09-06T11:46:54.575687: step 12261, loss 0.0180573, acc 0.98
2016-09-06T11:46:55.379525: step 12262, loss 0.0114105, acc 1
2016-09-06T11:46:56.157759: step 12263, loss 0.00587666, acc 1
2016-09-06T11:46:57.019021: step 12264, loss 0.00293002, acc 1
2016-09-06T11:46:57.817518: step 12265, loss 0.05581, acc 0.96
2016-09-06T11:46:58.623115: step 12266, loss 0.021228, acc 1
2016-09-06T11:46:59.458390: step 12267, loss 0.00392881, acc 1
2016-09-06T11:47:00.294562: step 12268, loss 0.11779, acc 0.98
2016-09-06T11:47:01.083226: step 12269, loss 0.00640617, acc 1
2016-09-06T11:47:01.941893: step 12270, loss 0.01633, acc 1
2016-09-06T11:47:02.764365: step 12271, loss 0.140512, acc 0.96
2016-09-06T11:47:03.568531: step 12272, loss 0.00262709, acc 1
2016-09-06T11:47:04.399292: step 12273, loss 0.012569, acc 1
2016-09-06T11:47:05.242206: step 12274, loss 0.02489, acc 0.98
2016-09-06T11:47:06.075469: step 12275, loss 0.0170601, acc 0.98
2016-09-06T11:47:06.898224: step 12276, loss 0.0289985, acc 1
2016-09-06T11:47:07.707263: step 12277, loss 0.00487218, acc 1
2016-09-06T11:47:08.528551: step 12278, loss 0.0206828, acc 1
2016-09-06T11:47:09.349061: step 12279, loss 0.00224385, acc 1
2016-09-06T11:47:10.165002: step 12280, loss 0.00214325, acc 1
2016-09-06T11:47:11.005754: step 12281, loss 0.00215694, acc 1
2016-09-06T11:47:11.843563: step 12282, loss 0.0051988, acc 1
2016-09-06T11:47:12.660328: step 12283, loss 0.0176616, acc 1
2016-09-06T11:47:13.457409: step 12284, loss 0.00231158, acc 1
2016-09-06T11:47:14.264037: step 12285, loss 0.00969592, acc 1
2016-09-06T11:47:15.081618: step 12286, loss 0.00351733, acc 1
2016-09-06T11:47:15.886337: step 12287, loss 0.00671958, acc 1
2016-09-06T11:47:16.634663: step 12288, loss 0.00630525, acc 1
2016-09-06T11:47:17.433311: step 12289, loss 0.0205322, acc 0.98
2016-09-06T11:47:18.273810: step 12290, loss 0.0093505, acc 1
2016-09-06T11:47:19.097773: step 12291, loss 0.00321899, acc 1
2016-09-06T11:47:19.897771: step 12292, loss 0.0229225, acc 0.98
2016-09-06T11:47:20.695521: step 12293, loss 0.00878804, acc 1
2016-09-06T11:47:21.531761: step 12294, loss 0.0190171, acc 1
2016-09-06T11:47:22.343373: step 12295, loss 0.00235013, acc 1
2016-09-06T11:47:23.141549: step 12296, loss 0.00669098, acc 1
2016-09-06T11:47:23.941028: step 12297, loss 0.00335713, acc 1
2016-09-06T11:47:24.741100: step 12298, loss 0.172827, acc 0.94
2016-09-06T11:47:25.535659: step 12299, loss 0.00231966, acc 1
2016-09-06T11:47:26.350268: step 12300, loss 0.00232632, acc 1

Evaluation:
2016-09-06T11:47:30.098664: step 12300, loss 2.33639, acc 0.745779

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-12300

2016-09-06T11:47:32.035048: step 12301, loss 0.00568261, acc 1
2016-09-06T11:47:32.850140: step 12302, loss 0.0109649, acc 1
2016-09-06T11:47:33.681954: step 12303, loss 0.026171, acc 1
2016-09-06T11:47:34.527436: step 12304, loss 0.00371212, acc 1
2016-09-06T11:47:35.277350: step 12305, loss 0.0066191, acc 1
2016-09-06T11:47:36.094115: step 12306, loss 0.00652997, acc 1
2016-09-06T11:47:36.915807: step 12307, loss 0.00202464, acc 1
2016-09-06T11:47:37.701565: step 12308, loss 0.00207463, acc 1
2016-09-06T11:47:38.514381: step 12309, loss 0.0036355, acc 1
2016-09-06T11:47:39.316308: step 12310, loss 0.0134019, acc 1
2016-09-06T11:47:40.108436: step 12311, loss 0.00248066, acc 1
2016-09-06T11:47:40.910538: step 12312, loss 0.0550627, acc 0.98
2016-09-06T11:47:41.750226: step 12313, loss 0.0100949, acc 1
2016-09-06T11:47:42.546258: step 12314, loss 0.00282506, acc 1
2016-09-06T11:47:43.350978: step 12315, loss 0.0248602, acc 0.98
2016-09-06T11:47:44.202614: step 12316, loss 0.00215679, acc 1
2016-09-06T11:47:44.978747: step 12317, loss 0.042408, acc 0.98
2016-09-06T11:47:45.781484: step 12318, loss 0.0426095, acc 0.98
2016-09-06T11:47:46.620844: step 12319, loss 0.00196873, acc 1
2016-09-06T11:47:47.411203: step 12320, loss 0.0124802, acc 1
2016-09-06T11:47:48.208569: step 12321, loss 0.00534279, acc 1
2016-09-06T11:47:49.024753: step 12322, loss 0.0206673, acc 0.98
2016-09-06T11:47:49.797976: step 12323, loss 0.0156216, acc 1
2016-09-06T11:47:50.611453: step 12324, loss 0.00201143, acc 1
2016-09-06T11:47:51.431531: step 12325, loss 0.00798062, acc 1
2016-09-06T11:47:52.250841: step 12326, loss 0.0166382, acc 1
2016-09-06T11:47:53.050097: step 12327, loss 0.0259093, acc 1
2016-09-06T11:47:53.851486: step 12328, loss 0.00206402, acc 1
2016-09-06T11:47:54.630233: step 12329, loss 0.0164832, acc 0.98
2016-09-06T11:47:55.443681: step 12330, loss 0.0321247, acc 0.98
2016-09-06T11:47:56.250175: step 12331, loss 0.0314901, acc 0.98
2016-09-06T11:47:57.024443: step 12332, loss 0.00791703, acc 1
2016-09-06T11:47:57.810763: step 12333, loss 0.00206903, acc 1
2016-09-06T11:47:58.610745: step 12334, loss 0.009539, acc 1
2016-09-06T11:47:59.409456: step 12335, loss 0.0369194, acc 0.98
2016-09-06T11:48:00.218013: step 12336, loss 0.00202278, acc 1
2016-09-06T11:48:01.045525: step 12337, loss 0.0206951, acc 1
2016-09-06T11:48:01.843051: step 12338, loss 0.081136, acc 0.98
2016-09-06T11:48:02.695477: step 12339, loss 0.00236887, acc 1
2016-09-06T11:48:03.492452: step 12340, loss 0.0195503, acc 1
2016-09-06T11:48:04.272358: step 12341, loss 0.0316535, acc 0.98
2016-09-06T11:48:05.108065: step 12342, loss 0.00688799, acc 1
2016-09-06T11:48:05.952389: step 12343, loss 0.00232471, acc 1
2016-09-06T11:48:06.752560: step 12344, loss 0.0113497, acc 1
2016-09-06T11:48:07.546749: step 12345, loss 0.00214712, acc 1
2016-09-06T11:48:08.357938: step 12346, loss 0.011203, acc 1
2016-09-06T11:48:09.128737: step 12347, loss 0.00217411, acc 1
2016-09-06T11:48:09.928733: step 12348, loss 0.00197203, acc 1
2016-09-06T11:48:10.726962: step 12349, loss 0.0396862, acc 0.98
2016-09-06T11:48:11.528887: step 12350, loss 0.00194677, acc 1
2016-09-06T11:48:12.317608: step 12351, loss 0.00194001, acc 1
2016-09-06T11:48:13.139145: step 12352, loss 0.00771991, acc 1
2016-09-06T11:48:13.928931: step 12353, loss 0.0261128, acc 0.98
2016-09-06T11:48:14.747383: step 12354, loss 0.0334371, acc 0.98
2016-09-06T11:48:15.585411: step 12355, loss 0.0224035, acc 0.98
2016-09-06T11:48:16.366767: step 12356, loss 0.0272836, acc 1
2016-09-06T11:48:17.177207: step 12357, loss 0.0455199, acc 0.98
2016-09-06T11:48:17.998831: step 12358, loss 0.00413228, acc 1
2016-09-06T11:48:18.795885: step 12359, loss 0.0100811, acc 1
2016-09-06T11:48:19.584749: step 12360, loss 0.00548809, acc 1
2016-09-06T11:48:20.421405: step 12361, loss 0.00221628, acc 1
2016-09-06T11:48:21.217410: step 12362, loss 0.00360499, acc 1
2016-09-06T11:48:22.000413: step 12363, loss 0.0199696, acc 1
2016-09-06T11:48:22.839311: step 12364, loss 0.0157387, acc 1
2016-09-06T11:48:23.625404: step 12365, loss 0.0480189, acc 0.96
2016-09-06T11:48:24.425973: step 12366, loss 0.0353646, acc 0.98
2016-09-06T11:48:25.257112: step 12367, loss 0.0563683, acc 0.98
2016-09-06T11:48:26.045410: step 12368, loss 0.0143259, acc 1
2016-09-06T11:48:26.853675: step 12369, loss 0.00225833, acc 1
2016-09-06T11:48:27.668458: step 12370, loss 0.0160354, acc 1
2016-09-06T11:48:28.441483: step 12371, loss 0.0316673, acc 0.98
2016-09-06T11:48:29.239190: step 12372, loss 0.00218325, acc 1
2016-09-06T11:48:30.083417: step 12373, loss 0.0202559, acc 0.98
2016-09-06T11:48:30.873662: step 12374, loss 0.00451201, acc 1
2016-09-06T11:48:31.668660: step 12375, loss 0.0523838, acc 0.98
2016-09-06T11:48:32.485771: step 12376, loss 0.00756841, acc 1
2016-09-06T11:48:33.241226: step 12377, loss 0.00631015, acc 1
2016-09-06T11:48:34.066823: step 12378, loss 0.00281998, acc 1
2016-09-06T11:48:34.869216: step 12379, loss 0.0540758, acc 0.98
2016-09-06T11:48:35.636602: step 12380, loss 0.00179071, acc 1
2016-09-06T11:48:36.454618: step 12381, loss 0.00254263, acc 1
2016-09-06T11:48:37.269378: step 12382, loss 0.0233949, acc 0.98
2016-09-06T11:48:38.066425: step 12383, loss 0.00774301, acc 1
2016-09-06T11:48:38.859960: step 12384, loss 0.00520302, acc 1
2016-09-06T11:48:39.693348: step 12385, loss 0.00323031, acc 1
2016-09-06T11:48:40.492216: step 12386, loss 0.00987601, acc 1
2016-09-06T11:48:41.277229: step 12387, loss 0.00224421, acc 1
2016-09-06T11:48:42.082608: step 12388, loss 0.00844704, acc 1
2016-09-06T11:48:42.928009: step 12389, loss 0.0164343, acc 1
2016-09-06T11:48:43.729081: step 12390, loss 0.00175877, acc 1
2016-09-06T11:48:44.548730: step 12391, loss 0.00747735, acc 1
2016-09-06T11:48:45.351552: step 12392, loss 0.0178918, acc 1
2016-09-06T11:48:46.140311: step 12393, loss 0.0158647, acc 1
2016-09-06T11:48:46.951521: step 12394, loss 0.146589, acc 0.98
2016-09-06T11:48:47.742543: step 12395, loss 0.0160955, acc 0.98
2016-09-06T11:48:48.586697: step 12396, loss 0.0243838, acc 0.98
2016-09-06T11:48:49.431961: step 12397, loss 0.0316399, acc 0.96
2016-09-06T11:48:50.199250: step 12398, loss 0.0311186, acc 1
2016-09-06T11:48:50.997491: step 12399, loss 0.00178406, acc 1
2016-09-06T11:48:51.806862: step 12400, loss 0.00378245, acc 1

Evaluation:
2016-09-06T11:48:55.504142: step 12400, loss 1.92786, acc 0.747655

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-12400

2016-09-06T11:48:57.349191: step 12401, loss 0.0140564, acc 1
2016-09-06T11:48:58.202508: step 12402, loss 0.00372319, acc 1
2016-09-06T11:48:58.996276: step 12403, loss 0.01565, acc 1
2016-09-06T11:48:59.845118: step 12404, loss 0.0223657, acc 0.98
2016-09-06T11:49:00.674511: step 12405, loss 0.00772597, acc 1
2016-09-06T11:49:01.499454: step 12406, loss 0.00294552, acc 1
2016-09-06T11:49:02.296076: step 12407, loss 0.0362178, acc 0.98
2016-09-06T11:49:03.099352: step 12408, loss 0.0364165, acc 0.98
2016-09-06T11:49:03.904501: step 12409, loss 0.00198395, acc 1
2016-09-06T11:49:04.711671: step 12410, loss 0.00204645, acc 1
2016-09-06T11:49:05.581092: step 12411, loss 0.0177093, acc 0.98
2016-09-06T11:49:06.383419: step 12412, loss 0.0145381, acc 1
2016-09-06T11:49:07.162147: step 12413, loss 0.0381967, acc 0.98
2016-09-06T11:49:08.010430: step 12414, loss 0.00414867, acc 1
2016-09-06T11:49:08.827763: step 12415, loss 0.0368097, acc 0.98
2016-09-06T11:49:09.614814: step 12416, loss 0.00688376, acc 1
2016-09-06T11:49:10.436739: step 12417, loss 0.0574663, acc 0.98
2016-09-06T11:49:11.227514: step 12418, loss 0.00587491, acc 1
2016-09-06T11:49:12.061466: step 12419, loss 0.00186493, acc 1
2016-09-06T11:49:12.913445: step 12420, loss 0.0369472, acc 0.98
2016-09-06T11:49:13.739183: step 12421, loss 0.00970937, acc 1
2016-09-06T11:49:14.526466: step 12422, loss 0.0507824, acc 0.96
2016-09-06T11:49:15.314528: step 12423, loss 0.0455844, acc 0.98
2016-09-06T11:49:16.140451: step 12424, loss 0.00576597, acc 1
2016-09-06T11:49:16.937780: step 12425, loss 0.00282255, acc 1
2016-09-06T11:49:17.716578: step 12426, loss 0.028871, acc 1
2016-09-06T11:49:18.530689: step 12427, loss 0.0174647, acc 0.98
2016-09-06T11:49:19.312574: step 12428, loss 0.00175491, acc 1
2016-09-06T11:49:20.110297: step 12429, loss 0.017672, acc 1
2016-09-06T11:49:20.903825: step 12430, loss 0.00219907, acc 1
2016-09-06T11:49:21.692908: step 12431, loss 0.00249358, acc 1
2016-09-06T11:49:22.503933: step 12432, loss 0.0525878, acc 0.98
2016-09-06T11:49:23.302506: step 12433, loss 0.00636509, acc 1
2016-09-06T11:49:24.122332: step 12434, loss 0.0298477, acc 0.98
2016-09-06T11:49:24.968023: step 12435, loss 0.0307545, acc 1
2016-09-06T11:49:25.801420: step 12436, loss 0.0046839, acc 1
2016-09-06T11:49:26.586869: step 12437, loss 0.0332388, acc 0.96
2016-09-06T11:49:27.401860: step 12438, loss 0.00408238, acc 1
2016-09-06T11:49:28.228176: step 12439, loss 0.0109712, acc 1
2016-09-06T11:49:29.013675: step 12440, loss 0.0016774, acc 1
2016-09-06T11:49:29.811010: step 12441, loss 0.0252044, acc 0.98
2016-09-06T11:49:30.641448: step 12442, loss 0.00334712, acc 1
2016-09-06T11:49:31.431227: step 12443, loss 0.00181948, acc 1
2016-09-06T11:49:32.204491: step 12444, loss 0.046929, acc 0.98
2016-09-06T11:49:33.019358: step 12445, loss 0.00170543, acc 1
2016-09-06T11:49:33.811949: step 12446, loss 0.0407991, acc 0.98
2016-09-06T11:49:34.632147: step 12447, loss 0.00866468, acc 1
2016-09-06T11:49:35.427625: step 12448, loss 0.0139883, acc 1
2016-09-06T11:49:36.221088: step 12449, loss 0.0257528, acc 0.98
2016-09-06T11:49:37.009229: step 12450, loss 0.0280578, acc 1
2016-09-06T11:49:37.838895: step 12451, loss 0.0127057, acc 1
2016-09-06T11:49:38.641713: step 12452, loss 0.0514848, acc 0.98
2016-09-06T11:49:39.457683: step 12453, loss 0.00223581, acc 1
2016-09-06T11:49:40.277796: step 12454, loss 0.0141159, acc 1
2016-09-06T11:49:41.074319: step 12455, loss 0.00257854, acc 1
2016-09-06T11:49:41.905653: step 12456, loss 0.137838, acc 0.98
2016-09-06T11:49:42.725049: step 12457, loss 0.0735665, acc 0.92
2016-09-06T11:49:43.515759: step 12458, loss 0.00478588, acc 1
2016-09-06T11:49:44.294917: step 12459, loss 0.00183564, acc 1
2016-09-06T11:49:45.121470: step 12460, loss 0.0174546, acc 0.98
2016-09-06T11:49:45.925472: step 12461, loss 0.00174729, acc 1
2016-09-06T11:49:46.729502: step 12462, loss 0.152534, acc 0.98
2016-09-06T11:49:47.541406: step 12463, loss 0.0348724, acc 0.98
2016-09-06T11:49:48.331183: step 12464, loss 0.00359885, acc 1
2016-09-06T11:49:49.141130: step 12465, loss 0.0255134, acc 0.98
2016-09-06T11:49:49.977956: step 12466, loss 0.0340804, acc 1
2016-09-06T11:49:50.757724: step 12467, loss 0.0168087, acc 1
2016-09-06T11:49:51.569468: step 12468, loss 0.0754472, acc 0.98
2016-09-06T11:49:52.412625: step 12469, loss 0.0080085, acc 1
2016-09-06T11:49:53.196407: step 12470, loss 0.0847445, acc 0.98
2016-09-06T11:49:53.993501: step 12471, loss 0.00233734, acc 1
2016-09-06T11:49:54.811729: step 12472, loss 0.00265899, acc 1
2016-09-06T11:49:55.605899: step 12473, loss 0.0358182, acc 0.98
2016-09-06T11:49:56.381491: step 12474, loss 0.0205725, acc 1
2016-09-06T11:49:57.188499: step 12475, loss 0.0782838, acc 0.96
2016-09-06T11:49:57.964720: step 12476, loss 0.0132792, acc 1
2016-09-06T11:49:58.776380: step 12477, loss 0.0269085, acc 1
2016-09-06T11:49:59.591997: step 12478, loss 0.0214013, acc 1
2016-09-06T11:50:00.411630: step 12479, loss 0.025161, acc 1
2016-09-06T11:50:01.147083: step 12480, loss 0.00599729, acc 1
2016-09-06T11:50:01.958403: step 12481, loss 0.0415675, acc 0.98
2016-09-06T11:50:02.786994: step 12482, loss 0.0214998, acc 1
2016-09-06T11:50:03.595037: step 12483, loss 0.00776284, acc 1
2016-09-06T11:50:04.409441: step 12484, loss 0.00458716, acc 1
2016-09-06T11:50:05.213783: step 12485, loss 0.018301, acc 0.98
2016-09-06T11:50:06.014846: step 12486, loss 0.0111319, acc 1
2016-09-06T11:50:06.834740: step 12487, loss 0.026934, acc 1
2016-09-06T11:50:07.651608: step 12488, loss 0.00415076, acc 1
2016-09-06T11:50:08.457563: step 12489, loss 0.0411249, acc 0.96
2016-09-06T11:50:09.285297: step 12490, loss 0.0170755, acc 1
2016-09-06T11:50:10.072502: step 12491, loss 0.0318555, acc 0.98
2016-09-06T11:50:10.882944: step 12492, loss 0.0112263, acc 1
2016-09-06T11:50:11.685816: step 12493, loss 0.00642611, acc 1
2016-09-06T11:50:12.475184: step 12494, loss 0.00446064, acc 1
2016-09-06T11:50:13.273682: step 12495, loss 0.0219586, acc 1
2016-09-06T11:50:14.094969: step 12496, loss 0.00286478, acc 1
2016-09-06T11:50:14.877196: step 12497, loss 0.00309063, acc 1
2016-09-06T11:50:15.693988: step 12498, loss 0.0029277, acc 1
2016-09-06T11:50:16.487929: step 12499, loss 0.00357388, acc 1
2016-09-06T11:50:17.290863: step 12500, loss 0.00289079, acc 1

Evaluation:
2016-09-06T11:50:21.070066: step 12500, loss 2.28692, acc 0.74015

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-12500

2016-09-06T11:50:22.983823: step 12501, loss 0.00730399, acc 1
2016-09-06T11:50:23.818407: step 12502, loss 0.0291462, acc 0.98
2016-09-06T11:50:24.669271: step 12503, loss 0.00865905, acc 1
2016-09-06T11:50:25.506727: step 12504, loss 0.01849, acc 0.98
2016-09-06T11:50:26.355367: step 12505, loss 0.00302757, acc 1
2016-09-06T11:50:27.131240: step 12506, loss 0.0414277, acc 0.98
2016-09-06T11:50:27.939550: step 12507, loss 0.00310669, acc 1
2016-09-06T11:50:28.775004: step 12508, loss 0.00288233, acc 1
2016-09-06T11:50:29.572360: step 12509, loss 0.0312807, acc 0.98
2016-09-06T11:50:30.378728: step 12510, loss 0.0233099, acc 0.98
2016-09-06T11:50:31.186228: step 12511, loss 0.00793804, acc 1
2016-09-06T11:50:31.965198: step 12512, loss 0.0318688, acc 0.98
2016-09-06T11:50:32.775872: step 12513, loss 0.0548404, acc 0.98
2016-09-06T11:50:33.608825: step 12514, loss 0.0220736, acc 1
2016-09-06T11:50:34.436956: step 12515, loss 0.00269715, acc 1
2016-09-06T11:50:35.254164: step 12516, loss 0.00365347, acc 1
2016-09-06T11:50:36.080519: step 12517, loss 0.0191529, acc 0.98
2016-09-06T11:50:36.893690: step 12518, loss 0.0158755, acc 1
2016-09-06T11:50:37.692203: step 12519, loss 0.00310577, acc 1
2016-09-06T11:50:38.525115: step 12520, loss 0.0158367, acc 1
2016-09-06T11:50:39.333448: step 12521, loss 0.00601814, acc 1
2016-09-06T11:50:40.167734: step 12522, loss 0.146405, acc 0.96
2016-09-06T11:50:40.994204: step 12523, loss 0.0368566, acc 0.98
2016-09-06T11:50:41.814278: step 12524, loss 0.0110093, acc 1
2016-09-06T11:50:42.611212: step 12525, loss 0.00419019, acc 1
2016-09-06T11:50:43.448141: step 12526, loss 0.0403274, acc 0.98
2016-09-06T11:50:44.285676: step 12527, loss 0.0790075, acc 0.98
2016-09-06T11:50:45.090446: step 12528, loss 0.0122038, acc 1
2016-09-06T11:50:45.918139: step 12529, loss 0.0174916, acc 1
2016-09-06T11:50:46.714975: step 12530, loss 0.0129009, acc 1
2016-09-06T11:50:47.517866: step 12531, loss 0.0163838, acc 1
2016-09-06T11:50:48.334394: step 12532, loss 0.0358144, acc 0.98
2016-09-06T11:50:49.138334: step 12533, loss 0.0441292, acc 0.96
2016-09-06T11:50:49.976112: step 12534, loss 0.0606368, acc 0.96
2016-09-06T11:50:50.821420: step 12535, loss 0.0264321, acc 0.98
2016-09-06T11:50:51.636423: step 12536, loss 0.00735529, acc 1
2016-09-06T11:50:52.463378: step 12537, loss 0.0178979, acc 0.98
2016-09-06T11:50:53.313477: step 12538, loss 0.0323261, acc 0.96
2016-09-06T11:50:54.151960: step 12539, loss 0.0221164, acc 1
2016-09-06T11:50:54.955444: step 12540, loss 0.0033872, acc 1
2016-09-06T11:50:55.800515: step 12541, loss 0.0243644, acc 1
2016-09-06T11:50:56.610364: step 12542, loss 0.0133125, acc 1
2016-09-06T11:50:57.422833: step 12543, loss 0.005384, acc 1
2016-09-06T11:50:58.269440: step 12544, loss 0.0133435, acc 1
2016-09-06T11:50:59.220057: step 12545, loss 0.00424709, acc 1
2016-09-06T11:51:00.249434: step 12546, loss 0.122799, acc 0.96
2016-09-06T11:51:01.370334: step 12547, loss 0.0176706, acc 1
2016-09-06T11:51:02.295539: step 12548, loss 0.0270708, acc 1
2016-09-06T11:51:03.226569: step 12549, loss 0.0198048, acc 0.98
2016-09-06T11:51:04.144001: step 12550, loss 0.00463566, acc 1
2016-09-06T11:51:05.009988: step 12551, loss 0.026423, acc 1
2016-09-06T11:51:06.079024: step 12552, loss 0.00474684, acc 1
2016-09-06T11:51:06.970909: step 12553, loss 0.0342297, acc 0.98
2016-09-06T11:51:07.868185: step 12554, loss 0.00568844, acc 1
2016-09-06T11:51:08.778208: step 12555, loss 0.0211554, acc 1
2016-09-06T11:51:09.761241: step 12556, loss 0.0143284, acc 1
2016-09-06T11:51:10.704304: step 12557, loss 0.00824727, acc 1
2016-09-06T11:51:11.820978: step 12558, loss 0.0489679, acc 0.98
2016-09-06T11:51:12.664504: step 12559, loss 0.00490152, acc 1
2016-09-06T11:51:13.667430: step 12560, loss 0.00503405, acc 1
2016-09-06T11:51:14.681123: step 12561, loss 0.00492695, acc 1
2016-09-06T11:51:15.755944: step 12562, loss 0.0178877, acc 1
2016-09-06T11:51:16.668831: step 12563, loss 0.00832579, acc 1
2016-09-06T11:51:17.606730: step 12564, loss 0.00477173, acc 1
2016-09-06T11:51:18.502191: step 12565, loss 0.0126034, acc 1
2016-09-06T11:51:19.384424: step 12566, loss 0.00962039, acc 1
2016-09-06T11:51:20.488371: step 12567, loss 0.00471579, acc 1
2016-09-06T11:51:21.319519: step 12568, loss 0.0520969, acc 0.96
2016-09-06T11:51:22.446533: step 12569, loss 0.00551638, acc 1
2016-09-06T11:51:23.379605: step 12570, loss 0.004585, acc 1
2016-09-06T11:51:24.214764: step 12571, loss 0.00607973, acc 1
2016-09-06T11:51:25.232512: step 12572, loss 0.004534, acc 1
2016-09-06T11:51:26.027075: step 12573, loss 0.00453077, acc 1
2016-09-06T11:51:26.958471: step 12574, loss 0.0096306, acc 1
2016-09-06T11:51:27.880976: step 12575, loss 0.0176392, acc 1
2016-09-06T11:51:28.966089: step 12576, loss 0.00431294, acc 1
2016-09-06T11:51:29.954552: step 12577, loss 0.0797547, acc 0.98
2016-09-06T11:51:30.806505: step 12578, loss 0.0475217, acc 0.96
2016-09-06T11:51:31.745409: step 12579, loss 0.00420111, acc 1
2016-09-06T11:51:32.590767: step 12580, loss 0.0261197, acc 0.98
2016-09-06T11:51:33.461411: step 12581, loss 0.00429434, acc 1
2016-09-06T11:51:34.395748: step 12582, loss 0.00638403, acc 1
2016-09-06T11:51:35.255239: step 12583, loss 0.00768212, acc 1
2016-09-06T11:51:36.095100: step 12584, loss 0.0471665, acc 0.98
2016-09-06T11:51:36.903885: step 12585, loss 0.0244661, acc 0.98
2016-09-06T11:51:37.717518: step 12586, loss 0.00485109, acc 1
2016-09-06T11:51:38.519032: step 12587, loss 0.090616, acc 0.98
2016-09-06T11:51:39.319500: step 12588, loss 0.0314971, acc 1
2016-09-06T11:51:40.135780: step 12589, loss 0.0109453, acc 1
2016-09-06T11:51:40.941269: step 12590, loss 0.0732488, acc 0.96
2016-09-06T11:51:41.745607: step 12591, loss 0.0238447, acc 0.98
2016-09-06T11:51:42.583003: step 12592, loss 0.0276095, acc 0.98
2016-09-06T11:51:43.417816: step 12593, loss 0.063197, acc 0.96
2016-09-06T11:51:44.224681: step 12594, loss 0.0609398, acc 0.98
2016-09-06T11:51:45.022487: step 12595, loss 0.0448304, acc 0.98
2016-09-06T11:51:45.843604: step 12596, loss 0.0047319, acc 1
2016-09-06T11:51:46.676429: step 12597, loss 0.00880424, acc 1
2016-09-06T11:51:47.516964: step 12598, loss 0.0102104, acc 1
2016-09-06T11:51:48.333734: step 12599, loss 0.0145408, acc 1
2016-09-06T11:51:49.152067: step 12600, loss 0.0487923, acc 0.98

Evaluation:
2016-09-06T11:51:52.879458: step 12600, loss 1.93573, acc 0.742026

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-12600

2016-09-06T11:51:54.871669: step 12601, loss 0.00296916, acc 1
2016-09-06T11:51:55.670234: step 12602, loss 0.0134801, acc 1
2016-09-06T11:51:56.473545: step 12603, loss 0.0133579, acc 1
2016-09-06T11:51:57.291785: step 12604, loss 0.00574708, acc 1
2016-09-06T11:51:58.093327: step 12605, loss 0.00513398, acc 1
2016-09-06T11:51:58.894469: step 12606, loss 0.0243021, acc 0.98
2016-09-06T11:51:59.708297: step 12607, loss 0.0930297, acc 0.96
2016-09-06T11:52:00.504165: step 12608, loss 0.0212911, acc 1
2016-09-06T11:52:01.299983: step 12609, loss 0.014128, acc 1
2016-09-06T11:52:02.127409: step 12610, loss 0.00988923, acc 1
2016-09-06T11:52:03.017014: step 12611, loss 0.0342362, acc 1
2016-09-06T11:52:03.854356: step 12612, loss 0.0132396, acc 1
2016-09-06T11:52:04.714241: step 12613, loss 0.0107306, acc 1
2016-09-06T11:52:05.539259: step 12614, loss 0.0629343, acc 0.96
2016-09-06T11:52:06.451470: step 12615, loss 0.0194304, acc 1
2016-09-06T11:52:07.267488: step 12616, loss 0.0060863, acc 1
2016-09-06T11:52:08.109291: step 12617, loss 0.05376, acc 0.96
2016-09-06T11:52:08.876402: step 12618, loss 0.0132339, acc 1
2016-09-06T11:52:09.704780: step 12619, loss 0.0104772, acc 1
2016-09-06T11:52:10.542444: step 12620, loss 0.00660738, acc 1
2016-09-06T11:52:11.361463: step 12621, loss 0.03023, acc 0.98
2016-09-06T11:52:12.152731: step 12622, loss 0.0755511, acc 0.98
2016-09-06T11:52:12.955782: step 12623, loss 0.00530839, acc 1
2016-09-06T11:52:13.747045: step 12624, loss 0.0357532, acc 1
2016-09-06T11:52:14.559209: step 12625, loss 0.0130205, acc 1
2016-09-06T11:52:15.400635: step 12626, loss 0.0324895, acc 0.98
2016-09-06T11:52:16.181401: step 12627, loss 0.0550321, acc 0.96
2016-09-06T11:52:17.001818: step 12628, loss 0.0123532, acc 1
2016-09-06T11:52:17.828510: step 12629, loss 0.053369, acc 0.96
2016-09-06T11:52:18.637725: step 12630, loss 0.0265835, acc 0.98
2016-09-06T11:52:19.448588: step 12631, loss 0.00668196, acc 1
2016-09-06T11:52:20.255163: step 12632, loss 0.0139311, acc 1
2016-09-06T11:52:21.039012: step 12633, loss 0.00709665, acc 1
2016-09-06T11:52:21.852850: step 12634, loss 0.00530081, acc 1
2016-09-06T11:52:22.674596: step 12635, loss 0.0164821, acc 1
2016-09-06T11:52:23.489672: step 12636, loss 0.0743545, acc 0.98
2016-09-06T11:52:24.302512: step 12637, loss 0.0124709, acc 1
2016-09-06T11:52:25.135315: step 12638, loss 0.0168728, acc 1
2016-09-06T11:52:25.969125: step 12639, loss 0.00485494, acc 1
2016-09-06T11:52:26.778870: step 12640, loss 0.0218406, acc 1
2016-09-06T11:52:27.610682: step 12641, loss 0.0412857, acc 0.98
2016-09-06T11:52:28.435157: step 12642, loss 0.0323622, acc 0.98
2016-09-06T11:52:29.236637: step 12643, loss 0.0195253, acc 0.98
2016-09-06T11:52:30.048575: step 12644, loss 0.0095281, acc 1
2016-09-06T11:52:30.875966: step 12645, loss 0.0157462, acc 1
2016-09-06T11:52:31.678694: step 12646, loss 0.0630508, acc 0.98
2016-09-06T11:52:32.505393: step 12647, loss 0.041183, acc 0.98
2016-09-06T11:52:33.286329: step 12648, loss 0.0192753, acc 0.98
2016-09-06T11:52:34.116723: step 12649, loss 0.0880975, acc 0.96
2016-09-06T11:52:34.948942: step 12650, loss 0.0524834, acc 0.98
2016-09-06T11:52:35.779536: step 12651, loss 0.025668, acc 0.98
2016-09-06T11:52:36.644674: step 12652, loss 0.0376181, acc 1
2016-09-06T11:52:37.477706: step 12653, loss 0.00650356, acc 1
2016-09-06T11:52:38.316744: step 12654, loss 0.0302229, acc 0.98
2016-09-06T11:52:39.122882: step 12655, loss 0.00992554, acc 1
2016-09-06T11:52:39.932953: step 12656, loss 0.00444065, acc 1
2016-09-06T11:52:40.781026: step 12657, loss 0.0165821, acc 1
2016-09-06T11:52:41.581621: step 12658, loss 0.0943851, acc 0.96
2016-09-06T11:52:42.384951: step 12659, loss 0.0185191, acc 1
2016-09-06T11:52:43.195801: step 12660, loss 0.0280261, acc 0.98
2016-09-06T11:52:43.988862: step 12661, loss 0.00697712, acc 1
2016-09-06T11:52:44.772592: step 12662, loss 0.00866612, acc 1
2016-09-06T11:52:45.591171: step 12663, loss 0.00652298, acc 1
2016-09-06T11:52:46.361376: step 12664, loss 0.0265787, acc 1
2016-09-06T11:52:47.162835: step 12665, loss 0.0282586, acc 0.98
2016-09-06T11:52:47.976204: step 12666, loss 0.0142623, acc 1
2016-09-06T11:52:48.777845: step 12667, loss 0.0264726, acc 0.98
2016-09-06T11:52:49.585367: step 12668, loss 0.0123525, acc 1
2016-09-06T11:52:50.400598: step 12669, loss 0.0190451, acc 0.98
2016-09-06T11:52:51.177410: step 12670, loss 0.00592404, acc 1
2016-09-06T11:52:51.982749: step 12671, loss 0.070506, acc 0.98
2016-09-06T11:52:52.778826: step 12672, loss 0.00994102, acc 1
2016-09-06T11:52:53.573060: step 12673, loss 0.0138573, acc 1
2016-09-06T11:52:54.374988: step 12674, loss 0.0551448, acc 0.96
2016-09-06T11:52:55.183299: step 12675, loss 0.0169483, acc 1
2016-09-06T11:52:55.970488: step 12676, loss 0.0251153, acc 0.98
2016-09-06T11:52:56.810412: step 12677, loss 0.0480789, acc 0.96
2016-09-06T11:52:57.638271: step 12678, loss 0.0302927, acc 0.98
2016-09-06T11:52:58.421262: step 12679, loss 0.0325768, acc 0.98
2016-09-06T11:52:59.218029: step 12680, loss 0.0161349, acc 1
2016-09-06T11:53:00.041859: step 12681, loss 0.0322871, acc 0.98
2016-09-06T11:53:00.868969: step 12682, loss 0.00761087, acc 1
2016-09-06T11:53:01.671026: step 12683, loss 0.00375679, acc 1
2016-09-06T11:53:02.529220: step 12684, loss 0.0103185, acc 1
2016-09-06T11:53:03.323905: step 12685, loss 0.0108836, acc 1
2016-09-06T11:53:04.158167: step 12686, loss 0.013066, acc 1
2016-09-06T11:53:04.988529: step 12687, loss 0.0638168, acc 0.96
2016-09-06T11:53:05.807590: step 12688, loss 0.00356385, acc 1
2016-09-06T11:53:06.627714: step 12689, loss 0.0191444, acc 0.98
2016-09-06T11:53:07.441665: step 12690, loss 0.0175278, acc 0.98
2016-09-06T11:53:08.264283: step 12691, loss 0.0353927, acc 0.98
2016-09-06T11:53:09.073045: step 12692, loss 0.00341967, acc 1
2016-09-06T11:53:09.894438: step 12693, loss 0.00428186, acc 1
2016-09-06T11:53:10.708050: step 12694, loss 0.00403561, acc 1
2016-09-06T11:53:11.530622: step 12695, loss 0.00695728, acc 1
2016-09-06T11:53:12.357972: step 12696, loss 0.00661647, acc 1
2016-09-06T11:53:13.163584: step 12697, loss 0.0242024, acc 0.98
2016-09-06T11:53:13.998869: step 12698, loss 0.0110922, acc 1
2016-09-06T11:53:14.811275: step 12699, loss 0.00355304, acc 1
2016-09-06T11:53:15.630935: step 12700, loss 0.00347118, acc 1

Evaluation:
2016-09-06T11:53:19.379078: step 12700, loss 2.30566, acc 0.753283

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-12700

2016-09-06T11:53:21.371799: step 12701, loss 0.0203862, acc 0.98
2016-09-06T11:53:22.230579: step 12702, loss 0.0136657, acc 1
2016-09-06T11:53:23.090746: step 12703, loss 0.00375406, acc 1
2016-09-06T11:53:23.880920: step 12704, loss 0.0176547, acc 0.98
2016-09-06T11:53:24.686893: step 12705, loss 0.00411348, acc 1
2016-09-06T11:53:25.492341: step 12706, loss 0.155113, acc 0.98
2016-09-06T11:53:26.303513: step 12707, loss 0.00421573, acc 1
2016-09-06T11:53:27.107707: step 12708, loss 0.0067907, acc 1
2016-09-06T11:53:27.929765: step 12709, loss 0.00761938, acc 1
2016-09-06T11:53:28.731617: step 12710, loss 0.00985009, acc 1
2016-09-06T11:53:29.550937: step 12711, loss 0.00401324, acc 1
2016-09-06T11:53:30.364452: step 12712, loss 0.00796145, acc 1
2016-09-06T11:53:31.156382: step 12713, loss 0.00368938, acc 1
2016-09-06T11:53:31.963899: step 12714, loss 0.00585659, acc 1
2016-09-06T11:53:32.776881: step 12715, loss 0.0220243, acc 0.98
2016-09-06T11:53:33.606887: step 12716, loss 0.0196783, acc 0.98
2016-09-06T11:53:34.400464: step 12717, loss 0.0041864, acc 1
2016-09-06T11:53:35.271433: step 12718, loss 0.0370635, acc 0.98
2016-09-06T11:53:36.081525: step 12719, loss 0.00495645, acc 1
2016-09-06T11:53:36.878278: step 12720, loss 0.00609702, acc 1
2016-09-06T11:53:37.711479: step 12721, loss 0.0208601, acc 1
2016-09-06T11:53:38.538077: step 12722, loss 0.00458253, acc 1
2016-09-06T11:53:39.403459: step 12723, loss 0.0466355, acc 0.96
2016-09-06T11:53:40.227319: step 12724, loss 0.0311442, acc 0.98
2016-09-06T11:53:41.027068: step 12725, loss 0.0092495, acc 1
2016-09-06T11:53:41.830367: step 12726, loss 0.0452701, acc 0.96
2016-09-06T11:53:42.675824: step 12727, loss 0.0155412, acc 1
2016-09-06T11:53:43.505900: step 12728, loss 0.00577727, acc 1
2016-09-06T11:53:44.322918: step 12729, loss 0.00744208, acc 1
2016-09-06T11:53:45.142858: step 12730, loss 0.0157293, acc 1
2016-09-06T11:53:45.988035: step 12731, loss 0.00989622, acc 1
2016-09-06T11:53:46.780562: step 12732, loss 0.00319987, acc 1
2016-09-06T11:53:47.581557: step 12733, loss 0.0166344, acc 1
2016-09-06T11:53:48.408703: step 12734, loss 0.00333476, acc 1
2016-09-06T11:53:49.214101: step 12735, loss 0.00302314, acc 1
2016-09-06T11:53:50.007862: step 12736, loss 0.0224517, acc 0.98
2016-09-06T11:53:50.794347: step 12737, loss 0.00970447, acc 1
2016-09-06T11:53:51.592813: step 12738, loss 0.0123861, acc 1
2016-09-06T11:53:52.420183: step 12739, loss 0.0139743, acc 1
2016-09-06T11:53:53.238682: step 12740, loss 0.0317206, acc 0.98
2016-09-06T11:53:54.028608: step 12741, loss 0.036536, acc 0.98
2016-09-06T11:53:54.844617: step 12742, loss 0.00629358, acc 1
2016-09-06T11:53:55.658976: step 12743, loss 0.005318, acc 1
2016-09-06T11:53:56.518629: step 12744, loss 0.00322749, acc 1
2016-09-06T11:53:57.312980: step 12745, loss 0.00597799, acc 1
2016-09-06T11:53:58.145236: step 12746, loss 0.0569196, acc 0.98
2016-09-06T11:53:58.917401: step 12747, loss 0.00934267, acc 1
2016-09-06T11:53:59.707964: step 12748, loss 0.0150354, acc 1
2016-09-06T11:54:00.566377: step 12749, loss 0.016129, acc 1
2016-09-06T11:54:01.349775: step 12750, loss 0.0807034, acc 0.96
2016-09-06T11:54:02.146695: step 12751, loss 0.00292514, acc 1
2016-09-06T11:54:02.957467: step 12752, loss 0.00296682, acc 1
2016-09-06T11:54:03.753389: step 12753, loss 0.00282326, acc 1
2016-09-06T11:54:04.553429: step 12754, loss 0.00340687, acc 1
2016-09-06T11:54:05.365666: step 12755, loss 0.00513923, acc 1
2016-09-06T11:54:06.139899: step 12756, loss 0.00391181, acc 1
2016-09-06T11:54:06.952697: step 12757, loss 0.0267429, acc 0.98
2016-09-06T11:54:07.768110: step 12758, loss 0.0246602, acc 0.98
2016-09-06T11:54:08.581404: step 12759, loss 0.115285, acc 0.98
2016-09-06T11:54:09.373904: step 12760, loss 0.00348571, acc 1
2016-09-06T11:54:10.193643: step 12761, loss 0.00285631, acc 1
2016-09-06T11:54:10.978671: step 12762, loss 0.0460559, acc 0.98
2016-09-06T11:54:11.805230: step 12763, loss 0.0129105, acc 1
2016-09-06T11:54:12.594225: step 12764, loss 0.0024207, acc 1
2016-09-06T11:54:13.380774: step 12765, loss 0.166346, acc 0.96
2016-09-06T11:54:14.179647: step 12766, loss 0.00890732, acc 1
2016-09-06T11:54:15.029457: step 12767, loss 0.00528064, acc 1
2016-09-06T11:54:15.854456: step 12768, loss 0.0149981, acc 1
2016-09-06T11:54:16.641552: step 12769, loss 0.0118175, acc 1
2016-09-06T11:54:17.482488: step 12770, loss 0.0259581, acc 0.98
2016-09-06T11:54:18.284261: step 12771, loss 0.0115736, acc 1
2016-09-06T11:54:19.097674: step 12772, loss 0.00348181, acc 1
2016-09-06T11:54:19.935347: step 12773, loss 0.0148204, acc 1
2016-09-06T11:54:20.748897: step 12774, loss 0.00477676, acc 1
2016-09-06T11:54:21.546077: step 12775, loss 0.0172349, acc 1
2016-09-06T11:54:22.383521: step 12776, loss 0.220553, acc 0.96
2016-09-06T11:54:23.208671: step 12777, loss 0.0730617, acc 0.96
2016-09-06T11:54:24.018651: step 12778, loss 0.00311612, acc 1
2016-09-06T11:54:24.833998: step 12779, loss 0.0280289, acc 0.98
2016-09-06T11:54:25.634276: step 12780, loss 0.0167231, acc 0.98
2016-09-06T11:54:26.436254: step 12781, loss 0.0517736, acc 0.96
2016-09-06T11:54:27.253974: step 12782, loss 0.0137122, acc 1
2016-09-06T11:54:28.042879: step 12783, loss 0.0114668, acc 1
2016-09-06T11:54:28.852953: step 12784, loss 0.0209976, acc 0.98
2016-09-06T11:54:29.676357: step 12785, loss 0.0270604, acc 1
2016-09-06T11:54:30.489161: step 12786, loss 0.0338085, acc 0.98
2016-09-06T11:54:31.299210: step 12787, loss 0.0472249, acc 0.96
2016-09-06T11:54:32.103488: step 12788, loss 0.0157965, acc 1
2016-09-06T11:54:32.919331: step 12789, loss 0.00382187, acc 1
2016-09-06T11:54:33.735220: step 12790, loss 0.00317264, acc 1
2016-09-06T11:54:34.563096: step 12791, loss 0.00643076, acc 1
2016-09-06T11:54:35.402609: step 12792, loss 0.0357897, acc 0.98
2016-09-06T11:54:36.191636: step 12793, loss 0.0107449, acc 1
2016-09-06T11:54:37.036224: step 12794, loss 0.00342229, acc 1
2016-09-06T11:54:37.896709: step 12795, loss 0.081455, acc 0.98
2016-09-06T11:54:38.738456: step 12796, loss 0.0840689, acc 0.98
2016-09-06T11:54:39.582854: step 12797, loss 0.023339, acc 1
2016-09-06T11:54:40.398774: step 12798, loss 0.00761934, acc 1
2016-09-06T11:54:41.179179: step 12799, loss 0.0172289, acc 1
2016-09-06T11:54:41.981065: step 12800, loss 0.0407579, acc 0.96

Evaluation:
2016-09-06T11:54:45.716266: step 12800, loss 1.89213, acc 0.756098

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-12800

2016-09-06T11:54:47.588141: step 12801, loss 0.00408036, acc 1
2016-09-06T11:54:48.432206: step 12802, loss 0.0267798, acc 0.98
2016-09-06T11:54:49.257240: step 12803, loss 0.00321873, acc 1
2016-09-06T11:54:50.072872: step 12804, loss 0.0154562, acc 1
2016-09-06T11:54:50.861078: step 12805, loss 0.0192957, acc 1
2016-09-06T11:54:51.684781: step 12806, loss 0.00862411, acc 1
2016-09-06T11:54:52.489280: step 12807, loss 0.0164213, acc 1
2016-09-06T11:54:53.267303: step 12808, loss 0.0207526, acc 1
2016-09-06T11:54:54.092685: step 12809, loss 0.019279, acc 0.98
2016-09-06T11:54:54.911468: step 12810, loss 0.0133597, acc 1
2016-09-06T11:54:55.699388: step 12811, loss 0.00456179, acc 1
2016-09-06T11:54:56.494625: step 12812, loss 0.0142309, acc 1
2016-09-06T11:54:57.312078: step 12813, loss 0.00411289, acc 1
2016-09-06T11:54:58.094640: step 12814, loss 0.00628608, acc 1
2016-09-06T11:54:58.910640: step 12815, loss 0.00335238, acc 1
2016-09-06T11:54:59.746626: step 12816, loss 0.0239526, acc 0.98
2016-09-06T11:55:00.568518: step 12817, loss 0.0239505, acc 1
2016-09-06T11:55:01.377848: step 12818, loss 0.0194999, acc 0.98
2016-09-06T11:55:02.193162: step 12819, loss 0.00280023, acc 1
2016-09-06T11:55:03.013431: step 12820, loss 0.0843411, acc 0.96
2016-09-06T11:55:03.820831: step 12821, loss 0.0135051, acc 1
2016-09-06T11:55:04.617437: step 12822, loss 0.0409857, acc 1
2016-09-06T11:55:05.405984: step 12823, loss 0.00250123, acc 1
2016-09-06T11:55:06.198347: step 12824, loss 0.00469924, acc 1
2016-09-06T11:55:07.032787: step 12825, loss 0.0223448, acc 0.98
2016-09-06T11:55:07.862503: step 12826, loss 0.00372114, acc 1
2016-09-06T11:55:08.669200: step 12827, loss 0.00411985, acc 1
2016-09-06T11:55:09.468253: step 12828, loss 0.0129682, acc 1
2016-09-06T11:55:10.269981: step 12829, loss 0.0143369, acc 1
2016-09-06T11:55:11.086002: step 12830, loss 0.00426057, acc 1
2016-09-06T11:55:11.896314: step 12831, loss 0.0173078, acc 1
2016-09-06T11:55:12.665656: step 12832, loss 0.0591798, acc 0.94
2016-09-06T11:55:13.470681: step 12833, loss 0.0216371, acc 0.98
2016-09-06T11:55:14.289410: step 12834, loss 0.0168423, acc 1
2016-09-06T11:55:15.080194: step 12835, loss 0.0029602, acc 1
2016-09-06T11:55:15.888737: step 12836, loss 0.0155161, acc 1
2016-09-06T11:55:16.718740: step 12837, loss 0.00543415, acc 1
2016-09-06T11:55:17.527413: step 12838, loss 0.0478606, acc 0.98
2016-09-06T11:55:18.334452: step 12839, loss 0.152781, acc 0.98
2016-09-06T11:55:19.178379: step 12840, loss 0.00539366, acc 1
2016-09-06T11:55:20.002153: step 12841, loss 0.0269266, acc 1
2016-09-06T11:55:20.857591: step 12842, loss 0.00522367, acc 1
2016-09-06T11:55:21.693516: step 12843, loss 0.00519031, acc 1
2016-09-06T11:55:22.519542: step 12844, loss 0.0285651, acc 0.98
2016-09-06T11:55:23.325962: step 12845, loss 0.00618125, acc 1
2016-09-06T11:55:24.168470: step 12846, loss 0.0097349, acc 1
2016-09-06T11:55:24.986625: step 12847, loss 0.0196252, acc 1
2016-09-06T11:55:25.799182: step 12848, loss 0.00401301, acc 1
2016-09-06T11:55:26.656879: step 12849, loss 0.0204212, acc 0.98
2016-09-06T11:55:27.454162: step 12850, loss 0.0196256, acc 0.98
2016-09-06T11:55:28.272279: step 12851, loss 0.00521728, acc 1
2016-09-06T11:55:29.111089: step 12852, loss 0.0195065, acc 1
2016-09-06T11:55:29.895714: step 12853, loss 0.111455, acc 0.98
2016-09-06T11:55:30.721807: step 12854, loss 0.112544, acc 0.98
2016-09-06T11:55:31.543907: step 12855, loss 0.00231519, acc 1
2016-09-06T11:55:32.339166: step 12856, loss 0.00227575, acc 1
2016-09-06T11:55:33.164665: step 12857, loss 0.0116183, acc 1
2016-09-06T11:55:33.968649: step 12858, loss 0.0222752, acc 0.98
2016-09-06T11:55:34.802821: step 12859, loss 0.0215813, acc 1
2016-09-06T11:55:35.622667: step 12860, loss 0.00289502, acc 1
2016-09-06T11:55:36.443545: step 12861, loss 0.0233202, acc 1
2016-09-06T11:55:37.232450: step 12862, loss 0.0186086, acc 1
2016-09-06T11:55:38.025121: step 12863, loss 0.014472, acc 1
2016-09-06T11:55:38.780975: step 12864, loss 0.00311133, acc 1
2016-09-06T11:55:39.613719: step 12865, loss 0.0190355, acc 0.98
2016-09-06T11:55:40.431534: step 12866, loss 0.026642, acc 1
2016-09-06T11:55:41.233946: step 12867, loss 0.00419, acc 1
2016-09-06T11:55:42.060046: step 12868, loss 0.0594771, acc 0.98
2016-09-06T11:55:42.859844: step 12869, loss 0.00732109, acc 1
2016-09-06T11:55:43.664726: step 12870, loss 0.0149894, acc 1
2016-09-06T11:55:44.481047: step 12871, loss 0.0229972, acc 0.98
2016-09-06T11:55:45.260234: step 12872, loss 0.0172104, acc 0.98
2016-09-06T11:55:46.051275: step 12873, loss 0.0243931, acc 0.98
2016-09-06T11:55:46.848430: step 12874, loss 0.0148604, acc 1
2016-09-06T11:55:47.629652: step 12875, loss 0.00235478, acc 1
2016-09-06T11:55:48.447772: step 12876, loss 0.00714711, acc 1
2016-09-06T11:55:49.312563: step 12877, loss 0.0174386, acc 1
2016-09-06T11:55:50.104396: step 12878, loss 0.0164589, acc 1
2016-09-06T11:55:50.884808: step 12879, loss 0.0197933, acc 1
2016-09-06T11:55:51.696898: step 12880, loss 0.00966142, acc 1
2016-09-06T11:55:52.498843: step 12881, loss 0.0677255, acc 0.96
2016-09-06T11:55:53.314359: step 12882, loss 0.00320908, acc 1
2016-09-06T11:55:54.134463: step 12883, loss 0.0175478, acc 1
2016-09-06T11:55:54.906394: step 12884, loss 0.0127868, acc 1
2016-09-06T11:55:55.700150: step 12885, loss 0.0217903, acc 0.98
2016-09-06T11:55:56.518293: step 12886, loss 0.0131733, acc 1
2016-09-06T11:55:57.298846: step 12887, loss 0.0098736, acc 1
2016-09-06T11:55:58.089408: step 12888, loss 0.0191817, acc 1
2016-09-06T11:55:58.921482: step 12889, loss 0.0473172, acc 0.98
2016-09-06T11:55:59.749372: step 12890, loss 0.0308001, acc 0.98
2016-09-06T11:56:00.620286: step 12891, loss 0.00409588, acc 1
2016-09-06T11:56:01.448188: step 12892, loss 0.0027805, acc 1
2016-09-06T11:56:02.252970: step 12893, loss 0.0158234, acc 1
2016-09-06T11:56:03.043523: step 12894, loss 0.0215041, acc 0.98
2016-09-06T11:56:03.888292: step 12895, loss 0.00253665, acc 1
2016-09-06T11:56:04.713602: step 12896, loss 0.0938849, acc 0.98
2016-09-06T11:56:05.518866: step 12897, loss 0.0161935, acc 1
2016-09-06T11:56:06.347858: step 12898, loss 0.0195539, acc 0.98
2016-09-06T11:56:07.192126: step 12899, loss 0.0154992, acc 1
2016-09-06T11:56:08.007890: step 12900, loss 0.00251364, acc 1

Evaluation:
2016-09-06T11:56:11.779739: step 12900, loss 1.98367, acc 0.757036

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-12900

2016-09-06T11:56:13.782694: step 12901, loss 0.0109542, acc 1
2016-09-06T11:56:14.586313: step 12902, loss 0.00236736, acc 1
2016-09-06T11:56:15.411247: step 12903, loss 0.0226035, acc 1
2016-09-06T11:56:16.234052: step 12904, loss 0.00274847, acc 1
2016-09-06T11:56:17.027257: step 12905, loss 0.0155975, acc 1
2016-09-06T11:56:17.820501: step 12906, loss 0.00257788, acc 1
2016-09-06T11:56:18.629146: step 12907, loss 0.0193633, acc 1
2016-09-06T11:56:19.443365: step 12908, loss 0.00709885, acc 1
2016-09-06T11:56:20.257932: step 12909, loss 0.00236085, acc 1
2016-09-06T11:56:21.112602: step 12910, loss 0.00637639, acc 1
2016-09-06T11:56:21.927452: step 12911, loss 0.00270686, acc 1
2016-09-06T11:56:22.716259: step 12912, loss 0.00852447, acc 1
2016-09-06T11:56:23.546559: step 12913, loss 0.0228864, acc 0.98
2016-09-06T11:56:24.382187: step 12914, loss 0.00244703, acc 1
2016-09-06T11:56:25.236959: step 12915, loss 0.0113867, acc 1
2016-09-06T11:56:26.050911: step 12916, loss 0.0364085, acc 0.98
2016-09-06T11:56:26.857672: step 12917, loss 0.0218088, acc 0.98
2016-09-06T11:56:27.635312: step 12918, loss 0.00248131, acc 1
2016-09-06T11:56:28.482464: step 12919, loss 0.0141999, acc 1
2016-09-06T11:56:29.282874: step 12920, loss 0.00624975, acc 1
2016-09-06T11:56:30.086655: step 12921, loss 0.0026346, acc 1
2016-09-06T11:56:30.888516: step 12922, loss 0.0416709, acc 0.98
2016-09-06T11:56:31.670992: step 12923, loss 0.0155093, acc 1
2016-09-06T11:56:32.481352: step 12924, loss 0.00941359, acc 1
2016-09-06T11:56:33.311261: step 12925, loss 0.0544139, acc 0.94
2016-09-06T11:56:34.112452: step 12926, loss 0.0128436, acc 1
2016-09-06T11:56:34.970938: step 12927, loss 0.0525932, acc 0.98
2016-09-06T11:56:35.808119: step 12928, loss 0.0670666, acc 0.98
2016-09-06T11:56:36.633103: step 12929, loss 0.00531094, acc 1
2016-09-06T11:56:37.414400: step 12930, loss 0.0652738, acc 0.96
2016-09-06T11:56:38.215900: step 12931, loss 0.009111, acc 1
2016-09-06T11:56:39.039232: step 12932, loss 0.057516, acc 1
2016-09-06T11:56:39.813832: step 12933, loss 0.0611315, acc 0.94
2016-09-06T11:56:40.612085: step 12934, loss 0.0147527, acc 1
2016-09-06T11:56:41.425478: step 12935, loss 0.0058003, acc 1
2016-09-06T11:56:42.214231: step 12936, loss 0.00243946, acc 1
2016-09-06T11:56:43.019327: step 12937, loss 0.018489, acc 0.98
2016-09-06T11:56:43.849425: step 12938, loss 0.00218991, acc 1
2016-09-06T11:56:44.627942: step 12939, loss 0.0237468, acc 0.98
2016-09-06T11:56:45.461085: step 12940, loss 0.0109328, acc 1
2016-09-06T11:56:46.285009: step 12941, loss 0.0106935, acc 1
2016-09-06T11:56:47.078517: step 12942, loss 0.0289692, acc 1
2016-09-06T11:56:47.889937: step 12943, loss 0.0240166, acc 0.98
2016-09-06T11:56:48.698315: step 12944, loss 0.00246389, acc 1
2016-09-06T11:56:49.497296: step 12945, loss 0.00468899, acc 1
2016-09-06T11:56:50.280084: step 12946, loss 0.00907793, acc 1
2016-09-06T11:56:51.109646: step 12947, loss 0.0173335, acc 1
2016-09-06T11:56:51.884571: step 12948, loss 0.0124074, acc 1
2016-09-06T11:56:52.680958: step 12949, loss 0.0172112, acc 1
2016-09-06T11:56:53.538459: step 12950, loss 0.0611538, acc 0.98
2016-09-06T11:56:54.371816: step 12951, loss 0.00230367, acc 1
2016-09-06T11:56:55.204085: step 12952, loss 0.0181091, acc 1
2016-09-06T11:56:56.067880: step 12953, loss 0.0269605, acc 0.98
2016-09-06T11:56:56.874413: step 12954, loss 0.00243955, acc 1
2016-09-06T11:56:57.685825: step 12955, loss 0.0181866, acc 1
2016-09-06T11:56:58.510624: step 12956, loss 0.00195446, acc 1
2016-09-06T11:56:59.338784: step 12957, loss 0.0168373, acc 1
2016-09-06T11:57:00.151169: step 12958, loss 0.00237427, acc 1
2016-09-06T11:57:01.023263: step 12959, loss 0.0168611, acc 0.98
2016-09-06T11:57:01.822686: step 12960, loss 0.00209653, acc 1
2016-09-06T11:57:02.632863: step 12961, loss 0.0105575, acc 1
2016-09-06T11:57:03.457885: step 12962, loss 0.0230611, acc 1
2016-09-06T11:57:04.275408: step 12963, loss 0.0022835, acc 1
2016-09-06T11:57:05.076965: step 12964, loss 0.0437557, acc 0.98
2016-09-06T11:57:05.923831: step 12965, loss 0.0138012, acc 1
2016-09-06T11:57:06.742579: step 12966, loss 0.00243439, acc 1
2016-09-06T11:57:07.536324: step 12967, loss 0.00580576, acc 1
2016-09-06T11:57:08.346923: step 12968, loss 0.00539585, acc 1
2016-09-06T11:57:09.172728: step 12969, loss 0.0182039, acc 0.98
2016-09-06T11:57:09.949351: step 12970, loss 0.00300406, acc 1
2016-09-06T11:57:10.755991: step 12971, loss 0.0311575, acc 0.98
2016-09-06T11:57:11.529893: step 12972, loss 0.0176964, acc 0.98
2016-09-06T11:57:12.362577: step 12973, loss 0.0195096, acc 1
2016-09-06T11:57:13.183970: step 12974, loss 0.0161191, acc 1
2016-09-06T11:57:13.991627: step 12975, loss 0.00227996, acc 1
2016-09-06T11:57:14.812509: step 12976, loss 0.0082232, acc 1
2016-09-06T11:57:15.604278: step 12977, loss 0.00229901, acc 1
2016-09-06T11:57:16.432123: step 12978, loss 0.00225438, acc 1
2016-09-06T11:57:17.207677: step 12979, loss 0.0521391, acc 0.98
2016-09-06T11:57:18.016755: step 12980, loss 0.0176176, acc 0.98
2016-09-06T11:57:18.814238: step 12981, loss 0.0125489, acc 1
2016-09-06T11:57:19.652578: step 12982, loss 0.00226807, acc 1
2016-09-06T11:57:20.471713: step 12983, loss 0.0757474, acc 0.98
2016-09-06T11:57:21.320390: step 12984, loss 0.00493719, acc 1
2016-09-06T11:57:22.132219: step 12985, loss 0.0299165, acc 0.98
2016-09-06T11:57:22.921448: step 12986, loss 0.00769922, acc 1
2016-09-06T11:57:23.750139: step 12987, loss 0.0159004, acc 1
2016-09-06T11:57:24.560584: step 12988, loss 0.00280407, acc 1
2016-09-06T11:57:25.362439: step 12989, loss 0.0187898, acc 0.98
2016-09-06T11:57:26.201679: step 12990, loss 0.00524313, acc 1
2016-09-06T11:57:27.016846: step 12991, loss 0.0258016, acc 0.98
2016-09-06T11:57:27.827210: step 12992, loss 0.00541506, acc 1
2016-09-06T11:57:28.679829: step 12993, loss 0.00476817, acc 1
2016-09-06T11:57:29.498457: step 12994, loss 0.0102198, acc 1
2016-09-06T11:57:30.302501: step 12995, loss 0.00615981, acc 1
2016-09-06T11:57:31.137411: step 12996, loss 0.00613939, acc 1
2016-09-06T11:57:31.961911: step 12997, loss 0.069047, acc 0.96
2016-09-06T11:57:32.804169: step 12998, loss 0.0364113, acc 0.98
2016-09-06T11:57:33.630652: step 12999, loss 0.0041884, acc 1
2016-09-06T11:57:34.438311: step 13000, loss 0.0105877, acc 1

Evaluation:
2016-09-06T11:57:38.148970: step 13000, loss 2.46657, acc 0.742026

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-13000

2016-09-06T11:57:40.189943: step 13001, loss 0.0334788, acc 0.98
2016-09-06T11:57:40.997802: step 13002, loss 0.0393295, acc 1
2016-09-06T11:57:41.828902: step 13003, loss 0.00800616, acc 1
2016-09-06T11:57:42.615635: step 13004, loss 0.00489198, acc 1
2016-09-06T11:57:43.443321: step 13005, loss 0.0190952, acc 0.98
2016-09-06T11:57:44.266656: step 13006, loss 0.0411162, acc 0.98
2016-09-06T11:57:45.082925: step 13007, loss 0.0631967, acc 0.98
2016-09-06T11:57:45.935541: step 13008, loss 0.0331558, acc 0.98
2016-09-06T11:57:46.750272: step 13009, loss 0.0375957, acc 0.98
2016-09-06T11:57:47.576454: step 13010, loss 0.0888005, acc 0.96
2016-09-06T11:57:48.384851: step 13011, loss 0.0167564, acc 1
2016-09-06T11:57:49.205625: step 13012, loss 0.00385326, acc 1
2016-09-06T11:57:50.024568: step 13013, loss 0.0054055, acc 1
2016-09-06T11:57:50.852457: step 13014, loss 0.0259491, acc 0.98
2016-09-06T11:57:51.682327: step 13015, loss 0.00309692, acc 1
2016-09-06T11:57:52.484402: step 13016, loss 0.00324172, acc 1
2016-09-06T11:57:53.283035: step 13017, loss 0.0088497, acc 1
2016-09-06T11:57:54.131707: step 13018, loss 0.015569, acc 1
2016-09-06T11:57:54.923541: step 13019, loss 0.0128887, acc 1
2016-09-06T11:57:55.726298: step 13020, loss 0.0278551, acc 0.98
2016-09-06T11:57:56.548252: step 13021, loss 0.0042889, acc 1
2016-09-06T11:57:57.362127: step 13022, loss 0.0814421, acc 0.98
2016-09-06T11:57:58.185438: step 13023, loss 0.00398142, acc 1
2016-09-06T11:57:59.027359: step 13024, loss 0.0265719, acc 0.98
2016-09-06T11:57:59.820058: step 13025, loss 0.0668757, acc 0.96
2016-09-06T11:58:00.640546: step 13026, loss 0.00454305, acc 1
2016-09-06T11:58:01.472566: step 13027, loss 0.0316885, acc 0.98
2016-09-06T11:58:02.294799: step 13028, loss 0.0042177, acc 1
2016-09-06T11:58:03.118786: step 13029, loss 0.00285392, acc 1
2016-09-06T11:58:03.955668: step 13030, loss 0.0045188, acc 1
2016-09-06T11:58:04.767505: step 13031, loss 0.0136597, acc 1
2016-09-06T11:58:05.574336: step 13032, loss 0.0208369, acc 1
2016-09-06T11:58:06.407697: step 13033, loss 0.0289741, acc 0.98
2016-09-06T11:58:07.239142: step 13034, loss 0.00323644, acc 1
2016-09-06T11:58:08.030935: step 13035, loss 0.024468, acc 0.98
2016-09-06T11:58:08.840464: step 13036, loss 0.0530745, acc 0.96
2016-09-06T11:58:09.664304: step 13037, loss 0.00261387, acc 1
2016-09-06T11:58:10.455604: step 13038, loss 0.00750828, acc 1
2016-09-06T11:58:11.272757: step 13039, loss 0.0171375, acc 1
2016-09-06T11:58:12.098525: step 13040, loss 0.00505859, acc 1
2016-09-06T11:58:12.899543: step 13041, loss 0.0214147, acc 0.98
2016-09-06T11:58:13.717483: step 13042, loss 0.0558409, acc 0.96
2016-09-06T11:58:14.543001: step 13043, loss 0.0162379, acc 1
2016-09-06T11:58:15.334294: step 13044, loss 0.041427, acc 0.98
2016-09-06T11:58:16.133412: step 13045, loss 0.0148634, acc 1
2016-09-06T11:58:16.968411: step 13046, loss 0.0123663, acc 1
2016-09-06T11:58:17.780253: step 13047, loss 0.0251017, acc 0.98
2016-09-06T11:58:18.573133: step 13048, loss 0.00418829, acc 1
2016-09-06T11:58:19.389788: step 13049, loss 0.0298797, acc 0.98
2016-09-06T11:58:20.180825: step 13050, loss 0.019053, acc 0.98
2016-09-06T11:58:20.972724: step 13051, loss 0.0164872, acc 1
2016-09-06T11:58:21.798255: step 13052, loss 0.00667364, acc 1
2016-09-06T11:58:22.565336: step 13053, loss 0.0171508, acc 1
2016-09-06T11:58:23.356709: step 13054, loss 0.00648178, acc 1
2016-09-06T11:58:24.187488: step 13055, loss 0.00268951, acc 1
2016-09-06T11:58:24.923051: step 13056, loss 0.0166282, acc 1
2016-09-06T11:58:25.752169: step 13057, loss 0.0273236, acc 1
2016-09-06T11:58:26.597429: step 13058, loss 0.0184316, acc 1
2016-09-06T11:58:27.387333: step 13059, loss 0.00237845, acc 1
2016-09-06T11:58:28.195699: step 13060, loss 0.0111103, acc 1
2016-09-06T11:58:29.014586: step 13061, loss 0.0034649, acc 1
2016-09-06T11:58:29.794717: step 13062, loss 0.0436827, acc 0.98
2016-09-06T11:58:30.609331: step 13063, loss 0.0338292, acc 0.98
2016-09-06T11:58:31.446533: step 13064, loss 0.0108244, acc 1
2016-09-06T11:58:32.252733: step 13065, loss 0.0242004, acc 0.98
2016-09-06T11:58:33.058431: step 13066, loss 0.00664627, acc 1
2016-09-06T11:58:33.876621: step 13067, loss 0.0024894, acc 1
2016-09-06T11:58:34.653498: step 13068, loss 0.00244095, acc 1
2016-09-06T11:58:35.443299: step 13069, loss 0.0384767, acc 0.98
2016-09-06T11:58:36.247124: step 13070, loss 0.0181071, acc 0.98
2016-09-06T11:58:37.042242: step 13071, loss 0.0106478, acc 1
2016-09-06T11:58:37.840355: step 13072, loss 0.0273801, acc 0.98
2016-09-06T11:58:38.671200: step 13073, loss 0.0375616, acc 0.98
2016-09-06T11:58:39.464188: step 13074, loss 0.00220671, acc 1
2016-09-06T11:58:40.266467: step 13075, loss 0.018545, acc 1
2016-09-06T11:58:41.080847: step 13076, loss 0.0407504, acc 0.98
2016-09-06T11:58:41.879180: step 13077, loss 0.00216419, acc 1
2016-09-06T11:58:42.686684: step 13078, loss 0.00217599, acc 1
2016-09-06T11:58:43.527052: step 13079, loss 0.00758344, acc 1
2016-09-06T11:58:44.293057: step 13080, loss 0.00832017, acc 1
2016-09-06T11:58:45.085696: step 13081, loss 0.00201089, acc 1
2016-09-06T11:58:45.891190: step 13082, loss 0.0125578, acc 1
2016-09-06T11:58:46.703807: step 13083, loss 0.00313896, acc 1
2016-09-06T11:58:47.515335: step 13084, loss 0.00203926, acc 1
2016-09-06T11:58:48.353484: step 13085, loss 0.0299986, acc 0.98
2016-09-06T11:58:49.129994: step 13086, loss 0.0257241, acc 0.98
2016-09-06T11:58:49.935473: step 13087, loss 0.00194011, acc 1
2016-09-06T11:58:50.779772: step 13088, loss 0.0196742, acc 0.98
2016-09-06T11:58:51.535960: step 13089, loss 0.0112901, acc 1
2016-09-06T11:58:52.324840: step 13090, loss 0.022492, acc 1
2016-09-06T11:58:53.157724: step 13091, loss 0.0229018, acc 0.98
2016-09-06T11:58:53.941298: step 13092, loss 0.0018849, acc 1
2016-09-06T11:58:54.744684: step 13093, loss 0.0179718, acc 0.98
2016-09-06T11:58:55.555214: step 13094, loss 0.0263714, acc 0.98
2016-09-06T11:58:56.359256: step 13095, loss 0.0249357, acc 0.98
2016-09-06T11:58:57.159854: step 13096, loss 0.00765758, acc 1
2016-09-06T11:58:57.954141: step 13097, loss 0.037653, acc 0.98
2016-09-06T11:58:58.734802: step 13098, loss 0.0120182, acc 1
2016-09-06T11:58:59.559214: step 13099, loss 0.0110273, acc 1
2016-09-06T11:59:00.399877: step 13100, loss 0.00496194, acc 1

Evaluation:
2016-09-06T11:59:04.140348: step 13100, loss 1.94988, acc 0.750469

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-13100

2016-09-06T11:59:06.056128: step 13101, loss 0.00182027, acc 1
2016-09-06T11:59:06.884202: step 13102, loss 0.00191836, acc 1
2016-09-06T11:59:07.703810: step 13103, loss 0.0135657, acc 1
2016-09-06T11:59:08.548590: step 13104, loss 0.00289724, acc 1
2016-09-06T11:59:09.360376: step 13105, loss 0.042599, acc 0.98
2016-09-06T11:59:10.162526: step 13106, loss 0.00211564, acc 1
2016-09-06T11:59:10.986007: step 13107, loss 0.0108594, acc 1
2016-09-06T11:59:11.791240: step 13108, loss 0.00196341, acc 1
2016-09-06T11:59:12.611804: step 13109, loss 0.0857351, acc 0.96
2016-09-06T11:59:13.417350: step 13110, loss 0.0156726, acc 1
2016-09-06T11:59:14.261207: step 13111, loss 0.0591552, acc 0.96
2016-09-06T11:59:15.091462: step 13112, loss 0.00188377, acc 1
2016-09-06T11:59:15.924301: step 13113, loss 0.00906218, acc 1
2016-09-06T11:59:16.729035: step 13114, loss 0.0487594, acc 0.98
2016-09-06T11:59:17.554463: step 13115, loss 0.00188176, acc 1
2016-09-06T11:59:18.400496: step 13116, loss 0.0018118, acc 1
2016-09-06T11:59:19.225073: step 13117, loss 0.0310612, acc 0.98
2016-09-06T11:59:20.064947: step 13118, loss 0.00755696, acc 1
2016-09-06T11:59:20.865628: step 13119, loss 0.00395874, acc 1
2016-09-06T11:59:21.690288: step 13120, loss 0.0167599, acc 1
2016-09-06T11:59:22.541028: step 13121, loss 0.0264769, acc 0.98
2016-09-06T11:59:23.336353: step 13122, loss 0.00616086, acc 1
2016-09-06T11:59:24.133525: step 13123, loss 0.0175541, acc 1
2016-09-06T11:59:24.958067: step 13124, loss 0.00757893, acc 1
2016-09-06T11:59:25.770279: step 13125, loss 0.0299473, acc 0.98
2016-09-06T11:59:26.582421: step 13126, loss 0.0070566, acc 1
2016-09-06T11:59:27.433405: step 13127, loss 0.00983496, acc 1
2016-09-06T11:59:28.261453: step 13128, loss 0.00205432, acc 1
2016-09-06T11:59:29.053504: step 13129, loss 0.00209697, acc 1
2016-09-06T11:59:29.897546: step 13130, loss 0.00212581, acc 1
2016-09-06T11:59:30.789178: step 13131, loss 0.0328797, acc 0.98
2016-09-06T11:59:31.560428: step 13132, loss 0.0115114, acc 1
2016-09-06T11:59:32.373762: step 13133, loss 0.00306068, acc 1
2016-09-06T11:59:33.201729: step 13134, loss 0.0345665, acc 0.98
2016-09-06T11:59:33.992676: step 13135, loss 0.0426012, acc 0.98
2016-09-06T11:59:34.823067: step 13136, loss 0.00367842, acc 1
2016-09-06T11:59:35.630403: step 13137, loss 0.0143554, acc 1
2016-09-06T11:59:36.438272: step 13138, loss 0.0139845, acc 1
2016-09-06T11:59:37.261731: step 13139, loss 0.016152, acc 1
2016-09-06T11:59:38.095886: step 13140, loss 0.00311063, acc 1
2016-09-06T11:59:38.912331: step 13141, loss 0.0248285, acc 1
2016-09-06T11:59:39.755388: step 13142, loss 0.0284155, acc 1
2016-09-06T11:59:40.568700: step 13143, loss 0.0178597, acc 0.98
2016-09-06T11:59:41.381441: step 13144, loss 0.0637399, acc 0.96
2016-09-06T11:59:42.223675: step 13145, loss 0.0101054, acc 1
2016-09-06T11:59:43.057829: step 13146, loss 0.0165719, acc 0.98
2016-09-06T11:59:43.847072: step 13147, loss 0.0173901, acc 1
2016-09-06T11:59:44.690452: step 13148, loss 0.00241654, acc 1
2016-09-06T11:59:45.536988: step 13149, loss 0.00524481, acc 1
2016-09-06T11:59:46.342311: step 13150, loss 0.0020955, acc 1
2016-09-06T11:59:47.161094: step 13151, loss 0.0134485, acc 1
2016-09-06T11:59:48.008563: step 13152, loss 0.00223951, acc 1
2016-09-06T11:59:48.811705: step 13153, loss 0.00202938, acc 1
2016-09-06T11:59:49.608577: step 13154, loss 0.00993232, acc 1
2016-09-06T11:59:50.453517: step 13155, loss 0.00279788, acc 1
2016-09-06T11:59:51.274122: step 13156, loss 0.00593637, acc 1
2016-09-06T11:59:52.067213: step 13157, loss 0.00442715, acc 1
2016-09-06T11:59:52.871807: step 13158, loss 0.0021614, acc 1
2016-09-06T11:59:53.683387: step 13159, loss 0.00206061, acc 1
2016-09-06T11:59:54.466191: step 13160, loss 0.0383432, acc 0.98
2016-09-06T11:59:55.286218: step 13161, loss 0.016452, acc 1
2016-09-06T11:59:56.113313: step 13162, loss 0.0596537, acc 0.96
2016-09-06T11:59:56.917535: step 13163, loss 0.0311231, acc 1
2016-09-06T11:59:57.721256: step 13164, loss 0.011214, acc 1
2016-09-06T11:59:58.529482: step 13165, loss 0.0631222, acc 0.96
2016-09-06T11:59:59.326901: step 13166, loss 0.020169, acc 0.98
2016-09-06T12:00:00.132154: step 13167, loss 0.00837832, acc 1
2016-09-06T12:00:01.015957: step 13168, loss 0.0692406, acc 0.96
2016-09-06T12:00:01.816881: step 13169, loss 0.00358575, acc 1
2016-09-06T12:00:02.613141: step 13170, loss 0.011599, acc 1
2016-09-06T12:00:03.401468: step 13171, loss 0.0184927, acc 1
2016-09-06T12:00:04.198234: step 13172, loss 0.018034, acc 1
2016-09-06T12:00:05.035521: step 13173, loss 0.0363465, acc 0.96
2016-09-06T12:00:05.845428: step 13174, loss 0.00222223, acc 1
2016-09-06T12:00:06.617516: step 13175, loss 0.0466099, acc 0.98
2016-09-06T12:00:07.435970: step 13176, loss 0.0424562, acc 0.96
2016-09-06T12:00:08.257079: step 13177, loss 0.0387253, acc 0.98
2016-09-06T12:00:09.056526: step 13178, loss 0.0151532, acc 1
2016-09-06T12:00:09.834413: step 13179, loss 0.0653147, acc 0.98
2016-09-06T12:00:10.639746: step 13180, loss 0.00203529, acc 1
2016-09-06T12:00:11.460236: step 13181, loss 0.0103727, acc 1
2016-09-06T12:00:12.263914: step 13182, loss 0.0111478, acc 1
2016-09-06T12:00:13.073446: step 13183, loss 0.0263056, acc 0.98
2016-09-06T12:00:13.862801: step 13184, loss 0.00200283, acc 1
2016-09-06T12:00:14.656018: step 13185, loss 0.0109139, acc 1
2016-09-06T12:00:15.465857: step 13186, loss 0.0192325, acc 1
2016-09-06T12:00:16.242034: step 13187, loss 0.0065158, acc 1
2016-09-06T12:00:17.051754: step 13188, loss 0.0274157, acc 0.98
2016-09-06T12:00:17.871998: step 13189, loss 0.00308529, acc 1
2016-09-06T12:00:18.668504: step 13190, loss 0.0505116, acc 0.96
2016-09-06T12:00:19.476902: step 13191, loss 0.0178557, acc 1
2016-09-06T12:00:20.289422: step 13192, loss 0.00216232, acc 1
2016-09-06T12:00:21.089663: step 13193, loss 0.00417741, acc 1
2016-09-06T12:00:21.885767: step 13194, loss 0.039996, acc 0.98
2016-09-06T12:00:22.697568: step 13195, loss 0.00650672, acc 1
2016-09-06T12:00:23.487940: step 13196, loss 0.00200922, acc 1
2016-09-06T12:00:24.301911: step 13197, loss 0.015931, acc 0.98
2016-09-06T12:00:25.119432: step 13198, loss 0.00571021, acc 1
2016-09-06T12:00:25.899981: step 13199, loss 0.00210556, acc 1
2016-09-06T12:00:26.694782: step 13200, loss 0.0480388, acc 0.96

Evaluation:
2016-09-06T12:00:30.416907: step 13200, loss 2.19723, acc 0.752345

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-13200

2016-09-06T12:00:32.290186: step 13201, loss 0.00232963, acc 1
2016-09-06T12:00:33.104544: step 13202, loss 0.0183334, acc 0.98
2016-09-06T12:00:33.930732: step 13203, loss 0.0658214, acc 0.98
2016-09-06T12:00:34.802138: step 13204, loss 0.00223505, acc 1
2016-09-06T12:00:35.614442: step 13205, loss 0.0171372, acc 1
2016-09-06T12:00:36.433998: step 13206, loss 0.0121925, acc 1
2016-09-06T12:00:37.251807: step 13207, loss 0.00842951, acc 1
2016-09-06T12:00:38.047554: step 13208, loss 0.0023922, acc 1
2016-09-06T12:00:38.855630: step 13209, loss 0.0135537, acc 1
2016-09-06T12:00:39.686875: step 13210, loss 0.0758434, acc 0.96
2016-09-06T12:00:40.515090: step 13211, loss 0.00406594, acc 1
2016-09-06T12:00:41.354786: step 13212, loss 0.024467, acc 1
2016-09-06T12:00:42.176055: step 13213, loss 0.0201085, acc 1
2016-09-06T12:00:42.956031: step 13214, loss 0.00337909, acc 1
2016-09-06T12:00:43.775726: step 13215, loss 0.00787417, acc 1
2016-09-06T12:00:44.607621: step 13216, loss 0.0242418, acc 0.98
2016-09-06T12:00:45.410829: step 13217, loss 0.0185093, acc 1
2016-09-06T12:00:46.207188: step 13218, loss 0.005021, acc 1
2016-09-06T12:00:46.983921: step 13219, loss 0.0107571, acc 1
2016-09-06T12:00:47.810224: step 13220, loss 0.0308808, acc 0.98
2016-09-06T12:00:48.645622: step 13221, loss 0.00180463, acc 1
2016-09-06T12:00:49.507464: step 13222, loss 0.0409802, acc 0.96
2016-09-06T12:00:50.305115: step 13223, loss 0.00381941, acc 1
2016-09-06T12:00:51.120787: step 13224, loss 0.00572906, acc 1
2016-09-06T12:00:51.977202: step 13225, loss 0.0212099, acc 1
2016-09-06T12:00:52.787903: step 13226, loss 0.00972075, acc 1
2016-09-06T12:00:53.590596: step 13227, loss 0.00475201, acc 1
2016-09-06T12:00:54.404520: step 13228, loss 0.00374944, acc 1
2016-09-06T12:00:55.196099: step 13229, loss 0.0270759, acc 0.98
2016-09-06T12:00:55.985946: step 13230, loss 0.00273318, acc 1
2016-09-06T12:00:56.825307: step 13231, loss 0.0764362, acc 0.98
2016-09-06T12:00:57.628537: step 13232, loss 0.0019875, acc 1
2016-09-06T12:00:58.435319: step 13233, loss 0.040797, acc 0.98
2016-09-06T12:00:59.285498: step 13234, loss 0.0161928, acc 1
2016-09-06T12:01:00.095439: step 13235, loss 0.00189234, acc 1
2016-09-06T12:01:00.973482: step 13236, loss 0.0066527, acc 1
2016-09-06T12:01:01.826018: step 13237, loss 0.00583361, acc 1
2016-09-06T12:01:02.691808: step 13238, loss 0.00180561, acc 1
2016-09-06T12:01:03.503016: step 13239, loss 0.00245197, acc 1
2016-09-06T12:01:04.324942: step 13240, loss 0.00200876, acc 1
2016-09-06T12:01:05.145043: step 13241, loss 0.024038, acc 1
2016-09-06T12:01:05.951364: step 13242, loss 0.00256008, acc 1
2016-09-06T12:01:06.752274: step 13243, loss 0.0115346, acc 1
2016-09-06T12:01:07.574304: step 13244, loss 0.0265428, acc 0.98
2016-09-06T12:01:08.370928: step 13245, loss 0.0338722, acc 0.98
2016-09-06T12:01:09.188524: step 13246, loss 0.00887273, acc 1
2016-09-06T12:01:10.029406: step 13247, loss 0.0122802, acc 1
2016-09-06T12:01:10.762038: step 13248, loss 0.00236953, acc 1
2016-09-06T12:01:11.580985: step 13249, loss 0.0222564, acc 0.98
2016-09-06T12:01:12.377887: step 13250, loss 0.125689, acc 0.94
2016-09-06T12:01:13.199401: step 13251, loss 0.0315906, acc 0.98
2016-09-06T12:01:14.010605: step 13252, loss 0.0206243, acc 0.98
2016-09-06T12:01:14.826460: step 13253, loss 0.0543709, acc 0.98
2016-09-06T12:01:15.646335: step 13254, loss 0.00227622, acc 1
2016-09-06T12:01:16.441828: step 13255, loss 0.0270898, acc 0.98
2016-09-06T12:01:17.281404: step 13256, loss 0.0252294, acc 1
2016-09-06T12:01:18.039487: step 13257, loss 0.0493879, acc 1
2016-09-06T12:01:18.835704: step 13258, loss 0.0655292, acc 0.96
2016-09-06T12:01:19.652100: step 13259, loss 0.0015704, acc 1
2016-09-06T12:01:20.432219: step 13260, loss 0.00311098, acc 1
2016-09-06T12:01:21.234680: step 13261, loss 0.0286621, acc 0.98
2016-09-06T12:01:22.070222: step 13262, loss 0.00264252, acc 1
2016-09-06T12:01:22.879404: step 13263, loss 0.0224844, acc 0.98
2016-09-06T12:01:23.686345: step 13264, loss 0.00206168, acc 1
2016-09-06T12:01:24.505375: step 13265, loss 0.0579458, acc 0.98
2016-09-06T12:01:25.288821: step 13266, loss 0.00190857, acc 1
2016-09-06T12:01:26.106649: step 13267, loss 0.013175, acc 1
2016-09-06T12:01:26.932988: step 13268, loss 0.0059829, acc 1
2016-09-06T12:01:27.715593: step 13269, loss 0.00173969, acc 1
2016-09-06T12:01:28.523440: step 13270, loss 0.00413657, acc 1
2016-09-06T12:01:29.349720: step 13271, loss 0.00408341, acc 1
2016-09-06T12:01:30.165937: step 13272, loss 0.0201794, acc 0.98
2016-09-06T12:01:30.992407: step 13273, loss 0.00466245, acc 1
2016-09-06T12:01:31.785424: step 13274, loss 0.0291872, acc 1
2016-09-06T12:01:32.564901: step 13275, loss 0.00228403, acc 1
2016-09-06T12:01:33.415691: step 13276, loss 0.0030038, acc 1
2016-09-06T12:01:34.239746: step 13277, loss 0.0147917, acc 1
2016-09-06T12:01:35.050210: step 13278, loss 0.012684, acc 1
2016-09-06T12:01:35.855817: step 13279, loss 0.00225108, acc 1
2016-09-06T12:01:36.684414: step 13280, loss 0.0211698, acc 0.98
2016-09-06T12:01:37.495788: step 13281, loss 0.0132643, acc 1
2016-09-06T12:01:38.309370: step 13282, loss 0.0160062, acc 1
2016-09-06T12:01:39.132606: step 13283, loss 0.00742596, acc 1
2016-09-06T12:01:39.961446: step 13284, loss 0.00329338, acc 1
2016-09-06T12:01:40.757304: step 13285, loss 0.0246925, acc 1
2016-09-06T12:01:41.577938: step 13286, loss 0.00640763, acc 1
2016-09-06T12:01:42.378222: step 13287, loss 0.0179895, acc 1
2016-09-06T12:01:43.196317: step 13288, loss 0.00193695, acc 1
2016-09-06T12:01:44.025277: step 13289, loss 0.0201195, acc 1
2016-09-06T12:01:44.834488: step 13290, loss 0.00193914, acc 1
2016-09-06T12:01:45.648605: step 13291, loss 0.0309185, acc 0.98
2016-09-06T12:01:46.494609: step 13292, loss 0.00695207, acc 1
2016-09-06T12:01:47.300214: step 13293, loss 0.00323594, acc 1
2016-09-06T12:01:48.116301: step 13294, loss 0.0184342, acc 1
2016-09-06T12:01:48.938956: step 13295, loss 0.00200825, acc 1
2016-09-06T12:01:49.749623: step 13296, loss 0.0201165, acc 1
2016-09-06T12:01:50.570809: step 13297, loss 0.00569578, acc 1
2016-09-06T12:01:51.390351: step 13298, loss 0.0023792, acc 1
2016-09-06T12:01:52.189577: step 13299, loss 0.00677584, acc 1
2016-09-06T12:01:52.999339: step 13300, loss 0.027773, acc 1

Evaluation:
2016-09-06T12:01:56.751884: step 13300, loss 2.49837, acc 0.748593

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-13300

2016-09-06T12:01:58.805491: step 13301, loss 0.016668, acc 0.98
2016-09-06T12:01:59.613280: step 13302, loss 0.00557041, acc 1
2016-09-06T12:02:00.450163: step 13303, loss 0.00226344, acc 1
2016-09-06T12:02:01.247799: step 13304, loss 0.00510863, acc 1
2016-09-06T12:02:02.056698: step 13305, loss 0.00214508, acc 1
2016-09-06T12:02:02.832016: step 13306, loss 0.0597738, acc 0.98
2016-09-06T12:02:03.637092: step 13307, loss 0.0620023, acc 0.98
2016-09-06T12:02:04.474415: step 13308, loss 0.00698398, acc 1
2016-09-06T12:02:05.249668: step 13309, loss 0.0113903, acc 1
2016-09-06T12:02:06.041250: step 13310, loss 0.01234, acc 1
2016-09-06T12:02:06.868189: step 13311, loss 0.00209388, acc 1
2016-09-06T12:02:07.651400: step 13312, loss 0.0112274, acc 1
2016-09-06T12:02:08.494460: step 13313, loss 0.00278458, acc 1
2016-09-06T12:02:09.304580: step 13314, loss 0.0199778, acc 1
2016-09-06T12:02:10.096516: step 13315, loss 0.00690722, acc 1
2016-09-06T12:02:10.879626: step 13316, loss 0.00412752, acc 1
2016-09-06T12:02:11.714295: step 13317, loss 0.00634656, acc 1
2016-09-06T12:02:12.491541: step 13318, loss 0.066468, acc 0.96
2016-09-06T12:02:13.310779: step 13319, loss 0.0151061, acc 1
2016-09-06T12:02:14.124238: step 13320, loss 0.0062398, acc 1
2016-09-06T12:02:14.933579: step 13321, loss 0.0157993, acc 1
2016-09-06T12:02:15.761390: step 13322, loss 0.030204, acc 0.98
2016-09-06T12:02:16.588342: step 13323, loss 0.015991, acc 1
2016-09-06T12:02:17.390353: step 13324, loss 0.0267186, acc 1
2016-09-06T12:02:18.200861: step 13325, loss 0.00692007, acc 1
2016-09-06T12:02:19.026246: step 13326, loss 0.0202546, acc 0.98
2016-09-06T12:02:19.825096: step 13327, loss 0.00334596, acc 1
2016-09-06T12:02:20.628083: step 13328, loss 0.0150689, acc 1
2016-09-06T12:02:21.452522: step 13329, loss 0.0122422, acc 1
2016-09-06T12:02:22.292665: step 13330, loss 0.00376015, acc 1
2016-09-06T12:02:23.102107: step 13331, loss 0.028547, acc 0.98
2016-09-06T12:02:23.949134: step 13332, loss 0.00211204, acc 1
2016-09-06T12:02:24.754011: step 13333, loss 0.00724187, acc 1
2016-09-06T12:02:25.571181: step 13334, loss 0.0539347, acc 0.98
2016-09-06T12:02:26.391590: step 13335, loss 0.00196417, acc 1
2016-09-06T12:02:27.221357: step 13336, loss 0.0107491, acc 1
2016-09-06T12:02:28.036052: step 13337, loss 0.0240045, acc 0.98
2016-09-06T12:02:28.858569: step 13338, loss 0.00407416, acc 1
2016-09-06T12:02:29.677923: step 13339, loss 0.043931, acc 0.96
2016-09-06T12:02:30.498790: step 13340, loss 0.00327994, acc 1
2016-09-06T12:02:31.349306: step 13341, loss 0.00232243, acc 1
2016-09-06T12:02:32.158086: step 13342, loss 0.00437335, acc 1
2016-09-06T12:02:32.990124: step 13343, loss 0.0142531, acc 1
2016-09-06T12:02:33.853869: step 13344, loss 0.00233783, acc 1
2016-09-06T12:02:34.669450: step 13345, loss 0.00191715, acc 1
2016-09-06T12:02:35.521419: step 13346, loss 0.00466573, acc 1
2016-09-06T12:02:36.336534: step 13347, loss 0.00862063, acc 1
2016-09-06T12:02:37.180574: step 13348, loss 0.0416142, acc 0.98
2016-09-06T12:02:37.997459: step 13349, loss 0.00726207, acc 1
2016-09-06T12:02:38.836765: step 13350, loss 0.0154088, acc 1
2016-09-06T12:02:39.660312: step 13351, loss 0.0339924, acc 0.98
2016-09-06T12:02:40.454897: step 13352, loss 0.00189086, acc 1
2016-09-06T12:02:41.270974: step 13353, loss 0.0158918, acc 1
2016-09-06T12:02:42.118490: step 13354, loss 0.00269319, acc 1
2016-09-06T12:02:42.918559: step 13355, loss 0.275347, acc 0.96
2016-09-06T12:02:43.766406: step 13356, loss 0.00397373, acc 1
2016-09-06T12:02:44.589123: step 13357, loss 0.0262262, acc 0.98
2016-09-06T12:02:45.419237: step 13358, loss 0.0339855, acc 0.98
2016-09-06T12:02:46.232562: step 13359, loss 0.00411853, acc 1
2016-09-06T12:02:47.077452: step 13360, loss 0.0392859, acc 0.98
2016-09-06T12:02:47.888563: step 13361, loss 0.0137334, acc 1
2016-09-06T12:02:48.698349: step 13362, loss 0.00949549, acc 1
2016-09-06T12:02:49.511021: step 13363, loss 0.0159585, acc 1
2016-09-06T12:02:50.299888: step 13364, loss 0.0234726, acc 0.98
2016-09-06T12:02:51.122947: step 13365, loss 0.0165641, acc 0.98
2016-09-06T12:02:51.942932: step 13366, loss 0.00822214, acc 1
2016-09-06T12:02:52.762441: step 13367, loss 0.0325041, acc 0.98
2016-09-06T12:02:53.563582: step 13368, loss 0.0119267, acc 1
2016-09-06T12:02:54.379502: step 13369, loss 0.00269128, acc 1
2016-09-06T12:02:55.188445: step 13370, loss 0.00379225, acc 1
2016-09-06T12:02:55.987407: step 13371, loss 0.0101034, acc 1
2016-09-06T12:02:56.789428: step 13372, loss 0.00377557, acc 1
2016-09-06T12:02:57.594543: step 13373, loss 0.0201012, acc 0.98
2016-09-06T12:02:58.366389: step 13374, loss 0.0342167, acc 0.98
2016-09-06T12:02:59.193102: step 13375, loss 0.0528533, acc 0.98
2016-09-06T12:03:00.039142: step 13376, loss 0.0367297, acc 0.98
2016-09-06T12:03:00.895110: step 13377, loss 0.0241926, acc 0.98
2016-09-06T12:03:01.697123: step 13378, loss 0.00497924, acc 1
2016-09-06T12:03:02.517344: step 13379, loss 0.00288369, acc 1
2016-09-06T12:03:03.301189: step 13380, loss 0.00610918, acc 1
2016-09-06T12:03:04.079778: step 13381, loss 0.00417618, acc 1
2016-09-06T12:03:04.875955: step 13382, loss 0.0119297, acc 1
2016-09-06T12:03:05.658796: step 13383, loss 0.00478756, acc 1
2016-09-06T12:03:06.464473: step 13384, loss 0.00375733, acc 1
2016-09-06T12:03:07.274506: step 13385, loss 0.00477281, acc 1
2016-09-06T12:03:08.076994: step 13386, loss 0.0370571, acc 0.98
2016-09-06T12:03:08.903879: step 13387, loss 0.00301943, acc 1
2016-09-06T12:03:09.706861: step 13388, loss 0.0382406, acc 0.98
2016-09-06T12:03:10.498003: step 13389, loss 0.0466197, acc 0.98
2016-09-06T12:03:11.313765: step 13390, loss 0.0108132, acc 1
2016-09-06T12:03:12.169285: step 13391, loss 0.068502, acc 0.96
2016-09-06T12:03:12.953588: step 13392, loss 0.0285751, acc 0.98
2016-09-06T12:03:13.756427: step 13393, loss 0.00295286, acc 1
2016-09-06T12:03:14.591381: step 13394, loss 0.0297229, acc 0.98
2016-09-06T12:03:15.383458: step 13395, loss 0.00291075, acc 1
2016-09-06T12:03:16.177493: step 13396, loss 0.0105539, acc 1
2016-09-06T12:03:16.977823: step 13397, loss 0.0292061, acc 0.98
2016-09-06T12:03:17.741496: step 13398, loss 0.0157299, acc 1
2016-09-06T12:03:18.556909: step 13399, loss 0.00335776, acc 1
2016-09-06T12:03:19.378755: step 13400, loss 0.00248536, acc 1

Evaluation:
2016-09-06T12:03:23.119645: step 13400, loss 2.36819, acc 0.746717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-13400

2016-09-06T12:03:25.016591: step 13401, loss 0.00650872, acc 1
2016-09-06T12:03:25.821581: step 13402, loss 0.00273374, acc 1
2016-09-06T12:03:26.632511: step 13403, loss 0.0544545, acc 0.98
2016-09-06T12:03:27.463823: step 13404, loss 0.00737373, acc 1
2016-09-06T12:03:28.267406: step 13405, loss 0.00232665, acc 1
2016-09-06T12:03:29.073560: step 13406, loss 0.0312542, acc 0.98
2016-09-06T12:03:29.867964: step 13407, loss 0.0689667, acc 0.96
2016-09-06T12:03:30.689376: step 13408, loss 0.0182502, acc 1
2016-09-06T12:03:31.538314: step 13409, loss 0.0167567, acc 1
2016-09-06T12:03:32.321964: step 13410, loss 0.0155963, acc 1
2016-09-06T12:03:33.111368: step 13411, loss 0.0214832, acc 0.98
2016-09-06T12:03:33.944958: step 13412, loss 0.00208024, acc 1
2016-09-06T12:03:34.720891: step 13413, loss 0.00215047, acc 1
2016-09-06T12:03:35.526660: step 13414, loss 0.0318, acc 0.96
2016-09-06T12:03:36.359660: step 13415, loss 0.0028456, acc 1
2016-09-06T12:03:37.152655: step 13416, loss 0.0103246, acc 1
2016-09-06T12:03:37.951723: step 13417, loss 0.0280701, acc 0.98
2016-09-06T12:03:38.780925: step 13418, loss 0.169334, acc 0.98
2016-09-06T12:03:39.571942: step 13419, loss 0.0133217, acc 1
2016-09-06T12:03:40.373756: step 13420, loss 0.0241699, acc 0.98
2016-09-06T12:03:41.189005: step 13421, loss 0.00627097, acc 1
2016-09-06T12:03:41.983111: step 13422, loss 0.0812307, acc 0.98
2016-09-06T12:03:42.771885: step 13423, loss 0.0452287, acc 0.98
2016-09-06T12:03:43.612732: step 13424, loss 0.00269689, acc 1
2016-09-06T12:03:44.403881: step 13425, loss 0.0293677, acc 1
2016-09-06T12:03:45.172694: step 13426, loss 0.055292, acc 0.98
2016-09-06T12:03:45.997420: step 13427, loss 0.0186675, acc 0.98
2016-09-06T12:03:46.793109: step 13428, loss 0.0336703, acc 0.98
2016-09-06T12:03:47.613782: step 13429, loss 0.00320843, acc 1
2016-09-06T12:03:48.420841: step 13430, loss 0.0864596, acc 0.94
2016-09-06T12:03:49.197473: step 13431, loss 0.0205511, acc 0.98
2016-09-06T12:03:49.990976: step 13432, loss 0.0233226, acc 0.98
2016-09-06T12:03:50.831792: step 13433, loss 0.024395, acc 1
2016-09-06T12:03:51.635902: step 13434, loss 0.0286538, acc 0.98
2016-09-06T12:03:52.417845: step 13435, loss 0.0438372, acc 0.98
2016-09-06T12:03:53.259588: step 13436, loss 0.0206927, acc 0.98
2016-09-06T12:03:54.049394: step 13437, loss 0.00486201, acc 1
2016-09-06T12:03:54.839955: step 13438, loss 0.0322899, acc 0.98
2016-09-06T12:03:55.671328: step 13439, loss 0.0187829, acc 0.98
2016-09-06T12:03:56.425322: step 13440, loss 0.0164226, acc 1
2016-09-06T12:03:57.225232: step 13441, loss 0.0163248, acc 1
2016-09-06T12:03:58.041686: step 13442, loss 0.0148915, acc 1
2016-09-06T12:03:58.850056: step 13443, loss 0.00726054, acc 1
2016-09-06T12:03:59.653701: step 13444, loss 0.0148518, acc 1
2016-09-06T12:04:00.519722: step 13445, loss 0.00528332, acc 1
2016-09-06T12:04:01.317626: step 13446, loss 0.00387926, acc 1
2016-09-06T12:04:02.130233: step 13447, loss 0.0134609, acc 1
2016-09-06T12:04:02.989379: step 13448, loss 0.0216478, acc 0.98
2016-09-06T12:04:03.794372: step 13449, loss 0.1321, acc 0.94
2016-09-06T12:04:04.627023: step 13450, loss 0.0271512, acc 1
2016-09-06T12:04:05.458142: step 13451, loss 0.0161495, acc 1
2016-09-06T12:04:06.299398: step 13452, loss 0.00350946, acc 1
2016-09-06T12:04:07.116803: step 13453, loss 0.0315553, acc 0.98
2016-09-06T12:04:07.982355: step 13454, loss 0.00658638, acc 1
2016-09-06T12:04:08.791221: step 13455, loss 0.0624819, acc 0.98
2016-09-06T12:04:09.596445: step 13456, loss 0.0152032, acc 1
2016-09-06T12:04:10.469806: step 13457, loss 0.0149763, acc 1
2016-09-06T12:04:11.284670: step 13458, loss 0.00427422, acc 1
2016-09-06T12:04:12.088902: step 13459, loss 0.0160361, acc 1
2016-09-06T12:04:12.928910: step 13460, loss 0.00524183, acc 1
2016-09-06T12:04:13.742279: step 13461, loss 0.0218202, acc 0.98
2016-09-06T12:04:14.557448: step 13462, loss 0.0101047, acc 1
2016-09-06T12:04:15.401086: step 13463, loss 0.0200984, acc 0.98
2016-09-06T12:04:16.232661: step 13464, loss 0.0202066, acc 0.98
2016-09-06T12:04:17.041593: step 13465, loss 0.00431295, acc 1
2016-09-06T12:04:17.855414: step 13466, loss 0.0273346, acc 1
2016-09-06T12:04:18.739241: step 13467, loss 0.00448844, acc 1
2016-09-06T12:04:19.555161: step 13468, loss 0.00297302, acc 1
2016-09-06T12:04:20.386523: step 13469, loss 0.00667737, acc 1
2016-09-06T12:04:21.249702: step 13470, loss 0.00596589, acc 1
2016-09-06T12:04:22.067578: step 13471, loss 0.00292249, acc 1
2016-09-06T12:04:22.878486: step 13472, loss 0.0172532, acc 1
2016-09-06T12:04:23.710942: step 13473, loss 0.00327593, acc 1
2016-09-06T12:04:24.525557: step 13474, loss 0.00791392, acc 1
2016-09-06T12:04:25.309538: step 13475, loss 0.00282125, acc 1
2016-09-06T12:04:26.124184: step 13476, loss 0.00449961, acc 1
2016-09-06T12:04:26.934855: step 13477, loss 0.0131576, acc 1
2016-09-06T12:04:27.773997: step 13478, loss 0.0104925, acc 1
2016-09-06T12:04:28.581128: step 13479, loss 0.0296899, acc 0.98
2016-09-06T12:04:29.410452: step 13480, loss 0.0027204, acc 1
2016-09-06T12:04:30.230690: step 13481, loss 0.00605328, acc 1
2016-09-06T12:04:31.042841: step 13482, loss 0.0217113, acc 0.98
2016-09-06T12:04:31.842410: step 13483, loss 0.0026835, acc 1
2016-09-06T12:04:32.649288: step 13484, loss 0.00327695, acc 1
2016-09-06T12:04:33.471252: step 13485, loss 0.00313751, acc 1
2016-09-06T12:04:34.291455: step 13486, loss 0.0286868, acc 0.98
2016-09-06T12:04:35.087320: step 13487, loss 0.0183006, acc 1
2016-09-06T12:04:35.942180: step 13488, loss 0.0246599, acc 0.98
2016-09-06T12:04:36.756676: step 13489, loss 0.00299539, acc 1
2016-09-06T12:04:37.542687: step 13490, loss 0.00256987, acc 1
2016-09-06T12:04:38.352903: step 13491, loss 0.0208061, acc 0.98
2016-09-06T12:04:39.186732: step 13492, loss 0.0552705, acc 0.98
2016-09-06T12:04:39.985437: step 13493, loss 0.0289804, acc 0.98
2016-09-06T12:04:40.809876: step 13494, loss 0.00573337, acc 1
2016-09-06T12:04:41.670489: step 13495, loss 0.00741151, acc 1
2016-09-06T12:04:42.479382: step 13496, loss 0.089586, acc 0.96
2016-09-06T12:04:43.340560: step 13497, loss 0.00384634, acc 1
2016-09-06T12:04:44.171466: step 13498, loss 0.0291208, acc 0.98
2016-09-06T12:04:44.972152: step 13499, loss 0.00768156, acc 1
2016-09-06T12:04:45.776103: step 13500, loss 0.00293996, acc 1

Evaluation:
2016-09-06T12:04:49.516240: step 13500, loss 2.54288, acc 0.742026

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-13500

2016-09-06T12:04:51.425848: step 13501, loss 0.0131392, acc 1
2016-09-06T12:04:52.218477: step 13502, loss 0.00220854, acc 1
2016-09-06T12:04:53.008926: step 13503, loss 0.00217082, acc 1
2016-09-06T12:04:53.789470: step 13504, loss 0.00282772, acc 1
2016-09-06T12:04:54.570961: step 13505, loss 0.00671325, acc 1
2016-09-06T12:04:55.401810: step 13506, loss 0.0045249, acc 1
2016-09-06T12:04:56.241761: step 13507, loss 0.0153429, acc 1
2016-09-06T12:04:57.009527: step 13508, loss 0.106006, acc 0.96
2016-09-06T12:04:57.818002: step 13509, loss 0.0177172, acc 1
2016-09-06T12:04:58.621192: step 13510, loss 0.0282712, acc 0.98
2016-09-06T12:04:59.434069: step 13511, loss 0.0137228, acc 1
2016-09-06T12:05:00.280184: step 13512, loss 0.0138623, acc 1
2016-09-06T12:05:01.106938: step 13513, loss 0.010539, acc 1
2016-09-06T12:05:01.908230: step 13514, loss 0.0048564, acc 1
2016-09-06T12:05:02.717066: step 13515, loss 0.0350851, acc 0.98
2016-09-06T12:05:03.546116: step 13516, loss 0.0155347, acc 1
2016-09-06T12:05:04.369450: step 13517, loss 0.0372062, acc 0.98
2016-09-06T12:05:05.219456: step 13518, loss 0.00823676, acc 1
2016-09-06T12:05:06.054520: step 13519, loss 0.00191044, acc 1
2016-09-06T12:05:06.858130: step 13520, loss 0.035371, acc 0.98
2016-09-06T12:05:07.683266: step 13521, loss 0.0217583, acc 1
2016-09-06T12:05:08.551526: step 13522, loss 0.00215117, acc 1
2016-09-06T12:05:09.355169: step 13523, loss 0.0549726, acc 0.96
2016-09-06T12:05:10.160493: step 13524, loss 0.00184529, acc 1
2016-09-06T12:05:11.037700: step 13525, loss 0.00330486, acc 1
2016-09-06T12:05:11.855451: step 13526, loss 0.0458091, acc 0.98
2016-09-06T12:05:12.694150: step 13527, loss 0.00320776, acc 1
2016-09-06T12:05:13.471679: step 13528, loss 0.0176523, acc 0.98
2016-09-06T12:05:14.306935: step 13529, loss 0.0398699, acc 0.98
2016-09-06T12:05:15.103217: step 13530, loss 0.00195008, acc 1
2016-09-06T12:05:15.910879: step 13531, loss 0.0321109, acc 0.96
2016-09-06T12:05:16.728269: step 13532, loss 0.0534865, acc 0.96
2016-09-06T12:05:17.534280: step 13533, loss 0.0187042, acc 1
2016-09-06T12:05:18.350698: step 13534, loss 0.00180923, acc 1
2016-09-06T12:05:19.162393: step 13535, loss 0.0121926, acc 1
2016-09-06T12:05:19.951330: step 13536, loss 0.00174547, acc 1
2016-09-06T12:05:20.752640: step 13537, loss 0.00186595, acc 1
2016-09-06T12:05:21.579573: step 13538, loss 0.0243712, acc 0.98
2016-09-06T12:05:22.386187: step 13539, loss 0.00875045, acc 1
2016-09-06T12:05:23.187865: step 13540, loss 0.00560077, acc 1
2016-09-06T12:05:23.993641: step 13541, loss 0.0131556, acc 1
2016-09-06T12:05:24.809791: step 13542, loss 0.0153014, acc 1
2016-09-06T12:05:25.608955: step 13543, loss 0.011983, acc 1
2016-09-06T12:05:26.425400: step 13544, loss 0.0121916, acc 1
2016-09-06T12:05:27.199483: step 13545, loss 0.0269773, acc 0.98
2016-09-06T12:05:28.009681: step 13546, loss 0.00221753, acc 1
2016-09-06T12:05:28.860856: step 13547, loss 0.0589689, acc 0.98
2016-09-06T12:05:29.646140: step 13548, loss 0.0182353, acc 1
2016-09-06T12:05:30.440324: step 13549, loss 0.0280489, acc 1
2016-09-06T12:05:31.253445: step 13550, loss 0.0283854, acc 1
2016-09-06T12:05:32.043117: step 13551, loss 0.0111446, acc 1
2016-09-06T12:05:32.834392: step 13552, loss 0.0321731, acc 0.98
2016-09-06T12:05:33.708628: step 13553, loss 0.00844836, acc 1
2016-09-06T12:05:34.488796: step 13554, loss 0.00209518, acc 1
2016-09-06T12:05:35.274652: step 13555, loss 0.0185103, acc 0.98
2016-09-06T12:05:36.091430: step 13556, loss 0.015361, acc 1
2016-09-06T12:05:36.891447: step 13557, loss 0.00298041, acc 1
2016-09-06T12:05:37.699249: step 13558, loss 0.0116851, acc 1
2016-09-06T12:05:38.492443: step 13559, loss 0.0170259, acc 0.98
2016-09-06T12:05:39.307933: step 13560, loss 0.0108219, acc 1
2016-09-06T12:05:40.131717: step 13561, loss 0.00205631, acc 1
2016-09-06T12:05:40.939032: step 13562, loss 0.00220488, acc 1
2016-09-06T12:05:41.746422: step 13563, loss 0.00272469, acc 1
2016-09-06T12:05:42.529410: step 13564, loss 0.0614549, acc 0.98
2016-09-06T12:05:43.345909: step 13565, loss 0.014117, acc 1
2016-09-06T12:05:44.143983: step 13566, loss 0.00266524, acc 1
2016-09-06T12:05:44.922103: step 13567, loss 0.00912098, acc 1
2016-09-06T12:05:45.761282: step 13568, loss 0.00908209, acc 1
2016-09-06T12:05:46.541109: step 13569, loss 0.0347844, acc 0.98
2016-09-06T12:05:47.377325: step 13570, loss 0.0194603, acc 0.98
2016-09-06T12:05:48.202619: step 13571, loss 0.0101992, acc 1
2016-09-06T12:05:48.998419: step 13572, loss 0.00288529, acc 1
2016-09-06T12:05:49.792279: step 13573, loss 0.0437333, acc 0.98
2016-09-06T12:05:50.610966: step 13574, loss 0.150705, acc 0.96
2016-09-06T12:05:51.410317: step 13575, loss 0.0134014, acc 1
2016-09-06T12:05:52.221120: step 13576, loss 0.00252185, acc 1
2016-09-06T12:05:53.063608: step 13577, loss 0.0122514, acc 1
2016-09-06T12:05:53.902774: step 13578, loss 0.0735524, acc 0.98
2016-09-06T12:05:54.720891: step 13579, loss 0.00388925, acc 1
2016-09-06T12:05:55.563994: step 13580, loss 0.00245272, acc 1
2016-09-06T12:05:56.403727: step 13581, loss 0.00315483, acc 1
2016-09-06T12:05:57.245874: step 13582, loss 0.0181125, acc 0.98
2016-09-06T12:05:58.069145: step 13583, loss 0.0424809, acc 0.98
2016-09-06T12:05:58.894317: step 13584, loss 0.0484156, acc 0.96
2016-09-06T12:05:59.726599: step 13585, loss 0.0236165, acc 0.98
2016-09-06T12:06:00.599090: step 13586, loss 0.0717206, acc 0.96
2016-09-06T12:06:01.453452: step 13587, loss 0.0222349, acc 0.98
2016-09-06T12:06:02.238518: step 13588, loss 0.0166406, acc 1
2016-09-06T12:06:03.022878: step 13589, loss 0.0375941, acc 0.98
2016-09-06T12:06:03.843460: step 13590, loss 0.0329286, acc 0.98
2016-09-06T12:06:04.644939: step 13591, loss 0.0031244, acc 1
2016-09-06T12:06:05.456342: step 13592, loss 0.00412551, acc 1
2016-09-06T12:06:06.300262: step 13593, loss 0.0522288, acc 0.98
2016-09-06T12:06:07.075855: step 13594, loss 0.00361511, acc 1
2016-09-06T12:06:07.881808: step 13595, loss 0.0043357, acc 1
2016-09-06T12:06:08.704191: step 13596, loss 0.034382, acc 0.96
2016-09-06T12:06:09.491894: step 13597, loss 0.0319711, acc 0.98
2016-09-06T12:06:10.292035: step 13598, loss 0.0153196, acc 1
2016-09-06T12:06:11.116902: step 13599, loss 0.00367677, acc 1
2016-09-06T12:06:11.911718: step 13600, loss 0.0369366, acc 0.98

Evaluation:
2016-09-06T12:06:15.722316: step 13600, loss 1.63296, acc 0.756098

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-13600

2016-09-06T12:06:17.606234: step 13601, loss 0.0486308, acc 0.96
2016-09-06T12:06:18.422691: step 13602, loss 0.00870019, acc 1
2016-09-06T12:06:19.226035: step 13603, loss 0.00270082, acc 1
2016-09-06T12:06:20.042664: step 13604, loss 0.00280855, acc 1
2016-09-06T12:06:20.870399: step 13605, loss 0.00382847, acc 1
2016-09-06T12:06:21.697492: step 13606, loss 0.0166592, acc 1
2016-09-06T12:06:22.518505: step 13607, loss 0.00240012, acc 1
2016-09-06T12:06:23.350002: step 13608, loss 0.0242932, acc 1
2016-09-06T12:06:24.175128: step 13609, loss 0.0249197, acc 1
2016-09-06T12:06:25.001348: step 13610, loss 0.00907164, acc 1
2016-09-06T12:06:25.860955: step 13611, loss 0.00929728, acc 1
2016-09-06T12:06:26.676604: step 13612, loss 0.0200325, acc 1
2016-09-06T12:06:27.479114: step 13613, loss 0.00643831, acc 1
2016-09-06T12:06:28.309436: step 13614, loss 0.0170872, acc 0.98
2016-09-06T12:06:29.116359: step 13615, loss 0.00419545, acc 1
2016-09-06T12:06:29.937447: step 13616, loss 0.00739018, acc 1
2016-09-06T12:06:30.757732: step 13617, loss 0.00256745, acc 1
2016-09-06T12:06:31.563720: step 13618, loss 0.0219057, acc 1
2016-09-06T12:06:32.385412: step 13619, loss 0.0249959, acc 0.98
2016-09-06T12:06:33.203736: step 13620, loss 0.0169796, acc 0.98
2016-09-06T12:06:34.019641: step 13621, loss 0.00250353, acc 1
2016-09-06T12:06:34.876030: step 13622, loss 0.0184521, acc 0.98
2016-09-06T12:06:35.705416: step 13623, loss 0.00354284, acc 1
2016-09-06T12:06:36.538287: step 13624, loss 0.0174285, acc 1
2016-09-06T12:06:37.347698: step 13625, loss 0.00463308, acc 1
2016-09-06T12:06:38.146568: step 13626, loss 0.0335041, acc 0.96
2016-09-06T12:06:39.016286: step 13627, loss 0.0311318, acc 0.96
2016-09-06T12:06:39.854412: step 13628, loss 0.00253451, acc 1
2016-09-06T12:06:40.661002: step 13629, loss 0.0171582, acc 1
2016-09-06T12:06:41.531021: step 13630, loss 0.00865165, acc 1
2016-09-06T12:06:42.314268: step 13631, loss 0.00270665, acc 1
2016-09-06T12:06:43.081481: step 13632, loss 0.00245063, acc 1
2016-09-06T12:06:43.898612: step 13633, loss 0.00517087, acc 1
2016-09-06T12:06:44.694080: step 13634, loss 0.0199553, acc 0.98
2016-09-06T12:06:45.504536: step 13635, loss 0.234101, acc 0.98
2016-09-06T12:06:46.322910: step 13636, loss 0.00863395, acc 1
2016-09-06T12:06:47.124208: step 13637, loss 0.018445, acc 0.98
2016-09-06T12:06:47.916736: step 13638, loss 0.0417469, acc 0.96
2016-09-06T12:06:48.741254: step 13639, loss 0.0415404, acc 0.98
2016-09-06T12:06:49.553854: step 13640, loss 0.017299, acc 0.98
2016-09-06T12:06:50.362371: step 13641, loss 0.0169692, acc 1
2016-09-06T12:06:51.197196: step 13642, loss 0.0271307, acc 0.98
2016-09-06T12:06:52.010706: step 13643, loss 0.00202608, acc 1
2016-09-06T12:06:52.843494: step 13644, loss 0.0279592, acc 0.98
2016-09-06T12:06:53.665971: step 13645, loss 0.0613387, acc 0.98
2016-09-06T12:06:54.490499: step 13646, loss 0.0195236, acc 1
2016-09-06T12:06:55.311555: step 13647, loss 0.0497357, acc 0.96
2016-09-06T12:06:56.147838: step 13648, loss 0.044417, acc 0.98
2016-09-06T12:06:56.995360: step 13649, loss 0.0673705, acc 0.98
2016-09-06T12:06:57.806723: step 13650, loss 0.00269962, acc 1
2016-09-06T12:06:58.627383: step 13651, loss 0.0643864, acc 0.98
2016-09-06T12:06:59.456320: step 13652, loss 0.00284956, acc 1
2016-09-06T12:07:00.264439: step 13653, loss 0.0390267, acc 0.98
2016-09-06T12:07:01.073070: step 13654, loss 0.00315836, acc 1
2016-09-06T12:07:01.880225: step 13655, loss 0.0078485, acc 1
2016-09-06T12:07:02.681247: step 13656, loss 0.0220562, acc 1
2016-09-06T12:07:03.488431: step 13657, loss 0.0122622, acc 1
2016-09-06T12:07:04.318914: step 13658, loss 0.00961909, acc 1
2016-09-06T12:07:05.110178: step 13659, loss 0.0142744, acc 1
2016-09-06T12:07:05.958989: step 13660, loss 0.0119693, acc 1
2016-09-06T12:07:06.781345: step 13661, loss 0.0102791, acc 1
2016-09-06T12:07:07.574877: step 13662, loss 0.0157736, acc 1
2016-09-06T12:07:08.367379: step 13663, loss 0.00303317, acc 1
2016-09-06T12:07:09.183036: step 13664, loss 0.00985739, acc 1
2016-09-06T12:07:09.963623: step 13665, loss 0.0191825, acc 1
2016-09-06T12:07:10.771770: step 13666, loss 0.0119339, acc 1
2016-09-06T12:07:11.606230: step 13667, loss 0.00197529, acc 1
2016-09-06T12:07:12.378950: step 13668, loss 0.0139933, acc 1
2016-09-06T12:07:13.188932: step 13669, loss 0.0121162, acc 1
2016-09-06T12:07:14.013135: step 13670, loss 0.0310094, acc 0.98
2016-09-06T12:07:14.777873: step 13671, loss 0.00225669, acc 1
2016-09-06T12:07:15.580935: step 13672, loss 0.00436149, acc 1
2016-09-06T12:07:16.389552: step 13673, loss 0.0175295, acc 0.98
2016-09-06T12:07:17.186559: step 13674, loss 0.00356688, acc 1
2016-09-06T12:07:17.998694: step 13675, loss 0.00705731, acc 1
2016-09-06T12:07:18.795594: step 13676, loss 0.00229715, acc 1
2016-09-06T12:07:19.579476: step 13677, loss 0.00231056, acc 1
2016-09-06T12:07:20.428287: step 13678, loss 0.00589757, acc 1
2016-09-06T12:07:21.255754: step 13679, loss 0.00806687, acc 1
2016-09-06T12:07:22.029777: step 13680, loss 0.0266154, acc 1
2016-09-06T12:07:22.844146: step 13681, loss 0.122264, acc 0.98
2016-09-06T12:07:23.683798: step 13682, loss 0.00229954, acc 1
2016-09-06T12:07:24.479809: step 13683, loss 0.00877863, acc 1
2016-09-06T12:07:25.287314: step 13684, loss 0.0193575, acc 1
2016-09-06T12:07:26.099085: step 13685, loss 0.0114887, acc 1
2016-09-06T12:07:26.860779: step 13686, loss 0.00301357, acc 1
2016-09-06T12:07:27.653216: step 13687, loss 0.0112552, acc 1
2016-09-06T12:07:28.464735: step 13688, loss 0.00227066, acc 1
2016-09-06T12:07:29.280051: step 13689, loss 0.00286195, acc 1
2016-09-06T12:07:30.070606: step 13690, loss 0.0184244, acc 0.98
2016-09-06T12:07:30.912716: step 13691, loss 0.00265016, acc 1
2016-09-06T12:07:31.699282: step 13692, loss 0.00274597, acc 1
2016-09-06T12:07:32.478204: step 13693, loss 0.0158281, acc 1
2016-09-06T12:07:33.293881: step 13694, loss 0.00892564, acc 1
2016-09-06T12:07:34.061237: step 13695, loss 0.00234672, acc 1
2016-09-06T12:07:34.886358: step 13696, loss 0.0168033, acc 1
2016-09-06T12:07:35.713281: step 13697, loss 0.00934624, acc 1
2016-09-06T12:07:36.505750: step 13698, loss 0.0137808, acc 1
2016-09-06T12:07:37.314888: step 13699, loss 0.0157539, acc 1
2016-09-06T12:07:38.157981: step 13700, loss 0.0302991, acc 0.98

Evaluation:
2016-09-06T12:07:41.872476: step 13700, loss 2.31659, acc 0.751407

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-13700

2016-09-06T12:07:43.917488: step 13701, loss 0.0215411, acc 0.98
2016-09-06T12:07:44.748802: step 13702, loss 0.064299, acc 0.98
2016-09-06T12:07:45.557872: step 13703, loss 0.00253042, acc 1
2016-09-06T12:07:46.370457: step 13704, loss 0.0188482, acc 1
2016-09-06T12:07:47.189391: step 13705, loss 0.012532, acc 1
2016-09-06T12:07:48.026157: step 13706, loss 0.00231894, acc 1
2016-09-06T12:07:48.841614: step 13707, loss 0.0478476, acc 0.98
2016-09-06T12:07:49.661842: step 13708, loss 0.0161461, acc 0.98
2016-09-06T12:07:50.495369: step 13709, loss 0.0024361, acc 1
2016-09-06T12:07:51.311477: step 13710, loss 0.0309127, acc 0.98
2016-09-06T12:07:52.206001: step 13711, loss 0.00280843, acc 1
2016-09-06T12:07:53.033366: step 13712, loss 0.00287448, acc 1
2016-09-06T12:07:53.852195: step 13713, loss 0.0183334, acc 0.98
2016-09-06T12:07:54.662377: step 13714, loss 0.0108184, acc 1
2016-09-06T12:07:55.469235: step 13715, loss 0.00519665, acc 1
2016-09-06T12:07:56.292390: step 13716, loss 0.00212883, acc 1
2016-09-06T12:07:57.083372: step 13717, loss 0.00237561, acc 1
2016-09-06T12:07:57.897961: step 13718, loss 0.0337907, acc 0.98
2016-09-06T12:07:58.723344: step 13719, loss 0.00212746, acc 1
2016-09-06T12:07:59.503377: step 13720, loss 0.0305706, acc 0.98
2016-09-06T12:08:00.357791: step 13721, loss 0.0427201, acc 0.96
2016-09-06T12:08:01.165087: step 13722, loss 0.00211501, acc 1
2016-09-06T12:08:01.962038: step 13723, loss 0.0652458, acc 0.98
2016-09-06T12:08:02.771376: step 13724, loss 0.00193244, acc 1
2016-09-06T12:08:03.582548: step 13725, loss 0.00247381, acc 1
2016-09-06T12:08:04.353448: step 13726, loss 0.00255309, acc 1
2016-09-06T12:08:05.191318: step 13727, loss 0.0112376, acc 1
2016-09-06T12:08:05.996310: step 13728, loss 0.00201497, acc 1
2016-09-06T12:08:06.768984: step 13729, loss 0.00189054, acc 1
2016-09-06T12:08:07.569535: step 13730, loss 0.0176205, acc 1
2016-09-06T12:08:08.408936: step 13731, loss 0.0148523, acc 1
2016-09-06T12:08:09.197718: step 13732, loss 0.00497876, acc 1
2016-09-06T12:08:09.984035: step 13733, loss 0.00282023, acc 1
2016-09-06T12:08:10.788794: step 13734, loss 0.00204192, acc 1
2016-09-06T12:08:11.576974: step 13735, loss 0.0112709, acc 1
2016-09-06T12:08:12.401501: step 13736, loss 0.00183758, acc 1
2016-09-06T12:08:13.237609: step 13737, loss 0.0192039, acc 1
2016-09-06T12:08:14.007442: step 13738, loss 0.018181, acc 1
2016-09-06T12:08:14.823587: step 13739, loss 0.0341107, acc 0.96
2016-09-06T12:08:15.662721: step 13740, loss 0.0335532, acc 0.98
2016-09-06T12:08:16.462802: step 13741, loss 0.014815, acc 1
2016-09-06T12:08:17.286781: step 13742, loss 0.035594, acc 0.98
2016-09-06T12:08:18.169260: step 13743, loss 0.00201321, acc 1
2016-09-06T12:08:19.005138: step 13744, loss 0.00857428, acc 1
2016-09-06T12:08:19.810140: step 13745, loss 0.0077465, acc 1
2016-09-06T12:08:20.650133: step 13746, loss 0.00194345, acc 1
2016-09-06T12:08:21.453373: step 13747, loss 0.00226523, acc 1
2016-09-06T12:08:22.274061: step 13748, loss 0.0376587, acc 0.98
2016-09-06T12:08:23.134191: step 13749, loss 0.00646149, acc 1
2016-09-06T12:08:23.953049: step 13750, loss 0.00183906, acc 1
2016-09-06T12:08:24.770125: step 13751, loss 0.0151973, acc 1
2016-09-06T12:08:25.578613: step 13752, loss 0.0551535, acc 0.98
2016-09-06T12:08:26.405407: step 13753, loss 0.00280647, acc 1
2016-09-06T12:08:27.228391: step 13754, loss 0.00181089, acc 1
2016-09-06T12:08:28.058048: step 13755, loss 0.0146425, acc 1
2016-09-06T12:08:28.863651: step 13756, loss 0.0200773, acc 0.98
2016-09-06T12:08:29.709100: step 13757, loss 0.0391573, acc 1
2016-09-06T12:08:30.549637: step 13758, loss 0.0154798, acc 1
2016-09-06T12:08:31.373779: step 13759, loss 0.00656123, acc 1
2016-09-06T12:08:32.157788: step 13760, loss 0.0165109, acc 0.98
2016-09-06T12:08:32.980530: step 13761, loss 0.00242343, acc 1
2016-09-06T12:08:33.787658: step 13762, loss 0.0122265, acc 1
2016-09-06T12:08:34.563775: step 13763, loss 0.00791922, acc 1
2016-09-06T12:08:35.343527: step 13764, loss 0.00527477, acc 1
2016-09-06T12:08:36.180691: step 13765, loss 0.0154054, acc 1
2016-09-06T12:08:36.974671: step 13766, loss 0.00221595, acc 1
2016-09-06T12:08:37.802009: step 13767, loss 0.0197503, acc 0.98
2016-09-06T12:08:38.621754: step 13768, loss 0.00194008, acc 1
2016-09-06T12:08:39.393094: step 13769, loss 0.00359705, acc 1
2016-09-06T12:08:40.192968: step 13770, loss 0.0257332, acc 1
2016-09-06T12:08:41.037946: step 13771, loss 0.00406102, acc 1
2016-09-06T12:08:41.811347: step 13772, loss 0.00196866, acc 1
2016-09-06T12:08:42.637306: step 13773, loss 0.0178522, acc 1
2016-09-06T12:08:43.473199: step 13774, loss 0.0107211, acc 1
2016-09-06T12:08:44.273299: step 13775, loss 0.00286516, acc 1
2016-09-06T12:08:45.091267: step 13776, loss 0.0303416, acc 0.98
2016-09-06T12:08:45.925732: step 13777, loss 0.0103305, acc 1
2016-09-06T12:08:46.740979: step 13778, loss 0.0084983, acc 1
2016-09-06T12:08:47.564340: step 13779, loss 0.0319283, acc 0.98
2016-09-06T12:08:48.403417: step 13780, loss 0.00263879, acc 1
2016-09-06T12:08:49.193448: step 13781, loss 0.00205865, acc 1
2016-09-06T12:08:49.995731: step 13782, loss 0.0259506, acc 0.98
2016-09-06T12:08:50.843771: step 13783, loss 0.00846989, acc 1
2016-09-06T12:08:51.675173: step 13784, loss 0.00740064, acc 1
2016-09-06T12:08:52.551371: step 13785, loss 0.00206807, acc 1
2016-09-06T12:08:53.379012: step 13786, loss 0.00208921, acc 1
2016-09-06T12:08:54.233447: step 13787, loss 0.00233694, acc 1
2016-09-06T12:08:55.037695: step 13788, loss 0.0346775, acc 0.96
2016-09-06T12:08:55.859674: step 13789, loss 0.0689248, acc 0.98
2016-09-06T12:08:56.672433: step 13790, loss 0.0145922, acc 1
2016-09-06T12:08:57.497396: step 13791, loss 0.00248836, acc 1
2016-09-06T12:08:58.303477: step 13792, loss 0.0382155, acc 1
2016-09-06T12:08:59.112190: step 13793, loss 0.007599, acc 1
2016-09-06T12:08:59.942996: step 13794, loss 0.0130531, acc 1
2016-09-06T12:09:00.743239: step 13795, loss 0.00509262, acc 1
2016-09-06T12:09:01.575476: step 13796, loss 0.00187814, acc 1
2016-09-06T12:09:02.375427: step 13797, loss 0.00189828, acc 1
2016-09-06T12:09:03.202448: step 13798, loss 0.0283467, acc 0.98
2016-09-06T12:09:04.035331: step 13799, loss 0.0287832, acc 0.98
2016-09-06T12:09:04.834706: step 13800, loss 0.0348069, acc 0.98

Evaluation:
2016-09-06T12:09:08.590515: step 13800, loss 2.64394, acc 0.748593

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-13800

2016-09-06T12:09:10.532839: step 13801, loss 0.0264709, acc 1
2016-09-06T12:09:11.374644: step 13802, loss 0.00307521, acc 1
2016-09-06T12:09:12.172536: step 13803, loss 0.0184728, acc 0.98
2016-09-06T12:09:12.957800: step 13804, loss 0.0135413, acc 1
2016-09-06T12:09:13.783984: step 13805, loss 0.124377, acc 0.98
2016-09-06T12:09:14.612126: step 13806, loss 0.0262944, acc 0.98
2016-09-06T12:09:15.450327: step 13807, loss 0.0348406, acc 0.98
2016-09-06T12:09:16.268905: step 13808, loss 0.00643139, acc 1
2016-09-06T12:09:17.087405: step 13809, loss 0.0453935, acc 0.96
2016-09-06T12:09:17.886661: step 13810, loss 0.0032972, acc 1
2016-09-06T12:09:18.754814: step 13811, loss 0.0167516, acc 1
2016-09-06T12:09:19.567150: step 13812, loss 0.0299905, acc 0.98
2016-09-06T12:09:20.367088: step 13813, loss 0.002165, acc 1
2016-09-06T12:09:21.178210: step 13814, loss 0.0429189, acc 0.98
2016-09-06T12:09:21.984816: step 13815, loss 0.00656461, acc 1
2016-09-06T12:09:22.781084: step 13816, loss 0.00493993, acc 1
2016-09-06T12:09:23.613363: step 13817, loss 0.0092355, acc 1
2016-09-06T12:09:24.425134: step 13818, loss 0.0167487, acc 1
2016-09-06T12:09:25.253997: step 13819, loss 0.0483519, acc 0.98
2016-09-06T12:09:26.105635: step 13820, loss 0.0255457, acc 0.98
2016-09-06T12:09:26.902382: step 13821, loss 0.00976103, acc 1
2016-09-06T12:09:27.738006: step 13822, loss 0.00326852, acc 1
2016-09-06T12:09:28.591655: step 13823, loss 0.0452734, acc 0.98
2016-09-06T12:09:29.358871: step 13824, loss 0.00159685, acc 1
2016-09-06T12:09:30.144208: step 13825, loss 0.0381374, acc 0.98
2016-09-06T12:09:30.967387: step 13826, loss 0.0234606, acc 0.98
2016-09-06T12:09:31.775166: step 13827, loss 0.0153164, acc 1
2016-09-06T12:09:32.574623: step 13828, loss 0.00595453, acc 1
2016-09-06T12:09:33.403521: step 13829, loss 0.0221244, acc 1
2016-09-06T12:09:34.225490: step 13830, loss 0.0340859, acc 0.98
2016-09-06T12:09:34.998889: step 13831, loss 0.0242953, acc 0.98
2016-09-06T12:09:35.810519: step 13832, loss 0.0154772, acc 1
2016-09-06T12:09:36.630157: step 13833, loss 0.0258662, acc 1
2016-09-06T12:09:37.429674: step 13834, loss 0.0077521, acc 1
2016-09-06T12:09:38.236573: step 13835, loss 0.0231277, acc 0.98
2016-09-06T12:09:39.046498: step 13836, loss 0.00345297, acc 1
2016-09-06T12:09:39.865625: step 13837, loss 0.0373394, acc 0.96
2016-09-06T12:09:40.682193: step 13838, loss 0.0191145, acc 0.98
2016-09-06T12:09:41.500277: step 13839, loss 0.0285809, acc 1
2016-09-06T12:09:42.306823: step 13840, loss 0.00605314, acc 1
2016-09-06T12:09:43.091222: step 13841, loss 0.0081219, acc 1
2016-09-06T12:09:43.922796: step 13842, loss 0.00224009, acc 1
2016-09-06T12:09:44.701770: step 13843, loss 0.0270502, acc 0.98
2016-09-06T12:09:45.477966: step 13844, loss 0.0345307, acc 0.98
2016-09-06T12:09:46.288378: step 13845, loss 0.00258669, acc 1
2016-09-06T12:09:47.086729: step 13846, loss 0.015807, acc 1
2016-09-06T12:09:47.930828: step 13847, loss 0.00215354, acc 1
2016-09-06T12:09:48.740302: step 13848, loss 0.011142, acc 1
2016-09-06T12:09:49.510361: step 13849, loss 0.00547662, acc 1
2016-09-06T12:09:50.341910: step 13850, loss 0.00302709, acc 1
2016-09-06T12:09:51.179517: step 13851, loss 0.0172489, acc 0.98
2016-09-06T12:09:51.991707: step 13852, loss 0.0144488, acc 1
2016-09-06T12:09:52.799940: step 13853, loss 0.00221676, acc 1
2016-09-06T12:09:53.622864: step 13854, loss 0.0190757, acc 0.98
2016-09-06T12:09:54.419663: step 13855, loss 0.00220369, acc 1
2016-09-06T12:09:55.237267: step 13856, loss 0.0279199, acc 0.98
2016-09-06T12:09:56.104409: step 13857, loss 0.00604457, acc 1
2016-09-06T12:09:56.930178: step 13858, loss 0.00240608, acc 1
2016-09-06T12:09:57.732307: step 13859, loss 0.00286641, acc 1
2016-09-06T12:09:58.562082: step 13860, loss 0.00591619, acc 1
2016-09-06T12:09:59.393072: step 13861, loss 0.0112471, acc 1
2016-09-06T12:10:00.240546: step 13862, loss 0.00551966, acc 1
2016-09-06T12:10:01.056894: step 13863, loss 0.00404542, acc 1
2016-09-06T12:10:01.875789: step 13864, loss 0.019427, acc 0.98
2016-09-06T12:10:02.696074: step 13865, loss 0.00713345, acc 1
2016-09-06T12:10:03.560773: step 13866, loss 0.00546896, acc 1
2016-09-06T12:10:04.372280: step 13867, loss 0.0112246, acc 1
2016-09-06T12:10:05.172060: step 13868, loss 0.00347286, acc 1
2016-09-06T12:10:05.993495: step 13869, loss 0.0110207, acc 1
2016-09-06T12:10:06.788551: step 13870, loss 0.0150457, acc 1
2016-09-06T12:10:07.601387: step 13871, loss 0.00243201, acc 1
2016-09-06T12:10:08.424213: step 13872, loss 0.00202768, acc 1
2016-09-06T12:10:09.239833: step 13873, loss 0.00742292, acc 1
2016-09-06T12:10:10.057491: step 13874, loss 0.00445582, acc 1
2016-09-06T12:10:10.861206: step 13875, loss 0.0091287, acc 1
2016-09-06T12:10:11.699894: step 13876, loss 0.0226098, acc 1
2016-09-06T12:10:12.512886: step 13877, loss 0.0186168, acc 0.98
2016-09-06T12:10:13.288559: step 13878, loss 0.00266703, acc 1
2016-09-06T12:10:14.113580: step 13879, loss 0.00199199, acc 1
2016-09-06T12:10:14.894451: step 13880, loss 0.0101642, acc 1
2016-09-06T12:10:15.716522: step 13881, loss 0.0187937, acc 0.98
2016-09-06T12:10:16.543423: step 13882, loss 0.0773988, acc 0.96
2016-09-06T12:10:17.333359: step 13883, loss 0.00208876, acc 1
2016-09-06T12:10:18.122733: step 13884, loss 0.00195875, acc 1
2016-09-06T12:10:18.959981: step 13885, loss 0.0293654, acc 0.98
2016-09-06T12:10:19.745650: step 13886, loss 0.00187404, acc 1
2016-09-06T12:10:20.554882: step 13887, loss 0.00185434, acc 1
2016-09-06T12:10:21.365403: step 13888, loss 0.0127678, acc 1
2016-09-06T12:10:22.150858: step 13889, loss 0.00362777, acc 1
2016-09-06T12:10:22.949243: step 13890, loss 0.0314578, acc 0.98
2016-09-06T12:10:23.776550: step 13891, loss 0.0285051, acc 1
2016-09-06T12:10:24.550304: step 13892, loss 0.0236016, acc 1
2016-09-06T12:10:25.374271: step 13893, loss 0.0380007, acc 0.98
2016-09-06T12:10:26.202318: step 13894, loss 0.00206907, acc 1
2016-09-06T12:10:26.997619: step 13895, loss 0.0101657, acc 1
2016-09-06T12:10:27.823398: step 13896, loss 0.00302175, acc 1
2016-09-06T12:10:28.649519: step 13897, loss 0.0292895, acc 0.98
2016-09-06T12:10:29.421418: step 13898, loss 0.00183174, acc 1
2016-09-06T12:10:30.198598: step 13899, loss 0.0159589, acc 0.98
2016-09-06T12:10:31.013835: step 13900, loss 0.00563138, acc 1

Evaluation:
2016-09-06T12:10:34.741326: step 13900, loss 2.17867, acc 0.749531

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-13900

2016-09-06T12:10:36.638488: step 13901, loss 0.014266, acc 1
2016-09-06T12:10:37.464082: step 13902, loss 0.0027966, acc 1
2016-09-06T12:10:38.284787: step 13903, loss 0.033815, acc 0.98
2016-09-06T12:10:39.098047: step 13904, loss 0.0163707, acc 0.98
2016-09-06T12:10:39.929443: step 13905, loss 0.00208814, acc 1
2016-09-06T12:10:40.730176: step 13906, loss 0.0051367, acc 1
2016-09-06T12:10:41.561384: step 13907, loss 0.0530938, acc 0.98
2016-09-06T12:10:42.394556: step 13908, loss 0.0466966, acc 0.98
2016-09-06T12:10:43.201303: step 13909, loss 0.0316183, acc 0.98
2016-09-06T12:10:43.999895: step 13910, loss 0.0323585, acc 0.98
2016-09-06T12:10:44.840336: step 13911, loss 0.00938348, acc 1
2016-09-06T12:10:45.661803: step 13912, loss 0.0533127, acc 0.96
2016-09-06T12:10:46.463967: step 13913, loss 0.00467638, acc 1
2016-09-06T12:10:47.261773: step 13914, loss 0.00206697, acc 1
2016-09-06T12:10:48.100527: step 13915, loss 0.0156407, acc 1
2016-09-06T12:10:48.897448: step 13916, loss 0.00171739, acc 1
2016-09-06T12:10:49.704712: step 13917, loss 0.00263279, acc 1
2016-09-06T12:10:50.546904: step 13918, loss 0.00238149, acc 1
2016-09-06T12:10:51.356375: step 13919, loss 0.0168277, acc 0.98
2016-09-06T12:10:52.164957: step 13920, loss 0.00181044, acc 1
2016-09-06T12:10:52.985530: step 13921, loss 0.00190916, acc 1
2016-09-06T12:10:53.792723: step 13922, loss 0.0143943, acc 1
2016-09-06T12:10:54.604078: step 13923, loss 0.00789817, acc 1
2016-09-06T12:10:55.479185: step 13924, loss 0.0255784, acc 0.98
2016-09-06T12:10:56.305706: step 13925, loss 0.0777714, acc 0.98
2016-09-06T12:10:57.117092: step 13926, loss 0.00998657, acc 1
2016-09-06T12:10:57.949922: step 13927, loss 0.00260306, acc 1
2016-09-06T12:10:58.777408: step 13928, loss 0.00261968, acc 1
2016-09-06T12:10:59.590013: step 13929, loss 0.0116365, acc 1
2016-09-06T12:11:00.433908: step 13930, loss 0.00237024, acc 1
2016-09-06T12:11:01.266208: step 13931, loss 0.0328252, acc 1
2016-09-06T12:11:02.102201: step 13932, loss 0.0229784, acc 0.98
2016-09-06T12:11:02.918905: step 13933, loss 0.0372413, acc 0.98
2016-09-06T12:11:03.698395: step 13934, loss 0.025978, acc 1
2016-09-06T12:11:04.502692: step 13935, loss 0.00229432, acc 1
2016-09-06T12:11:05.351689: step 13936, loss 0.042315, acc 0.96
2016-09-06T12:11:06.185061: step 13937, loss 0.154466, acc 0.94
2016-09-06T12:11:06.984972: step 13938, loss 0.0227486, acc 0.98
2016-09-06T12:11:07.807041: step 13939, loss 0.0207832, acc 1
2016-09-06T12:11:08.630754: step 13940, loss 0.00375549, acc 1
2016-09-06T12:11:09.425463: step 13941, loss 0.0128105, acc 1
2016-09-06T12:11:10.221856: step 13942, loss 0.00186895, acc 1
2016-09-06T12:11:11.048860: step 13943, loss 0.0279378, acc 0.98
2016-09-06T12:11:11.832137: step 13944, loss 0.0216454, acc 0.98
2016-09-06T12:11:12.649199: step 13945, loss 0.0027142, acc 1
2016-09-06T12:11:13.469403: step 13946, loss 0.0197854, acc 0.98
2016-09-06T12:11:14.265102: step 13947, loss 0.00204554, acc 1
2016-09-06T12:11:15.071690: step 13948, loss 0.0284522, acc 1
2016-09-06T12:11:15.900490: step 13949, loss 0.0264119, acc 0.98
2016-09-06T12:11:16.675048: step 13950, loss 0.00664096, acc 1
2016-09-06T12:11:17.509270: step 13951, loss 0.00488144, acc 1
2016-09-06T12:11:18.318762: step 13952, loss 0.00829575, acc 1
2016-09-06T12:11:19.127592: step 13953, loss 0.00317534, acc 1
2016-09-06T12:11:19.937881: step 13954, loss 0.0205948, acc 0.98
2016-09-06T12:11:20.757726: step 13955, loss 0.0295461, acc 0.98
2016-09-06T12:11:21.582574: step 13956, loss 0.0241941, acc 0.98
2016-09-06T12:11:22.382898: step 13957, loss 0.0177085, acc 1
2016-09-06T12:11:23.201699: step 13958, loss 0.00862829, acc 1
2016-09-06T12:11:24.014656: step 13959, loss 0.0309488, acc 0.98
2016-09-06T12:11:24.812871: step 13960, loss 0.0187747, acc 1
2016-09-06T12:11:25.639814: step 13961, loss 0.0127563, acc 1
2016-09-06T12:11:26.452398: step 13962, loss 0.0329118, acc 0.98
2016-09-06T12:11:27.285035: step 13963, loss 0.00241784, acc 1
2016-09-06T12:11:28.155625: step 13964, loss 0.00567918, acc 1
2016-09-06T12:11:28.981975: step 13965, loss 0.168404, acc 0.98
2016-09-06T12:11:29.786153: step 13966, loss 0.00206744, acc 1
2016-09-06T12:11:30.610594: step 13967, loss 0.0486369, acc 0.98
2016-09-06T12:11:31.424766: step 13968, loss 0.0249429, acc 0.98
2016-09-06T12:11:32.230078: step 13969, loss 0.0224007, acc 1
2016-09-06T12:11:33.076939: step 13970, loss 0.0171569, acc 0.98
2016-09-06T12:11:33.888575: step 13971, loss 0.0566316, acc 0.96
2016-09-06T12:11:34.688154: step 13972, loss 0.00172525, acc 1
2016-09-06T12:11:35.543970: step 13973, loss 0.00648525, acc 1
2016-09-06T12:11:36.355205: step 13974, loss 0.00706153, acc 1
2016-09-06T12:11:37.142664: step 13975, loss 0.00188996, acc 1
2016-09-06T12:11:37.978764: step 13976, loss 0.00527982, acc 1
2016-09-06T12:11:38.798451: step 13977, loss 0.0123976, acc 1
2016-09-06T12:11:39.613932: step 13978, loss 0.0494291, acc 0.98
2016-09-06T12:11:40.418343: step 13979, loss 0.00719158, acc 1
2016-09-06T12:11:41.221383: step 13980, loss 0.0275036, acc 0.98
2016-09-06T12:11:42.013425: step 13981, loss 0.0162045, acc 0.98
2016-09-06T12:11:42.828160: step 13982, loss 0.00560672, acc 1
2016-09-06T12:11:43.662559: step 13983, loss 0.0101472, acc 1
2016-09-06T12:11:44.483192: step 13984, loss 0.055539, acc 0.98
2016-09-06T12:11:45.283030: step 13985, loss 0.0110318, acc 1
2016-09-06T12:11:46.108596: step 13986, loss 0.0310216, acc 0.96
2016-09-06T12:11:46.918521: step 13987, loss 0.00404762, acc 1
2016-09-06T12:11:47.714824: step 13988, loss 0.00195584, acc 1
2016-09-06T12:11:48.545013: step 13989, loss 0.00241366, acc 1
2016-09-06T12:11:49.341438: step 13990, loss 0.0291787, acc 0.98
2016-09-06T12:11:50.155612: step 13991, loss 0.0182663, acc 0.98
2016-09-06T12:11:50.984049: step 13992, loss 0.00392881, acc 1
2016-09-06T12:11:51.776165: step 13993, loss 0.0228042, acc 0.98
2016-09-06T12:11:52.563359: step 13994, loss 0.022665, acc 0.98
2016-09-06T12:11:53.378712: step 13995, loss 0.0165634, acc 0.98
2016-09-06T12:11:54.160108: step 13996, loss 0.00863595, acc 1
2016-09-06T12:11:54.967235: step 13997, loss 0.0124081, acc 1
2016-09-06T12:11:55.805881: step 13998, loss 0.00392112, acc 1
2016-09-06T12:11:56.571156: step 13999, loss 0.0265081, acc 0.98
2016-09-06T12:11:57.358502: step 14000, loss 0.00172812, acc 1

Evaluation:
2016-09-06T12:12:01.102187: step 14000, loss 1.96877, acc 0.749531

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-14000

2016-09-06T12:12:03.113516: step 14001, loss 0.011705, acc 1
2016-09-06T12:12:03.907244: step 14002, loss 0.0111187, acc 1
2016-09-06T12:12:04.704808: step 14003, loss 0.0156583, acc 1
2016-09-06T12:12:05.507847: step 14004, loss 0.0285095, acc 0.98
2016-09-06T12:12:06.317651: step 14005, loss 0.0498172, acc 0.98
2016-09-06T12:12:07.124201: step 14006, loss 0.00368442, acc 1
2016-09-06T12:12:07.910349: step 14007, loss 0.00274683, acc 1
2016-09-06T12:12:08.701264: step 14008, loss 0.0272366, acc 0.98
2016-09-06T12:12:09.529015: step 14009, loss 0.00182673, acc 1
2016-09-06T12:12:10.331983: step 14010, loss 0.0403167, acc 0.96
2016-09-06T12:12:11.125655: step 14011, loss 0.0622213, acc 0.98
2016-09-06T12:12:11.945055: step 14012, loss 0.00237439, acc 1
2016-09-06T12:12:12.742393: step 14013, loss 0.00255707, acc 1
2016-09-06T12:12:13.583198: step 14014, loss 0.0118694, acc 1
2016-09-06T12:12:14.388183: step 14015, loss 0.0129424, acc 1
2016-09-06T12:12:15.147690: step 14016, loss 0.0527235, acc 0.954545
2016-09-06T12:12:15.990180: step 14017, loss 0.00345707, acc 1
2016-09-06T12:12:16.793219: step 14018, loss 0.0160249, acc 1
2016-09-06T12:12:17.636047: step 14019, loss 0.00233064, acc 1
2016-09-06T12:12:18.431227: step 14020, loss 0.0113841, acc 1
2016-09-06T12:12:19.236076: step 14021, loss 0.017187, acc 0.98
2016-09-06T12:12:20.054208: step 14022, loss 0.0219438, acc 1
2016-09-06T12:12:20.856249: step 14023, loss 0.00760096, acc 1
2016-09-06T12:12:21.655011: step 14024, loss 0.0870812, acc 0.96
2016-09-06T12:12:22.513073: step 14025, loss 0.0234399, acc 0.98
2016-09-06T12:12:23.306963: step 14026, loss 0.0116466, acc 1
2016-09-06T12:12:24.101934: step 14027, loss 0.0184421, acc 0.98
2016-09-06T12:12:24.922235: step 14028, loss 0.00194178, acc 1
2016-09-06T12:12:25.727985: step 14029, loss 0.00424276, acc 1
2016-09-06T12:12:26.536276: step 14030, loss 0.0018026, acc 1
2016-09-06T12:12:27.333257: step 14031, loss 0.0325068, acc 0.98
2016-09-06T12:12:28.101391: step 14032, loss 0.0156266, acc 1
2016-09-06T12:12:28.925405: step 14033, loss 0.00315001, acc 1
2016-09-06T12:12:29.749336: step 14034, loss 0.00172004, acc 1
2016-09-06T12:12:30.544019: step 14035, loss 0.0260404, acc 1
2016-09-06T12:12:31.323761: step 14036, loss 0.00188993, acc 1
2016-09-06T12:12:32.155064: step 14037, loss 0.00192676, acc 1
2016-09-06T12:12:32.927270: step 14038, loss 0.0026845, acc 1
2016-09-06T12:12:33.740569: step 14039, loss 0.00234133, acc 1
2016-09-06T12:12:34.574606: step 14040, loss 0.00409595, acc 1
2016-09-06T12:12:35.343582: step 14041, loss 0.0233695, acc 0.98
2016-09-06T12:12:36.149653: step 14042, loss 0.00780333, acc 1
2016-09-06T12:12:37.014611: step 14043, loss 0.0087672, acc 1
2016-09-06T12:12:37.801148: step 14044, loss 0.00175045, acc 1
2016-09-06T12:12:38.576098: step 14045, loss 0.0180956, acc 1
2016-09-06T12:12:39.396957: step 14046, loss 0.00180386, acc 1
2016-09-06T12:12:40.180092: step 14047, loss 0.00187046, acc 1
2016-09-06T12:12:40.992133: step 14048, loss 0.0264003, acc 0.98
2016-09-06T12:12:41.829639: step 14049, loss 0.00235565, acc 1
2016-09-06T12:12:42.618493: step 14050, loss 0.001995, acc 1
2016-09-06T12:12:43.417453: step 14051, loss 0.00352069, acc 1
2016-09-06T12:12:44.221084: step 14052, loss 0.0180843, acc 0.98
2016-09-06T12:12:45.001510: step 14053, loss 0.0188162, acc 1
2016-09-06T12:12:45.814330: step 14054, loss 0.0141707, acc 1
2016-09-06T12:12:46.627201: step 14055, loss 0.0016616, acc 1
2016-09-06T12:12:47.432227: step 14056, loss 0.00315319, acc 1
2016-09-06T12:12:48.267547: step 14057, loss 0.0158367, acc 1
2016-09-06T12:12:49.085236: step 14058, loss 0.00295961, acc 1
2016-09-06T12:12:49.886270: step 14059, loss 0.00201634, acc 1
2016-09-06T12:12:50.681427: step 14060, loss 0.0402667, acc 0.98
2016-09-06T12:12:51.504842: step 14061, loss 0.00163778, acc 1
2016-09-06T12:12:52.285455: step 14062, loss 0.00197892, acc 1
2016-09-06T12:12:53.086677: step 14063, loss 0.00329208, acc 1
2016-09-06T12:12:53.926324: step 14064, loss 0.0106616, acc 1
2016-09-06T12:12:54.774289: step 14065, loss 0.0153186, acc 1
2016-09-06T12:12:55.566065: step 14066, loss 0.0015947, acc 1
2016-09-06T12:12:56.396673: step 14067, loss 0.00161484, acc 1
2016-09-06T12:12:57.197091: step 14068, loss 0.00828765, acc 1
2016-09-06T12:12:58.047845: step 14069, loss 0.0338373, acc 0.98
2016-09-06T12:12:58.869513: step 14070, loss 0.03022, acc 0.98
2016-09-06T12:12:59.691548: step 14071, loss 0.00471248, acc 1
2016-09-06T12:13:00.530778: step 14072, loss 0.00219586, acc 1
2016-09-06T12:13:01.341790: step 14073, loss 0.00358293, acc 1
2016-09-06T12:13:02.175886: step 14074, loss 0.017738, acc 0.98
2016-09-06T12:13:02.987302: step 14075, loss 0.00153985, acc 1
2016-09-06T12:13:03.806197: step 14076, loss 0.0031136, acc 1
2016-09-06T12:13:04.651421: step 14077, loss 0.0619838, acc 0.98
2016-09-06T12:13:05.464113: step 14078, loss 0.00178585, acc 1
2016-09-06T12:13:06.311375: step 14079, loss 0.00154486, acc 1
2016-09-06T12:13:07.128008: step 14080, loss 0.00984156, acc 1
2016-09-06T12:13:07.940365: step 14081, loss 0.00278752, acc 1
2016-09-06T12:13:08.798714: step 14082, loss 0.0218863, acc 0.98
2016-09-06T12:13:09.592872: step 14083, loss 0.00395056, acc 1
2016-09-06T12:13:10.387746: step 14084, loss 0.0291792, acc 1
2016-09-06T12:13:11.188221: step 14085, loss 0.0311256, acc 0.98
2016-09-06T12:13:12.011684: step 14086, loss 0.00843446, acc 1
2016-09-06T12:13:12.799865: step 14087, loss 0.0187912, acc 1
2016-09-06T12:13:13.585582: step 14088, loss 0.00803158, acc 1
2016-09-06T12:13:14.382637: step 14089, loss 0.0548975, acc 0.96
2016-09-06T12:13:15.184689: step 14090, loss 0.00224088, acc 1
2016-09-06T12:13:15.996003: step 14091, loss 0.024651, acc 0.98
2016-09-06T12:13:16.791054: step 14092, loss 0.0292088, acc 0.98
2016-09-06T12:13:17.638871: step 14093, loss 0.00297551, acc 1
2016-09-06T12:13:18.473327: step 14094, loss 0.0958812, acc 0.98
2016-09-06T12:13:19.294898: step 14095, loss 0.0111177, acc 1
2016-09-06T12:13:20.078475: step 14096, loss 0.0158694, acc 0.98
2016-09-06T12:13:20.882845: step 14097, loss 0.0023663, acc 1
2016-09-06T12:13:21.721535: step 14098, loss 0.0166298, acc 1
2016-09-06T12:13:22.526786: step 14099, loss 0.0224069, acc 1
2016-09-06T12:13:23.345449: step 14100, loss 0.00590621, acc 1

Evaluation:
2016-09-06T12:13:27.130085: step 14100, loss 2.17984, acc 0.742964

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-14100

2016-09-06T12:13:29.091038: step 14101, loss 0.029484, acc 0.98
2016-09-06T12:13:29.904731: step 14102, loss 0.0194406, acc 1
2016-09-06T12:13:30.707892: step 14103, loss 0.00240333, acc 1
2016-09-06T12:13:31.503254: step 14104, loss 0.00571764, acc 1
2016-09-06T12:13:32.276620: step 14105, loss 0.046659, acc 0.96
2016-09-06T12:13:33.087124: step 14106, loss 0.00773267, acc 1
2016-09-06T12:13:33.890449: step 14107, loss 0.0238816, acc 1
2016-09-06T12:13:34.659044: step 14108, loss 0.00164059, acc 1
2016-09-06T12:13:35.479438: step 14109, loss 0.00295321, acc 1
2016-09-06T12:13:36.286891: step 14110, loss 0.038026, acc 0.98
2016-09-06T12:13:37.057693: step 14111, loss 0.0119517, acc 1
2016-09-06T12:13:37.871962: step 14112, loss 0.00178871, acc 1
2016-09-06T12:13:38.680759: step 14113, loss 0.00529416, acc 1
2016-09-06T12:13:39.490655: step 14114, loss 0.00451792, acc 1
2016-09-06T12:13:40.309938: step 14115, loss 0.00404718, acc 1
2016-09-06T12:13:41.162811: step 14116, loss 0.0198385, acc 0.98
2016-09-06T12:13:41.947915: step 14117, loss 0.0182234, acc 0.98
2016-09-06T12:13:42.753888: step 14118, loss 0.00204923, acc 1
2016-09-06T12:13:43.582834: step 14119, loss 0.00663219, acc 1
2016-09-06T12:13:44.353949: step 14120, loss 0.00332036, acc 1
2016-09-06T12:13:45.163197: step 14121, loss 0.0176661, acc 1
2016-09-06T12:13:45.980983: step 14122, loss 0.00228654, acc 1
2016-09-06T12:13:46.780832: step 14123, loss 0.00308125, acc 1
2016-09-06T12:13:47.592890: step 14124, loss 0.0377832, acc 0.98
2016-09-06T12:13:48.421267: step 14125, loss 0.0361381, acc 0.96
2016-09-06T12:13:49.199754: step 14126, loss 0.0596829, acc 0.96
2016-09-06T12:13:50.001012: step 14127, loss 0.00218393, acc 1
2016-09-06T12:13:50.801660: step 14128, loss 0.0321956, acc 0.98
2016-09-06T12:13:51.588685: step 14129, loss 0.0433786, acc 0.98
2016-09-06T12:13:52.378809: step 14130, loss 0.00205145, acc 1
2016-09-06T12:13:53.184283: step 14131, loss 0.0211612, acc 1
2016-09-06T12:13:53.965543: step 14132, loss 0.00922052, acc 1
2016-09-06T12:13:54.786658: step 14133, loss 0.0309903, acc 1
2016-09-06T12:13:55.620298: step 14134, loss 0.00194507, acc 1
2016-09-06T12:13:56.417700: step 14135, loss 0.00339777, acc 1
2016-09-06T12:13:57.219534: step 14136, loss 0.0181662, acc 1
2016-09-06T12:13:58.053221: step 14137, loss 0.00191382, acc 1
2016-09-06T12:13:58.853438: step 14138, loss 0.00244177, acc 1
2016-09-06T12:13:59.658129: step 14139, loss 0.00219415, acc 1
2016-09-06T12:14:00.523503: step 14140, loss 0.0101701, acc 1
2016-09-06T12:14:01.345932: step 14141, loss 0.0236923, acc 1
2016-09-06T12:14:02.150074: step 14142, loss 0.00274956, acc 1
2016-09-06T12:14:02.959093: step 14143, loss 0.0105412, acc 1
2016-09-06T12:14:03.770610: step 14144, loss 0.00212503, acc 1
2016-09-06T12:14:04.589708: step 14145, loss 0.00749537, acc 1
2016-09-06T12:14:05.439636: step 14146, loss 0.0121056, acc 1
2016-09-06T12:14:06.228042: step 14147, loss 0.00415049, acc 1
2016-09-06T12:14:07.026717: step 14148, loss 0.00230176, acc 1
2016-09-06T12:14:07.849653: step 14149, loss 0.00622788, acc 1
2016-09-06T12:14:08.667454: step 14150, loss 0.00194803, acc 1
2016-09-06T12:14:09.472526: step 14151, loss 0.0228207, acc 1
2016-09-06T12:14:10.330355: step 14152, loss 0.00220855, acc 1
2016-09-06T12:14:11.144282: step 14153, loss 0.00235955, acc 1
2016-09-06T12:14:11.942570: step 14154, loss 0.0146677, acc 1
2016-09-06T12:14:12.789473: step 14155, loss 0.00194508, acc 1
2016-09-06T12:14:13.559978: step 14156, loss 0.0151791, acc 1
2016-09-06T12:14:14.375501: step 14157, loss 0.0424792, acc 0.98
2016-09-06T12:14:15.213575: step 14158, loss 0.00193534, acc 1
2016-09-06T12:14:16.011862: step 14159, loss 0.00794212, acc 1
2016-09-06T12:14:16.830082: step 14160, loss 0.00600635, acc 1
2016-09-06T12:14:17.675131: step 14161, loss 0.00193493, acc 1
2016-09-06T12:14:18.469487: step 14162, loss 0.00322481, acc 1
2016-09-06T12:14:19.292587: step 14163, loss 0.0310865, acc 0.98
2016-09-06T12:14:20.131662: step 14164, loss 0.00323376, acc 1
2016-09-06T12:14:20.953425: step 14165, loss 0.0252298, acc 0.98
2016-09-06T12:14:21.773321: step 14166, loss 0.00237898, acc 1
2016-09-06T12:14:22.573490: step 14167, loss 0.00186512, acc 1
2016-09-06T12:14:23.389679: step 14168, loss 0.00899769, acc 1
2016-09-06T12:14:24.202688: step 14169, loss 0.0200763, acc 0.98
2016-09-06T12:14:25.032019: step 14170, loss 0.00879698, acc 1
2016-09-06T12:14:25.860625: step 14171, loss 0.00186879, acc 1
2016-09-06T12:14:26.660634: step 14172, loss 0.0409101, acc 0.98
2016-09-06T12:14:27.500693: step 14173, loss 0.00178778, acc 1
2016-09-06T12:14:28.323888: step 14174, loss 0.00178112, acc 1
2016-09-06T12:14:29.109359: step 14175, loss 0.0182843, acc 0.98
2016-09-06T12:14:29.924217: step 14176, loss 0.0416394, acc 0.98
2016-09-06T12:14:30.737506: step 14177, loss 0.0499286, acc 0.98
2016-09-06T12:14:31.518600: step 14178, loss 0.022956, acc 0.98
2016-09-06T12:14:32.349976: step 14179, loss 0.0741015, acc 0.98
2016-09-06T12:14:33.160015: step 14180, loss 0.00467634, acc 1
2016-09-06T12:14:33.953402: step 14181, loss 0.00162482, acc 1
2016-09-06T12:14:34.772362: step 14182, loss 0.031195, acc 0.96
2016-09-06T12:14:35.591267: step 14183, loss 0.00680489, acc 1
2016-09-06T12:14:36.358563: step 14184, loss 0.00426868, acc 1
2016-09-06T12:14:37.153230: step 14185, loss 0.0906117, acc 0.98
2016-09-06T12:14:37.999185: step 14186, loss 0.0246109, acc 0.98
2016-09-06T12:14:38.772719: step 14187, loss 0.0116311, acc 1
2016-09-06T12:14:39.575040: step 14188, loss 0.018168, acc 1
2016-09-06T12:14:40.393091: step 14189, loss 0.0178458, acc 0.98
2016-09-06T12:14:41.145913: step 14190, loss 0.00569471, acc 1
2016-09-06T12:14:41.931591: step 14191, loss 0.0534625, acc 0.96
2016-09-06T12:14:42.757908: step 14192, loss 0.00259061, acc 1
2016-09-06T12:14:43.570669: step 14193, loss 0.0189359, acc 1
2016-09-06T12:14:44.372008: step 14194, loss 0.0429388, acc 0.98
2016-09-06T12:14:45.182167: step 14195, loss 0.00223643, acc 1
2016-09-06T12:14:45.973409: step 14196, loss 0.0361229, acc 0.96
2016-09-06T12:14:46.772456: step 14197, loss 0.00170754, acc 1
2016-09-06T12:14:47.588220: step 14198, loss 0.00723002, acc 1
2016-09-06T12:14:48.379597: step 14199, loss 0.00191024, acc 1
2016-09-06T12:14:49.192378: step 14200, loss 0.00336382, acc 1

Evaluation:
2016-09-06T12:14:52.913183: step 14200, loss 2.17111, acc 0.747655

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-14200

2016-09-06T12:14:54.750699: step 14201, loss 0.0414005, acc 0.98
2016-09-06T12:14:55.557318: step 14202, loss 0.00439838, acc 1
2016-09-06T12:14:56.426727: step 14203, loss 0.0151083, acc 1
2016-09-06T12:14:57.257045: step 14204, loss 0.00177543, acc 1
2016-09-06T12:14:58.062924: step 14205, loss 0.0459313, acc 0.96
2016-09-06T12:14:58.903229: step 14206, loss 0.0156626, acc 1
2016-09-06T12:14:59.751262: step 14207, loss 0.0491762, acc 0.98
2016-09-06T12:15:00.503322: step 14208, loss 0.00305807, acc 1
2016-09-06T12:15:01.307017: step 14209, loss 0.0102338, acc 1
2016-09-06T12:15:02.125827: step 14210, loss 0.054259, acc 0.98
2016-09-06T12:15:02.912131: step 14211, loss 0.0163614, acc 1
2016-09-06T12:15:03.735183: step 14212, loss 0.0506702, acc 0.98
2016-09-06T12:15:04.551045: step 14213, loss 0.0173094, acc 1
2016-09-06T12:15:05.373489: step 14214, loss 0.00606621, acc 1
2016-09-06T12:15:06.195468: step 14215, loss 0.00342231, acc 1
2016-09-06T12:15:07.002884: step 14216, loss 0.0324688, acc 0.98
2016-09-06T12:15:07.803142: step 14217, loss 0.0449666, acc 0.96
2016-09-06T12:15:08.603672: step 14218, loss 0.0430121, acc 0.98
2016-09-06T12:15:09.409916: step 14219, loss 0.00410969, acc 1
2016-09-06T12:15:10.190509: step 14220, loss 0.00195863, acc 1
2016-09-06T12:15:10.987829: step 14221, loss 0.0525678, acc 0.98
2016-09-06T12:15:11.798598: step 14222, loss 0.0496687, acc 0.98
2016-09-06T12:15:12.591650: step 14223, loss 0.0298265, acc 0.98
2016-09-06T12:15:13.405378: step 14224, loss 0.00452139, acc 1
2016-09-06T12:15:14.225527: step 14225, loss 0.0448671, acc 0.98
2016-09-06T12:15:15.033440: step 14226, loss 0.00211462, acc 1
2016-09-06T12:15:15.851461: step 14227, loss 0.0189623, acc 0.98
2016-09-06T12:15:16.645402: step 14228, loss 0.00221549, acc 1
2016-09-06T12:15:17.451897: step 14229, loss 0.00408093, acc 1
2016-09-06T12:15:18.236273: step 14230, loss 0.00361146, acc 1
2016-09-06T12:15:19.091084: step 14231, loss 0.00275307, acc 1
2016-09-06T12:15:19.884114: step 14232, loss 0.00445022, acc 1
2016-09-06T12:15:20.678784: step 14233, loss 0.00290405, acc 1
2016-09-06T12:15:21.488059: step 14234, loss 0.012638, acc 1
2016-09-06T12:15:22.263045: step 14235, loss 0.00436981, acc 1
2016-09-06T12:15:23.114495: step 14236, loss 0.00296033, acc 1
2016-09-06T12:15:23.937871: step 14237, loss 0.0148996, acc 1
2016-09-06T12:15:24.731701: step 14238, loss 0.0057379, acc 1
2016-09-06T12:15:25.555161: step 14239, loss 0.00223888, acc 1
2016-09-06T12:15:26.406973: step 14240, loss 0.0443438, acc 0.98
2016-09-06T12:15:27.239173: step 14241, loss 0.00816134, acc 1
2016-09-06T12:15:28.045537: step 14242, loss 0.016075, acc 1
2016-09-06T12:15:28.861514: step 14243, loss 0.0103168, acc 1
2016-09-06T12:15:29.670246: step 14244, loss 0.00250034, acc 1
2016-09-06T12:15:30.468558: step 14245, loss 0.00428553, acc 1
2016-09-06T12:15:31.281238: step 14246, loss 0.016836, acc 0.98
2016-09-06T12:15:32.096776: step 14247, loss 0.0110953, acc 1
2016-09-06T12:15:32.921158: step 14248, loss 0.0114757, acc 1
2016-09-06T12:15:33.757432: step 14249, loss 0.00240878, acc 1
2016-09-06T12:15:34.565403: step 14250, loss 0.0647217, acc 0.98
2016-09-06T12:15:35.380807: step 14251, loss 0.00895603, acc 1
2016-09-06T12:15:36.206697: step 14252, loss 0.00405748, acc 1
2016-09-06T12:15:37.036333: step 14253, loss 0.0130609, acc 1
2016-09-06T12:15:37.849236: step 14254, loss 0.114921, acc 0.98
2016-09-06T12:15:38.705748: step 14255, loss 0.0152661, acc 1
2016-09-06T12:15:39.516741: step 14256, loss 0.0311212, acc 0.98
2016-09-06T12:15:40.301543: step 14257, loss 0.0155187, acc 1
2016-09-06T12:15:41.136233: step 14258, loss 0.00195747, acc 1
2016-09-06T12:15:41.952894: step 14259, loss 0.00600235, acc 1
2016-09-06T12:15:42.763391: step 14260, loss 0.0586323, acc 0.96
2016-09-06T12:15:43.606242: step 14261, loss 0.010898, acc 1
2016-09-06T12:15:44.456686: step 14262, loss 0.0264409, acc 0.98
2016-09-06T12:15:45.263592: step 14263, loss 0.00636961, acc 1
2016-09-06T12:15:46.083677: step 14264, loss 0.0243101, acc 0.98
2016-09-06T12:15:46.921905: step 14265, loss 0.00476602, acc 1
2016-09-06T12:15:47.769526: step 14266, loss 0.0156924, acc 1
2016-09-06T12:15:48.561399: step 14267, loss 0.0113224, acc 1
2016-09-06T12:15:49.387840: step 14268, loss 0.00210868, acc 1
2016-09-06T12:15:50.207845: step 14269, loss 0.0976713, acc 0.96
2016-09-06T12:15:51.001391: step 14270, loss 0.00578189, acc 1
2016-09-06T12:15:51.821009: step 14271, loss 0.00233751, acc 1
2016-09-06T12:15:52.671509: step 14272, loss 0.0183581, acc 1
2016-09-06T12:15:53.476553: step 14273, loss 0.00229261, acc 1
2016-09-06T12:15:54.317884: step 14274, loss 0.0174258, acc 0.98
2016-09-06T12:15:55.124665: step 14275, loss 0.00911067, acc 1
2016-09-06T12:15:55.922495: step 14276, loss 0.0117102, acc 1
2016-09-06T12:15:56.725461: step 14277, loss 0.0183845, acc 1
2016-09-06T12:15:57.508652: step 14278, loss 0.00712295, acc 1
2016-09-06T12:15:58.307726: step 14279, loss 0.0132173, acc 1
2016-09-06T12:15:59.150777: step 14280, loss 0.0460971, acc 0.98
2016-09-06T12:15:59.964344: step 14281, loss 0.0546244, acc 0.98
2016-09-06T12:16:00.794046: step 14282, loss 0.00296027, acc 1
2016-09-06T12:16:01.610437: step 14283, loss 0.0139694, acc 1
2016-09-06T12:16:02.420380: step 14284, loss 0.0167816, acc 0.98
2016-09-06T12:16:03.233164: step 14285, loss 0.00245669, acc 1
2016-09-06T12:16:04.077466: step 14286, loss 0.00890885, acc 1
2016-09-06T12:16:04.887877: step 14287, loss 0.0119242, acc 1
2016-09-06T12:16:05.706509: step 14288, loss 0.00272615, acc 1
2016-09-06T12:16:06.547172: step 14289, loss 0.00312127, acc 1
2016-09-06T12:16:07.376761: step 14290, loss 0.00272407, acc 1
2016-09-06T12:16:08.177387: step 14291, loss 0.035271, acc 0.98
2016-09-06T12:16:08.971178: step 14292, loss 0.0106611, acc 1
2016-09-06T12:16:09.834564: step 14293, loss 0.00413599, acc 1
2016-09-06T12:16:10.617057: step 14294, loss 0.0155723, acc 1
2016-09-06T12:16:11.417287: step 14295, loss 0.00293027, acc 1
2016-09-06T12:16:12.240546: step 14296, loss 0.0064136, acc 1
2016-09-06T12:16:13.022991: step 14297, loss 0.0125312, acc 1
2016-09-06T12:16:13.853983: step 14298, loss 0.00257621, acc 1
2016-09-06T12:16:14.664966: step 14299, loss 0.0235835, acc 0.98
2016-09-06T12:16:15.450146: step 14300, loss 0.00262309, acc 1

Evaluation:
2016-09-06T12:16:19.181121: step 14300, loss 3.10937, acc 0.741088

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-14300

2016-09-06T12:16:21.064067: step 14301, loss 0.00584534, acc 1
2016-09-06T12:16:21.934441: step 14302, loss 0.0933144, acc 0.98
2016-09-06T12:16:22.736708: step 14303, loss 0.0208682, acc 0.98
2016-09-06T12:16:23.553810: step 14304, loss 0.00247808, acc 1
2016-09-06T12:16:24.372449: step 14305, loss 0.00297452, acc 1
2016-09-06T12:16:25.178041: step 14306, loss 0.105559, acc 0.94
2016-09-06T12:16:25.996748: step 14307, loss 0.00573968, acc 1
2016-09-06T12:16:26.865467: step 14308, loss 0.0219037, acc 0.98
2016-09-06T12:16:27.668552: step 14309, loss 0.0026878, acc 1
2016-09-06T12:16:28.480402: step 14310, loss 0.00292859, acc 1
2016-09-06T12:16:29.322734: step 14311, loss 0.0157778, acc 1
2016-09-06T12:16:30.150150: step 14312, loss 0.00202794, acc 1
2016-09-06T12:16:30.981412: step 14313, loss 0.0377399, acc 0.98
2016-09-06T12:16:31.807827: step 14314, loss 0.0025281, acc 1
2016-09-06T12:16:32.606591: step 14315, loss 0.0173806, acc 1
2016-09-06T12:16:33.399103: step 14316, loss 0.0274094, acc 1
2016-09-06T12:16:34.231263: step 14317, loss 0.0613154, acc 0.96
2016-09-06T12:16:35.061474: step 14318, loss 0.023192, acc 1
2016-09-06T12:16:35.869053: step 14319, loss 0.00380396, acc 1
2016-09-06T12:16:36.718221: step 14320, loss 0.0680169, acc 0.98
2016-09-06T12:16:37.550299: step 14321, loss 0.0228112, acc 0.98
2016-09-06T12:16:38.386335: step 14322, loss 0.0135765, acc 1
2016-09-06T12:16:39.207930: step 14323, loss 0.0141393, acc 1
2016-09-06T12:16:40.023269: step 14324, loss 0.0040383, acc 1
2016-09-06T12:16:40.847928: step 14325, loss 0.00446983, acc 1
2016-09-06T12:16:41.660632: step 14326, loss 0.00205018, acc 1
2016-09-06T12:16:42.467778: step 14327, loss 0.00251496, acc 1
2016-09-06T12:16:43.239053: step 14328, loss 0.0028426, acc 1
2016-09-06T12:16:44.082831: step 14329, loss 0.00543875, acc 1
2016-09-06T12:16:44.901500: step 14330, loss 0.00657418, acc 1
2016-09-06T12:16:45.691020: step 14331, loss 0.00212107, acc 1
2016-09-06T12:16:46.482283: step 14332, loss 0.0186589, acc 1
2016-09-06T12:16:47.305534: step 14333, loss 0.0225225, acc 1
2016-09-06T12:16:48.074293: step 14334, loss 0.00167396, acc 1
2016-09-06T12:16:48.899804: step 14335, loss 0.0366906, acc 0.98
2016-09-06T12:16:49.705854: step 14336, loss 0.01879, acc 1
2016-09-06T12:16:50.477724: step 14337, loss 0.00977722, acc 1
2016-09-06T12:16:51.282234: step 14338, loss 0.0418228, acc 0.98
2016-09-06T12:16:52.157774: step 14339, loss 0.011466, acc 1
2016-09-06T12:16:52.968130: step 14340, loss 0.00259776, acc 1
2016-09-06T12:16:53.790461: step 14341, loss 0.00371369, acc 1
2016-09-06T12:16:54.659030: step 14342, loss 0.0143952, acc 1
2016-09-06T12:16:55.503768: step 14343, loss 0.00742506, acc 1
2016-09-06T12:16:56.313134: step 14344, loss 0.00207711, acc 1
2016-09-06T12:16:57.153271: step 14345, loss 0.0115068, acc 1
2016-09-06T12:16:57.941009: step 14346, loss 0.00457011, acc 1
2016-09-06T12:16:58.771192: step 14347, loss 0.0018436, acc 1
2016-09-06T12:16:59.587393: step 14348, loss 0.00205945, acc 1
2016-09-06T12:17:00.422023: step 14349, loss 0.00920125, acc 1
2016-09-06T12:17:01.235987: step 14350, loss 0.0221307, acc 1
2016-09-06T12:17:02.105157: step 14351, loss 0.00896718, acc 1
2016-09-06T12:17:02.896315: step 14352, loss 0.0304094, acc 0.98
2016-09-06T12:17:03.708933: step 14353, loss 0.00668863, acc 1
2016-09-06T12:17:04.536647: step 14354, loss 0.0163504, acc 1
2016-09-06T12:17:05.370478: step 14355, loss 0.0137573, acc 1
2016-09-06T12:17:06.170069: step 14356, loss 0.0963551, acc 0.98
2016-09-06T12:17:06.978712: step 14357, loss 0.15849, acc 0.98
2016-09-06T12:17:07.823137: step 14358, loss 0.030295, acc 0.98
2016-09-06T12:17:08.607643: step 14359, loss 0.00201932, acc 1
2016-09-06T12:17:09.420763: step 14360, loss 0.00248959, acc 1
2016-09-06T12:17:10.257888: step 14361, loss 0.0213915, acc 0.98
2016-09-06T12:17:11.055026: step 14362, loss 0.00461916, acc 1
2016-09-06T12:17:11.845635: step 14363, loss 0.00751914, acc 1
2016-09-06T12:17:12.659397: step 14364, loss 0.0103962, acc 1
2016-09-06T12:17:13.429115: step 14365, loss 0.00745503, acc 1
2016-09-06T12:17:14.235735: step 14366, loss 0.0345996, acc 0.98
2016-09-06T12:17:15.062652: step 14367, loss 0.0167963, acc 1
2016-09-06T12:17:15.855512: step 14368, loss 0.103716, acc 0.96
2016-09-06T12:17:16.656886: step 14369, loss 0.0466698, acc 0.98
2016-09-06T12:17:17.499711: step 14370, loss 0.0142122, acc 1
2016-09-06T12:17:18.307152: step 14371, loss 0.0621835, acc 0.98
2016-09-06T12:17:19.120946: step 14372, loss 0.017631, acc 0.98
2016-09-06T12:17:20.038345: step 14373, loss 0.0418228, acc 0.98
2016-09-06T12:17:21.034571: step 14374, loss 0.00473568, acc 1
2016-09-06T12:17:22.082493: step 14375, loss 0.00667303, acc 1
2016-09-06T12:17:22.983692: step 14376, loss 0.00884348, acc 1
2016-09-06T12:17:24.027071: step 14377, loss 0.0510615, acc 0.98
2016-09-06T12:17:25.247686: step 14378, loss 0.0186522, acc 1
2016-09-06T12:17:26.521900: step 14379, loss 0.0371602, acc 1
2016-09-06T12:17:27.884041: step 14380, loss 0.121354, acc 0.96
2016-09-06T12:17:29.139748: step 14381, loss 0.00667826, acc 1
2016-09-06T12:17:30.145426: step 14382, loss 0.0593936, acc 0.98
2016-09-06T12:17:31.365425: step 14383, loss 0.0585333, acc 0.98
2016-09-06T12:17:32.311802: step 14384, loss 0.00827379, acc 1
2016-09-06T12:17:33.622455: step 14385, loss 0.00925393, acc 1
2016-09-06T12:17:34.492802: step 14386, loss 0.089031, acc 0.98
2016-09-06T12:17:35.597875: step 14387, loss 0.0182095, acc 1
2016-09-06T12:17:37.083566: step 14388, loss 0.0424051, acc 0.98
2016-09-06T12:17:38.500522: step 14389, loss 0.0201047, acc 1
2016-09-06T12:17:39.937392: step 14390, loss 0.0740033, acc 0.98
2016-09-06T12:17:41.234374: step 14391, loss 0.0119502, acc 1
2016-09-06T12:17:42.813883: step 14392, loss 0.012289, acc 1
2016-09-06T12:17:44.019505: step 14393, loss 0.0126783, acc 1
2016-09-06T12:17:45.197640: step 14394, loss 0.0269777, acc 1
2016-09-06T12:17:46.729580: step 14395, loss 0.0309649, acc 1
2016-09-06T12:17:47.717515: step 14396, loss 0.0731553, acc 0.98
2016-09-06T12:17:48.992654: step 14397, loss 0.0389955, acc 1
2016-09-06T12:17:50.685934: step 14398, loss 0.0264921, acc 1
2016-09-06T12:17:51.940654: step 14399, loss 0.0237421, acc 1
2016-09-06T12:17:53.006111: step 14400, loss 0.122009, acc 0.977273

Evaluation:
2016-09-06T12:17:56.761358: step 14400, loss 5.30086, acc 0.727955

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-14400

2016-09-06T12:17:58.971261: step 14401, loss 0.0728314, acc 0.98
2016-09-06T12:18:00.463120: step 14402, loss 0.144408, acc 0.96
2016-09-06T12:18:01.796938: step 14403, loss 0.0363357, acc 0.98
2016-09-06T12:18:02.921529: step 14404, loss 0.0139523, acc 1
2016-09-06T12:18:04.145355: step 14405, loss 0.0472224, acc 0.98
2016-09-06T12:18:05.439757: step 14406, loss 0.0907518, acc 0.96
2016-09-06T12:18:06.643183: step 14407, loss 0.0631923, acc 0.98
2016-09-06T12:18:07.903829: step 14408, loss 0.0537509, acc 0.98
2016-09-06T12:18:09.169966: step 14409, loss 0.017772, acc 1
2016-09-06T12:18:10.181607: step 14410, loss 0.0135665, acc 1
2016-09-06T12:18:11.103189: step 14411, loss 0.11231, acc 0.98
2016-09-06T12:18:12.244738: step 14412, loss 0.0131889, acc 1
2016-09-06T12:18:13.284509: step 14413, loss 0.0130686, acc 1
2016-09-06T12:18:14.453250: step 14414, loss 0.0443206, acc 0.98
2016-09-06T12:18:15.600969: step 14415, loss 0.0381235, acc 0.98
2016-09-06T12:18:16.592254: step 14416, loss 0.0297508, acc 0.98
2016-09-06T12:18:17.752764: step 14417, loss 0.0131108, acc 1
2016-09-06T12:18:18.835856: step 14418, loss 0.012532, acc 1
2016-09-06T12:18:20.088525: step 14419, loss 0.0127266, acc 1
2016-09-06T12:18:21.325269: step 14420, loss 0.0165953, acc 1
2016-09-06T12:18:22.479862: step 14421, loss 0.139733, acc 0.98
2016-09-06T12:18:23.526715: step 14422, loss 0.0161575, acc 1
2016-09-06T12:18:24.532409: step 14423, loss 0.0119514, acc 1
2016-09-06T12:18:25.833428: step 14424, loss 0.0259918, acc 0.98
2016-09-06T12:18:26.996594: step 14425, loss 0.0118097, acc 1
2016-09-06T12:18:27.959110: step 14426, loss 0.0115222, acc 1
2016-09-06T12:18:28.989310: step 14427, loss 0.0280339, acc 0.98
2016-09-06T12:18:30.033202: step 14428, loss 0.0249818, acc 1
2016-09-06T12:18:31.112718: step 14429, loss 0.0279518, acc 0.98
2016-09-06T12:18:32.139334: step 14430, loss 0.0285503, acc 0.98
2016-09-06T12:18:33.269964: step 14431, loss 0.0108219, acc 1
2016-09-06T12:18:34.421477: step 14432, loss 0.0283755, acc 1
2016-09-06T12:18:35.403188: step 14433, loss 0.0665141, acc 0.94
2016-09-06T12:18:36.605438: step 14434, loss 0.0128868, acc 1
2016-09-06T12:18:37.779895: step 14435, loss 0.0104472, acc 1
2016-09-06T12:18:38.718905: step 14436, loss 0.010146, acc 1
2016-09-06T12:18:39.582508: step 14437, loss 0.0348481, acc 0.98
2016-09-06T12:18:40.478593: step 14438, loss 0.0239307, acc 0.98
2016-09-06T12:18:41.452754: step 14439, loss 0.153585, acc 0.98
2016-09-06T12:18:42.383371: step 14440, loss 0.00978424, acc 1
2016-09-06T12:18:43.354435: step 14441, loss 0.0210822, acc 1
2016-09-06T12:18:44.161499: step 14442, loss 0.0172159, acc 1
2016-09-06T12:18:45.059950: step 14443, loss 0.0396794, acc 0.98
2016-09-06T12:18:45.980556: step 14444, loss 0.0107926, acc 1
2016-09-06T12:18:46.967162: step 14445, loss 0.00964357, acc 1
2016-09-06T12:18:47.787166: step 14446, loss 0.0129536, acc 1
2016-09-06T12:18:48.655684: step 14447, loss 0.0639354, acc 0.96
2016-09-06T12:18:49.616109: step 14448, loss 0.118375, acc 0.96
2016-09-06T12:18:50.454896: step 14449, loss 0.0102945, acc 1
2016-09-06T12:18:51.438699: step 14450, loss 0.0244971, acc 1
2016-09-06T12:18:52.398798: step 14451, loss 0.0122567, acc 1
2016-09-06T12:18:53.285651: step 14452, loss 0.140116, acc 0.96
2016-09-06T12:18:54.167897: step 14453, loss 0.00878316, acc 1
2016-09-06T12:18:55.113974: step 14454, loss 0.00804141, acc 1
2016-09-06T12:18:56.035656: step 14455, loss 0.0194247, acc 1
2016-09-06T12:18:56.870795: step 14456, loss 0.00745855, acc 1
2016-09-06T12:18:57.717539: step 14457, loss 0.019692, acc 1
2016-09-06T12:18:58.561338: step 14458, loss 0.0218283, acc 0.98
2016-09-06T12:18:59.370409: step 14459, loss 0.0145644, acc 1
2016-09-06T12:19:00.336253: step 14460, loss 0.00724848, acc 1
2016-09-06T12:19:01.163432: step 14461, loss 0.041393, acc 0.98
2016-09-06T12:19:01.985461: step 14462, loss 0.0110429, acc 1
2016-09-06T12:19:02.779870: step 14463, loss 0.103363, acc 0.98
2016-09-06T12:19:03.599235: step 14464, loss 0.0262347, acc 0.98
2016-09-06T12:19:04.411019: step 14465, loss 0.00741826, acc 1
2016-09-06T12:19:05.245683: step 14466, loss 0.00814497, acc 1
2016-09-06T12:19:06.098657: step 14467, loss 0.0073735, acc 1
2016-09-06T12:19:06.963694: step 14468, loss 0.0324911, acc 0.98
2016-09-06T12:19:07.810597: step 14469, loss 0.0496482, acc 0.98
2016-09-06T12:19:08.613422: step 14470, loss 0.00866683, acc 1
2016-09-06T12:19:09.518215: step 14471, loss 0.0784537, acc 0.96
2016-09-06T12:19:10.324897: step 14472, loss 0.0088669, acc 1
2016-09-06T12:19:11.104944: step 14473, loss 0.0271283, acc 0.98
2016-09-06T12:19:11.917700: step 14474, loss 0.00628937, acc 1
2016-09-06T12:19:12.745815: step 14475, loss 0.0196872, acc 1
2016-09-06T12:19:13.547595: step 14476, loss 0.0227441, acc 0.98
2016-09-06T12:19:14.324044: step 14477, loss 0.00676244, acc 1
2016-09-06T12:19:15.198411: step 14478, loss 0.0335106, acc 1
2016-09-06T12:19:16.005334: step 14479, loss 0.0318713, acc 0.98
2016-09-06T12:19:16.830365: step 14480, loss 0.128724, acc 0.98
2016-09-06T12:19:17.677151: step 14481, loss 0.00579052, acc 1
2016-09-06T12:19:18.488402: step 14482, loss 0.00701621, acc 1
2016-09-06T12:19:19.293052: step 14483, loss 0.0313587, acc 0.98
2016-09-06T12:19:20.118784: step 14484, loss 0.0170957, acc 1
2016-09-06T12:19:20.921274: step 14485, loss 0.0138377, acc 1
2016-09-06T12:19:21.742871: step 14486, loss 0.0272481, acc 1
2016-09-06T12:19:22.578204: step 14487, loss 0.00924063, acc 1
2016-09-06T12:19:23.423933: step 14488, loss 0.0384472, acc 0.98
2016-09-06T12:19:24.242293: step 14489, loss 0.0285149, acc 1
2016-09-06T12:19:25.096239: step 14490, loss 0.0648709, acc 0.98
2016-09-06T12:19:25.940436: step 14491, loss 0.0146794, acc 1
2016-09-06T12:19:26.739095: step 14492, loss 0.0063003, acc 1
2016-09-06T12:19:27.557911: step 14493, loss 0.0301751, acc 0.98
2016-09-06T12:19:28.383163: step 14494, loss 0.0141073, acc 1
2016-09-06T12:19:29.215739: step 14495, loss 0.0196684, acc 0.98
2016-09-06T12:19:30.053180: step 14496, loss 0.00531458, acc 1
2016-09-06T12:19:30.886722: step 14497, loss 0.0050664, acc 1
2016-09-06T12:19:31.676912: step 14498, loss 0.036956, acc 0.98
2016-09-06T12:19:32.493956: step 14499, loss 0.00655172, acc 1
2016-09-06T12:19:33.332103: step 14500, loss 0.020992, acc 1

Evaluation:
2016-09-06T12:19:37.045459: step 14500, loss 2.09379, acc 0.731707

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-14500

2016-09-06T12:19:38.886186: step 14501, loss 0.0262402, acc 0.98
2016-09-06T12:19:39.725335: step 14502, loss 0.014377, acc 1
2016-09-06T12:19:40.552544: step 14503, loss 0.0208994, acc 0.98
2016-09-06T12:19:41.360246: step 14504, loss 0.00655964, acc 1
2016-09-06T12:19:42.191412: step 14505, loss 0.0106383, acc 1
2016-09-06T12:19:43.019388: step 14506, loss 0.0226928, acc 0.98
2016-09-06T12:19:43.854297: step 14507, loss 0.0201212, acc 0.98
2016-09-06T12:19:44.674978: step 14508, loss 0.0194685, acc 0.98
2016-09-06T12:19:45.485728: step 14509, loss 0.0285669, acc 0.98
2016-09-06T12:19:46.255801: step 14510, loss 0.0684408, acc 0.98
2016-09-06T12:19:47.046116: step 14511, loss 0.0596065, acc 0.98
2016-09-06T12:19:47.850576: step 14512, loss 0.0235979, acc 1
2016-09-06T12:19:48.641978: step 14513, loss 0.00549597, acc 1
2016-09-06T12:19:49.455781: step 14514, loss 0.00585485, acc 1
2016-09-06T12:19:50.289212: step 14515, loss 0.0174133, acc 1
2016-09-06T12:19:51.113404: step 14516, loss 0.00587108, acc 1
2016-09-06T12:19:51.917402: step 14517, loss 0.00409009, acc 1
2016-09-06T12:19:52.715212: step 14518, loss 0.00413872, acc 1
2016-09-06T12:19:53.514978: step 14519, loss 0.0195119, acc 1
2016-09-06T12:19:54.345737: step 14520, loss 0.0216465, acc 0.98
2016-09-06T12:19:55.153405: step 14521, loss 0.00552133, acc 1
2016-09-06T12:19:55.945543: step 14522, loss 0.0123819, acc 1
2016-09-06T12:19:56.778281: step 14523, loss 0.0127825, acc 1
2016-09-06T12:19:57.602023: step 14524, loss 0.00799314, acc 1
2016-09-06T12:19:58.401793: step 14525, loss 0.0411323, acc 0.98
2016-09-06T12:19:59.200927: step 14526, loss 0.013557, acc 1
2016-09-06T12:20:00.042072: step 14527, loss 0.221649, acc 0.94
2016-09-06T12:20:00.900120: step 14528, loss 0.0108198, acc 1
2016-09-06T12:20:01.730340: step 14529, loss 0.00749181, acc 1
2016-09-06T12:20:02.603050: step 14530, loss 0.0135537, acc 1
2016-09-06T12:20:03.399283: step 14531, loss 0.0169904, acc 1
2016-09-06T12:20:04.224119: step 14532, loss 0.0109091, acc 1
2016-09-06T12:20:05.046708: step 14533, loss 0.00470133, acc 1
2016-09-06T12:20:05.883643: step 14534, loss 0.053962, acc 0.98
2016-09-06T12:20:06.706424: step 14535, loss 0.0327711, acc 0.98
2016-09-06T12:20:07.552070: step 14536, loss 0.0146457, acc 1
2016-09-06T12:20:08.395961: step 14537, loss 0.0180034, acc 1
2016-09-06T12:20:09.182598: step 14538, loss 0.0276087, acc 0.98
2016-09-06T12:20:10.026370: step 14539, loss 0.0055052, acc 1
2016-09-06T12:20:10.845617: step 14540, loss 0.00847625, acc 1
2016-09-06T12:20:11.659162: step 14541, loss 0.0200199, acc 1
2016-09-06T12:20:12.453420: step 14542, loss 0.00414647, acc 1
2016-09-06T12:20:13.273084: step 14543, loss 0.00683331, acc 1
2016-09-06T12:20:14.040472: step 14544, loss 0.0129673, acc 1
2016-09-06T12:20:14.858114: step 14545, loss 0.00700835, acc 1
2016-09-06T12:20:15.690947: step 14546, loss 0.0241021, acc 1
2016-09-06T12:20:16.495735: step 14547, loss 0.0337224, acc 0.98
2016-09-06T12:20:17.281914: step 14548, loss 0.01791, acc 1
2016-09-06T12:20:18.097450: step 14549, loss 0.00850843, acc 1
2016-09-06T12:20:18.911233: step 14550, loss 0.0301028, acc 0.98
2016-09-06T12:20:19.717258: step 14551, loss 0.0117886, acc 1
2016-09-06T12:20:20.533850: step 14552, loss 0.0368751, acc 0.98
2016-09-06T12:20:21.338809: step 14553, loss 0.0283197, acc 0.98
2016-09-06T12:20:22.142141: step 14554, loss 0.0149319, acc 1
2016-09-06T12:20:22.969719: step 14555, loss 0.00357819, acc 1
2016-09-06T12:20:23.772943: step 14556, loss 0.017218, acc 1
2016-09-06T12:20:24.577511: step 14557, loss 0.0137241, acc 1
2016-09-06T12:20:25.401027: step 14558, loss 0.0208541, acc 0.98
2016-09-06T12:20:26.223174: step 14559, loss 0.0232988, acc 1
2016-09-06T12:20:27.029255: step 14560, loss 0.044655, acc 0.98
2016-09-06T12:20:27.850687: step 14561, loss 0.0236318, acc 1
2016-09-06T12:20:28.643189: step 14562, loss 0.0344957, acc 0.98
2016-09-06T12:20:29.443884: step 14563, loss 0.00363111, acc 1
2016-09-06T12:20:30.266549: step 14564, loss 0.0276687, acc 0.98
2016-09-06T12:20:31.072812: step 14565, loss 0.00465292, acc 1
2016-09-06T12:20:31.880619: step 14566, loss 0.0396153, acc 0.98
2016-09-06T12:20:32.695895: step 14567, loss 0.0095297, acc 1
2016-09-06T12:20:33.523741: step 14568, loss 0.00583666, acc 1
2016-09-06T12:20:34.335576: step 14569, loss 0.0823666, acc 0.92
2016-09-06T12:20:35.153741: step 14570, loss 0.00585477, acc 1
2016-09-06T12:20:35.967698: step 14571, loss 0.00375689, acc 1
2016-09-06T12:20:36.806083: step 14572, loss 0.0784447, acc 0.98
2016-09-06T12:20:37.656144: step 14573, loss 0.0164487, acc 1
2016-09-06T12:20:38.476508: step 14574, loss 0.0392277, acc 0.98
2016-09-06T12:20:39.299254: step 14575, loss 0.0188196, acc 0.98
2016-09-06T12:20:40.139975: step 14576, loss 0.00602005, acc 1
2016-09-06T12:20:40.953229: step 14577, loss 0.0138407, acc 1
2016-09-06T12:20:41.803124: step 14578, loss 0.0193705, acc 0.98
2016-09-06T12:20:42.630523: step 14579, loss 0.00474279, acc 1
2016-09-06T12:20:43.442199: step 14580, loss 0.0454142, acc 0.98
2016-09-06T12:20:44.230311: step 14581, loss 0.00551496, acc 1
2016-09-06T12:20:45.044210: step 14582, loss 0.0189603, acc 0.98
2016-09-06T12:20:45.879713: step 14583, loss 0.00537828, acc 1
2016-09-06T12:20:46.669815: step 14584, loss 0.00315411, acc 1
2016-09-06T12:20:47.482560: step 14585, loss 0.00880896, acc 1
2016-09-06T12:20:48.302301: step 14586, loss 0.0165077, acc 1
2016-09-06T12:20:49.106319: step 14587, loss 0.0361014, acc 0.96
2016-09-06T12:20:49.916948: step 14588, loss 0.00312442, acc 1
2016-09-06T12:20:50.745625: step 14589, loss 0.0088812, acc 1
2016-09-06T12:20:51.501978: step 14590, loss 0.0340707, acc 0.98
2016-09-06T12:20:52.313455: step 14591, loss 0.00851859, acc 1
2016-09-06T12:20:53.093847: step 14592, loss 0.0122054, acc 1
2016-09-06T12:20:53.901642: step 14593, loss 0.0133095, acc 1
2016-09-06T12:20:54.721994: step 14594, loss 0.0219257, acc 1
2016-09-06T12:20:55.546603: step 14595, loss 0.00861232, acc 1
2016-09-06T12:20:56.332729: step 14596, loss 0.0308161, acc 1
2016-09-06T12:20:57.139630: step 14597, loss 0.042809, acc 0.98
2016-09-06T12:20:57.979819: step 14598, loss 0.0191128, acc 0.98
2016-09-06T12:20:58.746407: step 14599, loss 0.0063099, acc 1
2016-09-06T12:20:59.574237: step 14600, loss 0.0331437, acc 0.98

Evaluation:
2016-09-06T12:21:03.330631: step 14600, loss 2.28678, acc 0.739212

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-14600

2016-09-06T12:21:05.264465: step 14601, loss 0.0623888, acc 0.98
2016-09-06T12:21:06.049386: step 14602, loss 0.0226854, acc 0.98
2016-09-06T12:21:06.853349: step 14603, loss 0.00305436, acc 1
2016-09-06T12:21:07.704295: step 14604, loss 0.004767, acc 1
2016-09-06T12:21:08.478460: step 14605, loss 0.070585, acc 0.98
2016-09-06T12:21:09.285373: step 14606, loss 0.0221092, acc 0.98
2016-09-06T12:21:10.117012: step 14607, loss 0.0921374, acc 0.98
2016-09-06T12:21:10.881795: step 14608, loss 0.00511729, acc 1
2016-09-06T12:21:11.673637: step 14609, loss 0.00700189, acc 1
2016-09-06T12:21:12.492817: step 14610, loss 0.00662878, acc 1
2016-09-06T12:21:13.281895: step 14611, loss 0.00465124, acc 1
2016-09-06T12:21:14.092317: step 14612, loss 0.00286562, acc 1
2016-09-06T12:21:14.909398: step 14613, loss 0.00364205, acc 1
2016-09-06T12:21:15.696171: step 14614, loss 0.00713504, acc 1
2016-09-06T12:21:16.507334: step 14615, loss 0.00367134, acc 1
2016-09-06T12:21:17.333029: step 14616, loss 0.00837204, acc 1
2016-09-06T12:21:18.133484: step 14617, loss 0.018262, acc 0.98
2016-09-06T12:21:18.927728: step 14618, loss 0.00548488, acc 1
2016-09-06T12:21:19.734693: step 14619, loss 0.00873308, acc 1
2016-09-06T12:21:20.543870: step 14620, loss 0.00773183, acc 1
2016-09-06T12:21:21.333272: step 14621, loss 0.0245435, acc 0.98
2016-09-06T12:21:22.165317: step 14622, loss 0.0230831, acc 0.98
2016-09-06T12:21:22.938372: step 14623, loss 0.0029935, acc 1
2016-09-06T12:21:23.760401: step 14624, loss 0.00700522, acc 1
2016-09-06T12:21:24.598797: step 14625, loss 0.0838967, acc 0.98
2016-09-06T12:21:25.397833: step 14626, loss 0.0100942, acc 1
2016-09-06T12:21:26.190584: step 14627, loss 0.00272615, acc 1
2016-09-06T12:21:27.023464: step 14628, loss 0.00354706, acc 1
2016-09-06T12:21:27.816939: step 14629, loss 0.0159435, acc 1
2016-09-06T12:21:28.635219: step 14630, loss 0.00629434, acc 1
2016-09-06T12:21:29.469225: step 14631, loss 0.0063213, acc 1
2016-09-06T12:21:30.255764: step 14632, loss 0.0551754, acc 0.98
2016-09-06T12:21:31.039665: step 14633, loss 0.002999, acc 1
2016-09-06T12:21:31.848082: step 14634, loss 0.00942984, acc 1
2016-09-06T12:21:32.622142: step 14635, loss 0.0361657, acc 0.96
2016-09-06T12:21:33.417779: step 14636, loss 0.00238803, acc 1
2016-09-06T12:21:34.240810: step 14637, loss 0.00436625, acc 1
2016-09-06T12:21:35.022491: step 14638, loss 0.0235171, acc 0.98
2016-09-06T12:21:35.828346: step 14639, loss 0.00253201, acc 1
2016-09-06T12:21:36.648887: step 14640, loss 0.0176275, acc 1
2016-09-06T12:21:37.433746: step 14641, loss 0.0429151, acc 0.96
2016-09-06T12:21:38.247564: step 14642, loss 0.00302861, acc 1
2016-09-06T12:21:39.072108: step 14643, loss 0.0242647, acc 0.98
2016-09-06T12:21:39.863798: step 14644, loss 0.0296192, acc 1
2016-09-06T12:21:40.673677: step 14645, loss 0.0119224, acc 1
2016-09-06T12:21:41.485943: step 14646, loss 0.0299358, acc 0.98
2016-09-06T12:21:42.268653: step 14647, loss 0.00317267, acc 1
2016-09-06T12:21:43.074347: step 14648, loss 0.00258223, acc 1
2016-09-06T12:21:43.902280: step 14649, loss 0.00448933, acc 1
2016-09-06T12:21:44.698831: step 14650, loss 0.15028, acc 0.94
2016-09-06T12:21:45.511036: step 14651, loss 0.00699866, acc 1
2016-09-06T12:21:46.329106: step 14652, loss 0.0146227, acc 1
2016-09-06T12:21:47.123263: step 14653, loss 0.0502555, acc 0.96
2016-09-06T12:21:47.922165: step 14654, loss 0.00258216, acc 1
2016-09-06T12:21:48.725497: step 14655, loss 0.00222425, acc 1
2016-09-06T12:21:49.548860: step 14656, loss 0.013839, acc 1
2016-09-06T12:21:50.351547: step 14657, loss 0.00363653, acc 1
2016-09-06T12:21:51.158151: step 14658, loss 0.0297811, acc 0.98
2016-09-06T12:21:51.958058: step 14659, loss 0.0286585, acc 1
2016-09-06T12:21:52.769034: step 14660, loss 0.0194633, acc 1
2016-09-06T12:21:53.573403: step 14661, loss 0.0210617, acc 0.98
2016-09-06T12:21:54.368712: step 14662, loss 0.00354082, acc 1
2016-09-06T12:21:55.182496: step 14663, loss 0.00586202, acc 1
2016-09-06T12:21:56.013480: step 14664, loss 0.00327906, acc 1
2016-09-06T12:21:56.788146: step 14665, loss 0.00958843, acc 1
2016-09-06T12:21:57.588149: step 14666, loss 0.0489856, acc 0.98
2016-09-06T12:21:58.404300: step 14667, loss 0.0202911, acc 1
2016-09-06T12:21:59.165737: step 14668, loss 0.0167722, acc 0.98
2016-09-06T12:21:59.982839: step 14669, loss 0.0211048, acc 1
2016-09-06T12:22:00.866108: step 14670, loss 0.10812, acc 0.98
2016-09-06T12:22:01.645740: step 14671, loss 0.00317142, acc 1
2016-09-06T12:22:02.448848: step 14672, loss 0.0542797, acc 0.98
2016-09-06T12:22:03.270574: step 14673, loss 0.00250805, acc 1
2016-09-06T12:22:04.056855: step 14674, loss 0.0149636, acc 1
2016-09-06T12:22:04.898904: step 14675, loss 0.0486344, acc 0.96
2016-09-06T12:22:05.718833: step 14676, loss 0.00219005, acc 1
2016-09-06T12:22:06.507768: step 14677, loss 0.0113258, acc 1
2016-09-06T12:22:07.290685: step 14678, loss 0.0269729, acc 0.98
2016-09-06T12:22:08.124317: step 14679, loss 0.00984628, acc 1
2016-09-06T12:22:08.922389: step 14680, loss 0.0407205, acc 1
2016-09-06T12:22:09.702471: step 14681, loss 0.0233435, acc 0.98
2016-09-06T12:22:10.546708: step 14682, loss 0.00271016, acc 1
2016-09-06T12:22:11.333405: step 14683, loss 0.0221953, acc 0.98
2016-09-06T12:22:12.122821: step 14684, loss 0.00205237, acc 1
2016-09-06T12:22:12.945053: step 14685, loss 0.00424544, acc 1
2016-09-06T12:22:13.719257: step 14686, loss 0.0108094, acc 1
2016-09-06T12:22:14.534011: step 14687, loss 0.00320039, acc 1
2016-09-06T12:22:15.368105: step 14688, loss 0.00603816, acc 1
2016-09-06T12:22:16.169250: step 14689, loss 0.00246193, acc 1
2016-09-06T12:22:16.957974: step 14690, loss 0.0702518, acc 0.98
2016-09-06T12:22:17.796008: step 14691, loss 0.00244963, acc 1
2016-09-06T12:22:18.601264: step 14692, loss 0.00419785, acc 1
2016-09-06T12:22:19.419535: step 14693, loss 0.0161866, acc 1
2016-09-06T12:22:20.258122: step 14694, loss 0.00280838, acc 1
2016-09-06T12:22:21.071310: step 14695, loss 0.00874841, acc 1
2016-09-06T12:22:21.873664: step 14696, loss 0.00571164, acc 1
2016-09-06T12:22:22.691620: step 14697, loss 0.0153917, acc 1
2016-09-06T12:22:23.498876: step 14698, loss 0.00246675, acc 1
2016-09-06T12:22:24.316195: step 14699, loss 0.0188769, acc 1
2016-09-06T12:22:25.177653: step 14700, loss 0.0180483, acc 1

Evaluation:
2016-09-06T12:22:28.909913: step 14700, loss 1.8331, acc 0.742026

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-14700

2016-09-06T12:22:30.906094: step 14701, loss 0.0550236, acc 0.98
2016-09-06T12:22:31.733546: step 14702, loss 0.0285683, acc 1
2016-09-06T12:22:32.578818: step 14703, loss 0.00386242, acc 1
2016-09-06T12:22:33.378491: step 14704, loss 0.0124489, acc 1
2016-09-06T12:22:34.197566: step 14705, loss 0.0180066, acc 0.98
2016-09-06T12:22:35.034124: step 14706, loss 0.0251754, acc 1
2016-09-06T12:22:35.859902: step 14707, loss 0.0132634, acc 1
2016-09-06T12:22:36.667456: step 14708, loss 0.00634078, acc 1
2016-09-06T12:22:37.490495: step 14709, loss 0.027043, acc 1
2016-09-06T12:22:38.306549: step 14710, loss 0.00218601, acc 1
2016-09-06T12:22:39.121407: step 14711, loss 0.0212797, acc 0.98
2016-09-06T12:22:39.943577: step 14712, loss 0.0256926, acc 1
2016-09-06T12:22:40.754392: step 14713, loss 0.00280742, acc 1
2016-09-06T12:22:41.538497: step 14714, loss 0.0299543, acc 0.98
2016-09-06T12:22:42.350567: step 14715, loss 0.00244443, acc 1
2016-09-06T12:22:43.163777: step 14716, loss 0.0366631, acc 0.98
2016-09-06T12:22:43.954730: step 14717, loss 0.0227293, acc 1
2016-09-06T12:22:44.825956: step 14718, loss 0.00267619, acc 1
2016-09-06T12:22:45.647456: step 14719, loss 0.0558399, acc 0.96
2016-09-06T12:22:46.436326: step 14720, loss 0.0156409, acc 1
2016-09-06T12:22:47.227138: step 14721, loss 0.00240805, acc 1
2016-09-06T12:22:48.037429: step 14722, loss 0.0171426, acc 0.98
2016-09-06T12:22:48.854198: step 14723, loss 0.00696404, acc 1
2016-09-06T12:22:49.642237: step 14724, loss 0.0208723, acc 1
2016-09-06T12:22:50.477634: step 14725, loss 0.0187909, acc 0.98
2016-09-06T12:22:51.266503: step 14726, loss 0.0214261, acc 0.98
2016-09-06T12:22:52.049900: step 14727, loss 0.00625017, acc 1
2016-09-06T12:22:52.896067: step 14728, loss 0.00243817, acc 1
2016-09-06T12:22:53.712671: step 14729, loss 0.00464757, acc 1
2016-09-06T12:22:54.519118: step 14730, loss 0.0236888, acc 0.98
2016-09-06T12:22:55.346779: step 14731, loss 0.0053643, acc 1
2016-09-06T12:22:56.167522: step 14732, loss 0.00497726, acc 1
2016-09-06T12:22:56.988298: step 14733, loss 0.005633, acc 1
2016-09-06T12:22:57.825125: step 14734, loss 0.0022189, acc 1
2016-09-06T12:22:58.624350: step 14735, loss 0.00629498, acc 1
2016-09-06T12:22:59.445120: step 14736, loss 0.0350318, acc 0.98
2016-09-06T12:23:00.282755: step 14737, loss 0.00249824, acc 1
2016-09-06T12:23:01.098425: step 14738, loss 0.0173207, acc 1
2016-09-06T12:23:01.912015: step 14739, loss 0.00256609, acc 1
2016-09-06T12:23:02.774580: step 14740, loss 0.0106015, acc 1
2016-09-06T12:23:03.577079: step 14741, loss 0.0187122, acc 0.98
2016-09-06T12:23:04.403872: step 14742, loss 0.0307251, acc 0.98
2016-09-06T12:23:05.245537: step 14743, loss 0.00495842, acc 1
2016-09-06T12:23:06.075051: step 14744, loss 0.0022432, acc 1
2016-09-06T12:23:06.870440: step 14745, loss 0.00277691, acc 1
2016-09-06T12:23:07.711761: step 14746, loss 0.00904092, acc 1
2016-09-06T12:23:08.549522: step 14747, loss 0.00227697, acc 1
2016-09-06T12:23:09.352329: step 14748, loss 0.00286278, acc 1
2016-09-06T12:23:10.163977: step 14749, loss 0.0151814, acc 1
2016-09-06T12:23:10.974620: step 14750, loss 0.0213094, acc 0.98
2016-09-06T12:23:11.770144: step 14751, loss 0.00229532, acc 1
2016-09-06T12:23:12.592049: step 14752, loss 0.0377783, acc 0.98
2016-09-06T12:23:13.397307: step 14753, loss 0.0128173, acc 1
2016-09-06T12:23:14.204541: step 14754, loss 0.0294895, acc 0.98
2016-09-06T12:23:15.069499: step 14755, loss 0.0554333, acc 0.98
2016-09-06T12:23:15.897328: step 14756, loss 0.0025309, acc 1
2016-09-06T12:23:16.706231: step 14757, loss 0.00212509, acc 1
2016-09-06T12:23:17.532643: step 14758, loss 0.0158617, acc 1
2016-09-06T12:23:18.369722: step 14759, loss 0.00915143, acc 1
2016-09-06T12:23:19.158353: step 14760, loss 0.00256719, acc 1
2016-09-06T12:23:19.987635: step 14761, loss 0.0841896, acc 0.96
2016-09-06T12:23:20.840220: step 14762, loss 0.00257522, acc 1
2016-09-06T12:23:21.670209: step 14763, loss 0.022413, acc 0.98
2016-09-06T12:23:22.494209: step 14764, loss 0.0201213, acc 1
2016-09-06T12:23:23.322035: step 14765, loss 0.0152781, acc 1
2016-09-06T12:23:24.159032: step 14766, loss 0.0405522, acc 0.98
2016-09-06T12:23:24.977010: step 14767, loss 0.0148947, acc 1
2016-09-06T12:23:25.849532: step 14768, loss 0.00696084, acc 1
2016-09-06T12:23:26.657579: step 14769, loss 0.00218901, acc 1
2016-09-06T12:23:27.461410: step 14770, loss 0.025309, acc 0.98
2016-09-06T12:23:28.281787: step 14771, loss 0.0310586, acc 0.98
2016-09-06T12:23:29.105259: step 14772, loss 0.00877099, acc 1
2016-09-06T12:23:29.915017: step 14773, loss 0.00243057, acc 1
2016-09-06T12:23:30.731040: step 14774, loss 0.00646844, acc 1
2016-09-06T12:23:31.555338: step 14775, loss 0.00310596, acc 1
2016-09-06T12:23:32.335836: step 14776, loss 0.00828703, acc 1
2016-09-06T12:23:33.140273: step 14777, loss 0.0238274, acc 0.98
2016-09-06T12:23:33.966866: step 14778, loss 0.00662847, acc 1
2016-09-06T12:23:34.794124: step 14779, loss 0.0140284, acc 1
2016-09-06T12:23:35.586668: step 14780, loss 0.00196033, acc 1
2016-09-06T12:23:36.422664: step 14781, loss 0.00409717, acc 1
2016-09-06T12:23:37.197551: step 14782, loss 0.00257346, acc 1
2016-09-06T12:23:38.050336: step 14783, loss 0.0249608, acc 0.98
2016-09-06T12:23:38.830426: step 14784, loss 0.00284912, acc 1
2016-09-06T12:23:39.593437: step 14785, loss 0.00190306, acc 1
2016-09-06T12:23:40.387714: step 14786, loss 0.0192127, acc 0.98
2016-09-06T12:23:41.203785: step 14787, loss 0.00361276, acc 1
2016-09-06T12:23:42.006089: step 14788, loss 0.021696, acc 0.98
2016-09-06T12:23:42.803910: step 14789, loss 0.00521606, acc 1
2016-09-06T12:23:43.630048: step 14790, loss 0.00583873, acc 1
2016-09-06T12:23:44.432497: step 14791, loss 0.00335536, acc 1
2016-09-06T12:23:45.236633: step 14792, loss 0.0150746, acc 1
2016-09-06T12:23:46.046382: step 14793, loss 0.0302827, acc 0.98
2016-09-06T12:23:46.860254: step 14794, loss 0.0756057, acc 0.96
2016-09-06T12:23:47.685500: step 14795, loss 0.00199308, acc 1
2016-09-06T12:23:48.497162: step 14796, loss 0.00214087, acc 1
2016-09-06T12:23:49.276424: step 14797, loss 0.0138733, acc 1
2016-09-06T12:23:50.108881: step 14798, loss 0.0241886, acc 0.98
2016-09-06T12:23:50.898092: step 14799, loss 0.076851, acc 0.96
2016-09-06T12:23:51.695003: step 14800, loss 0.00194533, acc 1

Evaluation:
2016-09-06T12:23:55.417004: step 14800, loss 2.11355, acc 0.745779

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-14800

2016-09-06T12:23:57.299430: step 14801, loss 0.00530593, acc 1
2016-09-06T12:23:58.117302: step 14802, loss 0.00531147, acc 1
2016-09-06T12:23:58.927239: step 14803, loss 0.00577086, acc 1
2016-09-06T12:23:59.749156: step 14804, loss 0.0179609, acc 0.98
2016-09-06T12:24:00.581240: step 14805, loss 0.00209519, acc 1
2016-09-06T12:24:01.405461: step 14806, loss 0.00286377, acc 1
2016-09-06T12:24:02.222662: step 14807, loss 0.00434484, acc 1
2016-09-06T12:24:03.029821: step 14808, loss 0.0125525, acc 1
2016-09-06T12:24:03.815291: step 14809, loss 0.00441392, acc 1
2016-09-06T12:24:04.636516: step 14810, loss 0.00654504, acc 1
2016-09-06T12:24:05.475642: step 14811, loss 0.00665778, acc 1
2016-09-06T12:24:06.258246: step 14812, loss 0.00188939, acc 1
2016-09-06T12:24:07.064222: step 14813, loss 0.00279878, acc 1
2016-09-06T12:24:07.897425: step 14814, loss 0.00415885, acc 1
2016-09-06T12:24:08.722473: step 14815, loss 0.00517833, acc 1
2016-09-06T12:24:09.545088: step 14816, loss 0.0345659, acc 0.98
2016-09-06T12:24:10.376626: step 14817, loss 0.0126048, acc 1
2016-09-06T12:24:11.197703: step 14818, loss 0.00699306, acc 1
2016-09-06T12:24:12.017923: step 14819, loss 0.00189972, acc 1
2016-09-06T12:24:12.842865: step 14820, loss 0.00191928, acc 1
2016-09-06T12:24:13.668190: step 14821, loss 0.0193032, acc 0.98
2016-09-06T12:24:14.464907: step 14822, loss 0.00442213, acc 1
2016-09-06T12:24:15.311347: step 14823, loss 0.00251511, acc 1
2016-09-06T12:24:16.117381: step 14824, loss 0.0102321, acc 1
2016-09-06T12:24:16.935465: step 14825, loss 0.00199274, acc 1
2016-09-06T12:24:17.734899: step 14826, loss 0.00205537, acc 1
2016-09-06T12:24:18.555145: step 14827, loss 0.0157045, acc 1
2016-09-06T12:24:19.362730: step 14828, loss 0.00436552, acc 1
2016-09-06T12:24:20.194612: step 14829, loss 0.00198107, acc 1
2016-09-06T12:24:20.988162: step 14830, loss 0.0323454, acc 0.98
2016-09-06T12:24:21.809842: step 14831, loss 0.00378422, acc 1
2016-09-06T12:24:22.657513: step 14832, loss 0.0191644, acc 1
2016-09-06T12:24:23.491685: step 14833, loss 0.0287459, acc 1
2016-09-06T12:24:24.336590: step 14834, loss 0.00251972, acc 1
2016-09-06T12:24:25.151543: step 14835, loss 0.0545798, acc 0.98
2016-09-06T12:24:25.981414: step 14836, loss 0.0273131, acc 1
2016-09-06T12:24:26.777402: step 14837, loss 0.0119574, acc 1
2016-09-06T12:24:27.603352: step 14838, loss 0.00896452, acc 1
2016-09-06T12:24:28.407555: step 14839, loss 0.00236229, acc 1
2016-09-06T12:24:29.183163: step 14840, loss 0.00209174, acc 1
2016-09-06T12:24:29.985272: step 14841, loss 0.00199116, acc 1
2016-09-06T12:24:30.801406: step 14842, loss 0.025982, acc 1
2016-09-06T12:24:31.589317: step 14843, loss 0.0094297, acc 1
2016-09-06T12:24:32.389586: step 14844, loss 0.00212815, acc 1
2016-09-06T12:24:33.251707: step 14845, loss 0.0224975, acc 1
2016-09-06T12:24:34.072329: step 14846, loss 0.00180454, acc 1
2016-09-06T12:24:34.889938: step 14847, loss 0.00449357, acc 1
2016-09-06T12:24:35.708674: step 14848, loss 0.0170559, acc 1
2016-09-06T12:24:36.541220: step 14849, loss 0.00945804, acc 1
2016-09-06T12:24:37.376480: step 14850, loss 0.00662199, acc 1
2016-09-06T12:24:38.204713: step 14851, loss 0.0193546, acc 1
2016-09-06T12:24:39.027152: step 14852, loss 0.125023, acc 0.94
2016-09-06T12:24:39.840437: step 14853, loss 0.0583068, acc 0.98
2016-09-06T12:24:40.702239: step 14854, loss 0.00576717, acc 1
2016-09-06T12:24:41.507595: step 14855, loss 0.00554035, acc 1
2016-09-06T12:24:42.336646: step 14856, loss 0.00179755, acc 1
2016-09-06T12:24:43.205405: step 14857, loss 0.0100103, acc 1
2016-09-06T12:24:44.018432: step 14858, loss 0.0169273, acc 0.98
2016-09-06T12:24:44.821318: step 14859, loss 0.0176655, acc 0.98
2016-09-06T12:24:45.640041: step 14860, loss 0.00529182, acc 1
2016-09-06T12:24:46.483714: step 14861, loss 0.0175909, acc 0.98
2016-09-06T12:24:47.288462: step 14862, loss 0.00549936, acc 1
2016-09-06T12:24:48.095298: step 14863, loss 0.00547199, acc 1
2016-09-06T12:24:48.918633: step 14864, loss 0.00191289, acc 1
2016-09-06T12:24:49.716908: step 14865, loss 0.00182308, acc 1
2016-09-06T12:24:50.535491: step 14866, loss 0.0136949, acc 1
2016-09-06T12:24:51.369613: step 14867, loss 0.0364005, acc 0.98
2016-09-06T12:24:52.142800: step 14868, loss 0.00177007, acc 1
2016-09-06T12:24:52.967415: step 14869, loss 0.00433137, acc 1
2016-09-06T12:24:53.786580: step 14870, loss 0.0189282, acc 0.98
2016-09-06T12:24:54.552473: step 14871, loss 0.0018339, acc 1
2016-09-06T12:24:55.363397: step 14872, loss 0.00456325, acc 1
2016-09-06T12:24:56.216780: step 14873, loss 0.00257824, acc 1
2016-09-06T12:24:57.001401: step 14874, loss 0.00194004, acc 1
2016-09-06T12:24:57.794535: step 14875, loss 0.023073, acc 0.98
2016-09-06T12:24:58.623517: step 14876, loss 0.00200479, acc 1
2016-09-06T12:24:59.403962: step 14877, loss 0.0111808, acc 1
2016-09-06T12:25:00.214532: step 14878, loss 0.00188207, acc 1
2016-09-06T12:25:01.050237: step 14879, loss 0.00358772, acc 1
2016-09-06T12:25:01.851678: step 14880, loss 0.0123276, acc 1
2016-09-06T12:25:02.662138: step 14881, loss 0.00184309, acc 1
2016-09-06T12:25:03.491614: step 14882, loss 0.00474608, acc 1
2016-09-06T12:25:04.312792: step 14883, loss 0.0174151, acc 0.98
2016-09-06T12:25:05.117543: step 14884, loss 0.00464712, acc 1
2016-09-06T12:25:05.977589: step 14885, loss 0.0548248, acc 0.98
2016-09-06T12:25:06.786241: step 14886, loss 0.00337639, acc 1
2016-09-06T12:25:07.557667: step 14887, loss 0.00664937, acc 1
2016-09-06T12:25:08.390585: step 14888, loss 0.00177698, acc 1
2016-09-06T12:25:09.190182: step 14889, loss 0.0277162, acc 0.98
2016-09-06T12:25:10.011956: step 14890, loss 0.00329591, acc 1
2016-09-06T12:25:10.836434: step 14891, loss 0.0273931, acc 0.98
2016-09-06T12:25:11.650303: step 14892, loss 0.00880854, acc 1
2016-09-06T12:25:12.496579: step 14893, loss 0.0211264, acc 0.98
2016-09-06T12:25:13.345150: step 14894, loss 0.0621237, acc 0.96
2016-09-06T12:25:14.141048: step 14895, loss 0.00168843, acc 1
2016-09-06T12:25:14.939469: step 14896, loss 0.0368708, acc 0.98
2016-09-06T12:25:15.768415: step 14897, loss 0.00472652, acc 1
2016-09-06T12:25:16.577273: step 14898, loss 0.166126, acc 0.96
2016-09-06T12:25:17.395982: step 14899, loss 0.0055822, acc 1
2016-09-06T12:25:18.243969: step 14900, loss 0.00541847, acc 1

Evaluation:
2016-09-06T12:25:22.002814: step 14900, loss 1.81598, acc 0.745779

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-14900

2016-09-06T12:25:23.948071: step 14901, loss 0.0546333, acc 0.98
2016-09-06T12:25:24.749013: step 14902, loss 0.0024253, acc 1
2016-09-06T12:25:25.587571: step 14903, loss 0.0037982, acc 1
2016-09-06T12:25:26.401491: step 14904, loss 0.00327851, acc 1
2016-09-06T12:25:27.196168: step 14905, loss 0.00236973, acc 1
2016-09-06T12:25:28.022805: step 14906, loss 0.00272187, acc 1
2016-09-06T12:25:28.861497: step 14907, loss 0.01149, acc 1
2016-09-06T12:25:29.677944: step 14908, loss 0.0135451, acc 1
2016-09-06T12:25:30.494500: step 14909, loss 0.0163675, acc 1
2016-09-06T12:25:31.290960: step 14910, loss 0.0344729, acc 0.98
2016-09-06T12:25:32.113894: step 14911, loss 0.0229726, acc 1
2016-09-06T12:25:32.995024: step 14912, loss 0.076905, acc 0.98
2016-09-06T12:25:33.813447: step 14913, loss 0.0404208, acc 0.98
2016-09-06T12:25:34.581715: step 14914, loss 0.0481214, acc 0.98
2016-09-06T12:25:35.410851: step 14915, loss 0.0277161, acc 0.98
2016-09-06T12:25:36.213472: step 14916, loss 0.0131463, acc 1
2016-09-06T12:25:37.025498: step 14917, loss 0.0237159, acc 0.98
2016-09-06T12:25:37.837411: step 14918, loss 0.0288007, acc 1
2016-09-06T12:25:38.664470: step 14919, loss 0.0476146, acc 0.96
2016-09-06T12:25:39.474402: step 14920, loss 0.0141611, acc 1
2016-09-06T12:25:40.273628: step 14921, loss 0.0231299, acc 0.98
2016-09-06T12:25:41.096826: step 14922, loss 0.0222134, acc 1
2016-09-06T12:25:41.896136: step 14923, loss 0.00497314, acc 1
2016-09-06T12:25:42.706947: step 14924, loss 0.00682787, acc 1
2016-09-06T12:25:43.540638: step 14925, loss 0.0283815, acc 1
2016-09-06T12:25:44.350267: step 14926, loss 0.0125735, acc 1
2016-09-06T12:25:45.167569: step 14927, loss 0.0125976, acc 1
2016-09-06T12:25:45.993281: step 14928, loss 0.045221, acc 0.96
2016-09-06T12:25:46.822400: step 14929, loss 0.0212703, acc 0.98
2016-09-06T12:25:47.639965: step 14930, loss 0.0213348, acc 0.98
2016-09-06T12:25:48.455567: step 14931, loss 0.0144237, acc 1
2016-09-06T12:25:49.281963: step 14932, loss 0.0027998, acc 1
2016-09-06T12:25:50.090790: step 14933, loss 0.0349092, acc 0.98
2016-09-06T12:25:50.925054: step 14934, loss 0.0397566, acc 0.98
2016-09-06T12:25:51.749085: step 14935, loss 0.00312568, acc 1
2016-09-06T12:25:52.590648: step 14936, loss 0.0223065, acc 0.98
2016-09-06T12:25:53.454468: step 14937, loss 0.00294236, acc 1
2016-09-06T12:25:54.267271: step 14938, loss 0.00431978, acc 1
2016-09-06T12:25:55.127823: step 14939, loss 0.0027993, acc 1
2016-09-06T12:25:55.966584: step 14940, loss 0.0456203, acc 0.98
2016-09-06T12:25:56.787480: step 14941, loss 0.0280311, acc 0.98
2016-09-06T12:25:57.583868: step 14942, loss 0.0423872, acc 0.98
2016-09-06T12:25:58.384274: step 14943, loss 0.0501325, acc 0.98
2016-09-06T12:25:59.200328: step 14944, loss 0.0193449, acc 0.98
2016-09-06T12:25:59.991212: step 14945, loss 0.0522358, acc 0.96
2016-09-06T12:26:00.824358: step 14946, loss 0.0249941, acc 1
2016-09-06T12:26:01.666791: step 14947, loss 0.00289453, acc 1
2016-09-06T12:26:02.533387: step 14948, loss 0.0126174, acc 1
2016-09-06T12:26:03.361087: step 14949, loss 0.0188806, acc 0.98
2016-09-06T12:26:04.213408: step 14950, loss 0.0193507, acc 0.98
2016-09-06T12:26:05.059400: step 14951, loss 0.00676311, acc 1
2016-09-06T12:26:05.909509: step 14952, loss 0.014898, acc 1
2016-09-06T12:26:06.750434: step 14953, loss 0.0405195, acc 0.98
2016-09-06T12:26:07.554366: step 14954, loss 0.170098, acc 0.92
2016-09-06T12:26:08.374885: step 14955, loss 0.00452398, acc 1
2016-09-06T12:26:09.214482: step 14956, loss 0.0082422, acc 1
2016-09-06T12:26:10.057534: step 14957, loss 0.0116352, acc 1
2016-09-06T12:26:10.857470: step 14958, loss 0.0223881, acc 0.98
2016-09-06T12:26:11.637498: step 14959, loss 0.0215548, acc 1
2016-09-06T12:26:12.438618: step 14960, loss 0.00360239, acc 1
2016-09-06T12:26:13.245965: step 14961, loss 0.0187595, acc 0.98
2016-09-06T12:26:14.057677: step 14962, loss 0.0301046, acc 1
2016-09-06T12:26:14.879564: step 14963, loss 0.00943844, acc 1
2016-09-06T12:26:15.695457: step 14964, loss 0.0321725, acc 1
2016-09-06T12:26:16.515217: step 14965, loss 0.00245657, acc 1
2016-09-06T12:26:17.315462: step 14966, loss 0.0255227, acc 1
2016-09-06T12:26:18.145713: step 14967, loss 0.00318902, acc 1
2016-09-06T12:26:18.956641: step 14968, loss 0.00260687, acc 1
2016-09-06T12:26:19.797658: step 14969, loss 0.020595, acc 0.98
2016-09-06T12:26:20.598309: step 14970, loss 0.0114873, acc 1
2016-09-06T12:26:21.437791: step 14971, loss 0.0192769, acc 0.98
2016-09-06T12:26:22.270410: step 14972, loss 0.0026403, acc 1
2016-09-06T12:26:23.049459: step 14973, loss 0.00387588, acc 1
2016-09-06T12:26:23.861445: step 14974, loss 0.0224908, acc 0.98
2016-09-06T12:26:24.687092: step 14975, loss 0.0128293, acc 1
2016-09-06T12:26:25.407905: step 14976, loss 0.010284, acc 1
2016-09-06T12:26:26.214289: step 14977, loss 0.0464912, acc 0.98
2016-09-06T12:26:27.027135: step 14978, loss 0.0210979, acc 0.98
2016-09-06T12:26:27.821790: step 14979, loss 0.00422604, acc 1
2016-09-06T12:26:28.641246: step 14980, loss 0.00262281, acc 1
2016-09-06T12:26:29.470289: step 14981, loss 0.0413843, acc 0.96
2016-09-06T12:26:30.274028: step 14982, loss 0.0115672, acc 1
2016-09-06T12:26:31.075658: step 14983, loss 0.0209907, acc 1
2016-09-06T12:26:31.907780: step 14984, loss 0.0168567, acc 0.98
2016-09-06T12:26:32.713729: step 14985, loss 0.0491952, acc 0.96
2016-09-06T12:26:33.547731: step 14986, loss 0.00619905, acc 1
2016-09-06T12:26:34.406405: step 14987, loss 0.0696086, acc 0.98
2016-09-06T12:26:35.220219: step 14988, loss 0.0026473, acc 1
2016-09-06T12:26:36.035972: step 14989, loss 0.0872711, acc 0.98
2016-09-06T12:26:36.874284: step 14990, loss 0.0179202, acc 0.98
2016-09-06T12:26:37.690016: step 14991, loss 0.100134, acc 0.96
2016-09-06T12:26:38.508456: step 14992, loss 0.002464, acc 1
2016-09-06T12:26:39.340283: step 14993, loss 0.00501346, acc 1
2016-09-06T12:26:40.169333: step 14994, loss 0.00243085, acc 1
2016-09-06T12:26:40.997431: step 14995, loss 0.0160345, acc 1
2016-09-06T12:26:41.838789: step 14996, loss 0.00241529, acc 1
2016-09-06T12:26:42.661759: step 14997, loss 0.00298482, acc 1
2016-09-06T12:26:43.444922: step 14998, loss 0.0356739, acc 0.98
2016-09-06T12:26:44.290265: step 14999, loss 0.0021797, acc 1
2016-09-06T12:26:45.129392: step 15000, loss 0.0237966, acc 1

Evaluation:
2016-09-06T12:26:48.870867: step 15000, loss 1.72123, acc 0.743902

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-15000

2016-09-06T12:26:50.689671: step 15001, loss 0.0115604, acc 1
2016-09-06T12:26:51.507788: step 15002, loss 0.00224628, acc 1
2016-09-06T12:26:52.344079: step 15003, loss 0.0159384, acc 1
2016-09-06T12:26:53.161648: step 15004, loss 0.0420339, acc 0.96
2016-09-06T12:26:54.025319: step 15005, loss 0.0248377, acc 0.98
2016-09-06T12:26:54.856997: step 15006, loss 0.00865037, acc 1
2016-09-06T12:26:55.655961: step 15007, loss 0.00208725, acc 1
2016-09-06T12:26:56.478655: step 15008, loss 0.0566083, acc 0.96
2016-09-06T12:26:57.289603: step 15009, loss 0.00312903, acc 1
2016-09-06T12:26:58.071241: step 15010, loss 0.0109628, acc 1
2016-09-06T12:26:58.870883: step 15011, loss 0.00524679, acc 1
2016-09-06T12:26:59.685749: step 15012, loss 0.00430792, acc 1
2016-09-06T12:27:00.509459: step 15013, loss 0.0193678, acc 1
2016-09-06T12:27:01.319277: step 15014, loss 0.0237797, acc 1
2016-09-06T12:27:02.149439: step 15015, loss 0.00266386, acc 1
2016-09-06T12:27:02.945396: step 15016, loss 0.00214455, acc 1
2016-09-06T12:27:03.743428: step 15017, loss 0.0158141, acc 0.98
2016-09-06T12:27:04.545291: step 15018, loss 0.0269141, acc 0.98
2016-09-06T12:27:05.344448: step 15019, loss 0.0282173, acc 0.98
2016-09-06T12:27:06.150533: step 15020, loss 0.0116627, acc 1
2016-09-06T12:27:06.984775: step 15021, loss 0.00198565, acc 1
2016-09-06T12:27:07.773568: step 15022, loss 0.0548681, acc 0.98
2016-09-06T12:27:08.589998: step 15023, loss 0.00198792, acc 1
2016-09-06T12:27:09.370625: step 15024, loss 0.00529994, acc 1
2016-09-06T12:27:10.155414: step 15025, loss 0.0245824, acc 1
2016-09-06T12:27:10.997417: step 15026, loss 0.00183493, acc 1
2016-09-06T12:27:11.822843: step 15027, loss 0.00800869, acc 1
2016-09-06T12:27:12.598641: step 15028, loss 0.0350297, acc 0.98
2016-09-06T12:27:13.387800: step 15029, loss 0.015646, acc 1
2016-09-06T12:27:14.213782: step 15030, loss 0.00280645, acc 1
2016-09-06T12:27:14.994502: step 15031, loss 0.00412715, acc 1
2016-09-06T12:27:15.798108: step 15032, loss 0.00351906, acc 1
2016-09-06T12:27:16.633625: step 15033, loss 0.00235912, acc 1
2016-09-06T12:27:17.433330: step 15034, loss 0.0648335, acc 0.98
2016-09-06T12:27:18.242825: step 15035, loss 0.00443897, acc 1
2016-09-06T12:27:19.070602: step 15036, loss 0.00596622, acc 1
2016-09-06T12:27:19.842807: step 15037, loss 0.0485167, acc 0.98
2016-09-06T12:27:20.653365: step 15038, loss 0.0207608, acc 0.98
2016-09-06T12:27:21.496271: step 15039, loss 0.00233081, acc 1
2016-09-06T12:27:22.290793: step 15040, loss 0.0433815, acc 0.96
2016-09-06T12:27:23.115362: step 15041, loss 0.00200576, acc 1
2016-09-06T12:27:23.962945: step 15042, loss 0.06298, acc 0.98
2016-09-06T12:27:24.789459: step 15043, loss 0.00778355, acc 1
2016-09-06T12:27:25.601087: step 15044, loss 0.0304322, acc 0.98
2016-09-06T12:27:26.435993: step 15045, loss 0.0318892, acc 0.98
2016-09-06T12:27:27.269457: step 15046, loss 0.022876, acc 0.98
2016-09-06T12:27:28.096297: step 15047, loss 0.0151919, acc 1
2016-09-06T12:27:28.954100: step 15048, loss 0.00492553, acc 1
2016-09-06T12:27:29.764974: step 15049, loss 0.0160075, acc 1
2016-09-06T12:27:30.607800: step 15050, loss 0.00181979, acc 1
2016-09-06T12:27:31.414800: step 15051, loss 0.0365203, acc 0.98
2016-09-06T12:27:32.210317: step 15052, loss 0.0257732, acc 0.98
2016-09-06T12:27:33.016265: step 15053, loss 0.00207426, acc 1
2016-09-06T12:27:33.824267: step 15054, loss 0.00717928, acc 1
2016-09-06T12:27:34.653889: step 15055, loss 0.0236541, acc 0.98
2016-09-06T12:27:35.479919: step 15056, loss 0.00238754, acc 1
2016-09-06T12:27:36.319319: step 15057, loss 0.00933331, acc 1
2016-09-06T12:27:37.123676: step 15058, loss 0.00396985, acc 1
2016-09-06T12:27:37.938589: step 15059, loss 0.0291202, acc 0.98
2016-09-06T12:27:38.725156: step 15060, loss 0.00253794, acc 1
2016-09-06T12:27:39.521565: step 15061, loss 0.029298, acc 1
2016-09-06T12:27:40.311558: step 15062, loss 0.0489914, acc 0.96
2016-09-06T12:27:41.123488: step 15063, loss 0.0091076, acc 1
2016-09-06T12:27:41.941055: step 15064, loss 0.0117811, acc 1
2016-09-06T12:27:42.740288: step 15065, loss 0.0145285, acc 1
2016-09-06T12:27:43.567386: step 15066, loss 0.010852, acc 1
2016-09-06T12:27:44.380779: step 15067, loss 0.0155961, acc 1
2016-09-06T12:27:45.182515: step 15068, loss 0.00197951, acc 1
2016-09-06T12:27:45.971983: step 15069, loss 0.00818594, acc 1
2016-09-06T12:27:46.795216: step 15070, loss 0.00203591, acc 1
2016-09-06T12:27:47.565397: step 15071, loss 0.0121019, acc 1
2016-09-06T12:27:48.381837: step 15072, loss 0.001984, acc 1
2016-09-06T12:27:49.202071: step 15073, loss 0.0039325, acc 1
2016-09-06T12:27:49.980255: step 15074, loss 0.0751444, acc 0.94
2016-09-06T12:27:50.794380: step 15075, loss 0.00201574, acc 1
2016-09-06T12:27:51.605640: step 15076, loss 0.0406322, acc 0.96
2016-09-06T12:27:52.400984: step 15077, loss 0.00236771, acc 1
2016-09-06T12:27:53.214712: step 15078, loss 0.00599836, acc 1
2016-09-06T12:27:54.004652: step 15079, loss 0.0229166, acc 0.98
2016-09-06T12:27:54.811135: step 15080, loss 0.00195278, acc 1
2016-09-06T12:27:55.634261: step 15081, loss 0.0230138, acc 1
2016-09-06T12:27:56.495001: step 15082, loss 0.00228929, acc 1
2016-09-06T12:27:57.296736: step 15083, loss 0.0405666, acc 0.96
2016-09-06T12:27:58.095372: step 15084, loss 0.00945967, acc 1
2016-09-06T12:27:58.922207: step 15085, loss 0.0298651, acc 0.98
2016-09-06T12:27:59.711434: step 15086, loss 0.0514682, acc 0.98
2016-09-06T12:28:00.541430: step 15087, loss 0.00310842, acc 1
2016-09-06T12:28:01.358661: step 15088, loss 0.0112265, acc 1
2016-09-06T12:28:02.156423: step 15089, loss 0.0583094, acc 0.96
2016-09-06T12:28:02.966623: step 15090, loss 0.0120572, acc 1
2016-09-06T12:28:03.786542: step 15091, loss 0.00305754, acc 1
2016-09-06T12:28:04.589208: step 15092, loss 0.0326562, acc 1
2016-09-06T12:28:05.383087: step 15093, loss 0.0107838, acc 1
2016-09-06T12:28:06.218019: step 15094, loss 0.00459371, acc 1
2016-09-06T12:28:07.013113: step 15095, loss 0.00237421, acc 1
2016-09-06T12:28:07.818413: step 15096, loss 0.0113138, acc 1
2016-09-06T12:28:08.636251: step 15097, loss 0.00203501, acc 1
2016-09-06T12:28:09.451114: step 15098, loss 0.00219573, acc 1
2016-09-06T12:28:10.257276: step 15099, loss 0.018047, acc 0.98
2016-09-06T12:28:11.090423: step 15100, loss 0.0238297, acc 1

Evaluation:
2016-09-06T12:28:14.830544: step 15100, loss 2.36949, acc 0.745779

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-15100

2016-09-06T12:28:16.745207: step 15101, loss 0.0137791, acc 1
2016-09-06T12:28:17.533946: step 15102, loss 0.00334118, acc 1
2016-09-06T12:28:18.351204: step 15103, loss 0.00359607, acc 1
2016-09-06T12:28:19.120422: step 15104, loss 0.00225342, acc 1
2016-09-06T12:28:19.918078: step 15105, loss 0.0131573, acc 1
2016-09-06T12:28:20.726006: step 15106, loss 0.0154599, acc 1
2016-09-06T12:28:21.500015: step 15107, loss 0.0179861, acc 0.98
2016-09-06T12:28:22.303959: step 15108, loss 0.00532052, acc 1
2016-09-06T12:28:23.113836: step 15109, loss 0.0928874, acc 0.98
2016-09-06T12:28:23.916610: step 15110, loss 0.0201175, acc 0.98
2016-09-06T12:28:24.697876: step 15111, loss 0.0144939, acc 1
2016-09-06T12:28:25.545761: step 15112, loss 0.0185346, acc 0.98
2016-09-06T12:28:26.321538: step 15113, loss 0.0264582, acc 0.98
2016-09-06T12:28:27.114983: step 15114, loss 0.031113, acc 0.98
2016-09-06T12:28:27.936974: step 15115, loss 0.00205849, acc 1
2016-09-06T12:28:28.737142: step 15116, loss 0.00230876, acc 1
2016-09-06T12:28:29.539459: step 15117, loss 0.0435073, acc 0.98
2016-09-06T12:28:30.355432: step 15118, loss 0.00400522, acc 1
2016-09-06T12:28:31.157811: step 15119, loss 0.00212708, acc 1
2016-09-06T12:28:31.965414: step 15120, loss 0.015327, acc 1
2016-09-06T12:28:32.823047: step 15121, loss 0.0542355, acc 0.98
2016-09-06T12:28:33.636527: step 15122, loss 0.190022, acc 0.94
2016-09-06T12:28:34.434272: step 15123, loss 0.0114078, acc 1
2016-09-06T12:28:35.280426: step 15124, loss 0.00597341, acc 1
2016-09-06T12:28:36.112961: step 15125, loss 0.0021158, acc 1
2016-09-06T12:28:36.922019: step 15126, loss 0.0125888, acc 1
2016-09-06T12:28:37.768028: step 15127, loss 0.00443212, acc 1
2016-09-06T12:28:38.596277: step 15128, loss 0.0017303, acc 1
2016-09-06T12:28:39.399783: step 15129, loss 0.00780367, acc 1
2016-09-06T12:28:40.202494: step 15130, loss 0.00517732, acc 1
2016-09-06T12:28:41.007673: step 15131, loss 0.0161847, acc 1
2016-09-06T12:28:41.829768: step 15132, loss 0.0254684, acc 0.98
2016-09-06T12:28:42.656522: step 15133, loss 0.0137645, acc 1
2016-09-06T12:28:43.486723: step 15134, loss 0.00301992, acc 1
2016-09-06T12:28:44.295673: step 15135, loss 0.00366822, acc 1
2016-09-06T12:28:45.127600: step 15136, loss 0.0759786, acc 0.94
2016-09-06T12:28:45.946156: step 15137, loss 0.0358128, acc 0.98
2016-09-06T12:28:46.757625: step 15138, loss 0.0168961, acc 0.98
2016-09-06T12:28:47.608240: step 15139, loss 0.0234881, acc 1
2016-09-06T12:28:48.421998: step 15140, loss 0.00789347, acc 1
2016-09-06T12:28:49.221902: step 15141, loss 0.00306666, acc 1
2016-09-06T12:28:50.053385: step 15142, loss 0.0164537, acc 1
2016-09-06T12:28:50.878767: step 15143, loss 0.00282452, acc 1
2016-09-06T12:28:51.647668: step 15144, loss 0.0109157, acc 1
2016-09-06T12:28:52.468634: step 15145, loss 0.0434321, acc 0.96
2016-09-06T12:28:53.294954: step 15146, loss 0.00403949, acc 1
2016-09-06T12:28:54.077593: step 15147, loss 0.00567956, acc 1
2016-09-06T12:28:54.923832: step 15148, loss 0.0105957, acc 1
2016-09-06T12:28:55.754267: step 15149, loss 0.00290915, acc 1
2016-09-06T12:28:56.544418: step 15150, loss 0.0474271, acc 0.98
2016-09-06T12:28:57.350446: step 15151, loss 0.0180503, acc 1
2016-09-06T12:28:58.180474: step 15152, loss 0.00592536, acc 1
2016-09-06T12:28:58.950185: step 15153, loss 0.0224722, acc 0.98
2016-09-06T12:28:59.748664: step 15154, loss 0.0225844, acc 1
2016-09-06T12:29:00.611270: step 15155, loss 0.0136051, acc 1
2016-09-06T12:29:01.411307: step 15156, loss 0.0384246, acc 0.96
2016-09-06T12:29:02.206153: step 15157, loss 0.0121917, acc 1
2016-09-06T12:29:03.026145: step 15158, loss 0.0126806, acc 1
2016-09-06T12:29:03.808545: step 15159, loss 0.00247709, acc 1
2016-09-06T12:29:04.641790: step 15160, loss 0.00253502, acc 1
2016-09-06T12:29:05.474265: step 15161, loss 0.00259131, acc 1
2016-09-06T12:29:06.250985: step 15162, loss 0.0265321, acc 1
2016-09-06T12:29:07.016298: step 15163, loss 0.0305693, acc 0.98
2016-09-06T12:29:07.837347: step 15164, loss 0.00274724, acc 1
2016-09-06T12:29:08.604594: step 15165, loss 0.016909, acc 0.98
2016-09-06T12:29:09.428319: step 15166, loss 0.00293002, acc 1
2016-09-06T12:29:10.271216: step 15167, loss 0.00269782, acc 1
2016-09-06T12:29:11.012336: step 15168, loss 0.00314534, acc 1
2016-09-06T12:29:11.828904: step 15169, loss 0.00399873, acc 1
2016-09-06T12:29:12.656605: step 15170, loss 0.00485711, acc 1
2016-09-06T12:29:13.447508: step 15171, loss 0.00252659, acc 1
2016-09-06T12:29:14.241466: step 15172, loss 0.00262983, acc 1
2016-09-06T12:29:15.071091: step 15173, loss 0.0170446, acc 1
2016-09-06T12:29:15.871308: step 15174, loss 0.00261576, acc 1
2016-09-06T12:29:16.670232: step 15175, loss 0.0176668, acc 1
2016-09-06T12:29:17.476492: step 15176, loss 0.017198, acc 1
2016-09-06T12:29:18.262803: step 15177, loss 0.0336098, acc 0.98
2016-09-06T12:29:19.091262: step 15178, loss 0.0176085, acc 0.98
2016-09-06T12:29:19.912343: step 15179, loss 0.00261655, acc 1
2016-09-06T12:29:20.723976: step 15180, loss 0.0636619, acc 0.98
2016-09-06T12:29:21.553588: step 15181, loss 0.0142069, acc 1
2016-09-06T12:29:22.371122: step 15182, loss 0.0176845, acc 0.98
2016-09-06T12:29:23.173210: step 15183, loss 0.039454, acc 0.98
2016-09-06T12:29:23.986125: step 15184, loss 0.00251471, acc 1
2016-09-06T12:29:24.833914: step 15185, loss 0.00282204, acc 1
2016-09-06T12:29:25.649839: step 15186, loss 0.00244832, acc 1
2016-09-06T12:29:26.486483: step 15187, loss 0.00902497, acc 1
2016-09-06T12:29:27.318967: step 15188, loss 0.0222757, acc 0.98
2016-09-06T12:29:28.143868: step 15189, loss 0.00214713, acc 1
2016-09-06T12:29:28.982029: step 15190, loss 0.00243869, acc 1
2016-09-06T12:29:29.829945: step 15191, loss 0.0437525, acc 0.98
2016-09-06T12:29:30.683441: step 15192, loss 0.00606595, acc 1
2016-09-06T12:29:31.478412: step 15193, loss 0.00753808, acc 1
2016-09-06T12:29:32.330367: step 15194, loss 0.0161927, acc 1
2016-09-06T12:29:33.182928: step 15195, loss 0.00385884, acc 1
2016-09-06T12:29:33.988880: step 15196, loss 0.010538, acc 1
2016-09-06T12:29:34.825883: step 15197, loss 0.015698, acc 1
2016-09-06T12:29:35.666290: step 15198, loss 0.030101, acc 0.98
2016-09-06T12:29:36.470093: step 15199, loss 0.00245968, acc 1
2016-09-06T12:29:37.277980: step 15200, loss 0.0152487, acc 1

Evaluation:
2016-09-06T12:29:40.987819: step 15200, loss 2.28747, acc 0.741088

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-15200

2016-09-06T12:29:42.871461: step 15201, loss 0.0357074, acc 0.98
2016-09-06T12:29:43.679807: step 15202, loss 0.010464, acc 1
2016-09-06T12:29:44.495973: step 15203, loss 0.00211005, acc 1
2016-09-06T12:29:45.301281: step 15204, loss 0.00197244, acc 1
2016-09-06T12:29:46.106927: step 15205, loss 0.0761825, acc 0.98
2016-09-06T12:29:46.933320: step 15206, loss 0.0156834, acc 1
2016-09-06T12:29:47.751503: step 15207, loss 0.0720956, acc 0.98
2016-09-06T12:29:48.573582: step 15208, loss 0.00323033, acc 1
2016-09-06T12:29:49.404027: step 15209, loss 0.00666775, acc 1
2016-09-06T12:29:50.195285: step 15210, loss 0.00327874, acc 1
2016-09-06T12:29:50.997747: step 15211, loss 0.0146547, acc 1
2016-09-06T12:29:51.833527: step 15212, loss 0.002775, acc 1
2016-09-06T12:29:52.640466: step 15213, loss 0.00843928, acc 1
2016-09-06T12:29:53.442264: step 15214, loss 0.0104846, acc 1
2016-09-06T12:29:54.249960: step 15215, loss 0.00254588, acc 1
2016-09-06T12:29:55.062288: step 15216, loss 0.0256843, acc 1
2016-09-06T12:29:55.859638: step 15217, loss 0.0877273, acc 0.98
2016-09-06T12:29:56.662155: step 15218, loss 0.00236579, acc 1
2016-09-06T12:29:57.488766: step 15219, loss 0.0166354, acc 0.98
2016-09-06T12:29:58.283295: step 15220, loss 0.013601, acc 1
2016-09-06T12:29:59.083409: step 15221, loss 0.0214685, acc 0.98
2016-09-06T12:29:59.911453: step 15222, loss 0.00218911, acc 1
2016-09-06T12:30:00.723265: step 15223, loss 0.00229937, acc 1
2016-09-06T12:30:01.555862: step 15224, loss 0.0131356, acc 1
2016-09-06T12:30:02.373261: step 15225, loss 0.00225852, acc 1
2016-09-06T12:30:03.153867: step 15226, loss 0.0362437, acc 0.96
2016-09-06T12:30:03.970762: step 15227, loss 0.00726301, acc 1
2016-09-06T12:30:04.784003: step 15228, loss 0.00181251, acc 1
2016-09-06T12:30:05.555733: step 15229, loss 0.0104114, acc 1
2016-09-06T12:30:06.381998: step 15230, loss 0.00306752, acc 1
2016-09-06T12:30:07.220744: step 15231, loss 0.00188682, acc 1
2016-09-06T12:30:08.019141: step 15232, loss 0.0178487, acc 1
2016-09-06T12:30:08.835671: step 15233, loss 0.025412, acc 0.98
2016-09-06T12:30:09.649874: step 15234, loss 0.00291883, acc 1
2016-09-06T12:30:10.444979: step 15235, loss 0.0166459, acc 0.98
2016-09-06T12:30:11.243202: step 15236, loss 0.0546925, acc 0.96
2016-09-06T12:30:12.102250: step 15237, loss 0.0389833, acc 0.96
2016-09-06T12:30:12.918969: step 15238, loss 0.0018807, acc 1
2016-09-06T12:30:13.722672: step 15239, loss 0.0132473, acc 1
2016-09-06T12:30:14.559316: step 15240, loss 0.0314773, acc 0.98
2016-09-06T12:30:15.362124: step 15241, loss 0.0156565, acc 1
2016-09-06T12:30:16.172614: step 15242, loss 0.00187318, acc 1
2016-09-06T12:30:17.002886: step 15243, loss 0.0265102, acc 1
2016-09-06T12:30:17.815113: step 15244, loss 0.0371465, acc 0.96
2016-09-06T12:30:18.623415: step 15245, loss 0.0304482, acc 0.98
2016-09-06T12:30:19.473223: step 15246, loss 0.0144607, acc 1
2016-09-06T12:30:20.268624: step 15247, loss 0.00430775, acc 1
2016-09-06T12:30:21.075627: step 15248, loss 0.00177234, acc 1
2016-09-06T12:30:21.916232: step 15249, loss 0.0227908, acc 0.98
2016-09-06T12:30:22.743082: step 15250, loss 0.0189982, acc 1
2016-09-06T12:30:23.533440: step 15251, loss 0.0411959, acc 0.96
2016-09-06T12:30:24.375828: step 15252, loss 0.00277995, acc 1
2016-09-06T12:30:25.173237: step 15253, loss 0.00359785, acc 1
2016-09-06T12:30:25.983329: step 15254, loss 0.00986344, acc 1
2016-09-06T12:30:26.837638: step 15255, loss 0.00174564, acc 1
2016-09-06T12:30:27.615962: step 15256, loss 0.0270011, acc 0.98
2016-09-06T12:30:28.418252: step 15257, loss 0.011282, acc 1
2016-09-06T12:30:29.255730: step 15258, loss 0.0346094, acc 0.98
2016-09-06T12:30:30.053418: step 15259, loss 0.0183288, acc 0.98
2016-09-06T12:30:30.839096: step 15260, loss 0.00167392, acc 1
2016-09-06T12:30:31.675017: step 15261, loss 0.00317844, acc 1
2016-09-06T12:30:32.448631: step 15262, loss 0.00180431, acc 1
2016-09-06T12:30:33.268479: step 15263, loss 0.0112605, acc 1
2016-09-06T12:30:34.106073: step 15264, loss 0.00166383, acc 1
2016-09-06T12:30:34.895453: step 15265, loss 0.00208464, acc 1
2016-09-06T12:30:35.708920: step 15266, loss 0.0166831, acc 0.98
2016-09-06T12:30:36.539759: step 15267, loss 0.0233519, acc 0.98
2016-09-06T12:30:37.380950: step 15268, loss 0.00654617, acc 1
2016-09-06T12:30:38.176693: step 15269, loss 0.0334974, acc 0.98
2016-09-06T12:30:38.974135: step 15270, loss 0.00385765, acc 1
2016-09-06T12:30:39.801415: step 15271, loss 0.00322725, acc 1
2016-09-06T12:30:40.582134: step 15272, loss 0.00159748, acc 1
2016-09-06T12:30:41.401666: step 15273, loss 0.0111619, acc 1
2016-09-06T12:30:42.226277: step 15274, loss 0.0121933, acc 1
2016-09-06T12:30:43.002172: step 15275, loss 0.0302998, acc 0.96
2016-09-06T12:30:43.796035: step 15276, loss 0.0536069, acc 0.98
2016-09-06T12:30:44.629511: step 15277, loss 0.0504985, acc 0.96
2016-09-06T12:30:45.434424: step 15278, loss 0.00410393, acc 1
2016-09-06T12:30:46.234215: step 15279, loss 0.00265396, acc 1
2016-09-06T12:30:47.050202: step 15280, loss 0.00753637, acc 1
2016-09-06T12:30:47.844038: step 15281, loss 0.0144619, acc 1
2016-09-06T12:30:48.661478: step 15282, loss 0.00230163, acc 1
2016-09-06T12:30:49.502926: step 15283, loss 0.0188518, acc 0.98
2016-09-06T12:30:50.281393: step 15284, loss 0.00230131, acc 1
2016-09-06T12:30:51.067778: step 15285, loss 0.00452879, acc 1
2016-09-06T12:30:51.911976: step 15286, loss 0.00365879, acc 1
2016-09-06T12:30:52.706150: step 15287, loss 0.00852985, acc 1
2016-09-06T12:30:53.511493: step 15288, loss 0.0392525, acc 0.98
2016-09-06T12:30:54.328906: step 15289, loss 0.00360867, acc 1
2016-09-06T12:30:55.128744: step 15290, loss 0.0455902, acc 0.98
2016-09-06T12:30:55.905778: step 15291, loss 0.0112736, acc 1
2016-09-06T12:30:56.737207: step 15292, loss 0.00167523, acc 1
2016-09-06T12:30:57.527703: step 15293, loss 0.064074, acc 0.98
2016-09-06T12:30:58.349207: step 15294, loss 0.0039147, acc 1
2016-09-06T12:30:59.150891: step 15295, loss 0.018482, acc 0.98
2016-09-06T12:30:59.953633: step 15296, loss 0.0262543, acc 1
2016-09-06T12:31:00.751332: step 15297, loss 0.00615259, acc 1
2016-09-06T12:31:01.563953: step 15298, loss 0.0153042, acc 1
2016-09-06T12:31:02.376989: step 15299, loss 0.00690466, acc 1
2016-09-06T12:31:03.156173: step 15300, loss 0.00921923, acc 1

Evaluation:
2016-09-06T12:31:06.892980: step 15300, loss 1.97685, acc 0.732645

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-15300

2016-09-06T12:31:08.765340: step 15301, loss 0.00368918, acc 1
2016-09-06T12:31:09.583471: step 15302, loss 0.0114686, acc 1
2016-09-06T12:31:10.391234: step 15303, loss 0.0294884, acc 0.98
2016-09-06T12:31:11.214192: step 15304, loss 0.0197106, acc 0.98
2016-09-06T12:31:12.017736: step 15305, loss 0.0135183, acc 1
2016-09-06T12:31:12.838511: step 15306, loss 0.0396345, acc 0.96
2016-09-06T12:31:13.676094: step 15307, loss 0.00165539, acc 1
2016-09-06T12:31:14.473514: step 15308, loss 0.0105865, acc 1
2016-09-06T12:31:15.273120: step 15309, loss 0.025565, acc 0.98
2016-09-06T12:31:16.083767: step 15310, loss 0.0349967, acc 0.98
2016-09-06T12:31:16.862933: step 15311, loss 0.00197969, acc 1
2016-09-06T12:31:17.655842: step 15312, loss 0.0103786, acc 1
2016-09-06T12:31:18.506614: step 15313, loss 0.00797712, acc 1
2016-09-06T12:31:19.289387: step 15314, loss 0.00974517, acc 1
2016-09-06T12:31:20.080142: step 15315, loss 0.00345479, acc 1
2016-09-06T12:31:20.875879: step 15316, loss 0.00175928, acc 1
2016-09-06T12:31:21.670520: step 15317, loss 0.0192945, acc 0.98
2016-09-06T12:31:22.472426: step 15318, loss 0.0117771, acc 1
2016-09-06T12:31:23.304255: step 15319, loss 0.00215473, acc 1
2016-09-06T12:31:24.088185: step 15320, loss 0.00493793, acc 1
2016-09-06T12:31:24.901849: step 15321, loss 0.00879206, acc 1
2016-09-06T12:31:25.733430: step 15322, loss 0.00748604, acc 1
2016-09-06T12:31:26.541154: step 15323, loss 0.00408137, acc 1
2016-09-06T12:31:27.320744: step 15324, loss 0.0253268, acc 0.98
2016-09-06T12:31:28.163692: step 15325, loss 0.0112104, acc 1
2016-09-06T12:31:28.953860: step 15326, loss 0.00308872, acc 1
2016-09-06T12:31:29.756396: step 15327, loss 0.00193143, acc 1
2016-09-06T12:31:30.587284: step 15328, loss 0.028369, acc 1
2016-09-06T12:31:31.360409: step 15329, loss 0.0531539, acc 0.94
2016-09-06T12:31:32.164113: step 15330, loss 0.0192576, acc 0.98
2016-09-06T12:31:32.976105: step 15331, loss 0.0149278, acc 1
2016-09-06T12:31:33.780938: step 15332, loss 0.00240132, acc 1
2016-09-06T12:31:34.597355: step 15333, loss 0.00187135, acc 1
2016-09-06T12:31:35.426615: step 15334, loss 0.022028, acc 0.98
2016-09-06T12:31:36.213969: step 15335, loss 0.00185782, acc 1
2016-09-06T12:31:36.998130: step 15336, loss 0.0198206, acc 0.98
2016-09-06T12:31:37.833022: step 15337, loss 0.0273851, acc 0.98
2016-09-06T12:31:38.641257: step 15338, loss 0.0141674, acc 1
2016-09-06T12:31:39.438911: step 15339, loss 0.0341197, acc 0.96
2016-09-06T12:31:40.261408: step 15340, loss 0.00198434, acc 1
2016-09-06T12:31:41.059165: step 15341, loss 0.00428731, acc 1
2016-09-06T12:31:41.867465: step 15342, loss 0.0287886, acc 0.98
2016-09-06T12:31:42.670644: step 15343, loss 0.028376, acc 0.98
2016-09-06T12:31:43.432744: step 15344, loss 0.00423076, acc 1
2016-09-06T12:31:44.238928: step 15345, loss 0.00540469, acc 1
2016-09-06T12:31:45.074958: step 15346, loss 0.0804263, acc 0.94
2016-09-06T12:31:45.851856: step 15347, loss 0.0166699, acc 0.98
2016-09-06T12:31:46.682695: step 15348, loss 0.0148879, acc 1
2016-09-06T12:31:47.491647: step 15349, loss 0.0078635, acc 1
2016-09-06T12:31:48.293628: step 15350, loss 0.0426211, acc 0.98
2016-09-06T12:31:49.074025: step 15351, loss 0.00277337, acc 1
2016-09-06T12:31:49.914050: step 15352, loss 0.00174677, acc 1
2016-09-06T12:31:50.681802: step 15353, loss 0.00557153, acc 1
2016-09-06T12:31:51.488056: step 15354, loss 0.0156151, acc 0.98
2016-09-06T12:31:52.328203: step 15355, loss 0.0110593, acc 1
2016-09-06T12:31:53.088940: step 15356, loss 0.00170551, acc 1
2016-09-06T12:31:53.888950: step 15357, loss 0.0280757, acc 0.98
2016-09-06T12:31:54.711233: step 15358, loss 0.0164714, acc 1
2016-09-06T12:31:55.508413: step 15359, loss 0.00179056, acc 1
2016-09-06T12:31:56.273725: step 15360, loss 0.00167758, acc 1
2016-09-06T12:31:57.087987: step 15361, loss 0.0365949, acc 0.98
2016-09-06T12:31:57.882511: step 15362, loss 0.0208087, acc 0.98
2016-09-06T12:31:58.689773: step 15363, loss 0.0140009, acc 1
2016-09-06T12:31:59.522300: step 15364, loss 0.00294929, acc 1
2016-09-06T12:32:00.327391: step 15365, loss 0.00758566, acc 1
2016-09-06T12:32:01.134856: step 15366, loss 0.0618583, acc 0.98
2016-09-06T12:32:01.984924: step 15367, loss 0.00884903, acc 1
2016-09-06T12:32:02.762848: step 15368, loss 0.0154406, acc 1
2016-09-06T12:32:03.529453: step 15369, loss 0.0308721, acc 0.98
2016-09-06T12:32:04.350154: step 15370, loss 0.00240685, acc 1
2016-09-06T12:32:05.142065: step 15371, loss 0.0347116, acc 0.98
2016-09-06T12:32:05.960902: step 15372, loss 0.00485401, acc 1
2016-09-06T12:32:06.781748: step 15373, loss 0.00793924, acc 1
2016-09-06T12:32:07.573411: step 15374, loss 0.0304541, acc 0.98
2016-09-06T12:32:08.379190: step 15375, loss 0.0209317, acc 1
2016-09-06T12:32:09.189096: step 15376, loss 0.00303037, acc 1
2016-09-06T12:32:09.967597: step 15377, loss 0.0183304, acc 0.98
2016-09-06T12:32:10.779958: step 15378, loss 0.00685389, acc 1
2016-09-06T12:32:11.584542: step 15379, loss 0.0204847, acc 1
2016-09-06T12:32:12.397674: step 15380, loss 0.00351733, acc 1
2016-09-06T12:32:13.238076: step 15381, loss 0.00334842, acc 1
2016-09-06T12:32:14.075353: step 15382, loss 0.0162651, acc 1
2016-09-06T12:32:14.863258: step 15383, loss 0.00338735, acc 1
2016-09-06T12:32:15.673093: step 15384, loss 0.0243843, acc 0.98
2016-09-06T12:32:16.512471: step 15385, loss 0.00293911, acc 1
2016-09-06T12:32:17.359423: step 15386, loss 0.00320028, acc 1
2016-09-06T12:32:18.161943: step 15387, loss 0.0073553, acc 1
2016-09-06T12:32:19.038881: step 15388, loss 0.00311874, acc 1
2016-09-06T12:32:19.839972: step 15389, loss 0.00834397, acc 1
2016-09-06T12:32:20.643496: step 15390, loss 0.0300646, acc 0.98
2016-09-06T12:32:21.517192: step 15391, loss 0.0218691, acc 0.98
2016-09-06T12:32:22.347261: step 15392, loss 0.0148228, acc 1
2016-09-06T12:32:23.149105: step 15393, loss 0.061331, acc 0.98
2016-09-06T12:32:23.978624: step 15394, loss 0.00431955, acc 1
2016-09-06T12:32:24.794606: step 15395, loss 0.0038826, acc 1
2016-09-06T12:32:25.596827: step 15396, loss 0.00336147, acc 1
2016-09-06T12:32:26.414879: step 15397, loss 0.0188592, acc 1
2016-09-06T12:32:27.220651: step 15398, loss 0.00359571, acc 1
2016-09-06T12:32:28.053018: step 15399, loss 0.00890544, acc 1
2016-09-06T12:32:28.898478: step 15400, loss 0.00341469, acc 1

Evaluation:
2016-09-06T12:32:32.598679: step 15400, loss 3.20959, acc 0.734522

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-15400

2016-09-06T12:32:34.484089: step 15401, loss 0.0144809, acc 1
2016-09-06T12:32:35.280518: step 15402, loss 0.00470881, acc 1
2016-09-06T12:32:36.099493: step 15403, loss 0.0618095, acc 0.98
2016-09-06T12:32:36.889783: step 15404, loss 0.00336443, acc 1
2016-09-06T12:32:37.701124: step 15405, loss 0.00333739, acc 1
2016-09-06T12:32:38.505837: step 15406, loss 0.014668, acc 1
2016-09-06T12:32:39.316585: step 15407, loss 0.0153451, acc 1
2016-09-06T12:32:40.143488: step 15408, loss 0.00892487, acc 1
2016-09-06T12:32:40.960633: step 15409, loss 0.00712465, acc 1
2016-09-06T12:32:41.797007: step 15410, loss 0.00319319, acc 1
2016-09-06T12:32:42.605805: step 15411, loss 0.0345546, acc 0.98
2016-09-06T12:32:43.420811: step 15412, loss 0.0574977, acc 0.96
2016-09-06T12:32:44.243057: step 15413, loss 0.0195579, acc 0.98
2016-09-06T12:32:45.060018: step 15414, loss 0.0477888, acc 0.98
2016-09-06T12:32:45.924779: step 15415, loss 0.0185377, acc 0.98
2016-09-06T12:32:46.738410: step 15416, loss 0.00292035, acc 1
2016-09-06T12:32:47.546599: step 15417, loss 0.00286598, acc 1
2016-09-06T12:32:48.354166: step 15418, loss 0.0178879, acc 0.98
2016-09-06T12:32:49.190089: step 15419, loss 0.00277472, acc 1
2016-09-06T12:32:49.962878: step 15420, loss 0.0031208, acc 1
2016-09-06T12:32:50.770636: step 15421, loss 0.0117229, acc 1
2016-09-06T12:32:51.568519: step 15422, loss 0.00346468, acc 1
2016-09-06T12:32:52.378673: step 15423, loss 0.0026413, acc 1
2016-09-06T12:32:53.190784: step 15424, loss 0.00362005, acc 1
2016-09-06T12:32:53.988274: step 15425, loss 0.00868948, acc 1
2016-09-06T12:32:54.826871: step 15426, loss 0.0455541, acc 0.98
2016-09-06T12:32:55.630521: step 15427, loss 0.0206036, acc 1
2016-09-06T12:32:56.447574: step 15428, loss 0.00451075, acc 1
2016-09-06T12:32:57.247999: step 15429, loss 0.0261025, acc 0.98
2016-09-06T12:32:58.050393: step 15430, loss 0.0430671, acc 0.96
2016-09-06T12:32:58.856068: step 15431, loss 0.00505519, acc 1
2016-09-06T12:32:59.665837: step 15432, loss 0.00309392, acc 1
2016-09-06T12:33:00.493974: step 15433, loss 0.0699682, acc 0.98
2016-09-06T12:33:01.332912: step 15434, loss 0.00955448, acc 1
2016-09-06T12:33:02.122674: step 15435, loss 0.00294344, acc 1
2016-09-06T12:33:02.916481: step 15436, loss 0.016347, acc 0.98
2016-09-06T12:33:03.746774: step 15437, loss 0.0042119, acc 1
2016-09-06T12:33:04.557252: step 15438, loss 0.0190294, acc 0.98
2016-09-06T12:33:05.380618: step 15439, loss 0.00627308, acc 1
2016-09-06T12:33:06.197951: step 15440, loss 0.0229781, acc 0.98
2016-09-06T12:33:06.984344: step 15441, loss 0.00232175, acc 1
2016-09-06T12:33:07.814738: step 15442, loss 0.0170307, acc 1
2016-09-06T12:33:08.643587: step 15443, loss 0.104216, acc 0.98
2016-09-06T12:33:09.473941: step 15444, loss 0.00473396, acc 1
2016-09-06T12:33:10.301036: step 15445, loss 0.0132735, acc 1
2016-09-06T12:33:11.125898: step 15446, loss 0.00343467, acc 1
2016-09-06T12:33:11.999212: step 15447, loss 0.00352548, acc 1
2016-09-06T12:33:12.834414: step 15448, loss 0.0408444, acc 0.98
2016-09-06T12:33:13.684671: step 15449, loss 0.0259073, acc 0.98
2016-09-06T12:33:14.518058: step 15450, loss 0.0355285, acc 0.98
2016-09-06T12:33:15.335941: step 15451, loss 0.0122379, acc 1
2016-09-06T12:33:16.166375: step 15452, loss 0.00223253, acc 1
2016-09-06T12:33:16.997465: step 15453, loss 0.00276298, acc 1
2016-09-06T12:33:17.782499: step 15454, loss 0.00775803, acc 1
2016-09-06T12:33:18.581659: step 15455, loss 0.0197994, acc 0.98
2016-09-06T12:33:19.415457: step 15456, loss 0.00379499, acc 1
2016-09-06T12:33:20.191841: step 15457, loss 0.0053779, acc 1
2016-09-06T12:33:21.014623: step 15458, loss 0.0468779, acc 0.98
2016-09-06T12:33:21.833655: step 15459, loss 0.0108064, acc 1
2016-09-06T12:33:22.630779: step 15460, loss 0.0181032, acc 0.98
2016-09-06T12:33:23.435652: step 15461, loss 0.00290735, acc 1
2016-09-06T12:33:24.258015: step 15462, loss 0.0121208, acc 1
2016-09-06T12:33:25.096794: step 15463, loss 0.00497618, acc 1
2016-09-06T12:33:25.954572: step 15464, loss 0.00336726, acc 1
2016-09-06T12:33:26.778506: step 15465, loss 0.0194389, acc 1
2016-09-06T12:33:27.612294: step 15466, loss 0.00315391, acc 1
2016-09-06T12:33:28.426257: step 15467, loss 0.0367112, acc 0.98
2016-09-06T12:33:29.446819: step 15468, loss 0.0262295, acc 0.98
2016-09-06T12:33:30.285283: step 15469, loss 0.0256344, acc 0.98
2016-09-06T12:33:31.075443: step 15470, loss 0.0494674, acc 0.96
2016-09-06T12:33:31.895819: step 15471, loss 0.0033441, acc 1
2016-09-06T12:33:32.759802: step 15472, loss 0.00369404, acc 1
2016-09-06T12:33:33.595050: step 15473, loss 0.0153781, acc 1
2016-09-06T12:33:34.411973: step 15474, loss 0.0211334, acc 1
2016-09-06T12:33:35.306699: step 15475, loss 0.0370528, acc 0.98
2016-09-06T12:33:36.097926: step 15476, loss 0.00464543, acc 1
2016-09-06T12:33:36.921919: step 15477, loss 0.0285387, acc 0.98
2016-09-06T12:33:37.799870: step 15478, loss 0.0034284, acc 1
2016-09-06T12:33:38.626431: step 15479, loss 0.00327975, acc 1
2016-09-06T12:33:39.475562: step 15480, loss 0.0097202, acc 1
2016-09-06T12:33:40.342163: step 15481, loss 0.00321956, acc 1
2016-09-06T12:33:41.182715: step 15482, loss 0.00321509, acc 1
2016-09-06T12:33:41.976507: step 15483, loss 0.00358534, acc 1
2016-09-06T12:33:42.818930: step 15484, loss 0.0325701, acc 0.98
2016-09-06T12:33:43.647249: step 15485, loss 0.0101508, acc 1
2016-09-06T12:33:44.459624: step 15486, loss 0.00339476, acc 1
2016-09-06T12:33:45.293230: step 15487, loss 0.0136635, acc 1
2016-09-06T12:33:46.104829: step 15488, loss 0.00587204, acc 1
2016-09-06T12:33:46.891271: step 15489, loss 0.00319026, acc 1
2016-09-06T12:33:47.701214: step 15490, loss 0.100877, acc 0.96
2016-09-06T12:33:48.497210: step 15491, loss 0.00518681, acc 1
2016-09-06T12:33:49.312485: step 15492, loss 0.00371268, acc 1
2016-09-06T12:33:50.107442: step 15493, loss 0.0306119, acc 0.98
2016-09-06T12:33:50.937938: step 15494, loss 0.00343473, acc 1
2016-09-06T12:33:51.732366: step 15495, loss 0.0179603, acc 1
2016-09-06T12:33:52.544667: step 15496, loss 0.0130747, acc 1
2016-09-06T12:33:53.449012: step 15497, loss 0.0504401, acc 0.98
2016-09-06T12:33:54.261867: step 15498, loss 0.0272267, acc 0.98
2016-09-06T12:33:55.086788: step 15499, loss 0.00303483, acc 1
2016-09-06T12:33:55.963799: step 15500, loss 0.00302881, acc 1

Evaluation:
2016-09-06T12:33:59.680330: step 15500, loss 3.21189, acc 0.739212

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-15500

2016-09-06T12:34:01.620693: step 15501, loss 0.00495573, acc 1
2016-09-06T12:34:02.425323: step 15502, loss 0.00306356, acc 1
2016-09-06T12:34:03.460671: step 15503, loss 0.0170291, acc 1
2016-09-06T12:34:04.305711: step 15504, loss 0.0394094, acc 0.98
2016-09-06T12:34:05.123294: step 15505, loss 0.00303918, acc 1
2016-09-06T12:34:05.989641: step 15506, loss 0.00328438, acc 1
2016-09-06T12:34:06.847773: step 15507, loss 0.0280965, acc 0.98
2016-09-06T12:34:07.649185: step 15508, loss 0.00931848, acc 1
2016-09-06T12:34:08.450924: step 15509, loss 0.00424141, acc 1
2016-09-06T12:34:09.259655: step 15510, loss 0.029363, acc 0.98
2016-09-06T12:34:10.076852: step 15511, loss 0.00346707, acc 1
2016-09-06T12:34:10.896179: step 15512, loss 0.00302493, acc 1
2016-09-06T12:34:11.721480: step 15513, loss 0.00303554, acc 1
2016-09-06T12:34:12.571369: step 15514, loss 0.00433499, acc 1
2016-09-06T12:34:13.437680: step 15515, loss 0.00942487, acc 1
2016-09-06T12:34:14.245815: step 15516, loss 0.0896183, acc 0.98
2016-09-06T12:34:15.073539: step 15517, loss 0.033444, acc 0.98
2016-09-06T12:34:15.879734: step 15518, loss 0.0381808, acc 0.98
2016-09-06T12:34:16.710497: step 15519, loss 0.003332, acc 1
2016-09-06T12:34:17.521509: step 15520, loss 0.0254737, acc 1
2016-09-06T12:34:18.290033: step 15521, loss 0.0722432, acc 0.96
2016-09-06T12:34:19.086169: step 15522, loss 0.0117942, acc 1
2016-09-06T12:34:19.917510: step 15523, loss 0.00262337, acc 1
2016-09-06T12:34:20.669342: step 15524, loss 0.00595516, acc 1
2016-09-06T12:34:21.467177: step 15525, loss 0.00256885, acc 1
2016-09-06T12:34:22.299881: step 15526, loss 0.0410372, acc 0.96
2016-09-06T12:34:23.093937: step 15527, loss 0.00526996, acc 1
2016-09-06T12:34:23.893221: step 15528, loss 0.00500134, acc 1
2016-09-06T12:34:24.729506: step 15529, loss 0.0561308, acc 0.98
2016-09-06T12:34:25.522574: step 15530, loss 0.0611596, acc 0.98
2016-09-06T12:34:26.312077: step 15531, loss 0.0340238, acc 0.98
2016-09-06T12:34:27.114122: step 15532, loss 0.00379986, acc 1
2016-09-06T12:34:27.897821: step 15533, loss 0.00438318, acc 1
2016-09-06T12:34:28.705704: step 15534, loss 0.0143474, acc 1
2016-09-06T12:34:29.501357: step 15535, loss 0.0265877, acc 1
2016-09-06T12:34:30.343704: step 15536, loss 0.00233172, acc 1
2016-09-06T12:34:31.206321: step 15537, loss 0.0174239, acc 0.98
2016-09-06T12:34:32.013549: step 15538, loss 0.0172681, acc 1
2016-09-06T12:34:32.826001: step 15539, loss 0.0175661, acc 0.98
2016-09-06T12:34:33.640171: step 15540, loss 0.0275202, acc 1
2016-09-06T12:34:34.441701: step 15541, loss 0.015337, acc 1
2016-09-06T12:34:35.216140: step 15542, loss 0.00706072, acc 1
2016-09-06T12:34:36.008974: step 15543, loss 0.00758141, acc 1
2016-09-06T12:34:36.846144: step 15544, loss 0.023572, acc 1
2016-09-06T12:34:37.631293: step 15545, loss 0.00945739, acc 1
2016-09-06T12:34:38.450545: step 15546, loss 0.0167177, acc 0.98
2016-09-06T12:34:39.267443: step 15547, loss 0.00787285, acc 1
2016-09-06T12:34:40.062554: step 15548, loss 0.00214808, acc 1
2016-09-06T12:34:40.852901: step 15549, loss 0.0131505, acc 1
2016-09-06T12:34:41.645035: step 15550, loss 0.00266546, acc 1
2016-09-06T12:34:42.434161: step 15551, loss 0.0215837, acc 0.98
2016-09-06T12:34:43.198695: step 15552, loss 0.0255882, acc 1
2016-09-06T12:34:44.014758: step 15553, loss 0.0270246, acc 0.98
2016-09-06T12:34:44.831807: step 15554, loss 0.0303751, acc 0.98
2016-09-06T12:34:45.654885: step 15555, loss 0.0873832, acc 0.98
2016-09-06T12:34:46.451273: step 15556, loss 0.00402152, acc 1
2016-09-06T12:34:47.227051: step 15557, loss 0.0448586, acc 0.98
2016-09-06T12:34:48.039755: step 15558, loss 0.0592208, acc 0.96
2016-09-06T12:34:48.843445: step 15559, loss 0.0380321, acc 0.98
2016-09-06T12:34:49.670831: step 15560, loss 0.0207022, acc 0.98
2016-09-06T12:34:50.468441: step 15561, loss 0.0168912, acc 1
2016-09-06T12:34:51.290338: step 15562, loss 0.0369188, acc 0.98
2016-09-06T12:34:52.064290: step 15563, loss 0.00206639, acc 1
2016-09-06T12:34:52.881322: step 15564, loss 0.0395797, acc 0.98
2016-09-06T12:34:53.734781: step 15565, loss 0.050779, acc 0.98
2016-09-06T12:34:54.519379: step 15566, loss 0.018071, acc 0.98
2016-09-06T12:34:55.328208: step 15567, loss 0.0466157, acc 0.96
2016-09-06T12:34:56.132362: step 15568, loss 0.00912355, acc 1
2016-09-06T12:34:56.936572: step 15569, loss 0.027456, acc 0.98
2016-09-06T12:34:57.736027: step 15570, loss 0.00326733, acc 1
2016-09-06T12:34:58.551688: step 15571, loss 0.00900311, acc 1
2016-09-06T12:34:59.345823: step 15572, loss 0.00254877, acc 1
2016-09-06T12:35:00.155338: step 15573, loss 0.00397393, acc 1
2016-09-06T12:35:01.021009: step 15574, loss 0.00270404, acc 1
2016-09-06T12:35:01.823115: step 15575, loss 0.00678591, acc 1
2016-09-06T12:35:02.634032: step 15576, loss 0.0251489, acc 1
2016-09-06T12:35:03.457355: step 15577, loss 0.00406045, acc 1
2016-09-06T12:35:04.279453: step 15578, loss 0.0169114, acc 0.98
2016-09-06T12:35:05.087003: step 15579, loss 0.0134453, acc 1
2016-09-06T12:35:05.922608: step 15580, loss 0.00237233, acc 1
2016-09-06T12:35:06.727969: step 15581, loss 0.00960197, acc 1
2016-09-06T12:35:07.546002: step 15582, loss 0.0323409, acc 0.98
2016-09-06T12:35:08.374531: step 15583, loss 0.0022559, acc 1
2016-09-06T12:35:09.184035: step 15584, loss 0.0193827, acc 1
2016-09-06T12:35:09.993969: step 15585, loss 0.023527, acc 0.98
2016-09-06T12:35:10.821444: step 15586, loss 0.00236067, acc 1
2016-09-06T12:35:11.617952: step 15587, loss 0.0022624, acc 1
2016-09-06T12:35:12.414470: step 15588, loss 0.00233372, acc 1
2016-09-06T12:35:13.260897: step 15589, loss 0.0077123, acc 1
2016-09-06T12:35:14.077717: step 15590, loss 0.0167243, acc 1
2016-09-06T12:35:14.916623: step 15591, loss 0.0204762, acc 0.98
2016-09-06T12:35:15.757394: step 15592, loss 0.00272402, acc 1
2016-09-06T12:35:16.576311: step 15593, loss 0.00229273, acc 1
2016-09-06T12:35:17.378297: step 15594, loss 0.00246636, acc 1
2016-09-06T12:35:18.236009: step 15595, loss 0.00401068, acc 1
2016-09-06T12:35:19.074252: step 15596, loss 0.00264261, acc 1
2016-09-06T12:35:19.850194: step 15597, loss 0.00228687, acc 1
2016-09-06T12:35:20.660617: step 15598, loss 0.00631727, acc 1
2016-09-06T12:35:21.463514: step 15599, loss 0.0119191, acc 1
2016-09-06T12:35:22.314383: step 15600, loss 0.043818, acc 0.98

Evaluation:
2016-09-06T12:35:26.049423: step 15600, loss 2.59081, acc 0.729831

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-15600

2016-09-06T12:35:28.032110: step 15601, loss 0.0178723, acc 1
2016-09-06T12:35:28.829554: step 15602, loss 0.00213402, acc 1
2016-09-06T12:35:29.598147: step 15603, loss 0.022083, acc 0.98
2016-09-06T12:35:30.444667: step 15604, loss 0.00988251, acc 1
2016-09-06T12:35:31.265945: step 15605, loss 0.0608007, acc 0.96
2016-09-06T12:35:32.063712: step 15606, loss 0.0328836, acc 0.98
2016-09-06T12:35:32.853211: step 15607, loss 0.00205719, acc 1
2016-09-06T12:35:33.679502: step 15608, loss 0.0025593, acc 1
2016-09-06T12:35:34.474206: step 15609, loss 0.00200722, acc 1
2016-09-06T12:35:35.247324: step 15610, loss 0.0103131, acc 1
2016-09-06T12:35:36.057549: step 15611, loss 0.0182248, acc 1
2016-09-06T12:35:36.859638: step 15612, loss 0.00200128, acc 1
2016-09-06T12:35:37.676267: step 15613, loss 0.0240485, acc 0.98
2016-09-06T12:35:38.525549: step 15614, loss 0.00198778, acc 1
2016-09-06T12:35:39.353451: step 15615, loss 0.0263729, acc 0.98
2016-09-06T12:35:40.184874: step 15616, loss 0.00708728, acc 1
2016-09-06T12:35:41.024814: step 15617, loss 0.00227522, acc 1
2016-09-06T12:35:41.833431: step 15618, loss 0.0125093, acc 1
2016-09-06T12:35:42.657459: step 15619, loss 0.0109726, acc 1
2016-09-06T12:35:43.489875: step 15620, loss 0.0205671, acc 1
2016-09-06T12:35:44.310099: step 15621, loss 0.0319216, acc 0.98
2016-09-06T12:35:45.126236: step 15622, loss 0.00562931, acc 1
2016-09-06T12:35:45.948356: step 15623, loss 0.00432335, acc 1
2016-09-06T12:35:46.774089: step 15624, loss 0.0202463, acc 0.98
2016-09-06T12:35:47.582637: step 15625, loss 0.0080263, acc 1
2016-09-06T12:35:48.411986: step 15626, loss 0.0034786, acc 1
2016-09-06T12:35:49.227188: step 15627, loss 0.0545593, acc 0.98
2016-09-06T12:35:50.054901: step 15628, loss 0.0275293, acc 0.98
2016-09-06T12:35:50.866714: step 15629, loss 0.00468354, acc 1
2016-09-06T12:35:51.698174: step 15630, loss 0.00653298, acc 1
2016-09-06T12:35:52.497366: step 15631, loss 0.0288166, acc 0.98
2016-09-06T12:35:53.327202: step 15632, loss 0.00186975, acc 1
2016-09-06T12:35:54.109248: step 15633, loss 0.00407463, acc 1
2016-09-06T12:35:54.926623: step 15634, loss 0.0455059, acc 0.98
2016-09-06T12:35:55.766782: step 15635, loss 0.023144, acc 0.98
2016-09-06T12:35:56.591807: step 15636, loss 0.0019541, acc 1
2016-09-06T12:35:57.362671: step 15637, loss 0.00188665, acc 1
2016-09-06T12:35:58.187176: step 15638, loss 0.0147383, acc 1
2016-09-06T12:35:59.003904: step 15639, loss 0.00191271, acc 1
2016-09-06T12:35:59.779708: step 15640, loss 0.0104511, acc 1
2016-09-06T12:36:00.609858: step 15641, loss 0.00282642, acc 1
2016-09-06T12:36:01.424716: step 15642, loss 0.0256341, acc 0.98
2016-09-06T12:36:02.218269: step 15643, loss 0.0131108, acc 1
2016-09-06T12:36:03.020720: step 15644, loss 0.00187868, acc 1
2016-09-06T12:36:03.834888: step 15645, loss 0.0101374, acc 1
2016-09-06T12:36:04.609058: step 15646, loss 0.0029648, acc 1
2016-09-06T12:36:05.424603: step 15647, loss 0.00362135, acc 1
2016-09-06T12:36:06.252038: step 15648, loss 0.00174713, acc 1
2016-09-06T12:36:07.035677: step 15649, loss 0.00164451, acc 1
2016-09-06T12:36:07.853837: step 15650, loss 0.0234476, acc 1
2016-09-06T12:36:08.660542: step 15651, loss 0.00311412, acc 1
2016-09-06T12:36:09.450595: step 15652, loss 0.00162279, acc 1
2016-09-06T12:36:10.295934: step 15653, loss 0.00624926, acc 1
2016-09-06T12:36:11.114798: step 15654, loss 0.00417032, acc 1
2016-09-06T12:36:11.903949: step 15655, loss 0.00227501, acc 1
2016-09-06T12:36:12.702883: step 15656, loss 0.00156405, acc 1
2016-09-06T12:36:13.539065: step 15657, loss 0.00746609, acc 1
2016-09-06T12:36:14.357640: step 15658, loss 0.00188803, acc 1
2016-09-06T12:36:15.152048: step 15659, loss 0.0156315, acc 1
2016-09-06T12:36:15.961408: step 15660, loss 0.00327155, acc 1
2016-09-06T12:36:16.759102: step 15661, loss 0.0210375, acc 1
2016-09-06T12:36:17.558675: step 15662, loss 0.0183671, acc 0.98
2016-09-06T12:36:18.419410: step 15663, loss 0.00160961, acc 1
2016-09-06T12:36:19.219808: step 15664, loss 0.0152423, acc 1
2016-09-06T12:36:20.026709: step 15665, loss 0.0207968, acc 0.98
2016-09-06T12:36:20.840437: step 15666, loss 0.00617084, acc 1
2016-09-06T12:36:21.630551: step 15667, loss 0.0161364, acc 0.98
2016-09-06T12:36:22.406620: step 15668, loss 0.00400881, acc 1
2016-09-06T12:36:23.259619: step 15669, loss 0.0187053, acc 1
2016-09-06T12:36:24.081352: step 15670, loss 0.00229654, acc 1
2016-09-06T12:36:24.893033: step 15671, loss 0.0497, acc 0.96
2016-09-06T12:36:25.714416: step 15672, loss 0.0225702, acc 1
2016-09-06T12:36:26.539765: step 15673, loss 0.00164739, acc 1
2016-09-06T12:36:27.394725: step 15674, loss 0.0016009, acc 1
2016-09-06T12:36:28.214464: step 15675, loss 0.00161599, acc 1
2016-09-06T12:36:29.017095: step 15676, loss 0.0143519, acc 1
2016-09-06T12:36:29.821148: step 15677, loss 0.0324139, acc 0.96
2016-09-06T12:36:30.661604: step 15678, loss 0.00219271, acc 1
2016-09-06T12:36:31.505633: step 15679, loss 0.0195392, acc 1
2016-09-06T12:36:32.301442: step 15680, loss 0.0159066, acc 0.98
2016-09-06T12:36:33.093409: step 15681, loss 0.00908823, acc 1
2016-09-06T12:36:33.926575: step 15682, loss 0.00831678, acc 1
2016-09-06T12:36:34.735556: step 15683, loss 0.0244096, acc 0.98
2016-09-06T12:36:35.576608: step 15684, loss 0.0424392, acc 0.98
2016-09-06T12:36:36.404711: step 15685, loss 0.0154904, acc 1
2016-09-06T12:36:37.204633: step 15686, loss 0.00367664, acc 1
2016-09-06T12:36:38.026516: step 15687, loss 0.00323399, acc 1
2016-09-06T12:36:38.837828: step 15688, loss 0.00164097, acc 1
2016-09-06T12:36:39.655188: step 15689, loss 0.0162545, acc 0.98
2016-09-06T12:36:40.484102: step 15690, loss 0.0163135, acc 1
2016-09-06T12:36:41.300904: step 15691, loss 0.0099398, acc 1
2016-09-06T12:36:42.107166: step 15692, loss 0.00164753, acc 1
2016-09-06T12:36:42.935654: step 15693, loss 0.0126044, acc 1
2016-09-06T12:36:43.770167: step 15694, loss 0.00589248, acc 1
2016-09-06T12:36:44.555546: step 15695, loss 0.00181219, acc 1
2016-09-06T12:36:45.338384: step 15696, loss 0.0252908, acc 1
2016-09-06T12:36:46.156993: step 15697, loss 0.00192996, acc 1
2016-09-06T12:36:46.936613: step 15698, loss 0.0295536, acc 0.98
2016-09-06T12:36:47.742915: step 15699, loss 0.00292147, acc 1
2016-09-06T12:36:48.597460: step 15700, loss 0.00193023, acc 1

Evaluation:
2016-09-06T12:36:52.350500: step 15700, loss 2.40247, acc 0.732645

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-15700

2016-09-06T12:36:54.104815: step 15701, loss 0.00840504, acc 1
2016-09-06T12:36:54.935956: step 15702, loss 0.0128071, acc 1
2016-09-06T12:36:55.764336: step 15703, loss 0.00214623, acc 1
2016-09-06T12:36:56.601549: step 15704, loss 0.00237926, acc 1
2016-09-06T12:36:57.416204: step 15705, loss 0.0268478, acc 0.98
2016-09-06T12:36:58.214421: step 15706, loss 0.00194702, acc 1
2016-09-06T12:36:59.021159: step 15707, loss 0.00194114, acc 1
2016-09-06T12:36:59.856712: step 15708, loss 0.0531692, acc 0.96
2016-09-06T12:37:00.682493: step 15709, loss 0.0235458, acc 0.98
2016-09-06T12:37:01.489157: step 15710, loss 0.00777317, acc 1
2016-09-06T12:37:02.326410: step 15711, loss 0.00196706, acc 1
2016-09-06T12:37:03.146963: step 15712, loss 0.0488027, acc 0.96
2016-09-06T12:37:03.943681: step 15713, loss 0.0251739, acc 0.98
2016-09-06T12:37:04.765863: step 15714, loss 0.00238323, acc 1
2016-09-06T12:37:05.570131: step 15715, loss 0.0115961, acc 1
2016-09-06T12:37:06.371102: step 15716, loss 0.00172619, acc 1
2016-09-06T12:37:07.197095: step 15717, loss 0.00180579, acc 1
2016-09-06T12:37:08.029319: step 15718, loss 0.0961619, acc 0.98
2016-09-06T12:37:08.833590: step 15719, loss 0.00185866, acc 1
2016-09-06T12:37:09.639088: step 15720, loss 0.00188732, acc 1
2016-09-06T12:37:10.460776: step 15721, loss 0.0175165, acc 0.98
2016-09-06T12:37:11.249931: step 15722, loss 0.0221191, acc 0.98
2016-09-06T12:37:12.058582: step 15723, loss 0.0191202, acc 1
2016-09-06T12:37:12.908388: step 15724, loss 0.00162942, acc 1
2016-09-06T12:37:13.751514: step 15725, loss 0.00619768, acc 1
2016-09-06T12:37:14.562429: step 15726, loss 0.00613468, acc 1
2016-09-06T12:37:15.383247: step 15727, loss 0.0455268, acc 0.96
2016-09-06T12:37:16.168919: step 15728, loss 0.00146458, acc 1
2016-09-06T12:37:16.978808: step 15729, loss 0.00536082, acc 1
2016-09-06T12:37:17.814937: step 15730, loss 0.0353497, acc 1
2016-09-06T12:37:18.645632: step 15731, loss 0.00145575, acc 1
2016-09-06T12:37:19.442968: step 15732, loss 0.0346458, acc 0.98
2016-09-06T12:37:20.282413: step 15733, loss 0.0185791, acc 0.98
2016-09-06T12:37:21.113059: step 15734, loss 0.00155081, acc 1
2016-09-06T12:37:21.913471: step 15735, loss 0.0015812, acc 1
2016-09-06T12:37:22.726505: step 15736, loss 0.00143544, acc 1
2016-09-06T12:37:23.516415: step 15737, loss 0.0545994, acc 0.98
2016-09-06T12:37:24.332436: step 15738, loss 0.0180115, acc 0.98
2016-09-06T12:37:25.173899: step 15739, loss 0.00800978, acc 1
2016-09-06T12:37:26.009505: step 15740, loss 0.00247708, acc 1
2016-09-06T12:37:26.843488: step 15741, loss 0.0372519, acc 0.98
2016-09-06T12:37:27.702791: step 15742, loss 0.00293415, acc 1
2016-09-06T12:37:28.498360: step 15743, loss 0.00466374, acc 1
2016-09-06T12:37:29.247290: step 15744, loss 0.00245014, acc 1
2016-09-06T12:37:30.083885: step 15745, loss 0.0113123, acc 1
2016-09-06T12:37:30.897067: step 15746, loss 0.0141529, acc 1
2016-09-06T12:37:31.696531: step 15747, loss 0.0150813, acc 1
2016-09-06T12:37:32.518634: step 15748, loss 0.0220534, acc 0.98
2016-09-06T12:37:33.346261: step 15749, loss 0.0310708, acc 0.96
2016-09-06T12:37:34.146403: step 15750, loss 0.00338723, acc 1
2016-09-06T12:37:34.968513: step 15751, loss 0.00148867, acc 1
2016-09-06T12:37:35.788736: step 15752, loss 0.00440053, acc 1
2016-09-06T12:37:36.583270: step 15753, loss 0.0627759, acc 0.96
2016-09-06T12:37:37.373265: step 15754, loss 0.0217371, acc 1
2016-09-06T12:37:38.178450: step 15755, loss 0.00131502, acc 1
2016-09-06T12:37:38.987584: step 15756, loss 0.00180062, acc 1
2016-09-06T12:37:39.815996: step 15757, loss 0.0125275, acc 1
2016-09-06T12:37:40.627012: step 15758, loss 0.00738828, acc 1
2016-09-06T12:37:41.408167: step 15759, loss 0.0328707, acc 1
2016-09-06T12:37:42.210446: step 15760, loss 0.00250421, acc 1
2016-09-06T12:37:43.019583: step 15761, loss 0.00161598, acc 1
2016-09-06T12:37:43.812523: step 15762, loss 0.00159337, acc 1
2016-09-06T12:37:44.621052: step 15763, loss 0.00170262, acc 1
2016-09-06T12:37:45.422477: step 15764, loss 0.00169707, acc 1
2016-09-06T12:37:46.216109: step 15765, loss 0.00196296, acc 1
2016-09-06T12:37:47.038004: step 15766, loss 0.00965365, acc 1
2016-09-06T12:37:47.821067: step 15767, loss 0.00159407, acc 1
2016-09-06T12:37:48.612040: step 15768, loss 0.00309946, acc 1
2016-09-06T12:37:49.453104: step 15769, loss 0.00163655, acc 1
2016-09-06T12:37:50.254609: step 15770, loss 0.00199147, acc 1
2016-09-06T12:37:51.045753: step 15771, loss 0.0563081, acc 0.98
2016-09-06T12:37:51.874757: step 15772, loss 0.00158539, acc 1
2016-09-06T12:37:52.670880: step 15773, loss 0.0241392, acc 0.98
2016-09-06T12:37:53.469365: step 15774, loss 0.0245034, acc 0.98
2016-09-06T12:37:54.283750: step 15775, loss 0.00603045, acc 1
2016-09-06T12:37:55.091354: step 15776, loss 0.0290358, acc 0.98
2016-09-06T12:37:55.894156: step 15777, loss 0.00883308, acc 1
2016-09-06T12:37:56.709763: step 15778, loss 0.00705195, acc 1
2016-09-06T12:37:57.506846: step 15779, loss 0.026199, acc 0.98
2016-09-06T12:37:58.314335: step 15780, loss 0.0114737, acc 1
2016-09-06T12:37:59.157088: step 15781, loss 0.00200035, acc 1
2016-09-06T12:37:59.956051: step 15782, loss 0.0194725, acc 0.98
2016-09-06T12:38:00.771629: step 15783, loss 0.00162798, acc 1
2016-09-06T12:38:01.568566: step 15784, loss 0.00181219, acc 1
2016-09-06T12:38:02.408705: step 15785, loss 0.0190286, acc 0.98
2016-09-06T12:38:03.192658: step 15786, loss 0.0184694, acc 0.98
2016-09-06T12:38:04.000902: step 15787, loss 0.00862541, acc 1
2016-09-06T12:38:04.837528: step 15788, loss 0.0183388, acc 0.98
2016-09-06T12:38:05.610695: step 15789, loss 0.0129332, acc 1
2016-09-06T12:38:06.418692: step 15790, loss 0.00353767, acc 1
2016-09-06T12:38:07.243874: step 15791, loss 0.00479284, acc 1
2016-09-06T12:38:08.054894: step 15792, loss 0.0124473, acc 1
2016-09-06T12:38:08.829814: step 15793, loss 0.0164912, acc 1
2016-09-06T12:38:09.652174: step 15794, loss 0.00148548, acc 1
2016-09-06T12:38:10.448062: step 15795, loss 0.0115067, acc 1
2016-09-06T12:38:11.253706: step 15796, loss 0.014646, acc 1
2016-09-06T12:38:12.093448: step 15797, loss 0.0224167, acc 0.98
2016-09-06T12:38:12.869598: step 15798, loss 0.00152518, acc 1
2016-09-06T12:38:13.665354: step 15799, loss 0.00167818, acc 1
2016-09-06T12:38:14.498797: step 15800, loss 0.00192306, acc 1

Evaluation:
2016-09-06T12:38:18.200336: step 15800, loss 2.75164, acc 0.738274

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-15800

2016-09-06T12:38:20.124349: step 15801, loss 0.014693, acc 1
2016-09-06T12:38:20.981450: step 15802, loss 0.120377, acc 0.96
2016-09-06T12:38:21.798706: step 15803, loss 0.00317523, acc 1
2016-09-06T12:38:22.592382: step 15804, loss 0.00172183, acc 1
2016-09-06T12:38:23.381253: step 15805, loss 0.0169019, acc 1
2016-09-06T12:38:24.170643: step 15806, loss 0.00224678, acc 1
2016-09-06T12:38:24.956780: step 15807, loss 0.00142037, acc 1
2016-09-06T12:38:25.780868: step 15808, loss 0.0186749, acc 0.98
2016-09-06T12:38:26.599189: step 15809, loss 0.162247, acc 0.98
2016-09-06T12:38:27.396315: step 15810, loss 0.0203055, acc 1
2016-09-06T12:38:28.215977: step 15811, loss 0.00673808, acc 1
2016-09-06T12:38:29.005387: step 15812, loss 0.0175343, acc 1
2016-09-06T12:38:29.825687: step 15813, loss 0.0204626, acc 0.98
2016-09-06T12:38:30.646665: step 15814, loss 0.00333409, acc 1
2016-09-06T12:38:31.459042: step 15815, loss 0.00612556, acc 1
2016-09-06T12:38:32.270140: step 15816, loss 0.00497802, acc 1
2016-09-06T12:38:33.095680: step 15817, loss 0.0326703, acc 0.98
2016-09-06T12:38:33.952660: step 15818, loss 0.00684076, acc 1
2016-09-06T12:38:34.771210: step 15819, loss 0.00508946, acc 1
2016-09-06T12:38:35.577830: step 15820, loss 0.00537398, acc 1
2016-09-06T12:38:36.419204: step 15821, loss 0.00239333, acc 1
2016-09-06T12:38:37.252998: step 15822, loss 0.0355628, acc 0.98
2016-09-06T12:38:38.061152: step 15823, loss 0.0189495, acc 0.98
2016-09-06T12:38:38.929415: step 15824, loss 0.0153008, acc 1
2016-09-06T12:38:39.734712: step 15825, loss 0.00206579, acc 1
2016-09-06T12:38:40.546485: step 15826, loss 0.0144002, acc 1
2016-09-06T12:38:41.361863: step 15827, loss 0.0355222, acc 1
2016-09-06T12:38:42.257883: step 15828, loss 0.00205435, acc 1
2016-09-06T12:38:43.060209: step 15829, loss 0.00390878, acc 1
2016-09-06T12:38:43.864177: step 15830, loss 0.0129651, acc 1
2016-09-06T12:38:44.664080: step 15831, loss 0.00211607, acc 1
2016-09-06T12:38:45.462018: step 15832, loss 0.00672467, acc 1
2016-09-06T12:38:46.275751: step 15833, loss 0.00318691, acc 1
2016-09-06T12:38:47.092708: step 15834, loss 0.00219858, acc 1
2016-09-06T12:38:47.931677: step 15835, loss 0.00785598, acc 1
2016-09-06T12:38:48.748641: step 15836, loss 0.00224554, acc 1
2016-09-06T12:38:49.551911: step 15837, loss 0.00269458, acc 1
2016-09-06T12:38:50.337613: step 15838, loss 0.0122365, acc 1
2016-09-06T12:38:51.135149: step 15839, loss 0.00282546, acc 1
2016-09-06T12:38:51.968961: step 15840, loss 0.00519888, acc 1
2016-09-06T12:38:52.744534: step 15841, loss 0.00228273, acc 1
2016-09-06T12:38:53.550589: step 15842, loss 0.0314173, acc 0.98
2016-09-06T12:38:54.353933: step 15843, loss 0.00237109, acc 1
2016-09-06T12:38:55.180976: step 15844, loss 0.00548448, acc 1
2016-09-06T12:38:55.989715: step 15845, loss 0.104962, acc 0.96
2016-09-06T12:38:56.810737: step 15846, loss 0.0127384, acc 1
2016-09-06T12:38:57.590704: step 15847, loss 0.00294091, acc 1
2016-09-06T12:38:58.378831: step 15848, loss 0.00241204, acc 1
2016-09-06T12:38:59.202101: step 15849, loss 0.00724274, acc 1
2016-09-06T12:38:59.996833: step 15850, loss 0.00479453, acc 1
2016-09-06T12:39:00.823115: step 15851, loss 0.022157, acc 0.98
2016-09-06T12:39:01.707964: step 15852, loss 0.00398146, acc 1
2016-09-06T12:39:02.539788: step 15853, loss 0.0201216, acc 1
2016-09-06T12:39:03.395568: step 15854, loss 0.00357181, acc 1
2016-09-06T12:39:04.256900: step 15855, loss 0.0167168, acc 1
2016-09-06T12:39:05.096053: step 15856, loss 0.116523, acc 0.94
2016-09-06T12:39:05.909917: step 15857, loss 0.0167602, acc 1
2016-09-06T12:39:06.723151: step 15858, loss 0.00764194, acc 1
2016-09-06T12:39:07.548752: step 15859, loss 0.0343039, acc 0.98
2016-09-06T12:39:08.360108: step 15860, loss 0.0147652, acc 1
2016-09-06T12:39:09.182474: step 15861, loss 0.00415709, acc 1
2016-09-06T12:39:10.053251: step 15862, loss 0.00327354, acc 1
2016-09-06T12:39:10.879902: step 15863, loss 0.00771946, acc 1
2016-09-06T12:39:11.672592: step 15864, loss 0.0893676, acc 0.98
2016-09-06T12:39:12.568257: step 15865, loss 0.00319787, acc 1
2016-09-06T12:39:13.358579: step 15866, loss 0.0143416, acc 1
2016-09-06T12:39:14.172763: step 15867, loss 0.017378, acc 1
2016-09-06T12:39:14.994117: step 15868, loss 0.0138833, acc 1
2016-09-06T12:39:15.781985: step 15869, loss 0.0267808, acc 0.98
2016-09-06T12:39:16.589267: step 15870, loss 0.00304122, acc 1
2016-09-06T12:39:17.423093: step 15871, loss 0.0169127, acc 1
2016-09-06T12:39:18.215755: step 15872, loss 0.0166484, acc 1
2016-09-06T12:39:19.007741: step 15873, loss 0.00677553, acc 1
2016-09-06T12:39:19.866264: step 15874, loss 0.0299508, acc 0.98
2016-09-06T12:39:20.678603: step 15875, loss 0.00923122, acc 1
2016-09-06T12:39:21.509353: step 15876, loss 0.0066772, acc 1
2016-09-06T12:39:22.337206: step 15877, loss 0.00403062, acc 1
2016-09-06T12:39:23.172748: step 15878, loss 0.00306375, acc 1
2016-09-06T12:39:23.993081: step 15879, loss 0.011446, acc 1
2016-09-06T12:39:24.837051: step 15880, loss 0.00371395, acc 1
2016-09-06T12:39:25.654855: step 15881, loss 0.00317902, acc 1
2016-09-06T12:39:26.452327: step 15882, loss 0.0180105, acc 0.98
2016-09-06T12:39:27.287925: step 15883, loss 0.00286067, acc 1
2016-09-06T12:39:28.098001: step 15884, loss 0.00281319, acc 1
2016-09-06T12:39:28.916280: step 15885, loss 0.00588044, acc 1
2016-09-06T12:39:29.753908: step 15886, loss 0.00312536, acc 1
2016-09-06T12:39:30.559751: step 15887, loss 0.00424529, acc 1
2016-09-06T12:39:31.355163: step 15888, loss 0.016347, acc 1
2016-09-06T12:39:32.202097: step 15889, loss 0.0298903, acc 0.98
2016-09-06T12:39:33.010954: step 15890, loss 0.026009, acc 1
2016-09-06T12:39:33.805236: step 15891, loss 0.00470594, acc 1
2016-09-06T12:39:34.631237: step 15892, loss 0.00618391, acc 1
2016-09-06T12:39:35.438891: step 15893, loss 0.0938456, acc 0.96
2016-09-06T12:39:36.239159: step 15894, loss 0.0174611, acc 0.98
2016-09-06T12:39:37.055061: step 15895, loss 0.00250444, acc 1
2016-09-06T12:39:37.877775: step 15896, loss 0.00318841, acc 1
2016-09-06T12:39:38.663791: step 15897, loss 0.0872466, acc 0.98
2016-09-06T12:39:39.457910: step 15898, loss 0.0443776, acc 0.98
2016-09-06T12:39:40.312753: step 15899, loss 0.0166763, acc 1
2016-09-06T12:39:41.113335: step 15900, loss 0.0312413, acc 0.98

Evaluation:
2016-09-06T12:39:44.835272: step 15900, loss 2.2418, acc 0.738274

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-15900

2016-09-06T12:39:46.751481: step 15901, loss 0.00252596, acc 1
2016-09-06T12:39:47.547037: step 15902, loss 0.0195304, acc 0.98
2016-09-06T12:39:48.376649: step 15903, loss 0.00221099, acc 1
2016-09-06T12:39:49.221414: step 15904, loss 0.0503331, acc 0.98
2016-09-06T12:39:50.048686: step 15905, loss 0.00903513, acc 1
2016-09-06T12:39:50.817717: step 15906, loss 0.0311826, acc 0.98
2016-09-06T12:39:51.650453: step 15907, loss 0.00664985, acc 1
2016-09-06T12:39:52.454297: step 15908, loss 0.00504644, acc 1
2016-09-06T12:39:53.207959: step 15909, loss 0.00204898, acc 1
2016-09-06T12:39:54.004998: step 15910, loss 0.0303588, acc 0.98
2016-09-06T12:39:54.801822: step 15911, loss 0.00220116, acc 1
2016-09-06T12:39:55.601433: step 15912, loss 0.00188113, acc 1
2016-09-06T12:39:56.404091: step 15913, loss 0.0327297, acc 0.98
2016-09-06T12:39:57.247413: step 15914, loss 0.00211827, acc 1
2016-09-06T12:39:58.045324: step 15915, loss 0.00624195, acc 1
2016-09-06T12:39:58.841353: step 15916, loss 0.0119, acc 1
2016-09-06T12:39:59.659610: step 15917, loss 0.00177474, acc 1
2016-09-06T12:40:00.469432: step 15918, loss 0.00737876, acc 1
2016-09-06T12:40:01.273144: step 15919, loss 0.0286753, acc 0.98
2016-09-06T12:40:02.110079: step 15920, loss 0.00689748, acc 1
2016-09-06T12:40:02.886769: step 15921, loss 0.00461662, acc 1
2016-09-06T12:40:03.678467: step 15922, loss 0.0217699, acc 1
2016-09-06T12:40:04.488484: step 15923, loss 0.00555778, acc 1
2016-09-06T12:40:05.302619: step 15924, loss 0.0311077, acc 1
2016-09-06T12:40:06.113678: step 15925, loss 0.0315234, acc 0.98
2016-09-06T12:40:06.924021: step 15926, loss 0.00834555, acc 1
2016-09-06T12:40:07.729503: step 15927, loss 0.00938806, acc 1
2016-09-06T12:40:08.537436: step 15928, loss 0.00264575, acc 1
2016-09-06T12:40:09.350058: step 15929, loss 0.0180025, acc 1
2016-09-06T12:40:10.154233: step 15930, loss 0.0157342, acc 1
2016-09-06T12:40:10.954290: step 15931, loss 0.0508036, acc 0.96
2016-09-06T12:40:11.780145: step 15932, loss 0.00194464, acc 1
2016-09-06T12:40:12.562569: step 15933, loss 0.0299338, acc 0.98
2016-09-06T12:40:13.374785: step 15934, loss 0.00295123, acc 1
2016-09-06T12:40:14.209724: step 15935, loss 0.0261359, acc 0.98
2016-09-06T12:40:14.922549: step 15936, loss 0.00192763, acc 1
2016-09-06T12:40:15.742244: step 15937, loss 0.00916788, acc 1
2016-09-06T12:40:16.597134: step 15938, loss 0.0110987, acc 1
2016-09-06T12:40:17.389765: step 15939, loss 0.00189953, acc 1
2016-09-06T12:40:18.220729: step 15940, loss 0.0293551, acc 1
2016-09-06T12:40:19.029920: step 15941, loss 0.0125004, acc 1
2016-09-06T12:40:19.808921: step 15942, loss 0.0291086, acc 1
2016-09-06T12:40:20.620219: step 15943, loss 0.00302865, acc 1
2016-09-06T12:40:21.468410: step 15944, loss 0.0351178, acc 0.98
2016-09-06T12:40:22.259982: step 15945, loss 0.0612288, acc 0.96
2016-09-06T12:40:23.042489: step 15946, loss 0.00272007, acc 1
2016-09-06T12:40:23.861771: step 15947, loss 0.00190528, acc 1
2016-09-06T12:40:24.629722: step 15948, loss 0.00258735, acc 1
2016-09-06T12:40:25.434655: step 15949, loss 0.0236373, acc 1
2016-09-06T12:40:26.255531: step 15950, loss 0.0337272, acc 0.96
2016-09-06T12:40:27.049947: step 15951, loss 0.00299573, acc 1
2016-09-06T12:40:27.854803: step 15952, loss 0.00497229, acc 1
2016-09-06T12:40:28.684062: step 15953, loss 0.0778088, acc 0.96
2016-09-06T12:40:29.497372: step 15954, loss 0.00188344, acc 1
2016-09-06T12:40:30.309875: step 15955, loss 0.00514915, acc 1
2016-09-06T12:40:31.139143: step 15956, loss 0.00297376, acc 1
2016-09-06T12:40:31.934862: step 15957, loss 0.0024274, acc 1
2016-09-06T12:40:32.729446: step 15958, loss 0.00260931, acc 1
2016-09-06T12:40:33.566285: step 15959, loss 0.00280619, acc 1
2016-09-06T12:40:34.348450: step 15960, loss 0.012996, acc 1
2016-09-06T12:40:35.152621: step 15961, loss 0.0148216, acc 1
2016-09-06T12:40:35.973154: step 15962, loss 0.00616534, acc 1
2016-09-06T12:40:36.795990: step 15963, loss 0.0341491, acc 0.98
2016-09-06T12:40:37.602649: step 15964, loss 0.00241753, acc 1
2016-09-06T12:40:38.430589: step 15965, loss 0.00295504, acc 1
2016-09-06T12:40:39.245805: step 15966, loss 0.00682113, acc 1
2016-09-06T12:40:40.077538: step 15967, loss 0.00182599, acc 1
2016-09-06T12:40:40.912922: step 15968, loss 0.0248464, acc 1
2016-09-06T12:40:41.750330: step 15969, loss 0.00315842, acc 1
2016-09-06T12:40:42.563662: step 15970, loss 0.00203128, acc 1
2016-09-06T12:40:43.410661: step 15971, loss 0.0202625, acc 0.98
2016-09-06T12:40:44.227890: step 15972, loss 0.0239752, acc 0.98
2016-09-06T12:40:45.011309: step 15973, loss 0.00856751, acc 1
2016-09-06T12:40:45.830066: step 15974, loss 0.0176943, acc 0.98
2016-09-06T12:40:46.625414: step 15975, loss 0.00287463, acc 1
2016-09-06T12:40:47.442049: step 15976, loss 0.0144425, acc 1
2016-09-06T12:40:48.256956: step 15977, loss 0.00185195, acc 1
2016-09-06T12:40:49.089265: step 15978, loss 0.0224198, acc 0.98
2016-09-06T12:40:49.898812: step 15979, loss 0.00590569, acc 1
2016-09-06T12:40:50.721157: step 15980, loss 0.00695182, acc 1
2016-09-06T12:40:51.516220: step 15981, loss 0.00179222, acc 1
2016-09-06T12:40:52.307792: step 15982, loss 0.0157626, acc 0.98
2016-09-06T12:40:53.135039: step 15983, loss 0.00288287, acc 1
2016-09-06T12:40:53.977428: step 15984, loss 0.0123587, acc 1
2016-09-06T12:40:54.828307: step 15985, loss 0.0129304, acc 1
2016-09-06T12:40:55.624946: step 15986, loss 0.00183088, acc 1
2016-09-06T12:40:56.441374: step 15987, loss 0.0263205, acc 1
2016-09-06T12:40:57.225671: step 15988, loss 0.0166665, acc 0.98
2016-09-06T12:40:58.022340: step 15989, loss 0.020861, acc 0.98
2016-09-06T12:40:58.860731: step 15990, loss 0.00384808, acc 1
2016-09-06T12:40:59.653446: step 15991, loss 0.0131816, acc 1
2016-09-06T12:41:00.485225: step 15992, loss 0.00328825, acc 1
2016-09-06T12:41:01.345097: step 15993, loss 0.0020459, acc 1
2016-09-06T12:41:02.134305: step 15994, loss 0.0300473, acc 1
2016-09-06T12:41:02.924731: step 15995, loss 0.024331, acc 0.98
2016-09-06T12:41:03.727120: step 15996, loss 0.00183189, acc 1
2016-09-06T12:41:04.510226: step 15997, loss 0.0237712, acc 1
2016-09-06T12:41:05.323172: step 15998, loss 0.0140016, acc 1
2016-09-06T12:41:06.149387: step 15999, loss 0.00983719, acc 1
2016-09-06T12:41:06.930330: step 16000, loss 0.0264181, acc 0.98

Evaluation:
2016-09-06T12:41:10.686378: step 16000, loss 2.75394, acc 0.738274

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-16000

2016-09-06T12:41:12.555118: step 16001, loss 0.00199629, acc 1
2016-09-06T12:41:13.365460: step 16002, loss 0.0107725, acc 1
2016-09-06T12:41:14.199762: step 16003, loss 0.00193879, acc 1
2016-09-06T12:41:15.063707: step 16004, loss 0.0319186, acc 0.98
2016-09-06T12:41:15.905823: step 16005, loss 0.0302262, acc 0.98
2016-09-06T12:41:16.695628: step 16006, loss 0.0188715, acc 0.98
2016-09-06T12:41:17.482612: step 16007, loss 0.00191456, acc 1
2016-09-06T12:41:18.304876: step 16008, loss 0.00217386, acc 1
2016-09-06T12:41:19.084443: step 16009, loss 0.0315119, acc 0.98
2016-09-06T12:41:19.900367: step 16010, loss 0.00231628, acc 1
2016-09-06T12:41:20.753102: step 16011, loss 0.0182692, acc 0.98
2016-09-06T12:41:21.550117: step 16012, loss 0.0160131, acc 0.98
2016-09-06T12:41:22.343141: step 16013, loss 0.0131779, acc 1
2016-09-06T12:41:23.171009: step 16014, loss 0.0134614, acc 1
2016-09-06T12:41:23.953754: step 16015, loss 0.00724758, acc 1
2016-09-06T12:41:24.742645: step 16016, loss 0.00185482, acc 1
2016-09-06T12:41:25.563843: step 16017, loss 0.00196832, acc 1
2016-09-06T12:41:26.366648: step 16018, loss 0.011475, acc 1
2016-09-06T12:41:27.183362: step 16019, loss 0.00639877, acc 1
2016-09-06T12:41:28.024682: step 16020, loss 0.00182953, acc 1
2016-09-06T12:41:28.795053: step 16021, loss 0.00702933, acc 1
2016-09-06T12:41:29.605451: step 16022, loss 0.0211215, acc 0.98
2016-09-06T12:41:30.413290: step 16023, loss 0.0285307, acc 0.98
2016-09-06T12:41:31.209579: step 16024, loss 0.00262431, acc 1
2016-09-06T12:41:32.094457: step 16025, loss 0.0202487, acc 0.98
2016-09-06T12:41:32.933547: step 16026, loss 0.00224026, acc 1
2016-09-06T12:41:33.742002: step 16027, loss 0.0180295, acc 1
2016-09-06T12:41:34.573474: step 16028, loss 0.0017909, acc 1
2016-09-06T12:41:35.432635: step 16029, loss 0.00643276, acc 1
2016-09-06T12:41:36.247951: step 16030, loss 0.00436425, acc 1
2016-09-06T12:41:37.059540: step 16031, loss 0.0222958, acc 0.98
2016-09-06T12:41:37.900927: step 16032, loss 0.00169788, acc 1
2016-09-06T12:41:38.720075: step 16033, loss 0.00168999, acc 1
2016-09-06T12:41:39.514105: step 16034, loss 0.00219595, acc 1
2016-09-06T12:41:40.357381: step 16035, loss 0.0234589, acc 0.98
2016-09-06T12:41:41.165464: step 16036, loss 0.0218559, acc 0.98
2016-09-06T12:41:41.973548: step 16037, loss 0.0206429, acc 1
2016-09-06T12:41:42.797153: step 16038, loss 0.0205063, acc 0.98
2016-09-06T12:41:43.603335: step 16039, loss 0.0161444, acc 1
2016-09-06T12:41:44.411094: step 16040, loss 0.00172946, acc 1
2016-09-06T12:41:45.237900: step 16041, loss 0.00273362, acc 1
2016-09-06T12:41:46.042699: step 16042, loss 0.00168396, acc 1
2016-09-06T12:41:46.841479: step 16043, loss 0.0207946, acc 1
2016-09-06T12:41:47.645046: step 16044, loss 0.0278717, acc 0.98
2016-09-06T12:41:48.450849: step 16045, loss 0.0154841, acc 1
2016-09-06T12:41:49.249469: step 16046, loss 0.01522, acc 1
2016-09-06T12:41:50.084941: step 16047, loss 0.00197558, acc 1
2016-09-06T12:41:50.927262: step 16048, loss 0.00194405, acc 1
2016-09-06T12:41:51.738582: step 16049, loss 0.0116676, acc 1
2016-09-06T12:41:52.549908: step 16050, loss 0.00156822, acc 1
2016-09-06T12:41:53.337578: step 16051, loss 0.0474321, acc 0.98
2016-09-06T12:41:54.135379: step 16052, loss 0.00159059, acc 1
2016-09-06T12:41:54.960991: step 16053, loss 0.00831968, acc 1
2016-09-06T12:41:55.803817: step 16054, loss 0.00154056, acc 1
2016-09-06T12:41:56.584989: step 16055, loss 0.00176468, acc 1
2016-09-06T12:41:57.398606: step 16056, loss 0.0334765, acc 0.98
2016-09-06T12:41:58.190522: step 16057, loss 0.0166734, acc 0.98
2016-09-06T12:41:58.980267: step 16058, loss 0.00919992, acc 1
2016-09-06T12:41:59.819392: step 16059, loss 0.00397699, acc 1
2016-09-06T12:42:00.649536: step 16060, loss 0.0131755, acc 1
2016-09-06T12:42:01.458123: step 16061, loss 0.0382995, acc 0.96
2016-09-06T12:42:02.256747: step 16062, loss 0.0199428, acc 0.98
2016-09-06T12:42:03.075450: step 16063, loss 0.0155554, acc 0.98
2016-09-06T12:42:03.839672: step 16064, loss 0.0555687, acc 0.96
2016-09-06T12:42:04.659043: step 16065, loss 0.0195066, acc 1
2016-09-06T12:42:05.455638: step 16066, loss 0.0206906, acc 0.98
2016-09-06T12:42:06.270559: step 16067, loss 0.0230617, acc 0.98
2016-09-06T12:42:07.068420: step 16068, loss 0.0554763, acc 0.98
2016-09-06T12:42:07.908145: step 16069, loss 0.0388457, acc 0.98
2016-09-06T12:42:08.723944: step 16070, loss 0.0180261, acc 0.98
2016-09-06T12:42:09.566938: step 16071, loss 0.00153057, acc 1
2016-09-06T12:42:10.384275: step 16072, loss 0.0248445, acc 1
2016-09-06T12:42:11.171379: step 16073, loss 0.00427703, acc 1
2016-09-06T12:42:11.970263: step 16074, loss 0.024344, acc 0.98
2016-09-06T12:42:12.794261: step 16075, loss 0.0211953, acc 0.98
2016-09-06T12:42:13.591337: step 16076, loss 0.00379094, acc 1
2016-09-06T12:42:14.372630: step 16077, loss 0.046721, acc 0.96
2016-09-06T12:42:15.213786: step 16078, loss 0.00143908, acc 1
2016-09-06T12:42:16.005949: step 16079, loss 0.00169709, acc 1
2016-09-06T12:42:16.825618: step 16080, loss 0.00194357, acc 1
2016-09-06T12:42:17.664563: step 16081, loss 0.00136419, acc 1
2016-09-06T12:42:18.481926: step 16082, loss 0.0297778, acc 0.98
2016-09-06T12:42:19.282971: step 16083, loss 0.0380772, acc 0.98
2016-09-06T12:42:20.100087: step 16084, loss 0.00224063, acc 1
2016-09-06T12:42:20.911572: step 16085, loss 0.0139646, acc 1
2016-09-06T12:42:21.720894: step 16086, loss 0.0126917, acc 1
2016-09-06T12:42:22.559354: step 16087, loss 0.0037997, acc 1
2016-09-06T12:42:23.365029: step 16088, loss 0.00625905, acc 1
2016-09-06T12:42:24.173376: step 16089, loss 0.00127575, acc 1
2016-09-06T12:42:25.002077: step 16090, loss 0.00213673, acc 1
2016-09-06T12:42:25.816293: step 16091, loss 0.00495938, acc 1
2016-09-06T12:42:26.637107: step 16092, loss 0.0203452, acc 1
2016-09-06T12:42:27.466172: step 16093, loss 0.0263937, acc 0.98
2016-09-06T12:42:28.285599: step 16094, loss 0.0020596, acc 1
2016-09-06T12:42:29.079274: step 16095, loss 0.00535109, acc 1
2016-09-06T12:42:29.919789: step 16096, loss 0.0104541, acc 1
2016-09-06T12:42:30.713559: step 16097, loss 0.0419785, acc 0.98
2016-09-06T12:42:31.527720: step 16098, loss 0.0119417, acc 1
2016-09-06T12:42:32.340459: step 16099, loss 0.00891714, acc 1
2016-09-06T12:42:33.144489: step 16100, loss 0.0017055, acc 1

Evaluation:
2016-09-06T12:42:36.877332: step 16100, loss 2.44211, acc 0.737336

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-16100

2016-09-06T12:42:38.811359: step 16101, loss 0.00141276, acc 1
2016-09-06T12:42:39.656272: step 16102, loss 0.0292925, acc 0.98
2016-09-06T12:42:40.478615: step 16103, loss 0.0103368, acc 1
2016-09-06T12:42:41.283395: step 16104, loss 0.00158719, acc 1
2016-09-06T12:42:42.138128: step 16105, loss 0.0202248, acc 1
2016-09-06T12:42:42.950924: step 16106, loss 0.00273367, acc 1
2016-09-06T12:42:43.749752: step 16107, loss 0.0343009, acc 0.98
2016-09-06T12:42:44.572474: step 16108, loss 0.00157283, acc 1
2016-09-06T12:42:45.391584: step 16109, loss 0.00167984, acc 1
2016-09-06T12:42:46.208484: step 16110, loss 0.00927909, acc 1
2016-09-06T12:42:47.040362: step 16111, loss 0.0218909, acc 1
2016-09-06T12:42:47.862852: step 16112, loss 0.00297645, acc 1
2016-09-06T12:42:48.684097: step 16113, loss 0.00434148, acc 1
2016-09-06T12:42:49.498282: step 16114, loss 0.0277904, acc 0.98
2016-09-06T12:42:50.326674: step 16115, loss 0.00167732, acc 1
2016-09-06T12:42:51.106722: step 16116, loss 0.031478, acc 0.98
2016-09-06T12:42:51.920503: step 16117, loss 0.00163928, acc 1
2016-09-06T12:42:52.746050: step 16118, loss 0.0947436, acc 0.98
2016-09-06T12:42:53.538374: step 16119, loss 0.00164861, acc 1
2016-09-06T12:42:54.325816: step 16120, loss 0.0125316, acc 1
2016-09-06T12:42:55.137625: step 16121, loss 0.00554219, acc 1
2016-09-06T12:42:55.959159: step 16122, loss 0.00891481, acc 1
2016-09-06T12:42:56.772042: step 16123, loss 0.0206591, acc 0.98
2016-09-06T12:42:57.598316: step 16124, loss 0.00152837, acc 1
2016-09-06T12:42:58.394485: step 16125, loss 0.0175106, acc 1
2016-09-06T12:42:59.199507: step 16126, loss 0.00763377, acc 1
2016-09-06T12:43:00.008909: step 16127, loss 0.0947018, acc 0.98
2016-09-06T12:43:00.775823: step 16128, loss 0.00175696, acc 1
2016-09-06T12:43:01.604746: step 16129, loss 0.00307542, acc 1
2016-09-06T12:43:02.430739: step 16130, loss 0.00193617, acc 1
2016-09-06T12:43:03.216583: step 16131, loss 0.00253735, acc 1
2016-09-06T12:43:04.066596: step 16132, loss 0.00352632, acc 1
2016-09-06T12:43:04.913823: step 16133, loss 0.0248479, acc 0.98
2016-09-06T12:43:05.734204: step 16134, loss 0.0176396, acc 1
2016-09-06T12:43:06.593553: step 16135, loss 0.0148359, acc 1
2016-09-06T12:43:07.488538: step 16136, loss 0.0168206, acc 1
2016-09-06T12:43:08.342368: step 16137, loss 0.0262997, acc 0.98
2016-09-06T12:43:09.287235: step 16138, loss 0.141989, acc 0.96
2016-09-06T12:43:10.255838: step 16139, loss 0.00460993, acc 1
2016-09-06T12:43:11.330200: step 16140, loss 0.00466097, acc 1
2016-09-06T12:43:12.264764: step 16141, loss 0.0479284, acc 0.96
2016-09-06T12:43:13.181786: step 16142, loss 0.0474266, acc 0.98
2016-09-06T12:43:14.113088: step 16143, loss 0.0134419, acc 1
2016-09-06T12:43:15.063402: step 16144, loss 0.00972495, acc 1
2016-09-06T12:43:16.083163: step 16145, loss 0.00533191, acc 1
2016-09-06T12:43:17.104305: step 16146, loss 0.00528513, acc 1
2016-09-06T12:43:18.021790: step 16147, loss 0.00536575, acc 1
2016-09-06T12:43:19.055997: step 16148, loss 0.00543992, acc 1
2016-09-06T12:43:20.014207: step 16149, loss 0.00681688, acc 1
2016-09-06T12:43:20.978615: step 16150, loss 0.0196117, acc 0.98
2016-09-06T12:43:21.818018: step 16151, loss 0.00551957, acc 1
2016-09-06T12:43:22.814507: step 16152, loss 0.0481826, acc 0.98
2016-09-06T12:43:23.761387: step 16153, loss 0.023336, acc 0.98
2016-09-06T12:43:24.784577: step 16154, loss 0.00567526, acc 1
2016-09-06T12:43:25.655948: step 16155, loss 0.0136751, acc 1
2016-09-06T12:43:26.519023: step 16156, loss 0.0113079, acc 1
2016-09-06T12:43:27.496021: step 16157, loss 0.00547703, acc 1
2016-09-06T12:43:28.374836: step 16158, loss 0.0234729, acc 0.98
2016-09-06T12:43:29.205943: step 16159, loss 0.00869333, acc 1
2016-09-06T12:43:30.136320: step 16160, loss 0.115469, acc 0.98
2016-09-06T12:43:30.985857: step 16161, loss 0.0335157, acc 0.98
2016-09-06T12:43:31.869093: step 16162, loss 0.0206379, acc 0.98
2016-09-06T12:43:32.760368: step 16163, loss 0.00507802, acc 1
2016-09-06T12:43:33.568479: step 16164, loss 0.00582155, acc 1
2016-09-06T12:43:34.405007: step 16165, loss 0.0361, acc 0.98
2016-09-06T12:43:35.288580: step 16166, loss 0.0187263, acc 1
2016-09-06T12:43:36.110583: step 16167, loss 0.0179906, acc 1
2016-09-06T12:43:36.954770: step 16168, loss 0.00704361, acc 1
2016-09-06T12:43:37.765490: step 16169, loss 0.00488224, acc 1
2016-09-06T12:43:38.565156: step 16170, loss 0.00737364, acc 1
2016-09-06T12:43:39.357845: step 16171, loss 0.0208625, acc 1
2016-09-06T12:43:40.183396: step 16172, loss 0.00493043, acc 1
2016-09-06T12:43:41.090143: step 16173, loss 0.00451618, acc 1
2016-09-06T12:43:41.957283: step 16174, loss 0.0294543, acc 0.98
2016-09-06T12:43:42.788150: step 16175, loss 0.00500898, acc 1
2016-09-06T12:43:43.626737: step 16176, loss 0.0327823, acc 0.98
2016-09-06T12:43:44.439959: step 16177, loss 0.0378603, acc 0.98
2016-09-06T12:43:45.265664: step 16178, loss 0.00435145, acc 1
2016-09-06T12:43:46.107797: step 16179, loss 0.00431974, acc 1
2016-09-06T12:43:46.878045: step 16180, loss 0.0103486, acc 1
2016-09-06T12:43:47.702823: step 16181, loss 0.015952, acc 1
2016-09-06T12:43:48.534293: step 16182, loss 0.0233754, acc 0.98
2016-09-06T12:43:49.350156: step 16183, loss 0.0331188, acc 0.98
2016-09-06T12:43:50.193635: step 16184, loss 0.00407323, acc 1
2016-09-06T12:43:51.040599: step 16185, loss 0.00465729, acc 1
2016-09-06T12:43:51.859329: step 16186, loss 0.0273595, acc 1
2016-09-06T12:43:52.669365: step 16187, loss 0.00582468, acc 1
2016-09-06T12:43:53.492494: step 16188, loss 0.0039579, acc 1
2016-09-06T12:43:54.313521: step 16189, loss 0.0171271, acc 1
2016-09-06T12:43:55.103304: step 16190, loss 0.00784981, acc 1
2016-09-06T12:43:55.946339: step 16191, loss 0.00444346, acc 1
2016-09-06T12:43:56.769733: step 16192, loss 0.0121583, acc 1
2016-09-06T12:43:57.597413: step 16193, loss 0.00847001, acc 1
2016-09-06T12:43:58.425828: step 16194, loss 0.0144255, acc 1
2016-09-06T12:43:59.245686: step 16195, loss 0.0163461, acc 1
2016-09-06T12:44:00.043904: step 16196, loss 0.0157888, acc 1
2016-09-06T12:44:00.882099: step 16197, loss 0.0195049, acc 1
2016-09-06T12:44:01.707752: step 16198, loss 0.003806, acc 1
2016-09-06T12:44:02.492826: step 16199, loss 0.0131585, acc 1
2016-09-06T12:44:03.328255: step 16200, loss 0.00883067, acc 1

Evaluation:
2016-09-06T12:44:07.057381: step 16200, loss 2.89944, acc 0.73546

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-16200

2016-09-06T12:44:08.971288: step 16201, loss 0.00363452, acc 1
2016-09-06T12:44:09.765377: step 16202, loss 0.00764831, acc 1
2016-09-06T12:44:10.602319: step 16203, loss 0.04679, acc 0.98
2016-09-06T12:44:11.433468: step 16204, loss 0.0173399, acc 1
2016-09-06T12:44:12.250297: step 16205, loss 0.0741414, acc 0.96
2016-09-06T12:44:13.069048: step 16206, loss 0.051089, acc 0.98
2016-09-06T12:44:13.865153: step 16207, loss 0.00804594, acc 1
2016-09-06T12:44:14.668507: step 16208, loss 0.00330191, acc 1
2016-09-06T12:44:15.448764: step 16209, loss 0.00381459, acc 1
2016-09-06T12:44:16.267422: step 16210, loss 0.00379358, acc 1
2016-09-06T12:44:17.064650: step 16211, loss 0.0083922, acc 1
2016-09-06T12:44:17.861711: step 16212, loss 0.00328948, acc 1
2016-09-06T12:44:18.681462: step 16213, loss 0.00318196, acc 1
2016-09-06T12:44:19.444298: step 16214, loss 0.0176635, acc 0.98
2016-09-06T12:44:20.271068: step 16215, loss 0.0030625, acc 1
2016-09-06T12:44:21.065715: step 16216, loss 0.0031409, acc 1
2016-09-06T12:44:21.813588: step 16217, loss 0.0112898, acc 1
2016-09-06T12:44:22.681531: step 16218, loss 0.00296728, acc 1
2016-09-06T12:44:23.513836: step 16219, loss 0.0191928, acc 1
2016-09-06T12:44:24.293422: step 16220, loss 0.00364853, acc 1
2016-09-06T12:44:25.076674: step 16221, loss 0.00369764, acc 1
2016-09-06T12:44:25.920689: step 16222, loss 0.00317009, acc 1
2016-09-06T12:44:26.696860: step 16223, loss 0.00440122, acc 1
2016-09-06T12:44:27.476465: step 16224, loss 0.0033369, acc 1
2016-09-06T12:44:28.294575: step 16225, loss 0.00285241, acc 1
2016-09-06T12:44:29.083755: step 16226, loss 0.0028255, acc 1
2016-09-06T12:44:29.919508: step 16227, loss 0.00743589, acc 1
2016-09-06T12:44:30.744029: step 16228, loss 0.0321839, acc 0.98
2016-09-06T12:44:31.529997: step 16229, loss 0.0142083, acc 1
2016-09-06T12:44:32.327541: step 16230, loss 0.00486211, acc 1
2016-09-06T12:44:33.142017: step 16231, loss 0.12545, acc 0.98
2016-09-06T12:44:33.926936: step 16232, loss 0.00343692, acc 1
2016-09-06T12:44:34.724533: step 16233, loss 0.0216851, acc 1
2016-09-06T12:44:35.553453: step 16234, loss 0.0037248, acc 1
2016-09-06T12:44:36.353934: step 16235, loss 0.0308843, acc 0.98
2016-09-06T12:44:37.155986: step 16236, loss 0.015355, acc 1
2016-09-06T12:44:37.986059: step 16237, loss 0.0294684, acc 0.98
2016-09-06T12:44:38.763853: step 16238, loss 0.0184464, acc 1
2016-09-06T12:44:39.575549: step 16239, loss 0.010266, acc 1
2016-09-06T12:44:40.395514: step 16240, loss 0.0227568, acc 1
2016-09-06T12:44:41.208306: step 16241, loss 0.01293, acc 1
2016-09-06T12:44:41.998835: step 16242, loss 0.00259903, acc 1
2016-09-06T12:44:42.797365: step 16243, loss 0.0223236, acc 1
2016-09-06T12:44:43.590640: step 16244, loss 0.071946, acc 0.98
2016-09-06T12:44:44.402798: step 16245, loss 0.0027659, acc 1
2016-09-06T12:44:45.224519: step 16246, loss 0.00261707, acc 1
2016-09-06T12:44:46.022786: step 16247, loss 0.0622035, acc 0.98
2016-09-06T12:44:46.838254: step 16248, loss 0.0439648, acc 0.96
2016-09-06T12:44:47.669386: step 16249, loss 0.00278838, acc 1
2016-09-06T12:44:48.440849: step 16250, loss 0.00277455, acc 1
2016-09-06T12:44:49.240363: step 16251, loss 0.0337402, acc 0.98
2016-09-06T12:44:50.063687: step 16252, loss 0.0218848, acc 0.98
2016-09-06T12:44:50.848842: step 16253, loss 0.0330858, acc 0.98
2016-09-06T12:44:51.631194: step 16254, loss 0.00401269, acc 1
2016-09-06T12:44:52.478578: step 16255, loss 0.0212455, acc 0.98
2016-09-06T12:44:53.258760: step 16256, loss 0.00807815, acc 1
2016-09-06T12:44:54.055570: step 16257, loss 0.0143437, acc 1
2016-09-06T12:44:54.857053: step 16258, loss 0.00439071, acc 1
2016-09-06T12:44:55.680316: step 16259, loss 0.0031761, acc 1
2016-09-06T12:44:56.466447: step 16260, loss 0.0113564, acc 1
2016-09-06T12:44:57.292381: step 16261, loss 0.00943444, acc 1
2016-09-06T12:44:58.081592: step 16262, loss 0.0123114, acc 1
2016-09-06T12:44:58.904009: step 16263, loss 0.00662833, acc 1
2016-09-06T12:44:59.777164: step 16264, loss 0.00334785, acc 1
2016-09-06T12:45:00.598955: step 16265, loss 0.0188458, acc 0.98
2016-09-06T12:45:01.398416: step 16266, loss 0.0170329, acc 1
2016-09-06T12:45:02.222763: step 16267, loss 0.00329212, acc 1
2016-09-06T12:45:03.019889: step 16268, loss 0.00341921, acc 1
2016-09-06T12:45:03.833135: step 16269, loss 0.0857014, acc 0.96
2016-09-06T12:45:04.648843: step 16270, loss 0.00325551, acc 1
2016-09-06T12:45:05.420772: step 16271, loss 0.00471898, acc 1
2016-09-06T12:45:06.240714: step 16272, loss 0.0166912, acc 1
2016-09-06T12:45:07.046694: step 16273, loss 0.0181452, acc 0.98
2016-09-06T12:45:07.831632: step 16274, loss 0.00422208, acc 1
2016-09-06T12:45:08.657395: step 16275, loss 0.0620251, acc 0.96
2016-09-06T12:45:09.471446: step 16276, loss 0.00954859, acc 1
2016-09-06T12:45:10.270362: step 16277, loss 0.0183353, acc 1
2016-09-06T12:45:11.071469: step 16278, loss 0.0139899, acc 1
2016-09-06T12:45:11.903292: step 16279, loss 0.00384851, acc 1
2016-09-06T12:45:12.754517: step 16280, loss 0.0030132, acc 1
2016-09-06T12:45:13.570794: step 16281, loss 0.00571907, acc 1
2016-09-06T12:45:14.366704: step 16282, loss 0.00297706, acc 1
2016-09-06T12:45:15.151900: step 16283, loss 0.00312197, acc 1
2016-09-06T12:45:15.936607: step 16284, loss 0.0155133, acc 1
2016-09-06T12:45:16.779110: step 16285, loss 0.00619887, acc 1
2016-09-06T12:45:17.580483: step 16286, loss 0.0194919, acc 0.98
2016-09-06T12:45:18.380048: step 16287, loss 0.00326189, acc 1
2016-09-06T12:45:19.197001: step 16288, loss 0.023245, acc 1
2016-09-06T12:45:19.967358: step 16289, loss 0.0281041, acc 1
2016-09-06T12:45:20.794967: step 16290, loss 0.028769, acc 0.98
2016-09-06T12:45:21.614167: step 16291, loss 0.00557266, acc 1
2016-09-06T12:45:22.413583: step 16292, loss 0.00288699, acc 1
2016-09-06T12:45:23.198799: step 16293, loss 0.00290707, acc 1
2016-09-06T12:45:24.057605: step 16294, loss 0.0168401, acc 1
2016-09-06T12:45:24.881959: step 16295, loss 0.00294854, acc 1
2016-09-06T12:45:25.688301: step 16296, loss 0.008917, acc 1
2016-09-06T12:45:26.503992: step 16297, loss 0.0314749, acc 1
2016-09-06T12:45:27.337665: step 16298, loss 0.0155453, acc 1
2016-09-06T12:45:28.171600: step 16299, loss 0.024029, acc 0.98
2016-09-06T12:45:29.006015: step 16300, loss 0.00283212, acc 1

Evaluation:
2016-09-06T12:45:32.744844: step 16300, loss 2.61508, acc 0.716698

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-16300

2016-09-06T12:45:34.525554: step 16301, loss 0.00422997, acc 1
2016-09-06T12:45:35.318060: step 16302, loss 0.0124391, acc 1
2016-09-06T12:45:36.141002: step 16303, loss 0.00549427, acc 1
2016-09-06T12:45:36.932768: step 16304, loss 0.0283839, acc 0.98
2016-09-06T12:45:37.774377: step 16305, loss 0.00452942, acc 1
2016-09-06T12:45:38.592667: step 16306, loss 0.055705, acc 0.98
2016-09-06T12:45:39.356331: step 16307, loss 0.00290639, acc 1
2016-09-06T12:45:40.153939: step 16308, loss 0.0117241, acc 1
2016-09-06T12:45:40.996205: step 16309, loss 0.0437912, acc 0.98
2016-09-06T12:45:41.777214: step 16310, loss 0.0033344, acc 1
2016-09-06T12:45:42.577821: step 16311, loss 0.00259923, acc 1
2016-09-06T12:45:43.391388: step 16312, loss 0.0111487, acc 1
2016-09-06T12:45:44.174785: step 16313, loss 0.018527, acc 1
2016-09-06T12:45:45.004531: step 16314, loss 0.0350452, acc 0.96
2016-09-06T12:45:45.886842: step 16315, loss 0.0321814, acc 0.98
2016-09-06T12:45:46.707380: step 16316, loss 0.00254261, acc 1
2016-09-06T12:45:47.536089: step 16317, loss 0.0045282, acc 1
2016-09-06T12:45:48.361539: step 16318, loss 0.00423594, acc 1
2016-09-06T12:45:49.145773: step 16319, loss 0.00265302, acc 1
2016-09-06T12:45:49.892130: step 16320, loss 0.0323077, acc 0.977273
2016-09-06T12:45:50.713505: step 16321, loss 0.00266935, acc 1
2016-09-06T12:45:51.517236: step 16322, loss 0.00999459, acc 1
2016-09-06T12:45:52.324212: step 16323, loss 0.0272812, acc 0.98
2016-09-06T12:45:53.133143: step 16324, loss 0.0153866, acc 1
2016-09-06T12:45:53.931049: step 16325, loss 0.0122168, acc 1
2016-09-06T12:45:54.741714: step 16326, loss 0.00393952, acc 1
2016-09-06T12:45:55.578767: step 16327, loss 0.00619067, acc 1
2016-09-06T12:45:56.406921: step 16328, loss 0.00256205, acc 1
2016-09-06T12:45:57.235358: step 16329, loss 0.0162716, acc 1
2016-09-06T12:45:58.061848: step 16330, loss 0.0191937, acc 0.98
2016-09-06T12:45:58.859542: step 16331, loss 0.00264006, acc 1
2016-09-06T12:45:59.686619: step 16332, loss 0.005786, acc 1
2016-09-06T12:46:00.549467: step 16333, loss 0.0100134, acc 1
2016-09-06T12:46:01.362006: step 16334, loss 0.0125981, acc 1
2016-09-06T12:46:02.156057: step 16335, loss 0.0708496, acc 0.96
2016-09-06T12:46:02.987568: step 16336, loss 0.0109414, acc 1
2016-09-06T12:46:03.812590: step 16337, loss 0.00384284, acc 1
2016-09-06T12:46:04.633571: step 16338, loss 0.00266282, acc 1
2016-09-06T12:46:05.475234: step 16339, loss 0.00278324, acc 1
2016-09-06T12:46:06.296697: step 16340, loss 0.00229365, acc 1
2016-09-06T12:46:07.104688: step 16341, loss 0.00232735, acc 1
2016-09-06T12:46:07.938104: step 16342, loss 0.00227211, acc 1
2016-09-06T12:46:08.755264: step 16343, loss 0.0027401, acc 1
2016-09-06T12:46:09.556345: step 16344, loss 0.0503373, acc 0.96
2016-09-06T12:46:10.378457: step 16345, loss 0.00254607, acc 1
2016-09-06T12:46:11.231954: step 16346, loss 0.00260332, acc 1
2016-09-06T12:46:12.015895: step 16347, loss 0.0304229, acc 0.98
2016-09-06T12:46:12.822783: step 16348, loss 0.00240431, acc 1
2016-09-06T12:46:13.636815: step 16349, loss 0.0144169, acc 1
2016-09-06T12:46:14.410548: step 16350, loss 0.141714, acc 0.92
2016-09-06T12:46:15.219893: step 16351, loss 0.0451956, acc 0.98
2016-09-06T12:46:16.025253: step 16352, loss 0.0348458, acc 0.96
2016-09-06T12:46:16.831241: step 16353, loss 0.0131255, acc 1
2016-09-06T12:46:17.628632: step 16354, loss 0.0230247, acc 0.98
2016-09-06T12:46:18.438345: step 16355, loss 0.00195862, acc 1
2016-09-06T12:46:19.234047: step 16356, loss 0.0097985, acc 1
2016-09-06T12:46:20.043199: step 16357, loss 0.0435112, acc 0.96
2016-09-06T12:46:20.897178: step 16358, loss 0.0355433, acc 0.98
2016-09-06T12:46:21.675311: step 16359, loss 0.00276336, acc 1
2016-09-06T12:46:22.460809: step 16360, loss 0.0107623, acc 1
2016-09-06T12:46:23.303391: step 16361, loss 0.0167476, acc 0.98
2016-09-06T12:46:24.115984: step 16362, loss 0.00594753, acc 1
2016-09-06T12:46:24.909458: step 16363, loss 0.0988367, acc 0.96
2016-09-06T12:46:25.728369: step 16364, loss 0.00484574, acc 1
2016-09-06T12:46:26.547087: step 16365, loss 0.0452454, acc 0.98
2016-09-06T12:46:27.367694: step 16366, loss 0.00757394, acc 1
2016-09-06T12:46:28.231340: step 16367, loss 0.0120873, acc 1
2016-09-06T12:46:29.031960: step 16368, loss 0.00279716, acc 1
2016-09-06T12:46:29.844240: step 16369, loss 0.00293017, acc 1
2016-09-06T12:46:30.689891: step 16370, loss 0.0131436, acc 1
2016-09-06T12:46:31.498577: step 16371, loss 0.00888858, acc 1
2016-09-06T12:46:32.309796: step 16372, loss 0.00407366, acc 1
2016-09-06T12:46:33.113517: step 16373, loss 0.0322311, acc 0.98
2016-09-06T12:46:33.917150: step 16374, loss 0.021267, acc 0.98
2016-09-06T12:46:34.718778: step 16375, loss 0.00271455, acc 1
2016-09-06T12:46:35.545767: step 16376, loss 0.0408925, acc 0.98
2016-09-06T12:46:36.354392: step 16377, loss 0.0028659, acc 1
2016-09-06T12:46:37.175211: step 16378, loss 0.0589987, acc 0.96
2016-09-06T12:46:38.026625: step 16379, loss 0.00283018, acc 1
2016-09-06T12:46:38.835167: step 16380, loss 0.00765565, acc 1
2016-09-06T12:46:39.641994: step 16381, loss 0.0119927, acc 1
2016-09-06T12:46:40.470444: step 16382, loss 0.00491784, acc 1
2016-09-06T12:46:41.277447: step 16383, loss 0.00647984, acc 1
2016-09-06T12:46:42.130920: step 16384, loss 0.0967374, acc 0.98
2016-09-06T12:46:42.933359: step 16385, loss 0.0239105, acc 0.98
2016-09-06T12:46:43.763756: step 16386, loss 0.236421, acc 0.98
2016-09-06T12:46:44.574695: step 16387, loss 0.00317379, acc 1
2016-09-06T12:46:45.404863: step 16388, loss 0.0149515, acc 1
2016-09-06T12:46:46.275393: step 16389, loss 0.0549357, acc 0.96
2016-09-06T12:46:47.083383: step 16390, loss 0.00344678, acc 1
2016-09-06T12:46:47.910170: step 16391, loss 0.0112652, acc 1
2016-09-06T12:46:48.731721: step 16392, loss 0.0174591, acc 1
2016-09-06T12:46:49.519279: step 16393, loss 0.0181349, acc 1
2016-09-06T12:46:50.329785: step 16394, loss 0.0055274, acc 1
2016-09-06T12:46:51.149005: step 16395, loss 0.00931727, acc 1
2016-09-06T12:46:51.956284: step 16396, loss 0.0191473, acc 0.98
2016-09-06T12:46:52.778074: step 16397, loss 0.0196663, acc 1
2016-09-06T12:46:53.617835: step 16398, loss 0.0639314, acc 0.96
2016-09-06T12:46:54.459699: step 16399, loss 0.00712667, acc 1
2016-09-06T12:46:55.253114: step 16400, loss 0.0122366, acc 1

Evaluation:
2016-09-06T12:46:58.971764: step 16400, loss 2.06614, acc 0.729831

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-16400

2016-09-06T12:47:00.916710: step 16401, loss 0.00246312, acc 1
2016-09-06T12:47:01.736368: step 16402, loss 0.00470924, acc 1
2016-09-06T12:47:02.539420: step 16403, loss 0.0272206, acc 0.98
2016-09-06T12:47:03.377769: step 16404, loss 0.00462812, acc 1
2016-09-06T12:47:04.200671: step 16405, loss 0.030285, acc 1
2016-09-06T12:47:05.020338: step 16406, loss 0.0229284, acc 1
2016-09-06T12:47:05.865473: step 16407, loss 0.00301729, acc 1
2016-09-06T12:47:06.696712: step 16408, loss 0.0280751, acc 0.98
2016-09-06T12:47:07.523469: step 16409, loss 0.0028877, acc 1
2016-09-06T12:47:08.344275: step 16410, loss 0.0049341, acc 1
2016-09-06T12:47:09.153008: step 16411, loss 0.0150933, acc 1
2016-09-06T12:47:09.969522: step 16412, loss 0.00262519, acc 1
2016-09-06T12:47:10.797717: step 16413, loss 0.0146813, acc 1
2016-09-06T12:47:11.595623: step 16414, loss 0.00279327, acc 1
2016-09-06T12:47:12.395888: step 16415, loss 0.0252387, acc 0.98
2016-09-06T12:47:13.209604: step 16416, loss 0.00270418, acc 1
2016-09-06T12:47:14.028567: step 16417, loss 0.00265671, acc 1
2016-09-06T12:47:14.839728: step 16418, loss 0.00426005, acc 1
2016-09-06T12:47:15.672192: step 16419, loss 0.0204969, acc 0.98
2016-09-06T12:47:16.503864: step 16420, loss 0.018047, acc 0.98
2016-09-06T12:47:17.322886: step 16421, loss 0.0030582, acc 1
2016-09-06T12:47:18.132769: step 16422, loss 0.0160676, acc 1
2016-09-06T12:47:18.923362: step 16423, loss 0.0177389, acc 1
2016-09-06T12:47:19.719654: step 16424, loss 0.0355809, acc 0.98
2016-09-06T12:47:20.531840: step 16425, loss 0.0160599, acc 1
2016-09-06T12:47:21.331127: step 16426, loss 0.00267508, acc 1
2016-09-06T12:47:22.112757: step 16427, loss 0.00850308, acc 1
2016-09-06T12:47:22.926853: step 16428, loss 0.00880948, acc 1
2016-09-06T12:47:23.757447: step 16429, loss 0.0184961, acc 0.98
2016-09-06T12:47:24.561282: step 16430, loss 0.0096038, acc 1
2016-09-06T12:47:25.358863: step 16431, loss 0.0276495, acc 0.98
2016-09-06T12:47:26.183560: step 16432, loss 0.0207347, acc 0.98
2016-09-06T12:47:26.939856: step 16433, loss 0.034727, acc 0.98
2016-09-06T12:47:27.757608: step 16434, loss 0.0139252, acc 1
2016-09-06T12:47:28.594164: step 16435, loss 0.0174855, acc 1
2016-09-06T12:47:29.363054: step 16436, loss 0.0228715, acc 0.98
2016-09-06T12:47:30.157300: step 16437, loss 0.00244428, acc 1
2016-09-06T12:47:30.967295: step 16438, loss 0.00242576, acc 1
2016-09-06T12:47:31.760868: step 16439, loss 0.00895926, acc 1
2016-09-06T12:47:32.569947: step 16440, loss 0.0355377, acc 0.98
2016-09-06T12:47:33.384183: step 16441, loss 0.00236389, acc 1
2016-09-06T12:47:34.200836: step 16442, loss 0.00233904, acc 1
2016-09-06T12:47:34.998183: step 16443, loss 0.0143126, acc 1
2016-09-06T12:47:35.806792: step 16444, loss 0.00308653, acc 1
2016-09-06T12:47:36.591184: step 16445, loss 0.00390065, acc 1
2016-09-06T12:47:37.390041: step 16446, loss 0.0124414, acc 1
2016-09-06T12:47:38.251033: step 16447, loss 0.0257216, acc 1
2016-09-06T12:47:39.042622: step 16448, loss 0.0249029, acc 0.98
2016-09-06T12:47:39.846013: step 16449, loss 0.00735056, acc 1
2016-09-06T12:47:40.682279: step 16450, loss 0.00220923, acc 1
2016-09-06T12:47:41.481480: step 16451, loss 0.00223302, acc 1
2016-09-06T12:47:42.283800: step 16452, loss 0.00965988, acc 1
2016-09-06T12:47:43.108389: step 16453, loss 0.0332119, acc 0.98
2016-09-06T12:47:43.922263: step 16454, loss 0.0209203, acc 0.98
2016-09-06T12:47:44.717216: step 16455, loss 0.0211654, acc 0.98
2016-09-06T12:47:45.525400: step 16456, loss 0.018147, acc 0.98
2016-09-06T12:47:46.303186: step 16457, loss 0.0140186, acc 1
2016-09-06T12:47:47.100595: step 16458, loss 0.206389, acc 0.96
2016-09-06T12:47:47.935272: step 16459, loss 0.00201082, acc 1
2016-09-06T12:47:48.726442: step 16460, loss 0.00196498, acc 1
2016-09-06T12:47:49.508606: step 16461, loss 0.0276571, acc 0.98
2016-09-06T12:47:50.327182: step 16462, loss 0.00594512, acc 1
2016-09-06T12:47:51.098454: step 16463, loss 0.00524771, acc 1
2016-09-06T12:47:51.888367: step 16464, loss 0.00879108, acc 1
2016-09-06T12:47:52.707324: step 16465, loss 0.00880877, acc 1
2016-09-06T12:47:53.500924: step 16466, loss 0.00993872, acc 1
2016-09-06T12:47:54.326879: step 16467, loss 0.0361519, acc 0.98
2016-09-06T12:47:55.147815: step 16468, loss 0.0040641, acc 1
2016-09-06T12:47:55.959135: step 16469, loss 0.0163146, acc 0.98
2016-09-06T12:47:56.770132: step 16470, loss 0.00375766, acc 1
2016-09-06T12:47:57.598392: step 16471, loss 0.00242951, acc 1
2016-09-06T12:47:58.366563: step 16472, loss 0.0021182, acc 1
2016-09-06T12:47:59.165936: step 16473, loss 0.00289455, acc 1
2016-09-06T12:47:59.965064: step 16474, loss 0.00229925, acc 1
2016-09-06T12:48:00.760562: step 16475, loss 0.0386678, acc 0.98
2016-09-06T12:48:01.565938: step 16476, loss 0.0167748, acc 1
2016-09-06T12:48:02.388047: step 16477, loss 0.00971253, acc 1
2016-09-06T12:48:03.192896: step 16478, loss 0.0187273, acc 0.98
2016-09-06T12:48:04.011682: step 16479, loss 0.00884466, acc 1
2016-09-06T12:48:04.855540: step 16480, loss 0.045497, acc 1
2016-09-06T12:48:05.649640: step 16481, loss 0.00521767, acc 1
2016-09-06T12:48:06.460131: step 16482, loss 0.0108448, acc 1
2016-09-06T12:48:07.308967: step 16483, loss 0.00747858, acc 1
2016-09-06T12:48:08.121039: step 16484, loss 0.00227449, acc 1
2016-09-06T12:48:08.943533: step 16485, loss 0.00229525, acc 1
2016-09-06T12:48:09.820530: step 16486, loss 0.0247239, acc 1
2016-09-06T12:48:10.683289: step 16487, loss 0.00843768, acc 1
2016-09-06T12:48:11.498307: step 16488, loss 0.00564751, acc 1
2016-09-06T12:48:12.362151: step 16489, loss 0.0279123, acc 0.98
2016-09-06T12:48:13.205691: step 16490, loss 0.0331196, acc 0.98
2016-09-06T12:48:14.039137: step 16491, loss 0.0531098, acc 0.96
2016-09-06T12:48:14.928458: step 16492, loss 0.00810165, acc 1
2016-09-06T12:48:15.753200: step 16493, loss 0.00264078, acc 1
2016-09-06T12:48:16.547051: step 16494, loss 0.00280667, acc 1
2016-09-06T12:48:17.368092: step 16495, loss 0.0288431, acc 0.98
2016-09-06T12:48:18.209049: step 16496, loss 0.00261263, acc 1
2016-09-06T12:48:19.027200: step 16497, loss 0.00340498, acc 1
2016-09-06T12:48:19.860638: step 16498, loss 0.0420637, acc 0.98
2016-09-06T12:48:20.698184: step 16499, loss 0.00375798, acc 1
2016-09-06T12:48:21.511408: step 16500, loss 0.0796422, acc 0.98

Evaluation:
2016-09-06T12:48:25.251988: step 16500, loss 2.61789, acc 0.737336

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-16500

2016-09-06T12:48:27.129509: step 16501, loss 0.00231277, acc 1
2016-09-06T12:48:27.948222: step 16502, loss 0.00228909, acc 1
2016-09-06T12:48:28.736484: step 16503, loss 0.00218769, acc 1
2016-09-06T12:48:29.540632: step 16504, loss 0.00223912, acc 1
2016-09-06T12:48:30.376200: step 16505, loss 0.0147842, acc 1
2016-09-06T12:48:31.174647: step 16506, loss 0.0114566, acc 1
2016-09-06T12:48:31.959370: step 16507, loss 0.0435153, acc 0.96
2016-09-06T12:48:32.793740: step 16508, loss 0.015539, acc 1
2016-09-06T12:48:33.589452: step 16509, loss 0.0192476, acc 1
2016-09-06T12:48:34.409861: step 16510, loss 0.00613277, acc 1
2016-09-06T12:48:35.232699: step 16511, loss 0.0242432, acc 0.98
2016-09-06T12:48:35.990570: step 16512, loss 0.0033049, acc 1
2016-09-06T12:48:36.811968: step 16513, loss 0.00221577, acc 1
2016-09-06T12:48:37.627009: step 16514, loss 0.0353029, acc 0.98
2016-09-06T12:48:38.435983: step 16515, loss 0.00514027, acc 1
2016-09-06T12:48:39.244138: step 16516, loss 0.00225758, acc 1
2016-09-06T12:48:40.080809: step 16517, loss 0.00827225, acc 1
2016-09-06T12:48:40.901745: step 16518, loss 0.0321566, acc 0.98
2016-09-06T12:48:41.718955: step 16519, loss 0.00567876, acc 1
2016-09-06T12:48:42.566546: step 16520, loss 0.0329233, acc 0.96
2016-09-06T12:48:43.372745: step 16521, loss 0.00372464, acc 1
2016-09-06T12:48:44.204887: step 16522, loss 0.0219393, acc 1
2016-09-06T12:48:45.039681: step 16523, loss 0.00282091, acc 1
2016-09-06T12:48:45.874736: step 16524, loss 0.0094131, acc 1
2016-09-06T12:48:46.659858: step 16525, loss 0.0357922, acc 0.98
2016-09-06T12:48:47.510212: step 16526, loss 0.018026, acc 0.98
2016-09-06T12:48:48.363330: step 16527, loss 0.0380702, acc 0.98
2016-09-06T12:48:49.174015: step 16528, loss 0.0022291, acc 1
2016-09-06T12:48:50.003469: step 16529, loss 0.0159452, acc 1
2016-09-06T12:48:50.814919: step 16530, loss 0.00224931, acc 1
2016-09-06T12:48:51.597372: step 16531, loss 0.0152091, acc 1
2016-09-06T12:48:52.443052: step 16532, loss 0.0022007, acc 1
2016-09-06T12:48:53.280411: step 16533, loss 0.0301119, acc 0.98
2016-09-06T12:48:54.088903: step 16534, loss 0.00680788, acc 1
2016-09-06T12:48:54.924925: step 16535, loss 0.00242238, acc 1
2016-09-06T12:48:55.756267: step 16536, loss 0.0429485, acc 0.98
2016-09-06T12:48:56.592201: step 16537, loss 0.00252613, acc 1
2016-09-06T12:48:57.410247: step 16538, loss 0.012594, acc 1
2016-09-06T12:48:58.249367: step 16539, loss 0.0146318, acc 1
2016-09-06T12:48:59.061876: step 16540, loss 0.00243067, acc 1
2016-09-06T12:48:59.877705: step 16541, loss 0.00334677, acc 1
2016-09-06T12:49:00.710385: step 16542, loss 0.00274026, acc 1
2016-09-06T12:49:01.498584: step 16543, loss 0.00219508, acc 1
2016-09-06T12:49:02.320215: step 16544, loss 0.0267583, acc 0.98
2016-09-06T12:49:03.202598: step 16545, loss 0.0175867, acc 0.98
2016-09-06T12:49:03.994256: step 16546, loss 0.034508, acc 0.96
2016-09-06T12:49:04.826106: step 16547, loss 0.00771646, acc 1
2016-09-06T12:49:05.657945: step 16548, loss 0.00241635, acc 1
2016-09-06T12:49:06.478388: step 16549, loss 0.00209957, acc 1
2016-09-06T12:49:07.291427: step 16550, loss 0.0103917, acc 1
2016-09-06T12:49:08.095636: step 16551, loss 0.0114028, acc 1
2016-09-06T12:49:08.915713: step 16552, loss 0.00211176, acc 1
2016-09-06T12:49:09.697371: step 16553, loss 0.00853597, acc 1
2016-09-06T12:49:10.505296: step 16554, loss 0.00205816, acc 1
2016-09-06T12:49:11.308945: step 16555, loss 0.00240553, acc 1
2016-09-06T12:49:12.106648: step 16556, loss 0.00205134, acc 1
2016-09-06T12:49:12.897780: step 16557, loss 0.00207747, acc 1
2016-09-06T12:49:13.723180: step 16558, loss 0.0117157, acc 1
2016-09-06T12:49:14.495126: step 16559, loss 0.0144257, acc 1
2016-09-06T12:49:15.313017: step 16560, loss 0.0351909, acc 0.98
2016-09-06T12:49:16.109924: step 16561, loss 0.00228564, acc 1
2016-09-06T12:49:16.917892: step 16562, loss 0.00210372, acc 1
2016-09-06T12:49:17.720317: step 16563, loss 0.0167537, acc 0.98
2016-09-06T12:49:18.571894: step 16564, loss 0.00199073, acc 1
2016-09-06T12:49:19.367316: step 16565, loss 0.0213857, acc 1
2016-09-06T12:49:20.175465: step 16566, loss 0.00271339, acc 1
2016-09-06T12:49:20.980199: step 16567, loss 0.0048537, acc 1
2016-09-06T12:49:21.772824: step 16568, loss 0.00300404, acc 1
2016-09-06T12:49:22.575901: step 16569, loss 0.00304394, acc 1
2016-09-06T12:49:23.386709: step 16570, loss 0.0149456, acc 1
2016-09-06T12:49:24.197658: step 16571, loss 0.00660096, acc 1
2016-09-06T12:49:25.001315: step 16572, loss 0.00232294, acc 1
2016-09-06T12:49:25.807426: step 16573, loss 0.00715926, acc 1
2016-09-06T12:49:26.611169: step 16574, loss 0.00563921, acc 1
2016-09-06T12:49:27.396266: step 16575, loss 0.00184117, acc 1
2016-09-06T12:49:28.234208: step 16576, loss 0.0114095, acc 1
2016-09-06T12:49:29.030828: step 16577, loss 0.00189219, acc 1
2016-09-06T12:49:29.821363: step 16578, loss 0.00702054, acc 1
2016-09-06T12:49:30.635647: step 16579, loss 0.0158623, acc 0.98
2016-09-06T12:49:31.416644: step 16580, loss 0.0233004, acc 1
2016-09-06T12:49:32.250080: step 16581, loss 0.00444419, acc 1
2016-09-06T12:49:33.065902: step 16582, loss 0.0018606, acc 1
2016-09-06T12:49:33.865254: step 16583, loss 0.0496922, acc 0.98
2016-09-06T12:49:34.651343: step 16584, loss 0.0124772, acc 1
2016-09-06T12:49:35.474171: step 16585, loss 0.0376446, acc 0.98
2016-09-06T12:49:36.255458: step 16586, loss 0.00181635, acc 1
2016-09-06T12:49:37.086680: step 16587, loss 0.0678521, acc 0.98
2016-09-06T12:49:37.911654: step 16588, loss 0.0163155, acc 0.98
2016-09-06T12:49:38.712870: step 16589, loss 0.00912651, acc 1
2016-09-06T12:49:39.524254: step 16590, loss 0.02973, acc 0.98
2016-09-06T12:49:40.365618: step 16591, loss 0.0145349, acc 1
2016-09-06T12:49:41.199531: step 16592, loss 0.0179298, acc 0.98
2016-09-06T12:49:42.017070: step 16593, loss 0.00287666, acc 1
2016-09-06T12:49:42.841338: step 16594, loss 0.00229218, acc 1
2016-09-06T12:49:43.645101: step 16595, loss 0.0188181, acc 0.98
2016-09-06T12:49:44.446289: step 16596, loss 0.00187563, acc 1
2016-09-06T12:49:45.261947: step 16597, loss 0.00165501, acc 1
2016-09-06T12:49:46.076122: step 16598, loss 0.0148774, acc 1
2016-09-06T12:49:46.895721: step 16599, loss 0.00161366, acc 1
2016-09-06T12:49:47.705361: step 16600, loss 0.0581431, acc 0.98

Evaluation:
2016-09-06T12:49:51.402209: step 16600, loss 2.15793, acc 0.731707

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-16600

2016-09-06T12:49:53.384127: step 16601, loss 0.0252498, acc 1
2016-09-06T12:49:54.163425: step 16602, loss 0.0186398, acc 1
2016-09-06T12:49:55.004660: step 16603, loss 0.0111353, acc 1
2016-09-06T12:49:55.803835: step 16604, loss 0.00174174, acc 1
2016-09-06T12:49:56.612363: step 16605, loss 0.00251275, acc 1
2016-09-06T12:49:57.444591: step 16606, loss 0.0163261, acc 0.98
2016-09-06T12:49:58.298191: step 16607, loss 0.0141177, acc 1
2016-09-06T12:49:59.130777: step 16608, loss 0.00150058, acc 1
2016-09-06T12:49:59.970716: step 16609, loss 0.00150462, acc 1
2016-09-06T12:50:00.830937: step 16610, loss 0.00380967, acc 1
2016-09-06T12:50:01.659564: step 16611, loss 0.00931017, acc 1
2016-09-06T12:50:02.484060: step 16612, loss 0.0125946, acc 1
2016-09-06T12:50:03.303834: step 16613, loss 0.0040416, acc 1
2016-09-06T12:50:04.078154: step 16614, loss 0.00203025, acc 1
2016-09-06T12:50:04.888871: step 16615, loss 0.00851165, acc 1
2016-09-06T12:50:05.690794: step 16616, loss 0.00643866, acc 1
2016-09-06T12:50:06.497980: step 16617, loss 0.0112949, acc 1
2016-09-06T12:50:07.306553: step 16618, loss 0.00180787, acc 1
2016-09-06T12:50:08.105249: step 16619, loss 0.015039, acc 1
2016-09-06T12:50:08.910618: step 16620, loss 0.0208784, acc 1
2016-09-06T12:50:09.721125: step 16621, loss 0.0189555, acc 0.98
2016-09-06T12:50:10.509898: step 16622, loss 0.0126454, acc 1
2016-09-06T12:50:11.327666: step 16623, loss 0.00158639, acc 1
2016-09-06T12:50:12.113720: step 16624, loss 0.0254671, acc 1
2016-09-06T12:50:12.938279: step 16625, loss 0.00228314, acc 1
2016-09-06T12:50:13.741816: step 16626, loss 0.00206308, acc 1
2016-09-06T12:50:14.562453: step 16627, loss 0.014455, acc 1
2016-09-06T12:50:15.383964: step 16628, loss 0.00201721, acc 1
2016-09-06T12:50:16.197359: step 16629, loss 0.00931958, acc 1
2016-09-06T12:50:17.007238: step 16630, loss 0.00170775, acc 1
2016-09-06T12:50:17.829622: step 16631, loss 0.0184154, acc 0.98
2016-09-06T12:50:18.616539: step 16632, loss 0.0390109, acc 0.98
2016-09-06T12:50:19.416801: step 16633, loss 0.00169407, acc 1
2016-09-06T12:50:20.215637: step 16634, loss 0.0192138, acc 1
2016-09-06T12:50:21.006969: step 16635, loss 0.00172895, acc 1
2016-09-06T12:50:21.848257: step 16636, loss 0.00225392, acc 1
2016-09-06T12:50:22.690411: step 16637, loss 0.0215964, acc 0.98
2016-09-06T12:50:23.485400: step 16638, loss 0.00175769, acc 1
2016-09-06T12:50:24.269294: step 16639, loss 0.0215747, acc 0.98
2016-09-06T12:50:25.082981: step 16640, loss 0.0145787, acc 1
2016-09-06T12:50:25.888318: step 16641, loss 0.00166851, acc 1
2016-09-06T12:50:26.742563: step 16642, loss 0.0225919, acc 0.98
2016-09-06T12:50:27.578230: step 16643, loss 0.00616373, acc 1
2016-09-06T12:50:28.372619: step 16644, loss 0.08451, acc 0.98
2016-09-06T12:50:29.177367: step 16645, loss 0.00414241, acc 1
2016-09-06T12:50:29.998785: step 16646, loss 0.00201303, acc 1
2016-09-06T12:50:30.796416: step 16647, loss 0.00529406, acc 1
2016-09-06T12:50:31.596992: step 16648, loss 0.0181626, acc 0.98
2016-09-06T12:50:32.425209: step 16649, loss 0.00369899, acc 1
2016-09-06T12:50:33.264306: step 16650, loss 0.0285468, acc 1
2016-09-06T12:50:34.083760: step 16651, loss 0.00158437, acc 1
2016-09-06T12:50:34.933671: step 16652, loss 0.00161431, acc 1
2016-09-06T12:50:35.753345: step 16653, loss 0.0217859, acc 0.98
2016-09-06T12:50:36.581149: step 16654, loss 0.00157563, acc 1
2016-09-06T12:50:37.403044: step 16655, loss 0.00337129, acc 1
2016-09-06T12:50:38.262625: step 16656, loss 0.0209455, acc 0.98
2016-09-06T12:50:39.074043: step 16657, loss 0.0018037, acc 1
2016-09-06T12:50:39.923604: step 16658, loss 0.0115045, acc 1
2016-09-06T12:50:40.755504: step 16659, loss 0.00768861, acc 1
2016-09-06T12:50:41.567941: step 16660, loss 0.00201793, acc 1
2016-09-06T12:50:42.376678: step 16661, loss 0.0465075, acc 0.94
2016-09-06T12:50:43.183752: step 16662, loss 0.0156543, acc 0.98
2016-09-06T12:50:43.969137: step 16663, loss 0.00183578, acc 1
2016-09-06T12:50:44.770972: step 16664, loss 0.00454888, acc 1
2016-09-06T12:50:45.586594: step 16665, loss 0.00287639, acc 1
2016-09-06T12:50:46.374693: step 16666, loss 0.0106417, acc 1
2016-09-06T12:50:47.180792: step 16667, loss 0.081639, acc 0.98
2016-09-06T12:50:48.020235: step 16668, loss 0.0146478, acc 1
2016-09-06T12:50:48.828020: step 16669, loss 0.0037632, acc 1
2016-09-06T12:50:49.662427: step 16670, loss 0.0124399, acc 1
2016-09-06T12:50:50.510991: step 16671, loss 0.00202929, acc 1
2016-09-06T12:50:51.312717: step 16672, loss 0.011767, acc 1
2016-09-06T12:50:52.112947: step 16673, loss 0.0239217, acc 0.98
2016-09-06T12:50:52.932283: step 16674, loss 0.00464886, acc 1
2016-09-06T12:50:53.710411: step 16675, loss 0.0116009, acc 1
2016-09-06T12:50:54.518507: step 16676, loss 0.00163628, acc 1
2016-09-06T12:50:55.343907: step 16677, loss 0.00982991, acc 1
2016-09-06T12:50:56.149610: step 16678, loss 0.00233469, acc 1
2016-09-06T12:50:56.962248: step 16679, loss 0.00730992, acc 1
2016-09-06T12:50:57.774133: step 16680, loss 0.0717904, acc 0.96
2016-09-06T12:50:58.563253: step 16681, loss 0.0093428, acc 1
2016-09-06T12:50:59.372454: step 16682, loss 0.00244728, acc 1
2016-09-06T12:51:00.250825: step 16683, loss 0.0217613, acc 1
2016-09-06T12:51:01.033946: step 16684, loss 0.0129502, acc 1
2016-09-06T12:51:01.829413: step 16685, loss 0.00273508, acc 1
2016-09-06T12:51:02.670474: step 16686, loss 0.0371468, acc 0.98
2016-09-06T12:51:03.470610: step 16687, loss 0.00208747, acc 1
2016-09-06T12:51:04.275373: step 16688, loss 0.0121294, acc 1
2016-09-06T12:51:05.089710: step 16689, loss 0.0206432, acc 0.98
2016-09-06T12:51:05.881439: step 16690, loss 0.0301486, acc 0.98
2016-09-06T12:51:06.695792: step 16691, loss 0.020296, acc 0.98
2016-09-06T12:51:07.544087: step 16692, loss 0.0159419, acc 1
2016-09-06T12:51:08.380256: step 16693, loss 0.0019164, acc 1
2016-09-06T12:51:09.226493: step 16694, loss 0.0140357, acc 1
2016-09-06T12:51:10.049404: step 16695, loss 0.0016212, acc 1
2016-09-06T12:51:10.852028: step 16696, loss 0.00315137, acc 1
2016-09-06T12:51:11.669590: step 16697, loss 0.00172562, acc 1
2016-09-06T12:51:12.502601: step 16698, loss 0.00758655, acc 1
2016-09-06T12:51:13.326327: step 16699, loss 0.0167613, acc 1
2016-09-06T12:51:14.157456: step 16700, loss 0.00192916, acc 1

Evaluation:
2016-09-06T12:51:17.864352: step 16700, loss 2.51404, acc 0.733584

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-16700

2016-09-06T12:51:19.736197: step 16701, loss 0.0621044, acc 0.96
2016-09-06T12:51:20.518288: step 16702, loss 0.00265499, acc 1
2016-09-06T12:51:21.345839: step 16703, loss 0.00195139, acc 1
2016-09-06T12:51:22.103441: step 16704, loss 0.00168752, acc 1
2016-09-06T12:51:22.926917: step 16705, loss 0.00166321, acc 1
2016-09-06T12:51:23.768761: step 16706, loss 0.0509779, acc 0.98
2016-09-06T12:51:24.581285: step 16707, loss 0.00522263, acc 1
2016-09-06T12:51:25.417726: step 16708, loss 0.00447367, acc 1
2016-09-06T12:51:26.239150: step 16709, loss 0.0275408, acc 1
2016-09-06T12:51:27.083238: step 16710, loss 0.00181613, acc 1
2016-09-06T12:51:27.906895: step 16711, loss 0.00902893, acc 1
2016-09-06T12:51:28.708609: step 16712, loss 0.00865556, acc 1
2016-09-06T12:51:29.528556: step 16713, loss 0.0315597, acc 0.98
2016-09-06T12:51:30.364383: step 16714, loss 0.0253201, acc 0.98
2016-09-06T12:51:31.184623: step 16715, loss 0.00158585, acc 1
2016-09-06T12:51:31.984245: step 16716, loss 0.0015883, acc 1
2016-09-06T12:51:32.829375: step 16717, loss 0.04159, acc 0.98
2016-09-06T12:51:33.631555: step 16718, loss 0.0131922, acc 1
2016-09-06T12:51:34.402015: step 16719, loss 0.0548914, acc 0.98
2016-09-06T12:51:35.247375: step 16720, loss 0.00515727, acc 1
2016-09-06T12:51:36.037399: step 16721, loss 0.00438008, acc 1
2016-09-06T12:51:36.838951: step 16722, loss 0.00151638, acc 1
2016-09-06T12:51:37.644002: step 16723, loss 0.00186221, acc 1
2016-09-06T12:51:38.437658: step 16724, loss 0.00179677, acc 1
2016-09-06T12:51:39.249459: step 16725, loss 0.0225365, acc 0.98
2016-09-06T12:51:40.094783: step 16726, loss 0.0639702, acc 0.96
2016-09-06T12:51:40.893788: step 16727, loss 0.00188517, acc 1
2016-09-06T12:51:41.707928: step 16728, loss 0.0660378, acc 0.96
2016-09-06T12:51:42.512853: step 16729, loss 0.00154692, acc 1
2016-09-06T12:51:43.309334: step 16730, loss 0.01214, acc 1
2016-09-06T12:51:44.125637: step 16731, loss 0.0022679, acc 1
2016-09-06T12:51:44.950777: step 16732, loss 0.00178311, acc 1
2016-09-06T12:51:45.776138: step 16733, loss 0.00636057, acc 1
2016-09-06T12:51:46.565663: step 16734, loss 0.0101754, acc 1
2016-09-06T12:51:47.382370: step 16735, loss 0.00637638, acc 1
2016-09-06T12:51:48.177051: step 16736, loss 0.0147732, acc 1
2016-09-06T12:51:48.968854: step 16737, loss 0.0204627, acc 1
2016-09-06T12:51:49.813505: step 16738, loss 0.009514, acc 1
2016-09-06T12:51:50.622201: step 16739, loss 0.00192083, acc 1
2016-09-06T12:51:51.441432: step 16740, loss 0.0031324, acc 1
2016-09-06T12:51:52.275269: step 16741, loss 0.0212182, acc 0.98
2016-09-06T12:51:53.095495: step 16742, loss 0.0305231, acc 0.98
2016-09-06T12:51:53.924724: step 16743, loss 0.00908409, acc 1
2016-09-06T12:51:54.782434: step 16744, loss 0.0238142, acc 0.98
2016-09-06T12:51:55.615867: step 16745, loss 0.0062151, acc 1
2016-09-06T12:51:56.422563: step 16746, loss 0.00180297, acc 1
2016-09-06T12:51:57.259768: step 16747, loss 0.0103721, acc 1
2016-09-06T12:51:58.074804: step 16748, loss 0.00335362, acc 1
2016-09-06T12:51:58.911575: step 16749, loss 0.00195994, acc 1
2016-09-06T12:51:59.760727: step 16750, loss 0.0226283, acc 0.98
2016-09-06T12:52:00.630330: step 16751, loss 0.0439463, acc 0.98
2016-09-06T12:52:01.439613: step 16752, loss 0.00181311, acc 1
2016-09-06T12:52:02.261598: step 16753, loss 0.00455434, acc 1
2016-09-06T12:52:03.083999: step 16754, loss 0.00234558, acc 1
2016-09-06T12:52:03.886099: step 16755, loss 0.0146286, acc 1
2016-09-06T12:52:04.705643: step 16756, loss 0.0179269, acc 0.98
2016-09-06T12:52:05.528343: step 16757, loss 0.0175945, acc 0.98
2016-09-06T12:52:06.334313: step 16758, loss 0.00277024, acc 1
2016-09-06T12:52:07.136638: step 16759, loss 0.0143246, acc 1
2016-09-06T12:52:07.970199: step 16760, loss 0.00204995, acc 1
2016-09-06T12:52:08.767269: step 16761, loss 0.00164827, acc 1
2016-09-06T12:52:09.595422: step 16762, loss 0.0359665, acc 0.98
2016-09-06T12:52:10.458340: step 16763, loss 0.0033395, acc 1
2016-09-06T12:52:11.272282: step 16764, loss 0.00201508, acc 1
2016-09-06T12:52:12.068983: step 16765, loss 0.0608051, acc 0.94
2016-09-06T12:52:12.909244: step 16766, loss 0.0015367, acc 1
2016-09-06T12:52:13.712183: step 16767, loss 0.00152232, acc 1
2016-09-06T12:52:14.506331: step 16768, loss 0.0342768, acc 0.98
2016-09-06T12:52:15.333800: step 16769, loss 0.0128588, acc 1
2016-09-06T12:52:16.160743: step 16770, loss 0.0170257, acc 0.98
2016-09-06T12:52:16.965165: step 16771, loss 0.0155255, acc 0.98
2016-09-06T12:52:17.794121: step 16772, loss 0.0053857, acc 1
2016-09-06T12:52:18.588988: step 16773, loss 0.0204318, acc 1
2016-09-06T12:52:19.392399: step 16774, loss 0.0046611, acc 1
2016-09-06T12:52:20.234754: step 16775, loss 0.00488804, acc 1
2016-09-06T12:52:21.029409: step 16776, loss 0.00174186, acc 1
2016-09-06T12:52:21.845352: step 16777, loss 0.0149492, acc 1
2016-09-06T12:52:22.674438: step 16778, loss 0.00219233, acc 1
2016-09-06T12:52:23.487428: step 16779, loss 0.0165362, acc 1
2016-09-06T12:52:24.326072: step 16780, loss 0.0188165, acc 1
2016-09-06T12:52:25.125727: step 16781, loss 0.0157081, acc 1
2016-09-06T12:52:25.943570: step 16782, loss 0.0177146, acc 1
2016-09-06T12:52:26.730655: step 16783, loss 0.0013854, acc 1
2016-09-06T12:52:27.519276: step 16784, loss 0.00929324, acc 1
2016-09-06T12:52:28.332346: step 16785, loss 0.0112271, acc 1
2016-09-06T12:52:29.127541: step 16786, loss 0.011938, acc 1
2016-09-06T12:52:29.945079: step 16787, loss 0.0178568, acc 0.98
2016-09-06T12:52:30.761854: step 16788, loss 0.00289057, acc 1
2016-09-06T12:52:31.562336: step 16789, loss 0.023437, acc 0.98
2016-09-06T12:52:32.372472: step 16790, loss 0.00552291, acc 1
2016-09-06T12:52:33.190161: step 16791, loss 0.00155188, acc 1
2016-09-06T12:52:33.974954: step 16792, loss 0.00172053, acc 1
2016-09-06T12:52:34.784403: step 16793, loss 0.00575481, acc 1
2016-09-06T12:52:35.599715: step 16794, loss 0.00173087, acc 1
2016-09-06T12:52:36.409826: step 16795, loss 0.105356, acc 0.98
2016-09-06T12:52:37.201961: step 16796, loss 0.00357897, acc 1
2016-09-06T12:52:38.006219: step 16797, loss 0.00162487, acc 1
2016-09-06T12:52:38.801362: step 16798, loss 0.00142458, acc 1
2016-09-06T12:52:39.604382: step 16799, loss 0.0162522, acc 0.98
2016-09-06T12:52:40.408838: step 16800, loss 0.00142002, acc 1

Evaluation:
2016-09-06T12:52:44.147005: step 16800, loss 2.43923, acc 0.733584

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-16800

2016-09-06T12:52:46.105047: step 16801, loss 0.00141417, acc 1
2016-09-06T12:52:46.925836: step 16802, loss 0.173964, acc 0.98
2016-09-06T12:52:47.729578: step 16803, loss 0.00262851, acc 1
2016-09-06T12:52:48.526019: step 16804, loss 0.00771479, acc 1
2016-09-06T12:52:49.341268: step 16805, loss 0.00277409, acc 1
2016-09-06T12:52:50.179199: step 16806, loss 0.0081119, acc 1
2016-09-06T12:52:50.981183: step 16807, loss 0.013246, acc 1
2016-09-06T12:52:51.788381: step 16808, loss 0.0188153, acc 1
2016-09-06T12:52:52.653505: step 16809, loss 0.0233345, acc 1
2016-09-06T12:52:53.438914: step 16810, loss 0.00776919, acc 1
2016-09-06T12:52:54.219977: step 16811, loss 0.0232457, acc 0.98
2016-09-06T12:52:55.065543: step 16812, loss 0.0336659, acc 0.98
2016-09-06T12:52:55.873043: step 16813, loss 0.0254863, acc 1
2016-09-06T12:52:56.677670: step 16814, loss 0.0145131, acc 1
2016-09-06T12:52:57.487786: step 16815, loss 0.00238991, acc 1
2016-09-06T12:52:58.283064: step 16816, loss 0.0728782, acc 0.96
2016-09-06T12:52:59.079863: step 16817, loss 0.0241533, acc 0.98
2016-09-06T12:52:59.887151: step 16818, loss 0.0202223, acc 0.98
2016-09-06T12:53:00.670879: step 16819, loss 0.062037, acc 0.98
2016-09-06T12:53:01.511773: step 16820, loss 0.00281923, acc 1
2016-09-06T12:53:02.337052: step 16821, loss 0.0517458, acc 0.96
2016-09-06T12:53:03.144226: step 16822, loss 0.00432464, acc 1
2016-09-06T12:53:03.966114: step 16823, loss 0.0152605, acc 1
2016-09-06T12:53:04.800416: step 16824, loss 0.00617603, acc 1
2016-09-06T12:53:05.582911: step 16825, loss 0.00294722, acc 1
2016-09-06T12:53:06.400433: step 16826, loss 0.00301558, acc 1
2016-09-06T12:53:07.201501: step 16827, loss 0.00322808, acc 1
2016-09-06T12:53:08.000479: step 16828, loss 0.00727096, acc 1
2016-09-06T12:53:08.809538: step 16829, loss 0.00659291, acc 1
2016-09-06T12:53:09.633237: step 16830, loss 0.0189035, acc 0.98
2016-09-06T12:53:10.462198: step 16831, loss 0.0111865, acc 1
2016-09-06T12:53:11.276743: step 16832, loss 0.0117571, acc 1
2016-09-06T12:53:12.098179: step 16833, loss 0.0148771, acc 1
2016-09-06T12:53:12.927411: step 16834, loss 0.00992676, acc 1
2016-09-06T12:53:13.760258: step 16835, loss 0.00313797, acc 1
2016-09-06T12:53:14.620798: step 16836, loss 0.00778745, acc 1
2016-09-06T12:53:15.494410: step 16837, loss 0.0472381, acc 0.98
2016-09-06T12:53:16.297364: step 16838, loss 0.00736848, acc 1
2016-09-06T12:53:17.145274: step 16839, loss 0.074984, acc 0.96
2016-09-06T12:53:17.971543: step 16840, loss 0.00299513, acc 1
2016-09-06T12:53:18.778710: step 16841, loss 0.024512, acc 0.98
2016-09-06T12:53:19.597664: step 16842, loss 0.0206142, acc 0.98
2016-09-06T12:53:20.411160: step 16843, loss 0.00288042, acc 1
2016-09-06T12:53:21.203594: step 16844, loss 0.0028883, acc 1
2016-09-06T12:53:22.054870: step 16845, loss 0.0189007, acc 0.98
2016-09-06T12:53:22.867205: step 16846, loss 0.0363547, acc 0.98
2016-09-06T12:53:23.688781: step 16847, loss 0.00911002, acc 1
2016-09-06T12:53:24.529258: step 16848, loss 0.00478401, acc 1
2016-09-06T12:53:25.370572: step 16849, loss 0.0028078, acc 1
2016-09-06T12:53:26.161496: step 16850, loss 0.0196786, acc 0.98
2016-09-06T12:53:26.948382: step 16851, loss 0.0167742, acc 1
2016-09-06T12:53:27.808378: step 16852, loss 0.00782281, acc 1
2016-09-06T12:53:28.618492: step 16853, loss 0.00292008, acc 1
2016-09-06T12:53:29.407646: step 16854, loss 0.00843912, acc 1
2016-09-06T12:53:30.214202: step 16855, loss 0.00261347, acc 1
2016-09-06T12:53:31.027323: step 16856, loss 0.00259398, acc 1
2016-09-06T12:53:31.838800: step 16857, loss 0.002575, acc 1
2016-09-06T12:53:32.653152: step 16858, loss 0.00254903, acc 1
2016-09-06T12:53:33.464150: step 16859, loss 0.00276807, acc 1
2016-09-06T12:53:34.277316: step 16860, loss 0.029433, acc 0.98
2016-09-06T12:53:35.129355: step 16861, loss 0.00260702, acc 1
2016-09-06T12:53:35.940209: step 16862, loss 0.0702259, acc 0.96
2016-09-06T12:53:36.763023: step 16863, loss 0.0199177, acc 0.98
2016-09-06T12:53:37.606064: step 16864, loss 0.0165627, acc 1
2016-09-06T12:53:38.421417: step 16865, loss 0.0188983, acc 0.98
2016-09-06T12:53:39.228315: step 16866, loss 0.0023197, acc 1
2016-09-06T12:53:40.056708: step 16867, loss 0.0157777, acc 1
2016-09-06T12:53:40.887387: step 16868, loss 0.00415969, acc 1
2016-09-06T12:53:41.696086: step 16869, loss 0.002854, acc 1
2016-09-06T12:53:42.515050: step 16870, loss 0.0168284, acc 0.98
2016-09-06T12:53:43.317857: step 16871, loss 0.00739061, acc 1
2016-09-06T12:53:44.101954: step 16872, loss 0.00586158, acc 1
2016-09-06T12:53:44.938433: step 16873, loss 0.0205829, acc 1
2016-09-06T12:53:45.773408: step 16874, loss 0.00801985, acc 1
2016-09-06T12:53:46.592947: step 16875, loss 0.0142175, acc 1
2016-09-06T12:53:47.453368: step 16876, loss 0.00263713, acc 1
2016-09-06T12:53:48.294995: step 16877, loss 0.00866703, acc 1
2016-09-06T12:53:49.108778: step 16878, loss 0.0182652, acc 0.98
2016-09-06T12:53:49.908749: step 16879, loss 0.0205904, acc 1
2016-09-06T12:53:50.738640: step 16880, loss 0.00226579, acc 1
2016-09-06T12:53:51.514406: step 16881, loss 0.0151276, acc 1
2016-09-06T12:53:52.331062: step 16882, loss 0.0153916, acc 1
2016-09-06T12:53:53.147106: step 16883, loss 0.00268307, acc 1
2016-09-06T12:53:53.932287: step 16884, loss 0.0345935, acc 0.98
2016-09-06T12:53:54.777376: step 16885, loss 0.00460113, acc 1
2016-09-06T12:53:55.599651: step 16886, loss 0.0179375, acc 0.98
2016-09-06T12:53:56.401909: step 16887, loss 0.0022643, acc 1
2016-09-06T12:53:57.188140: step 16888, loss 0.00306136, acc 1
2016-09-06T12:53:58.026371: step 16889, loss 0.00582763, acc 1
2016-09-06T12:53:58.839181: step 16890, loss 0.00565847, acc 1
2016-09-06T12:53:59.644882: step 16891, loss 0.0326193, acc 0.98
2016-09-06T12:54:00.495199: step 16892, loss 0.00224396, acc 1
2016-09-06T12:54:01.300373: step 16893, loss 0.0379371, acc 0.96
2016-09-06T12:54:02.104382: step 16894, loss 0.00295199, acc 1
2016-09-06T12:54:02.969694: step 16895, loss 0.00220527, acc 1
2016-09-06T12:54:03.710559: step 16896, loss 0.120281, acc 0.977273
2016-09-06T12:54:04.521689: step 16897, loss 0.00216145, acc 1
2016-09-06T12:54:05.361932: step 16898, loss 0.0611088, acc 0.98
2016-09-06T12:54:06.171654: step 16899, loss 0.00198523, acc 1
2016-09-06T12:54:06.985563: step 16900, loss 0.00206303, acc 1

Evaluation:
2016-09-06T12:54:10.696064: step 16900, loss 2.85399, acc 0.742964

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-16900

2016-09-06T12:54:12.681315: step 16901, loss 0.00763879, acc 1
2016-09-06T12:54:13.492810: step 16902, loss 0.0438994, acc 0.98
2016-09-06T12:54:14.280222: step 16903, loss 0.0398541, acc 0.98
2016-09-06T12:54:15.091176: step 16904, loss 0.0171457, acc 0.98
2016-09-06T12:54:15.897665: step 16905, loss 0.00463422, acc 1
2016-09-06T12:54:16.699056: step 16906, loss 0.0368796, acc 0.98
2016-09-06T12:54:17.528267: step 16907, loss 0.00179071, acc 1
2016-09-06T12:54:18.365420: step 16908, loss 0.0167443, acc 0.98
2016-09-06T12:54:19.151570: step 16909, loss 0.0233474, acc 0.98
2016-09-06T12:54:20.012364: step 16910, loss 0.0314652, acc 0.98
2016-09-06T12:54:20.836590: step 16911, loss 0.0177628, acc 0.98
2016-09-06T12:54:21.663227: step 16912, loss 0.0172891, acc 0.98
2016-09-06T12:54:22.509051: step 16913, loss 0.014713, acc 1
2016-09-06T12:54:23.318685: step 16914, loss 0.00229659, acc 1
2016-09-06T12:54:24.134525: step 16915, loss 0.0285017, acc 0.98
2016-09-06T12:54:24.972283: step 16916, loss 0.00184468, acc 1
2016-09-06T12:54:25.789748: step 16917, loss 0.00336506, acc 1
2016-09-06T12:54:26.599239: step 16918, loss 0.0103418, acc 1
2016-09-06T12:54:27.434074: step 16919, loss 0.00187657, acc 1
2016-09-06T12:54:28.264473: step 16920, loss 0.025678, acc 1
2016-09-06T12:54:29.031286: step 16921, loss 0.00251692, acc 1
2016-09-06T12:54:29.893408: step 16922, loss 0.00294386, acc 1
2016-09-06T12:54:30.711820: step 16923, loss 0.00223558, acc 1
2016-09-06T12:54:31.532648: step 16924, loss 0.00191863, acc 1
2016-09-06T12:54:32.369656: step 16925, loss 0.00244433, acc 1
2016-09-06T12:54:33.162548: step 16926, loss 0.00809635, acc 1
2016-09-06T12:54:33.954894: step 16927, loss 0.00182459, acc 1
2016-09-06T12:54:34.808465: step 16928, loss 0.0225665, acc 0.98
2016-09-06T12:54:35.596932: step 16929, loss 0.00840959, acc 1
2016-09-06T12:54:36.412792: step 16930, loss 0.081574, acc 0.98
2016-09-06T12:54:37.259202: step 16931, loss 0.00195383, acc 1
2016-09-06T12:54:38.090602: step 16932, loss 0.00177769, acc 1
2016-09-06T12:54:38.896349: step 16933, loss 0.0127346, acc 1
2016-09-06T12:54:39.728710: step 16934, loss 0.00276928, acc 1
2016-09-06T12:54:40.528590: step 16935, loss 0.0127199, acc 1
2016-09-06T12:54:41.290852: step 16936, loss 0.00251344, acc 1
2016-09-06T12:54:42.087105: step 16937, loss 0.0074105, acc 1
2016-09-06T12:54:42.908676: step 16938, loss 0.0071936, acc 1
2016-09-06T12:54:43.711362: step 16939, loss 0.098865, acc 0.96
2016-09-06T12:54:44.522498: step 16940, loss 0.0146477, acc 1
2016-09-06T12:54:45.347643: step 16941, loss 0.00323531, acc 1
2016-09-06T12:54:46.145834: step 16942, loss 0.00315147, acc 1
2016-09-06T12:54:46.955246: step 16943, loss 0.0109025, acc 1
2016-09-06T12:54:47.774950: step 16944, loss 0.026144, acc 1
2016-09-06T12:54:48.582647: step 16945, loss 0.00341648, acc 1
2016-09-06T12:54:49.425449: step 16946, loss 0.00169633, acc 1
2016-09-06T12:54:50.237048: step 16947, loss 0.00327302, acc 1
2016-09-06T12:54:51.049770: step 16948, loss 0.0658348, acc 0.98
2016-09-06T12:54:51.871904: step 16949, loss 0.0386878, acc 0.98
2016-09-06T12:54:52.743091: step 16950, loss 0.00620077, acc 1
2016-09-06T12:54:53.533444: step 16951, loss 0.0412599, acc 0.96
2016-09-06T12:54:54.354866: step 16952, loss 0.0194378, acc 0.98
2016-09-06T12:54:55.175785: step 16953, loss 0.00182966, acc 1
2016-09-06T12:54:56.013338: step 16954, loss 0.0190086, acc 1
2016-09-06T12:54:56.836057: step 16955, loss 0.00190771, acc 1
2016-09-06T12:54:57.675220: step 16956, loss 0.00217723, acc 1
2016-09-06T12:54:58.471856: step 16957, loss 0.0186129, acc 0.98
2016-09-06T12:54:59.286287: step 16958, loss 0.00239939, acc 1
2016-09-06T12:55:00.113019: step 16959, loss 0.00266082, acc 1
2016-09-06T12:55:00.938931: step 16960, loss 0.0428721, acc 0.98
2016-09-06T12:55:01.756645: step 16961, loss 0.0312435, acc 0.96
2016-09-06T12:55:02.589416: step 16962, loss 0.0146125, acc 1
2016-09-06T12:55:03.414031: step 16963, loss 0.0101072, acc 1
2016-09-06T12:55:04.254867: step 16964, loss 0.00866948, acc 1
2016-09-06T12:55:05.091652: step 16965, loss 0.00642435, acc 1
2016-09-06T12:55:05.909589: step 16966, loss 0.00871301, acc 1
2016-09-06T12:55:06.685389: step 16967, loss 0.00564674, acc 1
2016-09-06T12:55:07.478840: step 16968, loss 0.0040424, acc 1
2016-09-06T12:55:08.294401: step 16969, loss 0.00746576, acc 1
2016-09-06T12:55:09.106338: step 16970, loss 0.00184186, acc 1
2016-09-06T12:55:09.905849: step 16971, loss 0.016377, acc 0.98
2016-09-06T12:55:10.745101: step 16972, loss 0.0102513, acc 1
2016-09-06T12:55:11.524371: step 16973, loss 0.00192602, acc 1
2016-09-06T12:55:12.308026: step 16974, loss 0.0312609, acc 0.98
2016-09-06T12:55:13.155387: step 16975, loss 0.0161303, acc 1
2016-09-06T12:55:13.940459: step 16976, loss 0.00191219, acc 1
2016-09-06T12:55:14.730666: step 16977, loss 0.00194863, acc 1
2016-09-06T12:55:15.552272: step 16978, loss 0.0118219, acc 1
2016-09-06T12:55:16.343436: step 16979, loss 0.017698, acc 1
2016-09-06T12:55:17.134880: step 16980, loss 0.00413196, acc 1
2016-09-06T12:55:17.965826: step 16981, loss 0.00798329, acc 1
2016-09-06T12:55:18.757005: step 16982, loss 0.0112943, acc 1
2016-09-06T12:55:19.575347: step 16983, loss 0.00220112, acc 1
2016-09-06T12:55:20.404373: step 16984, loss 0.00275473, acc 1
2016-09-06T12:55:21.230158: step 16985, loss 0.00197891, acc 1
2016-09-06T12:55:22.060453: step 16986, loss 0.00211802, acc 1
2016-09-06T12:55:22.927789: step 16987, loss 0.100458, acc 0.96
2016-09-06T12:55:23.756691: step 16988, loss 0.00194953, acc 1
2016-09-06T12:55:24.557404: step 16989, loss 0.00356001, acc 1
2016-09-06T12:55:25.392500: step 16990, loss 0.00206209, acc 1
2016-09-06T12:55:26.208596: step 16991, loss 0.0146123, acc 1
2016-09-06T12:55:27.036181: step 16992, loss 0.00200582, acc 1
2016-09-06T12:55:27.881591: step 16993, loss 0.00496601, acc 1
2016-09-06T12:55:28.732803: step 16994, loss 0.0197955, acc 1
2016-09-06T12:55:29.531909: step 16995, loss 0.00175027, acc 1
2016-09-06T12:55:30.350817: step 16996, loss 0.0155128, acc 1
2016-09-06T12:55:31.174447: step 16997, loss 0.00179726, acc 1
2016-09-06T12:55:31.974161: step 16998, loss 0.0017356, acc 1
2016-09-06T12:55:32.805640: step 16999, loss 0.00173735, acc 1
2016-09-06T12:55:33.639458: step 17000, loss 0.00182047, acc 1

Evaluation:
2016-09-06T12:55:37.363716: step 17000, loss 2.66187, acc 0.74015

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-17000

2016-09-06T12:55:39.180143: step 17001, loss 0.0155866, acc 1
2016-09-06T12:55:40.008317: step 17002, loss 0.00171175, acc 1
2016-09-06T12:55:40.792186: step 17003, loss 0.0283689, acc 0.98
2016-09-06T12:55:41.577918: step 17004, loss 0.0191481, acc 0.98
2016-09-06T12:55:42.406218: step 17005, loss 0.0301996, acc 0.98
2016-09-06T12:55:43.227073: step 17006, loss 0.0271098, acc 1
2016-09-06T12:55:44.005244: step 17007, loss 0.0016454, acc 1
2016-09-06T12:55:44.818555: step 17008, loss 0.0374789, acc 0.98
2016-09-06T12:55:45.640336: step 17009, loss 0.00503452, acc 1
2016-09-06T12:55:46.464942: step 17010, loss 0.00161886, acc 1
2016-09-06T12:55:47.298950: step 17011, loss 0.0883613, acc 0.98
2016-09-06T12:55:48.141817: step 17012, loss 0.0102338, acc 1
2016-09-06T12:55:48.970123: step 17013, loss 0.00600563, acc 1
2016-09-06T12:55:49.796762: step 17014, loss 0.00152318, acc 1
2016-09-06T12:55:50.614194: step 17015, loss 0.0318331, acc 0.98
2016-09-06T12:55:51.412922: step 17016, loss 0.0103466, acc 1
2016-09-06T12:55:52.223686: step 17017, loss 0.00952732, acc 1
2016-09-06T12:55:53.043711: step 17018, loss 0.00172099, acc 1
2016-09-06T12:55:53.869379: step 17019, loss 0.0159116, acc 0.98
2016-09-06T12:55:54.709722: step 17020, loss 0.0391132, acc 0.98
2016-09-06T12:55:55.518833: step 17021, loss 0.0455809, acc 0.96
2016-09-06T12:55:56.317463: step 17022, loss 0.00574936, acc 1
2016-09-06T12:55:57.162680: step 17023, loss 0.0222119, acc 0.98
2016-09-06T12:55:58.007712: step 17024, loss 0.026335, acc 1
2016-09-06T12:55:58.854818: step 17025, loss 0.00222222, acc 1
2016-09-06T12:55:59.640873: step 17026, loss 0.0035874, acc 1
2016-09-06T12:56:00.504397: step 17027, loss 0.0373362, acc 0.98
2016-09-06T12:56:01.321878: step 17028, loss 0.0159907, acc 0.98
2016-09-06T12:56:02.145256: step 17029, loss 0.00879233, acc 1
2016-09-06T12:56:02.981685: step 17030, loss 0.0163546, acc 1
2016-09-06T12:56:03.770423: step 17031, loss 0.00849653, acc 1
2016-09-06T12:56:04.585733: step 17032, loss 0.0133932, acc 1
2016-09-06T12:56:05.416133: step 17033, loss 0.0380775, acc 0.98
2016-09-06T12:56:06.292698: step 17034, loss 0.039611, acc 0.98
2016-09-06T12:56:07.119296: step 17035, loss 0.00194431, acc 1
2016-09-06T12:56:07.936821: step 17036, loss 0.00154169, acc 1
2016-09-06T12:56:08.744023: step 17037, loss 0.0180872, acc 1
2016-09-06T12:56:09.541643: step 17038, loss 0.0135205, acc 1
2016-09-06T12:56:10.410161: step 17039, loss 0.0024982, acc 1
2016-09-06T12:56:11.228976: step 17040, loss 0.0299544, acc 0.98
2016-09-06T12:56:12.031507: step 17041, loss 0.00154514, acc 1
2016-09-06T12:56:12.860091: step 17042, loss 0.0246672, acc 1
2016-09-06T12:56:13.689675: step 17043, loss 0.0258663, acc 0.98
2016-09-06T12:56:14.450617: step 17044, loss 0.00161487, acc 1
2016-09-06T12:56:15.283493: step 17045, loss 0.010432, acc 1
2016-09-06T12:56:16.132195: step 17046, loss 0.0175541, acc 1
2016-09-06T12:56:16.944354: step 17047, loss 0.00159357, acc 1
2016-09-06T12:56:17.758068: step 17048, loss 0.00748536, acc 1
2016-09-06T12:56:18.603800: step 17049, loss 0.176261, acc 0.98
2016-09-06T12:56:19.363477: step 17050, loss 0.00539325, acc 1
2016-09-06T12:56:20.169863: step 17051, loss 0.00141512, acc 1
2016-09-06T12:56:20.979808: step 17052, loss 0.0315575, acc 0.98
2016-09-06T12:56:21.764489: step 17053, loss 0.0168329, acc 0.98
2016-09-06T12:56:22.551973: step 17054, loss 0.0054887, acc 1
2016-09-06T12:56:23.372116: step 17055, loss 0.00781721, acc 1
2016-09-06T12:56:24.174897: step 17056, loss 0.0125812, acc 1
2016-09-06T12:56:24.966349: step 17057, loss 0.0312325, acc 0.98
2016-09-06T12:56:25.780722: step 17058, loss 0.00870968, acc 1
2016-09-06T12:56:26.568453: step 17059, loss 0.0247216, acc 0.98
2016-09-06T12:56:27.388826: step 17060, loss 0.00207859, acc 1
2016-09-06T12:56:28.204780: step 17061, loss 0.00214175, acc 1
2016-09-06T12:56:28.999128: step 17062, loss 0.0851257, acc 0.96
2016-09-06T12:56:29.818189: step 17063, loss 0.00355234, acc 1
2016-09-06T12:56:30.631885: step 17064, loss 0.0243047, acc 0.98
2016-09-06T12:56:31.401235: step 17065, loss 0.0233116, acc 1
2016-09-06T12:56:32.229750: step 17066, loss 0.00169439, acc 1
2016-09-06T12:56:33.053779: step 17067, loss 0.0268152, acc 0.98
2016-09-06T12:56:33.875374: step 17068, loss 0.00191346, acc 1
2016-09-06T12:56:34.684848: step 17069, loss 0.00311443, acc 1
2016-09-06T12:56:35.497365: step 17070, loss 0.0178028, acc 0.98
2016-09-06T12:56:36.276724: step 17071, loss 0.0203287, acc 1
2016-09-06T12:56:37.062263: step 17072, loss 0.00311565, acc 1
2016-09-06T12:56:37.884637: step 17073, loss 0.0107445, acc 1
2016-09-06T12:56:38.675273: step 17074, loss 0.0441459, acc 0.98
2016-09-06T12:56:39.488196: step 17075, loss 0.0179255, acc 0.98
2016-09-06T12:56:40.328810: step 17076, loss 0.0266896, acc 1
2016-09-06T12:56:41.108361: step 17077, loss 0.00301612, acc 1
2016-09-06T12:56:41.911490: step 17078, loss 0.00296731, acc 1
2016-09-06T12:56:42.756530: step 17079, loss 0.00431934, acc 1
2016-09-06T12:56:43.521850: step 17080, loss 0.00200672, acc 1
2016-09-06T12:56:44.326864: step 17081, loss 0.0124117, acc 1
2016-09-06T12:56:45.154468: step 17082, loss 0.046589, acc 0.96
2016-09-06T12:56:45.941041: step 17083, loss 0.00489572, acc 1
2016-09-06T12:56:46.745026: step 17084, loss 0.00193631, acc 1
2016-09-06T12:56:47.576907: step 17085, loss 0.0143167, acc 1
2016-09-06T12:56:48.362815: step 17086, loss 0.00313455, acc 1
2016-09-06T12:56:49.165969: step 17087, loss 0.0257315, acc 0.98
2016-09-06T12:56:49.925249: step 17088, loss 0.0150665, acc 1
2016-09-06T12:56:50.741616: step 17089, loss 0.052767, acc 0.98
2016-09-06T12:56:51.550489: step 17090, loss 0.0780699, acc 0.94
2016-09-06T12:56:52.397741: step 17091, loss 0.00478014, acc 1
2016-09-06T12:56:53.197297: step 17092, loss 0.0117522, acc 1
2016-09-06T12:56:53.992718: step 17093, loss 0.029559, acc 1
2016-09-06T12:56:54.814284: step 17094, loss 0.0164765, acc 0.98
2016-09-06T12:56:55.611347: step 17095, loss 0.00313686, acc 1
2016-09-06T12:56:56.411027: step 17096, loss 0.00216968, acc 1
2016-09-06T12:56:57.216676: step 17097, loss 0.0108006, acc 1
2016-09-06T12:56:58.024548: step 17098, loss 0.0101856, acc 1
2016-09-06T12:56:58.867269: step 17099, loss 0.0016129, acc 1
2016-09-06T12:56:59.684060: step 17100, loss 0.00372138, acc 1

Evaluation:
2016-09-06T12:57:03.444299: step 17100, loss 2.49852, acc 0.729831

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-17100

2016-09-06T12:57:05.425475: step 17101, loss 0.00237764, acc 1
2016-09-06T12:57:06.224846: step 17102, loss 0.0217026, acc 0.98
2016-09-06T12:57:07.072349: step 17103, loss 0.0395735, acc 0.98
2016-09-06T12:57:07.877374: step 17104, loss 0.00165958, acc 1
2016-09-06T12:57:08.676282: step 17105, loss 0.0163034, acc 0.98
2016-09-06T12:57:09.506301: step 17106, loss 0.00176462, acc 1
2016-09-06T12:57:10.322333: step 17107, loss 0.00310336, acc 1
2016-09-06T12:57:11.169476: step 17108, loss 0.00215079, acc 1
2016-09-06T12:57:11.997335: step 17109, loss 0.00154668, acc 1
2016-09-06T12:57:12.802711: step 17110, loss 0.00156871, acc 1
2016-09-06T12:57:13.604964: step 17111, loss 0.00195298, acc 1
2016-09-06T12:57:14.404962: step 17112, loss 0.0319656, acc 0.96
2016-09-06T12:57:15.233880: step 17113, loss 0.0275949, acc 0.98
2016-09-06T12:57:16.064520: step 17114, loss 0.0170748, acc 0.98
2016-09-06T12:57:16.907882: step 17115, loss 0.00257873, acc 1
2016-09-06T12:57:17.724425: step 17116, loss 0.00655345, acc 1
2016-09-06T12:57:18.516649: step 17117, loss 0.0105946, acc 1
2016-09-06T12:57:19.338443: step 17118, loss 0.00706286, acc 1
2016-09-06T12:57:20.145205: step 17119, loss 0.00139597, acc 1
2016-09-06T12:57:20.926810: step 17120, loss 0.0188873, acc 1
2016-09-06T12:57:21.756599: step 17121, loss 0.00652666, acc 1
2016-09-06T12:57:22.560660: step 17122, loss 0.00570984, acc 1
2016-09-06T12:57:23.349489: step 17123, loss 0.00150191, acc 1
2016-09-06T12:57:24.178958: step 17124, loss 0.00163996, acc 1
2016-09-06T12:57:25.019843: step 17125, loss 0.0188943, acc 0.98
2016-09-06T12:57:25.831209: step 17126, loss 0.0195756, acc 0.98
2016-09-06T12:57:26.650380: step 17127, loss 0.00384691, acc 1
2016-09-06T12:57:27.456985: step 17128, loss 0.00144179, acc 1
2016-09-06T12:57:28.234158: step 17129, loss 0.00231573, acc 1
2016-09-06T12:57:29.034951: step 17130, loss 0.00159113, acc 1
2016-09-06T12:57:29.853861: step 17131, loss 0.0453308, acc 0.96
2016-09-06T12:57:30.653928: step 17132, loss 0.0104994, acc 1
2016-09-06T12:57:31.452604: step 17133, loss 0.00155561, acc 1
2016-09-06T12:57:32.250333: step 17134, loss 0.0019355, acc 1
2016-09-06T12:57:33.047798: step 17135, loss 0.0162175, acc 0.98
2016-09-06T12:57:33.869460: step 17136, loss 0.0161983, acc 0.98
2016-09-06T12:57:34.694711: step 17137, loss 0.00494129, acc 1
2016-09-06T12:57:35.505883: step 17138, loss 0.00279107, acc 1
2016-09-06T12:57:36.327823: step 17139, loss 0.0320905, acc 0.98
2016-09-06T12:57:37.168168: step 17140, loss 0.00808617, acc 1
2016-09-06T12:57:37.954182: step 17141, loss 0.0134734, acc 1
2016-09-06T12:57:38.777752: step 17142, loss 0.0306905, acc 0.98
2016-09-06T12:57:39.584866: step 17143, loss 0.00358644, acc 1
2016-09-06T12:57:40.368324: step 17144, loss 0.0016633, acc 1
2016-09-06T12:57:41.161307: step 17145, loss 0.0085129, acc 1
2016-09-06T12:57:42.006143: step 17146, loss 0.0205356, acc 1
2016-09-06T12:57:42.810209: step 17147, loss 0.0148689, acc 1
2016-09-06T12:57:43.616111: step 17148, loss 0.00175335, acc 1
2016-09-06T12:57:44.441761: step 17149, loss 0.0160746, acc 1
2016-09-06T12:57:45.266211: step 17150, loss 0.00171557, acc 1
2016-09-06T12:57:46.071730: step 17151, loss 0.00210695, acc 1
2016-09-06T12:57:46.909766: step 17152, loss 0.0439368, acc 0.98
2016-09-06T12:57:47.745052: step 17153, loss 0.00910863, acc 1
2016-09-06T12:57:48.563327: step 17154, loss 0.00230974, acc 1
2016-09-06T12:57:49.403216: step 17155, loss 0.00254726, acc 1
2016-09-06T12:57:50.224598: step 17156, loss 0.0165351, acc 1
2016-09-06T12:57:51.036396: step 17157, loss 0.00257403, acc 1
2016-09-06T12:57:51.878228: step 17158, loss 0.0019917, acc 1
2016-09-06T12:57:52.686570: step 17159, loss 0.0357331, acc 0.98
2016-09-06T12:57:53.490012: step 17160, loss 0.00847746, acc 1
2016-09-06T12:57:54.377751: step 17161, loss 0.144931, acc 0.96
2016-09-06T12:57:55.153645: step 17162, loss 0.00214218, acc 1
2016-09-06T12:57:55.969754: step 17163, loss 0.0290136, acc 0.98
2016-09-06T12:57:56.807704: step 17164, loss 0.0351038, acc 0.98
2016-09-06T12:57:57.672182: step 17165, loss 0.00311311, acc 1
2016-09-06T12:57:58.470457: step 17166, loss 0.0155048, acc 1
2016-09-06T12:57:59.295113: step 17167, loss 0.0344834, acc 0.98
2016-09-06T12:58:00.126845: step 17168, loss 0.0144901, acc 1
2016-09-06T12:58:00.981571: step 17169, loss 0.00184423, acc 1
2016-09-06T12:58:01.791845: step 17170, loss 0.0018273, acc 1
2016-09-06T12:58:02.611234: step 17171, loss 0.0129205, acc 1
2016-09-06T12:58:03.417048: step 17172, loss 0.00387431, acc 1
2016-09-06T12:58:04.234357: step 17173, loss 0.00288225, acc 1
2016-09-06T12:58:05.063866: step 17174, loss 0.00197305, acc 1
2016-09-06T12:58:05.895637: step 17175, loss 0.00553427, acc 1
2016-09-06T12:58:06.703330: step 17176, loss 0.00227322, acc 1
2016-09-06T12:58:07.535954: step 17177, loss 0.00327137, acc 1
2016-09-06T12:58:08.381720: step 17178, loss 0.00186617, acc 1
2016-09-06T12:58:09.191158: step 17179, loss 0.0192117, acc 0.98
2016-09-06T12:58:10.018253: step 17180, loss 0.00183098, acc 1
2016-09-06T12:58:10.831714: step 17181, loss 0.00208832, acc 1
2016-09-06T12:58:11.637933: step 17182, loss 0.00186035, acc 1
2016-09-06T12:58:12.462778: step 17183, loss 0.0126729, acc 1
2016-09-06T12:58:13.295647: step 17184, loss 0.00271104, acc 1
2016-09-06T12:58:14.129378: step 17185, loss 0.00233475, acc 1
2016-09-06T12:58:14.997981: step 17186, loss 0.0152711, acc 1
2016-09-06T12:58:15.846955: step 17187, loss 0.0491235, acc 0.98
2016-09-06T12:58:16.638753: step 17188, loss 0.0177331, acc 0.98
2016-09-06T12:58:17.431839: step 17189, loss 0.0147759, acc 1
2016-09-06T12:58:18.240120: step 17190, loss 0.00189258, acc 1
2016-09-06T12:58:19.009091: step 17191, loss 0.00799646, acc 1
2016-09-06T12:58:19.821249: step 17192, loss 0.00852427, acc 1
2016-09-06T12:58:20.632643: step 17193, loss 0.0156589, acc 1
2016-09-06T12:58:21.411421: step 17194, loss 0.00800744, acc 1
2016-09-06T12:58:22.210153: step 17195, loss 0.00388987, acc 1
2016-09-06T12:58:23.039567: step 17196, loss 0.0058805, acc 1
2016-09-06T12:58:23.874068: step 17197, loss 0.0260175, acc 1
2016-09-06T12:58:24.661621: step 17198, loss 0.00723103, acc 1
2016-09-06T12:58:25.507095: step 17199, loss 0.00174305, acc 1
2016-09-06T12:58:26.278380: step 17200, loss 0.00289501, acc 1

Evaluation:
2016-09-06T12:58:30.007192: step 17200, loss 2.6573, acc 0.739212

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-17200

2016-09-06T12:58:31.888249: step 17201, loss 0.023157, acc 0.98
2016-09-06T12:58:32.681456: step 17202, loss 0.0222638, acc 0.98
2016-09-06T12:58:33.479391: step 17203, loss 0.0032551, acc 1
2016-09-06T12:58:34.302697: step 17204, loss 0.0158876, acc 1
2016-09-06T12:58:35.109230: step 17205, loss 0.0126494, acc 1
2016-09-06T12:58:35.919050: step 17206, loss 0.00180668, acc 1
2016-09-06T12:58:36.737721: step 17207, loss 0.08802, acc 0.94
2016-09-06T12:58:37.539737: step 17208, loss 0.0126363, acc 1
2016-09-06T12:58:38.345395: step 17209, loss 0.001847, acc 1
2016-09-06T12:58:39.151137: step 17210, loss 0.0218975, acc 0.98
2016-09-06T12:58:39.981844: step 17211, loss 0.0181714, acc 0.98
2016-09-06T12:58:40.831720: step 17212, loss 0.0238776, acc 1
2016-09-06T12:58:41.637870: step 17213, loss 0.00214478, acc 1
2016-09-06T12:58:42.445871: step 17214, loss 0.00404835, acc 1
2016-09-06T12:58:43.229428: step 17215, loss 0.0676814, acc 0.98
2016-09-06T12:58:44.051926: step 17216, loss 0.0699395, acc 0.96
2016-09-06T12:58:44.864134: step 17217, loss 0.00361437, acc 1
2016-09-06T12:58:45.668369: step 17218, loss 0.00249984, acc 1
2016-09-06T12:58:46.484476: step 17219, loss 0.0253576, acc 0.98
2016-09-06T12:58:47.321663: step 17220, loss 0.00758775, acc 1
2016-09-06T12:58:48.134250: step 17221, loss 0.019227, acc 1
2016-09-06T12:58:48.965899: step 17222, loss 0.00252874, acc 1
2016-09-06T12:58:49.804116: step 17223, loss 0.00300261, acc 1
2016-09-06T12:58:50.612046: step 17224, loss 0.00253405, acc 1
2016-09-06T12:58:51.421508: step 17225, loss 0.0189793, acc 0.98
2016-09-06T12:58:52.250677: step 17226, loss 0.0176598, acc 1
2016-09-06T12:58:53.068360: step 17227, loss 0.00206273, acc 1
2016-09-06T12:58:53.905136: step 17228, loss 0.00311298, acc 1
2016-09-06T12:58:54.737540: step 17229, loss 0.0236738, acc 1
2016-09-06T12:58:55.557522: step 17230, loss 0.00286568, acc 1
2016-09-06T12:58:56.366008: step 17231, loss 0.00263079, acc 1
2016-09-06T12:58:57.185464: step 17232, loss 0.00944195, acc 1
2016-09-06T12:58:57.991425: step 17233, loss 0.00858878, acc 1
2016-09-06T12:58:58.789451: step 17234, loss 0.00229954, acc 1
2016-09-06T12:58:59.626603: step 17235, loss 0.00292088, acc 1
2016-09-06T12:59:00.450449: step 17236, loss 0.00224402, acc 1
2016-09-06T12:59:01.277424: step 17237, loss 0.00518184, acc 1
2016-09-06T12:59:02.115437: step 17238, loss 0.0145062, acc 1
2016-09-06T12:59:02.926601: step 17239, loss 0.00207603, acc 1
2016-09-06T12:59:03.736841: step 17240, loss 0.00210656, acc 1
2016-09-06T12:59:04.585970: step 17241, loss 0.00310981, acc 1
2016-09-06T12:59:05.405984: step 17242, loss 0.00253396, acc 1
2016-09-06T12:59:06.191730: step 17243, loss 0.0153252, acc 1
2016-09-06T12:59:07.001203: step 17244, loss 0.00322944, acc 1
2016-09-06T12:59:07.827886: step 17245, loss 0.00472615, acc 1
2016-09-06T12:59:08.647631: step 17246, loss 0.0861738, acc 0.98
2016-09-06T12:59:09.470406: step 17247, loss 0.0164272, acc 0.98
2016-09-06T12:59:10.327532: step 17248, loss 0.0369884, acc 0.98
2016-09-06T12:59:11.143384: step 17249, loss 0.0207643, acc 1
2016-09-06T12:59:11.984535: step 17250, loss 0.00442432, acc 1
2016-09-06T12:59:12.824617: step 17251, loss 0.00448754, acc 1
2016-09-06T12:59:13.627824: step 17252, loss 0.00196017, acc 1
2016-09-06T12:59:14.428424: step 17253, loss 0.00240628, acc 1
2016-09-06T12:59:15.287309: step 17254, loss 0.0120003, acc 1
2016-09-06T12:59:16.150817: step 17255, loss 0.00195568, acc 1
2016-09-06T12:59:16.987025: step 17256, loss 0.00447959, acc 1
2016-09-06T12:59:17.839288: step 17257, loss 0.0667788, acc 0.96
2016-09-06T12:59:18.656396: step 17258, loss 0.00732938, acc 1
2016-09-06T12:59:19.478131: step 17259, loss 0.0163546, acc 0.98
2016-09-06T12:59:20.257276: step 17260, loss 0.00259069, acc 1
2016-09-06T12:59:21.099504: step 17261, loss 0.00519819, acc 1
2016-09-06T12:59:21.884353: step 17262, loss 0.0230336, acc 0.98
2016-09-06T12:59:22.701641: step 17263, loss 0.00611006, acc 1
2016-09-06T12:59:23.491981: step 17264, loss 0.00198843, acc 1
2016-09-06T12:59:24.265922: step 17265, loss 0.00930498, acc 1
2016-09-06T12:59:25.067514: step 17266, loss 0.0181571, acc 0.98
2016-09-06T12:59:25.878963: step 17267, loss 0.0019499, acc 1
2016-09-06T12:59:26.647846: step 17268, loss 0.0484204, acc 0.96
2016-09-06T12:59:27.468427: step 17269, loss 0.0315073, acc 0.98
2016-09-06T12:59:28.288430: step 17270, loss 0.00599453, acc 1
2016-09-06T12:59:29.083105: step 17271, loss 0.00177874, acc 1
2016-09-06T12:59:29.887950: step 17272, loss 0.00198228, acc 1
2016-09-06T12:59:30.700678: step 17273, loss 0.026705, acc 0.98
2016-09-06T12:59:31.500301: step 17274, loss 0.015176, acc 1
2016-09-06T12:59:32.317505: step 17275, loss 0.0209701, acc 1
2016-09-06T12:59:33.145265: step 17276, loss 0.00421255, acc 1
2016-09-06T12:59:33.958132: step 17277, loss 0.0226271, acc 0.98
2016-09-06T12:59:34.772578: step 17278, loss 0.0920259, acc 0.96
2016-09-06T12:59:35.585645: step 17279, loss 0.0186501, acc 0.98
2016-09-06T12:59:36.326500: step 17280, loss 0.0121048, acc 1
2016-09-06T12:59:37.156082: step 17281, loss 0.011965, acc 1
2016-09-06T12:59:37.985350: step 17282, loss 0.00179471, acc 1
2016-09-06T12:59:38.790504: step 17283, loss 0.00161766, acc 1
2016-09-06T12:59:39.609483: step 17284, loss 0.00609882, acc 1
2016-09-06T12:59:40.437372: step 17285, loss 0.0362817, acc 0.98
2016-09-06T12:59:41.230307: step 17286, loss 0.0243716, acc 1
2016-09-06T12:59:42.035933: step 17287, loss 0.0103482, acc 1
2016-09-06T12:59:42.845334: step 17288, loss 0.00448507, acc 1
2016-09-06T12:59:43.622187: step 17289, loss 0.0252061, acc 1
2016-09-06T12:59:44.417430: step 17290, loss 0.0121658, acc 1
2016-09-06T12:59:45.224464: step 17291, loss 0.00970556, acc 1
2016-09-06T12:59:46.015898: step 17292, loss 0.00174552, acc 1
2016-09-06T12:59:46.813205: step 17293, loss 0.0480163, acc 0.96
2016-09-06T12:59:47.614614: step 17294, loss 0.0188874, acc 0.98
2016-09-06T12:59:48.406537: step 17295, loss 0.0391904, acc 0.96
2016-09-06T12:59:49.240737: step 17296, loss 0.00190111, acc 1
2016-09-06T12:59:50.042268: step 17297, loss 0.00284656, acc 1
2016-09-06T12:59:50.838436: step 17298, loss 0.00216103, acc 1
2016-09-06T12:59:51.654585: step 17299, loss 0.0165319, acc 0.98
2016-09-06T12:59:52.463505: step 17300, loss 0.00187048, acc 1

Evaluation:
2016-09-06T12:59:56.172694: step 17300, loss 2.66956, acc 0.739212

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-17300

2016-09-06T12:59:58.002077: step 17301, loss 0.00187705, acc 1
2016-09-06T12:59:58.818010: step 17302, loss 0.00219783, acc 1
2016-09-06T12:59:59.640373: step 17303, loss 0.11928, acc 0.96
2016-09-06T13:00:00.484119: step 17304, loss 0.0182192, acc 0.98
2016-09-06T13:00:01.316976: step 17305, loss 0.0221496, acc 0.98
2016-09-06T13:00:02.127531: step 17306, loss 0.0051347, acc 1
2016-09-06T13:00:02.938765: step 17307, loss 0.0168843, acc 0.98
2016-09-06T13:00:03.786124: step 17308, loss 0.00655186, acc 1
2016-09-06T13:00:04.615550: step 17309, loss 0.00239449, acc 1
2016-09-06T13:00:05.425386: step 17310, loss 0.00419448, acc 1
2016-09-06T13:00:06.266188: step 17311, loss 0.00237628, acc 1
2016-09-06T13:00:07.083875: step 17312, loss 0.019992, acc 0.98
2016-09-06T13:00:07.893688: step 17313, loss 0.00470284, acc 1
2016-09-06T13:00:08.701647: step 17314, loss 0.00382975, acc 1
2016-09-06T13:00:09.509831: step 17315, loss 0.00160663, acc 1
2016-09-06T13:00:10.303079: step 17316, loss 0.00161947, acc 1
2016-09-06T13:00:11.096827: step 17317, loss 0.0146937, acc 1
2016-09-06T13:00:11.915301: step 17318, loss 0.0252651, acc 1
2016-09-06T13:00:12.700380: step 17319, loss 0.00165312, acc 1
2016-09-06T13:00:13.543996: step 17320, loss 0.00182503, acc 1
2016-09-06T13:00:14.365218: step 17321, loss 0.00228868, acc 1
2016-09-06T13:00:15.180186: step 17322, loss 0.00190942, acc 1
2016-09-06T13:00:15.991055: step 17323, loss 0.0158231, acc 0.98
2016-09-06T13:00:16.801148: step 17324, loss 0.0016397, acc 1
2016-09-06T13:00:17.594075: step 17325, loss 0.00835713, acc 1
2016-09-06T13:00:18.425706: step 17326, loss 0.0431409, acc 0.98
2016-09-06T13:00:19.244630: step 17327, loss 0.0058428, acc 1
2016-09-06T13:00:20.068636: step 17328, loss 0.00175775, acc 1
2016-09-06T13:00:20.905421: step 17329, loss 0.00158743, acc 1
2016-09-06T13:00:21.740218: step 17330, loss 0.00162406, acc 1
2016-09-06T13:00:22.525866: step 17331, loss 0.0866179, acc 0.96
2016-09-06T13:00:23.327710: step 17332, loss 0.0310446, acc 0.98
2016-09-06T13:00:24.146089: step 17333, loss 0.0240182, acc 0.98
2016-09-06T13:00:24.968995: step 17334, loss 0.075031, acc 0.98
2016-09-06T13:00:25.787719: step 17335, loss 0.00170066, acc 1
2016-09-06T13:00:26.624249: step 17336, loss 0.00220458, acc 1
2016-09-06T13:00:27.450316: step 17337, loss 0.0175252, acc 1
2016-09-06T13:00:28.287369: step 17338, loss 0.0230812, acc 1
2016-09-06T13:00:29.148165: step 17339, loss 0.0016612, acc 1
2016-09-06T13:00:30.001512: step 17340, loss 0.00377687, acc 1
2016-09-06T13:00:30.840027: step 17341, loss 0.014283, acc 1
2016-09-06T13:00:31.662287: step 17342, loss 0.00181701, acc 1
2016-09-06T13:00:32.459146: step 17343, loss 0.0016641, acc 1
2016-09-06T13:00:33.267540: step 17344, loss 0.0728441, acc 0.96
2016-09-06T13:00:34.099969: step 17345, loss 0.010869, acc 1
2016-09-06T13:00:34.926406: step 17346, loss 0.0370864, acc 1
2016-09-06T13:00:35.702656: step 17347, loss 0.0164004, acc 1
2016-09-06T13:00:36.501421: step 17348, loss 0.0799177, acc 0.96
2016-09-06T13:00:37.290420: step 17349, loss 0.0154355, acc 1
2016-09-06T13:00:38.097089: step 17350, loss 0.0377513, acc 0.98
2016-09-06T13:00:38.924472: step 17351, loss 0.00408689, acc 1
2016-09-06T13:00:39.761424: step 17352, loss 0.0129454, acc 1
2016-09-06T13:00:40.586527: step 17353, loss 0.0213076, acc 1
2016-09-06T13:00:41.384320: step 17354, loss 0.00533827, acc 1
2016-09-06T13:00:42.222603: step 17355, loss 0.0162182, acc 1
2016-09-06T13:00:43.016361: step 17356, loss 0.0213311, acc 1
2016-09-06T13:00:43.822230: step 17357, loss 0.00187391, acc 1
2016-09-06T13:00:44.628275: step 17358, loss 0.00285696, acc 1
2016-09-06T13:00:45.392480: step 17359, loss 0.00330796, acc 1
2016-09-06T13:00:46.201259: step 17360, loss 0.00200142, acc 1
2016-09-06T13:00:47.029700: step 17361, loss 0.0082263, acc 1
2016-09-06T13:00:47.826781: step 17362, loss 0.026416, acc 0.98
2016-09-06T13:00:48.673836: step 17363, loss 0.0224721, acc 0.98
2016-09-06T13:00:49.508858: step 17364, loss 0.00258693, acc 1
2016-09-06T13:00:50.307436: step 17365, loss 0.0021378, acc 1
2016-09-06T13:00:51.113518: step 17366, loss 0.0405252, acc 0.98
2016-09-06T13:00:51.950979: step 17367, loss 0.00206916, acc 1
2016-09-06T13:00:52.785224: step 17368, loss 0.0038916, acc 1
2016-09-06T13:00:53.577019: step 17369, loss 0.0573756, acc 0.98
2016-09-06T13:00:54.414489: step 17370, loss 0.00209214, acc 1
2016-09-06T13:00:55.209430: step 17371, loss 0.00254072, acc 1
2016-09-06T13:00:56.008725: step 17372, loss 0.00203621, acc 1
2016-09-06T13:00:56.828459: step 17373, loss 0.00322021, acc 1
2016-09-06T13:00:57.646953: step 17374, loss 0.00244681, acc 1
2016-09-06T13:00:58.433460: step 17375, loss 0.00419649, acc 1
2016-09-06T13:00:59.273283: step 17376, loss 0.00454083, acc 1
2016-09-06T13:01:00.090877: step 17377, loss 0.00196027, acc 1
2016-09-06T13:01:00.922525: step 17378, loss 0.0149975, acc 1
2016-09-06T13:01:01.762033: step 17379, loss 0.0261401, acc 0.98
2016-09-06T13:01:02.568357: step 17380, loss 0.00848546, acc 1
2016-09-06T13:01:03.379551: step 17381, loss 0.00218456, acc 1
2016-09-06T13:01:04.212980: step 17382, loss 0.0138706, acc 1
2016-09-06T13:01:05.039847: step 17383, loss 0.00201127, acc 1
2016-09-06T13:01:05.864533: step 17384, loss 0.0254549, acc 0.98
2016-09-06T13:01:06.697739: step 17385, loss 0.0556656, acc 0.98
2016-09-06T13:01:07.503234: step 17386, loss 0.00367763, acc 1
2016-09-06T13:01:08.315269: step 17387, loss 0.131505, acc 0.96
2016-09-06T13:01:09.130621: step 17388, loss 0.0125096, acc 1
2016-09-06T13:01:09.967241: step 17389, loss 0.0283661, acc 0.98
2016-09-06T13:01:10.756856: step 17390, loss 0.0731059, acc 0.96
2016-09-06T13:01:11.599631: step 17391, loss 0.0103776, acc 1
2016-09-06T13:01:12.426914: step 17392, loss 0.0316453, acc 1
2016-09-06T13:01:13.248645: step 17393, loss 0.0112523, acc 1
2016-09-06T13:01:14.045726: step 17394, loss 0.0175056, acc 1
2016-09-06T13:01:14.865904: step 17395, loss 0.0322579, acc 0.98
2016-09-06T13:01:15.663674: step 17396, loss 0.00256181, acc 1
2016-09-06T13:01:16.487794: step 17397, loss 0.00681647, acc 1
2016-09-06T13:01:17.324540: step 17398, loss 0.00329809, acc 1
2016-09-06T13:01:18.144140: step 17399, loss 0.00620269, acc 1
2016-09-06T13:01:18.940877: step 17400, loss 0.00302131, acc 1

Evaluation:
2016-09-06T13:01:22.731501: step 17400, loss 3.90476, acc 0.730769

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-17400

2016-09-06T13:01:24.733582: step 17401, loss 0.0029611, acc 1
2016-09-06T13:01:25.609455: step 17402, loss 0.0029193, acc 1
2016-09-06T13:01:26.453843: step 17403, loss 0.00317218, acc 1
2016-09-06T13:01:27.379150: step 17404, loss 0.0264596, acc 0.98
2016-09-06T13:01:28.384446: step 17405, loss 0.0223523, acc 0.98
2016-09-06T13:01:29.207319: step 17406, loss 0.00698208, acc 1
2016-09-06T13:01:30.149421: step 17407, loss 0.0239532, acc 1
2016-09-06T13:01:31.060637: step 17408, loss 0.00503199, acc 1
2016-09-06T13:01:31.878170: step 17409, loss 0.00408615, acc 1
2016-09-06T13:01:32.677514: step 17410, loss 0.0119587, acc 1
2016-09-06T13:01:33.507899: step 17411, loss 0.00382956, acc 1
2016-09-06T13:01:34.537145: step 17412, loss 0.00339777, acc 1
2016-09-06T13:01:35.454766: step 17413, loss 0.0335913, acc 0.98
2016-09-06T13:01:36.405902: step 17414, loss 0.0538415, acc 0.96
2016-09-06T13:01:37.219802: step 17415, loss 0.111087, acc 0.98
2016-09-06T13:01:38.073479: step 17416, loss 0.0418332, acc 0.98
2016-09-06T13:01:38.908227: step 17417, loss 0.0190187, acc 0.98
2016-09-06T13:01:39.732754: step 17418, loss 0.00348597, acc 1
2016-09-06T13:01:40.564972: step 17419, loss 0.00309935, acc 1
2016-09-06T13:01:41.378291: step 17420, loss 0.00306455, acc 1
2016-09-06T13:01:42.204458: step 17421, loss 0.0114469, acc 1
2016-09-06T13:01:43.066363: step 17422, loss 0.00300827, acc 1
2016-09-06T13:01:43.898476: step 17423, loss 0.0211339, acc 0.98
2016-09-06T13:01:44.819551: step 17424, loss 0.00630219, acc 1
2016-09-06T13:01:45.647996: step 17425, loss 0.00390044, acc 1
2016-09-06T13:01:46.603157: step 17426, loss 0.00302508, acc 1
2016-09-06T13:01:47.411081: step 17427, loss 0.0353439, acc 0.98
2016-09-06T13:01:48.219433: step 17428, loss 0.00330475, acc 1
2016-09-06T13:01:49.068131: step 17429, loss 0.00289632, acc 1
2016-09-06T13:01:49.970129: step 17430, loss 0.124089, acc 0.98
2016-09-06T13:01:50.805501: step 17431, loss 0.00333166, acc 1
2016-09-06T13:01:51.611146: step 17432, loss 0.0679391, acc 0.98
2016-09-06T13:01:52.482960: step 17433, loss 0.0206395, acc 0.98
2016-09-06T13:01:53.274526: step 17434, loss 0.0249027, acc 1
2016-09-06T13:01:54.113119: step 17435, loss 0.00618532, acc 1
2016-09-06T13:01:54.950323: step 17436, loss 0.0229095, acc 1
2016-09-06T13:01:55.791456: step 17437, loss 0.00677107, acc 1
2016-09-06T13:01:56.632648: step 17438, loss 0.0112403, acc 1
2016-09-06T13:01:57.476092: step 17439, loss 0.115972, acc 0.98
2016-09-06T13:01:58.298557: step 17440, loss 0.0381043, acc 0.98
2016-09-06T13:01:59.094513: step 17441, loss 0.0304543, acc 0.98
2016-09-06T13:01:59.928552: step 17442, loss 0.019869, acc 0.98
2016-09-06T13:02:00.772607: step 17443, loss 0.0180295, acc 1
2016-09-06T13:02:01.553532: step 17444, loss 0.00360346, acc 1
2016-09-06T13:02:02.406137: step 17445, loss 0.00301642, acc 1
2016-09-06T13:02:03.204804: step 17446, loss 0.0225483, acc 0.98
2016-09-06T13:02:04.002647: step 17447, loss 0.0211825, acc 0.98
2016-09-06T13:02:04.809129: step 17448, loss 0.00593046, acc 1
2016-09-06T13:02:05.608093: step 17449, loss 0.0458972, acc 0.96
2016-09-06T13:02:06.413075: step 17450, loss 0.0141663, acc 1
2016-09-06T13:02:07.241765: step 17451, loss 0.0188609, acc 1
2016-09-06T13:02:08.095913: step 17452, loss 0.00990091, acc 1
2016-09-06T13:02:08.880871: step 17453, loss 0.00392993, acc 1
2016-09-06T13:02:09.679527: step 17454, loss 0.00606461, acc 1
2016-09-06T13:02:10.499438: step 17455, loss 0.0554957, acc 0.96
2016-09-06T13:02:11.508621: step 17456, loss 0.00348675, acc 1
2016-09-06T13:02:12.372988: step 17457, loss 0.035878, acc 1
2016-09-06T13:02:13.248255: step 17458, loss 0.0118298, acc 1
2016-09-06T13:02:14.209340: step 17459, loss 0.0160112, acc 1
2016-09-06T13:02:15.028551: step 17460, loss 0.00632961, acc 1
2016-09-06T13:02:15.906438: step 17461, loss 0.0277149, acc 0.98
2016-09-06T13:02:16.815241: step 17462, loss 0.00370234, acc 1
2016-09-06T13:02:17.691257: step 17463, loss 0.00397936, acc 1
2016-09-06T13:02:18.505062: step 17464, loss 0.00413323, acc 1
2016-09-06T13:02:19.333235: step 17465, loss 0.00680937, acc 1
2016-09-06T13:02:20.258207: step 17466, loss 0.0321734, acc 0.98
2016-09-06T13:02:21.121038: step 17467, loss 0.00381558, acc 1
2016-09-06T13:02:21.997098: step 17468, loss 0.00382755, acc 1
2016-09-06T13:02:22.841597: step 17469, loss 0.040759, acc 0.98
2016-09-06T13:02:23.633475: step 17470, loss 0.0210382, acc 0.98
2016-09-06T13:02:24.453824: step 17471, loss 0.00409487, acc 1
2016-09-06T13:02:25.209180: step 17472, loss 0.121665, acc 0.977273
2016-09-06T13:02:26.068490: step 17473, loss 0.0200393, acc 0.98
2016-09-06T13:02:26.931463: step 17474, loss 0.0490381, acc 0.96
2016-09-06T13:02:27.785826: step 17475, loss 0.00369584, acc 1
2016-09-06T13:02:28.619624: step 17476, loss 0.00374822, acc 1
2016-09-06T13:02:29.411818: step 17477, loss 0.016187, acc 1
2016-09-06T13:02:30.232924: step 17478, loss 0.00356293, acc 1
2016-09-06T13:02:31.105800: step 17479, loss 0.00352363, acc 1
2016-09-06T13:02:31.926314: step 17480, loss 0.00831908, acc 1
2016-09-06T13:02:32.730528: step 17481, loss 0.0150774, acc 1
2016-09-06T13:02:33.571054: step 17482, loss 0.00536999, acc 1
2016-09-06T13:02:34.393354: step 17483, loss 0.00342845, acc 1
2016-09-06T13:02:35.263726: step 17484, loss 0.00341176, acc 1
2016-09-06T13:02:36.083894: step 17485, loss 0.036012, acc 0.96
2016-09-06T13:02:36.889209: step 17486, loss 0.0193012, acc 0.98
2016-09-06T13:02:37.711081: step 17487, loss 0.148115, acc 0.96
2016-09-06T13:02:38.614694: step 17488, loss 0.00324945, acc 1
2016-09-06T13:02:39.408263: step 17489, loss 0.00337369, acc 1
2016-09-06T13:02:40.232433: step 17490, loss 0.00313498, acc 1
2016-09-06T13:02:41.095623: step 17491, loss 0.00316038, acc 1
2016-09-06T13:02:41.904186: step 17492, loss 0.00304516, acc 1
2016-09-06T13:02:42.683227: step 17493, loss 0.00291435, acc 1
2016-09-06T13:02:43.508275: step 17494, loss 0.0164542, acc 1
2016-09-06T13:02:44.318186: step 17495, loss 0.00287367, acc 1
2016-09-06T13:02:45.104899: step 17496, loss 0.0163018, acc 1
2016-09-06T13:02:45.936271: step 17497, loss 0.00293262, acc 1
2016-09-06T13:02:46.770242: step 17498, loss 0.026643, acc 0.98
2016-09-06T13:02:47.549403: step 17499, loss 0.00829712, acc 1
2016-09-06T13:02:48.349660: step 17500, loss 0.00323807, acc 1

Evaluation:
2016-09-06T13:02:52.089745: step 17500, loss 2.91533, acc 0.726079

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-17500

2016-09-06T13:02:54.082219: step 17501, loss 0.0200876, acc 0.98
2016-09-06T13:02:54.890044: step 17502, loss 0.0175975, acc 1
2016-09-06T13:02:55.694723: step 17503, loss 0.00261505, acc 1
2016-09-06T13:02:56.515630: step 17504, loss 0.0136192, acc 1
2016-09-06T13:02:57.297724: step 17505, loss 0.00248348, acc 1
2016-09-06T13:02:58.070614: step 17506, loss 0.0235102, acc 0.98
2016-09-06T13:02:58.876791: step 17507, loss 0.0035999, acc 1
2016-09-06T13:02:59.655718: step 17508, loss 0.00263481, acc 1
2016-09-06T13:03:00.526253: step 17509, loss 0.0159145, acc 1
2016-09-06T13:03:01.326404: step 17510, loss 0.067044, acc 0.98
2016-09-06T13:03:02.116229: step 17511, loss 0.00254417, acc 1
2016-09-06T13:03:02.956034: step 17512, loss 0.00364463, acc 1
2016-09-06T13:03:03.777688: step 17513, loss 0.116777, acc 0.98
2016-09-06T13:03:04.573972: step 17514, loss 0.00408332, acc 1
2016-09-06T13:03:05.371468: step 17515, loss 0.0512416, acc 0.98
2016-09-06T13:03:06.258594: step 17516, loss 0.0627168, acc 0.98
2016-09-06T13:03:07.081859: step 17517, loss 0.00194759, acc 1
2016-09-06T13:03:07.879787: step 17518, loss 0.00890155, acc 1
2016-09-06T13:03:08.694966: step 17519, loss 0.0217563, acc 0.98
2016-09-06T13:03:09.502110: step 17520, loss 0.00554348, acc 1
2016-09-06T13:03:10.321477: step 17521, loss 0.0346436, acc 1
2016-09-06T13:03:11.146511: step 17522, loss 0.00966956, acc 1
2016-09-06T13:03:11.943438: step 17523, loss 0.021497, acc 0.98
2016-09-06T13:03:12.749841: step 17524, loss 0.0664947, acc 0.96
2016-09-06T13:03:13.693516: step 17525, loss 0.0185102, acc 1
2016-09-06T13:03:14.518622: step 17526, loss 0.0164336, acc 1
2016-09-06T13:03:15.322403: step 17527, loss 0.0116786, acc 1
2016-09-06T13:03:16.127946: step 17528, loss 0.0152945, acc 1
2016-09-06T13:03:16.936236: step 17529, loss 0.0032256, acc 1
2016-09-06T13:03:17.732116: step 17530, loss 0.015233, acc 1
2016-09-06T13:03:18.570934: step 17531, loss 0.00334277, acc 1
2016-09-06T13:03:19.427660: step 17532, loss 0.00349366, acc 1
2016-09-06T13:03:20.286845: step 17533, loss 0.0116625, acc 1
2016-09-06T13:03:21.086380: step 17534, loss 0.0407631, acc 0.98
2016-09-06T13:03:21.899442: step 17535, loss 0.0044095, acc 1
2016-09-06T13:03:22.689181: step 17536, loss 0.00378316, acc 1
2016-09-06T13:03:23.516144: step 17537, loss 0.00374041, acc 1
2016-09-06T13:03:24.409876: step 17538, loss 0.0408031, acc 0.98
2016-09-06T13:03:25.253165: step 17539, loss 0.013761, acc 1
2016-09-06T13:03:26.072752: step 17540, loss 0.0190898, acc 1
2016-09-06T13:03:27.014964: step 17541, loss 0.00921744, acc 1
2016-09-06T13:03:27.836711: step 17542, loss 0.00384778, acc 1
2016-09-06T13:03:28.653401: step 17543, loss 0.00385456, acc 1
2016-09-06T13:03:29.508993: step 17544, loss 0.00431588, acc 1
2016-09-06T13:03:30.329415: step 17545, loss 0.00385704, acc 1
2016-09-06T13:03:31.129396: step 17546, loss 0.00385202, acc 1
2016-09-06T13:03:32.022544: step 17547, loss 0.0268974, acc 0.98
2016-09-06T13:03:32.860419: step 17548, loss 0.0310962, acc 0.98
2016-09-06T13:03:33.697402: step 17549, loss 0.00380986, acc 1
2016-09-06T13:03:34.511538: step 17550, loss 0.0226222, acc 0.98
2016-09-06T13:03:35.365445: step 17551, loss 0.0156516, acc 1
2016-09-06T13:03:36.256923: step 17552, loss 0.0243489, acc 0.98
2016-09-06T13:03:37.086763: step 17553, loss 0.00370488, acc 1
2016-09-06T13:03:37.947389: step 17554, loss 0.0206117, acc 0.98
2016-09-06T13:03:38.774597: step 17555, loss 0.015222, acc 1
2016-09-06T13:03:39.589446: step 17556, loss 0.00357253, acc 1
2016-09-06T13:03:40.410607: step 17557, loss 0.0170209, acc 1
2016-09-06T13:03:41.297853: step 17558, loss 0.00691674, acc 1
2016-09-06T13:03:42.130373: step 17559, loss 0.00346615, acc 1
2016-09-06T13:03:42.950316: step 17560, loss 0.0140647, acc 1
2016-09-06T13:03:43.849340: step 17561, loss 0.00340529, acc 1
2016-09-06T13:03:44.676363: step 17562, loss 0.00336219, acc 1
2016-09-06T13:03:45.475595: step 17563, loss 0.0565679, acc 0.98
2016-09-06T13:03:46.296353: step 17564, loss 0.00325724, acc 1
2016-09-06T13:03:47.177669: step 17565, loss 0.0517505, acc 0.98
2016-09-06T13:03:47.983717: step 17566, loss 0.00311678, acc 1
2016-09-06T13:03:48.811011: step 17567, loss 0.00304298, acc 1
2016-09-06T13:03:49.651272: step 17568, loss 0.00300532, acc 1
2016-09-06T13:03:50.480222: step 17569, loss 0.00292505, acc 1
2016-09-06T13:03:51.295158: step 17570, loss 0.105544, acc 0.98
2016-09-06T13:03:52.122400: step 17571, loss 0.00398962, acc 1
2016-09-06T13:03:52.938926: step 17572, loss 0.0134653, acc 1
2016-09-06T13:03:53.740301: step 17573, loss 0.00265186, acc 1
2016-09-06T13:03:54.533690: step 17574, loss 0.00809583, acc 1
2016-09-06T13:03:55.330831: step 17575, loss 0.00259612, acc 1
2016-09-06T13:03:56.158450: step 17576, loss 0.0026659, acc 1
2016-09-06T13:03:56.971011: step 17577, loss 0.0254672, acc 0.98
2016-09-06T13:03:57.753593: step 17578, loss 0.00266889, acc 1
2016-09-06T13:03:58.657395: step 17579, loss 0.017473, acc 1
2016-09-06T13:03:59.469287: step 17580, loss 0.016888, acc 0.98
2016-09-06T13:04:00.263795: step 17581, loss 0.0267923, acc 1
2016-09-06T13:04:01.065283: step 17582, loss 0.0989724, acc 0.98
2016-09-06T13:04:01.871325: step 17583, loss 0.00223628, acc 1
2016-09-06T13:04:02.665404: step 17584, loss 0.0353778, acc 1
2016-09-06T13:04:03.447694: step 17585, loss 0.0119704, acc 1
2016-09-06T13:04:04.253562: step 17586, loss 0.0185325, acc 0.98
2016-09-06T13:04:05.059394: step 17587, loss 0.0604446, acc 0.96
2016-09-06T13:04:05.895723: step 17588, loss 0.00286919, acc 1
2016-09-06T13:04:06.721747: step 17589, loss 0.0170206, acc 1
2016-09-06T13:04:07.517558: step 17590, loss 0.00804838, acc 1
2016-09-06T13:04:08.318138: step 17591, loss 0.0354965, acc 0.96
2016-09-06T13:04:09.177196: step 17592, loss 0.118184, acc 0.98
2016-09-06T13:04:10.002669: step 17593, loss 0.00378493, acc 1
2016-09-06T13:04:10.800004: step 17594, loss 0.0213695, acc 0.98
2016-09-06T13:04:11.655104: step 17595, loss 0.0487057, acc 0.98
2016-09-06T13:04:12.521241: step 17596, loss 0.00685809, acc 1
2016-09-06T13:04:13.337859: step 17597, loss 0.025583, acc 0.98
2016-09-06T13:04:14.131705: step 17598, loss 0.00449134, acc 1
2016-09-06T13:04:14.950420: step 17599, loss 0.0172717, acc 0.98
2016-09-06T13:04:15.763734: step 17600, loss 0.0697633, acc 0.98

Evaluation:
2016-09-06T13:04:19.501301: step 17600, loss 2.00379, acc 0.736398

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-17600

2016-09-06T13:04:21.351745: step 17601, loss 0.0285013, acc 0.98
2016-09-06T13:04:22.148233: step 17602, loss 0.0177328, acc 0.98
2016-09-06T13:04:22.934525: step 17603, loss 0.0210702, acc 1
2016-09-06T13:04:23.741686: step 17604, loss 0.00299877, acc 1
2016-09-06T13:04:24.513946: step 17605, loss 0.023188, acc 0.98
2016-09-06T13:04:25.349203: step 17606, loss 0.00741717, acc 1
2016-09-06T13:04:26.166214: step 17607, loss 0.0546789, acc 0.98
2016-09-06T13:04:26.938015: step 17608, loss 0.00305719, acc 1
2016-09-06T13:04:27.727565: step 17609, loss 0.0250461, acc 1
2016-09-06T13:04:28.535694: step 17610, loss 0.0318084, acc 1
2016-09-06T13:04:29.327231: step 17611, loss 0.00273164, acc 1
2016-09-06T13:04:30.136474: step 17612, loss 0.0150847, acc 1
2016-09-06T13:04:30.980951: step 17613, loss 0.0404194, acc 0.98
2016-09-06T13:04:31.796335: step 17614, loss 0.00428503, acc 1
2016-09-06T13:04:32.612945: step 17615, loss 0.00282548, acc 1
2016-09-06T13:04:33.426366: step 17616, loss 0.0213337, acc 1
2016-09-06T13:04:34.193596: step 17617, loss 0.018321, acc 1
2016-09-06T13:04:34.994229: step 17618, loss 0.0172403, acc 1
2016-09-06T13:04:35.851934: step 17619, loss 0.010319, acc 1
2016-09-06T13:04:36.662973: step 17620, loss 0.00336696, acc 1
2016-09-06T13:04:37.466537: step 17621, loss 0.014555, acc 1
2016-09-06T13:04:38.273351: step 17622, loss 0.0181003, acc 0.98
2016-09-06T13:04:39.086339: step 17623, loss 0.00252393, acc 1
2016-09-06T13:04:39.893845: step 17624, loss 0.0025505, acc 1
2016-09-06T13:04:40.713138: step 17625, loss 0.0235078, acc 0.98
2016-09-06T13:04:41.498855: step 17626, loss 0.00248524, acc 1
2016-09-06T13:04:42.302656: step 17627, loss 0.00314185, acc 1
2016-09-06T13:04:43.110037: step 17628, loss 0.02097, acc 1
2016-09-06T13:04:43.928680: step 17629, loss 0.0065182, acc 1
2016-09-06T13:04:44.759194: step 17630, loss 0.00270186, acc 1
2016-09-06T13:04:45.536707: step 17631, loss 0.00258356, acc 1
2016-09-06T13:04:46.320048: step 17632, loss 0.0250911, acc 1
2016-09-06T13:04:47.111047: step 17633, loss 0.0250403, acc 1
2016-09-06T13:04:47.929606: step 17634, loss 0.0104437, acc 1
2016-09-06T13:04:48.729573: step 17635, loss 0.0129847, acc 1
2016-09-06T13:04:49.520030: step 17636, loss 0.00677433, acc 1
2016-09-06T13:04:50.327968: step 17637, loss 0.00252701, acc 1
2016-09-06T13:04:51.109588: step 17638, loss 0.0162228, acc 1
2016-09-06T13:04:51.951800: step 17639, loss 0.00260747, acc 1
2016-09-06T13:04:52.781107: step 17640, loss 0.00277538, acc 1
2016-09-06T13:04:53.590917: step 17641, loss 0.0208677, acc 1
2016-09-06T13:04:54.400808: step 17642, loss 0.00457946, acc 1
2016-09-06T13:04:55.243702: step 17643, loss 0.00325698, acc 1
2016-09-06T13:04:56.057675: step 17644, loss 0.00808394, acc 1
2016-09-06T13:04:56.840442: step 17645, loss 0.00287833, acc 1
2016-09-06T13:04:57.677439: step 17646, loss 0.0348977, acc 1
2016-09-06T13:04:58.497149: step 17647, loss 0.0118737, acc 1
2016-09-06T13:04:59.305105: step 17648, loss 0.00278489, acc 1
2016-09-06T13:05:00.124215: step 17649, loss 0.0599112, acc 0.98
2016-09-06T13:05:00.964140: step 17650, loss 0.0228018, acc 1
2016-09-06T13:05:01.769196: step 17651, loss 0.0183597, acc 0.98
2016-09-06T13:05:02.634466: step 17652, loss 0.0996392, acc 0.98
2016-09-06T13:05:03.462200: step 17653, loss 0.0654146, acc 0.98
2016-09-06T13:05:04.270887: step 17654, loss 0.00984046, acc 1
2016-09-06T13:05:05.110442: step 17655, loss 0.00233657, acc 1
2016-09-06T13:05:05.955159: step 17656, loss 0.0533769, acc 0.98
2016-09-06T13:05:06.778004: step 17657, loss 0.0171977, acc 0.98
2016-09-06T13:05:07.606502: step 17658, loss 0.0386574, acc 0.96
2016-09-06T13:05:08.454098: step 17659, loss 0.0143181, acc 1
2016-09-06T13:05:09.259212: step 17660, loss 0.00812402, acc 1
2016-09-06T13:05:10.053328: step 17661, loss 0.00399255, acc 1
2016-09-06T13:05:10.883944: step 17662, loss 0.0199924, acc 0.98
2016-09-06T13:05:11.673441: step 17663, loss 0.0092527, acc 1
2016-09-06T13:05:12.421974: step 17664, loss 0.00192386, acc 1
2016-09-06T13:05:13.250173: step 17665, loss 0.0178183, acc 0.98
2016-09-06T13:05:14.039761: step 17666, loss 0.00515165, acc 1
2016-09-06T13:05:14.854227: step 17667, loss 0.00670754, acc 1
2016-09-06T13:05:15.665861: step 17668, loss 0.0410958, acc 0.98
2016-09-06T13:05:16.468571: step 17669, loss 0.0218416, acc 1
2016-09-06T13:05:17.271473: step 17670, loss 0.0168062, acc 1
2016-09-06T13:05:18.136922: step 17671, loss 0.0312336, acc 0.98
2016-09-06T13:05:18.911024: step 17672, loss 0.141358, acc 0.98
2016-09-06T13:05:19.720144: step 17673, loss 0.0229384, acc 1
2016-09-06T13:05:20.548971: step 17674, loss 0.0326973, acc 0.98
2016-09-06T13:05:21.356078: step 17675, loss 0.00410341, acc 1
2016-09-06T13:05:22.170059: step 17676, loss 0.00169525, acc 1
2016-09-06T13:05:22.993402: step 17677, loss 0.00196316, acc 1
2016-09-06T13:05:23.818334: step 17678, loss 0.0051829, acc 1
2016-09-06T13:05:24.622327: step 17679, loss 0.0374038, acc 1
2016-09-06T13:05:25.457762: step 17680, loss 0.00224778, acc 1
2016-09-06T13:05:26.267139: step 17681, loss 0.0124657, acc 1
2016-09-06T13:05:27.108378: step 17682, loss 0.00251685, acc 1
2016-09-06T13:05:27.935719: step 17683, loss 0.00898672, acc 1
2016-09-06T13:05:28.747335: step 17684, loss 0.00266076, acc 1
2016-09-06T13:05:29.565449: step 17685, loss 0.00584188, acc 1
2016-09-06T13:05:30.436666: step 17686, loss 0.00729623, acc 1
2016-09-06T13:05:31.236813: step 17687, loss 0.00207623, acc 1
2016-09-06T13:05:32.041412: step 17688, loss 0.00832254, acc 1
2016-09-06T13:05:32.865399: step 17689, loss 0.00294819, acc 1
2016-09-06T13:05:33.681413: step 17690, loss 0.00250963, acc 1
2016-09-06T13:05:34.490966: step 17691, loss 0.00244573, acc 1
2016-09-06T13:05:35.330991: step 17692, loss 0.00261051, acc 1
2016-09-06T13:05:36.134971: step 17693, loss 0.00654702, acc 1
2016-09-06T13:05:36.961540: step 17694, loss 0.016638, acc 1
2016-09-06T13:05:37.788250: step 17695, loss 0.00944448, acc 1
2016-09-06T13:05:38.596753: step 17696, loss 0.0209041, acc 0.98
2016-09-06T13:05:39.405729: step 17697, loss 0.00295211, acc 1
2016-09-06T13:05:40.239024: step 17698, loss 0.0141571, acc 1
2016-09-06T13:05:41.055152: step 17699, loss 0.00243066, acc 1
2016-09-06T13:05:41.818417: step 17700, loss 0.00226078, acc 1

Evaluation:
2016-09-06T13:05:45.633614: step 17700, loss 2.53616, acc 0.724203

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-17700

2016-09-06T13:05:47.508045: step 17701, loss 0.00771777, acc 1
2016-09-06T13:05:48.315606: step 17702, loss 0.00791112, acc 1
2016-09-06T13:05:49.144633: step 17703, loss 0.00362221, acc 1
2016-09-06T13:05:49.988876: step 17704, loss 0.010922, acc 1
2016-09-06T13:05:50.808194: step 17705, loss 0.0160454, acc 1
2016-09-06T13:05:51.594044: step 17706, loss 0.00242829, acc 1
2016-09-06T13:05:52.395785: step 17707, loss 0.00282512, acc 1
2016-09-06T13:05:53.205840: step 17708, loss 0.00225177, acc 1
2016-09-06T13:05:54.004599: step 17709, loss 0.00271537, acc 1
2016-09-06T13:05:54.818904: step 17710, loss 0.00266109, acc 1
2016-09-06T13:05:55.633496: step 17711, loss 0.015091, acc 1
2016-09-06T13:05:56.440284: step 17712, loss 0.00288283, acc 1
2016-09-06T13:05:57.243953: step 17713, loss 0.0574849, acc 0.96
2016-09-06T13:05:58.092127: step 17714, loss 0.0098677, acc 1
2016-09-06T13:05:58.941338: step 17715, loss 0.0130158, acc 1
2016-09-06T13:05:59.753115: step 17716, loss 0.0347289, acc 1
2016-09-06T13:06:00.615019: step 17717, loss 0.0197251, acc 0.98
2016-09-06T13:06:01.463062: step 17718, loss 0.00602279, acc 1
2016-09-06T13:06:02.260773: step 17719, loss 0.00284225, acc 1
2016-09-06T13:06:03.088160: step 17720, loss 0.00263045, acc 1
2016-09-06T13:06:03.907633: step 17721, loss 0.0136417, acc 1
2016-09-06T13:06:04.708857: step 17722, loss 0.0487396, acc 0.98
2016-09-06T13:06:05.543298: step 17723, loss 0.0050019, acc 1
2016-09-06T13:06:06.386192: step 17724, loss 0.00202168, acc 1
2016-09-06T13:06:07.201456: step 17725, loss 0.0118865, acc 1
2016-09-06T13:06:08.026179: step 17726, loss 0.00200591, acc 1
2016-09-06T13:06:08.857652: step 17727, loss 0.00199884, acc 1
2016-09-06T13:06:09.635799: step 17728, loss 0.0156719, acc 1
2016-09-06T13:06:10.442754: step 17729, loss 0.0150945, acc 1
2016-09-06T13:06:11.251107: step 17730, loss 0.00291737, acc 1
2016-09-06T13:06:12.055105: step 17731, loss 0.0369326, acc 0.98
2016-09-06T13:06:12.865573: step 17732, loss 0.00508385, acc 1
2016-09-06T13:06:13.676053: step 17733, loss 0.0367653, acc 0.98
2016-09-06T13:06:14.493994: step 17734, loss 0.0231651, acc 0.98
2016-09-06T13:06:15.302943: step 17735, loss 0.0171605, acc 0.98
2016-09-06T13:06:16.107382: step 17736, loss 0.041132, acc 0.98
2016-09-06T13:06:16.896277: step 17737, loss 0.0165636, acc 1
2016-09-06T13:06:17.707428: step 17738, loss 0.00205172, acc 1
2016-09-06T13:06:18.564735: step 17739, loss 0.0160583, acc 0.98
2016-09-06T13:06:19.375819: step 17740, loss 0.00910071, acc 1
2016-09-06T13:06:20.201465: step 17741, loss 0.014096, acc 1
2016-09-06T13:06:21.054194: step 17742, loss 0.0175243, acc 1
2016-09-06T13:06:21.871000: step 17743, loss 0.00537437, acc 1
2016-09-06T13:06:22.674133: step 17744, loss 0.00187563, acc 1
2016-09-06T13:06:23.480481: step 17745, loss 0.00245328, acc 1
2016-09-06T13:06:24.269332: step 17746, loss 0.00796425, acc 1
2016-09-06T13:06:25.091321: step 17747, loss 0.00880163, acc 1
2016-09-06T13:06:25.913665: step 17748, loss 0.00762956, acc 1
2016-09-06T13:06:26.714330: step 17749, loss 0.0142549, acc 1
2016-09-06T13:06:27.521740: step 17750, loss 0.0169722, acc 1
2016-09-06T13:06:28.368849: step 17751, loss 0.00175561, acc 1
2016-09-06T13:06:29.161409: step 17752, loss 0.0030444, acc 1
2016-09-06T13:06:29.975945: step 17753, loss 0.00185223, acc 1
2016-09-06T13:06:30.797941: step 17754, loss 0.00373496, acc 1
2016-09-06T13:06:31.603203: step 17755, loss 0.0052384, acc 1
2016-09-06T13:06:32.437876: step 17756, loss 0.00176526, acc 1
2016-09-06T13:06:33.286218: step 17757, loss 0.00907799, acc 1
2016-09-06T13:06:34.107153: step 17758, loss 0.00226283, acc 1
2016-09-06T13:06:34.901873: step 17759, loss 0.00438505, acc 1
2016-09-06T13:06:35.724679: step 17760, loss 0.00257918, acc 1
2016-09-06T13:06:36.506131: step 17761, loss 0.0021297, acc 1
2016-09-06T13:06:37.313246: step 17762, loss 0.00179759, acc 1
2016-09-06T13:06:38.147594: step 17763, loss 0.00365207, acc 1
2016-09-06T13:06:38.955808: step 17764, loss 0.02715, acc 0.98
2016-09-06T13:06:39.765740: step 17765, loss 0.00178986, acc 1
2016-09-06T13:06:40.613476: step 17766, loss 0.0105999, acc 1
2016-09-06T13:06:41.445513: step 17767, loss 0.0141448, acc 1
2016-09-06T13:06:42.255555: step 17768, loss 0.0271339, acc 0.98
2016-09-06T13:06:43.096510: step 17769, loss 0.0199046, acc 1
2016-09-06T13:06:43.908128: step 17770, loss 0.00198582, acc 1
2016-09-06T13:06:44.726800: step 17771, loss 0.0350355, acc 1
2016-09-06T13:06:45.534808: step 17772, loss 0.0512264, acc 0.98
2016-09-06T13:06:46.347751: step 17773, loss 0.0237401, acc 1
2016-09-06T13:06:47.130369: step 17774, loss 0.0921994, acc 0.96
2016-09-06T13:06:47.977217: step 17775, loss 0.00172146, acc 1
2016-09-06T13:06:48.810035: step 17776, loss 0.0181751, acc 0.98
2016-09-06T13:06:49.577933: step 17777, loss 0.00317673, acc 1
2016-09-06T13:06:50.379573: step 17778, loss 0.00192654, acc 1
2016-09-06T13:06:51.198753: step 17779, loss 0.016041, acc 1
2016-09-06T13:06:51.992362: step 17780, loss 0.0499969, acc 0.98
2016-09-06T13:06:52.827772: step 17781, loss 0.00800018, acc 1
2016-09-06T13:06:53.677338: step 17782, loss 0.00428861, acc 1
2016-09-06T13:06:54.470031: step 17783, loss 0.0170215, acc 0.98
2016-09-06T13:06:55.274721: step 17784, loss 0.113043, acc 0.98
2016-09-06T13:06:56.121566: step 17785, loss 0.00156904, acc 1
2016-09-06T13:06:56.909889: step 17786, loss 0.00292456, acc 1
2016-09-06T13:06:57.716451: step 17787, loss 0.00625227, acc 1
2016-09-06T13:06:58.551173: step 17788, loss 0.00736853, acc 1
2016-09-06T13:06:59.349499: step 17789, loss 0.00163655, acc 1
2016-09-06T13:07:00.163888: step 17790, loss 0.00150484, acc 1
2016-09-06T13:07:01.032076: step 17791, loss 0.0169539, acc 1
2016-09-06T13:07:01.845100: step 17792, loss 0.0159309, acc 1
2016-09-06T13:07:02.660412: step 17793, loss 0.00521085, acc 1
2016-09-06T13:07:03.494514: step 17794, loss 0.00957592, acc 1
2016-09-06T13:07:04.339740: step 17795, loss 0.0234565, acc 0.98
2016-09-06T13:07:05.193516: step 17796, loss 0.00631817, acc 1
2016-09-06T13:07:06.039069: step 17797, loss 0.0105382, acc 1
2016-09-06T13:07:06.833435: step 17798, loss 0.0554618, acc 0.96
2016-09-06T13:07:07.622934: step 17799, loss 0.00186286, acc 1
2016-09-06T13:07:08.457070: step 17800, loss 0.0074282, acc 1

Evaluation:
2016-09-06T13:07:12.165983: step 17800, loss 2.13178, acc 0.737336

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-17800

2016-09-06T13:07:14.050883: step 17801, loss 0.0295777, acc 0.98
2016-09-06T13:07:14.887077: step 17802, loss 0.0316064, acc 0.98
2016-09-06T13:07:15.716971: step 17803, loss 0.00233904, acc 1
2016-09-06T13:07:16.553664: step 17804, loss 0.00288975, acc 1
2016-09-06T13:07:17.351875: step 17805, loss 0.0430863, acc 0.98
2016-09-06T13:07:18.190434: step 17806, loss 0.0141465, acc 1
2016-09-06T13:07:19.004967: step 17807, loss 0.00232823, acc 1
2016-09-06T13:07:19.807310: step 17808, loss 0.0129218, acc 1
2016-09-06T13:07:20.652601: step 17809, loss 0.0114323, acc 1
2016-09-06T13:07:21.462064: step 17810, loss 0.0299019, acc 0.98
2016-09-06T13:07:22.271183: step 17811, loss 0.00706086, acc 1
2016-09-06T13:07:23.099781: step 17812, loss 0.0496751, acc 0.98
2016-09-06T13:07:23.917157: step 17813, loss 0.0153965, acc 1
2016-09-06T13:07:24.709711: step 17814, loss 0.011009, acc 1
2016-09-06T13:07:25.511755: step 17815, loss 0.00295936, acc 1
2016-09-06T13:07:26.307510: step 17816, loss 0.00188366, acc 1
2016-09-06T13:07:27.098099: step 17817, loss 0.0130616, acc 1
2016-09-06T13:07:27.917230: step 17818, loss 0.0111239, acc 1
2016-09-06T13:07:28.732711: step 17819, loss 0.00800343, acc 1
2016-09-06T13:07:29.513661: step 17820, loss 0.00851756, acc 1
2016-09-06T13:07:30.348318: step 17821, loss 0.049046, acc 0.98
2016-09-06T13:07:31.225596: step 17822, loss 0.0519057, acc 0.96
2016-09-06T13:07:32.039574: step 17823, loss 0.00216401, acc 1
2016-09-06T13:07:32.861150: step 17824, loss 0.0494003, acc 0.98
2016-09-06T13:07:33.662013: step 17825, loss 0.115143, acc 0.96
2016-09-06T13:07:34.470651: step 17826, loss 0.00199092, acc 1
2016-09-06T13:07:35.286772: step 17827, loss 0.00240799, acc 1
2016-09-06T13:07:36.116130: step 17828, loss 0.0019399, acc 1
2016-09-06T13:07:36.962513: step 17829, loss 0.00188208, acc 1
2016-09-06T13:07:37.781073: step 17830, loss 0.0609042, acc 0.98
2016-09-06T13:07:38.614766: step 17831, loss 0.00775962, acc 1
2016-09-06T13:07:39.438548: step 17832, loss 0.00378533, acc 1
2016-09-06T13:07:40.252456: step 17833, loss 0.0382381, acc 0.98
2016-09-06T13:07:41.110929: step 17834, loss 0.00399584, acc 1
2016-09-06T13:07:41.928666: step 17835, loss 0.00197702, acc 1
2016-09-06T13:07:42.737454: step 17836, loss 0.013929, acc 1
2016-09-06T13:07:43.561243: step 17837, loss 0.00179498, acc 1
2016-09-06T13:07:44.425359: step 17838, loss 0.0244532, acc 0.98
2016-09-06T13:07:45.235326: step 17839, loss 0.0153959, acc 1
2016-09-06T13:07:46.054062: step 17840, loss 0.00184927, acc 1
2016-09-06T13:07:46.875865: step 17841, loss 0.0109451, acc 1
2016-09-06T13:07:47.675512: step 17842, loss 0.00546428, acc 1
2016-09-06T13:07:48.459888: step 17843, loss 0.0060671, acc 1
2016-09-06T13:07:49.261315: step 17844, loss 0.011527, acc 1
2016-09-06T13:07:50.077458: step 17845, loss 0.00192079, acc 1
2016-09-06T13:07:50.892444: step 17846, loss 0.0386196, acc 0.98
2016-09-06T13:07:51.719360: step 17847, loss 0.00187646, acc 1
2016-09-06T13:07:52.511659: step 17848, loss 0.00201459, acc 1
2016-09-06T13:07:53.305079: step 17849, loss 0.00187537, acc 1
2016-09-06T13:07:54.119248: step 17850, loss 0.0270878, acc 0.98
2016-09-06T13:07:54.949637: step 17851, loss 0.0210679, acc 1
2016-09-06T13:07:55.755605: step 17852, loss 0.0018669, acc 1
2016-09-06T13:07:56.588705: step 17853, loss 0.0376594, acc 0.98
2016-09-06T13:07:57.362240: step 17854, loss 0.0301031, acc 0.98
2016-09-06T13:07:58.159860: step 17855, loss 0.0231824, acc 1
2016-09-06T13:07:58.919156: step 17856, loss 0.00515616, acc 1
2016-09-06T13:07:59.744145: step 17857, loss 0.00193834, acc 1
2016-09-06T13:08:00.583019: step 17858, loss 0.0159842, acc 1
2016-09-06T13:08:01.399644: step 17859, loss 0.00226868, acc 1
2016-09-06T13:08:02.212090: step 17860, loss 0.0151357, acc 1
2016-09-06T13:08:03.016145: step 17861, loss 0.00584213, acc 1
2016-09-06T13:08:03.852338: step 17862, loss 0.0104571, acc 1
2016-09-06T13:08:04.642695: step 17863, loss 0.0102604, acc 1
2016-09-06T13:08:05.400160: step 17864, loss 0.0164336, acc 0.98
2016-09-06T13:08:06.192097: step 17865, loss 0.0696698, acc 0.98
2016-09-06T13:08:06.989380: step 17866, loss 0.00530628, acc 1
2016-09-06T13:08:07.821636: step 17867, loss 0.0070365, acc 1
2016-09-06T13:08:08.676979: step 17868, loss 0.0152908, acc 1
2016-09-06T13:08:09.512107: step 17869, loss 0.00364654, acc 1
2016-09-06T13:08:10.336232: step 17870, loss 0.0441143, acc 0.96
2016-09-06T13:08:11.160916: step 17871, loss 0.0278143, acc 1
2016-09-06T13:08:11.958603: step 17872, loss 0.00460475, acc 1
2016-09-06T13:08:12.764545: step 17873, loss 0.00316115, acc 1
2016-09-06T13:08:13.579225: step 17874, loss 0.00181336, acc 1
2016-09-06T13:08:14.396309: step 17875, loss 0.00210519, acc 1
2016-09-06T13:08:15.207278: step 17876, loss 0.00182358, acc 1
2016-09-06T13:08:16.061252: step 17877, loss 0.00182568, acc 1
2016-09-06T13:08:16.851459: step 17878, loss 0.00185878, acc 1
2016-09-06T13:08:17.678074: step 17879, loss 0.00186629, acc 1
2016-09-06T13:08:18.547419: step 17880, loss 0.0029394, acc 1
2016-09-06T13:08:19.322312: step 17881, loss 0.00205581, acc 1
2016-09-06T13:08:20.112195: step 17882, loss 0.00722482, acc 1
2016-09-06T13:08:20.966022: step 17883, loss 0.00204404, acc 1
2016-09-06T13:08:21.777906: step 17884, loss 0.00661767, acc 1
2016-09-06T13:08:22.596591: step 17885, loss 0.00189135, acc 1
2016-09-06T13:08:23.483287: step 17886, loss 0.0024473, acc 1
2016-09-06T13:08:24.303505: step 17887, loss 0.00181334, acc 1
2016-09-06T13:08:25.127753: step 17888, loss 0.0540325, acc 0.98
2016-09-06T13:08:25.937216: step 17889, loss 0.00363149, acc 1
2016-09-06T13:08:26.756974: step 17890, loss 0.00181353, acc 1
2016-09-06T13:08:27.533527: step 17891, loss 0.0017469, acc 1
2016-09-06T13:08:28.338296: step 17892, loss 0.00170805, acc 1
2016-09-06T13:08:29.149622: step 17893, loss 0.0211451, acc 0.98
2016-09-06T13:08:29.960332: step 17894, loss 0.0125273, acc 1
2016-09-06T13:08:30.797847: step 17895, loss 0.0108099, acc 1
2016-09-06T13:08:31.624547: step 17896, loss 0.00170594, acc 1
2016-09-06T13:08:32.397365: step 17897, loss 0.00236112, acc 1
2016-09-06T13:08:33.187428: step 17898, loss 0.00716613, acc 1
2016-09-06T13:08:33.996412: step 17899, loss 0.0159345, acc 1
2016-09-06T13:08:34.815517: step 17900, loss 0.00965475, acc 1

Evaluation:
2016-09-06T13:08:38.536715: step 17900, loss 2.5201, acc 0.739212

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-17900

2016-09-06T13:08:40.434726: step 17901, loss 0.00166769, acc 1
2016-09-06T13:08:41.243936: step 17902, loss 0.00205646, acc 1
2016-09-06T13:08:42.069420: step 17903, loss 0.00454972, acc 1
2016-09-06T13:08:42.895941: step 17904, loss 0.0328413, acc 0.98
2016-09-06T13:08:43.689655: step 17905, loss 0.00539284, acc 1
2016-09-06T13:08:44.522337: step 17906, loss 0.018442, acc 1
2016-09-06T13:08:45.352090: step 17907, loss 0.00947356, acc 1
2016-09-06T13:08:46.124536: step 17908, loss 0.0430842, acc 0.96
2016-09-06T13:08:46.944031: step 17909, loss 0.0146314, acc 1
2016-09-06T13:08:47.782412: step 17910, loss 0.00214575, acc 1
2016-09-06T13:08:48.593848: step 17911, loss 0.0136189, acc 1
2016-09-06T13:08:49.395447: step 17912, loss 0.00573156, acc 1
2016-09-06T13:08:50.213000: step 17913, loss 0.0129782, acc 1
2016-09-06T13:08:51.021466: step 17914, loss 0.0411925, acc 0.96
2016-09-06T13:08:51.824045: step 17915, loss 0.00345292, acc 1
2016-09-06T13:08:52.625908: step 17916, loss 0.0105427, acc 1
2016-09-06T13:08:53.439315: step 17917, loss 0.0315474, acc 0.98
2016-09-06T13:08:54.228211: step 17918, loss 0.00187564, acc 1
2016-09-06T13:08:55.034593: step 17919, loss 0.00172952, acc 1
2016-09-06T13:08:55.850154: step 17920, loss 0.0101702, acc 1
2016-09-06T13:08:56.625520: step 17921, loss 0.0018018, acc 1
2016-09-06T13:08:57.448407: step 17922, loss 0.00251986, acc 1
2016-09-06T13:08:58.243159: step 17923, loss 0.00836506, acc 1
2016-09-06T13:08:59.049462: step 17924, loss 0.0279322, acc 0.98
2016-09-06T13:08:59.888635: step 17925, loss 0.0375958, acc 0.98
2016-09-06T13:09:00.750476: step 17926, loss 0.00282159, acc 1
2016-09-06T13:09:01.556717: step 17927, loss 0.00188794, acc 1
2016-09-06T13:09:02.378365: step 17928, loss 0.0199156, acc 1
2016-09-06T13:09:03.180905: step 17929, loss 0.0140498, acc 1
2016-09-06T13:09:03.933407: step 17930, loss 0.00204783, acc 1
2016-09-06T13:09:04.775026: step 17931, loss 0.00181124, acc 1
2016-09-06T13:09:05.594868: step 17932, loss 0.0160977, acc 1
2016-09-06T13:09:06.388681: step 17933, loss 0.00443527, acc 1
2016-09-06T13:09:07.190267: step 17934, loss 0.00191756, acc 1
2016-09-06T13:09:08.004884: step 17935, loss 0.00201197, acc 1
2016-09-06T13:09:08.773631: step 17936, loss 0.012014, acc 1
2016-09-06T13:09:09.572973: step 17937, loss 0.0151093, acc 1
2016-09-06T13:09:10.392330: step 17938, loss 0.00229755, acc 1
2016-09-06T13:09:11.206097: step 17939, loss 0.0442759, acc 0.98
2016-09-06T13:09:12.017457: step 17940, loss 0.0401185, acc 0.98
2016-09-06T13:09:12.819474: step 17941, loss 0.0204436, acc 0.98
2016-09-06T13:09:13.611373: step 17942, loss 0.00190744, acc 1
2016-09-06T13:09:14.416280: step 17943, loss 0.00183489, acc 1
2016-09-06T13:09:15.235810: step 17944, loss 0.0018149, acc 1
2016-09-06T13:09:16.037845: step 17945, loss 0.0019103, acc 1
2016-09-06T13:09:16.838276: step 17946, loss 0.13364, acc 0.98
2016-09-06T13:09:17.652325: step 17947, loss 0.00176582, acc 1
2016-09-06T13:09:18.440848: step 17948, loss 0.00183519, acc 1
2016-09-06T13:09:19.238162: step 17949, loss 0.00439742, acc 1
2016-09-06T13:09:20.072625: step 17950, loss 0.00187431, acc 1
2016-09-06T13:09:20.860518: step 17951, loss 0.00173365, acc 1
2016-09-06T13:09:21.663124: step 17952, loss 0.0019278, acc 1
2016-09-06T13:09:22.497427: step 17953, loss 0.00150367, acc 1
2016-09-06T13:09:23.271535: step 17954, loss 0.00204725, acc 1
2016-09-06T13:09:24.080961: step 17955, loss 0.00146787, acc 1
2016-09-06T13:09:24.909966: step 17956, loss 0.0209449, acc 0.98
2016-09-06T13:09:25.673666: step 17957, loss 0.0140394, acc 1
2016-09-06T13:09:26.472308: step 17958, loss 0.00162408, acc 1
2016-09-06T13:09:27.279891: step 17959, loss 0.0028401, acc 1
2016-09-06T13:09:28.078900: step 17960, loss 0.00142836, acc 1
2016-09-06T13:09:28.894566: step 17961, loss 0.00670823, acc 1
2016-09-06T13:09:29.682674: step 17962, loss 0.0338269, acc 0.96
2016-09-06T13:09:30.526315: step 17963, loss 0.0144453, acc 1
2016-09-06T13:09:31.317229: step 17964, loss 0.011537, acc 1
2016-09-06T13:09:32.135819: step 17965, loss 0.0213582, acc 1
2016-09-06T13:09:32.955431: step 17966, loss 0.0295435, acc 0.98
2016-09-06T13:09:33.780741: step 17967, loss 0.00177527, acc 1
2016-09-06T13:09:34.589400: step 17968, loss 0.02244, acc 0.98
2016-09-06T13:09:35.349932: step 17969, loss 0.0466057, acc 0.96
2016-09-06T13:09:36.162228: step 17970, loss 0.0816898, acc 0.98
2016-09-06T13:09:36.976878: step 17971, loss 0.0302584, acc 0.98
2016-09-06T13:09:37.755391: step 17972, loss 0.00405942, acc 1
2016-09-06T13:09:38.565945: step 17973, loss 0.00190404, acc 1
2016-09-06T13:09:39.370039: step 17974, loss 0.00192035, acc 1
2016-09-06T13:09:40.163345: step 17975, loss 0.00250479, acc 1
2016-09-06T13:09:40.985342: step 17976, loss 0.0240909, acc 1
2016-09-06T13:09:41.812444: step 17977, loss 0.00162552, acc 1
2016-09-06T13:09:42.610599: step 17978, loss 0.0098433, acc 1
2016-09-06T13:09:43.455108: step 17979, loss 0.0185436, acc 1
2016-09-06T13:09:44.274520: step 17980, loss 0.048537, acc 0.98
2016-09-06T13:09:45.041892: step 17981, loss 0.0467897, acc 0.96
2016-09-06T13:09:45.827609: step 17982, loss 0.019258, acc 0.98
2016-09-06T13:09:46.653394: step 17983, loss 0.0349437, acc 0.98
2016-09-06T13:09:47.447479: step 17984, loss 0.00429006, acc 1
2016-09-06T13:09:48.257404: step 17985, loss 0.0130454, acc 1
2016-09-06T13:09:49.065397: step 17986, loss 0.105094, acc 0.96
2016-09-06T13:09:49.859082: step 17987, loss 0.0216176, acc 1
2016-09-06T13:09:50.662987: step 17988, loss 0.0157653, acc 0.98
2016-09-06T13:09:51.500044: step 17989, loss 0.00873946, acc 1
2016-09-06T13:09:52.281483: step 17990, loss 0.0114692, acc 1
2016-09-06T13:09:53.086630: step 17991, loss 0.0149785, acc 1
2016-09-06T13:09:53.931565: step 17992, loss 0.00157836, acc 1
2016-09-06T13:09:54.728493: step 17993, loss 0.00423268, acc 1
2016-09-06T13:09:55.513740: step 17994, loss 0.048959, acc 0.98
2016-09-06T13:09:56.344755: step 17995, loss 0.00406745, acc 1
2016-09-06T13:09:57.173165: step 17996, loss 0.0116974, acc 1
2016-09-06T13:09:57.975405: step 17997, loss 0.0241807, acc 1
2016-09-06T13:09:58.825580: step 17998, loss 0.0423452, acc 0.98
2016-09-06T13:09:59.654828: step 17999, loss 0.00194338, acc 1
2016-09-06T13:10:00.481703: step 18000, loss 0.00920722, acc 1

Evaluation:
2016-09-06T13:10:04.212326: step 18000, loss 2.33569, acc 0.737336

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-18000

2016-09-06T13:10:06.093190: step 18001, loss 0.00441025, acc 1
2016-09-06T13:10:06.929275: step 18002, loss 0.0203284, acc 1
2016-09-06T13:10:07.753404: step 18003, loss 0.011812, acc 1
2016-09-06T13:10:08.588139: step 18004, loss 0.00184456, acc 1
2016-09-06T13:10:09.422447: step 18005, loss 0.00237665, acc 1
2016-09-06T13:10:10.230892: step 18006, loss 0.00534308, acc 1
2016-09-06T13:10:11.069205: step 18007, loss 0.00383487, acc 1
2016-09-06T13:10:11.851037: step 18008, loss 0.00393958, acc 1
2016-09-06T13:10:12.641851: step 18009, loss 0.00192309, acc 1
2016-09-06T13:10:13.478537: step 18010, loss 0.00229806, acc 1
2016-09-06T13:10:14.286448: step 18011, loss 0.0225776, acc 0.98
2016-09-06T13:10:15.094628: step 18012, loss 0.0340594, acc 0.98
2016-09-06T13:10:15.920307: step 18013, loss 0.0250982, acc 1
2016-09-06T13:10:16.725521: step 18014, loss 0.0211187, acc 0.98
2016-09-06T13:10:17.544538: step 18015, loss 0.00348889, acc 1
2016-09-06T13:10:18.365227: step 18016, loss 0.0305647, acc 0.98
2016-09-06T13:10:19.175061: step 18017, loss 0.00417057, acc 1
2016-09-06T13:10:19.984671: step 18018, loss 0.0103894, acc 1
2016-09-06T13:10:20.816411: step 18019, loss 0.00540773, acc 1
2016-09-06T13:10:21.612336: step 18020, loss 0.00219233, acc 1
2016-09-06T13:10:22.419245: step 18021, loss 0.00621357, acc 1
2016-09-06T13:10:23.253085: step 18022, loss 0.00420261, acc 1
2016-09-06T13:10:24.031736: step 18023, loss 0.00472131, acc 1
2016-09-06T13:10:24.867351: step 18024, loss 0.0135312, acc 1
2016-09-06T13:10:25.702350: step 18025, loss 0.0889141, acc 0.98
2016-09-06T13:10:26.521441: step 18026, loss 0.012899, acc 1
2016-09-06T13:10:27.321407: step 18027, loss 0.0172508, acc 0.98
2016-09-06T13:10:28.134251: step 18028, loss 0.0325426, acc 0.98
2016-09-06T13:10:28.941144: step 18029, loss 0.00657504, acc 1
2016-09-06T13:10:29.720057: step 18030, loss 0.00505403, acc 1
2016-09-06T13:10:30.505676: step 18031, loss 0.00248572, acc 1
2016-09-06T13:10:31.323532: step 18032, loss 0.00303546, acc 1
2016-09-06T13:10:32.132093: step 18033, loss 0.0213822, acc 0.98
2016-09-06T13:10:32.990640: step 18034, loss 0.0191987, acc 1
2016-09-06T13:10:33.820275: step 18035, loss 0.0253423, acc 1
2016-09-06T13:10:34.605353: step 18036, loss 0.0252655, acc 1
2016-09-06T13:10:35.395006: step 18037, loss 0.0024349, acc 1
2016-09-06T13:10:36.197357: step 18038, loss 0.0363185, acc 0.96
2016-09-06T13:10:36.983365: step 18039, loss 0.00213639, acc 1
2016-09-06T13:10:37.838986: step 18040, loss 0.00269178, acc 1
2016-09-06T13:10:38.691010: step 18041, loss 0.0229661, acc 1
2016-09-06T13:10:39.503886: step 18042, loss 0.034174, acc 1
2016-09-06T13:10:40.306248: step 18043, loss 0.0124723, acc 1
2016-09-06T13:10:41.151498: step 18044, loss 0.0024179, acc 1
2016-09-06T13:10:41.988932: step 18045, loss 0.0848403, acc 0.96
2016-09-06T13:10:42.810088: step 18046, loss 0.00838382, acc 1
2016-09-06T13:10:43.646208: step 18047, loss 0.00208565, acc 1
2016-09-06T13:10:44.393597: step 18048, loss 0.00210954, acc 1
2016-09-06T13:10:45.190887: step 18049, loss 0.0103851, acc 1
2016-09-06T13:10:46.038736: step 18050, loss 0.012512, acc 1
2016-09-06T13:10:46.845688: step 18051, loss 0.00264921, acc 1
2016-09-06T13:10:47.642325: step 18052, loss 0.00404148, acc 1
2016-09-06T13:10:48.457501: step 18053, loss 0.0109866, acc 1
2016-09-06T13:10:49.265549: step 18054, loss 0.0186563, acc 0.98
2016-09-06T13:10:50.119495: step 18055, loss 0.00312856, acc 1
2016-09-06T13:10:50.967152: step 18056, loss 0.0139533, acc 1
2016-09-06T13:10:51.774803: step 18057, loss 0.00908943, acc 1
2016-09-06T13:10:52.623414: step 18058, loss 0.0152784, acc 1
2016-09-06T13:10:53.447551: step 18059, loss 0.00201154, acc 1
2016-09-06T13:10:54.255490: step 18060, loss 0.00293435, acc 1
2016-09-06T13:10:55.077425: step 18061, loss 0.0284993, acc 0.98
2016-09-06T13:10:55.909482: step 18062, loss 0.00644338, acc 1
2016-09-06T13:10:56.743580: step 18063, loss 0.0166258, acc 0.98
2016-09-06T13:10:57.538078: step 18064, loss 0.0151246, acc 1
2016-09-06T13:10:58.336195: step 18065, loss 0.00479921, acc 1
2016-09-06T13:10:59.159770: step 18066, loss 0.00206, acc 1
2016-09-06T13:10:59.942249: step 18067, loss 0.00209764, acc 1
2016-09-06T13:11:00.817446: step 18068, loss 0.002094, acc 1
2016-09-06T13:11:01.635578: step 18069, loss 0.0022262, acc 1
2016-09-06T13:11:02.435565: step 18070, loss 0.00283141, acc 1
2016-09-06T13:11:03.231300: step 18071, loss 0.0337148, acc 0.98
2016-09-06T13:11:04.070349: step 18072, loss 0.0313973, acc 0.96
2016-09-06T13:11:04.857896: step 18073, loss 0.00464988, acc 1
2016-09-06T13:11:05.658349: step 18074, loss 0.00498028, acc 1
2016-09-06T13:11:06.483755: step 18075, loss 0.0769145, acc 0.98
2016-09-06T13:11:07.286440: step 18076, loss 0.0478932, acc 0.98
2016-09-06T13:11:08.092953: step 18077, loss 0.00313099, acc 1
2016-09-06T13:11:08.947102: step 18078, loss 0.00454934, acc 1
2016-09-06T13:11:09.765908: step 18079, loss 0.00189852, acc 1
2016-09-06T13:11:10.559974: step 18080, loss 0.0897702, acc 0.98
2016-09-06T13:11:11.404763: step 18081, loss 0.00204515, acc 1
2016-09-06T13:11:12.222833: step 18082, loss 0.0135824, acc 1
2016-09-06T13:11:13.016903: step 18083, loss 0.0040914, acc 1
2016-09-06T13:11:13.847713: step 18084, loss 0.0018292, acc 1
2016-09-06T13:11:14.653665: step 18085, loss 0.00224675, acc 1
2016-09-06T13:11:15.453031: step 18086, loss 0.00196563, acc 1
2016-09-06T13:11:16.312442: step 18087, loss 0.00774688, acc 1
2016-09-06T13:11:17.153574: step 18088, loss 0.0107954, acc 1
2016-09-06T13:11:17.949401: step 18089, loss 0.0148591, acc 1
2016-09-06T13:11:18.789121: step 18090, loss 0.00241106, acc 1
2016-09-06T13:11:19.625271: step 18091, loss 0.00819907, acc 1
2016-09-06T13:11:20.430392: step 18092, loss 0.0188614, acc 0.98
2016-09-06T13:11:21.292506: step 18093, loss 0.00230402, acc 1
2016-09-06T13:11:22.088034: step 18094, loss 0.00251515, acc 1
2016-09-06T13:11:22.902526: step 18095, loss 0.00798614, acc 1
2016-09-06T13:11:23.720997: step 18096, loss 0.00254022, acc 1
2016-09-06T13:11:24.534648: step 18097, loss 0.0132808, acc 1
2016-09-06T13:11:25.337883: step 18098, loss 0.00265541, acc 1
2016-09-06T13:11:26.170771: step 18099, loss 0.00289912, acc 1
2016-09-06T13:11:26.989574: step 18100, loss 0.0223305, acc 0.98

Evaluation:
2016-09-06T13:11:30.715819: step 18100, loss 2.84924, acc 0.748593

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-18100

2016-09-06T13:11:32.605805: step 18101, loss 0.00356548, acc 1
2016-09-06T13:11:33.441961: step 18102, loss 0.0130268, acc 1
2016-09-06T13:11:34.269256: step 18103, loss 0.00524819, acc 1
2016-09-06T13:11:35.060230: step 18104, loss 0.0257586, acc 0.98
2016-09-06T13:11:35.877350: step 18105, loss 0.00278424, acc 1
2016-09-06T13:11:36.666275: step 18106, loss 0.0337062, acc 0.98
2016-09-06T13:11:37.433358: step 18107, loss 0.00281533, acc 1
2016-09-06T13:11:38.252042: step 18108, loss 0.00282298, acc 1
2016-09-06T13:11:39.059371: step 18109, loss 0.00315785, acc 1
2016-09-06T13:11:39.884656: step 18110, loss 0.00281074, acc 1
2016-09-06T13:11:40.709366: step 18111, loss 0.00280273, acc 1
2016-09-06T13:11:41.539908: step 18112, loss 0.0109687, acc 1
2016-09-06T13:11:42.344738: step 18113, loss 0.00605417, acc 1
2016-09-06T13:11:43.126563: step 18114, loss 0.0162646, acc 1
2016-09-06T13:11:43.934546: step 18115, loss 0.0119112, acc 1
2016-09-06T13:11:44.756464: step 18116, loss 0.0423206, acc 0.98
2016-09-06T13:11:45.561475: step 18117, loss 0.0225391, acc 0.98
2016-09-06T13:11:46.382138: step 18118, loss 0.0202362, acc 1
2016-09-06T13:11:47.158513: step 18119, loss 0.0131314, acc 1
2016-09-06T13:11:48.017344: step 18120, loss 0.00278054, acc 1
2016-09-06T13:11:48.809708: step 18121, loss 0.00842525, acc 1
2016-09-06T13:11:49.575358: step 18122, loss 0.0213056, acc 0.98
2016-09-06T13:11:50.370498: step 18123, loss 0.0182855, acc 1
2016-09-06T13:11:51.222071: step 18124, loss 0.0168169, acc 0.98
2016-09-06T13:11:51.993482: step 18125, loss 0.00287823, acc 1
2016-09-06T13:11:52.805561: step 18126, loss 0.00287127, acc 1
2016-09-06T13:11:53.635719: step 18127, loss 0.0026751, acc 1
2016-09-06T13:11:54.487501: step 18128, loss 0.00972138, acc 1
2016-09-06T13:11:55.305728: step 18129, loss 0.00265567, acc 1
2016-09-06T13:11:56.162520: step 18130, loss 0.00263168, acc 1
2016-09-06T13:11:56.997994: step 18131, loss 0.00456544, acc 1
2016-09-06T13:11:57.808533: step 18132, loss 0.00258329, acc 1
2016-09-06T13:11:58.658053: step 18133, loss 0.0687562, acc 0.98
2016-09-06T13:11:59.467989: step 18134, loss 0.0601096, acc 0.98
2016-09-06T13:12:00.332149: step 18135, loss 0.00243244, acc 1
2016-09-06T13:12:01.136300: step 18136, loss 0.0140553, acc 1
2016-09-06T13:12:01.970282: step 18137, loss 0.00640207, acc 1
2016-09-06T13:12:02.785419: step 18138, loss 0.00277548, acc 1
2016-09-06T13:12:03.600513: step 18139, loss 0.0141627, acc 1
2016-09-06T13:12:04.403066: step 18140, loss 0.00224824, acc 1
2016-09-06T13:12:05.249446: step 18141, loss 0.00233984, acc 1
2016-09-06T13:12:06.057846: step 18142, loss 0.0101169, acc 1
2016-09-06T13:12:06.876547: step 18143, loss 0.0168758, acc 0.98
2016-09-06T13:12:07.693479: step 18144, loss 0.00496622, acc 1
2016-09-06T13:12:08.493786: step 18145, loss 0.00208372, acc 1
2016-09-06T13:12:09.340935: step 18146, loss 0.0147575, acc 1
2016-09-06T13:12:10.125449: step 18147, loss 0.00315373, acc 1
2016-09-06T13:12:10.904983: step 18148, loss 0.022018, acc 1
2016-09-06T13:12:11.734517: step 18149, loss 0.00228795, acc 1
2016-09-06T13:12:12.513159: step 18150, loss 0.00253339, acc 1
2016-09-06T13:12:13.371099: step 18151, loss 0.00355862, acc 1
2016-09-06T13:12:14.168741: step 18152, loss 0.00204557, acc 1
2016-09-06T13:12:14.942008: step 18153, loss 0.0156348, acc 1
2016-09-06T13:12:15.728710: step 18154, loss 0.00269403, acc 1
2016-09-06T13:12:16.562336: step 18155, loss 0.0380609, acc 0.96
2016-09-06T13:12:17.348625: step 18156, loss 0.0127308, acc 1
2016-09-06T13:12:18.160222: step 18157, loss 0.0114363, acc 1
2016-09-06T13:12:19.005592: step 18158, loss 0.0326107, acc 0.98
2016-09-06T13:12:19.817921: step 18159, loss 0.0019751, acc 1
2016-09-06T13:12:20.612287: step 18160, loss 0.00589969, acc 1
2016-09-06T13:12:21.451588: step 18161, loss 0.00319038, acc 1
2016-09-06T13:12:22.250954: step 18162, loss 0.00289232, acc 1
2016-09-06T13:12:23.060557: step 18163, loss 0.0289371, acc 0.98
2016-09-06T13:12:23.913464: step 18164, loss 0.00282999, acc 1
2016-09-06T13:12:24.707994: step 18165, loss 0.014752, acc 1
2016-09-06T13:12:25.503824: step 18166, loss 0.00273819, acc 1
2016-09-06T13:12:26.345532: step 18167, loss 0.0200664, acc 0.98
2016-09-06T13:12:27.159964: step 18168, loss 0.0135752, acc 1
2016-09-06T13:12:27.977693: step 18169, loss 0.00300784, acc 1
2016-09-06T13:12:28.816606: step 18170, loss 0.0177639, acc 0.98
2016-09-06T13:12:29.645460: step 18171, loss 0.00195198, acc 1
2016-09-06T13:12:30.466722: step 18172, loss 0.0108781, acc 1
2016-09-06T13:12:31.295778: step 18173, loss 0.0209592, acc 0.98
2016-09-06T13:12:32.104092: step 18174, loss 0.00193687, acc 1
2016-09-06T13:12:32.911443: step 18175, loss 0.00339105, acc 1
2016-09-06T13:12:33.731892: step 18176, loss 0.00292608, acc 1
2016-09-06T13:12:34.531580: step 18177, loss 0.00286866, acc 1
2016-09-06T13:12:35.356175: step 18178, loss 0.0112277, acc 1
2016-09-06T13:12:36.208848: step 18179, loss 0.0825986, acc 0.96
2016-09-06T13:12:37.037328: step 18180, loss 0.0194971, acc 0.98
2016-09-06T13:12:37.840927: step 18181, loss 0.0209595, acc 1
2016-09-06T13:12:38.671714: step 18182, loss 0.00179856, acc 1
2016-09-06T13:12:39.488876: step 18183, loss 0.00219286, acc 1
2016-09-06T13:12:40.273038: step 18184, loss 0.00211362, acc 1
2016-09-06T13:12:41.075244: step 18185, loss 0.0151696, acc 1
2016-09-06T13:12:41.881268: step 18186, loss 0.0109171, acc 1
2016-09-06T13:12:42.688537: step 18187, loss 0.00162652, acc 1
2016-09-06T13:12:43.481739: step 18188, loss 0.00176802, acc 1
2016-09-06T13:12:44.273388: step 18189, loss 0.0287563, acc 0.98
2016-09-06T13:12:45.071949: step 18190, loss 0.00184801, acc 1
2016-09-06T13:12:45.899918: step 18191, loss 0.0015885, acc 1
2016-09-06T13:12:46.731476: step 18192, loss 0.0168215, acc 0.98
2016-09-06T13:12:47.524236: step 18193, loss 0.00758607, acc 1
2016-09-06T13:12:48.324793: step 18194, loss 0.0433675, acc 0.96
2016-09-06T13:12:49.127477: step 18195, loss 0.00300412, acc 1
2016-09-06T13:12:49.921850: step 18196, loss 0.00259221, acc 1
2016-09-06T13:12:50.736452: step 18197, loss 0.0271897, acc 0.98
2016-09-06T13:12:51.547803: step 18198, loss 0.00323769, acc 1
2016-09-06T13:12:52.343192: step 18199, loss 0.00149982, acc 1
2016-09-06T13:12:53.173098: step 18200, loss 0.0265268, acc 0.98

Evaluation:
2016-09-06T13:12:56.875183: step 18200, loss 2.43004, acc 0.737336

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-18200

2016-09-06T13:12:58.803174: step 18201, loss 0.00150814, acc 1
2016-09-06T13:12:59.625787: step 18202, loss 0.00145643, acc 1
2016-09-06T13:13:00.449144: step 18203, loss 0.0048889, acc 1
2016-09-06T13:13:01.248828: step 18204, loss 0.00198465, acc 1
2016-09-06T13:13:02.069281: step 18205, loss 0.0498484, acc 0.98
2016-09-06T13:13:02.884323: step 18206, loss 0.00258194, acc 1
2016-09-06T13:13:03.671366: step 18207, loss 0.00143712, acc 1
2016-09-06T13:13:04.480535: step 18208, loss 0.0265768, acc 0.98
2016-09-06T13:13:05.325434: step 18209, loss 0.0510828, acc 0.96
2016-09-06T13:13:06.141442: step 18210, loss 0.00355796, acc 1
2016-09-06T13:13:06.950807: step 18211, loss 0.00141552, acc 1
2016-09-06T13:13:07.759337: step 18212, loss 0.00144471, acc 1
2016-09-06T13:13:08.562224: step 18213, loss 0.00142883, acc 1
2016-09-06T13:13:09.371538: step 18214, loss 0.0189814, acc 1
2016-09-06T13:13:10.199235: step 18215, loss 0.0015866, acc 1
2016-09-06T13:13:10.996412: step 18216, loss 0.00369439, acc 1
2016-09-06T13:13:11.811313: step 18217, loss 0.0647948, acc 0.94
2016-09-06T13:13:12.641666: step 18218, loss 0.00193827, acc 1
2016-09-06T13:13:13.440694: step 18219, loss 0.00278907, acc 1
2016-09-06T13:13:14.230527: step 18220, loss 0.0271493, acc 0.98
2016-09-06T13:13:15.044238: step 18221, loss 0.00175609, acc 1
2016-09-06T13:13:15.854450: step 18222, loss 0.0172804, acc 1
2016-09-06T13:13:16.658525: step 18223, loss 0.0139642, acc 1
2016-09-06T13:13:17.467049: step 18224, loss 0.00183808, acc 1
2016-09-06T13:13:18.305523: step 18225, loss 0.00146337, acc 1
2016-09-06T13:13:19.108146: step 18226, loss 0.020227, acc 1
2016-09-06T13:13:19.975702: step 18227, loss 0.0118771, acc 1
2016-09-06T13:13:20.855406: step 18228, loss 0.0274832, acc 1
2016-09-06T13:13:21.675467: step 18229, loss 0.00226469, acc 1
2016-09-06T13:13:22.488544: step 18230, loss 0.0016805, acc 1
2016-09-06T13:13:23.305055: step 18231, loss 0.00282675, acc 1
2016-09-06T13:13:24.113162: step 18232, loss 0.00155929, acc 1
2016-09-06T13:13:24.911221: step 18233, loss 0.0016399, acc 1
2016-09-06T13:13:25.749286: step 18234, loss 0.0153987, acc 1
2016-09-06T13:13:26.561275: step 18235, loss 0.0146965, acc 1
2016-09-06T13:13:27.364521: step 18236, loss 0.00160252, acc 1
2016-09-06T13:13:28.203152: step 18237, loss 0.0452726, acc 0.96
2016-09-06T13:13:29.032970: step 18238, loss 0.00599753, acc 1
2016-09-06T13:13:29.845200: step 18239, loss 0.0170373, acc 1
2016-09-06T13:13:30.625081: step 18240, loss 0.00164828, acc 1
2016-09-06T13:13:31.428821: step 18241, loss 0.00179623, acc 1
2016-09-06T13:13:32.235212: step 18242, loss 0.0201513, acc 1
2016-09-06T13:13:33.081945: step 18243, loss 0.00251092, acc 1
2016-09-06T13:13:33.938990: step 18244, loss 0.00209941, acc 1
2016-09-06T13:13:34.729426: step 18245, loss 0.0299397, acc 0.98
2016-09-06T13:13:35.569747: step 18246, loss 0.00345882, acc 1
2016-09-06T13:13:36.405092: step 18247, loss 0.021101, acc 0.98
2016-09-06T13:13:37.185979: step 18248, loss 0.0416291, acc 0.98
2016-09-06T13:13:38.004413: step 18249, loss 0.0799275, acc 0.96
2016-09-06T13:13:38.807367: step 18250, loss 0.00174752, acc 1
2016-09-06T13:13:39.573105: step 18251, loss 0.0014792, acc 1
2016-09-06T13:13:40.397814: step 18252, loss 0.00147811, acc 1
2016-09-06T13:13:41.221511: step 18253, loss 0.0170745, acc 0.98
2016-09-06T13:13:42.037423: step 18254, loss 0.0150522, acc 1
2016-09-06T13:13:42.841020: step 18255, loss 0.0205904, acc 1
2016-09-06T13:13:43.663703: step 18256, loss 0.00131019, acc 1
2016-09-06T13:13:44.455202: step 18257, loss 0.0132095, acc 1
2016-09-06T13:13:45.265327: step 18258, loss 0.00163341, acc 1
2016-09-06T13:13:46.085883: step 18259, loss 0.0232475, acc 0.98
2016-09-06T13:13:46.867069: step 18260, loss 0.00124597, acc 1
2016-09-06T13:13:47.673894: step 18261, loss 0.00147601, acc 1
2016-09-06T13:13:48.501654: step 18262, loss 0.013519, acc 1
2016-09-06T13:13:49.303274: step 18263, loss 0.0135661, acc 1
2016-09-06T13:13:50.103067: step 18264, loss 0.0184844, acc 0.98
2016-09-06T13:13:50.918459: step 18265, loss 0.00262363, acc 1
2016-09-06T13:13:51.697199: step 18266, loss 0.00131649, acc 1
2016-09-06T13:13:52.520696: step 18267, loss 0.00118388, acc 1
2016-09-06T13:13:53.346615: step 18268, loss 0.00461676, acc 1
2016-09-06T13:13:54.134044: step 18269, loss 0.00521965, acc 1
2016-09-06T13:13:54.939609: step 18270, loss 0.00695583, acc 1
2016-09-06T13:13:55.748590: step 18271, loss 0.00120773, acc 1
2016-09-06T13:13:56.526711: step 18272, loss 0.0016163, acc 1
2016-09-06T13:13:57.337734: step 18273, loss 0.00151612, acc 1
2016-09-06T13:13:58.164931: step 18274, loss 0.0144692, acc 1
2016-09-06T13:13:58.978545: step 18275, loss 0.00239065, acc 1
2016-09-06T13:13:59.774627: step 18276, loss 0.0011686, acc 1
2016-09-06T13:14:00.593719: step 18277, loss 0.0302911, acc 0.98
2016-09-06T13:14:01.387340: step 18278, loss 0.00862326, acc 1
2016-09-06T13:14:02.170606: step 18279, loss 0.00126143, acc 1
2016-09-06T13:14:02.989049: step 18280, loss 0.00116076, acc 1
2016-09-06T13:14:03.784779: step 18281, loss 0.0207078, acc 0.98
2016-09-06T13:14:04.615651: step 18282, loss 0.00216978, acc 1
2016-09-06T13:14:05.435929: step 18283, loss 0.00126205, acc 1
2016-09-06T13:14:06.243324: step 18284, loss 0.00225261, acc 1
2016-09-06T13:14:07.043604: step 18285, loss 0.00114233, acc 1
2016-09-06T13:14:07.862604: step 18286, loss 0.0015303, acc 1
2016-09-06T13:14:08.655757: step 18287, loss 0.00436117, acc 1
2016-09-06T13:14:09.465996: step 18288, loss 0.00528636, acc 1
2016-09-06T13:14:10.316033: step 18289, loss 0.00881692, acc 1
2016-09-06T13:14:11.111328: step 18290, loss 0.00340285, acc 1
2016-09-06T13:14:11.901545: step 18291, loss 0.00116184, acc 1
2016-09-06T13:14:12.701441: step 18292, loss 0.00854731, acc 1
2016-09-06T13:14:13.487888: step 18293, loss 0.0282223, acc 0.98
2016-09-06T13:14:14.277609: step 18294, loss 0.1694, acc 0.98
2016-09-06T13:14:15.098378: step 18295, loss 0.0189738, acc 0.98
2016-09-06T13:14:15.902744: step 18296, loss 0.0142147, acc 1
2016-09-06T13:14:16.700236: step 18297, loss 0.00140114, acc 1
2016-09-06T13:14:17.509601: step 18298, loss 0.00284023, acc 1
2016-09-06T13:14:18.321901: step 18299, loss 0.00174703, acc 1
2016-09-06T13:14:19.174753: step 18300, loss 0.00872643, acc 1

Evaluation:
2016-09-06T13:14:22.864803: step 18300, loss 2.10353, acc 0.739212

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-18300

2016-09-06T13:14:24.758204: step 18301, loss 0.0654143, acc 0.98
2016-09-06T13:14:25.568987: step 18302, loss 0.00176552, acc 1
2016-09-06T13:14:26.406609: step 18303, loss 0.00446635, acc 1
2016-09-06T13:14:27.245021: step 18304, loss 0.0185587, acc 1
2016-09-06T13:14:28.057642: step 18305, loss 0.00313402, acc 1
2016-09-06T13:14:28.870207: step 18306, loss 0.0334806, acc 0.98
2016-09-06T13:14:29.673411: step 18307, loss 0.00265309, acc 1
2016-09-06T13:14:30.488606: step 18308, loss 0.01227, acc 1
2016-09-06T13:14:31.308662: step 18309, loss 0.0364625, acc 0.98
2016-09-06T13:14:32.136207: step 18310, loss 0.0121111, acc 1
2016-09-06T13:14:32.905325: step 18311, loss 0.0113156, acc 1
2016-09-06T13:14:33.737740: step 18312, loss 0.00173103, acc 1
2016-09-06T13:14:34.541481: step 18313, loss 0.00233298, acc 1
2016-09-06T13:14:35.341262: step 18314, loss 0.00201962, acc 1
2016-09-06T13:14:36.171863: step 18315, loss 0.0411554, acc 0.96
2016-09-06T13:14:36.987394: step 18316, loss 0.0182317, acc 0.98
2016-09-06T13:14:37.785388: step 18317, loss 0.0217609, acc 1
2016-09-06T13:14:38.589772: step 18318, loss 0.00569141, acc 1
2016-09-06T13:14:39.394514: step 18319, loss 0.0025662, acc 1
2016-09-06T13:14:40.170449: step 18320, loss 0.00206206, acc 1
2016-09-06T13:14:40.993731: step 18321, loss 0.0115537, acc 1
2016-09-06T13:14:41.791938: step 18322, loss 0.0155298, acc 1
2016-09-06T13:14:42.582305: step 18323, loss 0.00584827, acc 1
2016-09-06T13:14:43.404324: step 18324, loss 0.0021962, acc 1
2016-09-06T13:14:44.229645: step 18325, loss 0.0254533, acc 1
2016-09-06T13:14:45.022523: step 18326, loss 0.0151814, acc 1
2016-09-06T13:14:45.840772: step 18327, loss 0.0020953, acc 1
2016-09-06T13:14:46.664602: step 18328, loss 0.00211972, acc 1
2016-09-06T13:14:47.475803: step 18329, loss 0.00431017, acc 1
2016-09-06T13:14:48.324993: step 18330, loss 0.0033371, acc 1
2016-09-06T13:14:49.153712: step 18331, loss 0.00987449, acc 1
2016-09-06T13:14:49.926874: step 18332, loss 0.00236492, acc 1
2016-09-06T13:14:50.695168: step 18333, loss 0.016167, acc 0.98
2016-09-06T13:14:51.510036: step 18334, loss 0.00326021, acc 1
2016-09-06T13:14:52.274326: step 18335, loss 0.00221905, acc 1
2016-09-06T13:14:53.093262: step 18336, loss 0.00207434, acc 1
2016-09-06T13:14:53.906284: step 18337, loss 0.00263725, acc 1
2016-09-06T13:14:54.690071: step 18338, loss 0.00726703, acc 1
2016-09-06T13:14:55.503161: step 18339, loss 0.0240637, acc 0.98
2016-09-06T13:14:56.306572: step 18340, loss 0.00536832, acc 1
2016-09-06T13:14:57.082410: step 18341, loss 0.00524388, acc 1
2016-09-06T13:14:57.890708: step 18342, loss 0.0020691, acc 1
2016-09-06T13:14:58.701044: step 18343, loss 0.00201856, acc 1
2016-09-06T13:14:59.529931: step 18344, loss 0.00212991, acc 1
2016-09-06T13:15:00.338037: step 18345, loss 0.0245287, acc 0.98
2016-09-06T13:15:01.190467: step 18346, loss 0.0158677, acc 1
2016-09-06T13:15:01.998587: step 18347, loss 0.0510723, acc 0.98
2016-09-06T13:15:02.799303: step 18348, loss 0.00856755, acc 1
2016-09-06T13:15:03.617229: step 18349, loss 0.00243711, acc 1
2016-09-06T13:15:04.402360: step 18350, loss 0.0354288, acc 0.98
2016-09-06T13:15:05.225352: step 18351, loss 0.00183982, acc 1
2016-09-06T13:15:06.022006: step 18352, loss 0.146209, acc 0.98
2016-09-06T13:15:06.813408: step 18353, loss 0.00323173, acc 1
2016-09-06T13:15:07.604534: step 18354, loss 0.0019412, acc 1
2016-09-06T13:15:08.440783: step 18355, loss 0.0410349, acc 0.98
2016-09-06T13:15:09.207646: step 18356, loss 0.00199187, acc 1
2016-09-06T13:15:10.008932: step 18357, loss 0.0099854, acc 1
2016-09-06T13:15:10.847779: step 18358, loss 0.00213754, acc 1
2016-09-06T13:15:11.661646: step 18359, loss 0.00705734, acc 1
2016-09-06T13:15:12.466543: step 18360, loss 0.0120164, acc 1
2016-09-06T13:15:13.265258: step 18361, loss 0.0147778, acc 1
2016-09-06T13:15:14.048297: step 18362, loss 0.00374816, acc 1
2016-09-06T13:15:14.898878: step 18363, loss 0.0177356, acc 0.98
2016-09-06T13:15:15.738124: step 18364, loss 0.011676, acc 1
2016-09-06T13:15:16.542216: step 18365, loss 0.0117384, acc 1
2016-09-06T13:15:17.361940: step 18366, loss 0.0026681, acc 1
2016-09-06T13:15:18.185593: step 18367, loss 0.00575268, acc 1
2016-09-06T13:15:19.019610: step 18368, loss 0.0210633, acc 0.98
2016-09-06T13:15:19.842974: step 18369, loss 0.0143139, acc 1
2016-09-06T13:15:20.719412: step 18370, loss 0.00353761, acc 1
2016-09-06T13:15:21.532326: step 18371, loss 0.00569304, acc 1
2016-09-06T13:15:22.334174: step 18372, loss 0.00212586, acc 1
2016-09-06T13:15:23.170875: step 18373, loss 0.00850204, acc 1
2016-09-06T13:15:24.001967: step 18374, loss 0.0170739, acc 0.98
2016-09-06T13:15:24.806344: step 18375, loss 0.0227565, acc 0.98
2016-09-06T13:15:25.657706: step 18376, loss 0.0362349, acc 0.98
2016-09-06T13:15:26.473641: step 18377, loss 0.0186446, acc 0.98
2016-09-06T13:15:27.281908: step 18378, loss 0.0288271, acc 0.98
2016-09-06T13:15:28.097487: step 18379, loss 0.00221243, acc 1
2016-09-06T13:15:28.902416: step 18380, loss 0.00212638, acc 1
2016-09-06T13:15:29.691133: step 18381, loss 0.0312795, acc 1
2016-09-06T13:15:30.519892: step 18382, loss 0.00218626, acc 1
2016-09-06T13:15:31.347201: step 18383, loss 0.0069943, acc 1
2016-09-06T13:15:32.133047: step 18384, loss 0.0163391, acc 0.98
2016-09-06T13:15:32.945546: step 18385, loss 0.00451492, acc 1
2016-09-06T13:15:33.757870: step 18386, loss 0.0355014, acc 0.96
2016-09-06T13:15:34.540236: step 18387, loss 0.0109364, acc 1
2016-09-06T13:15:35.364587: step 18388, loss 0.0322244, acc 0.98
2016-09-06T13:15:36.160822: step 18389, loss 0.0162368, acc 1
2016-09-06T13:15:36.965509: step 18390, loss 0.0125307, acc 1
2016-09-06T13:15:37.773832: step 18391, loss 0.00217315, acc 1
2016-09-06T13:15:38.613955: step 18392, loss 0.00205817, acc 1
2016-09-06T13:15:39.414361: step 18393, loss 0.00210761, acc 1
2016-09-06T13:15:40.242774: step 18394, loss 0.0253533, acc 0.98
2016-09-06T13:15:41.099814: step 18395, loss 0.00293133, acc 1
2016-09-06T13:15:41.900704: step 18396, loss 0.0371445, acc 0.98
2016-09-06T13:15:42.716631: step 18397, loss 0.00278754, acc 1
2016-09-06T13:15:43.572050: step 18398, loss 0.0464911, acc 0.98
2016-09-06T13:15:44.393370: step 18399, loss 0.00212645, acc 1
2016-09-06T13:15:45.180306: step 18400, loss 0.0482768, acc 0.98

Evaluation:
2016-09-06T13:15:48.918319: step 18400, loss 2.63348, acc 0.746717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-18400

2016-09-06T13:15:50.781275: step 18401, loss 0.0133364, acc 1
2016-09-06T13:15:51.587854: step 18402, loss 0.0436209, acc 0.96
2016-09-06T13:15:52.442231: step 18403, loss 0.00163746, acc 1
2016-09-06T13:15:53.270354: step 18404, loss 0.0150583, acc 1
2016-09-06T13:15:54.051955: step 18405, loss 0.0020862, acc 1
2016-09-06T13:15:54.853402: step 18406, loss 0.0175853, acc 1
2016-09-06T13:15:55.657784: step 18407, loss 0.0023537, acc 1
2016-09-06T13:15:56.478621: step 18408, loss 0.00243683, acc 1
2016-09-06T13:15:57.332834: step 18409, loss 0.0360668, acc 0.96
2016-09-06T13:15:58.226850: step 18410, loss 0.00409761, acc 1
2016-09-06T13:15:59.039128: step 18411, loss 0.0216066, acc 0.98
2016-09-06T13:15:59.852274: step 18412, loss 0.00217436, acc 1
2016-09-06T13:16:00.721411: step 18413, loss 0.062526, acc 0.96
2016-09-06T13:16:01.535001: step 18414, loss 0.0167161, acc 1
2016-09-06T13:16:02.359720: step 18415, loss 0.00386469, acc 1
2016-09-06T13:16:03.191564: step 18416, loss 0.00163655, acc 1
2016-09-06T13:16:04.008871: step 18417, loss 0.00958284, acc 1
2016-09-06T13:16:04.818913: step 18418, loss 0.02944, acc 0.98
2016-09-06T13:16:05.679092: step 18419, loss 0.00529061, acc 1
2016-09-06T13:16:06.487382: step 18420, loss 0.0383383, acc 0.98
2016-09-06T13:16:07.272484: step 18421, loss 0.00730349, acc 1
2016-09-06T13:16:08.100555: step 18422, loss 0.0259042, acc 0.98
2016-09-06T13:16:08.932952: step 18423, loss 0.00233888, acc 1
2016-09-06T13:16:09.716695: step 18424, loss 0.00149221, acc 1
2016-09-06T13:16:10.532081: step 18425, loss 0.00688302, acc 1
2016-09-06T13:16:11.374945: step 18426, loss 0.0256103, acc 1
2016-09-06T13:16:12.185455: step 18427, loss 0.019372, acc 1
2016-09-06T13:16:13.020704: step 18428, loss 0.00443537, acc 1
2016-09-06T13:16:13.833131: step 18429, loss 0.027271, acc 0.98
2016-09-06T13:16:14.656685: step 18430, loss 0.00155742, acc 1
2016-09-06T13:16:15.462892: step 18431, loss 0.0201794, acc 1
2016-09-06T13:16:16.222202: step 18432, loss 0.0016408, acc 1
2016-09-06T13:16:17.000316: step 18433, loss 0.00598584, acc 1
2016-09-06T13:16:17.813076: step 18434, loss 0.0179183, acc 1
2016-09-06T13:16:18.648091: step 18435, loss 0.0017494, acc 1
2016-09-06T13:16:19.472536: step 18436, loss 0.0619193, acc 0.98
2016-09-06T13:16:20.269944: step 18437, loss 0.0393272, acc 0.98
2016-09-06T13:16:21.069543: step 18438, loss 0.0583329, acc 0.98
2016-09-06T13:16:21.841803: step 18439, loss 0.0800073, acc 0.98
2016-09-06T13:16:22.613293: step 18440, loss 0.00529594, acc 1
2016-09-06T13:16:23.422366: step 18441, loss 0.0324106, acc 0.98
2016-09-06T13:16:24.212214: step 18442, loss 0.0149228, acc 1
2016-09-06T13:16:25.030407: step 18443, loss 0.00169085, acc 1
2016-09-06T13:16:25.857950: step 18444, loss 0.00268521, acc 1
2016-09-06T13:16:26.650021: step 18445, loss 0.00360599, acc 1
2016-09-06T13:16:27.445477: step 18446, loss 0.029861, acc 0.98
2016-09-06T13:16:28.268411: step 18447, loss 0.0248678, acc 1
2016-09-06T13:16:29.048724: step 18448, loss 0.00455024, acc 1
2016-09-06T13:16:29.848156: step 18449, loss 0.0166738, acc 0.98
2016-09-06T13:16:30.659116: step 18450, loss 0.00191432, acc 1
2016-09-06T13:16:31.460125: step 18451, loss 0.00566496, acc 1
2016-09-06T13:16:32.256204: step 18452, loss 0.00205342, acc 1
2016-09-06T13:16:33.075068: step 18453, loss 0.00183937, acc 1
2016-09-06T13:16:33.846947: step 18454, loss 0.00186074, acc 1
2016-09-06T13:16:34.680340: step 18455, loss 0.00191833, acc 1
2016-09-06T13:16:35.498267: step 18456, loss 0.0221836, acc 1
2016-09-06T13:16:36.307794: step 18457, loss 0.0123409, acc 1
2016-09-06T13:16:37.113440: step 18458, loss 0.0225212, acc 0.98
2016-09-06T13:16:37.931040: step 18459, loss 0.02041, acc 0.98
2016-09-06T13:16:38.734775: step 18460, loss 0.00319082, acc 1
2016-09-06T13:16:39.554872: step 18461, loss 0.0228839, acc 0.98
2016-09-06T13:16:40.381304: step 18462, loss 0.00187481, acc 1
2016-09-06T13:16:41.175850: step 18463, loss 0.00180532, acc 1
2016-09-06T13:16:41.967219: step 18464, loss 0.0210566, acc 0.98
2016-09-06T13:16:42.784002: step 18465, loss 0.0782044, acc 0.98
2016-09-06T13:16:43.563438: step 18466, loss 0.0146815, acc 1
2016-09-06T13:16:44.374749: step 18467, loss 0.00183786, acc 1
2016-09-06T13:16:45.209431: step 18468, loss 0.00187896, acc 1
2016-09-06T13:16:45.990957: step 18469, loss 0.038882, acc 0.98
2016-09-06T13:16:46.782362: step 18470, loss 0.0154701, acc 1
2016-09-06T13:16:47.626147: step 18471, loss 0.0336278, acc 0.98
2016-09-06T13:16:48.432396: step 18472, loss 0.0049166, acc 1
2016-09-06T13:16:49.225433: step 18473, loss 0.0144035, acc 1
2016-09-06T13:16:50.049504: step 18474, loss 0.00507773, acc 1
2016-09-06T13:16:50.816106: step 18475, loss 0.0525772, acc 0.96
2016-09-06T13:16:51.613045: step 18476, loss 0.00207248, acc 1
2016-09-06T13:16:52.447498: step 18477, loss 0.00248405, acc 1
2016-09-06T13:16:53.230096: step 18478, loss 0.0252651, acc 0.98
2016-09-06T13:16:54.049477: step 18479, loss 0.0262826, acc 0.98
2016-09-06T13:16:54.863580: step 18480, loss 0.0519013, acc 0.98
2016-09-06T13:16:55.652912: step 18481, loss 0.0183572, acc 1
2016-09-06T13:16:56.482964: step 18482, loss 0.00254548, acc 1
2016-09-06T13:16:57.305705: step 18483, loss 0.0168411, acc 0.98
2016-09-06T13:16:58.101821: step 18484, loss 0.0357424, acc 0.98
2016-09-06T13:16:58.897323: step 18485, loss 0.0518665, acc 0.98
2016-09-06T13:16:59.712514: step 18486, loss 0.0165789, acc 1
2016-09-06T13:17:00.523121: step 18487, loss 0.00283539, acc 1
2016-09-06T13:17:01.324545: step 18488, loss 0.00195231, acc 1
2016-09-06T13:17:02.113460: step 18489, loss 0.00248113, acc 1
2016-09-06T13:17:02.901611: step 18490, loss 0.0112804, acc 1
2016-09-06T13:17:03.729322: step 18491, loss 0.00258563, acc 1
2016-09-06T13:17:04.526363: step 18492, loss 0.00325348, acc 1
2016-09-06T13:17:05.313289: step 18493, loss 0.0165578, acc 1
2016-09-06T13:17:06.119868: step 18494, loss 0.00156528, acc 1
2016-09-06T13:17:06.923248: step 18495, loss 0.00185761, acc 1
2016-09-06T13:17:07.721475: step 18496, loss 0.00563505, acc 1
2016-09-06T13:17:08.547053: step 18497, loss 0.00431421, acc 1
2016-09-06T13:17:09.379340: step 18498, loss 0.0101399, acc 1
2016-09-06T13:17:10.173856: step 18499, loss 0.0106348, acc 1
2016-09-06T13:17:11.003191: step 18500, loss 0.0205599, acc 0.98

Evaluation:
2016-09-06T13:17:14.767043: step 18500, loss 2.45679, acc 0.738274

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-18500

2016-09-06T13:17:16.789148: step 18501, loss 0.0155665, acc 1
2016-09-06T13:17:17.601551: step 18502, loss 0.00857841, acc 1
2016-09-06T13:17:18.425712: step 18503, loss 0.00212685, acc 1
2016-09-06T13:17:19.259905: step 18504, loss 0.00363089, acc 1
2016-09-06T13:17:20.072175: step 18505, loss 0.0360221, acc 0.98
2016-09-06T13:17:20.903506: step 18506, loss 0.00175964, acc 1
2016-09-06T13:17:21.723729: step 18507, loss 0.0163719, acc 0.98
2016-09-06T13:17:22.516891: step 18508, loss 0.0150419, acc 1
2016-09-06T13:17:23.327672: step 18509, loss 0.00409893, acc 1
2016-09-06T13:17:24.157114: step 18510, loss 0.0152309, acc 1
2016-09-06T13:17:24.977467: step 18511, loss 0.00585828, acc 1
2016-09-06T13:17:25.803010: step 18512, loss 0.00191338, acc 1
2016-09-06T13:17:26.625331: step 18513, loss 0.00189199, acc 1
2016-09-06T13:17:27.474416: step 18514, loss 0.0161664, acc 0.98
2016-09-06T13:17:28.266995: step 18515, loss 0.0286457, acc 0.98
2016-09-06T13:17:29.052118: step 18516, loss 0.00474014, acc 1
2016-09-06T13:17:29.865880: step 18517, loss 0.0917175, acc 0.96
2016-09-06T13:17:30.640244: step 18518, loss 0.0167099, acc 1
2016-09-06T13:17:31.474803: step 18519, loss 0.00368572, acc 1
2016-09-06T13:17:32.279364: step 18520, loss 0.0053453, acc 1
2016-09-06T13:17:33.072936: step 18521, loss 0.00332518, acc 1
2016-09-06T13:17:33.882425: step 18522, loss 0.00346969, acc 1
2016-09-06T13:17:34.711949: step 18523, loss 0.0257675, acc 0.98
2016-09-06T13:17:35.504016: step 18524, loss 0.00181033, acc 1
2016-09-06T13:17:36.330476: step 18525, loss 0.00443688, acc 1
2016-09-06T13:17:37.149533: step 18526, loss 0.00624832, acc 1
2016-09-06T13:17:37.942627: step 18527, loss 0.0163644, acc 0.98
2016-09-06T13:17:38.741918: step 18528, loss 0.00178363, acc 1
2016-09-06T13:17:39.587361: step 18529, loss 0.00202265, acc 1
2016-09-06T13:17:40.350235: step 18530, loss 0.00261269, acc 1
2016-09-06T13:17:41.195266: step 18531, loss 0.0241547, acc 0.98
2016-09-06T13:17:42.015176: step 18532, loss 0.00185144, acc 1
2016-09-06T13:17:42.819207: step 18533, loss 0.013275, acc 1
2016-09-06T13:17:43.619973: step 18534, loss 0.00506536, acc 1
2016-09-06T13:17:44.438300: step 18535, loss 0.0222912, acc 0.98
2016-09-06T13:17:45.238992: step 18536, loss 0.0400348, acc 0.98
2016-09-06T13:17:46.043642: step 18537, loss 0.0161269, acc 0.98
2016-09-06T13:17:46.880254: step 18538, loss 0.021215, acc 0.98
2016-09-06T13:17:47.684204: step 18539, loss 0.031703, acc 0.98
2016-09-06T13:17:48.489229: step 18540, loss 0.0141238, acc 1
2016-09-06T13:17:49.293576: step 18541, loss 0.0650315, acc 0.96
2016-09-06T13:17:50.116987: step 18542, loss 0.0381558, acc 0.98
2016-09-06T13:17:50.941906: step 18543, loss 0.00158196, acc 1
2016-09-06T13:17:51.772991: step 18544, loss 0.0395721, acc 0.98
2016-09-06T13:17:52.577011: step 18545, loss 0.0160532, acc 0.98
2016-09-06T13:17:53.410423: step 18546, loss 0.00159362, acc 1
2016-09-06T13:17:54.229105: step 18547, loss 0.0488256, acc 0.98
2016-09-06T13:17:55.040016: step 18548, loss 0.0341474, acc 0.98
2016-09-06T13:17:55.851691: step 18549, loss 0.012251, acc 1
2016-09-06T13:17:56.724900: step 18550, loss 0.00171961, acc 1
2016-09-06T13:17:57.546893: step 18551, loss 0.0436708, acc 0.98
2016-09-06T13:17:58.371543: step 18552, loss 0.00775787, acc 1
2016-09-06T13:17:59.198465: step 18553, loss 0.00152917, acc 1
2016-09-06T13:18:00.019173: step 18554, loss 0.0184717, acc 0.98
2016-09-06T13:18:00.872818: step 18555, loss 0.00290027, acc 1
2016-09-06T13:18:01.679248: step 18556, loss 0.00211316, acc 1
2016-09-06T13:18:02.481850: step 18557, loss 0.0166265, acc 0.98
2016-09-06T13:18:03.290526: step 18558, loss 0.00504951, acc 1
2016-09-06T13:18:04.091346: step 18559, loss 0.00754762, acc 1
2016-09-06T13:18:04.907200: step 18560, loss 0.00329688, acc 1
2016-09-06T13:18:05.717564: step 18561, loss 0.00180653, acc 1
2016-09-06T13:18:06.516478: step 18562, loss 0.0404699, acc 0.98
2016-09-06T13:18:07.346491: step 18563, loss 0.00254697, acc 1
2016-09-06T13:18:08.127701: step 18564, loss 0.0161653, acc 1
2016-09-06T13:18:08.942131: step 18565, loss 0.00286862, acc 1
2016-09-06T13:18:09.779144: step 18566, loss 0.0454697, acc 0.96
2016-09-06T13:18:10.565315: step 18567, loss 0.0327936, acc 0.98
2016-09-06T13:18:11.376266: step 18568, loss 0.0116218, acc 1
2016-09-06T13:18:12.222002: step 18569, loss 0.0108279, acc 1
2016-09-06T13:18:13.028916: step 18570, loss 0.0472962, acc 0.96
2016-09-06T13:18:13.838539: step 18571, loss 0.00747821, acc 1
2016-09-06T13:18:14.660503: step 18572, loss 0.00266857, acc 1
2016-09-06T13:18:15.461496: step 18573, loss 0.0157318, acc 1
2016-09-06T13:18:16.256917: step 18574, loss 0.00162802, acc 1
2016-09-06T13:18:17.143851: step 18575, loss 0.00198566, acc 1
2016-09-06T13:18:17.967368: step 18576, loss 0.0283229, acc 1
2016-09-06T13:18:18.763338: step 18577, loss 0.00502942, acc 1
2016-09-06T13:18:19.552526: step 18578, loss 0.122826, acc 0.98
2016-09-06T13:18:20.336686: step 18579, loss 0.0224513, acc 0.98
2016-09-06T13:18:21.178888: step 18580, loss 0.0235399, acc 0.98
2016-09-06T13:18:21.989896: step 18581, loss 0.041746, acc 0.98
2016-09-06T13:18:22.797722: step 18582, loss 0.017951, acc 1
2016-09-06T13:18:23.607305: step 18583, loss 0.00154953, acc 1
2016-09-06T13:18:24.436397: step 18584, loss 0.00150616, acc 1
2016-09-06T13:18:25.250649: step 18585, loss 0.0129806, acc 1
2016-09-06T13:18:26.065274: step 18586, loss 0.00598832, acc 1
2016-09-06T13:18:26.921712: step 18587, loss 0.00654419, acc 1
2016-09-06T13:18:27.754012: step 18588, loss 0.0132637, acc 1
2016-09-06T13:18:28.575342: step 18589, loss 0.00912149, acc 1
2016-09-06T13:18:29.405016: step 18590, loss 0.0145264, acc 1
2016-09-06T13:18:30.211945: step 18591, loss 0.0155907, acc 1
2016-09-06T13:18:31.023023: step 18592, loss 0.0239422, acc 1
2016-09-06T13:18:31.859333: step 18593, loss 0.016692, acc 1
2016-09-06T13:18:32.670228: step 18594, loss 0.044991, acc 0.96
2016-09-06T13:18:33.474246: step 18595, loss 0.00180605, acc 1
2016-09-06T13:18:34.300684: step 18596, loss 0.0165294, acc 0.98
2016-09-06T13:18:35.157811: step 18597, loss 0.0240791, acc 0.98
2016-09-06T13:18:35.940222: step 18598, loss 0.00440754, acc 1
2016-09-06T13:18:36.733355: step 18599, loss 0.055604, acc 0.98
2016-09-06T13:18:37.523160: step 18600, loss 0.0179249, acc 0.98

Evaluation:
2016-09-06T13:18:41.243248: step 18600, loss 2.56824, acc 0.733584

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-18600

2016-09-06T13:18:43.097056: step 18601, loss 0.0374328, acc 0.98
2016-09-06T13:18:43.940566: step 18602, loss 0.00918453, acc 1
2016-09-06T13:18:44.768498: step 18603, loss 0.00267351, acc 1
2016-09-06T13:18:45.584272: step 18604, loss 0.00197425, acc 1
2016-09-06T13:18:46.404161: step 18605, loss 0.00196759, acc 1
2016-09-06T13:18:47.204481: step 18606, loss 0.0197266, acc 0.98
2016-09-06T13:18:48.038459: step 18607, loss 0.0121991, acc 1
2016-09-06T13:18:48.845071: step 18608, loss 0.00200647, acc 1
2016-09-06T13:18:49.651437: step 18609, loss 0.0449909, acc 0.98
2016-09-06T13:18:50.455268: step 18610, loss 0.0394917, acc 0.98
2016-09-06T13:18:51.266888: step 18611, loss 0.0019695, acc 1
2016-09-06T13:18:52.072431: step 18612, loss 0.0314484, acc 0.98
2016-09-06T13:18:52.867908: step 18613, loss 0.00198977, acc 1
2016-09-06T13:18:53.669819: step 18614, loss 0.00243734, acc 1
2016-09-06T13:18:54.485743: step 18615, loss 0.0166686, acc 0.98
2016-09-06T13:18:55.277458: step 18616, loss 0.0019242, acc 1
2016-09-06T13:18:56.091130: step 18617, loss 0.00455387, acc 1
2016-09-06T13:18:56.892759: step 18618, loss 0.0163292, acc 0.98
2016-09-06T13:18:57.715924: step 18619, loss 0.0356577, acc 0.98
2016-09-06T13:18:58.502666: step 18620, loss 0.00190218, acc 1
2016-09-06T13:18:59.306174: step 18621, loss 0.0207655, acc 1
2016-09-06T13:19:00.142025: step 18622, loss 0.0169682, acc 1
2016-09-06T13:19:00.979888: step 18623, loss 0.00934497, acc 1
2016-09-06T13:19:01.756983: step 18624, loss 0.00248838, acc 1
2016-09-06T13:19:02.563092: step 18625, loss 0.00612172, acc 1
2016-09-06T13:19:03.395024: step 18626, loss 0.0101488, acc 1
2016-09-06T13:19:04.227607: step 18627, loss 0.00240129, acc 1
2016-09-06T13:19:05.007788: step 18628, loss 0.00377297, acc 1
2016-09-06T13:19:05.801499: step 18629, loss 0.0120463, acc 1
2016-09-06T13:19:06.641612: step 18630, loss 0.00908377, acc 1
2016-09-06T13:19:07.408246: step 18631, loss 0.0225328, acc 0.98
2016-09-06T13:19:08.215670: step 18632, loss 0.0373327, acc 0.96
2016-09-06T13:19:09.025790: step 18633, loss 0.0255629, acc 0.98
2016-09-06T13:19:09.813831: step 18634, loss 0.00202583, acc 1
2016-09-06T13:19:10.644906: step 18635, loss 0.0174934, acc 0.98
2016-09-06T13:19:11.474016: step 18636, loss 0.00201707, acc 1
2016-09-06T13:19:12.287035: step 18637, loss 0.0230209, acc 1
2016-09-06T13:19:13.099743: step 18638, loss 0.0250184, acc 1
2016-09-06T13:19:13.941254: step 18639, loss 0.0244236, acc 1
2016-09-06T13:19:14.766537: step 18640, loss 0.0142585, acc 1
2016-09-06T13:19:15.584668: step 18641, loss 0.00449526, acc 1
2016-09-06T13:19:16.417943: step 18642, loss 0.00202571, acc 1
2016-09-06T13:19:17.220175: step 18643, loss 0.0262056, acc 0.98
2016-09-06T13:19:18.007551: step 18644, loss 0.00203991, acc 1
2016-09-06T13:19:18.822168: step 18645, loss 0.00259791, acc 1
2016-09-06T13:19:19.619026: step 18646, loss 0.00683062, acc 1
2016-09-06T13:19:20.444561: step 18647, loss 0.00226037, acc 1
2016-09-06T13:19:21.280931: step 18648, loss 0.0262745, acc 0.98
2016-09-06T13:19:22.103864: step 18649, loss 0.00202304, acc 1
2016-09-06T13:19:22.917084: step 18650, loss 0.00275314, acc 1
2016-09-06T13:19:23.735011: step 18651, loss 0.0030173, acc 1
2016-09-06T13:19:24.543722: step 18652, loss 0.00538969, acc 1
2016-09-06T13:19:25.365096: step 18653, loss 0.0167171, acc 0.98
2016-09-06T13:19:26.215726: step 18654, loss 0.00571646, acc 1
2016-09-06T13:19:27.035363: step 18655, loss 0.00198394, acc 1
2016-09-06T13:19:27.849050: step 18656, loss 0.0288622, acc 0.98
2016-09-06T13:19:28.704588: step 18657, loss 0.0219644, acc 0.98
2016-09-06T13:19:29.546126: step 18658, loss 0.00393427, acc 1
2016-09-06T13:19:30.334284: step 18659, loss 0.00457264, acc 1
2016-09-06T13:19:31.224728: step 18660, loss 0.0037044, acc 1
2016-09-06T13:19:32.013473: step 18661, loss 0.108903, acc 0.98
2016-09-06T13:19:32.813722: step 18662, loss 0.00305264, acc 1
2016-09-06T13:19:33.603288: step 18663, loss 0.0180972, acc 1
2016-09-06T13:19:34.425703: step 18664, loss 0.00306497, acc 1
2016-09-06T13:19:35.214656: step 18665, loss 0.00654616, acc 1
2016-09-06T13:19:35.994501: step 18666, loss 0.00208262, acc 1
2016-09-06T13:19:36.812328: step 18667, loss 0.00985683, acc 1
2016-09-06T13:19:37.598762: step 18668, loss 0.0019086, acc 1
2016-09-06T13:19:38.434593: step 18669, loss 0.00169846, acc 1
2016-09-06T13:19:39.249998: step 18670, loss 0.00170608, acc 1
2016-09-06T13:19:40.086098: step 18671, loss 0.0359868, acc 0.98
2016-09-06T13:19:40.890383: step 18672, loss 0.00206566, acc 1
2016-09-06T13:19:41.684733: step 18673, loss 0.034734, acc 0.96
2016-09-06T13:19:42.451068: step 18674, loss 0.00165672, acc 1
2016-09-06T13:19:43.261336: step 18675, loss 0.00221928, acc 1
2016-09-06T13:19:44.059726: step 18676, loss 0.00822031, acc 1
2016-09-06T13:19:44.845366: step 18677, loss 0.0285685, acc 0.98
2016-09-06T13:19:45.653641: step 18678, loss 0.0154136, acc 1
2016-09-06T13:19:46.463565: step 18679, loss 0.00165586, acc 1
2016-09-06T13:19:47.234716: step 18680, loss 0.00201375, acc 1
2016-09-06T13:19:48.058196: step 18681, loss 0.00740747, acc 1
2016-09-06T13:19:48.940334: step 18682, loss 0.0316108, acc 0.98
2016-09-06T13:19:49.742233: step 18683, loss 0.00723514, acc 1
2016-09-06T13:19:50.554177: step 18684, loss 0.00156623, acc 1
2016-09-06T13:19:51.340414: step 18685, loss 0.00807955, acc 1
2016-09-06T13:19:52.136418: step 18686, loss 0.00155804, acc 1
2016-09-06T13:19:52.953724: step 18687, loss 0.00164061, acc 1
2016-09-06T13:19:53.792529: step 18688, loss 0.00365112, acc 1
2016-09-06T13:19:54.575918: step 18689, loss 0.00158366, acc 1
2016-09-06T13:19:55.355544: step 18690, loss 0.00183919, acc 1
2016-09-06T13:19:56.181825: step 18691, loss 0.0162443, acc 0.98
2016-09-06T13:19:56.989418: step 18692, loss 0.0259524, acc 0.98
2016-09-06T13:19:57.785016: step 18693, loss 0.00261014, acc 1
2016-09-06T13:19:58.611288: step 18694, loss 0.00151803, acc 1
2016-09-06T13:19:59.393960: step 18695, loss 0.0142797, acc 1
2016-09-06T13:20:00.225085: step 18696, loss 0.00243939, acc 1
2016-09-06T13:20:01.061436: step 18697, loss 0.0508566, acc 0.98
2016-09-06T13:20:01.847244: step 18698, loss 0.00148181, acc 1
2016-09-06T13:20:02.656072: step 18699, loss 0.0300226, acc 0.98
2016-09-06T13:20:03.477929: step 18700, loss 0.0136693, acc 1

Evaluation:
2016-09-06T13:20:07.174046: step 18700, loss 2.64355, acc 0.738274

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-18700

2016-09-06T13:20:09.026695: step 18701, loss 0.00357888, acc 1
2016-09-06T13:20:09.891264: step 18702, loss 0.0116959, acc 1
2016-09-06T13:20:10.691899: step 18703, loss 0.00175552, acc 1
2016-09-06T13:20:11.474506: step 18704, loss 0.00154193, acc 1
2016-09-06T13:20:12.318286: step 18705, loss 0.00158686, acc 1
2016-09-06T13:20:13.126113: step 18706, loss 0.0145782, acc 1
2016-09-06T13:20:13.923219: step 18707, loss 0.00886837, acc 1
2016-09-06T13:20:14.733870: step 18708, loss 0.0913183, acc 0.98
2016-09-06T13:20:15.520419: step 18709, loss 0.0125906, acc 1
2016-09-06T13:20:16.311381: step 18710, loss 0.0255331, acc 1
2016-09-06T13:20:17.136894: step 18711, loss 0.00180723, acc 1
2016-09-06T13:20:17.990636: step 18712, loss 0.0108598, acc 1
2016-09-06T13:20:18.789617: step 18713, loss 0.00655507, acc 1
2016-09-06T13:20:19.591007: step 18714, loss 0.0172868, acc 0.98
2016-09-06T13:20:20.388582: step 18715, loss 0.00350606, acc 1
2016-09-06T13:20:21.175563: step 18716, loss 0.00269009, acc 1
2016-09-06T13:20:21.967347: step 18717, loss 0.019893, acc 1
2016-09-06T13:20:22.781203: step 18718, loss 0.00857209, acc 1
2016-09-06T13:20:23.559849: step 18719, loss 0.0173372, acc 1
2016-09-06T13:20:24.350380: step 18720, loss 0.00167286, acc 1
2016-09-06T13:20:25.169746: step 18721, loss 0.00208029, acc 1
2016-09-06T13:20:25.983181: step 18722, loss 0.0112288, acc 1
2016-09-06T13:20:26.784885: step 18723, loss 0.0579737, acc 0.98
2016-09-06T13:20:27.623562: step 18724, loss 0.065575, acc 0.98
2016-09-06T13:20:28.380637: step 18725, loss 0.0116245, acc 1
2016-09-06T13:20:29.207195: step 18726, loss 0.00198733, acc 1
2016-09-06T13:20:30.041469: step 18727, loss 0.0141776, acc 1
2016-09-06T13:20:30.806881: step 18728, loss 0.00161089, acc 1
2016-09-06T13:20:31.624123: step 18729, loss 0.0214674, acc 1
2016-09-06T13:20:32.435168: step 18730, loss 0.00168301, acc 1
2016-09-06T13:20:33.261597: step 18731, loss 0.01826, acc 0.98
2016-09-06T13:20:34.083225: step 18732, loss 0.00677389, acc 1
2016-09-06T13:20:34.883895: step 18733, loss 0.023969, acc 0.98
2016-09-06T13:20:35.673122: step 18734, loss 0.00766984, acc 1
2016-09-06T13:20:36.445427: step 18735, loss 0.001939, acc 1
2016-09-06T13:20:37.293671: step 18736, loss 0.0427434, acc 0.98
2016-09-06T13:20:38.066754: step 18737, loss 0.0120783, acc 1
2016-09-06T13:20:38.864981: step 18738, loss 0.00200857, acc 1
2016-09-06T13:20:39.658748: step 18739, loss 0.0276396, acc 0.98
2016-09-06T13:20:40.433058: step 18740, loss 0.00220957, acc 1
2016-09-06T13:20:41.258663: step 18741, loss 0.0303224, acc 0.98
2016-09-06T13:20:42.133097: step 18742, loss 0.00213699, acc 1
2016-09-06T13:20:42.939196: step 18743, loss 0.00948497, acc 1
2016-09-06T13:20:43.727519: step 18744, loss 0.0507995, acc 0.98
2016-09-06T13:20:44.535000: step 18745, loss 0.0020875, acc 1
2016-09-06T13:20:45.336647: step 18746, loss 0.00288108, acc 1
2016-09-06T13:20:46.142012: step 18747, loss 0.00240039, acc 1
2016-09-06T13:20:46.974827: step 18748, loss 0.00792064, acc 1
2016-09-06T13:20:47.821309: step 18749, loss 0.0314905, acc 0.98
2016-09-06T13:20:48.676370: step 18750, loss 0.00217862, acc 1
2016-09-06T13:20:49.488628: step 18751, loss 0.0262626, acc 0.98
2016-09-06T13:20:50.288338: step 18752, loss 0.017813, acc 0.98
2016-09-06T13:20:51.124300: step 18753, loss 0.00223533, acc 1
2016-09-06T13:20:51.949728: step 18754, loss 0.00530587, acc 1
2016-09-06T13:20:52.763998: step 18755, loss 0.0103339, acc 1
2016-09-06T13:20:53.610424: step 18756, loss 0.00215311, acc 1
2016-09-06T13:20:54.439769: step 18757, loss 0.00779782, acc 1
2016-09-06T13:20:55.267670: step 18758, loss 0.0106567, acc 1
2016-09-06T13:20:56.096318: step 18759, loss 0.00244901, acc 1
2016-09-06T13:20:56.939536: step 18760, loss 0.00793856, acc 1
2016-09-06T13:20:57.773956: step 18761, loss 0.00876327, acc 1
2016-09-06T13:20:58.560869: step 18762, loss 0.0163689, acc 1
2016-09-06T13:20:59.368256: step 18763, loss 0.0228433, acc 0.98
2016-09-06T13:21:00.197243: step 18764, loss 0.00240708, acc 1
2016-09-06T13:21:01.032801: step 18765, loss 0.0064083, acc 1
2016-09-06T13:21:01.852435: step 18766, loss 0.00229483, acc 1
2016-09-06T13:21:02.667495: step 18767, loss 0.0128649, acc 1
2016-09-06T13:21:03.463783: step 18768, loss 0.0341143, acc 0.98
2016-09-06T13:21:04.293135: step 18769, loss 0.00241065, acc 1
2016-09-06T13:21:05.109134: step 18770, loss 0.00224943, acc 1
2016-09-06T13:21:05.911598: step 18771, loss 0.0155199, acc 1
2016-09-06T13:21:06.746480: step 18772, loss 0.00225369, acc 1
2016-09-06T13:21:07.593657: step 18773, loss 0.0184071, acc 0.98
2016-09-06T13:21:08.371355: step 18774, loss 0.100335, acc 0.98
2016-09-06T13:21:09.183219: step 18775, loss 0.00226026, acc 1
2016-09-06T13:21:10.039860: step 18776, loss 0.0442796, acc 0.98
2016-09-06T13:21:10.917752: step 18777, loss 0.00481617, acc 1
2016-09-06T13:21:11.718903: step 18778, loss 0.0020616, acc 1
2016-09-06T13:21:12.549570: step 18779, loss 0.00319431, acc 1
2016-09-06T13:21:13.368519: step 18780, loss 0.0216975, acc 0.98
2016-09-06T13:21:14.187345: step 18781, loss 0.00331897, acc 1
2016-09-06T13:21:15.044398: step 18782, loss 0.00923792, acc 1
2016-09-06T13:21:15.835766: step 18783, loss 0.00200203, acc 1
2016-09-06T13:21:16.669489: step 18784, loss 0.0279358, acc 1
2016-09-06T13:21:17.495297: step 18785, loss 0.0153338, acc 1
2016-09-06T13:21:18.302769: step 18786, loss 0.0168309, acc 0.98
2016-09-06T13:21:19.107612: step 18787, loss 0.00191386, acc 1
2016-09-06T13:21:19.937573: step 18788, loss 0.00190853, acc 1
2016-09-06T13:21:20.744781: step 18789, loss 0.00184748, acc 1
2016-09-06T13:21:21.528959: step 18790, loss 0.012663, acc 1
2016-09-06T13:21:22.345577: step 18791, loss 0.0153072, acc 1
2016-09-06T13:21:23.178585: step 18792, loss 0.0238844, acc 0.98
2016-09-06T13:21:23.951825: step 18793, loss 0.0194038, acc 0.98
2016-09-06T13:21:24.746392: step 18794, loss 0.0374079, acc 0.98
2016-09-06T13:21:25.566924: step 18795, loss 0.00223009, acc 1
2016-09-06T13:21:26.383840: step 18796, loss 0.0276947, acc 0.98
2016-09-06T13:21:27.194924: step 18797, loss 0.003457, acc 1
2016-09-06T13:21:28.006871: step 18798, loss 0.0204736, acc 0.98
2016-09-06T13:21:28.802823: step 18799, loss 0.0255163, acc 0.98
2016-09-06T13:21:29.611118: step 18800, loss 0.00163644, acc 1

Evaluation:
2016-09-06T13:21:33.322097: step 18800, loss 2.86834, acc 0.727955

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-18800

2016-09-06T13:21:35.354872: step 18801, loss 0.106967, acc 0.98
2016-09-06T13:21:36.148570: step 18802, loss 0.0987401, acc 0.96
2016-09-06T13:21:36.949612: step 18803, loss 0.00748692, acc 1
2016-09-06T13:21:37.786306: step 18804, loss 0.036009, acc 0.96
2016-09-06T13:21:38.596507: step 18805, loss 0.0036241, acc 1
2016-09-06T13:21:39.408744: step 18806, loss 0.00188614, acc 1
2016-09-06T13:21:40.251769: step 18807, loss 0.00163928, acc 1
2016-09-06T13:21:41.054111: step 18808, loss 0.00206956, acc 1
2016-09-06T13:21:41.871109: step 18809, loss 0.00277348, acc 1
2016-09-06T13:21:42.715345: step 18810, loss 0.0244341, acc 0.98
2016-09-06T13:21:43.538538: step 18811, loss 0.0121637, acc 1
2016-09-06T13:21:44.383231: step 18812, loss 0.0153577, acc 1
2016-09-06T13:21:45.177388: step 18813, loss 0.010878, acc 1
2016-09-06T13:21:46.012595: step 18814, loss 0.0130566, acc 1
2016-09-06T13:21:46.833673: step 18815, loss 0.0203834, acc 1
2016-09-06T13:21:47.661562: step 18816, loss 0.00220897, acc 1
2016-09-06T13:21:48.501445: step 18817, loss 0.0022793, acc 1
2016-09-06T13:21:49.310886: step 18818, loss 0.0303406, acc 0.98
2016-09-06T13:21:50.115723: step 18819, loss 0.0122703, acc 1
2016-09-06T13:21:50.898818: step 18820, loss 0.0177743, acc 0.98
2016-09-06T13:21:51.688417: step 18821, loss 0.0238785, acc 0.98
2016-09-06T13:21:52.534246: step 18822, loss 0.0128403, acc 1
2016-09-06T13:21:53.335122: step 18823, loss 0.00738402, acc 1
2016-09-06T13:21:54.134235: step 18824, loss 0.0260678, acc 1
2016-09-06T13:21:54.973515: step 18825, loss 0.0349002, acc 0.96
2016-09-06T13:21:55.819003: step 18826, loss 0.0718113, acc 0.98
2016-09-06T13:21:56.628777: step 18827, loss 0.00374981, acc 1
2016-09-06T13:21:57.429142: step 18828, loss 0.00275499, acc 1
2016-09-06T13:21:58.244175: step 18829, loss 0.0214117, acc 1
2016-09-06T13:21:59.048682: step 18830, loss 0.00703233, acc 1
2016-09-06T13:21:59.859802: step 18831, loss 0.0290134, acc 0.98
2016-09-06T13:22:00.693942: step 18832, loss 0.00968645, acc 1
2016-09-06T13:22:01.485984: step 18833, loss 0.0484883, acc 0.98
2016-09-06T13:22:02.256697: step 18834, loss 0.00286592, acc 1
2016-09-06T13:22:03.072283: step 18835, loss 0.0128237, acc 1
2016-09-06T13:22:03.894661: step 18836, loss 0.00285811, acc 1
2016-09-06T13:22:04.696779: step 18837, loss 0.00494421, acc 1
2016-09-06T13:22:05.505402: step 18838, loss 0.0158553, acc 1
2016-09-06T13:22:06.295710: step 18839, loss 0.00606891, acc 1
2016-09-06T13:22:07.121417: step 18840, loss 0.0278734, acc 1
2016-09-06T13:22:07.927450: step 18841, loss 0.00350357, acc 1
2016-09-06T13:22:08.707840: step 18842, loss 0.00472378, acc 1
2016-09-06T13:22:09.492185: step 18843, loss 0.003324, acc 1
2016-09-06T13:22:10.284271: step 18844, loss 0.00295108, acc 1
2016-09-06T13:22:11.086607: step 18845, loss 0.0101433, acc 1
2016-09-06T13:22:11.925693: step 18846, loss 0.00755914, acc 1
2016-09-06T13:22:12.751492: step 18847, loss 0.00296954, acc 1
2016-09-06T13:22:13.547714: step 18848, loss 0.00776146, acc 1
2016-09-06T13:22:14.371585: step 18849, loss 0.0173389, acc 1
2016-09-06T13:22:15.184335: step 18850, loss 0.0484129, acc 0.96
2016-09-06T13:22:15.948913: step 18851, loss 0.00853379, acc 1
2016-09-06T13:22:16.755490: step 18852, loss 0.00313233, acc 1
2016-09-06T13:22:17.601334: step 18853, loss 0.0180079, acc 0.98
2016-09-06T13:22:18.365859: step 18854, loss 0.0197854, acc 0.98
2016-09-06T13:22:19.185063: step 18855, loss 0.0029136, acc 1
2016-09-06T13:22:20.032643: step 18856, loss 0.0743093, acc 0.98
2016-09-06T13:22:20.857725: step 18857, loss 0.0157111, acc 1
2016-09-06T13:22:21.694466: step 18858, loss 0.00269799, acc 1
2016-09-06T13:22:22.544616: step 18859, loss 0.00395707, acc 1
2016-09-06T13:22:23.352698: step 18860, loss 0.00259013, acc 1
2016-09-06T13:22:24.173142: step 18861, loss 0.00309458, acc 1
2016-09-06T13:22:24.995998: step 18862, loss 0.0025972, acc 1
2016-09-06T13:22:25.792738: step 18863, loss 0.0268058, acc 0.98
2016-09-06T13:22:26.594795: step 18864, loss 0.0106841, acc 1
2016-09-06T13:22:27.428350: step 18865, loss 0.00273092, acc 1
2016-09-06T13:22:28.289773: step 18866, loss 0.00542653, acc 1
2016-09-06T13:22:29.152835: step 18867, loss 0.0374254, acc 0.98
2016-09-06T13:22:30.007284: step 18868, loss 0.0136554, acc 1
2016-09-06T13:22:30.821563: step 18869, loss 0.0195897, acc 1
2016-09-06T13:22:31.620307: step 18870, loss 0.0220634, acc 0.98
2016-09-06T13:22:32.437579: step 18871, loss 0.00533664, acc 1
2016-09-06T13:22:33.259612: step 18872, loss 0.00242384, acc 1
2016-09-06T13:22:34.062238: step 18873, loss 0.00365552, acc 1
2016-09-06T13:22:34.858079: step 18874, loss 0.00430597, acc 1
2016-09-06T13:22:35.670721: step 18875, loss 0.00355119, acc 1
2016-09-06T13:22:36.434599: step 18876, loss 0.00253108, acc 1
2016-09-06T13:22:37.275661: step 18877, loss 0.0248774, acc 1
2016-09-06T13:22:38.133533: step 18878, loss 0.00249452, acc 1
2016-09-06T13:22:38.930418: step 18879, loss 0.00240424, acc 1
2016-09-06T13:22:39.734066: step 18880, loss 0.0131078, acc 1
2016-09-06T13:22:40.563739: step 18881, loss 0.00244184, acc 1
2016-09-06T13:22:41.314040: step 18882, loss 0.00288177, acc 1
2016-09-06T13:22:42.121539: step 18883, loss 0.0127228, acc 1
2016-09-06T13:22:42.942884: step 18884, loss 0.035756, acc 0.98
2016-09-06T13:22:43.737851: step 18885, loss 0.0183497, acc 0.98
2016-09-06T13:22:44.529150: step 18886, loss 0.00424343, acc 1
2016-09-06T13:22:45.361812: step 18887, loss 0.0129768, acc 1
2016-09-06T13:22:46.169268: step 18888, loss 0.030917, acc 0.98
2016-09-06T13:22:46.985000: step 18889, loss 0.0880784, acc 0.94
2016-09-06T13:22:47.818470: step 18890, loss 0.00228335, acc 1
2016-09-06T13:22:48.613293: step 18891, loss 0.0124957, acc 1
2016-09-06T13:22:49.415030: step 18892, loss 0.0166173, acc 0.98
2016-09-06T13:22:50.214899: step 18893, loss 0.00225703, acc 1
2016-09-06T13:22:51.061341: step 18894, loss 0.00451242, acc 1
2016-09-06T13:22:51.869014: step 18895, loss 0.0600113, acc 0.98
2016-09-06T13:22:52.688223: step 18896, loss 0.0159618, acc 1
2016-09-06T13:22:53.530237: step 18897, loss 0.00253486, acc 1
2016-09-06T13:22:54.367685: step 18898, loss 0.00267044, acc 1
2016-09-06T13:22:55.191887: step 18899, loss 0.0253101, acc 0.98
2016-09-06T13:22:56.007520: step 18900, loss 0.00193115, acc 1

Evaluation:
2016-09-06T13:22:59.774227: step 18900, loss 2.85769, acc 0.737336

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-18900

2016-09-06T13:23:01.693280: step 18901, loss 0.0375095, acc 0.96
2016-09-06T13:23:02.539680: step 18902, loss 0.0256588, acc 0.98
2016-09-06T13:23:03.380412: step 18903, loss 0.0441695, acc 0.98
2016-09-06T13:23:04.177494: step 18904, loss 0.00411186, acc 1
2016-09-06T13:23:05.015387: step 18905, loss 0.0330486, acc 0.98
2016-09-06T13:23:05.826233: step 18906, loss 0.00178778, acc 1
2016-09-06T13:23:06.632025: step 18907, loss 0.014142, acc 1
2016-09-06T13:23:07.485669: step 18908, loss 0.0221608, acc 0.98
2016-09-06T13:23:08.282205: step 18909, loss 0.0166838, acc 1
2016-09-06T13:23:09.068891: step 18910, loss 0.00175161, acc 1
2016-09-06T13:23:09.887582: step 18911, loss 0.00175936, acc 1
2016-09-06T13:23:10.692023: step 18912, loss 0.00180919, acc 1
2016-09-06T13:23:11.504277: step 18913, loss 0.0105011, acc 1
2016-09-06T13:23:12.333097: step 18914, loss 0.0308038, acc 0.98
2016-09-06T13:23:13.141405: step 18915, loss 0.0151874, acc 1
2016-09-06T13:23:13.948468: step 18916, loss 0.0180319, acc 0.98
2016-09-06T13:23:14.763527: step 18917, loss 0.00639077, acc 1
2016-09-06T13:23:15.589991: step 18918, loss 0.00236945, acc 1
2016-09-06T13:23:16.378116: step 18919, loss 0.0104091, acc 1
2016-09-06T13:23:17.207819: step 18920, loss 0.00173953, acc 1
2016-09-06T13:23:18.022338: step 18921, loss 0.0146911, acc 1
2016-09-06T13:23:18.813886: step 18922, loss 0.00260511, acc 1
2016-09-06T13:23:19.613756: step 18923, loss 0.0246052, acc 0.98
2016-09-06T13:23:20.421902: step 18924, loss 0.0303296, acc 0.98
2016-09-06T13:23:21.215088: step 18925, loss 0.0516355, acc 0.96
2016-09-06T13:23:22.026568: step 18926, loss 0.0177746, acc 0.98
2016-09-06T13:23:22.853749: step 18927, loss 0.00180993, acc 1
2016-09-06T13:23:23.644950: step 18928, loss 0.0166265, acc 1
2016-09-06T13:23:24.432952: step 18929, loss 0.0518341, acc 0.98
2016-09-06T13:23:25.249836: step 18930, loss 0.00709077, acc 1
2016-09-06T13:23:26.022550: step 18931, loss 0.00633374, acc 1
2016-09-06T13:23:26.861494: step 18932, loss 0.00251038, acc 1
2016-09-06T13:23:27.695241: step 18933, loss 0.00681727, acc 1
2016-09-06T13:23:28.490663: step 18934, loss 0.00198821, acc 1
2016-09-06T13:23:29.308679: step 18935, loss 0.0245411, acc 1
2016-09-06T13:23:30.147055: step 18936, loss 0.0397845, acc 0.98
2016-09-06T13:23:30.937062: step 18937, loss 0.00212176, acc 1
2016-09-06T13:23:31.720672: step 18938, loss 0.0266074, acc 0.98
2016-09-06T13:23:32.578476: step 18939, loss 0.00266265, acc 1
2016-09-06T13:23:33.374122: step 18940, loss 0.00225146, acc 1
2016-09-06T13:23:34.179781: step 18941, loss 0.0245375, acc 0.98
2016-09-06T13:23:34.971211: step 18942, loss 0.00213807, acc 1
2016-09-06T13:23:35.751347: step 18943, loss 0.0188153, acc 1
2016-09-06T13:23:36.565421: step 18944, loss 0.0118313, acc 1
2016-09-06T13:23:37.401126: step 18945, loss 0.00239216, acc 1
2016-09-06T13:23:38.186617: step 18946, loss 0.0456911, acc 0.98
2016-09-06T13:23:39.077200: step 18947, loss 0.00512303, acc 1
2016-09-06T13:23:39.915729: step 18948, loss 0.0409232, acc 0.98
2016-09-06T13:23:40.711396: step 18949, loss 0.0184909, acc 0.98
2016-09-06T13:23:41.518988: step 18950, loss 0.00437835, acc 1
2016-09-06T13:23:42.338686: step 18951, loss 0.030324, acc 1
2016-09-06T13:23:43.156412: step 18952, loss 0.00248602, acc 1
2016-09-06T13:23:43.983809: step 18953, loss 0.010248, acc 1
2016-09-06T13:23:44.841444: step 18954, loss 0.0272916, acc 0.98
2016-09-06T13:23:45.717667: step 18955, loss 0.00218306, acc 1
2016-09-06T13:23:46.574362: step 18956, loss 0.0165168, acc 1
2016-09-06T13:23:47.404633: step 18957, loss 0.00295907, acc 1
2016-09-06T13:23:48.223040: step 18958, loss 0.00373011, acc 1
2016-09-06T13:23:49.117589: step 18959, loss 0.0243375, acc 0.98
2016-09-06T13:23:49.915694: step 18960, loss 0.00498825, acc 1
2016-09-06T13:23:50.763926: step 18961, loss 0.00277319, acc 1
2016-09-06T13:23:51.610610: step 18962, loss 0.0170637, acc 0.98
2016-09-06T13:23:52.420137: step 18963, loss 0.00864259, acc 1
2016-09-06T13:23:53.249787: step 18964, loss 0.00346871, acc 1
2016-09-06T13:23:54.140267: step 18965, loss 0.00935164, acc 1
2016-09-06T13:23:54.990337: step 18966, loss 0.0225333, acc 1
2016-09-06T13:23:55.832045: step 18967, loss 0.00218605, acc 1
2016-09-06T13:23:56.666703: step 18968, loss 0.00221654, acc 1
2016-09-06T13:23:57.491230: step 18969, loss 0.00221868, acc 1
2016-09-06T13:23:58.294258: step 18970, loss 0.00221489, acc 1
2016-09-06T13:23:59.114061: step 18971, loss 0.0155843, acc 1
2016-09-06T13:23:59.918852: step 18972, loss 0.0135167, acc 1
2016-09-06T13:24:00.732620: step 18973, loss 0.00238942, acc 1
2016-09-06T13:24:01.584406: step 18974, loss 0.0203118, acc 0.98
2016-09-06T13:24:02.405530: step 18975, loss 0.00223812, acc 1
2016-09-06T13:24:03.220302: step 18976, loss 0.049851, acc 0.94
2016-09-06T13:24:04.218447: step 18977, loss 0.0580947, acc 0.96
2016-09-06T13:24:05.093396: step 18978, loss 0.0163093, acc 0.98
2016-09-06T13:24:05.965444: step 18979, loss 0.00226364, acc 1
2016-09-06T13:24:06.770398: step 18980, loss 0.0166979, acc 0.98
2016-09-06T13:24:07.622715: step 18981, loss 0.00234352, acc 1
2016-09-06T13:24:08.428158: step 18982, loss 0.0291052, acc 1
2016-09-06T13:24:09.250641: step 18983, loss 0.00649556, acc 1
2016-09-06T13:24:10.089553: step 18984, loss 0.00212268, acc 1
2016-09-06T13:24:10.955423: step 18985, loss 0.0410636, acc 0.98
2016-09-06T13:24:11.779255: step 18986, loss 0.0020772, acc 1
2016-09-06T13:24:12.594529: step 18987, loss 0.00367857, acc 1
2016-09-06T13:24:13.437076: step 18988, loss 0.00595838, acc 1
2016-09-06T13:24:14.228230: step 18989, loss 0.0318888, acc 0.98
2016-09-06T13:24:15.072802: step 18990, loss 0.023221, acc 1
2016-09-06T13:24:15.875754: step 18991, loss 0.00374678, acc 1
2016-09-06T13:24:16.685489: step 18992, loss 0.00203017, acc 1
2016-09-06T13:24:17.526887: step 18993, loss 0.0300474, acc 0.98
2016-09-06T13:24:18.347129: step 18994, loss 0.0193465, acc 1
2016-09-06T13:24:19.200353: step 18995, loss 0.0174534, acc 1
2016-09-06T13:24:20.027542: step 18996, loss 0.00198214, acc 1
2016-09-06T13:24:20.867102: step 18997, loss 0.00196047, acc 1
2016-09-06T13:24:21.685941: step 18998, loss 0.00194288, acc 1
2016-09-06T13:24:22.509675: step 18999, loss 0.00237388, acc 1
2016-09-06T13:24:23.297942: step 19000, loss 0.00206906, acc 1

Evaluation:
2016-09-06T13:24:26.976761: step 19000, loss 3.59298, acc 0.741088

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-19000

2016-09-06T13:24:28.888662: step 19001, loss 0.00230095, acc 1
2016-09-06T13:24:29.717228: step 19002, loss 0.0116368, acc 1
2016-09-06T13:24:30.534817: step 19003, loss 0.0177928, acc 0.98
2016-09-06T13:24:31.353635: step 19004, loss 0.00191323, acc 1
2016-09-06T13:24:32.169031: step 19005, loss 0.010934, acc 1
2016-09-06T13:24:33.037403: step 19006, loss 0.0018313, acc 1
2016-09-06T13:24:33.824029: step 19007, loss 0.00216158, acc 1
2016-09-06T13:24:34.597394: step 19008, loss 0.00180525, acc 1
2016-09-06T13:24:35.433864: step 19009, loss 0.0024677, acc 1
2016-09-06T13:24:36.230910: step 19010, loss 0.00217105, acc 1
2016-09-06T13:24:37.054022: step 19011, loss 0.00177257, acc 1
2016-09-06T13:24:37.869139: step 19012, loss 0.00176069, acc 1
2016-09-06T13:24:38.660513: step 19013, loss 0.0101007, acc 1
2016-09-06T13:24:39.477011: step 19014, loss 0.00775982, acc 1
2016-09-06T13:24:40.281913: step 19015, loss 0.0987069, acc 0.98
2016-09-06T13:24:41.117017: step 19016, loss 0.0148521, acc 1
2016-09-06T13:24:41.905837: step 19017, loss 0.0409907, acc 0.98
2016-09-06T13:24:42.793543: step 19018, loss 0.00177797, acc 1
2016-09-06T13:24:43.593464: step 19019, loss 0.00771873, acc 1
2016-09-06T13:24:44.395506: step 19020, loss 0.00276473, acc 1
2016-09-06T13:24:45.179037: step 19021, loss 0.00183943, acc 1
2016-09-06T13:24:45.979069: step 19022, loss 0.0251577, acc 0.98
2016-09-06T13:24:46.780100: step 19023, loss 0.00196887, acc 1
2016-09-06T13:24:47.581297: step 19024, loss 0.00177737, acc 1
2016-09-06T13:24:48.392180: step 19025, loss 0.00358308, acc 1
2016-09-06T13:24:49.256382: step 19026, loss 0.00239681, acc 1
2016-09-06T13:24:50.086599: step 19027, loss 0.00903613, acc 1
2016-09-06T13:24:50.901007: step 19028, loss 0.00171856, acc 1
2016-09-06T13:24:51.729291: step 19029, loss 0.0354738, acc 0.96
2016-09-06T13:24:52.554007: step 19030, loss 0.0017244, acc 1
2016-09-06T13:24:53.355897: step 19031, loss 0.00176136, acc 1
2016-09-06T13:24:54.174238: step 19032, loss 0.00256156, acc 1
2016-09-06T13:24:55.019170: step 19033, loss 0.00197923, acc 1
2016-09-06T13:24:55.891849: step 19034, loss 0.0564177, acc 0.98
2016-09-06T13:24:56.735733: step 19035, loss 0.00745256, acc 1
2016-09-06T13:24:57.708410: step 19036, loss 0.00224017, acc 1
2016-09-06T13:24:58.599767: step 19037, loss 0.00248556, acc 1
2016-09-06T13:24:59.505632: step 19038, loss 0.0309094, acc 1
2016-09-06T13:25:00.421021: step 19039, loss 0.0278801, acc 0.98
2016-09-06T13:25:01.388801: step 19040, loss 0.0295771, acc 0.98
2016-09-06T13:25:02.388959: step 19041, loss 0.0518089, acc 0.96
2016-09-06T13:25:03.259593: step 19042, loss 0.00971483, acc 1
2016-09-06T13:25:04.096805: step 19043, loss 0.00333034, acc 1
2016-09-06T13:25:05.054540: step 19044, loss 0.00484772, acc 1
2016-09-06T13:25:06.016128: step 19045, loss 0.0256107, acc 0.98
2016-09-06T13:25:07.164148: step 19046, loss 0.013189, acc 1
2016-09-06T13:25:08.077537: step 19047, loss 0.0542717, acc 0.98
2016-09-06T13:25:09.130106: step 19048, loss 0.0294749, acc 0.98
2016-09-06T13:25:10.040598: step 19049, loss 0.00446613, acc 1
2016-09-06T13:25:11.069402: step 19050, loss 0.00359552, acc 1
2016-09-06T13:25:11.926253: step 19051, loss 0.00355605, acc 1
2016-09-06T13:25:12.795935: step 19052, loss 0.00358643, acc 1
2016-09-06T13:25:13.720455: step 19053, loss 0.0150009, acc 1
2016-09-06T13:25:14.621195: step 19054, loss 0.00398475, acc 1
2016-09-06T13:25:15.505827: step 19055, loss 0.00408574, acc 1
2016-09-06T13:25:16.346185: step 19056, loss 0.00391018, acc 1
2016-09-06T13:25:17.281265: step 19057, loss 0.00398993, acc 1
2016-09-06T13:25:18.238789: step 19058, loss 0.0655929, acc 0.98
2016-09-06T13:25:19.127883: step 19059, loss 0.0232226, acc 0.98
2016-09-06T13:25:19.996397: step 19060, loss 0.019859, acc 0.98
2016-09-06T13:25:20.933467: step 19061, loss 0.0196021, acc 0.98
2016-09-06T13:25:21.876733: step 19062, loss 0.00364572, acc 1
2016-09-06T13:25:22.779249: step 19063, loss 0.0111417, acc 1
2016-09-06T13:25:23.588409: step 19064, loss 0.00360294, acc 1
2016-09-06T13:25:24.428420: step 19065, loss 0.0223448, acc 0.98
2016-09-06T13:25:25.403960: step 19066, loss 0.0929149, acc 0.98
2016-09-06T13:25:26.317679: step 19067, loss 0.0240889, acc 0.98
2016-09-06T13:25:27.147794: step 19068, loss 0.00339193, acc 1
2016-09-06T13:25:27.965437: step 19069, loss 0.00885812, acc 1
2016-09-06T13:25:28.810226: step 19070, loss 0.00329106, acc 1
2016-09-06T13:25:29.627989: step 19071, loss 0.00324893, acc 1
2016-09-06T13:25:30.544912: step 19072, loss 0.00880453, acc 1
2016-09-06T13:25:31.468663: step 19073, loss 0.0510721, acc 0.98
2016-09-06T13:25:32.313276: step 19074, loss 0.00346664, acc 1
2016-09-06T13:25:33.181470: step 19075, loss 0.0174882, acc 0.98
2016-09-06T13:25:34.026819: step 19076, loss 0.0425706, acc 0.98
2016-09-06T13:25:34.865732: step 19077, loss 0.0196364, acc 0.98
2016-09-06T13:25:35.727641: step 19078, loss 0.00565526, acc 1
2016-09-06T13:25:36.563948: step 19079, loss 0.00431885, acc 1
2016-09-06T13:25:37.381559: step 19080, loss 0.00300563, acc 1
2016-09-06T13:25:38.185059: step 19081, loss 0.0225212, acc 0.98
2016-09-06T13:25:38.990665: step 19082, loss 0.00287509, acc 1
2016-09-06T13:25:39.795059: step 19083, loss 0.0722773, acc 0.94
2016-09-06T13:25:40.616601: step 19084, loss 0.0138939, acc 1
2016-09-06T13:25:41.431582: step 19085, loss 0.00273765, acc 1
2016-09-06T13:25:42.247495: step 19086, loss 0.0184005, acc 0.98
2016-09-06T13:25:43.049196: step 19087, loss 0.00305749, acc 1
2016-09-06T13:25:43.869824: step 19088, loss 0.00511849, acc 1
2016-09-06T13:25:44.734690: step 19089, loss 0.00263658, acc 1
2016-09-06T13:25:45.561843: step 19090, loss 0.0466996, acc 0.98
2016-09-06T13:25:46.351289: step 19091, loss 0.0154917, acc 1
2016-09-06T13:25:47.167222: step 19092, loss 0.00252749, acc 1
2016-09-06T13:25:47.957407: step 19093, loss 0.0195206, acc 0.98
2016-09-06T13:25:48.749495: step 19094, loss 0.0267548, acc 0.98
2016-09-06T13:25:49.587267: step 19095, loss 0.00338822, acc 1
2016-09-06T13:25:50.405776: step 19096, loss 0.0226152, acc 0.98
2016-09-06T13:25:51.192639: step 19097, loss 0.0439732, acc 0.98
2016-09-06T13:25:52.012134: step 19098, loss 0.0407236, acc 0.96
2016-09-06T13:25:52.831454: step 19099, loss 0.0178596, acc 1
2016-09-06T13:25:53.628706: step 19100, loss 0.00388179, acc 1

Evaluation:
2016-09-06T13:25:57.342771: step 19100, loss 2.56083, acc 0.730769

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-19100

2016-09-06T13:25:59.287297: step 19101, loss 0.00259965, acc 1
2016-09-06T13:26:00.105405: step 19102, loss 0.00228, acc 1
2016-09-06T13:26:00.923975: step 19103, loss 0.16953, acc 0.98
2016-09-06T13:26:01.733262: step 19104, loss 0.00786425, acc 1
2016-09-06T13:26:02.545051: step 19105, loss 0.0184663, acc 0.98
2016-09-06T13:26:03.353701: step 19106, loss 0.0426504, acc 0.98
2016-09-06T13:26:04.142032: step 19107, loss 0.00246261, acc 1
2016-09-06T13:26:04.971377: step 19108, loss 0.0029288, acc 1
2016-09-06T13:26:05.774940: step 19109, loss 0.00389449, acc 1
2016-09-06T13:26:06.603482: step 19110, loss 0.023892, acc 0.98
2016-09-06T13:26:07.428456: step 19111, loss 0.0116226, acc 1
2016-09-06T13:26:08.202574: step 19112, loss 0.00271525, acc 1
2016-09-06T13:26:09.015161: step 19113, loss 0.0184902, acc 1
2016-09-06T13:26:09.838657: step 19114, loss 0.0086417, acc 1
2016-09-06T13:26:10.630710: step 19115, loss 0.0199862, acc 0.98
2016-09-06T13:26:11.416087: step 19116, loss 0.0180169, acc 1
2016-09-06T13:26:12.244675: step 19117, loss 0.0233545, acc 0.98
2016-09-06T13:26:13.026666: step 19118, loss 0.0277368, acc 1
2016-09-06T13:26:13.813579: step 19119, loss 0.0135195, acc 1
2016-09-06T13:26:14.615788: step 19120, loss 0.0395889, acc 0.98
2016-09-06T13:26:15.410532: step 19121, loss 0.0293269, acc 1
2016-09-06T13:26:16.207044: step 19122, loss 0.00221923, acc 1
2016-09-06T13:26:17.003995: step 19123, loss 0.00726414, acc 1
2016-09-06T13:26:17.801426: step 19124, loss 0.00326318, acc 1
2016-09-06T13:26:18.613906: step 19125, loss 0.0226146, acc 0.98
2016-09-06T13:26:19.419802: step 19126, loss 0.00235783, acc 1
2016-09-06T13:26:20.251275: step 19127, loss 0.0428523, acc 0.96
2016-09-06T13:26:21.038999: step 19128, loss 0.00270084, acc 1
2016-09-06T13:26:21.841124: step 19129, loss 0.00239754, acc 1
2016-09-06T13:26:22.620546: step 19130, loss 0.0024017, acc 1
2016-09-06T13:26:23.437762: step 19131, loss 0.00239183, acc 1
2016-09-06T13:26:24.269499: step 19132, loss 0.00305134, acc 1
2016-09-06T13:26:25.053317: step 19133, loss 0.0075844, acc 1
2016-09-06T13:26:25.862025: step 19134, loss 0.0531338, acc 0.96
2016-09-06T13:26:26.655543: step 19135, loss 0.016014, acc 1
2016-09-06T13:26:27.458677: step 19136, loss 0.0056961, acc 1
2016-09-06T13:26:28.285061: step 19137, loss 0.00258557, acc 1
2016-09-06T13:26:29.111813: step 19138, loss 0.00776798, acc 1
2016-09-06T13:26:29.894947: step 19139, loss 0.0339961, acc 0.98
2016-09-06T13:26:30.705847: step 19140, loss 0.00855864, acc 1
2016-09-06T13:26:31.522542: step 19141, loss 0.0136485, acc 1
2016-09-06T13:26:32.313646: step 19142, loss 0.0458382, acc 0.98
2016-09-06T13:26:33.111253: step 19143, loss 0.0217767, acc 1
2016-09-06T13:26:33.924270: step 19144, loss 0.0170201, acc 0.98
2016-09-06T13:26:34.701133: step 19145, loss 0.00434446, acc 1
2016-09-06T13:26:35.510934: step 19146, loss 0.011772, acc 1
2016-09-06T13:26:36.314483: step 19147, loss 0.00465601, acc 1
2016-09-06T13:26:37.137795: step 19148, loss 0.00227029, acc 1
2016-09-06T13:26:37.938560: step 19149, loss 0.0146934, acc 1
2016-09-06T13:26:38.741923: step 19150, loss 0.00344854, acc 1
2016-09-06T13:26:39.560169: step 19151, loss 0.0303368, acc 0.98
2016-09-06T13:26:40.359195: step 19152, loss 0.0225422, acc 1
2016-09-06T13:26:41.177489: step 19153, loss 0.00403517, acc 1
2016-09-06T13:26:41.966115: step 19154, loss 0.0024187, acc 1
2016-09-06T13:26:42.781894: step 19155, loss 0.0648895, acc 0.98
2016-09-06T13:26:43.623916: step 19156, loss 0.00419712, acc 1
2016-09-06T13:26:44.426088: step 19157, loss 0.00209387, acc 1
2016-09-06T13:26:45.230600: step 19158, loss 0.00203612, acc 1
2016-09-06T13:26:46.084068: step 19159, loss 0.00217226, acc 1
2016-09-06T13:26:46.897219: step 19160, loss 0.00208346, acc 1
2016-09-06T13:26:47.745839: step 19161, loss 0.00482543, acc 1
2016-09-06T13:26:48.609447: step 19162, loss 0.0114178, acc 1
2016-09-06T13:26:49.429338: step 19163, loss 0.0036811, acc 1
2016-09-06T13:26:50.251007: step 19164, loss 0.0113538, acc 1
2016-09-06T13:26:51.120596: step 19165, loss 0.106701, acc 0.98
2016-09-06T13:26:51.942328: step 19166, loss 0.0046589, acc 1
2016-09-06T13:26:52.753152: step 19167, loss 0.0264856, acc 0.98
2016-09-06T13:26:53.586346: step 19168, loss 0.0167751, acc 1
2016-09-06T13:26:54.401697: step 19169, loss 0.0214354, acc 0.98
2016-09-06T13:26:55.232580: step 19170, loss 0.0188213, acc 1
2016-09-06T13:26:56.069614: step 19171, loss 0.00450669, acc 1
2016-09-06T13:26:56.894021: step 19172, loss 0.00254021, acc 1
2016-09-06T13:26:57.691502: step 19173, loss 0.00222314, acc 1
2016-09-06T13:26:58.487498: step 19174, loss 0.0346105, acc 0.98
2016-09-06T13:26:59.311138: step 19175, loss 0.00278234, acc 1
2016-09-06T13:27:00.088117: step 19176, loss 0.158643, acc 0.98
2016-09-06T13:27:00.939746: step 19177, loss 0.0472794, acc 0.98
2016-09-06T13:27:01.747063: step 19178, loss 0.0246342, acc 0.98
2016-09-06T13:27:02.533720: step 19179, loss 0.00923389, acc 1
2016-09-06T13:27:03.329516: step 19180, loss 0.00223981, acc 1
2016-09-06T13:27:04.140718: step 19181, loss 0.0313284, acc 0.98
2016-09-06T13:27:04.916949: step 19182, loss 0.0208183, acc 1
2016-09-06T13:27:05.723074: step 19183, loss 0.0426463, acc 0.98
2016-09-06T13:27:06.543757: step 19184, loss 0.00373249, acc 1
2016-09-06T13:27:07.341574: step 19185, loss 0.0108367, acc 1
2016-09-06T13:27:08.174387: step 19186, loss 0.0347662, acc 1
2016-09-06T13:27:08.981590: step 19187, loss 0.0212426, acc 1
2016-09-06T13:27:09.773417: step 19188, loss 0.0486111, acc 0.96
2016-09-06T13:27:10.585248: step 19189, loss 0.00303465, acc 1
2016-09-06T13:27:11.399432: step 19190, loss 0.00295298, acc 1
2016-09-06T13:27:12.183445: step 19191, loss 0.00483982, acc 1
2016-09-06T13:27:13.003280: step 19192, loss 0.00481243, acc 1
2016-09-06T13:27:13.825460: step 19193, loss 0.0172134, acc 0.98
2016-09-06T13:27:14.728918: step 19194, loss 0.0169934, acc 0.98
2016-09-06T13:27:15.533784: step 19195, loss 0.0166762, acc 1
2016-09-06T13:27:16.367243: step 19196, loss 0.00535274, acc 1
2016-09-06T13:27:17.198974: step 19197, loss 0.00807403, acc 1
2016-09-06T13:27:18.013756: step 19198, loss 0.00298949, acc 1
2016-09-06T13:27:18.825671: step 19199, loss 0.0217231, acc 0.98
2016-09-06T13:27:19.574800: step 19200, loss 0.00269959, acc 1

Evaluation:
2016-09-06T13:27:23.297386: step 19200, loss 3.09585, acc 0.732645

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473123033/checkpoints/model-19200


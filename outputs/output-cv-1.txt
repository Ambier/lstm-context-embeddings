WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f2387007e90>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.
WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f2387007e50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.

Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=50
CHECKPOINT_EVERY=100
CV_INDEX=1
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=300
EVALUATE_EVERY=100
FILTER_SIZES=3,4,5
HIDDEN_DIM=300
L2_REG_LAMBDA=0.15
LOG_DEVICE_PLACEMENT=False
NUM_EPOCHS=100
NUM_FILTERS=100
WORD2VEC=GoogleNews-vectors-negative300.bin

Loading data...
Vocabulary Size: 18758
Train/Dev split: 9594/1066
Writing to /home/cil/lstm-context-embeddings/runs/1473106415

Load word2vec file GoogleNews-vectors-negative300.bin

2016-09-06T04:13:55.924835: step 1, loss 0.693147, acc 0.46
2016-09-06T04:13:56.739331: step 2, loss 0.683412, acc 0.58
2016-09-06T04:13:57.567713: step 3, loss 0.690523, acc 0.58
2016-09-06T04:13:58.373512: step 4, loss 0.695172, acc 0.52
2016-09-06T04:13:59.180002: step 5, loss 0.725044, acc 0.42
2016-09-06T04:14:00.031356: step 6, loss 0.669753, acc 0.58
2016-09-06T04:14:00.874073: step 7, loss 0.69413, acc 0.48
2016-09-06T04:14:01.650157: step 8, loss 0.685667, acc 0.5
2016-09-06T04:14:02.473259: step 9, loss 0.726344, acc 0.52
2016-09-06T04:14:03.290563: step 10, loss 0.734681, acc 0.46
2016-09-06T04:14:04.093355: step 11, loss 0.738141, acc 0.38
2016-09-06T04:14:04.896727: step 12, loss 0.67941, acc 0.62
2016-09-06T04:14:05.731397: step 13, loss 0.668459, acc 0.58
2016-09-06T04:14:06.514761: step 14, loss 0.722943, acc 0.56
2016-09-06T04:14:07.322510: step 15, loss 0.663412, acc 0.54
2016-09-06T04:14:08.135364: step 16, loss 0.73409, acc 0.48
2016-09-06T04:14:08.916026: step 17, loss 0.720775, acc 0.6
2016-09-06T04:14:09.724754: step 18, loss 0.684779, acc 0.64
2016-09-06T04:14:10.544564: step 19, loss 0.696713, acc 0.5
2016-09-06T04:14:11.383483: step 20, loss 0.687823, acc 0.5
2016-09-06T04:14:12.240376: step 21, loss 0.704165, acc 0.48
2016-09-06T04:14:13.071137: step 22, loss 0.696994, acc 0.5
2016-09-06T04:14:13.925447: step 23, loss 0.673771, acc 0.68
2016-09-06T04:14:14.710060: step 24, loss 0.681919, acc 0.54
2016-09-06T04:14:15.522706: step 25, loss 0.655414, acc 0.54
2016-09-06T04:14:16.333972: step 26, loss 0.664473, acc 0.54
2016-09-06T04:14:17.168407: step 27, loss 0.726998, acc 0.52
2016-09-06T04:14:18.017417: step 28, loss 0.696864, acc 0.52
2016-09-06T04:14:18.884105: step 29, loss 0.649114, acc 0.64
2016-09-06T04:14:19.706574: step 30, loss 0.658208, acc 0.66
2016-09-06T04:14:20.541893: step 31, loss 0.648256, acc 0.62
2016-09-06T04:14:21.347127: step 32, loss 0.573176, acc 0.76
2016-09-06T04:14:22.164152: step 33, loss 0.651476, acc 0.66
2016-09-06T04:14:22.984434: step 34, loss 0.5888, acc 0.7
2016-09-06T04:14:23.801747: step 35, loss 0.673242, acc 0.66
2016-09-06T04:14:24.610794: step 36, loss 0.606872, acc 0.7
2016-09-06T04:14:25.446691: step 37, loss 0.666753, acc 0.56
2016-09-06T04:14:26.253388: step 38, loss 0.598342, acc 0.64
2016-09-06T04:14:27.060708: step 39, loss 0.633967, acc 0.64
2016-09-06T04:14:27.873716: step 40, loss 0.574567, acc 0.68
2016-09-06T04:14:28.685323: step 41, loss 0.691734, acc 0.62
2016-09-06T04:14:29.504393: step 42, loss 0.584974, acc 0.66
2016-09-06T04:14:30.313339: step 43, loss 0.557272, acc 0.74
2016-09-06T04:14:31.112371: step 44, loss 0.818648, acc 0.52
2016-09-06T04:14:31.894341: step 45, loss 0.690607, acc 0.58
2016-09-06T04:14:32.714638: step 46, loss 0.686281, acc 0.6
2016-09-06T04:14:33.561336: step 47, loss 0.584505, acc 0.68
2016-09-06T04:14:34.343582: step 48, loss 0.579456, acc 0.72
2016-09-06T04:14:35.141825: step 49, loss 0.574341, acc 0.78
2016-09-06T04:14:35.944092: step 50, loss 0.60574, acc 0.66
2016-09-06T04:14:36.798853: step 51, loss 0.628176, acc 0.64
2016-09-06T04:14:37.622844: step 52, loss 0.622144, acc 0.66
2016-09-06T04:14:38.444092: step 53, loss 0.653532, acc 0.66
2016-09-06T04:14:39.224207: step 54, loss 0.618287, acc 0.72
2016-09-06T04:14:40.007765: step 55, loss 0.56315, acc 0.68
2016-09-06T04:14:40.806345: step 56, loss 0.571096, acc 0.7
2016-09-06T04:14:41.611906: step 57, loss 0.59586, acc 0.62
2016-09-06T04:14:42.461341: step 58, loss 0.466365, acc 0.8
2016-09-06T04:14:43.250048: step 59, loss 0.511877, acc 0.78
2016-09-06T04:14:44.057957: step 60, loss 0.680739, acc 0.66
2016-09-06T04:14:44.901608: step 61, loss 0.498256, acc 0.8
2016-09-06T04:14:45.773360: step 62, loss 0.655842, acc 0.62
2016-09-06T04:14:46.588000: step 63, loss 0.476299, acc 0.76
2016-09-06T04:14:47.403768: step 64, loss 0.483834, acc 0.78
2016-09-06T04:14:48.212837: step 65, loss 0.507842, acc 0.74
2016-09-06T04:14:49.002902: step 66, loss 0.636354, acc 0.66
2016-09-06T04:14:49.797454: step 67, loss 0.642892, acc 0.74
2016-09-06T04:14:50.629452: step 68, loss 0.634242, acc 0.62
2016-09-06T04:14:51.425938: step 69, loss 0.56844, acc 0.7
2016-09-06T04:14:52.233509: step 70, loss 0.483155, acc 0.74
2016-09-06T04:14:53.070151: step 71, loss 0.561922, acc 0.72
2016-09-06T04:14:53.880116: step 72, loss 0.552397, acc 0.68
2016-09-06T04:14:54.687889: step 73, loss 0.663304, acc 0.64
2016-09-06T04:14:55.518622: step 74, loss 0.508618, acc 0.8
2016-09-06T04:14:56.333572: step 75, loss 0.64434, acc 0.7
2016-09-06T04:14:57.129552: step 76, loss 0.642964, acc 0.66
2016-09-06T04:14:57.964222: step 77, loss 0.647925, acc 0.68
2016-09-06T04:14:58.758189: step 78, loss 0.644385, acc 0.66
2016-09-06T04:14:59.563287: step 79, loss 0.519324, acc 0.84
2016-09-06T04:15:00.413394: step 80, loss 0.60422, acc 0.64
2016-09-06T04:15:01.218381: step 81, loss 0.523285, acc 0.68
2016-09-06T04:15:02.027497: step 82, loss 0.580415, acc 0.72
2016-09-06T04:15:02.843821: step 83, loss 0.523778, acc 0.78
2016-09-06T04:15:03.657823: step 84, loss 0.553923, acc 0.74
2016-09-06T04:15:04.454786: step 85, loss 0.554694, acc 0.74
2016-09-06T04:15:05.277429: step 86, loss 0.541105, acc 0.76
2016-09-06T04:15:06.079887: step 87, loss 0.554596, acc 0.72
2016-09-06T04:15:06.870599: step 88, loss 0.620013, acc 0.72
2016-09-06T04:15:07.687750: step 89, loss 0.48974, acc 0.8
2016-09-06T04:15:08.500230: step 90, loss 0.466851, acc 0.78
2016-09-06T04:15:09.307875: step 91, loss 0.365054, acc 0.92
2016-09-06T04:15:10.140855: step 92, loss 0.4858, acc 0.8
2016-09-06T04:15:10.974480: step 93, loss 0.530711, acc 0.78
2016-09-06T04:15:11.781860: step 94, loss 0.525039, acc 0.74
2016-09-06T04:15:12.602020: step 95, loss 0.428016, acc 0.8
2016-09-06T04:15:13.411144: step 96, loss 0.42607, acc 0.76
2016-09-06T04:15:14.213749: step 97, loss 0.684461, acc 0.6
2016-09-06T04:15:15.044772: step 98, loss 0.55951, acc 0.7
2016-09-06T04:15:15.861024: step 99, loss 0.515584, acc 0.72
2016-09-06T04:15:16.634511: step 100, loss 0.529139, acc 0.74

Evaluation:
2016-09-06T04:15:20.349797: step 100, loss 0.492506, acc 0.766416

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-100

2016-09-06T04:15:22.333961: step 101, loss 0.661362, acc 0.64
2016-09-06T04:15:23.132322: step 102, loss 0.438197, acc 0.82
2016-09-06T04:15:23.981678: step 103, loss 0.574074, acc 0.7
2016-09-06T04:15:24.812881: step 104, loss 0.420626, acc 0.82
2016-09-06T04:15:25.627638: step 105, loss 0.5521, acc 0.74
2016-09-06T04:15:26.438858: step 106, loss 0.420814, acc 0.82
2016-09-06T04:15:27.253806: step 107, loss 0.514699, acc 0.76
2016-09-06T04:15:28.078166: step 108, loss 0.462244, acc 0.82
2016-09-06T04:15:28.881789: step 109, loss 0.461573, acc 0.82
2016-09-06T04:15:29.692016: step 110, loss 0.560837, acc 0.7
2016-09-06T04:15:30.508955: step 111, loss 0.626682, acc 0.72
2016-09-06T04:15:31.294063: step 112, loss 0.571001, acc 0.68
2016-09-06T04:15:32.099715: step 113, loss 0.548937, acc 0.76
2016-09-06T04:15:32.918112: step 114, loss 0.45769, acc 0.8
2016-09-06T04:15:33.716574: step 115, loss 0.480408, acc 0.82
2016-09-06T04:15:34.511996: step 116, loss 0.502629, acc 0.74
2016-09-06T04:15:35.342362: step 117, loss 0.491898, acc 0.78
2016-09-06T04:15:36.153719: step 118, loss 0.695776, acc 0.6
2016-09-06T04:15:36.957014: step 119, loss 0.489205, acc 0.76
2016-09-06T04:15:37.773578: step 120, loss 0.521854, acc 0.8
2016-09-06T04:15:38.557392: step 121, loss 0.523541, acc 0.76
2016-09-06T04:15:39.360603: step 122, loss 0.453314, acc 0.78
2016-09-06T04:15:40.171417: step 123, loss 0.430101, acc 0.8
2016-09-06T04:15:40.955800: step 124, loss 0.423158, acc 0.9
2016-09-06T04:15:41.782650: step 125, loss 0.383261, acc 0.84
2016-09-06T04:15:42.590663: step 126, loss 0.56468, acc 0.7
2016-09-06T04:15:43.387030: step 127, loss 0.658488, acc 0.72
2016-09-06T04:15:44.211175: step 128, loss 0.530498, acc 0.7
2016-09-06T04:15:45.036728: step 129, loss 0.522837, acc 0.76
2016-09-06T04:15:45.820102: step 130, loss 0.655373, acc 0.76
2016-09-06T04:15:46.617892: step 131, loss 0.651703, acc 0.7
2016-09-06T04:15:47.419814: step 132, loss 0.527739, acc 0.7
2016-09-06T04:15:48.187700: step 133, loss 0.451204, acc 0.8
2016-09-06T04:15:49.009228: step 134, loss 0.568229, acc 0.68
2016-09-06T04:15:49.827414: step 135, loss 0.38608, acc 0.9
2016-09-06T04:15:50.627388: step 136, loss 0.453475, acc 0.78
2016-09-06T04:15:51.432101: step 137, loss 0.504117, acc 0.74
2016-09-06T04:15:52.251502: step 138, loss 0.455459, acc 0.8
2016-09-06T04:15:53.046118: step 139, loss 0.553719, acc 0.72
2016-09-06T04:15:53.847891: step 140, loss 0.65753, acc 0.66
2016-09-06T04:15:54.671947: step 141, loss 0.586661, acc 0.66
2016-09-06T04:15:55.468445: step 142, loss 0.464912, acc 0.82
2016-09-06T04:15:56.264536: step 143, loss 0.489933, acc 0.78
2016-09-06T04:15:57.072067: step 144, loss 0.459205, acc 0.82
2016-09-06T04:15:57.846623: step 145, loss 0.48406, acc 0.8
2016-09-06T04:15:58.653870: step 146, loss 0.41102, acc 0.86
2016-09-06T04:15:59.459239: step 147, loss 0.447852, acc 0.78
2016-09-06T04:16:00.278316: step 148, loss 0.605437, acc 0.64
2016-09-06T04:16:01.078075: step 149, loss 0.466262, acc 0.78
2016-09-06T04:16:01.877150: step 150, loss 0.396994, acc 0.78
2016-09-06T04:16:02.673138: step 151, loss 0.445547, acc 0.84
2016-09-06T04:16:03.495050: step 152, loss 0.539427, acc 0.74
2016-09-06T04:16:04.304771: step 153, loss 0.587766, acc 0.7
2016-09-06T04:16:05.136902: step 154, loss 0.551991, acc 0.72
2016-09-06T04:16:05.948204: step 155, loss 0.625765, acc 0.72
2016-09-06T04:16:06.775857: step 156, loss 0.393433, acc 0.84
2016-09-06T04:16:07.558102: step 157, loss 0.521284, acc 0.68
2016-09-06T04:16:08.358699: step 158, loss 0.524856, acc 0.72
2016-09-06T04:16:09.178796: step 159, loss 0.559135, acc 0.74
2016-09-06T04:16:09.966032: step 160, loss 0.480263, acc 0.8
2016-09-06T04:16:10.759502: step 161, loss 0.641065, acc 0.72
2016-09-06T04:16:11.581421: step 162, loss 0.52495, acc 0.72
2016-09-06T04:16:12.391021: step 163, loss 0.437443, acc 0.78
2016-09-06T04:16:13.185116: step 164, loss 0.448821, acc 0.78
2016-09-06T04:16:14.010706: step 165, loss 0.531825, acc 0.7
2016-09-06T04:16:14.798874: step 166, loss 0.523429, acc 0.74
2016-09-06T04:16:15.593798: step 167, loss 0.486824, acc 0.78
2016-09-06T04:16:16.398817: step 168, loss 0.442639, acc 0.78
2016-09-06T04:16:17.178233: step 169, loss 0.489448, acc 0.78
2016-09-06T04:16:17.977745: step 170, loss 0.550047, acc 0.7
2016-09-06T04:16:18.792598: step 171, loss 0.457123, acc 0.8
2016-09-06T04:16:19.601599: step 172, loss 0.566605, acc 0.64
2016-09-06T04:16:20.414297: step 173, loss 0.514472, acc 0.72
2016-09-06T04:16:21.230881: step 174, loss 0.526638, acc 0.74
2016-09-06T04:16:22.022531: step 175, loss 0.569229, acc 0.68
2016-09-06T04:16:22.833594: step 176, loss 0.450689, acc 0.82
2016-09-06T04:16:23.667835: step 177, loss 0.428187, acc 0.76
2016-09-06T04:16:24.472213: step 178, loss 0.551618, acc 0.74
2016-09-06T04:16:25.256779: step 179, loss 0.352942, acc 0.84
2016-09-06T04:16:26.082095: step 180, loss 0.528457, acc 0.74
2016-09-06T04:16:26.883525: step 181, loss 0.443632, acc 0.76
2016-09-06T04:16:27.683445: step 182, loss 0.459228, acc 0.82
2016-09-06T04:16:28.488231: step 183, loss 0.375201, acc 0.86
2016-09-06T04:16:29.271411: step 184, loss 0.509464, acc 0.72
2016-09-06T04:16:30.083564: step 185, loss 0.395385, acc 0.82
2016-09-06T04:16:30.907341: step 186, loss 0.525203, acc 0.7
2016-09-06T04:16:31.693571: step 187, loss 0.569187, acc 0.68
2016-09-06T04:16:32.580878: step 188, loss 0.584263, acc 0.68
2016-09-06T04:16:33.408226: step 189, loss 0.338152, acc 0.88
2016-09-06T04:16:34.221967: step 190, loss 0.546812, acc 0.74
2016-09-06T04:16:35.040081: step 191, loss 0.379413, acc 0.78
2016-09-06T04:16:35.826418: step 192, loss 0.499563, acc 0.75
2016-09-06T04:16:36.622452: step 193, loss 0.428631, acc 0.82
2016-09-06T04:16:37.410030: step 194, loss 0.330715, acc 0.84
2016-09-06T04:16:38.242669: step 195, loss 0.284295, acc 0.96
2016-09-06T04:16:39.050532: step 196, loss 0.421394, acc 0.8
2016-09-06T04:16:39.843014: step 197, loss 0.466173, acc 0.72
2016-09-06T04:16:40.668585: step 198, loss 0.442442, acc 0.76
2016-09-06T04:16:41.470544: step 199, loss 0.336381, acc 0.82
2016-09-06T04:16:42.276290: step 200, loss 0.325254, acc 0.84

Evaluation:
2016-09-06T04:16:46.016891: step 200, loss 0.45297, acc 0.797373

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-200

2016-09-06T04:16:47.946661: step 201, loss 0.405579, acc 0.74
2016-09-06T04:16:48.754563: step 202, loss 0.373157, acc 0.84
2016-09-06T04:16:49.572229: step 203, loss 0.30215, acc 0.9
2016-09-06T04:16:50.408542: step 204, loss 0.369307, acc 0.88
2016-09-06T04:16:51.223527: step 205, loss 0.382511, acc 0.8
2016-09-06T04:16:52.027350: step 206, loss 0.354495, acc 0.8
2016-09-06T04:16:52.843990: step 207, loss 0.397723, acc 0.86
2016-09-06T04:16:53.651347: step 208, loss 0.470574, acc 0.82
2016-09-06T04:16:54.455756: step 209, loss 0.342909, acc 0.84
2016-09-06T04:16:55.297466: step 210, loss 0.515859, acc 0.72
2016-09-06T04:16:56.107883: step 211, loss 0.311356, acc 0.88
2016-09-06T04:16:56.923370: step 212, loss 0.293385, acc 0.9
2016-09-06T04:16:57.765821: step 213, loss 0.687865, acc 0.78
2016-09-06T04:16:58.601413: step 214, loss 0.343714, acc 0.86
2016-09-06T04:16:59.388481: step 215, loss 0.317733, acc 0.86
2016-09-06T04:17:00.252600: step 216, loss 0.339055, acc 0.92
2016-09-06T04:17:01.067557: step 217, loss 0.559002, acc 0.74
2016-09-06T04:17:01.878079: step 218, loss 0.421208, acc 0.82
2016-09-06T04:17:02.717030: step 219, loss 0.273405, acc 0.86
2016-09-06T04:17:03.526969: step 220, loss 0.381832, acc 0.84
2016-09-06T04:17:04.342346: step 221, loss 0.361427, acc 0.78
2016-09-06T04:17:05.166984: step 222, loss 0.286984, acc 0.88
2016-09-06T04:17:05.972560: step 223, loss 0.497015, acc 0.82
2016-09-06T04:17:06.773146: step 224, loss 0.331034, acc 0.9
2016-09-06T04:17:07.603923: step 225, loss 0.361799, acc 0.82
2016-09-06T04:17:08.451020: step 226, loss 0.326729, acc 0.86
2016-09-06T04:17:09.240138: step 227, loss 0.306825, acc 0.88
2016-09-06T04:17:10.071573: step 228, loss 0.389176, acc 0.84
2016-09-06T04:17:10.894905: step 229, loss 0.340175, acc 0.86
2016-09-06T04:17:11.685006: step 230, loss 0.435782, acc 0.78
2016-09-06T04:17:12.465068: step 231, loss 0.409749, acc 0.76
2016-09-06T04:17:13.277133: step 232, loss 0.394088, acc 0.86
2016-09-06T04:17:14.080693: step 233, loss 0.432885, acc 0.82
2016-09-06T04:17:14.898184: step 234, loss 0.408009, acc 0.76
2016-09-06T04:17:15.721459: step 235, loss 0.46842, acc 0.76
2016-09-06T04:17:16.509147: step 236, loss 0.319382, acc 0.8
2016-09-06T04:17:17.304506: step 237, loss 0.498313, acc 0.8
2016-09-06T04:17:18.107963: step 238, loss 0.367907, acc 0.86
2016-09-06T04:17:18.909704: step 239, loss 0.404431, acc 0.82
2016-09-06T04:17:19.707304: step 240, loss 0.32131, acc 0.84
2016-09-06T04:17:20.527946: step 241, loss 0.363953, acc 0.84
2016-09-06T04:17:21.331737: step 242, loss 0.245107, acc 0.88
2016-09-06T04:17:22.148837: step 243, loss 0.371242, acc 0.86
2016-09-06T04:17:22.952001: step 244, loss 0.395421, acc 0.82
2016-09-06T04:17:23.734804: step 245, loss 0.499906, acc 0.76
2016-09-06T04:17:24.530777: step 246, loss 0.473488, acc 0.8
2016-09-06T04:17:25.343881: step 247, loss 0.309312, acc 0.88
2016-09-06T04:17:26.116157: step 248, loss 0.400578, acc 0.84
2016-09-06T04:17:26.941428: step 249, loss 0.367539, acc 0.78
2016-09-06T04:17:27.735033: step 250, loss 0.50548, acc 0.8
2016-09-06T04:17:28.524449: step 251, loss 0.385262, acc 0.86
2016-09-06T04:17:29.331286: step 252, loss 0.447249, acc 0.82
2016-09-06T04:17:30.143542: step 253, loss 0.429606, acc 0.76
2016-09-06T04:17:30.983404: step 254, loss 0.448961, acc 0.84
2016-09-06T04:17:31.806046: step 255, loss 0.313747, acc 0.8
2016-09-06T04:17:32.644688: step 256, loss 0.373337, acc 0.82
2016-09-06T04:17:33.437716: step 257, loss 0.420737, acc 0.82
2016-09-06T04:17:34.241591: step 258, loss 0.43356, acc 0.78
2016-09-06T04:17:35.065750: step 259, loss 0.424521, acc 0.76
2016-09-06T04:17:35.852906: step 260, loss 0.37113, acc 0.8
2016-09-06T04:17:36.666026: step 261, loss 0.34942, acc 0.88
2016-09-06T04:17:37.473850: step 262, loss 0.354466, acc 0.84
2016-09-06T04:17:38.245478: step 263, loss 0.444748, acc 0.78
2016-09-06T04:17:39.053133: step 264, loss 0.435989, acc 0.76
2016-09-06T04:17:39.891580: step 265, loss 0.506793, acc 0.76
2016-09-06T04:17:40.652357: step 266, loss 0.449113, acc 0.8
2016-09-06T04:17:41.458139: step 267, loss 0.413131, acc 0.74
2016-09-06T04:17:42.286083: step 268, loss 0.449825, acc 0.82
2016-09-06T04:17:43.084869: step 269, loss 0.201826, acc 0.92
2016-09-06T04:17:43.921898: step 270, loss 0.260712, acc 0.9
2016-09-06T04:17:44.728691: step 271, loss 0.465068, acc 0.8
2016-09-06T04:17:45.502708: step 272, loss 0.684677, acc 0.7
2016-09-06T04:17:46.303623: step 273, loss 0.261301, acc 0.86
2016-09-06T04:17:47.152313: step 274, loss 0.523652, acc 0.76
2016-09-06T04:17:47.947443: step 275, loss 0.32822, acc 0.88
2016-09-06T04:17:48.731619: step 276, loss 0.381801, acc 0.8
2016-09-06T04:17:49.554934: step 277, loss 0.374848, acc 0.82
2016-09-06T04:17:50.346253: step 278, loss 0.380636, acc 0.82
2016-09-06T04:17:51.190567: step 279, loss 0.358053, acc 0.86
2016-09-06T04:17:52.020478: step 280, loss 0.294812, acc 0.88
2016-09-06T04:17:52.829589: step 281, loss 0.407975, acc 0.82
2016-09-06T04:17:53.630751: step 282, loss 0.28116, acc 0.9
2016-09-06T04:17:54.481736: step 283, loss 0.206843, acc 0.96
2016-09-06T04:17:55.289152: step 284, loss 0.223826, acc 0.92
2016-09-06T04:17:56.093201: step 285, loss 0.430545, acc 0.8
2016-09-06T04:17:56.934126: step 286, loss 0.286995, acc 0.88
2016-09-06T04:17:57.729482: step 287, loss 0.349572, acc 0.82
2016-09-06T04:17:58.542309: step 288, loss 0.265816, acc 0.88
2016-09-06T04:17:59.362170: step 289, loss 0.48132, acc 0.72
2016-09-06T04:18:00.163929: step 290, loss 0.416499, acc 0.82
2016-09-06T04:18:00.999991: step 291, loss 0.250608, acc 0.92
2016-09-06T04:18:01.848974: step 292, loss 0.464646, acc 0.82
2016-09-06T04:18:02.660606: step 293, loss 0.366016, acc 0.88
2016-09-06T04:18:03.484202: step 294, loss 0.610224, acc 0.76
2016-09-06T04:18:04.314338: step 295, loss 0.367054, acc 0.8
2016-09-06T04:18:05.122127: step 296, loss 0.3266, acc 0.88
2016-09-06T04:18:05.916579: step 297, loss 0.297462, acc 0.88
2016-09-06T04:18:06.758011: step 298, loss 0.354084, acc 0.82
2016-09-06T04:18:07.560500: step 299, loss 0.259277, acc 0.92
2016-09-06T04:18:08.359856: step 300, loss 0.268588, acc 0.9

Evaluation:
2016-09-06T04:18:12.102230: step 300, loss 0.494319, acc 0.782364

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-300

2016-09-06T04:18:13.998683: step 301, loss 0.315998, acc 0.9
2016-09-06T04:18:14.811981: step 302, loss 0.455695, acc 0.8
2016-09-06T04:18:15.633934: step 303, loss 0.445826, acc 0.78
2016-09-06T04:18:16.470779: step 304, loss 0.349087, acc 0.8
2016-09-06T04:18:17.269703: step 305, loss 0.398043, acc 0.84
2016-09-06T04:18:18.114531: step 306, loss 0.338511, acc 0.8
2016-09-06T04:18:18.942441: step 307, loss 0.406838, acc 0.84
2016-09-06T04:18:19.778467: step 308, loss 0.399052, acc 0.8
2016-09-06T04:18:20.549425: step 309, loss 0.326251, acc 0.84
2016-09-06T04:18:21.357577: step 310, loss 0.543667, acc 0.8
2016-09-06T04:18:22.164753: step 311, loss 0.453391, acc 0.76
2016-09-06T04:18:22.975156: step 312, loss 0.346389, acc 0.84
2016-09-06T04:18:23.795049: step 313, loss 0.338201, acc 0.84
2016-09-06T04:18:24.627931: step 314, loss 0.570631, acc 0.8
2016-09-06T04:18:25.433790: step 315, loss 0.300568, acc 0.86
2016-09-06T04:18:26.246254: step 316, loss 0.397881, acc 0.82
2016-09-06T04:18:27.044402: step 317, loss 0.40092, acc 0.78
2016-09-06T04:18:27.819665: step 318, loss 0.360109, acc 0.88
2016-09-06T04:18:28.627212: step 319, loss 0.310844, acc 0.86
2016-09-06T04:18:29.457325: step 320, loss 0.369157, acc 0.84
2016-09-06T04:18:30.258080: step 321, loss 0.441089, acc 0.8
2016-09-06T04:18:31.066061: step 322, loss 0.332146, acc 0.86
2016-09-06T04:18:31.902954: step 323, loss 0.301328, acc 0.86
2016-09-06T04:18:32.688309: step 324, loss 0.283734, acc 0.86
2016-09-06T04:18:33.497130: step 325, loss 0.404198, acc 0.84
2016-09-06T04:18:34.325636: step 326, loss 0.387924, acc 0.8
2016-09-06T04:18:35.100813: step 327, loss 0.423997, acc 0.82
2016-09-06T04:18:35.921430: step 328, loss 0.371212, acc 0.88
2016-09-06T04:18:36.710225: step 329, loss 0.499773, acc 0.84
2016-09-06T04:18:37.496432: step 330, loss 0.378268, acc 0.82
2016-09-06T04:18:38.290470: step 331, loss 0.289477, acc 0.9
2016-09-06T04:18:39.109263: step 332, loss 0.337159, acc 0.82
2016-09-06T04:18:39.908077: step 333, loss 0.316786, acc 0.8
2016-09-06T04:18:40.727133: step 334, loss 0.391827, acc 0.84
2016-09-06T04:18:41.536458: step 335, loss 0.319854, acc 0.9
2016-09-06T04:18:42.323020: step 336, loss 0.233118, acc 0.82
2016-09-06T04:18:43.114015: step 337, loss 0.339383, acc 0.9
2016-09-06T04:18:43.970781: step 338, loss 0.410479, acc 0.84
2016-09-06T04:18:44.736321: step 339, loss 0.437609, acc 0.8
2016-09-06T04:18:45.516633: step 340, loss 0.457339, acc 0.82
2016-09-06T04:18:46.349530: step 341, loss 0.24375, acc 0.88
2016-09-06T04:18:47.141642: step 342, loss 0.397809, acc 0.86
2016-09-06T04:18:47.980041: step 343, loss 0.283556, acc 0.88
2016-09-06T04:18:48.799333: step 344, loss 0.358451, acc 0.84
2016-09-06T04:18:49.585512: step 345, loss 0.277509, acc 0.88
2016-09-06T04:18:50.417041: step 346, loss 0.508599, acc 0.8
2016-09-06T04:18:51.233201: step 347, loss 0.317201, acc 0.88
2016-09-06T04:18:52.013551: step 348, loss 0.317658, acc 0.86
2016-09-06T04:18:52.867196: step 349, loss 0.314222, acc 0.86
2016-09-06T04:18:53.680579: step 350, loss 0.461729, acc 0.82
2016-09-06T04:18:54.482530: step 351, loss 0.363523, acc 0.84
2016-09-06T04:18:55.277912: step 352, loss 0.372452, acc 0.86
2016-09-06T04:18:56.090529: step 353, loss 0.369258, acc 0.84
2016-09-06T04:18:56.865398: step 354, loss 0.286762, acc 0.88
2016-09-06T04:18:57.654342: step 355, loss 0.389242, acc 0.8
2016-09-06T04:18:58.455988: step 356, loss 0.377049, acc 0.86
2016-09-06T04:18:59.246418: step 357, loss 0.353513, acc 0.82
2016-09-06T04:19:00.061847: step 358, loss 0.37846, acc 0.86
2016-09-06T04:19:00.917071: step 359, loss 0.473333, acc 0.8
2016-09-06T04:19:01.699487: step 360, loss 0.489804, acc 0.78
2016-09-06T04:19:02.487234: step 361, loss 0.32781, acc 0.86
2016-09-06T04:19:03.311995: step 362, loss 0.336509, acc 0.86
2016-09-06T04:19:04.094708: step 363, loss 0.354831, acc 0.84
2016-09-06T04:19:04.922748: step 364, loss 0.374024, acc 0.84
2016-09-06T04:19:05.770937: step 365, loss 0.259664, acc 0.88
2016-09-06T04:19:06.573107: step 366, loss 0.328938, acc 0.84
2016-09-06T04:19:07.387898: step 367, loss 0.504185, acc 0.82
2016-09-06T04:19:08.215686: step 368, loss 0.369073, acc 0.86
2016-09-06T04:19:09.023204: step 369, loss 0.397031, acc 0.84
2016-09-06T04:19:09.822393: step 370, loss 0.40841, acc 0.82
2016-09-06T04:19:10.658635: step 371, loss 0.383074, acc 0.84
2016-09-06T04:19:11.495799: step 372, loss 0.311311, acc 0.88
2016-09-06T04:19:12.305797: step 373, loss 0.440835, acc 0.78
2016-09-06T04:19:13.143997: step 374, loss 0.288363, acc 0.88
2016-09-06T04:19:13.947191: step 375, loss 0.248769, acc 0.88
2016-09-06T04:19:14.759791: step 376, loss 0.164157, acc 0.94
2016-09-06T04:19:15.594058: step 377, loss 0.416856, acc 0.82
2016-09-06T04:19:16.417566: step 378, loss 0.274777, acc 0.84
2016-09-06T04:19:17.210674: step 379, loss 0.418531, acc 0.8
2016-09-06T04:19:18.046531: step 380, loss 0.566485, acc 0.78
2016-09-06T04:19:18.843604: step 381, loss 0.289461, acc 0.88
2016-09-06T04:19:19.646001: step 382, loss 0.311879, acc 0.86
2016-09-06T04:19:20.466459: step 383, loss 0.412004, acc 0.86
2016-09-06T04:19:21.210033: step 384, loss 0.373126, acc 0.840909
2016-09-06T04:19:22.066285: step 385, loss 0.35733, acc 0.88
2016-09-06T04:19:22.882714: step 386, loss 0.226393, acc 0.96
2016-09-06T04:19:23.691954: step 387, loss 0.29795, acc 0.9
2016-09-06T04:19:24.493077: step 388, loss 0.32563, acc 0.84
2016-09-06T04:19:25.309209: step 389, loss 0.294622, acc 0.88
2016-09-06T04:19:26.121744: step 390, loss 0.268616, acc 0.88
2016-09-06T04:19:26.945075: step 391, loss 0.181427, acc 0.94
2016-09-06T04:19:27.808578: step 392, loss 0.212491, acc 0.94
2016-09-06T04:19:28.636684: step 393, loss 0.209477, acc 0.94
2016-09-06T04:19:29.433819: step 394, loss 0.231943, acc 0.88
2016-09-06T04:19:30.257101: step 395, loss 0.192152, acc 0.94
2016-09-06T04:19:31.089094: step 396, loss 0.345631, acc 0.84
2016-09-06T04:19:31.893632: step 397, loss 0.310612, acc 0.92
2016-09-06T04:19:32.738849: step 398, loss 0.217669, acc 0.92
2016-09-06T04:19:33.558058: step 399, loss 0.215978, acc 0.88
2016-09-06T04:19:34.348965: step 400, loss 0.147639, acc 0.94

Evaluation:
2016-09-06T04:19:38.045932: step 400, loss 0.564363, acc 0.793621

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-400

2016-09-06T04:19:39.935315: step 401, loss 0.349865, acc 0.88
2016-09-06T04:19:40.731331: step 402, loss 0.208088, acc 0.88
2016-09-06T04:19:41.530203: step 403, loss 0.16788, acc 0.9
2016-09-06T04:19:42.358380: step 404, loss 0.194771, acc 0.9
2016-09-06T04:19:43.159143: step 405, loss 0.290203, acc 0.9
2016-09-06T04:19:43.958001: step 406, loss 0.234724, acc 0.9
2016-09-06T04:19:44.779279: step 407, loss 0.13692, acc 0.96
2016-09-06T04:19:45.553837: step 408, loss 0.31852, acc 0.86
2016-09-06T04:19:46.350993: step 409, loss 0.358818, acc 0.88
2016-09-06T04:19:47.175920: step 410, loss 0.232656, acc 0.92
2016-09-06T04:19:47.981448: step 411, loss 0.242233, acc 0.9
2016-09-06T04:19:48.800525: step 412, loss 0.364301, acc 0.88
2016-09-06T04:19:49.606370: step 413, loss 0.49938, acc 0.82
2016-09-06T04:19:50.411730: step 414, loss 0.391878, acc 0.82
2016-09-06T04:19:51.231230: step 415, loss 0.236345, acc 0.88
2016-09-06T04:19:52.070006: step 416, loss 0.185028, acc 0.9
2016-09-06T04:19:52.888678: step 417, loss 0.251253, acc 0.88
2016-09-06T04:19:53.684024: step 418, loss 0.2594, acc 0.88
2016-09-06T04:19:54.463380: step 419, loss 0.273792, acc 0.9
2016-09-06T04:19:55.320615: step 420, loss 0.197369, acc 0.9
2016-09-06T04:19:56.119493: step 421, loss 0.187057, acc 0.96
2016-09-06T04:19:56.948061: step 422, loss 0.169163, acc 0.92
2016-09-06T04:19:57.770081: step 423, loss 0.268801, acc 0.9
2016-09-06T04:19:58.565145: step 424, loss 0.305299, acc 0.9
2016-09-06T04:19:59.376368: step 425, loss 0.299137, acc 0.86
2016-09-06T04:20:00.183283: step 426, loss 0.171324, acc 0.96
2016-09-06T04:20:00.979319: step 427, loss 0.282533, acc 0.86
2016-09-06T04:20:01.761857: step 428, loss 0.522485, acc 0.78
2016-09-06T04:20:02.594626: step 429, loss 0.199166, acc 0.94
2016-09-06T04:20:03.358803: step 430, loss 0.2605, acc 0.88
2016-09-06T04:20:04.176549: step 431, loss 0.13717, acc 0.92
2016-09-06T04:20:04.977956: step 432, loss 0.1894, acc 0.9
2016-09-06T04:20:05.753696: step 433, loss 0.135629, acc 0.92
2016-09-06T04:20:06.572514: step 434, loss 0.186906, acc 0.92
2016-09-06T04:20:07.386692: step 435, loss 0.254805, acc 0.94
2016-09-06T04:20:08.156131: step 436, loss 0.331919, acc 0.92
2016-09-06T04:20:08.966651: step 437, loss 0.324651, acc 0.84
2016-09-06T04:20:09.757775: step 438, loss 0.221973, acc 0.9
2016-09-06T04:20:10.550729: step 439, loss 0.273086, acc 0.9
2016-09-06T04:20:11.364911: step 440, loss 0.136284, acc 0.96
2016-09-06T04:20:12.163093: step 441, loss 0.219679, acc 0.92
2016-09-06T04:20:12.973592: step 442, loss 0.22198, acc 0.92
2016-09-06T04:20:13.785721: step 443, loss 0.213842, acc 0.88
2016-09-06T04:20:14.600475: step 444, loss 0.121701, acc 0.98
2016-09-06T04:20:15.388121: step 445, loss 0.254598, acc 0.86
2016-09-06T04:20:16.238797: step 446, loss 0.227394, acc 0.9
2016-09-06T04:20:17.057901: step 447, loss 0.263187, acc 0.88
2016-09-06T04:20:17.853606: step 448, loss 0.106662, acc 0.96
2016-09-06T04:20:18.694132: step 449, loss 0.19345, acc 0.94
2016-09-06T04:20:19.508498: step 450, loss 0.442899, acc 0.76
2016-09-06T04:20:20.287826: step 451, loss 0.234126, acc 0.94
2016-09-06T04:20:21.075894: step 452, loss 0.26743, acc 0.84
2016-09-06T04:20:21.892449: step 453, loss 0.226499, acc 0.96
2016-09-06T04:20:22.689862: step 454, loss 0.233832, acc 0.92
2016-09-06T04:20:23.468916: step 455, loss 0.464476, acc 0.76
2016-09-06T04:20:24.285147: step 456, loss 0.198035, acc 0.92
2016-09-06T04:20:25.084247: step 457, loss 0.262578, acc 0.88
2016-09-06T04:20:25.872265: step 458, loss 0.464441, acc 0.84
2016-09-06T04:20:26.697902: step 459, loss 0.242708, acc 0.88
2016-09-06T04:20:27.492873: step 460, loss 0.290835, acc 0.94
2016-09-06T04:20:28.310327: step 461, loss 0.229868, acc 0.86
2016-09-06T04:20:29.112602: step 462, loss 0.261872, acc 0.86
2016-09-06T04:20:29.889595: step 463, loss 0.280492, acc 0.9
2016-09-06T04:20:30.707595: step 464, loss 0.28109, acc 0.88
2016-09-06T04:20:31.513892: step 465, loss 0.283239, acc 0.88
2016-09-06T04:20:32.384388: step 466, loss 0.254963, acc 0.88
2016-09-06T04:20:33.177381: step 467, loss 0.283536, acc 0.82
2016-09-06T04:20:33.977021: step 468, loss 0.423315, acc 0.78
2016-09-06T04:20:34.756471: step 469, loss 0.199155, acc 0.9
2016-09-06T04:20:35.554021: step 470, loss 0.34954, acc 0.82
2016-09-06T04:20:36.354212: step 471, loss 0.202469, acc 0.92
2016-09-06T04:20:37.137140: step 472, loss 0.322362, acc 0.82
2016-09-06T04:20:37.962880: step 473, loss 0.31197, acc 0.86
2016-09-06T04:20:38.774132: step 474, loss 0.234938, acc 0.88
2016-09-06T04:20:39.566708: step 475, loss 0.321028, acc 0.82
2016-09-06T04:20:40.382856: step 476, loss 0.279675, acc 0.9
2016-09-06T04:20:41.224927: step 477, loss 0.201942, acc 0.92
2016-09-06T04:20:42.023305: step 478, loss 0.262393, acc 0.9
2016-09-06T04:20:42.825807: step 479, loss 0.325231, acc 0.92
2016-09-06T04:20:43.646599: step 480, loss 0.2118, acc 0.92
2016-09-06T04:20:44.487736: step 481, loss 0.209896, acc 0.86
2016-09-06T04:20:45.280986: step 482, loss 0.204045, acc 0.96
2016-09-06T04:20:46.098693: step 483, loss 0.184087, acc 0.92
2016-09-06T04:20:46.871271: step 484, loss 0.243998, acc 0.9
2016-09-06T04:20:47.712158: step 485, loss 0.258676, acc 0.88
2016-09-06T04:20:48.540019: step 486, loss 0.214787, acc 0.88
2016-09-06T04:20:49.325790: step 487, loss 0.112103, acc 0.94
2016-09-06T04:20:50.120522: step 488, loss 0.153963, acc 0.94
2016-09-06T04:20:50.940492: step 489, loss 0.339087, acc 0.92
2016-09-06T04:20:51.726107: step 490, loss 0.35805, acc 0.86
2016-09-06T04:20:52.524425: step 491, loss 0.228329, acc 0.88
2016-09-06T04:20:53.332522: step 492, loss 0.366349, acc 0.9
2016-09-06T04:20:54.129470: step 493, loss 0.217153, acc 0.9
2016-09-06T04:20:54.912885: step 494, loss 0.544545, acc 0.8
2016-09-06T04:20:55.736879: step 495, loss 0.408994, acc 0.84
2016-09-06T04:20:56.541814: step 496, loss 0.299028, acc 0.86
2016-09-06T04:20:57.359556: step 497, loss 0.116508, acc 0.98
2016-09-06T04:20:58.178265: step 498, loss 0.300339, acc 0.88
2016-09-06T04:20:58.961411: step 499, loss 0.296317, acc 0.86
2016-09-06T04:20:59.759313: step 500, loss 0.226152, acc 0.9

Evaluation:
2016-09-06T04:21:03.518145: step 500, loss 0.464821, acc 0.790807

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-500

2016-09-06T04:21:05.509632: step 501, loss 0.259923, acc 0.88
2016-09-06T04:21:06.311643: step 502, loss 0.318628, acc 0.86
2016-09-06T04:21:07.128663: step 503, loss 0.196331, acc 0.92
2016-09-06T04:21:07.940768: step 504, loss 0.27764, acc 0.86
2016-09-06T04:21:08.737652: step 505, loss 0.444422, acc 0.74
2016-09-06T04:21:09.556031: step 506, loss 0.181835, acc 0.94
2016-09-06T04:21:10.362839: step 507, loss 0.2527, acc 0.88
2016-09-06T04:21:11.154176: step 508, loss 0.257838, acc 0.94
2016-09-06T04:21:11.975515: step 509, loss 0.186497, acc 0.94
2016-09-06T04:21:12.827781: step 510, loss 0.155183, acc 0.96
2016-09-06T04:21:13.626934: step 511, loss 0.225457, acc 0.92
2016-09-06T04:21:14.452101: step 512, loss 0.214457, acc 0.9
2016-09-06T04:21:15.297297: step 513, loss 0.281396, acc 0.9
2016-09-06T04:21:16.112755: step 514, loss 0.260589, acc 0.84
2016-09-06T04:21:16.894378: step 515, loss 0.240623, acc 0.9
2016-09-06T04:21:17.719474: step 516, loss 0.158438, acc 0.92
2016-09-06T04:21:18.513723: step 517, loss 0.235807, acc 0.9
2016-09-06T04:21:19.322800: step 518, loss 0.163208, acc 0.94
2016-09-06T04:21:20.154775: step 519, loss 0.165209, acc 0.92
2016-09-06T04:21:20.972262: step 520, loss 0.213965, acc 0.94
2016-09-06T04:21:21.754992: step 521, loss 0.197568, acc 0.9
2016-09-06T04:21:22.590440: step 522, loss 0.205017, acc 0.92
2016-09-06T04:21:23.381416: step 523, loss 0.234833, acc 0.88
2016-09-06T04:21:24.169391: step 524, loss 0.348905, acc 0.84
2016-09-06T04:21:25.013129: step 525, loss 0.413057, acc 0.8
2016-09-06T04:21:25.826759: step 526, loss 0.235418, acc 0.9
2016-09-06T04:21:26.640263: step 527, loss 0.338942, acc 0.86
2016-09-06T04:21:27.448403: step 528, loss 0.24655, acc 0.94
2016-09-06T04:21:28.250754: step 529, loss 0.196808, acc 0.92
2016-09-06T04:21:29.099297: step 530, loss 0.241235, acc 0.88
2016-09-06T04:21:29.954202: step 531, loss 0.370078, acc 0.84
2016-09-06T04:21:30.776511: step 532, loss 0.371245, acc 0.86
2016-09-06T04:21:31.603181: step 533, loss 0.179605, acc 0.96
2016-09-06T04:21:32.430051: step 534, loss 0.310752, acc 0.86
2016-09-06T04:21:33.243320: step 535, loss 0.204605, acc 0.92
2016-09-06T04:21:34.073066: step 536, loss 0.145662, acc 0.96
2016-09-06T04:21:34.898146: step 537, loss 0.188078, acc 0.94
2016-09-06T04:21:35.703298: step 538, loss 0.274684, acc 0.9
2016-09-06T04:21:36.500715: step 539, loss 0.281227, acc 0.9
2016-09-06T04:21:37.339368: step 540, loss 0.208203, acc 0.92
2016-09-06T04:21:38.166726: step 541, loss 0.202871, acc 0.96
2016-09-06T04:21:38.956128: step 542, loss 0.207074, acc 0.9
2016-09-06T04:21:39.752037: step 543, loss 0.288306, acc 0.86
2016-09-06T04:21:40.545688: step 544, loss 0.237649, acc 0.92
2016-09-06T04:21:41.347546: step 545, loss 0.227424, acc 0.88
2016-09-06T04:21:42.178106: step 546, loss 0.333502, acc 0.84
2016-09-06T04:21:43.005536: step 547, loss 0.132404, acc 0.96
2016-09-06T04:21:43.799937: step 548, loss 0.212483, acc 0.92
2016-09-06T04:21:44.594338: step 549, loss 0.301229, acc 0.86
2016-09-06T04:21:45.412343: step 550, loss 0.248886, acc 0.84
2016-09-06T04:21:46.201417: step 551, loss 0.313537, acc 0.88
2016-09-06T04:21:47.009048: step 552, loss 0.283502, acc 0.86
2016-09-06T04:21:47.839383: step 553, loss 0.402661, acc 0.86
2016-09-06T04:21:48.647029: step 554, loss 0.280041, acc 0.88
2016-09-06T04:21:49.445302: step 555, loss 0.190881, acc 0.92
2016-09-06T04:21:50.246942: step 556, loss 0.282987, acc 0.88
2016-09-06T04:21:51.060569: step 557, loss 0.335118, acc 0.9
2016-09-06T04:21:51.865426: step 558, loss 0.233499, acc 0.94
2016-09-06T04:21:52.679940: step 559, loss 0.253655, acc 0.9
2016-09-06T04:21:53.467063: step 560, loss 0.366067, acc 0.84
2016-09-06T04:21:54.263842: step 561, loss 0.144559, acc 0.98
2016-09-06T04:21:55.080638: step 562, loss 0.286609, acc 0.88
2016-09-06T04:21:55.870127: step 563, loss 0.154985, acc 0.94
2016-09-06T04:21:56.658874: step 564, loss 0.31567, acc 0.8
2016-09-06T04:21:57.454122: step 565, loss 0.225947, acc 0.92
2016-09-06T04:21:58.249717: step 566, loss 0.229521, acc 0.98
2016-09-06T04:21:59.092774: step 567, loss 0.312295, acc 0.84
2016-09-06T04:21:59.925794: step 568, loss 0.181357, acc 0.92
2016-09-06T04:22:00.745450: step 569, loss 0.211572, acc 0.92
2016-09-06T04:22:01.583172: step 570, loss 0.248127, acc 0.88
2016-09-06T04:22:02.390727: step 571, loss 0.46069, acc 0.78
2016-09-06T04:22:03.178146: step 572, loss 0.15539, acc 0.92
2016-09-06T04:22:03.969582: step 573, loss 0.185258, acc 0.92
2016-09-06T04:22:04.780998: step 574, loss 0.160066, acc 0.98
2016-09-06T04:22:05.568183: step 575, loss 0.442149, acc 0.78
2016-09-06T04:22:06.327394: step 576, loss 0.371135, acc 0.863636
2016-09-06T04:22:07.165780: step 577, loss 0.156936, acc 0.94
2016-09-06T04:22:07.979055: step 578, loss 0.215791, acc 0.92
2016-09-06T04:22:08.817754: step 579, loss 0.104058, acc 0.96
2016-09-06T04:22:09.615158: step 580, loss 0.152676, acc 0.96
2016-09-06T04:22:10.412387: step 581, loss 0.202332, acc 0.94
2016-09-06T04:22:11.217219: step 582, loss 0.307642, acc 0.88
2016-09-06T04:22:12.013305: step 583, loss 0.186755, acc 0.94
2016-09-06T04:22:12.819876: step 584, loss 0.170522, acc 0.94
2016-09-06T04:22:13.633788: step 585, loss 0.253152, acc 0.9
2016-09-06T04:22:14.484429: step 586, loss 0.232242, acc 0.92
2016-09-06T04:22:15.247664: step 587, loss 0.195085, acc 0.9
2016-09-06T04:22:16.043066: step 588, loss 0.173403, acc 0.88
2016-09-06T04:22:16.885096: step 589, loss 0.248493, acc 0.92
2016-09-06T04:22:17.670659: step 590, loss 0.193648, acc 0.94
2016-09-06T04:22:18.465229: step 591, loss 0.101146, acc 0.96
2016-09-06T04:22:19.307269: step 592, loss 0.1758, acc 0.94
2016-09-06T04:22:20.083388: step 593, loss 0.175192, acc 0.92
2016-09-06T04:22:20.910842: step 594, loss 0.046122, acc 0.98
2016-09-06T04:22:21.722204: step 595, loss 0.187256, acc 0.94
2016-09-06T04:22:22.491178: step 596, loss 0.205807, acc 0.94
2016-09-06T04:22:23.269568: step 597, loss 0.126127, acc 0.96
2016-09-06T04:22:24.080013: step 598, loss 0.164122, acc 0.98
2016-09-06T04:22:24.853117: step 599, loss 0.246207, acc 0.88
2016-09-06T04:22:25.673618: step 600, loss 0.202031, acc 0.9

Evaluation:
2016-09-06T04:22:29.428371: step 600, loss 0.677869, acc 0.774859

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-600

2016-09-06T04:22:31.269137: step 601, loss 0.231856, acc 0.88
2016-09-06T04:22:32.078938: step 602, loss 0.167103, acc 0.96
2016-09-06T04:22:32.907732: step 603, loss 0.196378, acc 0.92
2016-09-06T04:22:33.708786: step 604, loss 0.175727, acc 0.96
2016-09-06T04:22:34.517123: step 605, loss 0.160386, acc 0.92
2016-09-06T04:22:35.341800: step 606, loss 0.0822964, acc 0.96
2016-09-06T04:22:36.145415: step 607, loss 0.0712118, acc 0.96
2016-09-06T04:22:36.962295: step 608, loss 0.0842206, acc 0.98
2016-09-06T04:22:37.787394: step 609, loss 0.0681961, acc 0.98
2016-09-06T04:22:38.630452: step 610, loss 0.103253, acc 0.92
2016-09-06T04:22:39.467463: step 611, loss 0.322243, acc 0.92
2016-09-06T04:22:40.249797: step 612, loss 0.159544, acc 0.96
2016-09-06T04:22:41.094644: step 613, loss 0.0734364, acc 0.98
2016-09-06T04:22:41.871773: step 614, loss 0.157023, acc 0.92
2016-09-06T04:22:42.645670: step 615, loss 0.217345, acc 0.9
2016-09-06T04:22:43.471252: step 616, loss 0.125903, acc 0.92
2016-09-06T04:22:44.264499: step 617, loss 0.219917, acc 0.92
2016-09-06T04:22:45.074773: step 618, loss 0.158241, acc 0.92
2016-09-06T04:22:45.901402: step 619, loss 0.183977, acc 0.94
2016-09-06T04:22:46.683189: step 620, loss 0.172292, acc 0.96
2016-09-06T04:22:47.485474: step 621, loss 0.0936515, acc 0.96
2016-09-06T04:22:48.314254: step 622, loss 0.105137, acc 0.96
2016-09-06T04:22:49.123856: step 623, loss 0.195017, acc 0.94
2016-09-06T04:22:49.950865: step 624, loss 0.267275, acc 0.86
2016-09-06T04:22:50.763359: step 625, loss 0.139848, acc 0.92
2016-09-06T04:22:51.543935: step 626, loss 0.215961, acc 0.9
2016-09-06T04:22:52.329674: step 627, loss 0.281105, acc 0.86
2016-09-06T04:22:53.143379: step 628, loss 0.203644, acc 0.94
2016-09-06T04:22:53.937081: step 629, loss 0.184883, acc 0.98
2016-09-06T04:22:54.751252: step 630, loss 0.195981, acc 0.94
2016-09-06T04:22:55.559260: step 631, loss 0.235385, acc 0.88
2016-09-06T04:22:56.351576: step 632, loss 0.167071, acc 0.94
2016-09-06T04:22:57.136921: step 633, loss 0.318371, acc 0.9
2016-09-06T04:22:57.960114: step 634, loss 0.203811, acc 0.88
2016-09-06T04:22:58.754848: step 635, loss 0.1631, acc 0.92
2016-09-06T04:22:59.570501: step 636, loss 0.154346, acc 0.94
2016-09-06T04:23:00.401467: step 637, loss 0.301832, acc 0.86
2016-09-06T04:23:01.183775: step 638, loss 0.22931, acc 0.9
2016-09-06T04:23:01.974396: step 639, loss 0.142447, acc 0.94
2016-09-06T04:23:02.787192: step 640, loss 0.229126, acc 0.94
2016-09-06T04:23:03.582382: step 641, loss 0.135049, acc 0.96
2016-09-06T04:23:04.403377: step 642, loss 0.217849, acc 0.86
2016-09-06T04:23:05.241717: step 643, loss 0.199722, acc 0.92
2016-09-06T04:23:06.025316: step 644, loss 0.187706, acc 0.9
2016-09-06T04:23:06.818345: step 645, loss 0.127059, acc 0.96
2016-09-06T04:23:07.641627: step 646, loss 0.200639, acc 0.86
2016-09-06T04:23:08.423847: step 647, loss 0.175583, acc 0.94
2016-09-06T04:23:09.254307: step 648, loss 0.181681, acc 0.88
2016-09-06T04:23:10.091380: step 649, loss 0.195511, acc 0.9
2016-09-06T04:23:10.887431: step 650, loss 0.257944, acc 0.88
2016-09-06T04:23:11.702582: step 651, loss 0.259876, acc 0.88
2016-09-06T04:23:12.524900: step 652, loss 0.210589, acc 0.9
2016-09-06T04:23:13.324856: step 653, loss 0.212405, acc 0.92
2016-09-06T04:23:14.107025: step 654, loss 0.224404, acc 0.9
2016-09-06T04:23:14.938445: step 655, loss 0.119304, acc 0.94
2016-09-06T04:23:15.728888: step 656, loss 0.153466, acc 0.92
2016-09-06T04:23:16.533683: step 657, loss 0.0672338, acc 1
2016-09-06T04:23:17.351411: step 658, loss 0.232045, acc 0.92
2016-09-06T04:23:18.133308: step 659, loss 0.276156, acc 0.86
2016-09-06T04:23:18.905524: step 660, loss 0.264347, acc 0.9
2016-09-06T04:23:19.704098: step 661, loss 0.148801, acc 0.94
2016-09-06T04:23:20.478777: step 662, loss 0.210753, acc 0.9
2016-09-06T04:23:21.288866: step 663, loss 0.212749, acc 0.88
2016-09-06T04:23:22.091524: step 664, loss 0.170791, acc 0.92
2016-09-06T04:23:22.925832: step 665, loss 0.273773, acc 0.86
2016-09-06T04:23:23.735647: step 666, loss 0.120188, acc 0.94
2016-09-06T04:23:24.585693: step 667, loss 0.116295, acc 0.96
2016-09-06T04:23:25.399805: step 668, loss 0.128442, acc 0.94
2016-09-06T04:23:26.179430: step 669, loss 0.28166, acc 0.86
2016-09-06T04:23:26.992506: step 670, loss 0.16647, acc 0.92
2016-09-06T04:23:27.769269: step 671, loss 0.144725, acc 0.96
2016-09-06T04:23:28.566620: step 672, loss 0.173938, acc 0.9
2016-09-06T04:23:29.396509: step 673, loss 0.130512, acc 0.92
2016-09-06T04:23:30.189454: step 674, loss 0.190608, acc 0.9
2016-09-06T04:23:30.996384: step 675, loss 0.168086, acc 0.96
2016-09-06T04:23:31.791189: step 676, loss 0.1394, acc 0.92
2016-09-06T04:23:32.577521: step 677, loss 0.19546, acc 0.94
2016-09-06T04:23:33.401396: step 678, loss 0.228664, acc 0.88
2016-09-06T04:23:34.207065: step 679, loss 0.114234, acc 0.94
2016-09-06T04:23:34.992686: step 680, loss 0.138677, acc 0.92
2016-09-06T04:23:35.779472: step 681, loss 0.14504, acc 0.96
2016-09-06T04:23:36.587334: step 682, loss 0.184754, acc 0.92
2016-09-06T04:23:37.372163: step 683, loss 0.24768, acc 0.9
2016-09-06T04:23:38.200331: step 684, loss 0.187346, acc 0.92
2016-09-06T04:23:39.023062: step 685, loss 0.18424, acc 0.86
2016-09-06T04:23:39.815112: step 686, loss 0.19264, acc 0.9
2016-09-06T04:23:40.656288: step 687, loss 0.238725, acc 0.92
2016-09-06T04:23:41.484683: step 688, loss 0.240743, acc 0.9
2016-09-06T04:23:42.267669: step 689, loss 0.17038, acc 0.9
2016-09-06T04:23:43.071568: step 690, loss 0.194031, acc 0.9
2016-09-06T04:23:43.907329: step 691, loss 0.195116, acc 0.92
2016-09-06T04:23:44.717615: step 692, loss 0.148118, acc 0.9
2016-09-06T04:23:45.506814: step 693, loss 0.119378, acc 0.96
2016-09-06T04:23:46.341645: step 694, loss 0.214167, acc 0.86
2016-09-06T04:23:47.109860: step 695, loss 0.219691, acc 0.92
2016-09-06T04:23:47.909386: step 696, loss 0.299697, acc 0.9
2016-09-06T04:23:48.759586: step 697, loss 0.197954, acc 0.9
2016-09-06T04:23:49.548837: step 698, loss 0.14505, acc 0.96
2016-09-06T04:23:50.344539: step 699, loss 0.120988, acc 0.96
2016-09-06T04:23:51.159991: step 700, loss 0.114498, acc 0.94

Evaluation:
2016-09-06T04:23:54.934566: step 700, loss 0.640654, acc 0.777674

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-700

2016-09-06T04:23:56.850771: step 701, loss 0.198769, acc 0.92
2016-09-06T04:23:57.660487: step 702, loss 0.230054, acc 0.92
2016-09-06T04:23:58.470299: step 703, loss 0.108535, acc 0.94
2016-09-06T04:23:59.238222: step 704, loss 0.135915, acc 0.94
2016-09-06T04:24:00.063895: step 705, loss 0.122795, acc 0.94
2016-09-06T04:24:00.900821: step 706, loss 0.103286, acc 0.94
2016-09-06T04:24:01.689439: step 707, loss 0.224113, acc 0.9
2016-09-06T04:24:02.510245: step 708, loss 0.138025, acc 0.96
2016-09-06T04:24:03.343498: step 709, loss 0.161239, acc 0.92
2016-09-06T04:24:04.143511: step 710, loss 0.124114, acc 0.92
2016-09-06T04:24:04.943456: step 711, loss 0.174111, acc 0.9
2016-09-06T04:24:05.760077: step 712, loss 0.247468, acc 0.88
2016-09-06T04:24:06.562633: step 713, loss 0.103216, acc 0.98
2016-09-06T04:24:07.382545: step 714, loss 0.273869, acc 0.9
2016-09-06T04:24:08.219535: step 715, loss 0.198542, acc 0.96
2016-09-06T04:24:09.033600: step 716, loss 0.16141, acc 0.96
2016-09-06T04:24:09.821552: step 717, loss 0.235971, acc 0.84
2016-09-06T04:24:10.662340: step 718, loss 0.15351, acc 0.92
2016-09-06T04:24:11.470828: step 719, loss 0.189821, acc 0.92
2016-09-06T04:24:12.267569: step 720, loss 0.162647, acc 0.92
2016-09-06T04:24:13.070052: step 721, loss 0.0798001, acc 0.98
2016-09-06T04:24:13.864205: step 722, loss 0.197526, acc 0.9
2016-09-06T04:24:14.668396: step 723, loss 0.278956, acc 0.82
2016-09-06T04:24:15.501911: step 724, loss 0.112576, acc 0.96
2016-09-06T04:24:16.378524: step 725, loss 0.184955, acc 0.92
2016-09-06T04:24:17.180921: step 726, loss 0.269116, acc 0.9
2016-09-06T04:24:17.996963: step 727, loss 0.232828, acc 0.86
2016-09-06T04:24:18.818137: step 728, loss 0.237175, acc 0.9
2016-09-06T04:24:19.632379: step 729, loss 0.254497, acc 0.84
2016-09-06T04:24:20.478518: step 730, loss 0.203391, acc 0.94
2016-09-06T04:24:21.324967: step 731, loss 0.133729, acc 0.96
2016-09-06T04:24:22.124438: step 732, loss 0.144923, acc 0.92
2016-09-06T04:24:22.969744: step 733, loss 0.111374, acc 0.94
2016-09-06T04:24:23.781695: step 734, loss 0.20991, acc 0.88
2016-09-06T04:24:24.582191: step 735, loss 0.171205, acc 0.94
2016-09-06T04:24:25.393485: step 736, loss 0.159019, acc 0.92
2016-09-06T04:24:26.218890: step 737, loss 0.256868, acc 0.86
2016-09-06T04:24:27.016193: step 738, loss 0.0798044, acc 1
2016-09-06T04:24:27.830336: step 739, loss 0.299508, acc 0.88
2016-09-06T04:24:28.629423: step 740, loss 0.163638, acc 0.92
2016-09-06T04:24:29.435869: step 741, loss 0.19723, acc 0.94
2016-09-06T04:24:30.255525: step 742, loss 0.131988, acc 0.96
2016-09-06T04:24:31.100056: step 743, loss 0.212763, acc 0.92
2016-09-06T04:24:31.916838: step 744, loss 0.166196, acc 0.96
2016-09-06T04:24:32.722942: step 745, loss 0.187694, acc 0.92
2016-09-06T04:24:33.567254: step 746, loss 0.267203, acc 0.92
2016-09-06T04:24:34.379357: step 747, loss 0.27739, acc 0.86
2016-09-06T04:24:35.211243: step 748, loss 0.096539, acc 0.96
2016-09-06T04:24:36.050929: step 749, loss 0.22483, acc 0.92
2016-09-06T04:24:36.860789: step 750, loss 0.127413, acc 0.96
2016-09-06T04:24:37.708626: step 751, loss 0.33204, acc 0.84
2016-09-06T04:24:38.554105: step 752, loss 0.24561, acc 0.86
2016-09-06T04:24:39.397665: step 753, loss 0.149936, acc 0.92
2016-09-06T04:24:40.226442: step 754, loss 0.177494, acc 0.92
2016-09-06T04:24:41.090034: step 755, loss 0.140308, acc 0.92
2016-09-06T04:24:41.914413: step 756, loss 0.216838, acc 0.92
2016-09-06T04:24:42.715762: step 757, loss 0.235885, acc 0.92
2016-09-06T04:24:43.516663: step 758, loss 0.254601, acc 0.92
2016-09-06T04:24:44.338162: step 759, loss 0.265815, acc 0.94
2016-09-06T04:24:45.140594: step 760, loss 0.227759, acc 0.9
2016-09-06T04:24:45.960164: step 761, loss 0.156108, acc 0.94
2016-09-06T04:24:46.768782: step 762, loss 0.27718, acc 0.9
2016-09-06T04:24:47.570193: step 763, loss 0.176081, acc 0.94
2016-09-06T04:24:48.376087: step 764, loss 0.102627, acc 0.96
2016-09-06T04:24:49.217768: step 765, loss 0.106035, acc 0.94
2016-09-06T04:24:49.990783: step 766, loss 0.1465, acc 0.94
2016-09-06T04:24:50.794419: step 767, loss 0.287826, acc 0.88
2016-09-06T04:24:51.546989: step 768, loss 0.127628, acc 0.977273
2016-09-06T04:24:52.386386: step 769, loss 0.0642157, acc 1
2016-09-06T04:24:53.177160: step 770, loss 0.119962, acc 0.96
2016-09-06T04:24:53.993731: step 771, loss 0.0925129, acc 0.98
2016-09-06T04:24:54.788089: step 772, loss 0.252639, acc 0.9
2016-09-06T04:24:55.584012: step 773, loss 0.117077, acc 0.98
2016-09-06T04:24:56.371663: step 774, loss 0.163317, acc 0.94
2016-09-06T04:24:57.177536: step 775, loss 0.0591725, acc 1
2016-09-06T04:24:58.001344: step 776, loss 0.0796558, acc 0.96
2016-09-06T04:24:58.843465: step 777, loss 0.128201, acc 0.98
2016-09-06T04:24:59.626151: step 778, loss 0.0674015, acc 0.98
2016-09-06T04:25:00.444228: step 779, loss 0.117322, acc 0.94
2016-09-06T04:25:01.277669: step 780, loss 0.11472, acc 0.94
2016-09-06T04:25:02.082105: step 781, loss 0.0981248, acc 0.96
2016-09-06T04:25:02.874768: step 782, loss 0.121347, acc 0.96
2016-09-06T04:25:03.721223: step 783, loss 0.165832, acc 0.88
2016-09-06T04:25:04.505429: step 784, loss 0.0841299, acc 0.94
2016-09-06T04:25:05.336031: step 785, loss 0.146398, acc 0.94
2016-09-06T04:25:06.151447: step 786, loss 0.142411, acc 0.94
2016-09-06T04:25:06.960672: step 787, loss 0.110061, acc 0.94
2016-09-06T04:25:07.768809: step 788, loss 0.195461, acc 0.9
2016-09-06T04:25:08.595469: step 789, loss 0.186372, acc 0.94
2016-09-06T04:25:09.384708: step 790, loss 0.172862, acc 0.92
2016-09-06T04:25:10.191147: step 791, loss 0.14253, acc 0.96
2016-09-06T04:25:11.019985: step 792, loss 0.16979, acc 0.94
2016-09-06T04:25:11.842718: step 793, loss 0.273834, acc 0.9
2016-09-06T04:25:12.659573: step 794, loss 0.09285, acc 0.94
2016-09-06T04:25:13.507717: step 795, loss 0.1373, acc 0.92
2016-09-06T04:25:14.327017: step 796, loss 0.0571636, acc 1
2016-09-06T04:25:15.138632: step 797, loss 0.198299, acc 0.88
2016-09-06T04:25:15.965670: step 798, loss 0.0775054, acc 0.98
2016-09-06T04:25:16.811405: step 799, loss 0.0990807, acc 0.98
2016-09-06T04:25:17.644140: step 800, loss 0.094718, acc 0.94

Evaluation:
2016-09-06T04:25:21.411162: step 800, loss 0.69056, acc 0.77955

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-800

2016-09-06T04:25:23.415377: step 801, loss 0.115369, acc 0.94
2016-09-06T04:25:24.245530: step 802, loss 0.119244, acc 0.96
2016-09-06T04:25:25.065663: step 803, loss 0.212409, acc 0.92
2016-09-06T04:25:25.865253: step 804, loss 0.308054, acc 0.88
2016-09-06T04:25:26.687289: step 805, loss 0.103677, acc 0.96
2016-09-06T04:25:27.492497: step 806, loss 0.0557089, acc 1
2016-09-06T04:25:28.313724: step 807, loss 0.170334, acc 0.94
2016-09-06T04:25:29.113677: step 808, loss 0.0735956, acc 0.98
2016-09-06T04:25:29.919884: step 809, loss 0.153667, acc 0.92
2016-09-06T04:25:30.749297: step 810, loss 0.112535, acc 0.94
2016-09-06T04:25:31.561186: step 811, loss 0.153857, acc 0.92
2016-09-06T04:25:32.333676: step 812, loss 0.0816454, acc 0.98
2016-09-06T04:25:33.157414: step 813, loss 0.120478, acc 0.96
2016-09-06T04:25:33.985947: step 814, loss 0.0719231, acc 0.96
2016-09-06T04:25:34.758144: step 815, loss 0.0330374, acc 1
2016-09-06T04:25:35.575740: step 816, loss 0.101687, acc 0.98
2016-09-06T04:25:36.411415: step 817, loss 0.256848, acc 0.92
2016-09-06T04:25:37.210181: step 818, loss 0.111452, acc 0.94
2016-09-06T04:25:38.020909: step 819, loss 0.12458, acc 0.94
2016-09-06T04:25:38.909065: step 820, loss 0.126231, acc 0.92
2016-09-06T04:25:39.725298: step 821, loss 0.0736034, acc 1
2016-09-06T04:25:40.554646: step 822, loss 0.122766, acc 0.98
2016-09-06T04:25:41.427266: step 823, loss 0.158226, acc 0.94
2016-09-06T04:25:42.230425: step 824, loss 0.14505, acc 0.94
2016-09-06T04:25:43.033477: step 825, loss 0.0463263, acc 1
2016-09-06T04:25:43.873939: step 826, loss 0.127458, acc 0.94
2016-09-06T04:25:44.676656: step 827, loss 0.0849658, acc 0.98
2016-09-06T04:25:45.513321: step 828, loss 0.195365, acc 0.9
2016-09-06T04:25:46.353455: step 829, loss 0.08689, acc 0.96
2016-09-06T04:25:47.170115: step 830, loss 0.135362, acc 0.98
2016-09-06T04:25:47.985489: step 831, loss 0.0744702, acc 0.98
2016-09-06T04:25:48.825719: step 832, loss 0.144933, acc 0.94
2016-09-06T04:25:49.647438: step 833, loss 0.147912, acc 0.92
2016-09-06T04:25:50.438585: step 834, loss 0.101025, acc 0.96
2016-09-06T04:25:51.278285: step 835, loss 0.115431, acc 0.96
2016-09-06T04:25:52.105424: step 836, loss 0.0660059, acc 0.98
2016-09-06T04:25:52.915793: step 837, loss 0.099711, acc 0.92
2016-09-06T04:25:53.744608: step 838, loss 0.0542301, acc 0.98
2016-09-06T04:25:54.620022: step 839, loss 0.0898626, acc 0.96
2016-09-06T04:25:55.445202: step 840, loss 0.292751, acc 0.88
2016-09-06T04:25:56.253441: step 841, loss 0.124182, acc 0.96
2016-09-06T04:25:57.080053: step 842, loss 0.052526, acc 1
2016-09-06T04:25:57.894428: step 843, loss 0.174405, acc 0.9
2016-09-06T04:25:58.738018: step 844, loss 0.0559615, acc 0.96
2016-09-06T04:25:59.592841: step 845, loss 0.0867899, acc 0.98
2016-09-06T04:26:00.429794: step 846, loss 0.188705, acc 0.9
2016-09-06T04:26:01.217225: step 847, loss 0.347728, acc 0.88
2016-09-06T04:26:02.057967: step 848, loss 0.168096, acc 0.92
2016-09-06T04:26:02.883836: step 849, loss 0.0739166, acc 0.98
2016-09-06T04:26:03.690457: step 850, loss 0.200146, acc 0.88
2016-09-06T04:26:04.506409: step 851, loss 0.222502, acc 0.9
2016-09-06T04:26:05.340336: step 852, loss 0.0906407, acc 0.98
2016-09-06T04:26:06.145141: step 853, loss 0.155105, acc 0.9
2016-09-06T04:26:06.959091: step 854, loss 0.0832485, acc 0.98
2016-09-06T04:26:07.797367: step 855, loss 0.136569, acc 0.92
2016-09-06T04:26:08.618302: step 856, loss 0.159785, acc 0.92
2016-09-06T04:26:09.413429: step 857, loss 0.158344, acc 0.92
2016-09-06T04:26:10.236479: step 858, loss 0.115439, acc 0.96
2016-09-06T04:26:11.010008: step 859, loss 0.0594784, acc 0.98
2016-09-06T04:26:11.819519: step 860, loss 0.0989358, acc 0.94
2016-09-06T04:26:12.641706: step 861, loss 0.218881, acc 0.9
2016-09-06T04:26:13.429438: step 862, loss 0.258874, acc 0.92
2016-09-06T04:26:14.271062: step 863, loss 0.112805, acc 0.94
2016-09-06T04:26:15.096069: step 864, loss 0.0654353, acc 0.98
2016-09-06T04:26:15.890602: step 865, loss 0.106878, acc 0.96
2016-09-06T04:26:16.720147: step 866, loss 0.123343, acc 0.92
2016-09-06T04:26:17.596413: step 867, loss 0.155183, acc 0.92
2016-09-06T04:26:18.400024: step 868, loss 0.100313, acc 0.96
2016-09-06T04:26:19.202419: step 869, loss 0.0796549, acc 0.96
2016-09-06T04:26:20.033995: step 870, loss 0.153147, acc 0.94
2016-09-06T04:26:20.832474: step 871, loss 0.0551316, acc 1
2016-09-06T04:26:21.640944: step 872, loss 0.165875, acc 0.92
2016-09-06T04:26:22.482177: step 873, loss 0.102314, acc 0.96
2016-09-06T04:26:23.290153: step 874, loss 0.153287, acc 0.92
2016-09-06T04:26:24.133247: step 875, loss 0.125628, acc 0.96
2016-09-06T04:26:24.961329: step 876, loss 0.145591, acc 0.92
2016-09-06T04:26:25.802672: step 877, loss 0.12805, acc 0.94
2016-09-06T04:26:26.605611: step 878, loss 0.14047, acc 0.92
2016-09-06T04:26:27.465416: step 879, loss 0.058454, acc 1
2016-09-06T04:26:28.257367: step 880, loss 0.129351, acc 0.94
2016-09-06T04:26:29.073432: step 881, loss 0.0615963, acc 0.98
2016-09-06T04:26:29.900233: step 882, loss 0.314451, acc 0.92
2016-09-06T04:26:30.702581: step 883, loss 0.0383308, acc 0.98
2016-09-06T04:26:31.522689: step 884, loss 0.0932011, acc 0.98
2016-09-06T04:26:32.419271: step 885, loss 0.124236, acc 0.94
2016-09-06T04:26:33.323663: step 886, loss 0.284927, acc 0.92
2016-09-06T04:26:34.135939: step 887, loss 0.211835, acc 0.9
2016-09-06T04:26:34.967060: step 888, loss 0.127742, acc 0.94
2016-09-06T04:26:35.824302: step 889, loss 0.0679127, acc 0.98
2016-09-06T04:26:36.597021: step 890, loss 0.0542523, acc 0.98
2016-09-06T04:26:37.421854: step 891, loss 0.114328, acc 0.92
2016-09-06T04:26:38.252793: step 892, loss 0.215554, acc 0.94
2016-09-06T04:26:39.077719: step 893, loss 0.100435, acc 0.94
2016-09-06T04:26:39.895052: step 894, loss 0.126777, acc 0.96
2016-09-06T04:26:40.727576: step 895, loss 0.179157, acc 0.9
2016-09-06T04:26:41.550596: step 896, loss 0.222607, acc 0.92
2016-09-06T04:26:42.361429: step 897, loss 0.0764971, acc 0.96
2016-09-06T04:26:43.187350: step 898, loss 0.139882, acc 0.94
2016-09-06T04:26:43.990730: step 899, loss 0.235771, acc 0.88
2016-09-06T04:26:44.799865: step 900, loss 0.181167, acc 0.96

Evaluation:
2016-09-06T04:26:48.545495: step 900, loss 0.676065, acc 0.774859

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-900

2016-09-06T04:26:50.421494: step 901, loss 0.245812, acc 0.88
2016-09-06T04:26:51.233500: step 902, loss 0.117901, acc 0.94
2016-09-06T04:26:52.036406: step 903, loss 0.1567, acc 0.92
2016-09-06T04:26:52.854692: step 904, loss 0.113586, acc 0.94
2016-09-06T04:26:53.678060: step 905, loss 0.0986112, acc 0.96
2016-09-06T04:26:54.481804: step 906, loss 0.118533, acc 0.94
2016-09-06T04:26:55.333345: step 907, loss 0.239019, acc 0.94
2016-09-06T04:26:56.144513: step 908, loss 0.236735, acc 0.9
2016-09-06T04:26:56.967540: step 909, loss 0.149873, acc 0.9
2016-09-06T04:26:57.788143: step 910, loss 0.146847, acc 0.94
2016-09-06T04:26:58.606293: step 911, loss 0.0699796, acc 1
2016-09-06T04:26:59.411195: step 912, loss 0.173929, acc 0.92
2016-09-06T04:27:00.232114: step 913, loss 0.183282, acc 0.94
2016-09-06T04:27:01.041585: step 914, loss 0.295511, acc 0.88
2016-09-06T04:27:01.845402: step 915, loss 0.137995, acc 0.96
2016-09-06T04:27:02.619359: step 916, loss 0.194458, acc 0.88
2016-09-06T04:27:03.453390: step 917, loss 0.136077, acc 0.92
2016-09-06T04:27:04.242200: step 918, loss 0.0779292, acc 0.98
2016-09-06T04:27:05.080933: step 919, loss 0.075684, acc 0.98
2016-09-06T04:27:05.871807: step 920, loss 0.0540783, acc 0.96
2016-09-06T04:27:06.656138: step 921, loss 0.176484, acc 0.9
2016-09-06T04:27:07.497414: step 922, loss 0.107411, acc 0.98
2016-09-06T04:27:08.315675: step 923, loss 0.120956, acc 0.94
2016-09-06T04:27:09.088822: step 924, loss 0.171418, acc 0.9
2016-09-06T04:27:09.929436: step 925, loss 0.169162, acc 0.92
2016-09-06T04:27:10.742282: step 926, loss 0.116678, acc 0.94
2016-09-06T04:27:11.548730: step 927, loss 0.119462, acc 0.94
2016-09-06T04:27:12.359362: step 928, loss 0.137197, acc 0.96
2016-09-06T04:27:13.178884: step 929, loss 0.155939, acc 0.94
2016-09-06T04:27:13.959995: step 930, loss 0.193654, acc 0.9
2016-09-06T04:27:14.763162: step 931, loss 0.105808, acc 0.92
2016-09-06T04:27:15.569817: step 932, loss 0.223803, acc 0.9
2016-09-06T04:27:16.363025: step 933, loss 0.121836, acc 0.92
2016-09-06T04:27:17.162911: step 934, loss 0.118936, acc 0.96
2016-09-06T04:27:17.979555: step 935, loss 0.180453, acc 0.92
2016-09-06T04:27:18.762210: step 936, loss 0.073235, acc 0.96
2016-09-06T04:27:19.596980: step 937, loss 0.180168, acc 0.92
2016-09-06T04:27:20.414850: step 938, loss 0.29482, acc 0.88
2016-09-06T04:27:21.207932: step 939, loss 0.331605, acc 0.92
2016-09-06T04:27:22.008115: step 940, loss 0.274988, acc 0.94
2016-09-06T04:27:22.829521: step 941, loss 0.192253, acc 0.96
2016-09-06T04:27:23.629134: step 942, loss 0.164761, acc 0.94
2016-09-06T04:27:24.435117: step 943, loss 0.0893525, acc 0.98
2016-09-06T04:27:25.251444: step 944, loss 0.206436, acc 0.94
2016-09-06T04:27:26.058390: step 945, loss 0.126992, acc 0.94
2016-09-06T04:27:26.872166: step 946, loss 0.172781, acc 0.94
2016-09-06T04:27:27.694222: step 947, loss 0.134148, acc 0.96
2016-09-06T04:27:28.496172: step 948, loss 0.148479, acc 0.94
2016-09-06T04:27:29.298024: step 949, loss 0.168582, acc 0.94
2016-09-06T04:27:30.140678: step 950, loss 0.141667, acc 0.96
2016-09-06T04:27:30.949194: step 951, loss 0.167741, acc 0.92
2016-09-06T04:27:31.780403: step 952, loss 0.102431, acc 0.98
2016-09-06T04:27:32.612232: step 953, loss 0.0827467, acc 0.98
2016-09-06T04:27:33.411717: step 954, loss 0.120645, acc 0.96
2016-09-06T04:27:34.218634: step 955, loss 0.0926313, acc 0.98
2016-09-06T04:27:35.089965: step 956, loss 0.171644, acc 0.86
2016-09-06T04:27:35.914797: step 957, loss 0.229555, acc 0.9
2016-09-06T04:27:36.725406: step 958, loss 0.130957, acc 0.94
2016-09-06T04:27:37.552487: step 959, loss 0.127896, acc 0.94
2016-09-06T04:27:38.315479: step 960, loss 0.149584, acc 0.931818
2016-09-06T04:27:39.131338: step 961, loss 0.0818668, acc 0.98
2016-09-06T04:27:39.970868: step 962, loss 0.132813, acc 0.94
2016-09-06T04:27:40.786384: step 963, loss 0.127483, acc 0.96
2016-09-06T04:27:41.631662: step 964, loss 0.111308, acc 0.96
2016-09-06T04:27:42.476512: step 965, loss 0.0810859, acc 0.98
2016-09-06T04:27:43.305534: step 966, loss 0.136059, acc 0.94
2016-09-06T04:27:44.096183: step 967, loss 0.066391, acc 0.98
2016-09-06T04:27:44.909181: step 968, loss 0.133064, acc 0.96
2016-09-06T04:27:45.732323: step 969, loss 0.279809, acc 0.86
2016-09-06T04:27:46.528475: step 970, loss 0.0538488, acc 1
2016-09-06T04:27:47.349653: step 971, loss 0.0194277, acc 1
2016-09-06T04:27:48.186987: step 972, loss 0.0884897, acc 0.94
2016-09-06T04:27:48.966002: step 973, loss 0.107239, acc 0.96
2016-09-06T04:27:49.766023: step 974, loss 0.0657287, acc 0.96
2016-09-06T04:27:50.583847: step 975, loss 0.0721303, acc 0.96
2016-09-06T04:27:51.393959: step 976, loss 0.0246594, acc 1
2016-09-06T04:27:52.217449: step 977, loss 0.175533, acc 0.94
2016-09-06T04:27:53.033621: step 978, loss 0.0351078, acc 0.98
2016-09-06T04:27:53.810626: step 979, loss 0.0450265, acc 1
2016-09-06T04:27:54.632125: step 980, loss 0.0927365, acc 0.96
2016-09-06T04:27:55.459131: step 981, loss 0.0644736, acc 0.98
2016-09-06T04:27:56.253229: step 982, loss 0.196652, acc 0.94
2016-09-06T04:27:57.056746: step 983, loss 0.0803173, acc 0.96
2016-09-06T04:27:57.842389: step 984, loss 0.0883236, acc 0.98
2016-09-06T04:27:58.640357: step 985, loss 0.0265299, acc 1
2016-09-06T04:27:59.482852: step 986, loss 0.11077, acc 0.92
2016-09-06T04:28:00.306171: step 987, loss 0.0663441, acc 0.96
2016-09-06T04:28:01.075208: step 988, loss 0.0585474, acc 0.98
2016-09-06T04:28:01.900991: step 989, loss 0.0860198, acc 0.98
2016-09-06T04:28:02.733294: step 990, loss 0.185153, acc 0.96
2016-09-06T04:28:03.526511: step 991, loss 0.167831, acc 0.96
2016-09-06T04:28:04.332264: step 992, loss 0.106635, acc 0.94
2016-09-06T04:28:05.155224: step 993, loss 0.256009, acc 0.92
2016-09-06T04:28:05.967780: step 994, loss 0.0871501, acc 0.96
2016-09-06T04:28:06.797683: step 995, loss 0.147984, acc 0.94
2016-09-06T04:28:07.621559: step 996, loss 0.0856107, acc 0.96
2016-09-06T04:28:08.423367: step 997, loss 0.0679999, acc 0.98
2016-09-06T04:28:09.258708: step 998, loss 0.116737, acc 0.96
2016-09-06T04:28:10.093388: step 999, loss 0.107833, acc 0.96
2016-09-06T04:28:10.912929: step 1000, loss 0.252743, acc 0.94

Evaluation:
2016-09-06T04:28:14.628945: step 1000, loss 0.831151, acc 0.773921

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-1000

2016-09-06T04:28:16.471508: step 1001, loss 0.0736348, acc 0.98
2016-09-06T04:28:17.281622: step 1002, loss 0.0271137, acc 1
2016-09-06T04:28:18.076926: step 1003, loss 0.107825, acc 0.94
2016-09-06T04:28:18.864823: step 1004, loss 0.0525497, acc 0.98
2016-09-06T04:28:19.670676: step 1005, loss 0.110694, acc 0.96
2016-09-06T04:28:20.420111: step 1006, loss 0.0621397, acc 1
2016-09-06T04:28:21.208856: step 1007, loss 0.117759, acc 0.96
2016-09-06T04:28:22.026365: step 1008, loss 0.148245, acc 0.92
2016-09-06T04:28:22.805351: step 1009, loss 0.230216, acc 0.88
2016-09-06T04:28:23.610405: step 1010, loss 0.0830693, acc 0.98
2016-09-06T04:28:24.423982: step 1011, loss 0.0549285, acc 1
2016-09-06T04:28:25.223079: step 1012, loss 0.102349, acc 0.96
2016-09-06T04:28:26.042649: step 1013, loss 0.0755324, acc 0.98
2016-09-06T04:28:26.856171: step 1014, loss 0.236971, acc 0.9
2016-09-06T04:28:27.649407: step 1015, loss 0.130223, acc 0.94
2016-09-06T04:28:28.482637: step 1016, loss 0.177658, acc 0.9
2016-09-06T04:28:29.286542: step 1017, loss 0.0797509, acc 0.96
2016-09-06T04:28:30.095320: step 1018, loss 0.140437, acc 0.92
2016-09-06T04:28:30.895686: step 1019, loss 0.105506, acc 0.94
2016-09-06T04:28:31.713039: step 1020, loss 0.0478118, acc 1
2016-09-06T04:28:32.518623: step 1021, loss 0.19124, acc 0.9
2016-09-06T04:28:33.341810: step 1022, loss 0.188972, acc 0.92
2016-09-06T04:28:34.145741: step 1023, loss 0.0617979, acc 0.98
2016-09-06T04:28:34.925412: step 1024, loss 0.0591986, acc 0.98
2016-09-06T04:28:35.784871: step 1025, loss 0.15646, acc 0.92
2016-09-06T04:28:36.590361: step 1026, loss 0.132002, acc 0.94
2016-09-06T04:28:37.404495: step 1027, loss 0.0365439, acc 1
2016-09-06T04:28:38.198662: step 1028, loss 0.129722, acc 0.94
2016-09-06T04:28:39.000662: step 1029, loss 0.177369, acc 0.96
2016-09-06T04:28:39.770283: step 1030, loss 0.119944, acc 0.92
2016-09-06T04:28:40.580849: step 1031, loss 0.124366, acc 0.96
2016-09-06T04:28:41.410167: step 1032, loss 0.0961337, acc 0.98
2016-09-06T04:28:42.193414: step 1033, loss 0.0494501, acc 1
2016-09-06T04:28:43.014138: step 1034, loss 0.0374705, acc 1
2016-09-06T04:28:43.835078: step 1035, loss 0.0728979, acc 0.96
2016-09-06T04:28:44.619141: step 1036, loss 0.0268095, acc 1
2016-09-06T04:28:45.404122: step 1037, loss 0.0458256, acc 0.98
2016-09-06T04:28:46.240834: step 1038, loss 0.0622813, acc 0.96
2016-09-06T04:28:47.030124: step 1039, loss 0.15707, acc 0.9
2016-09-06T04:28:47.818027: step 1040, loss 0.160627, acc 0.92
2016-09-06T04:28:48.647593: step 1041, loss 0.0850837, acc 0.94
2016-09-06T04:28:49.434360: step 1042, loss 0.0642458, acc 0.96
2016-09-06T04:28:50.242056: step 1043, loss 0.0850449, acc 0.94
2016-09-06T04:28:51.093540: step 1044, loss 0.0570543, acc 0.96
2016-09-06T04:28:51.945883: step 1045, loss 0.0438605, acc 0.98
2016-09-06T04:28:52.791072: step 1046, loss 0.122732, acc 0.94
2016-09-06T04:28:53.603376: step 1047, loss 0.101531, acc 0.96
2016-09-06T04:28:54.437433: step 1048, loss 0.0948954, acc 0.96
2016-09-06T04:28:55.254344: step 1049, loss 0.094164, acc 0.94
2016-09-06T04:28:56.124649: step 1050, loss 0.0127365, acc 1
2016-09-06T04:28:56.948940: step 1051, loss 0.109446, acc 0.94
2016-09-06T04:28:57.744428: step 1052, loss 0.0217576, acc 1
2016-09-06T04:28:58.569415: step 1053, loss 0.0296476, acc 1
2016-09-06T04:28:59.386342: step 1054, loss 0.0857062, acc 0.96
2016-09-06T04:29:00.210871: step 1055, loss 0.0785279, acc 0.96
2016-09-06T04:29:01.070550: step 1056, loss 0.178783, acc 0.96
2016-09-06T04:29:01.886243: step 1057, loss 0.042424, acc 1
2016-09-06T04:29:02.703331: step 1058, loss 0.169001, acc 0.92
2016-09-06T04:29:03.512253: step 1059, loss 0.159431, acc 0.88
2016-09-06T04:29:04.330834: step 1060, loss 0.0216971, acc 1
2016-09-06T04:29:05.135892: step 1061, loss 0.175761, acc 0.94
2016-09-06T04:29:05.940634: step 1062, loss 0.0659062, acc 0.96
2016-09-06T04:29:06.777763: step 1063, loss 0.140465, acc 0.98
2016-09-06T04:29:07.594054: step 1064, loss 0.0335796, acc 1
2016-09-06T04:29:08.406937: step 1065, loss 0.0799542, acc 0.96
2016-09-06T04:29:09.227313: step 1066, loss 0.0384614, acc 1
2016-09-06T04:29:10.043144: step 1067, loss 0.172105, acc 0.92
2016-09-06T04:29:10.859109: step 1068, loss 0.146974, acc 0.96
2016-09-06T04:29:11.719536: step 1069, loss 0.108418, acc 0.98
2016-09-06T04:29:12.525737: step 1070, loss 0.0778693, acc 0.96
2016-09-06T04:29:13.357897: step 1071, loss 0.0668247, acc 0.96
2016-09-06T04:29:14.252400: step 1072, loss 0.157731, acc 0.94
2016-09-06T04:29:15.050426: step 1073, loss 0.0513216, acc 1
2016-09-06T04:29:15.860069: step 1074, loss 0.0721233, acc 0.98
2016-09-06T04:29:16.706108: step 1075, loss 0.0813239, acc 0.98
2016-09-06T04:29:17.535430: step 1076, loss 0.127639, acc 0.94
2016-09-06T04:29:18.379798: step 1077, loss 0.0873638, acc 0.96
2016-09-06T04:29:19.196377: step 1078, loss 0.0795315, acc 0.96
2016-09-06T04:29:20.045128: step 1079, loss 0.172573, acc 0.94
2016-09-06T04:29:20.851746: step 1080, loss 0.207403, acc 0.94
2016-09-06T04:29:21.659067: step 1081, loss 0.115488, acc 0.96
2016-09-06T04:29:22.472487: step 1082, loss 0.19889, acc 0.94
2016-09-06T04:29:23.285449: step 1083, loss 0.0467584, acc 1
2016-09-06T04:29:24.106648: step 1084, loss 0.169705, acc 0.94
2016-09-06T04:29:24.931176: step 1085, loss 0.0679863, acc 1
2016-09-06T04:29:25.721490: step 1086, loss 0.120831, acc 0.92
2016-09-06T04:29:26.513127: step 1087, loss 0.103432, acc 0.94
2016-09-06T04:29:27.310744: step 1088, loss 0.114695, acc 0.94
2016-09-06T04:29:28.095871: step 1089, loss 0.246927, acc 0.88
2016-09-06T04:29:28.900461: step 1090, loss 0.215068, acc 0.96
2016-09-06T04:29:29.754901: step 1091, loss 0.0761477, acc 0.96
2016-09-06T04:29:30.551206: step 1092, loss 0.146174, acc 0.94
2016-09-06T04:29:31.352342: step 1093, loss 0.0864227, acc 0.96
2016-09-06T04:29:32.175946: step 1094, loss 0.113489, acc 0.94
2016-09-06T04:29:32.977295: step 1095, loss 0.192618, acc 0.94
2016-09-06T04:29:33.784853: step 1096, loss 0.140558, acc 0.94
2016-09-06T04:29:34.589370: step 1097, loss 0.149218, acc 0.88
2016-09-06T04:29:35.363458: step 1098, loss 0.173494, acc 0.92
2016-09-06T04:29:36.151712: step 1099, loss 0.0637357, acc 0.98
2016-09-06T04:29:36.990573: step 1100, loss 0.0600844, acc 0.98

Evaluation:
2016-09-06T04:29:40.709750: step 1100, loss 0.799641, acc 0.765478

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-1100

2016-09-06T04:29:42.680223: step 1101, loss 0.106509, acc 0.98
2016-09-06T04:29:43.501182: step 1102, loss 0.0957123, acc 0.96
2016-09-06T04:29:44.327192: step 1103, loss 0.139664, acc 0.94
2016-09-06T04:29:45.118599: step 1104, loss 0.117449, acc 0.94
2016-09-06T04:29:45.922012: step 1105, loss 0.141732, acc 0.9
2016-09-06T04:29:46.761644: step 1106, loss 0.108056, acc 0.98
2016-09-06T04:29:47.563684: step 1107, loss 0.180413, acc 0.9
2016-09-06T04:29:48.358904: step 1108, loss 0.0616395, acc 0.98
2016-09-06T04:29:49.203073: step 1109, loss 0.0934056, acc 0.94
2016-09-06T04:29:50.025465: step 1110, loss 0.113122, acc 0.94
2016-09-06T04:29:50.833127: step 1111, loss 0.123533, acc 0.94
2016-09-06T04:29:51.676424: step 1112, loss 0.0503314, acc 0.98
2016-09-06T04:29:52.496508: step 1113, loss 0.126576, acc 0.92
2016-09-06T04:29:53.325975: step 1114, loss 0.0837052, acc 0.96
2016-09-06T04:29:54.145466: step 1115, loss 0.0502755, acc 0.98
2016-09-06T04:29:54.954988: step 1116, loss 0.145112, acc 0.94
2016-09-06T04:29:55.777598: step 1117, loss 0.155273, acc 0.92
2016-09-06T04:29:56.606660: step 1118, loss 0.15068, acc 0.98
2016-09-06T04:29:57.462866: step 1119, loss 0.136026, acc 0.92
2016-09-06T04:29:58.257618: step 1120, loss 0.138831, acc 0.94
2016-09-06T04:29:59.084534: step 1121, loss 0.118977, acc 0.94
2016-09-06T04:29:59.885326: step 1122, loss 0.264134, acc 0.94
2016-09-06T04:30:00.700547: step 1123, loss 0.121223, acc 0.96
2016-09-06T04:30:01.522023: step 1124, loss 0.114439, acc 0.96
2016-09-06T04:30:02.348957: step 1125, loss 0.0655308, acc 0.96
2016-09-06T04:30:03.142746: step 1126, loss 0.0848788, acc 0.94
2016-09-06T04:30:03.933761: step 1127, loss 0.0235293, acc 1
2016-09-06T04:30:04.754187: step 1128, loss 0.0853196, acc 0.98
2016-09-06T04:30:05.540667: step 1129, loss 0.251181, acc 0.86
2016-09-06T04:30:06.346535: step 1130, loss 0.089201, acc 0.96
2016-09-06T04:30:07.171637: step 1131, loss 0.149192, acc 0.94
2016-09-06T04:30:07.957076: step 1132, loss 0.11135, acc 0.96
2016-09-06T04:30:08.780047: step 1133, loss 0.112524, acc 0.96
2016-09-06T04:30:09.600536: step 1134, loss 0.232667, acc 0.94
2016-09-06T04:30:10.400130: step 1135, loss 0.17966, acc 0.92
2016-09-06T04:30:11.202296: step 1136, loss 0.106121, acc 0.94
2016-09-06T04:30:12.010118: step 1137, loss 0.101694, acc 0.98
2016-09-06T04:30:12.793506: step 1138, loss 0.117219, acc 0.96
2016-09-06T04:30:13.618557: step 1139, loss 0.150218, acc 0.92
2016-09-06T04:30:14.429720: step 1140, loss 0.225504, acc 0.92
2016-09-06T04:30:15.241953: step 1141, loss 0.0424911, acc 0.98
2016-09-06T04:30:16.027819: step 1142, loss 0.0751762, acc 1
2016-09-06T04:30:16.829406: step 1143, loss 0.0654205, acc 0.98
2016-09-06T04:30:17.638653: step 1144, loss 0.0744736, acc 0.98
2016-09-06T04:30:18.456936: step 1145, loss 0.168145, acc 0.9
2016-09-06T04:30:19.280767: step 1146, loss 0.0860991, acc 0.94
2016-09-06T04:30:20.055984: step 1147, loss 0.0986281, acc 0.96
2016-09-06T04:30:20.874461: step 1148, loss 0.0852512, acc 0.96
2016-09-06T04:30:21.696374: step 1149, loss 0.084359, acc 0.98
2016-09-06T04:30:22.516267: step 1150, loss 0.148964, acc 0.9
2016-09-06T04:30:23.309130: step 1151, loss 0.0900904, acc 0.96
2016-09-06T04:30:24.077534: step 1152, loss 0.239563, acc 0.863636
2016-09-06T04:30:24.909641: step 1153, loss 0.107129, acc 0.96
2016-09-06T04:30:25.746232: step 1154, loss 0.0882578, acc 0.94
2016-09-06T04:30:26.586714: step 1155, loss 0.0239953, acc 1
2016-09-06T04:30:27.390724: step 1156, loss 0.151714, acc 0.92
2016-09-06T04:30:28.208043: step 1157, loss 0.0389337, acc 1
2016-09-06T04:30:29.025749: step 1158, loss 0.0496351, acc 0.98
2016-09-06T04:30:29.834457: step 1159, loss 0.091772, acc 0.96
2016-09-06T04:30:30.653781: step 1160, loss 0.0325131, acc 1
2016-09-06T04:30:31.467289: step 1161, loss 0.100399, acc 0.94
2016-09-06T04:30:32.300603: step 1162, loss 0.0581858, acc 0.98
2016-09-06T04:30:33.121543: step 1163, loss 0.113702, acc 0.96
2016-09-06T04:30:33.953469: step 1164, loss 0.0454029, acc 0.98
2016-09-06T04:30:34.772109: step 1165, loss 0.0797551, acc 0.96
2016-09-06T04:30:35.562894: step 1166, loss 0.119416, acc 0.94
2016-09-06T04:30:36.380988: step 1167, loss 0.109643, acc 0.94
2016-09-06T04:30:37.212731: step 1168, loss 0.177467, acc 0.94
2016-09-06T04:30:38.029486: step 1169, loss 0.10863, acc 0.96
2016-09-06T04:30:38.848602: step 1170, loss 0.0362281, acc 1
2016-09-06T04:30:39.665817: step 1171, loss 0.0505816, acc 0.98
2016-09-06T04:30:40.468914: step 1172, loss 0.0259717, acc 1
2016-09-06T04:30:41.309744: step 1173, loss 0.0762504, acc 0.98
2016-09-06T04:30:42.124379: step 1174, loss 0.12185, acc 0.94
2016-09-06T04:30:42.935281: step 1175, loss 0.0521849, acc 0.98
2016-09-06T04:30:43.792270: step 1176, loss 0.0829755, acc 0.96
2016-09-06T04:30:44.621969: step 1177, loss 0.0551153, acc 0.98
2016-09-06T04:30:45.397789: step 1178, loss 0.275442, acc 0.92
2016-09-06T04:30:46.251647: step 1179, loss 0.0497962, acc 1
2016-09-06T04:30:47.068094: step 1180, loss 0.0369053, acc 0.98
2016-09-06T04:30:47.862314: step 1181, loss 0.14416, acc 0.92
2016-09-06T04:30:48.676030: step 1182, loss 0.0410716, acc 0.98
2016-09-06T04:30:49.498791: step 1183, loss 0.0833656, acc 0.98
2016-09-06T04:30:50.281022: step 1184, loss 0.0510856, acc 0.98
2016-09-06T04:30:51.090354: step 1185, loss 0.174859, acc 0.96
2016-09-06T04:30:51.930838: step 1186, loss 0.0571764, acc 0.96
2016-09-06T04:30:52.732086: step 1187, loss 0.0214532, acc 1
2016-09-06T04:30:53.570623: step 1188, loss 0.13148, acc 0.92
2016-09-06T04:30:54.392139: step 1189, loss 0.0284659, acc 1
2016-09-06T04:30:55.184186: step 1190, loss 0.0996577, acc 0.96
2016-09-06T04:30:55.985401: step 1191, loss 0.133391, acc 0.96
2016-09-06T04:30:56.815718: step 1192, loss 0.0548872, acc 0.98
2016-09-06T04:30:57.593025: step 1193, loss 0.131496, acc 0.94
2016-09-06T04:30:58.391919: step 1194, loss 0.0518342, acc 0.98
2016-09-06T04:30:59.217548: step 1195, loss 0.0456865, acc 1
2016-09-06T04:31:00.003662: step 1196, loss 0.0476066, acc 0.98
2016-09-06T04:31:00.845575: step 1197, loss 0.00867975, acc 1
2016-09-06T04:31:01.680878: step 1198, loss 0.0602799, acc 0.98
2016-09-06T04:31:02.525379: step 1199, loss 0.0530124, acc 0.98
2016-09-06T04:31:03.313300: step 1200, loss 0.0754698, acc 0.96

Evaluation:
2016-09-06T04:31:07.044403: step 1200, loss 1.06852, acc 0.761726

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-1200

2016-09-06T04:31:08.978589: step 1201, loss 0.122992, acc 0.96
2016-09-06T04:31:09.783375: step 1202, loss 0.0220372, acc 1
2016-09-06T04:31:10.603831: step 1203, loss 0.0741203, acc 0.96
2016-09-06T04:31:11.435564: step 1204, loss 0.206688, acc 0.88
2016-09-06T04:31:12.199498: step 1205, loss 0.0534569, acc 0.96
2016-09-06T04:31:13.018942: step 1206, loss 0.0529836, acc 1
2016-09-06T04:31:13.829608: step 1207, loss 0.0548407, acc 0.98
2016-09-06T04:31:14.643470: step 1208, loss 0.0420774, acc 0.98
2016-09-06T04:31:15.475169: step 1209, loss 0.0595657, acc 0.96
2016-09-06T04:31:16.286609: step 1210, loss 0.261584, acc 0.92
2016-09-06T04:31:17.057903: step 1211, loss 0.0523508, acc 0.98
2016-09-06T04:31:17.862899: step 1212, loss 0.0701756, acc 0.98
2016-09-06T04:31:18.660294: step 1213, loss 0.0615046, acc 1
2016-09-06T04:31:19.448568: step 1214, loss 0.024205, acc 1
2016-09-06T04:31:20.250470: step 1215, loss 0.0314009, acc 1
2016-09-06T04:31:21.092227: step 1216, loss 0.0535914, acc 0.98
2016-09-06T04:31:21.908091: step 1217, loss 0.146802, acc 0.92
2016-09-06T04:31:22.705705: step 1218, loss 0.171374, acc 0.92
2016-09-06T04:31:23.494425: step 1219, loss 0.117855, acc 0.94
2016-09-06T04:31:24.267171: step 1220, loss 0.117375, acc 0.96
2016-09-06T04:31:25.082981: step 1221, loss 0.153675, acc 0.94
2016-09-06T04:31:25.900290: step 1222, loss 0.145783, acc 0.94
2016-09-06T04:31:26.694153: step 1223, loss 0.0964791, acc 0.96
2016-09-06T04:31:27.507321: step 1224, loss 0.0718805, acc 0.96
2016-09-06T04:31:28.322275: step 1225, loss 0.0985934, acc 0.94
2016-09-06T04:31:29.149404: step 1226, loss 0.0524176, acc 0.98
2016-09-06T04:31:29.943500: step 1227, loss 0.169019, acc 0.92
2016-09-06T04:31:30.772986: step 1228, loss 0.121458, acc 0.96
2016-09-06T04:31:31.561516: step 1229, loss 0.0602892, acc 0.98
2016-09-06T04:31:32.381396: step 1230, loss 0.081928, acc 0.94
2016-09-06T04:31:33.162990: step 1231, loss 0.145062, acc 0.96
2016-09-06T04:31:33.962721: step 1232, loss 0.101775, acc 0.96
2016-09-06T04:31:34.775505: step 1233, loss 0.150814, acc 0.96
2016-09-06T04:31:35.615449: step 1234, loss 0.0666621, acc 0.96
2016-09-06T04:31:36.420220: step 1235, loss 0.117564, acc 0.96
2016-09-06T04:31:37.221008: step 1236, loss 0.0558771, acc 0.96
2016-09-06T04:31:38.021233: step 1237, loss 0.122252, acc 0.94
2016-09-06T04:31:38.798313: step 1238, loss 0.117988, acc 0.92
2016-09-06T04:31:39.596159: step 1239, loss 0.0354537, acc 0.98
2016-09-06T04:31:40.403565: step 1240, loss 0.0963523, acc 0.96
2016-09-06T04:31:41.191027: step 1241, loss 0.163002, acc 0.9
2016-09-06T04:31:42.007562: step 1242, loss 0.0828425, acc 0.94
2016-09-06T04:31:42.829428: step 1243, loss 0.100149, acc 0.94
2016-09-06T04:31:43.628938: step 1244, loss 0.0256067, acc 1
2016-09-06T04:31:44.437707: step 1245, loss 0.145734, acc 0.96
2016-09-06T04:31:45.288788: step 1246, loss 0.122864, acc 0.92
2016-09-06T04:31:46.069448: step 1247, loss 0.103649, acc 0.92
2016-09-06T04:31:46.847072: step 1248, loss 0.0495646, acc 0.98
2016-09-06T04:31:47.686600: step 1249, loss 0.0489413, acc 1
2016-09-06T04:31:48.463907: step 1250, loss 0.0940095, acc 0.98
2016-09-06T04:31:49.301282: step 1251, loss 0.123765, acc 0.94
2016-09-06T04:31:50.118560: step 1252, loss 0.0824385, acc 0.98
2016-09-06T04:31:50.929270: step 1253, loss 0.117309, acc 0.92
2016-09-06T04:31:51.744389: step 1254, loss 0.059455, acc 0.98
2016-09-06T04:31:52.588819: step 1255, loss 0.0966428, acc 0.96
2016-09-06T04:31:53.414172: step 1256, loss 0.0299631, acc 1
2016-09-06T04:31:54.212716: step 1257, loss 0.190377, acc 0.92
2016-09-06T04:31:55.037411: step 1258, loss 0.112839, acc 0.96
2016-09-06T04:31:55.826721: step 1259, loss 0.0679668, acc 0.96
2016-09-06T04:31:56.619990: step 1260, loss 0.0474724, acc 1
2016-09-06T04:31:57.458307: step 1261, loss 0.0943146, acc 0.98
2016-09-06T04:31:58.247627: step 1262, loss 0.0666946, acc 0.96
2016-09-06T04:31:59.045060: step 1263, loss 0.0262833, acc 1
2016-09-06T04:31:59.877336: step 1264, loss 0.123662, acc 0.94
2016-09-06T04:32:00.702826: step 1265, loss 0.134469, acc 0.94
2016-09-06T04:32:01.538090: step 1266, loss 0.059575, acc 0.98
2016-09-06T04:32:02.376291: step 1267, loss 0.0629581, acc 0.98
2016-09-06T04:32:03.219510: step 1268, loss 0.145528, acc 0.96
2016-09-06T04:32:04.020325: step 1269, loss 0.118593, acc 0.92
2016-09-06T04:32:04.822644: step 1270, loss 0.0224495, acc 1
2016-09-06T04:32:05.637599: step 1271, loss 0.133264, acc 0.94
2016-09-06T04:32:06.450289: step 1272, loss 0.0538689, acc 0.98
2016-09-06T04:32:07.301610: step 1273, loss 0.124967, acc 0.94
2016-09-06T04:32:08.115119: step 1274, loss 0.0568033, acc 0.96
2016-09-06T04:32:08.946623: step 1275, loss 0.025297, acc 1
2016-09-06T04:32:09.768995: step 1276, loss 0.059161, acc 0.96
2016-09-06T04:32:10.600918: step 1277, loss 0.0416125, acc 0.98
2016-09-06T04:32:11.404587: step 1278, loss 0.122122, acc 0.94
2016-09-06T04:32:12.203460: step 1279, loss 0.0935536, acc 0.96
2016-09-06T04:32:13.044327: step 1280, loss 0.0437853, acc 0.98
2016-09-06T04:32:13.882004: step 1281, loss 0.198518, acc 0.9
2016-09-06T04:32:14.700664: step 1282, loss 0.247764, acc 0.88
2016-09-06T04:32:15.546708: step 1283, loss 0.132806, acc 0.94
2016-09-06T04:32:16.352342: step 1284, loss 0.109135, acc 0.94
2016-09-06T04:32:17.154995: step 1285, loss 0.0280697, acc 1
2016-09-06T04:32:17.985671: step 1286, loss 0.0668189, acc 0.98
2016-09-06T04:32:18.810107: step 1287, loss 0.0629019, acc 0.96
2016-09-06T04:32:19.635127: step 1288, loss 0.0792238, acc 0.96
2016-09-06T04:32:20.461799: step 1289, loss 0.119254, acc 0.92
2016-09-06T04:32:21.274690: step 1290, loss 0.0797086, acc 0.96
2016-09-06T04:32:22.121025: step 1291, loss 0.0865783, acc 0.96
2016-09-06T04:32:22.967719: step 1292, loss 0.0991576, acc 0.94
2016-09-06T04:32:23.797977: step 1293, loss 0.14406, acc 0.96
2016-09-06T04:32:24.587763: step 1294, loss 0.0671984, acc 0.98
2016-09-06T04:32:25.411433: step 1295, loss 0.0941062, acc 0.94
2016-09-06T04:32:26.218559: step 1296, loss 0.103138, acc 0.94
2016-09-06T04:32:27.038934: step 1297, loss 0.0435912, acc 0.98
2016-09-06T04:32:27.855114: step 1298, loss 0.0707632, acc 0.96
2016-09-06T04:32:28.698751: step 1299, loss 0.120493, acc 0.94
2016-09-06T04:32:29.536554: step 1300, loss 0.11157, acc 0.94

Evaluation:
2016-09-06T04:32:33.234882: step 1300, loss 1.03764, acc 0.757974

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-1300

2016-09-06T04:32:35.174872: step 1301, loss 0.0689542, acc 0.96
2016-09-06T04:32:35.986849: step 1302, loss 0.0774907, acc 0.98
2016-09-06T04:32:36.784370: step 1303, loss 0.0252402, acc 1
2016-09-06T04:32:37.609435: step 1304, loss 0.0838625, acc 0.96
2016-09-06T04:32:38.418108: step 1305, loss 0.0449686, acc 0.98
2016-09-06T04:32:39.249395: step 1306, loss 0.117326, acc 0.94
2016-09-06T04:32:40.058431: step 1307, loss 0.0473547, acc 1
2016-09-06T04:32:40.882181: step 1308, loss 0.0306468, acc 1
2016-09-06T04:32:41.682933: step 1309, loss 0.0552986, acc 0.96
2016-09-06T04:32:42.516233: step 1310, loss 0.103084, acc 0.96
2016-09-06T04:32:43.333715: step 1311, loss 0.132913, acc 0.94
2016-09-06T04:32:44.109235: step 1312, loss 0.195355, acc 0.92
2016-09-06T04:32:44.908565: step 1313, loss 0.163494, acc 0.92
2016-09-06T04:32:45.745916: step 1314, loss 0.0298589, acc 1
2016-09-06T04:32:46.524297: step 1315, loss 0.188055, acc 0.88
2016-09-06T04:32:47.332262: step 1316, loss 0.018807, acc 1
2016-09-06T04:32:48.143427: step 1317, loss 0.0858472, acc 0.94
2016-09-06T04:32:48.928758: step 1318, loss 0.124044, acc 0.94
2016-09-06T04:32:49.716458: step 1319, loss 0.0800564, acc 0.96
2016-09-06T04:32:50.551050: step 1320, loss 0.0986256, acc 0.96
2016-09-06T04:32:51.351927: step 1321, loss 0.150851, acc 0.92
2016-09-06T04:32:52.171046: step 1322, loss 0.106829, acc 0.94
2016-09-06T04:32:52.967713: step 1323, loss 0.0737914, acc 0.96
2016-09-06T04:32:53.781440: step 1324, loss 0.0570354, acc 0.98
2016-09-06T04:32:54.569306: step 1325, loss 0.101075, acc 0.98
2016-09-06T04:32:55.410740: step 1326, loss 0.0111869, acc 1
2016-09-06T04:32:56.210050: step 1327, loss 0.094237, acc 0.98
2016-09-06T04:32:56.998105: step 1328, loss 0.0607387, acc 0.98
2016-09-06T04:32:57.809094: step 1329, loss 0.1171, acc 0.94
2016-09-06T04:32:58.630094: step 1330, loss 0.118151, acc 0.96
2016-09-06T04:32:59.436123: step 1331, loss 0.0507962, acc 0.98
2016-09-06T04:33:00.260580: step 1332, loss 0.0512866, acc 0.98
2016-09-06T04:33:01.050411: step 1333, loss 0.130751, acc 0.92
2016-09-06T04:33:01.816108: step 1334, loss 0.127763, acc 0.92
2016-09-06T04:33:02.656641: step 1335, loss 0.0265127, acc 1
2016-09-06T04:33:03.420520: step 1336, loss 0.0852592, acc 0.94
2016-09-06T04:33:04.220076: step 1337, loss 0.0461994, acc 0.98
2016-09-06T04:33:05.037903: step 1338, loss 0.122442, acc 0.94
2016-09-06T04:33:05.837013: step 1339, loss 0.0943303, acc 0.96
2016-09-06T04:33:06.649725: step 1340, loss 0.0969292, acc 0.96
2016-09-06T04:33:07.472077: step 1341, loss 0.0373061, acc 0.98
2016-09-06T04:33:08.272224: step 1342, loss 0.157583, acc 0.94
2016-09-06T04:33:09.091719: step 1343, loss 0.0243621, acc 1
2016-09-06T04:33:09.870351: step 1344, loss 0.125489, acc 0.954545
2016-09-06T04:33:10.658547: step 1345, loss 0.11087, acc 0.94
2016-09-06T04:33:11.470130: step 1346, loss 0.0315438, acc 0.98
2016-09-06T04:33:12.285054: step 1347, loss 0.074712, acc 0.96
2016-09-06T04:33:13.094530: step 1348, loss 0.140476, acc 0.96
2016-09-06T04:33:13.943330: step 1349, loss 0.0420594, acc 1
2016-09-06T04:33:14.746252: step 1350, loss 0.0956165, acc 0.98
2016-09-06T04:33:15.537406: step 1351, loss 0.054233, acc 0.98
2016-09-06T04:33:16.372988: step 1352, loss 0.0349781, acc 1
2016-09-06T04:33:17.247320: step 1353, loss 0.135891, acc 0.94
2016-09-06T04:33:18.083744: step 1354, loss 0.0800238, acc 0.96
2016-09-06T04:33:18.919629: step 1355, loss 0.0902677, acc 0.96
2016-09-06T04:33:19.758202: step 1356, loss 0.127662, acc 0.96
2016-09-06T04:33:20.577792: step 1357, loss 0.0901619, acc 0.96
2016-09-06T04:33:21.399375: step 1358, loss 0.0547634, acc 0.98
2016-09-06T04:33:22.224641: step 1359, loss 0.0492121, acc 0.98
2016-09-06T04:33:23.069504: step 1360, loss 0.0189687, acc 1
2016-09-06T04:33:23.902813: step 1361, loss 0.0526474, acc 0.96
2016-09-06T04:33:24.730328: step 1362, loss 0.011602, acc 1
2016-09-06T04:33:25.536927: step 1363, loss 0.0392046, acc 0.98
2016-09-06T04:33:26.359666: step 1364, loss 0.0210694, acc 1
2016-09-06T04:33:27.210886: step 1365, loss 0.0304893, acc 0.98
2016-09-06T04:33:28.033513: step 1366, loss 0.158662, acc 0.92
2016-09-06T04:33:28.800049: step 1367, loss 0.0311638, acc 1
2016-09-06T04:33:29.599694: step 1368, loss 0.0745871, acc 0.98
2016-09-06T04:33:30.437594: step 1369, loss 0.0615149, acc 0.98
2016-09-06T04:33:31.226665: step 1370, loss 0.0619354, acc 1
2016-09-06T04:33:32.033568: step 1371, loss 0.0753439, acc 0.96
2016-09-06T04:33:32.882640: step 1372, loss 0.0420126, acc 1
2016-09-06T04:33:33.679682: step 1373, loss 0.078529, acc 0.96
2016-09-06T04:33:34.466421: step 1374, loss 0.141543, acc 0.96
2016-09-06T04:33:35.303662: step 1375, loss 0.18275, acc 0.9
2016-09-06T04:33:36.095629: step 1376, loss 0.098999, acc 0.96
2016-09-06T04:33:36.898453: step 1377, loss 0.133191, acc 0.94
2016-09-06T04:33:37.711470: step 1378, loss 0.0614366, acc 0.98
2016-09-06T04:33:38.490320: step 1379, loss 0.0491635, acc 1
2016-09-06T04:33:39.270876: step 1380, loss 0.0693403, acc 0.98
2016-09-06T04:33:40.099152: step 1381, loss 0.0411206, acc 0.98
2016-09-06T04:33:40.930168: step 1382, loss 0.0601465, acc 0.96
2016-09-06T04:33:41.744807: step 1383, loss 0.143927, acc 0.9
2016-09-06T04:33:42.547060: step 1384, loss 0.110024, acc 0.92
2016-09-06T04:33:43.339972: step 1385, loss 0.11975, acc 0.92
2016-09-06T04:33:44.157098: step 1386, loss 0.0364416, acc 1
2016-09-06T04:33:44.962971: step 1387, loss 0.0827767, acc 0.96
2016-09-06T04:33:45.731818: step 1388, loss 0.0420969, acc 0.96
2016-09-06T04:33:46.514615: step 1389, loss 0.033732, acc 1
2016-09-06T04:33:47.314893: step 1390, loss 0.0741057, acc 0.96
2016-09-06T04:33:48.117911: step 1391, loss 0.0219621, acc 1
2016-09-06T04:33:48.927462: step 1392, loss 0.12446, acc 0.96
2016-09-06T04:33:49.752195: step 1393, loss 0.154776, acc 0.9
2016-09-06T04:33:50.563058: step 1394, loss 0.0633756, acc 0.96
2016-09-06T04:33:51.360597: step 1395, loss 0.239913, acc 0.92
2016-09-06T04:33:52.199456: step 1396, loss 0.111742, acc 0.94
2016-09-06T04:33:52.998018: step 1397, loss 0.092029, acc 0.96
2016-09-06T04:33:53.789737: step 1398, loss 0.0389694, acc 0.98
2016-09-06T04:33:54.618182: step 1399, loss 0.0426635, acc 0.98
2016-09-06T04:33:55.401037: step 1400, loss 0.0977279, acc 0.96

Evaluation:
2016-09-06T04:33:59.167718: step 1400, loss 0.971628, acc 0.76454

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-1400

2016-09-06T04:34:01.068687: step 1401, loss 0.0289552, acc 1
2016-09-06T04:34:01.889639: step 1402, loss 0.0563825, acc 0.98
2016-09-06T04:34:02.696712: step 1403, loss 0.0662027, acc 0.96
2016-09-06T04:34:03.541763: step 1404, loss 0.0474238, acc 0.98
2016-09-06T04:34:04.353582: step 1405, loss 0.0518706, acc 0.96
2016-09-06T04:34:05.155701: step 1406, loss 0.105442, acc 0.96
2016-09-06T04:34:05.999304: step 1407, loss 0.0760563, acc 0.98
2016-09-06T04:34:06.869033: step 1408, loss 0.0400092, acc 1
2016-09-06T04:34:07.655476: step 1409, loss 0.0243044, acc 1
2016-09-06T04:34:08.471250: step 1410, loss 0.0442164, acc 0.98
2016-09-06T04:34:09.278101: step 1411, loss 0.0453233, acc 0.98
2016-09-06T04:34:10.073059: step 1412, loss 0.0748187, acc 1
2016-09-06T04:34:10.886610: step 1413, loss 0.182713, acc 0.9
2016-09-06T04:34:11.690228: step 1414, loss 0.11123, acc 0.96
2016-09-06T04:34:12.469396: step 1415, loss 0.0984345, acc 0.94
2016-09-06T04:34:13.265428: step 1416, loss 0.0246797, acc 0.98
2016-09-06T04:34:14.087902: step 1417, loss 0.208568, acc 0.94
2016-09-06T04:34:14.865049: step 1418, loss 0.0296453, acc 1
2016-09-06T04:34:15.697674: step 1419, loss 0.105791, acc 0.96
2016-09-06T04:34:16.498200: step 1420, loss 0.0362647, acc 0.98
2016-09-06T04:34:17.283801: step 1421, loss 0.0330052, acc 1
2016-09-06T04:34:18.123818: step 1422, loss 0.056139, acc 0.98
2016-09-06T04:34:18.964510: step 1423, loss 0.0506028, acc 0.98
2016-09-06T04:34:19.790700: step 1424, loss 0.0980587, acc 0.96
2016-09-06T04:34:20.615623: step 1425, loss 0.0231043, acc 1
2016-09-06T04:34:21.469573: step 1426, loss 0.0905415, acc 0.96
2016-09-06T04:34:22.273801: step 1427, loss 0.158725, acc 0.94
2016-09-06T04:34:23.099330: step 1428, loss 0.112314, acc 0.98
2016-09-06T04:34:23.979321: step 1429, loss 0.064132, acc 0.96
2016-09-06T04:34:24.785923: step 1430, loss 0.0817987, acc 0.96
2016-09-06T04:34:25.594043: step 1431, loss 0.0424076, acc 0.98
2016-09-06T04:34:26.410403: step 1432, loss 0.0637392, acc 0.98
2016-09-06T04:34:27.217374: step 1433, loss 0.0568297, acc 0.96
2016-09-06T04:34:28.025149: step 1434, loss 0.0534186, acc 0.98
2016-09-06T04:34:28.852639: step 1435, loss 0.064628, acc 0.96
2016-09-06T04:34:29.708924: step 1436, loss 0.0430338, acc 0.98
2016-09-06T04:34:30.542744: step 1437, loss 0.0879473, acc 0.96
2016-09-06T04:34:31.343886: step 1438, loss 0.0551236, acc 0.96
2016-09-06T04:34:32.172532: step 1439, loss 0.0653317, acc 0.98
2016-09-06T04:34:32.960906: step 1440, loss 0.0895825, acc 0.98
2016-09-06T04:34:33.760859: step 1441, loss 0.0815563, acc 0.94
2016-09-06T04:34:34.573433: step 1442, loss 0.0163978, acc 1
2016-09-06T04:34:35.355151: step 1443, loss 0.0220811, acc 1
2016-09-06T04:34:36.168165: step 1444, loss 0.0849781, acc 0.98
2016-09-06T04:34:37.019513: step 1445, loss 0.089843, acc 0.96
2016-09-06T04:34:37.826614: step 1446, loss 0.0344686, acc 1
2016-09-06T04:34:38.635923: step 1447, loss 0.0135503, acc 1
2016-09-06T04:34:39.463860: step 1448, loss 0.0669025, acc 0.98
2016-09-06T04:34:40.235070: step 1449, loss 0.0587169, acc 0.98
2016-09-06T04:34:41.037159: step 1450, loss 0.0740159, acc 0.94
2016-09-06T04:34:41.877947: step 1451, loss 0.279047, acc 0.92
2016-09-06T04:34:42.651750: step 1452, loss 0.0455733, acc 0.98
2016-09-06T04:34:43.496106: step 1453, loss 0.0963582, acc 0.96
2016-09-06T04:34:44.298383: step 1454, loss 0.0827213, acc 0.96
2016-09-06T04:34:45.074079: step 1455, loss 0.0128739, acc 1
2016-09-06T04:34:45.890056: step 1456, loss 0.118447, acc 0.94
2016-09-06T04:34:46.693395: step 1457, loss 0.0415724, acc 1
2016-09-06T04:34:47.475013: step 1458, loss 0.146134, acc 0.94
2016-09-06T04:34:48.282190: step 1459, loss 0.0743382, acc 0.94
2016-09-06T04:34:49.122627: step 1460, loss 0.275499, acc 0.9
2016-09-06T04:34:49.935630: step 1461, loss 0.142903, acc 0.92
2016-09-06T04:34:50.744944: step 1462, loss 0.0138867, acc 1
2016-09-06T04:34:51.578565: step 1463, loss 0.077889, acc 0.98
2016-09-06T04:34:52.353944: step 1464, loss 0.1366, acc 0.98
2016-09-06T04:34:53.130319: step 1465, loss 0.0657384, acc 0.96
2016-09-06T04:34:53.925655: step 1466, loss 0.0425486, acc 1
2016-09-06T04:34:54.724845: step 1467, loss 0.0781653, acc 0.94
2016-09-06T04:34:55.524735: step 1468, loss 0.135662, acc 0.94
2016-09-06T04:34:56.350005: step 1469, loss 0.0976424, acc 0.94
2016-09-06T04:34:57.154138: step 1470, loss 0.148107, acc 0.94
2016-09-06T04:34:57.964698: step 1471, loss 0.0685098, acc 1
2016-09-06T04:34:58.799476: step 1472, loss 0.053376, acc 0.98
2016-09-06T04:34:59.567777: step 1473, loss 0.0920593, acc 0.94
2016-09-06T04:35:00.397107: step 1474, loss 0.0788007, acc 0.98
2016-09-06T04:35:01.211912: step 1475, loss 0.0777502, acc 0.94
2016-09-06T04:35:01.989389: step 1476, loss 0.0358083, acc 1
2016-09-06T04:35:02.813677: step 1477, loss 0.0992652, acc 0.94
2016-09-06T04:35:03.624780: step 1478, loss 0.09874, acc 0.94
2016-09-06T04:35:04.399434: step 1479, loss 0.0871636, acc 0.94
2016-09-06T04:35:05.236019: step 1480, loss 0.0733251, acc 0.96
2016-09-06T04:35:06.037906: step 1481, loss 0.0314402, acc 0.98
2016-09-06T04:35:06.807914: step 1482, loss 0.0495562, acc 0.96
2016-09-06T04:35:07.622175: step 1483, loss 0.164998, acc 0.94
2016-09-06T04:35:08.454637: step 1484, loss 0.0610529, acc 0.98
2016-09-06T04:35:09.254012: step 1485, loss 0.160613, acc 0.94
2016-09-06T04:35:10.077492: step 1486, loss 0.00790341, acc 1
2016-09-06T04:35:10.923707: step 1487, loss 0.0243568, acc 0.98
2016-09-06T04:35:11.737145: step 1488, loss 0.0603788, acc 0.98
2016-09-06T04:35:12.552484: step 1489, loss 0.0713308, acc 0.96
2016-09-06T04:35:13.382364: step 1490, loss 0.0341195, acc 1
2016-09-06T04:35:14.190810: step 1491, loss 0.131864, acc 0.94
2016-09-06T04:35:14.997447: step 1492, loss 0.0670783, acc 0.96
2016-09-06T04:35:15.828174: step 1493, loss 0.0362531, acc 0.98
2016-09-06T04:35:16.626592: step 1494, loss 0.040291, acc 1
2016-09-06T04:35:17.426543: step 1495, loss 0.023824, acc 1
2016-09-06T04:35:18.243265: step 1496, loss 0.0362106, acc 0.98
2016-09-06T04:35:19.075153: step 1497, loss 0.0600099, acc 0.98
2016-09-06T04:35:19.894719: step 1498, loss 0.0343109, acc 1
2016-09-06T04:35:20.719349: step 1499, loss 0.160051, acc 0.94
2016-09-06T04:35:21.528707: step 1500, loss 0.131599, acc 0.96

Evaluation:
2016-09-06T04:35:25.268185: step 1500, loss 1.15136, acc 0.761726

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-1500

2016-09-06T04:35:27.271768: step 1501, loss 0.0328189, acc 0.98
2016-09-06T04:35:28.123120: step 1502, loss 0.027269, acc 1
2016-09-06T04:35:28.969138: step 1503, loss 0.162176, acc 0.96
2016-09-06T04:35:29.800569: step 1504, loss 0.0470311, acc 0.98
2016-09-06T04:35:30.601064: step 1505, loss 0.0850806, acc 0.94
2016-09-06T04:35:31.408371: step 1506, loss 0.0345034, acc 0.98
2016-09-06T04:35:32.204623: step 1507, loss 0.0794177, acc 0.96
2016-09-06T04:35:33.013501: step 1508, loss 0.0231377, acc 1
2016-09-06T04:35:33.834657: step 1509, loss 0.0581175, acc 0.94
2016-09-06T04:35:34.636599: step 1510, loss 0.172753, acc 0.92
2016-09-06T04:35:35.460936: step 1511, loss 0.106468, acc 0.94
2016-09-06T04:35:36.262931: step 1512, loss 0.0527754, acc 0.98
2016-09-06T04:35:37.047419: step 1513, loss 0.0695153, acc 0.98
2016-09-06T04:35:37.839835: step 1514, loss 0.160765, acc 0.94
2016-09-06T04:35:38.666307: step 1515, loss 0.0587026, acc 0.98
2016-09-06T04:35:39.437335: step 1516, loss 0.0866583, acc 0.96
2016-09-06T04:35:40.234352: step 1517, loss 0.0878874, acc 0.98
2016-09-06T04:35:41.069759: step 1518, loss 0.0198219, acc 1
2016-09-06T04:35:41.862664: step 1519, loss 0.0722535, acc 0.98
2016-09-06T04:35:42.670777: step 1520, loss 0.103176, acc 0.92
2016-09-06T04:35:43.505990: step 1521, loss 0.0383904, acc 1
2016-09-06T04:35:44.313548: step 1522, loss 0.158609, acc 0.92
2016-09-06T04:35:45.105720: step 1523, loss 0.0308166, acc 1
2016-09-06T04:35:45.955276: step 1524, loss 0.0935199, acc 0.96
2016-09-06T04:35:46.739586: step 1525, loss 0.0320261, acc 0.98
2016-09-06T04:35:47.523288: step 1526, loss 0.121854, acc 0.92
2016-09-06T04:35:48.341152: step 1527, loss 0.0974807, acc 0.96
2016-09-06T04:35:49.122306: step 1528, loss 0.131947, acc 0.94
2016-09-06T04:35:49.917472: step 1529, loss 0.0594308, acc 0.98
2016-09-06T04:35:50.751388: step 1530, loss 0.0924339, acc 0.96
2016-09-06T04:35:51.531336: step 1531, loss 0.0178489, acc 1
2016-09-06T04:35:52.334114: step 1532, loss 0.0462571, acc 0.98
2016-09-06T04:35:53.199541: step 1533, loss 0.0835473, acc 0.96
2016-09-06T04:35:54.005183: step 1534, loss 0.098978, acc 0.96
2016-09-06T04:35:54.805694: step 1535, loss 0.0813718, acc 0.98
2016-09-06T04:35:55.564336: step 1536, loss 0.0396662, acc 1
2016-09-06T04:35:56.354617: step 1537, loss 0.0359051, acc 1
2016-09-06T04:35:57.152594: step 1538, loss 0.0491437, acc 0.98
2016-09-06T04:35:57.970988: step 1539, loss 0.0391568, acc 0.96
2016-09-06T04:35:58.764977: step 1540, loss 0.0522434, acc 0.98
2016-09-06T04:35:59.574721: step 1541, loss 0.0496008, acc 1
2016-09-06T04:36:00.412123: step 1542, loss 0.0395107, acc 1
2016-09-06T04:36:01.219109: step 1543, loss 0.0271607, acc 1
2016-09-06T04:36:02.009843: step 1544, loss 0.280338, acc 0.92
2016-09-06T04:36:02.840153: step 1545, loss 0.105452, acc 0.92
2016-09-06T04:36:03.614377: step 1546, loss 0.0987703, acc 0.96
2016-09-06T04:36:04.418153: step 1547, loss 0.0479157, acc 0.96
2016-09-06T04:36:05.229951: step 1548, loss 0.00748602, acc 1
2016-09-06T04:36:06.021443: step 1549, loss 0.102075, acc 0.96
2016-09-06T04:36:06.817102: step 1550, loss 0.0576958, acc 0.98
2016-09-06T04:36:07.652744: step 1551, loss 0.0854704, acc 0.94
2016-09-06T04:36:08.464153: step 1552, loss 0.0876006, acc 0.98
2016-09-06T04:36:09.285295: step 1553, loss 0.0416547, acc 0.98
2016-09-06T04:36:10.104649: step 1554, loss 0.0260598, acc 1
2016-09-06T04:36:10.891422: step 1555, loss 0.0888125, acc 0.94
2016-09-06T04:36:11.723782: step 1556, loss 0.4752, acc 0.98
2016-09-06T04:36:12.532876: step 1557, loss 0.0958697, acc 0.96
2016-09-06T04:36:13.327108: step 1558, loss 0.162085, acc 0.94
2016-09-06T04:36:14.134609: step 1559, loss 0.0748968, acc 0.98
2016-09-06T04:36:14.934910: step 1560, loss 0.0209297, acc 1
2016-09-06T04:36:15.730547: step 1561, loss 0.0911336, acc 0.98
2016-09-06T04:36:16.547086: step 1562, loss 0.0923897, acc 0.94
2016-09-06T04:36:17.345209: step 1563, loss 0.0889491, acc 0.96
2016-09-06T04:36:18.157673: step 1564, loss 0.0453046, acc 0.98
2016-09-06T04:36:18.961414: step 1565, loss 0.0382153, acc 0.98
2016-09-06T04:36:19.789234: step 1566, loss 0.0200829, acc 1
2016-09-06T04:36:20.551238: step 1567, loss 0.0732263, acc 1
2016-09-06T04:36:21.363930: step 1568, loss 0.0242928, acc 1
2016-09-06T04:36:22.186182: step 1569, loss 0.13882, acc 0.92
2016-09-06T04:36:22.992372: step 1570, loss 0.0988406, acc 0.94
2016-09-06T04:36:23.785806: step 1571, loss 0.0780823, acc 0.96
2016-09-06T04:36:24.601896: step 1572, loss 0.133308, acc 0.94
2016-09-06T04:36:25.386595: step 1573, loss 0.0135282, acc 1
2016-09-06T04:36:26.180921: step 1574, loss 0.14546, acc 0.98
2016-09-06T04:36:26.981097: step 1575, loss 0.0624136, acc 0.98
2016-09-06T04:36:27.763919: step 1576, loss 0.101076, acc 0.96
2016-09-06T04:36:28.569310: step 1577, loss 0.128469, acc 0.94
2016-09-06T04:36:29.365792: step 1578, loss 0.0807117, acc 0.96
2016-09-06T04:36:30.181451: step 1579, loss 0.142083, acc 0.94
2016-09-06T04:36:30.980695: step 1580, loss 0.0269422, acc 1
2016-09-06T04:36:31.817064: step 1581, loss 0.049021, acc 0.98
2016-09-06T04:36:32.630842: step 1582, loss 0.133908, acc 0.9
2016-09-06T04:36:33.438320: step 1583, loss 0.060528, acc 0.96
2016-09-06T04:36:34.226702: step 1584, loss 0.066976, acc 0.98
2016-09-06T04:36:35.040655: step 1585, loss 0.114053, acc 0.94
2016-09-06T04:36:35.862772: step 1586, loss 0.0925651, acc 0.96
2016-09-06T04:36:36.681459: step 1587, loss 0.0305523, acc 1
2016-09-06T04:36:37.480103: step 1588, loss 0.0614258, acc 0.98
2016-09-06T04:36:38.274699: step 1589, loss 0.0320476, acc 0.98
2016-09-06T04:36:39.090503: step 1590, loss 0.0392089, acc 1
2016-09-06T04:36:39.880547: step 1591, loss 0.0602887, acc 0.98
2016-09-06T04:36:40.684146: step 1592, loss 0.075524, acc 0.96
2016-09-06T04:36:41.499715: step 1593, loss 0.0375393, acc 0.98
2016-09-06T04:36:42.296794: step 1594, loss 0.162187, acc 0.9
2016-09-06T04:36:43.122141: step 1595, loss 0.0471372, acc 0.98
2016-09-06T04:36:43.941347: step 1596, loss 0.0548876, acc 0.98
2016-09-06T04:36:44.727451: step 1597, loss 0.0943657, acc 0.98
2016-09-06T04:36:45.562288: step 1598, loss 0.127952, acc 0.94
2016-09-06T04:36:46.398153: step 1599, loss 0.0258313, acc 0.98
2016-09-06T04:36:47.209718: step 1600, loss 0.0479246, acc 0.96

Evaluation:
2016-09-06T04:36:50.940602: step 1600, loss 1.16426, acc 0.763602

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-1600

2016-09-06T04:36:52.842513: step 1601, loss 0.118221, acc 0.96
2016-09-06T04:36:53.671412: step 1602, loss 0.15747, acc 0.92
2016-09-06T04:36:54.444459: step 1603, loss 0.0352679, acc 0.98
2016-09-06T04:36:55.236087: step 1604, loss 0.0768941, acc 0.96
2016-09-06T04:36:56.065693: step 1605, loss 0.0530076, acc 0.98
2016-09-06T04:36:56.843968: step 1606, loss 0.0570795, acc 0.98
2016-09-06T04:36:57.718199: step 1607, loss 0.0469613, acc 0.96
2016-09-06T04:36:58.521933: step 1608, loss 0.0719615, acc 0.96
2016-09-06T04:36:59.322462: step 1609, loss 0.0548804, acc 0.98
2016-09-06T04:37:00.123100: step 1610, loss 0.0677929, acc 0.98
2016-09-06T04:37:01.040850: step 1611, loss 0.0729399, acc 0.96
2016-09-06T04:37:01.854100: step 1612, loss 0.0910559, acc 0.98
2016-09-06T04:37:02.670617: step 1613, loss 0.178131, acc 0.94
2016-09-06T04:37:03.496866: step 1614, loss 0.0195162, acc 1
2016-09-06T04:37:04.300041: step 1615, loss 0.0814301, acc 0.96
2016-09-06T04:37:05.102183: step 1616, loss 0.0601921, acc 1
2016-09-06T04:37:05.969305: step 1617, loss 0.0631453, acc 0.98
2016-09-06T04:37:06.818943: step 1618, loss 0.0411078, acc 1
2016-09-06T04:37:07.628770: step 1619, loss 0.0960205, acc 0.96
2016-09-06T04:37:08.469345: step 1620, loss 0.0720428, acc 0.98
2016-09-06T04:37:09.254835: step 1621, loss 0.0550977, acc 0.98
2016-09-06T04:37:10.045645: step 1622, loss 0.0227955, acc 1
2016-09-06T04:37:10.906070: step 1623, loss 0.0678996, acc 0.96
2016-09-06T04:37:11.725670: step 1624, loss 0.0757561, acc 0.96
2016-09-06T04:37:12.509431: step 1625, loss 0.130578, acc 0.92
2016-09-06T04:37:13.321368: step 1626, loss 0.0276457, acc 1
2016-09-06T04:37:14.125456: step 1627, loss 0.0461731, acc 0.96
2016-09-06T04:37:14.929420: step 1628, loss 0.0367381, acc 1
2016-09-06T04:37:15.743458: step 1629, loss 0.141836, acc 0.96
2016-09-06T04:37:16.597534: step 1630, loss 0.128121, acc 0.96
2016-09-06T04:37:17.435685: step 1631, loss 0.0711627, acc 0.98
2016-09-06T04:37:18.239967: step 1632, loss 0.00712761, acc 1
2016-09-06T04:37:19.055062: step 1633, loss 0.0522409, acc 0.98
2016-09-06T04:37:19.851107: step 1634, loss 0.0757061, acc 0.94
2016-09-06T04:37:20.616647: step 1635, loss 0.0193878, acc 1
2016-09-06T04:37:21.461970: step 1636, loss 0.0721585, acc 0.98
2016-09-06T04:37:22.245691: step 1637, loss 0.0894603, acc 0.94
2016-09-06T04:37:23.026466: step 1638, loss 0.0262349, acc 1
2016-09-06T04:37:23.860455: step 1639, loss 0.0222954, acc 1
2016-09-06T04:37:24.631128: step 1640, loss 0.0284832, acc 1
2016-09-06T04:37:25.434396: step 1641, loss 0.0539147, acc 0.96
2016-09-06T04:37:26.265389: step 1642, loss 0.0666357, acc 0.98
2016-09-06T04:37:27.045063: step 1643, loss 0.0643663, acc 0.96
2016-09-06T04:37:27.835502: step 1644, loss 0.0640395, acc 0.96
2016-09-06T04:37:28.650140: step 1645, loss 0.0504259, acc 1
2016-09-06T04:37:29.451873: step 1646, loss 0.223142, acc 0.96
2016-09-06T04:37:30.262698: step 1647, loss 0.0451976, acc 0.98
2016-09-06T04:37:31.087635: step 1648, loss 0.129319, acc 0.9
2016-09-06T04:37:31.884864: step 1649, loss 0.0414035, acc 0.98
2016-09-06T04:37:32.691550: step 1650, loss 0.0330889, acc 1
2016-09-06T04:37:33.522593: step 1651, loss 0.0637773, acc 0.98
2016-09-06T04:37:34.292137: step 1652, loss 0.0410668, acc 0.98
2016-09-06T04:37:35.098529: step 1653, loss 0.104645, acc 0.96
2016-09-06T04:37:35.947654: step 1654, loss 0.0367579, acc 1
2016-09-06T04:37:36.743715: step 1655, loss 0.0813255, acc 0.96
2016-09-06T04:37:37.536729: step 1656, loss 0.104666, acc 0.98
2016-09-06T04:37:38.345298: step 1657, loss 0.0714271, acc 0.96
2016-09-06T04:37:39.143359: step 1658, loss 0.0571694, acc 0.98
2016-09-06T04:37:39.948910: step 1659, loss 0.113795, acc 0.94
2016-09-06T04:37:40.790973: step 1660, loss 0.0494165, acc 0.98
2016-09-06T04:37:41.606869: step 1661, loss 0.0622175, acc 0.96
2016-09-06T04:37:42.420395: step 1662, loss 0.0567436, acc 0.98
2016-09-06T04:37:43.233961: step 1663, loss 0.0382477, acc 0.98
2016-09-06T04:37:44.026388: step 1664, loss 0.0316933, acc 1
2016-09-06T04:37:44.837204: step 1665, loss 0.0917955, acc 0.98
2016-09-06T04:37:45.715517: step 1666, loss 0.0811694, acc 0.96
2016-09-06T04:37:46.499240: step 1667, loss 0.0661047, acc 0.96
2016-09-06T04:37:47.324492: step 1668, loss 0.0551586, acc 0.98
2016-09-06T04:37:48.136165: step 1669, loss 0.0920809, acc 0.92
2016-09-06T04:37:48.947167: step 1670, loss 0.0305766, acc 1
2016-09-06T04:37:49.754383: step 1671, loss 0.0911571, acc 0.96
2016-09-06T04:37:50.563897: step 1672, loss 0.0379177, acc 1
2016-09-06T04:37:51.361984: step 1673, loss 0.0316265, acc 1
2016-09-06T04:37:52.157011: step 1674, loss 0.0667844, acc 0.96
2016-09-06T04:37:53.004397: step 1675, loss 0.0298761, acc 1
2016-09-06T04:37:53.826567: step 1676, loss 0.0233488, acc 0.98
2016-09-06T04:37:54.649307: step 1677, loss 0.0742374, acc 0.96
2016-09-06T04:37:55.473437: step 1678, loss 0.0583026, acc 0.96
2016-09-06T04:37:56.269840: step 1679, loss 0.0372866, acc 0.98
2016-09-06T04:37:57.055518: step 1680, loss 0.243778, acc 0.92
2016-09-06T04:37:57.910875: step 1681, loss 0.119269, acc 0.96
2016-09-06T04:37:58.707939: step 1682, loss 0.13272, acc 0.9
2016-09-06T04:37:59.496757: step 1683, loss 0.0249501, acc 1
2016-09-06T04:38:00.328725: step 1684, loss 0.124172, acc 0.96
2016-09-06T04:38:01.127600: step 1685, loss 0.0784497, acc 0.96
2016-09-06T04:38:01.924366: step 1686, loss 0.0872729, acc 0.96
2016-09-06T04:38:02.733032: step 1687, loss 0.0391355, acc 1
2016-09-06T04:38:03.551736: step 1688, loss 0.02408, acc 1
2016-09-06T04:38:04.366188: step 1689, loss 0.0577225, acc 0.98
2016-09-06T04:38:05.208636: step 1690, loss 0.00849768, acc 1
2016-09-06T04:38:06.038888: step 1691, loss 0.0912379, acc 0.98
2016-09-06T04:38:06.872094: step 1692, loss 0.0666276, acc 0.96
2016-09-06T04:38:07.699293: step 1693, loss 0.101256, acc 0.94
2016-09-06T04:38:08.540036: step 1694, loss 0.0378433, acc 0.98
2016-09-06T04:38:09.333590: step 1695, loss 0.0929397, acc 0.96
2016-09-06T04:38:10.131232: step 1696, loss 0.0779344, acc 0.96
2016-09-06T04:38:10.935965: step 1697, loss 0.0848705, acc 0.98
2016-09-06T04:38:11.730900: step 1698, loss 0.0968415, acc 0.96
2016-09-06T04:38:12.538988: step 1699, loss 0.108605, acc 0.94
2016-09-06T04:38:13.360452: step 1700, loss 0.0972682, acc 0.98

Evaluation:
2016-09-06T04:38:17.101080: step 1700, loss 1.08033, acc 0.768293

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-1700

2016-09-06T04:38:18.924336: step 1701, loss 0.0330585, acc 1
2016-09-06T04:38:19.757661: step 1702, loss 0.0780495, acc 0.96
2016-09-06T04:38:20.593264: step 1703, loss 0.0716615, acc 0.94
2016-09-06T04:38:21.406960: step 1704, loss 0.0376664, acc 1
2016-09-06T04:38:22.214926: step 1705, loss 0.102929, acc 0.92
2016-09-06T04:38:23.040732: step 1706, loss 0.124908, acc 0.92
2016-09-06T04:38:23.878698: step 1707, loss 0.0656221, acc 0.98
2016-09-06T04:38:24.691863: step 1708, loss 0.116455, acc 0.94
2016-09-06T04:38:25.516393: step 1709, loss 0.050571, acc 1
2016-09-06T04:38:26.297534: step 1710, loss 0.0485979, acc 0.98
2016-09-06T04:38:27.100819: step 1711, loss 0.0499219, acc 0.98
2016-09-06T04:38:27.915393: step 1712, loss 0.0562183, acc 0.96
2016-09-06T04:38:28.716204: step 1713, loss 0.0967004, acc 0.94
2016-09-06T04:38:29.514278: step 1714, loss 0.0303379, acc 0.98
2016-09-06T04:38:30.328578: step 1715, loss 0.0357282, acc 1
2016-09-06T04:38:31.124108: step 1716, loss 0.217729, acc 0.92
2016-09-06T04:38:31.919193: step 1717, loss 0.0427105, acc 0.98
2016-09-06T04:38:32.780639: step 1718, loss 0.0154294, acc 1
2016-09-06T04:38:33.550712: step 1719, loss 0.109901, acc 0.96
2016-09-06T04:38:34.345765: step 1720, loss 0.0812496, acc 0.96
2016-09-06T04:38:35.159023: step 1721, loss 0.0312727, acc 0.98
2016-09-06T04:38:35.969234: step 1722, loss 0.0795118, acc 0.94
2016-09-06T04:38:36.778736: step 1723, loss 0.0476712, acc 0.96
2016-09-06T04:38:37.603110: step 1724, loss 0.0485677, acc 0.96
2016-09-06T04:38:38.379367: step 1725, loss 0.0643617, acc 0.98
2016-09-06T04:38:39.166243: step 1726, loss 0.0851734, acc 0.98
2016-09-06T04:38:39.978919: step 1727, loss 0.12339, acc 0.92
2016-09-06T04:38:40.728030: step 1728, loss 0.0297627, acc 1
2016-09-06T04:38:41.527295: step 1729, loss 0.0588217, acc 0.96
2016-09-06T04:38:42.315311: step 1730, loss 0.0338149, acc 1
2016-09-06T04:38:43.108780: step 1731, loss 0.030905, acc 1
2016-09-06T04:38:43.943307: step 1732, loss 0.0739634, acc 0.96
2016-09-06T04:38:44.754650: step 1733, loss 0.123423, acc 0.94
2016-09-06T04:38:45.577522: step 1734, loss 0.151229, acc 0.92
2016-09-06T04:38:46.434067: step 1735, loss 0.104582, acc 0.96
2016-09-06T04:38:47.252149: step 1736, loss 0.0873165, acc 0.96
2016-09-06T04:38:48.029546: step 1737, loss 0.0247873, acc 1
2016-09-06T04:38:48.828779: step 1738, loss 0.0617086, acc 1
2016-09-06T04:38:49.642502: step 1739, loss 0.110431, acc 0.94
2016-09-06T04:38:50.415649: step 1740, loss 0.0470059, acc 0.98
2016-09-06T04:38:51.253258: step 1741, loss 0.0400215, acc 1
2016-09-06T04:38:52.051575: step 1742, loss 0.0749209, acc 0.98
2016-09-06T04:38:52.846282: step 1743, loss 0.0435908, acc 0.98
2016-09-06T04:38:53.681865: step 1744, loss 0.245852, acc 0.94
2016-09-06T04:38:54.492696: step 1745, loss 0.124985, acc 0.96
2016-09-06T04:38:55.291995: step 1746, loss 0.042444, acc 1
2016-09-06T04:38:56.125668: step 1747, loss 0.050852, acc 0.98
2016-09-06T04:38:56.943831: step 1748, loss 0.00908881, acc 1
2016-09-06T04:38:57.715693: step 1749, loss 0.0228794, acc 1
2016-09-06T04:38:58.510424: step 1750, loss 0.0466983, acc 1
2016-09-06T04:38:59.337964: step 1751, loss 0.141705, acc 0.96
2016-09-06T04:39:00.110284: step 1752, loss 0.0712062, acc 0.96
2016-09-06T04:39:00.922368: step 1753, loss 0.131858, acc 0.9
2016-09-06T04:39:01.732906: step 1754, loss 0.0487319, acc 0.98
2016-09-06T04:39:02.522098: step 1755, loss 0.0850103, acc 0.96
2016-09-06T04:39:03.295465: step 1756, loss 0.0268248, acc 0.98
2016-09-06T04:39:04.077151: step 1757, loss 0.0951741, acc 0.96
2016-09-06T04:39:04.876722: step 1758, loss 0.0545518, acc 0.98
2016-09-06T04:39:05.714911: step 1759, loss 0.0422644, acc 0.98
2016-09-06T04:39:06.515211: step 1760, loss 0.0464741, acc 0.98
2016-09-06T04:39:07.330742: step 1761, loss 0.154144, acc 0.94
2016-09-06T04:39:08.164174: step 1762, loss 0.0689133, acc 0.98
2016-09-06T04:39:08.965413: step 1763, loss 0.0326579, acc 1
2016-09-06T04:39:09.780576: step 1764, loss 0.0377456, acc 0.98
2016-09-06T04:39:10.593394: step 1765, loss 0.0256117, acc 1
2016-09-06T04:39:11.406073: step 1766, loss 0.0196823, acc 1
2016-09-06T04:39:12.208404: step 1767, loss 0.0775453, acc 0.96
2016-09-06T04:39:13.058761: step 1768, loss 0.0348845, acc 0.98
2016-09-06T04:39:13.874526: step 1769, loss 0.0499234, acc 0.96
2016-09-06T04:39:14.673147: step 1770, loss 0.0484159, acc 0.98
2016-09-06T04:39:15.459303: step 1771, loss 0.124828, acc 0.98
2016-09-06T04:39:16.268169: step 1772, loss 0.0149595, acc 1
2016-09-06T04:39:17.056895: step 1773, loss 0.00447135, acc 1
2016-09-06T04:39:17.890695: step 1774, loss 0.0645924, acc 0.96
2016-09-06T04:39:18.712000: step 1775, loss 0.00954779, acc 1
2016-09-06T04:39:19.503473: step 1776, loss 0.0311916, acc 1
2016-09-06T04:39:20.295361: step 1777, loss 0.0231497, acc 1
2016-09-06T04:39:21.115425: step 1778, loss 0.00788344, acc 1
2016-09-06T04:39:21.894010: step 1779, loss 0.130034, acc 0.96
2016-09-06T04:39:22.687783: step 1780, loss 0.0884452, acc 0.98
2016-09-06T04:39:23.497404: step 1781, loss 0.0261289, acc 1
2016-09-06T04:39:24.298938: step 1782, loss 0.0250637, acc 1
2016-09-06T04:39:25.112383: step 1783, loss 0.0212658, acc 1
2016-09-06T04:39:25.939467: step 1784, loss 0.0284397, acc 0.98
2016-09-06T04:39:26.745923: step 1785, loss 0.0450337, acc 0.98
2016-09-06T04:39:27.541405: step 1786, loss 0.103561, acc 0.94
2016-09-06T04:39:28.387965: step 1787, loss 0.111676, acc 0.92
2016-09-06T04:39:29.173189: step 1788, loss 0.0393713, acc 0.96
2016-09-06T04:39:29.963301: step 1789, loss 0.0395981, acc 0.98
2016-09-06T04:39:30.772202: step 1790, loss 0.0907722, acc 0.98
2016-09-06T04:39:31.569688: step 1791, loss 0.0112218, acc 1
2016-09-06T04:39:32.368246: step 1792, loss 0.111487, acc 0.96
2016-09-06T04:39:33.192971: step 1793, loss 0.0747734, acc 0.98
2016-09-06T04:39:33.985296: step 1794, loss 0.0681744, acc 0.96
2016-09-06T04:39:34.797502: step 1795, loss 0.0794694, acc 0.94
2016-09-06T04:39:35.620144: step 1796, loss 0.071604, acc 0.96
2016-09-06T04:39:36.433699: step 1797, loss 0.0141977, acc 1
2016-09-06T04:39:37.252069: step 1798, loss 0.244337, acc 0.94
2016-09-06T04:39:38.068566: step 1799, loss 0.0870023, acc 0.96
2016-09-06T04:39:38.868863: step 1800, loss 0.0370937, acc 1

Evaluation:
2016-09-06T04:39:42.632380: step 1800, loss 1.25055, acc 0.753283

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-1800

2016-09-06T04:39:44.552257: step 1801, loss 0.0886307, acc 0.94
2016-09-06T04:39:45.391807: step 1802, loss 0.0371988, acc 1
2016-09-06T04:39:46.195162: step 1803, loss 0.0737891, acc 0.96
2016-09-06T04:39:47.006221: step 1804, loss 0.100496, acc 0.96
2016-09-06T04:39:47.816160: step 1805, loss 0.0746084, acc 0.96
2016-09-06T04:39:48.638855: step 1806, loss 0.0845821, acc 0.96
2016-09-06T04:39:49.452766: step 1807, loss 0.0451409, acc 0.98
2016-09-06T04:39:50.280592: step 1808, loss 0.029362, acc 1
2016-09-06T04:39:51.103082: step 1809, loss 0.0151278, acc 1
2016-09-06T04:39:51.915660: step 1810, loss 0.0257636, acc 1
2016-09-06T04:39:52.757269: step 1811, loss 0.0811694, acc 0.94
2016-09-06T04:39:53.522998: step 1812, loss 0.0222027, acc 1
2016-09-06T04:39:54.378647: step 1813, loss 0.0774537, acc 0.96
2016-09-06T04:39:55.180962: step 1814, loss 0.069596, acc 0.96
2016-09-06T04:39:56.006515: step 1815, loss 0.0416973, acc 0.98
2016-09-06T04:39:56.802982: step 1816, loss 0.0612927, acc 0.98
2016-09-06T04:39:57.633079: step 1817, loss 0.0915328, acc 0.94
2016-09-06T04:39:58.445117: step 1818, loss 0.0117906, acc 1
2016-09-06T04:39:59.240169: step 1819, loss 0.0480835, acc 1
2016-09-06T04:40:00.127147: step 1820, loss 0.0214384, acc 1
2016-09-06T04:40:00.983071: step 1821, loss 0.0548768, acc 1
2016-09-06T04:40:01.810289: step 1822, loss 0.0758365, acc 0.98
2016-09-06T04:40:02.620558: step 1823, loss 0.0300534, acc 1
2016-09-06T04:40:03.454639: step 1824, loss 0.11328, acc 0.96
2016-09-06T04:40:04.256281: step 1825, loss 0.00716115, acc 1
2016-09-06T04:40:05.055973: step 1826, loss 0.0561464, acc 1
2016-09-06T04:40:05.884087: step 1827, loss 0.0140522, acc 1
2016-09-06T04:40:06.675080: step 1828, loss 0.0329182, acc 0.98
2016-09-06T04:40:07.508386: step 1829, loss 0.140407, acc 0.9
2016-09-06T04:40:08.353458: step 1830, loss 0.0245113, acc 0.98
2016-09-06T04:40:09.197509: step 1831, loss 0.0771189, acc 0.96
2016-09-06T04:40:10.011652: step 1832, loss 0.0621633, acc 0.96
2016-09-06T04:40:10.853351: step 1833, loss 0.0350167, acc 1
2016-09-06T04:40:11.689033: step 1834, loss 0.159049, acc 0.94
2016-09-06T04:40:12.485365: step 1835, loss 0.0552159, acc 0.96
2016-09-06T04:40:13.291044: step 1836, loss 0.064675, acc 0.96
2016-09-06T04:40:14.112025: step 1837, loss 0.0922676, acc 0.94
2016-09-06T04:40:14.923842: step 1838, loss 0.0705344, acc 0.98
2016-09-06T04:40:15.763277: step 1839, loss 0.0176092, acc 1
2016-09-06T04:40:16.567504: step 1840, loss 0.0555225, acc 0.96
2016-09-06T04:40:17.384707: step 1841, loss 0.0663141, acc 0.96
2016-09-06T04:40:18.205503: step 1842, loss 0.024524, acc 1
2016-09-06T04:40:19.014658: step 1843, loss 0.0424945, acc 0.98
2016-09-06T04:40:19.849294: step 1844, loss 0.0474256, acc 0.98
2016-09-06T04:40:20.673033: step 1845, loss 0.0854319, acc 0.96
2016-09-06T04:40:21.508512: step 1846, loss 0.0216696, acc 1
2016-09-06T04:40:22.320705: step 1847, loss 0.0363675, acc 0.98
2016-09-06T04:40:23.119549: step 1848, loss 0.0832025, acc 0.94
2016-09-06T04:40:23.961446: step 1849, loss 0.177846, acc 0.92
2016-09-06T04:40:24.784302: step 1850, loss 0.0868228, acc 0.98
2016-09-06T04:40:25.584381: step 1851, loss 0.0739387, acc 0.98
2016-09-06T04:40:26.401745: step 1852, loss 0.0157897, acc 1
2016-09-06T04:40:27.195135: step 1853, loss 0.115985, acc 0.96
2016-09-06T04:40:27.976842: step 1854, loss 0.0253064, acc 1
2016-09-06T04:40:28.777945: step 1855, loss 0.0514004, acc 0.98
2016-09-06T04:40:29.571996: step 1856, loss 0.116493, acc 0.94
2016-09-06T04:40:30.383976: step 1857, loss 0.00874223, acc 1
2016-09-06T04:40:31.224343: step 1858, loss 0.0377666, acc 1
2016-09-06T04:40:31.991491: step 1859, loss 0.0609214, acc 0.96
2016-09-06T04:40:32.792448: step 1860, loss 0.1639, acc 0.98
2016-09-06T04:40:33.621668: step 1861, loss 0.0348383, acc 0.98
2016-09-06T04:40:34.439841: step 1862, loss 0.0334, acc 0.98
2016-09-06T04:40:35.252781: step 1863, loss 0.0543133, acc 0.98
2016-09-06T04:40:36.061771: step 1864, loss 0.122391, acc 0.96
2016-09-06T04:40:36.852718: step 1865, loss 0.0463692, acc 0.98
2016-09-06T04:40:37.642904: step 1866, loss 0.0648937, acc 0.98
2016-09-06T04:40:38.464727: step 1867, loss 0.0764862, acc 0.98
2016-09-06T04:40:39.238199: step 1868, loss 0.128046, acc 0.94
2016-09-06T04:40:40.041848: step 1869, loss 0.0638883, acc 0.96
2016-09-06T04:40:40.880686: step 1870, loss 0.0214665, acc 1
2016-09-06T04:40:41.707087: step 1871, loss 0.02508, acc 1
2016-09-06T04:40:42.515235: step 1872, loss 0.034294, acc 1
2016-09-06T04:40:43.335488: step 1873, loss 0.0273124, acc 0.98
2016-09-06T04:40:44.124901: step 1874, loss 0.0344595, acc 1
2016-09-06T04:40:44.929581: step 1875, loss 0.0828212, acc 0.96
2016-09-06T04:40:45.745692: step 1876, loss 0.0411646, acc 0.98
2016-09-06T04:40:46.533028: step 1877, loss 0.0638169, acc 0.98
2016-09-06T04:40:47.370684: step 1878, loss 0.080123, acc 0.96
2016-09-06T04:40:48.209116: step 1879, loss 0.0461191, acc 0.98
2016-09-06T04:40:49.029774: step 1880, loss 0.00506136, acc 1
2016-09-06T04:40:49.853586: step 1881, loss 0.0293476, acc 1
2016-09-06T04:40:50.687879: step 1882, loss 0.0466609, acc 0.98
2016-09-06T04:40:51.493521: step 1883, loss 0.0856916, acc 0.98
2016-09-06T04:40:52.302260: step 1884, loss 0.0554169, acc 0.96
2016-09-06T04:40:53.117606: step 1885, loss 0.113928, acc 0.94
2016-09-06T04:40:53.937055: step 1886, loss 0.0138132, acc 1
2016-09-06T04:40:54.736594: step 1887, loss 0.0820547, acc 0.98
2016-09-06T04:40:55.557205: step 1888, loss 0.144143, acc 0.92
2016-09-06T04:40:56.383194: step 1889, loss 0.0992199, acc 0.96
2016-09-06T04:40:57.206177: step 1890, loss 0.0487727, acc 0.98
2016-09-06T04:40:58.038997: step 1891, loss 0.202378, acc 0.92
2016-09-06T04:40:58.839680: step 1892, loss 0.0673303, acc 0.98
2016-09-06T04:40:59.657733: step 1893, loss 0.0735138, acc 0.98
2016-09-06T04:41:00.498494: step 1894, loss 0.0507374, acc 0.98
2016-09-06T04:41:01.343000: step 1895, loss 0.040323, acc 1
2016-09-06T04:41:02.175953: step 1896, loss 0.0628019, acc 0.94
2016-09-06T04:41:02.981054: step 1897, loss 0.0638796, acc 1
2016-09-06T04:41:03.796288: step 1898, loss 0.0934273, acc 0.98
2016-09-06T04:41:04.584037: step 1899, loss 0.0855918, acc 0.96
2016-09-06T04:41:05.394995: step 1900, loss 0.0465627, acc 0.98

Evaluation:
2016-09-06T04:41:09.095693: step 1900, loss 1.2206, acc 0.756098

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-1900

2016-09-06T04:41:11.178960: step 1901, loss 0.0603666, acc 0.98
2016-09-06T04:41:12.007054: step 1902, loss 0.0489812, acc 0.98
2016-09-06T04:41:12.847988: step 1903, loss 0.110734, acc 0.94
2016-09-06T04:41:13.667463: step 1904, loss 0.0745535, acc 0.94
2016-09-06T04:41:14.493847: step 1905, loss 0.0260229, acc 1
2016-09-06T04:41:15.308883: step 1906, loss 0.117161, acc 0.98
2016-09-06T04:41:16.132992: step 1907, loss 0.0420201, acc 0.98
2016-09-06T04:41:16.992312: step 1908, loss 0.0367182, acc 0.98
2016-09-06T04:41:17.815078: step 1909, loss 0.0585783, acc 0.96
2016-09-06T04:41:18.651368: step 1910, loss 0.105636, acc 0.96
2016-09-06T04:41:19.454272: step 1911, loss 0.0461717, acc 0.98
2016-09-06T04:41:20.261381: step 1912, loss 0.112315, acc 0.96
2016-09-06T04:41:21.092572: step 1913, loss 0.031781, acc 1
2016-09-06T04:41:21.901405: step 1914, loss 0.0455471, acc 0.98
2016-09-06T04:41:22.712505: step 1915, loss 0.176476, acc 0.96
2016-09-06T04:41:23.535788: step 1916, loss 0.044203, acc 0.96
2016-09-06T04:41:24.393928: step 1917, loss 0.0724238, acc 0.98
2016-09-06T04:41:25.186849: step 1918, loss 0.0920727, acc 0.96
2016-09-06T04:41:26.001474: step 1919, loss 0.0928439, acc 0.94
2016-09-06T04:41:26.770836: step 1920, loss 0.0573698, acc 0.977273
2016-09-06T04:41:27.581680: step 1921, loss 0.044776, acc 1
2016-09-06T04:41:28.398854: step 1922, loss 0.0352825, acc 1
2016-09-06T04:41:29.225143: step 1923, loss 0.074176, acc 0.98
2016-09-06T04:41:30.024325: step 1924, loss 0.233231, acc 0.94
2016-09-06T04:41:30.844036: step 1925, loss 0.0594609, acc 0.98
2016-09-06T04:41:31.667568: step 1926, loss 0.0495195, acc 0.96
2016-09-06T04:41:32.454445: step 1927, loss 0.0613099, acc 0.98
2016-09-06T04:41:33.253996: step 1928, loss 0.0354271, acc 1
2016-09-06T04:41:34.082577: step 1929, loss 0.0700393, acc 0.98
2016-09-06T04:41:34.862661: step 1930, loss 0.0502569, acc 0.96
2016-09-06T04:41:35.687759: step 1931, loss 0.0511231, acc 0.96
2016-09-06T04:41:36.489434: step 1932, loss 0.182154, acc 0.96
2016-09-06T04:41:37.261534: step 1933, loss 0.0130128, acc 1
2016-09-06T04:41:38.044708: step 1934, loss 0.0188776, acc 1
2016-09-06T04:41:38.864057: step 1935, loss 0.0247442, acc 1
2016-09-06T04:41:39.644406: step 1936, loss 0.0547012, acc 0.96
2016-09-06T04:41:40.460642: step 1937, loss 0.0489406, acc 0.98
2016-09-06T04:41:41.279981: step 1938, loss 0.043489, acc 0.98
2016-09-06T04:41:42.069575: step 1939, loss 0.0375259, acc 0.98
2016-09-06T04:41:42.946874: step 1940, loss 0.0166699, acc 1
2016-09-06T04:41:43.760450: step 1941, loss 0.021346, acc 1
2016-09-06T04:41:44.556603: step 1942, loss 0.0401943, acc 0.98
2016-09-06T04:41:45.350671: step 1943, loss 0.0243864, acc 1
2016-09-06T04:41:46.178930: step 1944, loss 0.0264534, acc 1
2016-09-06T04:41:46.945788: step 1945, loss 0.0570178, acc 0.96
2016-09-06T04:41:47.728577: step 1946, loss 0.0554067, acc 0.98
2016-09-06T04:41:48.569400: step 1947, loss 0.106267, acc 0.94
2016-09-06T04:41:49.362008: step 1948, loss 0.0467914, acc 0.98
2016-09-06T04:41:50.162915: step 1949, loss 0.120706, acc 0.9
2016-09-06T04:41:50.987542: step 1950, loss 0.0363206, acc 0.98
2016-09-06T04:41:51.785415: step 1951, loss 0.127623, acc 0.96
2016-09-06T04:41:52.586484: step 1952, loss 0.0429658, acc 0.98
2016-09-06T04:41:53.419186: step 1953, loss 0.190775, acc 0.9
2016-09-06T04:41:54.219347: step 1954, loss 0.0165436, acc 1
2016-09-06T04:41:55.013458: step 1955, loss 0.0313178, acc 1
2016-09-06T04:41:55.847362: step 1956, loss 0.0533911, acc 1
2016-09-06T04:41:56.656605: step 1957, loss 0.057693, acc 0.96
2016-09-06T04:41:57.466949: step 1958, loss 0.0862556, acc 0.96
2016-09-06T04:41:58.292416: step 1959, loss 0.0339407, acc 1
2016-09-06T04:41:59.093500: step 1960, loss 0.107878, acc 0.94
2016-09-06T04:41:59.880729: step 1961, loss 0.0209165, acc 0.98
2016-09-06T04:42:00.761727: step 1962, loss 0.0345386, acc 1
2016-09-06T04:42:01.556290: step 1963, loss 0.0548678, acc 1
2016-09-06T04:42:02.368304: step 1964, loss 0.0144236, acc 1
2016-09-06T04:42:03.200639: step 1965, loss 0.0110163, acc 1
2016-09-06T04:42:04.040592: step 1966, loss 0.142813, acc 0.94
2016-09-06T04:42:04.870853: step 1967, loss 0.00826245, acc 1
2016-09-06T04:42:05.702807: step 1968, loss 0.0306199, acc 1
2016-09-06T04:42:06.504631: step 1969, loss 0.0769129, acc 0.98
2016-09-06T04:42:07.280781: step 1970, loss 0.0367621, acc 0.98
2016-09-06T04:42:08.127868: step 1971, loss 0.0352829, acc 1
2016-09-06T04:42:08.923662: step 1972, loss 0.059072, acc 0.98
2016-09-06T04:42:09.724296: step 1973, loss 0.0389865, acc 0.98
2016-09-06T04:42:10.547667: step 1974, loss 0.0325751, acc 0.98
2016-09-06T04:42:11.356940: step 1975, loss 0.0415259, acc 0.98
2016-09-06T04:42:12.163248: step 1976, loss 0.131246, acc 0.94
2016-09-06T04:42:13.020820: step 1977, loss 0.0141141, acc 1
2016-09-06T04:42:13.818423: step 1978, loss 0.0653344, acc 0.96
2016-09-06T04:42:14.635098: step 1979, loss 0.0584852, acc 0.98
2016-09-06T04:42:15.485629: step 1980, loss 0.117009, acc 0.96
2016-09-06T04:42:16.314798: step 1981, loss 0.0959816, acc 0.96
2016-09-06T04:42:17.109969: step 1982, loss 0.108795, acc 0.94
2016-09-06T04:42:17.927397: step 1983, loss 0.00558209, acc 1
2016-09-06T04:42:18.764814: step 1984, loss 0.0994981, acc 0.94
2016-09-06T04:42:19.567539: step 1985, loss 0.0752834, acc 0.94
2016-09-06T04:42:20.362620: step 1986, loss 0.159899, acc 0.94
2016-09-06T04:42:21.174438: step 1987, loss 0.013671, acc 1
2016-09-06T04:42:21.980216: step 1988, loss 0.168886, acc 0.94
2016-09-06T04:42:22.801375: step 1989, loss 0.0788924, acc 0.96
2016-09-06T04:42:23.593919: step 1990, loss 0.0747825, acc 0.96
2016-09-06T04:42:24.395068: step 1991, loss 0.0887412, acc 0.98
2016-09-06T04:42:25.207180: step 1992, loss 0.142583, acc 0.94
2016-09-06T04:42:26.040201: step 1993, loss 0.0758358, acc 0.98
2016-09-06T04:42:26.833457: step 1994, loss 0.0374993, acc 0.98
2016-09-06T04:42:27.618985: step 1995, loss 0.109887, acc 0.92
2016-09-06T04:42:28.451808: step 1996, loss 0.0292421, acc 1
2016-09-06T04:42:29.246941: step 1997, loss 0.0476715, acc 0.98
2016-09-06T04:42:30.036182: step 1998, loss 0.15815, acc 0.92
2016-09-06T04:42:30.867004: step 1999, loss 0.0799113, acc 0.96
2016-09-06T04:42:31.649261: step 2000, loss 0.108962, acc 0.96

Evaluation:
2016-09-06T04:42:35.372972: step 2000, loss 1.02488, acc 0.761726

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-2000

2016-09-06T04:42:37.256110: step 2001, loss 0.053909, acc 0.98
2016-09-06T04:42:38.077421: step 2002, loss 0.0201907, acc 1
2016-09-06T04:42:38.907848: step 2003, loss 0.0440798, acc 1
2016-09-06T04:42:39.733859: step 2004, loss 0.0111453, acc 1
2016-09-06T04:42:40.545154: step 2005, loss 0.0569715, acc 1
2016-09-06T04:42:41.335760: step 2006, loss 0.0402923, acc 0.98
2016-09-06T04:42:42.114675: step 2007, loss 0.0314736, acc 0.98
2016-09-06T04:42:42.924181: step 2008, loss 0.0593829, acc 0.96
2016-09-06T04:42:43.761561: step 2009, loss 0.0979591, acc 0.94
2016-09-06T04:42:44.570290: step 2010, loss 0.0436169, acc 0.98
2016-09-06T04:42:45.382634: step 2011, loss 0.0545986, acc 0.98
2016-09-06T04:42:46.160586: step 2012, loss 0.0196085, acc 1
2016-09-06T04:42:46.954774: step 2013, loss 0.105481, acc 0.92
2016-09-06T04:42:47.768194: step 2014, loss 0.118359, acc 0.96
2016-09-06T04:42:48.544663: step 2015, loss 0.0735892, acc 0.94
2016-09-06T04:42:49.346223: step 2016, loss 0.041378, acc 1
2016-09-06T04:42:50.176847: step 2017, loss 0.00804865, acc 1
2016-09-06T04:42:51.008889: step 2018, loss 0.0940117, acc 0.96
2016-09-06T04:42:51.813843: step 2019, loss 0.0821045, acc 0.96
2016-09-06T04:42:52.640347: step 2020, loss 0.00953424, acc 1
2016-09-06T04:42:53.415021: step 2021, loss 0.0746029, acc 0.98
2016-09-06T04:42:54.239391: step 2022, loss 0.0258864, acc 0.98
2016-09-06T04:42:55.032543: step 2023, loss 0.108439, acc 0.96
2016-09-06T04:42:55.818218: step 2024, loss 0.00683762, acc 1
2016-09-06T04:42:56.645780: step 2025, loss 0.031248, acc 0.98
2016-09-06T04:42:57.471487: step 2026, loss 0.035471, acc 0.98
2016-09-06T04:42:58.265114: step 2027, loss 0.0307272, acc 0.98
2016-09-06T04:42:59.049795: step 2028, loss 0.0184368, acc 1
2016-09-06T04:42:59.876482: step 2029, loss 0.0333795, acc 0.98
2016-09-06T04:43:00.688019: step 2030, loss 0.0566732, acc 0.98
2016-09-06T04:43:01.500595: step 2031, loss 0.0622882, acc 0.96
2016-09-06T04:43:02.323965: step 2032, loss 0.0824865, acc 0.96
2016-09-06T04:43:03.130496: step 2033, loss 0.0394001, acc 1
2016-09-06T04:43:03.935608: step 2034, loss 0.0769726, acc 0.96
2016-09-06T04:43:04.752269: step 2035, loss 0.188815, acc 0.88
2016-09-06T04:43:05.572251: step 2036, loss 0.0717827, acc 0.98
2016-09-06T04:43:06.396604: step 2037, loss 0.00607345, acc 1
2016-09-06T04:43:07.223386: step 2038, loss 0.0287843, acc 0.98
2016-09-06T04:43:08.070449: step 2039, loss 0.0543816, acc 0.96
2016-09-06T04:43:08.894853: step 2040, loss 0.130725, acc 0.96
2016-09-06T04:43:09.737400: step 2041, loss 0.0128077, acc 1
2016-09-06T04:43:10.578097: step 2042, loss 0.0210234, acc 1
2016-09-06T04:43:11.422510: step 2043, loss 0.040544, acc 0.98
2016-09-06T04:43:12.265772: step 2044, loss 0.047015, acc 0.96
2016-09-06T04:43:13.076801: step 2045, loss 0.0713837, acc 0.96
2016-09-06T04:43:13.884548: step 2046, loss 0.0386571, acc 0.98
2016-09-06T04:43:14.705999: step 2047, loss 0.0791309, acc 0.98
2016-09-06T04:43:15.515697: step 2048, loss 0.0276513, acc 0.98
2016-09-06T04:43:16.309860: step 2049, loss 0.0772151, acc 0.96
2016-09-06T04:43:17.136457: step 2050, loss 0.0668683, acc 1
2016-09-06T04:43:17.979273: step 2051, loss 0.0854094, acc 0.98
2016-09-06T04:43:18.809328: step 2052, loss 0.131779, acc 0.98
2016-09-06T04:43:19.591509: step 2053, loss 0.0272294, acc 1
2016-09-06T04:43:20.405437: step 2054, loss 0.0394681, acc 0.98
2016-09-06T04:43:21.193335: step 2055, loss 0.0695611, acc 0.98
2016-09-06T04:43:22.007371: step 2056, loss 0.0187702, acc 1
2016-09-06T04:43:22.847752: step 2057, loss 0.020391, acc 0.98
2016-09-06T04:43:23.658453: step 2058, loss 0.0667314, acc 0.96
2016-09-06T04:43:24.431154: step 2059, loss 0.0123936, acc 1
2016-09-06T04:43:25.233159: step 2060, loss 0.0248665, acc 0.98
2016-09-06T04:43:26.002595: step 2061, loss 0.10288, acc 0.96
2016-09-06T04:43:26.810237: step 2062, loss 0.0780703, acc 0.98
2016-09-06T04:43:27.635706: step 2063, loss 0.138525, acc 0.96
2016-09-06T04:43:28.441806: step 2064, loss 0.0388809, acc 1
2016-09-06T04:43:29.243597: step 2065, loss 0.0915622, acc 0.98
2016-09-06T04:43:30.058648: step 2066, loss 0.0630431, acc 0.96
2016-09-06T04:43:30.851036: step 2067, loss 0.0609812, acc 0.98
2016-09-06T04:43:31.664246: step 2068, loss 0.021981, acc 1
2016-09-06T04:43:32.484329: step 2069, loss 0.0261504, acc 1
2016-09-06T04:43:33.286279: step 2070, loss 0.0430785, acc 0.98
2016-09-06T04:43:34.103409: step 2071, loss 0.0756985, acc 0.96
2016-09-06T04:43:34.941213: step 2072, loss 0.0187134, acc 1
2016-09-06T04:43:35.718913: step 2073, loss 0.169596, acc 0.9
2016-09-06T04:43:36.525538: step 2074, loss 0.0304507, acc 1
2016-09-06T04:43:37.358859: step 2075, loss 0.0570927, acc 0.98
2016-09-06T04:43:38.128370: step 2076, loss 0.086252, acc 0.96
2016-09-06T04:43:38.956690: step 2077, loss 0.0329614, acc 0.98
2016-09-06T04:43:39.749559: step 2078, loss 0.0728326, acc 0.96
2016-09-06T04:43:40.550352: step 2079, loss 0.131751, acc 0.9
2016-09-06T04:43:41.349337: step 2080, loss 0.0673863, acc 0.96
2016-09-06T04:43:42.147825: step 2081, loss 0.0436211, acc 0.98
2016-09-06T04:43:42.937565: step 2082, loss 0.0176157, acc 1
2016-09-06T04:43:43.725588: step 2083, loss 0.0781801, acc 0.96
2016-09-06T04:43:44.544012: step 2084, loss 0.00624104, acc 1
2016-09-06T04:43:45.329402: step 2085, loss 0.0129905, acc 1
2016-09-06T04:43:46.148850: step 2086, loss 0.0817826, acc 0.98
2016-09-06T04:43:46.962147: step 2087, loss 0.100017, acc 0.96
2016-09-06T04:43:47.763974: step 2088, loss 0.133237, acc 0.96
2016-09-06T04:43:48.560059: step 2089, loss 0.179155, acc 0.9
2016-09-06T04:43:49.393419: step 2090, loss 0.0701203, acc 0.98
2016-09-06T04:43:50.203010: step 2091, loss 0.0480024, acc 0.96
2016-09-06T04:43:51.016714: step 2092, loss 0.0643245, acc 0.98
2016-09-06T04:43:51.858597: step 2093, loss 0.178816, acc 0.94
2016-09-06T04:43:52.691785: step 2094, loss 0.117232, acc 0.92
2016-09-06T04:43:53.517718: step 2095, loss 0.114929, acc 0.96
2016-09-06T04:43:54.381855: step 2096, loss 0.0443813, acc 0.98
2016-09-06T04:43:55.181744: step 2097, loss 0.0548606, acc 0.98
2016-09-06T04:43:56.005653: step 2098, loss 0.177602, acc 0.94
2016-09-06T04:43:56.905357: step 2099, loss 0.0176134, acc 1
2016-09-06T04:43:57.717370: step 2100, loss 0.072265, acc 0.96

Evaluation:
2016-09-06T04:44:01.473532: step 2100, loss 1.12537, acc 0.760788

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-2100

2016-09-06T04:44:03.312313: step 2101, loss 0.0309482, acc 1
2016-09-06T04:44:04.171612: step 2102, loss 0.0846431, acc 0.98
2016-09-06T04:44:04.986424: step 2103, loss 0.0569095, acc 0.96
2016-09-06T04:44:05.785099: step 2104, loss 0.145853, acc 0.96
2016-09-06T04:44:06.620999: step 2105, loss 0.190661, acc 0.96
2016-09-06T04:44:07.419367: step 2106, loss 0.083418, acc 0.96
2016-09-06T04:44:08.231723: step 2107, loss 0.0489384, acc 0.98
2016-09-06T04:44:09.072114: step 2108, loss 0.0389149, acc 1
2016-09-06T04:44:09.883628: step 2109, loss 0.0762015, acc 0.96
2016-09-06T04:44:10.706875: step 2110, loss 0.0457369, acc 0.96
2016-09-06T04:44:11.549296: step 2111, loss 0.0450579, acc 0.98
2016-09-06T04:44:12.343845: step 2112, loss 0.0408124, acc 1
2016-09-06T04:44:13.172455: step 2113, loss 0.0288439, acc 1
2016-09-06T04:44:13.991559: step 2114, loss 0.0979854, acc 0.96
2016-09-06T04:44:14.850592: step 2115, loss 0.0257314, acc 1
2016-09-06T04:44:15.632693: step 2116, loss 0.116243, acc 0.96
2016-09-06T04:44:16.405290: step 2117, loss 0.0430046, acc 1
2016-09-06T04:44:17.212862: step 2118, loss 0.0557753, acc 0.98
2016-09-06T04:44:17.993068: step 2119, loss 0.0406304, acc 0.98
2016-09-06T04:44:18.833357: step 2120, loss 0.039284, acc 0.98
2016-09-06T04:44:19.674699: step 2121, loss 0.0620277, acc 0.98
2016-09-06T04:44:20.498758: step 2122, loss 0.0922447, acc 0.94
2016-09-06T04:44:21.315371: step 2123, loss 0.0147858, acc 1
2016-09-06T04:44:22.133540: step 2124, loss 0.0105293, acc 1
2016-09-06T04:44:22.926500: step 2125, loss 0.0739814, acc 0.96
2016-09-06T04:44:23.737891: step 2126, loss 0.0556889, acc 0.98
2016-09-06T04:44:24.551183: step 2127, loss 0.0405908, acc 0.96
2016-09-06T04:44:25.374258: step 2128, loss 0.0150513, acc 1
2016-09-06T04:44:26.181271: step 2129, loss 0.0704104, acc 0.98
2016-09-06T04:44:27.024380: step 2130, loss 0.0848386, acc 0.98
2016-09-06T04:44:27.832822: step 2131, loss 0.0753097, acc 0.98
2016-09-06T04:44:28.660526: step 2132, loss 0.00826405, acc 1
2016-09-06T04:44:29.489117: step 2133, loss 0.0563355, acc 1
2016-09-06T04:44:30.269382: step 2134, loss 0.0291327, acc 1
2016-09-06T04:44:31.078129: step 2135, loss 0.0299613, acc 0.98
2016-09-06T04:44:31.921027: step 2136, loss 0.0164703, acc 1
2016-09-06T04:44:32.726302: step 2137, loss 0.0887474, acc 0.94
2016-09-06T04:44:33.520934: step 2138, loss 0.188346, acc 0.9
2016-09-06T04:44:34.342224: step 2139, loss 0.0348288, acc 0.98
2016-09-06T04:44:35.145979: step 2140, loss 0.0399462, acc 0.98
2016-09-06T04:44:35.950741: step 2141, loss 0.16128, acc 0.94
2016-09-06T04:44:36.793401: step 2142, loss 0.0115903, acc 1
2016-09-06T04:44:37.615636: step 2143, loss 0.0288476, acc 1
2016-09-06T04:44:38.422716: step 2144, loss 0.0126256, acc 1
2016-09-06T04:44:39.255892: step 2145, loss 0.0420527, acc 0.98
2016-09-06T04:44:40.057680: step 2146, loss 0.0300033, acc 1
2016-09-06T04:44:40.865525: step 2147, loss 0.0172077, acc 1
2016-09-06T04:44:41.692602: step 2148, loss 0.0463899, acc 0.98
2016-09-06T04:44:42.504790: step 2149, loss 0.00772182, acc 1
2016-09-06T04:44:43.313625: step 2150, loss 0.0469086, acc 0.98
2016-09-06T04:44:44.156708: step 2151, loss 0.0942671, acc 0.94
2016-09-06T04:44:45.005197: step 2152, loss 0.011337, acc 1
2016-09-06T04:44:45.796325: step 2153, loss 0.0683989, acc 0.96
2016-09-06T04:44:46.588382: step 2154, loss 0.0168026, acc 1
2016-09-06T04:44:47.426209: step 2155, loss 0.0689463, acc 0.96
2016-09-06T04:44:48.211413: step 2156, loss 0.0964137, acc 0.98
2016-09-06T04:44:48.997657: step 2157, loss 0.0385145, acc 1
2016-09-06T04:44:49.822026: step 2158, loss 0.0649179, acc 0.96
2016-09-06T04:44:50.599116: step 2159, loss 0.0467352, acc 0.96
2016-09-06T04:44:51.424695: step 2160, loss 0.0892892, acc 0.94
2016-09-06T04:44:52.247089: step 2161, loss 0.0982029, acc 0.9
2016-09-06T04:44:53.037498: step 2162, loss 0.0211549, acc 1
2016-09-06T04:44:53.832138: step 2163, loss 0.0651981, acc 0.98
2016-09-06T04:44:54.626278: step 2164, loss 0.0588838, acc 0.94
2016-09-06T04:44:55.437414: step 2165, loss 0.0255603, acc 1
2016-09-06T04:44:56.273215: step 2166, loss 0.0389473, acc 0.98
2016-09-06T04:44:57.132009: step 2167, loss 0.0264687, acc 0.98
2016-09-06T04:44:57.894039: step 2168, loss 0.111078, acc 0.96
2016-09-06T04:44:58.696837: step 2169, loss 0.0232649, acc 1
2016-09-06T04:44:59.522163: step 2170, loss 0.129433, acc 0.94
2016-09-06T04:45:00.315626: step 2171, loss 0.0224286, acc 1
2016-09-06T04:45:01.105833: step 2172, loss 0.0310366, acc 0.98
2016-09-06T04:45:01.923833: step 2173, loss 0.0832781, acc 0.98
2016-09-06T04:45:02.729427: step 2174, loss 0.0221066, acc 1
2016-09-06T04:45:03.550525: step 2175, loss 0.0390094, acc 0.98
2016-09-06T04:45:04.366973: step 2176, loss 0.0639823, acc 0.98
2016-09-06T04:45:05.192198: step 2177, loss 0.104688, acc 0.94
2016-09-06T04:45:06.001879: step 2178, loss 0.0804584, acc 0.96
2016-09-06T04:45:06.824947: step 2179, loss 0.0221281, acc 0.98
2016-09-06T04:45:07.617377: step 2180, loss 0.131073, acc 0.96
2016-09-06T04:45:08.410755: step 2181, loss 0.0357973, acc 0.98
2016-09-06T04:45:09.225617: step 2182, loss 0.0363934, acc 0.98
2016-09-06T04:45:09.990341: step 2183, loss 0.106634, acc 0.98
2016-09-06T04:45:10.779771: step 2184, loss 0.0286962, acc 1
2016-09-06T04:45:11.602972: step 2185, loss 0.063605, acc 0.96
2016-09-06T04:45:12.412501: step 2186, loss 0.10876, acc 0.94
2016-09-06T04:45:13.236134: step 2187, loss 0.0308885, acc 1
2016-09-06T04:45:14.085382: step 2188, loss 0.116359, acc 0.96
2016-09-06T04:45:14.865357: step 2189, loss 0.0296482, acc 0.98
2016-09-06T04:45:15.657087: step 2190, loss 0.0478478, acc 0.98
2016-09-06T04:45:16.487471: step 2191, loss 0.0749089, acc 0.98
2016-09-06T04:45:17.255641: step 2192, loss 0.0560919, acc 0.98
2016-09-06T04:45:18.092682: step 2193, loss 0.0117143, acc 1
2016-09-06T04:45:18.906013: step 2194, loss 0.043186, acc 0.98
2016-09-06T04:45:19.669429: step 2195, loss 0.0415606, acc 0.98
2016-09-06T04:45:20.483928: step 2196, loss 0.0845454, acc 0.96
2016-09-06T04:45:21.312434: step 2197, loss 0.0591416, acc 0.98
2016-09-06T04:45:22.105410: step 2198, loss 0.0377123, acc 0.98
2016-09-06T04:45:22.903913: step 2199, loss 0.00755427, acc 1
2016-09-06T04:45:23.741142: step 2200, loss 0.0865527, acc 0.96

Evaluation:
2016-09-06T04:45:27.474759: step 2200, loss 1.28847, acc 0.75985

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-2200

2016-09-06T04:45:29.440839: step 2201, loss 0.0205129, acc 1
2016-09-06T04:45:30.271029: step 2202, loss 0.0457964, acc 0.98
2016-09-06T04:45:31.115746: step 2203, loss 0.0500553, acc 0.98
2016-09-06T04:45:31.923379: step 2204, loss 0.0155535, acc 1
2016-09-06T04:45:32.735065: step 2205, loss 0.0139276, acc 1
2016-09-06T04:45:33.569057: step 2206, loss 0.0244194, acc 1
2016-09-06T04:45:34.371743: step 2207, loss 0.0885052, acc 0.98
2016-09-06T04:45:35.175331: step 2208, loss 0.037735, acc 0.98
2016-09-06T04:45:36.059143: step 2209, loss 0.0201379, acc 1
2016-09-06T04:45:36.859679: step 2210, loss 0.123589, acc 0.94
2016-09-06T04:45:37.680488: step 2211, loss 0.0907841, acc 0.98
2016-09-06T04:45:38.568223: step 2212, loss 0.0508774, acc 0.96
2016-09-06T04:45:39.401123: step 2213, loss 0.0373156, acc 1
2016-09-06T04:45:40.249820: step 2214, loss 0.0648399, acc 0.96
2016-09-06T04:45:41.068429: step 2215, loss 0.0648471, acc 0.98
2016-09-06T04:45:41.907507: step 2216, loss 0.125102, acc 0.98
2016-09-06T04:45:42.722492: step 2217, loss 0.0649813, acc 0.94
2016-09-06T04:45:43.512074: step 2218, loss 0.0669298, acc 0.98
2016-09-06T04:45:44.325642: step 2219, loss 0.0873334, acc 0.98
2016-09-06T04:45:45.119016: step 2220, loss 0.0274893, acc 0.98
2016-09-06T04:45:45.920714: step 2221, loss 0.0242966, acc 1
2016-09-06T04:45:46.738551: step 2222, loss 0.0775623, acc 0.96
2016-09-06T04:45:47.542087: step 2223, loss 0.0479489, acc 1
2016-09-06T04:45:48.346540: step 2224, loss 0.0394864, acc 0.98
2016-09-06T04:45:49.157616: step 2225, loss 0.110413, acc 0.98
2016-09-06T04:45:49.917707: step 2226, loss 0.0204699, acc 1
2016-09-06T04:45:50.751758: step 2227, loss 0.0643112, acc 0.98
2016-09-06T04:45:51.571519: step 2228, loss 0.0321151, acc 1
2016-09-06T04:45:52.395108: step 2229, loss 0.0287769, acc 0.98
2016-09-06T04:45:53.214880: step 2230, loss 0.0497694, acc 0.98
2016-09-06T04:45:54.056102: step 2231, loss 0.0190362, acc 1
2016-09-06T04:45:54.873396: step 2232, loss 0.0635439, acc 0.98
2016-09-06T04:45:55.674264: step 2233, loss 0.0621221, acc 0.98
2016-09-06T04:45:56.514108: step 2234, loss 0.106673, acc 0.98
2016-09-06T04:45:57.350111: step 2235, loss 0.00726806, acc 1
2016-09-06T04:45:58.169008: step 2236, loss 0.106423, acc 0.94
2016-09-06T04:45:59.023376: step 2237, loss 0.027992, acc 1
2016-09-06T04:45:59.809410: step 2238, loss 0.045588, acc 0.98
2016-09-06T04:46:00.654685: step 2239, loss 0.0394746, acc 0.98
2016-09-06T04:46:01.484007: step 2240, loss 0.058934, acc 0.98
2016-09-06T04:46:02.305088: step 2241, loss 0.0262603, acc 1
2016-09-06T04:46:03.135248: step 2242, loss 0.209658, acc 0.94
2016-09-06T04:46:03.974615: step 2243, loss 0.0321237, acc 0.98
2016-09-06T04:46:04.790648: step 2244, loss 0.0558695, acc 0.96
2016-09-06T04:46:05.606844: step 2245, loss 0.0557775, acc 0.98
2016-09-06T04:46:06.400794: step 2246, loss 0.0550959, acc 0.96
2016-09-06T04:46:07.227710: step 2247, loss 0.0791772, acc 0.96
2016-09-06T04:46:08.024909: step 2248, loss 0.0177618, acc 1
2016-09-06T04:46:08.827874: step 2249, loss 0.0872269, acc 0.96
2016-09-06T04:46:09.649062: step 2250, loss 0.0727123, acc 0.96
2016-09-06T04:46:10.451531: step 2251, loss 0.0381007, acc 0.98
2016-09-06T04:46:11.257204: step 2252, loss 0.0227972, acc 1
2016-09-06T04:46:12.072917: step 2253, loss 0.0416009, acc 0.98
2016-09-06T04:46:12.885620: step 2254, loss 0.02293, acc 1
2016-09-06T04:46:13.679531: step 2255, loss 0.0237574, acc 0.98
2016-09-06T04:46:14.503371: step 2256, loss 0.174402, acc 0.92
2016-09-06T04:46:15.271001: step 2257, loss 0.0664719, acc 0.96
2016-09-06T04:46:16.086329: step 2258, loss 0.0565871, acc 0.96
2016-09-06T04:46:16.902586: step 2259, loss 0.0697005, acc 0.96
2016-09-06T04:46:17.688517: step 2260, loss 0.0282674, acc 1
2016-09-06T04:46:18.480724: step 2261, loss 0.0184978, acc 1
2016-09-06T04:46:19.288666: step 2262, loss 0.0380374, acc 0.98
2016-09-06T04:46:20.084954: step 2263, loss 0.0778389, acc 0.98
2016-09-06T04:46:20.886319: step 2264, loss 0.0267566, acc 1
2016-09-06T04:46:21.748674: step 2265, loss 0.0931957, acc 0.94
2016-09-06T04:46:22.510472: step 2266, loss 0.0567254, acc 0.98
2016-09-06T04:46:23.333012: step 2267, loss 0.0561938, acc 0.96
2016-09-06T04:46:24.133906: step 2268, loss 0.0990821, acc 0.94
2016-09-06T04:46:24.938030: step 2269, loss 0.0351328, acc 0.98
2016-09-06T04:46:25.742648: step 2270, loss 0.0465864, acc 0.96
2016-09-06T04:46:26.579537: step 2271, loss 0.0712192, acc 0.94
2016-09-06T04:46:27.385459: step 2272, loss 0.0688696, acc 0.98
2016-09-06T04:46:28.177820: step 2273, loss 0.0751857, acc 0.96
2016-09-06T04:46:28.989979: step 2274, loss 0.10983, acc 0.98
2016-09-06T04:46:29.781153: step 2275, loss 0.0560451, acc 0.98
2016-09-06T04:46:30.599339: step 2276, loss 0.0101805, acc 1
2016-09-06T04:46:31.396022: step 2277, loss 0.0216252, acc 1
2016-09-06T04:46:32.208881: step 2278, loss 0.151275, acc 0.96
2016-09-06T04:46:32.999021: step 2279, loss 0.0374775, acc 0.96
2016-09-06T04:46:33.808883: step 2280, loss 0.0144009, acc 1
2016-09-06T04:46:34.627855: step 2281, loss 0.0550893, acc 0.96
2016-09-06T04:46:35.451696: step 2282, loss 0.0370891, acc 1
2016-09-06T04:46:36.270951: step 2283, loss 0.0302359, acc 1
2016-09-06T04:46:37.041969: step 2284, loss 0.0686074, acc 0.96
2016-09-06T04:46:37.835395: step 2285, loss 0.0137974, acc 1
2016-09-06T04:46:38.662654: step 2286, loss 0.0525377, acc 0.98
2016-09-06T04:46:39.461490: step 2287, loss 0.154703, acc 0.96
2016-09-06T04:46:40.253807: step 2288, loss 0.0398196, acc 0.98
2016-09-06T04:46:41.049731: step 2289, loss 0.042075, acc 0.98
2016-09-06T04:46:41.853753: step 2290, loss 0.134323, acc 0.94
2016-09-06T04:46:42.672153: step 2291, loss 0.0366001, acc 0.98
2016-09-06T04:46:43.474861: step 2292, loss 0.0472774, acc 0.98
2016-09-06T04:46:44.265595: step 2293, loss 0.0211273, acc 1
2016-09-06T04:46:45.083999: step 2294, loss 0.0203306, acc 1
2016-09-06T04:46:45.916116: step 2295, loss 0.0791167, acc 0.96
2016-09-06T04:46:46.691866: step 2296, loss 0.0367763, acc 0.98
2016-09-06T04:46:47.497975: step 2297, loss 0.0372324, acc 1
2016-09-06T04:46:48.295620: step 2298, loss 0.0387719, acc 0.98
2016-09-06T04:46:49.081432: step 2299, loss 0.123861, acc 0.96
2016-09-06T04:46:49.879602: step 2300, loss 0.0410535, acc 0.98

Evaluation:
2016-09-06T04:46:53.617123: step 2300, loss 1.39894, acc 0.757974

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-2300

2016-09-06T04:46:55.487176: step 2301, loss 0.0934551, acc 0.94
2016-09-06T04:46:56.285974: step 2302, loss 0.0399749, acc 0.98
2016-09-06T04:46:57.116552: step 2303, loss 0.0967454, acc 0.96
2016-09-06T04:46:57.852006: step 2304, loss 0.0193141, acc 1
2016-09-06T04:46:58.672152: step 2305, loss 0.041101, acc 1
2016-09-06T04:46:59.496200: step 2306, loss 0.0519963, acc 0.98
2016-09-06T04:47:00.338983: step 2307, loss 0.00611522, acc 1
2016-09-06T04:47:01.158084: step 2308, loss 0.0519533, acc 0.98
2016-09-06T04:47:01.992892: step 2309, loss 0.02798, acc 1
2016-09-06T04:47:02.806367: step 2310, loss 0.137052, acc 0.98
2016-09-06T04:47:03.612371: step 2311, loss 0.00811713, acc 1
2016-09-06T04:47:04.470534: step 2312, loss 0.0285316, acc 1
2016-09-06T04:47:05.274296: step 2313, loss 0.0442889, acc 0.98
2016-09-06T04:47:06.096181: step 2314, loss 0.0492534, acc 0.98
2016-09-06T04:47:06.922839: step 2315, loss 0.026445, acc 0.98
2016-09-06T04:47:07.763643: step 2316, loss 0.0691488, acc 0.96
2016-09-06T04:47:08.564486: step 2317, loss 0.093001, acc 0.98
2016-09-06T04:47:09.380053: step 2318, loss 0.0310097, acc 0.98
2016-09-06T04:47:10.214059: step 2319, loss 0.104782, acc 0.96
2016-09-06T04:47:10.992453: step 2320, loss 0.00984246, acc 1
2016-09-06T04:47:11.792228: step 2321, loss 0.0585345, acc 0.98
2016-09-06T04:47:12.636824: step 2322, loss 0.0667074, acc 0.98
2016-09-06T04:47:13.455192: step 2323, loss 0.0278667, acc 0.98
2016-09-06T04:47:14.276294: step 2324, loss 0.00698417, acc 1
2016-09-06T04:47:15.140913: step 2325, loss 0.00900853, acc 1
2016-09-06T04:47:15.916643: step 2326, loss 0.0248812, acc 1
2016-09-06T04:47:16.736782: step 2327, loss 0.0778019, acc 0.96
2016-09-06T04:47:17.581191: step 2328, loss 0.0712138, acc 0.98
2016-09-06T04:47:18.416507: step 2329, loss 0.023639, acc 1
2016-09-06T04:47:19.246412: step 2330, loss 0.18901, acc 0.94
2016-09-06T04:47:20.075614: step 2331, loss 0.0402022, acc 0.98
2016-09-06T04:47:20.847790: step 2332, loss 0.0364545, acc 0.98
2016-09-06T04:47:21.702137: step 2333, loss 0.0440199, acc 0.98
2016-09-06T04:47:22.530807: step 2334, loss 0.0593516, acc 0.96
2016-09-06T04:47:23.365902: step 2335, loss 0.0747518, acc 0.96
2016-09-06T04:47:24.197692: step 2336, loss 0.0281239, acc 1
2016-09-06T04:47:25.026809: step 2337, loss 0.0402176, acc 0.98
2016-09-06T04:47:25.855885: step 2338, loss 0.0148782, acc 1
2016-09-06T04:47:26.656024: step 2339, loss 0.00780706, acc 1
2016-09-06T04:47:27.456316: step 2340, loss 0.0729755, acc 0.96
2016-09-06T04:47:28.271860: step 2341, loss 0.0375097, acc 0.98
2016-09-06T04:47:29.065824: step 2342, loss 0.138362, acc 0.98
2016-09-06T04:47:29.873429: step 2343, loss 0.0567813, acc 0.96
2016-09-06T04:47:30.698016: step 2344, loss 0.0874226, acc 0.96
2016-09-06T04:47:31.500430: step 2345, loss 0.033181, acc 0.98
2016-09-06T04:47:32.301111: step 2346, loss 0.0158575, acc 1
2016-09-06T04:47:33.114330: step 2347, loss 0.0551928, acc 0.96
2016-09-06T04:47:33.893856: step 2348, loss 0.0582428, acc 0.96
2016-09-06T04:47:34.726356: step 2349, loss 0.0324111, acc 0.98
2016-09-06T04:47:35.542600: step 2350, loss 0.0887345, acc 0.96
2016-09-06T04:47:36.318783: step 2351, loss 0.0740337, acc 0.96
2016-09-06T04:47:37.134688: step 2352, loss 0.0457533, acc 0.98
2016-09-06T04:47:37.954840: step 2353, loss 0.0662598, acc 0.98
2016-09-06T04:47:38.754499: step 2354, loss 0.0663152, acc 0.98
2016-09-06T04:47:39.578123: step 2355, loss 0.0412952, acc 0.98
2016-09-06T04:47:40.432199: step 2356, loss 0.0759971, acc 0.96
2016-09-06T04:47:41.230041: step 2357, loss 0.0690358, acc 0.98
2016-09-06T04:47:42.005442: step 2358, loss 0.0187589, acc 1
2016-09-06T04:47:42.831938: step 2359, loss 0.0139309, acc 1
2016-09-06T04:47:43.588957: step 2360, loss 0.0146725, acc 1
2016-09-06T04:47:44.412596: step 2361, loss 0.0116021, acc 1
2016-09-06T04:47:45.257552: step 2362, loss 0.0904025, acc 0.94
2016-09-06T04:47:46.059821: step 2363, loss 0.056678, acc 0.96
2016-09-06T04:47:46.864047: step 2364, loss 0.0117708, acc 1
2016-09-06T04:47:47.667977: step 2365, loss 0.0130031, acc 1
2016-09-06T04:47:48.471203: step 2366, loss 0.0306027, acc 1
2016-09-06T04:47:49.266391: step 2367, loss 0.00999909, acc 1
2016-09-06T04:47:50.073826: step 2368, loss 0.025069, acc 0.98
2016-09-06T04:47:50.843743: step 2369, loss 0.100696, acc 0.98
2016-09-06T04:47:51.629866: step 2370, loss 0.0721338, acc 0.96
2016-09-06T04:47:52.452601: step 2371, loss 0.0558411, acc 0.98
2016-09-06T04:47:53.259639: step 2372, loss 0.0324001, acc 1
2016-09-06T04:47:54.075951: step 2373, loss 0.0186294, acc 1
2016-09-06T04:47:54.910471: step 2374, loss 0.128643, acc 0.98
2016-09-06T04:47:55.690934: step 2375, loss 0.0286988, acc 1
2016-09-06T04:47:56.471614: step 2376, loss 0.0201769, acc 1
2016-09-06T04:47:57.290383: step 2377, loss 0.0310115, acc 1
2016-09-06T04:47:58.104326: step 2378, loss 0.0108334, acc 1
2016-09-06T04:47:58.901814: step 2379, loss 0.116712, acc 0.92
2016-09-06T04:47:59.747853: step 2380, loss 0.0320065, acc 1
2016-09-06T04:48:00.589803: step 2381, loss 0.0510888, acc 0.98
2016-09-06T04:48:01.385609: step 2382, loss 0.0250338, acc 1
2016-09-06T04:48:02.217756: step 2383, loss 0.0509021, acc 0.96
2016-09-06T04:48:03.014416: step 2384, loss 0.043266, acc 0.96
2016-09-06T04:48:03.835374: step 2385, loss 0.0175474, acc 1
2016-09-06T04:48:04.677148: step 2386, loss 0.0306774, acc 0.98
2016-09-06T04:48:05.496551: step 2387, loss 0.0748063, acc 0.94
2016-09-06T04:48:06.304146: step 2388, loss 0.0137964, acc 1
2016-09-06T04:48:07.107329: step 2389, loss 0.0608547, acc 0.98
2016-09-06T04:48:07.911509: step 2390, loss 0.081014, acc 0.98
2016-09-06T04:48:08.705418: step 2391, loss 0.0329377, acc 0.98
2016-09-06T04:48:09.531962: step 2392, loss 0.0240323, acc 0.98
2016-09-06T04:48:10.325961: step 2393, loss 0.0273105, acc 1
2016-09-06T04:48:11.113411: step 2394, loss 0.0563076, acc 0.98
2016-09-06T04:48:11.950033: step 2395, loss 0.0615799, acc 0.96
2016-09-06T04:48:12.786280: step 2396, loss 0.0178449, acc 1
2016-09-06T04:48:13.611703: step 2397, loss 0.0730054, acc 0.96
2016-09-06T04:48:14.450929: step 2398, loss 0.075526, acc 0.94
2016-09-06T04:48:15.230468: step 2399, loss 0.129925, acc 0.98
2016-09-06T04:48:16.050654: step 2400, loss 0.0203999, acc 1

Evaluation:
2016-09-06T04:48:19.821999: step 2400, loss 1.6009, acc 0.755159

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-2400

2016-09-06T04:48:21.737839: step 2401, loss 0.0383181, acc 0.98
2016-09-06T04:48:22.552222: step 2402, loss 0.0377373, acc 1
2016-09-06T04:48:23.359203: step 2403, loss 0.0170646, acc 1
2016-09-06T04:48:24.176210: step 2404, loss 0.0499614, acc 0.98
2016-09-06T04:48:25.008371: step 2405, loss 0.0376563, acc 0.98
2016-09-06T04:48:25.809145: step 2406, loss 0.00542607, acc 1
2016-09-06T04:48:26.655967: step 2407, loss 0.152157, acc 0.96
2016-09-06T04:48:27.488036: step 2408, loss 0.0251672, acc 1
2016-09-06T04:48:28.276083: step 2409, loss 0.0329989, acc 1
2016-09-06T04:48:29.108407: step 2410, loss 0.0347712, acc 0.98
2016-09-06T04:48:29.965074: step 2411, loss 0.0482506, acc 0.98
2016-09-06T04:48:30.782679: step 2412, loss 0.0679567, acc 0.96
2016-09-06T04:48:31.581332: step 2413, loss 0.0308267, acc 0.98
2016-09-06T04:48:32.396644: step 2414, loss 0.0757387, acc 0.96
2016-09-06T04:48:33.217342: step 2415, loss 0.0205349, acc 1
2016-09-06T04:48:34.001793: step 2416, loss 0.0635275, acc 0.96
2016-09-06T04:48:34.819176: step 2417, loss 0.132412, acc 0.92
2016-09-06T04:48:35.629400: step 2418, loss 0.0109838, acc 1
2016-09-06T04:48:36.492393: step 2419, loss 0.0695776, acc 0.98
2016-09-06T04:48:37.329736: step 2420, loss 0.103049, acc 0.98
2016-09-06T04:48:38.120282: step 2421, loss 0.0372335, acc 0.98
2016-09-06T04:48:38.924522: step 2422, loss 0.0170531, acc 1
2016-09-06T04:48:39.753248: step 2423, loss 0.145139, acc 0.96
2016-09-06T04:48:40.568035: step 2424, loss 0.0391106, acc 0.98
2016-09-06T04:48:41.371363: step 2425, loss 0.0125249, acc 1
2016-09-06T04:48:42.179835: step 2426, loss 0.0634536, acc 0.98
2016-09-06T04:48:42.986308: step 2427, loss 0.00805655, acc 1
2016-09-06T04:48:43.825898: step 2428, loss 0.056264, acc 0.98
2016-09-06T04:48:44.687425: step 2429, loss 0.0730228, acc 0.98
2016-09-06T04:48:45.536021: step 2430, loss 0.0742778, acc 0.96
2016-09-06T04:48:46.368485: step 2431, loss 0.0341786, acc 0.98
2016-09-06T04:48:47.199197: step 2432, loss 0.058982, acc 0.98
2016-09-06T04:48:48.032648: step 2433, loss 0.0935189, acc 0.96
2016-09-06T04:48:48.866416: step 2434, loss 0.0974521, acc 0.96
2016-09-06T04:48:49.700474: step 2435, loss 0.0516994, acc 0.96
2016-09-06T04:48:50.542972: step 2436, loss 0.0260487, acc 0.98
2016-09-06T04:48:51.369637: step 2437, loss 0.00621072, acc 1
2016-09-06T04:48:52.183197: step 2438, loss 0.043604, acc 0.98
2016-09-06T04:48:52.987461: step 2439, loss 0.0675882, acc 0.98
2016-09-06T04:48:53.807245: step 2440, loss 0.0901452, acc 0.96
2016-09-06T04:48:54.632529: step 2441, loss 0.0550426, acc 0.98
2016-09-06T04:48:55.442901: step 2442, loss 0.0402453, acc 0.96
2016-09-06T04:48:56.228340: step 2443, loss 0.0130636, acc 1
2016-09-06T04:48:57.019499: step 2444, loss 0.167645, acc 0.94
2016-09-06T04:48:57.830180: step 2445, loss 0.046502, acc 0.98
2016-09-06T04:48:58.588683: step 2446, loss 0.0490732, acc 0.96
2016-09-06T04:48:59.399524: step 2447, loss 0.0629631, acc 0.96
2016-09-06T04:49:00.249033: step 2448, loss 0.100458, acc 0.96
2016-09-06T04:49:01.050490: step 2449, loss 0.0498326, acc 0.96
2016-09-06T04:49:01.858286: step 2450, loss 0.0590503, acc 0.98
2016-09-06T04:49:02.665448: step 2451, loss 0.112333, acc 0.96
2016-09-06T04:49:03.442608: step 2452, loss 0.0283356, acc 1
2016-09-06T04:49:04.213021: step 2453, loss 0.0497585, acc 0.96
2016-09-06T04:49:05.017806: step 2454, loss 0.0313717, acc 1
2016-09-06T04:49:05.791661: step 2455, loss 0.0512748, acc 0.96
2016-09-06T04:49:06.606033: step 2456, loss 0.0175276, acc 0.98
2016-09-06T04:49:07.416116: step 2457, loss 0.0344583, acc 1
2016-09-06T04:49:08.197472: step 2458, loss 0.0260551, acc 1
2016-09-06T04:49:09.000592: step 2459, loss 0.133656, acc 0.94
2016-09-06T04:49:09.816547: step 2460, loss 0.116706, acc 0.94
2016-09-06T04:49:10.594912: step 2461, loss 0.0216095, acc 0.98
2016-09-06T04:49:11.435152: step 2462, loss 0.0388078, acc 1
2016-09-06T04:49:12.237364: step 2463, loss 0.048399, acc 0.98
2016-09-06T04:49:13.023882: step 2464, loss 0.0768427, acc 0.96
2016-09-06T04:49:13.840966: step 2465, loss 0.0764415, acc 0.96
2016-09-06T04:49:14.654742: step 2466, loss 0.0324168, acc 0.98
2016-09-06T04:49:15.449036: step 2467, loss 0.129075, acc 0.94
2016-09-06T04:49:16.251907: step 2468, loss 0.0892057, acc 0.94
2016-09-06T04:49:17.057375: step 2469, loss 0.029672, acc 1
2016-09-06T04:49:17.880892: step 2470, loss 0.0204007, acc 1
2016-09-06T04:49:18.681838: step 2471, loss 0.0976876, acc 0.94
2016-09-06T04:49:19.531476: step 2472, loss 0.0974783, acc 0.96
2016-09-06T04:49:20.317108: step 2473, loss 0.0562769, acc 0.96
2016-09-06T04:49:21.146288: step 2474, loss 0.0630002, acc 0.96
2016-09-06T04:49:21.973106: step 2475, loss 0.0771848, acc 0.98
2016-09-06T04:49:22.763779: step 2476, loss 0.04299, acc 0.98
2016-09-06T04:49:23.563691: step 2477, loss 0.0452466, acc 0.96
2016-09-06T04:49:24.380350: step 2478, loss 0.033644, acc 0.98
2016-09-06T04:49:25.150753: step 2479, loss 0.0969869, acc 0.94
2016-09-06T04:49:25.967272: step 2480, loss 0.0314646, acc 0.98
2016-09-06T04:49:26.805968: step 2481, loss 0.0997654, acc 0.94
2016-09-06T04:49:27.605438: step 2482, loss 0.118914, acc 0.96
2016-09-06T04:49:28.397728: step 2483, loss 0.0346494, acc 1
2016-09-06T04:49:29.272500: step 2484, loss 0.0868785, acc 0.94
2016-09-06T04:49:30.069820: step 2485, loss 0.0662377, acc 0.94
2016-09-06T04:49:30.879110: step 2486, loss 0.0253604, acc 1
2016-09-06T04:49:31.695410: step 2487, loss 0.0451107, acc 0.96
2016-09-06T04:49:32.516967: step 2488, loss 0.0329867, acc 1
2016-09-06T04:49:33.345282: step 2489, loss 0.0868963, acc 0.96
2016-09-06T04:49:34.187435: step 2490, loss 0.031111, acc 1
2016-09-06T04:49:35.060956: step 2491, loss 0.0352681, acc 0.98
2016-09-06T04:49:35.875706: step 2492, loss 0.0189538, acc 1
2016-09-06T04:49:36.695180: step 2493, loss 0.058234, acc 0.98
2016-09-06T04:49:37.539319: step 2494, loss 0.01492, acc 1
2016-09-06T04:49:38.391771: step 2495, loss 0.0682808, acc 0.98
2016-09-06T04:49:39.144298: step 2496, loss 0.0339481, acc 0.977273
2016-09-06T04:49:39.953341: step 2497, loss 0.0555682, acc 0.98
2016-09-06T04:49:40.776901: step 2498, loss 0.0333275, acc 0.98
2016-09-06T04:49:41.635079: step 2499, loss 0.131996, acc 0.98
2016-09-06T04:49:42.452567: step 2500, loss 0.141647, acc 0.94

Evaluation:
2016-09-06T04:49:46.142110: step 2500, loss 1.48625, acc 0.754221

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-2500

2016-09-06T04:49:47.987174: step 2501, loss 0.0404731, acc 0.98
2016-09-06T04:49:48.818506: step 2502, loss 0.0257704, acc 0.98
2016-09-06T04:49:49.626451: step 2503, loss 0.0247755, acc 1
2016-09-06T04:49:50.458899: step 2504, loss 0.0246385, acc 1
2016-09-06T04:49:51.323262: step 2505, loss 0.0752102, acc 0.96
2016-09-06T04:49:52.136943: step 2506, loss 0.0930815, acc 0.92
2016-09-06T04:49:52.942741: step 2507, loss 0.0387684, acc 0.98
2016-09-06T04:49:53.776348: step 2508, loss 0.0180584, acc 1
2016-09-06T04:49:54.586474: step 2509, loss 0.0475983, acc 0.98
2016-09-06T04:49:55.398820: step 2510, loss 0.0930572, acc 0.98
2016-09-06T04:49:56.242491: step 2511, loss 0.0805869, acc 0.96
2016-09-06T04:49:57.052214: step 2512, loss 0.0124152, acc 1
2016-09-06T04:49:57.893456: step 2513, loss 0.0108994, acc 1
2016-09-06T04:49:58.698536: step 2514, loss 0.00420074, acc 1
2016-09-06T04:49:59.544623: step 2515, loss 0.0615881, acc 0.98
2016-09-06T04:50:00.322136: step 2516, loss 0.044, acc 0.98
2016-09-06T04:50:01.125311: step 2517, loss 0.0305032, acc 1
2016-09-06T04:50:01.963116: step 2518, loss 0.0713536, acc 0.98
2016-09-06T04:50:02.742026: step 2519, loss 0.0347611, acc 1
2016-09-06T04:50:03.544301: step 2520, loss 0.0250792, acc 1
2016-09-06T04:50:04.369712: step 2521, loss 0.139398, acc 0.94
2016-09-06T04:50:05.169441: step 2522, loss 0.0656594, acc 0.94
2016-09-06T04:50:05.971987: step 2523, loss 0.0408344, acc 0.98
2016-09-06T04:50:06.832534: step 2524, loss 0.0336673, acc 0.98
2016-09-06T04:50:07.637675: step 2525, loss 0.0755043, acc 0.96
2016-09-06T04:50:08.466648: step 2526, loss 0.0236826, acc 0.98
2016-09-06T04:50:09.266134: step 2527, loss 0.0271708, acc 0.98
2016-09-06T04:50:10.090873: step 2528, loss 0.0296834, acc 1
2016-09-06T04:50:10.916011: step 2529, loss 0.101973, acc 0.94
2016-09-06T04:50:11.736221: step 2530, loss 0.00997596, acc 1
2016-09-06T04:50:12.558426: step 2531, loss 0.0100255, acc 1
2016-09-06T04:50:13.376505: step 2532, loss 0.0554886, acc 0.98
2016-09-06T04:50:14.200066: step 2533, loss 0.00397072, acc 1
2016-09-06T04:50:15.025622: step 2534, loss 0.0263092, acc 1
2016-09-06T04:50:15.815160: step 2535, loss 0.0402784, acc 0.98
2016-09-06T04:50:16.669231: step 2536, loss 0.00920678, acc 1
2016-09-06T04:50:17.484889: step 2537, loss 0.0939045, acc 0.96
2016-09-06T04:50:18.299833: step 2538, loss 0.0150259, acc 1
2016-09-06T04:50:19.125324: step 2539, loss 0.0539572, acc 0.96
2016-09-06T04:50:19.935844: step 2540, loss 0.0176512, acc 1
2016-09-06T04:50:20.755002: step 2541, loss 0.00354764, acc 1
2016-09-06T04:50:21.582869: step 2542, loss 0.011538, acc 1
2016-09-06T04:50:22.405444: step 2543, loss 0.0689939, acc 0.98
2016-09-06T04:50:23.186759: step 2544, loss 0.0906805, acc 0.98
2016-09-06T04:50:24.033494: step 2545, loss 0.0863209, acc 0.96
2016-09-06T04:50:24.878008: step 2546, loss 0.00871316, acc 1
2016-09-06T04:50:25.683519: step 2547, loss 0.0196987, acc 1
2016-09-06T04:50:26.511906: step 2548, loss 0.116112, acc 0.92
2016-09-06T04:50:27.333448: step 2549, loss 0.0326638, acc 0.96
2016-09-06T04:50:28.126768: step 2550, loss 0.142121, acc 0.96
2016-09-06T04:50:28.966278: step 2551, loss 0.0349789, acc 0.98
2016-09-06T04:50:29.809315: step 2552, loss 0.0182597, acc 1
2016-09-06T04:50:30.644259: step 2553, loss 0.0390672, acc 1
2016-09-06T04:50:31.461412: step 2554, loss 0.027548, acc 0.98
2016-09-06T04:50:32.279815: step 2555, loss 0.0735122, acc 0.98
2016-09-06T04:50:33.096961: step 2556, loss 0.0956718, acc 0.96
2016-09-06T04:50:33.919636: step 2557, loss 0.0622337, acc 0.96
2016-09-06T04:50:34.738728: step 2558, loss 0.0201396, acc 1
2016-09-06T04:50:35.561866: step 2559, loss 0.0155473, acc 1
2016-09-06T04:50:36.386110: step 2560, loss 0.172368, acc 0.94
2016-09-06T04:50:37.233782: step 2561, loss 0.0491859, acc 0.98
2016-09-06T04:50:38.043149: step 2562, loss 0.0889867, acc 0.96
2016-09-06T04:50:38.856758: step 2563, loss 0.0292605, acc 0.98
2016-09-06T04:50:39.711211: step 2564, loss 0.178903, acc 0.9
2016-09-06T04:50:40.537870: step 2565, loss 0.0812211, acc 0.98
2016-09-06T04:50:41.342744: step 2566, loss 0.103803, acc 0.96
2016-09-06T04:50:42.172459: step 2567, loss 0.0786659, acc 0.94
2016-09-06T04:50:42.979416: step 2568, loss 0.0255946, acc 1
2016-09-06T04:50:43.789369: step 2569, loss 0.102382, acc 0.96
2016-09-06T04:50:44.596008: step 2570, loss 0.0442363, acc 0.98
2016-09-06T04:50:45.399448: step 2571, loss 0.0878599, acc 0.96
2016-09-06T04:50:46.169578: step 2572, loss 0.0310591, acc 1
2016-09-06T04:50:46.984298: step 2573, loss 0.0145245, acc 1
2016-09-06T04:50:47.804851: step 2574, loss 0.0740065, acc 0.96
2016-09-06T04:50:48.616444: step 2575, loss 0.103123, acc 0.96
2016-09-06T04:50:49.425595: step 2576, loss 0.0760115, acc 0.96
2016-09-06T04:50:50.255071: step 2577, loss 0.0263618, acc 1
2016-09-06T04:50:51.058378: step 2578, loss 0.0224452, acc 1
2016-09-06T04:50:51.855585: step 2579, loss 0.0880171, acc 0.96
2016-09-06T04:50:52.695050: step 2580, loss 0.0211745, acc 1
2016-09-06T04:50:53.494124: step 2581, loss 0.0783291, acc 0.96
2016-09-06T04:50:54.302732: step 2582, loss 0.0376043, acc 1
2016-09-06T04:50:55.139861: step 2583, loss 0.0589849, acc 0.98
2016-09-06T04:50:55.920111: step 2584, loss 0.0259626, acc 0.98
2016-09-06T04:50:56.704516: step 2585, loss 0.0644111, acc 0.98
2016-09-06T04:50:57.554289: step 2586, loss 0.020507, acc 1
2016-09-06T04:50:58.356970: step 2587, loss 0.0633166, acc 0.94
2016-09-06T04:50:59.137190: step 2588, loss 0.0363915, acc 0.98
2016-09-06T04:50:59.980819: step 2589, loss 0.0699098, acc 0.96
2016-09-06T04:51:00.825257: step 2590, loss 0.031186, acc 0.98
2016-09-06T04:51:01.655948: step 2591, loss 0.0438366, acc 0.98
2016-09-06T04:51:02.481915: step 2592, loss 0.0938587, acc 0.98
2016-09-06T04:51:03.304249: step 2593, loss 0.100285, acc 0.92
2016-09-06T04:51:04.112939: step 2594, loss 0.0421332, acc 1
2016-09-06T04:51:04.933422: step 2595, loss 0.0657239, acc 0.98
2016-09-06T04:51:05.768372: step 2596, loss 0.0534056, acc 0.96
2016-09-06T04:51:06.584329: step 2597, loss 0.0250673, acc 0.98
2016-09-06T04:51:07.429688: step 2598, loss 0.0233166, acc 0.98
2016-09-06T04:51:08.237072: step 2599, loss 0.0324089, acc 1
2016-09-06T04:51:09.023352: step 2600, loss 0.120683, acc 0.98

Evaluation:
2016-09-06T04:51:12.766610: step 2600, loss 1.44127, acc 0.746717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-2600

2016-09-06T04:51:14.647607: step 2601, loss 0.0323507, acc 0.98
2016-09-06T04:51:15.465183: step 2602, loss 0.071607, acc 0.96
2016-09-06T04:51:16.285413: step 2603, loss 0.0549988, acc 0.96
2016-09-06T04:51:17.141706: step 2604, loss 0.0441182, acc 0.98
2016-09-06T04:51:17.969604: step 2605, loss 0.159649, acc 0.96
2016-09-06T04:51:18.797255: step 2606, loss 0.0502651, acc 0.98
2016-09-06T04:51:19.622131: step 2607, loss 0.122523, acc 0.96
2016-09-06T04:51:20.465515: step 2608, loss 0.0564535, acc 0.96
2016-09-06T04:51:21.268777: step 2609, loss 0.0323142, acc 1
2016-09-06T04:51:22.091203: step 2610, loss 0.0400607, acc 0.98
2016-09-06T04:51:22.893265: step 2611, loss 0.0435706, acc 1
2016-09-06T04:51:23.707553: step 2612, loss 0.0882677, acc 0.94
2016-09-06T04:51:24.541016: step 2613, loss 0.0267005, acc 0.98
2016-09-06T04:51:25.386590: step 2614, loss 0.0160945, acc 1
2016-09-06T04:51:26.164116: step 2615, loss 0.0379836, acc 1
2016-09-06T04:51:26.963326: step 2616, loss 0.0582577, acc 0.98
2016-09-06T04:51:27.808439: step 2617, loss 0.0219968, acc 0.98
2016-09-06T04:51:28.605351: step 2618, loss 0.0280823, acc 0.98
2016-09-06T04:51:29.400967: step 2619, loss 0.0539912, acc 0.98
2016-09-06T04:51:30.212474: step 2620, loss 0.0423037, acc 0.98
2016-09-06T04:51:30.995221: step 2621, loss 0.0649712, acc 0.98
2016-09-06T04:51:31.791914: step 2622, loss 0.033537, acc 1
2016-09-06T04:51:32.611586: step 2623, loss 0.0304131, acc 0.98
2016-09-06T04:51:33.410102: step 2624, loss 0.0721505, acc 0.98
2016-09-06T04:51:34.221687: step 2625, loss 0.0393577, acc 0.98
2016-09-06T04:51:35.037745: step 2626, loss 0.113006, acc 0.92
2016-09-06T04:51:35.823746: step 2627, loss 0.133577, acc 0.96
2016-09-06T04:51:36.658045: step 2628, loss 0.0231093, acc 1
2016-09-06T04:51:37.497901: step 2629, loss 0.122505, acc 0.98
2016-09-06T04:51:38.310867: step 2630, loss 0.0396933, acc 0.98
2016-09-06T04:51:39.123882: step 2631, loss 0.0112332, acc 1
2016-09-06T04:51:39.948250: step 2632, loss 0.0290087, acc 1
2016-09-06T04:51:40.765531: step 2633, loss 0.0234662, acc 1
2016-09-06T04:51:41.573712: step 2634, loss 0.103301, acc 0.98
2016-09-06T04:51:42.414084: step 2635, loss 0.0091013, acc 1
2016-09-06T04:51:43.231117: step 2636, loss 0.0260734, acc 0.98
2016-09-06T04:51:44.055598: step 2637, loss 0.0777938, acc 0.94
2016-09-06T04:51:44.892893: step 2638, loss 0.0568575, acc 0.98
2016-09-06T04:51:45.727573: step 2639, loss 0.0838578, acc 0.98
2016-09-06T04:51:46.532659: step 2640, loss 0.182635, acc 0.88
2016-09-06T04:51:47.350779: step 2641, loss 0.0722813, acc 0.96
2016-09-06T04:51:48.160101: step 2642, loss 0.10135, acc 0.96
2016-09-06T04:51:48.975136: step 2643, loss 0.0675079, acc 0.98
2016-09-06T04:51:49.816590: step 2644, loss 0.0412375, acc 0.98
2016-09-06T04:51:50.627646: step 2645, loss 0.0282352, acc 0.98
2016-09-06T04:51:51.424611: step 2646, loss 0.025795, acc 1
2016-09-06T04:51:52.274140: step 2647, loss 0.0270908, acc 1
2016-09-06T04:51:53.088610: step 2648, loss 0.069303, acc 0.96
2016-09-06T04:51:53.924278: step 2649, loss 0.0309419, acc 0.98
2016-09-06T04:51:54.742264: step 2650, loss 0.0346506, acc 0.98
2016-09-06T04:51:55.574327: step 2651, loss 0.0364113, acc 1
2016-09-06T04:51:56.384862: step 2652, loss 0.0833438, acc 0.98
2016-09-06T04:51:57.200282: step 2653, loss 0.0683642, acc 0.94
2016-09-06T04:51:58.008402: step 2654, loss 0.0651117, acc 0.98
2016-09-06T04:51:58.800388: step 2655, loss 0.0283191, acc 1
2016-09-06T04:51:59.620757: step 2656, loss 0.115198, acc 0.94
2016-09-06T04:52:00.459104: step 2657, loss 0.136591, acc 0.94
2016-09-06T04:52:01.271668: step 2658, loss 0.047275, acc 1
2016-09-06T04:52:02.079899: step 2659, loss 0.0464081, acc 0.98
2016-09-06T04:52:02.942645: step 2660, loss 0.0181219, acc 1
2016-09-06T04:52:03.777079: step 2661, loss 0.0375792, acc 1
2016-09-06T04:52:04.587276: step 2662, loss 0.0595037, acc 0.96
2016-09-06T04:52:05.442040: step 2663, loss 0.0422339, acc 0.96
2016-09-06T04:52:06.272238: step 2664, loss 0.0517125, acc 0.96
2016-09-06T04:52:07.071223: step 2665, loss 0.0705127, acc 0.98
2016-09-06T04:52:07.905152: step 2666, loss 0.0584483, acc 0.98
2016-09-06T04:52:08.711652: step 2667, loss 0.0101655, acc 1
2016-09-06T04:52:09.541052: step 2668, loss 0.0370887, acc 0.98
2016-09-06T04:52:10.376934: step 2669, loss 0.0573218, acc 0.98
2016-09-06T04:52:11.175082: step 2670, loss 0.00767749, acc 1
2016-09-06T04:52:11.965735: step 2671, loss 0.104506, acc 0.9
2016-09-06T04:52:12.818997: step 2672, loss 0.00629467, acc 1
2016-09-06T04:52:13.671208: step 2673, loss 0.0811858, acc 0.96
2016-09-06T04:52:14.474883: step 2674, loss 0.0325243, acc 0.98
2016-09-06T04:52:15.288106: step 2675, loss 0.1915, acc 0.92
2016-09-06T04:52:16.089317: step 2676, loss 0.0966574, acc 0.94
2016-09-06T04:52:16.901397: step 2677, loss 0.0563466, acc 0.98
2016-09-06T04:52:17.733016: step 2678, loss 0.0637526, acc 0.98
2016-09-06T04:52:18.541605: step 2679, loss 0.0505213, acc 0.98
2016-09-06T04:52:19.340659: step 2680, loss 0.00465125, acc 1
2016-09-06T04:52:20.173976: step 2681, loss 0.0276246, acc 1
2016-09-06T04:52:20.981122: step 2682, loss 0.0646706, acc 0.96
2016-09-06T04:52:21.751668: step 2683, loss 0.0629342, acc 0.96
2016-09-06T04:52:22.546004: step 2684, loss 0.07272, acc 0.96
2016-09-06T04:52:23.376970: step 2685, loss 0.0312783, acc 1
2016-09-06T04:52:24.201452: step 2686, loss 0.00612922, acc 1
2016-09-06T04:52:24.986458: step 2687, loss 0.0221619, acc 1
2016-09-06T04:52:25.760974: step 2688, loss 0.00519554, acc 1
2016-09-06T04:52:26.559254: step 2689, loss 0.0233592, acc 1
2016-09-06T04:52:27.374421: step 2690, loss 0.13742, acc 0.92
2016-09-06T04:52:28.200236: step 2691, loss 0.0409972, acc 0.98
2016-09-06T04:52:29.012234: step 2692, loss 0.0331442, acc 0.98
2016-09-06T04:52:29.826596: step 2693, loss 0.0377543, acc 1
2016-09-06T04:52:30.660732: step 2694, loss 0.0278325, acc 0.98
2016-09-06T04:52:31.445958: step 2695, loss 0.024532, acc 1
2016-09-06T04:52:32.243736: step 2696, loss 0.0176545, acc 1
2016-09-06T04:52:33.072156: step 2697, loss 0.0647034, acc 0.96
2016-09-06T04:52:33.862633: step 2698, loss 0.122185, acc 0.98
2016-09-06T04:52:34.644325: step 2699, loss 0.0100982, acc 1
2016-09-06T04:52:35.469558: step 2700, loss 0.0439542, acc 0.98

Evaluation:
2016-09-06T04:52:39.196875: step 2700, loss 1.6236, acc 0.746717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-2700

2016-09-06T04:52:41.145409: step 2701, loss 0.0371598, acc 0.98
2016-09-06T04:52:41.961408: step 2702, loss 0.0399657, acc 0.98
2016-09-06T04:52:42.769180: step 2703, loss 0.0287124, acc 0.98
2016-09-06T04:52:43.563490: step 2704, loss 0.0820015, acc 0.98
2016-09-06T04:52:44.386122: step 2705, loss 0.115926, acc 0.92
2016-09-06T04:52:45.243523: step 2706, loss 0.0168785, acc 1
2016-09-06T04:52:46.039665: step 2707, loss 0.0519176, acc 0.98
2016-09-06T04:52:46.858782: step 2708, loss 0.00568261, acc 1
2016-09-06T04:52:47.700275: step 2709, loss 0.160292, acc 0.96
2016-09-06T04:52:48.529883: step 2710, loss 0.0519447, acc 0.96
2016-09-06T04:52:49.324678: step 2711, loss 0.0282296, acc 0.98
2016-09-06T04:52:50.165613: step 2712, loss 0.0587239, acc 0.96
2016-09-06T04:52:50.968569: step 2713, loss 0.105413, acc 0.96
2016-09-06T04:52:51.771800: step 2714, loss 0.0474998, acc 0.96
2016-09-06T04:52:52.604941: step 2715, loss 0.0169779, acc 1
2016-09-06T04:52:53.412572: step 2716, loss 0.0573669, acc 0.98
2016-09-06T04:52:54.249491: step 2717, loss 0.0227403, acc 0.98
2016-09-06T04:52:55.087378: step 2718, loss 0.00993871, acc 1
2016-09-06T04:52:55.935686: step 2719, loss 0.0219716, acc 0.98
2016-09-06T04:52:56.756159: step 2720, loss 0.0289047, acc 1
2016-09-06T04:52:57.602487: step 2721, loss 0.00556753, acc 1
2016-09-06T04:52:58.424613: step 2722, loss 0.0330131, acc 1
2016-09-06T04:52:59.215317: step 2723, loss 0.0772689, acc 0.94
2016-09-06T04:53:00.050290: step 2724, loss 0.0980043, acc 0.96
2016-09-06T04:53:00.931443: step 2725, loss 0.0312773, acc 0.98
2016-09-06T04:53:01.723974: step 2726, loss 0.0322661, acc 0.98
2016-09-06T04:53:02.541555: step 2727, loss 0.0583164, acc 0.96
2016-09-06T04:53:03.355944: step 2728, loss 0.0524924, acc 0.98
2016-09-06T04:53:04.163892: step 2729, loss 0.0333473, acc 1
2016-09-06T04:53:04.973334: step 2730, loss 0.0479114, acc 0.98
2016-09-06T04:53:05.802104: step 2731, loss 0.0907631, acc 0.98
2016-09-06T04:53:06.611036: step 2732, loss 0.00607436, acc 1
2016-09-06T04:53:07.433933: step 2733, loss 0.0113757, acc 1
2016-09-06T04:53:08.269411: step 2734, loss 0.0337299, acc 1
2016-09-06T04:53:09.090311: step 2735, loss 0.0397405, acc 0.98
2016-09-06T04:53:09.946395: step 2736, loss 0.0183275, acc 1
2016-09-06T04:53:10.806413: step 2737, loss 0.152936, acc 0.92
2016-09-06T04:53:11.612721: step 2738, loss 0.0283488, acc 0.98
2016-09-06T04:53:12.427394: step 2739, loss 0.0188679, acc 1
2016-09-06T04:53:13.286795: step 2740, loss 0.0343306, acc 0.98
2016-09-06T04:53:14.109046: step 2741, loss 0.0471271, acc 0.98
2016-09-06T04:53:14.919964: step 2742, loss 0.094926, acc 0.96
2016-09-06T04:53:15.752684: step 2743, loss 0.0635543, acc 0.96
2016-09-06T04:53:16.573152: step 2744, loss 0.0230055, acc 1
2016-09-06T04:53:17.365186: step 2745, loss 0.0304026, acc 0.98
2016-09-06T04:53:18.169650: step 2746, loss 0.0570087, acc 0.98
2016-09-06T04:53:19.006073: step 2747, loss 0.0856595, acc 0.98
2016-09-06T04:53:19.798449: step 2748, loss 0.0252393, acc 1
2016-09-06T04:53:20.613361: step 2749, loss 0.173732, acc 0.94
2016-09-06T04:53:21.421693: step 2750, loss 0.0453247, acc 0.98
2016-09-06T04:53:22.225505: step 2751, loss 0.0183273, acc 1
2016-09-06T04:53:23.010856: step 2752, loss 0.0306358, acc 1
2016-09-06T04:53:23.843189: step 2753, loss 0.119977, acc 0.96
2016-09-06T04:53:24.663910: step 2754, loss 0.0302613, acc 1
2016-09-06T04:53:25.457709: step 2755, loss 0.0196793, acc 1
2016-09-06T04:53:26.294691: step 2756, loss 0.12308, acc 0.96
2016-09-06T04:53:27.096035: step 2757, loss 0.0256375, acc 1
2016-09-06T04:53:27.901364: step 2758, loss 0.0215571, acc 0.98
2016-09-06T04:53:28.721076: step 2759, loss 0.0339008, acc 0.98
2016-09-06T04:53:29.521315: step 2760, loss 0.016886, acc 1
2016-09-06T04:53:30.362202: step 2761, loss 0.0504233, acc 0.98
2016-09-06T04:53:31.183920: step 2762, loss 0.0806713, acc 0.96
2016-09-06T04:53:32.010143: step 2763, loss 0.0335041, acc 0.98
2016-09-06T04:53:32.837628: step 2764, loss 0.018454, acc 1
2016-09-06T04:53:33.662238: step 2765, loss 0.0839541, acc 0.98
2016-09-06T04:53:34.540853: step 2766, loss 0.0456309, acc 0.98
2016-09-06T04:53:35.368924: step 2767, loss 0.0147541, acc 1
2016-09-06T04:53:36.195567: step 2768, loss 0.0325307, acc 0.98
2016-09-06T04:53:37.002036: step 2769, loss 0.0109988, acc 1
2016-09-06T04:53:37.811649: step 2770, loss 0.0300658, acc 0.98
2016-09-06T04:53:38.642527: step 2771, loss 0.0768395, acc 0.94
2016-09-06T04:53:39.463134: step 2772, loss 0.0517902, acc 0.98
2016-09-06T04:53:40.277728: step 2773, loss 0.0122976, acc 1
2016-09-06T04:53:41.119051: step 2774, loss 0.0178351, acc 1
2016-09-06T04:53:41.917435: step 2775, loss 0.00612497, acc 1
2016-09-06T04:53:42.715488: step 2776, loss 0.0605795, acc 0.98
2016-09-06T04:53:43.559481: step 2777, loss 0.0369728, acc 0.98
2016-09-06T04:53:44.389450: step 2778, loss 0.0327452, acc 0.98
2016-09-06T04:53:45.204202: step 2779, loss 0.0879801, acc 0.94
2016-09-06T04:53:46.028807: step 2780, loss 0.0202921, acc 0.98
2016-09-06T04:53:46.880739: step 2781, loss 0.0587708, acc 0.96
2016-09-06T04:53:47.726225: step 2782, loss 0.087752, acc 0.96
2016-09-06T04:53:48.541521: step 2783, loss 0.0180401, acc 0.98
2016-09-06T04:53:49.362647: step 2784, loss 0.0334532, acc 0.98
2016-09-06T04:53:50.207483: step 2785, loss 0.0638611, acc 0.96
2016-09-06T04:53:51.062238: step 2786, loss 0.0228445, acc 1
2016-09-06T04:53:51.898908: step 2787, loss 0.00650188, acc 1
2016-09-06T04:53:52.740947: step 2788, loss 0.0151165, acc 1
2016-09-06T04:53:53.539789: step 2789, loss 0.0423704, acc 0.98
2016-09-06T04:53:54.399961: step 2790, loss 0.0120593, acc 1
2016-09-06T04:53:55.208737: step 2791, loss 0.022845, acc 1
2016-09-06T04:53:55.998659: step 2792, loss 0.02722, acc 1
2016-09-06T04:53:56.818055: step 2793, loss 0.0341154, acc 0.98
2016-09-06T04:53:57.642585: step 2794, loss 0.00321195, acc 1
2016-09-06T04:53:58.435221: step 2795, loss 0.0243933, acc 1
2016-09-06T04:53:59.238040: step 2796, loss 0.0401275, acc 0.98
2016-09-06T04:54:00.057046: step 2797, loss 0.047675, acc 0.98
2016-09-06T04:54:00.929838: step 2798, loss 0.0523455, acc 0.96
2016-09-06T04:54:01.740403: step 2799, loss 0.0278037, acc 0.98
2016-09-06T04:54:02.589840: step 2800, loss 0.0705803, acc 0.94

Evaluation:
2016-09-06T04:54:06.361842: step 2800, loss 1.73729, acc 0.746717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-2800

2016-09-06T04:54:08.184684: step 2801, loss 0.0106965, acc 1
2016-09-06T04:54:09.003243: step 2802, loss 0.063072, acc 0.96
2016-09-06T04:54:09.853258: step 2803, loss 0.121853, acc 0.94
2016-09-06T04:54:10.697258: step 2804, loss 0.0551666, acc 0.96
2016-09-06T04:54:11.506017: step 2805, loss 0.0651115, acc 0.96
2016-09-06T04:54:12.324023: step 2806, loss 0.00366971, acc 1
2016-09-06T04:54:13.117974: step 2807, loss 0.0229222, acc 1
2016-09-06T04:54:13.915908: step 2808, loss 0.148327, acc 0.98
2016-09-06T04:54:14.720203: step 2809, loss 0.00443712, acc 1
2016-09-06T04:54:15.492631: step 2810, loss 0.00692713, acc 1
2016-09-06T04:54:16.289811: step 2811, loss 0.0181619, acc 1
2016-09-06T04:54:17.125279: step 2812, loss 0.0330234, acc 0.98
2016-09-06T04:54:17.942831: step 2813, loss 0.159873, acc 0.94
2016-09-06T04:54:18.745742: step 2814, loss 0.15497, acc 0.96
2016-09-06T04:54:19.555255: step 2815, loss 0.0129188, acc 1
2016-09-06T04:54:20.365034: step 2816, loss 0.0768601, acc 0.96
2016-09-06T04:54:21.148340: step 2817, loss 0.0194373, acc 1
2016-09-06T04:54:21.955562: step 2818, loss 0.0367351, acc 0.98
2016-09-06T04:54:22.719139: step 2819, loss 0.00455069, acc 1
2016-09-06T04:54:23.512240: step 2820, loss 0.0588217, acc 0.98
2016-09-06T04:54:24.314660: step 2821, loss 0.14776, acc 0.96
2016-09-06T04:54:25.120359: step 2822, loss 0.0653387, acc 0.94
2016-09-06T04:54:25.919129: step 2823, loss 0.0557282, acc 1
2016-09-06T04:54:26.748365: step 2824, loss 0.0797254, acc 0.96
2016-09-06T04:54:27.537107: step 2825, loss 0.0412305, acc 0.98
2016-09-06T04:54:28.325002: step 2826, loss 0.11857, acc 0.92
2016-09-06T04:54:29.132472: step 2827, loss 0.0107336, acc 1
2016-09-06T04:54:29.919994: step 2828, loss 0.019377, acc 1
2016-09-06T04:54:30.721451: step 2829, loss 0.0941287, acc 0.92
2016-09-06T04:54:31.540912: step 2830, loss 0.0575302, acc 0.96
2016-09-06T04:54:32.329446: step 2831, loss 0.025779, acc 0.98
2016-09-06T04:54:33.151802: step 2832, loss 0.0196664, acc 1
2016-09-06T04:54:33.957465: step 2833, loss 0.0306944, acc 1
2016-09-06T04:54:34.769047: step 2834, loss 0.0280493, acc 1
2016-09-06T04:54:35.578505: step 2835, loss 0.031181, acc 1
2016-09-06T04:54:36.373774: step 2836, loss 0.0715878, acc 0.98
2016-09-06T04:54:37.176398: step 2837, loss 0.0632252, acc 0.98
2016-09-06T04:54:38.010077: step 2838, loss 0.0465778, acc 0.98
2016-09-06T04:54:38.811232: step 2839, loss 0.0292768, acc 1
2016-09-06T04:54:39.631490: step 2840, loss 0.100935, acc 0.96
2016-09-06T04:54:40.441279: step 2841, loss 0.0192041, acc 1
2016-09-06T04:54:41.239260: step 2842, loss 0.00684799, acc 1
2016-09-06T04:54:42.037011: step 2843, loss 0.0696676, acc 0.98
2016-09-06T04:54:42.837138: step 2844, loss 0.0136654, acc 1
2016-09-06T04:54:43.701457: step 2845, loss 0.0541122, acc 0.96
2016-09-06T04:54:44.470429: step 2846, loss 0.028665, acc 0.98
2016-09-06T04:54:45.274871: step 2847, loss 0.0335106, acc 0.98
2016-09-06T04:54:46.093078: step 2848, loss 0.0386862, acc 0.98
2016-09-06T04:54:46.912045: step 2849, loss 0.108081, acc 0.96
2016-09-06T04:54:47.701401: step 2850, loss 0.0279274, acc 0.98
2016-09-06T04:54:48.550363: step 2851, loss 0.0219487, acc 0.98
2016-09-06T04:54:49.318155: step 2852, loss 0.0828862, acc 0.96
2016-09-06T04:54:50.149673: step 2853, loss 0.0200064, acc 0.98
2016-09-06T04:54:50.931218: step 2854, loss 0.0988466, acc 0.94
2016-09-06T04:54:51.719224: step 2855, loss 0.032179, acc 0.98
2016-09-06T04:54:52.509776: step 2856, loss 0.0708431, acc 0.96
2016-09-06T04:54:53.336286: step 2857, loss 0.0649118, acc 0.96
2016-09-06T04:54:54.146171: step 2858, loss 0.0217285, acc 0.98
2016-09-06T04:54:54.942132: step 2859, loss 0.0914187, acc 0.94
2016-09-06T04:54:55.761144: step 2860, loss 0.0937604, acc 0.96
2016-09-06T04:54:56.602135: step 2861, loss 0.0057875, acc 1
2016-09-06T04:54:57.391903: step 2862, loss 0.0310032, acc 0.98
2016-09-06T04:54:58.262379: step 2863, loss 0.105941, acc 0.98
2016-09-06T04:54:59.061513: step 2864, loss 0.0510347, acc 0.96
2016-09-06T04:54:59.875420: step 2865, loss 0.0156357, acc 1
2016-09-06T04:55:00.737965: step 2866, loss 0.0423644, acc 0.98
2016-09-06T04:55:01.549061: step 2867, loss 0.0508709, acc 0.96
2016-09-06T04:55:02.357567: step 2868, loss 0.060096, acc 0.98
2016-09-06T04:55:03.179712: step 2869, loss 0.109523, acc 0.98
2016-09-06T04:55:04.022522: step 2870, loss 0.0565614, acc 0.98
2016-09-06T04:55:04.849474: step 2871, loss 0.0765417, acc 0.96
2016-09-06T04:55:05.682719: step 2872, loss 0.0339976, acc 0.98
2016-09-06T04:55:06.488226: step 2873, loss 0.0334596, acc 0.98
2016-09-06T04:55:07.319538: step 2874, loss 0.052584, acc 0.98
2016-09-06T04:55:08.160523: step 2875, loss 0.0169202, acc 1
2016-09-06T04:55:08.974716: step 2876, loss 0.0592368, acc 0.96
2016-09-06T04:55:09.781419: step 2877, loss 0.0382887, acc 0.98
2016-09-06T04:55:10.610246: step 2878, loss 0.0696074, acc 0.96
2016-09-06T04:55:11.405059: step 2879, loss 0.00631029, acc 1
2016-09-06T04:55:12.159852: step 2880, loss 0.0102009, acc 1
2016-09-06T04:55:12.993052: step 2881, loss 0.0599314, acc 0.98
2016-09-06T04:55:13.827244: step 2882, loss 0.0756454, acc 0.96
2016-09-06T04:55:14.624746: step 2883, loss 0.0328001, acc 0.98
2016-09-06T04:55:15.447182: step 2884, loss 0.0272811, acc 0.98
2016-09-06T04:55:16.245581: step 2885, loss 0.0554806, acc 0.96
2016-09-06T04:55:17.056233: step 2886, loss 0.0859224, acc 0.96
2016-09-06T04:55:17.869603: step 2887, loss 0.0268625, acc 1
2016-09-06T04:55:18.674584: step 2888, loss 0.0833218, acc 0.98
2016-09-06T04:55:19.491770: step 2889, loss 0.0120996, acc 1
2016-09-06T04:55:20.311033: step 2890, loss 0.033876, acc 1
2016-09-06T04:55:21.129663: step 2891, loss 0.0300112, acc 0.98
2016-09-06T04:55:21.928475: step 2892, loss 0.0129015, acc 1
2016-09-06T04:55:22.746932: step 2893, loss 0.0225857, acc 1
2016-09-06T04:55:23.560672: step 2894, loss 0.0429264, acc 0.98
2016-09-06T04:55:24.363686: step 2895, loss 0.144325, acc 0.94
2016-09-06T04:55:25.194408: step 2896, loss 0.0170303, acc 1
2016-09-06T04:55:25.985004: step 2897, loss 0.113894, acc 0.96
2016-09-06T04:55:26.781421: step 2898, loss 0.00561591, acc 1
2016-09-06T04:55:27.563601: step 2899, loss 0.0461978, acc 0.98
2016-09-06T04:55:28.384566: step 2900, loss 0.0380029, acc 0.98

Evaluation:
2016-09-06T04:55:32.104220: step 2900, loss 1.92804, acc 0.736398

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-2900

2016-09-06T04:55:34.071024: step 2901, loss 0.0233924, acc 0.98
2016-09-06T04:55:34.913518: step 2902, loss 0.0875038, acc 0.98
2016-09-06T04:55:35.698353: step 2903, loss 0.0240002, acc 1
2016-09-06T04:55:36.501047: step 2904, loss 0.0361627, acc 1
2016-09-06T04:55:37.321263: step 2905, loss 0.0330131, acc 1
2016-09-06T04:55:38.138419: step 2906, loss 0.0376084, acc 0.98
2016-09-06T04:55:38.937042: step 2907, loss 0.0192652, acc 1
2016-09-06T04:55:39.747341: step 2908, loss 0.0328535, acc 0.98
2016-09-06T04:55:40.583090: step 2909, loss 0.0708468, acc 0.96
2016-09-06T04:55:41.365425: step 2910, loss 0.0102149, acc 1
2016-09-06T04:55:42.186029: step 2911, loss 0.0435704, acc 0.98
2016-09-06T04:55:43.011863: step 2912, loss 0.0275643, acc 1
2016-09-06T04:55:43.793453: step 2913, loss 0.0398202, acc 0.98
2016-09-06T04:55:44.603772: step 2914, loss 0.018486, acc 1
2016-09-06T04:55:45.422502: step 2915, loss 0.0159134, acc 1
2016-09-06T04:55:46.210101: step 2916, loss 0.152028, acc 0.98
2016-09-06T04:55:47.006854: step 2917, loss 0.0153489, acc 1
2016-09-06T04:55:47.817128: step 2918, loss 0.0817979, acc 0.96
2016-09-06T04:55:48.629426: step 2919, loss 0.0162098, acc 1
2016-09-06T04:55:49.425661: step 2920, loss 0.0186639, acc 1
2016-09-06T04:55:50.240936: step 2921, loss 0.0398164, acc 0.98
2016-09-06T04:55:51.044263: step 2922, loss 0.0120769, acc 1
2016-09-06T04:55:51.838486: step 2923, loss 0.144762, acc 0.94
2016-09-06T04:55:52.659769: step 2924, loss 0.0180672, acc 1
2016-09-06T04:55:53.438317: step 2925, loss 0.016277, acc 1
2016-09-06T04:55:54.250689: step 2926, loss 0.0206504, acc 1
2016-09-06T04:55:55.097521: step 2927, loss 0.0143503, acc 1
2016-09-06T04:55:55.937851: step 2928, loss 0.0729593, acc 0.96
2016-09-06T04:55:56.752618: step 2929, loss 0.0764194, acc 1
2016-09-06T04:55:57.558280: step 2930, loss 0.00359144, acc 1
2016-09-06T04:55:58.380440: step 2931, loss 0.0526018, acc 0.98
2016-09-06T04:55:59.183027: step 2932, loss 0.0285884, acc 1
2016-09-06T04:55:59.989007: step 2933, loss 0.0397279, acc 0.98
2016-09-06T04:56:00.801157: step 2934, loss 0.0300894, acc 0.98
2016-09-06T04:56:01.617829: step 2935, loss 0.0832153, acc 0.96
2016-09-06T04:56:02.478513: step 2936, loss 0.0248632, acc 0.98
2016-09-06T04:56:03.294842: step 2937, loss 0.00395614, acc 1
2016-09-06T04:56:04.097240: step 2938, loss 0.0767971, acc 0.92
2016-09-06T04:56:04.915996: step 2939, loss 0.0459694, acc 0.98
2016-09-06T04:56:05.756556: step 2940, loss 0.0465083, acc 0.98
2016-09-06T04:56:06.576236: step 2941, loss 0.0433936, acc 0.98
2016-09-06T04:56:07.416423: step 2942, loss 0.029527, acc 1
2016-09-06T04:56:08.226689: step 2943, loss 0.0531349, acc 0.98
2016-09-06T04:56:09.026445: step 2944, loss 0.0217486, acc 1
2016-09-06T04:56:09.840868: step 2945, loss 0.0507602, acc 0.96
2016-09-06T04:56:10.638158: step 2946, loss 0.0351888, acc 0.98
2016-09-06T04:56:11.456682: step 2947, loss 0.0131865, acc 1
2016-09-06T04:56:12.303713: step 2948, loss 0.0329812, acc 1
2016-09-06T04:56:13.146521: step 2949, loss 0.0366369, acc 0.98
2016-09-06T04:56:13.957588: step 2950, loss 0.0713203, acc 0.98
2016-09-06T04:56:14.777653: step 2951, loss 0.0137678, acc 1
2016-09-06T04:56:15.585136: step 2952, loss 0.00562988, acc 1
2016-09-06T04:56:16.422727: step 2953, loss 0.0179506, acc 1
2016-09-06T04:56:17.254691: step 2954, loss 0.0200668, acc 1
2016-09-06T04:56:18.072058: step 2955, loss 0.0360984, acc 0.98
2016-09-06T04:56:18.849599: step 2956, loss 0.0258309, acc 0.98
2016-09-06T04:56:19.662685: step 2957, loss 0.0173187, acc 1
2016-09-06T04:56:20.493603: step 2958, loss 0.0853293, acc 0.96
2016-09-06T04:56:21.272045: step 2959, loss 0.166807, acc 0.98
2016-09-06T04:56:22.072323: step 2960, loss 0.0693866, acc 0.96
2016-09-06T04:56:22.919635: step 2961, loss 0.0105965, acc 1
2016-09-06T04:56:23.697333: step 2962, loss 0.0564694, acc 0.98
2016-09-06T04:56:24.487751: step 2963, loss 0.0713188, acc 0.94
2016-09-06T04:56:25.314417: step 2964, loss 0.153534, acc 0.98
2016-09-06T04:56:26.100811: step 2965, loss 0.0512307, acc 0.98
2016-09-06T04:56:26.908725: step 2966, loss 0.0102577, acc 1
2016-09-06T04:56:27.739330: step 2967, loss 0.0224035, acc 1
2016-09-06T04:56:28.532058: step 2968, loss 0.0230112, acc 1
2016-09-06T04:56:29.328465: step 2969, loss 0.0978405, acc 0.96
2016-09-06T04:56:30.143062: step 2970, loss 0.110603, acc 0.96
2016-09-06T04:56:30.924002: step 2971, loss 0.0327732, acc 1
2016-09-06T04:56:31.718254: step 2972, loss 0.0131712, acc 1
2016-09-06T04:56:32.529072: step 2973, loss 0.0439338, acc 0.98
2016-09-06T04:56:33.339551: step 2974, loss 0.022423, acc 0.98
2016-09-06T04:56:34.179456: step 2975, loss 0.0411542, acc 1
2016-09-06T04:56:34.986237: step 2976, loss 0.0338887, acc 1
2016-09-06T04:56:35.756995: step 2977, loss 0.037145, acc 1
2016-09-06T04:56:36.545392: step 2978, loss 0.0245413, acc 0.98
2016-09-06T04:56:37.351800: step 2979, loss 0.0582191, acc 0.96
2016-09-06T04:56:38.159432: step 2980, loss 0.0636938, acc 0.96
2016-09-06T04:56:38.956573: step 2981, loss 0.0119728, acc 1
2016-09-06T04:56:39.769145: step 2982, loss 0.147665, acc 0.96
2016-09-06T04:56:40.551960: step 2983, loss 0.0246161, acc 1
2016-09-06T04:56:41.373742: step 2984, loss 0.164716, acc 0.96
2016-09-06T04:56:42.187253: step 2985, loss 0.0287867, acc 0.98
2016-09-06T04:56:42.961688: step 2986, loss 0.0537163, acc 0.98
2016-09-06T04:56:43.763445: step 2987, loss 0.0760711, acc 0.96
2016-09-06T04:56:44.567838: step 2988, loss 0.0238042, acc 1
2016-09-06T04:56:45.373568: step 2989, loss 0.0417492, acc 0.98
2016-09-06T04:56:46.185887: step 2990, loss 0.0594962, acc 0.96
2016-09-06T04:56:47.021350: step 2991, loss 0.0280208, acc 1
2016-09-06T04:56:47.822360: step 2992, loss 0.0513207, acc 0.98
2016-09-06T04:56:48.660656: step 2993, loss 0.111404, acc 0.94
2016-09-06T04:56:49.489528: step 2994, loss 0.091773, acc 0.96
2016-09-06T04:56:50.278752: step 2995, loss 0.0301338, acc 0.98
2016-09-06T04:56:51.101417: step 2996, loss 0.015735, acc 1
2016-09-06T04:56:51.931776: step 2997, loss 0.0188452, acc 1
2016-09-06T04:56:52.732804: step 2998, loss 0.00404715, acc 1
2016-09-06T04:56:53.552617: step 2999, loss 0.0803448, acc 0.98
2016-09-06T04:56:54.370407: step 3000, loss 0.0439786, acc 0.98

Evaluation:
2016-09-06T04:56:58.105349: step 3000, loss 1.40085, acc 0.757036

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-3000

2016-09-06T04:56:59.992616: step 3001, loss 0.0440944, acc 0.98
2016-09-06T04:57:00.819004: step 3002, loss 0.0559676, acc 0.98
2016-09-06T04:57:01.641434: step 3003, loss 0.016156, acc 1
2016-09-06T04:57:02.444238: step 3004, loss 0.0372082, acc 1
2016-09-06T04:57:03.282166: step 3005, loss 0.0262877, acc 0.98
2016-09-06T04:57:04.099230: step 3006, loss 0.0363405, acc 0.98
2016-09-06T04:57:04.909429: step 3007, loss 0.0519893, acc 0.98
2016-09-06T04:57:05.756987: step 3008, loss 0.0415676, acc 0.98
2016-09-06T04:57:06.581571: step 3009, loss 0.071105, acc 0.96
2016-09-06T04:57:07.402378: step 3010, loss 0.0524104, acc 0.96
2016-09-06T04:57:08.228070: step 3011, loss 0.042821, acc 0.98
2016-09-06T04:57:09.049074: step 3012, loss 0.0520848, acc 0.98
2016-09-06T04:57:09.851172: step 3013, loss 0.0372309, acc 0.98
2016-09-06T04:57:10.671082: step 3014, loss 0.0279354, acc 0.98
2016-09-06T04:57:11.496663: step 3015, loss 0.00848342, acc 1
2016-09-06T04:57:12.302123: step 3016, loss 0.0265149, acc 1
2016-09-06T04:57:13.143645: step 3017, loss 0.0328249, acc 0.98
2016-09-06T04:57:13.998488: step 3018, loss 0.0529312, acc 0.98
2016-09-06T04:57:14.826885: step 3019, loss 0.0330128, acc 0.98
2016-09-06T04:57:15.607020: step 3020, loss 0.0736181, acc 0.98
2016-09-06T04:57:16.441303: step 3021, loss 0.0366976, acc 1
2016-09-06T04:57:17.256582: step 3022, loss 0.0233733, acc 0.98
2016-09-06T04:57:18.027705: step 3023, loss 0.0609117, acc 0.98
2016-09-06T04:57:18.851931: step 3024, loss 0.0482847, acc 0.98
2016-09-06T04:57:19.680203: step 3025, loss 0.0439661, acc 0.96
2016-09-06T04:57:20.489385: step 3026, loss 0.161062, acc 0.94
2016-09-06T04:57:21.285651: step 3027, loss 0.0988393, acc 0.96
2016-09-06T04:57:22.102298: step 3028, loss 0.0414851, acc 1
2016-09-06T04:57:22.901280: step 3029, loss 0.0202399, acc 1
2016-09-06T04:57:23.716011: step 3030, loss 0.0324806, acc 1
2016-09-06T04:57:24.539003: step 3031, loss 0.0106043, acc 1
2016-09-06T04:57:25.321037: step 3032, loss 0.146267, acc 0.96
2016-09-06T04:57:26.122331: step 3033, loss 0.0247838, acc 1
2016-09-06T04:57:26.947833: step 3034, loss 0.0313278, acc 0.98
2016-09-06T04:57:27.751614: step 3035, loss 0.0636462, acc 0.98
2016-09-06T04:57:28.551160: step 3036, loss 0.0176205, acc 1
2016-09-06T04:57:29.368322: step 3037, loss 0.073208, acc 0.98
2016-09-06T04:57:30.162476: step 3038, loss 0.0338361, acc 1
2016-09-06T04:57:30.963308: step 3039, loss 0.0254555, acc 0.98
2016-09-06T04:57:31.808663: step 3040, loss 0.120181, acc 0.98
2016-09-06T04:57:32.584684: step 3041, loss 0.0377291, acc 0.98
2016-09-06T04:57:33.416877: step 3042, loss 0.062099, acc 0.98
2016-09-06T04:57:34.238591: step 3043, loss 0.0415468, acc 0.98
2016-09-06T04:57:35.032518: step 3044, loss 0.004946, acc 1
2016-09-06T04:57:35.811399: step 3045, loss 0.00522353, acc 1
2016-09-06T04:57:36.637401: step 3046, loss 0.0299081, acc 0.98
2016-09-06T04:57:37.406712: step 3047, loss 0.00790276, acc 1
2016-09-06T04:57:38.225287: step 3048, loss 0.024894, acc 1
2016-09-06T04:57:39.039660: step 3049, loss 0.0430762, acc 0.98
2016-09-06T04:57:39.846465: step 3050, loss 0.0167719, acc 1
2016-09-06T04:57:40.639924: step 3051, loss 0.0450187, acc 0.98
2016-09-06T04:57:41.478769: step 3052, loss 0.0505792, acc 0.98
2016-09-06T04:57:42.296911: step 3053, loss 0.0196721, acc 1
2016-09-06T04:57:43.097375: step 3054, loss 0.0369671, acc 0.98
2016-09-06T04:57:43.914014: step 3055, loss 0.0520356, acc 0.98
2016-09-06T04:57:44.714893: step 3056, loss 0.0212376, acc 1
2016-09-06T04:57:45.533702: step 3057, loss 0.102952, acc 0.98
2016-09-06T04:57:46.365332: step 3058, loss 0.0550978, acc 0.98
2016-09-06T04:57:47.156677: step 3059, loss 0.0328766, acc 0.98
2016-09-06T04:57:47.961569: step 3060, loss 0.0780972, acc 0.96
2016-09-06T04:57:48.788007: step 3061, loss 0.0186965, acc 1
2016-09-06T04:57:49.596499: step 3062, loss 0.062803, acc 0.96
2016-09-06T04:57:50.432849: step 3063, loss 0.0151852, acc 1
2016-09-06T04:57:51.248874: step 3064, loss 0.0532753, acc 0.98
2016-09-06T04:57:52.072652: step 3065, loss 0.0592897, acc 0.98
2016-09-06T04:57:52.967634: step 3066, loss 0.0352826, acc 0.98
2016-09-06T04:57:53.773840: step 3067, loss 0.115115, acc 0.98
2016-09-06T04:57:54.577477: step 3068, loss 0.012078, acc 1
2016-09-06T04:57:55.382444: step 3069, loss 0.0727605, acc 0.96
2016-09-06T04:57:56.222870: step 3070, loss 0.0403401, acc 0.98
2016-09-06T04:57:57.055148: step 3071, loss 0.065335, acc 0.96
2016-09-06T04:57:57.811571: step 3072, loss 0.00995628, acc 1
2016-09-06T04:57:58.649876: step 3073, loss 0.0369465, acc 0.98
2016-09-06T04:57:59.480416: step 3074, loss 0.0584053, acc 0.98
2016-09-06T04:58:00.288365: step 3075, loss 0.0734007, acc 0.96
2016-09-06T04:58:01.112484: step 3076, loss 0.0389012, acc 0.98
2016-09-06T04:58:01.921411: step 3077, loss 0.0835544, acc 0.98
2016-09-06T04:58:02.734362: step 3078, loss 0.0218402, acc 1
2016-09-06T04:58:03.556630: step 3079, loss 0.0162245, acc 1
2016-09-06T04:58:04.374060: step 3080, loss 0.0539271, acc 0.96
2016-09-06T04:58:05.186987: step 3081, loss 0.0721852, acc 0.96
2016-09-06T04:58:06.004566: step 3082, loss 0.0406557, acc 0.98
2016-09-06T04:58:06.820027: step 3083, loss 0.0448023, acc 0.98
2016-09-06T04:58:07.637694: step 3084, loss 0.0564042, acc 0.98
2016-09-06T04:58:08.437061: step 3085, loss 0.0274174, acc 1
2016-09-06T04:58:09.261583: step 3086, loss 0.022076, acc 1
2016-09-06T04:58:10.046488: step 3087, loss 0.166805, acc 0.96
2016-09-06T04:58:10.862973: step 3088, loss 0.0555899, acc 0.94
2016-09-06T04:58:11.716809: step 3089, loss 0.0463409, acc 0.98
2016-09-06T04:58:12.505053: step 3090, loss 0.0135873, acc 1
2016-09-06T04:58:13.298912: step 3091, loss 0.022353, acc 1
2016-09-06T04:58:14.116479: step 3092, loss 0.0152185, acc 1
2016-09-06T04:58:14.878843: step 3093, loss 0.0212383, acc 1
2016-09-06T04:58:15.692948: step 3094, loss 0.0126313, acc 1
2016-09-06T04:58:16.535672: step 3095, loss 0.0977681, acc 0.96
2016-09-06T04:58:17.329081: step 3096, loss 0.0114405, acc 1
2016-09-06T04:58:18.157444: step 3097, loss 0.0656102, acc 0.96
2016-09-06T04:58:18.994660: step 3098, loss 0.0373365, acc 1
2016-09-06T04:58:19.815948: step 3099, loss 0.0716523, acc 0.98
2016-09-06T04:58:20.658698: step 3100, loss 0.0228103, acc 1

Evaluation:
2016-09-06T04:58:24.430143: step 3100, loss 2.07387, acc 0.754221

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-3100

2016-09-06T04:58:26.299720: step 3101, loss 0.220707, acc 0.92
2016-09-06T04:58:27.111880: step 3102, loss 0.0420089, acc 1
2016-09-06T04:58:27.947431: step 3103, loss 0.0141926, acc 1
2016-09-06T04:58:28.762329: step 3104, loss 0.00815983, acc 1
2016-09-06T04:58:29.566312: step 3105, loss 0.0886498, acc 0.96
2016-09-06T04:58:30.371100: step 3106, loss 0.0242689, acc 0.98
2016-09-06T04:58:31.181490: step 3107, loss 0.00746994, acc 1
2016-09-06T04:58:31.979785: step 3108, loss 0.0899234, acc 0.98
2016-09-06T04:58:32.788911: step 3109, loss 0.010929, acc 1
2016-09-06T04:58:33.628420: step 3110, loss 0.0438227, acc 0.98
2016-09-06T04:58:34.448443: step 3111, loss 0.0923785, acc 0.98
2016-09-06T04:58:35.258897: step 3112, loss 0.0401324, acc 0.98
2016-09-06T04:58:36.092452: step 3113, loss 0.0307512, acc 0.98
2016-09-06T04:58:36.932666: step 3114, loss 0.0906386, acc 0.94
2016-09-06T04:58:37.756103: step 3115, loss 0.0796303, acc 0.98
2016-09-06T04:58:38.585348: step 3116, loss 0.0142959, acc 1
2016-09-06T04:58:39.421115: step 3117, loss 0.0906984, acc 0.96
2016-09-06T04:58:40.259216: step 3118, loss 0.0537548, acc 0.96
2016-09-06T04:58:41.090245: step 3119, loss 0.0558368, acc 0.96
2016-09-06T04:58:41.915350: step 3120, loss 0.0406305, acc 0.98
2016-09-06T04:58:42.706002: step 3121, loss 0.0644702, acc 0.96
2016-09-06T04:58:43.518605: step 3122, loss 0.0339595, acc 0.98
2016-09-06T04:58:44.336078: step 3123, loss 0.0281893, acc 0.98
2016-09-06T04:58:45.125815: step 3124, loss 0.0633176, acc 0.98
2016-09-06T04:58:45.946913: step 3125, loss 0.0836722, acc 0.98
2016-09-06T04:58:46.780149: step 3126, loss 0.0658361, acc 0.98
2016-09-06T04:58:47.598156: step 3127, loss 0.0664951, acc 0.98
2016-09-06T04:58:48.377483: step 3128, loss 0.0671126, acc 0.98
2016-09-06T04:58:49.205116: step 3129, loss 0.0381923, acc 0.96
2016-09-06T04:58:49.974384: step 3130, loss 0.0467207, acc 0.96
2016-09-06T04:58:50.776126: step 3131, loss 0.0525189, acc 0.96
2016-09-06T04:58:51.598353: step 3132, loss 0.108235, acc 0.96
2016-09-06T04:58:52.413670: step 3133, loss 0.162077, acc 0.92
2016-09-06T04:58:53.216329: step 3134, loss 0.0446164, acc 0.98
2016-09-06T04:58:54.058113: step 3135, loss 0.0985971, acc 0.98
2016-09-06T04:58:54.863060: step 3136, loss 0.0294132, acc 1
2016-09-06T04:58:55.662522: step 3137, loss 0.0602409, acc 0.94
2016-09-06T04:58:56.458621: step 3138, loss 0.0356299, acc 1
2016-09-06T04:58:57.240552: step 3139, loss 0.00591997, acc 1
2016-09-06T04:58:58.041479: step 3140, loss 0.160563, acc 0.96
2016-09-06T04:58:58.888829: step 3141, loss 0.0639374, acc 0.98
2016-09-06T04:58:59.691826: step 3142, loss 0.0913509, acc 0.96
2016-09-06T04:59:00.525827: step 3143, loss 0.0252405, acc 1
2016-09-06T04:59:01.350419: step 3144, loss 0.041832, acc 0.98
2016-09-06T04:59:02.142772: step 3145, loss 0.206288, acc 0.94
2016-09-06T04:59:02.952980: step 3146, loss 0.0190172, acc 1
2016-09-06T04:59:03.800845: step 3147, loss 0.161256, acc 0.96
2016-09-06T04:59:04.634369: step 3148, loss 0.0399776, acc 0.96
2016-09-06T04:59:05.427649: step 3149, loss 0.0513902, acc 0.96
2016-09-06T04:59:06.268057: step 3150, loss 0.0484316, acc 0.98
2016-09-06T04:59:07.098161: step 3151, loss 0.0811562, acc 0.96
2016-09-06T04:59:07.912440: step 3152, loss 0.0131343, acc 1
2016-09-06T04:59:08.744390: step 3153, loss 0.00711382, acc 1
2016-09-06T04:59:09.565946: step 3154, loss 0.0361143, acc 0.98
2016-09-06T04:59:10.380887: step 3155, loss 0.114987, acc 0.94
2016-09-06T04:59:11.208371: step 3156, loss 0.0409885, acc 0.98
2016-09-06T04:59:12.030460: step 3157, loss 0.0668437, acc 0.96
2016-09-06T04:59:12.834891: step 3158, loss 0.0277994, acc 1
2016-09-06T04:59:13.671382: step 3159, loss 0.039963, acc 0.98
2016-09-06T04:59:14.483853: step 3160, loss 0.0283333, acc 1
2016-09-06T04:59:15.313816: step 3161, loss 0.0251334, acc 1
2016-09-06T04:59:16.134907: step 3162, loss 0.0560504, acc 0.98
2016-09-06T04:59:16.917447: step 3163, loss 0.198562, acc 0.98
2016-09-06T04:59:17.725418: step 3164, loss 0.0129836, acc 1
2016-09-06T04:59:18.560874: step 3165, loss 0.0349127, acc 0.98
2016-09-06T04:59:19.384845: step 3166, loss 0.0130877, acc 1
2016-09-06T04:59:20.186914: step 3167, loss 0.0676027, acc 0.96
2016-09-06T04:59:21.028846: step 3168, loss 0.0283604, acc 0.98
2016-09-06T04:59:21.864618: step 3169, loss 0.0238152, acc 1
2016-09-06T04:59:22.666460: step 3170, loss 0.0602265, acc 1
2016-09-06T04:59:23.473644: step 3171, loss 0.0539934, acc 0.98
2016-09-06T04:59:24.289896: step 3172, loss 0.0723184, acc 0.96
2016-09-06T04:59:25.056639: step 3173, loss 0.0302394, acc 1
2016-09-06T04:59:25.826415: step 3174, loss 0.104696, acc 0.98
2016-09-06T04:59:26.633058: step 3175, loss 0.00521708, acc 1
2016-09-06T04:59:27.452361: step 3176, loss 0.00746276, acc 1
2016-09-06T04:59:28.253615: step 3177, loss 0.0192537, acc 1
2016-09-06T04:59:29.064241: step 3178, loss 0.114361, acc 0.94
2016-09-06T04:59:29.925638: step 3179, loss 0.0601301, acc 0.96
2016-09-06T04:59:30.733965: step 3180, loss 0.0631177, acc 0.96
2016-09-06T04:59:31.579734: step 3181, loss 0.00972818, acc 1
2016-09-06T04:59:32.376610: step 3182, loss 0.072586, acc 0.96
2016-09-06T04:59:33.187826: step 3183, loss 0.0657377, acc 0.98
2016-09-06T04:59:34.047182: step 3184, loss 0.120263, acc 0.94
2016-09-06T04:59:34.848774: step 3185, loss 0.0192083, acc 1
2016-09-06T04:59:35.627330: step 3186, loss 0.0832784, acc 0.96
2016-09-06T04:59:36.472456: step 3187, loss 0.0304583, acc 0.98
2016-09-06T04:59:37.296578: step 3188, loss 0.0139176, acc 1
2016-09-06T04:59:38.120530: step 3189, loss 0.0418551, acc 1
2016-09-06T04:59:38.950934: step 3190, loss 0.0191471, acc 1
2016-09-06T04:59:39.754410: step 3191, loss 0.0639896, acc 0.96
2016-09-06T04:59:40.569364: step 3192, loss 0.0555424, acc 0.98
2016-09-06T04:59:41.401349: step 3193, loss 0.0167502, acc 1
2016-09-06T04:59:42.188989: step 3194, loss 0.0075678, acc 1
2016-09-06T04:59:43.010154: step 3195, loss 0.0516971, acc 0.98
2016-09-06T04:59:43.831601: step 3196, loss 0.154727, acc 0.94
2016-09-06T04:59:44.619317: step 3197, loss 0.100982, acc 0.92
2016-09-06T04:59:45.404456: step 3198, loss 0.0221675, acc 1
2016-09-06T04:59:46.236106: step 3199, loss 0.0467945, acc 1
2016-09-06T04:59:47.021339: step 3200, loss 0.0424077, acc 0.98

Evaluation:
2016-09-06T04:59:50.740750: step 3200, loss 1.47169, acc 0.748593

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-3200

2016-09-06T04:59:52.571192: step 3201, loss 0.113135, acc 0.96
2016-09-06T04:59:53.383610: step 3202, loss 0.0600752, acc 0.98
2016-09-06T04:59:54.205996: step 3203, loss 0.0460285, acc 0.98
2016-09-06T04:59:55.023251: step 3204, loss 0.0308228, acc 0.98
2016-09-06T04:59:55.858537: step 3205, loss 0.236907, acc 0.9
2016-09-06T04:59:56.666415: step 3206, loss 0.0904225, acc 0.96
2016-09-06T04:59:57.478222: step 3207, loss 0.0119815, acc 1
2016-09-06T04:59:58.315935: step 3208, loss 0.0594842, acc 0.96
2016-09-06T04:59:59.107081: step 3209, loss 0.0303082, acc 0.98
2016-09-06T04:59:59.930464: step 3210, loss 0.0707408, acc 0.96
2016-09-06T05:00:00.791671: step 3211, loss 0.00677816, acc 1
2016-09-06T05:00:01.605564: step 3212, loss 0.0296234, acc 0.98
2016-09-06T05:00:02.409550: step 3213, loss 0.131919, acc 0.94
2016-09-06T05:00:03.252018: step 3214, loss 0.0339619, acc 0.98
2016-09-06T05:00:04.073980: step 3215, loss 0.0255189, acc 0.98
2016-09-06T05:00:04.873664: step 3216, loss 0.0175175, acc 1
2016-09-06T05:00:05.711071: step 3217, loss 0.090842, acc 0.96
2016-09-06T05:00:06.579485: step 3218, loss 0.0294587, acc 1
2016-09-06T05:00:07.352647: step 3219, loss 0.0818526, acc 0.94
2016-09-06T05:00:08.142103: step 3220, loss 0.00519636, acc 1
2016-09-06T05:00:08.993926: step 3221, loss 0.0167216, acc 1
2016-09-06T05:00:09.789603: step 3222, loss 0.0635337, acc 0.94
2016-09-06T05:00:10.643701: step 3223, loss 0.0391151, acc 1
2016-09-06T05:00:11.480588: step 3224, loss 0.0267363, acc 0.98
2016-09-06T05:00:12.283463: step 3225, loss 0.0208387, acc 1
2016-09-06T05:00:13.101871: step 3226, loss 0.0105468, acc 1
2016-09-06T05:00:13.923712: step 3227, loss 0.0262684, acc 1
2016-09-06T05:00:14.742332: step 3228, loss 0.0277786, acc 1
2016-09-06T05:00:15.558579: step 3229, loss 0.0321331, acc 0.98
2016-09-06T05:00:16.401184: step 3230, loss 0.00879152, acc 1
2016-09-06T05:00:17.250557: step 3231, loss 0.0631311, acc 0.98
2016-09-06T05:00:18.053677: step 3232, loss 0.157479, acc 0.92
2016-09-06T05:00:18.893515: step 3233, loss 0.114586, acc 0.92
2016-09-06T05:00:19.705066: step 3234, loss 0.0855385, acc 0.98
2016-09-06T05:00:20.499436: step 3235, loss 0.0319166, acc 0.98
2016-09-06T05:00:21.339658: step 3236, loss 0.111441, acc 0.94
2016-09-06T05:00:22.163216: step 3237, loss 0.0323546, acc 0.98
2016-09-06T05:00:22.969718: step 3238, loss 0.0503744, acc 0.98
2016-09-06T05:00:23.808950: step 3239, loss 0.125597, acc 0.96
2016-09-06T05:00:24.629364: step 3240, loss 0.0376994, acc 0.98
2016-09-06T05:00:25.429843: step 3241, loss 0.189707, acc 0.94
2016-09-06T05:00:26.243483: step 3242, loss 0.0198047, acc 1
2016-09-06T05:00:27.076842: step 3243, loss 0.0772553, acc 0.94
2016-09-06T05:00:27.903507: step 3244, loss 0.048911, acc 0.98
2016-09-06T05:00:28.737396: step 3245, loss 0.050565, acc 0.96
2016-09-06T05:00:29.583120: step 3246, loss 0.0293139, acc 0.98
2016-09-06T05:00:30.385107: step 3247, loss 0.0696553, acc 0.94
2016-09-06T05:00:31.207301: step 3248, loss 0.0810999, acc 0.98
2016-09-06T05:00:32.011219: step 3249, loss 0.0443994, acc 1
2016-09-06T05:00:32.827166: step 3250, loss 0.142727, acc 0.92
2016-09-06T05:00:33.665365: step 3251, loss 0.0323322, acc 0.98
2016-09-06T05:00:34.450091: step 3252, loss 0.0414755, acc 0.98
2016-09-06T05:00:35.304749: step 3253, loss 0.0402403, acc 1
2016-09-06T05:00:36.125948: step 3254, loss 0.0471141, acc 0.98
2016-09-06T05:00:36.945949: step 3255, loss 0.0403729, acc 0.98
2016-09-06T05:00:37.776904: step 3256, loss 0.0635026, acc 0.98
2016-09-06T05:00:38.606164: step 3257, loss 0.054871, acc 0.98
2016-09-06T05:00:39.440502: step 3258, loss 0.0282297, acc 0.98
2016-09-06T05:00:40.258684: step 3259, loss 0.147376, acc 0.94
2016-09-06T05:00:41.071441: step 3260, loss 0.0878571, acc 0.98
2016-09-06T05:00:41.934658: step 3261, loss 0.0568749, acc 0.96
2016-09-06T05:00:42.761491: step 3262, loss 0.0436866, acc 0.98
2016-09-06T05:00:43.581114: step 3263, loss 0.0165421, acc 1
2016-09-06T05:00:44.342435: step 3264, loss 0.144184, acc 0.931818
2016-09-06T05:00:45.159127: step 3265, loss 0.0168018, acc 1
2016-09-06T05:00:45.970409: step 3266, loss 0.0455769, acc 0.96
2016-09-06T05:00:46.818544: step 3267, loss 0.00966344, acc 1
2016-09-06T05:00:47.669263: step 3268, loss 0.028521, acc 0.98
2016-09-06T05:00:48.518911: step 3269, loss 0.0225785, acc 1
2016-09-06T05:00:49.320678: step 3270, loss 0.0417934, acc 0.98
2016-09-06T05:00:50.134022: step 3271, loss 0.0286064, acc 1
2016-09-06T05:00:50.928062: step 3272, loss 0.0424462, acc 1
2016-09-06T05:00:51.736313: step 3273, loss 0.0755146, acc 0.94
2016-09-06T05:00:52.559807: step 3274, loss 0.0780041, acc 0.96
2016-09-06T05:00:53.395975: step 3275, loss 0.0320887, acc 1
2016-09-06T05:00:54.201586: step 3276, loss 0.0450827, acc 0.98
2016-09-06T05:00:55.009750: step 3277, loss 0.0360568, acc 1
2016-09-06T05:00:55.826154: step 3278, loss 0.0457915, acc 0.98
2016-09-06T05:00:56.633861: step 3279, loss 0.169853, acc 0.96
2016-09-06T05:00:57.459474: step 3280, loss 0.0238574, acc 0.98
2016-09-06T05:00:58.266726: step 3281, loss 0.029976, acc 0.98
2016-09-06T05:00:59.056719: step 3282, loss 0.0218338, acc 1
2016-09-06T05:00:59.888576: step 3283, loss 0.0456812, acc 0.96
2016-09-06T05:01:00.691585: step 3284, loss 0.00459181, acc 1
2016-09-06T05:01:01.487434: step 3285, loss 0.00490945, acc 1
2016-09-06T05:01:02.321196: step 3286, loss 0.0470361, acc 0.98
2016-09-06T05:01:03.122269: step 3287, loss 0.0261578, acc 1
2016-09-06T05:01:03.905443: step 3288, loss 0.113577, acc 0.98
2016-09-06T05:01:04.746062: step 3289, loss 0.0240222, acc 1
2016-09-06T05:01:05.526977: step 3290, loss 0.0771341, acc 0.98
2016-09-06T05:01:06.359129: step 3291, loss 0.0110721, acc 1
2016-09-06T05:01:07.174077: step 3292, loss 0.032872, acc 1
2016-09-06T05:01:08.015906: step 3293, loss 0.0650613, acc 0.98
2016-09-06T05:01:08.839450: step 3294, loss 0.0366178, acc 0.96
2016-09-06T05:01:09.671225: step 3295, loss 0.0342716, acc 0.98
2016-09-06T05:01:10.500609: step 3296, loss 0.0184845, acc 1
2016-09-06T05:01:11.353730: step 3297, loss 0.0766572, acc 0.98
2016-09-06T05:01:12.191357: step 3298, loss 0.0230268, acc 1
2016-09-06T05:01:13.019098: step 3299, loss 0.0483541, acc 0.98
2016-09-06T05:01:13.852228: step 3300, loss 0.0279124, acc 0.98

Evaluation:
2016-09-06T05:01:17.570064: step 3300, loss 1.55781, acc 0.751407

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-3300

2016-09-06T05:01:19.530482: step 3301, loss 0.018373, acc 1
2016-09-06T05:01:20.338695: step 3302, loss 0.0354119, acc 0.98
2016-09-06T05:01:21.142288: step 3303, loss 0.154577, acc 0.96
2016-09-06T05:01:21.961580: step 3304, loss 0.00563062, acc 1
2016-09-06T05:01:22.819972: step 3305, loss 0.0257099, acc 0.98
2016-09-06T05:01:23.613935: step 3306, loss 0.0168855, acc 1
2016-09-06T05:01:24.417162: step 3307, loss 0.0675508, acc 0.98
2016-09-06T05:01:25.259985: step 3308, loss 0.0498331, acc 0.96
2016-09-06T05:01:26.054293: step 3309, loss 0.0172758, acc 1
2016-09-06T05:01:26.850653: step 3310, loss 0.0139501, acc 1
2016-09-06T05:01:27.677582: step 3311, loss 0.037389, acc 0.98
2016-09-06T05:01:28.482650: step 3312, loss 0.126048, acc 0.9
2016-09-06T05:01:29.312458: step 3313, loss 0.0145822, acc 1
2016-09-06T05:01:30.170785: step 3314, loss 0.0980768, acc 0.98
2016-09-06T05:01:30.979517: step 3315, loss 0.0246974, acc 0.98
2016-09-06T05:01:31.799199: step 3316, loss 0.141678, acc 0.96
2016-09-06T05:01:32.612050: step 3317, loss 0.0375828, acc 0.98
2016-09-06T05:01:33.450174: step 3318, loss 0.0409694, acc 0.98
2016-09-06T05:01:34.256709: step 3319, loss 0.118436, acc 0.96
2016-09-06T05:01:35.118867: step 3320, loss 0.0137734, acc 1
2016-09-06T05:01:35.923192: step 3321, loss 0.0168953, acc 1
2016-09-06T05:01:36.732646: step 3322, loss 0.0443676, acc 0.98
2016-09-06T05:01:37.545165: step 3323, loss 0.0142068, acc 1
2016-09-06T05:01:38.350266: step 3324, loss 0.0419843, acc 0.98
2016-09-06T05:01:39.164015: step 3325, loss 0.059515, acc 0.98
2016-09-06T05:01:40.005527: step 3326, loss 0.0464796, acc 0.98
2016-09-06T05:01:40.803006: step 3327, loss 0.0399891, acc 0.98
2016-09-06T05:01:41.652532: step 3328, loss 0.0252067, acc 1
2016-09-06T05:01:42.509101: step 3329, loss 0.065164, acc 0.96
2016-09-06T05:01:43.332943: step 3330, loss 0.0639251, acc 0.98
2016-09-06T05:01:44.104048: step 3331, loss 0.0779125, acc 0.96
2016-09-06T05:01:44.907814: step 3332, loss 0.0618705, acc 0.98
2016-09-06T05:01:45.739397: step 3333, loss 0.0412824, acc 0.98
2016-09-06T05:01:46.544660: step 3334, loss 0.0829909, acc 0.96
2016-09-06T05:01:47.369809: step 3335, loss 0.00700454, acc 1
2016-09-06T05:01:48.185944: step 3336, loss 0.0624314, acc 0.96
2016-09-06T05:01:48.966905: step 3337, loss 0.0243896, acc 1
2016-09-06T05:01:49.787150: step 3338, loss 0.0745239, acc 0.98
2016-09-06T05:01:50.609254: step 3339, loss 0.0315619, acc 1
2016-09-06T05:01:51.417452: step 3340, loss 0.0552311, acc 0.96
2016-09-06T05:01:52.224696: step 3341, loss 0.0576188, acc 0.98
2016-09-06T05:01:53.054679: step 3342, loss 0.0403339, acc 1
2016-09-06T05:01:53.871067: step 3343, loss 0.0229944, acc 1
2016-09-06T05:01:54.726211: step 3344, loss 0.0217559, acc 1
2016-09-06T05:01:55.561765: step 3345, loss 0.00543564, acc 1
2016-09-06T05:01:56.370132: step 3346, loss 0.0172237, acc 1
2016-09-06T05:01:57.210075: step 3347, loss 0.105731, acc 0.92
2016-09-06T05:01:58.058531: step 3348, loss 0.0223472, acc 0.98
2016-09-06T05:01:58.873390: step 3349, loss 0.0441258, acc 0.98
2016-09-06T05:01:59.680382: step 3350, loss 0.0799699, acc 0.98
2016-09-06T05:02:00.511809: step 3351, loss 0.00664642, acc 1
2016-09-06T05:02:01.365463: step 3352, loss 0.0630504, acc 0.98
2016-09-06T05:02:02.207887: step 3353, loss 0.017943, acc 1
2016-09-06T05:02:03.043996: step 3354, loss 0.00844928, acc 1
2016-09-06T05:02:03.843015: step 3355, loss 0.064562, acc 0.96
2016-09-06T05:02:04.627037: step 3356, loss 0.00398488, acc 1
2016-09-06T05:02:05.446339: step 3357, loss 0.028663, acc 0.98
2016-09-06T05:02:06.246068: step 3358, loss 0.154898, acc 0.96
2016-09-06T05:02:07.046581: step 3359, loss 0.0270987, acc 0.98
2016-09-06T05:02:07.882143: step 3360, loss 0.00642544, acc 1
2016-09-06T05:02:08.693523: step 3361, loss 0.00424208, acc 1
2016-09-06T05:02:09.491621: step 3362, loss 0.0779891, acc 0.94
2016-09-06T05:02:10.339151: step 3363, loss 0.00849524, acc 1
2016-09-06T05:02:11.135497: step 3364, loss 0.0664446, acc 0.98
2016-09-06T05:02:11.906219: step 3365, loss 0.0728622, acc 0.96
2016-09-06T05:02:12.710376: step 3366, loss 0.0236739, acc 0.98
2016-09-06T05:02:13.519304: step 3367, loss 0.0123931, acc 1
2016-09-06T05:02:14.310218: step 3368, loss 0.0216721, acc 0.98
2016-09-06T05:02:15.094066: step 3369, loss 0.0342653, acc 1
2016-09-06T05:02:15.874767: step 3370, loss 0.0131686, acc 1
2016-09-06T05:02:16.707406: step 3371, loss 0.0352818, acc 1
2016-09-06T05:02:17.555431: step 3372, loss 0.0420866, acc 1
2016-09-06T05:02:18.394877: step 3373, loss 0.0737249, acc 0.98
2016-09-06T05:02:19.194445: step 3374, loss 0.0438829, acc 0.96
2016-09-06T05:02:19.998528: step 3375, loss 0.00776177, acc 1
2016-09-06T05:02:20.828749: step 3376, loss 0.0629444, acc 0.94
2016-09-06T05:02:21.632665: step 3377, loss 0.0278616, acc 0.98
2016-09-06T05:02:22.424539: step 3378, loss 0.0327632, acc 0.98
2016-09-06T05:02:23.229141: step 3379, loss 0.0759175, acc 0.98
2016-09-06T05:02:24.011382: step 3380, loss 0.0395801, acc 0.98
2016-09-06T05:02:24.790715: step 3381, loss 0.0199187, acc 1
2016-09-06T05:02:25.599172: step 3382, loss 0.00997305, acc 1
2016-09-06T05:02:26.390783: step 3383, loss 0.149194, acc 0.96
2016-09-06T05:02:27.234875: step 3384, loss 0.0418873, acc 0.98
2016-09-06T05:02:28.041500: step 3385, loss 0.0212222, acc 1
2016-09-06T05:02:28.845892: step 3386, loss 0.0183879, acc 0.98
2016-09-06T05:02:29.651357: step 3387, loss 0.119243, acc 0.96
2016-09-06T05:02:30.444580: step 3388, loss 0.0293906, acc 1
2016-09-06T05:02:31.216175: step 3389, loss 0.0178103, acc 1
2016-09-06T05:02:32.002670: step 3390, loss 0.0196364, acc 0.98
2016-09-06T05:02:32.817985: step 3391, loss 0.03344, acc 0.96
2016-09-06T05:02:33.597011: step 3392, loss 0.0274723, acc 0.98
2016-09-06T05:02:34.425654: step 3393, loss 0.0133083, acc 1
2016-09-06T05:02:35.286871: step 3394, loss 0.0318337, acc 1
2016-09-06T05:02:36.046054: step 3395, loss 0.00618694, acc 1
2016-09-06T05:02:36.850239: step 3396, loss 0.0372713, acc 0.98
2016-09-06T05:02:37.661775: step 3397, loss 0.0437618, acc 0.98
2016-09-06T05:02:38.451229: step 3398, loss 0.0509325, acc 0.96
2016-09-06T05:02:39.275491: step 3399, loss 0.104059, acc 0.94
2016-09-06T05:02:40.089585: step 3400, loss 0.0128621, acc 1

Evaluation:
2016-09-06T05:02:43.823000: step 3400, loss 1.89929, acc 0.760788

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-3400

2016-09-06T05:02:45.728158: step 3401, loss 0.00986389, acc 1
2016-09-06T05:02:46.536734: step 3402, loss 0.0323817, acc 1
2016-09-06T05:02:47.364952: step 3403, loss 0.0126869, acc 1
2016-09-06T05:02:48.191735: step 3404, loss 0.00461684, acc 1
2016-09-06T05:02:49.034559: step 3405, loss 0.03545, acc 0.98
2016-09-06T05:02:49.832195: step 3406, loss 0.00600893, acc 1
2016-09-06T05:02:50.655665: step 3407, loss 0.0252908, acc 0.98
2016-09-06T05:02:51.479833: step 3408, loss 0.0206608, acc 1
2016-09-06T05:02:52.310742: step 3409, loss 0.272178, acc 0.96
2016-09-06T05:02:53.096721: step 3410, loss 0.0241541, acc 1
2016-09-06T05:02:53.896704: step 3411, loss 0.0857318, acc 0.92
2016-09-06T05:02:54.734209: step 3412, loss 0.172182, acc 0.94
2016-09-06T05:02:55.534873: step 3413, loss 0.0127924, acc 1
2016-09-06T05:02:56.326448: step 3414, loss 0.0830481, acc 0.98
2016-09-06T05:02:57.146552: step 3415, loss 0.160739, acc 0.98
2016-09-06T05:02:57.942226: step 3416, loss 0.0147873, acc 1
2016-09-06T05:02:58.761665: step 3417, loss 0.10087, acc 0.96
2016-09-06T05:02:59.562864: step 3418, loss 0.0673491, acc 0.96
2016-09-06T05:03:00.354980: step 3419, loss 0.0437621, acc 0.96
2016-09-06T05:03:01.146712: step 3420, loss 0.115371, acc 0.96
2016-09-06T05:03:01.973790: step 3421, loss 0.0961226, acc 0.98
2016-09-06T05:03:02.757556: step 3422, loss 0.0469269, acc 0.98
2016-09-06T05:03:03.560818: step 3423, loss 0.0352958, acc 0.98
2016-09-06T05:03:04.371180: step 3424, loss 0.058861, acc 0.98
2016-09-06T05:03:05.152795: step 3425, loss 0.115545, acc 0.94
2016-09-06T05:03:05.973379: step 3426, loss 0.0469249, acc 0.98
2016-09-06T05:03:06.785612: step 3427, loss 0.083936, acc 0.94
2016-09-06T05:03:07.583970: step 3428, loss 0.0289489, acc 0.98
2016-09-06T05:03:08.392698: step 3429, loss 0.103035, acc 0.96
2016-09-06T05:03:09.250030: step 3430, loss 0.0344859, acc 0.98
2016-09-06T05:03:10.061404: step 3431, loss 0.0571318, acc 0.98
2016-09-06T05:03:10.873414: step 3432, loss 0.0512506, acc 0.98
2016-09-06T05:03:11.719230: step 3433, loss 0.0646361, acc 0.96
2016-09-06T05:03:12.503613: step 3434, loss 0.0742971, acc 0.98
2016-09-06T05:03:13.330776: step 3435, loss 0.118385, acc 0.92
2016-09-06T05:03:14.170100: step 3436, loss 0.0660419, acc 0.96
2016-09-06T05:03:14.989955: step 3437, loss 0.0413815, acc 0.98
2016-09-06T05:03:15.781675: step 3438, loss 0.0351085, acc 0.98
2016-09-06T05:03:16.633556: step 3439, loss 0.0481918, acc 1
2016-09-06T05:03:17.465062: step 3440, loss 0.0596514, acc 0.98
2016-09-06T05:03:18.308693: step 3441, loss 0.0420729, acc 1
2016-09-06T05:03:19.170731: step 3442, loss 0.110046, acc 0.96
2016-09-06T05:03:19.977600: step 3443, loss 0.086966, acc 0.96
2016-09-06T05:03:20.774740: step 3444, loss 0.0632781, acc 0.98
2016-09-06T05:03:21.591829: step 3445, loss 0.0314024, acc 1
2016-09-06T05:03:22.397059: step 3446, loss 0.0627327, acc 0.98
2016-09-06T05:03:23.218483: step 3447, loss 0.00656471, acc 1
2016-09-06T05:03:24.036443: step 3448, loss 0.0309349, acc 0.98
2016-09-06T05:03:24.870479: step 3449, loss 0.0592883, acc 0.96
2016-09-06T05:03:25.668757: step 3450, loss 0.0549896, acc 0.98
2016-09-06T05:03:26.524773: step 3451, loss 0.0314908, acc 1
2016-09-06T05:03:27.360649: step 3452, loss 0.0261522, acc 0.98
2016-09-06T05:03:28.162095: step 3453, loss 0.0563842, acc 0.98
2016-09-06T05:03:28.955566: step 3454, loss 0.0190427, acc 1
2016-09-06T05:03:29.798939: step 3455, loss 0.0106025, acc 1
2016-09-06T05:03:30.514036: step 3456, loss 0.0316143, acc 1
2016-09-06T05:03:31.349221: step 3457, loss 0.0476478, acc 1
2016-09-06T05:03:32.165732: step 3458, loss 0.0797774, acc 0.92
2016-09-06T05:03:32.953252: step 3459, loss 0.029718, acc 0.98
2016-09-06T05:03:33.753058: step 3460, loss 0.039601, acc 1
2016-09-06T05:03:34.569441: step 3461, loss 0.0278232, acc 1
2016-09-06T05:03:35.369032: step 3462, loss 0.0947983, acc 0.96
2016-09-06T05:03:36.202753: step 3463, loss 0.0775511, acc 0.96
2016-09-06T05:03:37.020604: step 3464, loss 0.0377572, acc 0.96
2016-09-06T05:03:37.808081: step 3465, loss 0.0159359, acc 1
2016-09-06T05:03:38.628884: step 3466, loss 0.0212769, acc 1
2016-09-06T05:03:39.441214: step 3467, loss 0.0156235, acc 1
2016-09-06T05:03:40.215652: step 3468, loss 0.0327654, acc 0.98
2016-09-06T05:03:41.042854: step 3469, loss 0.039325, acc 0.96
2016-09-06T05:03:41.855481: step 3470, loss 0.0167413, acc 1
2016-09-06T05:03:42.652871: step 3471, loss 0.0563431, acc 0.96
2016-09-06T05:03:43.469084: step 3472, loss 0.0120303, acc 1
2016-09-06T05:03:44.279007: step 3473, loss 0.00601447, acc 1
2016-09-06T05:03:45.057612: step 3474, loss 0.00798108, acc 1
2016-09-06T05:03:45.873961: step 3475, loss 0.00849938, acc 1
2016-09-06T05:03:46.693139: step 3476, loss 0.0125807, acc 1
2016-09-06T05:03:47.518621: step 3477, loss 0.0126214, acc 1
2016-09-06T05:03:48.344773: step 3478, loss 0.0162744, acc 1
2016-09-06T05:03:49.177582: step 3479, loss 0.019904, acc 0.98
2016-09-06T05:03:49.953967: step 3480, loss 0.0425264, acc 0.98
2016-09-06T05:03:50.793686: step 3481, loss 0.0577938, acc 0.98
2016-09-06T05:03:51.618723: step 3482, loss 0.0734504, acc 0.98
2016-09-06T05:03:52.407920: step 3483, loss 0.0253516, acc 1
2016-09-06T05:03:53.194601: step 3484, loss 0.0335815, acc 0.98
2016-09-06T05:03:53.991332: step 3485, loss 0.0461804, acc 0.98
2016-09-06T05:03:54.795140: step 3486, loss 0.0109551, acc 1
2016-09-06T05:03:55.595947: step 3487, loss 0.00611096, acc 1
2016-09-06T05:03:56.438966: step 3488, loss 0.0286329, acc 1
2016-09-06T05:03:57.231199: step 3489, loss 0.107858, acc 0.96
2016-09-06T05:03:58.009453: step 3490, loss 0.0351479, acc 0.98
2016-09-06T05:03:58.848615: step 3491, loss 0.0399426, acc 0.98
2016-09-06T05:03:59.615080: step 3492, loss 0.0603063, acc 0.96
2016-09-06T05:04:00.455456: step 3493, loss 0.0599088, acc 0.98
2016-09-06T05:04:01.275097: step 3494, loss 0.0682537, acc 0.98
2016-09-06T05:04:02.040040: step 3495, loss 0.0205279, acc 1
2016-09-06T05:04:02.845004: step 3496, loss 0.147858, acc 0.96
2016-09-06T05:04:03.653506: step 3497, loss 0.0185844, acc 1
2016-09-06T05:04:04.479795: step 3498, loss 0.0446198, acc 0.98
2016-09-06T05:04:05.258580: step 3499, loss 0.0629698, acc 0.98
2016-09-06T05:04:06.069773: step 3500, loss 0.00471238, acc 1

Evaluation:
2016-09-06T05:04:09.801381: step 3500, loss 1.80264, acc 0.742026

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-3500

2016-09-06T05:04:11.586488: step 3501, loss 0.0351619, acc 0.98
2016-09-06T05:04:12.419722: step 3502, loss 0.0707355, acc 0.94
2016-09-06T05:04:13.220229: step 3503, loss 0.0134834, acc 1
2016-09-06T05:04:14.057827: step 3504, loss 0.02665, acc 1
2016-09-06T05:04:14.882653: step 3505, loss 0.0461457, acc 0.98
2016-09-06T05:04:15.708126: step 3506, loss 0.0145255, acc 1
2016-09-06T05:04:16.536234: step 3507, loss 0.0698886, acc 0.96
2016-09-06T05:04:17.340727: step 3508, loss 0.0401555, acc 0.98
2016-09-06T05:04:18.156376: step 3509, loss 0.0535849, acc 0.96
2016-09-06T05:04:18.953260: step 3510, loss 0.0150923, acc 1
2016-09-06T05:04:19.742625: step 3511, loss 0.014914, acc 1
2016-09-06T05:04:20.556501: step 3512, loss 0.0540213, acc 0.98
2016-09-06T05:04:21.341442: step 3513, loss 0.0138554, acc 1
2016-09-06T05:04:22.140813: step 3514, loss 0.130374, acc 0.92
2016-09-06T05:04:22.967989: step 3515, loss 0.0288205, acc 0.98
2016-09-06T05:04:23.747030: step 3516, loss 0.0257334, acc 1
2016-09-06T05:04:24.582747: step 3517, loss 0.0962297, acc 0.94
2016-09-06T05:04:25.379510: step 3518, loss 0.0666108, acc 0.96
2016-09-06T05:04:26.196815: step 3519, loss 0.0292117, acc 0.98
2016-09-06T05:04:26.986899: step 3520, loss 0.00934943, acc 1
2016-09-06T05:04:27.812666: step 3521, loss 0.0234394, acc 0.98
2016-09-06T05:04:28.593798: step 3522, loss 0.029732, acc 0.98
2016-09-06T05:04:29.418972: step 3523, loss 0.0281987, acc 0.98
2016-09-06T05:04:30.239459: step 3524, loss 0.0586906, acc 0.98
2016-09-06T05:04:31.032965: step 3525, loss 0.071863, acc 0.96
2016-09-06T05:04:31.851777: step 3526, loss 0.00404363, acc 1
2016-09-06T05:04:32.693127: step 3527, loss 0.00545771, acc 1
2016-09-06T05:04:33.508797: step 3528, loss 0.115472, acc 0.96
2016-09-06T05:04:34.324923: step 3529, loss 0.107591, acc 0.92
2016-09-06T05:04:35.151167: step 3530, loss 0.015016, acc 1
2016-09-06T05:04:35.953999: step 3531, loss 0.0651708, acc 0.96
2016-09-06T05:04:36.762401: step 3532, loss 0.041824, acc 0.96
2016-09-06T05:04:37.614654: step 3533, loss 0.0156597, acc 1
2016-09-06T05:04:38.392473: step 3534, loss 0.0482343, acc 0.98
2016-09-06T05:04:39.220085: step 3535, loss 0.0882166, acc 0.94
2016-09-06T05:04:40.054163: step 3536, loss 0.0269157, acc 0.98
2016-09-06T05:04:40.872488: step 3537, loss 0.0277748, acc 0.98
2016-09-06T05:04:41.668228: step 3538, loss 0.0242604, acc 1
2016-09-06T05:04:42.482437: step 3539, loss 0.0388707, acc 0.98
2016-09-06T05:04:43.282757: step 3540, loss 0.0108645, acc 1
2016-09-06T05:04:44.112582: step 3541, loss 0.0504758, acc 0.98
2016-09-06T05:04:44.967484: step 3542, loss 0.0237069, acc 1
2016-09-06T05:04:45.770951: step 3543, loss 0.0119339, acc 1
2016-09-06T05:04:46.606975: step 3544, loss 0.0561828, acc 0.94
2016-09-06T05:04:47.423917: step 3545, loss 0.0233455, acc 1
2016-09-06T05:04:48.218598: step 3546, loss 0.0172605, acc 1
2016-09-06T05:04:49.029657: step 3547, loss 0.0253857, acc 1
2016-09-06T05:04:49.868577: step 3548, loss 0.0315996, acc 0.98
2016-09-06T05:04:50.696024: step 3549, loss 0.0250523, acc 1
2016-09-06T05:04:51.504156: step 3550, loss 0.01596, acc 1
2016-09-06T05:04:52.325994: step 3551, loss 0.0291126, acc 0.98
2016-09-06T05:04:53.116500: step 3552, loss 0.0214465, acc 1
2016-09-06T05:04:53.912536: step 3553, loss 0.00925549, acc 1
2016-09-06T05:04:54.770925: step 3554, loss 0.0372735, acc 1
2016-09-06T05:04:55.597535: step 3555, loss 0.0563696, acc 0.98
2016-09-06T05:04:56.397410: step 3556, loss 0.0175919, acc 1
2016-09-06T05:04:57.232801: step 3557, loss 0.0563297, acc 0.98
2016-09-06T05:04:58.061659: step 3558, loss 0.01129, acc 1
2016-09-06T05:04:58.853060: step 3559, loss 0.0215986, acc 1
2016-09-06T05:04:59.672441: step 3560, loss 0.0534394, acc 0.98
2016-09-06T05:05:00.541133: step 3561, loss 0.0188723, acc 0.98
2016-09-06T05:05:01.356719: step 3562, loss 0.0248197, acc 0.98
2016-09-06T05:05:02.175880: step 3563, loss 0.0361249, acc 1
2016-09-06T05:05:03.022232: step 3564, loss 0.0274374, acc 1
2016-09-06T05:05:03.831765: step 3565, loss 0.0759755, acc 0.98
2016-09-06T05:05:04.641661: step 3566, loss 0.0367455, acc 0.98
2016-09-06T05:05:05.533945: step 3567, loss 0.044465, acc 0.98
2016-09-06T05:05:06.352008: step 3568, loss 0.169535, acc 0.92
2016-09-06T05:05:07.154833: step 3569, loss 0.00407629, acc 1
2016-09-06T05:05:07.986398: step 3570, loss 0.0180711, acc 1
2016-09-06T05:05:08.800717: step 3571, loss 0.0395876, acc 0.98
2016-09-06T05:05:09.633624: step 3572, loss 0.0215075, acc 1
2016-09-06T05:05:10.483052: step 3573, loss 0.0557631, acc 0.98
2016-09-06T05:05:11.278483: step 3574, loss 0.00673664, acc 1
2016-09-06T05:05:12.117754: step 3575, loss 0.0220048, acc 1
2016-09-06T05:05:12.919757: step 3576, loss 0.0238016, acc 1
2016-09-06T05:05:13.721511: step 3577, loss 0.040836, acc 0.98
2016-09-06T05:05:14.530630: step 3578, loss 0.0142361, acc 1
2016-09-06T05:05:15.339031: step 3579, loss 0.0227309, acc 1
2016-09-06T05:05:16.171475: step 3580, loss 0.0486591, acc 0.98
2016-09-06T05:05:16.969682: step 3581, loss 0.0363204, acc 0.98
2016-09-06T05:05:17.767908: step 3582, loss 0.0172185, acc 1
2016-09-06T05:05:18.584627: step 3583, loss 0.0078539, acc 1
2016-09-06T05:05:19.385492: step 3584, loss 0.032315, acc 0.98
2016-09-06T05:05:20.196501: step 3585, loss 0.0327328, acc 0.98
2016-09-06T05:05:21.055454: step 3586, loss 0.0213734, acc 0.98
2016-09-06T05:05:21.820158: step 3587, loss 0.0539889, acc 0.98
2016-09-06T05:05:22.624202: step 3588, loss 0.0495467, acc 0.98
2016-09-06T05:05:23.453326: step 3589, loss 0.0278752, acc 0.98
2016-09-06T05:05:24.243806: step 3590, loss 0.215877, acc 0.88
2016-09-06T05:05:25.026659: step 3591, loss 0.0278629, acc 1
2016-09-06T05:05:25.842934: step 3592, loss 0.0111187, acc 1
2016-09-06T05:05:26.682327: step 3593, loss 0.0389453, acc 0.98
2016-09-06T05:05:27.492300: step 3594, loss 0.0608972, acc 0.96
2016-09-06T05:05:28.310665: step 3595, loss 0.0326216, acc 1
2016-09-06T05:05:29.095226: step 3596, loss 0.0549725, acc 0.98
2016-09-06T05:05:29.878628: step 3597, loss 0.074385, acc 0.94
2016-09-06T05:05:30.704345: step 3598, loss 0.00437206, acc 1
2016-09-06T05:05:31.486801: step 3599, loss 0.0225428, acc 0.98
2016-09-06T05:05:32.320655: step 3600, loss 0.177843, acc 0.96

Evaluation:
2016-09-06T05:05:36.025125: step 3600, loss 1.65145, acc 0.741088

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-3600

2016-09-06T05:05:37.889101: step 3601, loss 0.0482573, acc 0.96
2016-09-06T05:05:38.716413: step 3602, loss 0.0449638, acc 0.96
2016-09-06T05:05:39.552796: step 3603, loss 0.0562097, acc 0.98
2016-09-06T05:05:40.377436: step 3604, loss 0.118947, acc 0.94
2016-09-06T05:05:41.170759: step 3605, loss 0.0358749, acc 0.98
2016-09-06T05:05:41.969764: step 3606, loss 0.0208808, acc 1
2016-09-06T05:05:42.812686: step 3607, loss 0.0129139, acc 1
2016-09-06T05:05:43.572691: step 3608, loss 0.020475, acc 0.98
2016-09-06T05:05:44.379314: step 3609, loss 0.0412778, acc 0.98
2016-09-06T05:05:45.212700: step 3610, loss 0.0263614, acc 1
2016-09-06T05:05:45.976039: step 3611, loss 0.0261209, acc 1
2016-09-06T05:05:46.761978: step 3612, loss 0.0423297, acc 0.98
2016-09-06T05:05:47.570496: step 3613, loss 0.0044329, acc 1
2016-09-06T05:05:48.350328: step 3614, loss 0.0144001, acc 1
2016-09-06T05:05:49.177848: step 3615, loss 0.0288784, acc 0.98
2016-09-06T05:05:49.978412: step 3616, loss 0.0556758, acc 0.98
2016-09-06T05:05:50.769786: step 3617, loss 0.0654158, acc 0.96
2016-09-06T05:05:51.578105: step 3618, loss 0.0431637, acc 0.98
2016-09-06T05:05:52.412844: step 3619, loss 0.0141299, acc 1
2016-09-06T05:05:53.214909: step 3620, loss 0.0125638, acc 1
2016-09-06T05:05:54.022872: step 3621, loss 0.00763234, acc 1
2016-09-06T05:05:54.835659: step 3622, loss 0.0534441, acc 0.96
2016-09-06T05:05:55.679566: step 3623, loss 0.0216172, acc 1
2016-09-06T05:05:56.496903: step 3624, loss 0.0974907, acc 0.94
2016-09-06T05:05:57.301961: step 3625, loss 0.0547285, acc 0.98
2016-09-06T05:05:58.069986: step 3626, loss 0.0828012, acc 0.98
2016-09-06T05:05:58.863069: step 3627, loss 0.0310339, acc 0.98
2016-09-06T05:05:59.680437: step 3628, loss 0.0214016, acc 1
2016-09-06T05:06:00.514381: step 3629, loss 0.0240248, acc 1
2016-09-06T05:06:01.316443: step 3630, loss 0.0558281, acc 0.96
2016-09-06T05:06:02.134180: step 3631, loss 0.0632561, acc 0.96
2016-09-06T05:06:02.935661: step 3632, loss 0.0756526, acc 0.96
2016-09-06T05:06:03.731379: step 3633, loss 0.0747773, acc 0.96
2016-09-06T05:06:04.550716: step 3634, loss 0.0942337, acc 0.94
2016-09-06T05:06:05.337270: step 3635, loss 0.0223163, acc 1
2016-09-06T05:06:06.134123: step 3636, loss 0.0980033, acc 0.94
2016-09-06T05:06:06.951549: step 3637, loss 0.00584682, acc 1
2016-09-06T05:06:07.746574: step 3638, loss 0.0191836, acc 1
2016-09-06T05:06:08.519320: step 3639, loss 0.0580908, acc 0.98
2016-09-06T05:06:09.356966: step 3640, loss 0.0141965, acc 1
2016-09-06T05:06:10.105708: step 3641, loss 0.049839, acc 0.98
2016-09-06T05:06:10.948122: step 3642, loss 0.0527798, acc 0.98
2016-09-06T05:06:11.800901: step 3643, loss 0.0828253, acc 0.96
2016-09-06T05:06:12.600891: step 3644, loss 0.0337392, acc 0.98
2016-09-06T05:06:13.402945: step 3645, loss 0.19175, acc 0.96
2016-09-06T05:06:14.186109: step 3646, loss 0.0745468, acc 0.98
2016-09-06T05:06:14.972056: step 3647, loss 0.0261352, acc 1
2016-09-06T05:06:15.729719: step 3648, loss 0.0298834, acc 1
2016-09-06T05:06:16.553517: step 3649, loss 0.0393966, acc 0.98
2016-09-06T05:06:17.367217: step 3650, loss 0.0392259, acc 0.96
2016-09-06T05:06:18.179459: step 3651, loss 0.00881191, acc 1
2016-09-06T05:06:19.008082: step 3652, loss 0.0474057, acc 1
2016-09-06T05:06:19.813298: step 3653, loss 0.0260359, acc 0.98
2016-09-06T05:06:20.632525: step 3654, loss 0.0983594, acc 0.98
2016-09-06T05:06:21.457429: step 3655, loss 0.0342409, acc 0.98
2016-09-06T05:06:22.267258: step 3656, loss 0.0331912, acc 0.98
2016-09-06T05:06:23.080645: step 3657, loss 0.0861146, acc 0.96
2016-09-06T05:06:23.923071: step 3658, loss 0.0549696, acc 0.96
2016-09-06T05:06:24.762820: step 3659, loss 0.00670752, acc 1
2016-09-06T05:06:25.572183: step 3660, loss 0.00432879, acc 1
2016-09-06T05:06:26.399490: step 3661, loss 0.00887518, acc 1
2016-09-06T05:06:27.229043: step 3662, loss 0.0628641, acc 0.98
2016-09-06T05:06:28.057119: step 3663, loss 0.050359, acc 0.96
2016-09-06T05:06:28.893111: step 3664, loss 0.120362, acc 0.94
2016-09-06T05:06:29.697412: step 3665, loss 0.0880946, acc 0.96
2016-09-06T05:06:30.509390: step 3666, loss 0.00522259, acc 1
2016-09-06T05:06:31.337975: step 3667, loss 0.042681, acc 0.98
2016-09-06T05:06:32.151077: step 3668, loss 0.0223638, acc 0.98
2016-09-06T05:06:32.973378: step 3669, loss 0.0121673, acc 1
2016-09-06T05:06:33.835267: step 3670, loss 0.0329774, acc 0.98
2016-09-06T05:06:34.658261: step 3671, loss 0.0500677, acc 0.96
2016-09-06T05:06:35.448898: step 3672, loss 0.0378258, acc 0.98
2016-09-06T05:06:36.271683: step 3673, loss 0.0289468, acc 0.98
2016-09-06T05:06:37.097144: step 3674, loss 0.0936349, acc 0.98
2016-09-06T05:06:37.892538: step 3675, loss 0.0899624, acc 0.96
2016-09-06T05:06:38.690645: step 3676, loss 0.156502, acc 0.96
2016-09-06T05:06:39.540699: step 3677, loss 0.0326097, acc 1
2016-09-06T05:06:40.364943: step 3678, loss 0.01766, acc 1
2016-09-06T05:06:41.185672: step 3679, loss 0.0879234, acc 0.96
2016-09-06T05:06:41.996213: step 3680, loss 0.0442448, acc 0.98
2016-09-06T05:06:42.801409: step 3681, loss 0.0342696, acc 0.98
2016-09-06T05:06:43.620666: step 3682, loss 0.0268072, acc 1
2016-09-06T05:06:44.443633: step 3683, loss 0.106501, acc 0.96
2016-09-06T05:06:45.253790: step 3684, loss 0.0342692, acc 0.98
2016-09-06T05:06:46.067799: step 3685, loss 0.0352101, acc 0.98
2016-09-06T05:06:46.917420: step 3686, loss 0.0448039, acc 0.96
2016-09-06T05:06:47.700577: step 3687, loss 0.042733, acc 0.98
2016-09-06T05:06:48.547290: step 3688, loss 0.0356564, acc 0.98
2016-09-06T05:06:49.369202: step 3689, loss 0.0492391, acc 0.96
2016-09-06T05:06:50.193481: step 3690, loss 0.0146536, acc 1
2016-09-06T05:06:51.044712: step 3691, loss 0.0380894, acc 0.98
2016-09-06T05:06:51.891333: step 3692, loss 0.00983873, acc 1
2016-09-06T05:06:52.672779: step 3693, loss 0.0037627, acc 1
2016-09-06T05:06:53.487055: step 3694, loss 0.0195897, acc 1
2016-09-06T05:06:54.303583: step 3695, loss 0.0402234, acc 0.98
2016-09-06T05:06:55.144990: step 3696, loss 0.0666012, acc 0.98
2016-09-06T05:06:55.995079: step 3697, loss 0.0374597, acc 1
2016-09-06T05:06:56.825500: step 3698, loss 0.00476664, acc 1
2016-09-06T05:06:57.636794: step 3699, loss 0.0963463, acc 0.96
2016-09-06T05:06:58.437950: step 3700, loss 0.0773669, acc 0.96

Evaluation:
2016-09-06T05:07:02.218727: step 3700, loss 1.55566, acc 0.757036

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-3700

2016-09-06T05:07:04.200456: step 3701, loss 0.0142141, acc 1
2016-09-06T05:07:05.021326: step 3702, loss 0.00674395, acc 1
2016-09-06T05:07:05.812254: step 3703, loss 0.0689889, acc 0.98
2016-09-06T05:07:06.613180: step 3704, loss 0.0799467, acc 0.98
2016-09-06T05:07:07.432314: step 3705, loss 0.0145495, acc 1
2016-09-06T05:07:08.243721: step 3706, loss 0.04944, acc 0.96
2016-09-06T05:07:09.039802: step 3707, loss 0.0314781, acc 1
2016-09-06T05:07:09.872691: step 3708, loss 0.14446, acc 0.98
2016-09-06T05:07:10.679285: step 3709, loss 0.0800977, acc 0.96
2016-09-06T05:07:11.481855: step 3710, loss 0.0481841, acc 0.98
2016-09-06T05:07:12.337628: step 3711, loss 0.00368955, acc 1
2016-09-06T05:07:13.139704: step 3712, loss 0.0282222, acc 0.98
2016-09-06T05:07:13.954370: step 3713, loss 0.0387031, acc 0.98
2016-09-06T05:07:14.761769: step 3714, loss 0.0241233, acc 0.98
2016-09-06T05:07:15.571761: step 3715, loss 0.0129245, acc 1
2016-09-06T05:07:16.404406: step 3716, loss 0.0642648, acc 0.98
2016-09-06T05:07:17.246860: step 3717, loss 0.0127028, acc 1
2016-09-06T05:07:18.037338: step 3718, loss 0.0623254, acc 0.96
2016-09-06T05:07:18.852397: step 3719, loss 0.0282965, acc 0.98
2016-09-06T05:07:19.690185: step 3720, loss 0.0274577, acc 1
2016-09-06T05:07:20.456210: step 3721, loss 0.0216902, acc 0.98
2016-09-06T05:07:21.261456: step 3722, loss 0.0280635, acc 0.98
2016-09-06T05:07:22.093274: step 3723, loss 0.0390458, acc 1
2016-09-06T05:07:22.928195: step 3724, loss 0.0600092, acc 0.98
2016-09-06T05:07:23.737468: step 3725, loss 0.039675, acc 1
2016-09-06T05:07:24.559756: step 3726, loss 0.00531413, acc 1
2016-09-06T05:07:25.360426: step 3727, loss 0.0318019, acc 1
2016-09-06T05:07:26.216436: step 3728, loss 0.0290297, acc 1
2016-09-06T05:07:27.047398: step 3729, loss 0.0172211, acc 0.98
2016-09-06T05:07:27.844430: step 3730, loss 0.00755729, acc 1
2016-09-06T05:07:28.629125: step 3731, loss 0.0286357, acc 0.98
2016-09-06T05:07:29.470268: step 3732, loss 0.0065429, acc 1
2016-09-06T05:07:30.275664: step 3733, loss 0.0179124, acc 1
2016-09-06T05:07:31.100280: step 3734, loss 0.0482378, acc 0.98
2016-09-06T05:07:31.889253: step 3735, loss 0.0287016, acc 0.98
2016-09-06T05:07:32.701605: step 3736, loss 0.0441297, acc 0.98
2016-09-06T05:07:33.519831: step 3737, loss 0.0593024, acc 0.98
2016-09-06T05:07:34.336354: step 3738, loss 0.0358763, acc 0.98
2016-09-06T05:07:35.181660: step 3739, loss 0.0362691, acc 0.98
2016-09-06T05:07:35.946079: step 3740, loss 0.0151953, acc 1
2016-09-06T05:07:36.749244: step 3741, loss 0.066581, acc 0.96
2016-09-06T05:07:37.574672: step 3742, loss 0.0342746, acc 0.98
2016-09-06T05:07:38.370791: step 3743, loss 0.0399128, acc 0.98
2016-09-06T05:07:39.206254: step 3744, loss 0.0562489, acc 0.98
2016-09-06T05:07:40.030381: step 3745, loss 0.0044796, acc 1
2016-09-06T05:07:40.832658: step 3746, loss 0.0352173, acc 0.96
2016-09-06T05:07:41.644427: step 3747, loss 0.0163876, acc 1
2016-09-06T05:07:42.446113: step 3748, loss 0.0903441, acc 0.98
2016-09-06T05:07:43.218078: step 3749, loss 0.165722, acc 0.96
2016-09-06T05:07:44.024506: step 3750, loss 0.0158477, acc 1
2016-09-06T05:07:44.858899: step 3751, loss 0.0499051, acc 0.96
2016-09-06T05:07:45.660713: step 3752, loss 0.0282806, acc 1
2016-09-06T05:07:46.498760: step 3753, loss 0.0510098, acc 0.96
2016-09-06T05:07:47.316877: step 3754, loss 0.0271717, acc 0.98
2016-09-06T05:07:48.116515: step 3755, loss 0.0379966, acc 0.96
2016-09-06T05:07:48.936420: step 3756, loss 0.0250773, acc 1
2016-09-06T05:07:49.792997: step 3757, loss 0.068018, acc 0.96
2016-09-06T05:07:50.609474: step 3758, loss 0.0683059, acc 0.98
2016-09-06T05:07:51.428305: step 3759, loss 0.00550689, acc 1
2016-09-06T05:07:52.249409: step 3760, loss 0.0304952, acc 0.98
2016-09-06T05:07:53.071633: step 3761, loss 0.0768753, acc 0.96
2016-09-06T05:07:53.890683: step 3762, loss 0.0895573, acc 0.98
2016-09-06T05:07:54.724255: step 3763, loss 0.0454125, acc 0.98
2016-09-06T05:07:55.568634: step 3764, loss 0.123996, acc 0.96
2016-09-06T05:07:56.407506: step 3765, loss 0.0399141, acc 0.98
2016-09-06T05:07:57.260256: step 3766, loss 0.00855941, acc 1
2016-09-06T05:07:58.096031: step 3767, loss 0.058269, acc 0.98
2016-09-06T05:07:58.888961: step 3768, loss 0.019444, acc 1
2016-09-06T05:07:59.694950: step 3769, loss 0.0219336, acc 1
2016-09-06T05:08:00.535253: step 3770, loss 0.0432774, acc 0.96
2016-09-06T05:08:01.338480: step 3771, loss 0.0254017, acc 1
2016-09-06T05:08:02.153321: step 3772, loss 0.0578072, acc 0.98
2016-09-06T05:08:02.974234: step 3773, loss 0.031644, acc 1
2016-09-06T05:08:03.752629: step 3774, loss 0.0238691, acc 0.98
2016-09-06T05:08:04.539220: step 3775, loss 0.0172883, acc 1
2016-09-06T05:08:05.373194: step 3776, loss 0.0183147, acc 1
2016-09-06T05:08:06.159906: step 3777, loss 0.023147, acc 1
2016-09-06T05:08:06.958662: step 3778, loss 0.0372141, acc 0.98
2016-09-06T05:08:07.781821: step 3779, loss 0.0224795, acc 1
2016-09-06T05:08:08.592983: step 3780, loss 0.0740914, acc 0.96
2016-09-06T05:08:09.417713: step 3781, loss 0.0209771, acc 0.98
2016-09-06T05:08:10.240666: step 3782, loss 0.0820529, acc 0.94
2016-09-06T05:08:11.011590: step 3783, loss 0.0677339, acc 0.98
2016-09-06T05:08:11.831746: step 3784, loss 0.0248708, acc 0.98
2016-09-06T05:08:12.651231: step 3785, loss 0.00611473, acc 1
2016-09-06T05:08:13.449221: step 3786, loss 0.0791017, acc 0.98
2016-09-06T05:08:14.258416: step 3787, loss 0.00545175, acc 1
2016-09-06T05:08:15.069685: step 3788, loss 0.00538595, acc 1
2016-09-06T05:08:15.892170: step 3789, loss 0.179024, acc 0.96
2016-09-06T05:08:16.704188: step 3790, loss 0.0831227, acc 0.98
2016-09-06T05:08:17.518122: step 3791, loss 0.039699, acc 0.98
2016-09-06T05:08:18.326224: step 3792, loss 0.0337777, acc 0.98
2016-09-06T05:08:19.141013: step 3793, loss 0.0410714, acc 0.98
2016-09-06T05:08:19.974112: step 3794, loss 0.0165758, acc 1
2016-09-06T05:08:20.791143: step 3795, loss 0.0199754, acc 1
2016-09-06T05:08:21.620926: step 3796, loss 0.0623571, acc 0.98
2016-09-06T05:08:22.459986: step 3797, loss 0.0313885, acc 0.98
2016-09-06T05:08:23.280730: step 3798, loss 0.0591006, acc 0.98
2016-09-06T05:08:24.090235: step 3799, loss 0.0662604, acc 0.98
2016-09-06T05:08:24.943652: step 3800, loss 0.00904169, acc 1

Evaluation:
2016-09-06T05:08:28.681869: step 3800, loss 1.59103, acc 0.749531

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-3800

2016-09-06T05:08:30.542512: step 3801, loss 0.0224216, acc 1
2016-09-06T05:08:31.332102: step 3802, loss 0.0902031, acc 0.98
2016-09-06T05:08:32.151179: step 3803, loss 0.033573, acc 0.98
2016-09-06T05:08:32.930799: step 3804, loss 0.0926101, acc 0.98
2016-09-06T05:08:33.747196: step 3805, loss 0.0377171, acc 0.96
2016-09-06T05:08:34.554500: step 3806, loss 0.0108224, acc 1
2016-09-06T05:08:35.380019: step 3807, loss 0.109009, acc 0.98
2016-09-06T05:08:36.195225: step 3808, loss 0.0901704, acc 0.94
2016-09-06T05:08:37.009663: step 3809, loss 0.0263309, acc 1
2016-09-06T05:08:37.807657: step 3810, loss 0.0288188, acc 1
2016-09-06T05:08:38.615984: step 3811, loss 0.00501774, acc 1
2016-09-06T05:08:39.463544: step 3812, loss 0.0915605, acc 0.96
2016-09-06T05:08:40.273858: step 3813, loss 0.0220115, acc 1
2016-09-06T05:08:41.077698: step 3814, loss 0.108381, acc 0.94
2016-09-06T05:08:41.895608: step 3815, loss 0.0875891, acc 0.96
2016-09-06T05:08:42.683210: step 3816, loss 0.0188871, acc 1
2016-09-06T05:08:43.478786: step 3817, loss 0.0410372, acc 0.98
2016-09-06T05:08:44.307551: step 3818, loss 0.0200311, acc 1
2016-09-06T05:08:45.120095: step 3819, loss 0.0357797, acc 0.98
2016-09-06T05:08:45.920668: step 3820, loss 0.018188, acc 1
2016-09-06T05:08:46.719952: step 3821, loss 0.014592, acc 1
2016-09-06T05:08:47.472406: step 3822, loss 0.123321, acc 0.92
2016-09-06T05:08:48.266865: step 3823, loss 0.0680034, acc 0.98
2016-09-06T05:08:49.093819: step 3824, loss 0.0282061, acc 0.98
2016-09-06T05:08:49.884321: step 3825, loss 0.0532838, acc 0.96
2016-09-06T05:08:50.686377: step 3826, loss 0.0381121, acc 1
2016-09-06T05:08:51.486577: step 3827, loss 0.0187127, acc 1
2016-09-06T05:08:52.285280: step 3828, loss 0.0220827, acc 0.98
2016-09-06T05:08:53.095482: step 3829, loss 0.0381364, acc 1
2016-09-06T05:08:53.921317: step 3830, loss 0.023616, acc 1
2016-09-06T05:08:54.712186: step 3831, loss 0.0113504, acc 1
2016-09-06T05:08:55.523989: step 3832, loss 0.012111, acc 1
2016-09-06T05:08:56.380157: step 3833, loss 0.0148054, acc 1
2016-09-06T05:08:57.177371: step 3834, loss 0.017951, acc 1
2016-09-06T05:08:57.977068: step 3835, loss 0.00841308, acc 1
2016-09-06T05:08:58.816387: step 3836, loss 0.00462989, acc 1
2016-09-06T05:08:59.599357: step 3837, loss 0.0617563, acc 0.96
2016-09-06T05:09:00.407055: step 3838, loss 0.0704855, acc 0.96
2016-09-06T05:09:01.231535: step 3839, loss 0.0604114, acc 0.98
2016-09-06T05:09:01.950130: step 3840, loss 0.0229302, acc 0.977273
2016-09-06T05:09:02.765525: step 3841, loss 0.00913731, acc 1
2016-09-06T05:09:03.587918: step 3842, loss 0.0295108, acc 0.98
2016-09-06T05:09:04.365240: step 3843, loss 0.0518197, acc 0.98
2016-09-06T05:09:05.179995: step 3844, loss 0.00417026, acc 1
2016-09-06T05:09:05.988091: step 3845, loss 0.0957971, acc 0.96
2016-09-06T05:09:06.812812: step 3846, loss 0.0503017, acc 0.96
2016-09-06T05:09:07.636700: step 3847, loss 0.0332007, acc 0.98
2016-09-06T05:09:08.471069: step 3848, loss 0.0123529, acc 1
2016-09-06T05:09:09.268516: step 3849, loss 0.0532578, acc 1
2016-09-06T05:09:10.080016: step 3850, loss 0.133823, acc 0.96
2016-09-06T05:09:10.902176: step 3851, loss 0.0238036, acc 0.98
2016-09-06T05:09:11.669192: step 3852, loss 0.013714, acc 1
2016-09-06T05:09:12.473504: step 3853, loss 0.0557697, acc 0.98
2016-09-06T05:09:13.285053: step 3854, loss 0.0202119, acc 1
2016-09-06T05:09:14.067645: step 3855, loss 0.0583838, acc 0.98
2016-09-06T05:09:14.911586: step 3856, loss 0.00943618, acc 1
2016-09-06T05:09:15.752869: step 3857, loss 0.00616466, acc 1
2016-09-06T05:09:16.563207: step 3858, loss 0.0572106, acc 0.98
2016-09-06T05:09:17.369269: step 3859, loss 0.0250078, acc 1
2016-09-06T05:09:18.213412: step 3860, loss 0.016027, acc 1
2016-09-06T05:09:19.029946: step 3861, loss 0.0663356, acc 0.98
2016-09-06T05:09:19.837946: step 3862, loss 0.0494302, acc 0.96
2016-09-06T05:09:20.660697: step 3863, loss 0.0396281, acc 0.98
2016-09-06T05:09:21.491468: step 3864, loss 0.104715, acc 0.94
2016-09-06T05:09:22.310364: step 3865, loss 0.0379302, acc 0.98
2016-09-06T05:09:23.157972: step 3866, loss 0.0259738, acc 1
2016-09-06T05:09:23.998391: step 3867, loss 0.0206676, acc 1
2016-09-06T05:09:24.838879: step 3868, loss 0.0189576, acc 1
2016-09-06T05:09:25.682994: step 3869, loss 0.0253847, acc 0.98
2016-09-06T05:09:26.494966: step 3870, loss 0.00321771, acc 1
2016-09-06T05:09:27.320834: step 3871, loss 0.0402251, acc 0.96
2016-09-06T05:09:28.119851: step 3872, loss 0.0243144, acc 0.98
2016-09-06T05:09:28.951098: step 3873, loss 0.0208719, acc 0.98
2016-09-06T05:09:29.778400: step 3874, loss 0.0643035, acc 0.96
2016-09-06T05:09:30.592754: step 3875, loss 0.0861125, acc 0.98
2016-09-06T05:09:31.416867: step 3876, loss 0.0414204, acc 0.96
2016-09-06T05:09:32.203047: step 3877, loss 0.011389, acc 1
2016-09-06T05:09:32.994687: step 3878, loss 0.0726786, acc 0.98
2016-09-06T05:09:33.833568: step 3879, loss 0.0208221, acc 1
2016-09-06T05:09:34.608367: step 3880, loss 0.0539847, acc 0.98
2016-09-06T05:09:35.442617: step 3881, loss 0.0557354, acc 0.96
2016-09-06T05:09:36.235471: step 3882, loss 0.0392212, acc 0.96
2016-09-06T05:09:37.041592: step 3883, loss 0.035936, acc 0.98
2016-09-06T05:09:37.849398: step 3884, loss 0.0110463, acc 1
2016-09-06T05:09:38.653425: step 3885, loss 0.0144005, acc 1
2016-09-06T05:09:39.458168: step 3886, loss 0.0446661, acc 0.98
2016-09-06T05:09:40.246509: step 3887, loss 0.022879, acc 1
2016-09-06T05:09:41.058109: step 3888, loss 0.0153522, acc 1
2016-09-06T05:09:41.861896: step 3889, loss 0.0620404, acc 0.98
2016-09-06T05:09:42.661234: step 3890, loss 0.0235767, acc 0.98
2016-09-06T05:09:43.488037: step 3891, loss 0.0160467, acc 1
2016-09-06T05:09:44.259078: step 3892, loss 0.0483489, acc 0.96
2016-09-06T05:09:45.069642: step 3893, loss 0.0340815, acc 0.98
2016-09-06T05:09:45.901679: step 3894, loss 0.0192172, acc 1
2016-09-06T05:09:46.697466: step 3895, loss 0.0372979, acc 0.98
2016-09-06T05:09:47.493393: step 3896, loss 0.0547631, acc 0.96
2016-09-06T05:09:48.333520: step 3897, loss 0.0241332, acc 0.98
2016-09-06T05:09:49.109768: step 3898, loss 0.0246891, acc 0.98
2016-09-06T05:09:49.917976: step 3899, loss 0.00999655, acc 1
2016-09-06T05:09:50.724281: step 3900, loss 0.0289668, acc 0.98

Evaluation:
2016-09-06T05:09:54.439577: step 3900, loss 1.63942, acc 0.757974

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-3900

2016-09-06T05:09:56.258342: step 3901, loss 0.0219487, acc 1
2016-09-06T05:09:57.083107: step 3902, loss 0.0540856, acc 0.96
2016-09-06T05:09:57.893483: step 3903, loss 0.00618031, acc 1
2016-09-06T05:09:58.732320: step 3904, loss 0.0408814, acc 0.96
2016-09-06T05:09:59.554853: step 3905, loss 0.00786747, acc 1
2016-09-06T05:10:00.422332: step 3906, loss 0.0527832, acc 0.98
2016-09-06T05:10:01.242801: step 3907, loss 0.00906326, acc 1
2016-09-06T05:10:02.058581: step 3908, loss 0.0500145, acc 0.96
2016-09-06T05:10:02.881216: step 3909, loss 0.0447986, acc 0.96
2016-09-06T05:10:03.666605: step 3910, loss 0.0566471, acc 0.96
2016-09-06T05:10:04.481530: step 3911, loss 0.0619725, acc 0.98
2016-09-06T05:10:05.292191: step 3912, loss 0.0139275, acc 1
2016-09-06T05:10:06.107932: step 3913, loss 0.0377233, acc 0.96
2016-09-06T05:10:06.914291: step 3914, loss 0.0113943, acc 1
2016-09-06T05:10:07.727083: step 3915, loss 0.0883817, acc 0.94
2016-09-06T05:10:08.502918: step 3916, loss 0.0425731, acc 0.98
2016-09-06T05:10:09.310896: step 3917, loss 0.0534976, acc 0.98
2016-09-06T05:10:10.108649: step 3918, loss 0.025084, acc 1
2016-09-06T05:10:10.923932: step 3919, loss 0.0332284, acc 0.98
2016-09-06T05:10:11.730605: step 3920, loss 0.0257423, acc 1
2016-09-06T05:10:12.575286: step 3921, loss 0.00792244, acc 1
2016-09-06T05:10:13.371408: step 3922, loss 0.0627973, acc 0.96
2016-09-06T05:10:14.191339: step 3923, loss 0.0482668, acc 0.96
2016-09-06T05:10:15.055530: step 3924, loss 0.0942154, acc 0.96
2016-09-06T05:10:15.866220: step 3925, loss 0.0646035, acc 0.96
2016-09-06T05:10:16.676259: step 3926, loss 0.0365295, acc 1
2016-09-06T05:10:17.526140: step 3927, loss 0.0127593, acc 1
2016-09-06T05:10:18.329044: step 3928, loss 0.0320555, acc 0.98
2016-09-06T05:10:19.153468: step 3929, loss 0.0198398, acc 1
2016-09-06T05:10:19.990119: step 3930, loss 0.00581141, acc 1
2016-09-06T05:10:20.790443: step 3931, loss 0.0169157, acc 1
2016-09-06T05:10:21.599638: step 3932, loss 0.00956429, acc 1
2016-09-06T05:10:22.431633: step 3933, loss 0.105713, acc 0.92
2016-09-06T05:10:23.253999: step 3934, loss 0.023641, acc 0.98
2016-09-06T05:10:24.078428: step 3935, loss 0.0970777, acc 0.94
2016-09-06T05:10:24.921983: step 3936, loss 0.0037708, acc 1
2016-09-06T05:10:25.750561: step 3937, loss 0.030357, acc 1
2016-09-06T05:10:26.543675: step 3938, loss 0.0436224, acc 1
2016-09-06T05:10:27.409425: step 3939, loss 0.0172419, acc 1
2016-09-06T05:10:28.230801: step 3940, loss 0.0163047, acc 1
2016-09-06T05:10:29.057463: step 3941, loss 0.00309112, acc 1
2016-09-06T05:10:29.885552: step 3942, loss 0.060334, acc 0.96
2016-09-06T05:10:30.724124: step 3943, loss 0.0200401, acc 0.98
2016-09-06T05:10:31.522792: step 3944, loss 0.021464, acc 1
2016-09-06T05:10:32.356889: step 3945, loss 0.049233, acc 0.98
2016-09-06T05:10:33.164378: step 3946, loss 0.189895, acc 0.94
2016-09-06T05:10:33.960181: step 3947, loss 0.139969, acc 0.94
2016-09-06T05:10:34.726154: step 3948, loss 0.0108407, acc 1
2016-09-06T05:10:35.527307: step 3949, loss 0.0181885, acc 1
2016-09-06T05:10:36.307220: step 3950, loss 0.0554117, acc 0.98
2016-09-06T05:10:37.122245: step 3951, loss 0.00792619, acc 1
2016-09-06T05:10:37.973763: step 3952, loss 0.0388736, acc 0.98
2016-09-06T05:10:38.773423: step 3953, loss 0.0514538, acc 0.96
2016-09-06T05:10:39.566426: step 3954, loss 0.00694011, acc 1
2016-09-06T05:10:40.372337: step 3955, loss 0.0180876, acc 1
2016-09-06T05:10:41.166831: step 3956, loss 0.022576, acc 0.98
2016-09-06T05:10:41.979319: step 3957, loss 0.104452, acc 0.96
2016-09-06T05:10:42.797465: step 3958, loss 0.00241691, acc 1
2016-09-06T05:10:43.585800: step 3959, loss 0.0266669, acc 1
2016-09-06T05:10:44.395692: step 3960, loss 0.0222513, acc 0.98
2016-09-06T05:10:45.230488: step 3961, loss 0.0214646, acc 1
2016-09-06T05:10:46.027558: step 3962, loss 0.0180042, acc 1
2016-09-06T05:10:46.833810: step 3963, loss 0.00463007, acc 1
2016-09-06T05:10:47.634136: step 3964, loss 0.0581146, acc 0.96
2016-09-06T05:10:48.412293: step 3965, loss 0.05878, acc 0.96
2016-09-06T05:10:49.229144: step 3966, loss 0.17921, acc 0.96
2016-09-06T05:10:50.065496: step 3967, loss 0.0238538, acc 0.98
2016-09-06T05:10:50.863095: step 3968, loss 0.0101728, acc 1
2016-09-06T05:10:51.642019: step 3969, loss 0.0115721, acc 1
2016-09-06T05:10:52.480713: step 3970, loss 0.0416283, acc 0.98
2016-09-06T05:10:53.265962: step 3971, loss 0.0430877, acc 0.96
2016-09-06T05:10:54.046640: step 3972, loss 0.0849492, acc 0.96
2016-09-06T05:10:54.876419: step 3973, loss 0.0283185, acc 1
2016-09-06T05:10:55.664141: step 3974, loss 0.037555, acc 1
2016-09-06T05:10:56.468005: step 3975, loss 0.0422677, acc 0.98
2016-09-06T05:10:57.289671: step 3976, loss 0.0262844, acc 1
2016-09-06T05:10:58.068846: step 3977, loss 0.0326842, acc 1
2016-09-06T05:10:58.915448: step 3978, loss 0.0364653, acc 1
2016-09-06T05:10:59.733635: step 3979, loss 0.00438482, acc 1
2016-09-06T05:11:00.556478: step 3980, loss 0.00838355, acc 1
2016-09-06T05:11:01.349630: step 3981, loss 0.112049, acc 0.96
2016-09-06T05:11:02.165258: step 3982, loss 0.0120068, acc 1
2016-09-06T05:11:02.969851: step 3983, loss 0.0162114, acc 1
2016-09-06T05:11:03.786257: step 3984, loss 0.0384133, acc 0.98
2016-09-06T05:11:04.609921: step 3985, loss 0.0132639, acc 1
2016-09-06T05:11:05.398622: step 3986, loss 0.108139, acc 0.98
2016-09-06T05:11:06.198633: step 3987, loss 0.0476326, acc 1
2016-09-06T05:11:07.034020: step 3988, loss 0.0307359, acc 0.98
2016-09-06T05:11:07.833069: step 3989, loss 0.17794, acc 0.96
2016-09-06T05:11:08.636575: step 3990, loss 0.024337, acc 1
2016-09-06T05:11:09.473330: step 3991, loss 0.00604615, acc 1
2016-09-06T05:11:10.287622: step 3992, loss 0.0150371, acc 1
2016-09-06T05:11:11.121612: step 3993, loss 0.0516553, acc 0.96
2016-09-06T05:11:11.953450: step 3994, loss 0.0672709, acc 0.94
2016-09-06T05:11:12.776682: step 3995, loss 0.0524982, acc 0.96
2016-09-06T05:11:13.598461: step 3996, loss 0.0316054, acc 1
2016-09-06T05:11:14.452233: step 3997, loss 0.0277703, acc 1
2016-09-06T05:11:15.281287: step 3998, loss 0.051633, acc 0.98
2016-09-06T05:11:16.110342: step 3999, loss 0.03299, acc 1
2016-09-06T05:11:16.933066: step 4000, loss 0.0561034, acc 0.98

Evaluation:
2016-09-06T05:11:20.661636: step 4000, loss 1.51518, acc 0.750469

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-4000

2016-09-06T05:11:22.564732: step 4001, loss 0.059701, acc 0.98
2016-09-06T05:11:23.368664: step 4002, loss 0.0693053, acc 0.96
2016-09-06T05:11:24.197708: step 4003, loss 0.0231019, acc 1
2016-09-06T05:11:25.037849: step 4004, loss 0.0207789, acc 1
2016-09-06T05:11:25.848722: step 4005, loss 0.0916278, acc 0.94
2016-09-06T05:11:26.667540: step 4006, loss 0.058087, acc 0.98
2016-09-06T05:11:27.473272: step 4007, loss 0.0722232, acc 0.96
2016-09-06T05:11:28.286096: step 4008, loss 0.0269592, acc 1
2016-09-06T05:11:29.103418: step 4009, loss 0.0782685, acc 0.96
2016-09-06T05:11:29.904587: step 4010, loss 0.0172336, acc 1
2016-09-06T05:11:30.712691: step 4011, loss 0.148561, acc 0.94
2016-09-06T05:11:31.546421: step 4012, loss 0.0170313, acc 1
2016-09-06T05:11:32.352346: step 4013, loss 0.0333356, acc 0.98
2016-09-06T05:11:33.180548: step 4014, loss 0.0142498, acc 1
2016-09-06T05:11:34.020933: step 4015, loss 0.0366181, acc 0.98
2016-09-06T05:11:34.808397: step 4016, loss 0.0913616, acc 0.94
2016-09-06T05:11:35.618575: step 4017, loss 0.0154625, acc 1
2016-09-06T05:11:36.433928: step 4018, loss 0.0339274, acc 0.98
2016-09-06T05:11:37.260746: step 4019, loss 0.0352118, acc 0.98
2016-09-06T05:11:38.040849: step 4020, loss 0.0515567, acc 0.98
2016-09-06T05:11:38.860623: step 4021, loss 0.0508753, acc 0.98
2016-09-06T05:11:39.663656: step 4022, loss 0.0575289, acc 0.98
2016-09-06T05:11:40.482343: step 4023, loss 0.0140291, acc 1
2016-09-06T05:11:41.295895: step 4024, loss 0.0229484, acc 1
2016-09-06T05:11:42.126049: step 4025, loss 0.0272942, acc 1
2016-09-06T05:11:42.923573: step 4026, loss 0.032721, acc 0.98
2016-09-06T05:11:43.716795: step 4027, loss 0.0964686, acc 0.94
2016-09-06T05:11:44.571366: step 4028, loss 0.00705937, acc 1
2016-09-06T05:11:45.368724: step 4029, loss 0.0482752, acc 0.96
2016-09-06T05:11:46.161488: step 4030, loss 0.0267252, acc 0.98
2016-09-06T05:11:46.974971: step 4031, loss 0.042765, acc 0.98
2016-09-06T05:11:47.699637: step 4032, loss 0.0200665, acc 1
2016-09-06T05:11:48.518717: step 4033, loss 0.0259963, acc 0.98
2016-09-06T05:11:49.323362: step 4034, loss 0.0754304, acc 0.98
2016-09-06T05:11:50.114945: step 4035, loss 0.00744216, acc 1
2016-09-06T05:11:50.937347: step 4036, loss 0.0531509, acc 0.98
2016-09-06T05:11:51.741788: step 4037, loss 0.0544075, acc 0.96
2016-09-06T05:11:52.574536: step 4038, loss 0.0634151, acc 0.96
2016-09-06T05:11:53.395528: step 4039, loss 0.0210458, acc 0.98
2016-09-06T05:11:54.234393: step 4040, loss 0.0339613, acc 1
2016-09-06T05:11:55.012130: step 4041, loss 0.0431678, acc 0.98
2016-09-06T05:11:55.830107: step 4042, loss 0.0228194, acc 1
2016-09-06T05:11:56.659083: step 4043, loss 0.0109455, acc 1
2016-09-06T05:11:57.463996: step 4044, loss 0.0680998, acc 0.96
2016-09-06T05:11:58.256194: step 4045, loss 0.0251798, acc 1
2016-09-06T05:11:59.053400: step 4046, loss 0.0200425, acc 0.98
2016-09-06T05:11:59.835167: step 4047, loss 0.0289051, acc 0.98
2016-09-06T05:12:00.638430: step 4048, loss 0.0153613, acc 1
2016-09-06T05:12:01.477968: step 4049, loss 0.029616, acc 0.98
2016-09-06T05:12:02.290537: step 4050, loss 0.0249412, acc 0.98
2016-09-06T05:12:03.106329: step 4051, loss 0.0302523, acc 1
2016-09-06T05:12:03.934968: step 4052, loss 0.00511618, acc 1
2016-09-06T05:12:04.746717: step 4053, loss 0.0177068, acc 1
2016-09-06T05:12:05.529460: step 4054, loss 0.021566, acc 0.98
2016-09-06T05:12:06.341437: step 4055, loss 0.0327205, acc 1
2016-09-06T05:12:07.151873: step 4056, loss 0.074309, acc 0.96
2016-09-06T05:12:07.963743: step 4057, loss 0.0417407, acc 0.98
2016-09-06T05:12:08.755189: step 4058, loss 0.0814034, acc 0.98
2016-09-06T05:12:09.573150: step 4059, loss 0.0179205, acc 1
2016-09-06T05:12:10.383963: step 4060, loss 0.120037, acc 0.96
2016-09-06T05:12:11.217863: step 4061, loss 0.050999, acc 0.98
2016-09-06T05:12:12.003317: step 4062, loss 0.161003, acc 0.98
2016-09-06T05:12:12.824148: step 4063, loss 0.00453087, acc 1
2016-09-06T05:12:13.645442: step 4064, loss 0.0330473, acc 0.98
2016-09-06T05:12:14.467336: step 4065, loss 0.0549029, acc 0.96
2016-09-06T05:12:15.250159: step 4066, loss 0.0101476, acc 1
2016-09-06T05:12:16.053815: step 4067, loss 0.0900412, acc 0.96
2016-09-06T05:12:16.844448: step 4068, loss 0.0727802, acc 0.94
2016-09-06T05:12:17.662175: step 4069, loss 0.00719126, acc 1
2016-09-06T05:12:18.469630: step 4070, loss 0.0552932, acc 0.98
2016-09-06T05:12:19.273355: step 4071, loss 0.0321513, acc 1
2016-09-06T05:12:20.065865: step 4072, loss 0.00438772, acc 1
2016-09-06T05:12:20.904559: step 4073, loss 0.026539, acc 1
2016-09-06T05:12:21.693444: step 4074, loss 0.0075989, acc 1
2016-09-06T05:12:22.503386: step 4075, loss 0.0705687, acc 0.94
2016-09-06T05:12:23.295687: step 4076, loss 0.0260562, acc 0.98
2016-09-06T05:12:24.080260: step 4077, loss 0.00394664, acc 1
2016-09-06T05:12:24.878758: step 4078, loss 0.0318211, acc 0.98
2016-09-06T05:12:25.688258: step 4079, loss 0.0290748, acc 1
2016-09-06T05:12:26.475383: step 4080, loss 0.0724821, acc 0.96
2016-09-06T05:12:27.271182: step 4081, loss 0.0587394, acc 0.96
2016-09-06T05:12:28.087936: step 4082, loss 0.0315242, acc 0.98
2016-09-06T05:12:28.869476: step 4083, loss 0.0284454, acc 1
2016-09-06T05:12:29.675774: step 4084, loss 0.037748, acc 0.96
2016-09-06T05:12:30.505724: step 4085, loss 0.0696202, acc 0.94
2016-09-06T05:12:31.290834: step 4086, loss 0.0060186, acc 1
2016-09-06T05:12:32.145397: step 4087, loss 0.0306443, acc 0.98
2016-09-06T05:12:32.982323: step 4088, loss 0.0133174, acc 1
2016-09-06T05:12:33.743212: step 4089, loss 0.0112323, acc 1
2016-09-06T05:12:34.567833: step 4090, loss 0.0227072, acc 1
2016-09-06T05:12:35.370871: step 4091, loss 0.00554764, acc 1
2016-09-06T05:12:36.164384: step 4092, loss 0.0102317, acc 1
2016-09-06T05:12:37.009052: step 4093, loss 0.0248477, acc 1
2016-09-06T05:12:37.817645: step 4094, loss 0.00839871, acc 1
2016-09-06T05:12:38.611465: step 4095, loss 0.00396136, acc 1
2016-09-06T05:12:39.400881: step 4096, loss 0.0216185, acc 1
2016-09-06T05:12:40.250818: step 4097, loss 0.0198693, acc 0.98
2016-09-06T05:12:41.034621: step 4098, loss 0.0302831, acc 0.98
2016-09-06T05:12:41.855121: step 4099, loss 0.0182797, acc 0.98
2016-09-06T05:12:42.670798: step 4100, loss 0.084521, acc 0.96

Evaluation:
2016-09-06T05:12:46.372379: step 4100, loss 1.90141, acc 0.755159

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-4100

2016-09-06T05:12:48.259023: step 4101, loss 0.104562, acc 0.98
2016-09-06T05:12:49.137553: step 4102, loss 0.0314028, acc 0.98
2016-09-06T05:12:49.945388: step 4103, loss 0.0657462, acc 0.96
2016-09-06T05:12:50.754023: step 4104, loss 0.00487834, acc 1
2016-09-06T05:12:51.575588: step 4105, loss 0.0668273, acc 0.98
2016-09-06T05:12:52.392480: step 4106, loss 0.0458018, acc 0.98
2016-09-06T05:12:53.198551: step 4107, loss 0.0110358, acc 1
2016-09-06T05:12:53.990224: step 4108, loss 0.0244466, acc 0.98
2016-09-06T05:12:54.809876: step 4109, loss 0.0168056, acc 1
2016-09-06T05:12:55.594525: step 4110, loss 0.0241904, acc 1
2016-09-06T05:12:56.415759: step 4111, loss 0.069553, acc 0.98
2016-09-06T05:12:57.313188: step 4112, loss 0.0356846, acc 0.96
2016-09-06T05:12:58.108568: step 4113, loss 0.0556356, acc 0.98
2016-09-06T05:12:58.916449: step 4114, loss 0.00469384, acc 1
2016-09-06T05:12:59.731020: step 4115, loss 0.0217422, acc 0.98
2016-09-06T05:13:00.550515: step 4116, loss 0.0109779, acc 1
2016-09-06T05:13:01.348700: step 4117, loss 0.056111, acc 0.94
2016-09-06T05:13:02.162959: step 4118, loss 0.028878, acc 0.98
2016-09-06T05:13:02.948430: step 4119, loss 0.00505984, acc 1
2016-09-06T05:13:03.765012: step 4120, loss 0.0195719, acc 1
2016-09-06T05:13:04.606236: step 4121, loss 0.0471161, acc 0.96
2016-09-06T05:13:05.401219: step 4122, loss 0.0168833, acc 1
2016-09-06T05:13:06.237460: step 4123, loss 0.0191751, acc 1
2016-09-06T05:13:07.084386: step 4124, loss 0.00517693, acc 1
2016-09-06T05:13:07.889547: step 4125, loss 0.038063, acc 0.96
2016-09-06T05:13:08.699719: step 4126, loss 0.102915, acc 0.98
2016-09-06T05:13:09.532005: step 4127, loss 0.0314897, acc 1
2016-09-06T05:13:10.349964: step 4128, loss 0.0529239, acc 0.96
2016-09-06T05:13:11.185285: step 4129, loss 0.00526056, acc 1
2016-09-06T05:13:12.007952: step 4130, loss 0.0205444, acc 1
2016-09-06T05:13:12.800855: step 4131, loss 0.00672533, acc 1
2016-09-06T05:13:13.630991: step 4132, loss 0.0495412, acc 0.96
2016-09-06T05:13:14.457503: step 4133, loss 0.0430564, acc 0.96
2016-09-06T05:13:15.287697: step 4134, loss 0.0118531, acc 1
2016-09-06T05:13:16.088436: step 4135, loss 0.0195326, acc 0.98
2016-09-06T05:13:16.928215: step 4136, loss 0.00974689, acc 1
2016-09-06T05:13:17.740124: step 4137, loss 0.0722044, acc 0.98
2016-09-06T05:13:18.570391: step 4138, loss 0.0143097, acc 1
2016-09-06T05:13:19.424330: step 4139, loss 0.0771943, acc 0.98
2016-09-06T05:13:20.259235: step 4140, loss 0.0231752, acc 1
2016-09-06T05:13:21.060174: step 4141, loss 0.0315979, acc 1
2016-09-06T05:13:21.862624: step 4142, loss 0.0222059, acc 1
2016-09-06T05:13:22.668358: step 4143, loss 0.00610263, acc 1
2016-09-06T05:13:23.437277: step 4144, loss 0.0418011, acc 1
2016-09-06T05:13:24.229771: step 4145, loss 0.0842595, acc 0.94
2016-09-06T05:13:25.044491: step 4146, loss 0.00965503, acc 1
2016-09-06T05:13:25.833043: step 4147, loss 0.128057, acc 0.98
2016-09-06T05:13:26.629954: step 4148, loss 0.0661724, acc 0.96
2016-09-06T05:13:27.442892: step 4149, loss 0.0374979, acc 0.98
2016-09-06T05:13:28.220478: step 4150, loss 0.0393177, acc 0.98
2016-09-06T05:13:29.025927: step 4151, loss 0.100435, acc 0.94
2016-09-06T05:13:29.819485: step 4152, loss 0.0143756, acc 1
2016-09-06T05:13:30.635039: step 4153, loss 0.066591, acc 0.98
2016-09-06T05:13:31.474862: step 4154, loss 0.00634954, acc 1
2016-09-06T05:13:32.274409: step 4155, loss 0.0246614, acc 1
2016-09-06T05:13:33.082871: step 4156, loss 0.0156054, acc 1
2016-09-06T05:13:33.872619: step 4157, loss 0.0332587, acc 0.98
2016-09-06T05:13:34.680461: step 4158, loss 0.018633, acc 1
2016-09-06T05:13:35.510610: step 4159, loss 0.0305266, acc 0.98
2016-09-06T05:13:36.327463: step 4160, loss 0.0140742, acc 1
2016-09-06T05:13:37.168055: step 4161, loss 0.0311459, acc 0.98
2016-09-06T05:13:37.933399: step 4162, loss 0.0333513, acc 0.98
2016-09-06T05:13:38.730634: step 4163, loss 0.027991, acc 1
2016-09-06T05:13:39.542160: step 4164, loss 0.0527564, acc 1
2016-09-06T05:13:40.336592: step 4165, loss 0.0592523, acc 0.96
2016-09-06T05:13:41.141780: step 4166, loss 0.0553034, acc 0.96
2016-09-06T05:13:41.939907: step 4167, loss 0.0537838, acc 0.98
2016-09-06T05:13:42.715630: step 4168, loss 0.0102288, acc 1
2016-09-06T05:13:43.526077: step 4169, loss 0.052362, acc 0.98
2016-09-06T05:13:44.336772: step 4170, loss 0.0260611, acc 0.98
2016-09-06T05:13:45.147920: step 4171, loss 0.00439289, acc 1
2016-09-06T05:13:45.954222: step 4172, loss 0.0129212, acc 1
2016-09-06T05:13:46.749474: step 4173, loss 0.0703106, acc 0.96
2016-09-06T05:13:47.568683: step 4174, loss 0.0252991, acc 0.98
2016-09-06T05:13:48.385313: step 4175, loss 0.0104105, acc 1
2016-09-06T05:13:49.207642: step 4176, loss 0.0890879, acc 0.98
2016-09-06T05:13:50.007697: step 4177, loss 0.049711, acc 0.98
2016-09-06T05:13:50.822282: step 4178, loss 0.0353208, acc 0.98
2016-09-06T05:13:51.641461: step 4179, loss 0.0651197, acc 0.98
2016-09-06T05:13:52.452104: step 4180, loss 0.0550644, acc 0.96
2016-09-06T05:13:53.248333: step 4181, loss 0.00531551, acc 1
2016-09-06T05:13:54.104589: step 4182, loss 0.0376324, acc 0.98
2016-09-06T05:13:54.914956: step 4183, loss 0.0568258, acc 0.94
2016-09-06T05:13:55.726961: step 4184, loss 0.00621216, acc 1
2016-09-06T05:13:56.537116: step 4185, loss 0.0105211, acc 1
2016-09-06T05:13:57.328178: step 4186, loss 0.049585, acc 0.98
2016-09-06T05:13:58.144494: step 4187, loss 0.0514729, acc 0.98
2016-09-06T05:13:58.953580: step 4188, loss 0.0506936, acc 0.98
2016-09-06T05:13:59.755125: step 4189, loss 0.0351556, acc 0.98
2016-09-06T05:14:00.580919: step 4190, loss 0.0289848, acc 0.98
2016-09-06T05:14:01.382468: step 4191, loss 0.154337, acc 0.96
2016-09-06T05:14:02.176855: step 4192, loss 0.11016, acc 0.98
2016-09-06T05:14:02.993705: step 4193, loss 0.0347272, acc 1
2016-09-06T05:14:03.817066: step 4194, loss 0.0914511, acc 0.92
2016-09-06T05:14:04.642076: step 4195, loss 0.0178526, acc 1
2016-09-06T05:14:05.464770: step 4196, loss 0.0343655, acc 0.98
2016-09-06T05:14:06.314850: step 4197, loss 0.0312378, acc 0.98
2016-09-06T05:14:07.121693: step 4198, loss 0.0371982, acc 0.96
2016-09-06T05:14:07.933689: step 4199, loss 0.0190699, acc 1
2016-09-06T05:14:08.743417: step 4200, loss 0.0837903, acc 0.98

Evaluation:
2016-09-06T05:14:12.496702: step 4200, loss 1.435, acc 0.753283

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-4200

2016-09-06T05:14:14.336165: step 4201, loss 0.0781069, acc 0.98
2016-09-06T05:14:15.153658: step 4202, loss 0.0674341, acc 0.96
2016-09-06T05:14:15.973565: step 4203, loss 0.0230747, acc 1
2016-09-06T05:14:16.742970: step 4204, loss 0.0291044, acc 1
2016-09-06T05:14:17.535824: step 4205, loss 0.0732623, acc 0.98
2016-09-06T05:14:18.377400: step 4206, loss 0.00681061, acc 1
2016-09-06T05:14:19.159716: step 4207, loss 0.0248485, acc 1
2016-09-06T05:14:20.008402: step 4208, loss 0.0199881, acc 1
2016-09-06T05:14:20.837402: step 4209, loss 0.027462, acc 1
2016-09-06T05:14:21.675539: step 4210, loss 0.0935125, acc 0.96
2016-09-06T05:14:22.484699: step 4211, loss 0.0283757, acc 1
2016-09-06T05:14:23.313951: step 4212, loss 0.0514637, acc 0.98
2016-09-06T05:14:24.127720: step 4213, loss 0.00823649, acc 1
2016-09-06T05:14:24.927962: step 4214, loss 0.0602517, acc 0.98
2016-09-06T05:14:25.773495: step 4215, loss 0.00447214, acc 1
2016-09-06T05:14:26.620455: step 4216, loss 0.0451436, acc 0.98
2016-09-06T05:14:27.439533: step 4217, loss 0.0269606, acc 0.98
2016-09-06T05:14:28.274440: step 4218, loss 0.0304098, acc 0.98
2016-09-06T05:14:29.087311: step 4219, loss 0.0379816, acc 1
2016-09-06T05:14:29.899924: step 4220, loss 0.00772735, acc 1
2016-09-06T05:14:30.711229: step 4221, loss 0.085446, acc 0.94
2016-09-06T05:14:31.527906: step 4222, loss 0.0915142, acc 0.98
2016-09-06T05:14:32.323868: step 4223, loss 0.0510507, acc 0.96
2016-09-06T05:14:33.135657: step 4224, loss 0.0529286, acc 0.977273
2016-09-06T05:14:33.949419: step 4225, loss 0.0803759, acc 0.96
2016-09-06T05:14:34.742418: step 4226, loss 0.0414085, acc 1
2016-09-06T05:14:35.558558: step 4227, loss 0.0417736, acc 1
2016-09-06T05:14:36.359360: step 4228, loss 0.00813413, acc 1
2016-09-06T05:14:37.149524: step 4229, loss 0.0759068, acc 0.96
2016-09-06T05:14:37.980318: step 4230, loss 0.0438763, acc 1
2016-09-06T05:14:38.788305: step 4231, loss 0.0286614, acc 1
2016-09-06T05:14:39.561019: step 4232, loss 0.0358483, acc 0.98
2016-09-06T05:14:40.370326: step 4233, loss 0.0319348, acc 0.98
2016-09-06T05:14:41.181040: step 4234, loss 0.0242753, acc 0.98
2016-09-06T05:14:41.988760: step 4235, loss 0.0154887, acc 1
2016-09-06T05:14:42.825295: step 4236, loss 0.00527988, acc 1
2016-09-06T05:14:43.673621: step 4237, loss 0.0579972, acc 0.96
2016-09-06T05:14:44.460291: step 4238, loss 0.114063, acc 0.96
2016-09-06T05:14:45.289525: step 4239, loss 0.0485054, acc 0.94
2016-09-06T05:14:46.115255: step 4240, loss 0.0383842, acc 0.98
2016-09-06T05:14:46.912986: step 4241, loss 0.0501347, acc 0.98
2016-09-06T05:14:47.739764: step 4242, loss 0.0154997, acc 1
2016-09-06T05:14:48.566176: step 4243, loss 0.0133241, acc 1
2016-09-06T05:14:49.364236: step 4244, loss 0.0177923, acc 1
2016-09-06T05:14:50.165978: step 4245, loss 0.0300949, acc 0.98
2016-09-06T05:14:50.987536: step 4246, loss 0.028507, acc 0.98
2016-09-06T05:14:51.803341: step 4247, loss 0.0292409, acc 0.98
2016-09-06T05:14:52.652586: step 4248, loss 0.0184982, acc 0.98
2016-09-06T05:14:53.488315: step 4249, loss 0.0264606, acc 1
2016-09-06T05:14:54.299834: step 4250, loss 0.0432091, acc 0.98
2016-09-06T05:14:55.102061: step 4251, loss 0.0512743, acc 0.98
2016-09-06T05:14:55.931069: step 4252, loss 0.0327789, acc 0.98
2016-09-06T05:14:56.740343: step 4253, loss 0.0369163, acc 1
2016-09-06T05:14:57.563762: step 4254, loss 0.0206086, acc 1
2016-09-06T05:14:58.407617: step 4255, loss 0.0362474, acc 0.98
2016-09-06T05:14:59.228456: step 4256, loss 0.0369452, acc 0.98
2016-09-06T05:15:00.037860: step 4257, loss 0.0382486, acc 0.98
2016-09-06T05:15:00.911821: step 4258, loss 0.0257995, acc 0.98
2016-09-06T05:15:01.747841: step 4259, loss 0.00902312, acc 1
2016-09-06T05:15:02.563932: step 4260, loss 0.00739258, acc 1
2016-09-06T05:15:03.377312: step 4261, loss 0.0113208, acc 1
2016-09-06T05:15:04.213228: step 4262, loss 0.0572147, acc 0.96
2016-09-06T05:15:05.037736: step 4263, loss 0.0445286, acc 0.98
2016-09-06T05:15:05.841518: step 4264, loss 0.0179075, acc 1
2016-09-06T05:15:06.632063: step 4265, loss 0.0539469, acc 0.96
2016-09-06T05:15:07.467935: step 4266, loss 0.0443207, acc 0.98
2016-09-06T05:15:08.281203: step 4267, loss 0.0408936, acc 0.98
2016-09-06T05:15:09.107182: step 4268, loss 0.0104564, acc 1
2016-09-06T05:15:09.912592: step 4269, loss 0.0198608, acc 1
2016-09-06T05:15:10.747719: step 4270, loss 0.0226655, acc 0.98
2016-09-06T05:15:11.600428: step 4271, loss 0.0520676, acc 0.96
2016-09-06T05:15:12.408900: step 4272, loss 0.144403, acc 0.94
2016-09-06T05:15:13.233968: step 4273, loss 0.0612278, acc 0.98
2016-09-06T05:15:14.062542: step 4274, loss 0.0309794, acc 0.98
2016-09-06T05:15:14.888361: step 4275, loss 0.0555613, acc 0.98
2016-09-06T05:15:15.726128: step 4276, loss 0.0790968, acc 0.96
2016-09-06T05:15:16.540067: step 4277, loss 0.124305, acc 0.94
2016-09-06T05:15:17.363804: step 4278, loss 0.00526747, acc 1
2016-09-06T05:15:18.183625: step 4279, loss 0.0713797, acc 0.98
2016-09-06T05:15:19.003090: step 4280, loss 0.0393963, acc 0.98
2016-09-06T05:15:19.824114: step 4281, loss 0.0206794, acc 1
2016-09-06T05:15:20.641416: step 4282, loss 0.0548839, acc 0.98
2016-09-06T05:15:21.499154: step 4283, loss 0.0579101, acc 0.98
2016-09-06T05:15:22.300692: step 4284, loss 0.0170606, acc 1
2016-09-06T05:15:23.110035: step 4285, loss 0.0770062, acc 0.94
2016-09-06T05:15:23.920205: step 4286, loss 0.00870462, acc 1
2016-09-06T05:15:24.760291: step 4287, loss 0.00444092, acc 1
2016-09-06T05:15:25.545049: step 4288, loss 0.0286818, acc 0.98
2016-09-06T05:15:26.351797: step 4289, loss 0.111012, acc 0.92
2016-09-06T05:15:27.187106: step 4290, loss 0.0561412, acc 1
2016-09-06T05:15:27.993243: step 4291, loss 0.0188992, acc 1
2016-09-06T05:15:28.815106: step 4292, loss 0.0730639, acc 0.96
2016-09-06T05:15:29.622482: step 4293, loss 0.169842, acc 0.94
2016-09-06T05:15:30.414946: step 4294, loss 0.0128316, acc 1
2016-09-06T05:15:31.205726: step 4295, loss 0.0231901, acc 1
2016-09-06T05:15:32.050994: step 4296, loss 0.0183081, acc 1
2016-09-06T05:15:32.847237: step 4297, loss 0.0398221, acc 0.98
2016-09-06T05:15:33.636616: step 4298, loss 0.0172954, acc 1
2016-09-06T05:15:34.471697: step 4299, loss 0.0243822, acc 1
2016-09-06T05:15:35.264195: step 4300, loss 0.0123934, acc 1

Evaluation:
2016-09-06T05:15:38.969370: step 4300, loss 1.58381, acc 0.744841

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-4300

2016-09-06T05:15:40.962797: step 4301, loss 0.053076, acc 0.98
2016-09-06T05:15:41.801870: step 4302, loss 0.0341804, acc 0.98
2016-09-06T05:15:42.606679: step 4303, loss 0.100389, acc 0.94
2016-09-06T05:15:43.434682: step 4304, loss 0.00800765, acc 1
2016-09-06T05:15:44.255567: step 4305, loss 0.00314406, acc 1
2016-09-06T05:15:45.078748: step 4306, loss 0.0131193, acc 1
2016-09-06T05:15:45.899035: step 4307, loss 0.0279062, acc 1
2016-09-06T05:15:46.760982: step 4308, loss 0.0197496, acc 1
2016-09-06T05:15:47.562404: step 4309, loss 0.0209648, acc 1
2016-09-06T05:15:48.360804: step 4310, loss 0.0221721, acc 1
2016-09-06T05:15:49.200697: step 4311, loss 0.022616, acc 0.98
2016-09-06T05:15:50.023665: step 4312, loss 0.0280896, acc 0.98
2016-09-06T05:15:50.841801: step 4313, loss 0.0559702, acc 0.98
2016-09-06T05:15:51.678832: step 4314, loss 0.017527, acc 1
2016-09-06T05:15:52.489445: step 4315, loss 0.0151072, acc 1
2016-09-06T05:15:53.282474: step 4316, loss 0.153591, acc 0.96
2016-09-06T05:15:54.128050: step 4317, loss 0.0417172, acc 0.98
2016-09-06T05:15:54.940574: step 4318, loss 0.012613, acc 1
2016-09-06T05:15:55.749935: step 4319, loss 0.0714207, acc 0.98
2016-09-06T05:15:56.591198: step 4320, loss 0.0213599, acc 1
2016-09-06T05:15:57.409983: step 4321, loss 0.0731913, acc 0.98
2016-09-06T05:15:58.240596: step 4322, loss 0.0525648, acc 0.98
2016-09-06T05:15:59.081783: step 4323, loss 0.0290377, acc 0.98
2016-09-06T05:15:59.906928: step 4324, loss 0.0234888, acc 1
2016-09-06T05:16:00.706962: step 4325, loss 0.0428037, acc 0.98
2016-09-06T05:16:01.527321: step 4326, loss 0.00493937, acc 1
2016-09-06T05:16:02.352565: step 4327, loss 0.00563611, acc 1
2016-09-06T05:16:03.150317: step 4328, loss 0.025244, acc 1
2016-09-06T05:16:03.950744: step 4329, loss 0.160498, acc 0.92
2016-09-06T05:16:04.752418: step 4330, loss 0.0141155, acc 1
2016-09-06T05:16:05.534327: step 4331, loss 0.0468556, acc 0.96
2016-09-06T05:16:06.357790: step 4332, loss 0.0142928, acc 1
2016-09-06T05:16:07.175577: step 4333, loss 0.0218263, acc 1
2016-09-06T05:16:07.957208: step 4334, loss 0.0333201, acc 0.98
2016-09-06T05:16:08.761991: step 4335, loss 0.012869, acc 1
2016-09-06T05:16:09.582672: step 4336, loss 0.0336607, acc 0.98
2016-09-06T05:16:10.362343: step 4337, loss 0.0315581, acc 1
2016-09-06T05:16:11.251079: step 4338, loss 0.0448439, acc 0.96
2016-09-06T05:16:12.054645: step 4339, loss 0.0374292, acc 0.98
2016-09-06T05:16:12.833682: step 4340, loss 0.0238242, acc 1
2016-09-06T05:16:13.655659: step 4341, loss 0.0596823, acc 0.98
2016-09-06T05:16:14.470987: step 4342, loss 0.0224285, acc 0.98
2016-09-06T05:16:15.257401: step 4343, loss 0.142726, acc 0.96
2016-09-06T05:16:16.044393: step 4344, loss 0.0183958, acc 0.98
2016-09-06T05:16:16.867085: step 4345, loss 0.00351473, acc 1
2016-09-06T05:16:17.673905: step 4346, loss 0.0165474, acc 1
2016-09-06T05:16:18.473356: step 4347, loss 0.0123565, acc 1
2016-09-06T05:16:19.288241: step 4348, loss 0.0126858, acc 1
2016-09-06T05:16:20.070159: step 4349, loss 0.0473096, acc 0.98
2016-09-06T05:16:20.897070: step 4350, loss 0.0285217, acc 0.98
2016-09-06T05:16:21.726433: step 4351, loss 0.0261049, acc 0.98
2016-09-06T05:16:22.519757: step 4352, loss 0.0221993, acc 0.98
2016-09-06T05:16:23.320587: step 4353, loss 0.0395791, acc 1
2016-09-06T05:16:24.125600: step 4354, loss 0.0316871, acc 0.98
2016-09-06T05:16:24.918046: step 4355, loss 0.0441589, acc 0.96
2016-09-06T05:16:25.698969: step 4356, loss 0.00669853, acc 1
2016-09-06T05:16:26.524331: step 4357, loss 0.0322776, acc 0.98
2016-09-06T05:16:27.302331: step 4358, loss 0.0637311, acc 0.96
2016-09-06T05:16:28.136875: step 4359, loss 0.0359369, acc 0.96
2016-09-06T05:16:28.969293: step 4360, loss 0.0212359, acc 0.98
2016-09-06T05:16:29.774077: step 4361, loss 0.0926751, acc 0.96
2016-09-06T05:16:30.584517: step 4362, loss 0.0693075, acc 0.96
2016-09-06T05:16:31.411120: step 4363, loss 0.0118151, acc 1
2016-09-06T05:16:32.227944: step 4364, loss 0.0757071, acc 0.96
2016-09-06T05:16:33.033800: step 4365, loss 0.0502159, acc 0.96
2016-09-06T05:16:33.854890: step 4366, loss 0.0148958, acc 1
2016-09-06T05:16:34.689412: step 4367, loss 0.0276306, acc 1
2016-09-06T05:16:35.498883: step 4368, loss 0.0164776, acc 1
2016-09-06T05:16:36.330498: step 4369, loss 0.0209911, acc 1
2016-09-06T05:16:37.167296: step 4370, loss 0.0281073, acc 1
2016-09-06T05:16:37.976218: step 4371, loss 0.0479752, acc 0.98
2016-09-06T05:16:38.794605: step 4372, loss 0.0418931, acc 0.98
2016-09-06T05:16:39.629382: step 4373, loss 0.0285361, acc 1
2016-09-06T05:16:40.454455: step 4374, loss 0.0814488, acc 0.98
2016-09-06T05:16:41.277494: step 4375, loss 0.0308294, acc 1
2016-09-06T05:16:42.090996: step 4376, loss 0.094501, acc 0.94
2016-09-06T05:16:42.916928: step 4377, loss 0.00773994, acc 1
2016-09-06T05:16:43.740052: step 4378, loss 0.085565, acc 0.96
2016-09-06T05:16:44.577340: step 4379, loss 0.023859, acc 0.98
2016-09-06T05:16:45.389402: step 4380, loss 0.00512265, acc 1
2016-09-06T05:16:46.213784: step 4381, loss 0.032071, acc 0.98
2016-09-06T05:16:47.023863: step 4382, loss 0.0362592, acc 0.98
2016-09-06T05:16:47.841068: step 4383, loss 0.0170696, acc 1
2016-09-06T05:16:48.657191: step 4384, loss 0.0179388, acc 1
2016-09-06T05:16:49.485783: step 4385, loss 0.0246576, acc 1
2016-09-06T05:16:50.312099: step 4386, loss 0.0687654, acc 0.98
2016-09-06T05:16:51.117225: step 4387, loss 0.0131617, acc 1
2016-09-06T05:16:51.974399: step 4388, loss 0.048901, acc 0.96
2016-09-06T05:16:52.793258: step 4389, loss 0.0734073, acc 0.98
2016-09-06T05:16:53.618371: step 4390, loss 0.0703559, acc 0.98
2016-09-06T05:16:54.447699: step 4391, loss 0.079467, acc 0.96
2016-09-06T05:16:55.246753: step 4392, loss 0.111543, acc 0.98
2016-09-06T05:16:56.050869: step 4393, loss 0.0410281, acc 0.98
2016-09-06T05:16:56.876460: step 4394, loss 0.0487921, acc 0.98
2016-09-06T05:16:57.700541: step 4395, loss 0.0654251, acc 0.98
2016-09-06T05:16:58.543212: step 4396, loss 0.0453206, acc 0.98
2016-09-06T05:16:59.416319: step 4397, loss 0.0111639, acc 1
2016-09-06T05:17:00.251374: step 4398, loss 0.0272621, acc 0.98
2016-09-06T05:17:01.062351: step 4399, loss 0.0524904, acc 0.96
2016-09-06T05:17:01.900885: step 4400, loss 0.0175249, acc 1

Evaluation:
2016-09-06T05:17:05.634261: step 4400, loss 1.6846, acc 0.758912

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-4400

2016-09-06T05:17:07.524169: step 4401, loss 0.0268271, acc 1
2016-09-06T05:17:08.343607: step 4402, loss 0.151342, acc 0.96
2016-09-06T05:17:09.206243: step 4403, loss 0.0124346, acc 1
2016-09-06T05:17:10.011838: step 4404, loss 0.0515771, acc 0.98
2016-09-06T05:17:10.831482: step 4405, loss 0.0454293, acc 0.98
2016-09-06T05:17:11.665273: step 4406, loss 0.00982108, acc 1
2016-09-06T05:17:12.487354: step 4407, loss 0.0108329, acc 1
2016-09-06T05:17:13.338328: step 4408, loss 0.0150368, acc 1
2016-09-06T05:17:14.137903: step 4409, loss 0.00560925, acc 1
2016-09-06T05:17:14.949635: step 4410, loss 0.025262, acc 0.98
2016-09-06T05:17:15.756589: step 4411, loss 0.0592909, acc 0.98
2016-09-06T05:17:16.602123: step 4412, loss 0.0409459, acc 0.98
2016-09-06T05:17:17.412811: step 4413, loss 0.11659, acc 0.94
2016-09-06T05:17:18.184898: step 4414, loss 0.0894011, acc 0.98
2016-09-06T05:17:19.011429: step 4415, loss 0.0744274, acc 0.96
2016-09-06T05:17:19.780215: step 4416, loss 0.00973535, acc 1
2016-09-06T05:17:20.558222: step 4417, loss 0.00922993, acc 1
2016-09-06T05:17:21.379063: step 4418, loss 0.0598488, acc 0.96
2016-09-06T05:17:22.196921: step 4419, loss 0.0136445, acc 1
2016-09-06T05:17:22.960506: step 4420, loss 0.0496425, acc 0.96
2016-09-06T05:17:23.784168: step 4421, loss 0.00498198, acc 1
2016-09-06T05:17:24.587057: step 4422, loss 0.0407582, acc 0.98
2016-09-06T05:17:25.402029: step 4423, loss 0.00611696, acc 1
2016-09-06T05:17:26.194614: step 4424, loss 0.0233045, acc 0.98
2016-09-06T05:17:27.006788: step 4425, loss 0.0430392, acc 0.96
2016-09-06T05:17:27.805864: step 4426, loss 0.0512153, acc 0.98
2016-09-06T05:17:28.631126: step 4427, loss 0.0712254, acc 0.96
2016-09-06T05:17:29.443135: step 4428, loss 0.0861244, acc 0.98
2016-09-06T05:17:30.231705: step 4429, loss 0.0194981, acc 1
2016-09-06T05:17:31.081754: step 4430, loss 0.0298421, acc 0.98
2016-09-06T05:17:31.891902: step 4431, loss 0.15392, acc 0.94
2016-09-06T05:17:32.701716: step 4432, loss 0.00717492, acc 1
2016-09-06T05:17:33.502463: step 4433, loss 0.035914, acc 0.98
2016-09-06T05:17:34.343182: step 4434, loss 0.00452856, acc 1
2016-09-06T05:17:35.115119: step 4435, loss 0.0502603, acc 0.98
2016-09-06T05:17:35.902640: step 4436, loss 0.0105083, acc 1
2016-09-06T05:17:36.730161: step 4437, loss 0.0721125, acc 0.98
2016-09-06T05:17:37.526117: step 4438, loss 0.069889, acc 0.94
2016-09-06T05:17:38.354594: step 4439, loss 0.0410943, acc 0.98
2016-09-06T05:17:39.189324: step 4440, loss 0.0733253, acc 0.96
2016-09-06T05:17:39.981307: step 4441, loss 0.0277289, acc 0.98
2016-09-06T05:17:40.786598: step 4442, loss 0.050638, acc 1
2016-09-06T05:17:41.607823: step 4443, loss 0.0211532, acc 1
2016-09-06T05:17:42.437233: step 4444, loss 0.023723, acc 0.98
2016-09-06T05:17:43.238846: step 4445, loss 0.0585433, acc 0.98
2016-09-06T05:17:44.086058: step 4446, loss 0.0163087, acc 1
2016-09-06T05:17:44.911547: step 4447, loss 0.0472568, acc 0.98
2016-09-06T05:17:45.729398: step 4448, loss 0.0245367, acc 1
2016-09-06T05:17:46.563813: step 4449, loss 0.0338669, acc 1
2016-09-06T05:17:47.410979: step 4450, loss 0.0728369, acc 0.98
2016-09-06T05:17:48.224326: step 4451, loss 0.0159228, acc 1
2016-09-06T05:17:49.059741: step 4452, loss 0.0354293, acc 0.98
2016-09-06T05:17:49.885222: step 4453, loss 0.0218799, acc 0.98
2016-09-06T05:17:50.712994: step 4454, loss 0.0344679, acc 1
2016-09-06T05:17:51.543532: step 4455, loss 0.0438786, acc 0.96
2016-09-06T05:17:52.364971: step 4456, loss 0.0218148, acc 1
2016-09-06T05:17:53.165448: step 4457, loss 0.122242, acc 0.98
2016-09-06T05:17:54.008769: step 4458, loss 0.124824, acc 0.96
2016-09-06T05:17:54.808842: step 4459, loss 0.0218469, acc 1
2016-09-06T05:17:55.596494: step 4460, loss 0.00374462, acc 1
2016-09-06T05:17:56.412274: step 4461, loss 0.0354212, acc 1
2016-09-06T05:17:57.222051: step 4462, loss 0.0145261, acc 1
2016-09-06T05:17:58.069319: step 4463, loss 0.0188305, acc 1
2016-09-06T05:17:58.867520: step 4464, loss 0.0230276, acc 1
2016-09-06T05:17:59.688825: step 4465, loss 0.0556246, acc 0.96
2016-09-06T05:18:00.520313: step 4466, loss 0.0184735, acc 1
2016-09-06T05:18:01.308769: step 4467, loss 0.00960737, acc 1
2016-09-06T05:18:02.122152: step 4468, loss 0.084434, acc 0.94
2016-09-06T05:18:02.905659: step 4469, loss 0.00688354, acc 1
2016-09-06T05:18:03.708179: step 4470, loss 0.0486806, acc 0.98
2016-09-06T05:18:04.530709: step 4471, loss 0.0118819, acc 1
2016-09-06T05:18:05.321932: step 4472, loss 0.0242723, acc 1
2016-09-06T05:18:06.106254: step 4473, loss 0.0236065, acc 1
2016-09-06T05:18:06.920967: step 4474, loss 0.0168966, acc 1
2016-09-06T05:18:07.703654: step 4475, loss 0.10397, acc 0.96
2016-09-06T05:18:08.504826: step 4476, loss 0.018397, acc 1
2016-09-06T05:18:09.339922: step 4477, loss 0.0527269, acc 0.98
2016-09-06T05:18:10.120621: step 4478, loss 0.00953423, acc 1
2016-09-06T05:18:10.936181: step 4479, loss 0.0157315, acc 1
2016-09-06T05:18:11.751255: step 4480, loss 0.0411074, acc 0.98
2016-09-06T05:18:12.570226: step 4481, loss 0.0140059, acc 1
2016-09-06T05:18:13.386152: step 4482, loss 0.0212887, acc 1
2016-09-06T05:18:14.201465: step 4483, loss 0.00886196, acc 1
2016-09-06T05:18:15.001104: step 4484, loss 0.0150288, acc 1
2016-09-06T05:18:15.814661: step 4485, loss 0.0608076, acc 0.98
2016-09-06T05:18:16.651154: step 4486, loss 0.026769, acc 1
2016-09-06T05:18:17.415455: step 4487, loss 0.00543173, acc 1
2016-09-06T05:18:18.225661: step 4488, loss 0.0167328, acc 1
2016-09-06T05:18:19.057308: step 4489, loss 0.0548338, acc 0.98
2016-09-06T05:18:19.840504: step 4490, loss 0.0309477, acc 0.98
2016-09-06T05:18:20.636264: step 4491, loss 0.0379362, acc 0.98
2016-09-06T05:18:21.471800: step 4492, loss 0.0359753, acc 0.98
2016-09-06T05:18:22.239134: step 4493, loss 0.0655787, acc 0.96
2016-09-06T05:18:23.037803: step 4494, loss 0.0310306, acc 1
2016-09-06T05:18:23.851877: step 4495, loss 0.028594, acc 1
2016-09-06T05:18:24.658117: step 4496, loss 0.0094976, acc 1
2016-09-06T05:18:25.461921: step 4497, loss 0.00348347, acc 1
2016-09-06T05:18:26.267157: step 4498, loss 0.0150476, acc 1
2016-09-06T05:18:27.050912: step 4499, loss 0.0492781, acc 0.96
2016-09-06T05:18:27.852253: step 4500, loss 0.011167, acc 1

Evaluation:
2016-09-06T05:18:31.601432: step 4500, loss 2.21963, acc 0.733584

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-4500

2016-09-06T05:18:33.435503: step 4501, loss 0.0506898, acc 0.98
2016-09-06T05:18:34.250122: step 4502, loss 0.0942978, acc 0.96
2016-09-06T05:18:35.099771: step 4503, loss 0.0175134, acc 1
2016-09-06T05:18:35.911455: step 4504, loss 0.0233147, acc 1
2016-09-06T05:18:36.710966: step 4505, loss 0.0441761, acc 0.98
2016-09-06T05:18:37.529915: step 4506, loss 0.0818203, acc 0.96
2016-09-06T05:18:38.347075: step 4507, loss 0.0328748, acc 0.98
2016-09-06T05:18:39.168401: step 4508, loss 0.0111682, acc 1
2016-09-06T05:18:40.023308: step 4509, loss 0.0436917, acc 1
2016-09-06T05:18:40.831698: step 4510, loss 0.031644, acc 0.98
2016-09-06T05:18:41.626558: step 4511, loss 0.0186049, acc 1
2016-09-06T05:18:42.449902: step 4512, loss 0.00626555, acc 1
2016-09-06T05:18:43.289456: step 4513, loss 0.00776929, acc 1
2016-09-06T05:18:44.090059: step 4514, loss 0.0364202, acc 0.98
2016-09-06T05:18:44.896982: step 4515, loss 0.0678764, acc 0.94
2016-09-06T05:18:45.714291: step 4516, loss 0.0358886, acc 0.96
2016-09-06T05:18:46.519628: step 4517, loss 0.0165129, acc 1
2016-09-06T05:18:47.329877: step 4518, loss 0.00354298, acc 1
2016-09-06T05:18:48.136960: step 4519, loss 0.0213744, acc 0.98
2016-09-06T05:18:48.908959: step 4520, loss 0.010008, acc 1
2016-09-06T05:18:49.724423: step 4521, loss 0.0125035, acc 1
2016-09-06T05:18:50.528728: step 4522, loss 0.020652, acc 0.98
2016-09-06T05:18:51.337664: step 4523, loss 0.0524825, acc 0.96
2016-09-06T05:18:52.153705: step 4524, loss 0.0969625, acc 0.98
2016-09-06T05:18:52.981520: step 4525, loss 0.0649842, acc 1
2016-09-06T05:18:53.765410: step 4526, loss 0.137231, acc 0.96
2016-09-06T05:18:54.584020: step 4527, loss 0.0220094, acc 0.98
2016-09-06T05:18:55.397286: step 4528, loss 0.0243841, acc 1
2016-09-06T05:18:56.172028: step 4529, loss 0.0147734, acc 1
2016-09-06T05:18:56.991690: step 4530, loss 0.0309293, acc 0.98
2016-09-06T05:18:57.819935: step 4531, loss 0.0464447, acc 1
2016-09-06T05:18:58.613405: step 4532, loss 0.0868927, acc 0.96
2016-09-06T05:18:59.450773: step 4533, loss 0.0284225, acc 0.98
2016-09-06T05:19:00.301562: step 4534, loss 0.00841992, acc 1
2016-09-06T05:19:01.091620: step 4535, loss 0.0193995, acc 1
2016-09-06T05:19:01.894138: step 4536, loss 0.0154405, acc 1
2016-09-06T05:19:02.683991: step 4537, loss 0.0263163, acc 1
2016-09-06T05:19:03.498514: step 4538, loss 0.00517221, acc 1
2016-09-06T05:19:04.308940: step 4539, loss 0.0207723, acc 0.98
2016-09-06T05:19:05.131932: step 4540, loss 0.0476376, acc 0.98
2016-09-06T05:19:05.966211: step 4541, loss 0.0180069, acc 1
2016-09-06T05:19:06.780359: step 4542, loss 0.00736619, acc 1
2016-09-06T05:19:07.611690: step 4543, loss 0.0410497, acc 0.98
2016-09-06T05:19:08.455993: step 4544, loss 0.0320921, acc 0.98
2016-09-06T05:19:09.265530: step 4545, loss 0.0197775, acc 1
2016-09-06T05:19:10.122104: step 4546, loss 0.00689033, acc 1
2016-09-06T05:19:10.966339: step 4547, loss 0.00385969, acc 1
2016-09-06T05:19:11.768139: step 4548, loss 0.00453351, acc 1
2016-09-06T05:19:12.583382: step 4549, loss 0.0218114, acc 0.98
2016-09-06T05:19:13.420627: step 4550, loss 0.0105334, acc 1
2016-09-06T05:19:14.237571: step 4551, loss 0.0940725, acc 0.96
2016-09-06T05:19:15.082003: step 4552, loss 0.00484079, acc 1
2016-09-06T05:19:15.877995: step 4553, loss 0.0818576, acc 0.96
2016-09-06T05:19:16.673407: step 4554, loss 0.0450671, acc 0.96
2016-09-06T05:19:17.510922: step 4555, loss 0.0174203, acc 1
2016-09-06T05:19:18.356498: step 4556, loss 0.0374089, acc 0.98
2016-09-06T05:19:19.167368: step 4557, loss 0.069224, acc 0.98
2016-09-06T05:19:19.981423: step 4558, loss 0.0284255, acc 1
2016-09-06T05:19:20.826579: step 4559, loss 0.0140513, acc 1
2016-09-06T05:19:21.615527: step 4560, loss 0.0162272, acc 1
2016-09-06T05:19:22.413393: step 4561, loss 0.0301055, acc 0.98
2016-09-06T05:19:23.254283: step 4562, loss 0.0539096, acc 0.96
2016-09-06T05:19:24.036945: step 4563, loss 0.0371735, acc 0.98
2016-09-06T05:19:24.815993: step 4564, loss 0.0170789, acc 1
2016-09-06T05:19:25.625373: step 4565, loss 0.0941136, acc 0.96
2016-09-06T05:19:26.413063: step 4566, loss 0.0122171, acc 1
2016-09-06T05:19:27.202367: step 4567, loss 0.0589636, acc 0.98
2016-09-06T05:19:28.021071: step 4568, loss 0.0127734, acc 1
2016-09-06T05:19:28.821376: step 4569, loss 0.0359748, acc 0.96
2016-09-06T05:19:29.646875: step 4570, loss 0.0188745, acc 1
2016-09-06T05:19:30.499973: step 4571, loss 0.0313234, acc 0.98
2016-09-06T05:19:31.264059: step 4572, loss 0.0373467, acc 1
2016-09-06T05:19:32.078218: step 4573, loss 0.0217426, acc 0.98
2016-09-06T05:19:32.938402: step 4574, loss 0.00877581, acc 1
2016-09-06T05:19:33.752625: step 4575, loss 0.0119865, acc 1
2016-09-06T05:19:34.564728: step 4576, loss 0.0398617, acc 1
2016-09-06T05:19:35.421660: step 4577, loss 0.0767597, acc 0.94
2016-09-06T05:19:36.258722: step 4578, loss 0.0258164, acc 1
2016-09-06T05:19:37.056386: step 4579, loss 0.00692991, acc 1
2016-09-06T05:19:37.899717: step 4580, loss 0.0376216, acc 0.98
2016-09-06T05:19:38.728056: step 4581, loss 0.0446712, acc 0.98
2016-09-06T05:19:39.553547: step 4582, loss 0.0408851, acc 0.98
2016-09-06T05:19:40.366267: step 4583, loss 0.0122928, acc 1
2016-09-06T05:19:41.171932: step 4584, loss 0.0120459, acc 1
2016-09-06T05:19:41.972697: step 4585, loss 0.0169908, acc 1
2016-09-06T05:19:42.807526: step 4586, loss 0.0192813, acc 0.98
2016-09-06T05:19:43.606969: step 4587, loss 0.0235409, acc 1
2016-09-06T05:19:44.428659: step 4588, loss 0.028713, acc 1
2016-09-06T05:19:45.254613: step 4589, loss 0.0279914, acc 0.98
2016-09-06T05:19:46.085741: step 4590, loss 0.0373595, acc 0.98
2016-09-06T05:19:46.883750: step 4591, loss 0.0402276, acc 1
2016-09-06T05:19:47.728247: step 4592, loss 0.0498087, acc 0.98
2016-09-06T05:19:48.576481: step 4593, loss 0.0641061, acc 0.98
2016-09-06T05:19:49.376588: step 4594, loss 0.0606623, acc 0.96
2016-09-06T05:19:50.194147: step 4595, loss 0.00696458, acc 1
2016-09-06T05:19:51.044083: step 4596, loss 0.0254487, acc 0.98
2016-09-06T05:19:51.834514: step 4597, loss 0.0863761, acc 0.96
2016-09-06T05:19:52.625830: step 4598, loss 0.0343578, acc 1
2016-09-06T05:19:53.441100: step 4599, loss 0.00450576, acc 1
2016-09-06T05:19:54.223724: step 4600, loss 0.00352674, acc 1

Evaluation:
2016-09-06T05:19:57.973427: step 4600, loss 2.00072, acc 0.742964

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-4600

2016-09-06T05:19:59.831254: step 4601, loss 0.00605199, acc 1
2016-09-06T05:20:00.664536: step 4602, loss 0.0112011, acc 1
2016-09-06T05:20:01.500338: step 4603, loss 0.0488665, acc 0.98
2016-09-06T05:20:02.362029: step 4604, loss 0.0119522, acc 1
2016-09-06T05:20:03.190547: step 4605, loss 0.0124102, acc 1
2016-09-06T05:20:03.988388: step 4606, loss 0.0107551, acc 1
2016-09-06T05:20:04.797834: step 4607, loss 0.0502477, acc 0.96
2016-09-06T05:20:05.566880: step 4608, loss 0.0362013, acc 0.977273
2016-09-06T05:20:06.359920: step 4609, loss 0.0116063, acc 1
2016-09-06T05:20:07.185446: step 4610, loss 0.0493696, acc 0.98
2016-09-06T05:20:07.994474: step 4611, loss 0.0313875, acc 0.98
2016-09-06T05:20:08.773555: step 4612, loss 0.0562501, acc 0.96
2016-09-06T05:20:09.583224: step 4613, loss 0.0302784, acc 1
2016-09-06T05:20:10.411951: step 4614, loss 0.0382474, acc 0.98
2016-09-06T05:20:11.199845: step 4615, loss 0.144002, acc 0.96
2016-09-06T05:20:11.992094: step 4616, loss 0.0255129, acc 1
2016-09-06T05:20:12.812163: step 4617, loss 0.058002, acc 0.98
2016-09-06T05:20:13.598654: step 4618, loss 0.145994, acc 0.96
2016-09-06T05:20:14.403988: step 4619, loss 0.0258838, acc 0.98
2016-09-06T05:20:15.227788: step 4620, loss 0.0192645, acc 0.98
2016-09-06T05:20:16.029372: step 4621, loss 0.0578934, acc 0.96
2016-09-06T05:20:16.824072: step 4622, loss 0.0259945, acc 0.98
2016-09-06T05:20:17.642727: step 4623, loss 0.037757, acc 1
2016-09-06T05:20:18.425310: step 4624, loss 0.00649385, acc 1
2016-09-06T05:20:19.216361: step 4625, loss 0.0419885, acc 0.98
2016-09-06T05:20:20.036823: step 4626, loss 0.0534115, acc 0.98
2016-09-06T05:20:20.820079: step 4627, loss 0.0120545, acc 1
2016-09-06T05:20:21.675762: step 4628, loss 0.0234671, acc 0.98
2016-09-06T05:20:22.470124: step 4629, loss 0.0827738, acc 0.98
2016-09-06T05:20:23.261575: step 4630, loss 0.0577675, acc 0.96
2016-09-06T05:20:24.098682: step 4631, loss 0.00476088, acc 1
2016-09-06T05:20:24.908963: step 4632, loss 0.0480207, acc 0.98
2016-09-06T05:20:25.693236: step 4633, loss 0.0285952, acc 1
2016-09-06T05:20:26.491979: step 4634, loss 0.0383177, acc 0.98
2016-09-06T05:20:27.309967: step 4635, loss 0.029763, acc 1
2016-09-06T05:20:28.105938: step 4636, loss 0.014502, acc 1
2016-09-06T05:20:28.908970: step 4637, loss 0.023795, acc 1
2016-09-06T05:20:29.725809: step 4638, loss 0.00561934, acc 1
2016-09-06T05:20:30.519730: step 4639, loss 0.0263986, acc 0.98
2016-09-06T05:20:31.318483: step 4640, loss 0.0267913, acc 0.98
2016-09-06T05:20:32.167501: step 4641, loss 0.0172464, acc 1
2016-09-06T05:20:32.973714: step 4642, loss 0.064147, acc 0.96
2016-09-06T05:20:33.775961: step 4643, loss 0.0224103, acc 0.98
2016-09-06T05:20:34.567398: step 4644, loss 0.0578413, acc 0.98
2016-09-06T05:20:35.352377: step 4645, loss 0.00456858, acc 1
2016-09-06T05:20:36.163053: step 4646, loss 0.022816, acc 0.98
2016-09-06T05:20:36.990004: step 4647, loss 0.0110635, acc 1
2016-09-06T05:20:37.793902: step 4648, loss 0.00422227, acc 1
2016-09-06T05:20:38.609213: step 4649, loss 0.0246525, acc 1
2016-09-06T05:20:39.442462: step 4650, loss 0.0404616, acc 0.96
2016-09-06T05:20:40.259113: step 4651, loss 0.0946348, acc 0.98
2016-09-06T05:20:41.093579: step 4652, loss 0.019745, acc 0.98
2016-09-06T05:20:41.928259: step 4653, loss 0.0272406, acc 0.98
2016-09-06T05:20:42.750735: step 4654, loss 0.018733, acc 1
2016-09-06T05:20:43.546004: step 4655, loss 0.0300185, acc 0.98
2016-09-06T05:20:44.377764: step 4656, loss 0.0256, acc 1
2016-09-06T05:20:45.207000: step 4657, loss 0.0363886, acc 1
2016-09-06T05:20:46.039778: step 4658, loss 0.00390768, acc 1
2016-09-06T05:20:46.868447: step 4659, loss 0.0151561, acc 1
2016-09-06T05:20:47.687590: step 4660, loss 0.024095, acc 0.98
2016-09-06T05:20:48.503100: step 4661, loss 0.0971851, acc 0.96
2016-09-06T05:20:49.333282: step 4662, loss 0.0181604, acc 1
2016-09-06T05:20:50.157431: step 4663, loss 0.0242796, acc 0.98
2016-09-06T05:20:50.978296: step 4664, loss 0.0117031, acc 1
2016-09-06T05:20:51.799537: step 4665, loss 0.00326619, acc 1
2016-09-06T05:20:52.626807: step 4666, loss 0.0334651, acc 1
2016-09-06T05:20:53.432178: step 4667, loss 0.00552435, acc 1
2016-09-06T05:20:54.224343: step 4668, loss 0.00329123, acc 1
2016-09-06T05:20:55.052254: step 4669, loss 0.0400314, acc 1
2016-09-06T05:20:55.839039: step 4670, loss 0.0149957, acc 1
2016-09-06T05:20:56.626346: step 4671, loss 0.0313097, acc 0.98
2016-09-06T05:20:57.433342: step 4672, loss 0.0466736, acc 0.98
2016-09-06T05:20:58.272146: step 4673, loss 0.0321628, acc 0.98
2016-09-06T05:20:59.078153: step 4674, loss 0.0360784, acc 0.98
2016-09-06T05:20:59.903608: step 4675, loss 0.0297691, acc 0.98
2016-09-06T05:21:00.720139: step 4676, loss 0.0334079, acc 1
2016-09-06T05:21:01.517915: step 4677, loss 0.0302177, acc 0.98
2016-09-06T05:21:02.315636: step 4678, loss 0.0139152, acc 1
2016-09-06T05:21:03.124308: step 4679, loss 0.0229869, acc 0.98
2016-09-06T05:21:03.951248: step 4680, loss 0.0137524, acc 1
2016-09-06T05:21:04.791737: step 4681, loss 0.0263075, acc 0.98
2016-09-06T05:21:05.581512: step 4682, loss 0.0171025, acc 1
2016-09-06T05:21:06.400308: step 4683, loss 0.0439655, acc 0.98
2016-09-06T05:21:07.233436: step 4684, loss 0.0178904, acc 0.98
2016-09-06T05:21:08.028063: step 4685, loss 0.00471526, acc 1
2016-09-06T05:21:08.836843: step 4686, loss 0.0455446, acc 0.98
2016-09-06T05:21:09.647632: step 4687, loss 0.0802482, acc 0.96
2016-09-06T05:21:10.441524: step 4688, loss 0.0284683, acc 0.98
2016-09-06T05:21:11.234766: step 4689, loss 0.0202963, acc 1
2016-09-06T05:21:12.041060: step 4690, loss 0.0179884, acc 0.98
2016-09-06T05:21:12.808522: step 4691, loss 0.0316655, acc 0.98
2016-09-06T05:21:13.600814: step 4692, loss 0.0058904, acc 1
2016-09-06T05:21:14.422349: step 4693, loss 0.0315954, acc 0.98
2016-09-06T05:21:15.212628: step 4694, loss 0.00639143, acc 1
2016-09-06T05:21:16.036521: step 4695, loss 0.00626806, acc 1
2016-09-06T05:21:16.852866: step 4696, loss 0.0798448, acc 0.98
2016-09-06T05:21:17.645920: step 4697, loss 0.0098895, acc 1
2016-09-06T05:21:18.453769: step 4698, loss 0.0076454, acc 1
2016-09-06T05:21:19.284258: step 4699, loss 0.0145392, acc 1
2016-09-06T05:21:20.058853: step 4700, loss 0.0977744, acc 0.98

Evaluation:
2016-09-06T05:21:23.770399: step 4700, loss 2.07406, acc 0.737336

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-4700

2016-09-06T05:21:25.671470: step 4701, loss 0.00479241, acc 1
2016-09-06T05:21:26.473156: step 4702, loss 0.0149191, acc 1
2016-09-06T05:21:27.245299: step 4703, loss 0.0163406, acc 1
2016-09-06T05:21:28.109414: step 4704, loss 0.0302915, acc 0.98
2016-09-06T05:21:28.938695: step 4705, loss 0.00351245, acc 1
2016-09-06T05:21:29.759402: step 4706, loss 0.126457, acc 0.94
2016-09-06T05:21:30.543834: step 4707, loss 0.0711402, acc 0.98
2016-09-06T05:21:31.365541: step 4708, loss 0.104141, acc 0.96
2016-09-06T05:21:32.159223: step 4709, loss 0.0253582, acc 1
2016-09-06T05:21:32.957295: step 4710, loss 0.00372864, acc 1
2016-09-06T05:21:33.781585: step 4711, loss 0.0917262, acc 0.98
2016-09-06T05:21:34.577967: step 4712, loss 0.0186749, acc 1
2016-09-06T05:21:35.395349: step 4713, loss 0.0348366, acc 0.98
2016-09-06T05:21:36.237441: step 4714, loss 0.0113861, acc 1
2016-09-06T05:21:37.029404: step 4715, loss 0.0440772, acc 1
2016-09-06T05:21:37.829396: step 4716, loss 0.0449544, acc 0.98
2016-09-06T05:21:38.622702: step 4717, loss 0.0620973, acc 0.94
2016-09-06T05:21:39.409966: step 4718, loss 0.0466592, acc 0.96
2016-09-06T05:21:40.222251: step 4719, loss 0.0112613, acc 1
2016-09-06T05:21:41.063510: step 4720, loss 0.0433019, acc 0.98
2016-09-06T05:21:41.835244: step 4721, loss 0.0400933, acc 0.98
2016-09-06T05:21:42.657835: step 4722, loss 0.00841281, acc 1
2016-09-06T05:21:43.496949: step 4723, loss 0.0361207, acc 0.98
2016-09-06T05:21:44.303116: step 4724, loss 0.036121, acc 1
2016-09-06T05:21:45.136267: step 4725, loss 0.023183, acc 1
2016-09-06T05:21:45.971679: step 4726, loss 0.0124598, acc 1
2016-09-06T05:21:46.772871: step 4727, loss 0.036345, acc 0.98
2016-09-06T05:21:47.572141: step 4728, loss 0.019249, acc 1
2016-09-06T05:21:48.438874: step 4729, loss 0.0124902, acc 1
2016-09-06T05:21:49.260567: step 4730, loss 0.0169092, acc 1
2016-09-06T05:21:50.065226: step 4731, loss 0.0324625, acc 0.98
2016-09-06T05:21:50.932713: step 4732, loss 0.0176573, acc 1
2016-09-06T05:21:51.758126: step 4733, loss 0.0406511, acc 0.98
2016-09-06T05:21:52.591113: step 4734, loss 0.0165851, acc 1
2016-09-06T05:21:53.424748: step 4735, loss 0.0128264, acc 1
2016-09-06T05:21:54.235776: step 4736, loss 0.0183254, acc 1
2016-09-06T05:21:55.091379: step 4737, loss 0.018048, acc 1
2016-09-06T05:21:55.907207: step 4738, loss 0.0379749, acc 0.98
2016-09-06T05:21:56.730154: step 4739, loss 0.0259384, acc 1
2016-09-06T05:21:57.522394: step 4740, loss 0.0134866, acc 1
2016-09-06T05:21:58.323482: step 4741, loss 0.0430381, acc 0.96
2016-09-06T05:21:59.146744: step 4742, loss 0.0683775, acc 0.96
2016-09-06T05:21:59.920620: step 4743, loss 0.0130628, acc 1
2016-09-06T05:22:00.760255: step 4744, loss 0.00396831, acc 1
2016-09-06T05:22:01.569637: step 4745, loss 0.0733976, acc 0.94
2016-09-06T05:22:02.371825: step 4746, loss 0.0400744, acc 0.98
2016-09-06T05:22:03.186934: step 4747, loss 0.00326147, acc 1
2016-09-06T05:22:04.002761: step 4748, loss 0.0889799, acc 0.98
2016-09-06T05:22:04.817528: step 4749, loss 0.0443655, acc 1
2016-09-06T05:22:05.620184: step 4750, loss 0.0368799, acc 0.96
2016-09-06T05:22:06.460197: step 4751, loss 0.0469143, acc 0.98
2016-09-06T05:22:07.260719: step 4752, loss 0.148609, acc 0.96
2016-09-06T05:22:08.057258: step 4753, loss 0.00557461, acc 1
2016-09-06T05:22:08.879945: step 4754, loss 0.0135826, acc 1
2016-09-06T05:22:09.675788: step 4755, loss 0.0759744, acc 0.94
2016-09-06T05:22:10.455647: step 4756, loss 0.00537104, acc 1
2016-09-06T05:22:11.257913: step 4757, loss 0.00848855, acc 1
2016-09-06T05:22:12.046481: step 4758, loss 0.0440963, acc 0.98
2016-09-06T05:22:12.887690: step 4759, loss 0.06758, acc 0.98
2016-09-06T05:22:13.698204: step 4760, loss 0.0283986, acc 0.98
2016-09-06T05:22:14.475896: step 4761, loss 0.00410588, acc 1
2016-09-06T05:22:15.298866: step 4762, loss 0.0159163, acc 1
2016-09-06T05:22:16.130079: step 4763, loss 0.0255012, acc 1
2016-09-06T05:22:16.919032: step 4764, loss 0.0747166, acc 0.92
2016-09-06T05:22:17.725798: step 4765, loss 0.110022, acc 0.96
2016-09-06T05:22:18.533098: step 4766, loss 0.0271359, acc 0.98
2016-09-06T05:22:19.308539: step 4767, loss 0.00880186, acc 1
2016-09-06T05:22:20.111226: step 4768, loss 0.0273079, acc 1
2016-09-06T05:22:20.962739: step 4769, loss 0.044084, acc 1
2016-09-06T05:22:21.796622: step 4770, loss 0.0967059, acc 0.96
2016-09-06T05:22:22.606386: step 4771, loss 0.0122938, acc 1
2016-09-06T05:22:23.422920: step 4772, loss 0.00855055, acc 1
2016-09-06T05:22:24.252991: step 4773, loss 0.0149217, acc 1
2016-09-06T05:22:25.073515: step 4774, loss 0.0305074, acc 1
2016-09-06T05:22:25.914649: step 4775, loss 0.0262984, acc 0.98
2016-09-06T05:22:26.745156: step 4776, loss 0.0231087, acc 0.98
2016-09-06T05:22:27.586342: step 4777, loss 0.0385074, acc 1
2016-09-06T05:22:28.428031: step 4778, loss 0.0489652, acc 1
2016-09-06T05:22:29.281165: step 4779, loss 0.0158882, acc 1
2016-09-06T05:22:30.120813: step 4780, loss 0.00446482, acc 1
2016-09-06T05:22:30.938768: step 4781, loss 0.0168308, acc 1
2016-09-06T05:22:31.775761: step 4782, loss 0.0680027, acc 0.96
2016-09-06T05:22:32.557810: step 4783, loss 0.105788, acc 0.96
2016-09-06T05:22:33.394953: step 4784, loss 0.0788747, acc 0.98
2016-09-06T05:22:34.214311: step 4785, loss 0.0719621, acc 0.98
2016-09-06T05:22:34.994031: step 4786, loss 0.095059, acc 0.94
2016-09-06T05:22:35.791067: step 4787, loss 0.0258529, acc 1
2016-09-06T05:22:36.596423: step 4788, loss 0.0283873, acc 0.98
2016-09-06T05:22:37.387688: step 4789, loss 0.007565, acc 1
2016-09-06T05:22:38.192757: step 4790, loss 0.0301359, acc 0.98
2016-09-06T05:22:38.987822: step 4791, loss 0.0187815, acc 0.98
2016-09-06T05:22:39.822856: step 4792, loss 0.0102944, acc 1
2016-09-06T05:22:40.630738: step 4793, loss 0.0378904, acc 1
2016-09-06T05:22:41.450803: step 4794, loss 0.0386103, acc 0.98
2016-09-06T05:22:42.243711: step 4795, loss 0.0331099, acc 0.98
2016-09-06T05:22:43.052875: step 4796, loss 0.0573824, acc 0.96
2016-09-06T05:22:43.889834: step 4797, loss 0.0279557, acc 1
2016-09-06T05:22:44.689412: step 4798, loss 0.0309286, acc 0.98
2016-09-06T05:22:45.494139: step 4799, loss 0.00835267, acc 1
2016-09-06T05:22:46.259176: step 4800, loss 0.00593213, acc 1

Evaluation:
2016-09-06T05:22:49.961163: step 4800, loss 1.86919, acc 0.742964

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-4800

2016-09-06T05:22:51.934799: step 4801, loss 0.0143857, acc 1
2016-09-06T05:22:52.758417: step 4802, loss 0.0435124, acc 0.98
2016-09-06T05:22:53.604921: step 4803, loss 0.00313675, acc 1
2016-09-06T05:22:54.403995: step 4804, loss 0.0763754, acc 0.96
2016-09-06T05:22:55.231498: step 4805, loss 0.0598149, acc 0.98
2016-09-06T05:22:56.039788: step 4806, loss 0.00816972, acc 1
2016-09-06T05:22:56.829706: step 4807, loss 0.014627, acc 1
2016-09-06T05:22:57.651157: step 4808, loss 0.0244129, acc 0.98
2016-09-06T05:22:58.498237: step 4809, loss 0.032538, acc 0.98
2016-09-06T05:22:59.291297: step 4810, loss 0.0152408, acc 1
2016-09-06T05:23:00.092591: step 4811, loss 0.00471029, acc 1
2016-09-06T05:23:00.964675: step 4812, loss 0.0101319, acc 1
2016-09-06T05:23:01.767441: step 4813, loss 0.0151814, acc 1
2016-09-06T05:23:02.555783: step 4814, loss 0.0531001, acc 0.96
2016-09-06T05:23:03.353123: step 4815, loss 0.105625, acc 0.96
2016-09-06T05:23:04.139122: step 4816, loss 0.0352427, acc 0.98
2016-09-06T05:23:04.958595: step 4817, loss 0.00970375, acc 1
2016-09-06T05:23:05.737409: step 4818, loss 0.010777, acc 1
2016-09-06T05:23:06.553189: step 4819, loss 0.00832245, acc 1
2016-09-06T05:23:07.374647: step 4820, loss 0.00394453, acc 1
2016-09-06T05:23:08.205325: step 4821, loss 0.00496907, acc 1
2016-09-06T05:23:08.993826: step 4822, loss 0.0148247, acc 1
2016-09-06T05:23:09.775183: step 4823, loss 0.0301473, acc 0.98
2016-09-06T05:23:10.602529: step 4824, loss 0.00725079, acc 1
2016-09-06T05:23:11.380820: step 4825, loss 0.0283076, acc 0.98
2016-09-06T05:23:12.190658: step 4826, loss 0.0142592, acc 1
2016-09-06T05:23:12.991598: step 4827, loss 0.0286115, acc 0.98
2016-09-06T05:23:13.768170: step 4828, loss 0.0340344, acc 0.98
2016-09-06T05:23:14.587157: step 4829, loss 0.0256909, acc 0.98
2016-09-06T05:23:15.434916: step 4830, loss 0.0193069, acc 1
2016-09-06T05:23:16.223946: step 4831, loss 0.0285124, acc 0.98
2016-09-06T05:23:17.031376: step 4832, loss 0.0165014, acc 1
2016-09-06T05:23:17.833951: step 4833, loss 0.0744872, acc 0.98
2016-09-06T05:23:18.640518: step 4834, loss 0.0332698, acc 0.98
2016-09-06T05:23:19.436451: step 4835, loss 0.00320174, acc 1
2016-09-06T05:23:20.270197: step 4836, loss 0.00928112, acc 1
2016-09-06T05:23:21.052460: step 4837, loss 0.0135526, acc 1
2016-09-06T05:23:21.856979: step 4838, loss 0.00456276, acc 1
2016-09-06T05:23:22.664376: step 4839, loss 0.0151849, acc 1
2016-09-06T05:23:23.456669: step 4840, loss 0.00830325, acc 1
2016-09-06T05:23:24.251704: step 4841, loss 0.044107, acc 0.96
2016-09-06T05:23:25.061898: step 4842, loss 0.0186742, acc 0.98
2016-09-06T05:23:25.886026: step 4843, loss 0.0283548, acc 0.98
2016-09-06T05:23:26.707914: step 4844, loss 0.0376859, acc 0.98
2016-09-06T05:23:27.540858: step 4845, loss 0.0035513, acc 1
2016-09-06T05:23:28.311277: step 4846, loss 0.014309, acc 1
2016-09-06T05:23:29.109487: step 4847, loss 0.00423896, acc 1
2016-09-06T05:23:29.963336: step 4848, loss 0.00568506, acc 1
2016-09-06T05:23:30.748427: step 4849, loss 0.0279286, acc 0.98
2016-09-06T05:23:31.565405: step 4850, loss 0.00316158, acc 1
2016-09-06T05:23:32.395617: step 4851, loss 0.0155506, acc 1
2016-09-06T05:23:33.202501: step 4852, loss 0.062896, acc 0.98
2016-09-06T05:23:34.013994: step 4853, loss 0.0294844, acc 0.98
2016-09-06T05:23:34.849732: step 4854, loss 0.0157961, acc 1
2016-09-06T05:23:35.709636: step 4855, loss 0.0267218, acc 0.98
2016-09-06T05:23:36.543438: step 4856, loss 0.0167957, acc 1
2016-09-06T05:23:37.376483: step 4857, loss 0.00665893, acc 1
2016-09-06T05:23:38.225692: step 4858, loss 0.141355, acc 0.96
2016-09-06T05:23:39.041298: step 4859, loss 0.00305301, acc 1
2016-09-06T05:23:39.868499: step 4860, loss 0.0364759, acc 0.98
2016-09-06T05:23:40.690211: step 4861, loss 0.0222764, acc 1
2016-09-06T05:23:41.494629: step 4862, loss 0.0152175, acc 1
2016-09-06T05:23:42.342906: step 4863, loss 0.00361271, acc 1
2016-09-06T05:23:43.153440: step 4864, loss 0.077749, acc 0.96
2016-09-06T05:23:43.944311: step 4865, loss 0.00536027, acc 1
2016-09-06T05:23:44.763127: step 4866, loss 0.0161496, acc 1
2016-09-06T05:23:45.575523: step 4867, loss 0.020926, acc 0.98
2016-09-06T05:23:46.395193: step 4868, loss 0.11214, acc 0.94
2016-09-06T05:23:47.233534: step 4869, loss 0.0795929, acc 0.96
2016-09-06T05:23:48.059446: step 4870, loss 0.00351329, acc 1
2016-09-06T05:23:48.854750: step 4871, loss 0.0333181, acc 0.98
2016-09-06T05:23:49.673561: step 4872, loss 0.150361, acc 0.94
2016-09-06T05:23:50.486644: step 4873, loss 0.0158995, acc 1
2016-09-06T05:23:51.253947: step 4874, loss 0.121851, acc 0.96
2016-09-06T05:23:52.049393: step 4875, loss 0.0148302, acc 1
2016-09-06T05:23:52.874668: step 4876, loss 0.0230645, acc 1
2016-09-06T05:23:53.681941: step 4877, loss 0.0286513, acc 0.98
2016-09-06T05:23:54.485269: step 4878, loss 0.0321687, acc 0.98
2016-09-06T05:23:55.298054: step 4879, loss 0.0275148, acc 0.98
2016-09-06T05:23:56.086316: step 4880, loss 0.0184662, acc 1
2016-09-06T05:23:56.897977: step 4881, loss 0.00309059, acc 1
2016-09-06T05:23:57.730550: step 4882, loss 0.0162984, acc 1
2016-09-06T05:23:58.560501: step 4883, loss 0.0755865, acc 0.94
2016-09-06T05:23:59.371423: step 4884, loss 0.0271985, acc 0.98
2016-09-06T05:24:00.189584: step 4885, loss 0.010079, acc 1
2016-09-06T05:24:01.001088: step 4886, loss 0.0102617, acc 1
2016-09-06T05:24:01.795432: step 4887, loss 0.00643275, acc 1
2016-09-06T05:24:02.605341: step 4888, loss 0.0371235, acc 0.98
2016-09-06T05:24:03.434976: step 4889, loss 0.0377857, acc 0.96
2016-09-06T05:24:04.237641: step 4890, loss 0.0136816, acc 1
2016-09-06T05:24:05.060566: step 4891, loss 0.0181324, acc 1
2016-09-06T05:24:05.892760: step 4892, loss 0.00394674, acc 1
2016-09-06T05:24:06.718102: step 4893, loss 0.021777, acc 0.98
2016-09-06T05:24:07.537291: step 4894, loss 0.0042896, acc 1
2016-09-06T05:24:08.313886: step 4895, loss 0.00920562, acc 1
2016-09-06T05:24:09.120895: step 4896, loss 0.00560277, acc 1
2016-09-06T05:24:09.962778: step 4897, loss 0.00483601, acc 1
2016-09-06T05:24:10.803718: step 4898, loss 0.0291887, acc 1
2016-09-06T05:24:11.624822: step 4899, loss 0.0515809, acc 0.98
2016-09-06T05:24:12.479918: step 4900, loss 0.0178349, acc 1

Evaluation:
2016-09-06T05:24:16.213075: step 4900, loss 2.23723, acc 0.749531

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-4900

2016-09-06T05:24:18.087319: step 4901, loss 0.0238354, acc 0.98
2016-09-06T05:24:18.913975: step 4902, loss 0.00475105, acc 1
2016-09-06T05:24:19.734896: step 4903, loss 0.00726716, acc 1
2016-09-06T05:24:20.535872: step 4904, loss 0.0489557, acc 0.96
2016-09-06T05:24:21.333959: step 4905, loss 0.0743884, acc 0.98
2016-09-06T05:24:22.172807: step 4906, loss 0.0409069, acc 0.98
2016-09-06T05:24:23.023444: step 4907, loss 0.0174838, acc 1
2016-09-06T05:24:23.846346: step 4908, loss 0.068972, acc 0.96
2016-09-06T05:24:24.689278: step 4909, loss 0.0970517, acc 0.96
2016-09-06T05:24:25.499980: step 4910, loss 0.0374007, acc 0.98
2016-09-06T05:24:26.309433: step 4911, loss 0.0145192, acc 1
2016-09-06T05:24:27.118043: step 4912, loss 0.114743, acc 0.96
2016-09-06T05:24:27.925992: step 4913, loss 0.0259737, acc 1
2016-09-06T05:24:28.715531: step 4914, loss 0.00784834, acc 1
2016-09-06T05:24:29.524842: step 4915, loss 0.0867913, acc 0.96
2016-09-06T05:24:30.338650: step 4916, loss 0.0288621, acc 0.98
2016-09-06T05:24:31.117016: step 4917, loss 0.0197612, acc 0.98
2016-09-06T05:24:31.953773: step 4918, loss 0.0110777, acc 1
2016-09-06T05:24:32.756762: step 4919, loss 0.0649429, acc 0.98
2016-09-06T05:24:33.545522: step 4920, loss 0.00947953, acc 1
2016-09-06T05:24:34.368345: step 4921, loss 0.0228587, acc 0.98
2016-09-06T05:24:35.172786: step 4922, loss 0.00335972, acc 1
2016-09-06T05:24:35.977408: step 4923, loss 0.0186614, acc 0.98
2016-09-06T05:24:36.801955: step 4924, loss 0.0557288, acc 0.96
2016-09-06T05:24:37.612366: step 4925, loss 0.0397626, acc 0.98
2016-09-06T05:24:38.430680: step 4926, loss 0.042432, acc 0.98
2016-09-06T05:24:39.224188: step 4927, loss 0.0233654, acc 0.98
2016-09-06T05:24:40.041375: step 4928, loss 0.0149397, acc 1
2016-09-06T05:24:40.839818: step 4929, loss 0.0417151, acc 0.96
2016-09-06T05:24:41.640449: step 4930, loss 0.0759286, acc 0.98
2016-09-06T05:24:42.481445: step 4931, loss 0.0297636, acc 1
2016-09-06T05:24:43.266352: step 4932, loss 0.0370319, acc 0.98
2016-09-06T05:24:44.052246: step 4933, loss 0.0190911, acc 0.98
2016-09-06T05:24:44.876241: step 4934, loss 0.0197551, acc 1
2016-09-06T05:24:45.654630: step 4935, loss 0.0124896, acc 1
2016-09-06T05:24:46.460562: step 4936, loss 0.0275932, acc 1
2016-09-06T05:24:47.285475: step 4937, loss 0.0293354, acc 0.98
2016-09-06T05:24:48.081116: step 4938, loss 0.0966453, acc 0.96
2016-09-06T05:24:48.866942: step 4939, loss 0.0289083, acc 1
2016-09-06T05:24:49.765895: step 4940, loss 0.00818019, acc 1
2016-09-06T05:24:50.563281: step 4941, loss 0.0226748, acc 0.98
2016-09-06T05:24:51.377682: step 4942, loss 0.00721441, acc 1
2016-09-06T05:24:52.256762: step 4943, loss 0.0477445, acc 0.98
2016-09-06T05:24:53.084905: step 4944, loss 0.0783317, acc 0.94
2016-09-06T05:24:53.908653: step 4945, loss 0.0185739, acc 1
2016-09-06T05:24:54.740641: step 4946, loss 0.0153008, acc 1
2016-09-06T05:24:55.550164: step 4947, loss 0.0712799, acc 0.98
2016-09-06T05:24:56.381123: step 4948, loss 0.02595, acc 0.98
2016-09-06T05:24:57.205949: step 4949, loss 0.0385128, acc 0.98
2016-09-06T05:24:58.013888: step 4950, loss 0.0368231, acc 1
2016-09-06T05:24:58.837148: step 4951, loss 0.0151469, acc 1
2016-09-06T05:24:59.683200: step 4952, loss 0.0137802, acc 1
2016-09-06T05:25:00.547757: step 4953, loss 0.0407099, acc 0.98
2016-09-06T05:25:01.372488: step 4954, loss 0.00296682, acc 1
2016-09-06T05:25:02.178225: step 4955, loss 0.0434401, acc 0.96
2016-09-06T05:25:03.037371: step 4956, loss 0.0579954, acc 0.96
2016-09-06T05:25:03.802089: step 4957, loss 0.0221247, acc 1
2016-09-06T05:25:04.599359: step 4958, loss 0.0419829, acc 0.96
2016-09-06T05:25:05.397127: step 4959, loss 0.0298695, acc 1
2016-09-06T05:25:06.226928: step 4960, loss 0.0234635, acc 0.98
2016-09-06T05:25:07.037811: step 4961, loss 0.0294547, acc 0.98
2016-09-06T05:25:07.861455: step 4962, loss 0.0803469, acc 0.98
2016-09-06T05:25:08.655084: step 4963, loss 0.0314702, acc 1
2016-09-06T05:25:09.453301: step 4964, loss 0.0137647, acc 1
2016-09-06T05:25:10.268591: step 4965, loss 0.00964604, acc 1
2016-09-06T05:25:11.062102: step 4966, loss 0.0415586, acc 1
2016-09-06T05:25:11.855994: step 4967, loss 0.0858878, acc 0.98
2016-09-06T05:25:12.660348: step 4968, loss 0.0297979, acc 0.98
2016-09-06T05:25:13.455136: step 4969, loss 0.0276873, acc 1
2016-09-06T05:25:14.267147: step 4970, loss 0.00940404, acc 1
2016-09-06T05:25:15.088932: step 4971, loss 0.0243248, acc 1
2016-09-06T05:25:15.887286: step 4972, loss 0.0314654, acc 1
2016-09-06T05:25:16.689559: step 4973, loss 0.00392874, acc 1
2016-09-06T05:25:17.516936: step 4974, loss 0.0192743, acc 1
2016-09-06T05:25:18.305639: step 4975, loss 0.0427154, acc 0.98
2016-09-06T05:25:19.123644: step 4976, loss 0.0121516, acc 1
2016-09-06T05:25:19.934857: step 4977, loss 0.0745026, acc 0.98
2016-09-06T05:25:20.775421: step 4978, loss 0.0747394, acc 0.96
2016-09-06T05:25:21.598922: step 4979, loss 0.0593703, acc 0.98
2016-09-06T05:25:22.430369: step 4980, loss 0.0526798, acc 0.96
2016-09-06T05:25:23.236686: step 4981, loss 0.0138874, acc 1
2016-09-06T05:25:24.057854: step 4982, loss 0.0218463, acc 1
2016-09-06T05:25:24.913773: step 4983, loss 0.14108, acc 0.96
2016-09-06T05:25:25.712859: step 4984, loss 0.00337425, acc 1
2016-09-06T05:25:26.543968: step 4985, loss 0.0355389, acc 0.98
2016-09-06T05:25:27.378262: step 4986, loss 0.0250253, acc 1
2016-09-06T05:25:28.189329: step 4987, loss 0.0547209, acc 0.98
2016-09-06T05:25:29.029436: step 4988, loss 0.0550493, acc 0.98
2016-09-06T05:25:29.833460: step 4989, loss 0.0359717, acc 0.98
2016-09-06T05:25:30.643640: step 4990, loss 0.0608296, acc 0.96
2016-09-06T05:25:31.447273: step 4991, loss 0.0177639, acc 1
2016-09-06T05:25:32.233019: step 4992, loss 0.0465306, acc 0.954545
2016-09-06T05:25:33.078559: step 4993, loss 0.0447102, acc 0.98
2016-09-06T05:25:33.903564: step 4994, loss 0.021528, acc 1
2016-09-06T05:25:34.742163: step 4995, loss 0.00529931, acc 1
2016-09-06T05:25:35.589543: step 4996, loss 0.0256986, acc 0.98
2016-09-06T05:25:36.415786: step 4997, loss 0.0332723, acc 0.98
2016-09-06T05:25:37.207913: step 4998, loss 0.0241969, acc 1
2016-09-06T05:25:38.033501: step 4999, loss 0.015471, acc 1
2016-09-06T05:25:38.821743: step 5000, loss 0.0113024, acc 1

Evaluation:
2016-09-06T05:25:42.576888: step 5000, loss 1.93628, acc 0.730769

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-5000

2016-09-06T05:25:44.584178: step 5001, loss 0.0348235, acc 1
2016-09-06T05:25:45.419453: step 5002, loss 0.0174969, acc 1
2016-09-06T05:25:46.220377: step 5003, loss 0.00522651, acc 1
2016-09-06T05:25:47.016732: step 5004, loss 0.00823014, acc 1
2016-09-06T05:25:47.818683: step 5005, loss 0.0172127, acc 1
2016-09-06T05:25:48.604498: step 5006, loss 0.0510852, acc 0.96
2016-09-06T05:25:49.431113: step 5007, loss 0.0430356, acc 0.98
2016-09-06T05:25:50.254102: step 5008, loss 0.0393726, acc 0.98
2016-09-06T05:25:51.030827: step 5009, loss 0.0450299, acc 0.98
2016-09-06T05:25:51.843098: step 5010, loss 0.0198229, acc 0.98
2016-09-06T05:25:52.657566: step 5011, loss 0.0278073, acc 0.98
2016-09-06T05:25:53.460596: step 5012, loss 0.0421351, acc 0.98
2016-09-06T05:25:54.280720: step 5013, loss 0.0370799, acc 0.98
2016-09-06T05:25:55.091993: step 5014, loss 0.0167848, acc 1
2016-09-06T05:25:55.902851: step 5015, loss 0.0531903, acc 0.98
2016-09-06T05:25:56.699988: step 5016, loss 0.0508133, acc 0.98
2016-09-06T05:25:57.525790: step 5017, loss 0.0441852, acc 0.96
2016-09-06T05:25:58.311571: step 5018, loss 0.0529456, acc 0.98
2016-09-06T05:25:59.119768: step 5019, loss 0.00380141, acc 1
2016-09-06T05:25:59.925228: step 5020, loss 0.0353159, acc 1
2016-09-06T05:26:00.736615: step 5021, loss 0.0355974, acc 0.98
2016-09-06T05:26:01.568008: step 5022, loss 0.0881549, acc 0.96
2016-09-06T05:26:02.405459: step 5023, loss 0.0140185, acc 1
2016-09-06T05:26:03.204217: step 5024, loss 0.0188305, acc 0.98
2016-09-06T05:26:04.008990: step 5025, loss 0.0258206, acc 1
2016-09-06T05:26:04.811406: step 5026, loss 0.0182445, acc 0.98
2016-09-06T05:26:05.605351: step 5027, loss 0.00367111, acc 1
2016-09-06T05:26:06.415226: step 5028, loss 0.0106564, acc 1
2016-09-06T05:26:07.243035: step 5029, loss 0.00339379, acc 1
2016-09-06T05:26:08.053925: step 5030, loss 0.0192775, acc 0.98
2016-09-06T05:26:08.856759: step 5031, loss 0.00654959, acc 1
2016-09-06T05:26:09.664210: step 5032, loss 0.00306068, acc 1
2016-09-06T05:26:10.458152: step 5033, loss 0.0377178, acc 0.96
2016-09-06T05:26:11.271083: step 5034, loss 0.00702693, acc 1
2016-09-06T05:26:12.079390: step 5035, loss 0.00542527, acc 1
2016-09-06T05:26:12.870145: step 5036, loss 0.137146, acc 0.98
2016-09-06T05:26:13.669373: step 5037, loss 0.00610556, acc 1
2016-09-06T05:26:14.487415: step 5038, loss 0.0416081, acc 0.98
2016-09-06T05:26:15.299157: step 5039, loss 0.0132365, acc 1
2016-09-06T05:26:16.070742: step 5040, loss 0.0132895, acc 1
2016-09-06T05:26:16.882334: step 5041, loss 0.01588, acc 1
2016-09-06T05:26:17.679611: step 5042, loss 0.00401447, acc 1
2016-09-06T05:26:18.517368: step 5043, loss 0.0514484, acc 0.98
2016-09-06T05:26:19.327096: step 5044, loss 0.0453632, acc 0.98
2016-09-06T05:26:20.101412: step 5045, loss 0.0230185, acc 0.98
2016-09-06T05:26:20.901824: step 5046, loss 0.0205597, acc 0.98
2016-09-06T05:26:21.727962: step 5047, loss 0.0790183, acc 0.98
2016-09-06T05:26:22.499642: step 5048, loss 0.0511036, acc 0.96
2016-09-06T05:26:23.296510: step 5049, loss 0.00806113, acc 1
2016-09-06T05:26:24.123554: step 5050, loss 0.0856871, acc 0.96
2016-09-06T05:26:24.919202: step 5051, loss 0.00749096, acc 1
2016-09-06T05:26:25.747376: step 5052, loss 0.00390971, acc 1
2016-09-06T05:26:26.560793: step 5053, loss 0.111238, acc 0.94
2016-09-06T05:26:27.352096: step 5054, loss 0.00976212, acc 1
2016-09-06T05:26:28.184002: step 5055, loss 0.0186097, acc 1
2016-09-06T05:26:29.010283: step 5056, loss 0.0352331, acc 0.96
2016-09-06T05:26:29.803357: step 5057, loss 0.00638912, acc 1
2016-09-06T05:26:30.627600: step 5058, loss 0.0189446, acc 1
2016-09-06T05:26:31.466584: step 5059, loss 0.00937314, acc 1
2016-09-06T05:26:32.282293: step 5060, loss 0.0917278, acc 0.94
2016-09-06T05:26:33.068743: step 5061, loss 0.025913, acc 1
2016-09-06T05:26:33.912381: step 5062, loss 0.00353674, acc 1
2016-09-06T05:26:34.701547: step 5063, loss 0.00755083, acc 1
2016-09-06T05:26:35.505572: step 5064, loss 0.0065694, acc 1
2016-09-06T05:26:36.316960: step 5065, loss 0.0177231, acc 1
2016-09-06T05:26:37.100914: step 5066, loss 0.0340855, acc 0.98
2016-09-06T05:26:37.941153: step 5067, loss 0.0296273, acc 1
2016-09-06T05:26:38.769703: step 5068, loss 0.00547501, acc 1
2016-09-06T05:26:39.585663: step 5069, loss 0.0099416, acc 1
2016-09-06T05:26:40.394415: step 5070, loss 0.015134, acc 1
2016-09-06T05:26:41.233882: step 5071, loss 0.0414867, acc 1
2016-09-06T05:26:42.027189: step 5072, loss 0.00625895, acc 1
2016-09-06T05:26:42.838065: step 5073, loss 0.00834553, acc 1
2016-09-06T05:26:43.674293: step 5074, loss 0.0141062, acc 1
2016-09-06T05:26:44.485751: step 5075, loss 0.104905, acc 0.94
2016-09-06T05:26:45.304121: step 5076, loss 0.045512, acc 0.98
2016-09-06T05:26:46.162863: step 5077, loss 0.01933, acc 1
2016-09-06T05:26:46.990811: step 5078, loss 0.0326139, acc 1
2016-09-06T05:26:47.760286: step 5079, loss 0.00319694, acc 1
2016-09-06T05:26:48.578230: step 5080, loss 0.0385521, acc 0.98
2016-09-06T05:26:49.376202: step 5081, loss 0.00568361, acc 1
2016-09-06T05:26:50.184118: step 5082, loss 0.0113598, acc 1
2016-09-06T05:26:51.034203: step 5083, loss 0.0454455, acc 0.96
2016-09-06T05:26:51.854285: step 5084, loss 0.00316366, acc 1
2016-09-06T05:26:52.665014: step 5085, loss 0.0582094, acc 0.96
2016-09-06T05:26:53.481767: step 5086, loss 0.0165021, acc 1
2016-09-06T05:26:54.312207: step 5087, loss 0.0230609, acc 0.98
2016-09-06T05:26:55.119728: step 5088, loss 0.045615, acc 0.98
2016-09-06T05:26:55.947832: step 5089, loss 0.00691281, acc 1
2016-09-06T05:26:56.767787: step 5090, loss 0.0671547, acc 0.98
2016-09-06T05:26:57.559176: step 5091, loss 0.025174, acc 1
2016-09-06T05:26:58.342618: step 5092, loss 0.102801, acc 0.98
2016-09-06T05:26:59.153155: step 5093, loss 0.0452593, acc 0.96
2016-09-06T05:26:59.933876: step 5094, loss 0.00607299, acc 1
2016-09-06T05:27:00.758129: step 5095, loss 0.0138847, acc 1
2016-09-06T05:27:01.564424: step 5096, loss 0.0298287, acc 0.98
2016-09-06T05:27:02.401621: step 5097, loss 0.016653, acc 1
2016-09-06T05:27:03.201049: step 5098, loss 0.0176021, acc 1
2016-09-06T05:27:04.015295: step 5099, loss 0.0330749, acc 0.98
2016-09-06T05:27:04.786455: step 5100, loss 0.0500694, acc 0.98

Evaluation:
2016-09-06T05:27:08.540920: step 5100, loss 2.17816, acc 0.739212

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-5100

2016-09-06T05:27:10.443966: step 5101, loss 0.0343281, acc 1
2016-09-06T05:27:11.266959: step 5102, loss 0.00756192, acc 1
2016-09-06T05:27:12.080503: step 5103, loss 0.00323831, acc 1
2016-09-06T05:27:12.918583: step 5104, loss 0.0420437, acc 0.96
2016-09-06T05:27:13.718379: step 5105, loss 0.0254925, acc 0.98
2016-09-06T05:27:14.530013: step 5106, loss 0.00336991, acc 1
2016-09-06T05:27:15.331929: step 5107, loss 0.0653325, acc 0.96
2016-09-06T05:27:16.141267: step 5108, loss 0.0893532, acc 0.96
2016-09-06T05:27:16.980086: step 5109, loss 0.0373119, acc 1
2016-09-06T05:27:17.815223: step 5110, loss 0.00318135, acc 1
2016-09-06T05:27:18.631556: step 5111, loss 0.0151351, acc 1
2016-09-06T05:27:19.414431: step 5112, loss 0.0243864, acc 1
2016-09-06T05:27:20.222402: step 5113, loss 0.0249299, acc 0.98
2016-09-06T05:27:21.036771: step 5114, loss 0.0132429, acc 1
2016-09-06T05:27:21.843082: step 5115, loss 0.0212706, acc 0.98
2016-09-06T05:27:22.649215: step 5116, loss 0.0380013, acc 0.96
2016-09-06T05:27:23.483619: step 5117, loss 0.0568665, acc 0.98
2016-09-06T05:27:24.286024: step 5118, loss 0.0183634, acc 1
2016-09-06T05:27:25.107980: step 5119, loss 0.00759549, acc 1
2016-09-06T05:27:25.942863: step 5120, loss 0.00993877, acc 1
2016-09-06T05:27:26.739803: step 5121, loss 0.0322356, acc 0.98
2016-09-06T05:27:27.531163: step 5122, loss 0.0178216, acc 1
2016-09-06T05:27:28.330294: step 5123, loss 0.043703, acc 0.98
2016-09-06T05:27:29.124921: step 5124, loss 0.0598335, acc 0.98
2016-09-06T05:27:29.906556: step 5125, loss 0.0134225, acc 1
2016-09-06T05:27:30.728095: step 5126, loss 0.0455697, acc 0.98
2016-09-06T05:27:31.491201: step 5127, loss 0.0615137, acc 0.98
2016-09-06T05:27:32.302897: step 5128, loss 0.021612, acc 0.98
2016-09-06T05:27:33.129958: step 5129, loss 0.00833028, acc 1
2016-09-06T05:27:33.903504: step 5130, loss 0.0789465, acc 0.96
2016-09-06T05:27:34.703807: step 5131, loss 0.0155704, acc 1
2016-09-06T05:27:35.540184: step 5132, loss 0.00857726, acc 1
2016-09-06T05:27:36.336910: step 5133, loss 0.0284049, acc 0.98
2016-09-06T05:27:37.150789: step 5134, loss 0.0190467, acc 0.98
2016-09-06T05:27:37.984374: step 5135, loss 0.0270412, acc 1
2016-09-06T05:27:38.777260: step 5136, loss 0.0255046, acc 1
2016-09-06T05:27:39.612424: step 5137, loss 0.00382227, acc 1
2016-09-06T05:27:40.508670: step 5138, loss 0.0129827, acc 1
2016-09-06T05:27:41.327226: step 5139, loss 0.0282194, acc 0.98
2016-09-06T05:27:42.148654: step 5140, loss 0.0102769, acc 1
2016-09-06T05:27:42.966234: step 5141, loss 0.0284749, acc 1
2016-09-06T05:27:43.759218: step 5142, loss 0.0883439, acc 0.94
2016-09-06T05:27:44.570283: step 5143, loss 0.0221485, acc 0.98
2016-09-06T05:27:45.393891: step 5144, loss 0.00310741, acc 1
2016-09-06T05:27:46.233400: step 5145, loss 0.0160384, acc 1
2016-09-06T05:27:47.049247: step 5146, loss 0.0102069, acc 1
2016-09-06T05:27:47.885534: step 5147, loss 0.0136342, acc 1
2016-09-06T05:27:48.708060: step 5148, loss 0.0552689, acc 0.96
2016-09-06T05:27:49.526784: step 5149, loss 0.0149837, acc 1
2016-09-06T05:27:50.342879: step 5150, loss 0.0223629, acc 0.98
2016-09-06T05:27:51.195313: step 5151, loss 0.048223, acc 0.98
2016-09-06T05:27:51.992774: step 5152, loss 0.0887436, acc 0.96
2016-09-06T05:27:52.813438: step 5153, loss 0.0860585, acc 0.96
2016-09-06T05:27:53.626317: step 5154, loss 0.0176697, acc 1
2016-09-06T05:27:54.457237: step 5155, loss 0.00711944, acc 1
2016-09-06T05:27:55.276609: step 5156, loss 0.0205095, acc 0.98
2016-09-06T05:27:56.110954: step 5157, loss 0.0472173, acc 0.98
2016-09-06T05:27:56.897396: step 5158, loss 0.0293081, acc 0.98
2016-09-06T05:27:57.690908: step 5159, loss 0.0031183, acc 1
2016-09-06T05:27:58.523475: step 5160, loss 0.0157222, acc 1
2016-09-06T05:27:59.283899: step 5161, loss 0.0239904, acc 1
2016-09-06T05:28:00.072547: step 5162, loss 0.051416, acc 0.98
2016-09-06T05:28:00.895534: step 5163, loss 0.0187693, acc 0.98
2016-09-06T05:28:01.687127: step 5164, loss 0.00628959, acc 1
2016-09-06T05:28:02.534630: step 5165, loss 0.0621764, acc 0.98
2016-09-06T05:28:03.364717: step 5166, loss 0.0177558, acc 1
2016-09-06T05:28:04.153827: step 5167, loss 0.041093, acc 0.98
2016-09-06T05:28:04.945207: step 5168, loss 0.0190295, acc 1
2016-09-06T05:28:05.756520: step 5169, loss 0.024498, acc 0.98
2016-09-06T05:28:06.531841: step 5170, loss 0.0393591, acc 0.98
2016-09-06T05:28:07.323115: step 5171, loss 0.0170222, acc 1
2016-09-06T05:28:08.150403: step 5172, loss 0.0359578, acc 0.96
2016-09-06T05:28:08.938299: step 5173, loss 0.00390178, acc 1
2016-09-06T05:28:09.740794: step 5174, loss 0.00950802, acc 1
2016-09-06T05:28:10.566579: step 5175, loss 0.00331938, acc 1
2016-09-06T05:28:11.369292: step 5176, loss 0.028363, acc 0.98
2016-09-06T05:28:12.168902: step 5177, loss 0.0355767, acc 1
2016-09-06T05:28:12.981557: step 5178, loss 0.00573225, acc 1
2016-09-06T05:28:13.776691: step 5179, loss 0.0189914, acc 1
2016-09-06T05:28:14.574957: step 5180, loss 0.0281563, acc 0.98
2016-09-06T05:28:15.405218: step 5181, loss 0.173413, acc 0.98
2016-09-06T05:28:16.207389: step 5182, loss 0.00509694, acc 1
2016-09-06T05:28:16.994544: step 5183, loss 0.0632406, acc 0.96
2016-09-06T05:28:17.758349: step 5184, loss 0.00817843, acc 1
2016-09-06T05:28:18.575158: step 5185, loss 0.0326063, acc 0.98
2016-09-06T05:28:19.392512: step 5186, loss 0.0646843, acc 0.96
2016-09-06T05:28:20.216808: step 5187, loss 0.0930575, acc 0.96
2016-09-06T05:28:20.998365: step 5188, loss 0.0830525, acc 0.96
2016-09-06T05:28:21.801816: step 5189, loss 0.0589068, acc 0.96
2016-09-06T05:28:22.621944: step 5190, loss 0.0220834, acc 0.98
2016-09-06T05:28:23.428342: step 5191, loss 0.0143933, acc 1
2016-09-06T05:28:24.226593: step 5192, loss 0.0236103, acc 0.98
2016-09-06T05:28:25.052304: step 5193, loss 0.0382082, acc 0.96
2016-09-06T05:28:25.847038: step 5194, loss 0.0418658, acc 0.98
2016-09-06T05:28:26.652752: step 5195, loss 0.00274374, acc 1
2016-09-06T05:28:27.503034: step 5196, loss 0.0131411, acc 1
2016-09-06T05:28:28.285125: step 5197, loss 0.0485805, acc 0.98
2016-09-06T05:28:29.085909: step 5198, loss 0.0251666, acc 1
2016-09-06T05:28:29.911498: step 5199, loss 0.0494413, acc 0.98
2016-09-06T05:28:30.679676: step 5200, loss 0.0279447, acc 1

Evaluation:
2016-09-06T05:28:34.409404: step 5200, loss 1.74984, acc 0.732645

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-5200

2016-09-06T05:28:36.311171: step 5201, loss 0.04354, acc 0.96
2016-09-06T05:28:37.118452: step 5202, loss 0.0029389, acc 1
2016-09-06T05:28:37.914422: step 5203, loss 0.0141218, acc 1
2016-09-06T05:28:38.748278: step 5204, loss 0.00401483, acc 1
2016-09-06T05:28:39.555956: step 5205, loss 0.0191348, acc 1
2016-09-06T05:28:40.375961: step 5206, loss 0.0375953, acc 0.98
2016-09-06T05:28:41.192458: step 5207, loss 0.0229831, acc 1
2016-09-06T05:28:42.045205: step 5208, loss 0.00350074, acc 1
2016-09-06T05:28:42.834247: step 5209, loss 0.0368422, acc 0.98
2016-09-06T05:28:43.614034: step 5210, loss 0.0916211, acc 0.94
2016-09-06T05:28:44.420981: step 5211, loss 0.103554, acc 0.94
2016-09-06T05:28:45.223243: step 5212, loss 0.0212646, acc 1
2016-09-06T05:28:46.052293: step 5213, loss 0.0320891, acc 0.98
2016-09-06T05:28:46.885320: step 5214, loss 0.0130439, acc 1
2016-09-06T05:28:47.649210: step 5215, loss 0.0490427, acc 0.96
2016-09-06T05:28:48.449018: step 5216, loss 0.00522493, acc 1
2016-09-06T05:28:49.262122: step 5217, loss 0.0418218, acc 0.98
2016-09-06T05:28:50.061445: step 5218, loss 0.0209753, acc 0.98
2016-09-06T05:28:50.895660: step 5219, loss 0.00554584, acc 1
2016-09-06T05:28:51.720400: step 5220, loss 0.0337673, acc 1
2016-09-06T05:28:52.517416: step 5221, loss 0.00238478, acc 1
2016-09-06T05:28:53.303107: step 5222, loss 0.0283323, acc 1
2016-09-06T05:28:54.116392: step 5223, loss 0.029278, acc 0.98
2016-09-06T05:28:54.903285: step 5224, loss 0.00366288, acc 1
2016-09-06T05:28:55.726031: step 5225, loss 0.0122346, acc 1
2016-09-06T05:28:56.522282: step 5226, loss 0.0106707, acc 1
2016-09-06T05:28:57.303990: step 5227, loss 0.00926948, acc 1
2016-09-06T05:28:58.103213: step 5228, loss 0.0818001, acc 0.98
2016-09-06T05:28:58.949534: step 5229, loss 0.00334868, acc 1
2016-09-06T05:28:59.739458: step 5230, loss 0.111673, acc 0.96
2016-09-06T05:29:00.588137: step 5231, loss 0.147962, acc 0.98
2016-09-06T05:29:01.421519: step 5232, loss 0.0247904, acc 0.98
2016-09-06T05:29:02.249322: step 5233, loss 0.0158541, acc 1
2016-09-06T05:29:03.051687: step 5234, loss 0.00815989, acc 1
2016-09-06T05:29:03.868340: step 5235, loss 0.0377133, acc 1
2016-09-06T05:29:04.660957: step 5236, loss 0.0199439, acc 1
2016-09-06T05:29:05.481744: step 5237, loss 0.0617844, acc 0.94
2016-09-06T05:29:06.341615: step 5238, loss 0.0232528, acc 0.98
2016-09-06T05:29:07.177026: step 5239, loss 0.00370768, acc 1
2016-09-06T05:29:07.993749: step 5240, loss 0.00833674, acc 1
2016-09-06T05:29:08.842201: step 5241, loss 0.0212516, acc 1
2016-09-06T05:29:09.658084: step 5242, loss 0.0864711, acc 0.96
2016-09-06T05:29:10.441366: step 5243, loss 0.00893536, acc 1
2016-09-06T05:29:11.268825: step 5244, loss 0.0447898, acc 0.96
2016-09-06T05:29:12.068092: step 5245, loss 0.0547845, acc 0.98
2016-09-06T05:29:12.876433: step 5246, loss 0.0270151, acc 1
2016-09-06T05:29:13.709750: step 5247, loss 0.00469614, acc 1
2016-09-06T05:29:14.528927: step 5248, loss 0.0160237, acc 1
2016-09-06T05:29:15.368640: step 5249, loss 0.0184432, acc 1
2016-09-06T05:29:16.203451: step 5250, loss 0.0274992, acc 0.98
2016-09-06T05:29:17.039167: step 5251, loss 0.0491778, acc 0.98
2016-09-06T05:29:17.821288: step 5252, loss 0.0371138, acc 1
2016-09-06T05:29:18.651594: step 5253, loss 0.0327751, acc 0.98
2016-09-06T05:29:19.475045: step 5254, loss 0.0266186, acc 1
2016-09-06T05:29:20.272527: step 5255, loss 0.0582639, acc 0.96
2016-09-06T05:29:21.079893: step 5256, loss 0.0442139, acc 0.98
2016-09-06T05:29:21.930377: step 5257, loss 0.0850053, acc 0.98
2016-09-06T05:29:22.736709: step 5258, loss 0.0117534, acc 1
2016-09-06T05:29:23.545057: step 5259, loss 0.037799, acc 1
2016-09-06T05:29:24.355748: step 5260, loss 0.025523, acc 0.98
2016-09-06T05:29:25.140491: step 5261, loss 0.0348365, acc 0.98
2016-09-06T05:29:25.934882: step 5262, loss 0.020682, acc 1
2016-09-06T05:29:26.744562: step 5263, loss 0.0185165, acc 1
2016-09-06T05:29:27.533975: step 5264, loss 0.0156958, acc 1
2016-09-06T05:29:28.315088: step 5265, loss 0.0154195, acc 1
2016-09-06T05:29:29.144152: step 5266, loss 0.0183227, acc 1
2016-09-06T05:29:29.950372: step 5267, loss 0.0345839, acc 1
2016-09-06T05:29:30.767415: step 5268, loss 0.0150901, acc 1
2016-09-06T05:29:31.588765: step 5269, loss 0.0338869, acc 1
2016-09-06T05:29:32.361613: step 5270, loss 0.0295208, acc 1
2016-09-06T05:29:33.166585: step 5271, loss 0.00531861, acc 1
2016-09-06T05:29:33.993375: step 5272, loss 0.0985086, acc 0.94
2016-09-06T05:29:34.780927: step 5273, loss 0.161195, acc 0.96
2016-09-06T05:29:35.584722: step 5274, loss 0.00495326, acc 1
2016-09-06T05:29:36.401439: step 5275, loss 0.0403915, acc 0.98
2016-09-06T05:29:37.193993: step 5276, loss 0.00632409, acc 1
2016-09-06T05:29:38.001400: step 5277, loss 0.00987824, acc 1
2016-09-06T05:29:38.847301: step 5278, loss 0.00671648, acc 1
2016-09-06T05:29:39.647150: step 5279, loss 0.0406378, acc 0.98
2016-09-06T05:29:40.447026: step 5280, loss 0.0185169, acc 1
2016-09-06T05:29:41.297364: step 5281, loss 0.00539266, acc 1
2016-09-06T05:29:42.094799: step 5282, loss 0.130394, acc 0.94
2016-09-06T05:29:42.902537: step 5283, loss 0.0143764, acc 1
2016-09-06T05:29:43.719782: step 5284, loss 0.00413977, acc 1
2016-09-06T05:29:44.531126: step 5285, loss 0.00944514, acc 1
2016-09-06T05:29:45.335827: step 5286, loss 0.0333458, acc 0.98
2016-09-06T05:29:46.203275: step 5287, loss 0.00638957, acc 1
2016-09-06T05:29:47.059780: step 5288, loss 0.0153337, acc 1
2016-09-06T05:29:47.873123: step 5289, loss 0.0537674, acc 0.96
2016-09-06T05:29:48.712081: step 5290, loss 0.024044, acc 0.98
2016-09-06T05:29:49.488606: step 5291, loss 0.043484, acc 0.98
2016-09-06T05:29:50.298076: step 5292, loss 0.0212217, acc 0.98
2016-09-06T05:29:51.141139: step 5293, loss 0.03875, acc 0.98
2016-09-06T05:29:51.941560: step 5294, loss 0.0474649, acc 0.98
2016-09-06T05:29:52.757434: step 5295, loss 0.00716293, acc 1
2016-09-06T05:29:53.607262: step 5296, loss 0.0138611, acc 1
2016-09-06T05:29:54.458815: step 5297, loss 0.131829, acc 0.98
2016-09-06T05:29:55.273294: step 5298, loss 0.00576141, acc 1
2016-09-06T05:29:56.167724: step 5299, loss 0.145605, acc 0.96
2016-09-06T05:29:56.987485: step 5300, loss 0.00490767, acc 1

Evaluation:
2016-09-06T05:30:00.693679: step 5300, loss 2.11346, acc 0.745779

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-5300

2016-09-06T05:30:02.579707: step 5301, loss 0.0183461, acc 0.98
2016-09-06T05:30:03.418603: step 5302, loss 0.00772308, acc 1
2016-09-06T05:30:04.252010: step 5303, loss 0.0175896, acc 1
2016-09-06T05:30:05.075755: step 5304, loss 0.00476787, acc 1
2016-09-06T05:30:05.904592: step 5305, loss 0.028886, acc 1
2016-09-06T05:30:06.726850: step 5306, loss 0.00332773, acc 1
2016-09-06T05:30:07.509402: step 5307, loss 0.00791316, acc 1
2016-09-06T05:30:08.312927: step 5308, loss 0.0391321, acc 0.98
2016-09-06T05:30:09.119331: step 5309, loss 0.0103701, acc 1
2016-09-06T05:30:09.930488: step 5310, loss 0.0155689, acc 1
2016-09-06T05:30:10.758230: step 5311, loss 0.0183953, acc 0.98
2016-09-06T05:30:11.606309: step 5312, loss 0.0163942, acc 1
2016-09-06T05:30:12.391721: step 5313, loss 0.018768, acc 1
2016-09-06T05:30:13.213453: step 5314, loss 0.0623706, acc 0.96
2016-09-06T05:30:14.041232: step 5315, loss 0.0597398, acc 0.96
2016-09-06T05:30:14.846490: step 5316, loss 0.0300878, acc 0.98
2016-09-06T05:30:15.667460: step 5317, loss 0.010896, acc 1
2016-09-06T05:30:16.512964: step 5318, loss 0.0271171, acc 0.98
2016-09-06T05:30:17.318547: step 5319, loss 0.00829264, acc 1
2016-09-06T05:30:18.132564: step 5320, loss 0.0105782, acc 1
2016-09-06T05:30:18.972261: step 5321, loss 0.00313658, acc 1
2016-09-06T05:30:19.794904: step 5322, loss 0.0791506, acc 0.94
2016-09-06T05:30:20.618055: step 5323, loss 0.00538813, acc 1
2016-09-06T05:30:21.444218: step 5324, loss 0.0244543, acc 0.98
2016-09-06T05:30:22.241386: step 5325, loss 0.0076761, acc 1
2016-09-06T05:30:23.091430: step 5326, loss 0.0196621, acc 0.98
2016-09-06T05:30:23.906054: step 5327, loss 0.0130518, acc 1
2016-09-06T05:30:24.705812: step 5328, loss 0.0569132, acc 0.96
2016-09-06T05:30:25.524297: step 5329, loss 0.0317687, acc 0.98
2016-09-06T05:30:26.362666: step 5330, loss 0.0580044, acc 0.98
2016-09-06T05:30:27.189892: step 5331, loss 0.0297456, acc 1
2016-09-06T05:30:27.987717: step 5332, loss 0.0446528, acc 0.98
2016-09-06T05:30:28.847504: step 5333, loss 0.00940306, acc 1
2016-09-06T05:30:29.660492: step 5334, loss 0.0224458, acc 1
2016-09-06T05:30:30.458803: step 5335, loss 0.0153821, acc 1
2016-09-06T05:30:31.293243: step 5336, loss 0.0212298, acc 0.98
2016-09-06T05:30:32.149662: step 5337, loss 0.0123375, acc 1
2016-09-06T05:30:32.972703: step 5338, loss 0.0935866, acc 0.94
2016-09-06T05:30:33.759005: step 5339, loss 0.0684674, acc 0.98
2016-09-06T05:30:34.589948: step 5340, loss 0.0874372, acc 0.96
2016-09-06T05:30:35.384146: step 5341, loss 0.0195542, acc 1
2016-09-06T05:30:36.209150: step 5342, loss 0.0204197, acc 0.98
2016-09-06T05:30:37.037255: step 5343, loss 0.0142949, acc 1
2016-09-06T05:30:37.857764: step 5344, loss 0.0548298, acc 0.98
2016-09-06T05:30:38.675273: step 5345, loss 0.0982459, acc 0.96
2016-09-06T05:30:39.516977: step 5346, loss 0.0148197, acc 1
2016-09-06T05:30:40.304379: step 5347, loss 0.0302731, acc 0.98
2016-09-06T05:30:41.126319: step 5348, loss 0.0286395, acc 1
2016-09-06T05:30:41.966408: step 5349, loss 0.0182882, acc 1
2016-09-06T05:30:42.765395: step 5350, loss 0.0396651, acc 0.98
2016-09-06T05:30:43.594999: step 5351, loss 0.0999801, acc 0.98
2016-09-06T05:30:44.422895: step 5352, loss 0.00875311, acc 1
2016-09-06T05:30:45.232672: step 5353, loss 0.0246272, acc 1
2016-09-06T05:30:46.027155: step 5354, loss 0.00922337, acc 1
2016-09-06T05:30:46.874382: step 5355, loss 0.00461688, acc 1
2016-09-06T05:30:47.699706: step 5356, loss 0.00805028, acc 1
2016-09-06T05:30:48.515983: step 5357, loss 0.021782, acc 1
2016-09-06T05:30:49.355323: step 5358, loss 0.0517123, acc 0.98
2016-09-06T05:30:50.190821: step 5359, loss 0.0376547, acc 0.98
2016-09-06T05:30:51.006028: step 5360, loss 0.0271083, acc 1
2016-09-06T05:30:51.818439: step 5361, loss 0.16955, acc 0.96
2016-09-06T05:30:52.639231: step 5362, loss 0.00824606, acc 1
2016-09-06T05:30:53.458590: step 5363, loss 0.0214787, acc 1
2016-09-06T05:30:54.284599: step 5364, loss 0.0340096, acc 0.98
2016-09-06T05:30:55.104210: step 5365, loss 0.00716282, acc 1
2016-09-06T05:30:55.881384: step 5366, loss 0.037441, acc 0.98
2016-09-06T05:30:56.679167: step 5367, loss 0.00508142, acc 1
2016-09-06T05:30:57.509738: step 5368, loss 0.00566011, acc 1
2016-09-06T05:30:58.317774: step 5369, loss 0.00834658, acc 1
2016-09-06T05:30:59.125015: step 5370, loss 0.0239579, acc 0.98
2016-09-06T05:30:59.965452: step 5371, loss 0.0181632, acc 1
2016-09-06T05:31:00.770959: step 5372, loss 0.155584, acc 0.96
2016-09-06T05:31:01.566455: step 5373, loss 0.0308153, acc 1
2016-09-06T05:31:02.385911: step 5374, loss 0.0274419, acc 0.98
2016-09-06T05:31:03.174550: step 5375, loss 0.0122989, acc 1
2016-09-06T05:31:03.905100: step 5376, loss 0.0113892, acc 1
2016-09-06T05:31:04.709282: step 5377, loss 0.0995963, acc 0.92
2016-09-06T05:31:05.511835: step 5378, loss 0.0292555, acc 1
2016-09-06T05:31:06.325975: step 5379, loss 0.00722333, acc 1
2016-09-06T05:31:07.151803: step 5380, loss 0.0444134, acc 0.98
2016-09-06T05:31:07.927357: step 5381, loss 0.0616842, acc 0.98
2016-09-06T05:31:08.775752: step 5382, loss 0.0106082, acc 1
2016-09-06T05:31:09.604185: step 5383, loss 0.0476497, acc 0.98
2016-09-06T05:31:10.403188: step 5384, loss 0.00979137, acc 1
2016-09-06T05:31:11.225419: step 5385, loss 0.0387507, acc 1
2016-09-06T05:31:12.025303: step 5386, loss 0.00551396, acc 1
2016-09-06T05:31:12.799555: step 5387, loss 0.0170821, acc 1
2016-09-06T05:31:13.607570: step 5388, loss 0.0114408, acc 1
2016-09-06T05:31:14.434591: step 5389, loss 0.109616, acc 0.94
2016-09-06T05:31:15.241443: step 5390, loss 0.0181477, acc 1
2016-09-06T05:31:16.040877: step 5391, loss 0.0263731, acc 0.98
2016-09-06T05:31:16.864290: step 5392, loss 0.0146907, acc 1
2016-09-06T05:31:17.669865: step 5393, loss 0.0454857, acc 0.98
2016-09-06T05:31:18.480540: step 5394, loss 0.0564207, acc 0.98
2016-09-06T05:31:19.304206: step 5395, loss 0.0214451, acc 1
2016-09-06T05:31:20.115099: step 5396, loss 0.00708986, acc 1
2016-09-06T05:31:20.925411: step 5397, loss 0.0138844, acc 1
2016-09-06T05:31:21.754297: step 5398, loss 0.0444079, acc 0.98
2016-09-06T05:31:22.550002: step 5399, loss 0.0111185, acc 1
2016-09-06T05:31:23.353795: step 5400, loss 0.0324959, acc 0.98

Evaluation:
2016-09-06T05:31:27.123424: step 5400, loss 2.03879, acc 0.733584

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-5400

2016-09-06T05:31:29.097264: step 5401, loss 0.127429, acc 0.94
2016-09-06T05:31:29.897697: step 5402, loss 0.0588944, acc 0.98
2016-09-06T05:31:30.678191: step 5403, loss 0.0380853, acc 0.98
2016-09-06T05:31:31.512905: step 5404, loss 0.0233709, acc 0.98
2016-09-06T05:31:32.342235: step 5405, loss 0.0198347, acc 1
2016-09-06T05:31:33.156673: step 5406, loss 0.00869431, acc 1
2016-09-06T05:31:34.005496: step 5407, loss 0.00726271, acc 1
2016-09-06T05:31:34.838058: step 5408, loss 0.00818481, acc 1
2016-09-06T05:31:35.653378: step 5409, loss 0.0269992, acc 0.98
2016-09-06T05:31:36.500189: step 5410, loss 0.0434579, acc 0.98
2016-09-06T05:31:37.346874: step 5411, loss 0.0405679, acc 0.98
2016-09-06T05:31:38.169034: step 5412, loss 0.038161, acc 0.98
2016-09-06T05:31:38.986685: step 5413, loss 0.015551, acc 1
2016-09-06T05:31:39.803418: step 5414, loss 0.0560704, acc 0.94
2016-09-06T05:31:40.625531: step 5415, loss 0.0217565, acc 0.98
2016-09-06T05:31:41.499542: step 5416, loss 0.025808, acc 0.98
2016-09-06T05:31:42.301640: step 5417, loss 0.0147124, acc 1
2016-09-06T05:31:43.113797: step 5418, loss 0.0147189, acc 1
2016-09-06T05:31:43.959811: step 5419, loss 0.0194007, acc 1
2016-09-06T05:31:44.775986: step 5420, loss 0.0152906, acc 1
2016-09-06T05:31:45.543081: step 5421, loss 0.0184642, acc 1
2016-09-06T05:31:46.351936: step 5422, loss 0.0192325, acc 1
2016-09-06T05:31:47.158849: step 5423, loss 0.0347509, acc 0.98
2016-09-06T05:31:47.950475: step 5424, loss 0.0709623, acc 0.98
2016-09-06T05:31:48.757011: step 5425, loss 0.0529738, acc 0.94
2016-09-06T05:31:49.576085: step 5426, loss 0.0114318, acc 1
2016-09-06T05:31:50.375060: step 5427, loss 0.0230812, acc 0.98
2016-09-06T05:31:51.188802: step 5428, loss 0.0405307, acc 0.96
2016-09-06T05:31:51.988213: step 5429, loss 0.040811, acc 1
2016-09-06T05:31:52.805584: step 5430, loss 0.106988, acc 0.98
2016-09-06T05:31:53.651747: step 5431, loss 0.0121415, acc 1
2016-09-06T05:31:54.471337: step 5432, loss 0.0124709, acc 1
2016-09-06T05:31:55.262975: step 5433, loss 0.0424449, acc 0.98
2016-09-06T05:31:56.073919: step 5434, loss 0.0968449, acc 0.94
2016-09-06T05:31:56.884167: step 5435, loss 0.080557, acc 0.98
2016-09-06T05:31:57.672934: step 5436, loss 0.0107753, acc 1
2016-09-06T05:31:58.480137: step 5437, loss 0.0307235, acc 1
2016-09-06T05:31:59.297549: step 5438, loss 0.0120607, acc 1
2016-09-06T05:32:00.081395: step 5439, loss 0.00431625, acc 1
2016-09-06T05:32:00.932414: step 5440, loss 0.0373343, acc 0.98
2016-09-06T05:32:01.747186: step 5441, loss 0.0195451, acc 1
2016-09-06T05:32:02.550534: step 5442, loss 0.0501021, acc 0.98
2016-09-06T05:32:03.354481: step 5443, loss 0.0303632, acc 0.98
2016-09-06T05:32:04.169893: step 5444, loss 0.0944008, acc 0.94
2016-09-06T05:32:04.972520: step 5445, loss 0.0967098, acc 0.98
2016-09-06T05:32:05.764587: step 5446, loss 0.0616953, acc 0.98
2016-09-06T05:32:06.583603: step 5447, loss 0.0196544, acc 1
2016-09-06T05:32:07.382536: step 5448, loss 0.00453313, acc 1
2016-09-06T05:32:08.198266: step 5449, loss 0.0212702, acc 1
2016-09-06T05:32:09.004174: step 5450, loss 0.0274951, acc 1
2016-09-06T05:32:09.775927: step 5451, loss 0.0458899, acc 0.98
2016-09-06T05:32:10.577839: step 5452, loss 0.0116752, acc 1
2016-09-06T05:32:11.394579: step 5453, loss 0.0540964, acc 0.98
2016-09-06T05:32:12.180680: step 5454, loss 0.0296099, acc 1
2016-09-06T05:32:12.994769: step 5455, loss 0.0394551, acc 0.98
2016-09-06T05:32:13.792133: step 5456, loss 0.012145, acc 1
2016-09-06T05:32:14.568856: step 5457, loss 0.0204057, acc 1
2016-09-06T05:32:15.375244: step 5458, loss 0.0169641, acc 1
2016-09-06T05:32:16.209164: step 5459, loss 0.0214607, acc 1
2016-09-06T05:32:16.982485: step 5460, loss 0.00410396, acc 1
2016-09-06T05:32:17.817011: step 5461, loss 0.0377836, acc 0.98
2016-09-06T05:32:18.626879: step 5462, loss 0.00656433, acc 1
2016-09-06T05:32:19.401723: step 5463, loss 0.00515682, acc 1
2016-09-06T05:32:20.249009: step 5464, loss 0.0501274, acc 0.98
2016-09-06T05:32:21.066676: step 5465, loss 0.085225, acc 0.96
2016-09-06T05:32:21.875075: step 5466, loss 0.020677, acc 1
2016-09-06T05:32:22.678107: step 5467, loss 0.0367843, acc 0.98
2016-09-06T05:32:23.523155: step 5468, loss 0.0052239, acc 1
2016-09-06T05:32:24.367204: step 5469, loss 0.0306902, acc 0.98
2016-09-06T05:32:25.192813: step 5470, loss 0.0150097, acc 1
2016-09-06T05:32:26.028513: step 5471, loss 0.0168193, acc 1
2016-09-06T05:32:26.822440: step 5472, loss 0.00610645, acc 1
2016-09-06T05:32:27.642173: step 5473, loss 0.00517173, acc 1
2016-09-06T05:32:28.462996: step 5474, loss 0.100011, acc 0.96
2016-09-06T05:32:29.294365: step 5475, loss 0.0181697, acc 0.98
2016-09-06T05:32:30.112167: step 5476, loss 0.024904, acc 0.98
2016-09-06T05:32:30.933113: step 5477, loss 0.033669, acc 0.98
2016-09-06T05:32:31.778853: step 5478, loss 0.0209194, acc 0.98
2016-09-06T05:32:32.641923: step 5479, loss 0.0276757, acc 0.98
2016-09-06T05:32:33.502454: step 5480, loss 0.0047153, acc 1
2016-09-06T05:32:34.348563: step 5481, loss 0.0269563, acc 0.98
2016-09-06T05:32:35.167994: step 5482, loss 0.0500899, acc 0.98
2016-09-06T05:32:35.968678: step 5483, loss 0.0488084, acc 0.96
2016-09-06T05:32:36.781264: step 5484, loss 0.0080204, acc 1
2016-09-06T05:32:37.584558: step 5485, loss 0.0308328, acc 0.98
2016-09-06T05:32:38.398436: step 5486, loss 0.0389036, acc 0.98
2016-09-06T05:32:39.234207: step 5487, loss 0.0154135, acc 1
2016-09-06T05:32:40.070512: step 5488, loss 0.0394373, acc 0.96
2016-09-06T05:32:40.891575: step 5489, loss 0.0337897, acc 0.98
2016-09-06T05:32:41.714896: step 5490, loss 0.00886969, acc 1
2016-09-06T05:32:42.555372: step 5491, loss 0.0563027, acc 0.96
2016-09-06T05:32:43.413049: step 5492, loss 0.069388, acc 0.98
2016-09-06T05:32:44.256486: step 5493, loss 0.0652796, acc 0.98
2016-09-06T05:32:45.111310: step 5494, loss 0.00400041, acc 1
2016-09-06T05:32:45.954513: step 5495, loss 0.0124004, acc 1
2016-09-06T05:32:46.785210: step 5496, loss 0.0181021, acc 1
2016-09-06T05:32:47.628169: step 5497, loss 0.029502, acc 0.98
2016-09-06T05:32:48.397955: step 5498, loss 0.00368554, acc 1
2016-09-06T05:32:49.198454: step 5499, loss 0.00453818, acc 1
2016-09-06T05:32:50.007757: step 5500, loss 0.040966, acc 0.98

Evaluation:
2016-09-06T05:32:53.748884: step 5500, loss 2.1829, acc 0.734522

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-5500

2016-09-06T05:32:55.735031: step 5501, loss 0.0351521, acc 0.98
2016-09-06T05:32:56.527860: step 5502, loss 0.00626795, acc 1
2016-09-06T05:32:57.326021: step 5503, loss 0.017576, acc 1
2016-09-06T05:32:58.101350: step 5504, loss 0.0130484, acc 1
2016-09-06T05:32:58.914740: step 5505, loss 0.00360254, acc 1
2016-09-06T05:32:59.734278: step 5506, loss 0.0532964, acc 0.98
2016-09-06T05:33:00.546935: step 5507, loss 0.0299221, acc 1
2016-09-06T05:33:01.357101: step 5508, loss 0.0898897, acc 0.94
2016-09-06T05:33:02.167414: step 5509, loss 0.0225021, acc 0.98
2016-09-06T05:33:02.968621: step 5510, loss 0.0103124, acc 1
2016-09-06T05:33:03.762089: step 5511, loss 0.0499068, acc 0.96
2016-09-06T05:33:04.584167: step 5512, loss 0.0221104, acc 0.98
2016-09-06T05:33:05.375838: step 5513, loss 0.0169329, acc 1
2016-09-06T05:33:06.180480: step 5514, loss 0.0367251, acc 0.98
2016-09-06T05:33:07.010121: step 5515, loss 0.0167039, acc 1
2016-09-06T05:33:07.814932: step 5516, loss 0.00427896, acc 1
2016-09-06T05:33:08.610195: step 5517, loss 0.0132261, acc 1
2016-09-06T05:33:09.420906: step 5518, loss 0.00565477, acc 1
2016-09-06T05:33:10.207205: step 5519, loss 0.0087536, acc 1
2016-09-06T05:33:10.999548: step 5520, loss 0.0644233, acc 0.98
2016-09-06T05:33:11.857372: step 5521, loss 0.0265308, acc 1
2016-09-06T05:33:12.664804: step 5522, loss 0.0289472, acc 0.98
2016-09-06T05:33:13.461053: step 5523, loss 0.0293561, acc 1
2016-09-06T05:33:14.289178: step 5524, loss 0.0220877, acc 0.98
2016-09-06T05:33:15.108519: step 5525, loss 0.0123178, acc 1
2016-09-06T05:33:15.944974: step 5526, loss 0.0425199, acc 0.96
2016-09-06T05:33:16.768166: step 5527, loss 0.00334575, acc 1
2016-09-06T05:33:17.581465: step 5528, loss 0.0120432, acc 1
2016-09-06T05:33:18.428468: step 5529, loss 0.00862832, acc 1
2016-09-06T05:33:19.275857: step 5530, loss 0.00367308, acc 1
2016-09-06T05:33:20.102925: step 5531, loss 0.0140966, acc 1
2016-09-06T05:33:20.938472: step 5532, loss 0.0494543, acc 0.98
2016-09-06T05:33:21.801725: step 5533, loss 0.0387709, acc 0.98
2016-09-06T05:33:22.615917: step 5534, loss 0.0187981, acc 1
2016-09-06T05:33:23.409811: step 5535, loss 0.00447466, acc 1
2016-09-06T05:33:24.234307: step 5536, loss 0.0219437, acc 1
2016-09-06T05:33:25.050461: step 5537, loss 0.0416093, acc 0.98
2016-09-06T05:33:25.871167: step 5538, loss 0.0108901, acc 1
2016-09-06T05:33:26.711461: step 5539, loss 0.028966, acc 0.98
2016-09-06T05:33:27.558785: step 5540, loss 0.00810499, acc 1
2016-09-06T05:33:28.349983: step 5541, loss 0.0525121, acc 0.98
2016-09-06T05:33:29.125641: step 5542, loss 0.0541648, acc 0.94
2016-09-06T05:33:29.960157: step 5543, loss 0.00898239, acc 1
2016-09-06T05:33:30.730753: step 5544, loss 0.0260662, acc 1
2016-09-06T05:33:31.566290: step 5545, loss 0.0609128, acc 0.96
2016-09-06T05:33:32.357864: step 5546, loss 0.0638058, acc 0.96
2016-09-06T05:33:33.129298: step 5547, loss 0.0177582, acc 1
2016-09-06T05:33:33.941432: step 5548, loss 0.00951091, acc 1
2016-09-06T05:33:34.766953: step 5549, loss 0.0228802, acc 0.98
2016-09-06T05:33:35.536268: step 5550, loss 0.00881866, acc 1
2016-09-06T05:33:36.349462: step 5551, loss 0.0719224, acc 0.96
2016-09-06T05:33:37.172465: step 5552, loss 0.0722034, acc 0.98
2016-09-06T05:33:37.969109: step 5553, loss 0.102923, acc 0.98
2016-09-06T05:33:38.762976: step 5554, loss 0.0885915, acc 0.96
2016-09-06T05:33:39.561251: step 5555, loss 0.0683656, acc 0.96
2016-09-06T05:33:40.373075: step 5556, loss 0.0210097, acc 0.98
2016-09-06T05:33:41.187458: step 5557, loss 0.0222702, acc 1
2016-09-06T05:33:42.009628: step 5558, loss 0.0202447, acc 1
2016-09-06T05:33:42.815545: step 5559, loss 0.0033822, acc 1
2016-09-06T05:33:43.619169: step 5560, loss 0.00503624, acc 1
2016-09-06T05:33:44.436242: step 5561, loss 0.0168787, acc 1
2016-09-06T05:33:45.230597: step 5562, loss 0.0167557, acc 1
2016-09-06T05:33:46.069546: step 5563, loss 0.0162177, acc 1
2016-09-06T05:33:46.865345: step 5564, loss 0.00685512, acc 1
2016-09-06T05:33:47.641339: step 5565, loss 0.0117031, acc 1
2016-09-06T05:33:48.465829: step 5566, loss 0.0185779, acc 1
2016-09-06T05:33:49.278989: step 5567, loss 0.107515, acc 0.96
2016-09-06T05:33:49.998021: step 5568, loss 0.0345517, acc 0.977273
2016-09-06T05:33:50.830907: step 5569, loss 0.0229894, acc 0.98
2016-09-06T05:33:51.648332: step 5570, loss 0.0786366, acc 0.96
2016-09-06T05:33:52.440026: step 5571, loss 0.0294723, acc 1
2016-09-06T05:33:53.254619: step 5572, loss 0.0222423, acc 1
2016-09-06T05:33:54.076115: step 5573, loss 0.0247729, acc 0.98
2016-09-06T05:33:54.858034: step 5574, loss 0.0183591, acc 1
2016-09-06T05:33:55.684789: step 5575, loss 0.048096, acc 0.96
2016-09-06T05:33:56.502996: step 5576, loss 0.0432452, acc 0.98
2016-09-06T05:33:57.340206: step 5577, loss 0.0843443, acc 0.96
2016-09-06T05:33:58.132270: step 5578, loss 0.0466581, acc 0.96
2016-09-06T05:33:58.966190: step 5579, loss 0.0196466, acc 0.98
2016-09-06T05:33:59.749106: step 5580, loss 0.0317343, acc 0.98
2016-09-06T05:34:00.579954: step 5581, loss 0.0164321, acc 1
2016-09-06T05:34:01.402093: step 5582, loss 0.0365128, acc 0.98
2016-09-06T05:34:02.234677: step 5583, loss 0.083675, acc 0.98
2016-09-06T05:34:03.042749: step 5584, loss 0.0140233, acc 1
2016-09-06T05:34:03.873017: step 5585, loss 0.041342, acc 0.98
2016-09-06T05:34:04.702176: step 5586, loss 0.0214327, acc 1
2016-09-06T05:34:05.521137: step 5587, loss 0.0118104, acc 1
2016-09-06T05:34:06.341746: step 5588, loss 0.00938413, acc 1
2016-09-06T05:34:07.153666: step 5589, loss 0.0481726, acc 0.98
2016-09-06T05:34:07.980354: step 5590, loss 0.0656727, acc 0.94
2016-09-06T05:34:08.809847: step 5591, loss 0.0375428, acc 0.98
2016-09-06T05:34:09.610987: step 5592, loss 0.0189132, acc 1
2016-09-06T05:34:10.415828: step 5593, loss 0.0362, acc 0.98
2016-09-06T05:34:11.264163: step 5594, loss 0.044384, acc 1
2016-09-06T05:34:12.103410: step 5595, loss 0.0199515, acc 1
2016-09-06T05:34:12.931520: step 5596, loss 0.0232023, acc 1
2016-09-06T05:34:13.779829: step 5597, loss 0.0329789, acc 1
2016-09-06T05:34:14.627339: step 5598, loss 0.00749187, acc 1
2016-09-06T05:34:15.430512: step 5599, loss 0.0156685, acc 1
2016-09-06T05:34:16.258441: step 5600, loss 0.0300159, acc 1

Evaluation:
2016-09-06T05:34:19.991896: step 5600, loss 1.87902, acc 0.742026

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-5600

2016-09-06T05:34:21.977188: step 5601, loss 0.025493, acc 1
2016-09-06T05:34:22.797678: step 5602, loss 0.0173969, acc 1
2016-09-06T05:34:23.627134: step 5603, loss 0.0838275, acc 0.96
2016-09-06T05:34:24.439064: step 5604, loss 0.0453115, acc 0.98
2016-09-06T05:34:25.290865: step 5605, loss 0.00430575, acc 1
2016-09-06T05:34:26.109335: step 5606, loss 0.0548723, acc 0.98
2016-09-06T05:34:26.949495: step 5607, loss 0.0423333, acc 0.98
2016-09-06T05:34:27.776806: step 5608, loss 0.028223, acc 0.98
2016-09-06T05:34:28.593441: step 5609, loss 0.00348337, acc 1
2016-09-06T05:34:29.448646: step 5610, loss 0.0110581, acc 1
2016-09-06T05:34:30.275825: step 5611, loss 0.206137, acc 0.96
2016-09-06T05:34:31.109481: step 5612, loss 0.00574479, acc 1
2016-09-06T05:34:31.952269: step 5613, loss 0.00290349, acc 1
2016-09-06T05:34:32.779443: step 5614, loss 0.0233478, acc 1
2016-09-06T05:34:33.576455: step 5615, loss 0.0210307, acc 0.98
2016-09-06T05:34:34.385698: step 5616, loss 0.0167776, acc 1
2016-09-06T05:34:35.216155: step 5617, loss 0.0583929, acc 0.96
2016-09-06T05:34:36.022189: step 5618, loss 0.00744688, acc 1
2016-09-06T05:34:36.840902: step 5619, loss 0.0111228, acc 1
2016-09-06T05:34:37.663580: step 5620, loss 0.0253395, acc 1
2016-09-06T05:34:38.453869: step 5621, loss 0.0264488, acc 0.98
2016-09-06T05:34:39.263606: step 5622, loss 0.0218965, acc 1
2016-09-06T05:34:40.067260: step 5623, loss 0.0216906, acc 1
2016-09-06T05:34:40.854728: step 5624, loss 0.12796, acc 0.96
2016-09-06T05:34:41.647290: step 5625, loss 0.00328332, acc 1
2016-09-06T05:34:42.488360: step 5626, loss 0.00671497, acc 1
2016-09-06T05:34:43.258308: step 5627, loss 0.0640683, acc 0.98
2016-09-06T05:34:44.065892: step 5628, loss 0.0117555, acc 1
2016-09-06T05:34:44.901426: step 5629, loss 0.0565493, acc 0.96
2016-09-06T05:34:45.705441: step 5630, loss 0.0311171, acc 1
2016-09-06T05:34:46.528465: step 5631, loss 0.00742251, acc 1
2016-09-06T05:34:47.334873: step 5632, loss 0.0180494, acc 1
2016-09-06T05:34:48.104782: step 5633, loss 0.0162714, acc 1
2016-09-06T05:34:48.937206: step 5634, loss 0.0574547, acc 0.96
2016-09-06T05:34:49.764188: step 5635, loss 0.0776224, acc 0.96
2016-09-06T05:34:50.537624: step 5636, loss 0.0339348, acc 0.98
2016-09-06T05:34:51.331332: step 5637, loss 0.025062, acc 1
2016-09-06T05:34:52.162719: step 5638, loss 0.0654836, acc 0.96
2016-09-06T05:34:52.938634: step 5639, loss 0.0253577, acc 1
2016-09-06T05:34:53.740239: step 5640, loss 0.0143358, acc 1
2016-09-06T05:34:54.584995: step 5641, loss 0.0376893, acc 0.98
2016-09-06T05:34:55.356457: step 5642, loss 0.0113141, acc 1
2016-09-06T05:34:56.195827: step 5643, loss 0.0159638, acc 1
2016-09-06T05:34:57.028181: step 5644, loss 0.0175924, acc 0.98
2016-09-06T05:34:57.855220: step 5645, loss 0.0613175, acc 0.98
2016-09-06T05:34:58.684907: step 5646, loss 0.0324811, acc 0.98
2016-09-06T05:34:59.525156: step 5647, loss 0.00680301, acc 1
2016-09-06T05:35:00.322602: step 5648, loss 0.0963259, acc 0.96
2016-09-06T05:35:01.124339: step 5649, loss 0.0181985, acc 0.98
2016-09-06T05:35:01.962008: step 5650, loss 0.00679453, acc 1
2016-09-06T05:35:02.792089: step 5651, loss 0.00431032, acc 1
2016-09-06T05:35:03.597765: step 5652, loss 0.0202441, acc 0.98
2016-09-06T05:35:04.443208: step 5653, loss 0.0382525, acc 0.98
2016-09-06T05:35:05.243410: step 5654, loss 0.0354294, acc 0.98
2016-09-06T05:35:06.033573: step 5655, loss 0.0132023, acc 1
2016-09-06T05:35:06.874316: step 5656, loss 0.0399626, acc 0.96
2016-09-06T05:35:07.735445: step 5657, loss 0.0648627, acc 0.98
2016-09-06T05:35:08.584200: step 5658, loss 0.00836403, acc 1
2016-09-06T05:35:09.386783: step 5659, loss 0.0187411, acc 1
2016-09-06T05:35:10.195883: step 5660, loss 0.00728298, acc 1
2016-09-06T05:35:11.028501: step 5661, loss 0.0644287, acc 0.96
2016-09-06T05:35:11.823993: step 5662, loss 0.0224269, acc 1
2016-09-06T05:35:12.652786: step 5663, loss 0.0242986, acc 1
2016-09-06T05:35:13.435742: step 5664, loss 0.0166414, acc 1
2016-09-06T05:35:14.233071: step 5665, loss 0.0101233, acc 1
2016-09-06T05:35:15.044357: step 5666, loss 0.02765, acc 0.98
2016-09-06T05:35:15.872881: step 5667, loss 0.00875645, acc 1
2016-09-06T05:35:16.677040: step 5668, loss 0.00330637, acc 1
2016-09-06T05:35:17.534330: step 5669, loss 0.00426895, acc 1
2016-09-06T05:35:18.313905: step 5670, loss 0.0097368, acc 1
2016-09-06T05:35:19.122922: step 5671, loss 0.0707597, acc 0.98
2016-09-06T05:35:19.963615: step 5672, loss 0.0039114, acc 1
2016-09-06T05:35:20.760234: step 5673, loss 0.0353059, acc 0.96
2016-09-06T05:35:21.556273: step 5674, loss 0.0282805, acc 0.98
2016-09-06T05:35:22.379720: step 5675, loss 0.0189844, acc 0.98
2016-09-06T05:35:23.179001: step 5676, loss 0.0402789, acc 1
2016-09-06T05:35:23.969406: step 5677, loss 0.0435336, acc 0.98
2016-09-06T05:35:24.799476: step 5678, loss 0.0464893, acc 0.98
2016-09-06T05:35:25.615070: step 5679, loss 0.00673943, acc 1
2016-09-06T05:35:26.420176: step 5680, loss 0.0831515, acc 0.94
2016-09-06T05:35:27.220197: step 5681, loss 0.077223, acc 0.98
2016-09-06T05:35:28.063427: step 5682, loss 0.0130565, acc 1
2016-09-06T05:35:28.879723: step 5683, loss 0.0095831, acc 1
2016-09-06T05:35:29.713829: step 5684, loss 0.0903345, acc 0.96
2016-09-06T05:35:30.507704: step 5685, loss 0.018829, acc 1
2016-09-06T05:35:31.305832: step 5686, loss 0.00343353, acc 1
2016-09-06T05:35:32.136907: step 5687, loss 0.0241622, acc 0.98
2016-09-06T05:35:32.962650: step 5688, loss 0.045186, acc 0.98
2016-09-06T05:35:33.765603: step 5689, loss 0.00347939, acc 1
2016-09-06T05:35:34.605144: step 5690, loss 0.00449525, acc 1
2016-09-06T05:35:35.419195: step 5691, loss 0.0568769, acc 0.96
2016-09-06T05:35:36.231003: step 5692, loss 0.00915648, acc 1
2016-09-06T05:35:37.045571: step 5693, loss 0.014294, acc 1
2016-09-06T05:35:37.864489: step 5694, loss 0.0367405, acc 0.98
2016-09-06T05:35:38.681481: step 5695, loss 0.0222309, acc 1
2016-09-06T05:35:39.497628: step 5696, loss 0.0983541, acc 0.96
2016-09-06T05:35:40.325310: step 5697, loss 0.0351108, acc 0.98
2016-09-06T05:35:41.138774: step 5698, loss 0.253092, acc 0.96
2016-09-06T05:35:41.969684: step 5699, loss 0.022132, acc 0.98
2016-09-06T05:35:42.788805: step 5700, loss 0.0265343, acc 1

Evaluation:
2016-09-06T05:35:46.496116: step 5700, loss 2.07401, acc 0.744841

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-5700

2016-09-06T05:35:48.400674: step 5701, loss 0.0397163, acc 0.96
2016-09-06T05:35:49.268428: step 5702, loss 0.0380961, acc 0.98
2016-09-06T05:35:50.072714: step 5703, loss 0.0413574, acc 0.98
2016-09-06T05:35:50.913959: step 5704, loss 0.0158241, acc 1
2016-09-06T05:35:51.734629: step 5705, loss 0.0105125, acc 1
2016-09-06T05:35:52.558826: step 5706, loss 0.0641588, acc 0.96
2016-09-06T05:35:53.359577: step 5707, loss 0.00797535, acc 1
2016-09-06T05:35:54.167457: step 5708, loss 0.00720459, acc 1
2016-09-06T05:35:54.993400: step 5709, loss 0.0292967, acc 1
2016-09-06T05:35:55.787725: step 5710, loss 0.0670801, acc 0.98
2016-09-06T05:35:56.578986: step 5711, loss 0.0950825, acc 0.94
2016-09-06T05:35:57.406240: step 5712, loss 0.0447675, acc 0.98
2016-09-06T05:35:58.180843: step 5713, loss 0.0396199, acc 0.98
2016-09-06T05:35:58.997340: step 5714, loss 0.00600626, acc 1
2016-09-06T05:35:59.809806: step 5715, loss 0.0652251, acc 0.98
2016-09-06T05:36:00.639055: step 5716, loss 0.0220275, acc 0.98
2016-09-06T05:36:01.438841: step 5717, loss 0.0211245, acc 0.98
2016-09-06T05:36:02.257271: step 5718, loss 0.0572362, acc 0.96
2016-09-06T05:36:03.040200: step 5719, loss 0.069485, acc 0.98
2016-09-06T05:36:03.834016: step 5720, loss 0.00685525, acc 1
2016-09-06T05:36:04.648935: step 5721, loss 0.0149151, acc 1
2016-09-06T05:36:05.462885: step 5722, loss 0.11553, acc 0.96
2016-09-06T05:36:06.257839: step 5723, loss 0.067253, acc 0.98
2016-09-06T05:36:07.078567: step 5724, loss 0.0640809, acc 0.96
2016-09-06T05:36:07.892197: step 5725, loss 0.0463839, acc 0.96
2016-09-06T05:36:08.696100: step 5726, loss 0.016168, acc 1
2016-09-06T05:36:09.513456: step 5727, loss 0.0249155, acc 0.98
2016-09-06T05:36:10.320297: step 5728, loss 0.0358998, acc 1
2016-09-06T05:36:11.131664: step 5729, loss 0.0839304, acc 0.96
2016-09-06T05:36:11.960747: step 5730, loss 0.0530751, acc 0.96
2016-09-06T05:36:12.733994: step 5731, loss 0.00445571, acc 1
2016-09-06T05:36:13.525361: step 5732, loss 0.0203606, acc 0.98
2016-09-06T05:36:14.331929: step 5733, loss 0.105494, acc 0.98
2016-09-06T05:36:15.103883: step 5734, loss 0.0154108, acc 1
2016-09-06T05:36:15.933417: step 5735, loss 0.00949627, acc 1
2016-09-06T05:36:16.737357: step 5736, loss 0.0555246, acc 0.96
2016-09-06T05:36:17.523399: step 5737, loss 0.0571333, acc 0.98
2016-09-06T05:36:18.332658: step 5738, loss 0.0620039, acc 0.98
2016-09-06T05:36:19.139507: step 5739, loss 0.0248818, acc 1
2016-09-06T05:36:19.919547: step 5740, loss 0.00712068, acc 1
2016-09-06T05:36:20.741967: step 5741, loss 0.0436765, acc 0.98
2016-09-06T05:36:21.573456: step 5742, loss 0.023394, acc 1
2016-09-06T05:36:22.361269: step 5743, loss 0.0856371, acc 0.96
2016-09-06T05:36:23.164973: step 5744, loss 0.0501654, acc 0.98
2016-09-06T05:36:23.979053: step 5745, loss 0.0643571, acc 0.96
2016-09-06T05:36:24.757300: step 5746, loss 0.0497747, acc 0.98
2016-09-06T05:36:25.570187: step 5747, loss 0.0576061, acc 1
2016-09-06T05:36:26.371530: step 5748, loss 0.0509413, acc 1
2016-09-06T05:36:27.200544: step 5749, loss 0.0163416, acc 1
2016-09-06T05:36:27.999423: step 5750, loss 0.0286144, acc 1
2016-09-06T05:36:28.848811: step 5751, loss 0.00477425, acc 1
2016-09-06T05:36:29.647194: step 5752, loss 0.0124364, acc 1
2016-09-06T05:36:30.454759: step 5753, loss 0.0162437, acc 1
2016-09-06T05:36:31.307264: step 5754, loss 0.0673131, acc 0.98
2016-09-06T05:36:32.131178: step 5755, loss 0.0777986, acc 0.92
2016-09-06T05:36:32.947715: step 5756, loss 0.00495222, acc 1
2016-09-06T05:36:33.753886: step 5757, loss 0.0140979, acc 1
2016-09-06T05:36:34.567954: step 5758, loss 0.00683327, acc 1
2016-09-06T05:36:35.412125: step 5759, loss 0.018572, acc 1
2016-09-06T05:36:36.164657: step 5760, loss 0.0169428, acc 1
2016-09-06T05:36:36.981086: step 5761, loss 0.0436264, acc 0.98
2016-09-06T05:36:37.807136: step 5762, loss 0.0572139, acc 0.98
2016-09-06T05:36:38.664990: step 5763, loss 0.0353749, acc 0.98
2016-09-06T05:36:39.456828: step 5764, loss 0.0183998, acc 1
2016-09-06T05:36:40.249131: step 5765, loss 0.0177112, acc 1
2016-09-06T05:36:41.089218: step 5766, loss 0.0427504, acc 0.98
2016-09-06T05:36:41.915830: step 5767, loss 0.0188205, acc 1
2016-09-06T05:36:42.735196: step 5768, loss 0.0103085, acc 1
2016-09-06T05:36:43.556381: step 5769, loss 0.02409, acc 1
2016-09-06T05:36:44.384627: step 5770, loss 0.0078815, acc 1
2016-09-06T05:36:45.182230: step 5771, loss 0.0046053, acc 1
2016-09-06T05:36:46.022704: step 5772, loss 0.0170235, acc 1
2016-09-06T05:36:46.810462: step 5773, loss 0.0758052, acc 0.98
2016-09-06T05:36:47.585653: step 5774, loss 0.0227291, acc 1
2016-09-06T05:36:48.411113: step 5775, loss 0.0487051, acc 0.98
2016-09-06T05:36:49.228808: step 5776, loss 0.00573025, acc 1
2016-09-06T05:36:50.025409: step 5777, loss 0.0470881, acc 0.98
2016-09-06T05:36:50.850129: step 5778, loss 0.00762265, acc 1
2016-09-06T05:36:51.639787: step 5779, loss 0.00695111, acc 1
2016-09-06T05:36:52.458234: step 5780, loss 0.0141098, acc 1
2016-09-06T05:36:53.291431: step 5781, loss 0.00526415, acc 1
2016-09-06T05:36:54.092392: step 5782, loss 0.019671, acc 1
2016-09-06T05:36:54.914062: step 5783, loss 0.0529272, acc 0.98
2016-09-06T05:36:55.741937: step 5784, loss 0.0635813, acc 0.94
2016-09-06T05:36:56.548478: step 5785, loss 0.0106978, acc 1
2016-09-06T05:36:57.377452: step 5786, loss 0.0228212, acc 1
2016-09-06T05:36:58.208681: step 5787, loss 0.0185984, acc 0.98
2016-09-06T05:36:59.005654: step 5788, loss 0.00671269, acc 1
2016-09-06T05:36:59.804511: step 5789, loss 0.0122366, acc 1
2016-09-06T05:37:00.645704: step 5790, loss 0.00490787, acc 1
2016-09-06T05:37:01.478185: step 5791, loss 0.0149769, acc 1
2016-09-06T05:37:02.268133: step 5792, loss 0.0104837, acc 1
2016-09-06T05:37:03.064994: step 5793, loss 0.0569213, acc 0.96
2016-09-06T05:37:03.891692: step 5794, loss 0.0334399, acc 0.98
2016-09-06T05:37:04.735350: step 5795, loss 0.00352913, acc 1
2016-09-06T05:37:05.525845: step 5796, loss 0.0480578, acc 0.98
2016-09-06T05:37:06.330996: step 5797, loss 0.0118637, acc 1
2016-09-06T05:37:07.113003: step 5798, loss 0.0278238, acc 0.98
2016-09-06T05:37:07.903540: step 5799, loss 0.0173573, acc 0.98
2016-09-06T05:37:08.756164: step 5800, loss 0.00976015, acc 1

Evaluation:
2016-09-06T05:37:12.469697: step 5800, loss 2.35234, acc 0.74015

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-5800

2016-09-06T05:37:14.346923: step 5801, loss 0.0362245, acc 1
2016-09-06T05:37:15.169948: step 5802, loss 0.00400174, acc 1
2016-09-06T05:37:15.989405: step 5803, loss 0.0341179, acc 1
2016-09-06T05:37:16.785383: step 5804, loss 0.0130524, acc 1
2016-09-06T05:37:17.625928: step 5805, loss 0.00459505, acc 1
2016-09-06T05:37:18.411976: step 5806, loss 0.0640749, acc 0.98
2016-09-06T05:37:19.208796: step 5807, loss 0.0602378, acc 0.96
2016-09-06T05:37:20.037463: step 5808, loss 0.0558924, acc 0.98
2016-09-06T05:37:20.850821: step 5809, loss 0.00817995, acc 1
2016-09-06T05:37:21.651220: step 5810, loss 0.00699054, acc 1
2016-09-06T05:37:22.437367: step 5811, loss 0.00987591, acc 1
2016-09-06T05:37:23.243709: step 5812, loss 0.0425687, acc 0.98
2016-09-06T05:37:24.014707: step 5813, loss 0.0306151, acc 1
2016-09-06T05:37:24.812289: step 5814, loss 0.0181265, acc 0.98
2016-09-06T05:37:25.633538: step 5815, loss 0.030429, acc 0.98
2016-09-06T05:37:26.401195: step 5816, loss 0.0715167, acc 0.98
2016-09-06T05:37:27.237226: step 5817, loss 0.00636599, acc 1
2016-09-06T05:37:28.086571: step 5818, loss 0.0435554, acc 0.98
2016-09-06T05:37:28.861899: step 5819, loss 0.00540351, acc 1
2016-09-06T05:37:29.662795: step 5820, loss 0.0117329, acc 1
2016-09-06T05:37:30.477573: step 5821, loss 0.0285782, acc 1
2016-09-06T05:37:31.306213: step 5822, loss 0.00308386, acc 1
2016-09-06T05:37:32.125643: step 5823, loss 0.0140056, acc 1
2016-09-06T05:37:32.947255: step 5824, loss 0.0559923, acc 0.98
2016-09-06T05:37:33.731406: step 5825, loss 0.00768243, acc 1
2016-09-06T05:37:34.526281: step 5826, loss 0.124917, acc 0.98
2016-09-06T05:37:35.325991: step 5827, loss 0.018254, acc 1
2016-09-06T05:37:36.107457: step 5828, loss 0.0935791, acc 0.96
2016-09-06T05:37:36.918791: step 5829, loss 0.0424118, acc 1
2016-09-06T05:37:37.755703: step 5830, loss 0.0267381, acc 0.98
2016-09-06T05:37:38.568416: step 5831, loss 0.0564497, acc 0.98
2016-09-06T05:37:39.381872: step 5832, loss 0.0198783, acc 1
2016-09-06T05:37:40.262897: step 5833, loss 0.038737, acc 0.98
2016-09-06T05:37:41.078050: step 5834, loss 0.013572, acc 1
2016-09-06T05:37:41.888396: step 5835, loss 0.0252556, acc 1
2016-09-06T05:37:42.742977: step 5836, loss 0.0313265, acc 0.98
2016-09-06T05:37:43.577594: step 5837, loss 0.0103867, acc 1
2016-09-06T05:37:44.420194: step 5838, loss 0.0578145, acc 0.96
2016-09-06T05:37:45.235270: step 5839, loss 0.00279831, acc 1
2016-09-06T05:37:46.052182: step 5840, loss 0.00522564, acc 1
2016-09-06T05:37:46.869806: step 5841, loss 0.00684707, acc 1
2016-09-06T05:37:47.743116: step 5842, loss 0.0190238, acc 0.98
2016-09-06T05:37:48.585930: step 5843, loss 0.00979958, acc 1
2016-09-06T05:37:49.368240: step 5844, loss 0.0427601, acc 0.98
2016-09-06T05:37:50.181681: step 5845, loss 0.0496567, acc 0.98
2016-09-06T05:37:50.988795: step 5846, loss 0.0542024, acc 0.98
2016-09-06T05:37:51.766283: step 5847, loss 0.00283939, acc 1
2016-09-06T05:37:52.576977: step 5848, loss 0.0033517, acc 1
2016-09-06T05:37:53.422029: step 5849, loss 0.0173284, acc 1
2016-09-06T05:37:54.206391: step 5850, loss 0.022083, acc 0.98
2016-09-06T05:37:55.005758: step 5851, loss 0.0107215, acc 1
2016-09-06T05:37:55.837606: step 5852, loss 0.00555801, acc 1
2016-09-06T05:37:56.625799: step 5853, loss 0.0201453, acc 1
2016-09-06T05:37:57.428597: step 5854, loss 0.00411884, acc 1
2016-09-06T05:37:58.253442: step 5855, loss 0.00923378, acc 1
2016-09-06T05:37:59.039251: step 5856, loss 0.015047, acc 1
2016-09-06T05:37:59.836124: step 5857, loss 0.00337011, acc 1
2016-09-06T05:38:00.644199: step 5858, loss 0.0514537, acc 0.98
2016-09-06T05:38:01.467426: step 5859, loss 0.0116485, acc 1
2016-09-06T05:38:02.259246: step 5860, loss 0.00665533, acc 1
2016-09-06T05:38:03.078055: step 5861, loss 0.0159958, acc 1
2016-09-06T05:38:03.862098: step 5862, loss 0.00626413, acc 1
2016-09-06T05:38:04.689516: step 5863, loss 0.00982823, acc 1
2016-09-06T05:38:05.496127: step 5864, loss 0.00327726, acc 1
2016-09-06T05:38:06.303876: step 5865, loss 0.0492873, acc 0.96
2016-09-06T05:38:07.108992: step 5866, loss 0.0118221, acc 1
2016-09-06T05:38:07.940798: step 5867, loss 0.0142812, acc 1
2016-09-06T05:38:08.742847: step 5868, loss 0.013881, acc 1
2016-09-06T05:38:09.542827: step 5869, loss 0.0849866, acc 0.98
2016-09-06T05:38:10.352614: step 5870, loss 0.0315913, acc 0.98
2016-09-06T05:38:11.155328: step 5871, loss 0.0055232, acc 1
2016-09-06T05:38:12.006046: step 5872, loss 0.0307833, acc 0.98
2016-09-06T05:38:12.826968: step 5873, loss 0.00943092, acc 1
2016-09-06T05:38:13.613711: step 5874, loss 0.0370856, acc 1
2016-09-06T05:38:14.424230: step 5875, loss 0.0416942, acc 0.98
2016-09-06T05:38:15.225957: step 5876, loss 0.0210754, acc 1
2016-09-06T05:38:16.030122: step 5877, loss 0.0257657, acc 0.98
2016-09-06T05:38:16.807271: step 5878, loss 0.00299628, acc 1
2016-09-06T05:38:17.644227: step 5879, loss 0.0240677, acc 0.98
2016-09-06T05:38:18.423099: step 5880, loss 0.0597894, acc 0.96
2016-09-06T05:38:19.211014: step 5881, loss 0.0148032, acc 1
2016-09-06T05:38:20.040717: step 5882, loss 0.027096, acc 0.98
2016-09-06T05:38:20.822278: step 5883, loss 0.0208884, acc 1
2016-09-06T05:38:21.630767: step 5884, loss 0.0179301, acc 1
2016-09-06T05:38:22.468384: step 5885, loss 0.0549511, acc 0.98
2016-09-06T05:38:23.226542: step 5886, loss 0.00783819, acc 1
2016-09-06T05:38:24.040313: step 5887, loss 0.0087329, acc 1
2016-09-06T05:38:24.873440: step 5888, loss 0.108537, acc 0.94
2016-09-06T05:38:25.694688: step 5889, loss 0.00716025, acc 1
2016-09-06T05:38:26.493646: step 5890, loss 0.0813561, acc 0.98
2016-09-06T05:38:27.306983: step 5891, loss 0.0563928, acc 0.98
2016-09-06T05:38:28.099102: step 5892, loss 0.00927762, acc 1
2016-09-06T05:38:28.902540: step 5893, loss 0.0439439, acc 0.98
2016-09-06T05:38:29.736632: step 5894, loss 0.0182037, acc 1
2016-09-06T05:38:30.558775: step 5895, loss 0.0705278, acc 0.98
2016-09-06T05:38:31.378124: step 5896, loss 0.0212116, acc 1
2016-09-06T05:38:32.216145: step 5897, loss 0.0220039, acc 0.98
2016-09-06T05:38:33.037737: step 5898, loss 0.0623649, acc 0.94
2016-09-06T05:38:33.820619: step 5899, loss 0.00617103, acc 1
2016-09-06T05:38:34.669145: step 5900, loss 0.00491617, acc 1

Evaluation:
2016-09-06T05:38:38.368585: step 5900, loss 2.26748, acc 0.745779

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-5900

2016-09-06T05:38:40.131258: step 5901, loss 0.115865, acc 0.96
2016-09-06T05:38:40.941678: step 5902, loss 0.00853248, acc 1
2016-09-06T05:38:41.753579: step 5903, loss 0.00465107, acc 1
2016-09-06T05:38:42.543913: step 5904, loss 0.0566164, acc 0.96
2016-09-06T05:38:43.371756: step 5905, loss 0.0599234, acc 0.98
2016-09-06T05:38:44.169633: step 5906, loss 0.0220801, acc 0.98
2016-09-06T05:38:44.980171: step 5907, loss 0.0242235, acc 0.98
2016-09-06T05:38:45.837629: step 5908, loss 0.00575747, acc 1
2016-09-06T05:38:46.673352: step 5909, loss 0.0331525, acc 0.98
2016-09-06T05:38:47.456244: step 5910, loss 0.0214798, acc 0.98
2016-09-06T05:38:48.273477: step 5911, loss 0.00288824, acc 1
2016-09-06T05:38:49.083739: step 5912, loss 0.00422693, acc 1
2016-09-06T05:38:49.881715: step 5913, loss 0.0122677, acc 1
2016-09-06T05:38:50.662208: step 5914, loss 0.0133534, acc 1
2016-09-06T05:38:51.509078: step 5915, loss 0.0198933, acc 1
2016-09-06T05:38:52.283303: step 5916, loss 0.0279622, acc 0.98
2016-09-06T05:38:53.079054: step 5917, loss 0.0596754, acc 0.98
2016-09-06T05:38:53.917984: step 5918, loss 0.0312081, acc 0.98
2016-09-06T05:38:54.700880: step 5919, loss 0.0348105, acc 0.98
2016-09-06T05:38:55.515835: step 5920, loss 0.0512104, acc 0.98
2016-09-06T05:38:56.335887: step 5921, loss 0.0146693, acc 1
2016-09-06T05:38:57.125632: step 5922, loss 0.0144969, acc 1
2016-09-06T05:38:57.937402: step 5923, loss 0.0208308, acc 0.98
2016-09-06T05:38:58.769326: step 5924, loss 0.00601915, acc 1
2016-09-06T05:38:59.589315: step 5925, loss 0.00873892, acc 1
2016-09-06T05:39:00.417729: step 5926, loss 0.0595534, acc 0.96
2016-09-06T05:39:01.279569: step 5927, loss 0.0148746, acc 1
2016-09-06T05:39:02.104480: step 5928, loss 0.0297569, acc 0.98
2016-09-06T05:39:02.908840: step 5929, loss 0.0538132, acc 0.98
2016-09-06T05:39:03.737016: step 5930, loss 0.0105498, acc 1
2016-09-06T05:39:04.559659: step 5931, loss 0.0121971, acc 1
2016-09-06T05:39:05.379713: step 5932, loss 0.0113637, acc 1
2016-09-06T05:39:06.208315: step 5933, loss 0.0715111, acc 0.98
2016-09-06T05:39:07.008959: step 5934, loss 0.0396637, acc 0.98
2016-09-06T05:39:07.797128: step 5935, loss 0.00604668, acc 1
2016-09-06T05:39:08.634833: step 5936, loss 0.0253426, acc 0.98
2016-09-06T05:39:09.436626: step 5937, loss 0.00506512, acc 1
2016-09-06T05:39:10.253833: step 5938, loss 0.0551821, acc 0.96
2016-09-06T05:39:11.080244: step 5939, loss 0.0207358, acc 1
2016-09-06T05:39:11.889601: step 5940, loss 0.0221903, acc 1
2016-09-06T05:39:12.696248: step 5941, loss 0.0172577, acc 1
2016-09-06T05:39:13.529881: step 5942, loss 0.00592131, acc 1
2016-09-06T05:39:14.326714: step 5943, loss 0.00306734, acc 1
2016-09-06T05:39:15.146856: step 5944, loss 0.00857676, acc 1
2016-09-06T05:39:16.012337: step 5945, loss 0.0286408, acc 1
2016-09-06T05:39:16.853183: step 5946, loss 0.0189844, acc 0.98
2016-09-06T05:39:17.637397: step 5947, loss 0.0132755, acc 1
2016-09-06T05:39:18.438822: step 5948, loss 0.00409583, acc 1
2016-09-06T05:39:19.253607: step 5949, loss 0.0172919, acc 1
2016-09-06T05:39:20.061396: step 5950, loss 0.0260971, acc 1
2016-09-06T05:39:20.860682: step 5951, loss 0.0341247, acc 0.98
2016-09-06T05:39:21.636253: step 5952, loss 0.00912237, acc 1
2016-09-06T05:39:22.447133: step 5953, loss 0.019826, acc 1
2016-09-06T05:39:23.309557: step 5954, loss 0.0450357, acc 0.98
2016-09-06T05:39:24.157683: step 5955, loss 0.00521474, acc 1
2016-09-06T05:39:24.941569: step 5956, loss 0.00324703, acc 1
2016-09-06T05:39:25.724002: step 5957, loss 0.0075075, acc 1
2016-09-06T05:39:26.561823: step 5958, loss 0.020797, acc 0.98
2016-09-06T05:39:27.343635: step 5959, loss 0.0137329, acc 1
2016-09-06T05:39:28.145131: step 5960, loss 0.0185928, acc 0.98
2016-09-06T05:39:28.952886: step 5961, loss 0.0590702, acc 0.96
2016-09-06T05:39:29.746389: step 5962, loss 0.00565633, acc 1
2016-09-06T05:39:30.550398: step 5963, loss 0.11142, acc 0.94
2016-09-06T05:39:31.365342: step 5964, loss 0.00606493, acc 1
2016-09-06T05:39:32.154607: step 5965, loss 0.0442588, acc 0.98
2016-09-06T05:39:32.962111: step 5966, loss 0.0504485, acc 0.98
2016-09-06T05:39:33.777909: step 5967, loss 0.0345305, acc 0.98
2016-09-06T05:39:34.574833: step 5968, loss 0.00374588, acc 1
2016-09-06T05:39:35.378507: step 5969, loss 0.161358, acc 0.94
2016-09-06T05:39:36.189248: step 5970, loss 0.0147686, acc 1
2016-09-06T05:39:36.986621: step 5971, loss 0.00790088, acc 1
2016-09-06T05:39:37.782339: step 5972, loss 0.0133563, acc 1
2016-09-06T05:39:38.625291: step 5973, loss 0.00365542, acc 1
2016-09-06T05:39:39.412890: step 5974, loss 0.00597948, acc 1
2016-09-06T05:39:40.221385: step 5975, loss 0.0303501, acc 0.98
2016-09-06T05:39:41.057449: step 5976, loss 0.00404293, acc 1
2016-09-06T05:39:41.849665: step 5977, loss 0.00947048, acc 1
2016-09-06T05:39:42.727091: step 5978, loss 0.0186562, acc 0.98
2016-09-06T05:39:43.532919: step 5979, loss 0.0046961, acc 1
2016-09-06T05:39:44.313918: step 5980, loss 0.00697419, acc 1
2016-09-06T05:39:45.133677: step 5981, loss 0.00707357, acc 1
2016-09-06T05:39:45.949535: step 5982, loss 0.0071234, acc 1
2016-09-06T05:39:46.739244: step 5983, loss 0.0302022, acc 0.98
2016-09-06T05:39:47.534271: step 5984, loss 0.0593797, acc 0.98
2016-09-06T05:39:48.378520: step 5985, loss 0.0690191, acc 0.96
2016-09-06T05:39:49.181976: step 5986, loss 0.0208666, acc 1
2016-09-06T05:39:49.995788: step 5987, loss 0.0307204, acc 0.98
2016-09-06T05:39:50.821312: step 5988, loss 0.0216303, acc 0.98
2016-09-06T05:39:51.632541: step 5989, loss 0.0043305, acc 1
2016-09-06T05:39:52.412211: step 5990, loss 0.0442819, acc 0.98
2016-09-06T05:39:53.254069: step 5991, loss 0.066878, acc 0.98
2016-09-06T05:39:54.053458: step 5992, loss 0.00463251, acc 1
2016-09-06T05:39:54.887020: step 5993, loss 0.0719008, acc 0.96
2016-09-06T05:39:55.698672: step 5994, loss 0.00411851, acc 1
2016-09-06T05:39:56.496641: step 5995, loss 0.0371295, acc 0.98
2016-09-06T05:39:57.298060: step 5996, loss 0.00948583, acc 1
2016-09-06T05:39:58.123944: step 5997, loss 0.011927, acc 1
2016-09-06T05:39:58.940247: step 5998, loss 0.0912672, acc 0.98
2016-09-06T05:39:59.752963: step 5999, loss 0.0237723, acc 0.98
2016-09-06T05:40:00.621218: step 6000, loss 0.00474953, acc 1

Evaluation:
2016-09-06T05:40:04.359176: step 6000, loss 2.42028, acc 0.73546

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-6000

2016-09-06T05:40:06.337886: step 6001, loss 0.0121524, acc 1
2016-09-06T05:40:07.149786: step 6002, loss 0.00722511, acc 1
2016-09-06T05:40:07.994842: step 6003, loss 0.00406206, acc 1
2016-09-06T05:40:08.797578: step 6004, loss 0.138581, acc 0.96
2016-09-06T05:40:09.607808: step 6005, loss 0.0428341, acc 1
2016-09-06T05:40:10.433201: step 6006, loss 0.0143203, acc 1
2016-09-06T05:40:11.256806: step 6007, loss 0.0309842, acc 0.98
2016-09-06T05:40:12.051555: step 6008, loss 0.0220687, acc 0.98
2016-09-06T05:40:12.869704: step 6009, loss 0.0124718, acc 1
2016-09-06T05:40:13.723457: step 6010, loss 0.0217719, acc 1
2016-09-06T05:40:14.505986: step 6011, loss 0.00653531, acc 1
2016-09-06T05:40:15.311750: step 6012, loss 0.00328537, acc 1
2016-09-06T05:40:16.138307: step 6013, loss 0.0320462, acc 0.98
2016-09-06T05:40:16.918328: step 6014, loss 0.0376892, acc 0.98
2016-09-06T05:40:17.699279: step 6015, loss 0.0142311, acc 1
2016-09-06T05:40:18.527886: step 6016, loss 0.0378578, acc 1
2016-09-06T05:40:19.341430: step 6017, loss 0.00920902, acc 1
2016-09-06T05:40:20.171220: step 6018, loss 0.0294635, acc 0.98
2016-09-06T05:40:20.981294: step 6019, loss 0.0145981, acc 1
2016-09-06T05:40:21.757996: step 6020, loss 0.0731132, acc 0.98
2016-09-06T05:40:22.567788: step 6021, loss 0.036437, acc 1
2016-09-06T05:40:23.388468: step 6022, loss 0.01365, acc 1
2016-09-06T05:40:24.174425: step 6023, loss 0.00935666, acc 1
2016-09-06T05:40:24.971364: step 6024, loss 0.107314, acc 0.94
2016-09-06T05:40:25.763739: step 6025, loss 0.039615, acc 0.98
2016-09-06T05:40:26.567496: step 6026, loss 0.0485263, acc 0.98
2016-09-06T05:40:27.401441: step 6027, loss 0.0165175, acc 1
2016-09-06T05:40:28.241481: step 6028, loss 0.0711786, acc 0.96
2016-09-06T05:40:29.045121: step 6029, loss 0.00369005, acc 1
2016-09-06T05:40:29.861229: step 6030, loss 0.0196831, acc 1
2016-09-06T05:40:30.717453: step 6031, loss 0.0131159, acc 1
2016-09-06T05:40:31.542317: step 6032, loss 0.0196914, acc 0.98
2016-09-06T05:40:32.313659: step 6033, loss 0.00454438, acc 1
2016-09-06T05:40:33.166208: step 6034, loss 0.019961, acc 0.98
2016-09-06T05:40:33.966890: step 6035, loss 0.102788, acc 0.94
2016-09-06T05:40:34.771121: step 6036, loss 0.00683636, acc 1
2016-09-06T05:40:35.602597: step 6037, loss 0.121447, acc 0.98
2016-09-06T05:40:36.412013: step 6038, loss 0.0307647, acc 0.98
2016-09-06T05:40:37.219897: step 6039, loss 0.00374249, acc 1
2016-09-06T05:40:38.052476: step 6040, loss 0.0258215, acc 1
2016-09-06T05:40:38.881450: step 6041, loss 0.0922505, acc 0.96
2016-09-06T05:40:39.726830: step 6042, loss 0.0117602, acc 1
2016-09-06T05:40:40.589346: step 6043, loss 0.0186773, acc 1
2016-09-06T05:40:41.460316: step 6044, loss 0.00398839, acc 1
2016-09-06T05:40:42.234739: step 6045, loss 0.036374, acc 0.98
2016-09-06T05:40:43.044536: step 6046, loss 0.0230589, acc 0.98
2016-09-06T05:40:43.852918: step 6047, loss 0.025212, acc 0.98
2016-09-06T05:40:44.682254: step 6048, loss 0.095593, acc 0.98
2016-09-06T05:40:45.490200: step 6049, loss 0.015427, acc 1
2016-09-06T05:40:46.314957: step 6050, loss 0.0241042, acc 1
2016-09-06T05:40:47.124549: step 6051, loss 0.00632165, acc 1
2016-09-06T05:40:47.966534: step 6052, loss 0.0228962, acc 1
2016-09-06T05:40:48.746148: step 6053, loss 0.0150451, acc 1
2016-09-06T05:40:49.548919: step 6054, loss 0.00566762, acc 1
2016-09-06T05:40:50.371906: step 6055, loss 0.0356408, acc 0.98
2016-09-06T05:40:51.194012: step 6056, loss 0.00812673, acc 1
2016-09-06T05:40:51.964639: step 6057, loss 0.0431566, acc 0.98
2016-09-06T05:40:52.768670: step 6058, loss 0.0143266, acc 1
2016-09-06T05:40:53.605534: step 6059, loss 0.0189926, acc 1
2016-09-06T05:40:54.411915: step 6060, loss 0.0117552, acc 1
2016-09-06T05:40:55.205753: step 6061, loss 0.0359955, acc 1
2016-09-06T05:40:56.039880: step 6062, loss 0.0349783, acc 0.98
2016-09-06T05:40:56.831926: step 6063, loss 0.0584409, acc 0.96
2016-09-06T05:40:57.613075: step 6064, loss 0.0611484, acc 0.96
2016-09-06T05:40:58.423495: step 6065, loss 0.0296253, acc 1
2016-09-06T05:40:59.214507: step 6066, loss 0.0325616, acc 0.98
2016-09-06T05:41:00.033470: step 6067, loss 0.105053, acc 0.96
2016-09-06T05:41:00.893275: step 6068, loss 0.0160749, acc 1
2016-09-06T05:41:01.696191: step 6069, loss 0.0723162, acc 0.98
2016-09-06T05:41:02.500287: step 6070, loss 0.00896608, acc 1
2016-09-06T05:41:03.332970: step 6071, loss 0.00873878, acc 1
2016-09-06T05:41:04.168650: step 6072, loss 0.0559922, acc 0.98
2016-09-06T05:41:04.952272: step 6073, loss 0.0847564, acc 0.98
2016-09-06T05:41:05.756534: step 6074, loss 0.00462508, acc 1
2016-09-06T05:41:06.566521: step 6075, loss 0.0240496, acc 0.98
2016-09-06T05:41:07.401408: step 6076, loss 0.0202232, acc 1
2016-09-06T05:41:08.233552: step 6077, loss 0.0448435, acc 0.96
2016-09-06T05:41:09.040518: step 6078, loss 0.0336001, acc 0.98
2016-09-06T05:41:09.864537: step 6079, loss 0.0285779, acc 0.98
2016-09-06T05:41:10.726365: step 6080, loss 0.0535862, acc 0.98
2016-09-06T05:41:11.549091: step 6081, loss 0.0522842, acc 0.96
2016-09-06T05:41:12.323101: step 6082, loss 0.0280128, acc 1
2016-09-06T05:41:13.152948: step 6083, loss 0.0356997, acc 0.96
2016-09-06T05:41:13.965876: step 6084, loss 0.0256398, acc 1
2016-09-06T05:41:14.783624: step 6085, loss 0.0292611, acc 1
2016-09-06T05:41:15.618502: step 6086, loss 0.0285747, acc 0.98
2016-09-06T05:41:16.442603: step 6087, loss 0.0327592, acc 0.98
2016-09-06T05:41:17.241533: step 6088, loss 0.00770695, acc 1
2016-09-06T05:41:18.087649: step 6089, loss 0.0200095, acc 0.98
2016-09-06T05:41:18.898347: step 6090, loss 0.00588739, acc 1
2016-09-06T05:41:19.698154: step 6091, loss 0.035516, acc 0.98
2016-09-06T05:41:20.523505: step 6092, loss 0.00749466, acc 1
2016-09-06T05:41:21.343215: step 6093, loss 0.00856483, acc 1
2016-09-06T05:41:22.146076: step 6094, loss 0.052581, acc 0.98
2016-09-06T05:41:22.958013: step 6095, loss 0.141704, acc 0.96
2016-09-06T05:41:23.774857: step 6096, loss 0.0314842, acc 0.98
2016-09-06T05:41:24.563099: step 6097, loss 0.0416631, acc 0.98
2016-09-06T05:41:25.365908: step 6098, loss 0.056715, acc 0.96
2016-09-06T05:41:26.186863: step 6099, loss 0.00686039, acc 1
2016-09-06T05:41:26.967892: step 6100, loss 0.00367818, acc 1

Evaluation:
2016-09-06T05:41:30.675583: step 6100, loss 2.03834, acc 0.749531

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-6100

2016-09-06T05:41:32.580357: step 6101, loss 0.0519799, acc 0.98
2016-09-06T05:41:33.429547: step 6102, loss 0.0286464, acc 0.98
2016-09-06T05:41:34.244791: step 6103, loss 0.0408273, acc 0.98
2016-09-06T05:41:35.079796: step 6104, loss 0.0914459, acc 0.98
2016-09-06T05:41:35.906362: step 6105, loss 0.0200996, acc 1
2016-09-06T05:41:36.723847: step 6106, loss 0.00563133, acc 1
2016-09-06T05:41:37.536928: step 6107, loss 0.0585582, acc 0.94
2016-09-06T05:41:38.363056: step 6108, loss 0.0204922, acc 1
2016-09-06T05:41:39.143242: step 6109, loss 0.0445671, acc 0.98
2016-09-06T05:41:39.935875: step 6110, loss 0.0240304, acc 0.98
2016-09-06T05:41:40.762111: step 6111, loss 0.0807979, acc 0.98
2016-09-06T05:41:41.543674: step 6112, loss 0.0674862, acc 0.94
2016-09-06T05:41:42.340727: step 6113, loss 0.0181262, acc 1
2016-09-06T05:41:43.166382: step 6114, loss 0.0152478, acc 1
2016-09-06T05:41:43.949648: step 6115, loss 0.0387964, acc 0.96
2016-09-06T05:41:44.740187: step 6116, loss 0.00907793, acc 1
2016-09-06T05:41:45.557249: step 6117, loss 0.00341918, acc 1
2016-09-06T05:41:46.347690: step 6118, loss 0.026199, acc 0.98
2016-09-06T05:41:47.163841: step 6119, loss 0.0221708, acc 0.98
2016-09-06T05:41:47.984100: step 6120, loss 0.01538, acc 1
2016-09-06T05:41:48.793444: step 6121, loss 0.112748, acc 0.94
2016-09-06T05:41:49.614976: step 6122, loss 0.02823, acc 0.98
2016-09-06T05:41:50.462407: step 6123, loss 0.0781699, acc 0.98
2016-09-06T05:41:51.269778: step 6124, loss 0.0229804, acc 0.98
2016-09-06T05:41:52.075893: step 6125, loss 0.0785266, acc 0.96
2016-09-06T05:41:52.901641: step 6126, loss 0.0213751, acc 0.98
2016-09-06T05:41:53.742273: step 6127, loss 0.0418422, acc 0.98
2016-09-06T05:41:54.582481: step 6128, loss 0.0232682, acc 1
2016-09-06T05:41:55.421644: step 6129, loss 0.0878769, acc 0.96
2016-09-06T05:41:56.218588: step 6130, loss 0.0755734, acc 0.94
2016-09-06T05:41:57.032650: step 6131, loss 0.0857204, acc 0.98
2016-09-06T05:41:57.856123: step 6132, loss 0.0654265, acc 0.94
2016-09-06T05:41:58.675874: step 6133, loss 0.0290089, acc 1
2016-09-06T05:41:59.474669: step 6134, loss 0.0276123, acc 0.98
2016-09-06T05:42:00.339561: step 6135, loss 0.0102406, acc 1
2016-09-06T05:42:01.131026: step 6136, loss 0.00920372, acc 1
2016-09-06T05:42:01.950315: step 6137, loss 0.0227602, acc 1
2016-09-06T05:42:02.790088: step 6138, loss 0.0275192, acc 0.98
2016-09-06T05:42:03.617006: step 6139, loss 0.066329, acc 0.98
2016-09-06T05:42:04.445810: step 6140, loss 0.00591254, acc 1
2016-09-06T05:42:05.275552: step 6141, loss 0.0266634, acc 1
2016-09-06T05:42:06.102025: step 6142, loss 0.0199392, acc 0.98
2016-09-06T05:42:06.902522: step 6143, loss 0.0277095, acc 0.98
2016-09-06T05:42:07.667911: step 6144, loss 0.0182308, acc 1
2016-09-06T05:42:08.470713: step 6145, loss 0.0125833, acc 1
2016-09-06T05:42:09.281193: step 6146, loss 0.0280621, acc 1
2016-09-06T05:42:10.094376: step 6147, loss 0.0195779, acc 0.98
2016-09-06T05:42:10.900428: step 6148, loss 0.0372532, acc 0.98
2016-09-06T05:42:11.700061: step 6149, loss 0.0180533, acc 1
2016-09-06T05:42:12.515196: step 6150, loss 0.0300702, acc 0.98
2016-09-06T05:42:13.325700: step 6151, loss 0.0208328, acc 0.98
2016-09-06T05:42:14.152782: step 6152, loss 0.00302628, acc 1
2016-09-06T05:42:14.970509: step 6153, loss 0.0298833, acc 1
2016-09-06T05:42:15.805463: step 6154, loss 0.0332383, acc 0.96
2016-09-06T05:42:16.594740: step 6155, loss 0.0292117, acc 0.98
2016-09-06T05:42:17.407890: step 6156, loss 0.00704597, acc 1
2016-09-06T05:42:18.201182: step 6157, loss 0.0246265, acc 1
2016-09-06T05:42:18.996617: step 6158, loss 0.0314765, acc 0.98
2016-09-06T05:42:19.815805: step 6159, loss 0.143256, acc 0.96
2016-09-06T05:42:20.645917: step 6160, loss 0.0140835, acc 1
2016-09-06T05:42:21.451520: step 6161, loss 0.0366189, acc 0.98
2016-09-06T05:42:22.252278: step 6162, loss 0.0146946, acc 1
2016-09-06T05:42:23.072321: step 6163, loss 0.0374372, acc 0.96
2016-09-06T05:42:23.860814: step 6164, loss 0.00580844, acc 1
2016-09-06T05:42:24.639911: step 6165, loss 0.00301022, acc 1
2016-09-06T05:42:25.461418: step 6166, loss 0.0831457, acc 0.98
2016-09-06T05:42:26.239693: step 6167, loss 0.048805, acc 0.96
2016-09-06T05:42:27.045962: step 6168, loss 0.0136248, acc 1
2016-09-06T05:42:27.865852: step 6169, loss 0.0244787, acc 0.98
2016-09-06T05:42:28.645968: step 6170, loss 0.0361566, acc 0.98
2016-09-06T05:42:29.462052: step 6171, loss 0.113552, acc 0.96
2016-09-06T05:42:30.270386: step 6172, loss 0.0186807, acc 1
2016-09-06T05:42:31.064554: step 6173, loss 0.0216768, acc 1
2016-09-06T05:42:31.855483: step 6174, loss 0.0201194, acc 1
2016-09-06T05:42:32.665382: step 6175, loss 0.0598, acc 0.98
2016-09-06T05:42:33.481279: step 6176, loss 0.0264357, acc 0.98
2016-09-06T05:42:34.287267: step 6177, loss 0.0353387, acc 1
2016-09-06T05:42:35.094528: step 6178, loss 0.0466324, acc 0.98
2016-09-06T05:42:35.888265: step 6179, loss 0.0438193, acc 0.98
2016-09-06T05:42:36.766187: step 6180, loss 0.0501887, acc 0.98
2016-09-06T05:42:37.567310: step 6181, loss 0.0106932, acc 1
2016-09-06T05:42:38.334349: step 6182, loss 0.00997408, acc 1
2016-09-06T05:42:39.122658: step 6183, loss 0.0289107, acc 0.98
2016-09-06T05:42:39.937036: step 6184, loss 0.00343471, acc 1
2016-09-06T05:42:40.735324: step 6185, loss 0.0318917, acc 0.98
2016-09-06T05:42:41.535750: step 6186, loss 0.0115391, acc 1
2016-09-06T05:42:42.337529: step 6187, loss 0.0503807, acc 0.98
2016-09-06T05:42:43.129033: step 6188, loss 0.0125543, acc 1
2016-09-06T05:42:43.945493: step 6189, loss 0.0275208, acc 0.98
2016-09-06T05:42:44.745789: step 6190, loss 0.0220229, acc 1
2016-09-06T05:42:45.549098: step 6191, loss 0.0175717, acc 1
2016-09-06T05:42:46.352719: step 6192, loss 0.0889449, acc 0.96
2016-09-06T05:42:47.199050: step 6193, loss 0.0698604, acc 0.96
2016-09-06T05:42:47.997621: step 6194, loss 0.00397096, acc 1
2016-09-06T05:42:48.794903: step 6195, loss 0.0658129, acc 0.94
2016-09-06T05:42:49.602477: step 6196, loss 0.00905239, acc 1
2016-09-06T05:42:50.406267: step 6197, loss 0.0523509, acc 0.94
2016-09-06T05:42:51.216306: step 6198, loss 0.019271, acc 1
2016-09-06T05:42:52.031838: step 6199, loss 0.0435783, acc 0.98
2016-09-06T05:42:52.840791: step 6200, loss 0.00475617, acc 1

Evaluation:
2016-09-06T05:42:56.577200: step 6200, loss 2.20073, acc 0.748593

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-6200

2016-09-06T05:42:58.456827: step 6201, loss 0.0456246, acc 0.98
2016-09-06T05:42:59.263181: step 6202, loss 0.0377016, acc 0.98
2016-09-06T05:43:00.058692: step 6203, loss 0.0033091, acc 1
2016-09-06T05:43:00.906060: step 6204, loss 0.0872149, acc 0.96
2016-09-06T05:43:01.709601: step 6205, loss 0.0518759, acc 1
2016-09-06T05:43:02.545612: step 6206, loss 0.0129823, acc 1
2016-09-06T05:43:03.340829: step 6207, loss 0.0042519, acc 1
2016-09-06T05:43:04.180168: step 6208, loss 0.0769302, acc 0.96
2016-09-06T05:43:04.947874: step 6209, loss 0.0156295, acc 1
2016-09-06T05:43:05.762392: step 6210, loss 0.00526329, acc 1
2016-09-06T05:43:06.573460: step 6211, loss 0.0329521, acc 0.98
2016-09-06T05:43:07.370676: step 6212, loss 0.0370269, acc 0.98
2016-09-06T05:43:08.197674: step 6213, loss 0.0229857, acc 1
2016-09-06T05:43:09.004182: step 6214, loss 0.0323026, acc 0.98
2016-09-06T05:43:09.822732: step 6215, loss 0.0206232, acc 1
2016-09-06T05:43:10.635902: step 6216, loss 0.023089, acc 0.98
2016-09-06T05:43:11.464233: step 6217, loss 0.0770592, acc 0.98
2016-09-06T05:43:12.234198: step 6218, loss 0.00685835, acc 1
2016-09-06T05:43:13.010622: step 6219, loss 0.0347999, acc 0.98
2016-09-06T05:43:13.853069: step 6220, loss 0.0657337, acc 0.96
2016-09-06T05:43:14.686083: step 6221, loss 0.0518224, acc 0.96
2016-09-06T05:43:15.501778: step 6222, loss 0.0246255, acc 1
2016-09-06T05:43:16.337133: step 6223, loss 0.00819155, acc 1
2016-09-06T05:43:17.179628: step 6224, loss 0.0432778, acc 0.98
2016-09-06T05:43:17.982720: step 6225, loss 0.00965308, acc 1
2016-09-06T05:43:18.799467: step 6226, loss 0.0159136, acc 1
2016-09-06T05:43:19.629045: step 6227, loss 0.0404862, acc 0.98
2016-09-06T05:43:20.453061: step 6228, loss 0.0100786, acc 1
2016-09-06T05:43:21.305461: step 6229, loss 0.0470318, acc 0.98
2016-09-06T05:43:22.134749: step 6230, loss 0.0434484, acc 0.98
2016-09-06T05:43:22.937277: step 6231, loss 0.0152836, acc 1
2016-09-06T05:43:23.751616: step 6232, loss 0.0320136, acc 0.98
2016-09-06T05:43:24.560351: step 6233, loss 0.002602, acc 1
2016-09-06T05:43:25.374838: step 6234, loss 0.00621949, acc 1
2016-09-06T05:43:26.237199: step 6235, loss 0.0433187, acc 0.98
2016-09-06T05:43:27.026450: step 6236, loss 0.00304698, acc 1
2016-09-06T05:43:27.844868: step 6237, loss 0.0282362, acc 0.98
2016-09-06T05:43:28.674246: step 6238, loss 0.00326193, acc 1
2016-09-06T05:43:29.477628: step 6239, loss 0.00914362, acc 1
2016-09-06T05:43:30.273385: step 6240, loss 0.00813715, acc 1
2016-09-06T05:43:31.113646: step 6241, loss 0.00349413, acc 1
2016-09-06T05:43:31.928294: step 6242, loss 0.0268334, acc 1
2016-09-06T05:43:32.756116: step 6243, loss 0.0360227, acc 0.98
2016-09-06T05:43:33.555042: step 6244, loss 0.0312988, acc 0.98
2016-09-06T05:43:34.395354: step 6245, loss 0.010149, acc 1
2016-09-06T05:43:35.224820: step 6246, loss 0.0201638, acc 0.98
2016-09-06T05:43:36.009404: step 6247, loss 0.00688939, acc 1
2016-09-06T05:43:36.830426: step 6248, loss 0.00392426, acc 1
2016-09-06T05:43:37.584001: step 6249, loss 0.0835296, acc 0.94
2016-09-06T05:43:38.407327: step 6250, loss 0.0200292, acc 0.98
2016-09-06T05:43:39.254486: step 6251, loss 0.0113868, acc 1
2016-09-06T05:43:40.041398: step 6252, loss 0.0752749, acc 0.96
2016-09-06T05:43:40.864358: step 6253, loss 0.0298815, acc 1
2016-09-06T05:43:41.686387: step 6254, loss 0.0451077, acc 0.96
2016-09-06T05:43:42.474933: step 6255, loss 0.00613389, acc 1
2016-09-06T05:43:43.267638: step 6256, loss 0.0116607, acc 1
2016-09-06T05:43:44.069358: step 6257, loss 0.0286899, acc 0.98
2016-09-06T05:43:44.848261: step 6258, loss 0.00334105, acc 1
2016-09-06T05:43:45.720572: step 6259, loss 0.0466193, acc 0.98
2016-09-06T05:43:46.553028: step 6260, loss 0.00945906, acc 1
2016-09-06T05:43:47.356653: step 6261, loss 0.00610975, acc 1
2016-09-06T05:43:48.180150: step 6262, loss 0.0154006, acc 1
2016-09-06T05:43:49.017353: step 6263, loss 0.0141349, acc 1
2016-09-06T05:43:49.813467: step 6264, loss 0.0649787, acc 0.96
2016-09-06T05:43:50.598949: step 6265, loss 0.00448924, acc 1
2016-09-06T05:43:51.472854: step 6266, loss 0.00383396, acc 1
2016-09-06T05:43:52.299714: step 6267, loss 0.0323621, acc 1
2016-09-06T05:43:53.124511: step 6268, loss 0.0677093, acc 0.98
2016-09-06T05:43:53.974170: step 6269, loss 0.00889498, acc 1
2016-09-06T05:43:54.813354: step 6270, loss 0.00976538, acc 1
2016-09-06T05:43:55.617596: step 6271, loss 0.0116021, acc 1
2016-09-06T05:43:56.457705: step 6272, loss 0.0275447, acc 0.98
2016-09-06T05:43:57.268929: step 6273, loss 0.0104208, acc 1
2016-09-06T05:43:58.083743: step 6274, loss 0.0366777, acc 0.98
2016-09-06T05:43:58.899472: step 6275, loss 0.030153, acc 0.98
2016-09-06T05:43:59.731945: step 6276, loss 0.0207897, acc 1
2016-09-06T05:44:00.568929: step 6277, loss 0.166851, acc 0.96
2016-09-06T05:44:01.406581: step 6278, loss 0.0320885, acc 1
2016-09-06T05:44:02.223726: step 6279, loss 0.0300353, acc 1
2016-09-06T05:44:03.016379: step 6280, loss 0.0249011, acc 1
2016-09-06T05:44:03.825917: step 6281, loss 0.0163765, acc 1
2016-09-06T05:44:04.639104: step 6282, loss 0.0284145, acc 1
2016-09-06T05:44:05.431329: step 6283, loss 0.0186407, acc 1
2016-09-06T05:44:06.234036: step 6284, loss 0.00458433, acc 1
2016-09-06T05:44:07.035756: step 6285, loss 0.0176728, acc 1
2016-09-06T05:44:07.798437: step 6286, loss 0.0119423, acc 1
2016-09-06T05:44:08.590758: step 6287, loss 0.0165958, acc 1
2016-09-06T05:44:09.423772: step 6288, loss 0.0319547, acc 0.98
2016-09-06T05:44:10.212477: step 6289, loss 0.0356705, acc 0.98
2016-09-06T05:44:11.026055: step 6290, loss 0.00709641, acc 1
2016-09-06T05:44:11.840039: step 6291, loss 0.0224507, acc 0.98
2016-09-06T05:44:12.639586: step 6292, loss 0.0531446, acc 0.98
2016-09-06T05:44:13.435994: step 6293, loss 0.0219835, acc 1
2016-09-06T05:44:14.240157: step 6294, loss 0.0345882, acc 0.96
2016-09-06T05:44:15.034450: step 6295, loss 0.00557123, acc 1
2016-09-06T05:44:15.850337: step 6296, loss 0.0103925, acc 1
2016-09-06T05:44:16.663657: step 6297, loss 0.13856, acc 0.96
2016-09-06T05:44:17.490050: step 6298, loss 0.00457566, acc 1
2016-09-06T05:44:18.332150: step 6299, loss 0.080242, acc 0.96
2016-09-06T05:44:19.162424: step 6300, loss 0.0390826, acc 1

Evaluation:
2016-09-06T05:44:22.913162: step 6300, loss 2.07971, acc 0.733584

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-6300

2016-09-06T05:44:24.785494: step 6301, loss 0.0133684, acc 1
2016-09-06T05:44:25.594159: step 6302, loss 0.0651802, acc 0.98
2016-09-06T05:44:26.443204: step 6303, loss 0.0043973, acc 1
2016-09-06T05:44:27.232576: step 6304, loss 0.0520888, acc 0.96
2016-09-06T05:44:28.043164: step 6305, loss 0.0302005, acc 1
2016-09-06T05:44:28.870633: step 6306, loss 0.0119187, acc 1
2016-09-06T05:44:29.676582: step 6307, loss 0.190581, acc 0.96
2016-09-06T05:44:30.521510: step 6308, loss 0.0725564, acc 0.98
2016-09-06T05:44:31.359520: step 6309, loss 0.0182434, acc 1
2016-09-06T05:44:32.155476: step 6310, loss 0.0185574, acc 1
2016-09-06T05:44:32.984991: step 6311, loss 0.0326062, acc 0.98
2016-09-06T05:44:33.806858: step 6312, loss 0.0168918, acc 1
2016-09-06T05:44:34.612532: step 6313, loss 0.0532984, acc 0.98
2016-09-06T05:44:35.426828: step 6314, loss 0.0454393, acc 0.98
2016-09-06T05:44:36.245815: step 6315, loss 0.0167959, acc 1
2016-09-06T05:44:37.047198: step 6316, loss 0.04466, acc 0.98
2016-09-06T05:44:37.901081: step 6317, loss 0.00520561, acc 1
2016-09-06T05:44:38.711785: step 6318, loss 0.00416917, acc 1
2016-09-06T05:44:39.518381: step 6319, loss 0.0239034, acc 1
2016-09-06T05:44:40.342677: step 6320, loss 0.048145, acc 0.98
2016-09-06T05:44:41.169223: step 6321, loss 0.0386848, acc 0.96
2016-09-06T05:44:41.976719: step 6322, loss 0.00556589, acc 1
2016-09-06T05:44:42.800520: step 6323, loss 0.0439556, acc 0.98
2016-09-06T05:44:43.663560: step 6324, loss 0.10344, acc 0.94
2016-09-06T05:44:44.582506: step 6325, loss 0.017613, acc 1
2016-09-06T05:44:45.396374: step 6326, loss 0.0421867, acc 0.98
2016-09-06T05:44:46.244016: step 6327, loss 0.017481, acc 1
2016-09-06T05:44:47.066753: step 6328, loss 0.00636642, acc 1
2016-09-06T05:44:47.889565: step 6329, loss 0.0171681, acc 1
2016-09-06T05:44:48.706534: step 6330, loss 0.218378, acc 0.94
2016-09-06T05:44:49.565814: step 6331, loss 0.027631, acc 0.98
2016-09-06T05:44:50.370320: step 6332, loss 0.00392641, acc 1
2016-09-06T05:44:51.183371: step 6333, loss 0.0423282, acc 0.96
2016-09-06T05:44:52.006307: step 6334, loss 0.00409584, acc 1
2016-09-06T05:44:52.829536: step 6335, loss 0.0068018, acc 1
2016-09-06T05:44:53.587089: step 6336, loss 0.00490151, acc 1
2016-09-06T05:44:54.435920: step 6337, loss 0.0383883, acc 0.98
2016-09-06T05:44:55.267790: step 6338, loss 0.0256256, acc 1
2016-09-06T05:44:56.122288: step 6339, loss 0.051195, acc 0.98
2016-09-06T05:44:56.949053: step 6340, loss 0.111836, acc 0.96
2016-09-06T05:44:57.761872: step 6341, loss 0.379613, acc 0.94
2016-09-06T05:44:58.550296: step 6342, loss 0.0267055, acc 0.98
2016-09-06T05:44:59.367874: step 6343, loss 0.0376764, acc 0.98
2016-09-06T05:45:00.177566: step 6344, loss 0.0244208, acc 1
2016-09-06T05:45:01.032545: step 6345, loss 0.12159, acc 0.96
2016-09-06T05:45:01.818222: step 6346, loss 0.0772754, acc 0.96
2016-09-06T05:45:02.646280: step 6347, loss 0.0178254, acc 1
2016-09-06T05:45:03.432111: step 6348, loss 0.0165704, acc 1
2016-09-06T05:45:04.236520: step 6349, loss 0.0589922, acc 0.94
2016-09-06T05:45:05.067687: step 6350, loss 0.126247, acc 0.96
2016-09-06T05:45:05.826586: step 6351, loss 0.0617631, acc 0.98
2016-09-06T05:45:06.651426: step 6352, loss 0.0246942, acc 1
2016-09-06T05:45:07.477396: step 6353, loss 0.00566176, acc 1
2016-09-06T05:45:08.270248: step 6354, loss 0.00390951, acc 1
2016-09-06T05:45:09.067869: step 6355, loss 0.0887675, acc 0.98
2016-09-06T05:45:09.917837: step 6356, loss 0.00450676, acc 1
2016-09-06T05:45:10.708995: step 6357, loss 0.00766225, acc 1
2016-09-06T05:45:11.515952: step 6358, loss 0.0108621, acc 1
2016-09-06T05:45:12.317499: step 6359, loss 0.0442797, acc 0.98
2016-09-06T05:45:13.100258: step 6360, loss 0.0187641, acc 1
2016-09-06T05:45:13.906842: step 6361, loss 0.0481319, acc 0.98
2016-09-06T05:45:14.745256: step 6362, loss 0.0714068, acc 0.96
2016-09-06T05:45:15.550881: step 6363, loss 0.0402252, acc 0.96
2016-09-06T05:45:16.358610: step 6364, loss 0.028521, acc 1
2016-09-06T05:45:17.183048: step 6365, loss 0.100177, acc 0.96
2016-09-06T05:45:17.984475: step 6366, loss 0.0180411, acc 1
2016-09-06T05:45:18.798667: step 6367, loss 0.0132381, acc 1
2016-09-06T05:45:19.609073: step 6368, loss 0.00536158, acc 1
2016-09-06T05:45:20.398399: step 6369, loss 0.0597025, acc 0.96
2016-09-06T05:45:21.201127: step 6370, loss 0.0326074, acc 0.98
2016-09-06T05:45:22.018904: step 6371, loss 0.0577147, acc 0.98
2016-09-06T05:45:22.817994: step 6372, loss 0.016856, acc 1
2016-09-06T05:45:23.633029: step 6373, loss 0.0252659, acc 1
2016-09-06T05:45:24.439280: step 6374, loss 0.0308221, acc 1
2016-09-06T05:45:25.249028: step 6375, loss 0.00469521, acc 1
2016-09-06T05:45:26.051821: step 6376, loss 0.010866, acc 1
2016-09-06T05:45:26.871315: step 6377, loss 0.0335447, acc 0.98
2016-09-06T05:45:27.667215: step 6378, loss 0.0153307, acc 1
2016-09-06T05:45:28.470744: step 6379, loss 0.0513215, acc 1
2016-09-06T05:45:29.311295: step 6380, loss 0.00455913, acc 1
2016-09-06T05:45:30.145519: step 6381, loss 0.00414961, acc 1
2016-09-06T05:45:30.979142: step 6382, loss 0.0352266, acc 0.98
2016-09-06T05:45:31.831773: step 6383, loss 0.0210818, acc 1
2016-09-06T05:45:32.675138: step 6384, loss 0.049968, acc 0.98
2016-09-06T05:45:33.443987: step 6385, loss 0.0310754, acc 0.98
2016-09-06T05:45:34.280502: step 6386, loss 0.0134831, acc 1
2016-09-06T05:45:35.085775: step 6387, loss 0.0393522, acc 1
2016-09-06T05:45:35.909951: step 6388, loss 0.0414513, acc 1
2016-09-06T05:45:36.746390: step 6389, loss 0.0324174, acc 0.98
2016-09-06T05:45:37.574063: step 6390, loss 0.0580266, acc 0.98
2016-09-06T05:45:38.393936: step 6391, loss 0.0400507, acc 0.98
2016-09-06T05:45:39.223490: step 6392, loss 0.018556, acc 1
2016-09-06T05:45:40.048277: step 6393, loss 0.0193067, acc 1
2016-09-06T05:45:40.835381: step 6394, loss 0.0340155, acc 0.98
2016-09-06T05:45:41.634533: step 6395, loss 0.0147621, acc 1
2016-09-06T05:45:42.442476: step 6396, loss 0.0414349, acc 0.96
2016-09-06T05:45:43.254325: step 6397, loss 0.0109566, acc 1
2016-09-06T05:45:44.083167: step 6398, loss 0.0638316, acc 0.98
2016-09-06T05:45:44.866098: step 6399, loss 0.00384903, acc 1
2016-09-06T05:45:45.665914: step 6400, loss 0.0315793, acc 0.98

Evaluation:
2016-09-06T05:45:49.397888: step 6400, loss 2.20319, acc 0.732645

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-6400

2016-09-06T05:45:51.428081: step 6401, loss 0.0268564, acc 1
2016-09-06T05:45:52.225463: step 6402, loss 0.0165135, acc 1
2016-09-06T05:45:53.025986: step 6403, loss 0.0213552, acc 1
2016-09-06T05:45:53.822789: step 6404, loss 0.0395551, acc 0.98
2016-09-06T05:45:54.633528: step 6405, loss 0.0271065, acc 1
2016-09-06T05:45:55.438533: step 6406, loss 0.0513148, acc 0.98
2016-09-06T05:45:56.238112: step 6407, loss 0.00831346, acc 1
2016-09-06T05:45:57.051519: step 6408, loss 0.00472375, acc 1
2016-09-06T05:45:57.854225: step 6409, loss 0.0236169, acc 0.98
2016-09-06T05:45:58.696238: step 6410, loss 0.0486724, acc 0.98
2016-09-06T05:45:59.513995: step 6411, loss 0.0208037, acc 1
2016-09-06T05:46:00.328040: step 6412, loss 0.0295376, acc 0.98
2016-09-06T05:46:01.124161: step 6413, loss 0.00519045, acc 1
2016-09-06T05:46:01.971187: step 6414, loss 0.0350272, acc 0.98
2016-09-06T05:46:02.770415: step 6415, loss 0.0187092, acc 1
2016-09-06T05:46:03.583010: step 6416, loss 0.00478404, acc 1
2016-09-06T05:46:04.438872: step 6417, loss 0.00935072, acc 1
2016-09-06T05:46:05.243044: step 6418, loss 0.0197025, acc 0.98
2016-09-06T05:46:06.054702: step 6419, loss 0.0189622, acc 1
2016-09-06T05:46:06.889397: step 6420, loss 0.00390531, acc 1
2016-09-06T05:46:07.691895: step 6421, loss 0.0408105, acc 0.98
2016-09-06T05:46:08.484680: step 6422, loss 0.0208084, acc 0.98
2016-09-06T05:46:09.327396: step 6423, loss 0.00335857, acc 1
2016-09-06T05:46:10.150428: step 6424, loss 0.00966024, acc 1
2016-09-06T05:46:11.004709: step 6425, loss 0.034504, acc 0.98
2016-09-06T05:46:11.834801: step 6426, loss 0.0224359, acc 0.98
2016-09-06T05:46:12.658931: step 6427, loss 0.0268404, acc 0.98
2016-09-06T05:46:13.475503: step 6428, loss 0.00350354, acc 1
2016-09-06T05:46:14.302190: step 6429, loss 0.0394019, acc 0.98
2016-09-06T05:46:15.098727: step 6430, loss 0.0537131, acc 0.98
2016-09-06T05:46:15.896023: step 6431, loss 0.0268308, acc 1
2016-09-06T05:46:16.742777: step 6432, loss 0.0177476, acc 0.98
2016-09-06T05:46:17.555386: step 6433, loss 0.00601182, acc 1
2016-09-06T05:46:18.388450: step 6434, loss 0.042981, acc 0.98
2016-09-06T05:46:19.194113: step 6435, loss 0.00358606, acc 1
2016-09-06T05:46:20.013599: step 6436, loss 0.0445052, acc 0.98
2016-09-06T05:46:20.787609: step 6437, loss 0.0915617, acc 0.94
2016-09-06T05:46:21.594742: step 6438, loss 0.0242027, acc 0.98
2016-09-06T05:46:22.395715: step 6439, loss 0.0105586, acc 1
2016-09-06T05:46:23.185830: step 6440, loss 0.0210017, acc 0.98
2016-09-06T05:46:24.050909: step 6441, loss 0.0209471, acc 1
2016-09-06T05:46:24.869451: step 6442, loss 0.00946415, acc 1
2016-09-06T05:46:25.656815: step 6443, loss 0.0190364, acc 0.98
2016-09-06T05:46:26.446106: step 6444, loss 0.0183532, acc 0.98
2016-09-06T05:46:27.259464: step 6445, loss 0.0324245, acc 0.98
2016-09-06T05:46:28.068894: step 6446, loss 0.0718057, acc 0.98
2016-09-06T05:46:28.878686: step 6447, loss 0.00475202, acc 1
2016-09-06T05:46:29.725540: step 6448, loss 0.0649158, acc 0.94
2016-09-06T05:46:30.536581: step 6449, loss 0.0112085, acc 1
2016-09-06T05:46:31.319590: step 6450, loss 0.0744441, acc 0.98
2016-09-06T05:46:32.128704: step 6451, loss 0.0400007, acc 0.98
2016-09-06T05:46:32.909763: step 6452, loss 0.0247464, acc 0.98
2016-09-06T05:46:33.740492: step 6453, loss 0.0513095, acc 0.96
2016-09-06T05:46:34.551646: step 6454, loss 0.00266912, acc 1
2016-09-06T05:46:35.334684: step 6455, loss 0.1071, acc 0.94
2016-09-06T05:46:36.144615: step 6456, loss 0.00375265, acc 1
2016-09-06T05:46:36.957534: step 6457, loss 0.025114, acc 0.98
2016-09-06T05:46:37.772878: step 6458, loss 0.00328343, acc 1
2016-09-06T05:46:38.580207: step 6459, loss 0.00250235, acc 1
2016-09-06T05:46:39.392915: step 6460, loss 0.00351751, acc 1
2016-09-06T05:46:40.169832: step 6461, loss 0.0309757, acc 0.98
2016-09-06T05:46:40.992422: step 6462, loss 0.0203176, acc 1
2016-09-06T05:46:41.815686: step 6463, loss 0.0228992, acc 1
2016-09-06T05:46:42.595683: step 6464, loss 0.0312126, acc 0.98
2016-09-06T05:46:43.393373: step 6465, loss 0.0034505, acc 1
2016-09-06T05:46:44.191293: step 6466, loss 0.01865, acc 1
2016-09-06T05:46:45.009410: step 6467, loss 0.0362235, acc 1
2016-09-06T05:46:45.842110: step 6468, loss 0.0169117, acc 1
2016-09-06T05:46:46.685610: step 6469, loss 0.0205801, acc 1
2016-09-06T05:46:47.529181: step 6470, loss 0.0370255, acc 0.98
2016-09-06T05:46:48.353266: step 6471, loss 0.0294635, acc 1
2016-09-06T05:46:49.206624: step 6472, loss 0.0124765, acc 1
2016-09-06T05:46:50.010351: step 6473, loss 0.0248246, acc 0.98
2016-09-06T05:46:50.831333: step 6474, loss 0.0288307, acc 1
2016-09-06T05:46:51.663531: step 6475, loss 0.00611182, acc 1
2016-09-06T05:46:52.478211: step 6476, loss 0.0118191, acc 1
2016-09-06T05:46:53.284798: step 6477, loss 0.0960952, acc 0.96
2016-09-06T05:46:54.113291: step 6478, loss 0.0065136, acc 1
2016-09-06T05:46:54.935196: step 6479, loss 0.0164369, acc 1
2016-09-06T05:46:55.746464: step 6480, loss 0.0226749, acc 0.98
2016-09-06T05:46:56.589131: step 6481, loss 0.0155979, acc 1
2016-09-06T05:46:57.408725: step 6482, loss 0.0165963, acc 1
2016-09-06T05:46:58.227187: step 6483, loss 0.0555545, acc 0.96
2016-09-06T05:46:59.076783: step 6484, loss 0.00904127, acc 1
2016-09-06T05:46:59.905424: step 6485, loss 0.0222333, acc 1
2016-09-06T05:47:00.746971: step 6486, loss 0.0328048, acc 1
2016-09-06T05:47:01.574624: step 6487, loss 0.00571086, acc 1
2016-09-06T05:47:02.393514: step 6488, loss 0.0169277, acc 1
2016-09-06T05:47:03.214271: step 6489, loss 0.00322385, acc 1
2016-09-06T05:47:04.041825: step 6490, loss 0.0153825, acc 1
2016-09-06T05:47:04.918767: step 6491, loss 0.00321668, acc 1
2016-09-06T05:47:05.737544: step 6492, loss 0.0167099, acc 1
2016-09-06T05:47:06.534166: step 6493, loss 0.0183497, acc 1
2016-09-06T05:47:07.385874: step 6494, loss 0.0383571, acc 0.98
2016-09-06T05:47:08.209551: step 6495, loss 0.00374151, acc 1
2016-09-06T05:47:09.015096: step 6496, loss 0.0353722, acc 0.96
2016-09-06T05:47:09.877417: step 6497, loss 0.0542826, acc 1
2016-09-06T05:47:10.681008: step 6498, loss 0.190882, acc 0.94
2016-09-06T05:47:11.494258: step 6499, loss 0.00298116, acc 1
2016-09-06T05:47:12.325064: step 6500, loss 0.00329093, acc 1

Evaluation:
2016-09-06T05:47:16.100124: step 6500, loss 2.23143, acc 0.734522

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-6500

2016-09-06T05:47:18.043712: step 6501, loss 0.00484877, acc 1
2016-09-06T05:47:18.847416: step 6502, loss 0.0189134, acc 1
2016-09-06T05:47:19.669369: step 6503, loss 0.0228781, acc 0.98
2016-09-06T05:47:20.500878: step 6504, loss 0.0399703, acc 0.96
2016-09-06T05:47:21.305035: step 6505, loss 0.0767764, acc 0.96
2016-09-06T05:47:22.127170: step 6506, loss 0.0346704, acc 1
2016-09-06T05:47:22.946137: step 6507, loss 0.033254, acc 0.98
2016-09-06T05:47:23.737111: step 6508, loss 0.0392673, acc 0.98
2016-09-06T05:47:24.536134: step 6509, loss 0.0278701, acc 1
2016-09-06T05:47:25.349894: step 6510, loss 0.065464, acc 0.96
2016-09-06T05:47:26.140839: step 6511, loss 0.163579, acc 0.96
2016-09-06T05:47:26.941824: step 6512, loss 0.0074589, acc 1
2016-09-06T05:47:27.743381: step 6513, loss 0.0188987, acc 1
2016-09-06T05:47:28.534216: step 6514, loss 0.0318761, acc 1
2016-09-06T05:47:29.341322: step 6515, loss 0.00391698, acc 1
2016-09-06T05:47:30.163622: step 6516, loss 0.0268975, acc 0.98
2016-09-06T05:47:30.927352: step 6517, loss 0.0251175, acc 0.98
2016-09-06T05:47:31.738356: step 6518, loss 0.0181418, acc 1
2016-09-06T05:47:32.567746: step 6519, loss 0.00274455, acc 1
2016-09-06T05:47:33.361405: step 6520, loss 0.0662416, acc 0.96
2016-09-06T05:47:34.147441: step 6521, loss 0.0343806, acc 0.98
2016-09-06T05:47:34.966944: step 6522, loss 0.0172973, acc 0.98
2016-09-06T05:47:35.753517: step 6523, loss 0.0367357, acc 0.96
2016-09-06T05:47:36.562114: step 6524, loss 0.027569, acc 1
2016-09-06T05:47:37.391777: step 6525, loss 0.033079, acc 0.98
2016-09-06T05:47:38.183602: step 6526, loss 0.0406421, acc 0.98
2016-09-06T05:47:39.011913: step 6527, loss 0.0786025, acc 0.96
2016-09-06T05:47:39.761287: step 6528, loss 0.00592546, acc 1
2016-09-06T05:47:40.573915: step 6529, loss 0.0402784, acc 0.98
2016-09-06T05:47:41.370840: step 6530, loss 0.0311315, acc 0.98
2016-09-06T05:47:42.213460: step 6531, loss 0.00248392, acc 1
2016-09-06T05:47:43.009702: step 6532, loss 0.0288169, acc 0.98
2016-09-06T05:47:43.816697: step 6533, loss 0.0915564, acc 0.94
2016-09-06T05:47:44.645945: step 6534, loss 0.0031454, acc 1
2016-09-06T05:47:45.409911: step 6535, loss 0.0422482, acc 0.98
2016-09-06T05:47:46.210616: step 6536, loss 0.0220379, acc 1
2016-09-06T05:47:47.059198: step 6537, loss 0.0230463, acc 1
2016-09-06T05:47:47.864338: step 6538, loss 0.0280094, acc 1
2016-09-06T05:47:48.678941: step 6539, loss 0.0078628, acc 1
2016-09-06T05:47:49.538753: step 6540, loss 0.0121753, acc 1
2016-09-06T05:47:50.353606: step 6541, loss 0.00814545, acc 1
2016-09-06T05:47:51.164457: step 6542, loss 0.0391781, acc 0.98
2016-09-06T05:47:51.994411: step 6543, loss 0.0591013, acc 0.98
2016-09-06T05:47:52.802208: step 6544, loss 0.0151511, acc 1
2016-09-06T05:47:53.614183: step 6545, loss 0.0831167, acc 0.96
2016-09-06T05:47:54.488799: step 6546, loss 0.0146116, acc 1
2016-09-06T05:47:55.304559: step 6547, loss 0.0144045, acc 1
2016-09-06T05:47:56.129294: step 6548, loss 0.00290418, acc 1
2016-09-06T05:47:56.948065: step 6549, loss 0.00417786, acc 1
2016-09-06T05:47:57.766633: step 6550, loss 0.0119293, acc 1
2016-09-06T05:47:58.567839: step 6551, loss 0.0142305, acc 1
2016-09-06T05:47:59.409304: step 6552, loss 0.0355316, acc 0.98
2016-09-06T05:48:00.226568: step 6553, loss 0.0554338, acc 0.98
2016-09-06T05:48:01.057193: step 6554, loss 0.0259515, acc 1
2016-09-06T05:48:01.888539: step 6555, loss 0.0136036, acc 1
2016-09-06T05:48:02.688611: step 6556, loss 0.0145065, acc 1
2016-09-06T05:48:03.530777: step 6557, loss 0.0485433, acc 0.98
2016-09-06T05:48:04.348056: step 6558, loss 0.0240916, acc 0.98
2016-09-06T05:48:05.162941: step 6559, loss 0.0375038, acc 0.98
2016-09-06T05:48:05.956322: step 6560, loss 0.0361031, acc 0.98
2016-09-06T05:48:06.792650: step 6561, loss 0.00578189, acc 1
2016-09-06T05:48:07.578746: step 6562, loss 0.0108632, acc 1
2016-09-06T05:48:08.376855: step 6563, loss 0.0348676, acc 0.98
2016-09-06T05:48:09.223268: step 6564, loss 0.0106244, acc 1
2016-09-06T05:48:10.025885: step 6565, loss 0.00388344, acc 1
2016-09-06T05:48:10.800201: step 6566, loss 0.0755037, acc 0.96
2016-09-06T05:48:11.628128: step 6567, loss 0.02226, acc 0.98
2016-09-06T05:48:12.441251: step 6568, loss 0.00283444, acc 1
2016-09-06T05:48:13.243047: step 6569, loss 0.0147297, acc 1
2016-09-06T05:48:14.046371: step 6570, loss 0.00788127, acc 1
2016-09-06T05:48:14.862544: step 6571, loss 0.0346853, acc 0.96
2016-09-06T05:48:15.669050: step 6572, loss 0.00977606, acc 1
2016-09-06T05:48:16.487878: step 6573, loss 0.00367921, acc 1
2016-09-06T05:48:17.322780: step 6574, loss 0.0291457, acc 1
2016-09-06T05:48:18.109349: step 6575, loss 0.00332058, acc 1
2016-09-06T05:48:18.907739: step 6576, loss 0.0155835, acc 1
2016-09-06T05:48:19.723001: step 6577, loss 0.0215948, acc 0.98
2016-09-06T05:48:20.549996: step 6578, loss 0.0029442, acc 1
2016-09-06T05:48:21.350671: step 6579, loss 0.0499186, acc 0.96
2016-09-06T05:48:22.165731: step 6580, loss 0.0247002, acc 1
2016-09-06T05:48:23.013616: step 6581, loss 0.0510233, acc 0.96
2016-09-06T05:48:23.837151: step 6582, loss 0.00628506, acc 1
2016-09-06T05:48:24.645054: step 6583, loss 0.0187295, acc 1
2016-09-06T05:48:25.443605: step 6584, loss 0.00461108, acc 1
2016-09-06T05:48:26.233463: step 6585, loss 0.00483293, acc 1
2016-09-06T05:48:27.032395: step 6586, loss 0.0278659, acc 0.98
2016-09-06T05:48:27.843614: step 6587, loss 0.0176427, acc 1
2016-09-06T05:48:28.683207: step 6588, loss 0.00666342, acc 1
2016-09-06T05:48:29.527012: step 6589, loss 0.0247146, acc 1
2016-09-06T05:48:30.333390: step 6590, loss 0.00473617, acc 1
2016-09-06T05:48:31.154565: step 6591, loss 0.00293484, acc 1
2016-09-06T05:48:32.012333: step 6592, loss 0.0418065, acc 0.98
2016-09-06T05:48:32.816005: step 6593, loss 0.00853313, acc 1
2016-09-06T05:48:33.620918: step 6594, loss 0.0153881, acc 1
2016-09-06T05:48:34.432437: step 6595, loss 0.0261239, acc 0.98
2016-09-06T05:48:35.297890: step 6596, loss 0.0690617, acc 0.96
2016-09-06T05:48:36.122376: step 6597, loss 0.0180138, acc 1
2016-09-06T05:48:36.946537: step 6598, loss 0.0151179, acc 1
2016-09-06T05:48:37.739779: step 6599, loss 0.0719408, acc 0.96
2016-09-06T05:48:38.544600: step 6600, loss 0.0362535, acc 0.98

Evaluation:
2016-09-06T05:48:42.278459: step 6600, loss 2.50505, acc 0.733584

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-6600

2016-09-06T05:48:44.215140: step 6601, loss 0.0175571, acc 1
2016-09-06T05:48:45.029975: step 6602, loss 0.00463916, acc 1
2016-09-06T05:48:45.846874: step 6603, loss 0.0252924, acc 0.98
2016-09-06T05:48:46.706023: step 6604, loss 0.0369499, acc 0.98
2016-09-06T05:48:47.557601: step 6605, loss 0.00300809, acc 1
2016-09-06T05:48:48.352882: step 6606, loss 0.0483419, acc 0.96
2016-09-06T05:48:49.145051: step 6607, loss 0.0186404, acc 1
2016-09-06T05:48:49.956690: step 6608, loss 0.0053366, acc 1
2016-09-06T05:48:50.742310: step 6609, loss 0.00266498, acc 1
2016-09-06T05:48:51.527614: step 6610, loss 0.0149418, acc 1
2016-09-06T05:48:52.345769: step 6611, loss 0.0225642, acc 1
2016-09-06T05:48:53.122347: step 6612, loss 0.00613507, acc 1
2016-09-06T05:48:53.946207: step 6613, loss 0.0090638, acc 1
2016-09-06T05:48:54.796619: step 6614, loss 0.0642944, acc 0.98
2016-09-06T05:48:55.589643: step 6615, loss 0.00417815, acc 1
2016-09-06T05:48:56.420446: step 6616, loss 0.0197829, acc 1
2016-09-06T05:48:57.275811: step 6617, loss 0.0419789, acc 0.98
2016-09-06T05:48:58.070761: step 6618, loss 0.0543557, acc 0.98
2016-09-06T05:48:58.901202: step 6619, loss 0.0571267, acc 0.96
2016-09-06T05:48:59.730160: step 6620, loss 0.00273406, acc 1
2016-09-06T05:49:00.548120: step 6621, loss 0.0251159, acc 0.98
2016-09-06T05:49:01.367524: step 6622, loss 0.00251988, acc 1
2016-09-06T05:49:02.184156: step 6623, loss 0.00565946, acc 1
2016-09-06T05:49:03.014137: step 6624, loss 0.00266926, acc 1
2016-09-06T05:49:03.837442: step 6625, loss 0.0715961, acc 0.98
2016-09-06T05:49:04.674406: step 6626, loss 0.00860053, acc 1
2016-09-06T05:49:05.496009: step 6627, loss 0.048036, acc 0.98
2016-09-06T05:49:06.301630: step 6628, loss 0.0186429, acc 1
2016-09-06T05:49:07.164858: step 6629, loss 0.0444676, acc 0.98
2016-09-06T05:49:07.959471: step 6630, loss 0.025179, acc 1
2016-09-06T05:49:08.761400: step 6631, loss 0.00408026, acc 1
2016-09-06T05:49:09.573911: step 6632, loss 0.018989, acc 0.98
2016-09-06T05:49:10.377046: step 6633, loss 0.0318862, acc 0.98
2016-09-06T05:49:11.170163: step 6634, loss 0.0241028, acc 0.98
2016-09-06T05:49:12.015286: step 6635, loss 0.0467431, acc 0.96
2016-09-06T05:49:12.836211: step 6636, loss 0.0906858, acc 0.96
2016-09-06T05:49:13.638352: step 6637, loss 0.111646, acc 0.96
2016-09-06T05:49:14.491085: step 6638, loss 0.0591587, acc 0.96
2016-09-06T05:49:15.323593: step 6639, loss 0.0521621, acc 0.98
2016-09-06T05:49:16.113712: step 6640, loss 0.0267788, acc 1
2016-09-06T05:49:16.925398: step 6641, loss 0.0358746, acc 0.98
2016-09-06T05:49:17.749735: step 6642, loss 0.0486414, acc 0.96
2016-09-06T05:49:18.543235: step 6643, loss 0.152892, acc 0.94
2016-09-06T05:49:19.326941: step 6644, loss 0.033316, acc 0.98
2016-09-06T05:49:20.177138: step 6645, loss 0.0226118, acc 0.98
2016-09-06T05:49:20.975761: step 6646, loss 0.0183578, acc 1
2016-09-06T05:49:21.761532: step 6647, loss 0.0458434, acc 0.98
2016-09-06T05:49:22.563954: step 6648, loss 0.00523134, acc 1
2016-09-06T05:49:23.333818: step 6649, loss 0.0308327, acc 0.98
2016-09-06T05:49:24.153302: step 6650, loss 0.0203898, acc 1
2016-09-06T05:49:24.980427: step 6651, loss 0.0260484, acc 0.98
2016-09-06T05:49:25.761557: step 6652, loss 0.0280724, acc 1
2016-09-06T05:49:26.556999: step 6653, loss 0.0602907, acc 0.96
2016-09-06T05:49:27.407759: step 6654, loss 0.0290644, acc 1
2016-09-06T05:49:28.198166: step 6655, loss 0.0470694, acc 0.96
2016-09-06T05:49:29.035836: step 6656, loss 0.0241281, acc 1
2016-09-06T05:49:29.895450: step 6657, loss 0.0258974, acc 0.98
2016-09-06T05:49:30.709012: step 6658, loss 0.0361299, acc 0.98
2016-09-06T05:49:31.546097: step 6659, loss 0.0373198, acc 0.98
2016-09-06T05:49:32.355888: step 6660, loss 0.0227109, acc 0.98
2016-09-06T05:49:33.162140: step 6661, loss 0.0301777, acc 1
2016-09-06T05:49:34.017073: step 6662, loss 0.0309744, acc 0.98
2016-09-06T05:49:34.890022: step 6663, loss 0.0109938, acc 1
2016-09-06T05:49:35.705081: step 6664, loss 0.00430188, acc 1
2016-09-06T05:49:36.538604: step 6665, loss 0.0237868, acc 1
2016-09-06T05:49:37.382381: step 6666, loss 0.0197377, acc 0.98
2016-09-06T05:49:38.185569: step 6667, loss 0.005349, acc 1
2016-09-06T05:49:38.979160: step 6668, loss 0.0189963, acc 1
2016-09-06T05:49:39.791557: step 6669, loss 0.0286948, acc 1
2016-09-06T05:49:40.604033: step 6670, loss 0.00426945, acc 1
2016-09-06T05:49:41.413096: step 6671, loss 0.0074876, acc 1
2016-09-06T05:49:42.238837: step 6672, loss 0.0249953, acc 0.98
2016-09-06T05:49:43.061068: step 6673, loss 0.09021, acc 0.98
2016-09-06T05:49:43.859394: step 6674, loss 0.0176623, acc 1
2016-09-06T05:49:44.669323: step 6675, loss 0.0231023, acc 1
2016-09-06T05:49:45.491911: step 6676, loss 0.00530459, acc 1
2016-09-06T05:49:46.306244: step 6677, loss 0.023423, acc 1
2016-09-06T05:49:47.126184: step 6678, loss 0.0212376, acc 1
2016-09-06T05:49:47.954379: step 6679, loss 0.0521162, acc 0.98
2016-09-06T05:49:48.738252: step 6680, loss 0.0129403, acc 1
2016-09-06T05:49:49.516073: step 6681, loss 0.0244095, acc 0.98
2016-09-06T05:49:50.357779: step 6682, loss 0.00415159, acc 1
2016-09-06T05:49:51.132748: step 6683, loss 0.241137, acc 0.96
2016-09-06T05:49:51.936390: step 6684, loss 0.0136636, acc 1
2016-09-06T05:49:52.773629: step 6685, loss 0.0782473, acc 0.96
2016-09-06T05:49:53.538638: step 6686, loss 0.0231635, acc 0.98
2016-09-06T05:49:54.337566: step 6687, loss 0.028829, acc 1
2016-09-06T05:49:55.154589: step 6688, loss 0.0608286, acc 0.98
2016-09-06T05:49:55.949650: step 6689, loss 0.0659656, acc 0.96
2016-09-06T05:49:56.757010: step 6690, loss 0.0418519, acc 0.98
2016-09-06T05:49:57.547674: step 6691, loss 0.0517505, acc 0.98
2016-09-06T05:49:58.359420: step 6692, loss 0.00374514, acc 1
2016-09-06T05:49:59.152197: step 6693, loss 0.00522033, acc 1
2016-09-06T05:49:59.978598: step 6694, loss 0.0321021, acc 0.98
2016-09-06T05:50:00.797461: step 6695, loss 0.0113825, acc 1
2016-09-06T05:50:01.638425: step 6696, loss 0.024745, acc 1
2016-09-06T05:50:02.424485: step 6697, loss 0.0913408, acc 0.98
2016-09-06T05:50:03.218682: step 6698, loss 0.0120647, acc 1
2016-09-06T05:50:04.045416: step 6699, loss 0.069629, acc 0.98
2016-09-06T05:50:04.858226: step 6700, loss 0.00305816, acc 1

Evaluation:
2016-09-06T05:50:08.519267: step 6700, loss 2.19421, acc 0.73546

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-6700

2016-09-06T05:50:10.457912: step 6701, loss 0.0172691, acc 1
2016-09-06T05:50:11.274604: step 6702, loss 0.0238058, acc 0.98
2016-09-06T05:50:12.079635: step 6703, loss 0.0520417, acc 0.96
2016-09-06T05:50:12.892414: step 6704, loss 0.0840656, acc 0.98
2016-09-06T05:50:13.702567: step 6705, loss 0.00862239, acc 1
2016-09-06T05:50:14.535185: step 6706, loss 0.034403, acc 0.98
2016-09-06T05:50:15.329972: step 6707, loss 0.0201017, acc 1
2016-09-06T05:50:16.143103: step 6708, loss 0.0393312, acc 0.96
2016-09-06T05:50:16.967353: step 6709, loss 0.0225669, acc 0.98
2016-09-06T05:50:17.742334: step 6710, loss 0.0142878, acc 1
2016-09-06T05:50:18.562609: step 6711, loss 0.00548542, acc 1
2016-09-06T05:50:19.390680: step 6712, loss 0.0252278, acc 0.98
2016-09-06T05:50:20.204578: step 6713, loss 0.0259338, acc 0.98
2016-09-06T05:50:20.999494: step 6714, loss 0.0248162, acc 0.98
2016-09-06T05:50:21.818971: step 6715, loss 0.0107024, acc 1
2016-09-06T05:50:22.606065: step 6716, loss 0.00645347, acc 1
2016-09-06T05:50:23.432326: step 6717, loss 0.0367194, acc 0.98
2016-09-06T05:50:24.259987: step 6718, loss 0.0664671, acc 0.96
2016-09-06T05:50:25.058321: step 6719, loss 0.0911751, acc 0.96
2016-09-06T05:50:25.812225: step 6720, loss 0.0469179, acc 0.977273
2016-09-06T05:50:26.641423: step 6721, loss 0.00403735, acc 1
2016-09-06T05:50:27.464454: step 6722, loss 0.104626, acc 0.94
2016-09-06T05:50:28.249566: step 6723, loss 0.0280372, acc 0.98
2016-09-06T05:50:29.053679: step 6724, loss 0.0651994, acc 0.98
2016-09-06T05:50:29.834202: step 6725, loss 0.0208529, acc 0.98
2016-09-06T05:50:30.642238: step 6726, loss 0.0437808, acc 0.96
2016-09-06T05:50:31.496219: step 6727, loss 0.0186673, acc 1
2016-09-06T05:50:32.271086: step 6728, loss 0.0261071, acc 1
2016-09-06T05:50:33.075082: step 6729, loss 0.0215904, acc 1
2016-09-06T05:50:33.900733: step 6730, loss 0.0913753, acc 0.98
2016-09-06T05:50:34.694967: step 6731, loss 0.0459663, acc 0.96
2016-09-06T05:50:35.484752: step 6732, loss 0.107447, acc 0.98
2016-09-06T05:50:36.295502: step 6733, loss 0.0283917, acc 1
2016-09-06T05:50:37.105533: step 6734, loss 0.0461173, acc 0.98
2016-09-06T05:50:37.923799: step 6735, loss 0.0398533, acc 0.98
2016-09-06T05:50:38.738746: step 6736, loss 0.0148656, acc 1
2016-09-06T05:50:39.526849: step 6737, loss 0.0371188, acc 0.98
2016-09-06T05:50:40.311524: step 6738, loss 0.00386295, acc 1
2016-09-06T05:50:41.154267: step 6739, loss 0.0172432, acc 1
2016-09-06T05:50:41.954261: step 6740, loss 0.0103469, acc 1
2016-09-06T05:50:42.749025: step 6741, loss 0.014894, acc 1
2016-09-06T05:50:43.570961: step 6742, loss 0.0156899, acc 1
2016-09-06T05:50:44.336254: step 6743, loss 0.0372375, acc 1
2016-09-06T05:50:45.174898: step 6744, loss 0.00803367, acc 1
2016-09-06T05:50:45.971790: step 6745, loss 0.013459, acc 1
2016-09-06T05:50:46.754624: step 6746, loss 0.0672576, acc 0.96
2016-09-06T05:50:47.576452: step 6747, loss 0.0182989, acc 1
2016-09-06T05:50:48.392923: step 6748, loss 0.022115, acc 1
2016-09-06T05:50:49.167562: step 6749, loss 0.0702818, acc 0.96
2016-09-06T05:50:49.984193: step 6750, loss 0.0206861, acc 1
2016-09-06T05:50:50.813868: step 6751, loss 0.00444544, acc 1
2016-09-06T05:50:51.620413: step 6752, loss 0.0178571, acc 1
2016-09-06T05:50:52.412602: step 6753, loss 0.0271584, acc 1
2016-09-06T05:50:53.251443: step 6754, loss 0.0441839, acc 0.98
2016-09-06T05:50:54.056866: step 6755, loss 0.0826077, acc 0.98
2016-09-06T05:50:54.881082: step 6756, loss 0.00540094, acc 1
2016-09-06T05:50:55.739969: step 6757, loss 0.0936356, acc 0.98
2016-09-06T05:50:56.552620: step 6758, loss 0.068381, acc 0.96
2016-09-06T05:50:57.359376: step 6759, loss 0.0891661, acc 0.96
2016-09-06T05:50:58.201001: step 6760, loss 0.00837462, acc 1
2016-09-06T05:50:59.010080: step 6761, loss 0.0380165, acc 1
2016-09-06T05:50:59.818991: step 6762, loss 0.0142317, acc 1
2016-09-06T05:51:00.700130: step 6763, loss 0.0169448, acc 1
2016-09-06T05:51:01.515939: step 6764, loss 0.0210814, acc 0.98
2016-09-06T05:51:02.324435: step 6765, loss 0.00594272, acc 1
2016-09-06T05:51:03.133952: step 6766, loss 0.0486525, acc 0.98
2016-09-06T05:51:03.938060: step 6767, loss 0.0262213, acc 0.98
2016-09-06T05:51:04.750339: step 6768, loss 0.00957744, acc 1
2016-09-06T05:51:05.612311: step 6769, loss 0.0430934, acc 0.98
2016-09-06T05:51:06.413422: step 6770, loss 0.0640643, acc 0.98
2016-09-06T05:51:07.234595: step 6771, loss 0.00430406, acc 1
2016-09-06T05:51:08.061873: step 6772, loss 0.0222301, acc 1
2016-09-06T05:51:08.884407: step 6773, loss 0.0365712, acc 1
2016-09-06T05:51:09.690569: step 6774, loss 0.0487817, acc 0.98
2016-09-06T05:51:10.474164: step 6775, loss 0.0335781, acc 0.98
2016-09-06T05:51:11.376315: step 6776, loss 0.0169172, acc 1
2016-09-06T05:51:12.184417: step 6777, loss 0.00481023, acc 1
2016-09-06T05:51:12.998622: step 6778, loss 0.0186831, acc 1
2016-09-06T05:51:13.828844: step 6779, loss 0.0907213, acc 0.96
2016-09-06T05:51:14.632219: step 6780, loss 0.0033448, acc 1
2016-09-06T05:51:15.435308: step 6781, loss 0.0529466, acc 0.98
2016-09-06T05:51:16.267430: step 6782, loss 0.00514206, acc 1
2016-09-06T05:51:17.104523: step 6783, loss 0.00436623, acc 1
2016-09-06T05:51:17.884882: step 6784, loss 0.0725208, acc 0.96
2016-09-06T05:51:18.731897: step 6785, loss 0.0763042, acc 0.94
2016-09-06T05:51:19.537991: step 6786, loss 0.0238639, acc 0.98
2016-09-06T05:51:20.349913: step 6787, loss 0.0300021, acc 0.98
2016-09-06T05:51:21.164484: step 6788, loss 0.0190668, acc 1
2016-09-06T05:51:21.955453: step 6789, loss 0.0418621, acc 0.98
2016-09-06T05:51:22.757081: step 6790, loss 0.0419207, acc 0.96
2016-09-06T05:51:23.579202: step 6791, loss 0.0289898, acc 0.98
2016-09-06T05:51:24.398166: step 6792, loss 0.0373393, acc 0.98
2016-09-06T05:51:25.205966: step 6793, loss 0.0178403, acc 1
2016-09-06T05:51:26.034132: step 6794, loss 0.0182959, acc 0.98
2016-09-06T05:51:26.848627: step 6795, loss 0.058529, acc 0.98
2016-09-06T05:51:27.665879: step 6796, loss 0.0167709, acc 1
2016-09-06T05:51:28.521383: step 6797, loss 0.00353128, acc 1
2016-09-06T05:51:29.340766: step 6798, loss 0.0181554, acc 0.98
2016-09-06T05:51:30.154077: step 6799, loss 0.122606, acc 0.94
2016-09-06T05:51:30.992204: step 6800, loss 0.00538414, acc 1

Evaluation:
2016-09-06T05:51:34.734198: step 6800, loss 2.20404, acc 0.736398

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-6800

2016-09-06T05:51:36.593394: step 6801, loss 0.0140571, acc 1
2016-09-06T05:51:37.406701: step 6802, loss 0.0816936, acc 0.98
2016-09-06T05:51:38.241970: step 6803, loss 0.0178397, acc 1
2016-09-06T05:51:39.088668: step 6804, loss 0.0204071, acc 0.98
2016-09-06T05:51:39.908459: step 6805, loss 0.0184962, acc 1
2016-09-06T05:51:40.753080: step 6806, loss 0.0167518, acc 1
2016-09-06T05:51:41.606198: step 6807, loss 0.00288884, acc 1
2016-09-06T05:51:42.419402: step 6808, loss 0.0115389, acc 1
2016-09-06T05:51:43.250405: step 6809, loss 0.0897851, acc 0.98
2016-09-06T05:51:44.070045: step 6810, loss 0.00376175, acc 1
2016-09-06T05:51:44.875526: step 6811, loss 0.0361287, acc 0.98
2016-09-06T05:51:45.678588: step 6812, loss 0.00299316, acc 1
2016-09-06T05:51:46.487107: step 6813, loss 0.0502125, acc 0.98
2016-09-06T05:51:47.293688: step 6814, loss 0.00887247, acc 1
2016-09-06T05:51:48.083808: step 6815, loss 0.0114599, acc 1
2016-09-06T05:51:48.888595: step 6816, loss 0.0487325, acc 0.98
2016-09-06T05:51:49.676529: step 6817, loss 0.0459308, acc 0.98
2016-09-06T05:51:50.512105: step 6818, loss 0.0318341, acc 1
2016-09-06T05:51:51.321999: step 6819, loss 0.00636547, acc 1
2016-09-06T05:51:52.109456: step 6820, loss 0.0247359, acc 0.98
2016-09-06T05:51:52.906260: step 6821, loss 0.0432448, acc 0.98
2016-09-06T05:51:53.733453: step 6822, loss 0.0109531, acc 1
2016-09-06T05:51:54.533660: step 6823, loss 0.0387938, acc 0.96
2016-09-06T05:51:55.346967: step 6824, loss 0.0154006, acc 1
2016-09-06T05:51:56.161205: step 6825, loss 0.034375, acc 0.98
2016-09-06T05:51:56.954748: step 6826, loss 0.00788692, acc 1
2016-09-06T05:51:57.750473: step 6827, loss 0.0417232, acc 0.98
2016-09-06T05:51:58.571743: step 6828, loss 0.0146533, acc 1
2016-09-06T05:51:59.348281: step 6829, loss 0.0411087, acc 0.96
2016-09-06T05:52:00.120732: step 6830, loss 0.0328076, acc 1
2016-09-06T05:52:00.971453: step 6831, loss 0.00373956, acc 1
2016-09-06T05:52:01.744414: step 6832, loss 0.0403829, acc 1
2016-09-06T05:52:02.540215: step 6833, loss 0.0313542, acc 0.98
2016-09-06T05:52:03.371416: step 6834, loss 0.0619015, acc 0.96
2016-09-06T05:52:04.175703: step 6835, loss 0.0362481, acc 0.98
2016-09-06T05:52:04.946142: step 6836, loss 0.0254566, acc 0.98
2016-09-06T05:52:05.793459: step 6837, loss 0.035757, acc 0.98
2016-09-06T05:52:06.600691: step 6838, loss 0.0167015, acc 1
2016-09-06T05:52:07.409488: step 6839, loss 0.0930196, acc 0.94
2016-09-06T05:52:08.221696: step 6840, loss 0.0040306, acc 1
2016-09-06T05:52:08.997557: step 6841, loss 0.009258, acc 1
2016-09-06T05:52:09.815732: step 6842, loss 0.00522521, acc 1
2016-09-06T05:52:10.633843: step 6843, loss 0.00305267, acc 1
2016-09-06T05:52:11.414496: step 6844, loss 0.0195754, acc 0.98
2016-09-06T05:52:12.247447: step 6845, loss 0.0688318, acc 0.96
2016-09-06T05:52:13.045382: step 6846, loss 0.0258766, acc 0.98
2016-09-06T05:52:13.810275: step 6847, loss 0.0154721, acc 1
2016-09-06T05:52:14.612633: step 6848, loss 0.0239922, acc 0.98
2016-09-06T05:52:15.437666: step 6849, loss 0.0234592, acc 0.98
2016-09-06T05:52:16.239028: step 6850, loss 0.00998918, acc 1
2016-09-06T05:52:17.053304: step 6851, loss 0.00326307, acc 1
2016-09-06T05:52:17.891631: step 6852, loss 0.0109353, acc 1
2016-09-06T05:52:18.701510: step 6853, loss 0.00364789, acc 1
2016-09-06T05:52:19.517671: step 6854, loss 0.0358481, acc 0.98
2016-09-06T05:52:20.348680: step 6855, loss 0.0509034, acc 0.98
2016-09-06T05:52:21.158117: step 6856, loss 0.0377532, acc 0.98
2016-09-06T05:52:21.988411: step 6857, loss 0.0354822, acc 0.98
2016-09-06T05:52:22.842785: step 6858, loss 0.0442464, acc 0.98
2016-09-06T05:52:23.645654: step 6859, loss 0.0239436, acc 0.98
2016-09-06T05:52:24.459528: step 6860, loss 0.00690199, acc 1
2016-09-06T05:52:25.285296: step 6861, loss 0.0375233, acc 0.98
2016-09-06T05:52:26.107205: step 6862, loss 0.0131938, acc 1
2016-09-06T05:52:26.919298: step 6863, loss 0.040393, acc 0.98
2016-09-06T05:52:27.761720: step 6864, loss 0.0202067, acc 0.98
2016-09-06T05:52:28.553414: step 6865, loss 0.0288543, acc 0.98
2016-09-06T05:52:29.370334: step 6866, loss 0.0385156, acc 0.98
2016-09-06T05:52:30.198476: step 6867, loss 0.0512403, acc 0.98
2016-09-06T05:52:31.015473: step 6868, loss 0.00411967, acc 1
2016-09-06T05:52:31.811131: step 6869, loss 0.0468353, acc 0.98
2016-09-06T05:52:32.634610: step 6870, loss 0.0522157, acc 0.96
2016-09-06T05:52:33.440997: step 6871, loss 0.00918576, acc 1
2016-09-06T05:52:34.277525: step 6872, loss 0.00329565, acc 1
2016-09-06T05:52:35.135045: step 6873, loss 0.0183906, acc 1
2016-09-06T05:52:35.977428: step 6874, loss 0.0337542, acc 0.98
2016-09-06T05:52:36.786239: step 6875, loss 0.0444693, acc 0.98
2016-09-06T05:52:37.604447: step 6876, loss 0.0241417, acc 0.98
2016-09-06T05:52:38.417440: step 6877, loss 0.00570094, acc 1
2016-09-06T05:52:39.174512: step 6878, loss 0.0183389, acc 1
2016-09-06T05:52:40.003213: step 6879, loss 0.0204101, acc 0.98
2016-09-06T05:52:40.830096: step 6880, loss 0.0755716, acc 0.96
2016-09-06T05:52:41.642782: step 6881, loss 0.0610775, acc 0.96
2016-09-06T05:52:42.469342: step 6882, loss 0.0250433, acc 1
2016-09-06T05:52:43.305231: step 6883, loss 0.0105576, acc 1
2016-09-06T05:52:44.084214: step 6884, loss 0.00721847, acc 1
2016-09-06T05:52:44.890768: step 6885, loss 0.00333879, acc 1
2016-09-06T05:52:45.699741: step 6886, loss 0.026379, acc 1
2016-09-06T05:52:46.498112: step 6887, loss 0.00938277, acc 1
2016-09-06T05:52:47.301219: step 6888, loss 0.0265282, acc 0.98
2016-09-06T05:52:48.103432: step 6889, loss 0.0907585, acc 0.98
2016-09-06T05:52:48.884955: step 6890, loss 0.145974, acc 0.96
2016-09-06T05:52:49.681672: step 6891, loss 0.025909, acc 0.98
2016-09-06T05:52:50.519091: step 6892, loss 0.00321904, acc 1
2016-09-06T05:52:51.289868: step 6893, loss 0.00322788, acc 1
2016-09-06T05:52:52.083962: step 6894, loss 0.00813094, acc 1
2016-09-06T05:52:52.913949: step 6895, loss 0.0551399, acc 0.98
2016-09-06T05:52:53.699214: step 6896, loss 0.0789418, acc 0.96
2016-09-06T05:52:54.506022: step 6897, loss 0.0200012, acc 0.98
2016-09-06T05:52:55.333754: step 6898, loss 0.135752, acc 0.94
2016-09-06T05:52:56.152237: step 6899, loss 0.129475, acc 0.94
2016-09-06T05:52:56.968768: step 6900, loss 0.0432499, acc 1

Evaluation:
2016-09-06T05:53:00.682033: step 6900, loss 2.14946, acc 0.719512

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-6900

2016-09-06T05:53:02.628399: step 6901, loss 0.00919699, acc 1
2016-09-06T05:53:03.424625: step 6902, loss 0.038493, acc 1
2016-09-06T05:53:04.241484: step 6903, loss 0.0759007, acc 0.98
2016-09-06T05:53:05.062956: step 6904, loss 0.00443778, acc 1
2016-09-06T05:53:05.858441: step 6905, loss 0.0353149, acc 0.96
2016-09-06T05:53:06.666558: step 6906, loss 0.0165048, acc 1
2016-09-06T05:53:07.467183: step 6907, loss 0.00803829, acc 1
2016-09-06T05:53:08.277124: step 6908, loss 0.00643099, acc 1
2016-09-06T05:53:09.097867: step 6909, loss 0.0652572, acc 0.96
2016-09-06T05:53:09.924948: step 6910, loss 0.00616632, acc 1
2016-09-06T05:53:10.718819: step 6911, loss 0.0103501, acc 1
2016-09-06T05:53:11.484458: step 6912, loss 0.0320875, acc 0.977273
2016-09-06T05:53:12.313556: step 6913, loss 0.0254891, acc 1
2016-09-06T05:53:13.122156: step 6914, loss 0.0234089, acc 0.98
2016-09-06T05:53:13.925075: step 6915, loss 0.0205272, acc 1
2016-09-06T05:53:14.758968: step 6916, loss 0.0557908, acc 0.96
2016-09-06T05:53:15.548343: step 6917, loss 0.0256629, acc 1
2016-09-06T05:53:16.383068: step 6918, loss 0.0325929, acc 1
2016-09-06T05:53:17.221634: step 6919, loss 0.00606911, acc 1
2016-09-06T05:53:18.007978: step 6920, loss 0.0150133, acc 1
2016-09-06T05:53:18.821403: step 6921, loss 0.0283376, acc 1
2016-09-06T05:53:19.634189: step 6922, loss 0.00738029, acc 1
2016-09-06T05:53:20.402015: step 6923, loss 0.00408944, acc 1
2016-09-06T05:53:21.221454: step 6924, loss 0.00418745, acc 1
2016-09-06T05:53:22.048002: step 6925, loss 0.0116311, acc 1
2016-09-06T05:53:22.834641: step 6926, loss 0.00593048, acc 1
2016-09-06T05:53:23.648537: step 6927, loss 0.0309378, acc 0.98
2016-09-06T05:53:24.446011: step 6928, loss 0.00537936, acc 1
2016-09-06T05:53:25.214420: step 6929, loss 0.0112154, acc 1
2016-09-06T05:53:26.010096: step 6930, loss 0.0160646, acc 1
2016-09-06T05:53:26.829928: step 6931, loss 0.0139299, acc 1
2016-09-06T05:53:27.636118: step 6932, loss 0.0142005, acc 1
2016-09-06T05:53:28.487445: step 6933, loss 0.00703678, acc 1
2016-09-06T05:53:29.298845: step 6934, loss 0.0479382, acc 0.96
2016-09-06T05:53:30.116994: step 6935, loss 0.0124328, acc 1
2016-09-06T05:53:30.917559: step 6936, loss 0.00437367, acc 1
2016-09-06T05:53:31.752908: step 6937, loss 0.0201935, acc 0.98
2016-09-06T05:53:32.551695: step 6938, loss 0.0253999, acc 0.98
2016-09-06T05:53:33.369694: step 6939, loss 0.0196211, acc 1
2016-09-06T05:53:34.206638: step 6940, loss 0.0295232, acc 0.98
2016-09-06T05:53:35.026423: step 6941, loss 0.0718772, acc 0.96
2016-09-06T05:53:35.821923: step 6942, loss 0.00373869, acc 1
2016-09-06T05:53:36.674953: step 6943, loss 0.0118042, acc 1
2016-09-06T05:53:37.505802: step 6944, loss 0.00530882, acc 1
2016-09-06T05:53:38.360528: step 6945, loss 0.0511195, acc 0.96
2016-09-06T05:53:39.186935: step 6946, loss 0.00632008, acc 1
2016-09-06T05:53:40.010692: step 6947, loss 0.00360202, acc 1
2016-09-06T05:53:40.809657: step 6948, loss 0.0754942, acc 0.98
2016-09-06T05:53:41.628843: step 6949, loss 0.0447463, acc 0.98
2016-09-06T05:53:42.447601: step 6950, loss 0.0377372, acc 0.96
2016-09-06T05:53:43.254255: step 6951, loss 0.0120315, acc 1
2016-09-06T05:53:44.107831: step 6952, loss 0.00498062, acc 1
2016-09-06T05:53:44.940719: step 6953, loss 0.0284408, acc 0.98
2016-09-06T05:53:45.725234: step 6954, loss 0.00335669, acc 1
2016-09-06T05:53:46.558097: step 6955, loss 0.0510838, acc 0.98
2016-09-06T05:53:47.383217: step 6956, loss 0.00379752, acc 1
2016-09-06T05:53:48.186892: step 6957, loss 0.00378181, acc 1
2016-09-06T05:53:48.987980: step 6958, loss 0.103719, acc 0.98
2016-09-06T05:53:49.808830: step 6959, loss 0.00358449, acc 1
2016-09-06T05:53:50.606831: step 6960, loss 0.150825, acc 0.98
2016-09-06T05:53:51.454013: step 6961, loss 0.0109788, acc 1
2016-09-06T05:53:52.245004: step 6962, loss 0.00692001, acc 1
2016-09-06T05:53:53.053396: step 6963, loss 0.0103563, acc 1
2016-09-06T05:53:53.877393: step 6964, loss 0.0396786, acc 0.98
2016-09-06T05:53:54.680007: step 6965, loss 0.0188619, acc 0.98
2016-09-06T05:53:55.481378: step 6966, loss 0.0617841, acc 0.98
2016-09-06T05:53:56.267698: step 6967, loss 0.0136301, acc 1
2016-09-06T05:53:57.087153: step 6968, loss 0.013144, acc 1
2016-09-06T05:53:57.867691: step 6969, loss 0.00351493, acc 1
2016-09-06T05:53:58.668389: step 6970, loss 0.0516019, acc 0.98
2016-09-06T05:53:59.510905: step 6971, loss 0.083309, acc 0.98
2016-09-06T05:54:00.309420: step 6972, loss 0.0026383, acc 1
2016-09-06T05:54:01.113414: step 6973, loss 0.0375507, acc 0.98
2016-09-06T05:54:01.951399: step 6974, loss 0.00235978, acc 1
2016-09-06T05:54:02.740320: step 6975, loss 0.00388257, acc 1
2016-09-06T05:54:03.537355: step 6976, loss 0.033149, acc 0.98
2016-09-06T05:54:04.346007: step 6977, loss 0.0296217, acc 0.98
2016-09-06T05:54:05.146794: step 6978, loss 0.0244303, acc 0.98
2016-09-06T05:54:05.949092: step 6979, loss 0.0256034, acc 0.98
2016-09-06T05:54:06.757715: step 6980, loss 0.0222898, acc 1
2016-09-06T05:54:07.532170: step 6981, loss 0.0338718, acc 0.98
2016-09-06T05:54:08.357411: step 6982, loss 0.0175704, acc 1
2016-09-06T05:54:09.189243: step 6983, loss 0.0110624, acc 1
2016-09-06T05:54:09.985531: step 6984, loss 0.112245, acc 0.98
2016-09-06T05:54:10.795686: step 6985, loss 0.00675902, acc 1
2016-09-06T05:54:11.611069: step 6986, loss 0.0403002, acc 0.98
2016-09-06T05:54:12.409486: step 6987, loss 0.0155516, acc 1
2016-09-06T05:54:13.208042: step 6988, loss 0.018847, acc 0.98
2016-09-06T05:54:14.026217: step 6989, loss 0.00450896, acc 1
2016-09-06T05:54:14.818697: step 6990, loss 0.0298602, acc 1
2016-09-06T05:54:15.607773: step 6991, loss 0.0245249, acc 0.98
2016-09-06T05:54:16.429502: step 6992, loss 0.00544573, acc 1
2016-09-06T05:54:17.203923: step 6993, loss 0.00433718, acc 1
2016-09-06T05:54:18.026149: step 6994, loss 0.00698205, acc 1
2016-09-06T05:54:18.819102: step 6995, loss 0.0153712, acc 1
2016-09-06T05:54:19.614606: step 6996, loss 0.00473219, acc 1
2016-09-06T05:54:20.430767: step 6997, loss 0.0219069, acc 1
2016-09-06T05:54:21.223129: step 6998, loss 0.0298184, acc 0.98
2016-09-06T05:54:22.020681: step 6999, loss 0.0483858, acc 0.98
2016-09-06T05:54:22.840521: step 7000, loss 0.0156371, acc 1

Evaluation:
2016-09-06T05:54:26.563525: step 7000, loss 2.20662, acc 0.752345

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-7000

2016-09-06T05:54:28.463922: step 7001, loss 0.0046189, acc 1
2016-09-06T05:54:29.274478: step 7002, loss 0.0335048, acc 0.98
2016-09-06T05:54:30.096369: step 7003, loss 0.0113168, acc 1
2016-09-06T05:54:30.919939: step 7004, loss 0.0031994, acc 1
2016-09-06T05:54:31.725663: step 7005, loss 0.0168028, acc 1
2016-09-06T05:54:32.538533: step 7006, loss 0.00488477, acc 1
2016-09-06T05:54:33.370062: step 7007, loss 0.0917047, acc 0.98
2016-09-06T05:54:34.144847: step 7008, loss 0.00369084, acc 1
2016-09-06T05:54:34.972156: step 7009, loss 0.00330323, acc 1
2016-09-06T05:54:35.796169: step 7010, loss 0.0914133, acc 0.96
2016-09-06T05:54:36.588936: step 7011, loss 0.00330117, acc 1
2016-09-06T05:54:37.374478: step 7012, loss 0.0275141, acc 0.98
2016-09-06T05:54:38.203858: step 7013, loss 0.0509667, acc 0.98
2016-09-06T05:54:38.977781: step 7014, loss 0.0164741, acc 1
2016-09-06T05:54:39.792829: step 7015, loss 0.0184504, acc 0.98
2016-09-06T05:54:40.632398: step 7016, loss 0.00361027, acc 1
2016-09-06T05:54:41.448830: step 7017, loss 0.0195994, acc 0.98
2016-09-06T05:54:42.259510: step 7018, loss 0.00663341, acc 1
2016-09-06T05:54:43.085263: step 7019, loss 0.031474, acc 0.98
2016-09-06T05:54:43.905869: step 7020, loss 0.0396337, acc 0.98
2016-09-06T05:54:44.718664: step 7021, loss 0.0418028, acc 0.98
2016-09-06T05:54:45.576578: step 7022, loss 0.150339, acc 0.96
2016-09-06T05:54:46.407509: step 7023, loss 0.0197956, acc 0.98
2016-09-06T05:54:47.232154: step 7024, loss 0.0233176, acc 1
2016-09-06T05:54:48.075431: step 7025, loss 0.0131847, acc 1
2016-09-06T05:54:48.876327: step 7026, loss 0.0120966, acc 1
2016-09-06T05:54:49.676346: step 7027, loss 0.0294125, acc 0.98
2016-09-06T05:54:50.517738: step 7028, loss 0.101062, acc 0.96
2016-09-06T05:54:51.328527: step 7029, loss 0.0203669, acc 1
2016-09-06T05:54:52.158526: step 7030, loss 0.00915814, acc 1
2016-09-06T05:54:52.989959: step 7031, loss 0.0848133, acc 0.98
2016-09-06T05:54:53.816631: step 7032, loss 0.0094135, acc 1
2016-09-06T05:54:54.636660: step 7033, loss 0.00369796, acc 1
2016-09-06T05:54:55.476405: step 7034, loss 0.00397357, acc 1
2016-09-06T05:54:56.325438: step 7035, loss 0.0185587, acc 0.98
2016-09-06T05:54:57.118867: step 7036, loss 0.0300537, acc 0.98
2016-09-06T05:54:57.911160: step 7037, loss 0.0216272, acc 0.98
2016-09-06T05:54:58.745889: step 7038, loss 0.0244711, acc 0.98
2016-09-06T05:54:59.530137: step 7039, loss 0.00674303, acc 1
2016-09-06T05:55:00.357659: step 7040, loss 0.0572444, acc 0.98
2016-09-06T05:55:01.170419: step 7041, loss 0.0230611, acc 1
2016-09-06T05:55:01.947457: step 7042, loss 0.01458, acc 1
2016-09-06T05:55:02.749916: step 7043, loss 0.0103058, acc 1
2016-09-06T05:55:03.597347: step 7044, loss 0.00581301, acc 1
2016-09-06T05:55:04.399168: step 7045, loss 0.0426694, acc 0.96
2016-09-06T05:55:05.206485: step 7046, loss 0.0189566, acc 0.98
2016-09-06T05:55:06.015650: step 7047, loss 0.0329255, acc 0.98
2016-09-06T05:55:06.800975: step 7048, loss 0.0172416, acc 1
2016-09-06T05:55:07.591300: step 7049, loss 0.0197632, acc 1
2016-09-06T05:55:08.419723: step 7050, loss 0.0221244, acc 1
2016-09-06T05:55:09.210563: step 7051, loss 0.00571673, acc 1
2016-09-06T05:55:10.028878: step 7052, loss 0.0078097, acc 1
2016-09-06T05:55:10.859555: step 7053, loss 0.0715469, acc 0.96
2016-09-06T05:55:11.664044: step 7054, loss 0.0084056, acc 1
2016-09-06T05:55:12.475384: step 7055, loss 0.0535054, acc 0.98
2016-09-06T05:55:13.285462: step 7056, loss 0.0140197, acc 1
2016-09-06T05:55:14.100638: step 7057, loss 0.0354205, acc 0.98
2016-09-06T05:55:14.905229: step 7058, loss 0.0233564, acc 0.98
2016-09-06T05:55:15.739187: step 7059, loss 0.0340109, acc 0.98
2016-09-06T05:55:16.557164: step 7060, loss 0.00532413, acc 1
2016-09-06T05:55:17.384734: step 7061, loss 0.0283624, acc 1
2016-09-06T05:55:18.232285: step 7062, loss 0.0951267, acc 0.98
2016-09-06T05:55:19.049364: step 7063, loss 0.00494598, acc 1
2016-09-06T05:55:19.875077: step 7064, loss 0.00499657, acc 1
2016-09-06T05:55:20.697410: step 7065, loss 0.0136333, acc 1
2016-09-06T05:55:21.514074: step 7066, loss 0.0372257, acc 0.96
2016-09-06T05:55:22.328912: step 7067, loss 0.0594131, acc 0.94
2016-09-06T05:55:23.149131: step 7068, loss 0.0182628, acc 1
2016-09-06T05:55:23.985246: step 7069, loss 0.0158126, acc 1
2016-09-06T05:55:24.802829: step 7070, loss 0.0263479, acc 1
2016-09-06T05:55:25.647908: step 7071, loss 0.0159686, acc 1
2016-09-06T05:55:26.461964: step 7072, loss 0.0371405, acc 0.98
2016-09-06T05:55:27.281230: step 7073, loss 0.093101, acc 0.94
2016-09-06T05:55:28.073251: step 7074, loss 0.00728616, acc 1
2016-09-06T05:55:28.903633: step 7075, loss 0.0307378, acc 1
2016-09-06T05:55:29.687716: step 7076, loss 0.04148, acc 0.98
2016-09-06T05:55:30.522794: step 7077, loss 0.00445868, acc 1
2016-09-06T05:55:31.344395: step 7078, loss 0.0183877, acc 1
2016-09-06T05:55:32.144401: step 7079, loss 0.00345439, acc 1
2016-09-06T05:55:32.934146: step 7080, loss 0.0170842, acc 1
2016-09-06T05:55:33.762675: step 7081, loss 0.0192968, acc 1
2016-09-06T05:55:34.557424: step 7082, loss 0.0343436, acc 0.98
2016-09-06T05:55:35.358509: step 7083, loss 0.0196091, acc 0.98
2016-09-06T05:55:36.212039: step 7084, loss 0.00618453, acc 1
2016-09-06T05:55:37.018993: step 7085, loss 0.0905325, acc 0.96
2016-09-06T05:55:37.808757: step 7086, loss 0.025612, acc 0.98
2016-09-06T05:55:38.609447: step 7087, loss 0.0494966, acc 0.98
2016-09-06T05:55:39.384290: step 7088, loss 0.0255396, acc 1
2016-09-06T05:55:40.173444: step 7089, loss 0.0166018, acc 1
2016-09-06T05:55:40.988918: step 7090, loss 0.0289036, acc 1
2016-09-06T05:55:41.769432: step 7091, loss 0.0179605, acc 1
2016-09-06T05:55:42.578359: step 7092, loss 0.0446713, acc 0.98
2016-09-06T05:55:43.409552: step 7093, loss 0.00627954, acc 1
2016-09-06T05:55:44.193656: step 7094, loss 0.0399674, acc 0.98
2016-09-06T05:55:44.998826: step 7095, loss 0.00336559, acc 1
2016-09-06T05:55:45.809782: step 7096, loss 0.0468156, acc 0.98
2016-09-06T05:55:46.583438: step 7097, loss 0.0086775, acc 1
2016-09-06T05:55:47.409834: step 7098, loss 0.0159174, acc 1
2016-09-06T05:55:48.264674: step 7099, loss 0.0270003, acc 0.98
2016-09-06T05:55:49.056401: step 7100, loss 0.0129883, acc 1

Evaluation:
2016-09-06T05:55:52.822270: step 7100, loss 2.61632, acc 0.733584

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-7100

2016-09-06T05:55:54.737441: step 7101, loss 0.0136401, acc 1
2016-09-06T05:55:55.561863: step 7102, loss 0.00504106, acc 1
2016-09-06T05:55:56.355079: step 7103, loss 0.00547057, acc 1
2016-09-06T05:55:57.118228: step 7104, loss 0.00383284, acc 1
2016-09-06T05:55:57.942940: step 7105, loss 0.0117249, acc 1
2016-09-06T05:55:58.734977: step 7106, loss 0.0900921, acc 0.94
2016-09-06T05:55:59.566407: step 7107, loss 0.00740842, acc 1
2016-09-06T05:56:00.404249: step 7108, loss 0.0149955, acc 1
2016-09-06T05:56:01.200818: step 7109, loss 0.0078852, acc 1
2016-09-06T05:56:02.010902: step 7110, loss 0.0676257, acc 0.98
2016-09-06T05:56:02.867338: step 7111, loss 0.01947, acc 1
2016-09-06T05:56:03.671392: step 7112, loss 0.0193373, acc 0.98
2016-09-06T05:56:04.497867: step 7113, loss 0.0911376, acc 0.94
2016-09-06T05:56:05.341077: step 7114, loss 0.00949908, acc 1
2016-09-06T05:56:06.157612: step 7115, loss 0.00824605, acc 1
2016-09-06T05:56:06.966054: step 7116, loss 0.00843529, acc 1
2016-09-06T05:56:07.797429: step 7117, loss 0.10383, acc 0.98
2016-09-06T05:56:08.597602: step 7118, loss 0.00279811, acc 1
2016-09-06T05:56:09.409183: step 7119, loss 0.0975737, acc 0.94
2016-09-06T05:56:10.237411: step 7120, loss 0.00328085, acc 1
2016-09-06T05:56:11.051226: step 7121, loss 0.00868307, acc 1
2016-09-06T05:56:11.879854: step 7122, loss 0.00253899, acc 1
2016-09-06T05:56:12.724430: step 7123, loss 0.0172534, acc 1
2016-09-06T05:56:13.550912: step 7124, loss 0.00462313, acc 1
2016-09-06T05:56:14.375651: step 7125, loss 0.00285212, acc 1
2016-09-06T05:56:15.236131: step 7126, loss 0.0197703, acc 0.98
2016-09-06T05:56:16.086519: step 7127, loss 0.00932968, acc 1
2016-09-06T05:56:16.870292: step 7128, loss 0.00544944, acc 1
2016-09-06T05:56:17.673067: step 7129, loss 0.0432611, acc 0.98
2016-09-06T05:56:18.494322: step 7130, loss 0.0326168, acc 1
2016-09-06T05:56:19.269981: step 7131, loss 0.0153421, acc 1
2016-09-06T05:56:20.107996: step 7132, loss 0.0124147, acc 1
2016-09-06T05:56:20.917910: step 7133, loss 0.0336208, acc 0.96
2016-09-06T05:56:21.689433: step 7134, loss 0.00244768, acc 1
2016-09-06T05:56:22.503893: step 7135, loss 0.0175895, acc 1
2016-09-06T05:56:23.300835: step 7136, loss 0.0173352, acc 0.98
2016-09-06T05:56:24.088099: step 7137, loss 0.0143722, acc 1
2016-09-06T05:56:24.872286: step 7138, loss 0.0578986, acc 0.98
2016-09-06T05:56:25.715917: step 7139, loss 0.0754887, acc 0.98
2016-09-06T05:56:26.513701: step 7140, loss 0.0218463, acc 1
2016-09-06T05:56:27.337254: step 7141, loss 0.00379251, acc 1
2016-09-06T05:56:28.149215: step 7142, loss 0.0208018, acc 1
2016-09-06T05:56:28.936306: step 7143, loss 0.02328, acc 0.98
2016-09-06T05:56:29.760765: step 7144, loss 0.00263828, acc 1
2016-09-06T05:56:30.611497: step 7145, loss 0.00891295, acc 1
2016-09-06T05:56:31.418720: step 7146, loss 0.00435535, acc 1
2016-09-06T05:56:32.249660: step 7147, loss 0.0362617, acc 1
2016-09-06T05:56:33.050256: step 7148, loss 0.00338827, acc 1
2016-09-06T05:56:33.864605: step 7149, loss 0.00295006, acc 1
2016-09-06T05:56:34.670797: step 7150, loss 0.00376124, acc 1
2016-09-06T05:56:35.486877: step 7151, loss 0.0126266, acc 1
2016-09-06T05:56:36.292177: step 7152, loss 0.0192191, acc 0.98
2016-09-06T05:56:37.105183: step 7153, loss 0.00935936, acc 1
2016-09-06T05:56:37.959051: step 7154, loss 0.00276124, acc 1
2016-09-06T05:56:38.753639: step 7155, loss 0.0884872, acc 0.96
2016-09-06T05:56:39.533428: step 7156, loss 0.0213243, acc 0.98
2016-09-06T05:56:40.385205: step 7157, loss 0.0211457, acc 1
2016-09-06T05:56:41.196955: step 7158, loss 0.00991931, acc 1
2016-09-06T05:56:42.058044: step 7159, loss 0.0638957, acc 0.96
2016-09-06T05:56:42.914348: step 7160, loss 0.113589, acc 0.98
2016-09-06T05:56:43.722144: step 7161, loss 0.0030067, acc 1
2016-09-06T05:56:44.524974: step 7162, loss 0.0327347, acc 0.98
2016-09-06T05:56:45.352253: step 7163, loss 0.0377422, acc 0.98
2016-09-06T05:56:46.171423: step 7164, loss 0.0196586, acc 0.98
2016-09-06T05:56:46.970221: step 7165, loss 0.0192214, acc 0.98
2016-09-06T05:56:47.824909: step 7166, loss 0.0304055, acc 0.98
2016-09-06T05:56:48.636468: step 7167, loss 0.00993568, acc 1
2016-09-06T05:56:49.442166: step 7168, loss 0.0851563, acc 0.94
2016-09-06T05:56:50.256730: step 7169, loss 0.00300488, acc 1
2016-09-06T05:56:51.070804: step 7170, loss 0.00437734, acc 1
2016-09-06T05:56:51.922574: step 7171, loss 0.0251209, acc 0.98
2016-09-06T05:56:52.750295: step 7172, loss 0.0146054, acc 1
2016-09-06T05:56:53.581917: step 7173, loss 0.0602006, acc 0.96
2016-09-06T05:56:54.377117: step 7174, loss 0.0336965, acc 1
2016-09-06T05:56:55.180681: step 7175, loss 0.0163687, acc 1
2016-09-06T05:56:56.015649: step 7176, loss 0.0154977, acc 1
2016-09-06T05:56:56.841814: step 7177, loss 0.0127215, acc 1
2016-09-06T05:56:57.654588: step 7178, loss 0.0147746, acc 1
2016-09-06T05:56:58.497906: step 7179, loss 0.0154443, acc 1
2016-09-06T05:56:59.318134: step 7180, loss 0.0758372, acc 0.98
2016-09-06T05:57:00.138637: step 7181, loss 0.017818, acc 1
2016-09-06T05:57:01.022386: step 7182, loss 0.0399335, acc 1
2016-09-06T05:57:01.828407: step 7183, loss 0.0516397, acc 0.94
2016-09-06T05:57:02.673303: step 7184, loss 0.0388053, acc 0.98
2016-09-06T05:57:03.521891: step 7185, loss 0.00382706, acc 1
2016-09-06T05:57:04.342425: step 7186, loss 0.00334085, acc 1
2016-09-06T05:57:05.143078: step 7187, loss 0.0129206, acc 1
2016-09-06T05:57:05.979472: step 7188, loss 0.00631362, acc 1
2016-09-06T05:57:06.811238: step 7189, loss 0.00494693, acc 1
2016-09-06T05:57:07.632801: step 7190, loss 0.0160945, acc 1
2016-09-06T05:57:08.445946: step 7191, loss 0.00287817, acc 1
2016-09-06T05:57:09.298539: step 7192, loss 0.0500163, acc 0.96
2016-09-06T05:57:10.100554: step 7193, loss 0.0225237, acc 0.98
2016-09-06T05:57:10.892339: step 7194, loss 0.00301882, acc 1
2016-09-06T05:57:11.695118: step 7195, loss 0.0421942, acc 0.96
2016-09-06T05:57:12.493074: step 7196, loss 0.00250766, acc 1
2016-09-06T05:57:13.301073: step 7197, loss 0.00514263, acc 1
2016-09-06T05:57:14.102631: step 7198, loss 0.00254034, acc 1
2016-09-06T05:57:14.884562: step 7199, loss 0.00784958, acc 1
2016-09-06T05:57:15.686811: step 7200, loss 0.00264531, acc 1

Evaluation:
2016-09-06T05:57:19.422623: step 7200, loss 2.46424, acc 0.734522

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-7200

2016-09-06T05:57:21.343842: step 7201, loss 0.00338556, acc 1
2016-09-06T05:57:22.150774: step 7202, loss 0.102947, acc 0.96
2016-09-06T05:57:22.981432: step 7203, loss 0.0177206, acc 1
2016-09-06T05:57:23.762604: step 7204, loss 0.00501985, acc 1
2016-09-06T05:57:24.582576: step 7205, loss 0.022857, acc 0.98
2016-09-06T05:57:25.418999: step 7206, loss 0.00268045, acc 1
2016-09-06T05:57:26.226587: step 7207, loss 0.00438321, acc 1
2016-09-06T05:57:27.038766: step 7208, loss 0.00267761, acc 1
2016-09-06T05:57:27.865486: step 7209, loss 0.0881783, acc 0.96
2016-09-06T05:57:28.696302: step 7210, loss 0.0103594, acc 1
2016-09-06T05:57:29.484913: step 7211, loss 0.016476, acc 1
2016-09-06T05:57:30.329484: step 7212, loss 0.0460157, acc 0.98
2016-09-06T05:57:31.164495: step 7213, loss 0.0219739, acc 1
2016-09-06T05:57:31.959177: step 7214, loss 0.0100518, acc 1
2016-09-06T05:57:32.753106: step 7215, loss 0.0131302, acc 1
2016-09-06T05:57:33.579297: step 7216, loss 0.0439326, acc 0.98
2016-09-06T05:57:34.389736: step 7217, loss 0.0194945, acc 1
2016-09-06T05:57:35.200176: step 7218, loss 0.0339063, acc 1
2016-09-06T05:57:36.037855: step 7219, loss 0.087451, acc 0.94
2016-09-06T05:57:36.866944: step 7220, loss 0.0193743, acc 1
2016-09-06T05:57:37.678279: step 7221, loss 0.0783312, acc 0.94
2016-09-06T05:57:38.510294: step 7222, loss 0.0116277, acc 1
2016-09-06T05:57:39.336761: step 7223, loss 0.0233609, acc 1
2016-09-06T05:57:40.150601: step 7224, loss 0.0663257, acc 0.96
2016-09-06T05:57:41.018236: step 7225, loss 0.0406046, acc 0.98
2016-09-06T05:57:41.843366: step 7226, loss 0.0026247, acc 1
2016-09-06T05:57:42.661062: step 7227, loss 0.00309998, acc 1
2016-09-06T05:57:43.563173: step 7228, loss 0.0145486, acc 1
2016-09-06T05:57:44.380612: step 7229, loss 0.0963114, acc 0.94
2016-09-06T05:57:45.185440: step 7230, loss 0.00252142, acc 1
2016-09-06T05:57:46.001270: step 7231, loss 0.0124393, acc 1
2016-09-06T05:57:46.829071: step 7232, loss 0.0189611, acc 1
2016-09-06T05:57:47.631659: step 7233, loss 0.0238258, acc 1
2016-09-06T05:57:48.465622: step 7234, loss 0.019372, acc 1
2016-09-06T05:57:49.326271: step 7235, loss 0.00975828, acc 1
2016-09-06T05:57:50.123183: step 7236, loss 0.00934494, acc 1
2016-09-06T05:57:50.924754: step 7237, loss 0.0190454, acc 1
2016-09-06T05:57:51.763909: step 7238, loss 0.0564149, acc 0.96
2016-09-06T05:57:52.572031: step 7239, loss 0.00299023, acc 1
2016-09-06T05:57:53.401639: step 7240, loss 0.0557025, acc 0.98
2016-09-06T05:57:54.250419: step 7241, loss 0.010483, acc 1
2016-09-06T05:57:55.076844: step 7242, loss 0.0520575, acc 0.96
2016-09-06T05:57:55.875094: step 7243, loss 0.0186798, acc 0.98
2016-09-06T05:57:56.702684: step 7244, loss 0.0166382, acc 1
2016-09-06T05:57:57.526234: step 7245, loss 0.0521982, acc 0.98
2016-09-06T05:57:58.342532: step 7246, loss 0.0166162, acc 1
2016-09-06T05:57:59.163466: step 7247, loss 0.0104723, acc 1
2016-09-06T05:57:59.977623: step 7248, loss 0.00341073, acc 1
2016-09-06T05:58:00.792902: step 7249, loss 0.0630226, acc 0.96
2016-09-06T05:58:01.642324: step 7250, loss 0.0200996, acc 1
2016-09-06T05:58:02.462201: step 7251, loss 0.0146149, acc 1
2016-09-06T05:58:03.281364: step 7252, loss 0.0302429, acc 0.98
2016-09-06T05:58:04.100683: step 7253, loss 0.0176488, acc 1
2016-09-06T05:58:04.899614: step 7254, loss 0.0197448, acc 1
2016-09-06T05:58:05.717565: step 7255, loss 0.0888643, acc 0.98
2016-09-06T05:58:06.549216: step 7256, loss 0.00705263, acc 1
2016-09-06T05:58:07.367426: step 7257, loss 0.00851756, acc 1
2016-09-06T05:58:08.175026: step 7258, loss 0.00797871, acc 1
2016-09-06T05:58:09.009441: step 7259, loss 0.0236693, acc 0.98
2016-09-06T05:58:09.844268: step 7260, loss 0.0110712, acc 1
2016-09-06T05:58:10.645238: step 7261, loss 0.00838841, acc 1
2016-09-06T05:58:11.445193: step 7262, loss 0.00702823, acc 1
2016-09-06T05:58:12.268046: step 7263, loss 0.0449923, acc 0.98
2016-09-06T05:58:13.069377: step 7264, loss 0.0220753, acc 1
2016-09-06T05:58:13.872691: step 7265, loss 0.0364815, acc 0.98
2016-09-06T05:58:14.713385: step 7266, loss 0.0146548, acc 1
2016-09-06T05:58:15.521305: step 7267, loss 0.0200155, acc 0.98
2016-09-06T05:58:16.330447: step 7268, loss 0.00371553, acc 1
2016-09-06T05:58:17.175677: step 7269, loss 0.00819809, acc 1
2016-09-06T05:58:17.972074: step 7270, loss 0.0178724, acc 0.98
2016-09-06T05:58:18.782972: step 7271, loss 0.0117248, acc 1
2016-09-06T05:58:19.612554: step 7272, loss 0.00861679, acc 1
2016-09-06T05:58:20.426001: step 7273, loss 0.0187949, acc 1
2016-09-06T05:58:21.245138: step 7274, loss 0.024884, acc 1
2016-09-06T05:58:22.068425: step 7275, loss 0.00559784, acc 1
2016-09-06T05:58:22.865543: step 7276, loss 0.00465111, acc 1
2016-09-06T05:58:23.683296: step 7277, loss 0.0206452, acc 0.98
2016-09-06T05:58:24.505979: step 7278, loss 0.0207454, acc 1
2016-09-06T05:58:25.369016: step 7279, loss 0.0612499, acc 0.96
2016-09-06T05:58:26.201819: step 7280, loss 0.00504487, acc 1
2016-09-06T05:58:27.045375: step 7281, loss 0.00685863, acc 1
2016-09-06T05:58:27.853697: step 7282, loss 0.132728, acc 0.96
2016-09-06T05:58:28.678260: step 7283, loss 0.0128863, acc 1
2016-09-06T05:58:29.501332: step 7284, loss 0.0443327, acc 0.98
2016-09-06T05:58:30.364525: step 7285, loss 0.00532515, acc 1
2016-09-06T05:58:31.158982: step 7286, loss 0.0266785, acc 0.98
2016-09-06T05:58:31.951174: step 7287, loss 0.0142895, acc 1
2016-09-06T05:58:32.763530: step 7288, loss 0.00894107, acc 1
2016-09-06T05:58:33.571019: step 7289, loss 0.00596285, acc 1
2016-09-06T05:58:34.395108: step 7290, loss 0.00701964, acc 1
2016-09-06T05:58:35.207088: step 7291, loss 0.0746331, acc 0.98
2016-09-06T05:58:35.971054: step 7292, loss 0.00559287, acc 1
2016-09-06T05:58:36.763558: step 7293, loss 0.0228713, acc 1
2016-09-06T05:58:37.573018: step 7294, loss 0.0481472, acc 0.96
2016-09-06T05:58:38.385132: step 7295, loss 0.0327009, acc 0.98
2016-09-06T05:58:39.140063: step 7296, loss 0.00397545, acc 1
2016-09-06T05:58:39.967149: step 7297, loss 0.0291747, acc 0.98
2016-09-06T05:58:40.778505: step 7298, loss 0.0232676, acc 1
2016-09-06T05:58:41.659723: step 7299, loss 0.0165806, acc 1
2016-09-06T05:58:42.478594: step 7300, loss 0.0106707, acc 1

Evaluation:
2016-09-06T05:58:46.176527: step 7300, loss 2.43373, acc 0.73546

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-7300

2016-09-06T05:58:48.046743: step 7301, loss 0.0277811, acc 1
2016-09-06T05:58:48.869303: step 7302, loss 0.0110702, acc 1
2016-09-06T05:58:49.688712: step 7303, loss 0.178752, acc 0.96
2016-09-06T05:58:50.498600: step 7304, loss 0.0337664, acc 0.98
2016-09-06T05:58:51.307324: step 7305, loss 0.0344738, acc 0.98
2016-09-06T05:58:52.146395: step 7306, loss 0.140127, acc 0.96
2016-09-06T05:58:52.958072: step 7307, loss 0.00316363, acc 1
2016-09-06T05:58:53.751337: step 7308, loss 0.00788598, acc 1
2016-09-06T05:58:54.589792: step 7309, loss 0.0325306, acc 0.98
2016-09-06T05:58:55.394223: step 7310, loss 0.0305655, acc 0.98
2016-09-06T05:58:56.200653: step 7311, loss 0.0455022, acc 1
2016-09-06T05:58:57.039173: step 7312, loss 0.00905343, acc 1
2016-09-06T05:58:57.850508: step 7313, loss 0.0235259, acc 1
2016-09-06T05:58:58.681363: step 7314, loss 0.036001, acc 0.98
2016-09-06T05:58:59.512508: step 7315, loss 0.0493483, acc 0.96
2016-09-06T05:59:00.343284: step 7316, loss 0.0312512, acc 0.98
2016-09-06T05:59:01.168641: step 7317, loss 0.0112465, acc 1
2016-09-06T05:59:01.978974: step 7318, loss 0.0258334, acc 0.98
2016-09-06T05:59:02.793923: step 7319, loss 0.0141613, acc 1
2016-09-06T05:59:03.613622: step 7320, loss 0.0246313, acc 0.98
2016-09-06T05:59:04.454890: step 7321, loss 0.0359811, acc 0.98
2016-09-06T05:59:05.269588: step 7322, loss 0.0404925, acc 0.96
2016-09-06T05:59:06.118596: step 7323, loss 0.00406783, acc 1
2016-09-06T05:59:06.957452: step 7324, loss 0.00554362, acc 1
2016-09-06T05:59:07.736090: step 7325, loss 0.018791, acc 1
2016-09-06T05:59:08.571393: step 7326, loss 0.060778, acc 0.98
2016-09-06T05:59:09.393327: step 7327, loss 0.0660328, acc 0.98
2016-09-06T05:59:10.224279: step 7328, loss 0.00975639, acc 1
2016-09-06T05:59:11.017924: step 7329, loss 0.0293624, acc 0.98
2016-09-06T05:59:11.858806: step 7330, loss 0.0179017, acc 1
2016-09-06T05:59:12.727741: step 7331, loss 0.00980855, acc 1
2016-09-06T05:59:13.526096: step 7332, loss 0.00542525, acc 1
2016-09-06T05:59:14.324536: step 7333, loss 0.00365595, acc 1
2016-09-06T05:59:15.159161: step 7334, loss 0.138542, acc 0.96
2016-09-06T05:59:15.974019: step 7335, loss 0.00784808, acc 1
2016-09-06T05:59:16.792309: step 7336, loss 0.00405047, acc 1
2016-09-06T05:59:17.621280: step 7337, loss 0.0256735, acc 0.98
2016-09-06T05:59:18.442348: step 7338, loss 0.00281617, acc 1
2016-09-06T05:59:19.243535: step 7339, loss 0.0435119, acc 0.98
2016-09-06T05:59:20.080582: step 7340, loss 0.00497338, acc 1
2016-09-06T05:59:20.887786: step 7341, loss 0.00259537, acc 1
2016-09-06T05:59:21.698783: step 7342, loss 0.0131671, acc 1
2016-09-06T05:59:22.521712: step 7343, loss 0.00744752, acc 1
2016-09-06T05:59:23.345260: step 7344, loss 0.0327905, acc 0.98
2016-09-06T05:59:24.157373: step 7345, loss 0.00442111, acc 1
2016-09-06T05:59:24.977408: step 7346, loss 0.0372951, acc 0.98
2016-09-06T05:59:25.807296: step 7347, loss 0.016025, acc 1
2016-09-06T05:59:26.643605: step 7348, loss 0.0200013, acc 1
2016-09-06T05:59:27.477589: step 7349, loss 0.0271402, acc 1
2016-09-06T05:59:28.319167: step 7350, loss 0.0154624, acc 1
2016-09-06T05:59:29.121758: step 7351, loss 0.00688804, acc 1
2016-09-06T05:59:29.943958: step 7352, loss 0.00332781, acc 1
2016-09-06T05:59:30.757532: step 7353, loss 0.0238138, acc 0.98
2016-09-06T05:59:31.566927: step 7354, loss 0.105448, acc 0.98
2016-09-06T05:59:32.386678: step 7355, loss 0.0171951, acc 0.98
2016-09-06T05:59:33.187859: step 7356, loss 0.00837765, acc 1
2016-09-06T05:59:33.985463: step 7357, loss 0.0146027, acc 1
2016-09-06T05:59:34.808817: step 7358, loss 0.034551, acc 0.98
2016-09-06T05:59:35.649502: step 7359, loss 0.00650871, acc 1
2016-09-06T05:59:36.453402: step 7360, loss 0.0195566, acc 0.98
2016-09-06T05:59:37.276032: step 7361, loss 0.111214, acc 0.96
2016-09-06T05:59:38.101681: step 7362, loss 0.0197073, acc 1
2016-09-06T05:59:38.894816: step 7363, loss 0.0215071, acc 1
2016-09-06T05:59:39.691538: step 7364, loss 0.0216177, acc 0.98
2016-09-06T05:59:40.486036: step 7365, loss 0.0123513, acc 1
2016-09-06T05:59:41.268061: step 7366, loss 0.0182874, acc 1
2016-09-06T05:59:42.068326: step 7367, loss 0.00278643, acc 1
2016-09-06T05:59:42.885185: step 7368, loss 0.0370828, acc 0.98
2016-09-06T05:59:43.692359: step 7369, loss 0.0223835, acc 1
2016-09-06T05:59:44.493122: step 7370, loss 0.00519588, acc 1
2016-09-06T05:59:45.323386: step 7371, loss 0.0512034, acc 0.98
2016-09-06T05:59:46.124212: step 7372, loss 0.0267946, acc 0.98
2016-09-06T05:59:46.890307: step 7373, loss 0.050035, acc 0.98
2016-09-06T05:59:47.716533: step 7374, loss 0.0478115, acc 0.98
2016-09-06T05:59:48.502507: step 7375, loss 0.0155885, acc 1
2016-09-06T05:59:49.312705: step 7376, loss 0.00347279, acc 1
2016-09-06T05:59:50.126137: step 7377, loss 0.0157279, acc 1
2016-09-06T05:59:50.917858: step 7378, loss 0.0292146, acc 0.98
2016-09-06T05:59:51.724637: step 7379, loss 0.0122672, acc 1
2016-09-06T05:59:52.544248: step 7380, loss 0.0617309, acc 0.98
2016-09-06T05:59:53.352854: step 7381, loss 0.0129623, acc 1
2016-09-06T05:59:54.144791: step 7382, loss 0.0236144, acc 1
2016-09-06T05:59:54.985471: step 7383, loss 0.00915896, acc 1
2016-09-06T05:59:55.797396: step 7384, loss 0.0347323, acc 0.98
2016-09-06T05:59:56.606436: step 7385, loss 0.0161096, acc 1
2016-09-06T05:59:57.434746: step 7386, loss 0.00446411, acc 1
2016-09-06T05:59:58.228805: step 7387, loss 0.0298544, acc 0.98
2016-09-06T05:59:59.021845: step 7388, loss 0.0192678, acc 0.98
2016-09-06T05:59:59.840612: step 7389, loss 0.00228012, acc 1
2016-09-06T06:00:00.649138: step 7390, loss 0.00659304, acc 1
2016-09-06T06:00:01.449474: step 7391, loss 0.0254324, acc 1
2016-09-06T06:00:02.258883: step 7392, loss 0.0157224, acc 1
2016-09-06T06:00:03.030103: step 7393, loss 0.00523384, acc 1
2016-09-06T06:00:03.859341: step 7394, loss 0.0474405, acc 0.96
2016-09-06T06:00:04.690502: step 7395, loss 0.00672351, acc 1
2016-09-06T06:00:05.497415: step 7396, loss 0.00530216, acc 1
2016-09-06T06:00:06.297910: step 7397, loss 0.00384922, acc 1
2016-09-06T06:00:07.106141: step 7398, loss 0.00438632, acc 1
2016-09-06T06:00:07.898184: step 7399, loss 0.00946119, acc 1
2016-09-06T06:00:08.667690: step 7400, loss 0.0034049, acc 1

Evaluation:
2016-09-06T06:00:12.413986: step 7400, loss 2.15961, acc 0.734522

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-7400

2016-09-06T06:00:14.300680: step 7401, loss 0.0207019, acc 1
2016-09-06T06:00:15.102652: step 7402, loss 0.00230014, acc 1
2016-09-06T06:00:15.911040: step 7403, loss 0.0385314, acc 0.98
2016-09-06T06:00:16.741420: step 7404, loss 0.0301267, acc 0.98
2016-09-06T06:00:17.543568: step 7405, loss 0.017775, acc 0.98
2016-09-06T06:00:18.395696: step 7406, loss 0.0146384, acc 1
2016-09-06T06:00:19.221764: step 7407, loss 0.0130504, acc 1
2016-09-06T06:00:20.023026: step 7408, loss 0.00817624, acc 1
2016-09-06T06:00:20.842317: step 7409, loss 0.00295116, acc 1
2016-09-06T06:00:21.658585: step 7410, loss 0.00332513, acc 1
2016-09-06T06:00:22.443078: step 7411, loss 0.084321, acc 0.96
2016-09-06T06:00:23.236064: step 7412, loss 0.012864, acc 1
2016-09-06T06:00:24.056173: step 7413, loss 0.0026322, acc 1
2016-09-06T06:00:24.861951: step 7414, loss 0.00558737, acc 1
2016-09-06T06:00:25.675227: step 7415, loss 0.138339, acc 0.98
2016-09-06T06:00:26.505631: step 7416, loss 0.0627721, acc 0.98
2016-09-06T06:00:27.304401: step 7417, loss 0.039229, acc 0.98
2016-09-06T06:00:28.132107: step 7418, loss 0.00226228, acc 1
2016-09-06T06:00:28.947971: step 7419, loss 0.00325752, acc 1
2016-09-06T06:00:29.742286: step 7420, loss 0.0072037, acc 1
2016-09-06T06:00:30.564650: step 7421, loss 0.00772675, acc 1
2016-09-06T06:00:31.378078: step 7422, loss 0.0327316, acc 0.98
2016-09-06T06:00:32.173999: step 7423, loss 0.0460985, acc 0.96
2016-09-06T06:00:32.984544: step 7424, loss 0.0158731, acc 1
2016-09-06T06:00:33.809309: step 7425, loss 0.0052804, acc 1
2016-09-06T06:00:34.594378: step 7426, loss 0.0267187, acc 0.98
2016-09-06T06:00:35.384838: step 7427, loss 0.0342164, acc 0.96
2016-09-06T06:00:36.207882: step 7428, loss 0.0215913, acc 1
2016-09-06T06:00:37.008191: step 7429, loss 0.0101325, acc 1
2016-09-06T06:00:37.804714: step 7430, loss 0.0247131, acc 0.98
2016-09-06T06:00:38.616520: step 7431, loss 0.0233023, acc 1
2016-09-06T06:00:39.393579: step 7432, loss 0.0444941, acc 0.98
2016-09-06T06:00:40.231576: step 7433, loss 0.0215332, acc 0.98
2016-09-06T06:00:41.054582: step 7434, loss 0.0363458, acc 1
2016-09-06T06:00:41.898228: step 7435, loss 0.00294513, acc 1
2016-09-06T06:00:42.726460: step 7436, loss 0.00292557, acc 1
2016-09-06T06:00:43.577585: step 7437, loss 0.0675187, acc 0.96
2016-09-06T06:00:44.442368: step 7438, loss 0.00403433, acc 1
2016-09-06T06:00:45.260769: step 7439, loss 0.0367665, acc 0.98
2016-09-06T06:00:46.083822: step 7440, loss 0.0444661, acc 0.96
2016-09-06T06:00:46.897511: step 7441, loss 0.0164075, acc 1
2016-09-06T06:00:47.700212: step 7442, loss 0.0437343, acc 0.98
2016-09-06T06:00:48.525199: step 7443, loss 0.0312239, acc 1
2016-09-06T06:00:49.339887: step 7444, loss 0.0657083, acc 0.96
2016-09-06T06:00:50.149323: step 7445, loss 0.010348, acc 1
2016-09-06T06:00:51.026800: step 7446, loss 0.0337978, acc 0.98
2016-09-06T06:00:51.847578: step 7447, loss 0.00929855, acc 1
2016-09-06T06:00:52.651089: step 7448, loss 0.015762, acc 1
2016-09-06T06:00:53.491267: step 7449, loss 0.00613348, acc 1
2016-09-06T06:00:54.296550: step 7450, loss 0.00503635, acc 1
2016-09-06T06:00:55.073067: step 7451, loss 0.0138147, acc 1
2016-09-06T06:00:55.900017: step 7452, loss 0.0258581, acc 1
2016-09-06T06:00:56.725511: step 7453, loss 0.0162427, acc 1
2016-09-06T06:00:57.513405: step 7454, loss 0.00570187, acc 1
2016-09-06T06:00:58.345516: step 7455, loss 0.00971216, acc 1
2016-09-06T06:00:59.162811: step 7456, loss 0.0910252, acc 0.96
2016-09-06T06:00:59.956804: step 7457, loss 0.0383686, acc 0.98
2016-09-06T06:01:00.772562: step 7458, loss 0.00326967, acc 1
2016-09-06T06:01:01.577430: step 7459, loss 0.00778248, acc 1
2016-09-06T06:01:02.357870: step 7460, loss 0.00301002, acc 1
2016-09-06T06:01:03.161010: step 7461, loss 0.00590351, acc 1
2016-09-06T06:01:03.978453: step 7462, loss 0.0849845, acc 0.94
2016-09-06T06:01:04.774043: step 7463, loss 0.0213945, acc 0.98
2016-09-06T06:01:05.584227: step 7464, loss 0.00932894, acc 1
2016-09-06T06:01:06.424392: step 7465, loss 0.0393076, acc 1
2016-09-06T06:01:07.220235: step 7466, loss 0.00397819, acc 1
2016-09-06T06:01:08.010593: step 7467, loss 0.00488958, acc 1
2016-09-06T06:01:08.838719: step 7468, loss 0.0281647, acc 0.98
2016-09-06T06:01:09.621696: step 7469, loss 0.00485912, acc 1
2016-09-06T06:01:10.415187: step 7470, loss 0.00657844, acc 1
2016-09-06T06:01:11.241612: step 7471, loss 0.0516455, acc 0.98
2016-09-06T06:01:12.050095: step 7472, loss 0.00358674, acc 1
2016-09-06T06:01:12.860996: step 7473, loss 0.0392477, acc 0.98
2016-09-06T06:01:13.671663: step 7474, loss 0.0115617, acc 1
2016-09-06T06:01:14.463499: step 7475, loss 0.0118747, acc 1
2016-09-06T06:01:15.246387: step 7476, loss 0.019562, acc 1
2016-09-06T06:01:16.085714: step 7477, loss 0.0188053, acc 0.98
2016-09-06T06:01:16.844888: step 7478, loss 0.0291988, acc 0.98
2016-09-06T06:01:17.654073: step 7479, loss 0.00248479, acc 1
2016-09-06T06:01:18.491470: step 7480, loss 0.0213066, acc 0.98
2016-09-06T06:01:19.258099: step 7481, loss 0.0377111, acc 0.98
2016-09-06T06:01:20.053395: step 7482, loss 0.0135694, acc 1
2016-09-06T06:01:20.881280: step 7483, loss 0.0141121, acc 1
2016-09-06T06:01:21.695369: step 7484, loss 0.00463602, acc 1
2016-09-06T06:01:22.505327: step 7485, loss 0.0138122, acc 1
2016-09-06T06:01:23.328303: step 7486, loss 0.0471797, acc 0.98
2016-09-06T06:01:24.117404: step 7487, loss 0.209736, acc 0.94
2016-09-06T06:01:24.861840: step 7488, loss 0.00773661, acc 1
2016-09-06T06:01:25.688548: step 7489, loss 0.0306904, acc 0.98
2016-09-06T06:01:26.485858: step 7490, loss 0.00321218, acc 1
2016-09-06T06:01:27.350279: step 7491, loss 0.0103192, acc 1
2016-09-06T06:01:28.180191: step 7492, loss 0.00368742, acc 1
2016-09-06T06:01:28.973821: step 7493, loss 0.0237313, acc 1
2016-09-06T06:01:29.780356: step 7494, loss 0.0151212, acc 1
2016-09-06T06:01:30.592931: step 7495, loss 0.0180933, acc 1
2016-09-06T06:01:31.378237: step 7496, loss 0.0104791, acc 1
2016-09-06T06:01:32.167110: step 7497, loss 0.0834923, acc 0.96
2016-09-06T06:01:32.989331: step 7498, loss 0.0334478, acc 0.98
2016-09-06T06:01:33.770521: step 7499, loss 0.00254071, acc 1
2016-09-06T06:01:34.601439: step 7500, loss 0.00909563, acc 1

Evaluation:
2016-09-06T06:01:38.309001: step 7500, loss 2.14671, acc 0.738274

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-7500

2016-09-06T06:01:40.219231: step 7501, loss 0.0359465, acc 0.96
2016-09-06T06:01:41.036495: step 7502, loss 0.0437379, acc 0.96
2016-09-06T06:01:41.859421: step 7503, loss 0.0995602, acc 0.94
2016-09-06T06:01:42.702483: step 7504, loss 0.0025269, acc 1
2016-09-06T06:01:43.506204: step 7505, loss 0.0247264, acc 0.98
2016-09-06T06:01:44.343614: step 7506, loss 0.0592677, acc 0.96
2016-09-06T06:01:45.164562: step 7507, loss 0.0530871, acc 0.98
2016-09-06T06:01:45.950136: step 7508, loss 0.00527271, acc 1
2016-09-06T06:01:46.771054: step 7509, loss 0.00437503, acc 1
2016-09-06T06:01:47.591232: step 7510, loss 0.0282688, acc 0.98
2016-09-06T06:01:48.378074: step 7511, loss 0.0486619, acc 0.96
2016-09-06T06:01:49.175429: step 7512, loss 0.0230041, acc 1
2016-09-06T06:01:50.009793: step 7513, loss 0.014855, acc 1
2016-09-06T06:01:50.812356: step 7514, loss 0.0251925, acc 1
2016-09-06T06:01:51.626372: step 7515, loss 0.0200046, acc 0.98
2016-09-06T06:01:52.445564: step 7516, loss 0.0155192, acc 1
2016-09-06T06:01:53.238106: step 7517, loss 0.0398193, acc 0.98
2016-09-06T06:01:54.040316: step 7518, loss 0.0420427, acc 0.98
2016-09-06T06:01:54.853036: step 7519, loss 0.00284083, acc 1
2016-09-06T06:01:55.684213: step 7520, loss 0.0402871, acc 0.98
2016-09-06T06:01:56.476799: step 7521, loss 0.0287426, acc 0.98
2016-09-06T06:01:57.301222: step 7522, loss 0.0621217, acc 0.98
2016-09-06T06:01:58.085407: step 7523, loss 0.00255133, acc 1
2016-09-06T06:01:58.878003: step 7524, loss 0.0204623, acc 1
2016-09-06T06:01:59.717681: step 7525, loss 0.00224036, acc 1
2016-09-06T06:02:00.530234: step 7526, loss 0.0055762, acc 1
2016-09-06T06:02:01.339902: step 7527, loss 0.00821796, acc 1
2016-09-06T06:02:02.170064: step 7528, loss 0.00504982, acc 1
2016-09-06T06:02:02.962038: step 7529, loss 0.00516081, acc 1
2016-09-06T06:02:03.772341: step 7530, loss 0.0262404, acc 0.98
2016-09-06T06:02:04.582001: step 7531, loss 0.0306497, acc 1
2016-09-06T06:02:05.378936: step 7532, loss 0.0274542, acc 1
2016-09-06T06:02:06.201096: step 7533, loss 0.0445015, acc 0.98
2016-09-06T06:02:07.037141: step 7534, loss 0.0229766, acc 1
2016-09-06T06:02:07.857540: step 7535, loss 0.00384883, acc 1
2016-09-06T06:02:08.687929: step 7536, loss 0.0487049, acc 0.98
2016-09-06T06:02:09.516971: step 7537, loss 0.0136329, acc 1
2016-09-06T06:02:10.324980: step 7538, loss 0.0116613, acc 1
2016-09-06T06:02:11.134073: step 7539, loss 0.00937841, acc 1
2016-09-06T06:02:11.955750: step 7540, loss 0.0486411, acc 0.98
2016-09-06T06:02:12.782715: step 7541, loss 0.0422775, acc 0.98
2016-09-06T06:02:13.621394: step 7542, loss 0.00792573, acc 1
2016-09-06T06:02:14.435205: step 7543, loss 0.0277671, acc 0.98
2016-09-06T06:02:15.228826: step 7544, loss 0.0028512, acc 1
2016-09-06T06:02:16.034336: step 7545, loss 0.0500471, acc 0.98
2016-09-06T06:02:16.866612: step 7546, loss 0.0255311, acc 0.98
2016-09-06T06:02:17.683665: step 7547, loss 0.0118521, acc 1
2016-09-06T06:02:18.525022: step 7548, loss 0.00312883, acc 1
2016-09-06T06:02:19.374352: step 7549, loss 0.0224223, acc 1
2016-09-06T06:02:20.177460: step 7550, loss 0.0180515, acc 1
2016-09-06T06:02:20.966101: step 7551, loss 0.00437729, acc 1
2016-09-06T06:02:21.806703: step 7552, loss 0.00400947, acc 1
2016-09-06T06:02:22.604854: step 7553, loss 0.0546382, acc 0.98
2016-09-06T06:02:23.408027: step 7554, loss 0.0591442, acc 0.96
2016-09-06T06:02:24.233761: step 7555, loss 0.00987045, acc 1
2016-09-06T06:02:25.052088: step 7556, loss 0.0407781, acc 1
2016-09-06T06:02:25.841898: step 7557, loss 0.0138597, acc 1
2016-09-06T06:02:26.679480: step 7558, loss 0.00380447, acc 1
2016-09-06T06:02:27.522869: step 7559, loss 0.0137819, acc 1
2016-09-06T06:02:28.310467: step 7560, loss 0.0052843, acc 1
2016-09-06T06:02:29.100740: step 7561, loss 0.0191865, acc 0.98
2016-09-06T06:02:29.944610: step 7562, loss 0.0660167, acc 0.98
2016-09-06T06:02:30.738503: step 7563, loss 0.0210556, acc 1
2016-09-06T06:02:31.538253: step 7564, loss 0.0290713, acc 0.98
2016-09-06T06:02:32.372106: step 7565, loss 0.028161, acc 0.98
2016-09-06T06:02:33.164511: step 7566, loss 0.0074562, acc 1
2016-09-06T06:02:33.985263: step 7567, loss 0.0151163, acc 1
2016-09-06T06:02:34.798191: step 7568, loss 0.0444508, acc 0.98
2016-09-06T06:02:35.591409: step 7569, loss 0.0026663, acc 1
2016-09-06T06:02:36.394235: step 7570, loss 0.0188447, acc 0.98
2016-09-06T06:02:37.215858: step 7571, loss 0.0487749, acc 0.98
2016-09-06T06:02:38.011048: step 7572, loss 0.0170313, acc 1
2016-09-06T06:02:38.821156: step 7573, loss 0.0186136, acc 1
2016-09-06T06:02:39.651660: step 7574, loss 0.0633245, acc 0.98
2016-09-06T06:02:40.441509: step 7575, loss 0.0157359, acc 1
2016-09-06T06:02:41.234660: step 7576, loss 0.051137, acc 0.98
2016-09-06T06:02:42.033740: step 7577, loss 0.0819548, acc 0.96
2016-09-06T06:02:42.824235: step 7578, loss 0.00438061, acc 1
2016-09-06T06:02:43.633032: step 7579, loss 0.0209188, acc 0.98
2016-09-06T06:02:44.432022: step 7580, loss 0.00403366, acc 1
2016-09-06T06:02:45.217442: step 7581, loss 0.00386177, acc 1
2016-09-06T06:02:46.042026: step 7582, loss 0.00873716, acc 1
2016-09-06T06:02:46.843020: step 7583, loss 0.0213025, acc 0.98
2016-09-06T06:02:47.625067: step 7584, loss 0.0371292, acc 0.96
2016-09-06T06:02:48.421996: step 7585, loss 0.0121813, acc 1
2016-09-06T06:02:49.250677: step 7586, loss 0.00793702, acc 1
2016-09-06T06:02:50.010465: step 7587, loss 0.00348608, acc 1
2016-09-06T06:02:50.823870: step 7588, loss 0.0154334, acc 1
2016-09-06T06:02:51.649795: step 7589, loss 0.00386431, acc 1
2016-09-06T06:02:52.440255: step 7590, loss 0.0035617, acc 1
2016-09-06T06:02:53.252660: step 7591, loss 0.00542762, acc 1
2016-09-06T06:02:54.082050: step 7592, loss 0.00964853, acc 1
2016-09-06T06:02:54.883008: step 7593, loss 0.019431, acc 1
2016-09-06T06:02:55.696835: step 7594, loss 0.0595408, acc 0.96
2016-09-06T06:02:56.519314: step 7595, loss 0.023703, acc 0.98
2016-09-06T06:02:57.314464: step 7596, loss 0.00240358, acc 1
2016-09-06T06:02:58.088466: step 7597, loss 0.0346664, acc 1
2016-09-06T06:02:58.922093: step 7598, loss 0.0400319, acc 0.98
2016-09-06T06:02:59.713397: step 7599, loss 0.00696892, acc 1
2016-09-06T06:03:00.538041: step 7600, loss 0.0142739, acc 1

Evaluation:
2016-09-06T06:03:04.251754: step 7600, loss 2.23629, acc 0.741088

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-7600

2016-09-06T06:03:06.115267: step 7601, loss 0.0148804, acc 1
2016-09-06T06:03:06.909462: step 7602, loss 0.00534729, acc 1
2016-09-06T06:03:07.734606: step 7603, loss 0.0712427, acc 0.98
2016-09-06T06:03:08.540001: step 7604, loss 0.00621517, acc 1
2016-09-06T06:03:09.336040: step 7605, loss 0.0790675, acc 0.92
2016-09-06T06:03:10.155654: step 7606, loss 0.00254434, acc 1
2016-09-06T06:03:11.018161: step 7607, loss 0.0196635, acc 0.98
2016-09-06T06:03:11.824401: step 7608, loss 0.0185402, acc 1
2016-09-06T06:03:12.632235: step 7609, loss 0.0150906, acc 1
2016-09-06T06:03:13.460071: step 7610, loss 0.00288295, acc 1
2016-09-06T06:03:14.265662: step 7611, loss 0.0212657, acc 0.98
2016-09-06T06:03:15.067090: step 7612, loss 0.024746, acc 1
2016-09-06T06:03:15.874398: step 7613, loss 0.0143136, acc 1
2016-09-06T06:03:16.669545: step 7614, loss 0.10896, acc 0.96
2016-09-06T06:03:17.455637: step 7615, loss 0.0311836, acc 1
2016-09-06T06:03:18.293564: step 7616, loss 0.0184289, acc 0.98
2016-09-06T06:03:19.060389: step 7617, loss 0.0206885, acc 1
2016-09-06T06:03:19.861261: step 7618, loss 0.0238175, acc 0.98
2016-09-06T06:03:20.669238: step 7619, loss 0.015613, acc 1
2016-09-06T06:03:21.458058: step 7620, loss 0.0165064, acc 1
2016-09-06T06:03:22.259734: step 7621, loss 0.0158243, acc 1
2016-09-06T06:03:23.085925: step 7622, loss 0.0197377, acc 0.98
2016-09-06T06:03:23.911904: step 7623, loss 0.035311, acc 0.98
2016-09-06T06:03:24.744969: step 7624, loss 0.0127411, acc 1
2016-09-06T06:03:25.634168: step 7625, loss 0.00315913, acc 1
2016-09-06T06:03:26.454747: step 7626, loss 0.0200527, acc 0.98
2016-09-06T06:03:27.266318: step 7627, loss 0.0361607, acc 0.98
2016-09-06T06:03:28.091540: step 7628, loss 0.0117573, acc 1
2016-09-06T06:03:28.885623: step 7629, loss 0.0268489, acc 1
2016-09-06T06:03:29.694220: step 7630, loss 0.00602819, acc 1
2016-09-06T06:03:30.509409: step 7631, loss 0.0130352, acc 1
2016-09-06T06:03:31.318370: step 7632, loss 0.0354675, acc 0.98
2016-09-06T06:03:32.132201: step 7633, loss 0.0077332, acc 1
2016-09-06T06:03:32.949766: step 7634, loss 0.00344342, acc 1
2016-09-06T06:03:33.758885: step 7635, loss 0.0215039, acc 0.98
2016-09-06T06:03:34.564815: step 7636, loss 0.0144167, acc 1
2016-09-06T06:03:35.439891: step 7637, loss 0.0104469, acc 1
2016-09-06T06:03:36.284658: step 7638, loss 0.0162947, acc 1
2016-09-06T06:03:37.119249: step 7639, loss 0.00376977, acc 1
2016-09-06T06:03:37.955256: step 7640, loss 0.00301889, acc 1
2016-09-06T06:03:38.755181: step 7641, loss 0.101283, acc 0.96
2016-09-06T06:03:39.576042: step 7642, loss 0.00627315, acc 1
2016-09-06T06:03:40.361137: step 7643, loss 0.022598, acc 0.98
2016-09-06T06:03:41.163949: step 7644, loss 0.0264232, acc 1
2016-09-06T06:03:41.964652: step 7645, loss 0.0183342, acc 0.98
2016-09-06T06:03:42.777035: step 7646, loss 0.065436, acc 0.98
2016-09-06T06:03:43.612023: step 7647, loss 0.101913, acc 0.94
2016-09-06T06:03:44.401274: step 7648, loss 0.0323701, acc 1
2016-09-06T06:03:45.194926: step 7649, loss 0.149143, acc 0.96
2016-09-06T06:03:46.004351: step 7650, loss 0.0557064, acc 0.94
2016-09-06T06:03:46.819663: step 7651, loss 0.0100327, acc 1
2016-09-06T06:03:47.653038: step 7652, loss 0.00214353, acc 1
2016-09-06T06:03:48.503431: step 7653, loss 0.0591185, acc 0.98
2016-09-06T06:03:49.284772: step 7654, loss 0.0236883, acc 1
2016-09-06T06:03:50.086283: step 7655, loss 0.0221155, acc 1
2016-09-06T06:03:50.912423: step 7656, loss 0.0114139, acc 1
2016-09-06T06:03:51.696900: step 7657, loss 0.0802232, acc 0.94
2016-09-06T06:03:52.487344: step 7658, loss 0.0515646, acc 0.98
2016-09-06T06:03:53.358508: step 7659, loss 0.00919545, acc 1
2016-09-06T06:03:54.151047: step 7660, loss 0.0491057, acc 0.98
2016-09-06T06:03:54.973593: step 7661, loss 0.0160275, acc 1
2016-09-06T06:03:55.773793: step 7662, loss 0.0240938, acc 1
2016-09-06T06:03:56.549516: step 7663, loss 0.0133779, acc 1
2016-09-06T06:03:57.337713: step 7664, loss 0.0299074, acc 0.98
2016-09-06T06:03:58.163955: step 7665, loss 0.00655492, acc 1
2016-09-06T06:03:58.948255: step 7666, loss 0.0916241, acc 0.94
2016-09-06T06:03:59.776388: step 7667, loss 0.0441322, acc 0.98
2016-09-06T06:04:00.605464: step 7668, loss 0.0383224, acc 0.98
2016-09-06T06:04:01.382483: step 7669, loss 0.00311341, acc 1
2016-09-06T06:04:02.163623: step 7670, loss 0.0240909, acc 1
2016-09-06T06:04:03.001509: step 7671, loss 0.00486127, acc 1
2016-09-06T06:04:03.801037: step 7672, loss 0.00332873, acc 1
2016-09-06T06:04:04.606993: step 7673, loss 0.0173746, acc 1
2016-09-06T06:04:05.388947: step 7674, loss 0.0256555, acc 1
2016-09-06T06:04:06.203611: step 7675, loss 0.044662, acc 0.98
2016-09-06T06:04:07.037256: step 7676, loss 0.0226779, acc 0.98
2016-09-06T06:04:07.866678: step 7677, loss 0.141785, acc 0.94
2016-09-06T06:04:08.664591: step 7678, loss 0.0150235, acc 1
2016-09-06T06:04:09.462044: step 7679, loss 0.059159, acc 0.98
2016-09-06T06:04:10.252595: step 7680, loss 0.0238719, acc 0.977273
2016-09-06T06:04:11.029456: step 7681, loss 0.0635249, acc 0.96
2016-09-06T06:04:11.832485: step 7682, loss 0.0213308, acc 1
2016-09-06T06:04:12.666611: step 7683, loss 0.00244594, acc 1
2016-09-06T06:04:13.443383: step 7684, loss 0.0414921, acc 0.96
2016-09-06T06:04:14.240090: step 7685, loss 0.0196674, acc 1
2016-09-06T06:04:15.079267: step 7686, loss 0.0360978, acc 0.98
2016-09-06T06:04:15.856356: step 7687, loss 0.0468115, acc 0.96
2016-09-06T06:04:16.657446: step 7688, loss 0.013926, acc 1
2016-09-06T06:04:17.477449: step 7689, loss 0.0681263, acc 0.94
2016-09-06T06:04:18.281223: step 7690, loss 0.00414512, acc 1
2016-09-06T06:04:19.097601: step 7691, loss 0.00254218, acc 1
2016-09-06T06:04:19.919380: step 7692, loss 0.00489113, acc 1
2016-09-06T06:04:20.706267: step 7693, loss 0.0215933, acc 1
2016-09-06T06:04:21.510097: step 7694, loss 0.0165954, acc 1
2016-09-06T06:04:22.333792: step 7695, loss 0.0579948, acc 0.98
2016-09-06T06:04:23.122903: step 7696, loss 0.0159609, acc 1
2016-09-06T06:04:23.904971: step 7697, loss 0.0412305, acc 0.96
2016-09-06T06:04:24.721865: step 7698, loss 0.00269011, acc 1
2016-09-06T06:04:25.500776: step 7699, loss 0.0352463, acc 0.96
2016-09-06T06:04:26.289699: step 7700, loss 0.00249615, acc 1

Evaluation:
2016-09-06T06:04:30.047106: step 7700, loss 2.10858, acc 0.742964

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-7700

2016-09-06T06:04:31.984668: step 7701, loss 0.021896, acc 1
2016-09-06T06:04:32.787094: step 7702, loss 0.117157, acc 0.98
2016-09-06T06:04:33.637444: step 7703, loss 0.0418488, acc 0.98
2016-09-06T06:04:34.478087: step 7704, loss 0.00835145, acc 1
2016-09-06T06:04:35.281328: step 7705, loss 0.0409709, acc 0.98
2016-09-06T06:04:36.070107: step 7706, loss 0.0825094, acc 0.96
2016-09-06T06:04:36.879744: step 7707, loss 0.0249357, acc 1
2016-09-06T06:04:37.691520: step 7708, loss 0.0436528, acc 0.98
2016-09-06T06:04:38.515298: step 7709, loss 0.0174235, acc 1
2016-09-06T06:04:39.325458: step 7710, loss 0.00263277, acc 1
2016-09-06T06:04:40.107089: step 7711, loss 0.00321769, acc 1
2016-09-06T06:04:40.924342: step 7712, loss 0.0173708, acc 0.98
2016-09-06T06:04:41.744308: step 7713, loss 0.0211596, acc 1
2016-09-06T06:04:42.530820: step 7714, loss 0.0221552, acc 1
2016-09-06T06:04:43.350969: step 7715, loss 0.00974096, acc 1
2016-09-06T06:04:44.212629: step 7716, loss 0.00967998, acc 1
2016-09-06T06:04:45.035476: step 7717, loss 0.00617657, acc 1
2016-09-06T06:04:45.837636: step 7718, loss 0.00306739, acc 1
2016-09-06T06:04:46.696560: step 7719, loss 0.0252769, acc 0.98
2016-09-06T06:04:47.515399: step 7720, loss 0.00307431, acc 1
2016-09-06T06:04:48.312988: step 7721, loss 0.0285527, acc 0.98
2016-09-06T06:04:49.140715: step 7722, loss 0.00373447, acc 1
2016-09-06T06:04:49.961240: step 7723, loss 0.0528792, acc 0.96
2016-09-06T06:04:50.788061: step 7724, loss 0.00399394, acc 1
2016-09-06T06:04:51.622641: step 7725, loss 0.010158, acc 1
2016-09-06T06:04:52.469366: step 7726, loss 0.00919959, acc 1
2016-09-06T06:04:53.295888: step 7727, loss 0.0125675, acc 1
2016-09-06T06:04:54.127277: step 7728, loss 0.0149236, acc 1
2016-09-06T06:04:54.941897: step 7729, loss 0.0163314, acc 1
2016-09-06T06:04:55.795000: step 7730, loss 0.00245088, acc 1
2016-09-06T06:04:56.623751: step 7731, loss 0.0042534, acc 1
2016-09-06T06:04:57.455979: step 7732, loss 0.0268269, acc 0.98
2016-09-06T06:04:58.244365: step 7733, loss 0.00528165, acc 1
2016-09-06T06:04:59.046721: step 7734, loss 0.065775, acc 0.98
2016-09-06T06:04:59.865398: step 7735, loss 0.0183281, acc 0.98
2016-09-06T06:05:00.664348: step 7736, loss 0.0111369, acc 1
2016-09-06T06:05:01.507757: step 7737, loss 0.0315772, acc 0.98
2016-09-06T06:05:02.366138: step 7738, loss 0.0228794, acc 0.98
2016-09-06T06:05:03.174192: step 7739, loss 0.00555854, acc 1
2016-09-06T06:05:03.981376: step 7740, loss 0.00515409, acc 1
2016-09-06T06:05:04.791887: step 7741, loss 0.0263815, acc 1
2016-09-06T06:05:05.594981: step 7742, loss 0.0265164, acc 0.98
2016-09-06T06:05:06.399296: step 7743, loss 0.00244763, acc 1
2016-09-06T06:05:07.236002: step 7744, loss 0.00406539, acc 1
2016-09-06T06:05:08.060839: step 7745, loss 0.00355224, acc 1
2016-09-06T06:05:08.874442: step 7746, loss 0.0709096, acc 0.98
2016-09-06T06:05:09.714895: step 7747, loss 0.00283614, acc 1
2016-09-06T06:05:10.537907: step 7748, loss 0.0626509, acc 0.98
2016-09-06T06:05:11.348181: step 7749, loss 0.016407, acc 1
2016-09-06T06:05:12.171477: step 7750, loss 0.0215053, acc 1
2016-09-06T06:05:13.002840: step 7751, loss 0.00284572, acc 1
2016-09-06T06:05:13.813217: step 7752, loss 0.0190773, acc 0.98
2016-09-06T06:05:14.654813: step 7753, loss 0.0183829, acc 0.98
2016-09-06T06:05:15.478584: step 7754, loss 0.00805686, acc 1
2016-09-06T06:05:16.303476: step 7755, loss 0.0222639, acc 0.98
2016-09-06T06:05:17.126887: step 7756, loss 0.0192863, acc 1
2016-09-06T06:05:17.972814: step 7757, loss 0.0147601, acc 1
2016-09-06T06:05:18.753782: step 7758, loss 0.0108436, acc 1
2016-09-06T06:05:19.590507: step 7759, loss 0.00565134, acc 1
2016-09-06T06:05:20.416261: step 7760, loss 0.0335562, acc 0.98
2016-09-06T06:05:21.210327: step 7761, loss 0.00577359, acc 1
2016-09-06T06:05:22.029560: step 7762, loss 0.0179648, acc 1
2016-09-06T06:05:22.851885: step 7763, loss 0.0222118, acc 0.98
2016-09-06T06:05:23.644570: step 7764, loss 0.00354931, acc 1
2016-09-06T06:05:24.445772: step 7765, loss 0.00231035, acc 1
2016-09-06T06:05:25.297386: step 7766, loss 0.0298436, acc 0.98
2016-09-06T06:05:26.083789: step 7767, loss 0.00264479, acc 1
2016-09-06T06:05:26.875711: step 7768, loss 0.0182222, acc 1
2016-09-06T06:05:27.685189: step 7769, loss 0.00617363, acc 1
2016-09-06T06:05:28.490794: step 7770, loss 0.00882026, acc 1
2016-09-06T06:05:29.286071: step 7771, loss 0.0115545, acc 1
2016-09-06T06:05:30.088154: step 7772, loss 0.00231861, acc 1
2016-09-06T06:05:30.894539: step 7773, loss 0.0153042, acc 1
2016-09-06T06:05:31.698793: step 7774, loss 0.0040433, acc 1
2016-09-06T06:05:32.528929: step 7775, loss 0.0240796, acc 0.98
2016-09-06T06:05:33.335096: step 7776, loss 0.011127, acc 1
2016-09-06T06:05:34.143399: step 7777, loss 0.02174, acc 0.98
2016-09-06T06:05:34.975569: step 7778, loss 0.0270743, acc 0.98
2016-09-06T06:05:35.747857: step 7779, loss 0.0250288, acc 0.98
2016-09-06T06:05:36.565144: step 7780, loss 0.00233114, acc 1
2016-09-06T06:05:37.385104: step 7781, loss 0.0349965, acc 0.98
2016-09-06T06:05:38.176399: step 7782, loss 0.00285045, acc 1
2016-09-06T06:05:38.983927: step 7783, loss 0.00781982, acc 1
2016-09-06T06:05:39.792044: step 7784, loss 0.00316198, acc 1
2016-09-06T06:05:40.591722: step 7785, loss 0.0263486, acc 1
2016-09-06T06:05:41.400562: step 7786, loss 0.0166814, acc 0.98
2016-09-06T06:05:42.238686: step 7787, loss 0.0168048, acc 0.98
2016-09-06T06:05:43.013248: step 7788, loss 0.0303864, acc 0.98
2016-09-06T06:05:43.841487: step 7789, loss 0.00530322, acc 1
2016-09-06T06:05:44.672180: step 7790, loss 0.0585048, acc 0.96
2016-09-06T06:05:45.511391: step 7791, loss 0.0174363, acc 1
2016-09-06T06:05:46.330058: step 7792, loss 0.0276667, acc 0.98
2016-09-06T06:05:47.150397: step 7793, loss 0.00240454, acc 1
2016-09-06T06:05:47.927455: step 7794, loss 0.114424, acc 0.94
2016-09-06T06:05:48.731558: step 7795, loss 0.0353277, acc 0.98
2016-09-06T06:05:49.601396: step 7796, loss 0.0893179, acc 0.96
2016-09-06T06:05:50.378927: step 7797, loss 0.012729, acc 1
2016-09-06T06:05:51.215988: step 7798, loss 0.0489265, acc 0.98
2016-09-06T06:05:52.062442: step 7799, loss 0.020241, acc 0.98
2016-09-06T06:05:52.895696: step 7800, loss 0.0341494, acc 0.98

Evaluation:
2016-09-06T06:05:56.623437: step 7800, loss 2.12028, acc 0.747655

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-7800

2016-09-06T06:05:58.566179: step 7801, loss 0.00306287, acc 1
2016-09-06T06:05:59.389454: step 7802, loss 0.0164625, acc 1
2016-09-06T06:06:00.205438: step 7803, loss 0.0335555, acc 0.98
2016-09-06T06:06:01.041052: step 7804, loss 0.00714362, acc 1
2016-09-06T06:06:01.917427: step 7805, loss 0.0484524, acc 0.98
2016-09-06T06:06:02.700748: step 7806, loss 0.00687935, acc 1
2016-09-06T06:06:03.527798: step 7807, loss 0.00487636, acc 1
2016-09-06T06:06:04.357455: step 7808, loss 0.0557093, acc 0.98
2016-09-06T06:06:05.158094: step 7809, loss 0.0157018, acc 1
2016-09-06T06:06:05.967622: step 7810, loss 0.0984958, acc 0.96
2016-09-06T06:06:06.786553: step 7811, loss 0.0321563, acc 0.98
2016-09-06T06:06:07.623489: step 7812, loss 0.0214682, acc 1
2016-09-06T06:06:08.441799: step 7813, loss 0.0611627, acc 0.96
2016-09-06T06:06:09.259790: step 7814, loss 0.0215452, acc 1
2016-09-06T06:06:10.097713: step 7815, loss 0.0284411, acc 0.98
2016-09-06T06:06:10.883714: step 7816, loss 0.00455488, acc 1
2016-09-06T06:06:11.679837: step 7817, loss 0.00211871, acc 1
2016-09-06T06:06:12.532714: step 7818, loss 0.0192376, acc 1
2016-09-06T06:06:13.327551: step 7819, loss 0.00164442, acc 1
2016-09-06T06:06:14.123713: step 7820, loss 0.00632624, acc 1
2016-09-06T06:06:14.928839: step 7821, loss 0.0510059, acc 0.94
2016-09-06T06:06:15.698982: step 7822, loss 0.00749325, acc 1
2016-09-06T06:06:16.531728: step 7823, loss 0.00678061, acc 1
2016-09-06T06:06:17.342163: step 7824, loss 0.0323345, acc 1
2016-09-06T06:06:18.116452: step 7825, loss 0.00618332, acc 1
2016-09-06T06:06:18.890452: step 7826, loss 0.00197788, acc 1
2016-09-06T06:06:19.716153: step 7827, loss 0.022322, acc 0.98
2016-09-06T06:06:20.533558: step 7828, loss 0.00957187, acc 1
2016-09-06T06:06:21.330370: step 7829, loss 0.0320222, acc 0.98
2016-09-06T06:06:22.157972: step 7830, loss 0.0607516, acc 0.98
2016-09-06T06:06:22.951095: step 7831, loss 0.00318169, acc 1
2016-09-06T06:06:23.777159: step 7832, loss 0.00341068, acc 1
2016-09-06T06:06:24.558075: step 7833, loss 0.00681794, acc 1
2016-09-06T06:06:25.363869: step 7834, loss 0.0254925, acc 0.98
2016-09-06T06:06:26.158207: step 7835, loss 0.044003, acc 0.98
2016-09-06T06:06:26.971338: step 7836, loss 0.0298913, acc 0.98
2016-09-06T06:06:27.762524: step 7837, loss 0.0174525, acc 0.98
2016-09-06T06:06:28.573614: step 7838, loss 0.0277697, acc 0.98
2016-09-06T06:06:29.391433: step 7839, loss 0.0480738, acc 0.96
2016-09-06T06:06:30.181517: step 7840, loss 0.0293083, acc 0.98
2016-09-06T06:06:31.009099: step 7841, loss 0.0633243, acc 0.94
2016-09-06T06:06:31.826909: step 7842, loss 0.12018, acc 0.94
2016-09-06T06:06:32.594086: step 7843, loss 0.0333783, acc 0.98
2016-09-06T06:06:33.408346: step 7844, loss 0.00445871, acc 1
2016-09-06T06:06:34.227481: step 7845, loss 0.00238117, acc 1
2016-09-06T06:06:35.021049: step 7846, loss 0.0191682, acc 0.98
2016-09-06T06:06:35.852915: step 7847, loss 0.0350271, acc 1
2016-09-06T06:06:36.674982: step 7848, loss 0.023331, acc 0.98
2016-09-06T06:06:37.458581: step 7849, loss 0.0440017, acc 0.98
2016-09-06T06:06:38.262933: step 7850, loss 0.0169089, acc 1
2016-09-06T06:06:39.077936: step 7851, loss 0.0201336, acc 1
2016-09-06T06:06:39.860385: step 7852, loss 0.0208739, acc 1
2016-09-06T06:06:40.657512: step 7853, loss 0.0108107, acc 1
2016-09-06T06:06:41.464784: step 7854, loss 0.023041, acc 0.98
2016-09-06T06:06:42.250742: step 7855, loss 0.121736, acc 0.94
2016-09-06T06:06:43.091507: step 7856, loss 0.00301069, acc 1
2016-09-06T06:06:43.922147: step 7857, loss 0.0154732, acc 1
2016-09-06T06:06:44.709459: step 7858, loss 0.0118621, acc 1
2016-09-06T06:06:45.525314: step 7859, loss 0.0159999, acc 1
2016-09-06T06:06:46.351719: step 7860, loss 0.0433286, acc 0.98
2016-09-06T06:06:47.127328: step 7861, loss 0.0167658, acc 0.98
2016-09-06T06:06:47.937981: step 7862, loss 0.0357532, acc 0.98
2016-09-06T06:06:48.793591: step 7863, loss 0.00984346, acc 1
2016-09-06T06:06:49.571089: step 7864, loss 0.0226972, acc 0.98
2016-09-06T06:06:50.346116: step 7865, loss 0.0223413, acc 0.98
2016-09-06T06:06:51.163666: step 7866, loss 0.0163401, acc 0.98
2016-09-06T06:06:51.935458: step 7867, loss 0.0674141, acc 0.94
2016-09-06T06:06:52.751874: step 7868, loss 0.00835627, acc 1
2016-09-06T06:06:53.568707: step 7869, loss 0.0396762, acc 1
2016-09-06T06:06:54.371004: step 7870, loss 0.046858, acc 0.96
2016-09-06T06:06:55.184420: step 7871, loss 0.00891109, acc 1
2016-09-06T06:06:55.966389: step 7872, loss 0.00617356, acc 1
2016-09-06T06:06:56.785629: step 7873, loss 0.0233273, acc 0.98
2016-09-06T06:06:57.602299: step 7874, loss 0.0330367, acc 0.98
2016-09-06T06:06:58.452828: step 7875, loss 0.00891973, acc 1
2016-09-06T06:06:59.310169: step 7876, loss 0.00422381, acc 1
2016-09-06T06:07:00.120214: step 7877, loss 0.0111782, acc 1
2016-09-06T06:07:00.974571: step 7878, loss 0.0267509, acc 0.98
2016-09-06T06:07:01.812724: step 7879, loss 0.0117901, acc 1
2016-09-06T06:07:02.628752: step 7880, loss 0.0156056, acc 1
2016-09-06T06:07:03.475928: step 7881, loss 0.0333586, acc 0.98
2016-09-06T06:07:04.292093: step 7882, loss 0.0597638, acc 0.96
2016-09-06T06:07:05.104006: step 7883, loss 0.00753972, acc 1
2016-09-06T06:07:05.924372: step 7884, loss 0.00306762, acc 1
2016-09-06T06:07:06.726898: step 7885, loss 0.0688999, acc 0.96
2016-09-06T06:07:07.520654: step 7886, loss 0.0302432, acc 0.98
2016-09-06T06:07:08.338248: step 7887, loss 0.0344369, acc 0.98
2016-09-06T06:07:09.149414: step 7888, loss 0.0124151, acc 1
2016-09-06T06:07:09.942205: step 7889, loss 0.0182893, acc 1
2016-09-06T06:07:10.758018: step 7890, loss 0.0119979, acc 1
2016-09-06T06:07:11.566989: step 7891, loss 0.0461811, acc 0.98
2016-09-06T06:07:12.376354: step 7892, loss 0.00280472, acc 1
2016-09-06T06:07:13.205844: step 7893, loss 0.00422123, acc 1
2016-09-06T06:07:14.023601: step 7894, loss 0.00478053, acc 1
2016-09-06T06:07:14.854988: step 7895, loss 0.0208283, acc 1
2016-09-06T06:07:15.678841: step 7896, loss 0.0444997, acc 0.98
2016-09-06T06:07:16.502090: step 7897, loss 0.00571438, acc 1
2016-09-06T06:07:17.280016: step 7898, loss 0.0167613, acc 1
2016-09-06T06:07:18.114938: step 7899, loss 0.00448339, acc 1
2016-09-06T06:07:18.960971: step 7900, loss 0.040111, acc 0.98

Evaluation:
2016-09-06T06:07:22.660104: step 7900, loss 2.71695, acc 0.742964

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-7900

2016-09-06T06:07:24.637198: step 7901, loss 0.0708634, acc 0.98
2016-09-06T06:07:25.466370: step 7902, loss 0.0110677, acc 1
2016-09-06T06:07:26.288537: step 7903, loss 0.03314, acc 0.98
2016-09-06T06:07:27.082556: step 7904, loss 0.00255143, acc 1
2016-09-06T06:07:27.865406: step 7905, loss 0.00630904, acc 1
2016-09-06T06:07:28.704872: step 7906, loss 0.0174969, acc 1
2016-09-06T06:07:29.520182: step 7907, loss 0.0153921, acc 1
2016-09-06T06:07:30.322403: step 7908, loss 0.00242776, acc 1
2016-09-06T06:07:31.157680: step 7909, loss 0.00404252, acc 1
2016-09-06T06:07:31.975303: step 7910, loss 0.0145143, acc 1
2016-09-06T06:07:32.785851: step 7911, loss 0.0400201, acc 0.98
2016-09-06T06:07:33.625270: step 7912, loss 0.0058011, acc 1
2016-09-06T06:07:34.425410: step 7913, loss 0.0338475, acc 0.98
2016-09-06T06:07:35.256184: step 7914, loss 0.0718411, acc 0.96
2016-09-06T06:07:36.104876: step 7915, loss 0.0515849, acc 0.98
2016-09-06T06:07:36.947077: step 7916, loss 0.00393911, acc 1
2016-09-06T06:07:37.788028: step 7917, loss 0.0482756, acc 0.98
2016-09-06T06:07:38.637735: step 7918, loss 0.0168124, acc 0.98
2016-09-06T06:07:39.456904: step 7919, loss 0.0357092, acc 0.96
2016-09-06T06:07:40.260345: step 7920, loss 0.0176525, acc 1
2016-09-06T06:07:41.090967: step 7921, loss 0.0432421, acc 0.98
2016-09-06T06:07:41.884539: step 7922, loss 0.00331589, acc 1
2016-09-06T06:07:42.692551: step 7923, loss 0.0125023, acc 1
2016-09-06T06:07:43.510925: step 7924, loss 0.0199792, acc 1
2016-09-06T06:07:44.330692: step 7925, loss 0.0426639, acc 1
2016-09-06T06:07:45.132488: step 7926, loss 0.0260945, acc 1
2016-09-06T06:07:45.940924: step 7927, loss 0.0629546, acc 0.98
2016-09-06T06:07:46.762610: step 7928, loss 0.0154356, acc 1
2016-09-06T06:07:47.567616: step 7929, loss 0.00246814, acc 1
2016-09-06T06:07:48.369758: step 7930, loss 0.0327479, acc 1
2016-09-06T06:07:49.192987: step 7931, loss 0.00961509, acc 1
2016-09-06T06:07:49.984543: step 7932, loss 0.002417, acc 1
2016-09-06T06:07:50.806278: step 7933, loss 0.0591645, acc 0.96
2016-09-06T06:07:51.637142: step 7934, loss 0.0047646, acc 1
2016-09-06T06:07:52.425420: step 7935, loss 0.00817456, acc 1
2016-09-06T06:07:53.233404: step 7936, loss 0.0231978, acc 1
2016-09-06T06:07:54.051837: step 7937, loss 0.0224879, acc 0.98
2016-09-06T06:07:54.849297: step 7938, loss 0.00383977, acc 1
2016-09-06T06:07:55.664720: step 7939, loss 0.0103084, acc 1
2016-09-06T06:07:56.470673: step 7940, loss 0.0313834, acc 1
2016-09-06T06:07:57.239715: step 7941, loss 0.0163529, acc 0.98
2016-09-06T06:07:58.047516: step 7942, loss 0.0206368, acc 1
2016-09-06T06:07:58.857602: step 7943, loss 0.0126082, acc 1
2016-09-06T06:07:59.632079: step 7944, loss 0.00196704, acc 1
2016-09-06T06:08:00.466873: step 7945, loss 0.00288617, acc 1
2016-09-06T06:08:01.280818: step 7946, loss 0.00261316, acc 1
2016-09-06T06:08:02.077466: step 7947, loss 0.00996973, acc 1
2016-09-06T06:08:02.887688: step 7948, loss 0.0158672, acc 1
2016-09-06T06:08:03.714887: step 7949, loss 0.00624631, acc 1
2016-09-06T06:08:04.501403: step 7950, loss 0.0160479, acc 1
2016-09-06T06:08:05.293392: step 7951, loss 0.0137494, acc 1
2016-09-06T06:08:06.123571: step 7952, loss 0.02006, acc 0.98
2016-09-06T06:08:06.893747: step 7953, loss 0.0277857, acc 0.98
2016-09-06T06:08:07.719318: step 7954, loss 0.00336804, acc 1
2016-09-06T06:08:08.553387: step 7955, loss 0.0646252, acc 0.98
2016-09-06T06:08:09.351921: step 7956, loss 0.00943614, acc 1
2016-09-06T06:08:10.196950: step 7957, loss 0.017116, acc 1
2016-09-06T06:08:11.032188: step 7958, loss 0.0234687, acc 1
2016-09-06T06:08:11.858110: step 7959, loss 0.00312525, acc 1
2016-09-06T06:08:12.651178: step 7960, loss 0.00419845, acc 1
2016-09-06T06:08:13.500024: step 7961, loss 0.048642, acc 0.98
2016-09-06T06:08:14.337204: step 7962, loss 0.00223046, acc 1
2016-09-06T06:08:15.145520: step 7963, loss 0.0149049, acc 1
2016-09-06T06:08:15.996783: step 7964, loss 0.00592512, acc 1
2016-09-06T06:08:16.797054: step 7965, loss 0.0038215, acc 1
2016-09-06T06:08:17.593343: step 7966, loss 0.0285339, acc 1
2016-09-06T06:08:18.432638: step 7967, loss 0.04336, acc 0.98
2016-09-06T06:08:19.247023: step 7968, loss 0.00222809, acc 1
2016-09-06T06:08:20.075904: step 7969, loss 0.016363, acc 1
2016-09-06T06:08:20.901736: step 7970, loss 0.021932, acc 1
2016-09-06T06:08:21.738829: step 7971, loss 0.00383036, acc 1
2016-09-06T06:08:22.512001: step 7972, loss 0.00232622, acc 1
2016-09-06T06:08:23.333084: step 7973, loss 0.0380321, acc 0.96
2016-09-06T06:08:24.147242: step 7974, loss 0.00779069, acc 1
2016-09-06T06:08:24.962970: step 7975, loss 0.0216406, acc 0.98
2016-09-06T06:08:25.783722: step 7976, loss 0.00226917, acc 1
2016-09-06T06:08:26.601257: step 7977, loss 0.0497482, acc 0.98
2016-09-06T06:08:27.392417: step 7978, loss 0.00457732, acc 1
2016-09-06T06:08:28.200544: step 7979, loss 0.0631015, acc 0.96
2016-09-06T06:08:29.002672: step 7980, loss 0.0133941, acc 1
2016-09-06T06:08:29.831692: step 7981, loss 0.0243322, acc 1
2016-09-06T06:08:30.628387: step 7982, loss 0.0960268, acc 0.96
2016-09-06T06:08:31.432077: step 7983, loss 0.00712519, acc 1
2016-09-06T06:08:32.233108: step 7984, loss 0.0476998, acc 0.98
2016-09-06T06:08:33.046303: step 7985, loss 0.00668368, acc 1
2016-09-06T06:08:33.934951: step 7986, loss 0.0172456, acc 0.98
2016-09-06T06:08:34.731201: step 7987, loss 0.0192573, acc 1
2016-09-06T06:08:35.518956: step 7988, loss 0.0285112, acc 0.98
2016-09-06T06:08:36.344984: step 7989, loss 0.012118, acc 1
2016-09-06T06:08:37.133392: step 7990, loss 0.00198716, acc 1
2016-09-06T06:08:37.941130: step 7991, loss 0.00208539, acc 1
2016-09-06T06:08:38.745319: step 7992, loss 0.0153698, acc 1
2016-09-06T06:08:39.523075: step 7993, loss 0.00567892, acc 1
2016-09-06T06:08:40.306486: step 7994, loss 0.0150374, acc 1
2016-09-06T06:08:41.129983: step 7995, loss 0.00195963, acc 1
2016-09-06T06:08:41.912539: step 7996, loss 0.0605698, acc 0.98
2016-09-06T06:08:42.726211: step 7997, loss 0.0124573, acc 1
2016-09-06T06:08:43.550229: step 7998, loss 0.0337096, acc 0.98
2016-09-06T06:08:44.370653: step 7999, loss 0.0270451, acc 1
2016-09-06T06:08:45.188918: step 8000, loss 0.0413982, acc 0.98

Evaluation:
2016-09-06T06:08:48.907949: step 8000, loss 2.30534, acc 0.743902

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-8000

2016-09-06T06:08:50.832427: step 8001, loss 0.00252429, acc 1
2016-09-06T06:08:51.669813: step 8002, loss 0.0852996, acc 0.96
2016-09-06T06:08:52.508353: step 8003, loss 0.0267641, acc 0.98
2016-09-06T06:08:53.320458: step 8004, loss 0.0378164, acc 0.98
2016-09-06T06:08:54.131197: step 8005, loss 0.0360081, acc 0.98
2016-09-06T06:08:54.940092: step 8006, loss 0.0241076, acc 1
2016-09-06T06:08:55.771419: step 8007, loss 0.016492, acc 1
2016-09-06T06:08:56.578002: step 8008, loss 0.00361554, acc 1
2016-09-06T06:08:57.378085: step 8009, loss 0.0018605, acc 1
2016-09-06T06:08:58.221267: step 8010, loss 0.0500901, acc 0.98
2016-09-06T06:08:59.060622: step 8011, loss 0.00645875, acc 1
2016-09-06T06:08:59.870723: step 8012, loss 0.00520143, acc 1
2016-09-06T06:09:00.729823: step 8013, loss 0.0242635, acc 0.98
2016-09-06T06:09:01.565621: step 8014, loss 0.0170106, acc 1
2016-09-06T06:09:02.402782: step 8015, loss 0.00626495, acc 1
2016-09-06T06:09:03.248774: step 8016, loss 0.025571, acc 1
2016-09-06T06:09:04.061988: step 8017, loss 0.0100052, acc 1
2016-09-06T06:09:04.897473: step 8018, loss 0.0630764, acc 0.96
2016-09-06T06:09:05.734910: step 8019, loss 0.0687224, acc 0.96
2016-09-06T06:09:06.560753: step 8020, loss 0.0114285, acc 1
2016-09-06T06:09:07.363142: step 8021, loss 0.0299589, acc 0.98
2016-09-06T06:09:08.162560: step 8022, loss 0.0343551, acc 0.96
2016-09-06T06:09:08.991987: step 8023, loss 0.00308404, acc 1
2016-09-06T06:09:09.822643: step 8024, loss 0.0243243, acc 0.98
2016-09-06T06:09:10.655649: step 8025, loss 0.0492714, acc 0.98
2016-09-06T06:09:11.482340: step 8026, loss 0.00495366, acc 1
2016-09-06T06:09:12.291787: step 8027, loss 0.00819075, acc 1
2016-09-06T06:09:13.092703: step 8028, loss 0.00548219, acc 1
2016-09-06T06:09:13.914190: step 8029, loss 0.0255273, acc 0.98
2016-09-06T06:09:14.738411: step 8030, loss 0.0444827, acc 0.96
2016-09-06T06:09:15.551200: step 8031, loss 0.0127967, acc 1
2016-09-06T06:09:16.389968: step 8032, loss 0.0839023, acc 0.94
2016-09-06T06:09:17.196223: step 8033, loss 0.0699524, acc 0.96
2016-09-06T06:09:18.012329: step 8034, loss 0.0169052, acc 1
2016-09-06T06:09:18.828502: step 8035, loss 0.0131153, acc 1
2016-09-06T06:09:19.652815: step 8036, loss 0.00755837, acc 1
2016-09-06T06:09:20.481100: step 8037, loss 0.00519169, acc 1
2016-09-06T06:09:21.323511: step 8038, loss 0.0290314, acc 0.98
2016-09-06T06:09:22.154456: step 8039, loss 0.0163551, acc 1
2016-09-06T06:09:22.974022: step 8040, loss 0.0211807, acc 1
2016-09-06T06:09:23.808658: step 8041, loss 0.0481306, acc 0.96
2016-09-06T06:09:24.646006: step 8042, loss 0.00797532, acc 1
2016-09-06T06:09:25.443972: step 8043, loss 0.0239698, acc 0.98
2016-09-06T06:09:26.268092: step 8044, loss 0.0227322, acc 0.98
2016-09-06T06:09:27.073090: step 8045, loss 0.0116889, acc 1
2016-09-06T06:09:27.908186: step 8046, loss 0.0469179, acc 0.98
2016-09-06T06:09:28.713191: step 8047, loss 0.00585383, acc 1
2016-09-06T06:09:29.557414: step 8048, loss 0.00329305, acc 1
2016-09-06T06:09:30.351446: step 8049, loss 0.0297368, acc 0.98
2016-09-06T06:09:31.153787: step 8050, loss 0.0100976, acc 1
2016-09-06T06:09:31.984700: step 8051, loss 0.147324, acc 0.98
2016-09-06T06:09:32.768696: step 8052, loss 0.034842, acc 0.98
2016-09-06T06:09:33.596644: step 8053, loss 0.0168752, acc 0.98
2016-09-06T06:09:34.405567: step 8054, loss 0.0586431, acc 0.96
2016-09-06T06:09:35.207990: step 8055, loss 0.00236694, acc 1
2016-09-06T06:09:36.024308: step 8056, loss 0.00299682, acc 1
2016-09-06T06:09:36.917007: step 8057, loss 0.00720606, acc 1
2016-09-06T06:09:37.736735: step 8058, loss 0.0248347, acc 0.98
2016-09-06T06:09:38.547432: step 8059, loss 0.0138294, acc 1
2016-09-06T06:09:39.366025: step 8060, loss 0.00218119, acc 1
2016-09-06T06:09:40.182246: step 8061, loss 0.0308381, acc 0.98
2016-09-06T06:09:40.987407: step 8062, loss 0.0246236, acc 0.98
2016-09-06T06:09:41.827552: step 8063, loss 0.055922, acc 0.98
2016-09-06T06:09:42.573758: step 8064, loss 0.00209789, acc 1
2016-09-06T06:09:43.388464: step 8065, loss 0.0498412, acc 0.96
2016-09-06T06:09:44.223538: step 8066, loss 0.179538, acc 0.96
2016-09-06T06:09:45.056827: step 8067, loss 0.00399514, acc 1
2016-09-06T06:09:45.877230: step 8068, loss 0.0298525, acc 0.98
2016-09-06T06:09:46.703916: step 8069, loss 0.0302114, acc 1
2016-09-06T06:09:47.518803: step 8070, loss 0.0121461, acc 1
2016-09-06T06:09:48.344210: step 8071, loss 0.0483578, acc 0.96
2016-09-06T06:09:49.199871: step 8072, loss 0.0218912, acc 1
2016-09-06T06:09:50.016160: step 8073, loss 0.0479495, acc 0.98
2016-09-06T06:09:50.849150: step 8074, loss 0.0450409, acc 0.96
2016-09-06T06:09:51.665829: step 8075, loss 0.0130428, acc 1
2016-09-06T06:09:52.477242: step 8076, loss 0.0111924, acc 1
2016-09-06T06:09:53.277993: step 8077, loss 0.0219691, acc 0.98
2016-09-06T06:09:54.075537: step 8078, loss 0.0381515, acc 0.98
2016-09-06T06:09:54.898293: step 8079, loss 0.0434929, acc 0.98
2016-09-06T06:09:55.704775: step 8080, loss 0.00591111, acc 1
2016-09-06T06:09:56.496175: step 8081, loss 0.0230576, acc 1
2016-09-06T06:09:57.330168: step 8082, loss 0.0157484, acc 1
2016-09-06T06:09:58.129769: step 8083, loss 0.0203775, acc 0.98
2016-09-06T06:09:58.975171: step 8084, loss 0.00299876, acc 1
2016-09-06T06:09:59.837323: step 8085, loss 0.00248947, acc 1
2016-09-06T06:10:00.680091: step 8086, loss 0.0154005, acc 1
2016-09-06T06:10:01.502194: step 8087, loss 0.004524, acc 1
2016-09-06T06:10:02.345250: step 8088, loss 0.0112324, acc 1
2016-09-06T06:10:03.179228: step 8089, loss 0.00317698, acc 1
2016-09-06T06:10:04.014982: step 8090, loss 0.007654, acc 1
2016-09-06T06:10:04.850677: step 8091, loss 0.00623407, acc 1
2016-09-06T06:10:05.662481: step 8092, loss 0.0131887, acc 1
2016-09-06T06:10:06.463367: step 8093, loss 0.00336522, acc 1
2016-09-06T06:10:07.291692: step 8094, loss 0.00265366, acc 1
2016-09-06T06:10:08.086117: step 8095, loss 0.00325773, acc 1
2016-09-06T06:10:08.899047: step 8096, loss 0.00971134, acc 1
2016-09-06T06:10:09.756291: step 8097, loss 0.00403773, acc 1
2016-09-06T06:10:10.564904: step 8098, loss 0.0217357, acc 1
2016-09-06T06:10:11.375571: step 8099, loss 0.00622022, acc 1
2016-09-06T06:10:12.197066: step 8100, loss 0.00438791, acc 1

Evaluation:
2016-09-06T06:10:15.983317: step 8100, loss 3.17505, acc 0.739212

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-8100

2016-09-06T06:10:17.924273: step 8101, loss 0.00278142, acc 1
2016-09-06T06:10:18.737508: step 8102, loss 0.0281227, acc 1
2016-09-06T06:10:19.560679: step 8103, loss 0.00601071, acc 1
2016-09-06T06:10:20.356314: step 8104, loss 0.00522585, acc 1
2016-09-06T06:10:21.149491: step 8105, loss 0.0240035, acc 0.98
2016-09-06T06:10:21.986548: step 8106, loss 0.0746181, acc 0.98
2016-09-06T06:10:22.822782: step 8107, loss 0.0339594, acc 0.98
2016-09-06T06:10:23.612313: step 8108, loss 0.0190713, acc 0.98
2016-09-06T06:10:24.412100: step 8109, loss 0.0623723, acc 0.98
2016-09-06T06:10:25.235731: step 8110, loss 0.0270135, acc 1
2016-09-06T06:10:26.037436: step 8111, loss 0.0160989, acc 1
2016-09-06T06:10:26.861609: step 8112, loss 0.0163731, acc 1
2016-09-06T06:10:27.709143: step 8113, loss 0.0174778, acc 1
2016-09-06T06:10:28.503813: step 8114, loss 0.00246439, acc 1
2016-09-06T06:10:29.298013: step 8115, loss 0.00596034, acc 1
2016-09-06T06:10:30.096783: step 8116, loss 0.0583401, acc 0.98
2016-09-06T06:10:30.894051: step 8117, loss 0.100496, acc 0.98
2016-09-06T06:10:31.731189: step 8118, loss 0.00242757, acc 1
2016-09-06T06:10:32.553050: step 8119, loss 0.0484864, acc 0.98
2016-09-06T06:10:33.380619: step 8120, loss 0.00703666, acc 1
2016-09-06T06:10:34.212329: step 8121, loss 0.00586083, acc 1
2016-09-06T06:10:35.048763: step 8122, loss 0.0487084, acc 0.96
2016-09-06T06:10:35.875152: step 8123, loss 0.0848449, acc 0.98
2016-09-06T06:10:36.711827: step 8124, loss 0.00234669, acc 1
2016-09-06T06:10:37.547150: step 8125, loss 0.0524888, acc 0.98
2016-09-06T06:10:38.397572: step 8126, loss 0.0183783, acc 0.98
2016-09-06T06:10:39.217126: step 8127, loss 0.00191774, acc 1
2016-09-06T06:10:40.057518: step 8128, loss 0.0281034, acc 0.98
2016-09-06T06:10:40.899420: step 8129, loss 0.0168024, acc 0.98
2016-09-06T06:10:41.725440: step 8130, loss 0.0166543, acc 1
2016-09-06T06:10:42.532657: step 8131, loss 0.025079, acc 0.98
2016-09-06T06:10:43.352560: step 8132, loss 0.0211276, acc 1
2016-09-06T06:10:44.132209: step 8133, loss 0.0145933, acc 1
2016-09-06T06:10:44.945281: step 8134, loss 0.0416043, acc 0.98
2016-09-06T06:10:45.753441: step 8135, loss 0.0463423, acc 0.98
2016-09-06T06:10:46.570400: step 8136, loss 0.0034323, acc 1
2016-09-06T06:10:47.372447: step 8137, loss 0.0278418, acc 0.98
2016-09-06T06:10:48.197776: step 8138, loss 0.135669, acc 0.98
2016-09-06T06:10:49.000072: step 8139, loss 0.0131815, acc 1
2016-09-06T06:10:49.804270: step 8140, loss 0.0190318, acc 1
2016-09-06T06:10:50.632905: step 8141, loss 0.00277413, acc 1
2016-09-06T06:10:51.420342: step 8142, loss 0.0550339, acc 0.96
2016-09-06T06:10:52.213333: step 8143, loss 0.00755211, acc 1
2016-09-06T06:10:53.032175: step 8144, loss 0.00294447, acc 1
2016-09-06T06:10:53.819893: step 8145, loss 0.00339535, acc 1
2016-09-06T06:10:54.618231: step 8146, loss 0.113682, acc 0.98
2016-09-06T06:10:55.454510: step 8147, loss 0.0171618, acc 1
2016-09-06T06:10:56.262163: step 8148, loss 0.0117513, acc 1
2016-09-06T06:10:57.081485: step 8149, loss 0.0101337, acc 1
2016-09-06T06:10:57.911642: step 8150, loss 0.00984728, acc 1
2016-09-06T06:10:58.689405: step 8151, loss 0.00715548, acc 1
2016-09-06T06:10:59.488122: step 8152, loss 0.0218262, acc 1
2016-09-06T06:11:00.306741: step 8153, loss 0.00814062, acc 1
2016-09-06T06:11:01.104324: step 8154, loss 0.00370569, acc 1
2016-09-06T06:11:01.911119: step 8155, loss 0.0338737, acc 0.98
2016-09-06T06:11:02.732450: step 8156, loss 0.00415364, acc 1
2016-09-06T06:11:03.521714: step 8157, loss 0.00431888, acc 1
2016-09-06T06:11:04.333638: step 8158, loss 0.0123248, acc 1
2016-09-06T06:11:05.163565: step 8159, loss 0.0112398, acc 1
2016-09-06T06:11:05.960850: step 8160, loss 0.00951374, acc 1
2016-09-06T06:11:06.773446: step 8161, loss 0.00294712, acc 1
2016-09-06T06:11:07.586958: step 8162, loss 0.0268591, acc 1
2016-09-06T06:11:08.368859: step 8163, loss 0.0038244, acc 1
2016-09-06T06:11:09.197493: step 8164, loss 0.0184002, acc 1
2016-09-06T06:11:10.013826: step 8165, loss 0.0165527, acc 1
2016-09-06T06:11:10.787744: step 8166, loss 0.0195922, acc 0.98
2016-09-06T06:11:11.587597: step 8167, loss 0.0260177, acc 0.98
2016-09-06T06:11:12.412975: step 8168, loss 0.00788508, acc 1
2016-09-06T06:11:13.178522: step 8169, loss 0.0145078, acc 1
2016-09-06T06:11:13.968671: step 8170, loss 0.00322254, acc 1
2016-09-06T06:11:14.784151: step 8171, loss 0.0195743, acc 1
2016-09-06T06:11:15.583475: step 8172, loss 0.0426593, acc 0.96
2016-09-06T06:11:16.393399: step 8173, loss 0.166948, acc 0.94
2016-09-06T06:11:17.216998: step 8174, loss 0.0414522, acc 0.98
2016-09-06T06:11:18.020610: step 8175, loss 0.00340364, acc 1
2016-09-06T06:11:18.820063: step 8176, loss 0.0349746, acc 1
2016-09-06T06:11:19.627386: step 8177, loss 0.0206329, acc 0.98
2016-09-06T06:11:20.421003: step 8178, loss 0.0247516, acc 0.98
2016-09-06T06:11:21.235245: step 8179, loss 0.0277072, acc 0.98
2016-09-06T06:11:22.044631: step 8180, loss 0.016119, acc 1
2016-09-06T06:11:22.838307: step 8181, loss 0.0698261, acc 0.96
2016-09-06T06:11:23.627078: step 8182, loss 0.00398915, acc 1
2016-09-06T06:11:24.441842: step 8183, loss 0.0978042, acc 0.98
2016-09-06T06:11:25.220130: step 8184, loss 0.00730344, acc 1
2016-09-06T06:11:26.039601: step 8185, loss 0.00339826, acc 1
2016-09-06T06:11:26.829480: step 8186, loss 0.00646462, acc 1
2016-09-06T06:11:27.609397: step 8187, loss 0.00466708, acc 1
2016-09-06T06:11:28.429438: step 8188, loss 0.0456859, acc 0.98
2016-09-06T06:11:29.245073: step 8189, loss 0.043027, acc 0.98
2016-09-06T06:11:30.053768: step 8190, loss 0.0287359, acc 0.98
2016-09-06T06:11:30.857698: step 8191, loss 0.0205897, acc 0.98
2016-09-06T06:11:31.649405: step 8192, loss 0.037641, acc 0.98
2016-09-06T06:11:32.424916: step 8193, loss 0.00442264, acc 1
2016-09-06T06:11:33.269561: step 8194, loss 0.0131925, acc 1
2016-09-06T06:11:34.090959: step 8195, loss 0.00417564, acc 1
2016-09-06T06:11:34.925234: step 8196, loss 0.00677676, acc 1
2016-09-06T06:11:35.727076: step 8197, loss 0.00397666, acc 1
2016-09-06T06:11:36.552585: step 8198, loss 0.0850853, acc 0.94
2016-09-06T06:11:37.355563: step 8199, loss 0.0254845, acc 0.98
2016-09-06T06:11:38.163882: step 8200, loss 0.0607949, acc 0.98

Evaluation:
2016-09-06T06:11:41.918939: step 8200, loss 2.31166, acc 0.731707

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-8200

2016-09-06T06:11:43.779692: step 8201, loss 0.00450533, acc 1
2016-09-06T06:11:44.595089: step 8202, loss 0.0396177, acc 0.96
2016-09-06T06:11:45.420901: step 8203, loss 0.00344581, acc 1
2016-09-06T06:11:46.238825: step 8204, loss 0.00555611, acc 1
2016-09-06T06:11:47.058036: step 8205, loss 0.0136806, acc 1
2016-09-06T06:11:47.856426: step 8206, loss 0.0149412, acc 1
2016-09-06T06:11:48.698573: step 8207, loss 0.028083, acc 0.98
2016-09-06T06:11:49.474089: step 8208, loss 0.0749805, acc 0.98
2016-09-06T06:11:50.286034: step 8209, loss 0.0136507, acc 1
2016-09-06T06:11:51.127572: step 8210, loss 0.0174588, acc 1
2016-09-06T06:11:51.945838: step 8211, loss 0.0170603, acc 1
2016-09-06T06:11:52.755869: step 8212, loss 0.0471787, acc 0.96
2016-09-06T06:11:53.574963: step 8213, loss 0.0400197, acc 0.98
2016-09-06T06:11:54.381647: step 8214, loss 0.0253475, acc 1
2016-09-06T06:11:55.189281: step 8215, loss 0.00386491, acc 1
2016-09-06T06:11:56.016915: step 8216, loss 0.00429029, acc 1
2016-09-06T06:11:56.840138: step 8217, loss 0.0186139, acc 0.98
2016-09-06T06:11:57.653447: step 8218, loss 0.0148274, acc 1
2016-09-06T06:11:58.495326: step 8219, loss 0.0614997, acc 0.98
2016-09-06T06:11:59.329934: step 8220, loss 0.0253134, acc 1
2016-09-06T06:12:00.177034: step 8221, loss 0.0484225, acc 0.98
2016-09-06T06:12:01.034731: step 8222, loss 0.0163101, acc 1
2016-09-06T06:12:01.871585: step 8223, loss 0.0299099, acc 0.98
2016-09-06T06:12:02.692543: step 8224, loss 0.0339634, acc 0.98
2016-09-06T06:12:03.525379: step 8225, loss 0.0442011, acc 0.98
2016-09-06T06:12:04.341095: step 8226, loss 0.00713094, acc 1
2016-09-06T06:12:05.148432: step 8227, loss 0.00454531, acc 1
2016-09-06T06:12:05.995134: step 8228, loss 0.00365664, acc 1
2016-09-06T06:12:06.827936: step 8229, loss 0.0237016, acc 0.98
2016-09-06T06:12:07.631141: step 8230, loss 0.00710547, acc 1
2016-09-06T06:12:08.453662: step 8231, loss 0.0230982, acc 0.98
2016-09-06T06:12:09.313191: step 8232, loss 0.0291447, acc 0.98
2016-09-06T06:12:10.141573: step 8233, loss 0.014259, acc 1
2016-09-06T06:12:10.957022: step 8234, loss 0.00409492, acc 1
2016-09-06T06:12:11.772738: step 8235, loss 0.0236875, acc 1
2016-09-06T06:12:12.612456: step 8236, loss 0.00777703, acc 1
2016-09-06T06:12:13.407376: step 8237, loss 0.00660875, acc 1
2016-09-06T06:12:14.226153: step 8238, loss 0.003444, acc 1
2016-09-06T06:12:15.027176: step 8239, loss 0.0698541, acc 0.94
2016-09-06T06:12:15.823968: step 8240, loss 0.0038578, acc 1
2016-09-06T06:12:16.651227: step 8241, loss 0.00489886, acc 1
2016-09-06T06:12:17.450971: step 8242, loss 0.0764772, acc 0.98
2016-09-06T06:12:18.273459: step 8243, loss 0.0227204, acc 0.98
2016-09-06T06:12:19.117439: step 8244, loss 0.0135298, acc 1
2016-09-06T06:12:19.925080: step 8245, loss 0.00333598, acc 1
2016-09-06T06:12:20.708475: step 8246, loss 0.0068722, acc 1
2016-09-06T06:12:21.566777: step 8247, loss 0.015856, acc 1
2016-09-06T06:12:22.378891: step 8248, loss 0.00979114, acc 1
2016-09-06T06:12:23.200633: step 8249, loss 0.00587965, acc 1
2016-09-06T06:12:24.056852: step 8250, loss 0.0332068, acc 0.98
2016-09-06T06:12:24.876165: step 8251, loss 0.0603095, acc 0.98
2016-09-06T06:12:25.674715: step 8252, loss 0.00420966, acc 1
2016-09-06T06:12:26.482263: step 8253, loss 0.020523, acc 1
2016-09-06T06:12:27.284603: step 8254, loss 0.0324759, acc 0.98
2016-09-06T06:12:28.088680: step 8255, loss 0.00575978, acc 1
2016-09-06T06:12:28.854824: step 8256, loss 0.0410398, acc 0.977273
2016-09-06T06:12:29.703618: step 8257, loss 0.0315963, acc 0.98
2016-09-06T06:12:30.500989: step 8258, loss 0.0912989, acc 0.98
2016-09-06T06:12:31.313284: step 8259, loss 0.00280119, acc 1
2016-09-06T06:12:32.155262: step 8260, loss 0.024531, acc 0.98
2016-09-06T06:12:32.957308: step 8261, loss 0.0526395, acc 0.96
2016-09-06T06:12:33.756242: step 8262, loss 0.106426, acc 0.96
2016-09-06T06:12:34.569885: step 8263, loss 0.0198159, acc 0.98
2016-09-06T06:12:35.366858: step 8264, loss 0.0173881, acc 1
2016-09-06T06:12:36.210228: step 8265, loss 0.0307916, acc 0.98
2016-09-06T06:12:37.042553: step 8266, loss 0.0289261, acc 0.98
2016-09-06T06:12:37.860092: step 8267, loss 0.0826608, acc 0.98
2016-09-06T06:12:38.695756: step 8268, loss 0.00388991, acc 1
2016-09-06T06:12:39.502524: step 8269, loss 0.0213048, acc 0.98
2016-09-06T06:12:40.345608: step 8270, loss 0.0355997, acc 0.98
2016-09-06T06:12:41.174282: step 8271, loss 0.0298368, acc 0.98
2016-09-06T06:12:41.986669: step 8272, loss 0.0077648, acc 1
2016-09-06T06:12:42.815477: step 8273, loss 0.0300421, acc 0.98
2016-09-06T06:12:43.631215: step 8274, loss 0.0031527, acc 1
2016-09-06T06:12:44.470424: step 8275, loss 0.040207, acc 0.98
2016-09-06T06:12:45.308269: step 8276, loss 0.00382137, acc 1
2016-09-06T06:12:46.111387: step 8277, loss 0.005222, acc 1
2016-09-06T06:12:46.944785: step 8278, loss 0.0343664, acc 1
2016-09-06T06:12:47.783144: step 8279, loss 0.029272, acc 1
2016-09-06T06:12:48.613455: step 8280, loss 0.0228682, acc 0.98
2016-09-06T06:12:49.445016: step 8281, loss 0.00478265, acc 1
2016-09-06T06:12:50.270708: step 8282, loss 0.0305841, acc 1
2016-09-06T06:12:51.066468: step 8283, loss 0.00497854, acc 1
2016-09-06T06:12:51.898565: step 8284, loss 0.0361933, acc 0.98
2016-09-06T06:12:52.720198: step 8285, loss 0.097155, acc 0.94
2016-09-06T06:12:53.519677: step 8286, loss 0.0125007, acc 1
2016-09-06T06:12:54.307284: step 8287, loss 0.039819, acc 0.98
2016-09-06T06:12:55.149392: step 8288, loss 0.00466607, acc 1
2016-09-06T06:12:55.939354: step 8289, loss 0.00432813, acc 1
2016-09-06T06:12:56.731921: step 8290, loss 0.00514846, acc 1
2016-09-06T06:12:57.561048: step 8291, loss 0.00308518, acc 1
2016-09-06T06:12:58.346788: step 8292, loss 0.0032561, acc 1
2016-09-06T06:12:59.161035: step 8293, loss 0.00316983, acc 1
2016-09-06T06:12:59.975928: step 8294, loss 0.0282332, acc 1
2016-09-06T06:13:00.797509: step 8295, loss 0.0126033, acc 1
2016-09-06T06:13:01.588001: step 8296, loss 0.00312371, acc 1
2016-09-06T06:13:02.413361: step 8297, loss 0.0346687, acc 0.98
2016-09-06T06:13:03.183400: step 8298, loss 0.00687911, acc 1
2016-09-06T06:13:03.973413: step 8299, loss 0.0316665, acc 1
2016-09-06T06:13:04.799441: step 8300, loss 0.0237562, acc 0.98

Evaluation:
2016-09-06T06:13:08.557208: step 8300, loss 2.6341, acc 0.745779

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-8300

2016-09-06T06:13:10.426910: step 8301, loss 0.0446017, acc 0.98
2016-09-06T06:13:11.245980: step 8302, loss 0.0345896, acc 0.98
2016-09-06T06:13:12.041549: step 8303, loss 0.0257018, acc 0.98
2016-09-06T06:13:12.919328: step 8304, loss 0.00499281, acc 1
2016-09-06T06:13:13.725932: step 8305, loss 0.0759085, acc 0.96
2016-09-06T06:13:14.565379: step 8306, loss 0.00292522, acc 1
2016-09-06T06:13:15.356454: step 8307, loss 0.0161166, acc 1
2016-09-06T06:13:16.160837: step 8308, loss 0.0379644, acc 0.98
2016-09-06T06:13:16.964472: step 8309, loss 0.0306894, acc 0.98
2016-09-06T06:13:17.740985: step 8310, loss 0.00686487, acc 1
2016-09-06T06:13:18.532981: step 8311, loss 0.00584998, acc 1
2016-09-06T06:13:19.351302: step 8312, loss 0.00728807, acc 1
2016-09-06T06:13:20.159155: step 8313, loss 0.00391536, acc 1
2016-09-06T06:13:20.971789: step 8314, loss 0.00812445, acc 1
2016-09-06T06:13:21.779081: step 8315, loss 0.00415284, acc 1
2016-09-06T06:13:22.566658: step 8316, loss 0.00426287, acc 1
2016-09-06T06:13:23.400861: step 8317, loss 0.0158936, acc 1
2016-09-06T06:13:24.243903: step 8318, loss 0.0217761, acc 0.98
2016-09-06T06:13:25.028634: step 8319, loss 0.00312025, acc 1
2016-09-06T06:13:25.804501: step 8320, loss 0.0266302, acc 0.98
2016-09-06T06:13:26.637025: step 8321, loss 0.00328386, acc 1
2016-09-06T06:13:27.429479: step 8322, loss 0.0050157, acc 1
2016-09-06T06:13:28.214382: step 8323, loss 0.0148684, acc 1
2016-09-06T06:13:29.041723: step 8324, loss 0.0651176, acc 0.98
2016-09-06T06:13:29.816212: step 8325, loss 0.0173387, acc 1
2016-09-06T06:13:30.623420: step 8326, loss 0.00534705, acc 1
2016-09-06T06:13:31.434363: step 8327, loss 0.0199013, acc 1
2016-09-06T06:13:32.244205: step 8328, loss 0.02233, acc 0.98
2016-09-06T06:13:33.071221: step 8329, loss 0.012148, acc 1
2016-09-06T06:13:33.877446: step 8330, loss 0.007284, acc 1
2016-09-06T06:13:34.673512: step 8331, loss 0.0592748, acc 0.96
2016-09-06T06:13:35.452592: step 8332, loss 0.0171714, acc 0.98
2016-09-06T06:13:36.282461: step 8333, loss 0.0340334, acc 0.96
2016-09-06T06:13:37.063360: step 8334, loss 0.0510897, acc 0.98
2016-09-06T06:13:37.885274: step 8335, loss 0.0606644, acc 0.96
2016-09-06T06:13:38.709450: step 8336, loss 0.0330514, acc 0.98
2016-09-06T06:13:39.493434: step 8337, loss 0.0131075, acc 1
2016-09-06T06:13:40.285133: step 8338, loss 0.00314538, acc 1
2016-09-06T06:13:41.126742: step 8339, loss 0.0291414, acc 1
2016-09-06T06:13:41.915020: step 8340, loss 0.0133191, acc 1
2016-09-06T06:13:42.738641: step 8341, loss 0.0381822, acc 0.98
2016-09-06T06:13:43.561906: step 8342, loss 0.0217033, acc 0.98
2016-09-06T06:13:44.383064: step 8343, loss 0.0031594, acc 1
2016-09-06T06:13:45.199285: step 8344, loss 0.0098442, acc 1
2016-09-06T06:13:46.016635: step 8345, loss 0.0306752, acc 0.98
2016-09-06T06:13:46.807742: step 8346, loss 0.00268228, acc 1
2016-09-06T06:13:47.613462: step 8347, loss 0.0230762, acc 0.98
2016-09-06T06:13:48.447749: step 8348, loss 0.0072012, acc 1
2016-09-06T06:13:49.250352: step 8349, loss 0.00269097, acc 1
2016-09-06T06:13:50.063293: step 8350, loss 0.0468369, acc 0.96
2016-09-06T06:13:50.905085: step 8351, loss 0.025707, acc 0.98
2016-09-06T06:13:51.711613: step 8352, loss 0.00804647, acc 1
2016-09-06T06:13:52.523611: step 8353, loss 0.0085593, acc 1
2016-09-06T06:13:53.373146: step 8354, loss 0.0165213, acc 1
2016-09-06T06:13:54.208733: step 8355, loss 0.0512788, acc 0.98
2016-09-06T06:13:55.021461: step 8356, loss 0.0220284, acc 0.98
2016-09-06T06:13:55.854185: step 8357, loss 0.0467941, acc 0.98
2016-09-06T06:13:56.705761: step 8358, loss 0.00971532, acc 1
2016-09-06T06:13:57.517922: step 8359, loss 0.0167615, acc 1
2016-09-06T06:13:58.356369: step 8360, loss 0.00429057, acc 1
2016-09-06T06:13:59.148051: step 8361, loss 0.0180589, acc 0.98
2016-09-06T06:13:59.932827: step 8362, loss 0.00293346, acc 1
2016-09-06T06:14:00.782398: step 8363, loss 0.0403138, acc 0.98
2016-09-06T06:14:01.622633: step 8364, loss 0.0180039, acc 0.98
2016-09-06T06:14:02.429704: step 8365, loss 0.0156869, acc 1
2016-09-06T06:14:03.243313: step 8366, loss 0.0332356, acc 1
2016-09-06T06:14:04.090932: step 8367, loss 0.00700051, acc 1
2016-09-06T06:14:04.893518: step 8368, loss 0.0250958, acc 0.98
2016-09-06T06:14:05.694996: step 8369, loss 0.0139828, acc 1
2016-09-06T06:14:06.514308: step 8370, loss 0.00257581, acc 1
2016-09-06T06:14:07.272546: step 8371, loss 0.0225833, acc 1
2016-09-06T06:14:08.108822: step 8372, loss 0.0244022, acc 0.98
2016-09-06T06:14:08.941415: step 8373, loss 0.00242534, acc 1
2016-09-06T06:14:09.741497: step 8374, loss 0.00478823, acc 1
2016-09-06T06:14:10.552731: step 8375, loss 0.0125122, acc 1
2016-09-06T06:14:11.361911: step 8376, loss 0.0329104, acc 0.98
2016-09-06T06:14:12.160977: step 8377, loss 0.00235461, acc 1
2016-09-06T06:14:12.965617: step 8378, loss 0.00684821, acc 1
2016-09-06T06:14:13.778722: step 8379, loss 0.00913443, acc 1
2016-09-06T06:14:14.565811: step 8380, loss 0.0100599, acc 1
2016-09-06T06:14:15.357219: step 8381, loss 0.0455245, acc 0.98
2016-09-06T06:14:16.177367: step 8382, loss 0.00389739, acc 1
2016-09-06T06:14:16.944955: step 8383, loss 0.00741099, acc 1
2016-09-06T06:14:17.751375: step 8384, loss 0.00374317, acc 1
2016-09-06T06:14:18.566505: step 8385, loss 0.00769166, acc 1
2016-09-06T06:14:19.342106: step 8386, loss 0.0185685, acc 0.98
2016-09-06T06:14:20.155399: step 8387, loss 0.00652488, acc 1
2016-09-06T06:14:20.966173: step 8388, loss 0.0954629, acc 0.96
2016-09-06T06:14:21.815656: step 8389, loss 0.0573702, acc 0.98
2016-09-06T06:14:22.609381: step 8390, loss 0.00266743, acc 1
2016-09-06T06:14:23.409436: step 8391, loss 0.0146353, acc 1
2016-09-06T06:14:24.196280: step 8392, loss 0.0233421, acc 0.98
2016-09-06T06:14:25.033586: step 8393, loss 0.00287171, acc 1
2016-09-06T06:14:25.857353: step 8394, loss 0.154294, acc 0.96
2016-09-06T06:14:26.646000: step 8395, loss 0.00208877, acc 1
2016-09-06T06:14:27.423743: step 8396, loss 0.00316377, acc 1
2016-09-06T06:14:28.242342: step 8397, loss 0.0764275, acc 0.96
2016-09-06T06:14:29.005403: step 8398, loss 0.10825, acc 0.98
2016-09-06T06:14:29.826403: step 8399, loss 0.00730053, acc 1
2016-09-06T06:14:30.635696: step 8400, loss 0.0342452, acc 0.98

Evaluation:
2016-09-06T06:14:34.351419: step 8400, loss 1.93823, acc 0.736398

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-8400

2016-09-06T06:14:36.119706: step 8401, loss 0.0108666, acc 1
2016-09-06T06:14:36.986662: step 8402, loss 0.0172515, acc 1
2016-09-06T06:14:37.811554: step 8403, loss 0.00684107, acc 1
2016-09-06T06:14:38.607560: step 8404, loss 0.0108237, acc 1
2016-09-06T06:14:39.430235: step 8405, loss 0.0575151, acc 0.98
2016-09-06T06:14:40.243024: step 8406, loss 0.0325113, acc 1
2016-09-06T06:14:41.049861: step 8407, loss 0.00809676, acc 1
2016-09-06T06:14:41.905152: step 8408, loss 0.0215503, acc 1
2016-09-06T06:14:42.766428: step 8409, loss 0.00819949, acc 1
2016-09-06T06:14:43.554911: step 8410, loss 0.02206, acc 0.98
2016-09-06T06:14:44.341815: step 8411, loss 0.00834411, acc 1
2016-09-06T06:14:45.188830: step 8412, loss 0.0283425, acc 1
2016-09-06T06:14:45.997308: step 8413, loss 0.0123236, acc 1
2016-09-06T06:14:46.820436: step 8414, loss 0.0328478, acc 0.98
2016-09-06T06:14:47.631935: step 8415, loss 0.0208027, acc 1
2016-09-06T06:14:48.397302: step 8416, loss 0.0313695, acc 0.98
2016-09-06T06:14:49.215655: step 8417, loss 0.0354316, acc 0.96
2016-09-06T06:14:50.051643: step 8418, loss 0.0226535, acc 0.98
2016-09-06T06:14:50.845450: step 8419, loss 0.00454125, acc 1
2016-09-06T06:14:51.641462: step 8420, loss 0.00639266, acc 1
2016-09-06T06:14:52.470448: step 8421, loss 0.0105948, acc 1
2016-09-06T06:14:53.281848: step 8422, loss 0.0182739, acc 0.98
2016-09-06T06:14:54.077133: step 8423, loss 0.0207947, acc 1
2016-09-06T06:14:54.888855: step 8424, loss 0.0858786, acc 0.96
2016-09-06T06:14:55.708515: step 8425, loss 0.0477582, acc 0.98
2016-09-06T06:14:56.520165: step 8426, loss 0.00707044, acc 1
2016-09-06T06:14:57.326999: step 8427, loss 0.0520047, acc 0.98
2016-09-06T06:14:58.116217: step 8428, loss 0.0118698, acc 1
2016-09-06T06:14:58.907774: step 8429, loss 0.00336269, acc 1
2016-09-06T06:14:59.735278: step 8430, loss 0.0201233, acc 0.98
2016-09-06T06:15:00.562678: step 8431, loss 0.138291, acc 0.94
2016-09-06T06:15:01.348982: step 8432, loss 0.0837172, acc 0.96
2016-09-06T06:15:02.318206: step 8433, loss 0.0632644, acc 0.96
2016-09-06T06:15:03.127034: step 8434, loss 0.0317203, acc 1
2016-09-06T06:15:03.938627: step 8435, loss 0.0146396, acc 1
2016-09-06T06:15:04.795720: step 8436, loss 0.0126005, acc 1
2016-09-06T06:15:05.620622: step 8437, loss 0.00808964, acc 1
2016-09-06T06:15:06.445457: step 8438, loss 0.0303212, acc 1
2016-09-06T06:15:07.491855: step 8439, loss 0.00324125, acc 1
2016-09-06T06:15:08.321162: step 8440, loss 0.0252351, acc 1
2016-09-06T06:15:09.165694: step 8441, loss 0.0171335, acc 1
2016-09-06T06:15:09.986896: step 8442, loss 0.0180411, acc 0.98
2016-09-06T06:15:10.801700: step 8443, loss 0.0241588, acc 1
2016-09-06T06:15:11.633150: step 8444, loss 0.0722329, acc 0.98
2016-09-06T06:15:12.475191: step 8445, loss 0.0305038, acc 0.98
2016-09-06T06:15:13.319479: step 8446, loss 0.0409854, acc 0.98
2016-09-06T06:15:14.121719: step 8447, loss 0.0229099, acc 0.98
2016-09-06T06:15:14.866484: step 8448, loss 0.00306187, acc 1
2016-09-06T06:15:15.709415: step 8449, loss 0.00795553, acc 1
2016-09-06T06:15:16.532269: step 8450, loss 0.0473873, acc 0.96
2016-09-06T06:15:17.319166: step 8451, loss 0.01257, acc 1
2016-09-06T06:15:18.125472: step 8452, loss 0.0094232, acc 1
2016-09-06T06:15:18.954859: step 8453, loss 0.00835465, acc 1
2016-09-06T06:15:19.773401: step 8454, loss 0.0138453, acc 1
2016-09-06T06:15:20.599737: step 8455, loss 0.00917539, acc 1
2016-09-06T06:15:21.404515: step 8456, loss 0.0191009, acc 1
2016-09-06T06:15:22.172778: step 8457, loss 0.0303221, acc 0.98
2016-09-06T06:15:23.003159: step 8458, loss 0.0181961, acc 1
2016-09-06T06:15:23.799001: step 8459, loss 0.0155137, acc 1
2016-09-06T06:15:24.598939: step 8460, loss 0.0273943, acc 0.98
2016-09-06T06:15:25.429190: step 8461, loss 0.0813007, acc 0.94
2016-09-06T06:15:26.260585: step 8462, loss 0.015791, acc 1
2016-09-06T06:15:27.075833: step 8463, loss 0.0206615, acc 1
2016-09-06T06:15:27.886931: step 8464, loss 0.0152595, acc 1
2016-09-06T06:15:28.690330: step 8465, loss 0.00645258, acc 1
2016-09-06T06:15:29.506631: step 8466, loss 0.00302404, acc 1
2016-09-06T06:15:30.347808: step 8467, loss 0.00652829, acc 1
2016-09-06T06:15:31.155663: step 8468, loss 0.0164237, acc 1
2016-09-06T06:15:31.932921: step 8469, loss 0.00346784, acc 1
2016-09-06T06:15:32.716394: step 8470, loss 0.0199761, acc 1
2016-09-06T06:15:33.524451: step 8471, loss 0.0271515, acc 0.98
2016-09-06T06:15:34.296172: step 8472, loss 0.261387, acc 0.98
2016-09-06T06:15:35.098652: step 8473, loss 0.0439496, acc 0.98
2016-09-06T06:15:35.948601: step 8474, loss 0.0298313, acc 1
2016-09-06T06:15:36.747766: step 8475, loss 0.0139261, acc 1
2016-09-06T06:15:37.567058: step 8476, loss 0.00918754, acc 1
2016-09-06T06:15:38.400627: step 8477, loss 0.0172149, acc 1
2016-09-06T06:15:39.200703: step 8478, loss 0.0153953, acc 1
2016-09-06T06:15:40.001580: step 8479, loss 0.0125783, acc 1
2016-09-06T06:15:40.836865: step 8480, loss 0.0120868, acc 1
2016-09-06T06:15:41.646998: step 8481, loss 0.00797355, acc 1
2016-09-06T06:15:42.453874: step 8482, loss 0.0149733, acc 1
2016-09-06T06:15:43.269434: step 8483, loss 0.00571872, acc 1
2016-09-06T06:15:44.079102: step 8484, loss 0.0159894, acc 1
2016-09-06T06:15:44.901112: step 8485, loss 0.0114076, acc 1
2016-09-06T06:15:45.757570: step 8486, loss 0.0313393, acc 0.98
2016-09-06T06:15:46.561859: step 8487, loss 0.178293, acc 0.98
2016-09-06T06:15:47.386200: step 8488, loss 0.00504113, acc 1
2016-09-06T06:15:48.220735: step 8489, loss 0.0147708, acc 1
2016-09-06T06:15:49.022485: step 8490, loss 0.00631554, acc 1
2016-09-06T06:15:49.836608: step 8491, loss 0.0985688, acc 0.94
2016-09-06T06:15:50.670190: step 8492, loss 0.00271362, acc 1
2016-09-06T06:15:51.484026: step 8493, loss 0.0141632, acc 1
2016-09-06T06:15:52.279528: step 8494, loss 0.117523, acc 0.96
2016-09-06T06:15:53.093909: step 8495, loss 0.0197997, acc 0.98
2016-09-06T06:15:53.917128: step 8496, loss 0.0235068, acc 0.98
2016-09-06T06:15:54.723921: step 8497, loss 0.0326096, acc 0.98
2016-09-06T06:15:55.543550: step 8498, loss 0.015363, acc 1
2016-09-06T06:15:56.364256: step 8499, loss 0.00822329, acc 1
2016-09-06T06:15:57.159612: step 8500, loss 0.0128608, acc 1

Evaluation:
2016-09-06T06:16:00.884098: step 8500, loss 1.50205, acc 0.743902

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-8500

2016-09-06T06:16:02.807526: step 8501, loss 0.035526, acc 0.98
2016-09-06T06:16:03.622893: step 8502, loss 0.0323412, acc 0.98
2016-09-06T06:16:04.438566: step 8503, loss 0.0287113, acc 0.98
2016-09-06T06:16:05.264082: step 8504, loss 0.0287777, acc 0.98
2016-09-06T06:16:06.080942: step 8505, loss 0.0129402, acc 1
2016-09-06T06:16:06.899802: step 8506, loss 0.00411731, acc 1
2016-09-06T06:16:07.740942: step 8507, loss 0.0388976, acc 0.98
2016-09-06T06:16:08.576317: step 8508, loss 0.00285644, acc 1
2016-09-06T06:16:09.392980: step 8509, loss 0.0305858, acc 1
2016-09-06T06:16:10.213812: step 8510, loss 0.0530286, acc 0.98
2016-09-06T06:16:11.018792: step 8511, loss 0.0228952, acc 0.98
2016-09-06T06:16:11.832366: step 8512, loss 0.040054, acc 0.98
2016-09-06T06:16:12.683428: step 8513, loss 0.00983629, acc 1
2016-09-06T06:16:13.478664: step 8514, loss 0.0144194, acc 1
2016-09-06T06:16:14.290405: step 8515, loss 0.0152835, acc 1
2016-09-06T06:16:15.115026: step 8516, loss 0.0468579, acc 0.96
2016-09-06T06:16:15.917956: step 8517, loss 0.0227929, acc 1
2016-09-06T06:16:16.708818: step 8518, loss 0.00513284, acc 1
2016-09-06T06:16:17.522778: step 8519, loss 0.0165908, acc 1
2016-09-06T06:16:18.346902: step 8520, loss 0.00259083, acc 1
2016-09-06T06:16:19.131182: step 8521, loss 0.0113226, acc 1
2016-09-06T06:16:19.955570: step 8522, loss 0.00939366, acc 1
2016-09-06T06:16:20.775461: step 8523, loss 0.0229782, acc 0.98
2016-09-06T06:16:21.581933: step 8524, loss 0.0239535, acc 1
2016-09-06T06:16:22.383803: step 8525, loss 0.0218299, acc 0.98
2016-09-06T06:16:23.196227: step 8526, loss 0.0656541, acc 0.98
2016-09-06T06:16:23.977156: step 8527, loss 0.0037924, acc 1
2016-09-06T06:16:24.788983: step 8528, loss 0.0284933, acc 0.98
2016-09-06T06:16:25.597615: step 8529, loss 0.00261532, acc 1
2016-09-06T06:16:26.393649: step 8530, loss 0.016083, acc 1
2016-09-06T06:16:27.206971: step 8531, loss 0.0366448, acc 0.98
2016-09-06T06:16:28.047371: step 8532, loss 0.00791862, acc 1
2016-09-06T06:16:28.842478: step 8533, loss 0.00513714, acc 1
2016-09-06T06:16:29.617092: step 8534, loss 0.0331456, acc 0.96
2016-09-06T06:16:30.444268: step 8535, loss 0.00274005, acc 1
2016-09-06T06:16:31.277397: step 8536, loss 0.00443344, acc 1
2016-09-06T06:16:32.095627: step 8537, loss 0.0543061, acc 0.94
2016-09-06T06:16:32.907282: step 8538, loss 0.0169043, acc 0.98
2016-09-06T06:16:33.681428: step 8539, loss 0.0112694, acc 1
2016-09-06T06:16:34.475143: step 8540, loss 0.0183495, acc 0.98
2016-09-06T06:16:35.328713: step 8541, loss 0.0525247, acc 0.96
2016-09-06T06:16:36.102029: step 8542, loss 0.00354966, acc 1
2016-09-06T06:16:36.894587: step 8543, loss 0.0221975, acc 1
2016-09-06T06:16:37.712662: step 8544, loss 0.0346309, acc 0.98
2016-09-06T06:16:38.521453: step 8545, loss 0.0834643, acc 0.96
2016-09-06T06:16:39.322420: step 8546, loss 0.0785346, acc 0.96
2016-09-06T06:16:40.119721: step 8547, loss 0.0119897, acc 1
2016-09-06T06:16:40.913589: step 8548, loss 0.0128442, acc 1
2016-09-06T06:16:41.706818: step 8549, loss 0.0198352, acc 1
2016-09-06T06:16:42.519282: step 8550, loss 0.0228694, acc 0.98
2016-09-06T06:16:43.347786: step 8551, loss 0.00402959, acc 1
2016-09-06T06:16:44.166874: step 8552, loss 0.00338679, acc 1
2016-09-06T06:16:44.986339: step 8553, loss 0.0188465, acc 0.98
2016-09-06T06:16:45.759212: step 8554, loss 0.0030779, acc 1
2016-09-06T06:16:46.593751: step 8555, loss 0.0515221, acc 0.98
2016-09-06T06:16:47.412372: step 8556, loss 0.0276434, acc 1
2016-09-06T06:16:48.206057: step 8557, loss 0.0256335, acc 1
2016-09-06T06:16:48.989424: step 8558, loss 0.0531127, acc 0.98
2016-09-06T06:16:49.797335: step 8559, loss 0.0026835, acc 1
2016-09-06T06:16:50.553949: step 8560, loss 0.0130967, acc 1
2016-09-06T06:16:51.370036: step 8561, loss 0.0171811, acc 1
2016-09-06T06:16:52.191126: step 8562, loss 0.0051713, acc 1
2016-09-06T06:16:52.988339: step 8563, loss 0.0582769, acc 0.94
2016-09-06T06:16:53.788666: step 8564, loss 0.00743669, acc 1
2016-09-06T06:16:54.614940: step 8565, loss 0.00949164, acc 1
2016-09-06T06:16:55.419150: step 8566, loss 0.00281143, acc 1
2016-09-06T06:16:56.211530: step 8567, loss 0.00762864, acc 1
2016-09-06T06:16:57.064326: step 8568, loss 0.00972677, acc 1
2016-09-06T06:16:57.842676: step 8569, loss 0.00483053, acc 1
2016-09-06T06:16:58.641783: step 8570, loss 0.00664515, acc 1
2016-09-06T06:16:59.488050: step 8571, loss 0.00493171, acc 1
2016-09-06T06:17:00.298918: step 8572, loss 0.117997, acc 0.96
2016-09-06T06:17:01.099956: step 8573, loss 0.0208866, acc 1
2016-09-06T06:17:01.910967: step 8574, loss 0.00467377, acc 1
2016-09-06T06:17:02.702207: step 8575, loss 0.11829, acc 0.98
2016-09-06T06:17:03.505423: step 8576, loss 0.00712377, acc 1
2016-09-06T06:17:04.305468: step 8577, loss 0.00889303, acc 1
2016-09-06T06:17:05.093033: step 8578, loss 0.168629, acc 0.96
2016-09-06T06:17:05.895481: step 8579, loss 0.0323544, acc 1
2016-09-06T06:17:06.737720: step 8580, loss 0.00228926, acc 1
2016-09-06T06:17:07.514669: step 8581, loss 0.0364241, acc 0.98
2016-09-06T06:17:08.307381: step 8582, loss 0.00400509, acc 1
2016-09-06T06:17:09.129742: step 8583, loss 0.0415725, acc 0.98
2016-09-06T06:17:09.907307: step 8584, loss 0.0828971, acc 0.96
2016-09-06T06:17:10.706215: step 8585, loss 0.0277255, acc 0.98
2016-09-06T06:17:11.517616: step 8586, loss 0.0365799, acc 0.98
2016-09-06T06:17:12.314605: step 8587, loss 0.0183472, acc 1
2016-09-06T06:17:13.110730: step 8588, loss 0.0044005, acc 1
2016-09-06T06:17:13.927448: step 8589, loss 0.0260576, acc 1
2016-09-06T06:17:14.721398: step 8590, loss 0.00878877, acc 1
2016-09-06T06:17:15.533324: step 8591, loss 0.0386647, acc 0.98
2016-09-06T06:17:16.358210: step 8592, loss 0.0308958, acc 0.98
2016-09-06T06:17:17.166259: step 8593, loss 0.0268268, acc 1
2016-09-06T06:17:17.980119: step 8594, loss 0.0234079, acc 1
2016-09-06T06:17:18.814571: step 8595, loss 0.019583, acc 1
2016-09-06T06:17:19.631694: step 8596, loss 0.0168419, acc 1
2016-09-06T06:17:20.440403: step 8597, loss 0.014666, acc 1
2016-09-06T06:17:21.269889: step 8598, loss 0.0164597, acc 1
2016-09-06T06:17:22.072009: step 8599, loss 0.00531514, acc 1
2016-09-06T06:17:22.867607: step 8600, loss 0.00421543, acc 1

Evaluation:
2016-09-06T06:17:26.615335: step 8600, loss 1.92083, acc 0.748593

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-8600

2016-09-06T06:17:28.526565: step 8601, loss 0.00582128, acc 1
2016-09-06T06:17:29.327686: step 8602, loss 0.0141727, acc 1
2016-09-06T06:17:30.130831: step 8603, loss 0.00329138, acc 1
2016-09-06T06:17:30.949326: step 8604, loss 0.0182715, acc 1
2016-09-06T06:17:31.801488: step 8605, loss 0.00937046, acc 1
2016-09-06T06:17:32.586147: step 8606, loss 0.0310249, acc 0.98
2016-09-06T06:17:33.398162: step 8607, loss 0.00475635, acc 1
2016-09-06T06:17:34.207903: step 8608, loss 0.0553219, acc 0.96
2016-09-06T06:17:34.999724: step 8609, loss 0.0274618, acc 1
2016-09-06T06:17:35.840498: step 8610, loss 0.00671858, acc 1
2016-09-06T06:17:36.638544: step 8611, loss 0.0228248, acc 0.98
2016-09-06T06:17:37.427786: step 8612, loss 0.00724598, acc 1
2016-09-06T06:17:38.279653: step 8613, loss 0.0324303, acc 0.98
2016-09-06T06:17:39.098853: step 8614, loss 0.0182819, acc 0.98
2016-09-06T06:17:39.923912: step 8615, loss 0.00354812, acc 1
2016-09-06T06:17:40.783760: step 8616, loss 0.00465273, acc 1
2016-09-06T06:17:41.601784: step 8617, loss 0.024916, acc 0.98
2016-09-06T06:17:42.394734: step 8618, loss 0.00404872, acc 1
2016-09-06T06:17:43.216990: step 8619, loss 0.027204, acc 0.98
2016-09-06T06:17:44.093531: step 8620, loss 0.00728762, acc 1
2016-09-06T06:17:44.911047: step 8621, loss 0.0112711, acc 1
2016-09-06T06:17:45.728890: step 8622, loss 0.0185104, acc 1
2016-09-06T06:17:46.558613: step 8623, loss 0.00443408, acc 1
2016-09-06T06:17:47.343147: step 8624, loss 0.00405977, acc 1
2016-09-06T06:17:48.177739: step 8625, loss 0.0135094, acc 1
2016-09-06T06:17:48.988863: step 8626, loss 0.0117118, acc 1
2016-09-06T06:17:49.797330: step 8627, loss 0.0124303, acc 1
2016-09-06T06:17:50.610931: step 8628, loss 0.0219027, acc 0.98
2016-09-06T06:17:51.471691: step 8629, loss 0.0125024, acc 1
2016-09-06T06:17:52.253494: step 8630, loss 0.00436912, acc 1
2016-09-06T06:17:53.054556: step 8631, loss 0.00968859, acc 1
2016-09-06T06:17:53.849759: step 8632, loss 0.0481545, acc 0.98
2016-09-06T06:17:54.647132: step 8633, loss 0.0227013, acc 0.98
2016-09-06T06:17:55.483592: step 8634, loss 0.00452252, acc 1
2016-09-06T06:17:56.301886: step 8635, loss 0.0538492, acc 0.98
2016-09-06T06:17:57.073187: step 8636, loss 0.00336733, acc 1
2016-09-06T06:17:57.856064: step 8637, loss 0.0179384, acc 1
2016-09-06T06:17:58.663873: step 8638, loss 0.0596417, acc 0.98
2016-09-06T06:17:59.467665: step 8639, loss 0.0298261, acc 0.98
2016-09-06T06:18:00.227448: step 8640, loss 0.0041814, acc 1
2016-09-06T06:18:01.039939: step 8641, loss 0.00369425, acc 1
2016-09-06T06:18:01.826196: step 8642, loss 0.0349567, acc 0.98
2016-09-06T06:18:02.664112: step 8643, loss 0.0233956, acc 1
2016-09-06T06:18:03.501864: step 8644, loss 0.0291137, acc 1
2016-09-06T06:18:04.308523: step 8645, loss 0.0514129, acc 0.96
2016-09-06T06:18:05.104269: step 8646, loss 0.0146021, acc 1
2016-09-06T06:18:05.933351: step 8647, loss 0.0141297, acc 1
2016-09-06T06:18:06.733772: step 8648, loss 0.0164328, acc 1
2016-09-06T06:18:07.556037: step 8649, loss 0.0278133, acc 0.98
2016-09-06T06:18:08.353533: step 8650, loss 0.0399862, acc 0.98
2016-09-06T06:18:09.147512: step 8651, loss 0.0199644, acc 1
2016-09-06T06:18:09.959122: step 8652, loss 0.0032124, acc 1
2016-09-06T06:18:10.804808: step 8653, loss 0.0600099, acc 0.98
2016-09-06T06:18:11.609468: step 8654, loss 0.0239884, acc 0.98
2016-09-06T06:18:12.433055: step 8655, loss 0.0399757, acc 0.98
2016-09-06T06:18:13.290677: step 8656, loss 0.0730065, acc 0.98
2016-09-06T06:18:14.091571: step 8657, loss 0.0262079, acc 0.98
2016-09-06T06:18:14.906452: step 8658, loss 0.00679306, acc 1
2016-09-06T06:18:15.756657: step 8659, loss 0.00837714, acc 1
2016-09-06T06:18:16.556104: step 8660, loss 0.00274724, acc 1
2016-09-06T06:18:17.364210: step 8661, loss 0.0069027, acc 1
2016-09-06T06:18:18.210797: step 8662, loss 0.0541777, acc 0.98
2016-09-06T06:18:19.041868: step 8663, loss 0.0149458, acc 1
2016-09-06T06:18:19.863676: step 8664, loss 0.0029644, acc 1
2016-09-06T06:18:20.692452: step 8665, loss 0.00482507, acc 1
2016-09-06T06:18:21.516736: step 8666, loss 0.0174598, acc 1
2016-09-06T06:18:22.351106: step 8667, loss 0.0487489, acc 0.98
2016-09-06T06:18:23.197995: step 8668, loss 0.00370089, acc 1
2016-09-06T06:18:23.998245: step 8669, loss 0.00453558, acc 1
2016-09-06T06:18:24.798306: step 8670, loss 0.0030481, acc 1
2016-09-06T06:18:25.628616: step 8671, loss 0.0225143, acc 0.98
2016-09-06T06:18:26.447845: step 8672, loss 0.00449802, acc 1
2016-09-06T06:18:27.258046: step 8673, loss 0.0246181, acc 0.98
2016-09-06T06:18:28.065022: step 8674, loss 0.0386427, acc 1
2016-09-06T06:18:28.898907: step 8675, loss 0.00353973, acc 1
2016-09-06T06:18:29.690261: step 8676, loss 0.0520764, acc 0.98
2016-09-06T06:18:30.493649: step 8677, loss 0.0211488, acc 0.98
2016-09-06T06:18:31.308847: step 8678, loss 0.0428324, acc 0.98
2016-09-06T06:18:32.105380: step 8679, loss 0.00714307, acc 1
2016-09-06T06:18:32.916175: step 8680, loss 0.00404484, acc 1
2016-09-06T06:18:33.749218: step 8681, loss 0.0478491, acc 0.98
2016-09-06T06:18:34.550960: step 8682, loss 0.00397348, acc 1
2016-09-06T06:18:35.381321: step 8683, loss 0.0171028, acc 1
2016-09-06T06:18:36.176562: step 8684, loss 0.016897, acc 0.98
2016-09-06T06:18:36.955662: step 8685, loss 0.00250463, acc 1
2016-09-06T06:18:37.757786: step 8686, loss 0.0101231, acc 1
2016-09-06T06:18:38.586090: step 8687, loss 0.00471972, acc 1
2016-09-06T06:18:39.368774: step 8688, loss 0.0182936, acc 0.98
2016-09-06T06:18:40.164301: step 8689, loss 0.0952495, acc 0.96
2016-09-06T06:18:40.982075: step 8690, loss 0.0020466, acc 1
2016-09-06T06:18:41.781467: step 8691, loss 0.0288835, acc 1
2016-09-06T06:18:42.576431: step 8692, loss 0.030355, acc 1
2016-09-06T06:18:43.411611: step 8693, loss 0.0140671, acc 1
2016-09-06T06:18:44.201896: step 8694, loss 0.00813793, acc 1
2016-09-06T06:18:44.999461: step 8695, loss 0.0118202, acc 1
2016-09-06T06:18:45.794888: step 8696, loss 0.0366178, acc 0.96
2016-09-06T06:18:46.632640: step 8697, loss 0.0234081, acc 1
2016-09-06T06:18:47.455270: step 8698, loss 0.0268675, acc 1
2016-09-06T06:18:48.273242: step 8699, loss 0.00578795, acc 1
2016-09-06T06:18:49.093360: step 8700, loss 0.0397967, acc 0.98

Evaluation:
2016-09-06T06:18:52.840153: step 8700, loss 2.0831, acc 0.746717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-8700

2016-09-06T06:18:54.704289: step 8701, loss 0.0218828, acc 1
2016-09-06T06:18:55.509365: step 8702, loss 0.00401828, acc 1
2016-09-06T06:18:56.329560: step 8703, loss 0.00346719, acc 1
2016-09-06T06:18:57.184943: step 8704, loss 0.012411, acc 1
2016-09-06T06:18:58.010053: step 8705, loss 0.0652691, acc 0.98
2016-09-06T06:18:58.795753: step 8706, loss 0.0372999, acc 0.96
2016-09-06T06:18:59.575853: step 8707, loss 0.0133624, acc 1
2016-09-06T06:19:00.418830: step 8708, loss 0.00293212, acc 1
2016-09-06T06:19:01.188583: step 8709, loss 0.0475433, acc 0.96
2016-09-06T06:19:01.977937: step 8710, loss 0.0200712, acc 0.98
2016-09-06T06:19:02.819642: step 8711, loss 0.00324361, acc 1
2016-09-06T06:19:03.578522: step 8712, loss 0.126936, acc 0.96
2016-09-06T06:19:04.373691: step 8713, loss 0.0470709, acc 0.98
2016-09-06T06:19:05.204758: step 8714, loss 0.00555152, acc 1
2016-09-06T06:19:05.997992: step 8715, loss 0.0229529, acc 0.98
2016-09-06T06:19:06.799302: step 8716, loss 0.0232625, acc 0.98
2016-09-06T06:19:07.630214: step 8717, loss 0.0168292, acc 1
2016-09-06T06:19:08.435753: step 8718, loss 0.0958774, acc 0.96
2016-09-06T06:19:09.238337: step 8719, loss 0.128883, acc 0.98
2016-09-06T06:19:10.054910: step 8720, loss 0.109774, acc 0.96
2016-09-06T06:19:10.843212: step 8721, loss 0.0201077, acc 1
2016-09-06T06:19:11.681589: step 8722, loss 0.00409678, acc 1
2016-09-06T06:19:12.498942: step 8723, loss 0.0545572, acc 0.98
2016-09-06T06:19:13.291514: step 8724, loss 0.0205094, acc 1
2016-09-06T06:19:14.062996: step 8725, loss 0.00872632, acc 1
2016-09-06T06:19:14.891118: step 8726, loss 0.0159907, acc 1
2016-09-06T06:19:15.681183: step 8727, loss 0.006738, acc 1
2016-09-06T06:19:16.507347: step 8728, loss 0.0430124, acc 0.96
2016-09-06T06:19:17.338283: step 8729, loss 0.0707919, acc 0.94
2016-09-06T06:19:18.103684: step 8730, loss 0.00312798, acc 1
2016-09-06T06:19:18.906221: step 8731, loss 0.0355046, acc 0.98
2016-09-06T06:19:19.711438: step 8732, loss 0.0151456, acc 1
2016-09-06T06:19:20.507642: step 8733, loss 0.0433091, acc 1
2016-09-06T06:19:21.341768: step 8734, loss 0.0141381, acc 1
2016-09-06T06:19:22.167741: step 8735, loss 0.0230428, acc 0.98
2016-09-06T06:19:22.952750: step 8736, loss 0.00269864, acc 1
2016-09-06T06:19:23.743523: step 8737, loss 0.00520182, acc 1
2016-09-06T06:19:24.595917: step 8738, loss 0.0154481, acc 1
2016-09-06T06:19:25.383933: step 8739, loss 0.00275468, acc 1
2016-09-06T06:19:26.188011: step 8740, loss 0.077584, acc 0.96
2016-09-06T06:19:27.031399: step 8741, loss 0.0177892, acc 1
2016-09-06T06:19:27.810528: step 8742, loss 0.0201538, acc 0.98
2016-09-06T06:19:28.607540: step 8743, loss 0.0192881, acc 0.98
2016-09-06T06:19:29.421313: step 8744, loss 0.0041075, acc 1
2016-09-06T06:19:30.224052: step 8745, loss 0.0236848, acc 0.98
2016-09-06T06:19:31.038479: step 8746, loss 0.00287943, acc 1
2016-09-06T06:19:31.879833: step 8747, loss 0.0582416, acc 0.96
2016-09-06T06:19:32.680575: step 8748, loss 0.0177139, acc 1
2016-09-06T06:19:33.495177: step 8749, loss 0.101454, acc 0.98
2016-09-06T06:19:34.319865: step 8750, loss 0.0241883, acc 1
2016-09-06T06:19:35.124738: step 8751, loss 0.00340607, acc 1
2016-09-06T06:19:35.920085: step 8752, loss 0.040671, acc 0.98
2016-09-06T06:19:36.764379: step 8753, loss 0.013937, acc 1
2016-09-06T06:19:37.583617: step 8754, loss 0.00365651, acc 1
2016-09-06T06:19:38.410664: step 8755, loss 0.0225891, acc 1
2016-09-06T06:19:39.248858: step 8756, loss 0.0444259, acc 0.98
2016-09-06T06:19:40.057212: step 8757, loss 0.0224205, acc 1
2016-09-06T06:19:40.869032: step 8758, loss 0.00335537, acc 1
2016-09-06T06:19:41.708859: step 8759, loss 0.00812082, acc 1
2016-09-06T06:19:42.514739: step 8760, loss 0.0217822, acc 0.98
2016-09-06T06:19:43.330565: step 8761, loss 0.0027233, acc 1
2016-09-06T06:19:44.166635: step 8762, loss 0.0234737, acc 0.98
2016-09-06T06:19:45.015483: step 8763, loss 0.0025742, acc 1
2016-09-06T06:19:45.816851: step 8764, loss 0.00291387, acc 1
2016-09-06T06:19:46.663899: step 8765, loss 0.0363591, acc 0.98
2016-09-06T06:19:47.476046: step 8766, loss 0.0144113, acc 1
2016-09-06T06:19:48.287029: step 8767, loss 0.0306699, acc 0.98
2016-09-06T06:19:49.123138: step 8768, loss 0.0171821, acc 1
2016-09-06T06:19:49.937860: step 8769, loss 0.0977706, acc 0.96
2016-09-06T06:19:50.744256: step 8770, loss 0.019669, acc 1
2016-09-06T06:19:51.549216: step 8771, loss 0.0102657, acc 1
2016-09-06T06:19:52.393361: step 8772, loss 0.0121808, acc 1
2016-09-06T06:19:53.165251: step 8773, loss 0.00563627, acc 1
2016-09-06T06:19:53.980460: step 8774, loss 0.0330167, acc 1
2016-09-06T06:19:54.784251: step 8775, loss 0.0073861, acc 1
2016-09-06T06:19:55.561326: step 8776, loss 0.0212016, acc 1
2016-09-06T06:19:56.379469: step 8777, loss 0.026491, acc 0.98
2016-09-06T06:19:57.197555: step 8778, loss 0.0338647, acc 0.98
2016-09-06T06:19:57.994665: step 8779, loss 0.00271011, acc 1
2016-09-06T06:19:58.797724: step 8780, loss 0.0039365, acc 1
2016-09-06T06:19:59.606351: step 8781, loss 0.0153648, acc 1
2016-09-06T06:20:00.465402: step 8782, loss 0.0238247, acc 0.98
2016-09-06T06:20:01.298495: step 8783, loss 0.0056916, acc 1
2016-09-06T06:20:02.106740: step 8784, loss 0.028797, acc 1
2016-09-06T06:20:02.906154: step 8785, loss 0.0260785, acc 0.98
2016-09-06T06:20:03.701019: step 8786, loss 0.0141468, acc 1
2016-09-06T06:20:04.515446: step 8787, loss 0.0201855, acc 1
2016-09-06T06:20:05.325582: step 8788, loss 0.0227689, acc 0.98
2016-09-06T06:20:06.145302: step 8789, loss 0.0384219, acc 0.98
2016-09-06T06:20:06.960972: step 8790, loss 0.0161364, acc 1
2016-09-06T06:20:07.754337: step 8791, loss 0.00509649, acc 1
2016-09-06T06:20:08.558221: step 8792, loss 0.00780648, acc 1
2016-09-06T06:20:09.397746: step 8793, loss 0.00838413, acc 1
2016-09-06T06:20:10.206141: step 8794, loss 0.00309903, acc 1
2016-09-06T06:20:11.017296: step 8795, loss 0.0999195, acc 0.98
2016-09-06T06:20:11.824240: step 8796, loss 0.0196392, acc 0.98
2016-09-06T06:20:12.628187: step 8797, loss 0.0155383, acc 1
2016-09-06T06:20:13.464490: step 8798, loss 0.0432506, acc 0.98
2016-09-06T06:20:14.298864: step 8799, loss 0.00307261, acc 1
2016-09-06T06:20:15.128836: step 8800, loss 0.0297937, acc 1

Evaluation:
2016-09-06T06:20:18.864397: step 8800, loss 2.74211, acc 0.737336

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-8800

2016-09-06T06:20:20.780967: step 8801, loss 0.0110343, acc 1
2016-09-06T06:20:21.637654: step 8802, loss 0.040448, acc 0.96
2016-09-06T06:20:22.457797: step 8803, loss 0.0707789, acc 0.98
2016-09-06T06:20:23.264094: step 8804, loss 0.0202052, acc 1
2016-09-06T06:20:24.094672: step 8805, loss 0.00871086, acc 1
2016-09-06T06:20:24.928113: step 8806, loss 0.0162492, acc 1
2016-09-06T06:20:25.730414: step 8807, loss 0.0082666, acc 1
2016-09-06T06:20:26.565402: step 8808, loss 0.0124551, acc 1
2016-09-06T06:20:27.387927: step 8809, loss 0.0877266, acc 0.98
2016-09-06T06:20:28.229800: step 8810, loss 0.0112941, acc 1
2016-09-06T06:20:29.086441: step 8811, loss 0.0775028, acc 0.98
2016-09-06T06:20:29.875306: step 8812, loss 0.0175324, acc 0.98
2016-09-06T06:20:30.686270: step 8813, loss 0.0328995, acc 0.98
2016-09-06T06:20:31.476489: step 8814, loss 0.0163311, acc 1
2016-09-06T06:20:32.323351: step 8815, loss 0.0557262, acc 0.96
2016-09-06T06:20:33.152534: step 8816, loss 0.00265134, acc 1
2016-09-06T06:20:33.952853: step 8817, loss 0.0169545, acc 0.98
2016-09-06T06:20:34.772663: step 8818, loss 0.0409112, acc 0.96
2016-09-06T06:20:35.585365: step 8819, loss 0.0180991, acc 1
2016-09-06T06:20:36.388284: step 8820, loss 0.0245631, acc 1
2016-09-06T06:20:37.248979: step 8821, loss 0.00358314, acc 1
2016-09-06T06:20:38.064923: step 8822, loss 0.051843, acc 0.98
2016-09-06T06:20:38.853115: step 8823, loss 0.00352987, acc 1
2016-09-06T06:20:39.677000: step 8824, loss 0.00317514, acc 1
2016-09-06T06:20:40.488997: step 8825, loss 0.0364097, acc 0.98
2016-09-06T06:20:41.331143: step 8826, loss 0.0336997, acc 1
2016-09-06T06:20:42.197214: step 8827, loss 0.0211758, acc 1
2016-09-06T06:20:43.004959: step 8828, loss 0.00446864, acc 1
2016-09-06T06:20:43.814276: step 8829, loss 0.032405, acc 1
2016-09-06T06:20:44.645833: step 8830, loss 0.00414312, acc 1
2016-09-06T06:20:45.493214: step 8831, loss 0.0130659, acc 1
2016-09-06T06:20:46.238904: step 8832, loss 0.0192202, acc 1
2016-09-06T06:20:47.070157: step 8833, loss 0.0271662, acc 0.98
2016-09-06T06:20:47.913475: step 8834, loss 0.0460723, acc 0.98
2016-09-06T06:20:48.733456: step 8835, loss 0.00764355, acc 1
2016-09-06T06:20:49.556277: step 8836, loss 0.0255429, acc 0.98
2016-09-06T06:20:50.383744: step 8837, loss 0.0356035, acc 1
2016-09-06T06:20:51.187182: step 8838, loss 0.263145, acc 0.92
2016-09-06T06:20:51.983403: step 8839, loss 0.00634176, acc 1
2016-09-06T06:20:52.801362: step 8840, loss 0.0100276, acc 1
2016-09-06T06:20:53.614756: step 8841, loss 0.0366678, acc 0.98
2016-09-06T06:20:54.412339: step 8842, loss 0.0557004, acc 0.98
2016-09-06T06:20:55.220951: step 8843, loss 0.0190703, acc 0.98
2016-09-06T06:20:56.007093: step 8844, loss 0.00628006, acc 1
2016-09-06T06:20:56.808063: step 8845, loss 0.0124746, acc 1
2016-09-06T06:20:57.608894: step 8846, loss 0.0184428, acc 1
2016-09-06T06:20:58.402202: step 8847, loss 0.103812, acc 0.94
2016-09-06T06:20:59.224972: step 8848, loss 0.00452619, acc 1
2016-09-06T06:21:00.062740: step 8849, loss 0.0232298, acc 0.98
2016-09-06T06:21:00.893569: step 8850, loss 0.00793591, acc 1
2016-09-06T06:21:01.684702: step 8851, loss 0.0197619, acc 0.98
2016-09-06T06:21:02.525952: step 8852, loss 0.00446163, acc 1
2016-09-06T06:21:03.300837: step 8853, loss 0.00443095, acc 1
2016-09-06T06:21:04.093634: step 8854, loss 0.0294341, acc 0.98
2016-09-06T06:21:04.921078: step 8855, loss 0.00485003, acc 1
2016-09-06T06:21:05.726195: step 8856, loss 0.0190088, acc 1
2016-09-06T06:21:06.519444: step 8857, loss 0.0176933, acc 1
2016-09-06T06:21:07.344499: step 8858, loss 0.10802, acc 0.96
2016-09-06T06:21:08.123525: step 8859, loss 0.167208, acc 0.94
2016-09-06T06:21:08.931693: step 8860, loss 0.00550267, acc 1
2016-09-06T06:21:09.785760: step 8861, loss 0.0330013, acc 0.98
2016-09-06T06:21:10.560533: step 8862, loss 0.00684514, acc 1
2016-09-06T06:21:11.385408: step 8863, loss 0.0106705, acc 1
2016-09-06T06:21:12.176199: step 8864, loss 0.0118717, acc 1
2016-09-06T06:21:12.968680: step 8865, loss 0.0184653, acc 1
2016-09-06T06:21:13.768936: step 8866, loss 0.0339296, acc 0.98
2016-09-06T06:21:14.574155: step 8867, loss 0.0260524, acc 0.98
2016-09-06T06:21:15.367458: step 8868, loss 0.0247035, acc 0.98
2016-09-06T06:21:16.182244: step 8869, loss 0.00685585, acc 1
2016-09-06T06:21:16.995659: step 8870, loss 0.0326606, acc 0.98
2016-09-06T06:21:17.784407: step 8871, loss 0.0216409, acc 1
2016-09-06T06:21:18.595088: step 8872, loss 0.00461102, acc 1
2016-09-06T06:21:19.416398: step 8873, loss 0.0423642, acc 0.98
2016-09-06T06:21:20.205492: step 8874, loss 0.172672, acc 0.96
2016-09-06T06:21:20.999705: step 8875, loss 0.0240949, acc 1
2016-09-06T06:21:21.809348: step 8876, loss 0.108358, acc 0.96
2016-09-06T06:21:22.597641: step 8877, loss 0.00898707, acc 1
2016-09-06T06:21:23.427082: step 8878, loss 0.00817573, acc 1
2016-09-06T06:21:24.243002: step 8879, loss 0.00447077, acc 1
2016-09-06T06:21:25.021458: step 8880, loss 0.0359144, acc 0.98
2016-09-06T06:21:25.827511: step 8881, loss 0.0713226, acc 0.96
2016-09-06T06:21:26.623416: step 8882, loss 0.0074441, acc 1
2016-09-06T06:21:27.444705: step 8883, loss 0.00693223, acc 1
2016-09-06T06:21:28.261462: step 8884, loss 0.0611544, acc 0.98
2016-09-06T06:21:29.068479: step 8885, loss 0.0150223, acc 1
2016-09-06T06:21:29.858020: step 8886, loss 0.0191826, acc 1
2016-09-06T06:21:30.658415: step 8887, loss 0.0100694, acc 1
2016-09-06T06:21:31.519456: step 8888, loss 0.0131583, acc 1
2016-09-06T06:21:32.339863: step 8889, loss 0.121986, acc 0.98
2016-09-06T06:21:33.137404: step 8890, loss 0.0317626, acc 0.98
2016-09-06T06:21:33.967745: step 8891, loss 0.00391089, acc 1
2016-09-06T06:21:34.757671: step 8892, loss 0.00627429, acc 1
2016-09-06T06:21:35.555012: step 8893, loss 0.0379835, acc 0.98
2016-09-06T06:21:36.363073: step 8894, loss 0.0179845, acc 1
2016-09-06T06:21:37.140153: step 8895, loss 0.00360511, acc 1
2016-09-06T06:21:37.942951: step 8896, loss 0.0313207, acc 1
2016-09-06T06:21:38.769457: step 8897, loss 0.0148656, acc 1
2016-09-06T06:21:39.544817: step 8898, loss 0.0229686, acc 1
2016-09-06T06:21:40.341645: step 8899, loss 0.0222779, acc 1
2016-09-06T06:21:41.147544: step 8900, loss 0.084614, acc 0.94

Evaluation:
2016-09-06T06:21:44.887903: step 8900, loss 2.00899, acc 0.747655

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-8900

2016-09-06T06:21:46.752354: step 8901, loss 0.0335719, acc 0.98
2016-09-06T06:21:47.580406: step 8902, loss 0.0543042, acc 0.98
2016-09-06T06:21:48.384692: step 8903, loss 0.0140202, acc 1
2016-09-06T06:21:49.199786: step 8904, loss 0.00335277, acc 1
2016-09-06T06:21:50.023817: step 8905, loss 0.0170651, acc 1
2016-09-06T06:21:50.848399: step 8906, loss 0.0213616, acc 1
2016-09-06T06:21:51.671880: step 8907, loss 0.0187486, acc 1
2016-09-06T06:21:52.500570: step 8908, loss 0.0246681, acc 1
2016-09-06T06:21:53.310449: step 8909, loss 0.00375414, acc 1
2016-09-06T06:21:54.082974: step 8910, loss 0.00960071, acc 1
2016-09-06T06:21:54.921562: step 8911, loss 0.0107394, acc 1
2016-09-06T06:21:55.740746: step 8912, loss 0.0156799, acc 1
2016-09-06T06:21:56.558212: step 8913, loss 0.00488966, acc 1
2016-09-06T06:21:57.366177: step 8914, loss 0.00863101, acc 1
2016-09-06T06:21:58.165285: step 8915, loss 0.0299743, acc 0.98
2016-09-06T06:21:58.928461: step 8916, loss 0.00608427, acc 1
2016-09-06T06:21:59.745534: step 8917, loss 0.0108023, acc 1
2016-09-06T06:22:00.567844: step 8918, loss 0.0286606, acc 0.98
2016-09-06T06:22:01.343860: step 8919, loss 0.0208584, acc 1
2016-09-06T06:22:02.139439: step 8920, loss 0.00349413, acc 1
2016-09-06T06:22:02.995877: step 8921, loss 0.0142792, acc 1
2016-09-06T06:22:03.838345: step 8922, loss 0.00819386, acc 1
2016-09-06T06:22:04.665752: step 8923, loss 0.00380611, acc 1
2016-09-06T06:22:05.528929: step 8924, loss 0.00359032, acc 1
2016-09-06T06:22:06.348337: step 8925, loss 0.0218026, acc 1
2016-09-06T06:22:07.175824: step 8926, loss 0.00416503, acc 1
2016-09-06T06:22:08.002561: step 8927, loss 0.00562012, acc 1
2016-09-06T06:22:08.810650: step 8928, loss 0.00387721, acc 1
2016-09-06T06:22:09.603931: step 8929, loss 0.0194571, acc 1
2016-09-06T06:22:10.446821: step 8930, loss 0.0274993, acc 1
2016-09-06T06:22:11.268525: step 8931, loss 0.0282937, acc 0.98
2016-09-06T06:22:12.110014: step 8932, loss 0.0317236, acc 0.98
2016-09-06T06:22:12.940827: step 8933, loss 0.0164543, acc 1
2016-09-06T06:22:13.757091: step 8934, loss 0.0159424, acc 1
2016-09-06T06:22:14.583543: step 8935, loss 0.0107936, acc 1
2016-09-06T06:22:15.413820: step 8936, loss 0.00336287, acc 1
2016-09-06T06:22:16.224657: step 8937, loss 0.02514, acc 1
2016-09-06T06:22:17.033274: step 8938, loss 0.00541723, acc 1
2016-09-06T06:22:17.842873: step 8939, loss 0.0153858, acc 1
2016-09-06T06:22:18.636376: step 8940, loss 0.012909, acc 1
2016-09-06T06:22:19.474512: step 8941, loss 0.0430162, acc 0.98
2016-09-06T06:22:20.283861: step 8942, loss 0.0222404, acc 1
2016-09-06T06:22:21.132804: step 8943, loss 0.00335952, acc 1
2016-09-06T06:22:21.910836: step 8944, loss 0.0246103, acc 0.98
2016-09-06T06:22:22.699739: step 8945, loss 0.0117424, acc 1
2016-09-06T06:22:23.525123: step 8946, loss 0.00393609, acc 1
2016-09-06T06:22:24.268263: step 8947, loss 0.056991, acc 0.96
2016-09-06T06:22:25.078148: step 8948, loss 0.00622729, acc 1
2016-09-06T06:22:25.898909: step 8949, loss 0.0342058, acc 0.98
2016-09-06T06:22:26.665550: step 8950, loss 0.00808454, acc 1
2016-09-06T06:22:27.515947: step 8951, loss 0.0349878, acc 0.98
2016-09-06T06:22:28.329026: step 8952, loss 0.0315416, acc 0.98
2016-09-06T06:22:29.138513: step 8953, loss 0.01822, acc 0.98
2016-09-06T06:22:29.946460: step 8954, loss 0.0225198, acc 0.98
2016-09-06T06:22:30.790484: step 8955, loss 0.00312288, acc 1
2016-09-06T06:22:31.573909: step 8956, loss 0.00522826, acc 1
2016-09-06T06:22:32.343735: step 8957, loss 0.011251, acc 1
2016-09-06T06:22:33.165890: step 8958, loss 0.0116695, acc 1
2016-09-06T06:22:33.955774: step 8959, loss 0.0482016, acc 0.98
2016-09-06T06:22:34.770206: step 8960, loss 0.00624699, acc 1
2016-09-06T06:22:35.582477: step 8961, loss 0.00660878, acc 1
2016-09-06T06:22:36.391149: step 8962, loss 0.0069116, acc 1
2016-09-06T06:22:37.227090: step 8963, loss 0.0320598, acc 0.98
2016-09-06T06:22:38.048684: step 8964, loss 0.00323959, acc 1
2016-09-06T06:22:38.843196: step 8965, loss 0.0299892, acc 0.98
2016-09-06T06:22:39.633414: step 8966, loss 0.0327774, acc 0.98
2016-09-06T06:22:40.462106: step 8967, loss 0.0512124, acc 0.96
2016-09-06T06:22:41.257909: step 8968, loss 0.00608955, acc 1
2016-09-06T06:22:42.052597: step 8969, loss 0.0159248, acc 1
2016-09-06T06:22:42.833946: step 8970, loss 0.0912227, acc 0.98
2016-09-06T06:22:43.657218: step 8971, loss 0.00310226, acc 1
2016-09-06T06:22:44.461385: step 8972, loss 0.00369109, acc 1
2016-09-06T06:22:45.285521: step 8973, loss 0.0375307, acc 1
2016-09-06T06:22:46.077401: step 8974, loss 0.00667732, acc 1
2016-09-06T06:22:46.903676: step 8975, loss 0.00287942, acc 1
2016-09-06T06:22:47.702393: step 8976, loss 0.0147529, acc 1
2016-09-06T06:22:48.471175: step 8977, loss 0.0199833, acc 1
2016-09-06T06:22:49.268616: step 8978, loss 0.0063189, acc 1
2016-09-06T06:22:50.088770: step 8979, loss 0.00269302, acc 1
2016-09-06T06:22:50.873974: step 8980, loss 0.0114686, acc 1
2016-09-06T06:22:51.671993: step 8981, loss 0.00373926, acc 1
2016-09-06T06:22:52.498498: step 8982, loss 0.0333108, acc 0.98
2016-09-06T06:22:53.312256: step 8983, loss 0.0439695, acc 0.98
2016-09-06T06:22:54.125112: step 8984, loss 0.0344698, acc 0.98
2016-09-06T06:22:54.958325: step 8985, loss 0.0171773, acc 1
2016-09-06T06:22:55.744435: step 8986, loss 0.00293082, acc 1
2016-09-06T06:22:56.543189: step 8987, loss 0.00268113, acc 1
2016-09-06T06:22:57.330748: step 8988, loss 0.00536492, acc 1
2016-09-06T06:22:58.140918: step 8989, loss 0.115423, acc 0.96
2016-09-06T06:22:58.922207: step 8990, loss 0.0522031, acc 0.96
2016-09-06T06:22:59.737915: step 8991, loss 0.0112768, acc 1
2016-09-06T06:23:00.552335: step 8992, loss 0.0615796, acc 0.98
2016-09-06T06:23:01.340684: step 8993, loss 0.0521878, acc 0.98
2016-09-06T06:23:02.170284: step 8994, loss 0.0319614, acc 0.98
2016-09-06T06:23:02.984926: step 8995, loss 0.0162571, acc 1
2016-09-06T06:23:03.779379: step 8996, loss 0.0063002, acc 1
2016-09-06T06:23:04.617683: step 8997, loss 0.00954887, acc 1
2016-09-06T06:23:05.425569: step 8998, loss 0.0243669, acc 1
2016-09-06T06:23:06.262026: step 8999, loss 0.00431084, acc 1
2016-09-06T06:23:07.080778: step 9000, loss 0.0244552, acc 0.98

Evaluation:
2016-09-06T06:23:10.835028: step 9000, loss 2.16321, acc 0.742026

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-9000

2016-09-06T06:23:12.727844: step 9001, loss 0.0323274, acc 0.98
2016-09-06T06:23:13.548513: step 9002, loss 0.00414968, acc 1
2016-09-06T06:23:14.361490: step 9003, loss 0.0176741, acc 1
2016-09-06T06:23:15.161509: step 9004, loss 0.00682135, acc 1
2016-09-06T06:23:15.997151: step 9005, loss 0.00325505, acc 1
2016-09-06T06:23:16.834817: step 9006, loss 0.0494305, acc 0.98
2016-09-06T06:23:17.649509: step 9007, loss 0.0151862, acc 1
2016-09-06T06:23:18.472997: step 9008, loss 0.00290429, acc 1
2016-09-06T06:23:19.291195: step 9009, loss 0.0278862, acc 0.98
2016-09-06T06:23:20.090729: step 9010, loss 0.0963644, acc 0.96
2016-09-06T06:23:20.906765: step 9011, loss 0.0117981, acc 1
2016-09-06T06:23:21.745437: step 9012, loss 0.0244722, acc 0.98
2016-09-06T06:23:22.549813: step 9013, loss 0.0239159, acc 0.98
2016-09-06T06:23:23.369681: step 9014, loss 0.00502574, acc 1
2016-09-06T06:23:24.202259: step 9015, loss 0.00334307, acc 1
2016-09-06T06:23:25.041696: step 9016, loss 0.00494079, acc 1
2016-09-06T06:23:25.860201: step 9017, loss 0.038459, acc 0.98
2016-09-06T06:23:26.701515: step 9018, loss 0.0197012, acc 1
2016-09-06T06:23:27.505937: step 9019, loss 0.016954, acc 1
2016-09-06T06:23:28.289446: step 9020, loss 0.00274248, acc 1
2016-09-06T06:23:29.112577: step 9021, loss 0.0665716, acc 0.98
2016-09-06T06:23:29.901607: step 9022, loss 0.018302, acc 0.98
2016-09-06T06:23:30.715071: step 9023, loss 0.0217801, acc 0.98
2016-09-06T06:23:31.480968: step 9024, loss 0.0887503, acc 0.977273
2016-09-06T06:23:32.302008: step 9025, loss 0.0185662, acc 0.98
2016-09-06T06:23:33.134284: step 9026, loss 0.0259267, acc 1
2016-09-06T06:23:33.951911: step 9027, loss 0.0514124, acc 0.96
2016-09-06T06:23:34.761707: step 9028, loss 0.00665819, acc 1
2016-09-06T06:23:35.558680: step 9029, loss 0.0210308, acc 1
2016-09-06T06:23:36.374465: step 9030, loss 0.00538949, acc 1
2016-09-06T06:23:37.210505: step 9031, loss 0.0108954, acc 1
2016-09-06T06:23:37.994795: step 9032, loss 0.0455008, acc 0.96
2016-09-06T06:23:38.793444: step 9033, loss 0.0256087, acc 1
2016-09-06T06:23:39.605454: step 9034, loss 0.00660778, acc 1
2016-09-06T06:23:40.381820: step 9035, loss 0.0137176, acc 1
2016-09-06T06:23:41.221118: step 9036, loss 0.00403164, acc 1
2016-09-06T06:23:42.037433: step 9037, loss 0.0161638, acc 1
2016-09-06T06:23:42.829362: step 9038, loss 0.0121533, acc 1
2016-09-06T06:23:43.624853: step 9039, loss 0.118011, acc 0.92
2016-09-06T06:23:44.486304: step 9040, loss 0.00364541, acc 1
2016-09-06T06:23:45.286666: step 9041, loss 0.0104366, acc 1
2016-09-06T06:23:46.104159: step 9042, loss 0.0780969, acc 0.98
2016-09-06T06:23:46.907540: step 9043, loss 0.0161982, acc 1
2016-09-06T06:23:47.688259: step 9044, loss 0.00454671, acc 1
2016-09-06T06:23:48.485288: step 9045, loss 0.0180847, acc 1
2016-09-06T06:23:49.314671: step 9046, loss 0.0274012, acc 0.98
2016-09-06T06:23:50.140515: step 9047, loss 0.0238152, acc 0.98
2016-09-06T06:23:50.925365: step 9048, loss 0.00280781, acc 1
2016-09-06T06:23:51.769972: step 9049, loss 0.0203363, acc 1
2016-09-06T06:23:52.587912: step 9050, loss 0.0223908, acc 1
2016-09-06T06:23:53.383968: step 9051, loss 0.00475899, acc 1
2016-09-06T06:23:54.203659: step 9052, loss 0.0338579, acc 0.98
2016-09-06T06:23:55.025186: step 9053, loss 0.0146402, acc 1
2016-09-06T06:23:55.836157: step 9054, loss 0.0643556, acc 0.98
2016-09-06T06:23:56.691805: step 9055, loss 0.00534504, acc 1
2016-09-06T06:23:57.488242: step 9056, loss 0.0115377, acc 1
2016-09-06T06:23:58.314882: step 9057, loss 0.0402225, acc 0.98
2016-09-06T06:23:59.134704: step 9058, loss 0.00420298, acc 1
2016-09-06T06:23:59.952931: step 9059, loss 0.0142128, acc 1
2016-09-06T06:24:00.788482: step 9060, loss 0.03007, acc 0.98
2016-09-06T06:24:01.608797: step 9061, loss 0.00286599, acc 1
2016-09-06T06:24:02.412917: step 9062, loss 0.0174174, acc 0.98
2016-09-06T06:24:03.232790: step 9063, loss 0.0179318, acc 1
2016-09-06T06:24:04.061041: step 9064, loss 0.0455193, acc 0.98
2016-09-06T06:24:04.860675: step 9065, loss 0.0357027, acc 1
2016-09-06T06:24:05.676534: step 9066, loss 0.00282019, acc 1
2016-09-06T06:24:06.517342: step 9067, loss 0.0743954, acc 0.96
2016-09-06T06:24:07.316204: step 9068, loss 0.02629, acc 0.98
2016-09-06T06:24:08.152261: step 9069, loss 0.0034837, acc 1
2016-09-06T06:24:08.990809: step 9070, loss 0.0324355, acc 0.98
2016-09-06T06:24:09.792393: step 9071, loss 0.0823699, acc 0.96
2016-09-06T06:24:10.585406: step 9072, loss 0.0110272, acc 1
2016-09-06T06:24:11.419741: step 9073, loss 0.0139082, acc 1
2016-09-06T06:24:12.219004: step 9074, loss 0.0291999, acc 0.98
2016-09-06T06:24:13.027702: step 9075, loss 0.0141643, acc 1
2016-09-06T06:24:13.848987: step 9076, loss 0.0220241, acc 0.98
2016-09-06T06:24:14.652282: step 9077, loss 0.0260543, acc 1
2016-09-06T06:24:15.434416: step 9078, loss 0.0139912, acc 1
2016-09-06T06:24:16.260203: step 9079, loss 0.0115131, acc 1
2016-09-06T06:24:17.054108: step 9080, loss 0.0129167, acc 1
2016-09-06T06:24:17.861641: step 9081, loss 0.0512222, acc 0.98
2016-09-06T06:24:18.682301: step 9082, loss 0.0308537, acc 0.98
2016-09-06T06:24:19.515076: step 9083, loss 0.00285185, acc 1
2016-09-06T06:24:20.299898: step 9084, loss 0.00550274, acc 1
2016-09-06T06:24:21.104298: step 9085, loss 0.0540601, acc 0.98
2016-09-06T06:24:21.919536: step 9086, loss 0.00289669, acc 1
2016-09-06T06:24:22.702171: step 9087, loss 0.00270618, acc 1
2016-09-06T06:24:23.538299: step 9088, loss 0.0200202, acc 0.98
2016-09-06T06:24:24.398374: step 9089, loss 0.0180624, acc 0.98
2016-09-06T06:24:25.225592: step 9090, loss 0.0807302, acc 0.96
2016-09-06T06:24:26.022933: step 9091, loss 0.00350403, acc 1
2016-09-06T06:24:26.868145: step 9092, loss 0.0157539, acc 1
2016-09-06T06:24:27.673852: step 9093, loss 0.0245241, acc 1
2016-09-06T06:24:28.488325: step 9094, loss 0.0218584, acc 1
2016-09-06T06:24:29.331399: step 9095, loss 0.0054284, acc 1
2016-09-06T06:24:30.154505: step 9096, loss 0.0164882, acc 0.98
2016-09-06T06:24:30.969278: step 9097, loss 0.0473569, acc 0.94
2016-09-06T06:24:31.799498: step 9098, loss 0.0073574, acc 1
2016-09-06T06:24:32.615409: step 9099, loss 0.0189274, acc 1
2016-09-06T06:24:33.470525: step 9100, loss 0.0156409, acc 1

Evaluation:
2016-09-06T06:24:37.255697: step 9100, loss 1.9662, acc 0.737336

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-9100

2016-09-06T06:24:39.123858: step 9101, loss 0.0164003, acc 1
2016-09-06T06:24:39.936612: step 9102, loss 0.0221345, acc 0.98
2016-09-06T06:24:40.749865: step 9103, loss 0.00551792, acc 1
2016-09-06T06:24:41.584496: step 9104, loss 0.00528194, acc 1
2016-09-06T06:24:42.425280: step 9105, loss 0.109001, acc 0.98
2016-09-06T06:24:43.206990: step 9106, loss 0.01012, acc 1
2016-09-06T06:24:44.027860: step 9107, loss 0.0158145, acc 1
2016-09-06T06:24:44.857444: step 9108, loss 0.0280958, acc 0.98
2016-09-06T06:24:45.687981: step 9109, loss 0.0114974, acc 1
2016-09-06T06:24:46.502317: step 9110, loss 0.0419467, acc 0.98
2016-09-06T06:24:47.322021: step 9111, loss 0.0495314, acc 0.98
2016-09-06T06:24:48.161258: step 9112, loss 0.029034, acc 1
2016-09-06T06:24:49.012444: step 9113, loss 0.0421684, acc 0.98
2016-09-06T06:24:49.845789: step 9114, loss 0.0202087, acc 0.98
2016-09-06T06:24:50.662378: step 9115, loss 0.0272613, acc 0.98
2016-09-06T06:24:51.448828: step 9116, loss 0.0358241, acc 0.98
2016-09-06T06:24:52.248367: step 9117, loss 0.0112771, acc 1
2016-09-06T06:24:53.041438: step 9118, loss 0.0123941, acc 1
2016-09-06T06:24:53.840241: step 9119, loss 0.0119465, acc 1
2016-09-06T06:24:54.650868: step 9120, loss 0.00316936, acc 1
2016-09-06T06:24:55.475034: step 9121, loss 0.0553446, acc 0.98
2016-09-06T06:24:56.285017: step 9122, loss 0.00418583, acc 1
2016-09-06T06:24:57.133269: step 9123, loss 0.00317644, acc 1
2016-09-06T06:24:57.970133: step 9124, loss 0.0432465, acc 0.98
2016-09-06T06:24:58.798376: step 9125, loss 0.00737132, acc 1
2016-09-06T06:24:59.613695: step 9126, loss 0.00742951, acc 1
2016-09-06T06:25:00.440031: step 9127, loss 0.0141139, acc 1
2016-09-06T06:25:01.265380: step 9128, loss 0.00259998, acc 1
2016-09-06T06:25:02.104585: step 9129, loss 0.0099631, acc 1
2016-09-06T06:25:02.905639: step 9130, loss 0.00313184, acc 1
2016-09-06T06:25:03.724170: step 9131, loss 0.0966147, acc 0.96
2016-09-06T06:25:04.589339: step 9132, loss 0.0150414, acc 1
2016-09-06T06:25:05.401880: step 9133, loss 0.0229167, acc 1
2016-09-06T06:25:06.219094: step 9134, loss 0.0444989, acc 0.98
2016-09-06T06:25:07.037305: step 9135, loss 0.00235802, acc 1
2016-09-06T06:25:07.855792: step 9136, loss 0.0424879, acc 0.98
2016-09-06T06:25:08.684805: step 9137, loss 0.0331097, acc 1
2016-09-06T06:25:09.488144: step 9138, loss 0.0240434, acc 1
2016-09-06T06:25:10.295398: step 9139, loss 0.0493857, acc 0.94
2016-09-06T06:25:11.068521: step 9140, loss 0.00636772, acc 1
2016-09-06T06:25:11.892218: step 9141, loss 0.0101036, acc 1
2016-09-06T06:25:12.717387: step 9142, loss 0.00317462, acc 1
2016-09-06T06:25:13.526659: step 9143, loss 0.0200561, acc 1
2016-09-06T06:25:14.321557: step 9144, loss 0.0435226, acc 0.98
2016-09-06T06:25:15.141635: step 9145, loss 0.0106979, acc 1
2016-09-06T06:25:15.952919: step 9146, loss 0.0333138, acc 0.98
2016-09-06T06:25:16.784780: step 9147, loss 0.0391271, acc 0.98
2016-09-06T06:25:17.598302: step 9148, loss 0.00236464, acc 1
2016-09-06T06:25:18.373077: step 9149, loss 0.00914711, acc 1
2016-09-06T06:25:19.162540: step 9150, loss 0.00555085, acc 1
2016-09-06T06:25:19.975614: step 9151, loss 0.00539779, acc 1
2016-09-06T06:25:20.727469: step 9152, loss 0.0921457, acc 0.96
2016-09-06T06:25:21.535813: step 9153, loss 0.0081335, acc 1
2016-09-06T06:25:22.362717: step 9154, loss 0.00920456, acc 1
2016-09-06T06:25:23.149441: step 9155, loss 0.0287104, acc 0.98
2016-09-06T06:25:23.957836: step 9156, loss 0.015388, acc 1
2016-09-06T06:25:24.781224: step 9157, loss 0.0599057, acc 0.98
2016-09-06T06:25:25.578134: step 9158, loss 0.00645332, acc 1
2016-09-06T06:25:26.407597: step 9159, loss 0.0280249, acc 0.98
2016-09-06T06:25:27.224553: step 9160, loss 0.00892414, acc 1
2016-09-06T06:25:28.021072: step 9161, loss 0.00234589, acc 1
2016-09-06T06:25:28.825648: step 9162, loss 0.0205645, acc 1
2016-09-06T06:25:29.637475: step 9163, loss 0.00243966, acc 1
2016-09-06T06:25:30.413395: step 9164, loss 0.0183384, acc 1
2016-09-06T06:25:31.214789: step 9165, loss 0.0210432, acc 0.98
2016-09-06T06:25:32.029853: step 9166, loss 0.00212829, acc 1
2016-09-06T06:25:32.821791: step 9167, loss 0.0088543, acc 1
2016-09-06T06:25:33.625280: step 9168, loss 0.0368822, acc 0.98
2016-09-06T06:25:34.459465: step 9169, loss 0.00296092, acc 1
2016-09-06T06:25:35.246539: step 9170, loss 0.023812, acc 0.98
2016-09-06T06:25:36.062125: step 9171, loss 0.0221691, acc 1
2016-09-06T06:25:36.883151: step 9172, loss 0.00262477, acc 1
2016-09-06T06:25:37.670929: step 9173, loss 0.0150032, acc 1
2016-09-06T06:25:38.483929: step 9174, loss 0.00638542, acc 1
2016-09-06T06:25:39.306196: step 9175, loss 0.00525562, acc 1
2016-09-06T06:25:40.108698: step 9176, loss 0.00308121, acc 1
2016-09-06T06:25:40.901988: step 9177, loss 0.0126859, acc 1
2016-09-06T06:25:41.705138: step 9178, loss 0.0022286, acc 1
2016-09-06T06:25:42.508998: step 9179, loss 0.0306907, acc 0.98
2016-09-06T06:25:43.312541: step 9180, loss 0.0269411, acc 1
2016-09-06T06:25:44.124505: step 9181, loss 0.00516167, acc 1
2016-09-06T06:25:44.929411: step 9182, loss 0.0241584, acc 0.98
2016-09-06T06:25:45.779243: step 9183, loss 0.00282062, acc 1
2016-09-06T06:25:46.589447: step 9184, loss 0.0300915, acc 0.98
2016-09-06T06:25:47.372443: step 9185, loss 0.00387085, acc 1
2016-09-06T06:25:48.183894: step 9186, loss 0.0200983, acc 1
2016-09-06T06:25:48.990815: step 9187, loss 0.0301293, acc 0.98
2016-09-06T06:25:49.797396: step 9188, loss 0.00912465, acc 1
2016-09-06T06:25:50.609650: step 9189, loss 0.00293348, acc 1
2016-09-06T06:25:51.414686: step 9190, loss 0.0150877, acc 1
2016-09-06T06:25:52.224964: step 9191, loss 0.0206639, acc 0.98
2016-09-06T06:25:53.057261: step 9192, loss 0.0106436, acc 1
2016-09-06T06:25:53.893674: step 9193, loss 0.0775405, acc 0.98
2016-09-06T06:25:54.694034: step 9194, loss 0.0311294, acc 0.98
2016-09-06T06:25:55.496122: step 9195, loss 0.0135205, acc 1
2016-09-06T06:25:56.367545: step 9196, loss 0.00271222, acc 1
2016-09-06T06:25:57.133854: step 9197, loss 0.0115176, acc 1
2016-09-06T06:25:57.986634: step 9198, loss 0.0785108, acc 0.98
2016-09-06T06:25:58.816686: step 9199, loss 0.0123547, acc 1
2016-09-06T06:25:59.648958: step 9200, loss 0.067321, acc 0.98

Evaluation:
2016-09-06T06:26:03.401400: step 9200, loss 2.72808, acc 0.74015

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-9200

2016-09-06T06:26:05.364252: step 9201, loss 0.0141968, acc 1
2016-09-06T06:26:06.205669: step 9202, loss 0.0755547, acc 0.98
2016-09-06T06:26:07.024091: step 9203, loss 0.0149025, acc 1
2016-09-06T06:26:07.833265: step 9204, loss 0.0297394, acc 1
2016-09-06T06:26:08.703293: step 9205, loss 0.00357414, acc 1
2016-09-06T06:26:09.521031: step 9206, loss 0.0041536, acc 1
2016-09-06T06:26:10.338364: step 9207, loss 0.00286293, acc 1
2016-09-06T06:26:11.193588: step 9208, loss 0.00377862, acc 1
2016-09-06T06:26:11.995248: step 9209, loss 0.00314724, acc 1
2016-09-06T06:26:12.816862: step 9210, loss 0.0169795, acc 0.98
2016-09-06T06:26:13.650457: step 9211, loss 0.0177494, acc 1
2016-09-06T06:26:14.461120: step 9212, loss 0.00418223, acc 1
2016-09-06T06:26:15.285131: step 9213, loss 0.132934, acc 0.96
2016-09-06T06:26:16.080643: step 9214, loss 0.025492, acc 0.98
2016-09-06T06:26:16.896813: step 9215, loss 0.0921081, acc 0.96
2016-09-06T06:26:17.650437: step 9216, loss 0.0148652, acc 1
2016-09-06T06:26:18.498698: step 9217, loss 0.0155109, acc 1
2016-09-06T06:26:19.288801: step 9218, loss 0.0454508, acc 0.96
2016-09-06T06:26:20.100985: step 9219, loss 0.0303003, acc 0.98
2016-09-06T06:26:20.918437: step 9220, loss 0.0237732, acc 1
2016-09-06T06:26:21.740172: step 9221, loss 0.0325223, acc 0.98
2016-09-06T06:26:22.551495: step 9222, loss 0.0181794, acc 0.98
2016-09-06T06:26:23.361353: step 9223, loss 0.00813912, acc 1
2016-09-06T06:26:24.190221: step 9224, loss 0.01964, acc 0.98
2016-09-06T06:26:24.987102: step 9225, loss 0.0231533, acc 0.98
2016-09-06T06:26:25.792297: step 9226, loss 0.0698833, acc 0.98
2016-09-06T06:26:26.628705: step 9227, loss 0.00235365, acc 1
2016-09-06T06:26:27.393462: step 9228, loss 0.00266414, acc 1
2016-09-06T06:26:28.199793: step 9229, loss 0.037534, acc 0.98
2016-09-06T06:26:29.033421: step 9230, loss 0.0198321, acc 0.98
2016-09-06T06:26:29.829560: step 9231, loss 0.0274589, acc 1
2016-09-06T06:26:30.654858: step 9232, loss 0.00361342, acc 1
2016-09-06T06:26:31.479173: step 9233, loss 0.0260462, acc 0.98
2016-09-06T06:26:32.303127: step 9234, loss 0.0243817, acc 1
2016-09-06T06:26:33.121750: step 9235, loss 0.0377415, acc 0.98
2016-09-06T06:26:33.988235: step 9236, loss 0.0058644, acc 1
2016-09-06T06:26:34.811695: step 9237, loss 0.00393974, acc 1
2016-09-06T06:26:35.617060: step 9238, loss 0.0619255, acc 0.98
2016-09-06T06:26:36.439856: step 9239, loss 0.0498478, acc 0.98
2016-09-06T06:26:37.301957: step 9240, loss 0.0161444, acc 1
2016-09-06T06:26:38.108611: step 9241, loss 0.0234652, acc 1
2016-09-06T06:26:38.924379: step 9242, loss 0.0386343, acc 0.98
2016-09-06T06:26:39.731562: step 9243, loss 0.00435183, acc 1
2016-09-06T06:26:40.541557: step 9244, loss 0.00265622, acc 1
2016-09-06T06:26:41.377176: step 9245, loss 0.019079, acc 1
2016-09-06T06:26:42.209422: step 9246, loss 0.00235649, acc 1
2016-09-06T06:26:43.051456: step 9247, loss 0.00676019, acc 1
2016-09-06T06:26:43.865033: step 9248, loss 0.0234075, acc 1
2016-09-06T06:26:44.674012: step 9249, loss 0.0275706, acc 0.98
2016-09-06T06:26:45.477573: step 9250, loss 0.0250049, acc 1
2016-09-06T06:26:46.304016: step 9251, loss 0.0600692, acc 0.96
2016-09-06T06:26:47.116641: step 9252, loss 0.0449858, acc 0.98
2016-09-06T06:26:47.919142: step 9253, loss 0.0521095, acc 0.98
2016-09-06T06:26:48.737213: step 9254, loss 0.0312708, acc 1
2016-09-06T06:26:49.574358: step 9255, loss 0.164534, acc 0.98
2016-09-06T06:26:50.385906: step 9256, loss 0.00293505, acc 1
2016-09-06T06:26:51.185455: step 9257, loss 0.0327527, acc 0.98
2016-09-06T06:26:52.004525: step 9258, loss 0.0500502, acc 0.98
2016-09-06T06:26:52.776189: step 9259, loss 0.0240979, acc 0.98
2016-09-06T06:26:53.601712: step 9260, loss 0.0675011, acc 0.98
2016-09-06T06:26:54.428343: step 9261, loss 0.0123194, acc 1
2016-09-06T06:26:55.207073: step 9262, loss 0.0232709, acc 1
2016-09-06T06:26:56.029343: step 9263, loss 0.0111026, acc 1
2016-09-06T06:26:56.854762: step 9264, loss 0.0167214, acc 1
2016-09-06T06:26:57.646662: step 9265, loss 0.0576535, acc 0.96
2016-09-06T06:26:58.449029: step 9266, loss 0.0110581, acc 1
2016-09-06T06:26:59.245614: step 9267, loss 0.0299688, acc 1
2016-09-06T06:27:00.034761: step 9268, loss 0.0367274, acc 0.98
2016-09-06T06:27:00.868033: step 9269, loss 0.0422471, acc 1
2016-09-06T06:27:01.711202: step 9270, loss 0.0745036, acc 0.94
2016-09-06T06:27:02.553462: step 9271, loss 0.0291645, acc 0.98
2016-09-06T06:27:03.360312: step 9272, loss 0.0104054, acc 1
2016-09-06T06:27:04.197444: step 9273, loss 0.0164138, acc 1
2016-09-06T06:27:04.996183: step 9274, loss 0.0483982, acc 0.98
2016-09-06T06:27:05.807040: step 9275, loss 0.0229498, acc 0.98
2016-09-06T06:27:06.620824: step 9276, loss 0.0223998, acc 0.98
2016-09-06T06:27:07.475579: step 9277, loss 0.0200612, acc 0.98
2016-09-06T06:27:08.262372: step 9278, loss 0.0235073, acc 0.98
2016-09-06T06:27:09.124271: step 9279, loss 0.00338613, acc 1
2016-09-06T06:27:09.965022: step 9280, loss 0.0279636, acc 0.98
2016-09-06T06:27:10.772822: step 9281, loss 0.0632629, acc 0.96
2016-09-06T06:27:11.621989: step 9282, loss 0.0214301, acc 1
2016-09-06T06:27:12.472191: step 9283, loss 0.0190328, acc 0.98
2016-09-06T06:27:13.273702: step 9284, loss 0.0160907, acc 1
2016-09-06T06:27:14.098801: step 9285, loss 0.0189133, acc 1
2016-09-06T06:27:14.930505: step 9286, loss 0.0116359, acc 1
2016-09-06T06:27:15.726011: step 9287, loss 0.00382774, acc 1
2016-09-06T06:27:16.538967: step 9288, loss 0.0303064, acc 0.98
2016-09-06T06:27:17.382684: step 9289, loss 0.0499241, acc 0.98
2016-09-06T06:27:18.181842: step 9290, loss 0.00636327, acc 1
2016-09-06T06:27:18.969421: step 9291, loss 0.0411033, acc 0.96
2016-09-06T06:27:19.782594: step 9292, loss 0.0110133, acc 1
2016-09-06T06:27:20.547313: step 9293, loss 0.0146363, acc 1
2016-09-06T06:27:21.359007: step 9294, loss 0.0194205, acc 1
2016-09-06T06:27:22.151981: step 9295, loss 0.00862781, acc 1
2016-09-06T06:27:22.931317: step 9296, loss 0.0169763, acc 1
2016-09-06T06:27:23.745278: step 9297, loss 0.00375328, acc 1
2016-09-06T06:27:24.548443: step 9298, loss 0.0114333, acc 1
2016-09-06T06:27:25.341288: step 9299, loss 0.0482641, acc 0.98
2016-09-06T06:27:26.148566: step 9300, loss 0.0119432, acc 1

Evaluation:
2016-09-06T06:27:29.948905: step 9300, loss 3.07309, acc 0.730769

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-9300

2016-09-06T06:27:31.927884: step 9301, loss 0.0127287, acc 1
2016-09-06T06:27:32.722041: step 9302, loss 0.00511198, acc 1
2016-09-06T06:27:33.523463: step 9303, loss 0.00357566, acc 1
2016-09-06T06:27:34.338451: step 9304, loss 0.00366362, acc 1
2016-09-06T06:27:35.101286: step 9305, loss 0.00612193, acc 1
2016-09-06T06:27:35.911099: step 9306, loss 0.00484017, acc 1
2016-09-06T06:27:36.728870: step 9307, loss 0.018114, acc 0.98
2016-09-06T06:27:37.510209: step 9308, loss 0.00344822, acc 1
2016-09-06T06:27:38.298831: step 9309, loss 0.0289035, acc 0.98
2016-09-06T06:27:39.118706: step 9310, loss 0.00660654, acc 1
2016-09-06T06:27:39.941504: step 9311, loss 0.0456681, acc 0.98
2016-09-06T06:27:40.732680: step 9312, loss 0.00516952, acc 1
2016-09-06T06:27:41.571057: step 9313, loss 0.00328388, acc 1
2016-09-06T06:27:42.346336: step 9314, loss 0.0375668, acc 0.98
2016-09-06T06:27:43.150489: step 9315, loss 0.0386308, acc 1
2016-09-06T06:27:43.974510: step 9316, loss 0.0237792, acc 0.98
2016-09-06T06:27:44.785959: step 9317, loss 0.015842, acc 1
2016-09-06T06:27:45.567088: step 9318, loss 0.0138771, acc 1
2016-09-06T06:27:46.392924: step 9319, loss 0.0103405, acc 1
2016-09-06T06:27:47.182113: step 9320, loss 0.00324571, acc 1
2016-09-06T06:27:47.978221: step 9321, loss 0.0164233, acc 1
2016-09-06T06:27:48.787865: step 9322, loss 0.0634102, acc 0.98
2016-09-06T06:27:49.601203: step 9323, loss 0.0429319, acc 0.98
2016-09-06T06:27:50.428128: step 9324, loss 0.0848296, acc 0.96
2016-09-06T06:27:51.231312: step 9325, loss 0.0328041, acc 0.98
2016-09-06T06:27:52.028611: step 9326, loss 0.0069583, acc 1
2016-09-06T06:27:52.804524: step 9327, loss 0.00524541, acc 1
2016-09-06T06:27:53.614526: step 9328, loss 0.0154404, acc 1
2016-09-06T06:27:54.405628: step 9329, loss 0.0590405, acc 0.98
2016-09-06T06:27:55.217940: step 9330, loss 0.0187039, acc 1
2016-09-06T06:27:56.050598: step 9331, loss 0.0474992, acc 0.98
2016-09-06T06:27:56.828374: step 9332, loss 0.00429354, acc 1
2016-09-06T06:27:57.621566: step 9333, loss 0.00572832, acc 1
2016-09-06T06:27:58.446949: step 9334, loss 0.00261541, acc 1
2016-09-06T06:27:59.236338: step 9335, loss 0.0189576, acc 1
2016-09-06T06:28:00.048417: step 9336, loss 0.0432464, acc 0.96
2016-09-06T06:28:00.935906: step 9337, loss 0.00264343, acc 1
2016-09-06T06:28:01.741563: step 9338, loss 0.022728, acc 0.98
2016-09-06T06:28:02.561250: step 9339, loss 0.0135885, acc 1
2016-09-06T06:28:03.430258: step 9340, loss 0.0717463, acc 0.96
2016-09-06T06:28:04.244590: step 9341, loss 0.0436812, acc 0.98
2016-09-06T06:28:05.087122: step 9342, loss 0.0122666, acc 1
2016-09-06T06:28:05.912432: step 9343, loss 0.0242968, acc 0.98
2016-09-06T06:28:06.718160: step 9344, loss 0.0299654, acc 0.98
2016-09-06T06:28:07.549723: step 9345, loss 0.00258566, acc 1
2016-09-06T06:28:08.379088: step 9346, loss 0.00535052, acc 1
2016-09-06T06:28:09.222374: step 9347, loss 0.0174059, acc 0.98
2016-09-06T06:28:10.068707: step 9348, loss 0.0109536, acc 1
2016-09-06T06:28:10.903028: step 9349, loss 0.00244092, acc 1
2016-09-06T06:28:11.713409: step 9350, loss 0.0222352, acc 0.98
2016-09-06T06:28:12.496272: step 9351, loss 0.032179, acc 0.98
2016-09-06T06:28:13.334314: step 9352, loss 0.0098562, acc 1
2016-09-06T06:28:14.155140: step 9353, loss 0.0574024, acc 0.96
2016-09-06T06:28:14.940988: step 9354, loss 0.0136813, acc 1
2016-09-06T06:28:15.754805: step 9355, loss 0.00236274, acc 1
2016-09-06T06:28:16.597604: step 9356, loss 0.0236068, acc 0.98
2016-09-06T06:28:17.382329: step 9357, loss 0.0116491, acc 1
2016-09-06T06:28:18.179025: step 9358, loss 0.0167257, acc 0.98
2016-09-06T06:28:19.017864: step 9359, loss 0.00232324, acc 1
2016-09-06T06:28:19.801074: step 9360, loss 0.0272918, acc 0.98
2016-09-06T06:28:20.606475: step 9361, loss 0.0240853, acc 0.98
2016-09-06T06:28:21.461803: step 9362, loss 0.0234201, acc 1
2016-09-06T06:28:22.233654: step 9363, loss 0.0199727, acc 1
2016-09-06T06:28:23.029028: step 9364, loss 0.00625307, acc 1
2016-09-06T06:28:23.842526: step 9365, loss 0.0236453, acc 0.98
2016-09-06T06:28:24.615920: step 9366, loss 0.0153861, acc 1
2016-09-06T06:28:25.432766: step 9367, loss 0.00245856, acc 1
2016-09-06T06:28:26.244801: step 9368, loss 0.0221653, acc 0.98
2016-09-06T06:28:27.051261: step 9369, loss 0.0484822, acc 0.98
2016-09-06T06:28:27.904756: step 9370, loss 0.00374978, acc 1
2016-09-06T06:28:28.724961: step 9371, loss 0.00277605, acc 1
2016-09-06T06:28:29.522366: step 9372, loss 0.0350457, acc 0.98
2016-09-06T06:28:30.352191: step 9373, loss 0.00260114, acc 1
2016-09-06T06:28:31.195706: step 9374, loss 0.00893693, acc 1
2016-09-06T06:28:32.032086: step 9375, loss 0.0105102, acc 1
2016-09-06T06:28:32.841354: step 9376, loss 0.0457442, acc 0.98
2016-09-06T06:28:33.666501: step 9377, loss 0.0301615, acc 0.98
2016-09-06T06:28:34.471607: step 9378, loss 0.027544, acc 0.98
2016-09-06T06:28:35.282901: step 9379, loss 0.0027136, acc 1
2016-09-06T06:28:36.118353: step 9380, loss 0.0143524, acc 1
2016-09-06T06:28:36.934567: step 9381, loss 0.00278748, acc 1
2016-09-06T06:28:37.739897: step 9382, loss 0.017392, acc 0.98
2016-09-06T06:28:38.581003: step 9383, loss 0.00796245, acc 1
2016-09-06T06:28:39.376168: step 9384, loss 0.0131497, acc 1
2016-09-06T06:28:40.197440: step 9385, loss 0.0502336, acc 0.98
2016-09-06T06:28:41.020525: step 9386, loss 0.0204808, acc 1
2016-09-06T06:28:41.834923: step 9387, loss 0.00357392, acc 1
2016-09-06T06:28:42.670071: step 9388, loss 0.00360674, acc 1
2016-09-06T06:28:43.529649: step 9389, loss 0.0156241, acc 1
2016-09-06T06:28:44.350643: step 9390, loss 0.0170598, acc 1
2016-09-06T06:28:45.159406: step 9391, loss 0.0136589, acc 1
2016-09-06T06:28:45.958518: step 9392, loss 0.00222402, acc 1
2016-09-06T06:28:46.782392: step 9393, loss 0.0126775, acc 1
2016-09-06T06:28:47.569405: step 9394, loss 0.0464436, acc 0.96
2016-09-06T06:28:48.382242: step 9395, loss 0.0112879, acc 1
2016-09-06T06:28:49.206604: step 9396, loss 0.10095, acc 0.96
2016-09-06T06:28:50.003994: step 9397, loss 0.00398421, acc 1
2016-09-06T06:28:50.797857: step 9398, loss 0.0231573, acc 0.98
2016-09-06T06:28:51.620048: step 9399, loss 0.00251101, acc 1
2016-09-06T06:28:52.403712: step 9400, loss 0.00222186, acc 1

Evaluation:
2016-09-06T06:28:56.152778: step 9400, loss 2.59753, acc 0.729831

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-9400

2016-09-06T06:28:58.104939: step 9401, loss 0.00533062, acc 1
2016-09-06T06:28:58.914875: step 9402, loss 0.0162443, acc 1
2016-09-06T06:28:59.743970: step 9403, loss 0.0230411, acc 1
2016-09-06T06:29:00.577457: step 9404, loss 0.0238669, acc 0.98
2016-09-06T06:29:01.396315: step 9405, loss 0.0164842, acc 1
2016-09-06T06:29:02.190647: step 9406, loss 0.0801111, acc 0.96
2016-09-06T06:29:02.975212: step 9407, loss 0.00463241, acc 1
2016-09-06T06:29:03.723677: step 9408, loss 0.00258888, acc 1
2016-09-06T06:29:04.526082: step 9409, loss 0.0152333, acc 1
2016-09-06T06:29:05.342468: step 9410, loss 0.0199763, acc 0.98
2016-09-06T06:29:06.166212: step 9411, loss 0.0462964, acc 0.98
2016-09-06T06:29:06.957879: step 9412, loss 0.0131807, acc 1
2016-09-06T06:29:07.756194: step 9413, loss 0.00772681, acc 1
2016-09-06T06:29:08.564011: step 9414, loss 0.0238739, acc 1
2016-09-06T06:29:09.360346: step 9415, loss 0.0179376, acc 0.98
2016-09-06T06:29:10.161690: step 9416, loss 0.0244617, acc 0.98
2016-09-06T06:29:11.053815: step 9417, loss 0.0136907, acc 1
2016-09-06T06:29:11.852063: step 9418, loss 0.018823, acc 1
2016-09-06T06:29:12.656324: step 9419, loss 0.0107285, acc 1
2016-09-06T06:29:13.468721: step 9420, loss 0.00987343, acc 1
2016-09-06T06:29:14.270033: step 9421, loss 0.0161066, acc 1
2016-09-06T06:29:15.043452: step 9422, loss 0.0253123, acc 1
2016-09-06T06:29:15.857456: step 9423, loss 0.00278209, acc 1
2016-09-06T06:29:16.639796: step 9424, loss 0.0153718, acc 1
2016-09-06T06:29:17.492871: step 9425, loss 0.01397, acc 1
2016-09-06T06:29:18.293567: step 9426, loss 0.00902531, acc 1
2016-09-06T06:29:19.100770: step 9427, loss 0.0167047, acc 1
2016-09-06T06:29:19.905760: step 9428, loss 0.0177534, acc 0.98
2016-09-06T06:29:20.738249: step 9429, loss 0.00299953, acc 1
2016-09-06T06:29:21.539896: step 9430, loss 0.0227944, acc 1
2016-09-06T06:29:22.349542: step 9431, loss 0.00646093, acc 1
2016-09-06T06:29:23.151884: step 9432, loss 0.011799, acc 1
2016-09-06T06:29:23.928535: step 9433, loss 0.00322709, acc 1
2016-09-06T06:29:24.737651: step 9434, loss 0.0452892, acc 0.98
2016-09-06T06:29:25.564066: step 9435, loss 0.0146407, acc 1
2016-09-06T06:29:26.363607: step 9436, loss 0.00955604, acc 1
2016-09-06T06:29:27.170304: step 9437, loss 0.00500896, acc 1
2016-09-06T06:29:28.017914: step 9438, loss 0.011709, acc 1
2016-09-06T06:29:28.840496: step 9439, loss 0.103774, acc 0.98
2016-09-06T06:29:29.643910: step 9440, loss 0.00951911, acc 1
2016-09-06T06:29:30.498433: step 9441, loss 0.0158195, acc 1
2016-09-06T06:29:31.299913: step 9442, loss 0.101864, acc 0.98
2016-09-06T06:29:32.118177: step 9443, loss 0.0136486, acc 1
2016-09-06T06:29:32.950341: step 9444, loss 0.00229923, acc 1
2016-09-06T06:29:33.801160: step 9445, loss 0.0111221, acc 1
2016-09-06T06:29:34.654359: step 9446, loss 0.047823, acc 0.96
2016-09-06T06:29:35.502586: step 9447, loss 0.0035216, acc 1
2016-09-06T06:29:36.312793: step 9448, loss 0.00203703, acc 1
2016-09-06T06:29:37.112373: step 9449, loss 0.00196527, acc 1
2016-09-06T06:29:37.935332: step 9450, loss 0.0126673, acc 1
2016-09-06T06:29:38.770267: step 9451, loss 0.0621237, acc 0.98
2016-09-06T06:29:39.553533: step 9452, loss 0.00317997, acc 1
2016-09-06T06:29:40.384540: step 9453, loss 0.00271434, acc 1
2016-09-06T06:29:41.199170: step 9454, loss 0.0210323, acc 1
2016-09-06T06:29:42.010677: step 9455, loss 0.0186764, acc 0.98
2016-09-06T06:29:42.825316: step 9456, loss 0.0130931, acc 1
2016-09-06T06:29:43.651412: step 9457, loss 0.00350583, acc 1
2016-09-06T06:29:44.464940: step 9458, loss 0.00232909, acc 1
2016-09-06T06:29:45.257465: step 9459, loss 0.0258162, acc 0.98
2016-09-06T06:29:46.053434: step 9460, loss 0.021019, acc 1
2016-09-06T06:29:46.839286: step 9461, loss 0.030918, acc 0.98
2016-09-06T06:29:47.635952: step 9462, loss 0.0222371, acc 0.98
2016-09-06T06:29:48.480620: step 9463, loss 0.0208162, acc 0.98
2016-09-06T06:29:49.243449: step 9464, loss 0.00352232, acc 1
2016-09-06T06:29:50.026593: step 9465, loss 0.00982091, acc 1
2016-09-06T06:29:50.825084: step 9466, loss 0.0312664, acc 0.98
2016-09-06T06:29:51.610577: step 9467, loss 0.00868366, acc 1
2016-09-06T06:29:52.416676: step 9468, loss 0.00226068, acc 1
2016-09-06T06:29:53.218665: step 9469, loss 0.0244994, acc 0.98
2016-09-06T06:29:54.024029: step 9470, loss 0.00471306, acc 1
2016-09-06T06:29:54.845958: step 9471, loss 0.00239147, acc 1
2016-09-06T06:29:55.672518: step 9472, loss 0.0188249, acc 0.98
2016-09-06T06:29:56.461010: step 9473, loss 0.0506763, acc 0.98
2016-09-06T06:29:57.295323: step 9474, loss 0.0333956, acc 0.98
2016-09-06T06:29:58.122331: step 9475, loss 0.0262192, acc 1
2016-09-06T06:29:58.915510: step 9476, loss 0.0310178, acc 0.98
2016-09-06T06:29:59.711402: step 9477, loss 0.0282389, acc 0.98
2016-09-06T06:30:00.564946: step 9478, loss 0.0028856, acc 1
2016-09-06T06:30:01.332366: step 9479, loss 0.00761734, acc 1
2016-09-06T06:30:02.176692: step 9480, loss 0.0140323, acc 1
2016-09-06T06:30:02.972405: step 9481, loss 0.0206225, acc 0.98
2016-09-06T06:30:03.747968: step 9482, loss 0.012662, acc 1
2016-09-06T06:30:04.570980: step 9483, loss 0.00766921, acc 1
2016-09-06T06:30:05.400983: step 9484, loss 0.030251, acc 0.98
2016-09-06T06:30:06.195235: step 9485, loss 0.0142509, acc 1
2016-09-06T06:30:06.978276: step 9486, loss 0.0385244, acc 0.98
2016-09-06T06:30:07.809680: step 9487, loss 0.00520676, acc 1
2016-09-06T06:30:08.602873: step 9488, loss 0.00576417, acc 1
2016-09-06T06:30:09.421713: step 9489, loss 0.00364128, acc 1
2016-09-06T06:30:10.212580: step 9490, loss 0.0173282, acc 0.98
2016-09-06T06:30:11.022350: step 9491, loss 0.0581776, acc 0.98
2016-09-06T06:30:11.838292: step 9492, loss 0.00269509, acc 1
2016-09-06T06:30:12.661551: step 9493, loss 0.0100846, acc 1
2016-09-06T06:30:13.453500: step 9494, loss 0.0024195, acc 1
2016-09-06T06:30:14.273764: step 9495, loss 0.00296653, acc 1
2016-09-06T06:30:15.072456: step 9496, loss 0.078548, acc 0.98
2016-09-06T06:30:15.843175: step 9497, loss 0.0150292, acc 1
2016-09-06T06:30:16.650482: step 9498, loss 0.00579464, acc 1
2016-09-06T06:30:17.490547: step 9499, loss 0.111736, acc 0.96
2016-09-06T06:30:18.267517: step 9500, loss 0.005356, acc 1

Evaluation:
2016-09-06T06:30:22.002716: step 9500, loss 2.3653, acc 0.732645

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-9500

2016-09-06T06:30:23.933398: step 9501, loss 0.00355211, acc 1
2016-09-06T06:30:24.742207: step 9502, loss 0.00870027, acc 1
2016-09-06T06:30:25.558401: step 9503, loss 0.010633, acc 1
2016-09-06T06:30:26.368357: step 9504, loss 0.00191801, acc 1
2016-09-06T06:30:27.197426: step 9505, loss 0.00952125, acc 1
2016-09-06T06:30:27.999679: step 9506, loss 0.00790971, acc 1
2016-09-06T06:30:28.799554: step 9507, loss 0.0205829, acc 0.98
2016-09-06T06:30:29.620685: step 9508, loss 0.00935049, acc 1
2016-09-06T06:30:30.407137: step 9509, loss 0.0108037, acc 1
2016-09-06T06:30:31.217218: step 9510, loss 0.00848457, acc 1
2016-09-06T06:30:32.089025: step 9511, loss 0.0214599, acc 1
2016-09-06T06:30:32.908771: step 9512, loss 0.00773382, acc 1
2016-09-06T06:30:33.726890: step 9513, loss 0.0409361, acc 0.98
2016-09-06T06:30:34.540096: step 9514, loss 0.0184184, acc 1
2016-09-06T06:30:35.365835: step 9515, loss 0.0722516, acc 0.96
2016-09-06T06:30:36.205633: step 9516, loss 0.0185692, acc 0.98
2016-09-06T06:30:37.049990: step 9517, loss 0.0229713, acc 1
2016-09-06T06:30:37.846338: step 9518, loss 0.0436579, acc 0.98
2016-09-06T06:30:38.658464: step 9519, loss 0.00223541, acc 1
2016-09-06T06:30:39.470901: step 9520, loss 0.0194609, acc 1
2016-09-06T06:30:40.287813: step 9521, loss 0.00424986, acc 1
2016-09-06T06:30:41.096713: step 9522, loss 0.0217076, acc 0.98
2016-09-06T06:30:41.970835: step 9523, loss 0.0388756, acc 0.98
2016-09-06T06:30:42.797286: step 9524, loss 0.00289144, acc 1
2016-09-06T06:30:43.587413: step 9525, loss 0.0522721, acc 0.98
2016-09-06T06:30:44.406872: step 9526, loss 0.00225183, acc 1
2016-09-06T06:30:45.226663: step 9527, loss 0.0329899, acc 0.98
2016-09-06T06:30:46.017076: step 9528, loss 0.0152777, acc 1
2016-09-06T06:30:46.846936: step 9529, loss 0.00213638, acc 1
2016-09-06T06:30:47.673282: step 9530, loss 0.0101469, acc 1
2016-09-06T06:30:48.451836: step 9531, loss 0.0122148, acc 1
2016-09-06T06:30:49.274926: step 9532, loss 0.0212715, acc 0.98
2016-09-06T06:30:50.103571: step 9533, loss 0.0216043, acc 0.98
2016-09-06T06:30:50.884949: step 9534, loss 0.0312555, acc 0.98
2016-09-06T06:30:51.704400: step 9535, loss 0.0377765, acc 0.96
2016-09-06T06:30:52.516410: step 9536, loss 0.048859, acc 0.96
2016-09-06T06:30:53.309333: step 9537, loss 0.0455215, acc 0.98
2016-09-06T06:30:54.117087: step 9538, loss 0.00224958, acc 1
2016-09-06T06:30:54.925751: step 9539, loss 0.0875992, acc 0.98
2016-09-06T06:30:55.716289: step 9540, loss 0.0269095, acc 0.98
2016-09-06T06:30:56.535446: step 9541, loss 0.00902801, acc 1
2016-09-06T06:30:57.355164: step 9542, loss 0.0119833, acc 1
2016-09-06T06:30:58.156574: step 9543, loss 0.026051, acc 0.98
2016-09-06T06:30:58.944484: step 9544, loss 0.00314812, acc 1
2016-09-06T06:30:59.763025: step 9545, loss 0.00842325, acc 1
2016-09-06T06:31:00.581581: step 9546, loss 0.036379, acc 1
2016-09-06T06:31:01.390442: step 9547, loss 0.00421644, acc 1
2016-09-06T06:31:02.211383: step 9548, loss 0.00218578, acc 1
2016-09-06T06:31:03.007362: step 9549, loss 0.0366336, acc 0.98
2016-09-06T06:31:03.817157: step 9550, loss 0.00279794, acc 1
2016-09-06T06:31:04.609388: step 9551, loss 0.030298, acc 0.98
2016-09-06T06:31:05.416275: step 9552, loss 0.0171601, acc 1
2016-09-06T06:31:06.200907: step 9553, loss 0.0061948, acc 1
2016-09-06T06:31:07.051563: step 9554, loss 0.0121589, acc 1
2016-09-06T06:31:07.843821: step 9555, loss 0.0140254, acc 1
2016-09-06T06:31:08.635235: step 9556, loss 0.003491, acc 1
2016-09-06T06:31:09.447243: step 9557, loss 0.0181362, acc 0.98
2016-09-06T06:31:10.226862: step 9558, loss 0.0190548, acc 0.98
2016-09-06T06:31:11.022708: step 9559, loss 0.00229138, acc 1
2016-09-06T06:31:11.844563: step 9560, loss 0.010165, acc 1
2016-09-06T06:31:12.639545: step 9561, loss 0.0638149, acc 0.98
2016-09-06T06:31:13.472788: step 9562, loss 0.00242591, acc 1
2016-09-06T06:31:14.284600: step 9563, loss 0.00355899, acc 1
2016-09-06T06:31:15.053652: step 9564, loss 0.0163295, acc 1
2016-09-06T06:31:15.881417: step 9565, loss 0.0117818, acc 1
2016-09-06T06:31:16.665195: step 9566, loss 0.00810213, acc 1
2016-09-06T06:31:17.461395: step 9567, loss 0.0022684, acc 1
2016-09-06T06:31:18.256412: step 9568, loss 0.0420045, acc 1
2016-09-06T06:31:19.077367: step 9569, loss 0.019838, acc 1
2016-09-06T06:31:19.865038: step 9570, loss 0.0434809, acc 0.98
2016-09-06T06:31:20.651732: step 9571, loss 0.0230353, acc 1
2016-09-06T06:31:21.467810: step 9572, loss 0.024042, acc 0.98
2016-09-06T06:31:22.289428: step 9573, loss 0.00228754, acc 1
2016-09-06T06:31:23.093731: step 9574, loss 0.048313, acc 1
2016-09-06T06:31:23.914178: step 9575, loss 0.0164833, acc 1
2016-09-06T06:31:24.724440: step 9576, loss 0.00573722, acc 1
2016-09-06T06:31:25.526722: step 9577, loss 0.0357049, acc 0.96
2016-09-06T06:31:26.360560: step 9578, loss 0.00241237, acc 1
2016-09-06T06:31:27.115602: step 9579, loss 0.0167119, acc 1
2016-09-06T06:31:27.914879: step 9580, loss 0.0480412, acc 0.98
2016-09-06T06:31:28.733555: step 9581, loss 0.00407473, acc 1
2016-09-06T06:31:29.535242: step 9582, loss 0.0606642, acc 0.98
2016-09-06T06:31:30.352913: step 9583, loss 0.0132243, acc 1
2016-09-06T06:31:31.168892: step 9584, loss 0.00378721, acc 1
2016-09-06T06:31:31.976758: step 9585, loss 0.0230205, acc 1
2016-09-06T06:31:32.768020: step 9586, loss 0.0328733, acc 0.98
2016-09-06T06:31:33.578529: step 9587, loss 0.0168431, acc 1
2016-09-06T06:31:34.384535: step 9588, loss 0.0428945, acc 0.98
2016-09-06T06:31:35.181127: step 9589, loss 0.0116397, acc 1
2016-09-06T06:31:36.013963: step 9590, loss 0.00909588, acc 1
2016-09-06T06:31:36.784725: step 9591, loss 0.0022055, acc 1
2016-09-06T06:31:37.586063: step 9592, loss 0.00292283, acc 1
2016-09-06T06:31:38.392913: step 9593, loss 0.017768, acc 0.98
2016-09-06T06:31:39.173637: step 9594, loss 0.0174593, acc 1
2016-09-06T06:31:39.987097: step 9595, loss 0.0230628, acc 1
2016-09-06T06:31:40.823682: step 9596, loss 0.00379874, acc 1
2016-09-06T06:31:41.621261: step 9597, loss 0.0202633, acc 0.98
2016-09-06T06:31:42.410622: step 9598, loss 0.0288825, acc 0.98
2016-09-06T06:31:43.208084: step 9599, loss 0.0301888, acc 0.98
2016-09-06T06:31:43.926235: step 9600, loss 0.00221173, acc 1

Evaluation:
2016-09-06T06:31:47.714044: step 9600, loss 2.70269, acc 0.727017

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-9600

2016-09-06T06:31:49.700285: step 9601, loss 0.0022075, acc 1
2016-09-06T06:31:50.487368: step 9602, loss 0.0135747, acc 1
2016-09-06T06:31:51.270169: step 9603, loss 0.00675071, acc 1
2016-09-06T06:31:52.079679: step 9604, loss 0.0114687, acc 1
2016-09-06T06:31:52.882220: step 9605, loss 0.00592588, acc 1
2016-09-06T06:31:53.699533: step 9606, loss 0.0302218, acc 1
2016-09-06T06:31:54.538525: step 9607, loss 0.0656537, acc 0.98
2016-09-06T06:31:55.355451: step 9608, loss 0.0296425, acc 0.98
2016-09-06T06:31:56.153123: step 9609, loss 0.0455632, acc 0.98
2016-09-06T06:31:56.938410: step 9610, loss 0.0221743, acc 1
2016-09-06T06:31:57.769652: step 9611, loss 0.00207044, acc 1
2016-09-06T06:31:58.578707: step 9612, loss 0.00277277, acc 1
2016-09-06T06:31:59.392685: step 9613, loss 0.0171878, acc 1
2016-09-06T06:32:00.230805: step 9614, loss 0.0128405, acc 1
2016-09-06T06:32:01.030488: step 9615, loss 0.0506134, acc 0.98
2016-09-06T06:32:01.846417: step 9616, loss 0.00223255, acc 1
2016-09-06T06:32:02.689176: step 9617, loss 0.0153068, acc 1
2016-09-06T06:32:03.499398: step 9618, loss 0.0173836, acc 0.98
2016-09-06T06:32:04.316367: step 9619, loss 0.0165229, acc 1
2016-09-06T06:32:05.170073: step 9620, loss 0.00212039, acc 1
2016-09-06T06:32:05.963564: step 9621, loss 0.00645654, acc 1
2016-09-06T06:32:06.777102: step 9622, loss 0.102901, acc 0.96
2016-09-06T06:32:07.606524: step 9623, loss 0.0208367, acc 0.98
2016-09-06T06:32:08.427607: step 9624, loss 0.00779163, acc 1
2016-09-06T06:32:09.231840: step 9625, loss 0.00287728, acc 1
2016-09-06T06:32:10.062858: step 9626, loss 0.073293, acc 0.96
2016-09-06T06:32:10.879734: step 9627, loss 0.0215548, acc 0.98
2016-09-06T06:32:11.698251: step 9628, loss 0.0450168, acc 0.98
2016-09-06T06:32:12.526705: step 9629, loss 0.0459324, acc 0.98
2016-09-06T06:32:13.328526: step 9630, loss 0.00581048, acc 1
2016-09-06T06:32:14.127322: step 9631, loss 0.0044825, acc 1
2016-09-06T06:32:14.961830: step 9632, loss 0.00948099, acc 1
2016-09-06T06:32:15.773369: step 9633, loss 0.0184913, acc 1
2016-09-06T06:32:16.570263: step 9634, loss 0.0342306, acc 0.96
2016-09-06T06:32:17.408478: step 9635, loss 0.00451973, acc 1
2016-09-06T06:32:18.224048: step 9636, loss 0.0417201, acc 0.96
2016-09-06T06:32:19.057728: step 9637, loss 0.0236519, acc 0.98
2016-09-06T06:32:19.899492: step 9638, loss 0.0257562, acc 0.98
2016-09-06T06:32:20.729949: step 9639, loss 0.0258975, acc 0.98
2016-09-06T06:32:21.535679: step 9640, loss 0.00210899, acc 1
2016-09-06T06:32:22.328010: step 9641, loss 0.0309509, acc 0.98
2016-09-06T06:32:23.158231: step 9642, loss 0.0131247, acc 1
2016-09-06T06:32:23.937708: step 9643, loss 0.0520661, acc 0.96
2016-09-06T06:32:24.768144: step 9644, loss 0.00186894, acc 1
2016-09-06T06:32:25.623006: step 9645, loss 0.00260893, acc 1
2016-09-06T06:32:26.406994: step 9646, loss 0.0162884, acc 1
2016-09-06T06:32:27.198240: step 9647, loss 0.0205707, acc 0.98
2016-09-06T06:32:28.060789: step 9648, loss 0.0199623, acc 0.98
2016-09-06T06:32:28.911561: step 9649, loss 0.00826702, acc 1
2016-09-06T06:32:29.751771: step 9650, loss 0.00193724, acc 1
2016-09-06T06:32:30.585360: step 9651, loss 0.0129814, acc 1
2016-09-06T06:32:31.409541: step 9652, loss 0.00905118, acc 1
2016-09-06T06:32:32.224421: step 9653, loss 0.0173242, acc 1
2016-09-06T06:32:33.081332: step 9654, loss 0.017549, acc 0.98
2016-09-06T06:32:33.920303: step 9655, loss 0.0074642, acc 1
2016-09-06T06:32:34.725604: step 9656, loss 0.00502373, acc 1
2016-09-06T06:32:35.570114: step 9657, loss 0.0502895, acc 0.98
2016-09-06T06:32:36.400749: step 9658, loss 0.00435554, acc 1
2016-09-06T06:32:37.208570: step 9659, loss 0.0145795, acc 1
2016-09-06T06:32:38.038384: step 9660, loss 0.00221115, acc 1
2016-09-06T06:32:38.879171: step 9661, loss 0.0128533, acc 1
2016-09-06T06:32:39.650963: step 9662, loss 0.00202334, acc 1
2016-09-06T06:32:40.477484: step 9663, loss 0.00207972, acc 1
2016-09-06T06:32:41.298576: step 9664, loss 0.0378869, acc 0.98
2016-09-06T06:32:42.089662: step 9665, loss 0.0134489, acc 1
2016-09-06T06:32:42.904004: step 9666, loss 0.0428019, acc 0.96
2016-09-06T06:32:43.710991: step 9667, loss 0.0235918, acc 1
2016-09-06T06:32:44.509681: step 9668, loss 0.0310725, acc 0.98
2016-09-06T06:32:45.323606: step 9669, loss 0.00714311, acc 1
2016-09-06T06:32:46.146028: step 9670, loss 0.00261006, acc 1
2016-09-06T06:32:46.947734: step 9671, loss 0.0165101, acc 0.98
2016-09-06T06:32:47.738886: step 9672, loss 0.00211096, acc 1
2016-09-06T06:32:48.544724: step 9673, loss 0.0165204, acc 1
2016-09-06T06:32:49.325853: step 9674, loss 0.00917828, acc 1
2016-09-06T06:32:50.125469: step 9675, loss 0.0159194, acc 1
2016-09-06T06:32:50.925898: step 9676, loss 0.0169424, acc 0.98
2016-09-06T06:32:51.725415: step 9677, loss 0.00536342, acc 1
2016-09-06T06:32:52.525626: step 9678, loss 0.108845, acc 0.96
2016-09-06T06:32:53.341123: step 9679, loss 0.0183251, acc 1
2016-09-06T06:32:54.175624: step 9680, loss 0.00219829, acc 1
2016-09-06T06:32:54.980746: step 9681, loss 0.00194889, acc 1
2016-09-06T06:32:55.854847: step 9682, loss 0.0197324, acc 0.98
2016-09-06T06:32:56.674893: step 9683, loss 0.021833, acc 0.98
2016-09-06T06:32:57.487057: step 9684, loss 0.00598984, acc 1
2016-09-06T06:32:58.327451: step 9685, loss 0.00267896, acc 1
2016-09-06T06:32:59.159520: step 9686, loss 0.0244103, acc 0.98
2016-09-06T06:32:59.973612: step 9687, loss 0.00178962, acc 1
2016-09-06T06:33:00.813186: step 9688, loss 0.0369356, acc 0.98
2016-09-06T06:33:01.634646: step 9689, loss 0.0106964, acc 1
2016-09-06T06:33:02.472190: step 9690, loss 0.00180912, acc 1
2016-09-06T06:33:03.324382: step 9691, loss 0.0600277, acc 0.98
2016-09-06T06:33:04.142799: step 9692, loss 0.0400999, acc 0.98
2016-09-06T06:33:04.968329: step 9693, loss 0.00510104, acc 1
2016-09-06T06:33:05.840698: step 9694, loss 0.00158077, acc 1
2016-09-06T06:33:06.673454: step 9695, loss 0.0598981, acc 0.96
2016-09-06T06:33:07.480913: step 9696, loss 0.0018778, acc 1
2016-09-06T06:33:08.305266: step 9697, loss 0.00310789, acc 1
2016-09-06T06:33:09.148830: step 9698, loss 0.0528136, acc 0.98
2016-09-06T06:33:09.919618: step 9699, loss 0.00190903, acc 1
2016-09-06T06:33:10.756292: step 9700, loss 0.0111271, acc 1

Evaluation:
2016-09-06T06:33:14.531704: step 9700, loss 2.11689, acc 0.737336

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-9700

2016-09-06T06:33:16.412890: step 9701, loss 0.00283909, acc 1
2016-09-06T06:33:17.211412: step 9702, loss 0.0306853, acc 0.98
2016-09-06T06:33:18.037827: step 9703, loss 0.0128106, acc 1
2016-09-06T06:33:18.840502: step 9704, loss 0.00151713, acc 1
2016-09-06T06:33:19.615663: step 9705, loss 0.0387386, acc 0.98
2016-09-06T06:33:20.448626: step 9706, loss 0.00386161, acc 1
2016-09-06T06:33:21.289040: step 9707, loss 0.0574084, acc 0.98
2016-09-06T06:33:22.075028: step 9708, loss 0.0110185, acc 1
2016-09-06T06:33:22.880874: step 9709, loss 0.0384516, acc 0.98
2016-09-06T06:33:23.701543: step 9710, loss 0.0235737, acc 1
2016-09-06T06:33:24.489055: step 9711, loss 0.00171119, acc 1
2016-09-06T06:33:25.294480: step 9712, loss 0.0191898, acc 0.98
2016-09-06T06:33:26.089624: step 9713, loss 0.105693, acc 0.96
2016-09-06T06:33:26.913227: step 9714, loss 0.121252, acc 0.98
2016-09-06T06:33:27.730461: step 9715, loss 0.0242218, acc 0.98
2016-09-06T06:33:28.560812: step 9716, loss 0.00502355, acc 1
2016-09-06T06:33:29.362927: step 9717, loss 0.00613414, acc 1
2016-09-06T06:33:30.189717: step 9718, loss 0.00617686, acc 1
2016-09-06T06:33:31.014795: step 9719, loss 0.03651, acc 0.98
2016-09-06T06:33:31.814719: step 9720, loss 0.0370496, acc 0.98
2016-09-06T06:33:32.607030: step 9721, loss 0.00997898, acc 1
2016-09-06T06:33:33.435274: step 9722, loss 0.00267446, acc 1
2016-09-06T06:33:34.232885: step 9723, loss 0.0156499, acc 1
2016-09-06T06:33:35.057428: step 9724, loss 0.00597428, acc 1
2016-09-06T06:33:35.890123: step 9725, loss 0.0116397, acc 1
2016-09-06T06:33:36.706241: step 9726, loss 0.0208302, acc 0.98
2016-09-06T06:33:37.529480: step 9727, loss 0.0294742, acc 0.98
2016-09-06T06:33:38.371396: step 9728, loss 0.026618, acc 0.98
2016-09-06T06:33:39.184012: step 9729, loss 0.00993369, acc 1
2016-09-06T06:33:40.033304: step 9730, loss 0.00336194, acc 1
2016-09-06T06:33:40.899955: step 9731, loss 0.0292865, acc 1
2016-09-06T06:33:41.709730: step 9732, loss 0.0777754, acc 0.98
2016-09-06T06:33:42.519115: step 9733, loss 0.0678888, acc 0.96
2016-09-06T06:33:43.377405: step 9734, loss 0.0197532, acc 0.98
2016-09-06T06:33:44.195697: step 9735, loss 0.016711, acc 1
2016-09-06T06:33:44.990823: step 9736, loss 0.00238485, acc 1
2016-09-06T06:33:45.797537: step 9737, loss 0.0402634, acc 0.96
2016-09-06T06:33:46.624869: step 9738, loss 0.0399774, acc 0.98
2016-09-06T06:33:47.441352: step 9739, loss 0.0169886, acc 0.98
2016-09-06T06:33:48.321534: step 9740, loss 0.00199435, acc 1
2016-09-06T06:33:49.136706: step 9741, loss 0.0212484, acc 1
2016-09-06T06:33:49.910718: step 9742, loss 0.00541237, acc 1
2016-09-06T06:33:50.714013: step 9743, loss 0.0205826, acc 0.98
2016-09-06T06:33:51.529354: step 9744, loss 0.0103766, acc 1
2016-09-06T06:33:52.361596: step 9745, loss 0.0121756, acc 1
2016-09-06T06:33:53.162315: step 9746, loss 0.0133994, acc 1
2016-09-06T06:33:53.970566: step 9747, loss 0.00918369, acc 1
2016-09-06T06:33:54.737048: step 9748, loss 0.0112627, acc 1
2016-09-06T06:33:55.539157: step 9749, loss 0.00942689, acc 1
2016-09-06T06:33:56.377771: step 9750, loss 0.0736653, acc 0.96
2016-09-06T06:33:57.166021: step 9751, loss 0.105875, acc 0.98
2016-09-06T06:33:57.967414: step 9752, loss 0.00321991, acc 1
2016-09-06T06:33:58.778884: step 9753, loss 0.016701, acc 1
2016-09-06T06:33:59.572154: step 9754, loss 0.00410353, acc 1
2016-09-06T06:34:00.409794: step 9755, loss 0.00795447, acc 1
2016-09-06T06:34:01.204465: step 9756, loss 0.0684699, acc 0.96
2016-09-06T06:34:01.955402: step 9757, loss 0.0283489, acc 0.98
2016-09-06T06:34:02.768678: step 9758, loss 0.0224801, acc 1
2016-09-06T06:34:03.596618: step 9759, loss 0.0178677, acc 0.98
2016-09-06T06:34:04.394374: step 9760, loss 0.0359536, acc 1
2016-09-06T06:34:05.181418: step 9761, loss 0.0510625, acc 1
2016-09-06T06:34:05.976429: step 9762, loss 0.00800655, acc 1
2016-09-06T06:34:06.794557: step 9763, loss 0.0127981, acc 1
2016-09-06T06:34:07.634587: step 9764, loss 0.00895711, acc 1
2016-09-06T06:34:08.445470: step 9765, loss 0.0147602, acc 1
2016-09-06T06:34:09.271302: step 9766, loss 0.0160429, acc 1
2016-09-06T06:34:10.066542: step 9767, loss 0.00861738, acc 1
2016-09-06T06:34:10.880829: step 9768, loss 0.0167559, acc 0.98
2016-09-06T06:34:11.658866: step 9769, loss 0.0153682, acc 1
2016-09-06T06:34:12.473904: step 9770, loss 0.00347377, acc 1
2016-09-06T06:34:13.283674: step 9771, loss 0.00931205, acc 1
2016-09-06T06:34:14.070113: step 9772, loss 0.00767041, acc 1
2016-09-06T06:34:14.885406: step 9773, loss 0.00315877, acc 1
2016-09-06T06:34:15.714840: step 9774, loss 0.0382093, acc 0.98
2016-09-06T06:34:16.506336: step 9775, loss 0.0360053, acc 0.98
2016-09-06T06:34:17.278219: step 9776, loss 0.00566847, acc 1
2016-09-06T06:34:18.093911: step 9777, loss 0.00332341, acc 1
2016-09-06T06:34:18.899987: step 9778, loss 0.0747669, acc 0.94
2016-09-06T06:34:19.708469: step 9779, loss 0.0233525, acc 0.98
2016-09-06T06:34:20.550255: step 9780, loss 0.0370196, acc 0.96
2016-09-06T06:34:21.356151: step 9781, loss 0.0032873, acc 1
2016-09-06T06:34:22.172150: step 9782, loss 0.00669972, acc 1
2016-09-06T06:34:22.976660: step 9783, loss 0.0169481, acc 1
2016-09-06T06:34:23.782625: step 9784, loss 0.0458592, acc 0.98
2016-09-06T06:34:24.589726: step 9785, loss 0.00973628, acc 1
2016-09-06T06:34:25.416578: step 9786, loss 0.0447295, acc 0.96
2016-09-06T06:34:26.249704: step 9787, loss 0.0071876, acc 1
2016-09-06T06:34:27.073741: step 9788, loss 0.0113373, acc 1
2016-09-06T06:34:27.885723: step 9789, loss 0.0136659, acc 1
2016-09-06T06:34:28.721712: step 9790, loss 0.0114194, acc 1
2016-09-06T06:34:29.530638: step 9791, loss 0.0181557, acc 1
2016-09-06T06:34:30.290013: step 9792, loss 0.00859666, acc 1
2016-09-06T06:34:31.099150: step 9793, loss 0.0189937, acc 0.98
2016-09-06T06:34:31.930301: step 9794, loss 0.101086, acc 0.96
2016-09-06T06:34:32.779055: step 9795, loss 0.00369771, acc 1
2016-09-06T06:34:33.601349: step 9796, loss 0.0403288, acc 0.98
2016-09-06T06:34:34.416499: step 9797, loss 0.081185, acc 0.98
2016-09-06T06:34:35.242553: step 9798, loss 0.006503, acc 1
2016-09-06T06:34:36.078543: step 9799, loss 0.0212186, acc 1
2016-09-06T06:34:36.894811: step 9800, loss 0.0146103, acc 1

Evaluation:
2016-09-06T06:34:40.598348: step 9800, loss 3.3761, acc 0.732645

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-9800

2016-09-06T06:34:42.505166: step 9801, loss 0.0392873, acc 0.98
2016-09-06T06:34:43.320836: step 9802, loss 0.00362766, acc 1
2016-09-06T06:34:44.154596: step 9803, loss 0.00729774, acc 1
2016-09-06T06:34:44.984377: step 9804, loss 0.00366973, acc 1
2016-09-06T06:34:45.784767: step 9805, loss 0.00674758, acc 1
2016-09-06T06:34:46.601683: step 9806, loss 0.0465755, acc 0.98
2016-09-06T06:34:47.502282: step 9807, loss 0.048984, acc 0.96
2016-09-06T06:34:48.331813: step 9808, loss 0.00364296, acc 1
2016-09-06T06:34:49.117965: step 9809, loss 0.0275538, acc 0.98
2016-09-06T06:34:49.927188: step 9810, loss 0.00536402, acc 1
2016-09-06T06:34:50.735999: step 9811, loss 0.00382338, acc 1
2016-09-06T06:34:51.529686: step 9812, loss 0.00574016, acc 1
2016-09-06T06:34:52.346462: step 9813, loss 0.00455399, acc 1
2016-09-06T06:34:53.202293: step 9814, loss 0.0262748, acc 1
2016-09-06T06:34:54.002355: step 9815, loss 0.0256478, acc 0.98
2016-09-06T06:34:54.824338: step 9816, loss 0.00571462, acc 1
2016-09-06T06:34:55.648809: step 9817, loss 0.00575877, acc 1
2016-09-06T06:34:56.441546: step 9818, loss 0.0177669, acc 1
2016-09-06T06:34:57.247031: step 9819, loss 0.0548817, acc 0.96
2016-09-06T06:34:58.054082: step 9820, loss 0.00377356, acc 1
2016-09-06T06:34:58.820570: step 9821, loss 0.101721, acc 0.98
2016-09-06T06:34:59.637267: step 9822, loss 0.00335289, acc 1
2016-09-06T06:35:00.494306: step 9823, loss 0.00368359, acc 1
2016-09-06T06:35:01.304801: step 9824, loss 0.0129649, acc 1
2016-09-06T06:35:02.125160: step 9825, loss 0.00357016, acc 1
2016-09-06T06:35:02.943516: step 9826, loss 0.0267141, acc 0.98
2016-09-06T06:35:03.784610: step 9827, loss 0.00316786, acc 1
2016-09-06T06:35:04.596857: step 9828, loss 0.00724758, acc 1
2016-09-06T06:35:05.428538: step 9829, loss 0.0031036, acc 1
2016-09-06T06:35:06.290644: step 9830, loss 0.0134311, acc 1
2016-09-06T06:35:07.109296: step 9831, loss 0.0218004, acc 1
2016-09-06T06:35:07.941789: step 9832, loss 0.0160414, acc 1
2016-09-06T06:35:08.733841: step 9833, loss 0.0132988, acc 1
2016-09-06T06:35:09.552416: step 9834, loss 0.00558128, acc 1
2016-09-06T06:35:10.365676: step 9835, loss 0.0508548, acc 0.98
2016-09-06T06:35:11.223943: step 9836, loss 0.0068243, acc 1
2016-09-06T06:35:12.045449: step 9837, loss 0.00305358, acc 1
2016-09-06T06:35:12.876087: step 9838, loss 0.00889871, acc 1
2016-09-06T06:35:13.689384: step 9839, loss 0.0170934, acc 1
2016-09-06T06:35:14.542212: step 9840, loss 0.0099051, acc 1
2016-09-06T06:35:15.394109: step 9841, loss 0.0116975, acc 1
2016-09-06T06:35:16.222441: step 9842, loss 0.00727892, acc 1
2016-09-06T06:35:17.018836: step 9843, loss 0.0213485, acc 0.98
2016-09-06T06:35:17.842965: step 9844, loss 0.00460989, acc 1
2016-09-06T06:35:18.665125: step 9845, loss 0.0260573, acc 0.98
2016-09-06T06:35:19.459429: step 9846, loss 0.0209729, acc 1
2016-09-06T06:35:20.253248: step 9847, loss 0.0115891, acc 1
2016-09-06T06:35:21.068579: step 9848, loss 0.0147975, acc 1
2016-09-06T06:35:21.859433: step 9849, loss 0.00526754, acc 1
2016-09-06T06:35:22.656211: step 9850, loss 0.0241377, acc 1
2016-09-06T06:35:23.466618: step 9851, loss 0.0469182, acc 0.98
2016-09-06T06:35:24.259046: step 9852, loss 0.00440019, acc 1
2016-09-06T06:35:25.047424: step 9853, loss 0.0232, acc 1
2016-09-06T06:35:25.858754: step 9854, loss 0.00504798, acc 1
2016-09-06T06:35:26.625346: step 9855, loss 0.00278218, acc 1
2016-09-06T06:35:27.428339: step 9856, loss 0.0382589, acc 1
2016-09-06T06:35:28.230164: step 9857, loss 0.017647, acc 0.98
2016-09-06T06:35:29.001371: step 9858, loss 0.0232538, acc 0.98
2016-09-06T06:35:29.828962: step 9859, loss 0.0509182, acc 0.98
2016-09-06T06:35:30.644072: step 9860, loss 0.00522655, acc 1
2016-09-06T06:35:31.439532: step 9861, loss 0.0337492, acc 1
2016-09-06T06:35:32.217683: step 9862, loss 0.0306244, acc 0.98
2016-09-06T06:35:33.034383: step 9863, loss 0.00470223, acc 1
2016-09-06T06:35:33.822526: step 9864, loss 0.00378225, acc 1
2016-09-06T06:35:34.663589: step 9865, loss 0.0810708, acc 0.96
2016-09-06T06:35:35.494417: step 9866, loss 0.00258881, acc 1
2016-09-06T06:35:36.271792: step 9867, loss 0.149636, acc 0.98
2016-09-06T06:35:37.059085: step 9868, loss 0.0121723, acc 1
2016-09-06T06:35:37.884263: step 9869, loss 0.00743598, acc 1
2016-09-06T06:35:38.657757: step 9870, loss 0.0104375, acc 1
2016-09-06T06:35:39.476910: step 9871, loss 0.0228853, acc 0.98
2016-09-06T06:35:40.319295: step 9872, loss 0.26606, acc 0.96
2016-09-06T06:35:41.117907: step 9873, loss 0.0214005, acc 1
2016-09-06T06:35:41.926977: step 9874, loss 0.0247637, acc 1
2016-09-06T06:35:42.755463: step 9875, loss 0.0259627, acc 1
2016-09-06T06:35:43.570948: step 9876, loss 0.00420251, acc 1
2016-09-06T06:35:44.354960: step 9877, loss 0.033804, acc 1
2016-09-06T06:35:45.174132: step 9878, loss 0.0227773, acc 1
2016-09-06T06:35:45.961316: step 9879, loss 0.00308415, acc 1
2016-09-06T06:35:46.755911: step 9880, loss 0.0567228, acc 0.96
2016-09-06T06:35:47.573887: step 9881, loss 0.0156227, acc 1
2016-09-06T06:35:48.353574: step 9882, loss 0.00548334, acc 1
2016-09-06T06:35:49.165446: step 9883, loss 0.0266075, acc 0.98
2016-09-06T06:35:49.986568: step 9884, loss 0.0030001, acc 1
2016-09-06T06:35:50.785124: step 9885, loss 0.01204, acc 1
2016-09-06T06:35:51.592575: step 9886, loss 0.0261057, acc 0.98
2016-09-06T06:35:52.420655: step 9887, loss 0.0110741, acc 1
2016-09-06T06:35:53.222055: step 9888, loss 0.00300389, acc 1
2016-09-06T06:35:54.026208: step 9889, loss 0.0038143, acc 1
2016-09-06T06:35:54.852136: step 9890, loss 0.0650692, acc 0.96
2016-09-06T06:35:55.644588: step 9891, loss 0.00320747, acc 1
2016-09-06T06:35:56.432564: step 9892, loss 0.0313531, acc 0.98
2016-09-06T06:35:57.281461: step 9893, loss 0.0260607, acc 0.98
2016-09-06T06:35:58.066835: step 9894, loss 0.0231043, acc 0.98
2016-09-06T06:35:58.872639: step 9895, loss 0.0036802, acc 1
2016-09-06T06:35:59.708092: step 9896, loss 0.00338019, acc 1
2016-09-06T06:36:00.528577: step 9897, loss 0.0279905, acc 0.98
2016-09-06T06:36:01.328994: step 9898, loss 0.0901429, acc 0.96
2016-09-06T06:36:02.141932: step 9899, loss 0.0237351, acc 0.98
2016-09-06T06:36:02.956198: step 9900, loss 0.00385667, acc 1

Evaluation:
2016-09-06T06:36:06.675581: step 9900, loss 2.65929, acc 0.725141

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-9900

2016-09-06T06:36:08.548802: step 9901, loss 0.0257037, acc 1
2016-09-06T06:36:09.369486: step 9902, loss 0.00992763, acc 1
2016-09-06T06:36:10.154270: step 9903, loss 0.00534658, acc 1
2016-09-06T06:36:10.942123: step 9904, loss 0.0331294, acc 0.98
2016-09-06T06:36:11.757925: step 9905, loss 0.0437005, acc 0.98
2016-09-06T06:36:12.554144: step 9906, loss 0.0237112, acc 1
2016-09-06T06:36:13.359849: step 9907, loss 0.0140037, acc 1
2016-09-06T06:36:14.222761: step 9908, loss 0.0835653, acc 0.96
2016-09-06T06:36:15.045782: step 9909, loss 0.0699852, acc 0.98
2016-09-06T06:36:15.845466: step 9910, loss 0.00639734, acc 1
2016-09-06T06:36:16.691371: step 9911, loss 0.0480289, acc 0.98
2016-09-06T06:36:17.490243: step 9912, loss 0.0293207, acc 1
2016-09-06T06:36:18.327776: step 9913, loss 0.00461919, acc 1
2016-09-06T06:36:19.197982: step 9914, loss 0.00450548, acc 1
2016-09-06T06:36:19.993038: step 9915, loss 0.00860952, acc 1
2016-09-06T06:36:20.811069: step 9916, loss 0.020381, acc 1
2016-09-06T06:36:21.616008: step 9917, loss 0.104442, acc 0.96
2016-09-06T06:36:22.421275: step 9918, loss 0.0198803, acc 1
2016-09-06T06:36:23.242086: step 9919, loss 0.00627908, acc 1
2016-09-06T06:36:24.058892: step 9920, loss 0.00327127, acc 1
2016-09-06T06:36:24.876785: step 9921, loss 0.0032735, acc 1
2016-09-06T06:36:25.716875: step 9922, loss 0.0184101, acc 1
2016-09-06T06:36:26.535884: step 9923, loss 0.0262116, acc 1
2016-09-06T06:36:27.355894: step 9924, loss 0.0267956, acc 1
2016-09-06T06:36:28.174168: step 9925, loss 0.0188613, acc 1
2016-09-06T06:36:29.020109: step 9926, loss 0.0206437, acc 0.98
2016-09-06T06:36:29.833148: step 9927, loss 0.0400073, acc 0.98
2016-09-06T06:36:30.629398: step 9928, loss 0.00919791, acc 1
2016-09-06T06:36:31.461022: step 9929, loss 0.0187361, acc 1
2016-09-06T06:36:32.271134: step 9930, loss 0.03819, acc 0.98
2016-09-06T06:36:33.076568: step 9931, loss 0.125667, acc 0.98
2016-09-06T06:36:33.905924: step 9932, loss 0.00310917, acc 1
2016-09-06T06:36:34.716288: step 9933, loss 0.0929956, acc 0.96
2016-09-06T06:36:35.523475: step 9934, loss 0.0492416, acc 0.98
2016-09-06T06:36:36.343170: step 9935, loss 0.00351206, acc 1
2016-09-06T06:36:37.153551: step 9936, loss 0.015293, acc 1
2016-09-06T06:36:37.959167: step 9937, loss 0.0776907, acc 0.96
2016-09-06T06:36:38.764928: step 9938, loss 0.0229515, acc 0.98
2016-09-06T06:36:39.602786: step 9939, loss 0.0242612, acc 0.98
2016-09-06T06:36:40.421841: step 9940, loss 0.0167173, acc 1
2016-09-06T06:36:41.227907: step 9941, loss 0.0109983, acc 1
2016-09-06T06:36:42.031902: step 9942, loss 0.0589497, acc 0.96
2016-09-06T06:36:42.832042: step 9943, loss 0.0199009, acc 1
2016-09-06T06:36:43.625411: step 9944, loss 0.00316877, acc 1
2016-09-06T06:36:44.466613: step 9945, loss 0.03284, acc 1
2016-09-06T06:36:45.260384: step 9946, loss 0.00739559, acc 1
2016-09-06T06:36:46.064097: step 9947, loss 0.0136619, acc 1
2016-09-06T06:36:46.892804: step 9948, loss 0.040599, acc 0.98
2016-09-06T06:36:47.724440: step 9949, loss 0.0167221, acc 1
2016-09-06T06:36:48.553454: step 9950, loss 0.0226688, acc 1
2016-09-06T06:36:49.373905: step 9951, loss 0.0231899, acc 0.98
2016-09-06T06:36:50.190676: step 9952, loss 0.0218272, acc 0.98
2016-09-06T06:36:50.997293: step 9953, loss 0.0938303, acc 0.9
2016-09-06T06:36:51.842344: step 9954, loss 0.00422689, acc 1
2016-09-06T06:36:52.641748: step 9955, loss 0.0262111, acc 0.98
2016-09-06T06:36:53.445664: step 9956, loss 0.00755802, acc 1
2016-09-06T06:36:54.265524: step 9957, loss 0.00369107, acc 1
2016-09-06T06:36:55.076486: step 9958, loss 0.0365736, acc 0.98
2016-09-06T06:36:55.911738: step 9959, loss 0.00895367, acc 1
2016-09-06T06:36:56.730730: step 9960, loss 0.0036767, acc 1
2016-09-06T06:36:57.549608: step 9961, loss 0.0580773, acc 0.94
2016-09-06T06:36:58.363671: step 9962, loss 0.0903257, acc 0.94
2016-09-06T06:36:59.211354: step 9963, loss 0.00837611, acc 1
2016-09-06T06:37:00.004573: step 9964, loss 0.0646804, acc 0.98
2016-09-06T06:37:00.853213: step 9965, loss 0.0282644, acc 0.98
2016-09-06T06:37:01.686002: step 9966, loss 0.0228804, acc 1
2016-09-06T06:37:02.507012: step 9967, loss 0.0442089, acc 0.96
2016-09-06T06:37:03.315044: step 9968, loss 0.00343248, acc 1
2016-09-06T06:37:04.120572: step 9969, loss 0.012171, acc 1
2016-09-06T06:37:04.961554: step 9970, loss 0.0191367, acc 0.98
2016-09-06T06:37:05.755910: step 9971, loss 0.0431518, acc 0.98
2016-09-06T06:37:06.552673: step 9972, loss 0.0389321, acc 1
2016-09-06T06:37:07.366467: step 9973, loss 0.0106514, acc 1
2016-09-06T06:37:08.146576: step 9974, loss 0.00522865, acc 1
2016-09-06T06:37:08.967691: step 9975, loss 0.0133791, acc 1
2016-09-06T06:37:09.816091: step 9976, loss 0.0328167, acc 0.98
2016-09-06T06:37:10.619763: step 9977, loss 0.0196638, acc 0.98
2016-09-06T06:37:11.413440: step 9978, loss 0.0277448, acc 1
2016-09-06T06:37:12.233004: step 9979, loss 0.0357251, acc 0.98
2016-09-06T06:37:13.037401: step 9980, loss 0.0185628, acc 1
2016-09-06T06:37:13.827035: step 9981, loss 0.0167638, acc 1
2016-09-06T06:37:14.639324: step 9982, loss 0.0274637, acc 0.98
2016-09-06T06:37:15.396883: step 9983, loss 0.00313377, acc 1
2016-09-06T06:37:16.160799: step 9984, loss 0.00304141, acc 1
2016-09-06T06:37:16.925356: step 9985, loss 0.0330728, acc 0.98
2016-09-06T06:37:17.721588: step 9986, loss 0.0339545, acc 0.98
2016-09-06T06:37:18.583433: step 9987, loss 0.0244055, acc 1
2016-09-06T06:37:19.412936: step 9988, loss 0.00599783, acc 1
2016-09-06T06:37:20.213828: step 9989, loss 0.00490511, acc 1
2016-09-06T06:37:21.013804: step 9990, loss 0.0127776, acc 1
2016-09-06T06:37:21.835838: step 9991, loss 0.00420295, acc 1
2016-09-06T06:37:22.616258: step 9992, loss 0.00476573, acc 1
2016-09-06T06:37:23.422710: step 9993, loss 0.015484, acc 1
2016-09-06T06:37:24.262790: step 9994, loss 0.0315602, acc 0.98
2016-09-06T06:37:25.053464: step 9995, loss 0.0186277, acc 0.98
2016-09-06T06:37:25.859953: step 9996, loss 0.00489908, acc 1
2016-09-06T06:37:26.670397: step 9997, loss 0.0191623, acc 0.98
2016-09-06T06:37:27.436713: step 9998, loss 0.0274992, acc 0.98
2016-09-06T06:37:28.251500: step 9999, loss 0.0323793, acc 0.98
2016-09-06T06:37:29.091694: step 10000, loss 0.00380724, acc 1

Evaluation:
2016-09-06T06:37:32.814038: step 10000, loss 3.0801, acc 0.727955

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-10000

2016-09-06T06:37:34.696015: step 10001, loss 0.00302381, acc 1
2016-09-06T06:37:35.527810: step 10002, loss 0.00539924, acc 1
2016-09-06T06:37:36.334866: step 10003, loss 0.00557812, acc 1
2016-09-06T06:37:37.138235: step 10004, loss 0.003308, acc 1
2016-09-06T06:37:37.986949: step 10005, loss 0.0151333, acc 1
2016-09-06T06:37:38.808503: step 10006, loss 0.0483694, acc 0.98
2016-09-06T06:37:39.597121: step 10007, loss 0.0218793, acc 0.98
2016-09-06T06:37:40.427029: step 10008, loss 0.0728813, acc 0.96
2016-09-06T06:37:41.277598: step 10009, loss 0.0449492, acc 0.98
2016-09-06T06:37:42.110448: step 10010, loss 0.0641763, acc 0.98
2016-09-06T06:37:42.917442: step 10011, loss 0.0146482, acc 1
2016-09-06T06:37:43.736255: step 10012, loss 0.0181284, acc 0.98
2016-09-06T06:37:44.523768: step 10013, loss 0.132799, acc 0.96
2016-09-06T06:37:45.319784: step 10014, loss 0.00257596, acc 1
2016-09-06T06:37:46.139792: step 10015, loss 0.00553367, acc 1
2016-09-06T06:37:46.982346: step 10016, loss 0.0242881, acc 0.98
2016-09-06T06:37:47.796875: step 10017, loss 0.0154629, acc 1
2016-09-06T06:37:48.610159: step 10018, loss 0.018073, acc 1
2016-09-06T06:37:49.388493: step 10019, loss 0.0127874, acc 1
2016-09-06T06:37:50.176716: step 10020, loss 0.0242994, acc 1
2016-09-06T06:37:51.006789: step 10021, loss 0.00425923, acc 1
2016-09-06T06:37:51.816590: step 10022, loss 0.0202374, acc 1
2016-09-06T06:37:52.631702: step 10023, loss 0.0165626, acc 1
2016-09-06T06:37:53.435592: step 10024, loss 0.00227624, acc 1
2016-09-06T06:37:54.236271: step 10025, loss 0.0281082, acc 0.98
2016-09-06T06:37:55.056710: step 10026, loss 0.0081434, acc 1
2016-09-06T06:37:55.858338: step 10027, loss 0.0703061, acc 0.98
2016-09-06T06:37:56.650919: step 10028, loss 0.00374584, acc 1
2016-09-06T06:37:57.450054: step 10029, loss 0.0129029, acc 1
2016-09-06T06:37:58.268083: step 10030, loss 0.0222574, acc 0.98
2016-09-06T06:37:59.046912: step 10031, loss 0.0243618, acc 1
2016-09-06T06:37:59.830637: step 10032, loss 0.00338582, acc 1
2016-09-06T06:38:00.667577: step 10033, loss 0.0631327, acc 0.98
2016-09-06T06:38:01.469529: step 10034, loss 0.00362972, acc 1
2016-09-06T06:38:02.285657: step 10035, loss 0.00225384, acc 1
2016-09-06T06:38:03.088223: step 10036, loss 0.0777411, acc 0.96
2016-09-06T06:38:03.855212: step 10037, loss 0.0676917, acc 0.98
2016-09-06T06:38:04.642886: step 10038, loss 0.00389886, acc 1
2016-09-06T06:38:05.474586: step 10039, loss 0.0648728, acc 0.98
2016-09-06T06:38:06.274823: step 10040, loss 0.01352, acc 1
2016-09-06T06:38:07.057021: step 10041, loss 0.00400376, acc 1
2016-09-06T06:38:07.891487: step 10042, loss 0.0463816, acc 0.98
2016-09-06T06:38:08.705072: step 10043, loss 0.0171491, acc 0.98
2016-09-06T06:38:09.510170: step 10044, loss 0.00252647, acc 1
2016-09-06T06:38:10.352293: step 10045, loss 0.161734, acc 0.94
2016-09-06T06:38:11.154212: step 10046, loss 0.00269168, acc 1
2016-09-06T06:38:11.952798: step 10047, loss 0.00244547, acc 1
2016-09-06T06:38:12.758291: step 10048, loss 0.0229326, acc 1
2016-09-06T06:38:13.539068: step 10049, loss 0.00697175, acc 1
2016-09-06T06:38:14.349003: step 10050, loss 0.00465217, acc 1
2016-09-06T06:38:15.184206: step 10051, loss 0.0282145, acc 0.98
2016-09-06T06:38:15.970496: step 10052, loss 0.0519304, acc 0.96
2016-09-06T06:38:16.780021: step 10053, loss 0.04292, acc 0.98
2016-09-06T06:38:17.590676: step 10054, loss 0.0284088, acc 0.98
2016-09-06T06:38:18.366098: step 10055, loss 0.00954477, acc 1
2016-09-06T06:38:19.169014: step 10056, loss 0.0530095, acc 0.98
2016-09-06T06:38:19.993574: step 10057, loss 0.0169037, acc 1
2016-09-06T06:38:20.817244: step 10058, loss 0.00314854, acc 1
2016-09-06T06:38:21.598261: step 10059, loss 0.0872036, acc 0.98
2016-09-06T06:38:22.424865: step 10060, loss 0.0267121, acc 0.98
2016-09-06T06:38:23.208492: step 10061, loss 0.0267747, acc 1
2016-09-06T06:38:24.004747: step 10062, loss 0.0324383, acc 0.98
2016-09-06T06:38:24.816662: step 10063, loss 0.00923464, acc 1
2016-09-06T06:38:25.617956: step 10064, loss 0.0158023, acc 1
2016-09-06T06:38:26.410363: step 10065, loss 0.00525138, acc 1
2016-09-06T06:38:27.227228: step 10066, loss 0.00729061, acc 1
2016-09-06T06:38:27.999253: step 10067, loss 0.0169959, acc 1
2016-09-06T06:38:28.803152: step 10068, loss 0.00239277, acc 1
2016-09-06T06:38:29.608816: step 10069, loss 0.0118834, acc 1
2016-09-06T06:38:30.403277: step 10070, loss 0.00309196, acc 1
2016-09-06T06:38:31.225090: step 10071, loss 0.00264422, acc 1
2016-09-06T06:38:32.050419: step 10072, loss 0.0169583, acc 0.98
2016-09-06T06:38:32.846693: step 10073, loss 0.021011, acc 1
2016-09-06T06:38:33.680120: step 10074, loss 0.00348637, acc 1
2016-09-06T06:38:34.519566: step 10075, loss 0.00909061, acc 1
2016-09-06T06:38:35.283530: step 10076, loss 0.0179015, acc 0.98
2016-09-06T06:38:36.082202: step 10077, loss 0.0118047, acc 1
2016-09-06T06:38:36.909502: step 10078, loss 0.00284159, acc 1
2016-09-06T06:38:37.703015: step 10079, loss 0.019347, acc 0.98
2016-09-06T06:38:38.506760: step 10080, loss 0.00321768, acc 1
2016-09-06T06:38:39.349594: step 10081, loss 0.00837157, acc 1
2016-09-06T06:38:40.126089: step 10082, loss 0.0193327, acc 0.98
2016-09-06T06:38:40.966098: step 10083, loss 0.00409891, acc 1
2016-09-06T06:38:41.818305: step 10084, loss 0.0122352, acc 1
2016-09-06T06:38:42.625094: step 10085, loss 0.00895745, acc 1
2016-09-06T06:38:43.417255: step 10086, loss 0.00280613, acc 1
2016-09-06T06:38:44.241328: step 10087, loss 0.00297799, acc 1
2016-09-06T06:38:45.055734: step 10088, loss 0.00341407, acc 1
2016-09-06T06:38:45.864678: step 10089, loss 0.0055716, acc 1
2016-09-06T06:38:46.696954: step 10090, loss 0.00584125, acc 1
2016-09-06T06:38:47.515316: step 10091, loss 0.01662, acc 1
2016-09-06T06:38:48.322617: step 10092, loss 0.0427421, acc 0.98
2016-09-06T06:38:49.144423: step 10093, loss 0.0468995, acc 0.98
2016-09-06T06:38:49.945440: step 10094, loss 0.0338897, acc 0.98
2016-09-06T06:38:50.759115: step 10095, loss 0.00328044, acc 1
2016-09-06T06:38:51.581703: step 10096, loss 0.0268948, acc 0.98
2016-09-06T06:38:52.386886: step 10097, loss 0.0224418, acc 1
2016-09-06T06:38:53.229254: step 10098, loss 0.0226908, acc 1
2016-09-06T06:38:54.078126: step 10099, loss 0.0343184, acc 0.98
2016-09-06T06:38:54.885091: step 10100, loss 0.00283159, acc 1

Evaluation:
2016-09-06T06:38:58.609215: step 10100, loss 3.42324, acc 0.72045

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-10100

2016-09-06T06:39:00.442532: step 10101, loss 0.00296236, acc 1
2016-09-06T06:39:01.259404: step 10102, loss 0.00737936, acc 1
2016-09-06T06:39:02.057444: step 10103, loss 0.0106962, acc 1
2016-09-06T06:39:02.859092: step 10104, loss 0.00329891, acc 1
2016-09-06T06:39:03.696237: step 10105, loss 0.00590234, acc 1
2016-09-06T06:39:04.507096: step 10106, loss 0.0028968, acc 1
2016-09-06T06:39:05.316948: step 10107, loss 0.0147598, acc 1
2016-09-06T06:39:06.143576: step 10108, loss 0.0182528, acc 1
2016-09-06T06:39:06.965461: step 10109, loss 0.0194983, acc 0.98
2016-09-06T06:39:07.790858: step 10110, loss 0.00326795, acc 1
2016-09-06T06:39:08.615385: step 10111, loss 0.0171843, acc 0.98
2016-09-06T06:39:09.396997: step 10112, loss 0.0547676, acc 0.98
2016-09-06T06:39:10.199523: step 10113, loss 0.0264609, acc 0.98
2016-09-06T06:39:11.066019: step 10114, loss 0.0129224, acc 1
2016-09-06T06:39:11.888209: step 10115, loss 0.034492, acc 0.98
2016-09-06T06:39:12.695182: step 10116, loss 0.00338418, acc 1
2016-09-06T06:39:13.511894: step 10117, loss 0.0385337, acc 0.98
2016-09-06T06:39:14.304062: step 10118, loss 0.00529926, acc 1
2016-09-06T06:39:15.120382: step 10119, loss 0.00273979, acc 1
2016-09-06T06:39:15.930672: step 10120, loss 0.00798648, acc 1
2016-09-06T06:39:16.752325: step 10121, loss 0.00297351, acc 1
2016-09-06T06:39:17.571507: step 10122, loss 0.00311538, acc 1
2016-09-06T06:39:18.405127: step 10123, loss 0.00257239, acc 1
2016-09-06T06:39:19.245537: step 10124, loss 0.00499882, acc 1
2016-09-06T06:39:20.078100: step 10125, loss 0.0424075, acc 0.98
2016-09-06T06:39:20.930628: step 10126, loss 0.00335009, acc 1
2016-09-06T06:39:21.765944: step 10127, loss 0.0226692, acc 0.98
2016-09-06T06:39:22.585437: step 10128, loss 0.0374253, acc 0.96
2016-09-06T06:39:23.404159: step 10129, loss 0.0163706, acc 1
2016-09-06T06:39:24.231787: step 10130, loss 0.00986988, acc 1
2016-09-06T06:39:25.008790: step 10131, loss 0.0641688, acc 0.98
2016-09-06T06:39:25.806290: step 10132, loss 0.0169565, acc 0.98
2016-09-06T06:39:26.658971: step 10133, loss 0.00322264, acc 1
2016-09-06T06:39:27.440435: step 10134, loss 0.0172611, acc 0.98
2016-09-06T06:39:28.240546: step 10135, loss 0.00245825, acc 1
2016-09-06T06:39:29.057309: step 10136, loss 0.00246104, acc 1
2016-09-06T06:39:29.824488: step 10137, loss 0.00295725, acc 1
2016-09-06T06:39:30.619893: step 10138, loss 0.00251653, acc 1
2016-09-06T06:39:31.442622: step 10139, loss 0.0373312, acc 0.98
2016-09-06T06:39:32.227002: step 10140, loss 0.051114, acc 0.98
2016-09-06T06:39:33.054073: step 10141, loss 0.0713926, acc 0.98
2016-09-06T06:39:33.886106: step 10142, loss 0.0190362, acc 0.98
2016-09-06T06:39:34.661005: step 10143, loss 0.00401161, acc 1
2016-09-06T06:39:35.468953: step 10144, loss 0.0704966, acc 0.94
2016-09-06T06:39:36.288290: step 10145, loss 0.0720099, acc 0.94
2016-09-06T06:39:37.055953: step 10146, loss 0.0242995, acc 0.98
2016-09-06T06:39:37.848276: step 10147, loss 0.0301191, acc 1
2016-09-06T06:39:38.681272: step 10148, loss 0.00254697, acc 1
2016-09-06T06:39:39.487790: step 10149, loss 0.0158136, acc 1
2016-09-06T06:39:40.301529: step 10150, loss 0.0148948, acc 1
2016-09-06T06:39:41.096581: step 10151, loss 0.0194032, acc 1
2016-09-06T06:39:41.893219: step 10152, loss 0.0143533, acc 1
2016-09-06T06:39:42.697796: step 10153, loss 0.225255, acc 0.94
2016-09-06T06:39:43.511283: step 10154, loss 0.0195048, acc 1
2016-09-06T06:39:44.306391: step 10155, loss 0.0370087, acc 0.98
2016-09-06T06:39:45.170946: step 10156, loss 0.00426681, acc 1
2016-09-06T06:39:45.987304: step 10157, loss 0.00317358, acc 1
2016-09-06T06:39:46.794047: step 10158, loss 0.0375078, acc 0.98
2016-09-06T06:39:47.606849: step 10159, loss 0.0249499, acc 1
2016-09-06T06:39:48.444315: step 10160, loss 0.00504429, acc 1
2016-09-06T06:39:49.287027: step 10161, loss 0.00513777, acc 1
2016-09-06T06:39:50.101268: step 10162, loss 0.120937, acc 0.92
2016-09-06T06:39:50.922285: step 10163, loss 0.0318736, acc 0.98
2016-09-06T06:39:51.722342: step 10164, loss 0.0874097, acc 0.98
2016-09-06T06:39:52.538553: step 10165, loss 0.00675313, acc 1
2016-09-06T06:39:53.371345: step 10166, loss 0.0813531, acc 0.96
2016-09-06T06:39:54.180391: step 10167, loss 0.0121857, acc 1
2016-09-06T06:39:55.011486: step 10168, loss 0.00279936, acc 1
2016-09-06T06:39:55.843686: step 10169, loss 0.0518649, acc 0.96
2016-09-06T06:39:56.668555: step 10170, loss 0.0285239, acc 1
2016-09-06T06:39:57.473943: step 10171, loss 0.0284369, acc 1
2016-09-06T06:39:58.290071: step 10172, loss 0.00581404, acc 1
2016-09-06T06:39:59.107294: step 10173, loss 0.021796, acc 1
2016-09-06T06:39:59.928547: step 10174, loss 0.0313033, acc 0.98
2016-09-06T06:40:00.778610: step 10175, loss 0.0166289, acc 1
2016-09-06T06:40:01.529927: step 10176, loss 0.105214, acc 0.977273
2016-09-06T06:40:02.384186: step 10177, loss 0.0175079, acc 1
2016-09-06T06:40:03.209098: step 10178, loss 0.0168923, acc 1
2016-09-06T06:40:04.035586: step 10179, loss 0.0268125, acc 0.98
2016-09-06T06:40:04.824137: step 10180, loss 0.0355929, acc 0.98
2016-09-06T06:40:05.653557: step 10181, loss 0.00928796, acc 1
2016-09-06T06:40:06.489394: step 10182, loss 0.045951, acc 0.98
2016-09-06T06:40:07.272368: step 10183, loss 0.00313895, acc 1
2016-09-06T06:40:08.077566: step 10184, loss 0.00512107, acc 1
2016-09-06T06:40:08.927065: step 10185, loss 0.0233975, acc 1
2016-09-06T06:40:09.735460: step 10186, loss 0.032532, acc 0.98
2016-09-06T06:40:10.529889: step 10187, loss 0.00488445, acc 1
2016-09-06T06:40:11.347151: step 10188, loss 0.0346271, acc 0.98
2016-09-06T06:40:12.125361: step 10189, loss 0.0101197, acc 1
2016-09-06T06:40:12.961597: step 10190, loss 0.0588469, acc 0.96
2016-09-06T06:40:13.798309: step 10191, loss 0.013037, acc 1
2016-09-06T06:40:14.595397: step 10192, loss 0.00390676, acc 1
2016-09-06T06:40:15.406659: step 10193, loss 0.083968, acc 0.98
2016-09-06T06:40:16.246006: step 10194, loss 0.0037305, acc 1
2016-09-06T06:40:17.051137: step 10195, loss 0.00337803, acc 1
2016-09-06T06:40:17.835314: step 10196, loss 0.00556668, acc 1
2016-09-06T06:40:18.682280: step 10197, loss 0.00510131, acc 1
2016-09-06T06:40:19.509908: step 10198, loss 0.0143361, acc 1
2016-09-06T06:40:20.358855: step 10199, loss 0.0196471, acc 1
2016-09-06T06:40:21.182647: step 10200, loss 0.0396691, acc 0.98

Evaluation:
2016-09-06T06:40:24.925387: step 10200, loss 2.25671, acc 0.732645

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-10200

2016-09-06T06:40:26.833473: step 10201, loss 0.00902556, acc 1
2016-09-06T06:40:27.648559: step 10202, loss 0.0202925, acc 1
2016-09-06T06:40:28.491411: step 10203, loss 0.013047, acc 1
2016-09-06T06:40:29.309198: step 10204, loss 0.00540282, acc 1
2016-09-06T06:40:30.111340: step 10205, loss 0.0230123, acc 0.98
2016-09-06T06:40:30.940255: step 10206, loss 0.00786828, acc 1
2016-09-06T06:40:31.763095: step 10207, loss 0.00319701, acc 1
2016-09-06T06:40:32.567696: step 10208, loss 0.00518615, acc 1
2016-09-06T06:40:33.409478: step 10209, loss 0.0264374, acc 1
2016-09-06T06:40:34.209612: step 10210, loss 0.0182353, acc 1
2016-09-06T06:40:35.040858: step 10211, loss 0.00459749, acc 1
2016-09-06T06:40:35.869094: step 10212, loss 0.0313783, acc 0.98
2016-09-06T06:40:36.751700: step 10213, loss 0.0125448, acc 1
2016-09-06T06:40:37.530978: step 10214, loss 0.0212638, acc 1
2016-09-06T06:40:38.327414: step 10215, loss 0.0565459, acc 0.96
2016-09-06T06:40:39.168306: step 10216, loss 0.00309641, acc 1
2016-09-06T06:40:39.957490: step 10217, loss 0.035709, acc 0.98
2016-09-06T06:40:40.776902: step 10218, loss 0.00969256, acc 1
2016-09-06T06:40:41.602865: step 10219, loss 0.046379, acc 0.98
2016-09-06T06:40:42.418647: step 10220, loss 0.00353824, acc 1
2016-09-06T06:40:43.284902: step 10221, loss 0.0127259, acc 1
2016-09-06T06:40:44.098930: step 10222, loss 0.00337666, acc 1
2016-09-06T06:40:44.880288: step 10223, loss 0.0264462, acc 0.98
2016-09-06T06:40:45.711867: step 10224, loss 0.0186206, acc 0.98
2016-09-06T06:40:46.548829: step 10225, loss 0.0382666, acc 0.98
2016-09-06T06:40:47.368376: step 10226, loss 0.00592847, acc 1
2016-09-06T06:40:48.180696: step 10227, loss 0.00903599, acc 1
2016-09-06T06:40:49.007674: step 10228, loss 0.0145824, acc 1
2016-09-06T06:40:49.798356: step 10229, loss 0.0171746, acc 1
2016-09-06T06:40:50.623427: step 10230, loss 0.0110273, acc 1
2016-09-06T06:40:51.464630: step 10231, loss 0.00533608, acc 1
2016-09-06T06:40:52.265358: step 10232, loss 0.00846986, acc 1
2016-09-06T06:40:53.082841: step 10233, loss 0.0100987, acc 1
2016-09-06T06:40:53.915401: step 10234, loss 0.031833, acc 0.98
2016-09-06T06:40:54.730466: step 10235, loss 0.00294999, acc 1
2016-09-06T06:40:55.544159: step 10236, loss 0.00315226, acc 1
2016-09-06T06:40:56.384926: step 10237, loss 0.0241302, acc 1
2016-09-06T06:40:57.193479: step 10238, loss 0.0129026, acc 1
2016-09-06T06:40:58.015553: step 10239, loss 0.0134511, acc 1
2016-09-06T06:40:58.838896: step 10240, loss 0.0134723, acc 1
2016-09-06T06:40:59.655817: step 10241, loss 0.0427406, acc 0.96
2016-09-06T06:41:00.500520: step 10242, loss 0.00871683, acc 1
2016-09-06T06:41:01.312826: step 10243, loss 0.0299494, acc 0.98
2016-09-06T06:41:02.131870: step 10244, loss 0.0381275, acc 0.98
2016-09-06T06:41:02.955080: step 10245, loss 0.0206074, acc 0.98
2016-09-06T06:41:03.732061: step 10246, loss 0.0197789, acc 0.98
2016-09-06T06:41:04.576994: step 10247, loss 0.0778533, acc 0.98
2016-09-06T06:41:05.382764: step 10248, loss 0.00280964, acc 1
2016-09-06T06:41:06.188513: step 10249, loss 0.0404184, acc 0.98
2016-09-06T06:41:06.995195: step 10250, loss 0.0143693, acc 1
2016-09-06T06:41:07.793429: step 10251, loss 0.0323198, acc 0.98
2016-09-06T06:41:08.624563: step 10252, loss 0.016782, acc 0.98
2016-09-06T06:41:09.489434: step 10253, loss 0.00572555, acc 1
2016-09-06T06:41:10.277105: step 10254, loss 0.0253297, acc 0.98
2016-09-06T06:41:11.114998: step 10255, loss 0.0334495, acc 0.98
2016-09-06T06:41:11.940819: step 10256, loss 0.0394789, acc 0.98
2016-09-06T06:41:12.755645: step 10257, loss 0.00258308, acc 1
2016-09-06T06:41:13.560123: step 10258, loss 0.0192655, acc 0.98
2016-09-06T06:41:14.389396: step 10259, loss 0.0623987, acc 0.98
2016-09-06T06:41:15.224109: step 10260, loss 0.00306441, acc 1
2016-09-06T06:41:16.022855: step 10261, loss 0.0497459, acc 0.98
2016-09-06T06:41:16.870325: step 10262, loss 0.0176467, acc 1
2016-09-06T06:41:17.750406: step 10263, loss 0.0038969, acc 1
2016-09-06T06:41:18.574637: step 10264, loss 0.0214713, acc 0.98
2016-09-06T06:41:19.397039: step 10265, loss 0.136505, acc 0.96
2016-09-06T06:41:20.222034: step 10266, loss 0.00383577, acc 1
2016-09-06T06:41:21.012397: step 10267, loss 0.0164182, acc 1
2016-09-06T06:41:21.818484: step 10268, loss 0.00218668, acc 1
2016-09-06T06:41:22.654329: step 10269, loss 0.0510857, acc 0.98
2016-09-06T06:41:23.465026: step 10270, loss 0.0158938, acc 1
2016-09-06T06:41:24.302253: step 10271, loss 0.11654, acc 0.96
2016-09-06T06:41:25.111662: step 10272, loss 0.00601215, acc 1
2016-09-06T06:41:25.897695: step 10273, loss 0.00443239, acc 1
2016-09-06T06:41:26.688920: step 10274, loss 0.0245438, acc 0.98
2016-09-06T06:41:27.498919: step 10275, loss 0.00577365, acc 1
2016-09-06T06:41:28.284124: step 10276, loss 0.0388428, acc 0.98
2016-09-06T06:41:29.075892: step 10277, loss 0.00956919, acc 1
2016-09-06T06:41:29.896378: step 10278, loss 0.00390719, acc 1
2016-09-06T06:41:30.683996: step 10279, loss 0.0265108, acc 1
2016-09-06T06:41:31.493421: step 10280, loss 0.00288068, acc 1
2016-09-06T06:41:32.306648: step 10281, loss 0.0268448, acc 1
2016-09-06T06:41:33.082943: step 10282, loss 0.0150605, acc 1
2016-09-06T06:41:33.906857: step 10283, loss 0.0207681, acc 0.98
2016-09-06T06:41:34.713812: step 10284, loss 0.0386316, acc 0.98
2016-09-06T06:41:35.504966: step 10285, loss 0.0175871, acc 0.98
2016-09-06T06:41:36.295698: step 10286, loss 0.0236436, acc 0.98
2016-09-06T06:41:37.117405: step 10287, loss 0.0140508, acc 1
2016-09-06T06:41:37.916515: step 10288, loss 0.0458693, acc 0.98
2016-09-06T06:41:38.721076: step 10289, loss 0.0228838, acc 1
2016-09-06T06:41:39.560496: step 10290, loss 0.00405033, acc 1
2016-09-06T06:41:40.349562: step 10291, loss 0.0307883, acc 0.98
2016-09-06T06:41:41.225512: step 10292, loss 0.00352797, acc 1
2016-09-06T06:41:42.041100: step 10293, loss 0.0101292, acc 1
2016-09-06T06:41:42.837615: step 10294, loss 0.00328187, acc 1
2016-09-06T06:41:43.625519: step 10295, loss 0.0598565, acc 0.98
2016-09-06T06:41:44.429449: step 10296, loss 0.0141637, acc 1
2016-09-06T06:41:45.233575: step 10297, loss 0.0031158, acc 1
2016-09-06T06:41:46.045038: step 10298, loss 0.0311011, acc 0.98
2016-09-06T06:41:46.856751: step 10299, loss 0.0159984, acc 1
2016-09-06T06:41:47.628282: step 10300, loss 0.039784, acc 1

Evaluation:
2016-09-06T06:41:51.354744: step 10300, loss 2.26789, acc 0.732645

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-10300

2016-09-06T06:41:53.251566: step 10301, loss 0.0651177, acc 0.96
2016-09-06T06:41:54.074811: step 10302, loss 0.00858842, acc 1
2016-09-06T06:41:54.895566: step 10303, loss 0.00816157, acc 1
2016-09-06T06:41:55.711683: step 10304, loss 0.015653, acc 1
2016-09-06T06:41:56.543077: step 10305, loss 0.00947294, acc 1
2016-09-06T06:41:57.331279: step 10306, loss 0.0682996, acc 0.96
2016-09-06T06:41:58.161661: step 10307, loss 0.0184851, acc 1
2016-09-06T06:41:58.964931: step 10308, loss 0.0227748, acc 1
2016-09-06T06:41:59.753489: step 10309, loss 0.0389222, acc 0.96
2016-09-06T06:42:00.581619: step 10310, loss 0.0196532, acc 1
2016-09-06T06:42:01.387194: step 10311, loss 0.0185213, acc 0.98
2016-09-06T06:42:02.175258: step 10312, loss 0.00263359, acc 1
2016-09-06T06:42:02.948660: step 10313, loss 0.0198061, acc 1
2016-09-06T06:42:03.797620: step 10314, loss 0.0164821, acc 1
2016-09-06T06:42:04.594964: step 10315, loss 0.00263021, acc 1
2016-09-06T06:42:05.398868: step 10316, loss 0.00903022, acc 1
2016-09-06T06:42:06.220921: step 10317, loss 0.0674066, acc 0.96
2016-09-06T06:42:07.033967: step 10318, loss 0.0170708, acc 1
2016-09-06T06:42:07.846754: step 10319, loss 0.0361021, acc 0.98
2016-09-06T06:42:08.644173: step 10320, loss 0.0273249, acc 0.98
2016-09-06T06:42:09.451809: step 10321, loss 0.0531732, acc 0.98
2016-09-06T06:42:10.263484: step 10322, loss 0.0342962, acc 0.98
2016-09-06T06:42:11.109969: step 10323, loss 0.0356373, acc 0.98
2016-09-06T06:42:11.947621: step 10324, loss 0.00980203, acc 1
2016-09-06T06:42:12.763518: step 10325, loss 0.0270512, acc 1
2016-09-06T06:42:13.576320: step 10326, loss 0.0167954, acc 0.98
2016-09-06T06:42:14.408046: step 10327, loss 0.00517552, acc 1
2016-09-06T06:42:15.220410: step 10328, loss 0.0177945, acc 1
2016-09-06T06:42:16.048538: step 10329, loss 0.0124322, acc 1
2016-09-06T06:42:16.886861: step 10330, loss 0.0422132, acc 0.98
2016-09-06T06:42:17.686510: step 10331, loss 0.00308568, acc 1
2016-09-06T06:42:18.505895: step 10332, loss 0.0180857, acc 1
2016-09-06T06:42:19.345702: step 10333, loss 0.0318024, acc 0.98
2016-09-06T06:42:20.146884: step 10334, loss 0.0108882, acc 1
2016-09-06T06:42:20.990566: step 10335, loss 0.00259071, acc 1
2016-09-06T06:42:21.831435: step 10336, loss 0.0162586, acc 1
2016-09-06T06:42:22.678191: step 10337, loss 0.0213917, acc 1
2016-09-06T06:42:23.488549: step 10338, loss 0.0062441, acc 1
2016-09-06T06:42:24.315593: step 10339, loss 0.0210655, acc 0.98
2016-09-06T06:42:25.114219: step 10340, loss 0.0026827, acc 1
2016-09-06T06:42:25.911633: step 10341, loss 0.00424383, acc 1
2016-09-06T06:42:26.733624: step 10342, loss 0.00385192, acc 1
2016-09-06T06:42:27.515273: step 10343, loss 0.00493251, acc 1
2016-09-06T06:42:28.305679: step 10344, loss 0.0135084, acc 1
2016-09-06T06:42:29.099886: step 10345, loss 0.0228591, acc 1
2016-09-06T06:42:29.937477: step 10346, loss 0.00302824, acc 1
2016-09-06T06:42:30.731147: step 10347, loss 0.00446013, acc 1
2016-09-06T06:42:31.565396: step 10348, loss 0.0413719, acc 0.98
2016-09-06T06:42:32.347523: step 10349, loss 0.0440814, acc 0.98
2016-09-06T06:42:33.160928: step 10350, loss 0.0169308, acc 1
2016-09-06T06:42:34.015665: step 10351, loss 0.045542, acc 0.98
2016-09-06T06:42:34.814154: step 10352, loss 0.00245194, acc 1
2016-09-06T06:42:35.609658: step 10353, loss 0.00296156, acc 1
2016-09-06T06:42:36.401925: step 10354, loss 0.0393405, acc 1
2016-09-06T06:42:37.177072: step 10355, loss 0.0295454, acc 0.98
2016-09-06T06:42:37.983939: step 10356, loss 0.0195522, acc 0.98
2016-09-06T06:42:38.834274: step 10357, loss 0.0159128, acc 1
2016-09-06T06:42:39.627714: step 10358, loss 0.033428, acc 0.98
2016-09-06T06:42:40.450686: step 10359, loss 0.00852262, acc 1
2016-09-06T06:42:41.293395: step 10360, loss 0.00259075, acc 1
2016-09-06T06:42:42.113565: step 10361, loss 0.0232405, acc 1
2016-09-06T06:42:42.932737: step 10362, loss 0.0226094, acc 0.98
2016-09-06T06:42:43.769622: step 10363, loss 0.0238115, acc 0.98
2016-09-06T06:42:44.572038: step 10364, loss 0.0172396, acc 0.98
2016-09-06T06:42:45.358841: step 10365, loss 0.0190802, acc 1
2016-09-06T06:42:46.199240: step 10366, loss 0.00261354, acc 1
2016-09-06T06:42:47.005064: step 10367, loss 0.0112211, acc 1
2016-09-06T06:42:47.773252: step 10368, loss 0.00265421, acc 1
2016-09-06T06:42:48.617938: step 10369, loss 0.0387735, acc 0.98
2016-09-06T06:42:49.456645: step 10370, loss 0.0148405, acc 1
2016-09-06T06:42:50.286601: step 10371, loss 0.0119894, acc 1
2016-09-06T06:42:51.146278: step 10372, loss 0.014424, acc 1
2016-09-06T06:42:52.024883: step 10373, loss 0.0173789, acc 0.98
2016-09-06T06:42:52.840484: step 10374, loss 0.0134876, acc 1
2016-09-06T06:42:53.669074: step 10375, loss 0.0106742, acc 1
2016-09-06T06:42:54.469440: step 10376, loss 0.00944721, acc 1
2016-09-06T06:42:55.279139: step 10377, loss 0.0190957, acc 0.98
2016-09-06T06:42:56.078798: step 10378, loss 0.0114994, acc 1
2016-09-06T06:42:56.908006: step 10379, loss 0.0101639, acc 1
2016-09-06T06:42:57.712232: step 10380, loss 0.00413689, acc 1
2016-09-06T06:42:58.534621: step 10381, loss 0.00292272, acc 1
2016-09-06T06:42:59.367588: step 10382, loss 0.0298222, acc 0.98
2016-09-06T06:43:00.173934: step 10383, loss 0.0283898, acc 0.98
2016-09-06T06:43:01.013322: step 10384, loss 0.025312, acc 0.98
2016-09-06T06:43:01.825319: step 10385, loss 0.028732, acc 0.98
2016-09-06T06:43:02.626065: step 10386, loss 0.00256501, acc 1
2016-09-06T06:43:03.428904: step 10387, loss 0.0151799, acc 1
2016-09-06T06:43:04.263762: step 10388, loss 0.00277151, acc 1
2016-09-06T06:43:05.084546: step 10389, loss 0.0407403, acc 0.98
2016-09-06T06:43:05.894861: step 10390, loss 0.0212991, acc 0.98
2016-09-06T06:43:06.724504: step 10391, loss 0.0138999, acc 1
2016-09-06T06:43:07.538422: step 10392, loss 0.0404789, acc 0.98
2016-09-06T06:43:08.355040: step 10393, loss 0.0191547, acc 0.98
2016-09-06T06:43:09.175313: step 10394, loss 0.0244744, acc 0.98
2016-09-06T06:43:10.003551: step 10395, loss 0.0201758, acc 0.98
2016-09-06T06:43:10.834830: step 10396, loss 0.0333409, acc 0.98
2016-09-06T06:43:11.688143: step 10397, loss 0.0400547, acc 0.98
2016-09-06T06:43:12.508794: step 10398, loss 0.0024478, acc 1
2016-09-06T06:43:13.353045: step 10399, loss 0.00254962, acc 1
2016-09-06T06:43:14.210625: step 10400, loss 0.00588053, acc 1

Evaluation:
2016-09-06T06:43:18.024687: step 10400, loss 2.64982, acc 0.730769

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-10400

2016-09-06T06:43:19.987130: step 10401, loss 0.0198162, acc 0.98
2016-09-06T06:43:20.853022: step 10402, loss 0.0167899, acc 0.98
2016-09-06T06:43:21.664362: step 10403, loss 0.0588674, acc 0.98
2016-09-06T06:43:22.511529: step 10404, loss 0.00256568, acc 1
2016-09-06T06:43:23.335696: step 10405, loss 0.00565563, acc 1
2016-09-06T06:43:24.153586: step 10406, loss 0.00524274, acc 1
2016-09-06T06:43:24.987561: step 10407, loss 0.0961572, acc 0.94
2016-09-06T06:43:25.798773: step 10408, loss 0.0173034, acc 1
2016-09-06T06:43:26.580755: step 10409, loss 0.0178146, acc 0.98
2016-09-06T06:43:27.409459: step 10410, loss 0.0069659, acc 1
2016-09-06T06:43:28.227534: step 10411, loss 0.0472008, acc 0.96
2016-09-06T06:43:29.043238: step 10412, loss 0.00206524, acc 1
2016-09-06T06:43:29.854777: step 10413, loss 0.00416975, acc 1
2016-09-06T06:43:30.727759: step 10414, loss 0.00307377, acc 1
2016-09-06T06:43:31.563820: step 10415, loss 0.0208419, acc 0.98
2016-09-06T06:43:32.378966: step 10416, loss 0.0227929, acc 1
2016-09-06T06:43:33.215374: step 10417, loss 0.0120569, acc 1
2016-09-06T06:43:34.022672: step 10418, loss 0.00202342, acc 1
2016-09-06T06:43:34.831419: step 10419, loss 0.012526, acc 1
2016-09-06T06:43:35.652025: step 10420, loss 0.0869133, acc 0.98
2016-09-06T06:43:36.443084: step 10421, loss 0.0106471, acc 1
2016-09-06T06:43:37.252701: step 10422, loss 0.0159757, acc 1
2016-09-06T06:43:38.081433: step 10423, loss 0.0776695, acc 0.98
2016-09-06T06:43:38.880581: step 10424, loss 0.00210403, acc 1
2016-09-06T06:43:39.697459: step 10425, loss 0.00384074, acc 1
2016-09-06T06:43:40.518555: step 10426, loss 0.0147257, acc 1
2016-09-06T06:43:41.320056: step 10427, loss 0.0288568, acc 0.98
2016-09-06T06:43:42.138670: step 10428, loss 0.00272612, acc 1
2016-09-06T06:43:42.970220: step 10429, loss 0.110254, acc 0.96
2016-09-06T06:43:43.773717: step 10430, loss 0.0204086, acc 1
2016-09-06T06:43:44.603757: step 10431, loss 0.00257809, acc 1
2016-09-06T06:43:45.429488: step 10432, loss 0.0204176, acc 0.98
2016-09-06T06:43:46.244243: step 10433, loss 0.00764485, acc 1
2016-09-06T06:43:47.051205: step 10434, loss 0.0370036, acc 0.98
2016-09-06T06:43:47.871617: step 10435, loss 0.0276875, acc 0.98
2016-09-06T06:43:48.748428: step 10436, loss 0.0806087, acc 0.94
2016-09-06T06:43:49.583517: step 10437, loss 0.03584, acc 1
2016-09-06T06:43:50.406965: step 10438, loss 0.0249659, acc 1
2016-09-06T06:43:51.205686: step 10439, loss 0.0394634, acc 0.98
2016-09-06T06:43:52.036371: step 10440, loss 0.00228135, acc 1
2016-09-06T06:43:52.922081: step 10441, loss 0.0042873, acc 1
2016-09-06T06:43:53.737552: step 10442, loss 0.0131974, acc 1
2016-09-06T06:43:54.532064: step 10443, loss 0.0359608, acc 0.98
2016-09-06T06:43:55.353771: step 10444, loss 0.0147019, acc 1
2016-09-06T06:43:56.177276: step 10445, loss 0.0107142, acc 1
2016-09-06T06:43:56.990083: step 10446, loss 0.0516964, acc 0.98
2016-09-06T06:43:57.806887: step 10447, loss 0.0563763, acc 0.98
2016-09-06T06:43:58.633686: step 10448, loss 0.00495759, acc 1
2016-09-06T06:43:59.430953: step 10449, loss 0.00363511, acc 1
2016-09-06T06:44:00.215299: step 10450, loss 0.00877231, acc 1
2016-09-06T06:44:01.049591: step 10451, loss 0.0179137, acc 1
2016-09-06T06:44:01.821111: step 10452, loss 0.00193727, acc 1
2016-09-06T06:44:02.629928: step 10453, loss 0.136376, acc 0.96
2016-09-06T06:44:03.457118: step 10454, loss 0.0362237, acc 0.96
2016-09-06T06:44:04.239836: step 10455, loss 0.0020166, acc 1
2016-09-06T06:44:05.061437: step 10456, loss 0.00441564, acc 1
2016-09-06T06:44:05.895958: step 10457, loss 0.0172663, acc 1
2016-09-06T06:44:06.706302: step 10458, loss 0.0111081, acc 1
2016-09-06T06:44:07.520077: step 10459, loss 0.00573885, acc 1
2016-09-06T06:44:08.346874: step 10460, loss 0.00186982, acc 1
2016-09-06T06:44:09.159510: step 10461, loss 0.0275657, acc 0.98
2016-09-06T06:44:10.013210: step 10462, loss 0.021372, acc 1
2016-09-06T06:44:10.849024: step 10463, loss 0.00283786, acc 1
2016-09-06T06:44:11.656644: step 10464, loss 0.00262979, acc 1
2016-09-06T06:44:12.453079: step 10465, loss 0.0162116, acc 1
2016-09-06T06:44:13.297059: step 10466, loss 0.0159316, acc 1
2016-09-06T06:44:14.138395: step 10467, loss 0.0183934, acc 1
2016-09-06T06:44:14.947559: step 10468, loss 0.087708, acc 0.96
2016-09-06T06:44:15.765269: step 10469, loss 0.0116561, acc 1
2016-09-06T06:44:16.582074: step 10470, loss 0.0239921, acc 0.98
2016-09-06T06:44:17.396998: step 10471, loss 0.0202523, acc 0.98
2016-09-06T06:44:18.212401: step 10472, loss 0.00338495, acc 1
2016-09-06T06:44:19.028907: step 10473, loss 0.0169893, acc 1
2016-09-06T06:44:19.826610: step 10474, loss 0.00359601, acc 1
2016-09-06T06:44:20.636266: step 10475, loss 0.0319167, acc 1
2016-09-06T06:44:21.445540: step 10476, loss 0.00701558, acc 1
2016-09-06T06:44:22.256362: step 10477, loss 0.0268354, acc 1
2016-09-06T06:44:23.083897: step 10478, loss 0.0214775, acc 0.98
2016-09-06T06:44:23.928107: step 10479, loss 0.0108764, acc 1
2016-09-06T06:44:24.723490: step 10480, loss 0.0173968, acc 1
2016-09-06T06:44:25.533124: step 10481, loss 0.00277437, acc 1
2016-09-06T06:44:26.330509: step 10482, loss 0.00262607, acc 1
2016-09-06T06:44:27.182078: step 10483, loss 0.0157489, acc 1
2016-09-06T06:44:28.003359: step 10484, loss 0.0130704, acc 1
2016-09-06T06:44:28.817805: step 10485, loss 0.00506041, acc 1
2016-09-06T06:44:29.620263: step 10486, loss 0.00322144, acc 1
2016-09-06T06:44:30.425583: step 10487, loss 0.00954365, acc 1
2016-09-06T06:44:31.282072: step 10488, loss 0.00578808, acc 1
2016-09-06T06:44:32.095304: step 10489, loss 0.018854, acc 0.98
2016-09-06T06:44:32.901710: step 10490, loss 0.0120407, acc 1
2016-09-06T06:44:33.732275: step 10491, loss 0.0116919, acc 1
2016-09-06T06:44:34.528084: step 10492, loss 0.00271605, acc 1
2016-09-06T06:44:35.372651: step 10493, loss 0.00584843, acc 1
2016-09-06T06:44:36.190237: step 10494, loss 0.00259121, acc 1
2016-09-06T06:44:36.993950: step 10495, loss 0.0595155, acc 0.98
2016-09-06T06:44:37.839244: step 10496, loss 0.0475291, acc 0.98
2016-09-06T06:44:38.670271: step 10497, loss 0.00741147, acc 1
2016-09-06T06:44:39.495019: step 10498, loss 0.00810734, acc 1
2016-09-06T06:44:40.343282: step 10499, loss 0.0175691, acc 0.98
2016-09-06T06:44:41.157833: step 10500, loss 0.00232532, acc 1

Evaluation:
2016-09-06T06:44:44.894341: step 10500, loss 2.50648, acc 0.734522

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-10500

2016-09-06T06:44:46.759051: step 10501, loss 0.0111586, acc 1
2016-09-06T06:44:47.573603: step 10502, loss 0.016617, acc 1
2016-09-06T06:44:48.449775: step 10503, loss 0.0265866, acc 0.98
2016-09-06T06:44:49.250391: step 10504, loss 0.0223962, acc 1
2016-09-06T06:44:50.067574: step 10505, loss 0.0054829, acc 1
2016-09-06T06:44:50.941243: step 10506, loss 0.0157982, acc 1
2016-09-06T06:44:51.729354: step 10507, loss 0.00280322, acc 1
2016-09-06T06:44:52.559102: step 10508, loss 0.0025345, acc 1
2016-09-06T06:44:53.380875: step 10509, loss 0.0136694, acc 1
2016-09-06T06:44:54.208644: step 10510, loss 0.00238672, acc 1
2016-09-06T06:44:55.024346: step 10511, loss 0.00758904, acc 1
2016-09-06T06:44:55.816590: step 10512, loss 0.0400152, acc 0.98
2016-09-06T06:44:56.642274: step 10513, loss 0.00236131, acc 1
2016-09-06T06:44:57.444684: step 10514, loss 0.0368002, acc 0.98
2016-09-06T06:44:58.277998: step 10515, loss 0.038618, acc 0.98
2016-09-06T06:44:59.096988: step 10516, loss 0.0238248, acc 0.98
2016-09-06T06:44:59.875387: step 10517, loss 0.0143641, acc 1
2016-09-06T06:45:00.716421: step 10518, loss 0.0251817, acc 1
2016-09-06T06:45:01.529228: step 10519, loss 0.00992744, acc 1
2016-09-06T06:45:02.298267: step 10520, loss 0.0218378, acc 1
2016-09-06T06:45:03.109857: step 10521, loss 0.0359873, acc 0.98
2016-09-06T06:45:03.938598: step 10522, loss 0.00253339, acc 1
2016-09-06T06:45:04.740943: step 10523, loss 0.00315934, acc 1
2016-09-06T06:45:05.542287: step 10524, loss 0.0365786, acc 0.96
2016-09-06T06:45:06.366763: step 10525, loss 0.00260037, acc 1
2016-09-06T06:45:07.151232: step 10526, loss 0.0200928, acc 0.98
2016-09-06T06:45:07.937102: step 10527, loss 0.00534015, acc 1
2016-09-06T06:45:08.744574: step 10528, loss 0.033245, acc 0.96
2016-09-06T06:45:09.524238: step 10529, loss 0.00492791, acc 1
2016-09-06T06:45:10.345624: step 10530, loss 0.0181847, acc 0.98
2016-09-06T06:45:11.173291: step 10531, loss 0.0121534, acc 1
2016-09-06T06:45:11.991497: step 10532, loss 0.00878338, acc 1
2016-09-06T06:45:12.797782: step 10533, loss 0.00707223, acc 1
2016-09-06T06:45:13.603532: step 10534, loss 0.0325514, acc 0.98
2016-09-06T06:45:14.366194: step 10535, loss 0.0024791, acc 1
2016-09-06T06:45:15.177165: step 10536, loss 0.0715268, acc 0.98
2016-09-06T06:45:15.995278: step 10537, loss 0.0753384, acc 0.98
2016-09-06T06:45:16.774011: step 10538, loss 0.00366079, acc 1
2016-09-06T06:45:17.597159: step 10539, loss 0.0219257, acc 1
2016-09-06T06:45:18.445575: step 10540, loss 0.00758709, acc 1
2016-09-06T06:45:19.215015: step 10541, loss 0.0255678, acc 0.98
2016-09-06T06:45:19.999270: step 10542, loss 0.0175245, acc 0.98
2016-09-06T06:45:20.809379: step 10543, loss 0.0180557, acc 0.98
2016-09-06T06:45:21.612379: step 10544, loss 0.0783044, acc 0.98
2016-09-06T06:45:22.409644: step 10545, loss 0.00583992, acc 1
2016-09-06T06:45:23.233387: step 10546, loss 0.0364886, acc 1
2016-09-06T06:45:24.027680: step 10547, loss 0.00440287, acc 1
2016-09-06T06:45:24.834404: step 10548, loss 0.0685245, acc 0.96
2016-09-06T06:45:25.672712: step 10549, loss 0.0626922, acc 0.98
2016-09-06T06:45:26.446576: step 10550, loss 0.00251582, acc 1
2016-09-06T06:45:27.245387: step 10551, loss 0.00191362, acc 1
2016-09-06T06:45:28.065614: step 10552, loss 0.00351042, acc 1
2016-09-06T06:45:28.861778: step 10553, loss 0.0112021, acc 1
2016-09-06T06:45:29.683702: step 10554, loss 0.0220551, acc 1
2016-09-06T06:45:30.516922: step 10555, loss 0.0110658, acc 1
2016-09-06T06:45:31.293422: step 10556, loss 0.00183166, acc 1
2016-09-06T06:45:32.132215: step 10557, loss 0.00252054, acc 1
2016-09-06T06:45:32.927264: step 10558, loss 0.0148317, acc 1
2016-09-06T06:45:33.709962: step 10559, loss 0.00728137, acc 1
2016-09-06T06:45:34.477554: step 10560, loss 0.0055919, acc 1
2016-09-06T06:45:35.307178: step 10561, loss 0.0480497, acc 0.98
2016-09-06T06:45:36.118138: step 10562, loss 0.0454994, acc 0.96
2016-09-06T06:45:36.967848: step 10563, loss 0.0135223, acc 1
2016-09-06T06:45:37.810024: step 10564, loss 0.0399902, acc 0.96
2016-09-06T06:45:38.617785: step 10565, loss 0.0221719, acc 0.98
2016-09-06T06:45:39.422855: step 10566, loss 0.053405, acc 0.96
2016-09-06T06:45:40.281930: step 10567, loss 0.00655309, acc 1
2016-09-06T06:45:41.060419: step 10568, loss 0.00384953, acc 1
2016-09-06T06:45:41.867121: step 10569, loss 0.0323777, acc 0.98
2016-09-06T06:45:42.680627: step 10570, loss 0.0192234, acc 1
2016-09-06T06:45:43.501425: step 10571, loss 0.00193215, acc 1
2016-09-06T06:45:44.322775: step 10572, loss 0.0220373, acc 0.98
2016-09-06T06:45:45.140154: step 10573, loss 0.0172633, acc 1
2016-09-06T06:45:45.955911: step 10574, loss 0.0132824, acc 1
2016-09-06T06:45:46.795383: step 10575, loss 0.0373741, acc 1
2016-09-06T06:45:47.651139: step 10576, loss 0.0788295, acc 0.98
2016-09-06T06:45:48.491315: step 10577, loss 0.0235762, acc 1
2016-09-06T06:45:49.315337: step 10578, loss 0.0379478, acc 0.98
2016-09-06T06:45:50.163619: step 10579, loss 0.00945845, acc 1
2016-09-06T06:45:51.011093: step 10580, loss 0.00406992, acc 1
2016-09-06T06:45:51.824546: step 10581, loss 0.00480492, acc 1
2016-09-06T06:45:52.623531: step 10582, loss 0.00203238, acc 1
2016-09-06T06:45:53.464948: step 10583, loss 0.0263315, acc 1
2016-09-06T06:45:54.258553: step 10584, loss 0.00194643, acc 1
2016-09-06T06:45:55.084528: step 10585, loss 0.0172072, acc 1
2016-09-06T06:45:55.881439: step 10586, loss 0.036269, acc 1
2016-09-06T06:45:56.676233: step 10587, loss 0.0143265, acc 1
2016-09-06T06:45:57.471006: step 10588, loss 0.0149769, acc 1
2016-09-06T06:45:58.305475: step 10589, loss 0.0392459, acc 0.96
2016-09-06T06:45:59.097041: step 10590, loss 0.0046755, acc 1
2016-09-06T06:45:59.926817: step 10591, loss 0.043846, acc 0.98
2016-09-06T06:46:00.744074: step 10592, loss 0.00208151, acc 1
2016-09-06T06:46:01.535116: step 10593, loss 0.0595292, acc 0.98
2016-09-06T06:46:02.339909: step 10594, loss 0.0185963, acc 0.98
2016-09-06T06:46:03.149621: step 10595, loss 0.00207106, acc 1
2016-09-06T06:46:03.919310: step 10596, loss 0.0248849, acc 0.98
2016-09-06T06:46:04.737178: step 10597, loss 0.00253054, acc 1
2016-09-06T06:46:05.566962: step 10598, loss 0.0302183, acc 1
2016-09-06T06:46:06.367885: step 10599, loss 0.0164903, acc 1
2016-09-06T06:46:07.147354: step 10600, loss 0.00334637, acc 1

Evaluation:
2016-09-06T06:46:10.878044: step 10600, loss 2.0867, acc 0.725141

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-10600

2016-09-06T06:46:12.864826: step 10601, loss 0.00523891, acc 1
2016-09-06T06:46:13.677577: step 10602, loss 0.0443187, acc 0.96
2016-09-06T06:46:14.580393: step 10603, loss 0.0573529, acc 0.96
2016-09-06T06:46:15.422649: step 10604, loss 0.0253768, acc 0.98
2016-09-06T06:46:16.241163: step 10605, loss 0.00293594, acc 1
2016-09-06T06:46:17.054220: step 10606, loss 0.0135675, acc 1
2016-09-06T06:46:17.865781: step 10607, loss 0.0206396, acc 1
2016-09-06T06:46:18.683258: step 10608, loss 0.0169431, acc 1
2016-09-06T06:46:19.506434: step 10609, loss 0.0365556, acc 0.98
2016-09-06T06:46:20.305186: step 10610, loss 0.00208245, acc 1
2016-09-06T06:46:21.132593: step 10611, loss 0.0104765, acc 1
2016-09-06T06:46:21.949603: step 10612, loss 0.0579833, acc 0.98
2016-09-06T06:46:22.813215: step 10613, loss 0.020347, acc 0.98
2016-09-06T06:46:23.660964: step 10614, loss 0.0533227, acc 0.98
2016-09-06T06:46:24.485506: step 10615, loss 0.052129, acc 0.98
2016-09-06T06:46:25.330672: step 10616, loss 0.0120558, acc 1
2016-09-06T06:46:26.151445: step 10617, loss 0.0588891, acc 0.98
2016-09-06T06:46:27.353122: step 10618, loss 0.0152354, acc 1
2016-09-06T06:46:28.160658: step 10619, loss 0.00197578, acc 1
2016-09-06T06:46:28.994907: step 10620, loss 0.0111967, acc 1
2016-09-06T06:46:29.805179: step 10621, loss 0.0303615, acc 0.98
2016-09-06T06:46:30.579562: step 10622, loss 0.0308687, acc 1
2016-09-06T06:46:31.400712: step 10623, loss 0.00863231, acc 1
2016-09-06T06:46:32.263088: step 10624, loss 0.0411794, acc 0.98
2016-09-06T06:46:33.089456: step 10625, loss 0.00664769, acc 1
2016-09-06T06:46:33.908721: step 10626, loss 0.00760173, acc 1
2016-09-06T06:46:34.716780: step 10627, loss 0.00700362, acc 1
2016-09-06T06:46:35.495260: step 10628, loss 0.0670506, acc 0.96
2016-09-06T06:46:36.328928: step 10629, loss 0.0778351, acc 0.96
2016-09-06T06:46:37.143556: step 10630, loss 0.00365445, acc 1
2016-09-06T06:46:37.959274: step 10631, loss 0.0145126, acc 1
2016-09-06T06:46:38.762403: step 10632, loss 0.0170205, acc 1
2016-09-06T06:46:39.596549: step 10633, loss 0.0438218, acc 0.98
2016-09-06T06:46:40.405017: step 10634, loss 0.00544714, acc 1
2016-09-06T06:46:41.224174: step 10635, loss 0.0551914, acc 0.98
2016-09-06T06:46:42.053311: step 10636, loss 0.0231063, acc 0.98
2016-09-06T06:46:42.863444: step 10637, loss 0.0358737, acc 0.98
2016-09-06T06:46:43.656236: step 10638, loss 0.0187348, acc 1
2016-09-06T06:46:44.493540: step 10639, loss 0.0205311, acc 1
2016-09-06T06:46:45.296800: step 10640, loss 0.0152681, acc 1
2016-09-06T06:46:46.080743: step 10641, loss 0.020543, acc 0.98
2016-09-06T06:46:46.904428: step 10642, loss 0.0144552, acc 1
2016-09-06T06:46:47.708325: step 10643, loss 0.00820575, acc 1
2016-09-06T06:46:48.517339: step 10644, loss 0.0142485, acc 1
2016-09-06T06:46:49.357767: step 10645, loss 0.0259806, acc 0.98
2016-09-06T06:46:50.162157: step 10646, loss 0.00269044, acc 1
2016-09-06T06:46:50.979825: step 10647, loss 0.00301513, acc 1
2016-09-06T06:46:51.810440: step 10648, loss 0.00637958, acc 1
2016-09-06T06:46:52.625629: step 10649, loss 0.0305135, acc 0.98
2016-09-06T06:46:53.427708: step 10650, loss 0.134441, acc 0.98
2016-09-06T06:46:54.246349: step 10651, loss 0.00370853, acc 1
2016-09-06T06:46:55.050488: step 10652, loss 0.00340309, acc 1
2016-09-06T06:46:55.870558: step 10653, loss 0.00327849, acc 1
2016-09-06T06:46:56.721082: step 10654, loss 0.00984781, acc 1
2016-09-06T06:46:57.557844: step 10655, loss 0.0189084, acc 0.98
2016-09-06T06:46:58.357463: step 10656, loss 0.00972135, acc 1
2016-09-06T06:46:59.194859: step 10657, loss 0.0269421, acc 1
2016-09-06T06:47:00.019210: step 10658, loss 0.0320371, acc 0.98
2016-09-06T06:47:00.856387: step 10659, loss 0.0631703, acc 0.98
2016-09-06T06:47:01.681437: step 10660, loss 0.00961452, acc 1
2016-09-06T06:47:02.502195: step 10661, loss 0.0353959, acc 0.98
2016-09-06T06:47:03.306362: step 10662, loss 0.00491529, acc 1
2016-09-06T06:47:04.118185: step 10663, loss 0.0120294, acc 1
2016-09-06T06:47:05.001794: step 10664, loss 0.00271961, acc 1
2016-09-06T06:47:05.791192: step 10665, loss 0.01294, acc 1
2016-09-06T06:47:06.628573: step 10666, loss 0.0148852, acc 1
2016-09-06T06:47:07.445454: step 10667, loss 0.0687178, acc 0.96
2016-09-06T06:47:08.258683: step 10668, loss 0.0237317, acc 0.98
2016-09-06T06:47:09.075187: step 10669, loss 0.0478964, acc 0.96
2016-09-06T06:47:09.904645: step 10670, loss 0.0306701, acc 1
2016-09-06T06:47:10.722143: step 10671, loss 0.0504018, acc 0.98
2016-09-06T06:47:11.535077: step 10672, loss 0.0418577, acc 0.96
2016-09-06T06:47:12.392310: step 10673, loss 0.0263724, acc 0.98
2016-09-06T06:47:13.215992: step 10674, loss 0.0228521, acc 0.98
2016-09-06T06:47:14.024890: step 10675, loss 0.028884, acc 1
2016-09-06T06:47:14.883669: step 10676, loss 0.0231291, acc 0.98
2016-09-06T06:47:15.688638: step 10677, loss 0.00518452, acc 1
2016-09-06T06:47:16.476845: step 10678, loss 0.00237107, acc 1
2016-09-06T06:47:17.294453: step 10679, loss 0.00314374, acc 1
2016-09-06T06:47:18.105546: step 10680, loss 0.00816565, acc 1
2016-09-06T06:47:18.927370: step 10681, loss 0.00445482, acc 1
2016-09-06T06:47:19.764773: step 10682, loss 0.00883265, acc 1
2016-09-06T06:47:20.577629: step 10683, loss 0.00240626, acc 1
2016-09-06T06:47:21.361509: step 10684, loss 0.00237804, acc 1
2016-09-06T06:47:22.169852: step 10685, loss 0.0156655, acc 1
2016-09-06T06:47:22.976934: step 10686, loss 0.0168891, acc 0.98
2016-09-06T06:47:23.766662: step 10687, loss 0.0140772, acc 1
2016-09-06T06:47:24.593525: step 10688, loss 0.0110595, acc 1
2016-09-06T06:47:25.422185: step 10689, loss 0.123039, acc 0.98
2016-09-06T06:47:26.213576: step 10690, loss 0.00553937, acc 1
2016-09-06T06:47:27.028087: step 10691, loss 0.00374605, acc 1
2016-09-06T06:47:27.837274: step 10692, loss 0.00601355, acc 1
2016-09-06T06:47:28.603058: step 10693, loss 0.00416583, acc 1
2016-09-06T06:47:29.421053: step 10694, loss 0.0231321, acc 1
2016-09-06T06:47:30.215229: step 10695, loss 0.0198178, acc 0.98
2016-09-06T06:47:30.999374: step 10696, loss 0.013208, acc 1
2016-09-06T06:47:31.815089: step 10697, loss 0.00316825, acc 1
2016-09-06T06:47:32.634150: step 10698, loss 0.0743706, acc 0.96
2016-09-06T06:47:33.447972: step 10699, loss 0.00221246, acc 1
2016-09-06T06:47:34.306046: step 10700, loss 0.021793, acc 0.98

Evaluation:
2016-09-06T06:47:38.032778: step 10700, loss 2.07292, acc 0.731707

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-10700

2016-09-06T06:47:39.904185: step 10701, loss 0.0138208, acc 1
2016-09-06T06:47:40.698462: step 10702, loss 0.00259644, acc 1
2016-09-06T06:47:41.525727: step 10703, loss 0.00252167, acc 1
2016-09-06T06:47:42.324313: step 10704, loss 0.0504804, acc 0.96
2016-09-06T06:47:43.130703: step 10705, loss 0.0229415, acc 0.98
2016-09-06T06:47:43.954083: step 10706, loss 0.0105322, acc 1
2016-09-06T06:47:44.766548: step 10707, loss 0.00876794, acc 1
2016-09-06T06:47:45.568294: step 10708, loss 0.0223374, acc 0.98
2016-09-06T06:47:46.408612: step 10709, loss 0.0203003, acc 1
2016-09-06T06:47:47.221157: step 10710, loss 0.00879532, acc 1
2016-09-06T06:47:48.043863: step 10711, loss 0.00709652, acc 1
2016-09-06T06:47:48.884614: step 10712, loss 0.0283021, acc 0.98
2016-09-06T06:47:49.691277: step 10713, loss 0.0474084, acc 0.98
2016-09-06T06:47:50.498635: step 10714, loss 0.00558562, acc 1
2016-09-06T06:47:51.301666: step 10715, loss 0.00345341, acc 1
2016-09-06T06:47:52.096879: step 10716, loss 0.0049534, acc 1
2016-09-06T06:47:52.906008: step 10717, loss 0.0289033, acc 0.98
2016-09-06T06:47:53.723111: step 10718, loss 0.0243792, acc 0.98
2016-09-06T06:47:54.591141: step 10719, loss 0.0058276, acc 1
2016-09-06T06:47:55.367622: step 10720, loss 0.0569696, acc 0.98
2016-09-06T06:47:56.196280: step 10721, loss 0.0301149, acc 1
2016-09-06T06:47:56.977855: step 10722, loss 0.0134844, acc 1
2016-09-06T06:47:57.759109: step 10723, loss 0.00298294, acc 1
2016-09-06T06:47:58.561160: step 10724, loss 0.00352096, acc 1
2016-09-06T06:47:59.375796: step 10725, loss 0.00460959, acc 1
2016-09-06T06:48:00.188365: step 10726, loss 0.0580589, acc 0.96
2016-09-06T06:48:01.035276: step 10727, loss 0.00442604, acc 1
2016-09-06T06:48:01.825813: step 10728, loss 0.00687858, acc 1
2016-09-06T06:48:02.611947: step 10729, loss 0.0246493, acc 0.98
2016-09-06T06:48:03.411163: step 10730, loss 0.00571307, acc 1
2016-09-06T06:48:04.253751: step 10731, loss 0.0267378, acc 1
2016-09-06T06:48:05.052144: step 10732, loss 0.0180788, acc 1
2016-09-06T06:48:05.835044: step 10733, loss 0.00331391, acc 1
2016-09-06T06:48:06.640235: step 10734, loss 0.0169965, acc 0.98
2016-09-06T06:48:07.443771: step 10735, loss 0.0279685, acc 1
2016-09-06T06:48:08.273537: step 10736, loss 0.0361672, acc 0.98
2016-09-06T06:48:09.110585: step 10737, loss 0.0285991, acc 0.98
2016-09-06T06:48:09.897348: step 10738, loss 0.0128935, acc 1
2016-09-06T06:48:10.746908: step 10739, loss 0.0517122, acc 0.96
2016-09-06T06:48:11.574567: step 10740, loss 0.0449921, acc 0.98
2016-09-06T06:48:12.386948: step 10741, loss 0.0148369, acc 1
2016-09-06T06:48:13.224855: step 10742, loss 0.00271556, acc 1
2016-09-06T06:48:14.042322: step 10743, loss 0.00232418, acc 1
2016-09-06T06:48:14.847610: step 10744, loss 0.023437, acc 0.98
2016-09-06T06:48:15.662627: step 10745, loss 0.0221408, acc 0.98
2016-09-06T06:48:16.496167: step 10746, loss 0.0180517, acc 0.98
2016-09-06T06:48:17.312880: step 10747, loss 0.017146, acc 1
2016-09-06T06:48:18.137459: step 10748, loss 0.0062171, acc 1
2016-09-06T06:48:18.975543: step 10749, loss 0.0354527, acc 0.98
2016-09-06T06:48:19.780689: step 10750, loss 0.014201, acc 1
2016-09-06T06:48:20.578048: step 10751, loss 0.0402533, acc 0.96
2016-09-06T06:48:21.346761: step 10752, loss 0.0170963, acc 1
2016-09-06T06:48:22.170685: step 10753, loss 0.0346997, acc 0.98
2016-09-06T06:48:22.981825: step 10754, loss 0.0238941, acc 0.98
2016-09-06T06:48:23.831238: step 10755, loss 0.00963605, acc 1
2016-09-06T06:48:24.656516: step 10756, loss 0.0393084, acc 0.98
2016-09-06T06:48:25.458456: step 10757, loss 0.062615, acc 0.96
2016-09-06T06:48:26.306894: step 10758, loss 0.0874669, acc 0.98
2016-09-06T06:48:27.125310: step 10759, loss 0.00222979, acc 1
2016-09-06T06:48:27.918807: step 10760, loss 0.0150504, acc 1
2016-09-06T06:48:28.741674: step 10761, loss 0.0192203, acc 0.98
2016-09-06T06:48:29.558141: step 10762, loss 0.0232618, acc 1
2016-09-06T06:48:30.405470: step 10763, loss 0.0154586, acc 1
2016-09-06T06:48:31.210310: step 10764, loss 0.0114601, acc 1
2016-09-06T06:48:32.029412: step 10765, loss 0.0192838, acc 0.98
2016-09-06T06:48:32.800050: step 10766, loss 0.00294715, acc 1
2016-09-06T06:48:33.606750: step 10767, loss 0.0197653, acc 1
2016-09-06T06:48:34.420625: step 10768, loss 0.00576956, acc 1
2016-09-06T06:48:35.214659: step 10769, loss 0.027738, acc 0.98
2016-09-06T06:48:36.039007: step 10770, loss 0.00427562, acc 1
2016-09-06T06:48:36.847963: step 10771, loss 0.0228614, acc 0.98
2016-09-06T06:48:37.635394: step 10772, loss 0.00468299, acc 1
2016-09-06T06:48:38.435410: step 10773, loss 0.0070979, acc 1
2016-09-06T06:48:39.235207: step 10774, loss 0.0159259, acc 1
2016-09-06T06:48:40.036218: step 10775, loss 0.0286222, acc 0.98
2016-09-06T06:48:40.891574: step 10776, loss 0.0201422, acc 1
2016-09-06T06:48:41.736050: step 10777, loss 0.00264685, acc 1
2016-09-06T06:48:42.552867: step 10778, loss 0.0100216, acc 1
2016-09-06T06:48:43.392857: step 10779, loss 0.0276532, acc 0.98
2016-09-06T06:48:44.246178: step 10780, loss 0.00762733, acc 1
2016-09-06T06:48:45.060236: step 10781, loss 0.00444585, acc 1
2016-09-06T06:48:45.868178: step 10782, loss 0.00224143, acc 1
2016-09-06T06:48:46.694820: step 10783, loss 0.00239911, acc 1
2016-09-06T06:48:47.513094: step 10784, loss 0.00488314, acc 1
2016-09-06T06:48:48.318632: step 10785, loss 0.00815916, acc 1
2016-09-06T06:48:49.138555: step 10786, loss 0.0164525, acc 1
2016-09-06T06:48:49.956111: step 10787, loss 0.00250172, acc 1
2016-09-06T06:48:50.798603: step 10788, loss 0.0239458, acc 0.98
2016-09-06T06:48:51.616037: step 10789, loss 0.00332114, acc 1
2016-09-06T06:48:52.456994: step 10790, loss 0.0262266, acc 0.98
2016-09-06T06:48:53.284846: step 10791, loss 0.0174336, acc 0.98
2016-09-06T06:48:54.118765: step 10792, loss 0.0021963, acc 1
2016-09-06T06:48:54.923274: step 10793, loss 0.00251861, acc 1
2016-09-06T06:48:55.713938: step 10794, loss 0.00465105, acc 1
2016-09-06T06:48:56.543540: step 10795, loss 0.0435572, acc 0.98
2016-09-06T06:48:57.353591: step 10796, loss 0.00428934, acc 1
2016-09-06T06:48:58.172474: step 10797, loss 0.00282347, acc 1
2016-09-06T06:48:58.965257: step 10798, loss 0.0322127, acc 0.98
2016-09-06T06:48:59.833437: step 10799, loss 0.00348853, acc 1
2016-09-06T06:49:00.673906: step 10800, loss 0.0196906, acc 1

Evaluation:
2016-09-06T06:49:04.389613: step 10800, loss 2.66852, acc 0.727955

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-10800

2016-09-06T06:49:06.246452: step 10801, loss 0.0463395, acc 0.98
2016-09-06T06:49:07.062665: step 10802, loss 0.0062758, acc 1
2016-09-06T06:49:07.873488: step 10803, loss 0.015202, acc 1
2016-09-06T06:49:08.718264: step 10804, loss 0.0287688, acc 0.98
2016-09-06T06:49:09.531421: step 10805, loss 0.0115456, acc 1
2016-09-06T06:49:10.322029: step 10806, loss 0.002159, acc 1
2016-09-06T06:49:11.156176: step 10807, loss 0.0656296, acc 0.96
2016-09-06T06:49:11.974861: step 10808, loss 0.00546277, acc 1
2016-09-06T06:49:12.761061: step 10809, loss 0.00464233, acc 1
2016-09-06T06:49:13.578867: step 10810, loss 0.0278893, acc 1
2016-09-06T06:49:14.370729: step 10811, loss 0.00313602, acc 1
2016-09-06T06:49:15.193047: step 10812, loss 0.00218169, acc 1
2016-09-06T06:49:16.002444: step 10813, loss 0.0793499, acc 0.98
2016-09-06T06:49:16.828599: step 10814, loss 0.0176767, acc 0.98
2016-09-06T06:49:17.616606: step 10815, loss 0.0170968, acc 0.98
2016-09-06T06:49:18.405368: step 10816, loss 0.00830043, acc 1
2016-09-06T06:49:19.259285: step 10817, loss 0.0271177, acc 0.98
2016-09-06T06:49:20.061530: step 10818, loss 0.0193464, acc 0.98
2016-09-06T06:49:20.861478: step 10819, loss 0.00293579, acc 1
2016-09-06T06:49:21.674540: step 10820, loss 0.0380294, acc 0.98
2016-09-06T06:49:22.495521: step 10821, loss 0.00998758, acc 1
2016-09-06T06:49:23.317614: step 10822, loss 0.00483103, acc 1
2016-09-06T06:49:24.128382: step 10823, loss 0.19042, acc 0.98
2016-09-06T06:49:24.896623: step 10824, loss 0.00205298, acc 1
2016-09-06T06:49:25.715300: step 10825, loss 0.0211677, acc 0.98
2016-09-06T06:49:26.537097: step 10826, loss 0.00887357, acc 1
2016-09-06T06:49:27.333883: step 10827, loss 0.00474189, acc 1
2016-09-06T06:49:28.116690: step 10828, loss 0.0079754, acc 1
2016-09-06T06:49:28.924417: step 10829, loss 0.00509252, acc 1
2016-09-06T06:49:29.739349: step 10830, loss 0.0865762, acc 0.94
2016-09-06T06:49:30.577730: step 10831, loss 0.0452692, acc 0.98
2016-09-06T06:49:31.374092: step 10832, loss 0.0596328, acc 0.98
2016-09-06T06:49:32.152992: step 10833, loss 0.00568202, acc 1
2016-09-06T06:49:32.970527: step 10834, loss 0.0037533, acc 1
2016-09-06T06:49:33.793353: step 10835, loss 0.00971669, acc 1
2016-09-06T06:49:34.602098: step 10836, loss 0.0093253, acc 1
2016-09-06T06:49:35.406876: step 10837, loss 0.0524751, acc 0.98
2016-09-06T06:49:36.249794: step 10838, loss 0.0372804, acc 0.98
2016-09-06T06:49:37.063402: step 10839, loss 0.00245503, acc 1
2016-09-06T06:49:37.873924: step 10840, loss 0.0118966, acc 1
2016-09-06T06:49:38.739161: step 10841, loss 0.0319311, acc 0.98
2016-09-06T06:49:39.554133: step 10842, loss 0.00814079, acc 1
2016-09-06T06:49:40.378321: step 10843, loss 0.00719564, acc 1
2016-09-06T06:49:41.248681: step 10844, loss 0.0119839, acc 1
2016-09-06T06:49:42.069841: step 10845, loss 0.0056558, acc 1
2016-09-06T06:49:42.897151: step 10846, loss 0.00228038, acc 1
2016-09-06T06:49:43.739734: step 10847, loss 0.00406799, acc 1
2016-09-06T06:49:44.571590: step 10848, loss 0.0171007, acc 0.98
2016-09-06T06:49:45.374191: step 10849, loss 0.00513464, acc 1
2016-09-06T06:49:46.189108: step 10850, loss 0.0247291, acc 0.98
2016-09-06T06:49:46.982849: step 10851, loss 0.0155559, acc 1
2016-09-06T06:49:47.799953: step 10852, loss 0.0281644, acc 0.98
2016-09-06T06:49:48.624273: step 10853, loss 0.0172945, acc 1
2016-09-06T06:49:49.460033: step 10854, loss 0.00268284, acc 1
2016-09-06T06:49:50.253090: step 10855, loss 0.00237492, acc 1
2016-09-06T06:49:51.045877: step 10856, loss 0.00379944, acc 1
2016-09-06T06:49:51.867305: step 10857, loss 0.0148944, acc 1
2016-09-06T06:49:52.668261: step 10858, loss 0.0251605, acc 0.98
2016-09-06T06:49:53.456517: step 10859, loss 0.0427591, acc 0.98
2016-09-06T06:49:54.277523: step 10860, loss 0.00575498, acc 1
2016-09-06T06:49:55.052372: step 10861, loss 0.0313832, acc 0.98
2016-09-06T06:49:55.870827: step 10862, loss 0.0324207, acc 0.98
2016-09-06T06:49:56.777351: step 10863, loss 0.00892175, acc 1
2016-09-06T06:49:57.550856: step 10864, loss 0.0354702, acc 0.98
2016-09-06T06:49:58.334125: step 10865, loss 0.0058138, acc 1
2016-09-06T06:49:59.160151: step 10866, loss 0.00273901, acc 1
2016-09-06T06:49:59.912939: step 10867, loss 0.0123707, acc 1
2016-09-06T06:50:00.756053: step 10868, loss 0.00329662, acc 1
2016-09-06T06:50:01.595219: step 10869, loss 0.0027488, acc 1
2016-09-06T06:50:02.367230: step 10870, loss 0.00542187, acc 1
2016-09-06T06:50:03.156008: step 10871, loss 0.00668283, acc 1
2016-09-06T06:50:03.988672: step 10872, loss 0.0415952, acc 0.98
2016-09-06T06:50:04.752367: step 10873, loss 0.0180054, acc 0.98
2016-09-06T06:50:05.564308: step 10874, loss 0.00258884, acc 1
2016-09-06T06:50:06.377860: step 10875, loss 0.00244703, acc 1
2016-09-06T06:50:07.170713: step 10876, loss 0.00461811, acc 1
2016-09-06T06:50:07.975015: step 10877, loss 0.00844281, acc 1
2016-09-06T06:50:08.818142: step 10878, loss 0.00246394, acc 1
2016-09-06T06:50:09.601895: step 10879, loss 0.0482689, acc 0.98
2016-09-06T06:50:10.418935: step 10880, loss 0.00240147, acc 1
2016-09-06T06:50:11.225618: step 10881, loss 0.00581318, acc 1
2016-09-06T06:50:12.056424: step 10882, loss 0.143022, acc 0.98
2016-09-06T06:50:12.873928: step 10883, loss 0.0118703, acc 1
2016-09-06T06:50:13.702981: step 10884, loss 0.0388063, acc 0.98
2016-09-06T06:50:14.504471: step 10885, loss 0.013035, acc 1
2016-09-06T06:50:15.327179: step 10886, loss 0.036783, acc 1
2016-09-06T06:50:16.165265: step 10887, loss 0.00557735, acc 1
2016-09-06T06:50:16.992131: step 10888, loss 0.0102215, acc 1
2016-09-06T06:50:17.780849: step 10889, loss 0.00329993, acc 1
2016-09-06T06:50:18.643835: step 10890, loss 0.00475954, acc 1
2016-09-06T06:50:19.495004: step 10891, loss 0.00507609, acc 1
2016-09-06T06:50:20.328822: step 10892, loss 0.00585152, acc 1
2016-09-06T06:50:21.150717: step 10893, loss 0.0173368, acc 0.98
2016-09-06T06:50:21.950753: step 10894, loss 0.00647648, acc 1
2016-09-06T06:50:22.770057: step 10895, loss 0.0501767, acc 0.98
2016-09-06T06:50:23.595730: step 10896, loss 0.00893517, acc 1
2016-09-06T06:50:24.444155: step 10897, loss 0.00274376, acc 1
2016-09-06T06:50:25.238417: step 10898, loss 0.0240029, acc 0.98
2016-09-06T06:50:26.076281: step 10899, loss 0.0520794, acc 0.96
2016-09-06T06:50:26.865109: step 10900, loss 0.00313139, acc 1

Evaluation:
2016-09-06T06:50:30.553577: step 10900, loss 2.54482, acc 0.721388

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-10900

2016-09-06T06:50:32.576906: step 10901, loss 0.00407661, acc 1
2016-09-06T06:50:33.409315: step 10902, loss 0.00257818, acc 1
2016-09-06T06:50:34.253296: step 10903, loss 0.00256029, acc 1
2016-09-06T06:50:35.049424: step 10904, loss 0.00265273, acc 1
2016-09-06T06:50:35.859693: step 10905, loss 0.00368278, acc 1
2016-09-06T06:50:36.684917: step 10906, loss 0.00275577, acc 1
2016-09-06T06:50:37.452065: step 10907, loss 0.00877786, acc 1
2016-09-06T06:50:38.238881: step 10908, loss 0.0121379, acc 1
2016-09-06T06:50:39.065220: step 10909, loss 0.0193584, acc 1
2016-09-06T06:50:39.848101: step 10910, loss 0.0252579, acc 0.98
2016-09-06T06:50:40.651969: step 10911, loss 0.0462663, acc 0.98
2016-09-06T06:50:41.519661: step 10912, loss 0.0213414, acc 1
2016-09-06T06:50:42.319192: step 10913, loss 0.0333194, acc 0.98
2016-09-06T06:50:43.114740: step 10914, loss 0.0350122, acc 0.96
2016-09-06T06:50:43.970615: step 10915, loss 0.00320573, acc 1
2016-09-06T06:50:44.843676: step 10916, loss 0.00291837, acc 1
2016-09-06T06:50:45.676688: step 10917, loss 0.00865576, acc 1
2016-09-06T06:50:46.531121: step 10918, loss 0.0161037, acc 1
2016-09-06T06:50:47.367505: step 10919, loss 0.004064, acc 1
2016-09-06T06:50:48.234634: step 10920, loss 0.0119564, acc 1
2016-09-06T06:50:49.071651: step 10921, loss 0.0788631, acc 0.96
2016-09-06T06:50:49.870647: step 10922, loss 0.0240804, acc 1
2016-09-06T06:50:50.676532: step 10923, loss 0.004672, acc 1
2016-09-06T06:50:51.517572: step 10924, loss 0.00364763, acc 1
2016-09-06T06:50:52.357398: step 10925, loss 0.00766192, acc 1
2016-09-06T06:50:53.140305: step 10926, loss 0.0157227, acc 1
2016-09-06T06:50:53.930554: step 10927, loss 0.038209, acc 0.98
2016-09-06T06:50:54.772218: step 10928, loss 0.00344053, acc 1
2016-09-06T06:50:55.548338: step 10929, loss 0.00795382, acc 1
2016-09-06T06:50:56.370818: step 10930, loss 0.0956844, acc 0.98
2016-09-06T06:50:57.200728: step 10931, loss 0.00382703, acc 1
2016-09-06T06:50:58.012012: step 10932, loss 0.0179582, acc 1
2016-09-06T06:50:58.813914: step 10933, loss 0.00383734, acc 1
2016-09-06T06:50:59.645008: step 10934, loss 0.00309676, acc 1
2016-09-06T06:51:00.446591: step 10935, loss 0.00495998, acc 1
2016-09-06T06:51:01.264535: step 10936, loss 0.0039796, acc 1
2016-09-06T06:51:02.083233: step 10937, loss 0.00814601, acc 1
2016-09-06T06:51:02.877191: step 10938, loss 0.0427925, acc 0.98
2016-09-06T06:51:03.680593: step 10939, loss 0.0155447, acc 1
2016-09-06T06:51:04.500018: step 10940, loss 0.00320935, acc 1
2016-09-06T06:51:05.279293: step 10941, loss 0.0246599, acc 0.98
2016-09-06T06:51:06.086479: step 10942, loss 0.00299768, acc 1
2016-09-06T06:51:06.922958: step 10943, loss 0.0280401, acc 0.98
2016-09-06T06:51:07.690442: step 10944, loss 0.00374029, acc 1
2016-09-06T06:51:08.531280: step 10945, loss 0.0105893, acc 1
2016-09-06T06:51:09.329447: step 10946, loss 0.0183819, acc 0.98
2016-09-06T06:51:10.118627: step 10947, loss 0.00286486, acc 1
2016-09-06T06:51:10.925836: step 10948, loss 0.0100536, acc 1
2016-09-06T06:51:11.725721: step 10949, loss 0.0174099, acc 0.98
2016-09-06T06:51:12.520061: step 10950, loss 0.00384751, acc 1
2016-09-06T06:51:13.339250: step 10951, loss 0.0112674, acc 1
2016-09-06T06:51:14.138185: step 10952, loss 0.0171171, acc 1
2016-09-06T06:51:14.960178: step 10953, loss 0.043368, acc 0.98
2016-09-06T06:51:15.773070: step 10954, loss 0.00266077, acc 1
2016-09-06T06:51:16.593978: step 10955, loss 0.00264465, acc 1
2016-09-06T06:51:17.396963: step 10956, loss 0.00272054, acc 1
2016-09-06T06:51:18.235289: step 10957, loss 0.00972978, acc 1
2016-09-06T06:51:19.063313: step 10958, loss 0.00406588, acc 1
2016-09-06T06:51:19.893593: step 10959, loss 0.030946, acc 0.96
2016-09-06T06:51:20.708864: step 10960, loss 0.00437252, acc 1
2016-09-06T06:51:21.528260: step 10961, loss 0.00252394, acc 1
2016-09-06T06:51:22.328686: step 10962, loss 0.00283334, acc 1
2016-09-06T06:51:23.144510: step 10963, loss 0.0532292, acc 0.98
2016-09-06T06:51:23.973950: step 10964, loss 0.00245763, acc 1
2016-09-06T06:51:24.779818: step 10965, loss 0.00333157, acc 1
2016-09-06T06:51:25.596707: step 10966, loss 0.0188137, acc 0.98
2016-09-06T06:51:26.444001: step 10967, loss 0.0151871, acc 1
2016-09-06T06:51:27.321936: step 10968, loss 0.00238644, acc 1
2016-09-06T06:51:28.150121: step 10969, loss 0.0346779, acc 0.98
2016-09-06T06:51:29.002491: step 10970, loss 0.0189402, acc 0.98
2016-09-06T06:51:29.814375: step 10971, loss 0.0884085, acc 0.96
2016-09-06T06:51:30.646890: step 10972, loss 0.00257543, acc 1
2016-09-06T06:51:31.499279: step 10973, loss 0.0446921, acc 0.96
2016-09-06T06:51:32.319703: step 10974, loss 0.00248687, acc 1
2016-09-06T06:51:33.147596: step 10975, loss 0.0037714, acc 1
2016-09-06T06:51:33.952297: step 10976, loss 0.00537062, acc 1
2016-09-06T06:51:34.781405: step 10977, loss 0.0118647, acc 1
2016-09-06T06:51:35.559980: step 10978, loss 0.0441461, acc 0.98
2016-09-06T06:51:36.370618: step 10979, loss 0.0287457, acc 0.98
2016-09-06T06:51:37.213481: step 10980, loss 0.00748012, acc 1
2016-09-06T06:51:38.005550: step 10981, loss 0.00901829, acc 1
2016-09-06T06:51:38.784172: step 10982, loss 0.00253119, acc 1
2016-09-06T06:51:39.573972: step 10983, loss 0.0114387, acc 1
2016-09-06T06:51:40.383943: step 10984, loss 0.0018217, acc 1
2016-09-06T06:51:41.222943: step 10985, loss 0.0265933, acc 0.98
2016-09-06T06:51:42.045674: step 10986, loss 0.00693756, acc 1
2016-09-06T06:51:42.857571: step 10987, loss 0.0447431, acc 0.98
2016-09-06T06:51:43.661035: step 10988, loss 0.00424201, acc 1
2016-09-06T06:51:44.491002: step 10989, loss 0.00300942, acc 1
2016-09-06T06:51:45.279025: step 10990, loss 0.0196608, acc 0.98
2016-09-06T06:51:46.085878: step 10991, loss 0.0294947, acc 0.98
2016-09-06T06:51:46.917032: step 10992, loss 0.0423532, acc 0.98
2016-09-06T06:51:47.738650: step 10993, loss 0.0128452, acc 1
2016-09-06T06:51:48.542801: step 10994, loss 0.00179205, acc 1
2016-09-06T06:51:49.361645: step 10995, loss 0.025409, acc 1
2016-09-06T06:51:50.172808: step 10996, loss 0.00736733, acc 1
2016-09-06T06:51:50.996596: step 10997, loss 0.045774, acc 0.96
2016-09-06T06:51:51.825496: step 10998, loss 0.00439493, acc 1
2016-09-06T06:51:52.660556: step 10999, loss 0.00920315, acc 1
2016-09-06T06:51:53.460853: step 11000, loss 0.00176374, acc 1

Evaluation:
2016-09-06T06:51:57.180339: step 11000, loss 2.22611, acc 0.736398

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-11000

2016-09-06T06:51:59.114174: step 11001, loss 0.00782702, acc 1
2016-09-06T06:51:59.901070: step 11002, loss 0.00479128, acc 1
2016-09-06T06:52:00.691210: step 11003, loss 0.0346517, acc 0.98
2016-09-06T06:52:01.522216: step 11004, loss 0.00218668, acc 1
2016-09-06T06:52:02.341299: step 11005, loss 0.0257258, acc 0.98
2016-09-06T06:52:03.166203: step 11006, loss 0.0166575, acc 1
2016-09-06T06:52:03.985185: step 11007, loss 0.00329473, acc 1
2016-09-06T06:52:04.811915: step 11008, loss 0.0241105, acc 1
2016-09-06T06:52:05.637207: step 11009, loss 0.00763942, acc 1
2016-09-06T06:52:06.457007: step 11010, loss 0.0172057, acc 1
2016-09-06T06:52:07.263951: step 11011, loss 0.00713056, acc 1
2016-09-06T06:52:08.086479: step 11012, loss 0.0740981, acc 0.98
2016-09-06T06:52:08.893807: step 11013, loss 0.015186, acc 1
2016-09-06T06:52:09.696829: step 11014, loss 0.0352139, acc 0.98
2016-09-06T06:52:10.507726: step 11015, loss 0.0264062, acc 0.98
2016-09-06T06:52:11.319808: step 11016, loss 0.0110659, acc 1
2016-09-06T06:52:12.116281: step 11017, loss 0.0441151, acc 0.96
2016-09-06T06:52:12.910125: step 11018, loss 0.0164599, acc 1
2016-09-06T06:52:13.727853: step 11019, loss 0.0156673, acc 1
2016-09-06T06:52:14.538044: step 11020, loss 0.0155725, acc 1
2016-09-06T06:52:15.349333: step 11021, loss 0.0279992, acc 0.98
2016-09-06T06:52:16.162310: step 11022, loss 0.0151276, acc 1
2016-09-06T06:52:16.986309: step 11023, loss 0.0108021, acc 1
2016-09-06T06:52:17.797001: step 11024, loss 0.0077694, acc 1
2016-09-06T06:52:18.656121: step 11025, loss 0.00201966, acc 1
2016-09-06T06:52:19.493806: step 11026, loss 0.034021, acc 0.98
2016-09-06T06:52:20.315400: step 11027, loss 0.0268461, acc 1
2016-09-06T06:52:21.157146: step 11028, loss 0.00960936, acc 1
2016-09-06T06:52:21.943287: step 11029, loss 0.00397074, acc 1
2016-09-06T06:52:22.768438: step 11030, loss 0.0323142, acc 0.98
2016-09-06T06:52:23.598366: step 11031, loss 0.002086, acc 1
2016-09-06T06:52:24.421954: step 11032, loss 0.0393265, acc 0.98
2016-09-06T06:52:25.246261: step 11033, loss 0.0623936, acc 0.98
2016-09-06T06:52:26.089309: step 11034, loss 0.00312029, acc 1
2016-09-06T06:52:26.903185: step 11035, loss 0.0302613, acc 0.98
2016-09-06T06:52:27.690114: step 11036, loss 0.00189573, acc 1
2016-09-06T06:52:28.512744: step 11037, loss 0.00776095, acc 1
2016-09-06T06:52:29.319626: step 11038, loss 0.00248951, acc 1
2016-09-06T06:52:30.118576: step 11039, loss 0.0243187, acc 0.98
2016-09-06T06:52:30.947951: step 11040, loss 0.0619223, acc 0.96
2016-09-06T06:52:31.745461: step 11041, loss 0.00255508, acc 1
2016-09-06T06:52:32.543032: step 11042, loss 0.0102611, acc 1
2016-09-06T06:52:33.373433: step 11043, loss 0.00218399, acc 1
2016-09-06T06:52:34.211678: step 11044, loss 0.00682745, acc 1
2016-09-06T06:52:35.003904: step 11045, loss 0.0462435, acc 0.98
2016-09-06T06:52:35.821152: step 11046, loss 0.00225981, acc 1
2016-09-06T06:52:36.627894: step 11047, loss 0.0247691, acc 1
2016-09-06T06:52:37.430791: step 11048, loss 0.00219312, acc 1
2016-09-06T06:52:38.243799: step 11049, loss 0.00685133, acc 1
2016-09-06T06:52:39.081571: step 11050, loss 0.0523997, acc 0.98
2016-09-06T06:52:39.847486: step 11051, loss 0.0160875, acc 1
2016-09-06T06:52:40.691053: step 11052, loss 0.00203015, acc 1
2016-09-06T06:52:41.497102: step 11053, loss 0.0355745, acc 0.98
2016-09-06T06:52:42.295396: step 11054, loss 0.0155548, acc 1
2016-09-06T06:52:43.107920: step 11055, loss 0.00224483, acc 1
2016-09-06T06:52:43.923467: step 11056, loss 0.020156, acc 0.98
2016-09-06T06:52:44.724621: step 11057, loss 0.0266105, acc 0.98
2016-09-06T06:52:45.543585: step 11058, loss 0.0344437, acc 0.98
2016-09-06T06:52:46.358299: step 11059, loss 0.00490464, acc 1
2016-09-06T06:52:47.150253: step 11060, loss 0.0489274, acc 0.98
2016-09-06T06:52:47.941245: step 11061, loss 0.02508, acc 1
2016-09-06T06:52:48.743276: step 11062, loss 0.00192455, acc 1
2016-09-06T06:52:49.543594: step 11063, loss 0.00265817, acc 1
2016-09-06T06:52:50.326351: step 11064, loss 0.140272, acc 0.96
2016-09-06T06:52:51.137484: step 11065, loss 0.0131432, acc 1
2016-09-06T06:52:51.923966: step 11066, loss 0.0159281, acc 1
2016-09-06T06:52:52.714672: step 11067, loss 0.00208849, acc 1
2016-09-06T06:52:53.552437: step 11068, loss 0.0589086, acc 0.98
2016-09-06T06:52:54.337624: step 11069, loss 0.0623453, acc 0.94
2016-09-06T06:52:55.180998: step 11070, loss 0.00209415, acc 1
2016-09-06T06:52:56.005799: step 11071, loss 0.0284664, acc 0.98
2016-09-06T06:52:56.834434: step 11072, loss 0.0416824, acc 0.98
2016-09-06T06:52:57.639469: step 11073, loss 0.00958361, acc 1
2016-09-06T06:52:58.461407: step 11074, loss 0.012102, acc 1
2016-09-06T06:52:59.278992: step 11075, loss 0.0175544, acc 1
2016-09-06T06:53:00.078409: step 11076, loss 0.0136311, acc 1
2016-09-06T06:53:00.964470: step 11077, loss 0.0153569, acc 1
2016-09-06T06:53:01.790901: step 11078, loss 0.0286232, acc 1
2016-09-06T06:53:02.609849: step 11079, loss 0.011452, acc 1
2016-09-06T06:53:03.467162: step 11080, loss 0.0135816, acc 1
2016-09-06T06:53:04.280352: step 11081, loss 0.0156651, acc 1
2016-09-06T06:53:05.103680: step 11082, loss 0.0230794, acc 0.98
2016-09-06T06:53:05.933633: step 11083, loss 0.00329684, acc 1
2016-09-06T06:53:06.757365: step 11084, loss 0.00268296, acc 1
2016-09-06T06:53:07.546252: step 11085, loss 0.0322129, acc 1
2016-09-06T06:53:08.361393: step 11086, loss 0.00265618, acc 1
2016-09-06T06:53:09.193127: step 11087, loss 0.00387399, acc 1
2016-09-06T06:53:10.028651: step 11088, loss 0.0325133, acc 0.98
2016-09-06T06:53:10.872788: step 11089, loss 0.0060688, acc 1
2016-09-06T06:53:11.743071: step 11090, loss 0.0323305, acc 0.98
2016-09-06T06:53:12.555414: step 11091, loss 0.0320988, acc 0.98
2016-09-06T06:53:13.375180: step 11092, loss 0.0246024, acc 0.98
2016-09-06T06:53:14.195238: step 11093, loss 0.0253983, acc 0.98
2016-09-06T06:53:15.014246: step 11094, loss 0.0196548, acc 0.98
2016-09-06T06:53:15.848648: step 11095, loss 0.00244655, acc 1
2016-09-06T06:53:16.670637: step 11096, loss 0.00223106, acc 1
2016-09-06T06:53:17.496200: step 11097, loss 0.0027539, acc 1
2016-09-06T06:53:18.301720: step 11098, loss 0.00353435, acc 1
2016-09-06T06:53:19.140179: step 11099, loss 0.0116849, acc 1
2016-09-06T06:53:19.944208: step 11100, loss 0.0355952, acc 0.98

Evaluation:
2016-09-06T06:53:23.696305: step 11100, loss 2.47323, acc 0.733584

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-11100

2016-09-06T06:53:25.551025: step 11101, loss 0.0025028, acc 1
2016-09-06T06:53:26.365705: step 11102, loss 0.029644, acc 0.98
2016-09-06T06:53:27.174608: step 11103, loss 0.00933184, acc 1
2016-09-06T06:53:27.991374: step 11104, loss 0.00260458, acc 1
2016-09-06T06:53:28.816104: step 11105, loss 0.0206939, acc 0.98
2016-09-06T06:53:29.638396: step 11106, loss 0.00824512, acc 1
2016-09-06T06:53:30.469407: step 11107, loss 0.0353631, acc 0.98
2016-09-06T06:53:31.309462: step 11108, loss 0.0432006, acc 0.98
2016-09-06T06:53:32.130989: step 11109, loss 0.101316, acc 0.98
2016-09-06T06:53:32.956142: step 11110, loss 0.0405882, acc 0.98
2016-09-06T06:53:33.767334: step 11111, loss 0.00235354, acc 1
2016-09-06T06:53:34.601401: step 11112, loss 0.00494334, acc 1
2016-09-06T06:53:35.395880: step 11113, loss 0.117943, acc 0.94
2016-09-06T06:53:36.224689: step 11114, loss 0.010722, acc 1
2016-09-06T06:53:37.028740: step 11115, loss 0.0261668, acc 1
2016-09-06T06:53:37.850704: step 11116, loss 0.0069508, acc 1
2016-09-06T06:53:38.694444: step 11117, loss 0.00225108, acc 1
2016-09-06T06:53:39.505252: step 11118, loss 0.00478792, acc 1
2016-09-06T06:53:40.302092: step 11119, loss 0.0166287, acc 1
2016-09-06T06:53:41.149098: step 11120, loss 0.0180391, acc 0.98
2016-09-06T06:53:41.986461: step 11121, loss 0.0152316, acc 1
2016-09-06T06:53:42.792103: step 11122, loss 0.0359968, acc 0.96
2016-09-06T06:53:43.601099: step 11123, loss 0.00501388, acc 1
2016-09-06T06:53:44.413511: step 11124, loss 0.0315167, acc 0.98
2016-09-06T06:53:45.218534: step 11125, loss 0.0356831, acc 0.98
2016-09-06T06:53:46.024016: step 11126, loss 0.0113677, acc 1
2016-09-06T06:53:46.847770: step 11127, loss 0.0304022, acc 0.98
2016-09-06T06:53:47.628256: step 11128, loss 0.00383181, acc 1
2016-09-06T06:53:48.456210: step 11129, loss 0.0117659, acc 1
2016-09-06T06:53:49.273921: step 11130, loss 0.0160545, acc 1
2016-09-06T06:53:50.061586: step 11131, loss 0.0224817, acc 0.98
2016-09-06T06:53:50.900563: step 11132, loss 0.00515285, acc 1
2016-09-06T06:53:51.752438: step 11133, loss 0.0248411, acc 1
2016-09-06T06:53:52.553294: step 11134, loss 0.020244, acc 0.98
2016-09-06T06:53:53.374461: step 11135, loss 0.0369779, acc 0.98
2016-09-06T06:53:54.160085: step 11136, loss 0.00383392, acc 1
2016-09-06T06:53:54.938048: step 11137, loss 0.0530847, acc 0.98
2016-09-06T06:53:55.712798: step 11138, loss 0.0535713, acc 0.96
2016-09-06T06:53:56.554504: step 11139, loss 0.0031197, acc 1
2016-09-06T06:53:57.325945: step 11140, loss 0.015522, acc 1
2016-09-06T06:53:58.136705: step 11141, loss 0.0609298, acc 0.98
2016-09-06T06:53:58.982325: step 11142, loss 0.01782, acc 1
2016-09-06T06:53:59.756853: step 11143, loss 0.00354318, acc 1
2016-09-06T06:54:00.577472: step 11144, loss 0.0168605, acc 0.98
2016-09-06T06:54:01.396692: step 11145, loss 0.0178053, acc 0.98
2016-09-06T06:54:02.184033: step 11146, loss 0.00396656, acc 1
2016-09-06T06:54:03.007503: step 11147, loss 0.00263788, acc 1
2016-09-06T06:54:03.846273: step 11148, loss 0.00266905, acc 1
2016-09-06T06:54:04.659357: step 11149, loss 0.0195534, acc 0.98
2016-09-06T06:54:05.474861: step 11150, loss 0.0301705, acc 1
2016-09-06T06:54:06.307957: step 11151, loss 0.0352212, acc 0.98
2016-09-06T06:54:07.131566: step 11152, loss 0.00259809, acc 1
2016-09-06T06:54:07.947068: step 11153, loss 0.00290479, acc 1
2016-09-06T06:54:08.772781: step 11154, loss 0.00249565, acc 1
2016-09-06T06:54:09.587227: step 11155, loss 0.011534, acc 1
2016-09-06T06:54:10.405681: step 11156, loss 0.00242729, acc 1
2016-09-06T06:54:11.251758: step 11157, loss 0.00300593, acc 1
2016-09-06T06:54:12.064159: step 11158, loss 0.0199074, acc 0.98
2016-09-06T06:54:12.853355: step 11159, loss 0.0110682, acc 1
2016-09-06T06:54:13.686611: step 11160, loss 0.00285193, acc 1
2016-09-06T06:54:14.508722: step 11161, loss 0.0195187, acc 0.98
2016-09-06T06:54:15.332924: step 11162, loss 0.0121743, acc 1
2016-09-06T06:54:16.156692: step 11163, loss 0.00267903, acc 1
2016-09-06T06:54:16.948986: step 11164, loss 0.0159931, acc 1
2016-09-06T06:54:17.746410: step 11165, loss 0.0024966, acc 1
2016-09-06T06:54:18.565653: step 11166, loss 0.00668252, acc 1
2016-09-06T06:54:19.362556: step 11167, loss 0.00677288, acc 1
2016-09-06T06:54:20.173685: step 11168, loss 0.00611808, acc 1
2016-09-06T06:54:21.020339: step 11169, loss 0.0259585, acc 0.98
2016-09-06T06:54:21.831526: step 11170, loss 0.0197385, acc 1
2016-09-06T06:54:22.649481: step 11171, loss 0.00291038, acc 1
2016-09-06T06:54:23.483112: step 11172, loss 0.00916381, acc 1
2016-09-06T06:54:24.297711: step 11173, loss 0.00256479, acc 1
2016-09-06T06:54:25.095404: step 11174, loss 0.0831297, acc 0.94
2016-09-06T06:54:25.900693: step 11175, loss 0.020441, acc 1
2016-09-06T06:54:26.713994: step 11176, loss 0.00479891, acc 1
2016-09-06T06:54:27.488045: step 11177, loss 0.0714411, acc 0.98
2016-09-06T06:54:28.324529: step 11178, loss 0.00584266, acc 1
2016-09-06T06:54:29.150480: step 11179, loss 0.0513407, acc 0.96
2016-09-06T06:54:29.984257: step 11180, loss 0.00787728, acc 1
2016-09-06T06:54:30.750445: step 11181, loss 0.00359174, acc 1
2016-09-06T06:54:31.557471: step 11182, loss 0.0163945, acc 1
2016-09-06T06:54:32.353687: step 11183, loss 0.00483667, acc 1
2016-09-06T06:54:33.141044: step 11184, loss 0.0047554, acc 1
2016-09-06T06:54:33.949625: step 11185, loss 0.0100389, acc 1
2016-09-06T06:54:34.735059: step 11186, loss 0.00207991, acc 1
2016-09-06T06:54:35.588833: step 11187, loss 0.0124266, acc 1
2016-09-06T06:54:36.437156: step 11188, loss 0.0495219, acc 0.98
2016-09-06T06:54:37.251631: step 11189, loss 0.00320368, acc 1
2016-09-06T06:54:38.068680: step 11190, loss 0.00353675, acc 1
2016-09-06T06:54:38.897132: step 11191, loss 0.0269231, acc 0.98
2016-09-06T06:54:39.741567: step 11192, loss 0.00856514, acc 1
2016-09-06T06:54:40.539277: step 11193, loss 0.0138617, acc 1
2016-09-06T06:54:41.395986: step 11194, loss 0.0176421, acc 1
2016-09-06T06:54:42.184444: step 11195, loss 0.00224213, acc 1
2016-09-06T06:54:43.000618: step 11196, loss 0.00268345, acc 1
2016-09-06T06:54:43.868391: step 11197, loss 0.0605364, acc 0.98
2016-09-06T06:54:44.685447: step 11198, loss 0.0026688, acc 1
2016-09-06T06:54:45.509536: step 11199, loss 0.0160685, acc 0.98
2016-09-06T06:54:46.341926: step 11200, loss 0.0303677, acc 0.98

Evaluation:
2016-09-06T06:54:50.075440: step 11200, loss 2.67624, acc 0.729831

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-11200

2016-09-06T06:54:52.017762: step 11201, loss 0.0119017, acc 1
2016-09-06T06:54:52.832640: step 11202, loss 0.0193803, acc 1
2016-09-06T06:54:53.684016: step 11203, loss 0.0119775, acc 1
2016-09-06T06:54:54.507538: step 11204, loss 0.0388797, acc 1
2016-09-06T06:54:55.380194: step 11205, loss 0.0239979, acc 1
2016-09-06T06:54:56.210394: step 11206, loss 0.0027654, acc 1
2016-09-06T06:54:57.029548: step 11207, loss 0.052694, acc 0.98
2016-09-06T06:54:57.848303: step 11208, loss 0.00829247, acc 1
2016-09-06T06:54:58.661635: step 11209, loss 0.00224845, acc 1
2016-09-06T06:54:59.478964: step 11210, loss 0.00640456, acc 1
2016-09-06T06:55:00.285905: step 11211, loss 0.00449495, acc 1
2016-09-06T06:55:01.093610: step 11212, loss 0.0137885, acc 1
2016-09-06T06:55:01.933461: step 11213, loss 0.0278569, acc 0.98
2016-09-06T06:55:02.756095: step 11214, loss 0.131416, acc 0.94
2016-09-06T06:55:03.556400: step 11215, loss 0.0242981, acc 0.98
2016-09-06T06:55:04.379548: step 11216, loss 0.0172095, acc 1
2016-09-06T06:55:05.191023: step 11217, loss 0.0118453, acc 1
2016-09-06T06:55:06.004074: step 11218, loss 0.00180725, acc 1
2016-09-06T06:55:06.874429: step 11219, loss 0.00583946, acc 1
2016-09-06T06:55:07.649415: step 11220, loss 0.00199793, acc 1
2016-09-06T06:55:08.477505: step 11221, loss 0.00601261, acc 1
2016-09-06T06:55:09.321163: step 11222, loss 0.0217862, acc 0.98
2016-09-06T06:55:10.131054: step 11223, loss 0.0121121, acc 1
2016-09-06T06:55:10.948400: step 11224, loss 0.0157996, acc 1
2016-09-06T06:55:11.817322: step 11225, loss 0.0111994, acc 1
2016-09-06T06:55:12.636901: step 11226, loss 0.0434561, acc 0.98
2016-09-06T06:55:13.414724: step 11227, loss 0.0202132, acc 0.98
2016-09-06T06:55:14.225872: step 11228, loss 0.00489447, acc 1
2016-09-06T06:55:15.000865: step 11229, loss 0.0183523, acc 0.98
2016-09-06T06:55:15.840567: step 11230, loss 0.00233144, acc 1
2016-09-06T06:55:16.667692: step 11231, loss 0.0234443, acc 0.98
2016-09-06T06:55:17.496082: step 11232, loss 0.00236251, acc 1
2016-09-06T06:55:18.292526: step 11233, loss 0.00248795, acc 1
2016-09-06T06:55:19.118555: step 11234, loss 0.0301195, acc 0.98
2016-09-06T06:55:19.953769: step 11235, loss 0.00798752, acc 1
2016-09-06T06:55:20.735140: step 11236, loss 0.0216901, acc 0.98
2016-09-06T06:55:21.540668: step 11237, loss 0.0276861, acc 0.98
2016-09-06T06:55:22.345040: step 11238, loss 0.0542009, acc 0.98
2016-09-06T06:55:23.150758: step 11239, loss 0.0216458, acc 1
2016-09-06T06:55:23.956286: step 11240, loss 0.00193816, acc 1
2016-09-06T06:55:24.788615: step 11241, loss 0.0179116, acc 1
2016-09-06T06:55:25.573212: step 11242, loss 0.0259845, acc 1
2016-09-06T06:55:26.385368: step 11243, loss 0.0260274, acc 0.98
2016-09-06T06:55:27.222686: step 11244, loss 0.0158002, acc 1
2016-09-06T06:55:28.028753: step 11245, loss 0.0297403, acc 0.98
2016-09-06T06:55:28.858378: step 11246, loss 0.0257734, acc 0.98
2016-09-06T06:55:29.721629: step 11247, loss 0.0154122, acc 1
2016-09-06T06:55:30.537307: step 11248, loss 0.029083, acc 1
2016-09-06T06:55:31.353672: step 11249, loss 0.0335965, acc 0.98
2016-09-06T06:55:32.179160: step 11250, loss 0.00260448, acc 1
2016-09-06T06:55:32.993779: step 11251, loss 0.0662706, acc 0.98
2016-09-06T06:55:33.791962: step 11252, loss 0.00326096, acc 1
2016-09-06T06:55:34.610258: step 11253, loss 0.00639705, acc 1
2016-09-06T06:55:35.438879: step 11254, loss 0.00172763, acc 1
2016-09-06T06:55:36.268978: step 11255, loss 0.00210817, acc 1
2016-09-06T06:55:37.101748: step 11256, loss 0.0518967, acc 0.98
2016-09-06T06:55:37.981505: step 11257, loss 0.0021333, acc 1
2016-09-06T06:55:38.780768: step 11258, loss 0.00188014, acc 1
2016-09-06T06:55:39.623943: step 11259, loss 0.00660467, acc 1
2016-09-06T06:55:40.430705: step 11260, loss 0.0107035, acc 1
2016-09-06T06:55:41.240857: step 11261, loss 0.020992, acc 1
2016-09-06T06:55:42.052194: step 11262, loss 0.0129497, acc 1
2016-09-06T06:55:42.882650: step 11263, loss 0.0135264, acc 1
2016-09-06T06:55:43.686993: step 11264, loss 0.0045955, acc 1
2016-09-06T06:55:44.491251: step 11265, loss 0.00345259, acc 1
2016-09-06T06:55:45.321493: step 11266, loss 0.0494794, acc 0.98
2016-09-06T06:55:46.106405: step 11267, loss 0.0855663, acc 0.96
2016-09-06T06:55:46.919873: step 11268, loss 0.00701396, acc 1
2016-09-06T06:55:47.766923: step 11269, loss 0.00461199, acc 1
2016-09-06T06:55:48.568861: step 11270, loss 0.020896, acc 1
2016-09-06T06:55:49.375636: step 11271, loss 0.00494269, acc 1
2016-09-06T06:55:50.185382: step 11272, loss 0.0392185, acc 0.96
2016-09-06T06:55:50.980229: step 11273, loss 0.0199567, acc 0.98
2016-09-06T06:55:51.773211: step 11274, loss 0.057089, acc 0.96
2016-09-06T06:55:52.588519: step 11275, loss 0.00463295, acc 1
2016-09-06T06:55:53.359085: step 11276, loss 0.0118203, acc 1
2016-09-06T06:55:54.163617: step 11277, loss 0.00780253, acc 1
2016-09-06T06:55:54.973072: step 11278, loss 0.0131207, acc 1
2016-09-06T06:55:55.769606: step 11279, loss 0.0380849, acc 0.98
2016-09-06T06:55:56.604662: step 11280, loss 0.0135187, acc 1
2016-09-06T06:55:57.452206: step 11281, loss 0.0300313, acc 0.98
2016-09-06T06:55:58.254825: step 11282, loss 0.0223319, acc 1
2016-09-06T06:55:59.052867: step 11283, loss 0.0225288, acc 1
2016-09-06T06:55:59.854836: step 11284, loss 0.00391877, acc 1
2016-09-06T06:56:00.665410: step 11285, loss 0.168252, acc 0.96
2016-09-06T06:56:01.492355: step 11286, loss 0.00797769, acc 1
2016-09-06T06:56:02.377592: step 11287, loss 0.00397691, acc 1
2016-09-06T06:56:03.198466: step 11288, loss 0.0229587, acc 0.98
2016-09-06T06:56:04.019192: step 11289, loss 0.0205883, acc 0.98
2016-09-06T06:56:04.838025: step 11290, loss 0.00725814, acc 1
2016-09-06T06:56:05.700212: step 11291, loss 0.0135255, acc 1
2016-09-06T06:56:06.530248: step 11292, loss 0.0278011, acc 1
2016-09-06T06:56:07.346953: step 11293, loss 0.0163164, acc 1
2016-09-06T06:56:08.207695: step 11294, loss 0.0215092, acc 1
2016-09-06T06:56:09.023844: step 11295, loss 0.010962, acc 1
2016-09-06T06:56:09.853882: step 11296, loss 0.0482008, acc 0.98
2016-09-06T06:56:10.663193: step 11297, loss 0.0235684, acc 1
2016-09-06T06:56:11.477019: step 11298, loss 0.00435284, acc 1
2016-09-06T06:56:12.287447: step 11299, loss 0.017631, acc 0.98
2016-09-06T06:56:13.122795: step 11300, loss 0.0820422, acc 0.96

Evaluation:
2016-09-06T06:56:16.849142: step 11300, loss 2.44971, acc 0.732645

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-11300

2016-09-06T06:56:18.807957: step 11301, loss 0.021409, acc 0.98
2016-09-06T06:56:19.628047: step 11302, loss 0.032862, acc 0.98
2016-09-06T06:56:20.467880: step 11303, loss 0.007277, acc 1
2016-09-06T06:56:21.307242: step 11304, loss 0.0691763, acc 0.96
2016-09-06T06:56:22.120931: step 11305, loss 0.0192552, acc 1
2016-09-06T06:56:22.943843: step 11306, loss 0.00489586, acc 1
2016-09-06T06:56:23.752323: step 11307, loss 0.00371901, acc 1
2016-09-06T06:56:24.560008: step 11308, loss 0.0674332, acc 0.96
2016-09-06T06:56:25.429246: step 11309, loss 0.00940299, acc 1
2016-09-06T06:56:26.227002: step 11310, loss 0.0249178, acc 0.98
2016-09-06T06:56:27.037290: step 11311, loss 0.0388254, acc 0.98
2016-09-06T06:56:27.849759: step 11312, loss 0.00963821, acc 1
2016-09-06T06:56:28.645976: step 11313, loss 0.0206143, acc 0.98
2016-09-06T06:56:29.467573: step 11314, loss 0.0466763, acc 0.98
2016-09-06T06:56:30.311798: step 11315, loss 0.0045227, acc 1
2016-09-06T06:56:31.123849: step 11316, loss 0.0294711, acc 1
2016-09-06T06:56:31.942098: step 11317, loss 0.00521348, acc 1
2016-09-06T06:56:32.809291: step 11318, loss 0.0233351, acc 0.98
2016-09-06T06:56:33.612799: step 11319, loss 0.00310419, acc 1
2016-09-06T06:56:34.412127: step 11320, loss 0.0212315, acc 0.98
2016-09-06T06:56:35.248579: step 11321, loss 0.0810242, acc 0.98
2016-09-06T06:56:36.087022: step 11322, loss 0.0222833, acc 0.98
2016-09-06T06:56:36.876392: step 11323, loss 0.0486236, acc 0.98
2016-09-06T06:56:37.680236: step 11324, loss 0.00302156, acc 1
2016-09-06T06:56:38.501192: step 11325, loss 0.0157233, acc 1
2016-09-06T06:56:39.289679: step 11326, loss 0.0638216, acc 0.96
2016-09-06T06:56:40.106015: step 11327, loss 0.0154453, acc 1
2016-09-06T06:56:40.887445: step 11328, loss 0.0250532, acc 0.977273
2016-09-06T06:56:41.677260: step 11329, loss 0.0216624, acc 0.98
2016-09-06T06:56:42.485491: step 11330, loss 0.0488375, acc 0.98
2016-09-06T06:56:43.329729: step 11331, loss 0.00426873, acc 1
2016-09-06T06:56:44.113330: step 11332, loss 0.0109199, acc 1
2016-09-06T06:56:44.917334: step 11333, loss 0.0422995, acc 0.98
2016-09-06T06:56:45.735810: step 11334, loss 0.0215238, acc 0.98
2016-09-06T06:56:46.523408: step 11335, loss 0.0333386, acc 0.98
2016-09-06T06:56:47.323919: step 11336, loss 0.025085, acc 0.98
2016-09-06T06:56:48.156119: step 11337, loss 0.0349626, acc 0.98
2016-09-06T06:56:48.949149: step 11338, loss 0.0114195, acc 1
2016-09-06T06:56:49.739940: step 11339, loss 0.00290626, acc 1
2016-09-06T06:56:50.568350: step 11340, loss 0.0145704, acc 1
2016-09-06T06:56:51.361069: step 11341, loss 0.0124865, acc 1
2016-09-06T06:56:52.154585: step 11342, loss 0.0261329, acc 1
2016-09-06T06:56:52.983657: step 11343, loss 0.00742153, acc 1
2016-09-06T06:56:53.776924: step 11344, loss 0.00306812, acc 1
2016-09-06T06:56:54.595669: step 11345, loss 0.00337604, acc 1
2016-09-06T06:56:55.427010: step 11346, loss 0.0039151, acc 1
2016-09-06T06:56:56.217457: step 11347, loss 0.005005, acc 1
2016-09-06T06:56:57.037671: step 11348, loss 0.00313758, acc 1
2016-09-06T06:56:57.850113: step 11349, loss 0.00537889, acc 1
2016-09-06T06:56:58.621804: step 11350, loss 0.00744948, acc 1
2016-09-06T06:56:59.431842: step 11351, loss 0.0232985, acc 1
2016-09-06T06:57:00.283074: step 11352, loss 0.0198256, acc 0.98
2016-09-06T06:57:01.077708: step 11353, loss 0.0338448, acc 0.98
2016-09-06T06:57:01.878586: step 11354, loss 0.0172829, acc 1
2016-09-06T06:57:02.699695: step 11355, loss 0.0484872, acc 0.98
2016-09-06T06:57:03.508606: step 11356, loss 0.0367361, acc 0.96
2016-09-06T06:57:04.312039: step 11357, loss 0.00428624, acc 1
2016-09-06T06:57:05.122930: step 11358, loss 0.00321795, acc 1
2016-09-06T06:57:05.930034: step 11359, loss 0.00324904, acc 1
2016-09-06T06:57:06.753937: step 11360, loss 0.0672433, acc 0.98
2016-09-06T06:57:07.598113: step 11361, loss 0.0232986, acc 0.98
2016-09-06T06:57:08.384909: step 11362, loss 0.0053822, acc 1
2016-09-06T06:57:09.182172: step 11363, loss 0.00535261, acc 1
2016-09-06T06:57:10.059004: step 11364, loss 0.0179195, acc 1
2016-09-06T06:57:10.882229: step 11365, loss 0.00370524, acc 1
2016-09-06T06:57:11.722976: step 11366, loss 0.0174184, acc 1
2016-09-06T06:57:12.534652: step 11367, loss 0.0196204, acc 0.98
2016-09-06T06:57:13.355397: step 11368, loss 0.00282403, acc 1
2016-09-06T06:57:14.155499: step 11369, loss 0.0266191, acc 0.98
2016-09-06T06:57:14.997628: step 11370, loss 0.0317776, acc 0.98
2016-09-06T06:57:15.812229: step 11371, loss 0.028036, acc 0.98
2016-09-06T06:57:16.638024: step 11372, loss 0.00286695, acc 1
2016-09-06T06:57:17.512076: step 11373, loss 0.00459314, acc 1
2016-09-06T06:57:18.330646: step 11374, loss 0.121894, acc 0.98
2016-09-06T06:57:19.122222: step 11375, loss 0.0168038, acc 0.98
2016-09-06T06:57:19.928421: step 11376, loss 0.018898, acc 1
2016-09-06T06:57:20.737217: step 11377, loss 0.0669464, acc 0.96
2016-09-06T06:57:21.580306: step 11378, loss 0.00253121, acc 1
2016-09-06T06:57:22.384272: step 11379, loss 0.003049, acc 1
2016-09-06T06:57:23.193023: step 11380, loss 0.00362603, acc 1
2016-09-06T06:57:23.967059: step 11381, loss 0.0430396, acc 0.98
2016-09-06T06:57:24.761918: step 11382, loss 0.00867465, acc 1
2016-09-06T06:57:25.576002: step 11383, loss 0.0233474, acc 1
2016-09-06T06:57:26.364925: step 11384, loss 0.0500174, acc 0.96
2016-09-06T06:57:27.189225: step 11385, loss 0.00777242, acc 1
2016-09-06T06:57:28.003488: step 11386, loss 0.133099, acc 0.96
2016-09-06T06:57:28.798026: step 11387, loss 0.0186816, acc 1
2016-09-06T06:57:29.586333: step 11388, loss 0.035139, acc 0.98
2016-09-06T06:57:30.414456: step 11389, loss 0.0384645, acc 0.98
2016-09-06T06:57:31.196090: step 11390, loss 0.0103021, acc 1
2016-09-06T06:57:31.995085: step 11391, loss 0.00655487, acc 1
2016-09-06T06:57:32.826599: step 11392, loss 0.0355259, acc 0.98
2016-09-06T06:57:33.634613: step 11393, loss 0.0903509, acc 0.98
2016-09-06T06:57:34.454383: step 11394, loss 0.022762, acc 0.98
2016-09-06T06:57:35.258734: step 11395, loss 0.0126303, acc 1
2016-09-06T06:57:36.048852: step 11396, loss 0.0253075, acc 0.98
2016-09-06T06:57:36.836603: step 11397, loss 0.0469517, acc 0.98
2016-09-06T06:57:37.652722: step 11398, loss 0.0500632, acc 0.96
2016-09-06T06:57:38.427295: step 11399, loss 0.0307256, acc 0.98
2016-09-06T06:57:39.249238: step 11400, loss 0.00533611, acc 1

Evaluation:
2016-09-06T06:57:43.003868: step 11400, loss 2.00179, acc 0.723265

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-11400

2016-09-06T06:57:44.895069: step 11401, loss 0.0220699, acc 0.98
2016-09-06T06:57:45.753789: step 11402, loss 0.00442388, acc 1
2016-09-06T06:57:46.585433: step 11403, loss 0.0182537, acc 1
2016-09-06T06:57:47.403456: step 11404, loss 0.0282904, acc 0.98
2016-09-06T06:57:48.236540: step 11405, loss 0.02093, acc 0.98
2016-09-06T06:57:49.032308: step 11406, loss 0.0116637, acc 1
2016-09-06T06:57:49.851296: step 11407, loss 0.0248182, acc 0.98
2016-09-06T06:57:50.658607: step 11408, loss 0.0523759, acc 0.98
2016-09-06T06:57:51.468331: step 11409, loss 0.003785, acc 1
2016-09-06T06:57:52.283483: step 11410, loss 0.00464326, acc 1
2016-09-06T06:57:53.069777: step 11411, loss 0.0458062, acc 0.98
2016-09-06T06:57:53.878987: step 11412, loss 0.00344182, acc 1
2016-09-06T06:57:54.706167: step 11413, loss 0.00744047, acc 1
2016-09-06T06:57:55.529684: step 11414, loss 0.0183482, acc 1
2016-09-06T06:57:56.327659: step 11415, loss 0.00395667, acc 1
2016-09-06T06:57:57.166207: step 11416, loss 0.0041102, acc 1
2016-09-06T06:57:57.938949: step 11417, loss 0.0602096, acc 0.98
2016-09-06T06:57:58.748037: step 11418, loss 0.00420775, acc 1
2016-09-06T06:57:59.551017: step 11419, loss 0.0430975, acc 0.96
2016-09-06T06:58:00.377122: step 11420, loss 0.00358405, acc 1
2016-09-06T06:58:01.165475: step 11421, loss 0.023691, acc 1
2016-09-06T06:58:01.964013: step 11422, loss 0.00518373, acc 1
2016-09-06T06:58:02.759316: step 11423, loss 0.0214938, acc 0.98
2016-09-06T06:58:03.553377: step 11424, loss 0.0442945, acc 0.98
2016-09-06T06:58:04.362860: step 11425, loss 0.0715675, acc 0.98
2016-09-06T06:58:05.157440: step 11426, loss 0.0260885, acc 1
2016-09-06T06:58:05.990474: step 11427, loss 0.0311536, acc 0.98
2016-09-06T06:58:06.829068: step 11428, loss 0.00662495, acc 1
2016-09-06T06:58:07.614478: step 11429, loss 0.026074, acc 0.98
2016-09-06T06:58:08.421683: step 11430, loss 0.0268552, acc 0.98
2016-09-06T06:58:09.226010: step 11431, loss 0.00865322, acc 1
2016-09-06T06:58:10.016902: step 11432, loss 0.00314151, acc 1
2016-09-06T06:58:10.833061: step 11433, loss 0.0167426, acc 1
2016-09-06T06:58:11.653857: step 11434, loss 0.025901, acc 0.98
2016-09-06T06:58:12.426606: step 11435, loss 0.0310217, acc 1
2016-09-06T06:58:13.229659: step 11436, loss 0.0289343, acc 0.98
2016-09-06T06:58:14.053656: step 11437, loss 0.0181275, acc 0.98
2016-09-06T06:58:14.850396: step 11438, loss 0.0436148, acc 0.98
2016-09-06T06:58:15.644463: step 11439, loss 0.00316972, acc 1
2016-09-06T06:58:16.457770: step 11440, loss 0.0387891, acc 0.98
2016-09-06T06:58:17.250940: step 11441, loss 0.00350481, acc 1
2016-09-06T06:58:18.057268: step 11442, loss 0.037472, acc 1
2016-09-06T06:58:18.893017: step 11443, loss 0.0105884, acc 1
2016-09-06T06:58:19.676380: step 11444, loss 0.00349822, acc 1
2016-09-06T06:58:20.474311: step 11445, loss 0.00457534, acc 1
2016-09-06T06:58:21.287814: step 11446, loss 0.02891, acc 0.98
2016-09-06T06:58:22.075798: step 11447, loss 0.0324197, acc 0.98
2016-09-06T06:58:22.878238: step 11448, loss 0.00596236, acc 1
2016-09-06T06:58:23.718335: step 11449, loss 0.00406606, acc 1
2016-09-06T06:58:24.524215: step 11450, loss 0.00315921, acc 1
2016-09-06T06:58:25.342324: step 11451, loss 0.00468425, acc 1
2016-09-06T06:58:26.169996: step 11452, loss 0.0171347, acc 1
2016-09-06T06:58:26.940511: step 11453, loss 0.031658, acc 1
2016-09-06T06:58:27.758881: step 11454, loss 0.0314583, acc 0.98
2016-09-06T06:58:28.597499: step 11455, loss 0.0224706, acc 0.98
2016-09-06T06:58:29.418322: step 11456, loss 0.0103952, acc 1
2016-09-06T06:58:30.232408: step 11457, loss 0.00290019, acc 1
2016-09-06T06:58:31.054887: step 11458, loss 0.0282658, acc 0.98
2016-09-06T06:58:31.873964: step 11459, loss 0.0165777, acc 1
2016-09-06T06:58:32.696380: step 11460, loss 0.00297972, acc 1
2016-09-06T06:58:33.517578: step 11461, loss 0.025948, acc 0.98
2016-09-06T06:58:34.345006: step 11462, loss 0.00532275, acc 1
2016-09-06T06:58:35.183347: step 11463, loss 0.0229402, acc 0.98
2016-09-06T06:58:36.029428: step 11464, loss 0.00276318, acc 1
2016-09-06T06:58:36.841450: step 11465, loss 0.0319484, acc 0.98
2016-09-06T06:58:37.675456: step 11466, loss 0.0410024, acc 0.98
2016-09-06T06:58:38.539218: step 11467, loss 0.00343713, acc 1
2016-09-06T06:58:39.335955: step 11468, loss 0.0108249, acc 1
2016-09-06T06:58:40.157052: step 11469, loss 0.0540106, acc 0.98
2016-09-06T06:58:40.995233: step 11470, loss 0.0181443, acc 0.98
2016-09-06T06:58:41.827108: step 11471, loss 0.0157315, acc 1
2016-09-06T06:58:42.621394: step 11472, loss 0.0165712, acc 1
2016-09-06T06:58:43.423730: step 11473, loss 0.00424001, acc 1
2016-09-06T06:58:44.242994: step 11474, loss 0.035226, acc 0.98
2016-09-06T06:58:45.026379: step 11475, loss 0.0601666, acc 0.94
2016-09-06T06:58:45.843786: step 11476, loss 0.0366383, acc 0.98
2016-09-06T06:58:46.680010: step 11477, loss 0.00977407, acc 1
2016-09-06T06:58:47.481640: step 11478, loss 0.00356277, acc 1
2016-09-06T06:58:48.282680: step 11479, loss 0.0036966, acc 1
2016-09-06T06:58:49.100433: step 11480, loss 0.00241415, acc 1
2016-09-06T06:58:49.892482: step 11481, loss 0.00735814, acc 1
2016-09-06T06:58:50.686905: step 11482, loss 0.00320757, acc 1
2016-09-06T06:58:51.499364: step 11483, loss 0.00542958, acc 1
2016-09-06T06:58:52.285313: step 11484, loss 0.0210289, acc 1
2016-09-06T06:58:53.099031: step 11485, loss 0.0168262, acc 0.98
2016-09-06T06:58:53.915016: step 11486, loss 0.00246426, acc 1
2016-09-06T06:58:54.699358: step 11487, loss 0.00234874, acc 1
2016-09-06T06:58:55.550087: step 11488, loss 0.0403715, acc 0.98
2016-09-06T06:58:56.381530: step 11489, loss 0.0425091, acc 0.96
2016-09-06T06:58:57.160159: step 11490, loss 0.0028313, acc 1
2016-09-06T06:58:57.993468: step 11491, loss 0.0114986, acc 1
2016-09-06T06:58:58.818592: step 11492, loss 0.0044779, acc 1
2016-09-06T06:58:59.597415: step 11493, loss 0.00379863, acc 1
2016-09-06T06:59:00.415223: step 11494, loss 0.00474697, acc 1
2016-09-06T06:59:01.220464: step 11495, loss 0.0332036, acc 0.98
2016-09-06T06:59:01.992354: step 11496, loss 0.0397348, acc 0.98
2016-09-06T06:59:02.805974: step 11497, loss 0.0211307, acc 0.98
2016-09-06T06:59:03.641073: step 11498, loss 0.00359206, acc 1
2016-09-06T06:59:04.435265: step 11499, loss 0.0123216, acc 1
2016-09-06T06:59:05.250835: step 11500, loss 0.189715, acc 0.98

Evaluation:
2016-09-06T06:59:08.980328: step 11500, loss 2.21308, acc 0.729831

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-11500

2016-09-06T06:59:10.926087: step 11501, loss 0.012862, acc 1
2016-09-06T06:59:11.739803: step 11502, loss 0.0398795, acc 0.96
2016-09-06T06:59:12.549148: step 11503, loss 0.0276951, acc 1
2016-09-06T06:59:13.399643: step 11504, loss 0.0317236, acc 0.98
2016-09-06T06:59:14.197132: step 11505, loss 0.0107835, acc 1
2016-09-06T06:59:14.998577: step 11506, loss 0.0117567, acc 1
2016-09-06T06:59:15.809325: step 11507, loss 0.0324058, acc 0.98
2016-09-06T06:59:16.649275: step 11508, loss 0.0264008, acc 0.98
2016-09-06T06:59:17.459270: step 11509, loss 0.0134393, acc 1
2016-09-06T06:59:18.285714: step 11510, loss 0.00579433, acc 1
2016-09-06T06:59:19.098674: step 11511, loss 0.00269823, acc 1
2016-09-06T06:59:19.907610: step 11512, loss 0.0190381, acc 1
2016-09-06T06:59:20.761556: step 11513, loss 0.0170798, acc 0.98
2016-09-06T06:59:21.585222: step 11514, loss 0.0260429, acc 1
2016-09-06T06:59:22.396582: step 11515, loss 0.0302268, acc 1
2016-09-06T06:59:23.220083: step 11516, loss 0.00218836, acc 1
2016-09-06T06:59:24.034290: step 11517, loss 0.045731, acc 0.98
2016-09-06T06:59:24.853686: step 11518, loss 0.0157654, acc 1
2016-09-06T06:59:25.705238: step 11519, loss 0.0155985, acc 1
2016-09-06T06:59:26.471625: step 11520, loss 0.0260639, acc 0.977273
2016-09-06T06:59:27.322565: step 11521, loss 0.00296281, acc 1
2016-09-06T06:59:28.140422: step 11522, loss 0.0198239, acc 1
2016-09-06T06:59:28.964629: step 11523, loss 0.0132922, acc 1
2016-09-06T06:59:29.758271: step 11524, loss 0.0135936, acc 1
2016-09-06T06:59:30.578393: step 11525, loss 0.0453, acc 0.98
2016-09-06T06:59:31.417808: step 11526, loss 0.0503255, acc 0.98
2016-09-06T06:59:32.231270: step 11527, loss 0.0337163, acc 0.98
2016-09-06T06:59:33.027447: step 11528, loss 0.0556953, acc 0.98
2016-09-06T06:59:33.841884: step 11529, loss 0.0297688, acc 0.98
2016-09-06T06:59:34.625484: step 11530, loss 0.0903783, acc 0.98
2016-09-06T06:59:35.406846: step 11531, loss 0.037413, acc 0.98
2016-09-06T06:59:36.233910: step 11532, loss 0.00399506, acc 1
2016-09-06T06:59:37.021757: step 11533, loss 0.0251999, acc 1
2016-09-06T06:59:37.840419: step 11534, loss 0.00894707, acc 1
2016-09-06T06:59:38.665006: step 11535, loss 0.0181235, acc 1
2016-09-06T06:59:39.473051: step 11536, loss 0.028579, acc 0.98
2016-09-06T06:59:40.264352: step 11537, loss 0.0157828, acc 1
2016-09-06T06:59:41.129725: step 11538, loss 0.00222094, acc 1
2016-09-06T06:59:41.915285: step 11539, loss 0.0339241, acc 1
2016-09-06T06:59:42.705390: step 11540, loss 0.00249982, acc 1
2016-09-06T06:59:43.541791: step 11541, loss 0.00860773, acc 1
2016-09-06T06:59:44.335087: step 11542, loss 0.0299603, acc 0.98
2016-09-06T06:59:45.138542: step 11543, loss 0.020626, acc 1
2016-09-06T06:59:45.958385: step 11544, loss 0.0165304, acc 1
2016-09-06T06:59:46.715510: step 11545, loss 0.0255487, acc 1
2016-09-06T06:59:47.527554: step 11546, loss 0.0290712, acc 0.98
2016-09-06T06:59:48.336658: step 11547, loss 0.0141063, acc 1
2016-09-06T06:59:49.140585: step 11548, loss 0.00516191, acc 1
2016-09-06T06:59:49.971604: step 11549, loss 0.055623, acc 0.98
2016-09-06T06:59:50.800942: step 11550, loss 0.0105234, acc 1
2016-09-06T06:59:51.603050: step 11551, loss 0.00368809, acc 1
2016-09-06T06:59:52.409952: step 11552, loss 0.0161504, acc 1
2016-09-06T06:59:53.227364: step 11553, loss 0.0140984, acc 1
2016-09-06T06:59:54.037942: step 11554, loss 0.00556524, acc 1
2016-09-06T06:59:54.863849: step 11555, loss 0.0313424, acc 0.98
2016-09-06T06:59:55.692958: step 11556, loss 0.0260501, acc 0.98
2016-09-06T06:59:56.486632: step 11557, loss 0.00334262, acc 1
2016-09-06T06:59:57.306086: step 11558, loss 0.0096224, acc 1
2016-09-06T06:59:58.128307: step 11559, loss 0.00268768, acc 1
2016-09-06T06:59:58.935415: step 11560, loss 0.00287034, acc 1
2016-09-06T06:59:59.757899: step 11561, loss 0.0146839, acc 1
2016-09-06T07:00:00.630363: step 11562, loss 0.0374376, acc 0.98
2016-09-06T07:00:01.430487: step 11563, loss 0.0110062, acc 1
2016-09-06T07:00:02.271738: step 11564, loss 0.0432409, acc 0.98
2016-09-06T07:00:03.096028: step 11565, loss 0.00255533, acc 1
2016-09-06T07:00:03.915261: step 11566, loss 0.0181792, acc 1
2016-09-06T07:00:04.738725: step 11567, loss 0.00631303, acc 1
2016-09-06T07:00:05.622360: step 11568, loss 0.00940743, acc 1
2016-09-06T07:00:06.434593: step 11569, loss 0.0129238, acc 1
2016-09-06T07:00:07.218112: step 11570, loss 0.0386002, acc 0.98
2016-09-06T07:00:08.042647: step 11571, loss 0.00342338, acc 1
2016-09-06T07:00:08.919234: step 11572, loss 0.00393586, acc 1
2016-09-06T07:00:09.761457: step 11573, loss 0.0520167, acc 0.94
2016-09-06T07:00:10.575685: step 11574, loss 0.00413802, acc 1
2016-09-06T07:00:11.386501: step 11575, loss 0.024445, acc 1
2016-09-06T07:00:12.190912: step 11576, loss 0.110997, acc 0.96
2016-09-06T07:00:12.979985: step 11577, loss 0.00234722, acc 1
2016-09-06T07:00:13.791142: step 11578, loss 0.0178372, acc 1
2016-09-06T07:00:14.581576: step 11579, loss 0.00373695, acc 1
2016-09-06T07:00:15.438754: step 11580, loss 0.00213348, acc 1
2016-09-06T07:00:16.284861: step 11581, loss 0.0358388, acc 1
2016-09-06T07:00:17.091216: step 11582, loss 0.00263782, acc 1
2016-09-06T07:00:17.896015: step 11583, loss 0.00229154, acc 1
2016-09-06T07:00:18.717660: step 11584, loss 0.0125037, acc 1
2016-09-06T07:00:19.532500: step 11585, loss 0.00225657, acc 1
2016-09-06T07:00:20.367418: step 11586, loss 0.00468971, acc 1
2016-09-06T07:00:21.221182: step 11587, loss 0.0384355, acc 0.98
2016-09-06T07:00:22.036341: step 11588, loss 0.0497097, acc 0.96
2016-09-06T07:00:22.860396: step 11589, loss 0.0387123, acc 0.98
2016-09-06T07:00:23.692537: step 11590, loss 0.00319481, acc 1
2016-09-06T07:00:24.504471: step 11591, loss 0.00215881, acc 1
2016-09-06T07:00:25.341283: step 11592, loss 0.00188263, acc 1
2016-09-06T07:00:26.156656: step 11593, loss 0.00807654, acc 1
2016-09-06T07:00:26.961780: step 11594, loss 0.00910002, acc 1
2016-09-06T07:00:27.773306: step 11595, loss 0.00733065, acc 1
2016-09-06T07:00:28.594040: step 11596, loss 0.0222436, acc 1
2016-09-06T07:00:29.397090: step 11597, loss 0.0400548, acc 0.96
2016-09-06T07:00:30.198696: step 11598, loss 0.01451, acc 1
2016-09-06T07:00:31.027838: step 11599, loss 0.0127321, acc 1
2016-09-06T07:00:31.835173: step 11600, loss 0.00514818, acc 1

Evaluation:
2016-09-06T07:00:35.534213: step 11600, loss 2.20609, acc 0.731707

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-11600

2016-09-06T07:00:37.377224: step 11601, loss 0.00333306, acc 1
2016-09-06T07:00:38.190409: step 11602, loss 0.018476, acc 0.98
2016-09-06T07:00:38.994442: step 11603, loss 0.0149547, acc 1
2016-09-06T07:00:39.804626: step 11604, loss 0.0023498, acc 1
2016-09-06T07:00:40.619206: step 11605, loss 0.0153137, acc 1
2016-09-06T07:00:41.464784: step 11606, loss 0.0469904, acc 0.98
2016-09-06T07:00:42.266193: step 11607, loss 0.0360645, acc 0.98
2016-09-06T07:00:43.096282: step 11608, loss 0.0227773, acc 0.98
2016-09-06T07:00:43.952787: step 11609, loss 0.0163385, acc 0.98
2016-09-06T07:00:44.762635: step 11610, loss 0.0319837, acc 0.98
2016-09-06T07:00:45.601900: step 11611, loss 0.00704541, acc 1
2016-09-06T07:00:46.425261: step 11612, loss 0.00208612, acc 1
2016-09-06T07:00:47.221835: step 11613, loss 0.00761478, acc 1
2016-09-06T07:00:48.076950: step 11614, loss 0.0251179, acc 0.98
2016-09-06T07:00:48.873848: step 11615, loss 0.0490362, acc 0.98
2016-09-06T07:00:49.674648: step 11616, loss 0.00274202, acc 1
2016-09-06T07:00:50.488944: step 11617, loss 0.00216639, acc 1
2016-09-06T07:00:51.301026: step 11618, loss 0.0524417, acc 0.96
2016-09-06T07:00:52.135952: step 11619, loss 0.064613, acc 0.98
2016-09-06T07:00:52.942506: step 11620, loss 0.0395065, acc 0.96
2016-09-06T07:00:53.753414: step 11621, loss 0.0360466, acc 0.98
2016-09-06T07:00:54.541424: step 11622, loss 0.00562197, acc 1
2016-09-06T07:00:55.360563: step 11623, loss 0.00564877, acc 1
2016-09-06T07:00:56.173735: step 11624, loss 0.0210296, acc 1
2016-09-06T07:00:56.956924: step 11625, loss 0.00788185, acc 1
2016-09-06T07:00:57.754697: step 11626, loss 0.0147354, acc 1
2016-09-06T07:00:58.592585: step 11627, loss 0.0171644, acc 0.98
2016-09-06T07:00:59.389390: step 11628, loss 0.0549143, acc 0.98
2016-09-06T07:01:00.194164: step 11629, loss 0.0180928, acc 1
2016-09-06T07:01:01.043282: step 11630, loss 0.0196713, acc 0.98
2016-09-06T07:01:01.849391: step 11631, loss 0.00186391, acc 1
2016-09-06T07:01:02.633531: step 11632, loss 0.013537, acc 1
2016-09-06T07:01:03.483842: step 11633, loss 0.0301418, acc 0.96
2016-09-06T07:01:04.336231: step 11634, loss 0.00591521, acc 1
2016-09-06T07:01:05.166654: step 11635, loss 0.0466604, acc 0.96
2016-09-06T07:01:06.011968: step 11636, loss 0.0203162, acc 0.98
2016-09-06T07:01:06.803645: step 11637, loss 0.00689394, acc 1
2016-09-06T07:01:07.574568: step 11638, loss 0.00214178, acc 1
2016-09-06T07:01:08.384265: step 11639, loss 0.0135809, acc 1
2016-09-06T07:01:09.193652: step 11640, loss 0.037701, acc 0.98
2016-09-06T07:01:10.005250: step 11641, loss 0.0176966, acc 1
2016-09-06T07:01:10.856196: step 11642, loss 0.0103139, acc 1
2016-09-06T07:01:11.653129: step 11643, loss 0.00240233, acc 1
2016-09-06T07:01:12.451802: step 11644, loss 0.0147975, acc 1
2016-09-06T07:01:13.280390: step 11645, loss 0.00355901, acc 1
2016-09-06T07:01:14.101003: step 11646, loss 0.00760022, acc 1
2016-09-06T07:01:14.909621: step 11647, loss 0.00907708, acc 1
2016-09-06T07:01:15.746233: step 11648, loss 0.00438692, acc 1
2016-09-06T07:01:16.577160: step 11649, loss 0.040502, acc 0.98
2016-09-06T07:01:17.388779: step 11650, loss 0.00520545, acc 1
2016-09-06T07:01:18.214498: step 11651, loss 0.0134571, acc 1
2016-09-06T07:01:19.028212: step 11652, loss 0.0131865, acc 1
2016-09-06T07:01:19.863933: step 11653, loss 0.00345773, acc 1
2016-09-06T07:01:20.711365: step 11654, loss 0.0143749, acc 1
2016-09-06T07:01:21.531197: step 11655, loss 0.0149647, acc 1
2016-09-06T07:01:22.317981: step 11656, loss 0.0694882, acc 0.98
2016-09-06T07:01:23.171069: step 11657, loss 0.0227688, acc 0.98
2016-09-06T07:01:24.033432: step 11658, loss 0.0331921, acc 0.96
2016-09-06T07:01:24.815580: step 11659, loss 0.00173641, acc 1
2016-09-06T07:01:25.618741: step 11660, loss 0.0051017, acc 1
2016-09-06T07:01:26.457863: step 11661, loss 0.0768877, acc 0.98
2016-09-06T07:01:27.230545: step 11662, loss 0.00169684, acc 1
2016-09-06T07:01:28.041784: step 11663, loss 0.00825195, acc 1
2016-09-06T07:01:28.863781: step 11664, loss 0.0229655, acc 1
2016-09-06T07:01:29.624476: step 11665, loss 0.00294441, acc 1
2016-09-06T07:01:30.423643: step 11666, loss 0.00824337, acc 1
2016-09-06T07:01:31.250362: step 11667, loss 0.0350134, acc 0.98
2016-09-06T07:01:32.048551: step 11668, loss 0.00328113, acc 1
2016-09-06T07:01:32.849075: step 11669, loss 0.0142808, acc 1
2016-09-06T07:01:33.679531: step 11670, loss 0.0189729, acc 1
2016-09-06T07:01:34.485302: step 11671, loss 0.00942798, acc 1
2016-09-06T07:01:35.309054: step 11672, loss 0.00361366, acc 1
2016-09-06T07:01:36.123915: step 11673, loss 0.00225509, acc 1
2016-09-06T07:01:36.928217: step 11674, loss 0.0251745, acc 0.98
2016-09-06T07:01:37.731098: step 11675, loss 0.0057952, acc 1
2016-09-06T07:01:38.556913: step 11676, loss 0.0420726, acc 0.96
2016-09-06T07:01:39.347214: step 11677, loss 0.0136157, acc 1
2016-09-06T07:01:40.149154: step 11678, loss 0.00207862, acc 1
2016-09-06T07:01:40.949859: step 11679, loss 0.00209626, acc 1
2016-09-06T07:01:41.722575: step 11680, loss 0.041474, acc 0.98
2016-09-06T07:01:42.552274: step 11681, loss 0.0155319, acc 1
2016-09-06T07:01:43.374525: step 11682, loss 0.0192193, acc 0.98
2016-09-06T07:01:44.187970: step 11683, loss 0.0235256, acc 1
2016-09-06T07:01:44.988902: step 11684, loss 0.00818027, acc 1
2016-09-06T07:01:45.823202: step 11685, loss 0.00191374, acc 1
2016-09-06T07:01:46.621252: step 11686, loss 0.0197959, acc 0.98
2016-09-06T07:01:47.425163: step 11687, loss 0.00441312, acc 1
2016-09-06T07:01:48.237309: step 11688, loss 0.00889812, acc 1
2016-09-06T07:01:49.029449: step 11689, loss 0.0327855, acc 0.98
2016-09-06T07:01:49.839288: step 11690, loss 0.0232939, acc 0.98
2016-09-06T07:01:50.646021: step 11691, loss 0.0185259, acc 0.98
2016-09-06T07:01:51.446412: step 11692, loss 0.00656802, acc 1
2016-09-06T07:01:52.259289: step 11693, loss 0.0126638, acc 1
2016-09-06T07:01:53.080276: step 11694, loss 0.022063, acc 0.98
2016-09-06T07:01:53.875758: step 11695, loss 0.0447369, acc 0.98
2016-09-06T07:01:54.690278: step 11696, loss 0.00184385, acc 1
2016-09-06T07:01:55.504333: step 11697, loss 0.00282041, acc 1
2016-09-06T07:01:56.304357: step 11698, loss 0.053634, acc 0.96
2016-09-06T07:01:57.107775: step 11699, loss 0.00396603, acc 1
2016-09-06T07:01:57.968362: step 11700, loss 0.00891166, acc 1

Evaluation:
2016-09-06T07:02:01.674741: step 11700, loss 2.68663, acc 0.726079

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-11700

2016-09-06T07:02:03.578009: step 11701, loss 0.00635216, acc 1
2016-09-06T07:02:04.427134: step 11702, loss 0.00245664, acc 1
2016-09-06T07:02:05.231853: step 11703, loss 0.0018974, acc 1
2016-09-06T07:02:06.020862: step 11704, loss 0.00225183, acc 1
2016-09-06T07:02:06.843936: step 11705, loss 0.0157445, acc 1
2016-09-06T07:02:07.669263: step 11706, loss 0.00285946, acc 1
2016-09-06T07:02:08.460353: step 11707, loss 0.00241216, acc 1
2016-09-06T07:02:09.270931: step 11708, loss 0.0116717, acc 1
2016-09-06T07:02:10.080412: step 11709, loss 0.0186184, acc 0.98
2016-09-06T07:02:10.877714: step 11710, loss 0.00493211, acc 1
2016-09-06T07:02:11.676901: step 11711, loss 0.0161408, acc 1
2016-09-06T07:02:12.437495: step 11712, loss 0.00197711, acc 1
2016-09-06T07:02:13.244658: step 11713, loss 0.0361771, acc 0.98
2016-09-06T07:02:14.037812: step 11714, loss 0.0589248, acc 0.98
2016-09-06T07:02:14.860389: step 11715, loss 0.00201762, acc 1
2016-09-06T07:02:15.664383: step 11716, loss 0.0154983, acc 1
2016-09-06T07:02:16.501138: step 11717, loss 0.00282234, acc 1
2016-09-06T07:02:17.317030: step 11718, loss 0.0187245, acc 0.98
2016-09-06T07:02:18.094677: step 11719, loss 0.00430936, acc 1
2016-09-06T07:02:18.911934: step 11720, loss 0.0207029, acc 1
2016-09-06T07:02:19.735005: step 11721, loss 0.0282325, acc 1
2016-09-06T07:02:20.500769: step 11722, loss 0.0155339, acc 1
2016-09-06T07:02:21.328550: step 11723, loss 0.00705722, acc 1
2016-09-06T07:02:22.158072: step 11724, loss 0.00202853, acc 1
2016-09-06T07:02:22.952995: step 11725, loss 0.00498378, acc 1
2016-09-06T07:02:23.745404: step 11726, loss 0.00472798, acc 1
2016-09-06T07:02:24.549396: step 11727, loss 0.0290376, acc 1
2016-09-06T07:02:25.316983: step 11728, loss 0.00246385, acc 1
2016-09-06T07:02:26.107326: step 11729, loss 0.00315552, acc 1
2016-09-06T07:02:26.946481: step 11730, loss 0.0210197, acc 0.98
2016-09-06T07:02:27.738751: step 11731, loss 0.0183329, acc 1
2016-09-06T07:02:28.548240: step 11732, loss 0.00278075, acc 1
2016-09-06T07:02:29.382862: step 11733, loss 0.00215675, acc 1
2016-09-06T07:02:30.173396: step 11734, loss 0.00921125, acc 1
2016-09-06T07:02:30.950523: step 11735, loss 0.0022191, acc 1
2016-09-06T07:02:31.782901: step 11736, loss 0.0219837, acc 0.98
2016-09-06T07:02:32.578368: step 11737, loss 0.00221686, acc 1
2016-09-06T07:02:33.405664: step 11738, loss 0.0185517, acc 1
2016-09-06T07:02:34.202130: step 11739, loss 0.0172274, acc 0.98
2016-09-06T07:02:34.958786: step 11740, loss 0.0179836, acc 0.98
2016-09-06T07:02:35.776516: step 11741, loss 0.00249247, acc 1
2016-09-06T07:02:36.582310: step 11742, loss 0.00606692, acc 1
2016-09-06T07:02:37.363264: step 11743, loss 0.00371019, acc 1
2016-09-06T07:02:38.183870: step 11744, loss 0.00232988, acc 1
2016-09-06T07:02:39.018052: step 11745, loss 0.00211237, acc 1
2016-09-06T07:02:39.820662: step 11746, loss 0.0206232, acc 0.98
2016-09-06T07:02:40.655044: step 11747, loss 0.00390769, acc 1
2016-09-06T07:02:41.480823: step 11748, loss 0.0164839, acc 1
2016-09-06T07:02:42.275345: step 11749, loss 0.00212988, acc 1
2016-09-06T07:02:43.070342: step 11750, loss 0.0155703, acc 1
2016-09-06T07:02:43.869446: step 11751, loss 0.0585994, acc 0.98
2016-09-06T07:02:44.668041: step 11752, loss 0.00214323, acc 1
2016-09-06T07:02:45.450794: step 11753, loss 0.0901319, acc 0.98
2016-09-06T07:02:46.298424: step 11754, loss 0.0115985, acc 1
2016-09-06T07:02:47.071213: step 11755, loss 0.0665726, acc 0.96
2016-09-06T07:02:47.868620: step 11756, loss 0.00218079, acc 1
2016-09-06T07:02:48.706028: step 11757, loss 0.00176856, acc 1
2016-09-06T07:02:49.497805: step 11758, loss 0.00640958, acc 1
2016-09-06T07:02:50.309972: step 11759, loss 0.00205696, acc 1
2016-09-06T07:02:51.087286: step 11760, loss 0.0021745, acc 1
2016-09-06T07:02:51.881018: step 11761, loss 0.00899525, acc 1
2016-09-06T07:02:52.703516: step 11762, loss 0.00170547, acc 1
2016-09-06T07:02:53.547542: step 11763, loss 0.0282737, acc 0.98
2016-09-06T07:02:54.333394: step 11764, loss 0.0026061, acc 1
2016-09-06T07:02:55.122243: step 11765, loss 0.0169384, acc 1
2016-09-06T07:02:55.961755: step 11766, loss 0.00667552, acc 1
2016-09-06T07:02:56.763804: step 11767, loss 0.0190566, acc 1
2016-09-06T07:02:57.566908: step 11768, loss 0.053068, acc 0.98
2016-09-06T07:02:58.385336: step 11769, loss 0.00198272, acc 1
2016-09-06T07:02:59.174824: step 11770, loss 0.0363743, acc 1
2016-09-06T07:02:59.982740: step 11771, loss 0.0113983, acc 1
2016-09-06T07:03:00.823625: step 11772, loss 0.0126302, acc 1
2016-09-06T07:03:01.609131: step 11773, loss 0.00792884, acc 1
2016-09-06T07:03:02.434638: step 11774, loss 0.00189438, acc 1
2016-09-06T07:03:03.274104: step 11775, loss 0.00218179, acc 1
2016-09-06T07:03:04.050197: step 11776, loss 0.01485, acc 1
2016-09-06T07:03:04.876331: step 11777, loss 0.0348673, acc 0.96
2016-09-06T07:03:05.709695: step 11778, loss 0.00385513, acc 1
2016-09-06T07:03:06.488006: step 11779, loss 0.0113644, acc 1
2016-09-06T07:03:07.300657: step 11780, loss 0.172429, acc 0.94
2016-09-06T07:03:08.124172: step 11781, loss 0.0136044, acc 1
2016-09-06T07:03:08.900594: step 11782, loss 0.0123052, acc 1
2016-09-06T07:03:09.730206: step 11783, loss 0.00282041, acc 1
2016-09-06T07:03:10.581003: step 11784, loss 0.00145865, acc 1
2016-09-06T07:03:11.403733: step 11785, loss 0.0386259, acc 0.98
2016-09-06T07:03:12.212774: step 11786, loss 0.00188003, acc 1
2016-09-06T07:03:13.061410: step 11787, loss 0.0219895, acc 0.98
2016-09-06T07:03:13.883963: step 11788, loss 0.0271836, acc 0.98
2016-09-06T07:03:14.693791: step 11789, loss 0.0217178, acc 0.98
2016-09-06T07:03:15.530040: step 11790, loss 0.00718055, acc 1
2016-09-06T07:03:16.338609: step 11791, loss 0.00507939, acc 1
2016-09-06T07:03:17.144199: step 11792, loss 0.0141312, acc 1
2016-09-06T07:03:17.981195: step 11793, loss 0.0326733, acc 0.98
2016-09-06T07:03:18.782845: step 11794, loss 0.0141473, acc 1
2016-09-06T07:03:19.622928: step 11795, loss 0.0232864, acc 0.98
2016-09-06T07:03:20.444327: step 11796, loss 0.0264902, acc 0.98
2016-09-06T07:03:21.239625: step 11797, loss 0.0019162, acc 1
2016-09-06T07:03:22.042472: step 11798, loss 0.0173681, acc 1
2016-09-06T07:03:22.880906: step 11799, loss 0.00166996, acc 1
2016-09-06T07:03:23.689773: step 11800, loss 0.0514425, acc 0.98

Evaluation:
2016-09-06T07:03:27.459276: step 11800, loss 2.01715, acc 0.726079

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-11800

2016-09-06T07:03:29.325725: step 11801, loss 0.0193034, acc 1
2016-09-06T07:03:30.164741: step 11802, loss 0.0041712, acc 1
2016-09-06T07:03:30.989445: step 11803, loss 0.017786, acc 0.98
2016-09-06T07:03:31.823663: step 11804, loss 0.00154446, acc 1
2016-09-06T07:03:32.639441: step 11805, loss 0.0123348, acc 1
2016-09-06T07:03:33.464473: step 11806, loss 0.049937, acc 0.98
2016-09-06T07:03:34.281274: step 11807, loss 0.00403958, acc 1
2016-09-06T07:03:35.160446: step 11808, loss 0.00612416, acc 1
2016-09-06T07:03:35.967139: step 11809, loss 0.0039667, acc 1
2016-09-06T07:03:36.766908: step 11810, loss 0.0769175, acc 0.96
2016-09-06T07:03:37.568408: step 11811, loss 0.0121818, acc 1
2016-09-06T07:03:38.376418: step 11812, loss 0.0262845, acc 0.98
2016-09-06T07:03:39.181744: step 11813, loss 0.0398207, acc 0.98
2016-09-06T07:03:39.963599: step 11814, loss 0.00548896, acc 1
2016-09-06T07:03:40.785416: step 11815, loss 0.00274967, acc 1
2016-09-06T07:03:41.573013: step 11816, loss 0.0203216, acc 0.98
2016-09-06T07:03:42.364532: step 11817, loss 0.0283324, acc 0.98
2016-09-06T07:03:43.206700: step 11818, loss 0.021299, acc 0.98
2016-09-06T07:03:43.997974: step 11819, loss 0.0174992, acc 1
2016-09-06T07:03:44.827308: step 11820, loss 0.0205793, acc 0.98
2016-09-06T07:03:45.657982: step 11821, loss 0.0174992, acc 0.98
2016-09-06T07:03:46.494744: step 11822, loss 0.0491871, acc 0.98
2016-09-06T07:03:47.334639: step 11823, loss 0.00240065, acc 1
2016-09-06T07:03:48.146194: step 11824, loss 0.00214083, acc 1
2016-09-06T07:03:48.937529: step 11825, loss 0.0295492, acc 0.98
2016-09-06T07:03:49.767409: step 11826, loss 0.00724517, acc 1
2016-09-06T07:03:50.599431: step 11827, loss 0.0453565, acc 0.98
2016-09-06T07:03:51.416707: step 11828, loss 0.00834943, acc 1
2016-09-06T07:03:52.229822: step 11829, loss 0.00258263, acc 1
2016-09-06T07:03:53.049908: step 11830, loss 0.0018372, acc 1
2016-09-06T07:03:53.867386: step 11831, loss 0.0290416, acc 0.98
2016-09-06T07:03:54.681882: step 11832, loss 0.0271735, acc 0.98
2016-09-06T07:03:55.524420: step 11833, loss 0.00285039, acc 1
2016-09-06T07:03:56.329989: step 11834, loss 0.00186413, acc 1
2016-09-06T07:03:57.138188: step 11835, loss 0.00186746, acc 1
2016-09-06T07:03:57.951177: step 11836, loss 0.00893311, acc 1
2016-09-06T07:03:58.757634: step 11837, loss 0.0285508, acc 0.98
2016-09-06T07:03:59.569827: step 11838, loss 0.00269939, acc 1
2016-09-06T07:04:00.442076: step 11839, loss 0.0397812, acc 0.98
2016-09-06T07:04:01.246937: step 11840, loss 0.0108732, acc 1
2016-09-06T07:04:02.054532: step 11841, loss 0.0148965, acc 1
2016-09-06T07:04:02.878910: step 11842, loss 0.0384778, acc 0.98
2016-09-06T07:04:03.690505: step 11843, loss 0.0304404, acc 0.98
2016-09-06T07:04:04.483263: step 11844, loss 0.00205386, acc 1
2016-09-06T07:04:05.305101: step 11845, loss 0.116752, acc 0.94
2016-09-06T07:04:06.121473: step 11846, loss 0.0291714, acc 1
2016-09-06T07:04:06.915338: step 11847, loss 0.0118244, acc 1
2016-09-06T07:04:07.706426: step 11848, loss 0.00941121, acc 1
2016-09-06T07:04:08.523512: step 11849, loss 0.017757, acc 0.98
2016-09-06T07:04:09.348947: step 11850, loss 0.0166953, acc 1
2016-09-06T07:04:10.169127: step 11851, loss 0.00177617, acc 1
2016-09-06T07:04:10.981823: step 11852, loss 0.00896694, acc 1
2016-09-06T07:04:11.772063: step 11853, loss 0.0401123, acc 0.96
2016-09-06T07:04:12.573819: step 11854, loss 0.00297778, acc 1
2016-09-06T07:04:13.399219: step 11855, loss 0.0252315, acc 0.98
2016-09-06T07:04:14.219108: step 11856, loss 0.0126178, acc 1
2016-09-06T07:04:15.030352: step 11857, loss 0.00403694, acc 1
2016-09-06T07:04:15.861202: step 11858, loss 0.0117313, acc 1
2016-09-06T07:04:16.644633: step 11859, loss 0.0103605, acc 1
2016-09-06T07:04:17.420469: step 11860, loss 0.00867629, acc 1
2016-09-06T07:04:18.240090: step 11861, loss 0.0207482, acc 0.98
2016-09-06T07:04:19.036695: step 11862, loss 0.0479154, acc 0.98
2016-09-06T07:04:19.869092: step 11863, loss 0.00217235, acc 1
2016-09-06T07:04:20.668052: step 11864, loss 0.00960812, acc 1
2016-09-06T07:04:21.433933: step 11865, loss 0.0024687, acc 1
2016-09-06T07:04:22.232378: step 11866, loss 0.0115849, acc 1
2016-09-06T07:04:23.022248: step 11867, loss 0.0295059, acc 0.98
2016-09-06T07:04:23.831302: step 11868, loss 0.0127092, acc 1
2016-09-06T07:04:24.663905: step 11869, loss 0.0188068, acc 0.98
2016-09-06T07:04:25.488403: step 11870, loss 0.0142869, acc 1
2016-09-06T07:04:26.313219: step 11871, loss 0.0739589, acc 0.98
2016-09-06T07:04:27.113482: step 11872, loss 0.179688, acc 0.94
2016-09-06T07:04:27.904351: step 11873, loss 0.0345678, acc 0.98
2016-09-06T07:04:28.690427: step 11874, loss 0.0317107, acc 0.98
2016-09-06T07:04:29.514412: step 11875, loss 0.00766747, acc 1
2016-09-06T07:04:30.364999: step 11876, loss 0.0278369, acc 0.98
2016-09-06T07:04:31.156646: step 11877, loss 0.00758825, acc 1
2016-09-06T07:04:31.957462: step 11878, loss 0.0282855, acc 1
2016-09-06T07:04:32.772462: step 11879, loss 0.0297552, acc 0.98
2016-09-06T07:04:33.551805: step 11880, loss 0.0192358, acc 1
2016-09-06T07:04:34.337768: step 11881, loss 0.0449271, acc 0.96
2016-09-06T07:04:35.173460: step 11882, loss 0.0566008, acc 0.96
2016-09-06T07:04:35.979655: step 11883, loss 0.0260378, acc 0.98
2016-09-06T07:04:36.803857: step 11884, loss 0.00335322, acc 1
2016-09-06T07:04:37.648455: step 11885, loss 0.00344618, acc 1
2016-09-06T07:04:38.456283: step 11886, loss 0.0113971, acc 1
2016-09-06T07:04:39.270177: step 11887, loss 0.0206686, acc 1
2016-09-06T07:04:40.141546: step 11888, loss 0.0565255, acc 0.98
2016-09-06T07:04:40.967377: step 11889, loss 0.0368397, acc 0.98
2016-09-06T07:04:41.792370: step 11890, loss 0.0524009, acc 0.96
2016-09-06T07:04:42.638220: step 11891, loss 0.019712, acc 0.98
2016-09-06T07:04:43.437897: step 11892, loss 0.0377745, acc 0.96
2016-09-06T07:04:44.235478: step 11893, loss 0.0813944, acc 0.98
2016-09-06T07:04:45.064127: step 11894, loss 0.026733, acc 0.98
2016-09-06T07:04:45.870148: step 11895, loss 0.0728512, acc 0.98
2016-09-06T07:04:46.644357: step 11896, loss 0.0126761, acc 1
2016-09-06T07:04:47.496206: step 11897, loss 0.00388624, acc 1
2016-09-06T07:04:48.343648: step 11898, loss 0.0440687, acc 0.98
2016-09-06T07:04:49.163702: step 11899, loss 0.00768845, acc 1
2016-09-06T07:04:50.005974: step 11900, loss 0.00328891, acc 1

Evaluation:
2016-09-06T07:04:53.721364: step 11900, loss 2.98385, acc 0.730769

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-11900

2016-09-06T07:04:55.702327: step 11901, loss 0.0562809, acc 0.96
2016-09-06T07:04:56.529470: step 11902, loss 0.00323359, acc 1
2016-09-06T07:04:57.367973: step 11903, loss 0.00345628, acc 1
2016-09-06T07:04:58.099922: step 11904, loss 0.00364897, acc 1
2016-09-06T07:04:58.937099: step 11905, loss 0.0538214, acc 0.96
2016-09-06T07:04:59.775654: step 11906, loss 0.036005, acc 1
2016-09-06T07:05:00.618429: step 11907, loss 0.0671722, acc 0.98
2016-09-06T07:05:01.408409: step 11908, loss 0.00361838, acc 1
2016-09-06T07:05:02.212524: step 11909, loss 0.0160648, acc 1
2016-09-06T07:05:03.019990: step 11910, loss 0.00405082, acc 1
2016-09-06T07:05:03.798797: step 11911, loss 0.027966, acc 0.98
2016-09-06T07:05:04.572904: step 11912, loss 0.00747211, acc 1
2016-09-06T07:05:05.407583: step 11913, loss 0.0255043, acc 0.98
2016-09-06T07:05:06.202043: step 11914, loss 0.0641922, acc 0.96
2016-09-06T07:05:07.002417: step 11915, loss 0.0192068, acc 0.98
2016-09-06T07:05:07.820428: step 11916, loss 0.00461126, acc 1
2016-09-06T07:05:08.608454: step 11917, loss 0.0239026, acc 0.98
2016-09-06T07:05:09.416816: step 11918, loss 0.0136372, acc 1
2016-09-06T07:05:10.259152: step 11919, loss 0.0312197, acc 0.98
2016-09-06T07:05:11.027219: step 11920, loss 0.00316197, acc 1
2016-09-06T07:05:11.809483: step 11921, loss 0.00255198, acc 1
2016-09-06T07:05:12.615260: step 11922, loss 0.00379098, acc 1
2016-09-06T07:05:13.404655: step 11923, loss 0.0202013, acc 1
2016-09-06T07:05:14.193329: step 11924, loss 0.00253773, acc 1
2016-09-06T07:05:15.006576: step 11925, loss 0.00270948, acc 1
2016-09-06T07:05:15.808971: step 11926, loss 0.00389218, acc 1
2016-09-06T07:05:16.620391: step 11927, loss 0.0172461, acc 1
2016-09-06T07:05:17.431163: step 11928, loss 0.00606897, acc 1
2016-09-06T07:05:18.234339: step 11929, loss 0.0395736, acc 0.96
2016-09-06T07:05:19.069886: step 11930, loss 0.0285017, acc 1
2016-09-06T07:05:19.915991: step 11931, loss 0.00285921, acc 1
2016-09-06T07:05:20.694861: step 11932, loss 0.00572481, acc 1
2016-09-06T07:05:21.499271: step 11933, loss 0.0117798, acc 1
2016-09-06T07:05:22.337222: step 11934, loss 0.0107151, acc 1
2016-09-06T07:05:23.127151: step 11935, loss 0.00251173, acc 1
2016-09-06T07:05:23.940029: step 11936, loss 0.087272, acc 0.96
2016-09-06T07:05:24.759207: step 11937, loss 0.0328821, acc 1
2016-09-06T07:05:25.561585: step 11938, loss 0.0380244, acc 0.98
2016-09-06T07:05:26.374187: step 11939, loss 0.0127572, acc 1
2016-09-06T07:05:27.194249: step 11940, loss 0.01135, acc 1
2016-09-06T07:05:28.009953: step 11941, loss 0.0038359, acc 1
2016-09-06T07:05:28.822255: step 11942, loss 0.0313838, acc 0.98
2016-09-06T07:05:29.641856: step 11943, loss 0.0147394, acc 1
2016-09-06T07:05:30.466080: step 11944, loss 0.0038185, acc 1
2016-09-06T07:05:31.249820: step 11945, loss 0.0243464, acc 1
2016-09-06T07:05:32.070875: step 11946, loss 0.00265425, acc 1
2016-09-06T07:05:32.898393: step 11947, loss 0.0217976, acc 0.98
2016-09-06T07:05:33.704742: step 11948, loss 0.00282449, acc 1
2016-09-06T07:05:34.533393: step 11949, loss 0.00260156, acc 1
2016-09-06T07:05:35.376921: step 11950, loss 0.00315141, acc 1
2016-09-06T07:05:36.189365: step 11951, loss 0.0169276, acc 0.98
2016-09-06T07:05:36.991213: step 11952, loss 0.00385127, acc 1
2016-09-06T07:05:37.801863: step 11953, loss 0.0177119, acc 1
2016-09-06T07:05:38.611683: step 11954, loss 0.00264331, acc 1
2016-09-06T07:05:39.428521: step 11955, loss 0.00268032, acc 1
2016-09-06T07:05:40.218896: step 11956, loss 0.0719907, acc 0.98
2016-09-06T07:05:41.030344: step 11957, loss 0.0273398, acc 0.98
2016-09-06T07:05:41.848779: step 11958, loss 0.00728522, acc 1
2016-09-06T07:05:42.675388: step 11959, loss 0.00795074, acc 1
2016-09-06T07:05:43.473102: step 11960, loss 0.00372324, acc 1
2016-09-06T07:05:44.325902: step 11961, loss 0.00382253, acc 1
2016-09-06T07:05:45.122201: step 11962, loss 0.0210718, acc 0.98
2016-09-06T07:05:45.904903: step 11963, loss 0.005061, acc 1
2016-09-06T07:05:46.745994: step 11964, loss 0.00565798, acc 1
2016-09-06T07:05:47.547213: step 11965, loss 0.00409841, acc 1
2016-09-06T07:05:48.373023: step 11966, loss 0.00301802, acc 1
2016-09-06T07:05:49.209706: step 11967, loss 0.00338819, acc 1
2016-09-06T07:05:50.024229: step 11968, loss 0.0589935, acc 0.98
2016-09-06T07:05:50.824673: step 11969, loss 0.0725605, acc 0.98
2016-09-06T07:05:51.686090: step 11970, loss 0.00514086, acc 1
2016-09-06T07:05:52.516365: step 11971, loss 0.0221897, acc 0.98
2016-09-06T07:05:53.319065: step 11972, loss 0.065527, acc 0.98
2016-09-06T07:05:54.134465: step 11973, loss 0.0425797, acc 0.98
2016-09-06T07:05:54.967999: step 11974, loss 0.0290539, acc 0.98
2016-09-06T07:05:55.768851: step 11975, loss 0.0165831, acc 1
2016-09-06T07:05:56.567185: step 11976, loss 0.00776294, acc 1
2016-09-06T07:05:57.406565: step 11977, loss 0.00711892, acc 1
2016-09-06T07:05:58.198335: step 11978, loss 0.0323346, acc 0.98
2016-09-06T07:05:58.994562: step 11979, loss 0.0253226, acc 0.98
2016-09-06T07:05:59.817036: step 11980, loss 0.016663, acc 1
2016-09-06T07:06:00.648929: step 11981, loss 0.00416369, acc 1
2016-09-06T07:06:01.472884: step 11982, loss 0.028476, acc 1
2016-09-06T07:06:02.292677: step 11983, loss 0.0791773, acc 0.98
2016-09-06T07:06:03.084835: step 11984, loss 0.00901141, acc 1
2016-09-06T07:06:03.892186: step 11985, loss 0.00385466, acc 1
2016-09-06T07:06:04.699683: step 11986, loss 0.0198558, acc 0.98
2016-09-06T07:06:05.541524: step 11987, loss 0.0157896, acc 1
2016-09-06T07:06:06.372429: step 11988, loss 0.00366684, acc 1
2016-09-06T07:06:07.185813: step 11989, loss 0.0232335, acc 1
2016-09-06T07:06:07.993754: step 11990, loss 0.0505866, acc 0.96
2016-09-06T07:06:08.806740: step 11991, loss 0.00324457, acc 1
2016-09-06T07:06:09.631628: step 11992, loss 0.0187839, acc 0.98
2016-09-06T07:06:10.431832: step 11993, loss 0.0562035, acc 0.98
2016-09-06T07:06:11.253423: step 11994, loss 0.0200726, acc 0.98
2016-09-06T07:06:12.056020: step 11995, loss 0.00755903, acc 1
2016-09-06T07:06:12.868594: step 11996, loss 0.0227826, acc 1
2016-09-06T07:06:13.676344: step 11997, loss 0.0267113, acc 0.98
2016-09-06T07:06:14.493814: step 11998, loss 0.0665753, acc 0.98
2016-09-06T07:06:15.308996: step 11999, loss 0.00333592, acc 1
2016-09-06T07:06:16.112579: step 12000, loss 0.0257724, acc 0.98

Evaluation:
2016-09-06T07:06:19.859094: step 12000, loss 3.02247, acc 0.726079

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-12000

2016-09-06T07:06:21.844873: step 12001, loss 0.0171913, acc 0.98
2016-09-06T07:06:22.671619: step 12002, loss 0.0366483, acc 0.98
2016-09-06T07:06:23.488483: step 12003, loss 0.0260943, acc 1
2016-09-06T07:06:24.335761: step 12004, loss 0.0181106, acc 1
2016-09-06T07:06:25.140539: step 12005, loss 0.0136224, acc 1
2016-09-06T07:06:25.934607: step 12006, loss 0.00364545, acc 1
2016-09-06T07:06:26.763082: step 12007, loss 0.00382562, acc 1
2016-09-06T07:06:27.577705: step 12008, loss 0.0867775, acc 0.98
2016-09-06T07:06:28.366236: step 12009, loss 0.0154733, acc 1
2016-09-06T07:06:29.189980: step 12010, loss 0.0059937, acc 1
2016-09-06T07:06:30.017738: step 12011, loss 0.0309126, acc 0.98
2016-09-06T07:06:30.810429: step 12012, loss 0.0334074, acc 0.98
2016-09-06T07:06:31.634220: step 12013, loss 0.0326847, acc 1
2016-09-06T07:06:32.451222: step 12014, loss 0.214099, acc 0.96
2016-09-06T07:06:33.238638: step 12015, loss 0.00360601, acc 1
2016-09-06T07:06:34.045776: step 12016, loss 0.03546, acc 1
2016-09-06T07:06:34.890562: step 12017, loss 0.0143633, acc 1
2016-09-06T07:06:35.680621: step 12018, loss 0.00443946, acc 1
2016-09-06T07:06:36.524512: step 12019, loss 0.0240795, acc 1
2016-09-06T07:06:37.323364: step 12020, loss 0.00462012, acc 1
2016-09-06T07:06:38.091050: step 12021, loss 0.0422412, acc 1
2016-09-06T07:06:38.904356: step 12022, loss 0.00364198, acc 1
2016-09-06T07:06:39.738665: step 12023, loss 0.0617809, acc 0.96
2016-09-06T07:06:40.533395: step 12024, loss 0.00366796, acc 1
2016-09-06T07:06:41.319276: step 12025, loss 0.00397366, acc 1
2016-09-06T07:06:42.154973: step 12026, loss 0.00372499, acc 1
2016-09-06T07:06:42.934819: step 12027, loss 0.0136966, acc 1
2016-09-06T07:06:43.768529: step 12028, loss 0.0193956, acc 0.98
2016-09-06T07:06:44.597540: step 12029, loss 0.0300233, acc 1
2016-09-06T07:06:45.382578: step 12030, loss 0.0123126, acc 1
2016-09-06T07:06:46.176421: step 12031, loss 0.00574773, acc 1
2016-09-06T07:06:46.967218: step 12032, loss 0.0051908, acc 1
2016-09-06T07:06:47.745437: step 12033, loss 0.0148018, acc 1
2016-09-06T07:06:48.566514: step 12034, loss 0.0162621, acc 1
2016-09-06T07:06:49.364378: step 12035, loss 0.0284251, acc 0.98
2016-09-06T07:06:50.147981: step 12036, loss 0.00484946, acc 1
2016-09-06T07:06:50.959992: step 12037, loss 0.00743861, acc 1
2016-09-06T07:06:51.763484: step 12038, loss 0.0331427, acc 0.98
2016-09-06T07:06:52.573085: step 12039, loss 0.00840023, acc 1
2016-09-06T07:06:53.377021: step 12040, loss 0.0101237, acc 1
2016-09-06T07:06:54.223530: step 12041, loss 0.0168605, acc 1
2016-09-06T07:06:55.025399: step 12042, loss 0.0223146, acc 0.98
2016-09-06T07:06:55.840553: step 12043, loss 0.00341527, acc 1
2016-09-06T07:06:56.656388: step 12044, loss 0.00343523, acc 1
2016-09-06T07:06:57.435725: step 12045, loss 0.0306559, acc 0.98
2016-09-06T07:06:58.270725: step 12046, loss 0.0192974, acc 0.98
2016-09-06T07:06:59.098947: step 12047, loss 0.0293252, acc 0.98
2016-09-06T07:06:59.919547: step 12048, loss 0.0273406, acc 1
2016-09-06T07:07:00.742818: step 12049, loss 0.00504996, acc 1
2016-09-06T07:07:01.553639: step 12050, loss 0.0177655, acc 1
2016-09-06T07:07:02.369406: step 12051, loss 0.0321159, acc 0.98
2016-09-06T07:07:03.165527: step 12052, loss 0.0317669, acc 0.98
2016-09-06T07:07:04.001978: step 12053, loss 0.0130122, acc 1
2016-09-06T07:07:04.782375: step 12054, loss 0.015868, acc 1
2016-09-06T07:07:05.567385: step 12055, loss 0.00789227, acc 1
2016-09-06T07:07:06.379658: step 12056, loss 0.00427982, acc 1
2016-09-06T07:07:07.163748: step 12057, loss 0.00387937, acc 1
2016-09-06T07:07:07.997338: step 12058, loss 0.00493649, acc 1
2016-09-06T07:07:08.837432: step 12059, loss 0.00365529, acc 1
2016-09-06T07:07:09.606922: step 12060, loss 0.0195327, acc 0.98
2016-09-06T07:07:10.411620: step 12061, loss 0.00320047, acc 1
2016-09-06T07:07:11.246770: step 12062, loss 0.0150399, acc 1
2016-09-06T07:07:12.035556: step 12063, loss 0.00315439, acc 1
2016-09-06T07:07:12.811256: step 12064, loss 0.0548106, acc 0.98
2016-09-06T07:07:13.615963: step 12065, loss 0.0323619, acc 0.96
2016-09-06T07:07:14.410576: step 12066, loss 0.0239013, acc 1
2016-09-06T07:07:15.204571: step 12067, loss 0.0250604, acc 1
2016-09-06T07:07:16.060142: step 12068, loss 0.0250943, acc 0.98
2016-09-06T07:07:16.860021: step 12069, loss 0.00301383, acc 1
2016-09-06T07:07:17.681936: step 12070, loss 0.0298657, acc 1
2016-09-06T07:07:18.513565: step 12071, loss 0.00324288, acc 1
2016-09-06T07:07:19.324705: step 12072, loss 0.0108114, acc 1
2016-09-06T07:07:20.138855: step 12073, loss 0.0197286, acc 0.98
2016-09-06T07:07:20.964580: step 12074, loss 0.00306698, acc 1
2016-09-06T07:07:21.775015: step 12075, loss 0.00281849, acc 1
2016-09-06T07:07:22.626164: step 12076, loss 0.0153071, acc 1
2016-09-06T07:07:23.445837: step 12077, loss 0.0040347, acc 1
2016-09-06T07:07:24.296084: step 12078, loss 0.0147171, acc 1
2016-09-06T07:07:25.130709: step 12079, loss 0.0209252, acc 1
2016-09-06T07:07:25.950328: step 12080, loss 0.00297195, acc 1
2016-09-06T07:07:26.765235: step 12081, loss 0.00493677, acc 1
2016-09-06T07:07:27.564041: step 12082, loss 0.0330851, acc 0.98
2016-09-06T07:07:28.407186: step 12083, loss 0.00289919, acc 1
2016-09-06T07:07:29.234132: step 12084, loss 0.09625, acc 0.98
2016-09-06T07:07:30.069771: step 12085, loss 0.00326849, acc 1
2016-09-06T07:07:30.908529: step 12086, loss 0.00268794, acc 1
2016-09-06T07:07:31.779027: step 12087, loss 0.00246218, acc 1
2016-09-06T07:07:32.562297: step 12088, loss 0.0152845, acc 1
2016-09-06T07:07:33.370931: step 12089, loss 0.111591, acc 0.98
2016-09-06T07:07:34.188057: step 12090, loss 0.0162916, acc 0.98
2016-09-06T07:07:35.006623: step 12091, loss 0.00637069, acc 1
2016-09-06T07:07:35.792985: step 12092, loss 0.00257984, acc 1
2016-09-06T07:07:36.618720: step 12093, loss 0.0274873, acc 1
2016-09-06T07:07:37.386185: step 12094, loss 0.0325221, acc 0.98
2016-09-06T07:07:38.188966: step 12095, loss 0.00446227, acc 1
2016-09-06T07:07:38.988061: step 12096, loss 0.00277615, acc 1
2016-09-06T07:07:39.769249: step 12097, loss 0.00218378, acc 1
2016-09-06T07:07:40.586671: step 12098, loss 0.0313197, acc 0.98
2016-09-06T07:07:41.391469: step 12099, loss 0.00812747, acc 1
2016-09-06T07:07:42.185411: step 12100, loss 0.0337904, acc 0.98

Evaluation:
2016-09-06T07:07:45.952644: step 12100, loss 2.57012, acc 0.724203

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-12100

2016-09-06T07:07:47.780726: step 12101, loss 0.0238757, acc 0.98
2016-09-06T07:07:48.586066: step 12102, loss 0.0207672, acc 0.98
2016-09-06T07:07:49.406371: step 12103, loss 0.00627248, acc 1
2016-09-06T07:07:50.254517: step 12104, loss 0.00555689, acc 1
2016-09-06T07:07:51.105099: step 12105, loss 0.00708779, acc 1
2016-09-06T07:07:51.903692: step 12106, loss 0.0185155, acc 1
2016-09-06T07:07:52.706194: step 12107, loss 0.00212902, acc 1
2016-09-06T07:07:53.495785: step 12108, loss 0.00814518, acc 1
2016-09-06T07:07:54.282210: step 12109, loss 0.0307304, acc 0.98
2016-09-06T07:07:55.118024: step 12110, loss 0.0161306, acc 1
2016-09-06T07:07:55.943340: step 12111, loss 0.0321608, acc 0.98
2016-09-06T07:07:56.739715: step 12112, loss 0.00975767, acc 1
2016-09-06T07:07:57.552658: step 12113, loss 0.0348567, acc 0.98
2016-09-06T07:07:58.393396: step 12114, loss 0.00531601, acc 1
2016-09-06T07:07:59.169924: step 12115, loss 0.01057, acc 1
2016-09-06T07:07:59.977120: step 12116, loss 0.00222635, acc 1
2016-09-06T07:08:00.839402: step 12117, loss 0.0265642, acc 0.98
2016-09-06T07:08:01.608578: step 12118, loss 0.0161992, acc 1
2016-09-06T07:08:02.399589: step 12119, loss 0.00326165, acc 1
2016-09-06T07:08:03.230233: step 12120, loss 0.0226514, acc 0.98
2016-09-06T07:08:04.030374: step 12121, loss 0.011265, acc 1
2016-09-06T07:08:04.858598: step 12122, loss 0.0412097, acc 0.98
2016-09-06T07:08:05.655891: step 12123, loss 0.0153033, acc 1
2016-09-06T07:08:06.439146: step 12124, loss 0.0312076, acc 0.98
2016-09-06T07:08:07.253879: step 12125, loss 0.00571717, acc 1
2016-09-06T07:08:08.057362: step 12126, loss 0.0190494, acc 0.98
2016-09-06T07:08:08.854554: step 12127, loss 0.0121466, acc 1
2016-09-06T07:08:09.662601: step 12128, loss 0.00239338, acc 1
2016-09-06T07:08:10.467572: step 12129, loss 0.035967, acc 1
2016-09-06T07:08:11.273443: step 12130, loss 0.00696642, acc 1
2016-09-06T07:08:12.088450: step 12131, loss 0.00368452, acc 1
2016-09-06T07:08:12.930681: step 12132, loss 0.00858945, acc 1
2016-09-06T07:08:13.722586: step 12133, loss 0.00339984, acc 1
2016-09-06T07:08:14.517379: step 12134, loss 0.008792, acc 1
2016-09-06T07:08:15.332242: step 12135, loss 0.00213583, acc 1
2016-09-06T07:08:16.122314: step 12136, loss 0.0021428, acc 1
2016-09-06T07:08:16.911027: step 12137, loss 0.0197467, acc 0.98
2016-09-06T07:08:17.769945: step 12138, loss 0.00227889, acc 1
2016-09-06T07:08:18.597782: step 12139, loss 0.0345653, acc 0.98
2016-09-06T07:08:19.418965: step 12140, loss 0.00215257, acc 1
2016-09-06T07:08:20.254654: step 12141, loss 0.00254944, acc 1
2016-09-06T07:08:21.099626: step 12142, loss 0.00754194, acc 1
2016-09-06T07:08:21.916149: step 12143, loss 0.00322081, acc 1
2016-09-06T07:08:22.781437: step 12144, loss 0.0065292, acc 1
2016-09-06T07:08:23.596349: step 12145, loss 0.0063017, acc 1
2016-09-06T07:08:24.377471: step 12146, loss 0.0144222, acc 1
2016-09-06T07:08:25.220214: step 12147, loss 0.0185342, acc 0.98
2016-09-06T07:08:26.038568: step 12148, loss 0.0816485, acc 0.96
2016-09-06T07:08:26.858752: step 12149, loss 0.0293508, acc 0.98
2016-09-06T07:08:27.689240: step 12150, loss 0.00996792, acc 1
2016-09-06T07:08:28.507674: step 12151, loss 0.00312642, acc 1
2016-09-06T07:08:29.310593: step 12152, loss 0.0261425, acc 0.98
2016-09-06T07:08:30.155367: step 12153, loss 0.0981347, acc 0.98
2016-09-06T07:08:31.023388: step 12154, loss 0.0526797, acc 0.98
2016-09-06T07:08:31.835240: step 12155, loss 0.00330413, acc 1
2016-09-06T07:08:32.625049: step 12156, loss 0.00193326, acc 1
2016-09-06T07:08:33.433406: step 12157, loss 0.00223928, acc 1
2016-09-06T07:08:34.237714: step 12158, loss 0.00213082, acc 1
2016-09-06T07:08:35.089260: step 12159, loss 0.00194744, acc 1
2016-09-06T07:08:35.938943: step 12160, loss 0.0321829, acc 0.98
2016-09-06T07:08:36.732065: step 12161, loss 0.00215685, acc 1
2016-09-06T07:08:37.543659: step 12162, loss 0.0732294, acc 0.96
2016-09-06T07:08:38.393456: step 12163, loss 0.0213396, acc 0.98
2016-09-06T07:08:39.215218: step 12164, loss 0.037253, acc 0.98
2016-09-06T07:08:40.027585: step 12165, loss 0.0253939, acc 1
2016-09-06T07:08:40.848571: step 12166, loss 0.0182056, acc 0.98
2016-09-06T07:08:41.670492: step 12167, loss 0.00603498, acc 1
2016-09-06T07:08:42.491149: step 12168, loss 0.00998231, acc 1
2016-09-06T07:08:43.321577: step 12169, loss 0.0292499, acc 1
2016-09-06T07:08:44.151142: step 12170, loss 0.00165718, acc 1
2016-09-06T07:08:44.963538: step 12171, loss 0.0347426, acc 0.98
2016-09-06T07:08:45.783306: step 12172, loss 0.0179403, acc 1
2016-09-06T07:08:46.624918: step 12173, loss 0.017908, acc 0.98
2016-09-06T07:08:47.461068: step 12174, loss 0.037145, acc 1
2016-09-06T07:08:48.296757: step 12175, loss 0.0228379, acc 0.98
2016-09-06T07:08:49.110717: step 12176, loss 0.00223354, acc 1
2016-09-06T07:08:49.906917: step 12177, loss 0.00176982, acc 1
2016-09-06T07:08:50.729366: step 12178, loss 0.00252421, acc 1
2016-09-06T07:08:51.571001: step 12179, loss 0.00788538, acc 1
2016-09-06T07:08:52.365518: step 12180, loss 0.00376707, acc 1
2016-09-06T07:08:53.179631: step 12181, loss 0.0077321, acc 1
2016-09-06T07:08:53.992111: step 12182, loss 0.0168918, acc 1
2016-09-06T07:08:54.792313: step 12183, loss 0.00326329, acc 1
2016-09-06T07:08:55.603800: step 12184, loss 0.00812011, acc 1
2016-09-06T07:08:56.445482: step 12185, loss 0.0103261, acc 1
2016-09-06T07:08:57.226630: step 12186, loss 0.00198822, acc 1
2016-09-06T07:08:58.023457: step 12187, loss 0.0276466, acc 0.98
2016-09-06T07:08:58.814357: step 12188, loss 0.0153025, acc 1
2016-09-06T07:08:59.597442: step 12189, loss 0.0047537, acc 1
2016-09-06T07:09:00.420272: step 12190, loss 0.00255402, acc 1
2016-09-06T07:09:01.224914: step 12191, loss 0.00203548, acc 1
2016-09-06T07:09:02.015555: step 12192, loss 0.00189341, acc 1
2016-09-06T07:09:02.846390: step 12193, loss 0.0374287, acc 0.98
2016-09-06T07:09:03.689439: step 12194, loss 0.017034, acc 0.98
2016-09-06T07:09:04.477330: step 12195, loss 0.012816, acc 1
2016-09-06T07:09:05.311351: step 12196, loss 0.00738159, acc 1
2016-09-06T07:09:06.097404: step 12197, loss 0.0150441, acc 1
2016-09-06T07:09:06.872889: step 12198, loss 0.00288006, acc 1
2016-09-06T07:09:07.667029: step 12199, loss 0.00865883, acc 1
2016-09-06T07:09:08.464552: step 12200, loss 0.0019191, acc 1

Evaluation:
2016-09-06T07:09:12.161354: step 12200, loss 2.9173, acc 0.727955

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-12200

2016-09-06T07:09:14.090542: step 12201, loss 0.00537515, acc 1
2016-09-06T07:09:14.921350: step 12202, loss 0.0155252, acc 1
2016-09-06T07:09:15.708379: step 12203, loss 0.0478184, acc 0.96
2016-09-06T07:09:16.518938: step 12204, loss 0.00415347, acc 1
2016-09-06T07:09:17.332948: step 12205, loss 0.069452, acc 0.98
2016-09-06T07:09:18.161272: step 12206, loss 0.0103864, acc 1
2016-09-06T07:09:19.005616: step 12207, loss 0.00220074, acc 1
2016-09-06T07:09:19.825470: step 12208, loss 0.0146306, acc 1
2016-09-06T07:09:20.647721: step 12209, loss 0.0105215, acc 1
2016-09-06T07:09:21.459299: step 12210, loss 0.00295481, acc 1
2016-09-06T07:09:22.268161: step 12211, loss 0.0124098, acc 1
2016-09-06T07:09:23.085965: step 12212, loss 0.00503508, acc 1
2016-09-06T07:09:23.894312: step 12213, loss 0.0427553, acc 0.98
2016-09-06T07:09:24.711540: step 12214, loss 0.00413387, acc 1
2016-09-06T07:09:25.541396: step 12215, loss 0.0170072, acc 1
2016-09-06T07:09:26.342334: step 12216, loss 0.0018719, acc 1
2016-09-06T07:09:27.170078: step 12217, loss 0.00400705, acc 1
2016-09-06T07:09:28.001144: step 12218, loss 0.00192385, acc 1
2016-09-06T07:09:28.807941: step 12219, loss 0.00264065, acc 1
2016-09-06T07:09:29.618331: step 12220, loss 0.0171499, acc 0.98
2016-09-06T07:09:30.448108: step 12221, loss 0.0393, acc 0.98
2016-09-06T07:09:31.248481: step 12222, loss 0.0106166, acc 1
2016-09-06T07:09:32.071703: step 12223, loss 0.0294824, acc 0.98
2016-09-06T07:09:32.883821: step 12224, loss 0.0528419, acc 0.96
2016-09-06T07:09:33.710453: step 12225, loss 0.00251871, acc 1
2016-09-06T07:09:34.519670: step 12226, loss 0.0126267, acc 1
2016-09-06T07:09:35.339699: step 12227, loss 0.0163022, acc 0.98
2016-09-06T07:09:36.152589: step 12228, loss 0.0170739, acc 1
2016-09-06T07:09:36.941422: step 12229, loss 0.070821, acc 0.96
2016-09-06T07:09:37.773817: step 12230, loss 0.00333227, acc 1
2016-09-06T07:09:38.601944: step 12231, loss 0.0893245, acc 0.94
2016-09-06T07:09:39.464767: step 12232, loss 0.0688518, acc 0.98
2016-09-06T07:09:40.300627: step 12233, loss 0.0171837, acc 0.98
2016-09-06T07:09:41.117397: step 12234, loss 0.158081, acc 0.94
2016-09-06T07:09:41.941373: step 12235, loss 0.0179043, acc 0.98
2016-09-06T07:09:42.767548: step 12236, loss 0.00161948, acc 1
2016-09-06T07:09:43.606671: step 12237, loss 0.00516394, acc 1
2016-09-06T07:09:44.405690: step 12238, loss 0.00676687, acc 1
2016-09-06T07:09:45.224501: step 12239, loss 0.00336255, acc 1
2016-09-06T07:09:46.044872: step 12240, loss 0.0537529, acc 0.96
2016-09-06T07:09:46.827595: step 12241, loss 0.00541133, acc 1
2016-09-06T07:09:47.600851: step 12242, loss 0.0242625, acc 1
2016-09-06T07:09:48.441225: step 12243, loss 0.0101856, acc 1
2016-09-06T07:09:49.211862: step 12244, loss 0.00670485, acc 1
2016-09-06T07:09:50.053450: step 12245, loss 0.0132562, acc 1
2016-09-06T07:09:50.877463: step 12246, loss 0.0100285, acc 1
2016-09-06T07:09:51.682203: step 12247, loss 0.00803639, acc 1
2016-09-06T07:09:52.478848: step 12248, loss 0.00781229, acc 1
2016-09-06T07:09:53.282963: step 12249, loss 0.0311625, acc 0.96
2016-09-06T07:09:54.071156: step 12250, loss 0.00186369, acc 1
2016-09-06T07:09:54.855400: step 12251, loss 0.00309882, acc 1
2016-09-06T07:09:55.690419: step 12252, loss 0.0186341, acc 1
2016-09-06T07:09:56.490857: step 12253, loss 0.00647852, acc 1
2016-09-06T07:09:57.287257: step 12254, loss 0.0350653, acc 0.98
2016-09-06T07:09:58.077339: step 12255, loss 0.0084157, acc 1
2016-09-06T07:09:58.886005: step 12256, loss 0.0198942, acc 1
2016-09-06T07:09:59.693146: step 12257, loss 0.0438995, acc 0.96
2016-09-06T07:10:00.531395: step 12258, loss 0.0479729, acc 0.98
2016-09-06T07:10:01.331579: step 12259, loss 0.0158238, acc 1
2016-09-06T07:10:02.147099: step 12260, loss 0.00181791, acc 1
2016-09-06T07:10:02.991448: step 12261, loss 0.00205671, acc 1
2016-09-06T07:10:03.815506: step 12262, loss 0.018984, acc 0.98
2016-09-06T07:10:04.616653: step 12263, loss 0.0109218, acc 1
2016-09-06T07:10:05.482631: step 12264, loss 0.00960014, acc 1
2016-09-06T07:10:06.309205: step 12265, loss 0.0207426, acc 1
2016-09-06T07:10:07.149037: step 12266, loss 0.00730549, acc 1
2016-09-06T07:10:07.997996: step 12267, loss 0.00570339, acc 1
2016-09-06T07:10:08.833346: step 12268, loss 0.0219302, acc 0.98
2016-09-06T07:10:09.662480: step 12269, loss 0.0200189, acc 0.98
2016-09-06T07:10:10.507241: step 12270, loss 0.0196213, acc 1
2016-09-06T07:10:11.334897: step 12271, loss 0.0190074, acc 0.98
2016-09-06T07:10:12.141458: step 12272, loss 0.00254827, acc 1
2016-09-06T07:10:12.929902: step 12273, loss 0.0167455, acc 1
2016-09-06T07:10:13.730180: step 12274, loss 0.0192352, acc 0.98
2016-09-06T07:10:14.507810: step 12275, loss 0.0163594, acc 0.98
2016-09-06T07:10:15.363660: step 12276, loss 0.126248, acc 0.98
2016-09-06T07:10:16.200232: step 12277, loss 0.00216134, acc 1
2016-09-06T07:10:16.991327: step 12278, loss 0.00215776, acc 1
2016-09-06T07:10:17.777802: step 12279, loss 0.0142806, acc 1
2016-09-06T07:10:18.596091: step 12280, loss 0.00215391, acc 1
2016-09-06T07:10:19.384948: step 12281, loss 0.00595193, acc 1
2016-09-06T07:10:20.208078: step 12282, loss 0.022598, acc 1
2016-09-06T07:10:21.033318: step 12283, loss 0.0186092, acc 0.98
2016-09-06T07:10:21.817452: step 12284, loss 0.00205211, acc 1
2016-09-06T07:10:22.656958: step 12285, loss 0.0232363, acc 1
2016-09-06T07:10:23.460388: step 12286, loss 0.00217012, acc 1
2016-09-06T07:10:24.256450: step 12287, loss 0.0130696, acc 1
2016-09-06T07:10:25.023484: step 12288, loss 0.00380215, acc 1
2016-09-06T07:10:25.873797: step 12289, loss 0.0228308, acc 0.98
2016-09-06T07:10:26.653851: step 12290, loss 0.0116866, acc 1
2016-09-06T07:10:27.471630: step 12291, loss 0.00792125, acc 1
2016-09-06T07:10:28.303085: step 12292, loss 0.00568862, acc 1
2016-09-06T07:10:29.094912: step 12293, loss 0.0185155, acc 0.98
2016-09-06T07:10:29.895770: step 12294, loss 0.0333115, acc 0.98
2016-09-06T07:10:30.697321: step 12295, loss 0.0258927, acc 0.98
2016-09-06T07:10:31.494234: step 12296, loss 0.031091, acc 0.98
2016-09-06T07:10:32.284046: step 12297, loss 0.239327, acc 0.94
2016-09-06T07:10:33.070240: step 12298, loss 0.00302562, acc 1
2016-09-06T07:10:33.884248: step 12299, loss 0.00204071, acc 1
2016-09-06T07:10:34.698362: step 12300, loss 0.00206482, acc 1

Evaluation:
2016-09-06T07:10:38.397001: step 12300, loss 2.41725, acc 0.721388

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-12300

2016-09-06T07:10:40.294299: step 12301, loss 0.0172571, acc 1
2016-09-06T07:10:41.137420: step 12302, loss 0.00278053, acc 1
2016-09-06T07:10:41.989551: step 12303, loss 0.0128435, acc 1
2016-09-06T07:10:42.815333: step 12304, loss 0.00386371, acc 1
2016-09-06T07:10:43.626427: step 12305, loss 0.0125834, acc 1
2016-09-06T07:10:44.427669: step 12306, loss 0.0399069, acc 0.98
2016-09-06T07:10:45.258554: step 12307, loss 0.0149904, acc 1
2016-09-06T07:10:46.037109: step 12308, loss 0.00210959, acc 1
2016-09-06T07:10:46.832668: step 12309, loss 0.00486043, acc 1
2016-09-06T07:10:47.652500: step 12310, loss 0.00286869, acc 1
2016-09-06T07:10:48.477932: step 12311, loss 0.0132989, acc 1
2016-09-06T07:10:49.269332: step 12312, loss 0.00925361, acc 1
2016-09-06T07:10:50.097457: step 12313, loss 0.00276251, acc 1
2016-09-06T07:10:50.916413: step 12314, loss 0.01043, acc 1
2016-09-06T07:10:51.700637: step 12315, loss 0.0153789, acc 1
2016-09-06T07:10:52.519997: step 12316, loss 0.0230485, acc 0.98
2016-09-06T07:10:53.303883: step 12317, loss 0.0441216, acc 0.96
2016-09-06T07:10:54.144769: step 12318, loss 0.00398766, acc 1
2016-09-06T07:10:54.963947: step 12319, loss 0.0390927, acc 0.98
2016-09-06T07:10:55.784025: step 12320, loss 0.00535893, acc 1
2016-09-06T07:10:56.582445: step 12321, loss 0.0279774, acc 0.98
2016-09-06T07:10:57.403054: step 12322, loss 0.0554322, acc 0.96
2016-09-06T07:10:58.221512: step 12323, loss 0.116235, acc 0.98
2016-09-06T07:10:59.017364: step 12324, loss 0.0174546, acc 0.98
2016-09-06T07:10:59.862949: step 12325, loss 0.00204344, acc 1
2016-09-06T07:11:00.715962: step 12326, loss 0.0554253, acc 0.98
2016-09-06T07:11:01.543519: step 12327, loss 0.033431, acc 0.98
2016-09-06T07:11:02.384654: step 12328, loss 0.00227279, acc 1
2016-09-06T07:11:03.192178: step 12329, loss 0.00961552, acc 1
2016-09-06T07:11:04.023584: step 12330, loss 0.0172588, acc 0.98
2016-09-06T07:11:04.841883: step 12331, loss 0.0501252, acc 0.98
2016-09-06T07:11:05.662165: step 12332, loss 0.00599978, acc 1
2016-09-06T07:11:06.481539: step 12333, loss 0.00225054, acc 1
2016-09-06T07:11:07.301452: step 12334, loss 0.0115128, acc 1
2016-09-06T07:11:08.133457: step 12335, loss 0.00882654, acc 1
2016-09-06T07:11:08.962636: step 12336, loss 0.0203103, acc 1
2016-09-06T07:11:09.776869: step 12337, loss 0.00666612, acc 1
2016-09-06T07:11:10.582990: step 12338, loss 0.00359321, acc 1
2016-09-06T07:11:11.378610: step 12339, loss 0.00787633, acc 1
2016-09-06T07:11:12.192417: step 12340, loss 0.0416723, acc 1
2016-09-06T07:11:12.996618: step 12341, loss 0.00954877, acc 1
2016-09-06T07:11:13.827843: step 12342, loss 0.118875, acc 0.96
2016-09-06T07:11:14.636926: step 12343, loss 0.005629, acc 1
2016-09-06T07:11:15.449861: step 12344, loss 0.0290803, acc 0.98
2016-09-06T07:11:16.242252: step 12345, loss 0.0236593, acc 0.98
2016-09-06T07:11:17.047161: step 12346, loss 0.0327398, acc 1
2016-09-06T07:11:17.866092: step 12347, loss 0.00203797, acc 1
2016-09-06T07:11:18.645002: step 12348, loss 0.0039116, acc 1
2016-09-06T07:11:19.480284: step 12349, loss 0.12202, acc 0.98
2016-09-06T07:11:20.292158: step 12350, loss 0.00498513, acc 1
2016-09-06T07:11:21.074071: step 12351, loss 0.00225425, acc 1
2016-09-06T07:11:21.855926: step 12352, loss 0.0325401, acc 0.98
2016-09-06T07:11:22.685363: step 12353, loss 0.00249462, acc 1
2016-09-06T07:11:23.478269: step 12354, loss 0.00762421, acc 1
2016-09-06T07:11:24.370832: step 12355, loss 0.021203, acc 1
2016-09-06T07:11:25.212681: step 12356, loss 0.0409981, acc 0.98
2016-09-06T07:11:26.042858: step 12357, loss 0.0251071, acc 1
2016-09-06T07:11:26.870307: step 12358, loss 0.0616496, acc 0.94
2016-09-06T07:11:27.692736: step 12359, loss 0.0149041, acc 1
2016-09-06T07:11:28.484417: step 12360, loss 0.00921162, acc 1
2016-09-06T07:11:29.306135: step 12361, loss 0.0023795, acc 1
2016-09-06T07:11:30.127967: step 12362, loss 0.0022641, acc 1
2016-09-06T07:11:30.936573: step 12363, loss 0.0212657, acc 0.98
2016-09-06T07:11:31.747877: step 12364, loss 0.0157768, acc 1
2016-09-06T07:11:32.568038: step 12365, loss 0.0199738, acc 0.98
2016-09-06T07:11:33.380364: step 12366, loss 0.0188253, acc 0.98
2016-09-06T07:11:34.198941: step 12367, loss 0.0215669, acc 1
2016-09-06T07:11:35.039895: step 12368, loss 0.00212061, acc 1
2016-09-06T07:11:35.873263: step 12369, loss 0.00214805, acc 1
2016-09-06T07:11:36.692193: step 12370, loss 0.00443331, acc 1
2016-09-06T07:11:37.546579: step 12371, loss 0.0255575, acc 0.98
2016-09-06T07:11:38.358822: step 12372, loss 0.0317247, acc 0.98
2016-09-06T07:11:39.171268: step 12373, loss 0.020592, acc 1
2016-09-06T07:11:39.984950: step 12374, loss 0.044037, acc 0.96
2016-09-06T07:11:40.789806: step 12375, loss 0.0127238, acc 1
2016-09-06T07:11:41.594196: step 12376, loss 0.00543387, acc 1
2016-09-06T07:11:42.424277: step 12377, loss 0.00351746, acc 1
2016-09-06T07:11:43.254270: step 12378, loss 0.00206917, acc 1
2016-09-06T07:11:44.040148: step 12379, loss 0.00429803, acc 1
2016-09-06T07:11:44.850049: step 12380, loss 0.00356139, acc 1
2016-09-06T07:11:45.666116: step 12381, loss 0.0616146, acc 0.96
2016-09-06T07:11:46.471212: step 12382, loss 0.0512354, acc 0.96
2016-09-06T07:11:47.281819: step 12383, loss 0.0157934, acc 1
2016-09-06T07:11:48.104786: step 12384, loss 0.00739411, acc 1
2016-09-06T07:11:48.931707: step 12385, loss 0.00217185, acc 1
2016-09-06T07:11:49.752285: step 12386, loss 0.002708, acc 1
2016-09-06T07:11:50.558672: step 12387, loss 0.029135, acc 0.98
2016-09-06T07:11:51.376906: step 12388, loss 0.0161277, acc 1
2016-09-06T07:11:52.170950: step 12389, loss 0.0100533, acc 1
2016-09-06T07:11:52.971688: step 12390, loss 0.00851334, acc 1
2016-09-06T07:11:53.734446: step 12391, loss 0.0368657, acc 0.96
2016-09-06T07:11:54.577915: step 12392, loss 0.00691378, acc 1
2016-09-06T07:11:55.424862: step 12393, loss 0.0230046, acc 0.98
2016-09-06T07:11:56.233224: step 12394, loss 0.00513338, acc 1
2016-09-06T07:11:57.095337: step 12395, loss 0.0284141, acc 0.98
2016-09-06T07:11:57.919317: step 12396, loss 0.0727199, acc 0.96
2016-09-06T07:11:58.737868: step 12397, loss 0.0403849, acc 0.96
2016-09-06T07:11:59.586974: step 12398, loss 0.0755384, acc 0.92
2016-09-06T07:12:00.470430: step 12399, loss 0.00372113, acc 1
2016-09-06T07:12:01.295506: step 12400, loss 0.0341805, acc 0.98

Evaluation:
2016-09-06T07:12:05.033533: step 12400, loss 3.39877, acc 0.718574

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-12400

2016-09-06T07:12:06.954583: step 12401, loss 0.00777155, acc 1
2016-09-06T07:12:07.829320: step 12402, loss 0.0158901, acc 1
2016-09-06T07:12:08.630706: step 12403, loss 0.0786415, acc 0.92
2016-09-06T07:12:09.434503: step 12404, loss 0.0687263, acc 0.96
2016-09-06T07:12:10.307942: step 12405, loss 0.0047527, acc 1
2016-09-06T07:12:11.116713: step 12406, loss 0.00386571, acc 1
2016-09-06T07:12:11.943655: step 12407, loss 0.0158298, acc 1
2016-09-06T07:12:12.758037: step 12408, loss 0.0201408, acc 0.98
2016-09-06T07:12:13.584285: step 12409, loss 0.00488153, acc 1
2016-09-06T07:12:14.360934: step 12410, loss 0.0235914, acc 0.98
2016-09-06T07:12:15.327395: step 12411, loss 0.00386248, acc 1
2016-09-06T07:12:16.198278: step 12412, loss 0.0178441, acc 0.98
2016-09-06T07:12:17.094865: step 12413, loss 0.0438905, acc 0.96
2016-09-06T07:12:17.910449: step 12414, loss 0.0190605, acc 1
2016-09-06T07:12:18.757474: step 12415, loss 0.0138898, acc 1
2016-09-06T07:12:19.667967: step 12416, loss 0.00833151, acc 1
2016-09-06T07:12:20.486587: step 12417, loss 0.00366567, acc 1
2016-09-06T07:12:21.295179: step 12418, loss 0.017162, acc 1
2016-09-06T07:12:22.128220: step 12419, loss 0.0143346, acc 1
2016-09-06T07:12:22.971924: step 12420, loss 0.00354001, acc 1
2016-09-06T07:12:23.764566: step 12421, loss 0.0216775, acc 0.98
2016-09-06T07:12:24.621021: step 12422, loss 0.0386348, acc 0.98
2016-09-06T07:12:25.429954: step 12423, loss 0.0192539, acc 1
2016-09-06T07:12:26.269651: step 12424, loss 0.0383379, acc 0.98
2016-09-06T07:12:27.116448: step 12425, loss 0.0147302, acc 1
2016-09-06T07:12:27.939071: step 12426, loss 0.0553206, acc 0.98
2016-09-06T07:12:28.754605: step 12427, loss 0.00338486, acc 1
2016-09-06T07:12:29.574000: step 12428, loss 0.00336339, acc 1
2016-09-06T07:12:30.374088: step 12429, loss 0.0294674, acc 1
2016-09-06T07:12:31.187259: step 12430, loss 0.00342488, acc 1
2016-09-06T07:12:32.007953: step 12431, loss 0.00333009, acc 1
2016-09-06T07:12:32.804864: step 12432, loss 0.00437088, acc 1
2016-09-06T07:12:33.611280: step 12433, loss 0.028298, acc 0.98
2016-09-06T07:12:34.429223: step 12434, loss 0.0235116, acc 0.98
2016-09-06T07:12:35.233569: step 12435, loss 0.0100356, acc 1
2016-09-06T07:12:36.058476: step 12436, loss 0.00343546, acc 1
2016-09-06T07:12:36.904854: step 12437, loss 0.015941, acc 1
2016-09-06T07:12:37.707329: step 12438, loss 0.238137, acc 0.96
2016-09-06T07:12:38.511806: step 12439, loss 0.00307998, acc 1
2016-09-06T07:12:39.331376: step 12440, loss 0.00296965, acc 1
2016-09-06T07:12:40.132211: step 12441, loss 0.0164049, acc 1
2016-09-06T07:12:40.943994: step 12442, loss 0.0027736, acc 1
2016-09-06T07:12:41.754652: step 12443, loss 0.00351196, acc 1
2016-09-06T07:12:42.573274: step 12444, loss 0.0642579, acc 0.96
2016-09-06T07:12:43.393560: step 12445, loss 0.0188516, acc 1
2016-09-06T07:12:44.213668: step 12446, loss 0.0707243, acc 0.98
2016-09-06T07:12:45.052938: step 12447, loss 0.0102905, acc 1
2016-09-06T07:12:45.862323: step 12448, loss 0.0459919, acc 1
2016-09-06T07:12:46.662836: step 12449, loss 0.0785033, acc 0.94
2016-09-06T07:12:47.463082: step 12450, loss 0.0172012, acc 1
2016-09-06T07:12:48.265530: step 12451, loss 0.0403688, acc 0.98
2016-09-06T07:12:49.079147: step 12452, loss 0.00414046, acc 1
2016-09-06T07:12:49.925653: step 12453, loss 0.0188912, acc 1
2016-09-06T07:12:50.726955: step 12454, loss 0.0174729, acc 0.98
2016-09-06T07:12:51.532590: step 12455, loss 0.00917097, acc 1
2016-09-06T07:12:52.368899: step 12456, loss 0.0314246, acc 1
2016-09-06T07:12:53.186171: step 12457, loss 0.0486358, acc 0.96
2016-09-06T07:12:54.005940: step 12458, loss 0.00395242, acc 1
2016-09-06T07:12:54.857584: step 12459, loss 0.0164589, acc 1
2016-09-06T07:12:55.674323: step 12460, loss 0.0171579, acc 1
2016-09-06T07:12:56.509421: step 12461, loss 0.0027238, acc 1
2016-09-06T07:12:57.348361: step 12462, loss 0.0224132, acc 0.98
2016-09-06T07:12:58.167763: step 12463, loss 0.0334888, acc 0.98
2016-09-06T07:12:58.975364: step 12464, loss 0.0090572, acc 1
2016-09-06T07:12:59.830329: step 12465, loss 0.0296518, acc 0.98
2016-09-06T07:13:00.681669: step 12466, loss 0.0257244, acc 0.98
2016-09-06T07:13:01.489564: step 12467, loss 0.00359, acc 1
2016-09-06T07:13:02.345999: step 12468, loss 0.0286011, acc 0.98
2016-09-06T07:13:03.219052: step 12469, loss 0.00348912, acc 1
2016-09-06T07:13:04.031804: step 12470, loss 0.0405218, acc 0.98
2016-09-06T07:13:04.838238: step 12471, loss 0.00409079, acc 1
2016-09-06T07:13:05.684998: step 12472, loss 0.00292747, acc 1
2016-09-06T07:13:06.497285: step 12473, loss 0.0268205, acc 1
2016-09-06T07:13:07.326436: step 12474, loss 0.00774209, acc 1
2016-09-06T07:13:08.149989: step 12475, loss 0.0242905, acc 0.98
2016-09-06T07:13:08.980697: step 12476, loss 0.00361351, acc 1
2016-09-06T07:13:09.826391: step 12477, loss 0.00912381, acc 1
2016-09-06T07:13:10.639809: step 12478, loss 0.0045786, acc 1
2016-09-06T07:13:11.435035: step 12479, loss 0.00395908, acc 1
2016-09-06T07:13:12.190993: step 12480, loss 0.00300532, acc 1
2016-09-06T07:13:13.001215: step 12481, loss 0.00369718, acc 1
2016-09-06T07:13:13.798357: step 12482, loss 0.0171059, acc 1
2016-09-06T07:13:14.626183: step 12483, loss 0.00287049, acc 1
2016-09-06T07:13:15.472288: step 12484, loss 0.00291551, acc 1
2016-09-06T07:13:16.284183: step 12485, loss 0.0197747, acc 0.98
2016-09-06T07:13:17.122568: step 12486, loss 0.0361108, acc 0.98
2016-09-06T07:13:17.972058: step 12487, loss 0.0766085, acc 0.98
2016-09-06T07:13:18.809496: step 12488, loss 0.0290472, acc 0.98
2016-09-06T07:13:19.639906: step 12489, loss 0.0381582, acc 0.96
2016-09-06T07:13:20.435604: step 12490, loss 0.0204574, acc 0.98
2016-09-06T07:13:21.225656: step 12491, loss 0.00359201, acc 1
2016-09-06T07:13:22.022832: step 12492, loss 0.00262233, acc 1
2016-09-06T07:13:22.845575: step 12493, loss 0.00632208, acc 1
2016-09-06T07:13:23.639284: step 12494, loss 0.007272, acc 1
2016-09-06T07:13:24.443276: step 12495, loss 0.0461396, acc 0.96
2016-09-06T07:13:25.274469: step 12496, loss 0.00932895, acc 1
2016-09-06T07:13:26.094930: step 12497, loss 0.00741677, acc 1
2016-09-06T07:13:26.872548: step 12498, loss 0.00268233, acc 1
2016-09-06T07:13:27.681680: step 12499, loss 0.0157669, acc 1
2016-09-06T07:13:28.498758: step 12500, loss 0.0121003, acc 1

Evaluation:
2016-09-06T07:13:32.194593: step 12500, loss 2.7662, acc 0.724203

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-12500

2016-09-06T07:13:34.016400: step 12501, loss 0.0067115, acc 1
2016-09-06T07:13:34.825285: step 12502, loss 0.0340642, acc 0.98
2016-09-06T07:13:35.656119: step 12503, loss 0.00280085, acc 1
2016-09-06T07:13:36.473258: step 12504, loss 0.0186052, acc 1
2016-09-06T07:13:37.329024: step 12505, loss 0.0107213, acc 1
2016-09-06T07:13:38.122925: step 12506, loss 0.0409007, acc 0.96
2016-09-06T07:13:38.942248: step 12507, loss 0.00406321, acc 1
2016-09-06T07:13:39.757410: step 12508, loss 0.0277968, acc 0.98
2016-09-06T07:13:40.582908: step 12509, loss 0.028845, acc 0.98
2016-09-06T07:13:41.365402: step 12510, loss 0.0125801, acc 1
2016-09-06T07:13:42.201075: step 12511, loss 0.00306546, acc 1
2016-09-06T07:13:43.027531: step 12512, loss 0.0489199, acc 0.98
2016-09-06T07:13:43.848467: step 12513, loss 0.0141025, acc 1
2016-09-06T07:13:44.671618: step 12514, loss 0.0327257, acc 0.98
2016-09-06T07:13:45.527869: step 12515, loss 0.00238635, acc 1
2016-09-06T07:13:46.355782: step 12516, loss 0.145044, acc 0.98
2016-09-06T07:13:47.181132: step 12517, loss 0.00221271, acc 1
2016-09-06T07:13:48.008722: step 12518, loss 0.0276882, acc 1
2016-09-06T07:13:48.812834: step 12519, loss 0.0104341, acc 1
2016-09-06T07:13:49.618262: step 12520, loss 0.00241401, acc 1
2016-09-06T07:13:50.444539: step 12521, loss 0.0518248, acc 0.98
2016-09-06T07:13:51.219324: step 12522, loss 0.00218558, acc 1
2016-09-06T07:13:52.028158: step 12523, loss 0.018547, acc 1
2016-09-06T07:13:52.849562: step 12524, loss 0.00231307, acc 1
2016-09-06T07:13:53.639561: step 12525, loss 0.0190873, acc 0.98
2016-09-06T07:13:54.434109: step 12526, loss 0.00928026, acc 1
2016-09-06T07:13:55.241211: step 12527, loss 0.00227129, acc 1
2016-09-06T07:13:56.050647: step 12528, loss 0.0457312, acc 0.98
2016-09-06T07:13:56.867543: step 12529, loss 0.0127585, acc 1
2016-09-06T07:13:57.693232: step 12530, loss 0.0117453, acc 1
2016-09-06T07:13:58.481416: step 12531, loss 0.0155752, acc 1
2016-09-06T07:13:59.333718: step 12532, loss 0.0304012, acc 0.98
2016-09-06T07:14:00.160176: step 12533, loss 0.0231949, acc 1
2016-09-06T07:14:01.000034: step 12534, loss 0.00359175, acc 1
2016-09-06T07:14:01.838727: step 12535, loss 0.00229627, acc 1
2016-09-06T07:14:02.653046: step 12536, loss 0.00588702, acc 1
2016-09-06T07:14:03.483303: step 12537, loss 0.00210863, acc 1
2016-09-06T07:14:04.293941: step 12538, loss 0.0704042, acc 0.96
2016-09-06T07:14:05.143290: step 12539, loss 0.0192375, acc 0.98
2016-09-06T07:14:05.934066: step 12540, loss 0.00248792, acc 1
2016-09-06T07:14:06.742127: step 12541, loss 0.031877, acc 0.98
2016-09-06T07:14:07.575810: step 12542, loss 0.0146525, acc 1
2016-09-06T07:14:08.383766: step 12543, loss 0.00218029, acc 1
2016-09-06T07:14:09.170867: step 12544, loss 0.0303908, acc 0.98
2016-09-06T07:14:10.006508: step 12545, loss 0.0025199, acc 1
2016-09-06T07:14:10.834215: step 12546, loss 0.0200452, acc 0.98
2016-09-06T07:14:11.644495: step 12547, loss 0.0159962, acc 1
2016-09-06T07:14:12.493412: step 12548, loss 0.0136311, acc 1
2016-09-06T07:14:13.315729: step 12549, loss 0.031613, acc 0.98
2016-09-06T07:14:14.133114: step 12550, loss 0.00249405, acc 1
2016-09-06T07:14:14.978569: step 12551, loss 0.00442284, acc 1
2016-09-06T07:14:15.781201: step 12552, loss 0.00250047, acc 1
2016-09-06T07:14:16.571546: step 12553, loss 0.0137271, acc 1
2016-09-06T07:14:17.373710: step 12554, loss 0.00486233, acc 1
2016-09-06T07:14:18.177505: step 12555, loss 0.0162013, acc 0.98
2016-09-06T07:14:18.979908: step 12556, loss 0.0161706, acc 1
2016-09-06T07:14:19.792444: step 12557, loss 0.00207454, acc 1
2016-09-06T07:14:20.601340: step 12558, loss 0.0216446, acc 0.98
2016-09-06T07:14:21.388034: step 12559, loss 0.0405985, acc 0.98
2016-09-06T07:14:22.205116: step 12560, loss 0.00726972, acc 1
2016-09-06T07:14:23.027071: step 12561, loss 0.00212721, acc 1
2016-09-06T07:14:23.824672: step 12562, loss 0.002743, acc 1
2016-09-06T07:14:24.622079: step 12563, loss 0.0068362, acc 1
2016-09-06T07:14:25.442555: step 12564, loss 0.00218796, acc 1
2016-09-06T07:14:26.241350: step 12565, loss 0.0179097, acc 0.98
2016-09-06T07:14:27.069463: step 12566, loss 0.0299254, acc 0.98
2016-09-06T07:14:27.895670: step 12567, loss 0.00257965, acc 1
2016-09-06T07:14:28.691038: step 12568, loss 0.0333549, acc 0.98
2016-09-06T07:14:29.502023: step 12569, loss 0.006048, acc 1
2016-09-06T07:14:30.331949: step 12570, loss 0.00415435, acc 1
2016-09-06T07:14:31.125035: step 12571, loss 0.0150274, acc 1
2016-09-06T07:14:31.910099: step 12572, loss 0.00322452, acc 1
2016-09-06T07:14:32.744457: step 12573, loss 0.00920124, acc 1
2016-09-06T07:14:33.514576: step 12574, loss 0.00332828, acc 1
2016-09-06T07:14:34.326924: step 12575, loss 0.0229531, acc 0.98
2016-09-06T07:14:35.157513: step 12576, loss 0.0021449, acc 1
2016-09-06T07:14:35.928428: step 12577, loss 0.00197748, acc 1
2016-09-06T07:14:36.732439: step 12578, loss 0.0114257, acc 1
2016-09-06T07:14:37.550223: step 12579, loss 0.00215055, acc 1
2016-09-06T07:14:38.352438: step 12580, loss 0.00385921, acc 1
2016-09-06T07:14:39.162860: step 12581, loss 0.00961024, acc 1
2016-09-06T07:14:39.974707: step 12582, loss 0.00588423, acc 1
2016-09-06T07:14:40.771950: step 12583, loss 0.00207917, acc 1
2016-09-06T07:14:41.596700: step 12584, loss 0.00201549, acc 1
2016-09-06T07:14:42.432339: step 12585, loss 0.00999169, acc 1
2016-09-06T07:14:43.228258: step 12586, loss 0.0113866, acc 1
2016-09-06T07:14:44.044356: step 12587, loss 0.0105197, acc 1
2016-09-06T07:14:44.849822: step 12588, loss 0.0138042, acc 1
2016-09-06T07:14:45.644643: step 12589, loss 0.0155279, acc 1
2016-09-06T07:14:46.451055: step 12590, loss 0.125156, acc 0.96
2016-09-06T07:14:47.256650: step 12591, loss 0.00195454, acc 1
2016-09-06T07:14:48.016640: step 12592, loss 0.0172137, acc 0.98
2016-09-06T07:14:48.828313: step 12593, loss 0.00389343, acc 1
2016-09-06T07:14:49.677570: step 12594, loss 0.00382883, acc 1
2016-09-06T07:14:50.498712: step 12595, loss 0.00448073, acc 1
2016-09-06T07:14:51.320040: step 12596, loss 0.00364487, acc 1
2016-09-06T07:14:52.167678: step 12597, loss 0.0307611, acc 0.98
2016-09-06T07:14:52.998629: step 12598, loss 0.00172618, acc 1
2016-09-06T07:14:53.802385: step 12599, loss 0.00658179, acc 1
2016-09-06T07:14:54.624901: step 12600, loss 0.029419, acc 0.98

Evaluation:
2016-09-06T07:14:58.336743: step 12600, loss 2.63746, acc 0.726079

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-12600

2016-09-06T07:15:00.329242: step 12601, loss 0.00267874, acc 1
2016-09-06T07:15:01.184493: step 12602, loss 0.00181712, acc 1
2016-09-06T07:15:01.994654: step 12603, loss 0.020949, acc 0.98
2016-09-06T07:15:02.800268: step 12604, loss 0.0129894, acc 1
2016-09-06T07:15:03.620948: step 12605, loss 0.0236445, acc 0.98
2016-09-06T07:15:04.446481: step 12606, loss 0.00584429, acc 1
2016-09-06T07:15:05.243206: step 12607, loss 0.0227602, acc 0.98
2016-09-06T07:15:06.045389: step 12608, loss 0.0032345, acc 1
2016-09-06T07:15:06.874473: step 12609, loss 0.104142, acc 0.98
2016-09-06T07:15:07.686508: step 12610, loss 0.0150923, acc 1
2016-09-06T07:15:08.489157: step 12611, loss 0.015579, acc 0.98
2016-09-06T07:15:09.341640: step 12612, loss 0.0299144, acc 0.98
2016-09-06T07:15:10.150421: step 12613, loss 0.00201764, acc 1
2016-09-06T07:15:10.951996: step 12614, loss 0.049729, acc 0.96
2016-09-06T07:15:11.779004: step 12615, loss 0.0039394, acc 1
2016-09-06T07:15:12.622523: step 12616, loss 0.0270721, acc 0.98
2016-09-06T07:15:13.453465: step 12617, loss 0.014397, acc 1
2016-09-06T07:15:14.285063: step 12618, loss 0.0135963, acc 1
2016-09-06T07:15:15.081639: step 12619, loss 0.00326023, acc 1
2016-09-06T07:15:15.880122: step 12620, loss 0.00257007, acc 1
2016-09-06T07:15:16.725543: step 12621, loss 0.0244638, acc 1
2016-09-06T07:15:17.532833: step 12622, loss 0.00479932, acc 1
2016-09-06T07:15:18.321445: step 12623, loss 0.0154757, acc 1
2016-09-06T07:15:19.126001: step 12624, loss 0.0193298, acc 1
2016-09-06T07:15:19.927667: step 12625, loss 0.0438044, acc 0.96
2016-09-06T07:15:20.743882: step 12626, loss 0.0154581, acc 1
2016-09-06T07:15:21.601365: step 12627, loss 0.0425604, acc 0.98
2016-09-06T07:15:22.416424: step 12628, loss 0.0540085, acc 0.98
2016-09-06T07:15:23.201138: step 12629, loss 0.0198807, acc 1
2016-09-06T07:15:24.021456: step 12630, loss 0.0236549, acc 0.98
2016-09-06T07:15:24.839186: step 12631, loss 0.0339687, acc 0.98
2016-09-06T07:15:25.596581: step 12632, loss 0.00238965, acc 1
2016-09-06T07:15:26.408853: step 12633, loss 0.00375072, acc 1
2016-09-06T07:15:27.236062: step 12634, loss 0.00410521, acc 1
2016-09-06T07:15:28.012456: step 12635, loss 0.00885586, acc 1
2016-09-06T07:15:28.824551: step 12636, loss 0.0137368, acc 1
2016-09-06T07:15:29.645322: step 12637, loss 0.0246076, acc 0.98
2016-09-06T07:15:30.450622: step 12638, loss 0.0463243, acc 0.98
2016-09-06T07:15:31.281198: step 12639, loss 0.00255956, acc 1
2016-09-06T07:15:32.087018: step 12640, loss 0.0524022, acc 0.98
2016-09-06T07:15:32.878245: step 12641, loss 0.0328526, acc 0.98
2016-09-06T07:15:33.713203: step 12642, loss 0.0381364, acc 0.96
2016-09-06T07:15:34.509795: step 12643, loss 0.00300492, acc 1
2016-09-06T07:15:35.293417: step 12644, loss 0.0034204, acc 1
2016-09-06T07:15:36.055706: step 12645, loss 0.0103872, acc 1
2016-09-06T07:15:36.862608: step 12646, loss 0.0173724, acc 1
2016-09-06T07:15:37.639832: step 12647, loss 0.00951047, acc 1
2016-09-06T07:15:38.454099: step 12648, loss 0.00376019, acc 1
2016-09-06T07:15:39.255503: step 12649, loss 0.0543853, acc 0.96
2016-09-06T07:15:40.056777: step 12650, loss 0.0127488, acc 1
2016-09-06T07:15:40.876369: step 12651, loss 0.0189943, acc 0.98
2016-09-06T07:15:41.680632: step 12652, loss 0.0511542, acc 0.98
2016-09-06T07:15:42.485804: step 12653, loss 0.00409406, acc 1
2016-09-06T07:15:43.288038: step 12654, loss 0.0248935, acc 0.98
2016-09-06T07:15:44.101122: step 12655, loss 0.0363912, acc 0.98
2016-09-06T07:15:44.884675: step 12656, loss 0.0039215, acc 1
2016-09-06T07:15:45.687474: step 12657, loss 0.00835823, acc 1
2016-09-06T07:15:46.531541: step 12658, loss 0.0175351, acc 1
2016-09-06T07:15:47.320264: step 12659, loss 0.022416, acc 1
2016-09-06T07:15:48.152696: step 12660, loss 0.0344624, acc 0.98
2016-09-06T07:15:48.967433: step 12661, loss 0.016341, acc 1
2016-09-06T07:15:49.744731: step 12662, loss 0.00428967, acc 1
2016-09-06T07:15:50.559323: step 12663, loss 0.00367586, acc 1
2016-09-06T07:15:51.430666: step 12664, loss 0.0039669, acc 1
2016-09-06T07:15:52.228379: step 12665, loss 0.00369945, acc 1
2016-09-06T07:15:53.048595: step 12666, loss 0.0171532, acc 1
2016-09-06T07:15:53.956955: step 12667, loss 0.0160065, acc 1
2016-09-06T07:15:54.755059: step 12668, loss 0.00369896, acc 1
2016-09-06T07:15:55.587893: step 12669, loss 0.0566784, acc 0.98
2016-09-06T07:15:56.402751: step 12670, loss 0.022395, acc 0.98
2016-09-06T07:15:57.233224: step 12671, loss 0.0436663, acc 0.96
2016-09-06T07:15:57.999433: step 12672, loss 0.00462887, acc 1
2016-09-06T07:15:58.830784: step 12673, loss 0.0300073, acc 0.98
2016-09-06T07:15:59.668305: step 12674, loss 0.0196987, acc 1
2016-09-06T07:16:00.531896: step 12675, loss 0.0306762, acc 0.98
2016-09-06T07:16:01.358728: step 12676, loss 0.0181628, acc 0.98
2016-09-06T07:16:02.190954: step 12677, loss 0.0333045, acc 0.96
2016-09-06T07:16:03.014911: step 12678, loss 0.00340929, acc 1
2016-09-06T07:16:03.832431: step 12679, loss 0.00820396, acc 1
2016-09-06T07:16:04.666727: step 12680, loss 0.0172549, acc 1
2016-09-06T07:16:05.469085: step 12681, loss 0.0107081, acc 1
2016-09-06T07:16:06.269573: step 12682, loss 0.0591905, acc 0.98
2016-09-06T07:16:07.106743: step 12683, loss 0.00349954, acc 1
2016-09-06T07:16:07.919410: step 12684, loss 0.00329613, acc 1
2016-09-06T07:16:08.713503: step 12685, loss 0.0204764, acc 1
2016-09-06T07:16:09.543625: step 12686, loss 0.0417975, acc 0.98
2016-09-06T07:16:10.325518: step 12687, loss 0.0421894, acc 0.98
2016-09-06T07:16:11.125688: step 12688, loss 0.00330425, acc 1
2016-09-06T07:16:11.937531: step 12689, loss 0.0548683, acc 0.98
2016-09-06T07:16:12.717395: step 12690, loss 0.00331758, acc 1
2016-09-06T07:16:13.507106: step 12691, loss 0.00367149, acc 1
2016-09-06T07:16:14.330239: step 12692, loss 0.00331705, acc 1
2016-09-06T07:16:15.101662: step 12693, loss 0.0155688, acc 1
2016-09-06T07:16:15.926243: step 12694, loss 0.0125437, acc 1
2016-09-06T07:16:16.762961: step 12695, loss 0.0226931, acc 0.98
2016-09-06T07:16:17.575957: step 12696, loss 0.0231021, acc 0.98
2016-09-06T07:16:18.410676: step 12697, loss 0.00354292, acc 1
2016-09-06T07:16:19.236094: step 12698, loss 0.0271968, acc 0.98
2016-09-06T07:16:20.057368: step 12699, loss 0.00557339, acc 1
2016-09-06T07:16:20.863359: step 12700, loss 0.00334009, acc 1

Evaluation:
2016-09-06T07:16:24.622959: step 12700, loss 3.40019, acc 0.726079

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-12700

2016-09-06T07:16:26.581453: step 12701, loss 0.0455085, acc 0.98
2016-09-06T07:16:27.380591: step 12702, loss 0.00334676, acc 1
2016-09-06T07:16:28.231563: step 12703, loss 0.00326207, acc 1
2016-09-06T07:16:29.139714: step 12704, loss 0.0126194, acc 1
2016-09-06T07:16:29.951951: step 12705, loss 0.0357446, acc 0.98
2016-09-06T07:16:30.745966: step 12706, loss 0.0175267, acc 1
2016-09-06T07:16:31.604256: step 12707, loss 0.00345411, acc 1
2016-09-06T07:16:32.426350: step 12708, loss 0.022849, acc 0.98
2016-09-06T07:16:33.245083: step 12709, loss 0.00315626, acc 1
2016-09-06T07:16:34.118326: step 12710, loss 0.0270863, acc 0.98
2016-09-06T07:16:34.963429: step 12711, loss 0.0262592, acc 0.98
2016-09-06T07:16:35.764435: step 12712, loss 0.00311379, acc 1
2016-09-06T07:16:36.564614: step 12713, loss 0.0377745, acc 0.98
2016-09-06T07:16:37.394178: step 12714, loss 0.00787678, acc 1
2016-09-06T07:16:38.198336: step 12715, loss 0.0713209, acc 0.96
2016-09-06T07:16:39.006358: step 12716, loss 0.0568177, acc 0.98
2016-09-06T07:16:39.831417: step 12717, loss 0.0188128, acc 0.98
2016-09-06T07:16:40.643026: step 12718, loss 0.00299791, acc 1
2016-09-06T07:16:41.470679: step 12719, loss 0.0306602, acc 0.98
2016-09-06T07:16:42.319163: step 12720, loss 0.00296626, acc 1
2016-09-06T07:16:43.123199: step 12721, loss 0.0761682, acc 0.98
2016-09-06T07:16:43.938803: step 12722, loss 0.00315534, acc 1
2016-09-06T07:16:44.767823: step 12723, loss 0.0052372, acc 1
2016-09-06T07:16:45.579705: step 12724, loss 0.0517678, acc 0.98
2016-09-06T07:16:46.390323: step 12725, loss 0.0656513, acc 0.98
2016-09-06T07:16:47.218284: step 12726, loss 0.00840602, acc 1
2016-09-06T07:16:48.021029: step 12727, loss 0.0200701, acc 0.98
2016-09-06T07:16:48.855319: step 12728, loss 0.0234284, acc 1
2016-09-06T07:16:49.691601: step 12729, loss 0.00912144, acc 1
2016-09-06T07:16:50.496804: step 12730, loss 0.0220644, acc 0.98
2016-09-06T07:16:51.322096: step 12731, loss 0.00345823, acc 1
2016-09-06T07:16:52.151834: step 12732, loss 0.00303364, acc 1
2016-09-06T07:16:52.977343: step 12733, loss 0.0160743, acc 1
2016-09-06T07:16:53.800203: step 12734, loss 0.00316994, acc 1
2016-09-06T07:16:54.653448: step 12735, loss 0.00382691, acc 1
2016-09-06T07:16:55.472879: step 12736, loss 0.0782595, acc 0.98
2016-09-06T07:16:56.271374: step 12737, loss 0.0582276, acc 0.96
2016-09-06T07:16:57.097794: step 12738, loss 0.0151905, acc 1
2016-09-06T07:16:57.916476: step 12739, loss 0.0152956, acc 1
2016-09-06T07:16:58.708900: step 12740, loss 0.0471124, acc 1
2016-09-06T07:16:59.492296: step 12741, loss 0.0127805, acc 1
2016-09-06T07:17:00.332608: step 12742, loss 0.0172021, acc 1
2016-09-06T07:17:01.121540: step 12743, loss 0.03403, acc 0.98
2016-09-06T07:17:01.946806: step 12744, loss 0.0415521, acc 0.98
2016-09-06T07:17:02.788123: step 12745, loss 0.126393, acc 0.96
2016-09-06T07:17:03.607197: step 12746, loss 0.00552797, acc 1
2016-09-06T07:17:04.424780: step 12747, loss 0.00702142, acc 1
2016-09-06T07:17:05.292707: step 12748, loss 0.0187327, acc 0.98
2016-09-06T07:17:06.114350: step 12749, loss 0.00714497, acc 1
2016-09-06T07:17:06.927909: step 12750, loss 0.00592959, acc 1
2016-09-06T07:17:07.758590: step 12751, loss 0.0287862, acc 1
2016-09-06T07:17:08.607511: step 12752, loss 0.0138, acc 1
2016-09-06T07:17:09.413789: step 12753, loss 0.00961743, acc 1
2016-09-06T07:17:10.224224: step 12754, loss 0.0228884, acc 1
2016-09-06T07:17:11.056101: step 12755, loss 0.0573767, acc 0.98
2016-09-06T07:17:11.872503: step 12756, loss 0.00311126, acc 1
2016-09-06T07:17:12.754887: step 12757, loss 0.0117771, acc 1
2016-09-06T07:17:13.571110: step 12758, loss 0.0409348, acc 0.98
2016-09-06T07:17:14.346736: step 12759, loss 0.00327428, acc 1
2016-09-06T07:17:15.168302: step 12760, loss 0.0144418, acc 1
2016-09-06T07:17:16.005223: step 12761, loss 0.0274309, acc 0.98
2016-09-06T07:17:16.829550: step 12762, loss 0.0150721, acc 1
2016-09-06T07:17:17.664232: step 12763, loss 0.0100896, acc 1
2016-09-06T07:17:18.467222: step 12764, loss 0.00319679, acc 1
2016-09-06T07:17:19.248946: step 12765, loss 0.0145323, acc 1
2016-09-06T07:17:20.024035: step 12766, loss 0.0304216, acc 0.98
2016-09-06T07:17:20.866190: step 12767, loss 0.0188676, acc 0.98
2016-09-06T07:17:21.683759: step 12768, loss 0.00329929, acc 1
2016-09-06T07:17:22.474530: step 12769, loss 0.00329328, acc 1
2016-09-06T07:17:23.282352: step 12770, loss 0.0210291, acc 1
2016-09-06T07:17:24.049708: step 12771, loss 0.0168146, acc 1
2016-09-06T07:17:24.875231: step 12772, loss 0.0180832, acc 1
2016-09-06T07:17:25.669891: step 12773, loss 0.0159506, acc 1
2016-09-06T07:17:26.458361: step 12774, loss 0.00335085, acc 1
2016-09-06T07:17:27.250594: step 12775, loss 0.00336612, acc 1
2016-09-06T07:17:28.076225: step 12776, loss 0.0169581, acc 1
2016-09-06T07:17:28.881761: step 12777, loss 0.0188357, acc 0.98
2016-09-06T07:17:29.696951: step 12778, loss 0.132664, acc 0.98
2016-09-06T07:17:30.515223: step 12779, loss 0.0468183, acc 0.98
2016-09-06T07:17:31.352157: step 12780, loss 0.0210004, acc 0.98
2016-09-06T07:17:32.166296: step 12781, loss 0.0257664, acc 0.98
2016-09-06T07:17:33.001437: step 12782, loss 0.00365593, acc 1
2016-09-06T07:17:33.797676: step 12783, loss 0.00323703, acc 1
2016-09-06T07:17:34.602968: step 12784, loss 0.0123905, acc 1
2016-09-06T07:17:35.419580: step 12785, loss 0.00494368, acc 1
2016-09-06T07:17:36.208555: step 12786, loss 0.00442265, acc 1
2016-09-06T07:17:37.015727: step 12787, loss 0.0176508, acc 1
2016-09-06T07:17:37.816014: step 12788, loss 0.00831954, acc 1
2016-09-06T07:17:38.610090: step 12789, loss 0.0194986, acc 0.98
2016-09-06T07:17:39.385842: step 12790, loss 0.00588416, acc 1
2016-09-06T07:17:40.222945: step 12791, loss 0.0595659, acc 0.98
2016-09-06T07:17:40.995951: step 12792, loss 0.0277098, acc 1
2016-09-06T07:17:41.829372: step 12793, loss 0.173432, acc 0.98
2016-09-06T07:17:42.631556: step 12794, loss 0.00347631, acc 1
2016-09-06T07:17:43.428967: step 12795, loss 0.0181632, acc 0.98
2016-09-06T07:17:44.240402: step 12796, loss 0.00547675, acc 1
2016-09-06T07:17:45.050466: step 12797, loss 0.0166589, acc 1
2016-09-06T07:17:45.814228: step 12798, loss 0.0255626, acc 1
2016-09-06T07:17:46.603206: step 12799, loss 0.0314965, acc 0.98
2016-09-06T07:17:47.422450: step 12800, loss 0.00842677, acc 1

Evaluation:
2016-09-06T07:17:51.134367: step 12800, loss 2.41005, acc 0.711069

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-12800

2016-09-06T07:17:52.974237: step 12801, loss 0.00417708, acc 1
2016-09-06T07:17:53.800732: step 12802, loss 0.0513097, acc 0.98
2016-09-06T07:17:54.611628: step 12803, loss 0.0277705, acc 0.98
2016-09-06T07:17:55.397509: step 12804, loss 0.00790451, acc 1
2016-09-06T07:17:56.266051: step 12805, loss 0.0203714, acc 0.98
2016-09-06T07:17:57.079891: step 12806, loss 0.0112882, acc 1
2016-09-06T07:17:57.890109: step 12807, loss 0.0164059, acc 1
2016-09-06T07:17:58.767415: step 12808, loss 0.0243791, acc 1
2016-09-06T07:17:59.604779: step 12809, loss 0.0240572, acc 0.98
2016-09-06T07:18:00.472801: step 12810, loss 0.0381144, acc 0.98
2016-09-06T07:18:01.278000: step 12811, loss 0.002682, acc 1
2016-09-06T07:18:02.116169: step 12812, loss 0.00426946, acc 1
2016-09-06T07:18:02.975981: step 12813, loss 0.0762261, acc 0.98
2016-09-06T07:18:03.804225: step 12814, loss 0.00386358, acc 1
2016-09-06T07:18:04.619530: step 12815, loss 0.00501544, acc 1
2016-09-06T07:18:05.437872: step 12816, loss 0.0421294, acc 0.98
2016-09-06T07:18:06.247797: step 12817, loss 0.0066541, acc 1
2016-09-06T07:18:07.068104: step 12818, loss 0.00583529, acc 1
2016-09-06T07:18:07.911064: step 12819, loss 0.036459, acc 0.96
2016-09-06T07:18:08.745944: step 12820, loss 0.0170754, acc 1
2016-09-06T07:18:09.568122: step 12821, loss 0.00405391, acc 1
2016-09-06T07:18:10.409556: step 12822, loss 0.0778493, acc 0.94
2016-09-06T07:18:11.198188: step 12823, loss 0.00339709, acc 1
2016-09-06T07:18:12.010974: step 12824, loss 0.00716917, acc 1
2016-09-06T07:18:12.826118: step 12825, loss 0.0361739, acc 0.98
2016-09-06T07:18:13.637108: step 12826, loss 0.0188752, acc 0.98
2016-09-06T07:18:14.444850: step 12827, loss 0.00309525, acc 1
2016-09-06T07:18:15.283048: step 12828, loss 0.0352505, acc 0.96
2016-09-06T07:18:16.057468: step 12829, loss 0.00714046, acc 1
2016-09-06T07:18:16.893086: step 12830, loss 0.0120822, acc 1
2016-09-06T07:18:17.710336: step 12831, loss 0.00253772, acc 1
2016-09-06T07:18:18.515986: step 12832, loss 0.0602444, acc 0.96
2016-09-06T07:18:19.343696: step 12833, loss 0.0146377, acc 1
2016-09-06T07:18:20.192066: step 12834, loss 0.129654, acc 0.98
2016-09-06T07:18:21.025429: step 12835, loss 0.0175458, acc 0.98
2016-09-06T07:18:21.835122: step 12836, loss 0.00235611, acc 1
2016-09-06T07:18:22.664159: step 12837, loss 0.00560217, acc 1
2016-09-06T07:18:23.461931: step 12838, loss 0.0188546, acc 1
2016-09-06T07:18:24.271620: step 12839, loss 0.0225516, acc 0.98
2016-09-06T07:18:25.085223: step 12840, loss 0.0163703, acc 0.98
2016-09-06T07:18:25.917636: step 12841, loss 0.0644967, acc 0.94
2016-09-06T07:18:26.728475: step 12842, loss 0.0258523, acc 0.98
2016-09-06T07:18:27.572138: step 12843, loss 0.0117077, acc 1
2016-09-06T07:18:28.400903: step 12844, loss 0.0212469, acc 0.98
2016-09-06T07:18:29.213362: step 12845, loss 0.00218507, acc 1
2016-09-06T07:18:30.050980: step 12846, loss 0.016887, acc 1
2016-09-06T07:18:30.868903: step 12847, loss 0.079149, acc 0.96
2016-09-06T07:18:31.680924: step 12848, loss 0.0022602, acc 1
2016-09-06T07:18:32.535967: step 12849, loss 0.00521616, acc 1
2016-09-06T07:18:33.338307: step 12850, loss 0.0292264, acc 0.98
2016-09-06T07:18:34.158833: step 12851, loss 0.00484332, acc 1
2016-09-06T07:18:34.984278: step 12852, loss 0.0513253, acc 0.98
2016-09-06T07:18:35.791619: step 12853, loss 0.00440896, acc 1
2016-09-06T07:18:36.593734: step 12854, loss 0.0109781, acc 1
2016-09-06T07:18:37.394555: step 12855, loss 0.00232536, acc 1
2016-09-06T07:18:38.213617: step 12856, loss 0.00270822, acc 1
2016-09-06T07:18:39.020969: step 12857, loss 0.0031407, acc 1
2016-09-06T07:18:39.822571: step 12858, loss 0.0162856, acc 1
2016-09-06T07:18:40.627020: step 12859, loss 0.0145218, acc 1
2016-09-06T07:18:41.434188: step 12860, loss 0.0153385, acc 1
2016-09-06T07:18:42.227086: step 12861, loss 0.0623675, acc 0.98
2016-09-06T07:18:43.022177: step 12862, loss 0.00358211, acc 1
2016-09-06T07:18:43.828858: step 12863, loss 0.0163298, acc 1
2016-09-06T07:18:44.592660: step 12864, loss 0.0024643, acc 1
2016-09-06T07:18:45.400791: step 12865, loss 0.00590154, acc 1
2016-09-06T07:18:46.237336: step 12866, loss 0.0660569, acc 0.98
2016-09-06T07:18:47.052712: step 12867, loss 0.0207579, acc 1
2016-09-06T07:18:47.901836: step 12868, loss 0.00248128, acc 1
2016-09-06T07:18:48.696078: step 12869, loss 0.0316243, acc 0.98
2016-09-06T07:18:49.517210: step 12870, loss 0.00923452, acc 1
2016-09-06T07:18:50.321138: step 12871, loss 0.105481, acc 0.96
2016-09-06T07:18:51.128971: step 12872, loss 0.00841043, acc 1
2016-09-06T07:18:51.968340: step 12873, loss 0.0530491, acc 0.96
2016-09-06T07:18:52.763664: step 12874, loss 0.00847662, acc 1
2016-09-06T07:18:53.549833: step 12875, loss 0.00276773, acc 1
2016-09-06T07:18:54.360338: step 12876, loss 0.00231472, acc 1
2016-09-06T07:18:55.188399: step 12877, loss 0.0270604, acc 0.98
2016-09-06T07:18:55.993367: step 12878, loss 0.0224178, acc 0.98
2016-09-06T07:18:56.790158: step 12879, loss 0.00909673, acc 1
2016-09-06T07:18:57.607424: step 12880, loss 0.00379491, acc 1
2016-09-06T07:18:58.368661: step 12881, loss 0.0154485, acc 1
2016-09-06T07:18:59.148414: step 12882, loss 0.0173785, acc 0.98
2016-09-06T07:18:59.959871: step 12883, loss 0.0175643, acc 0.98
2016-09-06T07:19:00.777784: step 12884, loss 0.0199261, acc 0.98
2016-09-06T07:19:01.598717: step 12885, loss 0.0374687, acc 0.98
2016-09-06T07:19:02.438334: step 12886, loss 0.0255782, acc 0.98
2016-09-06T07:19:03.221126: step 12887, loss 0.0562412, acc 0.96
2016-09-06T07:19:04.011986: step 12888, loss 0.0387081, acc 0.98
2016-09-06T07:19:04.825893: step 12889, loss 0.0110446, acc 1
2016-09-06T07:19:05.630772: step 12890, loss 0.206165, acc 0.96
2016-09-06T07:19:06.429010: step 12891, loss 0.0181456, acc 0.98
2016-09-06T07:19:07.270001: step 12892, loss 0.00604635, acc 1
2016-09-06T07:19:08.076800: step 12893, loss 0.028228, acc 1
2016-09-06T07:19:08.919036: step 12894, loss 0.0038301, acc 1
2016-09-06T07:19:09.752035: step 12895, loss 0.0216197, acc 0.98
2016-09-06T07:19:10.593806: step 12896, loss 0.0284567, acc 0.98
2016-09-06T07:19:11.393444: step 12897, loss 0.00884951, acc 1
2016-09-06T07:19:12.239808: step 12898, loss 0.0889862, acc 0.92
2016-09-06T07:19:13.040904: step 12899, loss 0.00448308, acc 1
2016-09-06T07:19:13.875310: step 12900, loss 0.016163, acc 1

Evaluation:
2016-09-06T07:19:17.656971: step 12900, loss 2.04731, acc 0.718574

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-12900

2016-09-06T07:19:19.659240: step 12901, loss 0.0102952, acc 1
2016-09-06T07:19:20.461087: step 12902, loss 0.0324939, acc 0.96
2016-09-06T07:19:21.287760: step 12903, loss 0.0175488, acc 0.98
2016-09-06T07:19:22.145256: step 12904, loss 0.0035148, acc 1
2016-09-06T07:19:22.954688: step 12905, loss 0.038201, acc 0.98
2016-09-06T07:19:23.783073: step 12906, loss 0.015832, acc 1
2016-09-06T07:19:24.609407: step 12907, loss 0.0353096, acc 0.98
2016-09-06T07:19:25.417323: step 12908, loss 0.00241728, acc 1
2016-09-06T07:19:26.229526: step 12909, loss 0.00728807, acc 1
2016-09-06T07:19:27.082940: step 12910, loss 0.00511592, acc 1
2016-09-06T07:19:27.892042: step 12911, loss 0.0375643, acc 0.96
2016-09-06T07:19:28.694663: step 12912, loss 0.0593479, acc 0.96
2016-09-06T07:19:29.501199: step 12913, loss 0.0173679, acc 1
2016-09-06T07:19:30.329503: step 12914, loss 0.00429953, acc 1
2016-09-06T07:19:31.117869: step 12915, loss 0.0172626, acc 0.98
2016-09-06T07:19:31.935555: step 12916, loss 0.0202016, acc 1
2016-09-06T07:19:32.755946: step 12917, loss 0.0211838, acc 1
2016-09-06T07:19:33.565115: step 12918, loss 0.0167856, acc 0.98
2016-09-06T07:19:34.358584: step 12919, loss 0.00430502, acc 1
2016-09-06T07:19:35.177466: step 12920, loss 0.0151619, acc 1
2016-09-06T07:19:36.001318: step 12921, loss 0.00252964, acc 1
2016-09-06T07:19:36.830844: step 12922, loss 0.0405837, acc 0.98
2016-09-06T07:19:37.644215: step 12923, loss 0.00254517, acc 1
2016-09-06T07:19:38.464580: step 12924, loss 0.00977341, acc 1
2016-09-06T07:19:39.260222: step 12925, loss 0.0329742, acc 0.98
2016-09-06T07:19:40.056081: step 12926, loss 0.00800456, acc 1
2016-09-06T07:19:40.837016: step 12927, loss 0.00249393, acc 1
2016-09-06T07:19:41.651373: step 12928, loss 0.0163441, acc 1
2016-09-06T07:19:42.483082: step 12929, loss 0.00368649, acc 1
2016-09-06T07:19:43.252345: step 12930, loss 0.0220198, acc 1
2016-09-06T07:19:44.067518: step 12931, loss 0.00252231, acc 1
2016-09-06T07:19:44.886720: step 12932, loss 0.0460288, acc 0.98
2016-09-06T07:19:45.674830: step 12933, loss 0.018793, acc 1
2016-09-06T07:19:46.481444: step 12934, loss 0.0446686, acc 0.98
2016-09-06T07:19:47.278924: step 12935, loss 0.0158543, acc 1
2016-09-06T07:19:48.053581: step 12936, loss 0.0569615, acc 0.98
2016-09-06T07:19:48.886491: step 12937, loss 0.0390697, acc 1
2016-09-06T07:19:49.713990: step 12938, loss 0.0102576, acc 1
2016-09-06T07:19:50.485093: step 12939, loss 0.0124018, acc 1
2016-09-06T07:19:51.307377: step 12940, loss 0.0141519, acc 1
2016-09-06T07:19:52.126481: step 12941, loss 0.0153566, acc 1
2016-09-06T07:19:52.913856: step 12942, loss 0.0033144, acc 1
2016-09-06T07:19:53.702078: step 12943, loss 0.0141865, acc 1
2016-09-06T07:19:54.508537: step 12944, loss 0.00577414, acc 1
2016-09-06T07:19:55.359807: step 12945, loss 0.00328067, acc 1
2016-09-06T07:19:56.167251: step 12946, loss 0.0341675, acc 0.98
2016-09-06T07:19:57.048235: step 12947, loss 0.0228546, acc 0.98
2016-09-06T07:19:57.841015: step 12948, loss 0.00236842, acc 1
2016-09-06T07:19:58.671534: step 12949, loss 0.0880644, acc 0.98
2016-09-06T07:19:59.484771: step 12950, loss 0.016547, acc 1
2016-09-06T07:20:00.296799: step 12951, loss 0.00505866, acc 1
2016-09-06T07:20:01.097620: step 12952, loss 0.00264556, acc 1
2016-09-06T07:20:01.938631: step 12953, loss 0.0166541, acc 0.98
2016-09-06T07:20:02.746360: step 12954, loss 0.00203113, acc 1
2016-09-06T07:20:03.561915: step 12955, loss 0.0308702, acc 0.98
2016-09-06T07:20:04.435316: step 12956, loss 0.0215735, acc 0.98
2016-09-06T07:20:05.250675: step 12957, loss 0.017184, acc 0.98
2016-09-06T07:20:06.059860: step 12958, loss 0.00597779, acc 1
2016-09-06T07:20:06.885872: step 12959, loss 0.00494057, acc 1
2016-09-06T07:20:07.709261: step 12960, loss 0.00263116, acc 1
2016-09-06T07:20:08.500912: step 12961, loss 0.0163266, acc 1
2016-09-06T07:20:09.330674: step 12962, loss 0.0510603, acc 0.94
2016-09-06T07:20:10.135479: step 12963, loss 0.00861155, acc 1
2016-09-06T07:20:10.937952: step 12964, loss 0.0350916, acc 1
2016-09-06T07:20:11.760318: step 12965, loss 0.00560657, acc 1
2016-09-06T07:20:12.564735: step 12966, loss 0.0219099, acc 0.98
2016-09-06T07:20:13.355257: step 12967, loss 0.00265631, acc 1
2016-09-06T07:20:14.180449: step 12968, loss 0.00191942, acc 1
2016-09-06T07:20:14.988729: step 12969, loss 0.0444129, acc 0.98
2016-09-06T07:20:15.784985: step 12970, loss 0.00230961, acc 1
2016-09-06T07:20:16.621783: step 12971, loss 0.0127331, acc 1
2016-09-06T07:20:17.462321: step 12972, loss 0.0266771, acc 0.98
2016-09-06T07:20:18.262519: step 12973, loss 0.0403747, acc 0.98
2016-09-06T07:20:19.077859: step 12974, loss 0.0155349, acc 1
2016-09-06T07:20:19.913648: step 12975, loss 0.00195372, acc 1
2016-09-06T07:20:20.676058: step 12976, loss 0.0248224, acc 1
2016-09-06T07:20:21.467282: step 12977, loss 0.00380334, acc 1
2016-09-06T07:20:22.304789: step 12978, loss 0.0160483, acc 1
2016-09-06T07:20:23.101275: step 12979, loss 0.0222197, acc 1
2016-09-06T07:20:23.905606: step 12980, loss 0.00250599, acc 1
2016-09-06T07:20:24.723444: step 12981, loss 0.0254288, acc 1
2016-09-06T07:20:25.502632: step 12982, loss 0.00197149, acc 1
2016-09-06T07:20:26.309606: step 12983, loss 0.0556641, acc 0.98
2016-09-06T07:20:27.135334: step 12984, loss 0.029199, acc 0.98
2016-09-06T07:20:27.917008: step 12985, loss 0.0020752, acc 1
2016-09-06T07:20:28.704248: step 12986, loss 0.0196316, acc 0.98
2016-09-06T07:20:29.518119: step 12987, loss 0.0023844, acc 1
2016-09-06T07:20:30.339696: step 12988, loss 0.0177024, acc 0.98
2016-09-06T07:20:31.142843: step 12989, loss 0.0118625, acc 1
2016-09-06T07:20:31.970320: step 12990, loss 0.00248059, acc 1
2016-09-06T07:20:32.771041: step 12991, loss 0.0387517, acc 0.96
2016-09-06T07:20:33.586757: step 12992, loss 0.0144267, acc 1
2016-09-06T07:20:34.423609: step 12993, loss 0.102939, acc 0.96
2016-09-06T07:20:35.224376: step 12994, loss 0.0160761, acc 0.98
2016-09-06T07:20:36.021680: step 12995, loss 0.0222107, acc 0.98
2016-09-06T07:20:36.843208: step 12996, loss 0.00397466, acc 1
2016-09-06T07:20:37.632447: step 12997, loss 0.0187958, acc 0.98
2016-09-06T07:20:38.415599: step 12998, loss 0.0429325, acc 0.96
2016-09-06T07:20:39.232531: step 12999, loss 0.00278866, acc 1
2016-09-06T07:20:40.014462: step 13000, loss 0.0184654, acc 1

Evaluation:
2016-09-06T07:20:43.776123: step 13000, loss 2.32608, acc 0.714822

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-13000

2016-09-06T07:20:45.644247: step 13001, loss 0.0131429, acc 1
2016-09-06T07:20:46.445080: step 13002, loss 0.0354562, acc 0.98
2016-09-06T07:20:47.248655: step 13003, loss 0.00360701, acc 1
2016-09-06T07:20:48.083556: step 13004, loss 0.00333719, acc 1
2016-09-06T07:20:48.878340: step 13005, loss 0.018711, acc 1
2016-09-06T07:20:49.704994: step 13006, loss 0.0120007, acc 1
2016-09-06T07:20:50.553508: step 13007, loss 0.0198821, acc 0.98
2016-09-06T07:20:51.386906: step 13008, loss 0.0302243, acc 1
2016-09-06T07:20:52.205437: step 13009, loss 0.00246224, acc 1
2016-09-06T07:20:53.020091: step 13010, loss 0.0226783, acc 1
2016-09-06T07:20:53.847479: step 13011, loss 0.0154886, acc 1
2016-09-06T07:20:54.650103: step 13012, loss 0.0328834, acc 0.98
2016-09-06T07:20:55.463956: step 13013, loss 0.010088, acc 1
2016-09-06T07:20:56.282778: step 13014, loss 0.0130068, acc 1
2016-09-06T07:20:57.101093: step 13015, loss 0.00970765, acc 1
2016-09-06T07:20:57.867030: step 13016, loss 0.0020739, acc 1
2016-09-06T07:20:58.672293: step 13017, loss 0.0182327, acc 0.98
2016-09-06T07:20:59.500257: step 13018, loss 0.013457, acc 1
2016-09-06T07:21:00.310164: step 13019, loss 0.00776979, acc 1
2016-09-06T07:21:01.130331: step 13020, loss 0.0262285, acc 0.98
2016-09-06T07:21:01.935158: step 13021, loss 0.0133066, acc 1
2016-09-06T07:21:02.696300: step 13022, loss 0.00237301, acc 1
2016-09-06T07:21:03.540621: step 13023, loss 0.069343, acc 0.98
2016-09-06T07:21:04.357773: step 13024, loss 0.0245358, acc 1
2016-09-06T07:21:05.141681: step 13025, loss 0.0503896, acc 0.96
2016-09-06T07:21:05.975647: step 13026, loss 0.0287812, acc 0.98
2016-09-06T07:21:06.774866: step 13027, loss 0.00715971, acc 1
2016-09-06T07:21:07.567656: step 13028, loss 0.0104399, acc 1
2016-09-06T07:21:08.365404: step 13029, loss 0.0370237, acc 0.96
2016-09-06T07:21:09.141933: step 13030, loss 0.0243117, acc 1
2016-09-06T07:21:09.953803: step 13031, loss 0.0157381, acc 1
2016-09-06T07:21:10.755784: step 13032, loss 0.0112374, acc 1
2016-09-06T07:21:11.539269: step 13033, loss 0.0468353, acc 0.98
2016-09-06T07:21:12.354600: step 13034, loss 0.00675568, acc 1
2016-09-06T07:21:13.146591: step 13035, loss 0.0158775, acc 1
2016-09-06T07:21:13.983214: step 13036, loss 0.00259672, acc 1
2016-09-06T07:21:14.765884: step 13037, loss 0.00570138, acc 1
2016-09-06T07:21:15.587727: step 13038, loss 0.016802, acc 1
2016-09-06T07:21:16.388453: step 13039, loss 0.0265571, acc 0.98
2016-09-06T07:21:17.197014: step 13040, loss 0.00203895, acc 1
2016-09-06T07:21:18.040802: step 13041, loss 0.0261055, acc 0.98
2016-09-06T07:21:18.816456: step 13042, loss 0.0420776, acc 0.98
2016-09-06T07:21:19.619157: step 13043, loss 0.0148187, acc 1
2016-09-06T07:21:20.428545: step 13044, loss 0.0206194, acc 0.98
2016-09-06T07:21:21.222647: step 13045, loss 0.0185839, acc 0.98
2016-09-06T07:21:22.021976: step 13046, loss 0.0149702, acc 1
2016-09-06T07:21:22.838217: step 13047, loss 0.00547775, acc 1
2016-09-06T07:21:23.631558: step 13048, loss 0.00196607, acc 1
2016-09-06T07:21:24.442537: step 13049, loss 0.0048186, acc 1
2016-09-06T07:21:25.259180: step 13050, loss 0.0415062, acc 0.96
2016-09-06T07:21:26.056787: step 13051, loss 0.00249024, acc 1
2016-09-06T07:21:26.869921: step 13052, loss 0.00190541, acc 1
2016-09-06T07:21:27.688617: step 13053, loss 0.0291082, acc 0.98
2016-09-06T07:21:28.462761: step 13054, loss 0.00901541, acc 1
2016-09-06T07:21:29.298012: step 13055, loss 0.0253034, acc 1
2016-09-06T07:21:30.053315: step 13056, loss 0.0195959, acc 0.977273
2016-09-06T07:21:30.857904: step 13057, loss 0.00275837, acc 1
2016-09-06T07:21:31.674681: step 13058, loss 0.0195598, acc 0.98
2016-09-06T07:21:32.522830: step 13059, loss 0.0231981, acc 1
2016-09-06T07:21:33.320802: step 13060, loss 0.00215393, acc 1
2016-09-06T07:21:34.137268: step 13061, loss 0.0172791, acc 1
2016-09-06T07:21:34.951355: step 13062, loss 0.00553689, acc 1
2016-09-06T07:21:35.741706: step 13063, loss 0.0529802, acc 0.96
2016-09-06T07:21:36.565381: step 13064, loss 0.021155, acc 0.98
2016-09-06T07:21:37.385619: step 13065, loss 0.019314, acc 0.98
2016-09-06T07:21:38.162831: step 13066, loss 0.00322852, acc 1
2016-09-06T07:21:38.976540: step 13067, loss 0.00211424, acc 1
2016-09-06T07:21:39.815160: step 13068, loss 0.00945976, acc 1
2016-09-06T07:21:40.613896: step 13069, loss 0.0199515, acc 1
2016-09-06T07:21:41.423469: step 13070, loss 0.0225248, acc 0.98
2016-09-06T07:21:42.251685: step 13071, loss 0.106282, acc 0.98
2016-09-06T07:21:43.046669: step 13072, loss 0.0127883, acc 1
2016-09-06T07:21:43.846077: step 13073, loss 0.0395875, acc 0.98
2016-09-06T07:21:44.673289: step 13074, loss 0.00522298, acc 1
2016-09-06T07:21:45.464488: step 13075, loss 0.0124008, acc 1
2016-09-06T07:21:46.259612: step 13076, loss 0.0016747, acc 1
2016-09-06T07:21:47.073569: step 13077, loss 0.0200231, acc 1
2016-09-06T07:21:47.852738: step 13078, loss 0.00982355, acc 1
2016-09-06T07:21:48.636092: step 13079, loss 0.00975502, acc 1
2016-09-06T07:21:49.469771: step 13080, loss 0.00172913, acc 1
2016-09-06T07:21:50.270027: step 13081, loss 0.0535465, acc 0.98
2016-09-06T07:21:51.081238: step 13082, loss 0.0360976, acc 0.98
2016-09-06T07:21:51.942746: step 13083, loss 0.0484724, acc 0.98
2016-09-06T07:21:52.775819: step 13084, loss 0.0160937, acc 1
2016-09-06T07:21:53.603407: step 13085, loss 0.0168059, acc 1
2016-09-06T07:21:54.422117: step 13086, loss 0.00185647, acc 1
2016-09-06T07:21:55.255325: step 13087, loss 0.0244968, acc 0.98
2016-09-06T07:21:56.076801: step 13088, loss 0.00249963, acc 1
2016-09-06T07:21:56.971169: step 13089, loss 0.0087925, acc 1
2016-09-06T07:21:57.839496: step 13090, loss 0.00393354, acc 1
2016-09-06T07:21:58.632193: step 13091, loss 0.00157899, acc 1
2016-09-06T07:21:59.446815: step 13092, loss 0.0131376, acc 1
2016-09-06T07:22:00.258082: step 13093, loss 0.0016119, acc 1
2016-09-06T07:22:01.063451: step 13094, loss 0.0335308, acc 0.96
2016-09-06T07:22:01.876701: step 13095, loss 0.0447739, acc 0.98
2016-09-06T07:22:02.708979: step 13096, loss 0.00236492, acc 1
2016-09-06T07:22:03.544055: step 13097, loss 0.00968088, acc 1
2016-09-06T07:22:04.378303: step 13098, loss 0.00191953, acc 1
2016-09-06T07:22:05.186888: step 13099, loss 0.00728386, acc 1
2016-09-06T07:22:05.961153: step 13100, loss 0.00184045, acc 1

Evaluation:
2016-09-06T07:22:09.696626: step 13100, loss 2.73186, acc 0.713884

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-13100

2016-09-06T07:22:11.565408: step 13101, loss 0.00182625, acc 1
2016-09-06T07:22:12.380681: step 13102, loss 0.00784398, acc 1
2016-09-06T07:22:13.182736: step 13103, loss 0.0152176, acc 1
2016-09-06T07:22:14.004468: step 13104, loss 0.0106423, acc 1
2016-09-06T07:22:14.835470: step 13105, loss 0.0031269, acc 1
2016-09-06T07:22:15.676624: step 13106, loss 0.00437637, acc 1
2016-09-06T07:22:16.496675: step 13107, loss 0.00999607, acc 1
2016-09-06T07:22:17.339813: step 13108, loss 0.0215325, acc 0.98
2016-09-06T07:22:18.121330: step 13109, loss 0.0165279, acc 1
2016-09-06T07:22:18.929333: step 13110, loss 0.002683, acc 1
2016-09-06T07:22:19.741101: step 13111, loss 0.00211655, acc 1
2016-09-06T07:22:20.514207: step 13112, loss 0.00609111, acc 1
2016-09-06T07:22:21.354231: step 13113, loss 0.00701333, acc 1
2016-09-06T07:22:22.178170: step 13114, loss 0.0135689, acc 1
2016-09-06T07:22:22.970864: step 13115, loss 0.00180834, acc 1
2016-09-06T07:22:23.803442: step 13116, loss 0.00186578, acc 1
2016-09-06T07:22:24.625087: step 13117, loss 0.0158572, acc 1
2016-09-06T07:22:25.430385: step 13118, loss 0.00227563, acc 1
2016-09-06T07:22:26.239606: step 13119, loss 0.00220131, acc 1
2016-09-06T07:22:27.037946: step 13120, loss 0.0198317, acc 0.98
2016-09-06T07:22:27.816390: step 13121, loss 0.00276368, acc 1
2016-09-06T07:22:28.611019: step 13122, loss 0.0355268, acc 0.98
2016-09-06T07:22:29.463232: step 13123, loss 0.0285258, acc 0.98
2016-09-06T07:22:30.249603: step 13124, loss 0.0806761, acc 0.98
2016-09-06T07:22:31.033317: step 13125, loss 0.0266243, acc 1
2016-09-06T07:22:31.846247: step 13126, loss 0.00202125, acc 1
2016-09-06T07:22:32.635792: step 13127, loss 0.0158871, acc 0.98
2016-09-06T07:22:33.441135: step 13128, loss 0.00286468, acc 1
2016-09-06T07:22:34.277754: step 13129, loss 0.0130821, acc 1
2016-09-06T07:22:35.054566: step 13130, loss 0.00221652, acc 1
2016-09-06T07:22:35.856062: step 13131, loss 0.00194173, acc 1
2016-09-06T07:22:36.710025: step 13132, loss 0.0161851, acc 0.98
2016-09-06T07:22:37.513577: step 13133, loss 0.0681702, acc 0.96
2016-09-06T07:22:38.305350: step 13134, loss 0.00677557, acc 1
2016-09-06T07:22:39.109661: step 13135, loss 0.0146085, acc 1
2016-09-06T07:22:39.876740: step 13136, loss 0.00649865, acc 1
2016-09-06T07:22:40.674501: step 13137, loss 0.00185645, acc 1
2016-09-06T07:22:41.519801: step 13138, loss 0.0177694, acc 1
2016-09-06T07:22:42.313566: step 13139, loss 0.0201155, acc 0.98
2016-09-06T07:22:43.094649: step 13140, loss 0.00185788, acc 1
2016-09-06T07:22:43.899326: step 13141, loss 0.00435694, acc 1
2016-09-06T07:22:44.707858: step 13142, loss 0.0301211, acc 0.98
2016-09-06T07:22:45.498505: step 13143, loss 0.00362967, acc 1
2016-09-06T07:22:46.310956: step 13144, loss 0.0203979, acc 1
2016-09-06T07:22:47.125741: step 13145, loss 0.00202763, acc 1
2016-09-06T07:22:47.938649: step 13146, loss 0.00187091, acc 1
2016-09-06T07:22:48.749977: step 13147, loss 0.00930776, acc 1
2016-09-06T07:22:49.539201: step 13148, loss 0.00188273, acc 1
2016-09-06T07:22:50.345826: step 13149, loss 0.107585, acc 0.98
2016-09-06T07:22:51.150705: step 13150, loss 0.00212183, acc 1
2016-09-06T07:22:51.960327: step 13151, loss 0.0119049, acc 1
2016-09-06T07:22:52.759623: step 13152, loss 0.0178376, acc 1
2016-09-06T07:22:53.589192: step 13153, loss 0.00286785, acc 1
2016-09-06T07:22:54.354924: step 13154, loss 0.0284917, acc 0.98
2016-09-06T07:22:55.148718: step 13155, loss 0.0068587, acc 1
2016-09-06T07:22:55.976354: step 13156, loss 0.00581459, acc 1
2016-09-06T07:22:56.765458: step 13157, loss 0.0128114, acc 1
2016-09-06T07:22:57.586426: step 13158, loss 0.0106121, acc 1
2016-09-06T07:22:58.408126: step 13159, loss 0.00414959, acc 1
2016-09-06T07:22:59.232945: step 13160, loss 0.0134701, acc 1
2016-09-06T07:23:00.027878: step 13161, loss 0.0268508, acc 0.98
2016-09-06T07:23:00.886790: step 13162, loss 0.00535978, acc 1
2016-09-06T07:23:01.683344: step 13163, loss 0.0688315, acc 0.96
2016-09-06T07:23:02.506069: step 13164, loss 0.0344537, acc 0.98
2016-09-06T07:23:03.338856: step 13165, loss 0.0392443, acc 0.98
2016-09-06T07:23:04.150766: step 13166, loss 0.098023, acc 0.96
2016-09-06T07:23:04.958615: step 13167, loss 0.0413211, acc 0.98
2016-09-06T07:23:05.789516: step 13168, loss 0.0175168, acc 0.98
2016-09-06T07:23:06.615532: step 13169, loss 0.0213879, acc 0.98
2016-09-06T07:23:07.427358: step 13170, loss 0.00254604, acc 1
2016-09-06T07:23:08.239383: step 13171, loss 0.00780412, acc 1
2016-09-06T07:23:09.059069: step 13172, loss 0.0127596, acc 1
2016-09-06T07:23:09.860699: step 13173, loss 0.00301261, acc 1
2016-09-06T07:23:10.687003: step 13174, loss 0.00237217, acc 1
2016-09-06T07:23:11.530201: step 13175, loss 0.0205568, acc 0.98
2016-09-06T07:23:12.334014: step 13176, loss 0.00974129, acc 1
2016-09-06T07:23:13.169270: step 13177, loss 0.00248026, acc 1
2016-09-06T07:23:13.993143: step 13178, loss 0.0028477, acc 1
2016-09-06T07:23:14.817384: step 13179, loss 0.0118358, acc 1
2016-09-06T07:23:15.643330: step 13180, loss 0.0657559, acc 0.96
2016-09-06T07:23:16.434243: step 13181, loss 0.0243995, acc 0.98
2016-09-06T07:23:17.251102: step 13182, loss 0.016434, acc 1
2016-09-06T07:23:18.099401: step 13183, loss 0.021946, acc 1
2016-09-06T07:23:18.886268: step 13184, loss 0.0459604, acc 0.96
2016-09-06T07:23:19.699293: step 13185, loss 0.00315433, acc 1
2016-09-06T07:23:20.516715: step 13186, loss 0.00498606, acc 1
2016-09-06T07:23:21.343752: step 13187, loss 0.0131621, acc 1
2016-09-06T07:23:22.163915: step 13188, loss 0.0174139, acc 0.98
2016-09-06T07:23:22.981668: step 13189, loss 0.00568896, acc 1
2016-09-06T07:23:23.821900: step 13190, loss 0.0899969, acc 0.96
2016-09-06T07:23:24.612857: step 13191, loss 0.00640912, acc 1
2016-09-06T07:23:25.399367: step 13192, loss 0.0221592, acc 0.98
2016-09-06T07:23:26.207455: step 13193, loss 0.00256487, acc 1
2016-09-06T07:23:26.966268: step 13194, loss 0.0156195, acc 1
2016-09-06T07:23:27.779685: step 13195, loss 0.00261853, acc 1
2016-09-06T07:23:28.581209: step 13196, loss 0.0025603, acc 1
2016-09-06T07:23:29.411824: step 13197, loss 0.0546903, acc 0.98
2016-09-06T07:23:30.207685: step 13198, loss 0.00256716, acc 1
2016-09-06T07:23:31.055332: step 13199, loss 0.0143715, acc 1
2016-09-06T07:23:31.843676: step 13200, loss 0.00339628, acc 1

Evaluation:
2016-09-06T07:23:35.599770: step 13200, loss 2.74792, acc 0.71576

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-13200

2016-09-06T07:23:37.436808: step 13201, loss 0.0209358, acc 0.98
2016-09-06T07:23:38.265763: step 13202, loss 0.0309187, acc 0.98
2016-09-06T07:23:39.054276: step 13203, loss 0.0152584, acc 1
2016-09-06T07:23:39.881222: step 13204, loss 0.00567585, acc 1
2016-09-06T07:23:40.672467: step 13205, loss 0.0103941, acc 1
2016-09-06T07:23:41.493449: step 13206, loss 0.014851, acc 1
2016-09-06T07:23:42.350486: step 13207, loss 0.00249193, acc 1
2016-09-06T07:23:43.172889: step 13208, loss 0.00249275, acc 1
2016-09-06T07:23:44.011700: step 13209, loss 0.0078309, acc 1
2016-09-06T07:23:44.827215: step 13210, loss 0.0270003, acc 0.98
2016-09-06T07:23:45.652922: step 13211, loss 0.00396024, acc 1
2016-09-06T07:23:46.448731: step 13212, loss 0.0200803, acc 1
2016-09-06T07:23:47.243144: step 13213, loss 0.0180044, acc 0.98
2016-09-06T07:23:48.071921: step 13214, loss 0.00253466, acc 1
2016-09-06T07:23:48.872394: step 13215, loss 0.00536015, acc 1
2016-09-06T07:23:49.679923: step 13216, loss 0.0287319, acc 1
2016-09-06T07:23:50.499331: step 13217, loss 0.0116082, acc 1
2016-09-06T07:23:51.299820: step 13218, loss 0.0953797, acc 0.96
2016-09-06T07:23:52.083023: step 13219, loss 0.0548963, acc 0.98
2016-09-06T07:23:52.935089: step 13220, loss 0.00347746, acc 1
2016-09-06T07:23:53.744192: step 13221, loss 0.00245717, acc 1
2016-09-06T07:23:54.544847: step 13222, loss 0.0186608, acc 0.98
2016-09-06T07:23:55.393216: step 13223, loss 0.0156226, acc 1
2016-09-06T07:23:56.184715: step 13224, loss 0.00666236, acc 1
2016-09-06T07:23:56.958922: step 13225, loss 0.0224918, acc 1
2016-09-06T07:23:57.806700: step 13226, loss 0.00221327, acc 1
2016-09-06T07:23:58.597540: step 13227, loss 0.0188527, acc 1
2016-09-06T07:23:59.403262: step 13228, loss 0.00949311, acc 1
2016-09-06T07:24:00.218088: step 13229, loss 0.0119557, acc 1
2016-09-06T07:24:01.034839: step 13230, loss 0.0451408, acc 0.96
2016-09-06T07:24:01.821476: step 13231, loss 0.0300807, acc 0.98
2016-09-06T07:24:02.622897: step 13232, loss 0.00364228, acc 1
2016-09-06T07:24:03.384928: step 13233, loss 0.0253335, acc 0.98
2016-09-06T07:24:04.196207: step 13234, loss 0.0103979, acc 1
2016-09-06T07:24:05.012325: step 13235, loss 0.016571, acc 0.98
2016-09-06T07:24:05.799446: step 13236, loss 0.0122954, acc 1
2016-09-06T07:24:06.611432: step 13237, loss 0.00875936, acc 1
2016-09-06T07:24:07.457423: step 13238, loss 0.0307321, acc 0.98
2016-09-06T07:24:08.244914: step 13239, loss 0.00233756, acc 1
2016-09-06T07:24:09.049895: step 13240, loss 0.0031276, acc 1
2016-09-06T07:24:09.890917: step 13241, loss 0.00235486, acc 1
2016-09-06T07:24:10.677406: step 13242, loss 0.0152199, acc 1
2016-09-06T07:24:11.451394: step 13243, loss 0.0691341, acc 0.96
2016-09-06T07:24:12.277711: step 13244, loss 0.00244746, acc 1
2016-09-06T07:24:13.063958: step 13245, loss 0.00593269, acc 1
2016-09-06T07:24:13.885626: step 13246, loss 0.0607871, acc 0.98
2016-09-06T07:24:14.715701: step 13247, loss 0.00539869, acc 1
2016-09-06T07:24:15.454508: step 13248, loss 0.00243719, acc 1
2016-09-06T07:24:16.281624: step 13249, loss 0.0198719, acc 0.98
2016-09-06T07:24:17.089811: step 13250, loss 0.0272934, acc 0.98
2016-09-06T07:24:17.892287: step 13251, loss 0.00760928, acc 1
2016-09-06T07:24:18.701227: step 13252, loss 0.00330199, acc 1
2016-09-06T07:24:19.521038: step 13253, loss 0.0909898, acc 0.94
2016-09-06T07:24:20.326555: step 13254, loss 0.0182376, acc 1
2016-09-06T07:24:21.127557: step 13255, loss 0.0112964, acc 1
2016-09-06T07:24:21.960934: step 13256, loss 0.00866509, acc 1
2016-09-06T07:24:22.731193: step 13257, loss 0.23097, acc 0.96
2016-09-06T07:24:23.549216: step 13258, loss 0.0531216, acc 0.96
2016-09-06T07:24:24.360259: step 13259, loss 0.0017153, acc 1
2016-09-06T07:24:25.157805: step 13260, loss 0.00255898, acc 1
2016-09-06T07:24:25.966633: step 13261, loss 0.021519, acc 1
2016-09-06T07:24:26.780372: step 13262, loss 0.0254195, acc 0.98
2016-09-06T07:24:27.581822: step 13263, loss 0.0379643, acc 0.98
2016-09-06T07:24:28.382645: step 13264, loss 0.0157086, acc 1
2016-09-06T07:24:29.214600: step 13265, loss 0.013477, acc 1
2016-09-06T07:24:30.018026: step 13266, loss 0.00234683, acc 1
2016-09-06T07:24:30.815894: step 13267, loss 0.00346337, acc 1
2016-09-06T07:24:31.662991: step 13268, loss 0.00698371, acc 1
2016-09-06T07:24:32.462545: step 13269, loss 0.0022524, acc 1
2016-09-06T07:24:33.245836: step 13270, loss 0.00369207, acc 1
2016-09-06T07:24:34.089258: step 13271, loss 0.0110403, acc 1
2016-09-06T07:24:34.887085: step 13272, loss 0.00756359, acc 1
2016-09-06T07:24:35.708411: step 13273, loss 0.00630657, acc 1
2016-09-06T07:24:36.526291: step 13274, loss 0.00455569, acc 1
2016-09-06T07:24:37.332326: step 13275, loss 0.0103157, acc 1
2016-09-06T07:24:38.154759: step 13276, loss 0.0038398, acc 1
2016-09-06T07:24:39.011569: step 13277, loss 0.0699924, acc 0.96
2016-09-06T07:24:39.830360: step 13278, loss 0.0151279, acc 1
2016-09-06T07:24:40.719889: step 13279, loss 0.0203927, acc 0.98
2016-09-06T07:24:41.564854: step 13280, loss 0.00615526, acc 1
2016-09-06T07:24:42.360509: step 13281, loss 0.0345935, acc 0.98
2016-09-06T07:24:43.167281: step 13282, loss 0.0268519, acc 0.98
2016-09-06T07:24:43.996961: step 13283, loss 0.00535468, acc 1
2016-09-06T07:24:44.832012: step 13284, loss 0.0196354, acc 0.98
2016-09-06T07:24:45.647097: step 13285, loss 0.00314213, acc 1
2016-09-06T07:24:46.480991: step 13286, loss 0.0520569, acc 0.98
2016-09-06T07:24:47.326717: step 13287, loss 0.0147396, acc 1
2016-09-06T07:24:48.137427: step 13288, loss 0.00264599, acc 1
2016-09-06T07:24:48.950657: step 13289, loss 0.070429, acc 0.98
2016-09-06T07:24:49.756439: step 13290, loss 0.0217721, acc 1
2016-09-06T07:24:50.547092: step 13291, loss 0.0283558, acc 1
2016-09-06T07:24:51.362099: step 13292, loss 0.00263903, acc 1
2016-09-06T07:24:52.194266: step 13293, loss 0.00326401, acc 1
2016-09-06T07:24:52.978346: step 13294, loss 0.0212933, acc 1
2016-09-06T07:24:53.789078: step 13295, loss 0.0101647, acc 1
2016-09-06T07:24:54.616592: step 13296, loss 0.0541645, acc 0.98
2016-09-06T07:24:55.417647: step 13297, loss 0.0473327, acc 0.98
2016-09-06T07:24:56.194572: step 13298, loss 0.00248645, acc 1
2016-09-06T07:24:57.008136: step 13299, loss 0.0122538, acc 1
2016-09-06T07:24:57.821383: step 13300, loss 0.00587681, acc 1

Evaluation:
2016-09-06T07:25:01.554013: step 13300, loss 2.71096, acc 0.718574

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-13300

2016-09-06T07:25:03.554592: step 13301, loss 0.0210342, acc 0.98
2016-09-06T07:25:04.401405: step 13302, loss 0.013076, acc 1
2016-09-06T07:25:05.176742: step 13303, loss 0.0043533, acc 1
2016-09-06T07:25:05.996192: step 13304, loss 0.002693, acc 1
2016-09-06T07:25:06.825707: step 13305, loss 0.0116869, acc 1
2016-09-06T07:25:07.627551: step 13306, loss 0.0156981, acc 1
2016-09-06T07:25:08.448674: step 13307, loss 0.00707234, acc 1
2016-09-06T07:25:09.269508: step 13308, loss 0.00238835, acc 1
2016-09-06T07:25:10.071539: step 13309, loss 0.0525863, acc 0.98
2016-09-06T07:25:10.906813: step 13310, loss 0.00850779, acc 1
2016-09-06T07:25:11.734756: step 13311, loss 0.00261635, acc 1
2016-09-06T07:25:12.566066: step 13312, loss 0.0336604, acc 1
2016-09-06T07:25:13.378891: step 13313, loss 0.0113443, acc 1
2016-09-06T07:25:14.212909: step 13314, loss 0.00240653, acc 1
2016-09-06T07:25:15.026729: step 13315, loss 0.0170682, acc 1
2016-09-06T07:25:15.842992: step 13316, loss 0.0312781, acc 0.98
2016-09-06T07:25:16.667740: step 13317, loss 0.0485013, acc 0.96
2016-09-06T07:25:17.476467: step 13318, loss 0.0265545, acc 1
2016-09-06T07:25:18.287296: step 13319, loss 0.0223136, acc 0.98
2016-09-06T07:25:19.120040: step 13320, loss 0.00255142, acc 1
2016-09-06T07:25:19.902611: step 13321, loss 0.0336109, acc 0.98
2016-09-06T07:25:20.735404: step 13322, loss 0.00237379, acc 1
2016-09-06T07:25:21.550994: step 13323, loss 0.00237775, acc 1
2016-09-06T07:25:22.348030: step 13324, loss 0.0290193, acc 1
2016-09-06T07:25:23.150323: step 13325, loss 0.00237457, acc 1
2016-09-06T07:25:23.982557: step 13326, loss 0.0112913, acc 1
2016-09-06T07:25:24.828933: step 13327, loss 0.00894333, acc 1
2016-09-06T07:25:25.659772: step 13328, loss 0.00233329, acc 1
2016-09-06T07:25:26.459382: step 13329, loss 0.0111711, acc 1
2016-09-06T07:25:27.253406: step 13330, loss 0.00815302, acc 1
2016-09-06T07:25:28.029231: step 13331, loss 0.111824, acc 0.94
2016-09-06T07:25:28.851813: step 13332, loss 0.0024599, acc 1
2016-09-06T07:25:29.688128: step 13333, loss 0.0189259, acc 0.98
2016-09-06T07:25:30.459606: step 13334, loss 0.0161245, acc 1
2016-09-06T07:25:31.267225: step 13335, loss 0.010204, acc 1
2016-09-06T07:25:32.110349: step 13336, loss 0.0411459, acc 0.98
2016-09-06T07:25:32.907396: step 13337, loss 0.0163769, acc 0.98
2016-09-06T07:25:33.689862: step 13338, loss 0.00305449, acc 1
2016-09-06T07:25:34.519559: step 13339, loss 0.0165119, acc 0.98
2016-09-06T07:25:35.297409: step 13340, loss 0.00218119, acc 1
2016-09-06T07:25:36.133472: step 13341, loss 0.0244059, acc 0.98
2016-09-06T07:25:36.953669: step 13342, loss 0.00210391, acc 1
2016-09-06T07:25:37.746291: step 13343, loss 0.037215, acc 0.96
2016-09-06T07:25:38.548295: step 13344, loss 0.00405801, acc 1
2016-09-06T07:25:39.347133: step 13345, loss 0.0136781, acc 1
2016-09-06T07:25:40.115821: step 13346, loss 0.0114013, acc 1
2016-09-06T07:25:40.927667: step 13347, loss 0.00309175, acc 1
2016-09-06T07:25:41.786595: step 13348, loss 0.0197023, acc 0.98
2016-09-06T07:25:42.571912: step 13349, loss 0.021829, acc 0.98
2016-09-06T07:25:43.359030: step 13350, loss 0.00328335, acc 1
2016-09-06T07:25:44.185216: step 13351, loss 0.00378798, acc 1
2016-09-06T07:25:44.980755: step 13352, loss 0.00335088, acc 1
2016-09-06T07:25:45.810908: step 13353, loss 0.0168676, acc 0.98
2016-09-06T07:25:46.763285: step 13354, loss 0.00351008, acc 1
2016-09-06T07:25:47.611977: step 13355, loss 0.0388157, acc 0.96
2016-09-06T07:25:48.697233: step 13356, loss 0.0299387, acc 0.98
2016-09-06T07:25:49.614935: step 13357, loss 0.195094, acc 0.94
2016-09-06T07:25:50.534626: step 13358, loss 0.0351228, acc 0.98
2016-09-06T07:25:51.554085: step 13359, loss 0.00311943, acc 1
2016-09-06T07:25:52.477553: step 13360, loss 0.0229739, acc 1
2016-09-06T07:25:53.482790: step 13361, loss 0.0335428, acc 0.98
2016-09-06T07:25:54.462180: step 13362, loss 0.00319565, acc 1
2016-09-06T07:25:55.578552: step 13363, loss 0.0247192, acc 0.98
2016-09-06T07:25:56.493585: step 13364, loss 0.0163092, acc 1
2016-09-06T07:25:57.395137: step 13365, loss 0.114127, acc 0.98
2016-09-06T07:25:58.204082: step 13366, loss 0.00338056, acc 1
2016-09-06T07:25:59.070442: step 13367, loss 0.00526267, acc 1
2016-09-06T07:25:59.955796: step 13368, loss 0.00499334, acc 1
2016-09-06T07:26:00.780231: step 13369, loss 0.00288857, acc 1
2016-09-06T07:26:01.613740: step 13370, loss 0.00292707, acc 1
2016-09-06T07:26:02.440623: step 13371, loss 0.0169978, acc 0.98
2016-09-06T07:26:03.233016: step 13372, loss 0.100463, acc 0.96
2016-09-06T07:26:04.012117: step 13373, loss 0.0206583, acc 0.98
2016-09-06T07:26:04.845190: step 13374, loss 0.00533806, acc 1
2016-09-06T07:26:05.607792: step 13375, loss 0.0559822, acc 0.96
2016-09-06T07:26:06.422935: step 13376, loss 0.00641884, acc 1
2016-09-06T07:26:07.239151: step 13377, loss 0.0392881, acc 0.98
2016-09-06T07:26:07.997468: step 13378, loss 0.0194763, acc 0.98
2016-09-06T07:26:08.839218: step 13379, loss 0.0246186, acc 1
2016-09-06T07:26:09.678765: step 13380, loss 0.0065024, acc 1
2016-09-06T07:26:10.476812: step 13381, loss 0.00862066, acc 1
2016-09-06T07:26:11.286587: step 13382, loss 0.00757179, acc 1
2016-09-06T07:26:12.095375: step 13383, loss 0.0359833, acc 0.98
2016-09-06T07:26:12.880961: step 13384, loss 0.00356016, acc 1
2016-09-06T07:26:13.681409: step 13385, loss 0.0172173, acc 1
2016-09-06T07:26:14.521450: step 13386, loss 0.0530161, acc 0.96
2016-09-06T07:26:15.333086: step 13387, loss 0.00506396, acc 1
2016-09-06T07:26:16.142207: step 13388, loss 0.00317818, acc 1
2016-09-06T07:26:16.968089: step 13389, loss 0.0438275, acc 0.98
2016-09-06T07:26:17.761729: step 13390, loss 0.0203695, acc 1
2016-09-06T07:26:18.564640: step 13391, loss 0.00342519, acc 1
2016-09-06T07:26:19.379287: step 13392, loss 0.0369208, acc 1
2016-09-06T07:26:20.163805: step 13393, loss 0.00510883, acc 1
2016-09-06T07:26:20.964095: step 13394, loss 0.0177072, acc 1
2016-09-06T07:26:21.831730: step 13395, loss 0.0035268, acc 1
2016-09-06T07:26:22.649400: step 13396, loss 0.0772285, acc 0.98
2016-09-06T07:26:23.464572: step 13397, loss 0.0239971, acc 0.98
2016-09-06T07:26:24.332585: step 13398, loss 0.00482778, acc 1
2016-09-06T07:26:25.187507: step 13399, loss 0.00384389, acc 1
2016-09-06T07:26:25.983477: step 13400, loss 0.00326061, acc 1

Evaluation:
2016-09-06T07:26:29.744051: step 13400, loss 3.25318, acc 0.712946

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-13400

2016-09-06T07:26:31.753064: step 13401, loss 0.00634977, acc 1
2016-09-06T07:26:32.567413: step 13402, loss 0.00321552, acc 1
2016-09-06T07:26:33.381552: step 13403, loss 0.00321499, acc 1
2016-09-06T07:26:34.208484: step 13404, loss 0.0145421, acc 1
2016-09-06T07:26:35.045146: step 13405, loss 0.0260173, acc 0.98
2016-09-06T07:26:35.840173: step 13406, loss 0.329776, acc 0.96
2016-09-06T07:26:36.702987: step 13407, loss 0.00365539, acc 1
2016-09-06T07:26:37.532412: step 13408, loss 0.171435, acc 0.94
2016-09-06T07:26:38.326714: step 13409, loss 0.0292725, acc 1
2016-09-06T07:26:39.145753: step 13410, loss 0.01284, acc 1
2016-09-06T07:26:39.995657: step 13411, loss 0.00344708, acc 1
2016-09-06T07:26:40.781284: step 13412, loss 0.0303233, acc 0.98
2016-09-06T07:26:41.586150: step 13413, loss 0.0113952, acc 1
2016-09-06T07:26:42.397535: step 13414, loss 0.0290565, acc 0.98
2016-09-06T07:26:43.187718: step 13415, loss 0.00964669, acc 1
2016-09-06T07:26:44.004849: step 13416, loss 0.0159277, acc 1
2016-09-06T07:26:44.832467: step 13417, loss 0.0479863, acc 0.98
2016-09-06T07:26:45.617543: step 13418, loss 0.026963, acc 1
2016-09-06T07:26:46.413656: step 13419, loss 0.0158793, acc 1
2016-09-06T07:26:47.207721: step 13420, loss 0.0182301, acc 0.98
2016-09-06T07:26:48.016127: step 13421, loss 0.0159239, acc 1
2016-09-06T07:26:48.822692: step 13422, loss 0.0374088, acc 0.98
2016-09-06T07:26:49.653501: step 13423, loss 0.0638265, acc 0.98
2016-09-06T07:26:50.444484: step 13424, loss 0.00351577, acc 1
2016-09-06T07:26:51.256445: step 13425, loss 0.00393073, acc 1
2016-09-06T07:26:52.080586: step 13426, loss 0.0169356, acc 1
2016-09-06T07:26:52.858628: step 13427, loss 0.0399589, acc 0.98
2016-09-06T07:26:53.694016: step 13428, loss 0.0285864, acc 0.98
2016-09-06T07:26:54.502898: step 13429, loss 0.0221125, acc 0.98
2016-09-06T07:26:55.305121: step 13430, loss 0.00991088, acc 1
2016-09-06T07:26:56.123860: step 13431, loss 0.0247554, acc 0.98
2016-09-06T07:26:57.010607: step 13432, loss 0.0159737, acc 1
2016-09-06T07:26:57.800711: step 13433, loss 0.00413781, acc 1
2016-09-06T07:26:58.604128: step 13434, loss 0.0169852, acc 1
2016-09-06T07:26:59.435864: step 13435, loss 0.0402968, acc 0.98
2016-09-06T07:27:00.250596: step 13436, loss 0.00434843, acc 1
2016-09-06T07:27:01.049197: step 13437, loss 0.0386112, acc 0.98
2016-09-06T07:27:01.891380: step 13438, loss 0.00473453, acc 1
2016-09-06T07:27:02.720095: step 13439, loss 0.0313038, acc 0.98
2016-09-06T07:27:03.494113: step 13440, loss 0.00391301, acc 1
2016-09-06T07:27:04.319012: step 13441, loss 0.0139145, acc 1
2016-09-06T07:27:05.137084: step 13442, loss 0.00583517, acc 1
2016-09-06T07:27:05.988588: step 13443, loss 0.00420758, acc 1
2016-09-06T07:27:06.826310: step 13444, loss 0.0367842, acc 0.98
2016-09-06T07:27:07.659255: step 13445, loss 0.00373855, acc 1
2016-09-06T07:27:08.509192: step 13446, loss 0.0464744, acc 0.98
2016-09-06T07:27:09.329820: step 13447, loss 0.0105101, acc 1
2016-09-06T07:27:10.143795: step 13448, loss 0.00535845, acc 1
2016-09-06T07:27:10.968062: step 13449, loss 0.0164873, acc 1
2016-09-06T07:27:11.776090: step 13450, loss 0.00696306, acc 1
2016-09-06T07:27:12.587145: step 13451, loss 0.00342766, acc 1
2016-09-06T07:27:13.398673: step 13452, loss 0.00367223, acc 1
2016-09-06T07:27:14.232709: step 13453, loss 0.0175605, acc 1
2016-09-06T07:27:15.069154: step 13454, loss 0.028323, acc 0.98
2016-09-06T07:27:15.853430: step 13455, loss 0.0587924, acc 0.96
2016-09-06T07:27:16.667805: step 13456, loss 0.0132603, acc 1
2016-09-06T07:27:17.498706: step 13457, loss 0.0182091, acc 0.98
2016-09-06T07:27:18.295090: step 13458, loss 0.00335289, acc 1
2016-09-06T07:27:19.098437: step 13459, loss 0.0165708, acc 1
2016-09-06T07:27:19.940272: step 13460, loss 0.00337651, acc 1
2016-09-06T07:27:20.729935: step 13461, loss 0.00403838, acc 1
2016-09-06T07:27:21.527773: step 13462, loss 0.0588022, acc 0.98
2016-09-06T07:27:22.389074: step 13463, loss 0.0100989, acc 1
2016-09-06T07:27:23.200954: step 13464, loss 0.024614, acc 0.98
2016-09-06T07:27:24.026692: step 13465, loss 0.0199395, acc 0.98
2016-09-06T07:27:24.838457: step 13466, loss 0.0528932, acc 0.96
2016-09-06T07:27:25.654660: step 13467, loss 0.00518651, acc 1
2016-09-06T07:27:26.495898: step 13468, loss 0.00849949, acc 1
2016-09-06T07:27:27.352379: step 13469, loss 0.0163643, acc 1
2016-09-06T07:27:28.166764: step 13470, loss 0.0124296, acc 1
2016-09-06T07:27:28.996013: step 13471, loss 0.0141913, acc 1
2016-09-06T07:27:29.857371: step 13472, loss 0.0295002, acc 0.98
2016-09-06T07:27:30.685793: step 13473, loss 0.0153291, acc 1
2016-09-06T07:27:31.494966: step 13474, loss 0.0390198, acc 0.98
2016-09-06T07:27:32.311592: step 13475, loss 0.052907, acc 0.98
2016-09-06T07:27:33.116565: step 13476, loss 0.0538303, acc 0.96
2016-09-06T07:27:33.935589: step 13477, loss 0.0030953, acc 1
2016-09-06T07:27:34.727033: step 13478, loss 0.0283666, acc 1
2016-09-06T07:27:35.544806: step 13479, loss 0.0184152, acc 0.98
2016-09-06T07:27:36.331593: step 13480, loss 0.00249583, acc 1
2016-09-06T07:27:37.136682: step 13481, loss 0.0103565, acc 1
2016-09-06T07:27:37.973667: step 13482, loss 0.00286643, acc 1
2016-09-06T07:27:38.787398: step 13483, loss 0.0095024, acc 1
2016-09-06T07:27:39.590005: step 13484, loss 0.0042161, acc 1
2016-09-06T07:27:40.423503: step 13485, loss 0.0263595, acc 0.98
2016-09-06T07:27:41.212744: step 13486, loss 0.00385646, acc 1
2016-09-06T07:27:42.015734: step 13487, loss 0.00862204, acc 1
2016-09-06T07:27:42.849700: step 13488, loss 0.00860821, acc 1
2016-09-06T07:27:43.624344: step 13489, loss 0.00508092, acc 1
2016-09-06T07:27:44.414368: step 13490, loss 0.00255031, acc 1
2016-09-06T07:27:45.249458: step 13491, loss 0.0071316, acc 1
2016-09-06T07:27:46.036602: step 13492, loss 0.0459569, acc 0.96
2016-09-06T07:27:46.853741: step 13493, loss 0.0261621, acc 0.98
2016-09-06T07:27:47.656139: step 13494, loss 0.00549127, acc 1
2016-09-06T07:27:48.461938: step 13495, loss 0.00304602, acc 1
2016-09-06T07:27:49.324287: step 13496, loss 0.00260957, acc 1
2016-09-06T07:27:50.157112: step 13497, loss 0.00527615, acc 1
2016-09-06T07:27:50.955486: step 13498, loss 0.0199213, acc 0.98
2016-09-06T07:27:51.784434: step 13499, loss 0.00836944, acc 1
2016-09-06T07:27:52.605462: step 13500, loss 0.00254482, acc 1

Evaluation:
2016-09-06T07:27:56.342677: step 13500, loss 2.81713, acc 0.722326

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-13500

2016-09-06T07:27:58.224341: step 13501, loss 0.035754, acc 0.98
2016-09-06T07:27:59.014417: step 13502, loss 0.0345192, acc 0.98
2016-09-06T07:27:59.825931: step 13503, loss 0.0132635, acc 1
2016-09-06T07:28:00.688911: step 13504, loss 0.00304266, acc 1
2016-09-06T07:28:01.502322: step 13505, loss 0.002504, acc 1
2016-09-06T07:28:02.331248: step 13506, loss 0.0174089, acc 0.98
2016-09-06T07:28:03.136094: step 13507, loss 0.0586402, acc 0.96
2016-09-06T07:28:03.966823: step 13508, loss 0.00585894, acc 1
2016-09-06T07:28:04.785544: step 13509, loss 0.0197774, acc 0.98
2016-09-06T07:28:05.605226: step 13510, loss 0.0184205, acc 1
2016-09-06T07:28:06.408462: step 13511, loss 0.00693745, acc 1
2016-09-06T07:28:07.249759: step 13512, loss 0.00230547, acc 1
2016-09-06T07:28:08.045271: step 13513, loss 0.0822969, acc 0.96
2016-09-06T07:28:08.831550: step 13514, loss 0.0198879, acc 0.98
2016-09-06T07:28:09.651520: step 13515, loss 0.0165022, acc 1
2016-09-06T07:28:10.459594: step 13516, loss 0.022589, acc 1
2016-09-06T07:28:11.269205: step 13517, loss 0.00481642, acc 1
2016-09-06T07:28:12.100563: step 13518, loss 0.0255592, acc 0.98
2016-09-06T07:28:12.926260: step 13519, loss 0.00426595, acc 1
2016-09-06T07:28:13.771603: step 13520, loss 0.00304444, acc 1
2016-09-06T07:28:14.598534: step 13521, loss 0.00560904, acc 1
2016-09-06T07:28:15.448855: step 13522, loss 0.0022499, acc 1
2016-09-06T07:28:16.241702: step 13523, loss 0.033757, acc 0.98
2016-09-06T07:28:17.045544: step 13524, loss 0.00332256, acc 1
2016-09-06T07:28:17.862442: step 13525, loss 0.019568, acc 0.98
2016-09-06T07:28:18.671330: step 13526, loss 0.0185085, acc 0.98
2016-09-06T07:28:19.476365: step 13527, loss 0.00224246, acc 1
2016-09-06T07:28:20.274437: step 13528, loss 0.0312933, acc 0.98
2016-09-06T07:28:21.085976: step 13529, loss 0.0339379, acc 0.98
2016-09-06T07:28:21.900924: step 13530, loss 0.0104973, acc 1
2016-09-06T07:28:22.757021: step 13531, loss 0.0131599, acc 1
2016-09-06T07:28:23.549464: step 13532, loss 0.00217183, acc 1
2016-09-06T07:28:24.367316: step 13533, loss 0.00222628, acc 1
2016-09-06T07:28:25.186002: step 13534, loss 0.00219006, acc 1
2016-09-06T07:28:25.993524: step 13535, loss 0.0259653, acc 0.98
2016-09-06T07:28:26.815301: step 13536, loss 0.00312759, acc 1
2016-09-06T07:28:27.675067: step 13537, loss 0.00215489, acc 1
2016-09-06T07:28:28.478721: step 13538, loss 0.0197033, acc 1
2016-09-06T07:28:29.294337: step 13539, loss 0.0358541, acc 0.98
2016-09-06T07:28:30.138011: step 13540, loss 0.0169937, acc 1
2016-09-06T07:28:30.947635: step 13541, loss 0.0128551, acc 1
2016-09-06T07:28:31.753623: step 13542, loss 0.00284401, acc 1
2016-09-06T07:28:32.564903: step 13543, loss 0.00239758, acc 1
2016-09-06T07:28:33.369847: step 13544, loss 0.00559017, acc 1
2016-09-06T07:28:34.173200: step 13545, loss 0.0403362, acc 0.98
2016-09-06T07:28:35.041807: step 13546, loss 0.00262396, acc 1
2016-09-06T07:28:35.882821: step 13547, loss 0.0904511, acc 0.98
2016-09-06T07:28:36.721978: step 13548, loss 0.00877264, acc 1
2016-09-06T07:28:37.563983: step 13549, loss 0.0436797, acc 0.98
2016-09-06T07:28:38.409401: step 13550, loss 0.028236, acc 0.98
2016-09-06T07:28:39.216186: step 13551, loss 0.00263678, acc 1
2016-09-06T07:28:40.028170: step 13552, loss 0.0326956, acc 0.96
2016-09-06T07:28:40.873337: step 13553, loss 0.017327, acc 0.98
2016-09-06T07:28:41.671828: step 13554, loss 0.00239244, acc 1
2016-09-06T07:28:42.469873: step 13555, loss 0.0378073, acc 0.98
2016-09-06T07:28:43.289242: step 13556, loss 0.00504179, acc 1
2016-09-06T07:28:44.091324: step 13557, loss 0.0193351, acc 0.98
2016-09-06T07:28:44.894228: step 13558, loss 0.00341477, acc 1
2016-09-06T07:28:45.711683: step 13559, loss 0.025146, acc 0.98
2016-09-06T07:28:46.508210: step 13560, loss 0.0173788, acc 0.98
2016-09-06T07:28:47.321518: step 13561, loss 0.00245343, acc 1
2016-09-06T07:28:48.148286: step 13562, loss 0.0119618, acc 1
2016-09-06T07:28:48.963797: step 13563, loss 0.00534713, acc 1
2016-09-06T07:28:49.790637: step 13564, loss 0.0146262, acc 1
2016-09-06T07:28:50.631925: step 13565, loss 0.0102156, acc 1
2016-09-06T07:28:51.444096: step 13566, loss 0.00217601, acc 1
2016-09-06T07:28:52.253802: step 13567, loss 0.00758817, acc 1
2016-09-06T07:28:53.078669: step 13568, loss 0.01755, acc 1
2016-09-06T07:28:53.884371: step 13569, loss 0.00315146, acc 1
2016-09-06T07:28:54.703146: step 13570, loss 0.0162121, acc 1
2016-09-06T07:28:55.568770: step 13571, loss 0.0239522, acc 1
2016-09-06T07:28:56.385178: step 13572, loss 0.007438, acc 1
2016-09-06T07:28:57.202244: step 13573, loss 0.00288925, acc 1
2016-09-06T07:28:58.048544: step 13574, loss 0.0463915, acc 1
2016-09-06T07:28:58.858066: step 13575, loss 0.0250583, acc 0.98
2016-09-06T07:28:59.689009: step 13576, loss 0.0122445, acc 1
2016-09-06T07:29:00.539223: step 13577, loss 0.0549365, acc 0.98
2016-09-06T07:29:01.379910: step 13578, loss 0.0256324, acc 0.98
2016-09-06T07:29:02.161262: step 13579, loss 0.00288175, acc 1
2016-09-06T07:29:02.979498: step 13580, loss 0.00269715, acc 1
2016-09-06T07:29:03.800319: step 13581, loss 0.0283776, acc 0.98
2016-09-06T07:29:04.586465: step 13582, loss 0.00912286, acc 1
2016-09-06T07:29:05.384523: step 13583, loss 0.0194035, acc 0.98
2016-09-06T07:29:06.212863: step 13584, loss 0.0330352, acc 0.98
2016-09-06T07:29:07.034009: step 13585, loss 0.00536613, acc 1
2016-09-06T07:29:07.879229: step 13586, loss 0.0125747, acc 1
2016-09-06T07:29:08.712116: step 13587, loss 0.0515876, acc 0.96
2016-09-06T07:29:09.527159: step 13588, loss 0.0134867, acc 1
2016-09-06T07:29:10.352551: step 13589, loss 0.0180118, acc 1
2016-09-06T07:29:11.191407: step 13590, loss 0.00723776, acc 1
2016-09-06T07:29:12.023231: step 13591, loss 0.00251236, acc 1
2016-09-06T07:29:12.837757: step 13592, loss 0.00323228, acc 1
2016-09-06T07:29:13.693730: step 13593, loss 0.00536946, acc 1
2016-09-06T07:29:14.495836: step 13594, loss 0.00256703, acc 1
2016-09-06T07:29:15.316638: step 13595, loss 0.00896346, acc 1
2016-09-06T07:29:16.137391: step 13596, loss 0.0045089, acc 1
2016-09-06T07:29:16.946570: step 13597, loss 0.0297911, acc 0.98
2016-09-06T07:29:17.769319: step 13598, loss 0.0121027, acc 1
2016-09-06T07:29:18.627999: step 13599, loss 0.00251665, acc 1
2016-09-06T07:29:19.429451: step 13600, loss 0.0192516, acc 1

Evaluation:
2016-09-06T07:29:23.151540: step 13600, loss 2.77006, acc 0.706379

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-13600

2016-09-06T07:29:25.040224: step 13601, loss 0.0265628, acc 0.98
2016-09-06T07:29:25.872356: step 13602, loss 0.00265836, acc 1
2016-09-06T07:29:26.664766: step 13603, loss 0.0101996, acc 1
2016-09-06T07:29:27.488813: step 13604, loss 0.00353419, acc 1
2016-09-06T07:29:28.337963: step 13605, loss 0.00950869, acc 1
2016-09-06T07:29:29.169832: step 13606, loss 0.0171255, acc 0.98
2016-09-06T07:29:30.008689: step 13607, loss 0.0026865, acc 1
2016-09-06T07:29:30.821290: step 13608, loss 0.0173091, acc 0.98
2016-09-06T07:29:31.676336: step 13609, loss 0.0554548, acc 0.96
2016-09-06T07:29:32.449406: step 13610, loss 0.00395998, acc 1
2016-09-06T07:29:33.282290: step 13611, loss 0.00267547, acc 1
2016-09-06T07:29:34.105788: step 13612, loss 0.0175908, acc 1
2016-09-06T07:29:34.933291: step 13613, loss 0.00265121, acc 1
2016-09-06T07:29:35.747070: step 13614, loss 0.0169652, acc 0.98
2016-09-06T07:29:36.576500: step 13615, loss 0.0222514, acc 0.98
2016-09-06T07:29:37.515574: step 13616, loss 0.00487585, acc 1
2016-09-06T07:29:38.334570: step 13617, loss 0.0247084, acc 0.98
2016-09-06T07:29:39.190364: step 13618, loss 0.0227082, acc 1
2016-09-06T07:29:40.037412: step 13619, loss 0.00325595, acc 1
2016-09-06T07:29:40.847196: step 13620, loss 0.0346958, acc 0.98
2016-09-06T07:29:41.682133: step 13621, loss 0.0198394, acc 0.98
2016-09-06T07:29:42.500589: step 13622, loss 0.0165908, acc 1
2016-09-06T07:29:43.287167: step 13623, loss 0.00362994, acc 1
2016-09-06T07:29:44.107077: step 13624, loss 0.00270442, acc 1
2016-09-06T07:29:44.920093: step 13625, loss 0.00260969, acc 1
2016-09-06T07:29:45.704822: step 13626, loss 0.0241942, acc 1
2016-09-06T07:29:46.513522: step 13627, loss 0.0162321, acc 1
2016-09-06T07:29:47.332313: step 13628, loss 0.00318413, acc 1
2016-09-06T07:29:48.120073: step 13629, loss 0.036321, acc 0.98
2016-09-06T07:29:48.944786: step 13630, loss 0.00606, acc 1
2016-09-06T07:29:49.784479: step 13631, loss 0.0173439, acc 0.98
2016-09-06T07:29:50.516485: step 13632, loss 0.00250009, acc 1
2016-09-06T07:29:51.324599: step 13633, loss 0.10773, acc 0.96
2016-09-06T07:29:52.128405: step 13634, loss 0.0487065, acc 0.96
2016-09-06T07:29:52.954666: step 13635, loss 0.00256693, acc 1
2016-09-06T07:29:53.781283: step 13636, loss 0.0120445, acc 1
2016-09-06T07:29:54.625778: step 13637, loss 0.046558, acc 0.96
2016-09-06T07:29:55.406740: step 13638, loss 0.0162161, acc 1
2016-09-06T07:29:56.200100: step 13639, loss 0.00228128, acc 1
2016-09-06T07:29:57.065146: step 13640, loss 0.0337138, acc 0.98
2016-09-06T07:29:57.858742: step 13641, loss 0.0695906, acc 0.98
2016-09-06T07:29:58.664824: step 13642, loss 0.0475926, acc 0.98
2016-09-06T07:29:59.487971: step 13643, loss 0.0345434, acc 0.98
2016-09-06T07:30:00.295572: step 13644, loss 0.00220568, acc 1
2016-09-06T07:30:01.101704: step 13645, loss 0.0158898, acc 1
2016-09-06T07:30:01.907661: step 13646, loss 0.0765074, acc 0.96
2016-09-06T07:30:02.693444: step 13647, loss 0.0425774, acc 0.98
2016-09-06T07:30:03.537166: step 13648, loss 0.00192812, acc 1
2016-09-06T07:30:04.356072: step 13649, loss 0.00616639, acc 1
2016-09-06T07:30:05.178134: step 13650, loss 0.0017762, acc 1
2016-09-06T07:30:05.992812: step 13651, loss 0.0151008, acc 1
2016-09-06T07:30:06.829109: step 13652, loss 0.00172806, acc 1
2016-09-06T07:30:07.631685: step 13653, loss 0.00275435, acc 1
2016-09-06T07:30:08.446564: step 13654, loss 0.0214216, acc 1
2016-09-06T07:30:09.273890: step 13655, loss 0.00254198, acc 1
2016-09-06T07:30:10.084051: step 13656, loss 0.00493285, acc 1
2016-09-06T07:30:10.883613: step 13657, loss 0.0144921, acc 1
2016-09-06T07:30:11.711617: step 13658, loss 0.0640376, acc 0.96
2016-09-06T07:30:12.501409: step 13659, loss 0.0109354, acc 1
2016-09-06T07:30:13.315940: step 13660, loss 0.00354307, acc 1
2016-09-06T07:30:14.145760: step 13661, loss 0.0575903, acc 0.98
2016-09-06T07:30:15.007106: step 13662, loss 0.0048381, acc 1
2016-09-06T07:30:15.856191: step 13663, loss 0.00955934, acc 1
2016-09-06T07:30:16.689907: step 13664, loss 0.00539567, acc 1
2016-09-06T07:30:17.475620: step 13665, loss 0.00236232, acc 1
2016-09-06T07:30:18.305525: step 13666, loss 0.00746526, acc 1
2016-09-06T07:30:19.165517: step 13667, loss 0.00216085, acc 1
2016-09-06T07:30:20.008696: step 13668, loss 0.00525913, acc 1
2016-09-06T07:30:20.812427: step 13669, loss 0.00184481, acc 1
2016-09-06T07:30:21.609406: step 13670, loss 0.0379521, acc 0.96
2016-09-06T07:30:22.437697: step 13671, loss 0.00271512, acc 1
2016-09-06T07:30:23.236169: step 13672, loss 0.00292726, acc 1
2016-09-06T07:30:24.036201: step 13673, loss 0.0230793, acc 0.98
2016-09-06T07:30:24.846425: step 13674, loss 0.00156533, acc 1
2016-09-06T07:30:25.633927: step 13675, loss 0.0166532, acc 1
2016-09-06T07:30:26.446664: step 13676, loss 0.011492, acc 1
2016-09-06T07:30:27.270533: step 13677, loss 0.00158046, acc 1
2016-09-06T07:30:28.045119: step 13678, loss 0.0209378, acc 0.98
2016-09-06T07:30:28.828748: step 13679, loss 0.00431992, acc 1
2016-09-06T07:30:29.639913: step 13680, loss 0.0140231, acc 1
2016-09-06T07:30:30.452115: step 13681, loss 0.0153563, acc 1
2016-09-06T07:30:31.276204: step 13682, loss 0.00184943, acc 1
2016-09-06T07:30:32.096895: step 13683, loss 0.00181378, acc 1
2016-09-06T07:30:32.866756: step 13684, loss 0.0320166, acc 0.96
2016-09-06T07:30:33.684502: step 13685, loss 0.135598, acc 0.96
2016-09-06T07:30:34.507427: step 13686, loss 0.00881661, acc 1
2016-09-06T07:30:35.279358: step 13687, loss 0.0109327, acc 1
2016-09-06T07:30:36.081070: step 13688, loss 0.00686627, acc 1
2016-09-06T07:30:36.888247: step 13689, loss 0.00239642, acc 1
2016-09-06T07:30:37.703101: step 13690, loss 0.105739, acc 0.98
2016-09-06T07:30:38.507046: step 13691, loss 0.0082299, acc 1
2016-09-06T07:30:39.317192: step 13692, loss 0.00142424, acc 1
2016-09-06T07:30:40.113357: step 13693, loss 0.0174546, acc 1
2016-09-06T07:30:40.914722: step 13694, loss 0.0029756, acc 1
2016-09-06T07:30:41.749489: step 13695, loss 0.00291827, acc 1
2016-09-06T07:30:42.554571: step 13696, loss 0.0484701, acc 0.98
2016-09-06T07:30:43.368605: step 13697, loss 0.00323072, acc 1
2016-09-06T07:30:44.199379: step 13698, loss 0.00260013, acc 1
2016-09-06T07:30:44.992662: step 13699, loss 0.0178342, acc 0.98
2016-09-06T07:30:45.793218: step 13700, loss 0.00804727, acc 1

Evaluation:
2016-09-06T07:30:49.533532: step 13700, loss 1.92698, acc 0.71576

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-13700

2016-09-06T07:30:51.507558: step 13701, loss 0.00593268, acc 1
2016-09-06T07:30:52.303905: step 13702, loss 0.0358213, acc 0.98
2016-09-06T07:30:53.122649: step 13703, loss 0.0158179, acc 1
2016-09-06T07:30:53.947718: step 13704, loss 0.00302998, acc 1
2016-09-06T07:30:54.766756: step 13705, loss 0.0149713, acc 1
2016-09-06T07:30:55.602835: step 13706, loss 0.00557026, acc 1
2016-09-06T07:30:56.437043: step 13707, loss 0.0435381, acc 0.98
2016-09-06T07:30:57.248101: step 13708, loss 0.0143159, acc 1
2016-09-06T07:30:58.079372: step 13709, loss 0.00535266, acc 1
2016-09-06T07:30:58.954075: step 13710, loss 0.0418215, acc 0.98
2016-09-06T07:30:59.785183: step 13711, loss 0.0614598, acc 0.96
2016-09-06T07:31:00.645198: step 13712, loss 0.00443562, acc 1
2016-09-06T07:31:01.472835: step 13713, loss 0.00174756, acc 1
2016-09-06T07:31:02.385116: step 13714, loss 0.0332363, acc 0.98
2016-09-06T07:31:03.219350: step 13715, loss 0.0373968, acc 0.98
2016-09-06T07:31:04.036892: step 13716, loss 0.00158884, acc 1
2016-09-06T07:31:04.888581: step 13717, loss 0.0693459, acc 0.98
2016-09-06T07:31:05.716524: step 13718, loss 0.0237884, acc 1
2016-09-06T07:31:06.557140: step 13719, loss 0.0018883, acc 1
2016-09-06T07:31:07.410990: step 13720, loss 0.0444808, acc 0.96
2016-09-06T07:31:08.231943: step 13721, loss 0.0310353, acc 0.98
2016-09-06T07:31:09.044096: step 13722, loss 0.0021332, acc 1
2016-09-06T07:31:09.861363: step 13723, loss 0.0230884, acc 0.98
2016-09-06T07:31:10.667367: step 13724, loss 0.0151106, acc 1
2016-09-06T07:31:11.486963: step 13725, loss 0.00377883, acc 1
2016-09-06T07:31:12.345579: step 13726, loss 0.00316474, acc 1
2016-09-06T07:31:13.153292: step 13727, loss 0.0257714, acc 0.98
2016-09-06T07:31:13.958126: step 13728, loss 0.00500245, acc 1
2016-09-06T07:31:14.802180: step 13729, loss 0.00328658, acc 1
2016-09-06T07:31:15.631319: step 13730, loss 0.034888, acc 0.98
2016-09-06T07:31:16.443025: step 13731, loss 0.0353887, acc 0.98
2016-09-06T07:31:17.260744: step 13732, loss 0.00200728, acc 1
2016-09-06T07:31:18.051877: step 13733, loss 0.00334597, acc 1
2016-09-06T07:31:18.846001: step 13734, loss 0.00223518, acc 1
2016-09-06T07:31:19.697523: step 13735, loss 0.0139714, acc 1
2016-09-06T07:31:20.511657: step 13736, loss 0.00224346, acc 1
2016-09-06T07:31:21.319712: step 13737, loss 0.0165218, acc 0.98
2016-09-06T07:31:22.170976: step 13738, loss 0.00237029, acc 1
2016-09-06T07:31:22.996708: step 13739, loss 0.0153022, acc 1
2016-09-06T07:31:23.808096: step 13740, loss 0.0144599, acc 1
2016-09-06T07:31:24.611260: step 13741, loss 0.0149771, acc 1
2016-09-06T07:31:25.468051: step 13742, loss 0.117469, acc 0.98
2016-09-06T07:31:26.276324: step 13743, loss 0.00262813, acc 1
2016-09-06T07:31:27.084925: step 13744, loss 0.00713395, acc 1
2016-09-06T07:31:27.901095: step 13745, loss 0.00247236, acc 1
2016-09-06T07:31:28.716478: step 13746, loss 0.00316476, acc 1
2016-09-06T07:31:29.535912: step 13747, loss 0.0879561, acc 0.96
2016-09-06T07:31:30.405609: step 13748, loss 0.077533, acc 0.98
2016-09-06T07:31:31.232947: step 13749, loss 0.0152746, acc 1
2016-09-06T07:31:32.032493: step 13750, loss 0.00928373, acc 1
2016-09-06T07:31:32.877478: step 13751, loss 0.00198644, acc 1
2016-09-06T07:31:33.700775: step 13752, loss 0.0330488, acc 0.98
2016-09-06T07:31:34.534944: step 13753, loss 0.00191511, acc 1
2016-09-06T07:31:35.345932: step 13754, loss 0.0107894, acc 1
2016-09-06T07:31:36.182529: step 13755, loss 0.00228492, acc 1
2016-09-06T07:31:37.009787: step 13756, loss 0.0183231, acc 0.98
2016-09-06T07:31:37.826120: step 13757, loss 0.0326678, acc 1
2016-09-06T07:31:38.675434: step 13758, loss 0.00288946, acc 1
2016-09-06T07:31:39.490238: step 13759, loss 0.0205821, acc 0.98
2016-09-06T07:31:40.317214: step 13760, loss 0.017232, acc 1
2016-09-06T07:31:41.142489: step 13761, loss 0.00965514, acc 1
2016-09-06T07:31:41.960053: step 13762, loss 0.0154955, acc 1
2016-09-06T07:31:42.767857: step 13763, loss 0.00472402, acc 1
2016-09-06T07:31:43.584890: step 13764, loss 0.00507638, acc 1
2016-09-06T07:31:44.391193: step 13765, loss 0.0155009, acc 1
2016-09-06T07:31:45.225354: step 13766, loss 0.0789113, acc 0.96
2016-09-06T07:31:46.048809: step 13767, loss 0.0368091, acc 0.98
2016-09-06T07:31:46.884115: step 13768, loss 0.0225738, acc 1
2016-09-06T07:31:47.704856: step 13769, loss 0.0146721, acc 1
2016-09-06T07:31:48.528855: step 13770, loss 0.0167524, acc 1
2016-09-06T07:31:49.377608: step 13771, loss 0.00809545, acc 1
2016-09-06T07:31:50.217164: step 13772, loss 0.00403083, acc 1
2016-09-06T07:31:51.034935: step 13773, loss 0.0426913, acc 1
2016-09-06T07:31:51.842390: step 13774, loss 0.0755324, acc 0.98
2016-09-06T07:31:52.636777: step 13775, loss 0.00332339, acc 1
2016-09-06T07:31:53.453815: step 13776, loss 0.0191778, acc 1
2016-09-06T07:31:54.264710: step 13777, loss 0.0306743, acc 0.98
2016-09-06T07:31:55.056269: step 13778, loss 0.0243237, acc 1
2016-09-06T07:31:55.901086: step 13779, loss 0.0262592, acc 0.98
2016-09-06T07:31:56.748522: step 13780, loss 0.00692678, acc 1
2016-09-06T07:31:57.555496: step 13781, loss 0.0172045, acc 1
2016-09-06T07:31:58.356318: step 13782, loss 0.013014, acc 1
2016-09-06T07:31:59.157449: step 13783, loss 0.00228485, acc 1
2016-09-06T07:31:59.943696: step 13784, loss 0.0141347, acc 1
2016-09-06T07:32:00.781651: step 13785, loss 0.0177225, acc 1
2016-09-06T07:32:01.614054: step 13786, loss 0.00220877, acc 1
2016-09-06T07:32:02.416705: step 13787, loss 0.00316866, acc 1
2016-09-06T07:32:03.213668: step 13788, loss 0.0576794, acc 0.98
2016-09-06T07:32:04.071695: step 13789, loss 0.0404468, acc 0.98
2016-09-06T07:32:04.873227: step 13790, loss 0.00261625, acc 1
2016-09-06T07:32:05.711828: step 13791, loss 0.00297143, acc 1
2016-09-06T07:32:06.513020: step 13792, loss 0.0270434, acc 1
2016-09-06T07:32:07.318278: step 13793, loss 0.0207186, acc 1
2016-09-06T07:32:08.105162: step 13794, loss 0.0187609, acc 0.98
2016-09-06T07:32:08.929418: step 13795, loss 0.0121961, acc 1
2016-09-06T07:32:09.743652: step 13796, loss 0.00266617, acc 1
2016-09-06T07:32:10.554193: step 13797, loss 0.00223217, acc 1
2016-09-06T07:32:11.389732: step 13798, loss 0.0148418, acc 1
2016-09-06T07:32:12.199087: step 13799, loss 0.0046143, acc 1
2016-09-06T07:32:13.015214: step 13800, loss 0.00825446, acc 1

Evaluation:
2016-09-06T07:32:16.781245: step 13800, loss 2.53529, acc 0.713884

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-13800

2016-09-06T07:32:18.777743: step 13801, loss 0.0463732, acc 0.96
2016-09-06T07:32:19.647422: step 13802, loss 0.00246229, acc 1
2016-09-06T07:32:20.415882: step 13803, loss 0.00852915, acc 1
2016-09-06T07:32:21.246744: step 13804, loss 0.0434819, acc 0.98
2016-09-06T07:32:22.055470: step 13805, loss 0.00667803, acc 1
2016-09-06T07:32:22.849891: step 13806, loss 0.0147562, acc 1
2016-09-06T07:32:23.679406: step 13807, loss 0.0142962, acc 1
2016-09-06T07:32:24.500129: step 13808, loss 0.0022437, acc 1
2016-09-06T07:32:25.282669: step 13809, loss 0.0170083, acc 0.98
2016-09-06T07:32:26.092782: step 13810, loss 0.0247652, acc 0.98
2016-09-06T07:32:26.895617: step 13811, loss 0.0129988, acc 1
2016-09-06T07:32:27.721396: step 13812, loss 0.0254144, acc 1
2016-09-06T07:32:28.536301: step 13813, loss 0.00230462, acc 1
2016-09-06T07:32:29.344738: step 13814, loss 0.00465632, acc 1
2016-09-06T07:32:30.124069: step 13815, loss 0.00221516, acc 1
2016-09-06T07:32:30.935130: step 13816, loss 0.0243199, acc 0.98
2016-09-06T07:32:31.732582: step 13817, loss 0.0256383, acc 0.98
2016-09-06T07:32:32.542377: step 13818, loss 0.0140855, acc 1
2016-09-06T07:32:33.374018: step 13819, loss 0.00261808, acc 1
2016-09-06T07:32:34.225481: step 13820, loss 0.00296524, acc 1
2016-09-06T07:32:35.022350: step 13821, loss 0.00744154, acc 1
2016-09-06T07:32:35.854034: step 13822, loss 0.00227329, acc 1
2016-09-06T07:32:36.655681: step 13823, loss 0.0025496, acc 1
2016-09-06T07:32:37.404449: step 13824, loss 0.00210456, acc 1
2016-09-06T07:32:38.214113: step 13825, loss 0.072938, acc 0.96
2016-09-06T07:32:39.022537: step 13826, loss 0.0263293, acc 1
2016-09-06T07:32:39.798504: step 13827, loss 0.0123721, acc 1
2016-09-06T07:32:40.601472: step 13828, loss 0.00262265, acc 1
2016-09-06T07:32:41.415622: step 13829, loss 0.0172926, acc 1
2016-09-06T07:32:42.214497: step 13830, loss 0.0217402, acc 1
2016-09-06T07:32:43.057977: step 13831, loss 0.00310642, acc 1
2016-09-06T07:32:43.881079: step 13832, loss 0.0692854, acc 0.98
2016-09-06T07:32:44.669710: step 13833, loss 0.0232893, acc 0.98
2016-09-06T07:32:45.463700: step 13834, loss 0.0863004, acc 0.98
2016-09-06T07:32:46.296061: step 13835, loss 0.00194606, acc 1
2016-09-06T07:32:47.087156: step 13836, loss 0.00195423, acc 1
2016-09-06T07:32:47.888931: step 13837, loss 0.00201523, acc 1
2016-09-06T07:32:48.709366: step 13838, loss 0.016629, acc 0.98
2016-09-06T07:32:49.507180: step 13839, loss 0.0651238, acc 0.96
2016-09-06T07:32:50.317653: step 13840, loss 0.00659204, acc 1
2016-09-06T07:32:51.169060: step 13841, loss 0.00420984, acc 1
2016-09-06T07:32:51.961586: step 13842, loss 0.00206231, acc 1
2016-09-06T07:32:52.762580: step 13843, loss 0.00680866, acc 1
2016-09-06T07:32:53.594456: step 13844, loss 0.00171683, acc 1
2016-09-06T07:32:54.385573: step 13845, loss 0.00277527, acc 1
2016-09-06T07:32:55.184242: step 13846, loss 0.00320587, acc 1
2016-09-06T07:32:55.997281: step 13847, loss 0.0261308, acc 0.98
2016-09-06T07:32:56.812659: step 13848, loss 0.0219132, acc 0.98
2016-09-06T07:32:57.613656: step 13849, loss 0.0166661, acc 1
2016-09-06T07:32:58.423013: step 13850, loss 0.0168025, acc 0.98
2016-09-06T07:32:59.217482: step 13851, loss 0.0168693, acc 0.98
2016-09-06T07:33:00.000584: step 13852, loss 0.0169969, acc 0.98
2016-09-06T07:33:00.890721: step 13853, loss 0.00965774, acc 1
2016-09-06T07:33:01.718469: step 13854, loss 0.00543746, acc 1
2016-09-06T07:33:02.533569: step 13855, loss 0.00362462, acc 1
2016-09-06T07:33:03.368530: step 13856, loss 0.0141387, acc 1
2016-09-06T07:33:04.178951: step 13857, loss 0.00563285, acc 1
2016-09-06T07:33:05.024807: step 13858, loss 0.0332523, acc 1
2016-09-06T07:33:05.869411: step 13859, loss 0.010741, acc 1
2016-09-06T07:33:06.705744: step 13860, loss 0.00391786, acc 1
2016-09-06T07:33:07.515479: step 13861, loss 0.0016735, acc 1
2016-09-06T07:33:08.342338: step 13862, loss 0.0179179, acc 1
2016-09-06T07:33:09.160350: step 13863, loss 0.0258889, acc 0.98
2016-09-06T07:33:09.977268: step 13864, loss 0.00173812, acc 1
2016-09-06T07:33:10.819811: step 13865, loss 0.011524, acc 1
2016-09-06T07:33:11.614738: step 13866, loss 0.00202994, acc 1
2016-09-06T07:33:12.442926: step 13867, loss 0.0653429, acc 0.98
2016-09-06T07:33:13.264604: step 13868, loss 0.00421587, acc 1
2016-09-06T07:33:14.091660: step 13869, loss 0.00186145, acc 1
2016-09-06T07:33:14.872149: step 13870, loss 0.0406333, acc 0.98
2016-09-06T07:33:15.686610: step 13871, loss 0.00374846, acc 1
2016-09-06T07:33:16.468380: step 13872, loss 0.025426, acc 0.98
2016-09-06T07:33:17.269208: step 13873, loss 0.0427494, acc 0.98
2016-09-06T07:33:18.102397: step 13874, loss 0.00356293, acc 1
2016-09-06T07:33:18.946747: step 13875, loss 0.00312008, acc 1
2016-09-06T07:33:19.760135: step 13876, loss 0.0143314, acc 1
2016-09-06T07:33:20.592537: step 13877, loss 0.0261822, acc 0.98
2016-09-06T07:33:21.425980: step 13878, loss 0.00183101, acc 1
2016-09-06T07:33:22.245344: step 13879, loss 0.0150477, acc 1
2016-09-06T07:33:23.071883: step 13880, loss 0.00191461, acc 1
2016-09-06T07:33:23.916782: step 13881, loss 0.00188968, acc 1
2016-09-06T07:33:24.725730: step 13882, loss 0.0234896, acc 0.98
2016-09-06T07:33:25.524455: step 13883, loss 0.0058194, acc 1
2016-09-06T07:33:26.339221: step 13884, loss 0.00168721, acc 1
2016-09-06T07:33:27.121319: step 13885, loss 0.0429554, acc 0.96
2016-09-06T07:33:27.942331: step 13886, loss 0.0030948, acc 1
2016-09-06T07:33:28.783476: step 13887, loss 0.00160645, acc 1
2016-09-06T07:33:29.585017: step 13888, loss 0.0247622, acc 1
2016-09-06T07:33:30.423969: step 13889, loss 0.00228542, acc 1
2016-09-06T07:33:31.269453: step 13890, loss 0.00225135, acc 1
2016-09-06T07:33:32.096592: step 13891, loss 0.00329605, acc 1
2016-09-06T07:33:32.909668: step 13892, loss 0.0268325, acc 1
2016-09-06T07:33:33.732326: step 13893, loss 0.0166687, acc 0.98
2016-09-06T07:33:34.559824: step 13894, loss 0.0346904, acc 0.98
2016-09-06T07:33:35.397435: step 13895, loss 0.00867247, acc 1
2016-09-06T07:33:36.212525: step 13896, loss 0.00171448, acc 1
2016-09-06T07:33:36.996223: step 13897, loss 0.0439732, acc 0.98
2016-09-06T07:33:37.828262: step 13898, loss 0.0018885, acc 1
2016-09-06T07:33:38.658844: step 13899, loss 0.00700181, acc 1
2016-09-06T07:33:39.468148: step 13900, loss 0.0479927, acc 0.96

Evaluation:
2016-09-06T07:33:43.184717: step 13900, loss 2.54942, acc 0.717636

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-13900

2016-09-06T07:33:45.138098: step 13901, loss 0.00352245, acc 1
2016-09-06T07:33:45.985351: step 13902, loss 0.0103481, acc 1
2016-09-06T07:33:46.804826: step 13903, loss 0.00953617, acc 1
2016-09-06T07:33:47.584717: step 13904, loss 0.141747, acc 0.96
2016-09-06T07:33:48.392848: step 13905, loss 0.00288528, acc 1
2016-09-06T07:33:49.219868: step 13906, loss 0.0254279, acc 0.98
2016-09-06T07:33:49.990422: step 13907, loss 0.0118835, acc 1
2016-09-06T07:33:50.805432: step 13908, loss 0.011357, acc 1
2016-09-06T07:33:51.633447: step 13909, loss 0.0692224, acc 0.98
2016-09-06T07:33:52.402814: step 13910, loss 0.0400385, acc 0.98
2016-09-06T07:33:53.196858: step 13911, loss 0.0291878, acc 0.98
2016-09-06T07:33:54.046547: step 13912, loss 0.00176242, acc 1
2016-09-06T07:33:54.836880: step 13913, loss 0.00187313, acc 1
2016-09-06T07:33:55.637384: step 13914, loss 0.0325528, acc 0.98
2016-09-06T07:33:56.484476: step 13915, loss 0.030283, acc 0.98
2016-09-06T07:33:57.272596: step 13916, loss 0.00231728, acc 1
2016-09-06T07:33:58.084149: step 13917, loss 0.0142529, acc 1
2016-09-06T07:33:58.899545: step 13918, loss 0.00420519, acc 1
2016-09-06T07:33:59.709297: step 13919, loss 0.00336233, acc 1
2016-09-06T07:34:00.522920: step 13920, loss 0.0021774, acc 1
2016-09-06T07:34:01.341844: step 13921, loss 0.00324875, acc 1
2016-09-06T07:34:02.142468: step 13922, loss 0.0255577, acc 1
2016-09-06T07:34:02.946613: step 13923, loss 0.0037654, acc 1
2016-09-06T07:34:03.769512: step 13924, loss 0.0186539, acc 0.98
2016-09-06T07:34:04.566684: step 13925, loss 0.012668, acc 1
2016-09-06T07:34:05.376843: step 13926, loss 0.00314363, acc 1
2016-09-06T07:34:06.185015: step 13927, loss 0.00941352, acc 1
2016-09-06T07:34:06.988505: step 13928, loss 0.0032739, acc 1
2016-09-06T07:34:07.783049: step 13929, loss 0.00465522, acc 1
2016-09-06T07:34:08.599198: step 13930, loss 0.00907151, acc 1
2016-09-06T07:34:09.424944: step 13931, loss 0.0321462, acc 0.98
2016-09-06T07:34:10.281758: step 13932, loss 0.0399129, acc 0.98
2016-09-06T07:34:11.146447: step 13933, loss 0.0564211, acc 0.98
2016-09-06T07:34:11.977931: step 13934, loss 0.0133348, acc 1
2016-09-06T07:34:12.808378: step 13935, loss 0.00277074, acc 1
2016-09-06T07:34:13.652898: step 13936, loss 0.0191644, acc 0.98
2016-09-06T07:34:14.458886: step 13937, loss 0.0477453, acc 0.98
2016-09-06T07:34:15.248450: step 13938, loss 0.0195886, acc 0.98
2016-09-06T07:34:16.087269: step 13939, loss 0.0130304, acc 1
2016-09-06T07:34:16.902882: step 13940, loss 0.0212385, acc 0.98
2016-09-06T07:34:17.731594: step 13941, loss 0.00552156, acc 1
2016-09-06T07:34:18.583506: step 13942, loss 0.00262565, acc 1
2016-09-06T07:34:19.415440: step 13943, loss 0.00315612, acc 1
2016-09-06T07:34:20.192326: step 13944, loss 0.00800255, acc 1
2016-09-06T07:34:20.981032: step 13945, loss 0.00293432, acc 1
2016-09-06T07:34:21.792332: step 13946, loss 0.00652806, acc 1
2016-09-06T07:34:22.587645: step 13947, loss 0.122939, acc 0.98
2016-09-06T07:34:23.378950: step 13948, loss 0.0152037, acc 1
2016-09-06T07:34:24.175653: step 13949, loss 0.0230798, acc 0.98
2016-09-06T07:34:24.978665: step 13950, loss 0.00253564, acc 1
2016-09-06T07:34:25.809066: step 13951, loss 0.0314359, acc 0.98
2016-09-06T07:34:26.643459: step 13952, loss 0.0166757, acc 1
2016-09-06T07:34:27.450663: step 13953, loss 0.0315975, acc 0.98
2016-09-06T07:34:28.274801: step 13954, loss 0.0352538, acc 1
2016-09-06T07:34:29.106207: step 13955, loss 0.0339312, acc 1
2016-09-06T07:34:29.887220: step 13956, loss 0.00618813, acc 1
2016-09-06T07:34:30.723739: step 13957, loss 0.00250559, acc 1
2016-09-06T07:34:31.541605: step 13958, loss 0.00428716, acc 1
2016-09-06T07:34:32.337944: step 13959, loss 0.0183736, acc 0.98
2016-09-06T07:34:33.149414: step 13960, loss 0.0286004, acc 0.98
2016-09-06T07:34:34.004044: step 13961, loss 0.0111678, acc 1
2016-09-06T07:34:34.815271: step 13962, loss 0.0224448, acc 0.98
2016-09-06T07:34:35.661987: step 13963, loss 0.0123023, acc 1
2016-09-06T07:34:36.490385: step 13964, loss 0.00355441, acc 1
2016-09-06T07:34:37.301279: step 13965, loss 0.00363928, acc 1
2016-09-06T07:34:38.126835: step 13966, loss 0.017626, acc 0.98
2016-09-06T07:34:38.991483: step 13967, loss 0.0608859, acc 0.98
2016-09-06T07:34:39.780555: step 13968, loss 0.0130646, acc 1
2016-09-06T07:34:40.568448: step 13969, loss 0.00268093, acc 1
2016-09-06T07:34:41.418164: step 13970, loss 0.0618108, acc 0.96
2016-09-06T07:34:42.256216: step 13971, loss 0.0218353, acc 0.98
2016-09-06T07:34:43.063473: step 13972, loss 0.0538381, acc 0.98
2016-09-06T07:34:43.916857: step 13973, loss 0.00574279, acc 1
2016-09-06T07:34:44.730427: step 13974, loss 0.0192711, acc 0.98
2016-09-06T07:34:45.566835: step 13975, loss 0.225543, acc 0.98
2016-09-06T07:34:46.388501: step 13976, loss 0.00242803, acc 1
2016-09-06T07:34:47.252927: step 13977, loss 0.00693752, acc 1
2016-09-06T07:34:48.017032: step 13978, loss 0.00233652, acc 1
2016-09-06T07:34:48.813337: step 13979, loss 0.068905, acc 0.96
2016-09-06T07:34:49.664564: step 13980, loss 0.0252742, acc 0.98
2016-09-06T07:34:50.431140: step 13981, loss 0.0186877, acc 0.98
2016-09-06T07:34:51.253485: step 13982, loss 0.066901, acc 0.96
2016-09-06T07:34:52.077945: step 13983, loss 0.0136728, acc 1
2016-09-06T07:34:52.858404: step 13984, loss 0.0318713, acc 0.98
2016-09-06T07:34:53.642440: step 13985, loss 0.0230204, acc 1
2016-09-06T07:34:54.452793: step 13986, loss 0.00299656, acc 1
2016-09-06T07:34:55.250561: step 13987, loss 0.0039589, acc 1
2016-09-06T07:34:56.052848: step 13988, loss 0.00297263, acc 1
2016-09-06T07:34:56.863829: step 13989, loss 0.00238375, acc 1
2016-09-06T07:34:57.655275: step 13990, loss 0.0124654, acc 1
2016-09-06T07:34:58.458828: step 13991, loss 0.00226306, acc 1
2016-09-06T07:34:59.272753: step 13992, loss 0.00305279, acc 1
2016-09-06T07:35:00.062154: step 13993, loss 0.0264708, acc 1
2016-09-06T07:35:00.904754: step 13994, loss 0.00569816, acc 1
2016-09-06T07:35:01.818952: step 13995, loss 0.00601312, acc 1
2016-09-06T07:35:02.615322: step 13996, loss 0.0162484, acc 1
2016-09-06T07:35:03.426549: step 13997, loss 0.020963, acc 0.98
2016-09-06T07:35:04.237352: step 13998, loss 0.010703, acc 1
2016-09-06T07:35:05.027329: step 13999, loss 0.0218268, acc 0.98
2016-09-06T07:35:05.875130: step 14000, loss 0.00272833, acc 1

Evaluation:
2016-09-06T07:35:09.593262: step 14000, loss 2.61832, acc 0.718574

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-14000

2016-09-06T07:35:11.639318: step 14001, loss 0.00702043, acc 1
2016-09-06T07:35:12.420496: step 14002, loss 0.0114462, acc 1
2016-09-06T07:35:13.225866: step 14003, loss 0.0164817, acc 1
2016-09-06T07:35:14.069845: step 14004, loss 0.0232613, acc 1
2016-09-06T07:35:14.913223: step 14005, loss 0.00379405, acc 1
2016-09-06T07:35:15.747771: step 14006, loss 0.0207582, acc 0.98
2016-09-06T07:35:16.589327: step 14007, loss 0.00284695, acc 1
2016-09-06T07:35:17.393134: step 14008, loss 0.00364127, acc 1
2016-09-06T07:35:18.213346: step 14009, loss 0.0106182, acc 1
2016-09-06T07:35:19.043474: step 14010, loss 0.00547866, acc 1
2016-09-06T07:35:19.865939: step 14011, loss 0.0207972, acc 1
2016-09-06T07:35:20.721349: step 14012, loss 0.0508975, acc 0.98
2016-09-06T07:35:21.557755: step 14013, loss 0.0359938, acc 0.98
2016-09-06T07:35:22.378831: step 14014, loss 0.025759, acc 0.98
2016-09-06T07:35:23.166305: step 14015, loss 0.0161995, acc 1
2016-09-06T07:35:23.935377: step 14016, loss 0.00308006, acc 1
2016-09-06T07:35:24.744138: step 14017, loss 0.00306152, acc 1
2016-09-06T07:35:25.571033: step 14018, loss 0.0167125, acc 1
2016-09-06T07:35:26.380541: step 14019, loss 0.00278469, acc 1
2016-09-06T07:35:27.193325: step 14020, loss 0.00439474, acc 1
2016-09-06T07:35:27.981608: step 14021, loss 0.0317367, acc 1
2016-09-06T07:35:28.809268: step 14022, loss 0.040664, acc 0.98
2016-09-06T07:35:29.623243: step 14023, loss 0.0185046, acc 0.98
2016-09-06T07:35:30.404401: step 14024, loss 0.0662894, acc 0.96
2016-09-06T07:35:31.241991: step 14025, loss 0.0466465, acc 0.96
2016-09-06T07:35:32.056079: step 14026, loss 0.0272195, acc 0.98
2016-09-06T07:35:32.882268: step 14027, loss 0.0030677, acc 1
2016-09-06T07:35:33.697151: step 14028, loss 0.00259484, acc 1
2016-09-06T07:35:34.524343: step 14029, loss 0.0144388, acc 1
2016-09-06T07:35:35.334276: step 14030, loss 0.0107118, acc 1
2016-09-06T07:35:36.145310: step 14031, loss 0.10638, acc 0.96
2016-09-06T07:35:36.977357: step 14032, loss 0.00272814, acc 1
2016-09-06T07:35:37.793501: step 14033, loss 0.002427, acc 1
2016-09-06T07:35:38.586227: step 14034, loss 0.00238159, acc 1
2016-09-06T07:35:39.426490: step 14035, loss 0.0159001, acc 1
2016-09-06T07:35:40.222405: step 14036, loss 0.00246376, acc 1
2016-09-06T07:35:41.021356: step 14037, loss 0.00231954, acc 1
2016-09-06T07:35:41.867186: step 14038, loss 0.0136687, acc 1
2016-09-06T07:35:42.685222: step 14039, loss 0.0140487, acc 1
2016-09-06T07:35:43.512314: step 14040, loss 0.00575447, acc 1
2016-09-06T07:35:44.340314: step 14041, loss 0.0677348, acc 0.98
2016-09-06T07:35:45.178876: step 14042, loss 0.0304014, acc 0.98
2016-09-06T07:35:45.974038: step 14043, loss 0.0210523, acc 0.98
2016-09-06T07:35:46.829397: step 14044, loss 0.00962051, acc 1
2016-09-06T07:35:47.641080: step 14045, loss 0.00242954, acc 1
2016-09-06T07:35:48.456394: step 14046, loss 0.0279109, acc 0.98
2016-09-06T07:35:49.286019: step 14047, loss 0.0278246, acc 0.98
2016-09-06T07:35:50.129412: step 14048, loss 0.0022247, acc 1
2016-09-06T07:35:50.946316: step 14049, loss 0.00558239, acc 1
2016-09-06T07:35:51.778822: step 14050, loss 0.0217649, acc 0.98
2016-09-06T07:35:52.591556: step 14051, loss 0.00205477, acc 1
2016-09-06T07:35:53.373421: step 14052, loss 0.00999939, acc 1
2016-09-06T07:35:54.168358: step 14053, loss 0.0027644, acc 1
2016-09-06T07:35:55.004337: step 14054, loss 0.0186217, acc 0.98
2016-09-06T07:35:55.794186: step 14055, loss 0.0233086, acc 0.98
2016-09-06T07:35:56.626757: step 14056, loss 0.00283937, acc 1
2016-09-06T07:35:57.449085: step 14057, loss 0.0159595, acc 0.98
2016-09-06T07:35:58.246072: step 14058, loss 0.00900631, acc 1
2016-09-06T07:35:59.066256: step 14059, loss 0.0614957, acc 0.98
2016-09-06T07:35:59.885954: step 14060, loss 0.00190419, acc 1
2016-09-06T07:36:00.666374: step 14061, loss 0.00192514, acc 1
2016-09-06T07:36:01.523111: step 14062, loss 0.0112444, acc 1
2016-09-06T07:36:02.354113: step 14063, loss 0.00242783, acc 1
2016-09-06T07:36:03.178449: step 14064, loss 0.0288967, acc 0.98
2016-09-06T07:36:04.019324: step 14065, loss 0.0138901, acc 1
2016-09-06T07:36:04.858600: step 14066, loss 0.001956, acc 1
2016-09-06T07:36:05.657069: step 14067, loss 0.00802534, acc 1
2016-09-06T07:36:06.488436: step 14068, loss 0.0297991, acc 0.98
2016-09-06T07:36:07.339067: step 14069, loss 0.0301299, acc 0.98
2016-09-06T07:36:08.162380: step 14070, loss 0.00875557, acc 1
2016-09-06T07:36:08.979763: step 14071, loss 0.0369323, acc 0.98
2016-09-06T07:36:09.812003: step 14072, loss 0.00286051, acc 1
2016-09-06T07:36:10.618197: step 14073, loss 0.00179869, acc 1
2016-09-06T07:36:11.425707: step 14074, loss 0.0459758, acc 0.98
2016-09-06T07:36:12.253852: step 14075, loss 0.00364704, acc 1
2016-09-06T07:36:13.080682: step 14076, loss 0.00226426, acc 1
2016-09-06T07:36:13.867630: step 14077, loss 0.00270023, acc 1
2016-09-06T07:36:14.718520: step 14078, loss 0.067054, acc 0.98
2016-09-06T07:36:15.517932: step 14079, loss 0.00195643, acc 1
2016-09-06T07:36:16.315225: step 14080, loss 0.0020286, acc 1
2016-09-06T07:36:17.140826: step 14081, loss 0.00556842, acc 1
2016-09-06T07:36:17.966000: step 14082, loss 0.00273255, acc 1
2016-09-06T07:36:18.810979: step 14083, loss 0.00423668, acc 1
2016-09-06T07:36:19.652075: step 14084, loss 0.0168868, acc 1
2016-09-06T07:36:20.467030: step 14085, loss 0.0437663, acc 0.98
2016-09-06T07:36:21.219127: step 14086, loss 0.0236096, acc 1
2016-09-06T07:36:22.007656: step 14087, loss 0.00807591, acc 1
2016-09-06T07:36:22.852840: step 14088, loss 0.0263166, acc 0.98
2016-09-06T07:36:23.635952: step 14089, loss 0.00909223, acc 1
2016-09-06T07:36:24.452458: step 14090, loss 0.00610284, acc 1
2016-09-06T07:36:25.270969: step 14091, loss 0.00934071, acc 1
2016-09-06T07:36:26.018503: step 14092, loss 0.0142399, acc 1
2016-09-06T07:36:26.824955: step 14093, loss 0.00970731, acc 1
2016-09-06T07:36:27.625912: step 14094, loss 0.00578728, acc 1
2016-09-06T07:36:28.412652: step 14095, loss 0.00444601, acc 1
2016-09-06T07:36:29.223204: step 14096, loss 0.0161506, acc 1
2016-09-06T07:36:30.045587: step 14097, loss 0.0049128, acc 1
2016-09-06T07:36:30.876210: step 14098, loss 0.010905, acc 1
2016-09-06T07:36:31.734069: step 14099, loss 0.017451, acc 1
2016-09-06T07:36:32.552981: step 14100, loss 0.0168757, acc 0.98

Evaluation:
2016-09-06T07:36:36.256725: step 14100, loss 3.00031, acc 0.717636

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-14100

2016-09-06T07:36:38.207640: step 14101, loss 0.00254557, acc 1
2016-09-06T07:36:39.062446: step 14102, loss 0.0195633, acc 1
2016-09-06T07:36:39.898370: step 14103, loss 0.00211794, acc 1
2016-09-06T07:36:40.689659: step 14104, loss 0.0240082, acc 0.98
2016-09-06T07:36:41.505652: step 14105, loss 0.00246563, acc 1
2016-09-06T07:36:42.313615: step 14106, loss 0.0222315, acc 0.98
2016-09-06T07:36:43.089689: step 14107, loss 0.0374972, acc 0.98
2016-09-06T07:36:43.884426: step 14108, loss 0.00290505, acc 1
2016-09-06T07:36:44.725747: step 14109, loss 0.0136619, acc 1
2016-09-06T07:36:45.523308: step 14110, loss 0.00245876, acc 1
2016-09-06T07:36:46.319365: step 14111, loss 0.0475297, acc 0.96
2016-09-06T07:36:47.112707: step 14112, loss 0.00220389, acc 1
2016-09-06T07:36:47.915124: step 14113, loss 0.00220111, acc 1
2016-09-06T07:36:48.705037: step 14114, loss 0.0103294, acc 1
2016-09-06T07:36:49.528655: step 14115, loss 0.0214012, acc 0.98
2016-09-06T07:36:50.325838: step 14116, loss 0.0344756, acc 0.98
2016-09-06T07:36:51.137159: step 14117, loss 0.00511727, acc 1
2016-09-06T07:36:51.981676: step 14118, loss 0.0056058, acc 1
2016-09-06T07:36:52.750122: step 14119, loss 0.00222441, acc 1
2016-09-06T07:36:53.541937: step 14120, loss 0.0031372, acc 1
2016-09-06T07:36:54.370195: step 14121, loss 0.00671306, acc 1
2016-09-06T07:36:55.163155: step 14122, loss 0.00227789, acc 1
2016-09-06T07:36:55.959065: step 14123, loss 0.0480265, acc 0.96
2016-09-06T07:36:56.780945: step 14124, loss 0.0200225, acc 0.98
2016-09-06T07:36:57.572371: step 14125, loss 0.040661, acc 1
2016-09-06T07:36:58.393705: step 14126, loss 0.0328868, acc 0.98
2016-09-06T07:36:59.238230: step 14127, loss 0.00224943, acc 1
2016-09-06T07:37:00.029018: step 14128, loss 0.0165917, acc 0.98
2016-09-06T07:37:00.849623: step 14129, loss 0.022003, acc 0.98
2016-09-06T07:37:01.687389: step 14130, loss 0.0335323, acc 0.98
2016-09-06T07:37:02.497629: step 14131, loss 0.027716, acc 1
2016-09-06T07:37:03.301332: step 14132, loss 0.0289638, acc 0.98
2016-09-06T07:37:04.139541: step 14133, loss 0.0214013, acc 0.98
2016-09-06T07:37:04.946133: step 14134, loss 0.00210197, acc 1
2016-09-06T07:37:05.751275: step 14135, loss 0.0115398, acc 1
2016-09-06T07:37:06.598691: step 14136, loss 0.0237626, acc 0.98
2016-09-06T07:37:07.396916: step 14137, loss 0.00207381, acc 1
2016-09-06T07:37:08.208579: step 14138, loss 0.0024548, acc 1
2016-09-06T07:37:09.036151: step 14139, loss 0.0152362, acc 1
2016-09-06T07:37:09.842268: step 14140, loss 0.0455233, acc 0.98
2016-09-06T07:37:10.684558: step 14141, loss 0.00212091, acc 1
2016-09-06T07:37:11.533032: step 14142, loss 0.00237647, acc 1
2016-09-06T07:37:12.350375: step 14143, loss 0.00254045, acc 1
2016-09-06T07:37:13.166098: step 14144, loss 0.00905941, acc 1
2016-09-06T07:37:14.002915: step 14145, loss 0.0274674, acc 0.98
2016-09-06T07:37:14.821362: step 14146, loss 0.0111048, acc 1
2016-09-06T07:37:15.675487: step 14147, loss 0.0218129, acc 1
2016-09-06T07:37:16.508989: step 14148, loss 0.00197223, acc 1
2016-09-06T07:37:17.316032: step 14149, loss 0.00187661, acc 1
2016-09-06T07:37:18.124576: step 14150, loss 0.0128244, acc 1
2016-09-06T07:37:18.922831: step 14151, loss 0.0163865, acc 0.98
2016-09-06T07:37:19.778090: step 14152, loss 0.015471, acc 1
2016-09-06T07:37:20.557927: step 14153, loss 0.00185792, acc 1
2016-09-06T07:37:21.344787: step 14154, loss 0.0100589, acc 1
2016-09-06T07:37:22.178669: step 14155, loss 0.00185334, acc 1
2016-09-06T07:37:22.962570: step 14156, loss 0.010442, acc 1
2016-09-06T07:37:23.772498: step 14157, loss 0.0616934, acc 0.94
2016-09-06T07:37:24.570813: step 14158, loss 0.00188602, acc 1
2016-09-06T07:37:25.379755: step 14159, loss 0.00179139, acc 1
2016-09-06T07:37:26.177238: step 14160, loss 0.0235779, acc 1
2016-09-06T07:37:27.032742: step 14161, loss 0.00179171, acc 1
2016-09-06T07:37:27.819828: step 14162, loss 0.0293611, acc 0.98
2016-09-06T07:37:28.650195: step 14163, loss 0.00913182, acc 1
2016-09-06T07:37:29.465219: step 14164, loss 0.00914378, acc 1
2016-09-06T07:37:30.249998: step 14165, loss 0.00461315, acc 1
2016-09-06T07:37:31.031128: step 14166, loss 0.00497911, acc 1
2016-09-06T07:37:31.850072: step 14167, loss 0.0208274, acc 0.98
2016-09-06T07:37:32.643009: step 14168, loss 0.00194472, acc 1
2016-09-06T07:37:33.430155: step 14169, loss 0.00204771, acc 1
2016-09-06T07:37:34.241539: step 14170, loss 0.00183889, acc 1
2016-09-06T07:37:35.053508: step 14171, loss 0.00187747, acc 1
2016-09-06T07:37:35.868285: step 14172, loss 0.0181539, acc 1
2016-09-06T07:37:36.708938: step 14173, loss 0.00183205, acc 1
2016-09-06T07:37:37.507376: step 14174, loss 0.0022321, acc 1
2016-09-06T07:37:38.304573: step 14175, loss 0.00180681, acc 1
2016-09-06T07:37:39.116391: step 14176, loss 0.00880056, acc 1
2016-09-06T07:37:39.899155: step 14177, loss 0.0445571, acc 0.98
2016-09-06T07:37:40.717468: step 14178, loss 0.0029778, acc 1
2016-09-06T07:37:41.521061: step 14179, loss 0.00179075, acc 1
2016-09-06T07:37:42.285537: step 14180, loss 0.00181624, acc 1
2016-09-06T07:37:43.091073: step 14181, loss 0.0017884, acc 1
2016-09-06T07:37:43.881391: step 14182, loss 0.020865, acc 0.98
2016-09-06T07:37:44.700348: step 14183, loss 0.0320276, acc 0.98
2016-09-06T07:37:45.540394: step 14184, loss 0.0019804, acc 1
2016-09-06T07:37:46.375399: step 14185, loss 0.0858468, acc 0.96
2016-09-06T07:37:47.161582: step 14186, loss 0.00168605, acc 1
2016-09-06T07:37:47.945769: step 14187, loss 0.0446567, acc 0.98
2016-09-06T07:37:48.750077: step 14188, loss 0.0611475, acc 0.98
2016-09-06T07:37:49.539814: step 14189, loss 0.0227234, acc 0.98
2016-09-06T07:37:50.360719: step 14190, loss 0.0177271, acc 1
2016-09-06T07:37:51.185534: step 14191, loss 0.0209851, acc 1
2016-09-06T07:37:51.975063: step 14192, loss 0.00242329, acc 1
2016-09-06T07:37:52.780641: step 14193, loss 0.0478738, acc 0.98
2016-09-06T07:37:53.626043: step 14194, loss 0.0286697, acc 0.98
2016-09-06T07:37:54.425591: step 14195, loss 0.00802948, acc 1
2016-09-06T07:37:55.204055: step 14196, loss 0.00801832, acc 1
2016-09-06T07:37:56.025414: step 14197, loss 0.00456339, acc 1
2016-09-06T07:37:56.800724: step 14198, loss 0.00199652, acc 1
2016-09-06T07:37:57.617990: step 14199, loss 0.00743928, acc 1
2016-09-06T07:37:58.427842: step 14200, loss 0.00415407, acc 1

Evaluation:
2016-09-06T07:38:02.167212: step 14200, loss 2.31411, acc 0.730769

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-14200

2016-09-06T07:38:04.040449: step 14201, loss 0.0462122, acc 0.98
2016-09-06T07:38:04.892483: step 14202, loss 0.0273372, acc 0.98
2016-09-06T07:38:05.709897: step 14203, loss 0.0107471, acc 1
2016-09-06T07:38:06.481435: step 14204, loss 0.00152825, acc 1
2016-09-06T07:38:07.289112: step 14205, loss 0.0474058, acc 0.98
2016-09-06T07:38:08.115025: step 14206, loss 0.0272907, acc 0.98
2016-09-06T07:38:08.910765: step 14207, loss 0.00242848, acc 1
2016-09-06T07:38:09.698371: step 14208, loss 0.00273659, acc 1
2016-09-06T07:38:10.531934: step 14209, loss 0.0120784, acc 1
2016-09-06T07:38:11.339376: step 14210, loss 0.0497769, acc 0.98
2016-09-06T07:38:12.177524: step 14211, loss 0.00159869, acc 1
2016-09-06T07:38:12.994250: step 14212, loss 0.0972057, acc 0.98
2016-09-06T07:38:13.821028: step 14213, loss 0.00694931, acc 1
2016-09-06T07:38:14.630455: step 14214, loss 0.0207804, acc 1
2016-09-06T07:38:15.445298: step 14215, loss 0.00530852, acc 1
2016-09-06T07:38:16.282907: step 14216, loss 0.00591211, acc 1
2016-09-06T07:38:17.124904: step 14217, loss 0.027738, acc 0.98
2016-09-06T07:38:17.928982: step 14218, loss 0.00339054, acc 1
2016-09-06T07:38:18.746629: step 14219, loss 0.0203695, acc 0.98
2016-09-06T07:38:19.555457: step 14220, loss 0.00283798, acc 1
2016-09-06T07:38:20.380478: step 14221, loss 0.00527501, acc 1
2016-09-06T07:38:21.200407: step 14222, loss 0.00700524, acc 1
2016-09-06T07:38:22.033616: step 14223, loss 0.0177703, acc 0.98
2016-09-06T07:38:22.878244: step 14224, loss 0.00236376, acc 1
2016-09-06T07:38:23.697766: step 14225, loss 0.00763092, acc 1
2016-09-06T07:38:24.507197: step 14226, loss 0.0063996, acc 1
2016-09-06T07:38:25.333928: step 14227, loss 0.0105188, acc 1
2016-09-06T07:38:26.138701: step 14228, loss 0.00210895, acc 1
2016-09-06T07:38:26.956337: step 14229, loss 0.0153525, acc 1
2016-09-06T07:38:27.796173: step 14230, loss 0.00300704, acc 1
2016-09-06T07:38:28.569928: step 14231, loss 0.0143366, acc 1
2016-09-06T07:38:29.365155: step 14232, loss 0.00194155, acc 1
2016-09-06T07:38:30.175768: step 14233, loss 0.00694513, acc 1
2016-09-06T07:38:30.998664: step 14234, loss 0.0261185, acc 1
2016-09-06T07:38:31.826264: step 14235, loss 0.00250009, acc 1
2016-09-06T07:38:32.645247: step 14236, loss 0.00289755, acc 1
2016-09-06T07:38:33.450171: step 14237, loss 0.0145535, acc 1
2016-09-06T07:38:34.285349: step 14238, loss 0.00218562, acc 1
2016-09-06T07:38:35.080099: step 14239, loss 0.00385137, acc 1
2016-09-06T07:38:35.895647: step 14240, loss 0.0105463, acc 1
2016-09-06T07:38:36.698630: step 14241, loss 0.00580864, acc 1
2016-09-06T07:38:37.500917: step 14242, loss 0.0125974, acc 1
2016-09-06T07:38:38.331577: step 14243, loss 0.002363, acc 1
2016-09-06T07:38:39.129193: step 14244, loss 0.129551, acc 0.98
2016-09-06T07:38:39.952404: step 14245, loss 0.00207449, acc 1
2016-09-06T07:38:40.798087: step 14246, loss 0.0192836, acc 0.98
2016-09-06T07:38:41.581553: step 14247, loss 0.00208012, acc 1
2016-09-06T07:38:42.385769: step 14248, loss 0.00191612, acc 1
2016-09-06T07:38:43.224799: step 14249, loss 0.00551157, acc 1
2016-09-06T07:38:44.029297: step 14250, loss 0.00689289, acc 1
2016-09-06T07:38:44.836479: step 14251, loss 0.0506018, acc 0.98
2016-09-06T07:38:45.658993: step 14252, loss 0.00251371, acc 1
2016-09-06T07:38:46.468216: step 14253, loss 0.00191215, acc 1
2016-09-06T07:38:47.277256: step 14254, loss 0.00381019, acc 1
2016-09-06T07:38:48.085746: step 14255, loss 0.0128587, acc 1
2016-09-06T07:38:48.921409: step 14256, loss 0.0497645, acc 0.98
2016-09-06T07:38:49.729639: step 14257, loss 0.00768299, acc 1
2016-09-06T07:38:50.576062: step 14258, loss 0.00221286, acc 1
2016-09-06T07:38:51.395597: step 14259, loss 0.00615637, acc 1
2016-09-06T07:38:52.219256: step 14260, loss 0.0670903, acc 0.94
2016-09-06T07:38:53.074889: step 14261, loss 0.0158608, acc 1
2016-09-06T07:38:53.886585: step 14262, loss 0.00547312, acc 1
2016-09-06T07:38:54.698851: step 14263, loss 0.0174725, acc 1
2016-09-06T07:38:55.558043: step 14264, loss 0.00202993, acc 1
2016-09-06T07:38:56.408472: step 14265, loss 0.00207891, acc 1
2016-09-06T07:38:57.203909: step 14266, loss 0.0181334, acc 1
2016-09-06T07:38:58.024656: step 14267, loss 0.00341862, acc 1
2016-09-06T07:38:58.880150: step 14268, loss 0.00277971, acc 1
2016-09-06T07:38:59.679886: step 14269, loss 0.016991, acc 0.98
2016-09-06T07:39:00.526518: step 14270, loss 0.0293061, acc 0.98
2016-09-06T07:39:01.326418: step 14271, loss 0.00183116, acc 1
2016-09-06T07:39:02.098208: step 14272, loss 0.0162457, acc 0.98
2016-09-06T07:39:02.913156: step 14273, loss 0.00688808, acc 1
2016-09-06T07:39:03.716297: step 14274, loss 0.00191969, acc 1
2016-09-06T07:39:04.525435: step 14275, loss 0.0154843, acc 1
2016-09-06T07:39:05.317298: step 14276, loss 0.026007, acc 1
2016-09-06T07:39:06.157412: step 14277, loss 0.0416705, acc 0.98
2016-09-06T07:39:06.985374: step 14278, loss 0.024991, acc 0.98
2016-09-06T07:39:07.814193: step 14279, loss 0.0293653, acc 0.98
2016-09-06T07:39:08.648899: step 14280, loss 0.00929268, acc 1
2016-09-06T07:39:09.476005: step 14281, loss 0.0197981, acc 0.98
2016-09-06T07:39:10.281094: step 14282, loss 0.00942223, acc 1
2016-09-06T07:39:11.105348: step 14283, loss 0.0183627, acc 1
2016-09-06T07:39:11.912843: step 14284, loss 0.0155217, acc 1
2016-09-06T07:39:12.719049: step 14285, loss 0.0145694, acc 1
2016-09-06T07:39:13.530628: step 14286, loss 0.0518037, acc 0.98
2016-09-06T07:39:14.329286: step 14287, loss 0.0110617, acc 1
2016-09-06T07:39:15.132566: step 14288, loss 0.0485169, acc 0.98
2016-09-06T07:39:15.962350: step 14289, loss 0.00321169, acc 1
2016-09-06T07:39:16.786142: step 14290, loss 0.00349626, acc 1
2016-09-06T07:39:17.583102: step 14291, loss 0.00902081, acc 1
2016-09-06T07:39:18.417854: step 14292, loss 0.00175675, acc 1
2016-09-06T07:39:19.241388: step 14293, loss 0.0152253, acc 1
2016-09-06T07:39:20.070820: step 14294, loss 0.00767835, acc 1
2016-09-06T07:39:20.920583: step 14295, loss 0.00232787, acc 1
2016-09-06T07:39:21.777790: step 14296, loss 0.00481822, acc 1
2016-09-06T07:39:22.569703: step 14297, loss 0.012457, acc 1
2016-09-06T07:39:23.412189: step 14298, loss 0.0116937, acc 1
2016-09-06T07:39:24.229823: step 14299, loss 0.0125551, acc 1
2016-09-06T07:39:25.031733: step 14300, loss 0.00184117, acc 1

Evaluation:
2016-09-06T07:39:28.767105: step 14300, loss 2.68174, acc 0.716698

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-14300

2016-09-06T07:39:30.727488: step 14301, loss 0.00388787, acc 1
2016-09-06T07:39:31.559496: step 14302, loss 0.0568318, acc 0.96
2016-09-06T07:39:32.375655: step 14303, loss 0.0226859, acc 0.98
2016-09-06T07:39:33.187191: step 14304, loss 0.00188495, acc 1
2016-09-06T07:39:33.988258: step 14305, loss 0.00201795, acc 1
2016-09-06T07:39:34.789675: step 14306, loss 0.0186432, acc 1
2016-09-06T07:39:35.573396: step 14307, loss 0.0144007, acc 1
2016-09-06T07:39:36.407436: step 14308, loss 0.00242263, acc 1
2016-09-06T07:39:37.223503: step 14309, loss 0.0558186, acc 0.98
2016-09-06T07:39:38.041841: step 14310, loss 0.0154283, acc 1
2016-09-06T07:39:38.903224: step 14311, loss 0.0101248, acc 1
2016-09-06T07:39:39.724387: step 14312, loss 0.0019202, acc 1
2016-09-06T07:39:40.545656: step 14313, loss 0.0164984, acc 0.98
2016-09-06T07:39:41.369412: step 14314, loss 0.00943742, acc 1
2016-09-06T07:39:42.177431: step 14315, loss 0.0313967, acc 1
2016-09-06T07:39:42.985328: step 14316, loss 0.0112149, acc 1
2016-09-06T07:39:43.808405: step 14317, loss 0.0390167, acc 0.96
2016-09-06T07:39:44.628602: step 14318, loss 0.0266247, acc 0.98
2016-09-06T07:39:45.459276: step 14319, loss 0.00195357, acc 1
2016-09-06T07:39:46.293550: step 14320, loss 0.132921, acc 0.98
2016-09-06T07:39:47.142920: step 14321, loss 0.118863, acc 0.98
2016-09-06T07:39:47.979045: step 14322, loss 0.00257083, acc 1
2016-09-06T07:39:48.835417: step 14323, loss 0.0279158, acc 1
2016-09-06T07:39:49.656218: step 14324, loss 0.0522808, acc 0.98
2016-09-06T07:39:50.458354: step 14325, loss 0.00730077, acc 1
2016-09-06T07:39:51.258480: step 14326, loss 0.00172522, acc 1
2016-09-06T07:39:52.073193: step 14327, loss 0.00773956, acc 1
2016-09-06T07:39:52.849882: step 14328, loss 0.0112528, acc 1
2016-09-06T07:39:53.641908: step 14329, loss 0.00279613, acc 1
2016-09-06T07:39:54.458135: step 14330, loss 0.0115492, acc 1
2016-09-06T07:39:55.249209: step 14331, loss 0.0210779, acc 0.98
2016-09-06T07:39:56.087719: step 14332, loss 0.0679602, acc 0.96
2016-09-06T07:39:56.910307: step 14333, loss 0.0398242, acc 0.98
2016-09-06T07:39:57.698238: step 14334, loss 0.00689201, acc 1
2016-09-06T07:39:58.531728: step 14335, loss 0.00774409, acc 1
2016-09-06T07:39:59.347013: step 14336, loss 0.00301503, acc 1
2016-09-06T07:40:00.127263: step 14337, loss 0.0312709, acc 0.98
2016-09-06T07:40:00.967911: step 14338, loss 0.0143461, acc 1
2016-09-06T07:40:01.784748: step 14339, loss 0.0669558, acc 0.98
2016-09-06T07:40:02.608139: step 14340, loss 0.00448676, acc 1
2016-09-06T07:40:03.409310: step 14341, loss 0.00195055, acc 1
2016-09-06T07:40:04.227478: step 14342, loss 0.0274003, acc 0.98
2016-09-06T07:40:05.031700: step 14343, loss 0.00947181, acc 1
2016-09-06T07:40:05.842451: step 14344, loss 0.00262406, acc 1
2016-09-06T07:40:06.661640: step 14345, loss 0.00569508, acc 1
2016-09-06T07:40:07.472588: step 14346, loss 0.00579697, acc 1
2016-09-06T07:40:08.287987: step 14347, loss 0.00208401, acc 1
2016-09-06T07:40:09.111762: step 14348, loss 0.0767, acc 0.98
2016-09-06T07:40:09.910463: step 14349, loss 0.0153977, acc 1
2016-09-06T07:40:10.738921: step 14350, loss 0.00323582, acc 1
2016-09-06T07:40:11.553225: step 14351, loss 0.0030133, acc 1
2016-09-06T07:40:12.341748: step 14352, loss 0.0751816, acc 0.94
2016-09-06T07:40:13.169928: step 14353, loss 0.0333925, acc 0.98
2016-09-06T07:40:14.011730: step 14354, loss 0.0210593, acc 0.98
2016-09-06T07:40:14.859789: step 14355, loss 0.00919599, acc 1
2016-09-06T07:40:15.665137: step 14356, loss 0.0165633, acc 1
2016-09-06T07:40:16.484107: step 14357, loss 0.0148799, acc 1
2016-09-06T07:40:17.276906: step 14358, loss 0.0474623, acc 0.96
2016-09-06T07:40:18.099584: step 14359, loss 0.0251743, acc 0.98
2016-09-06T07:40:18.908636: step 14360, loss 0.00216163, acc 1
2016-09-06T07:40:19.745005: step 14361, loss 0.0168697, acc 1
2016-09-06T07:40:20.557487: step 14362, loss 0.00243684, acc 1
2016-09-06T07:40:21.412325: step 14363, loss 0.0174184, acc 0.98
2016-09-06T07:40:22.213418: step 14364, loss 0.0209707, acc 0.98
2016-09-06T07:40:23.012163: step 14365, loss 0.0244391, acc 0.98
2016-09-06T07:40:23.853531: step 14366, loss 0.00264112, acc 1
2016-09-06T07:40:24.708038: step 14367, loss 0.00281262, acc 1
2016-09-06T07:40:25.527344: step 14368, loss 0.0165581, acc 1
2016-09-06T07:40:26.351602: step 14369, loss 0.0613301, acc 0.98
2016-09-06T07:40:27.180721: step 14370, loss 0.0193367, acc 1
2016-09-06T07:40:27.975896: step 14371, loss 0.0140333, acc 1
2016-09-06T07:40:28.786035: step 14372, loss 0.00218721, acc 1
2016-09-06T07:40:29.624745: step 14373, loss 0.019515, acc 0.98
2016-09-06T07:40:30.420439: step 14374, loss 0.0149462, acc 1
2016-09-06T07:40:31.235048: step 14375, loss 0.0193408, acc 0.98
2016-09-06T07:40:32.050299: step 14376, loss 0.00301828, acc 1
2016-09-06T07:40:32.861698: step 14377, loss 0.0247186, acc 0.98
2016-09-06T07:40:33.668986: step 14378, loss 0.0100141, acc 1
2016-09-06T07:40:34.530951: step 14379, loss 0.00262804, acc 1
2016-09-06T07:40:35.316867: step 14380, loss 0.0384307, acc 0.96
2016-09-06T07:40:36.147224: step 14381, loss 0.00457192, acc 1
2016-09-06T07:40:36.989334: step 14382, loss 0.0166453, acc 0.98
2016-09-06T07:40:37.807615: step 14383, loss 0.0260965, acc 0.98
2016-09-06T07:40:38.613813: step 14384, loss 0.00593268, acc 1
2016-09-06T07:40:39.463653: step 14385, loss 0.0114403, acc 1
2016-09-06T07:40:40.257646: step 14386, loss 0.0656496, acc 0.96
2016-09-06T07:40:41.079465: step 14387, loss 0.00600643, acc 1
2016-09-06T07:40:41.909520: step 14388, loss 0.0205982, acc 0.98
2016-09-06T07:40:42.723799: step 14389, loss 0.00265777, acc 1
2016-09-06T07:40:43.527589: step 14390, loss 0.0736407, acc 0.96
2016-09-06T07:40:44.359771: step 14391, loss 0.00235983, acc 1
2016-09-06T07:40:45.194207: step 14392, loss 0.0688358, acc 0.98
2016-09-06T07:40:45.990171: step 14393, loss 0.00217228, acc 1
2016-09-06T07:40:46.826692: step 14394, loss 0.0276676, acc 1
2016-09-06T07:40:47.632202: step 14395, loss 0.00205043, acc 1
2016-09-06T07:40:48.425097: step 14396, loss 0.0022157, acc 1
2016-09-06T07:40:49.248989: step 14397, loss 0.00750068, acc 1
2016-09-06T07:40:50.093476: step 14398, loss 0.00390405, acc 1
2016-09-06T07:40:50.938099: step 14399, loss 0.0208038, acc 1
2016-09-06T07:40:51.692230: step 14400, loss 0.0187355, acc 0.977273

Evaluation:
2016-09-06T07:40:55.460503: step 14400, loss 2.5203, acc 0.714822

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-14400

2016-09-06T07:40:57.425950: step 14401, loss 0.010843, acc 1
2016-09-06T07:40:58.241446: step 14402, loss 0.0033315, acc 1
2016-09-06T07:40:59.076554: step 14403, loss 0.0126187, acc 1
2016-09-06T07:40:59.929378: step 14404, loss 0.00236657, acc 1
2016-09-06T07:41:00.769205: step 14405, loss 0.0393858, acc 0.98
2016-09-06T07:41:01.594926: step 14406, loss 0.0291336, acc 0.98
2016-09-06T07:41:02.453971: step 14407, loss 0.0024212, acc 1
2016-09-06T07:41:03.274871: step 14408, loss 0.00504355, acc 1
2016-09-06T07:41:04.091328: step 14409, loss 0.0340735, acc 0.98
2016-09-06T07:41:04.933806: step 14410, loss 0.0549195, acc 0.98
2016-09-06T07:41:05.723186: step 14411, loss 0.0131899, acc 1
2016-09-06T07:41:06.528069: step 14412, loss 0.0174152, acc 0.98
2016-09-06T07:41:07.372116: step 14413, loss 0.0379954, acc 1
2016-09-06T07:41:08.198109: step 14414, loss 0.044975, acc 0.98
2016-09-06T07:41:09.008053: step 14415, loss 0.0612593, acc 0.96
2016-09-06T07:41:09.861592: step 14416, loss 0.0079676, acc 1
2016-09-06T07:41:10.680381: step 14417, loss 0.00201171, acc 1
2016-09-06T07:41:11.490624: step 14418, loss 0.00251991, acc 1
2016-09-06T07:41:12.316145: step 14419, loss 0.0145416, acc 1
2016-09-06T07:41:13.118498: step 14420, loss 0.00211284, acc 1
2016-09-06T07:41:13.910708: step 14421, loss 0.00498574, acc 1
2016-09-06T07:41:14.748629: step 14422, loss 0.0268278, acc 1
2016-09-06T07:41:15.577069: step 14423, loss 0.00247952, acc 1
2016-09-06T07:41:16.393624: step 14424, loss 0.0319101, acc 0.98
2016-09-06T07:41:17.187696: step 14425, loss 0.011448, acc 1
2016-09-06T07:41:18.027213: step 14426, loss 0.0261277, acc 1
2016-09-06T07:41:18.858679: step 14427, loss 0.00513776, acc 1
2016-09-06T07:41:19.686696: step 14428, loss 0.00329027, acc 1
2016-09-06T07:41:20.533435: step 14429, loss 0.00570775, acc 1
2016-09-06T07:41:21.352031: step 14430, loss 0.00218563, acc 1
2016-09-06T07:41:22.184439: step 14431, loss 0.160236, acc 0.98
2016-09-06T07:41:23.008796: step 14432, loss 0.0155566, acc 1
2016-09-06T07:41:23.820958: step 14433, loss 0.0018732, acc 1
2016-09-06T07:41:24.636216: step 14434, loss 0.00251345, acc 1
2016-09-06T07:41:25.482068: step 14435, loss 0.00437832, acc 1
2016-09-06T07:41:26.278229: step 14436, loss 0.0100368, acc 1
2016-09-06T07:41:27.089556: step 14437, loss 0.00177122, acc 1
2016-09-06T07:41:27.908259: step 14438, loss 0.0177603, acc 0.98
2016-09-06T07:41:28.697851: step 14439, loss 0.102488, acc 0.98
2016-09-06T07:41:29.527527: step 14440, loss 0.00183838, acc 1
2016-09-06T07:41:30.345644: step 14441, loss 0.0063704, acc 1
2016-09-06T07:41:31.169511: step 14442, loss 0.00920339, acc 1
2016-09-06T07:41:31.986056: step 14443, loss 0.0298054, acc 0.98
2016-09-06T07:41:32.831318: step 14444, loss 0.00264147, acc 1
2016-09-06T07:41:33.647173: step 14445, loss 0.00190592, acc 1
2016-09-06T07:41:34.444411: step 14446, loss 0.00831044, acc 1
2016-09-06T07:41:35.261677: step 14447, loss 0.0106734, acc 1
2016-09-06T07:41:36.077403: step 14448, loss 0.0105764, acc 1
2016-09-06T07:41:36.870250: step 14449, loss 0.013715, acc 1
2016-09-06T07:41:37.691689: step 14450, loss 0.00230183, acc 1
2016-09-06T07:41:38.503901: step 14451, loss 0.0168428, acc 1
2016-09-06T07:41:39.285646: step 14452, loss 0.016345, acc 1
2016-09-06T07:41:40.097474: step 14453, loss 0.0308231, acc 0.98
2016-09-06T07:41:40.937305: step 14454, loss 0.00321846, acc 1
2016-09-06T07:41:41.749049: step 14455, loss 0.0150678, acc 1
2016-09-06T07:41:42.539607: step 14456, loss 0.00290535, acc 1
2016-09-06T07:41:43.355339: step 14457, loss 0.00485913, acc 1
2016-09-06T07:41:44.144460: step 14458, loss 0.058468, acc 0.96
2016-09-06T07:41:44.952675: step 14459, loss 0.0152501, acc 1
2016-09-06T07:41:45.821189: step 14460, loss 0.00278441, acc 1
2016-09-06T07:41:46.638267: step 14461, loss 0.0599732, acc 0.98
2016-09-06T07:41:47.448806: step 14462, loss 0.00659906, acc 1
2016-09-06T07:41:48.271956: step 14463, loss 0.00584448, acc 1
2016-09-06T07:41:49.070143: step 14464, loss 0.0185194, acc 0.98
2016-09-06T07:41:49.880561: step 14465, loss 0.0119681, acc 1
2016-09-06T07:41:50.710999: step 14466, loss 0.00858663, acc 1
2016-09-06T07:41:51.525661: step 14467, loss 0.126963, acc 0.96
2016-09-06T07:41:52.338198: step 14468, loss 0.0167241, acc 1
2016-09-06T07:41:53.175234: step 14469, loss 0.00999774, acc 1
2016-09-06T07:41:53.977677: step 14470, loss 0.00271454, acc 1
2016-09-06T07:41:54.777593: step 14471, loss 0.00765205, acc 1
2016-09-06T07:41:55.615119: step 14472, loss 0.00267913, acc 1
2016-09-06T07:41:56.443381: step 14473, loss 0.00722048, acc 1
2016-09-06T07:41:57.267407: step 14474, loss 0.00276528, acc 1
2016-09-06T07:41:58.101938: step 14475, loss 0.0177323, acc 0.98
2016-09-06T07:41:58.915372: step 14476, loss 0.0165945, acc 1
2016-09-06T07:41:59.708912: step 14477, loss 0.00756233, acc 1
2016-09-06T07:42:00.541755: step 14478, loss 0.0044353, acc 1
2016-09-06T07:42:01.365503: step 14479, loss 0.00283467, acc 1
2016-09-06T07:42:02.173488: step 14480, loss 0.00464947, acc 1
2016-09-06T07:42:03.004697: step 14481, loss 0.00309845, acc 1
2016-09-06T07:42:03.887954: step 14482, loss 0.0240553, acc 0.98
2016-09-06T07:42:04.669861: step 14483, loss 0.169833, acc 0.96
2016-09-06T07:42:05.521036: step 14484, loss 0.216706, acc 0.98
2016-09-06T07:42:06.370953: step 14485, loss 0.00305077, acc 1
2016-09-06T07:42:07.146741: step 14486, loss 0.0164911, acc 1
2016-09-06T07:42:07.961421: step 14487, loss 0.00288872, acc 1
2016-09-06T07:42:08.778475: step 14488, loss 0.00438259, acc 1
2016-09-06T07:42:09.560502: step 14489, loss 0.0441408, acc 0.98
2016-09-06T07:42:10.362738: step 14490, loss 0.0067626, acc 1
2016-09-06T07:42:11.178243: step 14491, loss 0.00849318, acc 1
2016-09-06T07:42:11.954462: step 14492, loss 0.010872, acc 1
2016-09-06T07:42:12.766807: step 14493, loss 0.00460071, acc 1
2016-09-06T07:42:13.595556: step 14494, loss 0.0155654, acc 1
2016-09-06T07:42:14.388556: step 14495, loss 0.0202238, acc 0.98
2016-09-06T07:42:15.231426: step 14496, loss 0.00500054, acc 1
2016-09-06T07:42:16.029068: step 14497, loss 0.00669319, acc 1
2016-09-06T07:42:16.800674: step 14498, loss 0.00843999, acc 1
2016-09-06T07:42:17.610460: step 14499, loss 0.0429366, acc 0.98
2016-09-06T07:42:18.441654: step 14500, loss 0.00905654, acc 1

Evaluation:
2016-09-06T07:42:22.130984: step 14500, loss 3.72411, acc 0.704503

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-14500

2016-09-06T07:42:24.004106: step 14501, loss 0.0148741, acc 1
2016-09-06T07:42:24.856622: step 14502, loss 0.00558707, acc 1
2016-09-06T07:42:25.672451: step 14503, loss 0.00483784, acc 1
2016-09-06T07:42:26.489680: step 14504, loss 0.0267313, acc 0.98
2016-09-06T07:42:27.337630: step 14505, loss 0.0173308, acc 1
2016-09-06T07:42:28.148445: step 14506, loss 0.00522976, acc 1
2016-09-06T07:42:28.973299: step 14507, loss 0.0542933, acc 0.94
2016-09-06T07:42:29.849659: step 14508, loss 0.0589114, acc 0.98
2016-09-06T07:42:30.666699: step 14509, loss 0.0301238, acc 0.98
2016-09-06T07:42:31.457063: step 14510, loss 0.0185117, acc 1
2016-09-06T07:42:32.258143: step 14511, loss 0.0182685, acc 1
2016-09-06T07:42:33.081655: step 14512, loss 0.0218985, acc 1
2016-09-06T07:42:33.871365: step 14513, loss 0.0138221, acc 1
2016-09-06T07:42:34.688927: step 14514, loss 0.00497352, acc 1
2016-09-06T07:42:35.511449: step 14515, loss 0.0118406, acc 1
2016-09-06T07:42:36.320807: step 14516, loss 0.00501804, acc 1
2016-09-06T07:42:37.129328: step 14517, loss 0.0257896, acc 0.98
2016-09-06T07:42:37.922360: step 14518, loss 0.00491907, acc 1
2016-09-06T07:42:38.713822: step 14519, loss 0.0194089, acc 0.98
2016-09-06T07:42:39.508986: step 14520, loss 0.00803215, acc 1
2016-09-06T07:42:40.330285: step 14521, loss 0.00486231, acc 1
2016-09-06T07:42:41.111785: step 14522, loss 0.00488991, acc 1
2016-09-06T07:42:41.930647: step 14523, loss 0.00481691, acc 1
2016-09-06T07:42:42.725673: step 14524, loss 0.00866257, acc 1
2016-09-06T07:42:43.492910: step 14525, loss 0.0514657, acc 0.98
2016-09-06T07:42:44.312404: step 14526, loss 0.00626267, acc 1
2016-09-06T07:42:45.164105: step 14527, loss 0.00463303, acc 1
2016-09-06T07:42:45.945685: step 14528, loss 0.0125219, acc 1
2016-09-06T07:42:46.783498: step 14529, loss 0.00501828, acc 1
2016-09-06T07:42:47.613550: step 14530, loss 0.0381974, acc 0.96
2016-09-06T07:42:48.423290: step 14531, loss 0.00444594, acc 1
2016-09-06T07:42:49.230115: step 14532, loss 0.00445706, acc 1
2016-09-06T07:42:50.037492: step 14533, loss 0.017202, acc 1
2016-09-06T07:42:50.898620: step 14534, loss 0.037629, acc 0.96
2016-09-06T07:42:51.703394: step 14535, loss 0.0167121, acc 1
2016-09-06T07:42:52.516066: step 14536, loss 0.00428923, acc 1
2016-09-06T07:42:53.316321: step 14537, loss 0.0164492, acc 1
2016-09-06T07:42:54.124987: step 14538, loss 0.00847368, acc 1
2016-09-06T07:42:54.950331: step 14539, loss 0.00424978, acc 1
2016-09-06T07:42:55.760048: step 14540, loss 0.00405584, acc 1
2016-09-06T07:42:56.574425: step 14541, loss 0.0218965, acc 0.98
2016-09-06T07:42:57.441832: step 14542, loss 0.0039672, acc 1
2016-09-06T07:42:58.253657: step 14543, loss 0.00391817, acc 1
2016-09-06T07:42:59.054213: step 14544, loss 0.0256, acc 1
2016-09-06T07:42:59.881822: step 14545, loss 0.00384185, acc 1
2016-09-06T07:43:00.696406: step 14546, loss 0.00775158, acc 1
2016-09-06T07:43:01.499074: step 14547, loss 0.00387838, acc 1
2016-09-06T07:43:02.328549: step 14548, loss 0.00374468, acc 1
2016-09-06T07:43:03.135608: step 14549, loss 0.0042273, acc 1
2016-09-06T07:43:03.953425: step 14550, loss 0.0833736, acc 0.98
2016-09-06T07:43:04.805343: step 14551, loss 0.00357503, acc 1
2016-09-06T07:43:05.595205: step 14552, loss 0.0233123, acc 0.98
2016-09-06T07:43:06.398555: step 14553, loss 0.0118209, acc 1
2016-09-06T07:43:07.216059: step 14554, loss 0.00337131, acc 1
2016-09-06T07:43:08.019649: step 14555, loss 0.0033558, acc 1
2016-09-06T07:43:08.812023: step 14556, loss 0.0430013, acc 0.98
2016-09-06T07:43:09.637393: step 14557, loss 0.0045769, acc 1
2016-09-06T07:43:10.440566: step 14558, loss 0.00977729, acc 1
2016-09-06T07:43:11.242362: step 14559, loss 0.00377157, acc 1
2016-09-06T07:43:12.081049: step 14560, loss 0.0246081, acc 1
2016-09-06T07:43:12.901420: step 14561, loss 0.0809435, acc 0.96
2016-09-06T07:43:13.682043: step 14562, loss 0.0136381, acc 1
2016-09-06T07:43:14.504682: step 14563, loss 0.00385597, acc 1
2016-09-06T07:43:15.317386: step 14564, loss 0.0186494, acc 0.98
2016-09-06T07:43:16.110576: step 14565, loss 0.00318415, acc 1
2016-09-06T07:43:16.912215: step 14566, loss 0.0177684, acc 0.98
2016-09-06T07:43:17.727000: step 14567, loss 0.002899, acc 1
2016-09-06T07:43:18.488119: step 14568, loss 0.0156195, acc 1
2016-09-06T07:43:19.315736: step 14569, loss 0.0041475, acc 1
2016-09-06T07:43:20.140810: step 14570, loss 0.00285922, acc 1
2016-09-06T07:43:20.933930: step 14571, loss 0.00870852, acc 1
2016-09-06T07:43:21.760744: step 14572, loss 0.0138239, acc 1
2016-09-06T07:43:22.589662: step 14573, loss 0.129925, acc 0.98
2016-09-06T07:43:23.384983: step 14574, loss 0.0271573, acc 0.98
2016-09-06T07:43:24.217423: step 14575, loss 0.0279295, acc 1
2016-09-06T07:43:25.050468: step 14576, loss 0.00343985, acc 1
2016-09-06T07:43:25.838031: step 14577, loss 0.00352233, acc 1
2016-09-06T07:43:26.651895: step 14578, loss 0.0428446, acc 0.98
2016-09-06T07:43:27.471740: step 14579, loss 0.0994433, acc 0.96
2016-09-06T07:43:28.268493: step 14580, loss 0.0223503, acc 1
2016-09-06T07:43:29.062952: step 14581, loss 0.00251074, acc 1
2016-09-06T07:43:29.864941: step 14582, loss 0.0437814, acc 0.98
2016-09-06T07:43:30.637020: step 14583, loss 0.0353499, acc 0.98
2016-09-06T07:43:31.433675: step 14584, loss 0.0181474, acc 0.98
2016-09-06T07:43:32.299938: step 14585, loss 0.00564321, acc 1
2016-09-06T07:43:33.103342: step 14586, loss 0.0181948, acc 0.98
2016-09-06T07:43:33.915769: step 14587, loss 0.0226121, acc 1
2016-09-06T07:43:34.732652: step 14588, loss 0.00387148, acc 1
2016-09-06T07:43:35.519919: step 14589, loss 0.163922, acc 0.96
2016-09-06T07:43:36.319012: step 14590, loss 0.0120202, acc 1
2016-09-06T07:43:37.132645: step 14591, loss 0.0197778, acc 1
2016-09-06T07:43:37.852758: step 14592, loss 0.0106653, acc 1
2016-09-06T07:43:38.658755: step 14593, loss 0.0261257, acc 1
2016-09-06T07:43:39.510226: step 14594, loss 0.0439013, acc 0.96
2016-09-06T07:43:40.298637: step 14595, loss 0.00497923, acc 1
2016-09-06T07:43:41.102218: step 14596, loss 0.0227194, acc 0.98
2016-09-06T07:43:41.917812: step 14597, loss 0.0106842, acc 1
2016-09-06T07:43:42.702803: step 14598, loss 0.0399052, acc 0.96
2016-09-06T07:43:43.514599: step 14599, loss 0.00591651, acc 1
2016-09-06T07:43:44.341728: step 14600, loss 0.0154999, acc 1

Evaluation:
2016-09-06T07:43:48.047409: step 14600, loss 3.19171, acc 0.719512

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-14600

2016-09-06T07:43:49.940852: step 14601, loss 0.0125072, acc 1
2016-09-06T07:43:50.766912: step 14602, loss 0.0343923, acc 0.98
2016-09-06T07:43:51.587585: step 14603, loss 0.00400221, acc 1
2016-09-06T07:43:52.391490: step 14604, loss 0.0128992, acc 1
2016-09-06T07:43:53.200198: step 14605, loss 0.00535253, acc 1
2016-09-06T07:43:54.023332: step 14606, loss 0.00947542, acc 1
2016-09-06T07:43:54.794910: step 14607, loss 0.0440334, acc 0.98
2016-09-06T07:43:55.635376: step 14608, loss 0.00434658, acc 1
2016-09-06T07:43:56.422803: step 14609, loss 0.0750169, acc 0.96
2016-09-06T07:43:57.220509: step 14610, loss 0.00415566, acc 1
2016-09-06T07:43:58.033225: step 14611, loss 0.00834882, acc 1
2016-09-06T07:43:58.895730: step 14612, loss 0.00407328, acc 1
2016-09-06T07:43:59.688442: step 14613, loss 0.00413557, acc 1
2016-09-06T07:44:00.495969: step 14614, loss 0.0239605, acc 1
2016-09-06T07:44:01.320822: step 14615, loss 0.0483688, acc 0.98
2016-09-06T07:44:02.140026: step 14616, loss 0.00418777, acc 1
2016-09-06T07:44:02.938036: step 14617, loss 0.00455569, acc 1
2016-09-06T07:44:03.735262: step 14618, loss 0.0187778, acc 0.98
2016-09-06T07:44:04.552285: step 14619, loss 0.0227965, acc 0.98
2016-09-06T07:44:05.391487: step 14620, loss 0.0226617, acc 1
2016-09-06T07:44:06.227177: step 14621, loss 0.0672752, acc 0.96
2016-09-06T07:44:07.026959: step 14622, loss 0.00690899, acc 1
2016-09-06T07:44:07.843253: step 14623, loss 0.0301615, acc 0.98
2016-09-06T07:44:08.684633: step 14624, loss 0.0301524, acc 0.98
2016-09-06T07:44:09.496461: step 14625, loss 0.0321551, acc 0.98
2016-09-06T07:44:10.315586: step 14626, loss 0.0311163, acc 0.98
2016-09-06T07:44:11.162372: step 14627, loss 0.0702381, acc 0.98
2016-09-06T07:44:11.973321: step 14628, loss 0.0349848, acc 0.98
2016-09-06T07:44:12.804307: step 14629, loss 0.0033035, acc 1
2016-09-06T07:44:13.657410: step 14630, loss 0.00661547, acc 1
2016-09-06T07:44:14.469453: step 14631, loss 0.00341108, acc 1
2016-09-06T07:44:15.285898: step 14632, loss 0.00312558, acc 1
2016-09-06T07:44:16.115991: step 14633, loss 0.0316629, acc 0.98
2016-09-06T07:44:16.982404: step 14634, loss 0.00396755, acc 1
2016-09-06T07:44:17.755377: step 14635, loss 0.0723304, acc 0.96
2016-09-06T07:44:18.554735: step 14636, loss 0.0201864, acc 0.98
2016-09-06T07:44:19.393894: step 14637, loss 0.00290087, acc 1
2016-09-06T07:44:20.206976: step 14638, loss 0.00751261, acc 1
2016-09-06T07:44:21.045387: step 14639, loss 0.00323239, acc 1
2016-09-06T07:44:21.877863: step 14640, loss 0.0141798, acc 1
2016-09-06T07:44:22.677150: step 14641, loss 0.00768646, acc 1
2016-09-06T07:44:23.503666: step 14642, loss 0.00306887, acc 1
2016-09-06T07:44:24.330849: step 14643, loss 0.0435861, acc 0.96
2016-09-06T07:44:25.128828: step 14644, loss 0.0275269, acc 1
2016-09-06T07:44:25.962745: step 14645, loss 0.0181713, acc 1
2016-09-06T07:44:26.793677: step 14646, loss 0.0131228, acc 1
2016-09-06T07:44:27.629982: step 14647, loss 0.0235302, acc 0.98
2016-09-06T07:44:28.458828: step 14648, loss 0.00442559, acc 1
2016-09-06T07:44:29.318038: step 14649, loss 0.004064, acc 1
2016-09-06T07:44:30.139055: step 14650, loss 0.096266, acc 0.96
2016-09-06T07:44:30.954241: step 14651, loss 0.00537181, acc 1
2016-09-06T07:44:31.821050: step 14652, loss 0.00258339, acc 1
2016-09-06T07:44:32.646951: step 14653, loss 0.0406514, acc 0.96
2016-09-06T07:44:33.474171: step 14654, loss 0.00242265, acc 1
2016-09-06T07:44:34.287711: step 14655, loss 0.00276628, acc 1
2016-09-06T07:44:35.106116: step 14656, loss 0.045272, acc 0.96
2016-09-06T07:44:35.904895: step 14657, loss 0.0351741, acc 0.96
2016-09-06T07:44:36.715227: step 14658, loss 0.0720082, acc 0.98
2016-09-06T07:44:37.535848: step 14659, loss 0.00421768, acc 1
2016-09-06T07:44:38.313422: step 14660, loss 0.0681663, acc 0.94
2016-09-06T07:44:39.136302: step 14661, loss 0.0168743, acc 1
2016-09-06T07:44:39.957696: step 14662, loss 0.00388047, acc 1
2016-09-06T07:44:40.732036: step 14663, loss 0.0155391, acc 1
2016-09-06T07:44:41.528244: step 14664, loss 0.0269308, acc 1
2016-09-06T07:44:42.341604: step 14665, loss 0.00334943, acc 1
2016-09-06T07:44:43.137257: step 14666, loss 0.00567014, acc 1
2016-09-06T07:44:43.937108: step 14667, loss 0.0287094, acc 1
2016-09-06T07:44:44.760459: step 14668, loss 0.0390736, acc 0.98
2016-09-06T07:44:45.562208: step 14669, loss 0.0024758, acc 1
2016-09-06T07:44:46.370442: step 14670, loss 0.0101464, acc 1
2016-09-06T07:44:47.189539: step 14671, loss 0.0054883, acc 1
2016-09-06T07:44:47.990838: step 14672, loss 0.00274545, acc 1
2016-09-06T07:44:48.852910: step 14673, loss 0.00477984, acc 1
2016-09-06T07:44:49.668823: step 14674, loss 0.00926885, acc 1
2016-09-06T07:44:50.489636: step 14675, loss 0.0252917, acc 0.98
2016-09-06T07:44:51.305044: step 14676, loss 0.00703297, acc 1
2016-09-06T07:44:52.141355: step 14677, loss 0.00618673, acc 1
2016-09-06T07:44:52.971106: step 14678, loss 0.0430871, acc 0.98
2016-09-06T07:44:53.800203: step 14679, loss 0.00381441, acc 1
2016-09-06T07:44:54.662683: step 14680, loss 0.0275867, acc 1
2016-09-06T07:44:55.495447: step 14681, loss 0.0035054, acc 1
2016-09-06T07:44:56.310012: step 14682, loss 0.00279575, acc 1
2016-09-06T07:44:57.135801: step 14683, loss 0.0447916, acc 0.98
2016-09-06T07:44:57.950125: step 14684, loss 0.00274877, acc 1
2016-09-06T07:44:58.753440: step 14685, loss 0.00829725, acc 1
2016-09-06T07:44:59.601414: step 14686, loss 0.00320734, acc 1
2016-09-06T07:45:00.456252: step 14687, loss 0.00423297, acc 1
2016-09-06T07:45:01.262490: step 14688, loss 0.00368989, acc 1
2016-09-06T07:45:02.082379: step 14689, loss 0.00476319, acc 1
2016-09-06T07:45:02.901203: step 14690, loss 0.0181835, acc 1
2016-09-06T07:45:03.704680: step 14691, loss 0.0766595, acc 0.98
2016-09-06T07:45:04.519412: step 14692, loss 0.0102261, acc 1
2016-09-06T07:45:05.323778: step 14693, loss 0.0127466, acc 1
2016-09-06T07:45:06.118737: step 14694, loss 0.00263682, acc 1
2016-09-06T07:45:06.919123: step 14695, loss 0.0387504, acc 0.98
2016-09-06T07:45:07.743832: step 14696, loss 0.00571965, acc 1
2016-09-06T07:45:08.531797: step 14697, loss 0.0621537, acc 0.96
2016-09-06T07:45:09.346315: step 14698, loss 0.00382603, acc 1
2016-09-06T07:45:10.179930: step 14699, loss 0.0124617, acc 1
2016-09-06T07:45:10.944117: step 14700, loss 0.0155667, acc 1

Evaluation:
2016-09-06T07:45:14.679795: step 14700, loss 2.86313, acc 0.725141

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-14700

2016-09-06T07:45:16.652193: step 14701, loss 0.0165433, acc 1
2016-09-06T07:45:17.445434: step 14702, loss 0.0398748, acc 0.96
2016-09-06T07:45:18.257012: step 14703, loss 0.0024692, acc 1
2016-09-06T07:45:19.089090: step 14704, loss 0.0025641, acc 1
2016-09-06T07:45:19.898306: step 14705, loss 0.0147023, acc 1
2016-09-06T07:45:20.711293: step 14706, loss 0.00269677, acc 1
2016-09-06T07:45:21.510281: step 14707, loss 0.0187518, acc 0.98
2016-09-06T07:45:22.345918: step 14708, loss 0.00844224, acc 1
2016-09-06T07:45:23.144119: step 14709, loss 0.0830167, acc 0.98
2016-09-06T07:45:23.951516: step 14710, loss 0.00810722, acc 1
2016-09-06T07:45:24.766573: step 14711, loss 0.0206944, acc 0.98
2016-09-06T07:45:25.557547: step 14712, loss 0.00308736, acc 1
2016-09-06T07:45:26.397092: step 14713, loss 0.0115573, acc 1
2016-09-06T07:45:27.212132: step 14714, loss 0.00261372, acc 1
2016-09-06T07:45:28.042224: step 14715, loss 0.00373989, acc 1
2016-09-06T07:45:28.829756: step 14716, loss 0.0314127, acc 0.98
2016-09-06T07:45:29.686550: step 14717, loss 0.0359658, acc 0.96
2016-09-06T07:45:30.511942: step 14718, loss 0.00364604, acc 1
2016-09-06T07:45:31.317497: step 14719, loss 0.012755, acc 1
2016-09-06T07:45:32.196758: step 14720, loss 0.0159497, acc 1
2016-09-06T07:45:33.015536: step 14721, loss 0.0106239, acc 1
2016-09-06T07:45:33.802324: step 14722, loss 0.00958559, acc 1
2016-09-06T07:45:34.617617: step 14723, loss 0.0240965, acc 1
2016-09-06T07:45:35.431570: step 14724, loss 0.0283213, acc 0.98
2016-09-06T07:45:36.250111: step 14725, loss 0.0196176, acc 0.98
2016-09-06T07:45:37.070759: step 14726, loss 0.0161744, acc 1
2016-09-06T07:45:37.892039: step 14727, loss 0.0169745, acc 1
2016-09-06T07:45:38.701995: step 14728, loss 0.00248191, acc 1
2016-09-06T07:45:39.546023: step 14729, loss 0.0230014, acc 0.98
2016-09-06T07:45:40.367979: step 14730, loss 0.0154475, acc 1
2016-09-06T07:45:41.172220: step 14731, loss 0.002782, acc 1
2016-09-06T07:45:42.023818: step 14732, loss 0.00231739, acc 1
2016-09-06T07:45:42.869167: step 14733, loss 0.018137, acc 1
2016-09-06T07:45:43.682237: step 14734, loss 0.0022871, acc 1
2016-09-06T07:45:44.521707: step 14735, loss 0.0739065, acc 0.98
2016-09-06T07:45:45.330493: step 14736, loss 0.0199527, acc 1
2016-09-06T07:45:46.112569: step 14737, loss 0.00945983, acc 1
2016-09-06T07:45:46.919247: step 14738, loss 0.0827573, acc 0.98
2016-09-06T07:45:47.729930: step 14739, loss 0.0171615, acc 1
2016-09-06T07:45:48.518217: step 14740, loss 0.00249796, acc 1
2016-09-06T07:45:49.343098: step 14741, loss 0.0159731, acc 1
2016-09-06T07:45:50.158134: step 14742, loss 0.0309164, acc 0.98
2016-09-06T07:45:50.962271: step 14743, loss 0.00228707, acc 1
2016-09-06T07:45:51.797383: step 14744, loss 0.00459159, acc 1
2016-09-06T07:45:52.597947: step 14745, loss 0.00256739, acc 1
2016-09-06T07:45:53.390662: step 14746, loss 0.00283272, acc 1
2016-09-06T07:45:54.196272: step 14747, loss 0.00330646, acc 1
2016-09-06T07:45:54.995949: step 14748, loss 0.0211591, acc 1
2016-09-06T07:45:55.788897: step 14749, loss 0.0657537, acc 0.98
2016-09-06T07:45:56.584705: step 14750, loss 0.0372815, acc 0.98
2016-09-06T07:45:57.437887: step 14751, loss 0.00289908, acc 1
2016-09-06T07:45:58.278926: step 14752, loss 0.0255423, acc 0.98
2016-09-06T07:45:59.106410: step 14753, loss 0.0479632, acc 0.98
2016-09-06T07:45:59.951655: step 14754, loss 0.0419405, acc 0.96
2016-09-06T07:46:00.784418: step 14755, loss 0.0137104, acc 1
2016-09-06T07:46:01.579028: step 14756, loss 0.00319286, acc 1
2016-09-06T07:46:02.396785: step 14757, loss 0.0197442, acc 0.98
2016-09-06T07:46:03.228585: step 14758, loss 0.0145987, acc 1
2016-09-06T07:46:04.044578: step 14759, loss 0.00263381, acc 1
2016-09-06T07:46:04.930539: step 14760, loss 0.0237556, acc 0.98
2016-09-06T07:46:05.743745: step 14761, loss 0.0328085, acc 0.98
2016-09-06T07:46:06.568525: step 14762, loss 0.0614567, acc 0.98
2016-09-06T07:46:07.399378: step 14763, loss 0.0572627, acc 0.98
2016-09-06T07:46:08.248350: step 14764, loss 0.0295668, acc 0.98
2016-09-06T07:46:09.046713: step 14765, loss 0.00940852, acc 1
2016-09-06T07:46:09.858086: step 14766, loss 0.0247267, acc 1
2016-09-06T07:46:10.670459: step 14767, loss 0.030529, acc 1
2016-09-06T07:46:11.491245: step 14768, loss 0.00994555, acc 1
2016-09-06T07:46:12.311898: step 14769, loss 0.0466429, acc 0.98
2016-09-06T07:46:13.134948: step 14770, loss 0.0355668, acc 1
2016-09-06T07:46:13.947984: step 14771, loss 0.0202582, acc 0.98
2016-09-06T07:46:14.740725: step 14772, loss 0.00761241, acc 1
2016-09-06T07:46:15.534497: step 14773, loss 0.00194103, acc 1
2016-09-06T07:46:16.334969: step 14774, loss 0.00310728, acc 1
2016-09-06T07:46:17.146186: step 14775, loss 0.00191724, acc 1
2016-09-06T07:46:17.960497: step 14776, loss 0.0023645, acc 1
2016-09-06T07:46:18.748045: step 14777, loss 0.00318563, acc 1
2016-09-06T07:46:19.525717: step 14778, loss 0.0290944, acc 0.98
2016-09-06T07:46:20.351392: step 14779, loss 0.0339143, acc 0.98
2016-09-06T07:46:21.162239: step 14780, loss 0.00262145, acc 1
2016-09-06T07:46:21.960869: step 14781, loss 0.00615682, acc 1
2016-09-06T07:46:22.804758: step 14782, loss 0.00207309, acc 1
2016-09-06T07:46:23.596288: step 14783, loss 0.00245258, acc 1
2016-09-06T07:46:24.364783: step 14784, loss 0.00330956, acc 1
2016-09-06T07:46:25.177873: step 14785, loss 0.0315589, acc 0.98
2016-09-06T07:46:25.971060: step 14786, loss 0.00265734, acc 1
2016-09-06T07:46:26.772062: step 14787, loss 0.00195403, acc 1
2016-09-06T07:46:27.579976: step 14788, loss 0.00912156, acc 1
2016-09-06T07:46:28.375827: step 14789, loss 0.0222714, acc 0.98
2016-09-06T07:46:29.217628: step 14790, loss 0.0473832, acc 0.98
2016-09-06T07:46:30.045527: step 14791, loss 0.00707908, acc 1
2016-09-06T07:46:30.831184: step 14792, loss 0.02527, acc 1
2016-09-06T07:46:31.644521: step 14793, loss 0.0102609, acc 1
2016-09-06T07:46:32.459246: step 14794, loss 0.0219611, acc 0.98
2016-09-06T07:46:33.257361: step 14795, loss 0.0022636, acc 1
2016-09-06T07:46:34.055458: step 14796, loss 0.00196001, acc 1
2016-09-06T07:46:34.889418: step 14797, loss 0.0108865, acc 1
2016-09-06T07:46:35.681719: step 14798, loss 0.0149434, acc 1
2016-09-06T07:46:36.495725: step 14799, loss 0.0336535, acc 0.98
2016-09-06T07:46:37.321759: step 14800, loss 0.00213552, acc 1

Evaluation:
2016-09-06T07:46:41.047160: step 14800, loss 2.94697, acc 0.718574

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-14800

2016-09-06T07:46:42.876795: step 14801, loss 0.00204046, acc 1
2016-09-06T07:46:43.708354: step 14802, loss 0.011024, acc 1
2016-09-06T07:46:44.552160: step 14803, loss 0.00299287, acc 1
2016-09-06T07:46:45.360033: step 14804, loss 0.00472324, acc 1
2016-09-06T07:46:46.206182: step 14805, loss 0.00202413, acc 1
2016-09-06T07:46:46.997484: step 14806, loss 0.00339314, acc 1
2016-09-06T07:46:47.814762: step 14807, loss 0.0911988, acc 0.94
2016-09-06T07:46:48.651068: step 14808, loss 0.0320282, acc 0.98
2016-09-06T07:46:49.491696: step 14809, loss 0.0200476, acc 0.98
2016-09-06T07:46:50.277380: step 14810, loss 0.0194842, acc 1
2016-09-06T07:46:51.099040: step 14811, loss 0.0165906, acc 1
2016-09-06T07:46:51.927766: step 14812, loss 0.00195929, acc 1
2016-09-06T07:46:52.694557: step 14813, loss 0.0158664, acc 1
2016-09-06T07:46:53.495226: step 14814, loss 0.00214203, acc 1
2016-09-06T07:46:54.317201: step 14815, loss 0.00216792, acc 1
2016-09-06T07:46:55.109158: step 14816, loss 0.00519386, acc 1
2016-09-06T07:46:55.906080: step 14817, loss 0.0298208, acc 0.98
2016-09-06T07:46:56.732643: step 14818, loss 0.00257204, acc 1
2016-09-06T07:46:57.532315: step 14819, loss 0.0138704, acc 1
2016-09-06T07:46:58.332114: step 14820, loss 0.00454134, acc 1
2016-09-06T07:46:59.169432: step 14821, loss 0.00189821, acc 1
2016-09-06T07:46:59.983070: step 14822, loss 0.0162597, acc 1
2016-09-06T07:47:00.812994: step 14823, loss 0.0149496, acc 1
2016-09-06T07:47:01.649385: step 14824, loss 0.00218442, acc 1
2016-09-06T07:47:02.504476: step 14825, loss 0.00547057, acc 1
2016-09-06T07:47:03.287945: step 14826, loss 0.0187508, acc 0.98
2016-09-06T07:47:04.107629: step 14827, loss 0.0390459, acc 0.96
2016-09-06T07:47:04.919970: step 14828, loss 0.008135, acc 1
2016-09-06T07:47:05.713343: step 14829, loss 0.00202171, acc 1
2016-09-06T07:47:06.528664: step 14830, loss 0.00432815, acc 1
2016-09-06T07:47:07.329450: step 14831, loss 0.00461204, acc 1
2016-09-06T07:47:08.133480: step 14832, loss 0.00201199, acc 1
2016-09-06T07:47:08.977514: step 14833, loss 0.0116792, acc 1
2016-09-06T07:47:09.783484: step 14834, loss 0.00186762, acc 1
2016-09-06T07:47:10.607017: step 14835, loss 0.00550005, acc 1
2016-09-06T07:47:11.458205: step 14836, loss 0.00313657, acc 1
2016-09-06T07:47:12.253806: step 14837, loss 0.0147543, acc 1
2016-09-06T07:47:13.079159: step 14838, loss 0.00393636, acc 1
2016-09-06T07:47:13.893200: step 14839, loss 0.00698905, acc 1
2016-09-06T07:47:14.677460: step 14840, loss 0.00844974, acc 1
2016-09-06T07:47:15.506128: step 14841, loss 0.00200056, acc 1
2016-09-06T07:47:16.334126: step 14842, loss 0.0415641, acc 0.96
2016-09-06T07:47:17.155958: step 14843, loss 0.00194096, acc 1
2016-09-06T07:47:17.980798: step 14844, loss 0.0019389, acc 1
2016-09-06T07:47:18.808875: step 14845, loss 0.0340808, acc 0.98
2016-09-06T07:47:19.633413: step 14846, loss 0.00673014, acc 1
2016-09-06T07:47:20.460693: step 14847, loss 0.0019245, acc 1
2016-09-06T07:47:21.293993: step 14848, loss 0.00540758, acc 1
2016-09-06T07:47:22.123851: step 14849, loss 0.00814115, acc 1
2016-09-06T07:47:22.927789: step 14850, loss 0.00925712, acc 1
2016-09-06T07:47:23.770353: step 14851, loss 0.0429382, acc 0.98
2016-09-06T07:47:24.573084: step 14852, loss 0.0553352, acc 0.98
2016-09-06T07:47:25.375985: step 14853, loss 0.100873, acc 0.94
2016-09-06T07:47:26.192747: step 14854, loss 0.00187829, acc 1
2016-09-06T07:47:26.987752: step 14855, loss 0.00183573, acc 1
2016-09-06T07:47:27.765130: step 14856, loss 0.00186284, acc 1
2016-09-06T07:47:28.559196: step 14857, loss 0.0212951, acc 1
2016-09-06T07:47:29.382526: step 14858, loss 0.00230608, acc 1
2016-09-06T07:47:30.180513: step 14859, loss 0.0143095, acc 1
2016-09-06T07:47:30.983211: step 14860, loss 0.020302, acc 1
2016-09-06T07:47:31.791614: step 14861, loss 0.00186147, acc 1
2016-09-06T07:47:32.575511: step 14862, loss 0.0196765, acc 0.98
2016-09-06T07:47:33.357107: step 14863, loss 0.00303479, acc 1
2016-09-06T07:47:34.183156: step 14864, loss 0.0590093, acc 0.98
2016-09-06T07:47:34.951503: step 14865, loss 0.00163175, acc 1
2016-09-06T07:47:35.772662: step 14866, loss 0.0121882, acc 1
2016-09-06T07:47:36.586282: step 14867, loss 0.00183833, acc 1
2016-09-06T07:47:37.359992: step 14868, loss 0.00272021, acc 1
2016-09-06T07:47:38.188376: step 14869, loss 0.00468906, acc 1
2016-09-06T07:47:39.011654: step 14870, loss 0.0172507, acc 1
2016-09-06T07:47:39.802106: step 14871, loss 0.00294144, acc 1
2016-09-06T07:47:40.596340: step 14872, loss 0.0176366, acc 0.98
2016-09-06T07:47:41.436650: step 14873, loss 0.001566, acc 1
2016-09-06T07:47:42.218480: step 14874, loss 0.00142115, acc 1
2016-09-06T07:47:43.043879: step 14875, loss 0.0387258, acc 0.98
2016-09-06T07:47:43.886141: step 14876, loss 0.00152565, acc 1
2016-09-06T07:47:44.666917: step 14877, loss 0.00146458, acc 1
2016-09-06T07:47:45.463397: step 14878, loss 0.009566, acc 1
2016-09-06T07:47:46.285020: step 14879, loss 0.0014346, acc 1
2016-09-06T07:47:47.078203: step 14880, loss 0.00316083, acc 1
2016-09-06T07:47:47.873449: step 14881, loss 0.00836001, acc 1
2016-09-06T07:47:48.705443: step 14882, loss 0.0109017, acc 1
2016-09-06T07:47:49.517420: step 14883, loss 0.00263836, acc 1
2016-09-06T07:47:50.297266: step 14884, loss 0.00220455, acc 1
2016-09-06T07:47:51.139366: step 14885, loss 0.134392, acc 0.96
2016-09-06T07:47:51.905123: step 14886, loss 0.00137999, acc 1
2016-09-06T07:47:52.710096: step 14887, loss 0.00143237, acc 1
2016-09-06T07:47:53.545786: step 14888, loss 0.00130224, acc 1
2016-09-06T07:47:54.320968: step 14889, loss 0.00454606, acc 1
2016-09-06T07:47:55.128734: step 14890, loss 0.00508075, acc 1
2016-09-06T07:47:55.942579: step 14891, loss 0.0673385, acc 0.96
2016-09-06T07:47:56.739941: step 14892, loss 0.0388041, acc 0.96
2016-09-06T07:47:57.578640: step 14893, loss 0.0117522, acc 1
2016-09-06T07:47:58.385310: step 14894, loss 0.0222753, acc 1
2016-09-06T07:47:59.152661: step 14895, loss 0.00136095, acc 1
2016-09-06T07:47:59.978902: step 14896, loss 0.0361274, acc 0.98
2016-09-06T07:48:00.863967: step 14897, loss 0.0109191, acc 1
2016-09-06T07:48:01.674638: step 14898, loss 0.0173104, acc 1
2016-09-06T07:48:02.516938: step 14899, loss 0.00696276, acc 1
2016-09-06T07:48:03.335664: step 14900, loss 0.00139676, acc 1

Evaluation:
2016-09-06T07:48:07.059101: step 14900, loss 2.31432, acc 0.730769

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-14900

2016-09-06T07:48:08.927635: step 14901, loss 0.0108496, acc 1
2016-09-06T07:48:09.738145: step 14902, loss 0.00156026, acc 1
2016-09-06T07:48:10.564497: step 14903, loss 0.00671255, acc 1
2016-09-06T07:48:11.390868: step 14904, loss 0.0106703, acc 1
2016-09-06T07:48:12.229780: step 14905, loss 0.0016167, acc 1
2016-09-06T07:48:13.088462: step 14906, loss 0.0290474, acc 0.98
2016-09-06T07:48:13.916123: step 14907, loss 0.00780447, acc 1
2016-09-06T07:48:14.726057: step 14908, loss 0.0232099, acc 1
2016-09-06T07:48:15.527900: step 14909, loss 0.0459249, acc 0.96
2016-09-06T07:48:16.372881: step 14910, loss 0.00166314, acc 1
2016-09-06T07:48:17.208955: step 14911, loss 0.00379755, acc 1
2016-09-06T07:48:18.084897: step 14912, loss 0.0445552, acc 0.98
2016-09-06T07:48:18.934274: step 14913, loss 0.00219395, acc 1
2016-09-06T07:48:19.742928: step 14914, loss 0.020336, acc 0.98
2016-09-06T07:48:20.549917: step 14915, loss 0.00809606, acc 1
2016-09-06T07:48:21.368836: step 14916, loss 0.0780069, acc 0.98
2016-09-06T07:48:22.160418: step 14917, loss 0.0190514, acc 1
2016-09-06T07:48:22.980879: step 14918, loss 0.00215409, acc 1
2016-09-06T07:48:23.823215: step 14919, loss 0.0171009, acc 1
2016-09-06T07:48:24.624513: step 14920, loss 0.0072924, acc 1
2016-09-06T07:48:25.407820: step 14921, loss 0.00326503, acc 1
2016-09-06T07:48:26.225115: step 14922, loss 0.0425265, acc 0.98
2016-09-06T07:48:26.992201: step 14923, loss 0.00561071, acc 1
2016-09-06T07:48:27.801797: step 14924, loss 0.00178909, acc 1
2016-09-06T07:48:28.629016: step 14925, loss 0.0105976, acc 1
2016-09-06T07:48:29.443292: step 14926, loss 0.00617194, acc 1
2016-09-06T07:48:30.219015: step 14927, loss 0.0216594, acc 0.98
2016-09-06T07:48:31.037526: step 14928, loss 0.014884, acc 1
2016-09-06T07:48:31.813067: step 14929, loss 0.0035431, acc 1
2016-09-06T07:48:32.609661: step 14930, loss 0.0293846, acc 0.98
2016-09-06T07:48:33.436255: step 14931, loss 0.0249046, acc 1
2016-09-06T07:48:34.217334: step 14932, loss 0.0220981, acc 0.98
2016-09-06T07:48:35.016926: step 14933, loss 0.0116531, acc 1
2016-09-06T07:48:35.828725: step 14934, loss 0.00259598, acc 1
2016-09-06T07:48:36.630895: step 14935, loss 0.00636587, acc 1
2016-09-06T07:48:37.430763: step 14936, loss 0.00204569, acc 1
2016-09-06T07:48:38.309792: step 14937, loss 0.00238098, acc 1
2016-09-06T07:48:39.109439: step 14938, loss 0.00378685, acc 1
2016-09-06T07:48:39.937069: step 14939, loss 0.00583702, acc 1
2016-09-06T07:48:40.750818: step 14940, loss 0.0291367, acc 0.98
2016-09-06T07:48:41.525401: step 14941, loss 0.00211095, acc 1
2016-09-06T07:48:42.337449: step 14942, loss 0.0116071, acc 1
2016-09-06T07:48:43.152519: step 14943, loss 0.00232911, acc 1
2016-09-06T07:48:43.928481: step 14944, loss 0.0355822, acc 0.98
2016-09-06T07:48:44.734058: step 14945, loss 0.00780637, acc 1
2016-09-06T07:48:45.568376: step 14946, loss 0.00228548, acc 1
2016-09-06T07:48:46.335941: step 14947, loss 0.00217935, acc 1
2016-09-06T07:48:47.181966: step 14948, loss 0.00213651, acc 1
2016-09-06T07:48:47.970678: step 14949, loss 0.0100021, acc 1
2016-09-06T07:48:48.731157: step 14950, loss 0.0232266, acc 1
2016-09-06T07:48:49.547168: step 14951, loss 0.00606802, acc 1
2016-09-06T07:48:50.373553: step 14952, loss 0.015705, acc 1
2016-09-06T07:48:51.210088: step 14953, loss 0.128275, acc 0.94
2016-09-06T07:48:51.999002: step 14954, loss 0.022544, acc 0.98
2016-09-06T07:48:52.825634: step 14955, loss 0.0457458, acc 0.98
2016-09-06T07:48:53.631405: step 14956, loss 0.0185283, acc 0.98
2016-09-06T07:48:54.421437: step 14957, loss 0.00437176, acc 1
2016-09-06T07:48:55.228057: step 14958, loss 0.0158887, acc 1
2016-09-06T07:48:56.019707: step 14959, loss 0.0815664, acc 0.98
2016-09-06T07:48:56.820715: step 14960, loss 0.0020652, acc 1
2016-09-06T07:48:57.628534: step 14961, loss 0.00318294, acc 1
2016-09-06T07:48:58.409707: step 14962, loss 0.0122003, acc 1
2016-09-06T07:48:59.206355: step 14963, loss 0.00166321, acc 1
2016-09-06T07:49:00.042859: step 14964, loss 0.00816157, acc 1
2016-09-06T07:49:00.880009: step 14965, loss 0.00959218, acc 1
2016-09-06T07:49:01.657383: step 14966, loss 0.00228418, acc 1
2016-09-06T07:49:02.474160: step 14967, loss 0.00157106, acc 1
2016-09-06T07:49:03.251376: step 14968, loss 0.00197015, acc 1
2016-09-06T07:49:04.052182: step 14969, loss 0.00154955, acc 1
2016-09-06T07:49:04.873640: step 14970, loss 0.0179328, acc 0.98
2016-09-06T07:49:05.679379: step 14971, loss 0.0338475, acc 0.96
2016-09-06T07:49:06.474850: step 14972, loss 0.0110545, acc 1
2016-09-06T07:49:07.289986: step 14973, loss 0.00911735, acc 1
2016-09-06T07:49:08.079769: step 14974, loss 0.00444741, acc 1
2016-09-06T07:49:08.895367: step 14975, loss 0.0303847, acc 0.98
2016-09-06T07:49:09.656467: step 14976, loss 0.00147289, acc 1
2016-09-06T07:49:10.490024: step 14977, loss 0.0221308, acc 0.98
2016-09-06T07:49:11.288446: step 14978, loss 0.0168629, acc 1
2016-09-06T07:49:12.112984: step 14979, loss 0.00430597, acc 1
2016-09-06T07:49:12.917090: step 14980, loss 0.0057507, acc 1
2016-09-06T07:49:13.743369: step 14981, loss 0.0249049, acc 0.98
2016-09-06T07:49:14.528336: step 14982, loss 0.0228662, acc 0.98
2016-09-06T07:49:15.304878: step 14983, loss 0.00765179, acc 1
2016-09-06T07:49:16.122931: step 14984, loss 0.0127559, acc 1
2016-09-06T07:49:16.959522: step 14985, loss 0.0284148, acc 0.98
2016-09-06T07:49:17.772934: step 14986, loss 0.0222026, acc 0.98
2016-09-06T07:49:18.546864: step 14987, loss 0.0252995, acc 0.98
2016-09-06T07:49:19.359852: step 14988, loss 0.017803, acc 0.98
2016-09-06T07:49:20.139979: step 14989, loss 0.00423238, acc 1
2016-09-06T07:49:20.950149: step 14990, loss 0.0237678, acc 0.98
2016-09-06T07:49:21.762102: step 14991, loss 0.0455403, acc 0.98
2016-09-06T07:49:22.574337: step 14992, loss 0.00170128, acc 1
2016-09-06T07:49:23.410332: step 14993, loss 0.00423735, acc 1
2016-09-06T07:49:24.224864: step 14994, loss 0.00178132, acc 1
2016-09-06T07:49:25.033848: step 14995, loss 0.0232527, acc 0.98
2016-09-06T07:49:25.856274: step 14996, loss 0.00189094, acc 1
2016-09-06T07:49:26.667510: step 14997, loss 0.00279015, acc 1
2016-09-06T07:49:27.459326: step 14998, loss 0.0269658, acc 0.98
2016-09-06T07:49:28.270110: step 14999, loss 0.0120154, acc 1
2016-09-06T07:49:29.083648: step 15000, loss 0.00187208, acc 1

Evaluation:
2016-09-06T07:49:32.821384: step 15000, loss 2.5485, acc 0.728893

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-15000

2016-09-06T07:49:34.686469: step 15001, loss 0.0548288, acc 0.98
2016-09-06T07:49:35.478420: step 15002, loss 0.00757208, acc 1
2016-09-06T07:49:36.321187: step 15003, loss 0.00294779, acc 1
2016-09-06T07:49:37.113151: step 15004, loss 0.00449985, acc 1
2016-09-06T07:49:37.913373: step 15005, loss 0.00193293, acc 1
2016-09-06T07:49:38.728052: step 15006, loss 0.00960685, acc 1
2016-09-06T07:49:39.477888: step 15007, loss 0.00155569, acc 1
2016-09-06T07:49:40.288950: step 15008, loss 0.00449612, acc 1
2016-09-06T07:49:41.109082: step 15009, loss 0.00795125, acc 1
2016-09-06T07:49:41.906017: step 15010, loss 0.0090243, acc 1
2016-09-06T07:49:42.707629: step 15011, loss 0.00154832, acc 1
2016-09-06T07:49:43.516897: step 15012, loss 0.00153736, acc 1
2016-09-06T07:49:44.309458: step 15013, loss 0.00167543, acc 1
2016-09-06T07:49:45.118802: step 15014, loss 0.142709, acc 0.96
2016-09-06T07:49:45.940151: step 15015, loss 0.0144679, acc 1
2016-09-06T07:49:46.725014: step 15016, loss 0.00196227, acc 1
2016-09-06T07:49:47.529863: step 15017, loss 0.0351222, acc 0.98
2016-09-06T07:49:48.354496: step 15018, loss 0.00326898, acc 1
2016-09-06T07:49:49.141642: step 15019, loss 0.0183346, acc 1
2016-09-06T07:49:49.934329: step 15020, loss 0.0518429, acc 0.98
2016-09-06T07:49:50.768094: step 15021, loss 0.00188044, acc 1
2016-09-06T07:49:51.566117: step 15022, loss 0.0141198, acc 1
2016-09-06T07:49:52.363695: step 15023, loss 0.0109323, acc 1
2016-09-06T07:49:53.176736: step 15024, loss 0.0312914, acc 0.98
2016-09-06T07:49:53.973100: step 15025, loss 0.0314323, acc 0.98
2016-09-06T07:49:54.783539: step 15026, loss 0.00176024, acc 1
2016-09-06T07:49:55.588204: step 15027, loss 0.0136481, acc 1
2016-09-06T07:49:56.401565: step 15028, loss 0.0186041, acc 1
2016-09-06T07:49:57.192125: step 15029, loss 0.0138105, acc 1
2016-09-06T07:49:57.994577: step 15030, loss 0.0169585, acc 0.98
2016-09-06T07:49:58.848105: step 15031, loss 0.0241269, acc 0.98
2016-09-06T07:49:59.683067: step 15032, loss 0.00275203, acc 1
2016-09-06T07:50:00.543445: step 15033, loss 0.013968, acc 1
2016-09-06T07:50:01.336243: step 15034, loss 0.0177285, acc 1
2016-09-06T07:50:02.150992: step 15035, loss 0.00206636, acc 1
2016-09-06T07:50:02.981409: step 15036, loss 0.00202968, acc 1
2016-09-06T07:50:03.818989: step 15037, loss 0.0186873, acc 1
2016-09-06T07:50:04.624700: step 15038, loss 0.00557231, acc 1
2016-09-06T07:50:05.446999: step 15039, loss 0.00209706, acc 1
2016-09-06T07:50:06.240274: step 15040, loss 0.00333906, acc 1
2016-09-06T07:50:07.043609: step 15041, loss 0.0137413, acc 1
2016-09-06T07:50:07.886228: step 15042, loss 0.00247692, acc 1
2016-09-06T07:50:08.737460: step 15043, loss 0.0162637, acc 0.98
2016-09-06T07:50:09.567018: step 15044, loss 0.028669, acc 0.98
2016-09-06T07:50:10.429615: step 15045, loss 0.0119923, acc 1
2016-09-06T07:50:11.246536: step 15046, loss 0.00226436, acc 1
2016-09-06T07:50:12.063607: step 15047, loss 0.0122678, acc 1
2016-09-06T07:50:12.873618: step 15048, loss 0.00223729, acc 1
2016-09-06T07:50:13.673500: step 15049, loss 0.0977163, acc 0.98
2016-09-06T07:50:14.445395: step 15050, loss 0.00421966, acc 1
2016-09-06T07:50:15.290763: step 15051, loss 0.00698154, acc 1
2016-09-06T07:50:16.098837: step 15052, loss 0.0158083, acc 1
2016-09-06T07:50:16.881459: step 15053, loss 0.0182396, acc 1
2016-09-06T07:50:17.703686: step 15054, loss 0.00296416, acc 1
2016-09-06T07:50:18.489674: step 15055, loss 0.00971483, acc 1
2016-09-06T07:50:19.297447: step 15056, loss 0.00218046, acc 1
2016-09-06T07:50:20.128744: step 15057, loss 0.00209951, acc 1
2016-09-06T07:50:20.967339: step 15058, loss 0.0218833, acc 0.98
2016-09-06T07:50:21.749667: step 15059, loss 0.0153926, acc 1
2016-09-06T07:50:22.573477: step 15060, loss 0.00208642, acc 1
2016-09-06T07:50:23.411678: step 15061, loss 0.00209306, acc 1
2016-09-06T07:50:24.214521: step 15062, loss 0.004229, acc 1
2016-09-06T07:50:25.060213: step 15063, loss 0.0136458, acc 1
2016-09-06T07:50:25.897718: step 15064, loss 0.00337863, acc 1
2016-09-06T07:50:26.714673: step 15065, loss 0.0041161, acc 1
2016-09-06T07:50:27.532533: step 15066, loss 0.00213301, acc 1
2016-09-06T07:50:28.370190: step 15067, loss 0.00867328, acc 1
2016-09-06T07:50:29.199240: step 15068, loss 0.00209505, acc 1
2016-09-06T07:50:30.047181: step 15069, loss 0.00439168, acc 1
2016-09-06T07:50:30.864150: step 15070, loss 0.00203069, acc 1
2016-09-06T07:50:31.657316: step 15071, loss 0.037829, acc 0.98
2016-09-06T07:50:32.461961: step 15072, loss 0.00207474, acc 1
2016-09-06T07:50:33.288024: step 15073, loss 0.0020595, acc 1
2016-09-06T07:50:34.108958: step 15074, loss 0.0216658, acc 0.98
2016-09-06T07:50:34.947184: step 15075, loss 0.0203926, acc 1
2016-09-06T07:50:35.798262: step 15076, loss 0.0156426, acc 1
2016-09-06T07:50:36.621598: step 15077, loss 0.0189301, acc 1
2016-09-06T07:50:37.441135: step 15078, loss 0.00189763, acc 1
2016-09-06T07:50:38.260016: step 15079, loss 0.00189349, acc 1
2016-09-06T07:50:39.060450: step 15080, loss 0.00187504, acc 1
2016-09-06T07:50:39.894925: step 15081, loss 0.0289294, acc 1
2016-09-06T07:50:40.698427: step 15082, loss 0.00263721, acc 1
2016-09-06T07:50:41.524857: step 15083, loss 0.0319015, acc 1
2016-09-06T07:50:42.330570: step 15084, loss 0.0199383, acc 0.98
2016-09-06T07:50:43.148854: step 15085, loss 0.0146074, acc 1
2016-09-06T07:50:43.945045: step 15086, loss 0.0603055, acc 0.98
2016-09-06T07:50:44.752193: step 15087, loss 0.0169018, acc 0.98
2016-09-06T07:50:45.561356: step 15088, loss 0.051386, acc 0.98
2016-09-06T07:50:46.404987: step 15089, loss 0.00978554, acc 1
2016-09-06T07:50:47.172589: step 15090, loss 0.0157786, acc 1
2016-09-06T07:50:48.013851: step 15091, loss 0.0339149, acc 0.98
2016-09-06T07:50:48.820740: step 15092, loss 0.00186222, acc 1
2016-09-06T07:50:49.593722: step 15093, loss 0.00339016, acc 1
2016-09-06T07:50:50.385863: step 15094, loss 0.00181454, acc 1
2016-09-06T07:50:51.205947: step 15095, loss 0.015099, acc 1
2016-09-06T07:50:51.998963: step 15096, loss 0.0021737, acc 1
2016-09-06T07:50:52.800094: step 15097, loss 0.0019052, acc 1
2016-09-06T07:50:53.662347: step 15098, loss 0.0101833, acc 1
2016-09-06T07:50:54.496361: step 15099, loss 0.00206159, acc 1
2016-09-06T07:50:55.296800: step 15100, loss 0.00708768, acc 1

Evaluation:
2016-09-06T07:50:59.041583: step 15100, loss 2.56762, acc 0.719512

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-15100

2016-09-06T07:51:01.057381: step 15101, loss 0.0272216, acc 0.98
2016-09-06T07:51:01.865650: step 15102, loss 0.00447655, acc 1
2016-09-06T07:51:02.695431: step 15103, loss 0.0151859, acc 1
2016-09-06T07:51:03.512076: step 15104, loss 0.0017545, acc 1
2016-09-06T07:51:04.333782: step 15105, loss 0.0108379, acc 1
2016-09-06T07:51:05.138817: step 15106, loss 0.00295246, acc 1
2016-09-06T07:51:05.981536: step 15107, loss 0.00761474, acc 1
2016-09-06T07:51:06.799516: step 15108, loss 0.00711247, acc 1
2016-09-06T07:51:07.608254: step 15109, loss 0.0294765, acc 0.98
2016-09-06T07:51:08.457776: step 15110, loss 0.0218958, acc 1
2016-09-06T07:51:09.280656: step 15111, loss 0.0331006, acc 0.98
2016-09-06T07:51:10.089314: step 15112, loss 0.00982044, acc 1
2016-09-06T07:51:10.891767: step 15113, loss 0.00377129, acc 1
2016-09-06T07:51:11.683513: step 15114, loss 0.0577827, acc 0.98
2016-09-06T07:51:12.516346: step 15115, loss 0.00182359, acc 1
2016-09-06T07:51:13.342304: step 15116, loss 0.00181795, acc 1
2016-09-06T07:51:14.157436: step 15117, loss 0.048367, acc 0.96
2016-09-06T07:51:14.946176: step 15118, loss 0.00638772, acc 1
2016-09-06T07:51:15.764611: step 15119, loss 0.0053286, acc 1
2016-09-06T07:51:16.578912: step 15120, loss 0.0256875, acc 0.98
2016-09-06T07:51:17.368090: step 15121, loss 0.00234843, acc 1
2016-09-06T07:51:18.193113: step 15122, loss 0.0260623, acc 0.98
2016-09-06T07:51:19.051375: step 15123, loss 0.0050405, acc 1
2016-09-06T07:51:19.872964: step 15124, loss 0.101789, acc 0.98
2016-09-06T07:51:20.685027: step 15125, loss 0.00286219, acc 1
2016-09-06T07:51:21.513438: step 15126, loss 0.00437258, acc 1
2016-09-06T07:51:22.328219: step 15127, loss 0.0123044, acc 1
2016-09-06T07:51:23.140732: step 15128, loss 0.00199312, acc 1
2016-09-06T07:51:23.983326: step 15129, loss 0.017971, acc 0.98
2016-09-06T07:51:24.804667: step 15130, loss 0.00220691, acc 1
2016-09-06T07:51:25.628187: step 15131, loss 0.0164625, acc 1
2016-09-06T07:51:26.466916: step 15132, loss 0.0638546, acc 0.98
2016-09-06T07:51:27.294388: step 15133, loss 0.00188104, acc 1
2016-09-06T07:51:28.140256: step 15134, loss 0.00388047, acc 1
2016-09-06T07:51:28.959280: step 15135, loss 0.00211037, acc 1
2016-09-06T07:51:29.758650: step 15136, loss 0.0449694, acc 0.98
2016-09-06T07:51:30.581569: step 15137, loss 0.0439764, acc 0.96
2016-09-06T07:51:31.408948: step 15138, loss 0.00290412, acc 1
2016-09-06T07:51:32.228092: step 15139, loss 0.0193703, acc 1
2016-09-06T07:51:33.061600: step 15140, loss 0.00770033, acc 1
2016-09-06T07:51:33.898183: step 15141, loss 0.00184859, acc 1
2016-09-06T07:51:34.730105: step 15142, loss 0.0164302, acc 1
2016-09-06T07:51:35.532482: step 15143, loss 0.0115484, acc 1
2016-09-06T07:51:36.329648: step 15144, loss 0.0102216, acc 1
2016-09-06T07:51:37.138266: step 15145, loss 0.0539243, acc 0.96
2016-09-06T07:51:37.937382: step 15146, loss 0.0200144, acc 1
2016-09-06T07:51:38.764754: step 15147, loss 0.013453, acc 1
2016-09-06T07:51:39.606016: step 15148, loss 0.00244767, acc 1
2016-09-06T07:51:40.401537: step 15149, loss 0.0247581, acc 0.98
2016-09-06T07:51:41.189337: step 15150, loss 0.00193392, acc 1
2016-09-06T07:51:42.034499: step 15151, loss 0.0227207, acc 0.98
2016-09-06T07:51:42.810334: step 15152, loss 0.00259363, acc 1
2016-09-06T07:51:43.608619: step 15153, loss 0.0146474, acc 1
2016-09-06T07:51:44.449593: step 15154, loss 0.0207851, acc 0.98
2016-09-06T07:51:45.241889: step 15155, loss 0.00829107, acc 1
2016-09-06T07:51:46.034782: step 15156, loss 0.0649368, acc 0.96
2016-09-06T07:51:46.856230: step 15157, loss 0.00191396, acc 1
2016-09-06T07:51:47.622802: step 15158, loss 0.0197724, acc 1
2016-09-06T07:51:48.439351: step 15159, loss 0.00192034, acc 1
2016-09-06T07:51:49.286583: step 15160, loss 0.00531132, acc 1
2016-09-06T07:51:50.064057: step 15161, loss 0.0317268, acc 0.98
2016-09-06T07:51:50.848721: step 15162, loss 0.0191001, acc 0.98
2016-09-06T07:51:51.665265: step 15163, loss 0.0441793, acc 0.98
2016-09-06T07:51:52.460759: step 15164, loss 0.00207403, acc 1
2016-09-06T07:51:53.274831: step 15165, loss 0.0211532, acc 1
2016-09-06T07:51:54.091194: step 15166, loss 0.0237649, acc 0.98
2016-09-06T07:51:54.880421: step 15167, loss 0.0270352, acc 0.98
2016-09-06T07:51:55.635014: step 15168, loss 0.0156827, acc 1
2016-09-06T07:51:56.431047: step 15169, loss 0.0261082, acc 0.98
2016-09-06T07:51:57.235183: step 15170, loss 0.00379032, acc 1
2016-09-06T07:51:58.070172: step 15171, loss 0.0180181, acc 0.98
2016-09-06T07:51:58.896888: step 15172, loss 0.00814357, acc 1
2016-09-06T07:51:59.707153: step 15173, loss 0.0326088, acc 0.98
2016-09-06T07:52:00.561612: step 15174, loss 0.0100094, acc 1
2016-09-06T07:52:01.385009: step 15175, loss 0.00362022, acc 1
2016-09-06T07:52:02.208648: step 15176, loss 0.0174293, acc 0.98
2016-09-06T07:52:03.025315: step 15177, loss 0.0204572, acc 1
2016-09-06T07:52:03.837952: step 15178, loss 0.0216064, acc 0.98
2016-09-06T07:52:04.768136: step 15179, loss 0.00561015, acc 1
2016-09-06T07:52:05.585441: step 15180, loss 0.00339811, acc 1
2016-09-06T07:52:06.438066: step 15181, loss 0.00257483, acc 1
2016-09-06T07:52:07.240899: step 15182, loss 0.0292662, acc 0.98
2016-09-06T07:52:08.037337: step 15183, loss 0.00882882, acc 1
2016-09-06T07:52:08.873299: step 15184, loss 0.00264293, acc 1
2016-09-06T07:52:09.712309: step 15185, loss 0.0172374, acc 0.98
2016-09-06T07:52:10.532426: step 15186, loss 0.00259251, acc 1
2016-09-06T07:52:11.342879: step 15187, loss 0.00342877, acc 1
2016-09-06T07:52:12.136508: step 15188, loss 0.00271297, acc 1
2016-09-06T07:52:12.928043: step 15189, loss 0.00263652, acc 1
2016-09-06T07:52:13.756070: step 15190, loss 0.00311969, acc 1
2016-09-06T07:52:14.560314: step 15191, loss 0.00260037, acc 1
2016-09-06T07:52:15.370218: step 15192, loss 0.00647098, acc 1
2016-09-06T07:52:16.195433: step 15193, loss 0.00257548, acc 1
2016-09-06T07:52:16.986865: step 15194, loss 0.0184692, acc 1
2016-09-06T07:52:17.821329: step 15195, loss 0.0025959, acc 1
2016-09-06T07:52:18.815081: step 15196, loss 0.00355597, acc 1
2016-09-06T07:52:19.640547: step 15197, loss 0.0513288, acc 0.98
2016-09-06T07:52:20.485399: step 15198, loss 0.00252546, acc 1
2016-09-06T07:52:21.291458: step 15199, loss 0.0375357, acc 0.98
2016-09-06T07:52:22.118390: step 15200, loss 0.0186306, acc 1

Evaluation:
2016-09-06T07:52:25.837557: step 15200, loss 3.81635, acc 0.721388

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-15200

2016-09-06T07:52:27.778035: step 15201, loss 0.00449781, acc 1
2016-09-06T07:52:28.591063: step 15202, loss 0.0200293, acc 1
2016-09-06T07:52:29.433828: step 15203, loss 0.00237933, acc 1
2016-09-06T07:52:30.250100: step 15204, loss 0.0153407, acc 1
2016-09-06T07:52:31.058687: step 15205, loss 0.00238259, acc 1
2016-09-06T07:52:31.950896: step 15206, loss 0.00613331, acc 1
2016-09-06T07:52:32.761385: step 15207, loss 0.0023533, acc 1
2016-09-06T07:52:33.565772: step 15208, loss 0.00236542, acc 1
2016-09-06T07:52:34.374092: step 15209, loss 0.00844122, acc 1
2016-09-06T07:52:35.194864: step 15210, loss 0.00234516, acc 1
2016-09-06T07:52:35.986555: step 15211, loss 0.0611159, acc 0.98
2016-09-06T07:52:36.788794: step 15212, loss 0.00231908, acc 1
2016-09-06T07:52:37.604112: step 15213, loss 0.0388592, acc 0.98
2016-09-06T07:52:38.383440: step 15214, loss 0.00247161, acc 1
2016-09-06T07:52:39.196484: step 15215, loss 0.0149522, acc 1
2016-09-06T07:52:39.995981: step 15216, loss 0.014331, acc 1
2016-09-06T07:52:40.833811: step 15217, loss 0.0331388, acc 0.96
2016-09-06T07:52:41.636745: step 15218, loss 0.00216903, acc 1
2016-09-06T07:52:42.442794: step 15219, loss 0.0324669, acc 0.98
2016-09-06T07:52:43.245570: step 15220, loss 0.021959, acc 0.98
2016-09-06T07:52:44.065013: step 15221, loss 0.0158866, acc 1
2016-09-06T07:52:44.913356: step 15222, loss 0.013662, acc 1
2016-09-06T07:52:45.677902: step 15223, loss 0.00413133, acc 1
2016-09-06T07:52:46.477875: step 15224, loss 0.0045176, acc 1
2016-09-06T07:52:47.313392: step 15225, loss 0.00194251, acc 1
2016-09-06T07:52:48.100008: step 15226, loss 0.0181631, acc 0.98
2016-09-06T07:52:48.910138: step 15227, loss 0.00509552, acc 1
2016-09-06T07:52:49.736106: step 15228, loss 0.00236466, acc 1
2016-09-06T07:52:50.485333: step 15229, loss 0.0416655, acc 1
2016-09-06T07:52:51.302617: step 15230, loss 0.00196997, acc 1
2016-09-06T07:52:52.109272: step 15231, loss 0.0022391, acc 1
2016-09-06T07:52:52.879852: step 15232, loss 0.0668278, acc 0.94
2016-09-06T07:52:53.687892: step 15233, loss 0.0123711, acc 1
2016-09-06T07:52:54.511809: step 15234, loss 0.0211325, acc 1
2016-09-06T07:52:55.318158: step 15235, loss 0.0240792, acc 0.98
2016-09-06T07:52:56.123805: step 15236, loss 0.0523135, acc 0.96
2016-09-06T07:52:56.985932: step 15237, loss 0.00339299, acc 1
2016-09-06T07:52:57.761129: step 15238, loss 0.0083953, acc 1
2016-09-06T07:52:58.576913: step 15239, loss 0.0229106, acc 0.98
2016-09-06T07:52:59.397243: step 15240, loss 0.0037319, acc 1
2016-09-06T07:53:00.186671: step 15241, loss 0.0381062, acc 0.98
2016-09-06T07:53:01.037843: step 15242, loss 0.00174797, acc 1
2016-09-06T07:53:01.850648: step 15243, loss 0.0414384, acc 0.98
2016-09-06T07:53:02.633039: step 15244, loss 0.0160774, acc 0.98
2016-09-06T07:53:03.440383: step 15245, loss 0.00534302, acc 1
2016-09-06T07:53:04.249733: step 15246, loss 0.0146213, acc 1
2016-09-06T07:53:05.037483: step 15247, loss 0.00471168, acc 1
2016-09-06T07:53:05.863616: step 15248, loss 0.0021247, acc 1
2016-09-06T07:53:06.677236: step 15249, loss 0.00253705, acc 1
2016-09-06T07:53:07.472664: step 15250, loss 0.020918, acc 0.98
2016-09-06T07:53:08.268149: step 15251, loss 0.00706481, acc 1
2016-09-06T07:53:09.084437: step 15252, loss 0.00344029, acc 1
2016-09-06T07:53:09.876653: step 15253, loss 0.0118047, acc 1
2016-09-06T07:53:10.696328: step 15254, loss 0.00802317, acc 1
2016-09-06T07:53:11.522206: step 15255, loss 0.00177795, acc 1
2016-09-06T07:53:12.306418: step 15256, loss 0.00380416, acc 1
2016-09-06T07:53:13.083729: step 15257, loss 0.00609471, acc 1
2016-09-06T07:53:13.918680: step 15258, loss 0.0510369, acc 0.98
2016-09-06T07:53:14.721810: step 15259, loss 0.0272425, acc 0.98
2016-09-06T07:53:15.526450: step 15260, loss 0.00172022, acc 1
2016-09-06T07:53:16.324944: step 15261, loss 0.00286946, acc 1
2016-09-06T07:53:17.109217: step 15262, loss 0.00176957, acc 1
2016-09-06T07:53:17.927188: step 15263, loss 0.00970243, acc 1
2016-09-06T07:53:18.733119: step 15264, loss 0.00166587, acc 1
2016-09-06T07:53:19.501565: step 15265, loss 0.00167777, acc 1
2016-09-06T07:53:20.287584: step 15266, loss 0.0195959, acc 1
2016-09-06T07:53:21.108164: step 15267, loss 0.00859615, acc 1
2016-09-06T07:53:21.926906: step 15268, loss 0.00567065, acc 1
2016-09-06T07:53:22.768441: step 15269, loss 0.0104274, acc 1
2016-09-06T07:53:23.579992: step 15270, loss 0.00252972, acc 1
2016-09-06T07:53:24.339381: step 15271, loss 0.00622646, acc 1
2016-09-06T07:53:25.149325: step 15272, loss 0.00440031, acc 1
2016-09-06T07:53:25.963455: step 15273, loss 0.0406967, acc 0.98
2016-09-06T07:53:26.764532: step 15274, loss 0.00175158, acc 1
2016-09-06T07:53:27.585359: step 15275, loss 0.0448078, acc 0.98
2016-09-06T07:53:28.427359: step 15276, loss 0.0568157, acc 0.96
2016-09-06T07:53:29.227716: step 15277, loss 0.0326402, acc 0.98
2016-09-06T07:53:30.031401: step 15278, loss 0.098899, acc 0.98
2016-09-06T07:53:30.855133: step 15279, loss 0.0122884, acc 1
2016-09-06T07:53:31.667589: step 15280, loss 0.0347292, acc 0.98
2016-09-06T07:53:32.470687: step 15281, loss 0.0395494, acc 0.98
2016-09-06T07:53:33.288970: step 15282, loss 0.00403435, acc 1
2016-09-06T07:53:34.096518: step 15283, loss 0.00821588, acc 1
2016-09-06T07:53:34.906329: step 15284, loss 0.0204687, acc 0.98
2016-09-06T07:53:35.712620: step 15285, loss 0.00341531, acc 1
2016-09-06T07:53:36.556204: step 15286, loss 0.00126705, acc 1
2016-09-06T07:53:37.384638: step 15287, loss 0.019404, acc 1
2016-09-06T07:53:38.233295: step 15288, loss 0.0253158, acc 1
2016-09-06T07:53:39.046827: step 15289, loss 0.00183587, acc 1
2016-09-06T07:53:39.862179: step 15290, loss 0.00253295, acc 1
2016-09-06T07:53:40.724643: step 15291, loss 0.0177311, acc 0.98
2016-09-06T07:53:41.587612: step 15292, loss 0.0184332, acc 1
2016-09-06T07:53:42.420396: step 15293, loss 0.00324501, acc 1
2016-09-06T07:53:43.249530: step 15294, loss 0.00210157, acc 1
2016-09-06T07:53:44.080057: step 15295, loss 0.0283883, acc 0.98
2016-09-06T07:53:44.885980: step 15296, loss 0.0365708, acc 0.98
2016-09-06T07:53:45.707763: step 15297, loss 0.00165413, acc 1
2016-09-06T07:53:46.514476: step 15298, loss 0.0592246, acc 0.96
2016-09-06T07:53:47.317417: step 15299, loss 0.0610752, acc 0.98
2016-09-06T07:53:48.134179: step 15300, loss 0.00339013, acc 1

Evaluation:
2016-09-06T07:53:51.895155: step 15300, loss 2.36695, acc 0.723265

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-15300

2016-09-06T07:53:53.831711: step 15301, loss 0.0485146, acc 0.98
2016-09-06T07:53:54.619088: step 15302, loss 0.0246151, acc 0.98
2016-09-06T07:53:55.429438: step 15303, loss 0.0121699, acc 1
2016-09-06T07:53:56.246066: step 15304, loss 0.0376937, acc 0.98
2016-09-06T07:53:57.043861: step 15305, loss 0.0148434, acc 1
2016-09-06T07:53:57.833888: step 15306, loss 0.0108894, acc 1
2016-09-06T07:53:58.681662: step 15307, loss 0.00160322, acc 1
2016-09-06T07:53:59.468237: step 15308, loss 0.00188169, acc 1
2016-09-06T07:54:00.252141: step 15309, loss 0.029666, acc 1
2016-09-06T07:54:01.061524: step 15310, loss 0.0426436, acc 0.98
2016-09-06T07:54:01.868145: step 15311, loss 0.00221261, acc 1
2016-09-06T07:54:02.683967: step 15312, loss 0.0155169, acc 1
2016-09-06T07:54:03.511720: step 15313, loss 0.0023964, acc 1
2016-09-06T07:54:04.288171: step 15314, loss 0.0125539, acc 1
2016-09-06T07:54:05.088042: step 15315, loss 0.0270866, acc 1
2016-09-06T07:54:05.929753: step 15316, loss 0.00473431, acc 1
2016-09-06T07:54:06.725696: step 15317, loss 0.00932643, acc 1
2016-09-06T07:54:07.506601: step 15318, loss 0.0475544, acc 0.98
2016-09-06T07:54:08.330122: step 15319, loss 0.00169544, acc 1
2016-09-06T07:54:09.085454: step 15320, loss 0.00292488, acc 1
2016-09-06T07:54:09.919178: step 15321, loss 0.00334991, acc 1
2016-09-06T07:54:10.771597: step 15322, loss 0.00180001, acc 1
2016-09-06T07:54:11.572175: step 15323, loss 0.00290305, acc 1
2016-09-06T07:54:12.390364: step 15324, loss 0.00692042, acc 1
2016-09-06T07:54:13.237272: step 15325, loss 0.024378, acc 0.98
2016-09-06T07:54:14.046173: step 15326, loss 0.00246635, acc 1
2016-09-06T07:54:14.853600: step 15327, loss 0.0094161, acc 1
2016-09-06T07:54:15.688288: step 15328, loss 0.0492437, acc 0.96
2016-09-06T07:54:16.503323: step 15329, loss 0.0173867, acc 1
2016-09-06T07:54:17.309818: step 15330, loss 0.00640502, acc 1
2016-09-06T07:54:18.135000: step 15331, loss 0.00431902, acc 1
2016-09-06T07:54:18.947438: step 15332, loss 0.00362925, acc 1
2016-09-06T07:54:19.873916: step 15333, loss 0.00392851, acc 1
2016-09-06T07:54:20.712227: step 15334, loss 0.0519925, acc 0.96
2016-09-06T07:54:21.661032: step 15335, loss 0.0190966, acc 1
2016-09-06T07:54:22.479087: step 15336, loss 0.0160928, acc 1
2016-09-06T07:54:23.444124: step 15337, loss 0.075658, acc 0.96
2016-09-06T07:54:24.498152: step 15338, loss 0.00632439, acc 1
2016-09-06T07:54:25.353401: step 15339, loss 0.014488, acc 1
2016-09-06T07:54:26.296816: step 15340, loss 0.00497035, acc 1
2016-09-06T07:54:27.481094: step 15341, loss 0.0075146, acc 1
2016-09-06T07:54:28.727336: step 15342, loss 0.021217, acc 1
2016-09-06T07:54:29.673999: step 15343, loss 0.0312663, acc 0.98
2016-09-06T07:54:30.755754: step 15344, loss 0.00453843, acc 1
2016-09-06T07:54:31.884124: step 15345, loss 0.0596362, acc 0.98
2016-09-06T07:54:32.795964: step 15346, loss 0.0623126, acc 0.98
2016-09-06T07:54:33.894404: step 15347, loss 0.00545845, acc 1
2016-09-06T07:54:34.848966: step 15348, loss 0.0434224, acc 0.98
2016-09-06T07:54:35.693077: step 15349, loss 0.0293669, acc 0.98
2016-09-06T07:54:36.724320: step 15350, loss 0.00461519, acc 1
2016-09-06T07:54:37.594595: step 15351, loss 0.00459513, acc 1
2016-09-06T07:54:38.424189: step 15352, loss 0.00517786, acc 1
2016-09-06T07:54:39.408494: step 15353, loss 0.0046032, acc 1
2016-09-06T07:54:40.216443: step 15354, loss 0.00778336, acc 1
2016-09-06T07:54:41.155603: step 15355, loss 0.0218522, acc 1
2016-09-06T07:54:42.008635: step 15356, loss 0.00473445, acc 1
2016-09-06T07:54:42.845090: step 15357, loss 0.0229057, acc 1
2016-09-06T07:54:43.717378: step 15358, loss 0.00469725, acc 1
2016-09-06T07:54:44.543458: step 15359, loss 0.0201539, acc 0.98
2016-09-06T07:54:45.321405: step 15360, loss 0.00836999, acc 1
2016-09-06T07:54:46.160848: step 15361, loss 0.00484796, acc 1
2016-09-06T07:54:47.010457: step 15362, loss 0.0245136, acc 0.98
2016-09-06T07:54:47.896932: step 15363, loss 0.00470701, acc 1
2016-09-06T07:54:48.727135: step 15364, loss 0.00853754, acc 1
2016-09-06T07:54:49.642047: step 15365, loss 0.00546335, acc 1
2016-09-06T07:54:50.498750: step 15366, loss 0.0316957, acc 0.98
2016-09-06T07:54:51.403799: step 15367, loss 0.00463684, acc 1
2016-09-06T07:54:52.215464: step 15368, loss 0.015889, acc 1
2016-09-06T07:54:53.148069: step 15369, loss 0.0325819, acc 0.98
2016-09-06T07:54:54.009846: step 15370, loss 0.00487808, acc 1
2016-09-06T07:54:54.806235: step 15371, loss 0.00450482, acc 1
2016-09-06T07:54:55.705422: step 15372, loss 0.00536259, acc 1
2016-09-06T07:54:56.518960: step 15373, loss 0.0242856, acc 1
2016-09-06T07:54:57.568193: step 15374, loss 0.0179919, acc 1
2016-09-06T07:54:58.363867: step 15375, loss 0.0223753, acc 0.98
2016-09-06T07:54:59.367896: step 15376, loss 0.00542638, acc 1
2016-09-06T07:55:00.190351: step 15377, loss 0.0309049, acc 0.98
2016-09-06T07:55:01.033397: step 15378, loss 0.0042189, acc 1
2016-09-06T07:55:01.856155: step 15379, loss 0.0173617, acc 1
2016-09-06T07:55:02.678450: step 15380, loss 0.00421966, acc 1
2016-09-06T07:55:03.543677: step 15381, loss 0.00411588, acc 1
2016-09-06T07:55:04.374964: step 15382, loss 0.0150175, acc 1
2016-09-06T07:55:05.196713: step 15383, loss 0.0396811, acc 0.98
2016-09-06T07:55:06.029941: step 15384, loss 0.0159005, acc 1
2016-09-06T07:55:06.846885: step 15385, loss 0.0500401, acc 0.98
2016-09-06T07:55:07.677993: step 15386, loss 0.0400926, acc 0.96
2016-09-06T07:55:08.508095: step 15387, loss 0.0277089, acc 0.98
2016-09-06T07:55:09.326128: step 15388, loss 0.00374475, acc 1
2016-09-06T07:55:10.162375: step 15389, loss 0.00367737, acc 1
2016-09-06T07:55:10.971440: step 15390, loss 0.00357599, acc 1
2016-09-06T07:55:11.785509: step 15391, loss 0.00630929, acc 1
2016-09-06T07:55:12.611481: step 15392, loss 0.0035854, acc 1
2016-09-06T07:55:13.450356: step 15393, loss 0.163909, acc 0.96
2016-09-06T07:55:14.263521: step 15394, loss 0.00332244, acc 1
2016-09-06T07:55:15.083956: step 15395, loss 0.00321794, acc 1
2016-09-06T07:55:15.890282: step 15396, loss 0.00309548, acc 1
2016-09-06T07:55:16.734256: step 15397, loss 0.00384677, acc 1
2016-09-06T07:55:17.701367: step 15398, loss 0.0318457, acc 0.98
2016-09-06T07:55:18.738574: step 15399, loss 0.0139203, acc 1
2016-09-06T07:55:19.672028: step 15400, loss 0.00429203, acc 1

Evaluation:
2016-09-06T07:55:23.451923: step 15400, loss 3.30869, acc 0.733584

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-15400

2016-09-06T07:55:25.235149: step 15401, loss 0.00386126, acc 1
2016-09-06T07:55:26.138047: step 15402, loss 0.0652579, acc 0.98
2016-09-06T07:55:27.220704: step 15403, loss 0.0146825, acc 1
2016-09-06T07:55:28.120328: step 15404, loss 0.00275031, acc 1
2016-09-06T07:55:29.045096: step 15405, loss 0.00268228, acc 1
2016-09-06T07:55:30.109453: step 15406, loss 0.0246635, acc 1
2016-09-06T07:55:31.123831: step 15407, loss 0.0263155, acc 1
2016-09-06T07:55:32.141709: step 15408, loss 0.0543595, acc 0.98
2016-09-06T07:55:33.144016: step 15409, loss 0.0144349, acc 1
2016-09-06T07:55:34.095524: step 15410, loss 0.00872983, acc 1
2016-09-06T07:55:34.970610: step 15411, loss 0.00866211, acc 1
2016-09-06T07:55:35.817330: step 15412, loss 0.0155298, acc 1
2016-09-06T07:55:36.663442: step 15413, loss 0.0458688, acc 0.98
2016-09-06T07:55:37.459402: step 15414, loss 0.032209, acc 0.98
2016-09-06T07:55:38.237441: step 15415, loss 0.00567655, acc 1
2016-09-06T07:55:39.077388: step 15416, loss 0.00293387, acc 1
2016-09-06T07:55:39.861806: step 15417, loss 0.00263065, acc 1
2016-09-06T07:55:40.665508: step 15418, loss 0.0243918, acc 0.98
2016-09-06T07:55:41.504180: step 15419, loss 0.00915929, acc 1
2016-09-06T07:55:42.304311: step 15420, loss 0.00259697, acc 1
2016-09-06T07:55:43.093985: step 15421, loss 0.0337913, acc 1
2016-09-06T07:55:43.943775: step 15422, loss 0.00547228, acc 1
2016-09-06T07:55:44.747382: step 15423, loss 0.00258692, acc 1
2016-09-06T07:55:45.618887: step 15424, loss 0.00349946, acc 1
2016-09-06T07:55:46.467352: step 15425, loss 0.0190225, acc 0.98
2016-09-06T07:55:47.284244: step 15426, loss 0.0100446, acc 1
2016-09-06T07:55:48.129404: step 15427, loss 0.00273089, acc 1
2016-09-06T07:55:48.953049: step 15428, loss 0.0137409, acc 1
2016-09-06T07:55:49.756393: step 15429, loss 0.013836, acc 1
2016-09-06T07:55:50.542414: step 15430, loss 0.00347511, acc 1
2016-09-06T07:55:51.376240: step 15431, loss 0.0164798, acc 1
2016-09-06T07:55:52.200059: step 15432, loss 0.017978, acc 0.98
2016-09-06T07:55:53.010122: step 15433, loss 0.0749495, acc 0.98
2016-09-06T07:55:53.835300: step 15434, loss 0.00281647, acc 1
2016-09-06T07:55:54.645526: step 15435, loss 0.00285549, acc 1
2016-09-06T07:55:55.457260: step 15436, loss 0.0139595, acc 1
2016-09-06T07:55:56.312640: step 15437, loss 0.00318194, acc 1
2016-09-06T07:55:57.121644: step 15438, loss 0.00282715, acc 1
2016-09-06T07:55:57.936579: step 15439, loss 0.0698924, acc 0.98
2016-09-06T07:55:58.770785: step 15440, loss 0.00305078, acc 1
2016-09-06T07:55:59.572829: step 15441, loss 0.00260938, acc 1
2016-09-06T07:56:00.391972: step 15442, loss 0.00321958, acc 1
2016-09-06T07:56:01.181765: step 15443, loss 0.0280107, acc 0.98
2016-09-06T07:56:02.018769: step 15444, loss 0.0022432, acc 1
2016-09-06T07:56:02.798971: step 15445, loss 0.0168634, acc 0.98
2016-09-06T07:56:03.631962: step 15446, loss 0.013022, acc 1
2016-09-06T07:56:04.472323: step 15447, loss 0.00246832, acc 1
2016-09-06T07:56:05.258611: step 15448, loss 0.00454152, acc 1
2016-09-06T07:56:06.068868: step 15449, loss 0.0126345, acc 1
2016-09-06T07:56:06.884827: step 15450, loss 0.00225651, acc 1
2016-09-06T07:56:07.677347: step 15451, loss 0.0422308, acc 0.98
2016-09-06T07:56:08.487781: step 15452, loss 0.00207152, acc 1
2016-09-06T07:56:09.310590: step 15453, loss 0.0158633, acc 1
2016-09-06T07:56:10.121855: step 15454, loss 0.00236081, acc 1
2016-09-06T07:56:10.929187: step 15455, loss 0.0177825, acc 0.98
2016-09-06T07:56:11.743994: step 15456, loss 0.00201871, acc 1
2016-09-06T07:56:12.547584: step 15457, loss 0.00232516, acc 1
2016-09-06T07:56:13.353811: step 15458, loss 0.0289955, acc 1
2016-09-06T07:56:14.179996: step 15459, loss 0.0308863, acc 0.98
2016-09-06T07:56:14.990967: step 15460, loss 0.0100226, acc 1
2016-09-06T07:56:15.816546: step 15461, loss 0.0527462, acc 0.98
2016-09-06T07:56:16.662549: step 15462, loss 0.00184284, acc 1
2016-09-06T07:56:17.471460: step 15463, loss 0.00212581, acc 1
2016-09-06T07:56:18.279848: step 15464, loss 0.00179776, acc 1
2016-09-06T07:56:19.111805: step 15465, loss 0.00911644, acc 1
2016-09-06T07:56:19.943722: step 15466, loss 0.00221952, acc 1
2016-09-06T07:56:20.769432: step 15467, loss 0.0217706, acc 1
2016-09-06T07:56:21.589088: step 15468, loss 0.0421936, acc 0.98
2016-09-06T07:56:22.393428: step 15469, loss 0.0401332, acc 0.98
2016-09-06T07:56:23.210698: step 15470, loss 0.0556501, acc 0.96
2016-09-06T07:56:24.037338: step 15471, loss 0.00955226, acc 1
2016-09-06T07:56:24.843226: step 15472, loss 0.199894, acc 0.98
2016-09-06T07:56:25.651626: step 15473, loss 0.00243668, acc 1
2016-09-06T07:56:26.504840: step 15474, loss 0.00476617, acc 1
2016-09-06T07:56:27.336375: step 15475, loss 0.00342432, acc 1
2016-09-06T07:56:28.162478: step 15476, loss 0.0263627, acc 0.98
2016-09-06T07:56:29.012567: step 15477, loss 0.00348123, acc 1
2016-09-06T07:56:29.816479: step 15478, loss 0.00422822, acc 1
2016-09-06T07:56:30.593438: step 15479, loss 0.0229437, acc 0.98
2016-09-06T07:56:31.425099: step 15480, loss 0.0139705, acc 1
2016-09-06T07:56:32.262304: step 15481, loss 0.00565302, acc 1
2016-09-06T07:56:33.063502: step 15482, loss 0.0375288, acc 0.98
2016-09-06T07:56:33.853662: step 15483, loss 0.00419283, acc 1
2016-09-06T07:56:34.696792: step 15484, loss 0.0195007, acc 1
2016-09-06T07:56:35.513876: step 15485, loss 0.0178952, acc 1
2016-09-06T07:56:36.389950: step 15486, loss 0.00396908, acc 1
2016-09-06T07:56:37.302763: step 15487, loss 0.0298463, acc 1
2016-09-06T07:56:38.116587: step 15488, loss 0.194787, acc 0.98
2016-09-06T07:56:38.947114: step 15489, loss 0.004758, acc 1
2016-09-06T07:56:39.818753: step 15490, loss 0.0302203, acc 0.98
2016-09-06T07:56:40.644465: step 15491, loss 0.0183986, acc 1
2016-09-06T07:56:41.494372: step 15492, loss 0.00436162, acc 1
2016-09-06T07:56:42.390139: step 15493, loss 0.055759, acc 0.98
2016-09-06T07:56:43.212737: step 15494, loss 0.00578246, acc 1
2016-09-06T07:56:44.020808: step 15495, loss 0.0310777, acc 1
2016-09-06T07:56:44.831468: step 15496, loss 0.00464314, acc 1
2016-09-06T07:56:45.674234: step 15497, loss 0.0181964, acc 1
2016-09-06T07:56:46.496602: step 15498, loss 0.0452047, acc 0.96
2016-09-06T07:56:47.307423: step 15499, loss 0.00478312, acc 1
2016-09-06T07:56:48.199896: step 15500, loss 0.00449413, acc 1

Evaluation:
2016-09-06T07:56:51.959342: step 15500, loss 3.3762, acc 0.728893

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-15500

2016-09-06T07:56:53.884245: step 15501, loss 0.048436, acc 0.96
2016-09-06T07:56:54.693561: step 15502, loss 0.00786996, acc 1
2016-09-06T07:56:55.564810: step 15503, loss 0.0178963, acc 1
2016-09-06T07:56:56.381403: step 15504, loss 0.0178107, acc 1
2016-09-06T07:56:57.225181: step 15505, loss 0.00461536, acc 1
2016-09-06T07:56:58.050257: step 15506, loss 0.0222937, acc 0.98
2016-09-06T07:56:58.850378: step 15507, loss 0.0568445, acc 0.98
2016-09-06T07:56:59.691634: step 15508, loss 0.0133261, acc 1
2016-09-06T07:57:00.562573: step 15509, loss 0.0682567, acc 0.98
2016-09-06T07:57:01.377807: step 15510, loss 0.0271462, acc 0.98
2016-09-06T07:57:02.158553: step 15511, loss 0.00501569, acc 1
2016-09-06T07:57:02.989240: step 15512, loss 0.00406165, acc 1
2016-09-06T07:57:03.813946: step 15513, loss 0.0140332, acc 1
2016-09-06T07:57:04.623644: step 15514, loss 0.00401407, acc 1
2016-09-06T07:57:05.388897: step 15515, loss 0.00457605, acc 1
2016-09-06T07:57:06.223493: step 15516, loss 0.04144, acc 0.98
2016-09-06T07:57:06.999427: step 15517, loss 0.0348204, acc 0.96
2016-09-06T07:57:07.800370: step 15518, loss 0.0326023, acc 0.98
2016-09-06T07:57:08.622138: step 15519, loss 0.0087711, acc 1
2016-09-06T07:57:09.414837: step 15520, loss 0.0222319, acc 0.98
2016-09-06T07:57:10.222673: step 15521, loss 0.0232564, acc 0.98
2016-09-06T07:57:11.061217: step 15522, loss 0.0181731, acc 1
2016-09-06T07:57:11.854817: step 15523, loss 0.00620204, acc 1
2016-09-06T07:57:12.644823: step 15524, loss 0.0241287, acc 0.98
2016-09-06T07:57:13.490500: step 15525, loss 0.00604391, acc 1
2016-09-06T07:57:14.256411: step 15526, loss 0.0195832, acc 1
2016-09-06T07:57:15.061945: step 15527, loss 0.0478575, acc 0.98
2016-09-06T07:57:15.880492: step 15528, loss 0.00499615, acc 1
2016-09-06T07:57:16.638866: step 15529, loss 0.0193656, acc 1
2016-09-06T07:57:17.458740: step 15530, loss 0.017179, acc 1
2016-09-06T07:57:18.295421: step 15531, loss 0.0423336, acc 0.96
2016-09-06T07:57:19.090320: step 15532, loss 0.0231201, acc 0.98
2016-09-06T07:57:19.890297: step 15533, loss 0.0603273, acc 0.96
2016-09-06T07:57:20.692203: step 15534, loss 0.0103307, acc 1
2016-09-06T07:57:21.482576: step 15535, loss 0.0276887, acc 1
2016-09-06T07:57:22.299683: step 15536, loss 0.00324569, acc 1
2016-09-06T07:57:23.157413: step 15537, loss 0.00415369, acc 1
2016-09-06T07:57:23.959196: step 15538, loss 0.0679118, acc 0.98
2016-09-06T07:57:24.769269: step 15539, loss 0.00725379, acc 1
2016-09-06T07:57:25.561888: step 15540, loss 0.0192826, acc 0.98
2016-09-06T07:57:26.355069: step 15541, loss 0.0079213, acc 1
2016-09-06T07:57:27.139803: step 15542, loss 0.0223552, acc 1
2016-09-06T07:57:27.959559: step 15543, loss 0.00332087, acc 1
2016-09-06T07:57:28.730450: step 15544, loss 0.00335712, acc 1
2016-09-06T07:57:29.531022: step 15545, loss 0.00309146, acc 1
2016-09-06T07:57:30.346756: step 15546, loss 0.0164736, acc 1
2016-09-06T07:57:31.154108: step 15547, loss 0.00659351, acc 1
2016-09-06T07:57:31.961534: step 15548, loss 0.0032967, acc 1
2016-09-06T07:57:32.795902: step 15549, loss 0.0124855, acc 1
2016-09-06T07:57:33.574780: step 15550, loss 0.00970934, acc 1
2016-09-06T07:57:34.378586: step 15551, loss 0.00495605, acc 1
2016-09-06T07:57:35.140528: step 15552, loss 0.00330845, acc 1
2016-09-06T07:57:35.935755: step 15553, loss 0.00300664, acc 1
2016-09-06T07:57:36.757132: step 15554, loss 0.0458818, acc 0.96
2016-09-06T07:57:37.579202: step 15555, loss 0.00917175, acc 1
2016-09-06T07:57:38.368568: step 15556, loss 0.0164874, acc 1
2016-09-06T07:57:39.201486: step 15557, loss 0.0450067, acc 0.96
2016-09-06T07:57:40.003602: step 15558, loss 0.0162814, acc 1
2016-09-06T07:57:40.895395: step 15559, loss 0.00302181, acc 1
2016-09-06T07:57:41.788360: step 15560, loss 0.0650221, acc 0.96
2016-09-06T07:57:42.703185: step 15561, loss 0.0143248, acc 1
2016-09-06T07:57:43.604229: step 15562, loss 0.00307979, acc 1
2016-09-06T07:57:44.573406: step 15563, loss 0.0036387, acc 1
2016-09-06T07:57:45.597760: step 15564, loss 0.00353899, acc 1
2016-09-06T07:57:46.621773: step 15565, loss 0.014589, acc 1
2016-09-06T07:57:47.538340: step 15566, loss 0.0209742, acc 0.98
2016-09-06T07:57:48.454384: step 15567, loss 0.00316044, acc 1
2016-09-06T07:57:49.349340: step 15568, loss 0.0134938, acc 1
2016-09-06T07:57:50.269151: step 15569, loss 0.00281815, acc 1
2016-09-06T07:57:51.196424: step 15570, loss 0.0027008, acc 1
2016-09-06T07:57:52.097402: step 15571, loss 0.0160548, acc 1
2016-09-06T07:57:52.934263: step 15572, loss 0.00266618, acc 1
2016-09-06T07:57:53.740154: step 15573, loss 0.0195037, acc 0.98
2016-09-06T07:57:54.554448: step 15574, loss 0.00263071, acc 1
2016-09-06T07:57:55.337340: step 15575, loss 0.0282579, acc 0.98
2016-09-06T07:57:56.191510: step 15576, loss 0.00305413, acc 1
2016-09-06T07:57:57.007709: step 15577, loss 0.0314129, acc 0.98
2016-09-06T07:57:57.780668: step 15578, loss 0.0169069, acc 1
2016-09-06T07:57:58.581285: step 15579, loss 0.0175412, acc 1
2016-09-06T07:57:59.411879: step 15580, loss 0.0382856, acc 0.98
2016-09-06T07:58:00.234582: step 15581, loss 0.0185374, acc 0.98
2016-09-06T07:58:01.025130: step 15582, loss 0.00247672, acc 1
2016-09-06T07:58:01.852502: step 15583, loss 0.0108825, acc 1
2016-09-06T07:58:02.651071: step 15584, loss 0.00606504, acc 1
2016-09-06T07:58:03.424829: step 15585, loss 0.028603, acc 0.98
2016-09-06T07:58:04.292258: step 15586, loss 0.059839, acc 0.98
2016-09-06T07:58:05.089347: step 15587, loss 0.00230672, acc 1
2016-09-06T07:58:05.886920: step 15588, loss 0.00866365, acc 1
2016-09-06T07:58:06.686898: step 15589, loss 0.00226682, acc 1
2016-09-06T07:58:07.464262: step 15590, loss 0.0349816, acc 0.98
2016-09-06T07:58:08.306727: step 15591, loss 0.00677806, acc 1
2016-09-06T07:58:09.128941: step 15592, loss 0.0180321, acc 0.98
2016-09-06T07:58:09.935671: step 15593, loss 0.0208005, acc 1
2016-09-06T07:58:10.750703: step 15594, loss 0.00642404, acc 1
2016-09-06T07:58:11.573406: step 15595, loss 0.0657491, acc 0.98
2016-09-06T07:58:12.373840: step 15596, loss 0.0355227, acc 0.98
2016-09-06T07:58:13.190740: step 15597, loss 0.00232946, acc 1
2016-09-06T07:58:14.016071: step 15598, loss 0.0272644, acc 0.98
2016-09-06T07:58:14.815457: step 15599, loss 0.018239, acc 0.98
2016-09-06T07:58:15.651203: step 15600, loss 0.0185316, acc 1

Evaluation:
2016-09-06T07:58:19.383768: step 15600, loss 2.54392, acc 0.731707

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-15600

2016-09-06T07:58:21.251114: step 15601, loss 0.00905814, acc 1
2016-09-06T07:58:22.069064: step 15602, loss 0.00198956, acc 1
2016-09-06T07:58:22.888923: step 15603, loss 0.0323647, acc 0.98
2016-09-06T07:58:23.749123: step 15604, loss 0.0115809, acc 1
2016-09-06T07:58:24.526047: step 15605, loss 0.0157337, acc 1
2016-09-06T07:58:25.360285: step 15606, loss 0.0308554, acc 0.98
2016-09-06T07:58:26.178001: step 15607, loss 0.0251731, acc 0.98
2016-09-06T07:58:26.983084: step 15608, loss 0.00260396, acc 1
2016-09-06T07:58:27.803024: step 15609, loss 0.0310089, acc 0.98
2016-09-06T07:58:28.642935: step 15610, loss 0.0204969, acc 0.98
2016-09-06T07:58:29.456007: step 15611, loss 0.00459726, acc 1
2016-09-06T07:58:30.267588: step 15612, loss 0.00190991, acc 1
2016-09-06T07:58:31.093430: step 15613, loss 0.0330271, acc 0.98
2016-09-06T07:58:31.897511: step 15614, loss 0.00203836, acc 1
2016-09-06T07:58:32.725210: step 15615, loss 0.012178, acc 1
2016-09-06T07:58:33.532447: step 15616, loss 0.0213599, acc 1
2016-09-06T07:58:34.330630: step 15617, loss 0.00196527, acc 1
2016-09-06T07:58:35.154335: step 15618, loss 0.00510463, acc 1
2016-09-06T07:58:35.977716: step 15619, loss 0.00988201, acc 1
2016-09-06T07:58:36.794071: step 15620, loss 0.0333982, acc 0.98
2016-09-06T07:58:37.641975: step 15621, loss 0.0185018, acc 1
2016-09-06T07:58:38.474543: step 15622, loss 0.0781052, acc 0.98
2016-09-06T07:58:39.284572: step 15623, loss 0.0177084, acc 0.98
2016-09-06T07:58:40.096506: step 15624, loss 0.00502612, acc 1
2016-09-06T07:58:40.932207: step 15625, loss 0.0145015, acc 1
2016-09-06T07:58:41.746817: step 15626, loss 0.00856956, acc 1
2016-09-06T07:58:42.572499: step 15627, loss 0.0209191, acc 1
2016-09-06T07:58:43.401845: step 15628, loss 0.0287639, acc 1
2016-09-06T07:58:44.226442: step 15629, loss 0.0136861, acc 1
2016-09-06T07:58:45.011180: step 15630, loss 0.01387, acc 1
2016-09-06T07:58:45.840228: step 15631, loss 0.0456851, acc 0.98
2016-09-06T07:58:46.642167: step 15632, loss 0.00215545, acc 1
2016-09-06T07:58:47.413591: step 15633, loss 0.0094136, acc 1
2016-09-06T07:58:48.239807: step 15634, loss 0.00262965, acc 1
2016-09-06T07:58:49.070501: step 15635, loss 0.040324, acc 0.96
2016-09-06T07:58:49.854626: step 15636, loss 0.00286115, acc 1
2016-09-06T07:58:50.663170: step 15637, loss 0.0118913, acc 1
2016-09-06T07:58:51.489550: step 15638, loss 0.0289687, acc 0.98
2016-09-06T07:58:52.301957: step 15639, loss 0.00186077, acc 1
2016-09-06T07:58:53.096551: step 15640, loss 0.0191831, acc 1
2016-09-06T07:58:53.913067: step 15641, loss 0.0273586, acc 0.98
2016-09-06T07:58:54.690673: step 15642, loss 0.00410334, acc 1
2016-09-06T07:58:55.505194: step 15643, loss 0.0505357, acc 0.96
2016-09-06T07:58:56.315936: step 15644, loss 0.0045132, acc 1
2016-09-06T07:58:57.104236: step 15645, loss 0.00287893, acc 1
2016-09-06T07:58:57.913651: step 15646, loss 0.00258399, acc 1
2016-09-06T07:58:58.693339: step 15647, loss 0.0156658, acc 1
2016-09-06T07:58:59.510911: step 15648, loss 0.0072786, acc 1
2016-09-06T07:59:00.343913: step 15649, loss 0.00266608, acc 1
2016-09-06T07:59:01.141629: step 15650, loss 0.0241066, acc 1
2016-09-06T07:59:01.957435: step 15651, loss 0.00577841, acc 1
2016-09-06T07:59:02.774940: step 15652, loss 0.0283289, acc 0.98
2016-09-06T07:59:03.573429: step 15653, loss 0.0367632, acc 0.98
2016-09-06T07:59:04.355190: step 15654, loss 0.00237705, acc 1
2016-09-06T07:59:05.179175: step 15655, loss 0.0020456, acc 1
2016-09-06T07:59:06.026515: step 15656, loss 0.00192214, acc 1
2016-09-06T07:59:06.824389: step 15657, loss 0.0113642, acc 1
2016-09-06T07:59:07.629318: step 15658, loss 0.0703224, acc 0.98
2016-09-06T07:59:08.444411: step 15659, loss 0.0655808, acc 0.96
2016-09-06T07:59:09.262554: step 15660, loss 0.00573342, acc 1
2016-09-06T07:59:10.070223: step 15661, loss 0.0279222, acc 1
2016-09-06T07:59:10.912758: step 15662, loss 0.150754, acc 0.96
2016-09-06T07:59:11.728312: step 15663, loss 0.00159721, acc 1
2016-09-06T07:59:12.533328: step 15664, loss 0.0347155, acc 0.98
2016-09-06T07:59:13.398022: step 15665, loss 0.0176245, acc 1
2016-09-06T07:59:14.233553: step 15666, loss 0.0818966, acc 0.96
2016-09-06T07:59:15.050300: step 15667, loss 0.0160356, acc 0.98
2016-09-06T07:59:15.869704: step 15668, loss 0.00463854, acc 1
2016-09-06T07:59:16.695112: step 15669, loss 0.012557, acc 1
2016-09-06T07:59:17.533579: step 15670, loss 0.00192909, acc 1
2016-09-06T07:59:18.365263: step 15671, loss 0.0252306, acc 0.98
2016-09-06T07:59:19.188454: step 15672, loss 0.0161809, acc 1
2016-09-06T07:59:20.015819: step 15673, loss 0.00197704, acc 1
2016-09-06T07:59:20.882092: step 15674, loss 0.031971, acc 0.96
2016-09-06T07:59:21.682559: step 15675, loss 0.00319661, acc 1
2016-09-06T07:59:22.504781: step 15676, loss 0.0218941, acc 0.98
2016-09-06T07:59:23.316597: step 15677, loss 0.0325902, acc 0.98
2016-09-06T07:59:24.133091: step 15678, loss 0.00330226, acc 1
2016-09-06T07:59:24.928645: step 15679, loss 0.0148196, acc 1
2016-09-06T07:59:25.728712: step 15680, loss 0.0336238, acc 0.98
2016-09-06T07:59:26.526886: step 15681, loss 0.0312095, acc 0.98
2016-09-06T07:59:27.317637: step 15682, loss 0.00790118, acc 1
2016-09-06T07:59:28.145821: step 15683, loss 0.0266357, acc 1
2016-09-06T07:59:28.977700: step 15684, loss 0.00541269, acc 1
2016-09-06T07:59:29.776312: step 15685, loss 0.02748, acc 0.98
2016-09-06T07:59:30.568508: step 15686, loss 0.00225532, acc 1
2016-09-06T07:59:31.364875: step 15687, loss 0.023461, acc 0.98
2016-09-06T07:59:32.181114: step 15688, loss 0.0136422, acc 1
2016-09-06T07:59:32.974573: step 15689, loss 0.0175638, acc 0.98
2016-09-06T07:59:33.788772: step 15690, loss 0.072146, acc 0.96
2016-09-06T07:59:34.569151: step 15691, loss 0.00187028, acc 1
2016-09-06T07:59:35.406585: step 15692, loss 0.00176194, acc 1
2016-09-06T07:59:36.239934: step 15693, loss 0.0176056, acc 1
2016-09-06T07:59:37.040258: step 15694, loss 0.00190933, acc 1
2016-09-06T07:59:37.839718: step 15695, loss 0.00222066, acc 1
2016-09-06T07:59:38.690911: step 15696, loss 0.0176348, acc 0.98
2016-09-06T07:59:39.478105: step 15697, loss 0.0180689, acc 0.98
2016-09-06T07:59:40.260035: step 15698, loss 0.0151124, acc 1
2016-09-06T07:59:41.068276: step 15699, loss 0.0118746, acc 1
2016-09-06T07:59:41.865038: step 15700, loss 0.00750312, acc 1

Evaluation:
2016-09-06T07:59:45.621409: step 15700, loss 2.52023, acc 0.719512

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-15700

2016-09-06T07:59:47.532333: step 15701, loss 0.0062754, acc 1
2016-09-06T07:59:48.334353: step 15702, loss 0.0351117, acc 0.98
2016-09-06T07:59:49.143032: step 15703, loss 0.00504913, acc 1
2016-09-06T07:59:49.989445: step 15704, loss 0.0154911, acc 1
2016-09-06T07:59:50.799928: step 15705, loss 0.0142475, acc 1
2016-09-06T07:59:51.622878: step 15706, loss 0.0155584, acc 1
2016-09-06T07:59:52.452923: step 15707, loss 0.00195941, acc 1
2016-09-06T07:59:53.256322: step 15708, loss 0.00203835, acc 1
2016-09-06T07:59:54.071835: step 15709, loss 0.0113461, acc 1
2016-09-06T07:59:54.887663: step 15710, loss 0.0020908, acc 1
2016-09-06T07:59:55.719371: step 15711, loss 0.00198871, acc 1
2016-09-06T07:59:56.526923: step 15712, loss 0.025025, acc 0.98
2016-09-06T07:59:57.338722: step 15713, loss 0.142811, acc 0.98
2016-09-06T07:59:58.181879: step 15714, loss 0.0316189, acc 0.96
2016-09-06T07:59:58.978866: step 15715, loss 0.042763, acc 0.98
2016-09-06T07:59:59.762277: step 15716, loss 0.0188537, acc 0.98
2016-09-06T08:00:00.598168: step 15717, loss 0.00169011, acc 1
2016-09-06T08:00:01.372455: step 15718, loss 0.040622, acc 1
2016-09-06T08:00:02.163839: step 15719, loss 0.00707681, acc 1
2016-09-06T08:00:03.008394: step 15720, loss 0.031176, acc 0.98
2016-09-06T08:00:03.800978: step 15721, loss 0.0266564, acc 0.98
2016-09-06T08:00:04.581327: step 15722, loss 0.0121896, acc 1
2016-09-06T08:00:05.405442: step 15723, loss 0.00452316, acc 1
2016-09-06T08:00:06.175570: step 15724, loss 0.00215166, acc 1
2016-09-06T08:00:07.008122: step 15725, loss 0.00317391, acc 1
2016-09-06T08:00:07.824177: step 15726, loss 0.0301735, acc 0.96
2016-09-06T08:00:08.606279: step 15727, loss 0.0288318, acc 0.98
2016-09-06T08:00:09.398376: step 15728, loss 0.00179864, acc 1
2016-09-06T08:00:10.227472: step 15729, loss 0.0398562, acc 0.98
2016-09-06T08:00:11.027894: step 15730, loss 0.0316994, acc 1
2016-09-06T08:00:11.822774: step 15731, loss 0.012672, acc 1
2016-09-06T08:00:12.649712: step 15732, loss 0.0458623, acc 0.98
2016-09-06T08:00:13.421890: step 15733, loss 0.0135155, acc 1
2016-09-06T08:00:14.226789: step 15734, loss 0.00636646, acc 1
2016-09-06T08:00:15.053840: step 15735, loss 0.00203685, acc 1
2016-09-06T08:00:15.841861: step 15736, loss 0.00252896, acc 1
2016-09-06T08:00:16.674381: step 15737, loss 0.00201908, acc 1
2016-09-06T08:00:17.510174: step 15738, loss 0.00915586, acc 1
2016-09-06T08:00:18.287690: step 15739, loss 0.0179564, acc 0.98
2016-09-06T08:00:19.077962: step 15740, loss 0.00200932, acc 1
2016-09-06T08:00:19.913902: step 15741, loss 0.0411484, acc 0.98
2016-09-06T08:00:20.713949: step 15742, loss 0.00192809, acc 1
2016-09-06T08:00:21.514237: step 15743, loss 0.00861501, acc 1
2016-09-06T08:00:22.248601: step 15744, loss 0.00207689, acc 1
2016-09-06T08:00:23.068096: step 15745, loss 0.0177678, acc 0.98
2016-09-06T08:00:23.883804: step 15746, loss 0.00751852, acc 1
2016-09-06T08:00:24.707216: step 15747, loss 0.00211042, acc 1
2016-09-06T08:00:25.498048: step 15748, loss 0.00202495, acc 1
2016-09-06T08:00:26.329309: step 15749, loss 0.00247125, acc 1
2016-09-06T08:00:27.165522: step 15750, loss 0.0108287, acc 1
2016-09-06T08:00:27.954450: step 15751, loss 0.006879, acc 1
2016-09-06T08:00:28.768118: step 15752, loss 0.00389095, acc 1
2016-09-06T08:00:29.570623: step 15753, loss 0.016776, acc 0.98
2016-09-06T08:00:30.366069: step 15754, loss 0.0682954, acc 0.94
2016-09-06T08:00:31.180974: step 15755, loss 0.00180603, acc 1
2016-09-06T08:00:32.003859: step 15756, loss 0.00198978, acc 1
2016-09-06T08:00:32.800694: step 15757, loss 0.0815597, acc 0.96
2016-09-06T08:00:33.596970: step 15758, loss 0.0164541, acc 0.98
2016-09-06T08:00:34.416150: step 15759, loss 0.0248656, acc 1
2016-09-06T08:00:35.184910: step 15760, loss 0.00302798, acc 1
2016-09-06T08:00:35.984795: step 15761, loss 0.0111664, acc 1
2016-09-06T08:00:36.781035: step 15762, loss 0.00315341, acc 1
2016-09-06T08:00:37.569273: step 15763, loss 0.0100009, acc 1
2016-09-06T08:00:38.379982: step 15764, loss 0.00170348, acc 1
2016-09-06T08:00:39.227184: step 15765, loss 0.00593333, acc 1
2016-09-06T08:00:40.038133: step 15766, loss 0.0328123, acc 0.98
2016-09-06T08:00:40.816654: step 15767, loss 0.0199297, acc 0.98
2016-09-06T08:00:41.662201: step 15768, loss 0.00481295, acc 1
2016-09-06T08:00:42.464243: step 15769, loss 0.0158962, acc 1
2016-09-06T08:00:43.278116: step 15770, loss 0.00333648, acc 1
2016-09-06T08:00:44.113067: step 15771, loss 0.0170052, acc 0.98
2016-09-06T08:00:44.936588: step 15772, loss 0.0781238, acc 0.98
2016-09-06T08:00:45.778343: step 15773, loss 0.00259147, acc 1
2016-09-06T08:00:46.592238: step 15774, loss 0.0016922, acc 1
2016-09-06T08:00:47.405111: step 15775, loss 0.0102873, acc 1
2016-09-06T08:00:48.215512: step 15776, loss 0.00155128, acc 1
2016-09-06T08:00:49.053099: step 15777, loss 0.0174118, acc 0.98
2016-09-06T08:00:49.877885: step 15778, loss 0.0205311, acc 0.98
2016-09-06T08:00:50.687527: step 15779, loss 0.00289293, acc 1
2016-09-06T08:00:51.535838: step 15780, loss 0.0305449, acc 0.98
2016-09-06T08:00:52.344045: step 15781, loss 0.00515813, acc 1
2016-09-06T08:00:53.153610: step 15782, loss 0.0129449, acc 1
2016-09-06T08:00:54.012394: step 15783, loss 0.00489285, acc 1
2016-09-06T08:00:54.841706: step 15784, loss 0.00180361, acc 1
2016-09-06T08:00:55.656199: step 15785, loss 0.0203521, acc 1
2016-09-06T08:00:56.480944: step 15786, loss 0.00449686, acc 1
2016-09-06T08:00:57.301451: step 15787, loss 0.0215572, acc 1
2016-09-06T08:00:58.133555: step 15788, loss 0.00866602, acc 1
2016-09-06T08:00:58.979757: step 15789, loss 0.00321687, acc 1
2016-09-06T08:00:59.840757: step 15790, loss 0.00239987, acc 1
2016-09-06T08:01:00.673879: step 15791, loss 0.00174276, acc 1
2016-09-06T08:01:01.485060: step 15792, loss 0.0141689, acc 1
2016-09-06T08:01:02.324517: step 15793, loss 0.0434599, acc 0.98
2016-09-06T08:01:03.135874: step 15794, loss 0.00189204, acc 1
2016-09-06T08:01:03.957525: step 15795, loss 0.0543649, acc 0.96
2016-09-06T08:01:04.806568: step 15796, loss 0.00220969, acc 1
2016-09-06T08:01:05.651982: step 15797, loss 0.0280075, acc 1
2016-09-06T08:01:06.449091: step 15798, loss 0.0125515, acc 1
2016-09-06T08:01:07.276698: step 15799, loss 0.059454, acc 0.96
2016-09-06T08:01:08.088639: step 15800, loss 0.00872371, acc 1

Evaluation:
2016-09-06T08:01:11.846356: step 15800, loss 3.0994, acc 0.727955

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-15800

2016-09-06T08:01:13.709616: step 15801, loss 0.0020436, acc 1
2016-09-06T08:01:14.522809: step 15802, loss 0.0165322, acc 1
2016-09-06T08:01:15.352600: step 15803, loss 0.0431198, acc 0.98
2016-09-06T08:01:16.209471: step 15804, loss 0.00796054, acc 1
2016-09-06T08:01:17.042423: step 15805, loss 0.0225582, acc 0.98
2016-09-06T08:01:17.837433: step 15806, loss 0.00300605, acc 1
2016-09-06T08:01:18.640986: step 15807, loss 0.00201254, acc 1
2016-09-06T08:01:19.458382: step 15808, loss 0.0141684, acc 1
2016-09-06T08:01:20.271556: step 15809, loss 0.00203464, acc 1
2016-09-06T08:01:21.067263: step 15810, loss 0.0122518, acc 1
2016-09-06T08:01:21.883525: step 15811, loss 0.00233864, acc 1
2016-09-06T08:01:22.689775: step 15812, loss 0.0201964, acc 0.98
2016-09-06T08:01:23.505756: step 15813, loss 0.00692622, acc 1
2016-09-06T08:01:24.323256: step 15814, loss 0.0235512, acc 0.98
2016-09-06T08:01:25.143616: step 15815, loss 0.0110998, acc 1
2016-09-06T08:01:25.925227: step 15816, loss 0.00229079, acc 1
2016-09-06T08:01:26.739129: step 15817, loss 0.0256191, acc 1
2016-09-06T08:01:27.555851: step 15818, loss 0.00207024, acc 1
2016-09-06T08:01:28.362905: step 15819, loss 0.0101995, acc 1
2016-09-06T08:01:29.172619: step 15820, loss 0.0161223, acc 0.98
2016-09-06T08:01:29.986914: step 15821, loss 0.0590867, acc 0.98
2016-09-06T08:01:30.801396: step 15822, loss 0.0112063, acc 1
2016-09-06T08:01:31.602306: step 15823, loss 0.0254636, acc 1
2016-09-06T08:01:32.433265: step 15824, loss 0.0216406, acc 0.98
2016-09-06T08:01:33.218590: step 15825, loss 0.00257186, acc 1
2016-09-06T08:01:34.009252: step 15826, loss 0.0183705, acc 0.98
2016-09-06T08:01:34.860379: step 15827, loss 0.0276582, acc 1
2016-09-06T08:01:35.659450: step 15828, loss 0.00187663, acc 1
2016-09-06T08:01:36.469278: step 15829, loss 0.0273337, acc 0.98
2016-09-06T08:01:37.320970: step 15830, loss 0.0177986, acc 0.98
2016-09-06T08:01:38.176094: step 15831, loss 0.0035916, acc 1
2016-09-06T08:01:39.015488: step 15832, loss 0.00629795, acc 1
2016-09-06T08:01:39.881594: step 15833, loss 0.0215485, acc 0.98
2016-09-06T08:01:40.698819: step 15834, loss 0.00179483, acc 1
2016-09-06T08:01:41.538840: step 15835, loss 0.0433796, acc 0.98
2016-09-06T08:01:42.397510: step 15836, loss 0.00175141, acc 1
2016-09-06T08:01:43.212817: step 15837, loss 0.0486985, acc 0.98
2016-09-06T08:01:43.997762: step 15838, loss 0.00169519, acc 1
2016-09-06T08:01:44.830097: step 15839, loss 0.00170131, acc 1
2016-09-06T08:01:45.633946: step 15840, loss 0.00168531, acc 1
2016-09-06T08:01:46.430473: step 15841, loss 0.00216875, acc 1
2016-09-06T08:01:47.270394: step 15842, loss 0.0162947, acc 1
2016-09-06T08:01:48.087639: step 15843, loss 0.00348775, acc 1
2016-09-06T08:01:48.867413: step 15844, loss 0.022129, acc 1
2016-09-06T08:01:49.669943: step 15845, loss 0.0183304, acc 0.98
2016-09-06T08:01:50.507944: step 15846, loss 0.0103077, acc 1
2016-09-06T08:01:51.300248: step 15847, loss 0.00313117, acc 1
2016-09-06T08:01:52.115199: step 15848, loss 0.00343622, acc 1
2016-09-06T08:01:52.937492: step 15849, loss 0.0148399, acc 1
2016-09-06T08:01:53.724187: step 15850, loss 0.0219209, acc 1
2016-09-06T08:01:54.522588: step 15851, loss 0.0415042, acc 0.98
2016-09-06T08:01:55.359412: step 15852, loss 0.0233856, acc 0.98
2016-09-06T08:01:56.144346: step 15853, loss 0.0138774, acc 1
2016-09-06T08:01:56.944102: step 15854, loss 0.00806512, acc 1
2016-09-06T08:01:57.765624: step 15855, loss 0.00156816, acc 1
2016-09-06T08:01:58.552443: step 15856, loss 0.0307495, acc 0.98
2016-09-06T08:01:59.350028: step 15857, loss 0.0110283, acc 1
2016-09-06T08:02:00.182096: step 15858, loss 0.00401303, acc 1
2016-09-06T08:02:00.966908: step 15859, loss 0.0116972, acc 1
2016-09-06T08:02:01.764990: step 15860, loss 0.0418699, acc 0.98
2016-09-06T08:02:02.587069: step 15861, loss 0.0093702, acc 1
2016-09-06T08:02:03.386505: step 15862, loss 0.00219108, acc 1
2016-09-06T08:02:04.193415: step 15863, loss 0.0172508, acc 0.98
2016-09-06T08:02:05.025590: step 15864, loss 0.01911, acc 0.98
2016-09-06T08:02:05.819120: step 15865, loss 0.00264737, acc 1
2016-09-06T08:02:06.653690: step 15866, loss 0.00166607, acc 1
2016-09-06T08:02:07.447990: step 15867, loss 0.00168789, acc 1
2016-09-06T08:02:08.238722: step 15868, loss 0.00308082, acc 1
2016-09-06T08:02:09.049430: step 15869, loss 0.0329483, acc 0.98
2016-09-06T08:02:09.844113: step 15870, loss 0.00244054, acc 1
2016-09-06T08:02:10.648147: step 15871, loss 0.00691107, acc 1
2016-09-06T08:02:11.426181: step 15872, loss 0.00165977, acc 1
2016-09-06T08:02:12.243369: step 15873, loss 0.00161563, acc 1
2016-09-06T08:02:13.059549: step 15874, loss 0.017554, acc 0.98
2016-09-06T08:02:13.884709: step 15875, loss 0.00194829, acc 1
2016-09-06T08:02:14.701697: step 15876, loss 0.00160467, acc 1
2016-09-06T08:02:15.490396: step 15877, loss 0.00313228, acc 1
2016-09-06T08:02:16.286487: step 15878, loss 0.0348162, acc 0.98
2016-09-06T08:02:17.108592: step 15879, loss 0.0178879, acc 0.98
2016-09-06T08:02:17.878657: step 15880, loss 0.00830636, acc 1
2016-09-06T08:02:18.676600: step 15881, loss 0.0106679, acc 1
2016-09-06T08:02:19.504644: step 15882, loss 0.0276819, acc 1
2016-09-06T08:02:20.338171: step 15883, loss 0.00236687, acc 1
2016-09-06T08:02:21.158662: step 15884, loss 0.00153932, acc 1
2016-09-06T08:02:21.967840: step 15885, loss 0.169167, acc 0.96
2016-09-06T08:02:22.761340: step 15886, loss 0.00205798, acc 1
2016-09-06T08:02:23.538451: step 15887, loss 0.00152419, acc 1
2016-09-06T08:02:24.369930: step 15888, loss 0.0215136, acc 1
2016-09-06T08:02:25.156061: step 15889, loss 0.0177594, acc 0.98
2016-09-06T08:02:25.954975: step 15890, loss 0.0111379, acc 1
2016-09-06T08:02:26.787315: step 15891, loss 0.0044123, acc 1
2016-09-06T08:02:27.561773: step 15892, loss 0.00573144, acc 1
2016-09-06T08:02:28.365564: step 15893, loss 0.0428739, acc 0.98
2016-09-06T08:02:29.196392: step 15894, loss 0.0297805, acc 0.98
2016-09-06T08:02:29.993754: step 15895, loss 0.00546218, acc 1
2016-09-06T08:02:30.804845: step 15896, loss 0.00721445, acc 1
2016-09-06T08:02:31.613383: step 15897, loss 0.00594556, acc 1
2016-09-06T08:02:32.387336: step 15898, loss 0.00152494, acc 1
2016-09-06T08:02:33.182079: step 15899, loss 0.00974168, acc 1
2016-09-06T08:02:34.029185: step 15900, loss 0.0163964, acc 1

Evaluation:
2016-09-06T08:02:37.741507: step 15900, loss 2.77542, acc 0.726079

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-15900

2016-09-06T08:02:39.754336: step 15901, loss 0.00196861, acc 1
2016-09-06T08:02:40.590360: step 15902, loss 0.0224728, acc 0.98
2016-09-06T08:02:41.396695: step 15903, loss 0.050753, acc 0.98
2016-09-06T08:02:42.167646: step 15904, loss 0.0203492, acc 1
2016-09-06T08:02:42.950441: step 15905, loss 0.00793736, acc 1
2016-09-06T08:02:43.781786: step 15906, loss 0.0073809, acc 1
2016-09-06T08:02:44.569602: step 15907, loss 0.0299887, acc 0.98
2016-09-06T08:02:45.382406: step 15908, loss 0.00178118, acc 1
2016-09-06T08:02:46.209479: step 15909, loss 0.0112178, acc 1
2016-09-06T08:02:47.014112: step 15910, loss 0.0160553, acc 1
2016-09-06T08:02:47.834188: step 15911, loss 0.0154682, acc 1
2016-09-06T08:02:48.668250: step 15912, loss 0.0187269, acc 0.98
2016-09-06T08:02:49.450566: step 15913, loss 0.0339139, acc 0.96
2016-09-06T08:02:50.287959: step 15914, loss 0.00420688, acc 1
2016-09-06T08:02:51.106924: step 15915, loss 0.0211165, acc 0.98
2016-09-06T08:02:51.893314: step 15916, loss 0.00307444, acc 1
2016-09-06T08:02:52.688643: step 15917, loss 0.0020435, acc 1
2016-09-06T08:02:53.493655: step 15918, loss 0.0153016, acc 1
2016-09-06T08:02:54.298156: step 15919, loss 0.0126184, acc 1
2016-09-06T08:02:55.125002: step 15920, loss 0.00200841, acc 1
2016-09-06T08:02:55.972923: step 15921, loss 0.00762291, acc 1
2016-09-06T08:02:56.806204: step 15922, loss 0.0899008, acc 0.94
2016-09-06T08:02:57.621025: step 15923, loss 0.0128494, acc 1
2016-09-06T08:02:58.435494: step 15924, loss 0.029216, acc 0.98
2016-09-06T08:02:59.232561: step 15925, loss 0.00209785, acc 1
2016-09-06T08:03:00.030569: step 15926, loss 0.0147272, acc 1
2016-09-06T08:03:00.898592: step 15927, loss 0.00309455, acc 1
2016-09-06T08:03:01.736690: step 15928, loss 0.00241111, acc 1
2016-09-06T08:03:02.533434: step 15929, loss 0.00645711, acc 1
2016-09-06T08:03:03.359123: step 15930, loss 0.0134769, acc 1
2016-09-06T08:03:04.177109: step 15931, loss 0.00518697, acc 1
2016-09-06T08:03:04.994311: step 15932, loss 0.00275903, acc 1
2016-09-06T08:03:05.843355: step 15933, loss 0.0304727, acc 0.98
2016-09-06T08:03:06.667448: step 15934, loss 0.0406114, acc 0.98
2016-09-06T08:03:07.442858: step 15935, loss 0.0019342, acc 1
2016-09-06T08:03:08.210660: step 15936, loss 0.0099409, acc 1
2016-09-06T08:03:09.021976: step 15937, loss 0.0197951, acc 0.98
2016-09-06T08:03:09.841605: step 15938, loss 0.0400442, acc 0.98
2016-09-06T08:03:10.700367: step 15939, loss 0.00857218, acc 1
2016-09-06T08:03:11.493905: step 15940, loss 0.0294297, acc 0.98
2016-09-06T08:03:12.273450: step 15941, loss 0.0175279, acc 0.98
2016-09-06T08:03:13.148466: step 15942, loss 0.0308189, acc 0.98
2016-09-06T08:03:14.013402: step 15943, loss 0.00249802, acc 1
2016-09-06T08:03:14.771267: step 15944, loss 0.00833515, acc 1
2016-09-06T08:03:15.566374: step 15945, loss 0.0310323, acc 0.98
2016-09-06T08:03:16.370121: step 15946, loss 0.0302652, acc 0.98
2016-09-06T08:03:17.177903: step 15947, loss 0.00174552, acc 1
2016-09-06T08:03:17.988566: step 15948, loss 0.00175682, acc 1
2016-09-06T08:03:18.804815: step 15949, loss 0.00371996, acc 1
2016-09-06T08:03:19.607954: step 15950, loss 0.0219467, acc 0.98
2016-09-06T08:03:20.423511: step 15951, loss 0.0393656, acc 0.98
2016-09-06T08:03:21.242180: step 15952, loss 0.00174657, acc 1
2016-09-06T08:03:22.038284: step 15953, loss 0.00794455, acc 1
2016-09-06T08:03:22.861421: step 15954, loss 0.0030559, acc 1
2016-09-06T08:03:23.671973: step 15955, loss 0.0017308, acc 1
2016-09-06T08:03:24.441481: step 15956, loss 0.00171558, acc 1
2016-09-06T08:03:25.253355: step 15957, loss 0.00182153, acc 1
2016-09-06T08:03:26.105751: step 15958, loss 0.0016969, acc 1
2016-09-06T08:03:26.897729: step 15959, loss 0.0016782, acc 1
2016-09-06T08:03:27.710198: step 15960, loss 0.00219235, acc 1
2016-09-06T08:03:28.526482: step 15961, loss 0.00168239, acc 1
2016-09-06T08:03:29.309463: step 15962, loss 0.075678, acc 0.98
2016-09-06T08:03:30.111621: step 15963, loss 0.0261113, acc 0.98
2016-09-06T08:03:30.915542: step 15964, loss 0.00829375, acc 1
2016-09-06T08:03:31.707596: step 15965, loss 0.00153785, acc 1
2016-09-06T08:03:32.497825: step 15966, loss 0.0101148, acc 1
2016-09-06T08:03:33.351087: step 15967, loss 0.0120257, acc 1
2016-09-06T08:03:34.148981: step 15968, loss 0.00197852, acc 1
2016-09-06T08:03:34.948847: step 15969, loss 0.0100809, acc 1
2016-09-06T08:03:35.766770: step 15970, loss 0.00201497, acc 1
2016-09-06T08:03:36.569044: step 15971, loss 0.00734757, acc 1
2016-09-06T08:03:37.359386: step 15972, loss 0.0129598, acc 1
2016-09-06T08:03:38.153748: step 15973, loss 0.00151595, acc 1
2016-09-06T08:03:38.924147: step 15974, loss 0.0168324, acc 0.98
2016-09-06T08:03:39.727145: step 15975, loss 0.0256146, acc 0.98
2016-09-06T08:03:40.563193: step 15976, loss 0.00201897, acc 1
2016-09-06T08:03:41.342017: step 15977, loss 0.0330304, acc 0.96
2016-09-06T08:03:42.169078: step 15978, loss 0.00306676, acc 1
2016-09-06T08:03:42.999840: step 15979, loss 0.0365209, acc 0.96
2016-09-06T08:03:43.775534: step 15980, loss 0.00224352, acc 1
2016-09-06T08:03:44.589221: step 15981, loss 0.00149769, acc 1
2016-09-06T08:03:45.407324: step 15982, loss 0.0234713, acc 1
2016-09-06T08:03:46.193331: step 15983, loss 0.00213732, acc 1
2016-09-06T08:03:46.997976: step 15984, loss 0.0119448, acc 1
2016-09-06T08:03:47.832850: step 15985, loss 0.00876894, acc 1
2016-09-06T08:03:48.603648: step 15986, loss 0.0072716, acc 1
2016-09-06T08:03:49.414405: step 15987, loss 0.0217631, acc 0.98
2016-09-06T08:03:50.249347: step 15988, loss 0.0326649, acc 0.98
2016-09-06T08:03:51.017164: step 15989, loss 0.0213193, acc 1
2016-09-06T08:03:51.861853: step 15990, loss 0.00981462, acc 1
2016-09-06T08:03:52.669680: step 15991, loss 0.00160597, acc 1
2016-09-06T08:03:53.422003: step 15992, loss 0.011073, acc 1
2016-09-06T08:03:54.239179: step 15993, loss 0.0220623, acc 0.98
2016-09-06T08:03:55.043144: step 15994, loss 0.0649301, acc 0.98
2016-09-06T08:03:55.840264: step 15995, loss 0.00327205, acc 1
2016-09-06T08:03:56.649600: step 15996, loss 0.00158798, acc 1
2016-09-06T08:03:57.464975: step 15997, loss 0.00960062, acc 1
2016-09-06T08:03:58.251634: step 15998, loss 0.0162459, acc 0.98
2016-09-06T08:03:59.058004: step 15999, loss 0.00156688, acc 1
2016-09-06T08:03:59.858310: step 16000, loss 0.0147855, acc 1

Evaluation:
2016-09-06T08:04:03.588042: step 16000, loss 2.91565, acc 0.728893

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-16000

2016-09-06T08:04:05.518064: step 16001, loss 0.0016748, acc 1
2016-09-06T08:04:06.352195: step 16002, loss 0.00160805, acc 1
2016-09-06T08:04:07.166377: step 16003, loss 0.0124742, acc 1
2016-09-06T08:04:07.957695: step 16004, loss 0.0842965, acc 0.96
2016-09-06T08:04:08.748398: step 16005, loss 0.0266262, acc 1
2016-09-06T08:04:09.571223: step 16006, loss 0.0075457, acc 1
2016-09-06T08:04:10.399662: step 16007, loss 0.0203235, acc 0.98
2016-09-06T08:04:11.209924: step 16008, loss 0.0113998, acc 1
2016-09-06T08:04:12.015537: step 16009, loss 0.00613091, acc 1
2016-09-06T08:04:12.776687: step 16010, loss 0.00212506, acc 1
2016-09-06T08:04:13.589752: step 16011, loss 0.017867, acc 1
2016-09-06T08:04:14.438248: step 16012, loss 0.0153537, acc 0.98
2016-09-06T08:04:15.225395: step 16013, loss 0.00281406, acc 1
2016-09-06T08:04:16.021747: step 16014, loss 0.00176024, acc 1
2016-09-06T08:04:16.862152: step 16015, loss 0.0100068, acc 1
2016-09-06T08:04:17.631509: step 16016, loss 0.0149838, acc 1
2016-09-06T08:04:18.451489: step 16017, loss 0.00146531, acc 1
2016-09-06T08:04:19.288741: step 16018, loss 0.0174635, acc 1
2016-09-06T08:04:20.098017: step 16019, loss 0.0443277, acc 0.96
2016-09-06T08:04:20.927871: step 16020, loss 0.00166149, acc 1
2016-09-06T08:04:21.767560: step 16021, loss 0.00156062, acc 1
2016-09-06T08:04:22.563021: step 16022, loss 0.0328695, acc 0.98
2016-09-06T08:04:23.397396: step 16023, loss 0.00489753, acc 1
2016-09-06T08:04:24.231343: step 16024, loss 0.00141343, acc 1
2016-09-06T08:04:25.053480: step 16025, loss 0.0015181, acc 1
2016-09-06T08:04:25.871758: step 16026, loss 0.00768538, acc 1
2016-09-06T08:04:26.725083: step 16027, loss 0.0240404, acc 1
2016-09-06T08:04:27.542911: step 16028, loss 0.0018302, acc 1
2016-09-06T08:04:28.351616: step 16029, loss 0.00467078, acc 1
2016-09-06T08:04:29.177628: step 16030, loss 0.0103923, acc 1
2016-09-06T08:04:30.017423: step 16031, loss 0.002683, acc 1
2016-09-06T08:04:30.837404: step 16032, loss 0.0329838, acc 0.98
2016-09-06T08:04:31.680495: step 16033, loss 0.00135519, acc 1
2016-09-06T08:04:32.531826: step 16034, loss 0.0373371, acc 0.98
2016-09-06T08:04:33.335450: step 16035, loss 0.00146408, acc 1
2016-09-06T08:04:34.129379: step 16036, loss 0.0158331, acc 1
2016-09-06T08:04:34.945020: step 16037, loss 0.0342934, acc 0.98
2016-09-06T08:04:35.717405: step 16038, loss 0.00172331, acc 1
2016-09-06T08:04:36.533041: step 16039, loss 0.00155055, acc 1
2016-09-06T08:04:37.355593: step 16040, loss 0.00182347, acc 1
2016-09-06T08:04:38.184903: step 16041, loss 0.00249031, acc 1
2016-09-06T08:04:38.970430: step 16042, loss 0.00154609, acc 1
2016-09-06T08:04:39.806527: step 16043, loss 0.0162082, acc 1
2016-09-06T08:04:40.593590: step 16044, loss 0.0149616, acc 1
2016-09-06T08:04:41.381983: step 16045, loss 0.0119239, acc 1
2016-09-06T08:04:42.224981: step 16046, loss 0.0255849, acc 1
2016-09-06T08:04:43.031750: step 16047, loss 0.00136289, acc 1
2016-09-06T08:04:43.827181: step 16048, loss 0.047118, acc 0.98
2016-09-06T08:04:44.668002: step 16049, loss 0.0208801, acc 1
2016-09-06T08:04:45.471567: step 16050, loss 0.0139759, acc 1
2016-09-06T08:04:46.268593: step 16051, loss 0.0190077, acc 0.98
2016-09-06T08:04:47.078933: step 16052, loss 0.031044, acc 0.98
2016-09-06T08:04:47.880190: step 16053, loss 0.0590156, acc 0.98
2016-09-06T08:04:48.699556: step 16054, loss 0.00265119, acc 1
2016-09-06T08:04:49.515878: step 16055, loss 0.00223397, acc 1
2016-09-06T08:04:50.323395: step 16056, loss 0.0170721, acc 0.98
2016-09-06T08:04:51.148589: step 16057, loss 0.00149437, acc 1
2016-09-06T08:04:51.985079: step 16058, loss 0.00495105, acc 1
2016-09-06T08:04:52.810482: step 16059, loss 0.00429805, acc 1
2016-09-06T08:04:53.647007: step 16060, loss 0.0019414, acc 1
2016-09-06T08:04:54.491547: step 16061, loss 0.0217376, acc 1
2016-09-06T08:04:55.320447: step 16062, loss 0.00224975, acc 1
2016-09-06T08:04:56.123490: step 16063, loss 0.0161042, acc 0.98
2016-09-06T08:04:56.941609: step 16064, loss 0.0128373, acc 1
2016-09-06T08:04:57.744265: step 16065, loss 0.0230242, acc 0.98
2016-09-06T08:04:58.531676: step 16066, loss 0.0124804, acc 1
2016-09-06T08:04:59.358720: step 16067, loss 0.0261597, acc 0.98
2016-09-06T08:05:00.169492: step 16068, loss 0.0110799, acc 1
2016-09-06T08:05:01.055361: step 16069, loss 0.0734757, acc 0.98
2016-09-06T08:05:01.879567: step 16070, loss 0.0194312, acc 1
2016-09-06T08:05:02.700964: step 16071, loss 0.0366496, acc 0.98
2016-09-06T08:05:03.517964: step 16072, loss 0.016943, acc 0.98
2016-09-06T08:05:04.320903: step 16073, loss 0.00352907, acc 1
2016-09-06T08:05:05.142999: step 16074, loss 0.028788, acc 1
2016-09-06T08:05:05.935955: step 16075, loss 0.00310662, acc 1
2016-09-06T08:05:06.732701: step 16076, loss 0.00709553, acc 1
2016-09-06T08:05:07.537828: step 16077, loss 0.0216897, acc 1
2016-09-06T08:05:08.355003: step 16078, loss 0.00465108, acc 1
2016-09-06T08:05:09.170954: step 16079, loss 0.0113789, acc 1
2016-09-06T08:05:10.022613: step 16080, loss 0.044812, acc 0.96
2016-09-06T08:05:10.809437: step 16081, loss 0.0163895, acc 1
2016-09-06T08:05:11.645164: step 16082, loss 0.0215258, acc 1
2016-09-06T08:05:12.467820: step 16083, loss 0.0391394, acc 0.98
2016-09-06T08:05:13.261165: step 16084, loss 0.0134572, acc 1
2016-09-06T08:05:14.068139: step 16085, loss 0.0191003, acc 1
2016-09-06T08:05:14.890037: step 16086, loss 0.0146201, acc 1
2016-09-06T08:05:15.689404: step 16087, loss 0.00182401, acc 1
2016-09-06T08:05:16.497108: step 16088, loss 0.00260989, acc 1
2016-09-06T08:05:17.316794: step 16089, loss 0.0030582, acc 1
2016-09-06T08:05:18.118616: step 16090, loss 0.00184472, acc 1
2016-09-06T08:05:18.924319: step 16091, loss 0.00205215, acc 1
2016-09-06T08:05:19.790014: step 16092, loss 0.0302648, acc 0.96
2016-09-06T08:05:20.622385: step 16093, loss 0.0153604, acc 1
2016-09-06T08:05:21.443777: step 16094, loss 0.0109625, acc 1
2016-09-06T08:05:22.301691: step 16095, loss 0.0415103, acc 0.98
2016-09-06T08:05:23.115870: step 16096, loss 0.0623758, acc 0.96
2016-09-06T08:05:23.907404: step 16097, loss 0.013125, acc 1
2016-09-06T08:05:24.730861: step 16098, loss 0.00253037, acc 1
2016-09-06T08:05:25.531884: step 16099, loss 0.0238923, acc 1
2016-09-06T08:05:26.347695: step 16100, loss 0.00172246, acc 1

Evaluation:
2016-09-06T08:05:30.097342: step 16100, loss 2.89471, acc 0.718574

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-16100

2016-09-06T08:05:32.084911: step 16101, loss 0.00171277, acc 1
2016-09-06T08:05:32.888073: step 16102, loss 0.0159858, acc 0.98
2016-09-06T08:05:33.724757: step 16103, loss 0.0019868, acc 1
2016-09-06T08:05:34.557726: step 16104, loss 0.0176055, acc 0.98
2016-09-06T08:05:35.373893: step 16105, loss 0.0775097, acc 0.96
2016-09-06T08:05:36.200685: step 16106, loss 0.00442998, acc 1
2016-09-06T08:05:37.065161: step 16107, loss 0.0130011, acc 1
2016-09-06T08:05:37.865195: step 16108, loss 0.0285107, acc 0.98
2016-09-06T08:05:38.663400: step 16109, loss 0.00528645, acc 1
2016-09-06T08:05:39.443029: step 16110, loss 0.0184173, acc 1
2016-09-06T08:05:40.272581: step 16111, loss 0.00770786, acc 1
2016-09-06T08:05:41.061416: step 16112, loss 0.00156248, acc 1
2016-09-06T08:05:41.879005: step 16113, loss 0.0027905, acc 1
2016-09-06T08:05:42.677407: step 16114, loss 0.0173448, acc 1
2016-09-06T08:05:43.468422: step 16115, loss 0.0159253, acc 1
2016-09-06T08:05:44.277664: step 16116, loss 0.0408722, acc 0.96
2016-09-06T08:05:45.104032: step 16117, loss 0.00696431, acc 1
2016-09-06T08:05:45.884193: step 16118, loss 0.00157276, acc 1
2016-09-06T08:05:46.694261: step 16119, loss 0.00163996, acc 1
2016-09-06T08:05:47.501949: step 16120, loss 0.00196217, acc 1
2016-09-06T08:05:48.294512: step 16121, loss 0.021255, acc 1
2016-09-06T08:05:49.098422: step 16122, loss 0.0157574, acc 0.98
2016-09-06T08:05:49.903863: step 16123, loss 0.019616, acc 0.98
2016-09-06T08:05:50.688163: step 16124, loss 0.00166482, acc 1
2016-09-06T08:05:51.478493: step 16125, loss 0.0144047, acc 1
2016-09-06T08:05:52.305533: step 16126, loss 0.00265134, acc 1
2016-09-06T08:05:53.095341: step 16127, loss 0.00395426, acc 1
2016-09-06T08:05:53.833912: step 16128, loss 0.00208518, acc 1
2016-09-06T08:05:54.681649: step 16129, loss 0.0113642, acc 1
2016-09-06T08:05:55.492021: step 16130, loss 0.0290199, acc 0.98
2016-09-06T08:05:56.331925: step 16131, loss 0.00398679, acc 1
2016-09-06T08:05:57.155979: step 16132, loss 0.0170999, acc 0.98
2016-09-06T08:05:57.978428: step 16133, loss 0.0267051, acc 1
2016-09-06T08:05:58.789452: step 16134, loss 0.00937363, acc 1
2016-09-06T08:05:59.614266: step 16135, loss 0.0015788, acc 1
2016-09-06T08:06:00.448991: step 16136, loss 0.0073279, acc 1
2016-09-06T08:06:01.239952: step 16137, loss 0.0395592, acc 0.98
2016-09-06T08:06:02.045232: step 16138, loss 0.00240166, acc 1
2016-09-06T08:06:02.852905: step 16139, loss 0.00164913, acc 1
2016-09-06T08:06:03.649708: step 16140, loss 0.00172894, acc 1
2016-09-06T08:06:04.452042: step 16141, loss 0.0213421, acc 0.98
2016-09-06T08:06:05.259020: step 16142, loss 0.0255728, acc 1
2016-09-06T08:06:06.066822: step 16143, loss 0.00502575, acc 1
2016-09-06T08:06:06.910443: step 16144, loss 0.0017238, acc 1
2016-09-06T08:06:07.718539: step 16145, loss 0.0174601, acc 1
2016-09-06T08:06:08.556620: step 16146, loss 0.00236531, acc 1
2016-09-06T08:06:09.381006: step 16147, loss 0.040819, acc 0.98
2016-09-06T08:06:10.218526: step 16148, loss 0.00174696, acc 1
2016-09-06T08:06:11.033383: step 16149, loss 0.00234618, acc 1
2016-09-06T08:06:11.858706: step 16150, loss 0.00257422, acc 1
2016-09-06T08:06:12.688853: step 16151, loss 0.00545717, acc 1
2016-09-06T08:06:13.468993: step 16152, loss 0.00206875, acc 1
2016-09-06T08:06:14.314363: step 16153, loss 0.0108203, acc 1
2016-09-06T08:06:15.112883: step 16154, loss 0.0614295, acc 0.96
2016-09-06T08:06:15.933566: step 16155, loss 0.0120654, acc 1
2016-09-06T08:06:16.767629: step 16156, loss 0.0972733, acc 0.98
2016-09-06T08:06:17.562634: step 16157, loss 0.0283626, acc 0.98
2016-09-06T08:06:18.382237: step 16158, loss 0.00335064, acc 1
2016-09-06T08:06:19.205578: step 16159, loss 0.00181824, acc 1
2016-09-06T08:06:20.054074: step 16160, loss 0.00160252, acc 1
2016-09-06T08:06:20.861520: step 16161, loss 0.00808161, acc 1
2016-09-06T08:06:21.681959: step 16162, loss 0.00847596, acc 1
2016-09-06T08:06:22.512369: step 16163, loss 0.00150755, acc 1
2016-09-06T08:06:23.314834: step 16164, loss 0.0128964, acc 1
2016-09-06T08:06:24.123201: step 16165, loss 0.00244663, acc 1
2016-09-06T08:06:24.964726: step 16166, loss 0.0157459, acc 1
2016-09-06T08:06:25.751978: step 16167, loss 0.0071238, acc 1
2016-09-06T08:06:26.587459: step 16168, loss 0.0187807, acc 1
2016-09-06T08:06:27.396440: step 16169, loss 0.00802204, acc 1
2016-09-06T08:06:28.166144: step 16170, loss 0.13109, acc 0.98
2016-09-06T08:06:28.965280: step 16171, loss 0.03075, acc 0.98
2016-09-06T08:06:29.757405: step 16172, loss 0.00202608, acc 1
2016-09-06T08:06:30.559854: step 16173, loss 0.00186156, acc 1
2016-09-06T08:06:31.368695: step 16174, loss 0.0278755, acc 0.98
2016-09-06T08:06:32.194973: step 16175, loss 0.00231617, acc 1
2016-09-06T08:06:33.021358: step 16176, loss 0.00547274, acc 1
2016-09-06T08:06:33.833426: step 16177, loss 0.0712456, acc 0.98
2016-09-06T08:06:34.664577: step 16178, loss 0.00531534, acc 1
2016-09-06T08:06:35.466726: step 16179, loss 0.0911802, acc 0.98
2016-09-06T08:06:36.263705: step 16180, loss 0.0136239, acc 1
2016-09-06T08:06:37.084354: step 16181, loss 0.0331023, acc 0.98
2016-09-06T08:06:37.851012: step 16182, loss 0.00267544, acc 1
2016-09-06T08:06:38.655192: step 16183, loss 0.0116507, acc 1
2016-09-06T08:06:39.477277: step 16184, loss 0.0107807, acc 1
2016-09-06T08:06:40.253793: step 16185, loss 0.00682283, acc 1
2016-09-06T08:06:41.068590: step 16186, loss 0.0181028, acc 1
2016-09-06T08:06:41.899020: step 16187, loss 0.0297491, acc 0.98
2016-09-06T08:06:42.678171: step 16188, loss 0.00704113, acc 1
2016-09-06T08:06:43.482497: step 16189, loss 0.0290605, acc 1
2016-09-06T08:06:44.344134: step 16190, loss 0.015615, acc 1
2016-09-06T08:06:45.180017: step 16191, loss 0.00354395, acc 1
2016-09-06T08:06:45.996463: step 16192, loss 0.00847672, acc 1
2016-09-06T08:06:46.881710: step 16193, loss 0.0207096, acc 0.98
2016-09-06T08:06:47.702366: step 16194, loss 0.00851772, acc 1
2016-09-06T08:06:48.582431: step 16195, loss 0.0262826, acc 1
2016-09-06T08:06:49.427591: step 16196, loss 0.0956843, acc 0.96
2016-09-06T08:06:50.266566: step 16197, loss 0.0198717, acc 0.98
2016-09-06T08:06:51.067605: step 16198, loss 0.00380495, acc 1
2016-09-06T08:06:51.891581: step 16199, loss 0.0124114, acc 1
2016-09-06T08:06:52.721662: step 16200, loss 0.0166873, acc 1

Evaluation:
2016-09-06T08:06:56.467171: step 16200, loss 4.08814, acc 0.72045

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-16200

2016-09-06T08:06:58.399677: step 16201, loss 0.0155147, acc 1
2016-09-06T08:06:59.311172: step 16202, loss 0.00370166, acc 1
2016-09-06T08:07:00.136862: step 16203, loss 0.0204341, acc 1
2016-09-06T08:07:00.987351: step 16204, loss 0.00536151, acc 1
2016-09-06T08:07:01.804188: step 16205, loss 0.0632049, acc 0.98
2016-09-06T08:07:02.657407: step 16206, loss 0.0209157, acc 1
2016-09-06T08:07:03.589046: step 16207, loss 0.00348056, acc 1
2016-09-06T08:07:04.435533: step 16208, loss 0.00347855, acc 1
2016-09-06T08:07:05.280468: step 16209, loss 0.00345745, acc 1
2016-09-06T08:07:06.166416: step 16210, loss 0.00336781, acc 1
2016-09-06T08:07:06.989636: step 16211, loss 0.0567067, acc 0.98
2016-09-06T08:07:07.853581: step 16212, loss 0.00326194, acc 1
2016-09-06T08:07:08.684387: step 16213, loss 0.00511135, acc 1
2016-09-06T08:07:09.500510: step 16214, loss 0.00316756, acc 1
2016-09-06T08:07:10.320664: step 16215, loss 0.0188525, acc 0.98
2016-09-06T08:07:11.202226: step 16216, loss 0.00563848, acc 1
2016-09-06T08:07:12.025612: step 16217, loss 0.0123683, acc 1
2016-09-06T08:07:13.000246: step 16218, loss 0.00303046, acc 1
2016-09-06T08:07:13.823832: step 16219, loss 0.00341492, acc 1
2016-09-06T08:07:14.691188: step 16220, loss 0.0172028, acc 0.98
2016-09-06T08:07:15.508276: step 16221, loss 0.00825648, acc 1
2016-09-06T08:07:16.328481: step 16222, loss 0.00282212, acc 1
2016-09-06T08:07:17.134863: step 16223, loss 0.00285787, acc 1
2016-09-06T08:07:17.961878: step 16224, loss 0.00736344, acc 1
2016-09-06T08:07:18.788564: step 16225, loss 0.00281395, acc 1
2016-09-06T08:07:19.613483: step 16226, loss 0.01726, acc 1
2016-09-06T08:07:20.452209: step 16227, loss 0.0178162, acc 0.98
2016-09-06T08:07:21.305259: step 16228, loss 0.00267437, acc 1
2016-09-06T08:07:22.155187: step 16229, loss 0.202328, acc 0.94
2016-09-06T08:07:23.017900: step 16230, loss 0.00654087, acc 1
2016-09-06T08:07:23.837469: step 16231, loss 0.00394399, acc 1
2016-09-06T08:07:24.669240: step 16232, loss 0.0026905, acc 1
2016-09-06T08:07:25.489742: step 16233, loss 0.0150888, acc 1
2016-09-06T08:07:26.271088: step 16234, loss 0.0174551, acc 1
2016-09-06T08:07:27.076060: step 16235, loss 0.0665629, acc 0.96
2016-09-06T08:07:27.878074: step 16236, loss 0.0305215, acc 0.98
2016-09-06T08:07:28.660229: step 16237, loss 0.0571746, acc 0.96
2016-09-06T08:07:29.470124: step 16238, loss 0.0175389, acc 0.98
2016-09-06T08:07:30.281444: step 16239, loss 0.00197642, acc 1
2016-09-06T08:07:31.059681: step 16240, loss 0.051633, acc 0.98
2016-09-06T08:07:31.876671: step 16241, loss 0.0148443, acc 1
2016-09-06T08:07:32.711233: step 16242, loss 0.011796, acc 1
2016-09-06T08:07:33.491530: step 16243, loss 0.00676284, acc 1
2016-09-06T08:07:34.269334: step 16244, loss 0.00253132, acc 1
2016-09-06T08:07:35.093931: step 16245, loss 0.0302679, acc 1
2016-09-06T08:07:35.933897: step 16246, loss 0.0019829, acc 1
2016-09-06T08:07:36.739854: step 16247, loss 0.0335994, acc 0.98
2016-09-06T08:07:37.554234: step 16248, loss 0.00964821, acc 1
2016-09-06T08:07:38.361118: step 16249, loss 0.00793907, acc 1
2016-09-06T08:07:39.162012: step 16250, loss 0.0128669, acc 1
2016-09-06T08:07:40.015726: step 16251, loss 0.0271469, acc 0.98
2016-09-06T08:07:40.837466: step 16252, loss 0.0226431, acc 1
2016-09-06T08:07:41.664721: step 16253, loss 0.0253171, acc 1
2016-09-06T08:07:42.491000: step 16254, loss 0.0021067, acc 1
2016-09-06T08:07:43.310734: step 16255, loss 0.0179186, acc 1
2016-09-06T08:07:44.157039: step 16256, loss 0.00581005, acc 1
2016-09-06T08:07:44.979651: step 16257, loss 0.0020408, acc 1
2016-09-06T08:07:45.774185: step 16258, loss 0.0409457, acc 0.96
2016-09-06T08:07:46.615757: step 16259, loss 0.0269045, acc 0.98
2016-09-06T08:07:47.433049: step 16260, loss 0.0276117, acc 0.98
2016-09-06T08:07:48.240625: step 16261, loss 0.0119039, acc 1
2016-09-06T08:07:49.059393: step 16262, loss 0.0168657, acc 0.98
2016-09-06T08:07:49.882771: step 16263, loss 0.00405754, acc 1
2016-09-06T08:07:50.696220: step 16264, loss 0.00226375, acc 1
2016-09-06T08:07:51.521418: step 16265, loss 0.0156445, acc 1
2016-09-06T08:07:52.347731: step 16266, loss 0.0137297, acc 1
2016-09-06T08:07:53.148549: step 16267, loss 0.00341756, acc 1
2016-09-06T08:07:53.988325: step 16268, loss 0.00227281, acc 1
2016-09-06T08:07:54.821780: step 16269, loss 0.0390971, acc 0.96
2016-09-06T08:07:55.652507: step 16270, loss 0.00273541, acc 1
2016-09-06T08:07:56.451776: step 16271, loss 0.0102594, acc 1
2016-09-06T08:07:57.239228: step 16272, loss 0.00694517, acc 1
2016-09-06T08:07:58.048114: step 16273, loss 0.00344556, acc 1
2016-09-06T08:07:58.849995: step 16274, loss 0.0149659, acc 1
2016-09-06T08:07:59.664225: step 16275, loss 0.0335277, acc 0.98
2016-09-06T08:08:00.520717: step 16276, loss 0.00732121, acc 1
2016-09-06T08:08:01.300769: step 16277, loss 0.01379, acc 1
2016-09-06T08:08:02.102640: step 16278, loss 0.0108179, acc 1
2016-09-06T08:08:02.933360: step 16279, loss 0.00378842, acc 1
2016-09-06T08:08:03.742364: step 16280, loss 0.0022977, acc 1
2016-09-06T08:08:04.554725: step 16281, loss 0.0133566, acc 1
2016-09-06T08:08:05.431534: step 16282, loss 0.0149512, acc 1
2016-09-06T08:08:06.248866: step 16283, loss 0.00230205, acc 1
2016-09-06T08:08:07.044459: step 16284, loss 0.015525, acc 1
2016-09-06T08:08:07.880082: step 16285, loss 0.0757285, acc 0.98
2016-09-06T08:08:08.692246: step 16286, loss 0.0104294, acc 1
2016-09-06T08:08:09.499783: step 16287, loss 0.00221704, acc 1
2016-09-06T08:08:10.314292: step 16288, loss 0.033646, acc 0.98
2016-09-06T08:08:11.142935: step 16289, loss 0.00878213, acc 1
2016-09-06T08:08:11.942769: step 16290, loss 0.00615205, acc 1
2016-09-06T08:08:12.789219: step 16291, loss 0.0202322, acc 1
2016-09-06T08:08:13.604421: step 16292, loss 0.00501415, acc 1
2016-09-06T08:08:14.412658: step 16293, loss 0.0168559, acc 0.98
2016-09-06T08:08:15.232471: step 16294, loss 0.0298794, acc 0.98
2016-09-06T08:08:16.050413: step 16295, loss 0.0108578, acc 1
2016-09-06T08:08:16.869561: step 16296, loss 0.00344225, acc 1
2016-09-06T08:08:17.721401: step 16297, loss 0.0291316, acc 0.98
2016-09-06T08:08:18.521417: step 16298, loss 0.00267267, acc 1
2016-09-06T08:08:19.319804: step 16299, loss 0.00578193, acc 1
2016-09-06T08:08:20.162304: step 16300, loss 0.00939399, acc 1

Evaluation:
2016-09-06T08:08:23.871135: step 16300, loss 3.33833, acc 0.711069

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-16300

2016-09-06T08:08:25.736097: step 16301, loss 0.00214357, acc 1
2016-09-06T08:08:26.544746: step 16302, loss 0.0167234, acc 0.98
2016-09-06T08:08:27.372941: step 16303, loss 0.0148714, acc 1
2016-09-06T08:08:28.196536: step 16304, loss 0.00353824, acc 1
2016-09-06T08:08:29.033465: step 16305, loss 0.0432441, acc 0.98
2016-09-06T08:08:29.876200: step 16306, loss 0.0435512, acc 0.98
2016-09-06T08:08:30.725796: step 16307, loss 0.00511477, acc 1
2016-09-06T08:08:31.541379: step 16308, loss 0.0161553, acc 1
2016-09-06T08:08:32.353447: step 16309, loss 0.00209369, acc 1
2016-09-06T08:08:33.191365: step 16310, loss 0.00226582, acc 1
2016-09-06T08:08:33.984534: step 16311, loss 0.00233351, acc 1
2016-09-06T08:08:34.792014: step 16312, loss 0.00200334, acc 1
2016-09-06T08:08:35.598975: step 16313, loss 0.0534913, acc 0.98
2016-09-06T08:08:36.406037: step 16314, loss 0.00224467, acc 1
2016-09-06T08:08:37.205002: step 16315, loss 0.0178617, acc 0.98
2016-09-06T08:08:38.003740: step 16316, loss 0.0121191, acc 1
2016-09-06T08:08:38.771692: step 16317, loss 0.105282, acc 0.98
2016-09-06T08:08:39.580631: step 16318, loss 0.0330335, acc 0.98
2016-09-06T08:08:40.408007: step 16319, loss 0.0557162, acc 0.96
2016-09-06T08:08:41.128252: step 16320, loss 0.00167359, acc 1
2016-09-06T08:08:41.933636: step 16321, loss 0.00548567, acc 1
2016-09-06T08:08:42.753486: step 16322, loss 0.0340306, acc 0.98
2016-09-06T08:08:43.556983: step 16323, loss 0.0138859, acc 1
2016-09-06T08:08:44.371591: step 16324, loss 0.0017211, acc 1
2016-09-06T08:08:45.190859: step 16325, loss 0.0840278, acc 0.96
2016-09-06T08:08:45.980965: step 16326, loss 0.00333752, acc 1
2016-09-06T08:08:46.777196: step 16327, loss 0.00815949, acc 1
2016-09-06T08:08:47.566200: step 16328, loss 0.0354257, acc 0.96
2016-09-06T08:08:48.364270: step 16329, loss 0.0464565, acc 0.98
2016-09-06T08:08:49.175186: step 16330, loss 0.0026354, acc 1
2016-09-06T08:08:50.015592: step 16331, loss 0.0124481, acc 1
2016-09-06T08:08:50.796629: step 16332, loss 0.00176177, acc 1
2016-09-06T08:08:51.614509: step 16333, loss 0.00724009, acc 1
2016-09-06T08:08:52.442792: step 16334, loss 0.0205945, acc 1
2016-09-06T08:08:53.248951: step 16335, loss 0.0158476, acc 1
2016-09-06T08:08:54.039368: step 16336, loss 0.00179304, acc 1
2016-09-06T08:08:54.839911: step 16337, loss 0.075568, acc 0.98
2016-09-06T08:08:55.633716: step 16338, loss 0.00145567, acc 1
2016-09-06T08:08:56.455938: step 16339, loss 0.0156136, acc 0.98
2016-09-06T08:08:57.282309: step 16340, loss 0.00950297, acc 1
2016-09-06T08:08:58.076743: step 16341, loss 0.0178221, acc 0.98
2016-09-06T08:08:58.887769: step 16342, loss 0.00436207, acc 1
2016-09-06T08:08:59.704605: step 16343, loss 0.0145626, acc 1
2016-09-06T08:09:00.503437: step 16344, loss 0.0370997, acc 0.96
2016-09-06T08:09:01.301427: step 16345, loss 0.00964086, acc 1
2016-09-06T08:09:02.125142: step 16346, loss 0.0125947, acc 1
2016-09-06T08:09:02.941460: step 16347, loss 0.021522, acc 0.98
2016-09-06T08:09:03.755860: step 16348, loss 0.00200194, acc 1
2016-09-06T08:09:04.582038: step 16349, loss 0.0370639, acc 0.98
2016-09-06T08:09:05.387638: step 16350, loss 0.00200103, acc 1
2016-09-06T08:09:06.192038: step 16351, loss 0.00176271, acc 1
2016-09-06T08:09:07.015421: step 16352, loss 0.0294764, acc 0.98
2016-09-06T08:09:07.831326: step 16353, loss 0.00431134, acc 1
2016-09-06T08:09:08.632273: step 16354, loss 0.00245484, acc 1
2016-09-06T08:09:09.433561: step 16355, loss 0.00188861, acc 1
2016-09-06T08:09:10.266482: step 16356, loss 0.0245991, acc 0.98
2016-09-06T08:09:11.078913: step 16357, loss 0.00192888, acc 1
2016-09-06T08:09:11.890594: step 16358, loss 0.0315701, acc 0.98
2016-09-06T08:09:12.693832: step 16359, loss 0.00277181, acc 1
2016-09-06T08:09:13.517394: step 16360, loss 0.00183947, acc 1
2016-09-06T08:09:14.344851: step 16361, loss 0.0278667, acc 0.98
2016-09-06T08:09:15.176059: step 16362, loss 0.0145415, acc 1
2016-09-06T08:09:15.999460: step 16363, loss 0.038703, acc 0.98
2016-09-06T08:09:16.803324: step 16364, loss 0.00189439, acc 1
2016-09-06T08:09:17.622691: step 16365, loss 0.00279552, acc 1
2016-09-06T08:09:18.444170: step 16366, loss 0.009503, acc 1
2016-09-06T08:09:19.300325: step 16367, loss 0.0212083, acc 0.98
2016-09-06T08:09:20.131826: step 16368, loss 0.0167684, acc 1
2016-09-06T08:09:20.981303: step 16369, loss 0.0905293, acc 0.96
2016-09-06T08:09:21.813180: step 16370, loss 0.0023736, acc 1
2016-09-06T08:09:22.645265: step 16371, loss 0.00206884, acc 1
2016-09-06T08:09:23.445480: step 16372, loss 0.0288498, acc 0.98
2016-09-06T08:09:24.267939: step 16373, loss 0.0352189, acc 0.98
2016-09-06T08:09:25.090546: step 16374, loss 0.00306573, acc 1
2016-09-06T08:09:25.884112: step 16375, loss 0.0021976, acc 1
2016-09-06T08:09:26.700581: step 16376, loss 0.017441, acc 0.98
2016-09-06T08:09:27.551882: step 16377, loss 0.00792367, acc 1
2016-09-06T08:09:28.352438: step 16378, loss 0.0227237, acc 1
2016-09-06T08:09:29.195671: step 16379, loss 0.0062361, acc 1
2016-09-06T08:09:30.036384: step 16380, loss 0.00229219, acc 1
2016-09-06T08:09:30.849903: step 16381, loss 0.181412, acc 0.98
2016-09-06T08:09:31.666088: step 16382, loss 0.00750519, acc 1
2016-09-06T08:09:32.507380: step 16383, loss 0.00232778, acc 1
2016-09-06T08:09:33.310372: step 16384, loss 0.0198204, acc 1
2016-09-06T08:09:34.127358: step 16385, loss 0.00237018, acc 1
2016-09-06T08:09:34.982143: step 16386, loss 0.0141923, acc 1
2016-09-06T08:09:35.806531: step 16387, loss 0.0139685, acc 1
2016-09-06T08:09:36.637225: step 16388, loss 0.0130647, acc 1
2016-09-06T08:09:37.465843: step 16389, loss 0.0409994, acc 0.98
2016-09-06T08:09:38.309361: step 16390, loss 0.0127133, acc 1
2016-09-06T08:09:39.120149: step 16391, loss 0.0109162, acc 1
2016-09-06T08:09:39.947148: step 16392, loss 0.00270357, acc 1
2016-09-06T08:09:40.742692: step 16393, loss 0.0292168, acc 0.98
2016-09-06T08:09:41.584871: step 16394, loss 0.00440646, acc 1
2016-09-06T08:09:42.407149: step 16395, loss 0.0161867, acc 1
2016-09-06T08:09:43.215594: step 16396, loss 0.0119, acc 1
2016-09-06T08:09:44.016182: step 16397, loss 0.00509319, acc 1
2016-09-06T08:09:44.811200: step 16398, loss 0.0161511, acc 1
2016-09-06T08:09:45.621530: step 16399, loss 0.00316873, acc 1
2016-09-06T08:09:46.422562: step 16400, loss 0.0134164, acc 1

Evaluation:
2016-09-06T08:09:50.177376: step 16400, loss 2.79645, acc 0.732645

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-16400

2016-09-06T08:09:52.058427: step 16401, loss 0.00341848, acc 1
2016-09-06T08:09:52.897456: step 16402, loss 0.0201625, acc 0.98
2016-09-06T08:09:53.724208: step 16403, loss 0.0193992, acc 1
2016-09-06T08:09:54.538403: step 16404, loss 0.00286863, acc 1
2016-09-06T08:09:55.369788: step 16405, loss 0.0323788, acc 0.98
2016-09-06T08:09:56.164525: step 16406, loss 0.00346861, acc 1
2016-09-06T08:09:56.965603: step 16407, loss 0.012558, acc 1
2016-09-06T08:09:57.781424: step 16408, loss 0.0130179, acc 1
2016-09-06T08:09:58.594795: step 16409, loss 0.00477134, acc 1
2016-09-06T08:09:59.405113: step 16410, loss 0.002784, acc 1
2016-09-06T08:10:00.237419: step 16411, loss 0.018371, acc 0.98
2016-09-06T08:10:01.051201: step 16412, loss 0.00275622, acc 1
2016-09-06T08:10:01.853079: step 16413, loss 0.0436145, acc 0.98
2016-09-06T08:10:02.661790: step 16414, loss 0.00393774, acc 1
2016-09-06T08:10:03.442407: step 16415, loss 0.0317123, acc 0.98
2016-09-06T08:10:04.262355: step 16416, loss 0.00561642, acc 1
2016-09-06T08:10:05.138212: step 16417, loss 0.00266716, acc 1
2016-09-06T08:10:05.963701: step 16418, loss 0.00873556, acc 1
2016-09-06T08:10:06.773678: step 16419, loss 0.00273312, acc 1
2016-09-06T08:10:07.595764: step 16420, loss 0.032494, acc 0.98
2016-09-06T08:10:08.422031: step 16421, loss 0.0126395, acc 1
2016-09-06T08:10:09.226406: step 16422, loss 0.00255936, acc 1
2016-09-06T08:10:10.078324: step 16423, loss 0.0164382, acc 0.98
2016-09-06T08:10:10.913301: step 16424, loss 0.00270161, acc 1
2016-09-06T08:10:11.729008: step 16425, loss 0.0223969, acc 0.98
2016-09-06T08:10:12.559486: step 16426, loss 0.002444, acc 1
2016-09-06T08:10:13.406340: step 16427, loss 0.00504824, acc 1
2016-09-06T08:10:14.229635: step 16428, loss 0.0366703, acc 0.98
2016-09-06T08:10:15.059830: step 16429, loss 0.0275075, acc 1
2016-09-06T08:10:15.889934: step 16430, loss 0.0169163, acc 1
2016-09-06T08:10:16.674816: step 16431, loss 0.00238373, acc 1
2016-09-06T08:10:17.472446: step 16432, loss 0.0578254, acc 0.98
2016-09-06T08:10:18.287354: step 16433, loss 0.0349419, acc 0.98
2016-09-06T08:10:19.090377: step 16434, loss 0.00965944, acc 1
2016-09-06T08:10:19.942214: step 16435, loss 0.0348756, acc 0.98
2016-09-06T08:10:20.749149: step 16436, loss 0.0157033, acc 1
2016-09-06T08:10:21.512330: step 16437, loss 0.0131919, acc 1
2016-09-06T08:10:22.306983: step 16438, loss 0.00653843, acc 1
2016-09-06T08:10:23.140264: step 16439, loss 0.0175605, acc 0.98
2016-09-06T08:10:23.920719: step 16440, loss 0.0182649, acc 1
2016-09-06T08:10:24.761270: step 16441, loss 0.00234232, acc 1
2016-09-06T08:10:25.580745: step 16442, loss 0.00415404, acc 1
2016-09-06T08:10:26.376841: step 16443, loss 0.0022665, acc 1
2016-09-06T08:10:27.175231: step 16444, loss 0.00379789, acc 1
2016-09-06T08:10:27.991340: step 16445, loss 0.0112402, acc 1
2016-09-06T08:10:28.758477: step 16446, loss 0.00529965, acc 1
2016-09-06T08:10:29.571076: step 16447, loss 0.00602531, acc 1
2016-09-06T08:10:30.389477: step 16448, loss 0.0117998, acc 1
2016-09-06T08:10:31.186829: step 16449, loss 0.00903983, acc 1
2016-09-06T08:10:32.018310: step 16450, loss 0.00731502, acc 1
2016-09-06T08:10:32.829613: step 16451, loss 0.00691616, acc 1
2016-09-06T08:10:33.610419: step 16452, loss 0.0309984, acc 0.98
2016-09-06T08:10:34.477470: step 16453, loss 0.0153178, acc 1
2016-09-06T08:10:35.294461: step 16454, loss 0.0922312, acc 0.98
2016-09-06T08:10:36.120941: step 16455, loss 0.00221667, acc 1
2016-09-06T08:10:36.945710: step 16456, loss 0.00447718, acc 1
2016-09-06T08:10:37.779800: step 16457, loss 0.00663495, acc 1
2016-09-06T08:10:38.600043: step 16458, loss 0.0128645, acc 1
2016-09-06T08:10:39.414127: step 16459, loss 0.0022197, acc 1
2016-09-06T08:10:40.259900: step 16460, loss 0.00246831, acc 1
2016-09-06T08:10:41.067647: step 16461, loss 0.00856348, acc 1
2016-09-06T08:10:41.895956: step 16462, loss 0.00245459, acc 1
2016-09-06T08:10:42.743138: step 16463, loss 0.00867755, acc 1
2016-09-06T08:10:43.547196: step 16464, loss 0.0274057, acc 1
2016-09-06T08:10:44.373402: step 16465, loss 0.00552457, acc 1
2016-09-06T08:10:45.220459: step 16466, loss 0.0661249, acc 0.96
2016-09-06T08:10:46.069208: step 16467, loss 0.00397988, acc 1
2016-09-06T08:10:46.884821: step 16468, loss 0.00324282, acc 1
2016-09-06T08:10:47.683676: step 16469, loss 0.00245742, acc 1
2016-09-06T08:10:48.530934: step 16470, loss 0.00282633, acc 1
2016-09-06T08:10:49.329996: step 16471, loss 0.00245688, acc 1
2016-09-06T08:10:50.126525: step 16472, loss 0.0026424, acc 1
2016-09-06T08:10:50.926293: step 16473, loss 0.0132825, acc 1
2016-09-06T08:10:51.700670: step 16474, loss 0.00307519, acc 1
2016-09-06T08:10:52.495335: step 16475, loss 0.00361823, acc 1
2016-09-06T08:10:53.315044: step 16476, loss 0.00561103, acc 1
2016-09-06T08:10:54.100857: step 16477, loss 0.00335468, acc 1
2016-09-06T08:10:54.905085: step 16478, loss 0.018559, acc 0.98
2016-09-06T08:10:55.735828: step 16479, loss 0.00854691, acc 1
2016-09-06T08:10:56.513397: step 16480, loss 0.0167498, acc 1
2016-09-06T08:10:57.337623: step 16481, loss 0.0192018, acc 0.98
2016-09-06T08:10:58.157069: step 16482, loss 0.0230005, acc 0.98
2016-09-06T08:10:58.973424: step 16483, loss 0.0195068, acc 1
2016-09-06T08:10:59.800272: step 16484, loss 0.00879786, acc 1
2016-09-06T08:11:00.624003: step 16485, loss 0.00241881, acc 1
2016-09-06T08:11:01.411987: step 16486, loss 0.0622699, acc 0.98
2016-09-06T08:11:02.205488: step 16487, loss 0.00328421, acc 1
2016-09-06T08:11:03.023133: step 16488, loss 0.0196197, acc 1
2016-09-06T08:11:03.810503: step 16489, loss 0.0824594, acc 0.96
2016-09-06T08:11:04.622703: step 16490, loss 0.0108872, acc 1
2016-09-06T08:11:05.445752: step 16491, loss 0.0466369, acc 0.98
2016-09-06T08:11:06.226770: step 16492, loss 0.00253395, acc 1
2016-09-06T08:11:07.022290: step 16493, loss 0.0113371, acc 1
2016-09-06T08:11:07.827076: step 16494, loss 0.00721089, acc 1
2016-09-06T08:11:08.633896: step 16495, loss 0.0537987, acc 0.98
2016-09-06T08:11:09.442769: step 16496, loss 0.00216597, acc 1
2016-09-06T08:11:10.277535: step 16497, loss 0.0327909, acc 1
2016-09-06T08:11:11.053081: step 16498, loss 0.0174273, acc 1
2016-09-06T08:11:11.847896: step 16499, loss 0.0328399, acc 0.98
2016-09-06T08:11:12.699234: step 16500, loss 0.0024925, acc 1

Evaluation:
2016-09-06T08:11:16.431008: step 16500, loss 3.13971, acc 0.731707

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-16500

2016-09-06T08:11:18.391901: step 16501, loss 0.00224957, acc 1
2016-09-06T08:11:19.203442: step 16502, loss 0.00505842, acc 1
2016-09-06T08:11:20.021579: step 16503, loss 0.00443568, acc 1
2016-09-06T08:11:20.825557: step 16504, loss 0.00668889, acc 1
2016-09-06T08:11:21.631985: step 16505, loss 0.00225641, acc 1
2016-09-06T08:11:22.439562: step 16506, loss 0.0297862, acc 0.98
2016-09-06T08:11:23.248818: step 16507, loss 0.0143842, acc 1
2016-09-06T08:11:24.064722: step 16508, loss 0.00623153, acc 1
2016-09-06T08:11:24.918727: step 16509, loss 0.0359895, acc 0.98
2016-09-06T08:11:25.744330: step 16510, loss 0.0159923, acc 1
2016-09-06T08:11:26.577741: step 16511, loss 0.00246319, acc 1
2016-09-06T08:11:27.377786: step 16512, loss 0.00220266, acc 1
2016-09-06T08:11:28.187515: step 16513, loss 0.00257422, acc 1
2016-09-06T08:11:28.987087: step 16514, loss 0.0737652, acc 0.98
2016-09-06T08:11:29.822459: step 16515, loss 0.00682713, acc 1
2016-09-06T08:11:30.644886: step 16516, loss 0.0047399, acc 1
2016-09-06T08:11:31.454475: step 16517, loss 0.0228356, acc 0.98
2016-09-06T08:11:32.357560: step 16518, loss 0.0096069, acc 1
2016-09-06T08:11:33.171114: step 16519, loss 0.0390145, acc 0.98
2016-09-06T08:11:33.981756: step 16520, loss 0.0126023, acc 1
2016-09-06T08:11:34.843809: step 16521, loss 0.0326714, acc 0.98
2016-09-06T08:11:35.707919: step 16522, loss 0.00728302, acc 1
2016-09-06T08:11:36.493696: step 16523, loss 0.00858134, acc 1
2016-09-06T08:11:37.287494: step 16524, loss 0.00211267, acc 1
2016-09-06T08:11:38.095308: step 16525, loss 0.00702725, acc 1
2016-09-06T08:11:38.863591: step 16526, loss 0.0836939, acc 0.96
2016-09-06T08:11:39.703199: step 16527, loss 0.127828, acc 0.94
2016-09-06T08:11:40.515965: step 16528, loss 0.00245805, acc 1
2016-09-06T08:11:41.294350: step 16529, loss 0.0195408, acc 0.98
2016-09-06T08:11:42.113426: step 16530, loss 0.0019284, acc 1
2016-09-06T08:11:42.922372: step 16531, loss 0.00521151, acc 1
2016-09-06T08:11:43.710259: step 16532, loss 0.00326018, acc 1
2016-09-06T08:11:44.500462: step 16533, loss 0.00200999, acc 1
2016-09-06T08:11:45.318165: step 16534, loss 0.00716993, acc 1
2016-09-06T08:11:46.109454: step 16535, loss 0.0215296, acc 0.98
2016-09-06T08:11:46.943534: step 16536, loss 0.00858524, acc 1
2016-09-06T08:11:47.767314: step 16537, loss 0.00280308, acc 1
2016-09-06T08:11:48.530289: step 16538, loss 0.0254557, acc 1
2016-09-06T08:11:49.330298: step 16539, loss 0.0233654, acc 0.98
2016-09-06T08:11:50.175619: step 16540, loss 0.0025344, acc 1
2016-09-06T08:11:50.947540: step 16541, loss 0.0145048, acc 1
2016-09-06T08:11:51.746524: step 16542, loss 0.00348285, acc 1
2016-09-06T08:11:52.594571: step 16543, loss 0.0031714, acc 1
2016-09-06T08:11:53.380135: step 16544, loss 0.00668589, acc 1
2016-09-06T08:11:54.206681: step 16545, loss 0.00420405, acc 1
2016-09-06T08:11:55.040526: step 16546, loss 0.00218501, acc 1
2016-09-06T08:11:55.831022: step 16547, loss 0.00210652, acc 1
2016-09-06T08:11:56.632876: step 16548, loss 0.0801385, acc 0.98
2016-09-06T08:11:57.448282: step 16549, loss 0.00250166, acc 1
2016-09-06T08:11:58.242149: step 16550, loss 0.0238974, acc 0.98
2016-09-06T08:11:59.040581: step 16551, loss 0.0376743, acc 0.98
2016-09-06T08:11:59.849778: step 16552, loss 0.00213147, acc 1
2016-09-06T08:12:00.667598: step 16553, loss 0.0433912, acc 0.98
2016-09-06T08:12:01.489348: step 16554, loss 0.00549099, acc 1
2016-09-06T08:12:02.320173: step 16555, loss 0.0348356, acc 0.98
2016-09-06T08:12:03.140351: step 16556, loss 0.00241183, acc 1
2016-09-06T08:12:03.979746: step 16557, loss 0.00196361, acc 1
2016-09-06T08:12:04.815031: step 16558, loss 0.0114943, acc 1
2016-09-06T08:12:05.644759: step 16559, loss 0.0254438, acc 0.98
2016-09-06T08:12:06.468628: step 16560, loss 0.00581867, acc 1
2016-09-06T08:12:07.303713: step 16561, loss 0.0121273, acc 1
2016-09-06T08:12:08.118994: step 16562, loss 0.00265021, acc 1
2016-09-06T08:12:08.944694: step 16563, loss 0.0160594, acc 0.98
2016-09-06T08:12:09.791526: step 16564, loss 0.0249787, acc 1
2016-09-06T08:12:10.631810: step 16565, loss 0.0437406, acc 0.98
2016-09-06T08:12:11.439427: step 16566, loss 0.0188117, acc 1
2016-09-06T08:12:12.273879: step 16567, loss 0.00506737, acc 1
2016-09-06T08:12:13.085713: step 16568, loss 0.00280675, acc 1
2016-09-06T08:12:13.892819: step 16569, loss 0.0121384, acc 1
2016-09-06T08:12:14.696301: step 16570, loss 0.0483646, acc 0.96
2016-09-06T08:12:15.509405: step 16571, loss 0.00337162, acc 1
2016-09-06T08:12:16.312586: step 16572, loss 0.00386633, acc 1
2016-09-06T08:12:17.123723: step 16573, loss 0.0588912, acc 0.96
2016-09-06T08:12:17.975188: step 16574, loss 0.00185144, acc 1
2016-09-06T08:12:18.748235: step 16575, loss 0.0017859, acc 1
2016-09-06T08:12:19.540259: step 16576, loss 0.0238896, acc 1
2016-09-06T08:12:20.365112: step 16577, loss 0.00737509, acc 1
2016-09-06T08:12:21.160808: step 16578, loss 0.0102291, acc 1
2016-09-06T08:12:21.955986: step 16579, loss 0.0214379, acc 0.98
2016-09-06T08:12:22.801054: step 16580, loss 0.0118962, acc 1
2016-09-06T08:12:23.601407: step 16581, loss 0.00908589, acc 1
2016-09-06T08:12:24.413520: step 16582, loss 0.00183728, acc 1
2016-09-06T08:12:25.246421: step 16583, loss 0.017585, acc 0.98
2016-09-06T08:12:26.050369: step 16584, loss 0.001859, acc 1
2016-09-06T08:12:26.858322: step 16585, loss 0.0359687, acc 0.96
2016-09-06T08:12:27.718317: step 16586, loss 0.0110815, acc 1
2016-09-06T08:12:28.553780: step 16587, loss 0.024251, acc 1
2016-09-06T08:12:29.385153: step 16588, loss 0.0171583, acc 0.98
2016-09-06T08:12:30.220734: step 16589, loss 0.0173744, acc 1
2016-09-06T08:12:31.053741: step 16590, loss 0.00815011, acc 1
2016-09-06T08:12:31.860273: step 16591, loss 0.0216142, acc 0.98
2016-09-06T08:12:32.710868: step 16592, loss 0.00204141, acc 1
2016-09-06T08:12:33.515262: step 16593, loss 0.0576642, acc 0.98
2016-09-06T08:12:34.345459: step 16594, loss 0.028524, acc 0.98
2016-09-06T08:12:35.188015: step 16595, loss 0.017276, acc 1
2016-09-06T08:12:36.002365: step 16596, loss 0.00303932, acc 1
2016-09-06T08:12:36.795530: step 16597, loss 0.00225028, acc 1
2016-09-06T08:12:37.652418: step 16598, loss 0.0223166, acc 0.98
2016-09-06T08:12:38.502232: step 16599, loss 0.00188759, acc 1
2016-09-06T08:12:39.292282: step 16600, loss 0.00539402, acc 1

Evaluation:
2016-09-06T08:12:43.040295: step 16600, loss 3.11305, acc 0.726079

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-16600

2016-09-06T08:12:44.924016: step 16601, loss 0.00734786, acc 1
2016-09-06T08:12:45.746597: step 16602, loss 0.0147812, acc 1
2016-09-06T08:12:46.585084: step 16603, loss 0.0113845, acc 1
2016-09-06T08:12:47.413198: step 16604, loss 0.00179196, acc 1
2016-09-06T08:12:48.224051: step 16605, loss 0.00297768, acc 1
2016-09-06T08:12:49.001431: step 16606, loss 0.00194018, acc 1
2016-09-06T08:12:49.849671: step 16607, loss 0.0017972, acc 1
2016-09-06T08:12:50.717448: step 16608, loss 0.0123362, acc 1
2016-09-06T08:12:51.538265: step 16609, loss 0.00211362, acc 1
2016-09-06T08:12:52.352349: step 16610, loss 0.16295, acc 0.98
2016-09-06T08:12:53.198105: step 16611, loss 0.00280542, acc 1
2016-09-06T08:12:54.020033: step 16612, loss 0.0369087, acc 0.98
2016-09-06T08:12:54.812905: step 16613, loss 0.0368164, acc 0.98
2016-09-06T08:12:55.677703: step 16614, loss 0.00318573, acc 1
2016-09-06T08:12:56.483826: step 16615, loss 0.00200991, acc 1
2016-09-06T08:12:57.301305: step 16616, loss 0.00214104, acc 1
2016-09-06T08:12:58.163668: step 16617, loss 0.0160331, acc 0.98
2016-09-06T08:12:58.967447: step 16618, loss 0.0151389, acc 1
2016-09-06T08:12:59.800596: step 16619, loss 0.0256439, acc 1
2016-09-06T08:13:00.648770: step 16620, loss 0.0328045, acc 0.98
2016-09-06T08:13:01.492927: step 16621, loss 0.0153025, acc 1
2016-09-06T08:13:02.285874: step 16622, loss 0.0656245, acc 0.96
2016-09-06T08:13:03.089165: step 16623, loss 0.00190414, acc 1
2016-09-06T08:13:03.906034: step 16624, loss 0.0221927, acc 0.98
2016-09-06T08:13:04.700387: step 16625, loss 0.0260617, acc 0.98
2016-09-06T08:13:05.509058: step 16626, loss 0.00354081, acc 1
2016-09-06T08:13:06.340208: step 16627, loss 0.0233502, acc 1
2016-09-06T08:13:07.152601: step 16628, loss 0.00187097, acc 1
2016-09-06T08:13:07.963629: step 16629, loss 0.00395007, acc 1
2016-09-06T08:13:08.780927: step 16630, loss 0.00357914, acc 1
2016-09-06T08:13:09.557231: step 16631, loss 0.0358818, acc 0.98
2016-09-06T08:13:10.357191: step 16632, loss 0.00183737, acc 1
2016-09-06T08:13:11.191371: step 16633, loss 0.00206548, acc 1
2016-09-06T08:13:11.986738: step 16634, loss 0.00216372, acc 1
2016-09-06T08:13:12.786726: step 16635, loss 0.00203869, acc 1
2016-09-06T08:13:13.619400: step 16636, loss 0.024196, acc 1
2016-09-06T08:13:14.411510: step 16637, loss 0.0203546, acc 1
2016-09-06T08:13:15.217758: step 16638, loss 0.0022728, acc 1
2016-09-06T08:13:16.057370: step 16639, loss 0.0267387, acc 1
2016-09-06T08:13:16.852013: step 16640, loss 0.00188713, acc 1
2016-09-06T08:13:17.664451: step 16641, loss 0.00177563, acc 1
2016-09-06T08:13:18.521400: step 16642, loss 0.0134223, acc 1
2016-09-06T08:13:19.315827: step 16643, loss 0.00426039, acc 1
2016-09-06T08:13:20.122123: step 16644, loss 0.00880359, acc 1
2016-09-06T08:13:20.961420: step 16645, loss 0.0263889, acc 0.98
2016-09-06T08:13:21.769619: step 16646, loss 0.00211979, acc 1
2016-09-06T08:13:22.585292: step 16647, loss 0.0398658, acc 0.98
2016-09-06T08:13:23.405627: step 16648, loss 0.00252077, acc 1
2016-09-06T08:13:24.222343: step 16649, loss 0.00182853, acc 1
2016-09-06T08:13:25.029989: step 16650, loss 0.0207787, acc 0.98
2016-09-06T08:13:25.882519: step 16651, loss 0.0018693, acc 1
2016-09-06T08:13:26.708604: step 16652, loss 0.00169584, acc 1
2016-09-06T08:13:27.520557: step 16653, loss 0.0167279, acc 1
2016-09-06T08:13:28.349057: step 16654, loss 0.00168027, acc 1
2016-09-06T08:13:29.166869: step 16655, loss 0.0092056, acc 1
2016-09-06T08:13:29.992931: step 16656, loss 0.00627591, acc 1
2016-09-06T08:13:30.833429: step 16657, loss 0.00420736, acc 1
2016-09-06T08:13:31.663270: step 16658, loss 0.0144251, acc 1
2016-09-06T08:13:32.450386: step 16659, loss 0.00214214, acc 1
2016-09-06T08:13:33.279531: step 16660, loss 0.00189881, acc 1
2016-09-06T08:13:34.104708: step 16661, loss 0.0570598, acc 0.98
2016-09-06T08:13:34.895352: step 16662, loss 0.00250229, acc 1
2016-09-06T08:13:35.698378: step 16663, loss 0.00164266, acc 1
2016-09-06T08:13:36.501724: step 16664, loss 0.00168321, acc 1
2016-09-06T08:13:37.327521: step 16665, loss 0.00965643, acc 1
2016-09-06T08:13:38.144969: step 16666, loss 0.00169004, acc 1
2016-09-06T08:13:38.956684: step 16667, loss 0.00160334, acc 1
2016-09-06T08:13:39.750962: step 16668, loss 0.0291427, acc 0.98
2016-09-06T08:13:40.554466: step 16669, loss 0.0139776, acc 1
2016-09-06T08:13:41.362926: step 16670, loss 0.00246077, acc 1
2016-09-06T08:13:42.161090: step 16671, loss 0.0229847, acc 0.98
2016-09-06T08:13:42.970094: step 16672, loss 0.00409191, acc 1
2016-09-06T08:13:43.807907: step 16673, loss 0.0479117, acc 0.98
2016-09-06T08:13:44.624744: step 16674, loss 0.0070675, acc 1
2016-09-06T08:13:45.432890: step 16675, loss 0.0708144, acc 0.98
2016-09-06T08:13:46.256518: step 16676, loss 0.00166008, acc 1
2016-09-06T08:13:47.059033: step 16677, loss 0.00625366, acc 1
2016-09-06T08:13:47.867136: step 16678, loss 0.0180932, acc 0.98
2016-09-06T08:13:48.683956: step 16679, loss 0.0024686, acc 1
2016-09-06T08:13:49.485419: step 16680, loss 0.00925055, acc 1
2016-09-06T08:13:50.289307: step 16681, loss 0.0199833, acc 0.98
2016-09-06T08:13:51.113984: step 16682, loss 0.00649768, acc 1
2016-09-06T08:13:51.907903: step 16683, loss 0.0367846, acc 0.96
2016-09-06T08:13:52.712115: step 16684, loss 0.00164876, acc 1
2016-09-06T08:13:53.514571: step 16685, loss 0.0154311, acc 1
2016-09-06T08:13:54.301589: step 16686, loss 0.00802827, acc 1
2016-09-06T08:13:55.089414: step 16687, loss 0.0124455, acc 1
2016-09-06T08:13:55.931869: step 16688, loss 0.00160069, acc 1
2016-09-06T08:13:56.709433: step 16689, loss 0.0106121, acc 1
2016-09-06T08:13:57.510588: step 16690, loss 0.0275413, acc 0.98
2016-09-06T08:13:58.335233: step 16691, loss 0.0119454, acc 1
2016-09-06T08:13:59.136374: step 16692, loss 0.0361899, acc 1
2016-09-06T08:13:59.966604: step 16693, loss 0.00335282, acc 1
2016-09-06T08:14:00.856630: step 16694, loss 0.0136551, acc 1
2016-09-06T08:14:01.678532: step 16695, loss 0.00180415, acc 1
2016-09-06T08:14:02.485886: step 16696, loss 0.00301083, acc 1
2016-09-06T08:14:03.333122: step 16697, loss 0.00707426, acc 1
2016-09-06T08:14:04.153807: step 16698, loss 0.0179437, acc 1
2016-09-06T08:14:05.002376: step 16699, loss 0.0661437, acc 0.96
2016-09-06T08:14:05.814013: step 16700, loss 0.00184717, acc 1

Evaluation:
2016-09-06T08:14:09.543720: step 16700, loss 3.05755, acc 0.73546

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-16700

2016-09-06T08:14:11.439645: step 16701, loss 0.0532343, acc 0.98
2016-09-06T08:14:12.244309: step 16702, loss 0.00263823, acc 1
2016-09-06T08:14:13.090750: step 16703, loss 0.0921831, acc 0.98
2016-09-06T08:14:13.863086: step 16704, loss 0.00690887, acc 1
2016-09-06T08:14:14.698772: step 16705, loss 0.00675139, acc 1
2016-09-06T08:14:15.561021: step 16706, loss 0.00603332, acc 1
2016-09-06T08:14:16.372188: step 16707, loss 0.0286012, acc 0.98
2016-09-06T08:14:17.171330: step 16708, loss 0.00159677, acc 1
2016-09-06T08:14:18.000336: step 16709, loss 0.0295019, acc 0.98
2016-09-06T08:14:18.837969: step 16710, loss 0.00644797, acc 1
2016-09-06T08:14:19.672267: step 16711, loss 0.00165978, acc 1
2016-09-06T08:14:20.496221: step 16712, loss 0.00190889, acc 1
2016-09-06T08:14:21.305268: step 16713, loss 0.0430524, acc 0.98
2016-09-06T08:14:22.116175: step 16714, loss 0.0218774, acc 1
2016-09-06T08:14:22.925849: step 16715, loss 0.00171565, acc 1
2016-09-06T08:14:23.777824: step 16716, loss 0.054462, acc 0.98
2016-09-06T08:14:24.575939: step 16717, loss 0.0196293, acc 0.98
2016-09-06T08:14:25.387348: step 16718, loss 0.00384123, acc 1
2016-09-06T08:14:26.215141: step 16719, loss 0.0766681, acc 0.96
2016-09-06T08:14:27.009392: step 16720, loss 0.00736662, acc 1
2016-09-06T08:14:27.821633: step 16721, loss 0.00213783, acc 1
2016-09-06T08:14:28.658237: step 16722, loss 0.00172668, acc 1
2016-09-06T08:14:29.457055: step 16723, loss 0.0156424, acc 1
2016-09-06T08:14:30.254821: step 16724, loss 0.00700737, acc 1
2016-09-06T08:14:31.063833: step 16725, loss 0.00189292, acc 1
2016-09-06T08:14:31.829797: step 16726, loss 0.00437212, acc 1
2016-09-06T08:14:32.630014: step 16727, loss 0.00225604, acc 1
2016-09-06T08:14:33.467655: step 16728, loss 0.00212121, acc 1
2016-09-06T08:14:34.248987: step 16729, loss 0.0291215, acc 1
2016-09-06T08:14:35.059556: step 16730, loss 0.0439726, acc 0.96
2016-09-06T08:14:35.905163: step 16731, loss 0.0261695, acc 0.98
2016-09-06T08:14:36.707447: step 16732, loss 0.0429, acc 0.98
2016-09-06T08:14:37.516089: step 16733, loss 0.0300138, acc 1
2016-09-06T08:14:38.391640: step 16734, loss 0.00257798, acc 1
2016-09-06T08:14:39.193075: step 16735, loss 0.00280306, acc 1
2016-09-06T08:14:39.968086: step 16736, loss 0.00720987, acc 1
2016-09-06T08:14:40.832103: step 16737, loss 0.00268505, acc 1
2016-09-06T08:14:41.648513: step 16738, loss 0.0477742, acc 0.98
2016-09-06T08:14:42.493674: step 16739, loss 0.0494828, acc 0.98
2016-09-06T08:14:43.329830: step 16740, loss 0.00343116, acc 1
2016-09-06T08:14:44.143743: step 16741, loss 0.00369588, acc 1
2016-09-06T08:14:44.955876: step 16742, loss 0.0165762, acc 1
2016-09-06T08:14:45.774677: step 16743, loss 0.0158751, acc 1
2016-09-06T08:14:46.577121: step 16744, loss 0.00313088, acc 1
2016-09-06T08:14:47.417734: step 16745, loss 0.0101735, acc 1
2016-09-06T08:14:48.313474: step 16746, loss 0.00321243, acc 1
2016-09-06T08:14:49.127788: step 16747, loss 0.00660042, acc 1
2016-09-06T08:14:49.907058: step 16748, loss 0.0278262, acc 0.98
2016-09-06T08:14:50.737870: step 16749, loss 0.00334675, acc 1
2016-09-06T08:14:51.568360: step 16750, loss 0.0242668, acc 1
2016-09-06T08:14:52.370964: step 16751, loss 0.00365531, acc 1
2016-09-06T08:14:53.172856: step 16752, loss 0.00415887, acc 1
2016-09-06T08:14:54.060420: step 16753, loss 0.0260955, acc 0.98
2016-09-06T08:14:54.953429: step 16754, loss 0.00389555, acc 1
2016-09-06T08:14:55.840264: step 16755, loss 0.0209979, acc 0.98
2016-09-06T08:14:56.715888: step 16756, loss 0.0373142, acc 0.98
2016-09-06T08:14:57.561024: step 16757, loss 0.0338779, acc 0.98
2016-09-06T08:14:58.431492: step 16758, loss 0.00454577, acc 1
2016-09-06T08:14:59.235244: step 16759, loss 0.0126344, acc 1
2016-09-06T08:15:00.066353: step 16760, loss 0.00514197, acc 1
2016-09-06T08:15:00.948468: step 16761, loss 0.00342855, acc 1
2016-09-06T08:15:01.799040: step 16762, loss 0.0606353, acc 0.98
2016-09-06T08:15:02.618012: step 16763, loss 0.00338876, acc 1
2016-09-06T08:15:03.430043: step 16764, loss 0.00324052, acc 1
2016-09-06T08:15:04.240443: step 16765, loss 0.025644, acc 1
2016-09-06T08:15:05.130030: step 16766, loss 0.00318092, acc 1
2016-09-06T08:15:05.928536: step 16767, loss 0.00317445, acc 1
2016-09-06T08:15:06.740196: step 16768, loss 0.00396307, acc 1
2016-09-06T08:15:07.567721: step 16769, loss 0.0125185, acc 1
2016-09-06T08:15:08.418373: step 16770, loss 0.00597969, acc 1
2016-09-06T08:15:09.217401: step 16771, loss 0.0140832, acc 1
2016-09-06T08:15:10.021505: step 16772, loss 0.0385283, acc 0.98
2016-09-06T08:15:10.847194: step 16773, loss 0.0177637, acc 0.98
2016-09-06T08:15:11.695195: step 16774, loss 0.00585449, acc 1
2016-09-06T08:15:12.500482: step 16775, loss 0.0189, acc 0.98
2016-09-06T08:15:13.312849: step 16776, loss 0.00293107, acc 1
2016-09-06T08:15:14.081919: step 16777, loss 0.0239285, acc 0.98
2016-09-06T08:15:14.899394: step 16778, loss 0.00287519, acc 1
2016-09-06T08:15:15.701436: step 16779, loss 0.0230272, acc 0.98
2016-09-06T08:15:16.512427: step 16780, loss 0.0203787, acc 0.98
2016-09-06T08:15:17.322862: step 16781, loss 0.00300884, acc 1
2016-09-06T08:15:18.131109: step 16782, loss 0.00340647, acc 1
2016-09-06T08:15:18.921858: step 16783, loss 0.01037, acc 1
2016-09-06T08:15:19.747936: step 16784, loss 0.0130729, acc 1
2016-09-06T08:15:20.585373: step 16785, loss 0.00322133, acc 1
2016-09-06T08:15:21.393444: step 16786, loss 0.00456321, acc 1
2016-09-06T08:15:22.213987: step 16787, loss 0.00769747, acc 1
2016-09-06T08:15:23.033789: step 16788, loss 0.00269911, acc 1
2016-09-06T08:15:23.834695: step 16789, loss 0.0111325, acc 1
2016-09-06T08:15:24.635301: step 16790, loss 0.0140132, acc 1
2016-09-06T08:15:25.415145: step 16791, loss 0.00339914, acc 1
2016-09-06T08:15:26.212108: step 16792, loss 0.00585874, acc 1
2016-09-06T08:15:27.005511: step 16793, loss 0.0290199, acc 0.98
2016-09-06T08:15:27.827499: step 16794, loss 0.0153955, acc 1
2016-09-06T08:15:28.640022: step 16795, loss 0.0155072, acc 1
2016-09-06T08:15:29.479721: step 16796, loss 0.00394778, acc 1
2016-09-06T08:15:30.292665: step 16797, loss 0.142312, acc 0.98
2016-09-06T08:15:31.090128: step 16798, loss 0.00249514, acc 1
2016-09-06T08:15:31.915062: step 16799, loss 0.00422637, acc 1
2016-09-06T08:15:32.809191: step 16800, loss 0.0046426, acc 1

Evaluation:
2016-09-06T08:15:36.557396: step 16800, loss 3.2128, acc 0.732645

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-16800

2016-09-06T08:15:38.510985: step 16801, loss 0.00226425, acc 1
2016-09-06T08:15:39.354669: step 16802, loss 0.0861133, acc 0.98
2016-09-06T08:15:40.199957: step 16803, loss 0.0185722, acc 0.98
2016-09-06T08:15:41.004428: step 16804, loss 0.0131562, acc 1
2016-09-06T08:15:41.854891: step 16805, loss 0.00599108, acc 1
2016-09-06T08:15:42.709328: step 16806, loss 0.00855891, acc 1
2016-09-06T08:15:43.519362: step 16807, loss 0.00372703, acc 1
2016-09-06T08:15:44.338048: step 16808, loss 0.00198664, acc 1
2016-09-06T08:15:45.164051: step 16809, loss 0.0160498, acc 0.98
2016-09-06T08:15:45.983123: step 16810, loss 0.00269565, acc 1
2016-09-06T08:15:46.787704: step 16811, loss 0.0211845, acc 1
2016-09-06T08:15:47.582239: step 16812, loss 0.00311581, acc 1
2016-09-06T08:15:48.406616: step 16813, loss 0.0229875, acc 1
2016-09-06T08:15:49.190233: step 16814, loss 0.00926603, acc 1
2016-09-06T08:15:49.992429: step 16815, loss 0.00260214, acc 1
2016-09-06T08:15:50.824325: step 16816, loss 0.0247939, acc 1
2016-09-06T08:15:51.602039: step 16817, loss 0.00263972, acc 1
2016-09-06T08:15:52.412386: step 16818, loss 0.00380399, acc 1
2016-09-06T08:15:53.226466: step 16819, loss 0.0471631, acc 0.96
2016-09-06T08:15:54.020511: step 16820, loss 0.0169092, acc 0.98
2016-09-06T08:15:54.826958: step 16821, loss 0.025715, acc 0.98
2016-09-06T08:15:55.657675: step 16822, loss 0.00213613, acc 1
2016-09-06T08:15:56.453803: step 16823, loss 0.00207213, acc 1
2016-09-06T08:15:57.289509: step 16824, loss 0.00769818, acc 1
2016-09-06T08:15:58.079810: step 16825, loss 0.00198228, acc 1
2016-09-06T08:15:58.863325: step 16826, loss 0.0227094, acc 0.98
2016-09-06T08:15:59.666296: step 16827, loss 0.0201314, acc 0.98
2016-09-06T08:16:00.538804: step 16828, loss 0.00314281, acc 1
2016-09-06T08:16:01.345520: step 16829, loss 0.0181749, acc 0.98
2016-09-06T08:16:02.144832: step 16830, loss 0.00195335, acc 1
2016-09-06T08:16:02.936629: step 16831, loss 0.0111384, acc 1
2016-09-06T08:16:03.764252: step 16832, loss 0.00887608, acc 1
2016-09-06T08:16:04.574486: step 16833, loss 0.0159285, acc 1
2016-09-06T08:16:05.395437: step 16834, loss 0.0202604, acc 0.98
2016-09-06T08:16:06.219918: step 16835, loss 0.105186, acc 0.96
2016-09-06T08:16:07.054339: step 16836, loss 0.0976515, acc 0.98
2016-09-06T08:16:07.886989: step 16837, loss 0.00545838, acc 1
2016-09-06T08:16:08.707199: step 16838, loss 0.0145841, acc 1
2016-09-06T08:16:09.540965: step 16839, loss 0.0177604, acc 1
2016-09-06T08:16:10.396412: step 16840, loss 0.0165171, acc 1
2016-09-06T08:16:11.204108: step 16841, loss 0.00971812, acc 1
2016-09-06T08:16:12.044112: step 16842, loss 0.0873964, acc 0.98
2016-09-06T08:16:12.876563: step 16843, loss 0.0142908, acc 1
2016-09-06T08:16:13.724124: step 16844, loss 0.00243113, acc 1
2016-09-06T08:16:14.523758: step 16845, loss 0.0265059, acc 0.98
2016-09-06T08:16:15.337968: step 16846, loss 0.00255154, acc 1
2016-09-06T08:16:16.142270: step 16847, loss 0.00328955, acc 1
2016-09-06T08:16:16.954633: step 16848, loss 0.0310815, acc 0.98
2016-09-06T08:16:17.759917: step 16849, loss 0.00419053, acc 1
2016-09-06T08:16:18.571481: step 16850, loss 0.0376976, acc 0.96
2016-09-06T08:16:19.373413: step 16851, loss 0.0152798, acc 1
2016-09-06T08:16:20.194912: step 16852, loss 0.0239991, acc 0.98
2016-09-06T08:16:21.027336: step 16853, loss 0.0185259, acc 0.98
2016-09-06T08:16:21.872173: step 16854, loss 0.0501859, acc 0.98
2016-09-06T08:16:22.676429: step 16855, loss 0.00394333, acc 1
2016-09-06T08:16:23.491812: step 16856, loss 0.00457733, acc 1
2016-09-06T08:16:24.299493: step 16857, loss 0.00191999, acc 1
2016-09-06T08:16:25.120563: step 16858, loss 0.00240357, acc 1
2016-09-06T08:16:25.970668: step 16859, loss 0.0020986, acc 1
2016-09-06T08:16:26.827539: step 16860, loss 0.0184224, acc 1
2016-09-06T08:16:27.660789: step 16861, loss 0.00221626, acc 1
2016-09-06T08:16:28.495217: step 16862, loss 0.108818, acc 0.98
2016-09-06T08:16:29.322978: step 16863, loss 0.00210612, acc 1
2016-09-06T08:16:30.137112: step 16864, loss 0.0277017, acc 0.98
2016-09-06T08:16:30.963394: step 16865, loss 0.0108254, acc 1
2016-09-06T08:16:31.776359: step 16866, loss 0.0115182, acc 1
2016-09-06T08:16:32.594440: step 16867, loss 0.0021369, acc 1
2016-09-06T08:16:33.419690: step 16868, loss 0.0122601, acc 1
2016-09-06T08:16:34.242493: step 16869, loss 0.00210548, acc 1
2016-09-06T08:16:35.058823: step 16870, loss 0.021852, acc 1
2016-09-06T08:16:35.893105: step 16871, loss 0.00284668, acc 1
2016-09-06T08:16:36.674038: step 16872, loss 0.0139337, acc 1
2016-09-06T08:16:37.476070: step 16873, loss 0.0175042, acc 0.98
2016-09-06T08:16:38.291608: step 16874, loss 0.00225358, acc 1
2016-09-06T08:16:39.090921: step 16875, loss 0.0193727, acc 1
2016-09-06T08:16:39.908519: step 16876, loss 0.0126224, acc 1
2016-09-06T08:16:40.720089: step 16877, loss 0.010881, acc 1
2016-09-06T08:16:41.522264: step 16878, loss 0.022728, acc 1
2016-09-06T08:16:42.327866: step 16879, loss 0.0294632, acc 1
2016-09-06T08:16:43.147179: step 16880, loss 0.00257471, acc 1
2016-09-06T08:16:43.977037: step 16881, loss 0.0131521, acc 1
2016-09-06T08:16:44.754694: step 16882, loss 0.0284944, acc 1
2016-09-06T08:16:45.571969: step 16883, loss 0.0170919, acc 0.98
2016-09-06T08:16:46.395021: step 16884, loss 0.0298337, acc 0.98
2016-09-06T08:16:47.164619: step 16885, loss 0.0154832, acc 1
2016-09-06T08:16:47.982364: step 16886, loss 0.00247715, acc 1
2016-09-06T08:16:48.839497: step 16887, loss 0.00232232, acc 1
2016-09-06T08:16:49.637094: step 16888, loss 0.00228585, acc 1
2016-09-06T08:16:50.438106: step 16889, loss 0.0148822, acc 1
2016-09-06T08:16:51.270832: step 16890, loss 0.0159706, acc 1
2016-09-06T08:16:52.056996: step 16891, loss 0.0280291, acc 0.98
2016-09-06T08:16:52.876238: step 16892, loss 0.00226133, acc 1
2016-09-06T08:16:53.716263: step 16893, loss 0.18657, acc 0.96
2016-09-06T08:16:54.535857: step 16894, loss 0.00254062, acc 1
2016-09-06T08:16:55.339769: step 16895, loss 0.00342337, acc 1
2016-09-06T08:16:56.083060: step 16896, loss 0.00212489, acc 1
2016-09-06T08:16:56.911136: step 16897, loss 0.0033514, acc 1
2016-09-06T08:16:57.722693: step 16898, loss 0.0495104, acc 0.96
2016-09-06T08:16:58.575821: step 16899, loss 0.0107306, acc 1
2016-09-06T08:16:59.364980: step 16900, loss 0.00228359, acc 1

Evaluation:
2016-09-06T08:17:03.112556: step 16900, loss 3.04534, acc 0.729831

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-16900

2016-09-06T08:17:04.959326: step 16901, loss 0.00697899, acc 1
2016-09-06T08:17:05.786484: step 16902, loss 0.0174401, acc 1
2016-09-06T08:17:06.604715: step 16903, loss 0.0100621, acc 1
2016-09-06T08:17:07.441468: step 16904, loss 0.00292349, acc 1
2016-09-06T08:17:08.258478: step 16905, loss 0.0237019, acc 0.98
2016-09-06T08:17:09.036705: step 16906, loss 0.00776896, acc 1
2016-09-06T08:17:09.845898: step 16907, loss 0.00197137, acc 1
2016-09-06T08:17:10.678832: step 16908, loss 0.0785973, acc 0.96
2016-09-06T08:17:11.461702: step 16909, loss 0.00739771, acc 1
2016-09-06T08:17:12.270821: step 16910, loss 0.00982613, acc 1
2016-09-06T08:17:13.081397: step 16911, loss 0.00187287, acc 1
2016-09-06T08:17:13.907307: step 16912, loss 0.00230107, acc 1
2016-09-06T08:17:14.714356: step 16913, loss 0.0167317, acc 1
2016-09-06T08:17:15.568792: step 16914, loss 0.00185432, acc 1
2016-09-06T08:17:16.367034: step 16915, loss 0.00377138, acc 1
2016-09-06T08:17:17.174356: step 16916, loss 0.00185703, acc 1
2016-09-06T08:17:17.996365: step 16917, loss 0.00322357, acc 1
2016-09-06T08:17:18.814260: step 16918, loss 0.00966515, acc 1
2016-09-06T08:17:19.668840: step 16919, loss 0.0156833, acc 1
2016-09-06T08:17:20.525684: step 16920, loss 0.00613538, acc 1
2016-09-06T08:17:21.367003: step 16921, loss 0.00398958, acc 1
2016-09-06T08:17:22.190264: step 16922, loss 0.0226828, acc 1
2016-09-06T08:17:23.038668: step 16923, loss 0.0161259, acc 0.98
2016-09-06T08:17:23.851041: step 16924, loss 0.0087293, acc 1
2016-09-06T08:17:24.681311: step 16925, loss 0.0289106, acc 0.98
2016-09-06T08:17:25.495872: step 16926, loss 0.00196886, acc 1
2016-09-06T08:17:26.300503: step 16927, loss 0.00300459, acc 1
2016-09-06T08:17:27.106836: step 16928, loss 0.0177838, acc 1
2016-09-06T08:17:27.913082: step 16929, loss 0.00210826, acc 1
2016-09-06T08:17:28.705024: step 16930, loss 0.00791161, acc 1
2016-09-06T08:17:29.511618: step 16931, loss 0.00205018, acc 1
2016-09-06T08:17:30.336247: step 16932, loss 0.00482669, acc 1
2016-09-06T08:17:31.160163: step 16933, loss 0.00210084, acc 1
2016-09-06T08:17:31.946276: step 16934, loss 0.0191515, acc 1
2016-09-06T08:17:32.760224: step 16935, loss 0.0182581, acc 1
2016-09-06T08:17:33.563910: step 16936, loss 0.00212957, acc 1
2016-09-06T08:17:34.357581: step 16937, loss 0.0058177, acc 1
2016-09-06T08:17:35.179350: step 16938, loss 0.00248026, acc 1
2016-09-06T08:17:35.997753: step 16939, loss 0.0183752, acc 1
2016-09-06T08:17:36.819541: step 16940, loss 0.00258354, acc 1
2016-09-06T08:17:37.621079: step 16941, loss 0.0101113, acc 1
2016-09-06T08:17:38.468621: step 16942, loss 0.0490996, acc 0.98
2016-09-06T08:17:39.240829: step 16943, loss 0.00319912, acc 1
2016-09-06T08:17:40.048289: step 16944, loss 0.00225816, acc 1
2016-09-06T08:17:40.856544: step 16945, loss 0.00305664, acc 1
2016-09-06T08:17:41.670873: step 16946, loss 0.00212764, acc 1
2016-09-06T08:17:42.479334: step 16947, loss 0.0204638, acc 0.98
2016-09-06T08:17:43.295690: step 16948, loss 0.00916471, acc 1
2016-09-06T08:17:44.084815: step 16949, loss 0.0263004, acc 0.98
2016-09-06T08:17:44.914769: step 16950, loss 0.00748456, acc 1
2016-09-06T08:17:45.732010: step 16951, loss 0.015815, acc 1
2016-09-06T08:17:46.527090: step 16952, loss 0.00205036, acc 1
2016-09-06T08:17:47.328589: step 16953, loss 0.00298918, acc 1
2016-09-06T08:17:48.141336: step 16954, loss 0.0020776, acc 1
2016-09-06T08:17:48.928359: step 16955, loss 0.0268527, acc 0.98
2016-09-06T08:17:49.741403: step 16956, loss 0.00234369, acc 1
2016-09-06T08:17:50.545311: step 16957, loss 0.0229903, acc 0.98
2016-09-06T08:17:51.367203: step 16958, loss 0.00455447, acc 1
2016-09-06T08:17:52.154031: step 16959, loss 0.00202581, acc 1
2016-09-06T08:17:52.941774: step 16960, loss 0.0395163, acc 0.98
2016-09-06T08:17:53.730946: step 16961, loss 0.00233967, acc 1
2016-09-06T08:17:54.513953: step 16962, loss 0.0428745, acc 0.96
2016-09-06T08:17:55.302140: step 16963, loss 0.016123, acc 0.98
2016-09-06T08:17:56.083941: step 16964, loss 0.0273594, acc 1
2016-09-06T08:17:56.914853: step 16965, loss 0.0465387, acc 0.98
2016-09-06T08:17:57.741451: step 16966, loss 0.0694555, acc 0.98
2016-09-06T08:17:58.545240: step 16967, loss 0.00173387, acc 1
2016-09-06T08:17:59.348226: step 16968, loss 0.00172623, acc 1
2016-09-06T08:18:00.165152: step 16969, loss 0.0342183, acc 0.96
2016-09-06T08:18:00.993051: step 16970, loss 0.00448047, acc 1
2016-09-06T08:18:01.802900: step 16971, loss 0.09692, acc 0.98
2016-09-06T08:18:02.617755: step 16972, loss 0.0225339, acc 1
2016-09-06T08:18:03.453216: step 16973, loss 0.0145278, acc 1
2016-09-06T08:18:04.263407: step 16974, loss 0.0195809, acc 0.98
2016-09-06T08:18:05.099803: step 16975, loss 0.00735097, acc 1
2016-09-06T08:18:05.913827: step 16976, loss 0.0275286, acc 0.98
2016-09-06T08:18:06.767079: step 16977, loss 0.00166751, acc 1
2016-09-06T08:18:07.593130: step 16978, loss 0.0239191, acc 0.98
2016-09-06T08:18:08.405317: step 16979, loss 0.0225784, acc 1
2016-09-06T08:18:09.230832: step 16980, loss 0.00348731, acc 1
2016-09-06T08:18:10.059392: step 16981, loss 0.0175174, acc 0.98
2016-09-06T08:18:10.887588: step 16982, loss 0.0460227, acc 0.98
2016-09-06T08:18:11.701460: step 16983, loss 0.0142871, acc 1
2016-09-06T08:18:12.520163: step 16984, loss 0.00573321, acc 1
2016-09-06T08:18:13.359853: step 16985, loss 0.0428948, acc 0.98
2016-09-06T08:18:14.190291: step 16986, loss 0.017857, acc 1
2016-09-06T08:18:15.022329: step 16987, loss 0.0430017, acc 0.98
2016-09-06T08:18:15.836792: step 16988, loss 0.00389176, acc 1
2016-09-06T08:18:16.648194: step 16989, loss 0.00656286, acc 1
2016-09-06T08:18:17.483516: step 16990, loss 0.0125943, acc 1
2016-09-06T08:18:18.289420: step 16991, loss 0.00868077, acc 1
2016-09-06T08:18:19.078922: step 16992, loss 0.0183027, acc 0.98
2016-09-06T08:18:19.894273: step 16993, loss 0.00280631, acc 1
2016-09-06T08:18:20.707084: step 16994, loss 0.0468585, acc 0.98
2016-09-06T08:18:21.508511: step 16995, loss 0.0156533, acc 1
2016-09-06T08:18:22.328265: step 16996, loss 0.00248932, acc 1
2016-09-06T08:18:23.167199: step 16997, loss 0.00746617, acc 1
2016-09-06T08:18:23.952999: step 16998, loss 0.00251163, acc 1
2016-09-06T08:18:24.752579: step 16999, loss 0.00252425, acc 1
2016-09-06T08:18:25.557658: step 17000, loss 0.00265219, acc 1

Evaluation:
2016-09-06T08:18:29.246212: step 17000, loss 3.08849, acc 0.729831

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-17000

2016-09-06T08:18:31.045341: step 17001, loss 0.00710098, acc 1
2016-09-06T08:18:31.873555: step 17002, loss 0.00370966, acc 1
2016-09-06T08:18:32.689886: step 17003, loss 0.0170361, acc 1
2016-09-06T08:18:33.527606: step 17004, loss 0.0179373, acc 0.98
2016-09-06T08:18:34.413487: step 17005, loss 0.0334423, acc 0.98
2016-09-06T08:18:35.228210: step 17006, loss 0.18107, acc 0.98
2016-09-06T08:18:36.022310: step 17007, loss 0.00523601, acc 1
2016-09-06T08:18:36.851125: step 17008, loss 0.0375216, acc 1
2016-09-06T08:18:37.688597: step 17009, loss 0.00382443, acc 1
2016-09-06T08:18:38.497928: step 17010, loss 0.00359911, acc 1
2016-09-06T08:18:39.297724: step 17011, loss 0.0468401, acc 0.96
2016-09-06T08:18:40.106963: step 17012, loss 0.00388383, acc 1
2016-09-06T08:18:40.882403: step 17013, loss 0.00661253, acc 1
2016-09-06T08:18:41.717853: step 17014, loss 0.00315285, acc 1
2016-09-06T08:18:42.550557: step 17015, loss 0.0041855, acc 1
2016-09-06T08:18:43.341926: step 17016, loss 0.0118264, acc 1
2016-09-06T08:18:44.153483: step 17017, loss 0.027957, acc 0.98
2016-09-06T08:18:44.971588: step 17018, loss 0.00313198, acc 1
2016-09-06T08:18:45.757221: step 17019, loss 0.154332, acc 0.98
2016-09-06T08:18:46.563227: step 17020, loss 0.00349999, acc 1
2016-09-06T08:18:47.370825: step 17021, loss 0.0143135, acc 1
2016-09-06T08:18:48.170058: step 17022, loss 0.0176857, acc 1
2016-09-06T08:18:48.960366: step 17023, loss 0.0233128, acc 0.98
2016-09-06T08:18:49.792185: step 17024, loss 0.00871757, acc 1
2016-09-06T08:18:50.591169: step 17025, loss 0.00441857, acc 1
2016-09-06T08:18:51.390270: step 17026, loss 0.0294916, acc 0.98
2016-09-06T08:18:52.209574: step 17027, loss 0.00522869, acc 1
2016-09-06T08:18:53.001651: step 17028, loss 0.020156, acc 1
2016-09-06T08:18:53.802697: step 17029, loss 0.00582955, acc 1
2016-09-06T08:18:54.615579: step 17030, loss 0.00925707, acc 1
2016-09-06T08:18:55.426910: step 17031, loss 0.0193027, acc 0.98
2016-09-06T08:18:56.244870: step 17032, loss 0.00768684, acc 1
2016-09-06T08:18:57.061233: step 17033, loss 0.0243038, acc 0.98
2016-09-06T08:18:57.862431: step 17034, loss 0.0325264, acc 0.98
2016-09-06T08:18:58.671273: step 17035, loss 0.00336021, acc 1
2016-09-06T08:18:59.496502: step 17036, loss 0.00524028, acc 1
2016-09-06T08:19:00.321620: step 17037, loss 0.0316231, acc 0.98
2016-09-06T08:19:01.147165: step 17038, loss 0.00368011, acc 1
2016-09-06T08:19:01.999867: step 17039, loss 0.0197286, acc 1
2016-09-06T08:19:02.793764: step 17040, loss 0.0317654, acc 0.98
2016-09-06T08:19:03.609560: step 17041, loss 0.00335673, acc 1
2016-09-06T08:19:04.428468: step 17042, loss 0.00455311, acc 1
2016-09-06T08:19:05.246788: step 17043, loss 0.00402048, acc 1
2016-09-06T08:19:06.092335: step 17044, loss 0.0197101, acc 0.98
2016-09-06T08:19:06.925901: step 17045, loss 0.017974, acc 0.98
2016-09-06T08:19:07.766616: step 17046, loss 0.00759558, acc 1
2016-09-06T08:19:08.597248: step 17047, loss 0.00455766, acc 1
2016-09-06T08:19:09.427612: step 17048, loss 0.0105121, acc 1
2016-09-06T08:19:10.246402: step 17049, loss 0.0375685, acc 0.98
2016-09-06T08:19:11.018644: step 17050, loss 0.00898059, acc 1
2016-09-06T08:19:11.834924: step 17051, loss 0.0068817, acc 1
2016-09-06T08:19:12.679320: step 17052, loss 0.0174455, acc 1
2016-09-06T08:19:13.500535: step 17053, loss 0.0035011, acc 1
2016-09-06T08:19:14.301355: step 17054, loss 0.00350349, acc 1
2016-09-06T08:19:15.124289: step 17055, loss 0.00342729, acc 1
2016-09-06T08:19:15.941755: step 17056, loss 0.0122802, acc 1
2016-09-06T08:19:16.744700: step 17057, loss 0.0068549, acc 1
2016-09-06T08:19:17.579037: step 17058, loss 0.0101002, acc 1
2016-09-06T08:19:18.410480: step 17059, loss 0.00316463, acc 1
2016-09-06T08:19:19.229959: step 17060, loss 0.00315983, acc 1
2016-09-06T08:19:20.090008: step 17061, loss 0.011542, acc 1
2016-09-06T08:19:20.965404: step 17062, loss 0.0368509, acc 0.96
2016-09-06T08:19:21.786761: step 17063, loss 0.00324435, acc 1
2016-09-06T08:19:22.609433: step 17064, loss 0.00308344, acc 1
2016-09-06T08:19:23.418773: step 17065, loss 0.127593, acc 0.98
2016-09-06T08:19:24.262225: step 17066, loss 0.00297563, acc 1
2016-09-06T08:19:25.097488: step 17067, loss 0.10171, acc 0.98
2016-09-06T08:19:25.915738: step 17068, loss 0.00367202, acc 1
2016-09-06T08:19:26.734002: step 17069, loss 0.0291083, acc 0.98
2016-09-06T08:19:27.593753: step 17070, loss 0.00277603, acc 1
2016-09-06T08:19:28.419097: step 17071, loss 0.0577262, acc 0.96
2016-09-06T08:19:29.197453: step 17072, loss 0.00246205, acc 1
2016-09-06T08:19:29.978079: step 17073, loss 0.0183532, acc 0.98
2016-09-06T08:19:30.797943: step 17074, loss 0.0156122, acc 1
2016-09-06T08:19:31.589425: step 17075, loss 0.00577446, acc 1
2016-09-06T08:19:32.406558: step 17076, loss 0.0855818, acc 0.96
2016-09-06T08:19:33.217820: step 17077, loss 0.0072568, acc 1
2016-09-06T08:19:34.014221: step 17078, loss 0.00666746, acc 1
2016-09-06T08:19:34.833819: step 17079, loss 0.00255893, acc 1
2016-09-06T08:19:35.643485: step 17080, loss 0.00223113, acc 1
2016-09-06T08:19:36.434818: step 17081, loss 0.00384421, acc 1
2016-09-06T08:19:37.232630: step 17082, loss 0.0161757, acc 1
2016-09-06T08:19:38.078076: step 17083, loss 0.0360882, acc 0.98
2016-09-06T08:19:38.885266: step 17084, loss 0.0220935, acc 0.98
2016-09-06T08:19:39.686990: step 17085, loss 0.0159538, acc 1
2016-09-06T08:19:40.508445: step 17086, loss 0.0210078, acc 0.98
2016-09-06T08:19:41.259216: step 17087, loss 0.00763433, acc 1
2016-09-06T08:19:42.008070: step 17088, loss 0.0107216, acc 1
2016-09-06T08:19:42.821453: step 17089, loss 0.00514457, acc 1
2016-09-06T08:19:43.651132: step 17090, loss 0.00581619, acc 1
2016-09-06T08:19:44.505619: step 17091, loss 0.0639458, acc 0.98
2016-09-06T08:19:45.326085: step 17092, loss 0.00229277, acc 1
2016-09-06T08:19:46.123297: step 17093, loss 0.0147052, acc 1
2016-09-06T08:19:46.929762: step 17094, loss 0.0214221, acc 1
2016-09-06T08:19:47.738715: step 17095, loss 0.0019854, acc 1
2016-09-06T08:19:48.561199: step 17096, loss 0.012508, acc 1
2016-09-06T08:19:49.390599: step 17097, loss 0.0178115, acc 0.98
2016-09-06T08:19:50.212995: step 17098, loss 0.0344959, acc 0.98
2016-09-06T08:19:51.020585: step 17099, loss 0.00210499, acc 1
2016-09-06T08:19:51.833972: step 17100, loss 0.036662, acc 0.98

Evaluation:
2016-09-06T08:19:55.576806: step 17100, loss 2.39435, acc 0.722326

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-17100

2016-09-06T08:19:57.487290: step 17101, loss 0.00204883, acc 1
2016-09-06T08:19:58.279332: step 17102, loss 0.0182204, acc 0.98
2016-09-06T08:19:59.054276: step 17103, loss 0.00472966, acc 1
2016-09-06T08:19:59.889306: step 17104, loss 0.00319569, acc 1
2016-09-06T08:20:00.689704: step 17105, loss 0.00230583, acc 1
2016-09-06T08:20:01.473410: step 17106, loss 0.0191957, acc 0.98
2016-09-06T08:20:02.305396: step 17107, loss 0.00807916, acc 1
2016-09-06T08:20:03.109417: step 17108, loss 0.00228843, acc 1
2016-09-06T08:20:03.901512: step 17109, loss 0.00894281, acc 1
2016-09-06T08:20:04.705073: step 17110, loss 0.0203438, acc 1
2016-09-06T08:20:05.534006: step 17111, loss 0.0269606, acc 1
2016-09-06T08:20:06.331008: step 17112, loss 0.00390193, acc 1
2016-09-06T08:20:07.161009: step 17113, loss 0.028073, acc 0.98
2016-09-06T08:20:07.961966: step 17114, loss 0.0368567, acc 1
2016-09-06T08:20:08.786711: step 17115, loss 0.0131103, acc 1
2016-09-06T08:20:09.611536: step 17116, loss 0.0160791, acc 1
2016-09-06T08:20:10.421420: step 17117, loss 0.0299829, acc 1
2016-09-06T08:20:11.243529: step 17118, loss 0.00282197, acc 1
2016-09-06T08:20:12.061557: step 17119, loss 0.00203779, acc 1
2016-09-06T08:20:12.878735: step 17120, loss 0.00612606, acc 1
2016-09-06T08:20:13.706551: step 17121, loss 0.0406654, acc 0.98
2016-09-06T08:20:14.540862: step 17122, loss 0.00211552, acc 1
2016-09-06T08:20:15.380098: step 17123, loss 0.00268926, acc 1
2016-09-06T08:20:16.198107: step 17124, loss 0.0021446, acc 1
2016-09-06T08:20:17.023200: step 17125, loss 0.00274493, acc 1
2016-09-06T08:20:17.817134: step 17126, loss 0.106481, acc 0.94
2016-09-06T08:20:18.622021: step 17127, loss 0.0027493, acc 1
2016-09-06T08:20:19.460458: step 17128, loss 0.00201428, acc 1
2016-09-06T08:20:20.268342: step 17129, loss 0.00871023, acc 1
2016-09-06T08:20:21.082569: step 17130, loss 0.00194293, acc 1
2016-09-06T08:20:21.898325: step 17131, loss 0.10498, acc 0.94
2016-09-06T08:20:22.698375: step 17132, loss 0.00259578, acc 1
2016-09-06T08:20:23.506442: step 17133, loss 0.00445281, acc 1
2016-09-06T08:20:24.333413: step 17134, loss 0.00294456, acc 1
2016-09-06T08:20:25.146087: step 17135, loss 0.00226222, acc 1
2016-09-06T08:20:25.947181: step 17136, loss 0.0298742, acc 0.98
2016-09-06T08:20:26.777814: step 17137, loss 0.0152859, acc 1
2016-09-06T08:20:27.603830: step 17138, loss 0.00174287, acc 1
2016-09-06T08:20:28.401001: step 17139, loss 0.0489648, acc 0.98
2016-09-06T08:20:29.206141: step 17140, loss 0.0272996, acc 0.98
2016-09-06T08:20:30.015079: step 17141, loss 0.0173394, acc 1
2016-09-06T08:20:30.790076: step 17142, loss 0.0143689, acc 1
2016-09-06T08:20:31.604348: step 17143, loss 0.0655645, acc 0.98
2016-09-06T08:20:32.438621: step 17144, loss 0.0450434, acc 0.98
2016-09-06T08:20:33.253017: step 17145, loss 0.00419174, acc 1
2016-09-06T08:20:34.065271: step 17146, loss 0.0134196, acc 1
2016-09-06T08:20:34.885912: step 17147, loss 0.00219003, acc 1
2016-09-06T08:20:35.665098: step 17148, loss 0.00176262, acc 1
2016-09-06T08:20:36.462353: step 17149, loss 0.00319545, acc 1
2016-09-06T08:20:37.239775: step 17150, loss 0.0075685, acc 1
2016-09-06T08:20:38.048196: step 17151, loss 0.00439353, acc 1
2016-09-06T08:20:38.869832: step 17152, loss 0.0150012, acc 1
2016-09-06T08:20:39.729822: step 17153, loss 0.0158567, acc 1
2016-09-06T08:20:40.545915: step 17154, loss 0.0192662, acc 1
2016-09-06T08:20:41.371085: step 17155, loss 0.0352211, acc 0.96
2016-09-06T08:20:42.196465: step 17156, loss 0.0199023, acc 0.98
2016-09-06T08:20:43.010255: step 17157, loss 0.0041373, acc 1
2016-09-06T08:20:43.829712: step 17158, loss 0.0434006, acc 0.96
2016-09-06T08:20:44.645321: step 17159, loss 0.00497201, acc 1
2016-09-06T08:20:45.461412: step 17160, loss 0.00275069, acc 1
2016-09-06T08:20:46.274928: step 17161, loss 0.00956042, acc 1
2016-09-06T08:20:47.077585: step 17162, loss 0.00318841, acc 1
2016-09-06T08:20:47.892125: step 17163, loss 0.00942294, acc 1
2016-09-06T08:20:48.697092: step 17164, loss 0.0134276, acc 1
2016-09-06T08:20:49.525190: step 17165, loss 0.0171067, acc 0.98
2016-09-06T08:20:50.338387: step 17166, loss 0.00502135, acc 1
2016-09-06T08:20:51.150708: step 17167, loss 0.00361556, acc 1
2016-09-06T08:20:51.972794: step 17168, loss 0.0117531, acc 1
2016-09-06T08:20:52.789947: step 17169, loss 0.00236314, acc 1
2016-09-06T08:20:53.585502: step 17170, loss 0.00330104, acc 1
2016-09-06T08:20:54.421192: step 17171, loss 0.0537268, acc 0.96
2016-09-06T08:20:55.243438: step 17172, loss 0.00246675, acc 1
2016-09-06T08:20:56.050771: step 17173, loss 0.0029681, acc 1
2016-09-06T08:20:56.897800: step 17174, loss 0.018672, acc 1
2016-09-06T08:20:57.748644: step 17175, loss 0.00238763, acc 1
2016-09-06T08:20:58.545297: step 17176, loss 0.020686, acc 0.98
2016-09-06T08:20:59.386279: step 17177, loss 0.0417881, acc 0.98
2016-09-06T08:21:00.193515: step 17178, loss 0.0159612, acc 1
2016-09-06T08:21:01.011649: step 17179, loss 0.0163224, acc 0.98
2016-09-06T08:21:01.830813: step 17180, loss 0.00272732, acc 1
2016-09-06T08:21:02.640271: step 17181, loss 0.00323864, acc 1
2016-09-06T08:21:03.452990: step 17182, loss 0.00253292, acc 1
2016-09-06T08:21:04.312846: step 17183, loss 0.0154363, acc 1
2016-09-06T08:21:05.126437: step 17184, loss 0.00367464, acc 1
2016-09-06T08:21:05.931738: step 17185, loss 0.00227668, acc 1
2016-09-06T08:21:06.751541: step 17186, loss 0.00300431, acc 1
2016-09-06T08:21:07.570826: step 17187, loss 0.00417919, acc 1
2016-09-06T08:21:08.379139: step 17188, loss 0.00250387, acc 1
2016-09-06T08:21:09.218619: step 17189, loss 0.0336481, acc 0.98
2016-09-06T08:21:10.029486: step 17190, loss 0.00222011, acc 1
2016-09-06T08:21:10.808522: step 17191, loss 0.00219979, acc 1
2016-09-06T08:21:11.642924: step 17192, loss 0.00246462, acc 1
2016-09-06T08:21:12.482093: step 17193, loss 0.034087, acc 0.96
2016-09-06T08:21:13.273467: step 17194, loss 0.00215027, acc 1
2016-09-06T08:21:14.070479: step 17195, loss 0.0175244, acc 1
2016-09-06T08:21:14.907349: step 17196, loss 0.0310824, acc 0.98
2016-09-06T08:21:15.696944: step 17197, loss 0.0176444, acc 0.98
2016-09-06T08:21:16.476816: step 17198, loss 0.040608, acc 0.98
2016-09-06T08:21:17.288414: step 17199, loss 0.0020539, acc 1
2016-09-06T08:21:18.090718: step 17200, loss 0.00480612, acc 1

Evaluation:
2016-09-06T08:21:21.811750: step 17200, loss 2.87376, acc 0.723265

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-17200

2016-09-06T08:21:23.667051: step 17201, loss 0.023205, acc 1
2016-09-06T08:21:24.495601: step 17202, loss 0.00576663, acc 1
2016-09-06T08:21:25.285671: step 17203, loss 0.0314431, acc 0.98
2016-09-06T08:21:26.141277: step 17204, loss 0.00445372, acc 1
2016-09-06T08:21:27.041018: step 17205, loss 0.00587992, acc 1
2016-09-06T08:21:27.841592: step 17206, loss 0.00194569, acc 1
2016-09-06T08:21:28.647979: step 17207, loss 0.0170564, acc 0.98
2016-09-06T08:21:29.467821: step 17208, loss 0.0647687, acc 0.98
2016-09-06T08:21:30.268742: step 17209, loss 0.00225428, acc 1
2016-09-06T08:21:31.071484: step 17210, loss 0.0587525, acc 0.98
2016-09-06T08:21:31.898055: step 17211, loss 0.00189124, acc 1
2016-09-06T08:21:32.723134: step 17212, loss 0.00180228, acc 1
2016-09-06T08:21:33.525180: step 17213, loss 0.0273689, acc 0.98
2016-09-06T08:21:34.345437: step 17214, loss 0.0150318, acc 1
2016-09-06T08:21:35.155271: step 17215, loss 0.0112903, acc 1
2016-09-06T08:21:35.988029: step 17216, loss 0.00596527, acc 1
2016-09-06T08:21:36.806222: step 17217, loss 0.00172337, acc 1
2016-09-06T08:21:37.605450: step 17218, loss 0.0158042, acc 1
2016-09-06T08:21:38.404641: step 17219, loss 0.00184356, acc 1
2016-09-06T08:21:39.240269: step 17220, loss 0.00396352, acc 1
2016-09-06T08:21:40.046993: step 17221, loss 0.00214955, acc 1
2016-09-06T08:21:40.886895: step 17222, loss 0.00470912, acc 1
2016-09-06T08:21:41.726454: step 17223, loss 0.00550431, acc 1
2016-09-06T08:21:42.546857: step 17224, loss 0.0025581, acc 1
2016-09-06T08:21:43.342182: step 17225, loss 0.00261522, acc 1
2016-09-06T08:21:44.173606: step 17226, loss 0.0421966, acc 0.98
2016-09-06T08:21:44.976307: step 17227, loss 0.00239341, acc 1
2016-09-06T08:21:45.779359: step 17228, loss 0.00463617, acc 1
2016-09-06T08:21:46.601104: step 17229, loss 0.0446847, acc 0.94
2016-09-06T08:21:47.413677: step 17230, loss 0.018612, acc 0.98
2016-09-06T08:21:48.236465: step 17231, loss 0.00682098, acc 1
2016-09-06T08:21:49.075259: step 17232, loss 0.0158387, acc 1
2016-09-06T08:21:49.859138: step 17233, loss 0.00194296, acc 1
2016-09-06T08:21:50.674242: step 17234, loss 0.0338373, acc 0.98
2016-09-06T08:21:51.505435: step 17235, loss 0.0161506, acc 0.98
2016-09-06T08:21:52.289344: step 17236, loss 0.00343445, acc 1
2016-09-06T08:21:53.112572: step 17237, loss 0.0138229, acc 1
2016-09-06T08:21:53.947815: step 17238, loss 0.0246568, acc 0.98
2016-09-06T08:21:54.751312: step 17239, loss 0.00237093, acc 1
2016-09-06T08:21:55.557100: step 17240, loss 0.00183069, acc 1
2016-09-06T08:21:56.383838: step 17241, loss 0.0122507, acc 1
2016-09-06T08:21:57.189467: step 17242, loss 0.00187019, acc 1
2016-09-06T08:21:57.997112: step 17243, loss 0.00229391, acc 1
2016-09-06T08:21:58.793869: step 17244, loss 0.00700424, acc 1
2016-09-06T08:21:59.604442: step 17245, loss 0.00190838, acc 1
2016-09-06T08:22:00.427458: step 17246, loss 0.00270467, acc 1
2016-09-06T08:22:01.224239: step 17247, loss 0.0193108, acc 0.98
2016-09-06T08:22:02.056830: step 17248, loss 0.028491, acc 0.98
2016-09-06T08:22:02.853379: step 17249, loss 0.0575584, acc 0.98
2016-09-06T08:22:03.677414: step 17250, loss 0.00199422, acc 1
2016-09-06T08:22:04.502747: step 17251, loss 0.0131993, acc 1
2016-09-06T08:22:05.283998: step 17252, loss 0.00197372, acc 1
2016-09-06T08:22:06.112684: step 17253, loss 0.00189509, acc 1
2016-09-06T08:22:06.915627: step 17254, loss 0.0164189, acc 0.98
2016-09-06T08:22:07.688183: step 17255, loss 0.00273542, acc 1
2016-09-06T08:22:08.505841: step 17256, loss 0.0132574, acc 1
2016-09-06T08:22:09.367776: step 17257, loss 0.0724654, acc 0.98
2016-09-06T08:22:10.159621: step 17258, loss 0.040958, acc 0.98
2016-09-06T08:22:10.948708: step 17259, loss 0.00277545, acc 1
2016-09-06T08:22:11.751085: step 17260, loss 0.00258302, acc 1
2016-09-06T08:22:12.524213: step 17261, loss 0.0072685, acc 1
2016-09-06T08:22:13.328621: step 17262, loss 0.0204528, acc 0.98
2016-09-06T08:22:14.128503: step 17263, loss 0.0145697, acc 1
2016-09-06T08:22:14.909030: step 17264, loss 0.00165326, acc 1
2016-09-06T08:22:15.713402: step 17265, loss 0.0133451, acc 1
2016-09-06T08:22:16.533774: step 17266, loss 0.0180537, acc 1
2016-09-06T08:22:17.321458: step 17267, loss 0.00187662, acc 1
2016-09-06T08:22:18.136207: step 17268, loss 0.0308629, acc 0.98
2016-09-06T08:22:18.962618: step 17269, loss 0.017288, acc 1
2016-09-06T08:22:19.773591: step 17270, loss 0.00206018, acc 1
2016-09-06T08:22:20.580540: step 17271, loss 0.046371, acc 0.98
2016-09-06T08:22:21.399941: step 17272, loss 0.00225755, acc 1
2016-09-06T08:22:22.184709: step 17273, loss 0.0247325, acc 0.98
2016-09-06T08:22:22.985259: step 17274, loss 0.00626071, acc 1
2016-09-06T08:22:23.818162: step 17275, loss 0.00179329, acc 1
2016-09-06T08:22:24.621200: step 17276, loss 0.00171572, acc 1
2016-09-06T08:22:25.445774: step 17277, loss 0.0231338, acc 1
2016-09-06T08:22:26.268625: step 17278, loss 0.00264513, acc 1
2016-09-06T08:22:27.055566: step 17279, loss 0.00403402, acc 1
2016-09-06T08:22:27.790299: step 17280, loss 0.00192159, acc 1
2016-09-06T08:22:28.619044: step 17281, loss 0.00747927, acc 1
2016-09-06T08:22:29.442350: step 17282, loss 0.017666, acc 1
2016-09-06T08:22:30.289544: step 17283, loss 0.0269781, acc 0.98
2016-09-06T08:22:31.075668: step 17284, loss 0.0498046, acc 0.98
2016-09-06T08:22:31.874565: step 17285, loss 0.0284657, acc 0.98
2016-09-06T08:22:32.696675: step 17286, loss 0.00199571, acc 1
2016-09-06T08:22:33.495849: step 17287, loss 0.00319969, acc 1
2016-09-06T08:22:34.265384: step 17288, loss 0.0824865, acc 0.98
2016-09-06T08:22:35.080617: step 17289, loss 0.0337435, acc 0.96
2016-09-06T08:22:35.924787: step 17290, loss 0.0173051, acc 0.98
2016-09-06T08:22:36.733709: step 17291, loss 0.00181131, acc 1
2016-09-06T08:22:37.532744: step 17292, loss 0.0061356, acc 1
2016-09-06T08:22:38.360413: step 17293, loss 0.0265184, acc 0.98
2016-09-06T08:22:39.148463: step 17294, loss 0.016407, acc 1
2016-09-06T08:22:39.932809: step 17295, loss 0.0308338, acc 0.98
2016-09-06T08:22:40.783733: step 17296, loss 0.00185965, acc 1
2016-09-06T08:22:41.571665: step 17297, loss 0.0231998, acc 1
2016-09-06T08:22:42.361682: step 17298, loss 0.00198819, acc 1
2016-09-06T08:22:43.162244: step 17299, loss 0.0188229, acc 0.98
2016-09-06T08:22:43.982345: step 17300, loss 0.00190969, acc 1

Evaluation:
2016-09-06T08:22:47.702861: step 17300, loss 2.65439, acc 0.72045

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-17300

2016-09-06T08:22:49.657364: step 17301, loss 0.00193484, acc 1
2016-09-06T08:22:50.459530: step 17302, loss 0.00193797, acc 1
2016-09-06T08:22:51.259690: step 17303, loss 0.00195249, acc 1
2016-09-06T08:22:52.091190: step 17304, loss 0.00261514, acc 1
2016-09-06T08:22:52.899688: step 17305, loss 0.0289767, acc 1
2016-09-06T08:22:53.721945: step 17306, loss 0.0166793, acc 0.98
2016-09-06T08:22:54.518047: step 17307, loss 0.0660044, acc 0.98
2016-09-06T08:22:55.335912: step 17308, loss 0.00221821, acc 1
2016-09-06T08:22:56.129580: step 17309, loss 0.0473846, acc 0.98
2016-09-06T08:22:56.973097: step 17310, loss 0.00182082, acc 1
2016-09-06T08:22:57.812542: step 17311, loss 0.0444747, acc 0.98
2016-09-06T08:22:58.618572: step 17312, loss 0.0166005, acc 1
2016-09-06T08:22:59.419139: step 17313, loss 0.00620579, acc 1
2016-09-06T08:23:00.256028: step 17314, loss 0.0156751, acc 1
2016-09-06T08:23:01.036798: step 17315, loss 0.00173007, acc 1
2016-09-06T08:23:01.844712: step 17316, loss 0.119922, acc 0.96
2016-09-06T08:23:02.691946: step 17317, loss 0.00468061, acc 1
2016-09-06T08:23:03.548263: step 17318, loss 0.0102946, acc 1
2016-09-06T08:23:04.356538: step 17319, loss 0.0146226, acc 1
2016-09-06T08:23:05.170898: step 17320, loss 0.00568553, acc 1
2016-09-06T08:23:05.958810: step 17321, loss 0.00202727, acc 1
2016-09-06T08:23:06.766414: step 17322, loss 0.0171622, acc 1
2016-09-06T08:23:07.588898: step 17323, loss 0.0538669, acc 0.94
2016-09-06T08:23:08.425214: step 17324, loss 0.00194687, acc 1
2016-09-06T08:23:09.228190: step 17325, loss 0.00653232, acc 1
2016-09-06T08:23:10.049259: step 17326, loss 0.0162426, acc 1
2016-09-06T08:23:10.883373: step 17327, loss 0.00397652, acc 1
2016-09-06T08:23:11.704803: step 17328, loss 0.0110444, acc 1
2016-09-06T08:23:12.541706: step 17329, loss 0.00286807, acc 1
2016-09-06T08:23:13.357322: step 17330, loss 0.00192334, acc 1
2016-09-06T08:23:14.151052: step 17331, loss 0.0215311, acc 0.98
2016-09-06T08:23:14.969375: step 17332, loss 0.0194461, acc 1
2016-09-06T08:23:15.819179: step 17333, loss 0.0213206, acc 0.98
2016-09-06T08:23:16.596851: step 17334, loss 0.00480669, acc 1
2016-09-06T08:23:17.433065: step 17335, loss 0.0647479, acc 0.98
2016-09-06T08:23:18.241447: step 17336, loss 0.00359419, acc 1
2016-09-06T08:23:19.018283: step 17337, loss 0.00251724, acc 1
2016-09-06T08:23:19.863489: step 17338, loss 0.0276993, acc 0.98
2016-09-06T08:23:20.669398: step 17339, loss 0.00813388, acc 1
2016-09-06T08:23:21.477598: step 17340, loss 0.00171814, acc 1
2016-09-06T08:23:22.273822: step 17341, loss 0.00322419, acc 1
2016-09-06T08:23:23.063868: step 17342, loss 0.00195151, acc 1
2016-09-06T08:23:23.885249: step 17343, loss 0.00229648, acc 1
2016-09-06T08:23:24.717567: step 17344, loss 0.0215977, acc 1
2016-09-06T08:23:25.516028: step 17345, loss 0.00342693, acc 1
2016-09-06T08:23:26.316802: step 17346, loss 0.020649, acc 1
2016-09-06T08:23:27.138302: step 17347, loss 0.013943, acc 1
2016-09-06T08:23:27.950940: step 17348, loss 0.0182529, acc 1
2016-09-06T08:23:28.735815: step 17349, loss 0.0227953, acc 0.98
2016-09-06T08:23:29.548677: step 17350, loss 0.0173096, acc 0.98
2016-09-06T08:23:30.384485: step 17351, loss 0.0156522, acc 1
2016-09-06T08:23:31.202514: step 17352, loss 0.00185924, acc 1
2016-09-06T08:23:32.000227: step 17353, loss 0.075387, acc 0.98
2016-09-06T08:23:32.828613: step 17354, loss 0.00187394, acc 1
2016-09-06T08:23:33.607247: step 17355, loss 0.0275318, acc 1
2016-09-06T08:23:34.420975: step 17356, loss 0.00667505, acc 1
2016-09-06T08:23:35.251171: step 17357, loss 0.0199564, acc 0.98
2016-09-06T08:23:36.055967: step 17358, loss 0.0154735, acc 1
2016-09-06T08:23:36.862109: step 17359, loss 0.00205074, acc 1
2016-09-06T08:23:37.660028: step 17360, loss 0.00785259, acc 1
2016-09-06T08:23:38.434180: step 17361, loss 0.00190819, acc 1
2016-09-06T08:23:39.228916: step 17362, loss 0.00280264, acc 1
2016-09-06T08:23:40.061151: step 17363, loss 0.0198241, acc 0.98
2016-09-06T08:23:40.852551: step 17364, loss 0.00182122, acc 1
2016-09-06T08:23:41.672641: step 17365, loss 0.0100378, acc 1
2016-09-06T08:23:42.519645: step 17366, loss 0.0118073, acc 1
2016-09-06T08:23:43.320278: step 17367, loss 0.0350358, acc 0.98
2016-09-06T08:23:44.121653: step 17368, loss 0.00230163, acc 1
2016-09-06T08:23:44.924817: step 17369, loss 0.00935786, acc 1
2016-09-06T08:23:45.716922: step 17370, loss 0.00182071, acc 1
2016-09-06T08:23:46.514430: step 17371, loss 0.00264568, acc 1
2016-09-06T08:23:47.325423: step 17372, loss 0.00198786, acc 1
2016-09-06T08:23:48.147980: step 17373, loss 0.0117458, acc 1
2016-09-06T08:23:48.943575: step 17374, loss 0.00182664, acc 1
2016-09-06T08:23:49.782670: step 17375, loss 0.00943138, acc 1
2016-09-06T08:23:50.608965: step 17376, loss 0.00766716, acc 1
2016-09-06T08:23:51.435561: step 17377, loss 0.00262829, acc 1
2016-09-06T08:23:52.263378: step 17378, loss 0.041113, acc 1
2016-09-06T08:23:53.103570: step 17379, loss 0.00992096, acc 1
2016-09-06T08:23:53.929543: step 17380, loss 0.0389575, acc 0.98
2016-09-06T08:23:54.783089: step 17381, loss 0.0252156, acc 0.98
2016-09-06T08:23:55.605442: step 17382, loss 0.00331752, acc 1
2016-09-06T08:23:56.420643: step 17383, loss 0.00244392, acc 1
2016-09-06T08:23:57.235492: step 17384, loss 0.00278653, acc 1
2016-09-06T08:23:58.051399: step 17385, loss 0.023417, acc 0.98
2016-09-06T08:23:58.877399: step 17386, loss 0.00237254, acc 1
2016-09-06T08:23:59.722595: step 17387, loss 0.00939299, acc 1
2016-09-06T08:24:00.548129: step 17388, loss 0.00879536, acc 1
2016-09-06T08:24:01.345835: step 17389, loss 0.0152616, acc 1
2016-09-06T08:24:02.176341: step 17390, loss 0.0243351, acc 1
2016-09-06T08:24:02.979091: step 17391, loss 0.0017602, acc 1
2016-09-06T08:24:03.759543: step 17392, loss 0.00229628, acc 1
2016-09-06T08:24:04.570400: step 17393, loss 0.00196124, acc 1
2016-09-06T08:24:05.421253: step 17394, loss 0.0131353, acc 1
2016-09-06T08:24:06.213415: step 17395, loss 0.027997, acc 1
2016-09-06T08:24:07.007827: step 17396, loss 0.0120359, acc 1
2016-09-06T08:24:07.813251: step 17397, loss 0.0129349, acc 1
2016-09-06T08:24:08.605796: step 17398, loss 0.00181072, acc 1
2016-09-06T08:24:09.476033: step 17399, loss 0.00187556, acc 1
2016-09-06T08:24:10.295806: step 17400, loss 0.00233551, acc 1

Evaluation:
2016-09-06T08:24:14.008192: step 17400, loss 3.09377, acc 0.726079

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-17400

2016-09-06T08:24:15.894033: step 17401, loss 0.00208889, acc 1
2016-09-06T08:24:16.707356: step 17402, loss 0.0018348, acc 1
2016-09-06T08:24:17.518198: step 17403, loss 0.00182183, acc 1
2016-09-06T08:24:18.305515: step 17404, loss 0.144876, acc 0.96
2016-09-06T08:24:19.148358: step 17405, loss 0.011344, acc 1
2016-09-06T08:24:19.952991: step 17406, loss 0.00924721, acc 1
2016-09-06T08:24:20.777644: step 17407, loss 0.00616166, acc 1
2016-09-06T08:24:21.596465: step 17408, loss 0.0122617, acc 1
2016-09-06T08:24:22.436704: step 17409, loss 0.00264015, acc 1
2016-09-06T08:24:23.233534: step 17410, loss 0.0367624, acc 1
2016-09-06T08:24:24.067982: step 17411, loss 0.0164735, acc 1
2016-09-06T08:24:24.907924: step 17412, loss 0.00168202, acc 1
2016-09-06T08:24:25.698702: step 17413, loss 0.00173259, acc 1
2016-09-06T08:24:26.494717: step 17414, loss 0.0433664, acc 0.98
2016-09-06T08:24:27.330290: step 17415, loss 0.00926758, acc 1
2016-09-06T08:24:28.122078: step 17416, loss 0.0138876, acc 1
2016-09-06T08:24:28.926227: step 17417, loss 0.0136434, acc 1
2016-09-06T08:24:29.737110: step 17418, loss 0.0190555, acc 0.98
2016-09-06T08:24:30.514153: step 17419, loss 0.00160729, acc 1
2016-09-06T08:24:31.323077: step 17420, loss 0.00152711, acc 1
2016-09-06T08:24:32.141678: step 17421, loss 0.0240685, acc 0.98
2016-09-06T08:24:32.919078: step 17422, loss 0.00641307, acc 1
2016-09-06T08:24:33.719851: step 17423, loss 0.00466193, acc 1
2016-09-06T08:24:34.534208: step 17424, loss 0.0442348, acc 0.98
2016-09-06T08:24:35.319387: step 17425, loss 0.0215301, acc 0.98
2016-09-06T08:24:36.133779: step 17426, loss 0.0249079, acc 0.98
2016-09-06T08:24:36.970289: step 17427, loss 0.00306608, acc 1
2016-09-06T08:24:37.750685: step 17428, loss 0.0137798, acc 1
2016-09-06T08:24:38.546645: step 17429, loss 0.0134742, acc 1
2016-09-06T08:24:39.397889: step 17430, loss 0.0017042, acc 1
2016-09-06T08:24:40.165011: step 17431, loss 0.0095881, acc 1
2016-09-06T08:24:41.016860: step 17432, loss 0.00140459, acc 1
2016-09-06T08:24:41.843394: step 17433, loss 0.0307819, acc 0.98
2016-09-06T08:24:42.626710: step 17434, loss 0.00150882, acc 1
2016-09-06T08:24:43.407840: step 17435, loss 0.0145276, acc 1
2016-09-06T08:24:44.245025: step 17436, loss 0.0247747, acc 1
2016-09-06T08:24:45.029436: step 17437, loss 0.00368397, acc 1
2016-09-06T08:24:45.837410: step 17438, loss 0.00179345, acc 1
2016-09-06T08:24:46.650042: step 17439, loss 0.0499405, acc 0.96
2016-09-06T08:24:47.447685: step 17440, loss 0.0427072, acc 0.96
2016-09-06T08:24:48.258737: step 17441, loss 0.0318046, acc 1
2016-09-06T08:24:49.060280: step 17442, loss 0.00231282, acc 1
2016-09-06T08:24:49.866566: step 17443, loss 0.00148304, acc 1
2016-09-06T08:24:50.684066: step 17444, loss 0.00140629, acc 1
2016-09-06T08:24:51.503486: step 17445, loss 0.00689285, acc 1
2016-09-06T08:24:52.307772: step 17446, loss 0.0151383, acc 1
2016-09-06T08:24:53.129848: step 17447, loss 0.00148339, acc 1
2016-09-06T08:24:53.982903: step 17448, loss 0.00285993, acc 1
2016-09-06T08:24:54.778954: step 17449, loss 0.0443238, acc 1
2016-09-06T08:24:55.577270: step 17450, loss 0.00281091, acc 1
2016-09-06T08:24:56.421669: step 17451, loss 0.00353997, acc 1
2016-09-06T08:24:57.213845: step 17452, loss 0.00176399, acc 1
2016-09-06T08:24:58.035948: step 17453, loss 0.0210279, acc 0.98
2016-09-06T08:24:58.851860: step 17454, loss 0.011277, acc 1
2016-09-06T08:24:59.684945: step 17455, loss 0.0325526, acc 0.98
2016-09-06T08:25:00.524599: step 17456, loss 0.0774634, acc 0.98
2016-09-06T08:25:01.377113: step 17457, loss 0.00165326, acc 1
2016-09-06T08:25:02.176543: step 17458, loss 0.0343575, acc 0.98
2016-09-06T08:25:02.981925: step 17459, loss 0.0150218, acc 1
2016-09-06T08:25:03.833439: step 17460, loss 0.0301029, acc 0.98
2016-09-06T08:25:04.650548: step 17461, loss 0.00688337, acc 1
2016-09-06T08:25:05.494049: step 17462, loss 0.0172684, acc 1
2016-09-06T08:25:06.323199: step 17463, loss 0.00155133, acc 1
2016-09-06T08:25:07.144859: step 17464, loss 0.0103506, acc 1
2016-09-06T08:25:07.965585: step 17465, loss 0.00710008, acc 1
2016-09-06T08:25:08.756896: step 17466, loss 0.0145283, acc 1
2016-09-06T08:25:09.570442: step 17467, loss 0.0120139, acc 1
2016-09-06T08:25:10.385208: step 17468, loss 0.00248523, acc 1
2016-09-06T08:25:11.193041: step 17469, loss 0.0251798, acc 1
2016-09-06T08:25:12.022154: step 17470, loss 0.00459487, acc 1
2016-09-06T08:25:12.809413: step 17471, loss 0.00664627, acc 1
2016-09-06T08:25:13.569349: step 17472, loss 0.0018006, acc 1
2016-09-06T08:25:14.392864: step 17473, loss 0.010869, acc 1
2016-09-06T08:25:15.225510: step 17474, loss 0.014037, acc 1
2016-09-06T08:25:16.038977: step 17475, loss 0.0178351, acc 0.98
2016-09-06T08:25:16.912349: step 17476, loss 0.00186046, acc 1
2016-09-06T08:25:17.715529: step 17477, loss 0.0276545, acc 1
2016-09-06T08:25:18.541308: step 17478, loss 0.0264334, acc 1
2016-09-06T08:25:19.386862: step 17479, loss 0.00202959, acc 1
2016-09-06T08:25:20.197269: step 17480, loss 0.00886544, acc 1
2016-09-06T08:25:21.009900: step 17481, loss 0.0229786, acc 1
2016-09-06T08:25:21.885438: step 17482, loss 0.0515451, acc 0.96
2016-09-06T08:25:22.725125: step 17483, loss 0.00203352, acc 1
2016-09-06T08:25:23.612946: step 17484, loss 0.00440345, acc 1
2016-09-06T08:25:24.475236: step 17485, loss 0.00218612, acc 1
2016-09-06T08:25:25.274604: step 17486, loss 0.0134504, acc 1
2016-09-06T08:25:26.061463: step 17487, loss 0.0399625, acc 0.98
2016-09-06T08:25:26.880784: step 17488, loss 0.0775303, acc 0.98
2016-09-06T08:25:27.739624: step 17489, loss 0.0640999, acc 0.98
2016-09-06T08:25:28.532177: step 17490, loss 0.00205584, acc 1
2016-09-06T08:25:29.328803: step 17491, loss 0.0226307, acc 0.98
2016-09-06T08:25:30.175160: step 17492, loss 0.00189981, acc 1
2016-09-06T08:25:30.945405: step 17493, loss 0.00227324, acc 1
2016-09-06T08:25:31.754381: step 17494, loss 0.003143, acc 1
2016-09-06T08:25:32.561621: step 17495, loss 0.0144971, acc 1
2016-09-06T08:25:33.332820: step 17496, loss 0.00218697, acc 1
2016-09-06T08:25:34.140153: step 17497, loss 0.0183823, acc 0.98
2016-09-06T08:25:34.947309: step 17498, loss 0.0399336, acc 0.96
2016-09-06T08:25:35.770908: step 17499, loss 0.0101093, acc 1
2016-09-06T08:25:36.592881: step 17500, loss 0.0230818, acc 1

Evaluation:
2016-09-06T08:25:40.299908: step 17500, loss 2.89719, acc 0.718574

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-17500

2016-09-06T08:25:42.180301: step 17501, loss 0.0105954, acc 1
2016-09-06T08:25:42.990989: step 17502, loss 0.00445404, acc 1
2016-09-06T08:25:43.823002: step 17503, loss 0.0357571, acc 0.98
2016-09-06T08:25:44.643915: step 17504, loss 0.00460383, acc 1
2016-09-06T08:25:45.518625: step 17505, loss 0.00564815, acc 1
2016-09-06T08:25:46.337646: step 17506, loss 0.0141731, acc 1
2016-09-06T08:25:47.169441: step 17507, loss 0.019983, acc 0.98
2016-09-06T08:25:47.984311: step 17508, loss 0.025034, acc 0.98
2016-09-06T08:25:48.808124: step 17509, loss 0.00200422, acc 1
2016-09-06T08:25:49.628124: step 17510, loss 0.0496048, acc 0.96
2016-09-06T08:25:50.411017: step 17511, loss 0.00219216, acc 1
2016-09-06T08:25:51.216811: step 17512, loss 0.00202875, acc 1
2016-09-06T08:25:52.012804: step 17513, loss 0.031348, acc 0.98
2016-09-06T08:25:52.799446: step 17514, loss 0.0124271, acc 1
2016-09-06T08:25:53.611737: step 17515, loss 0.0410199, acc 0.98
2016-09-06T08:25:54.437391: step 17516, loss 0.00200142, acc 1
2016-09-06T08:25:55.219048: step 17517, loss 0.00451362, acc 1
2016-09-06T08:25:56.057590: step 17518, loss 0.00201907, acc 1
2016-09-06T08:25:56.852634: step 17519, loss 0.043718, acc 0.98
2016-09-06T08:25:57.647830: step 17520, loss 0.0305636, acc 0.98
2016-09-06T08:25:58.432348: step 17521, loss 0.0021, acc 1
2016-09-06T08:25:59.253811: step 17522, loss 0.00212994, acc 1
2016-09-06T08:26:00.036568: step 17523, loss 0.00194853, acc 1
2016-09-06T08:26:00.897688: step 17524, loss 0.0301876, acc 0.98
2016-09-06T08:26:01.722282: step 17525, loss 0.0220696, acc 0.98
2016-09-06T08:26:02.521483: step 17526, loss 0.00483628, acc 1
2016-09-06T08:26:03.329372: step 17527, loss 0.0304622, acc 0.98
2016-09-06T08:26:04.217080: step 17528, loss 0.00364459, acc 1
2016-09-06T08:26:05.056913: step 17529, loss 0.0020944, acc 1
2016-09-06T08:26:05.860382: step 17530, loss 0.0270372, acc 0.98
2016-09-06T08:26:06.706062: step 17531, loss 0.00278473, acc 1
2016-09-06T08:26:07.515544: step 17532, loss 0.00606152, acc 1
2016-09-06T08:26:08.315623: step 17533, loss 0.0166404, acc 1
2016-09-06T08:26:09.144665: step 17534, loss 0.0067779, acc 1
2016-09-06T08:26:09.956661: step 17535, loss 0.00184039, acc 1
2016-09-06T08:26:10.783967: step 17536, loss 0.0573348, acc 0.98
2016-09-06T08:26:11.613653: step 17537, loss 0.00188452, acc 1
2016-09-06T08:26:12.422347: step 17538, loss 0.00324293, acc 1
2016-09-06T08:26:13.253831: step 17539, loss 0.015873, acc 1
2016-09-06T08:26:14.097423: step 17540, loss 0.0386553, acc 0.98
2016-09-06T08:26:14.911815: step 17541, loss 0.0205436, acc 0.98
2016-09-06T08:26:15.739218: step 17542, loss 0.00217483, acc 1
2016-09-06T08:26:16.544898: step 17543, loss 0.0120182, acc 1
2016-09-06T08:26:17.357804: step 17544, loss 0.014997, acc 1
2016-09-06T08:26:18.131489: step 17545, loss 0.00505713, acc 1
2016-09-06T08:26:18.977402: step 17546, loss 0.00294206, acc 1
2016-09-06T08:26:19.778151: step 17547, loss 0.00215207, acc 1
2016-09-06T08:26:20.573683: step 17548, loss 0.00943499, acc 1
2016-09-06T08:26:21.378596: step 17549, loss 0.0133277, acc 1
2016-09-06T08:26:22.194184: step 17550, loss 0.0165934, acc 1
2016-09-06T08:26:22.976433: step 17551, loss 0.00988479, acc 1
2016-09-06T08:26:23.771879: step 17552, loss 0.0130345, acc 1
2016-09-06T08:26:24.616238: step 17553, loss 0.00183023, acc 1
2016-09-06T08:26:25.427216: step 17554, loss 0.00227584, acc 1
2016-09-06T08:26:26.265201: step 17555, loss 0.026999, acc 0.98
2016-09-06T08:26:27.069698: step 17556, loss 0.0022411, acc 1
2016-09-06T08:26:27.839396: step 17557, loss 0.00180864, acc 1
2016-09-06T08:26:28.637978: step 17558, loss 0.0167055, acc 1
2016-09-06T08:26:29.451095: step 17559, loss 0.00176847, acc 1
2016-09-06T08:26:30.237346: step 17560, loss 0.0110134, acc 1
2016-09-06T08:26:31.043120: step 17561, loss 0.100337, acc 0.98
2016-09-06T08:26:31.870844: step 17562, loss 0.00191266, acc 1
2016-09-06T08:26:32.689291: step 17563, loss 0.0728163, acc 0.96
2016-09-06T08:26:33.509773: step 17564, loss 0.0016408, acc 1
2016-09-06T08:26:34.341141: step 17565, loss 0.00820726, acc 1
2016-09-06T08:26:35.117338: step 17566, loss 0.00166433, acc 1
2016-09-06T08:26:35.939578: step 17567, loss 0.0173715, acc 0.98
2016-09-06T08:26:36.758660: step 17568, loss 0.00575319, acc 1
2016-09-06T08:26:37.525944: step 17569, loss 0.00188477, acc 1
2016-09-06T08:26:38.330485: step 17570, loss 0.00419849, acc 1
2016-09-06T08:26:39.146357: step 17571, loss 0.0184904, acc 1
2016-09-06T08:26:39.946054: step 17572, loss 0.0103476, acc 1
2016-09-06T08:26:40.758753: step 17573, loss 0.00519161, acc 1
2016-09-06T08:26:41.591960: step 17574, loss 0.00198219, acc 1
2016-09-06T08:26:42.394461: step 17575, loss 0.0238741, acc 0.98
2016-09-06T08:26:43.175546: step 17576, loss 0.021763, acc 0.98
2016-09-06T08:26:43.993526: step 17577, loss 0.036655, acc 0.98
2016-09-06T08:26:44.790408: step 17578, loss 0.00336448, acc 1
2016-09-06T08:26:45.568528: step 17579, loss 0.0144923, acc 1
2016-09-06T08:26:46.402006: step 17580, loss 0.0211366, acc 0.98
2016-09-06T08:26:47.185478: step 17581, loss 0.0231367, acc 1
2016-09-06T08:26:47.996451: step 17582, loss 0.0517134, acc 0.96
2016-09-06T08:26:48.843176: step 17583, loss 0.00176676, acc 1
2016-09-06T08:26:49.618290: step 17584, loss 0.0224205, acc 1
2016-09-06T08:26:50.454021: step 17585, loss 0.00220235, acc 1
2016-09-06T08:26:51.277313: step 17586, loss 0.00302426, acc 1
2016-09-06T08:26:52.108907: step 17587, loss 0.00223003, acc 1
2016-09-06T08:26:52.930663: step 17588, loss 0.052974, acc 0.96
2016-09-06T08:26:53.769049: step 17589, loss 0.0118711, acc 1
2016-09-06T08:26:54.570933: step 17590, loss 0.00183714, acc 1
2016-09-06T08:26:55.376500: step 17591, loss 0.00274879, acc 1
2016-09-06T08:26:56.200013: step 17592, loss 0.0068102, acc 1
2016-09-06T08:26:56.987433: step 17593, loss 0.0145083, acc 1
2016-09-06T08:26:57.823723: step 17594, loss 0.00622956, acc 1
2016-09-06T08:26:58.666276: step 17595, loss 0.00371876, acc 1
2016-09-06T08:26:59.519920: step 17596, loss 0.0309508, acc 0.98
2016-09-06T08:27:00.336780: step 17597, loss 0.00753665, acc 1
2016-09-06T08:27:01.163904: step 17598, loss 0.003492, acc 1
2016-09-06T08:27:01.967829: step 17599, loss 0.0296676, acc 0.98
2016-09-06T08:27:02.802036: step 17600, loss 0.00605109, acc 1

Evaluation:
2016-09-06T08:27:06.557780: step 17600, loss 3.39563, acc 0.709193

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-17600

2016-09-06T08:27:08.452215: step 17601, loss 0.0031149, acc 1
2016-09-06T08:27:09.279083: step 17602, loss 0.00337234, acc 1
2016-09-06T08:27:10.090810: step 17603, loss 0.00579319, acc 1
2016-09-06T08:27:10.944782: step 17604, loss 0.00331342, acc 1
2016-09-06T08:27:11.846884: step 17605, loss 0.0265412, acc 0.98
2016-09-06T08:27:12.604796: step 17606, loss 0.030646, acc 0.98
2016-09-06T08:27:13.429995: step 17607, loss 0.0137912, acc 1
2016-09-06T08:27:14.290411: step 17608, loss 0.0111356, acc 1
2016-09-06T08:27:15.228595: step 17609, loss 0.0177987, acc 0.98
2016-09-06T08:27:16.027061: step 17610, loss 0.0416831, acc 0.98
2016-09-06T08:27:16.879358: step 17611, loss 0.0119932, acc 1
2016-09-06T08:27:17.732417: step 17612, loss 0.00395062, acc 1
2016-09-06T08:27:18.546941: step 17613, loss 0.0486229, acc 0.96
2016-09-06T08:27:19.411691: step 17614, loss 0.00386933, acc 1
2016-09-06T08:27:20.450155: step 17615, loss 0.0276951, acc 0.98
2016-09-06T08:27:21.306077: step 17616, loss 0.0251419, acc 1
2016-09-06T08:27:22.178047: step 17617, loss 0.00356939, acc 1
2016-09-06T08:27:23.110728: step 17618, loss 0.0126996, acc 1
2016-09-06T08:27:23.998306: step 17619, loss 0.04722, acc 0.96
2016-09-06T08:27:24.833785: step 17620, loss 0.00348958, acc 1
2016-09-06T08:27:25.682894: step 17621, loss 0.047674, acc 0.98
2016-09-06T08:27:26.585105: step 17622, loss 0.00636748, acc 1
2016-09-06T08:27:27.475042: step 17623, loss 0.00347606, acc 1
2016-09-06T08:27:28.362896: step 17624, loss 0.00436295, acc 1
2016-09-06T08:27:29.227608: step 17625, loss 0.00623892, acc 1
2016-09-06T08:27:30.090740: step 17626, loss 0.00459116, acc 1
2016-09-06T08:27:30.929103: step 17627, loss 0.00320586, acc 1
2016-09-06T08:27:31.769401: step 17628, loss 0.0269341, acc 1
2016-09-06T08:27:32.730921: step 17629, loss 0.00317514, acc 1
2016-09-06T08:27:33.561555: step 17630, loss 0.018055, acc 1
2016-09-06T08:27:34.442421: step 17631, loss 0.00343958, acc 1
2016-09-06T08:27:35.284880: step 17632, loss 0.0182083, acc 1
2016-09-06T08:27:36.154729: step 17633, loss 0.00491697, acc 1
2016-09-06T08:27:37.082486: step 17634, loss 0.00332015, acc 1
2016-09-06T08:27:37.991064: step 17635, loss 0.00302816, acc 1
2016-09-06T08:27:38.788202: step 17636, loss 0.00318546, acc 1
2016-09-06T08:27:39.623689: step 17637, loss 0.00297917, acc 1
2016-09-06T08:27:40.471171: step 17638, loss 0.016784, acc 1
2016-09-06T08:27:41.315798: step 17639, loss 0.0141232, acc 1
2016-09-06T08:27:42.153500: step 17640, loss 0.0510372, acc 0.98
2016-09-06T08:27:43.029708: step 17641, loss 0.0485131, acc 0.98
2016-09-06T08:27:43.881416: step 17642, loss 0.00718312, acc 1
2016-09-06T08:27:44.723566: step 17643, loss 0.0176585, acc 0.98
2016-09-06T08:27:45.541015: step 17644, loss 0.0028643, acc 1
2016-09-06T08:27:46.468982: step 17645, loss 0.0119718, acc 1
2016-09-06T08:27:47.331157: step 17646, loss 0.0183024, acc 1
2016-09-06T08:27:48.181203: step 17647, loss 0.0493103, acc 0.98
2016-09-06T08:27:49.036796: step 17648, loss 0.00336005, acc 1
2016-09-06T08:27:49.860490: step 17649, loss 0.0147479, acc 1
2016-09-06T08:27:50.734584: step 17650, loss 0.0210484, acc 1
2016-09-06T08:27:51.527573: step 17651, loss 0.0131772, acc 1
2016-09-06T08:27:52.370752: step 17652, loss 0.0416754, acc 0.98
2016-09-06T08:27:53.261880: step 17653, loss 0.00272254, acc 1
2016-09-06T08:27:54.085209: step 17654, loss 0.0204607, acc 0.98
2016-09-06T08:27:54.885462: step 17655, loss 0.00271232, acc 1
2016-09-06T08:27:55.705538: step 17656, loss 0.00326647, acc 1
2016-09-06T08:27:56.516662: step 17657, loss 0.0202447, acc 0.98
2016-09-06T08:27:57.342950: step 17658, loss 0.0198903, acc 0.98
2016-09-06T08:27:58.170666: step 17659, loss 0.0310465, acc 0.98
2016-09-06T08:27:59.006179: step 17660, loss 0.00296704, acc 1
2016-09-06T08:27:59.816753: step 17661, loss 0.019644, acc 1
2016-09-06T08:28:00.637259: step 17662, loss 0.0184891, acc 0.98
2016-09-06T08:28:01.470604: step 17663, loss 0.00251158, acc 1
2016-09-06T08:28:02.232447: step 17664, loss 0.00917625, acc 1
2016-09-06T08:28:03.052385: step 17665, loss 0.0225837, acc 1
2016-09-06T08:28:03.882999: step 17666, loss 0.0557207, acc 0.98
2016-09-06T08:28:04.712258: step 17667, loss 0.00330419, acc 1
2016-09-06T08:28:05.513605: step 17668, loss 0.0117628, acc 1
2016-09-06T08:28:06.375875: step 17669, loss 0.0236921, acc 1
2016-09-06T08:28:07.179517: step 17670, loss 0.014267, acc 1
2016-09-06T08:28:08.023128: step 17671, loss 0.00261865, acc 1
2016-09-06T08:28:08.879609: step 17672, loss 0.00606399, acc 1
2016-09-06T08:28:09.706148: step 17673, loss 0.0173972, acc 1
2016-09-06T08:28:10.529425: step 17674, loss 0.0146762, acc 1
2016-09-06T08:28:11.393924: step 17675, loss 0.00254633, acc 1
2016-09-06T08:28:12.209934: step 17676, loss 0.00241552, acc 1
2016-09-06T08:28:13.017439: step 17677, loss 0.019815, acc 1
2016-09-06T08:28:13.840375: step 17678, loss 0.0345654, acc 0.96
2016-09-06T08:28:14.653921: step 17679, loss 0.0336742, acc 0.96
2016-09-06T08:28:15.447432: step 17680, loss 0.00255178, acc 1
2016-09-06T08:28:16.260691: step 17681, loss 0.00258431, acc 1
2016-09-06T08:28:17.093075: step 17682, loss 0.00246777, acc 1
2016-09-06T08:28:17.923558: step 17683, loss 0.00802602, acc 1
2016-09-06T08:28:18.696009: step 17684, loss 0.00227422, acc 1
2016-09-06T08:28:19.542875: step 17685, loss 0.00226608, acc 1
2016-09-06T08:28:20.325439: step 17686, loss 0.0813649, acc 0.96
2016-09-06T08:28:21.121040: step 17687, loss 0.00274616, acc 1
2016-09-06T08:28:21.937437: step 17688, loss 0.00673113, acc 1
2016-09-06T08:28:22.739376: step 17689, loss 0.0228581, acc 0.98
2016-09-06T08:28:23.553490: step 17690, loss 0.0155246, acc 1
2016-09-06T08:28:24.387796: step 17691, loss 0.176058, acc 0.96
2016-09-06T08:28:25.239282: step 17692, loss 0.00200037, acc 1
2016-09-06T08:28:26.070100: step 17693, loss 0.0144231, acc 1
2016-09-06T08:28:26.916208: step 17694, loss 0.0115129, acc 1
2016-09-06T08:28:27.731156: step 17695, loss 0.00182678, acc 1
2016-09-06T08:28:28.540397: step 17696, loss 0.0118946, acc 1
2016-09-06T08:28:29.403610: step 17697, loss 0.0608906, acc 0.98
2016-09-06T08:28:30.224559: step 17698, loss 0.0173006, acc 1
2016-09-06T08:28:31.018861: step 17699, loss 0.0150773, acc 1
2016-09-06T08:28:31.840272: step 17700, loss 0.0182981, acc 0.98

Evaluation:
2016-09-06T08:28:35.631259: step 17700, loss 2.68221, acc 0.692308

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-17700

2016-09-06T08:28:37.468281: step 17701, loss 0.00274491, acc 1
2016-09-06T08:28:38.310649: step 17702, loss 0.00976678, acc 1
2016-09-06T08:28:39.144416: step 17703, loss 0.0125285, acc 1
2016-09-06T08:28:39.956580: step 17704, loss 0.00199635, acc 1
2016-09-06T08:28:40.761841: step 17705, loss 0.00332087, acc 1
2016-09-06T08:28:41.589536: step 17706, loss 0.0101366, acc 1
2016-09-06T08:28:42.428941: step 17707, loss 0.0294244, acc 1
2016-09-06T08:28:43.251518: step 17708, loss 0.00206811, acc 1
2016-09-06T08:28:44.085616: step 17709, loss 0.00252347, acc 1
2016-09-06T08:28:44.932174: step 17710, loss 0.0121448, acc 1
2016-09-06T08:28:45.764476: step 17711, loss 0.0811343, acc 0.98
2016-09-06T08:28:46.553284: step 17712, loss 0.027664, acc 0.98
2016-09-06T08:28:47.366243: step 17713, loss 0.0172575, acc 1
2016-09-06T08:28:48.189099: step 17714, loss 0.00294794, acc 1
2016-09-06T08:28:49.003940: step 17715, loss 0.00726612, acc 1
2016-09-06T08:28:49.831471: step 17716, loss 0.0165927, acc 0.98
2016-09-06T08:28:50.658407: step 17717, loss 0.0158148, acc 1
2016-09-06T08:28:51.565618: step 17718, loss 0.0307852, acc 0.98
2016-09-06T08:28:52.415260: step 17719, loss 0.00409921, acc 1
2016-09-06T08:28:53.234528: step 17720, loss 0.00302779, acc 1
2016-09-06T08:28:54.050976: step 17721, loss 0.00309214, acc 1
2016-09-06T08:28:54.916334: step 17722, loss 0.0267298, acc 0.98
2016-09-06T08:28:55.850337: step 17723, loss 0.00313598, acc 1
2016-09-06T08:28:56.693622: step 17724, loss 0.0026239, acc 1
2016-09-06T08:28:57.544201: step 17725, loss 0.0490478, acc 0.98
2016-09-06T08:28:58.364897: step 17726, loss 0.00303433, acc 1
2016-09-06T08:28:59.279924: step 17727, loss 0.00259504, acc 1
2016-09-06T08:29:00.121006: step 17728, loss 0.0783204, acc 0.96
2016-09-06T08:29:00.988943: step 17729, loss 0.00346485, acc 1
2016-09-06T08:29:01.809899: step 17730, loss 0.0255615, acc 0.98
2016-09-06T08:29:02.601439: step 17731, loss 0.0167191, acc 0.98
2016-09-06T08:29:03.402872: step 17732, loss 0.0786147, acc 0.98
2016-09-06T08:29:04.252296: step 17733, loss 0.0114879, acc 1
2016-09-06T08:29:05.064676: step 17734, loss 0.0288998, acc 0.98
2016-09-06T08:29:05.865586: step 17735, loss 0.0798655, acc 0.98
2016-09-06T08:29:06.709876: step 17736, loss 0.00220708, acc 1
2016-09-06T08:29:07.501453: step 17737, loss 0.0190622, acc 0.98
2016-09-06T08:29:08.316611: step 17738, loss 0.0411579, acc 0.98
2016-09-06T08:29:09.131335: step 17739, loss 0.0347331, acc 0.98
2016-09-06T08:29:09.915443: step 17740, loss 0.0175007, acc 1
2016-09-06T08:29:10.735549: step 17741, loss 0.00222124, acc 1
2016-09-06T08:29:11.590680: step 17742, loss 0.0234979, acc 0.98
2016-09-06T08:29:12.434645: step 17743, loss 0.00378249, acc 1
2016-09-06T08:29:13.241408: step 17744, loss 0.0040409, acc 1
2016-09-06T08:29:14.056520: step 17745, loss 0.00230771, acc 1
2016-09-06T08:29:14.875529: step 17746, loss 0.00693752, acc 1
2016-09-06T08:29:15.712023: step 17747, loss 0.0392664, acc 0.98
2016-09-06T08:29:16.538449: step 17748, loss 0.00229506, acc 1
2016-09-06T08:29:17.363996: step 17749, loss 0.00323403, acc 1
2016-09-06T08:29:18.158043: step 17750, loss 0.0281016, acc 1
2016-09-06T08:29:19.014591: step 17751, loss 0.00341429, acc 1
2016-09-06T08:29:19.836497: step 17752, loss 0.0135099, acc 1
2016-09-06T08:29:20.622638: step 17753, loss 0.0141257, acc 1
2016-09-06T08:29:21.439542: step 17754, loss 0.011886, acc 1
2016-09-06T08:29:22.240514: step 17755, loss 0.0467506, acc 0.98
2016-09-06T08:29:23.064666: step 17756, loss 0.00197619, acc 1
2016-09-06T08:29:23.892513: step 17757, loss 0.0269554, acc 0.98
2016-09-06T08:29:24.735569: step 17758, loss 0.00221966, acc 1
2016-09-06T08:29:25.543241: step 17759, loss 0.0119108, acc 1
2016-09-06T08:29:26.363885: step 17760, loss 0.00187815, acc 1
2016-09-06T08:29:27.210188: step 17761, loss 0.0208325, acc 0.98
2016-09-06T08:29:28.007724: step 17762, loss 0.0291548, acc 0.98
2016-09-06T08:29:28.811655: step 17763, loss 0.00993263, acc 1
2016-09-06T08:29:29.669255: step 17764, loss 0.0282094, acc 0.98
2016-09-06T08:29:30.435165: step 17765, loss 0.00622799, acc 1
2016-09-06T08:29:31.241050: step 17766, loss 0.00893673, acc 1
2016-09-06T08:29:32.074055: step 17767, loss 0.00858556, acc 1
2016-09-06T08:29:32.863628: step 17768, loss 0.00233408, acc 1
2016-09-06T08:29:33.701526: step 17769, loss 0.010321, acc 1
2016-09-06T08:29:34.502456: step 17770, loss 0.00184054, acc 1
2016-09-06T08:29:35.266037: step 17771, loss 0.0102863, acc 1
2016-09-06T08:29:36.078022: step 17772, loss 0.00261049, acc 1
2016-09-06T08:29:36.930408: step 17773, loss 0.0593604, acc 0.98
2016-09-06T08:29:37.729191: step 17774, loss 0.0255002, acc 1
2016-09-06T08:29:38.529565: step 17775, loss 0.00676406, acc 1
2016-09-06T08:29:39.358838: step 17776, loss 0.0166261, acc 1
2016-09-06T08:29:40.174556: step 17777, loss 0.0136601, acc 1
2016-09-06T08:29:40.980866: step 17778, loss 0.0138339, acc 1
2016-09-06T08:29:41.786913: step 17779, loss 0.021229, acc 1
2016-09-06T08:29:42.579150: step 17780, loss 0.0027932, acc 1
2016-09-06T08:29:43.395829: step 17781, loss 0.00220145, acc 1
2016-09-06T08:29:44.231875: step 17782, loss 0.00185985, acc 1
2016-09-06T08:29:45.061471: step 17783, loss 0.0132993, acc 1
2016-09-06T08:29:45.884353: step 17784, loss 0.0112982, acc 1
2016-09-06T08:29:46.684727: step 17785, loss 0.00248033, acc 1
2016-09-06T08:29:47.502844: step 17786, loss 0.00211887, acc 1
2016-09-06T08:29:48.305684: step 17787, loss 0.00194809, acc 1
2016-09-06T08:29:49.136302: step 17788, loss 0.018069, acc 0.98
2016-09-06T08:29:49.979508: step 17789, loss 0.0319766, acc 0.98
2016-09-06T08:29:50.778247: step 17790, loss 0.00262283, acc 1
2016-09-06T08:29:51.659122: step 17791, loss 0.00801277, acc 1
2016-09-06T08:29:52.477543: step 17792, loss 0.00248028, acc 1
2016-09-06T08:29:53.338359: step 17793, loss 0.0042059, acc 1
2016-09-06T08:29:54.157142: step 17794, loss 0.0118122, acc 1
2016-09-06T08:29:54.978704: step 17795, loss 0.00253805, acc 1
2016-09-06T08:29:55.817246: step 17796, loss 0.00300166, acc 1
2016-09-06T08:29:56.629555: step 17797, loss 0.0503769, acc 0.96
2016-09-06T08:29:57.445434: step 17798, loss 0.00771848, acc 1
2016-09-06T08:29:58.297142: step 17799, loss 0.0383277, acc 0.98
2016-09-06T08:29:59.112257: step 17800, loss 0.00997699, acc 1

Evaluation:
2016-09-06T08:30:02.875128: step 17800, loss 3.81366, acc 0.707317

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-17800

2016-09-06T08:30:04.960504: step 17801, loss 0.0123361, acc 1
2016-09-06T08:30:05.795596: step 17802, loss 0.0274466, acc 0.98
2016-09-06T08:30:06.621709: step 17803, loss 0.00572748, acc 1
2016-09-06T08:30:07.505312: step 17804, loss 0.00203944, acc 1
2016-09-06T08:30:08.364420: step 17805, loss 0.0222232, acc 1
2016-09-06T08:30:09.177918: step 17806, loss 0.00207458, acc 1
2016-09-06T08:30:10.026074: step 17807, loss 0.0113372, acc 1
2016-09-06T08:30:10.966963: step 17808, loss 0.0270006, acc 0.98
2016-09-06T08:30:11.823448: step 17809, loss 0.00236294, acc 1
2016-09-06T08:30:12.801276: step 17810, loss 0.00341291, acc 1
2016-09-06T08:30:13.805540: step 17811, loss 0.0212425, acc 1
2016-09-06T08:30:14.725030: step 17812, loss 0.00383579, acc 1
2016-09-06T08:30:15.546713: step 17813, loss 0.0401168, acc 0.96
2016-09-06T08:30:16.356821: step 17814, loss 0.0182635, acc 1
2016-09-06T08:30:17.194488: step 17815, loss 0.00334741, acc 1
2016-09-06T08:30:18.038369: step 17816, loss 0.00203044, acc 1
2016-09-06T08:30:18.881925: step 17817, loss 0.00205539, acc 1
2016-09-06T08:30:19.690061: step 17818, loss 0.00208498, acc 1
2016-09-06T08:30:20.503197: step 17819, loss 0.00213415, acc 1
2016-09-06T08:30:21.311198: step 17820, loss 0.00373743, acc 1
2016-09-06T08:30:22.113598: step 17821, loss 0.0185312, acc 0.98
2016-09-06T08:30:22.919599: step 17822, loss 0.00221287, acc 1
2016-09-06T08:30:23.714706: step 17823, loss 0.00265203, acc 1
2016-09-06T08:30:24.524899: step 17824, loss 0.02041, acc 1
2016-09-06T08:30:25.356181: step 17825, loss 0.0224119, acc 1
2016-09-06T08:30:26.198315: step 17826, loss 0.00236074, acc 1
2016-09-06T08:30:27.060780: step 17827, loss 0.00258128, acc 1
2016-09-06T08:30:27.910897: step 17828, loss 0.00268934, acc 1
2016-09-06T08:30:28.783570: step 17829, loss 0.00251599, acc 1
2016-09-06T08:30:29.720816: step 17830, loss 0.00871749, acc 1
2016-09-06T08:30:30.573140: step 17831, loss 0.039608, acc 0.98
2016-09-06T08:30:31.417920: step 17832, loss 0.00565679, acc 1
2016-09-06T08:30:32.236696: step 17833, loss 0.00820015, acc 1
2016-09-06T08:30:33.053890: step 17834, loss 0.00266423, acc 1
2016-09-06T08:30:33.878617: step 17835, loss 0.0026566, acc 1
2016-09-06T08:30:34.711327: step 17836, loss 0.00282475, acc 1
2016-09-06T08:30:35.554977: step 17837, loss 0.00649215, acc 1
2016-09-06T08:30:36.393664: step 17838, loss 0.0151007, acc 1
2016-09-06T08:30:37.208628: step 17839, loss 0.023487, acc 0.98
2016-09-06T08:30:38.025638: step 17840, loss 0.00282602, acc 1
2016-09-06T08:30:38.901858: step 17841, loss 0.0289213, acc 0.98
2016-09-06T08:30:39.723422: step 17842, loss 0.0264176, acc 1
2016-09-06T08:30:40.556061: step 17843, loss 0.0140069, acc 1
2016-09-06T08:30:41.401398: step 17844, loss 0.0136555, acc 1
2016-09-06T08:30:42.478309: step 17845, loss 0.00279591, acc 1
2016-09-06T08:30:43.285445: step 17846, loss 0.00438376, acc 1
2016-09-06T08:30:44.094811: step 17847, loss 0.0028773, acc 1
2016-09-06T08:30:44.926223: step 17848, loss 0.00274749, acc 1
2016-09-06T08:30:45.733602: step 17849, loss 0.0310719, acc 0.98
2016-09-06T08:30:46.630000: step 17850, loss 0.0213056, acc 0.98
2016-09-06T08:30:47.466609: step 17851, loss 0.00269489, acc 1
2016-09-06T08:30:48.297529: step 17852, loss 0.00266533, acc 1
2016-09-06T08:30:49.102308: step 17853, loss 0.00923122, acc 1
2016-09-06T08:30:49.925463: step 17854, loss 0.00642011, acc 1
2016-09-06T08:30:50.774305: step 17855, loss 0.00268935, acc 1
2016-09-06T08:30:51.533393: step 17856, loss 0.00284186, acc 1
2016-09-06T08:30:52.345429: step 17857, loss 0.00260459, acc 1
2016-09-06T08:30:53.191860: step 17858, loss 0.0286095, acc 0.98
2016-09-06T08:30:54.009252: step 17859, loss 0.0537139, acc 0.96
2016-09-06T08:30:54.847593: step 17860, loss 0.0714526, acc 0.98
2016-09-06T08:30:55.690829: step 17861, loss 0.0324708, acc 0.98
2016-09-06T08:30:56.506570: step 17862, loss 0.0296292, acc 0.98
2016-09-06T08:30:57.322348: step 17863, loss 0.00261823, acc 1
2016-09-06T08:30:58.194418: step 17864, loss 0.00228233, acc 1
2016-09-06T08:30:59.033930: step 17865, loss 0.0153901, acc 1
2016-09-06T08:30:59.908421: step 17866, loss 0.0156318, acc 1
2016-09-06T08:31:00.752469: step 17867, loss 0.074987, acc 0.98
2016-09-06T08:31:01.616784: step 17868, loss 0.0523865, acc 0.96
2016-09-06T08:31:02.424173: step 17869, loss 0.0184863, acc 0.98
2016-09-06T08:31:03.236535: step 17870, loss 0.0855144, acc 0.96
2016-09-06T08:31:04.085323: step 17871, loss 0.0175226, acc 0.98
2016-09-06T08:31:04.894192: step 17872, loss 0.00218654, acc 1
2016-09-06T08:31:05.704001: step 17873, loss 0.0215888, acc 0.98
2016-09-06T08:31:06.541407: step 17874, loss 0.00223667, acc 1
2016-09-06T08:31:07.363545: step 17875, loss 0.0242958, acc 1
2016-09-06T08:31:08.176169: step 17876, loss 0.0042159, acc 1
2016-09-06T08:31:08.999342: step 17877, loss 0.00620981, acc 1
2016-09-06T08:31:09.820201: step 17878, loss 0.0324582, acc 0.98
2016-09-06T08:31:10.630638: step 17879, loss 0.0186208, acc 0.98
2016-09-06T08:31:11.457724: step 17880, loss 0.0325788, acc 0.98
2016-09-06T08:31:12.272689: step 17881, loss 0.00367277, acc 1
2016-09-06T08:31:13.086193: step 17882, loss 0.0301967, acc 1
2016-09-06T08:31:13.932960: step 17883, loss 0.00280893, acc 1
2016-09-06T08:31:14.743973: step 17884, loss 0.00275655, acc 1
2016-09-06T08:31:15.551796: step 17885, loss 0.0429676, acc 0.98
2016-09-06T08:31:16.359603: step 17886, loss 0.00652668, acc 1
2016-09-06T08:31:17.156275: step 17887, loss 0.0384278, acc 0.98
2016-09-06T08:31:17.968714: step 17888, loss 0.00240053, acc 1
2016-09-06T08:31:18.816288: step 17889, loss 0.00806947, acc 1
2016-09-06T08:31:19.662232: step 17890, loss 0.0374842, acc 1
2016-09-06T08:31:20.449402: step 17891, loss 0.00234461, acc 1
2016-09-06T08:31:21.281500: step 17892, loss 0.0169312, acc 0.98
2016-09-06T08:31:22.078940: step 17893, loss 0.00237352, acc 1
2016-09-06T08:31:22.879585: step 17894, loss 0.0198639, acc 0.98
2016-09-06T08:31:23.696677: step 17895, loss 0.0181672, acc 1
2016-09-06T08:31:24.526109: step 17896, loss 0.00247803, acc 1
2016-09-06T08:31:25.325506: step 17897, loss 0.015998, acc 1
2016-09-06T08:31:26.147796: step 17898, loss 0.0024045, acc 1
2016-09-06T08:31:26.975036: step 17899, loss 0.0315154, acc 0.98
2016-09-06T08:31:27.782703: step 17900, loss 0.0109434, acc 1

Evaluation:
2016-09-06T08:31:31.497630: step 17900, loss 2.88728, acc 0.711069

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-17900

2016-09-06T08:31:33.430483: step 17901, loss 0.00244868, acc 1
2016-09-06T08:31:34.226487: step 17902, loss 0.0875736, acc 0.96
2016-09-06T08:31:35.053408: step 17903, loss 0.00259291, acc 1
2016-09-06T08:31:35.893418: step 17904, loss 0.00423819, acc 1
2016-09-06T08:31:36.715699: step 17905, loss 0.0124129, acc 1
2016-09-06T08:31:37.505378: step 17906, loss 0.0027799, acc 1
2016-09-06T08:31:38.294576: step 17907, loss 0.0196689, acc 0.98
2016-09-06T08:31:39.103457: step 17908, loss 0.0695006, acc 0.98
2016-09-06T08:31:39.895688: step 17909, loss 0.0112432, acc 1
2016-09-06T08:31:40.691353: step 17910, loss 0.0225503, acc 1
2016-09-06T08:31:41.534376: step 17911, loss 0.00242741, acc 1
2016-09-06T08:31:42.319913: step 17912, loss 0.00818563, acc 1
2016-09-06T08:31:43.155238: step 17913, loss 0.00217583, acc 1
2016-09-06T08:31:43.976755: step 17914, loss 0.0307252, acc 1
2016-09-06T08:31:44.767652: step 17915, loss 0.0741108, acc 0.98
2016-09-06T08:31:45.553341: step 17916, loss 0.00380577, acc 1
2016-09-06T08:31:46.378418: step 17917, loss 0.0211914, acc 0.98
2016-09-06T08:31:47.156006: step 17918, loss 0.00718815, acc 1
2016-09-06T08:31:47.961517: step 17919, loss 0.00302371, acc 1
2016-09-06T08:31:48.764575: step 17920, loss 0.0235292, acc 0.98
2016-09-06T08:31:49.568621: step 17921, loss 0.00669836, acc 1
2016-09-06T08:31:50.362977: step 17922, loss 0.00379049, acc 1
2016-09-06T08:31:51.200100: step 17923, loss 0.015924, acc 1
2016-09-06T08:31:52.007074: step 17924, loss 0.0276974, acc 0.98
2016-09-06T08:31:52.797402: step 17925, loss 0.00233311, acc 1
2016-09-06T08:31:53.625467: step 17926, loss 0.0108489, acc 1
2016-09-06T08:31:54.400269: step 17927, loss 0.0152747, acc 1
2016-09-06T08:31:55.195025: step 17928, loss 0.00749213, acc 1
2016-09-06T08:31:56.009145: step 17929, loss 0.00222998, acc 1
2016-09-06T08:31:56.790325: step 17930, loss 0.0048146, acc 1
2016-09-06T08:31:57.609642: step 17931, loss 0.0284834, acc 0.98
2016-09-06T08:31:58.458750: step 17932, loss 0.0168594, acc 0.98
2016-09-06T08:31:59.201243: step 17933, loss 0.00751265, acc 1
2016-09-06T08:31:59.992866: step 17934, loss 0.0296256, acc 1
2016-09-06T08:32:00.842297: step 17935, loss 0.00848026, acc 1
2016-09-06T08:32:01.667657: step 17936, loss 0.00231362, acc 1
2016-09-06T08:32:02.472096: step 17937, loss 0.00326879, acc 1
2016-09-06T08:32:03.272280: step 17938, loss 0.00233952, acc 1
2016-09-06T08:32:04.076664: step 17939, loss 0.00798458, acc 1
2016-09-06T08:32:04.872113: step 17940, loss 0.00202612, acc 1
2016-09-06T08:32:05.703448: step 17941, loss 0.00206285, acc 1
2016-09-06T08:32:06.475329: step 17942, loss 0.0207439, acc 0.98
2016-09-06T08:32:07.285033: step 17943, loss 0.00913621, acc 1
2016-09-06T08:32:08.106465: step 17944, loss 0.0219974, acc 0.98
2016-09-06T08:32:08.880264: step 17945, loss 0.0192101, acc 0.98
2016-09-06T08:32:09.679943: step 17946, loss 0.00372787, acc 1
2016-09-06T08:32:10.513428: step 17947, loss 0.0164647, acc 1
2016-09-06T08:32:11.328200: step 17948, loss 0.00201227, acc 1
2016-09-06T08:32:12.163908: step 17949, loss 0.0079114, acc 1
2016-09-06T08:32:13.034300: step 17950, loss 0.00807247, acc 1
2016-09-06T08:32:13.855684: step 17951, loss 0.0198586, acc 0.98
2016-09-06T08:32:14.666829: step 17952, loss 0.00276699, acc 1
2016-09-06T08:32:15.482484: step 17953, loss 0.00202169, acc 1
2016-09-06T08:32:16.275734: step 17954, loss 0.00587213, acc 1
2016-09-06T08:32:17.087503: step 17955, loss 0.0210545, acc 0.98
2016-09-06T08:32:17.897802: step 17956, loss 0.00692741, acc 1
2016-09-06T08:32:18.720232: step 17957, loss 0.00366972, acc 1
2016-09-06T08:32:19.549059: step 17958, loss 0.0159931, acc 0.98
2016-09-06T08:32:20.368770: step 17959, loss 0.025585, acc 0.98
2016-09-06T08:32:21.188733: step 17960, loss 0.00202885, acc 1
2016-09-06T08:32:22.005419: step 17961, loss 0.00609082, acc 1
2016-09-06T08:32:22.829802: step 17962, loss 0.0179923, acc 0.98
2016-09-06T08:32:23.633379: step 17963, loss 0.0916631, acc 0.98
2016-09-06T08:32:24.452474: step 17964, loss 0.0523632, acc 0.96
2016-09-06T08:32:25.260879: step 17965, loss 0.0642647, acc 0.96
2016-09-06T08:32:26.075861: step 17966, loss 0.0172799, acc 0.98
2016-09-06T08:32:26.909791: step 17967, loss 0.00179339, acc 1
2016-09-06T08:32:27.747844: step 17968, loss 0.0240839, acc 0.98
2016-09-06T08:32:28.559242: step 17969, loss 0.00169038, acc 1
2016-09-06T08:32:29.362891: step 17970, loss 0.0145232, acc 1
2016-09-06T08:32:30.193748: step 17971, loss 0.0190964, acc 1
2016-09-06T08:32:31.022852: step 17972, loss 0.0025207, acc 1
2016-09-06T08:32:31.820852: step 17973, loss 0.0163784, acc 1
2016-09-06T08:32:32.635131: step 17974, loss 0.00163123, acc 1
2016-09-06T08:32:33.464047: step 17975, loss 0.0244388, acc 0.98
2016-09-06T08:32:34.255466: step 17976, loss 0.0131791, acc 1
2016-09-06T08:32:35.051545: step 17977, loss 0.00165235, acc 1
2016-09-06T08:32:35.875640: step 17978, loss 0.0202864, acc 0.98
2016-09-06T08:32:36.682131: step 17979, loss 0.00178069, acc 1
2016-09-06T08:32:37.493620: step 17980, loss 0.00384372, acc 1
2016-09-06T08:32:38.305409: step 17981, loss 0.00825026, acc 1
2016-09-06T08:32:39.085248: step 17982, loss 0.00169715, acc 1
2016-09-06T08:32:39.925482: step 17983, loss 0.0347621, acc 1
2016-09-06T08:32:40.749726: step 17984, loss 0.00254084, acc 1
2016-09-06T08:32:41.522490: step 17985, loss 0.00371015, acc 1
2016-09-06T08:32:42.337888: step 17986, loss 0.00176385, acc 1
2016-09-06T08:32:43.121362: step 17987, loss 0.00280794, acc 1
2016-09-06T08:32:43.949569: step 17988, loss 0.00215675, acc 1
2016-09-06T08:32:44.763133: step 17989, loss 0.0138455, acc 1
2016-09-06T08:32:45.594762: step 17990, loss 0.0184709, acc 0.98
2016-09-06T08:32:46.404176: step 17991, loss 0.0108757, acc 1
2016-09-06T08:32:47.207150: step 17992, loss 0.0138984, acc 1
2016-09-06T08:32:48.088228: step 17993, loss 0.0092105, acc 1
2016-09-06T08:32:48.912037: step 17994, loss 0.0272851, acc 0.98
2016-09-06T08:32:49.726455: step 17995, loss 0.00179416, acc 1
2016-09-06T08:32:50.594984: step 17996, loss 0.001924, acc 1
2016-09-06T08:32:51.428145: step 17997, loss 0.0160182, acc 1
2016-09-06T08:32:52.247646: step 17998, loss 0.00186116, acc 1
2016-09-06T08:32:53.061496: step 17999, loss 0.00460057, acc 1
2016-09-06T08:32:53.881466: step 18000, loss 0.0132489, acc 1

Evaluation:
2016-09-06T08:32:57.627614: step 18000, loss 2.93491, acc 0.719512

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-18000

2016-09-06T08:32:59.616500: step 18001, loss 0.00447994, acc 1
2016-09-06T08:33:00.461236: step 18002, loss 0.0154872, acc 1
2016-09-06T08:33:01.285858: step 18003, loss 0.0152495, acc 1
2016-09-06T08:33:02.079250: step 18004, loss 0.0971743, acc 0.98
2016-09-06T08:33:02.888537: step 18005, loss 0.00660859, acc 1
2016-09-06T08:33:03.717400: step 18006, loss 0.00795487, acc 1
2016-09-06T08:33:04.531030: step 18007, loss 0.00376701, acc 1
2016-09-06T08:33:05.330884: step 18008, loss 0.00177493, acc 1
2016-09-06T08:33:06.158692: step 18009, loss 0.00179479, acc 1
2016-09-06T08:33:06.968127: step 18010, loss 0.0017514, acc 1
2016-09-06T08:33:07.765272: step 18011, loss 0.0132965, acc 1
2016-09-06T08:33:08.574239: step 18012, loss 0.0368071, acc 0.96
2016-09-06T08:33:09.384982: step 18013, loss 0.00171481, acc 1
2016-09-06T08:33:10.181125: step 18014, loss 0.0686123, acc 0.98
2016-09-06T08:33:10.990750: step 18015, loss 0.0142149, acc 1
2016-09-06T08:33:11.788099: step 18016, loss 0.0256914, acc 0.98
2016-09-06T08:33:12.562597: step 18017, loss 0.0527024, acc 0.96
2016-09-06T08:33:13.403191: step 18018, loss 0.0346472, acc 0.98
2016-09-06T08:33:14.200104: step 18019, loss 0.0216893, acc 0.98
2016-09-06T08:33:15.001569: step 18020, loss 0.00161922, acc 1
2016-09-06T08:33:15.823960: step 18021, loss 0.00986469, acc 1
2016-09-06T08:33:16.625065: step 18022, loss 0.0301928, acc 0.96
2016-09-06T08:33:17.428013: step 18023, loss 0.0110316, acc 1
2016-09-06T08:33:18.254812: step 18024, loss 0.0123433, acc 1
2016-09-06T08:33:19.043450: step 18025, loss 0.0493309, acc 0.98
2016-09-06T08:33:19.841264: step 18026, loss 0.0203454, acc 0.98
2016-09-06T08:33:20.653762: step 18027, loss 0.00973972, acc 1
2016-09-06T08:33:21.441537: step 18028, loss 0.00201834, acc 1
2016-09-06T08:33:22.216996: step 18029, loss 0.0199695, acc 1
2016-09-06T08:33:23.041704: step 18030, loss 0.033503, acc 0.98
2016-09-06T08:33:23.835533: step 18031, loss 0.0158371, acc 0.98
2016-09-06T08:33:24.612201: step 18032, loss 0.00152469, acc 1
2016-09-06T08:33:25.429026: step 18033, loss 0.00538743, acc 1
2016-09-06T08:33:26.265729: step 18034, loss 0.0367878, acc 0.96
2016-09-06T08:33:27.069889: step 18035, loss 0.024278, acc 1
2016-09-06T08:33:27.850288: step 18036, loss 0.0395468, acc 0.98
2016-09-06T08:33:28.652038: step 18037, loss 0.0278672, acc 0.98
2016-09-06T08:33:29.461703: step 18038, loss 0.0806189, acc 0.98
2016-09-06T08:33:30.283326: step 18039, loss 0.0128915, acc 1
2016-09-06T08:33:31.083572: step 18040, loss 0.00154044, acc 1
2016-09-06T08:33:31.875933: step 18041, loss 0.0019324, acc 1
2016-09-06T08:33:32.698381: step 18042, loss 0.0181895, acc 0.98
2016-09-06T08:33:33.481656: step 18043, loss 0.00692834, acc 1
2016-09-06T08:33:34.282263: step 18044, loss 0.00300905, acc 1
2016-09-06T08:33:35.116767: step 18045, loss 0.0165359, acc 1
2016-09-06T08:33:35.895889: step 18046, loss 0.0042263, acc 1
2016-09-06T08:33:36.705572: step 18047, loss 0.0164086, acc 1
2016-09-06T08:33:37.461041: step 18048, loss 0.00358266, acc 1
2016-09-06T08:33:38.286312: step 18049, loss 0.0205168, acc 0.98
2016-09-06T08:33:39.108648: step 18050, loss 0.0107617, acc 1
2016-09-06T08:33:39.912765: step 18051, loss 0.0451832, acc 0.98
2016-09-06T08:33:40.718351: step 18052, loss 0.00170313, acc 1
2016-09-06T08:33:41.528655: step 18053, loss 0.017329, acc 0.98
2016-09-06T08:33:42.368297: step 18054, loss 0.0230802, acc 0.98
2016-09-06T08:33:43.160175: step 18055, loss 0.016178, acc 1
2016-09-06T08:33:43.957702: step 18056, loss 0.0224275, acc 0.98
2016-09-06T08:33:44.786551: step 18057, loss 0.0136745, acc 1
2016-09-06T08:33:45.614777: step 18058, loss 0.0210692, acc 0.98
2016-09-06T08:33:46.414476: step 18059, loss 0.00234932, acc 1
2016-09-06T08:33:47.255338: step 18060, loss 0.0260686, acc 0.98
2016-09-06T08:33:48.027301: step 18061, loss 0.0144837, acc 1
2016-09-06T08:33:48.817483: step 18062, loss 0.0291479, acc 1
2016-09-06T08:33:49.648482: step 18063, loss 0.0106584, acc 1
2016-09-06T08:33:50.433414: step 18064, loss 0.0241336, acc 0.98
2016-09-06T08:33:51.224294: step 18065, loss 0.035493, acc 0.98
2016-09-06T08:33:52.088180: step 18066, loss 0.00190667, acc 1
2016-09-06T08:33:52.875538: step 18067, loss 0.0135735, acc 1
2016-09-06T08:33:53.679483: step 18068, loss 0.00164223, acc 1
2016-09-06T08:33:54.485204: step 18069, loss 0.0151342, acc 1
2016-09-06T08:33:55.249634: step 18070, loss 0.00184282, acc 1
2016-09-06T08:33:56.053854: step 18071, loss 0.02294, acc 1
2016-09-06T08:33:56.883458: step 18072, loss 0.0111575, acc 1
2016-09-06T08:33:57.667677: step 18073, loss 0.0445815, acc 0.98
2016-09-06T08:33:58.450821: step 18074, loss 0.00188002, acc 1
2016-09-06T08:33:59.271756: step 18075, loss 0.0187072, acc 0.98
2016-09-06T08:34:00.079261: step 18076, loss 0.0644834, acc 0.96
2016-09-06T08:34:00.917824: step 18077, loss 0.0198412, acc 0.98
2016-09-06T08:34:01.712623: step 18078, loss 0.0108596, acc 1
2016-09-06T08:34:02.498266: step 18079, loss 0.00500126, acc 1
2016-09-06T08:34:03.282176: step 18080, loss 0.0157693, acc 0.98
2016-09-06T08:34:04.090435: step 18081, loss 0.0025516, acc 1
2016-09-06T08:34:04.869305: step 18082, loss 0.00455287, acc 1
2016-09-06T08:34:05.704839: step 18083, loss 0.00170473, acc 1
2016-09-06T08:34:06.549464: step 18084, loss 0.0343817, acc 0.98
2016-09-06T08:34:07.344189: step 18085, loss 0.00412818, acc 1
2016-09-06T08:34:08.132694: step 18086, loss 0.0275551, acc 0.98
2016-09-06T08:34:08.944212: step 18087, loss 0.0118059, acc 1
2016-09-06T08:34:09.746888: step 18088, loss 0.00162308, acc 1
2016-09-06T08:34:10.538851: step 18089, loss 0.017279, acc 0.98
2016-09-06T08:34:11.371036: step 18090, loss 0.0171989, acc 1
2016-09-06T08:34:12.177223: step 18091, loss 0.037489, acc 0.96
2016-09-06T08:34:12.966367: step 18092, loss 0.0015663, acc 1
2016-09-06T08:34:13.771126: step 18093, loss 0.0015986, acc 1
2016-09-06T08:34:14.559746: step 18094, loss 0.0404083, acc 0.98
2016-09-06T08:34:15.373737: step 18095, loss 0.00168137, acc 1
2016-09-06T08:34:16.218191: step 18096, loss 0.0148307, acc 1
2016-09-06T08:34:16.993101: step 18097, loss 0.0205229, acc 1
2016-09-06T08:34:17.807681: step 18098, loss 0.00233106, acc 1
2016-09-06T08:34:18.612277: step 18099, loss 0.0681084, acc 0.98
2016-09-06T08:34:19.401562: step 18100, loss 0.0267175, acc 0.98

Evaluation:
2016-09-06T08:34:23.136105: step 18100, loss 2.53328, acc 0.717636

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-18100

2016-09-06T08:34:25.031211: step 18101, loss 0.0334683, acc 0.98
2016-09-06T08:34:25.884393: step 18102, loss 0.0567487, acc 0.98
2016-09-06T08:34:26.691841: step 18103, loss 0.00146317, acc 1
2016-09-06T08:34:27.505760: step 18104, loss 0.00147011, acc 1
2016-09-06T08:34:28.334278: step 18105, loss 0.005765, acc 1
2016-09-06T08:34:29.122252: step 18106, loss 0.0135992, acc 1
2016-09-06T08:34:29.930073: step 18107, loss 0.00185104, acc 1
2016-09-06T08:34:30.741010: step 18108, loss 0.0013759, acc 1
2016-09-06T08:34:31.518119: step 18109, loss 0.0290693, acc 0.98
2016-09-06T08:34:32.327776: step 18110, loss 0.00233766, acc 1
2016-09-06T08:34:33.164350: step 18111, loss 0.00145604, acc 1
2016-09-06T08:34:33.972990: step 18112, loss 0.0256086, acc 1
2016-09-06T08:34:34.769337: step 18113, loss 0.00178529, acc 1
2016-09-06T08:34:35.572408: step 18114, loss 0.00654134, acc 1
2016-09-06T08:34:36.353776: step 18115, loss 0.00652237, acc 1
2016-09-06T08:34:37.154615: step 18116, loss 0.0128738, acc 1
2016-09-06T08:34:37.997504: step 18117, loss 0.0144569, acc 1
2016-09-06T08:34:38.797775: step 18118, loss 0.00161683, acc 1
2016-09-06T08:34:39.585617: step 18119, loss 0.0220217, acc 1
2016-09-06T08:34:40.442168: step 18120, loss 0.00246424, acc 1
2016-09-06T08:34:41.253004: step 18121, loss 0.0198286, acc 0.98
2016-09-06T08:34:42.063278: step 18122, loss 0.00140488, acc 1
2016-09-06T08:34:42.928228: step 18123, loss 0.0170926, acc 0.98
2016-09-06T08:34:43.744876: step 18124, loss 0.00542823, acc 1
2016-09-06T08:34:44.563076: step 18125, loss 0.00943438, acc 1
2016-09-06T08:34:45.408422: step 18126, loss 0.0135376, acc 1
2016-09-06T08:34:46.229413: step 18127, loss 0.0435425, acc 0.98
2016-09-06T08:34:47.044563: step 18128, loss 0.0232067, acc 0.98
2016-09-06T08:34:47.880138: step 18129, loss 0.00152904, acc 1
2016-09-06T08:34:48.692069: step 18130, loss 0.00160843, acc 1
2016-09-06T08:34:49.489891: step 18131, loss 0.00927236, acc 1
2016-09-06T08:34:50.351058: step 18132, loss 0.00670201, acc 1
2016-09-06T08:34:51.191345: step 18133, loss 0.0176946, acc 0.98
2016-09-06T08:34:52.009751: step 18134, loss 0.022816, acc 1
2016-09-06T08:34:52.810292: step 18135, loss 0.00209384, acc 1
2016-09-06T08:34:53.605856: step 18136, loss 0.0323953, acc 0.98
2016-09-06T08:34:54.428844: step 18137, loss 0.0171418, acc 0.98
2016-09-06T08:34:55.236229: step 18138, loss 0.00173922, acc 1
2016-09-06T08:34:56.063015: step 18139, loss 0.0018444, acc 1
2016-09-06T08:34:56.869080: step 18140, loss 0.00175263, acc 1
2016-09-06T08:34:57.666561: step 18141, loss 0.0185879, acc 0.98
2016-09-06T08:34:58.471246: step 18142, loss 0.00176915, acc 1
2016-09-06T08:34:59.302627: step 18143, loss 0.0109103, acc 1
2016-09-06T08:35:00.149603: step 18144, loss 0.00462959, acc 1
2016-09-06T08:35:00.987658: step 18145, loss 0.00193386, acc 1
2016-09-06T08:35:01.782208: step 18146, loss 0.0169192, acc 0.98
2016-09-06T08:35:02.587698: step 18147, loss 0.00188213, acc 1
2016-09-06T08:35:03.386057: step 18148, loss 0.0171922, acc 0.98
2016-09-06T08:35:04.175922: step 18149, loss 0.00268211, acc 1
2016-09-06T08:35:04.960368: step 18150, loss 0.00177737, acc 1
2016-09-06T08:35:05.776849: step 18151, loss 0.00856233, acc 1
2016-09-06T08:35:06.553686: step 18152, loss 0.00176684, acc 1
2016-09-06T08:35:07.343733: step 18153, loss 0.0188644, acc 1
2016-09-06T08:35:08.169544: step 18154, loss 0.00489524, acc 1
2016-09-06T08:35:08.969883: step 18155, loss 0.0421227, acc 0.98
2016-09-06T08:35:09.831271: step 18156, loss 0.0257593, acc 0.98
2016-09-06T08:35:10.684529: step 18157, loss 0.0451668, acc 0.98
2016-09-06T08:35:11.517417: step 18158, loss 0.0452698, acc 0.96
2016-09-06T08:35:12.319300: step 18159, loss 0.00196617, acc 1
2016-09-06T08:35:13.143727: step 18160, loss 0.0273448, acc 1
2016-09-06T08:35:13.970146: step 18161, loss 0.00822067, acc 1
2016-09-06T08:35:14.784071: step 18162, loss 0.00169459, acc 1
2016-09-06T08:35:15.620183: step 18163, loss 0.0302624, acc 1
2016-09-06T08:35:16.417981: step 18164, loss 0.0245734, acc 0.98
2016-09-06T08:35:17.225855: step 18165, loss 0.0139252, acc 1
2016-09-06T08:35:18.076355: step 18166, loss 0.00198649, acc 1
2016-09-06T08:35:18.892143: step 18167, loss 0.00162086, acc 1
2016-09-06T08:35:19.691734: step 18168, loss 0.00409239, acc 1
2016-09-06T08:35:20.530716: step 18169, loss 0.00165895, acc 1
2016-09-06T08:35:21.355362: step 18170, loss 0.00158812, acc 1
2016-09-06T08:35:22.158239: step 18171, loss 0.00173743, acc 1
2016-09-06T08:35:23.026535: step 18172, loss 0.0380238, acc 0.98
2016-09-06T08:35:23.834465: step 18173, loss 0.014711, acc 1
2016-09-06T08:35:24.655219: step 18174, loss 0.00154233, acc 1
2016-09-06T08:35:25.490219: step 18175, loss 0.0322758, acc 0.98
2016-09-06T08:35:26.300827: step 18176, loss 0.00862504, acc 1
2016-09-06T08:35:27.088886: step 18177, loss 0.0064049, acc 1
2016-09-06T08:35:27.923594: step 18178, loss 0.00143834, acc 1
2016-09-06T08:35:28.718793: step 18179, loss 0.00144097, acc 1
2016-09-06T08:35:29.506732: step 18180, loss 0.00932043, acc 1
2016-09-06T08:35:30.294127: step 18181, loss 0.00787903, acc 1
2016-09-06T08:35:31.113154: step 18182, loss 0.00802145, acc 1
2016-09-06T08:35:31.922740: step 18183, loss 0.0156605, acc 1
2016-09-06T08:35:32.731810: step 18184, loss 0.0016644, acc 1
2016-09-06T08:35:33.577929: step 18185, loss 0.00253722, acc 1
2016-09-06T08:35:34.378780: step 18186, loss 0.0349684, acc 0.98
2016-09-06T08:35:35.156051: step 18187, loss 0.00172894, acc 1
2016-09-06T08:35:36.008311: step 18188, loss 0.00137064, acc 1
2016-09-06T08:35:36.777204: step 18189, loss 0.0318485, acc 0.98
2016-09-06T08:35:37.611638: step 18190, loss 0.0169584, acc 0.98
2016-09-06T08:35:38.428791: step 18191, loss 0.00655999, acc 1
2016-09-06T08:35:39.248262: step 18192, loss 0.00665577, acc 1
2016-09-06T08:35:40.086139: step 18193, loss 0.012862, acc 1
2016-09-06T08:35:40.943746: step 18194, loss 0.0206429, acc 1
2016-09-06T08:35:41.758984: step 18195, loss 0.00152046, acc 1
2016-09-06T08:35:42.573363: step 18196, loss 0.00137624, acc 1
2016-09-06T08:35:43.392733: step 18197, loss 0.00300803, acc 1
2016-09-06T08:35:44.222294: step 18198, loss 0.0142329, acc 1
2016-09-06T08:35:45.068173: step 18199, loss 0.00168893, acc 1
2016-09-06T08:35:45.882875: step 18200, loss 0.00216738, acc 1

Evaluation:
2016-09-06T08:35:49.609803: step 18200, loss 3.02061, acc 0.721388

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-18200

2016-09-06T08:35:51.505647: step 18201, loss 0.0119228, acc 1
2016-09-06T08:35:52.299934: step 18202, loss 0.0014449, acc 1
2016-09-06T08:35:53.158521: step 18203, loss 0.001425, acc 1
2016-09-06T08:35:53.966019: step 18204, loss 0.0121721, acc 1
2016-09-06T08:35:54.782381: step 18205, loss 0.00224414, acc 1
2016-09-06T08:35:55.610433: step 18206, loss 0.0420679, acc 0.98
2016-09-06T08:35:56.432998: step 18207, loss 0.00143205, acc 1
2016-09-06T08:35:57.256975: step 18208, loss 0.111447, acc 0.94
2016-09-06T08:35:58.078912: step 18209, loss 0.0261079, acc 0.98
2016-09-06T08:35:58.876955: step 18210, loss 0.0150305, acc 1
2016-09-06T08:35:59.689809: step 18211, loss 0.00127256, acc 1
2016-09-06T08:36:00.524922: step 18212, loss 0.00135466, acc 1
2016-09-06T08:36:01.332992: step 18213, loss 0.00122568, acc 1
2016-09-06T08:36:02.146052: step 18214, loss 0.0202962, acc 0.98
2016-09-06T08:36:02.957541: step 18215, loss 0.00615505, acc 1
2016-09-06T08:36:03.755816: step 18216, loss 0.00138717, acc 1
2016-09-06T08:36:04.545122: step 18217, loss 0.0418922, acc 0.98
2016-09-06T08:36:05.344726: step 18218, loss 0.00129601, acc 1
2016-09-06T08:36:06.157397: step 18219, loss 0.0229399, acc 1
2016-09-06T08:36:06.949062: step 18220, loss 0.00717488, acc 1
2016-09-06T08:36:07.772025: step 18221, loss 0.00206636, acc 1
2016-09-06T08:36:08.585054: step 18222, loss 0.0314155, acc 0.98
2016-09-06T08:36:09.395235: step 18223, loss 0.105859, acc 0.96
2016-09-06T08:36:10.211484: step 18224, loss 0.00126172, acc 1
2016-09-06T08:36:11.031709: step 18225, loss 0.0195872, acc 1
2016-09-06T08:36:11.841306: step 18226, loss 0.0490717, acc 0.98
2016-09-06T08:36:12.670685: step 18227, loss 0.00303716, acc 1
2016-09-06T08:36:13.501698: step 18228, loss 0.0153143, acc 1
2016-09-06T08:36:14.317863: step 18229, loss 0.0194597, acc 0.98
2016-09-06T08:36:15.128369: step 18230, loss 0.0169576, acc 1
2016-09-06T08:36:15.980096: step 18231, loss 0.00122169, acc 1
2016-09-06T08:36:16.809198: step 18232, loss 0.00440294, acc 1
2016-09-06T08:36:17.639376: step 18233, loss 0.0102119, acc 1
2016-09-06T08:36:18.511161: step 18234, loss 0.0132259, acc 1
2016-09-06T08:36:19.348816: step 18235, loss 0.0208308, acc 1
2016-09-06T08:36:20.143539: step 18236, loss 0.00163666, acc 1
2016-09-06T08:36:20.993874: step 18237, loss 0.00604856, acc 1
2016-09-06T08:36:21.817564: step 18238, loss 0.0143259, acc 1
2016-09-06T08:36:22.638944: step 18239, loss 0.00220932, acc 1
2016-09-06T08:36:23.397126: step 18240, loss 0.0110912, acc 1
2016-09-06T08:36:24.220453: step 18241, loss 0.00736102, acc 1
2016-09-06T08:36:25.038569: step 18242, loss 0.0168674, acc 0.98
2016-09-06T08:36:25.868633: step 18243, loss 0.0018895, acc 1
2016-09-06T08:36:26.686212: step 18244, loss 0.00165236, acc 1
2016-09-06T08:36:27.513752: step 18245, loss 0.0126295, acc 1
2016-09-06T08:36:28.342537: step 18246, loss 0.00273865, acc 1
2016-09-06T08:36:29.195199: step 18247, loss 0.0161648, acc 0.98
2016-09-06T08:36:29.973463: step 18248, loss 0.00966066, acc 1
2016-09-06T08:36:30.815008: step 18249, loss 0.0179817, acc 1
2016-09-06T08:36:31.629014: step 18250, loss 0.0125784, acc 1
2016-09-06T08:36:32.434667: step 18251, loss 0.00499707, acc 1
2016-09-06T08:36:33.247448: step 18252, loss 0.00246448, acc 1
2016-09-06T08:36:34.081941: step 18253, loss 0.002759, acc 1
2016-09-06T08:36:34.885216: step 18254, loss 0.0190386, acc 1
2016-09-06T08:36:35.704640: step 18255, loss 0.0224682, acc 0.98
2016-09-06T08:36:36.553960: step 18256, loss 0.00187768, acc 1
2016-09-06T08:36:37.378772: step 18257, loss 0.0189512, acc 1
2016-09-06T08:36:38.202445: step 18258, loss 0.00287523, acc 1
2016-09-06T08:36:39.086165: step 18259, loss 0.00628523, acc 1
2016-09-06T08:36:39.899527: step 18260, loss 0.00192636, acc 1
2016-09-06T08:36:40.704304: step 18261, loss 0.00193795, acc 1
2016-09-06T08:36:41.576664: step 18262, loss 0.0301464, acc 0.98
2016-09-06T08:36:42.409110: step 18263, loss 0.00195516, acc 1
2016-09-06T08:36:43.252093: step 18264, loss 0.00285756, acc 1
2016-09-06T08:36:44.061165: step 18265, loss 0.00197352, acc 1
2016-09-06T08:36:44.905655: step 18266, loss 0.0153471, acc 1
2016-09-06T08:36:45.710794: step 18267, loss 0.0153133, acc 1
2016-09-06T08:36:46.666916: step 18268, loss 0.0158704, acc 1
2016-09-06T08:36:47.511542: step 18269, loss 0.0216666, acc 1
2016-09-06T08:36:48.317601: step 18270, loss 0.00447683, acc 1
2016-09-06T08:36:49.146776: step 18271, loss 0.00203663, acc 1
2016-09-06T08:36:49.960033: step 18272, loss 0.00206629, acc 1
2016-09-06T08:36:50.853501: step 18273, loss 0.0306579, acc 0.98
2016-09-06T08:36:51.679548: step 18274, loss 0.00445797, acc 1
2016-09-06T08:36:52.673989: step 18275, loss 0.00201968, acc 1
2016-09-06T08:36:53.516651: step 18276, loss 0.0070543, acc 1
2016-09-06T08:36:54.356227: step 18277, loss 0.00250358, acc 1
2016-09-06T08:36:55.196629: step 18278, loss 0.0135274, acc 1
2016-09-06T08:36:56.096009: step 18279, loss 0.0866482, acc 0.98
2016-09-06T08:36:57.005018: step 18280, loss 0.00193677, acc 1
2016-09-06T08:36:57.812474: step 18281, loss 0.00576341, acc 1
2016-09-06T08:36:58.704963: step 18282, loss 0.0024315, acc 1
2016-09-06T08:36:59.527515: step 18283, loss 0.0121752, acc 1
2016-09-06T08:37:00.368190: step 18284, loss 0.00244731, acc 1
2016-09-06T08:37:01.250040: step 18285, loss 0.00257429, acc 1
2016-09-06T08:37:02.086590: step 18286, loss 0.00200926, acc 1
2016-09-06T08:37:02.892180: step 18287, loss 0.00285095, acc 1
2016-09-06T08:37:03.725697: step 18288, loss 0.0108033, acc 1
2016-09-06T08:37:04.528619: step 18289, loss 0.0133444, acc 1
2016-09-06T08:37:05.325349: step 18290, loss 0.0019769, acc 1
2016-09-06T08:37:06.143284: step 18291, loss 0.00782926, acc 1
2016-09-06T08:37:07.008043: step 18292, loss 0.012302, acc 1
2016-09-06T08:37:07.911606: step 18293, loss 0.023485, acc 1
2016-09-06T08:37:08.770826: step 18294, loss 0.00186531, acc 1
2016-09-06T08:37:09.596450: step 18295, loss 0.018293, acc 0.98
2016-09-06T08:37:10.493063: step 18296, loss 0.021957, acc 0.98
2016-09-06T08:37:11.302408: step 18297, loss 0.00222023, acc 1
2016-09-06T08:37:12.182910: step 18298, loss 0.012096, acc 1
2016-09-06T08:37:13.016855: step 18299, loss 0.00300567, acc 1
2016-09-06T08:37:13.842487: step 18300, loss 0.00189231, acc 1

Evaluation:
2016-09-06T08:37:17.542552: step 18300, loss 3.37609, acc 0.724203

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-18300

2016-09-06T08:37:19.479246: step 18301, loss 0.0127228, acc 1
2016-09-06T08:37:20.322137: step 18302, loss 0.00199547, acc 1
2016-09-06T08:37:21.147820: step 18303, loss 0.00193359, acc 1
2016-09-06T08:37:21.981462: step 18304, loss 0.0275698, acc 0.98
2016-09-06T08:37:22.790028: step 18305, loss 0.0152323, acc 1
2016-09-06T08:37:23.595140: step 18306, loss 0.00202801, acc 1
2016-09-06T08:37:24.381211: step 18307, loss 0.00193347, acc 1
2016-09-06T08:37:25.219318: step 18308, loss 0.0211349, acc 1
2016-09-06T08:37:26.039522: step 18309, loss 0.0894814, acc 0.96
2016-09-06T08:37:26.845487: step 18310, loss 0.00191002, acc 1
2016-09-06T08:37:27.674017: step 18311, loss 0.00195581, acc 1
2016-09-06T08:37:28.508479: step 18312, loss 0.00361443, acc 1
2016-09-06T08:37:29.271489: step 18313, loss 0.00526328, acc 1
2016-09-06T08:37:30.112222: step 18314, loss 0.00181123, acc 1
2016-09-06T08:37:30.955965: step 18315, loss 0.0404948, acc 0.98
2016-09-06T08:37:31.764169: step 18316, loss 0.0162072, acc 1
2016-09-06T08:37:32.564179: step 18317, loss 0.00666487, acc 1
2016-09-06T08:37:33.409559: step 18318, loss 0.00342612, acc 1
2016-09-06T08:37:34.245958: step 18319, loss 0.00251779, acc 1
2016-09-06T08:37:35.058941: step 18320, loss 0.0104153, acc 1
2016-09-06T08:37:35.896305: step 18321, loss 0.0016993, acc 1
2016-09-06T08:37:36.700436: step 18322, loss 0.0123803, acc 1
2016-09-06T08:37:37.514187: step 18323, loss 0.0188674, acc 0.98
2016-09-06T08:37:38.372774: step 18324, loss 0.00165097, acc 1
2016-09-06T08:37:39.214776: step 18325, loss 0.00303481, acc 1
2016-09-06T08:37:40.023586: step 18326, loss 0.00499327, acc 1
2016-09-06T08:37:40.869971: step 18327, loss 0.00269679, acc 1
2016-09-06T08:37:41.701519: step 18328, loss 0.00373863, acc 1
2016-09-06T08:37:42.511529: step 18329, loss 0.00167617, acc 1
2016-09-06T08:37:43.337351: step 18330, loss 0.0101502, acc 1
2016-09-06T08:37:44.164486: step 18331, loss 0.0111784, acc 1
2016-09-06T08:37:44.971114: step 18332, loss 0.00163575, acc 1
2016-09-06T08:37:45.770725: step 18333, loss 0.0109799, acc 1
2016-09-06T08:37:46.586221: step 18334, loss 0.00164677, acc 1
2016-09-06T08:37:47.363663: step 18335, loss 0.00174611, acc 1
2016-09-06T08:37:48.189495: step 18336, loss 0.00163774, acc 1
2016-09-06T08:37:48.997944: step 18337, loss 0.00173557, acc 1
2016-09-06T08:37:49.795353: step 18338, loss 0.0066125, acc 1
2016-09-06T08:37:50.605268: step 18339, loss 0.00164238, acc 1
2016-09-06T08:37:51.427949: step 18340, loss 0.0160015, acc 1
2016-09-06T08:37:52.217381: step 18341, loss 0.0343968, acc 1
2016-09-06T08:37:53.039279: step 18342, loss 0.00163128, acc 1
2016-09-06T08:37:53.909761: step 18343, loss 0.00163679, acc 1
2016-09-06T08:37:54.673748: step 18344, loss 0.00166626, acc 1
2016-09-06T08:37:55.482078: step 18345, loss 0.0151455, acc 1
2016-09-06T08:37:56.305436: step 18346, loss 0.004062, acc 1
2016-09-06T08:37:57.118368: step 18347, loss 0.0369547, acc 0.98
2016-09-06T08:37:57.922397: step 18348, loss 0.014493, acc 1
2016-09-06T08:37:58.716753: step 18349, loss 0.0128554, acc 1
2016-09-06T08:37:59.497225: step 18350, loss 0.0151048, acc 1
2016-09-06T08:38:00.309910: step 18351, loss 0.00231113, acc 1
2016-09-06T08:38:01.152169: step 18352, loss 0.107906, acc 0.96
2016-09-06T08:38:01.968267: step 18353, loss 0.00379288, acc 1
2016-09-06T08:38:02.794502: step 18354, loss 0.00376343, acc 1
2016-09-06T08:38:03.619020: step 18355, loss 0.0345752, acc 0.98
2016-09-06T08:38:04.438240: step 18356, loss 0.00161054, acc 1
2016-09-06T08:38:05.300548: step 18357, loss 0.0341251, acc 0.98
2016-09-06T08:38:06.130094: step 18358, loss 0.00223724, acc 1
2016-09-06T08:38:06.949444: step 18359, loss 0.00261504, acc 1
2016-09-06T08:38:07.789404: step 18360, loss 0.0549382, acc 0.98
2016-09-06T08:38:08.613045: step 18361, loss 0.00185294, acc 1
2016-09-06T08:38:09.413960: step 18362, loss 0.00170738, acc 1
2016-09-06T08:38:10.230387: step 18363, loss 0.00165071, acc 1
2016-09-06T08:38:11.056014: step 18364, loss 0.0025743, acc 1
2016-09-06T08:38:11.877626: step 18365, loss 0.0378749, acc 0.98
2016-09-06T08:38:12.701697: step 18366, loss 0.0246707, acc 0.98
2016-09-06T08:38:13.554704: step 18367, loss 0.0180387, acc 1
2016-09-06T08:38:14.395046: step 18368, loss 0.00196861, acc 1
2016-09-06T08:38:15.203330: step 18369, loss 0.0174547, acc 0.98
2016-09-06T08:38:16.012083: step 18370, loss 0.00375703, acc 1
2016-09-06T08:38:16.854890: step 18371, loss 0.00188756, acc 1
2016-09-06T08:38:17.649950: step 18372, loss 0.0747749, acc 0.98
2016-09-06T08:38:18.472317: step 18373, loss 0.0129805, acc 1
2016-09-06T08:38:19.328221: step 18374, loss 0.0280266, acc 0.98
2016-09-06T08:38:20.113498: step 18375, loss 0.0102496, acc 1
2016-09-06T08:38:20.923046: step 18376, loss 0.0020052, acc 1
2016-09-06T08:38:21.761410: step 18377, loss 0.00207079, acc 1
2016-09-06T08:38:22.601475: step 18378, loss 0.0140229, acc 1
2016-09-06T08:38:23.391946: step 18379, loss 0.00222952, acc 1
2016-09-06T08:38:24.244436: step 18380, loss 0.00257038, acc 1
2016-09-06T08:38:25.117456: step 18381, loss 0.0522914, acc 0.98
2016-09-06T08:38:25.970571: step 18382, loss 0.00326487, acc 1
2016-09-06T08:38:26.805323: step 18383, loss 0.00450732, acc 1
2016-09-06T08:38:27.623363: step 18384, loss 0.0265562, acc 1
2016-09-06T08:38:28.433393: step 18385, loss 0.0174999, acc 1
2016-09-06T08:38:29.276738: step 18386, loss 0.0179079, acc 1
2016-09-06T08:38:30.082555: step 18387, loss 0.00257531, acc 1
2016-09-06T08:38:30.866642: step 18388, loss 0.0227346, acc 0.98
2016-09-06T08:38:31.711923: step 18389, loss 0.0224566, acc 0.98
2016-09-06T08:38:32.510775: step 18390, loss 0.0140444, acc 1
2016-09-06T08:38:33.316174: step 18391, loss 0.00196945, acc 1
2016-09-06T08:38:34.138592: step 18392, loss 0.00203145, acc 1
2016-09-06T08:38:34.953814: step 18393, loss 0.0148353, acc 1
2016-09-06T08:38:35.759566: step 18394, loss 0.00168225, acc 1
2016-09-06T08:38:36.568530: step 18395, loss 0.00249076, acc 1
2016-09-06T08:38:37.396827: step 18396, loss 0.0377925, acc 0.96
2016-09-06T08:38:38.179805: step 18397, loss 0.00779992, acc 1
2016-09-06T08:38:38.974196: step 18398, loss 0.125518, acc 0.94
2016-09-06T08:38:39.797247: step 18399, loss 0.00161151, acc 1
2016-09-06T08:38:40.612901: step 18400, loss 0.0533661, acc 0.96

Evaluation:
2016-09-06T08:38:44.315399: step 18400, loss 2.28236, acc 0.722326

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-18400

2016-09-06T08:38:46.225195: step 18401, loss 0.0401738, acc 0.98
2016-09-06T08:38:47.032255: step 18402, loss 0.00846182, acc 1
2016-09-06T08:38:47.828995: step 18403, loss 0.00828662, acc 1
2016-09-06T08:38:48.667437: step 18404, loss 0.00177971, acc 1
2016-09-06T08:38:49.478046: step 18405, loss 0.00205454, acc 1
2016-09-06T08:38:50.278108: step 18406, loss 0.028815, acc 0.98
2016-09-06T08:38:51.116455: step 18407, loss 0.00175291, acc 1
2016-09-06T08:38:51.913062: step 18408, loss 0.00526997, acc 1
2016-09-06T08:38:52.718574: step 18409, loss 0.0376431, acc 0.96
2016-09-06T08:38:53.545648: step 18410, loss 0.00883119, acc 1
2016-09-06T08:38:54.367001: step 18411, loss 0.0364742, acc 0.98
2016-09-06T08:38:55.178421: step 18412, loss 0.0261521, acc 0.98
2016-09-06T08:38:56.007632: step 18413, loss 0.00228707, acc 1
2016-09-06T08:38:56.843723: step 18414, loss 0.0306767, acc 0.98
2016-09-06T08:38:57.665314: step 18415, loss 0.0337022, acc 0.98
2016-09-06T08:38:58.449250: step 18416, loss 0.00174728, acc 1
2016-09-06T08:38:59.269335: step 18417, loss 0.0405102, acc 0.96
2016-09-06T08:39:00.060859: step 18418, loss 0.0272912, acc 1
2016-09-06T08:39:00.907100: step 18419, loss 0.00434895, acc 1
2016-09-06T08:39:01.721786: step 18420, loss 0.0386161, acc 0.96
2016-09-06T08:39:02.497683: step 18421, loss 0.00210135, acc 1
2016-09-06T08:39:03.286004: step 18422, loss 0.0534681, acc 0.94
2016-09-06T08:39:04.105471: step 18423, loss 0.0022604, acc 1
2016-09-06T08:39:04.917437: step 18424, loss 0.00189156, acc 1
2016-09-06T08:39:05.735202: step 18425, loss 0.00191587, acc 1
2016-09-06T08:39:06.558551: step 18426, loss 0.00576876, acc 1
2016-09-06T08:39:07.366292: step 18427, loss 0.0282958, acc 0.98
2016-09-06T08:39:08.166923: step 18428, loss 0.0176014, acc 0.98
2016-09-06T08:39:08.977236: step 18429, loss 0.03207, acc 0.98
2016-09-06T08:39:09.767165: step 18430, loss 0.0248375, acc 0.98
2016-09-06T08:39:10.587745: step 18431, loss 0.00225065, acc 1
2016-09-06T08:39:11.361318: step 18432, loss 0.00214936, acc 1
2016-09-06T08:39:12.140992: step 18433, loss 0.0198307, acc 0.98
2016-09-06T08:39:12.976566: step 18434, loss 0.0234416, acc 0.98
2016-09-06T08:39:13.823853: step 18435, loss 0.0136589, acc 1
2016-09-06T08:39:14.613611: step 18436, loss 0.0208133, acc 0.98
2016-09-06T08:39:15.427044: step 18437, loss 0.0164971, acc 1
2016-09-06T08:39:16.244234: step 18438, loss 0.018242, acc 1
2016-09-06T08:39:17.044349: step 18439, loss 0.00218377, acc 1
2016-09-06T08:39:17.857089: step 18440, loss 0.00487525, acc 1
2016-09-06T08:39:18.728864: step 18441, loss 0.0225152, acc 0.98
2016-09-06T08:39:19.561068: step 18442, loss 0.00771231, acc 1
2016-09-06T08:39:20.369234: step 18443, loss 0.00214222, acc 1
2016-09-06T08:39:21.240363: step 18444, loss 0.00354808, acc 1
2016-09-06T08:39:22.057959: step 18445, loss 0.00368749, acc 1
2016-09-06T08:39:22.890903: step 18446, loss 0.0883454, acc 0.98
2016-09-06T08:39:23.738370: step 18447, loss 0.0241931, acc 0.98
2016-09-06T08:39:24.567841: step 18448, loss 0.00202514, acc 1
2016-09-06T08:39:25.362330: step 18449, loss 0.00200283, acc 1
2016-09-06T08:39:26.192034: step 18450, loss 0.00236144, acc 1
2016-09-06T08:39:27.009487: step 18451, loss 0.0115705, acc 1
2016-09-06T08:39:27.795056: step 18452, loss 0.00191464, acc 1
2016-09-06T08:39:28.608404: step 18453, loss 0.00191128, acc 1
2016-09-06T08:39:29.409232: step 18454, loss 0.0264151, acc 1
2016-09-06T08:39:30.253463: step 18455, loss 0.0127977, acc 1
2016-09-06T08:39:31.062394: step 18456, loss 0.0480203, acc 0.98
2016-09-06T08:39:31.862257: step 18457, loss 0.00186386, acc 1
2016-09-06T08:39:32.650261: step 18458, loss 0.0138799, acc 1
2016-09-06T08:39:33.475897: step 18459, loss 0.0124617, acc 1
2016-09-06T08:39:34.299721: step 18460, loss 0.00242772, acc 1
2016-09-06T08:39:35.079758: step 18461, loss 0.0106747, acc 1
2016-09-06T08:39:35.882977: step 18462, loss 0.0091828, acc 1
2016-09-06T08:39:36.717080: step 18463, loss 0.011116, acc 1
2016-09-06T08:39:37.520336: step 18464, loss 0.0179656, acc 0.98
2016-09-06T08:39:38.313508: step 18465, loss 0.00624594, acc 1
2016-09-06T08:39:39.124217: step 18466, loss 0.0161894, acc 0.98
2016-09-06T08:39:39.919500: step 18467, loss 0.00201759, acc 1
2016-09-06T08:39:40.734331: step 18468, loss 0.00325882, acc 1
2016-09-06T08:39:41.569592: step 18469, loss 0.00316658, acc 1
2016-09-06T08:39:42.357162: step 18470, loss 0.0149305, acc 1
2016-09-06T08:39:43.208375: step 18471, loss 0.0189448, acc 0.98
2016-09-06T08:39:44.010946: step 18472, loss 0.00196943, acc 1
2016-09-06T08:39:44.809068: step 18473, loss 0.0108991, acc 1
2016-09-06T08:39:45.590329: step 18474, loss 0.00465732, acc 1
2016-09-06T08:39:46.403548: step 18475, loss 0.012186, acc 1
2016-09-06T08:39:47.198900: step 18476, loss 0.0018682, acc 1
2016-09-06T08:39:48.006113: step 18477, loss 0.00202155, acc 1
2016-09-06T08:39:48.821449: step 18478, loss 0.00323199, acc 1
2016-09-06T08:39:49.621389: step 18479, loss 0.00236651, acc 1
2016-09-06T08:39:50.448942: step 18480, loss 0.00239852, acc 1
2016-09-06T08:39:51.273600: step 18481, loss 0.0343272, acc 0.98
2016-09-06T08:39:52.066473: step 18482, loss 0.00187757, acc 1
2016-09-06T08:39:52.880753: step 18483, loss 0.00219489, acc 1
2016-09-06T08:39:53.706660: step 18484, loss 0.0242706, acc 0.98
2016-09-06T08:39:54.494432: step 18485, loss 0.0146794, acc 1
2016-09-06T08:39:55.295121: step 18486, loss 0.00185157, acc 1
2016-09-06T08:39:56.119622: step 18487, loss 0.0109408, acc 1
2016-09-06T08:39:56.934962: step 18488, loss 0.00211008, acc 1
2016-09-06T08:39:57.736231: step 18489, loss 0.00186298, acc 1
2016-09-06T08:39:58.572608: step 18490, loss 0.00219633, acc 1
2016-09-06T08:39:59.382749: step 18491, loss 0.00529939, acc 1
2016-09-06T08:40:00.208269: step 18492, loss 0.00555282, acc 1
2016-09-06T08:40:01.197638: step 18493, loss 0.0264461, acc 0.98
2016-09-06T08:40:02.032495: step 18494, loss 0.00559912, acc 1
2016-09-06T08:40:02.853594: step 18495, loss 0.00199544, acc 1
2016-09-06T08:40:03.661691: step 18496, loss 0.00249457, acc 1
2016-09-06T08:40:04.469691: step 18497, loss 0.0143262, acc 1
2016-09-06T08:40:05.273918: step 18498, loss 0.0278226, acc 0.98
2016-09-06T08:40:06.113139: step 18499, loss 0.0162868, acc 0.98
2016-09-06T08:40:06.976519: step 18500, loss 0.0264297, acc 0.98

Evaluation:
2016-09-06T08:40:10.683936: step 18500, loss 3.97839, acc 0.709193

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-18500

2016-09-06T08:40:12.665539: step 18501, loss 0.013176, acc 1
2016-09-06T08:40:13.485196: step 18502, loss 0.00187386, acc 1
2016-09-06T08:40:14.311562: step 18503, loss 0.00198349, acc 1
2016-09-06T08:40:15.156006: step 18504, loss 0.0300903, acc 0.98
2016-09-06T08:40:15.959165: step 18505, loss 0.00293536, acc 1
2016-09-06T08:40:16.778126: step 18506, loss 0.00278166, acc 1
2016-09-06T08:40:17.678802: step 18507, loss 0.0262787, acc 0.98
2016-09-06T08:40:18.541810: step 18508, loss 0.0124523, acc 1
2016-09-06T08:40:19.402261: step 18509, loss 0.00242025, acc 1
2016-09-06T08:40:20.352072: step 18510, loss 0.0692526, acc 0.98
2016-09-06T08:40:21.198811: step 18511, loss 0.0033726, acc 1
2016-09-06T08:40:22.067933: step 18512, loss 0.00258729, acc 1
2016-09-06T08:40:22.938938: step 18513, loss 0.00188869, acc 1
2016-09-06T08:40:23.785357: step 18514, loss 0.00350823, acc 1
2016-09-06T08:40:24.583539: step 18515, loss 0.0186563, acc 1
2016-09-06T08:40:25.427410: step 18516, loss 0.00390895, acc 1
2016-09-06T08:40:26.236200: step 18517, loss 0.00215652, acc 1
2016-09-06T08:40:27.046783: step 18518, loss 0.013396, acc 1
2016-09-06T08:40:27.897647: step 18519, loss 0.00551126, acc 1
2016-09-06T08:40:28.740053: step 18520, loss 0.00824085, acc 1
2016-09-06T08:40:29.509968: step 18521, loss 0.014215, acc 1
2016-09-06T08:40:30.383891: step 18522, loss 0.00192589, acc 1
2016-09-06T08:40:31.205390: step 18523, loss 0.0176795, acc 0.98
2016-09-06T08:40:32.082545: step 18524, loss 0.00197699, acc 1
2016-09-06T08:40:32.898284: step 18525, loss 0.0055205, acc 1
2016-09-06T08:40:33.794522: step 18526, loss 0.00202663, acc 1
2016-09-06T08:40:34.602096: step 18527, loss 0.0204628, acc 1
2016-09-06T08:40:35.465365: step 18528, loss 0.002058, acc 1
2016-09-06T08:40:36.309967: step 18529, loss 0.00207255, acc 1
2016-09-06T08:40:37.139551: step 18530, loss 0.00208989, acc 1
2016-09-06T08:40:37.944557: step 18531, loss 0.0188509, acc 0.98
2016-09-06T08:40:38.751610: step 18532, loss 0.00209915, acc 1
2016-09-06T08:40:39.624997: step 18533, loss 0.0209439, acc 1
2016-09-06T08:40:40.424594: step 18534, loss 0.00343408, acc 1
2016-09-06T08:40:41.343195: step 18535, loss 0.0021005, acc 1
2016-09-06T08:40:42.233848: step 18536, loss 0.00211971, acc 1
2016-09-06T08:40:43.070228: step 18537, loss 0.0154004, acc 1
2016-09-06T08:40:43.895825: step 18538, loss 0.00208972, acc 1
2016-09-06T08:40:44.728570: step 18539, loss 0.0167187, acc 1
2016-09-06T08:40:45.766260: step 18540, loss 0.0229455, acc 0.98
2016-09-06T08:40:46.594400: step 18541, loss 0.00381513, acc 1
2016-09-06T08:40:47.671594: step 18542, loss 0.0208747, acc 1
2016-09-06T08:40:48.495823: step 18543, loss 0.00205901, acc 1
2016-09-06T08:40:49.479270: step 18544, loss 0.0603499, acc 0.96
2016-09-06T08:40:50.322514: step 18545, loss 0.00611998, acc 1
2016-09-06T08:40:51.202352: step 18546, loss 0.0023337, acc 1
2016-09-06T08:40:52.058753: step 18547, loss 0.0909322, acc 0.96
2016-09-06T08:40:52.915256: step 18548, loss 0.017562, acc 0.98
2016-09-06T08:40:53.711607: step 18549, loss 0.0181043, acc 1
2016-09-06T08:40:54.555241: step 18550, loss 0.00188229, acc 1
2016-09-06T08:40:55.378980: step 18551, loss 0.0142004, acc 1
2016-09-06T08:40:56.223586: step 18552, loss 0.0317406, acc 0.98
2016-09-06T08:40:57.042193: step 18553, loss 0.0022636, acc 1
2016-09-06T08:40:57.942037: step 18554, loss 0.00177542, acc 1
2016-09-06T08:40:58.744076: step 18555, loss 0.0471839, acc 0.98
2016-09-06T08:40:59.555042: step 18556, loss 0.0428268, acc 0.96
2016-09-06T08:41:00.389998: step 18557, loss 0.0273316, acc 0.98
2016-09-06T08:41:01.206782: step 18558, loss 0.00168974, acc 1
2016-09-06T08:41:02.016492: step 18559, loss 0.00598046, acc 1
2016-09-06T08:41:02.852515: step 18560, loss 0.0189897, acc 1
2016-09-06T08:41:03.657432: step 18561, loss 0.00237932, acc 1
2016-09-06T08:41:04.500659: step 18562, loss 0.00180648, acc 1
2016-09-06T08:41:05.337482: step 18563, loss 0.0511241, acc 0.96
2016-09-06T08:41:06.156570: step 18564, loss 0.00186301, acc 1
2016-09-06T08:41:06.961623: step 18565, loss 0.0844781, acc 0.98
2016-09-06T08:41:07.793335: step 18566, loss 0.00216653, acc 1
2016-09-06T08:41:08.604585: step 18567, loss 0.00208368, acc 1
2016-09-06T08:41:09.389431: step 18568, loss 0.00920129, acc 1
2016-09-06T08:41:10.221162: step 18569, loss 0.0122445, acc 1
2016-09-06T08:41:11.063613: step 18570, loss 0.0220756, acc 0.98
2016-09-06T08:41:11.862915: step 18571, loss 0.00235579, acc 1
2016-09-06T08:41:12.674713: step 18572, loss 0.00267962, acc 1
2016-09-06T08:41:13.585312: step 18573, loss 0.0135706, acc 1
2016-09-06T08:41:14.545742: step 18574, loss 0.00555838, acc 1
2016-09-06T08:41:15.419356: step 18575, loss 0.00476041, acc 1
2016-09-06T08:41:16.379910: step 18576, loss 0.0231341, acc 0.98
2016-09-06T08:41:17.225408: step 18577, loss 0.00641116, acc 1
2016-09-06T08:41:18.087471: step 18578, loss 0.0201524, acc 1
2016-09-06T08:41:18.920719: step 18579, loss 0.0471621, acc 0.96
2016-09-06T08:41:19.928853: step 18580, loss 0.0229097, acc 0.98
2016-09-06T08:41:20.880786: step 18581, loss 0.0308037, acc 0.98
2016-09-06T08:41:21.715800: step 18582, loss 0.00369976, acc 1
2016-09-06T08:41:22.584926: step 18583, loss 0.00409943, acc 1
2016-09-06T08:41:23.606233: step 18584, loss 0.00370524, acc 1
2016-09-06T08:41:24.528850: step 18585, loss 0.0147495, acc 1
2016-09-06T08:41:25.449668: step 18586, loss 0.00392935, acc 1
2016-09-06T08:41:26.594881: step 18587, loss 0.00465663, acc 1
2016-09-06T08:41:27.425246: step 18588, loss 0.0696731, acc 0.96
2016-09-06T08:41:28.393146: step 18589, loss 0.00500112, acc 1
2016-09-06T08:41:29.265090: step 18590, loss 0.0361486, acc 0.98
2016-09-06T08:41:30.109801: step 18591, loss 0.0102661, acc 1
2016-09-06T08:41:30.919639: step 18592, loss 0.021063, acc 0.98
2016-09-06T08:41:31.781785: step 18593, loss 0.0173387, acc 1
2016-09-06T08:41:32.634429: step 18594, loss 0.00817712, acc 1
2016-09-06T08:41:33.463485: step 18595, loss 0.007089, acc 1
2016-09-06T08:41:34.334862: step 18596, loss 0.0155855, acc 1
2016-09-06T08:41:35.335616: step 18597, loss 0.00374035, acc 1
2016-09-06T08:41:36.163099: step 18598, loss 0.0388718, acc 0.98
2016-09-06T08:41:36.991105: step 18599, loss 0.0181389, acc 1
2016-09-06T08:41:37.846675: step 18600, loss 0.00419076, acc 1

Evaluation:
2016-09-06T08:41:41.531056: step 18600, loss 3.79744, acc 0.72045

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-18600

2016-09-06T08:41:43.459127: step 18601, loss 0.0499125, acc 0.94
2016-09-06T08:41:44.305279: step 18602, loss 0.0034228, acc 1
2016-09-06T08:41:45.144815: step 18603, loss 0.0298353, acc 0.98
2016-09-06T08:41:45.979484: step 18604, loss 0.0263565, acc 0.98
2016-09-06T08:41:46.816619: step 18605, loss 0.117814, acc 0.98
2016-09-06T08:41:47.792910: step 18606, loss 0.0201034, acc 0.98
2016-09-06T08:41:48.625265: step 18607, loss 0.0741097, acc 0.98
2016-09-06T08:41:49.482577: step 18608, loss 0.00303706, acc 1
2016-09-06T08:41:50.319558: step 18609, loss 0.0644429, acc 0.98
2016-09-06T08:41:51.159804: step 18610, loss 0.00375264, acc 1
2016-09-06T08:41:51.971126: step 18611, loss 0.0090746, acc 1
2016-09-06T08:41:52.777283: step 18612, loss 0.0250211, acc 1
2016-09-06T08:41:53.640452: step 18613, loss 0.00470994, acc 1
2016-09-06T08:41:54.482730: step 18614, loss 0.00383606, acc 1
2016-09-06T08:41:55.287979: step 18615, loss 0.00649041, acc 1
2016-09-06T08:41:56.115899: step 18616, loss 0.013412, acc 1
2016-09-06T08:41:56.943690: step 18617, loss 0.0248495, acc 1
2016-09-06T08:41:57.757729: step 18618, loss 0.0377317, acc 0.96
2016-09-06T08:41:58.608329: step 18619, loss 0.0427777, acc 0.96
2016-09-06T08:41:59.411034: step 18620, loss 0.00258262, acc 1
2016-09-06T08:42:00.229526: step 18621, loss 0.0197731, acc 1
2016-09-06T08:42:01.051047: step 18622, loss 0.00394535, acc 1
2016-09-06T08:42:01.854667: step 18623, loss 0.00340491, acc 1
2016-09-06T08:42:02.617396: step 18624, loss 0.00268443, acc 1
2016-09-06T08:42:03.442823: step 18625, loss 0.0373613, acc 0.98
2016-09-06T08:42:04.269139: step 18626, loss 0.0239566, acc 1
2016-09-06T08:42:05.081535: step 18627, loss 0.00280551, acc 1
2016-09-06T08:42:05.920367: step 18628, loss 0.0032657, acc 1
2016-09-06T08:42:06.766027: step 18629, loss 0.0188969, acc 0.98
2016-09-06T08:42:07.556656: step 18630, loss 0.0420905, acc 0.98
2016-09-06T08:42:08.402278: step 18631, loss 0.00857778, acc 1
2016-09-06T08:42:09.219983: step 18632, loss 0.0231472, acc 0.98
2016-09-06T08:42:10.056396: step 18633, loss 0.0630802, acc 0.98
2016-09-06T08:42:10.865937: step 18634, loss 0.00927864, acc 1
2016-09-06T08:42:11.701699: step 18635, loss 0.00289613, acc 1
2016-09-06T08:42:12.473794: step 18636, loss 0.00446719, acc 1
2016-09-06T08:42:13.281036: step 18637, loss 0.0135566, acc 1
2016-09-06T08:42:14.118251: step 18638, loss 0.0030417, acc 1
2016-09-06T08:42:14.914994: step 18639, loss 0.0218262, acc 1
2016-09-06T08:42:15.751649: step 18640, loss 0.00336436, acc 1
2016-09-06T08:42:16.742538: step 18641, loss 0.00510008, acc 1
2016-09-06T08:42:17.544012: step 18642, loss 0.00315986, acc 1
2016-09-06T08:42:18.411194: step 18643, loss 0.00317772, acc 1
2016-09-06T08:42:19.294410: step 18644, loss 0.00314955, acc 1
2016-09-06T08:42:20.138042: step 18645, loss 0.00377935, acc 1
2016-09-06T08:42:20.976353: step 18646, loss 0.00723905, acc 1
2016-09-06T08:42:21.843255: step 18647, loss 0.0183468, acc 0.98
2016-09-06T08:42:22.712899: step 18648, loss 0.0280706, acc 0.98
2016-09-06T08:42:23.509104: step 18649, loss 0.0095295, acc 1
2016-09-06T08:42:24.324934: step 18650, loss 0.00847193, acc 1
2016-09-06T08:42:25.197558: step 18651, loss 0.00321144, acc 1
2016-09-06T08:42:26.017596: step 18652, loss 0.00323578, acc 1
2016-09-06T08:42:26.822985: step 18653, loss 0.0180646, acc 1
2016-09-06T08:42:27.731279: step 18654, loss 0.00311124, acc 1
2016-09-06T08:42:28.619162: step 18655, loss 0.00776015, acc 1
2016-09-06T08:42:29.453815: step 18656, loss 0.00494106, acc 1
2016-09-06T08:42:30.270742: step 18657, loss 0.0105654, acc 1
2016-09-06T08:42:31.098676: step 18658, loss 0.00556164, acc 1
2016-09-06T08:42:31.954646: step 18659, loss 0.0329219, acc 0.98
2016-09-06T08:42:32.760193: step 18660, loss 0.0650247, acc 0.96
2016-09-06T08:42:33.579100: step 18661, loss 0.00297112, acc 1
2016-09-06T08:42:34.388804: step 18662, loss 0.0307536, acc 0.98
2016-09-06T08:42:35.207433: step 18663, loss 0.0156621, acc 1
2016-09-06T08:42:36.081065: step 18664, loss 0.00285243, acc 1
2016-09-06T08:42:36.910120: step 18665, loss 0.0192357, acc 0.98
2016-09-06T08:42:37.746630: step 18666, loss 0.0341506, acc 0.98
2016-09-06T08:42:38.623582: step 18667, loss 0.0169598, acc 1
2016-09-06T08:42:39.463130: step 18668, loss 0.00281524, acc 1
2016-09-06T08:42:40.267738: step 18669, loss 0.00341003, acc 1
2016-09-06T08:42:41.075180: step 18670, loss 0.012241, acc 1
2016-09-06T08:42:41.933973: step 18671, loss 0.0135383, acc 1
2016-09-06T08:42:42.741281: step 18672, loss 0.00435686, acc 1
2016-09-06T08:42:43.557839: step 18673, loss 0.0392154, acc 0.96
2016-09-06T08:42:44.384056: step 18674, loss 0.00275966, acc 1
2016-09-06T08:42:45.176266: step 18675, loss 0.0229976, acc 0.98
2016-09-06T08:42:46.001120: step 18676, loss 0.018893, acc 1
2016-09-06T08:42:46.847591: step 18677, loss 0.0258119, acc 1
2016-09-06T08:42:47.673995: step 18678, loss 0.0176011, acc 1
2016-09-06T08:42:48.509628: step 18679, loss 0.00239937, acc 1
2016-09-06T08:42:49.402452: step 18680, loss 0.0170607, acc 0.98
2016-09-06T08:42:50.219534: step 18681, loss 0.00709804, acc 1
2016-09-06T08:42:51.032730: step 18682, loss 0.0032953, acc 1
2016-09-06T08:42:51.869421: step 18683, loss 0.00269783, acc 1
2016-09-06T08:42:52.688267: step 18684, loss 0.00401112, acc 1
2016-09-06T08:42:53.500254: step 18685, loss 0.0472329, acc 1
2016-09-06T08:42:54.333079: step 18686, loss 0.00228759, acc 1
2016-09-06T08:42:55.158527: step 18687, loss 0.00227737, acc 1
2016-09-06T08:42:55.950206: step 18688, loss 0.030968, acc 0.98
2016-09-06T08:42:56.761942: step 18689, loss 0.00235065, acc 1
2016-09-06T08:42:57.566717: step 18690, loss 0.0525581, acc 0.98
2016-09-06T08:42:58.350636: step 18691, loss 0.00246415, acc 1
2016-09-06T08:42:59.174131: step 18692, loss 0.0309793, acc 1
2016-09-06T08:42:59.983628: step 18693, loss 0.0261742, acc 0.98
2016-09-06T08:43:00.793660: step 18694, loss 0.0163898, acc 0.98
2016-09-06T08:43:01.594173: step 18695, loss 0.138991, acc 0.98
2016-09-06T08:43:02.419689: step 18696, loss 0.0134971, acc 1
2016-09-06T08:43:03.245399: step 18697, loss 0.0290804, acc 0.98
2016-09-06T08:43:04.070771: step 18698, loss 0.00398564, acc 1
2016-09-06T08:43:04.881822: step 18699, loss 0.0373538, acc 0.98
2016-09-06T08:43:05.635787: step 18700, loss 0.0168852, acc 1

Evaluation:
2016-09-06T08:43:09.383978: step 18700, loss 2.5713, acc 0.722326

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-18700

2016-09-06T08:43:11.162550: step 18701, loss 0.00214999, acc 1
2016-09-06T08:43:11.957447: step 18702, loss 0.006181, acc 1
2016-09-06T08:43:12.773941: step 18703, loss 0.00721874, acc 1
2016-09-06T08:43:13.593602: step 18704, loss 0.00240171, acc 1
2016-09-06T08:43:14.421820: step 18705, loss 0.0174865, acc 1
2016-09-06T08:43:15.243393: step 18706, loss 0.00297797, acc 1
2016-09-06T08:43:16.086162: step 18707, loss 0.114982, acc 0.94
2016-09-06T08:43:16.899504: step 18708, loss 0.00215247, acc 1
2016-09-06T08:43:17.713045: step 18709, loss 0.0161139, acc 1
2016-09-06T08:43:18.531753: step 18710, loss 0.0195718, acc 1
2016-09-06T08:43:19.358397: step 18711, loss 0.00427378, acc 1
2016-09-06T08:43:20.173181: step 18712, loss 0.0116065, acc 1
2016-09-06T08:43:21.003780: step 18713, loss 0.00699364, acc 1
2016-09-06T08:43:21.828554: step 18714, loss 0.00238649, acc 1
2016-09-06T08:43:22.619916: step 18715, loss 0.0396033, acc 0.98
2016-09-06T08:43:23.451954: step 18716, loss 0.00248047, acc 1
2016-09-06T08:43:24.297770: step 18717, loss 0.00302505, acc 1
2016-09-06T08:43:25.093359: step 18718, loss 0.0151275, acc 1
2016-09-06T08:43:25.887434: step 18719, loss 0.0184887, acc 0.98
2016-09-06T08:43:26.698324: step 18720, loss 0.00245025, acc 1
2016-09-06T08:43:27.471815: step 18721, loss 0.00252973, acc 1
2016-09-06T08:43:28.332012: step 18722, loss 0.013551, acc 1
2016-09-06T08:43:29.181312: step 18723, loss 0.00248782, acc 1
2016-09-06T08:43:29.970495: step 18724, loss 0.00519797, acc 1
2016-09-06T08:43:30.776088: step 18725, loss 0.00278797, acc 1
2016-09-06T08:43:31.586750: step 18726, loss 0.0121203, acc 1
2016-09-06T08:43:32.395659: step 18727, loss 0.0226139, acc 0.98
2016-09-06T08:43:33.201825: step 18728, loss 0.00294413, acc 1
2016-09-06T08:43:34.014876: step 18729, loss 0.00276211, acc 1
2016-09-06T08:43:34.812370: step 18730, loss 0.00249133, acc 1
2016-09-06T08:43:35.627249: step 18731, loss 0.0204083, acc 0.98
2016-09-06T08:43:36.464702: step 18732, loss 0.0727481, acc 0.98
2016-09-06T08:43:37.291276: step 18733, loss 0.0288854, acc 1
2016-09-06T08:43:38.107723: step 18734, loss 0.00553442, acc 1
2016-09-06T08:43:38.938241: step 18735, loss 0.00244447, acc 1
2016-09-06T08:43:39.752841: step 18736, loss 0.0190391, acc 0.98
2016-09-06T08:43:40.563563: step 18737, loss 0.00283655, acc 1
2016-09-06T08:43:41.391260: step 18738, loss 0.00232525, acc 1
2016-09-06T08:43:42.218797: step 18739, loss 0.0364094, acc 0.98
2016-09-06T08:43:43.035038: step 18740, loss 0.0103394, acc 1
2016-09-06T08:43:43.872623: step 18741, loss 0.00223384, acc 1
2016-09-06T08:43:44.676799: step 18742, loss 0.00254627, acc 1
2016-09-06T08:43:45.491308: step 18743, loss 0.0181507, acc 1
2016-09-06T08:43:46.321364: step 18744, loss 0.00644362, acc 1
2016-09-06T08:43:47.115721: step 18745, loss 0.00215185, acc 1
2016-09-06T08:43:47.927774: step 18746, loss 0.00286162, acc 1
2016-09-06T08:43:48.785332: step 18747, loss 0.00215484, acc 1
2016-09-06T08:43:49.619250: step 18748, loss 0.0181948, acc 0.98
2016-09-06T08:43:50.424401: step 18749, loss 0.00386637, acc 1
2016-09-06T08:43:51.241257: step 18750, loss 0.00216776, acc 1
2016-09-06T08:43:52.080957: step 18751, loss 0.019801, acc 0.98
2016-09-06T08:43:52.881688: step 18752, loss 0.0329388, acc 0.96
2016-09-06T08:43:53.687261: step 18753, loss 0.00410315, acc 1
2016-09-06T08:43:54.498016: step 18754, loss 0.00587568, acc 1
2016-09-06T08:43:55.281507: step 18755, loss 0.0170908, acc 0.98
2016-09-06T08:43:56.086160: step 18756, loss 0.00283464, acc 1
2016-09-06T08:43:56.908425: step 18757, loss 0.0142601, acc 1
2016-09-06T08:43:57.690002: step 18758, loss 0.0215969, acc 0.98
2016-09-06T08:43:58.527204: step 18759, loss 0.032672, acc 0.98
2016-09-06T08:43:59.340305: step 18760, loss 0.00314104, acc 1
2016-09-06T08:44:00.140309: step 18761, loss 0.0120576, acc 1
2016-09-06T08:44:01.001831: step 18762, loss 0.0199686, acc 0.98
2016-09-06T08:44:01.841725: step 18763, loss 0.00226712, acc 1
2016-09-06T08:44:02.649772: step 18764, loss 0.00532196, acc 1
2016-09-06T08:44:03.456046: step 18765, loss 0.0218317, acc 0.98
2016-09-06T08:44:04.273219: step 18766, loss 0.0110499, acc 1
2016-09-06T08:44:05.080643: step 18767, loss 0.0196228, acc 1
2016-09-06T08:44:05.895584: step 18768, loss 0.0203293, acc 0.98
2016-09-06T08:44:06.764291: step 18769, loss 0.0019245, acc 1
2016-09-06T08:44:07.598802: step 18770, loss 0.0125319, acc 1
2016-09-06T08:44:08.421282: step 18771, loss 0.0165073, acc 0.98
2016-09-06T08:44:09.245537: step 18772, loss 0.00346409, acc 1
2016-09-06T08:44:10.049063: step 18773, loss 0.0320357, acc 0.96
2016-09-06T08:44:10.872944: step 18774, loss 0.0270958, acc 0.98
2016-09-06T08:44:11.728677: step 18775, loss 0.00348987, acc 1
2016-09-06T08:44:12.510837: step 18776, loss 0.00192206, acc 1
2016-09-06T08:44:13.345572: step 18777, loss 0.00193357, acc 1
2016-09-06T08:44:14.168555: step 18778, loss 0.00234879, acc 1
2016-09-06T08:44:14.982055: step 18779, loss 0.00191079, acc 1
2016-09-06T08:44:15.794545: step 18780, loss 0.0360299, acc 0.96
2016-09-06T08:44:16.641040: step 18781, loss 0.00198463, acc 1
2016-09-06T08:44:17.448434: step 18782, loss 0.00397158, acc 1
2016-09-06T08:44:18.255148: step 18783, loss 0.00216365, acc 1
2016-09-06T08:44:19.053689: step 18784, loss 0.0204147, acc 0.98
2016-09-06T08:44:19.866922: step 18785, loss 0.0200501, acc 0.98
2016-09-06T08:44:20.652532: step 18786, loss 0.012106, acc 1
2016-09-06T08:44:21.449673: step 18787, loss 0.00365085, acc 1
2016-09-06T08:44:22.346160: step 18788, loss 0.00288453, acc 1
2016-09-06T08:44:23.158284: step 18789, loss 0.013912, acc 1
2016-09-06T08:44:23.973842: step 18790, loss 0.0175887, acc 1
2016-09-06T08:44:24.805574: step 18791, loss 0.0152162, acc 1
2016-09-06T08:44:25.621595: step 18792, loss 0.00219404, acc 1
2016-09-06T08:44:26.467398: step 18793, loss 0.0277694, acc 0.98
2016-09-06T08:44:27.317807: step 18794, loss 0.00188331, acc 1
2016-09-06T08:44:28.139329: step 18795, loss 0.0285849, acc 0.98
2016-09-06T08:44:28.975412: step 18796, loss 0.00552943, acc 1
2016-09-06T08:44:29.807245: step 18797, loss 0.00306288, acc 1
2016-09-06T08:44:30.615818: step 18798, loss 0.011794, acc 1
2016-09-06T08:44:31.459839: step 18799, loss 0.00972533, acc 1
2016-09-06T08:44:32.319545: step 18800, loss 0.00902477, acc 1

Evaluation:
2016-09-06T08:44:36.055165: step 18800, loss 3.68886, acc 0.724203

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-18800

2016-09-06T08:44:38.034754: step 18801, loss 0.0048689, acc 1
2016-09-06T08:44:38.892685: step 18802, loss 0.0443423, acc 0.98
2016-09-06T08:44:39.687993: step 18803, loss 0.0018756, acc 1
2016-09-06T08:44:40.521232: step 18804, loss 0.0194113, acc 0.98
2016-09-06T08:44:41.339483: step 18805, loss 0.00185176, acc 1
2016-09-06T08:44:42.159238: step 18806, loss 0.00189867, acc 1
2016-09-06T08:44:42.993048: step 18807, loss 0.00185385, acc 1
2016-09-06T08:44:43.787011: step 18808, loss 0.00185794, acc 1
2016-09-06T08:44:44.626798: step 18809, loss 0.0021652, acc 1
2016-09-06T08:44:45.456168: step 18810, loss 0.0022864, acc 1
2016-09-06T08:44:46.280507: step 18811, loss 0.00250344, acc 1
2016-09-06T08:44:47.115278: step 18812, loss 0.0025871, acc 1
2016-09-06T08:44:47.974398: step 18813, loss 0.0159099, acc 1
2016-09-06T08:44:48.813546: step 18814, loss 0.00214628, acc 1
2016-09-06T08:44:49.625483: step 18815, loss 0.00181719, acc 1
2016-09-06T08:44:50.402040: step 18816, loss 0.00216237, acc 1
2016-09-06T08:44:51.272958: step 18817, loss 0.00759056, acc 1
2016-09-06T08:44:52.067361: step 18818, loss 0.0042006, acc 1
2016-09-06T08:44:52.907278: step 18819, loss 0.00176743, acc 1
2016-09-06T08:44:53.721576: step 18820, loss 0.00218521, acc 1
2016-09-06T08:44:54.504877: step 18821, loss 0.0240281, acc 0.98
2016-09-06T08:44:55.310925: step 18822, loss 0.0192625, acc 0.98
2016-09-06T08:44:56.122433: step 18823, loss 0.00174391, acc 1
2016-09-06T08:44:56.906251: step 18824, loss 0.016409, acc 1
2016-09-06T08:44:57.740922: step 18825, loss 0.0338995, acc 1
2016-09-06T08:44:58.576541: step 18826, loss 0.00894031, acc 1
2016-09-06T08:44:59.442627: step 18827, loss 0.00194295, acc 1
2016-09-06T08:45:00.261823: step 18828, loss 0.031302, acc 0.98
2016-09-06T08:45:01.079480: step 18829, loss 0.0212334, acc 0.98
2016-09-06T08:45:01.890061: step 18830, loss 0.0157718, acc 1
2016-09-06T08:45:02.708196: step 18831, loss 0.0526011, acc 0.98
2016-09-06T08:45:03.555004: step 18832, loss 0.00176368, acc 1
2016-09-06T08:45:04.347703: step 18833, loss 0.00183543, acc 1
2016-09-06T08:45:05.154330: step 18834, loss 0.00173587, acc 1
2016-09-06T08:45:05.981275: step 18835, loss 0.0120672, acc 1
2016-09-06T08:45:06.805371: step 18836, loss 0.00186335, acc 1
2016-09-06T08:45:07.626311: step 18837, loss 0.0208096, acc 0.98
2016-09-06T08:45:08.471294: step 18838, loss 0.0216776, acc 1
2016-09-06T08:45:09.313588: step 18839, loss 0.0018943, acc 1
2016-09-06T08:45:10.163184: step 18840, loss 0.00479206, acc 1
2016-09-06T08:45:10.997986: step 18841, loss 0.0875593, acc 0.98
2016-09-06T08:45:11.773778: step 18842, loss 0.00608191, acc 1
2016-09-06T08:45:12.569223: step 18843, loss 0.00941458, acc 1
2016-09-06T08:45:13.432748: step 18844, loss 0.0214069, acc 0.98
2016-09-06T08:45:14.250197: step 18845, loss 0.0520518, acc 0.96
2016-09-06T08:45:15.078383: step 18846, loss 0.00602432, acc 1
2016-09-06T08:45:15.903944: step 18847, loss 0.00151406, acc 1
2016-09-06T08:45:16.709725: step 18848, loss 0.00147747, acc 1
2016-09-06T08:45:17.498209: step 18849, loss 0.00175818, acc 1
2016-09-06T08:45:18.309309: step 18850, loss 0.01535, acc 1
2016-09-06T08:45:19.135243: step 18851, loss 0.0016819, acc 1
2016-09-06T08:45:19.923065: step 18852, loss 0.025342, acc 0.98
2016-09-06T08:45:20.736231: step 18853, loss 0.00155426, acc 1
2016-09-06T08:45:21.565445: step 18854, loss 0.0281216, acc 0.98
2016-09-06T08:45:22.344146: step 18855, loss 0.0156023, acc 0.98
2016-09-06T08:45:23.162625: step 18856, loss 0.00138694, acc 1
2016-09-06T08:45:23.999014: step 18857, loss 0.0149936, acc 1
2016-09-06T08:45:24.788092: step 18858, loss 0.0073815, acc 1
2016-09-06T08:45:25.589472: step 18859, loss 0.0317627, acc 0.98
2016-09-06T08:45:26.418768: step 18860, loss 0.00587528, acc 1
2016-09-06T08:45:27.221760: step 18861, loss 0.00138979, acc 1
2016-09-06T08:45:28.026000: step 18862, loss 0.00140702, acc 1
2016-09-06T08:45:28.828613: step 18863, loss 0.0119526, acc 1
2016-09-06T08:45:29.611794: step 18864, loss 0.00135315, acc 1
2016-09-06T08:45:30.411521: step 18865, loss 0.00193521, acc 1
2016-09-06T08:45:31.217562: step 18866, loss 0.00134339, acc 1
2016-09-06T08:45:32.013858: step 18867, loss 0.0181162, acc 0.98
2016-09-06T08:45:32.823898: step 18868, loss 0.0211401, acc 0.98
2016-09-06T08:45:33.659247: step 18869, loss 0.0229493, acc 0.98
2016-09-06T08:45:34.433703: step 18870, loss 0.014157, acc 1
2016-09-06T08:45:35.228733: step 18871, loss 0.0161018, acc 0.98
2016-09-06T08:45:36.074260: step 18872, loss 0.00147672, acc 1
2016-09-06T08:45:36.857475: step 18873, loss 0.0341924, acc 0.96
2016-09-06T08:45:37.673393: step 18874, loss 0.00207625, acc 1
2016-09-06T08:45:38.489943: step 18875, loss 0.00155886, acc 1
2016-09-06T08:45:39.280126: step 18876, loss 0.00129061, acc 1
2016-09-06T08:45:40.110213: step 18877, loss 0.0368752, acc 0.98
2016-09-06T08:45:40.936151: step 18878, loss 0.00536037, acc 1
2016-09-06T08:45:41.797525: step 18879, loss 0.00126436, acc 1
2016-09-06T08:45:42.581737: step 18880, loss 0.116502, acc 0.98
2016-09-06T08:45:43.433401: step 18881, loss 0.0816326, acc 0.98
2016-09-06T08:45:44.262420: step 18882, loss 0.00153685, acc 1
2016-09-06T08:45:45.074618: step 18883, loss 0.0051018, acc 1
2016-09-06T08:45:45.919468: step 18884, loss 0.0129199, acc 1
2016-09-06T08:45:46.733388: step 18885, loss 0.0424033, acc 0.96
2016-09-06T08:45:47.532695: step 18886, loss 0.0120706, acc 1
2016-09-06T08:45:48.351583: step 18887, loss 0.0480022, acc 0.96
2016-09-06T08:45:49.157997: step 18888, loss 0.0087345, acc 1
2016-09-06T08:45:49.961405: step 18889, loss 0.00478739, acc 1
2016-09-06T08:45:50.775958: step 18890, loss 0.00516455, acc 1
2016-09-06T08:45:51.583996: step 18891, loss 0.0123504, acc 1
2016-09-06T08:45:52.395183: step 18892, loss 0.0120006, acc 1
2016-09-06T08:45:53.252038: step 18893, loss 0.0157084, acc 1
2016-09-06T08:45:54.058444: step 18894, loss 0.00436346, acc 1
2016-09-06T08:45:54.857971: step 18895, loss 0.00221843, acc 1
2016-09-06T08:45:55.710675: step 18896, loss 0.0553916, acc 0.96
2016-09-06T08:45:56.544369: step 18897, loss 0.0020603, acc 1
2016-09-06T08:45:57.320817: step 18898, loss 0.00513153, acc 1
2016-09-06T08:45:58.133263: step 18899, loss 0.0128276, acc 1
2016-09-06T08:45:58.945527: step 18900, loss 0.0129796, acc 1

Evaluation:
2016-09-06T08:46:02.705648: step 18900, loss 3.04563, acc 0.725141

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-18900

2016-09-06T08:46:04.611462: step 18901, loss 0.00231424, acc 1
2016-09-06T08:46:05.447602: step 18902, loss 0.0139608, acc 1
2016-09-06T08:46:06.280927: step 18903, loss 0.00279202, acc 1
2016-09-06T08:46:07.098747: step 18904, loss 0.00278122, acc 1
2016-09-06T08:46:07.910977: step 18905, loss 0.0337135, acc 0.98
2016-09-06T08:46:08.723557: step 18906, loss 0.00239133, acc 1
2016-09-06T08:46:09.525254: step 18907, loss 0.0103959, acc 1
2016-09-06T08:46:10.305836: step 18908, loss 0.00241799, acc 1
2016-09-06T08:46:11.114796: step 18909, loss 0.00259028, acc 1
2016-09-06T08:46:11.911536: step 18910, loss 0.00249308, acc 1
2016-09-06T08:46:12.695072: step 18911, loss 0.00245418, acc 1
2016-09-06T08:46:13.541874: step 18912, loss 0.00243523, acc 1
2016-09-06T08:46:14.345550: step 18913, loss 0.00664762, acc 1
2016-09-06T08:46:15.185399: step 18914, loss 0.0484664, acc 0.96
2016-09-06T08:46:16.018286: step 18915, loss 0.0217086, acc 0.98
2016-09-06T08:46:16.871374: step 18916, loss 0.00400702, acc 1
2016-09-06T08:46:17.709966: step 18917, loss 0.0735936, acc 0.96
2016-09-06T08:46:18.553199: step 18918, loss 0.00737234, acc 1
2016-09-06T08:46:19.388069: step 18919, loss 0.0132067, acc 1
2016-09-06T08:46:20.205315: step 18920, loss 0.00234448, acc 1
2016-09-06T08:46:21.023167: step 18921, loss 0.00223187, acc 1
2016-09-06T08:46:21.840858: step 18922, loss 0.00387939, acc 1
2016-09-06T08:46:22.664982: step 18923, loss 0.0231547, acc 1
2016-09-06T08:46:23.512961: step 18924, loss 0.0437665, acc 0.96
2016-09-06T08:46:24.321738: step 18925, loss 0.0179483, acc 1
2016-09-06T08:46:25.125377: step 18926, loss 0.0154575, acc 1
2016-09-06T08:46:25.954989: step 18927, loss 0.00211716, acc 1
2016-09-06T08:46:26.789654: step 18928, loss 0.0155518, acc 1
2016-09-06T08:46:27.589624: step 18929, loss 0.00777597, acc 1
2016-09-06T08:46:28.391730: step 18930, loss 0.00201452, acc 1
2016-09-06T08:46:29.216933: step 18931, loss 0.00226667, acc 1
2016-09-06T08:46:30.025253: step 18932, loss 0.00219729, acc 1
2016-09-06T08:46:30.876782: step 18933, loss 0.00250487, acc 1
2016-09-06T08:46:31.672419: step 18934, loss 0.00196225, acc 1
2016-09-06T08:46:32.441218: step 18935, loss 0.0366288, acc 0.98
2016-09-06T08:46:33.263752: step 18936, loss 0.0276235, acc 0.98
2016-09-06T08:46:34.076113: step 18937, loss 0.00423276, acc 1
2016-09-06T08:46:34.850099: step 18938, loss 0.00187908, acc 1
2016-09-06T08:46:35.694345: step 18939, loss 0.00185241, acc 1
2016-09-06T08:46:36.500246: step 18940, loss 0.0184944, acc 0.98
2016-09-06T08:46:37.314848: step 18941, loss 0.0132905, acc 1
2016-09-06T08:46:38.116805: step 18942, loss 0.0113024, acc 1
2016-09-06T08:46:38.945724: step 18943, loss 0.0149479, acc 1
2016-09-06T08:46:39.738282: step 18944, loss 0.0181614, acc 0.98
2016-09-06T08:46:40.536065: step 18945, loss 0.00736299, acc 1
2016-09-06T08:46:41.348064: step 18946, loss 0.138251, acc 0.98
2016-09-06T08:46:42.152029: step 18947, loss 0.00327171, acc 1
2016-09-06T08:46:42.962870: step 18948, loss 0.00200572, acc 1
2016-09-06T08:46:43.816029: step 18949, loss 0.00318443, acc 1
2016-09-06T08:46:44.656778: step 18950, loss 0.0157899, acc 1
2016-09-06T08:46:45.463967: step 18951, loss 0.0142621, acc 1
2016-09-06T08:46:46.303615: step 18952, loss 0.0142271, acc 1
2016-09-06T08:46:47.128971: step 18953, loss 0.00326686, acc 1
2016-09-06T08:46:47.940366: step 18954, loss 0.016247, acc 0.98
2016-09-06T08:46:48.791070: step 18955, loss 0.00164384, acc 1
2016-09-06T08:46:49.589746: step 18956, loss 0.00880424, acc 1
2016-09-06T08:46:50.378116: step 18957, loss 0.00912509, acc 1
2016-09-06T08:46:51.178221: step 18958, loss 0.00487247, acc 1
2016-09-06T08:46:51.983163: step 18959, loss 0.0200174, acc 1
2016-09-06T08:46:52.800379: step 18960, loss 0.118232, acc 0.98
2016-09-06T08:46:53.626446: step 18961, loss 0.0243835, acc 1
2016-09-06T08:46:54.445974: step 18962, loss 0.0212801, acc 0.98
2016-09-06T08:46:55.268900: step 18963, loss 0.0269608, acc 0.98
2016-09-06T08:46:56.100360: step 18964, loss 0.00200167, acc 1
2016-09-06T08:46:56.960528: step 18965, loss 0.0161974, acc 0.98
2016-09-06T08:46:57.777351: step 18966, loss 0.0150999, acc 1
2016-09-06T08:46:58.591828: step 18967, loss 0.00214742, acc 1
2016-09-06T08:46:59.411125: step 18968, loss 0.00698317, acc 1
2016-09-06T08:47:00.206504: step 18969, loss 0.0165659, acc 1
2016-09-06T08:47:01.040014: step 18970, loss 0.010822, acc 1
2016-09-06T08:47:01.881159: step 18971, loss 0.0227906, acc 0.98
2016-09-06T08:47:02.659619: step 18972, loss 0.00215847, acc 1
2016-09-06T08:47:03.458292: step 18973, loss 0.0166248, acc 0.98
2016-09-06T08:47:04.263379: step 18974, loss 0.0052992, acc 1
2016-09-06T08:47:05.052986: step 18975, loss 0.0158618, acc 1
2016-09-06T08:47:05.858812: step 18976, loss 0.0173685, acc 0.98
2016-09-06T08:47:06.688664: step 18977, loss 0.0021431, acc 1
2016-09-06T08:47:07.474069: step 18978, loss 0.00202901, acc 1
2016-09-06T08:47:08.283508: step 18979, loss 0.00201187, acc 1
2016-09-06T08:47:09.109165: step 18980, loss 0.00249358, acc 1
2016-09-06T08:47:09.933541: step 18981, loss 0.00199857, acc 1
2016-09-06T08:47:10.716990: step 18982, loss 0.0344409, acc 0.98
2016-09-06T08:47:11.527968: step 18983, loss 0.00248349, acc 1
2016-09-06T08:47:12.331410: step 18984, loss 0.00352941, acc 1
2016-09-06T08:47:13.136159: step 18985, loss 0.0309924, acc 0.98
2016-09-06T08:47:13.999116: step 18986, loss 0.00253738, acc 1
2016-09-06T08:47:14.812443: step 18987, loss 0.00857192, acc 1
2016-09-06T08:47:15.609325: step 18988, loss 0.00255646, acc 1
2016-09-06T08:47:16.424156: step 18989, loss 0.00208506, acc 1
2016-09-06T08:47:17.206592: step 18990, loss 0.04898, acc 0.98
2016-09-06T08:47:18.022438: step 18991, loss 0.0270018, acc 0.98
2016-09-06T08:47:18.893527: step 18992, loss 0.0210479, acc 0.98
2016-09-06T08:47:19.717689: step 18993, loss 0.0189822, acc 0.98
2016-09-06T08:47:20.556586: step 18994, loss 0.0281394, acc 1
2016-09-06T08:47:21.415106: step 18995, loss 0.0153487, acc 1
2016-09-06T08:47:22.221340: step 18996, loss 0.0190331, acc 0.98
2016-09-06T08:47:23.030631: step 18997, loss 0.00222641, acc 1
2016-09-06T08:47:23.845536: step 18998, loss 0.00235833, acc 1
2016-09-06T08:47:24.649608: step 18999, loss 0.00218195, acc 1
2016-09-06T08:47:25.458745: step 19000, loss 0.00221922, acc 1

Evaluation:
2016-09-06T08:47:29.225239: step 19000, loss 2.61026, acc 0.718574

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-19000

2016-09-06T08:47:31.184846: step 19001, loss 0.00244686, acc 1
2016-09-06T08:47:32.052252: step 19002, loss 0.0153961, acc 1
2016-09-06T08:47:32.860576: step 19003, loss 0.0158684, acc 1
2016-09-06T08:47:33.675995: step 19004, loss 0.0312897, acc 0.98
2016-09-06T08:47:34.484338: step 19005, loss 0.0200096, acc 0.98
2016-09-06T08:47:35.303844: step 19006, loss 0.00569062, acc 1
2016-09-06T08:47:36.143923: step 19007, loss 0.0342667, acc 0.98
2016-09-06T08:47:36.895315: step 19008, loss 0.00253453, acc 1
2016-09-06T08:47:37.712984: step 19009, loss 0.0109513, acc 1
2016-09-06T08:47:38.554722: step 19010, loss 0.0204126, acc 1
2016-09-06T08:47:39.403831: step 19011, loss 0.00243704, acc 1
2016-09-06T08:47:40.256660: step 19012, loss 0.03628, acc 1
2016-09-06T08:47:41.054023: step 19013, loss 0.0595212, acc 0.98
2016-09-06T08:47:41.867997: step 19014, loss 0.0064058, acc 1
2016-09-06T08:47:42.676145: step 19015, loss 0.00329148, acc 1
2016-09-06T08:47:43.498087: step 19016, loss 0.0240611, acc 0.98
2016-09-06T08:47:44.340429: step 19017, loss 0.0342767, acc 0.98
2016-09-06T08:47:45.154036: step 19018, loss 0.0320604, acc 0.98
2016-09-06T08:47:45.975734: step 19019, loss 0.00217212, acc 1
2016-09-06T08:47:46.820078: step 19020, loss 0.00221322, acc 1
2016-09-06T08:47:47.634034: step 19021, loss 0.0137327, acc 1
2016-09-06T08:47:48.458049: step 19022, loss 0.00234698, acc 1
2016-09-06T08:47:49.289600: step 19023, loss 0.0438385, acc 0.96
2016-09-06T08:47:50.101010: step 19024, loss 0.00252938, acc 1
2016-09-06T08:47:50.896070: step 19025, loss 0.00339002, acc 1
2016-09-06T08:47:51.899537: step 19026, loss 0.00259464, acc 1
2016-09-06T08:47:52.743019: step 19027, loss 0.00391764, acc 1
2016-09-06T08:47:53.552780: step 19028, loss 0.00364176, acc 1
2016-09-06T08:47:54.368414: step 19029, loss 0.00233511, acc 1
2016-09-06T08:47:55.242390: step 19030, loss 0.00768779, acc 1
2016-09-06T08:47:56.044748: step 19031, loss 0.0159214, acc 1
2016-09-06T08:47:56.849146: step 19032, loss 0.00407461, acc 1
2016-09-06T08:47:57.727167: step 19033, loss 0.0149436, acc 1
2016-09-06T08:47:58.531939: step 19034, loss 0.0406467, acc 0.98
2016-09-06T08:47:59.357231: step 19035, loss 0.0650887, acc 0.98
2016-09-06T08:48:00.221997: step 19036, loss 0.0222616, acc 0.98
2016-09-06T08:48:01.049990: step 19037, loss 0.00242369, acc 1
2016-09-06T08:48:01.857396: step 19038, loss 0.0297399, acc 0.98
2016-09-06T08:48:02.674033: step 19039, loss 0.00276101, acc 1
2016-09-06T08:48:03.481390: step 19040, loss 0.00241229, acc 1
2016-09-06T08:48:04.298235: step 19041, loss 0.00281669, acc 1
2016-09-06T08:48:05.132009: step 19042, loss 0.00268646, acc 1
2016-09-06T08:48:05.952195: step 19043, loss 0.0024739, acc 1
2016-09-06T08:48:06.792544: step 19044, loss 0.00292493, acc 1
2016-09-06T08:48:07.624520: step 19045, loss 0.00252623, acc 1
2016-09-06T08:48:08.459944: step 19046, loss 0.0154064, acc 1
2016-09-06T08:48:09.249676: step 19047, loss 0.002548, acc 1
2016-09-06T08:48:10.167978: step 19048, loss 0.00256436, acc 1
2016-09-06T08:48:11.106801: step 19049, loss 0.0235017, acc 1
2016-09-06T08:48:12.010970: step 19050, loss 0.00625356, acc 1
2016-09-06T08:48:12.823523: step 19051, loss 0.018921, acc 0.98
2016-09-06T08:48:13.838263: step 19052, loss 0.00256862, acc 1
2016-09-06T08:48:14.773184: step 19053, loss 0.00453772, acc 1
2016-09-06T08:48:15.659914: step 19054, loss 0.00643393, acc 1
2016-09-06T08:48:16.473400: step 19055, loss 0.109067, acc 0.98
2016-09-06T08:48:17.404800: step 19056, loss 0.0032619, acc 1
2016-09-06T08:48:18.324073: step 19057, loss 0.0254099, acc 0.98
2016-09-06T08:48:19.185376: step 19058, loss 0.00244892, acc 1
2016-09-06T08:48:19.994766: step 19059, loss 0.0357867, acc 0.98
2016-09-06T08:48:20.850415: step 19060, loss 0.0375489, acc 0.98
2016-09-06T08:48:21.700863: step 19061, loss 0.0133459, acc 1
2016-09-06T08:48:22.525379: step 19062, loss 0.00234112, acc 1
2016-09-06T08:48:23.354079: step 19063, loss 0.00268213, acc 1
2016-09-06T08:48:24.158694: step 19064, loss 0.00235659, acc 1
2016-09-06T08:48:24.957551: step 19065, loss 0.00245936, acc 1
2016-09-06T08:48:25.759883: step 19066, loss 0.0327598, acc 0.98
2016-09-06T08:48:26.570058: step 19067, loss 0.00926769, acc 1
2016-09-06T08:48:27.362096: step 19068, loss 0.0024061, acc 1
2016-09-06T08:48:28.170105: step 19069, loss 0.0347898, acc 0.98
2016-09-06T08:48:28.973154: step 19070, loss 0.00398317, acc 1
2016-09-06T08:48:29.768573: step 19071, loss 0.00241833, acc 1
2016-09-06T08:48:30.573465: step 19072, loss 0.00808767, acc 1
2016-09-06T08:48:31.388546: step 19073, loss 0.0166316, acc 0.98
2016-09-06T08:48:32.188271: step 19074, loss 0.045841, acc 0.96
2016-09-06T08:48:33.014495: step 19075, loss 0.00295232, acc 1
2016-09-06T08:48:33.862184: step 19076, loss 0.0164103, acc 1
2016-09-06T08:48:34.636313: step 19077, loss 0.023406, acc 0.98
2016-09-06T08:48:35.470130: step 19078, loss 0.00327757, acc 1
2016-09-06T08:48:36.321568: step 19079, loss 0.00253717, acc 1
2016-09-06T08:48:37.131134: step 19080, loss 0.00277353, acc 1
2016-09-06T08:48:37.944411: step 19081, loss 0.0306569, acc 0.98
2016-09-06T08:48:38.768751: step 19082, loss 0.00288892, acc 1
2016-09-06T08:48:39.584647: step 19083, loss 0.00278866, acc 1
2016-09-06T08:48:40.394450: step 19084, loss 0.00386756, acc 1
2016-09-06T08:48:41.237505: step 19085, loss 0.00291122, acc 1
2016-09-06T08:48:42.026319: step 19086, loss 0.0956813, acc 0.96
2016-09-06T08:48:42.850665: step 19087, loss 0.00301738, acc 1
2016-09-06T08:48:43.666418: step 19088, loss 0.0696091, acc 0.96
2016-09-06T08:48:44.472180: step 19089, loss 0.0030716, acc 1
2016-09-06T08:48:45.285728: step 19090, loss 0.00595108, acc 1
2016-09-06T08:48:46.103233: step 19091, loss 0.047288, acc 0.98
2016-09-06T08:48:46.912841: step 19092, loss 0.00308348, acc 1
2016-09-06T08:48:47.821158: step 19093, loss 0.017439, acc 0.98
2016-09-06T08:48:48.640829: step 19094, loss 0.00311142, acc 1
2016-09-06T08:48:49.430014: step 19095, loss 0.00305985, acc 1
2016-09-06T08:48:50.243139: step 19096, loss 0.0206042, acc 0.98
2016-09-06T08:48:51.123572: step 19097, loss 0.00532499, acc 1
2016-09-06T08:48:51.928135: step 19098, loss 0.00311192, acc 1
2016-09-06T08:48:52.734083: step 19099, loss 0.0253954, acc 0.98
2016-09-06T08:48:53.569864: step 19100, loss 0.00333538, acc 1

Evaluation:
2016-09-06T08:48:57.309390: step 19100, loss 3.19085, acc 0.723265

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-19100

2016-09-06T08:48:59.181378: step 19101, loss 0.048175, acc 0.98
2016-09-06T08:49:00.019320: step 19102, loss 0.00416366, acc 1
2016-09-06T08:49:00.878106: step 19103, loss 0.00334179, acc 1
2016-09-06T08:49:01.710889: step 19104, loss 0.00307262, acc 1
2016-09-06T08:49:02.479408: step 19105, loss 0.00678563, acc 1
2016-09-06T08:49:03.273123: step 19106, loss 0.22893, acc 0.96
2016-09-06T08:49:04.089668: step 19107, loss 0.0165237, acc 1
2016-09-06T08:49:04.874651: step 19108, loss 0.00410111, acc 1
2016-09-06T08:49:05.683278: step 19109, loss 0.0540858, acc 0.96
2016-09-06T08:49:06.489825: step 19110, loss 0.00873958, acc 1
2016-09-06T08:49:07.287965: step 19111, loss 0.0492851, acc 0.98
2016-09-06T08:49:08.086171: step 19112, loss 0.00607754, acc 1
2016-09-06T08:49:08.886493: step 19113, loss 0.0107405, acc 1
2016-09-06T08:49:09.705450: step 19114, loss 0.00796799, acc 1
2016-09-06T08:49:10.533882: step 19115, loss 0.01664, acc 1
2016-09-06T08:49:11.369491: step 19116, loss 0.032567, acc 0.98
2016-09-06T08:49:12.167231: step 19117, loss 0.0296024, acc 0.98
2016-09-06T08:49:12.987671: step 19118, loss 0.00427346, acc 1
2016-09-06T08:49:13.769967: step 19119, loss 0.008814, acc 1
2016-09-06T08:49:14.621321: step 19120, loss 0.0158849, acc 1
2016-09-06T08:49:15.553696: step 19121, loss 0.00475995, acc 1
2016-09-06T08:49:16.391896: step 19122, loss 0.00988794, acc 1
2016-09-06T08:49:17.231146: step 19123, loss 0.0155987, acc 1
2016-09-06T08:49:18.069302: step 19124, loss 0.0178747, acc 1
2016-09-06T08:49:19.100877: step 19125, loss 0.0203114, acc 0.98
2016-09-06T08:49:20.084441: step 19126, loss 0.00474931, acc 1
2016-09-06T08:49:20.967276: step 19127, loss 0.00531845, acc 1
2016-09-06T08:49:22.052411: step 19128, loss 0.00721536, acc 1
2016-09-06T08:49:22.976076: step 19129, loss 0.00497945, acc 1
2016-09-06T08:49:23.878648: step 19130, loss 0.0050248, acc 1
2016-09-06T08:49:24.898309: step 19131, loss 0.0191447, acc 0.98
2016-09-06T08:49:25.910929: step 19132, loss 0.0185812, acc 1
2016-09-06T08:49:26.926097: step 19133, loss 0.140638, acc 0.96
2016-09-06T08:49:27.934357: step 19134, loss 0.0265181, acc 0.98
2016-09-06T08:49:28.892719: step 19135, loss 0.00537803, acc 1
2016-09-06T08:49:29.880319: step 19136, loss 0.0205846, acc 0.98
2016-09-06T08:49:31.075110: step 19137, loss 0.00504927, acc 1
2016-09-06T08:49:32.197813: step 19138, loss 0.0201919, acc 0.98
2016-09-06T08:49:33.430122: step 19139, loss 0.0222083, acc 1
2016-09-06T08:49:34.553706: step 19140, loss 0.00540985, acc 1
2016-09-06T08:49:35.530910: step 19141, loss 0.0375121, acc 0.98
2016-09-06T08:49:36.359158: step 19142, loss 0.0366257, acc 0.98
2016-09-06T08:49:37.342806: step 19143, loss 0.0313049, acc 0.98
2016-09-06T08:49:38.558223: step 19144, loss 0.00857763, acc 1
2016-09-06T08:49:39.438923: step 19145, loss 0.0213057, acc 0.98
2016-09-06T08:49:40.387273: step 19146, loss 0.0241091, acc 0.98
2016-09-06T08:49:41.370607: step 19147, loss 0.00481338, acc 1
2016-09-06T08:49:42.393393: step 19148, loss 0.0161734, acc 1
2016-09-06T08:49:43.280762: step 19149, loss 0.0671937, acc 0.98
2016-09-06T08:49:44.482525: step 19150, loss 0.00494009, acc 1
2016-09-06T08:49:45.370359: step 19151, loss 0.0441566, acc 0.98
2016-09-06T08:49:46.218878: step 19152, loss 0.0135308, acc 1
2016-09-06T08:49:47.091538: step 19153, loss 0.019961, acc 0.98
2016-09-06T08:49:48.034630: step 19154, loss 0.0360215, acc 0.98
2016-09-06T08:49:48.920590: step 19155, loss 0.0183886, acc 1
2016-09-06T08:49:49.861326: step 19156, loss 0.0150046, acc 1
2016-09-06T08:49:50.662407: step 19157, loss 0.00421597, acc 1
2016-09-06T08:49:51.531032: step 19158, loss 0.0193925, acc 1
2016-09-06T08:49:52.330770: step 19159, loss 0.00418389, acc 1
2016-09-06T08:49:53.127357: step 19160, loss 0.0041929, acc 1
2016-09-06T08:49:54.082987: step 19161, loss 0.0350509, acc 0.98
2016-09-06T08:49:54.957460: step 19162, loss 0.00420403, acc 1
2016-09-06T08:49:55.796917: step 19163, loss 0.00434827, acc 1
2016-09-06T08:49:56.605597: step 19164, loss 0.0752861, acc 0.96
2016-09-06T08:49:57.450165: step 19165, loss 0.00466053, acc 1
2016-09-06T08:49:58.341301: step 19166, loss 0.04803, acc 0.98
2016-09-06T08:49:59.141647: step 19167, loss 0.0045736, acc 1
2016-09-06T08:49:59.966214: step 19168, loss 0.0295258, acc 0.98
2016-09-06T08:50:00.829818: step 19169, loss 0.022385, acc 0.98
2016-09-06T08:50:01.661442: step 19170, loss 0.00387714, acc 1
2016-09-06T08:50:02.501049: step 19171, loss 0.0564305, acc 0.98
2016-09-06T08:50:03.396751: step 19172, loss 0.00373069, acc 1
2016-09-06T08:50:04.243663: step 19173, loss 0.00792019, acc 1
2016-09-06T08:50:05.061675: step 19174, loss 0.0286289, acc 0.98
2016-09-06T08:50:05.908479: step 19175, loss 0.0468942, acc 0.98
2016-09-06T08:50:06.722185: step 19176, loss 0.0318733, acc 0.98
2016-09-06T08:50:07.622552: step 19177, loss 0.0237718, acc 0.98
2016-09-06T08:50:08.454637: step 19178, loss 0.00605323, acc 1
2016-09-06T08:50:09.319237: step 19179, loss 0.012394, acc 1
2016-09-06T08:50:10.132187: step 19180, loss 0.0038637, acc 1
2016-09-06T08:50:10.949636: step 19181, loss 0.0119976, acc 1
2016-09-06T08:50:11.788070: step 19182, loss 0.00342192, acc 1
2016-09-06T08:50:12.585417: step 19183, loss 0.0219902, acc 1
2016-09-06T08:50:13.398654: step 19184, loss 0.00422892, acc 1
2016-09-06T08:50:14.221799: step 19185, loss 0.00487116, acc 1
2016-09-06T08:50:15.015326: step 19186, loss 0.00592348, acc 1
2016-09-06T08:50:15.832441: step 19187, loss 0.0927054, acc 0.98
2016-09-06T08:50:16.673408: step 19188, loss 0.020884, acc 0.98
2016-09-06T08:50:17.500297: step 19189, loss 0.00622585, acc 1
2016-09-06T08:50:18.343287: step 19190, loss 0.00384417, acc 1
2016-09-06T08:50:19.166519: step 19191, loss 0.00619753, acc 1
2016-09-06T08:50:19.999201: step 19192, loss 0.00642284, acc 1
2016-09-06T08:50:20.846483: step 19193, loss 0.0375649, acc 0.96
2016-09-06T08:50:21.667594: step 19194, loss 0.0171819, acc 1
2016-09-06T08:50:22.523115: step 19195, loss 0.0114982, acc 1
2016-09-06T08:50:23.334606: step 19196, loss 0.103394, acc 0.98
2016-09-06T08:50:24.144268: step 19197, loss 0.0863175, acc 0.96
2016-09-06T08:50:24.950115: step 19198, loss 0.0190221, acc 1
2016-09-06T08:50:25.769384: step 19199, loss 0.0187131, acc 0.98
2016-09-06T08:50:26.560293: step 19200, loss 0.00476183, acc 1

Evaluation:
2016-09-06T08:50:30.271592: step 19200, loss 2.28743, acc 0.71576

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473106415/checkpoints/model-19200


WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7fcbccb86e90>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.
WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7fcbccb86e50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.

Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=50
CHECKPOINT_EVERY=100
CV_INDEX=1
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=300
EVALUATE_EVERY=100
FILTER_SIZES=3,4,5
HIDDEN_DIM=300
L2_REG_LAMBDA=0
LOG_DEVICE_PLACEMENT=False
NUM_EPOCHS=30
NUM_FILTERS=100
WORD2VEC=GoogleNews-vectors-negative300.bin

Loading data...
Vocabulary Size: 18758
Train/Dev split: 9594/1066
Writing to /home/cil/lstm-context-embeddings/runs/1473161068

Load word2vec file GoogleNews-vectors-negative300.bin

2016-09-06T19:24:47.951483: step 1, loss 0.693147, acc 0.42
2016-09-06T19:24:48.645754: step 2, loss 0.74962, acc 0.32
2016-09-06T19:24:49.350311: step 3, loss 0.694661, acc 0.52
2016-09-06T19:24:50.018764: step 4, loss 0.691369, acc 0.48
2016-09-06T19:24:50.707549: step 5, loss 0.695243, acc 0.46
2016-09-06T19:24:51.406312: step 6, loss 0.715988, acc 0.38
2016-09-06T19:24:52.079835: step 7, loss 0.703942, acc 0.48
2016-09-06T19:24:52.763400: step 8, loss 0.692779, acc 0.56
2016-09-06T19:24:53.436412: step 9, loss 0.697398, acc 0.44
2016-09-06T19:24:54.122009: step 10, loss 0.708332, acc 0.46
2016-09-06T19:24:54.805585: step 11, loss 0.694174, acc 0.58
2016-09-06T19:24:55.496336: step 12, loss 0.695264, acc 0.5
2016-09-06T19:24:56.189861: step 13, loss 0.708077, acc 0.42
2016-09-06T19:24:56.874174: step 14, loss 0.687771, acc 0.54
2016-09-06T19:24:57.540130: step 15, loss 0.675395, acc 0.7
2016-09-06T19:24:58.223013: step 16, loss 0.701396, acc 0.42
2016-09-06T19:24:58.929008: step 17, loss 0.673056, acc 0.62
2016-09-06T19:24:59.596537: step 18, loss 0.704305, acc 0.42
2016-09-06T19:25:00.332827: step 19, loss 0.704011, acc 0.52
2016-09-06T19:25:01.010953: step 20, loss 0.681994, acc 0.56
2016-09-06T19:25:01.677488: step 21, loss 0.675958, acc 0.66
2016-09-06T19:25:02.367883: step 22, loss 0.667803, acc 0.66
2016-09-06T19:25:03.066000: step 23, loss 0.667726, acc 0.52
2016-09-06T19:25:03.758523: step 24, loss 0.67224, acc 0.62
2016-09-06T19:25:04.408619: step 25, loss 0.645804, acc 0.6
2016-09-06T19:25:05.118815: step 26, loss 0.696033, acc 0.66
2016-09-06T19:25:05.814665: step 27, loss 0.639599, acc 0.68
2016-09-06T19:25:06.503616: step 28, loss 0.67443, acc 0.54
2016-09-06T19:25:07.196764: step 29, loss 0.631554, acc 0.68
2016-09-06T19:25:07.898254: step 30, loss 0.653638, acc 0.66
2016-09-06T19:25:08.579067: step 31, loss 0.7153, acc 0.64
2016-09-06T19:25:09.241008: step 32, loss 0.67483, acc 0.6
2016-09-06T19:25:09.942937: step 33, loss 0.580751, acc 0.74
2016-09-06T19:25:10.645470: step 34, loss 0.639389, acc 0.66
2016-09-06T19:25:11.344087: step 35, loss 0.603861, acc 0.7
2016-09-06T19:25:12.030717: step 36, loss 0.694487, acc 0.52
2016-09-06T19:25:12.733473: step 37, loss 0.677935, acc 0.52
2016-09-06T19:25:13.456596: step 38, loss 0.613347, acc 0.68
2016-09-06T19:25:14.122744: step 39, loss 0.581594, acc 0.68
2016-09-06T19:25:14.790794: step 40, loss 0.724659, acc 0.46
2016-09-06T19:25:15.466666: step 41, loss 0.56743, acc 0.7
2016-09-06T19:25:16.167015: step 42, loss 0.599131, acc 0.7
2016-09-06T19:25:16.854760: step 43, loss 0.670601, acc 0.56
2016-09-06T19:25:17.535704: step 44, loss 0.653911, acc 0.64
2016-09-06T19:25:18.254071: step 45, loss 0.645081, acc 0.64
2016-09-06T19:25:18.932189: step 46, loss 0.598267, acc 0.72
2016-09-06T19:25:19.621897: step 47, loss 0.567577, acc 0.7
2016-09-06T19:25:20.316268: step 48, loss 0.587256, acc 0.72
2016-09-06T19:25:21.021880: step 49, loss 0.532347, acc 0.72
2016-09-06T19:25:21.718712: step 50, loss 0.51441, acc 0.74
2016-09-06T19:25:22.408306: step 51, loss 0.564337, acc 0.64
2016-09-06T19:25:23.107327: step 52, loss 0.438371, acc 0.78
2016-09-06T19:25:23.777146: step 53, loss 0.826744, acc 0.48
2016-09-06T19:25:24.463506: step 54, loss 0.667166, acc 0.64
2016-09-06T19:25:25.148231: step 55, loss 0.585777, acc 0.66
2016-09-06T19:25:25.817813: step 56, loss 0.553099, acc 0.8
2016-09-06T19:25:26.515065: step 57, loss 0.559518, acc 0.7
2016-09-06T19:25:27.177476: step 58, loss 0.592406, acc 0.62
2016-09-06T19:25:27.872456: step 59, loss 0.559114, acc 0.72
2016-09-06T19:25:28.553757: step 60, loss 0.585628, acc 0.64
2016-09-06T19:25:29.259711: step 61, loss 0.573019, acc 0.7
2016-09-06T19:25:29.931554: step 62, loss 0.533895, acc 0.8
2016-09-06T19:25:30.619456: step 63, loss 0.554839, acc 0.66
2016-09-06T19:25:31.290312: step 64, loss 0.525344, acc 0.74
2016-09-06T19:25:31.964216: step 65, loss 0.672631, acc 0.6
2016-09-06T19:25:32.678342: step 66, loss 0.561953, acc 0.7
2016-09-06T19:25:33.349663: step 67, loss 0.537047, acc 0.76
2016-09-06T19:25:34.034197: step 68, loss 0.584691, acc 0.68
2016-09-06T19:25:34.728260: step 69, loss 0.603147, acc 0.72
2016-09-06T19:25:35.412428: step 70, loss 0.568503, acc 0.7
2016-09-06T19:25:36.079074: step 71, loss 0.420167, acc 0.82
2016-09-06T19:25:36.769066: step 72, loss 0.543981, acc 0.7
2016-09-06T19:25:37.473332: step 73, loss 0.545395, acc 0.68
2016-09-06T19:25:38.143652: step 74, loss 0.51518, acc 0.74
2016-09-06T19:25:38.842034: step 75, loss 0.572078, acc 0.68
2016-09-06T19:25:39.521826: step 76, loss 0.533769, acc 0.66
2016-09-06T19:25:40.209295: step 77, loss 0.564542, acc 0.66
2016-09-06T19:25:40.898216: step 78, loss 0.501887, acc 0.78
2016-09-06T19:25:41.583195: step 79, loss 0.467568, acc 0.78
2016-09-06T19:25:42.276953: step 80, loss 0.524887, acc 0.76
2016-09-06T19:25:42.969939: step 81, loss 0.495916, acc 0.8
2016-09-06T19:25:43.640005: step 82, loss 0.682112, acc 0.7
2016-09-06T19:25:44.330937: step 83, loss 0.521456, acc 0.74
2016-09-06T19:25:45.044596: step 84, loss 0.627286, acc 0.66
2016-09-06T19:25:45.736562: step 85, loss 0.59014, acc 0.68
2016-09-06T19:25:46.425296: step 86, loss 0.485205, acc 0.78
2016-09-06T19:25:47.142020: step 87, loss 0.610147, acc 0.68
2016-09-06T19:25:47.846576: step 88, loss 0.501292, acc 0.74
2016-09-06T19:25:48.531111: step 89, loss 0.515722, acc 0.76
2016-09-06T19:25:49.217776: step 90, loss 0.592864, acc 0.66
2016-09-06T19:25:49.910114: step 91, loss 0.43907, acc 0.74
2016-09-06T19:25:50.589950: step 92, loss 0.534359, acc 0.72
2016-09-06T19:25:51.246727: step 93, loss 0.444593, acc 0.82
2016-09-06T19:25:51.950904: step 94, loss 0.579643, acc 0.68
2016-09-06T19:25:52.641130: step 95, loss 0.625997, acc 0.68
2016-09-06T19:25:53.315355: step 96, loss 0.510871, acc 0.7
2016-09-06T19:25:54.019425: step 97, loss 0.580829, acc 0.68
2016-09-06T19:25:54.719205: step 98, loss 0.523278, acc 0.7
2016-09-06T19:25:55.395957: step 99, loss 0.418394, acc 0.82
2016-09-06T19:25:56.063473: step 100, loss 0.429376, acc 0.8

Evaluation:
2016-09-06T19:25:59.276100: step 100, loss 0.46813, acc 0.795497

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-100

2016-09-06T19:26:00.951936: step 101, loss 0.478181, acc 0.8
2016-09-06T19:26:01.666238: step 102, loss 0.497783, acc 0.76
2016-09-06T19:26:02.352862: step 103, loss 0.523923, acc 0.78
2016-09-06T19:26:03.066401: step 104, loss 0.592227, acc 0.72
2016-09-06T19:26:03.760217: step 105, loss 0.490596, acc 0.78
2016-09-06T19:26:04.445882: step 106, loss 0.494187, acc 0.72
2016-09-06T19:26:05.159588: step 107, loss 0.40303, acc 0.84
2016-09-06T19:26:05.831568: step 108, loss 0.561936, acc 0.68
2016-09-06T19:26:06.493272: step 109, loss 0.411469, acc 0.88
2016-09-06T19:26:07.180556: step 110, loss 0.52765, acc 0.74
2016-09-06T19:26:07.861671: step 111, loss 0.566448, acc 0.74
2016-09-06T19:26:08.564234: step 112, loss 0.557703, acc 0.74
2016-09-06T19:26:09.251484: step 113, loss 0.561112, acc 0.66
2016-09-06T19:26:09.949875: step 114, loss 0.502422, acc 0.8
2016-09-06T19:26:10.633767: step 115, loss 0.635512, acc 0.68
2016-09-06T19:26:11.324686: step 116, loss 0.518135, acc 0.74
2016-09-06T19:26:12.003219: step 117, loss 0.568431, acc 0.66
2016-09-06T19:26:12.694458: step 118, loss 0.43617, acc 0.78
2016-09-06T19:26:13.389000: step 119, loss 0.547295, acc 0.78
2016-09-06T19:26:14.051718: step 120, loss 0.551268, acc 0.72
2016-09-06T19:26:14.757385: step 121, loss 0.472582, acc 0.78
2016-09-06T19:26:15.437953: step 122, loss 0.55815, acc 0.7
2016-09-06T19:26:16.118040: step 123, loss 0.461081, acc 0.78
2016-09-06T19:26:16.802361: step 124, loss 0.51002, acc 0.74
2016-09-06T19:26:17.522578: step 125, loss 0.605171, acc 0.76
2016-09-06T19:26:18.204172: step 126, loss 0.545898, acc 0.72
2016-09-06T19:26:18.872255: step 127, loss 0.533476, acc 0.7
2016-09-06T19:26:19.564696: step 128, loss 0.446805, acc 0.78
2016-09-06T19:26:20.233206: step 129, loss 0.476883, acc 0.76
2016-09-06T19:26:20.904102: step 130, loss 0.440111, acc 0.78
2016-09-06T19:26:21.586512: step 131, loss 0.46029, acc 0.8
2016-09-06T19:26:22.280913: step 132, loss 0.526495, acc 0.82
2016-09-06T19:26:22.966811: step 133, loss 0.512358, acc 0.7
2016-09-06T19:26:23.654844: step 134, loss 0.567183, acc 0.72
2016-09-06T19:26:24.369245: step 135, loss 0.461584, acc 0.74
2016-09-06T19:26:25.034114: step 136, loss 0.501002, acc 0.76
2016-09-06T19:26:25.717787: step 137, loss 0.486186, acc 0.8
2016-09-06T19:26:26.401025: step 138, loss 0.433656, acc 0.76
2016-09-06T19:26:27.092231: step 139, loss 0.465307, acc 0.72
2016-09-06T19:26:27.790432: step 140, loss 0.566154, acc 0.64
2016-09-06T19:26:28.483618: step 141, loss 0.475336, acc 0.78
2016-09-06T19:26:29.199876: step 142, loss 0.426739, acc 0.78
2016-09-06T19:26:29.924887: step 143, loss 0.522684, acc 0.8
2016-09-06T19:26:30.634177: step 144, loss 0.463181, acc 0.76
2016-09-06T19:26:31.324124: step 145, loss 0.531917, acc 0.72
2016-09-06T19:26:32.008603: step 146, loss 0.545234, acc 0.74
2016-09-06T19:26:32.734648: step 147, loss 0.412081, acc 0.78
2016-09-06T19:26:33.405526: step 148, loss 0.412847, acc 0.8
2016-09-06T19:26:34.106866: step 149, loss 0.506623, acc 0.74
2016-09-06T19:26:34.788926: step 150, loss 0.526044, acc 0.76
2016-09-06T19:26:35.484093: step 151, loss 0.50447, acc 0.76
2016-09-06T19:26:36.184099: step 152, loss 0.456586, acc 0.8
2016-09-06T19:26:36.866164: step 153, loss 0.46831, acc 0.82
2016-09-06T19:26:37.575848: step 154, loss 0.524948, acc 0.74
2016-09-06T19:26:38.267279: step 155, loss 0.441792, acc 0.8
2016-09-06T19:26:38.952837: step 156, loss 0.621359, acc 0.58
2016-09-06T19:26:39.637036: step 157, loss 0.526749, acc 0.74
2016-09-06T19:26:40.327303: step 158, loss 0.48174, acc 0.68
2016-09-06T19:26:41.024613: step 159, loss 0.540505, acc 0.74
2016-09-06T19:26:41.709077: step 160, loss 0.434026, acc 0.8
2016-09-06T19:26:42.391564: step 161, loss 0.494085, acc 0.74
2016-09-06T19:26:43.063933: step 162, loss 0.373855, acc 0.88
2016-09-06T19:26:43.754673: step 163, loss 0.553518, acc 0.74
2016-09-06T19:26:44.443212: step 164, loss 0.446365, acc 0.8
2016-09-06T19:26:45.111425: step 165, loss 0.428641, acc 0.86
2016-09-06T19:26:45.804118: step 166, loss 0.466343, acc 0.82
2016-09-06T19:26:46.499578: step 167, loss 0.521417, acc 0.76
2016-09-06T19:26:47.202337: step 168, loss 0.466412, acc 0.78
2016-09-06T19:26:47.879743: step 169, loss 0.614991, acc 0.7
2016-09-06T19:26:48.568993: step 170, loss 0.562958, acc 0.76
2016-09-06T19:26:49.246038: step 171, loss 0.486214, acc 0.78
2016-09-06T19:26:49.933430: step 172, loss 0.486743, acc 0.74
2016-09-06T19:26:50.603053: step 173, loss 0.52561, acc 0.7
2016-09-06T19:26:51.293325: step 174, loss 0.526343, acc 0.76
2016-09-06T19:26:52.000316: step 175, loss 0.560961, acc 0.72
2016-09-06T19:26:52.681819: step 176, loss 0.416444, acc 0.8
2016-09-06T19:26:53.359663: step 177, loss 0.534777, acc 0.72
2016-09-06T19:26:54.063098: step 178, loss 0.474735, acc 0.8
2016-09-06T19:26:54.734451: step 179, loss 0.476134, acc 0.76
2016-09-06T19:26:55.428409: step 180, loss 0.323437, acc 0.88
2016-09-06T19:26:56.117225: step 181, loss 0.606645, acc 0.76
2016-09-06T19:26:56.808306: step 182, loss 0.565889, acc 0.74
2016-09-06T19:26:57.474648: step 183, loss 0.52195, acc 0.78
2016-09-06T19:26:58.153540: step 184, loss 0.470563, acc 0.8
2016-09-06T19:26:58.850436: step 185, loss 0.55115, acc 0.68
2016-09-06T19:26:59.549435: step 186, loss 0.469283, acc 0.76
2016-09-06T19:27:00.237449: step 187, loss 0.471713, acc 0.8
2016-09-06T19:27:00.911210: step 188, loss 0.393526, acc 0.84
2016-09-06T19:27:01.610257: step 189, loss 0.511343, acc 0.78
2016-09-06T19:27:02.272131: step 190, loss 0.523555, acc 0.74
2016-09-06T19:27:02.954478: step 191, loss 0.494478, acc 0.8
2016-09-06T19:27:03.616277: step 192, loss 0.443274, acc 0.840909
2016-09-06T19:27:04.290563: step 193, loss 0.370384, acc 0.84
2016-09-06T19:27:04.949680: step 194, loss 0.349751, acc 0.84
2016-09-06T19:27:05.612209: step 195, loss 0.373809, acc 0.86
2016-09-06T19:27:06.315180: step 196, loss 0.377653, acc 0.86
2016-09-06T19:27:07.011698: step 197, loss 0.325048, acc 0.86
2016-09-06T19:27:07.717780: step 198, loss 0.277316, acc 0.9
2016-09-06T19:27:08.411616: step 199, loss 0.381828, acc 0.8
2016-09-06T19:27:09.106039: step 200, loss 0.399286, acc 0.8

Evaluation:
2016-09-06T19:27:12.240375: step 200, loss 0.432418, acc 0.801126

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-200

2016-09-06T19:27:13.919122: step 201, loss 0.51056, acc 0.78
2016-09-06T19:27:14.612032: step 202, loss 0.485884, acc 0.82
2016-09-06T19:27:15.279661: step 203, loss 0.480817, acc 0.78
2016-09-06T19:27:15.958122: step 204, loss 0.558163, acc 0.8
2016-09-06T19:27:16.622202: step 205, loss 0.409477, acc 0.78
2016-09-06T19:27:17.357123: step 206, loss 0.357589, acc 0.76
2016-09-06T19:27:18.040655: step 207, loss 0.402983, acc 0.84
2016-09-06T19:27:18.723095: step 208, loss 0.315779, acc 0.88
2016-09-06T19:27:19.414261: step 209, loss 0.319404, acc 0.88
2016-09-06T19:27:20.101553: step 210, loss 0.350563, acc 0.84
2016-09-06T19:27:20.817432: step 211, loss 0.384074, acc 0.82
2016-09-06T19:27:21.478909: step 212, loss 0.318705, acc 0.86
2016-09-06T19:27:22.184577: step 213, loss 0.484441, acc 0.82
2016-09-06T19:27:22.861435: step 214, loss 0.359677, acc 0.84
2016-09-06T19:27:23.561631: step 215, loss 0.398446, acc 0.84
2016-09-06T19:27:24.231645: step 216, loss 0.375426, acc 0.8
2016-09-06T19:27:24.910726: step 217, loss 0.429184, acc 0.76
2016-09-06T19:27:25.611058: step 218, loss 0.334489, acc 0.88
2016-09-06T19:27:26.289623: step 219, loss 0.321891, acc 0.86
2016-09-06T19:27:26.980328: step 220, loss 0.532952, acc 0.82
2016-09-06T19:27:27.667401: step 221, loss 0.350861, acc 0.86
2016-09-06T19:27:28.355286: step 222, loss 0.314426, acc 0.84
2016-09-06T19:27:29.033162: step 223, loss 0.396638, acc 0.82
2016-09-06T19:27:29.725258: step 224, loss 0.453678, acc 0.82
2016-09-06T19:27:30.440088: step 225, loss 0.42137, acc 0.82
2016-09-06T19:27:31.099530: step 226, loss 0.219395, acc 0.92
2016-09-06T19:27:31.818796: step 227, loss 0.418867, acc 0.84
2016-09-06T19:27:32.527926: step 228, loss 0.469349, acc 0.78
2016-09-06T19:27:33.207339: step 229, loss 0.383541, acc 0.84
2016-09-06T19:27:33.892939: step 230, loss 0.399453, acc 0.86
2016-09-06T19:27:34.570044: step 231, loss 0.249507, acc 0.94
2016-09-06T19:27:35.286877: step 232, loss 0.437481, acc 0.74
2016-09-06T19:27:35.965517: step 233, loss 0.372767, acc 0.84
2016-09-06T19:27:36.633557: step 234, loss 0.410606, acc 0.84
2016-09-06T19:27:37.306678: step 235, loss 0.454543, acc 0.82
2016-09-06T19:27:38.000850: step 236, loss 0.395073, acc 0.8
2016-09-06T19:27:38.701232: step 237, loss 0.349092, acc 0.84
2016-09-06T19:27:39.377252: step 238, loss 0.247924, acc 0.88
2016-09-06T19:27:40.095237: step 239, loss 0.228777, acc 0.9
2016-09-06T19:27:40.774101: step 240, loss 0.319819, acc 0.9
2016-09-06T19:27:41.459662: step 241, loss 0.243963, acc 0.92
2016-09-06T19:27:42.139544: step 242, loss 0.448938, acc 0.78
2016-09-06T19:27:42.818805: step 243, loss 0.362109, acc 0.84
2016-09-06T19:27:43.507820: step 244, loss 0.297863, acc 0.92
2016-09-06T19:27:44.189640: step 245, loss 0.371481, acc 0.84
2016-09-06T19:27:44.896960: step 246, loss 0.35628, acc 0.78
2016-09-06T19:27:45.577891: step 247, loss 0.361214, acc 0.86
2016-09-06T19:27:46.261981: step 248, loss 0.479111, acc 0.76
2016-09-06T19:27:46.934997: step 249, loss 0.464125, acc 0.76
2016-09-06T19:27:47.616838: step 250, loss 0.480324, acc 0.8
2016-09-06T19:27:48.324674: step 251, loss 0.332564, acc 0.84
2016-09-06T19:27:49.019004: step 252, loss 0.350641, acc 0.78
2016-09-06T19:27:49.725858: step 253, loss 0.415876, acc 0.82
2016-09-06T19:27:50.411745: step 254, loss 0.432412, acc 0.82
2016-09-06T19:27:51.126605: step 255, loss 0.337078, acc 0.84
2016-09-06T19:27:51.839972: step 256, loss 0.391018, acc 0.78
2016-09-06T19:27:52.513842: step 257, loss 0.308616, acc 0.88
2016-09-06T19:27:53.208263: step 258, loss 0.289609, acc 0.92
2016-09-06T19:27:53.845476: step 259, loss 0.379302, acc 0.86
2016-09-06T19:27:54.532653: step 260, loss 0.315157, acc 0.84
2016-09-06T19:27:55.190528: step 261, loss 0.498755, acc 0.72
2016-09-06T19:27:55.872852: step 262, loss 0.44577, acc 0.8
2016-09-06T19:27:56.558075: step 263, loss 0.368072, acc 0.82
2016-09-06T19:27:57.245543: step 264, loss 0.376584, acc 0.82
2016-09-06T19:27:57.934492: step 265, loss 0.383785, acc 0.84
2016-09-06T19:27:58.622337: step 266, loss 0.384047, acc 0.86
2016-09-06T19:27:59.337360: step 267, loss 0.533544, acc 0.78
2016-09-06T19:28:00.022362: step 268, loss 0.408774, acc 0.78
2016-09-06T19:28:00.742061: step 269, loss 0.360383, acc 0.86
2016-09-06T19:28:01.425307: step 270, loss 0.344082, acc 0.86
2016-09-06T19:28:02.102319: step 271, loss 0.333065, acc 0.88
2016-09-06T19:28:02.806770: step 272, loss 0.337724, acc 0.82
2016-09-06T19:28:03.491613: step 273, loss 0.256624, acc 0.9
2016-09-06T19:28:04.204742: step 274, loss 0.250259, acc 0.92
2016-09-06T19:28:04.897889: step 275, loss 0.302719, acc 0.84
2016-09-06T19:28:05.584700: step 276, loss 0.429909, acc 0.8
2016-09-06T19:28:06.263160: step 277, loss 0.295828, acc 0.88
2016-09-06T19:28:06.939732: step 278, loss 0.462467, acc 0.78
2016-09-06T19:28:07.639494: step 279, loss 0.528453, acc 0.78
2016-09-06T19:28:08.307635: step 280, loss 0.399353, acc 0.82
2016-09-06T19:28:09.017765: step 281, loss 0.547673, acc 0.76
2016-09-06T19:28:09.695472: step 282, loss 0.411973, acc 0.88
2016-09-06T19:28:10.376645: step 283, loss 0.28806, acc 0.86
2016-09-06T19:28:11.051097: step 284, loss 0.266935, acc 0.92
2016-09-06T19:28:11.735474: step 285, loss 0.309411, acc 0.86
2016-09-06T19:28:12.416800: step 286, loss 0.332876, acc 0.82
2016-09-06T19:28:13.070272: step 287, loss 0.342831, acc 0.84
2016-09-06T19:28:13.766471: step 288, loss 0.356457, acc 0.8
2016-09-06T19:28:14.435134: step 289, loss 0.340287, acc 0.88
2016-09-06T19:28:15.106815: step 290, loss 0.316248, acc 0.88
2016-09-06T19:28:15.815471: step 291, loss 0.274062, acc 0.92
2016-09-06T19:28:16.516349: step 292, loss 0.310869, acc 0.78
2016-09-06T19:28:17.202917: step 293, loss 0.38993, acc 0.8
2016-09-06T19:28:17.882084: step 294, loss 0.265756, acc 0.86
2016-09-06T19:28:18.585991: step 295, loss 0.510642, acc 0.8
2016-09-06T19:28:19.271242: step 296, loss 0.406995, acc 0.82
2016-09-06T19:28:19.962420: step 297, loss 0.38532, acc 0.78
2016-09-06T19:28:20.641910: step 298, loss 0.268184, acc 0.9
2016-09-06T19:28:21.342953: step 299, loss 0.314361, acc 0.86
2016-09-06T19:28:22.045865: step 300, loss 0.352382, acc 0.88

Evaluation:
2016-09-06T19:28:25.158766: step 300, loss 0.477711, acc 0.793621

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-300

2016-09-06T19:28:26.839818: step 301, loss 0.392546, acc 0.8
2016-09-06T19:28:27.519126: step 302, loss 0.356877, acc 0.8
2016-09-06T19:28:28.219657: step 303, loss 0.384506, acc 0.84
2016-09-06T19:28:28.895349: step 304, loss 0.124431, acc 0.98
2016-09-06T19:28:29.577992: step 305, loss 0.294804, acc 0.88
2016-09-06T19:28:30.250574: step 306, loss 0.474334, acc 0.78
2016-09-06T19:28:30.953109: step 307, loss 0.322725, acc 0.88
2016-09-06T19:28:31.660303: step 308, loss 0.262373, acc 0.86
2016-09-06T19:28:32.325580: step 309, loss 0.706817, acc 0.7
2016-09-06T19:28:33.022793: step 310, loss 0.475812, acc 0.78
2016-09-06T19:28:33.690391: step 311, loss 0.396573, acc 0.84
2016-09-06T19:28:34.395986: step 312, loss 0.39043, acc 0.78
2016-09-06T19:28:35.084027: step 313, loss 0.297504, acc 0.92
2016-09-06T19:28:35.790947: step 314, loss 0.47992, acc 0.8
2016-09-06T19:28:36.470142: step 315, loss 0.370608, acc 0.84
2016-09-06T19:28:37.133524: step 316, loss 0.416998, acc 0.8
2016-09-06T19:28:37.839724: step 317, loss 0.354784, acc 0.86
2016-09-06T19:28:38.513647: step 318, loss 0.37726, acc 0.88
2016-09-06T19:28:39.227286: step 319, loss 0.332065, acc 0.88
2016-09-06T19:28:39.907965: step 320, loss 0.421417, acc 0.74
2016-09-06T19:28:40.590484: step 321, loss 0.336936, acc 0.86
2016-09-06T19:28:41.291343: step 322, loss 0.340995, acc 0.88
2016-09-06T19:28:41.989723: step 323, loss 0.327244, acc 0.86
2016-09-06T19:28:42.706876: step 324, loss 0.330941, acc 0.88
2016-09-06T19:28:43.390069: step 325, loss 0.342048, acc 0.82
2016-09-06T19:28:44.071330: step 326, loss 0.280103, acc 0.86
2016-09-06T19:28:44.762060: step 327, loss 0.364228, acc 0.8
2016-09-06T19:28:45.445247: step 328, loss 0.29417, acc 0.86
2016-09-06T19:28:46.129118: step 329, loss 0.215794, acc 0.9
2016-09-06T19:28:46.813644: step 330, loss 0.318677, acc 0.82
2016-09-06T19:28:47.517263: step 331, loss 0.356189, acc 0.86
2016-09-06T19:28:48.196142: step 332, loss 0.453776, acc 0.8
2016-09-06T19:28:48.910990: step 333, loss 0.342252, acc 0.78
2016-09-06T19:28:49.607920: step 334, loss 0.363011, acc 0.78
2016-09-06T19:28:50.307177: step 335, loss 0.553867, acc 0.78
2016-09-06T19:28:51.007097: step 336, loss 0.335123, acc 0.86
2016-09-06T19:28:51.677729: step 337, loss 0.404774, acc 0.84
2016-09-06T19:28:52.365738: step 338, loss 0.403756, acc 0.82
2016-09-06T19:28:53.054889: step 339, loss 0.326972, acc 0.88
2016-09-06T19:28:53.746754: step 340, loss 0.325892, acc 0.88
2016-09-06T19:28:54.442762: step 341, loss 0.509059, acc 0.78
2016-09-06T19:28:55.118323: step 342, loss 0.459549, acc 0.8
2016-09-06T19:28:55.810819: step 343, loss 0.353233, acc 0.82
2016-09-06T19:28:56.472014: step 344, loss 0.22253, acc 0.94
2016-09-06T19:28:57.158543: step 345, loss 0.211141, acc 0.92
2016-09-06T19:28:57.834663: step 346, loss 0.534972, acc 0.74
2016-09-06T19:28:58.510828: step 347, loss 0.348964, acc 0.88
2016-09-06T19:28:59.209216: step 348, loss 0.500934, acc 0.8
2016-09-06T19:28:59.885744: step 349, loss 0.357007, acc 0.84
2016-09-06T19:29:00.624261: step 350, loss 0.24548, acc 0.94
2016-09-06T19:29:01.310000: step 351, loss 0.37606, acc 0.8
2016-09-06T19:29:01.993374: step 352, loss 0.403897, acc 0.78
2016-09-06T19:29:02.661519: step 353, loss 0.359506, acc 0.84
2016-09-06T19:29:03.353110: step 354, loss 0.517087, acc 0.72
2016-09-06T19:29:04.042906: step 355, loss 0.32883, acc 0.88
2016-09-06T19:29:04.720856: step 356, loss 0.490686, acc 0.8
2016-09-06T19:29:05.437068: step 357, loss 0.353445, acc 0.86
2016-09-06T19:29:06.098903: step 358, loss 0.271235, acc 0.88
2016-09-06T19:29:06.795540: step 359, loss 0.333807, acc 0.86
2016-09-06T19:29:07.486192: step 360, loss 0.499836, acc 0.68
2016-09-06T19:29:08.164140: step 361, loss 0.372509, acc 0.84
2016-09-06T19:29:08.852728: step 362, loss 0.321961, acc 0.82
2016-09-06T19:29:09.541282: step 363, loss 0.302646, acc 0.84
2016-09-06T19:29:10.269041: step 364, loss 0.388622, acc 0.84
2016-09-06T19:29:10.942184: step 365, loss 0.301952, acc 0.88
2016-09-06T19:29:11.647138: step 366, loss 0.395998, acc 0.84
2016-09-06T19:29:12.331970: step 367, loss 0.488404, acc 0.8
2016-09-06T19:29:13.011759: step 368, loss 0.386896, acc 0.82
2016-09-06T19:29:13.690755: step 369, loss 0.226822, acc 0.94
2016-09-06T19:29:14.364908: step 370, loss 0.382671, acc 0.84
2016-09-06T19:29:15.089386: step 371, loss 0.357577, acc 0.78
2016-09-06T19:29:15.773382: step 372, loss 0.256144, acc 0.88
2016-09-06T19:29:16.435255: step 373, loss 0.328005, acc 0.86
2016-09-06T19:29:17.110695: step 374, loss 0.321867, acc 0.82
2016-09-06T19:29:17.796027: step 375, loss 0.498199, acc 0.82
2016-09-06T19:29:18.498913: step 376, loss 0.475068, acc 0.7
2016-09-06T19:29:19.198545: step 377, loss 0.461415, acc 0.8
2016-09-06T19:29:19.909960: step 378, loss 0.420233, acc 0.78
2016-09-06T19:29:20.601050: step 379, loss 0.306899, acc 0.88
2016-09-06T19:29:21.292813: step 380, loss 0.474958, acc 0.8
2016-09-06T19:29:21.975342: step 381, loss 0.261565, acc 0.9
2016-09-06T19:29:22.661404: step 382, loss 0.273835, acc 0.92
2016-09-06T19:29:23.330770: step 383, loss 0.272437, acc 0.9
2016-09-06T19:29:23.959571: step 384, loss 0.293553, acc 0.931818
2016-09-06T19:29:24.691978: step 385, loss 0.294392, acc 0.82
2016-09-06T19:29:25.392333: step 386, loss 0.187263, acc 0.96
2016-09-06T19:29:26.097568: step 387, loss 0.246925, acc 0.9
2016-09-06T19:29:26.776075: step 388, loss 0.249392, acc 0.86
2016-09-06T19:29:27.459864: step 389, loss 0.224206, acc 0.88
2016-09-06T19:29:28.140673: step 390, loss 0.207805, acc 0.92
2016-09-06T19:29:28.815892: step 391, loss 0.15694, acc 0.94
2016-09-06T19:29:29.508225: step 392, loss 0.28369, acc 0.86
2016-09-06T19:29:30.175671: step 393, loss 0.174996, acc 0.96
2016-09-06T19:29:30.853645: step 394, loss 0.261885, acc 0.84
2016-09-06T19:29:31.544399: step 395, loss 0.0918397, acc 0.96
2016-09-06T19:29:32.204147: step 396, loss 0.384758, acc 0.84
2016-09-06T19:29:32.888621: step 397, loss 0.373417, acc 0.88
2016-09-06T19:29:33.572687: step 398, loss 0.413543, acc 0.8
2016-09-06T19:29:34.297845: step 399, loss 0.240448, acc 0.94
2016-09-06T19:29:34.963044: step 400, loss 0.199273, acc 0.96

Evaluation:
2016-09-06T19:29:38.091064: step 400, loss 0.466928, acc 0.806754

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-400

2016-09-06T19:29:39.687404: step 401, loss 0.121547, acc 0.96
2016-09-06T19:29:40.373829: step 402, loss 0.46044, acc 0.82
2016-09-06T19:29:41.062939: step 403, loss 0.354472, acc 0.86
2016-09-06T19:29:41.752424: step 404, loss 0.247553, acc 0.92
2016-09-06T19:29:42.457307: step 405, loss 0.309189, acc 0.84
2016-09-06T19:29:43.158786: step 406, loss 0.339534, acc 0.86
2016-09-06T19:29:43.853736: step 407, loss 0.20841, acc 0.96
2016-09-06T19:29:44.529198: step 408, loss 0.258114, acc 0.86
2016-09-06T19:29:45.242171: step 409, loss 0.202978, acc 0.92
2016-09-06T19:29:45.932086: step 410, loss 0.246906, acc 0.88
2016-09-06T19:29:46.611592: step 411, loss 0.16415, acc 0.96
2016-09-06T19:29:47.294596: step 412, loss 0.336162, acc 0.84
2016-09-06T19:29:47.984598: step 413, loss 0.237476, acc 0.94
2016-09-06T19:29:48.705692: step 414, loss 0.132621, acc 0.94
2016-09-06T19:29:49.377627: step 415, loss 0.381544, acc 0.84
2016-09-06T19:29:50.074652: step 416, loss 0.208945, acc 0.92
2016-09-06T19:29:50.766732: step 417, loss 0.322746, acc 0.86
2016-09-06T19:29:51.450812: step 418, loss 0.248129, acc 0.92
2016-09-06T19:29:52.136481: step 419, loss 0.217517, acc 0.98
2016-09-06T19:29:52.820444: step 420, loss 0.206442, acc 0.9
2016-09-06T19:29:53.511819: step 421, loss 0.131609, acc 0.94
2016-09-06T19:29:54.191591: step 422, loss 0.240469, acc 0.92
2016-09-06T19:29:54.891492: step 423, loss 0.128229, acc 0.96
2016-09-06T19:29:55.563781: step 424, loss 0.107045, acc 0.96
2016-09-06T19:29:56.253665: step 425, loss 0.207989, acc 0.92
2016-09-06T19:29:56.927288: step 426, loss 0.334821, acc 0.88
2016-09-06T19:29:57.602074: step 427, loss 0.268054, acc 0.86
2016-09-06T19:29:58.304335: step 428, loss 0.258257, acc 0.86
2016-09-06T19:29:58.962142: step 429, loss 0.264153, acc 0.88
2016-09-06T19:29:59.675552: step 430, loss 0.318665, acc 0.86
2016-09-06T19:30:00.378589: step 431, loss 0.27003, acc 0.92
2016-09-06T19:30:01.077400: step 432, loss 0.190425, acc 0.92
2016-09-06T19:30:01.771274: step 433, loss 0.197932, acc 0.96
2016-09-06T19:30:02.449249: step 434, loss 0.283885, acc 0.82
2016-09-06T19:30:03.167207: step 435, loss 0.25853, acc 0.92
2016-09-06T19:30:03.835402: step 436, loss 0.19386, acc 0.94
2016-09-06T19:30:04.529497: step 437, loss 0.17429, acc 0.94
2016-09-06T19:30:05.232354: step 438, loss 0.342444, acc 0.82
2016-09-06T19:30:05.918733: step 439, loss 0.31695, acc 0.8
2016-09-06T19:30:06.592175: step 440, loss 0.324462, acc 0.88
2016-09-06T19:30:07.275143: step 441, loss 0.194449, acc 0.94
2016-09-06T19:30:07.981840: step 442, loss 0.237197, acc 0.9
2016-09-06T19:30:08.656379: step 443, loss 0.231566, acc 0.9
2016-09-06T19:30:09.341094: step 444, loss 0.15137, acc 0.96
2016-09-06T19:30:10.026750: step 445, loss 0.277982, acc 0.86
2016-09-06T19:30:10.712639: step 446, loss 0.122501, acc 0.96
2016-09-06T19:30:11.404840: step 447, loss 0.241705, acc 0.88
2016-09-06T19:30:12.114029: step 448, loss 0.143448, acc 0.96
2016-09-06T19:30:12.805503: step 449, loss 0.159603, acc 0.96
2016-09-06T19:30:13.478501: step 450, loss 0.237116, acc 0.88
2016-09-06T19:30:14.200555: step 451, loss 0.140136, acc 0.96
2016-09-06T19:30:14.903658: step 452, loss 0.177588, acc 0.94
2016-09-06T19:30:15.580323: step 453, loss 0.292191, acc 0.9
2016-09-06T19:30:16.275784: step 454, loss 0.279417, acc 0.9
2016-09-06T19:30:16.964493: step 455, loss 0.376008, acc 0.88
2016-09-06T19:30:17.664300: step 456, loss 0.114271, acc 0.98
2016-09-06T19:30:18.315024: step 457, loss 0.198875, acc 0.96
2016-09-06T19:30:18.991564: step 458, loss 0.211146, acc 0.88
2016-09-06T19:30:19.658556: step 459, loss 0.274662, acc 0.84
2016-09-06T19:30:20.355494: step 460, loss 0.25958, acc 0.86
2016-09-06T19:30:21.041419: step 461, loss 0.170259, acc 0.92
2016-09-06T19:30:21.733638: step 462, loss 0.380827, acc 0.82
2016-09-06T19:30:22.431403: step 463, loss 0.318645, acc 0.92
2016-09-06T19:30:23.134631: step 464, loss 0.169569, acc 0.9
2016-09-06T19:30:23.825154: step 465, loss 0.170979, acc 0.92
2016-09-06T19:30:24.498537: step 466, loss 0.360994, acc 0.86
2016-09-06T19:30:25.172195: step 467, loss 0.36112, acc 0.82
2016-09-06T19:30:25.866479: step 468, loss 0.262287, acc 0.92
2016-09-06T19:30:26.554291: step 469, loss 0.250391, acc 0.9
2016-09-06T19:30:27.266282: step 470, loss 0.406426, acc 0.8
2016-09-06T19:30:27.946091: step 471, loss 0.317092, acc 0.88
2016-09-06T19:30:28.641882: step 472, loss 0.194371, acc 0.92
2016-09-06T19:30:29.335286: step 473, loss 0.17398, acc 0.96
2016-09-06T19:30:29.999388: step 474, loss 0.341573, acc 0.88
2016-09-06T19:30:30.673474: step 475, loss 0.267966, acc 0.9
2016-09-06T19:30:31.345960: step 476, loss 0.351432, acc 0.86
2016-09-06T19:30:32.052157: step 477, loss 0.215872, acc 0.9
2016-09-06T19:30:32.725752: step 478, loss 0.233209, acc 0.92
2016-09-06T19:30:33.417984: step 479, loss 0.213234, acc 0.88
2016-09-06T19:30:34.117197: step 480, loss 0.170389, acc 0.94
2016-09-06T19:30:34.803987: step 481, loss 0.23723, acc 0.94
2016-09-06T19:30:35.485258: step 482, loss 0.33956, acc 0.84
2016-09-06T19:30:36.169422: step 483, loss 0.672692, acc 0.82
2016-09-06T19:30:36.859727: step 484, loss 0.387857, acc 0.9
2016-09-06T19:30:37.549490: step 485, loss 0.37098, acc 0.88
2016-09-06T19:30:38.221988: step 486, loss 0.327476, acc 0.86
2016-09-06T19:30:38.917835: step 487, loss 0.251728, acc 0.88
2016-09-06T19:30:39.611348: step 488, loss 0.226814, acc 0.9
2016-09-06T19:30:40.291575: step 489, loss 0.238886, acc 0.88
2016-09-06T19:30:40.983467: step 490, loss 0.303789, acc 0.92
2016-09-06T19:30:41.704351: step 491, loss 0.254256, acc 0.88
2016-09-06T19:30:42.365977: step 492, loss 0.336152, acc 0.88
2016-09-06T19:30:43.051138: step 493, loss 0.278198, acc 0.86
2016-09-06T19:30:43.749137: step 494, loss 0.289065, acc 0.84
2016-09-06T19:30:44.425843: step 495, loss 0.203938, acc 0.88
2016-09-06T19:30:45.134922: step 496, loss 0.224842, acc 0.9
2016-09-06T19:30:45.814255: step 497, loss 0.206823, acc 0.92
2016-09-06T19:30:46.513587: step 498, loss 0.406811, acc 0.84
2016-09-06T19:30:47.168577: step 499, loss 0.248615, acc 0.86
2016-09-06T19:30:47.838837: step 500, loss 0.240668, acc 0.9

Evaluation:
2016-09-06T19:30:50.961010: step 500, loss 0.480241, acc 0.781426

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-500

2016-09-06T19:30:52.666073: step 501, loss 0.178668, acc 0.94
2016-09-06T19:30:53.362474: step 502, loss 0.328455, acc 0.86
2016-09-06T19:30:54.078149: step 503, loss 0.213317, acc 0.92
2016-09-06T19:30:54.761058: step 504, loss 0.161024, acc 0.92
2016-09-06T19:30:55.434727: step 505, loss 0.357715, acc 0.8
2016-09-06T19:30:56.147684: step 506, loss 0.331671, acc 0.84
2016-09-06T19:30:56.813845: step 507, loss 0.193083, acc 0.92
2016-09-06T19:30:57.489196: step 508, loss 0.242747, acc 0.92
2016-09-06T19:30:58.183657: step 509, loss 0.37503, acc 0.88
2016-09-06T19:30:58.880403: step 510, loss 0.276464, acc 0.92
2016-09-06T19:30:59.564927: step 511, loss 0.235104, acc 0.92
2016-09-06T19:31:00.272687: step 512, loss 0.193569, acc 0.94
2016-09-06T19:31:00.962848: step 513, loss 0.167271, acc 0.88
2016-09-06T19:31:01.618175: step 514, loss 0.281507, acc 0.86
2016-09-06T19:31:02.305309: step 515, loss 0.299077, acc 0.88
2016-09-06T19:31:02.989652: step 516, loss 0.278207, acc 0.84
2016-09-06T19:31:03.681485: step 517, loss 0.228742, acc 0.88
2016-09-06T19:31:04.361770: step 518, loss 0.203325, acc 0.94
2016-09-06T19:31:05.037827: step 519, loss 0.235094, acc 0.92
2016-09-06T19:31:05.729590: step 520, loss 0.291596, acc 0.92
2016-09-06T19:31:06.405451: step 521, loss 0.267764, acc 0.9
2016-09-06T19:31:07.082480: step 522, loss 0.129849, acc 0.94
2016-09-06T19:31:07.773831: step 523, loss 0.222717, acc 0.88
2016-09-06T19:31:08.435991: step 524, loss 0.264748, acc 0.88
2016-09-06T19:31:09.123660: step 525, loss 0.253998, acc 0.9
2016-09-06T19:31:09.794287: step 526, loss 0.140789, acc 0.96
2016-09-06T19:31:10.524782: step 527, loss 0.315413, acc 0.86
2016-09-06T19:31:11.206147: step 528, loss 0.149248, acc 0.98
2016-09-06T19:31:11.900465: step 529, loss 0.178969, acc 0.9
2016-09-06T19:31:12.580283: step 530, loss 0.284348, acc 0.9
2016-09-06T19:31:13.259086: step 531, loss 0.28986, acc 0.82
2016-09-06T19:31:13.950863: step 532, loss 0.280824, acc 0.88
2016-09-06T19:31:14.621516: step 533, loss 0.177328, acc 0.92
2016-09-06T19:31:15.323666: step 534, loss 0.191498, acc 0.9
2016-09-06T19:31:15.999028: step 535, loss 0.278172, acc 0.82
2016-09-06T19:31:16.683031: step 536, loss 0.379379, acc 0.82
2016-09-06T19:31:17.385995: step 537, loss 0.353149, acc 0.9
2016-09-06T19:31:18.061511: step 538, loss 0.42515, acc 0.84
2016-09-06T19:31:18.751885: step 539, loss 0.147501, acc 0.94
2016-09-06T19:31:19.438640: step 540, loss 0.223784, acc 0.9
2016-09-06T19:31:20.149185: step 541, loss 0.23042, acc 0.86
2016-09-06T19:31:20.833412: step 542, loss 0.261104, acc 0.9
2016-09-06T19:31:21.527157: step 543, loss 0.364797, acc 0.9
2016-09-06T19:31:22.210715: step 544, loss 0.18634, acc 0.9
2016-09-06T19:31:22.896726: step 545, loss 0.307205, acc 0.86
2016-09-06T19:31:23.588621: step 546, loss 0.353091, acc 0.82
2016-09-06T19:31:24.276142: step 547, loss 0.253141, acc 0.9
2016-09-06T19:31:24.989543: step 548, loss 0.32053, acc 0.86
2016-09-06T19:31:25.674863: step 549, loss 0.299374, acc 0.86
2016-09-06T19:31:26.388399: step 550, loss 0.338028, acc 0.86
2016-09-06T19:31:27.092112: step 551, loss 0.254645, acc 0.86
2016-09-06T19:31:27.780798: step 552, loss 0.38944, acc 0.8
2016-09-06T19:31:28.461650: step 553, loss 0.27047, acc 0.84
2016-09-06T19:31:29.118197: step 554, loss 0.222987, acc 0.86
2016-09-06T19:31:29.814921: step 555, loss 0.175579, acc 0.96
2016-09-06T19:31:30.480785: step 556, loss 0.239791, acc 0.9
2016-09-06T19:31:31.161474: step 557, loss 0.25265, acc 0.92
2016-09-06T19:31:31.850270: step 558, loss 0.258322, acc 0.9
2016-09-06T19:31:32.540953: step 559, loss 0.286724, acc 0.84
2016-09-06T19:31:33.232796: step 560, loss 0.190914, acc 0.94
2016-09-06T19:31:33.930065: step 561, loss 0.286931, acc 0.84
2016-09-06T19:31:34.628276: step 562, loss 0.251184, acc 0.88
2016-09-06T19:31:35.310843: step 563, loss 0.233414, acc 0.86
2016-09-06T19:31:35.997737: step 564, loss 0.158586, acc 0.94
2016-09-06T19:31:36.685849: step 565, loss 0.232674, acc 0.88
2016-09-06T19:31:37.369706: step 566, loss 0.160779, acc 0.94
2016-09-06T19:31:38.045309: step 567, loss 0.199652, acc 0.94
2016-09-06T19:31:38.718437: step 568, loss 0.281709, acc 0.84
2016-09-06T19:31:39.416823: step 569, loss 0.129134, acc 0.94
2016-09-06T19:31:40.086190: step 570, loss 0.199032, acc 0.94
2016-09-06T19:31:40.786868: step 571, loss 0.243037, acc 0.86
2016-09-06T19:31:41.469284: step 572, loss 0.22768, acc 0.88
2016-09-06T19:31:42.146231: step 573, loss 0.320579, acc 0.84
2016-09-06T19:31:42.831901: step 574, loss 0.314149, acc 0.88
2016-09-06T19:31:43.522859: step 575, loss 0.112596, acc 0.98
2016-09-06T19:31:44.171129: step 576, loss 0.375565, acc 0.886364
2016-09-06T19:31:44.843385: step 577, loss 0.0727811, acc 0.98
2016-09-06T19:31:45.568129: step 578, loss 0.15723, acc 0.92
2016-09-06T19:31:46.263459: step 579, loss 0.0638472, acc 1
2016-09-06T19:31:46.944446: step 580, loss 0.191403, acc 0.92
2016-09-06T19:31:47.604767: step 581, loss 0.0855365, acc 0.98
2016-09-06T19:31:48.307984: step 582, loss 0.334722, acc 0.88
2016-09-06T19:31:49.040776: step 583, loss 0.0546259, acc 1
2016-09-06T19:31:49.729565: step 584, loss 0.24134, acc 0.86
2016-09-06T19:31:50.419119: step 585, loss 0.0909502, acc 0.98
2016-09-06T19:31:51.087775: step 586, loss 0.218545, acc 0.96
2016-09-06T19:31:51.789247: step 587, loss 0.145432, acc 0.96
2016-09-06T19:31:52.487765: step 588, loss 0.100918, acc 0.96
2016-09-06T19:31:53.186249: step 589, loss 0.138418, acc 0.92
2016-09-06T19:31:53.914206: step 590, loss 0.0754821, acc 0.98
2016-09-06T19:31:54.607761: step 591, loss 0.0558788, acc 0.98
2016-09-06T19:31:55.295880: step 592, loss 0.163465, acc 0.94
2016-09-06T19:31:55.976442: step 593, loss 0.149714, acc 0.94
2016-09-06T19:31:56.677032: step 594, loss 0.0932181, acc 0.96
2016-09-06T19:31:57.361279: step 595, loss 0.259422, acc 0.92
2016-09-06T19:31:58.041027: step 596, loss 0.184584, acc 0.86
2016-09-06T19:31:58.746916: step 597, loss 0.144613, acc 0.9
2016-09-06T19:31:59.410476: step 598, loss 0.0892401, acc 0.96
2016-09-06T19:32:00.088160: step 599, loss 0.0346611, acc 0.98
2016-09-06T19:32:00.810705: step 600, loss 0.251693, acc 0.88

Evaluation:
2016-09-06T19:32:03.915603: step 600, loss 0.668484, acc 0.787054

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-600

2016-09-06T19:32:05.636484: step 601, loss 0.08252, acc 0.98
2016-09-06T19:32:06.334822: step 602, loss 0.283182, acc 0.92
2016-09-06T19:32:07.038044: step 603, loss 0.0672655, acc 0.98
2016-09-06T19:32:07.723022: step 604, loss 0.182377, acc 0.94
2016-09-06T19:32:08.433625: step 605, loss 0.160465, acc 0.94
2016-09-06T19:32:09.123551: step 606, loss 0.215007, acc 0.94
2016-09-06T19:32:09.833571: step 607, loss 0.104321, acc 0.96
2016-09-06T19:32:10.527192: step 608, loss 0.102642, acc 0.98
2016-09-06T19:32:11.213359: step 609, loss 0.400751, acc 0.9
2016-09-06T19:32:11.912775: step 610, loss 0.270901, acc 0.96
2016-09-06T19:32:12.587071: step 611, loss 0.115664, acc 0.94
2016-09-06T19:32:13.269787: step 612, loss 0.235246, acc 0.9
2016-09-06T19:32:13.962764: step 613, loss 0.174694, acc 0.94
2016-09-06T19:32:14.656794: step 614, loss 0.10086, acc 0.94
2016-09-06T19:32:15.350612: step 615, loss 0.143621, acc 0.92
2016-09-06T19:32:16.042625: step 616, loss 0.0691474, acc 1
2016-09-06T19:32:16.741183: step 617, loss 0.134229, acc 0.92
2016-09-06T19:32:17.416052: step 618, loss 0.13261, acc 0.96
2016-09-06T19:32:18.087129: step 619, loss 0.235034, acc 0.9
2016-09-06T19:32:18.788149: step 620, loss 0.115726, acc 0.94
2016-09-06T19:32:19.472994: step 621, loss 0.103499, acc 0.96
2016-09-06T19:32:20.161205: step 622, loss 0.222022, acc 0.88
2016-09-06T19:32:20.842217: step 623, loss 0.143671, acc 0.96
2016-09-06T19:32:21.566941: step 624, loss 0.307466, acc 0.88
2016-09-06T19:32:22.262306: step 625, loss 0.223793, acc 0.88
2016-09-06T19:32:22.932225: step 626, loss 0.172463, acc 0.92
2016-09-06T19:32:23.619719: step 627, loss 0.0677511, acc 0.98
2016-09-06T19:32:24.314885: step 628, loss 0.241145, acc 0.88
2016-09-06T19:32:25.017640: step 629, loss 0.105743, acc 0.98
2016-09-06T19:32:25.689807: step 630, loss 0.137007, acc 0.96
2016-09-06T19:32:26.410856: step 631, loss 0.25395, acc 0.88
2016-09-06T19:32:27.082009: step 632, loss 0.0729882, acc 1
2016-09-06T19:32:27.770602: step 633, loss 0.0772939, acc 1
2016-09-06T19:32:28.455064: step 634, loss 0.267122, acc 0.94
2016-09-06T19:32:29.151327: step 635, loss 0.124171, acc 0.94
2016-09-06T19:32:29.850357: step 636, loss 0.0715421, acc 0.98
2016-09-06T19:32:30.518320: step 637, loss 0.165749, acc 0.92
2016-09-06T19:32:31.233773: step 638, loss 0.118195, acc 0.92
2016-09-06T19:32:31.940147: step 639, loss 0.0927979, acc 0.98
2016-09-06T19:32:32.616515: step 640, loss 0.133653, acc 0.94
2016-09-06T19:32:33.301927: step 641, loss 0.123869, acc 0.98
2016-09-06T19:32:34.000383: step 642, loss 0.178078, acc 0.94
2016-09-06T19:32:34.693325: step 643, loss 0.127067, acc 0.98
2016-09-06T19:32:35.364748: step 644, loss 0.104924, acc 0.96
2016-09-06T19:32:36.080001: step 645, loss 0.154612, acc 0.94
2016-09-06T19:32:36.769374: step 646, loss 0.089867, acc 0.98
2016-09-06T19:32:37.451936: step 647, loss 0.232165, acc 0.9
2016-09-06T19:32:38.129257: step 648, loss 0.165106, acc 0.94
2016-09-06T19:32:38.819198: step 649, loss 0.188977, acc 0.92
2016-09-06T19:32:39.520385: step 650, loss 0.198067, acc 0.88
2016-09-06T19:32:40.183958: step 651, loss 0.0948886, acc 0.96
2016-09-06T19:32:40.888776: step 652, loss 0.114356, acc 0.92
2016-09-06T19:32:41.584636: step 653, loss 0.230657, acc 0.9
2016-09-06T19:32:42.261458: step 654, loss 0.379469, acc 0.86
2016-09-06T19:32:42.966091: step 655, loss 0.188276, acc 0.92
2016-09-06T19:32:43.657029: step 656, loss 0.150608, acc 0.94
2016-09-06T19:32:44.357898: step 657, loss 0.191867, acc 0.92
2016-09-06T19:32:45.037265: step 658, loss 0.158363, acc 0.96
2016-09-06T19:32:45.720389: step 659, loss 0.10783, acc 0.96
2016-09-06T19:32:46.401280: step 660, loss 0.136786, acc 0.94
2016-09-06T19:32:47.089298: step 661, loss 0.175909, acc 0.9
2016-09-06T19:32:47.779165: step 662, loss 0.227481, acc 0.88
2016-09-06T19:32:48.460076: step 663, loss 0.245504, acc 0.88
2016-09-06T19:32:49.156069: step 664, loss 0.139277, acc 0.9
2016-09-06T19:32:49.809981: step 665, loss 0.105625, acc 0.96
2016-09-06T19:32:50.512243: step 666, loss 0.284328, acc 0.88
2016-09-06T19:32:51.198874: step 667, loss 0.307303, acc 0.92
2016-09-06T19:32:51.877972: step 668, loss 0.281279, acc 0.88
2016-09-06T19:32:52.559905: step 669, loss 0.0527663, acc 0.98
2016-09-06T19:32:53.266647: step 670, loss 0.281008, acc 0.86
2016-09-06T19:32:54.009958: step 671, loss 0.170504, acc 0.92
2016-09-06T19:32:54.690869: step 672, loss 0.24052, acc 0.92
2016-09-06T19:32:55.391648: step 673, loss 0.113449, acc 0.96
2016-09-06T19:32:56.082137: step 674, loss 0.115883, acc 0.98
2016-09-06T19:32:56.781040: step 675, loss 0.169613, acc 0.96
2016-09-06T19:32:57.468584: step 676, loss 0.288726, acc 0.86
2016-09-06T19:32:58.141116: step 677, loss 0.173791, acc 0.94
2016-09-06T19:32:58.825714: step 678, loss 0.136681, acc 0.92
2016-09-06T19:32:59.497093: step 679, loss 0.200397, acc 0.94
2016-09-06T19:33:00.199930: step 680, loss 0.11899, acc 0.96
2016-09-06T19:33:00.896941: step 681, loss 0.127266, acc 0.94
2016-09-06T19:33:01.584565: step 682, loss 0.0709443, acc 0.98
2016-09-06T19:33:02.258983: step 683, loss 0.213065, acc 0.92
2016-09-06T19:33:02.958799: step 684, loss 0.24344, acc 0.9
2016-09-06T19:33:03.659649: step 685, loss 0.198442, acc 0.92
2016-09-06T19:33:04.331976: step 686, loss 0.392736, acc 0.88
2016-09-06T19:33:05.020164: step 687, loss 0.157315, acc 0.92
2016-09-06T19:33:05.698000: step 688, loss 0.232588, acc 0.88
2016-09-06T19:33:06.399109: step 689, loss 0.154676, acc 0.92
2016-09-06T19:33:07.091094: step 690, loss 0.348022, acc 0.82
2016-09-06T19:33:07.778549: step 691, loss 0.201124, acc 0.88
2016-09-06T19:33:08.476614: step 692, loss 0.146446, acc 0.92
2016-09-06T19:33:09.148924: step 693, loss 0.154816, acc 0.92
2016-09-06T19:33:09.815416: step 694, loss 0.132258, acc 0.96
2016-09-06T19:33:10.504163: step 695, loss 0.184754, acc 0.92
2016-09-06T19:33:11.190318: step 696, loss 0.162788, acc 0.92
2016-09-06T19:33:11.866090: step 697, loss 0.314939, acc 0.86
2016-09-06T19:33:12.562655: step 698, loss 0.409423, acc 0.86
2016-09-06T19:33:13.254661: step 699, loss 0.0951252, acc 0.98
2016-09-06T19:33:13.926876: step 700, loss 0.24032, acc 0.86

Evaluation:
2016-09-06T19:33:17.098246: step 700, loss 0.592458, acc 0.777674

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-700

2016-09-06T19:33:18.770577: step 701, loss 0.174148, acc 0.92
2016-09-06T19:33:19.486214: step 702, loss 0.314875, acc 0.88
2016-09-06T19:33:20.211340: step 703, loss 0.0741748, acc 0.96
2016-09-06T19:33:20.914833: step 704, loss 0.195138, acc 0.9
2016-09-06T19:33:21.655386: step 705, loss 0.210672, acc 0.94
2016-09-06T19:33:22.336747: step 706, loss 0.124966, acc 0.94
2016-09-06T19:33:23.014477: step 707, loss 0.185806, acc 0.96
2016-09-06T19:33:23.712431: step 708, loss 0.188625, acc 0.92
2016-09-06T19:33:24.395097: step 709, loss 0.146856, acc 0.9
2016-09-06T19:33:25.083174: step 710, loss 0.110034, acc 0.96
2016-09-06T19:33:25.767434: step 711, loss 0.153963, acc 0.92
2016-09-06T19:33:26.486671: step 712, loss 0.205857, acc 0.88
2016-09-06T19:33:27.164020: step 713, loss 0.338873, acc 0.88
2016-09-06T19:33:27.836883: step 714, loss 0.0927502, acc 0.98
2016-09-06T19:33:28.521892: step 715, loss 0.133918, acc 0.94
2016-09-06T19:33:29.196411: step 716, loss 0.197398, acc 0.92
2016-09-06T19:33:29.895373: step 717, loss 0.184587, acc 0.94
2016-09-06T19:33:30.582793: step 718, loss 0.18933, acc 0.94
2016-09-06T19:33:31.310751: step 719, loss 0.269545, acc 0.86
2016-09-06T19:33:32.005813: step 720, loss 0.288342, acc 0.86
2016-09-06T19:33:32.704125: step 721, loss 0.239035, acc 0.9
2016-09-06T19:33:33.383425: step 722, loss 0.113347, acc 0.94
2016-09-06T19:33:34.077841: step 723, loss 0.135024, acc 0.96
2016-09-06T19:33:34.782550: step 724, loss 0.165679, acc 0.94
2016-09-06T19:33:35.446002: step 725, loss 0.142867, acc 0.96
2016-09-06T19:33:36.149483: step 726, loss 0.206033, acc 0.88
2016-09-06T19:33:36.833898: step 727, loss 0.187825, acc 0.94
2016-09-06T19:33:37.522207: step 728, loss 0.240548, acc 0.94
2016-09-06T19:33:38.197682: step 729, loss 0.165323, acc 0.96
2016-09-06T19:33:38.898120: step 730, loss 0.134951, acc 0.96
2016-09-06T19:33:39.603912: step 731, loss 0.248279, acc 0.86
2016-09-06T19:33:40.276413: step 732, loss 0.201663, acc 0.9
2016-09-06T19:33:41.002349: step 733, loss 0.127923, acc 0.96
2016-09-06T19:33:41.686131: step 734, loss 0.252491, acc 0.86
2016-09-06T19:33:42.390679: step 735, loss 0.138321, acc 0.94
2016-09-06T19:33:43.065938: step 736, loss 0.181196, acc 0.92
2016-09-06T19:33:43.772151: step 737, loss 0.0896658, acc 0.94
2016-09-06T19:33:44.485507: step 738, loss 0.106408, acc 0.98
2016-09-06T19:33:45.181294: step 739, loss 0.194638, acc 0.94
2016-09-06T19:33:45.866116: step 740, loss 0.249306, acc 0.88
2016-09-06T19:33:46.556551: step 741, loss 0.132031, acc 0.94
2016-09-06T19:33:47.263763: step 742, loss 0.168554, acc 0.9
2016-09-06T19:33:47.948031: step 743, loss 0.166688, acc 0.94
2016-09-06T19:33:48.632586: step 744, loss 0.255525, acc 0.86
2016-09-06T19:33:49.322785: step 745, loss 0.214398, acc 0.9
2016-09-06T19:33:49.992098: step 746, loss 0.201196, acc 0.92
2016-09-06T19:33:50.674187: step 747, loss 0.203564, acc 0.9
2016-09-06T19:33:51.375906: step 748, loss 0.150922, acc 0.92
2016-09-06T19:33:52.067800: step 749, loss 0.205892, acc 0.92
2016-09-06T19:33:52.774083: step 750, loss 0.261152, acc 0.84
2016-09-06T19:33:53.445402: step 751, loss 0.0528702, acc 1
2016-09-06T19:33:54.158995: step 752, loss 0.143703, acc 0.94
2016-09-06T19:33:54.848297: step 753, loss 0.178174, acc 0.92
2016-09-06T19:33:55.540775: step 754, loss 0.1204, acc 0.94
2016-09-06T19:33:56.238487: step 755, loss 0.0843517, acc 0.98
2016-09-06T19:33:56.914000: step 756, loss 0.180289, acc 0.92
2016-09-06T19:33:57.608959: step 757, loss 0.16636, acc 0.96
2016-09-06T19:33:58.279904: step 758, loss 0.143749, acc 0.92
2016-09-06T19:33:58.982004: step 759, loss 0.433822, acc 0.88
2016-09-06T19:33:59.673372: step 760, loss 0.08253, acc 0.96
2016-09-06T19:34:00.412084: step 761, loss 0.293771, acc 0.88
2016-09-06T19:34:01.096389: step 762, loss 0.110378, acc 0.96
2016-09-06T19:34:01.772568: step 763, loss 0.263785, acc 0.88
2016-09-06T19:34:02.462796: step 764, loss 0.130902, acc 0.96
2016-09-06T19:34:03.136358: step 765, loss 0.213927, acc 0.92
2016-09-06T19:34:03.852510: step 766, loss 0.119874, acc 0.96
2016-09-06T19:34:04.533156: step 767, loss 0.260276, acc 0.88
2016-09-06T19:34:05.180802: step 768, loss 0.247057, acc 0.909091
2016-09-06T19:34:05.885132: step 769, loss 0.130042, acc 0.92
2016-09-06T19:34:06.572172: step 770, loss 0.268338, acc 0.9
2016-09-06T19:34:07.313867: step 771, loss 0.224627, acc 0.92
2016-09-06T19:34:08.005530: step 772, loss 0.120565, acc 0.98
2016-09-06T19:34:08.680589: step 773, loss 0.0341424, acc 1
2016-09-06T19:34:09.359787: step 774, loss 0.104754, acc 0.94
2016-09-06T19:34:10.039443: step 775, loss 0.12525, acc 0.94
2016-09-06T19:34:10.725829: step 776, loss 0.159768, acc 0.92
2016-09-06T19:34:11.438488: step 777, loss 0.0892053, acc 0.98
2016-09-06T19:34:12.154238: step 778, loss 0.189611, acc 0.9
2016-09-06T19:34:12.841921: step 779, loss 0.149964, acc 0.92
2016-09-06T19:34:13.520989: step 780, loss 0.162205, acc 0.94
2016-09-06T19:34:14.202305: step 781, loss 0.145743, acc 0.94
2016-09-06T19:34:14.872121: step 782, loss 0.199715, acc 0.92
2016-09-06T19:34:15.552734: step 783, loss 0.134163, acc 0.92
2016-09-06T19:34:16.222242: step 784, loss 0.0357326, acc 1
2016-09-06T19:34:16.898408: step 785, loss 0.111775, acc 0.94
2016-09-06T19:34:17.567869: step 786, loss 0.0797008, acc 0.98
2016-09-06T19:34:18.274345: step 787, loss 0.194621, acc 0.9
2016-09-06T19:34:18.987918: step 788, loss 0.108629, acc 0.96
2016-09-06T19:34:19.683743: step 789, loss 0.157754, acc 0.92
2016-09-06T19:34:20.390343: step 790, loss 0.0486818, acc 0.98
2016-09-06T19:34:21.088490: step 791, loss 0.0577936, acc 0.98
2016-09-06T19:34:21.803393: step 792, loss 0.152377, acc 0.96
2016-09-06T19:34:22.486136: step 793, loss 0.0721061, acc 0.98
2016-09-06T19:34:23.175894: step 794, loss 0.155478, acc 0.92
2016-09-06T19:34:23.853614: step 795, loss 0.22065, acc 0.88
2016-09-06T19:34:24.555076: step 796, loss 0.129003, acc 0.92
2016-09-06T19:34:25.239012: step 797, loss 0.0667978, acc 0.98
2016-09-06T19:34:25.922820: step 798, loss 0.150495, acc 0.96
2016-09-06T19:34:26.631555: step 799, loss 0.0311512, acc 1
2016-09-06T19:34:27.313000: step 800, loss 0.131793, acc 0.94

Evaluation:
2016-09-06T19:34:30.458275: step 800, loss 0.780044, acc 0.774859

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-800

2016-09-06T19:34:32.208635: step 801, loss 0.0425103, acc 1
2016-09-06T19:34:32.919430: step 802, loss 0.239168, acc 0.92
2016-09-06T19:34:33.618059: step 803, loss 0.0794838, acc 0.98
2016-09-06T19:34:34.316477: step 804, loss 0.161678, acc 0.9
2016-09-06T19:34:35.025651: step 805, loss 0.136407, acc 0.94
2016-09-06T19:34:35.685718: step 806, loss 0.317689, acc 0.88
2016-09-06T19:34:36.371912: step 807, loss 0.0570362, acc 0.96
2016-09-06T19:34:37.085109: step 808, loss 0.0484809, acc 0.98
2016-09-06T19:34:37.779253: step 809, loss 0.123218, acc 0.96
2016-09-06T19:34:38.474844: step 810, loss 0.1462, acc 0.94
2016-09-06T19:34:39.141011: step 811, loss 0.120551, acc 0.94
2016-09-06T19:34:39.846827: step 812, loss 0.0613712, acc 0.98
2016-09-06T19:34:40.511145: step 813, loss 0.135449, acc 0.94
2016-09-06T19:34:41.173640: step 814, loss 0.155568, acc 0.94
2016-09-06T19:34:41.861940: step 815, loss 0.0721333, acc 0.96
2016-09-06T19:34:42.555503: step 816, loss 0.158489, acc 0.94
2016-09-06T19:34:43.242433: step 817, loss 0.146087, acc 0.92
2016-09-06T19:34:43.932138: step 818, loss 0.06927, acc 0.98
2016-09-06T19:34:44.633262: step 819, loss 0.105614, acc 0.96
2016-09-06T19:34:45.300098: step 820, loss 0.0768614, acc 0.98
2016-09-06T19:34:45.984859: step 821, loss 0.0776983, acc 0.98
2016-09-06T19:34:46.672684: step 822, loss 0.0749706, acc 0.96
2016-09-06T19:34:47.371215: step 823, loss 0.208003, acc 0.9
2016-09-06T19:34:48.032839: step 824, loss 0.138939, acc 0.92
2016-09-06T19:34:48.729551: step 825, loss 0.0517206, acc 0.98
2016-09-06T19:34:49.440308: step 826, loss 0.116918, acc 0.94
2016-09-06T19:34:50.124936: step 827, loss 0.224435, acc 0.9
2016-09-06T19:34:50.836424: step 828, loss 0.163532, acc 0.9
2016-09-06T19:34:51.512454: step 829, loss 0.174785, acc 0.96
2016-09-06T19:34:52.184857: step 830, loss 0.0628583, acc 0.96
2016-09-06T19:34:52.854084: step 831, loss 0.134368, acc 0.94
2016-09-06T19:34:53.541057: step 832, loss 0.0704973, acc 0.98
2016-09-06T19:34:54.253324: step 833, loss 0.128717, acc 0.92
2016-09-06T19:34:54.956728: step 834, loss 0.17803, acc 0.94
2016-09-06T19:34:55.633638: step 835, loss 0.219978, acc 0.9
2016-09-06T19:34:56.321449: step 836, loss 0.0808229, acc 0.98
2016-09-06T19:34:56.995826: step 837, loss 0.187002, acc 0.9
2016-09-06T19:34:57.685697: step 838, loss 0.142367, acc 0.96
2016-09-06T19:34:58.372385: step 839, loss 0.0605628, acc 0.96
2016-09-06T19:34:59.077316: step 840, loss 0.205931, acc 0.9
2016-09-06T19:34:59.754552: step 841, loss 0.045638, acc 0.98
2016-09-06T19:35:00.466033: step 842, loss 0.0885737, acc 0.98
2016-09-06T19:35:01.158957: step 843, loss 0.0949929, acc 0.96
2016-09-06T19:35:01.854391: step 844, loss 0.0436853, acc 0.96
2016-09-06T19:35:02.547461: step 845, loss 0.190441, acc 0.88
2016-09-06T19:35:03.218452: step 846, loss 0.0496121, acc 0.96
2016-09-06T19:35:03.929669: step 847, loss 0.0639458, acc 0.98
2016-09-06T19:35:04.608718: step 848, loss 0.140105, acc 0.96
2016-09-06T19:35:05.300767: step 849, loss 0.0528711, acc 1
2016-09-06T19:35:05.995951: step 850, loss 0.143319, acc 0.96
2016-09-06T19:35:06.691916: step 851, loss 0.0609201, acc 0.98
2016-09-06T19:35:07.392822: step 852, loss 0.159864, acc 0.94
2016-09-06T19:35:08.061344: step 853, loss 0.112727, acc 0.96
2016-09-06T19:35:08.764017: step 854, loss 0.0788678, acc 0.96
2016-09-06T19:35:09.462088: step 855, loss 0.0581474, acc 1
2016-09-06T19:35:10.165437: step 856, loss 0.195089, acc 0.9
2016-09-06T19:35:10.835720: step 857, loss 0.142961, acc 0.92
2016-09-06T19:35:11.514787: step 858, loss 0.0762678, acc 0.96
2016-09-06T19:35:12.205696: step 859, loss 0.12765, acc 0.96
2016-09-06T19:35:12.869844: step 860, loss 0.10621, acc 0.96
2016-09-06T19:35:13.562467: step 861, loss 0.149066, acc 0.92
2016-09-06T19:35:14.259757: step 862, loss 0.0395059, acc 1
2016-09-06T19:35:14.951626: step 863, loss 0.271926, acc 0.9
2016-09-06T19:35:15.661855: step 864, loss 0.0939151, acc 0.94
2016-09-06T19:35:16.347227: step 865, loss 0.0730863, acc 0.94
2016-09-06T19:35:17.040764: step 866, loss 0.123057, acc 0.92
2016-09-06T19:35:17.709681: step 867, loss 0.123613, acc 0.94
2016-09-06T19:35:18.398344: step 868, loss 0.0705457, acc 0.96
2016-09-06T19:35:19.094022: step 869, loss 0.219454, acc 0.94
2016-09-06T19:35:19.778670: step 870, loss 0.0546515, acc 0.98
2016-09-06T19:35:20.451428: step 871, loss 0.163162, acc 0.96
2016-09-06T19:35:21.132975: step 872, loss 0.0847754, acc 0.98
2016-09-06T19:35:21.810383: step 873, loss 0.0518042, acc 0.98
2016-09-06T19:35:22.477457: step 874, loss 0.123434, acc 0.96
2016-09-06T19:35:23.182466: step 875, loss 0.197619, acc 0.9
2016-09-06T19:35:23.870978: step 876, loss 0.112876, acc 0.94
2016-09-06T19:35:24.574208: step 877, loss 0.186333, acc 0.88
2016-09-06T19:35:25.255970: step 878, loss 0.155144, acc 0.92
2016-09-06T19:35:25.935450: step 879, loss 0.103133, acc 0.98
2016-09-06T19:35:26.632548: step 880, loss 0.0998728, acc 0.98
2016-09-06T19:35:27.296746: step 881, loss 0.215675, acc 0.9
2016-09-06T19:35:27.993655: step 882, loss 0.147637, acc 0.96
2016-09-06T19:35:28.670932: step 883, loss 0.0632778, acc 0.98
2016-09-06T19:35:29.356278: step 884, loss 0.199098, acc 0.9
2016-09-06T19:35:30.056549: step 885, loss 0.074728, acc 0.98
2016-09-06T19:35:30.741361: step 886, loss 0.194947, acc 0.98
2016-09-06T19:35:31.427518: step 887, loss 0.195572, acc 0.9
2016-09-06T19:35:32.091004: step 888, loss 0.107021, acc 0.94
2016-09-06T19:35:32.784895: step 889, loss 0.149481, acc 0.94
2016-09-06T19:35:33.476725: step 890, loss 0.100311, acc 0.94
2016-09-06T19:35:34.141372: step 891, loss 0.174681, acc 0.92
2016-09-06T19:35:34.816649: step 892, loss 0.167742, acc 0.94
2016-09-06T19:35:35.496999: step 893, loss 0.154612, acc 0.94
2016-09-06T19:35:36.204820: step 894, loss 0.141013, acc 0.94
2016-09-06T19:35:36.865041: step 895, loss 0.197339, acc 0.88
2016-09-06T19:35:37.574486: step 896, loss 0.165784, acc 0.98
2016-09-06T19:35:38.248738: step 897, loss 0.103075, acc 0.96
2016-09-06T19:35:38.938303: step 898, loss 0.258206, acc 0.88
2016-09-06T19:35:39.602955: step 899, loss 0.224968, acc 0.88
2016-09-06T19:35:40.313375: step 900, loss 0.102073, acc 0.98

Evaluation:
2016-09-06T19:35:43.470349: step 900, loss 0.724175, acc 0.769231

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-900

2016-09-06T19:35:45.180431: step 901, loss 0.108102, acc 0.94
2016-09-06T19:35:45.870278: step 902, loss 0.137109, acc 0.94
2016-09-06T19:35:46.528810: step 903, loss 0.111232, acc 0.96
2016-09-06T19:35:47.232403: step 904, loss 0.124148, acc 0.94
2016-09-06T19:35:47.910011: step 905, loss 0.0685467, acc 0.96
2016-09-06T19:35:48.591734: step 906, loss 0.107963, acc 0.96
2016-09-06T19:35:49.273888: step 907, loss 0.106739, acc 0.94
2016-09-06T19:35:49.954499: step 908, loss 0.30098, acc 0.92
2016-09-06T19:35:50.634237: step 909, loss 0.112089, acc 0.94
2016-09-06T19:35:51.299715: step 910, loss 0.0668952, acc 0.94
2016-09-06T19:35:52.006765: step 911, loss 0.21604, acc 0.92
2016-09-06T19:35:52.675620: step 912, loss 0.138853, acc 0.9
2016-09-06T19:35:53.358025: step 913, loss 0.0590652, acc 0.98
2016-09-06T19:35:54.062257: step 914, loss 0.100736, acc 0.98
2016-09-06T19:35:54.767218: step 915, loss 0.170053, acc 0.9
2016-09-06T19:35:55.455047: step 916, loss 0.194216, acc 0.9
2016-09-06T19:35:56.128795: step 917, loss 0.0698722, acc 0.98
2016-09-06T19:35:56.831430: step 918, loss 0.10101, acc 0.96
2016-09-06T19:35:57.501224: step 919, loss 0.203645, acc 0.92
2016-09-06T19:35:58.178301: step 920, loss 0.0652125, acc 0.96
2016-09-06T19:35:58.857632: step 921, loss 0.127907, acc 0.92
2016-09-06T19:35:59.550636: step 922, loss 0.235283, acc 0.92
2016-09-06T19:36:00.301554: step 923, loss 0.0865811, acc 0.96
2016-09-06T19:36:00.952811: step 924, loss 0.0885421, acc 0.96
2016-09-06T19:36:01.639131: step 925, loss 0.250246, acc 0.9
2016-09-06T19:36:02.328760: step 926, loss 0.120155, acc 0.96
2016-09-06T19:36:03.012036: step 927, loss 0.0474393, acc 1
2016-09-06T19:36:03.700613: step 928, loss 0.285086, acc 0.88
2016-09-06T19:36:04.377089: step 929, loss 0.149214, acc 0.92
2016-09-06T19:36:05.078870: step 930, loss 0.119813, acc 0.94
2016-09-06T19:36:05.763280: step 931, loss 0.07237, acc 0.96
2016-09-06T19:36:06.471187: step 932, loss 0.0648266, acc 0.98
2016-09-06T19:36:07.134599: step 933, loss 0.15038, acc 0.94
2016-09-06T19:36:07.809652: step 934, loss 0.0902141, acc 0.96
2016-09-06T19:36:08.490566: step 935, loss 0.109163, acc 0.96
2016-09-06T19:36:09.178300: step 936, loss 0.325773, acc 0.92
2016-09-06T19:36:09.880047: step 937, loss 0.16427, acc 0.94
2016-09-06T19:36:10.536179: step 938, loss 0.168721, acc 0.9
2016-09-06T19:36:11.250243: step 939, loss 0.190026, acc 0.94
2016-09-06T19:36:11.927139: step 940, loss 0.094334, acc 0.98
2016-09-06T19:36:12.607916: step 941, loss 0.118815, acc 0.96
2016-09-06T19:36:13.269657: step 942, loss 0.178924, acc 0.94
2016-09-06T19:36:13.954465: step 943, loss 0.0808479, acc 0.98
2016-09-06T19:36:14.648108: step 944, loss 0.153877, acc 0.92
2016-09-06T19:36:15.364154: step 945, loss 0.106836, acc 0.96
2016-09-06T19:36:16.090567: step 946, loss 0.244794, acc 0.88
2016-09-06T19:36:16.762626: step 947, loss 0.140618, acc 0.94
2016-09-06T19:36:17.440156: step 948, loss 0.0974022, acc 0.96
2016-09-06T19:36:18.141035: step 949, loss 0.151735, acc 0.96
2016-09-06T19:36:18.847423: step 950, loss 0.274891, acc 0.88
2016-09-06T19:36:19.543686: step 951, loss 0.103915, acc 0.96
2016-09-06T19:36:20.212811: step 952, loss 0.138085, acc 0.9
2016-09-06T19:36:20.923790: step 953, loss 0.154823, acc 0.94
2016-09-06T19:36:21.618828: step 954, loss 0.154834, acc 0.92
2016-09-06T19:36:22.327905: step 955, loss 0.0793938, acc 1
2016-09-06T19:36:23.010128: step 956, loss 0.141605, acc 0.9
2016-09-06T19:36:23.708694: step 957, loss 0.117332, acc 0.94
2016-09-06T19:36:24.409202: step 958, loss 0.132361, acc 0.94
2016-09-06T19:36:25.074248: step 959, loss 0.13616, acc 0.96
2016-09-06T19:36:25.746827: step 960, loss 0.0514775, acc 0.977273
2016-09-06T19:36:26.433493: step 961, loss 0.115386, acc 0.94
2016-09-06T19:36:27.142524: step 962, loss 0.0688472, acc 0.98
2016-09-06T19:36:27.891303: step 963, loss 0.076781, acc 0.98
2016-09-06T19:36:28.570524: step 964, loss 0.0473723, acc 0.98
2016-09-06T19:36:29.276960: step 965, loss 0.162218, acc 0.94
2016-09-06T19:36:29.953414: step 966, loss 0.185774, acc 0.92
2016-09-06T19:36:30.638218: step 967, loss 0.0281523, acc 0.98
2016-09-06T19:36:31.329725: step 968, loss 0.071135, acc 0.98
2016-09-06T19:36:32.023521: step 969, loss 0.0716034, acc 0.96
2016-09-06T19:36:32.721538: step 970, loss 0.0525348, acc 0.98
2016-09-06T19:36:33.396062: step 971, loss 0.0341773, acc 0.98
2016-09-06T19:36:34.084219: step 972, loss 0.071603, acc 0.94
2016-09-06T19:36:34.755313: step 973, loss 0.0748418, acc 0.96
2016-09-06T19:36:35.436987: step 974, loss 0.127904, acc 0.92
2016-09-06T19:36:36.110824: step 975, loss 0.131855, acc 0.96
2016-09-06T19:36:36.796586: step 976, loss 0.0255122, acc 1
2016-09-06T19:36:37.487779: step 977, loss 0.0848888, acc 0.94
2016-09-06T19:36:38.211747: step 978, loss 0.0687317, acc 0.96
2016-09-06T19:36:38.917391: step 979, loss 0.0807175, acc 0.94
2016-09-06T19:36:39.584356: step 980, loss 0.0241588, acc 0.98
2016-09-06T19:36:40.268579: step 981, loss 0.0285369, acc 1
2016-09-06T19:36:40.961367: step 982, loss 0.0623118, acc 0.94
2016-09-06T19:36:41.648296: step 983, loss 0.0661848, acc 0.96
2016-09-06T19:36:42.347149: step 984, loss 0.0482914, acc 0.98
2016-09-06T19:36:43.007870: step 985, loss 0.176089, acc 0.94
2016-09-06T19:36:43.706862: step 986, loss 0.129769, acc 0.96
2016-09-06T19:36:44.389613: step 987, loss 0.134109, acc 0.96
2016-09-06T19:36:45.070828: step 988, loss 0.109565, acc 0.98
2016-09-06T19:36:45.752770: step 989, loss 0.0936781, acc 0.96
2016-09-06T19:36:46.456950: step 990, loss 0.072529, acc 0.96
2016-09-06T19:36:47.152441: step 991, loss 0.107345, acc 0.96
2016-09-06T19:36:47.824865: step 992, loss 0.0502195, acc 0.98
2016-09-06T19:36:48.528697: step 993, loss 0.0801284, acc 0.96
2016-09-06T19:36:49.186888: step 994, loss 0.0644283, acc 0.98
2016-09-06T19:36:49.876852: step 995, loss 0.0623148, acc 0.98
2016-09-06T19:36:50.572793: step 996, loss 0.0578264, acc 0.98
2016-09-06T19:36:51.258968: step 997, loss 0.158739, acc 0.96
2016-09-06T19:36:51.952592: step 998, loss 0.105496, acc 0.94
2016-09-06T19:36:52.636661: step 999, loss 0.0502488, acc 0.98
2016-09-06T19:36:53.352366: step 1000, loss 0.0414605, acc 1

Evaluation:
2016-09-06T19:36:56.514833: step 1000, loss 1.03807, acc 0.761726

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-1000

2016-09-06T19:36:58.206155: step 1001, loss 0.123525, acc 0.94
2016-09-06T19:36:58.900735: step 1002, loss 0.104367, acc 0.96
2016-09-06T19:36:59.601228: step 1003, loss 0.196604, acc 0.92
2016-09-06T19:37:00.297940: step 1004, loss 0.080947, acc 0.96
2016-09-06T19:37:00.987064: step 1005, loss 0.0900027, acc 0.94
2016-09-06T19:37:01.705848: step 1006, loss 0.211093, acc 0.94
2016-09-06T19:37:02.369425: step 1007, loss 0.021519, acc 1
2016-09-06T19:37:03.054270: step 1008, loss 0.0830838, acc 0.98
2016-09-06T19:37:03.746457: step 1009, loss 0.0685831, acc 0.98
2016-09-06T19:37:04.425993: step 1010, loss 0.0398998, acc 1
2016-09-06T19:37:05.131160: step 1011, loss 0.0523624, acc 0.98
2016-09-06T19:37:05.834981: step 1012, loss 0.117171, acc 0.94
2016-09-06T19:37:06.535337: step 1013, loss 0.183144, acc 0.94
2016-09-06T19:37:07.221609: step 1014, loss 0.131191, acc 0.96
2016-09-06T19:37:07.899379: step 1015, loss 0.162967, acc 0.96
2016-09-06T19:37:08.576229: step 1016, loss 0.0874871, acc 0.96
2016-09-06T19:37:09.269496: step 1017, loss 0.0715196, acc 0.96
2016-09-06T19:37:09.960037: step 1018, loss 0.209503, acc 0.94
2016-09-06T19:37:10.637796: step 1019, loss 0.0681059, acc 1
2016-09-06T19:37:11.332942: step 1020, loss 0.193381, acc 0.92
2016-09-06T19:37:11.999156: step 1021, loss 0.102344, acc 0.96
2016-09-06T19:37:12.676642: step 1022, loss 0.180471, acc 0.88
2016-09-06T19:37:13.358390: step 1023, loss 0.11716, acc 0.94
2016-09-06T19:37:14.036740: step 1024, loss 0.126261, acc 0.96
2016-09-06T19:37:14.712346: step 1025, loss 0.0516254, acc 0.98
2016-09-06T19:37:15.414334: step 1026, loss 0.0386911, acc 1
2016-09-06T19:37:16.131461: step 1027, loss 0.0681428, acc 0.96
2016-09-06T19:37:16.800968: step 1028, loss 0.0871368, acc 0.96
2016-09-06T19:37:17.503147: step 1029, loss 0.17372, acc 0.88
2016-09-06T19:37:18.205539: step 1030, loss 0.0835268, acc 0.94
2016-09-06T19:37:18.904577: step 1031, loss 0.0594501, acc 0.98
2016-09-06T19:37:19.599932: step 1032, loss 0.046978, acc 0.98
2016-09-06T19:37:20.258564: step 1033, loss 0.0341934, acc 1
2016-09-06T19:37:20.952791: step 1034, loss 0.022263, acc 1
2016-09-06T19:37:21.629205: step 1035, loss 0.0497986, acc 0.98
2016-09-06T19:37:22.319556: step 1036, loss 0.0613771, acc 0.96
2016-09-06T19:37:23.014522: step 1037, loss 0.155298, acc 0.92
2016-09-06T19:37:23.698952: step 1038, loss 0.239441, acc 0.94
2016-09-06T19:37:24.386761: step 1039, loss 0.0685256, acc 0.94
2016-09-06T19:37:25.051143: step 1040, loss 0.0989903, acc 0.94
2016-09-06T19:37:25.756519: step 1041, loss 0.0913497, acc 0.96
2016-09-06T19:37:26.421297: step 1042, loss 0.0619645, acc 0.98
2016-09-06T19:37:27.112578: step 1043, loss 0.18898, acc 0.9
2016-09-06T19:37:27.809840: step 1044, loss 0.175069, acc 0.96
2016-09-06T19:37:28.498076: step 1045, loss 0.0607347, acc 0.98
2016-09-06T19:37:29.193796: step 1046, loss 0.0822776, acc 0.98
2016-09-06T19:37:29.887402: step 1047, loss 0.124412, acc 0.94
2016-09-06T19:37:30.594105: step 1048, loss 0.117883, acc 0.96
2016-09-06T19:37:31.291002: step 1049, loss 0.0516183, acc 0.96
2016-09-06T19:37:31.990751: step 1050, loss 0.0802585, acc 0.96
2016-09-06T19:37:32.675377: step 1051, loss 0.152095, acc 0.92
2016-09-06T19:37:33.357070: step 1052, loss 0.157811, acc 0.94
2016-09-06T19:37:34.056891: step 1053, loss 0.0435322, acc 1
2016-09-06T19:37:34.740930: step 1054, loss 0.103604, acc 0.94
2016-09-06T19:37:35.479538: step 1055, loss 0.141555, acc 0.92
2016-09-06T19:37:36.169732: step 1056, loss 0.0933026, acc 0.94
2016-09-06T19:37:36.879589: step 1057, loss 0.0760345, acc 0.96
2016-09-06T19:37:37.604643: step 1058, loss 0.104316, acc 0.9
2016-09-06T19:37:38.319886: step 1059, loss 0.0774385, acc 0.98
2016-09-06T19:37:39.025642: step 1060, loss 0.131941, acc 0.94
2016-09-06T19:37:39.685248: step 1061, loss 0.0308734, acc 1
2016-09-06T19:37:40.379432: step 1062, loss 0.0208798, acc 1
2016-09-06T19:37:41.059350: step 1063, loss 0.175326, acc 0.9
2016-09-06T19:37:41.744735: step 1064, loss 0.0589302, acc 0.98
2016-09-06T19:37:42.425516: step 1065, loss 0.101332, acc 0.94
2016-09-06T19:37:43.119294: step 1066, loss 0.255043, acc 0.9
2016-09-06T19:37:43.823309: step 1067, loss 0.163468, acc 0.94
2016-09-06T19:37:44.498423: step 1068, loss 0.121573, acc 0.96
2016-09-06T19:37:45.205324: step 1069, loss 0.143995, acc 0.92
2016-09-06T19:37:45.891172: step 1070, loss 0.116947, acc 0.94
2016-09-06T19:37:46.587121: step 1071, loss 0.0485001, acc 0.98
2016-09-06T19:37:47.282780: step 1072, loss 0.167539, acc 0.94
2016-09-06T19:37:47.941834: step 1073, loss 0.104992, acc 0.96
2016-09-06T19:37:48.644694: step 1074, loss 0.15331, acc 0.92
2016-09-06T19:37:49.331958: step 1075, loss 0.129143, acc 0.94
2016-09-06T19:37:50.002215: step 1076, loss 0.110103, acc 0.94
2016-09-06T19:37:50.693372: step 1077, loss 0.140495, acc 0.92
2016-09-06T19:37:51.373763: step 1078, loss 0.128711, acc 0.96
2016-09-06T19:37:52.058865: step 1079, loss 0.151809, acc 0.92
2016-09-06T19:37:52.742280: step 1080, loss 0.0773665, acc 0.92
2016-09-06T19:37:53.433871: step 1081, loss 0.0439608, acc 1
2016-09-06T19:37:54.112602: step 1082, loss 0.171378, acc 0.9
2016-09-06T19:37:54.793315: step 1083, loss 0.0352557, acc 0.98
2016-09-06T19:37:55.506690: step 1084, loss 0.0949293, acc 0.98
2016-09-06T19:37:56.181507: step 1085, loss 0.174774, acc 0.92
2016-09-06T19:37:56.860050: step 1086, loss 0.071883, acc 0.96
2016-09-06T19:37:57.568132: step 1087, loss 0.0641003, acc 0.96
2016-09-06T19:37:58.314481: step 1088, loss 0.101918, acc 0.94
2016-09-06T19:37:58.992048: step 1089, loss 0.086816, acc 0.94
2016-09-06T19:37:59.683276: step 1090, loss 0.0751947, acc 0.98
2016-09-06T19:38:00.432015: step 1091, loss 0.149319, acc 0.92
2016-09-06T19:38:01.125026: step 1092, loss 0.112236, acc 0.96
2016-09-06T19:38:01.863774: step 1093, loss 0.054215, acc 1
2016-09-06T19:38:02.551716: step 1094, loss 0.1642, acc 0.88
2016-09-06T19:38:03.220429: step 1095, loss 0.0149157, acc 1
2016-09-06T19:38:03.897765: step 1096, loss 0.0711113, acc 0.98
2016-09-06T19:38:04.582670: step 1097, loss 0.294315, acc 0.84
2016-09-06T19:38:05.253683: step 1098, loss 0.0452827, acc 0.98
2016-09-06T19:38:05.958077: step 1099, loss 0.0654481, acc 0.96
2016-09-06T19:38:06.700168: step 1100, loss 0.188207, acc 0.94

Evaluation:
2016-09-06T19:38:09.873099: step 1100, loss 0.938868, acc 0.772983

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-1100

2016-09-06T19:38:11.604776: step 1101, loss 0.0768803, acc 0.98
2016-09-06T19:38:12.300431: step 1102, loss 0.0206954, acc 1
2016-09-06T19:38:12.988570: step 1103, loss 0.0559875, acc 0.98
2016-09-06T19:38:13.674638: step 1104, loss 0.171882, acc 0.88
2016-09-06T19:38:14.361727: step 1105, loss 0.143032, acc 0.92
2016-09-06T19:38:15.060925: step 1106, loss 0.0673579, acc 0.96
2016-09-06T19:38:15.728093: step 1107, loss 0.10801, acc 0.94
2016-09-06T19:38:16.442370: step 1108, loss 0.0836715, acc 0.96
2016-09-06T19:38:17.124364: step 1109, loss 0.125211, acc 0.96
2016-09-06T19:38:17.799920: step 1110, loss 0.0737103, acc 0.96
2016-09-06T19:38:18.489326: step 1111, loss 0.106046, acc 0.92
2016-09-06T19:38:19.167071: step 1112, loss 0.0707132, acc 0.98
2016-09-06T19:38:19.862624: step 1113, loss 0.0685965, acc 0.96
2016-09-06T19:38:20.522282: step 1114, loss 0.123205, acc 0.94
2016-09-06T19:38:21.237241: step 1115, loss 0.145983, acc 0.92
2016-09-06T19:38:21.946919: step 1116, loss 0.0580134, acc 0.98
2016-09-06T19:38:22.624729: step 1117, loss 0.0325652, acc 0.98
2016-09-06T19:38:23.305484: step 1118, loss 0.06142, acc 0.96
2016-09-06T19:38:23.975039: step 1119, loss 0.0504273, acc 0.98
2016-09-06T19:38:24.677436: step 1120, loss 0.0454232, acc 1
2016-09-06T19:38:25.333198: step 1121, loss 0.0153013, acc 1
2016-09-06T19:38:26.053044: step 1122, loss 0.0577262, acc 0.94
2016-09-06T19:38:26.730804: step 1123, loss 0.202418, acc 0.94
2016-09-06T19:38:27.408824: step 1124, loss 0.0952831, acc 0.94
2016-09-06T19:38:28.093723: step 1125, loss 0.117849, acc 0.9
2016-09-06T19:38:28.777827: step 1126, loss 0.193616, acc 0.92
2016-09-06T19:38:29.474139: step 1127, loss 0.187613, acc 0.94
2016-09-06T19:38:30.155488: step 1128, loss 0.0806042, acc 0.96
2016-09-06T19:38:30.842194: step 1129, loss 0.124162, acc 0.94
2016-09-06T19:38:31.526233: step 1130, loss 0.231235, acc 0.92
2016-09-06T19:38:32.224707: step 1131, loss 0.121402, acc 0.9
2016-09-06T19:38:32.910179: step 1132, loss 0.175774, acc 0.96
2016-09-06T19:38:33.574371: step 1133, loss 0.064462, acc 0.98
2016-09-06T19:38:34.267147: step 1134, loss 0.116429, acc 0.96
2016-09-06T19:38:34.944648: step 1135, loss 0.0315605, acc 1
2016-09-06T19:38:35.654544: step 1136, loss 0.0408399, acc 1
2016-09-06T19:38:36.346486: step 1137, loss 0.0500391, acc 0.98
2016-09-06T19:38:37.037324: step 1138, loss 0.187675, acc 0.96
2016-09-06T19:38:37.736377: step 1139, loss 0.0885753, acc 0.96
2016-09-06T19:38:38.427687: step 1140, loss 0.040735, acc 1
2016-09-06T19:38:39.121307: step 1141, loss 0.0530684, acc 0.98
2016-09-06T19:38:39.803483: step 1142, loss 0.16461, acc 0.94
2016-09-06T19:38:40.479439: step 1143, loss 0.0747293, acc 0.94
2016-09-06T19:38:41.182328: step 1144, loss 0.152341, acc 0.92
2016-09-06T19:38:41.897616: step 1145, loss 0.0881051, acc 0.96
2016-09-06T19:38:42.607185: step 1146, loss 0.102663, acc 0.92
2016-09-06T19:38:43.285358: step 1147, loss 0.137899, acc 0.9
2016-09-06T19:38:43.986351: step 1148, loss 0.0651118, acc 0.98
2016-09-06T19:38:44.653408: step 1149, loss 0.214425, acc 0.88
2016-09-06T19:38:45.336011: step 1150, loss 0.12288, acc 0.94
2016-09-06T19:38:46.029431: step 1151, loss 0.121954, acc 0.94
2016-09-06T19:38:46.660368: step 1152, loss 0.121962, acc 0.954545
2016-09-06T19:38:47.394254: step 1153, loss 0.132075, acc 0.96
2016-09-06T19:38:48.069845: step 1154, loss 0.0214217, acc 1
2016-09-06T19:38:48.767332: step 1155, loss 0.101733, acc 0.98
2016-09-06T19:38:49.460684: step 1156, loss 0.150859, acc 0.94
2016-09-06T19:38:50.152753: step 1157, loss 0.0958756, acc 0.96
2016-09-06T19:38:50.841466: step 1158, loss 0.166162, acc 0.9
2016-09-06T19:38:51.525405: step 1159, loss 0.0343163, acc 1
2016-09-06T19:38:52.211387: step 1160, loss 0.253605, acc 0.88
2016-09-06T19:38:52.887310: step 1161, loss 0.0895717, acc 0.96
2016-09-06T19:38:53.608185: step 1162, loss 0.0705942, acc 0.96
2016-09-06T19:38:54.293450: step 1163, loss 0.0355194, acc 1
2016-09-06T19:38:54.973123: step 1164, loss 0.13453, acc 0.98
2016-09-06T19:38:55.649225: step 1165, loss 0.0660059, acc 0.98
2016-09-06T19:38:56.328893: step 1166, loss 0.0400481, acc 1
2016-09-06T19:38:57.013247: step 1167, loss 0.152406, acc 0.96
2016-09-06T19:38:57.701960: step 1168, loss 0.0391689, acc 0.98
2016-09-06T19:38:58.411671: step 1169, loss 0.0730904, acc 0.98
2016-09-06T19:38:59.084046: step 1170, loss 0.0506106, acc 0.98
2016-09-06T19:38:59.771005: step 1171, loss 0.0803123, acc 0.96
2016-09-06T19:39:00.499288: step 1172, loss 0.0885649, acc 0.96
2016-09-06T19:39:01.197006: step 1173, loss 0.0291831, acc 1
2016-09-06T19:39:01.921028: step 1174, loss 0.0921878, acc 0.94
2016-09-06T19:39:02.582663: step 1175, loss 0.12477, acc 0.94
2016-09-06T19:39:03.300938: step 1176, loss 0.122277, acc 0.96
2016-09-06T19:39:03.976751: step 1177, loss 0.111737, acc 0.92
2016-09-06T19:39:04.677006: step 1178, loss 0.193728, acc 0.9
2016-09-06T19:39:05.367033: step 1179, loss 0.0338437, acc 1
2016-09-06T19:39:06.049904: step 1180, loss 0.149889, acc 0.9
2016-09-06T19:39:06.741430: step 1181, loss 0.139811, acc 0.9
2016-09-06T19:39:07.394600: step 1182, loss 0.145555, acc 0.94
2016-09-06T19:39:08.100976: step 1183, loss 0.0538345, acc 0.98
2016-09-06T19:39:08.792701: step 1184, loss 0.185779, acc 0.94
2016-09-06T19:39:09.470903: step 1185, loss 0.0581867, acc 0.98
2016-09-06T19:39:10.151323: step 1186, loss 0.0410505, acc 0.98
2016-09-06T19:39:10.849721: step 1187, loss 0.0963973, acc 0.96
2016-09-06T19:39:11.530664: step 1188, loss 0.107376, acc 0.92
2016-09-06T19:39:12.197186: step 1189, loss 0.0423661, acc 0.96
2016-09-06T19:39:12.905673: step 1190, loss 0.109385, acc 0.98
2016-09-06T19:39:13.591973: step 1191, loss 0.117904, acc 0.96
2016-09-06T19:39:14.286376: step 1192, loss 0.0788157, acc 0.94
2016-09-06T19:39:14.975714: step 1193, loss 0.0849575, acc 0.96
2016-09-06T19:39:15.666363: step 1194, loss 0.0471045, acc 0.96
2016-09-06T19:39:16.354524: step 1195, loss 0.138577, acc 0.92
2016-09-06T19:39:17.019043: step 1196, loss 0.181087, acc 0.92
2016-09-06T19:39:17.740555: step 1197, loss 0.0168518, acc 1
2016-09-06T19:39:18.437890: step 1198, loss 0.0720964, acc 0.98
2016-09-06T19:39:19.143632: step 1199, loss 0.0608712, acc 0.96
2016-09-06T19:39:19.836122: step 1200, loss 0.125314, acc 0.96

Evaluation:
2016-09-06T19:39:23.001184: step 1200, loss 0.986627, acc 0.772983

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-1200

2016-09-06T19:39:24.694316: step 1201, loss 0.0668221, acc 0.98
2016-09-06T19:39:25.395222: step 1202, loss 0.0265988, acc 1
2016-09-06T19:39:26.084153: step 1203, loss 0.032695, acc 1
2016-09-06T19:39:26.746063: step 1204, loss 0.0304423, acc 0.98
2016-09-06T19:39:27.421384: step 1205, loss 0.0364101, acc 0.98
2016-09-06T19:39:28.128969: step 1206, loss 0.127891, acc 0.96
2016-09-06T19:39:28.843470: step 1207, loss 0.124983, acc 0.94
2016-09-06T19:39:29.521425: step 1208, loss 0.0418172, acc 0.98
2016-09-06T19:39:30.199392: step 1209, loss 0.127612, acc 0.94
2016-09-06T19:39:30.919662: step 1210, loss 0.147818, acc 0.96
2016-09-06T19:39:31.579271: step 1211, loss 0.0653374, acc 0.98
2016-09-06T19:39:32.273858: step 1212, loss 0.0461209, acc 0.98
2016-09-06T19:39:32.953975: step 1213, loss 0.101416, acc 0.96
2016-09-06T19:39:33.636098: step 1214, loss 0.215954, acc 0.94
2016-09-06T19:39:34.358000: step 1215, loss 0.083013, acc 0.96
2016-09-06T19:39:35.041468: step 1216, loss 0.0624269, acc 0.98
2016-09-06T19:39:35.736574: step 1217, loss 0.139674, acc 0.96
2016-09-06T19:39:36.421841: step 1218, loss 0.0812368, acc 0.98
2016-09-06T19:39:37.103700: step 1219, loss 0.00662016, acc 1
2016-09-06T19:39:37.789216: step 1220, loss 0.0568756, acc 0.98
2016-09-06T19:39:38.481163: step 1221, loss 0.111876, acc 0.94
2016-09-06T19:39:39.160957: step 1222, loss 0.163304, acc 0.94
2016-09-06T19:39:39.809670: step 1223, loss 0.0925365, acc 0.96
2016-09-06T19:39:40.499774: step 1224, loss 0.0354512, acc 1
2016-09-06T19:39:41.178452: step 1225, loss 0.0430426, acc 0.98
2016-09-06T19:39:41.865820: step 1226, loss 0.178275, acc 0.94
2016-09-06T19:39:42.579528: step 1227, loss 0.0198246, acc 1
2016-09-06T19:39:43.271934: step 1228, loss 0.215508, acc 0.9
2016-09-06T19:39:43.989039: step 1229, loss 0.1546, acc 0.9
2016-09-06T19:39:44.669769: step 1230, loss 0.0791266, acc 0.94
2016-09-06T19:39:45.370604: step 1231, loss 0.110118, acc 0.96
2016-09-06T19:39:46.060035: step 1232, loss 0.0604906, acc 0.98
2016-09-06T19:39:46.749287: step 1233, loss 0.0483079, acc 0.98
2016-09-06T19:39:47.427971: step 1234, loss 0.0995304, acc 0.9
2016-09-06T19:39:48.113253: step 1235, loss 0.0557553, acc 0.98
2016-09-06T19:39:48.800161: step 1236, loss 0.171431, acc 0.92
2016-09-06T19:39:49.443276: step 1237, loss 0.0711164, acc 0.96
2016-09-06T19:39:50.158250: step 1238, loss 0.161845, acc 0.9
2016-09-06T19:39:50.824502: step 1239, loss 0.0386331, acc 1
2016-09-06T19:39:51.523539: step 1240, loss 0.0635497, acc 0.98
2016-09-06T19:39:52.205404: step 1241, loss 0.0495532, acc 0.96
2016-09-06T19:39:52.884625: step 1242, loss 0.126933, acc 0.92
2016-09-06T19:39:53.569371: step 1243, loss 0.0894815, acc 0.94
2016-09-06T19:39:54.253354: step 1244, loss 0.0790676, acc 0.94
2016-09-06T19:39:54.971149: step 1245, loss 0.0698737, acc 0.98
2016-09-06T19:39:55.654766: step 1246, loss 0.0860621, acc 0.96
2016-09-06T19:39:56.328937: step 1247, loss 0.0908682, acc 0.94
2016-09-06T19:39:57.016128: step 1248, loss 0.133278, acc 0.94
2016-09-06T19:39:57.735640: step 1249, loss 0.188514, acc 0.92
2016-09-06T19:39:58.427454: step 1250, loss 0.153299, acc 0.94
2016-09-06T19:39:59.092478: step 1251, loss 0.200519, acc 0.92
2016-09-06T19:39:59.783213: step 1252, loss 0.0654636, acc 0.96
2016-09-06T19:40:00.525013: step 1253, loss 0.175235, acc 0.88
2016-09-06T19:40:01.214309: step 1254, loss 0.065843, acc 0.96
2016-09-06T19:40:01.914980: step 1255, loss 0.096342, acc 0.94
2016-09-06T19:40:02.615663: step 1256, loss 0.101452, acc 0.94
2016-09-06T19:40:03.331567: step 1257, loss 0.146152, acc 0.96
2016-09-06T19:40:04.011424: step 1258, loss 0.0750409, acc 0.98
2016-09-06T19:40:04.690361: step 1259, loss 0.1773, acc 0.98
2016-09-06T19:40:05.396603: step 1260, loss 0.0884419, acc 0.96
2016-09-06T19:40:06.112031: step 1261, loss 0.0752314, acc 0.96
2016-09-06T19:40:06.818389: step 1262, loss 0.13006, acc 0.96
2016-09-06T19:40:07.499558: step 1263, loss 0.17278, acc 0.94
2016-09-06T19:40:08.220420: step 1264, loss 0.0395904, acc 1
2016-09-06T19:40:08.888928: step 1265, loss 0.153871, acc 0.96
2016-09-06T19:40:09.586738: step 1266, loss 0.0604256, acc 0.96
2016-09-06T19:40:10.284422: step 1267, loss 0.146964, acc 0.96
2016-09-06T19:40:10.995947: step 1268, loss 0.0685786, acc 0.96
2016-09-06T19:40:11.689431: step 1269, loss 0.0762437, acc 0.96
2016-09-06T19:40:12.363540: step 1270, loss 0.0621524, acc 0.96
2016-09-06T19:40:13.076087: step 1271, loss 0.055319, acc 0.98
2016-09-06T19:40:13.766837: step 1272, loss 0.063422, acc 0.98
2016-09-06T19:40:14.461245: step 1273, loss 0.0671647, acc 0.96
2016-09-06T19:40:15.156754: step 1274, loss 0.128363, acc 0.9
2016-09-06T19:40:15.853342: step 1275, loss 0.0495088, acc 0.98
2016-09-06T19:40:16.550183: step 1276, loss 0.126969, acc 0.96
2016-09-06T19:40:17.249848: step 1277, loss 0.0754737, acc 0.96
2016-09-06T19:40:17.949128: step 1278, loss 0.0361864, acc 0.98
2016-09-06T19:40:18.620910: step 1279, loss 0.0733722, acc 0.96
2016-09-06T19:40:19.331318: step 1280, loss 0.137432, acc 0.96
2016-09-06T19:40:20.003453: step 1281, loss 0.083972, acc 0.96
2016-09-06T19:40:20.687885: step 1282, loss 0.0622771, acc 0.98
2016-09-06T19:40:21.396787: step 1283, loss 0.118934, acc 0.96
2016-09-06T19:40:22.085480: step 1284, loss 0.153899, acc 0.94
2016-09-06T19:40:22.776255: step 1285, loss 0.0992699, acc 0.96
2016-09-06T19:40:23.459009: step 1286, loss 0.0235732, acc 1
2016-09-06T19:40:24.166108: step 1287, loss 0.126626, acc 0.98
2016-09-06T19:40:24.860179: step 1288, loss 0.0522494, acc 0.98
2016-09-06T19:40:25.519952: step 1289, loss 0.0691487, acc 0.98
2016-09-06T19:40:26.241538: step 1290, loss 0.111414, acc 0.98
2016-09-06T19:40:26.934857: step 1291, loss 0.0662618, acc 0.98
2016-09-06T19:40:27.624969: step 1292, loss 0.0492927, acc 0.98
2016-09-06T19:40:28.316750: step 1293, loss 0.196799, acc 0.92
2016-09-06T19:40:29.009658: step 1294, loss 0.0843705, acc 0.94
2016-09-06T19:40:29.715291: step 1295, loss 0.0413416, acc 0.98
2016-09-06T19:40:30.382341: step 1296, loss 0.0720771, acc 0.96
2016-09-06T19:40:31.098226: step 1297, loss 0.0565459, acc 0.96
2016-09-06T19:40:31.782326: step 1298, loss 0.094039, acc 0.96
2016-09-06T19:40:32.470890: step 1299, loss 0.077133, acc 0.96
2016-09-06T19:40:33.172296: step 1300, loss 0.121999, acc 0.92

Evaluation:
2016-09-06T19:40:36.307584: step 1300, loss 1.04345, acc 0.766416

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-1300

2016-09-06T19:40:37.992947: step 1301, loss 0.337743, acc 0.9
2016-09-06T19:40:38.694444: step 1302, loss 0.0592803, acc 0.96
2016-09-06T19:40:39.391115: step 1303, loss 0.202525, acc 0.9
2016-09-06T19:40:40.067435: step 1304, loss 0.0872999, acc 0.96
2016-09-06T19:40:40.763618: step 1305, loss 0.0336209, acc 0.98
2016-09-06T19:40:41.471689: step 1306, loss 0.114462, acc 0.92
2016-09-06T19:40:42.151771: step 1307, loss 0.15459, acc 0.96
2016-09-06T19:40:42.839821: step 1308, loss 0.0431362, acc 1
2016-09-06T19:40:43.526785: step 1309, loss 0.433315, acc 0.88
2016-09-06T19:40:44.223312: step 1310, loss 0.0803969, acc 0.98
2016-09-06T19:40:44.902597: step 1311, loss 0.0639916, acc 1
2016-09-06T19:40:45.582477: step 1312, loss 0.0746822, acc 0.94
2016-09-06T19:40:46.276770: step 1313, loss 0.123955, acc 0.96
2016-09-06T19:40:46.961263: step 1314, loss 0.0357327, acc 1
2016-09-06T19:40:47.646978: step 1315, loss 0.154424, acc 0.94
2016-09-06T19:40:48.331258: step 1316, loss 0.064702, acc 1
2016-09-06T19:40:49.043369: step 1317, loss 0.0498886, acc 1
2016-09-06T19:40:49.715676: step 1318, loss 0.0621153, acc 1
2016-09-06T19:40:50.377247: step 1319, loss 0.089679, acc 0.96
2016-09-06T19:40:51.048504: step 1320, loss 0.142953, acc 0.94
2016-09-06T19:40:51.751270: step 1321, loss 0.114289, acc 0.94
2016-09-06T19:40:52.447946: step 1322, loss 0.0869202, acc 0.96
2016-09-06T19:40:53.143028: step 1323, loss 0.0196725, acc 1
2016-09-06T19:40:53.871678: step 1324, loss 0.188804, acc 0.94
2016-09-06T19:40:54.538186: step 1325, loss 0.0882878, acc 0.94
2016-09-06T19:40:55.219253: step 1326, loss 0.17859, acc 0.96
2016-09-06T19:40:55.901818: step 1327, loss 0.225046, acc 0.9
2016-09-06T19:40:56.599536: step 1328, loss 0.0908007, acc 0.98
2016-09-06T19:40:57.287675: step 1329, loss 0.0556937, acc 0.98
2016-09-06T19:40:57.978991: step 1330, loss 0.313778, acc 0.86
2016-09-06T19:40:58.697128: step 1331, loss 0.088462, acc 0.94
2016-09-06T19:40:59.366507: step 1332, loss 0.0560623, acc 0.98
2016-09-06T19:41:00.044680: step 1333, loss 0.043751, acc 0.96
2016-09-06T19:41:00.758826: step 1334, loss 0.0921407, acc 0.96
2016-09-06T19:41:01.437760: step 1335, loss 0.0689861, acc 0.96
2016-09-06T19:41:02.134712: step 1336, loss 0.162742, acc 0.92
2016-09-06T19:41:02.820455: step 1337, loss 0.0548118, acc 0.98
2016-09-06T19:41:03.525201: step 1338, loss 0.331368, acc 0.92
2016-09-06T19:41:04.220758: step 1339, loss 0.0595307, acc 1
2016-09-06T19:41:04.904489: step 1340, loss 0.076077, acc 0.98
2016-09-06T19:41:05.583295: step 1341, loss 0.0740583, acc 1
2016-09-06T19:41:06.295696: step 1342, loss 0.101244, acc 0.94
2016-09-06T19:41:06.971443: step 1343, loss 0.0810866, acc 0.96
2016-09-06T19:41:07.583511: step 1344, loss 0.11266, acc 0.954545
2016-09-06T19:41:08.305868: step 1345, loss 0.105103, acc 0.94
2016-09-06T19:41:08.995283: step 1346, loss 0.0569127, acc 0.96
2016-09-06T19:41:09.685754: step 1347, loss 0.10389, acc 0.9
2016-09-06T19:41:10.374837: step 1348, loss 0.0349075, acc 0.98
2016-09-06T19:41:11.091150: step 1349, loss 0.0588673, acc 0.98
2016-09-06T19:41:11.798238: step 1350, loss 0.091069, acc 0.96
2016-09-06T19:41:12.462970: step 1351, loss 0.0364058, acc 1
2016-09-06T19:41:13.192469: step 1352, loss 0.0667269, acc 0.94
2016-09-06T19:41:13.881779: step 1353, loss 0.0811266, acc 0.92
2016-09-06T19:41:14.578243: step 1354, loss 0.0587276, acc 0.98
2016-09-06T19:41:15.271171: step 1355, loss 0.0525352, acc 0.96
2016-09-06T19:41:15.967047: step 1356, loss 0.0466462, acc 1
2016-09-06T19:41:16.682283: step 1357, loss 0.0697707, acc 0.96
2016-09-06T19:41:17.361689: step 1358, loss 0.1666, acc 0.88
2016-09-06T19:41:18.040699: step 1359, loss 0.0189469, acc 1
2016-09-06T19:41:18.736712: step 1360, loss 0.0290298, acc 1
2016-09-06T19:41:19.449194: step 1361, loss 0.089984, acc 1
2016-09-06T19:41:20.164911: step 1362, loss 0.0890502, acc 0.98
2016-09-06T19:41:20.876239: step 1363, loss 0.090827, acc 0.96
2016-09-06T19:41:21.563950: step 1364, loss 0.0228406, acc 1
2016-09-06T19:41:22.244569: step 1365, loss 0.074333, acc 0.94
2016-09-06T19:41:22.919946: step 1366, loss 0.065438, acc 0.96
2016-09-06T19:41:23.613721: step 1367, loss 0.0281152, acc 0.98
2016-09-06T19:41:24.290825: step 1368, loss 0.12419, acc 0.92
2016-09-06T19:41:24.977933: step 1369, loss 0.0143227, acc 1
2016-09-06T19:41:25.640539: step 1370, loss 0.0742234, acc 0.96
2016-09-06T19:41:26.344779: step 1371, loss 0.0469218, acc 0.98
2016-09-06T19:41:27.024854: step 1372, loss 0.107937, acc 0.94
2016-09-06T19:41:27.709602: step 1373, loss 0.0374986, acc 0.98
2016-09-06T19:41:28.398474: step 1374, loss 0.172574, acc 0.94
2016-09-06T19:41:29.100298: step 1375, loss 0.0247539, acc 1
2016-09-06T19:41:29.799412: step 1376, loss 0.111373, acc 0.96
2016-09-06T19:41:30.477000: step 1377, loss 0.0316598, acc 0.98
2016-09-06T19:41:31.174289: step 1378, loss 0.0277987, acc 1
2016-09-06T19:41:31.870874: step 1379, loss 0.0702786, acc 0.96
2016-09-06T19:41:32.571402: step 1380, loss 0.0697029, acc 0.96
2016-09-06T19:41:33.270908: step 1381, loss 0.00657418, acc 1
2016-09-06T19:41:33.956215: step 1382, loss 0.111519, acc 0.94
2016-09-06T19:41:34.651687: step 1383, loss 0.210534, acc 0.94
2016-09-06T19:41:35.314498: step 1384, loss 0.0613408, acc 0.98
2016-09-06T19:41:36.021922: step 1385, loss 0.0677676, acc 0.96
2016-09-06T19:41:36.723529: step 1386, loss 0.0636686, acc 0.98
2016-09-06T19:41:37.397166: step 1387, loss 0.073118, acc 0.96
2016-09-06T19:41:38.114339: step 1388, loss 0.0682369, acc 0.98
2016-09-06T19:41:38.816898: step 1389, loss 0.0548095, acc 0.98
2016-09-06T19:41:39.508039: step 1390, loss 0.0755975, acc 0.98
2016-09-06T19:41:40.190947: step 1391, loss 0.05859, acc 0.98
2016-09-06T19:41:40.866455: step 1392, loss 0.214453, acc 0.9
2016-09-06T19:41:41.548321: step 1393, loss 0.0464992, acc 0.98
2016-09-06T19:41:42.247419: step 1394, loss 0.0690482, acc 0.96
2016-09-06T19:41:42.964743: step 1395, loss 0.0244631, acc 0.98
2016-09-06T19:41:43.653291: step 1396, loss 0.0880218, acc 0.94
2016-09-06T19:41:44.388147: step 1397, loss 0.0445097, acc 0.96
2016-09-06T19:41:45.084297: step 1398, loss 0.132971, acc 0.94
2016-09-06T19:41:45.793573: step 1399, loss 0.0804896, acc 0.94
2016-09-06T19:41:46.485836: step 1400, loss 0.0765349, acc 0.94

Evaluation:
2016-09-06T19:41:49.645719: step 1400, loss 1.07237, acc 0.757974

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-1400

2016-09-06T19:41:51.314872: step 1401, loss 0.0418439, acc 0.98
2016-09-06T19:41:51.999092: step 1402, loss 0.0466099, acc 1
2016-09-06T19:41:52.700496: step 1403, loss 0.0188504, acc 1
2016-09-06T19:41:53.384527: step 1404, loss 0.0483731, acc 0.98
2016-09-06T19:41:54.091597: step 1405, loss 0.0970828, acc 0.94
2016-09-06T19:41:54.781485: step 1406, loss 0.116301, acc 0.94
2016-09-06T19:41:55.464528: step 1407, loss 0.181432, acc 0.92
2016-09-06T19:41:56.156365: step 1408, loss 0.0436232, acc 0.98
2016-09-06T19:41:56.851181: step 1409, loss 0.0651807, acc 0.96
2016-09-06T19:41:57.558935: step 1410, loss 0.1081, acc 0.94
2016-09-06T19:41:58.237729: step 1411, loss 0.0309771, acc 0.98
2016-09-06T19:41:58.937472: step 1412, loss 0.0401013, acc 0.98
2016-09-06T19:41:59.641775: step 1413, loss 0.0418322, acc 0.98
2016-09-06T19:42:00.374379: step 1414, loss 0.0925828, acc 0.94
2016-09-06T19:42:01.058426: step 1415, loss 0.114142, acc 0.92
2016-09-06T19:42:01.769355: step 1416, loss 0.0903179, acc 0.98
2016-09-06T19:42:02.476421: step 1417, loss 0.0198491, acc 1
2016-09-06T19:42:03.152993: step 1418, loss 0.163819, acc 0.92
2016-09-06T19:42:03.857639: step 1419, loss 0.025165, acc 1
2016-09-06T19:42:04.554153: step 1420, loss 0.0768687, acc 0.94
2016-09-06T19:42:05.226286: step 1421, loss 0.127618, acc 0.92
2016-09-06T19:42:05.918311: step 1422, loss 0.331225, acc 0.94
2016-09-06T19:42:06.603077: step 1423, loss 0.040411, acc 0.98
2016-09-06T19:42:07.312395: step 1424, loss 0.124956, acc 0.92
2016-09-06T19:42:08.013337: step 1425, loss 0.0952581, acc 0.96
2016-09-06T19:42:08.687405: step 1426, loss 0.0455311, acc 0.98
2016-09-06T19:42:09.386992: step 1427, loss 0.201214, acc 0.96
2016-09-06T19:42:10.053564: step 1428, loss 0.00762036, acc 1
2016-09-06T19:42:10.740524: step 1429, loss 0.063968, acc 0.98
2016-09-06T19:42:11.403500: step 1430, loss 0.0607377, acc 0.98
2016-09-06T19:42:12.103579: step 1431, loss 0.041953, acc 0.98
2016-09-06T19:42:12.762296: step 1432, loss 0.118532, acc 0.94
2016-09-06T19:42:13.448191: step 1433, loss 0.110596, acc 0.94
2016-09-06T19:42:14.164830: step 1434, loss 0.0448367, acc 0.98
2016-09-06T19:42:14.853426: step 1435, loss 0.0518424, acc 1
2016-09-06T19:42:15.559640: step 1436, loss 0.0453693, acc 0.98
2016-09-06T19:42:16.219270: step 1437, loss 0.107777, acc 0.98
2016-09-06T19:42:16.924987: step 1438, loss 0.0544445, acc 0.98
2016-09-06T19:42:17.604310: step 1439, loss 0.0802827, acc 0.96
2016-09-06T19:42:18.274092: step 1440, loss 0.0518976, acc 0.96
2016-09-06T19:42:18.950127: step 1441, loss 0.0577139, acc 0.98
2016-09-06T19:42:19.624601: step 1442, loss 0.0810017, acc 0.96
2016-09-06T19:42:20.309210: step 1443, loss 0.126556, acc 0.92
2016-09-06T19:42:20.985576: step 1444, loss 0.0400899, acc 0.96
2016-09-06T19:42:21.680130: step 1445, loss 0.0353055, acc 0.98
2016-09-06T19:42:22.349790: step 1446, loss 0.126107, acc 0.96
2016-09-06T19:42:23.018074: step 1447, loss 0.0888612, acc 0.96
2016-09-06T19:42:23.702076: step 1448, loss 0.0952374, acc 0.96
2016-09-06T19:42:24.383996: step 1449, loss 0.112301, acc 0.98
2016-09-06T19:42:25.121302: step 1450, loss 0.0362182, acc 1
2016-09-06T19:42:25.820679: step 1451, loss 0.0752682, acc 0.96
2016-09-06T19:42:26.528688: step 1452, loss 0.133262, acc 0.94
2016-09-06T19:42:27.191264: step 1453, loss 0.0979449, acc 0.94
2016-09-06T19:42:27.866480: step 1454, loss 0.0333647, acc 0.98
2016-09-06T19:42:28.537988: step 1455, loss 0.162473, acc 0.94
2016-09-06T19:42:29.207950: step 1456, loss 0.0805058, acc 0.96
2016-09-06T19:42:29.905752: step 1457, loss 0.066472, acc 0.94
2016-09-06T19:42:30.591619: step 1458, loss 0.0142691, acc 1
2016-09-06T19:42:31.282743: step 1459, loss 0.105417, acc 0.96
2016-09-06T19:42:31.960368: step 1460, loss 0.0983614, acc 0.98
2016-09-06T19:42:32.661401: step 1461, loss 0.00948965, acc 1
2016-09-06T19:42:33.342112: step 1462, loss 0.0427156, acc 1
2016-09-06T19:42:34.027338: step 1463, loss 0.0735398, acc 0.98
2016-09-06T19:42:34.706148: step 1464, loss 0.0440296, acc 0.98
2016-09-06T19:42:35.385303: step 1465, loss 0.121808, acc 0.96
2016-09-06T19:42:36.084717: step 1466, loss 0.0750516, acc 0.94
2016-09-06T19:42:36.748593: step 1467, loss 0.103692, acc 0.98
2016-09-06T19:42:37.436272: step 1468, loss 0.08171, acc 0.98
2016-09-06T19:42:38.130456: step 1469, loss 0.067724, acc 0.96
2016-09-06T19:42:38.833096: step 1470, loss 0.0462816, acc 0.98
2016-09-06T19:42:39.538433: step 1471, loss 0.215451, acc 0.94
2016-09-06T19:42:40.220944: step 1472, loss 0.0762102, acc 0.98
2016-09-06T19:42:40.931581: step 1473, loss 0.0641071, acc 0.96
2016-09-06T19:42:41.600069: step 1474, loss 0.0563332, acc 1
2016-09-06T19:42:42.269950: step 1475, loss 0.121489, acc 0.92
2016-09-06T19:42:42.963167: step 1476, loss 0.066793, acc 0.98
2016-09-06T19:42:43.654036: step 1477, loss 0.0757211, acc 0.96
2016-09-06T19:42:44.353562: step 1478, loss 0.0206023, acc 1
2016-09-06T19:42:45.044838: step 1479, loss 0.0351052, acc 0.98
2016-09-06T19:42:45.733131: step 1480, loss 0.0228307, acc 1
2016-09-06T19:42:46.425363: step 1481, loss 0.0638748, acc 0.96
2016-09-06T19:42:47.101681: step 1482, loss 0.00270341, acc 1
2016-09-06T19:42:47.806355: step 1483, loss 0.108623, acc 0.94
2016-09-06T19:42:48.501055: step 1484, loss 0.0882532, acc 0.96
2016-09-06T19:42:49.188692: step 1485, loss 0.0856481, acc 0.94
2016-09-06T19:42:49.862679: step 1486, loss 0.0467962, acc 1
2016-09-06T19:42:50.580885: step 1487, loss 0.0236308, acc 1
2016-09-06T19:42:51.268203: step 1488, loss 0.138565, acc 0.96
2016-09-06T19:42:51.944607: step 1489, loss 0.0553349, acc 1
2016-09-06T19:42:52.636379: step 1490, loss 0.0393389, acc 0.98
2016-09-06T19:42:53.332547: step 1491, loss 0.0616509, acc 0.98
2016-09-06T19:42:54.058767: step 1492, loss 0.0664179, acc 0.98
2016-09-06T19:42:54.718359: step 1493, loss 0.198321, acc 0.94
2016-09-06T19:42:55.423457: step 1494, loss 0.0227111, acc 0.98
2016-09-06T19:42:56.095585: step 1495, loss 0.0580561, acc 0.96
2016-09-06T19:42:56.778956: step 1496, loss 0.0395118, acc 0.98
2016-09-06T19:42:57.482967: step 1497, loss 0.0490155, acc 0.98
2016-09-06T19:42:58.162069: step 1498, loss 0.0961932, acc 0.94
2016-09-06T19:42:58.842866: step 1499, loss 0.0740051, acc 0.98
2016-09-06T19:42:59.506686: step 1500, loss 0.0878454, acc 0.94

Evaluation:
2016-09-06T19:43:02.700723: step 1500, loss 1.17323, acc 0.770169

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-1500

2016-09-06T19:43:04.462185: step 1501, loss 0.0584139, acc 0.98
2016-09-06T19:43:05.141386: step 1502, loss 0.0520768, acc 0.98
2016-09-06T19:43:05.821209: step 1503, loss 0.127311, acc 0.96
2016-09-06T19:43:06.519482: step 1504, loss 0.111769, acc 0.96
2016-09-06T19:43:07.216158: step 1505, loss 0.0107911, acc 1
2016-09-06T19:43:07.908927: step 1506, loss 0.0128233, acc 1
2016-09-06T19:43:08.600247: step 1507, loss 0.0498381, acc 0.98
2016-09-06T19:43:09.273087: step 1508, loss 0.0171735, acc 1
2016-09-06T19:43:09.960535: step 1509, loss 0.154538, acc 0.9
2016-09-06T19:43:10.652300: step 1510, loss 0.0760772, acc 0.96
2016-09-06T19:43:11.340263: step 1511, loss 0.0304246, acc 1
2016-09-06T19:43:12.014641: step 1512, loss 0.0486675, acc 0.96
2016-09-06T19:43:12.727784: step 1513, loss 0.206494, acc 0.98
2016-09-06T19:43:13.443784: step 1514, loss 0.0578724, acc 0.98
2016-09-06T19:43:14.124545: step 1515, loss 0.030137, acc 1
2016-09-06T19:43:14.811205: step 1516, loss 0.0600665, acc 0.96
2016-09-06T19:43:15.487547: step 1517, loss 0.105802, acc 0.94
2016-09-06T19:43:16.186472: step 1518, loss 0.226785, acc 0.94
2016-09-06T19:43:16.879875: step 1519, loss 0.0466236, acc 0.98
2016-09-06T19:43:17.550140: step 1520, loss 0.0376836, acc 1
2016-09-06T19:43:18.302647: step 1521, loss 0.107025, acc 0.96
2016-09-06T19:43:19.016901: step 1522, loss 0.0318419, acc 1
2016-09-06T19:43:19.707535: step 1523, loss 0.0755201, acc 0.98
2016-09-06T19:43:20.397787: step 1524, loss 0.0569903, acc 0.98
2016-09-06T19:43:21.096387: step 1525, loss 0.0789426, acc 0.98
2016-09-06T19:43:21.790389: step 1526, loss 0.0615551, acc 0.98
2016-09-06T19:43:22.465743: step 1527, loss 0.148192, acc 0.94
2016-09-06T19:43:23.153645: step 1528, loss 0.0642508, acc 0.96
2016-09-06T19:43:23.859119: step 1529, loss 0.0571564, acc 1
2016-09-06T19:43:24.585186: step 1530, loss 0.106423, acc 0.94
2016-09-06T19:43:25.304715: step 1531, loss 0.125644, acc 0.92
2016-09-06T19:43:25.958498: step 1532, loss 0.0456026, acc 0.98
2016-09-06T19:43:26.668130: step 1533, loss 0.113793, acc 0.96
2016-09-06T19:43:27.355248: step 1534, loss 0.129473, acc 0.92
2016-09-06T19:43:28.036325: step 1535, loss 0.0900916, acc 0.94
2016-09-06T19:43:28.674792: step 1536, loss 0.0342641, acc 0.977273
2016-09-06T19:43:29.367732: step 1537, loss 0.0319056, acc 1
2016-09-06T19:43:30.053281: step 1538, loss 0.0354346, acc 0.98
2016-09-06T19:43:30.725957: step 1539, loss 0.119623, acc 0.96
2016-09-06T19:43:31.418373: step 1540, loss 0.0318604, acc 1
2016-09-06T19:43:32.101384: step 1541, loss 0.069406, acc 0.96
2016-09-06T19:43:32.813569: step 1542, loss 0.0144703, acc 1
2016-09-06T19:43:33.492287: step 1543, loss 0.0562604, acc 0.98
2016-09-06T19:43:34.189614: step 1544, loss 0.150168, acc 0.94
2016-09-06T19:43:34.877254: step 1545, loss 0.063752, acc 0.98
2016-09-06T19:43:35.550702: step 1546, loss 0.0756007, acc 0.96
2016-09-06T19:43:36.244052: step 1547, loss 0.0329764, acc 1
2016-09-06T19:43:36.902021: step 1548, loss 0.0799481, acc 0.96
2016-09-06T19:43:37.577118: step 1549, loss 0.0893625, acc 0.92
2016-09-06T19:43:38.263306: step 1550, loss 0.13947, acc 0.94
2016-09-06T19:43:38.949380: step 1551, loss 0.0160984, acc 1
2016-09-06T19:43:39.642051: step 1552, loss 0.0599435, acc 0.98
2016-09-06T19:43:40.339934: step 1553, loss 0.173544, acc 0.92
2016-09-06T19:43:41.042710: step 1554, loss 0.0117777, acc 1
2016-09-06T19:43:41.720368: step 1555, loss 0.0585594, acc 0.98
2016-09-06T19:43:42.426088: step 1556, loss 0.0423931, acc 1
2016-09-06T19:43:43.109364: step 1557, loss 0.0447731, acc 0.98
2016-09-06T19:43:43.798334: step 1558, loss 0.139897, acc 0.92
2016-09-06T19:43:44.501896: step 1559, loss 0.0620832, acc 0.96
2016-09-06T19:43:45.165781: step 1560, loss 0.0854865, acc 0.96
2016-09-06T19:43:45.879095: step 1561, loss 0.108655, acc 0.96
2016-09-06T19:43:46.555746: step 1562, loss 0.0813032, acc 0.94
2016-09-06T19:43:47.237949: step 1563, loss 0.0713731, acc 0.96
2016-09-06T19:43:47.933784: step 1564, loss 0.0866085, acc 0.98
2016-09-06T19:43:48.626470: step 1565, loss 0.0225585, acc 1
2016-09-06T19:43:49.334513: step 1566, loss 0.0516066, acc 0.98
2016-09-06T19:43:50.016472: step 1567, loss 0.0923815, acc 0.96
2016-09-06T19:43:50.712248: step 1568, loss 0.0283942, acc 0.98
2016-09-06T19:43:51.378746: step 1569, loss 0.0487558, acc 0.98
2016-09-06T19:43:52.084108: step 1570, loss 0.0383819, acc 0.98
2016-09-06T19:43:52.780955: step 1571, loss 0.0737002, acc 0.98
2016-09-06T19:43:53.464736: step 1572, loss 0.0669302, acc 0.98
2016-09-06T19:43:54.162194: step 1573, loss 0.00574197, acc 1
2016-09-06T19:43:54.840390: step 1574, loss 0.0789594, acc 0.98
2016-09-06T19:43:55.535754: step 1575, loss 0.0244861, acc 0.98
2016-09-06T19:43:56.222522: step 1576, loss 0.0508381, acc 0.98
2016-09-06T19:43:56.903133: step 1577, loss 0.0662671, acc 0.98
2016-09-06T19:43:57.578851: step 1578, loss 0.0398188, acc 0.98
2016-09-06T19:43:58.269939: step 1579, loss 0.0681643, acc 0.96
2016-09-06T19:43:58.951963: step 1580, loss 0.121647, acc 0.96
2016-09-06T19:43:59.605974: step 1581, loss 0.0748076, acc 0.98
2016-09-06T19:44:00.328311: step 1582, loss 0.0498629, acc 1
2016-09-06T19:44:01.020821: step 1583, loss 0.0429519, acc 0.98
2016-09-06T19:44:01.708489: step 1584, loss 0.117344, acc 0.98
2016-09-06T19:44:02.389934: step 1585, loss 0.0984377, acc 0.96
2016-09-06T19:44:03.088906: step 1586, loss 0.128798, acc 0.94
2016-09-06T19:44:03.782103: step 1587, loss 0.0491091, acc 0.98
2016-09-06T19:44:04.452895: step 1588, loss 0.00644347, acc 1
2016-09-06T19:44:05.154212: step 1589, loss 0.0410696, acc 1
2016-09-06T19:44:05.814909: step 1590, loss 0.0702458, acc 0.96
2016-09-06T19:44:06.487188: step 1591, loss 0.0655971, acc 0.98
2016-09-06T19:44:07.171860: step 1592, loss 0.0202282, acc 1
2016-09-06T19:44:07.870000: step 1593, loss 0.0285712, acc 1
2016-09-06T19:44:08.558355: step 1594, loss 0.0946491, acc 0.98
2016-09-06T19:44:09.227880: step 1595, loss 0.0088971, acc 1
2016-09-06T19:44:09.958449: step 1596, loss 0.0660313, acc 0.98
2016-09-06T19:44:10.652150: step 1597, loss 0.0588951, acc 0.98
2016-09-06T19:44:11.359162: step 1598, loss 0.049198, acc 0.98
2016-09-06T19:44:12.049386: step 1599, loss 0.0471881, acc 1
2016-09-06T19:44:12.736651: step 1600, loss 0.0255206, acc 0.98

Evaluation:
2016-09-06T19:44:15.892850: step 1600, loss 1.43308, acc 0.765478

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-1600

2016-09-06T19:44:17.592569: step 1601, loss 0.0608918, acc 0.98
2016-09-06T19:44:18.284438: step 1602, loss 0.0681967, acc 0.96
2016-09-06T19:44:18.957570: step 1603, loss 0.0247751, acc 1
2016-09-06T19:44:19.665113: step 1604, loss 0.0442225, acc 0.98
2016-09-06T19:44:20.372039: step 1605, loss 0.146613, acc 0.96
2016-09-06T19:44:21.055319: step 1606, loss 0.00528306, acc 1
2016-09-06T19:44:21.769140: step 1607, loss 0.130407, acc 0.98
2016-09-06T19:44:22.437526: step 1608, loss 0.034203, acc 0.98
2016-09-06T19:44:23.144780: step 1609, loss 0.0958248, acc 0.92
2016-09-06T19:44:23.825610: step 1610, loss 0.185838, acc 0.92
2016-09-06T19:44:24.506825: step 1611, loss 0.0760164, acc 0.92
2016-09-06T19:44:25.208642: step 1612, loss 0.053624, acc 0.98
2016-09-06T19:44:25.899839: step 1613, loss 0.185602, acc 0.94
2016-09-06T19:44:26.579633: step 1614, loss 0.0938298, acc 0.94
2016-09-06T19:44:27.247708: step 1615, loss 0.184188, acc 0.92
2016-09-06T19:44:27.957366: step 1616, loss 0.0892773, acc 0.96
2016-09-06T19:44:28.625013: step 1617, loss 0.0195344, acc 1
2016-09-06T19:44:29.292925: step 1618, loss 0.0442495, acc 0.96
2016-09-06T19:44:29.960581: step 1619, loss 0.0927082, acc 0.94
2016-09-06T19:44:30.640696: step 1620, loss 0.181286, acc 0.9
2016-09-06T19:44:31.323826: step 1621, loss 0.0660553, acc 0.96
2016-09-06T19:44:32.018538: step 1622, loss 0.0665702, acc 0.98
2016-09-06T19:44:32.725262: step 1623, loss 0.129283, acc 0.94
2016-09-06T19:44:33.400236: step 1624, loss 0.0299072, acc 0.98
2016-09-06T19:44:34.076320: step 1625, loss 0.0408199, acc 1
2016-09-06T19:44:34.768906: step 1626, loss 0.156655, acc 0.92
2016-09-06T19:44:35.468559: step 1627, loss 0.0474117, acc 1
2016-09-06T19:44:36.163178: step 1628, loss 0.0683451, acc 0.96
2016-09-06T19:44:36.851501: step 1629, loss 0.0630734, acc 1
2016-09-06T19:44:37.557564: step 1630, loss 0.0425094, acc 1
2016-09-06T19:44:38.225731: step 1631, loss 0.0994099, acc 0.94
2016-09-06T19:44:38.902366: step 1632, loss 0.0308638, acc 0.98
2016-09-06T19:44:39.598483: step 1633, loss 0.0499964, acc 0.96
2016-09-06T19:44:40.265748: step 1634, loss 0.0607274, acc 0.98
2016-09-06T19:44:40.956515: step 1635, loss 0.131554, acc 0.92
2016-09-06T19:44:41.645415: step 1636, loss 0.174889, acc 0.92
2016-09-06T19:44:42.372561: step 1637, loss 0.0355361, acc 0.98
2016-09-06T19:44:43.030664: step 1638, loss 0.0103961, acc 1
2016-09-06T19:44:43.722528: step 1639, loss 0.159299, acc 0.94
2016-09-06T19:44:44.423018: step 1640, loss 0.0256538, acc 0.98
2016-09-06T19:44:45.117061: step 1641, loss 0.0307568, acc 0.98
2016-09-06T19:44:45.821390: step 1642, loss 0.0426718, acc 1
2016-09-06T19:44:46.497355: step 1643, loss 0.0677153, acc 0.98
2016-09-06T19:44:47.203386: step 1644, loss 0.0360617, acc 1
2016-09-06T19:44:47.897647: step 1645, loss 0.231083, acc 0.92
2016-09-06T19:44:48.597236: step 1646, loss 0.110663, acc 0.96
2016-09-06T19:44:49.279512: step 1647, loss 0.151675, acc 0.94
2016-09-06T19:44:49.998145: step 1648, loss 0.150653, acc 0.92
2016-09-06T19:44:50.697333: step 1649, loss 0.12469, acc 0.96
2016-09-06T19:44:51.378208: step 1650, loss 0.0607672, acc 0.96
2016-09-06T19:44:52.088015: step 1651, loss 0.0282071, acc 1
2016-09-06T19:44:52.800845: step 1652, loss 0.0998297, acc 0.96
2016-09-06T19:44:53.508603: step 1653, loss 0.0432027, acc 1
2016-09-06T19:44:54.207107: step 1654, loss 0.0111681, acc 1
2016-09-06T19:44:54.883805: step 1655, loss 0.0785275, acc 0.96
2016-09-06T19:44:55.611316: step 1656, loss 0.0457232, acc 0.98
2016-09-06T19:44:56.300774: step 1657, loss 0.147991, acc 0.94
2016-09-06T19:44:57.015376: step 1658, loss 0.0363453, acc 1
2016-09-06T19:44:57.721837: step 1659, loss 0.180219, acc 0.94
2016-09-06T19:44:58.396065: step 1660, loss 0.0933246, acc 0.98
2016-09-06T19:44:59.079754: step 1661, loss 0.0907577, acc 0.94
2016-09-06T19:44:59.759069: step 1662, loss 0.0642649, acc 0.98
2016-09-06T19:45:00.488131: step 1663, loss 0.0853146, acc 0.94
2016-09-06T19:45:01.171023: step 1664, loss 0.0984383, acc 0.94
2016-09-06T19:45:01.858503: step 1665, loss 0.0613817, acc 0.98
2016-09-06T19:45:02.556951: step 1666, loss 0.0258034, acc 0.98
2016-09-06T19:45:03.253194: step 1667, loss 0.120248, acc 0.92
2016-09-06T19:45:03.955925: step 1668, loss 0.0807779, acc 0.94
2016-09-06T19:45:04.625023: step 1669, loss 0.136485, acc 0.92
2016-09-06T19:45:05.320837: step 1670, loss 0.0555519, acc 0.98
2016-09-06T19:45:06.012643: step 1671, loss 0.063394, acc 0.98
2016-09-06T19:45:06.702559: step 1672, loss 0.0311574, acc 0.98
2016-09-06T19:45:07.398255: step 1673, loss 0.0540354, acc 0.96
2016-09-06T19:45:08.096535: step 1674, loss 0.0269464, acc 0.98
2016-09-06T19:45:08.828230: step 1675, loss 0.0312831, acc 0.98
2016-09-06T19:45:09.517370: step 1676, loss 0.051361, acc 0.96
2016-09-06T19:45:10.193113: step 1677, loss 0.0780784, acc 0.98
2016-09-06T19:45:10.878089: step 1678, loss 0.0112087, acc 1
2016-09-06T19:45:11.571103: step 1679, loss 0.0829671, acc 0.96
2016-09-06T19:45:12.233909: step 1680, loss 0.0486697, acc 0.96
2016-09-06T19:45:12.911318: step 1681, loss 0.262145, acc 0.9
2016-09-06T19:45:13.622359: step 1682, loss 0.0204049, acc 0.98
2016-09-06T19:45:14.291359: step 1683, loss 0.026504, acc 1
2016-09-06T19:45:14.990010: step 1684, loss 0.0851965, acc 0.94
2016-09-06T19:45:15.690268: step 1685, loss 0.248171, acc 0.9
2016-09-06T19:45:16.388054: step 1686, loss 0.0656846, acc 0.96
2016-09-06T19:45:17.081183: step 1687, loss 0.0870624, acc 0.96
2016-09-06T19:45:17.776902: step 1688, loss 0.0940855, acc 0.94
2016-09-06T19:45:18.487462: step 1689, loss 0.0265721, acc 0.98
2016-09-06T19:45:19.162880: step 1690, loss 0.0508874, acc 0.98
2016-09-06T19:45:19.853002: step 1691, loss 0.0944496, acc 0.96
2016-09-06T19:45:20.547552: step 1692, loss 0.0919867, acc 0.98
2016-09-06T19:45:21.237517: step 1693, loss 0.0510749, acc 0.96
2016-09-06T19:45:21.929162: step 1694, loss 0.0806208, acc 0.96
2016-09-06T19:45:22.604514: step 1695, loss 0.0295583, acc 1
2016-09-06T19:45:23.284993: step 1696, loss 0.0729022, acc 0.98
2016-09-06T19:45:23.949220: step 1697, loss 0.076953, acc 0.96
2016-09-06T19:45:24.640450: step 1698, loss 0.0410244, acc 1
2016-09-06T19:45:25.331003: step 1699, loss 0.151227, acc 0.94
2016-09-06T19:45:26.020812: step 1700, loss 0.0699375, acc 0.96

Evaluation:
2016-09-06T19:45:29.188781: step 1700, loss 1.0707, acc 0.754221

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-1700

2016-09-06T19:45:30.847547: step 1701, loss 0.0279959, acc 1
2016-09-06T19:45:31.534748: step 1702, loss 0.0804886, acc 0.96
2016-09-06T19:45:32.234177: step 1703, loss 0.025274, acc 0.98
2016-09-06T19:45:32.947527: step 1704, loss 0.0576881, acc 0.98
2016-09-06T19:45:33.647163: step 1705, loss 0.0648676, acc 1
2016-09-06T19:45:34.341140: step 1706, loss 0.102099, acc 0.94
2016-09-06T19:45:35.037854: step 1707, loss 0.0755498, acc 0.98
2016-09-06T19:45:35.734582: step 1708, loss 0.144574, acc 0.96
2016-09-06T19:45:36.439085: step 1709, loss 0.0779893, acc 0.98
2016-09-06T19:45:37.109858: step 1710, loss 0.0408398, acc 0.98
2016-09-06T19:45:37.814574: step 1711, loss 0.0183521, acc 1
2016-09-06T19:45:38.497996: step 1712, loss 0.0541361, acc 0.96
2016-09-06T19:45:39.186874: step 1713, loss 0.0859537, acc 0.96
2016-09-06T19:45:39.881452: step 1714, loss 0.131317, acc 0.96
2016-09-06T19:45:40.568094: step 1715, loss 0.0152826, acc 1
2016-09-06T19:45:41.268222: step 1716, loss 0.0689612, acc 0.94
2016-09-06T19:45:41.930367: step 1717, loss 0.0138524, acc 1
2016-09-06T19:45:42.625355: step 1718, loss 0.0149671, acc 1
2016-09-06T19:45:43.323131: step 1719, loss 0.064587, acc 0.98
2016-09-06T19:45:44.060394: step 1720, loss 0.14812, acc 0.96
2016-09-06T19:45:44.745446: step 1721, loss 0.0147819, acc 1
2016-09-06T19:45:45.438878: step 1722, loss 0.110893, acc 0.96
2016-09-06T19:45:46.155758: step 1723, loss 0.09971, acc 0.92
2016-09-06T19:45:46.852122: step 1724, loss 0.0198936, acc 1
2016-09-06T19:45:47.543189: step 1725, loss 0.209443, acc 0.94
2016-09-06T19:45:48.223847: step 1726, loss 0.0498802, acc 0.96
2016-09-06T19:45:48.929962: step 1727, loss 0.0539273, acc 0.98
2016-09-06T19:45:49.568555: step 1728, loss 0.067379, acc 0.954545
2016-09-06T19:45:50.251478: step 1729, loss 0.0584883, acc 0.96
2016-09-06T19:45:50.978852: step 1730, loss 0.0410597, acc 0.98
2016-09-06T19:45:51.660555: step 1731, loss 0.0753477, acc 0.96
2016-09-06T19:45:52.357932: step 1732, loss 0.0634152, acc 0.96
2016-09-06T19:45:53.035574: step 1733, loss 0.0156493, acc 1
2016-09-06T19:45:53.708347: step 1734, loss 0.0940426, acc 0.98
2016-09-06T19:45:54.377710: step 1735, loss 0.0958948, acc 0.94
2016-09-06T19:45:55.058594: step 1736, loss 0.0852225, acc 0.94
2016-09-06T19:45:55.756236: step 1737, loss 0.018007, acc 1
2016-09-06T19:45:56.433319: step 1738, loss 0.0298575, acc 1
2016-09-06T19:45:57.105646: step 1739, loss 0.0284033, acc 0.98
2016-09-06T19:45:57.811749: step 1740, loss 0.0559945, acc 0.98
2016-09-06T19:45:58.519825: step 1741, loss 0.0600278, acc 0.96
2016-09-06T19:45:59.231109: step 1742, loss 0.123219, acc 0.92
2016-09-06T19:45:59.931790: step 1743, loss 0.0231428, acc 0.98
2016-09-06T19:46:00.701723: step 1744, loss 0.101364, acc 0.96
2016-09-06T19:46:01.412667: step 1745, loss 0.168133, acc 0.96
2016-09-06T19:46:02.084577: step 1746, loss 0.108182, acc 0.94
2016-09-06T19:46:02.777334: step 1747, loss 0.077851, acc 0.92
2016-09-06T19:46:03.482168: step 1748, loss 0.0312751, acc 0.98
2016-09-06T19:46:04.173157: step 1749, loss 0.0625831, acc 0.98
2016-09-06T19:46:04.845119: step 1750, loss 0.0270267, acc 0.98
2016-09-06T19:46:05.507782: step 1751, loss 0.0344396, acc 0.98
2016-09-06T19:46:06.188068: step 1752, loss 0.176592, acc 0.94
2016-09-06T19:46:06.870520: step 1753, loss 0.0526411, acc 0.96
2016-09-06T19:46:07.563860: step 1754, loss 0.0428005, acc 0.98
2016-09-06T19:46:08.244980: step 1755, loss 0.00947789, acc 1
2016-09-06T19:46:08.951353: step 1756, loss 0.0094642, acc 1
2016-09-06T19:46:09.612644: step 1757, loss 0.0586473, acc 0.98
2016-09-06T19:46:10.312735: step 1758, loss 0.0101942, acc 1
2016-09-06T19:46:10.996220: step 1759, loss 0.0738667, acc 0.96
2016-09-06T19:46:11.676996: step 1760, loss 0.0844279, acc 0.96
2016-09-06T19:46:12.342506: step 1761, loss 0.0294354, acc 0.98
2016-09-06T19:46:13.038549: step 1762, loss 0.00856959, acc 1
2016-09-06T19:46:13.743497: step 1763, loss 0.163994, acc 0.94
2016-09-06T19:46:14.429524: step 1764, loss 0.0261266, acc 0.98
2016-09-06T19:46:15.116814: step 1765, loss 0.0900474, acc 0.98
2016-09-06T19:46:15.807733: step 1766, loss 0.0396162, acc 1
2016-09-06T19:46:16.493668: step 1767, loss 0.0576073, acc 0.96
2016-09-06T19:46:17.182131: step 1768, loss 0.0325364, acc 1
2016-09-06T19:46:17.874657: step 1769, loss 0.0608406, acc 0.98
2016-09-06T19:46:18.584033: step 1770, loss 0.0673555, acc 0.96
2016-09-06T19:46:19.270294: step 1771, loss 0.135116, acc 0.92
2016-09-06T19:46:19.951352: step 1772, loss 0.0701771, acc 0.96
2016-09-06T19:46:20.640796: step 1773, loss 0.0465552, acc 0.98
2016-09-06T19:46:21.326730: step 1774, loss 0.0369643, acc 0.98
2016-09-06T19:46:22.010692: step 1775, loss 0.0315796, acc 0.98
2016-09-06T19:46:22.688430: step 1776, loss 0.0371073, acc 0.98
2016-09-06T19:46:23.407977: step 1777, loss 0.0303747, acc 1
2016-09-06T19:46:24.075318: step 1778, loss 0.0998059, acc 0.92
2016-09-06T19:46:24.747582: step 1779, loss 0.0482637, acc 0.96
2016-09-06T19:46:25.435977: step 1780, loss 0.00494717, acc 1
2016-09-06T19:46:26.137687: step 1781, loss 0.0109628, acc 1
2016-09-06T19:46:26.832008: step 1782, loss 0.19655, acc 0.96
2016-09-06T19:46:27.507948: step 1783, loss 0.0245719, acc 0.98
2016-09-06T19:46:28.200653: step 1784, loss 0.0193172, acc 0.98
2016-09-06T19:46:28.893714: step 1785, loss 0.0729328, acc 0.96
2016-09-06T19:46:29.588632: step 1786, loss 0.126296, acc 0.94
2016-09-06T19:46:30.276058: step 1787, loss 0.0580521, acc 0.96
2016-09-06T19:46:30.994661: step 1788, loss 0.0172411, acc 1
2016-09-06T19:46:31.698377: step 1789, loss 0.0266612, acc 1
2016-09-06T19:46:32.372864: step 1790, loss 0.0784978, acc 0.98
2016-09-06T19:46:33.086531: step 1791, loss 0.148599, acc 0.96
2016-09-06T19:46:33.777768: step 1792, loss 0.0559262, acc 0.96
2016-09-06T19:46:34.461956: step 1793, loss 0.0306194, acc 0.98
2016-09-06T19:46:35.175153: step 1794, loss 0.157521, acc 0.96
2016-09-06T19:46:35.873355: step 1795, loss 0.0397101, acc 0.98
2016-09-06T19:46:36.588164: step 1796, loss 0.0662177, acc 0.96
2016-09-06T19:46:37.261262: step 1797, loss 0.100602, acc 0.94
2016-09-06T19:46:37.953804: step 1798, loss 0.00654182, acc 1
2016-09-06T19:46:38.648648: step 1799, loss 0.0751879, acc 0.96
2016-09-06T19:46:39.333233: step 1800, loss 0.0222369, acc 0.98

Evaluation:
2016-09-06T19:46:42.500639: step 1800, loss 1.20351, acc 0.75985

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-1800

2016-09-06T19:46:44.162479: step 1801, loss 0.113016, acc 0.94
2016-09-06T19:46:44.854926: step 1802, loss 0.111945, acc 0.92
2016-09-06T19:46:45.528994: step 1803, loss 0.0150388, acc 1
2016-09-06T19:46:46.234300: step 1804, loss 0.0439031, acc 0.96
2016-09-06T19:46:46.913387: step 1805, loss 0.0586971, acc 0.94
2016-09-06T19:46:47.586504: step 1806, loss 0.0410394, acc 1
2016-09-06T19:46:48.286327: step 1807, loss 0.0660857, acc 0.96
2016-09-06T19:46:48.994555: step 1808, loss 0.0717467, acc 0.96
2016-09-06T19:46:49.733010: step 1809, loss 0.0252997, acc 0.98
2016-09-06T19:46:50.400770: step 1810, loss 0.034997, acc 0.98
2016-09-06T19:46:51.132778: step 1811, loss 0.0728134, acc 0.96
2016-09-06T19:46:51.823953: step 1812, loss 0.0697574, acc 0.96
2016-09-06T19:46:52.500885: step 1813, loss 0.0634117, acc 0.96
2016-09-06T19:46:53.179997: step 1814, loss 0.135767, acc 0.96
2016-09-06T19:46:53.878320: step 1815, loss 0.0480155, acc 0.98
2016-09-06T19:46:54.576655: step 1816, loss 0.039354, acc 0.98
2016-09-06T19:46:55.309821: step 1817, loss 0.0781435, acc 0.96
2016-09-06T19:46:56.010481: step 1818, loss 0.0809141, acc 0.94
2016-09-06T19:46:56.688016: step 1819, loss 0.00809899, acc 1
2016-09-06T19:46:57.382440: step 1820, loss 0.0747869, acc 0.92
2016-09-06T19:46:58.079990: step 1821, loss 0.0342475, acc 0.98
2016-09-06T19:46:58.775150: step 1822, loss 0.0478189, acc 0.98
2016-09-06T19:46:59.473605: step 1823, loss 0.0511237, acc 0.98
2016-09-06T19:47:00.137978: step 1824, loss 0.0262932, acc 1
2016-09-06T19:47:00.858294: step 1825, loss 0.027089, acc 0.98
2016-09-06T19:47:01.532320: step 1826, loss 0.0611687, acc 0.98
2016-09-06T19:47:02.212818: step 1827, loss 0.159313, acc 0.92
2016-09-06T19:47:02.899354: step 1828, loss 0.0499262, acc 1
2016-09-06T19:47:03.577455: step 1829, loss 0.0474904, acc 0.98
2016-09-06T19:47:04.288752: step 1830, loss 0.0134646, acc 1
2016-09-06T19:47:04.963391: step 1831, loss 0.0398415, acc 1
2016-09-06T19:47:05.648487: step 1832, loss 0.0491659, acc 0.98
2016-09-06T19:47:06.362609: step 1833, loss 0.0185696, acc 1
2016-09-06T19:47:07.061015: step 1834, loss 0.0240274, acc 1
2016-09-06T19:47:07.741919: step 1835, loss 0.0376046, acc 0.98
2016-09-06T19:47:08.429753: step 1836, loss 0.030405, acc 1
2016-09-06T19:47:09.123452: step 1837, loss 0.0226787, acc 1
2016-09-06T19:47:09.803040: step 1838, loss 0.0654656, acc 0.96
2016-09-06T19:47:10.499674: step 1839, loss 0.0498069, acc 0.96
2016-09-06T19:47:11.199260: step 1840, loss 0.163492, acc 0.92
2016-09-06T19:47:11.897586: step 1841, loss 0.0796084, acc 0.98
2016-09-06T19:47:12.589545: step 1842, loss 0.0697028, acc 0.96
2016-09-06T19:47:13.260023: step 1843, loss 0.128829, acc 0.94
2016-09-06T19:47:13.968555: step 1844, loss 0.0193813, acc 1
2016-09-06T19:47:14.691555: step 1845, loss 0.113269, acc 0.96
2016-09-06T19:47:15.391629: step 1846, loss 0.0329316, acc 0.98
2016-09-06T19:47:16.118635: step 1847, loss 0.0510969, acc 0.96
2016-09-06T19:47:16.832758: step 1848, loss 0.03567, acc 0.96
2016-09-06T19:47:17.537891: step 1849, loss 0.0161169, acc 1
2016-09-06T19:47:18.217344: step 1850, loss 0.0385635, acc 0.98
2016-09-06T19:47:18.897709: step 1851, loss 0.00438605, acc 1
2016-09-06T19:47:19.570508: step 1852, loss 0.00739734, acc 1
2016-09-06T19:47:20.255405: step 1853, loss 0.0907924, acc 0.96
2016-09-06T19:47:20.965460: step 1854, loss 0.0073245, acc 1
2016-09-06T19:47:21.629112: step 1855, loss 0.072384, acc 0.98
2016-09-06T19:47:22.329013: step 1856, loss 0.0555875, acc 0.96
2016-09-06T19:47:23.006173: step 1857, loss 0.0161526, acc 1
2016-09-06T19:47:23.689745: step 1858, loss 0.0498491, acc 0.98
2016-09-06T19:47:24.372203: step 1859, loss 0.139332, acc 0.92
2016-09-06T19:47:25.044582: step 1860, loss 0.225307, acc 0.92
2016-09-06T19:47:25.738165: step 1861, loss 0.132093, acc 0.94
2016-09-06T19:47:26.445817: step 1862, loss 0.083803, acc 0.98
2016-09-06T19:47:27.151536: step 1863, loss 0.0429688, acc 0.96
2016-09-06T19:47:27.842786: step 1864, loss 0.0161515, acc 1
2016-09-06T19:47:28.529062: step 1865, loss 0.0568453, acc 0.98
2016-09-06T19:47:29.203191: step 1866, loss 0.0082236, acc 1
2016-09-06T19:47:29.888815: step 1867, loss 0.0852166, acc 0.94
2016-09-06T19:47:30.582924: step 1868, loss 0.0881323, acc 0.96
2016-09-06T19:47:31.251800: step 1869, loss 0.183246, acc 0.94
2016-09-06T19:47:31.982262: step 1870, loss 0.0528582, acc 0.96
2016-09-06T19:47:32.729500: step 1871, loss 0.075764, acc 0.96
2016-09-06T19:47:33.419224: step 1872, loss 0.0777204, acc 0.96
2016-09-06T19:47:34.126180: step 1873, loss 0.101311, acc 0.94
2016-09-06T19:47:34.815357: step 1874, loss 0.0499973, acc 1
2016-09-06T19:47:35.515119: step 1875, loss 0.062178, acc 0.96
2016-09-06T19:47:36.190686: step 1876, loss 0.0918473, acc 0.94
2016-09-06T19:47:36.896298: step 1877, loss 0.0821809, acc 0.96
2016-09-06T19:47:37.592211: step 1878, loss 0.0371188, acc 1
2016-09-06T19:47:38.279938: step 1879, loss 0.0451984, acc 1
2016-09-06T19:47:38.958123: step 1880, loss 0.0547994, acc 1
2016-09-06T19:47:39.645723: step 1881, loss 0.0302518, acc 1
2016-09-06T19:47:40.365667: step 1882, loss 0.0891705, acc 0.94
2016-09-06T19:47:41.043071: step 1883, loss 0.0498893, acc 0.98
2016-09-06T19:47:41.731623: step 1884, loss 0.0393592, acc 0.98
2016-09-06T19:47:42.433837: step 1885, loss 0.131393, acc 0.96
2016-09-06T19:47:43.144621: step 1886, loss 0.0259765, acc 1
2016-09-06T19:47:43.829487: step 1887, loss 0.0245916, acc 1
2016-09-06T19:47:44.515377: step 1888, loss 0.063472, acc 0.98
2016-09-06T19:47:45.212261: step 1889, loss 0.0234669, acc 1
2016-09-06T19:47:45.874551: step 1890, loss 0.0303776, acc 1
2016-09-06T19:47:46.544983: step 1891, loss 0.115001, acc 0.94
2016-09-06T19:47:47.226375: step 1892, loss 0.0229198, acc 1
2016-09-06T19:47:47.918911: step 1893, loss 0.172707, acc 0.94
2016-09-06T19:47:48.619787: step 1894, loss 0.0581585, acc 0.96
2016-09-06T19:47:49.288441: step 1895, loss 0.0825711, acc 0.96
2016-09-06T19:47:49.996083: step 1896, loss 0.0474244, acc 0.96
2016-09-06T19:47:50.668650: step 1897, loss 0.053358, acc 0.96
2016-09-06T19:47:51.351773: step 1898, loss 0.00448503, acc 1
2016-09-06T19:47:52.038729: step 1899, loss 0.00449842, acc 1
2016-09-06T19:47:52.719761: step 1900, loss 0.0291567, acc 0.98

Evaluation:
2016-09-06T19:47:55.883900: step 1900, loss 1.51923, acc 0.752345

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-1900

2016-09-06T19:47:57.714064: step 1901, loss 0.120928, acc 0.94
2016-09-06T19:47:58.416957: step 1902, loss 0.0126848, acc 1
2016-09-06T19:47:59.093437: step 1903, loss 0.0413258, acc 0.98
2016-09-06T19:47:59.786740: step 1904, loss 0.144719, acc 0.98
2016-09-06T19:48:00.495660: step 1905, loss 0.105809, acc 0.94
2016-09-06T19:48:01.194590: step 1906, loss 0.071197, acc 0.98
2016-09-06T19:48:01.906872: step 1907, loss 0.036018, acc 0.98
2016-09-06T19:48:02.591062: step 1908, loss 0.0595831, acc 0.96
2016-09-06T19:48:03.286408: step 1909, loss 0.027486, acc 1
2016-09-06T19:48:04.019767: step 1910, loss 0.164656, acc 0.92
2016-09-06T19:48:04.706484: step 1911, loss 0.0219786, acc 0.98
2016-09-06T19:48:05.392581: step 1912, loss 0.268419, acc 0.92
2016-09-06T19:48:06.087304: step 1913, loss 0.024804, acc 1
2016-09-06T19:48:06.815578: step 1914, loss 0.0080433, acc 1
2016-09-06T19:48:07.492582: step 1915, loss 0.0428709, acc 0.98
2016-09-06T19:48:08.181912: step 1916, loss 0.07555, acc 0.98
2016-09-06T19:48:08.872863: step 1917, loss 0.0876807, acc 0.98
2016-09-06T19:48:09.575094: step 1918, loss 0.141292, acc 0.9
2016-09-06T19:48:10.242838: step 1919, loss 0.0216779, acc 1
2016-09-06T19:48:10.888397: step 1920, loss 0.00473306, acc 1
2016-09-06T19:48:11.573263: step 1921, loss 0.0424506, acc 0.98
2016-09-06T19:48:12.273970: step 1922, loss 0.0986726, acc 0.96
2016-09-06T19:48:12.975709: step 1923, loss 0.0613766, acc 0.96
2016-09-06T19:48:13.652038: step 1924, loss 0.0625531, acc 0.98
2016-09-06T19:48:14.360389: step 1925, loss 0.0438222, acc 0.98
2016-09-06T19:48:15.045219: step 1926, loss 0.157504, acc 0.96
2016-09-06T19:48:15.738845: step 1927, loss 0.0159824, acc 1
2016-09-06T19:48:16.443264: step 1928, loss 0.261096, acc 0.92
2016-09-06T19:48:17.118648: step 1929, loss 0.0329835, acc 0.98
2016-09-06T19:48:17.798289: step 1930, loss 0.065161, acc 0.94
2016-09-06T19:48:18.493008: step 1931, loss 0.0652644, acc 0.96
2016-09-06T19:48:19.178152: step 1932, loss 0.0468643, acc 0.98
2016-09-06T19:48:19.864565: step 1933, loss 0.0388489, acc 1
2016-09-06T19:48:20.563559: step 1934, loss 0.0694967, acc 0.94
2016-09-06T19:48:21.264866: step 1935, loss 0.0614042, acc 0.98
2016-09-06T19:48:21.953210: step 1936, loss 0.0510843, acc 1
2016-09-06T19:48:22.646263: step 1937, loss 0.0738137, acc 0.96
2016-09-06T19:48:23.347726: step 1938, loss 0.0253715, acc 1
2016-09-06T19:48:24.045932: step 1939, loss 0.0516285, acc 0.96
2016-09-06T19:48:24.743361: step 1940, loss 0.0352621, acc 0.98
2016-09-06T19:48:25.416151: step 1941, loss 0.0635806, acc 0.96
2016-09-06T19:48:26.111842: step 1942, loss 0.0320783, acc 0.96
2016-09-06T19:48:26.801465: step 1943, loss 0.0628147, acc 0.98
2016-09-06T19:48:27.489983: step 1944, loss 0.107363, acc 0.94
2016-09-06T19:48:28.168560: step 1945, loss 0.0482727, acc 0.98
2016-09-06T19:48:28.854191: step 1946, loss 0.101675, acc 0.94
2016-09-06T19:48:29.562204: step 1947, loss 0.0667942, acc 0.96
2016-09-06T19:48:30.228766: step 1948, loss 0.0335927, acc 0.98
2016-09-06T19:48:30.921517: step 1949, loss 0.0288247, acc 1
2016-09-06T19:48:31.598263: step 1950, loss 0.127532, acc 0.96
2016-09-06T19:48:32.275283: step 1951, loss 0.0515885, acc 0.98
2016-09-06T19:48:32.967828: step 1952, loss 0.055618, acc 0.96
2016-09-06T19:48:33.665343: step 1953, loss 0.0267262, acc 0.98
2016-09-06T19:48:34.361004: step 1954, loss 0.0771558, acc 0.96
2016-09-06T19:48:35.057049: step 1955, loss 0.0314378, acc 0.98
2016-09-06T19:48:35.758557: step 1956, loss 0.0756865, acc 0.98
2016-09-06T19:48:36.444525: step 1957, loss 0.0567552, acc 0.98
2016-09-06T19:48:37.141396: step 1958, loss 0.0354091, acc 0.98
2016-09-06T19:48:37.833109: step 1959, loss 0.0126392, acc 1
2016-09-06T19:48:38.514586: step 1960, loss 0.0850391, acc 0.98
2016-09-06T19:48:39.217900: step 1961, loss 0.135516, acc 0.92
2016-09-06T19:48:39.867171: step 1962, loss 0.0656783, acc 1
2016-09-06T19:48:40.577050: step 1963, loss 0.106291, acc 0.96
2016-09-06T19:48:41.259883: step 1964, loss 0.0331884, acc 1
2016-09-06T19:48:41.958091: step 1965, loss 0.0825931, acc 0.96
2016-09-06T19:48:42.669097: step 1966, loss 0.0557304, acc 1
2016-09-06T19:48:43.371209: step 1967, loss 0.0390901, acc 0.98
2016-09-06T19:48:44.095291: step 1968, loss 0.00940496, acc 1
2016-09-06T19:48:44.777543: step 1969, loss 0.0388371, acc 0.98
2016-09-06T19:48:45.480934: step 1970, loss 0.0162413, acc 1
2016-09-06T19:48:46.156588: step 1971, loss 0.032812, acc 0.98
2016-09-06T19:48:46.858352: step 1972, loss 0.00768785, acc 1
2016-09-06T19:48:47.536201: step 1973, loss 0.0477035, acc 0.98
2016-09-06T19:48:48.245017: step 1974, loss 0.0512291, acc 0.98
2016-09-06T19:48:48.984628: step 1975, loss 0.123329, acc 0.94
2016-09-06T19:48:49.680674: step 1976, loss 0.109839, acc 0.94
2016-09-06T19:48:50.375341: step 1977, loss 0.01304, acc 1
2016-09-06T19:48:51.060604: step 1978, loss 0.0256966, acc 1
2016-09-06T19:48:51.745385: step 1979, loss 0.136377, acc 0.96
2016-09-06T19:48:52.446527: step 1980, loss 0.0829524, acc 0.96
2016-09-06T19:48:53.120135: step 1981, loss 0.0224741, acc 0.98
2016-09-06T19:48:53.822201: step 1982, loss 0.100486, acc 0.96
2016-09-06T19:48:54.514026: step 1983, loss 0.0624688, acc 0.96
2016-09-06T19:48:55.170863: step 1984, loss 0.0546549, acc 0.98
2016-09-06T19:48:55.845357: step 1985, loss 0.063783, acc 0.96
2016-09-06T19:48:56.526043: step 1986, loss 0.0389386, acc 0.98
2016-09-06T19:48:57.227953: step 1987, loss 0.0723805, acc 0.98
2016-09-06T19:48:57.902837: step 1988, loss 0.108428, acc 0.98
2016-09-06T19:48:58.613481: step 1989, loss 0.0890692, acc 0.96
2016-09-06T19:48:59.286270: step 1990, loss 0.0107325, acc 1
2016-09-06T19:48:59.979250: step 1991, loss 0.0690932, acc 0.98
2016-09-06T19:49:00.724289: step 1992, loss 0.173756, acc 0.96
2016-09-06T19:49:01.426619: step 1993, loss 0.0433606, acc 0.96
2016-09-06T19:49:02.129297: step 1994, loss 0.0360749, acc 1
2016-09-06T19:49:02.793801: step 1995, loss 0.0486748, acc 0.98
2016-09-06T19:49:03.457904: step 1996, loss 0.0419358, acc 0.96
2016-09-06T19:49:04.157697: step 1997, loss 0.196763, acc 0.9
2016-09-06T19:49:04.859177: step 1998, loss 0.0616176, acc 0.96
2016-09-06T19:49:05.551060: step 1999, loss 0.0440147, acc 0.98
2016-09-06T19:49:06.243615: step 2000, loss 0.0918517, acc 0.94

Evaluation:
2016-09-06T19:49:09.392995: step 2000, loss 1.22035, acc 0.753283

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-2000

2016-09-06T19:49:11.020325: step 2001, loss 0.048864, acc 0.98
2016-09-06T19:49:11.721750: step 2002, loss 0.0915658, acc 0.96
2016-09-06T19:49:12.388625: step 2003, loss 0.04564, acc 0.98
2016-09-06T19:49:13.087836: step 2004, loss 0.12266, acc 0.96
2016-09-06T19:49:13.762502: step 2005, loss 0.0155493, acc 1
2016-09-06T19:49:14.448516: step 2006, loss 0.042971, acc 0.98
2016-09-06T19:49:15.142524: step 2007, loss 0.0497132, acc 1
2016-09-06T19:49:15.828614: step 2008, loss 0.00795295, acc 1
2016-09-06T19:49:16.526818: step 2009, loss 0.0997923, acc 0.96
2016-09-06T19:49:17.199830: step 2010, loss 0.0174926, acc 1
2016-09-06T19:49:17.894292: step 2011, loss 0.0216262, acc 1
2016-09-06T19:49:18.587890: step 2012, loss 0.032372, acc 1
2016-09-06T19:49:19.283151: step 2013, loss 0.0476702, acc 0.96
2016-09-06T19:49:19.989863: step 2014, loss 0.0444759, acc 0.98
2016-09-06T19:49:20.695632: step 2015, loss 0.034918, acc 0.98
2016-09-06T19:49:21.403377: step 2016, loss 0.153403, acc 0.98
2016-09-06T19:49:22.081723: step 2017, loss 0.0660325, acc 0.94
2016-09-06T19:49:22.770009: step 2018, loss 0.051091, acc 0.98
2016-09-06T19:49:23.457673: step 2019, loss 0.0482951, acc 0.98
2016-09-06T19:49:24.149407: step 2020, loss 0.0369583, acc 0.96
2016-09-06T19:49:24.843406: step 2021, loss 0.0524078, acc 0.98
2016-09-06T19:49:25.500770: step 2022, loss 0.0198372, acc 1
2016-09-06T19:49:26.204468: step 2023, loss 0.112894, acc 0.98
2016-09-06T19:49:26.874999: step 2024, loss 0.0219675, acc 1
2016-09-06T19:49:27.560220: step 2025, loss 0.0199669, acc 1
2016-09-06T19:49:28.244276: step 2026, loss 0.0233077, acc 1
2016-09-06T19:49:28.954250: step 2027, loss 0.030407, acc 0.98
2016-09-06T19:49:29.651673: step 2028, loss 0.0152173, acc 1
2016-09-06T19:49:30.318616: step 2029, loss 0.0833612, acc 0.96
2016-09-06T19:49:31.018069: step 2030, loss 0.0424485, acc 0.96
2016-09-06T19:49:31.678865: step 2031, loss 0.047368, acc 0.98
2016-09-06T19:49:32.364223: step 2032, loss 0.00275191, acc 1
2016-09-06T19:49:33.048765: step 2033, loss 0.0513603, acc 0.94
2016-09-06T19:49:33.730441: step 2034, loss 0.0814739, acc 0.94
2016-09-06T19:49:34.404809: step 2035, loss 0.0598104, acc 0.98
2016-09-06T19:49:35.095042: step 2036, loss 0.0174443, acc 1
2016-09-06T19:49:35.843103: step 2037, loss 0.0324916, acc 1
2016-09-06T19:49:36.529788: step 2038, loss 0.00509359, acc 1
2016-09-06T19:49:37.210787: step 2039, loss 0.189324, acc 0.96
2016-09-06T19:49:37.876620: step 2040, loss 0.0766684, acc 0.96
2016-09-06T19:49:38.555567: step 2041, loss 0.0243331, acc 1
2016-09-06T19:49:39.236353: step 2042, loss 0.00754266, acc 1
2016-09-06T19:49:39.912990: step 2043, loss 0.104568, acc 0.98
2016-09-06T19:49:40.628300: step 2044, loss 0.024176, acc 1
2016-09-06T19:49:41.299967: step 2045, loss 0.0112047, acc 1
2016-09-06T19:49:41.987992: step 2046, loss 0.0481204, acc 0.98
2016-09-06T19:49:42.689111: step 2047, loss 0.0178732, acc 1
2016-09-06T19:49:43.376980: step 2048, loss 0.100814, acc 0.98
2016-09-06T19:49:44.046086: step 2049, loss 0.0744039, acc 0.96
2016-09-06T19:49:44.727649: step 2050, loss 0.215355, acc 0.94
2016-09-06T19:49:45.423049: step 2051, loss 0.162985, acc 0.96
2016-09-06T19:49:46.108167: step 2052, loss 0.0418415, acc 0.98
2016-09-06T19:49:46.778213: step 2053, loss 0.0512651, acc 0.98
2016-09-06T19:49:47.488985: step 2054, loss 0.0690894, acc 0.98
2016-09-06T19:49:48.169889: step 2055, loss 0.027901, acc 0.98
2016-09-06T19:49:48.869586: step 2056, loss 0.0619437, acc 0.98
2016-09-06T19:49:49.536295: step 2057, loss 0.0537343, acc 0.98
2016-09-06T19:49:50.226292: step 2058, loss 0.0366015, acc 0.98
2016-09-06T19:49:50.904884: step 2059, loss 0.088405, acc 0.96
2016-09-06T19:49:51.593681: step 2060, loss 0.0396195, acc 0.98
2016-09-06T19:49:52.273929: step 2061, loss 0.0191623, acc 1
2016-09-06T19:49:52.969217: step 2062, loss 0.0562797, acc 0.98
2016-09-06T19:49:53.650170: step 2063, loss 0.037934, acc 1
2016-09-06T19:49:54.345547: step 2064, loss 0.0307326, acc 0.98
2016-09-06T19:49:55.036366: step 2065, loss 0.0568678, acc 0.96
2016-09-06T19:49:55.697877: step 2066, loss 0.0254379, acc 0.98
2016-09-06T19:49:56.366665: step 2067, loss 0.0719143, acc 0.98
2016-09-06T19:49:57.056644: step 2068, loss 0.101239, acc 0.92
2016-09-06T19:49:57.740922: step 2069, loss 0.0300619, acc 1
2016-09-06T19:49:58.417664: step 2070, loss 0.0485497, acc 0.98
2016-09-06T19:49:59.090273: step 2071, loss 0.0687359, acc 0.98
2016-09-06T19:49:59.758616: step 2072, loss 0.0513704, acc 0.96
2016-09-06T19:50:00.471195: step 2073, loss 0.0427685, acc 1
2016-09-06T19:50:01.188162: step 2074, loss 0.0134033, acc 1
2016-09-06T19:50:01.887957: step 2075, loss 0.0689316, acc 0.98
2016-09-06T19:50:02.579318: step 2076, loss 0.0308785, acc 0.98
2016-09-06T19:50:03.275654: step 2077, loss 0.0615408, acc 0.98
2016-09-06T19:50:03.980492: step 2078, loss 0.0713432, acc 0.94
2016-09-06T19:50:04.727342: step 2079, loss 0.00490701, acc 1
2016-09-06T19:50:05.414646: step 2080, loss 0.0761335, acc 0.94
2016-09-06T19:50:06.132608: step 2081, loss 0.0590333, acc 0.96
2016-09-06T19:50:06.827315: step 2082, loss 0.138627, acc 0.94
2016-09-06T19:50:07.529269: step 2083, loss 0.0369239, acc 0.98
2016-09-06T19:50:08.247398: step 2084, loss 0.0476431, acc 0.98
2016-09-06T19:50:08.929898: step 2085, loss 0.0411576, acc 0.96
2016-09-06T19:50:09.609807: step 2086, loss 0.0895919, acc 0.98
2016-09-06T19:50:10.286731: step 2087, loss 0.00746543, acc 1
2016-09-06T19:50:10.977056: step 2088, loss 0.0260033, acc 0.98
2016-09-06T19:50:11.674363: step 2089, loss 0.0661775, acc 0.96
2016-09-06T19:50:12.380501: step 2090, loss 0.139583, acc 0.94
2016-09-06T19:50:13.078237: step 2091, loss 0.067285, acc 0.98
2016-09-06T19:50:13.758034: step 2092, loss 0.0181183, acc 1
2016-09-06T19:50:14.429226: step 2093, loss 0.186655, acc 0.92
2016-09-06T19:50:15.143467: step 2094, loss 0.0391406, acc 0.96
2016-09-06T19:50:15.838016: step 2095, loss 0.0499569, acc 0.98
2016-09-06T19:50:16.512810: step 2096, loss 0.0777871, acc 0.96
2016-09-06T19:50:17.198148: step 2097, loss 0.15941, acc 0.96
2016-09-06T19:50:17.899399: step 2098, loss 0.124389, acc 0.96
2016-09-06T19:50:18.568249: step 2099, loss 0.0342364, acc 0.98
2016-09-06T19:50:19.234409: step 2100, loss 0.0546504, acc 0.98

Evaluation:
2016-09-06T19:50:22.393161: step 2100, loss 1.27236, acc 0.758912

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-2100

2016-09-06T19:50:24.098031: step 2101, loss 0.0462671, acc 0.98
2016-09-06T19:50:24.784770: step 2102, loss 0.0523924, acc 0.98
2016-09-06T19:50:25.481623: step 2103, loss 0.0100195, acc 1
2016-09-06T19:50:26.187987: step 2104, loss 0.0723535, acc 0.96
2016-09-06T19:50:26.896823: step 2105, loss 0.066579, acc 0.96
2016-09-06T19:50:27.594342: step 2106, loss 0.0311546, acc 0.98
2016-09-06T19:50:28.280487: step 2107, loss 0.0811629, acc 0.96
2016-09-06T19:50:28.958160: step 2108, loss 0.00784923, acc 1
2016-09-06T19:50:29.671608: step 2109, loss 0.0820552, acc 0.94
2016-09-06T19:50:30.349470: step 2110, loss 0.0288131, acc 0.98
2016-09-06T19:50:31.062061: step 2111, loss 0.0284469, acc 0.98
2016-09-06T19:50:31.691161: step 2112, loss 0.0326499, acc 1
2016-09-06T19:50:32.389364: step 2113, loss 0.0559968, acc 0.96
2016-09-06T19:50:33.078750: step 2114, loss 0.0785871, acc 0.96
2016-09-06T19:50:33.773697: step 2115, loss 0.204618, acc 0.88
2016-09-06T19:50:34.448277: step 2116, loss 0.0181896, acc 1
2016-09-06T19:50:35.155268: step 2117, loss 0.058681, acc 0.98
2016-09-06T19:50:35.851191: step 2118, loss 0.0696029, acc 0.96
2016-09-06T19:50:36.537957: step 2119, loss 0.0187312, acc 1
2016-09-06T19:50:37.254929: step 2120, loss 0.0547714, acc 0.98
2016-09-06T19:50:37.963024: step 2121, loss 0.0812752, acc 0.96
2016-09-06T19:50:38.636368: step 2122, loss 0.0431009, acc 0.96
2016-09-06T19:50:39.319392: step 2123, loss 0.028484, acc 0.98
2016-09-06T19:50:40.004909: step 2124, loss 0.0485369, acc 1
2016-09-06T19:50:40.708829: step 2125, loss 0.083409, acc 0.94
2016-09-06T19:50:41.401556: step 2126, loss 0.102191, acc 0.96
2016-09-06T19:50:42.086326: step 2127, loss 0.0538186, acc 0.96
2016-09-06T19:50:42.766400: step 2128, loss 0.0195511, acc 1
2016-09-06T19:50:43.468895: step 2129, loss 0.13185, acc 0.92
2016-09-06T19:50:44.137656: step 2130, loss 0.139293, acc 0.94
2016-09-06T19:50:44.820589: step 2131, loss 0.04171, acc 0.96
2016-09-06T19:50:45.519596: step 2132, loss 0.0757632, acc 0.96
2016-09-06T19:50:46.178264: step 2133, loss 0.0195445, acc 1
2016-09-06T19:50:46.866100: step 2134, loss 0.0362767, acc 0.98
2016-09-06T19:50:47.563166: step 2135, loss 0.0571816, acc 0.98
2016-09-06T19:50:48.245058: step 2136, loss 0.109372, acc 0.94
2016-09-06T19:50:48.931953: step 2137, loss 0.0271096, acc 1
2016-09-06T19:50:49.606787: step 2138, loss 0.0686575, acc 0.98
2016-09-06T19:50:50.306589: step 2139, loss 0.00533448, acc 1
2016-09-06T19:50:50.965291: step 2140, loss 0.0396238, acc 0.98
2016-09-06T19:50:51.671996: step 2141, loss 0.0771423, acc 0.96
2016-09-06T19:50:52.371000: step 2142, loss 0.0766846, acc 0.94
2016-09-06T19:50:53.047058: step 2143, loss 0.0114839, acc 1
2016-09-06T19:50:53.712667: step 2144, loss 0.0223878, acc 0.98
2016-09-06T19:50:54.409593: step 2145, loss 0.0367658, acc 0.96
2016-09-06T19:50:55.106048: step 2146, loss 0.0810359, acc 0.96
2016-09-06T19:50:55.774752: step 2147, loss 0.0455737, acc 0.98
2016-09-06T19:50:56.471283: step 2148, loss 0.0606922, acc 0.94
2016-09-06T19:50:57.168771: step 2149, loss 0.0186864, acc 1
2016-09-06T19:50:57.872012: step 2150, loss 0.0360601, acc 1
2016-09-06T19:50:58.548868: step 2151, loss 0.0181014, acc 1
2016-09-06T19:50:59.212673: step 2152, loss 0.0525808, acc 0.96
2016-09-06T19:50:59.900264: step 2153, loss 0.0426283, acc 0.96
2016-09-06T19:51:00.602288: step 2154, loss 0.0503647, acc 0.98
2016-09-06T19:51:01.317837: step 2155, loss 0.0180669, acc 1
2016-09-06T19:51:02.012199: step 2156, loss 0.0786617, acc 0.96
2016-09-06T19:51:02.699281: step 2157, loss 0.0299027, acc 0.98
2016-09-06T19:51:03.382644: step 2158, loss 0.0274003, acc 0.98
2016-09-06T19:51:04.065747: step 2159, loss 0.0492417, acc 0.98
2016-09-06T19:51:04.736891: step 2160, loss 0.0399384, acc 0.98
2016-09-06T19:51:05.411579: step 2161, loss 0.105489, acc 0.92
2016-09-06T19:51:06.110868: step 2162, loss 0.0496765, acc 0.98
2016-09-06T19:51:06.800076: step 2163, loss 0.0936675, acc 0.94
2016-09-06T19:51:07.511517: step 2164, loss 0.0266837, acc 0.98
2016-09-06T19:51:08.211079: step 2165, loss 0.00722398, acc 1
2016-09-06T19:51:08.906437: step 2166, loss 0.179603, acc 0.9
2016-09-06T19:51:09.599984: step 2167, loss 0.0686969, acc 0.96
2016-09-06T19:51:10.291429: step 2168, loss 0.00983637, acc 1
2016-09-06T19:51:10.982472: step 2169, loss 0.0167248, acc 1
2016-09-06T19:51:11.655718: step 2170, loss 0.0566645, acc 0.98
2016-09-06T19:51:12.334835: step 2171, loss 0.072312, acc 0.96
2016-09-06T19:51:13.021207: step 2172, loss 0.0450847, acc 0.98
2016-09-06T19:51:13.726008: step 2173, loss 0.0623576, acc 0.96
2016-09-06T19:51:14.441758: step 2174, loss 0.0555836, acc 0.96
2016-09-06T19:51:15.130826: step 2175, loss 0.0370096, acc 0.98
2016-09-06T19:51:15.810110: step 2176, loss 0.0266116, acc 0.98
2016-09-06T19:51:16.498084: step 2177, loss 0.0665779, acc 0.96
2016-09-06T19:51:17.191374: step 2178, loss 0.055243, acc 0.98
2016-09-06T19:51:17.876065: step 2179, loss 0.0318513, acc 0.98
2016-09-06T19:51:18.575382: step 2180, loss 0.0846724, acc 0.96
2016-09-06T19:51:19.288254: step 2181, loss 0.0343911, acc 0.98
2016-09-06T19:51:19.960382: step 2182, loss 0.0412662, acc 0.96
2016-09-06T19:51:20.655575: step 2183, loss 0.0540293, acc 0.98
2016-09-06T19:51:21.340030: step 2184, loss 0.0149879, acc 1
2016-09-06T19:51:22.039832: step 2185, loss 0.0217946, acc 1
2016-09-06T19:51:22.726858: step 2186, loss 0.0282808, acc 1
2016-09-06T19:51:23.396539: step 2187, loss 0.0401364, acc 0.98
2016-09-06T19:51:24.117256: step 2188, loss 0.0338777, acc 0.98
2016-09-06T19:51:24.799425: step 2189, loss 0.181191, acc 0.94
2016-09-06T19:51:25.487433: step 2190, loss 0.0616252, acc 0.98
2016-09-06T19:51:26.180247: step 2191, loss 0.00884358, acc 1
2016-09-06T19:51:26.870132: step 2192, loss 0.0679292, acc 0.98
2016-09-06T19:51:27.564314: step 2193, loss 0.0294979, acc 0.98
2016-09-06T19:51:28.269691: step 2194, loss 0.0316859, acc 0.98
2016-09-06T19:51:28.977994: step 2195, loss 0.0610976, acc 0.96
2016-09-06T19:51:29.669321: step 2196, loss 0.135762, acc 0.96
2016-09-06T19:51:30.357663: step 2197, loss 0.0413375, acc 1
2016-09-06T19:51:31.054652: step 2198, loss 0.0173837, acc 1
2016-09-06T19:51:31.751428: step 2199, loss 0.0387016, acc 0.98
2016-09-06T19:51:32.456091: step 2200, loss 0.0144644, acc 1

Evaluation:
2016-09-06T19:51:35.579153: step 2200, loss 1.68934, acc 0.750469

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-2200

2016-09-06T19:51:37.387914: step 2201, loss 0.00395985, acc 1
2016-09-06T19:51:38.093615: step 2202, loss 0.156689, acc 0.98
2016-09-06T19:51:38.771832: step 2203, loss 0.0157069, acc 1
2016-09-06T19:51:39.480889: step 2204, loss 0.105717, acc 0.92
2016-09-06T19:51:40.188911: step 2205, loss 0.0309995, acc 0.98
2016-09-06T19:51:40.876426: step 2206, loss 0.066557, acc 0.98
2016-09-06T19:51:41.563969: step 2207, loss 0.0233129, acc 1
2016-09-06T19:51:42.243599: step 2208, loss 0.0294791, acc 0.98
2016-09-06T19:51:42.939423: step 2209, loss 0.10276, acc 0.94
2016-09-06T19:51:43.644196: step 2210, loss 0.0975189, acc 0.98
2016-09-06T19:51:44.366854: step 2211, loss 0.0976533, acc 0.96
2016-09-06T19:51:45.040064: step 2212, loss 0.144938, acc 0.98
2016-09-06T19:51:45.735200: step 2213, loss 0.177862, acc 0.92
2016-09-06T19:51:46.405192: step 2214, loss 0.0413326, acc 0.98
2016-09-06T19:51:47.087516: step 2215, loss 0.032522, acc 0.98
2016-09-06T19:51:47.770242: step 2216, loss 0.0381927, acc 1
2016-09-06T19:51:48.449372: step 2217, loss 0.0528968, acc 0.96
2016-09-06T19:51:49.131605: step 2218, loss 0.0247536, acc 0.98
2016-09-06T19:51:49.833367: step 2219, loss 0.0627192, acc 0.96
2016-09-06T19:51:50.539244: step 2220, loss 0.0336019, acc 1
2016-09-06T19:51:51.221441: step 2221, loss 0.129563, acc 0.96
2016-09-06T19:51:51.921536: step 2222, loss 0.0227659, acc 0.98
2016-09-06T19:51:52.619287: step 2223, loss 0.111792, acc 0.92
2016-09-06T19:51:53.308493: step 2224, loss 0.0518293, acc 0.98
2016-09-06T19:51:54.007499: step 2225, loss 0.0612027, acc 0.98
2016-09-06T19:51:54.736231: step 2226, loss 0.122828, acc 0.96
2016-09-06T19:51:55.450752: step 2227, loss 0.0681524, acc 0.96
2016-09-06T19:51:56.133400: step 2228, loss 0.103468, acc 0.98
2016-09-06T19:51:56.816606: step 2229, loss 0.0156352, acc 1
2016-09-06T19:51:57.492173: step 2230, loss 0.0291122, acc 0.98
2016-09-06T19:51:58.177883: step 2231, loss 0.0592054, acc 0.96
2016-09-06T19:51:58.879591: step 2232, loss 0.127563, acc 0.94
2016-09-06T19:51:59.562722: step 2233, loss 0.112958, acc 0.94
2016-09-06T19:52:00.274277: step 2234, loss 0.0361681, acc 0.98
2016-09-06T19:52:00.951548: step 2235, loss 0.00523071, acc 1
2016-09-06T19:52:01.642911: step 2236, loss 0.0833377, acc 0.98
2016-09-06T19:52:02.316379: step 2237, loss 0.0380025, acc 1
2016-09-06T19:52:03.018917: step 2238, loss 0.0459326, acc 0.98
2016-09-06T19:52:03.726925: step 2239, loss 0.0648515, acc 0.94
2016-09-06T19:52:04.395017: step 2240, loss 0.0195877, acc 1
2016-09-06T19:52:05.108115: step 2241, loss 0.0270382, acc 0.98
2016-09-06T19:52:05.791366: step 2242, loss 0.0545953, acc 0.98
2016-09-06T19:52:06.485730: step 2243, loss 0.0756829, acc 0.96
2016-09-06T19:52:07.159701: step 2244, loss 0.0283609, acc 0.98
2016-09-06T19:52:07.848310: step 2245, loss 0.129342, acc 0.92
2016-09-06T19:52:08.565536: step 2246, loss 0.0523792, acc 0.98
2016-09-06T19:52:09.252814: step 2247, loss 0.0979659, acc 0.96
2016-09-06T19:52:09.949371: step 2248, loss 0.135358, acc 0.94
2016-09-06T19:52:10.652661: step 2249, loss 0.0849958, acc 0.96
2016-09-06T19:52:11.347956: step 2250, loss 0.0239259, acc 0.98
2016-09-06T19:52:12.033003: step 2251, loss 0.021289, acc 1
2016-09-06T19:52:12.684687: step 2252, loss 0.0697022, acc 0.98
2016-09-06T19:52:13.378796: step 2253, loss 0.0547674, acc 0.96
2016-09-06T19:52:14.066775: step 2254, loss 0.0697627, acc 0.96
2016-09-06T19:52:14.749641: step 2255, loss 0.0371116, acc 1
2016-09-06T19:52:15.442600: step 2256, loss 0.0291355, acc 0.98
2016-09-06T19:52:16.141399: step 2257, loss 0.0827054, acc 0.96
2016-09-06T19:52:16.820260: step 2258, loss 0.017726, acc 1
2016-09-06T19:52:17.493130: step 2259, loss 0.0823531, acc 0.94
2016-09-06T19:52:18.244765: step 2260, loss 0.087596, acc 0.96
2016-09-06T19:52:19.092307: step 2261, loss 0.0742877, acc 0.98
2016-09-06T19:52:19.813502: step 2262, loss 0.0365266, acc 0.98
2016-09-06T19:52:20.501499: step 2263, loss 0.0399752, acc 1
2016-09-06T19:52:21.177611: step 2264, loss 0.0435039, acc 0.98
2016-09-06T19:52:21.878250: step 2265, loss 0.0147974, acc 1
2016-09-06T19:52:22.553323: step 2266, loss 0.0764447, acc 0.98
2016-09-06T19:52:23.252412: step 2267, loss 0.0223893, acc 1
2016-09-06T19:52:23.955919: step 2268, loss 0.00328874, acc 1
2016-09-06T19:52:24.637644: step 2269, loss 0.0256979, acc 1
2016-09-06T19:52:25.324915: step 2270, loss 0.103475, acc 0.96
2016-09-06T19:52:25.989411: step 2271, loss 0.0889593, acc 0.96
2016-09-06T19:52:26.694397: step 2272, loss 0.14297, acc 0.94
2016-09-06T19:52:27.384961: step 2273, loss 0.0602314, acc 0.98
2016-09-06T19:52:28.067438: step 2274, loss 0.0394595, acc 0.96
2016-09-06T19:52:28.743512: step 2275, loss 0.0467983, acc 1
2016-09-06T19:52:29.430627: step 2276, loss 0.0352463, acc 0.98
2016-09-06T19:52:30.129013: step 2277, loss 0.0626562, acc 0.98
2016-09-06T19:52:30.813467: step 2278, loss 0.0571977, acc 0.96
2016-09-06T19:52:31.510892: step 2279, loss 0.0418095, acc 0.96
2016-09-06T19:52:32.196311: step 2280, loss 0.0378391, acc 0.98
2016-09-06T19:52:32.890989: step 2281, loss 0.099132, acc 0.96
2016-09-06T19:52:33.600010: step 2282, loss 0.036761, acc 1
2016-09-06T19:52:34.311834: step 2283, loss 0.0525258, acc 0.96
2016-09-06T19:52:35.014710: step 2284, loss 0.0237506, acc 1
2016-09-06T19:52:35.700021: step 2285, loss 0.0263339, acc 1
2016-09-06T19:52:36.407527: step 2286, loss 0.0372848, acc 1
2016-09-06T19:52:37.110645: step 2287, loss 0.0102347, acc 1
2016-09-06T19:52:37.795288: step 2288, loss 0.0611962, acc 0.96
2016-09-06T19:52:38.481949: step 2289, loss 0.0319579, acc 0.98
2016-09-06T19:52:39.138096: step 2290, loss 0.020045, acc 1
2016-09-06T19:52:39.892656: step 2291, loss 0.0551408, acc 0.98
2016-09-06T19:52:40.587821: step 2292, loss 0.0606336, acc 0.98
2016-09-06T19:52:41.266428: step 2293, loss 0.0286531, acc 0.98
2016-09-06T19:52:41.956097: step 2294, loss 0.0395964, acc 0.98
2016-09-06T19:52:42.663453: step 2295, loss 0.00325039, acc 1
2016-09-06T19:52:43.376849: step 2296, loss 0.0722213, acc 0.96
2016-09-06T19:52:44.036013: step 2297, loss 0.069589, acc 0.96
2016-09-06T19:52:44.745296: step 2298, loss 0.109414, acc 0.94
2016-09-06T19:52:45.419389: step 2299, loss 0.0699666, acc 0.94
2016-09-06T19:52:46.112480: step 2300, loss 0.0102397, acc 1

Evaluation:
2016-09-06T19:52:49.286076: step 2300, loss 1.56833, acc 0.750469

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-2300

2016-09-06T19:52:50.957795: step 2301, loss 0.0450685, acc 0.98
2016-09-06T19:52:51.648198: step 2302, loss 0.118399, acc 0.98
2016-09-06T19:52:52.327648: step 2303, loss 0.0598644, acc 0.98
2016-09-06T19:52:52.975012: step 2304, loss 0.0285433, acc 0.977273
2016-09-06T19:52:53.633375: step 2305, loss 0.0779559, acc 0.96
2016-09-06T19:52:54.331085: step 2306, loss 0.0191162, acc 1
2016-09-06T19:52:55.039833: step 2307, loss 0.0234586, acc 1
2016-09-06T19:52:55.714262: step 2308, loss 0.0499522, acc 0.98
2016-09-06T19:52:56.399527: step 2309, loss 0.0996731, acc 0.94
2016-09-06T19:52:57.100453: step 2310, loss 0.041854, acc 0.98
2016-09-06T19:52:57.775983: step 2311, loss 0.0154566, acc 1
2016-09-06T19:52:58.434744: step 2312, loss 0.02419, acc 1
2016-09-06T19:52:59.124732: step 2313, loss 0.0214845, acc 1
2016-09-06T19:52:59.814895: step 2314, loss 0.091195, acc 0.96
2016-09-06T19:53:00.530735: step 2315, loss 0.00880581, acc 1
2016-09-06T19:53:01.223966: step 2316, loss 0.0324374, acc 1
2016-09-06T19:53:01.907563: step 2317, loss 0.0303919, acc 1
2016-09-06T19:53:02.597037: step 2318, loss 0.0242328, acc 1
2016-09-06T19:53:03.258385: step 2319, loss 0.0729904, acc 0.98
2016-09-06T19:53:03.970056: step 2320, loss 0.0587227, acc 0.94
2016-09-06T19:53:04.662156: step 2321, loss 0.0568256, acc 0.98
2016-09-06T19:53:05.353887: step 2322, loss 0.00673946, acc 1
2016-09-06T19:53:06.041704: step 2323, loss 0.0439559, acc 0.98
2016-09-06T19:53:06.715025: step 2324, loss 0.0166528, acc 1
2016-09-06T19:53:07.425517: step 2325, loss 0.041677, acc 0.98
2016-09-06T19:53:08.108318: step 2326, loss 0.0239179, acc 0.98
2016-09-06T19:53:08.815697: step 2327, loss 0.0156732, acc 1
2016-09-06T19:53:09.508918: step 2328, loss 0.182566, acc 0.94
2016-09-06T19:53:10.195360: step 2329, loss 0.096166, acc 0.94
2016-09-06T19:53:10.875056: step 2330, loss 0.00765645, acc 1
2016-09-06T19:53:11.565348: step 2331, loss 0.023472, acc 1
2016-09-06T19:53:12.288436: step 2332, loss 0.0340606, acc 0.98
2016-09-06T19:53:12.972537: step 2333, loss 0.0949567, acc 0.96
2016-09-06T19:53:13.653436: step 2334, loss 0.0416419, acc 0.98
2016-09-06T19:53:14.351185: step 2335, loss 0.0487279, acc 0.98
2016-09-06T19:53:15.045632: step 2336, loss 0.00443728, acc 1
2016-09-06T19:53:15.742652: step 2337, loss 0.0178767, acc 1
2016-09-06T19:53:16.414768: step 2338, loss 0.0118344, acc 1
2016-09-06T19:53:17.106677: step 2339, loss 0.00530645, acc 1
2016-09-06T19:53:17.781951: step 2340, loss 0.0744319, acc 0.98
2016-09-06T19:53:18.479033: step 2341, loss 0.00354129, acc 1
2016-09-06T19:53:19.166396: step 2342, loss 0.0371086, acc 1
2016-09-06T19:53:19.873367: step 2343, loss 0.0876213, acc 0.96
2016-09-06T19:53:20.554566: step 2344, loss 0.0134408, acc 1
2016-09-06T19:53:21.235707: step 2345, loss 0.012112, acc 1
2016-09-06T19:53:21.946238: step 2346, loss 0.0317912, acc 0.98
2016-09-06T19:53:22.623734: step 2347, loss 0.135928, acc 0.96
2016-09-06T19:53:23.293243: step 2348, loss 0.0335149, acc 1
2016-09-06T19:53:23.987030: step 2349, loss 0.13514, acc 0.96
2016-09-06T19:53:24.668009: step 2350, loss 0.0226302, acc 1
2016-09-06T19:53:25.359707: step 2351, loss 0.0406821, acc 0.98
2016-09-06T19:53:26.024240: step 2352, loss 0.0190954, acc 1
2016-09-06T19:53:26.745363: step 2353, loss 0.0658872, acc 0.98
2016-09-06T19:53:27.479195: step 2354, loss 0.0744642, acc 0.96
2016-09-06T19:53:28.147602: step 2355, loss 0.0171935, acc 0.98
2016-09-06T19:53:28.844034: step 2356, loss 0.0377556, acc 0.98
2016-09-06T19:53:29.527950: step 2357, loss 0.015967, acc 1
2016-09-06T19:53:30.214435: step 2358, loss 0.027973, acc 1
2016-09-06T19:53:30.883473: step 2359, loss 0.0288082, acc 1
2016-09-06T19:53:31.595118: step 2360, loss 0.0387115, acc 1
2016-09-06T19:53:32.281032: step 2361, loss 0.0181721, acc 1
2016-09-06T19:53:32.948708: step 2362, loss 0.0412178, acc 0.98
2016-09-06T19:53:33.637578: step 2363, loss 0.0381572, acc 0.96
2016-09-06T19:53:34.313322: step 2364, loss 0.0253044, acc 0.98
2016-09-06T19:53:35.011780: step 2365, loss 0.0374822, acc 0.98
2016-09-06T19:53:35.687703: step 2366, loss 0.0572352, acc 0.96
2016-09-06T19:53:36.403298: step 2367, loss 0.0494459, acc 0.98
2016-09-06T19:53:37.086775: step 2368, loss 0.0530614, acc 0.96
2016-09-06T19:53:37.775797: step 2369, loss 0.0248798, acc 0.98
2016-09-06T19:53:38.480192: step 2370, loss 0.0374377, acc 0.98
2016-09-06T19:53:39.178147: step 2371, loss 0.0119738, acc 1
2016-09-06T19:53:39.870831: step 2372, loss 0.0163557, acc 1
2016-09-06T19:53:40.536994: step 2373, loss 0.0243508, acc 1
2016-09-06T19:53:41.234676: step 2374, loss 0.0300064, acc 0.98
2016-09-06T19:53:41.926343: step 2375, loss 0.0467038, acc 0.98
2016-09-06T19:53:42.614878: step 2376, loss 0.22352, acc 0.92
2016-09-06T19:53:43.306235: step 2377, loss 0.0604215, acc 0.96
2016-09-06T19:53:43.975240: step 2378, loss 0.144549, acc 0.94
2016-09-06T19:53:44.675207: step 2379, loss 0.0216607, acc 1
2016-09-06T19:53:45.335791: step 2380, loss 0.0495976, acc 0.96
2016-09-06T19:53:46.042437: step 2381, loss 0.0318797, acc 1
2016-09-06T19:53:46.729281: step 2382, loss 0.0471048, acc 0.98
2016-09-06T19:53:47.407697: step 2383, loss 0.0140652, acc 1
2016-09-06T19:53:48.109029: step 2384, loss 0.0331709, acc 0.98
2016-09-06T19:53:48.805612: step 2385, loss 0.0941791, acc 0.94
2016-09-06T19:53:49.502617: step 2386, loss 0.120494, acc 0.94
2016-09-06T19:53:50.186824: step 2387, loss 0.0563875, acc 0.98
2016-09-06T19:53:50.898170: step 2388, loss 0.029645, acc 0.98
2016-09-06T19:53:51.594092: step 2389, loss 0.0840615, acc 0.96
2016-09-06T19:53:52.287963: step 2390, loss 0.0172828, acc 1
2016-09-06T19:53:52.961706: step 2391, loss 0.0468144, acc 0.98
2016-09-06T19:53:53.647258: step 2392, loss 0.0339287, acc 1
2016-09-06T19:53:54.355879: step 2393, loss 0.0287283, acc 1
2016-09-06T19:53:55.025730: step 2394, loss 0.0761935, acc 0.98
2016-09-06T19:53:55.735618: step 2395, loss 0.162005, acc 0.94
2016-09-06T19:53:56.429069: step 2396, loss 0.0613184, acc 0.98
2016-09-06T19:53:57.115893: step 2397, loss 0.0234535, acc 1
2016-09-06T19:53:57.798697: step 2398, loss 0.104041, acc 0.94
2016-09-06T19:53:58.491064: step 2399, loss 0.138333, acc 0.96
2016-09-06T19:53:59.227714: step 2400, loss 0.0253195, acc 0.98

Evaluation:
2016-09-06T19:54:02.392995: step 2400, loss 1.40732, acc 0.753283

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-2400

2016-09-06T19:54:04.038211: step 2401, loss 0.15646, acc 0.94
2016-09-06T19:54:04.724884: step 2402, loss 0.0317453, acc 1
2016-09-06T19:54:05.426418: step 2403, loss 0.0341393, acc 0.98
2016-09-06T19:54:06.103978: step 2404, loss 0.0847205, acc 0.96
2016-09-06T19:54:06.804968: step 2405, loss 0.143488, acc 0.92
2016-09-06T19:54:07.489841: step 2406, loss 0.0116467, acc 1
2016-09-06T19:54:08.164214: step 2407, loss 0.0691088, acc 0.98
2016-09-06T19:54:08.878114: step 2408, loss 0.0982313, acc 0.94
2016-09-06T19:54:09.548607: step 2409, loss 0.138978, acc 0.94
2016-09-06T19:54:10.227755: step 2410, loss 0.0570566, acc 0.98
2016-09-06T19:54:10.904993: step 2411, loss 0.0802955, acc 0.96
2016-09-06T19:54:11.585063: step 2412, loss 0.0432217, acc 0.98
2016-09-06T19:54:12.271716: step 2413, loss 0.128855, acc 0.96
2016-09-06T19:54:12.962908: step 2414, loss 0.0845682, acc 0.98
2016-09-06T19:54:13.668300: step 2415, loss 0.0266242, acc 0.98
2016-09-06T19:54:14.367019: step 2416, loss 0.0447578, acc 0.98
2016-09-06T19:54:15.045770: step 2417, loss 0.0401304, acc 0.98
2016-09-06T19:54:15.723669: step 2418, loss 0.0468438, acc 0.98
2016-09-06T19:54:16.390498: step 2419, loss 0.0483128, acc 0.98
2016-09-06T19:54:17.084546: step 2420, loss 0.0274292, acc 1
2016-09-06T19:54:17.760732: step 2421, loss 0.0326372, acc 0.98
2016-09-06T19:54:18.456086: step 2422, loss 0.130463, acc 0.98
2016-09-06T19:54:19.140127: step 2423, loss 0.0681348, acc 0.98
2016-09-06T19:54:19.831551: step 2424, loss 0.0455231, acc 0.98
2016-09-06T19:54:20.521158: step 2425, loss 0.0394371, acc 0.98
2016-09-06T19:54:21.235510: step 2426, loss 0.080442, acc 0.96
2016-09-06T19:54:21.918076: step 2427, loss 0.0333261, acc 0.98
2016-09-06T19:54:22.583100: step 2428, loss 0.0475881, acc 0.98
2016-09-06T19:54:23.276572: step 2429, loss 0.0471746, acc 0.98
2016-09-06T19:54:23.957988: step 2430, loss 0.0528691, acc 0.98
2016-09-06T19:54:24.645421: step 2431, loss 0.0200254, acc 1
2016-09-06T19:54:25.328832: step 2432, loss 0.077137, acc 0.96
2016-09-06T19:54:26.011309: step 2433, loss 0.0266715, acc 1
2016-09-06T19:54:26.696818: step 2434, loss 0.0352259, acc 0.98
2016-09-06T19:54:27.376231: step 2435, loss 0.158163, acc 0.96
2016-09-06T19:54:28.117287: step 2436, loss 0.0575979, acc 0.96
2016-09-06T19:54:28.828856: step 2437, loss 0.0380591, acc 0.98
2016-09-06T19:54:29.522673: step 2438, loss 0.0110922, acc 1
2016-09-06T19:54:30.202204: step 2439, loss 0.0507882, acc 0.96
2016-09-06T19:54:30.890408: step 2440, loss 0.0290499, acc 1
2016-09-06T19:54:31.572073: step 2441, loss 0.0100846, acc 1
2016-09-06T19:54:32.232419: step 2442, loss 0.0181881, acc 0.98
2016-09-06T19:54:32.964073: step 2443, loss 0.0228011, acc 0.98
2016-09-06T19:54:33.666584: step 2444, loss 0.0371035, acc 0.98
2016-09-06T19:54:34.367047: step 2445, loss 0.0502749, acc 0.98
2016-09-06T19:54:35.051053: step 2446, loss 0.020203, acc 1
2016-09-06T19:54:35.738869: step 2447, loss 0.00807685, acc 1
2016-09-06T19:54:36.432486: step 2448, loss 0.0548281, acc 0.98
2016-09-06T19:54:37.095544: step 2449, loss 0.0329661, acc 0.98
2016-09-06T19:54:37.792682: step 2450, loss 0.0129398, acc 1
2016-09-06T19:54:38.473424: step 2451, loss 0.0403879, acc 0.98
2016-09-06T19:54:39.151808: step 2452, loss 0.0509602, acc 0.98
2016-09-06T19:54:39.825805: step 2453, loss 0.0616724, acc 0.98
2016-09-06T19:54:40.508998: step 2454, loss 0.0973879, acc 0.94
2016-09-06T19:54:41.188238: step 2455, loss 0.0171539, acc 1
2016-09-06T19:54:41.849608: step 2456, loss 0.200114, acc 0.92
2016-09-06T19:54:42.544657: step 2457, loss 0.0196143, acc 1
2016-09-06T19:54:43.227808: step 2458, loss 0.0399093, acc 0.98
2016-09-06T19:54:43.910519: step 2459, loss 0.10805, acc 0.96
2016-09-06T19:54:44.600928: step 2460, loss 0.0708349, acc 0.98
2016-09-06T19:54:45.280590: step 2461, loss 0.0349481, acc 0.98
2016-09-06T19:54:45.949352: step 2462, loss 0.0548292, acc 0.96
2016-09-06T19:54:46.640476: step 2463, loss 0.116946, acc 0.92
2016-09-06T19:54:47.348259: step 2464, loss 0.127255, acc 0.94
2016-09-06T19:54:48.046047: step 2465, loss 0.00256337, acc 1
2016-09-06T19:54:48.724205: step 2466, loss 0.0253854, acc 0.98
2016-09-06T19:54:49.407768: step 2467, loss 0.00383004, acc 1
2016-09-06T19:54:50.096363: step 2468, loss 0.0473209, acc 0.98
2016-09-06T19:54:50.795066: step 2469, loss 0.14674, acc 0.94
2016-09-06T19:54:51.468518: step 2470, loss 0.0485146, acc 0.96
2016-09-06T19:54:52.189313: step 2471, loss 0.152505, acc 0.92
2016-09-06T19:54:52.895872: step 2472, loss 0.109599, acc 0.98
2016-09-06T19:54:53.589854: step 2473, loss 0.0182923, acc 1
2016-09-06T19:54:54.265947: step 2474, loss 0.0518598, acc 0.98
2016-09-06T19:54:54.950933: step 2475, loss 0.0220123, acc 1
2016-09-06T19:54:55.636072: step 2476, loss 0.0639568, acc 0.98
2016-09-06T19:54:56.317139: step 2477, loss 0.060644, acc 0.94
2016-09-06T19:54:57.027641: step 2478, loss 0.206022, acc 0.94
2016-09-06T19:54:57.713633: step 2479, loss 0.13938, acc 0.94
2016-09-06T19:54:58.404830: step 2480, loss 0.0851767, acc 0.94
2016-09-06T19:54:59.121500: step 2481, loss 0.0466712, acc 0.98
2016-09-06T19:54:59.817595: step 2482, loss 0.05977, acc 0.96
2016-09-06T19:55:00.558847: step 2483, loss 0.0391744, acc 0.98
2016-09-06T19:55:01.246141: step 2484, loss 0.0429775, acc 0.98
2016-09-06T19:55:01.929898: step 2485, loss 0.0330367, acc 1
2016-09-06T19:55:02.623030: step 2486, loss 0.034013, acc 1
2016-09-06T19:55:03.317932: step 2487, loss 0.065518, acc 0.98
2016-09-06T19:55:04.036298: step 2488, loss 0.126147, acc 0.92
2016-09-06T19:55:04.710194: step 2489, loss 0.0278943, acc 1
2016-09-06T19:55:05.394091: step 2490, loss 0.0375119, acc 0.98
2016-09-06T19:55:06.061154: step 2491, loss 0.0108832, acc 1
2016-09-06T19:55:06.740596: step 2492, loss 0.106372, acc 0.92
2016-09-06T19:55:07.423186: step 2493, loss 0.018769, acc 0.98
2016-09-06T19:55:08.121356: step 2494, loss 0.12192, acc 0.98
2016-09-06T19:55:08.849728: step 2495, loss 0.27693, acc 0.9
2016-09-06T19:55:09.499304: step 2496, loss 0.0116775, acc 1
2016-09-06T19:55:10.190544: step 2497, loss 0.0205454, acc 1
2016-09-06T19:55:10.868850: step 2498, loss 0.032801, acc 0.98
2016-09-06T19:55:11.557055: step 2499, loss 0.045484, acc 0.96
2016-09-06T19:55:12.244979: step 2500, loss 0.0444192, acc 1

Evaluation:
2016-09-06T19:55:15.357196: step 2500, loss 1.54558, acc 0.736398

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-2500

2016-09-06T19:55:16.977377: step 2501, loss 0.0569007, acc 0.98
2016-09-06T19:55:17.684890: step 2502, loss 0.080556, acc 0.98
2016-09-06T19:55:18.370503: step 2503, loss 0.0155074, acc 1
2016-09-06T19:55:19.050881: step 2504, loss 0.0382436, acc 0.98
2016-09-06T19:55:19.742793: step 2505, loss 0.0205011, acc 1
2016-09-06T19:55:20.419149: step 2506, loss 0.00659966, acc 1
2016-09-06T19:55:21.334088: step 2507, loss 0.0310235, acc 0.98
2016-09-06T19:55:22.027043: step 2508, loss 0.0160943, acc 1
2016-09-06T19:55:22.714940: step 2509, loss 0.0630969, acc 0.96
2016-09-06T19:55:23.421040: step 2510, loss 0.0775657, acc 0.94
2016-09-06T19:55:24.115099: step 2511, loss 0.0337898, acc 1
2016-09-06T19:55:24.813152: step 2512, loss 0.0130009, acc 1
2016-09-06T19:55:25.487589: step 2513, loss 0.066234, acc 0.96
2016-09-06T19:55:26.180445: step 2514, loss 0.0292296, acc 1
2016-09-06T19:55:26.881641: step 2515, loss 0.0313874, acc 0.98
2016-09-06T19:55:27.560519: step 2516, loss 0.0651097, acc 0.94
2016-09-06T19:55:28.266813: step 2517, loss 0.0517771, acc 0.96
2016-09-06T19:55:28.935816: step 2518, loss 0.00135638, acc 1
2016-09-06T19:55:29.619679: step 2519, loss 0.0441245, acc 0.98
2016-09-06T19:55:30.308606: step 2520, loss 0.0392754, acc 0.98
2016-09-06T19:55:31.007601: step 2521, loss 0.0805059, acc 0.98
2016-09-06T19:55:31.696766: step 2522, loss 0.0500561, acc 0.96
2016-09-06T19:55:32.358964: step 2523, loss 0.0194696, acc 1
2016-09-06T19:55:33.045077: step 2524, loss 0.0187923, acc 1
2016-09-06T19:55:33.712293: step 2525, loss 0.0228262, acc 1
2016-09-06T19:55:34.411146: step 2526, loss 0.013268, acc 1
2016-09-06T19:55:35.089116: step 2527, loss 0.000543031, acc 1
2016-09-06T19:55:35.797578: step 2528, loss 0.0206883, acc 1
2016-09-06T19:55:36.476860: step 2529, loss 0.0148364, acc 1
2016-09-06T19:55:37.154130: step 2530, loss 0.027991, acc 0.98
2016-09-06T19:55:37.855124: step 2531, loss 0.167317, acc 0.94
2016-09-06T19:55:38.528444: step 2532, loss 0.0549301, acc 0.96
2016-09-06T19:55:39.211095: step 2533, loss 0.000460485, acc 1
2016-09-06T19:55:39.912886: step 2534, loss 0.0494145, acc 0.98
2016-09-06T19:55:40.603333: step 2535, loss 0.0516402, acc 0.96
2016-09-06T19:55:41.296184: step 2536, loss 0.0495951, acc 0.96
2016-09-06T19:55:41.979513: step 2537, loss 0.0269372, acc 1
2016-09-06T19:55:42.676936: step 2538, loss 0.0190421, acc 1
2016-09-06T19:55:43.345370: step 2539, loss 0.0911325, acc 0.94
2016-09-06T19:55:44.018783: step 2540, loss 0.0241091, acc 1
2016-09-06T19:55:44.714775: step 2541, loss 0.0210335, acc 1
2016-09-06T19:55:45.393245: step 2542, loss 0.0951066, acc 0.96
2016-09-06T19:55:46.092210: step 2543, loss 0.0124833, acc 1
2016-09-06T19:55:46.807871: step 2544, loss 0.0849863, acc 0.96
2016-09-06T19:55:47.508785: step 2545, loss 0.129939, acc 0.94
2016-09-06T19:55:48.172258: step 2546, loss 0.0781304, acc 0.98
2016-09-06T19:55:48.841546: step 2547, loss 0.054066, acc 0.98
2016-09-06T19:55:49.543014: step 2548, loss 0.044344, acc 0.98
2016-09-06T19:55:50.234315: step 2549, loss 0.00830666, acc 1
2016-09-06T19:55:50.918097: step 2550, loss 0.00153709, acc 1
2016-09-06T19:55:51.612843: step 2551, loss 0.0367584, acc 1
2016-09-06T19:55:52.323554: step 2552, loss 0.117593, acc 0.96
2016-09-06T19:55:53.048690: step 2553, loss 0.023984, acc 1
2016-09-06T19:55:53.750586: step 2554, loss 0.0208504, acc 1
2016-09-06T19:55:54.450420: step 2555, loss 0.0244085, acc 1
2016-09-06T19:55:55.136422: step 2556, loss 0.00797426, acc 1
2016-09-06T19:55:55.864214: step 2557, loss 0.00666656, acc 1
2016-09-06T19:55:56.557770: step 2558, loss 0.0378275, acc 0.98
2016-09-06T19:55:57.244169: step 2559, loss 0.0537474, acc 0.98
2016-09-06T19:55:57.917559: step 2560, loss 0.0574027, acc 0.96
2016-09-06T19:55:58.599706: step 2561, loss 0.0327368, acc 0.96
2016-09-06T19:55:59.302942: step 2562, loss 0.031288, acc 0.98
2016-09-06T19:55:59.980256: step 2563, loss 0.0401077, acc 1
2016-09-06T19:56:00.716786: step 2564, loss 0.0155397, acc 1
2016-09-06T19:56:01.380522: step 2565, loss 0.089109, acc 0.96
2016-09-06T19:56:02.082835: step 2566, loss 0.015404, acc 1
2016-09-06T19:56:02.773541: step 2567, loss 0.0792906, acc 0.98
2016-09-06T19:56:03.465110: step 2568, loss 0.0620891, acc 0.98
2016-09-06T19:56:04.143486: step 2569, loss 0.00674874, acc 1
2016-09-06T19:56:04.831057: step 2570, loss 0.064698, acc 0.94
2016-09-06T19:56:05.542265: step 2571, loss 0.0314813, acc 1
2016-09-06T19:56:06.240926: step 2572, loss 0.0263466, acc 1
2016-09-06T19:56:06.936427: step 2573, loss 0.0668567, acc 0.96
2016-09-06T19:56:07.616809: step 2574, loss 0.0211023, acc 0.98
2016-09-06T19:56:08.301674: step 2575, loss 0.0759517, acc 0.96
2016-09-06T19:56:08.977972: step 2576, loss 0.0664353, acc 0.98
2016-09-06T19:56:09.643786: step 2577, loss 0.0656368, acc 0.94
2016-09-06T19:56:10.369282: step 2578, loss 0.0272153, acc 1
2016-09-06T19:56:11.053944: step 2579, loss 0.0422373, acc 0.98
2016-09-06T19:56:11.747463: step 2580, loss 0.0266672, acc 1
2016-09-06T19:56:12.428194: step 2581, loss 0.0760222, acc 0.96
2016-09-06T19:56:13.110959: step 2582, loss 0.0714971, acc 0.98
2016-09-06T19:56:13.829419: step 2583, loss 0.024307, acc 1
2016-09-06T19:56:14.492800: step 2584, loss 0.0259911, acc 1
2016-09-06T19:56:15.191140: step 2585, loss 0.126471, acc 0.92
2016-09-06T19:56:15.876278: step 2586, loss 0.0324164, acc 0.98
2016-09-06T19:56:16.578735: step 2587, loss 0.016594, acc 1
2016-09-06T19:56:17.273203: step 2588, loss 0.0924002, acc 0.96
2016-09-06T19:56:17.949655: step 2589, loss 0.0829308, acc 0.96
2016-09-06T19:56:18.645965: step 2590, loss 0.0193389, acc 1
2016-09-06T19:56:19.312626: step 2591, loss 0.0754972, acc 0.98
2016-09-06T19:56:20.024208: step 2592, loss 0.141899, acc 0.92
2016-09-06T19:56:20.705456: step 2593, loss 0.020481, acc 1
2016-09-06T19:56:21.403720: step 2594, loss 0.00988705, acc 1
2016-09-06T19:56:22.083296: step 2595, loss 0.0996171, acc 0.92
2016-09-06T19:56:22.781517: step 2596, loss 0.152707, acc 0.96
2016-09-06T19:56:23.498763: step 2597, loss 0.0376404, acc 0.98
2016-09-06T19:56:24.162178: step 2598, loss 0.0636216, acc 0.98
2016-09-06T19:56:24.856098: step 2599, loss 0.050537, acc 0.94
2016-09-06T19:56:25.563274: step 2600, loss 0.0824601, acc 0.94

Evaluation:
2016-09-06T19:56:28.702413: step 2600, loss 1.77374, acc 0.746717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-2600

2016-09-06T19:56:30.446146: step 2601, loss 0.0228255, acc 1
2016-09-06T19:56:31.147842: step 2602, loss 0.0372471, acc 0.98
2016-09-06T19:56:31.843834: step 2603, loss 0.0208935, acc 0.98
2016-09-06T19:56:32.519791: step 2604, loss 0.0779722, acc 0.96
2016-09-06T19:56:33.220354: step 2605, loss 0.0868542, acc 0.94
2016-09-06T19:56:33.935422: step 2606, loss 0.0256435, acc 1
2016-09-06T19:56:34.621927: step 2607, loss 0.0311218, acc 0.98
2016-09-06T19:56:35.306932: step 2608, loss 0.0400891, acc 0.98
2016-09-06T19:56:36.011640: step 2609, loss 0.0477306, acc 0.98
2016-09-06T19:56:36.716352: step 2610, loss 0.0284549, acc 1
2016-09-06T19:56:37.381242: step 2611, loss 0.0180444, acc 1
2016-09-06T19:56:38.081017: step 2612, loss 0.0439113, acc 0.96
2016-09-06T19:56:38.767186: step 2613, loss 0.00949023, acc 1
2016-09-06T19:56:39.469296: step 2614, loss 0.0598958, acc 0.96
2016-09-06T19:56:40.158282: step 2615, loss 0.0357664, acc 1
2016-09-06T19:56:40.837651: step 2616, loss 0.103565, acc 0.96
2016-09-06T19:56:41.535085: step 2617, loss 0.0280932, acc 1
2016-09-06T19:56:42.216114: step 2618, loss 0.059555, acc 0.98
2016-09-06T19:56:42.921909: step 2619, loss 0.000536404, acc 1
2016-09-06T19:56:43.615019: step 2620, loss 0.0966034, acc 0.94
2016-09-06T19:56:44.300639: step 2621, loss 0.0190117, acc 0.98
2016-09-06T19:56:45.016682: step 2622, loss 0.0967828, acc 0.94
2016-09-06T19:56:45.670865: step 2623, loss 0.0183442, acc 1
2016-09-06T19:56:46.370716: step 2624, loss 0.0196139, acc 1
2016-09-06T19:56:47.050476: step 2625, loss 0.0622693, acc 0.96
2016-09-06T19:56:47.736651: step 2626, loss 0.0521119, acc 0.98
2016-09-06T19:56:48.410561: step 2627, loss 0.0754049, acc 0.98
2016-09-06T19:56:49.086469: step 2628, loss 0.0377296, acc 0.98
2016-09-06T19:56:49.768465: step 2629, loss 0.0610568, acc 0.98
2016-09-06T19:56:50.463530: step 2630, loss 0.019862, acc 0.98
2016-09-06T19:56:51.161256: step 2631, loss 0.0284428, acc 0.98
2016-09-06T19:56:51.841928: step 2632, loss 0.045754, acc 0.96
2016-09-06T19:56:52.512963: step 2633, loss 0.0945573, acc 0.98
2016-09-06T19:56:53.216378: step 2634, loss 0.0529126, acc 0.98
2016-09-06T19:56:53.902800: step 2635, loss 0.0505407, acc 0.98
2016-09-06T19:56:54.581214: step 2636, loss 0.00496393, acc 1
2016-09-06T19:56:55.257619: step 2637, loss 0.021367, acc 1
2016-09-06T19:56:55.958528: step 2638, loss 0.0355732, acc 1
2016-09-06T19:56:56.636694: step 2639, loss 0.0590048, acc 0.98
2016-09-06T19:56:57.308120: step 2640, loss 0.0809979, acc 0.94
2016-09-06T19:56:57.978878: step 2641, loss 0.0426636, acc 0.98
2016-09-06T19:56:58.655135: step 2642, loss 0.0345838, acc 0.98
2016-09-06T19:56:59.373080: step 2643, loss 0.0192938, acc 1
2016-09-06T19:57:00.057208: step 2644, loss 0.0498398, acc 0.98
2016-09-06T19:57:00.759815: step 2645, loss 0.0492164, acc 0.96
2016-09-06T19:57:01.434102: step 2646, loss 0.0150299, acc 1
2016-09-06T19:57:02.146397: step 2647, loss 0.0934082, acc 0.96
2016-09-06T19:57:02.810476: step 2648, loss 0.00913569, acc 1
2016-09-06T19:57:03.509980: step 2649, loss 0.0364809, acc 0.96
2016-09-06T19:57:04.189327: step 2650, loss 0.240034, acc 0.92
2016-09-06T19:57:04.861869: step 2651, loss 0.0228756, acc 1
2016-09-06T19:57:05.560268: step 2652, loss 0.00204371, acc 1
2016-09-06T19:57:06.232319: step 2653, loss 0.0123738, acc 1
2016-09-06T19:57:06.936709: step 2654, loss 0.0559492, acc 0.96
2016-09-06T19:57:07.631201: step 2655, loss 0.0226068, acc 0.98
2016-09-06T19:57:08.320074: step 2656, loss 0.165828, acc 0.94
2016-09-06T19:57:09.007884: step 2657, loss 0.00244122, acc 1
2016-09-06T19:57:09.725863: step 2658, loss 0.0135237, acc 1
2016-09-06T19:57:10.430509: step 2659, loss 0.00935766, acc 1
2016-09-06T19:57:11.101722: step 2660, loss 0.0304922, acc 0.98
2016-09-06T19:57:11.814328: step 2661, loss 0.0517138, acc 0.98
2016-09-06T19:57:12.503025: step 2662, loss 0.0492472, acc 0.98
2016-09-06T19:57:13.197434: step 2663, loss 0.00471192, acc 1
2016-09-06T19:57:13.898948: step 2664, loss 0.0587026, acc 0.96
2016-09-06T19:57:14.586995: step 2665, loss 0.0994899, acc 0.96
2016-09-06T19:57:15.323975: step 2666, loss 0.0246193, acc 0.98
2016-09-06T19:57:16.004631: step 2667, loss 0.0206708, acc 1
2016-09-06T19:57:16.692234: step 2668, loss 0.141321, acc 0.94
2016-09-06T19:57:17.383271: step 2669, loss 0.0757248, acc 0.98
2016-09-06T19:57:18.063263: step 2670, loss 0.184258, acc 0.94
2016-09-06T19:57:18.789695: step 2671, loss 0.0688887, acc 0.98
2016-09-06T19:57:19.451056: step 2672, loss 0.0221903, acc 0.98
2016-09-06T19:57:20.154038: step 2673, loss 0.0227485, acc 0.98
2016-09-06T19:57:20.831914: step 2674, loss 0.0622402, acc 0.96
2016-09-06T19:57:21.519002: step 2675, loss 0.0131376, acc 1
2016-09-06T19:57:22.212014: step 2676, loss 0.0334708, acc 1
2016-09-06T19:57:22.907776: step 2677, loss 0.0421188, acc 0.98
2016-09-06T19:57:23.600734: step 2678, loss 0.0354074, acc 0.98
2016-09-06T19:57:24.287350: step 2679, loss 0.039057, acc 0.98
2016-09-06T19:57:24.969240: step 2680, loss 0.044499, acc 0.98
2016-09-06T19:57:25.641742: step 2681, loss 0.0208287, acc 1
2016-09-06T19:57:26.333286: step 2682, loss 0.0696866, acc 0.96
2016-09-06T19:57:27.028814: step 2683, loss 0.0297789, acc 0.98
2016-09-06T19:57:27.714432: step 2684, loss 0.00940655, acc 1
2016-09-06T19:57:28.407282: step 2685, loss 0.103172, acc 0.92
2016-09-06T19:57:29.077766: step 2686, loss 0.0385403, acc 1
2016-09-06T19:57:29.785511: step 2687, loss 0.178031, acc 0.98
2016-09-06T19:57:30.407174: step 2688, loss 0.0551452, acc 0.977273
2016-09-06T19:57:31.086365: step 2689, loss 0.0932868, acc 0.98
2016-09-06T19:57:31.768900: step 2690, loss 0.0707588, acc 0.96
2016-09-06T19:57:32.450191: step 2691, loss 0.0998352, acc 0.92
2016-09-06T19:57:33.127263: step 2692, loss 0.0449746, acc 0.96
2016-09-06T19:57:33.810522: step 2693, loss 0.0128011, acc 1
2016-09-06T19:57:34.518674: step 2694, loss 0.140591, acc 0.96
2016-09-06T19:57:35.199748: step 2695, loss 0.0161224, acc 0.98
2016-09-06T19:57:35.870052: step 2696, loss 0.0261207, acc 0.98
2016-09-06T19:57:36.549956: step 2697, loss 0.0202288, acc 0.98
2016-09-06T19:57:37.239787: step 2698, loss 0.036817, acc 0.98
2016-09-06T19:57:37.921047: step 2699, loss 0.00492872, acc 1
2016-09-06T19:57:38.598761: step 2700, loss 0.0289405, acc 1

Evaluation:
2016-09-06T19:57:41.732625: step 2700, loss 1.62091, acc 0.750469

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-2700

2016-09-06T19:57:43.414432: step 2701, loss 0.0403265, acc 0.96
2016-09-06T19:57:44.106264: step 2702, loss 0.0267699, acc 0.98
2016-09-06T19:57:44.761823: step 2703, loss 0.04091, acc 0.98
2016-09-06T19:57:45.461849: step 2704, loss 0.0397559, acc 0.96
2016-09-06T19:57:46.144983: step 2705, loss 0.0882537, acc 0.94
2016-09-06T19:57:46.846630: step 2706, loss 0.0203359, acc 1
2016-09-06T19:57:47.547131: step 2707, loss 0.0419431, acc 0.98
2016-09-06T19:57:48.225961: step 2708, loss 0.0203361, acc 1
2016-09-06T19:57:48.939872: step 2709, loss 0.0395984, acc 1
2016-09-06T19:57:49.619250: step 2710, loss 0.0122639, acc 1
2016-09-06T19:57:50.333154: step 2711, loss 0.050164, acc 0.98
2016-09-06T19:57:51.029870: step 2712, loss 0.0775784, acc 0.98
2016-09-06T19:57:51.725799: step 2713, loss 0.0139164, acc 1
2016-09-06T19:57:52.406017: step 2714, loss 0.0270752, acc 1
2016-09-06T19:57:53.081854: step 2715, loss 0.00999938, acc 1
2016-09-06T19:57:53.790477: step 2716, loss 0.00599007, acc 1
2016-09-06T19:57:54.481069: step 2717, loss 0.0592645, acc 0.94
2016-09-06T19:57:55.154510: step 2718, loss 0.0157075, acc 1
2016-09-06T19:57:55.827081: step 2719, loss 0.00177935, acc 1
2016-09-06T19:57:56.521389: step 2720, loss 0.037304, acc 0.98
2016-09-06T19:57:57.202325: step 2721, loss 0.0681287, acc 0.98
2016-09-06T19:57:57.882105: step 2722, loss 0.0193699, acc 1
2016-09-06T19:57:58.581674: step 2723, loss 0.0786348, acc 0.96
2016-09-06T19:57:59.258320: step 2724, loss 0.0248052, acc 1
2016-09-06T19:57:59.961194: step 2725, loss 0.0463016, acc 0.98
2016-09-06T19:58:00.681139: step 2726, loss 0.0336624, acc 0.98
2016-09-06T19:58:01.391743: step 2727, loss 0.056861, acc 0.94
2016-09-06T19:58:02.087767: step 2728, loss 0.0217698, acc 1
2016-09-06T19:58:02.756287: step 2729, loss 0.0755052, acc 0.98
2016-09-06T19:58:03.458557: step 2730, loss 0.0505942, acc 0.96
2016-09-06T19:58:04.133276: step 2731, loss 0.0584446, acc 0.96
2016-09-06T19:58:04.814404: step 2732, loss 0.0321182, acc 0.98
2016-09-06T19:58:05.544420: step 2733, loss 0.0389915, acc 0.98
2016-09-06T19:58:06.221010: step 2734, loss 0.0455147, acc 0.96
2016-09-06T19:58:06.939098: step 2735, loss 0.0287507, acc 0.98
2016-09-06T19:58:07.613894: step 2736, loss 0.0304291, acc 1
2016-09-06T19:58:08.316538: step 2737, loss 0.0302385, acc 0.98
2016-09-06T19:58:08.988501: step 2738, loss 0.0371271, acc 0.98
2016-09-06T19:58:09.667312: step 2739, loss 0.0458707, acc 0.96
2016-09-06T19:58:10.365487: step 2740, loss 0.0041372, acc 1
2016-09-06T19:58:11.051685: step 2741, loss 0.000812485, acc 1
2016-09-06T19:58:11.770539: step 2742, loss 0.126822, acc 0.96
2016-09-06T19:58:12.456741: step 2743, loss 0.103794, acc 0.96
2016-09-06T19:58:13.147210: step 2744, loss 0.00959536, acc 1
2016-09-06T19:58:13.821627: step 2745, loss 0.00182679, acc 1
2016-09-06T19:58:14.516679: step 2746, loss 0.0174973, acc 0.98
2016-09-06T19:58:15.196379: step 2747, loss 0.00627827, acc 1
2016-09-06T19:58:15.876794: step 2748, loss 0.081919, acc 0.98
2016-09-06T19:58:16.581588: step 2749, loss 0.014535, acc 1
2016-09-06T19:58:17.257435: step 2750, loss 0.0107374, acc 1
2016-09-06T19:58:17.952277: step 2751, loss 0.0916848, acc 0.96
2016-09-06T19:58:18.638476: step 2752, loss 0.0261789, acc 0.98
2016-09-06T19:58:19.335059: step 2753, loss 0.0353324, acc 0.98
2016-09-06T19:58:20.010382: step 2754, loss 0.0937827, acc 0.98
2016-09-06T19:58:20.698964: step 2755, loss 0.167663, acc 0.96
2016-09-06T19:58:21.399372: step 2756, loss 0.0352463, acc 1
2016-09-06T19:58:22.090017: step 2757, loss 0.0575515, acc 0.98
2016-09-06T19:58:22.807656: step 2758, loss 0.0192244, acc 1
2016-09-06T19:58:23.487384: step 2759, loss 0.0493951, acc 0.98
2016-09-06T19:58:24.190755: step 2760, loss 0.0434643, acc 0.96
2016-09-06T19:58:24.876490: step 2761, loss 0.0226246, acc 1
2016-09-06T19:58:25.534693: step 2762, loss 0.0201085, acc 1
2016-09-06T19:58:26.242669: step 2763, loss 0.0516359, acc 0.96
2016-09-06T19:58:26.917937: step 2764, loss 0.0177638, acc 1
2016-09-06T19:58:27.597419: step 2765, loss 0.0379069, acc 0.98
2016-09-06T19:58:28.281954: step 2766, loss 0.0503972, acc 0.96
2016-09-06T19:58:28.975285: step 2767, loss 0.0192826, acc 0.98
2016-09-06T19:58:29.660930: step 2768, loss 0.038811, acc 0.98
2016-09-06T19:58:30.343860: step 2769, loss 0.0138222, acc 1
2016-09-06T19:58:31.044354: step 2770, loss 0.0124732, acc 1
2016-09-06T19:58:31.712271: step 2771, loss 0.0935681, acc 0.96
2016-09-06T19:58:32.382146: step 2772, loss 0.0497528, acc 1
2016-09-06T19:58:33.056842: step 2773, loss 0.00438717, acc 1
2016-09-06T19:58:33.751781: step 2774, loss 0.0522339, acc 0.96
2016-09-06T19:58:34.448532: step 2775, loss 0.0155103, acc 1
2016-09-06T19:58:35.139208: step 2776, loss 0.0111821, acc 1
2016-09-06T19:58:35.856656: step 2777, loss 0.0449245, acc 0.98
2016-09-06T19:58:36.539596: step 2778, loss 0.0570271, acc 0.98
2016-09-06T19:58:37.233214: step 2779, loss 0.0370391, acc 0.98
2016-09-06T19:58:37.943253: step 2780, loss 0.0281286, acc 0.98
2016-09-06T19:58:38.617123: step 2781, loss 0.00656194, acc 1
2016-09-06T19:58:39.315636: step 2782, loss 0.0557024, acc 0.96
2016-09-06T19:58:39.996853: step 2783, loss 0.0584841, acc 0.98
2016-09-06T19:58:40.715870: step 2784, loss 0.0426418, acc 0.98
2016-09-06T19:58:41.422595: step 2785, loss 0.0788383, acc 0.94
2016-09-06T19:58:42.096013: step 2786, loss 0.156412, acc 0.96
2016-09-06T19:58:42.774229: step 2787, loss 0.022561, acc 0.98
2016-09-06T19:58:43.471568: step 2788, loss 0.074774, acc 0.96
2016-09-06T19:58:44.166033: step 2789, loss 0.0106584, acc 1
2016-09-06T19:58:44.843618: step 2790, loss 0.0303022, acc 0.98
2016-09-06T19:58:45.552355: step 2791, loss 0.15879, acc 0.94
2016-09-06T19:58:46.248956: step 2792, loss 0.0117665, acc 1
2016-09-06T19:58:46.934781: step 2793, loss 0.00765965, acc 1
2016-09-06T19:58:47.607217: step 2794, loss 0.148657, acc 0.94
2016-09-06T19:58:48.302436: step 2795, loss 0.00476998, acc 1
2016-09-06T19:58:49.017584: step 2796, loss 0.0541027, acc 0.98
2016-09-06T19:58:49.698498: step 2797, loss 0.0713195, acc 0.96
2016-09-06T19:58:50.435023: step 2798, loss 0.129732, acc 0.96
2016-09-06T19:58:51.113687: step 2799, loss 0.0941541, acc 0.98
2016-09-06T19:58:51.801340: step 2800, loss 0.0336368, acc 0.98

Evaluation:
2016-09-06T19:58:54.963568: step 2800, loss 1.60196, acc 0.762664

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-2800

2016-09-06T19:58:56.573851: step 2801, loss 0.01119, acc 1
2016-09-06T19:58:57.260966: step 2802, loss 0.0274908, acc 0.98
2016-09-06T19:58:57.937477: step 2803, loss 0.0250752, acc 1
2016-09-06T19:58:58.612692: step 2804, loss 0.0549709, acc 0.98
2016-09-06T19:58:59.278414: step 2805, loss 0.00909383, acc 1
2016-09-06T19:58:59.987167: step 2806, loss 0.0180525, acc 1
2016-09-06T19:59:00.732693: step 2807, loss 0.0248991, acc 1
2016-09-06T19:59:01.425663: step 2808, loss 0.0121004, acc 1
2016-09-06T19:59:02.116543: step 2809, loss 0.0747178, acc 0.94
2016-09-06T19:59:02.822354: step 2810, loss 0.0817005, acc 0.96
2016-09-06T19:59:03.530135: step 2811, loss 0.0285331, acc 0.98
2016-09-06T19:59:04.210488: step 2812, loss 0.0447557, acc 0.96
2016-09-06T19:59:04.904796: step 2813, loss 0.0743189, acc 0.96
2016-09-06T19:59:05.606408: step 2814, loss 0.0256673, acc 1
2016-09-06T19:59:06.276213: step 2815, loss 0.042778, acc 0.98
2016-09-06T19:59:06.964428: step 2816, loss 0.191457, acc 0.94
2016-09-06T19:59:07.625342: step 2817, loss 0.0234693, acc 0.98
2016-09-06T19:59:08.341196: step 2818, loss 0.0194773, acc 1
2016-09-06T19:59:09.029841: step 2819, loss 0.0699441, acc 1
2016-09-06T19:59:09.716936: step 2820, loss 0.0372314, acc 0.98
2016-09-06T19:59:10.414931: step 2821, loss 0.0989851, acc 0.94
2016-09-06T19:59:11.104981: step 2822, loss 0.0392687, acc 0.96
2016-09-06T19:59:11.810859: step 2823, loss 0.0210444, acc 0.98
2016-09-06T19:59:12.482694: step 2824, loss 0.0537319, acc 0.98
2016-09-06T19:59:13.190024: step 2825, loss 0.0342181, acc 0.98
2016-09-06T19:59:13.874824: step 2826, loss 0.0240626, acc 0.98
2016-09-06T19:59:14.552742: step 2827, loss 0.0608748, acc 0.98
2016-09-06T19:59:15.233361: step 2828, loss 0.0138784, acc 1
2016-09-06T19:59:15.926985: step 2829, loss 0.0231796, acc 1
2016-09-06T19:59:16.617010: step 2830, loss 0.0513123, acc 0.98
2016-09-06T19:59:17.276851: step 2831, loss 0.0145964, acc 1
2016-09-06T19:59:17.975195: step 2832, loss 0.00204213, acc 1
2016-09-06T19:59:18.656422: step 2833, loss 0.077651, acc 0.96
2016-09-06T19:59:19.342537: step 2834, loss 0.0235525, acc 0.98
2016-09-06T19:59:20.041366: step 2835, loss 0.048626, acc 0.98
2016-09-06T19:59:20.736259: step 2836, loss 0.103775, acc 0.96
2016-09-06T19:59:21.479959: step 2837, loss 0.0390797, acc 1
2016-09-06T19:59:22.156154: step 2838, loss 0.00706514, acc 1
2016-09-06T19:59:22.864553: step 2839, loss 0.036725, acc 0.96
2016-09-06T19:59:23.552227: step 2840, loss 0.00200797, acc 1
2016-09-06T19:59:24.237070: step 2841, loss 0.0930083, acc 0.94
2016-09-06T19:59:24.927111: step 2842, loss 0.14895, acc 0.96
2016-09-06T19:59:25.601144: step 2843, loss 0.0425823, acc 0.98
2016-09-06T19:59:26.307633: step 2844, loss 0.000423231, acc 1
2016-09-06T19:59:26.998719: step 2845, loss 0.0313191, acc 1
2016-09-06T19:59:27.686277: step 2846, loss 0.186847, acc 0.94
2016-09-06T19:59:28.371567: step 2847, loss 0.00223803, acc 1
2016-09-06T19:59:29.036639: step 2848, loss 0.0865356, acc 0.98
2016-09-06T19:59:29.724988: step 2849, loss 0.0018018, acc 1
2016-09-06T19:59:30.404884: step 2850, loss 0.0359585, acc 0.96
2016-09-06T19:59:31.106640: step 2851, loss 0.0205485, acc 0.98
2016-09-06T19:59:31.766080: step 2852, loss 0.027236, acc 1
2016-09-06T19:59:32.457681: step 2853, loss 0.0841472, acc 0.92
2016-09-06T19:59:33.133350: step 2854, loss 0.0197745, acc 1
2016-09-06T19:59:33.826729: step 2855, loss 0.026691, acc 0.98
2016-09-06T19:59:34.503589: step 2856, loss 0.0611445, acc 0.98
2016-09-06T19:59:35.190047: step 2857, loss 0.0117265, acc 1
2016-09-06T19:59:35.919317: step 2858, loss 0.0493213, acc 0.96
2016-09-06T19:59:36.593567: step 2859, loss 0.106528, acc 0.96
2016-09-06T19:59:37.283854: step 2860, loss 0.0837183, acc 0.96
2016-09-06T19:59:38.004893: step 2861, loss 0.0191442, acc 1
2016-09-06T19:59:38.685565: step 2862, loss 0.05202, acc 0.98
2016-09-06T19:59:39.354216: step 2863, loss 0.0557228, acc 0.98
2016-09-06T19:59:40.031281: step 2864, loss 0.0227633, acc 1
2016-09-06T19:59:40.729483: step 2865, loss 0.0415916, acc 0.98
2016-09-06T19:59:41.405388: step 2866, loss 0.042124, acc 0.98
2016-09-06T19:59:42.104308: step 2867, loss 0.0224955, acc 1
2016-09-06T19:59:42.784223: step 2868, loss 0.104346, acc 0.98
2016-09-06T19:59:43.475978: step 2869, loss 0.015976, acc 1
2016-09-06T19:59:44.157336: step 2870, loss 0.0103669, acc 1
2016-09-06T19:59:44.847931: step 2871, loss 0.0519901, acc 0.96
2016-09-06T19:59:45.552484: step 2872, loss 0.0946484, acc 0.94
2016-09-06T19:59:46.250950: step 2873, loss 0.0513118, acc 0.98
2016-09-06T19:59:46.938647: step 2874, loss 0.026392, acc 0.98
2016-09-06T19:59:47.608959: step 2875, loss 0.0261165, acc 0.98
2016-09-06T19:59:48.288459: step 2876, loss 0.0322111, acc 0.98
2016-09-06T19:59:48.976225: step 2877, loss 0.00896773, acc 1
2016-09-06T19:59:49.647203: step 2878, loss 0.0296357, acc 0.98
2016-09-06T19:59:50.335502: step 2879, loss 0.0653786, acc 0.94
2016-09-06T19:59:50.968056: step 2880, loss 0.00913929, acc 1
2016-09-06T19:59:51.675414: step 2881, loss 0.0279584, acc 0.98
2016-09-06T19:59:52.353396: step 2882, loss 0.247679, acc 0.94
2016-09-06T19:59:53.033212: step 2883, loss 0.066003, acc 0.98
2016-09-06T19:59:53.722571: step 2884, loss 0.0350316, acc 1
2016-09-06T19:59:54.420566: step 2885, loss 0.0196155, acc 0.98
2016-09-06T19:59:55.134180: step 2886, loss 0.0667854, acc 0.94
2016-09-06T19:59:55.813275: step 2887, loss 0.00991313, acc 1
2016-09-06T19:59:56.513316: step 2888, loss 0.0572987, acc 0.96
2016-09-06T19:59:57.216737: step 2889, loss 0.0165462, acc 1
2016-09-06T19:59:57.910196: step 2890, loss 0.00527214, acc 1
2016-09-06T19:59:58.593889: step 2891, loss 0.0214857, acc 0.98
2016-09-06T19:59:59.268249: step 2892, loss 0.0169931, acc 1
2016-09-06T19:59:59.971868: step 2893, loss 0.13677, acc 0.96
2016-09-06T20:00:00.668555: step 2894, loss 0.02999, acc 1
2016-09-06T20:00:01.356581: step 2895, loss 0.0357333, acc 0.98
2016-09-06T20:00:02.046652: step 2896, loss 0.0671353, acc 0.98
2016-09-06T20:00:02.735030: step 2897, loss 0.0499165, acc 0.98
2016-09-06T20:00:03.412847: step 2898, loss 0.004524, acc 1
2016-09-06T20:00:04.117250: step 2899, loss 0.0577139, acc 0.98
2016-09-06T20:00:04.827265: step 2900, loss 0.00416628, acc 1

Evaluation:
2016-09-06T20:00:07.987292: step 2900, loss 1.71208, acc 0.733584

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-2900

2016-09-06T20:00:09.736732: step 2901, loss 0.0702648, acc 0.96
2016-09-06T20:00:10.428160: step 2902, loss 0.0157693, acc 1
2016-09-06T20:00:11.125060: step 2903, loss 0.0084347, acc 1
2016-09-06T20:00:11.813089: step 2904, loss 0.250352, acc 0.9
2016-09-06T20:00:12.508619: step 2905, loss 0.0129431, acc 1
2016-09-06T20:00:13.231268: step 2906, loss 0.064817, acc 0.98
2016-09-06T20:00:13.907708: step 2907, loss 0.120355, acc 0.92
2016-09-06T20:00:14.600719: step 2908, loss 0.00828185, acc 1
2016-09-06T20:00:15.306008: step 2909, loss 0.0215558, acc 1
2016-09-06T20:00:15.993481: step 2910, loss 0.0292388, acc 0.98
2016-09-06T20:00:16.676429: step 2911, loss 0.0134723, acc 1
2016-09-06T20:00:17.358349: step 2912, loss 0.0345324, acc 0.96
2016-09-06T20:00:18.055008: step 2913, loss 0.0180775, acc 1
2016-09-06T20:00:18.729282: step 2914, loss 0.0280001, acc 1
2016-09-06T20:00:19.419812: step 2915, loss 0.0494325, acc 0.98
2016-09-06T20:00:20.099453: step 2916, loss 0.0378008, acc 0.98
2016-09-06T20:00:20.780090: step 2917, loss 0.0554138, acc 0.98
2016-09-06T20:00:21.471526: step 2918, loss 0.0218547, acc 1
2016-09-06T20:00:22.174903: step 2919, loss 0.0507105, acc 0.96
2016-09-06T20:00:22.890716: step 2920, loss 0.0941177, acc 0.98
2016-09-06T20:00:23.569147: step 2921, loss 0.0671196, acc 0.96
2016-09-06T20:00:24.266399: step 2922, loss 0.0376628, acc 1
2016-09-06T20:00:24.962544: step 2923, loss 0.0687605, acc 0.96
2016-09-06T20:00:25.654048: step 2924, loss 0.0267848, acc 1
2016-09-06T20:00:26.334171: step 2925, loss 0.0590521, acc 0.96
2016-09-06T20:00:27.022190: step 2926, loss 0.0018695, acc 1
2016-09-06T20:00:27.717992: step 2927, loss 0.017392, acc 1
2016-09-06T20:00:28.390527: step 2928, loss 0.089233, acc 0.96
2016-09-06T20:00:29.053925: step 2929, loss 0.0265099, acc 1
2016-09-06T20:00:29.756364: step 2930, loss 0.0835997, acc 0.96
2016-09-06T20:00:30.453076: step 2931, loss 0.0137076, acc 1
2016-09-06T20:00:31.155061: step 2932, loss 0.000115797, acc 1
2016-09-06T20:00:31.827509: step 2933, loss 0.0076477, acc 1
2016-09-06T20:00:32.534936: step 2934, loss 0.0212993, acc 0.98
2016-09-06T20:00:33.204021: step 2935, loss 0.0340648, acc 1
2016-09-06T20:00:33.894476: step 2936, loss 0.040236, acc 0.96
2016-09-06T20:00:34.567989: step 2937, loss 0.0294461, acc 0.98
2016-09-06T20:00:35.265710: step 2938, loss 0.0177936, acc 1
2016-09-06T20:00:35.974935: step 2939, loss 0.0128706, acc 1
2016-09-06T20:00:36.646961: step 2940, loss 0.029411, acc 0.98
2016-09-06T20:00:37.353545: step 2941, loss 0.0041115, acc 1
2016-09-06T20:00:38.038943: step 2942, loss 0.00638082, acc 1
2016-09-06T20:00:38.729083: step 2943, loss 0.0604454, acc 0.98
2016-09-06T20:00:39.429675: step 2944, loss 0.0213142, acc 0.98
2016-09-06T20:00:40.097771: step 2945, loss 0.00857306, acc 1
2016-09-06T20:00:40.774145: step 2946, loss 0.0413325, acc 0.98
2016-09-06T20:00:41.455131: step 2947, loss 0.107092, acc 0.96
2016-09-06T20:00:42.171573: step 2948, loss 0.0448106, acc 0.98
2016-09-06T20:00:42.860816: step 2949, loss 0.0444143, acc 0.98
2016-09-06T20:00:43.561464: step 2950, loss 0.00109299, acc 1
2016-09-06T20:00:44.240516: step 2951, loss 0.0144237, acc 0.98
2016-09-06T20:00:44.924541: step 2952, loss 0.0667427, acc 0.98
2016-09-06T20:00:45.621603: step 2953, loss 0.15475, acc 0.98
2016-09-06T20:00:46.294525: step 2954, loss 0.301823, acc 0.96
2016-09-06T20:00:47.013494: step 2955, loss 0.021824, acc 0.98
2016-09-06T20:00:47.680351: step 2956, loss 0.0336923, acc 0.96
2016-09-06T20:00:48.365937: step 2957, loss 0.0699791, acc 0.96
2016-09-06T20:00:49.044071: step 2958, loss 0.0263189, acc 0.98
2016-09-06T20:00:49.731178: step 2959, loss 0.149181, acc 0.92
2016-09-06T20:00:50.423840: step 2960, loss 0.0362752, acc 1
2016-09-06T20:00:51.085948: step 2961, loss 0.0212299, acc 0.98
2016-09-06T20:00:51.789706: step 2962, loss 0.0485954, acc 0.98
2016-09-06T20:00:52.438404: step 2963, loss 0.0667466, acc 0.96
2016-09-06T20:00:53.109457: step 2964, loss 0.109166, acc 0.98
2016-09-06T20:00:53.793642: step 2965, loss 0.0185726, acc 1
2016-09-06T20:00:54.493248: step 2966, loss 0.0211845, acc 1
2016-09-06T20:00:55.189350: step 2967, loss 0.10548, acc 0.96
2016-09-06T20:00:55.876985: step 2968, loss 0.0037152, acc 1
2016-09-06T20:00:56.587235: step 2969, loss 0.0430227, acc 0.96
2016-09-06T20:00:57.283409: step 2970, loss 0.0160024, acc 1
2016-09-06T20:00:57.966486: step 2971, loss 0.0864407, acc 0.98
2016-09-06T20:00:58.655182: step 2972, loss 0.0566263, acc 0.96
2016-09-06T20:00:59.346226: step 2973, loss 0.0312407, acc 0.98
2016-09-06T20:01:00.047892: step 2974, loss 0.077882, acc 0.96
2016-09-06T20:01:00.746775: step 2975, loss 0.0342852, acc 1
2016-09-06T20:01:01.446356: step 2976, loss 0.028813, acc 1
2016-09-06T20:01:02.125864: step 2977, loss 0.0250182, acc 0.98
2016-09-06T20:01:02.823310: step 2978, loss 0.0204046, acc 1
2016-09-06T20:01:03.512893: step 2979, loss 0.125403, acc 0.98
2016-09-06T20:01:04.188719: step 2980, loss 0.045842, acc 0.96
2016-09-06T20:01:04.885763: step 2981, loss 0.0354223, acc 1
2016-09-06T20:01:05.565878: step 2982, loss 0.0215463, acc 0.98
2016-09-06T20:01:06.292726: step 2983, loss 0.0287648, acc 0.98
2016-09-06T20:01:06.989536: step 2984, loss 0.0553267, acc 0.96
2016-09-06T20:01:07.671195: step 2985, loss 0.0507042, acc 0.98
2016-09-06T20:01:08.363455: step 2986, loss 0.0124101, acc 1
2016-09-06T20:01:09.047388: step 2987, loss 0.0186549, acc 1
2016-09-06T20:01:09.741318: step 2988, loss 0.138438, acc 0.92
2016-09-06T20:01:10.435497: step 2989, loss 0.0229109, acc 1
2016-09-06T20:01:11.104656: step 2990, loss 0.029689, acc 1
2016-09-06T20:01:11.798570: step 2991, loss 0.049733, acc 0.98
2016-09-06T20:01:12.478026: step 2992, loss 0.0193572, acc 1
2016-09-06T20:01:13.145649: step 2993, loss 0.0124535, acc 1
2016-09-06T20:01:13.824988: step 2994, loss 0.040262, acc 0.98
2016-09-06T20:01:14.505403: step 2995, loss 0.046981, acc 0.98
2016-09-06T20:01:15.183326: step 2996, loss 0.0661082, acc 0.96
2016-09-06T20:01:15.892609: step 2997, loss 0.0251441, acc 0.98
2016-09-06T20:01:16.581950: step 2998, loss 0.0738491, acc 0.96
2016-09-06T20:01:17.269619: step 2999, loss 0.0245084, acc 1
2016-09-06T20:01:17.952370: step 3000, loss 0.20991, acc 0.96

Evaluation:
2016-09-06T20:01:21.087262: step 3000, loss 1.58061, acc 0.750469

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-3000

2016-09-06T20:01:22.810748: step 3001, loss 0.0620468, acc 0.96
2016-09-06T20:01:23.490027: step 3002, loss 0.0860723, acc 0.94
2016-09-06T20:01:24.208466: step 3003, loss 0.000758766, acc 1
2016-09-06T20:01:24.887517: step 3004, loss 0.0421987, acc 0.98
2016-09-06T20:01:25.576168: step 3005, loss 0.0463175, acc 0.96
2016-09-06T20:01:26.260024: step 3006, loss 0.142538, acc 0.92
2016-09-06T20:01:26.929705: step 3007, loss 0.0067079, acc 1
2016-09-06T20:01:27.615028: step 3008, loss 0.0845985, acc 0.96
2016-09-06T20:01:28.282943: step 3009, loss 0.00817121, acc 1
2016-09-06T20:01:28.960544: step 3010, loss 0.0496889, acc 0.98
2016-09-06T20:01:29.640704: step 3011, loss 0.141107, acc 0.96
2016-09-06T20:01:30.345941: step 3012, loss 0.029062, acc 1
2016-09-06T20:01:31.040517: step 3013, loss 0.0271108, acc 0.98
2016-09-06T20:01:31.712711: step 3014, loss 0.0450081, acc 0.98
2016-09-06T20:01:32.399508: step 3015, loss 0.0147876, acc 1
2016-09-06T20:01:33.080886: step 3016, loss 0.0128193, acc 1
2016-09-06T20:01:33.786341: step 3017, loss 0.0247585, acc 0.98
2016-09-06T20:01:34.460713: step 3018, loss 0.00303644, acc 1
2016-09-06T20:01:35.160145: step 3019, loss 0.0640585, acc 0.96
2016-09-06T20:01:35.850502: step 3020, loss 0.0379261, acc 0.98
2016-09-06T20:01:36.525547: step 3021, loss 0.048166, acc 0.96
2016-09-06T20:01:37.212653: step 3022, loss 0.0445117, acc 0.96
2016-09-06T20:01:37.898037: step 3023, loss 0.0186542, acc 0.98
2016-09-06T20:01:38.608130: step 3024, loss 0.0168326, acc 1
2016-09-06T20:01:39.278866: step 3025, loss 0.0017451, acc 1
2016-09-06T20:01:39.984497: step 3026, loss 0.0209627, acc 1
2016-09-06T20:01:40.684381: step 3027, loss 0.0314467, acc 0.98
2016-09-06T20:01:41.371195: step 3028, loss 0.0328225, acc 0.98
2016-09-06T20:01:42.070955: step 3029, loss 0.0260315, acc 1
2016-09-06T20:01:42.750389: step 3030, loss 0.0840643, acc 0.94
2016-09-06T20:01:43.458888: step 3031, loss 0.036757, acc 1
2016-09-06T20:01:44.146688: step 3032, loss 0.0192462, acc 0.98
2016-09-06T20:01:44.846022: step 3033, loss 0.00300339, acc 1
2016-09-06T20:01:45.527326: step 3034, loss 0.258562, acc 0.96
2016-09-06T20:01:46.215296: step 3035, loss 0.087344, acc 0.96
2016-09-06T20:01:46.901050: step 3036, loss 0.00296721, acc 1
2016-09-06T20:01:47.570240: step 3037, loss 0.0824684, acc 0.98
2016-09-06T20:01:48.281671: step 3038, loss 0.0586466, acc 0.98
2016-09-06T20:01:48.954644: step 3039, loss 0.0344043, acc 1
2016-09-06T20:01:49.621274: step 3040, loss 0.0358181, acc 1
2016-09-06T20:01:50.314788: step 3041, loss 0.00523894, acc 1
2016-09-06T20:01:51.005797: step 3042, loss 0.0233975, acc 0.98
2016-09-06T20:01:51.694137: step 3043, loss 0.0264922, acc 0.98
2016-09-06T20:01:52.377462: step 3044, loss 0.0805285, acc 0.96
2016-09-06T20:01:53.076030: step 3045, loss 0.0907385, acc 0.98
2016-09-06T20:01:53.746213: step 3046, loss 0.0348816, acc 0.98
2016-09-06T20:01:54.437470: step 3047, loss 0.0397462, acc 1
2016-09-06T20:01:55.112845: step 3048, loss 0.035235, acc 1
2016-09-06T20:01:55.800942: step 3049, loss 0.0287765, acc 1
2016-09-06T20:01:56.482761: step 3050, loss 0.0213928, acc 1
2016-09-06T20:01:57.185935: step 3051, loss 0.0155398, acc 1
2016-09-06T20:01:57.898042: step 3052, loss 0.0176036, acc 1
2016-09-06T20:01:58.568153: step 3053, loss 0.0153964, acc 0.98
2016-09-06T20:01:59.249469: step 3054, loss 0.0437797, acc 0.98
2016-09-06T20:01:59.933036: step 3055, loss 0.0210162, acc 1
2016-09-06T20:02:00.648842: step 3056, loss 0.0673574, acc 0.96
2016-09-06T20:02:01.353362: step 3057, loss 0.154304, acc 0.96
2016-09-06T20:02:02.024918: step 3058, loss 0.0392179, acc 0.98
2016-09-06T20:02:02.723872: step 3059, loss 0.0762972, acc 0.94
2016-09-06T20:02:03.408852: step 3060, loss 0.0566205, acc 0.94
2016-09-06T20:02:04.086040: step 3061, loss 0.0233398, acc 0.98
2016-09-06T20:02:04.776579: step 3062, loss 0.0499445, acc 0.96
2016-09-06T20:02:05.445973: step 3063, loss 0.00827731, acc 1
2016-09-06T20:02:06.140597: step 3064, loss 0.106069, acc 0.94
2016-09-06T20:02:06.850874: step 3065, loss 0.0575965, acc 0.96
2016-09-06T20:02:07.560286: step 3066, loss 0.01329, acc 1
2016-09-06T20:02:08.236068: step 3067, loss 0.162936, acc 0.92
2016-09-06T20:02:08.907554: step 3068, loss 0.0248351, acc 0.98
2016-09-06T20:02:09.593982: step 3069, loss 0.0359562, acc 0.98
2016-09-06T20:02:10.271039: step 3070, loss 0.0254279, acc 1
2016-09-06T20:02:10.966806: step 3071, loss 0.0673074, acc 0.94
2016-09-06T20:02:11.615980: step 3072, loss 0.00429168, acc 1
2016-09-06T20:02:12.326062: step 3073, loss 0.0750383, acc 0.98
2016-09-06T20:02:12.994391: step 3074, loss 0.0276347, acc 0.98
2016-09-06T20:02:13.691783: step 3075, loss 0.0913322, acc 0.96
2016-09-06T20:02:14.378447: step 3076, loss 0.0393472, acc 0.96
2016-09-06T20:02:15.076357: step 3077, loss 0.0657657, acc 0.94
2016-09-06T20:02:15.749359: step 3078, loss 0.0417092, acc 0.98
2016-09-06T20:02:16.433742: step 3079, loss 0.0605153, acc 0.98
2016-09-06T20:02:17.131521: step 3080, loss 0.0415466, acc 0.98
2016-09-06T20:02:17.812027: step 3081, loss 0.0554079, acc 0.98
2016-09-06T20:02:18.497868: step 3082, loss 0.0198424, acc 1
2016-09-06T20:02:19.205337: step 3083, loss 0.0584795, acc 0.98
2016-09-06T20:02:19.883876: step 3084, loss 0.109245, acc 0.96
2016-09-06T20:02:20.571225: step 3085, loss 0.0676425, acc 0.98
2016-09-06T20:02:21.256053: step 3086, loss 0.0452716, acc 0.98
2016-09-06T20:02:21.975458: step 3087, loss 0.0325538, acc 0.98
2016-09-06T20:02:22.649442: step 3088, loss 0.00931809, acc 1
2016-09-06T20:02:23.344838: step 3089, loss 0.0328368, acc 0.98
2016-09-06T20:02:24.040565: step 3090, loss 0.0281404, acc 1
2016-09-06T20:02:24.721154: step 3091, loss 0.121516, acc 0.96
2016-09-06T20:02:25.395085: step 3092, loss 0.0131486, acc 1
2016-09-06T20:02:26.067587: step 3093, loss 0.1115, acc 0.96
2016-09-06T20:02:26.761736: step 3094, loss 0.00185115, acc 1
2016-09-06T20:02:27.435795: step 3095, loss 0.0723024, acc 0.96
2016-09-06T20:02:28.143034: step 3096, loss 0.0101955, acc 1
2016-09-06T20:02:28.836933: step 3097, loss 0.00612933, acc 1
2016-09-06T20:02:29.526908: step 3098, loss 0.0418491, acc 0.98
2016-09-06T20:02:30.203669: step 3099, loss 0.000292159, acc 1
2016-09-06T20:02:30.892587: step 3100, loss 0.0150658, acc 1

Evaluation:
2016-09-06T20:02:34.024623: step 3100, loss 1.49836, acc 0.729831

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-3100

2016-09-06T20:02:35.680846: step 3101, loss 0.0509059, acc 0.98
2016-09-06T20:02:36.401815: step 3102, loss 0.0907491, acc 0.96
2016-09-06T20:02:37.089082: step 3103, loss 0.0463361, acc 0.98
2016-09-06T20:02:37.778340: step 3104, loss 0.0501603, acc 0.98
2016-09-06T20:02:38.470050: step 3105, loss 0.00809284, acc 1
2016-09-06T20:02:39.160751: step 3106, loss 0.0422623, acc 0.96
2016-09-06T20:02:39.850422: step 3107, loss 0.046008, acc 0.98
2016-09-06T20:02:40.521599: step 3108, loss 0.0597821, acc 0.96
2016-09-06T20:02:41.227043: step 3109, loss 0.049902, acc 0.98
2016-09-06T20:02:41.918744: step 3110, loss 0.0701737, acc 0.96
2016-09-06T20:02:42.595376: step 3111, loss 0.0234995, acc 1
2016-09-06T20:02:43.274626: step 3112, loss 0.0362413, acc 0.96
2016-09-06T20:02:43.982328: step 3113, loss 0.0567311, acc 0.98
2016-09-06T20:02:44.678483: step 3114, loss 0.0496867, acc 0.96
2016-09-06T20:02:45.388672: step 3115, loss 0.0745802, acc 0.96
2016-09-06T20:02:46.104330: step 3116, loss 0.103393, acc 0.96
2016-09-06T20:02:46.789322: step 3117, loss 0.162686, acc 0.94
2016-09-06T20:02:47.483927: step 3118, loss 0.0286549, acc 1
2016-09-06T20:02:48.174680: step 3119, loss 0.00571649, acc 1
2016-09-06T20:02:48.851914: step 3120, loss 0.0577818, acc 0.98
2016-09-06T20:02:49.524122: step 3121, loss 0.0586514, acc 0.96
2016-09-06T20:02:50.197679: step 3122, loss 0.0161257, acc 1
2016-09-06T20:02:50.896417: step 3123, loss 0.026042, acc 0.98
2016-09-06T20:02:51.572532: step 3124, loss 0.000615874, acc 1
2016-09-06T20:02:52.274282: step 3125, loss 0.0103804, acc 1
2016-09-06T20:02:52.970214: step 3126, loss 0.0252429, acc 1
2016-09-06T20:02:53.664227: step 3127, loss 0.0511114, acc 1
2016-09-06T20:02:54.405111: step 3128, loss 0.0140334, acc 1
2016-09-06T20:02:55.078883: step 3129, loss 0.020111, acc 0.98
2016-09-06T20:02:55.757861: step 3130, loss 0.0386701, acc 0.98
2016-09-06T20:02:56.430451: step 3131, loss 0.0295354, acc 0.98
2016-09-06T20:02:57.118324: step 3132, loss 0.049541, acc 0.98
2016-09-06T20:02:57.803746: step 3133, loss 0.0033375, acc 1
2016-09-06T20:02:58.489224: step 3134, loss 0.0586216, acc 0.96
2016-09-06T20:02:59.187077: step 3135, loss 0.111065, acc 0.94
2016-09-06T20:02:59.854671: step 3136, loss 0.0216096, acc 1
2016-09-06T20:03:00.589741: step 3137, loss 0.00349084, acc 1
2016-09-06T20:03:01.280056: step 3138, loss 0.0835732, acc 0.94
2016-09-06T20:03:01.955652: step 3139, loss 0.0195742, acc 0.98
2016-09-06T20:03:02.654159: step 3140, loss 0.0055461, acc 1
2016-09-06T20:03:03.323949: step 3141, loss 0.0580801, acc 0.96
2016-09-06T20:03:04.019365: step 3142, loss 0.0200154, acc 0.98
2016-09-06T20:03:04.681658: step 3143, loss 0.0466465, acc 0.96
2016-09-06T20:03:05.392286: step 3144, loss 0.0169115, acc 1
2016-09-06T20:03:06.113366: step 3145, loss 0.0460063, acc 0.98
2016-09-06T20:03:06.809046: step 3146, loss 0.0292105, acc 0.98
2016-09-06T20:03:07.501750: step 3147, loss 0.0270835, acc 0.98
2016-09-06T20:03:08.182749: step 3148, loss 0.0409131, acc 0.98
2016-09-06T20:03:08.871011: step 3149, loss 0.0846072, acc 0.96
2016-09-06T20:03:09.561744: step 3150, loss 0.0238899, acc 0.98
2016-09-06T20:03:10.243866: step 3151, loss 0.0149929, acc 0.98
2016-09-06T20:03:10.909967: step 3152, loss 0.0399778, acc 0.96
2016-09-06T20:03:11.608367: step 3153, loss 0.0369742, acc 0.98
2016-09-06T20:03:12.298142: step 3154, loss 0.0203738, acc 1
2016-09-06T20:03:12.987543: step 3155, loss 0.0225491, acc 0.98
2016-09-06T20:03:13.707171: step 3156, loss 0.0951914, acc 0.96
2016-09-06T20:03:14.377964: step 3157, loss 0.133812, acc 0.98
2016-09-06T20:03:15.066583: step 3158, loss 0.0210224, acc 1
2016-09-06T20:03:15.763267: step 3159, loss 0.0212905, acc 0.98
2016-09-06T20:03:16.470326: step 3160, loss 0.036711, acc 0.98
2016-09-06T20:03:17.154812: step 3161, loss 0.0359109, acc 0.98
2016-09-06T20:03:17.830135: step 3162, loss 0.0233971, acc 1
2016-09-06T20:03:18.544478: step 3163, loss 0.0293823, acc 0.98
2016-09-06T20:03:19.242202: step 3164, loss 0.0340931, acc 1
2016-09-06T20:03:19.931083: step 3165, loss 0.0406525, acc 0.96
2016-09-06T20:03:20.599943: step 3166, loss 0.0122237, acc 1
2016-09-06T20:03:21.289592: step 3167, loss 0.10346, acc 0.92
2016-09-06T20:03:21.979118: step 3168, loss 0.125677, acc 0.96
2016-09-06T20:03:22.667844: step 3169, loss 0.0334867, acc 0.98
2016-09-06T20:03:23.365285: step 3170, loss 0.0358186, acc 0.96
2016-09-06T20:03:24.045039: step 3171, loss 0.111472, acc 0.98
2016-09-06T20:03:24.746539: step 3172, loss 0.0442936, acc 0.98
2016-09-06T20:03:25.436943: step 3173, loss 0.0249601, acc 0.98
2016-09-06T20:03:26.144049: step 3174, loss 0.00470036, acc 1
2016-09-06T20:03:26.854038: step 3175, loss 0.0273334, acc 1
2016-09-06T20:03:27.547175: step 3176, loss 0.0351391, acc 1
2016-09-06T20:03:28.242112: step 3177, loss 0.0255341, acc 0.98
2016-09-06T20:03:28.943840: step 3178, loss 0.0772372, acc 0.96
2016-09-06T20:03:29.617877: step 3179, loss 0.0269676, acc 0.98
2016-09-06T20:03:30.322088: step 3180, loss 0.0393882, acc 0.98
2016-09-06T20:03:30.992721: step 3181, loss 0.0600645, acc 0.96
2016-09-06T20:03:31.699371: step 3182, loss 0.0507703, acc 0.96
2016-09-06T20:03:32.393398: step 3183, loss 0.0128488, acc 1
2016-09-06T20:03:33.068832: step 3184, loss 0.0171459, acc 0.98
2016-09-06T20:03:33.748182: step 3185, loss 0.0156358, acc 1
2016-09-06T20:03:34.431690: step 3186, loss 0.0643628, acc 0.96
2016-09-06T20:03:35.133505: step 3187, loss 0.0909047, acc 0.96
2016-09-06T20:03:35.813787: step 3188, loss 0.00952175, acc 1
2016-09-06T20:03:36.520639: step 3189, loss 0.0247556, acc 1
2016-09-06T20:03:37.200836: step 3190, loss 0.0564907, acc 0.98
2016-09-06T20:03:37.889044: step 3191, loss 0.0833333, acc 0.94
2016-09-06T20:03:38.566437: step 3192, loss 0.00521628, acc 1
2016-09-06T20:03:39.260627: step 3193, loss 0.1299, acc 0.92
2016-09-06T20:03:39.967663: step 3194, loss 0.251603, acc 0.96
2016-09-06T20:03:40.651056: step 3195, loss 0.0136431, acc 1
2016-09-06T20:03:41.369029: step 3196, loss 0.0419674, acc 0.96
2016-09-06T20:03:42.039650: step 3197, loss 0.0318902, acc 0.98
2016-09-06T20:03:42.707580: step 3198, loss 0.0142161, acc 1
2016-09-06T20:03:43.376523: step 3199, loss 0.00839248, acc 1
2016-09-06T20:03:44.061418: step 3200, loss 0.0199269, acc 1

Evaluation:
2016-09-06T20:03:47.238968: step 3200, loss 2.14574, acc 0.742964

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-3200

2016-09-06T20:03:48.927387: step 3201, loss 0.00404736, acc 1
2016-09-06T20:03:49.631295: step 3202, loss 0.105726, acc 0.96
2016-09-06T20:03:50.292161: step 3203, loss 0.117599, acc 0.96
2016-09-06T20:03:50.996759: step 3204, loss 0.0580884, acc 0.98
2016-09-06T20:03:51.683146: step 3205, loss 0.0816657, acc 0.94
2016-09-06T20:03:52.364469: step 3206, loss 0.0070634, acc 1
2016-09-06T20:03:53.039045: step 3207, loss 0.00667703, acc 1
2016-09-06T20:03:53.740673: step 3208, loss 0.0093317, acc 1
2016-09-06T20:03:54.441254: step 3209, loss 0.0257332, acc 1
2016-09-06T20:03:55.130482: step 3210, loss 0.0255879, acc 1
2016-09-06T20:03:55.828151: step 3211, loss 0.0146341, acc 1
2016-09-06T20:03:56.525455: step 3212, loss 0.0780978, acc 0.96
2016-09-06T20:03:57.210456: step 3213, loss 0.0161799, acc 1
2016-09-06T20:03:57.897596: step 3214, loss 0.0975418, acc 0.98
2016-09-06T20:03:58.594184: step 3215, loss 0.0147844, acc 1
2016-09-06T20:03:59.275543: step 3216, loss 0.16986, acc 0.98
2016-09-06T20:03:59.942555: step 3217, loss 0.163657, acc 0.94
2016-09-06T20:04:00.656416: step 3218, loss 0.0156892, acc 0.98
2016-09-06T20:04:01.313429: step 3219, loss 0.0461087, acc 0.98
2016-09-06T20:04:01.986070: step 3220, loss 0.0519451, acc 0.98
2016-09-06T20:04:02.681211: step 3221, loss 0.0349181, acc 0.98
2016-09-06T20:04:03.365536: step 3222, loss 0.0449569, acc 0.98
2016-09-06T20:04:04.062705: step 3223, loss 0.0394407, acc 0.96
2016-09-06T20:04:04.720684: step 3224, loss 0.0469046, acc 0.98
2016-09-06T20:04:05.414766: step 3225, loss 0.00353489, acc 1
2016-09-06T20:04:06.084510: step 3226, loss 0.0133659, acc 1
2016-09-06T20:04:06.758068: step 3227, loss 0.0312503, acc 0.98
2016-09-06T20:04:07.469992: step 3228, loss 0.000649562, acc 1
2016-09-06T20:04:08.153416: step 3229, loss 0.00723668, acc 1
2016-09-06T20:04:08.836476: step 3230, loss 0.116316, acc 0.96
2016-09-06T20:04:09.530563: step 3231, loss 0.0522183, acc 0.98
2016-09-06T20:04:10.240524: step 3232, loss 0.0322072, acc 0.96
2016-09-06T20:04:10.893029: step 3233, loss 0.0132583, acc 1
2016-09-06T20:04:11.582623: step 3234, loss 0.0291385, acc 0.98
2016-09-06T20:04:12.267498: step 3235, loss 0.00640979, acc 1
2016-09-06T20:04:12.961213: step 3236, loss 0.00138206, acc 1
2016-09-06T20:04:13.686974: step 3237, loss 0.0339547, acc 1
2016-09-06T20:04:14.369512: step 3238, loss 0.0825652, acc 0.98
2016-09-06T20:04:15.076327: step 3239, loss 0.0127098, acc 1
2016-09-06T20:04:15.781018: step 3240, loss 0.0153345, acc 1
2016-09-06T20:04:16.493090: step 3241, loss 0.0271816, acc 0.98
2016-09-06T20:04:17.202263: step 3242, loss 0.0531194, acc 0.98
2016-09-06T20:04:17.894969: step 3243, loss 0.000335593, acc 1
2016-09-06T20:04:18.606082: step 3244, loss 0.119429, acc 0.96
2016-09-06T20:04:19.309976: step 3245, loss 0.0310954, acc 1
2016-09-06T20:04:20.004081: step 3246, loss 0.0832537, acc 0.98
2016-09-06T20:04:20.673806: step 3247, loss 0.0118526, acc 1
2016-09-06T20:04:21.367849: step 3248, loss 0.0735407, acc 0.98
2016-09-06T20:04:22.060062: step 3249, loss 0.017772, acc 1
2016-09-06T20:04:22.738245: step 3250, loss 0.105502, acc 0.96
2016-09-06T20:04:23.451966: step 3251, loss 0.126828, acc 0.96
2016-09-06T20:04:24.132689: step 3252, loss 0.0060137, acc 1
2016-09-06T20:04:24.816180: step 3253, loss 0.0127657, acc 1
2016-09-06T20:04:25.523256: step 3254, loss 0.0810128, acc 0.98
2016-09-06T20:04:26.211901: step 3255, loss 0.0586181, acc 0.96
2016-09-06T20:04:26.899283: step 3256, loss 0.0260933, acc 1
2016-09-06T20:04:27.558361: step 3257, loss 0.00998021, acc 1
2016-09-06T20:04:28.258259: step 3258, loss 0.0420916, acc 0.98
2016-09-06T20:04:28.919215: step 3259, loss 0.0251193, acc 1
2016-09-06T20:04:29.591416: step 3260, loss 0.0334898, acc 1
2016-09-06T20:04:30.266255: step 3261, loss 0.0253076, acc 1
2016-09-06T20:04:30.940436: step 3262, loss 0.151558, acc 0.94
2016-09-06T20:04:31.630959: step 3263, loss 0.0174748, acc 1
2016-09-06T20:04:32.288383: step 3264, loss 0.0409288, acc 0.977273
2016-09-06T20:04:33.005695: step 3265, loss 0.0454365, acc 0.98
2016-09-06T20:04:33.691136: step 3266, loss 0.00457656, acc 1
2016-09-06T20:04:34.375615: step 3267, loss 0.085606, acc 0.94
2016-09-06T20:04:35.057769: step 3268, loss 0.0623432, acc 0.98
2016-09-06T20:04:35.756194: step 3269, loss 0.0475584, acc 0.98
2016-09-06T20:04:36.432039: step 3270, loss 0.0474377, acc 0.96
2016-09-06T20:04:37.097898: step 3271, loss 0.0138403, acc 1
2016-09-06T20:04:37.808012: step 3272, loss 0.0394053, acc 0.96
2016-09-06T20:04:38.472787: step 3273, loss 0.0269563, acc 1
2016-09-06T20:04:39.176674: step 3274, loss 0.00131987, acc 1
2016-09-06T20:04:39.851329: step 3275, loss 0.00101746, acc 1
2016-09-06T20:04:40.536651: step 3276, loss 0.0869961, acc 0.98
2016-09-06T20:04:41.217453: step 3277, loss 0.0461484, acc 1
2016-09-06T20:04:41.921695: step 3278, loss 0.0766136, acc 0.96
2016-09-06T20:04:42.630907: step 3279, loss 0.000324168, acc 1
2016-09-06T20:04:43.292084: step 3280, loss 0.0308373, acc 0.98
2016-09-06T20:04:43.996540: step 3281, loss 0.0306489, acc 0.98
2016-09-06T20:04:44.678355: step 3282, loss 0.00337307, acc 1
2016-09-06T20:04:45.354680: step 3283, loss 0.0822126, acc 0.96
2016-09-06T20:04:46.042435: step 3284, loss 0.0443972, acc 0.98
2016-09-06T20:04:46.726936: step 3285, loss 0.0277167, acc 0.98
2016-09-06T20:04:47.406406: step 3286, loss 0.0342223, acc 0.98
2016-09-06T20:04:48.074867: step 3287, loss 0.0140039, acc 1
2016-09-06T20:04:48.749362: step 3288, loss 0.0854207, acc 0.96
2016-09-06T20:04:49.442781: step 3289, loss 0.0256667, acc 0.98
2016-09-06T20:04:50.140876: step 3290, loss 0.041763, acc 0.98
2016-09-06T20:04:50.836583: step 3291, loss 0.0176076, acc 1
2016-09-06T20:04:51.526609: step 3292, loss 0.0136052, acc 1
2016-09-06T20:04:52.217645: step 3293, loss 0.0547346, acc 0.96
2016-09-06T20:04:52.917968: step 3294, loss 0.0817117, acc 0.98
2016-09-06T20:04:53.617248: step 3295, loss 0.016048, acc 1
2016-09-06T20:04:54.309436: step 3296, loss 0.00702111, acc 1
2016-09-06T20:04:55.013719: step 3297, loss 0.000663072, acc 1
2016-09-06T20:04:55.723847: step 3298, loss 0.00174131, acc 1
2016-09-06T20:04:56.401111: step 3299, loss 0.0983793, acc 0.96
2016-09-06T20:04:57.118405: step 3300, loss 0.0166867, acc 1

Evaluation:
2016-09-06T20:05:00.289723: step 3300, loss 1.72913, acc 0.742964

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-3300

2016-09-06T20:05:02.002842: step 3301, loss 0.00926557, acc 1
2016-09-06T20:05:02.691570: step 3302, loss 0.0203116, acc 1
2016-09-06T20:05:03.415589: step 3303, loss 0.0322477, acc 1
2016-09-06T20:05:04.097814: step 3304, loss 0.016313, acc 1
2016-09-06T20:05:04.790444: step 3305, loss 0.0741184, acc 0.94
2016-09-06T20:05:05.490672: step 3306, loss 0.0139519, acc 1
2016-09-06T20:05:06.164546: step 3307, loss 0.0317054, acc 1
2016-09-06T20:05:06.839819: step 3308, loss 0.0363209, acc 1
2016-09-06T20:05:07.524821: step 3309, loss 0.0199168, acc 1
2016-09-06T20:05:08.210419: step 3310, loss 0.0397321, acc 0.98
2016-09-06T20:05:08.897884: step 3311, loss 0.0144218, acc 1
2016-09-06T20:05:09.588143: step 3312, loss 0.0142795, acc 1
2016-09-06T20:05:10.302951: step 3313, loss 0.0687002, acc 0.96
2016-09-06T20:05:11.007462: step 3314, loss 0.0175176, acc 0.98
2016-09-06T20:05:11.699682: step 3315, loss 0.065385, acc 0.98
2016-09-06T20:05:12.379220: step 3316, loss 0.0398457, acc 0.98
2016-09-06T20:05:13.057388: step 3317, loss 0.0235962, acc 0.98
2016-09-06T20:05:13.757879: step 3318, loss 0.0198288, acc 1
2016-09-06T20:05:14.440814: step 3319, loss 0.0870542, acc 0.94
2016-09-06T20:05:15.136332: step 3320, loss 0.188384, acc 0.96
2016-09-06T20:05:15.817003: step 3321, loss 0.0519508, acc 0.96
2016-09-06T20:05:16.498406: step 3322, loss 0.00297153, acc 1
2016-09-06T20:05:17.202948: step 3323, loss 0.079077, acc 0.98
2016-09-06T20:05:17.875510: step 3324, loss 0.00138701, acc 1
2016-09-06T20:05:18.566222: step 3325, loss 0.00628876, acc 1
2016-09-06T20:05:19.240016: step 3326, loss 0.00401103, acc 1
2016-09-06T20:05:19.938376: step 3327, loss 0.199893, acc 0.98
2016-09-06T20:05:20.623391: step 3328, loss 0.0363188, acc 1
2016-09-06T20:05:21.309198: step 3329, loss 0.0111774, acc 1
2016-09-06T20:05:21.993755: step 3330, loss 0.0780034, acc 0.94
2016-09-06T20:05:22.701936: step 3331, loss 0.0775774, acc 0.98
2016-09-06T20:05:23.397895: step 3332, loss 0.0919271, acc 0.96
2016-09-06T20:05:24.057809: step 3333, loss 0.102828, acc 0.9
2016-09-06T20:05:24.751790: step 3334, loss 0.0151521, acc 1
2016-09-06T20:05:25.432229: step 3335, loss 0.0109319, acc 1
2016-09-06T20:05:26.100098: step 3336, loss 0.0213539, acc 1
2016-09-06T20:05:26.785198: step 3337, loss 0.0184766, acc 1
2016-09-06T20:05:27.470346: step 3338, loss 0.0293575, acc 1
2016-09-06T20:05:28.157743: step 3339, loss 0.0707751, acc 0.94
2016-09-06T20:05:28.821680: step 3340, loss 0.0756427, acc 0.96
2016-09-06T20:05:29.512021: step 3341, loss 0.0468066, acc 0.98
2016-09-06T20:05:30.176382: step 3342, loss 0.00402011, acc 1
2016-09-06T20:05:30.870681: step 3343, loss 0.0378293, acc 0.96
2016-09-06T20:05:31.568662: step 3344, loss 0.0334721, acc 0.98
2016-09-06T20:05:32.257724: step 3345, loss 0.0287492, acc 0.98
2016-09-06T20:05:32.949537: step 3346, loss 0.0727125, acc 0.96
2016-09-06T20:05:33.651896: step 3347, loss 0.134345, acc 0.94
2016-09-06T20:05:34.351775: step 3348, loss 0.0325971, acc 0.98
2016-09-06T20:05:35.016404: step 3349, loss 0.0163219, acc 1
2016-09-06T20:05:35.688738: step 3350, loss 0.0229991, acc 0.98
2016-09-06T20:05:36.362606: step 3351, loss 0.0383312, acc 0.98
2016-09-06T20:05:37.049686: step 3352, loss 0.0237719, acc 1
2016-09-06T20:05:37.750909: step 3353, loss 0.0586886, acc 0.96
2016-09-06T20:05:38.445570: step 3354, loss 0.00857132, acc 1
2016-09-06T20:05:39.146804: step 3355, loss 0.0145036, acc 1
2016-09-06T20:05:39.823990: step 3356, loss 0.0127548, acc 1
2016-09-06T20:05:40.511533: step 3357, loss 0.0321592, acc 0.98
2016-09-06T20:05:41.200355: step 3358, loss 0.0287653, acc 1
2016-09-06T20:05:41.894271: step 3359, loss 0.0615182, acc 0.96
2016-09-06T20:05:42.566125: step 3360, loss 0.0740057, acc 0.96
2016-09-06T20:05:43.247091: step 3361, loss 0.0483127, acc 0.96
2016-09-06T20:05:43.958357: step 3362, loss 0.123385, acc 0.94
2016-09-06T20:05:44.630284: step 3363, loss 0.0652543, acc 0.98
2016-09-06T20:05:45.325862: step 3364, loss 0.0230073, acc 1
2016-09-06T20:05:46.037142: step 3365, loss 0.0730042, acc 0.98
2016-09-06T20:05:46.727210: step 3366, loss 0.000697102, acc 1
2016-09-06T20:05:47.397855: step 3367, loss 0.0200124, acc 0.98
2016-09-06T20:05:48.095829: step 3368, loss 0.024476, acc 1
2016-09-06T20:05:48.821450: step 3369, loss 0.169932, acc 0.96
2016-09-06T20:05:49.503927: step 3370, loss 0.0231542, acc 1
2016-09-06T20:05:50.185785: step 3371, loss 0.0109189, acc 1
2016-09-06T20:05:50.875010: step 3372, loss 0.0344612, acc 0.98
2016-09-06T20:05:51.566196: step 3373, loss 0.0317787, acc 0.98
2016-09-06T20:05:52.250526: step 3374, loss 0.0232303, acc 0.98
2016-09-06T20:05:52.917849: step 3375, loss 0.0146717, acc 1
2016-09-06T20:05:53.623465: step 3376, loss 0.0278395, acc 1
2016-09-06T20:05:54.302714: step 3377, loss 0.0247718, acc 0.98
2016-09-06T20:05:54.979487: step 3378, loss 0.0406801, acc 0.96
2016-09-06T20:05:55.663612: step 3379, loss 0.0196198, acc 1
2016-09-06T20:05:56.348129: step 3380, loss 0.0292486, acc 0.98
2016-09-06T20:05:57.037692: step 3381, loss 0.0104338, acc 1
2016-09-06T20:05:57.705243: step 3382, loss 0.00289411, acc 1
2016-09-06T20:05:58.413320: step 3383, loss 0.0322737, acc 0.98
2016-09-06T20:05:59.081891: step 3384, loss 0.0267954, acc 1
2016-09-06T20:05:59.768703: step 3385, loss 0.0312972, acc 0.98
2016-09-06T20:06:00.479165: step 3386, loss 0.0366323, acc 0.96
2016-09-06T20:06:01.173982: step 3387, loss 0.0220243, acc 0.98
2016-09-06T20:06:01.864239: step 3388, loss 0.0372602, acc 0.98
2016-09-06T20:06:02.530648: step 3389, loss 0.00372437, acc 1
2016-09-06T20:06:03.227062: step 3390, loss 0.00445699, acc 1
2016-09-06T20:06:03.914580: step 3391, loss 0.00424625, acc 1
2016-09-06T20:06:04.607781: step 3392, loss 0.0503863, acc 0.96
2016-09-06T20:06:05.273007: step 3393, loss 0.00340666, acc 1
2016-09-06T20:06:05.960276: step 3394, loss 0.0578436, acc 0.96
2016-09-06T20:06:06.633756: step 3395, loss 0.105414, acc 0.94
2016-09-06T20:06:07.325506: step 3396, loss 0.0797047, acc 0.94
2016-09-06T20:06:08.012422: step 3397, loss 0.0253034, acc 1
2016-09-06T20:06:08.688572: step 3398, loss 0.133257, acc 0.96
2016-09-06T20:06:09.372162: step 3399, loss 0.000200683, acc 1
2016-09-06T20:06:10.070973: step 3400, loss 0.000762073, acc 1

Evaluation:
2016-09-06T20:06:13.228995: step 3400, loss 2.1089, acc 0.754221

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-3400

2016-09-06T20:06:14.881452: step 3401, loss 0.157143, acc 0.96
2016-09-06T20:06:15.577431: step 3402, loss 0.000344223, acc 1
2016-09-06T20:06:16.266446: step 3403, loss 0.0101665, acc 1
2016-09-06T20:06:16.956578: step 3404, loss 0.0457481, acc 0.98
2016-09-06T20:06:17.658369: step 3405, loss 0.0324932, acc 0.98
2016-09-06T20:06:18.363572: step 3406, loss 0.00771338, acc 1
2016-09-06T20:06:19.065865: step 3407, loss 0.00728209, acc 1
2016-09-06T20:06:19.748870: step 3408, loss 0.204168, acc 0.92
2016-09-06T20:06:20.443010: step 3409, loss 0.0482694, acc 0.98
2016-09-06T20:06:21.148509: step 3410, loss 0.0512761, acc 0.96
2016-09-06T20:06:21.810326: step 3411, loss 0.0466272, acc 0.98
2016-09-06T20:06:22.495830: step 3412, loss 0.0200503, acc 1
2016-09-06T20:06:23.169475: step 3413, loss 0.0241346, acc 0.98
2016-09-06T20:06:23.856131: step 3414, loss 0.0578387, acc 0.96
2016-09-06T20:06:24.545122: step 3415, loss 0.0365274, acc 0.98
2016-09-06T20:06:25.236946: step 3416, loss 0.0342914, acc 0.96
2016-09-06T20:06:25.939893: step 3417, loss 0.00334691, acc 1
2016-09-06T20:06:26.641068: step 3418, loss 0.0712004, acc 0.96
2016-09-06T20:06:27.373855: step 3419, loss 0.0602504, acc 0.98
2016-09-06T20:06:28.049674: step 3420, loss 0.00644058, acc 1
2016-09-06T20:06:28.733481: step 3421, loss 0.0437922, acc 0.98
2016-09-06T20:06:29.452853: step 3422, loss 0.054504, acc 0.96
2016-09-06T20:06:30.180964: step 3423, loss 0.0274761, acc 1
2016-09-06T20:06:30.907340: step 3424, loss 0.0315782, acc 0.98
2016-09-06T20:06:31.600529: step 3425, loss 0.0116754, acc 1
2016-09-06T20:06:32.262703: step 3426, loss 0.0201337, acc 0.98
2016-09-06T20:06:32.955501: step 3427, loss 0.031551, acc 1
2016-09-06T20:06:33.632540: step 3428, loss 0.0307689, acc 0.98
2016-09-06T20:06:34.357363: step 3429, loss 0.0271013, acc 1
2016-09-06T20:06:35.036094: step 3430, loss 0.0407119, acc 0.98
2016-09-06T20:06:35.756985: step 3431, loss 0.0324095, acc 0.96
2016-09-06T20:06:36.430501: step 3432, loss 0.131602, acc 0.92
2016-09-06T20:06:37.095454: step 3433, loss 0.0695854, acc 0.96
2016-09-06T20:06:37.797694: step 3434, loss 0.0273131, acc 0.98
2016-09-06T20:06:38.489944: step 3435, loss 0.0375163, acc 0.98
2016-09-06T20:06:39.188101: step 3436, loss 0.121338, acc 0.94
2016-09-06T20:06:39.855761: step 3437, loss 0.0484698, acc 0.98
2016-09-06T20:06:40.577844: step 3438, loss 0.0539767, acc 0.98
2016-09-06T20:06:41.260056: step 3439, loss 0.0112489, acc 1
2016-09-06T20:06:41.954028: step 3440, loss 0.0390676, acc 0.98
2016-09-06T20:06:42.657826: step 3441, loss 0.00180835, acc 1
2016-09-06T20:06:43.329743: step 3442, loss 0.0385284, acc 1
2016-09-06T20:06:44.033207: step 3443, loss 0.0502442, acc 0.98
2016-09-06T20:06:44.710279: step 3444, loss 0.0111305, acc 1
2016-09-06T20:06:45.430845: step 3445, loss 0.0212981, acc 0.98
2016-09-06T20:06:46.139768: step 3446, loss 0.0321166, acc 0.98
2016-09-06T20:06:46.835496: step 3447, loss 0.00582463, acc 1
2016-09-06T20:06:47.525517: step 3448, loss 0.0465299, acc 0.98
2016-09-06T20:06:48.227133: step 3449, loss 0.040017, acc 0.96
2016-09-06T20:06:48.924755: step 3450, loss 0.0901934, acc 0.98
2016-09-06T20:06:49.598430: step 3451, loss 0.227912, acc 0.96
2016-09-06T20:06:50.271884: step 3452, loss 0.0854816, acc 0.94
2016-09-06T20:06:50.962422: step 3453, loss 0.0206026, acc 1
2016-09-06T20:06:51.662545: step 3454, loss 0.0317143, acc 1
2016-09-06T20:06:52.347125: step 3455, loss 0.0573973, acc 0.94
2016-09-06T20:06:52.978282: step 3456, loss 0.0326033, acc 0.977273
2016-09-06T20:06:53.664195: step 3457, loss 0.033692, acc 0.98
2016-09-06T20:06:54.338770: step 3458, loss 0.0428348, acc 0.98
2016-09-06T20:06:55.050359: step 3459, loss 0.0746842, acc 0.98
2016-09-06T20:06:55.748031: step 3460, loss 0.032151, acc 0.98
2016-09-06T20:06:56.423279: step 3461, loss 0.0219473, acc 0.98
2016-09-06T20:06:57.105543: step 3462, loss 0.0513886, acc 0.98
2016-09-06T20:06:57.804779: step 3463, loss 0.0235684, acc 0.98
2016-09-06T20:06:58.498266: step 3464, loss 0.0059461, acc 1
2016-09-06T20:06:59.165693: step 3465, loss 0.0186914, acc 1
2016-09-06T20:06:59.868815: step 3466, loss 0.079524, acc 0.96
2016-09-06T20:07:00.584482: step 3467, loss 0.0617751, acc 0.98
2016-09-06T20:07:01.276679: step 3468, loss 0.0565181, acc 0.96
2016-09-06T20:07:01.958120: step 3469, loss 0.105042, acc 0.94
2016-09-06T20:07:02.659492: step 3470, loss 0.0460113, acc 0.98
2016-09-06T20:07:03.343100: step 3471, loss 0.00121881, acc 1
2016-09-06T20:07:04.035042: step 3472, loss 0.0114937, acc 1
2016-09-06T20:07:04.709251: step 3473, loss 0.0235177, acc 0.98
2016-09-06T20:07:05.400835: step 3474, loss 0.0346729, acc 1
2016-09-06T20:07:06.085918: step 3475, loss 0.0344227, acc 0.98
2016-09-06T20:07:06.765145: step 3476, loss 0.0280857, acc 0.98
2016-09-06T20:07:07.441835: step 3477, loss 0.0306486, acc 1
2016-09-06T20:07:08.137779: step 3478, loss 0.0474866, acc 0.94
2016-09-06T20:07:08.813521: step 3479, loss 0.0671782, acc 0.98
2016-09-06T20:07:09.518630: step 3480, loss 0.027112, acc 1
2016-09-06T20:07:10.214003: step 3481, loss 0.000871194, acc 1
2016-09-06T20:07:10.904017: step 3482, loss 0.0427407, acc 0.98
2016-09-06T20:07:11.602476: step 3483, loss 0.0200268, acc 0.98
2016-09-06T20:07:12.282871: step 3484, loss 0.0214922, acc 0.98
2016-09-06T20:07:13.005629: step 3485, loss 0.0882422, acc 0.98
2016-09-06T20:07:13.684860: step 3486, loss 0.131621, acc 0.96
2016-09-06T20:07:14.349145: step 3487, loss 0.0116891, acc 1
2016-09-06T20:07:15.016660: step 3488, loss 0.0530598, acc 0.98
2016-09-06T20:07:15.705404: step 3489, loss 0.0293297, acc 0.98
2016-09-06T20:07:16.379824: step 3490, loss 0.017751, acc 0.98
2016-09-06T20:07:17.061030: step 3491, loss 0.0186465, acc 0.98
2016-09-06T20:07:17.775682: step 3492, loss 0.0347807, acc 1
2016-09-06T20:07:18.456464: step 3493, loss 0.00346761, acc 1
2016-09-06T20:07:19.153134: step 3494, loss 0.0742425, acc 0.94
2016-09-06T20:07:19.860835: step 3495, loss 0.0034882, acc 1
2016-09-06T20:07:20.570767: step 3496, loss 0.0581054, acc 0.98
2016-09-06T20:07:21.264690: step 3497, loss 0.0251477, acc 1
2016-09-06T20:07:21.934829: step 3498, loss 0.0606438, acc 0.96
2016-09-06T20:07:22.623961: step 3499, loss 0.0220847, acc 1
2016-09-06T20:07:23.302172: step 3500, loss 0.000953156, acc 1

Evaluation:
2016-09-06T20:07:26.445400: step 3500, loss 2.08899, acc 0.739212

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-3500

2016-09-06T20:07:28.183960: step 3501, loss 0.0274201, acc 1
2016-09-06T20:07:28.873803: step 3502, loss 0.0662212, acc 0.96
2016-09-06T20:07:29.560755: step 3503, loss 0.000828196, acc 1
2016-09-06T20:07:30.249883: step 3504, loss 0.0152486, acc 1
2016-09-06T20:07:30.959759: step 3505, loss 0.0187026, acc 1
2016-09-06T20:07:31.615185: step 3506, loss 0.00725429, acc 1
2016-09-06T20:07:32.320958: step 3507, loss 0.00576639, acc 1
2016-09-06T20:07:32.991328: step 3508, loss 0.0144345, acc 1
2016-09-06T20:07:33.669037: step 3509, loss 0.00831213, acc 1
2016-09-06T20:07:34.343811: step 3510, loss 0.0232182, acc 0.98
2016-09-06T20:07:35.002769: step 3511, loss 0.103663, acc 0.96
2016-09-06T20:07:35.695537: step 3512, loss 0.0236056, acc 1
2016-09-06T20:07:36.358595: step 3513, loss 0.0192473, acc 1
2016-09-06T20:07:37.050285: step 3514, loss 0.0707838, acc 0.98
2016-09-06T20:07:37.732476: step 3515, loss 0.128346, acc 0.96
2016-09-06T20:07:38.423708: step 3516, loss 0.0170904, acc 0.98
2016-09-06T20:07:39.097045: step 3517, loss 0.0280782, acc 0.98
2016-09-06T20:07:39.776633: step 3518, loss 0.0151616, acc 1
2016-09-06T20:07:40.452510: step 3519, loss 0.0188683, acc 1
2016-09-06T20:07:41.161844: step 3520, loss 0.0273811, acc 1
2016-09-06T20:07:41.858033: step 3521, loss 0.00237144, acc 1
2016-09-06T20:07:42.533435: step 3522, loss 0.0425158, acc 0.98
2016-09-06T20:07:43.223636: step 3523, loss 0.0637513, acc 0.98
2016-09-06T20:07:43.932710: step 3524, loss 0.0186997, acc 0.98
2016-09-06T20:07:44.662323: step 3525, loss 0.0836581, acc 0.96
2016-09-06T20:07:45.364177: step 3526, loss 0.00623948, acc 1
2016-09-06T20:07:46.024255: step 3527, loss 0.0190886, acc 1
2016-09-06T20:07:46.714027: step 3528, loss 0.00927685, acc 1
2016-09-06T20:07:47.390648: step 3529, loss 0.0104348, acc 1
2016-09-06T20:07:48.081003: step 3530, loss 0.0401713, acc 0.96
2016-09-06T20:07:48.766791: step 3531, loss 0.0748929, acc 0.96
2016-09-06T20:07:49.460757: step 3532, loss 0.00655052, acc 1
2016-09-06T20:07:50.161834: step 3533, loss 0.0414644, acc 0.96
2016-09-06T20:07:50.833313: step 3534, loss 0.0283644, acc 0.98
2016-09-06T20:07:51.562938: step 3535, loss 0.00173816, acc 1
2016-09-06T20:07:52.254269: step 3536, loss 0.0811184, acc 0.98
2016-09-06T20:07:52.962633: step 3537, loss 0.0214335, acc 1
2016-09-06T20:07:53.641556: step 3538, loss 0.0133781, acc 1
2016-09-06T20:07:54.388499: step 3539, loss 0.0160624, acc 1
2016-09-06T20:07:55.149461: step 3540, loss 0.130093, acc 0.96
2016-09-06T20:07:55.851105: step 3541, loss 0.0977556, acc 0.98
2016-09-06T20:07:56.534618: step 3542, loss 0.0128024, acc 1
2016-09-06T20:07:57.207611: step 3543, loss 0.232342, acc 0.94
2016-09-06T20:07:57.877482: step 3544, loss 0.00336602, acc 1
2016-09-06T20:07:58.574836: step 3545, loss 0.046001, acc 0.98
2016-09-06T20:07:59.239384: step 3546, loss 0.0199991, acc 1
2016-09-06T20:07:59.939651: step 3547, loss 0.0399452, acc 0.98
2016-09-06T20:08:00.639912: step 3548, loss 0.0319228, acc 0.98
2016-09-06T20:08:01.327692: step 3549, loss 0.00409879, acc 1
2016-09-06T20:08:02.037605: step 3550, loss 0.0183976, acc 1
2016-09-06T20:08:02.705503: step 3551, loss 0.0242698, acc 0.98
2016-09-06T20:08:03.419419: step 3552, loss 0.026066, acc 1
2016-09-06T20:08:04.107026: step 3553, loss 0.0383242, acc 0.98
2016-09-06T20:08:04.810046: step 3554, loss 0.037588, acc 1
2016-09-06T20:08:05.502063: step 3555, loss 0.0218951, acc 1
2016-09-06T20:08:06.215373: step 3556, loss 0.104258, acc 0.98
2016-09-06T20:08:06.888401: step 3557, loss 0.0191485, acc 0.98
2016-09-06T20:08:07.587015: step 3558, loss 0.0524553, acc 0.98
2016-09-06T20:08:08.312502: step 3559, loss 0.0233011, acc 1
2016-09-06T20:08:08.986776: step 3560, loss 0.0115714, acc 1
2016-09-06T20:08:09.681399: step 3561, loss 0.0291284, acc 0.98
2016-09-06T20:08:10.373768: step 3562, loss 0.0131837, acc 1
2016-09-06T20:08:11.054396: step 3563, loss 0.0463978, acc 0.98
2016-09-06T20:08:11.766143: step 3564, loss 0.0122696, acc 1
2016-09-06T20:08:12.427850: step 3565, loss 0.00415134, acc 1
2016-09-06T20:08:13.129937: step 3566, loss 0.0457953, acc 0.98
2016-09-06T20:08:13.799599: step 3567, loss 0.10964, acc 0.98
2016-09-06T20:08:14.465887: step 3568, loss 0.0391472, acc 0.98
2016-09-06T20:08:15.175002: step 3569, loss 0.00197774, acc 1
2016-09-06T20:08:15.868669: step 3570, loss 0.0379092, acc 0.98
2016-09-06T20:08:16.559339: step 3571, loss 0.055398, acc 0.98
2016-09-06T20:08:17.239709: step 3572, loss 0.0170119, acc 1
2016-09-06T20:08:17.959632: step 3573, loss 0.0123927, acc 1
2016-09-06T20:08:18.637787: step 3574, loss 0.0201204, acc 1
2016-09-06T20:08:19.309263: step 3575, loss 0.0322511, acc 0.98
2016-09-06T20:08:20.016616: step 3576, loss 0.0628155, acc 0.96
2016-09-06T20:08:20.715195: step 3577, loss 0.0535738, acc 0.98
2016-09-06T20:08:21.403906: step 3578, loss 0.0265693, acc 0.98
2016-09-06T20:08:22.070106: step 3579, loss 0.0194535, acc 1
2016-09-06T20:08:22.758560: step 3580, loss 0.0569626, acc 0.96
2016-09-06T20:08:23.417591: step 3581, loss 0.0655294, acc 0.98
2016-09-06T20:08:24.097188: step 3582, loss 0.0457841, acc 0.98
2016-09-06T20:08:24.784927: step 3583, loss 0.00925936, acc 1
2016-09-06T20:08:25.476201: step 3584, loss 0.0156114, acc 1
2016-09-06T20:08:26.171249: step 3585, loss 0.0306932, acc 1
2016-09-06T20:08:26.839768: step 3586, loss 0.0406699, acc 0.96
2016-09-06T20:08:27.547780: step 3587, loss 0.063828, acc 1
2016-09-06T20:08:28.212569: step 3588, loss 0.0149326, acc 1
2016-09-06T20:08:28.904534: step 3589, loss 0.153634, acc 0.94
2016-09-06T20:08:29.598682: step 3590, loss 0.0175041, acc 1
2016-09-06T20:08:30.286202: step 3591, loss 0.00110254, acc 1
2016-09-06T20:08:30.972885: step 3592, loss 0.0361164, acc 0.96
2016-09-06T20:08:31.653701: step 3593, loss 0.068481, acc 0.96
2016-09-06T20:08:32.365466: step 3594, loss 0.0706662, acc 0.96
2016-09-06T20:08:33.025336: step 3595, loss 0.0692081, acc 0.96
2016-09-06T20:08:33.707193: step 3596, loss 0.0497375, acc 0.98
2016-09-06T20:08:34.404635: step 3597, loss 0.0170831, acc 1
2016-09-06T20:08:35.076257: step 3598, loss 0.0131379, acc 1
2016-09-06T20:08:35.738232: step 3599, loss 0.0197846, acc 1
2016-09-06T20:08:36.447088: step 3600, loss 0.0290949, acc 1

Evaluation:
2016-09-06T20:08:39.597540: step 3600, loss 1.62601, acc 0.745779

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-3600

2016-09-06T20:08:41.224854: step 3601, loss 0.009305, acc 1
2016-09-06T20:08:41.923231: step 3602, loss 0.0187671, acc 0.98
2016-09-06T20:08:42.590234: step 3603, loss 0.0165668, acc 0.98
2016-09-06T20:08:43.283545: step 3604, loss 0.0852817, acc 0.98
2016-09-06T20:08:43.977976: step 3605, loss 0.07454, acc 0.98
2016-09-06T20:08:44.659293: step 3606, loss 0.0301646, acc 1
2016-09-06T20:08:45.353549: step 3607, loss 0.00486562, acc 1
2016-09-06T20:08:46.040653: step 3608, loss 0.0290086, acc 0.98
2016-09-06T20:08:46.757481: step 3609, loss 0.0097065, acc 1
2016-09-06T20:08:47.425164: step 3610, loss 0.072376, acc 0.98
2016-09-06T20:08:48.105234: step 3611, loss 0.0635943, acc 0.96
2016-09-06T20:08:48.819049: step 3612, loss 0.0596146, acc 0.98
2016-09-06T20:08:49.492717: step 3613, loss 0.0237569, acc 0.98
2016-09-06T20:08:50.189372: step 3614, loss 0.032218, acc 1
2016-09-06T20:08:50.891395: step 3615, loss 0.0105406, acc 1
2016-09-06T20:08:51.591304: step 3616, loss 0.0499171, acc 0.98
2016-09-06T20:08:52.273179: step 3617, loss 0.00819544, acc 1
2016-09-06T20:08:52.951469: step 3618, loss 0.0393452, acc 0.98
2016-09-06T20:08:53.653050: step 3619, loss 0.029436, acc 0.98
2016-09-06T20:08:54.338761: step 3620, loss 0.0646044, acc 0.98
2016-09-06T20:08:55.041062: step 3621, loss 0.0742268, acc 0.96
2016-09-06T20:08:55.715739: step 3622, loss 0.112066, acc 0.94
2016-09-06T20:08:56.411055: step 3623, loss 0.0943556, acc 0.98
2016-09-06T20:08:57.114474: step 3624, loss 0.025615, acc 1
2016-09-06T20:08:57.797166: step 3625, loss 0.0213199, acc 1
2016-09-06T20:08:58.488695: step 3626, loss 0.0939035, acc 0.94
2016-09-06T20:08:59.182308: step 3627, loss 0.00574347, acc 1
2016-09-06T20:08:59.894498: step 3628, loss 0.00140138, acc 1
2016-09-06T20:09:00.607546: step 3629, loss 0.0680443, acc 0.98
2016-09-06T20:09:01.316619: step 3630, loss 0.07397, acc 0.94
2016-09-06T20:09:01.990591: step 3631, loss 0.0217787, acc 0.98
2016-09-06T20:09:02.688494: step 3632, loss 0.019997, acc 1
2016-09-06T20:09:03.374152: step 3633, loss 0.0339362, acc 1
2016-09-06T20:09:04.088811: step 3634, loss 0.0377339, acc 1
2016-09-06T20:09:04.799265: step 3635, loss 0.0374567, acc 0.98
2016-09-06T20:09:05.504189: step 3636, loss 0.0367593, acc 0.98
2016-09-06T20:09:06.178176: step 3637, loss 0.00820287, acc 1
2016-09-06T20:09:06.857048: step 3638, loss 0.0125883, acc 1
2016-09-06T20:09:07.527257: step 3639, loss 0.0142404, acc 1
2016-09-06T20:09:08.204670: step 3640, loss 0.0288969, acc 0.98
2016-09-06T20:09:08.896484: step 3641, loss 0.0266028, acc 1
2016-09-06T20:09:09.596167: step 3642, loss 0.00507748, acc 1
2016-09-06T20:09:10.266983: step 3643, loss 0.049258, acc 0.98
2016-09-06T20:09:10.977823: step 3644, loss 0.0181963, acc 1
2016-09-06T20:09:11.667731: step 3645, loss 0.044252, acc 1
2016-09-06T20:09:12.342794: step 3646, loss 0.0424868, acc 0.98
2016-09-06T20:09:13.036232: step 3647, loss 0.0165981, acc 1
2016-09-06T20:09:13.689306: step 3648, loss 0.0272239, acc 0.977273
2016-09-06T20:09:14.410354: step 3649, loss 0.0226497, acc 1
2016-09-06T20:09:15.085254: step 3650, loss 0.0211043, acc 1
2016-09-06T20:09:15.777589: step 3651, loss 0.019332, acc 1
2016-09-06T20:09:16.488794: step 3652, loss 0.0167679, acc 1
2016-09-06T20:09:17.163874: step 3653, loss 0.0134925, acc 1
2016-09-06T20:09:17.838815: step 3654, loss 0.0893241, acc 0.94
2016-09-06T20:09:18.518853: step 3655, loss 0.149909, acc 0.96
2016-09-06T20:09:19.238905: step 3656, loss 0.0472402, acc 0.98
2016-09-06T20:09:19.925471: step 3657, loss 0.0338894, acc 0.98
2016-09-06T20:09:20.631658: step 3658, loss 0.0950393, acc 0.96
2016-09-06T20:09:21.322696: step 3659, loss 0.129411, acc 0.96
2016-09-06T20:09:22.028880: step 3660, loss 0.0066905, acc 1
2016-09-06T20:09:22.721133: step 3661, loss 0.0328575, acc 1
2016-09-06T20:09:23.389323: step 3662, loss 0.00100145, acc 1
2016-09-06T20:09:24.098567: step 3663, loss 0.0157875, acc 0.98
2016-09-06T20:09:24.778713: step 3664, loss 0.0252844, acc 1
2016-09-06T20:09:25.462279: step 3665, loss 0.00907116, acc 1
2016-09-06T20:09:26.146412: step 3666, loss 0.0136518, acc 1
2016-09-06T20:09:26.817907: step 3667, loss 0.0122503, acc 1
2016-09-06T20:09:27.518734: step 3668, loss 0.000599311, acc 1
2016-09-06T20:09:28.199398: step 3669, loss 0.025073, acc 1
2016-09-06T20:09:28.914850: step 3670, loss 0.0252347, acc 0.98
2016-09-06T20:09:29.599602: step 3671, loss 0.00745436, acc 1
2016-09-06T20:09:30.282622: step 3672, loss 0.0437467, acc 0.98
2016-09-06T20:09:30.979121: step 3673, loss 0.0190784, acc 1
2016-09-06T20:09:31.684024: step 3674, loss 0.0433333, acc 0.98
2016-09-06T20:09:32.407260: step 3675, loss 0.0302769, acc 0.98
2016-09-06T20:09:33.083072: step 3676, loss 0.0249586, acc 0.98
2016-09-06T20:09:33.805247: step 3677, loss 0.0154998, acc 1
2016-09-06T20:09:34.489126: step 3678, loss 0.0117631, acc 1
2016-09-06T20:09:35.168935: step 3679, loss 0.00511744, acc 1
2016-09-06T20:09:35.868339: step 3680, loss 0.0218765, acc 0.98
2016-09-06T20:09:36.565924: step 3681, loss 0.00285072, acc 1
2016-09-06T20:09:37.247691: step 3682, loss 0.00368404, acc 1
2016-09-06T20:09:37.919373: step 3683, loss 0.0091428, acc 1
2016-09-06T20:09:38.620483: step 3684, loss 0.079495, acc 0.98
2016-09-06T20:09:39.304090: step 3685, loss 0.0133344, acc 1
2016-09-06T20:09:39.993505: step 3686, loss 0.0367647, acc 0.98
2016-09-06T20:09:40.705048: step 3687, loss 0.016632, acc 0.98
2016-09-06T20:09:41.393723: step 3688, loss 0.0668758, acc 0.98
2016-09-06T20:09:42.097386: step 3689, loss 0.0370442, acc 0.98
2016-09-06T20:09:42.785830: step 3690, loss 0.0452607, acc 0.96
2016-09-06T20:09:43.499714: step 3691, loss 0.0639582, acc 0.98
2016-09-06T20:09:44.197643: step 3692, loss 0.0570698, acc 0.98
2016-09-06T20:09:44.885403: step 3693, loss 0.0803551, acc 0.96
2016-09-06T20:09:45.573931: step 3694, loss 0.0505978, acc 0.98
2016-09-06T20:09:46.237586: step 3695, loss 0.0063246, acc 1
2016-09-06T20:09:46.944464: step 3696, loss 0.00628257, acc 1
2016-09-06T20:09:47.637161: step 3697, loss 0.0415988, acc 0.98
2016-09-06T20:09:48.336941: step 3698, loss 0.00425508, acc 1
2016-09-06T20:09:49.039513: step 3699, loss 0.104495, acc 0.96
2016-09-06T20:09:49.723506: step 3700, loss 0.0195095, acc 0.98

Evaluation:
2016-09-06T20:09:52.898603: step 3700, loss 2.05672, acc 0.742964

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-3700

2016-09-06T20:09:54.618682: step 3701, loss 0.000533192, acc 1
2016-09-06T20:09:55.319041: step 3702, loss 0.112731, acc 0.98
2016-09-06T20:09:55.985512: step 3703, loss 0.0541575, acc 0.96
2016-09-06T20:09:56.659710: step 3704, loss 0.0197413, acc 0.98
2016-09-06T20:09:57.364647: step 3705, loss 0.0518777, acc 0.98
2016-09-06T20:09:58.052699: step 3706, loss 0.0353795, acc 0.98
2016-09-06T20:09:58.738603: step 3707, loss 0.0271303, acc 0.98
2016-09-06T20:09:59.409675: step 3708, loss 0.0187743, acc 0.98
2016-09-06T20:10:00.109036: step 3709, loss 0.00801397, acc 1
2016-09-06T20:10:00.816827: step 3710, loss 0.0357381, acc 0.96
2016-09-06T20:10:01.516812: step 3711, loss 0.0878394, acc 0.98
2016-09-06T20:10:02.217000: step 3712, loss 0.0153973, acc 1
2016-09-06T20:10:02.911003: step 3713, loss 0.0205149, acc 0.98
2016-09-06T20:10:03.584265: step 3714, loss 0.152277, acc 0.98
2016-09-06T20:10:04.261200: step 3715, loss 0.0144366, acc 1
2016-09-06T20:10:04.976946: step 3716, loss 0.0195627, acc 0.98
2016-09-06T20:10:05.638772: step 3717, loss 0.0528801, acc 0.98
2016-09-06T20:10:06.322400: step 3718, loss 0.0278861, acc 0.98
2016-09-06T20:10:07.006541: step 3719, loss 0.00228809, acc 1
2016-09-06T20:10:07.687790: step 3720, loss 0.00772232, acc 1
2016-09-06T20:10:08.372902: step 3721, loss 0.00793389, acc 1
2016-09-06T20:10:09.046297: step 3722, loss 0.0639494, acc 0.98
2016-09-06T20:10:09.786051: step 3723, loss 0.0312769, acc 0.98
2016-09-06T20:10:10.482976: step 3724, loss 0.0263041, acc 1
2016-09-06T20:10:11.164488: step 3725, loss 0.0427, acc 0.98
2016-09-06T20:10:11.852071: step 3726, loss 0.0319125, acc 1
2016-09-06T20:10:12.550265: step 3727, loss 0.021165, acc 0.98
2016-09-06T20:10:13.252397: step 3728, loss 0.0259724, acc 1
2016-09-06T20:10:13.929101: step 3729, loss 0.0798282, acc 0.96
2016-09-06T20:10:14.661768: step 3730, loss 0.026058, acc 1
2016-09-06T20:10:15.353423: step 3731, loss 0.0172786, acc 1
2016-09-06T20:10:16.043660: step 3732, loss 0.0266159, acc 0.98
2016-09-06T20:10:16.734108: step 3733, loss 0.000313328, acc 1
2016-09-06T20:10:17.427164: step 3734, loss 0.0169306, acc 1
2016-09-06T20:10:18.134694: step 3735, loss 0.104458, acc 0.94
2016-09-06T20:10:18.809994: step 3736, loss 0.00715058, acc 1
2016-09-06T20:10:19.493843: step 3737, loss 0.0273604, acc 1
2016-09-06T20:10:20.199312: step 3738, loss 0.0900618, acc 0.98
2016-09-06T20:10:20.890073: step 3739, loss 0.000879166, acc 1
2016-09-06T20:10:21.571147: step 3740, loss 0.056043, acc 0.98
2016-09-06T20:10:22.281142: step 3741, loss 0.0611222, acc 0.98
2016-09-06T20:10:22.968910: step 3742, loss 0.100038, acc 0.96
2016-09-06T20:10:23.637426: step 3743, loss 0.0502441, acc 0.98
2016-09-06T20:10:24.316522: step 3744, loss 0.0216327, acc 0.98
2016-09-06T20:10:24.993378: step 3745, loss 0.0163569, acc 1
2016-09-06T20:10:25.695226: step 3746, loss 0.0162899, acc 1
2016-09-06T20:10:26.381407: step 3747, loss 0.026906, acc 0.98
2016-09-06T20:10:27.072114: step 3748, loss 0.0319135, acc 1
2016-09-06T20:10:27.818487: step 3749, loss 0.05697, acc 0.98
2016-09-06T20:10:28.513232: step 3750, loss 0.0202338, acc 1
2016-09-06T20:10:29.210851: step 3751, loss 0.0243895, acc 0.98
2016-09-06T20:10:29.902620: step 3752, loss 0.0156361, acc 1
2016-09-06T20:10:30.606042: step 3753, loss 0.0170774, acc 1
2016-09-06T20:10:31.291287: step 3754, loss 0.202575, acc 0.94
2016-09-06T20:10:31.971412: step 3755, loss 0.0275321, acc 0.98
2016-09-06T20:10:32.664733: step 3756, loss 0.0233919, acc 1
2016-09-06T20:10:33.355275: step 3757, loss 0.024988, acc 1
2016-09-06T20:10:34.049753: step 3758, loss 0.0126563, acc 1
2016-09-06T20:10:34.750105: step 3759, loss 0.151761, acc 0.96
2016-09-06T20:10:35.429417: step 3760, loss 0.0208016, acc 1
2016-09-06T20:10:36.113681: step 3761, loss 0.0473251, acc 0.96
2016-09-06T20:10:36.801226: step 3762, loss 0.10956, acc 0.96
2016-09-06T20:10:37.545218: step 3763, loss 0.0238846, acc 1
2016-09-06T20:10:38.222290: step 3764, loss 0.0905786, acc 0.94
2016-09-06T20:10:38.919188: step 3765, loss 0.0204836, acc 0.98
2016-09-06T20:10:39.627374: step 3766, loss 0.0130136, acc 1
2016-09-06T20:10:40.308042: step 3767, loss 0.00339877, acc 1
2016-09-06T20:10:41.002017: step 3768, loss 0.0815153, acc 0.96
2016-09-06T20:10:41.703527: step 3769, loss 0.0568756, acc 0.98
2016-09-06T20:10:42.380087: step 3770, loss 0.00105045, acc 1
2016-09-06T20:10:43.038409: step 3771, loss 0.00671197, acc 1
2016-09-06T20:10:43.716152: step 3772, loss 0.043496, acc 0.98
2016-09-06T20:10:44.415221: step 3773, loss 0.0105504, acc 1
2016-09-06T20:10:45.113052: step 3774, loss 0.030668, acc 1
2016-09-06T20:10:45.814729: step 3775, loss 0.0303325, acc 1
2016-09-06T20:10:46.484661: step 3776, loss 0.0237316, acc 0.98
2016-09-06T20:10:47.166781: step 3777, loss 0.0302375, acc 1
2016-09-06T20:10:47.859225: step 3778, loss 0.108672, acc 0.94
2016-09-06T20:10:48.545149: step 3779, loss 0.193969, acc 0.9
2016-09-06T20:10:49.238532: step 3780, loss 0.0213588, acc 1
2016-09-06T20:10:49.934155: step 3781, loss 0.0466031, acc 0.98
2016-09-06T20:10:50.651739: step 3782, loss 0.0274536, acc 0.98
2016-09-06T20:10:51.318289: step 3783, loss 0.044244, acc 0.98
2016-09-06T20:10:52.001340: step 3784, loss 0.035451, acc 0.98
2016-09-06T20:10:52.700802: step 3785, loss 0.019599, acc 0.98
2016-09-06T20:10:53.412742: step 3786, loss 0.0283106, acc 0.98
2016-09-06T20:10:54.106417: step 3787, loss 0.0526635, acc 0.98
2016-09-06T20:10:54.779506: step 3788, loss 0.0452048, acc 0.98
2016-09-06T20:10:55.480288: step 3789, loss 0.0160761, acc 0.98
2016-09-06T20:10:56.151495: step 3790, loss 0.0385465, acc 0.98
2016-09-06T20:10:56.837113: step 3791, loss 0.0840007, acc 0.98
2016-09-06T20:10:57.531758: step 3792, loss 0.0617779, acc 0.96
2016-09-06T20:10:58.212368: step 3793, loss 0.0590771, acc 0.96
2016-09-06T20:10:58.926930: step 3794, loss 0.00890501, acc 1
2016-09-06T20:10:59.595784: step 3795, loss 0.0194871, acc 0.98
2016-09-06T20:11:00.332669: step 3796, loss 0.0760952, acc 0.94
2016-09-06T20:11:01.011217: step 3797, loss 0.0544352, acc 0.98
2016-09-06T20:11:01.724323: step 3798, loss 0.0007551, acc 1
2016-09-06T20:11:02.405586: step 3799, loss 0.0236368, acc 1
2016-09-06T20:11:03.116142: step 3800, loss 0.0359257, acc 1

Evaluation:
2016-09-06T20:11:06.288548: step 3800, loss 1.90863, acc 0.741088

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-3800

2016-09-06T20:11:07.915742: step 3801, loss 0.00282132, acc 1
2016-09-06T20:11:08.614499: step 3802, loss 0.0265514, acc 0.98
2016-09-06T20:11:09.306467: step 3803, loss 0.040635, acc 0.98
2016-09-06T20:11:10.001774: step 3804, loss 0.0120392, acc 1
2016-09-06T20:11:10.689278: step 3805, loss 0.000709014, acc 1
2016-09-06T20:11:11.369338: step 3806, loss 0.0535035, acc 0.96
2016-09-06T20:11:12.065864: step 3807, loss 0.0292572, acc 0.98
2016-09-06T20:11:12.769818: step 3808, loss 0.0778115, acc 0.98
2016-09-06T20:11:13.464633: step 3809, loss 0.035762, acc 0.98
2016-09-06T20:11:14.131204: step 3810, loss 0.0835229, acc 0.98
2016-09-06T20:11:14.810939: step 3811, loss 0.0662067, acc 0.98
2016-09-06T20:11:15.515217: step 3812, loss 0.0162151, acc 0.98
2016-09-06T20:11:16.232062: step 3813, loss 0.0172097, acc 1
2016-09-06T20:11:16.908001: step 3814, loss 0.0387171, acc 0.96
2016-09-06T20:11:17.593197: step 3815, loss 0.0770021, acc 0.96
2016-09-06T20:11:18.292638: step 3816, loss 0.0144974, acc 1
2016-09-06T20:11:18.971450: step 3817, loss 0.0491448, acc 0.96
2016-09-06T20:11:19.636610: step 3818, loss 0.0360693, acc 0.98
2016-09-06T20:11:20.335547: step 3819, loss 0.0112551, acc 1
2016-09-06T20:11:21.031814: step 3820, loss 0.0350835, acc 0.98
2016-09-06T20:11:21.720831: step 3821, loss 0.034487, acc 0.98
2016-09-06T20:11:22.403080: step 3822, loss 0.0344492, acc 0.98
2016-09-06T20:11:23.105261: step 3823, loss 0.0313414, acc 0.98
2016-09-06T20:11:23.781825: step 3824, loss 0.0315997, acc 0.98
2016-09-06T20:11:24.466634: step 3825, loss 0.00753909, acc 1
2016-09-06T20:11:25.164114: step 3826, loss 0.0308582, acc 1
2016-09-06T20:11:25.858655: step 3827, loss 0.0517537, acc 0.98
2016-09-06T20:11:26.554779: step 3828, loss 0.0463378, acc 0.98
2016-09-06T20:11:27.221828: step 3829, loss 0.0106062, acc 1
2016-09-06T20:11:27.932533: step 3830, loss 0.00992001, acc 1
2016-09-06T20:11:28.604457: step 3831, loss 0.00468107, acc 1
2016-09-06T20:11:29.298152: step 3832, loss 0.0807269, acc 0.92
2016-09-06T20:11:29.980550: step 3833, loss 0.0293153, acc 0.98
2016-09-06T20:11:30.666092: step 3834, loss 0.0415527, acc 0.96
2016-09-06T20:11:31.362453: step 3835, loss 0.0744557, acc 0.96
2016-09-06T20:11:32.028371: step 3836, loss 0.0504477, acc 0.98
2016-09-06T20:11:32.728568: step 3837, loss 0.0346602, acc 1
2016-09-06T20:11:33.407812: step 3838, loss 0.00395494, acc 1
2016-09-06T20:11:34.092991: step 3839, loss 0.044856, acc 0.96
2016-09-06T20:11:34.737753: step 3840, loss 0.000181741, acc 1
2016-09-06T20:11:35.421014: step 3841, loss 0.0263633, acc 0.98
2016-09-06T20:11:36.114743: step 3842, loss 0.0212083, acc 1
2016-09-06T20:11:36.791764: step 3843, loss 0.0316121, acc 1
2016-09-06T20:11:37.525359: step 3844, loss 0.0100164, acc 1
2016-09-06T20:11:38.206463: step 3845, loss 0.0247776, acc 1
2016-09-06T20:11:38.921432: step 3846, loss 0.00596395, acc 1
2016-09-06T20:11:39.598445: step 3847, loss 0.0120973, acc 1
2016-09-06T20:11:40.294829: step 3848, loss 0.0468906, acc 0.98
2016-09-06T20:11:40.980814: step 3849, loss 0.0444129, acc 0.98
2016-09-06T20:11:41.652191: step 3850, loss 0.00271362, acc 1
2016-09-06T20:11:42.341812: step 3851, loss 0.0106829, acc 1
2016-09-06T20:11:43.018286: step 3852, loss 0.0125909, acc 1
2016-09-06T20:11:43.701228: step 3853, loss 0.0685583, acc 0.96
2016-09-06T20:11:44.381914: step 3854, loss 0.0537136, acc 0.94
2016-09-06T20:11:45.062633: step 3855, loss 0.103067, acc 0.96
2016-09-06T20:11:45.744711: step 3856, loss 0.0093256, acc 1
2016-09-06T20:11:46.432943: step 3857, loss 0.0683898, acc 0.98
2016-09-06T20:11:47.145772: step 3858, loss 0.00990758, acc 1
2016-09-06T20:11:47.807638: step 3859, loss 0.0124933, acc 1
2016-09-06T20:11:48.469673: step 3860, loss 0.0329007, acc 0.98
2016-09-06T20:11:49.167224: step 3861, loss 0.131068, acc 0.96
2016-09-06T20:11:49.834616: step 3862, loss 0.00135966, acc 1
2016-09-06T20:11:50.508141: step 3863, loss 0.0166221, acc 0.98
2016-09-06T20:11:51.211187: step 3864, loss 0.151803, acc 0.92
2016-09-06T20:11:51.930921: step 3865, loss 0.0259762, acc 1
2016-09-06T20:11:52.593677: step 3866, loss 0.0451915, acc 0.98
2016-09-06T20:11:53.278696: step 3867, loss 0.0852861, acc 0.98
2016-09-06T20:11:53.967193: step 3868, loss 0.0296438, acc 0.98
2016-09-06T20:11:54.661094: step 3869, loss 0.0259697, acc 0.98
2016-09-06T20:11:55.346376: step 3870, loss 0.00773121, acc 1
2016-09-06T20:11:56.036691: step 3871, loss 0.0087804, acc 1
2016-09-06T20:11:56.762861: step 3872, loss 0.0141408, acc 1
2016-09-06T20:11:57.432780: step 3873, loss 0.0282298, acc 0.98
2016-09-06T20:11:58.141353: step 3874, loss 0.0156223, acc 1
2016-09-06T20:11:58.812500: step 3875, loss 0.062035, acc 0.98
2016-09-06T20:11:59.487560: step 3876, loss 0.193764, acc 0.96
2016-09-06T20:12:00.199924: step 3877, loss 0.000910565, acc 1
2016-09-06T20:12:00.887906: step 3878, loss 0.0660753, acc 0.96
2016-09-06T20:12:01.584651: step 3879, loss 0.0383986, acc 0.98
2016-09-06T20:12:02.268057: step 3880, loss 0.0358207, acc 0.98
2016-09-06T20:12:02.951578: step 3881, loss 0.0285482, acc 0.98
2016-09-06T20:12:03.639015: step 3882, loss 0.0324526, acc 0.98
2016-09-06T20:12:04.349114: step 3883, loss 0.0716909, acc 0.98
2016-09-06T20:12:05.025486: step 3884, loss 0.0685968, acc 0.96
2016-09-06T20:12:05.707867: step 3885, loss 0.056895, acc 0.98
2016-09-06T20:12:06.433647: step 3886, loss 0.0285428, acc 0.98
2016-09-06T20:12:07.132361: step 3887, loss 0.00346287, acc 1
2016-09-06T20:12:07.829948: step 3888, loss 0.0122306, acc 1
2016-09-06T20:12:08.538171: step 3889, loss 0.0516061, acc 0.96
2016-09-06T20:12:09.223759: step 3890, loss 0.000729908, acc 1
2016-09-06T20:12:09.909595: step 3891, loss 0.0535952, acc 0.96
2016-09-06T20:12:10.575544: step 3892, loss 0.049885, acc 0.98
2016-09-06T20:12:11.277879: step 3893, loss 0.00808497, acc 1
2016-09-06T20:12:11.941511: step 3894, loss 0.0277671, acc 0.98
2016-09-06T20:12:12.621070: step 3895, loss 0.0262773, acc 1
2016-09-06T20:12:13.298151: step 3896, loss 0.015059, acc 1
2016-09-06T20:12:13.972144: step 3897, loss 0.0126295, acc 1
2016-09-06T20:12:14.654320: step 3898, loss 0.0983094, acc 0.96
2016-09-06T20:12:15.346994: step 3899, loss 0.103629, acc 0.98
2016-09-06T20:12:16.052124: step 3900, loss 0.00279191, acc 1

Evaluation:
2016-09-06T20:12:19.164703: step 3900, loss 1.94076, acc 0.73546

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-3900

2016-09-06T20:12:20.867155: step 3901, loss 0.0833906, acc 0.96
2016-09-06T20:12:21.543614: step 3902, loss 0.0140785, acc 1
2016-09-06T20:12:22.233350: step 3903, loss 0.00853044, acc 1
2016-09-06T20:12:22.905449: step 3904, loss 0.0831907, acc 0.94
2016-09-06T20:12:23.593087: step 3905, loss 0.017887, acc 1
2016-09-06T20:12:24.293680: step 3906, loss 0.0528248, acc 0.96
2016-09-06T20:12:24.978651: step 3907, loss 0.0254378, acc 0.98
2016-09-06T20:12:25.692046: step 3908, loss 0.0398167, acc 0.96
2016-09-06T20:12:26.398077: step 3909, loss 0.0285978, acc 1
2016-09-06T20:12:27.087559: step 3910, loss 0.00978651, acc 1
2016-09-06T20:12:27.789708: step 3911, loss 0.0244335, acc 1
2016-09-06T20:12:28.467135: step 3912, loss 0.0182536, acc 1
2016-09-06T20:12:29.179170: step 3913, loss 0.00418781, acc 1
2016-09-06T20:12:29.848286: step 3914, loss 0.0249224, acc 1
2016-09-06T20:12:30.542869: step 3915, loss 0.00553467, acc 1
2016-09-06T20:12:31.221282: step 3916, loss 0.0774505, acc 0.94
2016-09-06T20:12:31.917586: step 3917, loss 0.0337105, acc 0.98
2016-09-06T20:12:32.616238: step 3918, loss 0.0211669, acc 1
2016-09-06T20:12:33.303697: step 3919, loss 0.0165955, acc 1
2016-09-06T20:12:33.997072: step 3920, loss 0.0308703, acc 0.98
2016-09-06T20:12:34.670387: step 3921, loss 0.0594649, acc 0.98
2016-09-06T20:12:35.369047: step 3922, loss 0.012838, acc 1
2016-09-06T20:12:36.070960: step 3923, loss 0.017454, acc 1
2016-09-06T20:12:36.751079: step 3924, loss 0.0216884, acc 1
2016-09-06T20:12:37.431698: step 3925, loss 0.0397225, acc 0.98
2016-09-06T20:12:38.128288: step 3926, loss 0.0162915, acc 1
2016-09-06T20:12:38.814982: step 3927, loss 0.0832847, acc 0.98
2016-09-06T20:12:39.472366: step 3928, loss 0.00936486, acc 1
2016-09-06T20:12:40.177920: step 3929, loss 0.101441, acc 0.96
2016-09-06T20:12:40.864832: step 3930, loss 0.00793181, acc 1
2016-09-06T20:12:41.538698: step 3931, loss 0.0180113, acc 0.98
2016-09-06T20:12:42.227801: step 3932, loss 0.025958, acc 0.98
2016-09-06T20:12:42.905682: step 3933, loss 0.00887181, acc 1
2016-09-06T20:12:43.615554: step 3934, loss 0.0264096, acc 0.98
2016-09-06T20:12:44.289612: step 3935, loss 0.0251052, acc 0.98
2016-09-06T20:12:44.989434: step 3936, loss 0.0233023, acc 0.98
2016-09-06T20:12:45.675556: step 3937, loss 0.0554189, acc 0.96
2016-09-06T20:12:46.378382: step 3938, loss 0.0125971, acc 1
2016-09-06T20:12:47.063678: step 3939, loss 0.0453452, acc 0.98
2016-09-06T20:12:47.761102: step 3940, loss 0.126334, acc 0.96
2016-09-06T20:12:48.477965: step 3941, loss 0.0318352, acc 1
2016-09-06T20:12:49.151539: step 3942, loss 0.0002323, acc 1
2016-09-06T20:12:49.837586: step 3943, loss 0.0157496, acc 1
2016-09-06T20:12:50.518783: step 3944, loss 0.0652842, acc 0.98
2016-09-06T20:12:51.198745: step 3945, loss 0.0242793, acc 1
2016-09-06T20:12:51.902792: step 3946, loss 0.0115693, acc 1
2016-09-06T20:12:52.583591: step 3947, loss 0.0182615, acc 0.98
2016-09-06T20:12:53.281467: step 3948, loss 0.0288651, acc 0.98
2016-09-06T20:12:53.982480: step 3949, loss 0.0231227, acc 0.98
2016-09-06T20:12:54.680852: step 3950, loss 0.0593505, acc 0.98
2016-09-06T20:12:55.371775: step 3951, loss 0.00181707, acc 1
2016-09-06T20:12:56.065282: step 3952, loss 0.0213859, acc 1
2016-09-06T20:12:56.770585: step 3953, loss 0.00176441, acc 1
2016-09-06T20:12:57.439940: step 3954, loss 0.019564, acc 0.98
2016-09-06T20:12:58.157754: step 3955, loss 0.00515239, acc 1
2016-09-06T20:12:58.837111: step 3956, loss 0.00100093, acc 1
2016-09-06T20:12:59.528340: step 3957, loss 0.0059761, acc 1
2016-09-06T20:13:00.231766: step 3958, loss 0.00713842, acc 1
2016-09-06T20:13:00.906488: step 3959, loss 0.063385, acc 0.96
2016-09-06T20:13:01.597967: step 3960, loss 0.0524703, acc 0.98
2016-09-06T20:13:02.260935: step 3961, loss 0.0464756, acc 0.96
2016-09-06T20:13:02.958457: step 3962, loss 0.0142081, acc 1
2016-09-06T20:13:03.618051: step 3963, loss 0.0238168, acc 0.98
2016-09-06T20:13:04.296081: step 3964, loss 0.0148901, acc 1
2016-09-06T20:13:04.981045: step 3965, loss 0.033391, acc 0.98
2016-09-06T20:13:05.662343: step 3966, loss 0.00165477, acc 1
2016-09-06T20:13:06.365512: step 3967, loss 0.0226607, acc 0.98
2016-09-06T20:13:07.054044: step 3968, loss 0.0738804, acc 0.96
2016-09-06T20:13:07.750887: step 3969, loss 0.000965768, acc 1
2016-09-06T20:13:08.436999: step 3970, loss 0.0469309, acc 0.96
2016-09-06T20:13:09.127218: step 3971, loss 0.160189, acc 0.9
2016-09-06T20:13:09.821535: step 3972, loss 0.063112, acc 0.96
2016-09-06T20:13:10.497331: step 3973, loss 0.0173273, acc 1
2016-09-06T20:13:11.179122: step 3974, loss 0.00214543, acc 1
2016-09-06T20:13:11.860840: step 3975, loss 0.0897121, acc 0.98
2016-09-06T20:13:12.574907: step 3976, loss 0.0134977, acc 1
2016-09-06T20:13:13.237408: step 3977, loss 0.00452447, acc 1
2016-09-06T20:13:13.909537: step 3978, loss 0.00419177, acc 1
2016-09-06T20:13:14.593749: step 3979, loss 0.0858348, acc 0.98
2016-09-06T20:13:15.269030: step 3980, loss 0.0191882, acc 0.98
2016-09-06T20:13:15.935267: step 3981, loss 0.00903481, acc 1
2016-09-06T20:13:16.620526: step 3982, loss 0.00341558, acc 1
2016-09-06T20:13:17.323588: step 3983, loss 0.00478563, acc 1
2016-09-06T20:13:17.996940: step 3984, loss 0.0784268, acc 0.98
2016-09-06T20:13:18.710945: step 3985, loss 0.0414207, acc 0.98
2016-09-06T20:13:19.404912: step 3986, loss 0.0335522, acc 0.98
2016-09-06T20:13:20.103845: step 3987, loss 0.00621519, acc 1
2016-09-06T20:13:20.801940: step 3988, loss 0.040209, acc 1
2016-09-06T20:13:21.479669: step 3989, loss 0.105459, acc 0.96
2016-09-06T20:13:22.180275: step 3990, loss 0.00353283, acc 1
2016-09-06T20:13:22.865286: step 3991, loss 0.038685, acc 0.98
2016-09-06T20:13:23.551455: step 3992, loss 0.0044922, acc 1
2016-09-06T20:13:24.256917: step 3993, loss 0.0215172, acc 1
2016-09-06T20:13:24.936312: step 3994, loss 0.0166331, acc 1
2016-09-06T20:13:25.623157: step 3995, loss 0.00254786, acc 1
2016-09-06T20:13:26.282014: step 3996, loss 0.00286928, acc 1
2016-09-06T20:13:26.978517: step 3997, loss 0.0201726, acc 1
2016-09-06T20:13:27.643051: step 3998, loss 0.0280074, acc 1
2016-09-06T20:13:28.365742: step 3999, loss 0.0179606, acc 0.98
2016-09-06T20:13:29.046615: step 4000, loss 0.0221076, acc 0.98

Evaluation:
2016-09-06T20:13:32.186516: step 4000, loss 2.31261, acc 0.73546

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-4000

2016-09-06T20:13:33.962874: step 4001, loss 0.00379485, acc 1
2016-09-06T20:13:34.670614: step 4002, loss 0.0913765, acc 0.96
2016-09-06T20:13:35.373558: step 4003, loss 0.0113569, acc 1
2016-09-06T20:13:36.029402: step 4004, loss 0.0459095, acc 0.98
2016-09-06T20:13:36.737359: step 4005, loss 0.00735768, acc 1
2016-09-06T20:13:37.417685: step 4006, loss 0.0162249, acc 1
2016-09-06T20:13:38.115745: step 4007, loss 0.00108452, acc 1
2016-09-06T20:13:38.800203: step 4008, loss 0.034115, acc 0.98
2016-09-06T20:13:39.459279: step 4009, loss 0.0101373, acc 1
2016-09-06T20:13:40.165120: step 4010, loss 0.00260983, acc 1
2016-09-06T20:13:40.837886: step 4011, loss 0.0564595, acc 0.98
2016-09-06T20:13:41.539985: step 4012, loss 0.0290398, acc 0.98
2016-09-06T20:13:42.225004: step 4013, loss 0.0267958, acc 1
2016-09-06T20:13:42.921651: step 4014, loss 0.0927102, acc 0.94
2016-09-06T20:13:43.593921: step 4015, loss 0.0233358, acc 0.98
2016-09-06T20:13:44.275807: step 4016, loss 0.0524611, acc 0.98
2016-09-06T20:13:44.996709: step 4017, loss 0.0262208, acc 0.98
2016-09-06T20:13:45.683416: step 4018, loss 0.034536, acc 1
2016-09-06T20:13:46.379415: step 4019, loss 0.0650333, acc 0.96
2016-09-06T20:13:47.074220: step 4020, loss 0.113939, acc 0.96
2016-09-06T20:13:47.768676: step 4021, loss 0.00127997, acc 1
2016-09-06T20:13:48.453869: step 4022, loss 0.0150327, acc 0.98
2016-09-06T20:13:49.130623: step 4023, loss 0.0257488, acc 0.98
2016-09-06T20:13:49.837267: step 4024, loss 0.0236753, acc 1
2016-09-06T20:13:50.541798: step 4025, loss 0.0363386, acc 0.98
2016-09-06T20:13:51.221909: step 4026, loss 0.0146707, acc 1
2016-09-06T20:13:51.904340: step 4027, loss 0.0522069, acc 0.98
2016-09-06T20:13:52.587997: step 4028, loss 0.0066314, acc 1
2016-09-06T20:13:53.281396: step 4029, loss 0.0441348, acc 0.96
2016-09-06T20:13:53.958446: step 4030, loss 0.0667907, acc 0.98
2016-09-06T20:13:54.660905: step 4031, loss 0.0346389, acc 0.98
2016-09-06T20:13:55.298768: step 4032, loss 0.00549394, acc 1
2016-09-06T20:13:55.990133: step 4033, loss 0.0317199, acc 0.96
2016-09-06T20:13:56.677901: step 4034, loss 0.0973565, acc 0.96
2016-09-06T20:13:57.357597: step 4035, loss 0.0512674, acc 0.96
2016-09-06T20:13:58.054298: step 4036, loss 0.0187275, acc 0.98
2016-09-06T20:13:58.748904: step 4037, loss 0.0238897, acc 1
2016-09-06T20:13:59.476655: step 4038, loss 0.0313926, acc 1
2016-09-06T20:14:00.161394: step 4039, loss 0.00104878, acc 1
2016-09-06T20:14:00.881141: step 4040, loss 0.0121496, acc 1
2016-09-06T20:14:01.576355: step 4041, loss 0.00284994, acc 1
2016-09-06T20:14:02.277477: step 4042, loss 0.0149795, acc 1
2016-09-06T20:14:02.991358: step 4043, loss 0.0193523, acc 0.98
2016-09-06T20:14:03.659484: step 4044, loss 0.0217555, acc 1
2016-09-06T20:14:04.377209: step 4045, loss 0.0482199, acc 0.98
2016-09-06T20:14:05.068035: step 4046, loss 0.0286145, acc 1
2016-09-06T20:14:05.769038: step 4047, loss 0.00122657, acc 1
2016-09-06T20:14:06.467682: step 4048, loss 0.00518845, acc 1
2016-09-06T20:14:07.154279: step 4049, loss 0.0517509, acc 0.94
2016-09-06T20:14:07.866481: step 4050, loss 0.0183131, acc 0.98
2016-09-06T20:14:08.555538: step 4051, loss 0.0340482, acc 0.98
2016-09-06T20:14:09.259487: step 4052, loss 0.0423007, acc 0.98
2016-09-06T20:14:09.953691: step 4053, loss 0.000778209, acc 1
2016-09-06T20:14:10.635058: step 4054, loss 0.0130224, acc 1
2016-09-06T20:14:11.321455: step 4055, loss 0.0124036, acc 1
2016-09-06T20:14:11.990201: step 4056, loss 0.0723896, acc 0.98
2016-09-06T20:14:12.729537: step 4057, loss 0.0150645, acc 0.98
2016-09-06T20:14:13.419708: step 4058, loss 0.0497589, acc 0.96
2016-09-06T20:14:14.102525: step 4059, loss 0.00397218, acc 1
2016-09-06T20:14:14.773886: step 4060, loss 0.0265665, acc 0.98
2016-09-06T20:14:15.462828: step 4061, loss 0.0399584, acc 1
2016-09-06T20:14:16.151663: step 4062, loss 0.112455, acc 0.94
2016-09-06T20:14:16.832263: step 4063, loss 0.0338727, acc 0.98
2016-09-06T20:14:17.537882: step 4064, loss 0.0125939, acc 1
2016-09-06T20:14:18.241434: step 4065, loss 0.000333566, acc 1
2016-09-06T20:14:18.942267: step 4066, loss 0.0351005, acc 0.98
2016-09-06T20:14:19.628332: step 4067, loss 0.0177385, acc 1
2016-09-06T20:14:20.335414: step 4068, loss 0.0579022, acc 0.94
2016-09-06T20:14:21.026584: step 4069, loss 8.12576e-05, acc 1
2016-09-06T20:14:21.697839: step 4070, loss 0.018713, acc 0.98
2016-09-06T20:14:22.423248: step 4071, loss 0.00961897, acc 1
2016-09-06T20:14:23.108799: step 4072, loss 0.051029, acc 0.96
2016-09-06T20:14:23.808297: step 4073, loss 0.0232707, acc 1
2016-09-06T20:14:24.492652: step 4074, loss 0.0958472, acc 0.96
2016-09-06T20:14:25.201211: step 4075, loss 0.0534608, acc 1
2016-09-06T20:14:25.890908: step 4076, loss 0.016675, acc 1
2016-09-06T20:14:26.570996: step 4077, loss 0.013378, acc 1
2016-09-06T20:14:27.241721: step 4078, loss 0.079032, acc 0.98
2016-09-06T20:14:27.935859: step 4079, loss 0.0291802, acc 1
2016-09-06T20:14:28.650484: step 4080, loss 0.0119392, acc 1
2016-09-06T20:14:29.332032: step 4081, loss 0.0278983, acc 0.98
2016-09-06T20:14:30.050224: step 4082, loss 0.0314541, acc 0.98
2016-09-06T20:14:30.753299: step 4083, loss 0.0147211, acc 1
2016-09-06T20:14:31.416816: step 4084, loss 0.0142001, acc 1
2016-09-06T20:14:32.093846: step 4085, loss 0.0113821, acc 1
2016-09-06T20:14:32.778156: step 4086, loss 0.0380956, acc 0.98
2016-09-06T20:14:33.459547: step 4087, loss 0.0430893, acc 0.98
2016-09-06T20:14:34.133574: step 4088, loss 0.0375241, acc 0.96
2016-09-06T20:14:34.819801: step 4089, loss 0.0378761, acc 0.96
2016-09-06T20:14:35.533538: step 4090, loss 0.112067, acc 0.96
2016-09-06T20:14:36.185136: step 4091, loss 0.0234884, acc 1
2016-09-06T20:14:36.850819: step 4092, loss 0.00307992, acc 1
2016-09-06T20:14:37.532789: step 4093, loss 0.0304144, acc 0.98
2016-09-06T20:14:38.207532: step 4094, loss 0.0413844, acc 0.98
2016-09-06T20:14:38.871558: step 4095, loss 0.0532092, acc 0.96
2016-09-06T20:14:39.550260: step 4096, loss 0.0148342, acc 1
2016-09-06T20:14:40.235604: step 4097, loss 0.0223487, acc 1
2016-09-06T20:14:40.891539: step 4098, loss 0.0291486, acc 0.98
2016-09-06T20:14:41.609576: step 4099, loss 0.0302401, acc 1
2016-09-06T20:14:42.289426: step 4100, loss 0.0271264, acc 1

Evaluation:
2016-09-06T20:14:45.418071: step 4100, loss 2.27585, acc 0.724203

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-4100

2016-09-06T20:14:47.155518: step 4101, loss 0.0292432, acc 0.98
2016-09-06T20:14:47.821781: step 4102, loss 0.0792498, acc 0.96
2016-09-06T20:14:48.497531: step 4103, loss 0.0245151, acc 1
2016-09-06T20:14:49.182330: step 4104, loss 0.101287, acc 0.98
2016-09-06T20:14:49.878130: step 4105, loss 0.120735, acc 0.96
2016-09-06T20:14:50.539551: step 4106, loss 0.0223319, acc 0.98
2016-09-06T20:14:51.257449: step 4107, loss 0.0295679, acc 0.96
2016-09-06T20:14:51.970801: step 4108, loss 0.0437855, acc 1
2016-09-06T20:14:52.651333: step 4109, loss 0.0630563, acc 0.98
2016-09-06T20:14:53.327106: step 4110, loss 0.0453945, acc 0.98
2016-09-06T20:14:54.019799: step 4111, loss 0.023389, acc 1
2016-09-06T20:14:54.723961: step 4112, loss 0.00700377, acc 1
2016-09-06T20:14:55.388612: step 4113, loss 0.0766655, acc 0.98
2016-09-06T20:14:56.072793: step 4114, loss 0.0311261, acc 0.98
2016-09-06T20:14:56.766531: step 4115, loss 0.022269, acc 0.98
2016-09-06T20:14:57.449876: step 4116, loss 0.00590677, acc 1
2016-09-06T20:14:58.137079: step 4117, loss 0.0129017, acc 1
2016-09-06T20:14:58.806919: step 4118, loss 0.0297861, acc 0.98
2016-09-06T20:14:59.487949: step 4119, loss 0.0804806, acc 0.92
2016-09-06T20:15:00.139452: step 4120, loss 0.00352814, acc 1
2016-09-06T20:15:00.877801: step 4121, loss 0.0507492, acc 0.98
2016-09-06T20:15:01.573424: step 4122, loss 0.069371, acc 0.96
2016-09-06T20:15:02.253098: step 4123, loss 0.00253944, acc 1
2016-09-06T20:15:02.971300: step 4124, loss 0.0156569, acc 1
2016-09-06T20:15:03.648423: step 4125, loss 0.0196677, acc 0.98
2016-09-06T20:15:04.341401: step 4126, loss 0.012086, acc 1
2016-09-06T20:15:05.010257: step 4127, loss 0.0455877, acc 0.98
2016-09-06T20:15:05.718849: step 4128, loss 0.0166539, acc 1
2016-09-06T20:15:06.407025: step 4129, loss 0.025029, acc 1
2016-09-06T20:15:07.142391: step 4130, loss 0.0171423, acc 0.98
2016-09-06T20:15:07.829080: step 4131, loss 0.0458098, acc 0.98
2016-09-06T20:15:08.538277: step 4132, loss 0.013122, acc 1
2016-09-06T20:15:09.247084: step 4133, loss 0.0638709, acc 0.98
2016-09-06T20:15:09.939937: step 4134, loss 0.00824968, acc 1
2016-09-06T20:15:10.632724: step 4135, loss 0.00191185, acc 1
2016-09-06T20:15:11.309621: step 4136, loss 0.0744331, acc 0.98
2016-09-06T20:15:11.997377: step 4137, loss 0.0547197, acc 0.96
2016-09-06T20:15:12.692793: step 4138, loss 0.0524508, acc 0.98
2016-09-06T20:15:13.366977: step 4139, loss 0.00994484, acc 1
2016-09-06T20:15:14.066886: step 4140, loss 0.00828043, acc 1
2016-09-06T20:15:14.740976: step 4141, loss 0.0388722, acc 0.98
2016-09-06T20:15:15.412359: step 4142, loss 0.0121666, acc 1
2016-09-06T20:15:16.097513: step 4143, loss 0.00737921, acc 1
2016-09-06T20:15:16.784751: step 4144, loss 0.0052963, acc 1
2016-09-06T20:15:17.477067: step 4145, loss 0.0847183, acc 0.94
2016-09-06T20:15:18.165787: step 4146, loss 0.0297155, acc 1
2016-09-06T20:15:18.880648: step 4147, loss 0.0372961, acc 0.96
2016-09-06T20:15:19.559564: step 4148, loss 0.00606686, acc 1
2016-09-06T20:15:20.261644: step 4149, loss 0.0248329, acc 0.98
2016-09-06T20:15:20.968765: step 4150, loss 0.00963025, acc 1
2016-09-06T20:15:21.657530: step 4151, loss 0.0489976, acc 0.96
2016-09-06T20:15:22.347164: step 4152, loss 0.0340072, acc 0.98
2016-09-06T20:15:22.999034: step 4153, loss 0.0293207, acc 0.98
2016-09-06T20:15:23.723873: step 4154, loss 0.01539, acc 1
2016-09-06T20:15:24.395279: step 4155, loss 0.00990079, acc 1
2016-09-06T20:15:25.072319: step 4156, loss 0.0227818, acc 0.98
2016-09-06T20:15:25.741926: step 4157, loss 0.0148778, acc 1
2016-09-06T20:15:26.424854: step 4158, loss 0.0429074, acc 0.98
2016-09-06T20:15:27.097543: step 4159, loss 0.0207681, acc 1
2016-09-06T20:15:27.762483: step 4160, loss 0.0173922, acc 1
2016-09-06T20:15:28.471356: step 4161, loss 0.00538353, acc 1
2016-09-06T20:15:29.153103: step 4162, loss 0.0197743, acc 1
2016-09-06T20:15:29.840044: step 4163, loss 0.0498405, acc 0.98
2016-09-06T20:15:30.523227: step 4164, loss 0.0115803, acc 1
2016-09-06T20:15:31.203703: step 4165, loss 0.0819109, acc 0.92
2016-09-06T20:15:31.880622: step 4166, loss 0.0012647, acc 1
2016-09-06T20:15:32.573238: step 4167, loss 3.82944e-05, acc 1
2016-09-06T20:15:33.262348: step 4168, loss 0.001184, acc 1
2016-09-06T20:15:33.934125: step 4169, loss 0.138929, acc 0.96
2016-09-06T20:15:34.610491: step 4170, loss 0.000177258, acc 1
2016-09-06T20:15:35.285035: step 4171, loss 0.0150857, acc 1
2016-09-06T20:15:35.974709: step 4172, loss 0.0319388, acc 0.98
2016-09-06T20:15:36.646706: step 4173, loss 0.00833139, acc 1
2016-09-06T20:15:37.325997: step 4174, loss 0.000468871, acc 1
2016-09-06T20:15:38.047471: step 4175, loss 0.00829651, acc 1
2016-09-06T20:15:38.717363: step 4176, loss 0.0710876, acc 0.96
2016-09-06T20:15:39.406397: step 4177, loss 0.117664, acc 0.94
2016-09-06T20:15:40.077488: step 4178, loss 0.0353913, acc 0.98
2016-09-06T20:15:40.758512: step 4179, loss 0.126621, acc 0.94
2016-09-06T20:15:41.445477: step 4180, loss 0.0746656, acc 0.98
2016-09-06T20:15:42.133646: step 4181, loss 0.0419109, acc 0.98
2016-09-06T20:15:42.841142: step 4182, loss 0.0250835, acc 0.98
2016-09-06T20:15:43.522941: step 4183, loss 0.0338879, acc 0.96
2016-09-06T20:15:44.210928: step 4184, loss 0.024683, acc 0.98
2016-09-06T20:15:44.921858: step 4185, loss 0.0350786, acc 0.98
2016-09-06T20:15:45.610759: step 4186, loss 0.0088162, acc 1
2016-09-06T20:15:46.299220: step 4187, loss 0.0104555, acc 1
2016-09-06T20:15:47.018927: step 4188, loss 0.00123817, acc 1
2016-09-06T20:15:47.716890: step 4189, loss 0.0707987, acc 0.98
2016-09-06T20:15:48.371365: step 4190, loss 0.0323672, acc 0.98
2016-09-06T20:15:49.059339: step 4191, loss 0.0360405, acc 0.98
2016-09-06T20:15:49.745892: step 4192, loss 0.0196493, acc 1
2016-09-06T20:15:50.429281: step 4193, loss 0.0316043, acc 0.98
2016-09-06T20:15:51.116061: step 4194, loss 0.0123275, acc 1
2016-09-06T20:15:51.801992: step 4195, loss 0.0137395, acc 1
2016-09-06T20:15:52.524446: step 4196, loss 0.00484022, acc 1
2016-09-06T20:15:53.190614: step 4197, loss 0.0475294, acc 0.98
2016-09-06T20:15:53.868486: step 4198, loss 0.0443896, acc 0.96
2016-09-06T20:15:54.571687: step 4199, loss 0.0794851, acc 0.98
2016-09-06T20:15:55.270185: step 4200, loss 0.0293015, acc 0.98

Evaluation:
2016-09-06T20:15:58.420503: step 4200, loss 2.10299, acc 0.724203

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-4200

2016-09-06T20:16:00.114333: step 4201, loss 0.0301115, acc 0.98
2016-09-06T20:16:00.842222: step 4202, loss 0.00396132, acc 1
2016-09-06T20:16:01.512394: step 4203, loss 0.0523398, acc 0.98
2016-09-06T20:16:02.224179: step 4204, loss 0.0886681, acc 0.98
2016-09-06T20:16:02.910882: step 4205, loss 0.0447611, acc 0.98
2016-09-06T20:16:03.598942: step 4206, loss 0.0445629, acc 0.98
2016-09-06T20:16:04.279700: step 4207, loss 0.0256068, acc 1
2016-09-06T20:16:04.954571: step 4208, loss 0.0269338, acc 0.98
2016-09-06T20:16:05.630976: step 4209, loss 0.0339917, acc 0.96
2016-09-06T20:16:06.290888: step 4210, loss 0.0745796, acc 0.96
2016-09-06T20:16:06.988232: step 4211, loss 0.00158269, acc 1
2016-09-06T20:16:07.647713: step 4212, loss 0.00713011, acc 1
2016-09-06T20:16:08.337041: step 4213, loss 0.00797318, acc 1
2016-09-06T20:16:09.022408: step 4214, loss 0.0214472, acc 1
2016-09-06T20:16:09.712855: step 4215, loss 0.000861662, acc 1
2016-09-06T20:16:10.422116: step 4216, loss 0.063522, acc 0.96
2016-09-06T20:16:11.087612: step 4217, loss 0.0844797, acc 0.96
2016-09-06T20:16:11.808778: step 4218, loss 0.0164356, acc 1
2016-09-06T20:16:12.502684: step 4219, loss 0.0370723, acc 0.98
2016-09-06T20:16:13.207301: step 4220, loss 0.00148327, acc 1
2016-09-06T20:16:13.901747: step 4221, loss 0.0281374, acc 0.98
2016-09-06T20:16:14.588803: step 4222, loss 0.00575122, acc 1
2016-09-06T20:16:15.284834: step 4223, loss 0.0484833, acc 0.98
2016-09-06T20:16:15.905696: step 4224, loss 0.0188219, acc 1
2016-09-06T20:16:16.607911: step 4225, loss 0.0100474, acc 1
2016-09-06T20:16:17.271867: step 4226, loss 0.0475857, acc 1
2016-09-06T20:16:17.950295: step 4227, loss 0.0296391, acc 0.98
2016-09-06T20:16:18.638976: step 4228, loss 0.0254473, acc 0.98
2016-09-06T20:16:19.323009: step 4229, loss 0.01721, acc 1
2016-09-06T20:16:20.020711: step 4230, loss 0.0225903, acc 0.98
2016-09-06T20:16:20.681837: step 4231, loss 0.0155949, acc 1
2016-09-06T20:16:21.385988: step 4232, loss 0.0236356, acc 0.98
2016-09-06T20:16:22.064211: step 4233, loss 0.00233044, acc 1
2016-09-06T20:16:22.734176: step 4234, loss 0.0230889, acc 0.98
2016-09-06T20:16:23.421411: step 4235, loss 0.0310598, acc 0.98
2016-09-06T20:16:24.104190: step 4236, loss 0.016586, acc 1
2016-09-06T20:16:24.792258: step 4237, loss 0.0850443, acc 0.96
2016-09-06T20:16:25.493106: step 4238, loss 0.0364368, acc 1
2016-09-06T20:16:26.202519: step 4239, loss 0.016576, acc 1
2016-09-06T20:16:26.878052: step 4240, loss 0.00208582, acc 1
2016-09-06T20:16:27.565822: step 4241, loss 0.0184601, acc 0.98
2016-09-06T20:16:28.261822: step 4242, loss 0.026037, acc 0.98
2016-09-06T20:16:28.970152: step 4243, loss 0.0209937, acc 1
2016-09-06T20:16:29.648649: step 4244, loss 0.00844362, acc 1
2016-09-06T20:16:30.323809: step 4245, loss 0.011749, acc 1
2016-09-06T20:16:31.035801: step 4246, loss 0.00129885, acc 1
2016-09-06T20:16:31.709014: step 4247, loss 0.0167793, acc 0.98
2016-09-06T20:16:32.394466: step 4248, loss 0.0390725, acc 1
2016-09-06T20:16:33.086832: step 4249, loss 0.00526215, acc 1
2016-09-06T20:16:33.783830: step 4250, loss 0.0531227, acc 0.98
2016-09-06T20:16:34.495744: step 4251, loss 0.00336037, acc 1
2016-09-06T20:16:35.167619: step 4252, loss 0.0451337, acc 0.96
2016-09-06T20:16:35.869389: step 4253, loss 0.00882428, acc 1
2016-09-06T20:16:36.547233: step 4254, loss 0.00926359, acc 1
2016-09-06T20:16:37.238046: step 4255, loss 0.00228039, acc 1
2016-09-06T20:16:37.950222: step 4256, loss 0.0136312, acc 1
2016-09-06T20:16:38.642059: step 4257, loss 0.0501316, acc 0.96
2016-09-06T20:16:39.372604: step 4258, loss 0.0408242, acc 0.96
2016-09-06T20:16:40.047893: step 4259, loss 0.0104252, acc 1
2016-09-06T20:16:40.728683: step 4260, loss 0.0215408, acc 1
2016-09-06T20:16:41.406794: step 4261, loss 0.000672902, acc 1
2016-09-06T20:16:42.089404: step 4262, loss 0.0472732, acc 0.98
2016-09-06T20:16:42.768929: step 4263, loss 0.000544235, acc 1
2016-09-06T20:16:43.458352: step 4264, loss 0.0206637, acc 0.98
2016-09-06T20:16:44.163004: step 4265, loss 0.0530916, acc 0.98
2016-09-06T20:16:44.815715: step 4266, loss 0.0173109, acc 0.98
2016-09-06T20:16:45.506546: step 4267, loss 0.0332713, acc 0.98
2016-09-06T20:16:46.208974: step 4268, loss 0.0122331, acc 1
2016-09-06T20:16:46.879656: step 4269, loss 0.00172528, acc 1
2016-09-06T20:16:47.548433: step 4270, loss 0.0165835, acc 0.98
2016-09-06T20:16:48.234278: step 4271, loss 0.0136608, acc 1
2016-09-06T20:16:48.938229: step 4272, loss 0.0301779, acc 0.98
2016-09-06T20:16:49.606826: step 4273, loss 0.0202626, acc 0.98
2016-09-06T20:16:50.300039: step 4274, loss 0.0693715, acc 0.98
2016-09-06T20:16:50.970160: step 4275, loss 0.0108958, acc 1
2016-09-06T20:16:51.662585: step 4276, loss 0.000146354, acc 1
2016-09-06T20:16:52.355167: step 4277, loss 0.000679613, acc 1
2016-09-06T20:16:53.044108: step 4278, loss 0.143514, acc 0.96
2016-09-06T20:16:53.743171: step 4279, loss 0.0235342, acc 1
2016-09-06T20:16:54.495867: step 4280, loss 0.00112913, acc 1
2016-09-06T20:16:55.177585: step 4281, loss 0.00404083, acc 1
2016-09-06T20:16:55.895897: step 4282, loss 0.0470757, acc 0.96
2016-09-06T20:16:56.597911: step 4283, loss 0.0415249, acc 1
2016-09-06T20:16:57.286235: step 4284, loss 0.112353, acc 0.96
2016-09-06T20:16:57.945384: step 4285, loss 0.00483935, acc 1
2016-09-06T20:16:58.657667: step 4286, loss 0.063445, acc 0.96
2016-09-06T20:16:59.325674: step 4287, loss 0.0112006, acc 1
2016-09-06T20:17:00.011347: step 4288, loss 0.00617505, acc 1
2016-09-06T20:17:00.741499: step 4289, loss 0.000119984, acc 1
2016-09-06T20:17:01.426376: step 4290, loss 0.00603555, acc 1
2016-09-06T20:17:02.123396: step 4291, loss 0.00599322, acc 1
2016-09-06T20:17:02.784168: step 4292, loss 0.0444898, acc 0.96
2016-09-06T20:17:03.469244: step 4293, loss 0.0528341, acc 0.98
2016-09-06T20:17:04.161162: step 4294, loss 0.00144281, acc 1
2016-09-06T20:17:04.835040: step 4295, loss 0.0345315, acc 0.98
2016-09-06T20:17:05.507703: step 4296, loss 0.0141518, acc 1
2016-09-06T20:17:06.207021: step 4297, loss 0.0101261, acc 1
2016-09-06T20:17:06.891572: step 4298, loss 0.0310893, acc 0.98
2016-09-06T20:17:07.556204: step 4299, loss 0.018812, acc 1
2016-09-06T20:17:08.247446: step 4300, loss 0.0503824, acc 0.96

Evaluation:
2016-09-06T20:17:11.379884: step 4300, loss 2.20108, acc 0.738274

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-4300

2016-09-06T20:17:13.108735: step 4301, loss 0.0366945, acc 0.98
2016-09-06T20:17:13.787310: step 4302, loss 0.0359511, acc 0.98
2016-09-06T20:17:14.470649: step 4303, loss 0.0108008, acc 1
2016-09-06T20:17:15.172900: step 4304, loss 0.0559209, acc 0.98
2016-09-06T20:17:15.842899: step 4305, loss 0.0101261, acc 1
2016-09-06T20:17:16.539635: step 4306, loss 0.0160248, acc 1
2016-09-06T20:17:17.214247: step 4307, loss 0.0299204, acc 0.98
2016-09-06T20:17:17.908625: step 4308, loss 0.0170649, acc 1
2016-09-06T20:17:18.588950: step 4309, loss 0.00323034, acc 1
2016-09-06T20:17:19.289458: step 4310, loss 0.208441, acc 0.98
2016-09-06T20:17:19.982783: step 4311, loss 0.0441146, acc 0.96
2016-09-06T20:17:20.663026: step 4312, loss 0.086508, acc 0.98
2016-09-06T20:17:21.344694: step 4313, loss 0.0173422, acc 1
2016-09-06T20:17:22.012080: step 4314, loss 0.004291, acc 1
2016-09-06T20:17:22.729280: step 4315, loss 0.0242358, acc 0.98
2016-09-06T20:17:23.394917: step 4316, loss 0.0664184, acc 0.96
2016-09-06T20:17:24.068824: step 4317, loss 0.00625424, acc 1
2016-09-06T20:17:24.740539: step 4318, loss 0.038343, acc 0.98
2016-09-06T20:17:25.424796: step 4319, loss 0.0283412, acc 0.98
2016-09-06T20:17:26.111706: step 4320, loss 0.0140375, acc 1
2016-09-06T20:17:26.802298: step 4321, loss 0.0167659, acc 1
2016-09-06T20:17:27.489924: step 4322, loss 0.0236424, acc 0.98
2016-09-06T20:17:28.162395: step 4323, loss 0.0284316, acc 1
2016-09-06T20:17:28.895249: step 4324, loss 0.0203292, acc 0.98
2016-09-06T20:17:29.594574: step 4325, loss 0.0254445, acc 0.98
2016-09-06T20:17:30.274701: step 4326, loss 0.009706, acc 1
2016-09-06T20:17:30.974678: step 4327, loss 0.151213, acc 0.94
2016-09-06T20:17:31.697961: step 4328, loss 0.00364226, acc 1
2016-09-06T20:17:32.401444: step 4329, loss 0.165399, acc 0.92
2016-09-06T20:17:33.071576: step 4330, loss 0.069012, acc 0.96
2016-09-06T20:17:33.747451: step 4331, loss 0.00157136, acc 1
2016-09-06T20:17:34.425280: step 4332, loss 0.01245, acc 1
2016-09-06T20:17:35.106724: step 4333, loss 0.0492208, acc 0.98
2016-09-06T20:17:35.789836: step 4334, loss 0.0342545, acc 0.96
2016-09-06T20:17:36.461323: step 4335, loss 0.0223463, acc 0.98
2016-09-06T20:17:37.163919: step 4336, loss 0.00796424, acc 1
2016-09-06T20:17:37.864848: step 4337, loss 0.0271896, acc 0.98
2016-09-06T20:17:38.564433: step 4338, loss 0.0403864, acc 0.98
2016-09-06T20:17:39.275022: step 4339, loss 0.03346, acc 0.98
2016-09-06T20:17:39.967327: step 4340, loss 0.0398438, acc 0.98
2016-09-06T20:17:40.655995: step 4341, loss 0.0163362, acc 0.98
2016-09-06T20:17:41.334231: step 4342, loss 0.00950371, acc 1
2016-09-06T20:17:42.045379: step 4343, loss 0.0122055, acc 1
2016-09-06T20:17:42.745812: step 4344, loss 0.0598455, acc 0.96
2016-09-06T20:17:43.428596: step 4345, loss 0.0506829, acc 0.96
2016-09-06T20:17:44.135008: step 4346, loss 0.0323299, acc 0.98
2016-09-06T20:17:44.822918: step 4347, loss 0.0512639, acc 0.96
2016-09-06T20:17:45.526929: step 4348, loss 0.0353343, acc 0.96
2016-09-06T20:17:46.201900: step 4349, loss 0.00982795, acc 1
2016-09-06T20:17:46.925756: step 4350, loss 0.0357243, acc 1
2016-09-06T20:17:47.631285: step 4351, loss 0.0526791, acc 0.96
2016-09-06T20:17:48.330901: step 4352, loss 0.0757282, acc 0.96
2016-09-06T20:17:49.018976: step 4353, loss 0.0597675, acc 0.98
2016-09-06T20:17:49.676184: step 4354, loss 0.0304815, acc 1
2016-09-06T20:17:50.374409: step 4355, loss 0.0284736, acc 0.98
2016-09-06T20:17:51.060182: step 4356, loss 0.026703, acc 1
2016-09-06T20:17:51.750884: step 4357, loss 0.0477286, acc 0.98
2016-09-06T20:17:52.422240: step 4358, loss 0.11113, acc 0.94
2016-09-06T20:17:53.109123: step 4359, loss 0.00466003, acc 1
2016-09-06T20:17:53.808069: step 4360, loss 0.00876741, acc 1
2016-09-06T20:17:54.516483: step 4361, loss 0.0404266, acc 0.98
2016-09-06T20:17:55.218935: step 4362, loss 0.0240687, acc 0.98
2016-09-06T20:17:55.904781: step 4363, loss 0.0366672, acc 1
2016-09-06T20:17:56.603694: step 4364, loss 0.0388483, acc 0.96
2016-09-06T20:17:57.287267: step 4365, loss 0.0264312, acc 1
2016-09-06T20:17:57.990156: step 4366, loss 0.0560081, acc 0.96
2016-09-06T20:17:58.669216: step 4367, loss 0.005079, acc 1
2016-09-06T20:17:59.344875: step 4368, loss 0.0617115, acc 0.96
2016-09-06T20:18:00.044346: step 4369, loss 0.0117261, acc 1
2016-09-06T20:18:00.801040: step 4370, loss 0.0181847, acc 0.98
2016-09-06T20:18:01.481337: step 4371, loss 0.0814016, acc 0.98
2016-09-06T20:18:02.178196: step 4372, loss 0.0325676, acc 1
2016-09-06T20:18:02.858918: step 4373, loss 0.0488705, acc 0.98
2016-09-06T20:18:03.578100: step 4374, loss 0.00816223, acc 1
2016-09-06T20:18:04.280068: step 4375, loss 0.0185331, acc 1
2016-09-06T20:18:04.978346: step 4376, loss 0.0380238, acc 0.96
2016-09-06T20:18:05.652821: step 4377, loss 0.00558387, acc 1
2016-09-06T20:18:06.328983: step 4378, loss 0.00189716, acc 1
2016-09-06T20:18:07.014850: step 4379, loss 0.0169511, acc 1
2016-09-06T20:18:07.687060: step 4380, loss 0.000121293, acc 1
2016-09-06T20:18:08.381830: step 4381, loss 0.019116, acc 0.98
2016-09-06T20:18:09.051773: step 4382, loss 0.0286681, acc 0.98
2016-09-06T20:18:09.758395: step 4383, loss 0.0176272, acc 0.98
2016-09-06T20:18:10.437108: step 4384, loss 0.0422457, acc 0.98
2016-09-06T20:18:11.103580: step 4385, loss 0.00731111, acc 1
2016-09-06T20:18:11.842488: step 4386, loss 0.029545, acc 0.98
2016-09-06T20:18:12.489181: step 4387, loss 0.0130274, acc 1
2016-09-06T20:18:13.208785: step 4388, loss 0.00566504, acc 1
2016-09-06T20:18:13.896382: step 4389, loss 0.00548492, acc 1
2016-09-06T20:18:14.592535: step 4390, loss 0.0469882, acc 0.98
2016-09-06T20:18:15.275130: step 4391, loss 0.0069465, acc 1
2016-09-06T20:18:15.968016: step 4392, loss 0.0564633, acc 0.96
2016-09-06T20:18:16.656535: step 4393, loss 0.0140845, acc 1
2016-09-06T20:18:17.365865: step 4394, loss 0.0030774, acc 1
2016-09-06T20:18:18.102419: step 4395, loss 0.0138633, acc 1
2016-09-06T20:18:18.793368: step 4396, loss 0.0116971, acc 1
2016-09-06T20:18:19.498065: step 4397, loss 0.0217905, acc 0.98
2016-09-06T20:18:20.171029: step 4398, loss 0.218273, acc 0.94
2016-09-06T20:18:20.877695: step 4399, loss 0.0277518, acc 0.98
2016-09-06T20:18:21.590489: step 4400, loss 0.0975973, acc 0.98

Evaluation:
2016-09-06T20:18:24.720304: step 4400, loss 2.28003, acc 0.739212

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-4400

2016-09-06T20:18:26.425350: step 4401, loss 0.051291, acc 0.98
2016-09-06T20:18:27.090863: step 4402, loss 0.01771, acc 1
2016-09-06T20:18:27.779235: step 4403, loss 0.189898, acc 0.96
2016-09-06T20:18:28.461294: step 4404, loss 0.118121, acc 0.98
2016-09-06T20:18:29.149775: step 4405, loss 0.0118585, acc 1
2016-09-06T20:18:29.835237: step 4406, loss 0.00359964, acc 1
2016-09-06T20:18:30.525626: step 4407, loss 0.0200542, acc 1
2016-09-06T20:18:31.225985: step 4408, loss 0.0626162, acc 0.96
2016-09-06T20:18:31.921903: step 4409, loss 0.012309, acc 1
2016-09-06T20:18:32.609024: step 4410, loss 0.010621, acc 1
2016-09-06T20:18:33.293031: step 4411, loss 0.0191349, acc 1
2016-09-06T20:18:33.974795: step 4412, loss 0.0243746, acc 0.98
2016-09-06T20:18:34.681456: step 4413, loss 0.0582535, acc 0.98
2016-09-06T20:18:35.351172: step 4414, loss 0.0315987, acc 1
2016-09-06T20:18:36.070042: step 4415, loss 0.0165551, acc 1
2016-09-06T20:18:36.695705: step 4416, loss 0.0138288, acc 1
2016-09-06T20:18:37.363685: step 4417, loss 0.0351192, acc 0.98
2016-09-06T20:18:38.061742: step 4418, loss 0.124622, acc 0.94
2016-09-06T20:18:38.735454: step 4419, loss 0.0293341, acc 0.98
2016-09-06T20:18:39.431686: step 4420, loss 0.00561137, acc 1
2016-09-06T20:18:40.130732: step 4421, loss 0.0443531, acc 0.96
2016-09-06T20:18:40.842225: step 4422, loss 0.103057, acc 0.98
2016-09-06T20:18:41.531048: step 4423, loss 0.0227202, acc 1
2016-09-06T20:18:42.206697: step 4424, loss 0.0625906, acc 0.98
2016-09-06T20:18:42.899417: step 4425, loss 0.00916582, acc 1
2016-09-06T20:18:43.599443: step 4426, loss 0.0421022, acc 0.96
2016-09-06T20:18:44.296930: step 4427, loss 0.00963838, acc 1
2016-09-06T20:18:44.976708: step 4428, loss 0.000227907, acc 1
2016-09-06T20:18:45.688269: step 4429, loss 0.0791367, acc 0.96
2016-09-06T20:18:46.401906: step 4430, loss 0.0492937, acc 1
2016-09-06T20:18:47.090035: step 4431, loss 0.017173, acc 0.98
2016-09-06T20:18:47.770872: step 4432, loss 0.0083739, acc 1
2016-09-06T20:18:48.445907: step 4433, loss 0.0683824, acc 0.96
2016-09-06T20:18:49.117457: step 4434, loss 0.0172493, acc 0.98
2016-09-06T20:18:49.786688: step 4435, loss 0.00417204, acc 1
2016-09-06T20:18:50.508318: step 4436, loss 0.0232101, acc 0.98
2016-09-06T20:18:51.216695: step 4437, loss 0.0210077, acc 0.98
2016-09-06T20:18:51.906178: step 4438, loss 0.00674697, acc 1
2016-09-06T20:18:52.615837: step 4439, loss 0.00194438, acc 1
2016-09-06T20:18:53.312180: step 4440, loss 0.0686967, acc 0.96
2016-09-06T20:18:54.021321: step 4441, loss 0.107855, acc 0.96
2016-09-06T20:18:54.705210: step 4442, loss 0.0547354, acc 0.96
2016-09-06T20:18:55.400384: step 4443, loss 0.0294184, acc 0.98
2016-09-06T20:18:56.087720: step 4444, loss 0.00623944, acc 1
2016-09-06T20:18:56.786559: step 4445, loss 0.0285999, acc 0.98
2016-09-06T20:18:57.472913: step 4446, loss 0.0250817, acc 0.98
2016-09-06T20:18:58.160746: step 4447, loss 0.000214303, acc 1
2016-09-06T20:18:58.875745: step 4448, loss 0.0512527, acc 0.96
2016-09-06T20:18:59.557876: step 4449, loss 0.0332981, acc 0.96
2016-09-06T20:19:00.254611: step 4450, loss 0.0403595, acc 0.96
2016-09-06T20:19:00.924777: step 4451, loss 0.0730293, acc 0.98
2016-09-06T20:19:01.613285: step 4452, loss 0.041615, acc 0.98
2016-09-06T20:19:02.294079: step 4453, loss 0.000435627, acc 1
2016-09-06T20:19:02.985565: step 4454, loss 0.0407357, acc 0.98
2016-09-06T20:19:03.694333: step 4455, loss 0.022195, acc 1
2016-09-06T20:19:04.412986: step 4456, loss 0.0165734, acc 1
2016-09-06T20:19:05.094963: step 4457, loss 0.0433233, acc 0.98
2016-09-06T20:19:05.795489: step 4458, loss 0.0194989, acc 1
2016-09-06T20:19:06.470440: step 4459, loss 0.0308915, acc 1
2016-09-06T20:19:07.181513: step 4460, loss 0.0147951, acc 1
2016-09-06T20:19:07.851205: step 4461, loss 0.011652, acc 1
2016-09-06T20:19:08.542757: step 4462, loss 0.0143515, acc 0.98
2016-09-06T20:19:09.206543: step 4463, loss 0.0133763, acc 1
2016-09-06T20:19:09.884930: step 4464, loss 0.0434228, acc 0.98
2016-09-06T20:19:10.560353: step 4465, loss 0.0429886, acc 0.98
2016-09-06T20:19:11.250723: step 4466, loss 0.00287577, acc 1
2016-09-06T20:19:11.925412: step 4467, loss 0.0187039, acc 1
2016-09-06T20:19:12.602030: step 4468, loss 0.00276128, acc 1
2016-09-06T20:19:13.315504: step 4469, loss 0.00160688, acc 1
2016-09-06T20:19:13.986554: step 4470, loss 0.00398234, acc 1
2016-09-06T20:19:14.675687: step 4471, loss 0.0277362, acc 1
2016-09-06T20:19:15.359639: step 4472, loss 0.0219302, acc 0.98
2016-09-06T20:19:16.047127: step 4473, loss 0.0233802, acc 0.98
2016-09-06T20:19:16.733808: step 4474, loss 0.0569402, acc 0.98
2016-09-06T20:19:17.412801: step 4475, loss 0.0169233, acc 1
2016-09-06T20:19:18.115068: step 4476, loss 0.0495187, acc 0.98
2016-09-06T20:19:18.778477: step 4477, loss 0.0209894, acc 0.98
2016-09-06T20:19:19.474184: step 4478, loss 0.14557, acc 0.94
2016-09-06T20:19:20.170747: step 4479, loss 0.0112459, acc 1
2016-09-06T20:19:20.871533: step 4480, loss 0.0171347, acc 1
2016-09-06T20:19:21.557673: step 4481, loss 0.0228986, acc 1
2016-09-06T20:19:22.236118: step 4482, loss 0.00993403, acc 1
2016-09-06T20:19:22.973249: step 4483, loss 0.0111878, acc 1
2016-09-06T20:19:23.644552: step 4484, loss 0.0139045, acc 1
2016-09-06T20:19:24.334116: step 4485, loss 0.0157804, acc 1
2016-09-06T20:19:25.018045: step 4486, loss 0.168032, acc 0.96
2016-09-06T20:19:25.695829: step 4487, loss 0.0218863, acc 1
2016-09-06T20:19:26.372749: step 4488, loss 0.0305581, acc 0.98
2016-09-06T20:19:27.039366: step 4489, loss 0.0175398, acc 1
2016-09-06T20:19:27.746395: step 4490, loss 0.0323436, acc 0.98
2016-09-06T20:19:28.421415: step 4491, loss 0.0177258, acc 0.98
2016-09-06T20:19:29.098705: step 4492, loss 0.0815643, acc 0.98
2016-09-06T20:19:29.775247: step 4493, loss 0.0382416, acc 1
2016-09-06T20:19:30.459905: step 4494, loss 0.0437733, acc 0.98
2016-09-06T20:19:31.149726: step 4495, loss 8.76726e-05, acc 1
2016-09-06T20:19:31.847371: step 4496, loss 0.14941, acc 0.98
2016-09-06T20:19:32.539956: step 4497, loss 0.0563305, acc 0.98
2016-09-06T20:19:33.217083: step 4498, loss 0.0272239, acc 1
2016-09-06T20:19:33.915732: step 4499, loss 0.0265407, acc 1
2016-09-06T20:19:34.597407: step 4500, loss 0.0349301, acc 0.98

Evaluation:
2016-09-06T20:19:37.724956: step 4500, loss 2.32174, acc 0.719512

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-4500

2016-09-06T20:19:39.387722: step 4501, loss 0.0274925, acc 1
2016-09-06T20:19:40.067473: step 4502, loss 0.0175131, acc 0.98
2016-09-06T20:19:40.754922: step 4503, loss 0.0116316, acc 1
2016-09-06T20:19:41.412493: step 4504, loss 0.054801, acc 0.98
2016-09-06T20:19:42.132864: step 4505, loss 0.0842871, acc 0.96
2016-09-06T20:19:42.797651: step 4506, loss 0.0286714, acc 0.98
2016-09-06T20:19:43.489787: step 4507, loss 0.0164982, acc 1
2016-09-06T20:19:44.188597: step 4508, loss 0.0471978, acc 0.96
2016-09-06T20:19:44.890507: step 4509, loss 0.0133903, acc 1
2016-09-06T20:19:45.587695: step 4510, loss 0.0552758, acc 0.96
2016-09-06T20:19:46.302839: step 4511, loss 0.0295306, acc 0.98
2016-09-06T20:19:47.011327: step 4512, loss 0.0268646, acc 0.98
2016-09-06T20:19:47.706224: step 4513, loss 0.00692601, acc 1
2016-09-06T20:19:48.379806: step 4514, loss 0.0112289, acc 1
2016-09-06T20:19:49.083792: step 4515, loss 0.0251918, acc 1
2016-09-06T20:19:49.780814: step 4516, loss 0.0190054, acc 0.98
2016-09-06T20:19:50.485818: step 4517, loss 0.00928341, acc 1
2016-09-06T20:19:51.175821: step 4518, loss 0.000954633, acc 1
2016-09-06T20:19:51.882333: step 4519, loss 0.0354944, acc 0.98
2016-09-06T20:19:52.577708: step 4520, loss 0.0393505, acc 0.98
2016-09-06T20:19:53.280910: step 4521, loss 0.0804186, acc 0.98
2016-09-06T20:19:53.954389: step 4522, loss 0.061038, acc 0.96
2016-09-06T20:19:54.640861: step 4523, loss 0.0142626, acc 1
2016-09-06T20:19:55.362653: step 4524, loss 0.0297028, acc 0.98
2016-09-06T20:19:56.030533: step 4525, loss 0.0266812, acc 0.98
2016-09-06T20:19:56.724655: step 4526, loss 0.0315205, acc 0.98
2016-09-06T20:19:57.429442: step 4527, loss 0.0225966, acc 0.98
2016-09-06T20:19:58.120032: step 4528, loss 0.0640727, acc 0.96
2016-09-06T20:19:58.790594: step 4529, loss 0.0341403, acc 0.98
2016-09-06T20:19:59.477343: step 4530, loss 0.0036833, acc 1
2016-09-06T20:20:00.162354: step 4531, loss 0.0231211, acc 1
2016-09-06T20:20:00.885185: step 4532, loss 0.00934082, acc 1
2016-09-06T20:20:01.564840: step 4533, loss 0.000371496, acc 1
2016-09-06T20:20:02.273437: step 4534, loss 0.0256696, acc 0.98
2016-09-06T20:20:02.966850: step 4535, loss 0.0186053, acc 1
2016-09-06T20:20:03.665117: step 4536, loss 0.00137817, acc 1
2016-09-06T20:20:04.369928: step 4537, loss 0.0566356, acc 0.96
2016-09-06T20:20:05.093811: step 4538, loss 0.0371107, acc 1
2016-09-06T20:20:05.798036: step 4539, loss 0.0311056, acc 0.98
2016-09-06T20:20:06.502638: step 4540, loss 0.00425925, acc 1
2016-09-06T20:20:07.201693: step 4541, loss 0.0166661, acc 1
2016-09-06T20:20:07.896174: step 4542, loss 0.0259031, acc 1
2016-09-06T20:20:08.607063: step 4543, loss 0.00723952, acc 1
2016-09-06T20:20:09.287117: step 4544, loss 0.0537139, acc 0.96
2016-09-06T20:20:09.960474: step 4545, loss 0.016409, acc 0.98
2016-09-06T20:20:10.644306: step 4546, loss 0.0633453, acc 0.98
2016-09-06T20:20:11.330607: step 4547, loss 0.145764, acc 0.94
2016-09-06T20:20:12.012098: step 4548, loss 0.00863949, acc 1
2016-09-06T20:20:12.716629: step 4549, loss 0.0355974, acc 1
2016-09-06T20:20:13.436904: step 4550, loss 0.0190983, acc 0.98
2016-09-06T20:20:14.113218: step 4551, loss 0.0253913, acc 0.98
2016-09-06T20:20:14.795663: step 4552, loss 0.000438908, acc 1
2016-09-06T20:20:15.504408: step 4553, loss 0.0339483, acc 0.96
2016-09-06T20:20:16.192137: step 4554, loss 0.0106523, acc 1
2016-09-06T20:20:16.891961: step 4555, loss 0.038424, acc 0.98
2016-09-06T20:20:17.588497: step 4556, loss 0.030991, acc 0.98
2016-09-06T20:20:18.283326: step 4557, loss 0.0649857, acc 0.98
2016-09-06T20:20:18.961986: step 4558, loss 0.0258893, acc 0.98
2016-09-06T20:20:19.629477: step 4559, loss 0.0150822, acc 1
2016-09-06T20:20:20.302629: step 4560, loss 0.00474016, acc 1
2016-09-06T20:20:20.992433: step 4561, loss 0.0163441, acc 1
2016-09-06T20:20:21.669533: step 4562, loss 0.00519875, acc 1
2016-09-06T20:20:22.350579: step 4563, loss 0.0213487, acc 0.98
2016-09-06T20:20:23.068430: step 4564, loss 0.0427193, acc 0.98
2016-09-06T20:20:23.750366: step 4565, loss 0.0343074, acc 0.98
2016-09-06T20:20:24.422869: step 4566, loss 0.110372, acc 0.98
2016-09-06T20:20:25.129155: step 4567, loss 0.0122888, acc 1
2016-09-06T20:20:25.823231: step 4568, loss 0.0364667, acc 0.96
2016-09-06T20:20:26.510741: step 4569, loss 0.000290805, acc 1
2016-09-06T20:20:27.185099: step 4570, loss 0.10062, acc 0.98
2016-09-06T20:20:27.898792: step 4571, loss 0.0438076, acc 0.98
2016-09-06T20:20:28.561287: step 4572, loss 0.00150436, acc 1
2016-09-06T20:20:29.250954: step 4573, loss 0.000766388, acc 1
2016-09-06T20:20:29.943965: step 4574, loss 0.0384562, acc 0.96
2016-09-06T20:20:30.628610: step 4575, loss 0.0921357, acc 0.96
2016-09-06T20:20:31.379090: step 4576, loss 0.118305, acc 0.96
2016-09-06T20:20:32.045554: step 4577, loss 0.00225121, acc 1
2016-09-06T20:20:32.772855: step 4578, loss 0.00204424, acc 1
2016-09-06T20:20:33.462306: step 4579, loss 0.0314773, acc 1
2016-09-06T20:20:34.191353: step 4580, loss 0.0540252, acc 0.98
2016-09-06T20:20:34.908787: step 4581, loss 0.000420093, acc 1
2016-09-06T20:20:35.576443: step 4582, loss 0.018552, acc 1
2016-09-06T20:20:36.284400: step 4583, loss 0.0033919, acc 1
2016-09-06T20:20:36.949702: step 4584, loss 0.00794266, acc 1
2016-09-06T20:20:37.634990: step 4585, loss 0.0464465, acc 0.96
2016-09-06T20:20:38.314742: step 4586, loss 0.0360457, acc 0.98
2016-09-06T20:20:39.018651: step 4587, loss 0.0818951, acc 0.98
2016-09-06T20:20:39.719365: step 4588, loss 0.0101262, acc 1
2016-09-06T20:20:40.390423: step 4589, loss 0.0128037, acc 1
2016-09-06T20:20:41.087181: step 4590, loss 0.102656, acc 0.94
2016-09-06T20:20:41.791906: step 4591, loss 0.00417937, acc 1
2016-09-06T20:20:42.484661: step 4592, loss 0.0451765, acc 0.96
2016-09-06T20:20:43.153444: step 4593, loss 0.00831074, acc 1
2016-09-06T20:20:43.842752: step 4594, loss 0.0435819, acc 1
2016-09-06T20:20:44.518282: step 4595, loss 0.000485789, acc 1
2016-09-06T20:20:45.171571: step 4596, loss 0.0574901, acc 0.98
2016-09-06T20:20:45.882666: step 4597, loss 0.0120991, acc 1
2016-09-06T20:20:46.567351: step 4598, loss 0.0163747, acc 1
2016-09-06T20:20:47.245134: step 4599, loss 0.00995417, acc 1
2016-09-06T20:20:47.926656: step 4600, loss 0.0240477, acc 1

Evaluation:
2016-09-06T20:20:51.064354: step 4600, loss 1.97505, acc 0.738274

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-4600

2016-09-06T20:20:52.725053: step 4601, loss 0.0185477, acc 1
2016-09-06T20:20:53.399445: step 4602, loss 0.0343238, acc 0.98
2016-09-06T20:20:54.091160: step 4603, loss 0.0751627, acc 0.94
2016-09-06T20:20:54.784050: step 4604, loss 0.0394062, acc 0.98
2016-09-06T20:20:55.492264: step 4605, loss 0.0262333, acc 0.98
2016-09-06T20:20:56.184151: step 4606, loss 0.0240337, acc 1
2016-09-06T20:20:56.865761: step 4607, loss 0.0327329, acc 1
2016-09-06T20:20:57.496660: step 4608, loss 0.00637878, acc 1
2016-09-06T20:20:58.193544: step 4609, loss 0.0173195, acc 1
2016-09-06T20:20:58.897508: step 4610, loss 0.00364135, acc 1
2016-09-06T20:20:59.576839: step 4611, loss 0.0419921, acc 0.98
2016-09-06T20:21:00.297248: step 4612, loss 0.0543864, acc 0.98
2016-09-06T20:21:00.969988: step 4613, loss 0.0493801, acc 0.98
2016-09-06T20:21:01.646517: step 4614, loss 0.0143115, acc 1
2016-09-06T20:21:02.357357: step 4615, loss 0.0162871, acc 1
2016-09-06T20:21:03.075277: step 4616, loss 0.0663541, acc 0.98
2016-09-06T20:21:03.772607: step 4617, loss 0.0297755, acc 0.98
2016-09-06T20:21:04.435565: step 4618, loss 0.0335683, acc 0.98
2016-09-06T20:21:05.151360: step 4619, loss 0.0218518, acc 1
2016-09-06T20:21:05.808981: step 4620, loss 0.00389669, acc 1
2016-09-06T20:21:06.471399: step 4621, loss 0.103651, acc 0.98
2016-09-06T20:21:07.147038: step 4622, loss 0.00531156, acc 1
2016-09-06T20:21:07.850212: step 4623, loss 0.00119208, acc 1
2016-09-06T20:21:08.546310: step 4624, loss 0.00342487, acc 1
2016-09-06T20:21:09.222172: step 4625, loss 0.0267017, acc 0.98
2016-09-06T20:21:09.946297: step 4626, loss 0.000954689, acc 1
2016-09-06T20:21:10.649966: step 4627, loss 0.00638754, acc 1
2016-09-06T20:21:11.343816: step 4628, loss 0.00680035, acc 1
2016-09-06T20:21:12.036311: step 4629, loss 0.0321722, acc 0.98
2016-09-06T20:21:12.710007: step 4630, loss 0.0391137, acc 0.98
2016-09-06T20:21:13.394086: step 4631, loss 0.000971171, acc 1
2016-09-06T20:21:14.057868: step 4632, loss 0.107848, acc 0.92
2016-09-06T20:21:14.774976: step 4633, loss 0.0372788, acc 0.98
2016-09-06T20:21:15.462707: step 4634, loss 0.00847996, acc 1
2016-09-06T20:21:16.147988: step 4635, loss 0.0887452, acc 0.98
2016-09-06T20:21:16.836216: step 4636, loss 0.0252914, acc 0.98
2016-09-06T20:21:17.524040: step 4637, loss 0.133855, acc 0.98
2016-09-06T20:21:18.218288: step 4638, loss 0.0166903, acc 1
2016-09-06T20:21:18.930776: step 4639, loss 0.0107331, acc 1
2016-09-06T20:21:19.646245: step 4640, loss 0.108417, acc 0.96
2016-09-06T20:21:20.340935: step 4641, loss 0.0173282, acc 0.98
2016-09-06T20:21:21.018439: step 4642, loss 0.00334712, acc 1
2016-09-06T20:21:21.706416: step 4643, loss 0.0213555, acc 1
2016-09-06T20:21:22.411277: step 4644, loss 0.0225531, acc 0.98
2016-09-06T20:21:23.140200: step 4645, loss 2.50869e-05, acc 1
2016-09-06T20:21:23.826837: step 4646, loss 0.00894381, acc 1
2016-09-06T20:21:24.513451: step 4647, loss 0.0116925, acc 1
2016-09-06T20:21:25.206172: step 4648, loss 0.0257165, acc 1
2016-09-06T20:21:25.903228: step 4649, loss 0.0109432, acc 1
2016-09-06T20:21:26.599350: step 4650, loss 0.00572645, acc 1
2016-09-06T20:21:27.261590: step 4651, loss 0.0429941, acc 0.98
2016-09-06T20:21:27.964167: step 4652, loss 0.0433816, acc 0.98
2016-09-06T20:21:28.646610: step 4653, loss 0.10654, acc 0.94
2016-09-06T20:21:29.330230: step 4654, loss 0.022118, acc 0.98
2016-09-06T20:21:30.021730: step 4655, loss 0.00848017, acc 1
2016-09-06T20:21:30.724138: step 4656, loss 0.0467167, acc 0.98
2016-09-06T20:21:31.412869: step 4657, loss 0.0336284, acc 1
2016-09-06T20:21:32.079217: step 4658, loss 0.00334099, acc 1
2016-09-06T20:21:32.783923: step 4659, loss 0.00968203, acc 1
2016-09-06T20:21:33.454321: step 4660, loss 0.00469908, acc 1
2016-09-06T20:21:34.144028: step 4661, loss 0.00368566, acc 1
2016-09-06T20:21:34.836935: step 4662, loss 0.0556718, acc 0.98
2016-09-06T20:21:35.510817: step 4663, loss 0.0771353, acc 0.94
2016-09-06T20:21:36.225297: step 4664, loss 0.0348292, acc 0.98
2016-09-06T20:21:36.901349: step 4665, loss 0.00417136, acc 1
2016-09-06T20:21:37.599233: step 4666, loss 0.0189941, acc 1
2016-09-06T20:21:38.263249: step 4667, loss 0.135763, acc 0.94
2016-09-06T20:21:38.946577: step 4668, loss 0.0171415, acc 1
2016-09-06T20:21:39.653921: step 4669, loss 0.00863043, acc 1
2016-09-06T20:21:40.356609: step 4670, loss 0.0450076, acc 0.98
2016-09-06T20:21:41.045216: step 4671, loss 0.00964151, acc 1
2016-09-06T20:21:41.719450: step 4672, loss 0.0189252, acc 1
2016-09-06T20:21:42.430185: step 4673, loss 0.0289095, acc 0.98
2016-09-06T20:21:43.110813: step 4674, loss 0.0530476, acc 0.98
2016-09-06T20:21:43.806228: step 4675, loss 0.00600207, acc 1
2016-09-06T20:21:44.499294: step 4676, loss 0.0211419, acc 1
2016-09-06T20:21:45.190783: step 4677, loss 0.0395462, acc 0.98
2016-09-06T20:21:45.882505: step 4678, loss 0.0697408, acc 0.98
2016-09-06T20:21:46.543167: step 4679, loss 0.00659126, acc 1
2016-09-06T20:21:47.243465: step 4680, loss 0.0258862, acc 0.98
2016-09-06T20:21:47.936502: step 4681, loss 0.022455, acc 0.98
2016-09-06T20:21:48.633098: step 4682, loss 0.00781094, acc 1
2016-09-06T20:21:49.326298: step 4683, loss 0.0611868, acc 0.98
2016-09-06T20:21:50.017893: step 4684, loss 0.0746816, acc 0.98
2016-09-06T20:21:50.723297: step 4685, loss 0.037052, acc 0.98
2016-09-06T20:21:51.386162: step 4686, loss 0.0504873, acc 0.98
2016-09-06T20:21:52.070733: step 4687, loss 0.0167669, acc 1
2016-09-06T20:21:52.766037: step 4688, loss 0.00905123, acc 1
2016-09-06T20:21:53.459882: step 4689, loss 0.0723472, acc 0.98
2016-09-06T20:21:54.154556: step 4690, loss 0.0271748, acc 0.98
2016-09-06T20:21:54.851561: step 4691, loss 0.0275765, acc 1
2016-09-06T20:21:55.555556: step 4692, loss 0.0502958, acc 0.98
2016-09-06T20:21:56.234835: step 4693, loss 0.027723, acc 0.98
2016-09-06T20:21:56.911286: step 4694, loss 0.146102, acc 0.98
2016-09-06T20:21:57.601585: step 4695, loss 0.0509245, acc 0.98
2016-09-06T20:21:58.288477: step 4696, loss 0.0157852, acc 0.98
2016-09-06T20:21:58.977124: step 4697, loss 0.0695585, acc 0.96
2016-09-06T20:21:59.671445: step 4698, loss 0.0240976, acc 1
2016-09-06T20:22:00.407677: step 4699, loss 0.019012, acc 0.98
2016-09-06T20:22:01.073599: step 4700, loss 0.0378329, acc 0.98

Evaluation:
2016-09-06T20:22:04.226847: step 4700, loss 1.87052, acc 0.73546

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-4700

2016-09-06T20:22:05.977719: step 4701, loss 0.00258494, acc 1
2016-09-06T20:22:06.673503: step 4702, loss 0.0180077, acc 1
2016-09-06T20:22:07.375112: step 4703, loss 0.0364336, acc 1
2016-09-06T20:22:08.064241: step 4704, loss 0.0426703, acc 0.98
2016-09-06T20:22:08.746355: step 4705, loss 0.0145478, acc 1
2016-09-06T20:22:09.430559: step 4706, loss 0.0166042, acc 0.98
2016-09-06T20:22:10.142292: step 4707, loss 0.0727893, acc 0.98
2016-09-06T20:22:10.822439: step 4708, loss 0.0451727, acc 0.96
2016-09-06T20:22:11.515191: step 4709, loss 0.0286616, acc 0.98
2016-09-06T20:22:12.196206: step 4710, loss 0.00283296, acc 1
2016-09-06T20:22:12.869572: step 4711, loss 0.0643794, acc 0.98
2016-09-06T20:22:13.558089: step 4712, loss 0.0326049, acc 0.98
2016-09-06T20:22:14.226410: step 4713, loss 0.0203239, acc 1
2016-09-06T20:22:14.950530: step 4714, loss 0.00929687, acc 1
2016-09-06T20:22:15.633616: step 4715, loss 0.0276991, acc 0.98
2016-09-06T20:22:16.313372: step 4716, loss 0.0386304, acc 0.98
2016-09-06T20:22:16.992190: step 4717, loss 0.0416493, acc 0.98
2016-09-06T20:22:17.678390: step 4718, loss 0.0809939, acc 0.96
2016-09-06T20:22:18.380588: step 4719, loss 0.268052, acc 0.96
2016-09-06T20:22:19.054061: step 4720, loss 0.00438428, acc 1
2016-09-06T20:22:19.754400: step 4721, loss 0.0317138, acc 0.98
2016-09-06T20:22:20.438742: step 4722, loss 0.065377, acc 0.96
2016-09-06T20:22:21.113854: step 4723, loss 0.0405265, acc 0.98
2016-09-06T20:22:21.805140: step 4724, loss 0.0209011, acc 0.98
2016-09-06T20:22:22.499977: step 4725, loss 0.0375342, acc 0.96
2016-09-06T20:22:23.190080: step 4726, loss 0.122063, acc 0.96
2016-09-06T20:22:23.852499: step 4727, loss 0.0497376, acc 0.98
2016-09-06T20:22:24.556494: step 4728, loss 0.00236202, acc 1
2016-09-06T20:22:25.245724: step 4729, loss 0.0257614, acc 1
2016-09-06T20:22:25.926738: step 4730, loss 0.0369196, acc 1
2016-09-06T20:22:26.620128: step 4731, loss 0.00463476, acc 1
2016-09-06T20:22:27.323396: step 4732, loss 0.0271606, acc 1
2016-09-06T20:22:28.045544: step 4733, loss 0.047423, acc 0.98
2016-09-06T20:22:28.728613: step 4734, loss 0.0579867, acc 0.96
2016-09-06T20:22:29.432853: step 4735, loss 0.07274, acc 0.98
2016-09-06T20:22:30.127163: step 4736, loss 0.0404244, acc 0.98
2016-09-06T20:22:30.806756: step 4737, loss 0.00724239, acc 1
2016-09-06T20:22:31.489025: step 4738, loss 0.0414927, acc 0.98
2016-09-06T20:22:32.177122: step 4739, loss 0.125925, acc 0.94
2016-09-06T20:22:32.868726: step 4740, loss 0.00944212, acc 1
2016-09-06T20:22:33.554677: step 4741, loss 0.0427708, acc 0.96
2016-09-06T20:22:34.243755: step 4742, loss 0.0505425, acc 0.98
2016-09-06T20:22:34.948285: step 4743, loss 0.000256797, acc 1
2016-09-06T20:22:35.638783: step 4744, loss 0.075479, acc 0.96
2016-09-06T20:22:36.325722: step 4745, loss 0.061549, acc 0.96
2016-09-06T20:22:37.008285: step 4746, loss 0.00878471, acc 1
2016-09-06T20:22:37.700427: step 4747, loss 0.0287937, acc 0.98
2016-09-06T20:22:38.396909: step 4748, loss 0.0411412, acc 0.96
2016-09-06T20:22:39.070328: step 4749, loss 0.0985572, acc 0.94
2016-09-06T20:22:39.764902: step 4750, loss 0.00422158, acc 1
2016-09-06T20:22:40.447407: step 4751, loss 0.039358, acc 0.98
2016-09-06T20:22:41.157960: step 4752, loss 0.0550421, acc 0.98
2016-09-06T20:22:41.820796: step 4753, loss 0.0245394, acc 0.98
2016-09-06T20:22:42.532784: step 4754, loss 0.0114351, acc 1
2016-09-06T20:22:43.212155: step 4755, loss 0.00636928, acc 1
2016-09-06T20:22:43.901541: step 4756, loss 0.0317774, acc 1
2016-09-06T20:22:44.582534: step 4757, loss 0.0532604, acc 0.98
2016-09-06T20:22:45.275379: step 4758, loss 0.0253884, acc 1
2016-09-06T20:22:45.978879: step 4759, loss 0.0322209, acc 0.96
2016-09-06T20:22:46.644099: step 4760, loss 0.0278052, acc 0.98
2016-09-06T20:22:47.358859: step 4761, loss 0.0401607, acc 0.98
2016-09-06T20:22:48.022363: step 4762, loss 0.09103, acc 0.98
2016-09-06T20:22:48.701760: step 4763, loss 0.0351761, acc 0.98
2016-09-06T20:22:49.388264: step 4764, loss 0.0168574, acc 0.98
2016-09-06T20:22:50.065346: step 4765, loss 0.0162886, acc 1
2016-09-06T20:22:50.782187: step 4766, loss 0.0562283, acc 0.96
2016-09-06T20:22:51.457608: step 4767, loss 0.0528056, acc 0.98
2016-09-06T20:22:52.174328: step 4768, loss 0.0180164, acc 0.98
2016-09-06T20:22:52.858697: step 4769, loss 0.00437257, acc 1
2016-09-06T20:22:53.544629: step 4770, loss 0.0129576, acc 1
2016-09-06T20:22:54.229914: step 4771, loss 0.01054, acc 1
2016-09-06T20:22:54.914623: step 4772, loss 0.0151717, acc 1
2016-09-06T20:22:55.610982: step 4773, loss 0.0215219, acc 1
2016-09-06T20:22:56.270849: step 4774, loss 0.0660305, acc 0.98
2016-09-06T20:22:56.974737: step 4775, loss 0.0164026, acc 0.98
2016-09-06T20:22:57.657044: step 4776, loss 0.0284173, acc 0.98
2016-09-06T20:22:58.351732: step 4777, loss 0.0343155, acc 0.98
2016-09-06T20:22:59.054214: step 4778, loss 0.0179367, acc 0.98
2016-09-06T20:22:59.758137: step 4779, loss 0.00357791, acc 1
2016-09-06T20:23:00.505027: step 4780, loss 0.0043546, acc 1
2016-09-06T20:23:01.176778: step 4781, loss 0.00492648, acc 1
2016-09-06T20:23:01.868236: step 4782, loss 0.137123, acc 0.94
2016-09-06T20:23:02.549691: step 4783, loss 0.0395233, acc 0.98
2016-09-06T20:23:03.227793: step 4784, loss 0.0139946, acc 1
2016-09-06T20:23:03.917876: step 4785, loss 0.0206791, acc 0.98
2016-09-06T20:23:04.610401: step 4786, loss 0.0321075, acc 0.98
2016-09-06T20:23:05.301361: step 4787, loss 0.0155212, acc 0.98
2016-09-06T20:23:05.983524: step 4788, loss 0.0911483, acc 0.94
2016-09-06T20:23:06.677008: step 4789, loss 0.0426161, acc 0.96
2016-09-06T20:23:07.358733: step 4790, loss 0.000111156, acc 1
2016-09-06T20:23:08.058670: step 4791, loss 0.0769453, acc 0.98
2016-09-06T20:23:08.758317: step 4792, loss 0.022943, acc 0.98
2016-09-06T20:23:09.446114: step 4793, loss 0.0167299, acc 1
2016-09-06T20:23:10.130618: step 4794, loss 0.0225855, acc 0.98
2016-09-06T20:23:10.797125: step 4795, loss 0.00960082, acc 1
2016-09-06T20:23:11.500812: step 4796, loss 0.00336024, acc 1
2016-09-06T20:23:12.186090: step 4797, loss 0.0509264, acc 0.96
2016-09-06T20:23:12.854733: step 4798, loss 0.0364465, acc 0.98
2016-09-06T20:23:13.550903: step 4799, loss 0.0148305, acc 1
2016-09-06T20:23:14.182683: step 4800, loss 0.00171228, acc 1

Evaluation:
2016-09-06T20:23:17.338649: step 4800, loss 2.24968, acc 0.73546

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-4800

2016-09-06T20:23:19.056951: step 4801, loss 0.0305614, acc 0.98
2016-09-06T20:23:19.762513: step 4802, loss 0.000326676, acc 1
2016-09-06T20:23:20.441089: step 4803, loss 0.00495389, acc 1
2016-09-06T20:23:21.136756: step 4804, loss 0.0132439, acc 1
2016-09-06T20:23:21.825951: step 4805, loss 0.000370198, acc 1
2016-09-06T20:23:22.502558: step 4806, loss 0.106197, acc 0.96
2016-09-06T20:23:23.177965: step 4807, loss 0.0244425, acc 0.98
2016-09-06T20:23:23.886329: step 4808, loss 0.083078, acc 0.96
2016-09-06T20:23:24.595673: step 4809, loss 0.240789, acc 0.94
2016-09-06T20:23:25.264681: step 4810, loss 0.0182416, acc 0.98
2016-09-06T20:23:25.986253: step 4811, loss 0.0297643, acc 0.98
2016-09-06T20:23:26.672111: step 4812, loss 0.0273243, acc 1
2016-09-06T20:23:27.364376: step 4813, loss 0.0256796, acc 0.98
2016-09-06T20:23:28.043418: step 4814, loss 0.0251747, acc 0.98
2016-09-06T20:23:28.715962: step 4815, loss 0.000323523, acc 1
2016-09-06T20:23:29.394054: step 4816, loss 0.000531463, acc 1
2016-09-06T20:23:30.074402: step 4817, loss 0.0255939, acc 0.98
2016-09-06T20:23:30.782749: step 4818, loss 0.111153, acc 0.96
2016-09-06T20:23:31.467607: step 4819, loss 0.0656087, acc 0.94
2016-09-06T20:23:32.155616: step 4820, loss 0.0199437, acc 1
2016-09-06T20:23:32.869018: step 4821, loss 0.0123831, acc 1
2016-09-06T20:23:33.560071: step 4822, loss 0.0392192, acc 0.98
2016-09-06T20:23:34.273373: step 4823, loss 0.0150701, acc 1
2016-09-06T20:23:34.972979: step 4824, loss 0.0560343, acc 0.96
2016-09-06T20:23:35.646861: step 4825, loss 0.0192099, acc 0.98
2016-09-06T20:23:36.349846: step 4826, loss 0.103388, acc 0.98
2016-09-06T20:23:37.019267: step 4827, loss 0.0150355, acc 1
2016-09-06T20:23:37.712221: step 4828, loss 0.0140486, acc 1
2016-09-06T20:23:38.400896: step 4829, loss 0.0326167, acc 0.98
2016-09-06T20:23:39.148335: step 4830, loss 0.00886721, acc 1
2016-09-06T20:23:39.892239: step 4831, loss 0.0315125, acc 0.98
2016-09-06T20:23:40.593607: step 4832, loss 0.00537464, acc 1
2016-09-06T20:23:41.277428: step 4833, loss 0.0238987, acc 1
2016-09-06T20:23:41.963512: step 4834, loss 0.0169779, acc 0.98
2016-09-06T20:23:42.658194: step 4835, loss 0.053146, acc 0.96
2016-09-06T20:23:43.325423: step 4836, loss 0.0231932, acc 1
2016-09-06T20:23:44.005823: step 4837, loss 0.0103806, acc 1
2016-09-06T20:23:44.680051: step 4838, loss 0.024175, acc 1
2016-09-06T20:23:45.371012: step 4839, loss 0.00104392, acc 1
2016-09-06T20:23:46.045417: step 4840, loss 0.0392205, acc 0.98
2016-09-06T20:23:46.736626: step 4841, loss 0.0256384, acc 1
2016-09-06T20:23:47.445393: step 4842, loss 0.104697, acc 0.96
2016-09-06T20:23:48.117313: step 4843, loss 0.0126517, acc 1
2016-09-06T20:23:48.814379: step 4844, loss 0.0331254, acc 0.98
2016-09-06T20:23:49.503061: step 4845, loss 0.0444131, acc 0.98
2016-09-06T20:23:50.195321: step 4846, loss 0.00648141, acc 1
2016-09-06T20:23:50.910357: step 4847, loss 0.0496981, acc 0.98
2016-09-06T20:23:51.584807: step 4848, loss 0.00100812, acc 1
2016-09-06T20:23:52.271282: step 4849, loss 0.126813, acc 0.96
2016-09-06T20:23:52.960510: step 4850, loss 0.0235297, acc 0.98
2016-09-06T20:23:53.648406: step 4851, loss 0.0214699, acc 1
2016-09-06T20:23:54.339908: step 4852, loss 0.00234251, acc 1
2016-09-06T20:23:55.034089: step 4853, loss 0.000302991, acc 1
2016-09-06T20:23:55.727777: step 4854, loss 0.0216141, acc 0.98
2016-09-06T20:23:56.401934: step 4855, loss 0.0622486, acc 0.96
2016-09-06T20:23:57.105760: step 4856, loss 0.00273746, acc 1
2016-09-06T20:23:57.803777: step 4857, loss 0.033238, acc 0.98
2016-09-06T20:23:58.483088: step 4858, loss 0.0340201, acc 0.98
2016-09-06T20:23:59.184088: step 4859, loss 0.0625774, acc 0.96
2016-09-06T20:23:59.909903: step 4860, loss 0.0115898, acc 1
2016-09-06T20:24:00.666131: step 4861, loss 0.0140002, acc 1
2016-09-06T20:24:01.336601: step 4862, loss 0.140854, acc 0.92
2016-09-06T20:24:02.027826: step 4863, loss 0.0293074, acc 0.98
2016-09-06T20:24:02.719297: step 4864, loss 0.0230375, acc 1
2016-09-06T20:24:03.389068: step 4865, loss 0.0128541, acc 1
2016-09-06T20:24:04.078105: step 4866, loss 0.0805018, acc 0.98
2016-09-06T20:24:04.756058: step 4867, loss 0.00661366, acc 1
2016-09-06T20:24:05.491311: step 4868, loss 0.0339824, acc 0.98
2016-09-06T20:24:06.161486: step 4869, loss 0.0310198, acc 0.98
2016-09-06T20:24:06.859365: step 4870, loss 0.00933203, acc 1
2016-09-06T20:24:07.561646: step 4871, loss 0.0166254, acc 1
2016-09-06T20:24:08.250169: step 4872, loss 0.0124692, acc 1
2016-09-06T20:24:08.932819: step 4873, loss 0.00426364, acc 1
2016-09-06T20:24:09.602896: step 4874, loss 0.0219809, acc 1
2016-09-06T20:24:10.309699: step 4875, loss 0.0259359, acc 1
2016-09-06T20:24:11.012374: step 4876, loss 0.0619011, acc 0.96
2016-09-06T20:24:11.713414: step 4877, loss 0.0206075, acc 1
2016-09-06T20:24:12.383190: step 4878, loss 0.0172477, acc 0.98
2016-09-06T20:24:13.076625: step 4879, loss 0.00288135, acc 1
2016-09-06T20:24:13.776484: step 4880, loss 0.010634, acc 1
2016-09-06T20:24:14.438767: step 4881, loss 0.0202126, acc 0.98
2016-09-06T20:24:15.153476: step 4882, loss 0.0384895, acc 0.98
2016-09-06T20:24:15.859550: step 4883, loss 0.0416354, acc 0.98
2016-09-06T20:24:16.547881: step 4884, loss 0.000176015, acc 1
2016-09-06T20:24:17.228995: step 4885, loss 0.00481158, acc 1
2016-09-06T20:24:17.913415: step 4886, loss 0.000124291, acc 1
2016-09-06T20:24:18.597926: step 4887, loss 0.012421, acc 1
2016-09-06T20:24:19.263977: step 4888, loss 0.00487884, acc 1
2016-09-06T20:24:19.977851: step 4889, loss 0.0467993, acc 0.98
2016-09-06T20:24:20.670103: step 4890, loss 0.0351741, acc 0.96
2016-09-06T20:24:21.348534: step 4891, loss 0.0457894, acc 0.98
2016-09-06T20:24:22.037590: step 4892, loss 0.0553412, acc 0.98
2016-09-06T20:24:22.731095: step 4893, loss 0.0298793, acc 1
2016-09-06T20:24:23.470175: step 4894, loss 0.0321606, acc 0.98
2016-09-06T20:24:24.134674: step 4895, loss 0.0437834, acc 0.96
2016-09-06T20:24:24.841112: step 4896, loss 0.0265694, acc 1
2016-09-06T20:24:25.523613: step 4897, loss 0.0305058, acc 0.98
2016-09-06T20:24:26.208477: step 4898, loss 0.0151479, acc 0.98
2016-09-06T20:24:26.892848: step 4899, loss 0.0775103, acc 0.96
2016-09-06T20:24:27.578117: step 4900, loss 0.0247437, acc 0.98

Evaluation:
2016-09-06T20:24:30.729439: step 4900, loss 2.26459, acc 0.745779

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-4900

2016-09-06T20:24:32.428751: step 4901, loss 0.00559513, acc 1
2016-09-06T20:24:33.145005: step 4902, loss 0.00166827, acc 1
2016-09-06T20:24:33.802631: step 4903, loss 0.0199108, acc 0.98
2016-09-06T20:24:34.467790: step 4904, loss 0.00799945, acc 1
2016-09-06T20:24:35.147123: step 4905, loss 0.0562179, acc 0.98
2016-09-06T20:24:35.844097: step 4906, loss 0.0431764, acc 0.98
2016-09-06T20:24:36.530238: step 4907, loss 0.016092, acc 0.98
2016-09-06T20:24:37.208759: step 4908, loss 0.0461909, acc 0.98
2016-09-06T20:24:37.903451: step 4909, loss 0.0233128, acc 0.98
2016-09-06T20:24:38.579611: step 4910, loss 0.0634002, acc 0.94
2016-09-06T20:24:39.266544: step 4911, loss 0.00621061, acc 1
2016-09-06T20:24:39.940614: step 4912, loss 0.0219234, acc 1
2016-09-06T20:24:40.624965: step 4913, loss 0.043219, acc 0.98
2016-09-06T20:24:41.299332: step 4914, loss 0.0271771, acc 1
2016-09-06T20:24:41.988385: step 4915, loss 0.0330315, acc 0.96
2016-09-06T20:24:42.686958: step 4916, loss 0.000701841, acc 1
2016-09-06T20:24:43.358669: step 4917, loss 0.0128503, acc 1
2016-09-06T20:24:44.052363: step 4918, loss 0.0269714, acc 0.98
2016-09-06T20:24:44.747844: step 4919, loss 0.0269761, acc 1
2016-09-06T20:24:45.431237: step 4920, loss 0.000170949, acc 1
2016-09-06T20:24:46.134456: step 4921, loss 0.0212213, acc 1
2016-09-06T20:24:46.814230: step 4922, loss 0.0560856, acc 0.98
2016-09-06T20:24:47.508503: step 4923, loss 0.000896728, acc 1
2016-09-06T20:24:48.177289: step 4924, loss 0.0311482, acc 0.96
2016-09-06T20:24:48.880010: step 4925, loss 0.0567905, acc 0.98
2016-09-06T20:24:49.566650: step 4926, loss 0.0506675, acc 0.98
2016-09-06T20:24:50.240179: step 4927, loss 0.0179327, acc 1
2016-09-06T20:24:50.922261: step 4928, loss 0.0282079, acc 0.98
2016-09-06T20:24:51.595893: step 4929, loss 0.0142304, acc 1
2016-09-06T20:24:52.294945: step 4930, loss 0.0493206, acc 0.94
2016-09-06T20:24:52.973647: step 4931, loss 0.0696948, acc 0.98
2016-09-06T20:24:53.710343: step 4932, loss 0.0478659, acc 0.98
2016-09-06T20:24:54.432067: step 4933, loss 0.0246411, acc 1
2016-09-06T20:24:55.135719: step 4934, loss 0.00545086, acc 1
2016-09-06T20:24:55.839959: step 4935, loss 0.00283625, acc 1
2016-09-06T20:24:56.506321: step 4936, loss 0.00499061, acc 1
2016-09-06T20:24:57.209844: step 4937, loss 0.00532584, acc 1
2016-09-06T20:24:57.881733: step 4938, loss 0.0105049, acc 1
2016-09-06T20:24:58.560238: step 4939, loss 0.0308355, acc 0.98
2016-09-06T20:24:59.247824: step 4940, loss 0.0487457, acc 0.98
2016-09-06T20:24:59.946328: step 4941, loss 0.0637581, acc 0.98
2016-09-06T20:25:00.652071: step 4942, loss 0.000769066, acc 1
2016-09-06T20:25:01.322091: step 4943, loss 0.000672168, acc 1
2016-09-06T20:25:02.014264: step 4944, loss 0.0746648, acc 0.96
2016-09-06T20:25:02.694010: step 4945, loss 0.0308283, acc 0.98
2016-09-06T20:25:03.400770: step 4946, loss 0.0116349, acc 1
2016-09-06T20:25:04.097441: step 4947, loss 0.00911277, acc 1
2016-09-06T20:25:04.792126: step 4948, loss 0.0308405, acc 1
2016-09-06T20:25:05.483302: step 4949, loss 0.0624741, acc 0.96
2016-09-06T20:25:06.158337: step 4950, loss 0.000303351, acc 1
2016-09-06T20:25:06.875242: step 4951, loss 0.0340227, acc 0.98
2016-09-06T20:25:07.564146: step 4952, loss 0.0134132, acc 1
2016-09-06T20:25:08.234668: step 4953, loss 0.033481, acc 0.96
2016-09-06T20:25:08.932108: step 4954, loss 0.0287881, acc 0.98
2016-09-06T20:25:09.618279: step 4955, loss 0.023325, acc 0.98
2016-09-06T20:25:10.320755: step 4956, loss 0.0274778, acc 0.98
2016-09-06T20:25:11.020912: step 4957, loss 0.00394928, acc 1
2016-09-06T20:25:11.730292: step 4958, loss 0.0490608, acc 0.96
2016-09-06T20:25:12.421570: step 4959, loss 0.0307098, acc 0.98
2016-09-06T20:25:13.112282: step 4960, loss 0.0469425, acc 0.98
2016-09-06T20:25:13.811033: step 4961, loss 0.00176959, acc 1
2016-09-06T20:25:14.477468: step 4962, loss 0.0144983, acc 1
2016-09-06T20:25:15.179820: step 4963, loss 0.205544, acc 0.98
2016-09-06T20:25:15.855011: step 4964, loss 0.00943795, acc 1
2016-09-06T20:25:16.589540: step 4965, loss 0.0444229, acc 0.96
2016-09-06T20:25:17.286534: step 4966, loss 0.00860583, acc 1
2016-09-06T20:25:17.965922: step 4967, loss 0.0215456, acc 0.98
2016-09-06T20:25:18.657485: step 4968, loss 0.00819234, acc 1
2016-09-06T20:25:19.351009: step 4969, loss 0.0320403, acc 0.98
2016-09-06T20:25:20.053019: step 4970, loss 0.0314873, acc 1
2016-09-06T20:25:20.713372: step 4971, loss 0.0172748, acc 1
2016-09-06T20:25:21.414416: step 4972, loss 0.0141264, acc 0.98
2016-09-06T20:25:22.114420: step 4973, loss 0.0166106, acc 0.98
2016-09-06T20:25:22.807606: step 4974, loss 0.0526751, acc 0.98
2016-09-06T20:25:23.478856: step 4975, loss 0.00208208, acc 1
2016-09-06T20:25:24.162521: step 4976, loss 0.0054205, acc 1
2016-09-06T20:25:24.871840: step 4977, loss 0.0347364, acc 1
2016-09-06T20:25:25.557508: step 4978, loss 0.0255835, acc 0.98
2016-09-06T20:25:26.228340: step 4979, loss 0.0217688, acc 1
2016-09-06T20:25:26.912584: step 4980, loss 0.0374084, acc 1
2016-09-06T20:25:27.604428: step 4981, loss 0.027813, acc 0.98
2016-09-06T20:25:28.304022: step 4982, loss 0.0144762, acc 1
2016-09-06T20:25:28.981156: step 4983, loss 0.000127634, acc 1
2016-09-06T20:25:29.695117: step 4984, loss 0.0366269, acc 0.98
2016-09-06T20:25:30.375858: step 4985, loss 0.0300393, acc 0.98
2016-09-06T20:25:31.039493: step 4986, loss 0.000814321, acc 1
2016-09-06T20:25:31.736164: step 4987, loss 0.0479838, acc 0.96
2016-09-06T20:25:32.427140: step 4988, loss 5.89081e-05, acc 1
2016-09-06T20:25:33.114407: step 4989, loss 0.0372842, acc 0.98
2016-09-06T20:25:33.804895: step 4990, loss 0.0455362, acc 0.98
2016-09-06T20:25:34.506801: step 4991, loss 0.125635, acc 0.94
2016-09-06T20:25:35.129307: step 4992, loss 0.0261043, acc 1
2016-09-06T20:25:35.829234: step 4993, loss 0.0147485, acc 1
2016-09-06T20:25:36.541257: step 4994, loss 0.0866106, acc 0.96
2016-09-06T20:25:37.248281: step 4995, loss 0.0104074, acc 1
2016-09-06T20:25:37.933324: step 4996, loss 0.0340112, acc 0.98
2016-09-06T20:25:38.619600: step 4997, loss 0.0520837, acc 0.98
2016-09-06T20:25:39.336977: step 4998, loss 0.000725284, acc 1
2016-09-06T20:25:40.003813: step 4999, loss 0.00845824, acc 1
2016-09-06T20:25:40.679666: step 5000, loss 0.0870893, acc 0.94

Evaluation:
2016-09-06T20:25:43.865852: step 5000, loss 2.02621, acc 0.736398

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-5000

2016-09-06T20:25:45.633326: step 5001, loss 0.0123655, acc 1
2016-09-06T20:25:46.327403: step 5002, loss 0.0123219, acc 1
2016-09-06T20:25:47.000267: step 5003, loss 0.00646581, acc 1
2016-09-06T20:25:47.699290: step 5004, loss 0.0125934, acc 1
2016-09-06T20:25:48.375018: step 5005, loss 0.0687946, acc 0.96
2016-09-06T20:25:49.074650: step 5006, loss 0.0348043, acc 0.96
2016-09-06T20:25:49.795132: step 5007, loss 0.00834508, acc 1
2016-09-06T20:25:50.482246: step 5008, loss 0.0227601, acc 0.98
2016-09-06T20:25:51.149299: step 5009, loss 0.0284948, acc 1
2016-09-06T20:25:51.833147: step 5010, loss 0.0175539, acc 1
2016-09-06T20:25:52.557716: step 5011, loss 0.0279987, acc 1
2016-09-06T20:25:53.226471: step 5012, loss 0.0354719, acc 0.98
2016-09-06T20:25:53.909692: step 5013, loss 0.0357307, acc 0.98
2016-09-06T20:25:54.594980: step 5014, loss 0.00118409, acc 1
2016-09-06T20:25:55.263982: step 5015, loss 0.000676275, acc 1
2016-09-06T20:25:55.950361: step 5016, loss 0.0243383, acc 0.98
2016-09-06T20:25:56.628874: step 5017, loss 0.000270318, acc 1
2016-09-06T20:25:57.332801: step 5018, loss 0.0334633, acc 1
2016-09-06T20:25:58.057196: step 5019, loss 0.0159523, acc 1
2016-09-06T20:25:58.729070: step 5020, loss 0.0140561, acc 1
2016-09-06T20:25:59.404426: step 5021, loss 0.0108238, acc 1
2016-09-06T20:26:00.094750: step 5022, loss 0.0154683, acc 1
2016-09-06T20:26:00.818557: step 5023, loss 0.0259643, acc 0.98
2016-09-06T20:26:01.491195: step 5024, loss 0.00436772, acc 1
2016-09-06T20:26:02.188804: step 5025, loss 0.011611, acc 1
2016-09-06T20:26:02.869979: step 5026, loss 0.0429366, acc 0.98
2016-09-06T20:26:03.562206: step 5027, loss 0.0211527, acc 1
2016-09-06T20:26:04.258688: step 5028, loss 0.0107975, acc 1
2016-09-06T20:26:04.946718: step 5029, loss 0.00741689, acc 1
2016-09-06T20:26:05.659750: step 5030, loss 0.0279954, acc 0.98
2016-09-06T20:26:06.321771: step 5031, loss 0.125815, acc 0.96
2016-09-06T20:26:07.019703: step 5032, loss 0.0427532, acc 0.98
2016-09-06T20:26:07.691068: step 5033, loss 0.050041, acc 0.98
2016-09-06T20:26:08.359797: step 5034, loss 0.00189059, acc 1
2016-09-06T20:26:09.071565: step 5035, loss 0.00125704, acc 1
2016-09-06T20:26:09.729097: step 5036, loss 0.00886296, acc 1
2016-09-06T20:26:10.410859: step 5037, loss 0.0462892, acc 0.96
2016-09-06T20:26:11.077638: step 5038, loss 0.0163897, acc 1
2016-09-06T20:26:11.769495: step 5039, loss 0.00347888, acc 1
2016-09-06T20:26:12.439597: step 5040, loss 0.0143914, acc 1
2016-09-06T20:26:13.121942: step 5041, loss 0.0238329, acc 0.98
2016-09-06T20:26:13.802973: step 5042, loss 0.00568029, acc 1
2016-09-06T20:26:14.468923: step 5043, loss 0.0426765, acc 0.98
2016-09-06T20:26:15.163559: step 5044, loss 0.0169951, acc 0.98
2016-09-06T20:26:15.859893: step 5045, loss 0.0110552, acc 1
2016-09-06T20:26:16.570037: step 5046, loss 0.055232, acc 0.98
2016-09-06T20:26:17.244695: step 5047, loss 0.0468262, acc 0.96
2016-09-06T20:26:17.909819: step 5048, loss 0.00642004, acc 1
2016-09-06T20:26:18.614053: step 5049, loss 0.156531, acc 0.98
2016-09-06T20:26:19.278970: step 5050, loss 0.0125888, acc 1
2016-09-06T20:26:19.972974: step 5051, loss 0.00178098, acc 1
2016-09-06T20:26:20.653410: step 5052, loss 0.00180486, acc 1
2016-09-06T20:26:21.353126: step 5053, loss 0.0105605, acc 1
2016-09-06T20:26:22.042976: step 5054, loss 0.00852416, acc 1
2016-09-06T20:26:22.716617: step 5055, loss 0.0336858, acc 0.96
2016-09-06T20:26:23.387778: step 5056, loss 0.00565163, acc 1
2016-09-06T20:26:24.077011: step 5057, loss 0.000658095, acc 1
2016-09-06T20:26:24.769557: step 5058, loss 0.000260989, acc 1
2016-09-06T20:26:25.454417: step 5059, loss 0.00838354, acc 1
2016-09-06T20:26:26.173111: step 5060, loss 0.0241327, acc 0.98
2016-09-06T20:26:26.831667: step 5061, loss 0.033316, acc 1
2016-09-06T20:26:27.530895: step 5062, loss 0.101063, acc 0.98
2016-09-06T20:26:28.231529: step 5063, loss 0.0580963, acc 0.98
2016-09-06T20:26:28.913453: step 5064, loss 0.00611514, acc 1
2016-09-06T20:26:29.598141: step 5065, loss 0.0687526, acc 0.94
2016-09-06T20:26:30.289999: step 5066, loss 0.0431992, acc 1
2016-09-06T20:26:31.006436: step 5067, loss 0.0600077, acc 0.96
2016-09-06T20:26:31.699386: step 5068, loss 0.0119938, acc 1
2016-09-06T20:26:32.387347: step 5069, loss 0.0475083, acc 0.98
2016-09-06T20:26:33.086327: step 5070, loss 0.0121122, acc 1
2016-09-06T20:26:33.767054: step 5071, loss 0.0046164, acc 1
2016-09-06T20:26:34.458798: step 5072, loss 0.06844, acc 0.96
2016-09-06T20:26:35.145342: step 5073, loss 0.0225212, acc 0.98
2016-09-06T20:26:35.847533: step 5074, loss 0.0305092, acc 0.96
2016-09-06T20:26:36.523237: step 5075, loss 0.0147724, acc 1
2016-09-06T20:26:37.213507: step 5076, loss 0.00636721, acc 1
2016-09-06T20:26:37.907481: step 5077, loss 0.0578794, acc 0.98
2016-09-06T20:26:38.597530: step 5078, loss 0.0534448, acc 0.98
2016-09-06T20:26:39.293528: step 5079, loss 0.0450037, acc 0.98
2016-09-06T20:26:39.982842: step 5080, loss 0.0166381, acc 1
2016-09-06T20:26:40.695353: step 5081, loss 0.0455431, acc 0.96
2016-09-06T20:26:41.387689: step 5082, loss 0.0131864, acc 1
2016-09-06T20:26:42.081512: step 5083, loss 0.00485341, acc 1
2016-09-06T20:26:42.776830: step 5084, loss 0.0601575, acc 0.94
2016-09-06T20:26:43.492189: step 5085, loss 0.0237183, acc 0.98
2016-09-06T20:26:44.184861: step 5086, loss 0.00224194, acc 1
2016-09-06T20:26:44.860648: step 5087, loss 0.0474443, acc 0.98
2016-09-06T20:26:45.554790: step 5088, loss 0.0279563, acc 0.98
2016-09-06T20:26:46.226840: step 5089, loss 0.0176726, acc 0.98
2016-09-06T20:26:46.915000: step 5090, loss 0.0149316, acc 0.98
2016-09-06T20:26:47.595120: step 5091, loss 0.0178668, acc 1
2016-09-06T20:26:48.286697: step 5092, loss 0.0198052, acc 1
2016-09-06T20:26:48.985977: step 5093, loss 0.0101885, acc 1
2016-09-06T20:26:49.647846: step 5094, loss 0.0245094, acc 1
2016-09-06T20:26:50.353791: step 5095, loss 0.0652973, acc 0.98
2016-09-06T20:26:51.042112: step 5096, loss 0.0106148, acc 1
2016-09-06T20:26:51.726235: step 5097, loss 0.0337492, acc 0.98
2016-09-06T20:26:52.400780: step 5098, loss 0.0128695, acc 1
2016-09-06T20:26:53.082076: step 5099, loss 0.0163521, acc 1
2016-09-06T20:26:53.787603: step 5100, loss 0.0314931, acc 0.98

Evaluation:
2016-09-06T20:26:56.957907: step 5100, loss 2.21968, acc 0.730769

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-5100

2016-09-06T20:26:58.738207: step 5101, loss 0.0118337, acc 1
2016-09-06T20:26:59.422494: step 5102, loss 0.0334284, acc 0.98
2016-09-06T20:27:00.115991: step 5103, loss 0.00309514, acc 1
2016-09-06T20:27:00.823282: step 5104, loss 0.139751, acc 0.96
2016-09-06T20:27:01.515928: step 5105, loss 0.0518121, acc 0.96
2016-09-06T20:27:02.241559: step 5106, loss 0.0180054, acc 1
2016-09-06T20:27:02.894418: step 5107, loss 0.00456019, acc 1
2016-09-06T20:27:03.615577: step 5108, loss 0.0135703, acc 1
2016-09-06T20:27:04.305812: step 5109, loss 0.0987743, acc 0.96
2016-09-06T20:27:04.987275: step 5110, loss 0.076639, acc 0.98
2016-09-06T20:27:05.681458: step 5111, loss 0.0714049, acc 0.98
2016-09-06T20:27:06.376551: step 5112, loss 0.000454957, acc 1
2016-09-06T20:27:07.084288: step 5113, loss 0.0252166, acc 1
2016-09-06T20:27:07.769563: step 5114, loss 0.00299874, acc 1
2016-09-06T20:27:08.449958: step 5115, loss 0.00021185, acc 1
2016-09-06T20:27:09.138136: step 5116, loss 0.00790723, acc 1
2016-09-06T20:27:09.827585: step 5117, loss 0.0691637, acc 0.96
2016-09-06T20:27:10.516854: step 5118, loss 0.0509612, acc 0.98
2016-09-06T20:27:11.207715: step 5119, loss 0.0719365, acc 0.96
2016-09-06T20:27:11.916385: step 5120, loss 0.028237, acc 0.98
2016-09-06T20:27:12.610882: step 5121, loss 0.0157632, acc 1
2016-09-06T20:27:13.299428: step 5122, loss 0.0205861, acc 0.98
2016-09-06T20:27:13.994140: step 5123, loss 0.0720272, acc 0.94
2016-09-06T20:27:14.681727: step 5124, loss 0.0206053, acc 0.98
2016-09-06T20:27:15.368374: step 5125, loss 0.0350626, acc 0.98
2016-09-06T20:27:16.059252: step 5126, loss 0.000864216, acc 1
2016-09-06T20:27:16.776836: step 5127, loss 0.0328244, acc 0.96
2016-09-06T20:27:17.496638: step 5128, loss 0.00445957, acc 1
2016-09-06T20:27:18.191906: step 5129, loss 0.00207189, acc 1
2016-09-06T20:27:18.884350: step 5130, loss 0.0195712, acc 0.98
2016-09-06T20:27:19.571854: step 5131, loss 0.0231789, acc 0.98
2016-09-06T20:27:20.248458: step 5132, loss 0.0291578, acc 0.98
2016-09-06T20:27:20.895938: step 5133, loss 0.00803745, acc 1
2016-09-06T20:27:21.603374: step 5134, loss 0.0226877, acc 0.98
2016-09-06T20:27:22.274780: step 5135, loss 0.00872511, acc 1
2016-09-06T20:27:22.964622: step 5136, loss 0.0663347, acc 0.98
2016-09-06T20:27:23.651569: step 5137, loss 0.000202826, acc 1
2016-09-06T20:27:24.332611: step 5138, loss 0.0013085, acc 1
2016-09-06T20:27:25.011831: step 5139, loss 0.0200685, acc 0.98
2016-09-06T20:27:25.693454: step 5140, loss 0.102987, acc 0.96
2016-09-06T20:27:26.403420: step 5141, loss 0.0167718, acc 1
2016-09-06T20:27:27.106449: step 5142, loss 0.0522048, acc 0.96
2016-09-06T20:27:27.791897: step 5143, loss 0.00460738, acc 1
2016-09-06T20:27:28.487174: step 5144, loss 0.00874075, acc 1
2016-09-06T20:27:29.177592: step 5145, loss 0.0106653, acc 1
2016-09-06T20:27:29.888632: step 5146, loss 0.354288, acc 0.92
2016-09-06T20:27:30.603388: step 5147, loss 0.00779481, acc 1
2016-09-06T20:27:31.287386: step 5148, loss 0.000612993, acc 1
2016-09-06T20:27:31.974847: step 5149, loss 0.011398, acc 1
2016-09-06T20:27:32.667950: step 5150, loss 0.0197088, acc 0.98
2016-09-06T20:27:33.378109: step 5151, loss 0.00148467, acc 1
2016-09-06T20:27:34.072526: step 5152, loss 0.0224836, acc 1
2016-09-06T20:27:34.769908: step 5153, loss 0.00851859, acc 1
2016-09-06T20:27:35.411809: step 5154, loss 0.039963, acc 0.96
2016-09-06T20:27:36.094050: step 5155, loss 0.0245061, acc 0.98
2016-09-06T20:27:36.777772: step 5156, loss 0.00104173, acc 1
2016-09-06T20:27:37.442979: step 5157, loss 0.019349, acc 0.98
2016-09-06T20:27:38.121663: step 5158, loss 0.0639689, acc 0.94
2016-09-06T20:27:38.802295: step 5159, loss 0.016061, acc 0.98
2016-09-06T20:27:39.484349: step 5160, loss 0.0196787, acc 1
2016-09-06T20:27:40.133536: step 5161, loss 0.0214489, acc 1
2016-09-06T20:27:40.842512: step 5162, loss 0.0260714, acc 0.98
2016-09-06T20:27:41.521342: step 5163, loss 0.0164035, acc 1
2016-09-06T20:27:42.207957: step 5164, loss 0.0558254, acc 0.98
2016-09-06T20:27:42.912061: step 5165, loss 0.00622746, acc 1
2016-09-06T20:27:43.594728: step 5166, loss 0.0189211, acc 1
2016-09-06T20:27:44.288757: step 5167, loss 0.0676463, acc 0.98
2016-09-06T20:27:44.973887: step 5168, loss 0.0368148, acc 0.98
2016-09-06T20:27:45.681377: step 5169, loss 0.00537265, acc 1
2016-09-06T20:27:46.382479: step 5170, loss 0.189216, acc 0.94
2016-09-06T20:27:47.053051: step 5171, loss 0.0243146, acc 0.98
2016-09-06T20:27:47.737520: step 5172, loss 0.0381046, acc 0.98
2016-09-06T20:27:48.428561: step 5173, loss 0.0270286, acc 0.98
2016-09-06T20:27:49.118004: step 5174, loss 0.00286913, acc 1
2016-09-06T20:27:49.796212: step 5175, loss 0.0178443, acc 0.98
2016-09-06T20:27:50.512897: step 5176, loss 0.0928757, acc 0.96
2016-09-06T20:27:51.186509: step 5177, loss 0.0309697, acc 0.98
2016-09-06T20:27:51.870114: step 5178, loss 0.0299349, acc 0.98
2016-09-06T20:27:52.545858: step 5179, loss 0.0359485, acc 0.98
2016-09-06T20:27:53.226727: step 5180, loss 0.0268456, acc 0.98
2016-09-06T20:27:53.916803: step 5181, loss 0.0179649, acc 1
2016-09-06T20:27:54.585976: step 5182, loss 0.00787967, acc 1
2016-09-06T20:27:55.303006: step 5183, loss 0.0392814, acc 0.98
2016-09-06T20:27:55.951164: step 5184, loss 0.00678246, acc 1
2016-09-06T20:27:56.642313: step 5185, loss 0.0259024, acc 0.98
2016-09-06T20:27:57.330176: step 5186, loss 0.0105807, acc 1
2016-09-06T20:27:58.011947: step 5187, loss 0.0374269, acc 0.96
2016-09-06T20:27:58.696187: step 5188, loss 0.0238171, acc 0.98
2016-09-06T20:27:59.363080: step 5189, loss 0.0292539, acc 0.98
2016-09-06T20:28:00.068525: step 5190, loss 0.00494075, acc 1
2016-09-06T20:28:00.799901: step 5191, loss 0.0285125, acc 0.98
2016-09-06T20:28:01.487893: step 5192, loss 0.0221859, acc 1
2016-09-06T20:28:02.182866: step 5193, loss 0.0219314, acc 0.98
2016-09-06T20:28:02.862331: step 5194, loss 0.0766488, acc 0.98
2016-09-06T20:28:03.551991: step 5195, loss 0.00167042, acc 1
2016-09-06T20:28:04.212298: step 5196, loss 0.0578043, acc 0.96
2016-09-06T20:28:04.909555: step 5197, loss 0.132753, acc 0.94
2016-09-06T20:28:05.585883: step 5198, loss 0.00511898, acc 1
2016-09-06T20:28:06.275486: step 5199, loss 0.0179257, acc 1
2016-09-06T20:28:06.958101: step 5200, loss 0.0257614, acc 1

Evaluation:
2016-09-06T20:28:10.115332: step 5200, loss 1.98482, acc 0.742026

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-5200

2016-09-06T20:28:11.785166: step 5201, loss 0.0134662, acc 1
2016-09-06T20:28:12.480615: step 5202, loss 0.0612801, acc 0.96
2016-09-06T20:28:13.164252: step 5203, loss 0.0219231, acc 1
2016-09-06T20:28:13.828554: step 5204, loss 0.0264379, acc 0.98
2016-09-06T20:28:14.521040: step 5205, loss 0.0496382, acc 0.98
2016-09-06T20:28:15.214904: step 5206, loss 0.0260726, acc 0.98
2016-09-06T20:28:15.883713: step 5207, loss 0.00453154, acc 1
2016-09-06T20:28:16.557981: step 5208, loss 0.0718096, acc 0.96
2016-09-06T20:28:17.278018: step 5209, loss 0.000368041, acc 1
2016-09-06T20:28:17.982854: step 5210, loss 0.0334445, acc 0.98
2016-09-06T20:28:18.648468: step 5211, loss 0.0040104, acc 1
2016-09-06T20:28:19.354388: step 5212, loss 0.015465, acc 0.98
2016-09-06T20:28:20.048185: step 5213, loss 0.0371267, acc 0.98
2016-09-06T20:28:20.733033: step 5214, loss 0.0225825, acc 0.98
2016-09-06T20:28:21.399394: step 5215, loss 0.0194902, acc 0.98
2016-09-06T20:28:22.075033: step 5216, loss 0.0129643, acc 1
2016-09-06T20:28:22.763256: step 5217, loss 0.00545569, acc 1
2016-09-06T20:28:23.415000: step 5218, loss 0.0172576, acc 0.98
2016-09-06T20:28:24.105411: step 5219, loss 0.0467096, acc 0.96
2016-09-06T20:28:24.797604: step 5220, loss 0.034194, acc 0.98
2016-09-06T20:28:25.480683: step 5221, loss 0.023109, acc 0.98
2016-09-06T20:28:26.167135: step 5222, loss 0.0749855, acc 0.98
2016-09-06T20:28:26.850271: step 5223, loss 0.0043596, acc 1
2016-09-06T20:28:27.550425: step 5224, loss 0.0149975, acc 1
2016-09-06T20:28:28.212204: step 5225, loss 0.0264595, acc 0.98
2016-09-06T20:28:28.911012: step 5226, loss 0.0529849, acc 0.96
2016-09-06T20:28:29.576310: step 5227, loss 0.0305136, acc 0.98
2016-09-06T20:28:30.265354: step 5228, loss 0.0160608, acc 1
2016-09-06T20:28:30.964370: step 5229, loss 0.0250237, acc 1
2016-09-06T20:28:31.663248: step 5230, loss 0.0363984, acc 0.98
2016-09-06T20:28:32.361857: step 5231, loss 0.000601626, acc 1
2016-09-06T20:28:33.027388: step 5232, loss 0.00318942, acc 1
2016-09-06T20:28:33.713988: step 5233, loss 0.0677539, acc 0.96
2016-09-06T20:28:34.402614: step 5234, loss 0.000521949, acc 1
2016-09-06T20:28:35.101985: step 5235, loss 0.0753592, acc 0.96
2016-09-06T20:28:35.807032: step 5236, loss 0.163399, acc 0.96
2016-09-06T20:28:36.481623: step 5237, loss 0.00326757, acc 1
2016-09-06T20:28:37.177722: step 5238, loss 0.00016808, acc 1
2016-09-06T20:28:37.836412: step 5239, loss 0.0625733, acc 0.94
2016-09-06T20:28:38.545998: step 5240, loss 0.00743409, acc 1
2016-09-06T20:28:39.233916: step 5241, loss 0.0159309, acc 1
2016-09-06T20:28:39.922538: step 5242, loss 0.0405461, acc 0.96
2016-09-06T20:28:40.587927: step 5243, loss 0.0118426, acc 1
2016-09-06T20:28:41.273100: step 5244, loss 0.0775418, acc 0.96
2016-09-06T20:28:41.963061: step 5245, loss 0.0123596, acc 1
2016-09-06T20:28:42.636445: step 5246, loss 0.0185178, acc 1
2016-09-06T20:28:43.349507: step 5247, loss 0.0317443, acc 0.98
2016-09-06T20:28:44.006241: step 5248, loss 0.0201117, acc 1
2016-09-06T20:28:44.679422: step 5249, loss 0.00569843, acc 1
2016-09-06T20:28:45.368085: step 5250, loss 0.0285915, acc 0.98
2016-09-06T20:28:46.069696: step 5251, loss 0.0097696, acc 1
2016-09-06T20:28:46.768977: step 5252, loss 0.0448843, acc 0.98
2016-09-06T20:28:47.437926: step 5253, loss 0.0365086, acc 0.98
2016-09-06T20:28:48.171501: step 5254, loss 0.0091418, acc 1
2016-09-06T20:28:48.880126: step 5255, loss 0.020199, acc 1
2016-09-06T20:28:49.569428: step 5256, loss 0.124149, acc 0.96
2016-09-06T20:28:50.283559: step 5257, loss 0.040012, acc 0.98
2016-09-06T20:28:50.976102: step 5258, loss 0.0258302, acc 0.98
2016-09-06T20:28:51.677524: step 5259, loss 0.0136609, acc 1
2016-09-06T20:28:52.366219: step 5260, loss 0.0100407, acc 1
2016-09-06T20:28:53.071404: step 5261, loss 0.0369574, acc 1
2016-09-06T20:28:53.764214: step 5262, loss 0.0352512, acc 1
2016-09-06T20:28:54.446323: step 5263, loss 0.0188429, acc 0.98
2016-09-06T20:28:55.129510: step 5264, loss 0.00356219, acc 1
2016-09-06T20:28:55.818839: step 5265, loss 0.00662979, acc 1
2016-09-06T20:28:56.524996: step 5266, loss 0.0791855, acc 0.96
2016-09-06T20:28:57.208193: step 5267, loss 0.00379973, acc 1
2016-09-06T20:28:57.890752: step 5268, loss 0.182515, acc 0.96
2016-09-06T20:28:58.579828: step 5269, loss 0.0132246, acc 1
2016-09-06T20:28:59.263147: step 5270, loss 0.00188648, acc 1
2016-09-06T20:28:59.955094: step 5271, loss 0.0390591, acc 1
2016-09-06T20:29:00.682332: step 5272, loss 0.0174219, acc 0.98
2016-09-06T20:29:01.373967: step 5273, loss 0.0270652, acc 1
2016-09-06T20:29:02.061720: step 5274, loss 0.0197255, acc 1
2016-09-06T20:29:02.773891: step 5275, loss 0.000650519, acc 1
2016-09-06T20:29:03.469732: step 5276, loss 0.00665071, acc 1
2016-09-06T20:29:04.151155: step 5277, loss 0.0343544, acc 0.96
2016-09-06T20:29:04.843396: step 5278, loss 0.0116795, acc 1
2016-09-06T20:29:05.502826: step 5279, loss 0.0406106, acc 0.96
2016-09-06T20:29:06.213868: step 5280, loss 0.0138867, acc 1
2016-09-06T20:29:06.882915: step 5281, loss 0.0235394, acc 0.98
2016-09-06T20:29:07.570113: step 5282, loss 0.0206103, acc 1
2016-09-06T20:29:08.253328: step 5283, loss 0.0538446, acc 0.96
2016-09-06T20:29:08.951129: step 5284, loss 0.0572895, acc 0.96
2016-09-06T20:29:09.641780: step 5285, loss 0.011007, acc 1
2016-09-06T20:29:10.311628: step 5286, loss 0.00621521, acc 1
2016-09-06T20:29:11.010836: step 5287, loss 0.295308, acc 0.94
2016-09-06T20:29:11.689768: step 5288, loss 0.0107254, acc 1
2016-09-06T20:29:12.385924: step 5289, loss 0.01955, acc 0.98
2016-09-06T20:29:13.102488: step 5290, loss 0.0358191, acc 0.98
2016-09-06T20:29:13.786738: step 5291, loss 0.00133967, acc 1
2016-09-06T20:29:14.467797: step 5292, loss 0.105024, acc 0.98
2016-09-06T20:29:15.145407: step 5293, loss 0.0528471, acc 0.98
2016-09-06T20:29:15.876958: step 5294, loss 0.0305434, acc 1
2016-09-06T20:29:16.588470: step 5295, loss 0.0343281, acc 0.98
2016-09-06T20:29:17.281806: step 5296, loss 0.0247584, acc 0.98
2016-09-06T20:29:17.965648: step 5297, loss 0.0252337, acc 1
2016-09-06T20:29:18.628058: step 5298, loss 0.0359433, acc 0.98
2016-09-06T20:29:19.312977: step 5299, loss 0.0316989, acc 1
2016-09-06T20:29:19.965843: step 5300, loss 0.0345362, acc 0.98

Evaluation:
2016-09-06T20:29:23.093543: step 5300, loss 1.8201, acc 0.730769

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-5300

2016-09-06T20:29:24.823400: step 5301, loss 0.0155683, acc 1
2016-09-06T20:29:25.531815: step 5302, loss 0.00690469, acc 1
2016-09-06T20:29:26.220418: step 5303, loss 0.0329582, acc 1
2016-09-06T20:29:26.934869: step 5304, loss 0.00508882, acc 1
2016-09-06T20:29:27.609041: step 5305, loss 0.0268839, acc 0.98
2016-09-06T20:29:28.294104: step 5306, loss 0.0322235, acc 0.98
2016-09-06T20:29:28.997659: step 5307, loss 0.0248936, acc 0.98
2016-09-06T20:29:29.678471: step 5308, loss 0.0269026, acc 0.98
2016-09-06T20:29:30.376297: step 5309, loss 0.0581857, acc 0.96
2016-09-06T20:29:31.082150: step 5310, loss 0.0384844, acc 0.98
2016-09-06T20:29:31.790999: step 5311, loss 0.0137354, acc 1
2016-09-06T20:29:32.480381: step 5312, loss 0.0951802, acc 0.96
2016-09-06T20:29:33.152303: step 5313, loss 0.00544238, acc 1
2016-09-06T20:29:33.865003: step 5314, loss 0.022689, acc 0.98
2016-09-06T20:29:34.537671: step 5315, loss 0.0493071, acc 0.98
2016-09-06T20:29:35.214020: step 5316, loss 0.0296794, acc 0.98
2016-09-06T20:29:35.886460: step 5317, loss 0.0428814, acc 0.98
2016-09-06T20:29:36.553355: step 5318, loss 0.0224821, acc 0.98
2016-09-06T20:29:37.251439: step 5319, loss 0.00185707, acc 1
2016-09-06T20:29:37.938199: step 5320, loss 0.00564742, acc 1
2016-09-06T20:29:38.641243: step 5321, loss 0.000293394, acc 1
2016-09-06T20:29:39.313927: step 5322, loss 0.0260594, acc 0.98
2016-09-06T20:29:39.995094: step 5323, loss 0.0544967, acc 0.96
2016-09-06T20:29:40.667550: step 5324, loss 0.0441913, acc 0.98
2016-09-06T20:29:41.349602: step 5325, loss 0.0177589, acc 0.98
2016-09-06T20:29:42.052004: step 5326, loss 0.204689, acc 0.98
2016-09-06T20:29:42.741741: step 5327, loss 0.0159243, acc 0.98
2016-09-06T20:29:43.448034: step 5328, loss 0.0233066, acc 1
2016-09-06T20:29:44.109708: step 5329, loss 0.0205963, acc 0.98
2016-09-06T20:29:44.807405: step 5330, loss 0.0157543, acc 1
2016-09-06T20:29:45.501035: step 5331, loss 0.0134309, acc 1
2016-09-06T20:29:46.194091: step 5332, loss 0.0178546, acc 1
2016-09-06T20:29:46.894316: step 5333, loss 0.0981729, acc 0.96
2016-09-06T20:29:47.567753: step 5334, loss 0.0170483, acc 0.98
2016-09-06T20:29:48.292776: step 5335, loss 0.0299401, acc 1
2016-09-06T20:29:48.986390: step 5336, loss 0.0186562, acc 1
2016-09-06T20:29:49.688881: step 5337, loss 0.0136261, acc 1
2016-09-06T20:29:50.382674: step 5338, loss 0.0110037, acc 1
2016-09-06T20:29:51.053821: step 5339, loss 0.0296195, acc 0.98
2016-09-06T20:29:51.763288: step 5340, loss 0.000266498, acc 1
2016-09-06T20:29:52.425333: step 5341, loss 0.0056049, acc 1
2016-09-06T20:29:53.137203: step 5342, loss 0.0322507, acc 0.98
2016-09-06T20:29:53.820368: step 5343, loss 0.0229775, acc 1
2016-09-06T20:29:54.506916: step 5344, loss 0.123179, acc 0.98
2016-09-06T20:29:55.203992: step 5345, loss 0.00594529, acc 1
2016-09-06T20:29:55.895077: step 5346, loss 0.00571763, acc 1
2016-09-06T20:29:56.585120: step 5347, loss 0.0554763, acc 0.96
2016-09-06T20:29:57.256356: step 5348, loss 0.00480022, acc 1
2016-09-06T20:29:57.958090: step 5349, loss 0.0185919, acc 0.98
2016-09-06T20:29:58.647600: step 5350, loss 0.030158, acc 1
2016-09-06T20:29:59.355053: step 5351, loss 0.042128, acc 0.98
2016-09-06T20:30:00.060716: step 5352, loss 0.0336991, acc 0.96
2016-09-06T20:30:00.791021: step 5353, loss 0.0338846, acc 0.98
2016-09-06T20:30:01.517906: step 5354, loss 0.0182174, acc 0.98
2016-09-06T20:30:02.233822: step 5355, loss 0.0208335, acc 1
2016-09-06T20:30:02.912539: step 5356, loss 0.00629383, acc 1
2016-09-06T20:30:03.586045: step 5357, loss 0.0292618, acc 0.98
2016-09-06T20:30:04.275239: step 5358, loss 0.0265583, acc 0.98
2016-09-06T20:30:04.983772: step 5359, loss 0.127365, acc 0.96
2016-09-06T20:30:05.642846: step 5360, loss 0.0399676, acc 0.98
2016-09-06T20:30:06.338014: step 5361, loss 0.00774987, acc 1
2016-09-06T20:30:07.008020: step 5362, loss 0.0347967, acc 0.98
2016-09-06T20:30:07.711885: step 5363, loss 0.0692003, acc 0.98
2016-09-06T20:30:08.403005: step 5364, loss 0.00696185, acc 1
2016-09-06T20:30:09.094009: step 5365, loss 0.00138909, acc 1
2016-09-06T20:30:09.804386: step 5366, loss 0.0043294, acc 1
2016-09-06T20:30:10.475408: step 5367, loss 0.001262, acc 1
2016-09-06T20:30:11.187555: step 5368, loss 0.04042, acc 0.96
2016-09-06T20:30:11.879059: step 5369, loss 0.0292337, acc 0.98
2016-09-06T20:30:12.589083: step 5370, loss 0.181167, acc 0.98
2016-09-06T20:30:13.267878: step 5371, loss 0.0269793, acc 0.98
2016-09-06T20:30:13.942985: step 5372, loss 0.00863781, acc 1
2016-09-06T20:30:14.645904: step 5373, loss 0.0215334, acc 0.98
2016-09-06T20:30:15.309143: step 5374, loss 0.00753182, acc 1
2016-09-06T20:30:16.015937: step 5375, loss 0.01404, acc 1
2016-09-06T20:30:16.661797: step 5376, loss 0.000748524, acc 1
2016-09-06T20:30:17.360872: step 5377, loss 0.00230312, acc 1
2016-09-06T20:30:18.060731: step 5378, loss 0.0370518, acc 0.96
2016-09-06T20:30:18.740236: step 5379, loss 0.0441288, acc 0.96
2016-09-06T20:30:19.459899: step 5380, loss 0.00804373, acc 1
2016-09-06T20:30:20.123949: step 5381, loss 0.0151151, acc 1
2016-09-06T20:30:20.829140: step 5382, loss 0.0458143, acc 0.96
2016-09-06T20:30:21.511834: step 5383, loss 0.0213511, acc 1
2016-09-06T20:30:22.193542: step 5384, loss 0.0316012, acc 0.98
2016-09-06T20:30:22.889330: step 5385, loss 0.00293196, acc 1
2016-09-06T20:30:23.564604: step 5386, loss 0.00134807, acc 1
2016-09-06T20:30:24.245524: step 5387, loss 0.100687, acc 0.96
2016-09-06T20:30:24.928049: step 5388, loss 0.0124234, acc 1
2016-09-06T20:30:25.632990: step 5389, loss 0.0352429, acc 1
2016-09-06T20:30:26.343036: step 5390, loss 0.0261767, acc 0.98
2016-09-06T20:30:27.039601: step 5391, loss 0.0173395, acc 1
2016-09-06T20:30:27.725940: step 5392, loss 0.0139845, acc 1
2016-09-06T20:30:28.404211: step 5393, loss 0.0159153, acc 1
2016-09-06T20:30:29.111572: step 5394, loss 0.00328213, acc 1
2016-09-06T20:30:29.785033: step 5395, loss 0.034354, acc 0.98
2016-09-06T20:30:30.484503: step 5396, loss 0.00652373, acc 1
2016-09-06T20:30:31.171997: step 5397, loss 0.0308891, acc 0.98
2016-09-06T20:30:31.880124: step 5398, loss 0.00600984, acc 1
2016-09-06T20:30:32.585693: step 5399, loss 0.0118434, acc 1
2016-09-06T20:30:33.262223: step 5400, loss 0.00376431, acc 1

Evaluation:
2016-09-06T20:30:36.442945: step 5400, loss 2.20905, acc 0.739212

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-5400

2016-09-06T20:30:38.140193: step 5401, loss 0.0170962, acc 0.98
2016-09-06T20:30:38.848790: step 5402, loss 0.024772, acc 0.98
2016-09-06T20:30:39.540686: step 5403, loss 0.0924073, acc 0.98
2016-09-06T20:30:40.224838: step 5404, loss 0.00346236, acc 1
2016-09-06T20:30:40.914278: step 5405, loss 0.0123258, acc 1
2016-09-06T20:30:41.642662: step 5406, loss 0.0144402, acc 1
2016-09-06T20:30:42.351585: step 5407, loss 0.0135182, acc 1
2016-09-06T20:30:43.028138: step 5408, loss 0.00760419, acc 1
2016-09-06T20:30:43.717613: step 5409, loss 0.00170096, acc 1
2016-09-06T20:30:44.415119: step 5410, loss 0.027652, acc 0.98
2016-09-06T20:30:45.125764: step 5411, loss 0.0517675, acc 0.98
2016-09-06T20:30:45.814414: step 5412, loss 0.0136981, acc 1
2016-09-06T20:30:46.476028: step 5413, loss 0.125925, acc 0.98
2016-09-06T20:30:47.183608: step 5414, loss 0.0285156, acc 0.98
2016-09-06T20:30:47.841287: step 5415, loss 0.00230554, acc 1
2016-09-06T20:30:48.527593: step 5416, loss 0.0886318, acc 0.96
2016-09-06T20:30:49.202831: step 5417, loss 0.0390469, acc 0.98
2016-09-06T20:30:49.890943: step 5418, loss 0.050275, acc 0.98
2016-09-06T20:30:50.581097: step 5419, loss 0.0178111, acc 1
2016-09-06T20:30:51.274068: step 5420, loss 0.00447988, acc 1
2016-09-06T20:30:52.018317: step 5421, loss 0.00366804, acc 1
2016-09-06T20:30:52.722557: step 5422, loss 0.0181005, acc 1
2016-09-06T20:30:53.403639: step 5423, loss 0.00587276, acc 1
2016-09-06T20:30:54.096193: step 5424, loss 0.0185921, acc 0.98
2016-09-06T20:30:54.785214: step 5425, loss 0.0202322, acc 1
2016-09-06T20:30:55.477536: step 5426, loss 0.00504729, acc 1
2016-09-06T20:30:56.142186: step 5427, loss 0.0137977, acc 1
2016-09-06T20:30:56.874287: step 5428, loss 0.00165503, acc 1
2016-09-06T20:30:57.552883: step 5429, loss 0.0174473, acc 1
2016-09-06T20:30:58.247547: step 5430, loss 0.0709928, acc 0.98
2016-09-06T20:30:58.945495: step 5431, loss 0.0649722, acc 0.96
2016-09-06T20:30:59.628862: step 5432, loss 0.04917, acc 0.98
2016-09-06T20:31:00.325687: step 5433, loss 0.00565024, acc 1
2016-09-06T20:31:00.999912: step 5434, loss 0.119794, acc 0.98
2016-09-06T20:31:01.705161: step 5435, loss 0.0427222, acc 0.98
2016-09-06T20:31:02.390845: step 5436, loss 6.04078e-05, acc 1
2016-09-06T20:31:03.070655: step 5437, loss 0.0312375, acc 0.98
2016-09-06T20:31:03.753786: step 5438, loss 0.041604, acc 0.98
2016-09-06T20:31:04.447642: step 5439, loss 0.0179105, acc 1
2016-09-06T20:31:05.156652: step 5440, loss 0.00923464, acc 1
2016-09-06T20:31:05.833483: step 5441, loss 0.0295933, acc 0.98
2016-09-06T20:31:06.529802: step 5442, loss 0.00415616, acc 1
2016-09-06T20:31:07.211384: step 5443, loss 0.0177724, acc 1
2016-09-06T20:31:07.894965: step 5444, loss 0.00934063, acc 1
2016-09-06T20:31:08.584963: step 5445, loss 0.074092, acc 0.94
2016-09-06T20:31:09.267568: step 5446, loss 0.0234165, acc 0.98
2016-09-06T20:31:09.966081: step 5447, loss 0.063477, acc 0.92
2016-09-06T20:31:10.660618: step 5448, loss 0.0121383, acc 1
2016-09-06T20:31:11.364470: step 5449, loss 0.0148041, acc 1
2016-09-06T20:31:12.074189: step 5450, loss 0.0313068, acc 1
2016-09-06T20:31:12.756859: step 5451, loss 0.0264004, acc 0.98
2016-09-06T20:31:13.442929: step 5452, loss 0.0447743, acc 0.98
2016-09-06T20:31:14.137028: step 5453, loss 0.0516493, acc 0.98
2016-09-06T20:31:14.844668: step 5454, loss 0.0277536, acc 0.98
2016-09-06T20:31:15.522016: step 5455, loss 0.0449239, acc 0.98
2016-09-06T20:31:16.213737: step 5456, loss 0.00589858, acc 1
2016-09-06T20:31:16.906065: step 5457, loss 0.0264986, acc 1
2016-09-06T20:31:17.589784: step 5458, loss 0.0458722, acc 0.98
2016-09-06T20:31:18.262155: step 5459, loss 0.000316717, acc 1
2016-09-06T20:31:18.958008: step 5460, loss 0.0144037, acc 1
2016-09-06T20:31:19.663785: step 5461, loss 0.0247295, acc 0.98
2016-09-06T20:31:20.329865: step 5462, loss 8.76072e-05, acc 1
2016-09-06T20:31:21.011134: step 5463, loss 0.00162449, acc 1
2016-09-06T20:31:21.695822: step 5464, loss 0.00497623, acc 1
2016-09-06T20:31:22.395518: step 5465, loss 0.0218976, acc 0.98
2016-09-06T20:31:23.074194: step 5466, loss 0.0194606, acc 1
2016-09-06T20:31:23.745778: step 5467, loss 0.00104392, acc 1
2016-09-06T20:31:24.448306: step 5468, loss 0.0211342, acc 0.98
2016-09-06T20:31:25.141244: step 5469, loss 0.0117996, acc 1
2016-09-06T20:31:25.846548: step 5470, loss 0.0173549, acc 1
2016-09-06T20:31:26.544939: step 5471, loss 0.0476295, acc 0.98
2016-09-06T20:31:27.236362: step 5472, loss 0.0103436, acc 1
2016-09-06T20:31:27.930252: step 5473, loss 0.0141689, acc 0.98
2016-09-06T20:31:28.619999: step 5474, loss 0.0234321, acc 0.98
2016-09-06T20:31:29.334374: step 5475, loss 0.0194115, acc 1
2016-09-06T20:31:30.012217: step 5476, loss 0.0645489, acc 0.96
2016-09-06T20:31:30.709443: step 5477, loss 0.00335334, acc 1
2016-09-06T20:31:31.421600: step 5478, loss 0.00105469, acc 1
2016-09-06T20:31:32.129649: step 5479, loss 0.0114488, acc 1
2016-09-06T20:31:32.861796: step 5480, loss 0.00402737, acc 1
2016-09-06T20:31:33.537384: step 5481, loss 0.0252303, acc 0.98
2016-09-06T20:31:34.218257: step 5482, loss 0.0747765, acc 0.98
2016-09-06T20:31:34.912910: step 5483, loss 0.0039883, acc 1
2016-09-06T20:31:35.604626: step 5484, loss 0.0199303, acc 0.98
2016-09-06T20:31:36.295721: step 5485, loss 0.00709571, acc 1
2016-09-06T20:31:36.986669: step 5486, loss 0.0956989, acc 0.96
2016-09-06T20:31:37.691265: step 5487, loss 0.0614234, acc 0.96
2016-09-06T20:31:38.366859: step 5488, loss 0.0355995, acc 0.98
2016-09-06T20:31:39.048356: step 5489, loss 0.0115065, acc 1
2016-09-06T20:31:39.745019: step 5490, loss 0.0233079, acc 0.98
2016-09-06T20:31:40.422264: step 5491, loss 0.0788936, acc 0.96
2016-09-06T20:31:41.121152: step 5492, loss 0.000403977, acc 1
2016-09-06T20:31:41.797199: step 5493, loss 3.49783e-05, acc 1
2016-09-06T20:31:42.488011: step 5494, loss 0.00124027, acc 1
2016-09-06T20:31:43.162397: step 5495, loss 0.00808659, acc 1
2016-09-06T20:31:43.833840: step 5496, loss 0.0306761, acc 0.98
2016-09-06T20:31:44.533088: step 5497, loss 0.0118232, acc 1
2016-09-06T20:31:45.210445: step 5498, loss 0.0173867, acc 1
2016-09-06T20:31:45.902379: step 5499, loss 0.00365876, acc 1
2016-09-06T20:31:46.570898: step 5500, loss 0.0568979, acc 0.98

Evaluation:
2016-09-06T20:31:49.742959: step 5500, loss 2.23323, acc 0.728893

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-5500

2016-09-06T20:31:51.415911: step 5501, loss 0.00100958, acc 1
2016-09-06T20:31:52.139370: step 5502, loss 0.0214566, acc 1
2016-09-06T20:31:52.797392: step 5503, loss 0.0367551, acc 0.98
2016-09-06T20:31:53.477898: step 5504, loss 0.0478968, acc 0.96
2016-09-06T20:31:54.148659: step 5505, loss 0.0070339, acc 1
2016-09-06T20:31:54.828087: step 5506, loss 0.0186358, acc 1
2016-09-06T20:31:55.539159: step 5507, loss 0.0742499, acc 0.92
2016-09-06T20:31:56.248646: step 5508, loss 0.019674, acc 1
2016-09-06T20:31:56.957805: step 5509, loss 0.0333369, acc 1
2016-09-06T20:31:57.642608: step 5510, loss 0.0371601, acc 0.98
2016-09-06T20:31:58.323280: step 5511, loss 0.0183935, acc 0.98
2016-09-06T20:31:59.028483: step 5512, loss 0.0584383, acc 0.98
2016-09-06T20:31:59.716888: step 5513, loss 0.0153315, acc 1
2016-09-06T20:32:00.428529: step 5514, loss 0.00581621, acc 1
2016-09-06T20:32:01.120065: step 5515, loss 0.0153563, acc 1
2016-09-06T20:32:01.844184: step 5516, loss 0.0193227, acc 0.98
2016-09-06T20:32:02.548560: step 5517, loss 0.0321681, acc 0.98
2016-09-06T20:32:03.253234: step 5518, loss 0.00167583, acc 1
2016-09-06T20:32:03.940238: step 5519, loss 0.0325505, acc 0.98
2016-09-06T20:32:04.625839: step 5520, loss 0.0163294, acc 1
2016-09-06T20:32:05.327198: step 5521, loss 0.179505, acc 0.92
2016-09-06T20:32:05.999944: step 5522, loss 0.00620322, acc 1
2016-09-06T20:32:06.695819: step 5523, loss 0.0228459, acc 0.98
2016-09-06T20:32:07.378760: step 5524, loss 0.0250303, acc 1
2016-09-06T20:32:08.073426: step 5525, loss 0.0276383, acc 1
2016-09-06T20:32:08.753793: step 5526, loss 0.0285617, acc 0.98
2016-09-06T20:32:09.442189: step 5527, loss 0.032241, acc 1
2016-09-06T20:32:10.194798: step 5528, loss 0.0290694, acc 0.98
2016-09-06T20:32:10.897769: step 5529, loss 5.22743e-05, acc 1
2016-09-06T20:32:11.576637: step 5530, loss 0.0532143, acc 0.96
2016-09-06T20:32:12.270976: step 5531, loss 0.0208472, acc 1
2016-09-06T20:32:12.967056: step 5532, loss 0.00160387, acc 1
2016-09-06T20:32:13.679887: step 5533, loss 0.00858302, acc 1
2016-09-06T20:32:14.334972: step 5534, loss 0.0304416, acc 0.98
2016-09-06T20:32:15.065223: step 5535, loss 0.0020775, acc 1
2016-09-06T20:32:15.749881: step 5536, loss 0.0131663, acc 1
2016-09-06T20:32:16.443368: step 5537, loss 3.97873e-05, acc 1
2016-09-06T20:32:17.128234: step 5538, loss 0.0160006, acc 1
2016-09-06T20:32:17.819524: step 5539, loss 0.0303698, acc 0.98
2016-09-06T20:32:18.527359: step 5540, loss 0.0104151, acc 1
2016-09-06T20:32:19.216033: step 5541, loss 0.00282562, acc 1
2016-09-06T20:32:19.897257: step 5542, loss 0.0235882, acc 1
2016-09-06T20:32:20.595287: step 5543, loss 0.012346, acc 1
2016-09-06T20:32:21.290463: step 5544, loss 0.0380238, acc 1
2016-09-06T20:32:22.000759: step 5545, loss 0.0512311, acc 0.98
2016-09-06T20:32:22.701061: step 5546, loss 0.0418739, acc 1
2016-09-06T20:32:23.413781: step 5547, loss 0.013987, acc 1
2016-09-06T20:32:24.088681: step 5548, loss 0.0347366, acc 0.96
2016-09-06T20:32:24.763604: step 5549, loss 0.082687, acc 0.94
2016-09-06T20:32:25.463141: step 5550, loss 0.0339869, acc 0.98
2016-09-06T20:32:26.153768: step 5551, loss 0.00111567, acc 1
2016-09-06T20:32:26.852327: step 5552, loss 0.039355, acc 0.98
2016-09-06T20:32:27.519833: step 5553, loss 0.0267596, acc 0.98
2016-09-06T20:32:28.212831: step 5554, loss 0.182803, acc 0.96
2016-09-06T20:32:28.888044: step 5555, loss 0.0147147, acc 0.98
2016-09-06T20:32:29.565614: step 5556, loss 0.00011358, acc 1
2016-09-06T20:32:30.255123: step 5557, loss 0.00188139, acc 1
2016-09-06T20:32:30.946395: step 5558, loss 0.000800629, acc 1
2016-09-06T20:32:31.632525: step 5559, loss 0.00234124, acc 1
2016-09-06T20:32:32.325437: step 5560, loss 0.0245176, acc 0.98
2016-09-06T20:32:33.019490: step 5561, loss 0.0185832, acc 0.98
2016-09-06T20:32:33.708314: step 5562, loss 0.0165777, acc 1
2016-09-06T20:32:34.388648: step 5563, loss 0.0442314, acc 0.98
2016-09-06T20:32:35.068160: step 5564, loss 0.0130378, acc 1
2016-09-06T20:32:35.754440: step 5565, loss 0.0182489, acc 1
2016-09-06T20:32:36.442582: step 5566, loss 0.0296663, acc 0.98
2016-09-06T20:32:37.142930: step 5567, loss 0.00916289, acc 1
2016-09-06T20:32:37.796819: step 5568, loss 0.00117229, acc 1
2016-09-06T20:32:38.494814: step 5569, loss 0.0274645, acc 0.98
2016-09-06T20:32:39.222422: step 5570, loss 0.0192817, acc 1
2016-09-06T20:32:39.926157: step 5571, loss 0.0646674, acc 0.96
2016-09-06T20:32:40.614003: step 5572, loss 0.0487704, acc 0.98
2016-09-06T20:32:41.308119: step 5573, loss 0.0542761, acc 0.98
2016-09-06T20:32:41.980807: step 5574, loss 0.00820358, acc 1
2016-09-06T20:32:42.673574: step 5575, loss 0.00911284, acc 1
2016-09-06T20:32:43.353747: step 5576, loss 0.00111834, acc 1
2016-09-06T20:32:44.039022: step 5577, loss 0.0281844, acc 0.98
2016-09-06T20:32:44.739586: step 5578, loss 0.0147702, acc 0.98
2016-09-06T20:32:45.430626: step 5579, loss 0.0558753, acc 0.98
2016-09-06T20:32:46.124829: step 5580, loss 0.0151957, acc 0.98
2016-09-06T20:32:46.795189: step 5581, loss 0.0262715, acc 0.98
2016-09-06T20:32:47.491704: step 5582, loss 0.0574217, acc 0.98
2016-09-06T20:32:48.162576: step 5583, loss 0.024486, acc 0.98
2016-09-06T20:32:48.866825: step 5584, loss 0.00165265, acc 1
2016-09-06T20:32:49.574626: step 5585, loss 0.0832128, acc 0.96
2016-09-06T20:32:50.279508: step 5586, loss 0.00217561, acc 1
2016-09-06T20:32:51.006648: step 5587, loss 0.0434323, acc 0.98
2016-09-06T20:32:51.680916: step 5588, loss 0.00208971, acc 1
2016-09-06T20:32:52.374936: step 5589, loss 0.0400446, acc 0.98
2016-09-06T20:32:53.055237: step 5590, loss 0.0281265, acc 0.98
2016-09-06T20:32:53.749676: step 5591, loss 0.105006, acc 0.96
2016-09-06T20:32:54.449752: step 5592, loss 0.053404, acc 0.96
2016-09-06T20:32:55.164507: step 5593, loss 0.0166635, acc 0.98
2016-09-06T20:32:55.865787: step 5594, loss 0.0537698, acc 0.96
2016-09-06T20:32:56.530998: step 5595, loss 0.0457207, acc 0.98
2016-09-06T20:32:57.221228: step 5596, loss 0.00101808, acc 1
2016-09-06T20:32:57.907769: step 5597, loss 0.0364854, acc 0.98
2016-09-06T20:32:58.626577: step 5598, loss 0.0435295, acc 0.98
2016-09-06T20:32:59.306266: step 5599, loss 0.0335002, acc 1
2016-09-06T20:33:00.003422: step 5600, loss 0.00137393, acc 1

Evaluation:
2016-09-06T20:33:03.183622: step 5600, loss 2.57711, acc 0.725141

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-5600

2016-09-06T20:33:04.911574: step 5601, loss 0.000236038, acc 1
2016-09-06T20:33:05.614702: step 5602, loss 0.0458262, acc 0.98
2016-09-06T20:33:06.303382: step 5603, loss 0.0980103, acc 0.94
2016-09-06T20:33:06.981523: step 5604, loss 0.0699161, acc 0.96
2016-09-06T20:33:07.677523: step 5605, loss 0.000117818, acc 1
2016-09-06T20:33:08.353272: step 5606, loss 0.0977145, acc 0.98
2016-09-06T20:33:09.071534: step 5607, loss 0.0108402, acc 1
2016-09-06T20:33:09.756379: step 5608, loss 0.0281742, acc 0.98
2016-09-06T20:33:10.458102: step 5609, loss 0.0140423, acc 1
2016-09-06T20:33:11.149660: step 5610, loss 0.016124, acc 1
2016-09-06T20:33:11.834303: step 5611, loss 0.0328541, acc 1
2016-09-06T20:33:12.534797: step 5612, loss 0.0303084, acc 0.98
2016-09-06T20:33:13.232695: step 5613, loss 0.0446601, acc 0.98
2016-09-06T20:33:13.956232: step 5614, loss 0.0463798, acc 0.98
2016-09-06T20:33:14.646434: step 5615, loss 0.000305416, acc 1
2016-09-06T20:33:15.338264: step 5616, loss 0.00100765, acc 1
2016-09-06T20:33:16.002573: step 5617, loss 0.0523109, acc 0.96
2016-09-06T20:33:16.681127: step 5618, loss 0.0116251, acc 1
2016-09-06T20:33:17.373111: step 5619, loss 0.0115348, acc 1
2016-09-06T20:33:18.036020: step 5620, loss 0.0734551, acc 0.98
2016-09-06T20:33:18.751765: step 5621, loss 0.00535972, acc 1
2016-09-06T20:33:19.425264: step 5622, loss 0.0325995, acc 0.98
2016-09-06T20:33:20.099246: step 5623, loss 0.0860851, acc 0.96
2016-09-06T20:33:20.781909: step 5624, loss 0.000653252, acc 1
2016-09-06T20:33:21.459125: step 5625, loss 0.0386995, acc 0.98
2016-09-06T20:33:22.155061: step 5626, loss 0.0858375, acc 0.98
2016-09-06T20:33:22.842697: step 5627, loss 0.0296476, acc 1
2016-09-06T20:33:23.551193: step 5628, loss 0.00882639, acc 1
2016-09-06T20:33:24.236107: step 5629, loss 0.000155863, acc 1
2016-09-06T20:33:24.908005: step 5630, loss 0.0413024, acc 0.96
2016-09-06T20:33:25.636443: step 5631, loss 0.0104653, acc 1
2016-09-06T20:33:26.314185: step 5632, loss 0.0187089, acc 1
2016-09-06T20:33:27.002544: step 5633, loss 0.0174302, acc 0.98
2016-09-06T20:33:27.661745: step 5634, loss 0.0127842, acc 1
2016-09-06T20:33:28.366926: step 5635, loss 0.00489458, acc 1
2016-09-06T20:33:29.057829: step 5636, loss 0.020036, acc 1
2016-09-06T20:33:29.733912: step 5637, loss 0.0158528, acc 1
2016-09-06T20:33:30.431685: step 5638, loss 0.00692669, acc 1
2016-09-06T20:33:31.123799: step 5639, loss 0.005383, acc 1
2016-09-06T20:33:31.805751: step 5640, loss 0.0117421, acc 1
2016-09-06T20:33:32.464865: step 5641, loss 0.0433828, acc 0.98
2016-09-06T20:33:33.182514: step 5642, loss 0.0300846, acc 0.98
2016-09-06T20:33:33.862684: step 5643, loss 0.0181798, acc 0.98
2016-09-06T20:33:34.547233: step 5644, loss 0.0111637, acc 1
2016-09-06T20:33:35.246226: step 5645, loss 0.0472357, acc 0.96
2016-09-06T20:33:35.919895: step 5646, loss 0.022686, acc 0.98
2016-09-06T20:33:36.612218: step 5647, loss 0.0280097, acc 0.98
2016-09-06T20:33:37.277359: step 5648, loss 0.0607302, acc 0.96
2016-09-06T20:33:37.989663: step 5649, loss 0.0152893, acc 0.98
2016-09-06T20:33:38.679035: step 5650, loss 0.00765456, acc 1
2016-09-06T20:33:39.368098: step 5651, loss 0.03231, acc 1
2016-09-06T20:33:40.059428: step 5652, loss 0.000159437, acc 1
2016-09-06T20:33:40.753298: step 5653, loss 0.00519799, acc 1
2016-09-06T20:33:41.436102: step 5654, loss 0.0125969, acc 1
2016-09-06T20:33:42.135902: step 5655, loss 0.0392603, acc 1
2016-09-06T20:33:42.852737: step 5656, loss 0.0332901, acc 0.98
2016-09-06T20:33:43.518696: step 5657, loss 0.0716433, acc 0.96
2016-09-06T20:33:44.193614: step 5658, loss 0.0792569, acc 0.98
2016-09-06T20:33:44.883253: step 5659, loss 0.000638342, acc 1
2016-09-06T20:33:45.573364: step 5660, loss 0.0728533, acc 0.98
2016-09-06T20:33:46.277144: step 5661, loss 0.0405256, acc 0.96
2016-09-06T20:33:46.962425: step 5662, loss 0.0512098, acc 0.98
2016-09-06T20:33:47.683210: step 5663, loss 0.00760289, acc 1
2016-09-06T20:33:48.354308: step 5664, loss 0.0351379, acc 0.96
2016-09-06T20:33:49.033619: step 5665, loss 0.00386218, acc 1
2016-09-06T20:33:49.706571: step 5666, loss 0.0221384, acc 0.98
2016-09-06T20:33:50.382222: step 5667, loss 0.0246891, acc 1
2016-09-06T20:33:51.053158: step 5668, loss 0.0206751, acc 0.98
2016-09-06T20:33:51.745215: step 5669, loss 0.100042, acc 0.98
2016-09-06T20:33:52.447242: step 5670, loss 4.51427e-05, acc 1
2016-09-06T20:33:53.126797: step 5671, loss 0.014831, acc 1
2016-09-06T20:33:53.811315: step 5672, loss 0.0148656, acc 1
2016-09-06T20:33:54.504519: step 5673, loss 0.015778, acc 1
2016-09-06T20:33:55.174188: step 5674, loss 0.0472566, acc 0.98
2016-09-06T20:33:55.865801: step 5675, loss 0.00880104, acc 1
2016-09-06T20:33:56.547493: step 5676, loss 0.0100564, acc 1
2016-09-06T20:33:57.253328: step 5677, loss 0.0326826, acc 0.96
2016-09-06T20:33:57.937803: step 5678, loss 0.0486738, acc 0.96
2016-09-06T20:33:58.638389: step 5679, loss 0.0132639, acc 1
2016-09-06T20:33:59.319023: step 5680, loss 0.0398584, acc 0.96
2016-09-06T20:33:59.998941: step 5681, loss 0.0120766, acc 1
2016-09-06T20:34:00.727169: step 5682, loss 0.106159, acc 0.96
2016-09-06T20:34:01.373630: step 5683, loss 0.058719, acc 0.96
2016-09-06T20:34:02.092221: step 5684, loss 0.0265959, acc 0.98
2016-09-06T20:34:02.763750: step 5685, loss 0.0226326, acc 0.98
2016-09-06T20:34:03.449062: step 5686, loss 0.011771, acc 1
2016-09-06T20:34:04.128269: step 5687, loss 0.0275908, acc 0.98
2016-09-06T20:34:04.826711: step 5688, loss 0.0800534, acc 0.94
2016-09-06T20:34:05.525389: step 5689, loss 0.0657433, acc 0.96
2016-09-06T20:34:06.213266: step 5690, loss 0.010117, acc 1
2016-09-06T20:34:06.926911: step 5691, loss 0.00117568, acc 1
2016-09-06T20:34:07.617243: step 5692, loss 0.0467291, acc 0.98
2016-09-06T20:34:08.300784: step 5693, loss 0.0755865, acc 0.98
2016-09-06T20:34:08.997970: step 5694, loss 0.083648, acc 0.94
2016-09-06T20:34:09.687671: step 5695, loss 0.00150466, acc 1
2016-09-06T20:34:10.404624: step 5696, loss 0.00251132, acc 1
2016-09-06T20:34:11.068633: step 5697, loss 0.00174056, acc 1
2016-09-06T20:34:11.760014: step 5698, loss 0.0868825, acc 0.94
2016-09-06T20:34:12.462642: step 5699, loss 0.115434, acc 0.92
2016-09-06T20:34:13.155786: step 5700, loss 0.0261904, acc 0.98

Evaluation:
2016-09-06T20:34:16.294829: step 5700, loss 2.03718, acc 0.733584

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473161068/checkpoints/model-5700

2016-09-06T20:34:18.080741: step 5701, loss 0.078534, acc 0.96
2016-09-06T20:34:18.777863: step 5702, loss 0.014571, acc 0.98
2016-09-06T20:34:19.458497: step 5703, loss 0.0323153, acc 0.98
2016-09-06T20:34:20.169861: step 5704, loss 0.0122553, acc 1
2016-09-06T20:34:20.877585: step 5705, loss 0.00734033, acc 1
2016-09-06T20:34:21.572993: step 5706, loss 0.0210379, acc 0.98
2016-09-06T20:34:22.259065: step 5707, loss 0.0177976, acc 0.98
2016-09-06T20:34:22.951161: step 5708, loss 0.0246671, acc 1
2016-09-06T20:34:23.682141: step 5709, loss 0.0386154, acc 0.98
2016-09-06T20:34:24.348454: step 5710, loss 0.0182147, acc 1
2016-09-06T20:34:25.043615: step 5711, loss 0.0145419, acc 1
2016-09-06T20:34:25.704898: step 5712, loss 0.0223644, acc 1
2016-09-06T20:34:26.368930: step 5713, loss 0.0279022, acc 0.98
2016-09-06T20:34:27.049553: step 5714, loss 0.00449196, acc 1
2016-09-06T20:34:27.716818: step 5715, loss 0.000600891, acc 1
2016-09-06T20:34:28.397699: step 5716, loss 0.0871335, acc 0.94
2016-09-06T20:34:29.064622: step 5717, loss 0.0317101, acc 0.98
2016-09-06T20:34:29.800360: step 5718, loss 0.00289858, acc 1
2016-09-06T20:34:30.500725: step 5719, loss 0.00648303, acc 1
2016-09-06T20:34:31.195527: step 5720, loss 0.0149463, acc 0.98
2016-09-06T20:34:31.875060: step 5721, loss 0.0137597, acc 1
2016-09-06T20:34:32.564562: step 5722, loss 0.147494, acc 0.96
2016-09-06T20:34:33.263468: step 5723, loss 0.000607977, acc 1
2016-09-06T20:34:33.931426: step 5724, loss 0.000238411, acc 1
2016-09-06T20:34:34.631196: step 5725, loss 0.0018179, acc 1
2016-09-06T20:34:35.331610: step 5726, loss 0.046443, acc 1
2016-09-06T20:34:36.033511: step 5727, loss 0.0332202, acc 0.98
2016-09-06T20:34:36.713204: step 5728, loss 0.0250659, acc 0.98
2016-09-06T20:34:37.377223: step 5729, loss 0.000218782, acc 1
2016-09-06T20:34:38.109193: step 5730, loss 0.00520794, acc 1
2016-09-06T20:34:38.779906: step 5731, loss 0.0606313, acc 0.98
2016-09-06T20:34:39.480848: step 5732, loss 0.0237677, acc 0.98
2016-09-06T20:34:40.167538: step 5733, loss 0.00586319, acc 1
2016-09-06T20:34:40.852919: step 5734, loss 0.0317355, acc 0.98
2016-09-06T20:34:41.534884: step 5735, loss 0.00165519, acc 1
2016-09-06T20:34:42.214524: step 5736, loss 0.0162768, acc 0.98
2016-09-06T20:34:42.924348: step 5737, loss 0.0300372, acc 0.98
2016-09-06T20:34:43.587746: step 5738, loss 0.0112803, acc 1
2016-09-06T20:34:44.278036: step 5739, loss 0.0107011, acc 1
2016-09-06T20:34:44.971240: step 5740, loss 0.017596, acc 1
2016-09-06T20:34:45.656309: step 5741, loss 0.0168764, acc 0.98
2016-09-06T20:34:46.351960: step 5742, loss 0.040398, acc 0.98
2016-09-06T20:34:47.045408: step 5743, loss 0.00976203, acc 1
2016-09-06T20:34:47.747625: step 5744, loss 0.0809665, acc 0.96
2016-09-06T20:34:48.413331: step 5745, loss 0.00179853, acc 1
2016-09-06T20:34:49.107774: step 5746, loss 0.00454061, acc 1
2016-09-06T20:34:49.798342: step 5747, loss 0.00890825, acc 1
2016-09-06T20:34:50.486466: step 5748, loss 0.0146696, acc 1
2016-09-06T20:34:51.178542: step 5749, loss 0.0100279, acc 1
2016-09-06T20:34:51.879622: step 5750, loss 0.00586869, acc 1
2016-09-06T20:34:52.611437: step 5751, loss 0.0194535, acc 0.98
2016-09-06T20:34:53.325844: step 5752, loss 0.0433977, acc 0.98
2016-09-06T20:34:54.006323: step 5753, loss 0.00561244, acc 1
2016-09-06T20:34:54.704545: step 5754, loss 0.0124923, acc 1
2016-09-06T20:34:55.402433: step 5755, loss 0.0342331, acc 1
2016-09-06T20:34:56.082966: step 5756, loss 0.013162, acc 1
2016-09-06T20:34:56.749707: step 5757, loss 0.0297568, acc 0.98
2016-09-06T20:34:57.464107: step 5758, loss 0.0299688, acc 0.98
2016-09-06T20:34:58.146710: step 5759, loss 0.00755683, acc 1
2016-09-06T20:34:58.808749: step 5760, loss 0.00304038, acc 1

WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f36eb45be90>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.
WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f36eb45bed0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.

Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=50
CHECKPOINT_EVERY=100
DROPOUT_KEEP_PROB=0.3
EMBEDDING_DIM=300
EVALUATE_EVERY=100
FILTER_SIZES=3,4,5
HIDDEN_DIM=300
L2_REG_LAMBDA=0
LOG_DEVICE_PLACEMENT=False
NUM_EPOCHS=30
NUM_FILTERS=100
WORD2VEC=GoogleNews-vectors-negative300.bin

Loading data...
Vocabulary Size: 18758
Train/Dev split: 9662/1000
Writing to /home/cil/lstm-context-embeddings/runs/1473245614

Load word2vec file GoogleNews-vectors-negative300.bin

2016-09-07T18:53:53.022768: step 1, loss 0.693147, acc 0.44
2016-09-07T18:53:53.688254: step 2, loss 0.722176, acc 0.44
2016-09-07T18:53:54.371289: step 3, loss 0.69604, acc 0.4
2016-09-07T18:53:55.064830: step 4, loss 0.687405, acc 0.54
2016-09-07T18:53:55.752089: step 5, loss 0.653334, acc 0.64
2016-09-07T18:53:56.432002: step 6, loss 0.706612, acc 0.56
2016-09-07T18:53:57.114874: step 7, loss 0.773245, acc 0.46
2016-09-07T18:53:57.778375: step 8, loss 0.765642, acc 0.42
2016-09-07T18:53:58.450790: step 9, loss 0.673319, acc 0.62
2016-09-07T18:53:59.126162: step 10, loss 0.676788, acc 0.54
2016-09-07T18:53:59.786801: step 11, loss 0.692133, acc 0.64
2016-09-07T18:54:00.506880: step 12, loss 0.726789, acc 0.5
2016-09-07T18:54:01.176671: step 13, loss 0.700285, acc 0.54
2016-09-07T18:54:01.846245: step 14, loss 0.690272, acc 0.54
2016-09-07T18:54:02.515404: step 15, loss 0.717231, acc 0.5
2016-09-07T18:54:03.170586: step 16, loss 0.726563, acc 0.5
2016-09-07T18:54:03.838598: step 17, loss 0.698347, acc 0.44
2016-09-07T18:54:04.509807: step 18, loss 0.709, acc 0.54
2016-09-07T18:54:05.168493: step 19, loss 0.678986, acc 0.62
2016-09-07T18:54:05.832612: step 20, loss 0.689995, acc 0.52
2016-09-07T18:54:06.482343: step 21, loss 0.6864, acc 0.58
2016-09-07T18:54:07.145715: step 22, loss 0.713209, acc 0.54
2016-09-07T18:54:07.812834: step 23, loss 0.759906, acc 0.52
2016-09-07T18:54:08.493166: step 24, loss 0.711199, acc 0.54
2016-09-07T18:54:09.167887: step 25, loss 0.695092, acc 0.48
2016-09-07T18:54:09.853474: step 26, loss 0.712914, acc 0.44
2016-09-07T18:54:10.527187: step 27, loss 0.68362, acc 0.64
2016-09-07T18:54:11.187635: step 28, loss 0.703805, acc 0.56
2016-09-07T18:54:11.849650: step 29, loss 0.710985, acc 0.48
2016-09-07T18:54:12.503330: step 30, loss 0.725919, acc 0.54
2016-09-07T18:54:13.160051: step 31, loss 0.656406, acc 0.6
2016-09-07T18:54:13.844006: step 32, loss 0.704569, acc 0.48
2016-09-07T18:54:14.505190: step 33, loss 0.696473, acc 0.54
2016-09-07T18:54:15.170664: step 34, loss 0.66788, acc 0.62
2016-09-07T18:54:15.831631: step 35, loss 0.612397, acc 0.66
2016-09-07T18:54:16.507529: step 36, loss 0.719826, acc 0.48
2016-09-07T18:54:17.182836: step 37, loss 0.589656, acc 0.74
2016-09-07T18:54:17.854588: step 38, loss 0.697864, acc 0.7
2016-09-07T18:54:18.527743: step 39, loss 0.613311, acc 0.7
2016-09-07T18:54:19.190297: step 40, loss 0.672525, acc 0.6
2016-09-07T18:54:19.852594: step 41, loss 0.655852, acc 0.6
2016-09-07T18:54:20.505025: step 42, loss 0.658827, acc 0.58
2016-09-07T18:54:21.171023: step 43, loss 0.785829, acc 0.42
2016-09-07T18:54:21.835669: step 44, loss 0.742344, acc 0.44
2016-09-07T18:54:22.494136: step 45, loss 0.669083, acc 0.6
2016-09-07T18:54:23.152502: step 46, loss 0.638887, acc 0.64
2016-09-07T18:54:23.832988: step 47, loss 0.654787, acc 0.64
2016-09-07T18:54:24.512832: step 48, loss 0.669336, acc 0.56
2016-09-07T18:54:25.185261: step 49, loss 0.628364, acc 0.66
2016-09-07T18:54:25.859274: step 50, loss 0.704943, acc 0.56
2016-09-07T18:54:26.511217: step 51, loss 0.665183, acc 0.58
2016-09-07T18:54:27.184110: step 52, loss 0.628814, acc 0.78
2016-09-07T18:54:27.843538: step 53, loss 0.62071, acc 0.64
2016-09-07T18:54:28.503693: step 54, loss 0.597143, acc 0.74
2016-09-07T18:54:29.182081: step 55, loss 0.62382, acc 0.72
2016-09-07T18:54:29.867556: step 56, loss 0.605277, acc 0.62
2016-09-07T18:54:30.528265: step 57, loss 0.576365, acc 0.74
2016-09-07T18:54:31.192856: step 58, loss 0.642398, acc 0.58
2016-09-07T18:54:31.857291: step 59, loss 0.675182, acc 0.74
2016-09-07T18:54:32.514244: step 60, loss 0.716854, acc 0.62
2016-09-07T18:54:33.202508: step 61, loss 0.620798, acc 0.6
2016-09-07T18:54:33.850871: step 62, loss 0.558128, acc 0.66
2016-09-07T18:54:34.520660: step 63, loss 0.53064, acc 0.78
2016-09-07T18:54:35.192607: step 64, loss 0.536222, acc 0.72
2016-09-07T18:54:35.873225: step 65, loss 0.500733, acc 0.74
2016-09-07T18:54:36.566062: step 66, loss 0.569848, acc 0.66
2016-09-07T18:54:37.234712: step 67, loss 0.564327, acc 0.76
2016-09-07T18:54:37.914034: step 68, loss 0.679632, acc 0.56
2016-09-07T18:54:38.585362: step 69, loss 0.489314, acc 0.8
2016-09-07T18:54:39.235180: step 70, loss 0.559259, acc 0.74
2016-09-07T18:54:39.894451: step 71, loss 0.569347, acc 0.78
2016-09-07T18:54:40.555771: step 72, loss 0.578293, acc 0.7
2016-09-07T18:54:41.235086: step 73, loss 0.555348, acc 0.76
2016-09-07T18:54:41.905364: step 74, loss 0.550064, acc 0.72
2016-09-07T18:54:42.574530: step 75, loss 0.609504, acc 0.64
2016-09-07T18:54:43.234698: step 76, loss 0.675089, acc 0.64
2016-09-07T18:54:43.888326: step 77, loss 0.551475, acc 0.76
2016-09-07T18:54:44.562558: step 78, loss 0.683523, acc 0.56
2016-09-07T18:54:45.220704: step 79, loss 0.551698, acc 0.72
2016-09-07T18:54:45.884873: step 80, loss 0.690702, acc 0.64
2016-09-07T18:54:46.525848: step 81, loss 0.522473, acc 0.8
2016-09-07T18:54:47.178653: step 82, loss 0.525934, acc 0.74
2016-09-07T18:54:47.834951: step 83, loss 0.572714, acc 0.76
2016-09-07T18:54:48.487273: step 84, loss 0.510006, acc 0.74
2016-09-07T18:54:49.151408: step 85, loss 0.569843, acc 0.7
2016-09-07T18:54:49.837022: step 86, loss 0.447984, acc 0.76
2016-09-07T18:54:50.509102: step 87, loss 0.667265, acc 0.62
2016-09-07T18:54:51.170438: step 88, loss 0.420298, acc 0.86
2016-09-07T18:54:51.831344: step 89, loss 0.435584, acc 0.8
2016-09-07T18:54:52.497379: step 90, loss 0.653433, acc 0.68
2016-09-07T18:54:53.182412: step 91, loss 0.685018, acc 0.66
2016-09-07T18:54:53.848226: step 92, loss 0.49035, acc 0.74
2016-09-07T18:54:54.510949: step 93, loss 0.579276, acc 0.7
2016-09-07T18:54:55.167555: step 94, loss 0.459945, acc 0.78
2016-09-07T18:54:55.828191: step 95, loss 0.438721, acc 0.8
2016-09-07T18:54:56.488566: step 96, loss 0.506267, acc 0.74
2016-09-07T18:54:57.154534: step 97, loss 0.563546, acc 0.68
2016-09-07T18:54:57.811372: step 98, loss 0.469138, acc 0.78
2016-09-07T18:54:58.473919: step 99, loss 0.608642, acc 0.64
2016-09-07T18:54:59.145887: step 100, loss 0.590359, acc 0.68

Evaluation:
2016-09-07T18:55:02.072881: step 100, loss 0.517929, acc 0.752

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-100

2016-09-07T18:55:03.752426: step 101, loss 0.545777, acc 0.8
2016-09-07T18:55:04.428410: step 102, loss 0.741245, acc 0.68
2016-09-07T18:55:05.095203: step 103, loss 0.497316, acc 0.72
2016-09-07T18:55:05.746035: step 104, loss 0.524668, acc 0.74
2016-09-07T18:55:06.408043: step 105, loss 0.487992, acc 0.82
2016-09-07T18:55:07.088551: step 106, loss 0.484783, acc 0.78
2016-09-07T18:55:07.768169: step 107, loss 0.560103, acc 0.74
2016-09-07T18:55:08.439998: step 108, loss 0.466267, acc 0.76
2016-09-07T18:55:09.098529: step 109, loss 0.494094, acc 0.82
2016-09-07T18:55:09.759568: step 110, loss 0.46916, acc 0.76
2016-09-07T18:55:10.434954: step 111, loss 0.544566, acc 0.72
2016-09-07T18:55:11.138026: step 112, loss 0.471701, acc 0.76
2016-09-07T18:55:11.804754: step 113, loss 0.501666, acc 0.78
2016-09-07T18:55:12.476688: step 114, loss 0.49185, acc 0.76
2016-09-07T18:55:13.169162: step 115, loss 0.475513, acc 0.78
2016-09-07T18:55:13.830780: step 116, loss 0.392053, acc 0.8
2016-09-07T18:55:14.487792: step 117, loss 0.812681, acc 0.6
2016-09-07T18:55:15.163446: step 118, loss 0.523904, acc 0.76
2016-09-07T18:55:15.857865: step 119, loss 0.572149, acc 0.72
2016-09-07T18:55:16.521658: step 120, loss 0.487663, acc 0.78
2016-09-07T18:55:17.197822: step 121, loss 0.55624, acc 0.78
2016-09-07T18:55:17.845223: step 122, loss 0.435795, acc 0.84
2016-09-07T18:55:18.532190: step 123, loss 0.516081, acc 0.72
2016-09-07T18:55:19.195239: step 124, loss 0.618639, acc 0.72
2016-09-07T18:55:19.885211: step 125, loss 0.753213, acc 0.58
2016-09-07T18:55:20.586955: step 126, loss 0.644446, acc 0.7
2016-09-07T18:55:21.267741: step 127, loss 0.610581, acc 0.66
2016-09-07T18:55:21.936477: step 128, loss 0.465749, acc 0.76
2016-09-07T18:55:22.617198: step 129, loss 0.491243, acc 0.74
2016-09-07T18:55:23.286811: step 130, loss 0.517592, acc 0.7
2016-09-07T18:55:23.959025: step 131, loss 0.530637, acc 0.74
2016-09-07T18:55:24.629866: step 132, loss 0.518027, acc 0.8
2016-09-07T18:55:25.285273: step 133, loss 0.566, acc 0.66
2016-09-07T18:55:25.936927: step 134, loss 0.453165, acc 0.86
2016-09-07T18:55:26.607706: step 135, loss 0.599876, acc 0.62
2016-09-07T18:55:27.272227: step 136, loss 0.551452, acc 0.72
2016-09-07T18:55:27.933666: step 137, loss 0.516458, acc 0.76
2016-09-07T18:55:28.603284: step 138, loss 0.618736, acc 0.72
2016-09-07T18:55:29.270961: step 139, loss 0.477703, acc 0.82
2016-09-07T18:55:29.930611: step 140, loss 0.444075, acc 0.82
2016-09-07T18:55:30.587452: step 141, loss 0.592431, acc 0.68
2016-09-07T18:55:31.249599: step 142, loss 0.479154, acc 0.76
2016-09-07T18:55:31.918487: step 143, loss 0.536783, acc 0.76
2016-09-07T18:55:32.603720: step 144, loss 0.49413, acc 0.7
2016-09-07T18:55:33.271423: step 145, loss 0.518637, acc 0.72
2016-09-07T18:55:33.938547: step 146, loss 0.542683, acc 0.7
2016-09-07T18:55:34.644580: step 147, loss 0.613031, acc 0.62
2016-09-07T18:55:35.315154: step 148, loss 0.371107, acc 0.88
2016-09-07T18:55:35.970433: step 149, loss 0.561666, acc 0.68
2016-09-07T18:55:36.648896: step 150, loss 0.518674, acc 0.74
2016-09-07T18:55:37.324866: step 151, loss 0.663597, acc 0.66
2016-09-07T18:55:37.999987: step 152, loss 0.49813, acc 0.72
2016-09-07T18:55:38.663155: step 153, loss 0.574886, acc 0.68
2016-09-07T18:55:39.327533: step 154, loss 0.598873, acc 0.7
2016-09-07T18:55:39.995125: step 155, loss 0.513652, acc 0.78
2016-09-07T18:55:40.640819: step 156, loss 0.395871, acc 0.82
2016-09-07T18:55:41.303071: step 157, loss 0.512917, acc 0.72
2016-09-07T18:55:41.967321: step 158, loss 0.600004, acc 0.62
2016-09-07T18:55:42.633487: step 159, loss 0.551264, acc 0.8
2016-09-07T18:55:43.287334: step 160, loss 0.582842, acc 0.68
2016-09-07T18:55:43.936359: step 161, loss 0.606605, acc 0.74
2016-09-07T18:55:44.603410: step 162, loss 0.615985, acc 0.6
2016-09-07T18:55:45.272145: step 163, loss 0.480835, acc 0.7
2016-09-07T18:55:45.916689: step 164, loss 0.42457, acc 0.8
2016-09-07T18:55:46.587537: step 165, loss 0.486256, acc 0.78
2016-09-07T18:55:47.265196: step 166, loss 0.4611, acc 0.78
2016-09-07T18:55:47.932112: step 167, loss 0.538187, acc 0.7
2016-09-07T18:55:48.582015: step 168, loss 0.62774, acc 0.64
2016-09-07T18:55:49.245205: step 169, loss 0.587762, acc 0.7
2016-09-07T18:55:49.909518: step 170, loss 0.570763, acc 0.68
2016-09-07T18:55:50.583574: step 171, loss 0.58302, acc 0.68
2016-09-07T18:55:51.243958: step 172, loss 0.553449, acc 0.68
2016-09-07T18:55:51.924232: step 173, loss 0.570229, acc 0.64
2016-09-07T18:55:52.606744: step 174, loss 0.497239, acc 0.74
2016-09-07T18:55:53.274410: step 175, loss 0.496591, acc 0.78
2016-09-07T18:55:53.958380: step 176, loss 0.462646, acc 0.84
2016-09-07T18:55:54.624602: step 177, loss 0.380153, acc 0.84
2016-09-07T18:55:55.278375: step 178, loss 0.510589, acc 0.72
2016-09-07T18:55:55.932448: step 179, loss 0.47047, acc 0.8
2016-09-07T18:55:56.589385: step 180, loss 0.308603, acc 0.88
2016-09-07T18:55:57.247069: step 181, loss 0.6089, acc 0.72
2016-09-07T18:55:57.914990: step 182, loss 0.550589, acc 0.72
2016-09-07T18:55:58.581689: step 183, loss 0.48433, acc 0.8
2016-09-07T18:55:59.232132: step 184, loss 0.652411, acc 0.62
2016-09-07T18:55:59.888312: step 185, loss 0.655345, acc 0.74
2016-09-07T18:56:00.603538: step 186, loss 0.529042, acc 0.76
2016-09-07T18:56:01.281209: step 187, loss 0.562916, acc 0.64
2016-09-07T18:56:01.942237: step 188, loss 0.444669, acc 0.8
2016-09-07T18:56:02.601317: step 189, loss 0.36313, acc 0.88
2016-09-07T18:56:03.258400: step 190, loss 0.465845, acc 0.8
2016-09-07T18:56:03.939693: step 191, loss 0.618358, acc 0.72
2016-09-07T18:56:04.647405: step 192, loss 0.519085, acc 0.66
2016-09-07T18:56:05.301920: step 193, loss 0.453932, acc 0.8
2016-09-07T18:56:05.669885: step 194, loss 0.225784, acc 0.916667
2016-09-07T18:56:06.343196: step 195, loss 0.421241, acc 0.74
2016-09-07T18:56:07.014620: step 196, loss 0.388048, acc 0.86
2016-09-07T18:56:07.679466: step 197, loss 0.317905, acc 0.9
2016-09-07T18:56:08.345448: step 198, loss 0.356448, acc 0.84
2016-09-07T18:56:09.007108: step 199, loss 0.355652, acc 0.86
2016-09-07T18:56:09.676830: step 200, loss 0.411148, acc 0.82

Evaluation:
2016-09-07T18:56:12.559466: step 200, loss 0.470873, acc 0.786

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-200

2016-09-07T18:56:14.175588: step 201, loss 0.361244, acc 0.82
2016-09-07T18:56:14.843403: step 202, loss 0.392723, acc 0.82
2016-09-07T18:56:15.520261: step 203, loss 0.239095, acc 0.9
2016-09-07T18:56:16.180339: step 204, loss 0.531619, acc 0.84
2016-09-07T18:56:16.860542: step 205, loss 0.322429, acc 0.88
2016-09-07T18:56:17.545297: step 206, loss 0.344614, acc 0.82
2016-09-07T18:56:18.213675: step 207, loss 0.612395, acc 0.72
2016-09-07T18:56:18.865146: step 208, loss 0.378557, acc 0.9
2016-09-07T18:56:19.539200: step 209, loss 0.348113, acc 0.9
2016-09-07T18:56:20.202896: step 210, loss 0.373054, acc 0.82
2016-09-07T18:56:20.883241: step 211, loss 0.216602, acc 0.96
2016-09-07T18:56:21.552981: step 212, loss 0.627079, acc 0.78
2016-09-07T18:56:22.224651: step 213, loss 0.304516, acc 0.88
2016-09-07T18:56:22.891252: step 214, loss 0.561863, acc 0.78
2016-09-07T18:56:23.549481: step 215, loss 0.257311, acc 0.88
2016-09-07T18:56:24.216767: step 216, loss 0.524407, acc 0.7
2016-09-07T18:56:24.880396: step 217, loss 0.423116, acc 0.84
2016-09-07T18:56:25.545050: step 218, loss 0.449035, acc 0.8
2016-09-07T18:56:26.202186: step 219, loss 0.355176, acc 0.84
2016-09-07T18:56:26.868742: step 220, loss 0.277626, acc 0.92
2016-09-07T18:56:27.543919: step 221, loss 0.326552, acc 0.8
2016-09-07T18:56:28.221480: step 222, loss 0.379176, acc 0.8
2016-09-07T18:56:28.881661: step 223, loss 0.451195, acc 0.78
2016-09-07T18:56:29.552148: step 224, loss 0.400187, acc 0.82
2016-09-07T18:56:30.216154: step 225, loss 0.501298, acc 0.72
2016-09-07T18:56:30.880851: step 226, loss 0.422356, acc 0.82
2016-09-07T18:56:31.541423: step 227, loss 0.249384, acc 0.92
2016-09-07T18:56:32.220128: step 228, loss 0.338488, acc 0.86
2016-09-07T18:56:32.880792: step 229, loss 0.290267, acc 0.86
2016-09-07T18:56:33.546784: step 230, loss 0.37505, acc 0.74
2016-09-07T18:56:34.210476: step 231, loss 0.317551, acc 0.92
2016-09-07T18:56:34.905504: step 232, loss 0.269912, acc 0.88
2016-09-07T18:56:35.570834: step 233, loss 0.527366, acc 0.74
2016-09-07T18:56:36.253443: step 234, loss 0.543022, acc 0.76
2016-09-07T18:56:36.922385: step 235, loss 0.271258, acc 0.92
2016-09-07T18:56:37.578636: step 236, loss 0.321696, acc 0.9
2016-09-07T18:56:38.233985: step 237, loss 0.487881, acc 0.8
2016-09-07T18:56:38.894212: step 238, loss 0.482426, acc 0.8
2016-09-07T18:56:39.558452: step 239, loss 0.433785, acc 0.8
2016-09-07T18:56:40.232996: step 240, loss 0.388324, acc 0.8
2016-09-07T18:56:40.916332: step 241, loss 0.357218, acc 0.82
2016-09-07T18:56:41.585312: step 242, loss 0.369392, acc 0.82
2016-09-07T18:56:42.260787: step 243, loss 0.451216, acc 0.8
2016-09-07T18:56:42.926654: step 244, loss 0.307578, acc 0.84
2016-09-07T18:56:43.594075: step 245, loss 0.24994, acc 0.88
2016-09-07T18:56:44.251315: step 246, loss 0.357205, acc 0.78
2016-09-07T18:56:44.927453: step 247, loss 0.355605, acc 0.84
2016-09-07T18:56:45.606020: step 248, loss 0.346725, acc 0.86
2016-09-07T18:56:46.268777: step 249, loss 0.416832, acc 0.8
2016-09-07T18:56:46.936907: step 250, loss 0.379603, acc 0.82
2016-09-07T18:56:47.605477: step 251, loss 0.527576, acc 0.74
2016-09-07T18:56:48.268006: step 252, loss 0.365796, acc 0.8
2016-09-07T18:56:48.922255: step 253, loss 0.407693, acc 0.8
2016-09-07T18:56:49.579563: step 254, loss 0.625805, acc 0.72
2016-09-07T18:56:50.229764: step 255, loss 0.442989, acc 0.78
2016-09-07T18:56:50.915640: step 256, loss 0.29616, acc 0.88
2016-09-07T18:56:51.589933: step 257, loss 0.350822, acc 0.86
2016-09-07T18:56:52.266507: step 258, loss 0.528138, acc 0.72
2016-09-07T18:56:52.933616: step 259, loss 0.549773, acc 0.66
2016-09-07T18:56:53.602032: step 260, loss 0.322062, acc 0.88
2016-09-07T18:56:54.279903: step 261, loss 0.2683, acc 0.9
2016-09-07T18:56:54.923533: step 262, loss 0.379309, acc 0.84
2016-09-07T18:56:55.588045: step 263, loss 0.456031, acc 0.78
2016-09-07T18:56:56.271301: step 264, loss 0.380309, acc 0.82
2016-09-07T18:56:56.910089: step 265, loss 0.400129, acc 0.82
2016-09-07T18:56:57.555232: step 266, loss 0.309482, acc 0.84
2016-09-07T18:56:58.229045: step 267, loss 0.283635, acc 0.84
2016-09-07T18:56:58.882116: step 268, loss 0.365958, acc 0.8
2016-09-07T18:56:59.539222: step 269, loss 0.314698, acc 0.88
2016-09-07T18:57:00.249838: step 270, loss 0.262969, acc 0.86
2016-09-07T18:57:00.908636: step 271, loss 0.328883, acc 0.82
2016-09-07T18:57:01.575892: step 272, loss 0.433194, acc 0.8
2016-09-07T18:57:02.229374: step 273, loss 0.327142, acc 0.82
2016-09-07T18:57:02.897503: step 274, loss 0.52122, acc 0.82
2016-09-07T18:57:03.560429: step 275, loss 0.412266, acc 0.82
2016-09-07T18:57:04.226784: step 276, loss 0.477198, acc 0.8
2016-09-07T18:57:04.893034: step 277, loss 0.459157, acc 0.82
2016-09-07T18:57:05.553547: step 278, loss 0.332558, acc 0.88
2016-09-07T18:57:06.223903: step 279, loss 0.460697, acc 0.82
2016-09-07T18:57:06.886735: step 280, loss 0.550022, acc 0.76
2016-09-07T18:57:07.545284: step 281, loss 0.383073, acc 0.78
2016-09-07T18:57:08.207663: step 282, loss 0.31417, acc 0.86
2016-09-07T18:57:08.876380: step 283, loss 0.380465, acc 0.82
2016-09-07T18:57:09.546188: step 284, loss 0.463268, acc 0.82
2016-09-07T18:57:10.219305: step 285, loss 0.446888, acc 0.82
2016-09-07T18:57:10.885593: step 286, loss 0.4217, acc 0.82
2016-09-07T18:57:11.536708: step 287, loss 0.357848, acc 0.8
2016-09-07T18:57:12.190586: step 288, loss 0.360532, acc 0.84
2016-09-07T18:57:12.872690: step 289, loss 0.422793, acc 0.76
2016-09-07T18:57:13.552307: step 290, loss 0.351605, acc 0.82
2016-09-07T18:57:14.204723: step 291, loss 0.272239, acc 0.9
2016-09-07T18:57:14.880769: step 292, loss 0.245656, acc 0.92
2016-09-07T18:57:15.555231: step 293, loss 0.48164, acc 0.74
2016-09-07T18:57:16.222916: step 294, loss 0.317848, acc 0.84
2016-09-07T18:57:16.897662: step 295, loss 0.415837, acc 0.84
2016-09-07T18:57:17.559110: step 296, loss 0.307761, acc 0.82
2016-09-07T18:57:18.214787: step 297, loss 0.304714, acc 0.84
2016-09-07T18:57:18.870807: step 298, loss 0.315702, acc 0.86
2016-09-07T18:57:19.529502: step 299, loss 0.506789, acc 0.78
2016-09-07T18:57:20.194890: step 300, loss 0.233529, acc 0.92

Evaluation:
2016-09-07T18:57:23.065796: step 300, loss 0.461458, acc 0.79

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-300

2016-09-07T18:57:24.646191: step 301, loss 0.401142, acc 0.84
2016-09-07T18:57:25.320658: step 302, loss 0.257347, acc 0.88
2016-09-07T18:57:25.989725: step 303, loss 0.542257, acc 0.88
2016-09-07T18:57:26.661686: step 304, loss 0.29987, acc 0.88
2016-09-07T18:57:27.332059: step 305, loss 0.41015, acc 0.84
2016-09-07T18:57:28.005650: step 306, loss 0.290757, acc 0.9
2016-09-07T18:57:28.688537: step 307, loss 0.361182, acc 0.84
2016-09-07T18:57:29.369190: step 308, loss 0.411533, acc 0.78
2016-09-07T18:57:30.022523: step 309, loss 0.444539, acc 0.82
2016-09-07T18:57:30.687341: step 310, loss 0.330702, acc 0.84
2016-09-07T18:57:31.365248: step 311, loss 0.291607, acc 0.82
2016-09-07T18:57:32.031192: step 312, loss 0.299566, acc 0.86
2016-09-07T18:57:32.706491: step 313, loss 0.41142, acc 0.84
2016-09-07T18:57:33.374429: step 314, loss 0.477007, acc 0.76
2016-09-07T18:57:34.038669: step 315, loss 0.510284, acc 0.7
2016-09-07T18:57:34.705126: step 316, loss 0.368526, acc 0.84
2016-09-07T18:57:35.359944: step 317, loss 0.36371, acc 0.84
2016-09-07T18:57:36.027343: step 318, loss 0.216265, acc 0.98
2016-09-07T18:57:36.729583: step 319, loss 0.241996, acc 0.96
2016-09-07T18:57:37.419600: step 320, loss 0.405853, acc 0.82
2016-09-07T18:57:38.094427: step 321, loss 0.309133, acc 0.88
2016-09-07T18:57:38.770263: step 322, loss 0.352369, acc 0.84
2016-09-07T18:57:39.448257: step 323, loss 0.325051, acc 0.82
2016-09-07T18:57:40.119261: step 324, loss 0.312275, acc 0.84
2016-09-07T18:57:40.789803: step 325, loss 0.507216, acc 0.8
2016-09-07T18:57:41.450099: step 326, loss 0.558596, acc 0.78
2016-09-07T18:57:42.114486: step 327, loss 0.3859, acc 0.8
2016-09-07T18:57:42.796858: step 328, loss 0.521609, acc 0.74
2016-09-07T18:57:43.467283: step 329, loss 0.440029, acc 0.78
2016-09-07T18:57:44.134201: step 330, loss 0.385862, acc 0.86
2016-09-07T18:57:44.789207: step 331, loss 0.28146, acc 0.86
2016-09-07T18:57:45.465155: step 332, loss 0.269209, acc 0.9
2016-09-07T18:57:46.132706: step 333, loss 0.377603, acc 0.8
2016-09-07T18:57:46.799686: step 334, loss 0.253909, acc 0.88
2016-09-07T18:57:47.460040: step 335, loss 0.342703, acc 0.82
2016-09-07T18:57:48.101714: step 336, loss 0.366504, acc 0.84
2016-09-07T18:57:48.765663: step 337, loss 0.398832, acc 0.84
2016-09-07T18:57:49.432544: step 338, loss 0.403395, acc 0.84
2016-09-07T18:57:50.108254: step 339, loss 0.363326, acc 0.86
2016-09-07T18:57:50.808450: step 340, loss 0.30731, acc 0.86
2016-09-07T18:57:51.467662: step 341, loss 0.268722, acc 0.9
2016-09-07T18:57:52.141686: step 342, loss 0.29494, acc 0.84
2016-09-07T18:57:52.809986: step 343, loss 0.334121, acc 0.84
2016-09-07T18:57:53.484030: step 344, loss 0.488718, acc 0.76
2016-09-07T18:57:54.159108: step 345, loss 0.366366, acc 0.88
2016-09-07T18:57:54.843537: step 346, loss 0.548176, acc 0.8
2016-09-07T18:57:55.494110: step 347, loss 0.300779, acc 0.86
2016-09-07T18:57:56.165928: step 348, loss 0.28269, acc 0.9
2016-09-07T18:57:56.839385: step 349, loss 0.505206, acc 0.84
2016-09-07T18:57:57.490823: step 350, loss 0.257185, acc 0.86
2016-09-07T18:57:58.173412: step 351, loss 0.576503, acc 0.76
2016-09-07T18:57:58.832619: step 352, loss 0.420547, acc 0.76
2016-09-07T18:57:59.520081: step 353, loss 0.387103, acc 0.82
2016-09-07T18:58:00.185147: step 354, loss 0.401447, acc 0.72
2016-09-07T18:58:00.878107: step 355, loss 0.429808, acc 0.86
2016-09-07T18:58:01.528980: step 356, loss 0.338453, acc 0.84
2016-09-07T18:58:02.184898: step 357, loss 0.457685, acc 0.78
2016-09-07T18:58:02.863865: step 358, loss 0.334868, acc 0.82
2016-09-07T18:58:03.518685: step 359, loss 0.449993, acc 0.8
2016-09-07T18:58:04.188598: step 360, loss 0.4492, acc 0.76
2016-09-07T18:58:04.852647: step 361, loss 0.459786, acc 0.78
2016-09-07T18:58:05.525336: step 362, loss 0.404232, acc 0.82
2016-09-07T18:58:06.205329: step 363, loss 0.299477, acc 0.9
2016-09-07T18:58:06.885254: step 364, loss 0.326615, acc 0.84
2016-09-07T18:58:07.541725: step 365, loss 0.475188, acc 0.74
2016-09-07T18:58:08.223204: step 366, loss 0.348841, acc 0.88
2016-09-07T18:58:08.879165: step 367, loss 0.339751, acc 0.88
2016-09-07T18:58:09.542900: step 368, loss 0.397467, acc 0.8
2016-09-07T18:58:10.207531: step 369, loss 0.414043, acc 0.82
2016-09-07T18:58:10.865706: step 370, loss 0.496138, acc 0.78
2016-09-07T18:58:11.536472: step 371, loss 0.448615, acc 0.8
2016-09-07T18:58:12.208728: step 372, loss 0.436827, acc 0.82
2016-09-07T18:58:12.877088: step 373, loss 0.615832, acc 0.76
2016-09-07T18:58:13.531421: step 374, loss 0.485812, acc 0.82
2016-09-07T18:58:14.199908: step 375, loss 0.366737, acc 0.84
2016-09-07T18:58:14.885039: step 376, loss 0.354525, acc 0.88
2016-09-07T18:58:15.576120: step 377, loss 0.479441, acc 0.76
2016-09-07T18:58:16.235195: step 378, loss 0.30954, acc 0.88
2016-09-07T18:58:16.883695: step 379, loss 0.378188, acc 0.86
2016-09-07T18:58:17.545274: step 380, loss 0.523205, acc 0.76
2016-09-07T18:58:18.194890: step 381, loss 0.466581, acc 0.74
2016-09-07T18:58:18.863295: step 382, loss 0.36339, acc 0.88
2016-09-07T18:58:19.516155: step 383, loss 0.428459, acc 0.8
2016-09-07T18:58:20.188442: step 384, loss 0.466751, acc 0.76
2016-09-07T18:58:20.838213: step 385, loss 0.379665, acc 0.84
2016-09-07T18:58:21.525736: step 386, loss 0.357132, acc 0.86
2016-09-07T18:58:22.200178: step 387, loss 0.453295, acc 0.78
2016-09-07T18:58:22.553300: step 388, loss 0.528161, acc 0.833333
2016-09-07T18:58:23.212797: step 389, loss 0.396073, acc 0.84
2016-09-07T18:58:23.894795: step 390, loss 0.25905, acc 0.92
2016-09-07T18:58:24.563869: step 391, loss 0.271444, acc 0.92
2016-09-07T18:58:25.252065: step 392, loss 0.209593, acc 0.94
2016-09-07T18:58:25.923488: step 393, loss 0.229368, acc 0.88
2016-09-07T18:58:26.594184: step 394, loss 0.287192, acc 0.86
2016-09-07T18:58:27.271306: step 395, loss 0.272896, acc 0.92
2016-09-07T18:58:27.923713: step 396, loss 0.239338, acc 0.92
2016-09-07T18:58:28.573098: step 397, loss 0.245785, acc 0.92
2016-09-07T18:58:29.243224: step 398, loss 0.265502, acc 0.92
2016-09-07T18:58:29.902022: step 399, loss 0.257301, acc 0.88
2016-09-07T18:58:30.563088: step 400, loss 0.157633, acc 0.96

Evaluation:
2016-09-07T18:58:33.496744: step 400, loss 0.505078, acc 0.779

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-400

2016-09-07T18:58:35.094549: step 401, loss 0.234536, acc 0.9
2016-09-07T18:58:35.772777: step 402, loss 0.270922, acc 0.86
2016-09-07T18:58:36.435570: step 403, loss 0.305647, acc 0.92
2016-09-07T18:58:37.086783: step 404, loss 0.236319, acc 0.88
2016-09-07T18:58:37.743729: step 405, loss 0.281522, acc 0.9
2016-09-07T18:58:38.416208: step 406, loss 0.174449, acc 0.92
2016-09-07T18:58:39.086803: step 407, loss 0.0560887, acc 1
2016-09-07T18:58:39.766360: step 408, loss 0.283989, acc 0.9
2016-09-07T18:58:40.451900: step 409, loss 0.542336, acc 0.82
2016-09-07T18:58:41.133270: step 410, loss 0.219927, acc 0.9
2016-09-07T18:58:41.794979: step 411, loss 0.285744, acc 0.9
2016-09-07T18:58:42.466768: step 412, loss 0.112037, acc 0.96
2016-09-07T18:58:43.144752: step 413, loss 0.311661, acc 0.82
2016-09-07T18:58:43.806457: step 414, loss 0.416408, acc 0.8
2016-09-07T18:58:44.493556: step 415, loss 0.308232, acc 0.88
2016-09-07T18:58:45.164483: step 416, loss 0.246691, acc 0.94
2016-09-07T18:58:45.824941: step 417, loss 0.207235, acc 0.9
2016-09-07T18:58:46.498072: step 418, loss 0.325733, acc 0.84
2016-09-07T18:58:47.184247: step 419, loss 0.123207, acc 0.96
2016-09-07T18:58:47.851062: step 420, loss 0.225089, acc 0.94
2016-09-07T18:58:48.525880: step 421, loss 0.4443, acc 0.84
2016-09-07T18:58:49.215831: step 422, loss 0.262331, acc 0.86
2016-09-07T18:58:49.908745: step 423, loss 0.236677, acc 0.9
2016-09-07T18:58:50.600335: step 424, loss 0.358024, acc 0.86
2016-09-07T18:58:51.278136: step 425, loss 0.244022, acc 0.94
2016-09-07T18:58:51.941745: step 426, loss 0.204003, acc 0.94
2016-09-07T18:58:52.598682: step 427, loss 0.338148, acc 0.88
2016-09-07T18:58:53.254174: step 428, loss 0.417466, acc 0.82
2016-09-07T18:58:53.923857: step 429, loss 0.214879, acc 0.9
2016-09-07T18:58:54.595200: step 430, loss 0.203458, acc 0.92
2016-09-07T18:58:55.248755: step 431, loss 0.373622, acc 0.86
2016-09-07T18:58:55.920965: step 432, loss 0.332411, acc 0.9
2016-09-07T18:58:56.594568: step 433, loss 0.28359, acc 0.92
2016-09-07T18:58:57.282073: step 434, loss 0.371116, acc 0.88
2016-09-07T18:58:57.960085: step 435, loss 0.260427, acc 0.86
2016-09-07T18:58:58.631617: step 436, loss 0.437391, acc 0.78
2016-09-07T18:58:59.301917: step 437, loss 0.223673, acc 0.92
2016-09-07T18:58:59.972960: step 438, loss 0.18863, acc 0.92
2016-09-07T18:59:00.669031: step 439, loss 0.129834, acc 0.94
2016-09-07T18:59:01.335204: step 440, loss 0.295465, acc 0.88
2016-09-07T18:59:02.016621: step 441, loss 0.313916, acc 0.86
2016-09-07T18:59:02.686706: step 442, loss 0.153672, acc 0.94
2016-09-07T18:59:03.347530: step 443, loss 0.217005, acc 0.9
2016-09-07T18:59:04.007745: step 444, loss 0.370478, acc 0.8
2016-09-07T18:59:04.675119: step 445, loss 0.247684, acc 0.94
2016-09-07T18:59:05.353086: step 446, loss 0.234889, acc 0.88
2016-09-07T18:59:06.022915: step 447, loss 0.300287, acc 0.84
2016-09-07T18:59:06.690909: step 448, loss 0.136396, acc 0.96
2016-09-07T18:59:07.392038: step 449, loss 0.149135, acc 0.96
2016-09-07T18:59:08.060289: step 450, loss 0.270465, acc 0.9
2016-09-07T18:59:08.720036: step 451, loss 0.199896, acc 0.92
2016-09-07T18:59:09.391960: step 452, loss 0.248714, acc 0.88
2016-09-07T18:59:10.078435: step 453, loss 0.220124, acc 0.92
2016-09-07T18:59:10.740876: step 454, loss 0.16947, acc 0.92
2016-09-07T18:59:11.404377: step 455, loss 0.29886, acc 0.86
2016-09-07T18:59:12.081217: step 456, loss 0.225044, acc 0.94
2016-09-07T18:59:12.736727: step 457, loss 0.218308, acc 0.86
2016-09-07T18:59:13.386483: step 458, loss 0.33646, acc 0.88
2016-09-07T18:59:14.042418: step 459, loss 0.333417, acc 0.8
2016-09-07T18:59:14.719360: step 460, loss 0.269016, acc 0.88
2016-09-07T18:59:15.382609: step 461, loss 0.223157, acc 0.94
2016-09-07T18:59:16.044163: step 462, loss 0.365014, acc 0.82
2016-09-07T18:59:16.708468: step 463, loss 0.315343, acc 0.86
2016-09-07T18:59:17.390232: step 464, loss 0.351837, acc 0.76
2016-09-07T18:59:18.058590: step 465, loss 0.324374, acc 0.84
2016-09-07T18:59:18.727860: step 466, loss 0.156736, acc 0.96
2016-09-07T18:59:19.419112: step 467, loss 0.230896, acc 0.88
2016-09-07T18:59:20.068790: step 468, loss 0.166837, acc 0.92
2016-09-07T18:59:20.742581: step 469, loss 0.295625, acc 0.84
2016-09-07T18:59:21.418427: step 470, loss 0.320976, acc 0.88
2016-09-07T18:59:22.075245: step 471, loss 0.326493, acc 0.88
2016-09-07T18:59:22.731472: step 472, loss 0.353677, acc 0.86
2016-09-07T18:59:23.407536: step 473, loss 0.273256, acc 0.82
2016-09-07T18:59:24.062946: step 474, loss 0.308906, acc 0.82
2016-09-07T18:59:24.732246: step 475, loss 0.138272, acc 0.98
2016-09-07T18:59:25.390974: step 476, loss 0.215481, acc 0.92
2016-09-07T18:59:26.083076: step 477, loss 0.332037, acc 0.86
2016-09-07T18:59:26.756434: step 478, loss 0.278149, acc 0.9
2016-09-07T18:59:27.413267: step 479, loss 0.375625, acc 0.8
2016-09-07T18:59:28.085730: step 480, loss 0.234436, acc 0.9
2016-09-07T18:59:28.748268: step 481, loss 0.155134, acc 0.94
2016-09-07T18:59:29.411659: step 482, loss 0.200452, acc 0.88
2016-09-07T18:59:30.095664: step 483, loss 0.239381, acc 0.9
2016-09-07T18:59:30.773407: step 484, loss 0.153585, acc 0.96
2016-09-07T18:59:31.439907: step 485, loss 0.378141, acc 0.82
2016-09-07T18:59:32.113707: step 486, loss 0.25085, acc 0.9
2016-09-07T18:59:32.792353: step 487, loss 0.187811, acc 0.94
2016-09-07T18:59:33.444788: step 488, loss 0.292685, acc 0.9
2016-09-07T18:59:34.127710: step 489, loss 0.324155, acc 0.88
2016-09-07T18:59:34.790668: step 490, loss 0.242235, acc 0.9
2016-09-07T18:59:35.453616: step 491, loss 0.249921, acc 0.82
2016-09-07T18:59:36.137023: step 492, loss 0.148128, acc 0.98
2016-09-07T18:59:36.797032: step 493, loss 0.21048, acc 0.94
2016-09-07T18:59:37.462243: step 494, loss 0.291304, acc 0.84
2016-09-07T18:59:38.134686: step 495, loss 0.353816, acc 0.88
2016-09-07T18:59:38.790382: step 496, loss 0.241897, acc 0.86
2016-09-07T18:59:39.450155: step 497, loss 0.268809, acc 0.92
2016-09-07T18:59:40.141108: step 498, loss 0.382615, acc 0.84
2016-09-07T18:59:40.806394: step 499, loss 0.343041, acc 0.88
2016-09-07T18:59:41.469163: step 500, loss 0.17984, acc 0.96

Evaluation:
2016-09-07T18:59:44.413108: step 500, loss 0.49488, acc 0.799

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-500

2016-09-07T18:59:46.062073: step 501, loss 0.335178, acc 0.86
2016-09-07T18:59:46.739853: step 502, loss 0.316527, acc 0.86
2016-09-07T18:59:47.406779: step 503, loss 0.239307, acc 0.9
2016-09-07T18:59:48.087005: step 504, loss 0.266526, acc 0.88
2016-09-07T18:59:48.777644: step 505, loss 0.319763, acc 0.86
2016-09-07T18:59:49.442300: step 506, loss 0.257021, acc 0.88
2016-09-07T18:59:50.111294: step 507, loss 0.22513, acc 0.92
2016-09-07T18:59:50.771310: step 508, loss 0.177735, acc 0.94
2016-09-07T18:59:51.437268: step 509, loss 0.248415, acc 0.88
2016-09-07T18:59:52.118632: step 510, loss 0.378792, acc 0.76
2016-09-07T18:59:52.796100: step 511, loss 0.173389, acc 0.9
2016-09-07T18:59:53.436677: step 512, loss 0.21543, acc 0.92
2016-09-07T18:59:54.126003: step 513, loss 0.231507, acc 0.96
2016-09-07T18:59:54.790220: step 514, loss 0.442157, acc 0.74
2016-09-07T18:59:55.472920: step 515, loss 0.222208, acc 0.88
2016-09-07T18:59:56.164295: step 516, loss 0.249077, acc 0.88
2016-09-07T18:59:56.851656: step 517, loss 0.198274, acc 0.9
2016-09-07T18:59:57.524048: step 518, loss 0.3366, acc 0.9
2016-09-07T18:59:58.195831: step 519, loss 0.368953, acc 0.8
2016-09-07T18:59:58.843499: step 520, loss 0.367154, acc 0.86
2016-09-07T18:59:59.508301: step 521, loss 0.251698, acc 0.88
2016-09-07T19:00:00.170352: step 522, loss 0.24578, acc 0.92
2016-09-07T19:00:00.877971: step 523, loss 0.343882, acc 0.82
2016-09-07T19:00:01.553340: step 524, loss 0.396265, acc 0.78
2016-09-07T19:00:02.211440: step 525, loss 0.335388, acc 0.86
2016-09-07T19:00:02.864725: step 526, loss 0.249755, acc 0.88
2016-09-07T19:00:03.532948: step 527, loss 0.223213, acc 0.9
2016-09-07T19:00:04.208504: step 528, loss 0.234379, acc 0.92
2016-09-07T19:00:04.873299: step 529, loss 0.33373, acc 0.86
2016-09-07T19:00:05.533868: step 530, loss 0.200451, acc 0.9
2016-09-07T19:00:06.202759: step 531, loss 0.256674, acc 0.92
2016-09-07T19:00:06.878702: step 532, loss 0.235578, acc 0.9
2016-09-07T19:00:07.538562: step 533, loss 0.332015, acc 0.82
2016-09-07T19:00:08.193891: step 534, loss 0.275707, acc 0.86
2016-09-07T19:00:08.857110: step 535, loss 0.2178, acc 0.92
2016-09-07T19:00:09.527543: step 536, loss 0.208812, acc 0.88
2016-09-07T19:00:10.200497: step 537, loss 0.190014, acc 0.94
2016-09-07T19:00:10.889780: step 538, loss 0.331595, acc 0.86
2016-09-07T19:00:11.562343: step 539, loss 0.294209, acc 0.86
2016-09-07T19:00:12.225332: step 540, loss 0.242604, acc 0.9
2016-09-07T19:00:12.910906: step 541, loss 0.413735, acc 0.86
2016-09-07T19:00:13.586985: step 542, loss 0.365636, acc 0.84
2016-09-07T19:00:14.252513: step 543, loss 0.27769, acc 0.86
2016-09-07T19:00:14.929926: step 544, loss 0.200481, acc 0.92
2016-09-07T19:00:15.606000: step 545, loss 0.154382, acc 0.94
2016-09-07T19:00:16.277431: step 546, loss 0.274765, acc 0.86
2016-09-07T19:00:16.943558: step 547, loss 0.255365, acc 0.9
2016-09-07T19:00:17.624625: step 548, loss 0.40592, acc 0.88
2016-09-07T19:00:18.294408: step 549, loss 0.248086, acc 0.86
2016-09-07T19:00:18.974967: step 550, loss 0.207963, acc 0.88
2016-09-07T19:00:19.655421: step 551, loss 0.229858, acc 0.88
2016-09-07T19:00:20.329907: step 552, loss 0.35253, acc 0.86
2016-09-07T19:00:21.007785: step 553, loss 0.242714, acc 0.88
2016-09-07T19:00:21.689789: step 554, loss 0.173067, acc 0.92
2016-09-07T19:00:22.353901: step 555, loss 0.229395, acc 0.88
2016-09-07T19:00:23.029271: step 556, loss 0.226275, acc 0.9
2016-09-07T19:00:23.704392: step 557, loss 0.329667, acc 0.88
2016-09-07T19:00:24.374684: step 558, loss 0.366646, acc 0.86
2016-09-07T19:00:25.073784: step 559, loss 0.279848, acc 0.82
2016-09-07T19:00:25.731905: step 560, loss 0.324757, acc 0.9
2016-09-07T19:00:26.398372: step 561, loss 0.257381, acc 0.9
2016-09-07T19:00:27.094669: step 562, loss 0.158749, acc 0.96
2016-09-07T19:00:27.757207: step 563, loss 0.321098, acc 0.86
2016-09-07T19:00:28.440981: step 564, loss 0.330266, acc 0.86
2016-09-07T19:00:29.108027: step 565, loss 0.15001, acc 0.96
2016-09-07T19:00:29.793526: step 566, loss 0.555837, acc 0.76
2016-09-07T19:00:30.457695: step 567, loss 0.373212, acc 0.84
2016-09-07T19:00:31.117638: step 568, loss 0.23181, acc 0.88
2016-09-07T19:00:31.782554: step 569, loss 0.307156, acc 0.88
2016-09-07T19:00:32.448819: step 570, loss 0.230389, acc 0.92
2016-09-07T19:00:33.111535: step 571, loss 0.207185, acc 0.94
2016-09-07T19:00:33.769672: step 572, loss 0.266087, acc 0.88
2016-09-07T19:00:34.431471: step 573, loss 0.269289, acc 0.92
2016-09-07T19:00:35.091450: step 574, loss 0.204204, acc 0.9
2016-09-07T19:00:35.742529: step 575, loss 0.26013, acc 0.88
2016-09-07T19:00:36.422238: step 576, loss 0.247375, acc 0.92
2016-09-07T19:00:37.107565: step 577, loss 0.322673, acc 0.84
2016-09-07T19:00:37.793341: step 578, loss 0.37516, acc 0.78
2016-09-07T19:00:38.492813: step 579, loss 0.247192, acc 0.88
2016-09-07T19:00:39.155449: step 580, loss 0.247731, acc 0.88
2016-09-07T19:00:39.825107: step 581, loss 0.241618, acc 0.88
2016-09-07T19:00:40.184192: step 582, loss 0.322346, acc 0.75
2016-09-07T19:00:40.868534: step 583, loss 0.130685, acc 0.98
2016-09-07T19:00:41.542055: step 584, loss 0.172683, acc 0.94
2016-09-07T19:00:42.209201: step 585, loss 0.299141, acc 0.84
2016-09-07T19:00:42.882963: step 586, loss 0.253157, acc 0.9
2016-09-07T19:00:43.571488: step 587, loss 0.180636, acc 0.94
2016-09-07T19:00:44.251586: step 588, loss 0.302845, acc 0.9
2016-09-07T19:00:44.917010: step 589, loss 0.0998781, acc 0.96
2016-09-07T19:00:45.575882: step 590, loss 0.197407, acc 0.96
2016-09-07T19:00:46.245745: step 591, loss 0.0824534, acc 0.98
2016-09-07T19:00:46.914062: step 592, loss 0.340234, acc 0.9
2016-09-07T19:00:47.566650: step 593, loss 0.113381, acc 0.96
2016-09-07T19:00:48.236899: step 594, loss 0.0586543, acc 1
2016-09-07T19:00:48.910535: step 595, loss 0.274203, acc 0.86
2016-09-07T19:00:49.563996: step 596, loss 0.0983531, acc 0.96
2016-09-07T19:00:50.227124: step 597, loss 0.189868, acc 0.9
2016-09-07T19:00:50.890434: step 598, loss 0.210766, acc 0.9
2016-09-07T19:00:51.562962: step 599, loss 0.188047, acc 0.88
2016-09-07T19:00:52.243834: step 600, loss 0.0364772, acc 1

Evaluation:
2016-09-07T19:00:55.168993: step 600, loss 0.624415, acc 0.778

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-600

2016-09-07T19:00:56.782977: step 601, loss 0.0901661, acc 0.98
2016-09-07T19:00:57.440515: step 602, loss 0.134027, acc 0.92
2016-09-07T19:00:58.119435: step 603, loss 0.0762274, acc 0.98
2016-09-07T19:00:58.783041: step 604, loss 0.168017, acc 0.92
2016-09-07T19:00:59.469792: step 605, loss 0.277429, acc 0.9
2016-09-07T19:01:00.154242: step 606, loss 0.189668, acc 0.94
2016-09-07T19:01:00.873755: step 607, loss 0.15634, acc 0.96
2016-09-07T19:01:01.551058: step 608, loss 0.11435, acc 0.94
2016-09-07T19:01:02.207804: step 609, loss 0.225041, acc 0.88
2016-09-07T19:01:02.873013: step 610, loss 0.10499, acc 0.96
2016-09-07T19:01:03.535585: step 611, loss 0.12177, acc 0.94
2016-09-07T19:01:04.222769: step 612, loss 0.0690128, acc 0.98
2016-09-07T19:01:04.911206: step 613, loss 0.14184, acc 0.94
2016-09-07T19:01:05.574448: step 614, loss 0.169773, acc 0.9
2016-09-07T19:01:06.243710: step 615, loss 0.607562, acc 0.94
2016-09-07T19:01:06.926768: step 616, loss 0.216757, acc 0.94
2016-09-07T19:01:07.596150: step 617, loss 0.116175, acc 0.96
2016-09-07T19:01:08.275664: step 618, loss 0.185772, acc 0.94
2016-09-07T19:01:08.955393: step 619, loss 0.141931, acc 0.94
2016-09-07T19:01:09.638678: step 620, loss 0.148854, acc 0.94
2016-09-07T19:01:10.306335: step 621, loss 0.106102, acc 0.94
2016-09-07T19:01:10.971916: step 622, loss 0.0976794, acc 0.96
2016-09-07T19:01:11.633662: step 623, loss 0.251712, acc 0.9
2016-09-07T19:01:12.305703: step 624, loss 0.268897, acc 0.9
2016-09-07T19:01:12.970344: step 625, loss 0.0709493, acc 0.98
2016-09-07T19:01:13.640687: step 626, loss 0.474055, acc 0.84
2016-09-07T19:01:14.307934: step 627, loss 0.231199, acc 0.9
2016-09-07T19:01:14.987869: step 628, loss 0.122695, acc 0.96
2016-09-07T19:01:15.661065: step 629, loss 0.270743, acc 0.86
2016-09-07T19:01:16.313265: step 630, loss 0.166087, acc 0.92
2016-09-07T19:01:16.965587: step 631, loss 0.167811, acc 0.94
2016-09-07T19:01:17.613897: step 632, loss 0.18423, acc 0.92
2016-09-07T19:01:18.281613: step 633, loss 0.162112, acc 0.94
2016-09-07T19:01:18.959228: step 634, loss 0.189738, acc 0.92
2016-09-07T19:01:19.620383: step 635, loss 0.172585, acc 0.92
2016-09-07T19:01:20.282572: step 636, loss 0.339235, acc 0.88
2016-09-07T19:01:20.936758: step 637, loss 0.299255, acc 0.92
2016-09-07T19:01:21.616702: step 638, loss 0.194791, acc 0.9
2016-09-07T19:01:22.277725: step 639, loss 0.247069, acc 0.88
2016-09-07T19:01:22.936600: step 640, loss 0.214653, acc 0.94
2016-09-07T19:01:23.598002: step 641, loss 0.258517, acc 0.9
2016-09-07T19:01:24.272029: step 642, loss 0.229284, acc 0.9
2016-09-07T19:01:24.963443: step 643, loss 0.227034, acc 0.86
2016-09-07T19:01:25.654902: step 644, loss 0.16859, acc 0.92
2016-09-07T19:01:26.320641: step 645, loss 0.189674, acc 0.9
2016-09-07T19:01:27.001011: step 646, loss 0.310168, acc 0.92
2016-09-07T19:01:27.687199: step 647, loss 0.150676, acc 0.96
2016-09-07T19:01:28.362010: step 648, loss 0.260689, acc 0.92
2016-09-07T19:01:29.018843: step 649, loss 0.0971953, acc 0.96
2016-09-07T19:01:29.674529: step 650, loss 0.189586, acc 0.9
2016-09-07T19:01:30.326600: step 651, loss 0.151669, acc 0.96
2016-09-07T19:01:30.990675: step 652, loss 0.0939404, acc 0.94
2016-09-07T19:01:31.662756: step 653, loss 0.174146, acc 0.96
2016-09-07T19:01:32.330903: step 654, loss 0.244885, acc 0.88
2016-09-07T19:01:32.992048: step 655, loss 0.163233, acc 0.96
2016-09-07T19:01:33.654800: step 656, loss 0.218176, acc 0.9
2016-09-07T19:01:34.322385: step 657, loss 0.177788, acc 0.92
2016-09-07T19:01:34.989420: step 658, loss 0.27993, acc 0.88
2016-09-07T19:01:35.665974: step 659, loss 0.260323, acc 0.88
2016-09-07T19:01:36.351422: step 660, loss 0.123215, acc 0.98
2016-09-07T19:01:37.023918: step 661, loss 0.213503, acc 0.9
2016-09-07T19:01:37.697067: step 662, loss 0.20693, acc 0.9
2016-09-07T19:01:38.362822: step 663, loss 0.170668, acc 0.92
2016-09-07T19:01:39.029943: step 664, loss 0.210923, acc 0.88
2016-09-07T19:01:39.698294: step 665, loss 0.192351, acc 0.92
2016-09-07T19:01:40.360433: step 666, loss 0.190449, acc 0.92
2016-09-07T19:01:41.038321: step 667, loss 0.117219, acc 0.96
2016-09-07T19:01:41.703416: step 668, loss 0.242914, acc 0.86
2016-09-07T19:01:42.378005: step 669, loss 0.247938, acc 0.9
2016-09-07T19:01:43.050886: step 670, loss 0.0866459, acc 0.98
2016-09-07T19:01:43.694245: step 671, loss 0.133467, acc 0.92
2016-09-07T19:01:44.355566: step 672, loss 0.17509, acc 0.92
2016-09-07T19:01:45.024953: step 673, loss 0.199361, acc 0.92
2016-09-07T19:01:45.700493: step 674, loss 0.222688, acc 0.9
2016-09-07T19:01:46.365279: step 675, loss 0.263479, acc 0.92
2016-09-07T19:01:47.022817: step 676, loss 0.140998, acc 0.94
2016-09-07T19:01:47.686957: step 677, loss 0.140311, acc 0.9
2016-09-07T19:01:48.367419: step 678, loss 0.196863, acc 0.88
2016-09-07T19:01:49.034202: step 679, loss 0.111678, acc 0.96
2016-09-07T19:01:49.698663: step 680, loss 0.186859, acc 0.88
2016-09-07T19:01:50.366846: step 681, loss 0.0995861, acc 0.94
2016-09-07T19:01:51.043949: step 682, loss 0.258882, acc 0.9
2016-09-07T19:01:51.716611: step 683, loss 0.208903, acc 0.9
2016-09-07T19:01:52.375764: step 684, loss 0.319271, acc 0.84
2016-09-07T19:01:53.043216: step 685, loss 0.152313, acc 0.94
2016-09-07T19:01:53.702712: step 686, loss 0.242393, acc 0.9
2016-09-07T19:01:54.366627: step 687, loss 0.147615, acc 0.92
2016-09-07T19:01:55.054796: step 688, loss 0.168519, acc 0.94
2016-09-07T19:01:55.730789: step 689, loss 0.150994, acc 0.94
2016-09-07T19:01:56.396950: step 690, loss 0.252282, acc 0.88
2016-09-07T19:01:57.068827: step 691, loss 0.403011, acc 0.82
2016-09-07T19:01:57.742381: step 692, loss 0.18095, acc 0.92
2016-09-07T19:01:58.412122: step 693, loss 0.31208, acc 0.92
2016-09-07T19:01:59.097449: step 694, loss 0.114155, acc 0.96
2016-09-07T19:01:59.754376: step 695, loss 0.191598, acc 0.92
2016-09-07T19:02:00.466751: step 696, loss 0.250994, acc 0.9
2016-09-07T19:02:01.130902: step 697, loss 0.18081, acc 0.94
2016-09-07T19:02:01.793453: step 698, loss 0.320451, acc 0.9
2016-09-07T19:02:02.461646: step 699, loss 0.244126, acc 0.88
2016-09-07T19:02:03.141297: step 700, loss 0.190793, acc 0.94

Evaluation:
2016-09-07T19:02:06.044189: step 700, loss 0.509397, acc 0.792

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-700

2016-09-07T19:02:07.634241: step 701, loss 0.215252, acc 0.92
2016-09-07T19:02:08.281675: step 702, loss 0.18471, acc 0.92
2016-09-07T19:02:08.946558: step 703, loss 0.152876, acc 0.94
2016-09-07T19:02:09.619529: step 704, loss 0.181212, acc 0.88
2016-09-07T19:02:10.284233: step 705, loss 0.172125, acc 0.94
2016-09-07T19:02:10.962696: step 706, loss 0.140907, acc 0.92
2016-09-07T19:02:11.639094: step 707, loss 0.106903, acc 0.94
2016-09-07T19:02:12.310050: step 708, loss 0.1929, acc 0.92
2016-09-07T19:02:12.974070: step 709, loss 0.212746, acc 0.88
2016-09-07T19:02:13.667521: step 710, loss 0.100551, acc 0.94
2016-09-07T19:02:14.337979: step 711, loss 0.144631, acc 0.92
2016-09-07T19:02:14.990062: step 712, loss 0.124835, acc 0.98
2016-09-07T19:02:15.655412: step 713, loss 0.157001, acc 0.94
2016-09-07T19:02:16.312759: step 714, loss 0.218267, acc 0.9
2016-09-07T19:02:16.993681: step 715, loss 0.0935881, acc 0.96
2016-09-07T19:02:17.673626: step 716, loss 0.254636, acc 0.9
2016-09-07T19:02:18.330484: step 717, loss 0.174935, acc 0.9
2016-09-07T19:02:19.006899: step 718, loss 0.321193, acc 0.86
2016-09-07T19:02:19.689681: step 719, loss 0.313649, acc 0.88
2016-09-07T19:02:20.356203: step 720, loss 0.189181, acc 0.9
2016-09-07T19:02:21.014817: step 721, loss 0.139979, acc 0.92
2016-09-07T19:02:21.681919: step 722, loss 0.452222, acc 0.88
2016-09-07T19:02:22.368700: step 723, loss 0.184813, acc 0.9
2016-09-07T19:02:23.042485: step 724, loss 0.177039, acc 0.92
2016-09-07T19:02:23.721281: step 725, loss 0.157176, acc 0.94
2016-09-07T19:02:24.402735: step 726, loss 0.0934138, acc 0.94
2016-09-07T19:02:25.067395: step 727, loss 0.233916, acc 0.9
2016-09-07T19:02:25.728472: step 728, loss 0.277266, acc 0.86
2016-09-07T19:02:26.416266: step 729, loss 0.155198, acc 0.94
2016-09-07T19:02:27.111210: step 730, loss 0.169873, acc 0.96
2016-09-07T19:02:27.781615: step 731, loss 0.0828846, acc 1
2016-09-07T19:02:28.471752: step 732, loss 0.329352, acc 0.94
2016-09-07T19:02:29.143228: step 733, loss 0.22183, acc 0.88
2016-09-07T19:02:29.817668: step 734, loss 0.249995, acc 0.9
2016-09-07T19:02:30.479372: step 735, loss 0.213313, acc 0.88
2016-09-07T19:02:31.123348: step 736, loss 0.371476, acc 0.82
2016-09-07T19:02:31.806833: step 737, loss 0.163606, acc 0.92
2016-09-07T19:02:32.473399: step 738, loss 0.175271, acc 0.94
2016-09-07T19:02:33.135040: step 739, loss 0.239791, acc 0.9
2016-09-07T19:02:33.806014: step 740, loss 0.20369, acc 0.9
2016-09-07T19:02:34.478381: step 741, loss 0.207278, acc 0.88
2016-09-07T19:02:35.142165: step 742, loss 0.224635, acc 0.88
2016-09-07T19:02:35.806538: step 743, loss 0.181901, acc 0.88
2016-09-07T19:02:36.471132: step 744, loss 0.348898, acc 0.86
2016-09-07T19:02:37.145640: step 745, loss 0.250453, acc 0.94
2016-09-07T19:02:37.834395: step 746, loss 0.170518, acc 0.92
2016-09-07T19:02:38.498517: step 747, loss 0.285081, acc 0.86
2016-09-07T19:02:39.166877: step 748, loss 0.169676, acc 0.9
2016-09-07T19:02:39.822570: step 749, loss 0.115644, acc 0.94
2016-09-07T19:02:40.520315: step 750, loss 0.200742, acc 0.94
2016-09-07T19:02:41.199079: step 751, loss 0.332117, acc 0.82
2016-09-07T19:02:41.867357: step 752, loss 0.164483, acc 0.94
2016-09-07T19:02:42.538741: step 753, loss 0.21933, acc 0.94
2016-09-07T19:02:43.220495: step 754, loss 0.326969, acc 0.84
2016-09-07T19:02:43.899864: step 755, loss 0.109332, acc 0.98
2016-09-07T19:02:44.573360: step 756, loss 0.117359, acc 0.94
2016-09-07T19:02:45.269497: step 757, loss 0.333544, acc 0.88
2016-09-07T19:02:45.923660: step 758, loss 0.0948691, acc 0.94
2016-09-07T19:02:46.593436: step 759, loss 0.305818, acc 0.86
2016-09-07T19:02:47.278909: step 760, loss 0.174443, acc 0.94
2016-09-07T19:02:47.933732: step 761, loss 0.277691, acc 0.92
2016-09-07T19:02:48.591518: step 762, loss 0.321227, acc 0.88
2016-09-07T19:02:49.271352: step 763, loss 0.103161, acc 0.94
2016-09-07T19:02:49.932257: step 764, loss 0.109857, acc 0.96
2016-09-07T19:02:50.605281: step 765, loss 0.0720665, acc 1
2016-09-07T19:02:51.279938: step 766, loss 0.191998, acc 0.88
2016-09-07T19:02:51.942838: step 767, loss 0.378309, acc 0.8
2016-09-07T19:02:52.606115: step 768, loss 0.207612, acc 0.9
2016-09-07T19:02:53.294267: step 769, loss 0.283945, acc 0.86
2016-09-07T19:02:53.959648: step 770, loss 0.170534, acc 0.92
2016-09-07T19:02:54.638179: step 771, loss 0.226437, acc 0.86
2016-09-07T19:02:55.347946: step 772, loss 0.1292, acc 0.94
2016-09-07T19:02:56.023195: step 773, loss 0.198938, acc 0.9
2016-09-07T19:02:56.678200: step 774, loss 0.3201, acc 0.86
2016-09-07T19:02:57.350595: step 775, loss 0.24096, acc 0.92
2016-09-07T19:02:57.717539: step 776, loss 0.101975, acc 0.916667
2016-09-07T19:02:58.382974: step 777, loss 0.0980694, acc 0.98
2016-09-07T19:02:59.056544: step 778, loss 0.130798, acc 0.96
2016-09-07T19:02:59.735979: step 779, loss 0.194652, acc 0.92
2016-09-07T19:03:00.444458: step 780, loss 0.0902514, acc 0.98
2016-09-07T19:03:01.107108: step 781, loss 0.121047, acc 0.96
2016-09-07T19:03:01.779207: step 782, loss 0.135553, acc 0.94
2016-09-07T19:03:02.436950: step 783, loss 0.152368, acc 0.96
2016-09-07T19:03:03.120380: step 784, loss 0.233455, acc 0.88
2016-09-07T19:03:03.778224: step 785, loss 0.186282, acc 0.92
2016-09-07T19:03:04.455018: step 786, loss 0.0743077, acc 0.96
2016-09-07T19:03:05.109922: step 787, loss 0.0480978, acc 1
2016-09-07T19:03:05.773114: step 788, loss 0.195006, acc 0.9
2016-09-07T19:03:06.429780: step 789, loss 0.0418795, acc 1
2016-09-07T19:03:07.087369: step 790, loss 0.0571659, acc 1
2016-09-07T19:03:07.759649: step 791, loss 0.173156, acc 0.92
2016-09-07T19:03:08.424410: step 792, loss 0.0908671, acc 0.98
2016-09-07T19:03:09.094590: step 793, loss 0.120658, acc 0.92
2016-09-07T19:03:09.752666: step 794, loss 0.208537, acc 0.88
2016-09-07T19:03:10.415859: step 795, loss 0.153258, acc 0.96
2016-09-07T19:03:11.096579: step 796, loss 0.175521, acc 0.94
2016-09-07T19:03:11.767025: step 797, loss 0.163772, acc 0.94
2016-09-07T19:03:12.443265: step 798, loss 0.10293, acc 0.92
2016-09-07T19:03:13.112953: step 799, loss 0.134297, acc 0.94
2016-09-07T19:03:13.804300: step 800, loss 0.124389, acc 0.92

Evaluation:
2016-09-07T19:03:16.732945: step 800, loss 0.715178, acc 0.777

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-800

2016-09-07T19:03:18.389429: step 801, loss 0.0752672, acc 0.98
2016-09-07T19:03:19.061635: step 802, loss 0.0719841, acc 0.96
2016-09-07T19:03:19.731541: step 803, loss 0.107006, acc 0.96
2016-09-07T19:03:20.392105: step 804, loss 0.0674154, acc 0.98
2016-09-07T19:03:21.056971: step 805, loss 0.108018, acc 0.94
2016-09-07T19:03:21.725336: step 806, loss 0.108054, acc 0.94
2016-09-07T19:03:22.395067: step 807, loss 0.0669328, acc 0.96
2016-09-07T19:03:23.052962: step 808, loss 0.127822, acc 0.94
2016-09-07T19:03:23.715773: step 809, loss 0.114684, acc 0.94
2016-09-07T19:03:24.392224: step 810, loss 0.0434139, acc 1
2016-09-07T19:03:25.058889: step 811, loss 0.156751, acc 0.92
2016-09-07T19:03:25.719525: step 812, loss 0.13245, acc 0.94
2016-09-07T19:03:26.382158: step 813, loss 0.128289, acc 0.9
2016-09-07T19:03:27.063360: step 814, loss 0.127098, acc 0.94
2016-09-07T19:03:27.748384: step 815, loss 0.229898, acc 0.92
2016-09-07T19:03:28.426937: step 816, loss 0.0759209, acc 0.96
2016-09-07T19:03:29.086683: step 817, loss 0.213227, acc 0.88
2016-09-07T19:03:29.758795: step 818, loss 0.143064, acc 0.96
2016-09-07T19:03:30.432093: step 819, loss 0.118761, acc 0.94
2016-09-07T19:03:31.100023: step 820, loss 0.11118, acc 0.98
2016-09-07T19:03:31.772361: step 821, loss 0.0823864, acc 0.98
2016-09-07T19:03:32.426825: step 822, loss 0.267139, acc 0.9
2016-09-07T19:03:33.084310: step 823, loss 0.1392, acc 0.94
2016-09-07T19:03:33.744483: step 824, loss 0.156524, acc 0.94
2016-09-07T19:03:34.423616: step 825, loss 0.0960056, acc 0.98
2016-09-07T19:03:35.119539: step 826, loss 0.172242, acc 0.92
2016-09-07T19:03:35.802263: step 827, loss 0.206168, acc 0.96
2016-09-07T19:03:36.475496: step 828, loss 0.108997, acc 0.96
2016-09-07T19:03:37.137892: step 829, loss 0.199834, acc 0.94
2016-09-07T19:03:37.810656: step 830, loss 0.0950335, acc 0.94
2016-09-07T19:03:38.471172: step 831, loss 0.10107, acc 0.94
2016-09-07T19:03:39.159014: step 832, loss 0.135093, acc 0.94
2016-09-07T19:03:39.833151: step 833, loss 0.277931, acc 0.92
2016-09-07T19:03:40.498608: step 834, loss 0.0361426, acc 1
2016-09-07T19:03:41.177559: step 835, loss 0.0888942, acc 0.98
2016-09-07T19:03:41.886983: step 836, loss 0.174214, acc 0.98
2016-09-07T19:03:42.560796: step 837, loss 0.189744, acc 0.94
2016-09-07T19:03:43.224588: step 838, loss 0.0776483, acc 0.96
2016-09-07T19:03:43.884121: step 839, loss 0.110495, acc 0.94
2016-09-07T19:03:44.557907: step 840, loss 0.248247, acc 0.88
2016-09-07T19:03:45.222449: step 841, loss 0.179008, acc 0.92
2016-09-07T19:03:45.908139: step 842, loss 0.189194, acc 0.96
2016-09-07T19:03:46.592361: step 843, loss 0.094331, acc 0.98
2016-09-07T19:03:47.261455: step 844, loss 0.324393, acc 0.9
2016-09-07T19:03:47.947453: step 845, loss 0.112026, acc 0.96
2016-09-07T19:03:48.625079: step 846, loss 0.163639, acc 0.92
2016-09-07T19:03:49.283139: step 847, loss 0.128698, acc 0.94
2016-09-07T19:03:49.961588: step 848, loss 0.155792, acc 0.92
2016-09-07T19:03:50.636674: step 849, loss 0.129422, acc 0.96
2016-09-07T19:03:51.280688: step 850, loss 0.148806, acc 0.96
2016-09-07T19:03:51.933502: step 851, loss 0.135337, acc 0.94
2016-09-07T19:03:52.600750: step 852, loss 0.192944, acc 0.96
2016-09-07T19:03:53.270556: step 853, loss 0.208336, acc 0.94
2016-09-07T19:03:53.940376: step 854, loss 0.241173, acc 0.9
2016-09-07T19:03:54.609768: step 855, loss 0.174554, acc 0.92
2016-09-07T19:03:55.269376: step 856, loss 0.105808, acc 0.96
2016-09-07T19:03:55.939464: step 857, loss 0.0497522, acc 1
2016-09-07T19:03:56.621601: step 858, loss 0.0794565, acc 0.98
2016-09-07T19:03:57.302633: step 859, loss 0.143397, acc 0.94
2016-09-07T19:03:57.962856: step 860, loss 0.167144, acc 0.92
2016-09-07T19:03:58.637619: step 861, loss 0.0646616, acc 0.98
2016-09-07T19:03:59.313009: step 862, loss 0.254449, acc 0.9
2016-09-07T19:03:59.994448: step 863, loss 0.0740203, acc 0.96
2016-09-07T19:04:00.703208: step 864, loss 0.103378, acc 0.96
2016-09-07T19:04:01.377747: step 865, loss 0.0939935, acc 0.98
2016-09-07T19:04:02.038302: step 866, loss 0.127123, acc 0.96
2016-09-07T19:04:02.691295: step 867, loss 0.19448, acc 0.96
2016-09-07T19:04:03.353706: step 868, loss 0.168987, acc 0.94
2016-09-07T19:04:04.020166: step 869, loss 0.140854, acc 0.94
2016-09-07T19:04:04.681069: step 870, loss 0.186472, acc 0.9
2016-09-07T19:04:05.361604: step 871, loss 0.155411, acc 0.94
2016-09-07T19:04:06.050814: step 872, loss 0.0997408, acc 0.96
2016-09-07T19:04:06.707727: step 873, loss 0.162777, acc 0.94
2016-09-07T19:04:07.381818: step 874, loss 0.155397, acc 0.96
2016-09-07T19:04:08.053734: step 875, loss 0.143397, acc 0.94
2016-09-07T19:04:08.727287: step 876, loss 0.221206, acc 0.9
2016-09-07T19:04:09.413336: step 877, loss 0.159966, acc 0.96
2016-09-07T19:04:10.083564: step 878, loss 0.106299, acc 0.92
2016-09-07T19:04:10.757415: step 879, loss 0.149959, acc 0.94
2016-09-07T19:04:11.431916: step 880, loss 0.181638, acc 0.9
2016-09-07T19:04:12.107248: step 881, loss 0.221564, acc 0.9
2016-09-07T19:04:12.802033: step 882, loss 0.0569451, acc 0.98
2016-09-07T19:04:13.466586: step 883, loss 0.168858, acc 0.92
2016-09-07T19:04:14.130653: step 884, loss 0.145159, acc 0.96
2016-09-07T19:04:14.802535: step 885, loss 0.179697, acc 0.9
2016-09-07T19:04:15.468359: step 886, loss 0.166103, acc 0.94
2016-09-07T19:04:16.160977: step 887, loss 0.164212, acc 0.88
2016-09-07T19:04:16.811690: step 888, loss 0.120955, acc 0.94
2016-09-07T19:04:17.481578: step 889, loss 0.191647, acc 0.92
2016-09-07T19:04:18.146202: step 890, loss 0.100842, acc 0.96
2016-09-07T19:04:18.822381: step 891, loss 0.210282, acc 0.94
2016-09-07T19:04:19.486071: step 892, loss 0.135656, acc 0.94
2016-09-07T19:04:20.153096: step 893, loss 0.0546801, acc 0.98
2016-09-07T19:04:20.829339: step 894, loss 0.153276, acc 0.9
2016-09-07T19:04:21.501972: step 895, loss 0.157359, acc 0.94
2016-09-07T19:04:22.157595: step 896, loss 0.170496, acc 0.94
2016-09-07T19:04:22.806304: step 897, loss 0.0745487, acc 0.96
2016-09-07T19:04:23.459528: step 898, loss 0.116896, acc 0.94
2016-09-07T19:04:24.130740: step 899, loss 0.115239, acc 0.94
2016-09-07T19:04:24.793572: step 900, loss 0.0786215, acc 1

Evaluation:
2016-09-07T19:04:27.713620: step 900, loss 0.650889, acc 0.767

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-900

2016-09-07T19:04:29.351760: step 901, loss 0.17278, acc 0.88
2016-09-07T19:04:30.018286: step 902, loss 0.163422, acc 0.9
2016-09-07T19:04:30.687088: step 903, loss 0.110914, acc 0.98
2016-09-07T19:04:31.367747: step 904, loss 0.0854027, acc 0.98
2016-09-07T19:04:32.038739: step 905, loss 0.0882525, acc 0.96
2016-09-07T19:04:32.699174: step 906, loss 0.207342, acc 0.92
2016-09-07T19:04:33.365262: step 907, loss 0.0832587, acc 0.96
2016-09-07T19:04:34.016849: step 908, loss 0.0997043, acc 0.94
2016-09-07T19:04:34.662992: step 909, loss 0.122688, acc 0.96
2016-09-07T19:04:35.308577: step 910, loss 0.0650793, acc 0.96
2016-09-07T19:04:35.969750: step 911, loss 0.0850266, acc 0.96
2016-09-07T19:04:36.650002: step 912, loss 0.0743621, acc 0.98
2016-09-07T19:04:37.318450: step 913, loss 0.195958, acc 0.9
2016-09-07T19:04:37.994589: step 914, loss 0.0760866, acc 0.96
2016-09-07T19:04:38.659447: step 915, loss 0.234877, acc 0.88
2016-09-07T19:04:39.314247: step 916, loss 0.139809, acc 0.94
2016-09-07T19:04:39.981522: step 917, loss 0.0548142, acc 0.98
2016-09-07T19:04:40.643523: step 918, loss 0.147255, acc 0.94
2016-09-07T19:04:41.299171: step 919, loss 0.145485, acc 0.94
2016-09-07T19:04:41.956386: step 920, loss 0.251363, acc 0.88
2016-09-07T19:04:42.624888: step 921, loss 0.0776619, acc 0.96
2016-09-07T19:04:43.290645: step 922, loss 0.347901, acc 0.88
2016-09-07T19:04:43.961741: step 923, loss 0.16637, acc 0.92
2016-09-07T19:04:44.630075: step 924, loss 0.078117, acc 1
2016-09-07T19:04:45.288096: step 925, loss 0.109532, acc 0.98
2016-09-07T19:04:45.966533: step 926, loss 0.20591, acc 0.94
2016-09-07T19:04:46.654236: step 927, loss 0.16413, acc 0.94
2016-09-07T19:04:47.328431: step 928, loss 0.133824, acc 0.96
2016-09-07T19:04:48.001256: step 929, loss 0.183821, acc 0.94
2016-09-07T19:04:48.655712: step 930, loss 0.0948824, acc 0.96
2016-09-07T19:04:49.326551: step 931, loss 0.146314, acc 0.92
2016-09-07T19:04:49.980890: step 932, loss 0.177401, acc 0.94
2016-09-07T19:04:50.666215: step 933, loss 0.0664193, acc 0.96
2016-09-07T19:04:51.334581: step 934, loss 0.159448, acc 0.96
2016-09-07T19:04:52.026019: step 935, loss 0.223825, acc 0.92
2016-09-07T19:04:52.689492: step 936, loss 0.131426, acc 0.96
2016-09-07T19:04:53.355465: step 937, loss 0.141497, acc 0.92
2016-09-07T19:04:54.039262: step 938, loss 0.198291, acc 0.92
2016-09-07T19:04:54.727927: step 939, loss 0.129713, acc 0.96
2016-09-07T19:04:55.400847: step 940, loss 0.110281, acc 0.96
2016-09-07T19:04:56.060362: step 941, loss 0.160907, acc 0.92
2016-09-07T19:04:56.709514: step 942, loss 0.131164, acc 0.96
2016-09-07T19:04:57.388937: step 943, loss 0.178719, acc 0.88
2016-09-07T19:04:58.067764: step 944, loss 0.137632, acc 0.94
2016-09-07T19:04:58.740734: step 945, loss 0.355467, acc 0.84
2016-09-07T19:04:59.407564: step 946, loss 0.298177, acc 0.86
2016-09-07T19:05:00.085417: step 947, loss 0.0668003, acc 0.98
2016-09-07T19:05:00.783502: step 948, loss 0.323992, acc 0.86
2016-09-07T19:05:01.458167: step 949, loss 0.112675, acc 0.96
2016-09-07T19:05:02.112635: step 950, loss 0.16739, acc 0.94
2016-09-07T19:05:02.764277: step 951, loss 0.098292, acc 0.94
2016-09-07T19:05:03.431579: step 952, loss 0.16404, acc 0.92
2016-09-07T19:05:04.103614: step 953, loss 0.0829764, acc 0.96
2016-09-07T19:05:04.775920: step 954, loss 0.184895, acc 0.9
2016-09-07T19:05:05.444169: step 955, loss 0.187323, acc 0.94
2016-09-07T19:05:06.115752: step 956, loss 0.0745757, acc 1
2016-09-07T19:05:06.765883: step 957, loss 0.189542, acc 0.94
2016-09-07T19:05:07.434606: step 958, loss 0.165929, acc 0.9
2016-09-07T19:05:08.091980: step 959, loss 0.0893478, acc 0.96
2016-09-07T19:05:08.761007: step 960, loss 0.124603, acc 0.96
2016-09-07T19:05:09.443155: step 961, loss 0.214345, acc 0.94
2016-09-07T19:05:10.103469: step 962, loss 0.128813, acc 0.94
2016-09-07T19:05:10.776038: step 963, loss 0.136574, acc 0.94
2016-09-07T19:05:11.454068: step 964, loss 0.148601, acc 0.92
2016-09-07T19:05:12.161983: step 965, loss 0.0947892, acc 0.94
2016-09-07T19:05:12.830103: step 966, loss 0.113528, acc 0.96
2016-09-07T19:05:13.482526: step 967, loss 0.210229, acc 0.9
2016-09-07T19:05:14.149820: step 968, loss 0.0490714, acc 1
2016-09-07T19:05:14.810622: step 969, loss 0.308909, acc 0.9
2016-09-07T19:05:15.169309: step 970, loss 0.105779, acc 1
2016-09-07T19:05:15.840593: step 971, loss 0.063881, acc 0.96
2016-09-07T19:05:16.514784: step 972, loss 0.0890082, acc 0.94
2016-09-07T19:05:17.185939: step 973, loss 0.208232, acc 0.9
2016-09-07T19:05:17.847636: step 974, loss 0.131431, acc 0.94
2016-09-07T19:05:18.516092: step 975, loss 0.0981236, acc 0.98
2016-09-07T19:05:19.182705: step 976, loss 0.165913, acc 0.92
2016-09-07T19:05:19.857336: step 977, loss 0.0989653, acc 0.94
2016-09-07T19:05:20.518965: step 978, loss 0.11346, acc 0.98
2016-09-07T19:05:21.179266: step 979, loss 0.0584344, acc 0.98
2016-09-07T19:05:21.862386: step 980, loss 0.163912, acc 0.92
2016-09-07T19:05:22.539939: step 981, loss 0.132541, acc 0.96
2016-09-07T19:05:23.224417: step 982, loss 0.1214, acc 0.96
2016-09-07T19:05:23.897220: step 983, loss 0.229833, acc 0.88
2016-09-07T19:05:24.551629: step 984, loss 0.105288, acc 0.94
2016-09-07T19:05:25.226452: step 985, loss 0.0835486, acc 0.96
2016-09-07T19:05:25.883742: step 986, loss 0.147231, acc 0.94
2016-09-07T19:05:26.546340: step 987, loss 0.0742398, acc 0.98
2016-09-07T19:05:27.198646: step 988, loss 0.0383715, acc 1
2016-09-07T19:05:27.874014: step 989, loss 0.130039, acc 0.9
2016-09-07T19:05:28.546274: step 990, loss 0.0344607, acc 0.98
2016-09-07T19:05:29.213190: step 991, loss 0.161327, acc 0.94
2016-09-07T19:05:29.867519: step 992, loss 0.199191, acc 0.92
2016-09-07T19:05:30.532807: step 993, loss 0.100178, acc 0.96
2016-09-07T19:05:31.200871: step 994, loss 0.0894856, acc 0.96
2016-09-07T19:05:31.858252: step 995, loss 0.10559, acc 0.96
2016-09-07T19:05:32.527571: step 996, loss 0.0876753, acc 0.98
2016-09-07T19:05:33.187395: step 997, loss 0.19229, acc 0.92
2016-09-07T19:05:33.861725: step 998, loss 0.124829, acc 0.94
2016-09-07T19:05:34.545737: step 999, loss 0.095178, acc 0.96
2016-09-07T19:05:35.220521: step 1000, loss 0.0737452, acc 0.98

Evaluation:
2016-09-07T19:05:38.139993: step 1000, loss 0.690099, acc 0.788

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-1000

2016-09-07T19:05:39.735217: step 1001, loss 0.124582, acc 0.96
2016-09-07T19:05:40.395924: step 1002, loss 0.148395, acc 0.94
2016-09-07T19:05:41.065149: step 1003, loss 0.115566, acc 0.92
2016-09-07T19:05:41.753292: step 1004, loss 0.161816, acc 0.9
2016-09-07T19:05:42.421394: step 1005, loss 0.0841833, acc 0.94
2016-09-07T19:05:43.096080: step 1006, loss 0.0853226, acc 0.96
2016-09-07T19:05:43.764909: step 1007, loss 0.105828, acc 0.96
2016-09-07T19:05:44.415405: step 1008, loss 0.0983675, acc 0.96
2016-09-07T19:05:45.085727: step 1009, loss 0.094069, acc 0.98
2016-09-07T19:05:45.737017: step 1010, loss 0.0887699, acc 0.98
2016-09-07T19:05:46.405236: step 1011, loss 0.0869233, acc 0.98
2016-09-07T19:05:47.082498: step 1012, loss 0.0711525, acc 0.94
2016-09-07T19:05:47.767519: step 1013, loss 0.0433532, acc 1
2016-09-07T19:05:48.431691: step 1014, loss 0.123879, acc 0.96
2016-09-07T19:05:49.106412: step 1015, loss 0.0475444, acc 0.96
2016-09-07T19:05:49.765726: step 1016, loss 0.0824632, acc 0.94
2016-09-07T19:05:50.442202: step 1017, loss 0.0412681, acc 1
2016-09-07T19:05:51.103709: step 1018, loss 0.113322, acc 0.96
2016-09-07T19:05:51.754519: step 1019, loss 0.0699394, acc 0.96
2016-09-07T19:05:52.413476: step 1020, loss 0.075312, acc 0.98
2016-09-07T19:05:53.081933: step 1021, loss 0.103103, acc 0.96
2016-09-07T19:05:53.752169: step 1022, loss 0.0830477, acc 0.96
2016-09-07T19:05:54.414921: step 1023, loss 0.0926777, acc 0.98
2016-09-07T19:05:55.078430: step 1024, loss 0.126612, acc 0.96
2016-09-07T19:05:55.754784: step 1025, loss 0.121117, acc 0.94
2016-09-07T19:05:56.422295: step 1026, loss 0.131744, acc 0.94
2016-09-07T19:05:57.100401: step 1027, loss 0.111324, acc 0.94
2016-09-07T19:05:57.812344: step 1028, loss 0.0716511, acc 0.98
2016-09-07T19:05:58.464107: step 1029, loss 0.107419, acc 0.98
2016-09-07T19:05:59.140968: step 1030, loss 0.11555, acc 0.98
2016-09-07T19:05:59.803216: step 1031, loss 0.0824051, acc 0.96
2016-09-07T19:06:00.519058: step 1032, loss 0.0786923, acc 0.98
2016-09-07T19:06:01.205571: step 1033, loss 0.0924651, acc 0.94
2016-09-07T19:06:01.881917: step 1034, loss 0.214646, acc 0.96
2016-09-07T19:06:02.539629: step 1035, loss 0.144009, acc 0.94
2016-09-07T19:06:03.211201: step 1036, loss 0.0747197, acc 0.98
2016-09-07T19:06:03.899727: step 1037, loss 0.00626632, acc 1
2016-09-07T19:06:04.558635: step 1038, loss 0.120816, acc 0.96
2016-09-07T19:06:05.237090: step 1039, loss 0.100362, acc 0.98
2016-09-07T19:06:05.919964: step 1040, loss 0.0738401, acc 0.96
2016-09-07T19:06:06.590188: step 1041, loss 0.0278261, acc 1
2016-09-07T19:06:07.248558: step 1042, loss 0.0333876, acc 0.98
2016-09-07T19:06:07.919373: step 1043, loss 0.341551, acc 0.88
2016-09-07T19:06:08.591120: step 1044, loss 0.0515582, acc 0.98
2016-09-07T19:06:09.298573: step 1045, loss 0.0880481, acc 0.94
2016-09-07T19:06:09.987079: step 1046, loss 0.159874, acc 0.94
2016-09-07T19:06:10.671821: step 1047, loss 0.22176, acc 0.94
2016-09-07T19:06:11.340859: step 1048, loss 0.0896569, acc 0.98
2016-09-07T19:06:12.010177: step 1049, loss 0.118129, acc 0.96
2016-09-07T19:06:12.696340: step 1050, loss 0.0953179, acc 0.94
2016-09-07T19:06:13.366099: step 1051, loss 0.0684132, acc 0.96
2016-09-07T19:06:14.034845: step 1052, loss 0.15706, acc 0.94
2016-09-07T19:06:14.691744: step 1053, loss 0.0575509, acc 0.98
2016-09-07T19:06:15.361849: step 1054, loss 0.178307, acc 0.94
2016-09-07T19:06:16.019304: step 1055, loss 0.0852047, acc 0.96
2016-09-07T19:06:16.684849: step 1056, loss 0.222832, acc 0.88
2016-09-07T19:06:17.352264: step 1057, loss 0.0682412, acc 1
2016-09-07T19:06:18.015594: step 1058, loss 0.0998181, acc 0.98
2016-09-07T19:06:18.678944: step 1059, loss 0.225505, acc 0.86
2016-09-07T19:06:19.339588: step 1060, loss 0.13752, acc 0.96
2016-09-07T19:06:19.985205: step 1061, loss 0.186219, acc 0.98
2016-09-07T19:06:20.630012: step 1062, loss 0.0765752, acc 0.98
2016-09-07T19:06:21.321440: step 1063, loss 0.149503, acc 0.92
2016-09-07T19:06:21.982770: step 1064, loss 0.0972679, acc 0.96
2016-09-07T19:06:22.643860: step 1065, loss 0.110394, acc 0.96
2016-09-07T19:06:23.320577: step 1066, loss 0.203979, acc 0.9
2016-09-07T19:06:23.988485: step 1067, loss 0.161833, acc 0.92
2016-09-07T19:06:24.656746: step 1068, loss 0.0482094, acc 1
2016-09-07T19:06:25.318370: step 1069, loss 0.146029, acc 0.94
2016-09-07T19:06:25.977629: step 1070, loss 0.088127, acc 0.96
2016-09-07T19:06:26.649128: step 1071, loss 0.0527927, acc 1
2016-09-07T19:06:27.320026: step 1072, loss 0.101511, acc 0.96
2016-09-07T19:06:27.992386: step 1073, loss 0.175907, acc 0.92
2016-09-07T19:06:28.650018: step 1074, loss 0.0969498, acc 0.94
2016-09-07T19:06:29.308685: step 1075, loss 0.123375, acc 0.98
2016-09-07T19:06:29.984012: step 1076, loss 0.117682, acc 0.92
2016-09-07T19:06:30.659762: step 1077, loss 0.0969165, acc 0.94
2016-09-07T19:06:31.315570: step 1078, loss 0.249112, acc 0.9
2016-09-07T19:06:31.953873: step 1079, loss 0.17209, acc 0.9
2016-09-07T19:06:32.627539: step 1080, loss 0.0947023, acc 0.96
2016-09-07T19:06:33.308673: step 1081, loss 0.0787406, acc 0.96
2016-09-07T19:06:33.993851: step 1082, loss 0.0267698, acc 1
2016-09-07T19:06:34.657665: step 1083, loss 0.185494, acc 0.92
2016-09-07T19:06:35.338071: step 1084, loss 0.0615343, acc 0.96
2016-09-07T19:06:36.002174: step 1085, loss 0.11492, acc 0.94
2016-09-07T19:06:36.677391: step 1086, loss 0.142145, acc 0.94
2016-09-07T19:06:37.348357: step 1087, loss 0.131487, acc 0.94
2016-09-07T19:06:38.009816: step 1088, loss 0.143767, acc 0.96
2016-09-07T19:06:38.679265: step 1089, loss 0.145988, acc 0.94
2016-09-07T19:06:39.356974: step 1090, loss 0.133434, acc 0.94
2016-09-07T19:06:40.024816: step 1091, loss 0.123599, acc 0.92
2016-09-07T19:06:40.688267: step 1092, loss 0.114218, acc 0.94
2016-09-07T19:06:41.389747: step 1093, loss 0.11741, acc 0.92
2016-09-07T19:06:42.048469: step 1094, loss 0.0784923, acc 0.96
2016-09-07T19:06:42.714820: step 1095, loss 0.0503389, acc 0.98
2016-09-07T19:06:43.389830: step 1096, loss 0.104687, acc 0.92
2016-09-07T19:06:44.050731: step 1097, loss 0.194228, acc 0.9
2016-09-07T19:06:44.727267: step 1098, loss 0.0888521, acc 0.96
2016-09-07T19:06:45.388549: step 1099, loss 0.156344, acc 0.96
2016-09-07T19:06:46.063376: step 1100, loss 0.0854751, acc 0.94

Evaluation:
2016-09-07T19:06:49.012194: step 1100, loss 0.836815, acc 0.764

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-1100

2016-09-07T19:06:50.669339: step 1101, loss 0.064844, acc 1
2016-09-07T19:06:51.345708: step 1102, loss 0.111061, acc 0.96
2016-09-07T19:06:52.021400: step 1103, loss 0.0529211, acc 0.96
2016-09-07T19:06:52.699605: step 1104, loss 0.0613611, acc 0.98
2016-09-07T19:06:53.371248: step 1105, loss 0.171442, acc 0.96
2016-09-07T19:06:54.042979: step 1106, loss 0.20651, acc 0.92
2016-09-07T19:06:54.688894: step 1107, loss 0.175869, acc 0.94
2016-09-07T19:06:55.339230: step 1108, loss 0.0251591, acc 1
2016-09-07T19:06:55.996537: step 1109, loss 0.152677, acc 0.92
2016-09-07T19:06:56.656534: step 1110, loss 0.143348, acc 0.92
2016-09-07T19:06:57.336723: step 1111, loss 0.180781, acc 0.94
2016-09-07T19:06:58.009680: step 1112, loss 0.168219, acc 0.9
2016-09-07T19:06:58.681604: step 1113, loss 0.0934874, acc 0.96
2016-09-07T19:06:59.349725: step 1114, loss 0.140482, acc 0.92
2016-09-07T19:07:00.021454: step 1115, loss 0.0405521, acc 1
2016-09-07T19:07:00.718311: step 1116, loss 0.141004, acc 0.94
2016-09-07T19:07:01.394264: step 1117, loss 0.0737651, acc 1
2016-09-07T19:07:02.060187: step 1118, loss 0.15712, acc 0.94
2016-09-07T19:07:02.732090: step 1119, loss 0.173883, acc 0.96
2016-09-07T19:07:03.395627: step 1120, loss 0.153747, acc 0.94
2016-09-07T19:07:04.068342: step 1121, loss 0.145546, acc 0.96
2016-09-07T19:07:04.728333: step 1122, loss 0.164157, acc 0.92
2016-09-07T19:07:05.400219: step 1123, loss 0.147407, acc 0.94
2016-09-07T19:07:06.067746: step 1124, loss 0.122002, acc 0.92
2016-09-07T19:07:06.731075: step 1125, loss 0.210984, acc 0.92
2016-09-07T19:07:07.404518: step 1126, loss 0.100554, acc 0.98
2016-09-07T19:07:08.071179: step 1127, loss 0.076038, acc 0.96
2016-09-07T19:07:08.760562: step 1128, loss 0.139274, acc 0.92
2016-09-07T19:07:09.455211: step 1129, loss 0.0951949, acc 0.94
2016-09-07T19:07:10.134652: step 1130, loss 0.123551, acc 0.94
2016-09-07T19:07:10.791309: step 1131, loss 0.0852634, acc 0.98
2016-09-07T19:07:11.462576: step 1132, loss 0.153029, acc 0.88
2016-09-07T19:07:12.148986: step 1133, loss 0.050724, acc 1
2016-09-07T19:07:12.827859: step 1134, loss 0.0444154, acc 0.98
2016-09-07T19:07:13.482619: step 1135, loss 0.165836, acc 0.98
2016-09-07T19:07:14.166945: step 1136, loss 0.0769919, acc 0.98
2016-09-07T19:07:14.839278: step 1137, loss 0.0937102, acc 0.96
2016-09-07T19:07:15.525034: step 1138, loss 0.137425, acc 0.94
2016-09-07T19:07:16.187259: step 1139, loss 0.0579964, acc 0.96
2016-09-07T19:07:16.857585: step 1140, loss 0.14464, acc 0.94
2016-09-07T19:07:17.517771: step 1141, loss 0.0757931, acc 0.96
2016-09-07T19:07:18.190997: step 1142, loss 0.0494576, acc 1
2016-09-07T19:07:18.868770: step 1143, loss 0.145712, acc 0.94
2016-09-07T19:07:19.533077: step 1144, loss 0.0278262, acc 1
2016-09-07T19:07:20.201856: step 1145, loss 0.123502, acc 0.96
2016-09-07T19:07:20.874258: step 1146, loss 0.185714, acc 0.94
2016-09-07T19:07:21.549093: step 1147, loss 0.120427, acc 0.92
2016-09-07T19:07:22.239458: step 1148, loss 0.105015, acc 0.94
2016-09-07T19:07:22.916985: step 1149, loss 0.11964, acc 0.9
2016-09-07T19:07:23.581336: step 1150, loss 0.126834, acc 0.92
2016-09-07T19:07:24.241911: step 1151, loss 0.0969643, acc 0.9
2016-09-07T19:07:24.914159: step 1152, loss 0.117112, acc 0.94
2016-09-07T19:07:25.588299: step 1153, loss 0.0663963, acc 1
2016-09-07T19:07:26.262276: step 1154, loss 0.0266154, acc 1
2016-09-07T19:07:26.960165: step 1155, loss 0.0538625, acc 1
2016-09-07T19:07:27.632674: step 1156, loss 0.0932569, acc 0.96
2016-09-07T19:07:28.311620: step 1157, loss 0.15706, acc 0.92
2016-09-07T19:07:28.994465: step 1158, loss 0.182069, acc 0.9
2016-09-07T19:07:29.670958: step 1159, loss 0.0939615, acc 0.94
2016-09-07T19:07:30.345768: step 1160, loss 0.0643608, acc 0.98
2016-09-07T19:07:31.024818: step 1161, loss 0.115813, acc 0.96
2016-09-07T19:07:31.696505: step 1162, loss 0.0812671, acc 0.96
2016-09-07T19:07:32.369352: step 1163, loss 0.0731486, acc 0.98
2016-09-07T19:07:32.721197: step 1164, loss 0.0833566, acc 1
2016-09-07T19:07:33.400419: step 1165, loss 0.169665, acc 0.92
2016-09-07T19:07:34.074708: step 1166, loss 0.059325, acc 0.98
2016-09-07T19:07:34.741478: step 1167, loss 0.0626831, acc 0.98
2016-09-07T19:07:35.417044: step 1168, loss 0.0430608, acc 1
2016-09-07T19:07:36.095942: step 1169, loss 0.138874, acc 0.94
2016-09-07T19:07:36.798308: step 1170, loss 0.215088, acc 0.94
2016-09-07T19:07:37.451087: step 1171, loss 0.0918213, acc 0.96
2016-09-07T19:07:38.132998: step 1172, loss 0.0470652, acc 0.98
2016-09-07T19:07:38.792043: step 1173, loss 0.0384981, acc 0.98
2016-09-07T19:07:39.469310: step 1174, loss 0.0181736, acc 1
2016-09-07T19:07:40.137159: step 1175, loss 0.0969151, acc 0.96
2016-09-07T19:07:40.807218: step 1176, loss 0.221417, acc 0.9
2016-09-07T19:07:41.467019: step 1177, loss 0.0275167, acc 0.98
2016-09-07T19:07:42.138299: step 1178, loss 0.0282782, acc 1
2016-09-07T19:07:42.828849: step 1179, loss 0.0434266, acc 0.98
2016-09-07T19:07:43.479594: step 1180, loss 0.0750954, acc 0.96
2016-09-07T19:07:44.165161: step 1181, loss 0.0257052, acc 1
2016-09-07T19:07:44.847483: step 1182, loss 0.0211531, acc 1
2016-09-07T19:07:45.545006: step 1183, loss 0.110498, acc 0.98
2016-09-07T19:07:46.216177: step 1184, loss 0.0544344, acc 0.98
2016-09-07T19:07:46.899715: step 1185, loss 0.160777, acc 0.94
2016-09-07T19:07:47.562317: step 1186, loss 0.131257, acc 0.96
2016-09-07T19:07:48.239659: step 1187, loss 0.0512589, acc 0.98
2016-09-07T19:07:48.905267: step 1188, loss 0.0802442, acc 0.96
2016-09-07T19:07:49.564375: step 1189, loss 0.0870267, acc 0.96
2016-09-07T19:07:50.240018: step 1190, loss 0.032686, acc 0.98
2016-09-07T19:07:50.892289: step 1191, loss 0.0386436, acc 0.98
2016-09-07T19:07:51.557987: step 1192, loss 0.0813758, acc 0.96
2016-09-07T19:07:52.225600: step 1193, loss 0.0523991, acc 0.98
2016-09-07T19:07:52.902075: step 1194, loss 0.100033, acc 0.96
2016-09-07T19:07:53.570211: step 1195, loss 0.0433661, acc 0.98
2016-09-07T19:07:54.232575: step 1196, loss 0.0987861, acc 0.94
2016-09-07T19:07:54.909332: step 1197, loss 0.0279099, acc 1
2016-09-07T19:07:55.580258: step 1198, loss 0.0499139, acc 0.96
2016-09-07T19:07:56.247593: step 1199, loss 0.122638, acc 0.94
2016-09-07T19:07:56.932113: step 1200, loss 0.139086, acc 0.96

Evaluation:
2016-09-07T19:07:59.910378: step 1200, loss 1.07878, acc 0.754

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-1200

2016-09-07T19:08:01.638460: step 1201, loss 0.0527707, acc 0.98
2016-09-07T19:08:02.289753: step 1202, loss 0.10096, acc 0.94
2016-09-07T19:08:02.963079: step 1203, loss 0.0582577, acc 0.98
2016-09-07T19:08:03.638489: step 1204, loss 0.125406, acc 0.96
2016-09-07T19:08:04.305440: step 1205, loss 0.0538534, acc 0.98
2016-09-07T19:08:04.990286: step 1206, loss 0.105478, acc 0.94
2016-09-07T19:08:05.667112: step 1207, loss 0.0995836, acc 0.96
2016-09-07T19:08:06.346481: step 1208, loss 0.0256747, acc 1
2016-09-07T19:08:07.002389: step 1209, loss 0.121892, acc 0.92
2016-09-07T19:08:07.663231: step 1210, loss 0.128909, acc 0.94
2016-09-07T19:08:08.332161: step 1211, loss 0.122177, acc 0.96
2016-09-07T19:08:08.995949: step 1212, loss 0.108817, acc 0.96
2016-09-07T19:08:09.668276: step 1213, loss 0.125359, acc 0.96
2016-09-07T19:08:10.331497: step 1214, loss 0.0684351, acc 0.96
2016-09-07T19:08:11.011191: step 1215, loss 0.293812, acc 0.94
2016-09-07T19:08:11.682045: step 1216, loss 0.0288969, acc 1
2016-09-07T19:08:12.376250: step 1217, loss 0.0428621, acc 1
2016-09-07T19:08:13.053358: step 1218, loss 0.0643107, acc 0.98
2016-09-07T19:08:13.717123: step 1219, loss 0.0939293, acc 0.96
2016-09-07T19:08:14.401417: step 1220, loss 0.162889, acc 0.94
2016-09-07T19:08:15.072393: step 1221, loss 0.16679, acc 0.92
2016-09-07T19:08:15.749335: step 1222, loss 0.0786569, acc 0.94
2016-09-07T19:08:16.407298: step 1223, loss 0.112948, acc 0.92
2016-09-07T19:08:17.078288: step 1224, loss 0.0888777, acc 0.96
2016-09-07T19:08:17.737515: step 1225, loss 0.0597327, acc 0.98
2016-09-07T19:08:18.415815: step 1226, loss 0.0806265, acc 0.96
2016-09-07T19:08:19.102004: step 1227, loss 0.0561906, acc 0.98
2016-09-07T19:08:19.772484: step 1228, loss 0.129755, acc 0.96
2016-09-07T19:08:20.446113: step 1229, loss 0.208127, acc 0.92
2016-09-07T19:08:21.118985: step 1230, loss 0.0933563, acc 0.96
2016-09-07T19:08:21.770414: step 1231, loss 0.0458828, acc 0.98
2016-09-07T19:08:22.452222: step 1232, loss 0.0706448, acc 0.98
2016-09-07T19:08:23.108670: step 1233, loss 0.0218533, acc 1
2016-09-07T19:08:23.768758: step 1234, loss 0.0729826, acc 0.96
2016-09-07T19:08:24.415555: step 1235, loss 0.153978, acc 0.9
2016-09-07T19:08:25.099955: step 1236, loss 0.0687644, acc 0.96
2016-09-07T19:08:25.762160: step 1237, loss 0.0795279, acc 0.98
2016-09-07T19:08:26.423297: step 1238, loss 0.0140259, acc 1
2016-09-07T19:08:27.099961: step 1239, loss 0.0445231, acc 0.98
2016-09-07T19:08:27.769280: step 1240, loss 0.264983, acc 0.86
2016-09-07T19:08:28.426716: step 1241, loss 0.0713872, acc 0.94
2016-09-07T19:08:29.092639: step 1242, loss 0.0386681, acc 0.98
2016-09-07T19:08:29.764666: step 1243, loss 0.0951048, acc 0.96
2016-09-07T19:08:30.422791: step 1244, loss 0.206667, acc 0.92
2016-09-07T19:08:31.101555: step 1245, loss 0.164816, acc 0.9
2016-09-07T19:08:31.770842: step 1246, loss 0.161545, acc 0.94
2016-09-07T19:08:32.438038: step 1247, loss 0.0541687, acc 0.98
2016-09-07T19:08:33.102317: step 1248, loss 0.0492618, acc 0.98
2016-09-07T19:08:33.768567: step 1249, loss 0.176683, acc 0.92
2016-09-07T19:08:34.436809: step 1250, loss 0.0545282, acc 1
2016-09-07T19:08:35.116539: step 1251, loss 0.0573971, acc 0.98
2016-09-07T19:08:35.781340: step 1252, loss 0.0923194, acc 0.94
2016-09-07T19:08:36.461830: step 1253, loss 0.1612, acc 0.92
2016-09-07T19:08:37.140156: step 1254, loss 0.079339, acc 0.96
2016-09-07T19:08:37.815230: step 1255, loss 0.102573, acc 0.98
2016-09-07T19:08:38.495593: step 1256, loss 0.0994081, acc 0.94
2016-09-07T19:08:39.182844: step 1257, loss 0.0459356, acc 1
2016-09-07T19:08:39.858436: step 1258, loss 0.0820675, acc 0.94
2016-09-07T19:08:40.532555: step 1259, loss 0.101171, acc 0.96
2016-09-07T19:08:41.193744: step 1260, loss 0.0657727, acc 0.96
2016-09-07T19:08:41.846755: step 1261, loss 0.0620443, acc 0.96
2016-09-07T19:08:42.516778: step 1262, loss 0.0653591, acc 0.98
2016-09-07T19:08:43.187344: step 1263, loss 0.0626011, acc 0.98
2016-09-07T19:08:43.868415: step 1264, loss 0.131215, acc 0.96
2016-09-07T19:08:44.547224: step 1265, loss 0.0795181, acc 0.98
2016-09-07T19:08:45.212360: step 1266, loss 0.100624, acc 0.96
2016-09-07T19:08:45.880962: step 1267, loss 0.142264, acc 0.94
2016-09-07T19:08:46.535122: step 1268, loss 0.136214, acc 0.92
2016-09-07T19:08:47.210502: step 1269, loss 0.0936556, acc 0.96
2016-09-07T19:08:47.883832: step 1270, loss 0.0691385, acc 0.98
2016-09-07T19:08:48.554597: step 1271, loss 0.040373, acc 0.98
2016-09-07T19:08:49.246049: step 1272, loss 0.138119, acc 0.94
2016-09-07T19:08:49.929810: step 1273, loss 0.0881105, acc 0.96
2016-09-07T19:08:50.608184: step 1274, loss 0.0429491, acc 0.98
2016-09-07T19:08:51.276356: step 1275, loss 0.144455, acc 0.94
2016-09-07T19:08:51.940973: step 1276, loss 0.0506009, acc 0.98
2016-09-07T19:08:52.614536: step 1277, loss 0.178226, acc 0.94
2016-09-07T19:08:53.287489: step 1278, loss 0.0891575, acc 0.96
2016-09-07T19:08:53.966017: step 1279, loss 0.0997734, acc 0.94
2016-09-07T19:08:54.630372: step 1280, loss 0.0991176, acc 0.98
2016-09-07T19:08:55.290889: step 1281, loss 0.100781, acc 0.96
2016-09-07T19:08:55.986660: step 1282, loss 0.0990206, acc 0.96
2016-09-07T19:08:56.652453: step 1283, loss 0.0727473, acc 0.96
2016-09-07T19:08:57.321887: step 1284, loss 0.0697933, acc 0.98
2016-09-07T19:08:57.998803: step 1285, loss 0.0801951, acc 0.96
2016-09-07T19:08:58.657837: step 1286, loss 0.226786, acc 0.94
2016-09-07T19:08:59.339778: step 1287, loss 0.0868238, acc 0.96
2016-09-07T19:09:00.033093: step 1288, loss 0.0931492, acc 0.98
2016-09-07T19:09:00.747032: step 1289, loss 0.246722, acc 0.88
2016-09-07T19:09:01.438707: step 1290, loss 0.0379011, acc 0.98
2016-09-07T19:09:02.133835: step 1291, loss 0.116002, acc 0.92
2016-09-07T19:09:02.820358: step 1292, loss 0.200927, acc 0.94
2016-09-07T19:09:03.521821: step 1293, loss 0.12678, acc 0.92
2016-09-07T19:09:04.231743: step 1294, loss 0.0632062, acc 0.98
2016-09-07T19:09:04.922645: step 1295, loss 0.175889, acc 0.92
2016-09-07T19:09:05.608061: step 1296, loss 0.132787, acc 0.92
2016-09-07T19:09:06.265050: step 1297, loss 0.0416593, acc 1
2016-09-07T19:09:06.929049: step 1298, loss 0.0735618, acc 0.96
2016-09-07T19:09:07.600252: step 1299, loss 0.0570664, acc 0.96
2016-09-07T19:09:08.271013: step 1300, loss 0.175148, acc 0.94

Evaluation:
2016-09-07T19:09:11.267597: step 1300, loss 0.745459, acc 0.773

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-1300

2016-09-07T19:09:12.871240: step 1301, loss 0.0921155, acc 0.96
2016-09-07T19:09:13.542669: step 1302, loss 0.0488853, acc 0.98
2016-09-07T19:09:14.215086: step 1303, loss 0.146343, acc 0.94
2016-09-07T19:09:14.885044: step 1304, loss 0.0848455, acc 0.94
2016-09-07T19:09:15.571516: step 1305, loss 0.0695823, acc 0.98
2016-09-07T19:09:16.257058: step 1306, loss 0.109377, acc 0.94
2016-09-07T19:09:16.931015: step 1307, loss 0.089784, acc 0.96
2016-09-07T19:09:17.614864: step 1308, loss 0.115356, acc 0.96
2016-09-07T19:09:18.308764: step 1309, loss 0.108118, acc 0.94
2016-09-07T19:09:19.000183: step 1310, loss 0.0355267, acc 1
2016-09-07T19:09:19.677910: step 1311, loss 0.116328, acc 0.96
2016-09-07T19:09:20.334688: step 1312, loss 0.0963536, acc 0.96
2016-09-07T19:09:21.018412: step 1313, loss 0.0712933, acc 0.98
2016-09-07T19:09:21.690411: step 1314, loss 0.0586369, acc 0.96
2016-09-07T19:09:22.366096: step 1315, loss 0.315169, acc 0.92
2016-09-07T19:09:23.037234: step 1316, loss 0.0499628, acc 1
2016-09-07T19:09:23.703676: step 1317, loss 0.0959313, acc 0.94
2016-09-07T19:09:24.364973: step 1318, loss 0.238228, acc 0.96
2016-09-07T19:09:25.026824: step 1319, loss 0.0340507, acc 0.98
2016-09-07T19:09:25.676216: step 1320, loss 0.0402283, acc 0.98
2016-09-07T19:09:26.341566: step 1321, loss 0.0550378, acc 0.98
2016-09-07T19:09:27.035579: step 1322, loss 0.230834, acc 0.86
2016-09-07T19:09:27.703559: step 1323, loss 0.155425, acc 0.94
2016-09-07T19:09:28.375212: step 1324, loss 0.0419452, acc 1
2016-09-07T19:09:29.046698: step 1325, loss 0.218057, acc 0.9
2016-09-07T19:09:29.710524: step 1326, loss 0.0529927, acc 0.98
2016-09-07T19:09:30.374559: step 1327, loss 0.0324772, acc 1
2016-09-07T19:09:31.056542: step 1328, loss 0.100655, acc 0.94
2016-09-07T19:09:31.745073: step 1329, loss 0.0442344, acc 0.98
2016-09-07T19:09:32.427349: step 1330, loss 0.0628647, acc 0.96
2016-09-07T19:09:33.093186: step 1331, loss 0.0796122, acc 0.96
2016-09-07T19:09:33.742931: step 1332, loss 0.036058, acc 1
2016-09-07T19:09:34.398619: step 1333, loss 0.0424074, acc 0.98
2016-09-07T19:09:35.059424: step 1334, loss 0.0676261, acc 1
2016-09-07T19:09:35.713837: step 1335, loss 0.0697468, acc 0.96
2016-09-07T19:09:36.364647: step 1336, loss 0.139351, acc 0.94
2016-09-07T19:09:37.034491: step 1337, loss 0.126964, acc 0.94
2016-09-07T19:09:37.701292: step 1338, loss 0.0298771, acc 1
2016-09-07T19:09:38.356394: step 1339, loss 0.0901376, acc 0.96
2016-09-07T19:09:39.020038: step 1340, loss 0.0685875, acc 0.96
2016-09-07T19:09:39.677265: step 1341, loss 0.034117, acc 0.98
2016-09-07T19:09:40.363019: step 1342, loss 0.110886, acc 0.94
2016-09-07T19:09:41.030726: step 1343, loss 0.0925178, acc 0.96
2016-09-07T19:09:41.741838: step 1344, loss 0.0549572, acc 1
2016-09-07T19:09:42.415038: step 1345, loss 0.0575376, acc 0.98
2016-09-07T19:09:43.087414: step 1346, loss 0.0505959, acc 0.98
2016-09-07T19:09:43.761188: step 1347, loss 0.0820551, acc 0.96
2016-09-07T19:09:44.418897: step 1348, loss 0.113309, acc 0.96
2016-09-07T19:09:45.094769: step 1349, loss 0.0429732, acc 0.98
2016-09-07T19:09:45.753309: step 1350, loss 0.022418, acc 1
2016-09-07T19:09:46.422193: step 1351, loss 0.192832, acc 0.96
2016-09-07T19:09:47.085742: step 1352, loss 0.0854901, acc 0.98
2016-09-07T19:09:47.755270: step 1353, loss 0.0275095, acc 0.98
2016-09-07T19:09:48.436084: step 1354, loss 0.234958, acc 0.88
2016-09-07T19:09:49.097547: step 1355, loss 0.0884215, acc 0.98
2016-09-07T19:09:49.770748: step 1356, loss 0.0149955, acc 1
2016-09-07T19:09:50.431818: step 1357, loss 0.0206177, acc 1
2016-09-07T19:09:50.796928: step 1358, loss 0.227969, acc 0.833333
2016-09-07T19:09:51.442501: step 1359, loss 0.0481318, acc 0.98
2016-09-07T19:09:52.131831: step 1360, loss 0.107878, acc 0.98
2016-09-07T19:09:52.799168: step 1361, loss 0.221875, acc 0.94
2016-09-07T19:09:53.472875: step 1362, loss 0.0220692, acc 1
2016-09-07T19:09:54.153825: step 1363, loss 0.128655, acc 0.96
2016-09-07T19:09:54.834170: step 1364, loss 0.0823671, acc 0.96
2016-09-07T19:09:55.516803: step 1365, loss 0.0558055, acc 1
2016-09-07T19:09:56.174856: step 1366, loss 0.0625807, acc 0.96
2016-09-07T19:09:56.839265: step 1367, loss 0.176195, acc 0.92
2016-09-07T19:09:57.511766: step 1368, loss 0.155074, acc 0.94
2016-09-07T19:09:58.185335: step 1369, loss 0.0650772, acc 0.94
2016-09-07T19:09:58.845105: step 1370, loss 0.0294277, acc 1
2016-09-07T19:09:59.515395: step 1371, loss 0.144628, acc 0.96
2016-09-07T19:10:00.179495: step 1372, loss 0.133431, acc 0.94
2016-09-07T19:10:00.894299: step 1373, loss 0.0695988, acc 0.98
2016-09-07T19:10:01.560596: step 1374, loss 0.0597291, acc 1
2016-09-07T19:10:02.220382: step 1375, loss 0.0818078, acc 0.96
2016-09-07T19:10:02.882433: step 1376, loss 0.0279659, acc 1
2016-09-07T19:10:03.543392: step 1377, loss 0.103015, acc 0.94
2016-09-07T19:10:04.201128: step 1378, loss 0.031538, acc 1
2016-09-07T19:10:04.874313: step 1379, loss 0.115819, acc 0.96
2016-09-07T19:10:05.552587: step 1380, loss 0.0943332, acc 0.96
2016-09-07T19:10:06.207717: step 1381, loss 0.0304207, acc 1
2016-09-07T19:10:06.873143: step 1382, loss 0.0915511, acc 0.96
2016-09-07T19:10:07.556564: step 1383, loss 0.0409563, acc 1
2016-09-07T19:10:08.225613: step 1384, loss 0.0822148, acc 0.94
2016-09-07T19:10:08.882472: step 1385, loss 0.0722916, acc 0.98
2016-09-07T19:10:09.542717: step 1386, loss 0.124753, acc 0.92
2016-09-07T19:10:10.197928: step 1387, loss 0.125214, acc 0.94
2016-09-07T19:10:10.872264: step 1388, loss 0.0434824, acc 0.98
2016-09-07T19:10:11.548732: step 1389, loss 0.0771387, acc 0.96
2016-09-07T19:10:12.220318: step 1390, loss 0.107527, acc 0.96
2016-09-07T19:10:12.899757: step 1391, loss 0.0497578, acc 0.98
2016-09-07T19:10:13.555904: step 1392, loss 0.0530042, acc 1
2016-09-07T19:10:14.233717: step 1393, loss 0.0171829, acc 1
2016-09-07T19:10:14.917451: step 1394, loss 0.0721744, acc 0.96
2016-09-07T19:10:15.588777: step 1395, loss 0.0506364, acc 0.98
2016-09-07T19:10:16.272942: step 1396, loss 0.0875575, acc 0.98
2016-09-07T19:10:16.936439: step 1397, loss 0.0347942, acc 0.98
2016-09-07T19:10:17.610434: step 1398, loss 0.056153, acc 0.98
2016-09-07T19:10:18.287031: step 1399, loss 0.0894066, acc 0.96
2016-09-07T19:10:18.958744: step 1400, loss 0.101499, acc 0.94

Evaluation:
2016-09-07T19:10:21.989001: step 1400, loss 1.00296, acc 0.765

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-1400

2016-09-07T19:10:23.639579: step 1401, loss 0.0602845, acc 0.96
2016-09-07T19:10:24.304472: step 1402, loss 0.0204551, acc 0.98
2016-09-07T19:10:24.980739: step 1403, loss 0.0254927, acc 0.98
2016-09-07T19:10:25.649676: step 1404, loss 0.0215835, acc 1
2016-09-07T19:10:26.312607: step 1405, loss 0.137424, acc 0.92
2016-09-07T19:10:27.013435: step 1406, loss 0.0735981, acc 0.98
2016-09-07T19:10:27.678659: step 1407, loss 0.0481116, acc 0.98
2016-09-07T19:10:28.345437: step 1408, loss 0.075914, acc 0.96
2016-09-07T19:10:28.999630: step 1409, loss 0.0582739, acc 0.96
2016-09-07T19:10:29.681878: step 1410, loss 0.0470769, acc 0.98
2016-09-07T19:10:30.353196: step 1411, loss 0.0260252, acc 0.98
2016-09-07T19:10:31.019703: step 1412, loss 0.267668, acc 0.94
2016-09-07T19:10:31.677862: step 1413, loss 0.0194157, acc 1
2016-09-07T19:10:32.337195: step 1414, loss 0.0818006, acc 0.94
2016-09-07T19:10:33.003593: step 1415, loss 0.160786, acc 0.96
2016-09-07T19:10:33.663112: step 1416, loss 0.0295836, acc 0.98
2016-09-07T19:10:34.328691: step 1417, loss 0.067188, acc 0.96
2016-09-07T19:10:35.012999: step 1418, loss 0.0312623, acc 0.98
2016-09-07T19:10:35.670853: step 1419, loss 0.112004, acc 0.92
2016-09-07T19:10:36.339692: step 1420, loss 0.0499529, acc 0.98
2016-09-07T19:10:37.010160: step 1421, loss 0.158394, acc 0.9
2016-09-07T19:10:37.671392: step 1422, loss 0.174096, acc 0.92
2016-09-07T19:10:38.329224: step 1423, loss 0.087679, acc 0.96
2016-09-07T19:10:38.997507: step 1424, loss 0.204989, acc 0.94
2016-09-07T19:10:39.655282: step 1425, loss 0.0945912, acc 0.94
2016-09-07T19:10:40.342777: step 1426, loss 0.0932652, acc 0.96
2016-09-07T19:10:41.006416: step 1427, loss 0.12274, acc 0.96
2016-09-07T19:10:41.692771: step 1428, loss 0.0688635, acc 0.96
2016-09-07T19:10:42.366849: step 1429, loss 0.128127, acc 0.94
2016-09-07T19:10:43.036372: step 1430, loss 0.0361233, acc 0.98
2016-09-07T19:10:43.691482: step 1431, loss 0.109989, acc 0.94
2016-09-07T19:10:44.351785: step 1432, loss 0.036773, acc 1
2016-09-07T19:10:45.025769: step 1433, loss 0.031395, acc 0.98
2016-09-07T19:10:45.694491: step 1434, loss 0.11034, acc 0.94
2016-09-07T19:10:46.381148: step 1435, loss 0.0512458, acc 0.98
2016-09-07T19:10:47.050097: step 1436, loss 0.117276, acc 0.92
2016-09-07T19:10:47.700123: step 1437, loss 0.108913, acc 0.96
2016-09-07T19:10:48.349572: step 1438, loss 0.0942795, acc 0.92
2016-09-07T19:10:49.014476: step 1439, loss 0.161748, acc 0.92
2016-09-07T19:10:49.683334: step 1440, loss 0.10736, acc 0.94
2016-09-07T19:10:50.358105: step 1441, loss 0.0664455, acc 0.98
2016-09-07T19:10:51.027879: step 1442, loss 0.0329654, acc 1
2016-09-07T19:10:51.679614: step 1443, loss 0.0624787, acc 0.96
2016-09-07T19:10:52.351218: step 1444, loss 0.0367431, acc 1
2016-09-07T19:10:53.016088: step 1445, loss 0.0800278, acc 0.98
2016-09-07T19:10:53.714272: step 1446, loss 0.0632447, acc 0.98
2016-09-07T19:10:54.404611: step 1447, loss 0.0659984, acc 0.98
2016-09-07T19:10:55.081990: step 1448, loss 0.150346, acc 0.92
2016-09-07T19:10:55.739351: step 1449, loss 0.190519, acc 0.94
2016-09-07T19:10:56.388966: step 1450, loss 0.189259, acc 0.9
2016-09-07T19:10:57.065847: step 1451, loss 0.0774703, acc 0.96
2016-09-07T19:10:57.729064: step 1452, loss 0.0670279, acc 0.98
2016-09-07T19:10:58.405616: step 1453, loss 0.0934787, acc 0.96
2016-09-07T19:10:59.071985: step 1454, loss 0.0558133, acc 1
2016-09-07T19:10:59.745192: step 1455, loss 0.0710165, acc 0.98
2016-09-07T19:11:00.447229: step 1456, loss 0.133536, acc 0.96
2016-09-07T19:11:01.114188: step 1457, loss 0.108476, acc 0.92
2016-09-07T19:11:01.768853: step 1458, loss 0.0464166, acc 1
2016-09-07T19:11:02.438884: step 1459, loss 0.0489869, acc 1
2016-09-07T19:11:03.096225: step 1460, loss 0.109744, acc 0.96
2016-09-07T19:11:03.764252: step 1461, loss 0.0615811, acc 1
2016-09-07T19:11:04.426932: step 1462, loss 0.0505814, acc 0.98
2016-09-07T19:11:05.097970: step 1463, loss 0.0627389, acc 0.98
2016-09-07T19:11:05.756098: step 1464, loss 0.157859, acc 0.96
2016-09-07T19:11:06.415367: step 1465, loss 0.0829848, acc 0.94
2016-09-07T19:11:07.089862: step 1466, loss 0.0899589, acc 0.96
2016-09-07T19:11:07.759829: step 1467, loss 0.108493, acc 0.94
2016-09-07T19:11:08.415168: step 1468, loss 0.0849738, acc 0.96
2016-09-07T19:11:09.093335: step 1469, loss 0.111308, acc 0.92
2016-09-07T19:11:09.765418: step 1470, loss 0.0340852, acc 0.98
2016-09-07T19:11:10.477122: step 1471, loss 0.0635616, acc 0.98
2016-09-07T19:11:11.140135: step 1472, loss 0.0865506, acc 0.96
2016-09-07T19:11:11.811139: step 1473, loss 0.0180613, acc 1
2016-09-07T19:11:12.466408: step 1474, loss 0.0828672, acc 0.98
2016-09-07T19:11:13.155723: step 1475, loss 0.103789, acc 0.96
2016-09-07T19:11:13.823167: step 1476, loss 0.0762038, acc 0.96
2016-09-07T19:11:14.492918: step 1477, loss 0.0177152, acc 1
2016-09-07T19:11:15.152026: step 1478, loss 0.111773, acc 0.94
2016-09-07T19:11:15.826514: step 1479, loss 0.103815, acc 0.94
2016-09-07T19:11:16.478857: step 1480, loss 0.0793252, acc 0.94
2016-09-07T19:11:17.141017: step 1481, loss 0.0588008, acc 0.98
2016-09-07T19:11:17.834806: step 1482, loss 0.0111251, acc 1
2016-09-07T19:11:18.502965: step 1483, loss 0.178284, acc 0.88
2016-09-07T19:11:19.154281: step 1484, loss 0.056356, acc 0.94
2016-09-07T19:11:19.833459: step 1485, loss 0.0821971, acc 0.94
2016-09-07T19:11:20.521409: step 1486, loss 0.0506375, acc 0.98
2016-09-07T19:11:21.198784: step 1487, loss 0.0656491, acc 0.98
2016-09-07T19:11:21.867631: step 1488, loss 0.0674846, acc 0.98
2016-09-07T19:11:22.533780: step 1489, loss 0.115214, acc 0.94
2016-09-07T19:11:23.221543: step 1490, loss 0.032727, acc 1
2016-09-07T19:11:23.895411: step 1491, loss 0.052297, acc 0.98
2016-09-07T19:11:24.560288: step 1492, loss 0.0483394, acc 0.98
2016-09-07T19:11:25.213062: step 1493, loss 0.256121, acc 0.92
2016-09-07T19:11:25.877201: step 1494, loss 0.085017, acc 0.98
2016-09-07T19:11:26.530981: step 1495, loss 0.0646592, acc 0.98
2016-09-07T19:11:27.195738: step 1496, loss 0.128243, acc 0.98
2016-09-07T19:11:27.852556: step 1497, loss 0.0680579, acc 0.98
2016-09-07T19:11:28.515381: step 1498, loss 0.0663439, acc 0.98
2016-09-07T19:11:29.182517: step 1499, loss 0.112727, acc 0.94
2016-09-07T19:11:29.855531: step 1500, loss 0.0898448, acc 0.94

Evaluation:
2016-09-07T19:11:32.873334: step 1500, loss 0.921951, acc 0.76

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-1500

2016-09-07T19:11:34.540250: step 1501, loss 0.115606, acc 0.92
2016-09-07T19:11:35.222683: step 1502, loss 0.0177955, acc 1
2016-09-07T19:11:35.909792: step 1503, loss 0.0743877, acc 0.96
2016-09-07T19:11:36.593142: step 1504, loss 0.0953787, acc 0.98
2016-09-07T19:11:37.259697: step 1505, loss 0.0912683, acc 0.96
2016-09-07T19:11:37.924353: step 1506, loss 0.0854632, acc 0.92
2016-09-07T19:11:38.590330: step 1507, loss 0.0429964, acc 0.98
2016-09-07T19:11:39.247273: step 1508, loss 0.018505, acc 1
2016-09-07T19:11:39.914065: step 1509, loss 0.0950886, acc 0.98
2016-09-07T19:11:40.561925: step 1510, loss 0.0758434, acc 0.96
2016-09-07T19:11:41.239670: step 1511, loss 0.132773, acc 0.94
2016-09-07T19:11:41.918468: step 1512, loss 0.156747, acc 0.96
2016-09-07T19:11:42.580593: step 1513, loss 0.118394, acc 0.98
2016-09-07T19:11:43.259314: step 1514, loss 0.0456836, acc 0.96
2016-09-07T19:11:43.935518: step 1515, loss 0.0720669, acc 0.94
2016-09-07T19:11:44.592246: step 1516, loss 0.0379537, acc 0.98
2016-09-07T19:11:45.268609: step 1517, loss 0.0338376, acc 1
2016-09-07T19:11:45.934800: step 1518, loss 0.0875829, acc 0.96
2016-09-07T19:11:46.605606: step 1519, loss 0.0520054, acc 0.98
2016-09-07T19:11:47.290287: step 1520, loss 0.0716682, acc 0.96
2016-09-07T19:11:47.958883: step 1521, loss 0.0235558, acc 1
2016-09-07T19:11:48.620481: step 1522, loss 0.217447, acc 0.92
2016-09-07T19:11:49.287996: step 1523, loss 0.0737953, acc 0.94
2016-09-07T19:11:49.975086: step 1524, loss 0.14658, acc 0.92
2016-09-07T19:11:50.634267: step 1525, loss 0.0242342, acc 1
2016-09-07T19:11:51.319851: step 1526, loss 0.0272902, acc 1
2016-09-07T19:11:51.999692: step 1527, loss 0.169216, acc 0.9
2016-09-07T19:11:52.670404: step 1528, loss 0.0769346, acc 0.96
2016-09-07T19:11:53.336123: step 1529, loss 0.102999, acc 0.96
2016-09-07T19:11:53.996831: step 1530, loss 0.063439, acc 0.96
2016-09-07T19:11:54.676088: step 1531, loss 0.0426949, acc 0.98
2016-09-07T19:11:55.352618: step 1532, loss 0.124716, acc 0.94
2016-09-07T19:11:56.046636: step 1533, loss 0.042603, acc 1
2016-09-07T19:11:56.716795: step 1534, loss 0.165265, acc 0.94
2016-09-07T19:11:57.389268: step 1535, loss 0.0747909, acc 0.96
2016-09-07T19:11:58.059614: step 1536, loss 0.0232287, acc 0.98
2016-09-07T19:11:58.728707: step 1537, loss 0.0800594, acc 0.96
2016-09-07T19:11:59.401163: step 1538, loss 0.0998039, acc 0.96
2016-09-07T19:12:00.081242: step 1539, loss 0.0255986, acc 1
2016-09-07T19:12:00.790026: step 1540, loss 0.0386631, acc 0.98
2016-09-07T19:12:01.453920: step 1541, loss 0.0966878, acc 0.96
2016-09-07T19:12:02.121758: step 1542, loss 0.0497538, acc 0.98
2016-09-07T19:12:02.792516: step 1543, loss 0.0620538, acc 0.98
2016-09-07T19:12:03.479146: step 1544, loss 0.0309519, acc 0.98
2016-09-07T19:12:04.126036: step 1545, loss 0.171388, acc 0.94
2016-09-07T19:12:04.804430: step 1546, loss 0.213591, acc 0.92
2016-09-07T19:12:05.473744: step 1547, loss 0.188137, acc 0.9
2016-09-07T19:12:06.158478: step 1548, loss 0.067031, acc 0.96
2016-09-07T19:12:06.839483: step 1549, loss 0.0223888, acc 1
2016-09-07T19:12:07.525994: step 1550, loss 0.0678523, acc 0.98
2016-09-07T19:12:08.189164: step 1551, loss 0.0670274, acc 0.98
2016-09-07T19:12:08.557751: step 1552, loss 0.000880606, acc 1
2016-09-07T19:12:09.246916: step 1553, loss 0.0525063, acc 1
2016-09-07T19:12:09.903388: step 1554, loss 0.0423132, acc 0.98
2016-09-07T19:12:10.607626: step 1555, loss 0.0614845, acc 0.96
2016-09-07T19:12:11.264227: step 1556, loss 0.0154372, acc 1
2016-09-07T19:12:11.925413: step 1557, loss 0.0621027, acc 0.96
2016-09-07T19:12:12.593901: step 1558, loss 0.192122, acc 0.96
2016-09-07T19:12:13.265529: step 1559, loss 0.0219341, acc 1
2016-09-07T19:12:13.935379: step 1560, loss 0.104653, acc 0.92
2016-09-07T19:12:14.612049: step 1561, loss 0.0283654, acc 1
2016-09-07T19:12:15.285364: step 1562, loss 0.0823413, acc 0.96
2016-09-07T19:12:15.960434: step 1563, loss 0.0657313, acc 0.96
2016-09-07T19:12:16.625810: step 1564, loss 0.0781692, acc 0.98
2016-09-07T19:12:17.308151: step 1565, loss 0.144458, acc 0.92
2016-09-07T19:12:17.979183: step 1566, loss 0.0763212, acc 0.96
2016-09-07T19:12:18.657868: step 1567, loss 0.0189078, acc 1
2016-09-07T19:12:19.327849: step 1568, loss 0.0585497, acc 0.98
2016-09-07T19:12:19.993521: step 1569, loss 0.183482, acc 0.94
2016-09-07T19:12:20.662820: step 1570, loss 0.02487, acc 0.98
2016-09-07T19:12:21.324706: step 1571, loss 0.10164, acc 0.96
2016-09-07T19:12:21.996980: step 1572, loss 0.160181, acc 0.98
2016-09-07T19:12:22.648550: step 1573, loss 0.0407339, acc 0.98
2016-09-07T19:12:23.319188: step 1574, loss 0.0503799, acc 1
2016-09-07T19:12:24.002673: step 1575, loss 0.0900722, acc 0.94
2016-09-07T19:12:24.659475: step 1576, loss 0.159161, acc 0.92
2016-09-07T19:12:25.321792: step 1577, loss 0.09293, acc 0.96
2016-09-07T19:12:26.001938: step 1578, loss 0.0109967, acc 1
2016-09-07T19:12:26.659515: step 1579, loss 0.0694512, acc 0.96
2016-09-07T19:12:27.322186: step 1580, loss 0.137947, acc 0.96
2016-09-07T19:12:27.987185: step 1581, loss 0.0263967, acc 1
2016-09-07T19:12:28.667922: step 1582, loss 0.106324, acc 0.94
2016-09-07T19:12:29.338831: step 1583, loss 0.0773403, acc 0.96
2016-09-07T19:12:30.008744: step 1584, loss 0.116663, acc 0.94
2016-09-07T19:12:30.683794: step 1585, loss 0.132346, acc 0.98
2016-09-07T19:12:31.357052: step 1586, loss 0.0692092, acc 0.96
2016-09-07T19:12:32.008831: step 1587, loss 0.0921098, acc 0.96
2016-09-07T19:12:32.683851: step 1588, loss 0.10229, acc 0.98
2016-09-07T19:12:33.347308: step 1589, loss 0.0634265, acc 0.98
2016-09-07T19:12:34.023259: step 1590, loss 0.056546, acc 0.98
2016-09-07T19:12:34.680546: step 1591, loss 0.107009, acc 0.94
2016-09-07T19:12:35.345292: step 1592, loss 0.0752685, acc 0.98
2016-09-07T19:12:36.014690: step 1593, loss 0.0821526, acc 0.96
2016-09-07T19:12:36.697360: step 1594, loss 0.0652846, acc 0.98
2016-09-07T19:12:37.347440: step 1595, loss 0.0440555, acc 0.98
2016-09-07T19:12:38.025299: step 1596, loss 0.0463697, acc 0.98
2016-09-07T19:12:38.687785: step 1597, loss 0.174387, acc 0.92
2016-09-07T19:12:39.402987: step 1598, loss 0.11058, acc 0.94
2016-09-07T19:12:40.065635: step 1599, loss 0.0126486, acc 1
2016-09-07T19:12:40.748246: step 1600, loss 0.0442227, acc 0.96

Evaluation:
2016-09-07T19:12:43.770409: step 1600, loss 0.964601, acc 0.769

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-1600

2016-09-07T19:12:45.361404: step 1601, loss 0.0264624, acc 1
2016-09-07T19:12:46.030276: step 1602, loss 0.0272361, acc 0.98
2016-09-07T19:12:46.700294: step 1603, loss 0.0681143, acc 0.96
2016-09-07T19:12:47.371067: step 1604, loss 0.0897022, acc 0.92
2016-09-07T19:12:48.029748: step 1605, loss 0.0268148, acc 0.98
2016-09-07T19:12:48.688053: step 1606, loss 0.0704083, acc 0.96
2016-09-07T19:12:49.370154: step 1607, loss 0.0814437, acc 0.98
2016-09-07T19:12:50.032311: step 1608, loss 0.187446, acc 0.94
2016-09-07T19:12:50.697701: step 1609, loss 0.107381, acc 0.94
2016-09-07T19:12:51.365943: step 1610, loss 0.0227336, acc 0.98
2016-09-07T19:12:52.037428: step 1611, loss 0.00894783, acc 1
2016-09-07T19:12:52.702875: step 1612, loss 0.0172423, acc 1
2016-09-07T19:12:53.378939: step 1613, loss 0.0915289, acc 0.96
2016-09-07T19:12:54.053384: step 1614, loss 0.0275338, acc 1
2016-09-07T19:12:54.720033: step 1615, loss 0.0435242, acc 0.98
2016-09-07T19:12:55.401880: step 1616, loss 0.138177, acc 0.94
2016-09-07T19:12:56.058943: step 1617, loss 0.0246169, acc 1
2016-09-07T19:12:56.733429: step 1618, loss 0.102029, acc 0.94
2016-09-07T19:12:57.384721: step 1619, loss 0.09361, acc 0.92
2016-09-07T19:12:58.047228: step 1620, loss 0.106458, acc 0.94
2016-09-07T19:12:58.728105: step 1621, loss 0.0573963, acc 0.96
2016-09-07T19:12:59.372899: step 1622, loss 0.126704, acc 0.94
2016-09-07T19:13:00.053947: step 1623, loss 0.065957, acc 0.96
2016-09-07T19:13:00.782345: step 1624, loss 0.0416694, acc 0.98
2016-09-07T19:13:01.444628: step 1625, loss 0.0250779, acc 1
2016-09-07T19:13:02.104539: step 1626, loss 0.0296883, acc 1
2016-09-07T19:13:02.755044: step 1627, loss 0.0766313, acc 0.96
2016-09-07T19:13:03.406212: step 1628, loss 0.320182, acc 0.92
2016-09-07T19:13:04.085422: step 1629, loss 0.12038, acc 0.98
2016-09-07T19:13:04.742428: step 1630, loss 0.0912429, acc 0.94
2016-09-07T19:13:05.411494: step 1631, loss 0.0519072, acc 1
2016-09-07T19:13:06.075977: step 1632, loss 0.0463331, acc 0.98
2016-09-07T19:13:06.718058: step 1633, loss 0.0774614, acc 0.98
2016-09-07T19:13:07.385119: step 1634, loss 0.125685, acc 0.94
2016-09-07T19:13:08.071779: step 1635, loss 0.0804364, acc 0.94
2016-09-07T19:13:08.736870: step 1636, loss 0.052694, acc 0.98
2016-09-07T19:13:09.403556: step 1637, loss 0.0651325, acc 0.96
2016-09-07T19:13:10.092737: step 1638, loss 0.0239634, acc 1
2016-09-07T19:13:10.757994: step 1639, loss 0.0352785, acc 0.98
2016-09-07T19:13:11.426480: step 1640, loss 0.0479781, acc 0.98
2016-09-07T19:13:12.102602: step 1641, loss 0.0663303, acc 0.98
2016-09-07T19:13:12.775269: step 1642, loss 0.121486, acc 0.92
2016-09-07T19:13:13.453718: step 1643, loss 0.0726351, acc 0.96
2016-09-07T19:13:14.133397: step 1644, loss 0.0235241, acc 1
2016-09-07T19:13:14.820821: step 1645, loss 0.0839614, acc 0.96
2016-09-07T19:13:15.503438: step 1646, loss 0.134219, acc 0.92
2016-09-07T19:13:16.168579: step 1647, loss 0.0349559, acc 1
2016-09-07T19:13:16.855634: step 1648, loss 0.0410842, acc 0.96
2016-09-07T19:13:17.530162: step 1649, loss 0.060207, acc 0.98
2016-09-07T19:13:18.206972: step 1650, loss 0.0994019, acc 0.96
2016-09-07T19:13:18.881566: step 1651, loss 0.0713554, acc 0.96
2016-09-07T19:13:19.540747: step 1652, loss 0.1058, acc 0.96
2016-09-07T19:13:20.222667: step 1653, loss 0.0279575, acc 0.98
2016-09-07T19:13:20.889959: step 1654, loss 0.0389434, acc 1
2016-09-07T19:13:21.557716: step 1655, loss 0.0435881, acc 0.98
2016-09-07T19:13:22.226862: step 1656, loss 0.0937025, acc 0.98
2016-09-07T19:13:22.897378: step 1657, loss 0.058817, acc 0.96
2016-09-07T19:13:23.574907: step 1658, loss 0.130708, acc 0.94
2016-09-07T19:13:24.230926: step 1659, loss 0.0676761, acc 0.94
2016-09-07T19:13:24.921795: step 1660, loss 0.0303315, acc 1
2016-09-07T19:13:25.589614: step 1661, loss 0.0944779, acc 0.96
2016-09-07T19:13:26.238863: step 1662, loss 0.0422733, acc 0.98
2016-09-07T19:13:26.907163: step 1663, loss 0.0742076, acc 0.94
2016-09-07T19:13:27.574708: step 1664, loss 0.0230485, acc 1
2016-09-07T19:13:28.243729: step 1665, loss 0.143932, acc 0.94
2016-09-07T19:13:28.892292: step 1666, loss 0.0555899, acc 0.96
2016-09-07T19:13:29.575246: step 1667, loss 0.177717, acc 0.96
2016-09-07T19:13:30.242910: step 1668, loss 0.0436154, acc 1
2016-09-07T19:13:30.908592: step 1669, loss 0.0823373, acc 0.96
2016-09-07T19:13:31.574865: step 1670, loss 0.0559336, acc 0.96
2016-09-07T19:13:32.245785: step 1671, loss 0.0610925, acc 0.96
2016-09-07T19:13:32.922621: step 1672, loss 0.108322, acc 0.94
2016-09-07T19:13:33.610649: step 1673, loss 0.0097085, acc 1
2016-09-07T19:13:34.293698: step 1674, loss 0.065366, acc 0.96
2016-09-07T19:13:34.982259: step 1675, loss 0.030001, acc 1
2016-09-07T19:13:35.638664: step 1676, loss 0.169119, acc 0.92
2016-09-07T19:13:36.329520: step 1677, loss 0.0939303, acc 0.98
2016-09-07T19:13:36.980825: step 1678, loss 0.12142, acc 0.96
2016-09-07T19:13:37.638380: step 1679, loss 0.275838, acc 0.92
2016-09-07T19:13:38.292602: step 1680, loss 0.0244831, acc 1
2016-09-07T19:13:38.946000: step 1681, loss 0.0758851, acc 0.98
2016-09-07T19:13:39.640020: step 1682, loss 0.111944, acc 0.96
2016-09-07T19:13:40.306671: step 1683, loss 0.118363, acc 0.96
2016-09-07T19:13:40.987855: step 1684, loss 0.155052, acc 0.94
2016-09-07T19:13:41.642286: step 1685, loss 0.157665, acc 0.92
2016-09-07T19:13:42.315669: step 1686, loss 0.12834, acc 0.92
2016-09-07T19:13:42.989929: step 1687, loss 0.131341, acc 0.94
2016-09-07T19:13:43.671003: step 1688, loss 0.0373791, acc 0.98
2016-09-07T19:13:44.343559: step 1689, loss 0.109554, acc 0.96
2016-09-07T19:13:45.027419: step 1690, loss 0.066025, acc 0.96
2016-09-07T19:13:45.699608: step 1691, loss 0.0156186, acc 1
2016-09-07T19:13:46.375496: step 1692, loss 0.126603, acc 0.94
2016-09-07T19:13:47.045678: step 1693, loss 0.210839, acc 0.96
2016-09-07T19:13:47.732044: step 1694, loss 0.0753414, acc 0.96
2016-09-07T19:13:48.415124: step 1695, loss 0.0388374, acc 1
2016-09-07T19:13:49.074016: step 1696, loss 0.0144, acc 1
2016-09-07T19:13:49.756377: step 1697, loss 0.04611, acc 0.98
2016-09-07T19:13:50.434983: step 1698, loss 0.0636672, acc 0.96
2016-09-07T19:13:51.087047: step 1699, loss 0.105283, acc 0.96
2016-09-07T19:13:51.762545: step 1700, loss 0.0724381, acc 0.96

Evaluation:
2016-09-07T19:13:54.803439: step 1700, loss 1.04721, acc 0.75

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-1700

2016-09-07T19:13:56.412643: step 1701, loss 0.125822, acc 0.92
2016-09-07T19:13:57.076889: step 1702, loss 0.0238466, acc 1
2016-09-07T19:13:57.748748: step 1703, loss 0.0571718, acc 0.96
2016-09-07T19:13:58.414791: step 1704, loss 0.138236, acc 0.94
2016-09-07T19:13:59.070787: step 1705, loss 0.0780431, acc 0.96
2016-09-07T19:13:59.725146: step 1706, loss 0.352206, acc 0.92
2016-09-07T19:14:00.418592: step 1707, loss 0.151983, acc 0.9
2016-09-07T19:14:01.067500: step 1708, loss 0.0444479, acc 0.98
2016-09-07T19:14:01.725282: step 1709, loss 0.0130327, acc 1
2016-09-07T19:14:02.381408: step 1710, loss 0.183143, acc 0.92
2016-09-07T19:14:03.066490: step 1711, loss 0.103478, acc 0.96
2016-09-07T19:14:03.757861: step 1712, loss 0.0395196, acc 0.98
2016-09-07T19:14:04.430331: step 1713, loss 0.072472, acc 0.94
2016-09-07T19:14:05.087147: step 1714, loss 0.027191, acc 1
2016-09-07T19:14:05.774957: step 1715, loss 0.0658204, acc 0.96
2016-09-07T19:14:06.435006: step 1716, loss 0.0643409, acc 0.96
2016-09-07T19:14:07.096629: step 1717, loss 0.0417496, acc 0.98
2016-09-07T19:14:07.743078: step 1718, loss 0.140398, acc 0.92
2016-09-07T19:14:08.393141: step 1719, loss 0.0198821, acc 1
2016-09-07T19:14:09.053375: step 1720, loss 0.0910637, acc 0.98
2016-09-07T19:14:09.713472: step 1721, loss 0.0683624, acc 0.98
2016-09-07T19:14:10.387657: step 1722, loss 0.101084, acc 0.92
2016-09-07T19:14:11.104839: step 1723, loss 0.120147, acc 0.94
2016-09-07T19:14:11.799219: step 1724, loss 0.104772, acc 0.96
2016-09-07T19:14:12.469778: step 1725, loss 0.15819, acc 0.92
2016-09-07T19:14:13.145222: step 1726, loss 0.00580557, acc 1
2016-09-07T19:14:13.818974: step 1727, loss 0.102294, acc 0.98
2016-09-07T19:14:14.494313: step 1728, loss 0.0987915, acc 0.96
2016-09-07T19:14:15.149623: step 1729, loss 0.0828109, acc 0.96
2016-09-07T19:14:15.820078: step 1730, loss 0.121683, acc 0.96
2016-09-07T19:14:16.487435: step 1731, loss 0.042884, acc 0.96
2016-09-07T19:14:17.144511: step 1732, loss 0.0725959, acc 0.96
2016-09-07T19:14:17.815033: step 1733, loss 0.0692896, acc 0.98
2016-09-07T19:14:18.490029: step 1734, loss 0.0643293, acc 0.96
2016-09-07T19:14:19.140362: step 1735, loss 0.0208797, acc 0.98
2016-09-07T19:14:19.795590: step 1736, loss 0.0442808, acc 0.98
2016-09-07T19:14:20.449971: step 1737, loss 0.0760234, acc 0.98
2016-09-07T19:14:21.118537: step 1738, loss 0.0349079, acc 1
2016-09-07T19:14:21.777445: step 1739, loss 0.136533, acc 0.92
2016-09-07T19:14:22.450862: step 1740, loss 0.058169, acc 0.98
2016-09-07T19:14:23.133535: step 1741, loss 0.0834023, acc 0.94
2016-09-07T19:14:23.795921: step 1742, loss 0.0733821, acc 0.98
2016-09-07T19:14:24.473026: step 1743, loss 0.208089, acc 0.94
2016-09-07T19:14:25.145916: step 1744, loss 0.0917392, acc 0.94
2016-09-07T19:14:25.838360: step 1745, loss 0.0578588, acc 0.98
2016-09-07T19:14:26.200389: step 1746, loss 0.00179433, acc 1
2016-09-07T19:14:26.886153: step 1747, loss 0.123441, acc 0.94
2016-09-07T19:14:27.555052: step 1748, loss 0.126701, acc 0.98
2016-09-07T19:14:28.238088: step 1749, loss 0.163499, acc 0.96
2016-09-07T19:14:28.927447: step 1750, loss 0.0179813, acc 1
2016-09-07T19:14:29.599325: step 1751, loss 0.0640574, acc 0.98
2016-09-07T19:14:30.274302: step 1752, loss 0.0526606, acc 1
2016-09-07T19:14:30.930725: step 1753, loss 0.0553491, acc 0.98
2016-09-07T19:14:31.604554: step 1754, loss 0.0478173, acc 0.98
2016-09-07T19:14:32.268477: step 1755, loss 0.045019, acc 0.98
2016-09-07T19:14:32.945258: step 1756, loss 0.0301402, acc 1
2016-09-07T19:14:33.602650: step 1757, loss 0.0291695, acc 1
2016-09-07T19:14:34.283611: step 1758, loss 0.0910416, acc 0.96
2016-09-07T19:14:34.953660: step 1759, loss 0.0212768, acc 1
2016-09-07T19:14:35.622394: step 1760, loss 0.0518557, acc 0.98
2016-09-07T19:14:36.272883: step 1761, loss 0.00159887, acc 1
2016-09-07T19:14:36.931635: step 1762, loss 0.0589335, acc 0.96
2016-09-07T19:14:37.611599: step 1763, loss 0.0296654, acc 0.98
2016-09-07T19:14:38.267143: step 1764, loss 0.183149, acc 0.92
2016-09-07T19:14:38.935269: step 1765, loss 0.0645617, acc 0.94
2016-09-07T19:14:39.598879: step 1766, loss 0.0605104, acc 0.98
2016-09-07T19:14:40.271173: step 1767, loss 0.0667472, acc 0.96
2016-09-07T19:14:40.933069: step 1768, loss 0.0961053, acc 0.94
2016-09-07T19:14:41.617271: step 1769, loss 0.0106418, acc 1
2016-09-07T19:14:42.276002: step 1770, loss 0.0371879, acc 0.98
2016-09-07T19:14:42.939842: step 1771, loss 0.0398921, acc 0.98
2016-09-07T19:14:43.604893: step 1772, loss 0.0147432, acc 1
2016-09-07T19:14:44.274897: step 1773, loss 0.016513, acc 1
2016-09-07T19:14:44.938296: step 1774, loss 0.0917913, acc 0.98
2016-09-07T19:14:45.612474: step 1775, loss 0.0666223, acc 0.96
2016-09-07T19:14:46.294074: step 1776, loss 0.0115745, acc 1
2016-09-07T19:14:46.965986: step 1777, loss 0.0431475, acc 0.96
2016-09-07T19:14:47.636894: step 1778, loss 0.0785055, acc 0.96
2016-09-07T19:14:48.311343: step 1779, loss 0.0433413, acc 0.98
2016-09-07T19:14:48.973131: step 1780, loss 0.0743606, acc 0.94
2016-09-07T19:14:49.653981: step 1781, loss 0.0486606, acc 0.98
2016-09-07T19:14:50.322989: step 1782, loss 0.0411852, acc 0.98
2016-09-07T19:14:50.989549: step 1783, loss 0.048108, acc 0.98
2016-09-07T19:14:51.654646: step 1784, loss 0.0426274, acc 1
2016-09-07T19:14:52.328536: step 1785, loss 0.0325598, acc 0.98
2016-09-07T19:14:53.009919: step 1786, loss 0.0535783, acc 0.98
2016-09-07T19:14:53.675938: step 1787, loss 0.139562, acc 0.96
2016-09-07T19:14:54.391706: step 1788, loss 0.0806417, acc 0.98
2016-09-07T19:14:55.066740: step 1789, loss 0.0834045, acc 0.96
2016-09-07T19:14:55.738791: step 1790, loss 0.00090181, acc 1
2016-09-07T19:14:56.393004: step 1791, loss 0.0163753, acc 1
2016-09-07T19:14:57.050629: step 1792, loss 0.170721, acc 0.92
2016-09-07T19:14:57.715156: step 1793, loss 0.06329, acc 0.94
2016-09-07T19:14:58.395594: step 1794, loss 0.118991, acc 0.96
2016-09-07T19:14:59.068397: step 1795, loss 0.0142552, acc 1
2016-09-07T19:14:59.722296: step 1796, loss 0.0478773, acc 0.98
2016-09-07T19:15:00.412849: step 1797, loss 0.00286464, acc 1
2016-09-07T19:15:01.080322: step 1798, loss 0.0546015, acc 0.98
2016-09-07T19:15:01.742790: step 1799, loss 0.082101, acc 0.96
2016-09-07T19:15:02.396432: step 1800, loss 0.0427898, acc 0.98

Evaluation:
2016-09-07T19:15:05.436906: step 1800, loss 1.17155, acc 0.774

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-1800

2016-09-07T19:15:07.097974: step 1801, loss 0.0675098, acc 0.96
2016-09-07T19:15:07.762345: step 1802, loss 0.0778268, acc 0.96
2016-09-07T19:15:08.416783: step 1803, loss 0.0718908, acc 0.96
2016-09-07T19:15:09.085623: step 1804, loss 0.0481128, acc 0.98
2016-09-07T19:15:09.750064: step 1805, loss 0.0606016, acc 0.98
2016-09-07T19:15:10.405664: step 1806, loss 0.0525089, acc 0.96
2016-09-07T19:15:11.078828: step 1807, loss 0.0875274, acc 0.96
2016-09-07T19:15:11.763013: step 1808, loss 0.0568385, acc 0.96
2016-09-07T19:15:12.421818: step 1809, loss 0.0966818, acc 0.94
2016-09-07T19:15:13.099821: step 1810, loss 0.0458317, acc 1
2016-09-07T19:15:13.762031: step 1811, loss 0.0357301, acc 1
2016-09-07T19:15:14.430870: step 1812, loss 0.0780941, acc 0.98
2016-09-07T19:15:15.085289: step 1813, loss 0.0507271, acc 0.96
2016-09-07T19:15:15.730889: step 1814, loss 0.0909014, acc 0.96
2016-09-07T19:15:16.413304: step 1815, loss 0.0492917, acc 0.96
2016-09-07T19:15:17.077245: step 1816, loss 0.0118762, acc 1
2016-09-07T19:15:17.750693: step 1817, loss 0.075715, acc 0.98
2016-09-07T19:15:18.424451: step 1818, loss 0.0216987, acc 1
2016-09-07T19:15:19.093820: step 1819, loss 0.136191, acc 0.94
2016-09-07T19:15:19.748706: step 1820, loss 0.0789552, acc 0.98
2016-09-07T19:15:20.401477: step 1821, loss 0.0286744, acc 0.98
2016-09-07T19:15:21.056301: step 1822, loss 0.0712963, acc 0.94
2016-09-07T19:15:21.739311: step 1823, loss 0.081353, acc 0.96
2016-09-07T19:15:22.387002: step 1824, loss 0.0640657, acc 0.96
2016-09-07T19:15:23.069205: step 1825, loss 0.128025, acc 0.96
2016-09-07T19:15:23.749931: step 1826, loss 0.0523275, acc 0.98
2016-09-07T19:15:24.436917: step 1827, loss 0.130664, acc 0.96
2016-09-07T19:15:25.119777: step 1828, loss 0.0221523, acc 0.98
2016-09-07T19:15:25.800753: step 1829, loss 0.190519, acc 0.92
2016-09-07T19:15:26.464694: step 1830, loss 0.0268257, acc 1
2016-09-07T19:15:27.131558: step 1831, loss 0.0482439, acc 0.98
2016-09-07T19:15:27.804456: step 1832, loss 0.0415749, acc 0.98
2016-09-07T19:15:28.492202: step 1833, loss 0.17957, acc 0.94
2016-09-07T19:15:29.157359: step 1834, loss 0.067431, acc 0.98
2016-09-07T19:15:29.845032: step 1835, loss 0.0705852, acc 0.96
2016-09-07T19:15:30.532604: step 1836, loss 0.00677795, acc 1
2016-09-07T19:15:31.207555: step 1837, loss 0.0586241, acc 0.98
2016-09-07T19:15:31.875266: step 1838, loss 0.0893855, acc 0.94
2016-09-07T19:15:32.545944: step 1839, loss 0.170313, acc 0.9
2016-09-07T19:15:33.214704: step 1840, loss 0.033476, acc 0.98
2016-09-07T19:15:33.889846: step 1841, loss 0.0887218, acc 0.96
2016-09-07T19:15:34.559442: step 1842, loss 0.0636395, acc 0.98
2016-09-07T19:15:35.242599: step 1843, loss 0.0579645, acc 0.98
2016-09-07T19:15:35.904780: step 1844, loss 0.0860547, acc 0.98
2016-09-07T19:15:36.564821: step 1845, loss 0.0211716, acc 0.98
2016-09-07T19:15:37.234933: step 1846, loss 0.0896326, acc 0.96
2016-09-07T19:15:37.913186: step 1847, loss 0.0390159, acc 0.98
2016-09-07T19:15:38.574953: step 1848, loss 0.0350451, acc 0.98
2016-09-07T19:15:39.249742: step 1849, loss 0.0638419, acc 0.98
2016-09-07T19:15:39.942543: step 1850, loss 0.02343, acc 1
2016-09-07T19:15:40.597712: step 1851, loss 0.0505211, acc 0.98
2016-09-07T19:15:41.246170: step 1852, loss 0.132027, acc 0.92
2016-09-07T19:15:41.920329: step 1853, loss 0.209477, acc 0.92
2016-09-07T19:15:42.589798: step 1854, loss 0.13413, acc 0.96
2016-09-07T19:15:43.248582: step 1855, loss 0.0681248, acc 0.96
2016-09-07T19:15:43.920459: step 1856, loss 0.0642855, acc 0.94
2016-09-07T19:15:44.602669: step 1857, loss 0.104298, acc 0.96
2016-09-07T19:15:45.245760: step 1858, loss 0.0362674, acc 0.98
2016-09-07T19:15:45.938887: step 1859, loss 0.15929, acc 0.98
2016-09-07T19:15:46.622624: step 1860, loss 0.0489203, acc 0.98
2016-09-07T19:15:47.298727: step 1861, loss 0.0653499, acc 0.98
2016-09-07T19:15:47.960068: step 1862, loss 0.0232879, acc 1
2016-09-07T19:15:48.610221: step 1863, loss 0.0843651, acc 0.96
2016-09-07T19:15:49.280153: step 1864, loss 0.102565, acc 0.94
2016-09-07T19:15:49.941019: step 1865, loss 0.0947536, acc 0.94
2016-09-07T19:15:50.616880: step 1866, loss 0.0926487, acc 0.94
2016-09-07T19:15:51.273380: step 1867, loss 0.0664909, acc 1
2016-09-07T19:15:51.951858: step 1868, loss 0.0694324, acc 0.96
2016-09-07T19:15:52.610885: step 1869, loss 0.00783733, acc 1
2016-09-07T19:15:53.291138: step 1870, loss 0.0710138, acc 0.96
2016-09-07T19:15:53.958457: step 1871, loss 0.113311, acc 0.94
2016-09-07T19:15:54.622865: step 1872, loss 0.0305539, acc 0.98
2016-09-07T19:15:55.288839: step 1873, loss 0.121866, acc 0.94
2016-09-07T19:15:55.955152: step 1874, loss 0.0359284, acc 0.98
2016-09-07T19:15:56.623221: step 1875, loss 0.0336118, acc 0.98
2016-09-07T19:15:57.279604: step 1876, loss 0.0349455, acc 1
2016-09-07T19:15:57.946797: step 1877, loss 0.0485026, acc 0.98
2016-09-07T19:15:58.609666: step 1878, loss 0.0959089, acc 0.96
2016-09-07T19:15:59.284372: step 1879, loss 0.0923212, acc 0.96
2016-09-07T19:15:59.974089: step 1880, loss 0.0430835, acc 0.96
2016-09-07T19:16:00.664479: step 1881, loss 0.0588706, acc 0.98
2016-09-07T19:16:01.328492: step 1882, loss 0.058263, acc 1
2016-09-07T19:16:01.990664: step 1883, loss 0.0918806, acc 0.96
2016-09-07T19:16:02.633677: step 1884, loss 0.0289125, acc 1
2016-09-07T19:16:03.303332: step 1885, loss 0.109183, acc 0.94
2016-09-07T19:16:03.971727: step 1886, loss 0.057312, acc 0.98
2016-09-07T19:16:04.651882: step 1887, loss 0.0792665, acc 0.96
2016-09-07T19:16:05.313162: step 1888, loss 0.0645663, acc 0.94
2016-09-07T19:16:05.970066: step 1889, loss 0.059448, acc 0.96
2016-09-07T19:16:06.627843: step 1890, loss 0.0183039, acc 1
2016-09-07T19:16:07.314220: step 1891, loss 0.073323, acc 0.96
2016-09-07T19:16:07.998288: step 1892, loss 0.0685407, acc 0.96
2016-09-07T19:16:08.661852: step 1893, loss 0.0243188, acc 1
2016-09-07T19:16:09.351864: step 1894, loss 0.134584, acc 0.98
2016-09-07T19:16:10.019259: step 1895, loss 0.0971235, acc 0.94
2016-09-07T19:16:10.687290: step 1896, loss 0.0656314, acc 0.98
2016-09-07T19:16:11.350027: step 1897, loss 0.177257, acc 0.96
2016-09-07T19:16:12.026078: step 1898, loss 0.0962842, acc 0.98
2016-09-07T19:16:12.706265: step 1899, loss 0.0993382, acc 0.94
2016-09-07T19:16:13.390264: step 1900, loss 0.112292, acc 0.98

Evaluation:
2016-09-07T19:16:16.452071: step 1900, loss 1.06381, acc 0.762

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-1900

2016-09-07T19:16:18.094897: step 1901, loss 0.0355312, acc 1
2016-09-07T19:16:18.762842: step 1902, loss 0.0340873, acc 0.98
2016-09-07T19:16:19.438815: step 1903, loss 0.0213778, acc 1
2016-09-07T19:16:20.102022: step 1904, loss 0.118259, acc 0.96
2016-09-07T19:16:20.782034: step 1905, loss 0.0188279, acc 1
2016-09-07T19:16:21.458321: step 1906, loss 0.043158, acc 0.98
2016-09-07T19:16:22.127705: step 1907, loss 0.105615, acc 0.96
2016-09-07T19:16:22.805004: step 1908, loss 0.0246839, acc 0.98
2016-09-07T19:16:23.451978: step 1909, loss 0.0441013, acc 0.96
2016-09-07T19:16:24.102709: step 1910, loss 0.0498747, acc 0.98
2016-09-07T19:16:24.786878: step 1911, loss 0.0427178, acc 0.98
2016-09-07T19:16:25.506212: step 1912, loss 0.0636876, acc 0.96
2016-09-07T19:16:26.189908: step 1913, loss 0.014657, acc 1
2016-09-07T19:16:26.852565: step 1914, loss 0.0917294, acc 0.94
2016-09-07T19:16:27.519819: step 1915, loss 0.159885, acc 0.88
2016-09-07T19:16:28.203848: step 1916, loss 0.127078, acc 0.94
2016-09-07T19:16:28.889945: step 1917, loss 0.0338575, acc 0.98
2016-09-07T19:16:29.563188: step 1918, loss 0.225707, acc 0.94
2016-09-07T19:16:30.251108: step 1919, loss 0.0122097, acc 1
2016-09-07T19:16:30.916927: step 1920, loss 0.0126573, acc 1
2016-09-07T19:16:31.583666: step 1921, loss 0.112968, acc 0.94
2016-09-07T19:16:32.265677: step 1922, loss 0.096926, acc 0.94
2016-09-07T19:16:32.939809: step 1923, loss 0.0580322, acc 0.98
2016-09-07T19:16:33.620776: step 1924, loss 0.0822698, acc 0.98
2016-09-07T19:16:34.286924: step 1925, loss 0.118691, acc 0.94
2016-09-07T19:16:34.936148: step 1926, loss 0.0750741, acc 0.96
2016-09-07T19:16:35.583974: step 1927, loss 0.158575, acc 0.94
2016-09-07T19:16:36.260274: step 1928, loss 0.142907, acc 0.96
2016-09-07T19:16:36.911307: step 1929, loss 0.039839, acc 0.98
2016-09-07T19:16:37.569856: step 1930, loss 0.109357, acc 0.94
2016-09-07T19:16:38.233568: step 1931, loss 0.0339373, acc 0.98
2016-09-07T19:16:38.918593: step 1932, loss 0.061675, acc 0.98
2016-09-07T19:16:39.565664: step 1933, loss 0.0638429, acc 0.96
2016-09-07T19:16:40.229918: step 1934, loss 0.0162145, acc 1
2016-09-07T19:16:40.911699: step 1935, loss 0.16556, acc 0.94
2016-09-07T19:16:41.596325: step 1936, loss 0.089943, acc 0.98
2016-09-07T19:16:42.286007: step 1937, loss 0.134777, acc 0.96
2016-09-07T19:16:42.962842: step 1938, loss 0.0401171, acc 0.98
2016-09-07T19:16:43.623350: step 1939, loss 0.0819081, acc 0.96
2016-09-07T19:16:43.988144: step 1940, loss 0.567976, acc 0.833333
2016-09-07T19:16:44.674295: step 1941, loss 0.0524112, acc 0.96
2016-09-07T19:16:45.337099: step 1942, loss 0.0641133, acc 1
2016-09-07T19:16:46.017515: step 1943, loss 0.0611652, acc 0.98
2016-09-07T19:16:46.685845: step 1944, loss 0.0617522, acc 0.96
2016-09-07T19:16:47.376919: step 1945, loss 0.107846, acc 0.96
2016-09-07T19:16:48.040964: step 1946, loss 0.0612247, acc 0.98
2016-09-07T19:16:48.717479: step 1947, loss 0.0919701, acc 0.94
2016-09-07T19:16:49.380510: step 1948, loss 0.0333861, acc 1
2016-09-07T19:16:50.042922: step 1949, loss 0.0625892, acc 0.96
2016-09-07T19:16:50.710372: step 1950, loss 0.0423475, acc 0.98
2016-09-07T19:16:51.368658: step 1951, loss 0.0403158, acc 1
2016-09-07T19:16:52.043076: step 1952, loss 0.102973, acc 0.96
2016-09-07T19:16:52.719735: step 1953, loss 0.0835469, acc 0.96
2016-09-07T19:16:53.403855: step 1954, loss 0.113056, acc 0.92
2016-09-07T19:16:54.079581: step 1955, loss 0.0801962, acc 0.98
2016-09-07T19:16:54.742622: step 1956, loss 0.040512, acc 0.98
2016-09-07T19:16:55.399270: step 1957, loss 0.145627, acc 0.94
2016-09-07T19:16:56.068012: step 1958, loss 0.0766911, acc 0.98
2016-09-07T19:16:56.741396: step 1959, loss 0.132697, acc 0.92
2016-09-07T19:16:57.400069: step 1960, loss 0.0621217, acc 0.98
2016-09-07T19:16:58.060699: step 1961, loss 0.0310475, acc 0.98
2016-09-07T19:16:58.745776: step 1962, loss 0.0617184, acc 0.96
2016-09-07T19:16:59.405121: step 1963, loss 0.0255578, acc 1
2016-09-07T19:17:00.075392: step 1964, loss 0.013842, acc 1
2016-09-07T19:17:00.772459: step 1965, loss 0.0497918, acc 0.98
2016-09-07T19:17:01.435436: step 1966, loss 0.0925911, acc 0.96
2016-09-07T19:17:02.102899: step 1967, loss 0.0136388, acc 1
2016-09-07T19:17:02.773945: step 1968, loss 0.0230435, acc 0.98
2016-09-07T19:17:03.448844: step 1969, loss 0.0265839, acc 1
2016-09-07T19:17:04.141476: step 1970, loss 0.0346019, acc 0.98
2016-09-07T19:17:04.835543: step 1971, loss 0.00969461, acc 1
2016-09-07T19:17:05.512189: step 1972, loss 0.11296, acc 0.98
2016-09-07T19:17:06.187523: step 1973, loss 0.0453304, acc 0.96
2016-09-07T19:17:06.857869: step 1974, loss 0.0260282, acc 1
2016-09-07T19:17:07.532363: step 1975, loss 0.0631433, acc 0.96
2016-09-07T19:17:08.225450: step 1976, loss 0.0792873, acc 0.96
2016-09-07T19:17:08.941514: step 1977, loss 0.0792145, acc 0.94
2016-09-07T19:17:09.606909: step 1978, loss 0.19507, acc 0.94
2016-09-07T19:17:10.291293: step 1979, loss 0.0384499, acc 0.98
2016-09-07T19:17:10.967712: step 1980, loss 0.0286391, acc 0.98
2016-09-07T19:17:11.650193: step 1981, loss 0.0541333, acc 0.98
2016-09-07T19:17:12.332005: step 1982, loss 0.109738, acc 0.94
2016-09-07T19:17:13.004989: step 1983, loss 0.0102406, acc 1
2016-09-07T19:17:13.673459: step 1984, loss 0.06243, acc 0.94
2016-09-07T19:17:14.345765: step 1985, loss 0.0489665, acc 0.98
2016-09-07T19:17:15.017811: step 1986, loss 0.145498, acc 0.92
2016-09-07T19:17:15.693942: step 1987, loss 0.0819219, acc 0.98
2016-09-07T19:17:16.363636: step 1988, loss 0.00431852, acc 1
2016-09-07T19:17:17.028710: step 1989, loss 0.120785, acc 0.96
2016-09-07T19:17:17.700445: step 1990, loss 0.0660469, acc 0.98
2016-09-07T19:17:18.372006: step 1991, loss 0.0468407, acc 0.98
2016-09-07T19:17:19.049415: step 1992, loss 0.0234543, acc 0.98
2016-09-07T19:17:19.710396: step 1993, loss 0.0108233, acc 1
2016-09-07T19:17:20.379551: step 1994, loss 0.0733907, acc 0.96
2016-09-07T19:17:21.047407: step 1995, loss 0.0420969, acc 0.98
2016-09-07T19:17:21.722006: step 1996, loss 0.0496366, acc 0.98
2016-09-07T19:17:22.379559: step 1997, loss 0.108796, acc 0.92
2016-09-07T19:17:23.032235: step 1998, loss 0.0269567, acc 0.98
2016-09-07T19:17:23.718519: step 1999, loss 0.114027, acc 0.96
2016-09-07T19:17:24.387106: step 2000, loss 0.143215, acc 0.94

Evaluation:
2016-09-07T19:17:27.479711: step 2000, loss 0.992801, acc 0.754

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-2000

2016-09-07T19:17:29.182824: step 2001, loss 0.0683369, acc 0.96
2016-09-07T19:17:29.852696: step 2002, loss 0.0347316, acc 0.98
2016-09-07T19:17:30.522893: step 2003, loss 0.0676822, acc 1
2016-09-07T19:17:31.188361: step 2004, loss 0.0420036, acc 1
2016-09-07T19:17:31.847079: step 2005, loss 0.0520804, acc 0.98
2016-09-07T19:17:32.513661: step 2006, loss 0.0869276, acc 0.96
2016-09-07T19:17:33.194209: step 2007, loss 0.0953253, acc 0.92
2016-09-07T19:17:33.839547: step 2008, loss 0.0517242, acc 0.98
2016-09-07T19:17:34.498286: step 2009, loss 0.0580498, acc 0.96
2016-09-07T19:17:35.189425: step 2010, loss 0.060546, acc 0.98
2016-09-07T19:17:35.863557: step 2011, loss 0.0216141, acc 1
2016-09-07T19:17:36.519058: step 2012, loss 0.0362217, acc 0.98
2016-09-07T19:17:37.191522: step 2013, loss 0.00304039, acc 1
2016-09-07T19:17:37.856875: step 2014, loss 0.0292975, acc 1
2016-09-07T19:17:38.538311: step 2015, loss 0.0418545, acc 0.98
2016-09-07T19:17:39.209607: step 2016, loss 0.0900888, acc 0.94
2016-09-07T19:17:39.894306: step 2017, loss 0.121705, acc 0.98
2016-09-07T19:17:40.567935: step 2018, loss 0.065858, acc 0.98
2016-09-07T19:17:41.213227: step 2019, loss 0.00352782, acc 1
2016-09-07T19:17:41.871075: step 2020, loss 0.119581, acc 0.94
2016-09-07T19:17:42.541435: step 2021, loss 0.0372842, acc 0.98
2016-09-07T19:17:43.215548: step 2022, loss 0.00873665, acc 1
2016-09-07T19:17:43.878860: step 2023, loss 0.0384782, acc 0.98
2016-09-07T19:17:44.546006: step 2024, loss 0.0114451, acc 1
2016-09-07T19:17:45.199322: step 2025, loss 0.0124479, acc 1
2016-09-07T19:17:45.866363: step 2026, loss 0.0566909, acc 0.96
2016-09-07T19:17:46.567994: step 2027, loss 0.0708569, acc 0.96
2016-09-07T19:17:47.229443: step 2028, loss 0.115384, acc 0.94
2016-09-07T19:17:47.904766: step 2029, loss 0.0104538, acc 1
2016-09-07T19:17:48.581118: step 2030, loss 0.140276, acc 0.94
2016-09-07T19:17:49.236573: step 2031, loss 0.12665, acc 0.96
2016-09-07T19:17:49.911043: step 2032, loss 0.0332829, acc 1
2016-09-07T19:17:50.559348: step 2033, loss 0.0507748, acc 0.98
2016-09-07T19:17:51.236695: step 2034, loss 0.0997756, acc 0.96
2016-09-07T19:17:51.899182: step 2035, loss 0.0597556, acc 0.98
2016-09-07T19:17:52.578251: step 2036, loss 0.0267905, acc 1
2016-09-07T19:17:53.248236: step 2037, loss 0.0313628, acc 1
2016-09-07T19:17:53.903789: step 2038, loss 0.0407525, acc 0.98
2016-09-07T19:17:54.611975: step 2039, loss 0.141705, acc 0.96
2016-09-07T19:17:55.272418: step 2040, loss 0.078183, acc 0.96
2016-09-07T19:17:55.920077: step 2041, loss 0.108695, acc 0.96
2016-09-07T19:17:56.581953: step 2042, loss 0.0294268, acc 1
2016-09-07T19:17:57.248070: step 2043, loss 0.0361356, acc 0.98
2016-09-07T19:17:57.917867: step 2044, loss 0.196672, acc 0.92
2016-09-07T19:17:58.589366: step 2045, loss 0.0316803, acc 1
2016-09-07T19:17:59.259508: step 2046, loss 0.147111, acc 0.96
2016-09-07T19:17:59.919775: step 2047, loss 0.0712455, acc 0.94
2016-09-07T19:18:00.630643: step 2048, loss 0.0441016, acc 0.98
2016-09-07T19:18:01.309867: step 2049, loss 0.0746598, acc 0.96
2016-09-07T19:18:01.991526: step 2050, loss 0.028011, acc 0.98
2016-09-07T19:18:02.660907: step 2051, loss 0.0775807, acc 0.96
2016-09-07T19:18:03.342065: step 2052, loss 0.0314299, acc 0.98
2016-09-07T19:18:04.015737: step 2053, loss 0.0863002, acc 0.96
2016-09-07T19:18:04.692555: step 2054, loss 0.0315928, acc 1
2016-09-07T19:18:05.375069: step 2055, loss 0.0486907, acc 1
2016-09-07T19:18:06.048271: step 2056, loss 0.0463879, acc 1
2016-09-07T19:18:06.711159: step 2057, loss 0.00909848, acc 1
2016-09-07T19:18:07.383286: step 2058, loss 0.0594581, acc 0.96
2016-09-07T19:18:08.046824: step 2059, loss 0.0840266, acc 0.96
2016-09-07T19:18:08.708994: step 2060, loss 0.146492, acc 0.96
2016-09-07T19:18:09.390699: step 2061, loss 0.0259254, acc 1
2016-09-07T19:18:10.072229: step 2062, loss 0.0843084, acc 0.96
2016-09-07T19:18:10.742609: step 2063, loss 0.0550938, acc 1
2016-09-07T19:18:11.421969: step 2064, loss 0.0138328, acc 1
2016-09-07T19:18:12.076848: step 2065, loss 0.0491831, acc 0.98
2016-09-07T19:18:12.757460: step 2066, loss 0.145808, acc 0.96
2016-09-07T19:18:13.395432: step 2067, loss 0.0466998, acc 0.96
2016-09-07T19:18:14.066402: step 2068, loss 0.041221, acc 1
2016-09-07T19:18:14.740443: step 2069, loss 0.0267363, acc 1
2016-09-07T19:18:15.402134: step 2070, loss 0.0706519, acc 0.96
2016-09-07T19:18:16.073109: step 2071, loss 0.112948, acc 0.96
2016-09-07T19:18:16.743727: step 2072, loss 0.021552, acc 0.98
2016-09-07T19:18:17.405409: step 2073, loss 0.0159429, acc 1
2016-09-07T19:18:18.072452: step 2074, loss 0.0203787, acc 1
2016-09-07T19:18:18.732867: step 2075, loss 0.0607553, acc 0.96
2016-09-07T19:18:19.401751: step 2076, loss 0.0319206, acc 0.98
2016-09-07T19:18:20.070509: step 2077, loss 0.136166, acc 0.96
2016-09-07T19:18:20.739988: step 2078, loss 0.0109423, acc 1
2016-09-07T19:18:21.406598: step 2079, loss 0.0350763, acc 1
2016-09-07T19:18:22.073375: step 2080, loss 0.0196586, acc 0.98
2016-09-07T19:18:22.746621: step 2081, loss 0.0334942, acc 0.98
2016-09-07T19:18:23.410456: step 2082, loss 0.0238287, acc 1
2016-09-07T19:18:24.081955: step 2083, loss 0.0316254, acc 1
2016-09-07T19:18:24.758808: step 2084, loss 0.0571057, acc 0.96
2016-09-07T19:18:25.404570: step 2085, loss 0.0820064, acc 0.96
2016-09-07T19:18:26.074086: step 2086, loss 0.0181419, acc 1
2016-09-07T19:18:26.741714: step 2087, loss 0.0780697, acc 0.96
2016-09-07T19:18:27.414409: step 2088, loss 0.0929584, acc 0.96
2016-09-07T19:18:28.088334: step 2089, loss 0.0443599, acc 0.98
2016-09-07T19:18:28.756567: step 2090, loss 0.00217201, acc 1
2016-09-07T19:18:29.418715: step 2091, loss 0.0553968, acc 0.96
2016-09-07T19:18:30.098179: step 2092, loss 0.0781184, acc 0.98
2016-09-07T19:18:30.744688: step 2093, loss 0.103383, acc 0.98
2016-09-07T19:18:31.440352: step 2094, loss 0.179728, acc 0.96
2016-09-07T19:18:32.096229: step 2095, loss 0.0369645, acc 1
2016-09-07T19:18:32.773766: step 2096, loss 0.0407637, acc 0.98
2016-09-07T19:18:33.447936: step 2097, loss 0.00246508, acc 1
2016-09-07T19:18:34.104596: step 2098, loss 0.0730429, acc 0.96
2016-09-07T19:18:34.774070: step 2099, loss 0.186359, acc 0.94
2016-09-07T19:18:35.460175: step 2100, loss 0.0303581, acc 0.98

Evaluation:
2016-09-07T19:18:38.499703: step 2100, loss 1.2453, acc 0.758

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-2100

2016-09-07T19:18:40.296241: step 2101, loss 0.0375899, acc 0.98
2016-09-07T19:18:40.955678: step 2102, loss 0.0829464, acc 0.96
2016-09-07T19:18:41.626823: step 2103, loss 0.00420395, acc 1
2016-09-07T19:18:42.301113: step 2104, loss 0.120395, acc 0.94
2016-09-07T19:18:42.980834: step 2105, loss 0.0357817, acc 0.96
2016-09-07T19:18:43.650057: step 2106, loss 0.137738, acc 0.94
2016-09-07T19:18:44.316920: step 2107, loss 0.0390953, acc 0.98
2016-09-07T19:18:44.986703: step 2108, loss 0.0033729, acc 1
2016-09-07T19:18:45.640189: step 2109, loss 0.103583, acc 0.92
2016-09-07T19:18:46.317316: step 2110, loss 0.0413618, acc 0.96
2016-09-07T19:18:46.968302: step 2111, loss 0.0462923, acc 0.98
2016-09-07T19:18:47.642205: step 2112, loss 0.0846283, acc 0.94
2016-09-07T19:18:48.329317: step 2113, loss 0.0512, acc 0.98
2016-09-07T19:18:49.001841: step 2114, loss 0.0684934, acc 0.96
2016-09-07T19:18:49.689152: step 2115, loss 0.110615, acc 0.98
2016-09-07T19:18:50.359158: step 2116, loss 0.0294876, acc 0.98
2016-09-07T19:18:51.030601: step 2117, loss 0.129571, acc 0.96
2016-09-07T19:18:51.696728: step 2118, loss 0.0175823, acc 1
2016-09-07T19:18:52.377646: step 2119, loss 0.0712405, acc 0.98
2016-09-07T19:18:53.044924: step 2120, loss 0.031796, acc 0.98
2016-09-07T19:18:53.733366: step 2121, loss 0.0666112, acc 0.94
2016-09-07T19:18:54.396307: step 2122, loss 0.0361639, acc 0.98
2016-09-07T19:18:55.071551: step 2123, loss 0.128128, acc 0.9
2016-09-07T19:18:55.706576: step 2124, loss 0.0213367, acc 0.98
2016-09-07T19:18:56.369327: step 2125, loss 0.0171794, acc 1
2016-09-07T19:18:57.023328: step 2126, loss 0.0374934, acc 0.98
2016-09-07T19:18:57.675277: step 2127, loss 0.106332, acc 0.92
2016-09-07T19:18:58.347432: step 2128, loss 0.041726, acc 0.98
2016-09-07T19:18:59.024860: step 2129, loss 0.0388641, acc 0.96
2016-09-07T19:18:59.709589: step 2130, loss 0.02659, acc 1
2016-09-07T19:19:00.427037: step 2131, loss 0.0613432, acc 0.98
2016-09-07T19:19:01.088836: step 2132, loss 0.0675966, acc 1
2016-09-07T19:19:01.753963: step 2133, loss 0.0204313, acc 1
2016-09-07T19:19:02.096121: step 2134, loss 0.0282201, acc 1
2016-09-07T19:19:02.793910: step 2135, loss 0.100369, acc 0.96
2016-09-07T19:19:03.476921: step 2136, loss 0.0369455, acc 0.98
2016-09-07T19:19:04.139765: step 2137, loss 0.0386486, acc 0.98
2016-09-07T19:19:04.821272: step 2138, loss 0.123572, acc 0.94
2016-09-07T19:19:05.492261: step 2139, loss 0.0477333, acc 0.98
2016-09-07T19:19:06.171578: step 2140, loss 0.0259647, acc 1
2016-09-07T19:19:06.839556: step 2141, loss 0.019545, acc 0.98
2016-09-07T19:19:07.508401: step 2142, loss 0.0280338, acc 1
2016-09-07T19:19:08.185838: step 2143, loss 0.0220981, acc 1
2016-09-07T19:19:08.838830: step 2144, loss 0.0336504, acc 0.98
2016-09-07T19:19:09.499561: step 2145, loss 0.047946, acc 0.96
2016-09-07T19:19:10.171468: step 2146, loss 0.0504637, acc 0.98
2016-09-07T19:19:10.828983: step 2147, loss 0.0352496, acc 0.96
2016-09-07T19:19:11.507006: step 2148, loss 0.0727063, acc 0.96
2016-09-07T19:19:12.182264: step 2149, loss 0.0372128, acc 0.98
2016-09-07T19:19:12.867770: step 2150, loss 0.123095, acc 0.98
2016-09-07T19:19:13.548591: step 2151, loss 0.104184, acc 0.94
2016-09-07T19:19:14.212062: step 2152, loss 0.102456, acc 0.98
2016-09-07T19:19:14.889356: step 2153, loss 0.0329111, acc 0.98
2016-09-07T19:19:15.549648: step 2154, loss 0.0498337, acc 0.96
2016-09-07T19:19:16.227905: step 2155, loss 0.0819076, acc 0.96
2016-09-07T19:19:16.898944: step 2156, loss 0.0203727, acc 1
2016-09-07T19:19:17.554104: step 2157, loss 0.125555, acc 0.96
2016-09-07T19:19:18.226838: step 2158, loss 0.112107, acc 0.96
2016-09-07T19:19:18.889587: step 2159, loss 0.0424995, acc 0.98
2016-09-07T19:19:19.561234: step 2160, loss 0.0283255, acc 0.98
2016-09-07T19:19:20.218704: step 2161, loss 0.0178107, acc 1
2016-09-07T19:19:20.880205: step 2162, loss 0.0519992, acc 0.96
2016-09-07T19:19:21.544435: step 2163, loss 0.0302536, acc 0.98
2016-09-07T19:19:22.239581: step 2164, loss 0.0246136, acc 1
2016-09-07T19:19:22.909178: step 2165, loss 0.0897237, acc 0.94
2016-09-07T19:19:23.614111: step 2166, loss 0.230116, acc 0.92
2016-09-07T19:19:24.270622: step 2167, loss 0.010455, acc 1
2016-09-07T19:19:24.932572: step 2168, loss 0.0440486, acc 0.98
2016-09-07T19:19:25.592384: step 2169, loss 0.0384702, acc 0.98
2016-09-07T19:19:26.251980: step 2170, loss 0.0131001, acc 1
2016-09-07T19:19:26.908668: step 2171, loss 0.0304781, acc 0.98
2016-09-07T19:19:27.594224: step 2172, loss 0.0731412, acc 0.96
2016-09-07T19:19:28.262745: step 2173, loss 0.0838877, acc 0.98
2016-09-07T19:19:28.942388: step 2174, loss 0.0140387, acc 1
2016-09-07T19:19:29.615985: step 2175, loss 0.0590028, acc 0.96
2016-09-07T19:19:30.295282: step 2176, loss 0.0487265, acc 0.98
2016-09-07T19:19:30.968128: step 2177, loss 0.0247724, acc 1
2016-09-07T19:19:31.636561: step 2178, loss 0.0164169, acc 1
2016-09-07T19:19:32.309558: step 2179, loss 0.0687618, acc 0.94
2016-09-07T19:19:32.971098: step 2180, loss 0.01645, acc 1
2016-09-07T19:19:33.637675: step 2181, loss 0.0467211, acc 0.98
2016-09-07T19:19:34.331046: step 2182, loss 0.0871381, acc 0.98
2016-09-07T19:19:35.008540: step 2183, loss 0.0279109, acc 0.98
2016-09-07T19:19:35.682683: step 2184, loss 0.160487, acc 0.98
2016-09-07T19:19:36.370247: step 2185, loss 0.0321033, acc 0.98
2016-09-07T19:19:37.047604: step 2186, loss 0.0278019, acc 0.98
2016-09-07T19:19:37.731697: step 2187, loss 0.0169158, acc 1
2016-09-07T19:19:38.395538: step 2188, loss 0.0844563, acc 0.94
2016-09-07T19:19:39.078191: step 2189, loss 0.0375481, acc 0.98
2016-09-07T19:19:39.759715: step 2190, loss 0.0615856, acc 0.98
2016-09-07T19:19:40.421460: step 2191, loss 0.0309218, acc 1
2016-09-07T19:19:41.103367: step 2192, loss 0.0365472, acc 0.98
2016-09-07T19:19:41.768734: step 2193, loss 0.0200167, acc 1
2016-09-07T19:19:42.422870: step 2194, loss 0.0640545, acc 0.96
2016-09-07T19:19:43.101581: step 2195, loss 0.117741, acc 0.96
2016-09-07T19:19:43.781356: step 2196, loss 0.0272857, acc 0.98
2016-09-07T19:19:44.449866: step 2197, loss 0.149267, acc 0.94
2016-09-07T19:19:45.123451: step 2198, loss 0.105164, acc 0.96
2016-09-07T19:19:45.795346: step 2199, loss 0.00689643, acc 1
2016-09-07T19:19:46.449361: step 2200, loss 0.112206, acc 0.96

Evaluation:
2016-09-07T19:19:49.524518: step 2200, loss 1.45647, acc 0.74

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-2200

2016-09-07T19:19:51.224987: step 2201, loss 0.0107616, acc 1
2016-09-07T19:19:51.913979: step 2202, loss 0.063515, acc 0.98
2016-09-07T19:19:52.596416: step 2203, loss 0.0138321, acc 1
2016-09-07T19:19:53.281245: step 2204, loss 0.0649925, acc 0.98
2016-09-07T19:19:53.940706: step 2205, loss 0.0245774, acc 0.98
2016-09-07T19:19:54.621006: step 2206, loss 0.000812284, acc 1
2016-09-07T19:19:55.297493: step 2207, loss 0.124185, acc 0.98
2016-09-07T19:19:55.961023: step 2208, loss 0.0241579, acc 1
2016-09-07T19:19:56.622721: step 2209, loss 0.145869, acc 0.96
2016-09-07T19:19:57.280997: step 2210, loss 0.0555578, acc 0.98
2016-09-07T19:19:57.948277: step 2211, loss 0.0616188, acc 0.96
2016-09-07T19:19:58.612248: step 2212, loss 0.0166651, acc 1
2016-09-07T19:19:59.288772: step 2213, loss 0.0376877, acc 1
2016-09-07T19:19:59.962425: step 2214, loss 0.0154399, acc 1
2016-09-07T19:20:00.661489: step 2215, loss 0.0684027, acc 0.96
2016-09-07T19:20:01.343402: step 2216, loss 0.0502246, acc 0.96
2016-09-07T19:20:02.021587: step 2217, loss 0.0535738, acc 0.98
2016-09-07T19:20:02.686825: step 2218, loss 0.048028, acc 0.98
2016-09-07T19:20:03.380666: step 2219, loss 0.0152042, acc 1
2016-09-07T19:20:04.035544: step 2220, loss 0.0579954, acc 0.96
2016-09-07T19:20:04.703296: step 2221, loss 0.0340416, acc 0.98
2016-09-07T19:20:05.365391: step 2222, loss 0.0217188, acc 0.98
2016-09-07T19:20:06.034055: step 2223, loss 0.1269, acc 0.92
2016-09-07T19:20:06.673267: step 2224, loss 0.0785708, acc 0.94
2016-09-07T19:20:07.345496: step 2225, loss 0.0229532, acc 0.98
2016-09-07T19:20:08.004103: step 2226, loss 0.0346216, acc 0.98
2016-09-07T19:20:08.674457: step 2227, loss 0.048528, acc 0.98
2016-09-07T19:20:09.334236: step 2228, loss 0.0362307, acc 1
2016-09-07T19:20:10.038661: step 2229, loss 0.0602592, acc 0.96
2016-09-07T19:20:10.681451: step 2230, loss 0.0668558, acc 0.94
2016-09-07T19:20:11.350400: step 2231, loss 0.0451547, acc 0.98
2016-09-07T19:20:12.016202: step 2232, loss 0.0501209, acc 0.96
2016-09-07T19:20:12.680479: step 2233, loss 0.0413562, acc 0.98
2016-09-07T19:20:13.366296: step 2234, loss 0.0198092, acc 1
2016-09-07T19:20:14.061431: step 2235, loss 0.0299206, acc 0.98
2016-09-07T19:20:14.726198: step 2236, loss 0.030324, acc 1
2016-09-07T19:20:15.378019: step 2237, loss 0.08816, acc 0.92
2016-09-07T19:20:16.036827: step 2238, loss 0.0621673, acc 0.96
2016-09-07T19:20:16.700785: step 2239, loss 0.0596922, acc 0.98
2016-09-07T19:20:17.380118: step 2240, loss 0.0428492, acc 0.98
2016-09-07T19:20:18.047332: step 2241, loss 0.0263281, acc 0.98
2016-09-07T19:20:18.710335: step 2242, loss 0.076718, acc 0.94
2016-09-07T19:20:19.384261: step 2243, loss 0.0386637, acc 0.98
2016-09-07T19:20:20.063262: step 2244, loss 0.366669, acc 0.94
2016-09-07T19:20:20.760441: step 2245, loss 0.0611108, acc 0.94
2016-09-07T19:20:21.438214: step 2246, loss 0.00977431, acc 1
2016-09-07T19:20:22.126028: step 2247, loss 0.231975, acc 0.94
2016-09-07T19:20:22.801517: step 2248, loss 0.0213051, acc 1
2016-09-07T19:20:23.478883: step 2249, loss 0.0683571, acc 0.96
2016-09-07T19:20:24.152508: step 2250, loss 0.054878, acc 0.96
2016-09-07T19:20:24.838653: step 2251, loss 0.0116227, acc 1
2016-09-07T19:20:25.502507: step 2252, loss 0.0805006, acc 0.96
2016-09-07T19:20:26.165939: step 2253, loss 0.0601608, acc 0.96
2016-09-07T19:20:26.843286: step 2254, loss 0.117574, acc 0.96
2016-09-07T19:20:27.518700: step 2255, loss 0.0869681, acc 0.94
2016-09-07T19:20:28.178267: step 2256, loss 0.036938, acc 0.98
2016-09-07T19:20:28.855390: step 2257, loss 0.0081165, acc 1
2016-09-07T19:20:29.522323: step 2258, loss 0.00921143, acc 1
2016-09-07T19:20:30.200489: step 2259, loss 0.068744, acc 0.96
2016-09-07T19:20:30.888479: step 2260, loss 0.0594624, acc 0.98
2016-09-07T19:20:31.557706: step 2261, loss 0.0229483, acc 1
2016-09-07T19:20:32.216467: step 2262, loss 0.0140414, acc 1
2016-09-07T19:20:32.872783: step 2263, loss 0.00996652, acc 1
2016-09-07T19:20:33.554519: step 2264, loss 0.143738, acc 0.96
2016-09-07T19:20:34.237676: step 2265, loss 0.0122955, acc 1
2016-09-07T19:20:34.925786: step 2266, loss 0.0345077, acc 0.98
2016-09-07T19:20:35.600740: step 2267, loss 0.131167, acc 0.94
2016-09-07T19:20:36.299477: step 2268, loss 0.0859977, acc 0.94
2016-09-07T19:20:36.963114: step 2269, loss 0.0153234, acc 1
2016-09-07T19:20:37.626611: step 2270, loss 0.0249597, acc 1
2016-09-07T19:20:38.278610: step 2271, loss 0.0839978, acc 0.98
2016-09-07T19:20:38.940937: step 2272, loss 0.0281985, acc 1
2016-09-07T19:20:39.605220: step 2273, loss 0.021122, acc 1
2016-09-07T19:20:40.261183: step 2274, loss 0.0308489, acc 0.98
2016-09-07T19:20:40.918750: step 2275, loss 0.100195, acc 0.92
2016-09-07T19:20:41.601942: step 2276, loss 0.0550587, acc 0.98
2016-09-07T19:20:42.270215: step 2277, loss 0.0238834, acc 0.98
2016-09-07T19:20:42.955460: step 2278, loss 0.0303008, acc 1
2016-09-07T19:20:43.633377: step 2279, loss 0.0614219, acc 0.96
2016-09-07T19:20:44.326347: step 2280, loss 0.0271134, acc 1
2016-09-07T19:20:45.036070: step 2281, loss 0.0867562, acc 0.98
2016-09-07T19:20:45.706596: step 2282, loss 0.0850137, acc 0.94
2016-09-07T19:20:46.362291: step 2283, loss 0.147744, acc 0.94
2016-09-07T19:20:47.052166: step 2284, loss 0.0178344, acc 1
2016-09-07T19:20:47.732369: step 2285, loss 0.0752168, acc 0.96
2016-09-07T19:20:48.399114: step 2286, loss 0.0654987, acc 0.98
2016-09-07T19:20:49.071380: step 2287, loss 0.0836662, acc 0.94
2016-09-07T19:20:49.740073: step 2288, loss 0.0607901, acc 0.98
2016-09-07T19:20:50.397921: step 2289, loss 0.0469706, acc 0.98
2016-09-07T19:20:51.068416: step 2290, loss 0.0729059, acc 0.96
2016-09-07T19:20:51.743289: step 2291, loss 0.029233, acc 0.98
2016-09-07T19:20:52.396043: step 2292, loss 0.032226, acc 0.96
2016-09-07T19:20:53.082072: step 2293, loss 0.0990415, acc 0.98
2016-09-07T19:20:53.765680: step 2294, loss 0.0533332, acc 0.96
2016-09-07T19:20:54.432647: step 2295, loss 0.0422167, acc 0.98
2016-09-07T19:20:55.090831: step 2296, loss 0.0883064, acc 0.94
2016-09-07T19:20:55.766143: step 2297, loss 0.0971229, acc 0.98
2016-09-07T19:20:56.433198: step 2298, loss 0.0829973, acc 0.96
2016-09-07T19:20:57.114690: step 2299, loss 0.113548, acc 0.98
2016-09-07T19:20:57.786124: step 2300, loss 0.137313, acc 0.92

Evaluation:
2016-09-07T19:21:00.900170: step 2300, loss 1.34697, acc 0.754

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-2300

2016-09-07T19:21:02.500040: step 2301, loss 0.0288867, acc 0.98
2016-09-07T19:21:03.162518: step 2302, loss 0.0352562, acc 0.98
2016-09-07T19:21:03.822414: step 2303, loss 0.143069, acc 0.98
2016-09-07T19:21:04.480323: step 2304, loss 0.186414, acc 0.96
2016-09-07T19:21:05.161558: step 2305, loss 0.0333576, acc 1
2016-09-07T19:21:05.819993: step 2306, loss 0.0194801, acc 1
2016-09-07T19:21:06.483770: step 2307, loss 0.0347295, acc 1
2016-09-07T19:21:07.175336: step 2308, loss 0.18848, acc 0.96
2016-09-07T19:21:07.871500: step 2309, loss 0.0812311, acc 0.96
2016-09-07T19:21:08.541982: step 2310, loss 0.0783338, acc 0.94
2016-09-07T19:21:09.201139: step 2311, loss 0.0826081, acc 0.96
2016-09-07T19:21:09.877384: step 2312, loss 0.0895789, acc 0.94
2016-09-07T19:21:10.547186: step 2313, loss 0.0483077, acc 0.96
2016-09-07T19:21:11.246976: step 2314, loss 0.0701773, acc 0.98
2016-09-07T19:21:11.912516: step 2315, loss 0.0698624, acc 0.98
2016-09-07T19:21:12.596407: step 2316, loss 0.0286683, acc 1
2016-09-07T19:21:13.269264: step 2317, loss 0.0292179, acc 1
2016-09-07T19:21:13.939478: step 2318, loss 0.0613286, acc 0.98
2016-09-07T19:21:14.610259: step 2319, loss 0.0746195, acc 0.98
2016-09-07T19:21:15.272233: step 2320, loss 0.156617, acc 0.98
2016-09-07T19:21:15.936500: step 2321, loss 0.0532715, acc 0.98
2016-09-07T19:21:16.604110: step 2322, loss 0.0262754, acc 1
2016-09-07T19:21:17.301642: step 2323, loss 0.03868, acc 1
2016-09-07T19:21:17.975140: step 2324, loss 0.0792132, acc 0.98
2016-09-07T19:21:18.649044: step 2325, loss 0.0890669, acc 0.94
2016-09-07T19:21:19.316954: step 2326, loss 0.0719936, acc 0.98
2016-09-07T19:21:19.995107: step 2327, loss 0.0384843, acc 0.98
2016-09-07T19:21:20.355461: step 2328, loss 0.0782498, acc 1
2016-09-07T19:21:21.023153: step 2329, loss 0.0530602, acc 0.98
2016-09-07T19:21:21.695725: step 2330, loss 0.0372911, acc 1
2016-09-07T19:21:22.371645: step 2331, loss 0.0708824, acc 0.98
2016-09-07T19:21:23.048790: step 2332, loss 0.0228364, acc 1
2016-09-07T19:21:23.712271: step 2333, loss 0.0486334, acc 0.98
2016-09-07T19:21:24.367998: step 2334, loss 0.0834989, acc 0.96
2016-09-07T19:21:25.029003: step 2335, loss 0.0461336, acc 0.98
2016-09-07T19:21:25.701800: step 2336, loss 0.0452086, acc 0.98
2016-09-07T19:21:26.383516: step 2337, loss 0.0661514, acc 0.96
2016-09-07T19:21:27.037780: step 2338, loss 0.0386901, acc 0.98
2016-09-07T19:21:27.713002: step 2339, loss 0.0555626, acc 0.96
2016-09-07T19:21:28.385619: step 2340, loss 0.0887038, acc 0.96
2016-09-07T19:21:29.062909: step 2341, loss 0.0806606, acc 0.96
2016-09-07T19:21:29.769477: step 2342, loss 0.0272773, acc 0.98
2016-09-07T19:21:30.441448: step 2343, loss 0.0439883, acc 0.98
2016-09-07T19:21:31.122731: step 2344, loss 0.00707398, acc 1
2016-09-07T19:21:31.790553: step 2345, loss 0.112346, acc 0.96
2016-09-07T19:21:32.472938: step 2346, loss 0.194477, acc 0.96
2016-09-07T19:21:33.134569: step 2347, loss 0.0168016, acc 1
2016-09-07T19:21:33.793231: step 2348, loss 0.0114613, acc 1
2016-09-07T19:21:34.465467: step 2349, loss 0.0166259, acc 0.98
2016-09-07T19:21:35.133888: step 2350, loss 0.120086, acc 0.96
2016-09-07T19:21:35.795166: step 2351, loss 0.0179747, acc 1
2016-09-07T19:21:36.462894: step 2352, loss 0.0318005, acc 1
2016-09-07T19:21:37.136024: step 2353, loss 0.0306048, acc 0.98
2016-09-07T19:21:37.797895: step 2354, loss 0.0198939, acc 1
2016-09-07T19:21:38.479848: step 2355, loss 0.0829919, acc 0.98
2016-09-07T19:21:39.182021: step 2356, loss 0.00534818, acc 1
2016-09-07T19:21:39.842086: step 2357, loss 0.0892899, acc 0.96
2016-09-07T19:21:40.516194: step 2358, loss 0.0830661, acc 0.96
2016-09-07T19:21:41.171458: step 2359, loss 0.0587262, acc 0.96
2016-09-07T19:21:41.856707: step 2360, loss 0.115378, acc 0.94
2016-09-07T19:21:42.525318: step 2361, loss 0.0689846, acc 0.98
2016-09-07T19:21:43.190307: step 2362, loss 0.179302, acc 0.94
2016-09-07T19:21:43.876743: step 2363, loss 0.0259095, acc 0.98
2016-09-07T19:21:44.552152: step 2364, loss 0.0620431, acc 0.96
2016-09-07T19:21:45.221219: step 2365, loss 0.0397334, acc 0.98
2016-09-07T19:21:45.881271: step 2366, loss 0.103801, acc 0.92
2016-09-07T19:21:46.547387: step 2367, loss 0.0513607, acc 0.98
2016-09-07T19:21:47.216400: step 2368, loss 0.116605, acc 0.98
2016-09-07T19:21:47.913460: step 2369, loss 0.105204, acc 0.96
2016-09-07T19:21:48.573772: step 2370, loss 0.11781, acc 0.96
2016-09-07T19:21:49.244692: step 2371, loss 0.0326163, acc 0.98
2016-09-07T19:21:49.907057: step 2372, loss 0.102345, acc 0.96
2016-09-07T19:21:50.573684: step 2373, loss 0.0849536, acc 0.92
2016-09-07T19:21:51.254983: step 2374, loss 0.0202268, acc 1
2016-09-07T19:21:51.934020: step 2375, loss 0.0786972, acc 0.96
2016-09-07T19:21:52.600515: step 2376, loss 0.0535655, acc 0.98
2016-09-07T19:21:53.267513: step 2377, loss 0.0267806, acc 1
2016-09-07T19:21:53.951886: step 2378, loss 0.0144671, acc 1
2016-09-07T19:21:54.651030: step 2379, loss 0.0273782, acc 0.98
2016-09-07T19:21:55.332784: step 2380, loss 0.062143, acc 0.96
2016-09-07T19:21:56.001294: step 2381, loss 0.0336801, acc 0.98
2016-09-07T19:21:56.664805: step 2382, loss 0.0054894, acc 1
2016-09-07T19:21:57.328050: step 2383, loss 0.0582135, acc 0.98
2016-09-07T19:21:58.008878: step 2384, loss 0.0846491, acc 0.98
2016-09-07T19:21:58.700345: step 2385, loss 0.0266817, acc 1
2016-09-07T19:21:59.383759: step 2386, loss 0.0834563, acc 0.96
2016-09-07T19:22:00.061120: step 2387, loss 0.0641057, acc 0.98
2016-09-07T19:22:00.776329: step 2388, loss 0.0606757, acc 0.98
2016-09-07T19:22:01.458891: step 2389, loss 0.0297984, acc 0.98
2016-09-07T19:22:02.137981: step 2390, loss 0.0690011, acc 0.94
2016-09-07T19:22:02.814445: step 2391, loss 0.0439471, acc 0.98
2016-09-07T19:22:03.492180: step 2392, loss 0.0394117, acc 1
2016-09-07T19:22:04.157054: step 2393, loss 0.111987, acc 0.92
2016-09-07T19:22:04.839947: step 2394, loss 0.0216609, acc 1
2016-09-07T19:22:05.511308: step 2395, loss 0.0660098, acc 0.98
2016-09-07T19:22:06.187806: step 2396, loss 0.0148779, acc 1
2016-09-07T19:22:06.851200: step 2397, loss 0.0312604, acc 0.98
2016-09-07T19:22:07.498422: step 2398, loss 0.110245, acc 0.92
2016-09-07T19:22:08.163313: step 2399, loss 0.110198, acc 0.98
2016-09-07T19:22:08.849942: step 2400, loss 0.0294408, acc 0.98

Evaluation:
2016-09-07T19:22:11.927666: step 2400, loss 1.22428, acc 0.75

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-2400

2016-09-07T19:22:13.731510: step 2401, loss 0.0436067, acc 0.98
2016-09-07T19:22:14.412349: step 2402, loss 0.0273929, acc 0.98
2016-09-07T19:22:15.084715: step 2403, loss 0.0370645, acc 0.98
2016-09-07T19:22:15.762183: step 2404, loss 0.128179, acc 0.92
2016-09-07T19:22:16.435189: step 2405, loss 0.0829381, acc 0.98
2016-09-07T19:22:17.110222: step 2406, loss 0.049238, acc 0.98
2016-09-07T19:22:17.777043: step 2407, loss 0.0676449, acc 0.94
2016-09-07T19:22:18.421802: step 2408, loss 0.0470048, acc 0.98
2016-09-07T19:22:19.082794: step 2409, loss 0.0535654, acc 0.98
2016-09-07T19:22:19.737384: step 2410, loss 0.00363594, acc 1
2016-09-07T19:22:20.410060: step 2411, loss 0.118081, acc 0.98
2016-09-07T19:22:21.079202: step 2412, loss 0.0374423, acc 0.98
2016-09-07T19:22:21.759292: step 2413, loss 0.0265324, acc 0.98
2016-09-07T19:22:22.427329: step 2414, loss 0.0324722, acc 1
2016-09-07T19:22:23.100435: step 2415, loss 0.0165061, acc 1
2016-09-07T19:22:23.768638: step 2416, loss 0.00573222, acc 1
2016-09-07T19:22:24.445330: step 2417, loss 0.0765639, acc 0.98
2016-09-07T19:22:25.160503: step 2418, loss 0.0464393, acc 0.98
2016-09-07T19:22:25.837815: step 2419, loss 0.0879923, acc 0.96
2016-09-07T19:22:26.531016: step 2420, loss 0.0233899, acc 0.98
2016-09-07T19:22:27.211800: step 2421, loss 0.0189173, acc 1
2016-09-07T19:22:27.878794: step 2422, loss 0.0193929, acc 1
2016-09-07T19:22:28.541901: step 2423, loss 0.0434195, acc 0.98
2016-09-07T19:22:29.202079: step 2424, loss 0.182019, acc 0.94
2016-09-07T19:22:29.859708: step 2425, loss 0.0201158, acc 0.98
2016-09-07T19:22:30.532384: step 2426, loss 0.0779837, acc 0.94
2016-09-07T19:22:31.217014: step 2427, loss 0.0345558, acc 0.98
2016-09-07T19:22:31.884595: step 2428, loss 0.0312075, acc 1
2016-09-07T19:22:32.546288: step 2429, loss 0.0172321, acc 1
2016-09-07T19:22:33.213141: step 2430, loss 0.0602203, acc 0.96
2016-09-07T19:22:33.880562: step 2431, loss 0.0358147, acc 0.98
2016-09-07T19:22:34.537421: step 2432, loss 0.0551901, acc 0.98
2016-09-07T19:22:35.239732: step 2433, loss 0.0782775, acc 0.94
2016-09-07T19:22:35.904081: step 2434, loss 0.0755589, acc 0.94
2016-09-07T19:22:36.582301: step 2435, loss 0.0944439, acc 0.94
2016-09-07T19:22:37.250342: step 2436, loss 0.0259715, acc 1
2016-09-07T19:22:37.925122: step 2437, loss 0.151999, acc 0.98
2016-09-07T19:22:38.610466: step 2438, loss 0.0224969, acc 1
2016-09-07T19:22:39.275983: step 2439, loss 0.059686, acc 0.96
2016-09-07T19:22:39.946344: step 2440, loss 0.00922448, acc 1
2016-09-07T19:22:40.591349: step 2441, loss 0.0812794, acc 0.98
2016-09-07T19:22:41.267372: step 2442, loss 0.0678272, acc 0.94
2016-09-07T19:22:41.938429: step 2443, loss 0.0892432, acc 0.96
2016-09-07T19:22:42.622883: step 2444, loss 0.0528708, acc 1
2016-09-07T19:22:43.279042: step 2445, loss 0.0674146, acc 0.96
2016-09-07T19:22:43.942476: step 2446, loss 0.0318435, acc 0.98
2016-09-07T19:22:44.612201: step 2447, loss 0.160233, acc 0.94
2016-09-07T19:22:45.297502: step 2448, loss 0.112612, acc 0.98
2016-09-07T19:22:45.965979: step 2449, loss 0.0865016, acc 0.96
2016-09-07T19:22:46.644646: step 2450, loss 0.0934123, acc 0.96
2016-09-07T19:22:47.313939: step 2451, loss 0.00835776, acc 1
2016-09-07T19:22:47.999027: step 2452, loss 0.0413935, acc 0.98
2016-09-07T19:22:48.666352: step 2453, loss 0.0235854, acc 1
2016-09-07T19:22:49.353235: step 2454, loss 0.0193374, acc 1
2016-09-07T19:22:50.022113: step 2455, loss 0.033075, acc 0.98
2016-09-07T19:22:50.716686: step 2456, loss 0.0107049, acc 1
2016-09-07T19:22:51.405779: step 2457, loss 0.153366, acc 0.94
2016-09-07T19:22:52.080082: step 2458, loss 0.0387787, acc 0.96
2016-09-07T19:22:52.762039: step 2459, loss 0.0108788, acc 1
2016-09-07T19:22:53.425418: step 2460, loss 0.148549, acc 0.98
2016-09-07T19:22:54.075990: step 2461, loss 0.0216138, acc 1
2016-09-07T19:22:54.733106: step 2462, loss 0.150763, acc 0.92
2016-09-07T19:22:55.389484: step 2463, loss 0.0577827, acc 0.98
2016-09-07T19:22:56.095109: step 2464, loss 0.0224892, acc 0.98
2016-09-07T19:22:56.771754: step 2465, loss 0.0272858, acc 0.98
2016-09-07T19:22:57.440491: step 2466, loss 0.0270617, acc 0.98
2016-09-07T19:22:58.108430: step 2467, loss 0.0144483, acc 1
2016-09-07T19:22:58.784890: step 2468, loss 0.0751004, acc 0.94
2016-09-07T19:22:59.461788: step 2469, loss 0.070847, acc 0.98
2016-09-07T19:23:00.125197: step 2470, loss 0.0756377, acc 0.96
2016-09-07T19:23:00.830012: step 2471, loss 0.0330197, acc 1
2016-09-07T19:23:01.503422: step 2472, loss 0.0504037, acc 1
2016-09-07T19:23:02.216906: step 2473, loss 0.0708428, acc 0.98
2016-09-07T19:23:02.888137: step 2474, loss 0.0847933, acc 0.92
2016-09-07T19:23:03.548183: step 2475, loss 0.0515196, acc 0.96
2016-09-07T19:23:04.220247: step 2476, loss 0.0290497, acc 1
2016-09-07T19:23:04.911038: step 2477, loss 0.0371343, acc 0.98
2016-09-07T19:23:05.569675: step 2478, loss 0.0833483, acc 0.96
2016-09-07T19:23:06.221329: step 2479, loss 0.0509708, acc 0.96
2016-09-07T19:23:06.883102: step 2480, loss 0.152392, acc 0.94
2016-09-07T19:23:07.537334: step 2481, loss 0.0525995, acc 0.96
2016-09-07T19:23:08.206137: step 2482, loss 0.0343407, acc 1
2016-09-07T19:23:08.895068: step 2483, loss 0.0935118, acc 0.98
2016-09-07T19:23:09.576198: step 2484, loss 0.0131315, acc 1
2016-09-07T19:23:10.243263: step 2485, loss 0.0105912, acc 1
2016-09-07T19:23:10.902781: step 2486, loss 0.0531531, acc 0.98
2016-09-07T19:23:11.589612: step 2487, loss 0.0178456, acc 1
2016-09-07T19:23:12.254765: step 2488, loss 0.0785136, acc 0.98
2016-09-07T19:23:12.931516: step 2489, loss 0.101486, acc 0.98
2016-09-07T19:23:13.598221: step 2490, loss 0.0312775, acc 1
2016-09-07T19:23:14.260906: step 2491, loss 0.0117921, acc 1
2016-09-07T19:23:14.928809: step 2492, loss 0.042252, acc 1
2016-09-07T19:23:15.601447: step 2493, loss 0.0419965, acc 0.98
2016-09-07T19:23:16.285515: step 2494, loss 0.0352997, acc 0.98
2016-09-07T19:23:16.945152: step 2495, loss 0.0222549, acc 1
2016-09-07T19:23:17.609239: step 2496, loss 0.0201267, acc 0.98
2016-09-07T19:23:18.286966: step 2497, loss 0.0467431, acc 0.96
2016-09-07T19:23:18.955085: step 2498, loss 0.0831584, acc 0.94
2016-09-07T19:23:19.614717: step 2499, loss 0.0202426, acc 0.98
2016-09-07T19:23:20.273945: step 2500, loss 0.0662193, acc 0.96

Evaluation:
2016-09-07T19:23:23.401209: step 2500, loss 1.47007, acc 0.759

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-2500

2016-09-07T19:23:25.108053: step 2501, loss 0.0488164, acc 0.98
2016-09-07T19:23:25.797266: step 2502, loss 0.11514, acc 0.96
2016-09-07T19:23:26.469578: step 2503, loss 0.109354, acc 0.96
2016-09-07T19:23:27.164852: step 2504, loss 0.0774188, acc 0.96
2016-09-07T19:23:27.837191: step 2505, loss 0.0442172, acc 0.98
2016-09-07T19:23:28.502998: step 2506, loss 0.0311017, acc 0.98
2016-09-07T19:23:29.171928: step 2507, loss 0.0588359, acc 0.96
2016-09-07T19:23:29.838753: step 2508, loss 0.0740679, acc 0.96
2016-09-07T19:23:30.507207: step 2509, loss 0.152216, acc 0.96
2016-09-07T19:23:31.200880: step 2510, loss 0.0427389, acc 0.96
2016-09-07T19:23:31.865961: step 2511, loss 0.0180817, acc 1
2016-09-07T19:23:32.539944: step 2512, loss 0.0114103, acc 1
2016-09-07T19:23:33.207598: step 2513, loss 0.0717703, acc 0.96
2016-09-07T19:23:33.867676: step 2514, loss 0.0386636, acc 1
2016-09-07T19:23:34.533972: step 2515, loss 0.149022, acc 0.94
2016-09-07T19:23:35.209191: step 2516, loss 0.062226, acc 0.96
2016-09-07T19:23:35.874709: step 2517, loss 0.0467551, acc 0.98
2016-09-07T19:23:36.533340: step 2518, loss 0.0290428, acc 0.96
2016-09-07T19:23:37.189544: step 2519, loss 0.055568, acc 0.98
2016-09-07T19:23:37.854324: step 2520, loss 0.0940624, acc 0.94
2016-09-07T19:23:38.513648: step 2521, loss 0.0448448, acc 0.98
2016-09-07T19:23:38.862015: step 2522, loss 0.00521082, acc 1
2016-09-07T19:23:39.548392: step 2523, loss 0.0124371, acc 1
2016-09-07T19:23:40.203599: step 2524, loss 0.0280651, acc 1
2016-09-07T19:23:40.872814: step 2525, loss 0.0394344, acc 0.98
2016-09-07T19:23:41.539794: step 2526, loss 0.0366928, acc 0.98
2016-09-07T19:23:42.216130: step 2527, loss 0.0126939, acc 1
2016-09-07T19:23:42.889036: step 2528, loss 0.137542, acc 0.9
2016-09-07T19:23:43.567241: step 2529, loss 0.0449265, acc 0.98
2016-09-07T19:23:44.238380: step 2530, loss 0.0194044, acc 1
2016-09-07T19:23:44.903020: step 2531, loss 0.0436649, acc 0.96
2016-09-07T19:23:45.587877: step 2532, loss 0.0286406, acc 1
2016-09-07T19:23:46.259825: step 2533, loss 0.0272277, acc 0.98
2016-09-07T19:23:46.945619: step 2534, loss 0.0122101, acc 1
2016-09-07T19:23:47.626436: step 2535, loss 0.139936, acc 0.96
2016-09-07T19:23:48.324215: step 2536, loss 0.166477, acc 0.92
2016-09-07T19:23:48.999640: step 2537, loss 0.0162124, acc 1
2016-09-07T19:23:49.671182: step 2538, loss 0.00180195, acc 1
2016-09-07T19:23:50.338040: step 2539, loss 0.0462885, acc 1
2016-09-07T19:23:51.011345: step 2540, loss 0.00102515, acc 1
2016-09-07T19:23:51.686787: step 2541, loss 0.0144349, acc 1
2016-09-07T19:23:52.363719: step 2542, loss 0.0323091, acc 1
2016-09-07T19:23:53.035001: step 2543, loss 0.0384146, acc 1
2016-09-07T19:23:53.714708: step 2544, loss 0.0644483, acc 0.96
2016-09-07T19:23:54.371608: step 2545, loss 0.0320922, acc 0.98
2016-09-07T19:23:55.078117: step 2546, loss 0.0243371, acc 0.98
2016-09-07T19:23:55.748747: step 2547, loss 0.0729426, acc 0.96
2016-09-07T19:23:56.402583: step 2548, loss 0.00478455, acc 1
2016-09-07T19:23:57.076808: step 2549, loss 0.0163111, acc 0.98
2016-09-07T19:23:57.719745: step 2550, loss 0.0512631, acc 0.96
2016-09-07T19:23:58.386298: step 2551, loss 0.127537, acc 0.96
2016-09-07T19:23:59.051243: step 2552, loss 0.0201278, acc 1
2016-09-07T19:23:59.699522: step 2553, loss 0.0426866, acc 0.98
2016-09-07T19:24:00.405804: step 2554, loss 0.0261489, acc 0.98
2016-09-07T19:24:01.060890: step 2555, loss 0.0417098, acc 0.98
2016-09-07T19:24:01.711033: step 2556, loss 0.00345247, acc 1
2016-09-07T19:24:02.374371: step 2557, loss 0.0391732, acc 0.98
2016-09-07T19:24:03.034651: step 2558, loss 0.225748, acc 0.94
2016-09-07T19:24:03.681834: step 2559, loss 0.0827086, acc 0.94
2016-09-07T19:24:04.347806: step 2560, loss 0.0633852, acc 0.96
2016-09-07T19:24:05.033658: step 2561, loss 0.0187301, acc 1
2016-09-07T19:24:05.703734: step 2562, loss 0.0404732, acc 0.96
2016-09-07T19:24:06.381338: step 2563, loss 0.209977, acc 0.96
2016-09-07T19:24:07.061752: step 2564, loss 0.0399812, acc 1
2016-09-07T19:24:07.733884: step 2565, loss 0.0379694, acc 0.98
2016-09-07T19:24:08.414390: step 2566, loss 0.0256246, acc 0.98
2016-09-07T19:24:09.086130: step 2567, loss 0.0247319, acc 1
2016-09-07T19:24:09.766238: step 2568, loss 0.134097, acc 0.94
2016-09-07T19:24:10.427110: step 2569, loss 0.094875, acc 0.98
2016-09-07T19:24:11.099913: step 2570, loss 0.0139925, acc 1
2016-09-07T19:24:11.777819: step 2571, loss 0.0216566, acc 1
2016-09-07T19:24:12.442717: step 2572, loss 0.0492726, acc 0.96
2016-09-07T19:24:13.102573: step 2573, loss 0.0281811, acc 1
2016-09-07T19:24:13.771737: step 2574, loss 0.0227375, acc 0.98
2016-09-07T19:24:14.459762: step 2575, loss 0.0212409, acc 0.98
2016-09-07T19:24:15.119097: step 2576, loss 0.0058859, acc 1
2016-09-07T19:24:15.796573: step 2577, loss 0.0204761, acc 0.98
2016-09-07T19:24:16.472221: step 2578, loss 0.00688345, acc 1
2016-09-07T19:24:17.138851: step 2579, loss 0.0480013, acc 0.98
2016-09-07T19:24:17.798585: step 2580, loss 0.0310379, acc 0.98
2016-09-07T19:24:18.462775: step 2581, loss 0.0179626, acc 1
2016-09-07T19:24:19.107947: step 2582, loss 0.0210122, acc 1
2016-09-07T19:24:19.771349: step 2583, loss 0.0399227, acc 0.98
2016-09-07T19:24:20.410754: step 2584, loss 0.0413766, acc 0.98
2016-09-07T19:24:21.086331: step 2585, loss 0.0619251, acc 1
2016-09-07T19:24:21.763350: step 2586, loss 0.0267853, acc 1
2016-09-07T19:24:22.434224: step 2587, loss 0.0412356, acc 0.98
2016-09-07T19:24:23.110365: step 2588, loss 0.0298036, acc 1
2016-09-07T19:24:23.778611: step 2589, loss 0.0547052, acc 0.96
2016-09-07T19:24:24.443686: step 2590, loss 0.00880443, acc 1
2016-09-07T19:24:25.134303: step 2591, loss 0.0701814, acc 0.96
2016-09-07T19:24:25.802505: step 2592, loss 0.0597175, acc 0.96
2016-09-07T19:24:26.478065: step 2593, loss 0.0676065, acc 0.96
2016-09-07T19:24:27.153973: step 2594, loss 0.0282658, acc 0.98
2016-09-07T19:24:27.815297: step 2595, loss 0.0085809, acc 1
2016-09-07T19:24:28.477226: step 2596, loss 0.0246055, acc 1
2016-09-07T19:24:29.148452: step 2597, loss 0.0157851, acc 1
2016-09-07T19:24:29.829112: step 2598, loss 0.0331564, acc 0.96
2016-09-07T19:24:30.522998: step 2599, loss 0.0716407, acc 0.98
2016-09-07T19:24:31.178855: step 2600, loss 0.0051279, acc 1

Evaluation:
2016-09-07T19:24:34.314565: step 2600, loss 1.37997, acc 0.76

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-2600

2016-09-07T19:24:36.058055: step 2601, loss 0.00464237, acc 1
2016-09-07T19:24:36.719655: step 2602, loss 0.0693905, acc 0.94
2016-09-07T19:24:37.387578: step 2603, loss 0.0699021, acc 0.96
2016-09-07T19:24:38.057547: step 2604, loss 0.0330446, acc 0.98
2016-09-07T19:24:38.729086: step 2605, loss 0.00183932, acc 1
2016-09-07T19:24:39.405422: step 2606, loss 0.0101295, acc 1
2016-09-07T19:24:40.076245: step 2607, loss 0.134688, acc 0.94
2016-09-07T19:24:40.779065: step 2608, loss 0.0196377, acc 1
2016-09-07T19:24:41.443683: step 2609, loss 0.0377766, acc 0.98
2016-09-07T19:24:42.110537: step 2610, loss 0.0476904, acc 0.98
2016-09-07T19:24:42.768594: step 2611, loss 0.024051, acc 1
2016-09-07T19:24:43.426101: step 2612, loss 0.108263, acc 0.98
2016-09-07T19:24:44.105233: step 2613, loss 0.021653, acc 0.98
2016-09-07T19:24:44.755864: step 2614, loss 0.0179133, acc 1
2016-09-07T19:24:45.411493: step 2615, loss 0.0288169, acc 1
2016-09-07T19:24:46.074297: step 2616, loss 0.0481791, acc 0.98
2016-09-07T19:24:46.733390: step 2617, loss 0.230811, acc 0.94
2016-09-07T19:24:47.404706: step 2618, loss 0.0193884, acc 1
2016-09-07T19:24:48.067958: step 2619, loss 0.0496327, acc 0.98
2016-09-07T19:24:48.743256: step 2620, loss 0.0476313, acc 0.96
2016-09-07T19:24:49.406227: step 2621, loss 0.0725499, acc 0.96
2016-09-07T19:24:50.067352: step 2622, loss 0.0675123, acc 0.98
2016-09-07T19:24:50.728327: step 2623, loss 0.0116292, acc 1
2016-09-07T19:24:51.396601: step 2624, loss 0.0125181, acc 1
2016-09-07T19:24:52.064711: step 2625, loss 0.0232361, acc 1
2016-09-07T19:24:52.736069: step 2626, loss 0.153354, acc 0.96
2016-09-07T19:24:53.402785: step 2627, loss 0.0323985, acc 0.98
2016-09-07T19:24:54.078829: step 2628, loss 0.0675711, acc 0.96
2016-09-07T19:24:54.729129: step 2629, loss 0.0231608, acc 1
2016-09-07T19:24:55.375328: step 2630, loss 0.0323723, acc 1
2016-09-07T19:24:56.032179: step 2631, loss 0.0557853, acc 0.98
2016-09-07T19:24:56.703297: step 2632, loss 0.0519711, acc 0.96
2016-09-07T19:24:57.393179: step 2633, loss 0.153738, acc 0.88
2016-09-07T19:24:58.069489: step 2634, loss 0.0240975, acc 0.98
2016-09-07T19:24:58.726807: step 2635, loss 0.00867864, acc 1
2016-09-07T19:24:59.392633: step 2636, loss 0.0124027, acc 1
2016-09-07T19:25:00.061787: step 2637, loss 0.0451805, acc 0.96
2016-09-07T19:25:00.761616: step 2638, loss 0.0921057, acc 0.94
2016-09-07T19:25:01.415261: step 2639, loss 0.0713285, acc 0.96
2016-09-07T19:25:02.083425: step 2640, loss 0.089242, acc 0.98
2016-09-07T19:25:02.755633: step 2641, loss 0.0181169, acc 1
2016-09-07T19:25:03.436684: step 2642, loss 0.0303026, acc 0.98
2016-09-07T19:25:04.146981: step 2643, loss 0.0234311, acc 0.98
2016-09-07T19:25:04.803220: step 2644, loss 0.0775484, acc 0.98
2016-09-07T19:25:05.484454: step 2645, loss 0.048582, acc 0.96
2016-09-07T19:25:06.150326: step 2646, loss 0.0334149, acc 1
2016-09-07T19:25:06.813913: step 2647, loss 0.0217941, acc 1
2016-09-07T19:25:07.466020: step 2648, loss 0.0537777, acc 0.96
2016-09-07T19:25:08.124346: step 2649, loss 0.0738465, acc 0.98
2016-09-07T19:25:08.793988: step 2650, loss 0.0872366, acc 0.98
2016-09-07T19:25:09.455340: step 2651, loss 0.00636089, acc 1
2016-09-07T19:25:10.132886: step 2652, loss 0.0535523, acc 0.98
2016-09-07T19:25:10.785227: step 2653, loss 0.0729761, acc 0.98
2016-09-07T19:25:11.460106: step 2654, loss 0.0297639, acc 0.98
2016-09-07T19:25:12.121140: step 2655, loss 0.0329096, acc 0.98
2016-09-07T19:25:12.797589: step 2656, loss 0.0519062, acc 0.98
2016-09-07T19:25:13.463213: step 2657, loss 0.0651899, acc 0.94
2016-09-07T19:25:14.132440: step 2658, loss 0.0480511, acc 0.96
2016-09-07T19:25:14.810892: step 2659, loss 0.0971553, acc 0.96
2016-09-07T19:25:15.481464: step 2660, loss 0.00126197, acc 1
2016-09-07T19:25:16.157329: step 2661, loss 0.124134, acc 0.98
2016-09-07T19:25:16.826426: step 2662, loss 0.0776772, acc 0.96
2016-09-07T19:25:17.484524: step 2663, loss 0.0316124, acc 0.98
2016-09-07T19:25:18.135062: step 2664, loss 0.0311448, acc 1
2016-09-07T19:25:18.820626: step 2665, loss 0.0567197, acc 0.96
2016-09-07T19:25:19.494091: step 2666, loss 0.0178698, acc 0.98
2016-09-07T19:25:20.157940: step 2667, loss 0.0546658, acc 0.98
2016-09-07T19:25:20.824935: step 2668, loss 0.0609214, acc 0.96
2016-09-07T19:25:21.497348: step 2669, loss 0.0345091, acc 0.98
2016-09-07T19:25:22.185335: step 2670, loss 0.0626869, acc 0.98
2016-09-07T19:25:22.849921: step 2671, loss 0.0853989, acc 0.96
2016-09-07T19:25:23.521650: step 2672, loss 0.0437127, acc 0.96
2016-09-07T19:25:24.212766: step 2673, loss 0.0392394, acc 0.98
2016-09-07T19:25:24.893341: step 2674, loss 0.105359, acc 0.96
2016-09-07T19:25:25.564104: step 2675, loss 0.0581241, acc 0.96
2016-09-07T19:25:26.231604: step 2676, loss 0.0425978, acc 1
2016-09-07T19:25:26.906017: step 2677, loss 0.0376217, acc 1
2016-09-07T19:25:27.583944: step 2678, loss 0.123206, acc 0.94
2016-09-07T19:25:28.273099: step 2679, loss 0.0138486, acc 1
2016-09-07T19:25:28.950936: step 2680, loss 0.00616699, acc 1
2016-09-07T19:25:29.604113: step 2681, loss 0.00600569, acc 1
2016-09-07T19:25:30.256320: step 2682, loss 0.1032, acc 0.96
2016-09-07T19:25:30.916599: step 2683, loss 0.0385082, acc 1
2016-09-07T19:25:31.573623: step 2684, loss 0.0209772, acc 1
2016-09-07T19:25:32.234690: step 2685, loss 0.0034213, acc 1
2016-09-07T19:25:32.896264: step 2686, loss 0.0277128, acc 1
2016-09-07T19:25:33.556280: step 2687, loss 0.0345857, acc 1
2016-09-07T19:25:34.210057: step 2688, loss 0.0297736, acc 0.98
2016-09-07T19:25:34.858860: step 2689, loss 0.00501774, acc 1
2016-09-07T19:25:35.509135: step 2690, loss 0.0622402, acc 0.98
2016-09-07T19:25:36.177634: step 2691, loss 0.0139032, acc 1
2016-09-07T19:25:36.860864: step 2692, loss 0.0536929, acc 1
2016-09-07T19:25:37.537154: step 2693, loss 0.0997382, acc 0.96
2016-09-07T19:25:38.209493: step 2694, loss 0.0292012, acc 1
2016-09-07T19:25:38.862618: step 2695, loss 0.0247217, acc 1
2016-09-07T19:25:39.522675: step 2696, loss 0.0437212, acc 0.98
2016-09-07T19:25:40.192119: step 2697, loss 0.208992, acc 0.96
2016-09-07T19:25:40.862648: step 2698, loss 0.0519962, acc 0.96
2016-09-07T19:25:41.522554: step 2699, loss 0.0373988, acc 0.98
2016-09-07T19:25:42.206143: step 2700, loss 0.0218333, acc 1

Evaluation:
2016-09-07T19:25:45.332243: step 2700, loss 1.67479, acc 0.762

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-2700

2016-09-07T19:25:47.012736: step 2701, loss 0.15149, acc 0.94
2016-09-07T19:25:47.669521: step 2702, loss 0.0668983, acc 0.98
2016-09-07T19:25:48.342230: step 2703, loss 0.0649135, acc 0.94
2016-09-07T19:25:49.013026: step 2704, loss 0.0185532, acc 1
2016-09-07T19:25:49.672396: step 2705, loss 0.0277288, acc 0.98
2016-09-07T19:25:50.338393: step 2706, loss 0.0316929, acc 0.98
2016-09-07T19:25:51.021441: step 2707, loss 0.0695263, acc 0.96
2016-09-07T19:25:51.697358: step 2708, loss 0.0246552, acc 0.98
2016-09-07T19:25:52.358218: step 2709, loss 0.0806248, acc 0.94
2016-09-07T19:25:53.073159: step 2710, loss 0.00153333, acc 1
2016-09-07T19:25:53.771319: step 2711, loss 0.0531312, acc 0.96
2016-09-07T19:25:54.443000: step 2712, loss 0.0884192, acc 0.98
2016-09-07T19:25:55.129646: step 2713, loss 0.042647, acc 0.98
2016-09-07T19:25:55.802348: step 2714, loss 0.0553557, acc 0.96
2016-09-07T19:25:56.460873: step 2715, loss 0.0504547, acc 0.98
2016-09-07T19:25:56.818549: step 2716, loss 0.0687721, acc 0.916667
2016-09-07T19:25:57.501231: step 2717, loss 0.0439992, acc 0.98
2016-09-07T19:25:58.169704: step 2718, loss 0.0154014, acc 1
2016-09-07T19:25:58.834481: step 2719, loss 0.0794597, acc 0.96
2016-09-07T19:25:59.505779: step 2720, loss 0.0132691, acc 1
2016-09-07T19:26:00.182569: step 2721, loss 0.0325798, acc 1
2016-09-07T19:26:00.904847: step 2722, loss 0.0267597, acc 1
2016-09-07T19:26:01.575991: step 2723, loss 0.0354526, acc 0.98
2016-09-07T19:26:02.255490: step 2724, loss 0.0219151, acc 1
2016-09-07T19:26:02.931422: step 2725, loss 0.0040038, acc 1
2016-09-07T19:26:03.606103: step 2726, loss 0.0255277, acc 0.98
2016-09-07T19:26:04.284639: step 2727, loss 0.0224344, acc 1
2016-09-07T19:26:04.974787: step 2728, loss 0.0373713, acc 1
2016-09-07T19:26:05.652305: step 2729, loss 0.0607604, acc 0.96
2016-09-07T19:26:06.338277: step 2730, loss 0.0133823, acc 1
2016-09-07T19:26:07.014242: step 2731, loss 0.0136622, acc 1
2016-09-07T19:26:07.689809: step 2732, loss 0.0897513, acc 0.98
2016-09-07T19:26:08.354265: step 2733, loss 0.0117653, acc 1
2016-09-07T19:26:09.026616: step 2734, loss 0.0723706, acc 0.94
2016-09-07T19:26:09.698130: step 2735, loss 0.0614921, acc 0.96
2016-09-07T19:26:10.398419: step 2736, loss 0.104213, acc 0.96
2016-09-07T19:26:11.068141: step 2737, loss 0.0265064, acc 1
2016-09-07T19:26:11.773154: step 2738, loss 0.0361916, acc 1
2016-09-07T19:26:12.441588: step 2739, loss 0.0514929, acc 0.98
2016-09-07T19:26:13.114908: step 2740, loss 0.0326153, acc 0.98
2016-09-07T19:26:13.783162: step 2741, loss 0.0195203, acc 0.98
2016-09-07T19:26:14.475480: step 2742, loss 0.0323325, acc 0.98
2016-09-07T19:26:15.140529: step 2743, loss 0.0702605, acc 0.96
2016-09-07T19:26:15.795553: step 2744, loss 0.0158805, acc 1
2016-09-07T19:26:16.474626: step 2745, loss 0.0434691, acc 0.96
2016-09-07T19:26:17.132261: step 2746, loss 0.190616, acc 0.96
2016-09-07T19:26:17.814138: step 2747, loss 0.0412397, acc 0.98
2016-09-07T19:26:18.484587: step 2748, loss 0.0746281, acc 0.96
2016-09-07T19:26:19.172241: step 2749, loss 0.0351832, acc 0.98
2016-09-07T19:26:19.849022: step 2750, loss 0.112759, acc 0.96
2016-09-07T19:26:20.536879: step 2751, loss 0.103434, acc 0.94
2016-09-07T19:26:21.200078: step 2752, loss 0.0419761, acc 0.96
2016-09-07T19:26:21.883414: step 2753, loss 0.139215, acc 0.96
2016-09-07T19:26:22.557428: step 2754, loss 0.0639291, acc 0.98
2016-09-07T19:26:23.234789: step 2755, loss 0.157849, acc 0.94
2016-09-07T19:26:23.923010: step 2756, loss 0.000337444, acc 1
2016-09-07T19:26:24.600347: step 2757, loss 0.09125, acc 0.96
2016-09-07T19:26:25.292058: step 2758, loss 0.0939832, acc 0.98
2016-09-07T19:26:25.961576: step 2759, loss 0.124188, acc 0.94
2016-09-07T19:26:26.651210: step 2760, loss 0.142662, acc 0.96
2016-09-07T19:26:27.339294: step 2761, loss 0.0353339, acc 1
2016-09-07T19:26:28.017618: step 2762, loss 0.108477, acc 0.94
2016-09-07T19:26:28.687152: step 2763, loss 0.0414023, acc 0.98
2016-09-07T19:26:29.352503: step 2764, loss 0.061306, acc 0.98
2016-09-07T19:26:30.030053: step 2765, loss 0.0366367, acc 1
2016-09-07T19:26:30.709319: step 2766, loss 0.0368369, acc 1
2016-09-07T19:26:31.383660: step 2767, loss 0.00755893, acc 1
2016-09-07T19:26:32.052230: step 2768, loss 0.0318743, acc 0.98
2016-09-07T19:26:32.709599: step 2769, loss 0.0265184, acc 1
2016-09-07T19:26:33.376866: step 2770, loss 0.0164987, acc 0.98
2016-09-07T19:26:34.045839: step 2771, loss 0.0145215, acc 1
2016-09-07T19:26:34.718097: step 2772, loss 0.0206517, acc 1
2016-09-07T19:26:35.387703: step 2773, loss 0.0764174, acc 0.96
2016-09-07T19:26:36.045142: step 2774, loss 0.00888418, acc 1
2016-09-07T19:26:36.712677: step 2775, loss 0.0240352, acc 0.98
2016-09-07T19:26:37.384752: step 2776, loss 0.0509645, acc 0.98
2016-09-07T19:26:38.064006: step 2777, loss 0.0514686, acc 0.98
2016-09-07T19:26:38.733366: step 2778, loss 0.030362, acc 0.98
2016-09-07T19:26:39.395037: step 2779, loss 0.0574382, acc 0.98
2016-09-07T19:26:40.065111: step 2780, loss 0.0530966, acc 0.96
2016-09-07T19:26:40.737328: step 2781, loss 0.0690324, acc 0.98
2016-09-07T19:26:41.405936: step 2782, loss 0.0318, acc 0.98
2016-09-07T19:26:42.082607: step 2783, loss 0.0907134, acc 0.96
2016-09-07T19:26:42.760129: step 2784, loss 0.0562143, acc 0.96
2016-09-07T19:26:43.420568: step 2785, loss 0.0028483, acc 1
2016-09-07T19:26:44.120399: step 2786, loss 0.0401743, acc 0.96
2016-09-07T19:26:44.800652: step 2787, loss 0.0108636, acc 1
2016-09-07T19:26:45.460396: step 2788, loss 0.00666476, acc 1
2016-09-07T19:26:46.134795: step 2789, loss 0.026047, acc 0.98
2016-09-07T19:26:46.794071: step 2790, loss 0.0194182, acc 0.98
2016-09-07T19:26:47.462844: step 2791, loss 0.0206295, acc 1
2016-09-07T19:26:48.128465: step 2792, loss 0.0738887, acc 0.96
2016-09-07T19:26:48.834257: step 2793, loss 0.0339435, acc 0.98
2016-09-07T19:26:49.508295: step 2794, loss 0.00362326, acc 1
2016-09-07T19:26:50.186563: step 2795, loss 0.0406554, acc 0.98
2016-09-07T19:26:50.864264: step 2796, loss 0.031409, acc 0.98
2016-09-07T19:26:51.538171: step 2797, loss 0.0244812, acc 0.98
2016-09-07T19:26:52.220572: step 2798, loss 0.0277345, acc 0.98
2016-09-07T19:26:52.885040: step 2799, loss 0.0820248, acc 0.94
2016-09-07T19:26:53.578642: step 2800, loss 0.0153354, acc 1

Evaluation:
2016-09-07T19:26:56.710138: step 2800, loss 1.71439, acc 0.76

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-2800

2016-09-07T19:26:58.407999: step 2801, loss 0.0819563, acc 0.94
2016-09-07T19:26:59.091988: step 2802, loss 0.0167401, acc 1
2016-09-07T19:26:59.781092: step 2803, loss 0.0216107, acc 1
2016-09-07T19:27:00.484924: step 2804, loss 0.0661658, acc 0.98
2016-09-07T19:27:01.146838: step 2805, loss 0.0224931, acc 0.98
2016-09-07T19:27:01.810225: step 2806, loss 0.0342421, acc 1
2016-09-07T19:27:02.483492: step 2807, loss 0.0705932, acc 0.94
2016-09-07T19:27:03.162941: step 2808, loss 0.0709543, acc 0.98
2016-09-07T19:27:03.827346: step 2809, loss 0.029961, acc 0.98
2016-09-07T19:27:04.482906: step 2810, loss 0.0541293, acc 0.96
2016-09-07T19:27:05.175112: step 2811, loss 0.0271621, acc 0.98
2016-09-07T19:27:05.839037: step 2812, loss 0.0417159, acc 0.98
2016-09-07T19:27:06.514257: step 2813, loss 0.0427458, acc 0.98
2016-09-07T19:27:07.182062: step 2814, loss 0.043971, acc 0.98
2016-09-07T19:27:07.843970: step 2815, loss 0.0943499, acc 0.96
2016-09-07T19:27:08.518244: step 2816, loss 0.0645097, acc 0.96
2016-09-07T19:27:09.193713: step 2817, loss 0.0272082, acc 0.98
2016-09-07T19:27:09.848331: step 2818, loss 0.0450915, acc 0.96
2016-09-07T19:27:10.519084: step 2819, loss 0.0259198, acc 0.98
2016-09-07T19:27:11.188994: step 2820, loss 0.0241519, acc 0.98
2016-09-07T19:27:11.847388: step 2821, loss 0.0183374, acc 1
2016-09-07T19:27:12.534668: step 2822, loss 0.0533267, acc 0.96
2016-09-07T19:27:13.191695: step 2823, loss 0.0201991, acc 1
2016-09-07T19:27:13.866269: step 2824, loss 0.0120948, acc 1
2016-09-07T19:27:14.559584: step 2825, loss 0.040184, acc 0.98
2016-09-07T19:27:15.264768: step 2826, loss 0.0706393, acc 0.98
2016-09-07T19:27:15.936055: step 2827, loss 0.0230669, acc 1
2016-09-07T19:27:16.586818: step 2828, loss 0.0353419, acc 0.98
2016-09-07T19:27:17.257715: step 2829, loss 0.0459658, acc 0.98
2016-09-07T19:27:17.938605: step 2830, loss 0.0329803, acc 0.98
2016-09-07T19:27:18.604643: step 2831, loss 0.0807457, acc 0.96
2016-09-07T19:27:19.285921: step 2832, loss 0.0877881, acc 0.96
2016-09-07T19:27:19.953687: step 2833, loss 0.0137821, acc 1
2016-09-07T19:27:20.611804: step 2834, loss 0.0120659, acc 1
2016-09-07T19:27:21.297424: step 2835, loss 0.00232428, acc 1
2016-09-07T19:27:21.958815: step 2836, loss 0.188517, acc 0.96
2016-09-07T19:27:22.635675: step 2837, loss 0.0307295, acc 1
2016-09-07T19:27:23.323904: step 2838, loss 0.0907515, acc 0.94
2016-09-07T19:27:24.002067: step 2839, loss 0.0124176, acc 1
2016-09-07T19:27:24.657326: step 2840, loss 0.0193637, acc 1
2016-09-07T19:27:25.337312: step 2841, loss 0.154525, acc 0.92
2016-09-07T19:27:26.020075: step 2842, loss 0.164144, acc 0.96
2016-09-07T19:27:26.693507: step 2843, loss 0.0234157, acc 1
2016-09-07T19:27:27.351989: step 2844, loss 0.00535626, acc 1
2016-09-07T19:27:28.019227: step 2845, loss 0.0511465, acc 0.98
2016-09-07T19:27:28.699029: step 2846, loss 0.00629127, acc 1
2016-09-07T19:27:29.361390: step 2847, loss 0.0448475, acc 0.98
2016-09-07T19:27:30.034160: step 2848, loss 0.0435079, acc 0.96
2016-09-07T19:27:30.738941: step 2849, loss 0.0121927, acc 1
2016-09-07T19:27:31.433574: step 2850, loss 0.0259396, acc 0.98
2016-09-07T19:27:32.085276: step 2851, loss 0.0244547, acc 1
2016-09-07T19:27:32.778229: step 2852, loss 0.0488729, acc 0.98
2016-09-07T19:27:33.455385: step 2853, loss 0.0458686, acc 0.98
2016-09-07T19:27:34.133044: step 2854, loss 0.194879, acc 0.98
2016-09-07T19:27:34.808405: step 2855, loss 0.0522299, acc 0.98
2016-09-07T19:27:35.491159: step 2856, loss 0.0270854, acc 1
2016-09-07T19:27:36.152916: step 2857, loss 0.0362441, acc 0.98
2016-09-07T19:27:36.823231: step 2858, loss 0.0539677, acc 0.96
2016-09-07T19:27:37.494684: step 2859, loss 0.154441, acc 0.92
2016-09-07T19:27:38.171829: step 2860, loss 0.0477603, acc 0.98
2016-09-07T19:27:38.827979: step 2861, loss 0.0607323, acc 0.96
2016-09-07T19:27:39.499815: step 2862, loss 0.187648, acc 0.98
2016-09-07T19:27:40.193085: step 2863, loss 0.0715254, acc 0.96
2016-09-07T19:27:40.858209: step 2864, loss 0.0247186, acc 1
2016-09-07T19:27:41.542237: step 2865, loss 0.0233102, acc 1
2016-09-07T19:27:42.218800: step 2866, loss 0.00401943, acc 1
2016-09-07T19:27:42.884561: step 2867, loss 0.10201, acc 0.92
2016-09-07T19:27:43.557294: step 2868, loss 0.160101, acc 0.9
2016-09-07T19:27:44.226303: step 2869, loss 0.0345204, acc 0.98
2016-09-07T19:27:44.894353: step 2870, loss 0.171204, acc 0.98
2016-09-07T19:27:45.553026: step 2871, loss 0.0190812, acc 1
2016-09-07T19:27:46.214452: step 2872, loss 0.00772502, acc 1
2016-09-07T19:27:46.867119: step 2873, loss 0.0545491, acc 0.96
2016-09-07T19:27:47.530960: step 2874, loss 0.0406686, acc 0.98
2016-09-07T19:27:48.207445: step 2875, loss 0.00381071, acc 1
2016-09-07T19:27:48.886358: step 2876, loss 0.054603, acc 0.96
2016-09-07T19:27:49.554741: step 2877, loss 0.0690209, acc 0.98
2016-09-07T19:27:50.222499: step 2878, loss 0.116058, acc 0.94
2016-09-07T19:27:50.885468: step 2879, loss 0.00245383, acc 1
2016-09-07T19:27:51.539933: step 2880, loss 0.0716896, acc 0.96
2016-09-07T19:27:52.221117: step 2881, loss 0.00522405, acc 1
2016-09-07T19:27:52.888746: step 2882, loss 0.0497465, acc 0.98
2016-09-07T19:27:53.541570: step 2883, loss 0.0274726, acc 1
2016-09-07T19:27:54.206708: step 2884, loss 0.0156379, acc 1
2016-09-07T19:27:54.872057: step 2885, loss 0.130989, acc 0.94
2016-09-07T19:27:55.541447: step 2886, loss 0.0999291, acc 0.98
2016-09-07T19:27:56.197751: step 2887, loss 0.0791095, acc 0.94
2016-09-07T19:27:56.880518: step 2888, loss 0.0444639, acc 1
2016-09-07T19:27:57.533397: step 2889, loss 0.0643603, acc 0.98
2016-09-07T19:27:58.187495: step 2890, loss 0.027343, acc 1
2016-09-07T19:27:58.858498: step 2891, loss 0.0238987, acc 1
2016-09-07T19:27:59.526158: step 2892, loss 0.197617, acc 0.94
2016-09-07T19:28:00.190644: step 2893, loss 0.0280574, acc 1
2016-09-07T19:28:00.905061: step 2894, loss 0.0113646, acc 1
2016-09-07T19:28:01.578677: step 2895, loss 0.0969592, acc 0.94
2016-09-07T19:28:02.242966: step 2896, loss 0.0681581, acc 0.98
2016-09-07T19:28:02.923716: step 2897, loss 0.145173, acc 0.96
2016-09-07T19:28:03.624872: step 2898, loss 0.056898, acc 0.96
2016-09-07T19:28:04.319343: step 2899, loss 0.0105026, acc 1
2016-09-07T19:28:05.009816: step 2900, loss 0.0577965, acc 0.98

Evaluation:
2016-09-07T19:28:08.175717: step 2900, loss 1.20231, acc 0.752

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-2900

2016-09-07T19:28:09.864178: step 2901, loss 0.0448727, acc 0.98
2016-09-07T19:28:10.534224: step 2902, loss 0.130912, acc 0.94
2016-09-07T19:28:11.203017: step 2903, loss 0.0956466, acc 0.96
2016-09-07T19:28:11.883380: step 2904, loss 0.0247002, acc 1
2016-09-07T19:28:12.539229: step 2905, loss 0.043493, acc 1
2016-09-07T19:28:13.195119: step 2906, loss 0.0277644, acc 1
2016-09-07T19:28:13.852149: step 2907, loss 0.0324152, acc 0.98
2016-09-07T19:28:14.512326: step 2908, loss 0.0181207, acc 1
2016-09-07T19:28:15.180100: step 2909, loss 0.0222221, acc 1
2016-09-07T19:28:15.540388: step 2910, loss 0.171039, acc 0.916667
2016-09-07T19:28:16.209179: step 2911, loss 0.0449426, acc 0.96
2016-09-07T19:28:16.881333: step 2912, loss 0.0381401, acc 0.98
2016-09-07T19:28:17.545018: step 2913, loss 0.0487357, acc 0.98
2016-09-07T19:28:18.240996: step 2914, loss 0.0308283, acc 1
2016-09-07T19:28:18.908348: step 2915, loss 0.0390325, acc 1
2016-09-07T19:28:19.567427: step 2916, loss 0.112639, acc 0.96
2016-09-07T19:28:20.238785: step 2917, loss 0.0243512, acc 0.98
2016-09-07T19:28:20.928655: step 2918, loss 0.0585201, acc 0.96
2016-09-07T19:28:21.611406: step 2919, loss 0.130236, acc 0.98
2016-09-07T19:28:22.287199: step 2920, loss 0.0613918, acc 0.96
2016-09-07T19:28:22.941818: step 2921, loss 0.0245242, acc 0.98
2016-09-07T19:28:23.637267: step 2922, loss 0.0843683, acc 0.98
2016-09-07T19:28:24.307494: step 2923, loss 0.0981038, acc 0.96
2016-09-07T19:28:24.976219: step 2924, loss 0.0925599, acc 0.94
2016-09-07T19:28:25.670142: step 2925, loss 0.0382097, acc 0.96
2016-09-07T19:28:26.343756: step 2926, loss 0.0264438, acc 1
2016-09-07T19:28:27.012484: step 2927, loss 0.065284, acc 0.98
2016-09-07T19:28:27.687139: step 2928, loss 0.0432157, acc 0.96
2016-09-07T19:28:28.378344: step 2929, loss 0.0469796, acc 0.96
2016-09-07T19:28:29.071282: step 2930, loss 0.0479176, acc 0.96
2016-09-07T19:28:29.731554: step 2931, loss 0.0337782, acc 0.98
2016-09-07T19:28:30.395956: step 2932, loss 0.0365344, acc 0.98
2016-09-07T19:28:31.085135: step 2933, loss 0.0361439, acc 1
2016-09-07T19:28:31.760094: step 2934, loss 0.0122248, acc 1
2016-09-07T19:28:32.430322: step 2935, loss 0.0157926, acc 1
2016-09-07T19:28:33.092446: step 2936, loss 0.169206, acc 0.98
2016-09-07T19:28:33.759322: step 2937, loss 0.040721, acc 0.98
2016-09-07T19:28:34.452329: step 2938, loss 0.0399634, acc 0.98
2016-09-07T19:28:35.123491: step 2939, loss 0.0464853, acc 0.96
2016-09-07T19:28:35.792993: step 2940, loss 0.0319641, acc 1
2016-09-07T19:28:36.449450: step 2941, loss 0.0326704, acc 0.98
2016-09-07T19:28:37.124252: step 2942, loss 0.0764381, acc 0.96
2016-09-07T19:28:37.792644: step 2943, loss 0.0366166, acc 0.98
2016-09-07T19:28:38.448910: step 2944, loss 0.0637045, acc 0.98
2016-09-07T19:28:39.109047: step 2945, loss 0.022707, acc 1
2016-09-07T19:28:39.780657: step 2946, loss 0.0383579, acc 0.98
2016-09-07T19:28:40.449734: step 2947, loss 0.0211837, acc 0.98
2016-09-07T19:28:41.118705: step 2948, loss 0.0138372, acc 1
2016-09-07T19:28:41.787485: step 2949, loss 0.0547695, acc 0.98
2016-09-07T19:28:42.455562: step 2950, loss 0.00211401, acc 1
2016-09-07T19:28:43.128348: step 2951, loss 0.116353, acc 0.92
2016-09-07T19:28:43.783158: step 2952, loss 0.106942, acc 0.98
2016-09-07T19:28:44.452891: step 2953, loss 0.00610497, acc 1
2016-09-07T19:28:45.116610: step 2954, loss 0.105849, acc 0.94
2016-09-07T19:28:45.782469: step 2955, loss 0.0749573, acc 0.96
2016-09-07T19:28:46.432775: step 2956, loss 0.0278081, acc 0.98
2016-09-07T19:28:47.102271: step 2957, loss 0.00457965, acc 1
2016-09-07T19:28:47.765368: step 2958, loss 0.116502, acc 0.96
2016-09-07T19:28:48.458944: step 2959, loss 0.0947595, acc 0.96
2016-09-07T19:28:49.131850: step 2960, loss 0.00343594, acc 1
2016-09-07T19:28:49.813036: step 2961, loss 0.0251334, acc 1
2016-09-07T19:28:50.472992: step 2962, loss 0.00502368, acc 1
2016-09-07T19:28:51.140760: step 2963, loss 0.00137587, acc 1
2016-09-07T19:28:51.813771: step 2964, loss 0.0571122, acc 0.98
2016-09-07T19:28:52.507591: step 2965, loss 0.0223293, acc 1
2016-09-07T19:28:53.196144: step 2966, loss 0.0422826, acc 0.98
2016-09-07T19:28:53.869205: step 2967, loss 0.0138295, acc 1
2016-09-07T19:28:54.534454: step 2968, loss 0.0697962, acc 0.98
2016-09-07T19:28:55.194293: step 2969, loss 0.00875708, acc 1
2016-09-07T19:28:55.860834: step 2970, loss 0.0214896, acc 0.98
2016-09-07T19:28:56.534259: step 2971, loss 0.0806866, acc 0.96
2016-09-07T19:28:57.196248: step 2972, loss 0.0151658, acc 1
2016-09-07T19:28:57.850534: step 2973, loss 0.0470975, acc 0.96
2016-09-07T19:28:58.534124: step 2974, loss 0.0436989, acc 0.96
2016-09-07T19:28:59.209877: step 2975, loss 0.0521743, acc 0.98
2016-09-07T19:28:59.885607: step 2976, loss 0.211753, acc 0.92
2016-09-07T19:29:00.594685: step 2977, loss 0.0730092, acc 0.96
2016-09-07T19:29:01.285928: step 2978, loss 0.0386765, acc 0.98
2016-09-07T19:29:01.957683: step 2979, loss 0.0409801, acc 0.98
2016-09-07T19:29:02.625912: step 2980, loss 0.0632164, acc 0.98
2016-09-07T19:29:03.312170: step 2981, loss 0.01142, acc 1
2016-09-07T19:29:03.972734: step 2982, loss 0.0930758, acc 0.92
2016-09-07T19:29:04.628746: step 2983, loss 0.0323817, acc 0.98
2016-09-07T19:29:05.288459: step 2984, loss 0.0448827, acc 0.98
2016-09-07T19:29:05.955351: step 2985, loss 0.0469745, acc 0.98
2016-09-07T19:29:06.643368: step 2986, loss 0.0825809, acc 0.96
2016-09-07T19:29:07.352313: step 2987, loss 0.0328249, acc 0.98
2016-09-07T19:29:08.034731: step 2988, loss 0.0093157, acc 1
2016-09-07T19:29:08.710151: step 2989, loss 0.128549, acc 0.96
2016-09-07T19:29:09.423936: step 2990, loss 0.0432337, acc 0.98
2016-09-07T19:29:10.101378: step 2991, loss 0.014575, acc 1
2016-09-07T19:29:10.767644: step 2992, loss 0.0219264, acc 1
2016-09-07T19:29:11.416284: step 2993, loss 0.0262485, acc 0.98
2016-09-07T19:29:12.097183: step 2994, loss 0.0371086, acc 1
2016-09-07T19:29:12.764779: step 2995, loss 0.00169241, acc 1
2016-09-07T19:29:13.435277: step 2996, loss 0.037227, acc 0.98
2016-09-07T19:29:14.099602: step 2997, loss 0.0304617, acc 1
2016-09-07T19:29:14.757335: step 2998, loss 0.0196967, acc 0.98
2016-09-07T19:29:15.418751: step 2999, loss 0.0425095, acc 0.98
2016-09-07T19:29:16.074975: step 3000, loss 0.0261822, acc 1

Evaluation:
2016-09-07T19:29:19.253202: step 3000, loss 1.39783, acc 0.763

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-3000

2016-09-07T19:29:20.848511: step 3001, loss 0.0554377, acc 0.98
2016-09-07T19:29:21.517980: step 3002, loss 0.0385899, acc 0.98
2016-09-07T19:29:22.200054: step 3003, loss 0.0128326, acc 1
2016-09-07T19:29:22.868313: step 3004, loss 0.0538053, acc 0.96
2016-09-07T19:29:23.538152: step 3005, loss 0.0226985, acc 1
2016-09-07T19:29:24.230687: step 3006, loss 0.0694128, acc 0.96
2016-09-07T19:29:24.906098: step 3007, loss 0.0246128, acc 1
2016-09-07T19:29:25.581921: step 3008, loss 0.128452, acc 0.94
2016-09-07T19:29:26.258468: step 3009, loss 0.129103, acc 0.94
2016-09-07T19:29:26.922930: step 3010, loss 0.0319436, acc 0.98
2016-09-07T19:29:27.575274: step 3011, loss 0.0100167, acc 1
2016-09-07T19:29:28.216727: step 3012, loss 0.08664, acc 0.96
2016-09-07T19:29:28.902839: step 3013, loss 0.0378561, acc 0.98
2016-09-07T19:29:29.585960: step 3014, loss 0.0307059, acc 0.98
2016-09-07T19:29:30.238008: step 3015, loss 0.0441485, acc 0.98
2016-09-07T19:29:30.908144: step 3016, loss 0.0714089, acc 0.98
2016-09-07T19:29:31.581242: step 3017, loss 0.0341703, acc 0.98
2016-09-07T19:29:32.251583: step 3018, loss 0.0301188, acc 1
2016-09-07T19:29:32.914481: step 3019, loss 0.266553, acc 0.94
2016-09-07T19:29:33.572335: step 3020, loss 0.034328, acc 0.98
2016-09-07T19:29:34.225184: step 3021, loss 0.039777, acc 1
2016-09-07T19:29:34.891154: step 3022, loss 0.0337856, acc 0.96
2016-09-07T19:29:35.550218: step 3023, loss 0.00442903, acc 1
2016-09-07T19:29:36.225933: step 3024, loss 0.103469, acc 0.98
2016-09-07T19:29:36.906190: step 3025, loss 0.0182989, acc 0.98
2016-09-07T19:29:37.571875: step 3026, loss 0.0263949, acc 1
2016-09-07T19:29:38.242279: step 3027, loss 0.146758, acc 0.96
2016-09-07T19:29:38.906132: step 3028, loss 0.0504427, acc 0.98
2016-09-07T19:29:39.567496: step 3029, loss 0.0287154, acc 0.98
2016-09-07T19:29:40.237051: step 3030, loss 0.0617135, acc 0.96
2016-09-07T19:29:40.891801: step 3031, loss 0.0242169, acc 0.98
2016-09-07T19:29:41.582231: step 3032, loss 0.0159172, acc 1
2016-09-07T19:29:42.260311: step 3033, loss 0.0148199, acc 1
2016-09-07T19:29:42.936024: step 3034, loss 0.0269319, acc 0.98
2016-09-07T19:29:43.601990: step 3035, loss 0.0404537, acc 0.98
2016-09-07T19:29:44.280321: step 3036, loss 0.0479513, acc 0.98
2016-09-07T19:29:44.957030: step 3037, loss 0.0366736, acc 0.98
2016-09-07T19:29:45.632286: step 3038, loss 0.0225804, acc 1
2016-09-07T19:29:46.306162: step 3039, loss 0.0399627, acc 0.98
2016-09-07T19:29:46.985913: step 3040, loss 0.0388455, acc 0.96
2016-09-07T19:29:47.650110: step 3041, loss 0.0391683, acc 0.98
2016-09-07T19:29:48.328342: step 3042, loss 0.0374358, acc 1
2016-09-07T19:29:49.018180: step 3043, loss 0.0453365, acc 0.98
2016-09-07T19:29:49.681246: step 3044, loss 0.0261498, acc 0.98
2016-09-07T19:29:50.352231: step 3045, loss 0.0426235, acc 0.98
2016-09-07T19:29:51.034135: step 3046, loss 0.0230146, acc 0.98
2016-09-07T19:29:51.694285: step 3047, loss 0.028068, acc 1
2016-09-07T19:29:52.373203: step 3048, loss 0.00381996, acc 1
2016-09-07T19:29:53.045998: step 3049, loss 0.040242, acc 0.98
2016-09-07T19:29:53.706396: step 3050, loss 0.0597417, acc 1
2016-09-07T19:29:54.382829: step 3051, loss 0.0164283, acc 1
2016-09-07T19:29:55.053452: step 3052, loss 0.0219278, acc 0.98
2016-09-07T19:29:55.753026: step 3053, loss 0.0231706, acc 1
2016-09-07T19:29:56.417447: step 3054, loss 0.0709476, acc 0.98
2016-09-07T19:29:57.077524: step 3055, loss 0.0318109, acc 0.96
2016-09-07T19:29:57.746654: step 3056, loss 0.0248477, acc 0.98
2016-09-07T19:29:58.413994: step 3057, loss 0.0392187, acc 0.98
2016-09-07T19:29:59.076795: step 3058, loss 0.0457518, acc 0.98
2016-09-07T19:29:59.767098: step 3059, loss 0.0141382, acc 1
2016-09-07T19:30:00.468262: step 3060, loss 0.0204079, acc 0.98
2016-09-07T19:30:01.140228: step 3061, loss 0.0541312, acc 0.96
2016-09-07T19:30:01.799781: step 3062, loss 0.080828, acc 0.94
2016-09-07T19:30:02.475090: step 3063, loss 0.0623387, acc 0.94
2016-09-07T19:30:03.136899: step 3064, loss 0.0496014, acc 0.98
2016-09-07T19:30:03.797215: step 3065, loss 0.06806, acc 0.98
2016-09-07T19:30:04.450709: step 3066, loss 0.0371969, acc 0.98
2016-09-07T19:30:05.091466: step 3067, loss 0.0123213, acc 1
2016-09-07T19:30:05.749699: step 3068, loss 0.0185501, acc 0.98
2016-09-07T19:30:06.426069: step 3069, loss 0.00631368, acc 1
2016-09-07T19:30:07.103826: step 3070, loss 0.0156711, acc 1
2016-09-07T19:30:07.756608: step 3071, loss 0.00725476, acc 1
2016-09-07T19:30:08.443313: step 3072, loss 0.0278726, acc 0.98
2016-09-07T19:30:09.112653: step 3073, loss 0.00226562, acc 1
2016-09-07T19:30:09.779578: step 3074, loss 0.0281351, acc 0.98
2016-09-07T19:30:10.438208: step 3075, loss 0.0146942, acc 1
2016-09-07T19:30:11.108224: step 3076, loss 0.076637, acc 0.94
2016-09-07T19:30:11.786289: step 3077, loss 0.117487, acc 0.98
2016-09-07T19:30:12.458252: step 3078, loss 0.0808252, acc 0.98
2016-09-07T19:30:13.120398: step 3079, loss 0.0301804, acc 0.98
2016-09-07T19:30:13.791715: step 3080, loss 0.043758, acc 0.98
2016-09-07T19:30:14.456150: step 3081, loss 0.0313327, acc 0.98
2016-09-07T19:30:15.163007: step 3082, loss 0.196439, acc 0.96
2016-09-07T19:30:15.832983: step 3083, loss 0.0894074, acc 0.96
2016-09-07T19:30:16.505231: step 3084, loss 0.0744152, acc 0.96
2016-09-07T19:30:17.177125: step 3085, loss 0.0283334, acc 0.98
2016-09-07T19:30:17.849583: step 3086, loss 0.012944, acc 1
2016-09-07T19:30:18.519373: step 3087, loss 0.00785169, acc 1
2016-09-07T19:30:19.174015: step 3088, loss 0.0436572, acc 0.96
2016-09-07T19:30:19.827860: step 3089, loss 0.101403, acc 0.96
2016-09-07T19:30:20.505982: step 3090, loss 0.0120463, acc 1
2016-09-07T19:30:21.182572: step 3091, loss 0.011154, acc 1
2016-09-07T19:30:21.849623: step 3092, loss 0.00181956, acc 1
2016-09-07T19:30:22.503901: step 3093, loss 0.00352623, acc 1
2016-09-07T19:30:23.172592: step 3094, loss 0.039668, acc 0.96
2016-09-07T19:30:23.863602: step 3095, loss 0.00515941, acc 1
2016-09-07T19:30:24.557697: step 3096, loss 0.067642, acc 0.96
2016-09-07T19:30:25.247963: step 3097, loss 0.0260074, acc 1
2016-09-07T19:30:25.940472: step 3098, loss 0.00341181, acc 1
2016-09-07T19:30:26.614848: step 3099, loss 0.0189144, acc 1
2016-09-07T19:30:27.276790: step 3100, loss 0.0613726, acc 0.96

Evaluation:
2016-09-07T19:30:30.448693: step 3100, loss 1.54059, acc 0.761

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-3100

2016-09-07T19:30:32.111252: step 3101, loss 0.188243, acc 0.92
2016-09-07T19:30:32.783910: step 3102, loss 0.0125506, acc 1
2016-09-07T19:30:33.453246: step 3103, loss 0.00113455, acc 1
2016-09-07T19:30:33.814542: step 3104, loss 0.000119609, acc 1
2016-09-07T19:30:34.475672: step 3105, loss 0.0478683, acc 1
2016-09-07T19:30:35.129620: step 3106, loss 0.00713789, acc 1
2016-09-07T19:30:35.798525: step 3107, loss 0.0943989, acc 0.94
2016-09-07T19:30:36.477822: step 3108, loss 0.0482286, acc 0.98
2016-09-07T19:30:37.142680: step 3109, loss 0.036688, acc 0.98
2016-09-07T19:30:37.804849: step 3110, loss 0.0781601, acc 0.96
2016-09-07T19:30:38.475188: step 3111, loss 0.00223871, acc 1
2016-09-07T19:30:39.150674: step 3112, loss 0.216524, acc 0.96
2016-09-07T19:30:39.821240: step 3113, loss 0.0178902, acc 1
2016-09-07T19:30:40.486267: step 3114, loss 0.0230823, acc 1
2016-09-07T19:30:41.178531: step 3115, loss 0.00813331, acc 1
2016-09-07T19:30:41.863729: step 3116, loss 0.0142849, acc 1
2016-09-07T19:30:42.526396: step 3117, loss 0.0663047, acc 0.98
2016-09-07T19:30:43.200063: step 3118, loss 0.0318591, acc 0.98
2016-09-07T19:30:43.873042: step 3119, loss 0.121652, acc 0.96
2016-09-07T19:30:44.547555: step 3120, loss 0.109417, acc 0.96
2016-09-07T19:30:45.219109: step 3121, loss 0.0569897, acc 0.96
2016-09-07T19:30:45.893005: step 3122, loss 0.064093, acc 0.96
2016-09-07T19:30:46.562601: step 3123, loss 0.0540977, acc 0.94
2016-09-07T19:30:47.233355: step 3124, loss 0.00342318, acc 1
2016-09-07T19:30:47.892729: step 3125, loss 0.0681043, acc 0.98
2016-09-07T19:30:48.567234: step 3126, loss 0.052578, acc 0.96
2016-09-07T19:30:49.232974: step 3127, loss 0.0197348, acc 1
2016-09-07T19:30:49.893960: step 3128, loss 0.059433, acc 0.96
2016-09-07T19:30:50.563751: step 3129, loss 0.0779279, acc 0.96
2016-09-07T19:30:51.231641: step 3130, loss 0.0111826, acc 1
2016-09-07T19:30:51.893797: step 3131, loss 0.013869, acc 1
2016-09-07T19:30:52.577621: step 3132, loss 0.0119763, acc 1
2016-09-07T19:30:53.249446: step 3133, loss 0.0115284, acc 1
2016-09-07T19:30:53.912209: step 3134, loss 0.0317135, acc 0.98
2016-09-07T19:30:54.582793: step 3135, loss 0.0426063, acc 0.98
2016-09-07T19:30:55.254587: step 3136, loss 0.117972, acc 0.94
2016-09-07T19:30:55.923638: step 3137, loss 0.0451296, acc 0.98
2016-09-07T19:30:56.592961: step 3138, loss 0.0278168, acc 0.98
2016-09-07T19:30:57.256046: step 3139, loss 0.0222082, acc 1
2016-09-07T19:30:57.921900: step 3140, loss 0.14963, acc 0.96
2016-09-07T19:30:58.602287: step 3141, loss 0.129679, acc 0.98
2016-09-07T19:30:59.255240: step 3142, loss 0.121642, acc 0.94
2016-09-07T19:30:59.923393: step 3143, loss 0.0022684, acc 1
2016-09-07T19:31:00.630208: step 3144, loss 0.0114994, acc 1
2016-09-07T19:31:01.300162: step 3145, loss 0.0620548, acc 0.96
2016-09-07T19:31:01.961649: step 3146, loss 0.0583744, acc 0.98
2016-09-07T19:31:02.644359: step 3147, loss 0.0104515, acc 1
2016-09-07T19:31:03.298545: step 3148, loss 0.0447336, acc 0.96
2016-09-07T19:31:03.949498: step 3149, loss 0.11339, acc 0.98
2016-09-07T19:31:04.634401: step 3150, loss 0.0880035, acc 0.96
2016-09-07T19:31:05.297094: step 3151, loss 0.136777, acc 0.96
2016-09-07T19:31:05.965684: step 3152, loss 0.01972, acc 1
2016-09-07T19:31:06.630989: step 3153, loss 0.0178338, acc 1
2016-09-07T19:31:07.315185: step 3154, loss 0.0487056, acc 0.98
2016-09-07T19:31:07.983614: step 3155, loss 0.14747, acc 0.96
2016-09-07T19:31:08.667738: step 3156, loss 0.016933, acc 0.98
2016-09-07T19:31:09.341687: step 3157, loss 0.0209309, acc 1
2016-09-07T19:31:09.988713: step 3158, loss 0.102148, acc 0.98
2016-09-07T19:31:10.662922: step 3159, loss 0.0337974, acc 0.98
2016-09-07T19:31:11.334306: step 3160, loss 0.0597907, acc 0.96
2016-09-07T19:31:12.003628: step 3161, loss 0.0278918, acc 0.98
2016-09-07T19:31:12.709705: step 3162, loss 0.0363084, acc 0.98
2016-09-07T19:31:13.398498: step 3163, loss 0.0299245, acc 0.98
2016-09-07T19:31:14.076684: step 3164, loss 0.140696, acc 0.98
2016-09-07T19:31:14.763821: step 3165, loss 0.0230301, acc 1
2016-09-07T19:31:15.431118: step 3166, loss 0.0582458, acc 0.98
2016-09-07T19:31:16.089452: step 3167, loss 0.117639, acc 0.92
2016-09-07T19:31:16.732282: step 3168, loss 0.0299083, acc 1
2016-09-07T19:31:17.406958: step 3169, loss 0.0676444, acc 0.96
2016-09-07T19:31:18.078281: step 3170, loss 0.0698811, acc 0.96
2016-09-07T19:31:18.750101: step 3171, loss 0.0050445, acc 1
2016-09-07T19:31:19.417974: step 3172, loss 0.00716737, acc 1
2016-09-07T19:31:20.083279: step 3173, loss 0.136041, acc 0.98
2016-09-07T19:31:20.734959: step 3174, loss 0.0378958, acc 0.98
2016-09-07T19:31:21.418360: step 3175, loss 0.0109461, acc 1
2016-09-07T19:31:22.093689: step 3176, loss 0.0374929, acc 1
2016-09-07T19:31:22.755281: step 3177, loss 0.0225603, acc 0.98
2016-09-07T19:31:23.421044: step 3178, loss 0.0025255, acc 1
2016-09-07T19:31:24.088401: step 3179, loss 0.0203381, acc 1
2016-09-07T19:31:24.752279: step 3180, loss 0.041511, acc 1
2016-09-07T19:31:25.452505: step 3181, loss 0.0165342, acc 1
2016-09-07T19:31:26.101286: step 3182, loss 0.0042487, acc 1
2016-09-07T19:31:26.758095: step 3183, loss 0.0457262, acc 0.98
2016-09-07T19:31:27.422224: step 3184, loss 0.0089775, acc 1
2016-09-07T19:31:28.086756: step 3185, loss 0.0315889, acc 0.98
2016-09-07T19:31:28.751682: step 3186, loss 0.0712413, acc 0.96
2016-09-07T19:31:29.419468: step 3187, loss 0.0475326, acc 0.96
2016-09-07T19:31:30.094594: step 3188, loss 0.0291731, acc 0.98
2016-09-07T19:31:30.773140: step 3189, loss 0.033784, acc 0.98
2016-09-07T19:31:31.455243: step 3190, loss 0.0650054, acc 0.98
2016-09-07T19:31:32.134775: step 3191, loss 0.00425696, acc 1
2016-09-07T19:31:32.800421: step 3192, loss 0.0335563, acc 1
2016-09-07T19:31:33.497229: step 3193, loss 0.00363703, acc 1
2016-09-07T19:31:34.179570: step 3194, loss 0.0304364, acc 0.98
2016-09-07T19:31:34.854459: step 3195, loss 0.0159925, acc 1
2016-09-07T19:31:35.520272: step 3196, loss 0.133627, acc 0.96
2016-09-07T19:31:36.205486: step 3197, loss 0.0453725, acc 0.96
2016-09-07T19:31:36.885981: step 3198, loss 0.0144416, acc 1
2016-09-07T19:31:37.558205: step 3199, loss 0.0314612, acc 0.98
2016-09-07T19:31:38.242723: step 3200, loss 0.0822583, acc 0.96

Evaluation:
2016-09-07T19:31:41.438228: step 3200, loss 1.65301, acc 0.737

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-3200

2016-09-07T19:31:43.208696: step 3201, loss 0.0450853, acc 0.96
2016-09-07T19:31:43.877132: step 3202, loss 0.189849, acc 0.9
2016-09-07T19:31:44.543115: step 3203, loss 0.0468662, acc 0.96
2016-09-07T19:31:45.211388: step 3204, loss 0.112326, acc 0.94
2016-09-07T19:31:45.882058: step 3205, loss 0.0182015, acc 0.98
2016-09-07T19:31:46.545132: step 3206, loss 0.0160605, acc 1
2016-09-07T19:31:47.216329: step 3207, loss 0.0735396, acc 0.98
2016-09-07T19:31:47.882757: step 3208, loss 0.0317238, acc 0.98
2016-09-07T19:31:48.554990: step 3209, loss 0.0183754, acc 0.98
2016-09-07T19:31:49.210339: step 3210, loss 0.0333144, acc 0.98
2016-09-07T19:31:49.894067: step 3211, loss 0.0389584, acc 0.98
2016-09-07T19:31:50.544140: step 3212, loss 0.0280704, acc 1
2016-09-07T19:31:51.207616: step 3213, loss 0.0470724, acc 0.98
2016-09-07T19:31:51.873993: step 3214, loss 0.0086163, acc 1
2016-09-07T19:31:52.529637: step 3215, loss 0.0628715, acc 0.98
2016-09-07T19:31:53.203142: step 3216, loss 0.00233066, acc 1
2016-09-07T19:31:53.871529: step 3217, loss 0.0580698, acc 0.98
2016-09-07T19:31:54.550702: step 3218, loss 0.0607543, acc 0.96
2016-09-07T19:31:55.222866: step 3219, loss 0.047588, acc 0.96
2016-09-07T19:31:55.877439: step 3220, loss 0.0290442, acc 1
2016-09-07T19:31:56.549961: step 3221, loss 0.065638, acc 0.96
2016-09-07T19:31:57.226363: step 3222, loss 0.0154402, acc 0.98
2016-09-07T19:31:57.891428: step 3223, loss 0.0903196, acc 0.96
2016-09-07T19:31:58.586473: step 3224, loss 0.0983888, acc 0.96
2016-09-07T19:31:59.253248: step 3225, loss 0.0421509, acc 0.98
2016-09-07T19:31:59.931966: step 3226, loss 0.0358299, acc 0.98
2016-09-07T19:32:00.648846: step 3227, loss 0.0401743, acc 0.98
2016-09-07T19:32:01.353681: step 3228, loss 0.0262095, acc 0.98
2016-09-07T19:32:02.017361: step 3229, loss 0.0995893, acc 0.96
2016-09-07T19:32:02.693943: step 3230, loss 0.110323, acc 0.98
2016-09-07T19:32:03.384582: step 3231, loss 0.163895, acc 0.94
2016-09-07T19:32:04.062679: step 3232, loss 0.0112464, acc 1
2016-09-07T19:32:04.731150: step 3233, loss 0.0283388, acc 0.98
2016-09-07T19:32:05.395653: step 3234, loss 0.100021, acc 0.98
2016-09-07T19:32:06.065838: step 3235, loss 0.147491, acc 0.94
2016-09-07T19:32:06.732050: step 3236, loss 0.0643421, acc 0.96
2016-09-07T19:32:07.392494: step 3237, loss 0.00426449, acc 1
2016-09-07T19:32:08.057693: step 3238, loss 0.0256977, acc 1
2016-09-07T19:32:08.737514: step 3239, loss 0.0342367, acc 0.98
2016-09-07T19:32:09.404870: step 3240, loss 0.0763728, acc 0.96
2016-09-07T19:32:10.070201: step 3241, loss 0.0287656, acc 0.98
2016-09-07T19:32:10.752791: step 3242, loss 0.0100444, acc 1
2016-09-07T19:32:11.435103: step 3243, loss 0.0260674, acc 1
2016-09-07T19:32:12.090025: step 3244, loss 0.0769252, acc 0.96
2016-09-07T19:32:12.762708: step 3245, loss 0.0586034, acc 0.96
2016-09-07T19:32:13.429514: step 3246, loss 0.0430345, acc 0.98
2016-09-07T19:32:14.097029: step 3247, loss 0.0167844, acc 1
2016-09-07T19:32:14.768248: step 3248, loss 0.0476334, acc 0.98
2016-09-07T19:32:15.454683: step 3249, loss 0.0334881, acc 0.98
2016-09-07T19:32:16.123200: step 3250, loss 0.0476596, acc 0.98
2016-09-07T19:32:16.799169: step 3251, loss 0.024323, acc 1
2016-09-07T19:32:17.478617: step 3252, loss 0.00806172, acc 1
2016-09-07T19:32:18.149843: step 3253, loss 0.136574, acc 0.92
2016-09-07T19:32:18.832212: step 3254, loss 0.0135582, acc 1
2016-09-07T19:32:19.516685: step 3255, loss 0.0510984, acc 0.98
2016-09-07T19:32:20.193306: step 3256, loss 0.10367, acc 0.98
2016-09-07T19:32:20.863107: step 3257, loss 0.0778578, acc 0.94
2016-09-07T19:32:21.526873: step 3258, loss 0.0593262, acc 0.96
2016-09-07T19:32:22.213237: step 3259, loss 0.1418, acc 0.94
2016-09-07T19:32:22.896715: step 3260, loss 0.0130348, acc 1
2016-09-07T19:32:23.557977: step 3261, loss 0.0738674, acc 0.98
2016-09-07T19:32:24.229595: step 3262, loss 0.0272699, acc 0.98
2016-09-07T19:32:24.892650: step 3263, loss 0.0303666, acc 0.98
2016-09-07T19:32:25.561933: step 3264, loss 0.023193, acc 1
2016-09-07T19:32:26.216736: step 3265, loss 0.0522347, acc 0.98
2016-09-07T19:32:26.884692: step 3266, loss 0.0476613, acc 0.98
2016-09-07T19:32:27.546198: step 3267, loss 0.0291884, acc 1
2016-09-07T19:32:28.210172: step 3268, loss 0.0085967, acc 1
2016-09-07T19:32:28.886861: step 3269, loss 0.0455456, acc 0.98
2016-09-07T19:32:29.570638: step 3270, loss 0.0191137, acc 0.98
2016-09-07T19:32:30.265165: step 3271, loss 0.0180231, acc 1
2016-09-07T19:32:30.942960: step 3272, loss 0.0112609, acc 1
2016-09-07T19:32:31.610994: step 3273, loss 0.0466763, acc 0.98
2016-09-07T19:32:32.282330: step 3274, loss 0.0510838, acc 0.96
2016-09-07T19:32:32.966586: step 3275, loss 0.042631, acc 0.96
2016-09-07T19:32:33.632854: step 3276, loss 0.0474766, acc 0.96
2016-09-07T19:32:34.297847: step 3277, loss 0.0176225, acc 1
2016-09-07T19:32:34.965599: step 3278, loss 0.00530748, acc 1
2016-09-07T19:32:35.640007: step 3279, loss 0.0670596, acc 0.96
2016-09-07T19:32:36.329856: step 3280, loss 0.112029, acc 0.96
2016-09-07T19:32:36.987275: step 3281, loss 0.00302704, acc 1
2016-09-07T19:32:37.657885: step 3282, loss 0.046665, acc 0.98
2016-09-07T19:32:38.350036: step 3283, loss 0.056416, acc 0.96
2016-09-07T19:32:39.027870: step 3284, loss 0.0797216, acc 0.96
2016-09-07T19:32:39.691098: step 3285, loss 0.0342534, acc 1
2016-09-07T19:32:40.365898: step 3286, loss 0.0877866, acc 0.94
2016-09-07T19:32:41.037504: step 3287, loss 0.146053, acc 0.96
2016-09-07T19:32:41.708215: step 3288, loss 0.0462018, acc 0.98
2016-09-07T19:32:42.380954: step 3289, loss 0.00366211, acc 1
2016-09-07T19:32:43.033445: step 3290, loss 0.128774, acc 0.96
2016-09-07T19:32:43.705654: step 3291, loss 0.0473775, acc 0.98
2016-09-07T19:32:44.380877: step 3292, loss 0.0587961, acc 0.98
2016-09-07T19:32:45.047436: step 3293, loss 0.0725031, acc 0.98
2016-09-07T19:32:45.709663: step 3294, loss 0.0947018, acc 0.96
2016-09-07T19:32:46.361558: step 3295, loss 0.0526651, acc 0.96
2016-09-07T19:32:47.025916: step 3296, loss 0.0351042, acc 0.98
2016-09-07T19:32:47.695532: step 3297, loss 0.0459501, acc 0.98
2016-09-07T19:32:48.046047: step 3298, loss 0.0929919, acc 1
2016-09-07T19:32:48.716555: step 3299, loss 0.0442371, acc 1
2016-09-07T19:32:49.390486: step 3300, loss 0.0232796, acc 0.98

Evaluation:
2016-09-07T19:32:52.615716: step 3300, loss 1.46063, acc 0.732

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-3300

2016-09-07T19:32:54.269074: step 3301, loss 0.0282951, acc 0.98
2016-09-07T19:32:54.980843: step 3302, loss 0.00942758, acc 1
2016-09-07T19:32:55.648891: step 3303, loss 0.0138936, acc 1
2016-09-07T19:32:56.311168: step 3304, loss 0.0444435, acc 0.98
2016-09-07T19:32:57.006952: step 3305, loss 0.156713, acc 0.96
2016-09-07T19:32:57.681683: step 3306, loss 0.0751213, acc 0.94
2016-09-07T19:32:58.351069: step 3307, loss 0.0211038, acc 0.98
2016-09-07T19:32:59.030154: step 3308, loss 0.0215911, acc 1
2016-09-07T19:32:59.708337: step 3309, loss 0.0253513, acc 1
2016-09-07T19:33:00.413118: step 3310, loss 0.0194158, acc 1
2016-09-07T19:33:01.079755: step 3311, loss 0.145334, acc 0.9
2016-09-07T19:33:01.723031: step 3312, loss 0.0434948, acc 0.98
2016-09-07T19:33:02.413206: step 3313, loss 0.0500193, acc 0.98
2016-09-07T19:33:03.076866: step 3314, loss 0.0120564, acc 1
2016-09-07T19:33:03.738714: step 3315, loss 0.0297509, acc 1
2016-09-07T19:33:04.391513: step 3316, loss 0.0777716, acc 0.98
2016-09-07T19:33:05.069551: step 3317, loss 0.0244716, acc 1
2016-09-07T19:33:05.714313: step 3318, loss 0.125135, acc 0.96
2016-09-07T19:33:06.401786: step 3319, loss 0.0290239, acc 1
2016-09-07T19:33:07.075513: step 3320, loss 0.117217, acc 0.9
2016-09-07T19:33:07.757584: step 3321, loss 0.0668301, acc 0.98
2016-09-07T19:33:08.443720: step 3322, loss 0.0458711, acc 0.96
2016-09-07T19:33:09.107811: step 3323, loss 0.0590257, acc 0.96
2016-09-07T19:33:09.785891: step 3324, loss 0.0676394, acc 0.96
2016-09-07T19:33:10.462457: step 3325, loss 0.0129346, acc 1
2016-09-07T19:33:11.128576: step 3326, loss 0.0102843, acc 1
2016-09-07T19:33:11.798779: step 3327, loss 0.0226996, acc 1
2016-09-07T19:33:12.502605: step 3328, loss 0.0684038, acc 0.96
2016-09-07T19:33:13.190957: step 3329, loss 0.0300523, acc 1
2016-09-07T19:33:13.871809: step 3330, loss 0.158841, acc 0.94
2016-09-07T19:33:14.545695: step 3331, loss 0.0207567, acc 0.98
2016-09-07T19:33:15.220184: step 3332, loss 0.0264009, acc 1
2016-09-07T19:33:15.893089: step 3333, loss 0.0693123, acc 0.96
2016-09-07T19:33:16.562064: step 3334, loss 0.0497976, acc 0.98
2016-09-07T19:33:17.235837: step 3335, loss 0.0630189, acc 0.96
2016-09-07T19:33:17.909639: step 3336, loss 0.0419501, acc 0.98
2016-09-07T19:33:18.585034: step 3337, loss 0.00687932, acc 1
2016-09-07T19:33:19.260407: step 3338, loss 0.00401541, acc 1
2016-09-07T19:33:19.934707: step 3339, loss 0.0227487, acc 1
2016-09-07T19:33:20.623345: step 3340, loss 0.0632362, acc 0.96
2016-09-07T19:33:21.302444: step 3341, loss 0.00340884, acc 1
2016-09-07T19:33:21.965659: step 3342, loss 0.000445887, acc 1
2016-09-07T19:33:22.651273: step 3343, loss 0.0366628, acc 0.98
2016-09-07T19:33:23.320748: step 3344, loss 0.100283, acc 0.96
2016-09-07T19:33:24.019334: step 3345, loss 0.017263, acc 1
2016-09-07T19:33:24.704501: step 3346, loss 0.0504118, acc 0.96
2016-09-07T19:33:25.378760: step 3347, loss 0.011546, acc 1
2016-09-07T19:33:26.067451: step 3348, loss 0.0263287, acc 1
2016-09-07T19:33:26.737521: step 3349, loss 0.0178098, acc 1
2016-09-07T19:33:27.407696: step 3350, loss 0.0743434, acc 0.96
2016-09-07T19:33:28.084915: step 3351, loss 0.0308581, acc 0.98
2016-09-07T19:33:28.783715: step 3352, loss 0.0404127, acc 0.98
2016-09-07T19:33:29.446725: step 3353, loss 0.0403199, acc 0.98
2016-09-07T19:33:30.113717: step 3354, loss 0.102382, acc 0.98
2016-09-07T19:33:30.792313: step 3355, loss 0.0495271, acc 0.96
2016-09-07T19:33:31.464780: step 3356, loss 0.0476162, acc 0.98
2016-09-07T19:33:32.113789: step 3357, loss 0.142361, acc 0.96
2016-09-07T19:33:32.802098: step 3358, loss 0.0263284, acc 0.98
2016-09-07T19:33:33.474212: step 3359, loss 0.190716, acc 0.96
2016-09-07T19:33:34.144808: step 3360, loss 0.0283751, acc 1
2016-09-07T19:33:34.811824: step 3361, loss 0.0279542, acc 0.98
2016-09-07T19:33:35.489040: step 3362, loss 0.0962136, acc 0.96
2016-09-07T19:33:36.164283: step 3363, loss 0.117317, acc 0.96
2016-09-07T19:33:36.833382: step 3364, loss 0.198483, acc 0.92
2016-09-07T19:33:37.488491: step 3365, loss 0.0112402, acc 1
2016-09-07T19:33:38.150391: step 3366, loss 0.0201882, acc 0.98
2016-09-07T19:33:38.815305: step 3367, loss 0.0279129, acc 1
2016-09-07T19:33:39.473166: step 3368, loss 0.0469879, acc 0.98
2016-09-07T19:33:40.137775: step 3369, loss 0.00748613, acc 1
2016-09-07T19:33:40.850912: step 3370, loss 0.126273, acc 0.96
2016-09-07T19:33:41.504194: step 3371, loss 0.0314166, acc 1
2016-09-07T19:33:42.174257: step 3372, loss 0.0467526, acc 0.98
2016-09-07T19:33:42.833726: step 3373, loss 0.0149337, acc 1
2016-09-07T19:33:43.491669: step 3374, loss 0.0463409, acc 0.98
2016-09-07T19:33:44.141664: step 3375, loss 0.00478126, acc 1
2016-09-07T19:33:44.820833: step 3376, loss 0.0255418, acc 0.98
2016-09-07T19:33:45.463837: step 3377, loss 0.0610313, acc 0.96
2016-09-07T19:33:46.143539: step 3378, loss 0.0217197, acc 1
2016-09-07T19:33:46.818012: step 3379, loss 0.0506968, acc 0.98
2016-09-07T19:33:47.483143: step 3380, loss 0.0390088, acc 0.98
2016-09-07T19:33:48.156757: step 3381, loss 0.018727, acc 1
2016-09-07T19:33:48.823809: step 3382, loss 0.0138451, acc 1
2016-09-07T19:33:49.481993: step 3383, loss 0.00805878, acc 1
2016-09-07T19:33:50.150377: step 3384, loss 0.0621341, acc 0.98
2016-09-07T19:33:50.840122: step 3385, loss 0.0379734, acc 0.98
2016-09-07T19:33:51.501030: step 3386, loss 0.073553, acc 0.98
2016-09-07T19:33:52.175154: step 3387, loss 0.169114, acc 0.96
2016-09-07T19:33:52.880399: step 3388, loss 0.07157, acc 0.96
2016-09-07T19:33:53.551618: step 3389, loss 0.0210354, acc 1
2016-09-07T19:33:54.242726: step 3390, loss 0.120987, acc 0.98
2016-09-07T19:33:54.917376: step 3391, loss 0.056456, acc 0.98
2016-09-07T19:33:55.610188: step 3392, loss 0.0266348, acc 0.98
2016-09-07T19:33:56.278159: step 3393, loss 0.0476167, acc 0.98
2016-09-07T19:33:56.943407: step 3394, loss 0.037025, acc 0.98
2016-09-07T19:33:57.621130: step 3395, loss 0.0115035, acc 1
2016-09-07T19:33:58.279548: step 3396, loss 0.137055, acc 0.94
2016-09-07T19:33:58.952386: step 3397, loss 0.0299238, acc 0.98
2016-09-07T19:33:59.633532: step 3398, loss 0.0199119, acc 1
2016-09-07T19:34:00.342722: step 3399, loss 0.018139, acc 0.98
2016-09-07T19:34:01.004557: step 3400, loss 0.0275912, acc 1

Evaluation:
2016-09-07T19:34:04.240356: step 3400, loss 1.62426, acc 0.739

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-3400

2016-09-07T19:34:05.928772: step 3401, loss 0.0577888, acc 0.98
2016-09-07T19:34:06.613756: step 3402, loss 0.175591, acc 0.94
2016-09-07T19:34:07.265047: step 3403, loss 0.00684695, acc 1
2016-09-07T19:34:07.932020: step 3404, loss 0.0399433, acc 0.98
2016-09-07T19:34:08.591073: step 3405, loss 0.148353, acc 0.96
2016-09-07T19:34:09.279687: step 3406, loss 0.0354567, acc 1
2016-09-07T19:34:09.963828: step 3407, loss 0.0128852, acc 1
2016-09-07T19:34:10.632957: step 3408, loss 0.0279691, acc 0.98
2016-09-07T19:34:11.311909: step 3409, loss 0.0236184, acc 1
2016-09-07T19:34:11.973161: step 3410, loss 0.180318, acc 0.96
2016-09-07T19:34:12.634176: step 3411, loss 0.0977633, acc 0.98
2016-09-07T19:34:13.310983: step 3412, loss 0.213656, acc 0.98
2016-09-07T19:34:13.998368: step 3413, loss 0.0414137, acc 0.98
2016-09-07T19:34:14.657491: step 3414, loss 0.0437471, acc 1
2016-09-07T19:34:15.344452: step 3415, loss 0.118289, acc 0.98
2016-09-07T19:34:16.022241: step 3416, loss 0.0147944, acc 1
2016-09-07T19:34:16.680364: step 3417, loss 0.0301553, acc 1
2016-09-07T19:34:17.353737: step 3418, loss 0.0301286, acc 0.98
2016-09-07T19:34:18.022863: step 3419, loss 0.0227206, acc 1
2016-09-07T19:34:18.694241: step 3420, loss 0.144357, acc 0.94
2016-09-07T19:34:19.374921: step 3421, loss 0.0215791, acc 0.98
2016-09-07T19:34:20.067117: step 3422, loss 0.0154949, acc 1
2016-09-07T19:34:20.731664: step 3423, loss 0.157787, acc 0.94
2016-09-07T19:34:21.407014: step 3424, loss 0.0423321, acc 0.98
2016-09-07T19:34:22.080827: step 3425, loss 0.0666346, acc 0.96
2016-09-07T19:34:22.758489: step 3426, loss 0.0337574, acc 1
2016-09-07T19:34:23.440898: step 3427, loss 0.0205939, acc 0.98
2016-09-07T19:34:24.099181: step 3428, loss 0.1505, acc 0.96
2016-09-07T19:34:24.769730: step 3429, loss 0.0455909, acc 0.98
2016-09-07T19:34:25.426483: step 3430, loss 0.0426031, acc 0.98
2016-09-07T19:34:26.099473: step 3431, loss 0.0520339, acc 0.96
2016-09-07T19:34:26.761154: step 3432, loss 0.127837, acc 0.94
2016-09-07T19:34:27.463896: step 3433, loss 0.041762, acc 1
2016-09-07T19:34:28.145107: step 3434, loss 0.0643347, acc 0.98
2016-09-07T19:34:28.830509: step 3435, loss 0.0536196, acc 1
2016-09-07T19:34:29.500033: step 3436, loss 0.0201397, acc 1
2016-09-07T19:34:30.165246: step 3437, loss 0.131941, acc 0.96
2016-09-07T19:34:30.836835: step 3438, loss 0.0487197, acc 0.96
2016-09-07T19:34:31.522457: step 3439, loss 0.145949, acc 0.92
2016-09-07T19:34:32.192182: step 3440, loss 0.0375693, acc 0.98
2016-09-07T19:34:32.885057: step 3441, loss 0.0284986, acc 0.98
2016-09-07T19:34:33.560172: step 3442, loss 0.0497182, acc 0.96
2016-09-07T19:34:34.239694: step 3443, loss 0.0577297, acc 0.98
2016-09-07T19:34:34.893905: step 3444, loss 0.0651657, acc 0.96
2016-09-07T19:34:35.565134: step 3445, loss 0.119804, acc 0.94
2016-09-07T19:34:36.222458: step 3446, loss 0.0411417, acc 0.98
2016-09-07T19:34:36.899732: step 3447, loss 0.0545861, acc 0.98
2016-09-07T19:34:37.572352: step 3448, loss 0.00261378, acc 1
2016-09-07T19:34:38.245940: step 3449, loss 0.0391608, acc 0.98
2016-09-07T19:34:38.921844: step 3450, loss 0.0322468, acc 1
2016-09-07T19:34:39.601708: step 3451, loss 0.0318172, acc 0.98
2016-09-07T19:34:40.275518: step 3452, loss 0.0623178, acc 0.98
2016-09-07T19:34:40.940704: step 3453, loss 0.0572918, acc 0.98
2016-09-07T19:34:41.624368: step 3454, loss 0.00660436, acc 1
2016-09-07T19:34:42.320410: step 3455, loss 0.0221232, acc 1
2016-09-07T19:34:43.007464: step 3456, loss 0.0110488, acc 1
2016-09-07T19:34:43.696193: step 3457, loss 0.0179697, acc 1
2016-09-07T19:34:44.372547: step 3458, loss 0.0248005, acc 0.98
2016-09-07T19:34:45.021424: step 3459, loss 0.169612, acc 0.92
2016-09-07T19:34:45.707913: step 3460, loss 0.0178709, acc 1
2016-09-07T19:34:46.368752: step 3461, loss 0.142797, acc 0.92
2016-09-07T19:34:47.056749: step 3462, loss 0.00868619, acc 1
2016-09-07T19:34:47.752322: step 3463, loss 0.0883913, acc 0.96
2016-09-07T19:34:48.425363: step 3464, loss 0.0261637, acc 1
2016-09-07T19:34:49.098100: step 3465, loss 0.0128648, acc 1
2016-09-07T19:34:49.763933: step 3466, loss 0.0285791, acc 0.98
2016-09-07T19:34:50.444671: step 3467, loss 0.123988, acc 0.96
2016-09-07T19:34:51.117410: step 3468, loss 0.0321747, acc 0.98
2016-09-07T19:34:51.795878: step 3469, loss 0.0416379, acc 0.96
2016-09-07T19:34:52.469964: step 3470, loss 0.0316302, acc 0.98
2016-09-07T19:34:53.146306: step 3471, loss 0.0474354, acc 0.98
2016-09-07T19:34:53.823314: step 3472, loss 0.022055, acc 0.98
2016-09-07T19:34:54.493493: step 3473, loss 0.0275974, acc 1
2016-09-07T19:34:55.154950: step 3474, loss 0.0648193, acc 0.96
2016-09-07T19:34:55.812361: step 3475, loss 0.0114681, acc 1
2016-09-07T19:34:56.483312: step 3476, loss 0.00415699, acc 1
2016-09-07T19:34:57.139663: step 3477, loss 0.0198135, acc 1
2016-09-07T19:34:57.812069: step 3478, loss 0.0241747, acc 0.98
2016-09-07T19:34:58.486580: step 3479, loss 0.0314021, acc 0.98
2016-09-07T19:34:59.159044: step 3480, loss 0.00935228, acc 1
2016-09-07T19:34:59.845915: step 3481, loss 0.0144155, acc 1
2016-09-07T19:35:00.549105: step 3482, loss 0.0140474, acc 1
2016-09-07T19:35:01.244407: step 3483, loss 0.052848, acc 0.96
2016-09-07T19:35:01.899999: step 3484, loss 0.0104577, acc 1
2016-09-07T19:35:02.580057: step 3485, loss 0.0877333, acc 0.92
2016-09-07T19:35:03.260855: step 3486, loss 0.0132493, acc 1
2016-09-07T19:35:03.927620: step 3487, loss 0.031053, acc 1
2016-09-07T19:35:04.591143: step 3488, loss 0.0433231, acc 0.98
2016-09-07T19:35:05.244616: step 3489, loss 0.0238825, acc 1
2016-09-07T19:35:05.915896: step 3490, loss 0.0310206, acc 1
2016-09-07T19:35:06.594784: step 3491, loss 0.0355086, acc 0.98
2016-09-07T19:35:06.961770: step 3492, loss 0.0616961, acc 0.916667
2016-09-07T19:35:07.642031: step 3493, loss 0.0264509, acc 0.98
2016-09-07T19:35:08.307337: step 3494, loss 0.0342535, acc 0.98
2016-09-07T19:35:09.004150: step 3495, loss 0.045884, acc 0.98
2016-09-07T19:35:09.677380: step 3496, loss 0.177763, acc 0.98
2016-09-07T19:35:10.346335: step 3497, loss 0.000206177, acc 1
2016-09-07T19:35:11.031066: step 3498, loss 0.0134154, acc 1
2016-09-07T19:35:11.701937: step 3499, loss 0.0403766, acc 0.98
2016-09-07T19:35:12.361536: step 3500, loss 0.0245027, acc 0.98

Evaluation:
2016-09-07T19:35:15.628429: step 3500, loss 1.65537, acc 0.731

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-3500

2016-09-07T19:35:17.358444: step 3501, loss 0.0944729, acc 0.98
2016-09-07T19:35:18.018804: step 3502, loss 0.0036077, acc 1
2016-09-07T19:35:18.704784: step 3503, loss 0.00615063, acc 1
2016-09-07T19:35:19.381509: step 3504, loss 0.059662, acc 0.96
2016-09-07T19:35:20.054457: step 3505, loss 0.0760548, acc 0.98
2016-09-07T19:35:20.720287: step 3506, loss 0.0890214, acc 0.94
2016-09-07T19:35:21.390281: step 3507, loss 0.012808, acc 1
2016-09-07T19:35:22.076352: step 3508, loss 0.0192994, acc 0.98
2016-09-07T19:35:22.751614: step 3509, loss 0.0199379, acc 0.98
2016-09-07T19:35:23.431298: step 3510, loss 0.00631586, acc 1
2016-09-07T19:35:24.092750: step 3511, loss 0.00588005, acc 1
2016-09-07T19:35:24.765974: step 3512, loss 0.00228518, acc 1
2016-09-07T19:35:25.433499: step 3513, loss 0.0561035, acc 0.98
2016-09-07T19:35:26.096276: step 3514, loss 0.0370822, acc 0.98
2016-09-07T19:35:26.750998: step 3515, loss 0.0011981, acc 1
2016-09-07T19:35:27.414879: step 3516, loss 0.0215501, acc 0.98
2016-09-07T19:35:28.083572: step 3517, loss 0.0699996, acc 0.96
2016-09-07T19:35:28.748557: step 3518, loss 0.0100096, acc 1
2016-09-07T19:35:29.429985: step 3519, loss 0.00952786, acc 1
2016-09-07T19:35:30.094051: step 3520, loss 0.0123401, acc 1
2016-09-07T19:35:30.765256: step 3521, loss 0.0194343, acc 1
2016-09-07T19:35:31.433053: step 3522, loss 0.0664811, acc 0.98
2016-09-07T19:35:32.099166: step 3523, loss 0.133664, acc 0.96
2016-09-07T19:35:32.767933: step 3524, loss 0.0574666, acc 0.98
2016-09-07T19:35:33.448645: step 3525, loss 0.0125418, acc 1
2016-09-07T19:35:34.126498: step 3526, loss 0.0304455, acc 0.98
2016-09-07T19:35:34.802346: step 3527, loss 0.00922131, acc 1
2016-09-07T19:35:35.483220: step 3528, loss 0.122827, acc 0.96
2016-09-07T19:35:36.155294: step 3529, loss 0.00013046, acc 1
2016-09-07T19:35:36.846999: step 3530, loss 0.112674, acc 0.98
2016-09-07T19:35:37.533494: step 3531, loss 0.00312319, acc 1
2016-09-07T19:35:38.203189: step 3532, loss 0.000826108, acc 1
2016-09-07T19:35:38.885959: step 3533, loss 0.0197042, acc 0.98
2016-09-07T19:35:39.570706: step 3534, loss 0.0173284, acc 1
2016-09-07T19:35:40.237651: step 3535, loss 0.034647, acc 0.98
2016-09-07T19:35:40.919713: step 3536, loss 0.116983, acc 0.96
2016-09-07T19:35:41.603468: step 3537, loss 0.0228994, acc 1
2016-09-07T19:35:42.270744: step 3538, loss 0.0980838, acc 0.96
2016-09-07T19:35:42.962880: step 3539, loss 0.22635, acc 0.96
2016-09-07T19:35:43.637730: step 3540, loss 0.00146875, acc 1
2016-09-07T19:35:44.310024: step 3541, loss 0.00849304, acc 1
2016-09-07T19:35:44.980715: step 3542, loss 0.0923224, acc 0.94
2016-09-07T19:35:45.654926: step 3543, loss 0.0407147, acc 0.98
2016-09-07T19:35:46.340446: step 3544, loss 0.0999926, acc 0.96
2016-09-07T19:35:47.000744: step 3545, loss 0.0496103, acc 0.96
2016-09-07T19:35:47.669480: step 3546, loss 0.108402, acc 0.96
2016-09-07T19:35:48.362025: step 3547, loss 0.110083, acc 0.94
2016-09-07T19:35:49.030883: step 3548, loss 0.0189701, acc 1
2016-09-07T19:35:49.715188: step 3549, loss 0.0711447, acc 0.98
2016-09-07T19:35:50.368906: step 3550, loss 0.0156653, acc 1
2016-09-07T19:35:51.043397: step 3551, loss 0.0262276, acc 1
2016-09-07T19:35:51.696600: step 3552, loss 0.00561827, acc 1
2016-09-07T19:35:52.365286: step 3553, loss 0.0786564, acc 0.96
2016-09-07T19:35:53.014263: step 3554, loss 0.0152739, acc 1
2016-09-07T19:35:53.681356: step 3555, loss 0.084807, acc 0.98
2016-09-07T19:35:54.355881: step 3556, loss 0.0451841, acc 0.98
2016-09-07T19:35:55.041070: step 3557, loss 0.0222747, acc 1
2016-09-07T19:35:55.724400: step 3558, loss 0.034379, acc 0.96
2016-09-07T19:35:56.402199: step 3559, loss 0.0058874, acc 1
2016-09-07T19:35:57.064665: step 3560, loss 0.0165679, acc 1
2016-09-07T19:35:57.757642: step 3561, loss 0.0265231, acc 0.98
2016-09-07T19:35:58.439917: step 3562, loss 0.0249024, acc 1
2016-09-07T19:35:59.110422: step 3563, loss 0.0422288, acc 0.98
2016-09-07T19:35:59.784108: step 3564, loss 0.0513428, acc 0.98
2016-09-07T19:36:00.497514: step 3565, loss 0.00186054, acc 1
2016-09-07T19:36:01.161970: step 3566, loss 0.0446238, acc 0.96
2016-09-07T19:36:01.845618: step 3567, loss 0.00339422, acc 1
2016-09-07T19:36:02.523927: step 3568, loss 0.042247, acc 0.96
2016-09-07T19:36:03.200330: step 3569, loss 0.010916, acc 1
2016-09-07T19:36:03.868491: step 3570, loss 0.0911567, acc 0.96
2016-09-07T19:36:04.527460: step 3571, loss 0.0312231, acc 0.98
2016-09-07T19:36:05.204455: step 3572, loss 0.0244488, acc 0.98
2016-09-07T19:36:05.868158: step 3573, loss 0.121899, acc 0.96
2016-09-07T19:36:06.549298: step 3574, loss 0.125838, acc 0.98
2016-09-07T19:36:07.234859: step 3575, loss 0.0193468, acc 1
2016-09-07T19:36:07.896496: step 3576, loss 0.0071153, acc 1
2016-09-07T19:36:08.565279: step 3577, loss 0.0217437, acc 1
2016-09-07T19:36:09.248796: step 3578, loss 0.201893, acc 0.92
2016-09-07T19:36:09.912736: step 3579, loss 0.0601876, acc 0.98
2016-09-07T19:36:10.596684: step 3580, loss 0.0344139, acc 1
2016-09-07T19:36:11.282827: step 3581, loss 0.0329674, acc 1
2016-09-07T19:36:11.934004: step 3582, loss 0.0211389, acc 1
2016-09-07T19:36:12.636163: step 3583, loss 0.0344365, acc 0.98
2016-09-07T19:36:13.294566: step 3584, loss 0.0180119, acc 1
2016-09-07T19:36:13.970508: step 3585, loss 0.0311488, acc 0.98
2016-09-07T19:36:14.639444: step 3586, loss 0.0598956, acc 0.98
2016-09-07T19:36:15.306621: step 3587, loss 0.0720197, acc 0.96
2016-09-07T19:36:15.988201: step 3588, loss 0.0154567, acc 1
2016-09-07T19:36:16.652442: step 3589, loss 0.0408591, acc 0.98
2016-09-07T19:36:17.314824: step 3590, loss 0.0300733, acc 1
2016-09-07T19:36:17.993710: step 3591, loss 0.029191, acc 0.98
2016-09-07T19:36:18.677240: step 3592, loss 0.0467741, acc 0.96
2016-09-07T19:36:19.351358: step 3593, loss 0.0252477, acc 1
2016-09-07T19:36:20.036775: step 3594, loss 0.0281474, acc 0.98
2016-09-07T19:36:20.714132: step 3595, loss 0.0208436, acc 1
2016-09-07T19:36:21.386391: step 3596, loss 0.016801, acc 0.98
2016-09-07T19:36:22.059358: step 3597, loss 0.0148132, acc 1
2016-09-07T19:36:22.743259: step 3598, loss 0.0783814, acc 0.98
2016-09-07T19:36:23.412386: step 3599, loss 0.0575552, acc 0.96
2016-09-07T19:36:24.073200: step 3600, loss 0.0191388, acc 1

Evaluation:
2016-09-07T19:36:27.325823: step 3600, loss 1.45963, acc 0.756

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-3600

2016-09-07T19:36:29.005848: step 3601, loss 0.0333579, acc 0.98
2016-09-07T19:36:29.661739: step 3602, loss 0.0305464, acc 0.98
2016-09-07T19:36:30.339920: step 3603, loss 0.0621484, acc 0.96
2016-09-07T19:36:31.023024: step 3604, loss 0.00387456, acc 1
2016-09-07T19:36:31.702932: step 3605, loss 0.0198575, acc 1
2016-09-07T19:36:32.387937: step 3606, loss 0.0949621, acc 0.98
2016-09-07T19:36:33.073901: step 3607, loss 0.0141777, acc 1
2016-09-07T19:36:33.738237: step 3608, loss 0.0376605, acc 1
2016-09-07T19:36:34.406058: step 3609, loss 0.0104591, acc 1
2016-09-07T19:36:35.077447: step 3610, loss 0.162086, acc 0.92
2016-09-07T19:36:35.743602: step 3611, loss 0.0101906, acc 1
2016-09-07T19:36:36.419321: step 3612, loss 0.110491, acc 0.94
2016-09-07T19:36:37.095725: step 3613, loss 0.00832235, acc 1
2016-09-07T19:36:37.772161: step 3614, loss 0.0348675, acc 0.98
2016-09-07T19:36:38.426830: step 3615, loss 0.0061815, acc 1
2016-09-07T19:36:39.094411: step 3616, loss 0.0176255, acc 1
2016-09-07T19:36:39.754800: step 3617, loss 0.128794, acc 0.96
2016-09-07T19:36:40.423114: step 3618, loss 0.0311479, acc 0.98
2016-09-07T19:36:41.089487: step 3619, loss 0.0524182, acc 0.98
2016-09-07T19:36:41.764555: step 3620, loss 0.277428, acc 0.96
2016-09-07T19:36:42.442084: step 3621, loss 0.0268082, acc 0.98
2016-09-07T19:36:43.113776: step 3622, loss 0.0553131, acc 0.98
2016-09-07T19:36:43.828628: step 3623, loss 0.024635, acc 0.98
2016-09-07T19:36:44.515243: step 3624, loss 0.0174952, acc 1
2016-09-07T19:36:45.185419: step 3625, loss 0.0631677, acc 0.96
2016-09-07T19:36:45.850779: step 3626, loss 0.00934315, acc 1
2016-09-07T19:36:46.510045: step 3627, loss 0.0282485, acc 1
2016-09-07T19:36:47.177994: step 3628, loss 0.0625769, acc 0.98
2016-09-07T19:36:47.848355: step 3629, loss 0.0482395, acc 0.98
2016-09-07T19:36:48.534033: step 3630, loss 0.014215, acc 1
2016-09-07T19:36:49.195345: step 3631, loss 0.0158193, acc 1
2016-09-07T19:36:49.878132: step 3632, loss 0.13948, acc 0.96
2016-09-07T19:36:50.560586: step 3633, loss 0.0510816, acc 0.98
2016-09-07T19:36:51.240939: step 3634, loss 0.00519482, acc 1
2016-09-07T19:36:51.928376: step 3635, loss 0.00884599, acc 1
2016-09-07T19:36:52.593126: step 3636, loss 0.052369, acc 0.98
2016-09-07T19:36:53.239213: step 3637, loss 0.0235043, acc 1
2016-09-07T19:36:53.919157: step 3638, loss 0.0243864, acc 1
2016-09-07T19:36:54.581743: step 3639, loss 0.0564802, acc 0.98
2016-09-07T19:36:55.236227: step 3640, loss 0.0159581, acc 1
2016-09-07T19:36:55.902744: step 3641, loss 0.0401417, acc 1
2016-09-07T19:36:56.551510: step 3642, loss 0.0934486, acc 0.94
2016-09-07T19:36:57.222118: step 3643, loss 0.0809077, acc 0.96
2016-09-07T19:36:57.886929: step 3644, loss 0.107127, acc 0.98
2016-09-07T19:36:58.546620: step 3645, loss 0.0314764, acc 0.98
2016-09-07T19:36:59.216968: step 3646, loss 0.0188076, acc 1
2016-09-07T19:36:59.895059: step 3647, loss 0.0615793, acc 0.96
2016-09-07T19:37:00.595843: step 3648, loss 0.0199133, acc 0.98
2016-09-07T19:37:01.265564: step 3649, loss 0.084181, acc 0.94
2016-09-07T19:37:01.936367: step 3650, loss 0.150339, acc 0.94
2016-09-07T19:37:02.598479: step 3651, loss 0.0784787, acc 0.98
2016-09-07T19:37:03.281060: step 3652, loss 0.116123, acc 0.98
2016-09-07T19:37:03.963699: step 3653, loss 0.0357743, acc 0.98
2016-09-07T19:37:04.635815: step 3654, loss 0.0510171, acc 0.98
2016-09-07T19:37:05.307134: step 3655, loss 0.0338476, acc 0.98
2016-09-07T19:37:05.991817: step 3656, loss 0.0118052, acc 1
2016-09-07T19:37:06.669611: step 3657, loss 0.0186504, acc 1
2016-09-07T19:37:07.327537: step 3658, loss 0.0867726, acc 0.94
2016-09-07T19:37:08.008793: step 3659, loss 0.0396378, acc 0.98
2016-09-07T19:37:08.686345: step 3660, loss 0.0232959, acc 0.98
2016-09-07T19:37:09.360141: step 3661, loss 0.0336462, acc 0.98
2016-09-07T19:37:10.052507: step 3662, loss 0.0874767, acc 0.94
2016-09-07T19:37:10.709705: step 3663, loss 0.0391494, acc 0.98
2016-09-07T19:37:11.377273: step 3664, loss 0.0346684, acc 0.98
2016-09-07T19:37:12.063403: step 3665, loss 0.0379854, acc 0.98
2016-09-07T19:37:12.746564: step 3666, loss 0.16182, acc 0.94
2016-09-07T19:37:13.431911: step 3667, loss 0.039155, acc 0.96
2016-09-07T19:37:14.106786: step 3668, loss 0.0229479, acc 0.98
2016-09-07T19:37:14.795059: step 3669, loss 0.0299991, acc 1
2016-09-07T19:37:15.488159: step 3670, loss 0.00790377, acc 1
2016-09-07T19:37:16.156791: step 3671, loss 0.0490051, acc 1
2016-09-07T19:37:16.839281: step 3672, loss 0.128884, acc 0.94
2016-09-07T19:37:17.501032: step 3673, loss 0.0241551, acc 1
2016-09-07T19:37:18.149094: step 3674, loss 0.0313574, acc 1
2016-09-07T19:37:18.800154: step 3675, loss 0.00388127, acc 1
2016-09-07T19:37:19.500341: step 3676, loss 0.0355993, acc 0.98
2016-09-07T19:37:20.143693: step 3677, loss 0.0115873, acc 1
2016-09-07T19:37:20.829377: step 3678, loss 0.0281263, acc 0.98
2016-09-07T19:37:21.497548: step 3679, loss 0.0753049, acc 0.98
2016-09-07T19:37:22.171772: step 3680, loss 0.0491633, acc 0.96
2016-09-07T19:37:22.837707: step 3681, loss 0.00318927, acc 1
2016-09-07T19:37:23.513706: step 3682, loss 0.0321379, acc 0.98
2016-09-07T19:37:24.185806: step 3683, loss 0.0240026, acc 1
2016-09-07T19:37:24.854250: step 3684, loss 0.0154624, acc 1
2016-09-07T19:37:25.527569: step 3685, loss 0.0209795, acc 1
2016-09-07T19:37:25.891263: step 3686, loss 0.0268869, acc 1
2016-09-07T19:37:26.571721: step 3687, loss 0.0199816, acc 1
2016-09-07T19:37:27.251489: step 3688, loss 0.0365567, acc 0.98
2016-09-07T19:37:27.962108: step 3689, loss 0.0678429, acc 0.96
2016-09-07T19:37:28.649645: step 3690, loss 0.0193097, acc 0.98
2016-09-07T19:37:29.326065: step 3691, loss 0.00247753, acc 1
2016-09-07T19:37:29.994028: step 3692, loss 0.0743408, acc 0.96
2016-09-07T19:37:30.656566: step 3693, loss 0.0044552, acc 1
2016-09-07T19:37:31.334322: step 3694, loss 0.0845279, acc 0.96
2016-09-07T19:37:32.006543: step 3695, loss 0.0560848, acc 0.98
2016-09-07T19:37:32.673785: step 3696, loss 0.0734482, acc 0.98
2016-09-07T19:37:33.337662: step 3697, loss 0.0362509, acc 0.98
2016-09-07T19:37:33.996226: step 3698, loss 0.0163964, acc 0.98
2016-09-07T19:37:34.668270: step 3699, loss 0.0196643, acc 1
2016-09-07T19:37:35.392583: step 3700, loss 0.0422422, acc 0.98

Evaluation:
2016-09-07T19:37:38.626720: step 3700, loss 1.71841, acc 0.747

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-3700

2016-09-07T19:37:40.400258: step 3701, loss 0.00579114, acc 1
2016-09-07T19:37:41.069193: step 3702, loss 0.00462514, acc 1
2016-09-07T19:37:41.742862: step 3703, loss 0.0218581, acc 1
2016-09-07T19:37:42.410145: step 3704, loss 0.0225657, acc 1
2016-09-07T19:37:43.086588: step 3705, loss 0.0287535, acc 0.98
2016-09-07T19:37:43.764711: step 3706, loss 0.0887919, acc 0.98
2016-09-07T19:37:44.428475: step 3707, loss 0.069001, acc 0.94
2016-09-07T19:37:45.110611: step 3708, loss 0.0137682, acc 1
2016-09-07T19:37:45.771469: step 3709, loss 0.108978, acc 0.96
2016-09-07T19:37:46.429094: step 3710, loss 0.00519952, acc 1
2016-09-07T19:37:47.097695: step 3711, loss 0.0018647, acc 1
2016-09-07T19:37:47.758262: step 3712, loss 0.0294334, acc 0.98
2016-09-07T19:37:48.416879: step 3713, loss 0.0145663, acc 1
2016-09-07T19:37:49.085021: step 3714, loss 0.00283138, acc 1
2016-09-07T19:37:49.738521: step 3715, loss 0.0223919, acc 1
2016-09-07T19:37:50.425744: step 3716, loss 0.00928524, acc 1
2016-09-07T19:37:51.093004: step 3717, loss 0.0244297, acc 1
2016-09-07T19:37:51.762425: step 3718, loss 0.0330937, acc 1
2016-09-07T19:37:52.440228: step 3719, loss 0.010874, acc 1
2016-09-07T19:37:53.124639: step 3720, loss 0.0192285, acc 1
2016-09-07T19:37:53.794130: step 3721, loss 0.0205643, acc 1
2016-09-07T19:37:54.463815: step 3722, loss 0.0644719, acc 0.94
2016-09-07T19:37:55.133240: step 3723, loss 0.0267364, acc 1
2016-09-07T19:37:55.798067: step 3724, loss 0.0857938, acc 0.96
2016-09-07T19:37:56.473818: step 3725, loss 0.00394324, acc 1
2016-09-07T19:37:57.148946: step 3726, loss 0.0019545, acc 1
2016-09-07T19:37:57.835295: step 3727, loss 0.0710701, acc 0.98
2016-09-07T19:37:58.505229: step 3728, loss 0.0174369, acc 1
2016-09-07T19:37:59.181056: step 3729, loss 0.00741322, acc 1
2016-09-07T19:37:59.832242: step 3730, loss 0.0529601, acc 0.98
2016-09-07T19:38:00.538026: step 3731, loss 0.0193368, acc 0.98
2016-09-07T19:38:01.206739: step 3732, loss 0.0049105, acc 1
2016-09-07T19:38:01.887715: step 3733, loss 0.142369, acc 0.98
2016-09-07T19:38:02.550216: step 3734, loss 0.0742926, acc 0.96
2016-09-07T19:38:03.214614: step 3735, loss 0.0191455, acc 0.98
2016-09-07T19:38:03.894818: step 3736, loss 0.00320461, acc 1
2016-09-07T19:38:04.554351: step 3737, loss 0.0975769, acc 0.94
2016-09-07T19:38:05.202295: step 3738, loss 0.0341242, acc 1
2016-09-07T19:38:05.860715: step 3739, loss 0.0275559, acc 1
2016-09-07T19:38:06.529309: step 3740, loss 0.0415938, acc 0.96
2016-09-07T19:38:07.209883: step 3741, loss 0.0514341, acc 0.98
2016-09-07T19:38:07.881903: step 3742, loss 0.0386378, acc 1
2016-09-07T19:38:08.562868: step 3743, loss 0.0210199, acc 1
2016-09-07T19:38:09.226523: step 3744, loss 0.000722569, acc 1
2016-09-07T19:38:09.897423: step 3745, loss 0.00203434, acc 1
2016-09-07T19:38:10.576606: step 3746, loss 0.00546982, acc 1
2016-09-07T19:38:11.239885: step 3747, loss 0.0720429, acc 0.98
2016-09-07T19:38:11.908520: step 3748, loss 0.0186715, acc 0.98
2016-09-07T19:38:12.576757: step 3749, loss 0.00157501, acc 1
2016-09-07T19:38:13.243264: step 3750, loss 0.0321663, acc 0.98
2016-09-07T19:38:13.941477: step 3751, loss 0.0519849, acc 0.96
2016-09-07T19:38:14.631593: step 3752, loss 0.0201389, acc 1
2016-09-07T19:38:15.307802: step 3753, loss 0.0248327, acc 1
2016-09-07T19:38:15.981679: step 3754, loss 0.0339019, acc 0.98
2016-09-07T19:38:16.658297: step 3755, loss 0.0350127, acc 0.98
2016-09-07T19:38:17.337073: step 3756, loss 0.00825855, acc 1
2016-09-07T19:38:18.006997: step 3757, loss 0.00789047, acc 1
2016-09-07T19:38:18.668898: step 3758, loss 0.0546192, acc 0.96
2016-09-07T19:38:19.330236: step 3759, loss 0.00214228, acc 1
2016-09-07T19:38:20.015210: step 3760, loss 0.0963169, acc 0.96
2016-09-07T19:38:20.697414: step 3761, loss 0.0236359, acc 0.98
2016-09-07T19:38:21.396109: step 3762, loss 0.0517501, acc 0.98
2016-09-07T19:38:22.079478: step 3763, loss 0.0352049, acc 0.98
2016-09-07T19:38:22.756218: step 3764, loss 0.00797984, acc 1
2016-09-07T19:38:23.425820: step 3765, loss 0.071926, acc 0.98
2016-09-07T19:38:24.103540: step 3766, loss 0.0274372, acc 0.98
2016-09-07T19:38:24.785307: step 3767, loss 0.105495, acc 0.96
2016-09-07T19:38:25.443371: step 3768, loss 0.00120614, acc 1
2016-09-07T19:38:26.116586: step 3769, loss 0.0733967, acc 0.96
2016-09-07T19:38:26.783045: step 3770, loss 0.0178629, acc 0.98
2016-09-07T19:38:27.474490: step 3771, loss 0.209985, acc 0.94
2016-09-07T19:38:28.136689: step 3772, loss 0.0507346, acc 0.98
2016-09-07T19:38:28.786009: step 3773, loss 0.00321631, acc 1
2016-09-07T19:38:29.466496: step 3774, loss 0.0157754, acc 1
2016-09-07T19:38:30.127183: step 3775, loss 0.153813, acc 0.96
2016-09-07T19:38:30.799971: step 3776, loss 0.0432454, acc 0.98
2016-09-07T19:38:31.467675: step 3777, loss 0.0501441, acc 0.96
2016-09-07T19:38:32.132279: step 3778, loss 0.0450397, acc 0.98
2016-09-07T19:38:32.816900: step 3779, loss 0.0523498, acc 0.96
2016-09-07T19:38:33.502876: step 3780, loss 0.0885021, acc 0.96
2016-09-07T19:38:34.176443: step 3781, loss 0.0371282, acc 0.96
2016-09-07T19:38:34.844557: step 3782, loss 0.0124861, acc 1
2016-09-07T19:38:35.524702: step 3783, loss 0.0668796, acc 0.96
2016-09-07T19:38:36.213905: step 3784, loss 0.0342065, acc 0.98
2016-09-07T19:38:36.894195: step 3785, loss 0.025731, acc 1
2016-09-07T19:38:37.571443: step 3786, loss 0.0999699, acc 0.94
2016-09-07T19:38:38.240115: step 3787, loss 0.0114456, acc 1
2016-09-07T19:38:38.926822: step 3788, loss 0.0295311, acc 1
2016-09-07T19:38:39.593650: step 3789, loss 0.0196896, acc 0.98
2016-09-07T19:38:40.239704: step 3790, loss 0.089872, acc 0.98
2016-09-07T19:38:40.906877: step 3791, loss 0.0172493, acc 1
2016-09-07T19:38:41.566329: step 3792, loss 0.0295651, acc 0.98
2016-09-07T19:38:42.225571: step 3793, loss 0.0321683, acc 0.98
2016-09-07T19:38:42.906317: step 3794, loss 0.0146252, acc 1
2016-09-07T19:38:43.561673: step 3795, loss 0.0469057, acc 0.96
2016-09-07T19:38:44.226007: step 3796, loss 0.00499207, acc 1
2016-09-07T19:38:44.873387: step 3797, loss 0.044425, acc 1
2016-09-07T19:38:45.546549: step 3798, loss 0.0198638, acc 0.98
2016-09-07T19:38:46.242921: step 3799, loss 0.0321548, acc 1
2016-09-07T19:38:46.910998: step 3800, loss 0.0226893, acc 0.98

Evaluation:
2016-09-07T19:38:50.176660: step 3800, loss 1.62388, acc 0.745

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-3800

2016-09-07T19:38:51.859700: step 3801, loss 0.00642951, acc 1
2016-09-07T19:38:52.523137: step 3802, loss 0.181217, acc 0.94
2016-09-07T19:38:53.189893: step 3803, loss 0.0160652, acc 1
2016-09-07T19:38:53.858290: step 3804, loss 0.0401023, acc 0.98
2016-09-07T19:38:54.539987: step 3805, loss 0.0117668, acc 1
2016-09-07T19:38:55.225036: step 3806, loss 0.100202, acc 0.96
2016-09-07T19:38:55.904166: step 3807, loss 0.0630955, acc 0.98
2016-09-07T19:38:56.565705: step 3808, loss 0.0225435, acc 1
2016-09-07T19:38:57.242659: step 3809, loss 0.0213742, acc 0.98
2016-09-07T19:38:57.905091: step 3810, loss 0.0211217, acc 0.98
2016-09-07T19:38:58.569439: step 3811, loss 0.0793446, acc 0.96
2016-09-07T19:38:59.225090: step 3812, loss 0.0208523, acc 0.98
2016-09-07T19:38:59.898920: step 3813, loss 0.0225151, acc 1
2016-09-07T19:39:00.638574: step 3814, loss 0.0435271, acc 0.98
2016-09-07T19:39:01.309249: step 3815, loss 0.0246574, acc 1
2016-09-07T19:39:01.986948: step 3816, loss 0.0749315, acc 0.96
2016-09-07T19:39:02.680536: step 3817, loss 0.031442, acc 1
2016-09-07T19:39:03.364007: step 3818, loss 0.0881613, acc 0.98
2016-09-07T19:39:04.051793: step 3819, loss 0.0227925, acc 0.98
2016-09-07T19:39:04.724020: step 3820, loss 0.0178644, acc 0.98
2016-09-07T19:39:05.395296: step 3821, loss 0.0439085, acc 0.98
2016-09-07T19:39:06.061646: step 3822, loss 0.00307765, acc 1
2016-09-07T19:39:06.748425: step 3823, loss 0.059971, acc 0.98
2016-09-07T19:39:07.416748: step 3824, loss 0.0031568, acc 1
2016-09-07T19:39:08.100536: step 3825, loss 0.0126674, acc 1
2016-09-07T19:39:08.786322: step 3826, loss 0.0993197, acc 0.94
2016-09-07T19:39:09.469288: step 3827, loss 0.02323, acc 0.98
2016-09-07T19:39:10.140518: step 3828, loss 0.0270237, acc 0.98
2016-09-07T19:39:10.808692: step 3829, loss 0.02933, acc 0.98
2016-09-07T19:39:11.488652: step 3830, loss 0.056716, acc 0.98
2016-09-07T19:39:12.195764: step 3831, loss 0.000126414, acc 1
2016-09-07T19:39:12.876445: step 3832, loss 0.184392, acc 0.92
2016-09-07T19:39:13.554677: step 3833, loss 0.0258625, acc 0.98
2016-09-07T19:39:14.226051: step 3834, loss 0.0281135, acc 0.98
2016-09-07T19:39:14.913637: step 3835, loss 0.0122355, acc 1
2016-09-07T19:39:15.568687: step 3836, loss 0.00119277, acc 1
2016-09-07T19:39:16.233051: step 3837, loss 0.0349761, acc 1
2016-09-07T19:39:16.897180: step 3838, loss 0.132343, acc 0.94
2016-09-07T19:39:17.570050: step 3839, loss 0.040319, acc 0.98
2016-09-07T19:39:18.246040: step 3840, loss 0.0494541, acc 1
2016-09-07T19:39:18.910125: step 3841, loss 0.0335545, acc 0.98
2016-09-07T19:39:19.586931: step 3842, loss 0.0208044, acc 1
2016-09-07T19:39:20.263396: step 3843, loss 0.00405944, acc 1
2016-09-07T19:39:20.942160: step 3844, loss 0.0145228, acc 1
2016-09-07T19:39:21.622513: step 3845, loss 0.0421741, acc 0.96
2016-09-07T19:39:22.285578: step 3846, loss 0.0164664, acc 1
2016-09-07T19:39:22.962833: step 3847, loss 0.0472604, acc 0.98
2016-09-07T19:39:23.641275: step 3848, loss 0.0140676, acc 1
2016-09-07T19:39:24.324624: step 3849, loss 0.00643101, acc 1
2016-09-07T19:39:25.019120: step 3850, loss 0.00517109, acc 1
2016-09-07T19:39:25.688704: step 3851, loss 0.0461494, acc 0.98
2016-09-07T19:39:26.348991: step 3852, loss 0.0786635, acc 0.96
2016-09-07T19:39:27.022306: step 3853, loss 0.0122691, acc 1
2016-09-07T19:39:27.698346: step 3854, loss 0.0386328, acc 0.98
2016-09-07T19:39:28.388082: step 3855, loss 0.0303855, acc 0.98
2016-09-07T19:39:29.051664: step 3856, loss 0.0650214, acc 0.98
2016-09-07T19:39:29.722208: step 3857, loss 0.0967747, acc 0.98
2016-09-07T19:39:30.420722: step 3858, loss 0.0418061, acc 0.96
2016-09-07T19:39:31.108150: step 3859, loss 0.0275401, acc 0.98
2016-09-07T19:39:31.800728: step 3860, loss 0.00896835, acc 1
2016-09-07T19:39:32.456684: step 3861, loss 0.0382444, acc 0.98
2016-09-07T19:39:33.130814: step 3862, loss 0.0456568, acc 0.98
2016-09-07T19:39:33.792771: step 3863, loss 0.00669273, acc 1
2016-09-07T19:39:34.472576: step 3864, loss 0.00675648, acc 1
2016-09-07T19:39:35.141922: step 3865, loss 0.038009, acc 1
2016-09-07T19:39:35.820267: step 3866, loss 0.0376542, acc 1
2016-09-07T19:39:36.501768: step 3867, loss 0.0221794, acc 0.98
2016-09-07T19:39:37.180670: step 3868, loss 0.0219261, acc 1
2016-09-07T19:39:37.840308: step 3869, loss 0.0155606, acc 1
2016-09-07T19:39:38.487812: step 3870, loss 0.0428184, acc 0.96
2016-09-07T19:39:39.140869: step 3871, loss 0.0217112, acc 1
2016-09-07T19:39:39.797353: step 3872, loss 0.00222807, acc 1
2016-09-07T19:39:40.467795: step 3873, loss 0.0399676, acc 0.98
2016-09-07T19:39:41.133889: step 3874, loss 0.00340436, acc 1
2016-09-07T19:39:41.796716: step 3875, loss 0.045543, acc 0.98
2016-09-07T19:39:42.463323: step 3876, loss 0.0440998, acc 0.96
2016-09-07T19:39:43.172064: step 3877, loss 0.0224913, acc 0.98
2016-09-07T19:39:43.817684: step 3878, loss 0.0313307, acc 0.98
2016-09-07T19:39:44.519096: step 3879, loss 0.0303826, acc 0.98
2016-09-07T19:39:44.878975: step 3880, loss 0.00445927, acc 1
2016-09-07T19:39:45.544204: step 3881, loss 0.0241481, acc 1
2016-09-07T19:39:46.220352: step 3882, loss 0.0369333, acc 0.98
2016-09-07T19:39:46.893441: step 3883, loss 0.0259186, acc 1
2016-09-07T19:39:47.548157: step 3884, loss 0.138852, acc 0.96
2016-09-07T19:39:48.227805: step 3885, loss 0.0980747, acc 0.98
2016-09-07T19:39:48.905637: step 3886, loss 0.0125943, acc 1
2016-09-07T19:39:49.575739: step 3887, loss 0.0257165, acc 0.98
2016-09-07T19:39:50.261257: step 3888, loss 0.0588105, acc 0.98
2016-09-07T19:39:50.941442: step 3889, loss 0.0233759, acc 0.98
2016-09-07T19:39:51.604997: step 3890, loss 0.0384016, acc 0.98
2016-09-07T19:39:52.267468: step 3891, loss 0.00256937, acc 1
2016-09-07T19:39:52.923857: step 3892, loss 0.0430731, acc 0.98
2016-09-07T19:39:53.573319: step 3893, loss 0.16096, acc 0.96
2016-09-07T19:39:54.238884: step 3894, loss 0.0245735, acc 1
2016-09-07T19:39:54.902302: step 3895, loss 0.0070212, acc 1
2016-09-07T19:39:55.571622: step 3896, loss 0.027028, acc 0.98
2016-09-07T19:39:56.255020: step 3897, loss 0.0704473, acc 0.98
2016-09-07T19:39:56.931105: step 3898, loss 0.00140882, acc 1
2016-09-07T19:39:57.627207: step 3899, loss 0.022225, acc 1
2016-09-07T19:39:58.296500: step 3900, loss 0.0197362, acc 0.98

Evaluation:
2016-09-07T19:40:01.599597: step 3900, loss 1.91104, acc 0.744

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-3900

2016-09-07T19:40:03.254648: step 3901, loss 0.0141202, acc 1
2016-09-07T19:40:03.926218: step 3902, loss 0.0320729, acc 0.98
2016-09-07T19:40:04.591037: step 3903, loss 0.0498871, acc 0.98
2016-09-07T19:40:05.250999: step 3904, loss 0.000871252, acc 1
2016-09-07T19:40:05.930404: step 3905, loss 0.047805, acc 1
2016-09-07T19:40:06.598792: step 3906, loss 0.00571484, acc 1
2016-09-07T19:40:07.266668: step 3907, loss 0.00204774, acc 1
2016-09-07T19:40:07.941321: step 3908, loss 0.04934, acc 0.98
2016-09-07T19:40:08.600056: step 3909, loss 0.0382908, acc 0.98
2016-09-07T19:40:09.276923: step 3910, loss 0.0307925, acc 1
2016-09-07T19:40:09.943922: step 3911, loss 0.00884162, acc 1
2016-09-07T19:40:10.633222: step 3912, loss 0.0826763, acc 0.98
2016-09-07T19:40:11.301207: step 3913, loss 0.0154035, acc 1
2016-09-07T19:40:11.962261: step 3914, loss 0.056069, acc 0.98
2016-09-07T19:40:12.622457: step 3915, loss 0.0235828, acc 0.98
2016-09-07T19:40:13.303779: step 3916, loss 0.0449041, acc 0.98
2016-09-07T19:40:13.967078: step 3917, loss 0.00188896, acc 1
2016-09-07T19:40:14.629028: step 3918, loss 0.0494072, acc 0.98
2016-09-07T19:40:15.296969: step 3919, loss 0.0020765, acc 1
2016-09-07T19:40:15.945841: step 3920, loss 0.0100322, acc 1
2016-09-07T19:40:16.645831: step 3921, loss 0.0336894, acc 1
2016-09-07T19:40:17.303002: step 3922, loss 0.10771, acc 0.96
2016-09-07T19:40:17.991994: step 3923, loss 0.00635841, acc 1
2016-09-07T19:40:18.659434: step 3924, loss 0.0969135, acc 0.98
2016-09-07T19:40:19.324744: step 3925, loss 0.0241605, acc 1
2016-09-07T19:40:19.990717: step 3926, loss 0.0390443, acc 0.98
2016-09-07T19:40:20.659393: step 3927, loss 0.0353105, acc 0.98
2016-09-07T19:40:21.343313: step 3928, loss 0.00165558, acc 1
2016-09-07T19:40:22.022800: step 3929, loss 0.0204921, acc 0.98
2016-09-07T19:40:22.700387: step 3930, loss 0.0612228, acc 0.96
2016-09-07T19:40:23.355963: step 3931, loss 0.0397415, acc 0.98
2016-09-07T19:40:24.013587: step 3932, loss 0.0446907, acc 0.98
2016-09-07T19:40:24.672791: step 3933, loss 0.00707466, acc 1
2016-09-07T19:40:25.379795: step 3934, loss 0.0305739, acc 0.98
2016-09-07T19:40:26.049783: step 3935, loss 0.0475848, acc 1
2016-09-07T19:40:26.722661: step 3936, loss 0.0458838, acc 1
2016-09-07T19:40:27.408548: step 3937, loss 0.0313008, acc 0.98
2016-09-07T19:40:28.073748: step 3938, loss 0.0107983, acc 1
2016-09-07T19:40:28.723879: step 3939, loss 0.193788, acc 0.96
2016-09-07T19:40:29.394581: step 3940, loss 0.00069013, acc 1
2016-09-07T19:40:30.064964: step 3941, loss 0.0393756, acc 1
2016-09-07T19:40:30.743951: step 3942, loss 0.00806234, acc 1
2016-09-07T19:40:31.408349: step 3943, loss 0.0295694, acc 1
2016-09-07T19:40:32.065632: step 3944, loss 0.0777554, acc 0.96
2016-09-07T19:40:32.709882: step 3945, loss 0.0637009, acc 0.98
2016-09-07T19:40:33.380901: step 3946, loss 0.0611988, acc 0.98
2016-09-07T19:40:34.065044: step 3947, loss 0.0266969, acc 1
2016-09-07T19:40:34.715392: step 3948, loss 0.0317824, acc 0.98
2016-09-07T19:40:35.389213: step 3949, loss 0.0497466, acc 0.98
2016-09-07T19:40:36.060165: step 3950, loss 0.0472133, acc 0.98
2016-09-07T19:40:36.735429: step 3951, loss 0.0242063, acc 0.98
2016-09-07T19:40:37.424621: step 3952, loss 0.0521746, acc 0.96
2016-09-07T19:40:38.114250: step 3953, loss 0.00790388, acc 1
2016-09-07T19:40:38.794847: step 3954, loss 0.00213981, acc 1
2016-09-07T19:40:39.467604: step 3955, loss 0.0199608, acc 1
2016-09-07T19:40:40.161017: step 3956, loss 0.0661819, acc 0.96
2016-09-07T19:40:40.828312: step 3957, loss 0.0138269, acc 1
2016-09-07T19:40:41.530538: step 3958, loss 0.0223094, acc 0.98
2016-09-07T19:40:42.198654: step 3959, loss 0.0619925, acc 0.96
2016-09-07T19:40:42.859519: step 3960, loss 0.00396039, acc 1
2016-09-07T19:40:43.549352: step 3961, loss 0.0718957, acc 0.96
2016-09-07T19:40:44.217263: step 3962, loss 0.000972807, acc 1
2016-09-07T19:40:44.885293: step 3963, loss 0.0014826, acc 1
2016-09-07T19:40:45.577196: step 3964, loss 0.0322641, acc 0.98
2016-09-07T19:40:46.261024: step 3965, loss 0.112303, acc 0.98
2016-09-07T19:40:46.948460: step 3966, loss 0.07606, acc 0.94
2016-09-07T19:40:47.656376: step 3967, loss 0.0180386, acc 1
2016-09-07T19:40:48.357090: step 3968, loss 0.0262533, acc 0.98
2016-09-07T19:40:49.023726: step 3969, loss 0.0349454, acc 1
2016-09-07T19:40:49.701940: step 3970, loss 0.076557, acc 0.96
2016-09-07T19:40:50.402425: step 3971, loss 0.0607972, acc 0.98
2016-09-07T19:40:51.069016: step 3972, loss 0.0468561, acc 0.98
2016-09-07T19:40:51.741852: step 3973, loss 0.00401584, acc 1
2016-09-07T19:40:52.409641: step 3974, loss 0.0484162, acc 0.98
2016-09-07T19:40:53.062519: step 3975, loss 0.0245007, acc 0.98
2016-09-07T19:40:53.730131: step 3976, loss 0.0463269, acc 0.96
2016-09-07T19:40:54.395998: step 3977, loss 0.00464571, acc 1
2016-09-07T19:40:55.077995: step 3978, loss 0.156587, acc 0.96
2016-09-07T19:40:55.755435: step 3979, loss 0.0436124, acc 0.98
2016-09-07T19:40:56.452909: step 3980, loss 0.0483589, acc 0.98
2016-09-07T19:40:57.116709: step 3981, loss 0.0177361, acc 1
2016-09-07T19:40:57.787783: step 3982, loss 0.0149658, acc 1
2016-09-07T19:40:58.457791: step 3983, loss 0.0301712, acc 0.98
2016-09-07T19:40:59.148579: step 3984, loss 0.0109058, acc 1
2016-09-07T19:40:59.812100: step 3985, loss 0.0283353, acc 0.98
2016-09-07T19:41:00.515060: step 3986, loss 0.0368189, acc 0.98
2016-09-07T19:41:01.182945: step 3987, loss 0.0366246, acc 0.98
2016-09-07T19:41:01.843750: step 3988, loss 0.0860105, acc 0.94
2016-09-07T19:41:02.510767: step 3989, loss 0.0390135, acc 1
2016-09-07T19:41:03.187442: step 3990, loss 0.0615808, acc 0.98
2016-09-07T19:41:03.876141: step 3991, loss 0.100297, acc 0.96
2016-09-07T19:41:04.552964: step 3992, loss 0.00644656, acc 1
2016-09-07T19:41:05.214795: step 3993, loss 0.00863913, acc 1
2016-09-07T19:41:05.907731: step 3994, loss 0.102728, acc 0.94
2016-09-07T19:41:06.593983: step 3995, loss 0.00769148, acc 1
2016-09-07T19:41:07.269874: step 3996, loss 0.118712, acc 0.96
2016-09-07T19:41:07.946688: step 3997, loss 0.0192279, acc 0.98
2016-09-07T19:41:08.625287: step 3998, loss 0.00423716, acc 1
2016-09-07T19:41:09.283933: step 3999, loss 0.0867509, acc 0.94
2016-09-07T19:41:09.969920: step 4000, loss 0.0751821, acc 0.96

Evaluation:
2016-09-07T19:41:13.254301: step 4000, loss 1.77859, acc 0.734

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-4000

2016-09-07T19:41:14.873034: step 4001, loss 0.019122, acc 0.98
2016-09-07T19:41:15.555252: step 4002, loss 0.0999559, acc 0.92
2016-09-07T19:41:16.227638: step 4003, loss 0.0552565, acc 0.96
2016-09-07T19:41:16.935252: step 4004, loss 0.0222937, acc 1
2016-09-07T19:41:17.607323: step 4005, loss 0.0763909, acc 0.96
2016-09-07T19:41:18.296203: step 4006, loss 0.00959475, acc 1
2016-09-07T19:41:18.975976: step 4007, loss 0.0215283, acc 0.98
2016-09-07T19:41:19.652560: step 4008, loss 0.0121051, acc 1
2016-09-07T19:41:20.324163: step 4009, loss 0.279216, acc 0.98
2016-09-07T19:41:20.996185: step 4010, loss 0.0509821, acc 0.96
2016-09-07T19:41:21.697763: step 4011, loss 0.141591, acc 0.94
2016-09-07T19:41:22.386946: step 4012, loss 0.114166, acc 0.94
2016-09-07T19:41:23.075121: step 4013, loss 0.00193995, acc 1
2016-09-07T19:41:23.769979: step 4014, loss 0.0688339, acc 0.96
2016-09-07T19:41:24.452362: step 4015, loss 0.0469143, acc 0.98
2016-09-07T19:41:25.143822: step 4016, loss 0.0420715, acc 0.96
2016-09-07T19:41:25.820043: step 4017, loss 0.0130598, acc 1
2016-09-07T19:41:26.478433: step 4018, loss 0.00172709, acc 1
2016-09-07T19:41:27.180766: step 4019, loss 0.0305659, acc 0.98
2016-09-07T19:41:27.857908: step 4020, loss 0.0306075, acc 1
2016-09-07T19:41:28.530862: step 4021, loss 0.0671658, acc 0.98
2016-09-07T19:41:29.215715: step 4022, loss 0.0532407, acc 0.96
2016-09-07T19:41:29.892141: step 4023, loss 0.0229315, acc 1
2016-09-07T19:41:30.583583: step 4024, loss 0.0132204, acc 1
2016-09-07T19:41:31.277588: step 4025, loss 0.0211597, acc 1
2016-09-07T19:41:31.961020: step 4026, loss 0.00956665, acc 1
2016-09-07T19:41:32.651100: step 4027, loss 0.0315206, acc 0.98
2016-09-07T19:41:33.332717: step 4028, loss 0.034458, acc 1
2016-09-07T19:41:34.010917: step 4029, loss 0.0677269, acc 0.98
2016-09-07T19:41:34.678125: step 4030, loss 0.00826717, acc 1
2016-09-07T19:41:35.376473: step 4031, loss 0.0213569, acc 1
2016-09-07T19:41:36.041934: step 4032, loss 0.100282, acc 0.96
2016-09-07T19:41:36.703382: step 4033, loss 0.0877412, acc 0.98
2016-09-07T19:41:37.366398: step 4034, loss 0.031179, acc 0.98
2016-09-07T19:41:38.029658: step 4035, loss 0.042059, acc 0.98
2016-09-07T19:41:38.691337: step 4036, loss 0.0151777, acc 1
2016-09-07T19:41:39.371759: step 4037, loss 0.0964034, acc 0.96
2016-09-07T19:41:40.051329: step 4038, loss 0.0361641, acc 0.98
2016-09-07T19:41:40.731868: step 4039, loss 0.0430191, acc 0.98
2016-09-07T19:41:41.412385: step 4040, loss 0.0206573, acc 0.98
2016-09-07T19:41:42.066216: step 4041, loss 0.0511885, acc 0.98
2016-09-07T19:41:42.734956: step 4042, loss 0.0863937, acc 0.94
2016-09-07T19:41:43.412171: step 4043, loss 0.0064915, acc 1
2016-09-07T19:41:44.107045: step 4044, loss 0.0398891, acc 0.96
2016-09-07T19:41:44.765437: step 4045, loss 0.0122548, acc 1
2016-09-07T19:41:45.451423: step 4046, loss 0.0233818, acc 0.98
2016-09-07T19:41:46.117870: step 4047, loss 0.0137224, acc 1
2016-09-07T19:41:46.802821: step 4048, loss 0.0154475, acc 1
2016-09-07T19:41:47.478718: step 4049, loss 0.0476802, acc 0.96
2016-09-07T19:41:48.157319: step 4050, loss 0.0600584, acc 0.98
2016-09-07T19:41:48.835082: step 4051, loss 0.0249002, acc 0.98
2016-09-07T19:41:49.499690: step 4052, loss 0.0295799, acc 1
2016-09-07T19:41:50.165578: step 4053, loss 0.0626857, acc 0.98
2016-09-07T19:41:50.832501: step 4054, loss 0.0047808, acc 1
2016-09-07T19:41:51.509153: step 4055, loss 0.00132795, acc 1
2016-09-07T19:41:52.172183: step 4056, loss 0.0722541, acc 0.96
2016-09-07T19:41:52.828196: step 4057, loss 0.0120067, acc 1
2016-09-07T19:41:53.480385: step 4058, loss 0.0779496, acc 0.98
2016-09-07T19:41:54.163083: step 4059, loss 0.0121838, acc 1
2016-09-07T19:41:54.829572: step 4060, loss 0.104366, acc 0.96
2016-09-07T19:41:55.495657: step 4061, loss 0.0743924, acc 0.94
2016-09-07T19:41:56.167870: step 4062, loss 0.0169536, acc 0.98
2016-09-07T19:41:56.841520: step 4063, loss 0.0564769, acc 0.98
2016-09-07T19:41:57.520170: step 4064, loss 0.000860288, acc 1
2016-09-07T19:41:58.179829: step 4065, loss 0.00561662, acc 1
2016-09-07T19:41:58.863664: step 4066, loss 0.00199969, acc 1
2016-09-07T19:41:59.532757: step 4067, loss 0.0268006, acc 0.98
2016-09-07T19:42:00.244693: step 4068, loss 0.00502695, acc 1
2016-09-07T19:42:00.942219: step 4069, loss 0.00500211, acc 1
2016-09-07T19:42:01.615169: step 4070, loss 0.0514312, acc 0.96
2016-09-07T19:42:02.271836: step 4071, loss 0.0440204, acc 0.96
2016-09-07T19:42:02.931261: step 4072, loss 0.0157267, acc 1
2016-09-07T19:42:03.618944: step 4073, loss 0.0250323, acc 0.98
2016-09-07T19:42:03.980319: step 4074, loss 0.0924946, acc 0.916667
2016-09-07T19:42:04.679644: step 4075, loss 0.0426323, acc 0.98
2016-09-07T19:42:05.354255: step 4076, loss 0.0530048, acc 0.96
2016-09-07T19:42:06.012968: step 4077, loss 0.0243887, acc 0.98
2016-09-07T19:42:06.680856: step 4078, loss 0.026666, acc 0.98
2016-09-07T19:42:07.348411: step 4079, loss 0.0585933, acc 0.96
2016-09-07T19:42:08.039504: step 4080, loss 0.0637574, acc 0.96
2016-09-07T19:42:08.727992: step 4081, loss 0.0048556, acc 1
2016-09-07T19:42:09.394964: step 4082, loss 0.0316176, acc 0.98
2016-09-07T19:42:10.052453: step 4083, loss 0.0208251, acc 0.98
2016-09-07T19:42:10.727895: step 4084, loss 0.0429409, acc 0.96
2016-09-07T19:42:11.405733: step 4085, loss 0.0729687, acc 0.98
2016-09-07T19:42:12.075792: step 4086, loss 0.0515787, acc 0.96
2016-09-07T19:42:12.754376: step 4087, loss 0.0646432, acc 0.98
2016-09-07T19:42:13.419834: step 4088, loss 0.0253758, acc 0.98
2016-09-07T19:42:14.089686: step 4089, loss 0.0306295, acc 0.98
2016-09-07T19:42:14.763041: step 4090, loss 0.0026367, acc 1
2016-09-07T19:42:15.450040: step 4091, loss 0.0331714, acc 1
2016-09-07T19:42:16.111290: step 4092, loss 0.0815234, acc 0.96
2016-09-07T19:42:16.783100: step 4093, loss 0.0389998, acc 0.98
2016-09-07T19:42:17.456805: step 4094, loss 0.0172485, acc 0.98
2016-09-07T19:42:18.117915: step 4095, loss 0.0382061, acc 1
2016-09-07T19:42:18.794088: step 4096, loss 0.0212756, acc 1
2016-09-07T19:42:19.473359: step 4097, loss 0.0720497, acc 0.96
2016-09-07T19:42:20.163589: step 4098, loss 0.070056, acc 0.98
2016-09-07T19:42:20.829074: step 4099, loss 0.0252501, acc 1
2016-09-07T19:42:21.511089: step 4100, loss 0.0195142, acc 0.98

Evaluation:
2016-09-07T19:42:24.786983: step 4100, loss 2.29022, acc 0.736

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-4100

2016-09-07T19:42:26.463772: step 4101, loss 0.0161068, acc 1
2016-09-07T19:42:27.139161: step 4102, loss 0.00317505, acc 1
2016-09-07T19:42:27.823143: step 4103, loss 0.0455876, acc 0.98
2016-09-07T19:42:28.508999: step 4104, loss 0.0125231, acc 1
2016-09-07T19:42:29.182902: step 4105, loss 0.0136389, acc 1
2016-09-07T19:42:29.835662: step 4106, loss 0.0703549, acc 0.96
2016-09-07T19:42:30.509654: step 4107, loss 0.00636057, acc 1
2016-09-07T19:42:31.182044: step 4108, loss 0.00945276, acc 1
2016-09-07T19:42:31.871617: step 4109, loss 0.00183626, acc 1
2016-09-07T19:42:32.542773: step 4110, loss 0.0362391, acc 0.98
2016-09-07T19:42:33.213146: step 4111, loss 0.0275323, acc 0.98
2016-09-07T19:42:33.877545: step 4112, loss 0.0228372, acc 0.98
2016-09-07T19:42:34.548748: step 4113, loss 0.0266456, acc 0.98
2016-09-07T19:42:35.214340: step 4114, loss 0.296563, acc 0.94
2016-09-07T19:42:35.889255: step 4115, loss 0.0161164, acc 1
2016-09-07T19:42:36.562600: step 4116, loss 0.0454188, acc 0.98
2016-09-07T19:42:37.236101: step 4117, loss 3.09949e-05, acc 1
2016-09-07T19:42:37.901940: step 4118, loss 0.0120981, acc 1
2016-09-07T19:42:38.578336: step 4119, loss 0.0246038, acc 0.98
2016-09-07T19:42:39.258676: step 4120, loss 0.0465396, acc 0.98
2016-09-07T19:42:39.942914: step 4121, loss 0.0129375, acc 1
2016-09-07T19:42:40.616146: step 4122, loss 0.0524102, acc 0.98
2016-09-07T19:42:41.286634: step 4123, loss 0.0223176, acc 0.98
2016-09-07T19:42:41.960264: step 4124, loss 0.0159631, acc 1
2016-09-07T19:42:42.614335: step 4125, loss 0.0252814, acc 1
2016-09-07T19:42:43.302361: step 4126, loss 0.0159029, acc 1
2016-09-07T19:42:43.973951: step 4127, loss 0.000655719, acc 1
2016-09-07T19:42:44.650599: step 4128, loss 0.0450482, acc 0.98
2016-09-07T19:42:45.327908: step 4129, loss 0.0545192, acc 0.94
2016-09-07T19:42:46.020789: step 4130, loss 0.0535649, acc 0.98
2016-09-07T19:42:46.714153: step 4131, loss 0.0339424, acc 0.98
2016-09-07T19:42:47.393513: step 4132, loss 0.00655086, acc 1
2016-09-07T19:42:48.091372: step 4133, loss 0.0199734, acc 0.98
2016-09-07T19:42:48.775809: step 4134, loss 0.021951, acc 0.98
2016-09-07T19:42:49.441322: step 4135, loss 0.0260657, acc 0.98
2016-09-07T19:42:50.120727: step 4136, loss 0.0203029, acc 0.98
2016-09-07T19:42:50.786644: step 4137, loss 0.0346876, acc 0.98
2016-09-07T19:42:51.450542: step 4138, loss 0.00935647, acc 1
2016-09-07T19:42:52.118239: step 4139, loss 0.0464613, acc 0.96
2016-09-07T19:42:52.789072: step 4140, loss 0.0431917, acc 0.98
2016-09-07T19:42:53.471316: step 4141, loss 0.0106429, acc 1
2016-09-07T19:42:54.129670: step 4142, loss 0.00821664, acc 1
2016-09-07T19:42:54.786169: step 4143, loss 0.0636134, acc 0.98
2016-09-07T19:42:55.455240: step 4144, loss 0.0502599, acc 0.98
2016-09-07T19:42:56.135810: step 4145, loss 0.00747311, acc 1
2016-09-07T19:42:56.808460: step 4146, loss 0.0392107, acc 0.98
2016-09-07T19:42:57.496259: step 4147, loss 0.0237859, acc 0.98
2016-09-07T19:42:58.154101: step 4148, loss 0.0289783, acc 0.98
2016-09-07T19:42:58.822882: step 4149, loss 0.0284608, acc 0.98
2016-09-07T19:42:59.485456: step 4150, loss 0.0805867, acc 0.94
2016-09-07T19:43:00.147715: step 4151, loss 0.0305961, acc 0.98
2016-09-07T19:43:00.865077: step 4152, loss 0.00493602, acc 1
2016-09-07T19:43:01.540763: step 4153, loss 0.0334239, acc 1
2016-09-07T19:43:02.212839: step 4154, loss 0.0691505, acc 0.96
2016-09-07T19:43:02.911414: step 4155, loss 0.0615049, acc 0.98
2016-09-07T19:43:03.596524: step 4156, loss 0.00329363, acc 1
2016-09-07T19:43:04.277416: step 4157, loss 0.0245945, acc 1
2016-09-07T19:43:04.982725: step 4158, loss 0.00701053, acc 1
2016-09-07T19:43:05.657331: step 4159, loss 0.0460019, acc 0.98
2016-09-07T19:43:06.348510: step 4160, loss 0.00673341, acc 1
2016-09-07T19:43:07.017116: step 4161, loss 0.0223168, acc 0.98
2016-09-07T19:43:07.704878: step 4162, loss 0.024305, acc 0.98
2016-09-07T19:43:08.388656: step 4163, loss 0.00756265, acc 1
2016-09-07T19:43:09.078873: step 4164, loss 0.056197, acc 0.94
2016-09-07T19:43:09.761161: step 4165, loss 0.0203097, acc 1
2016-09-07T19:43:10.429715: step 4166, loss 0.074418, acc 0.96
2016-09-07T19:43:11.104011: step 4167, loss 0.0374004, acc 0.96
2016-09-07T19:43:11.766561: step 4168, loss 0.0237604, acc 0.98
2016-09-07T19:43:12.424293: step 4169, loss 0.0613195, acc 0.98
2016-09-07T19:43:13.091923: step 4170, loss 0.00432956, acc 1
2016-09-07T19:43:13.781619: step 4171, loss 0.000318472, acc 1
2016-09-07T19:43:14.455530: step 4172, loss 0.0545815, acc 0.96
2016-09-07T19:43:15.112051: step 4173, loss 0.0477547, acc 0.98
2016-09-07T19:43:15.779629: step 4174, loss 0.00697619, acc 1
2016-09-07T19:43:16.439978: step 4175, loss 0.00191445, acc 1
2016-09-07T19:43:17.132088: step 4176, loss 0.0350815, acc 0.96
2016-09-07T19:43:17.786322: step 4177, loss 0.0183735, acc 0.98
2016-09-07T19:43:18.468586: step 4178, loss 0.0609079, acc 0.98
2016-09-07T19:43:19.117714: step 4179, loss 0.0416357, acc 0.98
2016-09-07T19:43:19.803251: step 4180, loss 0.0400055, acc 0.98
2016-09-07T19:43:20.479065: step 4181, loss 0.0141171, acc 1
2016-09-07T19:43:21.150327: step 4182, loss 0.0367335, acc 0.98
2016-09-07T19:43:21.818280: step 4183, loss 0.00577846, acc 1
2016-09-07T19:43:22.514244: step 4184, loss 0.00947556, acc 1
2016-09-07T19:43:23.211412: step 4185, loss 0.00654319, acc 1
2016-09-07T19:43:23.901830: step 4186, loss 0.0117076, acc 1
2016-09-07T19:43:24.565456: step 4187, loss 0.0362139, acc 0.98
2016-09-07T19:43:25.241843: step 4188, loss 0.0198794, acc 1
2016-09-07T19:43:25.934576: step 4189, loss 0.00942101, acc 1
2016-09-07T19:43:26.602836: step 4190, loss 0.086195, acc 0.94
2016-09-07T19:43:27.263985: step 4191, loss 0.305853, acc 0.96
2016-09-07T19:43:27.936579: step 4192, loss 0.0146062, acc 1
2016-09-07T19:43:28.597770: step 4193, loss 0.00194497, acc 1
2016-09-07T19:43:29.259478: step 4194, loss 0.0291194, acc 1
2016-09-07T19:43:29.941442: step 4195, loss 0.0162782, acc 0.98
2016-09-07T19:43:30.649008: step 4196, loss 0.00099668, acc 1
2016-09-07T19:43:31.320298: step 4197, loss 0.0149585, acc 1
2016-09-07T19:43:31.983046: step 4198, loss 0.0240964, acc 1
2016-09-07T19:43:32.649109: step 4199, loss 0.037135, acc 0.98
2016-09-07T19:43:33.340646: step 4200, loss 0.0140637, acc 1

Evaluation:
2016-09-07T19:43:36.631201: step 4200, loss 2.07469, acc 0.738

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-4200

2016-09-07T19:43:38.353443: step 4201, loss 0.116035, acc 0.98
2016-09-07T19:43:39.039007: step 4202, loss 0.00690568, acc 1
2016-09-07T19:43:39.724527: step 4203, loss 0.016677, acc 0.98
2016-09-07T19:43:40.393136: step 4204, loss 0.147832, acc 0.98
2016-09-07T19:43:41.065250: step 4205, loss 0.0528399, acc 0.96
2016-09-07T19:43:41.721250: step 4206, loss 0.0314218, acc 1
2016-09-07T19:43:42.389330: step 4207, loss 0.0116207, acc 1
2016-09-07T19:43:43.071592: step 4208, loss 0.0259982, acc 1
2016-09-07T19:43:43.733750: step 4209, loss 0.0635871, acc 0.98
2016-09-07T19:43:44.401350: step 4210, loss 0.17817, acc 0.94
2016-09-07T19:43:45.052723: step 4211, loss 0.025694, acc 1
2016-09-07T19:43:45.727968: step 4212, loss 0.00633588, acc 1
2016-09-07T19:43:46.417757: step 4213, loss 0.0119177, acc 1
2016-09-07T19:43:47.088626: step 4214, loss 0.0124147, acc 1
2016-09-07T19:43:47.750961: step 4215, loss 0.000859413, acc 1
2016-09-07T19:43:48.411881: step 4216, loss 0.0198241, acc 0.98
2016-09-07T19:43:49.079467: step 4217, loss 0.100386, acc 0.96
2016-09-07T19:43:49.751903: step 4218, loss 0.03055, acc 0.98
2016-09-07T19:43:50.415625: step 4219, loss 0.00351476, acc 1
2016-09-07T19:43:51.075085: step 4220, loss 0.000643912, acc 1
2016-09-07T19:43:51.763990: step 4221, loss 0.0382079, acc 0.98
2016-09-07T19:43:52.429321: step 4222, loss 0.0618425, acc 0.98
2016-09-07T19:43:53.084813: step 4223, loss 0.0651491, acc 0.96
2016-09-07T19:43:53.751126: step 4224, loss 0.00213225, acc 1
2016-09-07T19:43:54.438443: step 4225, loss 0.086189, acc 0.96
2016-09-07T19:43:55.115629: step 4226, loss 0.0453511, acc 0.96
2016-09-07T19:43:55.770250: step 4227, loss 0.0287686, acc 1
2016-09-07T19:43:56.436424: step 4228, loss 0.0824973, acc 0.94
2016-09-07T19:43:57.099270: step 4229, loss 0.0156344, acc 1
2016-09-07T19:43:57.763395: step 4230, loss 0.00220065, acc 1
2016-09-07T19:43:58.417840: step 4231, loss 0.0101671, acc 1
2016-09-07T19:43:59.100463: step 4232, loss 0.0321489, acc 0.98
2016-09-07T19:43:59.756849: step 4233, loss 0.0035903, acc 1
2016-09-07T19:44:00.470464: step 4234, loss 0.0212014, acc 1
2016-09-07T19:44:01.158185: step 4235, loss 0.025351, acc 1
2016-09-07T19:44:01.827682: step 4236, loss 0.0380896, acc 0.96
2016-09-07T19:44:02.517601: step 4237, loss 0.0163181, acc 1
2016-09-07T19:44:03.216082: step 4238, loss 0.0139425, acc 1
2016-09-07T19:44:03.894755: step 4239, loss 0.0321788, acc 1
2016-09-07T19:44:04.553675: step 4240, loss 0.0569173, acc 0.96
2016-09-07T19:44:05.234825: step 4241, loss 0.0319371, acc 0.98
2016-09-07T19:44:05.913724: step 4242, loss 0.0185947, acc 0.98
2016-09-07T19:44:06.598525: step 4243, loss 0.0390941, acc 0.98
2016-09-07T19:44:07.258056: step 4244, loss 0.175732, acc 0.94
2016-09-07T19:44:07.939490: step 4245, loss 0.0619169, acc 0.96
2016-09-07T19:44:08.622531: step 4246, loss 0.0273873, acc 1
2016-09-07T19:44:09.300281: step 4247, loss 0.0189318, acc 0.98
2016-09-07T19:44:09.963612: step 4248, loss 0.0262524, acc 0.98
2016-09-07T19:44:10.655889: step 4249, loss 0.109264, acc 0.96
2016-09-07T19:44:11.347734: step 4250, loss 0.0566363, acc 0.98
2016-09-07T19:44:12.033683: step 4251, loss 0.0395325, acc 0.96
2016-09-07T19:44:12.697310: step 4252, loss 0.0134389, acc 1
2016-09-07T19:44:13.376558: step 4253, loss 0.00867538, acc 1
2016-09-07T19:44:14.081206: step 4254, loss 0.0199045, acc 1
2016-09-07T19:44:14.766318: step 4255, loss 0.0115662, acc 1
2016-09-07T19:44:15.446396: step 4256, loss 0.00128899, acc 1
2016-09-07T19:44:16.125354: step 4257, loss 0.033327, acc 0.98
2016-09-07T19:44:16.802278: step 4258, loss 0.0476016, acc 0.96
2016-09-07T19:44:17.507021: step 4259, loss 0.0269079, acc 0.98
2016-09-07T19:44:18.183476: step 4260, loss 0.0267603, acc 0.98
2016-09-07T19:44:18.852059: step 4261, loss 0.0309393, acc 0.98
2016-09-07T19:44:19.538568: step 4262, loss 0.000693184, acc 1
2016-09-07T19:44:20.207606: step 4263, loss 0.0200008, acc 1
2016-09-07T19:44:20.863106: step 4264, loss 0.0250515, acc 0.98
2016-09-07T19:44:21.508926: step 4265, loss 0.189577, acc 0.96
2016-09-07T19:44:22.148920: step 4266, loss 0.0130319, acc 1
2016-09-07T19:44:22.817447: step 4267, loss 0.00589982, acc 1
2016-09-07T19:44:23.182500: step 4268, loss 0.00195514, acc 1
2016-09-07T19:44:23.874554: step 4269, loss 0.0250093, acc 0.98
2016-09-07T19:44:24.557390: step 4270, loss 0.0350201, acc 0.98
2016-09-07T19:44:25.227577: step 4271, loss 0.0652046, acc 0.94
2016-09-07T19:44:25.879183: step 4272, loss 0.00598537, acc 1
2016-09-07T19:44:26.529828: step 4273, loss 0.0468267, acc 0.96
2016-09-07T19:44:27.173297: step 4274, loss 0.0476384, acc 0.96
2016-09-07T19:44:27.859861: step 4275, loss 0.0489246, acc 0.96
2016-09-07T19:44:28.545141: step 4276, loss 0.0423792, acc 0.98
2016-09-07T19:44:29.227755: step 4277, loss 0.00500653, acc 1
2016-09-07T19:44:29.904822: step 4278, loss 0.051614, acc 0.96
2016-09-07T19:44:30.576535: step 4279, loss 0.0676149, acc 0.98
2016-09-07T19:44:31.246655: step 4280, loss 0.0785353, acc 0.98
2016-09-07T19:44:31.916389: step 4281, loss 0.0352686, acc 0.96
2016-09-07T19:44:32.588740: step 4282, loss 0.0117994, acc 1
2016-09-07T19:44:33.257223: step 4283, loss 0.0114926, acc 1
2016-09-07T19:44:33.928265: step 4284, loss 0.00723115, acc 1
2016-09-07T19:44:34.613199: step 4285, loss 0.0305974, acc 1
2016-09-07T19:44:35.287626: step 4286, loss 0.0127259, acc 1
2016-09-07T19:44:35.963666: step 4287, loss 0.0426876, acc 0.96
2016-09-07T19:44:36.639670: step 4288, loss 0.0243995, acc 0.98
2016-09-07T19:44:37.317738: step 4289, loss 0.0463639, acc 0.98
2016-09-07T19:44:37.992851: step 4290, loss 0.0434, acc 0.96
2016-09-07T19:44:38.692183: step 4291, loss 0.0472915, acc 0.98
2016-09-07T19:44:39.367009: step 4292, loss 0.0260699, acc 0.98
2016-09-07T19:44:40.039024: step 4293, loss 0.0813736, acc 0.96
2016-09-07T19:44:40.712036: step 4294, loss 0.0306157, acc 0.98
2016-09-07T19:44:41.376445: step 4295, loss 0.0357634, acc 0.98
2016-09-07T19:44:42.050302: step 4296, loss 0.0196549, acc 0.98
2016-09-07T19:44:42.726867: step 4297, loss 0.015341, acc 1
2016-09-07T19:44:43.407029: step 4298, loss 0.00452982, acc 1
2016-09-07T19:44:44.087239: step 4299, loss 0.00743297, acc 1
2016-09-07T19:44:44.758964: step 4300, loss 0.0606252, acc 0.96

Evaluation:
2016-09-07T19:44:48.035783: step 4300, loss 1.95185, acc 0.742

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-4300

2016-09-07T19:44:49.765358: step 4301, loss 0.0290902, acc 0.98
2016-09-07T19:44:50.431761: step 4302, loss 0.0216122, acc 1
2016-09-07T19:44:51.143251: step 4303, loss 0.00285224, acc 1
2016-09-07T19:44:51.822566: step 4304, loss 0.141709, acc 0.94
2016-09-07T19:44:52.514775: step 4305, loss 0.00209135, acc 1
2016-09-07T19:44:53.198477: step 4306, loss 0.0769601, acc 0.96
2016-09-07T19:44:53.880268: step 4307, loss 0.000828485, acc 1
2016-09-07T19:44:54.562327: step 4308, loss 0.000724219, acc 1
2016-09-07T19:44:55.240855: step 4309, loss 0.131151, acc 0.96
2016-09-07T19:44:55.909788: step 4310, loss 0.0292388, acc 0.98
2016-09-07T19:44:56.575752: step 4311, loss 0.0338878, acc 0.98
2016-09-07T19:44:57.247112: step 4312, loss 0.0179228, acc 1
2016-09-07T19:44:57.915250: step 4313, loss 0.00120352, acc 1
2016-09-07T19:44:58.587300: step 4314, loss 0.0511375, acc 0.96
2016-09-07T19:44:59.262643: step 4315, loss 0.00716107, acc 1
2016-09-07T19:44:59.943720: step 4316, loss 0.00699586, acc 1
2016-09-07T19:45:00.664818: step 4317, loss 0.0203841, acc 1
2016-09-07T19:45:01.331206: step 4318, loss 0.0393354, acc 0.98
2016-09-07T19:45:02.008944: step 4319, loss 0.0688864, acc 0.96
2016-09-07T19:45:02.677062: step 4320, loss 0.00229807, acc 1
2016-09-07T19:45:03.384544: step 4321, loss 0.0139437, acc 1
2016-09-07T19:45:04.067309: step 4322, loss 0.00064899, acc 1
2016-09-07T19:45:04.746757: step 4323, loss 0.0302279, acc 0.98
2016-09-07T19:45:05.414114: step 4324, loss 0.0240583, acc 0.98
2016-09-07T19:45:06.096245: step 4325, loss 0.0275231, acc 0.98
2016-09-07T19:45:06.767182: step 4326, loss 0.000142983, acc 1
2016-09-07T19:45:07.432524: step 4327, loss 0.155588, acc 0.94
2016-09-07T19:45:08.115965: step 4328, loss 0.0580708, acc 0.96
2016-09-07T19:45:08.783418: step 4329, loss 0.0463802, acc 0.98
2016-09-07T19:45:09.446553: step 4330, loss 0.0990052, acc 0.94
2016-09-07T19:45:10.139123: step 4331, loss 0.111207, acc 0.96
2016-09-07T19:45:10.806062: step 4332, loss 0.0651792, acc 0.98
2016-09-07T19:45:11.466200: step 4333, loss 0.0379724, acc 1
2016-09-07T19:45:12.149732: step 4334, loss 0.0745341, acc 0.94
2016-09-07T19:45:12.830044: step 4335, loss 0.0405207, acc 0.98
2016-09-07T19:45:13.495965: step 4336, loss 0.042739, acc 0.98
2016-09-07T19:45:14.164434: step 4337, loss 0.00818508, acc 1
2016-09-07T19:45:14.830567: step 4338, loss 0.048823, acc 0.98
2016-09-07T19:45:15.517079: step 4339, loss 0.0244492, acc 0.98
2016-09-07T19:45:16.188857: step 4340, loss 0.0177085, acc 0.98
2016-09-07T19:45:16.871733: step 4341, loss 9.60009e-05, acc 1
2016-09-07T19:45:17.537260: step 4342, loss 0.0235021, acc 0.98
2016-09-07T19:45:18.213724: step 4343, loss 0.1294, acc 0.98
2016-09-07T19:45:18.885886: step 4344, loss 0.0619657, acc 0.96
2016-09-07T19:45:19.572194: step 4345, loss 0.0552019, acc 0.98
2016-09-07T19:45:20.232525: step 4346, loss 0.0612259, acc 0.96
2016-09-07T19:45:20.919394: step 4347, loss 0.0240118, acc 1
2016-09-07T19:45:21.592819: step 4348, loss 0.0158929, acc 0.98
2016-09-07T19:45:22.266940: step 4349, loss 0.0435391, acc 0.98
2016-09-07T19:45:22.931178: step 4350, loss 0.0390023, acc 0.98
2016-09-07T19:45:23.607073: step 4351, loss 0.0313449, acc 0.98
2016-09-07T19:45:24.267912: step 4352, loss 0.0027702, acc 1
2016-09-07T19:45:24.944558: step 4353, loss 0.0179151, acc 0.98
2016-09-07T19:45:25.624852: step 4354, loss 0.0342318, acc 0.98
2016-09-07T19:45:26.316677: step 4355, loss 0.0157793, acc 1
2016-09-07T19:45:26.991050: step 4356, loss 0.0362383, acc 1
2016-09-07T19:45:27.674373: step 4357, loss 0.0269706, acc 1
2016-09-07T19:45:28.346963: step 4358, loss 0.040236, acc 0.98
2016-09-07T19:45:29.023552: step 4359, loss 0.0605273, acc 0.98
2016-09-07T19:45:29.718302: step 4360, loss 0.00633636, acc 1
2016-09-07T19:45:30.400807: step 4361, loss 0.0441206, acc 0.98
2016-09-07T19:45:31.074115: step 4362, loss 0.0252876, acc 1
2016-09-07T19:45:31.737662: step 4363, loss 0.0346658, acc 0.98
2016-09-07T19:45:32.401518: step 4364, loss 0.0548864, acc 0.96
2016-09-07T19:45:33.076597: step 4365, loss 0.00784667, acc 1
2016-09-07T19:45:33.751449: step 4366, loss 0.0301911, acc 1
2016-09-07T19:45:34.416976: step 4367, loss 0.0803902, acc 0.98
2016-09-07T19:45:35.092152: step 4368, loss 0.0118346, acc 1
2016-09-07T19:45:35.772549: step 4369, loss 0.00671393, acc 1
2016-09-07T19:45:36.464725: step 4370, loss 0.0135925, acc 1
2016-09-07T19:45:37.154355: step 4371, loss 0.0876245, acc 0.98
2016-09-07T19:45:37.816743: step 4372, loss 0.116439, acc 0.94
2016-09-07T19:45:38.472061: step 4373, loss 0.00787545, acc 1
2016-09-07T19:45:39.124194: step 4374, loss 0.0253723, acc 1
2016-09-07T19:45:39.799564: step 4375, loss 0.0430503, acc 0.98
2016-09-07T19:45:40.485129: step 4376, loss 0.0535977, acc 0.96
2016-09-07T19:45:41.159253: step 4377, loss 0.165141, acc 0.96
2016-09-07T19:45:41.849099: step 4378, loss 0.00857627, acc 1
2016-09-07T19:45:42.548559: step 4379, loss 0.0510249, acc 0.96
2016-09-07T19:45:43.248847: step 4380, loss 0.0024695, acc 1
2016-09-07T19:45:43.930337: step 4381, loss 0.00100755, acc 1
2016-09-07T19:45:44.601244: step 4382, loss 0.0327502, acc 0.98
2016-09-07T19:45:45.279029: step 4383, loss 0.0111176, acc 1
2016-09-07T19:45:45.954898: step 4384, loss 0.0532729, acc 0.96
2016-09-07T19:45:46.634934: step 4385, loss 0.0697646, acc 0.96
2016-09-07T19:45:47.336936: step 4386, loss 0.0725887, acc 0.96
2016-09-07T19:45:48.017279: step 4387, loss 0.0125606, acc 1
2016-09-07T19:45:48.688877: step 4388, loss 0.0294039, acc 1
2016-09-07T19:45:49.367214: step 4389, loss 0.0853584, acc 0.96
2016-09-07T19:45:50.037527: step 4390, loss 0.0633346, acc 0.94
2016-09-07T19:45:50.704707: step 4391, loss 0.0885203, acc 0.96
2016-09-07T19:45:51.373811: step 4392, loss 0.032694, acc 0.98
2016-09-07T19:45:52.028752: step 4393, loss 0.0692186, acc 0.96
2016-09-07T19:45:52.698182: step 4394, loss 0.0228905, acc 1
2016-09-07T19:45:53.362348: step 4395, loss 0.0339352, acc 1
2016-09-07T19:45:54.030495: step 4396, loss 0.0704348, acc 0.94
2016-09-07T19:45:54.690700: step 4397, loss 0.0685398, acc 0.98
2016-09-07T19:45:55.354301: step 4398, loss 0.0536764, acc 0.98
2016-09-07T19:45:56.008599: step 4399, loss 0.054469, acc 0.98
2016-09-07T19:45:56.672147: step 4400, loss 0.126698, acc 0.96

Evaluation:
2016-09-07T19:45:59.896238: step 4400, loss 1.75158, acc 0.75

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-4400

2016-09-07T19:46:01.615932: step 4401, loss 0.0537296, acc 1
2016-09-07T19:46:02.286975: step 4402, loss 0.0296569, acc 1
2016-09-07T19:46:02.936014: step 4403, loss 0.170424, acc 0.96
2016-09-07T19:46:03.608194: step 4404, loss 0.0218943, acc 1
2016-09-07T19:46:04.262274: step 4405, loss 0.0205021, acc 0.98
2016-09-07T19:46:04.946301: step 4406, loss 0.00855332, acc 1
2016-09-07T19:46:05.623514: step 4407, loss 0.0296711, acc 0.98
2016-09-07T19:46:06.311594: step 4408, loss 0.0506581, acc 0.98
2016-09-07T19:46:06.985132: step 4409, loss 0.011394, acc 1
2016-09-07T19:46:07.667602: step 4410, loss 0.0287371, acc 1
2016-09-07T19:46:08.338844: step 4411, loss 0.0249682, acc 1
2016-09-07T19:46:09.019359: step 4412, loss 0.0193903, acc 1
2016-09-07T19:46:09.673881: step 4413, loss 0.0248037, acc 0.98
2016-09-07T19:46:10.348787: step 4414, loss 0.0153362, acc 1
2016-09-07T19:46:11.022317: step 4415, loss 0.0163261, acc 1
2016-09-07T19:46:11.700019: step 4416, loss 0.103043, acc 0.96
2016-09-07T19:46:12.371098: step 4417, loss 0.0142846, acc 1
2016-09-07T19:46:13.078931: step 4418, loss 0.0319859, acc 0.98
2016-09-07T19:46:13.752791: step 4419, loss 0.0209246, acc 0.98
2016-09-07T19:46:14.433856: step 4420, loss 0.0561408, acc 0.98
2016-09-07T19:46:15.132331: step 4421, loss 0.0400065, acc 0.98
2016-09-07T19:46:15.812541: step 4422, loss 0.0454396, acc 0.98
2016-09-07T19:46:16.485331: step 4423, loss 0.0171238, acc 1
2016-09-07T19:46:17.161695: step 4424, loss 0.017981, acc 1
2016-09-07T19:46:17.825981: step 4425, loss 0.0160808, acc 1
2016-09-07T19:46:18.506948: step 4426, loss 0.0266085, acc 0.98
2016-09-07T19:46:19.158198: step 4427, loss 0.0169381, acc 1
2016-09-07T19:46:19.818811: step 4428, loss 0.0291431, acc 0.98
2016-09-07T19:46:20.478907: step 4429, loss 0.0664023, acc 0.94
2016-09-07T19:46:21.135319: step 4430, loss 0.127502, acc 0.96
2016-09-07T19:46:21.817712: step 4431, loss 0.0113092, acc 1
2016-09-07T19:46:22.492300: step 4432, loss 0.0563783, acc 0.96
2016-09-07T19:46:23.167414: step 4433, loss 0.0111616, acc 1
2016-09-07T19:46:23.844199: step 4434, loss 0.0380188, acc 1
2016-09-07T19:46:24.529950: step 4435, loss 0.133863, acc 0.96
2016-09-07T19:46:25.199822: step 4436, loss 0.0164817, acc 0.98
2016-09-07T19:46:25.878584: step 4437, loss 0.0030326, acc 1
2016-09-07T19:46:26.538561: step 4438, loss 0.130259, acc 0.96
2016-09-07T19:46:27.208520: step 4439, loss 0.0269324, acc 1
2016-09-07T19:46:27.874984: step 4440, loss 0.0813366, acc 0.98
2016-09-07T19:46:28.538141: step 4441, loss 0.0750944, acc 0.96
2016-09-07T19:46:29.209959: step 4442, loss 0.000454991, acc 1
2016-09-07T19:46:29.885615: step 4443, loss 0.0318233, acc 0.98
2016-09-07T19:46:30.548532: step 4444, loss 0.0109089, acc 1
2016-09-07T19:46:31.228645: step 4445, loss 0.0351457, acc 0.98
2016-09-07T19:46:31.896024: step 4446, loss 0.0347555, acc 0.98
2016-09-07T19:46:32.574729: step 4447, loss 0.0607659, acc 0.96
2016-09-07T19:46:33.253638: step 4448, loss 0.0148969, acc 1
2016-09-07T19:46:33.964251: step 4449, loss 0.0228621, acc 0.98
2016-09-07T19:46:34.648722: step 4450, loss 0.0296849, acc 0.98
2016-09-07T19:46:35.340927: step 4451, loss 0.0123774, acc 1
2016-09-07T19:46:36.016793: step 4452, loss 0.0167161, acc 1
2016-09-07T19:46:36.705496: step 4453, loss 0.00876887, acc 1
2016-09-07T19:46:37.395132: step 4454, loss 0.00693305, acc 1
2016-09-07T19:46:38.098691: step 4455, loss 0.0180541, acc 1
2016-09-07T19:46:38.771777: step 4456, loss 0.0290842, acc 0.98
2016-09-07T19:46:39.449019: step 4457, loss 0.0626443, acc 0.96
2016-09-07T19:46:40.109360: step 4458, loss 0.0461778, acc 0.98
2016-09-07T19:46:40.783099: step 4459, loss 0.0657196, acc 0.94
2016-09-07T19:46:41.474437: step 4460, loss 0.00828913, acc 1
2016-09-07T19:46:42.152305: step 4461, loss 0.00752734, acc 1
2016-09-07T19:46:42.515725: step 4462, loss 0.0569413, acc 1
2016-09-07T19:46:43.198089: step 4463, loss 0.0330763, acc 0.98
2016-09-07T19:46:43.885717: step 4464, loss 0.0408933, acc 0.98
2016-09-07T19:46:44.547758: step 4465, loss 0.00484686, acc 1
2016-09-07T19:46:45.225901: step 4466, loss 0.0655239, acc 0.98
2016-09-07T19:46:45.900626: step 4467, loss 0.00836075, acc 1
2016-09-07T19:46:46.584533: step 4468, loss 0.0293215, acc 1
2016-09-07T19:46:47.260246: step 4469, loss 0.0131843, acc 1
2016-09-07T19:46:47.961449: step 4470, loss 0.0207315, acc 1
2016-09-07T19:46:48.639252: step 4471, loss 0.015527, acc 1
2016-09-07T19:46:49.308660: step 4472, loss 0.0115991, acc 1
2016-09-07T19:46:49.971408: step 4473, loss 0.0612563, acc 0.98
2016-09-07T19:46:50.654743: step 4474, loss 0.0286753, acc 0.98
2016-09-07T19:46:51.324762: step 4475, loss 0.111091, acc 0.92
2016-09-07T19:46:51.990108: step 4476, loss 0.0116662, acc 1
2016-09-07T19:46:52.645765: step 4477, loss 0.00918579, acc 1
2016-09-07T19:46:53.309490: step 4478, loss 0.0269326, acc 0.98
2016-09-07T19:46:53.990671: step 4479, loss 0.00926061, acc 1
2016-09-07T19:46:54.675562: step 4480, loss 0.0131375, acc 1
2016-09-07T19:46:55.346897: step 4481, loss 0.0301806, acc 0.98
2016-09-07T19:46:56.018877: step 4482, loss 0.00828935, acc 1
2016-09-07T19:46:56.682348: step 4483, loss 0.0515359, acc 0.96
2016-09-07T19:46:57.355465: step 4484, loss 0.0425476, acc 0.98
2016-09-07T19:46:58.005615: step 4485, loss 0.043093, acc 0.98
2016-09-07T19:46:58.673950: step 4486, loss 0.00470711, acc 1
2016-09-07T19:46:59.332109: step 4487, loss 0.0668349, acc 0.98
2016-09-07T19:47:00.042781: step 4488, loss 0.019802, acc 1
2016-09-07T19:47:00.749418: step 4489, loss 0.00273691, acc 1
2016-09-07T19:47:01.412082: step 4490, loss 0.0164484, acc 1
2016-09-07T19:47:02.077636: step 4491, loss 0.180319, acc 0.96
2016-09-07T19:47:02.748984: step 4492, loss 0.00706941, acc 1
2016-09-07T19:47:03.428607: step 4493, loss 0.00550382, acc 1
2016-09-07T19:47:04.083903: step 4494, loss 0.0251695, acc 0.98
2016-09-07T19:47:04.770856: step 4495, loss 0.0110419, acc 1
2016-09-07T19:47:05.458425: step 4496, loss 0.0483362, acc 0.98
2016-09-07T19:47:06.133827: step 4497, loss 0.026892, acc 1
2016-09-07T19:47:06.804369: step 4498, loss 0.308383, acc 0.98
2016-09-07T19:47:07.467012: step 4499, loss 0.00233078, acc 1
2016-09-07T19:47:08.146548: step 4500, loss 0.0563902, acc 0.96

Evaluation:
2016-09-07T19:47:11.434485: step 4500, loss 1.91218, acc 0.751

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-4500

2016-09-07T19:47:13.239441: step 4501, loss 0.0196029, acc 1
2016-09-07T19:47:13.899254: step 4502, loss 0.000122261, acc 1
2016-09-07T19:47:14.563181: step 4503, loss 0.0293985, acc 0.98
2016-09-07T19:47:15.253997: step 4504, loss 0.0229954, acc 0.98
2016-09-07T19:47:15.924037: step 4505, loss 0.0118374, acc 1
2016-09-07T19:47:16.586148: step 4506, loss 0.0277973, acc 0.98
2016-09-07T19:47:17.242246: step 4507, loss 0.0367688, acc 0.96
2016-09-07T19:47:17.901674: step 4508, loss 0.0201602, acc 1
2016-09-07T19:47:18.580289: step 4509, loss 0.0203814, acc 1
2016-09-07T19:47:19.245489: step 4510, loss 0.0212482, acc 1
2016-09-07T19:47:19.911946: step 4511, loss 0.00363891, acc 1
2016-09-07T19:47:20.613783: step 4512, loss 0.0771826, acc 0.96
2016-09-07T19:47:21.278927: step 4513, loss 0.104381, acc 0.96
2016-09-07T19:47:21.954680: step 4514, loss 0.0391956, acc 0.98
2016-09-07T19:47:22.639905: step 4515, loss 0.00812858, acc 1
2016-09-07T19:47:23.323038: step 4516, loss 0.0202291, acc 1
2016-09-07T19:47:23.976555: step 4517, loss 0.0101144, acc 1
2016-09-07T19:47:24.645940: step 4518, loss 0.0112093, acc 1
2016-09-07T19:47:25.335342: step 4519, loss 0.030518, acc 1
2016-09-07T19:47:26.018980: step 4520, loss 0.00351111, acc 1
2016-09-07T19:47:26.690127: step 4521, loss 0.176287, acc 0.94
2016-09-07T19:47:27.354236: step 4522, loss 0.0249168, acc 0.98
2016-09-07T19:47:28.018623: step 4523, loss 0.0118993, acc 1
2016-09-07T19:47:28.694221: step 4524, loss 0.0277287, acc 0.98
2016-09-07T19:47:29.371025: step 4525, loss 0.0399673, acc 0.98
2016-09-07T19:47:30.037767: step 4526, loss 0.0600569, acc 0.96
2016-09-07T19:47:30.713120: step 4527, loss 0.0208292, acc 1
2016-09-07T19:47:31.365449: step 4528, loss 0.120273, acc 0.94
2016-09-07T19:47:32.038092: step 4529, loss 0.0276393, acc 0.98
2016-09-07T19:47:32.719402: step 4530, loss 3.79402e-05, acc 1
2016-09-07T19:47:33.393477: step 4531, loss 0.0211737, acc 1
2016-09-07T19:47:34.061205: step 4532, loss 0.0934514, acc 0.96
2016-09-07T19:47:34.732180: step 4533, loss 0.0617142, acc 0.98
2016-09-07T19:47:35.413129: step 4534, loss 0.0209762, acc 0.98
2016-09-07T19:47:36.084647: step 4535, loss 0.00367764, acc 1
2016-09-07T19:47:36.767106: step 4536, loss 0.063147, acc 0.98
2016-09-07T19:47:37.446757: step 4537, loss 0.0340633, acc 0.98
2016-09-07T19:47:38.133295: step 4538, loss 0.0973809, acc 0.94
2016-09-07T19:47:38.800415: step 4539, loss 0.0282441, acc 0.98
2016-09-07T19:47:39.464603: step 4540, loss 0.0740534, acc 0.98
2016-09-07T19:47:40.137288: step 4541, loss 0.0682685, acc 0.98
2016-09-07T19:47:40.785632: step 4542, loss 0.0356522, acc 1
2016-09-07T19:47:41.457580: step 4543, loss 0.0394395, acc 0.98
2016-09-07T19:47:42.133257: step 4544, loss 0.0205593, acc 0.98
2016-09-07T19:47:42.794039: step 4545, loss 0.00069355, acc 1
2016-09-07T19:47:43.460337: step 4546, loss 0.027014, acc 0.98
2016-09-07T19:47:44.111468: step 4547, loss 0.0253689, acc 0.98
2016-09-07T19:47:44.788493: step 4548, loss 0.00522316, acc 1
2016-09-07T19:47:45.471999: step 4549, loss 0.0818884, acc 0.96
2016-09-07T19:47:46.141330: step 4550, loss 0.0653368, acc 0.94
2016-09-07T19:47:46.817419: step 4551, loss 0.159549, acc 0.98
2016-09-07T19:47:47.634330: step 4552, loss 0.0214824, acc 1
2016-09-07T19:47:48.317562: step 4553, loss 0.0229835, acc 0.98
2016-09-07T19:47:48.995418: step 4554, loss 0.0691969, acc 0.98
2016-09-07T19:47:49.663099: step 4555, loss 0.0664462, acc 0.98
2016-09-07T19:47:50.337943: step 4556, loss 0.0521186, acc 0.98
2016-09-07T19:47:51.020326: step 4557, loss 0.0459301, acc 1
2016-09-07T19:47:51.699318: step 4558, loss 0.0394151, acc 0.98
2016-09-07T19:47:52.374514: step 4559, loss 0.0312796, acc 0.98
2016-09-07T19:47:53.057490: step 4560, loss 0.04524, acc 1
2016-09-07T19:47:53.738399: step 4561, loss 0.033792, acc 1
2016-09-07T19:47:54.427318: step 4562, loss 0.0593467, acc 0.96
2016-09-07T19:47:55.104509: step 4563, loss 0.0580236, acc 0.96
2016-09-07T19:47:55.782599: step 4564, loss 0.0259348, acc 1
2016-09-07T19:47:56.440728: step 4565, loss 0.0257974, acc 1
2016-09-07T19:47:57.106060: step 4566, loss 0.030357, acc 0.98
2016-09-07T19:47:57.779774: step 4567, loss 0.0311061, acc 0.98
2016-09-07T19:47:58.497073: step 4568, loss 0.15218, acc 0.96
2016-09-07T19:47:59.171529: step 4569, loss 0.0209131, acc 0.98
2016-09-07T19:47:59.833361: step 4570, loss 0.0520089, acc 0.98
2016-09-07T19:48:00.564801: step 4571, loss 0.0176296, acc 1
2016-09-07T19:48:01.243429: step 4572, loss 0.0194555, acc 1
2016-09-07T19:48:01.919192: step 4573, loss 0.0406641, acc 0.98
2016-09-07T19:48:02.601095: step 4574, loss 0.0266826, acc 0.98
2016-09-07T19:48:03.293442: step 4575, loss 0.0235512, acc 1
2016-09-07T19:48:03.988457: step 4576, loss 0.036916, acc 0.98
2016-09-07T19:48:04.709999: step 4577, loss 0.0130881, acc 1
2016-09-07T19:48:05.398903: step 4578, loss 0.00552438, acc 1
2016-09-07T19:48:06.058211: step 4579, loss 0.0262987, acc 0.98
2016-09-07T19:48:06.736311: step 4580, loss 0.0374314, acc 1
2016-09-07T19:48:07.412329: step 4581, loss 0.0230197, acc 1
2016-09-07T19:48:08.088610: step 4582, loss 0.0949537, acc 0.92
2016-09-07T19:48:08.775233: step 4583, loss 0.0207635, acc 0.98
2016-09-07T19:48:09.446345: step 4584, loss 0.0300892, acc 0.98
2016-09-07T19:48:10.117489: step 4585, loss 7.56208e-05, acc 1
2016-09-07T19:48:10.787848: step 4586, loss 0.0133445, acc 1
2016-09-07T19:48:11.462323: step 4587, loss 0.154168, acc 0.94
2016-09-07T19:48:12.117092: step 4588, loss 0.0296015, acc 0.98
2016-09-07T19:48:12.804445: step 4589, loss 0.031036, acc 0.98
2016-09-07T19:48:13.483965: step 4590, loss 0.00853627, acc 1
2016-09-07T19:48:14.140190: step 4591, loss 0.002998, acc 1
2016-09-07T19:48:14.809956: step 4592, loss 0.0111327, acc 1
2016-09-07T19:48:15.490685: step 4593, loss 0.0275511, acc 0.98
2016-09-07T19:48:16.148836: step 4594, loss 0.0328247, acc 1
2016-09-07T19:48:16.819431: step 4595, loss 0.00164499, acc 1
2016-09-07T19:48:17.494105: step 4596, loss 0.0169295, acc 1
2016-09-07T19:48:18.192343: step 4597, loss 0.0344978, acc 0.98
2016-09-07T19:48:18.877657: step 4598, loss 0.0153738, acc 1
2016-09-07T19:48:19.539326: step 4599, loss 0.00342801, acc 1
2016-09-07T19:48:20.217407: step 4600, loss 0.0379882, acc 0.98

Evaluation:
2016-09-07T19:48:23.465847: step 4600, loss 2.00178, acc 0.757

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-4600

2016-09-07T19:48:25.161399: step 4601, loss 0.111799, acc 0.96
2016-09-07T19:48:25.829059: step 4602, loss 0.0651974, acc 1
2016-09-07T19:48:26.500005: step 4603, loss 0.0340511, acc 0.98
2016-09-07T19:48:27.169856: step 4604, loss 0.0142318, acc 1
2016-09-07T19:48:27.837106: step 4605, loss 0.0269404, acc 0.98
2016-09-07T19:48:28.508277: step 4606, loss 0.0757576, acc 0.98
2016-09-07T19:48:29.175333: step 4607, loss 0.0733601, acc 0.94
2016-09-07T19:48:29.836917: step 4608, loss 0.0475552, acc 0.96
2016-09-07T19:48:30.498305: step 4609, loss 0.00645449, acc 1
2016-09-07T19:48:31.176610: step 4610, loss 0.0536911, acc 0.98
2016-09-07T19:48:31.835740: step 4611, loss 0.00688229, acc 1
2016-09-07T19:48:32.516578: step 4612, loss 0.0195825, acc 1
2016-09-07T19:48:33.197152: step 4613, loss 0.0285932, acc 0.98
2016-09-07T19:48:33.875437: step 4614, loss 0.0259948, acc 1
2016-09-07T19:48:34.553907: step 4615, loss 0.0187346, acc 0.98
2016-09-07T19:48:35.212342: step 4616, loss 0.0402315, acc 0.98
2016-09-07T19:48:35.886129: step 4617, loss 0.117521, acc 0.94
2016-09-07T19:48:36.559134: step 4618, loss 0.00989534, acc 1
2016-09-07T19:48:37.241320: step 4619, loss 0.00231943, acc 1
2016-09-07T19:48:37.899176: step 4620, loss 0.141135, acc 0.98
2016-09-07T19:48:38.587919: step 4621, loss 0.0043904, acc 1
2016-09-07T19:48:39.258765: step 4622, loss 0.0225708, acc 1
2016-09-07T19:48:39.950166: step 4623, loss 0.0500696, acc 0.98
2016-09-07T19:48:40.631288: step 4624, loss 0.104802, acc 0.98
2016-09-07T19:48:41.304341: step 4625, loss 0.091146, acc 0.98
2016-09-07T19:48:41.973005: step 4626, loss 0.0779251, acc 0.98
2016-09-07T19:48:42.724676: step 4627, loss 0.0190929, acc 1
2016-09-07T19:48:43.377972: step 4628, loss 0.00693378, acc 1
2016-09-07T19:48:44.040583: step 4629, loss 0.0191053, acc 1
2016-09-07T19:48:44.708753: step 4630, loss 0.0448103, acc 0.96
2016-09-07T19:48:45.380367: step 4631, loss 0.0220162, acc 1
2016-09-07T19:48:46.061910: step 4632, loss 0.0318843, acc 0.98
2016-09-07T19:48:46.724885: step 4633, loss 0.00858454, acc 1
2016-09-07T19:48:47.390379: step 4634, loss 0.017266, acc 0.98
2016-09-07T19:48:48.064132: step 4635, loss 0.0293105, acc 0.98
2016-09-07T19:48:48.811295: step 4636, loss 0.0453252, acc 0.96
2016-09-07T19:48:49.486345: step 4637, loss 0.0785503, acc 0.96
2016-09-07T19:48:50.171983: step 4638, loss 0.0145145, acc 1
2016-09-07T19:48:50.827464: step 4639, loss 0.0321017, acc 0.98
2016-09-07T19:48:51.511455: step 4640, loss 0.0178181, acc 0.98
2016-09-07T19:48:52.180180: step 4641, loss 0.0656404, acc 0.98
2016-09-07T19:48:52.831351: step 4642, loss 0.00483894, acc 1
2016-09-07T19:48:53.511007: step 4643, loss 0.0642731, acc 0.94
2016-09-07T19:48:54.173629: step 4644, loss 0.0973102, acc 0.94
2016-09-07T19:48:54.838358: step 4645, loss 0.00201469, acc 1
2016-09-07T19:48:55.520205: step 4646, loss 0.0160492, acc 1
2016-09-07T19:48:56.186049: step 4647, loss 0.000253008, acc 1
2016-09-07T19:48:56.868005: step 4648, loss 0.0586963, acc 0.96
2016-09-07T19:48:57.539485: step 4649, loss 0.0249009, acc 1
2016-09-07T19:48:58.216735: step 4650, loss 0.0156439, acc 0.98
2016-09-07T19:48:58.894732: step 4651, loss 0.0139432, acc 1
2016-09-07T19:48:59.574032: step 4652, loss 0.0226859, acc 0.98
2016-09-07T19:49:00.256337: step 4653, loss 0.085077, acc 0.98
2016-09-07T19:49:00.931275: step 4654, loss 0.152979, acc 0.96
2016-09-07T19:49:01.595169: step 4655, loss 0.0184381, acc 1
2016-09-07T19:49:01.963216: step 4656, loss 0.013246, acc 1
2016-09-07T19:49:02.639335: step 4657, loss 0.0130329, acc 1
2016-09-07T19:49:03.318475: step 4658, loss 0.0817566, acc 0.96
2016-09-07T19:49:04.009320: step 4659, loss 0.0045292, acc 1
2016-09-07T19:49:04.684621: step 4660, loss 0.102786, acc 0.98
2016-09-07T19:49:05.379588: step 4661, loss 0.0260166, acc 1
2016-09-07T19:49:06.062615: step 4662, loss 0.0217868, acc 1
2016-09-07T19:49:06.723220: step 4663, loss 0.0867532, acc 0.98
2016-09-07T19:49:07.391177: step 4664, loss 0.00811088, acc 1
2016-09-07T19:49:08.059724: step 4665, loss 0.000891758, acc 1
2016-09-07T19:49:08.728248: step 4666, loss 0.0475829, acc 0.96
2016-09-07T19:49:09.404413: step 4667, loss 0.00288009, acc 1
2016-09-07T19:49:10.098717: step 4668, loss 0.0957576, acc 0.98
2016-09-07T19:49:10.767175: step 4669, loss 0.0528648, acc 0.98
2016-09-07T19:49:11.443664: step 4670, loss 0.0601053, acc 0.96
2016-09-07T19:49:12.121340: step 4671, loss 0.00392133, acc 1
2016-09-07T19:49:12.814783: step 4672, loss 0.0443628, acc 0.98
2016-09-07T19:49:13.478975: step 4673, loss 0.0228561, acc 1
2016-09-07T19:49:14.145744: step 4674, loss 0.014678, acc 1
2016-09-07T19:49:14.806046: step 4675, loss 0.0287333, acc 1
2016-09-07T19:49:15.485862: step 4676, loss 0.0356305, acc 1
2016-09-07T19:49:16.169681: step 4677, loss 0.021856, acc 0.98
2016-09-07T19:49:16.838896: step 4678, loss 0.0128364, acc 1
2016-09-07T19:49:17.534840: step 4679, loss 0.00811795, acc 1
2016-09-07T19:49:18.208676: step 4680, loss 0.0206976, acc 0.98
2016-09-07T19:49:18.890387: step 4681, loss 0.00850418, acc 1
2016-09-07T19:49:19.565188: step 4682, loss 0.0175792, acc 0.98
2016-09-07T19:49:20.246273: step 4683, loss 0.00744111, acc 1
2016-09-07T19:49:20.924323: step 4684, loss 0.0303742, acc 0.98
2016-09-07T19:49:21.587440: step 4685, loss 0.0644399, acc 0.96
2016-09-07T19:49:22.255460: step 4686, loss 0.00920994, acc 1
2016-09-07T19:49:22.923721: step 4687, loss 0.0694908, acc 0.96
2016-09-07T19:49:23.599112: step 4688, loss 0.0876561, acc 0.94
2016-09-07T19:49:24.279932: step 4689, loss 0.0190698, acc 1
2016-09-07T19:49:24.958035: step 4690, loss 0.0543245, acc 0.96
2016-09-07T19:49:25.740319: step 4691, loss 0.0044973, acc 1
2016-09-07T19:49:26.444517: step 4692, loss 0.0121349, acc 1
2016-09-07T19:49:27.114730: step 4693, loss 0.0444087, acc 0.98
2016-09-07T19:49:27.780808: step 4694, loss 0.0048928, acc 1
2016-09-07T19:49:28.450479: step 4695, loss 0.00154616, acc 1
2016-09-07T19:49:29.125495: step 4696, loss 0.000996632, acc 1
2016-09-07T19:49:29.820998: step 4697, loss 0.0271346, acc 0.98
2016-09-07T19:49:30.502207: step 4698, loss 0.00952249, acc 1
2016-09-07T19:49:31.172850: step 4699, loss 0.0118622, acc 1
2016-09-07T19:49:31.853905: step 4700, loss 0.0259496, acc 0.98

Evaluation:
2016-09-07T19:49:35.084727: step 4700, loss 2.24586, acc 0.747

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-4700

2016-09-07T19:49:36.775233: step 4701, loss 0.134589, acc 0.96
2016-09-07T19:49:37.468226: step 4702, loss 0.0128991, acc 1
2016-09-07T19:49:38.146315: step 4703, loss 0.0457418, acc 0.98
2016-09-07T19:49:38.827988: step 4704, loss 0.0309301, acc 0.98
2016-09-07T19:49:39.497805: step 4705, loss 0.00388365, acc 1
2016-09-07T19:49:40.157380: step 4706, loss 0.0429267, acc 0.96
2016-09-07T19:49:40.837023: step 4707, loss 0.0404887, acc 0.98
2016-09-07T19:49:41.577280: step 4708, loss 0.0329452, acc 0.98
2016-09-07T19:49:42.234667: step 4709, loss 0.0133582, acc 1
2016-09-07T19:49:42.908054: step 4710, loss 0.0300445, acc 0.98
2016-09-07T19:49:43.569250: step 4711, loss 0.00481171, acc 1
2016-09-07T19:49:44.241302: step 4712, loss 0.0150949, acc 1
2016-09-07T19:49:44.906242: step 4713, loss 0.164269, acc 0.94
2016-09-07T19:49:45.582931: step 4714, loss 0.000429667, acc 1
2016-09-07T19:49:46.254884: step 4715, loss 0.00794836, acc 1
2016-09-07T19:49:46.930922: step 4716, loss 0.02426, acc 0.98
2016-09-07T19:49:47.608219: step 4717, loss 0.0391817, acc 0.96
2016-09-07T19:49:48.293052: step 4718, loss 0.0211358, acc 0.98
2016-09-07T19:49:48.966481: step 4719, loss 0.0480199, acc 1
2016-09-07T19:49:49.646653: step 4720, loss 0.013497, acc 1
2016-09-07T19:49:50.304274: step 4721, loss 0.0583053, acc 0.94
2016-09-07T19:49:50.978038: step 4722, loss 0.00207936, acc 1
2016-09-07T19:49:51.648208: step 4723, loss 0.0385331, acc 0.98
2016-09-07T19:49:52.319723: step 4724, loss 0.0340997, acc 0.96
2016-09-07T19:49:52.984691: step 4725, loss 0.0153452, acc 0.98
2016-09-07T19:49:53.663182: step 4726, loss 0.0429875, acc 0.98
2016-09-07T19:49:54.323223: step 4727, loss 0.00987353, acc 1
2016-09-07T19:49:55.001346: step 4728, loss 0.0347583, acc 0.96
2016-09-07T19:49:55.687506: step 4729, loss 0.000441477, acc 1
2016-09-07T19:49:56.361032: step 4730, loss 0.00905313, acc 1
2016-09-07T19:49:57.043073: step 4731, loss 0.0172639, acc 0.98
2016-09-07T19:49:57.913896: step 4732, loss 0.0398877, acc 0.96
2016-09-07T19:49:58.604203: step 4733, loss 0.0724241, acc 0.98
2016-09-07T19:49:59.300532: step 4734, loss 0.00298402, acc 1
2016-09-07T19:49:59.960176: step 4735, loss 0.0344222, acc 0.98
2016-09-07T19:50:00.647005: step 4736, loss 0.0630432, acc 0.98
2016-09-07T19:50:01.324492: step 4737, loss 0.0111353, acc 1
2016-09-07T19:50:02.007318: step 4738, loss 0.0185083, acc 0.98
2016-09-07T19:50:02.676472: step 4739, loss 0.0647993, acc 0.94
2016-09-07T19:50:03.361772: step 4740, loss 0.000178432, acc 1
2016-09-07T19:50:04.043893: step 4741, loss 0.0247046, acc 0.98
2016-09-07T19:50:04.734724: step 4742, loss 8.08086e-05, acc 1
2016-09-07T19:50:05.409460: step 4743, loss 0.00385094, acc 1
2016-09-07T19:50:06.093222: step 4744, loss 0.0279642, acc 0.98
2016-09-07T19:50:06.768892: step 4745, loss 0.0352758, acc 0.98
2016-09-07T19:50:07.485897: step 4746, loss 0.0156505, acc 1
2016-09-07T19:50:08.161529: step 4747, loss 0.03096, acc 0.98
2016-09-07T19:50:08.838013: step 4748, loss 0.0201291, acc 0.98
2016-09-07T19:50:09.534906: step 4749, loss 0.070754, acc 0.98
2016-09-07T19:50:10.221738: step 4750, loss 0.00666932, acc 1
2016-09-07T19:50:10.905974: step 4751, loss 0.0807581, acc 0.96
2016-09-07T19:50:11.584948: step 4752, loss 0.00876994, acc 1
2016-09-07T19:50:12.261032: step 4753, loss 0.0280942, acc 1
2016-09-07T19:50:12.937821: step 4754, loss 0.0491454, acc 0.98
2016-09-07T19:50:13.595408: step 4755, loss 0.0102546, acc 1
2016-09-07T19:50:14.284345: step 4756, loss 0.0902437, acc 0.96
2016-09-07T19:50:14.969880: step 4757, loss 0.00064555, acc 1
2016-09-07T19:50:15.657544: step 4758, loss 0.0580386, acc 0.98
2016-09-07T19:50:16.343469: step 4759, loss 0.0340223, acc 0.96
2016-09-07T19:50:17.023039: step 4760, loss 0.0157019, acc 0.98
2016-09-07T19:50:17.682241: step 4761, loss 0.0046744, acc 1
2016-09-07T19:50:18.376916: step 4762, loss 0.0100599, acc 1
2016-09-07T19:50:19.042188: step 4763, loss 0.0491123, acc 0.98
2016-09-07T19:50:19.727844: step 4764, loss 0.0953116, acc 0.96
2016-09-07T19:50:20.407242: step 4765, loss 0.0166332, acc 0.98
2016-09-07T19:50:21.075385: step 4766, loss 0.0175414, acc 1
2016-09-07T19:50:21.770816: step 4767, loss 0.0839757, acc 0.96
2016-09-07T19:50:22.456477: step 4768, loss 0.00137979, acc 1
2016-09-07T19:50:23.142477: step 4769, loss 0.0224905, acc 1
2016-09-07T19:50:23.815244: step 4770, loss 0.0364963, acc 0.98
2016-09-07T19:50:24.493542: step 4771, loss 0.0126919, acc 1
2016-09-07T19:50:25.171921: step 4772, loss 0.0645519, acc 0.96
2016-09-07T19:50:25.844651: step 4773, loss 0.0372203, acc 0.96
2016-09-07T19:50:26.525937: step 4774, loss 0.0398636, acc 0.98
2016-09-07T19:50:27.200558: step 4775, loss 0.0235407, acc 0.98
2016-09-07T19:50:27.878967: step 4776, loss 0.0368848, acc 0.96
2016-09-07T19:50:28.549720: step 4777, loss 0.0521988, acc 0.96
2016-09-07T19:50:29.214559: step 4778, loss 0.00267851, acc 1
2016-09-07T19:50:29.901358: step 4779, loss 0.039263, acc 0.96
2016-09-07T19:50:30.585035: step 4780, loss 0.00380561, acc 1
2016-09-07T19:50:31.254623: step 4781, loss 0.0523319, acc 0.98
2016-09-07T19:50:31.929437: step 4782, loss 0.0266235, acc 1
2016-09-07T19:50:32.611555: step 4783, loss 0.0164614, acc 0.98
2016-09-07T19:50:33.279669: step 4784, loss 0.000798686, acc 1
2016-09-07T19:50:33.942069: step 4785, loss 0.0120555, acc 1
2016-09-07T19:50:34.672443: step 4786, loss 0.062138, acc 0.98
2016-09-07T19:50:35.448475: step 4787, loss 0.0355145, acc 0.96
2016-09-07T19:50:36.133465: step 4788, loss 0.143775, acc 0.9
2016-09-07T19:50:36.798442: step 4789, loss 0.0427062, acc 0.98
2016-09-07T19:50:37.473395: step 4790, loss 0.0327622, acc 1
2016-09-07T19:50:38.147487: step 4791, loss 0.0536971, acc 0.96
2016-09-07T19:50:38.812480: step 4792, loss 0.0107999, acc 1
2016-09-07T19:50:39.501580: step 4793, loss 0.0479368, acc 0.98
2016-09-07T19:50:40.193840: step 4794, loss 0.00387367, acc 1
2016-09-07T19:50:40.864396: step 4795, loss 0.00845266, acc 1
2016-09-07T19:50:41.556575: step 4796, loss 0.0704975, acc 0.96
2016-09-07T19:50:42.253977: step 4797, loss 0.0324316, acc 0.98
2016-09-07T19:50:42.935223: step 4798, loss 0.0167816, acc 0.98
2016-09-07T19:50:43.638030: step 4799, loss 0.0316017, acc 0.98
2016-09-07T19:50:44.333868: step 4800, loss 7.14196e-05, acc 1

Evaluation:
2016-09-07T19:50:47.651192: step 4800, loss 2.1238, acc 0.749

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-4800

2016-09-07T19:50:49.295581: step 4801, loss 0.09303, acc 0.96
2016-09-07T19:50:49.963140: step 4802, loss 0.0390631, acc 0.96
2016-09-07T19:50:50.631733: step 4803, loss 0.0158348, acc 0.98
2016-09-07T19:50:51.332685: step 4804, loss 0.0204372, acc 0.98
2016-09-07T19:50:52.005257: step 4805, loss 0.000808854, acc 1
2016-09-07T19:50:52.683444: step 4806, loss 0.0117029, acc 1
2016-09-07T19:50:53.334854: step 4807, loss 0.0490571, acc 0.96
2016-09-07T19:50:53.990208: step 4808, loss 0.0498766, acc 0.96
2016-09-07T19:50:54.649389: step 4809, loss 0.0636032, acc 0.94
2016-09-07T19:50:55.332186: step 4810, loss 0.0605995, acc 0.98
2016-09-07T19:50:56.026730: step 4811, loss 0.0283124, acc 0.98
2016-09-07T19:50:56.695432: step 4812, loss 0.00105228, acc 1
2016-09-07T19:50:57.352014: step 4813, loss 0.0196343, acc 1
2016-09-07T19:50:58.022276: step 4814, loss 0.101188, acc 0.98
2016-09-07T19:50:58.710207: step 4815, loss 0.0169493, acc 0.98
2016-09-07T19:50:59.375711: step 4816, loss 0.0104865, acc 1
2016-09-07T19:51:00.055592: step 4817, loss 0.00845498, acc 1
2016-09-07T19:51:00.753367: step 4818, loss 0.0245357, acc 1
2016-09-07T19:51:01.412755: step 4819, loss 0.0270645, acc 0.98
2016-09-07T19:51:02.085814: step 4820, loss 0.0231939, acc 1
2016-09-07T19:51:02.768737: step 4821, loss 0.00792994, acc 1
2016-09-07T19:51:03.441126: step 4822, loss 0.0453664, acc 0.98
2016-09-07T19:51:04.196875: step 4823, loss 0.056839, acc 0.98
2016-09-07T19:51:04.852377: step 4824, loss 0.0321122, acc 0.98
2016-09-07T19:51:05.517252: step 4825, loss 0.0260174, acc 0.98
2016-09-07T19:51:06.202137: step 4826, loss 0.0992349, acc 0.94
2016-09-07T19:51:06.885859: step 4827, loss 0.00325089, acc 1
2016-09-07T19:51:07.562031: step 4828, loss 0.0109205, acc 1
2016-09-07T19:51:08.232218: step 4829, loss 0.00565987, acc 1
2016-09-07T19:51:08.926509: step 4830, loss 0.000748569, acc 1
2016-09-07T19:51:09.604773: step 4831, loss 0.0934113, acc 0.96
2016-09-07T19:51:10.280305: step 4832, loss 0.0686992, acc 0.96
2016-09-07T19:51:10.950778: step 4833, loss 0.0132115, acc 1
2016-09-07T19:51:11.623561: step 4834, loss 0.0105053, acc 1
2016-09-07T19:51:12.280666: step 4835, loss 0.0391477, acc 0.96
2016-09-07T19:51:12.961417: step 4836, loss 0.0329463, acc 0.98
2016-09-07T19:51:13.627296: step 4837, loss 0.0384454, acc 1
2016-09-07T19:51:14.289683: step 4838, loss 0.00400789, acc 1
2016-09-07T19:51:14.959641: step 4839, loss 0.0215639, acc 0.98
2016-09-07T19:51:15.612696: step 4840, loss 0.0122068, acc 1
2016-09-07T19:51:16.287067: step 4841, loss 0.0128538, acc 1
2016-09-07T19:51:16.978770: step 4842, loss 0.0253503, acc 0.98
2016-09-07T19:51:17.656144: step 4843, loss 0.0391035, acc 0.98
2016-09-07T19:51:18.332373: step 4844, loss 0.00480592, acc 1
2016-09-07T19:51:18.976885: step 4845, loss 0.0631936, acc 0.98
2016-09-07T19:51:19.658536: step 4846, loss 0.0725049, acc 0.98
2016-09-07T19:51:20.321227: step 4847, loss 0.0202646, acc 0.98
2016-09-07T19:51:20.969038: step 4848, loss 0.0171117, acc 0.98
2016-09-07T19:51:21.650872: step 4849, loss 0.00894464, acc 1
2016-09-07T19:51:22.016152: step 4850, loss 0.050605, acc 1
2016-09-07T19:51:22.694205: step 4851, loss 0.0773937, acc 0.96
2016-09-07T19:51:23.352638: step 4852, loss 0.0333703, acc 1
2016-09-07T19:51:24.033901: step 4853, loss 0.0499783, acc 0.98
2016-09-07T19:51:24.711256: step 4854, loss 0.0326119, acc 0.98
2016-09-07T19:51:25.376017: step 4855, loss 0.0202237, acc 0.98
2016-09-07T19:51:26.058350: step 4856, loss 0.0494361, acc 0.98
2016-09-07T19:51:26.733330: step 4857, loss 0.0105838, acc 1
2016-09-07T19:51:27.391873: step 4858, loss 0.0331104, acc 0.98
2016-09-07T19:51:28.048740: step 4859, loss 0.01325, acc 1
2016-09-07T19:51:28.707060: step 4860, loss 0.0103285, acc 1
2016-09-07T19:51:29.376668: step 4861, loss 0.00746019, acc 1
2016-09-07T19:51:30.051940: step 4862, loss 0.0385948, acc 0.96
2016-09-07T19:51:30.717467: step 4863, loss 0.0509952, acc 0.96
2016-09-07T19:51:31.377701: step 4864, loss 0.0268909, acc 0.98
2016-09-07T19:51:32.058136: step 4865, loss 0.00370295, acc 1
2016-09-07T19:51:32.748731: step 4866, loss 0.0543416, acc 0.94
2016-09-07T19:51:33.418378: step 4867, loss 0.115377, acc 0.98
2016-09-07T19:51:34.096152: step 4868, loss 0.0043469, acc 1
2016-09-07T19:51:34.784870: step 4869, loss 0.0531022, acc 0.94
2016-09-07T19:51:35.465848: step 4870, loss 0.0125761, acc 1
2016-09-07T19:51:36.185980: step 4871, loss 0.0922883, acc 0.98
2016-09-07T19:51:36.853819: step 4872, loss 0.0239625, acc 1
2016-09-07T19:51:37.533973: step 4873, loss 0.0117803, acc 1
2016-09-07T19:51:38.214061: step 4874, loss 0.0478076, acc 0.98
2016-09-07T19:51:38.926229: step 4875, loss 0.0445595, acc 0.98
2016-09-07T19:51:39.664783: step 4876, loss 0.12362, acc 0.98
2016-09-07T19:51:40.325715: step 4877, loss 0.00045612, acc 1
2016-09-07T19:51:40.980155: step 4878, loss 0.0476965, acc 0.96
2016-09-07T19:51:41.688720: step 4879, loss 0.0150887, acc 1
2016-09-07T19:51:42.347985: step 4880, loss 0.0653261, acc 0.96
2016-09-07T19:51:43.015225: step 4881, loss 0.0225761, acc 1
2016-09-07T19:51:43.683301: step 4882, loss 0.0370287, acc 1
2016-09-07T19:51:44.370095: step 4883, loss 0.0353647, acc 0.98
2016-09-07T19:51:45.038959: step 4884, loss 0.0450883, acc 0.98
2016-09-07T19:51:45.713053: step 4885, loss 0.0433306, acc 0.96
2016-09-07T19:51:46.389580: step 4886, loss 0.0269485, acc 0.98
2016-09-07T19:51:47.060301: step 4887, loss 0.0142266, acc 1
2016-09-07T19:51:47.709495: step 4888, loss 0.0574213, acc 0.98
2016-09-07T19:51:48.384907: step 4889, loss 0.00293549, acc 1
2016-09-07T19:51:49.061356: step 4890, loss 0.0303112, acc 0.98
2016-09-07T19:51:49.748120: step 4891, loss 0.0225979, acc 1
2016-09-07T19:51:50.414995: step 4892, loss 0.00744061, acc 1
2016-09-07T19:51:51.082035: step 4893, loss 0.0429223, acc 0.98
2016-09-07T19:51:51.776656: step 4894, loss 0.00850294, acc 1
2016-09-07T19:51:52.480052: step 4895, loss 0.0351168, acc 0.98
2016-09-07T19:51:53.151960: step 4896, loss 0.0241101, acc 1
2016-09-07T19:51:53.818161: step 4897, loss 0.0275433, acc 0.98
2016-09-07T19:51:54.494875: step 4898, loss 0.0555995, acc 0.98
2016-09-07T19:51:55.167835: step 4899, loss 0.10595, acc 0.96
2016-09-07T19:51:55.850532: step 4900, loss 0.0341662, acc 0.98

Evaluation:
2016-09-07T19:51:59.045298: step 4900, loss 1.93147, acc 0.741

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-4900

2016-09-07T19:52:00.881407: step 4901, loss 0.0316589, acc 1
2016-09-07T19:52:01.578025: step 4902, loss 0.0169122, acc 1
2016-09-07T19:52:02.268378: step 4903, loss 0.0851797, acc 0.98
2016-09-07T19:52:02.952877: step 4904, loss 0.0181249, acc 1
2016-09-07T19:52:03.631674: step 4905, loss 0.0210195, acc 1
2016-09-07T19:52:04.296543: step 4906, loss 0.29307, acc 0.92
2016-09-07T19:52:04.972027: step 4907, loss 0.00915394, acc 1
2016-09-07T19:52:05.645110: step 4908, loss 0.000974959, acc 1
2016-09-07T19:52:06.314203: step 4909, loss 0.013364, acc 1
2016-09-07T19:52:06.981834: step 4910, loss 0.0217039, acc 1
2016-09-07T19:52:07.650157: step 4911, loss 0.0353627, acc 0.98
2016-09-07T19:52:08.326557: step 4912, loss 0.0185862, acc 0.98
2016-09-07T19:52:08.999374: step 4913, loss 0.0591767, acc 0.98
2016-09-07T19:52:09.660093: step 4914, loss 0.0264066, acc 1
2016-09-07T19:52:10.340057: step 4915, loss 0.0297285, acc 0.98
2016-09-07T19:52:11.011842: step 4916, loss 0.0389414, acc 0.98
2016-09-07T19:52:11.673615: step 4917, loss 0.0213611, acc 1
2016-09-07T19:52:12.350728: step 4918, loss 0.0692243, acc 0.96
2016-09-07T19:52:13.019516: step 4919, loss 0.0949895, acc 0.96
2016-09-07T19:52:13.692584: step 4920, loss 0.0353305, acc 0.98
2016-09-07T19:52:14.355491: step 4921, loss 0.0496793, acc 0.96
2016-09-07T19:52:15.017292: step 4922, loss 0.0391143, acc 0.98
2016-09-07T19:52:15.674971: step 4923, loss 0.000882033, acc 1
2016-09-07T19:52:16.331115: step 4924, loss 0.0308281, acc 1
2016-09-07T19:52:17.014557: step 4925, loss 0.0106354, acc 1
2016-09-07T19:52:17.680851: step 4926, loss 0.0356647, acc 1
2016-09-07T19:52:18.392812: step 4927, loss 0.0544871, acc 0.96
2016-09-07T19:52:19.181524: step 4928, loss 0.0197748, acc 0.98
2016-09-07T19:52:19.843286: step 4929, loss 0.00288689, acc 1
2016-09-07T19:52:20.533439: step 4930, loss 0.0221923, acc 0.98
2016-09-07T19:52:21.191416: step 4931, loss 0.0453105, acc 1
2016-09-07T19:52:21.892264: step 4932, loss 0.0290587, acc 0.98
2016-09-07T19:52:22.567680: step 4933, loss 0.0121816, acc 1
2016-09-07T19:52:23.225959: step 4934, loss 0.132789, acc 0.94
2016-09-07T19:52:23.964948: step 4935, loss 0.0330212, acc 0.98
2016-09-07T19:52:24.660300: step 4936, loss 0.0179248, acc 1
2016-09-07T19:52:25.344480: step 4937, loss 0.0215904, acc 0.98
2016-09-07T19:52:25.999773: step 4938, loss 0.0220069, acc 0.98
2016-09-07T19:52:26.679335: step 4939, loss 0.0173334, acc 1
2016-09-07T19:52:27.360772: step 4940, loss 0.0749064, acc 0.94
2016-09-07T19:52:28.032038: step 4941, loss 0.00206126, acc 1
2016-09-07T19:52:28.697847: step 4942, loss 0.00420165, acc 1
2016-09-07T19:52:29.363790: step 4943, loss 0.000832597, acc 1
2016-09-07T19:52:30.041550: step 4944, loss 0.0483462, acc 0.98
2016-09-07T19:52:30.719914: step 4945, loss 0.0152492, acc 0.98
2016-09-07T19:52:31.393629: step 4946, loss 0.0126281, acc 1
2016-09-07T19:52:32.084893: step 4947, loss 0.025664, acc 0.98
2016-09-07T19:52:32.780983: step 4948, loss 0.0424306, acc 0.98
2016-09-07T19:52:33.453568: step 4949, loss 0.0196862, acc 1
2016-09-07T19:52:34.135277: step 4950, loss 0.0435572, acc 0.98
2016-09-07T19:52:34.802997: step 4951, loss 0.00484767, acc 1
2016-09-07T19:52:35.489237: step 4952, loss 0.0456786, acc 0.98
2016-09-07T19:52:36.169846: step 4953, loss 0.0160563, acc 1
2016-09-07T19:52:36.855521: step 4954, loss 0.0123726, acc 1
2016-09-07T19:52:37.523189: step 4955, loss 0.0560106, acc 0.96
2016-09-07T19:52:38.204595: step 4956, loss 0.0525937, acc 0.98
2016-09-07T19:52:38.913720: step 4957, loss 0.051759, acc 0.98
2016-09-07T19:52:39.591674: step 4958, loss 0.0144787, acc 1
2016-09-07T19:52:40.253531: step 4959, loss 0.0129175, acc 1
2016-09-07T19:52:40.908104: step 4960, loss 0.00685946, acc 1
2016-09-07T19:52:41.576292: step 4961, loss 0.0839587, acc 0.96
2016-09-07T19:52:42.262244: step 4962, loss 0.000162791, acc 1
2016-09-07T19:52:42.917142: step 4963, loss 0.00141445, acc 1
2016-09-07T19:52:43.608395: step 4964, loss 0.0320457, acc 0.98
2016-09-07T19:52:44.282327: step 4965, loss 0.0992, acc 0.98
2016-09-07T19:52:44.954978: step 4966, loss 0.0197244, acc 1
2016-09-07T19:52:45.632299: step 4967, loss 0.0157439, acc 1
2016-09-07T19:52:46.308149: step 4968, loss 0.0292891, acc 0.98
2016-09-07T19:52:46.989364: step 4969, loss 0.00111579, acc 1
2016-09-07T19:52:47.668363: step 4970, loss 0.0484834, acc 0.98
2016-09-07T19:52:48.350239: step 4971, loss 0.041213, acc 0.98
2016-09-07T19:52:49.099710: step 4972, loss 0.0419891, acc 0.98
2016-09-07T19:52:49.768422: step 4973, loss 0.00306309, acc 1
2016-09-07T19:52:50.445705: step 4974, loss 0.00651193, acc 1
2016-09-07T19:52:51.102850: step 4975, loss 0.013165, acc 1
2016-09-07T19:52:51.777113: step 4976, loss 0.0482816, acc 0.98
2016-09-07T19:52:52.451959: step 4977, loss 0.0342419, acc 0.98
2016-09-07T19:52:53.135295: step 4978, loss 0.00440385, acc 1
2016-09-07T19:52:53.807628: step 4979, loss 0.000412146, acc 1
2016-09-07T19:52:54.490856: step 4980, loss 0.073053, acc 0.96
2016-09-07T19:52:55.207796: step 4981, loss 0.0680563, acc 0.94
2016-09-07T19:52:55.889360: step 4982, loss 0.0462665, acc 0.96
2016-09-07T19:52:56.557197: step 4983, loss 0.061634, acc 0.98
2016-09-07T19:52:57.229061: step 4984, loss 0.0118244, acc 1
2016-09-07T19:52:57.894346: step 4985, loss 0.00254852, acc 1
2016-09-07T19:52:58.567219: step 4986, loss 0.0055575, acc 1
2016-09-07T19:52:59.233794: step 4987, loss 0.0166316, acc 0.98
2016-09-07T19:52:59.905609: step 4988, loss 0.00524841, acc 1
2016-09-07T19:53:00.625587: step 4989, loss 0.0075046, acc 1
2016-09-07T19:53:01.304493: step 4990, loss 0.058261, acc 0.98
2016-09-07T19:53:01.967165: step 4991, loss 0.0142466, acc 1
2016-09-07T19:53:02.631110: step 4992, loss 0.0195793, acc 0.98
2016-09-07T19:53:03.302844: step 4993, loss 0.0120168, acc 1
2016-09-07T19:53:03.989667: step 4994, loss 0.0138346, acc 1
2016-09-07T19:53:04.678373: step 4995, loss 0.00972285, acc 1
2016-09-07T19:53:05.358031: step 4996, loss 0.0398014, acc 0.98
2016-09-07T19:53:06.045079: step 4997, loss 0.00394855, acc 1
2016-09-07T19:53:06.724181: step 4998, loss 0.0579445, acc 0.96
2016-09-07T19:53:07.400689: step 4999, loss 0.0483809, acc 0.98
2016-09-07T19:53:08.074290: step 5000, loss 9.28917e-05, acc 1

Evaluation:
2016-09-07T19:53:11.338750: step 5000, loss 2.20228, acc 0.754

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-5000

2016-09-07T19:53:13.138361: step 5001, loss 0.0185201, acc 0.98
2016-09-07T19:53:13.803433: step 5002, loss 0.0236123, acc 0.98
2016-09-07T19:53:14.465765: step 5003, loss 0.0416645, acc 0.96
2016-09-07T19:53:15.129417: step 5004, loss 0.0240408, acc 0.98
2016-09-07T19:53:15.796457: step 5005, loss 0.0191334, acc 1
2016-09-07T19:53:16.468597: step 5006, loss 0.00128036, acc 1
2016-09-07T19:53:17.149017: step 5007, loss 0.00769908, acc 1
2016-09-07T19:53:17.821659: step 5008, loss 0.0356925, acc 0.98
2016-09-07T19:53:18.487209: step 5009, loss 0.00654215, acc 1
2016-09-07T19:53:19.172427: step 5010, loss 0.0874152, acc 0.98
2016-09-07T19:53:19.833394: step 5011, loss 0.037736, acc 0.98
2016-09-07T19:53:20.513162: step 5012, loss 0.0500956, acc 0.98
2016-09-07T19:53:21.191806: step 5013, loss 0.0281452, acc 0.98
2016-09-07T19:53:21.858925: step 5014, loss 0.166453, acc 0.96
2016-09-07T19:53:22.523203: step 5015, loss 0.0129747, acc 1
2016-09-07T19:53:23.202214: step 5016, loss 0.0303305, acc 0.98
2016-09-07T19:53:23.870583: step 5017, loss 0.00880451, acc 1
2016-09-07T19:53:24.610261: step 5018, loss 0.0258617, acc 0.98
2016-09-07T19:53:25.280339: step 5019, loss 0.0520447, acc 0.96
2016-09-07T19:53:25.986819: step 5020, loss 0.0525486, acc 0.98
2016-09-07T19:53:26.649022: step 5021, loss 0.0195755, acc 0.98
2016-09-07T19:53:27.346057: step 5022, loss 0.0291276, acc 0.98
2016-09-07T19:53:28.019011: step 5023, loss 0.00561112, acc 1
2016-09-07T19:53:28.689292: step 5024, loss 0.0121196, acc 1
2016-09-07T19:53:29.403316: step 5025, loss 0.0382225, acc 0.98
2016-09-07T19:53:30.071386: step 5026, loss 0.0318573, acc 1
2016-09-07T19:53:30.750038: step 5027, loss 0.00295343, acc 1
2016-09-07T19:53:31.413810: step 5028, loss 0.000300672, acc 1
2016-09-07T19:53:32.085949: step 5029, loss 0.0253231, acc 1
2016-09-07T19:53:32.745058: step 5030, loss 0.0144169, acc 1
2016-09-07T19:53:33.414427: step 5031, loss 0.0139449, acc 1
2016-09-07T19:53:34.086580: step 5032, loss 0.0309007, acc 0.98
2016-09-07T19:53:34.757609: step 5033, loss 0.0157663, acc 0.98
2016-09-07T19:53:35.430919: step 5034, loss 0.0189816, acc 1
2016-09-07T19:53:36.113534: step 5035, loss 0.0190634, acc 1
2016-09-07T19:53:36.795514: step 5036, loss 0.0312957, acc 0.96
2016-09-07T19:53:37.487748: step 5037, loss 0.0724061, acc 0.96
2016-09-07T19:53:38.172296: step 5038, loss 0.0440675, acc 0.98
2016-09-07T19:53:38.857943: step 5039, loss 0.0373317, acc 0.98
2016-09-07T19:53:39.529601: step 5040, loss 0.00577519, acc 1
2016-09-07T19:53:40.191502: step 5041, loss 0.0238199, acc 0.98
2016-09-07T19:53:40.850284: step 5042, loss 0.110582, acc 0.96
2016-09-07T19:53:41.503757: step 5043, loss 0.0565663, acc 0.96
2016-09-07T19:53:41.870030: step 5044, loss 0.0285391, acc 1
2016-09-07T19:53:42.550538: step 5045, loss 0.0464167, acc 0.96
2016-09-07T19:53:43.243400: step 5046, loss 0.0204265, acc 1
2016-09-07T19:53:43.916768: step 5047, loss 0.0671989, acc 0.96
2016-09-07T19:53:44.594660: step 5048, loss 0.032869, acc 0.98
2016-09-07T19:53:45.267281: step 5049, loss 0.00124536, acc 1
2016-09-07T19:53:45.952215: step 5050, loss 0.0687179, acc 0.96
2016-09-07T19:53:46.635443: step 5051, loss 0.0145142, acc 0.98
2016-09-07T19:53:47.298082: step 5052, loss 0.0128717, acc 1
2016-09-07T19:53:47.963046: step 5053, loss 0.0211846, acc 1
2016-09-07T19:53:48.630184: step 5054, loss 0.0669327, acc 0.98
2016-09-07T19:53:49.302764: step 5055, loss 0.0282539, acc 0.98
2016-09-07T19:53:49.969280: step 5056, loss 0.0165372, acc 1
2016-09-07T19:53:50.635570: step 5057, loss 0.00854407, acc 1
2016-09-07T19:53:51.320016: step 5058, loss 0.021498, acc 1
2016-09-07T19:53:51.993308: step 5059, loss 0.0299283, acc 0.98
2016-09-07T19:53:52.664264: step 5060, loss 0.0138535, acc 1
2016-09-07T19:53:53.360549: step 5061, loss 0.0206018, acc 0.98
2016-09-07T19:53:54.045843: step 5062, loss 0.167444, acc 0.96
2016-09-07T19:53:54.705223: step 5063, loss 0.0324534, acc 0.98
2016-09-07T19:53:55.385489: step 5064, loss 0.0159319, acc 1
2016-09-07T19:53:56.051249: step 5065, loss 0.0117881, acc 1
2016-09-07T19:53:56.728720: step 5066, loss 0.0730153, acc 0.96
2016-09-07T19:53:57.415210: step 5067, loss 0.00582569, acc 1
2016-09-07T19:53:58.072431: step 5068, loss 0.00663011, acc 1
2016-09-07T19:53:58.761210: step 5069, loss 0.00285473, acc 1
2016-09-07T19:53:59.448369: step 5070, loss 0.000983241, acc 1
2016-09-07T19:54:00.119442: step 5071, loss 0.0323807, acc 0.98
2016-09-07T19:54:00.836567: step 5072, loss 0.0021034, acc 1
2016-09-07T19:54:01.524506: step 5073, loss 0.00733546, acc 1
2016-09-07T19:54:02.206718: step 5074, loss 0.0796304, acc 0.94
2016-09-07T19:54:02.897377: step 5075, loss 0.00297868, acc 1
2016-09-07T19:54:03.574590: step 5076, loss 0.0993711, acc 0.92
2016-09-07T19:54:04.251025: step 5077, loss 0.032265, acc 0.98
2016-09-07T19:54:04.930145: step 5078, loss 0.0379539, acc 1
2016-09-07T19:54:05.595425: step 5079, loss 0.00916543, acc 1
2016-09-07T19:54:06.272965: step 5080, loss 0.0225904, acc 1
2016-09-07T19:54:06.936774: step 5081, loss 0.00870446, acc 1
2016-09-07T19:54:07.608039: step 5082, loss 0.000239231, acc 1
2016-09-07T19:54:08.278223: step 5083, loss 0.00322935, acc 1
2016-09-07T19:54:08.948899: step 5084, loss 0.00321545, acc 1
2016-09-07T19:54:09.651377: step 5085, loss 0.0296811, acc 0.96
2016-09-07T19:54:10.336080: step 5086, loss 0.0580761, acc 0.98
2016-09-07T19:54:11.029028: step 5087, loss 0.000708486, acc 1
2016-09-07T19:54:11.700412: step 5088, loss 0.0276135, acc 0.98
2016-09-07T19:54:12.365671: step 5089, loss 0.0782968, acc 0.94
2016-09-07T19:54:13.035423: step 5090, loss 0.0221312, acc 1
2016-09-07T19:54:13.761127: step 5091, loss 0.0136588, acc 1
2016-09-07T19:54:14.513914: step 5092, loss 0.0500045, acc 0.98
2016-09-07T19:54:15.186346: step 5093, loss 0.0897752, acc 0.98
2016-09-07T19:54:15.867103: step 5094, loss 0.0319162, acc 1
2016-09-07T19:54:16.561453: step 5095, loss 0.00720455, acc 1
2016-09-07T19:54:17.232685: step 5096, loss 0.0624676, acc 0.96
2016-09-07T19:54:17.912388: step 5097, loss 0.0294662, acc 1
2016-09-07T19:54:18.576423: step 5098, loss 0.258457, acc 0.98
2016-09-07T19:54:19.258399: step 5099, loss 0.00332783, acc 1
2016-09-07T19:54:19.932089: step 5100, loss 0.0184648, acc 0.98

Evaluation:
2016-09-07T19:54:23.219717: step 5100, loss 2.13678, acc 0.736

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-5100

2016-09-07T19:54:24.970519: step 5101, loss 0.0336019, acc 1
2016-09-07T19:54:25.665789: step 5102, loss 0.0731636, acc 0.98
2016-09-07T19:54:26.350917: step 5103, loss 0.0395586, acc 0.96
2016-09-07T19:54:27.033821: step 5104, loss 0.0407167, acc 0.98
2016-09-07T19:54:27.704831: step 5105, loss 0.0166371, acc 0.98
2016-09-07T19:54:28.367776: step 5106, loss 0.00717043, acc 1
2016-09-07T19:54:29.036597: step 5107, loss 0.0826888, acc 0.96
2016-09-07T19:54:29.715215: step 5108, loss 0.0100872, acc 1
2016-09-07T19:54:30.386286: step 5109, loss 0.198456, acc 0.94
2016-09-07T19:54:31.047344: step 5110, loss 0.0129684, acc 1
2016-09-07T19:54:31.722147: step 5111, loss 0.0353213, acc 0.98
2016-09-07T19:54:32.391647: step 5112, loss 0.0119208, acc 1
2016-09-07T19:54:33.058446: step 5113, loss 0.00389951, acc 1
2016-09-07T19:54:33.723575: step 5114, loss 0.0428361, acc 0.98
2016-09-07T19:54:34.391471: step 5115, loss 0.0140296, acc 1
2016-09-07T19:54:35.067145: step 5116, loss 0.0132863, acc 1
2016-09-07T19:54:35.735208: step 5117, loss 0.00180671, acc 1
2016-09-07T19:54:36.401864: step 5118, loss 0.00915397, acc 1
2016-09-07T19:54:37.071182: step 5119, loss 0.0102332, acc 1
2016-09-07T19:54:37.766364: step 5120, loss 0.0573285, acc 0.98
2016-09-07T19:54:38.964784: step 5121, loss 0.0531001, acc 0.96
2016-09-07T19:54:39.632449: step 5122, loss 0.00258145, acc 1
2016-09-07T19:54:40.311698: step 5123, loss 0.0059915, acc 1
2016-09-07T19:54:41.004771: step 5124, loss 0.0382105, acc 0.98
2016-09-07T19:54:41.700165: step 5125, loss 0.00468959, acc 1
2016-09-07T19:54:42.380197: step 5126, loss 0.00308609, acc 1
2016-09-07T19:54:43.055660: step 5127, loss 0.0379097, acc 0.98
2016-09-07T19:54:43.762519: step 5128, loss 0.0488994, acc 0.98
2016-09-07T19:54:44.458290: step 5129, loss 0.0610671, acc 0.98
2016-09-07T19:54:45.137321: step 5130, loss 0.0195037, acc 1
2016-09-07T19:54:45.813128: step 5131, loss 0.00723965, acc 1
2016-09-07T19:54:46.489963: step 5132, loss 0.127143, acc 0.96
2016-09-07T19:54:47.163292: step 5133, loss 0.0464008, acc 0.96
2016-09-07T19:54:47.831985: step 5134, loss 0.0228019, acc 1
2016-09-07T19:54:48.501357: step 5135, loss 0.037345, acc 0.98
2016-09-07T19:54:49.167582: step 5136, loss 0.0504892, acc 0.96
2016-09-07T19:54:49.850026: step 5137, loss 0.0343869, acc 0.98
2016-09-07T19:54:50.525855: step 5138, loss 0.0019235, acc 1
2016-09-07T19:54:51.214724: step 5139, loss 0.113265, acc 0.96
2016-09-07T19:54:51.883369: step 5140, loss 0.0269239, acc 0.98
2016-09-07T19:54:52.537421: step 5141, loss 0.0190327, acc 0.98
2016-09-07T19:54:53.195624: step 5142, loss 0.0581099, acc 0.96
2016-09-07T19:54:53.882459: step 5143, loss 0.0195368, acc 1
2016-09-07T19:54:54.568148: step 5144, loss 0.0289602, acc 0.96
2016-09-07T19:54:55.254812: step 5145, loss 0.0155561, acc 1
2016-09-07T19:54:55.940857: step 5146, loss 0.000648305, acc 1
2016-09-07T19:54:56.618848: step 5147, loss 0.0176434, acc 1
2016-09-07T19:54:57.318484: step 5148, loss 0.00809931, acc 1
2016-09-07T19:54:58.007224: step 5149, loss 0.0142409, acc 1
2016-09-07T19:54:58.682987: step 5150, loss 0.0179712, acc 1
2016-09-07T19:54:59.348255: step 5151, loss 0.00620728, acc 1
2016-09-07T19:55:00.021509: step 5152, loss 0.00242975, acc 1
2016-09-07T19:55:00.731062: step 5153, loss 0.144985, acc 0.96
2016-09-07T19:55:01.425514: step 5154, loss 0.0831838, acc 0.96
2016-09-07T19:55:02.091493: step 5155, loss 0.162799, acc 0.92
2016-09-07T19:55:02.752522: step 5156, loss 0.00694265, acc 1
2016-09-07T19:55:03.423830: step 5157, loss 0.0047571, acc 1
2016-09-07T19:55:04.079236: step 5158, loss 0.0519603, acc 0.94
2016-09-07T19:55:04.748634: step 5159, loss 0.0870623, acc 0.98
2016-09-07T19:55:05.395201: step 5160, loss 0.0300252, acc 1
2016-09-07T19:55:06.038138: step 5161, loss 0.051532, acc 0.96
2016-09-07T19:55:06.711880: step 5162, loss 0.0694012, acc 0.98
2016-09-07T19:55:07.387712: step 5163, loss 0.00491662, acc 1
2016-09-07T19:55:08.052578: step 5164, loss 0.0838158, acc 0.92
2016-09-07T19:55:08.706116: step 5165, loss 0.0140314, acc 1
2016-09-07T19:55:09.381250: step 5166, loss 0.0517218, acc 0.98
2016-09-07T19:55:10.053027: step 5167, loss 0.0181722, acc 1
2016-09-07T19:55:10.719100: step 5168, loss 0.0177356, acc 0.98
2016-09-07T19:55:11.400542: step 5169, loss 0.044594, acc 0.98
2016-09-07T19:55:12.097931: step 5170, loss 0.0294701, acc 0.98
2016-09-07T19:55:12.781140: step 5171, loss 0.0507457, acc 0.96
2016-09-07T19:55:13.458717: step 5172, loss 0.018532, acc 0.98
2016-09-07T19:55:14.149272: step 5173, loss 0.00510099, acc 1
2016-09-07T19:55:14.833401: step 5174, loss 0.0243577, acc 0.98
2016-09-07T19:55:15.503688: step 5175, loss 0.0291519, acc 1
2016-09-07T19:55:16.175932: step 5176, loss 0.0285771, acc 0.98
2016-09-07T19:55:16.845946: step 5177, loss 0.00842903, acc 1
2016-09-07T19:55:17.532728: step 5178, loss 0.0126866, acc 1
2016-09-07T19:55:18.225612: step 5179, loss 0.141467, acc 0.96
2016-09-07T19:55:18.908436: step 5180, loss 0.0103377, acc 1
2016-09-07T19:55:19.592436: step 5181, loss 0.0052961, acc 1
2016-09-07T19:55:20.284720: step 5182, loss 0.000734556, acc 1
2016-09-07T19:55:20.948585: step 5183, loss 0.0138726, acc 1
2016-09-07T19:55:21.626884: step 5184, loss 0.0306527, acc 0.98
2016-09-07T19:55:22.299061: step 5185, loss 0.0361246, acc 0.98
2016-09-07T19:55:22.972526: step 5186, loss 0.0129216, acc 1
2016-09-07T19:55:23.647006: step 5187, loss 0.0213285, acc 0.98
2016-09-07T19:55:24.334202: step 5188, loss 0.11922, acc 0.98
2016-09-07T19:55:25.001727: step 5189, loss 0.0135187, acc 1
2016-09-07T19:55:25.675676: step 5190, loss 0.0114214, acc 1
2016-09-07T19:55:26.346095: step 5191, loss 0.0113491, acc 1
2016-09-07T19:55:27.031172: step 5192, loss 0.0830705, acc 0.98
2016-09-07T19:55:27.715160: step 5193, loss 0.00424612, acc 1
2016-09-07T19:55:28.400562: step 5194, loss 0.0188742, acc 1
2016-09-07T19:55:29.070190: step 5195, loss 0.0323554, acc 0.98
2016-09-07T19:55:29.746505: step 5196, loss 0.0474134, acc 1
2016-09-07T19:55:30.424416: step 5197, loss 0.157682, acc 0.98
2016-09-07T19:55:31.087466: step 5198, loss 0.0156816, acc 1
2016-09-07T19:55:31.752219: step 5199, loss 0.0800793, acc 0.92
2016-09-07T19:55:32.436362: step 5200, loss 0.0113263, acc 1

Evaluation:
2016-09-07T19:55:35.713604: step 5200, loss 1.8511, acc 0.757

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-5200

2016-09-07T19:55:37.513592: step 5201, loss 0.00170414, acc 1
2016-09-07T19:55:38.191453: step 5202, loss 0.1327, acc 0.94
2016-09-07T19:55:38.877312: step 5203, loss 0.0523091, acc 0.98
2016-09-07T19:55:39.552930: step 5204, loss 0.00330762, acc 1
2016-09-07T19:55:40.221180: step 5205, loss 0.0377971, acc 0.98
2016-09-07T19:55:40.928879: step 5206, loss 0.0086837, acc 1
2016-09-07T19:55:41.623592: step 5207, loss 0.0103807, acc 1
2016-09-07T19:55:42.299585: step 5208, loss 0.0104014, acc 1
2016-09-07T19:55:42.976836: step 5209, loss 0.0150025, acc 0.98
2016-09-07T19:55:43.643536: step 5210, loss 0.0151134, acc 1
2016-09-07T19:55:44.348270: step 5211, loss 0.0895253, acc 0.96
2016-09-07T19:55:45.015549: step 5212, loss 0.0270597, acc 1
2016-09-07T19:55:45.674001: step 5213, loss 0.03657, acc 0.98
2016-09-07T19:55:46.366059: step 5214, loss 0.0318904, acc 1
2016-09-07T19:55:47.035892: step 5215, loss 0.015493, acc 1
2016-09-07T19:55:47.712259: step 5216, loss 0.0139052, acc 1
2016-09-07T19:55:48.380411: step 5217, loss 0.0091402, acc 1
2016-09-07T19:55:49.065842: step 5218, loss 0.0247292, acc 0.98
2016-09-07T19:55:49.755659: step 5219, loss 0.00285327, acc 1
2016-09-07T19:55:50.447003: step 5220, loss 0.00812263, acc 1
2016-09-07T19:55:51.123859: step 5221, loss 0.00801749, acc 1
2016-09-07T19:55:51.823855: step 5222, loss 0.0189037, acc 1
2016-09-07T19:55:52.493740: step 5223, loss 0.00386114, acc 1
2016-09-07T19:55:53.161974: step 5224, loss 0.0148773, acc 1
2016-09-07T19:55:53.844609: step 5225, loss 0.037836, acc 0.98
2016-09-07T19:55:54.520696: step 5226, loss 0.136666, acc 0.96
2016-09-07T19:55:55.198095: step 5227, loss 0.0139822, acc 1
2016-09-07T19:55:55.884495: step 5228, loss 0.000169396, acc 1
2016-09-07T19:55:56.549152: step 5229, loss 0.0254543, acc 1
2016-09-07T19:55:57.234749: step 5230, loss 0.0493784, acc 0.98
2016-09-07T19:55:57.900063: step 5231, loss 0.0903317, acc 0.98
2016-09-07T19:55:58.577423: step 5232, loss 0.0960153, acc 0.98
2016-09-07T19:55:59.248750: step 5233, loss 0.00625117, acc 1
2016-09-07T19:55:59.928574: step 5234, loss 0.038384, acc 0.98
2016-09-07T19:56:00.646099: step 5235, loss 0.0742071, acc 0.96
2016-09-07T19:56:01.319240: step 5236, loss 0.0298496, acc 0.98
2016-09-07T19:56:01.993327: step 5237, loss 0.021285, acc 1
2016-09-07T19:56:02.345507: step 5238, loss 0.00182988, acc 1
2016-09-07T19:56:03.032718: step 5239, loss 0.00976092, acc 1
2016-09-07T19:56:03.695543: step 5240, loss 0.0249702, acc 1
2016-09-07T19:56:04.363482: step 5241, loss 0.0226484, acc 1
2016-09-07T19:56:05.012406: step 5242, loss 0.0779807, acc 0.98
2016-09-07T19:56:05.672992: step 5243, loss 0.0692867, acc 0.96
2016-09-07T19:56:06.352701: step 5244, loss 0.0427073, acc 0.98
2016-09-07T19:56:07.019990: step 5245, loss 0.041778, acc 0.98
2016-09-07T19:56:07.686009: step 5246, loss 0.00990363, acc 1
2016-09-07T19:56:08.350178: step 5247, loss 0.0178823, acc 0.98
2016-09-07T19:56:09.034002: step 5248, loss 0.0214743, acc 0.98
2016-09-07T19:56:09.704257: step 5249, loss 0.0294842, acc 0.98
2016-09-07T19:56:10.378353: step 5250, loss 0.051918, acc 0.98
2016-09-07T19:56:11.040548: step 5251, loss 0.0437675, acc 0.96
2016-09-07T19:56:11.698278: step 5252, loss 0.0549642, acc 0.96
2016-09-07T19:56:12.366641: step 5253, loss 0.0161683, acc 1
2016-09-07T19:56:13.050606: step 5254, loss 0.000502566, acc 1
2016-09-07T19:56:13.719043: step 5255, loss 0.00566463, acc 1
2016-09-07T19:56:14.392963: step 5256, loss 0.00284209, acc 1
2016-09-07T19:56:15.068871: step 5257, loss 0.0189931, acc 1
2016-09-07T19:56:15.740878: step 5258, loss 0.0320656, acc 0.98
2016-09-07T19:56:16.430728: step 5259, loss 0.00327542, acc 1
2016-09-07T19:56:17.092774: step 5260, loss 0.0238672, acc 1
2016-09-07T19:56:17.775520: step 5261, loss 0.00627702, acc 1
2016-09-07T19:56:18.455281: step 5262, loss 0.00176464, acc 1
2016-09-07T19:56:19.111569: step 5263, loss 0.0157021, acc 1
2016-09-07T19:56:19.792634: step 5264, loss 0.00385045, acc 1
2016-09-07T19:56:20.455927: step 5265, loss 0.0272842, acc 0.98
2016-09-07T19:56:21.135836: step 5266, loss 0.0775449, acc 0.96
2016-09-07T19:56:21.806967: step 5267, loss 0.00880644, acc 1
2016-09-07T19:56:22.481546: step 5268, loss 0.0441228, acc 0.98
2016-09-07T19:56:23.155697: step 5269, loss 0.0137486, acc 1
2016-09-07T19:56:23.843529: step 5270, loss 0.0269931, acc 0.98
2016-09-07T19:56:24.545450: step 5271, loss 0.0532972, acc 0.98
2016-09-07T19:56:25.222734: step 5272, loss 0.029772, acc 0.98
2016-09-07T19:56:25.886941: step 5273, loss 0.0326934, acc 0.98
2016-09-07T19:56:26.577096: step 5274, loss 0.00518954, acc 1
2016-09-07T19:56:27.260560: step 5275, loss 0.0617051, acc 0.98
2016-09-07T19:56:27.978757: step 5276, loss 0.0382713, acc 0.98
2016-09-07T19:56:28.657835: step 5277, loss 0.000147422, acc 1
2016-09-07T19:56:29.339491: step 5278, loss 0.0219676, acc 0.98
2016-09-07T19:56:30.017765: step 5279, loss 0.0255507, acc 1
2016-09-07T19:56:30.699044: step 5280, loss 0.00813029, acc 1
2016-09-07T19:56:31.388083: step 5281, loss 0.033454, acc 0.98
2016-09-07T19:56:32.077765: step 5282, loss 0.00230771, acc 1
2016-09-07T19:56:32.740275: step 5283, loss 0.0163233, acc 1
2016-09-07T19:56:33.427566: step 5284, loss 0.100946, acc 0.94
2016-09-07T19:56:34.120388: step 5285, loss 0.0527837, acc 0.98
2016-09-07T19:56:34.783651: step 5286, loss 0.0398383, acc 0.98
2016-09-07T19:56:35.464584: step 5287, loss 0.0149473, acc 1
2016-09-07T19:56:36.155886: step 5288, loss 0.0383648, acc 1
2016-09-07T19:56:36.827030: step 5289, loss 0.0185362, acc 1
2016-09-07T19:56:37.505694: step 5290, loss 0.0136836, acc 1
2016-09-07T19:56:38.181432: step 5291, loss 0.0646424, acc 0.98
2016-09-07T19:56:38.862552: step 5292, loss 0.0926286, acc 0.96
2016-09-07T19:56:39.539582: step 5293, loss 0.00784578, acc 1
2016-09-07T19:56:40.215214: step 5294, loss 0.00999422, acc 1
2016-09-07T19:56:40.901857: step 5295, loss 0.0270643, acc 1
2016-09-07T19:56:41.561955: step 5296, loss 0.00140658, acc 1
2016-09-07T19:56:42.225056: step 5297, loss 0.039261, acc 0.98
2016-09-07T19:56:42.888013: step 5298, loss 0.0298749, acc 0.98
2016-09-07T19:56:43.551158: step 5299, loss 0.0507848, acc 0.98
2016-09-07T19:56:44.223296: step 5300, loss 0.0491651, acc 0.98

Evaluation:
2016-09-07T19:56:47.511958: step 5300, loss 1.97908, acc 0.75

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-5300

2016-09-07T19:56:49.232915: step 5301, loss 0.112379, acc 0.96
2016-09-07T19:56:49.906502: step 5302, loss 0.0184326, acc 1
2016-09-07T19:56:50.571862: step 5303, loss 0.0373523, acc 0.98
2016-09-07T19:56:51.236538: step 5304, loss 0.023778, acc 0.98
2016-09-07T19:56:51.909853: step 5305, loss 0.0619428, acc 0.96
2016-09-07T19:56:52.565700: step 5306, loss 0.0398112, acc 0.96
2016-09-07T19:56:53.235843: step 5307, loss 0.0325816, acc 0.98
2016-09-07T19:56:53.891724: step 5308, loss 0.0287836, acc 0.98
2016-09-07T19:56:54.560551: step 5309, loss 0.0525183, acc 0.96
2016-09-07T19:56:55.235931: step 5310, loss 0.0114242, acc 1
2016-09-07T19:56:55.922219: step 5311, loss 0.00664054, acc 1
2016-09-07T19:56:56.590795: step 5312, loss 0.003448, acc 1
2016-09-07T19:56:57.248586: step 5313, loss 0.00950844, acc 1
2016-09-07T19:56:57.930101: step 5314, loss 0.0774434, acc 0.96
2016-09-07T19:56:58.614037: step 5315, loss 0.0229083, acc 1
2016-09-07T19:56:59.295061: step 5316, loss 0.00493448, acc 1
2016-09-07T19:56:59.998675: step 5317, loss 0.0305874, acc 0.98
2016-09-07T19:57:00.739926: step 5318, loss 0.0807357, acc 0.92
2016-09-07T19:57:01.424594: step 5319, loss 0.0664771, acc 0.96
2016-09-07T19:57:02.095270: step 5320, loss 0.0872478, acc 0.94
2016-09-07T19:57:02.769755: step 5321, loss 0.00318111, acc 1
2016-09-07T19:57:03.442211: step 5322, loss 0.0343502, acc 0.98
2016-09-07T19:57:04.106018: step 5323, loss 0.0153059, acc 1
2016-09-07T19:57:04.775410: step 5324, loss 0.00653968, acc 1
2016-09-07T19:57:05.445224: step 5325, loss 0.0213361, acc 0.98
2016-09-07T19:57:06.136836: step 5326, loss 0.0196039, acc 0.98
2016-09-07T19:57:06.795630: step 5327, loss 0.0135526, acc 1
2016-09-07T19:57:07.471241: step 5328, loss 0.0206701, acc 1
2016-09-07T19:57:08.153516: step 5329, loss 0.0279967, acc 0.98
2016-09-07T19:57:08.849005: step 5330, loss 0.0090494, acc 1
2016-09-07T19:57:09.515039: step 5331, loss 0.0449084, acc 0.96
2016-09-07T19:57:10.185683: step 5332, loss 0.178337, acc 0.98
2016-09-07T19:57:10.855781: step 5333, loss 0.074218, acc 0.96
2016-09-07T19:57:11.528920: step 5334, loss 0.0446634, acc 0.98
2016-09-07T19:57:12.217899: step 5335, loss 0.0173398, acc 1
2016-09-07T19:57:12.899984: step 5336, loss 0.0738421, acc 0.96
2016-09-07T19:57:13.607743: step 5337, loss 0.0300524, acc 0.98
2016-09-07T19:57:14.289687: step 5338, loss 0.0569615, acc 0.96
2016-09-07T19:57:14.992186: step 5339, loss 0.0584574, acc 0.98
2016-09-07T19:57:15.680186: step 5340, loss 0.0634368, acc 0.96
2016-09-07T19:57:16.353226: step 5341, loss 0.0852839, acc 0.96
2016-09-07T19:57:17.028498: step 5342, loss 0.0160372, acc 1
2016-09-07T19:57:17.722361: step 5343, loss 0.0544732, acc 0.96
2016-09-07T19:57:18.366758: step 5344, loss 0.0186271, acc 0.98
2016-09-07T19:57:19.055416: step 5345, loss 0.0269142, acc 1
2016-09-07T19:57:19.728444: step 5346, loss 0.0541779, acc 0.98
2016-09-07T19:57:20.390720: step 5347, loss 0.0231965, acc 1
2016-09-07T19:57:21.057280: step 5348, loss 0.0854079, acc 0.96
2016-09-07T19:57:21.729905: step 5349, loss 0.10071, acc 0.94
2016-09-07T19:57:22.409965: step 5350, loss 0.0322266, acc 0.98
2016-09-07T19:57:23.074165: step 5351, loss 0.0180613, acc 0.98
2016-09-07T19:57:23.724460: step 5352, loss 0.0297768, acc 0.98
2016-09-07T19:57:24.387132: step 5353, loss 0.083912, acc 0.98
2016-09-07T19:57:25.058319: step 5354, loss 0.0394624, acc 0.98
2016-09-07T19:57:25.744381: step 5355, loss 0.0293929, acc 0.98
2016-09-07T19:57:26.415077: step 5356, loss 0.0280091, acc 1
2016-09-07T19:57:27.081096: step 5357, loss 0.0325315, acc 0.98
2016-09-07T19:57:27.741974: step 5358, loss 0.0417836, acc 0.98
2016-09-07T19:57:28.412357: step 5359, loss 0.0093157, acc 1
2016-09-07T19:57:29.088398: step 5360, loss 0.0989704, acc 0.94
2016-09-07T19:57:29.774044: step 5361, loss 0.0381927, acc 0.98
2016-09-07T19:57:30.458548: step 5362, loss 0.0190823, acc 1
2016-09-07T19:57:31.128657: step 5363, loss 0.0483799, acc 0.98
2016-09-07T19:57:31.804837: step 5364, loss 0.031215, acc 0.98
2016-09-07T19:57:32.476656: step 5365, loss 0.043906, acc 0.98
2016-09-07T19:57:33.164587: step 5366, loss 0.00166246, acc 1
2016-09-07T19:57:33.851081: step 5367, loss 0.0277168, acc 0.98
2016-09-07T19:57:34.522966: step 5368, loss 0.058408, acc 0.98
2016-09-07T19:57:35.183984: step 5369, loss 0.0847992, acc 0.96
2016-09-07T19:57:35.903266: step 5370, loss 0.0157987, acc 0.98
2016-09-07T19:57:36.567648: step 5371, loss 0.00677768, acc 1
2016-09-07T19:57:37.251951: step 5372, loss 0.0460997, acc 0.98
2016-09-07T19:57:37.922946: step 5373, loss 0.0554336, acc 0.98
2016-09-07T19:57:38.588762: step 5374, loss 0.00194117, acc 1
2016-09-07T19:57:39.257963: step 5375, loss 0.00587372, acc 1
2016-09-07T19:57:39.939188: step 5376, loss 0.00534777, acc 1
2016-09-07T19:57:40.598840: step 5377, loss 0.00183195, acc 1
2016-09-07T19:57:41.274405: step 5378, loss 0.035499, acc 1
2016-09-07T19:57:41.949548: step 5379, loss 0.0531292, acc 0.98
2016-09-07T19:57:42.717778: step 5380, loss 0.0381696, acc 0.98
2016-09-07T19:57:43.385638: step 5381, loss 0.0883388, acc 0.96
2016-09-07T19:57:44.069771: step 5382, loss 0.0386777, acc 0.98
2016-09-07T19:57:44.741818: step 5383, loss 0.0139685, acc 1
2016-09-07T19:57:45.427206: step 5384, loss 0.00613931, acc 1
2016-09-07T19:57:46.114354: step 5385, loss 0.0770264, acc 0.96
2016-09-07T19:57:46.793961: step 5386, loss 0.284494, acc 0.96
2016-09-07T19:57:47.455739: step 5387, loss 0.0427153, acc 0.96
2016-09-07T19:57:48.117879: step 5388, loss 0.000305978, acc 1
2016-09-07T19:57:48.781908: step 5389, loss 0.0505755, acc 0.98
2016-09-07T19:57:49.458283: step 5390, loss 0.0297811, acc 0.98
2016-09-07T19:57:50.126397: step 5391, loss 0.0306787, acc 0.98
2016-09-07T19:57:50.821274: step 5392, loss 0.0445586, acc 0.96
2016-09-07T19:57:51.505152: step 5393, loss 0.0343183, acc 0.98
2016-09-07T19:57:52.191460: step 5394, loss 0.0250521, acc 1
2016-09-07T19:57:52.889389: step 5395, loss 0.00622914, acc 1
2016-09-07T19:57:53.582949: step 5396, loss 0.00518435, acc 1
2016-09-07T19:57:54.270154: step 5397, loss 0.00928622, acc 1
2016-09-07T19:57:54.932754: step 5398, loss 0.0296904, acc 0.98
2016-09-07T19:57:55.616159: step 5399, loss 0.0976023, acc 0.96
2016-09-07T19:57:56.306597: step 5400, loss 0.0259917, acc 0.98

Evaluation:
2016-09-07T19:57:59.608203: step 5400, loss 1.81142, acc 0.744

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-5400

2016-09-07T19:58:01.294851: step 5401, loss 0.000134789, acc 1
2016-09-07T19:58:01.995894: step 5402, loss 0.0753978, acc 0.94
2016-09-07T19:58:02.674270: step 5403, loss 0.0167639, acc 1
2016-09-07T19:58:03.356706: step 5404, loss 0.0354408, acc 0.98
2016-09-07T19:58:04.032062: step 5405, loss 0.0286012, acc 0.98
2016-09-07T19:58:04.697337: step 5406, loss 0.0339116, acc 0.98
2016-09-07T19:58:05.382993: step 5407, loss 0.131252, acc 0.98
2016-09-07T19:58:06.071201: step 5408, loss 0.156232, acc 0.94
2016-09-07T19:58:06.731435: step 5409, loss 0.0246586, acc 0.98
2016-09-07T19:58:07.405084: step 5410, loss 0.0390662, acc 0.98
2016-09-07T19:58:08.080735: step 5411, loss 0.0314394, acc 0.98
2016-09-07T19:58:08.784537: step 5412, loss 0.0125638, acc 1
2016-09-07T19:58:09.475299: step 5413, loss 0.266628, acc 0.98
2016-09-07T19:58:10.170023: step 5414, loss 0.00745411, acc 1
2016-09-07T19:58:10.859652: step 5415, loss 0.0419289, acc 0.98
2016-09-07T19:58:11.539203: step 5416, loss 0.00604299, acc 1
2016-09-07T19:58:12.214764: step 5417, loss 0.0117659, acc 1
2016-09-07T19:58:12.899362: step 5418, loss 0.0152342, acc 1
2016-09-07T19:58:13.584724: step 5419, loss 0.0287243, acc 1
2016-09-07T19:58:14.239856: step 5420, loss 0.0119404, acc 1
2016-09-07T19:58:14.904970: step 5421, loss 0.00610186, acc 1
2016-09-07T19:58:15.580016: step 5422, loss 0.0181233, acc 0.98
2016-09-07T19:58:16.285338: step 5423, loss 0.126151, acc 0.98
2016-09-07T19:58:16.966420: step 5424, loss 0.0137122, acc 1
2016-09-07T19:58:17.644856: step 5425, loss 0.0298776, acc 1
2016-09-07T19:58:18.314473: step 5426, loss 0.0238772, acc 0.98
2016-09-07T19:58:18.994658: step 5427, loss 0.0194483, acc 1
2016-09-07T19:58:19.665202: step 5428, loss 0.0935006, acc 0.94
2016-09-07T19:58:20.344263: step 5429, loss 0.0579758, acc 0.96
2016-09-07T19:58:21.049932: step 5430, loss 0.0216545, acc 1
2016-09-07T19:58:21.724591: step 5431, loss 0.00886936, acc 1
2016-09-07T19:58:22.092910: step 5432, loss 0.0644293, acc 0.916667
2016-09-07T19:58:22.785880: step 5433, loss 0.032756, acc 0.98
2016-09-07T19:58:23.460924: step 5434, loss 0.0635774, acc 0.98
2016-09-07T19:58:24.116317: step 5435, loss 0.0367743, acc 1
2016-09-07T19:58:24.804898: step 5436, loss 0.0104712, acc 1
2016-09-07T19:58:25.492681: step 5437, loss 0.0374252, acc 0.98
2016-09-07T19:58:26.211500: step 5438, loss 0.0446783, acc 0.98
2016-09-07T19:58:26.930972: step 5439, loss 0.0197185, acc 0.98
2016-09-07T19:58:27.606011: step 5440, loss 0.00686968, acc 1
2016-09-07T19:58:28.290092: step 5441, loss 0.00279069, acc 1
2016-09-07T19:58:28.964255: step 5442, loss 0.0130004, acc 1
2016-09-07T19:58:29.629609: step 5443, loss 0.0238994, acc 1
2016-09-07T19:58:30.307284: step 5444, loss 0.0154303, acc 0.98
2016-09-07T19:58:31.011946: step 5445, loss 0.0484877, acc 0.98
2016-09-07T19:58:31.692946: step 5446, loss 0.073063, acc 0.96
2016-09-07T19:58:32.365451: step 5447, loss 0.0225753, acc 0.98
2016-09-07T19:58:33.031142: step 5448, loss 0.0362687, acc 0.98
2016-09-07T19:58:33.719655: step 5449, loss 0.028948, acc 0.98
2016-09-07T19:58:34.377373: step 5450, loss 0.0227532, acc 0.98
2016-09-07T19:58:35.057001: step 5451, loss 0.0211168, acc 1
2016-09-07T19:58:35.732181: step 5452, loss 0.0412621, acc 0.96
2016-09-07T19:58:36.416447: step 5453, loss 0.130017, acc 0.94
2016-09-07T19:58:37.096392: step 5454, loss 0.0236546, acc 0.98
2016-09-07T19:58:37.754790: step 5455, loss 0.0365741, acc 0.98
2016-09-07T19:58:38.430731: step 5456, loss 0.00430414, acc 1
2016-09-07T19:58:39.129515: step 5457, loss 0.0187341, acc 1
2016-09-07T19:58:39.807918: step 5458, loss 0.0490024, acc 0.96
2016-09-07T19:58:40.471527: step 5459, loss 0.0157343, acc 1
2016-09-07T19:58:41.161627: step 5460, loss 0.0330335, acc 0.98
2016-09-07T19:58:41.843682: step 5461, loss 0.0635766, acc 0.98
2016-09-07T19:58:42.513238: step 5462, loss 0.0287388, acc 1
2016-09-07T19:58:43.201424: step 5463, loss 0.0264258, acc 0.98
2016-09-07T19:58:43.854055: step 5464, loss 0.118179, acc 0.94
2016-09-07T19:58:44.514535: step 5465, loss 0.0400177, acc 0.96
2016-09-07T19:58:45.220030: step 5466, loss 0.00325554, acc 1
2016-09-07T19:58:45.902278: step 5467, loss 0.00913488, acc 1
2016-09-07T19:58:46.559539: step 5468, loss 0.0441454, acc 0.96
2016-09-07T19:58:47.222368: step 5469, loss 0.0174745, acc 0.98
2016-09-07T19:58:47.876352: step 5470, loss 0.0237023, acc 0.98
2016-09-07T19:58:48.561872: step 5471, loss 0.0234143, acc 0.98
2016-09-07T19:58:49.240085: step 5472, loss 0.000674918, acc 1
2016-09-07T19:58:49.909482: step 5473, loss 0.023813, acc 1
2016-09-07T19:58:50.585361: step 5474, loss 0.00207129, acc 1
2016-09-07T19:58:51.255459: step 5475, loss 0.0221711, acc 1
2016-09-07T19:58:51.947256: step 5476, loss 0.00132769, acc 1
2016-09-07T19:58:52.646108: step 5477, loss 0.0159144, acc 1
2016-09-07T19:58:53.324379: step 5478, loss 0.0599277, acc 0.94
2016-09-07T19:58:53.991695: step 5479, loss 0.0188322, acc 1
2016-09-07T19:58:54.653899: step 5480, loss 0.0215855, acc 1
2016-09-07T19:58:55.324370: step 5481, loss 0.00711909, acc 1
2016-09-07T19:58:55.990918: step 5482, loss 0.00840852, acc 1
2016-09-07T19:58:56.656960: step 5483, loss 0.00246794, acc 1
2016-09-07T19:58:57.320183: step 5484, loss 0.0307787, acc 0.98
2016-09-07T19:58:57.986323: step 5485, loss 0.00457094, acc 1
2016-09-07T19:58:58.652604: step 5486, loss 0.0811582, acc 0.98
2016-09-07T19:58:59.326080: step 5487, loss 0.000639333, acc 1
2016-09-07T19:59:00.012239: step 5488, loss 0.110416, acc 0.98
2016-09-07T19:59:00.729428: step 5489, loss 0.0105826, acc 1
2016-09-07T19:59:01.496542: step 5490, loss 0.00324635, acc 1
2016-09-07T19:59:02.188477: step 5491, loss 0.0126539, acc 1
2016-09-07T19:59:02.843460: step 5492, loss 0.0150333, acc 1
2016-09-07T19:59:03.502600: step 5493, loss 0.0996663, acc 0.94
2016-09-07T19:59:04.199801: step 5494, loss 0.000442474, acc 1
2016-09-07T19:59:04.862880: step 5495, loss 0.0286601, acc 0.98
2016-09-07T19:59:05.528513: step 5496, loss 0.0284014, acc 1
2016-09-07T19:59:06.204229: step 5497, loss 0.0275111, acc 1
2016-09-07T19:59:06.892740: step 5498, loss 0.0386581, acc 0.98
2016-09-07T19:59:07.573452: step 5499, loss 0.00591181, acc 1
2016-09-07T19:59:08.253326: step 5500, loss 0.0158049, acc 0.98

Evaluation:
2016-09-07T19:59:11.525892: step 5500, loss 2.09489, acc 0.737

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-5500

2016-09-07T19:59:13.287426: step 5501, loss 0.0321291, acc 0.98
2016-09-07T19:59:13.965637: step 5502, loss 0.0459027, acc 0.96
2016-09-07T19:59:14.657882: step 5503, loss 0.135946, acc 0.96
2016-09-07T19:59:15.335090: step 5504, loss 0.0294519, acc 1
2016-09-07T19:59:16.017332: step 5505, loss 0.00388428, acc 1
2016-09-07T19:59:16.703457: step 5506, loss 0.0204003, acc 1
2016-09-07T19:59:17.379837: step 5507, loss 0.0264999, acc 0.98
2016-09-07T19:59:18.054588: step 5508, loss 0.0198728, acc 0.98
2016-09-07T19:59:18.721510: step 5509, loss 0.130195, acc 0.94
2016-09-07T19:59:19.373272: step 5510, loss 0.00201427, acc 1
2016-09-07T19:59:20.042452: step 5511, loss 0.0126517, acc 1
2016-09-07T19:59:20.729688: step 5512, loss 0.0194273, acc 0.98
2016-09-07T19:59:21.404690: step 5513, loss 0.140013, acc 0.98
2016-09-07T19:59:22.072984: step 5514, loss 0.081581, acc 0.94
2016-09-07T19:59:22.743419: step 5515, loss 0.0532813, acc 0.96
2016-09-07T19:59:23.415319: step 5516, loss 0.010253, acc 1
2016-09-07T19:59:24.082901: step 5517, loss 0.0639104, acc 0.98
2016-09-07T19:59:24.753226: step 5518, loss 0.0189557, acc 1
2016-09-07T19:59:25.419777: step 5519, loss 0.00565845, acc 1
2016-09-07T19:59:26.099417: step 5520, loss 0.0325244, acc 0.98
2016-09-07T19:59:26.787764: step 5521, loss 0.00098282, acc 1
2016-09-07T19:59:27.478422: step 5522, loss 0.0164591, acc 1
2016-09-07T19:59:28.173359: step 5523, loss 0.0302619, acc 1
2016-09-07T19:59:28.843618: step 5524, loss 0.0321968, acc 0.96
2016-09-07T19:59:29.517969: step 5525, loss 0.00309815, acc 1
2016-09-07T19:59:30.203715: step 5526, loss 0.0378964, acc 0.98
2016-09-07T19:59:30.915332: step 5527, loss 0.0320827, acc 0.98
2016-09-07T19:59:31.600515: step 5528, loss 0.012939, acc 1
2016-09-07T19:59:32.312437: step 5529, loss 0.0024789, acc 1
2016-09-07T19:59:32.969982: step 5530, loss 0.0871451, acc 0.94
2016-09-07T19:59:33.636335: step 5531, loss 0.0184471, acc 1
2016-09-07T19:59:34.306596: step 5532, loss 0.0272067, acc 0.98
2016-09-07T19:59:34.972327: step 5533, loss 0.00215455, acc 1
2016-09-07T19:59:35.659954: step 5534, loss 0.0292966, acc 0.98
2016-09-07T19:59:36.346639: step 5535, loss 0.0130888, acc 1
2016-09-07T19:59:37.015748: step 5536, loss 0.0618484, acc 0.96
2016-09-07T19:59:37.714930: step 5537, loss 0.000116935, acc 1
2016-09-07T19:59:38.402804: step 5538, loss 0.00851874, acc 1
2016-09-07T19:59:39.068385: step 5539, loss 0.0246475, acc 0.98
2016-09-07T19:59:39.742552: step 5540, loss 0.0248387, acc 0.98
2016-09-07T19:59:40.407552: step 5541, loss 0.0300871, acc 0.98
2016-09-07T19:59:41.081924: step 5542, loss 0.0145548, acc 1
2016-09-07T19:59:41.759173: step 5543, loss 0.0778712, acc 0.96
2016-09-07T19:59:42.430601: step 5544, loss 0.0232643, acc 0.98
2016-09-07T19:59:43.100973: step 5545, loss 0.00929145, acc 1
2016-09-07T19:59:43.767505: step 5546, loss 0.0144469, acc 1
2016-09-07T19:59:44.465305: step 5547, loss 0.0205102, acc 0.98
2016-09-07T19:59:45.149704: step 5548, loss 0.0136338, acc 1
2016-09-07T19:59:45.834206: step 5549, loss 0.0366979, acc 0.98
2016-09-07T19:59:46.505698: step 5550, loss 0.0369315, acc 0.98
2016-09-07T19:59:47.179094: step 5551, loss 0.0299312, acc 0.98
2016-09-07T19:59:47.840448: step 5552, loss 0.0843927, acc 0.96
2016-09-07T19:59:48.531200: step 5553, loss 0.0158019, acc 1
2016-09-07T19:59:49.208632: step 5554, loss 0.00838634, acc 1
2016-09-07T19:59:49.879870: step 5555, loss 0.00256204, acc 1
2016-09-07T19:59:50.540023: step 5556, loss 0.000181615, acc 1
2016-09-07T19:59:51.227700: step 5557, loss 0.0391986, acc 0.96
2016-09-07T19:59:51.915621: step 5558, loss 0.0113053, acc 1
2016-09-07T19:59:52.588451: step 5559, loss 0.131658, acc 0.98
2016-09-07T19:59:53.242369: step 5560, loss 0.0439913, acc 0.96
2016-09-07T19:59:53.908768: step 5561, loss 0.0266192, acc 1
2016-09-07T19:59:54.583636: step 5562, loss 0.00368103, acc 1
2016-09-07T19:59:55.259501: step 5563, loss 0.0503904, acc 0.98
2016-09-07T19:59:55.912032: step 5564, loss 0.0460614, acc 0.98
2016-09-07T19:59:56.570052: step 5565, loss 0.000399927, acc 1
2016-09-07T19:59:57.239181: step 5566, loss 0.0163145, acc 0.98
2016-09-07T19:59:57.921262: step 5567, loss 0.0770761, acc 0.98
2016-09-07T19:59:58.595341: step 5568, loss 0.082956, acc 0.98
2016-09-07T19:59:59.262470: step 5569, loss 0.0523875, acc 0.98
2016-09-07T19:59:59.941817: step 5570, loss 0.000989789, acc 1
2016-09-07T20:00:00.674988: step 5571, loss 0.00797387, acc 1
2016-09-07T20:00:01.348397: step 5572, loss 0.0413662, acc 0.96
2016-09-07T20:00:02.024763: step 5573, loss 0.00121008, acc 1
2016-09-07T20:00:02.701531: step 5574, loss 0.0054786, acc 1
2016-09-07T20:00:03.363255: step 5575, loss 0.0325095, acc 1
2016-09-07T20:00:04.036520: step 5576, loss 0.0299482, acc 0.98
2016-09-07T20:00:04.724118: step 5577, loss 0.0224423, acc 0.98
2016-09-07T20:00:05.371033: step 5578, loss 0.0260074, acc 0.98
2016-09-07T20:00:06.048451: step 5579, loss 0.018413, acc 0.98
2016-09-07T20:00:06.725675: step 5580, loss 0.0494109, acc 0.98
2016-09-07T20:00:07.390124: step 5581, loss 0.0304772, acc 0.98
2016-09-07T20:00:08.057786: step 5582, loss 0.0242059, acc 0.98
2016-09-07T20:00:08.743713: step 5583, loss 0.0319254, acc 1
2016-09-07T20:00:09.411374: step 5584, loss 0.0140936, acc 1
2016-09-07T20:00:10.142386: step 5585, loss 0.0480843, acc 0.98
2016-09-07T20:00:10.802429: step 5586, loss 0.034305, acc 0.98
2016-09-07T20:00:11.484044: step 5587, loss 0.0588937, acc 0.98
2016-09-07T20:00:12.183323: step 5588, loss 0.00785315, acc 1
2016-09-07T20:00:12.906948: step 5589, loss 0.0112832, acc 1
2016-09-07T20:00:13.592027: step 5590, loss 0.0136759, acc 1
2016-09-07T20:00:14.262221: step 5591, loss 0.0010227, acc 1
2016-09-07T20:00:14.948673: step 5592, loss 0.149817, acc 0.98
2016-09-07T20:00:15.621138: step 5593, loss 0.0315278, acc 0.98
2016-09-07T20:00:16.334808: step 5594, loss 0.0186058, acc 0.98
2016-09-07T20:00:17.001092: step 5595, loss 0.000350677, acc 1
2016-09-07T20:00:17.689179: step 5596, loss 0.0509183, acc 0.98
2016-09-07T20:00:18.349435: step 5597, loss 0.0309976, acc 0.98
2016-09-07T20:00:19.038458: step 5598, loss 0.0208214, acc 0.98
2016-09-07T20:00:19.729448: step 5599, loss 4.97993e-05, acc 1
2016-09-07T20:00:20.405640: step 5600, loss 0.032152, acc 0.96

Evaluation:
2016-09-07T20:00:23.712240: step 5600, loss 2.33353, acc 0.739

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-5600

2016-09-07T20:00:25.477813: step 5601, loss 0.0617996, acc 0.98
2016-09-07T20:00:26.147936: step 5602, loss 0.0413282, acc 0.98
2016-09-07T20:00:26.815990: step 5603, loss 0.0182311, acc 0.98
2016-09-07T20:00:27.495139: step 5604, loss 0.0271296, acc 0.98
2016-09-07T20:00:28.146713: step 5605, loss 0.00625565, acc 1
2016-09-07T20:00:28.824978: step 5606, loss 0.0389628, acc 0.96
2016-09-07T20:00:29.504598: step 5607, loss 0.0189295, acc 0.98
2016-09-07T20:00:30.186489: step 5608, loss 0.0270029, acc 0.98
2016-09-07T20:00:30.850606: step 5609, loss 0.00352086, acc 1
2016-09-07T20:00:31.508001: step 5610, loss 0.0422946, acc 0.98
2016-09-07T20:00:32.164945: step 5611, loss 0.0281039, acc 0.98
2016-09-07T20:00:32.829327: step 5612, loss 0.0221506, acc 0.98
2016-09-07T20:00:33.505800: step 5613, loss 0.00447291, acc 1
2016-09-07T20:00:34.180659: step 5614, loss 0.160919, acc 0.98
2016-09-07T20:00:34.852990: step 5615, loss 0.015096, acc 0.98
2016-09-07T20:00:35.525573: step 5616, loss 0.0162888, acc 1
2016-09-07T20:00:36.215576: step 5617, loss 0.009544, acc 1
2016-09-07T20:00:36.883948: step 5618, loss 0.000497475, acc 1
2016-09-07T20:00:37.558037: step 5619, loss 0.0261207, acc 1
2016-09-07T20:00:38.231320: step 5620, loss 0.00137445, acc 1
2016-09-07T20:00:38.890028: step 5621, loss 0.0346128, acc 0.98
2016-09-07T20:00:39.548031: step 5622, loss 0.0581193, acc 0.96
2016-09-07T20:00:40.230023: step 5623, loss 0.0303557, acc 0.98
2016-09-07T20:00:40.914658: step 5624, loss 0.020026, acc 0.98
2016-09-07T20:00:41.598464: step 5625, loss 0.000155663, acc 1
2016-09-07T20:00:41.954923: step 5626, loss 0.00440967, acc 1
2016-09-07T20:00:42.630558: step 5627, loss 0.0437298, acc 0.96
2016-09-07T20:00:43.313386: step 5628, loss 0.00667307, acc 1
2016-09-07T20:00:43.989232: step 5629, loss 0.0454357, acc 0.98
2016-09-07T20:00:44.671511: step 5630, loss 0.0637476, acc 0.98
2016-09-07T20:00:45.339555: step 5631, loss 0.0323742, acc 0.98
2016-09-07T20:00:46.012000: step 5632, loss 0.000730528, acc 1
2016-09-07T20:00:46.685360: step 5633, loss 0.0144275, acc 0.98
2016-09-07T20:00:47.367460: step 5634, loss 0.0264011, acc 1
2016-09-07T20:00:48.068069: step 5635, loss 0.00952825, acc 1
2016-09-07T20:00:48.737669: step 5636, loss 0.000230851, acc 1
2016-09-07T20:00:49.505254: step 5637, loss 0.235998, acc 0.96
2016-09-07T20:00:50.161803: step 5638, loss 0.015074, acc 0.98
2016-09-07T20:00:50.828618: step 5639, loss 0.0234213, acc 1
2016-09-07T20:00:51.494809: step 5640, loss 0.041986, acc 1
2016-09-07T20:00:52.199327: step 5641, loss 0.00541867, acc 1
2016-09-07T20:00:52.858806: step 5642, loss 0.00207983, acc 1
2016-09-07T20:00:53.524521: step 5643, loss 0.0464692, acc 0.96
2016-09-07T20:00:54.217898: step 5644, loss 0.0237278, acc 1
2016-09-07T20:00:54.887984: step 5645, loss 0.0591696, acc 0.96
2016-09-07T20:00:55.572133: step 5646, loss 0.000112983, acc 1
2016-09-07T20:00:56.239881: step 5647, loss 0.0509059, acc 0.98
2016-09-07T20:00:56.911045: step 5648, loss 0.0126195, acc 1
2016-09-07T20:00:57.596022: step 5649, loss 0.0328217, acc 0.98
2016-09-07T20:00:58.289233: step 5650, loss 0.044378, acc 0.96
2016-09-07T20:00:58.953124: step 5651, loss 0.012817, acc 1
2016-09-07T20:00:59.638359: step 5652, loss 0.00138241, acc 1
2016-09-07T20:01:00.343746: step 5653, loss 0.0148346, acc 1
2016-09-07T20:01:01.010801: step 5654, loss 0.0121009, acc 1
2016-09-07T20:01:01.675043: step 5655, loss 0.026105, acc 0.98
2016-09-07T20:01:02.396429: step 5656, loss 0.0103204, acc 1
2016-09-07T20:01:03.080021: step 5657, loss 0.0228852, acc 0.98
2016-09-07T20:01:03.745263: step 5658, loss 0.0410638, acc 1
2016-09-07T20:01:04.406211: step 5659, loss 0.00901514, acc 1
2016-09-07T20:01:05.069361: step 5660, loss 0.0584041, acc 0.96
2016-09-07T20:01:05.746580: step 5661, loss 0.00147436, acc 1
2016-09-07T20:01:06.427146: step 5662, loss 0.0130983, acc 1
2016-09-07T20:01:07.115878: step 5663, loss 0.0311303, acc 1
2016-09-07T20:01:07.793856: step 5664, loss 0.023655, acc 0.98
2016-09-07T20:01:08.490059: step 5665, loss 0.0261652, acc 0.98
2016-09-07T20:01:09.156653: step 5666, loss 0.0128514, acc 1
2016-09-07T20:01:09.848406: step 5667, loss 0.0138455, acc 1
2016-09-07T20:01:10.520015: step 5668, loss 0.00305658, acc 1
2016-09-07T20:01:11.180192: step 5669, loss 0.0173279, acc 1
2016-09-07T20:01:11.837487: step 5670, loss 0.0110801, acc 1
2016-09-07T20:01:12.511633: step 5671, loss 0.0280061, acc 1
2016-09-07T20:01:13.189451: step 5672, loss 0.061822, acc 0.96
2016-09-07T20:01:13.907670: step 5673, loss 0.0376443, acc 0.98
2016-09-07T20:01:14.574922: step 5674, loss 0.0693768, acc 0.96
2016-09-07T20:01:15.244951: step 5675, loss 0.00040583, acc 1
2016-09-07T20:01:15.925095: step 5676, loss 0.0186709, acc 0.98
2016-09-07T20:01:16.585899: step 5677, loss 0.00444814, acc 1
2016-09-07T20:01:17.321932: step 5678, loss 0.00977431, acc 1
2016-09-07T20:01:17.990596: step 5679, loss 0.0474167, acc 0.98
2016-09-07T20:01:18.699836: step 5680, loss 0.0148039, acc 1
2016-09-07T20:01:19.371210: step 5681, loss 0.0146171, acc 0.98
2016-09-07T20:01:20.039310: step 5682, loss 0.0201625, acc 1
2016-09-07T20:01:20.711349: step 5683, loss 0.0178312, acc 1
2016-09-07T20:01:21.381381: step 5684, loss 0.0386249, acc 0.98
2016-09-07T20:01:22.055947: step 5685, loss 0.0542346, acc 0.96
2016-09-07T20:01:22.730858: step 5686, loss 0.00066485, acc 1
2016-09-07T20:01:23.436890: step 5687, loss 0.0122693, acc 1
2016-09-07T20:01:24.128815: step 5688, loss 0.0193616, acc 1
2016-09-07T20:01:24.815175: step 5689, loss 0.00320846, acc 1
2016-09-07T20:01:25.490069: step 5690, loss 0.0496691, acc 1
2016-09-07T20:01:26.175511: step 5691, loss 0.0107454, acc 1
2016-09-07T20:01:26.845746: step 5692, loss 0.0135399, acc 1
2016-09-07T20:01:27.516625: step 5693, loss 0.0198443, acc 1
2016-09-07T20:01:28.206541: step 5694, loss 0.00692125, acc 1
2016-09-07T20:01:28.873276: step 5695, loss 0.0315251, acc 0.98
2016-09-07T20:01:29.550107: step 5696, loss 0.0584133, acc 0.98
2016-09-07T20:01:30.237566: step 5697, loss 0.00257859, acc 1
2016-09-07T20:01:30.887947: step 5698, loss 0.0132761, acc 1
2016-09-07T20:01:31.560556: step 5699, loss 0.000542795, acc 1
2016-09-07T20:01:32.248332: step 5700, loss 0.0232238, acc 0.98

Evaluation:
2016-09-07T20:01:35.549569: step 5700, loss 2.50263, acc 0.746

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-5700

2016-09-07T20:01:37.224802: step 5701, loss 0.0164805, acc 0.98
2016-09-07T20:01:37.919144: step 5702, loss 0.0833008, acc 0.94
2016-09-07T20:01:38.585833: step 5703, loss 0.024344, acc 0.98
2016-09-07T20:01:39.259461: step 5704, loss 0.000140137, acc 1
2016-09-07T20:01:39.941077: step 5705, loss 0.240878, acc 0.96
2016-09-07T20:01:40.624203: step 5706, loss 0.025924, acc 0.98
2016-09-07T20:01:41.278102: step 5707, loss 0.187144, acc 0.96
2016-09-07T20:01:41.942709: step 5708, loss 0.142427, acc 0.96
2016-09-07T20:01:42.619369: step 5709, loss 0.102389, acc 0.96
2016-09-07T20:01:43.283134: step 5710, loss 0.000499723, acc 1
2016-09-07T20:01:43.969412: step 5711, loss 0.0157084, acc 1
2016-09-07T20:01:44.652160: step 5712, loss 0.0337407, acc 0.98
2016-09-07T20:01:45.332303: step 5713, loss 0.0051352, acc 1
2016-09-07T20:01:46.014640: step 5714, loss 0.0453939, acc 0.98
2016-09-07T20:01:46.695532: step 5715, loss 0.0372232, acc 0.96
2016-09-07T20:01:47.334043: step 5716, loss 0.02963, acc 0.96
2016-09-07T20:01:48.014853: step 5717, loss 0.000419445, acc 1
2016-09-07T20:01:48.694794: step 5718, loss 0.0322048, acc 0.98
2016-09-07T20:01:49.451743: step 5719, loss 0.0011431, acc 1
2016-09-07T20:01:50.114441: step 5720, loss 0.0466574, acc 0.96
2016-09-07T20:01:50.792437: step 5721, loss 0.125924, acc 0.96
2016-09-07T20:01:51.486889: step 5722, loss 0.0058093, acc 1
2016-09-07T20:01:52.155032: step 5723, loss 0.0124767, acc 1
2016-09-07T20:01:52.841775: step 5724, loss 0.0444681, acc 0.98
2016-09-07T20:01:53.504988: step 5725, loss 0.0290547, acc 0.98
2016-09-07T20:01:54.148900: step 5726, loss 0.00807325, acc 1
2016-09-07T20:01:54.818584: step 5727, loss 0.0107666, acc 1
2016-09-07T20:01:55.484055: step 5728, loss 0.0867542, acc 0.98
2016-09-07T20:01:56.157747: step 5729, loss 0.00399269, acc 1
2016-09-07T20:01:56.824440: step 5730, loss 0.00820701, acc 1
2016-09-07T20:01:57.522263: step 5731, loss 0.0200844, acc 0.98
2016-09-07T20:01:58.184819: step 5732, loss 0.0053595, acc 1
2016-09-07T20:01:58.869580: step 5733, loss 0.023678, acc 0.98
2016-09-07T20:01:59.535255: step 5734, loss 0.0238491, acc 1
2016-09-07T20:02:00.248811: step 5735, loss 0.0409633, acc 0.96
2016-09-07T20:02:00.895346: step 5736, loss 0.0340685, acc 0.98
2016-09-07T20:02:01.572391: step 5737, loss 0.0371848, acc 0.98
2016-09-07T20:02:02.250398: step 5738, loss 0.0999345, acc 0.96
2016-09-07T20:02:02.912479: step 5739, loss 0.0298107, acc 0.98
2016-09-07T20:02:03.590132: step 5740, loss 0.0133421, acc 1
2016-09-07T20:02:04.255095: step 5741, loss 0.00328606, acc 1
2016-09-07T20:02:04.924290: step 5742, loss 0.0334016, acc 1
2016-09-07T20:02:05.617021: step 5743, loss 0.0179007, acc 1
2016-09-07T20:02:06.301012: step 5744, loss 0.0345407, acc 0.96
2016-09-07T20:02:06.991928: step 5745, loss 0.0100268, acc 1
2016-09-07T20:02:07.659199: step 5746, loss 0.0232153, acc 1
2016-09-07T20:02:08.346850: step 5747, loss 0.0495903, acc 0.98
2016-09-07T20:02:09.052567: step 5748, loss 0.0497548, acc 0.98
2016-09-07T20:02:09.720973: step 5749, loss 0.000947551, acc 1
2016-09-07T20:02:10.390046: step 5750, loss 0.0509819, acc 0.98
2016-09-07T20:02:11.062998: step 5751, loss 0.0811962, acc 0.96
2016-09-07T20:02:11.725077: step 5752, loss 0.0770406, acc 0.96
2016-09-07T20:02:12.385610: step 5753, loss 0.0654789, acc 0.98
2016-09-07T20:02:13.065538: step 5754, loss 0.0275352, acc 0.98
2016-09-07T20:02:13.749281: step 5755, loss 0.0576834, acc 0.98
2016-09-07T20:02:14.420528: step 5756, loss 0.05268, acc 0.98
2016-09-07T20:02:15.105968: step 5757, loss 0.0421061, acc 0.98
2016-09-07T20:02:15.762686: step 5758, loss 0.0352614, acc 0.98
2016-09-07T20:02:16.421693: step 5759, loss 0.0105777, acc 1
2016-09-07T20:02:17.101403: step 5760, loss 0.0431097, acc 0.96
2016-09-07T20:02:17.767549: step 5761, loss 0.0181816, acc 1
2016-09-07T20:02:18.450785: step 5762, loss 0.00339934, acc 1
2016-09-07T20:02:19.130243: step 5763, loss 0.0395201, acc 0.98
2016-09-07T20:02:19.811819: step 5764, loss 0.00672829, acc 1
2016-09-07T20:02:20.484184: step 5765, loss 0.0291626, acc 0.98
2016-09-07T20:02:21.167767: step 5766, loss 0.00834135, acc 1
2016-09-07T20:02:21.866237: step 5767, loss 0.0100889, acc 1
2016-09-07T20:02:22.548420: step 5768, loss 0.00909905, acc 1
2016-09-07T20:02:23.220825: step 5769, loss 0.0207273, acc 1
2016-09-07T20:02:23.889297: step 5770, loss 0.0621397, acc 0.96
2016-09-07T20:02:24.544215: step 5771, loss 0.0103014, acc 1
2016-09-07T20:02:25.223007: step 5772, loss 0.0155399, acc 1
2016-09-07T20:02:25.923540: step 5773, loss 0.0107498, acc 1
2016-09-07T20:02:26.608128: step 5774, loss 0.057274, acc 0.98
2016-09-07T20:02:27.290241: step 5775, loss 0.00624071, acc 1
2016-09-07T20:02:27.981853: step 5776, loss 0.0170385, acc 0.98
2016-09-07T20:02:28.665235: step 5777, loss 0.0136979, acc 1
2016-09-07T20:02:29.328664: step 5778, loss 0.0323424, acc 0.98
2016-09-07T20:02:30.097485: step 5779, loss 0.0358562, acc 0.98
2016-09-07T20:02:30.767756: step 5780, loss 0.0609335, acc 0.98
2016-09-07T20:02:31.456329: step 5781, loss 0.0379869, acc 0.98
2016-09-07T20:02:32.123960: step 5782, loss 0.00462013, acc 1
2016-09-07T20:02:32.801207: step 5783, loss 0.00140232, acc 1
2016-09-07T20:02:33.500557: step 5784, loss 0.0300493, acc 0.98
2016-09-07T20:02:34.178775: step 5785, loss 0.054551, acc 0.96
2016-09-07T20:02:34.857601: step 5786, loss 0.0154186, acc 1
2016-09-07T20:02:35.542811: step 5787, loss 0.039808, acc 1
2016-09-07T20:02:36.224751: step 5788, loss 0.0142329, acc 1
2016-09-07T20:02:36.924111: step 5789, loss 0.000331731, acc 1
2016-09-07T20:02:37.589386: step 5790, loss 0.0361799, acc 0.98
2016-09-07T20:02:38.279732: step 5791, loss 0.00384589, acc 1
2016-09-07T20:02:38.966765: step 5792, loss 0.0184896, acc 1
2016-09-07T20:02:39.640309: step 5793, loss 0.00749836, acc 1
2016-09-07T20:02:40.298444: step 5794, loss 0.0680601, acc 0.96
2016-09-07T20:02:40.984943: step 5795, loss 0.0380851, acc 1
2016-09-07T20:02:41.683564: step 5796, loss 0.080348, acc 0.96
2016-09-07T20:02:42.377215: step 5797, loss 0.0497862, acc 0.96
2016-09-07T20:02:43.057142: step 5798, loss 0.0568781, acc 0.96
2016-09-07T20:02:43.737513: step 5799, loss 0.0608388, acc 0.96
2016-09-07T20:02:44.424336: step 5800, loss 0.00904867, acc 1

Evaluation:
2016-09-07T20:02:47.765957: step 5800, loss 2.31948, acc 0.75

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473245614/checkpoints/model-5800

2016-09-07T20:02:49.496762: step 5801, loss 0.0264568, acc 1
2016-09-07T20:02:50.178650: step 5802, loss 0.0183715, acc 1
2016-09-07T20:02:50.857235: step 5803, loss 0.0225653, acc 0.98
2016-09-07T20:02:51.547980: step 5804, loss 0.0251986, acc 1
2016-09-07T20:02:52.237440: step 5805, loss 0.0260187, acc 0.98
2016-09-07T20:02:52.928020: step 5806, loss 0.0197819, acc 1
2016-09-07T20:02:53.616680: step 5807, loss 0.015468, acc 1
2016-09-07T20:02:54.296126: step 5808, loss 0.00493542, acc 1
2016-09-07T20:02:54.985233: step 5809, loss 0.0189201, acc 1
2016-09-07T20:02:55.667508: step 5810, loss 0.022124, acc 0.98
2016-09-07T20:02:56.366175: step 5811, loss 0.00792199, acc 1
2016-09-07T20:02:57.048493: step 5812, loss 0.0757317, acc 0.98
2016-09-07T20:02:57.724948: step 5813, loss 0.0342981, acc 0.98
2016-09-07T20:02:58.415308: step 5814, loss 0.0242674, acc 0.98
2016-09-07T20:02:59.085894: step 5815, loss 0.0264825, acc 0.98
2016-09-07T20:02:59.751909: step 5816, loss 0.197983, acc 0.96
2016-09-07T20:03:00.478292: step 5817, loss 0.0178549, acc 1
2016-09-07T20:03:01.273695: step 5818, loss 0.0765335, acc 0.98
2016-09-07T20:03:01.955745: step 5819, loss 3.93257e-05, acc 1
2016-09-07T20:03:02.332238: step 5820, loss 0.0359182, acc 1

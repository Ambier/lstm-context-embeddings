WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f1f87147e90>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.
WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f1f87147ed0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.

Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=50
CHECKPOINT_EVERY=100
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=300
EVALUATE_EVERY=100
FILTER_SIZES=3,4,5
HIDDEN_DIM=300
L2_REG_LAMBDA=0.15
LOG_DEVICE_PLACEMENT=False
NUM_EPOCHS=30
NUM_FILTERS=100
WORD2VEC=GoogleNews-vectors-negative300.bin

Loading data...
Vocabulary Size: 18758
Train/Dev split: 9662/1000
Writing to /home/cil/lstm-context-embeddings/runs/1473249784

Load word2vec file GoogleNews-vectors-negative300.bin

2016-09-07T20:03:22.804163: step 1, loss 0.693147, acc 0.44
2016-09-07T20:03:23.457106: step 2, loss 0.723116, acc 0.44
2016-09-07T20:03:24.106555: step 3, loss 0.702961, acc 0.4
2016-09-07T20:03:24.778872: step 4, loss 0.691544, acc 0.52
2016-09-07T20:03:25.442960: step 5, loss 0.667002, acc 0.64
2016-09-07T20:03:26.114721: step 6, loss 0.697833, acc 0.56
2016-09-07T20:03:26.774606: step 7, loss 0.744332, acc 0.46
2016-09-07T20:03:27.435520: step 8, loss 0.759044, acc 0.42
2016-09-07T20:03:28.099877: step 9, loss 0.678473, acc 0.6
2016-09-07T20:03:28.774774: step 10, loss 0.693762, acc 0.5
2016-09-07T20:03:29.452727: step 11, loss 0.695238, acc 0.48
2016-09-07T20:03:30.110206: step 12, loss 0.69657, acc 0.46
2016-09-07T20:03:30.781461: step 13, loss 0.695141, acc 0.58
2016-09-07T20:03:31.440990: step 14, loss 0.685783, acc 0.54
2016-09-07T20:03:32.127599: step 15, loss 0.690281, acc 0.54
2016-09-07T20:03:32.788669: step 16, loss 0.705024, acc 0.52
2016-09-07T20:03:33.467915: step 17, loss 0.683065, acc 0.5
2016-09-07T20:03:34.136711: step 18, loss 0.733123, acc 0.36
2016-09-07T20:03:34.798839: step 19, loss 0.697461, acc 0.44
2016-09-07T20:03:35.481315: step 20, loss 0.685735, acc 0.46
2016-09-07T20:03:36.144805: step 21, loss 0.677246, acc 0.58
2016-09-07T20:03:36.812088: step 22, loss 0.680615, acc 0.54
2016-09-07T20:03:37.487717: step 23, loss 0.691625, acc 0.52
2016-09-07T20:03:38.140126: step 24, loss 0.715102, acc 0.48
2016-09-07T20:03:38.825213: step 25, loss 0.701638, acc 0.5
2016-09-07T20:03:39.500485: step 26, loss 0.692706, acc 0.5
2016-09-07T20:03:40.169542: step 27, loss 0.689674, acc 0.52
2016-09-07T20:03:40.841619: step 28, loss 0.681534, acc 0.62
2016-09-07T20:03:41.522636: step 29, loss 0.679916, acc 0.5
2016-09-07T20:03:42.185773: step 30, loss 0.702522, acc 0.48
2016-09-07T20:03:42.836897: step 31, loss 0.6322, acc 0.68
2016-09-07T20:03:43.497329: step 32, loss 0.648677, acc 0.56
2016-09-07T20:03:44.160347: step 33, loss 0.662155, acc 0.52
2016-09-07T20:03:44.836793: step 34, loss 0.626753, acc 0.66
2016-09-07T20:03:45.511074: step 35, loss 0.627893, acc 0.6
2016-09-07T20:03:46.170992: step 36, loss 0.727618, acc 0.62
2016-09-07T20:03:46.841642: step 37, loss 0.6435, acc 0.68
2016-09-07T20:03:47.526573: step 38, loss 0.688459, acc 0.64
2016-09-07T20:03:48.191857: step 39, loss 0.611386, acc 0.7
2016-09-07T20:03:48.834025: step 40, loss 0.611718, acc 0.66
2016-09-07T20:03:49.514738: step 41, loss 0.596855, acc 0.66
2016-09-07T20:03:50.182205: step 42, loss 0.595913, acc 0.7
2016-09-07T20:03:50.843936: step 43, loss 0.594205, acc 0.72
2016-09-07T20:03:51.515185: step 44, loss 0.602397, acc 0.64
2016-09-07T20:03:52.182470: step 45, loss 0.614422, acc 0.66
2016-09-07T20:03:52.847971: step 46, loss 0.524379, acc 0.78
2016-09-07T20:03:53.493413: step 47, loss 0.505871, acc 0.76
2016-09-07T20:03:54.157553: step 48, loss 0.611945, acc 0.64
2016-09-07T20:03:54.822343: step 49, loss 0.540264, acc 0.8
2016-09-07T20:03:55.490389: step 50, loss 0.82406, acc 0.58
2016-09-07T20:03:56.158698: step 51, loss 0.717369, acc 0.64
2016-09-07T20:03:56.810795: step 52, loss 0.778434, acc 0.6
2016-09-07T20:03:57.461157: step 53, loss 0.666268, acc 0.6
2016-09-07T20:03:58.151132: step 54, loss 0.63099, acc 0.64
2016-09-07T20:03:58.816730: step 55, loss 0.598196, acc 0.68
2016-09-07T20:03:59.495232: step 56, loss 0.525482, acc 0.74
2016-09-07T20:04:00.156780: step 57, loss 0.610912, acc 0.62
2016-09-07T20:04:00.855770: step 58, loss 0.650487, acc 0.62
2016-09-07T20:04:01.522903: step 59, loss 0.584754, acc 0.72
2016-09-07T20:04:02.190741: step 60, loss 0.657511, acc 0.62
2016-09-07T20:04:02.902867: step 61, loss 0.611612, acc 0.68
2016-09-07T20:04:03.588281: step 62, loss 0.561749, acc 0.68
2016-09-07T20:04:04.241115: step 63, loss 0.633889, acc 0.68
2016-09-07T20:04:04.900402: step 64, loss 0.547402, acc 0.68
2016-09-07T20:04:05.563344: step 65, loss 0.462841, acc 0.82
2016-09-07T20:04:06.214660: step 66, loss 0.613462, acc 0.68
2016-09-07T20:04:06.875278: step 67, loss 0.54298, acc 0.7
2016-09-07T20:04:07.537618: step 68, loss 0.660628, acc 0.66
2016-09-07T20:04:08.203871: step 69, loss 0.520238, acc 0.72
2016-09-07T20:04:08.878385: step 70, loss 0.586688, acc 0.66
2016-09-07T20:04:09.553729: step 71, loss 0.595314, acc 0.68
2016-09-07T20:04:10.226236: step 72, loss 0.553853, acc 0.74
2016-09-07T20:04:10.894349: step 73, loss 0.483883, acc 0.78
2016-09-07T20:04:11.563175: step 74, loss 0.474394, acc 0.82
2016-09-07T20:04:12.244721: step 75, loss 0.549106, acc 0.74
2016-09-07T20:04:12.908851: step 76, loss 0.643931, acc 0.6
2016-09-07T20:04:13.568308: step 77, loss 0.498137, acc 0.78
2016-09-07T20:04:14.225281: step 78, loss 0.607069, acc 0.64
2016-09-07T20:04:14.908350: step 79, loss 0.528537, acc 0.72
2016-09-07T20:04:15.569165: step 80, loss 0.665711, acc 0.66
2016-09-07T20:04:16.245180: step 81, loss 0.516583, acc 0.76
2016-09-07T20:04:16.906475: step 82, loss 0.510716, acc 0.76
2016-09-07T20:04:17.617993: step 83, loss 0.576782, acc 0.74
2016-09-07T20:04:18.289316: step 84, loss 0.488597, acc 0.72
2016-09-07T20:04:18.966647: step 85, loss 0.585607, acc 0.68
2016-09-07T20:04:19.639116: step 86, loss 0.41342, acc 0.74
2016-09-07T20:04:20.317718: step 87, loss 0.645265, acc 0.64
2016-09-07T20:04:20.987962: step 88, loss 0.451033, acc 0.78
2016-09-07T20:04:21.653473: step 89, loss 0.451777, acc 0.78
2016-09-07T20:04:22.339723: step 90, loss 0.622893, acc 0.66
2016-09-07T20:04:23.020280: step 91, loss 0.513336, acc 0.72
2016-09-07T20:04:23.679610: step 92, loss 0.474238, acc 0.76
2016-09-07T20:04:24.337826: step 93, loss 0.596207, acc 0.62
2016-09-07T20:04:25.003160: step 94, loss 0.421413, acc 0.74
2016-09-07T20:04:25.689705: step 95, loss 0.40106, acc 0.82
2016-09-07T20:04:26.354092: step 96, loss 0.429233, acc 0.82
2016-09-07T20:04:27.032864: step 97, loss 0.525184, acc 0.74
2016-09-07T20:04:27.691301: step 98, loss 0.497892, acc 0.8
2016-09-07T20:04:28.361884: step 99, loss 0.633059, acc 0.68
2016-09-07T20:04:29.031696: step 100, loss 0.57586, acc 0.68

Evaluation:
2016-09-07T20:04:32.261638: step 100, loss 0.503322, acc 0.763

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-100

2016-09-07T20:04:34.006220: step 101, loss 0.500973, acc 0.86
2016-09-07T20:04:34.678643: step 102, loss 0.718913, acc 0.64
2016-09-07T20:04:35.342041: step 103, loss 0.456939, acc 0.76
2016-09-07T20:04:36.011362: step 104, loss 0.422299, acc 0.78
2016-09-07T20:04:36.691802: step 105, loss 0.505637, acc 0.74
2016-09-07T20:04:37.356802: step 106, loss 0.476952, acc 0.78
2016-09-07T20:04:38.022207: step 107, loss 0.505598, acc 0.78
2016-09-07T20:04:38.690256: step 108, loss 0.42183, acc 0.8
2016-09-07T20:04:39.368730: step 109, loss 0.516616, acc 0.72
2016-09-07T20:04:40.047942: step 110, loss 0.466609, acc 0.78
2016-09-07T20:04:40.691581: step 111, loss 0.528015, acc 0.82
2016-09-07T20:04:41.368656: step 112, loss 0.494778, acc 0.72
2016-09-07T20:04:42.029494: step 113, loss 0.499787, acc 0.76
2016-09-07T20:04:42.698016: step 114, loss 0.459221, acc 0.74
2016-09-07T20:04:43.350794: step 115, loss 0.407834, acc 0.8
2016-09-07T20:04:44.005087: step 116, loss 0.366333, acc 0.8
2016-09-07T20:04:44.664484: step 117, loss 0.784979, acc 0.6
2016-09-07T20:04:45.328981: step 118, loss 0.497178, acc 0.68
2016-09-07T20:04:45.993929: step 119, loss 0.597962, acc 0.7
2016-09-07T20:04:46.655061: step 120, loss 0.409052, acc 0.78
2016-09-07T20:04:47.314593: step 121, loss 0.648325, acc 0.72
2016-09-07T20:04:47.983723: step 122, loss 0.434809, acc 0.82
2016-09-07T20:04:48.646387: step 123, loss 0.544758, acc 0.74
2016-09-07T20:04:49.306812: step 124, loss 0.475293, acc 0.78
2016-09-07T20:04:50.010681: step 125, loss 0.6769, acc 0.68
2016-09-07T20:04:50.706563: step 126, loss 0.620476, acc 0.68
2016-09-07T20:04:51.382198: step 127, loss 0.62089, acc 0.68
2016-09-07T20:04:52.039730: step 128, loss 0.411573, acc 0.8
2016-09-07T20:04:52.699961: step 129, loss 0.44319, acc 0.8
2016-09-07T20:04:53.365080: step 130, loss 0.504098, acc 0.7
2016-09-07T20:04:54.024456: step 131, loss 0.478176, acc 0.86
2016-09-07T20:04:54.697117: step 132, loss 0.48872, acc 0.76
2016-09-07T20:04:55.356869: step 133, loss 0.580823, acc 0.66
2016-09-07T20:04:56.025345: step 134, loss 0.461026, acc 0.84
2016-09-07T20:04:56.688675: step 135, loss 0.560725, acc 0.72
2016-09-07T20:04:57.349582: step 136, loss 0.520169, acc 0.78
2016-09-07T20:04:57.998388: step 137, loss 0.473903, acc 0.74
2016-09-07T20:04:58.653661: step 138, loss 0.557921, acc 0.74
2016-09-07T20:04:59.354619: step 139, loss 0.455086, acc 0.74
2016-09-07T20:05:00.020316: step 140, loss 0.355545, acc 0.9
2016-09-07T20:05:00.720748: step 141, loss 0.563664, acc 0.68
2016-09-07T20:05:01.372994: step 142, loss 0.526885, acc 0.74
2016-09-07T20:05:02.050278: step 143, loss 0.594675, acc 0.64
2016-09-07T20:05:02.744801: step 144, loss 0.508001, acc 0.78
2016-09-07T20:05:03.403393: step 145, loss 0.46149, acc 0.8
2016-09-07T20:05:04.078472: step 146, loss 0.519977, acc 0.72
2016-09-07T20:05:04.748145: step 147, loss 0.554723, acc 0.68
2016-09-07T20:05:05.413019: step 148, loss 0.387992, acc 0.86
2016-09-07T20:05:06.082868: step 149, loss 0.543915, acc 0.74
2016-09-07T20:05:06.752513: step 150, loss 0.535737, acc 0.68
2016-09-07T20:05:07.424189: step 151, loss 0.577293, acc 0.7
2016-09-07T20:05:08.099933: step 152, loss 0.473613, acc 0.76
2016-09-07T20:05:08.766075: step 153, loss 0.601776, acc 0.72
2016-09-07T20:05:09.430411: step 154, loss 0.570946, acc 0.68
2016-09-07T20:05:10.103994: step 155, loss 0.511312, acc 0.78
2016-09-07T20:05:10.767668: step 156, loss 0.408252, acc 0.8
2016-09-07T20:05:11.420580: step 157, loss 0.468291, acc 0.7
2016-09-07T20:05:12.080752: step 158, loss 0.63175, acc 0.62
2016-09-07T20:05:12.748345: step 159, loss 0.507937, acc 0.78
2016-09-07T20:05:13.415381: step 160, loss 0.536606, acc 0.68
2016-09-07T20:05:14.075222: step 161, loss 0.568413, acc 0.72
2016-09-07T20:05:14.763543: step 162, loss 0.625381, acc 0.6
2016-09-07T20:05:15.420380: step 163, loss 0.457311, acc 0.74
2016-09-07T20:05:16.098632: step 164, loss 0.430697, acc 0.82
2016-09-07T20:05:16.769744: step 165, loss 0.483878, acc 0.82
2016-09-07T20:05:17.439571: step 166, loss 0.462564, acc 0.76
2016-09-07T20:05:18.122315: step 167, loss 0.527283, acc 0.72
2016-09-07T20:05:18.782694: step 168, loss 0.597231, acc 0.66
2016-09-07T20:05:19.454152: step 169, loss 0.551526, acc 0.7
2016-09-07T20:05:20.114129: step 170, loss 0.549993, acc 0.66
2016-09-07T20:05:20.774491: step 171, loss 0.591346, acc 0.6
2016-09-07T20:05:21.443512: step 172, loss 0.522559, acc 0.72
2016-09-07T20:05:22.108120: step 173, loss 0.540281, acc 0.72
2016-09-07T20:05:22.803272: step 174, loss 0.496847, acc 0.72
2016-09-07T20:05:23.478943: step 175, loss 0.460309, acc 0.8
2016-09-07T20:05:24.139444: step 176, loss 0.443399, acc 0.88
2016-09-07T20:05:24.815712: step 177, loss 0.375158, acc 0.84
2016-09-07T20:05:25.473231: step 178, loss 0.472635, acc 0.78
2016-09-07T20:05:26.157220: step 179, loss 0.425759, acc 0.82
2016-09-07T20:05:26.814914: step 180, loss 0.30379, acc 0.86
2016-09-07T20:05:27.484995: step 181, loss 0.544157, acc 0.66
2016-09-07T20:05:28.151470: step 182, loss 0.545613, acc 0.7
2016-09-07T20:05:28.834910: step 183, loss 0.450023, acc 0.8
2016-09-07T20:05:29.502899: step 184, loss 0.635998, acc 0.64
2016-09-07T20:05:30.178073: step 185, loss 0.599328, acc 0.74
2016-09-07T20:05:30.847763: step 186, loss 0.449407, acc 0.78
2016-09-07T20:05:31.508512: step 187, loss 0.482985, acc 0.74
2016-09-07T20:05:32.181363: step 188, loss 0.485044, acc 0.74
2016-09-07T20:05:32.838249: step 189, loss 0.421689, acc 0.82
2016-09-07T20:05:33.499751: step 190, loss 0.487506, acc 0.78
2016-09-07T20:05:34.175429: step 191, loss 0.550711, acc 0.72
2016-09-07T20:05:34.870094: step 192, loss 0.524584, acc 0.7
2016-09-07T20:05:35.543720: step 193, loss 0.420652, acc 0.82
2016-09-07T20:05:35.899460: step 194, loss 0.262475, acc 1
2016-09-07T20:05:36.554319: step 195, loss 0.44129, acc 0.82
2016-09-07T20:05:37.233681: step 196, loss 0.392597, acc 0.82
2016-09-07T20:05:37.905442: step 197, loss 0.296535, acc 0.92
2016-09-07T20:05:38.573863: step 198, loss 0.358742, acc 0.88
2016-09-07T20:05:39.250722: step 199, loss 0.346195, acc 0.82
2016-09-07T20:05:39.907493: step 200, loss 0.492254, acc 0.78

Evaluation:
2016-09-07T20:05:43.122121: step 200, loss 0.453703, acc 0.791

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-200

2016-09-07T20:05:44.838210: step 201, loss 0.400214, acc 0.78
2016-09-07T20:05:45.533440: step 202, loss 0.391185, acc 0.76
2016-09-07T20:05:46.219987: step 203, loss 0.258892, acc 0.88
2016-09-07T20:05:46.891749: step 204, loss 0.562523, acc 0.76
2016-09-07T20:05:47.567767: step 205, loss 0.380666, acc 0.84
2016-09-07T20:05:48.233374: step 206, loss 0.299068, acc 0.86
2016-09-07T20:05:48.906160: step 207, loss 0.651475, acc 0.68
2016-09-07T20:05:49.571541: step 208, loss 0.33771, acc 0.88
2016-09-07T20:05:50.238286: step 209, loss 0.283636, acc 0.88
2016-09-07T20:05:50.918648: step 210, loss 0.323967, acc 0.88
2016-09-07T20:05:51.585131: step 211, loss 0.269511, acc 0.92
2016-09-07T20:05:52.260882: step 212, loss 0.625077, acc 0.72
2016-09-07T20:05:52.949895: step 213, loss 0.294177, acc 0.86
2016-09-07T20:05:53.612760: step 214, loss 0.536412, acc 0.74
2016-09-07T20:05:54.286846: step 215, loss 0.256334, acc 0.9
2016-09-07T20:05:54.950972: step 216, loss 0.462273, acc 0.72
2016-09-07T20:05:55.607881: step 217, loss 0.342616, acc 0.86
2016-09-07T20:05:56.269544: step 218, loss 0.458217, acc 0.76
2016-09-07T20:05:56.958485: step 219, loss 0.314755, acc 0.88
2016-09-07T20:05:57.620137: step 220, loss 0.234088, acc 0.92
2016-09-07T20:05:58.281116: step 221, loss 0.272954, acc 0.9
2016-09-07T20:05:58.956066: step 222, loss 0.338251, acc 0.76
2016-09-07T20:05:59.630012: step 223, loss 0.440497, acc 0.78
2016-09-07T20:06:00.339938: step 224, loss 0.39167, acc 0.82
2016-09-07T20:06:01.008844: step 225, loss 0.463186, acc 0.76
2016-09-07T20:06:01.679842: step 226, loss 0.421184, acc 0.84
2016-09-07T20:06:02.352619: step 227, loss 0.231824, acc 0.96
2016-09-07T20:06:03.026835: step 228, loss 0.351858, acc 0.84
2016-09-07T20:06:03.690646: step 229, loss 0.239329, acc 0.88
2016-09-07T20:06:04.367534: step 230, loss 0.401648, acc 0.78
2016-09-07T20:06:05.034435: step 231, loss 0.345266, acc 0.8
2016-09-07T20:06:05.715229: step 232, loss 0.302769, acc 0.86
2016-09-07T20:06:06.410763: step 233, loss 0.489457, acc 0.74
2016-09-07T20:06:07.089993: step 234, loss 0.506316, acc 0.8
2016-09-07T20:06:07.773489: step 235, loss 0.279452, acc 0.92
2016-09-07T20:06:08.450107: step 236, loss 0.261845, acc 0.9
2016-09-07T20:06:09.126899: step 237, loss 0.452209, acc 0.76
2016-09-07T20:06:09.807284: step 238, loss 0.383142, acc 0.8
2016-09-07T20:06:10.465841: step 239, loss 0.517064, acc 0.76
2016-09-07T20:06:11.148549: step 240, loss 0.351855, acc 0.86
2016-09-07T20:06:11.829864: step 241, loss 0.332069, acc 0.88
2016-09-07T20:06:12.497650: step 242, loss 0.428633, acc 0.84
2016-09-07T20:06:13.166828: step 243, loss 0.493393, acc 0.78
2016-09-07T20:06:13.855489: step 244, loss 0.295259, acc 0.86
2016-09-07T20:06:14.528163: step 245, loss 0.248118, acc 0.9
2016-09-07T20:06:15.198418: step 246, loss 0.309535, acc 0.82
2016-09-07T20:06:15.871318: step 247, loss 0.309847, acc 0.88
2016-09-07T20:06:16.551873: step 248, loss 0.308965, acc 0.86
2016-09-07T20:06:17.246880: step 249, loss 0.391335, acc 0.8
2016-09-07T20:06:17.914931: step 250, loss 0.389839, acc 0.82
2016-09-07T20:06:18.599748: step 251, loss 0.521435, acc 0.78
2016-09-07T20:06:19.279571: step 252, loss 0.297301, acc 0.82
2016-09-07T20:06:19.956886: step 253, loss 0.401098, acc 0.8
2016-09-07T20:06:20.638096: step 254, loss 0.640553, acc 0.74
2016-09-07T20:06:21.348862: step 255, loss 0.448248, acc 0.78
2016-09-07T20:06:22.035067: step 256, loss 0.297852, acc 0.88
2016-09-07T20:06:22.704490: step 257, loss 0.34935, acc 0.86
2016-09-07T20:06:23.388504: step 258, loss 0.505127, acc 0.7
2016-09-07T20:06:24.056732: step 259, loss 0.422486, acc 0.8
2016-09-07T20:06:24.719986: step 260, loss 0.325944, acc 0.86
2016-09-07T20:06:25.390987: step 261, loss 0.270749, acc 0.92
2016-09-07T20:06:26.070774: step 262, loss 0.329746, acc 0.86
2016-09-07T20:06:26.731126: step 263, loss 0.428401, acc 0.8
2016-09-07T20:06:27.401352: step 264, loss 0.319788, acc 0.84
2016-09-07T20:06:28.070935: step 265, loss 0.348409, acc 0.9
2016-09-07T20:06:28.740397: step 266, loss 0.329781, acc 0.86
2016-09-07T20:06:29.404944: step 267, loss 0.230239, acc 0.92
2016-09-07T20:06:30.086141: step 268, loss 0.396074, acc 0.78
2016-09-07T20:06:30.758026: step 269, loss 0.308288, acc 0.86
2016-09-07T20:06:31.431139: step 270, loss 0.298013, acc 0.88
2016-09-07T20:06:32.104357: step 271, loss 0.335435, acc 0.82
2016-09-07T20:06:32.774779: step 272, loss 0.329433, acc 0.82
2016-09-07T20:06:33.453399: step 273, loss 0.351237, acc 0.84
2016-09-07T20:06:34.116884: step 274, loss 0.499638, acc 0.82
2016-09-07T20:06:34.786958: step 275, loss 0.373543, acc 0.78
2016-09-07T20:06:35.456459: step 276, loss 0.419012, acc 0.78
2016-09-07T20:06:36.133231: step 277, loss 0.433722, acc 0.76
2016-09-07T20:06:36.818784: step 278, loss 0.331175, acc 0.86
2016-09-07T20:06:37.504024: step 279, loss 0.421093, acc 0.82
2016-09-07T20:06:38.183825: step 280, loss 0.529863, acc 0.78
2016-09-07T20:06:38.862858: step 281, loss 0.358532, acc 0.82
2016-09-07T20:06:39.548921: step 282, loss 0.283495, acc 0.94
2016-09-07T20:06:40.216451: step 283, loss 0.431711, acc 0.84
2016-09-07T20:06:40.885732: step 284, loss 0.413152, acc 0.8
2016-09-07T20:06:41.562687: step 285, loss 0.380064, acc 0.84
2016-09-07T20:06:42.234580: step 286, loss 0.385232, acc 0.8
2016-09-07T20:06:42.909202: step 287, loss 0.363603, acc 0.82
2016-09-07T20:06:43.570137: step 288, loss 0.378583, acc 0.82
2016-09-07T20:06:44.243619: step 289, loss 0.40392, acc 0.76
2016-09-07T20:06:44.912055: step 290, loss 0.304839, acc 0.86
2016-09-07T20:06:45.580141: step 291, loss 0.270443, acc 0.94
2016-09-07T20:06:46.262632: step 292, loss 0.27933, acc 0.94
2016-09-07T20:06:46.937068: step 293, loss 0.484127, acc 0.76
2016-09-07T20:06:47.608115: step 294, loss 0.305546, acc 0.84
2016-09-07T20:06:48.280301: step 295, loss 0.348451, acc 0.86
2016-09-07T20:06:48.956336: step 296, loss 0.279891, acc 0.88
2016-09-07T20:06:49.631107: step 297, loss 0.348091, acc 0.84
2016-09-07T20:06:50.308768: step 298, loss 0.291821, acc 0.88
2016-09-07T20:06:50.980200: step 299, loss 0.551089, acc 0.76
2016-09-07T20:06:51.663139: step 300, loss 0.267457, acc 0.88

Evaluation:
2016-09-07T20:06:54.952728: step 300, loss 0.466393, acc 0.798

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-300

2016-09-07T20:06:56.570372: step 301, loss 0.362851, acc 0.8
2016-09-07T20:06:57.231983: step 302, loss 0.255313, acc 0.88
2016-09-07T20:06:57.895534: step 303, loss 0.460412, acc 0.88
2016-09-07T20:06:58.574326: step 304, loss 0.348843, acc 0.9
2016-09-07T20:06:59.231612: step 305, loss 0.41849, acc 0.78
2016-09-07T20:06:59.884116: step 306, loss 0.291539, acc 0.86
2016-09-07T20:07:00.579384: step 307, loss 0.361822, acc 0.86
2016-09-07T20:07:01.240492: step 308, loss 0.358754, acc 0.84
2016-09-07T20:07:01.899752: step 309, loss 0.397663, acc 0.84
2016-09-07T20:07:02.564737: step 310, loss 0.309527, acc 0.84
2016-09-07T20:07:03.236489: step 311, loss 0.297712, acc 0.86
2016-09-07T20:07:03.902269: step 312, loss 0.279734, acc 0.92
2016-09-07T20:07:04.567555: step 313, loss 0.361829, acc 0.86
2016-09-07T20:07:05.231205: step 314, loss 0.52272, acc 0.82
2016-09-07T20:07:05.898082: step 315, loss 0.48143, acc 0.74
2016-09-07T20:07:06.579695: step 316, loss 0.391775, acc 0.8
2016-09-07T20:07:07.262330: step 317, loss 0.392471, acc 0.82
2016-09-07T20:07:07.936300: step 318, loss 0.262038, acc 0.92
2016-09-07T20:07:08.632630: step 319, loss 0.262663, acc 0.9
2016-09-07T20:07:09.313371: step 320, loss 0.39232, acc 0.8
2016-09-07T20:07:09.983623: step 321, loss 0.275191, acc 0.9
2016-09-07T20:07:10.654001: step 322, loss 0.386202, acc 0.8
2016-09-07T20:07:11.344523: step 323, loss 0.401458, acc 0.8
2016-09-07T20:07:12.017552: step 324, loss 0.280333, acc 0.88
2016-09-07T20:07:12.680387: step 325, loss 0.522228, acc 0.76
2016-09-07T20:07:13.350272: step 326, loss 0.556764, acc 0.82
2016-09-07T20:07:14.006707: step 327, loss 0.392815, acc 0.82
2016-09-07T20:07:14.687308: step 328, loss 0.41075, acc 0.76
2016-09-07T20:07:15.358259: step 329, loss 0.384763, acc 0.8
2016-09-07T20:07:16.019260: step 330, loss 0.415754, acc 0.8
2016-09-07T20:07:16.705550: step 331, loss 0.222374, acc 0.88
2016-09-07T20:07:17.364690: step 332, loss 0.29449, acc 0.9
2016-09-07T20:07:18.030503: step 333, loss 0.312292, acc 0.88
2016-09-07T20:07:18.698483: step 334, loss 0.289363, acc 0.9
2016-09-07T20:07:19.363879: step 335, loss 0.310091, acc 0.86
2016-09-07T20:07:20.055821: step 336, loss 0.413758, acc 0.82
2016-09-07T20:07:20.719368: step 337, loss 0.396561, acc 0.84
2016-09-07T20:07:21.388131: step 338, loss 0.44975, acc 0.72
2016-09-07T20:07:22.059127: step 339, loss 0.311336, acc 0.88
2016-09-07T20:07:22.735453: step 340, loss 0.317237, acc 0.82
2016-09-07T20:07:23.396029: step 341, loss 0.311839, acc 0.86
2016-09-07T20:07:24.054397: step 342, loss 0.348231, acc 0.8
2016-09-07T20:07:24.747290: step 343, loss 0.306069, acc 0.86
2016-09-07T20:07:25.419726: step 344, loss 0.441487, acc 0.76
2016-09-07T20:07:26.111868: step 345, loss 0.389453, acc 0.84
2016-09-07T20:07:26.781440: step 346, loss 0.505367, acc 0.78
2016-09-07T20:07:27.443756: step 347, loss 0.308934, acc 0.88
2016-09-07T20:07:28.117427: step 348, loss 0.293346, acc 0.86
2016-09-07T20:07:28.787971: step 349, loss 0.42264, acc 0.86
2016-09-07T20:07:29.473277: step 350, loss 0.304698, acc 0.84
2016-09-07T20:07:30.152440: step 351, loss 0.524632, acc 0.72
2016-09-07T20:07:30.810260: step 352, loss 0.383548, acc 0.82
2016-09-07T20:07:31.488273: step 353, loss 0.376994, acc 0.8
2016-09-07T20:07:32.174396: step 354, loss 0.415965, acc 0.78
2016-09-07T20:07:32.826815: step 355, loss 0.327501, acc 0.88
2016-09-07T20:07:33.500413: step 356, loss 0.357647, acc 0.82
2016-09-07T20:07:34.170820: step 357, loss 0.41695, acc 0.8
2016-09-07T20:07:34.837738: step 358, loss 0.347753, acc 0.78
2016-09-07T20:07:35.534459: step 359, loss 0.468267, acc 0.82
2016-09-07T20:07:36.197901: step 360, loss 0.491087, acc 0.82
2016-09-07T20:07:36.862378: step 361, loss 0.42355, acc 0.8
2016-09-07T20:07:37.529844: step 362, loss 0.372638, acc 0.84
2016-09-07T20:07:38.209495: step 363, loss 0.270901, acc 0.92
2016-09-07T20:07:38.886907: step 364, loss 0.350914, acc 0.84
2016-09-07T20:07:39.556405: step 365, loss 0.451565, acc 0.74
2016-09-07T20:07:40.246861: step 366, loss 0.387619, acc 0.84
2016-09-07T20:07:40.916735: step 367, loss 0.367238, acc 0.86
2016-09-07T20:07:41.596121: step 368, loss 0.347593, acc 0.86
2016-09-07T20:07:42.279051: step 369, loss 0.40198, acc 0.82
2016-09-07T20:07:42.949664: step 370, loss 0.404994, acc 0.8
2016-09-07T20:07:43.622437: step 371, loss 0.421349, acc 0.78
2016-09-07T20:07:44.302495: step 372, loss 0.333471, acc 0.82
2016-09-07T20:07:44.983774: step 373, loss 0.546361, acc 0.76
2016-09-07T20:07:45.665582: step 374, loss 0.473169, acc 0.84
2016-09-07T20:07:46.329321: step 375, loss 0.367182, acc 0.86
2016-09-07T20:07:47.007621: step 376, loss 0.310817, acc 0.9
2016-09-07T20:07:47.687272: step 377, loss 0.471331, acc 0.74
2016-09-07T20:07:48.367340: step 378, loss 0.332178, acc 0.86
2016-09-07T20:07:49.074867: step 379, loss 0.337635, acc 0.86
2016-09-07T20:07:49.745213: step 380, loss 0.483982, acc 0.8
2016-09-07T20:07:50.412083: step 381, loss 0.423809, acc 0.8
2016-09-07T20:07:51.067820: step 382, loss 0.395704, acc 0.78
2016-09-07T20:07:51.730788: step 383, loss 0.397555, acc 0.8
2016-09-07T20:07:52.382388: step 384, loss 0.459055, acc 0.76
2016-09-07T20:07:53.079593: step 385, loss 0.369558, acc 0.86
2016-09-07T20:07:53.746760: step 386, loss 0.360979, acc 0.86
2016-09-07T20:07:54.433860: step 387, loss 0.411303, acc 0.8
2016-09-07T20:07:54.809782: step 388, loss 0.35285, acc 0.833333
2016-09-07T20:07:55.469140: step 389, loss 0.379911, acc 0.82
2016-09-07T20:07:56.159092: step 390, loss 0.24727, acc 0.88
2016-09-07T20:07:56.824334: step 391, loss 0.264784, acc 0.92
2016-09-07T20:07:57.518837: step 392, loss 0.200468, acc 0.92
2016-09-07T20:07:58.194246: step 393, loss 0.222253, acc 0.84
2016-09-07T20:07:58.894365: step 394, loss 0.274946, acc 0.88
2016-09-07T20:07:59.579285: step 395, loss 0.251515, acc 0.92
2016-09-07T20:08:00.302457: step 396, loss 0.187666, acc 0.92
2016-09-07T20:08:00.975623: step 397, loss 0.217243, acc 0.92
2016-09-07T20:08:01.658632: step 398, loss 0.241346, acc 0.92
2016-09-07T20:08:02.335880: step 399, loss 0.226338, acc 0.92
2016-09-07T20:08:03.004080: step 400, loss 0.137709, acc 0.96

Evaluation:
2016-09-07T20:08:06.288254: step 400, loss 0.602774, acc 0.8

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-400

2016-09-07T20:08:07.924861: step 401, loss 0.298377, acc 0.84
2016-09-07T20:08:08.597282: step 402, loss 0.375392, acc 0.88
2016-09-07T20:08:09.278320: step 403, loss 0.310898, acc 0.92
2016-09-07T20:08:09.945151: step 404, loss 0.196482, acc 0.9
2016-09-07T20:08:10.620928: step 405, loss 0.248135, acc 0.9
2016-09-07T20:08:11.297994: step 406, loss 0.201115, acc 0.92
2016-09-07T20:08:11.981570: step 407, loss 0.0680793, acc 0.98
2016-09-07T20:08:12.651438: step 408, loss 0.331845, acc 0.86
2016-09-07T20:08:13.323817: step 409, loss 0.413838, acc 0.88
2016-09-07T20:08:14.027798: step 410, loss 0.185776, acc 0.9
2016-09-07T20:08:14.698668: step 411, loss 0.258646, acc 0.92
2016-09-07T20:08:15.378244: step 412, loss 0.117959, acc 0.96
2016-09-07T20:08:16.063407: step 413, loss 0.257538, acc 0.9
2016-09-07T20:08:16.716449: step 414, loss 0.353674, acc 0.84
2016-09-07T20:08:17.378603: step 415, loss 0.323941, acc 0.86
2016-09-07T20:08:18.026697: step 416, loss 0.234381, acc 0.92
2016-09-07T20:08:18.693330: step 417, loss 0.201927, acc 0.92
2016-09-07T20:08:19.365853: step 418, loss 0.27773, acc 0.86
2016-09-07T20:08:20.051487: step 419, loss 0.126067, acc 0.96
2016-09-07T20:08:20.738916: step 420, loss 0.210333, acc 0.92
2016-09-07T20:08:21.428416: step 421, loss 0.404386, acc 0.86
2016-09-07T20:08:22.113526: step 422, loss 0.245965, acc 0.9
2016-09-07T20:08:22.770440: step 423, loss 0.24007, acc 0.92
2016-09-07T20:08:23.427143: step 424, loss 0.380822, acc 0.84
2016-09-07T20:08:24.085354: step 425, loss 0.172193, acc 0.96
2016-09-07T20:08:24.756400: step 426, loss 0.19557, acc 0.96
2016-09-07T20:08:25.412901: step 427, loss 0.262054, acc 0.86
2016-09-07T20:08:26.091206: step 428, loss 0.381061, acc 0.8
2016-09-07T20:08:26.774106: step 429, loss 0.160349, acc 0.94
2016-09-07T20:08:27.449030: step 430, loss 0.168786, acc 0.94
2016-09-07T20:08:28.109402: step 431, loss 0.331514, acc 0.86
2016-09-07T20:08:28.769424: step 432, loss 0.240296, acc 0.96
2016-09-07T20:08:29.433323: step 433, loss 0.177483, acc 0.94
2016-09-07T20:08:30.111712: step 434, loss 0.40178, acc 0.86
2016-09-07T20:08:30.790824: step 435, loss 0.299375, acc 0.86
2016-09-07T20:08:31.452283: step 436, loss 0.413285, acc 0.8
2016-09-07T20:08:32.117521: step 437, loss 0.182888, acc 0.96
2016-09-07T20:08:32.806920: step 438, loss 0.184963, acc 0.9
2016-09-07T20:08:33.469462: step 439, loss 0.103525, acc 0.96
2016-09-07T20:08:34.140050: step 440, loss 0.295827, acc 0.88
2016-09-07T20:08:34.793756: step 441, loss 0.268659, acc 0.9
2016-09-07T20:08:35.452679: step 442, loss 0.156537, acc 0.92
2016-09-07T20:08:36.108248: step 443, loss 0.271476, acc 0.86
2016-09-07T20:08:36.795845: step 444, loss 0.364886, acc 0.76
2016-09-07T20:08:37.476635: step 445, loss 0.29252, acc 0.9
2016-09-07T20:08:38.154081: step 446, loss 0.236625, acc 0.86
2016-09-07T20:08:38.825642: step 447, loss 0.251554, acc 0.9
2016-09-07T20:08:39.504271: step 448, loss 0.113611, acc 1
2016-09-07T20:08:40.251939: step 449, loss 0.151488, acc 0.96
2016-09-07T20:08:40.925438: step 450, loss 0.268831, acc 0.86
2016-09-07T20:08:41.603008: step 451, loss 0.253906, acc 0.92
2016-09-07T20:08:42.285469: step 452, loss 0.279663, acc 0.88
2016-09-07T20:08:42.978379: step 453, loss 0.250901, acc 0.88
2016-09-07T20:08:43.668092: step 454, loss 0.197554, acc 0.9
2016-09-07T20:08:44.336636: step 455, loss 0.314004, acc 0.84
2016-09-07T20:08:45.021332: step 456, loss 0.145643, acc 0.96
2016-09-07T20:08:45.712357: step 457, loss 0.205794, acc 0.88
2016-09-07T20:08:46.383723: step 458, loss 0.269413, acc 0.92
2016-09-07T20:08:47.058827: step 459, loss 0.311983, acc 0.84
2016-09-07T20:08:47.726320: step 460, loss 0.278993, acc 0.94
2016-09-07T20:08:48.405165: step 461, loss 0.201508, acc 0.94
2016-09-07T20:08:49.083772: step 462, loss 0.338427, acc 0.84
2016-09-07T20:08:49.736665: step 463, loss 0.268636, acc 0.9
2016-09-07T20:08:50.382306: step 464, loss 0.331721, acc 0.82
2016-09-07T20:08:51.058127: step 465, loss 0.335811, acc 0.8
2016-09-07T20:08:51.752669: step 466, loss 0.123546, acc 0.96
2016-09-07T20:08:52.429238: step 467, loss 0.226987, acc 0.9
2016-09-07T20:08:53.103900: step 468, loss 0.198988, acc 0.9
2016-09-07T20:08:53.763720: step 469, loss 0.251796, acc 0.86
2016-09-07T20:08:54.444209: step 470, loss 0.2551, acc 0.88
2016-09-07T20:08:55.121026: step 471, loss 0.283907, acc 0.84
2016-09-07T20:08:55.806457: step 472, loss 0.313042, acc 0.88
2016-09-07T20:08:56.482691: step 473, loss 0.226536, acc 0.9
2016-09-07T20:08:57.155450: step 474, loss 0.34886, acc 0.82
2016-09-07T20:08:57.838897: step 475, loss 0.190376, acc 0.94
2016-09-07T20:08:58.516253: step 476, loss 0.168612, acc 0.94
2016-09-07T20:08:59.182049: step 477, loss 0.313361, acc 0.9
2016-09-07T20:08:59.855075: step 478, loss 0.279361, acc 0.86
2016-09-07T20:09:00.552868: step 479, loss 0.339268, acc 0.82
2016-09-07T20:09:01.225590: step 480, loss 0.218083, acc 0.9
2016-09-07T20:09:01.904834: step 481, loss 0.162769, acc 0.94
2016-09-07T20:09:02.575833: step 482, loss 0.179717, acc 0.92
2016-09-07T20:09:03.237132: step 483, loss 0.212415, acc 0.88
2016-09-07T20:09:03.921190: step 484, loss 0.142586, acc 0.98
2016-09-07T20:09:04.593262: step 485, loss 0.355465, acc 0.82
2016-09-07T20:09:05.281603: step 486, loss 0.283923, acc 0.86
2016-09-07T20:09:05.951522: step 487, loss 0.182673, acc 0.94
2016-09-07T20:09:06.611500: step 488, loss 0.320761, acc 0.84
2016-09-07T20:09:07.274767: step 489, loss 0.367921, acc 0.88
2016-09-07T20:09:07.934734: step 490, loss 0.250916, acc 0.9
2016-09-07T20:09:08.605530: step 491, loss 0.166063, acc 0.96
2016-09-07T20:09:09.278126: step 492, loss 0.169347, acc 0.96
2016-09-07T20:09:09.941741: step 493, loss 0.161435, acc 0.94
2016-09-07T20:09:10.607755: step 494, loss 0.243231, acc 0.92
2016-09-07T20:09:11.288436: step 495, loss 0.335677, acc 0.88
2016-09-07T20:09:11.946661: step 496, loss 0.238574, acc 0.88
2016-09-07T20:09:12.603208: step 497, loss 0.318247, acc 0.9
2016-09-07T20:09:13.281365: step 498, loss 0.421874, acc 0.84
2016-09-07T20:09:13.952459: step 499, loss 0.288088, acc 0.88
2016-09-07T20:09:14.637366: step 500, loss 0.230702, acc 0.92

Evaluation:
2016-09-07T20:09:17.960370: step 500, loss 0.511554, acc 0.778

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-500

2016-09-07T20:09:19.699479: step 501, loss 0.323016, acc 0.86
2016-09-07T20:09:20.375426: step 502, loss 0.266783, acc 0.88
2016-09-07T20:09:21.069120: step 503, loss 0.207155, acc 0.92
2016-09-07T20:09:21.738008: step 504, loss 0.239406, acc 0.86
2016-09-07T20:09:22.411237: step 505, loss 0.241499, acc 0.9
2016-09-07T20:09:23.105342: step 506, loss 0.238845, acc 0.9
2016-09-07T20:09:23.788910: step 507, loss 0.205033, acc 0.92
2016-09-07T20:09:24.456770: step 508, loss 0.165425, acc 0.94
2016-09-07T20:09:25.133505: step 509, loss 0.155766, acc 0.96
2016-09-07T20:09:25.818459: step 510, loss 0.412308, acc 0.8
2016-09-07T20:09:26.554424: step 511, loss 0.209905, acc 0.9
2016-09-07T20:09:27.277218: step 512, loss 0.208615, acc 0.94
2016-09-07T20:09:27.940011: step 513, loss 0.20977, acc 0.98
2016-09-07T20:09:28.612443: step 514, loss 0.348448, acc 0.78
2016-09-07T20:09:29.286449: step 515, loss 0.190321, acc 0.9
2016-09-07T20:09:29.953575: step 516, loss 0.273468, acc 0.86
2016-09-07T20:09:30.635967: step 517, loss 0.167762, acc 0.94
2016-09-07T20:09:31.335038: step 518, loss 0.335333, acc 0.88
2016-09-07T20:09:32.007706: step 519, loss 0.332064, acc 0.86
2016-09-07T20:09:32.691455: step 520, loss 0.332615, acc 0.86
2016-09-07T20:09:33.370722: step 521, loss 0.23306, acc 0.9
2016-09-07T20:09:34.045377: step 522, loss 0.224858, acc 0.9
2016-09-07T20:09:34.716692: step 523, loss 0.367251, acc 0.82
2016-09-07T20:09:35.383004: step 524, loss 0.369573, acc 0.82
2016-09-07T20:09:36.051888: step 525, loss 0.316652, acc 0.88
2016-09-07T20:09:36.721760: step 526, loss 0.305008, acc 0.88
2016-09-07T20:09:37.401005: step 527, loss 0.210562, acc 0.92
2016-09-07T20:09:38.084659: step 528, loss 0.237832, acc 0.92
2016-09-07T20:09:38.759982: step 529, loss 0.320536, acc 0.84
2016-09-07T20:09:39.429250: step 530, loss 0.197227, acc 0.92
2016-09-07T20:09:40.118580: step 531, loss 0.256433, acc 0.92
2016-09-07T20:09:40.794140: step 532, loss 0.281835, acc 0.86
2016-09-07T20:09:41.468072: step 533, loss 0.321817, acc 0.84
2016-09-07T20:09:42.143161: step 534, loss 0.30163, acc 0.86
2016-09-07T20:09:42.823291: step 535, loss 0.231737, acc 0.94
2016-09-07T20:09:43.485335: step 536, loss 0.235228, acc 0.88
2016-09-07T20:09:44.149065: step 537, loss 0.193526, acc 0.92
2016-09-07T20:09:44.830767: step 538, loss 0.32468, acc 0.84
2016-09-07T20:09:45.512989: step 539, loss 0.25709, acc 0.88
2016-09-07T20:09:46.187526: step 540, loss 0.306293, acc 0.88
2016-09-07T20:09:46.850630: step 541, loss 0.426209, acc 0.84
2016-09-07T20:09:47.540950: step 542, loss 0.31461, acc 0.86
2016-09-07T20:09:48.234155: step 543, loss 0.272821, acc 0.86
2016-09-07T20:09:48.909287: step 544, loss 0.251052, acc 0.9
2016-09-07T20:09:49.572244: step 545, loss 0.216796, acc 0.94
2016-09-07T20:09:50.236730: step 546, loss 0.183625, acc 0.9
2016-09-07T20:09:50.930675: step 547, loss 0.270823, acc 0.88
2016-09-07T20:09:51.611815: step 548, loss 0.363376, acc 0.86
2016-09-07T20:09:52.302175: step 549, loss 0.265746, acc 0.86
2016-09-07T20:09:52.976344: step 550, loss 0.192548, acc 0.92
2016-09-07T20:09:53.650455: step 551, loss 0.2574, acc 0.92
2016-09-07T20:09:54.320007: step 552, loss 0.33148, acc 0.88
2016-09-07T20:09:55.001287: step 553, loss 0.271405, acc 0.86
2016-09-07T20:09:55.666655: step 554, loss 0.151645, acc 0.94
2016-09-07T20:09:56.347415: step 555, loss 0.224342, acc 0.88
2016-09-07T20:09:57.016903: step 556, loss 0.2512, acc 0.9
2016-09-07T20:09:57.675578: step 557, loss 0.334109, acc 0.84
2016-09-07T20:09:58.347201: step 558, loss 0.382636, acc 0.84
2016-09-07T20:09:59.028508: step 559, loss 0.284305, acc 0.84
2016-09-07T20:09:59.713075: step 560, loss 0.286542, acc 0.9
2016-09-07T20:10:00.444623: step 561, loss 0.255662, acc 0.88
2016-09-07T20:10:01.109733: step 562, loss 0.120336, acc 0.96
2016-09-07T20:10:01.787277: step 563, loss 0.341939, acc 0.88
2016-09-07T20:10:02.449667: step 564, loss 0.31713, acc 0.86
2016-09-07T20:10:03.130850: step 565, loss 0.124314, acc 0.94
2016-09-07T20:10:03.805760: step 566, loss 0.513762, acc 0.78
2016-09-07T20:10:04.474326: step 567, loss 0.38204, acc 0.86
2016-09-07T20:10:05.148753: step 568, loss 0.19934, acc 0.94
2016-09-07T20:10:05.816516: step 569, loss 0.333277, acc 0.84
2016-09-07T20:10:06.487869: step 570, loss 0.213807, acc 0.92
2016-09-07T20:10:07.178318: step 571, loss 0.197397, acc 0.96
2016-09-07T20:10:07.861830: step 572, loss 0.299716, acc 0.86
2016-09-07T20:10:08.535731: step 573, loss 0.341488, acc 0.88
2016-09-07T20:10:09.215597: step 574, loss 0.174646, acc 0.94
2016-09-07T20:10:09.893567: step 575, loss 0.28369, acc 0.88
2016-09-07T20:10:10.581712: step 576, loss 0.206505, acc 0.96
2016-09-07T20:10:11.260420: step 577, loss 0.251487, acc 0.92
2016-09-07T20:10:11.960769: step 578, loss 0.370124, acc 0.82
2016-09-07T20:10:12.652233: step 579, loss 0.22906, acc 0.86
2016-09-07T20:10:13.313791: step 580, loss 0.206497, acc 0.84
2016-09-07T20:10:13.986663: step 581, loss 0.25468, acc 0.92
2016-09-07T20:10:14.341933: step 582, loss 0.26373, acc 0.75
2016-09-07T20:10:15.037102: step 583, loss 0.0855454, acc 0.98
2016-09-07T20:10:15.723148: step 584, loss 0.185884, acc 0.9
2016-09-07T20:10:16.406232: step 585, loss 0.325512, acc 0.9
2016-09-07T20:10:17.112379: step 586, loss 0.121216, acc 0.96
2016-09-07T20:10:17.796070: step 587, loss 0.175561, acc 0.94
2016-09-07T20:10:18.485418: step 588, loss 0.295048, acc 0.88
2016-09-07T20:10:19.161113: step 589, loss 0.0969838, acc 0.94
2016-09-07T20:10:19.838408: step 590, loss 0.209124, acc 0.92
2016-09-07T20:10:20.508026: step 591, loss 0.0693298, acc 0.98
2016-09-07T20:10:21.188982: step 592, loss 0.313987, acc 0.9
2016-09-07T20:10:21.868272: step 593, loss 0.0719882, acc 0.98
2016-09-07T20:10:22.545957: step 594, loss 0.0525033, acc 0.98
2016-09-07T20:10:23.234551: step 595, loss 0.287079, acc 0.86
2016-09-07T20:10:23.911695: step 596, loss 0.0868535, acc 0.98
2016-09-07T20:10:24.581381: step 597, loss 0.1172, acc 0.94
2016-09-07T20:10:25.249136: step 598, loss 0.253018, acc 0.9
2016-09-07T20:10:25.926226: step 599, loss 0.187764, acc 0.88
2016-09-07T20:10:26.585529: step 600, loss 0.0482817, acc 0.98

Evaluation:
2016-09-07T20:10:29.917372: step 600, loss 0.591862, acc 0.788

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-600

2016-09-07T20:10:31.710004: step 601, loss 0.114394, acc 0.96
2016-09-07T20:10:32.392277: step 602, loss 0.145056, acc 0.92
2016-09-07T20:10:33.073554: step 603, loss 0.0965691, acc 0.96
2016-09-07T20:10:33.730125: step 604, loss 0.190667, acc 0.9
2016-09-07T20:10:34.403395: step 605, loss 0.237976, acc 0.96
2016-09-07T20:10:35.074412: step 606, loss 0.149661, acc 0.94
2016-09-07T20:10:35.734992: step 607, loss 0.163331, acc 0.94
2016-09-07T20:10:36.429875: step 608, loss 0.0949869, acc 0.98
2016-09-07T20:10:37.114920: step 609, loss 0.202862, acc 0.88
2016-09-07T20:10:37.799725: step 610, loss 0.0786833, acc 0.98
2016-09-07T20:10:38.490870: step 611, loss 0.15296, acc 0.94
2016-09-07T20:10:39.177425: step 612, loss 0.0709959, acc 0.98
2016-09-07T20:10:39.862717: step 613, loss 0.176427, acc 0.9
2016-09-07T20:10:40.535866: step 614, loss 0.200385, acc 0.88
2016-09-07T20:10:41.229233: step 615, loss 0.151613, acc 0.92
2016-09-07T20:10:41.924111: step 616, loss 0.127387, acc 0.94
2016-09-07T20:10:42.588312: step 617, loss 0.0747714, acc 0.98
2016-09-07T20:10:43.257820: step 618, loss 0.213697, acc 0.88
2016-09-07T20:10:43.939765: step 619, loss 0.108623, acc 0.94
2016-09-07T20:10:44.612974: step 620, loss 0.190279, acc 0.92
2016-09-07T20:10:45.281435: step 621, loss 0.0609273, acc 0.98
2016-09-07T20:10:45.934542: step 622, loss 0.0663481, acc 0.98
2016-09-07T20:10:46.617151: step 623, loss 0.216793, acc 0.92
2016-09-07T20:10:47.313315: step 624, loss 0.315664, acc 0.84
2016-09-07T20:10:48.010605: step 625, loss 0.0304834, acc 1
2016-09-07T20:10:48.689349: step 626, loss 0.329172, acc 0.88
2016-09-07T20:10:49.373445: step 627, loss 0.298637, acc 0.86
2016-09-07T20:10:50.042000: step 628, loss 0.174995, acc 0.92
2016-09-07T20:10:50.739692: step 629, loss 0.352, acc 0.86
2016-09-07T20:10:51.435354: step 630, loss 0.129665, acc 0.96
2016-09-07T20:10:52.117744: step 631, loss 0.15191, acc 0.96
2016-09-07T20:10:52.809863: step 632, loss 0.22725, acc 0.88
2016-09-07T20:10:53.483823: step 633, loss 0.130906, acc 0.96
2016-09-07T20:10:54.169860: step 634, loss 0.156391, acc 0.92
2016-09-07T20:10:54.849921: step 635, loss 0.146699, acc 0.94
2016-09-07T20:10:55.512891: step 636, loss 0.293044, acc 0.88
2016-09-07T20:10:56.187687: step 637, loss 0.211051, acc 0.92
2016-09-07T20:10:56.864806: step 638, loss 0.18297, acc 0.9
2016-09-07T20:10:57.551947: step 639, loss 0.192771, acc 0.88
2016-09-07T20:10:58.229851: step 640, loss 0.174488, acc 0.96
2016-09-07T20:10:58.912456: step 641, loss 0.193741, acc 0.92
2016-09-07T20:10:59.596485: step 642, loss 0.213334, acc 0.94
2016-09-07T20:11:00.286802: step 643, loss 0.221292, acc 0.9
2016-09-07T20:11:00.946888: step 644, loss 0.120848, acc 0.94
2016-09-07T20:11:01.636045: step 645, loss 0.167871, acc 0.94
2016-09-07T20:11:02.323812: step 646, loss 0.246701, acc 0.9
2016-09-07T20:11:02.992636: step 647, loss 0.129337, acc 0.94
2016-09-07T20:11:03.674618: step 648, loss 0.227176, acc 0.94
2016-09-07T20:11:04.326218: step 649, loss 0.119362, acc 0.98
2016-09-07T20:11:04.990294: step 650, loss 0.121209, acc 0.98
2016-09-07T20:11:05.664984: step 651, loss 0.130345, acc 0.96
2016-09-07T20:11:06.330449: step 652, loss 0.10397, acc 0.98
2016-09-07T20:11:06.994726: step 653, loss 0.162728, acc 0.92
2016-09-07T20:11:07.651169: step 654, loss 0.342116, acc 0.86
2016-09-07T20:11:08.318771: step 655, loss 0.219133, acc 0.94
2016-09-07T20:11:08.985200: step 656, loss 0.207422, acc 0.92
2016-09-07T20:11:09.656487: step 657, loss 0.180118, acc 0.92
2016-09-07T20:11:10.337483: step 658, loss 0.267928, acc 0.88
2016-09-07T20:11:10.994571: step 659, loss 0.23046, acc 0.88
2016-09-07T20:11:11.676057: step 660, loss 0.119779, acc 0.98
2016-09-07T20:11:12.340633: step 661, loss 0.278284, acc 0.88
2016-09-07T20:11:13.021566: step 662, loss 0.22816, acc 0.88
2016-09-07T20:11:13.721307: step 663, loss 0.191667, acc 0.94
2016-09-07T20:11:14.394098: step 664, loss 0.227335, acc 0.86
2016-09-07T20:11:15.080884: step 665, loss 0.229692, acc 0.9
2016-09-07T20:11:15.760883: step 666, loss 0.159347, acc 0.94
2016-09-07T20:11:16.418590: step 667, loss 0.124302, acc 0.94
2016-09-07T20:11:17.088924: step 668, loss 0.22839, acc 0.84
2016-09-07T20:11:17.762091: step 669, loss 0.27316, acc 0.9
2016-09-07T20:11:18.430567: step 670, loss 0.0790714, acc 0.98
2016-09-07T20:11:19.118728: step 671, loss 0.0973772, acc 0.96
2016-09-07T20:11:19.787448: step 672, loss 0.185111, acc 0.9
2016-09-07T20:11:20.470339: step 673, loss 0.130271, acc 0.96
2016-09-07T20:11:21.138580: step 674, loss 0.163759, acc 0.94
2016-09-07T20:11:21.820636: step 675, loss 0.195997, acc 0.92
2016-09-07T20:11:22.490903: step 676, loss 0.153447, acc 0.92
2016-09-07T20:11:23.152969: step 677, loss 0.133091, acc 0.94
2016-09-07T20:11:23.838398: step 678, loss 0.22797, acc 0.86
2016-09-07T20:11:24.531842: step 679, loss 0.11053, acc 0.96
2016-09-07T20:11:25.223208: step 680, loss 0.126336, acc 0.94
2016-09-07T20:11:25.909163: step 681, loss 0.102237, acc 0.96
2016-09-07T20:11:26.600177: step 682, loss 0.177411, acc 0.92
2016-09-07T20:11:27.294230: step 683, loss 0.214213, acc 0.88
2016-09-07T20:11:27.966631: step 684, loss 0.240682, acc 0.9
2016-09-07T20:11:28.637906: step 685, loss 0.194181, acc 0.92
2016-09-07T20:11:29.297500: step 686, loss 0.234357, acc 0.9
2016-09-07T20:11:29.967449: step 687, loss 0.0876822, acc 0.94
2016-09-07T20:11:30.653998: step 688, loss 0.148, acc 0.96
2016-09-07T20:11:31.339660: step 689, loss 0.144859, acc 0.96
2016-09-07T20:11:32.015078: step 690, loss 0.268552, acc 0.9
2016-09-07T20:11:32.688625: step 691, loss 0.332162, acc 0.86
2016-09-07T20:11:33.352550: step 692, loss 0.193409, acc 0.92
2016-09-07T20:11:34.036409: step 693, loss 0.248305, acc 0.88
2016-09-07T20:11:34.721534: step 694, loss 0.124018, acc 0.96
2016-09-07T20:11:35.404239: step 695, loss 0.199716, acc 0.92
2016-09-07T20:11:36.069184: step 696, loss 0.30522, acc 0.88
2016-09-07T20:11:36.731246: step 697, loss 0.141203, acc 0.94
2016-09-07T20:11:37.387070: step 698, loss 0.215308, acc 0.86
2016-09-07T20:11:38.056919: step 699, loss 0.207116, acc 0.9
2016-09-07T20:11:38.745554: step 700, loss 0.214278, acc 0.9

Evaluation:
2016-09-07T20:11:42.093913: step 700, loss 0.60572, acc 0.779

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-700

2016-09-07T20:11:43.749135: step 701, loss 0.22203, acc 0.9
2016-09-07T20:11:44.430442: step 702, loss 0.147515, acc 0.96
2016-09-07T20:11:45.099918: step 703, loss 0.0904503, acc 0.96
2016-09-07T20:11:45.769385: step 704, loss 0.15766, acc 0.96
2016-09-07T20:11:46.443452: step 705, loss 0.143928, acc 0.92
2016-09-07T20:11:47.139822: step 706, loss 0.133774, acc 0.92
2016-09-07T20:11:47.816383: step 707, loss 0.159361, acc 0.92
2016-09-07T20:11:48.495825: step 708, loss 0.235203, acc 0.92
2016-09-07T20:11:49.164693: step 709, loss 0.185503, acc 0.96
2016-09-07T20:11:49.849321: step 710, loss 0.088058, acc 0.98
2016-09-07T20:11:50.542317: step 711, loss 0.135647, acc 0.96
2016-09-07T20:11:51.223709: step 712, loss 0.129403, acc 0.94
2016-09-07T20:11:51.912062: step 713, loss 0.163758, acc 0.94
2016-09-07T20:11:52.586439: step 714, loss 0.156824, acc 0.94
2016-09-07T20:11:53.288642: step 715, loss 0.0977175, acc 0.96
2016-09-07T20:11:53.977498: step 716, loss 0.228224, acc 0.9
2016-09-07T20:11:54.667794: step 717, loss 0.237013, acc 0.88
2016-09-07T20:11:55.339131: step 718, loss 0.206696, acc 0.9
2016-09-07T20:11:56.022089: step 719, loss 0.188641, acc 0.92
2016-09-07T20:11:56.702393: step 720, loss 0.103135, acc 0.96
2016-09-07T20:11:57.379460: step 721, loss 0.195723, acc 0.9
2016-09-07T20:11:58.049508: step 722, loss 0.33975, acc 0.88
2016-09-07T20:11:58.716932: step 723, loss 0.160817, acc 0.96
2016-09-07T20:11:59.384430: step 724, loss 0.142823, acc 0.92
2016-09-07T20:12:00.057748: step 725, loss 0.15636, acc 0.94
2016-09-07T20:12:00.764740: step 726, loss 0.116173, acc 0.96
2016-09-07T20:12:01.446944: step 727, loss 0.215043, acc 0.94
2016-09-07T20:12:02.112038: step 728, loss 0.362122, acc 0.82
2016-09-07T20:12:02.778116: step 729, loss 0.200167, acc 0.92
2016-09-07T20:12:03.459028: step 730, loss 0.154846, acc 0.94
2016-09-07T20:12:04.122262: step 731, loss 0.105079, acc 0.98
2016-09-07T20:12:04.798555: step 732, loss 0.200169, acc 0.94
2016-09-07T20:12:05.464707: step 733, loss 0.184976, acc 0.96
2016-09-07T20:12:06.142653: step 734, loss 0.14964, acc 0.98
2016-09-07T20:12:06.827159: step 735, loss 0.151948, acc 0.94
2016-09-07T20:12:07.516020: step 736, loss 0.38557, acc 0.8
2016-09-07T20:12:08.196239: step 737, loss 0.143952, acc 0.94
2016-09-07T20:12:08.868545: step 738, loss 0.164698, acc 0.96
2016-09-07T20:12:09.530806: step 739, loss 0.161752, acc 0.94
2016-09-07T20:12:10.208123: step 740, loss 0.162004, acc 0.92
2016-09-07T20:12:10.894822: step 741, loss 0.131552, acc 0.92
2016-09-07T20:12:11.557773: step 742, loss 0.201268, acc 0.92
2016-09-07T20:12:12.234240: step 743, loss 0.152864, acc 0.94
2016-09-07T20:12:12.897434: step 744, loss 0.273986, acc 0.86
2016-09-07T20:12:13.563404: step 745, loss 0.206569, acc 0.94
2016-09-07T20:12:14.242064: step 746, loss 0.169204, acc 0.94
2016-09-07T20:12:14.911286: step 747, loss 0.22806, acc 0.92
2016-09-07T20:12:15.584171: step 748, loss 0.171257, acc 0.9
2016-09-07T20:12:16.262015: step 749, loss 0.0825247, acc 0.96
2016-09-07T20:12:16.947333: step 750, loss 0.157877, acc 0.92
2016-09-07T20:12:17.616325: step 751, loss 0.281142, acc 0.82
2016-09-07T20:12:18.295902: step 752, loss 0.171479, acc 0.92
2016-09-07T20:12:18.981334: step 753, loss 0.261825, acc 0.92
2016-09-07T20:12:19.669153: step 754, loss 0.346609, acc 0.84
2016-09-07T20:12:20.343587: step 755, loss 0.176767, acc 0.92
2016-09-07T20:12:21.017213: step 756, loss 0.123757, acc 0.94
2016-09-07T20:12:21.685947: step 757, loss 0.217254, acc 0.92
2016-09-07T20:12:22.363120: step 758, loss 0.118458, acc 0.94
2016-09-07T20:12:23.026837: step 759, loss 0.252627, acc 0.84
2016-09-07T20:12:23.704746: step 760, loss 0.122856, acc 0.96
2016-09-07T20:12:24.383593: step 761, loss 0.336478, acc 0.86
2016-09-07T20:12:25.076035: step 762, loss 0.233015, acc 0.86
2016-09-07T20:12:25.738780: step 763, loss 0.131162, acc 0.92
2016-09-07T20:12:26.411108: step 764, loss 0.064911, acc 1
2016-09-07T20:12:27.080600: step 765, loss 0.10441, acc 0.96
2016-09-07T20:12:27.749655: step 766, loss 0.171851, acc 0.88
2016-09-07T20:12:28.438958: step 767, loss 0.364333, acc 0.86
2016-09-07T20:12:29.126350: step 768, loss 0.191262, acc 0.9
2016-09-07T20:12:29.812812: step 769, loss 0.277198, acc 0.86
2016-09-07T20:12:30.468861: step 770, loss 0.196595, acc 0.9
2016-09-07T20:12:31.143310: step 771, loss 0.190085, acc 0.92
2016-09-07T20:12:31.862152: step 772, loss 0.0960898, acc 0.98
2016-09-07T20:12:32.523212: step 773, loss 0.197924, acc 0.92
2016-09-07T20:12:33.222246: step 774, loss 0.251701, acc 0.9
2016-09-07T20:12:33.895877: step 775, loss 0.190158, acc 0.92
2016-09-07T20:12:34.258927: step 776, loss 0.105647, acc 0.916667
2016-09-07T20:12:34.920672: step 777, loss 0.084082, acc 0.98
2016-09-07T20:12:35.590673: step 778, loss 0.123405, acc 0.92
2016-09-07T20:12:36.280164: step 779, loss 0.181044, acc 0.9
2016-09-07T20:12:36.967814: step 780, loss 0.0632162, acc 1
2016-09-07T20:12:37.642404: step 781, loss 0.0900022, acc 0.98
2016-09-07T20:12:38.316542: step 782, loss 0.131489, acc 0.96
2016-09-07T20:12:39.012874: step 783, loss 0.16203, acc 0.94
2016-09-07T20:12:39.681917: step 784, loss 0.342043, acc 0.92
2016-09-07T20:12:40.365627: step 785, loss 0.213329, acc 0.88
2016-09-07T20:12:41.058371: step 786, loss 0.0845432, acc 0.98
2016-09-07T20:12:41.717821: step 787, loss 0.0650891, acc 0.98
2016-09-07T20:12:42.387626: step 788, loss 0.214114, acc 0.94
2016-09-07T20:12:43.063129: step 789, loss 0.0356146, acc 1
2016-09-07T20:12:43.725064: step 790, loss 0.0756079, acc 1
2016-09-07T20:12:44.406442: step 791, loss 0.10231, acc 0.96
2016-09-07T20:12:45.084059: step 792, loss 0.0930544, acc 0.96
2016-09-07T20:12:45.772738: step 793, loss 0.0987386, acc 0.98
2016-09-07T20:12:46.457475: step 794, loss 0.126776, acc 0.94
2016-09-07T20:12:47.143193: step 795, loss 0.118994, acc 0.96
2016-09-07T20:12:47.829757: step 796, loss 0.164775, acc 0.96
2016-09-07T20:12:48.503983: step 797, loss 0.185012, acc 0.92
2016-09-07T20:12:49.173362: step 798, loss 0.0956579, acc 0.96
2016-09-07T20:12:49.856055: step 799, loss 0.119665, acc 0.92
2016-09-07T20:12:50.530750: step 800, loss 0.10114, acc 0.96

Evaluation:
2016-09-07T20:12:53.868979: step 800, loss 0.70265, acc 0.782

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-800

2016-09-07T20:12:55.626601: step 801, loss 0.0963973, acc 0.98
2016-09-07T20:12:56.327276: step 802, loss 0.0537242, acc 0.96
2016-09-07T20:12:57.011487: step 803, loss 0.0700706, acc 0.98
2016-09-07T20:12:57.683466: step 804, loss 0.0750013, acc 0.96
2016-09-07T20:12:58.357292: step 805, loss 0.101751, acc 0.96
2016-09-07T20:12:59.041184: step 806, loss 0.0994033, acc 0.96
2016-09-07T20:12:59.714120: step 807, loss 0.0976925, acc 0.98
2016-09-07T20:13:00.431210: step 808, loss 0.13735, acc 0.92
2016-09-07T20:13:01.093920: step 809, loss 0.120418, acc 0.92
2016-09-07T20:13:01.779711: step 810, loss 0.058628, acc 1
2016-09-07T20:13:02.434342: step 811, loss 0.103149, acc 0.96
2016-09-07T20:13:03.095533: step 812, loss 0.214082, acc 0.9
2016-09-07T20:13:03.770287: step 813, loss 0.0982839, acc 0.96
2016-09-07T20:13:04.443808: step 814, loss 0.119111, acc 0.94
2016-09-07T20:13:05.128480: step 815, loss 0.294124, acc 0.92
2016-09-07T20:13:05.818075: step 816, loss 0.0454735, acc 0.98
2016-09-07T20:13:06.470604: step 817, loss 0.256857, acc 0.88
2016-09-07T20:13:07.119818: step 818, loss 0.131516, acc 0.94
2016-09-07T20:13:07.802944: step 819, loss 0.123568, acc 0.96
2016-09-07T20:13:08.478190: step 820, loss 0.0991554, acc 0.98
2016-09-07T20:13:09.149742: step 821, loss 0.145973, acc 0.92
2016-09-07T20:13:09.809488: step 822, loss 0.104118, acc 0.94
2016-09-07T20:13:10.493825: step 823, loss 0.132822, acc 0.96
2016-09-07T20:13:11.166898: step 824, loss 0.153898, acc 0.94
2016-09-07T20:13:11.861039: step 825, loss 0.181309, acc 0.96
2016-09-07T20:13:12.532970: step 826, loss 0.167951, acc 0.92
2016-09-07T20:13:13.197284: step 827, loss 0.144245, acc 0.98
2016-09-07T20:13:13.872850: step 828, loss 0.128095, acc 0.96
2016-09-07T20:13:14.535085: step 829, loss 0.141995, acc 0.94
2016-09-07T20:13:15.205828: step 830, loss 0.0662853, acc 0.96
2016-09-07T20:13:15.887487: step 831, loss 0.106994, acc 0.96
2016-09-07T20:13:16.576738: step 832, loss 0.150335, acc 0.92
2016-09-07T20:13:17.257530: step 833, loss 0.165186, acc 0.94
2016-09-07T20:13:17.944582: step 834, loss 0.0701324, acc 0.98
2016-09-07T20:13:18.657719: step 835, loss 0.0725314, acc 0.98
2016-09-07T20:13:19.331232: step 836, loss 0.0332383, acc 1
2016-09-07T20:13:20.024055: step 837, loss 0.130847, acc 0.96
2016-09-07T20:13:20.698688: step 838, loss 0.0894443, acc 0.98
2016-09-07T20:13:21.391164: step 839, loss 0.165267, acc 0.94
2016-09-07T20:13:22.064596: step 840, loss 0.223464, acc 0.9
2016-09-07T20:13:22.747368: step 841, loss 0.143544, acc 0.92
2016-09-07T20:13:23.417455: step 842, loss 0.232408, acc 0.94
2016-09-07T20:13:24.099773: step 843, loss 0.0868433, acc 0.96
2016-09-07T20:13:24.789150: step 844, loss 0.22313, acc 0.94
2016-09-07T20:13:25.477122: step 845, loss 0.144131, acc 0.94
2016-09-07T20:13:26.152210: step 846, loss 0.133314, acc 0.94
2016-09-07T20:13:26.829920: step 847, loss 0.117556, acc 0.96
2016-09-07T20:13:27.493790: step 848, loss 0.149768, acc 0.96
2016-09-07T20:13:28.180734: step 849, loss 0.0891837, acc 0.98
2016-09-07T20:13:28.890786: step 850, loss 0.184218, acc 0.94
2016-09-07T20:13:29.579478: step 851, loss 0.136121, acc 0.94
2016-09-07T20:13:30.243224: step 852, loss 0.21709, acc 0.94
2016-09-07T20:13:30.907619: step 853, loss 0.213566, acc 0.94
2016-09-07T20:13:31.585087: step 854, loss 0.229988, acc 0.9
2016-09-07T20:13:32.270465: step 855, loss 0.185039, acc 0.9
2016-09-07T20:13:32.940123: step 856, loss 0.100646, acc 0.96
2016-09-07T20:13:33.610114: step 857, loss 0.0538001, acc 0.98
2016-09-07T20:13:34.271720: step 858, loss 0.0742799, acc 0.98
2016-09-07T20:13:34.952392: step 859, loss 0.132299, acc 0.96
2016-09-07T20:13:35.614909: step 860, loss 0.166269, acc 0.94
2016-09-07T20:13:36.295685: step 861, loss 0.0739368, acc 0.96
2016-09-07T20:13:36.986362: step 862, loss 0.165971, acc 0.94
2016-09-07T20:13:37.668840: step 863, loss 0.0534562, acc 0.98
2016-09-07T20:13:38.336673: step 864, loss 0.086588, acc 0.98
2016-09-07T20:13:39.013147: step 865, loss 0.0716268, acc 0.98
2016-09-07T20:13:39.679868: step 866, loss 0.120671, acc 0.96
2016-09-07T20:13:40.361146: step 867, loss 0.18014, acc 0.9
2016-09-07T20:13:41.026609: step 868, loss 0.104378, acc 0.96
2016-09-07T20:13:41.714280: step 869, loss 0.117841, acc 0.96
2016-09-07T20:13:42.387448: step 870, loss 0.163085, acc 0.94
2016-09-07T20:13:43.069686: step 871, loss 0.202289, acc 0.9
2016-09-07T20:13:43.741304: step 872, loss 0.113886, acc 0.98
2016-09-07T20:13:44.407279: step 873, loss 0.132283, acc 0.94
2016-09-07T20:13:45.085829: step 874, loss 0.199443, acc 0.94
2016-09-07T20:13:45.760415: step 875, loss 0.0909606, acc 0.96
2016-09-07T20:13:46.434696: step 876, loss 0.179223, acc 0.92
2016-09-07T20:13:47.115480: step 877, loss 0.134779, acc 0.96
2016-09-07T20:13:47.819463: step 878, loss 0.0560544, acc 0.98
2016-09-07T20:13:48.515047: step 879, loss 0.075624, acc 1
2016-09-07T20:13:49.201855: step 880, loss 0.236577, acc 0.88
2016-09-07T20:13:49.897334: step 881, loss 0.174346, acc 0.94
2016-09-07T20:13:50.565331: step 882, loss 0.0330423, acc 1
2016-09-07T20:13:51.265237: step 883, loss 0.124743, acc 0.96
2016-09-07T20:13:51.978284: step 884, loss 0.142306, acc 0.94
2016-09-07T20:13:52.645911: step 885, loss 0.161763, acc 0.92
2016-09-07T20:13:53.328769: step 886, loss 0.086278, acc 0.96
2016-09-07T20:13:54.012195: step 887, loss 0.210458, acc 0.88
2016-09-07T20:13:54.696688: step 888, loss 0.145774, acc 0.94
2016-09-07T20:13:55.370578: step 889, loss 0.242057, acc 0.9
2016-09-07T20:13:56.037956: step 890, loss 0.06759, acc 0.98
2016-09-07T20:13:56.725673: step 891, loss 0.164037, acc 0.94
2016-09-07T20:13:57.402814: step 892, loss 0.165648, acc 0.92
2016-09-07T20:13:58.068170: step 893, loss 0.0819107, acc 0.98
2016-09-07T20:13:58.757030: step 894, loss 0.141069, acc 0.9
2016-09-07T20:13:59.411647: step 895, loss 0.257299, acc 0.94
2016-09-07T20:14:00.101317: step 896, loss 0.142224, acc 0.98
2016-09-07T20:14:00.809900: step 897, loss 0.0771776, acc 0.98
2016-09-07T20:14:01.491991: step 898, loss 0.14707, acc 0.92
2016-09-07T20:14:02.168836: step 899, loss 0.145706, acc 0.96
2016-09-07T20:14:02.851664: step 900, loss 0.0983946, acc 0.96

Evaluation:
2016-09-07T20:14:06.234353: step 900, loss 0.782075, acc 0.762

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-900

2016-09-07T20:14:07.978914: step 901, loss 0.141532, acc 0.92
2016-09-07T20:14:08.664711: step 902, loss 0.182073, acc 0.96
2016-09-07T20:14:09.326102: step 903, loss 0.0972483, acc 0.96
2016-09-07T20:14:10.024297: step 904, loss 0.0761337, acc 0.98
2016-09-07T20:14:10.727535: step 905, loss 0.125143, acc 0.96
2016-09-07T20:14:11.393495: step 906, loss 0.159064, acc 0.96
2016-09-07T20:14:12.081219: step 907, loss 0.0447884, acc 1
2016-09-07T20:14:12.761711: step 908, loss 0.136794, acc 0.94
2016-09-07T20:14:13.443277: step 909, loss 0.0558485, acc 0.98
2016-09-07T20:14:14.143169: step 910, loss 0.0837457, acc 0.98
2016-09-07T20:14:14.841549: step 911, loss 0.132567, acc 0.94
2016-09-07T20:14:15.512811: step 912, loss 0.0734782, acc 0.96
2016-09-07T20:14:16.183249: step 913, loss 0.188926, acc 0.92
2016-09-07T20:14:16.879216: step 914, loss 0.103473, acc 0.96
2016-09-07T20:14:17.552766: step 915, loss 0.254144, acc 0.9
2016-09-07T20:14:18.248464: step 916, loss 0.115368, acc 0.94
2016-09-07T20:14:18.915782: step 917, loss 0.0647917, acc 0.98
2016-09-07T20:14:19.575185: step 918, loss 0.222662, acc 0.9
2016-09-07T20:14:20.244419: step 919, loss 0.1266, acc 0.92
2016-09-07T20:14:20.896874: step 920, loss 0.159036, acc 0.92
2016-09-07T20:14:21.572926: step 921, loss 0.129193, acc 0.94
2016-09-07T20:14:22.240230: step 922, loss 0.258013, acc 0.92
2016-09-07T20:14:22.935436: step 923, loss 0.105543, acc 0.94
2016-09-07T20:14:23.627852: step 924, loss 0.102655, acc 0.96
2016-09-07T20:14:24.306011: step 925, loss 0.130611, acc 0.96
2016-09-07T20:14:24.986417: step 926, loss 0.206015, acc 0.9
2016-09-07T20:14:25.668802: step 927, loss 0.193458, acc 0.9
2016-09-07T20:14:26.362684: step 928, loss 0.14935, acc 0.94
2016-09-07T20:14:27.046596: step 929, loss 0.145092, acc 0.92
2016-09-07T20:14:27.734983: step 930, loss 0.113239, acc 0.96
2016-09-07T20:14:28.400127: step 931, loss 0.133198, acc 0.92
2016-09-07T20:14:29.096302: step 932, loss 0.138595, acc 0.94
2016-09-07T20:14:29.774151: step 933, loss 0.0764596, acc 0.96
2016-09-07T20:14:30.465612: step 934, loss 0.0736309, acc 1
2016-09-07T20:14:31.163682: step 935, loss 0.271464, acc 0.9
2016-09-07T20:14:31.840538: step 936, loss 0.120994, acc 0.98
2016-09-07T20:14:32.533953: step 937, loss 0.127568, acc 0.96
2016-09-07T20:14:33.199115: step 938, loss 0.150551, acc 0.92
2016-09-07T20:14:33.892832: step 939, loss 0.114034, acc 0.96
2016-09-07T20:14:34.563935: step 940, loss 0.107769, acc 0.98
2016-09-07T20:14:35.232012: step 941, loss 0.102388, acc 0.96
2016-09-07T20:14:35.896539: step 942, loss 0.094, acc 1
2016-09-07T20:14:36.555602: step 943, loss 0.125549, acc 0.92
2016-09-07T20:14:37.253328: step 944, loss 0.153716, acc 0.96
2016-09-07T20:14:37.931055: step 945, loss 0.337293, acc 0.82
2016-09-07T20:14:38.605405: step 946, loss 0.201076, acc 0.88
2016-09-07T20:14:39.278319: step 947, loss 0.0895592, acc 0.96
2016-09-07T20:14:39.951103: step 948, loss 0.235597, acc 0.88
2016-09-07T20:14:40.629185: step 949, loss 0.117976, acc 0.94
2016-09-07T20:14:41.286989: step 950, loss 0.108017, acc 0.96
2016-09-07T20:14:41.958590: step 951, loss 0.0968934, acc 0.94
2016-09-07T20:14:42.630045: step 952, loss 0.143911, acc 0.94
2016-09-07T20:14:43.312967: step 953, loss 0.124923, acc 0.94
2016-09-07T20:14:44.001683: step 954, loss 0.148704, acc 0.94
2016-09-07T20:14:44.658639: step 955, loss 0.170674, acc 0.92
2016-09-07T20:14:45.344446: step 956, loss 0.0517512, acc 1
2016-09-07T20:14:46.023393: step 957, loss 0.143987, acc 0.94
2016-09-07T20:14:46.699109: step 958, loss 0.179731, acc 0.94
2016-09-07T20:14:47.376146: step 959, loss 0.0975043, acc 0.94
2016-09-07T20:14:48.044170: step 960, loss 0.0803781, acc 0.96
2016-09-07T20:14:48.686165: step 961, loss 0.141867, acc 0.96
2016-09-07T20:14:49.353945: step 962, loss 0.12543, acc 0.96
2016-09-07T20:14:50.031554: step 963, loss 0.181127, acc 0.94
2016-09-07T20:14:50.704710: step 964, loss 0.0900752, acc 0.96
2016-09-07T20:14:51.388255: step 965, loss 0.0896752, acc 0.94
2016-09-07T20:14:52.065785: step 966, loss 0.149861, acc 0.92
2016-09-07T20:14:52.741456: step 967, loss 0.158268, acc 0.92
2016-09-07T20:14:53.412364: step 968, loss 0.0500474, acc 1
2016-09-07T20:14:54.086368: step 969, loss 0.131449, acc 0.92
2016-09-07T20:14:54.445976: step 970, loss 0.102955, acc 1
2016-09-07T20:14:55.117736: step 971, loss 0.108209, acc 0.96
2016-09-07T20:14:55.811362: step 972, loss 0.079876, acc 0.96
2016-09-07T20:14:56.486735: step 973, loss 0.187683, acc 0.92
2016-09-07T20:14:57.176782: step 974, loss 0.140361, acc 0.96
2016-09-07T20:14:57.874393: step 975, loss 0.0746104, acc 0.98
2016-09-07T20:14:58.549841: step 976, loss 0.130599, acc 0.94
2016-09-07T20:14:59.238855: step 977, loss 0.0990842, acc 0.96
2016-09-07T20:14:59.903873: step 978, loss 0.0987682, acc 0.98
2016-09-07T20:15:00.612367: step 979, loss 0.0499824, acc 0.98
2016-09-07T20:15:01.280117: step 980, loss 0.278147, acc 0.9
2016-09-07T20:15:01.962702: step 981, loss 0.168068, acc 0.94
2016-09-07T20:15:02.643596: step 982, loss 0.158815, acc 0.96
2016-09-07T20:15:03.323672: step 983, loss 0.176359, acc 0.9
2016-09-07T20:15:03.987362: step 984, loss 0.0653629, acc 0.98
2016-09-07T20:15:04.679025: step 985, loss 0.0708156, acc 0.98
2016-09-07T20:15:05.357974: step 986, loss 0.104328, acc 0.98
2016-09-07T20:15:06.031003: step 987, loss 0.0927158, acc 0.96
2016-09-07T20:15:06.717101: step 988, loss 0.020938, acc 1
2016-09-07T20:15:07.393251: step 989, loss 0.127582, acc 0.92
2016-09-07T20:15:08.065919: step 990, loss 0.0291164, acc 1
2016-09-07T20:15:08.729442: step 991, loss 0.119161, acc 0.96
2016-09-07T20:15:09.409975: step 992, loss 0.182868, acc 0.94
2016-09-07T20:15:10.096036: step 993, loss 0.0488061, acc 1
2016-09-07T20:15:10.777544: step 994, loss 0.114796, acc 0.96
2016-09-07T20:15:11.467414: step 995, loss 0.123373, acc 0.96
2016-09-07T20:15:12.143694: step 996, loss 0.103329, acc 0.96
2016-09-07T20:15:12.818600: step 997, loss 0.0668965, acc 0.96
2016-09-07T20:15:13.497624: step 998, loss 0.0905199, acc 0.96
2016-09-07T20:15:14.185026: step 999, loss 0.124607, acc 0.94
2016-09-07T20:15:14.865074: step 1000, loss 0.0909243, acc 0.98

Evaluation:
2016-09-07T20:15:18.163552: step 1000, loss 0.890583, acc 0.782

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-1000

2016-09-07T20:15:19.820959: step 1001, loss 0.0497577, acc 0.98
2016-09-07T20:15:20.508104: step 1002, loss 0.0941302, acc 0.96
2016-09-07T20:15:21.184828: step 1003, loss 0.138065, acc 0.96
2016-09-07T20:15:21.879176: step 1004, loss 0.192112, acc 0.9
2016-09-07T20:15:22.538719: step 1005, loss 0.206027, acc 0.96
2016-09-07T20:15:23.214538: step 1006, loss 0.0793185, acc 0.96
2016-09-07T20:15:23.900994: step 1007, loss 0.0841574, acc 0.96
2016-09-07T20:15:24.620644: step 1008, loss 0.0589909, acc 0.96
2016-09-07T20:15:25.302464: step 1009, loss 0.0525938, acc 0.98
2016-09-07T20:15:25.987375: step 1010, loss 0.149524, acc 0.94
2016-09-07T20:15:26.663656: step 1011, loss 0.109909, acc 0.96
2016-09-07T20:15:27.355867: step 1012, loss 0.037797, acc 0.98
2016-09-07T20:15:28.031274: step 1013, loss 0.0421248, acc 1
2016-09-07T20:15:28.709401: step 1014, loss 0.0775741, acc 0.96
2016-09-07T20:15:29.372984: step 1015, loss 0.0295436, acc 0.98
2016-09-07T20:15:30.036660: step 1016, loss 0.0778465, acc 0.94
2016-09-07T20:15:30.715403: step 1017, loss 0.13141, acc 0.96
2016-09-07T20:15:31.388221: step 1018, loss 0.128899, acc 0.94
2016-09-07T20:15:32.093807: step 1019, loss 0.0837523, acc 0.98
2016-09-07T20:15:32.774916: step 1020, loss 0.096305, acc 0.96
2016-09-07T20:15:33.457456: step 1021, loss 0.133032, acc 0.94
2016-09-07T20:15:34.135488: step 1022, loss 0.0511741, acc 0.96
2016-09-07T20:15:34.796075: step 1023, loss 0.0648524, acc 0.96
2016-09-07T20:15:35.463228: step 1024, loss 0.0770468, acc 0.96
2016-09-07T20:15:36.145090: step 1025, loss 0.222137, acc 0.9
2016-09-07T20:15:36.807168: step 1026, loss 0.154942, acc 0.94
2016-09-07T20:15:37.520176: step 1027, loss 0.0732484, acc 0.96
2016-09-07T20:15:38.210334: step 1028, loss 0.0837089, acc 0.96
2016-09-07T20:15:38.888006: step 1029, loss 0.0830279, acc 0.96
2016-09-07T20:15:39.578501: step 1030, loss 0.0401136, acc 1
2016-09-07T20:15:40.253220: step 1031, loss 0.123397, acc 0.92
2016-09-07T20:15:40.915750: step 1032, loss 0.0705417, acc 0.96
2016-09-07T20:15:41.590009: step 1033, loss 0.131473, acc 0.94
2016-09-07T20:15:42.265094: step 1034, loss 0.139453, acc 0.98
2016-09-07T20:15:42.934828: step 1035, loss 0.1321, acc 0.94
2016-09-07T20:15:43.620596: step 1036, loss 0.0660743, acc 1
2016-09-07T20:15:44.296553: step 1037, loss 0.0204227, acc 1
2016-09-07T20:15:44.956152: step 1038, loss 0.156207, acc 0.94
2016-09-07T20:15:45.621426: step 1039, loss 0.128721, acc 0.98
2016-09-07T20:15:46.297980: step 1040, loss 0.0737489, acc 0.96
2016-09-07T20:15:46.968141: step 1041, loss 0.0767335, acc 0.96
2016-09-07T20:15:47.649685: step 1042, loss 0.029799, acc 0.98
2016-09-07T20:15:48.318466: step 1043, loss 0.233566, acc 0.9
2016-09-07T20:15:48.984912: step 1044, loss 0.0164985, acc 1
2016-09-07T20:15:49.658855: step 1045, loss 0.0991269, acc 0.94
2016-09-07T20:15:50.329300: step 1046, loss 0.127688, acc 0.96
2016-09-07T20:15:50.990896: step 1047, loss 0.139464, acc 0.92
2016-09-07T20:15:51.667908: step 1048, loss 0.0830145, acc 0.96
2016-09-07T20:15:52.342041: step 1049, loss 0.118734, acc 0.94
2016-09-07T20:15:53.016587: step 1050, loss 0.134333, acc 0.94
2016-09-07T20:15:53.696459: step 1051, loss 0.0998029, acc 0.96
2016-09-07T20:15:54.381255: step 1052, loss 0.111544, acc 0.98
2016-09-07T20:15:55.068537: step 1053, loss 0.0537425, acc 0.98
2016-09-07T20:15:55.759884: step 1054, loss 0.0937047, acc 0.96
2016-09-07T20:15:56.435918: step 1055, loss 0.0725084, acc 0.98
2016-09-07T20:15:57.097393: step 1056, loss 0.191296, acc 0.92
2016-09-07T20:15:57.778551: step 1057, loss 0.0842363, acc 0.98
2016-09-07T20:15:58.460112: step 1058, loss 0.103085, acc 0.98
2016-09-07T20:15:59.142408: step 1059, loss 0.179217, acc 0.9
2016-09-07T20:15:59.820621: step 1060, loss 0.10515, acc 0.98
2016-09-07T20:16:00.538163: step 1061, loss 0.0482725, acc 1
2016-09-07T20:16:01.220663: step 1062, loss 0.0725349, acc 0.98
2016-09-07T20:16:01.890784: step 1063, loss 0.183225, acc 0.92
2016-09-07T20:16:02.549882: step 1064, loss 0.0931432, acc 0.98
2016-09-07T20:16:03.209253: step 1065, loss 0.129664, acc 0.94
2016-09-07T20:16:03.876997: step 1066, loss 0.168163, acc 0.94
2016-09-07T20:16:04.548938: step 1067, loss 0.0934402, acc 0.98
2016-09-07T20:16:05.238868: step 1068, loss 0.0663503, acc 0.98
2016-09-07T20:16:05.914362: step 1069, loss 0.145856, acc 0.9
2016-09-07T20:16:06.568307: step 1070, loss 0.0723865, acc 0.96
2016-09-07T20:16:07.253069: step 1071, loss 0.0598216, acc 0.98
2016-09-07T20:16:07.934618: step 1072, loss 0.0559425, acc 1
2016-09-07T20:16:08.626203: step 1073, loss 0.218519, acc 0.94
2016-09-07T20:16:09.315741: step 1074, loss 0.0955145, acc 0.96
2016-09-07T20:16:09.979382: step 1075, loss 0.102568, acc 0.96
2016-09-07T20:16:10.658313: step 1076, loss 0.0903021, acc 0.92
2016-09-07T20:16:11.341692: step 1077, loss 0.139356, acc 0.94
2016-09-07T20:16:12.014289: step 1078, loss 0.233085, acc 0.92
2016-09-07T20:16:12.679744: step 1079, loss 0.230398, acc 0.88
2016-09-07T20:16:13.356138: step 1080, loss 0.215933, acc 0.94
2016-09-07T20:16:14.040074: step 1081, loss 0.0751461, acc 0.98
2016-09-07T20:16:14.702437: step 1082, loss 0.0133968, acc 1
2016-09-07T20:16:15.374336: step 1083, loss 0.217279, acc 0.86
2016-09-07T20:16:16.048096: step 1084, loss 0.0515497, acc 1
2016-09-07T20:16:16.733875: step 1085, loss 0.118313, acc 0.96
2016-09-07T20:16:17.416277: step 1086, loss 0.157329, acc 0.94
2016-09-07T20:16:18.103698: step 1087, loss 0.0697262, acc 0.98
2016-09-07T20:16:18.784792: step 1088, loss 0.0605813, acc 0.98
2016-09-07T20:16:19.494973: step 1089, loss 0.134374, acc 0.96
2016-09-07T20:16:20.185611: step 1090, loss 0.2139, acc 0.9
2016-09-07T20:16:20.882074: step 1091, loss 0.16882, acc 0.94
2016-09-07T20:16:21.584732: step 1092, loss 0.086528, acc 0.96
2016-09-07T20:16:22.252844: step 1093, loss 0.178763, acc 0.92
2016-09-07T20:16:22.935455: step 1094, loss 0.152601, acc 0.94
2016-09-07T20:16:23.628761: step 1095, loss 0.0572162, acc 0.98
2016-09-07T20:16:24.320945: step 1096, loss 0.0986096, acc 0.94
2016-09-07T20:16:24.992729: step 1097, loss 0.224928, acc 0.92
2016-09-07T20:16:25.642109: step 1098, loss 0.0802042, acc 0.98
2016-09-07T20:16:26.306267: step 1099, loss 0.0578854, acc 0.98
2016-09-07T20:16:26.977011: step 1100, loss 0.0856918, acc 1

Evaluation:
2016-09-07T20:16:30.276829: step 1100, loss 0.809073, acc 0.77

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-1100

2016-09-07T20:16:32.028232: step 1101, loss 0.0788968, acc 1
2016-09-07T20:16:32.695059: step 1102, loss 0.18249, acc 0.96
2016-09-07T20:16:33.377412: step 1103, loss 0.0395087, acc 1
2016-09-07T20:16:34.059072: step 1104, loss 0.0881573, acc 0.98
2016-09-07T20:16:34.733279: step 1105, loss 0.143486, acc 0.94
2016-09-07T20:16:35.408017: step 1106, loss 0.256589, acc 0.88
2016-09-07T20:16:36.088670: step 1107, loss 0.114608, acc 0.94
2016-09-07T20:16:36.766486: step 1108, loss 0.0361357, acc 0.98
2016-09-07T20:16:37.441286: step 1109, loss 0.101996, acc 0.96
2016-09-07T20:16:38.099305: step 1110, loss 0.12703, acc 0.9
2016-09-07T20:16:38.771984: step 1111, loss 0.0899593, acc 0.96
2016-09-07T20:16:39.465697: step 1112, loss 0.123584, acc 0.92
2016-09-07T20:16:40.159535: step 1113, loss 0.0279362, acc 1
2016-09-07T20:16:40.841123: step 1114, loss 0.0642146, acc 0.98
2016-09-07T20:16:41.516206: step 1115, loss 0.0504292, acc 0.96
2016-09-07T20:16:42.192172: step 1116, loss 0.164496, acc 0.94
2016-09-07T20:16:42.868484: step 1117, loss 0.103282, acc 0.94
2016-09-07T20:16:43.565771: step 1118, loss 0.0597428, acc 0.96
2016-09-07T20:16:44.236797: step 1119, loss 0.0940029, acc 0.98
2016-09-07T20:16:44.904305: step 1120, loss 0.181531, acc 0.94
2016-09-07T20:16:45.585184: step 1121, loss 0.142289, acc 0.94
2016-09-07T20:16:46.262172: step 1122, loss 0.21946, acc 0.9
2016-09-07T20:16:46.950267: step 1123, loss 0.10788, acc 0.94
2016-09-07T20:16:47.618394: step 1124, loss 0.118288, acc 0.98
2016-09-07T20:16:48.304191: step 1125, loss 0.167083, acc 0.88
2016-09-07T20:16:48.985600: step 1126, loss 0.115642, acc 0.98
2016-09-07T20:16:49.665870: step 1127, loss 0.0717284, acc 0.98
2016-09-07T20:16:50.345375: step 1128, loss 0.105603, acc 0.96
2016-09-07T20:16:51.017950: step 1129, loss 0.0910459, acc 0.94
2016-09-07T20:16:51.695856: step 1130, loss 0.0955452, acc 0.96
2016-09-07T20:16:52.379031: step 1131, loss 0.0539904, acc 0.98
2016-09-07T20:16:53.058715: step 1132, loss 0.137699, acc 0.92
2016-09-07T20:16:53.734611: step 1133, loss 0.0650267, acc 0.96
2016-09-07T20:16:54.408086: step 1134, loss 0.0483171, acc 0.98
2016-09-07T20:16:55.086204: step 1135, loss 0.0490878, acc 1
2016-09-07T20:16:55.756055: step 1136, loss 0.0369708, acc 1
2016-09-07T20:16:56.432966: step 1137, loss 0.104417, acc 0.94
2016-09-07T20:16:57.098942: step 1138, loss 0.148754, acc 0.94
2016-09-07T20:16:57.770645: step 1139, loss 0.0709249, acc 0.98
2016-09-07T20:16:58.437570: step 1140, loss 0.0822114, acc 0.98
2016-09-07T20:16:59.093285: step 1141, loss 0.0705591, acc 0.98
2016-09-07T20:16:59.767551: step 1142, loss 0.0608648, acc 0.98
2016-09-07T20:17:00.470740: step 1143, loss 0.132694, acc 0.96
2016-09-07T20:17:01.153440: step 1144, loss 0.0255036, acc 1
2016-09-07T20:17:01.817544: step 1145, loss 0.114599, acc 0.96
2016-09-07T20:17:02.496978: step 1146, loss 0.19057, acc 0.92
2016-09-07T20:17:03.183104: step 1147, loss 0.176583, acc 0.9
2016-09-07T20:17:03.865969: step 1148, loss 0.0899093, acc 0.98
2016-09-07T20:17:04.545629: step 1149, loss 0.0979634, acc 0.92
2016-09-07T20:17:05.228656: step 1150, loss 0.0847184, acc 0.96
2016-09-07T20:17:05.921446: step 1151, loss 0.101557, acc 0.94
2016-09-07T20:17:06.598912: step 1152, loss 0.0554393, acc 0.96
2016-09-07T20:17:07.265055: step 1153, loss 0.0912909, acc 0.96
2016-09-07T20:17:07.924226: step 1154, loss 0.0319255, acc 1
2016-09-07T20:17:08.622633: step 1155, loss 0.0985383, acc 0.94
2016-09-07T20:17:09.291764: step 1156, loss 0.0769268, acc 0.96
2016-09-07T20:17:09.961668: step 1157, loss 0.135587, acc 0.92
2016-09-07T20:17:10.647502: step 1158, loss 0.209154, acc 0.92
2016-09-07T20:17:11.331185: step 1159, loss 0.0680236, acc 0.98
2016-09-07T20:17:11.981769: step 1160, loss 0.0915671, acc 0.98
2016-09-07T20:17:12.670048: step 1161, loss 0.0970937, acc 0.96
2016-09-07T20:17:13.338526: step 1162, loss 0.102114, acc 0.94
2016-09-07T20:17:14.008492: step 1163, loss 0.0823761, acc 0.96
2016-09-07T20:17:14.366715: step 1164, loss 0.119415, acc 1
2016-09-07T20:17:15.048190: step 1165, loss 0.12189, acc 0.94
2016-09-07T20:17:15.710926: step 1166, loss 0.036515, acc 1
2016-09-07T20:17:16.375456: step 1167, loss 0.0629587, acc 0.98
2016-09-07T20:17:17.046187: step 1168, loss 0.0510774, acc 0.98
2016-09-07T20:17:17.740324: step 1169, loss 0.198258, acc 0.94
2016-09-07T20:17:18.407678: step 1170, loss 0.262051, acc 0.92
2016-09-07T20:17:19.088592: step 1171, loss 0.0881722, acc 0.96
2016-09-07T20:17:19.781440: step 1172, loss 0.0234378, acc 1
2016-09-07T20:17:20.444791: step 1173, loss 0.03312, acc 1
2016-09-07T20:17:21.100950: step 1174, loss 0.0701767, acc 0.96
2016-09-07T20:17:21.767064: step 1175, loss 0.0643793, acc 0.98
2016-09-07T20:17:22.437862: step 1176, loss 0.246274, acc 0.9
2016-09-07T20:17:23.114571: step 1177, loss 0.0326864, acc 1
2016-09-07T20:17:23.802300: step 1178, loss 0.0302519, acc 1
2016-09-07T20:17:24.474994: step 1179, loss 0.0283486, acc 0.98
2016-09-07T20:17:25.135502: step 1180, loss 0.0751102, acc 0.96
2016-09-07T20:17:25.808019: step 1181, loss 0.0882463, acc 0.98
2016-09-07T20:17:26.479045: step 1182, loss 0.051401, acc 1
2016-09-07T20:17:27.167406: step 1183, loss 0.120789, acc 0.96
2016-09-07T20:17:27.815812: step 1184, loss 0.072234, acc 0.98
2016-09-07T20:17:28.500229: step 1185, loss 0.0571968, acc 0.98
2016-09-07T20:17:29.168141: step 1186, loss 0.105555, acc 0.96
2016-09-07T20:17:29.844245: step 1187, loss 0.0464492, acc 0.98
2016-09-07T20:17:30.503248: step 1188, loss 0.109488, acc 0.94
2016-09-07T20:17:31.172800: step 1189, loss 0.0664963, acc 0.96
2016-09-07T20:17:31.838424: step 1190, loss 0.0684057, acc 0.96
2016-09-07T20:17:32.513458: step 1191, loss 0.063767, acc 0.96
2016-09-07T20:17:33.189357: step 1192, loss 0.0702837, acc 0.98
2016-09-07T20:17:33.842276: step 1193, loss 0.0748396, acc 0.98
2016-09-07T20:17:34.530203: step 1194, loss 0.0664452, acc 0.98
2016-09-07T20:17:35.203906: step 1195, loss 0.0446022, acc 0.98
2016-09-07T20:17:35.862756: step 1196, loss 0.0958988, acc 0.96
2016-09-07T20:17:36.526633: step 1197, loss 0.0230886, acc 1
2016-09-07T20:17:37.183699: step 1198, loss 0.0552022, acc 0.98
2016-09-07T20:17:37.850823: step 1199, loss 0.118814, acc 0.94
2016-09-07T20:17:38.518757: step 1200, loss 0.0880095, acc 0.96

Evaluation:
2016-09-07T20:17:41.402428: step 1200, loss 1.10313, acc 0.747

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-1200

2016-09-07T20:17:43.004903: step 1201, loss 0.0662761, acc 0.98
2016-09-07T20:17:43.685344: step 1202, loss 0.111897, acc 0.98
2016-09-07T20:17:44.365151: step 1203, loss 0.0755389, acc 0.98
2016-09-07T20:17:45.042797: step 1204, loss 0.0908088, acc 0.98
2016-09-07T20:17:45.708528: step 1205, loss 0.128696, acc 0.96
2016-09-07T20:17:46.385986: step 1206, loss 0.081757, acc 0.98
2016-09-07T20:17:47.051676: step 1207, loss 0.127406, acc 0.94
2016-09-07T20:17:47.730620: step 1208, loss 0.0236035, acc 1
2016-09-07T20:17:48.385813: step 1209, loss 0.0719042, acc 0.94
2016-09-07T20:17:49.075037: step 1210, loss 0.177963, acc 0.96
2016-09-07T20:17:49.744933: step 1211, loss 0.0973447, acc 0.94
2016-09-07T20:17:50.420462: step 1212, loss 0.0737485, acc 1
2016-09-07T20:17:51.078428: step 1213, loss 0.0504464, acc 0.98
2016-09-07T20:17:51.742673: step 1214, loss 0.0343304, acc 1
2016-09-07T20:17:52.398319: step 1215, loss 0.222126, acc 0.96
2016-09-07T20:17:53.065494: step 1216, loss 0.0295499, acc 1
2016-09-07T20:17:53.774779: step 1217, loss 0.0483383, acc 0.98
2016-09-07T20:17:54.452927: step 1218, loss 0.0652433, acc 0.98
2016-09-07T20:17:55.120920: step 1219, loss 0.0952602, acc 0.92
2016-09-07T20:17:55.798360: step 1220, loss 0.103182, acc 0.94
2016-09-07T20:17:56.459842: step 1221, loss 0.11414, acc 0.96
2016-09-07T20:17:57.134761: step 1222, loss 0.0393007, acc 0.98
2016-09-07T20:17:57.810914: step 1223, loss 0.0740774, acc 0.98
2016-09-07T20:17:58.477344: step 1224, loss 0.0833939, acc 0.94
2016-09-07T20:17:59.160398: step 1225, loss 0.0687942, acc 0.94
2016-09-07T20:17:59.841833: step 1226, loss 0.122568, acc 0.96
2016-09-07T20:18:00.546122: step 1227, loss 0.07458, acc 0.96
2016-09-07T20:18:01.197561: step 1228, loss 0.134157, acc 0.94
2016-09-07T20:18:01.878857: step 1229, loss 0.160593, acc 0.94
2016-09-07T20:18:02.565382: step 1230, loss 0.121065, acc 0.96
2016-09-07T20:18:03.230837: step 1231, loss 0.0458137, acc 0.96
2016-09-07T20:18:03.884180: step 1232, loss 0.0742283, acc 0.96
2016-09-07T20:18:04.551497: step 1233, loss 0.0533275, acc 0.98
2016-09-07T20:18:05.224865: step 1234, loss 0.0953274, acc 0.94
2016-09-07T20:18:05.894851: step 1235, loss 0.100497, acc 0.96
2016-09-07T20:18:06.577453: step 1236, loss 0.0897386, acc 0.94
2016-09-07T20:18:07.262624: step 1237, loss 0.0417971, acc 1
2016-09-07T20:18:07.915307: step 1238, loss 0.0174278, acc 1
2016-09-07T20:18:08.588137: step 1239, loss 0.0351486, acc 1
2016-09-07T20:18:09.258356: step 1240, loss 0.283102, acc 0.88
2016-09-07T20:18:09.936962: step 1241, loss 0.0723413, acc 0.96
2016-09-07T20:18:10.582736: step 1242, loss 0.0600742, acc 0.96
2016-09-07T20:18:11.253617: step 1243, loss 0.0565843, acc 0.98
2016-09-07T20:18:11.919194: step 1244, loss 0.165559, acc 0.94
2016-09-07T20:18:12.607710: step 1245, loss 0.143635, acc 0.92
2016-09-07T20:18:13.271790: step 1246, loss 0.164687, acc 0.94
2016-09-07T20:18:13.942871: step 1247, loss 0.0785587, acc 0.96
2016-09-07T20:18:14.633251: step 1248, loss 0.0533321, acc 0.96
2016-09-07T20:18:15.296250: step 1249, loss 0.202484, acc 0.86
2016-09-07T20:18:15.974620: step 1250, loss 0.0675656, acc 1
2016-09-07T20:18:16.631908: step 1251, loss 0.0323651, acc 1
2016-09-07T20:18:17.299128: step 1252, loss 0.118105, acc 0.94
2016-09-07T20:18:17.964943: step 1253, loss 0.230039, acc 0.92
2016-09-07T20:18:18.629106: step 1254, loss 0.0640112, acc 0.98
2016-09-07T20:18:19.289738: step 1255, loss 0.113979, acc 0.98
2016-09-07T20:18:19.931928: step 1256, loss 0.107196, acc 0.96
2016-09-07T20:18:20.589419: step 1257, loss 0.0704995, acc 0.98
2016-09-07T20:18:21.252209: step 1258, loss 0.116421, acc 0.94
2016-09-07T20:18:21.912938: step 1259, loss 0.0751273, acc 0.96
2016-09-07T20:18:22.579336: step 1260, loss 0.104235, acc 0.98
2016-09-07T20:18:23.251827: step 1261, loss 0.0733199, acc 0.96
2016-09-07T20:18:23.922825: step 1262, loss 0.0718904, acc 0.98
2016-09-07T20:18:24.585477: step 1263, loss 0.0747884, acc 0.98
2016-09-07T20:18:25.248773: step 1264, loss 0.129753, acc 0.96
2016-09-07T20:18:25.912008: step 1265, loss 0.0557625, acc 1
2016-09-07T20:18:26.593004: step 1266, loss 0.0848023, acc 0.96
2016-09-07T20:18:27.252760: step 1267, loss 0.107641, acc 0.92
2016-09-07T20:18:27.931464: step 1268, loss 0.127352, acc 0.94
2016-09-07T20:18:28.596049: step 1269, loss 0.0829608, acc 0.96
2016-09-07T20:18:29.261121: step 1270, loss 0.0599963, acc 0.96
2016-09-07T20:18:29.950390: step 1271, loss 0.0264441, acc 1
2016-09-07T20:18:30.615136: step 1272, loss 0.0748861, acc 0.96
2016-09-07T20:18:31.275851: step 1273, loss 0.0934511, acc 0.98
2016-09-07T20:18:31.953906: step 1274, loss 0.0553445, acc 0.98
2016-09-07T20:18:32.627501: step 1275, loss 0.138735, acc 0.94
2016-09-07T20:18:33.290847: step 1276, loss 0.0204186, acc 1
2016-09-07T20:18:33.967805: step 1277, loss 0.126651, acc 0.96
2016-09-07T20:18:34.636305: step 1278, loss 0.0569306, acc 0.96
2016-09-07T20:18:35.304010: step 1279, loss 0.0889586, acc 0.96
2016-09-07T20:18:35.966090: step 1280, loss 0.0966165, acc 0.96
2016-09-07T20:18:36.651715: step 1281, loss 0.12367, acc 0.96
2016-09-07T20:18:37.359415: step 1282, loss 0.0758875, acc 0.98
2016-09-07T20:18:38.047245: step 1283, loss 0.0837646, acc 0.98
2016-09-07T20:18:38.716804: step 1284, loss 0.0582331, acc 0.98
2016-09-07T20:18:39.373567: step 1285, loss 0.131996, acc 0.96
2016-09-07T20:18:40.059652: step 1286, loss 0.064115, acc 0.94
2016-09-07T20:18:40.739025: step 1287, loss 0.12838, acc 0.96
2016-09-07T20:18:41.407211: step 1288, loss 0.072123, acc 0.98
2016-09-07T20:18:42.074717: step 1289, loss 0.276174, acc 0.88
2016-09-07T20:18:42.744654: step 1290, loss 0.0514675, acc 0.98
2016-09-07T20:18:43.424640: step 1291, loss 0.126769, acc 0.92
2016-09-07T20:18:44.086362: step 1292, loss 0.0885563, acc 0.96
2016-09-07T20:18:44.769192: step 1293, loss 0.13006, acc 0.96
2016-09-07T20:18:45.450135: step 1294, loss 0.143621, acc 0.96
2016-09-07T20:18:46.112749: step 1295, loss 0.140627, acc 0.96
2016-09-07T20:18:46.804960: step 1296, loss 0.112222, acc 0.92
2016-09-07T20:18:47.479716: step 1297, loss 0.0277517, acc 1
2016-09-07T20:18:48.159413: step 1298, loss 0.0609584, acc 0.96
2016-09-07T20:18:48.842297: step 1299, loss 0.0898821, acc 0.96
2016-09-07T20:18:49.504249: step 1300, loss 0.231908, acc 0.92

Evaluation:
2016-09-07T20:18:52.400848: step 1300, loss 0.855417, acc 0.758

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-1300

2016-09-07T20:18:54.002811: step 1301, loss 0.156347, acc 0.92
2016-09-07T20:18:54.686614: step 1302, loss 0.0496592, acc 0.98
2016-09-07T20:18:55.360559: step 1303, loss 0.0878054, acc 0.94
2016-09-07T20:18:56.015491: step 1304, loss 0.0741581, acc 0.94
2016-09-07T20:18:56.687626: step 1305, loss 0.0993616, acc 0.98
2016-09-07T20:18:57.347598: step 1306, loss 0.089981, acc 0.96
2016-09-07T20:18:58.006258: step 1307, loss 0.0914472, acc 0.96
2016-09-07T20:18:58.661349: step 1308, loss 0.143362, acc 0.96
2016-09-07T20:18:59.329874: step 1309, loss 0.142881, acc 0.94
2016-09-07T20:18:59.986033: step 1310, loss 0.052477, acc 1
2016-09-07T20:19:00.666635: step 1311, loss 0.158207, acc 0.94
2016-09-07T20:19:01.343095: step 1312, loss 0.0534094, acc 0.98
2016-09-07T20:19:02.007741: step 1313, loss 0.0920624, acc 0.98
2016-09-07T20:19:02.666691: step 1314, loss 0.0599921, acc 0.96
2016-09-07T20:19:03.316685: step 1315, loss 0.241915, acc 0.92
2016-09-07T20:19:03.982517: step 1316, loss 0.0580287, acc 1
2016-09-07T20:19:04.652674: step 1317, loss 0.100688, acc 0.94
2016-09-07T20:19:05.317472: step 1318, loss 0.209248, acc 0.94
2016-09-07T20:19:05.991611: step 1319, loss 0.127215, acc 0.98
2016-09-07T20:19:06.670164: step 1320, loss 0.0512, acc 0.98
2016-09-07T20:19:07.331483: step 1321, loss 0.0637924, acc 0.96
2016-09-07T20:19:08.013254: step 1322, loss 0.130987, acc 0.9
2016-09-07T20:19:08.678478: step 1323, loss 0.0952778, acc 0.96
2016-09-07T20:19:09.361108: step 1324, loss 0.0887201, acc 0.96
2016-09-07T20:19:10.046094: step 1325, loss 0.133032, acc 0.94
2016-09-07T20:19:10.715900: step 1326, loss 0.0791499, acc 0.98
2016-09-07T20:19:11.377547: step 1327, loss 0.0590581, acc 0.98
2016-09-07T20:19:12.050197: step 1328, loss 0.0564993, acc 0.96
2016-09-07T20:19:12.709274: step 1329, loss 0.0167389, acc 1
2016-09-07T20:19:13.395672: step 1330, loss 0.0989328, acc 0.96
2016-09-07T20:19:14.077695: step 1331, loss 0.0567561, acc 0.98
2016-09-07T20:19:14.726438: step 1332, loss 0.0785618, acc 0.98
2016-09-07T20:19:15.392280: step 1333, loss 0.0757472, acc 0.98
2016-09-07T20:19:16.060968: step 1334, loss 0.100843, acc 0.96
2016-09-07T20:19:16.739834: step 1335, loss 0.103716, acc 0.96
2016-09-07T20:19:17.413097: step 1336, loss 0.104815, acc 0.96
2016-09-07T20:19:18.081310: step 1337, loss 0.174769, acc 0.94
2016-09-07T20:19:18.751601: step 1338, loss 0.0437739, acc 1
2016-09-07T20:19:19.403830: step 1339, loss 0.0746193, acc 0.96
2016-09-07T20:19:20.054688: step 1340, loss 0.058317, acc 1
2016-09-07T20:19:20.723754: step 1341, loss 0.0251321, acc 1
2016-09-07T20:19:21.407260: step 1342, loss 0.0774462, acc 0.96
2016-09-07T20:19:22.061879: step 1343, loss 0.0733551, acc 0.98
2016-09-07T20:19:22.749179: step 1344, loss 0.0476502, acc 1
2016-09-07T20:19:23.421339: step 1345, loss 0.035899, acc 1
2016-09-07T20:19:24.078911: step 1346, loss 0.109757, acc 0.96
2016-09-07T20:19:24.748961: step 1347, loss 0.111982, acc 0.94
2016-09-07T20:19:25.431650: step 1348, loss 0.0506217, acc 0.98
2016-09-07T20:19:26.101609: step 1349, loss 0.0568732, acc 0.98
2016-09-07T20:19:26.786165: step 1350, loss 0.0307064, acc 1
2016-09-07T20:19:27.446217: step 1351, loss 0.242407, acc 0.92
2016-09-07T20:19:28.141528: step 1352, loss 0.120922, acc 0.98
2016-09-07T20:19:28.827024: step 1353, loss 0.0278297, acc 0.98
2016-09-07T20:19:29.502000: step 1354, loss 0.177482, acc 0.9
2016-09-07T20:19:30.184544: step 1355, loss 0.037327, acc 1
2016-09-07T20:19:30.850069: step 1356, loss 0.0356898, acc 0.98
2016-09-07T20:19:31.531609: step 1357, loss 0.0302598, acc 0.98
2016-09-07T20:19:31.890137: step 1358, loss 0.284628, acc 0.916667
2016-09-07T20:19:32.554774: step 1359, loss 0.0438235, acc 0.96
2016-09-07T20:19:33.218331: step 1360, loss 0.129518, acc 0.98
2016-09-07T20:19:33.880741: step 1361, loss 0.152108, acc 0.96
2016-09-07T20:19:34.539582: step 1362, loss 0.0300269, acc 1
2016-09-07T20:19:35.196275: step 1363, loss 0.0908829, acc 0.96
2016-09-07T20:19:35.887655: step 1364, loss 0.0608334, acc 0.96
2016-09-07T20:19:36.567097: step 1365, loss 0.0744618, acc 0.98
2016-09-07T20:19:37.251711: step 1366, loss 0.1337, acc 0.94
2016-09-07T20:19:37.914428: step 1367, loss 0.034515, acc 1
2016-09-07T20:19:38.569827: step 1368, loss 0.0950464, acc 0.96
2016-09-07T20:19:39.238065: step 1369, loss 0.0573008, acc 0.98
2016-09-07T20:19:39.911417: step 1370, loss 0.0294349, acc 0.98
2016-09-07T20:19:40.563004: step 1371, loss 0.177844, acc 0.94
2016-09-07T20:19:41.224225: step 1372, loss 0.114765, acc 0.94
2016-09-07T20:19:41.895288: step 1373, loss 0.0730436, acc 0.98
2016-09-07T20:19:42.551206: step 1374, loss 0.04527, acc 1
2016-09-07T20:19:43.218642: step 1375, loss 0.0793135, acc 0.98
2016-09-07T20:19:43.906809: step 1376, loss 0.0262044, acc 1
2016-09-07T20:19:44.575995: step 1377, loss 0.106949, acc 0.94
2016-09-07T20:19:45.250282: step 1378, loss 0.0490595, acc 0.98
2016-09-07T20:19:45.913283: step 1379, loss 0.114397, acc 0.92
2016-09-07T20:19:46.580637: step 1380, loss 0.0898237, acc 0.94
2016-09-07T20:19:47.235626: step 1381, loss 0.0346319, acc 1
2016-09-07T20:19:47.906496: step 1382, loss 0.0441053, acc 0.98
2016-09-07T20:19:48.588080: step 1383, loss 0.0640116, acc 0.98
2016-09-07T20:19:49.232798: step 1384, loss 0.0871926, acc 0.94
2016-09-07T20:19:49.903236: step 1385, loss 0.0501109, acc 0.98
2016-09-07T20:19:50.563818: step 1386, loss 0.129766, acc 0.94
2016-09-07T20:19:51.247097: step 1387, loss 0.089581, acc 0.96
2016-09-07T20:19:51.920669: step 1388, loss 0.0292547, acc 1
2016-09-07T20:19:52.593116: step 1389, loss 0.0595112, acc 1
2016-09-07T20:19:53.258626: step 1390, loss 0.07535, acc 0.98
2016-09-07T20:19:53.933766: step 1391, loss 0.0508292, acc 1
2016-09-07T20:19:54.606458: step 1392, loss 0.101998, acc 0.98
2016-09-07T20:19:55.292110: step 1393, loss 0.0131007, acc 1
2016-09-07T20:19:55.950518: step 1394, loss 0.0528117, acc 1
2016-09-07T20:19:56.614110: step 1395, loss 0.0424196, acc 0.98
2016-09-07T20:19:57.277482: step 1396, loss 0.125763, acc 0.94
2016-09-07T20:19:57.938478: step 1397, loss 0.0351554, acc 0.98
2016-09-07T20:19:58.619842: step 1398, loss 0.0367213, acc 0.98
2016-09-07T20:19:59.283628: step 1399, loss 0.0514508, acc 1
2016-09-07T20:19:59.954802: step 1400, loss 0.060838, acc 0.96

Evaluation:
2016-09-07T20:20:02.838077: step 1400, loss 1.24959, acc 0.763

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-1400

2016-09-07T20:20:04.403285: step 1401, loss 0.0743303, acc 0.96
2016-09-07T20:20:05.084124: step 1402, loss 0.0531255, acc 0.98
2016-09-07T20:20:05.759188: step 1403, loss 0.0153551, acc 1
2016-09-07T20:20:06.420415: step 1404, loss 0.027927, acc 1
2016-09-07T20:20:07.080036: step 1405, loss 0.123059, acc 0.94
2016-09-07T20:20:07.785245: step 1406, loss 0.117165, acc 0.94
2016-09-07T20:20:08.462876: step 1407, loss 0.0518027, acc 0.96
2016-09-07T20:20:09.146929: step 1408, loss 0.0793264, acc 0.96
2016-09-07T20:20:09.809299: step 1409, loss 0.0624439, acc 0.96
2016-09-07T20:20:10.484614: step 1410, loss 0.0176181, acc 1
2016-09-07T20:20:11.148452: step 1411, loss 0.0149778, acc 1
2016-09-07T20:20:11.817155: step 1412, loss 0.230283, acc 0.9
2016-09-07T20:20:12.487375: step 1413, loss 0.0171105, acc 1
2016-09-07T20:20:13.161097: step 1414, loss 0.0664219, acc 0.96
2016-09-07T20:20:13.827057: step 1415, loss 0.137755, acc 0.98
2016-09-07T20:20:14.494808: step 1416, loss 0.0155983, acc 1
2016-09-07T20:20:15.171699: step 1417, loss 0.0315506, acc 0.98
2016-09-07T20:20:15.839997: step 1418, loss 0.0231602, acc 1
2016-09-07T20:20:16.512660: step 1419, loss 0.12663, acc 0.96
2016-09-07T20:20:17.180178: step 1420, loss 0.0666316, acc 0.96
2016-09-07T20:20:17.863655: step 1421, loss 0.112368, acc 0.94
2016-09-07T20:20:18.534012: step 1422, loss 0.0662432, acc 0.98
2016-09-07T20:20:19.212617: step 1423, loss 0.141827, acc 0.96
2016-09-07T20:20:19.888173: step 1424, loss 0.189412, acc 0.94
2016-09-07T20:20:20.577275: step 1425, loss 0.0919394, acc 0.94
2016-09-07T20:20:21.258554: step 1426, loss 0.0614087, acc 0.96
2016-09-07T20:20:21.918140: step 1427, loss 0.110992, acc 0.96
2016-09-07T20:20:22.604083: step 1428, loss 0.0470451, acc 1
2016-09-07T20:20:23.282407: step 1429, loss 0.172655, acc 0.92
2016-09-07T20:20:23.951657: step 1430, loss 0.0361276, acc 0.98
2016-09-07T20:20:24.614838: step 1431, loss 0.0731469, acc 0.94
2016-09-07T20:20:25.272660: step 1432, loss 0.0604856, acc 0.98
2016-09-07T20:20:25.940478: step 1433, loss 0.0454391, acc 1
2016-09-07T20:20:26.590573: step 1434, loss 0.0764236, acc 0.98
2016-09-07T20:20:27.253301: step 1435, loss 0.0888847, acc 0.98
2016-09-07T20:20:27.937150: step 1436, loss 0.114751, acc 0.92
2016-09-07T20:20:28.623075: step 1437, loss 0.10664, acc 0.96
2016-09-07T20:20:29.300930: step 1438, loss 0.0647412, acc 0.98
2016-09-07T20:20:29.977926: step 1439, loss 0.0989961, acc 0.94
2016-09-07T20:20:30.653023: step 1440, loss 0.0630439, acc 0.94
2016-09-07T20:20:31.317686: step 1441, loss 0.0652526, acc 0.96
2016-09-07T20:20:31.987621: step 1442, loss 0.0648942, acc 0.96
2016-09-07T20:20:32.654359: step 1443, loss 0.0430746, acc 0.96
2016-09-07T20:20:33.329394: step 1444, loss 0.040373, acc 0.98
2016-09-07T20:20:34.003258: step 1445, loss 0.0794024, acc 0.98
2016-09-07T20:20:34.653905: step 1446, loss 0.1128, acc 0.96
2016-09-07T20:20:35.322615: step 1447, loss 0.042804, acc 0.98
2016-09-07T20:20:35.985972: step 1448, loss 0.216864, acc 0.92
2016-09-07T20:20:36.641726: step 1449, loss 0.209964, acc 0.92
2016-09-07T20:20:37.320304: step 1450, loss 0.145166, acc 0.94
2016-09-07T20:20:37.989540: step 1451, loss 0.062472, acc 0.94
2016-09-07T20:20:38.647200: step 1452, loss 0.078228, acc 0.96
2016-09-07T20:20:39.307580: step 1453, loss 0.0746106, acc 0.96
2016-09-07T20:20:39.990677: step 1454, loss 0.0713585, acc 0.98
2016-09-07T20:20:40.650445: step 1455, loss 0.0602024, acc 0.98
2016-09-07T20:20:41.341453: step 1456, loss 0.138682, acc 0.94
2016-09-07T20:20:41.994347: step 1457, loss 0.0771742, acc 0.96
2016-09-07T20:20:42.665051: step 1458, loss 0.059002, acc 0.96
2016-09-07T20:20:43.332268: step 1459, loss 0.0433105, acc 1
2016-09-07T20:20:44.003835: step 1460, loss 0.0773021, acc 0.96
2016-09-07T20:20:44.654903: step 1461, loss 0.0533222, acc 1
2016-09-07T20:20:45.321449: step 1462, loss 0.0814896, acc 0.96
2016-09-07T20:20:46.001190: step 1463, loss 0.0539573, acc 0.96
2016-09-07T20:20:46.669390: step 1464, loss 0.198869, acc 0.96
2016-09-07T20:20:47.338578: step 1465, loss 0.102742, acc 0.96
2016-09-07T20:20:48.001580: step 1466, loss 0.0677916, acc 0.96
2016-09-07T20:20:48.662240: step 1467, loss 0.14246, acc 0.92
2016-09-07T20:20:49.345593: step 1468, loss 0.0673969, acc 0.98
2016-09-07T20:20:50.003245: step 1469, loss 0.0682371, acc 0.98
2016-09-07T20:20:50.664240: step 1470, loss 0.0240157, acc 1
2016-09-07T20:20:51.351901: step 1471, loss 0.0519281, acc 0.98
2016-09-07T20:20:51.997974: step 1472, loss 0.0928176, acc 0.98
2016-09-07T20:20:52.650138: step 1473, loss 0.0193524, acc 1
2016-09-07T20:20:53.325394: step 1474, loss 0.068748, acc 0.96
2016-09-07T20:20:53.998473: step 1475, loss 0.103938, acc 0.94
2016-09-07T20:20:54.683928: step 1476, loss 0.0890965, acc 0.96
2016-09-07T20:20:55.370999: step 1477, loss 0.0274023, acc 1
2016-09-07T20:20:56.035816: step 1478, loss 0.124771, acc 0.94
2016-09-07T20:20:56.712057: step 1479, loss 0.110961, acc 0.94
2016-09-07T20:20:57.365687: step 1480, loss 0.0474721, acc 1
2016-09-07T20:20:58.049987: step 1481, loss 0.055146, acc 0.96
2016-09-07T20:20:58.708446: step 1482, loss 0.0370513, acc 1
2016-09-07T20:20:59.371375: step 1483, loss 0.162466, acc 0.88
2016-09-07T20:21:00.043407: step 1484, loss 0.108841, acc 0.9
2016-09-07T20:21:00.779119: step 1485, loss 0.119069, acc 0.94
2016-09-07T20:21:01.457544: step 1486, loss 0.0762067, acc 0.96
2016-09-07T20:21:02.120256: step 1487, loss 0.0374428, acc 0.98
2016-09-07T20:21:02.776111: step 1488, loss 0.0385382, acc 0.98
2016-09-07T20:21:03.463757: step 1489, loss 0.0960657, acc 0.96
2016-09-07T20:21:04.136069: step 1490, loss 0.0717196, acc 0.96
2016-09-07T20:21:04.786783: step 1491, loss 0.0596993, acc 0.98
2016-09-07T20:21:05.455359: step 1492, loss 0.0718764, acc 0.98
2016-09-07T20:21:06.138370: step 1493, loss 0.0545195, acc 0.98
2016-09-07T20:21:06.817493: step 1494, loss 0.120517, acc 0.96
2016-09-07T20:21:07.497437: step 1495, loss 0.0583015, acc 1
2016-09-07T20:21:08.196648: step 1496, loss 0.0651443, acc 0.98
2016-09-07T20:21:08.867664: step 1497, loss 0.112352, acc 0.96
2016-09-07T20:21:09.550871: step 1498, loss 0.0437987, acc 0.98
2016-09-07T20:21:10.209443: step 1499, loss 0.118499, acc 0.96
2016-09-07T20:21:10.856026: step 1500, loss 0.0458555, acc 1

Evaluation:
2016-09-07T20:21:13.726253: step 1500, loss 1.07891, acc 0.769

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-1500

2016-09-07T20:21:15.382212: step 1501, loss 0.0808064, acc 0.96
2016-09-07T20:21:16.048669: step 1502, loss 0.0377717, acc 1
2016-09-07T20:21:16.715985: step 1503, loss 0.0534039, acc 0.98
2016-09-07T20:21:17.387457: step 1504, loss 0.132196, acc 0.94
2016-09-07T20:21:18.068317: step 1505, loss 0.0624671, acc 0.98
2016-09-07T20:21:18.735223: step 1506, loss 0.166452, acc 0.94
2016-09-07T20:21:19.426941: step 1507, loss 0.0612863, acc 0.96
2016-09-07T20:21:20.091405: step 1508, loss 0.00584364, acc 1
2016-09-07T20:21:20.755485: step 1509, loss 0.162662, acc 0.96
2016-09-07T20:21:21.427801: step 1510, loss 0.0605701, acc 0.96
2016-09-07T20:21:22.111022: step 1511, loss 0.122864, acc 0.96
2016-09-07T20:21:22.782050: step 1512, loss 0.132493, acc 0.96
2016-09-07T20:21:23.457233: step 1513, loss 0.0841191, acc 0.96
2016-09-07T20:21:24.149495: step 1514, loss 0.0293537, acc 0.98
2016-09-07T20:21:24.818396: step 1515, loss 0.0453169, acc 0.98
2016-09-07T20:21:25.481450: step 1516, loss 0.158667, acc 0.96
2016-09-07T20:21:26.166692: step 1517, loss 0.0279527, acc 1
2016-09-07T20:21:26.836552: step 1518, loss 0.061091, acc 0.96
2016-09-07T20:21:27.497790: step 1519, loss 0.0831816, acc 0.94
2016-09-07T20:21:28.178750: step 1520, loss 0.0462255, acc 0.98
2016-09-07T20:21:28.854506: step 1521, loss 0.0306732, acc 0.98
2016-09-07T20:21:29.520623: step 1522, loss 0.187561, acc 0.94
2016-09-07T20:21:30.209059: step 1523, loss 0.0645304, acc 0.96
2016-09-07T20:21:30.899748: step 1524, loss 0.171152, acc 0.94
2016-09-07T20:21:31.566758: step 1525, loss 0.048041, acc 0.98
2016-09-07T20:21:32.257529: step 1526, loss 0.0304987, acc 0.98
2016-09-07T20:21:32.936054: step 1527, loss 0.156901, acc 0.92
2016-09-07T20:21:33.605785: step 1528, loss 0.0589049, acc 0.98
2016-09-07T20:21:34.274917: step 1529, loss 0.0548506, acc 0.98
2016-09-07T20:21:34.964069: step 1530, loss 0.0634182, acc 0.94
2016-09-07T20:21:35.633568: step 1531, loss 0.0709414, acc 0.96
2016-09-07T20:21:36.316936: step 1532, loss 0.100982, acc 0.94
2016-09-07T20:21:37.021440: step 1533, loss 0.0654973, acc 0.98
2016-09-07T20:21:37.696988: step 1534, loss 0.0644072, acc 0.98
2016-09-07T20:21:38.376669: step 1535, loss 0.0578611, acc 0.98
2016-09-07T20:21:39.050624: step 1536, loss 0.00978437, acc 1
2016-09-07T20:21:39.709141: step 1537, loss 0.0512655, acc 0.96
2016-09-07T20:21:40.367170: step 1538, loss 0.105049, acc 0.96
2016-09-07T20:21:41.027969: step 1539, loss 0.0675844, acc 0.98
2016-09-07T20:21:41.716481: step 1540, loss 0.0414001, acc 0.96
2016-09-07T20:21:42.398293: step 1541, loss 0.0712978, acc 0.96
2016-09-07T20:21:43.066981: step 1542, loss 0.0375807, acc 0.98
2016-09-07T20:21:43.749065: step 1543, loss 0.0565425, acc 0.98
2016-09-07T20:21:44.423008: step 1544, loss 0.122512, acc 0.96
2016-09-07T20:21:45.074715: step 1545, loss 0.12713, acc 0.96
2016-09-07T20:21:45.743789: step 1546, loss 0.16211, acc 0.94
2016-09-07T20:21:46.419515: step 1547, loss 0.225483, acc 0.92
2016-09-07T20:21:47.086989: step 1548, loss 0.0663892, acc 0.96
2016-09-07T20:21:47.758535: step 1549, loss 0.0178442, acc 1
2016-09-07T20:21:48.428492: step 1550, loss 0.0747507, acc 0.98
2016-09-07T20:21:49.095972: step 1551, loss 0.0920724, acc 0.96
2016-09-07T20:21:49.459971: step 1552, loss 0.00699256, acc 1
2016-09-07T20:21:50.140867: step 1553, loss 0.0572391, acc 1
2016-09-07T20:21:50.836430: step 1554, loss 0.0604909, acc 0.96
2016-09-07T20:21:51.496460: step 1555, loss 0.0757242, acc 1
2016-09-07T20:21:52.177903: step 1556, loss 0.0102026, acc 1
2016-09-07T20:21:52.861791: step 1557, loss 0.0852287, acc 0.98
2016-09-07T20:21:53.524301: step 1558, loss 0.135118, acc 0.96
2016-09-07T20:21:54.174199: step 1559, loss 0.0431433, acc 1
2016-09-07T20:21:54.851178: step 1560, loss 0.0681653, acc 0.96
2016-09-07T20:21:55.495376: step 1561, loss 0.0397347, acc 0.98
2016-09-07T20:21:56.159371: step 1562, loss 0.0759499, acc 0.94
2016-09-07T20:21:56.833528: step 1563, loss 0.0684331, acc 0.96
2016-09-07T20:21:57.516278: step 1564, loss 0.0913157, acc 0.96
2016-09-07T20:21:58.199546: step 1565, loss 0.108107, acc 0.94
2016-09-07T20:21:58.892440: step 1566, loss 0.0644221, acc 0.96
2016-09-07T20:21:59.559991: step 1567, loss 0.0278065, acc 1
2016-09-07T20:22:00.241409: step 1568, loss 0.065958, acc 0.98
2016-09-07T20:22:00.888810: step 1569, loss 0.0930894, acc 0.94
2016-09-07T20:22:01.546640: step 1570, loss 0.0456573, acc 0.96
2016-09-07T20:22:02.224156: step 1571, loss 0.0492429, acc 0.98
2016-09-07T20:22:02.896666: step 1572, loss 0.0383575, acc 1
2016-09-07T20:22:03.557551: step 1573, loss 0.035221, acc 1
2016-09-07T20:22:04.221692: step 1574, loss 0.08124, acc 0.96
2016-09-07T20:22:04.889316: step 1575, loss 0.0187043, acc 1
2016-09-07T20:22:05.543395: step 1576, loss 0.184366, acc 0.92
2016-09-07T20:22:06.212414: step 1577, loss 0.0456335, acc 0.98
2016-09-07T20:22:06.872969: step 1578, loss 0.00594178, acc 1
2016-09-07T20:22:07.556283: step 1579, loss 0.0968394, acc 0.96
2016-09-07T20:22:08.223316: step 1580, loss 0.167208, acc 0.96
2016-09-07T20:22:08.895116: step 1581, loss 0.0232616, acc 1
2016-09-07T20:22:09.551650: step 1582, loss 0.164368, acc 0.94
2016-09-07T20:22:10.220311: step 1583, loss 0.0747408, acc 0.98
2016-09-07T20:22:10.882939: step 1584, loss 0.0842774, acc 0.96
2016-09-07T20:22:11.543789: step 1585, loss 0.0740345, acc 0.98
2016-09-07T20:22:12.219107: step 1586, loss 0.0584295, acc 0.96
2016-09-07T20:22:12.879692: step 1587, loss 0.030924, acc 1
2016-09-07T20:22:13.558222: step 1588, loss 0.0764963, acc 0.98
2016-09-07T20:22:14.218829: step 1589, loss 0.0436769, acc 0.98
2016-09-07T20:22:14.884148: step 1590, loss 0.0858692, acc 0.98
2016-09-07T20:22:15.568852: step 1591, loss 0.0826359, acc 0.98
2016-09-07T20:22:16.222393: step 1592, loss 0.0580266, acc 0.98
2016-09-07T20:22:16.882555: step 1593, loss 0.196167, acc 0.92
2016-09-07T20:22:17.543881: step 1594, loss 0.100593, acc 0.94
2016-09-07T20:22:18.219358: step 1595, loss 0.0827203, acc 0.98
2016-09-07T20:22:18.907969: step 1596, loss 0.0518095, acc 0.98
2016-09-07T20:22:19.577452: step 1597, loss 0.103479, acc 0.96
2016-09-07T20:22:20.277035: step 1598, loss 0.0690493, acc 0.96
2016-09-07T20:22:20.941776: step 1599, loss 0.0305481, acc 1
2016-09-07T20:22:21.610547: step 1600, loss 0.060953, acc 0.96

Evaluation:
2016-09-07T20:22:24.489658: step 1600, loss 1.13105, acc 0.741

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-1600

2016-09-07T20:22:26.069620: step 1601, loss 0.020656, acc 1
2016-09-07T20:22:26.746704: step 1602, loss 0.0298724, acc 1
2016-09-07T20:22:27.407556: step 1603, loss 0.0730468, acc 0.96
2016-09-07T20:22:28.052652: step 1604, loss 0.0560154, acc 0.98
2016-09-07T20:22:28.732037: step 1605, loss 0.0215084, acc 1
2016-09-07T20:22:29.396063: step 1606, loss 0.0510102, acc 0.96
2016-09-07T20:22:30.061812: step 1607, loss 0.0490087, acc 1
2016-09-07T20:22:30.726130: step 1608, loss 0.0869743, acc 0.98
2016-09-07T20:22:31.384139: step 1609, loss 0.129707, acc 0.96
2016-09-07T20:22:32.052576: step 1610, loss 0.0134154, acc 1
2016-09-07T20:22:32.738287: step 1611, loss 0.0300816, acc 1
2016-09-07T20:22:33.404171: step 1612, loss 0.0715575, acc 0.96
2016-09-07T20:22:34.067901: step 1613, loss 0.135171, acc 0.96
2016-09-07T20:22:34.732273: step 1614, loss 0.0849224, acc 0.96
2016-09-07T20:22:35.393731: step 1615, loss 0.0364026, acc 0.98
2016-09-07T20:22:36.077897: step 1616, loss 0.0952097, acc 0.96
2016-09-07T20:22:36.749947: step 1617, loss 0.0424502, acc 0.98
2016-09-07T20:22:37.430836: step 1618, loss 0.122364, acc 0.98
2016-09-07T20:22:38.095229: step 1619, loss 0.123068, acc 0.96
2016-09-07T20:22:38.762900: step 1620, loss 0.0718662, acc 0.92
2016-09-07T20:22:39.446714: step 1621, loss 0.0538672, acc 0.98
2016-09-07T20:22:40.118776: step 1622, loss 0.047359, acc 0.98
2016-09-07T20:22:40.794347: step 1623, loss 0.0557159, acc 0.98
2016-09-07T20:22:41.471325: step 1624, loss 0.0413697, acc 0.98
2016-09-07T20:22:42.143094: step 1625, loss 0.15622, acc 0.96
2016-09-07T20:22:42.831300: step 1626, loss 0.0306964, acc 1
2016-09-07T20:22:43.511726: step 1627, loss 0.131286, acc 0.94
2016-09-07T20:22:44.181369: step 1628, loss 0.0578156, acc 1
2016-09-07T20:22:44.860052: step 1629, loss 0.129111, acc 0.98
2016-09-07T20:22:45.530209: step 1630, loss 0.0911287, acc 0.94
2016-09-07T20:22:46.204298: step 1631, loss 0.0507995, acc 0.98
2016-09-07T20:22:46.867659: step 1632, loss 0.0511461, acc 0.96
2016-09-07T20:22:47.537933: step 1633, loss 0.0635995, acc 0.98
2016-09-07T20:22:48.224372: step 1634, loss 0.113818, acc 0.96
2016-09-07T20:22:48.902263: step 1635, loss 0.0877723, acc 0.98
2016-09-07T20:22:49.560456: step 1636, loss 0.044701, acc 0.98
2016-09-07T20:22:50.247946: step 1637, loss 0.0322607, acc 0.98
2016-09-07T20:22:50.923605: step 1638, loss 0.0318494, acc 1
2016-09-07T20:22:51.578430: step 1639, loss 0.0160715, acc 1
2016-09-07T20:22:52.250689: step 1640, loss 0.0652722, acc 0.98
2016-09-07T20:22:52.948072: step 1641, loss 0.0439474, acc 0.98
2016-09-07T20:22:53.615908: step 1642, loss 0.110921, acc 0.94
2016-09-07T20:22:54.301345: step 1643, loss 0.0913175, acc 0.94
2016-09-07T20:22:54.984286: step 1644, loss 0.0601058, acc 0.96
2016-09-07T20:22:55.652817: step 1645, loss 0.186834, acc 0.94
2016-09-07T20:22:56.320410: step 1646, loss 0.0853219, acc 0.94
2016-09-07T20:22:56.994131: step 1647, loss 0.0604415, acc 0.98
2016-09-07T20:22:57.663737: step 1648, loss 0.0297312, acc 1
2016-09-07T20:22:58.325867: step 1649, loss 0.0465786, acc 1
2016-09-07T20:22:59.002382: step 1650, loss 0.173266, acc 0.88
2016-09-07T20:22:59.712720: step 1651, loss 0.0836832, acc 0.96
2016-09-07T20:23:00.395708: step 1652, loss 0.0314143, acc 0.98
2016-09-07T20:23:01.067880: step 1653, loss 0.017238, acc 1
2016-09-07T20:23:01.711959: step 1654, loss 0.0332833, acc 1
2016-09-07T20:23:02.390157: step 1655, loss 0.104145, acc 0.94
2016-09-07T20:23:03.060972: step 1656, loss 0.0430043, acc 1
2016-09-07T20:23:03.719605: step 1657, loss 0.0905883, acc 0.98
2016-09-07T20:23:04.386024: step 1658, loss 0.0922527, acc 0.94
2016-09-07T20:23:05.060194: step 1659, loss 0.0564899, acc 0.96
2016-09-07T20:23:05.751795: step 1660, loss 0.0226343, acc 1
2016-09-07T20:23:06.418408: step 1661, loss 0.0980806, acc 0.96
2016-09-07T20:23:07.077293: step 1662, loss 0.0338808, acc 1
2016-09-07T20:23:07.755792: step 1663, loss 0.0895047, acc 0.96
2016-09-07T20:23:08.419064: step 1664, loss 0.00789915, acc 1
2016-09-07T20:23:09.086720: step 1665, loss 0.0785623, acc 0.98
2016-09-07T20:23:09.749787: step 1666, loss 0.0330358, acc 0.98
2016-09-07T20:23:10.413767: step 1667, loss 0.138649, acc 0.96
2016-09-07T20:23:11.083844: step 1668, loss 0.0346157, acc 0.98
2016-09-07T20:23:11.749762: step 1669, loss 0.0880274, acc 0.96
2016-09-07T20:23:12.403571: step 1670, loss 0.0441178, acc 0.98
2016-09-07T20:23:13.085844: step 1671, loss 0.0480018, acc 1
2016-09-07T20:23:13.743292: step 1672, loss 0.0682131, acc 0.98
2016-09-07T20:23:14.424452: step 1673, loss 0.0178912, acc 1
2016-09-07T20:23:15.102827: step 1674, loss 0.11113, acc 0.94
2016-09-07T20:23:15.762384: step 1675, loss 0.0284364, acc 1
2016-09-07T20:23:16.438420: step 1676, loss 0.0408729, acc 0.98
2016-09-07T20:23:17.111340: step 1677, loss 0.115492, acc 0.96
2016-09-07T20:23:17.783223: step 1678, loss 0.0892933, acc 0.96
2016-09-07T20:23:18.451406: step 1679, loss 0.135401, acc 0.94
2016-09-07T20:23:19.128560: step 1680, loss 0.0154966, acc 1
2016-09-07T20:23:19.818690: step 1681, loss 0.0966476, acc 0.98
2016-09-07T20:23:20.486894: step 1682, loss 0.133539, acc 0.94
2016-09-07T20:23:21.159543: step 1683, loss 0.0868521, acc 0.96
2016-09-07T20:23:21.826699: step 1684, loss 0.125812, acc 0.92
2016-09-07T20:23:22.514325: step 1685, loss 0.0445731, acc 0.98
2016-09-07T20:23:23.184345: step 1686, loss 0.0454906, acc 0.96
2016-09-07T20:23:23.836580: step 1687, loss 0.0735976, acc 0.98
2016-09-07T20:23:24.504988: step 1688, loss 0.0328146, acc 1
2016-09-07T20:23:25.173377: step 1689, loss 0.134267, acc 0.92
2016-09-07T20:23:25.850127: step 1690, loss 0.0426767, acc 0.98
2016-09-07T20:23:26.524176: step 1691, loss 0.0152363, acc 1
2016-09-07T20:23:27.207367: step 1692, loss 0.103822, acc 0.94
2016-09-07T20:23:27.879624: step 1693, loss 0.190031, acc 0.96
2016-09-07T20:23:28.558364: step 1694, loss 0.169346, acc 0.96
2016-09-07T20:23:29.226142: step 1695, loss 0.0413306, acc 1
2016-09-07T20:23:29.904870: step 1696, loss 0.0188815, acc 1
2016-09-07T20:23:30.561512: step 1697, loss 0.0352333, acc 1
2016-09-07T20:23:31.212672: step 1698, loss 0.0525678, acc 1
2016-09-07T20:23:31.868716: step 1699, loss 0.0458591, acc 0.98
2016-09-07T20:23:32.535566: step 1700, loss 0.0581723, acc 0.98

Evaluation:
2016-09-07T20:23:35.397696: step 1700, loss 1.0455, acc 0.752

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-1700

2016-09-07T20:23:37.059107: step 1701, loss 0.0746722, acc 0.96
2016-09-07T20:23:37.737208: step 1702, loss 0.0917218, acc 0.98
2016-09-07T20:23:38.416775: step 1703, loss 0.0832299, acc 0.96
2016-09-07T20:23:39.077146: step 1704, loss 0.177782, acc 0.94
2016-09-07T20:23:39.751041: step 1705, loss 0.0726178, acc 0.96
2016-09-07T20:23:40.434213: step 1706, loss 0.275696, acc 0.9
2016-09-07T20:23:41.092446: step 1707, loss 0.101427, acc 0.92
2016-09-07T20:23:41.753548: step 1708, loss 0.040637, acc 1
2016-09-07T20:23:42.403695: step 1709, loss 0.0113244, acc 1
2016-09-07T20:23:43.066288: step 1710, loss 0.163604, acc 0.92
2016-09-07T20:23:43.721841: step 1711, loss 0.0964232, acc 0.96
2016-09-07T20:23:44.374921: step 1712, loss 0.0542142, acc 0.96
2016-09-07T20:23:45.012329: step 1713, loss 0.0787865, acc 0.96
2016-09-07T20:23:45.659282: step 1714, loss 0.0383609, acc 1
2016-09-07T20:23:46.324769: step 1715, loss 0.0676901, acc 0.96
2016-09-07T20:23:46.991853: step 1716, loss 0.0508635, acc 0.96
2016-09-07T20:23:47.656443: step 1717, loss 0.0425778, acc 1
2016-09-07T20:23:48.328522: step 1718, loss 0.11402, acc 0.96
2016-09-07T20:23:49.005452: step 1719, loss 0.0239001, acc 1
2016-09-07T20:23:49.687992: step 1720, loss 0.0861596, acc 0.98
2016-09-07T20:23:50.356722: step 1721, loss 0.165678, acc 0.92
2016-09-07T20:23:51.029896: step 1722, loss 0.101966, acc 0.94
2016-09-07T20:23:51.734773: step 1723, loss 0.13137, acc 0.94
2016-09-07T20:23:52.439925: step 1724, loss 0.0486978, acc 0.96
2016-09-07T20:23:53.098931: step 1725, loss 0.138946, acc 0.92
2016-09-07T20:23:53.755085: step 1726, loss 0.0573071, acc 0.98
2016-09-07T20:23:54.411610: step 1727, loss 0.02769, acc 1
2016-09-07T20:23:55.096821: step 1728, loss 0.13878, acc 0.96
2016-09-07T20:23:55.768805: step 1729, loss 0.0816451, acc 0.94
2016-09-07T20:23:56.445329: step 1730, loss 0.063058, acc 0.94
2016-09-07T20:23:57.118183: step 1731, loss 0.074622, acc 0.98
2016-09-07T20:23:57.792101: step 1732, loss 0.0431603, acc 0.98
2016-09-07T20:23:58.463737: step 1733, loss 0.0977719, acc 0.96
2016-09-07T20:23:59.131117: step 1734, loss 0.0534878, acc 1
2016-09-07T20:23:59.792147: step 1735, loss 0.0516803, acc 0.96
2016-09-07T20:24:00.498953: step 1736, loss 0.071425, acc 0.96
2016-09-07T20:24:01.164625: step 1737, loss 0.0560124, acc 0.98
2016-09-07T20:24:01.846724: step 1738, loss 0.0430976, acc 1
2016-09-07T20:24:02.521773: step 1739, loss 0.136724, acc 0.92
2016-09-07T20:24:03.163130: step 1740, loss 0.028546, acc 1
2016-09-07T20:24:03.824168: step 1741, loss 0.0440537, acc 1
2016-09-07T20:24:04.490201: step 1742, loss 0.0322906, acc 1
2016-09-07T20:24:05.153661: step 1743, loss 0.155032, acc 0.96
2016-09-07T20:24:05.824852: step 1744, loss 0.0919986, acc 0.98
2016-09-07T20:24:06.497130: step 1745, loss 0.0272608, acc 1
2016-09-07T20:24:06.897266: step 1746, loss 0.00578628, acc 1
2016-09-07T20:24:07.565312: step 1747, loss 0.0825377, acc 0.96
2016-09-07T20:24:08.221199: step 1748, loss 0.244578, acc 0.98
2016-09-07T20:24:08.883218: step 1749, loss 0.282326, acc 0.94
2016-09-07T20:24:09.542343: step 1750, loss 0.0175293, acc 1
2016-09-07T20:24:10.198090: step 1751, loss 0.0514252, acc 0.98
2016-09-07T20:24:10.866760: step 1752, loss 0.0240155, acc 1
2016-09-07T20:24:11.539025: step 1753, loss 0.0525582, acc 0.98
2016-09-07T20:24:12.220525: step 1754, loss 0.0502857, acc 0.96
2016-09-07T20:24:12.884645: step 1755, loss 0.0183747, acc 1
2016-09-07T20:24:13.551286: step 1756, loss 0.0193686, acc 1
2016-09-07T20:24:14.205964: step 1757, loss 0.0404064, acc 1
2016-09-07T20:24:14.859393: step 1758, loss 0.107912, acc 0.98
2016-09-07T20:24:15.507184: step 1759, loss 0.0393643, acc 1
2016-09-07T20:24:16.189569: step 1760, loss 0.0637527, acc 0.96
2016-09-07T20:24:16.853687: step 1761, loss 0.00823415, acc 1
2016-09-07T20:24:17.531904: step 1762, loss 0.0363406, acc 0.96
2016-09-07T20:24:18.187807: step 1763, loss 0.0337189, acc 0.98
2016-09-07T20:24:18.865374: step 1764, loss 0.137707, acc 0.94
2016-09-07T20:24:19.540675: step 1765, loss 0.0801316, acc 0.94
2016-09-07T20:24:20.224991: step 1766, loss 0.0311077, acc 0.98
2016-09-07T20:24:20.898928: step 1767, loss 0.0859816, acc 0.96
2016-09-07T20:24:21.545387: step 1768, loss 0.0533646, acc 0.98
2016-09-07T20:24:22.220958: step 1769, loss 0.0177889, acc 1
2016-09-07T20:24:22.879513: step 1770, loss 0.0834534, acc 0.98
2016-09-07T20:24:23.529750: step 1771, loss 0.0454723, acc 1
2016-09-07T20:24:24.202758: step 1772, loss 0.00945299, acc 1
2016-09-07T20:24:24.897540: step 1773, loss 0.0205051, acc 1
2016-09-07T20:24:25.576929: step 1774, loss 0.0635839, acc 0.98
2016-09-07T20:24:26.257443: step 1775, loss 0.0886396, acc 0.96
2016-09-07T20:24:26.926959: step 1776, loss 0.0237963, acc 1
2016-09-07T20:24:27.604810: step 1777, loss 0.0328038, acc 0.98
2016-09-07T20:24:28.276396: step 1778, loss 0.0704747, acc 0.96
2016-09-07T20:24:28.948029: step 1779, loss 0.0419821, acc 0.98
2016-09-07T20:24:29.605706: step 1780, loss 0.0632431, acc 0.96
2016-09-07T20:24:30.275179: step 1781, loss 0.050739, acc 0.98
2016-09-07T20:24:30.946514: step 1782, loss 0.0402773, acc 1
2016-09-07T20:24:31.620045: step 1783, loss 0.0138106, acc 1
2016-09-07T20:24:32.279919: step 1784, loss 0.1079, acc 0.94
2016-09-07T20:24:32.958358: step 1785, loss 0.0133867, acc 1
2016-09-07T20:24:33.609128: step 1786, loss 0.111037, acc 0.98
2016-09-07T20:24:34.263780: step 1787, loss 0.0862569, acc 0.98
2016-09-07T20:24:34.958884: step 1788, loss 0.0463677, acc 0.96
2016-09-07T20:24:35.632499: step 1789, loss 0.0132554, acc 1
2016-09-07T20:24:36.301518: step 1790, loss 0.0106325, acc 1
2016-09-07T20:24:36.975953: step 1791, loss 0.0148634, acc 1
2016-09-07T20:24:37.631606: step 1792, loss 0.110795, acc 0.94
2016-09-07T20:24:38.304156: step 1793, loss 0.12091, acc 0.92
2016-09-07T20:24:38.963562: step 1794, loss 0.0899059, acc 0.98
2016-09-07T20:24:39.632825: step 1795, loss 0.0358184, acc 1
2016-09-07T20:24:40.300707: step 1796, loss 0.0260328, acc 1
2016-09-07T20:24:40.951166: step 1797, loss 0.0110787, acc 1
2016-09-07T20:24:41.619811: step 1798, loss 0.0368988, acc 0.98
2016-09-07T20:24:42.306738: step 1799, loss 0.0387475, acc 0.96
2016-09-07T20:24:42.977036: step 1800, loss 0.110196, acc 0.98

Evaluation:
2016-09-07T20:24:45.870344: step 1800, loss 1.279, acc 0.753

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-1800

2016-09-07T20:24:47.522205: step 1801, loss 0.0679856, acc 0.96
2016-09-07T20:24:48.183967: step 1802, loss 0.101461, acc 0.94
2016-09-07T20:24:48.873485: step 1803, loss 0.0555422, acc 0.98
2016-09-07T20:24:49.545280: step 1804, loss 0.0217139, acc 1
2016-09-07T20:24:50.223149: step 1805, loss 0.0372026, acc 0.98
2016-09-07T20:24:50.896540: step 1806, loss 0.0603188, acc 0.96
2016-09-07T20:24:51.576144: step 1807, loss 0.0839668, acc 0.96
2016-09-07T20:24:52.246041: step 1808, loss 0.0776128, acc 0.94
2016-09-07T20:24:52.932947: step 1809, loss 0.0840935, acc 0.96
2016-09-07T20:24:53.619774: step 1810, loss 0.0306357, acc 1
2016-09-07T20:24:54.295630: step 1811, loss 0.0414712, acc 1
2016-09-07T20:24:54.959174: step 1812, loss 0.0911805, acc 0.96
2016-09-07T20:24:55.626903: step 1813, loss 0.00952786, acc 1
2016-09-07T20:24:56.303560: step 1814, loss 0.119008, acc 0.94
2016-09-07T20:24:56.967042: step 1815, loss 0.029561, acc 0.98
2016-09-07T20:24:57.649048: step 1816, loss 0.0182711, acc 1
2016-09-07T20:24:58.330984: step 1817, loss 0.0130692, acc 1
2016-09-07T20:24:59.001977: step 1818, loss 0.0392153, acc 0.96
2016-09-07T20:24:59.665652: step 1819, loss 0.18776, acc 0.9
2016-09-07T20:25:00.378707: step 1820, loss 0.0177258, acc 1
2016-09-07T20:25:01.055032: step 1821, loss 0.0302362, acc 0.98
2016-09-07T20:25:01.716452: step 1822, loss 0.0401183, acc 0.96
2016-09-07T20:25:02.376329: step 1823, loss 0.12953, acc 0.96
2016-09-07T20:25:03.045437: step 1824, loss 0.0563181, acc 0.96
2016-09-07T20:25:03.722788: step 1825, loss 0.14789, acc 0.96
2016-09-07T20:25:04.400310: step 1826, loss 0.0369458, acc 1
2016-09-07T20:25:05.079692: step 1827, loss 0.103581, acc 0.96
2016-09-07T20:25:05.769190: step 1828, loss 0.0218942, acc 1
2016-09-07T20:25:06.445448: step 1829, loss 0.149214, acc 0.92
2016-09-07T20:25:07.107697: step 1830, loss 0.0171818, acc 1
2016-09-07T20:25:07.790933: step 1831, loss 0.037368, acc 0.98
2016-09-07T20:25:08.495521: step 1832, loss 0.043101, acc 1
2016-09-07T20:25:09.194083: step 1833, loss 0.120054, acc 0.94
2016-09-07T20:25:09.863408: step 1834, loss 0.0646266, acc 0.98
2016-09-07T20:25:10.535548: step 1835, loss 0.0596646, acc 0.98
2016-09-07T20:25:11.207423: step 1836, loss 0.0104194, acc 1
2016-09-07T20:25:11.885082: step 1837, loss 0.0759815, acc 0.98
2016-09-07T20:25:12.554403: step 1838, loss 0.04247, acc 1
2016-09-07T20:25:13.221448: step 1839, loss 0.193579, acc 0.9
2016-09-07T20:25:13.888809: step 1840, loss 0.0386736, acc 0.98
2016-09-07T20:25:14.548262: step 1841, loss 0.108083, acc 0.98
2016-09-07T20:25:15.211961: step 1842, loss 0.0433011, acc 0.98
2016-09-07T20:25:15.886074: step 1843, loss 0.0683988, acc 0.96
2016-09-07T20:25:16.570854: step 1844, loss 0.0822101, acc 0.96
2016-09-07T20:25:17.252070: step 1845, loss 0.012726, acc 1
2016-09-07T20:25:17.919519: step 1846, loss 0.0794675, acc 0.96
2016-09-07T20:25:18.602560: step 1847, loss 0.137743, acc 0.92
2016-09-07T20:25:19.264495: step 1848, loss 0.0339031, acc 0.98
2016-09-07T20:25:19.948816: step 1849, loss 0.0814775, acc 0.98
2016-09-07T20:25:20.626403: step 1850, loss 0.0270449, acc 1
2016-09-07T20:25:21.336648: step 1851, loss 0.0335751, acc 0.98
2016-09-07T20:25:22.017592: step 1852, loss 0.0677867, acc 0.98
2016-09-07T20:25:22.669551: step 1853, loss 0.129192, acc 0.96
2016-09-07T20:25:23.340193: step 1854, loss 0.084471, acc 0.96
2016-09-07T20:25:24.004069: step 1855, loss 0.0606261, acc 0.96
2016-09-07T20:25:24.671646: step 1856, loss 0.0630047, acc 0.96
2016-09-07T20:25:25.355489: step 1857, loss 0.232762, acc 0.9
2016-09-07T20:25:26.017946: step 1858, loss 0.0171294, acc 1
2016-09-07T20:25:26.677117: step 1859, loss 0.031536, acc 1
2016-09-07T20:25:27.349127: step 1860, loss 0.0934693, acc 0.98
2016-09-07T20:25:28.013416: step 1861, loss 0.104376, acc 0.96
2016-09-07T20:25:28.665291: step 1862, loss 0.0437719, acc 0.98
2016-09-07T20:25:29.338906: step 1863, loss 0.0455192, acc 0.98
2016-09-07T20:25:30.022746: step 1864, loss 0.117047, acc 0.96
2016-09-07T20:25:30.705382: step 1865, loss 0.0748201, acc 0.96
2016-09-07T20:25:31.384053: step 1866, loss 0.0657017, acc 0.96
2016-09-07T20:25:32.052228: step 1867, loss 0.090384, acc 0.96
2016-09-07T20:25:32.729562: step 1868, loss 0.107473, acc 0.96
2016-09-07T20:25:33.392650: step 1869, loss 0.0397417, acc 0.98
2016-09-07T20:25:34.064841: step 1870, loss 0.0443832, acc 0.98
2016-09-07T20:25:34.745786: step 1871, loss 0.0808779, acc 0.98
2016-09-07T20:25:35.413218: step 1872, loss 0.0521732, acc 0.98
2016-09-07T20:25:36.066813: step 1873, loss 0.0975218, acc 0.96
2016-09-07T20:25:36.743206: step 1874, loss 0.0251544, acc 1
2016-09-07T20:25:37.427217: step 1875, loss 0.0493082, acc 0.98
2016-09-07T20:25:38.091313: step 1876, loss 0.0267226, acc 1
2016-09-07T20:25:38.762329: step 1877, loss 0.0539629, acc 0.98
2016-09-07T20:25:39.432605: step 1878, loss 0.071826, acc 0.96
2016-09-07T20:25:40.103543: step 1879, loss 0.166678, acc 0.94
2016-09-07T20:25:40.773009: step 1880, loss 0.0262155, acc 0.98
2016-09-07T20:25:41.441280: step 1881, loss 0.0341527, acc 1
2016-09-07T20:25:42.109964: step 1882, loss 0.0660236, acc 0.96
2016-09-07T20:25:42.781644: step 1883, loss 0.108987, acc 0.94
2016-09-07T20:25:43.469389: step 1884, loss 0.0659144, acc 0.98
2016-09-07T20:25:44.128049: step 1885, loss 0.0662482, acc 0.96
2016-09-07T20:25:44.800258: step 1886, loss 0.0562578, acc 0.98
2016-09-07T20:25:45.469475: step 1887, loss 0.0948507, acc 0.98
2016-09-07T20:25:46.130138: step 1888, loss 0.10346, acc 0.94
2016-09-07T20:25:46.798447: step 1889, loss 0.0333968, acc 1
2016-09-07T20:25:47.493215: step 1890, loss 0.0307238, acc 0.98
2016-09-07T20:25:48.185871: step 1891, loss 0.0737617, acc 0.96
2016-09-07T20:25:48.854700: step 1892, loss 0.0465474, acc 0.98
2016-09-07T20:25:49.527494: step 1893, loss 0.0195781, acc 1
2016-09-07T20:25:50.206469: step 1894, loss 0.0394135, acc 0.96
2016-09-07T20:25:50.872205: step 1895, loss 0.0616972, acc 0.94
2016-09-07T20:25:51.547344: step 1896, loss 0.15127, acc 0.96
2016-09-07T20:25:52.222231: step 1897, loss 0.105108, acc 0.98
2016-09-07T20:25:52.883022: step 1898, loss 0.107788, acc 0.98
2016-09-07T20:25:53.552721: step 1899, loss 0.168877, acc 0.94
2016-09-07T20:25:54.225031: step 1900, loss 0.13038, acc 0.94

Evaluation:
2016-09-07T20:25:57.076527: step 1900, loss 1.17912, acc 0.752

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-1900

2016-09-07T20:25:58.692767: step 1901, loss 0.0421562, acc 1
2016-09-07T20:25:59.346957: step 1902, loss 0.0322753, acc 0.98
2016-09-07T20:26:00.006618: step 1903, loss 0.0562006, acc 0.98
2016-09-07T20:26:00.708075: step 1904, loss 0.116475, acc 0.98
2016-09-07T20:26:01.372934: step 1905, loss 0.0328779, acc 0.98
2016-09-07T20:26:02.040548: step 1906, loss 0.0589636, acc 0.98
2016-09-07T20:26:02.721907: step 1907, loss 0.0780643, acc 0.98
2016-09-07T20:26:03.394675: step 1908, loss 0.0385011, acc 0.98
2016-09-07T20:26:04.057844: step 1909, loss 0.037285, acc 0.98
2016-09-07T20:26:04.727783: step 1910, loss 0.0538474, acc 0.96
2016-09-07T20:26:05.402395: step 1911, loss 0.0387318, acc 0.98
2016-09-07T20:26:06.086161: step 1912, loss 0.0669195, acc 0.96
2016-09-07T20:26:06.789110: step 1913, loss 0.01091, acc 1
2016-09-07T20:26:07.454529: step 1914, loss 0.0433018, acc 0.96
2016-09-07T20:26:08.121241: step 1915, loss 0.0991243, acc 0.96
2016-09-07T20:26:08.801672: step 1916, loss 0.144266, acc 0.94
2016-09-07T20:26:09.461661: step 1917, loss 0.0610949, acc 0.96
2016-09-07T20:26:10.116726: step 1918, loss 0.0290757, acc 0.98
2016-09-07T20:26:10.790600: step 1919, loss 0.0236463, acc 0.98
2016-09-07T20:26:11.451791: step 1920, loss 0.015266, acc 1
2016-09-07T20:26:12.121764: step 1921, loss 0.033469, acc 1
2016-09-07T20:26:12.798528: step 1922, loss 0.0837062, acc 0.98
2016-09-07T20:26:13.442035: step 1923, loss 0.0445665, acc 0.96
2016-09-07T20:26:14.121205: step 1924, loss 0.0376706, acc 0.98
2016-09-07T20:26:14.801422: step 1925, loss 0.097178, acc 0.96
2016-09-07T20:26:15.496874: step 1926, loss 0.0635534, acc 0.98
2016-09-07T20:26:16.163878: step 1927, loss 0.200972, acc 0.9
2016-09-07T20:26:16.844965: step 1928, loss 0.144435, acc 0.96
2016-09-07T20:26:17.520365: step 1929, loss 0.0313519, acc 1
2016-09-07T20:26:18.191828: step 1930, loss 0.0718323, acc 0.96
2016-09-07T20:26:18.864807: step 1931, loss 0.0218537, acc 1
2016-09-07T20:26:19.544460: step 1932, loss 0.0439609, acc 0.98
2016-09-07T20:26:20.215720: step 1933, loss 0.080569, acc 0.94
2016-09-07T20:26:20.884528: step 1934, loss 0.0261205, acc 1
2016-09-07T20:26:21.547484: step 1935, loss 0.0861137, acc 0.96
2016-09-07T20:26:22.216980: step 1936, loss 0.0681932, acc 0.96
2016-09-07T20:26:22.882221: step 1937, loss 0.0310733, acc 1
2016-09-07T20:26:23.549038: step 1938, loss 0.0891163, acc 0.98
2016-09-07T20:26:24.216695: step 1939, loss 0.0950288, acc 0.98
2016-09-07T20:26:24.576520: step 1940, loss 0.229281, acc 0.916667
2016-09-07T20:26:25.238599: step 1941, loss 0.0432404, acc 0.98
2016-09-07T20:26:25.912889: step 1942, loss 0.0664775, acc 0.96
2016-09-07T20:26:26.569531: step 1943, loss 0.0335083, acc 1
2016-09-07T20:26:27.231174: step 1944, loss 0.0811468, acc 0.94
2016-09-07T20:26:27.896534: step 1945, loss 0.105857, acc 0.98
2016-09-07T20:26:28.582836: step 1946, loss 0.0533286, acc 0.98
2016-09-07T20:26:29.275195: step 1947, loss 0.0551439, acc 0.98
2016-09-07T20:26:29.958896: step 1948, loss 0.0229884, acc 1
2016-09-07T20:26:30.637436: step 1949, loss 0.0507969, acc 0.98
2016-09-07T20:26:31.297834: step 1950, loss 0.0385419, acc 0.98
2016-09-07T20:26:31.962708: step 1951, loss 0.0418442, acc 1
2016-09-07T20:26:32.620748: step 1952, loss 0.065838, acc 0.98
2016-09-07T20:26:33.292286: step 1953, loss 0.0371826, acc 1
2016-09-07T20:26:33.968643: step 1954, loss 0.0661533, acc 0.96
2016-09-07T20:26:34.636645: step 1955, loss 0.0837304, acc 0.96
2016-09-07T20:26:35.299842: step 1956, loss 0.0233137, acc 0.98
2016-09-07T20:26:35.980160: step 1957, loss 0.155102, acc 0.94
2016-09-07T20:26:36.641484: step 1958, loss 0.0435694, acc 0.96
2016-09-07T20:26:37.305251: step 1959, loss 0.102785, acc 0.96
2016-09-07T20:26:37.978326: step 1960, loss 0.0484993, acc 0.98
2016-09-07T20:26:38.646422: step 1961, loss 0.0202587, acc 1
2016-09-07T20:26:39.322038: step 1962, loss 0.0530676, acc 0.98
2016-09-07T20:26:39.997831: step 1963, loss 0.0744992, acc 0.98
2016-09-07T20:26:40.670231: step 1964, loss 0.0244082, acc 0.98
2016-09-07T20:26:41.337466: step 1965, loss 0.0276649, acc 1
2016-09-07T20:26:42.017464: step 1966, loss 0.101253, acc 0.98
2016-09-07T20:26:42.669083: step 1967, loss 0.00983646, acc 1
2016-09-07T20:26:43.334282: step 1968, loss 0.0604377, acc 0.96
2016-09-07T20:26:44.001105: step 1969, loss 0.0508014, acc 1
2016-09-07T20:26:44.678545: step 1970, loss 0.0995337, acc 0.96
2016-09-07T20:26:45.340133: step 1971, loss 0.00866654, acc 1
2016-09-07T20:26:46.017110: step 1972, loss 0.0638154, acc 0.98
2016-09-07T20:26:46.691990: step 1973, loss 0.064003, acc 0.96
2016-09-07T20:26:47.361096: step 1974, loss 0.0125127, acc 1
2016-09-07T20:26:48.022098: step 1975, loss 0.113405, acc 0.96
2016-09-07T20:26:48.696571: step 1976, loss 0.14849, acc 0.96
2016-09-07T20:26:49.358609: step 1977, loss 0.100207, acc 0.94
2016-09-07T20:26:50.047926: step 1978, loss 0.0844346, acc 0.94
2016-09-07T20:26:50.717117: step 1979, loss 0.0498508, acc 0.98
2016-09-07T20:26:51.397998: step 1980, loss 0.0760116, acc 0.98
2016-09-07T20:26:52.063069: step 1981, loss 0.126658, acc 0.96
2016-09-07T20:26:52.740905: step 1982, loss 0.0758174, acc 0.96
2016-09-07T20:26:53.416330: step 1983, loss 0.0156786, acc 1
2016-09-07T20:26:54.078199: step 1984, loss 0.0278421, acc 0.98
2016-09-07T20:26:54.737902: step 1985, loss 0.0247306, acc 1
2016-09-07T20:26:55.402910: step 1986, loss 0.1509, acc 0.92
2016-09-07T20:26:56.084485: step 1987, loss 0.0399787, acc 0.98
2016-09-07T20:26:56.747786: step 1988, loss 0.0306414, acc 1
2016-09-07T20:26:57.425984: step 1989, loss 0.138719, acc 0.96
2016-09-07T20:26:58.092522: step 1990, loss 0.0576869, acc 0.98
2016-09-07T20:26:58.772534: step 1991, loss 0.0554494, acc 0.96
2016-09-07T20:26:59.456571: step 1992, loss 0.0320382, acc 0.98
2016-09-07T20:27:00.121626: step 1993, loss 0.066768, acc 0.94
2016-09-07T20:27:00.825419: step 1994, loss 0.077191, acc 0.96
2016-09-07T20:27:01.508855: step 1995, loss 0.105284, acc 0.96
2016-09-07T20:27:02.176423: step 1996, loss 0.0869984, acc 0.98
2016-09-07T20:27:02.864686: step 1997, loss 0.130769, acc 0.92
2016-09-07T20:27:03.544614: step 1998, loss 0.0230887, acc 1
2016-09-07T20:27:04.215233: step 1999, loss 0.0895395, acc 0.98
2016-09-07T20:27:04.875409: step 2000, loss 0.0869472, acc 0.96

Evaluation:
2016-09-07T20:27:07.758564: step 2000, loss 1.14724, acc 0.747

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-2000

2016-09-07T20:27:09.399214: step 2001, loss 0.0620062, acc 0.96
2016-09-07T20:27:10.087602: step 2002, loss 0.0568803, acc 0.98
2016-09-07T20:27:10.743385: step 2003, loss 0.090457, acc 0.98
2016-09-07T20:27:11.390515: step 2004, loss 0.0572369, acc 1
2016-09-07T20:27:12.061365: step 2005, loss 0.0424633, acc 1
2016-09-07T20:27:12.730288: step 2006, loss 0.0601935, acc 1
2016-09-07T20:27:13.401952: step 2007, loss 0.0541281, acc 0.98
2016-09-07T20:27:14.078036: step 2008, loss 0.0530613, acc 0.98
2016-09-07T20:27:14.741680: step 2009, loss 0.0405241, acc 0.98
2016-09-07T20:27:15.414109: step 2010, loss 0.060581, acc 0.98
2016-09-07T20:27:16.083940: step 2011, loss 0.0509753, acc 1
2016-09-07T20:27:16.764913: step 2012, loss 0.0420029, acc 1
2016-09-07T20:27:17.449741: step 2013, loss 0.00818256, acc 1
2016-09-07T20:27:18.116205: step 2014, loss 0.037628, acc 0.98
2016-09-07T20:27:18.796795: step 2015, loss 0.0752382, acc 0.98
2016-09-07T20:27:19.479474: step 2016, loss 0.0702356, acc 0.96
2016-09-07T20:27:20.158313: step 2017, loss 0.0381883, acc 0.98
2016-09-07T20:27:20.848837: step 2018, loss 0.0360524, acc 1
2016-09-07T20:27:21.522418: step 2019, loss 0.0132265, acc 1
2016-09-07T20:27:22.177019: step 2020, loss 0.115132, acc 0.92
2016-09-07T20:27:22.844126: step 2021, loss 0.0853559, acc 0.98
2016-09-07T20:27:23.524653: step 2022, loss 0.028169, acc 1
2016-09-07T20:27:24.193693: step 2023, loss 0.0419908, acc 0.98
2016-09-07T20:27:24.868752: step 2024, loss 0.0179353, acc 1
2016-09-07T20:27:25.548454: step 2025, loss 0.00933772, acc 1
2016-09-07T20:27:26.224016: step 2026, loss 0.0822309, acc 0.96
2016-09-07T20:27:26.892757: step 2027, loss 0.0419615, acc 1
2016-09-07T20:27:27.560016: step 2028, loss 0.0959013, acc 0.96
2016-09-07T20:27:28.244674: step 2029, loss 0.0527153, acc 0.98
2016-09-07T20:27:28.917655: step 2030, loss 0.0694427, acc 0.96
2016-09-07T20:27:29.587717: step 2031, loss 0.314976, acc 0.9
2016-09-07T20:27:30.262564: step 2032, loss 0.0248012, acc 1
2016-09-07T20:27:30.933558: step 2033, loss 0.0320342, acc 1
2016-09-07T20:27:31.601301: step 2034, loss 0.0902247, acc 0.98
2016-09-07T20:27:32.262749: step 2035, loss 0.120443, acc 0.94
2016-09-07T20:27:32.922067: step 2036, loss 0.0343895, acc 0.98
2016-09-07T20:27:33.590015: step 2037, loss 0.0302673, acc 0.98
2016-09-07T20:27:34.259427: step 2038, loss 0.0462669, acc 0.98
2016-09-07T20:27:34.917394: step 2039, loss 0.148592, acc 0.92
2016-09-07T20:27:35.610363: step 2040, loss 0.125986, acc 0.98
2016-09-07T20:27:36.275016: step 2041, loss 0.0425456, acc 0.98
2016-09-07T20:27:36.956708: step 2042, loss 0.0350702, acc 1
2016-09-07T20:27:37.623625: step 2043, loss 0.0482821, acc 1
2016-09-07T20:27:38.281507: step 2044, loss 0.156697, acc 0.9
2016-09-07T20:27:38.939217: step 2045, loss 0.0655798, acc 0.96
2016-09-07T20:27:39.617313: step 2046, loss 0.0988938, acc 0.96
2016-09-07T20:27:40.299737: step 2047, loss 0.0784923, acc 0.96
2016-09-07T20:27:40.968027: step 2048, loss 0.0307122, acc 0.98
2016-09-07T20:27:41.649801: step 2049, loss 0.0887522, acc 0.96
2016-09-07T20:27:42.332807: step 2050, loss 0.0253917, acc 0.98
2016-09-07T20:27:42.994319: step 2051, loss 0.078005, acc 0.96
2016-09-07T20:27:43.688796: step 2052, loss 0.0546479, acc 0.98
2016-09-07T20:27:44.369023: step 2053, loss 0.0424592, acc 0.98
2016-09-07T20:27:45.048476: step 2054, loss 0.0436167, acc 1
2016-09-07T20:27:45.710968: step 2055, loss 0.0283781, acc 1
2016-09-07T20:27:46.368102: step 2056, loss 0.0649366, acc 0.98
2016-09-07T20:27:47.042169: step 2057, loss 0.0708008, acc 0.98
2016-09-07T20:27:47.708593: step 2058, loss 0.0624428, acc 0.96
2016-09-07T20:27:48.396662: step 2059, loss 0.0248004, acc 1
2016-09-07T20:27:49.071813: step 2060, loss 0.0949633, acc 0.96
2016-09-07T20:27:49.723792: step 2061, loss 0.0179416, acc 1
2016-09-07T20:27:50.385847: step 2062, loss 0.134382, acc 0.96
2016-09-07T20:27:51.048996: step 2063, loss 0.0248542, acc 1
2016-09-07T20:27:51.730258: step 2064, loss 0.0327345, acc 1
2016-09-07T20:27:52.422689: step 2065, loss 0.0317279, acc 1
2016-09-07T20:27:53.085850: step 2066, loss 0.0354648, acc 0.98
2016-09-07T20:27:53.755960: step 2067, loss 0.064547, acc 0.94
2016-09-07T20:27:54.437080: step 2068, loss 0.0386207, acc 0.98
2016-09-07T20:27:55.131071: step 2069, loss 0.0762954, acc 0.96
2016-09-07T20:27:55.805350: step 2070, loss 0.0758178, acc 0.96
2016-09-07T20:27:56.455394: step 2071, loss 0.127994, acc 0.92
2016-09-07T20:27:57.110327: step 2072, loss 0.0670684, acc 0.96
2016-09-07T20:27:57.783394: step 2073, loss 0.0221763, acc 1
2016-09-07T20:27:58.449407: step 2074, loss 0.0290588, acc 0.98
2016-09-07T20:27:59.115337: step 2075, loss 0.0464376, acc 0.98
2016-09-07T20:27:59.769262: step 2076, loss 0.040922, acc 0.98
2016-09-07T20:28:00.482088: step 2077, loss 0.118007, acc 0.96
2016-09-07T20:28:01.142235: step 2078, loss 0.00797553, acc 1
2016-09-07T20:28:01.820170: step 2079, loss 0.0458342, acc 0.98
2016-09-07T20:28:02.486557: step 2080, loss 0.0236174, acc 1
2016-09-07T20:28:03.155001: step 2081, loss 0.0153041, acc 1
2016-09-07T20:28:03.827292: step 2082, loss 0.0302914, acc 1
2016-09-07T20:28:04.484827: step 2083, loss 0.0422928, acc 1
2016-09-07T20:28:05.152552: step 2084, loss 0.048661, acc 1
2016-09-07T20:28:05.817304: step 2085, loss 0.074718, acc 0.96
2016-09-07T20:28:06.475980: step 2086, loss 0.0884999, acc 0.98
2016-09-07T20:28:07.138726: step 2087, loss 0.0553279, acc 0.98
2016-09-07T20:28:07.814138: step 2088, loss 0.0876355, acc 0.96
2016-09-07T20:28:08.473279: step 2089, loss 0.0472163, acc 0.98
2016-09-07T20:28:09.173756: step 2090, loss 0.0166234, acc 1
2016-09-07T20:28:09.850922: step 2091, loss 0.0959776, acc 0.98
2016-09-07T20:28:10.518792: step 2092, loss 0.0611469, acc 0.98
2016-09-07T20:28:11.194525: step 2093, loss 0.0815973, acc 0.94
2016-09-07T20:28:11.868971: step 2094, loss 0.117364, acc 0.96
2016-09-07T20:28:12.541035: step 2095, loss 0.0678311, acc 0.94
2016-09-07T20:28:13.201161: step 2096, loss 0.0260782, acc 1
2016-09-07T20:28:13.861164: step 2097, loss 0.00856067, acc 1
2016-09-07T20:28:14.525802: step 2098, loss 0.111692, acc 0.96
2016-09-07T20:28:15.203649: step 2099, loss 0.0979335, acc 0.94
2016-09-07T20:28:15.874440: step 2100, loss 0.0276579, acc 1

Evaluation:
2016-09-07T20:28:18.763499: step 2100, loss 1.33776, acc 0.746

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-2100

2016-09-07T20:28:20.465238: step 2101, loss 0.0609372, acc 0.96
2016-09-07T20:28:21.159288: step 2102, loss 0.0218729, acc 1
2016-09-07T20:28:21.833792: step 2103, loss 0.00511586, acc 1
2016-09-07T20:28:22.515215: step 2104, loss 0.060709, acc 0.96
2016-09-07T20:28:23.190542: step 2105, loss 0.025569, acc 1
2016-09-07T20:28:23.848412: step 2106, loss 0.0645073, acc 0.96
2016-09-07T20:28:24.531429: step 2107, loss 0.0274775, acc 0.98
2016-09-07T20:28:25.201783: step 2108, loss 0.0158917, acc 1
2016-09-07T20:28:25.875996: step 2109, loss 0.0929805, acc 0.96
2016-09-07T20:28:26.551841: step 2110, loss 0.104173, acc 0.96
2016-09-07T20:28:27.231952: step 2111, loss 0.0761987, acc 0.94
2016-09-07T20:28:27.906075: step 2112, loss 0.0766074, acc 0.96
2016-09-07T20:28:28.557513: step 2113, loss 0.0417062, acc 0.98
2016-09-07T20:28:29.223514: step 2114, loss 0.0606464, acc 0.94
2016-09-07T20:28:29.904769: step 2115, loss 0.120886, acc 0.94
2016-09-07T20:28:30.589142: step 2116, loss 0.0454728, acc 0.98
2016-09-07T20:28:31.249884: step 2117, loss 0.145913, acc 0.94
2016-09-07T20:28:31.897196: step 2118, loss 0.0117266, acc 1
2016-09-07T20:28:32.556854: step 2119, loss 0.0441428, acc 0.98
2016-09-07T20:28:33.205384: step 2120, loss 0.0349808, acc 1
2016-09-07T20:28:33.876704: step 2121, loss 0.0798557, acc 0.94
2016-09-07T20:28:34.555958: step 2122, loss 0.029857, acc 1
2016-09-07T20:28:35.210842: step 2123, loss 0.130358, acc 0.92
2016-09-07T20:28:35.869123: step 2124, loss 0.0244029, acc 1
2016-09-07T20:28:36.552203: step 2125, loss 0.0228226, acc 0.98
2016-09-07T20:28:37.231504: step 2126, loss 0.0288494, acc 1
2016-09-07T20:28:37.903302: step 2127, loss 0.0799873, acc 0.94
2016-09-07T20:28:38.569122: step 2128, loss 0.0299363, acc 0.98
2016-09-07T20:28:39.229065: step 2129, loss 0.0386384, acc 1
2016-09-07T20:28:39.916422: step 2130, loss 0.0401326, acc 1
2016-09-07T20:28:40.596350: step 2131, loss 0.0433005, acc 0.98
2016-09-07T20:28:41.284447: step 2132, loss 0.0508169, acc 0.98
2016-09-07T20:28:41.962287: step 2133, loss 0.0161314, acc 1
2016-09-07T20:28:42.324075: step 2134, loss 0.012334, acc 1
2016-09-07T20:28:42.991806: step 2135, loss 0.0578496, acc 0.96
2016-09-07T20:28:43.674118: step 2136, loss 0.06422, acc 0.98
2016-09-07T20:28:44.344723: step 2137, loss 0.0254344, acc 0.98
2016-09-07T20:28:45.014481: step 2138, loss 0.0685803, acc 0.98
2016-09-07T20:28:45.693696: step 2139, loss 0.0495565, acc 0.98
2016-09-07T20:28:46.346076: step 2140, loss 0.0492583, acc 0.98
2016-09-07T20:28:47.035652: step 2141, loss 0.0543234, acc 0.96
2016-09-07T20:28:47.721018: step 2142, loss 0.0197339, acc 1
2016-09-07T20:28:48.395219: step 2143, loss 0.0315154, acc 1
2016-09-07T20:28:49.051055: step 2144, loss 0.0777015, acc 0.96
2016-09-07T20:28:49.710971: step 2145, loss 0.0551588, acc 0.98
2016-09-07T20:28:50.387872: step 2146, loss 0.0458916, acc 0.96
2016-09-07T20:28:51.064191: step 2147, loss 0.0290934, acc 0.98
2016-09-07T20:28:51.760056: step 2148, loss 0.103372, acc 0.96
2016-09-07T20:28:52.444533: step 2149, loss 0.0826075, acc 0.98
2016-09-07T20:28:53.126967: step 2150, loss 0.120934, acc 0.96
2016-09-07T20:28:53.804451: step 2151, loss 0.0570188, acc 0.98
2016-09-07T20:28:54.468846: step 2152, loss 0.013502, acc 1
2016-09-07T20:28:55.135695: step 2153, loss 0.0637805, acc 0.96
2016-09-07T20:28:55.815352: step 2154, loss 0.0392333, acc 0.96
2016-09-07T20:28:56.483958: step 2155, loss 0.0570536, acc 0.96
2016-09-07T20:28:57.154511: step 2156, loss 0.0394604, acc 0.98
2016-09-07T20:28:57.821663: step 2157, loss 0.101418, acc 0.94
2016-09-07T20:28:58.493557: step 2158, loss 0.111627, acc 0.94
2016-09-07T20:28:59.165716: step 2159, loss 0.0494589, acc 0.98
2016-09-07T20:28:59.825194: step 2160, loss 0.0544947, acc 0.96
2016-09-07T20:29:00.535537: step 2161, loss 0.0259871, acc 0.98
2016-09-07T20:29:01.199848: step 2162, loss 0.0189291, acc 1
2016-09-07T20:29:01.882819: step 2163, loss 0.055881, acc 0.98
2016-09-07T20:29:02.554913: step 2164, loss 0.0404946, acc 1
2016-09-07T20:29:03.220239: step 2165, loss 0.143651, acc 0.92
2016-09-07T20:29:03.880929: step 2166, loss 0.114737, acc 0.94
2016-09-07T20:29:04.583622: step 2167, loss 0.0238051, acc 1
2016-09-07T20:29:05.267348: step 2168, loss 0.0591458, acc 0.98
2016-09-07T20:29:05.941149: step 2169, loss 0.038778, acc 0.98
2016-09-07T20:29:06.610432: step 2170, loss 0.0167259, acc 1
2016-09-07T20:29:07.273373: step 2171, loss 0.0316135, acc 0.98
2016-09-07T20:29:07.938103: step 2172, loss 0.0873245, acc 0.96
2016-09-07T20:29:08.597625: step 2173, loss 0.0473788, acc 0.98
2016-09-07T20:29:09.266944: step 2174, loss 0.0228805, acc 1
2016-09-07T20:29:09.943975: step 2175, loss 0.128195, acc 0.98
2016-09-07T20:29:10.609619: step 2176, loss 0.0466458, acc 0.98
2016-09-07T20:29:11.286855: step 2177, loss 0.0344984, acc 1
2016-09-07T20:29:11.951733: step 2178, loss 0.0633134, acc 0.98
2016-09-07T20:29:12.616817: step 2179, loss 0.081918, acc 0.98
2016-09-07T20:29:13.275686: step 2180, loss 0.0563253, acc 0.96
2016-09-07T20:29:13.944020: step 2181, loss 0.0476996, acc 1
2016-09-07T20:29:14.605908: step 2182, loss 0.101326, acc 0.96
2016-09-07T20:29:15.280536: step 2183, loss 0.0409725, acc 0.96
2016-09-07T20:29:15.954440: step 2184, loss 0.134329, acc 0.98
2016-09-07T20:29:16.632044: step 2185, loss 0.15431, acc 0.96
2016-09-07T20:29:17.309041: step 2186, loss 0.0288837, acc 0.98
2016-09-07T20:29:17.998066: step 2187, loss 0.0108544, acc 1
2016-09-07T20:29:18.677127: step 2188, loss 0.0521422, acc 0.98
2016-09-07T20:29:19.355339: step 2189, loss 0.0982056, acc 0.96
2016-09-07T20:29:20.018761: step 2190, loss 0.0822351, acc 1
2016-09-07T20:29:20.683576: step 2191, loss 0.0370242, acc 0.98
2016-09-07T20:29:21.350348: step 2192, loss 0.0713061, acc 0.96
2016-09-07T20:29:22.027225: step 2193, loss 0.0265785, acc 1
2016-09-07T20:29:22.706414: step 2194, loss 0.0394338, acc 1
2016-09-07T20:29:23.382639: step 2195, loss 0.0835778, acc 0.96
2016-09-07T20:29:24.060654: step 2196, loss 0.0311395, acc 0.98
2016-09-07T20:29:24.736560: step 2197, loss 0.145352, acc 0.96
2016-09-07T20:29:25.429828: step 2198, loss 0.133406, acc 0.96
2016-09-07T20:29:26.087593: step 2199, loss 0.0152243, acc 1
2016-09-07T20:29:26.749687: step 2200, loss 0.0910875, acc 0.98

Evaluation:
2016-09-07T20:29:29.610120: step 2200, loss 1.26951, acc 0.746

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-2200

2016-09-07T20:29:31.235665: step 2201, loss 0.0498826, acc 0.98
2016-09-07T20:29:31.913868: step 2202, loss 0.0529578, acc 0.98
2016-09-07T20:29:32.587133: step 2203, loss 0.0252109, acc 1
2016-09-07T20:29:33.256843: step 2204, loss 0.0725904, acc 0.98
2016-09-07T20:29:33.932786: step 2205, loss 0.0213125, acc 1
2016-09-07T20:29:34.615141: step 2206, loss 0.00689802, acc 1
2016-09-07T20:29:35.281270: step 2207, loss 0.0241444, acc 1
2016-09-07T20:29:35.967853: step 2208, loss 0.0242825, acc 1
2016-09-07T20:29:36.642802: step 2209, loss 0.0858783, acc 0.96
2016-09-07T20:29:37.312436: step 2210, loss 0.0525499, acc 0.96
2016-09-07T20:29:37.977475: step 2211, loss 0.0451848, acc 0.98
2016-09-07T20:29:38.643806: step 2212, loss 0.0484635, acc 0.96
2016-09-07T20:29:39.306212: step 2213, loss 0.0557332, acc 0.98
2016-09-07T20:29:39.990417: step 2214, loss 0.0309837, acc 0.98
2016-09-07T20:29:40.665786: step 2215, loss 0.0579325, acc 0.96
2016-09-07T20:29:41.334407: step 2216, loss 0.0761912, acc 0.96
2016-09-07T20:29:42.008471: step 2217, loss 0.0212904, acc 0.98
2016-09-07T20:29:42.675296: step 2218, loss 0.0402652, acc 0.98
2016-09-07T20:29:43.355538: step 2219, loss 0.00774287, acc 1
2016-09-07T20:29:44.028626: step 2220, loss 0.110087, acc 0.96
2016-09-07T20:29:44.725506: step 2221, loss 0.0553195, acc 0.96
2016-09-07T20:29:45.390599: step 2222, loss 0.0190987, acc 1
2016-09-07T20:29:46.066434: step 2223, loss 0.0903147, acc 0.94
2016-09-07T20:29:46.738783: step 2224, loss 0.0575223, acc 0.98
2016-09-07T20:29:47.426928: step 2225, loss 0.0182459, acc 1
2016-09-07T20:29:48.100408: step 2226, loss 0.0281135, acc 0.98
2016-09-07T20:29:48.759137: step 2227, loss 0.0431971, acc 0.98
2016-09-07T20:29:49.416492: step 2228, loss 0.0357569, acc 1
2016-09-07T20:29:50.088348: step 2229, loss 0.0793715, acc 0.98
2016-09-07T20:29:50.810707: step 2230, loss 0.0585353, acc 0.98
2016-09-07T20:29:51.487455: step 2231, loss 0.0595732, acc 0.98
2016-09-07T20:29:52.153354: step 2232, loss 0.0592992, acc 0.94
2016-09-07T20:29:52.818543: step 2233, loss 0.0190096, acc 1
2016-09-07T20:29:53.478340: step 2234, loss 0.0278049, acc 1
2016-09-07T20:29:54.154814: step 2235, loss 0.0390081, acc 0.98
2016-09-07T20:29:54.805883: step 2236, loss 0.0214707, acc 1
2016-09-07T20:29:55.480898: step 2237, loss 0.0691862, acc 0.96
2016-09-07T20:29:56.160860: step 2238, loss 0.0640915, acc 0.96
2016-09-07T20:29:56.835162: step 2239, loss 0.0571003, acc 0.98
2016-09-07T20:29:57.500802: step 2240, loss 0.0584888, acc 0.98
2016-09-07T20:29:58.169518: step 2241, loss 0.0415935, acc 0.98
2016-09-07T20:29:58.832497: step 2242, loss 0.0478214, acc 0.98
2016-09-07T20:29:59.505874: step 2243, loss 0.0766378, acc 0.96
2016-09-07T20:30:00.194854: step 2244, loss 0.175695, acc 0.94
2016-09-07T20:30:00.923898: step 2245, loss 0.0633556, acc 0.98
2016-09-07T20:30:01.614886: step 2246, loss 0.0218119, acc 0.98
2016-09-07T20:30:02.283666: step 2247, loss 0.0723892, acc 0.98
2016-09-07T20:30:02.957410: step 2248, loss 0.0267377, acc 1
2016-09-07T20:30:03.610813: step 2249, loss 0.0287298, acc 1
2016-09-07T20:30:04.276544: step 2250, loss 0.0568826, acc 0.96
2016-09-07T20:30:04.952930: step 2251, loss 0.0137511, acc 1
2016-09-07T20:30:05.630008: step 2252, loss 0.107339, acc 0.96
2016-09-07T20:30:06.326190: step 2253, loss 0.0353378, acc 0.98
2016-09-07T20:30:07.010753: step 2254, loss 0.0727817, acc 0.96
2016-09-07T20:30:07.680880: step 2255, loss 0.093876, acc 0.94
2016-09-07T20:30:08.347338: step 2256, loss 0.0277754, acc 1
2016-09-07T20:30:09.031226: step 2257, loss 0.0258096, acc 1
2016-09-07T20:30:09.696029: step 2258, loss 0.0111899, acc 1
2016-09-07T20:30:10.362590: step 2259, loss 0.0587071, acc 0.96
2016-09-07T20:30:11.042365: step 2260, loss 0.0439547, acc 0.98
2016-09-07T20:30:11.725414: step 2261, loss 0.0471901, acc 0.98
2016-09-07T20:30:12.394899: step 2262, loss 0.025282, acc 1
2016-09-07T20:30:13.064488: step 2263, loss 0.0520123, acc 0.98
2016-09-07T20:30:13.739080: step 2264, loss 0.0515088, acc 0.98
2016-09-07T20:30:14.409470: step 2265, loss 0.0143948, acc 1
2016-09-07T20:30:15.073501: step 2266, loss 0.0606174, acc 0.96
2016-09-07T20:30:15.752566: step 2267, loss 0.0734537, acc 0.98
2016-09-07T20:30:16.414118: step 2268, loss 0.0756364, acc 0.94
2016-09-07T20:30:17.094433: step 2269, loss 0.0137287, acc 1
2016-09-07T20:30:17.751825: step 2270, loss 0.0282711, acc 0.98
2016-09-07T20:30:18.426631: step 2271, loss 0.09413, acc 0.96
2016-09-07T20:30:19.098583: step 2272, loss 0.025228, acc 1
2016-09-07T20:30:19.767035: step 2273, loss 0.018005, acc 1
2016-09-07T20:30:20.429157: step 2274, loss 0.0393694, acc 0.98
2016-09-07T20:30:21.100217: step 2275, loss 0.0738476, acc 0.96
2016-09-07T20:30:21.772314: step 2276, loss 0.143415, acc 0.96
2016-09-07T20:30:22.455365: step 2277, loss 0.0507805, acc 0.98
2016-09-07T20:30:23.111531: step 2278, loss 0.0294748, acc 1
2016-09-07T20:30:23.803407: step 2279, loss 0.102526, acc 0.96
2016-09-07T20:30:24.462036: step 2280, loss 0.0380874, acc 1
2016-09-07T20:30:25.139294: step 2281, loss 0.0706365, acc 0.98
2016-09-07T20:30:25.808776: step 2282, loss 0.107464, acc 0.94
2016-09-07T20:30:26.486462: step 2283, loss 0.0672492, acc 0.96
2016-09-07T20:30:27.156798: step 2284, loss 0.0147292, acc 1
2016-09-07T20:30:27.813225: step 2285, loss 0.0634124, acc 0.96
2016-09-07T20:30:28.488060: step 2286, loss 0.0314446, acc 1
2016-09-07T20:30:29.178361: step 2287, loss 0.0404674, acc 0.98
2016-09-07T20:30:29.859786: step 2288, loss 0.0523569, acc 0.96
2016-09-07T20:30:30.546808: step 2289, loss 0.170429, acc 0.94
2016-09-07T20:30:31.222842: step 2290, loss 0.0640455, acc 0.98
2016-09-07T20:30:31.900045: step 2291, loss 0.051301, acc 0.98
2016-09-07T20:30:32.570347: step 2292, loss 0.0559712, acc 0.98
2016-09-07T20:30:33.219208: step 2293, loss 0.0775477, acc 0.96
2016-09-07T20:30:33.890777: step 2294, loss 0.0770276, acc 0.98
2016-09-07T20:30:34.587472: step 2295, loss 0.0166146, acc 1
2016-09-07T20:30:35.255423: step 2296, loss 0.135994, acc 0.96
2016-09-07T20:30:35.926319: step 2297, loss 0.0692975, acc 0.96
2016-09-07T20:30:36.592709: step 2298, loss 0.0891427, acc 0.96
2016-09-07T20:30:37.261428: step 2299, loss 0.0490012, acc 0.98
2016-09-07T20:30:37.930461: step 2300, loss 0.195451, acc 0.9

Evaluation:
2016-09-07T20:30:40.812907: step 2300, loss 1.33271, acc 0.751

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-2300

2016-09-07T20:30:42.447611: step 2301, loss 0.0349951, acc 0.98
2016-09-07T20:30:43.117530: step 2302, loss 0.0342391, acc 1
2016-09-07T20:30:43.777101: step 2303, loss 0.0920739, acc 0.98
2016-09-07T20:30:44.439748: step 2304, loss 0.110252, acc 0.96
2016-09-07T20:30:45.123769: step 2305, loss 0.0299201, acc 0.98
2016-09-07T20:30:45.795865: step 2306, loss 0.0238546, acc 1
2016-09-07T20:30:46.458850: step 2307, loss 0.0263045, acc 1
2016-09-07T20:30:47.131526: step 2308, loss 0.0535727, acc 0.98
2016-09-07T20:30:47.811149: step 2309, loss 0.168651, acc 0.96
2016-09-07T20:30:48.484837: step 2310, loss 0.0676385, acc 0.96
2016-09-07T20:30:49.176027: step 2311, loss 0.0507955, acc 0.96
2016-09-07T20:30:49.843853: step 2312, loss 0.0621607, acc 1
2016-09-07T20:30:50.514190: step 2313, loss 0.0456035, acc 0.98
2016-09-07T20:30:51.197092: step 2314, loss 0.059939, acc 0.98
2016-09-07T20:30:51.878286: step 2315, loss 0.0859214, acc 0.96
2016-09-07T20:30:52.539496: step 2316, loss 0.0160978, acc 1
2016-09-07T20:30:53.202292: step 2317, loss 0.04131, acc 1
2016-09-07T20:30:53.879852: step 2318, loss 0.049362, acc 1
2016-09-07T20:30:54.537315: step 2319, loss 0.0831272, acc 0.98
2016-09-07T20:30:55.206766: step 2320, loss 0.187084, acc 0.94
2016-09-07T20:30:55.892081: step 2321, loss 0.0569436, acc 0.98
2016-09-07T20:30:56.561042: step 2322, loss 0.0176084, acc 1
2016-09-07T20:30:57.239233: step 2323, loss 0.0596582, acc 0.98
2016-09-07T20:30:57.923729: step 2324, loss 0.0523043, acc 0.98
2016-09-07T20:30:58.608391: step 2325, loss 0.107746, acc 0.94
2016-09-07T20:30:59.276512: step 2326, loss 0.0361296, acc 0.98
2016-09-07T20:30:59.946358: step 2327, loss 0.0320065, acc 1
2016-09-07T20:31:00.363870: step 2328, loss 0.0806071, acc 0.916667
2016-09-07T20:31:01.050547: step 2329, loss 0.0319602, acc 1
2016-09-07T20:31:01.747198: step 2330, loss 0.0395292, acc 1
2016-09-07T20:31:02.425139: step 2331, loss 0.0821967, acc 0.92
2016-09-07T20:31:03.086960: step 2332, loss 0.0457624, acc 0.98
2016-09-07T20:31:03.762960: step 2333, loss 0.09186, acc 0.98
2016-09-07T20:31:04.435586: step 2334, loss 0.0421915, acc 0.98
2016-09-07T20:31:05.093462: step 2335, loss 0.0474623, acc 0.96
2016-09-07T20:31:05.761509: step 2336, loss 0.0487368, acc 0.98
2016-09-07T20:31:06.441892: step 2337, loss 0.0277218, acc 0.98
2016-09-07T20:31:07.102990: step 2338, loss 0.0822259, acc 0.98
2016-09-07T20:31:07.774139: step 2339, loss 0.0796897, acc 0.98
2016-09-07T20:31:08.450650: step 2340, loss 0.0557828, acc 0.96
2016-09-07T20:31:09.135509: step 2341, loss 0.0220386, acc 1
2016-09-07T20:31:09.801664: step 2342, loss 0.0542119, acc 0.98
2016-09-07T20:31:10.469461: step 2343, loss 0.0190181, acc 1
2016-09-07T20:31:11.125860: step 2344, loss 0.00941113, acc 1
2016-09-07T20:31:11.802630: step 2345, loss 0.0617925, acc 0.98
2016-09-07T20:31:12.489113: step 2346, loss 0.176612, acc 0.94
2016-09-07T20:31:13.159664: step 2347, loss 0.0511067, acc 0.98
2016-09-07T20:31:13.826599: step 2348, loss 0.00782398, acc 1
2016-09-07T20:31:14.518996: step 2349, loss 0.0261889, acc 0.98
2016-09-07T20:31:15.190330: step 2350, loss 0.0996759, acc 0.96
2016-09-07T20:31:15.852487: step 2351, loss 0.0854225, acc 0.98
2016-09-07T20:31:16.515430: step 2352, loss 0.0217432, acc 1
2016-09-07T20:31:17.187438: step 2353, loss 0.0310344, acc 0.98
2016-09-07T20:31:17.851618: step 2354, loss 0.0193853, acc 1
2016-09-07T20:31:18.524264: step 2355, loss 0.0254232, acc 0.98
2016-09-07T20:31:19.183310: step 2356, loss 0.0145693, acc 1
2016-09-07T20:31:19.883091: step 2357, loss 0.0336101, acc 0.98
2016-09-07T20:31:20.541010: step 2358, loss 0.108953, acc 0.96
2016-09-07T20:31:21.206244: step 2359, loss 0.0454757, acc 0.98
2016-09-07T20:31:21.882565: step 2360, loss 0.0554669, acc 0.96
2016-09-07T20:31:22.541330: step 2361, loss 0.0925124, acc 0.98
2016-09-07T20:31:23.231263: step 2362, loss 0.111376, acc 0.94
2016-09-07T20:31:23.901281: step 2363, loss 0.0268315, acc 0.98
2016-09-07T20:31:24.570966: step 2364, loss 0.0731859, acc 0.96
2016-09-07T20:31:25.232985: step 2365, loss 0.0579907, acc 0.98
2016-09-07T20:31:25.912525: step 2366, loss 0.0826545, acc 0.94
2016-09-07T20:31:26.579058: step 2367, loss 0.0462943, acc 1
2016-09-07T20:31:27.246490: step 2368, loss 0.0552054, acc 0.98
2016-09-07T20:31:27.926124: step 2369, loss 0.0942847, acc 0.94
2016-09-07T20:31:28.597388: step 2370, loss 0.106389, acc 0.96
2016-09-07T20:31:29.269794: step 2371, loss 0.0386116, acc 0.98
2016-09-07T20:31:29.926604: step 2372, loss 0.102471, acc 0.94
2016-09-07T20:31:30.598132: step 2373, loss 0.0935642, acc 0.96
2016-09-07T20:31:31.293276: step 2374, loss 0.0473817, acc 0.98
2016-09-07T20:31:31.977987: step 2375, loss 0.0335472, acc 0.98
2016-09-07T20:31:32.646493: step 2376, loss 0.0529525, acc 0.98
2016-09-07T20:31:33.320200: step 2377, loss 0.0435174, acc 0.98
2016-09-07T20:31:33.985676: step 2378, loss 0.0203333, acc 1
2016-09-07T20:31:34.666908: step 2379, loss 0.0581181, acc 0.98
2016-09-07T20:31:35.321645: step 2380, loss 0.0361303, acc 0.96
2016-09-07T20:31:35.975574: step 2381, loss 0.0218836, acc 1
2016-09-07T20:31:36.634408: step 2382, loss 0.00661024, acc 1
2016-09-07T20:31:37.294072: step 2383, loss 0.0380675, acc 0.98
2016-09-07T20:31:37.965997: step 2384, loss 0.028045, acc 1
2016-09-07T20:31:38.634298: step 2385, loss 0.0165211, acc 1
2016-09-07T20:31:39.304546: step 2386, loss 0.125302, acc 0.96
2016-09-07T20:31:39.998064: step 2387, loss 0.0302039, acc 1
2016-09-07T20:31:40.657285: step 2388, loss 0.0486136, acc 0.96
2016-09-07T20:31:41.315678: step 2389, loss 0.0524491, acc 0.98
2016-09-07T20:31:41.994340: step 2390, loss 0.0501737, acc 0.98
2016-09-07T20:31:42.648641: step 2391, loss 0.04747, acc 0.98
2016-09-07T20:31:43.316775: step 2392, loss 0.0548744, acc 1
2016-09-07T20:31:43.995604: step 2393, loss 0.0894304, acc 0.94
2016-09-07T20:31:44.656867: step 2394, loss 0.0448395, acc 0.98
2016-09-07T20:31:45.323367: step 2395, loss 0.0308511, acc 1
2016-09-07T20:31:45.991592: step 2396, loss 0.0389789, acc 0.98
2016-09-07T20:31:46.663240: step 2397, loss 0.0293046, acc 0.98
2016-09-07T20:31:47.321547: step 2398, loss 0.0630189, acc 0.96
2016-09-07T20:31:47.995982: step 2399, loss 0.0795815, acc 0.98
2016-09-07T20:31:48.674809: step 2400, loss 0.057877, acc 0.98

Evaluation:
2016-09-07T20:31:51.558430: step 2400, loss 1.47043, acc 0.736

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-2400

2016-09-07T20:31:53.334043: step 2401, loss 0.0172441, acc 1
2016-09-07T20:31:54.010255: step 2402, loss 0.0179853, acc 1
2016-09-07T20:31:54.683971: step 2403, loss 0.0596513, acc 0.98
2016-09-07T20:31:55.340850: step 2404, loss 0.114351, acc 0.94
2016-09-07T20:31:56.018538: step 2405, loss 0.086198, acc 0.94
2016-09-07T20:31:56.678587: step 2406, loss 0.0614056, acc 0.98
2016-09-07T20:31:57.353672: step 2407, loss 0.0809438, acc 0.94
2016-09-07T20:31:58.047198: step 2408, loss 0.0411927, acc 0.98
2016-09-07T20:31:58.714408: step 2409, loss 0.0606836, acc 0.96
2016-09-07T20:31:59.372412: step 2410, loss 0.00871185, acc 1
2016-09-07T20:32:00.041213: step 2411, loss 0.0474843, acc 0.98
2016-09-07T20:32:00.785683: step 2412, loss 0.0235343, acc 1
2016-09-07T20:32:01.471941: step 2413, loss 0.0261252, acc 0.98
2016-09-07T20:32:02.133853: step 2414, loss 0.0274556, acc 1
2016-09-07T20:32:02.827043: step 2415, loss 0.00851806, acc 1
2016-09-07T20:32:03.498055: step 2416, loss 0.0096272, acc 1
2016-09-07T20:32:04.163797: step 2417, loss 0.0744535, acc 0.98
2016-09-07T20:32:04.837402: step 2418, loss 0.0520952, acc 0.96
2016-09-07T20:32:05.544416: step 2419, loss 0.121308, acc 0.98
2016-09-07T20:32:06.217082: step 2420, loss 0.0326755, acc 0.98
2016-09-07T20:32:06.870008: step 2421, loss 0.0210546, acc 1
2016-09-07T20:32:07.540516: step 2422, loss 0.0205023, acc 1
2016-09-07T20:32:08.201077: step 2423, loss 0.0275793, acc 0.98
2016-09-07T20:32:08.893681: step 2424, loss 0.0652147, acc 0.96
2016-09-07T20:32:09.572596: step 2425, loss 0.0179948, acc 1
2016-09-07T20:32:10.247723: step 2426, loss 0.0640904, acc 0.98
2016-09-07T20:32:10.904041: step 2427, loss 0.042447, acc 0.98
2016-09-07T20:32:11.582158: step 2428, loss 0.0389407, acc 1
2016-09-07T20:32:12.259449: step 2429, loss 0.0621205, acc 0.98
2016-09-07T20:32:12.938709: step 2430, loss 0.0698739, acc 0.96
2016-09-07T20:32:13.612943: step 2431, loss 0.0809949, acc 0.98
2016-09-07T20:32:14.303500: step 2432, loss 0.0215461, acc 1
2016-09-07T20:32:14.979095: step 2433, loss 0.0924957, acc 0.92
2016-09-07T20:32:15.640002: step 2434, loss 0.0820604, acc 0.96
2016-09-07T20:32:16.309684: step 2435, loss 0.0991487, acc 0.96
2016-09-07T20:32:16.982920: step 2436, loss 0.0227026, acc 1
2016-09-07T20:32:17.661802: step 2437, loss 0.0329827, acc 1
2016-09-07T20:32:18.338330: step 2438, loss 0.110716, acc 0.98
2016-09-07T20:32:19.021257: step 2439, loss 0.0899341, acc 0.96
2016-09-07T20:32:19.688980: step 2440, loss 0.0129533, acc 1
2016-09-07T20:32:20.340601: step 2441, loss 0.101023, acc 0.98
2016-09-07T20:32:20.994472: step 2442, loss 0.0815311, acc 0.94
2016-09-07T20:32:21.661789: step 2443, loss 0.0853513, acc 0.94
2016-09-07T20:32:22.358442: step 2444, loss 0.0549102, acc 1
2016-09-07T20:32:23.043997: step 2445, loss 0.0728091, acc 0.94
2016-09-07T20:32:23.711346: step 2446, loss 0.0864198, acc 0.98
2016-09-07T20:32:24.391199: step 2447, loss 0.147791, acc 0.94
2016-09-07T20:32:25.063542: step 2448, loss 0.0371271, acc 1
2016-09-07T20:32:25.736287: step 2449, loss 0.0509801, acc 0.98
2016-09-07T20:32:26.402679: step 2450, loss 0.0858466, acc 0.98
2016-09-07T20:32:27.086071: step 2451, loss 0.0184978, acc 1
2016-09-07T20:32:27.758376: step 2452, loss 0.0257099, acc 1
2016-09-07T20:32:28.426895: step 2453, loss 0.0212182, acc 1
2016-09-07T20:32:29.086890: step 2454, loss 0.0457956, acc 0.98
2016-09-07T20:32:29.768254: step 2455, loss 0.0376152, acc 0.98
2016-09-07T20:32:30.451652: step 2456, loss 0.0168838, acc 1
2016-09-07T20:32:31.137893: step 2457, loss 0.0943747, acc 0.94
2016-09-07T20:32:31.809634: step 2458, loss 0.0574303, acc 0.96
2016-09-07T20:32:32.479398: step 2459, loss 0.0135787, acc 1
2016-09-07T20:32:33.146326: step 2460, loss 0.0522097, acc 0.98
2016-09-07T20:32:33.820068: step 2461, loss 0.0549896, acc 0.96
2016-09-07T20:32:34.494959: step 2462, loss 0.0643686, acc 1
2016-09-07T20:32:35.166953: step 2463, loss 0.0276233, acc 1
2016-09-07T20:32:35.840895: step 2464, loss 0.0326613, acc 0.98
2016-09-07T20:32:36.507016: step 2465, loss 0.0363489, acc 1
2016-09-07T20:32:37.179785: step 2466, loss 0.0711305, acc 0.98
2016-09-07T20:32:37.851862: step 2467, loss 0.00580392, acc 1
2016-09-07T20:32:38.532090: step 2468, loss 0.0517968, acc 0.98
2016-09-07T20:32:39.207068: step 2469, loss 0.10171, acc 0.98
2016-09-07T20:32:39.895683: step 2470, loss 0.0360604, acc 0.98
2016-09-07T20:32:40.576459: step 2471, loss 0.0175436, acc 1
2016-09-07T20:32:41.234456: step 2472, loss 0.0320051, acc 0.98
2016-09-07T20:32:41.909696: step 2473, loss 0.100481, acc 0.98
2016-09-07T20:32:42.592793: step 2474, loss 0.0836121, acc 0.96
2016-09-07T20:32:43.259192: step 2475, loss 0.0549476, acc 0.98
2016-09-07T20:32:43.926976: step 2476, loss 0.0320438, acc 1
2016-09-07T20:32:44.585268: step 2477, loss 0.036853, acc 0.98
2016-09-07T20:32:45.262281: step 2478, loss 0.0466429, acc 0.98
2016-09-07T20:32:45.941338: step 2479, loss 0.0574106, acc 0.96
2016-09-07T20:32:46.609666: step 2480, loss 0.17562, acc 0.92
2016-09-07T20:32:47.280467: step 2481, loss 0.0549684, acc 0.96
2016-09-07T20:32:47.949223: step 2482, loss 0.0555887, acc 1
2016-09-07T20:32:48.640384: step 2483, loss 0.0944582, acc 0.98
2016-09-07T20:32:49.362894: step 2484, loss 0.0127059, acc 1
2016-09-07T20:32:50.035143: step 2485, loss 0.00539689, acc 1
2016-09-07T20:32:50.706619: step 2486, loss 0.160095, acc 0.94
2016-09-07T20:32:51.396807: step 2487, loss 0.0349364, acc 0.98
2016-09-07T20:32:52.092424: step 2488, loss 0.0644404, acc 0.96
2016-09-07T20:32:52.756706: step 2489, loss 0.105648, acc 0.96
2016-09-07T20:32:53.433227: step 2490, loss 0.0450303, acc 1
2016-09-07T20:32:54.113713: step 2491, loss 0.0189898, acc 1
2016-09-07T20:32:54.784432: step 2492, loss 0.0241268, acc 1
2016-09-07T20:32:55.441360: step 2493, loss 0.0365585, acc 0.98
2016-09-07T20:32:56.134471: step 2494, loss 0.0489957, acc 0.96
2016-09-07T20:32:56.807986: step 2495, loss 0.0346805, acc 0.98
2016-09-07T20:32:57.480474: step 2496, loss 0.0195356, acc 0.98
2016-09-07T20:32:58.139821: step 2497, loss 0.0306942, acc 1
2016-09-07T20:32:58.805550: step 2498, loss 0.0943292, acc 0.94
2016-09-07T20:32:59.463818: step 2499, loss 0.0292355, acc 0.98
2016-09-07T20:33:00.120602: step 2500, loss 0.0472288, acc 0.98

Evaluation:
2016-09-07T20:33:03.070754: step 2500, loss 1.47302, acc 0.744

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-2500

2016-09-07T20:33:04.659235: step 2501, loss 0.0462653, acc 0.98
2016-09-07T20:33:05.304999: step 2502, loss 0.0296146, acc 0.98
2016-09-07T20:33:05.998939: step 2503, loss 0.0485013, acc 0.98
2016-09-07T20:33:06.681187: step 2504, loss 0.052674, acc 0.98
2016-09-07T20:33:07.337868: step 2505, loss 0.0878798, acc 0.98
2016-09-07T20:33:07.993702: step 2506, loss 0.0171776, acc 1
2016-09-07T20:33:08.668119: step 2507, loss 0.0664388, acc 0.94
2016-09-07T20:33:09.343349: step 2508, loss 0.102954, acc 0.94
2016-09-07T20:33:10.014776: step 2509, loss 0.0863571, acc 0.96
2016-09-07T20:33:10.679329: step 2510, loss 0.0607342, acc 1
2016-09-07T20:33:11.346184: step 2511, loss 0.0134152, acc 1
2016-09-07T20:33:12.029787: step 2512, loss 0.086592, acc 0.98
2016-09-07T20:33:12.697284: step 2513, loss 0.0677056, acc 0.96
2016-09-07T20:33:13.379153: step 2514, loss 0.0371002, acc 0.98
2016-09-07T20:33:14.037763: step 2515, loss 0.0919773, acc 0.94
2016-09-07T20:33:14.705718: step 2516, loss 0.0482262, acc 0.98
2016-09-07T20:33:15.385997: step 2517, loss 0.0357583, acc 0.98
2016-09-07T20:33:16.057021: step 2518, loss 0.0174332, acc 1
2016-09-07T20:33:16.732979: step 2519, loss 0.0684782, acc 0.98
2016-09-07T20:33:17.408320: step 2520, loss 0.0535961, acc 0.98
2016-09-07T20:33:18.078233: step 2521, loss 0.0202369, acc 1
2016-09-07T20:33:18.425436: step 2522, loss 0.0194823, acc 1
2016-09-07T20:33:19.103960: step 2523, loss 0.0215445, acc 1
2016-09-07T20:33:19.773042: step 2524, loss 0.0345873, acc 0.98
2016-09-07T20:33:20.447762: step 2525, loss 0.0442888, acc 0.98
2016-09-07T20:33:21.113447: step 2526, loss 0.0235403, acc 1
2016-09-07T20:33:21.782756: step 2527, loss 0.0114497, acc 1
2016-09-07T20:33:22.454021: step 2528, loss 0.135761, acc 0.94
2016-09-07T20:33:23.116110: step 2529, loss 0.0097287, acc 1
2016-09-07T20:33:23.779960: step 2530, loss 0.0427348, acc 0.98
2016-09-07T20:33:24.455311: step 2531, loss 0.0345616, acc 0.98
2016-09-07T20:33:25.122567: step 2532, loss 0.0257065, acc 1
2016-09-07T20:33:25.789412: step 2533, loss 0.0309385, acc 0.98
2016-09-07T20:33:26.445457: step 2534, loss 0.0143447, acc 1
2016-09-07T20:33:27.112099: step 2535, loss 0.0996991, acc 0.96
2016-09-07T20:33:27.769784: step 2536, loss 0.137398, acc 0.94
2016-09-07T20:33:28.460700: step 2537, loss 0.00968512, acc 1
2016-09-07T20:33:29.150008: step 2538, loss 0.0108728, acc 1
2016-09-07T20:33:29.836499: step 2539, loss 0.116501, acc 0.94
2016-09-07T20:33:30.508785: step 2540, loss 0.00363356, acc 1
2016-09-07T20:33:31.176549: step 2541, loss 0.0144436, acc 1
2016-09-07T20:33:31.830401: step 2542, loss 0.0255527, acc 1
2016-09-07T20:33:32.487156: step 2543, loss 0.0509287, acc 0.98
2016-09-07T20:33:33.144536: step 2544, loss 0.067246, acc 0.96
2016-09-07T20:33:33.838812: step 2545, loss 0.0959783, acc 0.96
2016-09-07T20:33:34.523819: step 2546, loss 0.020806, acc 0.98
2016-09-07T20:33:35.242679: step 2547, loss 0.141259, acc 0.98
2016-09-07T20:33:35.919189: step 2548, loss 0.0157626, acc 1
2016-09-07T20:33:36.590642: step 2549, loss 0.0263182, acc 1
2016-09-07T20:33:37.254285: step 2550, loss 0.033927, acc 1
2016-09-07T20:33:37.906353: step 2551, loss 0.0965987, acc 0.98
2016-09-07T20:33:38.578607: step 2552, loss 0.0114023, acc 1
2016-09-07T20:33:39.264854: step 2553, loss 0.0700356, acc 0.96
2016-09-07T20:33:39.948027: step 2554, loss 0.049653, acc 0.98
2016-09-07T20:33:40.620614: step 2555, loss 0.0384062, acc 0.98
2016-09-07T20:33:41.295291: step 2556, loss 0.00666253, acc 1
2016-09-07T20:33:41.992398: step 2557, loss 0.0224068, acc 1
2016-09-07T20:33:42.715734: step 2558, loss 0.0905603, acc 0.98
2016-09-07T20:33:43.423339: step 2559, loss 0.0883857, acc 0.96
2016-09-07T20:33:44.110739: step 2560, loss 0.123096, acc 0.94
2016-09-07T20:33:44.772229: step 2561, loss 0.0319274, acc 0.98
2016-09-07T20:33:45.451397: step 2562, loss 0.0159729, acc 1
2016-09-07T20:33:46.116052: step 2563, loss 0.0963531, acc 0.96
2016-09-07T20:33:46.783969: step 2564, loss 0.0343273, acc 0.98
2016-09-07T20:33:47.459794: step 2565, loss 0.0958744, acc 0.98
2016-09-07T20:33:48.128914: step 2566, loss 0.0315177, acc 0.98
2016-09-07T20:33:48.800699: step 2567, loss 0.0633233, acc 0.96
2016-09-07T20:33:49.471608: step 2568, loss 0.126426, acc 0.92
2016-09-07T20:33:50.128568: step 2569, loss 0.0323439, acc 1
2016-09-07T20:33:50.805975: step 2570, loss 0.0133302, acc 1
2016-09-07T20:33:51.476086: step 2571, loss 0.0241257, acc 1
2016-09-07T20:33:52.142397: step 2572, loss 0.0484615, acc 1
2016-09-07T20:33:52.804643: step 2573, loss 0.0268952, acc 1
2016-09-07T20:33:53.476963: step 2574, loss 0.0201748, acc 0.98
2016-09-07T20:33:54.140505: step 2575, loss 0.0213865, acc 0.98
2016-09-07T20:33:54.824204: step 2576, loss 0.00612517, acc 1
2016-09-07T20:33:55.499948: step 2577, loss 0.00779031, acc 1
2016-09-07T20:33:56.173198: step 2578, loss 0.0123542, acc 1
2016-09-07T20:33:56.855605: step 2579, loss 0.0484642, acc 0.98
2016-09-07T20:33:57.537732: step 2580, loss 0.047442, acc 0.96
2016-09-07T20:33:58.216780: step 2581, loss 0.0357754, acc 0.98
2016-09-07T20:33:58.898826: step 2582, loss 0.0241919, acc 1
2016-09-07T20:33:59.562424: step 2583, loss 0.0334941, acc 1
2016-09-07T20:34:00.259572: step 2584, loss 0.0388068, acc 0.96
2016-09-07T20:34:00.928741: step 2585, loss 0.0624838, acc 0.96
2016-09-07T20:34:01.616284: step 2586, loss 0.0588985, acc 0.98
2016-09-07T20:34:02.300078: step 2587, loss 0.0454304, acc 0.98
2016-09-07T20:34:02.970453: step 2588, loss 0.0198244, acc 1
2016-09-07T20:34:03.637957: step 2589, loss 0.0529427, acc 0.96
2016-09-07T20:34:04.314134: step 2590, loss 0.00374432, acc 1
2016-09-07T20:34:04.995449: step 2591, loss 0.0663423, acc 0.96
2016-09-07T20:34:05.660330: step 2592, loss 0.0449175, acc 0.98
2016-09-07T20:34:06.330008: step 2593, loss 0.0597387, acc 0.96
2016-09-07T20:34:07.014193: step 2594, loss 0.0264942, acc 1
2016-09-07T20:34:07.692456: step 2595, loss 0.00671447, acc 1
2016-09-07T20:34:08.360348: step 2596, loss 0.0298874, acc 0.98
2016-09-07T20:34:09.024398: step 2597, loss 0.0150133, acc 1
2016-09-07T20:34:09.701011: step 2598, loss 0.0353549, acc 1
2016-09-07T20:34:10.375736: step 2599, loss 0.0212881, acc 1
2016-09-07T20:34:11.039426: step 2600, loss 0.00832813, acc 1

Evaluation:
2016-09-07T20:34:13.948590: step 2600, loss 1.72415, acc 0.727

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-2600

2016-09-07T20:34:15.679756: step 2601, loss 0.0174756, acc 1
2016-09-07T20:34:16.335771: step 2602, loss 0.0682124, acc 0.96
2016-09-07T20:34:17.031403: step 2603, loss 0.0823305, acc 0.96
2016-09-07T20:34:17.695462: step 2604, loss 0.0490965, acc 0.98
2016-09-07T20:34:18.384333: step 2605, loss 0.00698094, acc 1
2016-09-07T20:34:19.052763: step 2606, loss 0.00772093, acc 1
2016-09-07T20:34:19.722202: step 2607, loss 0.100829, acc 0.96
2016-09-07T20:34:20.380946: step 2608, loss 0.039947, acc 0.98
2016-09-07T20:34:21.074493: step 2609, loss 0.0549911, acc 0.96
2016-09-07T20:34:21.744260: step 2610, loss 0.0335313, acc 0.98
2016-09-07T20:34:22.404051: step 2611, loss 0.0180945, acc 1
2016-09-07T20:34:23.078759: step 2612, loss 0.0750939, acc 0.98
2016-09-07T20:34:23.744223: step 2613, loss 0.0317061, acc 0.98
2016-09-07T20:34:24.423719: step 2614, loss 0.0203367, acc 1
2016-09-07T20:34:25.100428: step 2615, loss 0.0305607, acc 1
2016-09-07T20:34:25.762692: step 2616, loss 0.014301, acc 1
2016-09-07T20:34:26.415135: step 2617, loss 0.0951659, acc 0.92
2016-09-07T20:34:27.090480: step 2618, loss 0.0277703, acc 0.98
2016-09-07T20:34:27.756889: step 2619, loss 0.0561078, acc 0.98
2016-09-07T20:34:28.428050: step 2620, loss 0.0462067, acc 0.96
2016-09-07T20:34:29.091570: step 2621, loss 0.0630965, acc 0.98
2016-09-07T20:34:29.762954: step 2622, loss 0.0545676, acc 0.98
2016-09-07T20:34:30.423105: step 2623, loss 0.0128214, acc 1
2016-09-07T20:34:31.091956: step 2624, loss 0.0193502, acc 1
2016-09-07T20:34:31.768891: step 2625, loss 0.0185506, acc 1
2016-09-07T20:34:32.432174: step 2626, loss 0.123727, acc 0.96
2016-09-07T20:34:33.101128: step 2627, loss 0.0153937, acc 1
2016-09-07T20:34:33.758904: step 2628, loss 0.0517113, acc 0.98
2016-09-07T20:34:34.428956: step 2629, loss 0.0242875, acc 1
2016-09-07T20:34:35.110689: step 2630, loss 0.0301582, acc 0.98
2016-09-07T20:34:35.786928: step 2631, loss 0.0855699, acc 0.96
2016-09-07T20:34:36.471425: step 2632, loss 0.0384149, acc 0.98
2016-09-07T20:34:37.137030: step 2633, loss 0.121075, acc 0.94
2016-09-07T20:34:37.782456: step 2634, loss 0.0187786, acc 1
2016-09-07T20:34:38.450440: step 2635, loss 0.0243786, acc 1
2016-09-07T20:34:39.137599: step 2636, loss 0.0141274, acc 1
2016-09-07T20:34:39.803402: step 2637, loss 0.0468566, acc 0.96
2016-09-07T20:34:40.467482: step 2638, loss 0.064244, acc 0.96
2016-09-07T20:34:41.117201: step 2639, loss 0.0444696, acc 0.98
2016-09-07T20:34:41.775234: step 2640, loss 0.0380985, acc 1
2016-09-07T20:34:42.460872: step 2641, loss 0.00750895, acc 1
2016-09-07T20:34:43.142834: step 2642, loss 0.0477858, acc 0.98
2016-09-07T20:34:43.819636: step 2643, loss 0.0556378, acc 0.98
2016-09-07T20:34:44.499945: step 2644, loss 0.0770204, acc 0.98
2016-09-07T20:34:45.176350: step 2645, loss 0.0996353, acc 0.96
2016-09-07T20:34:45.851818: step 2646, loss 0.0357194, acc 0.98
2016-09-07T20:34:46.525636: step 2647, loss 0.0121922, acc 1
2016-09-07T20:34:47.206386: step 2648, loss 0.0377982, acc 0.98
2016-09-07T20:34:47.871094: step 2649, loss 0.0307053, acc 0.98
2016-09-07T20:34:48.542739: step 2650, loss 0.0920451, acc 0.98
2016-09-07T20:34:49.208763: step 2651, loss 0.00683364, acc 1
2016-09-07T20:34:49.887009: step 2652, loss 0.0274554, acc 0.98
2016-09-07T20:34:50.558699: step 2653, loss 0.0571352, acc 0.96
2016-09-07T20:34:51.221105: step 2654, loss 0.0278321, acc 0.98
2016-09-07T20:34:51.882043: step 2655, loss 0.0326819, acc 0.98
2016-09-07T20:34:52.537074: step 2656, loss 0.0644543, acc 1
2016-09-07T20:34:53.199979: step 2657, loss 0.0668486, acc 0.98
2016-09-07T20:34:53.879210: step 2658, loss 0.0430919, acc 0.98
2016-09-07T20:34:54.559911: step 2659, loss 0.0573121, acc 0.98
2016-09-07T20:34:55.219856: step 2660, loss 0.00324661, acc 1
2016-09-07T20:34:55.874626: step 2661, loss 0.0890554, acc 0.98
2016-09-07T20:34:56.533066: step 2662, loss 0.0672901, acc 0.96
2016-09-07T20:34:57.214935: step 2663, loss 0.0758297, acc 0.98
2016-09-07T20:34:57.869772: step 2664, loss 0.0411761, acc 0.98
2016-09-07T20:34:58.537875: step 2665, loss 0.0784611, acc 0.96
2016-09-07T20:34:59.222004: step 2666, loss 0.0116728, acc 1
2016-09-07T20:34:59.910852: step 2667, loss 0.0326133, acc 0.98
2016-09-07T20:35:00.631514: step 2668, loss 0.0939527, acc 0.96
2016-09-07T20:35:01.310606: step 2669, loss 0.0463384, acc 0.96
2016-09-07T20:35:01.984116: step 2670, loss 0.0597272, acc 0.96
2016-09-07T20:35:02.658672: step 2671, loss 0.0573103, acc 0.98
2016-09-07T20:35:03.304749: step 2672, loss 0.035959, acc 0.96
2016-09-07T20:35:03.974583: step 2673, loss 0.0354415, acc 1
2016-09-07T20:35:04.693655: step 2674, loss 0.0509192, acc 0.98
2016-09-07T20:35:05.377428: step 2675, loss 0.0476603, acc 0.96
2016-09-07T20:35:06.058899: step 2676, loss 0.0367232, acc 1
2016-09-07T20:35:06.715026: step 2677, loss 0.0431374, acc 1
2016-09-07T20:35:07.398654: step 2678, loss 0.0408132, acc 0.98
2016-09-07T20:35:08.079833: step 2679, loss 0.0177484, acc 1
2016-09-07T20:35:08.742498: step 2680, loss 0.00899401, acc 1
2016-09-07T20:35:09.404576: step 2681, loss 0.00405411, acc 1
2016-09-07T20:35:10.078005: step 2682, loss 0.0763005, acc 0.98
2016-09-07T20:35:10.732998: step 2683, loss 0.0334636, acc 1
2016-09-07T20:35:11.398838: step 2684, loss 0.0297852, acc 1
2016-09-07T20:35:12.064511: step 2685, loss 0.00466552, acc 1
2016-09-07T20:35:12.744213: step 2686, loss 0.0153847, acc 1
2016-09-07T20:35:13.418074: step 2687, loss 0.064115, acc 0.96
2016-09-07T20:35:14.084658: step 2688, loss 0.01778, acc 1
2016-09-07T20:35:14.759034: step 2689, loss 0.011596, acc 1
2016-09-07T20:35:15.423042: step 2690, loss 0.0464465, acc 0.98
2016-09-07T20:35:16.097716: step 2691, loss 0.0169653, acc 1
2016-09-07T20:35:16.763446: step 2692, loss 0.0651962, acc 0.96
2016-09-07T20:35:17.446416: step 2693, loss 0.0761291, acc 0.96
2016-09-07T20:35:18.126367: step 2694, loss 0.0258801, acc 1
2016-09-07T20:35:18.808544: step 2695, loss 0.0252976, acc 1
2016-09-07T20:35:19.500752: step 2696, loss 0.0589149, acc 0.98
2016-09-07T20:35:20.179473: step 2697, loss 0.0616291, acc 0.98
2016-09-07T20:35:20.855926: step 2698, loss 0.0629784, acc 0.96
2016-09-07T20:35:21.528246: step 2699, loss 0.104691, acc 0.92
2016-09-07T20:35:22.195043: step 2700, loss 0.0472138, acc 0.98

Evaluation:
2016-09-07T20:35:25.098219: step 2700, loss 1.86323, acc 0.718

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-2700

2016-09-07T20:35:26.709348: step 2701, loss 0.0968146, acc 0.96
2016-09-07T20:35:27.400592: step 2702, loss 0.0601831, acc 0.98
2016-09-07T20:35:28.057608: step 2703, loss 0.07338, acc 0.94
2016-09-07T20:35:28.726978: step 2704, loss 0.0129789, acc 1
2016-09-07T20:35:29.400526: step 2705, loss 0.124703, acc 0.94
2016-09-07T20:35:30.086643: step 2706, loss 0.0224486, acc 0.98
2016-09-07T20:35:30.771887: step 2707, loss 0.022536, acc 1
2016-09-07T20:35:31.438646: step 2708, loss 0.0221185, acc 1
2016-09-07T20:35:32.101895: step 2709, loss 0.0917357, acc 0.94
2016-09-07T20:35:32.754723: step 2710, loss 0.00718697, acc 1
2016-09-07T20:35:33.429990: step 2711, loss 0.0511037, acc 0.96
2016-09-07T20:35:34.098359: step 2712, loss 0.0959816, acc 0.94
2016-09-07T20:35:34.768527: step 2713, loss 0.0340065, acc 0.98
2016-09-07T20:35:35.442796: step 2714, loss 0.0992493, acc 0.96
2016-09-07T20:35:36.116377: step 2715, loss 0.0315735, acc 0.98
2016-09-07T20:35:36.489261: step 2716, loss 0.12458, acc 0.916667
2016-09-07T20:35:37.153241: step 2717, loss 0.0555703, acc 0.98
2016-09-07T20:35:37.810783: step 2718, loss 0.0391878, acc 0.98
2016-09-07T20:35:38.471523: step 2719, loss 0.0934018, acc 0.96
2016-09-07T20:35:39.143299: step 2720, loss 0.0381068, acc 1
2016-09-07T20:35:39.829211: step 2721, loss 0.0321588, acc 1
2016-09-07T20:35:40.519940: step 2722, loss 0.0788704, acc 0.96
2016-09-07T20:35:41.191898: step 2723, loss 0.0138494, acc 1
2016-09-07T20:35:41.847881: step 2724, loss 0.059136, acc 0.98
2016-09-07T20:35:42.530758: step 2725, loss 0.0195863, acc 1
2016-09-07T20:35:43.195129: step 2726, loss 0.0149973, acc 1
2016-09-07T20:35:43.864093: step 2727, loss 0.0266456, acc 1
2016-09-07T20:35:44.573364: step 2728, loss 0.0495071, acc 0.96
2016-09-07T20:35:45.308803: step 2729, loss 0.0669222, acc 0.96
2016-09-07T20:35:45.977855: step 2730, loss 0.0177052, acc 1
2016-09-07T20:35:46.645687: step 2731, loss 0.0210021, acc 1
2016-09-07T20:35:47.299022: step 2732, loss 0.0681327, acc 0.98
2016-09-07T20:35:47.983630: step 2733, loss 0.0289821, acc 1
2016-09-07T20:35:48.666534: step 2734, loss 0.0486766, acc 0.96
2016-09-07T20:35:49.337450: step 2735, loss 0.0728923, acc 0.94
2016-09-07T20:35:49.994213: step 2736, loss 0.164799, acc 0.94
2016-09-07T20:35:50.704582: step 2737, loss 0.0213352, acc 1
2016-09-07T20:35:51.385788: step 2738, loss 0.0207791, acc 1
2016-09-07T20:35:52.053446: step 2739, loss 0.0201554, acc 1
2016-09-07T20:35:52.714059: step 2740, loss 0.0807252, acc 0.98
2016-09-07T20:35:53.379486: step 2741, loss 0.00918081, acc 1
2016-09-07T20:35:54.054039: step 2742, loss 0.0461117, acc 0.98
2016-09-07T20:35:54.715766: step 2743, loss 0.063251, acc 0.96
2016-09-07T20:35:55.399678: step 2744, loss 0.0104543, acc 1
2016-09-07T20:35:56.065744: step 2745, loss 0.0219585, acc 1
2016-09-07T20:35:56.722219: step 2746, loss 0.129916, acc 0.96
2016-09-07T20:35:57.386770: step 2747, loss 0.0339651, acc 0.98
2016-09-07T20:35:58.062481: step 2748, loss 0.144847, acc 0.9
2016-09-07T20:35:58.727984: step 2749, loss 0.0315656, acc 0.98
2016-09-07T20:35:59.403871: step 2750, loss 0.129524, acc 0.96
2016-09-07T20:36:00.071464: step 2751, loss 0.0570045, acc 0.96
2016-09-07T20:36:00.794086: step 2752, loss 0.0300739, acc 1
2016-09-07T20:36:01.449287: step 2753, loss 0.018723, acc 1
2016-09-07T20:36:02.117149: step 2754, loss 0.0247717, acc 1
2016-09-07T20:36:02.778976: step 2755, loss 0.206106, acc 0.94
2016-09-07T20:36:03.453088: step 2756, loss 0.00535518, acc 1
2016-09-07T20:36:04.122053: step 2757, loss 0.0734536, acc 0.96
2016-09-07T20:36:04.803307: step 2758, loss 0.0583457, acc 0.98
2016-09-07T20:36:05.490285: step 2759, loss 0.202385, acc 0.96
2016-09-07T20:36:06.162000: step 2760, loss 0.10156, acc 0.96
2016-09-07T20:36:06.833778: step 2761, loss 0.0337027, acc 1
2016-09-07T20:36:07.517330: step 2762, loss 0.139245, acc 0.94
2016-09-07T20:36:08.193127: step 2763, loss 0.0455865, acc 1
2016-09-07T20:36:08.870550: step 2764, loss 0.051739, acc 0.96
2016-09-07T20:36:09.545917: step 2765, loss 0.0254575, acc 1
2016-09-07T20:36:10.224925: step 2766, loss 0.0406575, acc 0.98
2016-09-07T20:36:10.909739: step 2767, loss 0.0231426, acc 1
2016-09-07T20:36:11.577766: step 2768, loss 0.0316091, acc 0.98
2016-09-07T20:36:12.252873: step 2769, loss 0.0529202, acc 0.98
2016-09-07T20:36:12.914729: step 2770, loss 0.0347326, acc 0.98
2016-09-07T20:36:13.601412: step 2771, loss 0.0221914, acc 1
2016-09-07T20:36:14.293967: step 2772, loss 0.0328678, acc 1
2016-09-07T20:36:14.965306: step 2773, loss 0.0579145, acc 0.98
2016-09-07T20:36:15.622430: step 2774, loss 0.00896865, acc 1
2016-09-07T20:36:16.291424: step 2775, loss 0.0314075, acc 0.98
2016-09-07T20:36:16.961082: step 2776, loss 0.0235682, acc 1
2016-09-07T20:36:17.620963: step 2777, loss 0.0851084, acc 0.98
2016-09-07T20:36:18.291037: step 2778, loss 0.0278032, acc 1
2016-09-07T20:36:18.953480: step 2779, loss 0.0656915, acc 0.98
2016-09-07T20:36:19.634173: step 2780, loss 0.105399, acc 0.92
2016-09-07T20:36:20.316415: step 2781, loss 0.0641709, acc 0.96
2016-09-07T20:36:20.994156: step 2782, loss 0.0534155, acc 0.96
2016-09-07T20:36:21.669158: step 2783, loss 0.0973196, acc 0.98
2016-09-07T20:36:22.332096: step 2784, loss 0.12213, acc 0.96
2016-09-07T20:36:22.992040: step 2785, loss 0.00797226, acc 1
2016-09-07T20:36:23.661337: step 2786, loss 0.127565, acc 0.94
2016-09-07T20:36:24.341930: step 2787, loss 0.0206473, acc 1
2016-09-07T20:36:25.003072: step 2788, loss 0.0172159, acc 1
2016-09-07T20:36:25.684468: step 2789, loss 0.00831163, acc 1
2016-09-07T20:36:26.368746: step 2790, loss 0.155956, acc 0.98
2016-09-07T20:36:27.024367: step 2791, loss 0.0429289, acc 1
2016-09-07T20:36:27.692737: step 2792, loss 0.0592014, acc 0.96
2016-09-07T20:36:28.364186: step 2793, loss 0.0344936, acc 1
2016-09-07T20:36:29.055596: step 2794, loss 0.0178882, acc 1
2016-09-07T20:36:29.728321: step 2795, loss 0.0576007, acc 0.96
2016-09-07T20:36:30.397349: step 2796, loss 0.0492141, acc 1
2016-09-07T20:36:31.076240: step 2797, loss 0.0279674, acc 1
2016-09-07T20:36:31.752573: step 2798, loss 0.0470642, acc 0.98
2016-09-07T20:36:32.405754: step 2799, loss 0.115887, acc 0.92
2016-09-07T20:36:33.058899: step 2800, loss 0.0221758, acc 1

Evaluation:
2016-09-07T20:36:35.944389: step 2800, loss 1.416, acc 0.755

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-2800

2016-09-07T20:36:37.789689: step 2801, loss 0.0512242, acc 0.96
2016-09-07T20:36:38.461849: step 2802, loss 0.0412576, acc 0.98
2016-09-07T20:36:39.134334: step 2803, loss 0.0296295, acc 0.98
2016-09-07T20:36:39.803186: step 2804, loss 0.0226809, acc 1
2016-09-07T20:36:40.482452: step 2805, loss 0.044081, acc 0.96
2016-09-07T20:36:41.143062: step 2806, loss 0.065373, acc 0.98
2016-09-07T20:36:41.805468: step 2807, loss 0.0787401, acc 0.96
2016-09-07T20:36:42.481263: step 2808, loss 0.061416, acc 0.98
2016-09-07T20:36:43.167169: step 2809, loss 0.0345608, acc 1
2016-09-07T20:36:43.834603: step 2810, loss 0.0346062, acc 0.98
2016-09-07T20:36:44.518042: step 2811, loss 0.0497481, acc 0.98
2016-09-07T20:36:45.198765: step 2812, loss 0.0244885, acc 1
2016-09-07T20:36:45.883589: step 2813, loss 0.0217589, acc 0.98
2016-09-07T20:36:46.559870: step 2814, loss 0.0298596, acc 0.98
2016-09-07T20:36:47.236986: step 2815, loss 0.0622566, acc 0.96
2016-09-07T20:36:47.909417: step 2816, loss 0.06877, acc 0.96
2016-09-07T20:36:48.576544: step 2817, loss 0.018282, acc 1
2016-09-07T20:36:49.262903: step 2818, loss 0.133813, acc 0.96
2016-09-07T20:36:49.917347: step 2819, loss 0.0520418, acc 0.96
2016-09-07T20:36:50.579573: step 2820, loss 0.0240606, acc 1
2016-09-07T20:36:51.250513: step 2821, loss 0.0212412, acc 1
2016-09-07T20:36:51.911203: step 2822, loss 0.102117, acc 0.96
2016-09-07T20:36:52.582297: step 2823, loss 0.0124218, acc 1
2016-09-07T20:36:53.263352: step 2824, loss 0.0205502, acc 0.98
2016-09-07T20:36:53.945378: step 2825, loss 0.0614011, acc 0.98
2016-09-07T20:36:54.605085: step 2826, loss 0.0237471, acc 1
2016-09-07T20:36:55.275205: step 2827, loss 0.0389183, acc 1
2016-09-07T20:36:55.939564: step 2828, loss 0.0243692, acc 0.98
2016-09-07T20:36:56.624455: step 2829, loss 0.0403292, acc 0.98
2016-09-07T20:36:57.291249: step 2830, loss 0.0458314, acc 1
2016-09-07T20:36:57.965791: step 2831, loss 0.0773154, acc 0.98
2016-09-07T20:36:58.636041: step 2832, loss 0.0719936, acc 0.98
2016-09-07T20:36:59.312041: step 2833, loss 0.0169197, acc 1
2016-09-07T20:36:59.967581: step 2834, loss 0.0152456, acc 1
2016-09-07T20:37:00.683899: step 2835, loss 0.0249989, acc 0.98
2016-09-07T20:37:01.365090: step 2836, loss 0.102343, acc 0.96
2016-09-07T20:37:02.022819: step 2837, loss 0.171162, acc 0.94
2016-09-07T20:37:02.695284: step 2838, loss 0.0356374, acc 1
2016-09-07T20:37:03.369394: step 2839, loss 0.00958661, acc 1
2016-09-07T20:37:04.040388: step 2840, loss 0.0273532, acc 1
2016-09-07T20:37:04.718822: step 2841, loss 0.115133, acc 0.94
2016-09-07T20:37:05.404073: step 2842, loss 0.0340802, acc 0.98
2016-09-07T20:37:06.086050: step 2843, loss 0.0329185, acc 1
2016-09-07T20:37:06.747471: step 2844, loss 0.0147226, acc 1
2016-09-07T20:37:07.402600: step 2845, loss 0.0650885, acc 0.98
2016-09-07T20:37:08.068206: step 2846, loss 0.0224121, acc 1
2016-09-07T20:37:08.734196: step 2847, loss 0.0351719, acc 0.98
2016-09-07T20:37:09.401032: step 2848, loss 0.0460005, acc 0.98
2016-09-07T20:37:10.068104: step 2849, loss 0.010763, acc 1
2016-09-07T20:37:10.730041: step 2850, loss 0.0328723, acc 0.98
2016-09-07T20:37:11.407644: step 2851, loss 0.0353897, acc 0.98
2016-09-07T20:37:12.090162: step 2852, loss 0.0349196, acc 0.98
2016-09-07T20:37:12.755473: step 2853, loss 0.0218475, acc 1
2016-09-07T20:37:13.410525: step 2854, loss 0.00888389, acc 1
2016-09-07T20:37:14.100744: step 2855, loss 0.0874705, acc 0.96
2016-09-07T20:37:14.778219: step 2856, loss 0.0288931, acc 1
2016-09-07T20:37:15.453610: step 2857, loss 0.0451733, acc 0.98
2016-09-07T20:37:16.136470: step 2858, loss 0.0597361, acc 0.96
2016-09-07T20:37:16.816338: step 2859, loss 0.0712831, acc 0.96
2016-09-07T20:37:17.516883: step 2860, loss 0.00812124, acc 1
2016-09-07T20:37:18.196275: step 2861, loss 0.0230368, acc 1
2016-09-07T20:37:18.862615: step 2862, loss 0.0166215, acc 1
2016-09-07T20:37:19.559397: step 2863, loss 0.0878506, acc 0.96
2016-09-07T20:37:20.238841: step 2864, loss 0.0122039, acc 1
2016-09-07T20:37:20.930573: step 2865, loss 0.0159323, acc 1
2016-09-07T20:37:21.607041: step 2866, loss 0.00717545, acc 1
2016-09-07T20:37:22.284064: step 2867, loss 0.0824554, acc 0.92
2016-09-07T20:37:22.952064: step 2868, loss 0.0932791, acc 0.94
2016-09-07T20:37:23.618200: step 2869, loss 0.0149355, acc 1
2016-09-07T20:37:24.287001: step 2870, loss 0.114536, acc 0.98
2016-09-07T20:37:24.957688: step 2871, loss 0.0371684, acc 0.98
2016-09-07T20:37:25.636338: step 2872, loss 0.0047909, acc 1
2016-09-07T20:37:26.310255: step 2873, loss 0.0127879, acc 1
2016-09-07T20:37:26.971765: step 2874, loss 0.0464299, acc 0.98
2016-09-07T20:37:27.663354: step 2875, loss 0.0050671, acc 1
2016-09-07T20:37:28.342588: step 2876, loss 0.0544994, acc 0.96
2016-09-07T20:37:29.019561: step 2877, loss 0.0740248, acc 0.98
2016-09-07T20:37:29.687748: step 2878, loss 0.0597581, acc 0.96
2016-09-07T20:37:30.371119: step 2879, loss 0.00414712, acc 1
2016-09-07T20:37:31.053818: step 2880, loss 0.0557896, acc 0.96
2016-09-07T20:37:31.727754: step 2881, loss 0.00639824, acc 1
2016-09-07T20:37:32.397690: step 2882, loss 0.0283727, acc 1
2016-09-07T20:37:33.064260: step 2883, loss 0.0112056, acc 1
2016-09-07T20:37:33.743699: step 2884, loss 0.0190182, acc 1
2016-09-07T20:37:34.427719: step 2885, loss 0.100569, acc 0.94
2016-09-07T20:37:35.078605: step 2886, loss 0.110611, acc 0.94
2016-09-07T20:37:35.741388: step 2887, loss 0.0669849, acc 0.96
2016-09-07T20:37:36.395770: step 2888, loss 0.0303729, acc 1
2016-09-07T20:37:37.060145: step 2889, loss 0.0219008, acc 1
2016-09-07T20:37:37.722105: step 2890, loss 0.027808, acc 0.98
2016-09-07T20:37:38.384675: step 2891, loss 0.0242488, acc 0.98
2016-09-07T20:37:39.040117: step 2892, loss 0.0466624, acc 0.98
2016-09-07T20:37:39.709806: step 2893, loss 0.00420373, acc 1
2016-09-07T20:37:40.378460: step 2894, loss 0.00968613, acc 1
2016-09-07T20:37:41.058834: step 2895, loss 0.0859435, acc 0.96
2016-09-07T20:37:41.729405: step 2896, loss 0.0861971, acc 0.96
2016-09-07T20:37:42.393334: step 2897, loss 0.115867, acc 0.94
2016-09-07T20:37:43.057956: step 2898, loss 0.0910231, acc 0.98
2016-09-07T20:37:43.721730: step 2899, loss 0.00439623, acc 1
2016-09-07T20:37:44.381706: step 2900, loss 0.0536865, acc 0.96

Evaluation:
2016-09-07T20:37:47.282508: step 2900, loss 1.54943, acc 0.751

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-2900

2016-09-07T20:37:48.994155: step 2901, loss 0.0288436, acc 0.98
2016-09-07T20:37:49.657805: step 2902, loss 0.204626, acc 0.9
2016-09-07T20:37:50.323221: step 2903, loss 0.082184, acc 0.96
2016-09-07T20:37:51.000832: step 2904, loss 0.0326146, acc 1
2016-09-07T20:37:51.685679: step 2905, loss 0.109786, acc 0.96
2016-09-07T20:37:52.370577: step 2906, loss 0.036645, acc 1
2016-09-07T20:37:53.033637: step 2907, loss 0.0480632, acc 0.98
2016-09-07T20:37:53.699820: step 2908, loss 0.0331684, acc 1
2016-09-07T20:37:54.379603: step 2909, loss 0.0345117, acc 1
2016-09-07T20:37:54.737922: step 2910, loss 0.10199, acc 0.916667
2016-09-07T20:37:55.412627: step 2911, loss 0.0346484, acc 0.98
2016-09-07T20:37:56.092554: step 2912, loss 0.0304293, acc 0.98
2016-09-07T20:37:56.761086: step 2913, loss 0.0666162, acc 0.96
2016-09-07T20:37:57.452964: step 2914, loss 0.033433, acc 1
2016-09-07T20:37:58.124587: step 2915, loss 0.0504007, acc 1
2016-09-07T20:37:58.791914: step 2916, loss 0.0446035, acc 0.96
2016-09-07T20:37:59.464865: step 2917, loss 0.0446237, acc 0.98
2016-09-07T20:38:00.140403: step 2918, loss 0.0582763, acc 0.98
2016-09-07T20:38:00.866140: step 2919, loss 0.191231, acc 0.96
2016-09-07T20:38:01.537015: step 2920, loss 0.0597082, acc 0.98
2016-09-07T20:38:02.223535: step 2921, loss 0.0105017, acc 1
2016-09-07T20:38:02.887043: step 2922, loss 0.0565527, acc 0.98
2016-09-07T20:38:03.558692: step 2923, loss 0.0644285, acc 0.96
2016-09-07T20:38:04.212339: step 2924, loss 0.0561131, acc 0.98
2016-09-07T20:38:04.886355: step 2925, loss 0.0267508, acc 0.98
2016-09-07T20:38:05.551487: step 2926, loss 0.0226477, acc 0.98
2016-09-07T20:38:06.249023: step 2927, loss 0.145433, acc 0.96
2016-09-07T20:38:06.945386: step 2928, loss 0.0677987, acc 0.98
2016-09-07T20:38:07.615780: step 2929, loss 0.0372431, acc 0.96
2016-09-07T20:38:08.289440: step 2930, loss 0.062758, acc 0.96
2016-09-07T20:38:08.950148: step 2931, loss 0.0281213, acc 0.98
2016-09-07T20:38:09.631203: step 2932, loss 0.0304296, acc 1
2016-09-07T20:38:10.325564: step 2933, loss 0.0369846, acc 0.98
2016-09-07T20:38:11.002680: step 2934, loss 0.0369263, acc 0.98
2016-09-07T20:38:11.654757: step 2935, loss 0.0313492, acc 0.98
2016-09-07T20:38:12.315792: step 2936, loss 0.0205569, acc 0.98
2016-09-07T20:38:12.984567: step 2937, loss 0.0259046, acc 0.98
2016-09-07T20:38:13.655540: step 2938, loss 0.0678974, acc 0.96
2016-09-07T20:38:14.328101: step 2939, loss 0.0212063, acc 1
2016-09-07T20:38:14.987906: step 2940, loss 0.0244885, acc 1
2016-09-07T20:38:15.654474: step 2941, loss 0.0276051, acc 0.98
2016-09-07T20:38:16.312893: step 2942, loss 0.0536345, acc 0.96
2016-09-07T20:38:16.979639: step 2943, loss 0.0342063, acc 0.98
2016-09-07T20:38:17.651552: step 2944, loss 0.0158355, acc 1
2016-09-07T20:38:18.328417: step 2945, loss 0.0172002, acc 1
2016-09-07T20:38:19.015343: step 2946, loss 0.0468705, acc 1
2016-09-07T20:38:19.686365: step 2947, loss 0.022213, acc 1
2016-09-07T20:38:20.356195: step 2948, loss 0.00761971, acc 1
2016-09-07T20:38:21.013350: step 2949, loss 0.0391783, acc 0.98
2016-09-07T20:38:21.684508: step 2950, loss 0.0706199, acc 0.98
2016-09-07T20:38:22.356901: step 2951, loss 0.0663595, acc 0.96
2016-09-07T20:38:23.022361: step 2952, loss 0.00567632, acc 1
2016-09-07T20:38:23.670631: step 2953, loss 0.00736154, acc 1
2016-09-07T20:38:24.341577: step 2954, loss 0.134474, acc 0.94
2016-09-07T20:38:25.008640: step 2955, loss 0.0686867, acc 0.94
2016-09-07T20:38:25.697635: step 2956, loss 0.0594774, acc 0.98
2016-09-07T20:38:26.362527: step 2957, loss 0.0303263, acc 0.98
2016-09-07T20:38:27.037448: step 2958, loss 0.0273041, acc 0.98
2016-09-07T20:38:27.696077: step 2959, loss 0.102849, acc 0.94
2016-09-07T20:38:28.372050: step 2960, loss 0.0441371, acc 0.98
2016-09-07T20:38:29.058874: step 2961, loss 0.0409928, acc 0.98
2016-09-07T20:38:29.729946: step 2962, loss 0.00749808, acc 1
2016-09-07T20:38:30.400416: step 2963, loss 0.0145115, acc 1
2016-09-07T20:38:31.070744: step 2964, loss 0.0600661, acc 0.96
2016-09-07T20:38:31.724779: step 2965, loss 0.0199653, acc 1
2016-09-07T20:38:32.387748: step 2966, loss 0.0346453, acc 0.98
2016-09-07T20:38:33.069334: step 2967, loss 0.0119085, acc 1
2016-09-07T20:38:33.743953: step 2968, loss 0.00657387, acc 1
2016-09-07T20:38:34.412369: step 2969, loss 0.0134683, acc 1
2016-09-07T20:38:35.098311: step 2970, loss 0.0512473, acc 0.98
2016-09-07T20:38:35.760917: step 2971, loss 0.0765506, acc 0.98
2016-09-07T20:38:36.434863: step 2972, loss 0.0288562, acc 1
2016-09-07T20:38:37.105251: step 2973, loss 0.0333189, acc 1
2016-09-07T20:38:37.782851: step 2974, loss 0.0423942, acc 0.98
2016-09-07T20:38:38.442251: step 2975, loss 0.125575, acc 0.98
2016-09-07T20:38:39.110059: step 2976, loss 0.132866, acc 0.94
2016-09-07T20:38:39.777393: step 2977, loss 0.0182784, acc 1
2016-09-07T20:38:40.438644: step 2978, loss 0.0244676, acc 0.98
2016-09-07T20:38:41.104324: step 2979, loss 0.0513303, acc 0.98
2016-09-07T20:38:41.786782: step 2980, loss 0.0765196, acc 0.96
2016-09-07T20:38:42.455181: step 2981, loss 0.0089734, acc 1
2016-09-07T20:38:43.120583: step 2982, loss 0.0831171, acc 0.94
2016-09-07T20:38:43.793818: step 2983, loss 0.0380027, acc 0.98
2016-09-07T20:38:44.480376: step 2984, loss 0.0439318, acc 0.98
2016-09-07T20:38:45.160439: step 2985, loss 0.0290259, acc 1
2016-09-07T20:38:45.817527: step 2986, loss 0.0989018, acc 0.94
2016-09-07T20:38:46.479840: step 2987, loss 0.0256354, acc 0.98
2016-09-07T20:38:47.155407: step 2988, loss 0.00373285, acc 1
2016-09-07T20:38:47.839316: step 2989, loss 0.124889, acc 0.98
2016-09-07T20:38:48.522100: step 2990, loss 0.0316554, acc 0.98
2016-09-07T20:38:49.199884: step 2991, loss 0.0186722, acc 1
2016-09-07T20:38:49.892024: step 2992, loss 0.0214293, acc 1
2016-09-07T20:38:50.560689: step 2993, loss 0.0242503, acc 0.98
2016-09-07T20:38:51.223383: step 2994, loss 0.0375625, acc 1
2016-09-07T20:38:51.894745: step 2995, loss 0.00763291, acc 1
2016-09-07T20:38:52.567608: step 2996, loss 0.0267084, acc 1
2016-09-07T20:38:53.242738: step 2997, loss 0.0210782, acc 1
2016-09-07T20:38:53.915821: step 2998, loss 0.0290425, acc 0.98
2016-09-07T20:38:54.594194: step 2999, loss 0.033783, acc 1
2016-09-07T20:38:55.258008: step 3000, loss 0.0314314, acc 0.98

Evaluation:
2016-09-07T20:38:58.142763: step 3000, loss 1.62804, acc 0.741

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-3000

2016-09-07T20:38:59.727060: step 3001, loss 0.0248496, acc 1
2016-09-07T20:39:00.432982: step 3002, loss 0.0389105, acc 1
2016-09-07T20:39:01.108391: step 3003, loss 0.00711286, acc 1
2016-09-07T20:39:01.777766: step 3004, loss 0.062239, acc 0.96
2016-09-07T20:39:02.431405: step 3005, loss 0.0222537, acc 1
2016-09-07T20:39:03.089040: step 3006, loss 0.0634357, acc 0.98
2016-09-07T20:39:03.762338: step 3007, loss 0.0397605, acc 1
2016-09-07T20:39:04.432172: step 3008, loss 0.0651455, acc 0.94
2016-09-07T20:39:05.105701: step 3009, loss 0.121573, acc 0.96
2016-09-07T20:39:05.753595: step 3010, loss 0.0314724, acc 0.98
2016-09-07T20:39:06.417347: step 3011, loss 0.0282491, acc 1
2016-09-07T20:39:07.086479: step 3012, loss 0.178104, acc 0.96
2016-09-07T20:39:07.767011: step 3013, loss 0.0123966, acc 1
2016-09-07T20:39:08.451578: step 3014, loss 0.00618293, acc 1
2016-09-07T20:39:09.102323: step 3015, loss 0.021266, acc 1
2016-09-07T20:39:09.783652: step 3016, loss 0.0276163, acc 1
2016-09-07T20:39:10.464781: step 3017, loss 0.0518435, acc 0.98
2016-09-07T20:39:11.140113: step 3018, loss 0.0115748, acc 1
2016-09-07T20:39:11.833976: step 3019, loss 0.143479, acc 0.92
2016-09-07T20:39:12.510993: step 3020, loss 0.0225931, acc 1
2016-09-07T20:39:13.172990: step 3021, loss 0.0483919, acc 1
2016-09-07T20:39:13.849318: step 3022, loss 0.0268623, acc 0.98
2016-09-07T20:39:14.525245: step 3023, loss 0.00549088, acc 1
2016-09-07T20:39:15.193845: step 3024, loss 0.0624898, acc 0.98
2016-09-07T20:39:15.847890: step 3025, loss 0.00603619, acc 1
2016-09-07T20:39:16.506377: step 3026, loss 0.0199077, acc 1
2016-09-07T20:39:17.156695: step 3027, loss 0.0727553, acc 0.98
2016-09-07T20:39:17.833458: step 3028, loss 0.0491611, acc 0.96
2016-09-07T20:39:18.511546: step 3029, loss 0.0176456, acc 1
2016-09-07T20:39:19.199020: step 3030, loss 0.0331905, acc 0.98
2016-09-07T20:39:19.867863: step 3031, loss 0.0220115, acc 1
2016-09-07T20:39:20.541015: step 3032, loss 0.0137147, acc 1
2016-09-07T20:39:21.197566: step 3033, loss 0.0183269, acc 1
2016-09-07T20:39:21.876209: step 3034, loss 0.0233898, acc 0.98
2016-09-07T20:39:22.552863: step 3035, loss 0.0586489, acc 0.96
2016-09-07T20:39:23.215623: step 3036, loss 0.0401701, acc 0.98
2016-09-07T20:39:23.882571: step 3037, loss 0.0387516, acc 0.98
2016-09-07T20:39:24.531746: step 3038, loss 0.0349326, acc 0.98
2016-09-07T20:39:25.187311: step 3039, loss 0.00977226, acc 1
2016-09-07T20:39:25.847651: step 3040, loss 0.0489893, acc 0.96
2016-09-07T20:39:26.518059: step 3041, loss 0.0336573, acc 1
2016-09-07T20:39:27.193583: step 3042, loss 0.0318957, acc 0.98
2016-09-07T20:39:27.869462: step 3043, loss 0.0234107, acc 1
2016-09-07T20:39:28.538793: step 3044, loss 0.0200178, acc 1
2016-09-07T20:39:29.207026: step 3045, loss 0.07744, acc 0.98
2016-09-07T20:39:29.881965: step 3046, loss 0.035977, acc 0.98
2016-09-07T20:39:30.558298: step 3047, loss 0.0833187, acc 0.96
2016-09-07T20:39:31.211635: step 3048, loss 0.00481955, acc 1
2016-09-07T20:39:31.882955: step 3049, loss 0.0600525, acc 0.96
2016-09-07T20:39:32.544487: step 3050, loss 0.0789285, acc 0.96
2016-09-07T20:39:33.224988: step 3051, loss 0.033915, acc 1
2016-09-07T20:39:33.901824: step 3052, loss 0.0300865, acc 0.98
2016-09-07T20:39:34.565234: step 3053, loss 0.0392736, acc 0.98
2016-09-07T20:39:35.238328: step 3054, loss 0.063506, acc 0.98
2016-09-07T20:39:35.933667: step 3055, loss 0.0206785, acc 0.98
2016-09-07T20:39:36.586022: step 3056, loss 0.0251662, acc 0.98
2016-09-07T20:39:37.249011: step 3057, loss 0.0493112, acc 0.98
2016-09-07T20:39:37.916555: step 3058, loss 0.0414031, acc 0.98
2016-09-07T20:39:38.579215: step 3059, loss 0.0457414, acc 0.98
2016-09-07T20:39:39.249412: step 3060, loss 0.0596327, acc 0.98
2016-09-07T20:39:39.934429: step 3061, loss 0.0492505, acc 0.96
2016-09-07T20:39:40.615460: step 3062, loss 0.0666785, acc 0.98
2016-09-07T20:39:41.299288: step 3063, loss 0.0801084, acc 0.96
2016-09-07T20:39:41.980269: step 3064, loss 0.0318822, acc 0.98
2016-09-07T20:39:42.653215: step 3065, loss 0.0282325, acc 1
2016-09-07T20:39:43.312853: step 3066, loss 0.0212027, acc 0.98
2016-09-07T20:39:43.984576: step 3067, loss 0.0198569, acc 1
2016-09-07T20:39:44.651316: step 3068, loss 0.145806, acc 0.98
2016-09-07T20:39:45.344217: step 3069, loss 0.107636, acc 0.98
2016-09-07T20:39:46.016324: step 3070, loss 0.0428907, acc 0.98
2016-09-07T20:39:46.687494: step 3071, loss 0.115048, acc 0.98
2016-09-07T20:39:47.358995: step 3072, loss 0.0312096, acc 0.98
2016-09-07T20:39:48.011760: step 3073, loss 0.00728128, acc 1
2016-09-07T20:39:48.689716: step 3074, loss 0.0158282, acc 1
2016-09-07T20:39:49.353734: step 3075, loss 0.0419362, acc 0.98
2016-09-07T20:39:50.015277: step 3076, loss 0.0617764, acc 0.96
2016-09-07T20:39:50.679332: step 3077, loss 0.00728392, acc 1
2016-09-07T20:39:51.344889: step 3078, loss 0.0382753, acc 0.98
2016-09-07T20:39:52.050229: step 3079, loss 0.0229232, acc 1
2016-09-07T20:39:52.719459: step 3080, loss 0.0628182, acc 0.96
2016-09-07T20:39:53.383397: step 3081, loss 0.0388182, acc 1
2016-09-07T20:39:54.055186: step 3082, loss 0.044599, acc 0.96
2016-09-07T20:39:54.725228: step 3083, loss 0.0258335, acc 0.98
2016-09-07T20:39:55.378793: step 3084, loss 0.0305897, acc 0.98
2016-09-07T20:39:56.057882: step 3085, loss 0.0332667, acc 1
2016-09-07T20:39:56.751051: step 3086, loss 0.0130042, acc 1
2016-09-07T20:39:57.442398: step 3087, loss 0.0204541, acc 1
2016-09-07T20:39:58.120573: step 3088, loss 0.0284514, acc 0.98
2016-09-07T20:39:58.801426: step 3089, loss 0.15395, acc 0.96
2016-09-07T20:39:59.485235: step 3090, loss 0.0232071, acc 0.98
2016-09-07T20:40:00.148413: step 3091, loss 0.00870858, acc 1
2016-09-07T20:40:00.839798: step 3092, loss 0.00611547, acc 1
2016-09-07T20:40:01.507004: step 3093, loss 0.0117578, acc 1
2016-09-07T20:40:02.149312: step 3094, loss 0.0164824, acc 1
2016-09-07T20:40:02.809379: step 3095, loss 0.110958, acc 0.96
2016-09-07T20:40:03.457781: step 3096, loss 0.0230748, acc 0.98
2016-09-07T20:40:04.133270: step 3097, loss 0.0495636, acc 0.98
2016-09-07T20:40:04.795535: step 3098, loss 0.0945835, acc 0.98
2016-09-07T20:40:05.446656: step 3099, loss 0.0230602, acc 1
2016-09-07T20:40:06.113361: step 3100, loss 0.042312, acc 0.98

Evaluation:
2016-09-07T20:40:08.977853: step 3100, loss 1.56377, acc 0.743

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-3100

2016-09-07T20:40:10.617450: step 3101, loss 0.138034, acc 0.94
2016-09-07T20:40:11.294860: step 3102, loss 0.0198209, acc 0.98
2016-09-07T20:40:11.970403: step 3103, loss 0.00376407, acc 1
2016-09-07T20:40:12.335760: step 3104, loss 0.00657282, acc 1
2016-09-07T20:40:13.013667: step 3105, loss 0.0543464, acc 1
2016-09-07T20:40:13.685726: step 3106, loss 0.0566018, acc 0.98
2016-09-07T20:40:14.353083: step 3107, loss 0.0498447, acc 1
2016-09-07T20:40:15.009108: step 3108, loss 0.0575984, acc 1
2016-09-07T20:40:15.693520: step 3109, loss 0.0529075, acc 0.98
2016-09-07T20:40:16.367127: step 3110, loss 0.0929501, acc 0.96
2016-09-07T20:40:17.049002: step 3111, loss 0.00607403, acc 1
2016-09-07T20:40:17.736074: step 3112, loss 0.0871812, acc 0.96
2016-09-07T20:40:18.398273: step 3113, loss 0.029243, acc 0.98
2016-09-07T20:40:19.052725: step 3114, loss 0.0395228, acc 1
2016-09-07T20:40:19.728403: step 3115, loss 0.00715701, acc 1
2016-09-07T20:40:20.394673: step 3116, loss 0.0107508, acc 1
2016-09-07T20:40:21.076038: step 3117, loss 0.0300844, acc 1
2016-09-07T20:40:21.769214: step 3118, loss 0.0483095, acc 0.96
2016-09-07T20:40:22.430325: step 3119, loss 0.0713476, acc 0.96
2016-09-07T20:40:23.095311: step 3120, loss 0.135905, acc 0.96
2016-09-07T20:40:23.785710: step 3121, loss 0.117635, acc 0.96
2016-09-07T20:40:24.455744: step 3122, loss 0.0493783, acc 0.98
2016-09-07T20:40:25.160376: step 3123, loss 0.0795762, acc 0.94
2016-09-07T20:40:25.853446: step 3124, loss 0.00533047, acc 1
2016-09-07T20:40:26.511463: step 3125, loss 0.0845626, acc 0.92
2016-09-07T20:40:27.171953: step 3126, loss 0.0413793, acc 0.98
2016-09-07T20:40:27.850865: step 3127, loss 0.0298524, acc 0.98
2016-09-07T20:40:28.535385: step 3128, loss 0.0315115, acc 0.98
2016-09-07T20:40:29.192601: step 3129, loss 0.023756, acc 1
2016-09-07T20:40:29.865302: step 3130, loss 0.0296752, acc 0.98
2016-09-07T20:40:30.545096: step 3131, loss 0.0111351, acc 1
2016-09-07T20:40:31.208509: step 3132, loss 0.018877, acc 1
2016-09-07T20:40:31.895104: step 3133, loss 0.0339854, acc 0.98
2016-09-07T20:40:32.558688: step 3134, loss 0.027545, acc 1
2016-09-07T20:40:33.233839: step 3135, loss 0.0511584, acc 0.98
2016-09-07T20:40:33.914252: step 3136, loss 0.0653167, acc 0.96
2016-09-07T20:40:34.590391: step 3137, loss 0.0365746, acc 0.98
2016-09-07T20:40:35.249042: step 3138, loss 0.0268653, acc 0.98
2016-09-07T20:40:35.943891: step 3139, loss 0.0212354, acc 1
2016-09-07T20:40:36.631530: step 3140, loss 0.138006, acc 0.94
2016-09-07T20:40:37.314428: step 3141, loss 0.0706061, acc 0.98
2016-09-07T20:40:37.969461: step 3142, loss 0.0432597, acc 0.98
2016-09-07T20:40:38.633379: step 3143, loss 0.0361301, acc 0.98
2016-09-07T20:40:39.295191: step 3144, loss 0.00323965, acc 1
2016-09-07T20:40:39.964572: step 3145, loss 0.122916, acc 0.98
2016-09-07T20:40:40.624214: step 3146, loss 0.0274591, acc 1
2016-09-07T20:40:41.284771: step 3147, loss 0.00635764, acc 1
2016-09-07T20:40:41.982903: step 3148, loss 0.0262184, acc 1
2016-09-07T20:40:42.655399: step 3149, loss 0.0771306, acc 0.98
2016-09-07T20:40:43.327964: step 3150, loss 0.103444, acc 0.96
2016-09-07T20:40:44.000153: step 3151, loss 0.130251, acc 0.98
2016-09-07T20:40:44.675085: step 3152, loss 0.0320774, acc 1
2016-09-07T20:40:45.351684: step 3153, loss 0.0126174, acc 1
2016-09-07T20:40:46.044090: step 3154, loss 0.0621857, acc 0.96
2016-09-07T20:40:46.743640: step 3155, loss 0.0436476, acc 0.98
2016-09-07T20:40:47.410903: step 3156, loss 0.0178099, acc 1
2016-09-07T20:40:48.080196: step 3157, loss 0.0282132, acc 1
2016-09-07T20:40:48.767538: step 3158, loss 0.0486229, acc 0.98
2016-09-07T20:40:49.437091: step 3159, loss 0.0288841, acc 0.98
2016-09-07T20:40:50.086949: step 3160, loss 0.103831, acc 0.98
2016-09-07T20:40:50.775458: step 3161, loss 0.0440993, acc 0.98
2016-09-07T20:40:51.436422: step 3162, loss 0.0614065, acc 0.98
2016-09-07T20:40:52.107126: step 3163, loss 0.0212857, acc 1
2016-09-07T20:40:52.756116: step 3164, loss 0.0320434, acc 1
2016-09-07T20:40:53.425884: step 3165, loss 0.0276444, acc 1
2016-09-07T20:40:54.086895: step 3166, loss 0.0193599, acc 1
2016-09-07T20:40:54.747148: step 3167, loss 0.0792822, acc 0.94
2016-09-07T20:40:55.420702: step 3168, loss 0.022385, acc 1
2016-09-07T20:40:56.089686: step 3169, loss 0.0621711, acc 0.96
2016-09-07T20:40:56.790405: step 3170, loss 0.0664021, acc 0.94
2016-09-07T20:40:57.451053: step 3171, loss 0.0114583, acc 1
2016-09-07T20:40:58.114737: step 3172, loss 0.00616522, acc 1
2016-09-07T20:40:58.785430: step 3173, loss 0.137534, acc 0.96
2016-09-07T20:40:59.463015: step 3174, loss 0.0406716, acc 0.96
2016-09-07T20:41:00.122408: step 3175, loss 0.00772077, acc 1
2016-09-07T20:41:00.822483: step 3176, loss 0.0346594, acc 0.98
2016-09-07T20:41:01.503264: step 3177, loss 0.00646288, acc 1
2016-09-07T20:41:02.183334: step 3178, loss 0.00442063, acc 1
2016-09-07T20:41:02.867127: step 3179, loss 0.0113624, acc 1
2016-09-07T20:41:03.525103: step 3180, loss 0.0333242, acc 1
2016-09-07T20:41:04.195979: step 3181, loss 0.0137998, acc 1
2016-09-07T20:41:04.859518: step 3182, loss 0.00924857, acc 1
2016-09-07T20:41:05.553945: step 3183, loss 0.0206531, acc 1
2016-09-07T20:41:06.216053: step 3184, loss 0.0103394, acc 1
2016-09-07T20:41:06.896578: step 3185, loss 0.0372821, acc 0.98
2016-09-07T20:41:07.555791: step 3186, loss 0.0874726, acc 0.96
2016-09-07T20:41:08.215543: step 3187, loss 0.0371604, acc 1
2016-09-07T20:41:08.869882: step 3188, loss 0.0188313, acc 1
2016-09-07T20:41:09.527442: step 3189, loss 0.0319759, acc 0.98
2016-09-07T20:41:10.187172: step 3190, loss 0.0966746, acc 0.94
2016-09-07T20:41:10.848416: step 3191, loss 0.014092, acc 1
2016-09-07T20:41:11.524647: step 3192, loss 0.0232807, acc 1
2016-09-07T20:41:12.185107: step 3193, loss 0.00706061, acc 1
2016-09-07T20:41:12.860875: step 3194, loss 0.0762914, acc 0.98
2016-09-07T20:41:13.542088: step 3195, loss 0.0138176, acc 1
2016-09-07T20:41:14.207305: step 3196, loss 0.0673707, acc 0.96
2016-09-07T20:41:14.876242: step 3197, loss 0.0154805, acc 1
2016-09-07T20:41:15.535126: step 3198, loss 0.0110493, acc 1
2016-09-07T20:41:16.185072: step 3199, loss 0.0659072, acc 0.96
2016-09-07T20:41:16.862161: step 3200, loss 0.0732645, acc 0.98

Evaluation:
2016-09-07T20:41:19.767003: step 3200, loss 1.9964, acc 0.744

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-3200

2016-09-07T20:41:21.425697: step 3201, loss 0.0114167, acc 1
2016-09-07T20:41:22.120913: step 3202, loss 0.162666, acc 0.9
2016-09-07T20:41:22.787268: step 3203, loss 0.0118573, acc 1
2016-09-07T20:41:23.471839: step 3204, loss 0.0500648, acc 0.98
2016-09-07T20:41:24.144306: step 3205, loss 0.0737359, acc 0.94
2016-09-07T20:41:24.802335: step 3206, loss 0.0158014, acc 1
2016-09-07T20:41:25.467679: step 3207, loss 0.0942397, acc 0.94
2016-09-07T20:41:26.140969: step 3208, loss 0.03509, acc 0.98
2016-09-07T20:41:26.796924: step 3209, loss 0.0176834, acc 1
2016-09-07T20:41:27.454817: step 3210, loss 0.0185727, acc 1
2016-09-07T20:41:28.112221: step 3211, loss 0.0377302, acc 0.96
2016-09-07T20:41:28.770948: step 3212, loss 0.0545495, acc 0.98
2016-09-07T20:41:29.438182: step 3213, loss 0.031927, acc 1
2016-09-07T20:41:30.104084: step 3214, loss 0.0123473, acc 1
2016-09-07T20:41:30.771051: step 3215, loss 0.114646, acc 0.94
2016-09-07T20:41:31.430283: step 3216, loss 0.0163711, acc 1
2016-09-07T20:41:32.115158: step 3217, loss 0.036187, acc 1
2016-09-07T20:41:32.790371: step 3218, loss 0.040595, acc 0.98
2016-09-07T20:41:33.443942: step 3219, loss 0.0629723, acc 0.96
2016-09-07T20:41:34.100140: step 3220, loss 0.0224954, acc 1
2016-09-07T20:41:34.777459: step 3221, loss 0.0671586, acc 0.96
2016-09-07T20:41:35.452074: step 3222, loss 0.0160086, acc 1
2016-09-07T20:41:36.148392: step 3223, loss 0.132142, acc 0.98
2016-09-07T20:41:36.847544: step 3224, loss 0.0381525, acc 0.98
2016-09-07T20:41:37.519845: step 3225, loss 0.0649936, acc 0.96
2016-09-07T20:41:38.196378: step 3226, loss 0.0307586, acc 1
2016-09-07T20:41:38.875054: step 3227, loss 0.0212809, acc 1
2016-09-07T20:41:39.525888: step 3228, loss 0.0251219, acc 1
2016-09-07T20:41:40.200113: step 3229, loss 0.097366, acc 0.96
2016-09-07T20:41:40.867430: step 3230, loss 0.106501, acc 0.96
2016-09-07T20:41:41.529084: step 3231, loss 0.115057, acc 0.98
2016-09-07T20:41:42.179019: step 3232, loss 0.0436575, acc 0.98
2016-09-07T20:41:42.855158: step 3233, loss 0.0398109, acc 0.98
2016-09-07T20:41:43.520201: step 3234, loss 0.0841895, acc 0.98
2016-09-07T20:41:44.201800: step 3235, loss 0.0651905, acc 0.98
2016-09-07T20:41:44.871802: step 3236, loss 0.0478446, acc 0.96
2016-09-07T20:41:45.547864: step 3237, loss 0.0136673, acc 1
2016-09-07T20:41:46.217780: step 3238, loss 0.0339721, acc 1
2016-09-07T20:41:46.891249: step 3239, loss 0.0378381, acc 0.98
2016-09-07T20:41:47.560343: step 3240, loss 0.110713, acc 0.94
2016-09-07T20:41:48.215995: step 3241, loss 0.0475233, acc 0.98
2016-09-07T20:41:48.891796: step 3242, loss 0.0139666, acc 1
2016-09-07T20:41:49.555813: step 3243, loss 0.0205382, acc 1
2016-09-07T20:41:50.232162: step 3244, loss 0.0796513, acc 0.96
2016-09-07T20:41:50.904889: step 3245, loss 0.0352232, acc 1
2016-09-07T20:41:51.621417: step 3246, loss 0.0413109, acc 1
2016-09-07T20:41:52.282972: step 3247, loss 0.0272198, acc 1
2016-09-07T20:41:52.973481: step 3248, loss 0.0532047, acc 0.98
2016-09-07T20:41:53.637223: step 3249, loss 0.0516173, acc 0.98
2016-09-07T20:41:54.313880: step 3250, loss 0.0353065, acc 0.98
2016-09-07T20:41:54.973787: step 3251, loss 0.0247839, acc 1
2016-09-07T20:41:55.638778: step 3252, loss 0.010372, acc 1
2016-09-07T20:41:56.309548: step 3253, loss 0.134215, acc 0.94
2016-09-07T20:41:56.978645: step 3254, loss 0.00653797, acc 1
2016-09-07T20:41:57.652698: step 3255, loss 0.0501119, acc 0.98
2016-09-07T20:41:58.299345: step 3256, loss 0.0270671, acc 0.98
2016-09-07T20:41:58.945408: step 3257, loss 0.0645674, acc 0.96
2016-09-07T20:41:59.612738: step 3258, loss 0.10114, acc 0.94
2016-09-07T20:42:00.328418: step 3259, loss 0.0990516, acc 0.98
2016-09-07T20:42:00.990490: step 3260, loss 0.00925528, acc 1
2016-09-07T20:42:01.654827: step 3261, loss 0.0510781, acc 0.98
2016-09-07T20:42:02.335518: step 3262, loss 0.0297395, acc 0.98
2016-09-07T20:42:03.008608: step 3263, loss 0.0345776, acc 0.98
2016-09-07T20:42:03.686548: step 3264, loss 0.0301202, acc 1
2016-09-07T20:42:04.351407: step 3265, loss 0.0265755, acc 1
2016-09-07T20:42:05.014509: step 3266, loss 0.0620433, acc 0.98
2016-09-07T20:42:05.677670: step 3267, loss 0.0528647, acc 0.98
2016-09-07T20:42:06.366201: step 3268, loss 0.0118578, acc 1
2016-09-07T20:42:07.037579: step 3269, loss 0.00521259, acc 1
2016-09-07T20:42:07.712953: step 3270, loss 0.0193879, acc 1
2016-09-07T20:42:08.389302: step 3271, loss 0.016269, acc 1
2016-09-07T20:42:09.059631: step 3272, loss 0.0169574, acc 1
2016-09-07T20:42:09.725060: step 3273, loss 0.0464728, acc 0.96
2016-09-07T20:42:10.408797: step 3274, loss 0.0400601, acc 0.98
2016-09-07T20:42:11.086529: step 3275, loss 0.0205724, acc 1
2016-09-07T20:42:11.762396: step 3276, loss 0.0245461, acc 0.98
2016-09-07T20:42:12.437728: step 3277, loss 0.0142293, acc 1
2016-09-07T20:42:13.097056: step 3278, loss 0.00563752, acc 1
2016-09-07T20:42:13.763140: step 3279, loss 0.0555236, acc 0.96
2016-09-07T20:42:14.425797: step 3280, loss 0.0881413, acc 0.96
2016-09-07T20:42:15.100606: step 3281, loss 0.00846745, acc 1
2016-09-07T20:42:15.772514: step 3282, loss 0.0258207, acc 1
2016-09-07T20:42:16.454065: step 3283, loss 0.0513351, acc 0.98
2016-09-07T20:42:17.124291: step 3284, loss 0.0408488, acc 0.98
2016-09-07T20:42:17.789264: step 3285, loss 0.0535514, acc 0.96
2016-09-07T20:42:18.457576: step 3286, loss 0.121973, acc 0.96
2016-09-07T20:42:19.132682: step 3287, loss 0.152028, acc 0.96
2016-09-07T20:42:19.797264: step 3288, loss 0.0137207, acc 1
2016-09-07T20:42:20.479226: step 3289, loss 0.00977398, acc 1
2016-09-07T20:42:21.153188: step 3290, loss 0.125832, acc 0.98
2016-09-07T20:42:21.837611: step 3291, loss 0.0652127, acc 0.98
2016-09-07T20:42:22.506665: step 3292, loss 0.0387047, acc 0.98
2016-09-07T20:42:23.176016: step 3293, loss 0.0359651, acc 0.98
2016-09-07T20:42:23.851412: step 3294, loss 0.113384, acc 0.94
2016-09-07T20:42:24.531176: step 3295, loss 0.0434927, acc 0.98
2016-09-07T20:42:25.183915: step 3296, loss 0.0413458, acc 0.98
2016-09-07T20:42:25.861055: step 3297, loss 0.0223435, acc 0.98
2016-09-07T20:42:26.234028: step 3298, loss 0.0809638, acc 1
2016-09-07T20:42:26.915406: step 3299, loss 0.0643618, acc 0.94
2016-09-07T20:42:27.573498: step 3300, loss 0.0334413, acc 0.98

Evaluation:
2016-09-07T20:42:30.491122: step 3300, loss 1.33878, acc 0.752

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-3300

2016-09-07T20:42:32.118985: step 3301, loss 0.0283807, acc 1
2016-09-07T20:42:32.783730: step 3302, loss 0.0226972, acc 1
2016-09-07T20:42:33.452938: step 3303, loss 0.0313211, acc 1
2016-09-07T20:42:34.130573: step 3304, loss 0.0714431, acc 0.96
2016-09-07T20:42:34.800598: step 3305, loss 0.155446, acc 0.96
2016-09-07T20:42:35.469155: step 3306, loss 0.0541843, acc 0.98
2016-09-07T20:42:36.162357: step 3307, loss 0.014438, acc 1
2016-09-07T20:42:36.853820: step 3308, loss 0.0249916, acc 1
2016-09-07T20:42:37.535611: step 3309, loss 0.00513874, acc 1
2016-09-07T20:42:38.198624: step 3310, loss 0.0208669, acc 1
2016-09-07T20:42:38.878496: step 3311, loss 0.0891793, acc 0.98
2016-09-07T20:42:39.547147: step 3312, loss 0.0283958, acc 0.98
2016-09-07T20:42:40.238480: step 3313, loss 0.0425022, acc 0.98
2016-09-07T20:42:40.924488: step 3314, loss 0.012987, acc 1
2016-09-07T20:42:41.594938: step 3315, loss 0.0449021, acc 0.96
2016-09-07T20:42:42.261749: step 3316, loss 0.100323, acc 0.96
2016-09-07T20:42:42.932951: step 3317, loss 0.0284034, acc 1
2016-09-07T20:42:43.606275: step 3318, loss 0.0494851, acc 0.94
2016-09-07T20:42:44.282482: step 3319, loss 0.03184, acc 1
2016-09-07T20:42:44.975610: step 3320, loss 0.112965, acc 0.92
2016-09-07T20:42:45.648315: step 3321, loss 0.0573697, acc 0.98
2016-09-07T20:42:46.316055: step 3322, loss 0.0212116, acc 0.98
2016-09-07T20:42:46.986871: step 3323, loss 0.0313934, acc 1
2016-09-07T20:42:47.679891: step 3324, loss 0.067477, acc 0.96
2016-09-07T20:42:48.346025: step 3325, loss 0.00475829, acc 1
2016-09-07T20:42:49.016660: step 3326, loss 0.0121077, acc 1
2016-09-07T20:42:49.686053: step 3327, loss 0.00650717, acc 1
2016-09-07T20:42:50.359187: step 3328, loss 0.0422893, acc 0.98
2016-09-07T20:42:51.023858: step 3329, loss 0.0247848, acc 0.98
2016-09-07T20:42:51.677450: step 3330, loss 0.064562, acc 0.98
2016-09-07T20:42:52.344961: step 3331, loss 0.0198454, acc 1
2016-09-07T20:42:53.023069: step 3332, loss 0.0840439, acc 0.96
2016-09-07T20:42:53.693020: step 3333, loss 0.0299732, acc 0.98
2016-09-07T20:42:54.355258: step 3334, loss 0.0470894, acc 0.98
2016-09-07T20:42:55.020746: step 3335, loss 0.0268923, acc 1
2016-09-07T20:42:55.679995: step 3336, loss 0.0494732, acc 0.98
2016-09-07T20:42:56.350658: step 3337, loss 0.0140868, acc 1
2016-09-07T20:42:57.026518: step 3338, loss 0.00562752, acc 1
2016-09-07T20:42:57.688855: step 3339, loss 0.0276097, acc 1
2016-09-07T20:42:58.355011: step 3340, loss 0.0544999, acc 0.96
2016-09-07T20:42:59.003176: step 3341, loss 0.00414965, acc 1
2016-09-07T20:42:59.668458: step 3342, loss 0.0174401, acc 1
2016-09-07T20:43:00.365357: step 3343, loss 0.0941495, acc 0.96
2016-09-07T20:43:01.024950: step 3344, loss 0.0744898, acc 0.94
2016-09-07T20:43:01.699054: step 3345, loss 0.00405781, acc 1
2016-09-07T20:43:02.371427: step 3346, loss 0.0499763, acc 0.96
2016-09-07T20:43:03.029624: step 3347, loss 0.0130899, acc 1
2016-09-07T20:43:03.702144: step 3348, loss 0.0158638, acc 1
2016-09-07T20:43:04.379488: step 3349, loss 0.0132478, acc 1
2016-09-07T20:43:05.053309: step 3350, loss 0.0187339, acc 1
2016-09-07T20:43:05.702401: step 3351, loss 0.0261027, acc 0.98
2016-09-07T20:43:06.375569: step 3352, loss 0.0525571, acc 0.96
2016-09-07T20:43:07.073042: step 3353, loss 0.0317441, acc 1
2016-09-07T20:43:07.768834: step 3354, loss 0.152779, acc 0.94
2016-09-07T20:43:08.438216: step 3355, loss 0.0679541, acc 0.96
2016-09-07T20:43:09.102899: step 3356, loss 0.0542696, acc 0.98
2016-09-07T20:43:09.765939: step 3357, loss 0.0166974, acc 1
2016-09-07T20:43:10.439955: step 3358, loss 0.0105827, acc 1
2016-09-07T20:43:11.108411: step 3359, loss 0.0378878, acc 1
2016-09-07T20:43:11.767312: step 3360, loss 0.0210716, acc 1
2016-09-07T20:43:12.442041: step 3361, loss 0.0332049, acc 0.98
2016-09-07T20:43:13.115258: step 3362, loss 0.0603121, acc 0.98
2016-09-07T20:43:13.797336: step 3363, loss 0.0893826, acc 0.96
2016-09-07T20:43:14.457492: step 3364, loss 0.0752927, acc 0.96
2016-09-07T20:43:15.163354: step 3365, loss 0.0184256, acc 1
2016-09-07T20:43:15.820505: step 3366, loss 0.0109908, acc 1
2016-09-07T20:43:16.498852: step 3367, loss 0.0231457, acc 0.98
2016-09-07T20:43:17.170659: step 3368, loss 0.06213, acc 0.96
2016-09-07T20:43:17.827319: step 3369, loss 0.00723271, acc 1
2016-09-07T20:43:18.494374: step 3370, loss 0.032698, acc 0.98
2016-09-07T20:43:19.158671: step 3371, loss 0.00351861, acc 1
2016-09-07T20:43:19.839091: step 3372, loss 0.0267829, acc 0.98
2016-09-07T20:43:20.519583: step 3373, loss 0.0400443, acc 0.98
2016-09-07T20:43:21.219159: step 3374, loss 0.0479546, acc 0.98
2016-09-07T20:43:21.891954: step 3375, loss 0.00448707, acc 1
2016-09-07T20:43:22.565900: step 3376, loss 0.0253645, acc 0.98
2016-09-07T20:43:23.225001: step 3377, loss 0.0318082, acc 0.98
2016-09-07T20:43:23.908153: step 3378, loss 0.0168759, acc 1
2016-09-07T20:43:24.570791: step 3379, loss 0.0206901, acc 0.98
2016-09-07T20:43:25.240838: step 3380, loss 0.0143585, acc 1
2016-09-07T20:43:25.910977: step 3381, loss 0.0651802, acc 0.98
2016-09-07T20:43:26.603034: step 3382, loss 0.0139389, acc 1
2016-09-07T20:43:27.263639: step 3383, loss 0.0143811, acc 1
2016-09-07T20:43:27.928461: step 3384, loss 0.0155226, acc 1
2016-09-07T20:43:28.597383: step 3385, loss 0.0396045, acc 0.98
2016-09-07T20:43:29.259411: step 3386, loss 0.0928663, acc 0.96
2016-09-07T20:43:29.930855: step 3387, loss 0.127555, acc 0.92
2016-09-07T20:43:30.599875: step 3388, loss 0.0206824, acc 1
2016-09-07T20:43:31.277604: step 3389, loss 0.0180169, acc 1
2016-09-07T20:43:31.951057: step 3390, loss 0.158955, acc 0.98
2016-09-07T20:43:32.648263: step 3391, loss 0.0786501, acc 0.94
2016-09-07T20:43:33.332860: step 3392, loss 0.0147617, acc 1
2016-09-07T20:43:33.975294: step 3393, loss 0.0592406, acc 0.96
2016-09-07T20:43:34.664802: step 3394, loss 0.0356583, acc 0.98
2016-09-07T20:43:35.330863: step 3395, loss 0.0477777, acc 0.98
2016-09-07T20:43:36.004893: step 3396, loss 0.0747974, acc 0.96
2016-09-07T20:43:36.673700: step 3397, loss 0.0262064, acc 0.98
2016-09-07T20:43:37.350604: step 3398, loss 0.0357556, acc 1
2016-09-07T20:43:38.008870: step 3399, loss 0.0164712, acc 1
2016-09-07T20:43:38.666932: step 3400, loss 0.0188057, acc 1

Evaluation:
2016-09-07T20:43:41.568260: step 3400, loss 1.5608, acc 0.746

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-3400

2016-09-07T20:43:43.221999: step 3401, loss 0.0278622, acc 1
2016-09-07T20:43:43.900179: step 3402, loss 0.0580934, acc 0.96
2016-09-07T20:43:44.570664: step 3403, loss 0.0119228, acc 1
2016-09-07T20:43:45.238700: step 3404, loss 0.0200225, acc 1
2016-09-07T20:43:45.905793: step 3405, loss 0.0664758, acc 0.96
2016-09-07T20:43:46.579062: step 3406, loss 0.0460185, acc 0.98
2016-09-07T20:43:47.265869: step 3407, loss 0.0368909, acc 0.98
2016-09-07T20:43:47.946858: step 3408, loss 0.0105285, acc 1
2016-09-07T20:43:48.627281: step 3409, loss 0.0235715, acc 1
2016-09-07T20:43:49.290130: step 3410, loss 0.0202603, acc 0.98
2016-09-07T20:43:49.967204: step 3411, loss 0.0845708, acc 0.96
2016-09-07T20:43:50.636555: step 3412, loss 0.0724353, acc 0.98
2016-09-07T20:43:51.307256: step 3413, loss 0.043892, acc 0.96
2016-09-07T20:43:51.964086: step 3414, loss 0.0817182, acc 1
2016-09-07T20:43:52.640390: step 3415, loss 0.00674531, acc 1
2016-09-07T20:43:53.303367: step 3416, loss 0.00437962, acc 1
2016-09-07T20:43:53.981247: step 3417, loss 0.0260117, acc 0.98
2016-09-07T20:43:54.626981: step 3418, loss 0.014331, acc 1
2016-09-07T20:43:55.306072: step 3419, loss 0.0191211, acc 1
2016-09-07T20:43:55.988902: step 3420, loss 0.0787288, acc 0.96
2016-09-07T20:43:56.653689: step 3421, loss 0.0472286, acc 0.98
2016-09-07T20:43:57.310849: step 3422, loss 0.00861794, acc 1
2016-09-07T20:43:57.976890: step 3423, loss 0.0569917, acc 0.98
2016-09-07T20:43:58.638222: step 3424, loss 0.0311578, acc 1
2016-09-07T20:43:59.310909: step 3425, loss 0.0419338, acc 1
2016-09-07T20:43:59.974094: step 3426, loss 0.0241488, acc 0.98
2016-09-07T20:44:00.750261: step 3427, loss 0.0121915, acc 1
2016-09-07T20:44:01.451229: step 3428, loss 0.252439, acc 0.96
2016-09-07T20:44:02.133527: step 3429, loss 0.0351524, acc 0.96
2016-09-07T20:44:02.819089: step 3430, loss 0.0231775, acc 1
2016-09-07T20:44:03.482062: step 3431, loss 0.0423932, acc 0.98
2016-09-07T20:44:04.143105: step 3432, loss 0.0409797, acc 0.96
2016-09-07T20:44:04.807309: step 3433, loss 0.0278465, acc 1
2016-09-07T20:44:05.473711: step 3434, loss 0.0607345, acc 0.98
2016-09-07T20:44:06.137309: step 3435, loss 0.0265009, acc 0.98
2016-09-07T20:44:06.834189: step 3436, loss 0.0122673, acc 1
2016-09-07T20:44:07.499205: step 3437, loss 0.00480378, acc 1
2016-09-07T20:44:08.170919: step 3438, loss 0.0288652, acc 1
2016-09-07T20:44:08.839068: step 3439, loss 0.100294, acc 0.94
2016-09-07T20:44:09.510138: step 3440, loss 0.092622, acc 0.96
2016-09-07T20:44:10.170090: step 3441, loss 0.0233909, acc 1
2016-09-07T20:44:10.842058: step 3442, loss 0.0224176, acc 1
2016-09-07T20:44:11.524185: step 3443, loss 0.0878782, acc 0.96
2016-09-07T20:44:12.189709: step 3444, loss 0.0256167, acc 1
2016-09-07T20:44:12.860136: step 3445, loss 0.0728967, acc 0.96
2016-09-07T20:44:13.546869: step 3446, loss 0.0409178, acc 0.98
2016-09-07T20:44:14.217207: step 3447, loss 0.0732947, acc 0.98
2016-09-07T20:44:14.884603: step 3448, loss 0.00539579, acc 1
2016-09-07T20:44:15.579488: step 3449, loss 0.0362132, acc 0.98
2016-09-07T20:44:16.246568: step 3450, loss 0.036672, acc 0.98
2016-09-07T20:44:16.899924: step 3451, loss 0.0431351, acc 0.98
2016-09-07T20:44:17.565678: step 3452, loss 0.0565331, acc 0.98
2016-09-07T20:44:18.256960: step 3453, loss 0.0546711, acc 0.98
2016-09-07T20:44:18.946800: step 3454, loss 0.017301, acc 1
2016-09-07T20:44:19.622501: step 3455, loss 0.0202787, acc 1
2016-09-07T20:44:20.284960: step 3456, loss 0.0106954, acc 1
2016-09-07T20:44:20.956745: step 3457, loss 0.0148475, acc 1
2016-09-07T20:44:21.633284: step 3458, loss 0.0273279, acc 0.98
2016-09-07T20:44:22.318930: step 3459, loss 0.134947, acc 0.92
2016-09-07T20:44:23.000493: step 3460, loss 0.029308, acc 0.98
2016-09-07T20:44:23.655027: step 3461, loss 0.0951014, acc 0.96
2016-09-07T20:44:24.313056: step 3462, loss 0.00688214, acc 1
2016-09-07T20:44:24.993786: step 3463, loss 0.0198146, acc 1
2016-09-07T20:44:25.676900: step 3464, loss 0.0349354, acc 1
2016-09-07T20:44:26.341438: step 3465, loss 0.0103239, acc 1
2016-09-07T20:44:27.008013: step 3466, loss 0.0231985, acc 0.98
2016-09-07T20:44:27.679300: step 3467, loss 0.102043, acc 0.96
2016-09-07T20:44:28.359453: step 3468, loss 0.0644617, acc 0.96
2016-09-07T20:44:29.034376: step 3469, loss 0.0336494, acc 0.98
2016-09-07T20:44:29.676290: step 3470, loss 0.020211, acc 0.98
2016-09-07T20:44:30.345292: step 3471, loss 0.0246732, acc 0.98
2016-09-07T20:44:31.027079: step 3472, loss 0.0218978, acc 1
2016-09-07T20:44:31.706802: step 3473, loss 0.0346806, acc 1
2016-09-07T20:44:32.364342: step 3474, loss 0.0317719, acc 0.98
2016-09-07T20:44:33.033970: step 3475, loss 0.00593498, acc 1
2016-09-07T20:44:33.703499: step 3476, loss 0.0214289, acc 0.98
2016-09-07T20:44:34.390027: step 3477, loss 0.0208794, acc 1
2016-09-07T20:44:35.048373: step 3478, loss 0.0196803, acc 0.98
2016-09-07T20:44:35.716582: step 3479, loss 0.030951, acc 0.98
2016-09-07T20:44:36.382256: step 3480, loss 0.0212769, acc 1
2016-09-07T20:44:37.057970: step 3481, loss 0.01596, acc 1
2016-09-07T20:44:37.723565: step 3482, loss 0.019265, acc 0.98
2016-09-07T20:44:38.390039: step 3483, loss 0.0283372, acc 1
2016-09-07T20:44:39.052360: step 3484, loss 0.00827555, acc 1
2016-09-07T20:44:39.732528: step 3485, loss 0.0553192, acc 0.96
2016-09-07T20:44:40.399766: step 3486, loss 0.0140642, acc 1
2016-09-07T20:44:41.064725: step 3487, loss 0.0363826, acc 1
2016-09-07T20:44:41.747475: step 3488, loss 0.0376111, acc 0.96
2016-09-07T20:44:42.416440: step 3489, loss 0.0322559, acc 1
2016-09-07T20:44:43.107072: step 3490, loss 0.0729231, acc 0.96
2016-09-07T20:44:43.776303: step 3491, loss 0.0328184, acc 0.98
2016-09-07T20:44:44.150399: step 3492, loss 0.0685298, acc 0.916667
2016-09-07T20:44:44.824789: step 3493, loss 0.0329032, acc 0.98
2016-09-07T20:44:45.529353: step 3494, loss 0.0372259, acc 0.98
2016-09-07T20:44:46.189765: step 3495, loss 0.0468185, acc 0.98
2016-09-07T20:44:46.863156: step 3496, loss 0.0158635, acc 1
2016-09-07T20:44:47.534151: step 3497, loss 0.00462079, acc 1
2016-09-07T20:44:48.201186: step 3498, loss 0.0121745, acc 1
2016-09-07T20:44:48.862943: step 3499, loss 0.0650215, acc 0.96
2016-09-07T20:44:49.522994: step 3500, loss 0.0153916, acc 1

Evaluation:
2016-09-07T20:44:52.451642: step 3500, loss 1.81222, acc 0.761

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-3500

2016-09-07T20:44:54.100863: step 3501, loss 0.101582, acc 0.96
2016-09-07T20:44:54.767847: step 3502, loss 0.00633929, acc 1
2016-09-07T20:44:55.455872: step 3503, loss 0.0126004, acc 1
2016-09-07T20:44:56.139700: step 3504, loss 0.0222595, acc 1
2016-09-07T20:44:56.792946: step 3505, loss 0.10611, acc 0.98
2016-09-07T20:44:57.472631: step 3506, loss 0.128129, acc 0.94
2016-09-07T20:44:58.137004: step 3507, loss 0.0120144, acc 1
2016-09-07T20:44:58.812428: step 3508, loss 0.0262768, acc 0.98
2016-09-07T20:44:59.499410: step 3509, loss 0.0158456, acc 1
2016-09-07T20:45:00.171574: step 3510, loss 0.00459446, acc 1
2016-09-07T20:45:00.885702: step 3511, loss 0.0109063, acc 1
2016-09-07T20:45:01.568336: step 3512, loss 0.00677979, acc 1
2016-09-07T20:45:02.235063: step 3513, loss 0.0500067, acc 0.96
2016-09-07T20:45:02.903960: step 3514, loss 0.0808918, acc 0.96
2016-09-07T20:45:03.578002: step 3515, loss 0.00861792, acc 1
2016-09-07T20:45:04.248906: step 3516, loss 0.0297034, acc 0.98
2016-09-07T20:45:04.918998: step 3517, loss 0.0723375, acc 0.96
2016-09-07T20:45:05.598735: step 3518, loss 0.0262875, acc 0.98
2016-09-07T20:45:06.261856: step 3519, loss 0.00381585, acc 1
2016-09-07T20:45:06.939856: step 3520, loss 0.0131634, acc 1
2016-09-07T20:45:07.608809: step 3521, loss 0.0379794, acc 0.98
2016-09-07T20:45:08.289686: step 3522, loss 0.0278202, acc 1
2016-09-07T20:45:08.965878: step 3523, loss 0.118515, acc 0.96
2016-09-07T20:45:09.636469: step 3524, loss 0.0593289, acc 0.98
2016-09-07T20:45:10.299488: step 3525, loss 0.0160911, acc 1
2016-09-07T20:45:10.965986: step 3526, loss 0.0649576, acc 0.94
2016-09-07T20:45:11.648512: step 3527, loss 0.025801, acc 1
2016-09-07T20:45:12.311087: step 3528, loss 0.136398, acc 0.96
2016-09-07T20:45:13.015336: step 3529, loss 0.00344174, acc 1
2016-09-07T20:45:13.683709: step 3530, loss 0.055655, acc 0.98
2016-09-07T20:45:14.351378: step 3531, loss 0.00542095, acc 1
2016-09-07T20:45:15.022328: step 3532, loss 0.00321231, acc 1
2016-09-07T20:45:15.680280: step 3533, loss 0.0329704, acc 0.98
2016-09-07T20:45:16.344401: step 3534, loss 0.0146044, acc 1
2016-09-07T20:45:17.017938: step 3535, loss 0.0230359, acc 1
2016-09-07T20:45:17.690436: step 3536, loss 0.0476543, acc 0.98
2016-09-07T20:45:18.363287: step 3537, loss 0.0358749, acc 0.98
2016-09-07T20:45:19.033802: step 3538, loss 0.0484964, acc 0.98
2016-09-07T20:45:19.711180: step 3539, loss 0.0687015, acc 0.96
2016-09-07T20:45:20.386478: step 3540, loss 0.0262529, acc 0.98
2016-09-07T20:45:21.048067: step 3541, loss 0.0112511, acc 1
2016-09-07T20:45:21.728878: step 3542, loss 0.0567816, acc 0.96
2016-09-07T20:45:22.409950: step 3543, loss 0.0773734, acc 0.96
2016-09-07T20:45:23.079583: step 3544, loss 0.0499794, acc 0.98
2016-09-07T20:45:23.746633: step 3545, loss 0.0154068, acc 1
2016-09-07T20:45:24.412961: step 3546, loss 0.0305463, acc 1
2016-09-07T20:45:25.094944: step 3547, loss 0.0627744, acc 0.96
2016-09-07T20:45:25.777785: step 3548, loss 0.0184436, acc 1
2016-09-07T20:45:26.429876: step 3549, loss 0.041035, acc 0.98
2016-09-07T20:45:27.105275: step 3550, loss 0.0118405, acc 1
2016-09-07T20:45:27.787259: step 3551, loss 0.0240955, acc 0.98
2016-09-07T20:45:28.467095: step 3552, loss 0.00473838, acc 1
2016-09-07T20:45:29.133872: step 3553, loss 0.0691418, acc 0.98
2016-09-07T20:45:29.798991: step 3554, loss 0.0131937, acc 1
2016-09-07T20:45:30.477331: step 3555, loss 0.0281586, acc 0.98
2016-09-07T20:45:31.157261: step 3556, loss 0.0358583, acc 1
2016-09-07T20:45:31.829805: step 3557, loss 0.0715406, acc 0.98
2016-09-07T20:45:32.501211: step 3558, loss 0.0375876, acc 0.98
2016-09-07T20:45:33.175603: step 3559, loss 0.00966678, acc 1
2016-09-07T20:45:33.859530: step 3560, loss 0.0289258, acc 1
2016-09-07T20:45:34.513809: step 3561, loss 0.00961154, acc 1
2016-09-07T20:45:35.176314: step 3562, loss 0.01125, acc 1
2016-09-07T20:45:35.832465: step 3563, loss 0.0353014, acc 0.98
2016-09-07T20:45:36.492114: step 3564, loss 0.0775131, acc 0.96
2016-09-07T20:45:37.187712: step 3565, loss 0.00414304, acc 1
2016-09-07T20:45:37.856707: step 3566, loss 0.011374, acc 1
2016-09-07T20:45:38.528420: step 3567, loss 0.00499193, acc 1
2016-09-07T20:45:39.183560: step 3568, loss 0.0329198, acc 0.98
2016-09-07T20:45:39.842768: step 3569, loss 0.00982294, acc 1
2016-09-07T20:45:40.496502: step 3570, loss 0.116525, acc 0.96
2016-09-07T20:45:41.159252: step 3571, loss 0.0091632, acc 1
2016-09-07T20:45:41.817812: step 3572, loss 0.0299456, acc 0.98
2016-09-07T20:45:42.466425: step 3573, loss 0.036147, acc 0.98
2016-09-07T20:45:43.124544: step 3574, loss 0.104449, acc 0.98
2016-09-07T20:45:43.777372: step 3575, loss 0.028599, acc 1
2016-09-07T20:45:44.469040: step 3576, loss 0.0218603, acc 1
2016-09-07T20:45:45.144630: step 3577, loss 0.00478717, acc 1
2016-09-07T20:45:45.798234: step 3578, loss 0.180396, acc 0.94
2016-09-07T20:45:46.457583: step 3579, loss 0.0446558, acc 0.98
2016-09-07T20:45:47.123817: step 3580, loss 0.0190268, acc 1
2016-09-07T20:45:47.794613: step 3581, loss 0.0178956, acc 1
2016-09-07T20:45:48.456980: step 3582, loss 0.0281336, acc 1
2016-09-07T20:45:49.132254: step 3583, loss 0.0759583, acc 0.96
2016-09-07T20:45:49.812510: step 3584, loss 0.00744397, acc 1
2016-09-07T20:45:50.477572: step 3585, loss 0.0519048, acc 0.98
2016-09-07T20:45:51.150784: step 3586, loss 0.0364723, acc 1
2016-09-07T20:45:51.824162: step 3587, loss 0.0398614, acc 0.98
2016-09-07T20:45:52.496424: step 3588, loss 0.0359334, acc 0.96
2016-09-07T20:45:53.164251: step 3589, loss 0.0444841, acc 0.96
2016-09-07T20:45:53.823858: step 3590, loss 0.027393, acc 1
2016-09-07T20:45:54.482718: step 3591, loss 0.0259817, acc 0.98
2016-09-07T20:45:55.170818: step 3592, loss 0.0466909, acc 0.96
2016-09-07T20:45:55.834683: step 3593, loss 0.0139189, acc 1
2016-09-07T20:45:56.482755: step 3594, loss 0.0298774, acc 0.98
2016-09-07T20:45:57.160744: step 3595, loss 0.0337729, acc 1
2016-09-07T20:45:57.842575: step 3596, loss 0.0183668, acc 1
2016-09-07T20:45:58.513764: step 3597, loss 0.0203961, acc 1
2016-09-07T20:45:59.171869: step 3598, loss 0.0790339, acc 0.98
2016-09-07T20:45:59.840884: step 3599, loss 0.0260963, acc 1
2016-09-07T20:46:00.540403: step 3600, loss 0.019846, acc 1

Evaluation:
2016-09-07T20:46:03.453695: step 3600, loss 1.64754, acc 0.735

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-3600

2016-09-07T20:46:05.059168: step 3601, loss 0.0220588, acc 1
2016-09-07T20:46:05.726294: step 3602, loss 0.00946048, acc 1
2016-09-07T20:46:06.392565: step 3603, loss 0.0923397, acc 0.98
2016-09-07T20:46:07.044549: step 3604, loss 0.0035385, acc 1
2016-09-07T20:46:07.705681: step 3605, loss 0.0173112, acc 1
2016-09-07T20:46:08.378086: step 3606, loss 0.0256789, acc 1
2016-09-07T20:46:09.068839: step 3607, loss 0.0291425, acc 0.98
2016-09-07T20:46:09.726960: step 3608, loss 0.0333266, acc 0.98
2016-09-07T20:46:10.410039: step 3609, loss 0.00642544, acc 1
2016-09-07T20:46:11.097112: step 3610, loss 0.0794706, acc 0.94
2016-09-07T20:46:11.773301: step 3611, loss 0.00603905, acc 1
2016-09-07T20:46:12.432438: step 3612, loss 0.109379, acc 0.96
2016-09-07T20:46:13.101586: step 3613, loss 0.00547561, acc 1
2016-09-07T20:46:13.790530: step 3614, loss 0.0307044, acc 0.98
2016-09-07T20:46:14.461249: step 3615, loss 0.00951571, acc 1
2016-09-07T20:46:15.126616: step 3616, loss 0.0124725, acc 1
2016-09-07T20:46:15.800168: step 3617, loss 0.0734001, acc 0.96
2016-09-07T20:46:16.469347: step 3618, loss 0.0344973, acc 0.96
2016-09-07T20:46:17.136709: step 3619, loss 0.102311, acc 0.96
2016-09-07T20:46:17.832461: step 3620, loss 0.0510275, acc 0.98
2016-09-07T20:46:18.499115: step 3621, loss 0.0206568, acc 1
2016-09-07T20:46:19.181901: step 3622, loss 0.0630067, acc 0.98
2016-09-07T20:46:19.852236: step 3623, loss 0.133091, acc 0.96
2016-09-07T20:46:20.519336: step 3624, loss 0.0158426, acc 1
2016-09-07T20:46:21.186561: step 3625, loss 0.0249454, acc 1
2016-09-07T20:46:21.860482: step 3626, loss 0.0152528, acc 1
2016-09-07T20:46:22.570993: step 3627, loss 0.0143241, acc 1
2016-09-07T20:46:23.247180: step 3628, loss 0.0121174, acc 1
2016-09-07T20:46:23.908933: step 3629, loss 0.0749987, acc 0.98
2016-09-07T20:46:24.581589: step 3630, loss 0.00673283, acc 1
2016-09-07T20:46:25.245440: step 3631, loss 0.0882905, acc 0.98
2016-09-07T20:46:25.929324: step 3632, loss 0.255058, acc 0.96
2016-09-07T20:46:26.601396: step 3633, loss 0.0341189, acc 1
2016-09-07T20:46:27.284542: step 3634, loss 0.00794338, acc 1
2016-09-07T20:46:27.957944: step 3635, loss 0.00698612, acc 1
2016-09-07T20:46:28.649840: step 3636, loss 0.00906156, acc 1
2016-09-07T20:46:29.313524: step 3637, loss 0.0397608, acc 0.98
2016-09-07T20:46:29.998917: step 3638, loss 0.0119346, acc 1
2016-09-07T20:46:30.658871: step 3639, loss 0.0728468, acc 0.96
2016-09-07T20:46:31.321348: step 3640, loss 0.0186635, acc 1
2016-09-07T20:46:31.989788: step 3641, loss 0.0830549, acc 0.94
2016-09-07T20:46:32.663205: step 3642, loss 0.0569987, acc 0.96
2016-09-07T20:46:33.355673: step 3643, loss 0.0478026, acc 0.96
2016-09-07T20:46:34.055106: step 3644, loss 0.0607799, acc 0.98
2016-09-07T20:46:34.747859: step 3645, loss 0.0262166, acc 0.98
2016-09-07T20:46:35.410885: step 3646, loss 0.0272092, acc 1
2016-09-07T20:46:36.096691: step 3647, loss 0.0583463, acc 0.96
2016-09-07T20:46:36.763796: step 3648, loss 0.0314066, acc 0.98
2016-09-07T20:46:37.429607: step 3649, loss 0.0413356, acc 0.98
2016-09-07T20:46:38.089255: step 3650, loss 0.085214, acc 0.94
2016-09-07T20:46:38.745666: step 3651, loss 0.109197, acc 0.98
2016-09-07T20:46:39.399465: step 3652, loss 0.138543, acc 0.98
2016-09-07T20:46:40.074549: step 3653, loss 0.151395, acc 0.96
2016-09-07T20:46:40.742325: step 3654, loss 0.0377669, acc 0.98
2016-09-07T20:46:41.408612: step 3655, loss 0.00519078, acc 1
2016-09-07T20:46:42.069234: step 3656, loss 0.00848449, acc 1
2016-09-07T20:46:42.740593: step 3657, loss 0.0071243, acc 1
2016-09-07T20:46:43.402867: step 3658, loss 0.106753, acc 0.94
2016-09-07T20:46:44.083150: step 3659, loss 0.0434472, acc 0.98
2016-09-07T20:46:44.753297: step 3660, loss 0.0566377, acc 0.96
2016-09-07T20:46:45.431492: step 3661, loss 0.0429144, acc 0.98
2016-09-07T20:46:46.112908: step 3662, loss 0.109974, acc 0.94
2016-09-07T20:46:46.772903: step 3663, loss 0.0219934, acc 1
2016-09-07T20:46:47.447302: step 3664, loss 0.0184344, acc 1
2016-09-07T20:46:48.127880: step 3665, loss 0.0858521, acc 0.96
2016-09-07T20:46:48.799109: step 3666, loss 0.11942, acc 0.96
2016-09-07T20:46:49.459260: step 3667, loss 0.102208, acc 0.96
2016-09-07T20:46:50.114860: step 3668, loss 0.0335915, acc 0.98
2016-09-07T20:46:50.790777: step 3669, loss 0.0346374, acc 0.98
2016-09-07T20:46:51.466950: step 3670, loss 0.0177639, acc 1
2016-09-07T20:46:52.145909: step 3671, loss 0.0385453, acc 1
2016-09-07T20:46:52.820385: step 3672, loss 0.0214995, acc 0.98
2016-09-07T20:46:53.512418: step 3673, loss 0.0607907, acc 0.98
2016-09-07T20:46:54.199648: step 3674, loss 0.0689507, acc 0.96
2016-09-07T20:46:54.883602: step 3675, loss 0.0420728, acc 0.98
2016-09-07T20:46:55.556963: step 3676, loss 0.144871, acc 0.96
2016-09-07T20:46:56.238968: step 3677, loss 0.0157656, acc 1
2016-09-07T20:46:56.903186: step 3678, loss 0.0736185, acc 0.96
2016-09-07T20:46:57.579764: step 3679, loss 0.029698, acc 1
2016-09-07T20:46:58.262678: step 3680, loss 0.0779757, acc 0.94
2016-09-07T20:46:58.939876: step 3681, loss 0.00589034, acc 1
2016-09-07T20:46:59.587048: step 3682, loss 0.0356541, acc 0.98
2016-09-07T20:47:00.280035: step 3683, loss 0.0238548, acc 1
2016-09-07T20:47:00.937514: step 3684, loss 0.0272098, acc 1
2016-09-07T20:47:01.593888: step 3685, loss 0.0324142, acc 0.98
2016-09-07T20:47:01.946777: step 3686, loss 0.0401467, acc 1
2016-09-07T20:47:02.630281: step 3687, loss 0.0251366, acc 1
2016-09-07T20:47:03.313752: step 3688, loss 0.0228834, acc 1
2016-09-07T20:47:03.996653: step 3689, loss 0.0737756, acc 0.96
2016-09-07T20:47:04.692075: step 3690, loss 0.0251414, acc 1
2016-09-07T20:47:05.342064: step 3691, loss 0.00885031, acc 1
2016-09-07T20:47:06.024621: step 3692, loss 0.0649181, acc 0.96
2016-09-07T20:47:06.722284: step 3693, loss 0.039046, acc 0.98
2016-09-07T20:47:07.409424: step 3694, loss 0.143161, acc 0.96
2016-09-07T20:47:08.071669: step 3695, loss 0.0333124, acc 0.98
2016-09-07T20:47:08.743691: step 3696, loss 0.0272665, acc 0.98
2016-09-07T20:47:09.428953: step 3697, loss 0.0399917, acc 0.98
2016-09-07T20:47:10.096272: step 3698, loss 0.0215872, acc 1
2016-09-07T20:47:10.776309: step 3699, loss 0.0262653, acc 1
2016-09-07T20:47:11.452854: step 3700, loss 0.033467, acc 1

Evaluation:
2016-09-07T20:47:14.359483: step 3700, loss 1.75056, acc 0.732

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-3700

2016-09-07T20:47:15.975895: step 3701, loss 0.0167642, acc 1
2016-09-07T20:47:16.640926: step 3702, loss 0.0236403, acc 0.98
2016-09-07T20:47:17.308044: step 3703, loss 0.0247167, acc 1
2016-09-07T20:47:18.001811: step 3704, loss 0.0389059, acc 0.98
2016-09-07T20:47:18.657870: step 3705, loss 0.0652213, acc 1
2016-09-07T20:47:19.335728: step 3706, loss 0.0832198, acc 0.96
2016-09-07T20:47:20.004053: step 3707, loss 0.0663025, acc 0.98
2016-09-07T20:47:20.682049: step 3708, loss 0.0219016, acc 1
2016-09-07T20:47:21.359109: step 3709, loss 0.0581413, acc 0.96
2016-09-07T20:47:22.060088: step 3710, loss 0.0130383, acc 1
2016-09-07T20:47:22.721161: step 3711, loss 0.0061465, acc 1
2016-09-07T20:47:23.390520: step 3712, loss 0.00610185, acc 1
2016-09-07T20:47:24.053694: step 3713, loss 0.0183201, acc 0.98
2016-09-07T20:47:24.735054: step 3714, loss 0.00453409, acc 1
2016-09-07T20:47:25.415898: step 3715, loss 0.0137608, acc 1
2016-09-07T20:47:26.078701: step 3716, loss 0.0208882, acc 1
2016-09-07T20:47:26.741043: step 3717, loss 0.0605179, acc 0.98
2016-09-07T20:47:27.410485: step 3718, loss 0.0563536, acc 0.96
2016-09-07T20:47:28.100924: step 3719, loss 0.02964, acc 1
2016-09-07T20:47:28.764394: step 3720, loss 0.00705189, acc 1
2016-09-07T20:47:29.445654: step 3721, loss 0.0326292, acc 1
2016-09-07T20:47:30.105578: step 3722, loss 0.0406426, acc 0.96
2016-09-07T20:47:30.776030: step 3723, loss 0.0184617, acc 1
2016-09-07T20:47:31.452763: step 3724, loss 0.0758648, acc 0.96
2016-09-07T20:47:32.135686: step 3725, loss 0.0075629, acc 1
2016-09-07T20:47:32.809422: step 3726, loss 0.00417111, acc 1
2016-09-07T20:47:33.476448: step 3727, loss 0.0485054, acc 0.98
2016-09-07T20:47:34.149897: step 3728, loss 0.011747, acc 1
2016-09-07T20:47:34.850211: step 3729, loss 0.00771785, acc 1
2016-09-07T20:47:35.527644: step 3730, loss 0.0651619, acc 0.96
2016-09-07T20:47:36.204806: step 3731, loss 0.0506234, acc 0.96
2016-09-07T20:47:36.877398: step 3732, loss 0.0113699, acc 1
2016-09-07T20:47:37.529328: step 3733, loss 0.0509696, acc 0.96
2016-09-07T20:47:38.198514: step 3734, loss 0.0912372, acc 0.94
2016-09-07T20:47:38.855419: step 3735, loss 0.00999788, acc 1
2016-09-07T20:47:39.526125: step 3736, loss 0.0345014, acc 0.98
2016-09-07T20:47:40.199495: step 3737, loss 0.0631936, acc 0.98
2016-09-07T20:47:40.863753: step 3738, loss 0.038518, acc 0.96
2016-09-07T20:47:41.534287: step 3739, loss 0.00556576, acc 1
2016-09-07T20:47:42.208352: step 3740, loss 0.029717, acc 0.98
2016-09-07T20:47:42.873802: step 3741, loss 0.0339775, acc 0.98
2016-09-07T20:47:43.543219: step 3742, loss 0.0604552, acc 0.96
2016-09-07T20:47:44.218117: step 3743, loss 0.0299691, acc 0.98
2016-09-07T20:47:44.894402: step 3744, loss 0.00527709, acc 1
2016-09-07T20:47:45.574170: step 3745, loss 0.0192528, acc 0.98
2016-09-07T20:47:46.255051: step 3746, loss 0.0278401, acc 0.98
2016-09-07T20:47:46.945718: step 3747, loss 0.0423864, acc 1
2016-09-07T20:47:47.606792: step 3748, loss 0.0245954, acc 0.98
2016-09-07T20:47:48.292166: step 3749, loss 0.00483407, acc 1
2016-09-07T20:47:48.956439: step 3750, loss 0.0429766, acc 0.98
2016-09-07T20:47:49.619307: step 3751, loss 0.0561282, acc 0.98
2016-09-07T20:47:50.292283: step 3752, loss 0.0161934, acc 1
2016-09-07T20:47:50.991193: step 3753, loss 0.0214514, acc 0.98
2016-09-07T20:47:51.657116: step 3754, loss 0.00658906, acc 1
2016-09-07T20:47:52.359194: step 3755, loss 0.0604277, acc 0.98
2016-09-07T20:47:53.041772: step 3756, loss 0.035072, acc 0.98
2016-09-07T20:47:53.719354: step 3757, loss 0.0187378, acc 1
2016-09-07T20:47:54.391964: step 3758, loss 0.0837512, acc 0.96
2016-09-07T20:47:55.072385: step 3759, loss 0.0301626, acc 0.98
2016-09-07T20:47:55.741342: step 3760, loss 0.125551, acc 0.94
2016-09-07T20:47:56.419564: step 3761, loss 0.0594796, acc 0.98
2016-09-07T20:47:57.100656: step 3762, loss 0.057808, acc 0.98
2016-09-07T20:47:57.763816: step 3763, loss 0.0310403, acc 0.98
2016-09-07T20:47:58.416073: step 3764, loss 0.00645253, acc 1
2016-09-07T20:47:59.077414: step 3765, loss 0.0578111, acc 0.96
2016-09-07T20:47:59.746580: step 3766, loss 0.0270893, acc 0.98
2016-09-07T20:48:00.470512: step 3767, loss 0.0948495, acc 0.96
2016-09-07T20:48:01.135485: step 3768, loss 0.00469413, acc 1
2016-09-07T20:48:01.802587: step 3769, loss 0.074519, acc 0.96
2016-09-07T20:48:02.483076: step 3770, loss 0.00843097, acc 1
2016-09-07T20:48:03.159675: step 3771, loss 0.120203, acc 0.92
2016-09-07T20:48:03.834986: step 3772, loss 0.073787, acc 0.98
2016-09-07T20:48:04.500200: step 3773, loss 0.00611668, acc 1
2016-09-07T20:48:05.173798: step 3774, loss 0.00916403, acc 1
2016-09-07T20:48:05.864111: step 3775, loss 0.267492, acc 0.94
2016-09-07T20:48:06.537329: step 3776, loss 0.0478705, acc 0.98
2016-09-07T20:48:07.198103: step 3777, loss 0.0632167, acc 0.96
2016-09-07T20:48:07.860777: step 3778, loss 0.038013, acc 0.98
2016-09-07T20:48:08.522483: step 3779, loss 0.0354761, acc 0.98
2016-09-07T20:48:09.191019: step 3780, loss 0.094068, acc 0.96
2016-09-07T20:48:09.848610: step 3781, loss 0.0422558, acc 0.98
2016-09-07T20:48:10.512663: step 3782, loss 0.0239493, acc 1
2016-09-07T20:48:11.172962: step 3783, loss 0.109602, acc 0.96
2016-09-07T20:48:11.857388: step 3784, loss 0.0426615, acc 1
2016-09-07T20:48:12.527149: step 3785, loss 0.0323951, acc 0.98
2016-09-07T20:48:13.194710: step 3786, loss 0.0443766, acc 0.98
2016-09-07T20:48:13.861721: step 3787, loss 0.0228532, acc 1
2016-09-07T20:48:14.540391: step 3788, loss 0.0497082, acc 0.98
2016-09-07T20:48:15.219094: step 3789, loss 0.0323255, acc 1
2016-09-07T20:48:15.886698: step 3790, loss 0.0128777, acc 1
2016-09-07T20:48:16.558479: step 3791, loss 0.0789421, acc 0.98
2016-09-07T20:48:17.214785: step 3792, loss 0.049012, acc 0.96
2016-09-07T20:48:17.891563: step 3793, loss 0.0327034, acc 0.98
2016-09-07T20:48:18.561861: step 3794, loss 0.0114364, acc 1
2016-09-07T20:48:19.228428: step 3795, loss 0.0259871, acc 1
2016-09-07T20:48:19.913434: step 3796, loss 0.00620269, acc 1
2016-09-07T20:48:20.583658: step 3797, loss 0.0366103, acc 0.98
2016-09-07T20:48:21.265840: step 3798, loss 0.0182629, acc 1
2016-09-07T20:48:21.937234: step 3799, loss 0.0418347, acc 0.98
2016-09-07T20:48:22.605016: step 3800, loss 0.0196834, acc 0.98

Evaluation:
2016-09-07T20:48:25.532106: step 3800, loss 1.83261, acc 0.733

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-3800

2016-09-07T20:48:27.214816: step 3801, loss 0.00602319, acc 1
2016-09-07T20:48:27.882704: step 3802, loss 0.109581, acc 0.9
2016-09-07T20:48:28.573807: step 3803, loss 0.0188282, acc 1
2016-09-07T20:48:29.247019: step 3804, loss 0.0627639, acc 0.96
2016-09-07T20:48:29.921656: step 3805, loss 0.0044985, acc 1
2016-09-07T20:48:30.608199: step 3806, loss 0.28302, acc 0.94
2016-09-07T20:48:31.276863: step 3807, loss 0.016371, acc 1
2016-09-07T20:48:31.945458: step 3808, loss 0.0235281, acc 1
2016-09-07T20:48:32.610571: step 3809, loss 0.00665155, acc 1
2016-09-07T20:48:33.281644: step 3810, loss 0.019001, acc 0.98
2016-09-07T20:48:33.957062: step 3811, loss 0.0516129, acc 0.98
2016-09-07T20:48:34.633551: step 3812, loss 0.0139532, acc 1
2016-09-07T20:48:35.286528: step 3813, loss 0.0119867, acc 1
2016-09-07T20:48:35.985738: step 3814, loss 0.0748189, acc 0.96
2016-09-07T20:48:36.660772: step 3815, loss 0.0213923, acc 0.98
2016-09-07T20:48:37.348943: step 3816, loss 0.106761, acc 0.96
2016-09-07T20:48:38.035755: step 3817, loss 0.0322099, acc 1
2016-09-07T20:48:38.753345: step 3818, loss 0.0373111, acc 0.98
2016-09-07T20:48:39.423696: step 3819, loss 0.0384052, acc 0.98
2016-09-07T20:48:40.129152: step 3820, loss 0.0209553, acc 1
2016-09-07T20:48:40.813739: step 3821, loss 0.0560889, acc 0.98
2016-09-07T20:48:41.467967: step 3822, loss 0.0196648, acc 1
2016-09-07T20:48:42.147683: step 3823, loss 0.0388422, acc 1
2016-09-07T20:48:42.824622: step 3824, loss 0.00366653, acc 1
2016-09-07T20:48:43.501179: step 3825, loss 0.0160927, acc 1
2016-09-07T20:48:44.166873: step 3826, loss 0.0984144, acc 0.92
2016-09-07T20:48:44.820524: step 3827, loss 0.0929245, acc 0.98
2016-09-07T20:48:45.489856: step 3828, loss 0.0311477, acc 0.98
2016-09-07T20:48:46.165249: step 3829, loss 0.0229185, acc 0.98
2016-09-07T20:48:46.839161: step 3830, loss 0.0383609, acc 0.98
2016-09-07T20:48:47.513287: step 3831, loss 0.00362863, acc 1
2016-09-07T20:48:48.188509: step 3832, loss 0.0705748, acc 0.96
2016-09-07T20:48:48.860474: step 3833, loss 0.0107871, acc 1
2016-09-07T20:48:49.533945: step 3834, loss 0.0264965, acc 0.98
2016-09-07T20:48:50.198985: step 3835, loss 0.0176308, acc 1
2016-09-07T20:48:50.878760: step 3836, loss 0.0064075, acc 1
2016-09-07T20:48:51.552326: step 3837, loss 0.0407728, acc 0.98
2016-09-07T20:48:52.229837: step 3838, loss 0.139153, acc 0.94
2016-09-07T20:48:52.910225: step 3839, loss 0.0210928, acc 1
2016-09-07T20:48:53.575027: step 3840, loss 0.0603985, acc 0.96
2016-09-07T20:48:54.252453: step 3841, loss 0.0268943, acc 1
2016-09-07T20:48:54.932091: step 3842, loss 0.0148627, acc 1
2016-09-07T20:48:55.615609: step 3843, loss 0.00342736, acc 1
2016-09-07T20:48:56.299404: step 3844, loss 0.0165733, acc 1
2016-09-07T20:48:56.962173: step 3845, loss 0.0217537, acc 1
2016-09-07T20:48:57.641724: step 3846, loss 0.0150727, acc 1
2016-09-07T20:48:58.304019: step 3847, loss 0.0412422, acc 1
2016-09-07T20:48:58.974722: step 3848, loss 0.0165822, acc 1
2016-09-07T20:48:59.642448: step 3849, loss 0.0033536, acc 1
2016-09-07T20:49:00.340436: step 3850, loss 0.00405177, acc 1
2016-09-07T20:49:01.025954: step 3851, loss 0.0536204, acc 0.96
2016-09-07T20:49:01.686476: step 3852, loss 0.033351, acc 0.98
2016-09-07T20:49:02.350455: step 3853, loss 0.0106628, acc 1
2016-09-07T20:49:03.018519: step 3854, loss 0.051043, acc 0.98
2016-09-07T20:49:03.701357: step 3855, loss 0.0283201, acc 1
2016-09-07T20:49:04.375366: step 3856, loss 0.0870976, acc 0.94
2016-09-07T20:49:05.051530: step 3857, loss 0.0528039, acc 0.98
2016-09-07T20:49:05.718903: step 3858, loss 0.0301244, acc 0.98
2016-09-07T20:49:06.387664: step 3859, loss 0.0748378, acc 0.96
2016-09-07T20:49:07.058380: step 3860, loss 0.00942853, acc 1
2016-09-07T20:49:07.717824: step 3861, loss 0.05063, acc 0.98
2016-09-07T20:49:08.384494: step 3862, loss 0.0356768, acc 0.98
2016-09-07T20:49:09.044966: step 3863, loss 0.107793, acc 0.96
2016-09-07T20:49:09.714667: step 3864, loss 0.00419183, acc 1
2016-09-07T20:49:10.378337: step 3865, loss 0.0362142, acc 1
2016-09-07T20:49:11.039170: step 3866, loss 0.0493934, acc 0.98
2016-09-07T20:49:11.693317: step 3867, loss 0.0207907, acc 0.98
2016-09-07T20:49:12.389922: step 3868, loss 0.0478431, acc 0.98
2016-09-07T20:49:13.081678: step 3869, loss 0.0143127, acc 1
2016-09-07T20:49:13.757093: step 3870, loss 0.0245947, acc 0.98
2016-09-07T20:49:14.429942: step 3871, loss 0.0183595, acc 1
2016-09-07T20:49:15.109217: step 3872, loss 0.018586, acc 1
2016-09-07T20:49:15.779663: step 3873, loss 0.0513006, acc 0.98
2016-09-07T20:49:16.456768: step 3874, loss 0.017943, acc 1
2016-09-07T20:49:17.134702: step 3875, loss 0.0750635, acc 0.96
2016-09-07T20:49:17.799364: step 3876, loss 0.0451171, acc 0.98
2016-09-07T20:49:18.488597: step 3877, loss 0.0211381, acc 1
2016-09-07T20:49:19.157719: step 3878, loss 0.0300921, acc 0.98
2016-09-07T20:49:19.831900: step 3879, loss 0.0253114, acc 1
2016-09-07T20:49:20.182832: step 3880, loss 0.0115893, acc 1
2016-09-07T20:49:20.857679: step 3881, loss 0.0240046, acc 0.98
2016-09-07T20:49:21.528242: step 3882, loss 0.0631285, acc 0.96
2016-09-07T20:49:22.226949: step 3883, loss 0.0311786, acc 1
2016-09-07T20:49:22.898663: step 3884, loss 0.0697078, acc 0.98
2016-09-07T20:49:23.572565: step 3885, loss 0.0545175, acc 0.98
2016-09-07T20:49:24.245047: step 3886, loss 0.0128263, acc 1
2016-09-07T20:49:24.922430: step 3887, loss 0.0299156, acc 0.98
2016-09-07T20:49:25.594882: step 3888, loss 0.0643217, acc 0.96
2016-09-07T20:49:26.265820: step 3889, loss 0.0387672, acc 0.98
2016-09-07T20:49:26.933941: step 3890, loss 0.0160541, acc 1
2016-09-07T20:49:27.585515: step 3891, loss 0.00375559, acc 1
2016-09-07T20:49:28.256704: step 3892, loss 0.0303046, acc 1
2016-09-07T20:49:28.919745: step 3893, loss 0.0898421, acc 0.98
2016-09-07T20:49:29.594755: step 3894, loss 0.0226825, acc 0.98
2016-09-07T20:49:30.254075: step 3895, loss 0.0212192, acc 0.98
2016-09-07T20:49:30.916330: step 3896, loss 0.0267291, acc 0.98
2016-09-07T20:49:31.563852: step 3897, loss 0.0630573, acc 0.98
2016-09-07T20:49:32.234282: step 3898, loss 0.00595233, acc 1
2016-09-07T20:49:32.908324: step 3899, loss 0.0463197, acc 0.98
2016-09-07T20:49:33.580266: step 3900, loss 0.0532282, acc 0.98

Evaluation:
2016-09-07T20:49:36.485299: step 3900, loss 1.70792, acc 0.745

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-3900

2016-09-07T20:49:38.156467: step 3901, loss 0.0241065, acc 1
2016-09-07T20:49:38.832476: step 3902, loss 0.0451265, acc 1
2016-09-07T20:49:39.510047: step 3903, loss 0.0455477, acc 0.98
2016-09-07T20:49:40.189764: step 3904, loss 0.00488003, acc 1
2016-09-07T20:49:40.857498: step 3905, loss 0.0495329, acc 0.96
2016-09-07T20:49:41.520115: step 3906, loss 0.0115856, acc 1
2016-09-07T20:49:42.205767: step 3907, loss 0.00551598, acc 1
2016-09-07T20:49:42.889832: step 3908, loss 0.0746608, acc 0.96
2016-09-07T20:49:43.562990: step 3909, loss 0.0101305, acc 1
2016-09-07T20:49:44.222344: step 3910, loss 0.0297355, acc 1
2016-09-07T20:49:44.884947: step 3911, loss 0.0384301, acc 0.98
2016-09-07T20:49:45.545541: step 3912, loss 0.151144, acc 0.96
2016-09-07T20:49:46.220195: step 3913, loss 0.0167765, acc 1
2016-09-07T20:49:46.896510: step 3914, loss 0.0666038, acc 0.98
2016-09-07T20:49:47.565269: step 3915, loss 0.0269579, acc 0.98
2016-09-07T20:49:48.236423: step 3916, loss 0.0536063, acc 0.98
2016-09-07T20:49:48.908877: step 3917, loss 0.0158761, acc 1
2016-09-07T20:49:49.558818: step 3918, loss 0.0796264, acc 0.98
2016-09-07T20:49:50.223946: step 3919, loss 0.00511262, acc 1
2016-09-07T20:49:50.901297: step 3920, loss 0.00745505, acc 1
2016-09-07T20:49:51.558704: step 3921, loss 0.0470933, acc 1
2016-09-07T20:49:52.231008: step 3922, loss 0.131653, acc 0.96
2016-09-07T20:49:52.895117: step 3923, loss 0.0203462, acc 1
2016-09-07T20:49:53.551185: step 3924, loss 0.0687489, acc 0.98
2016-09-07T20:49:54.231114: step 3925, loss 0.0320763, acc 1
2016-09-07T20:49:54.915605: step 3926, loss 0.0303202, acc 0.98
2016-09-07T20:49:55.600115: step 3927, loss 0.0931751, acc 0.96
2016-09-07T20:49:56.265667: step 3928, loss 0.0102789, acc 1
2016-09-07T20:49:56.939857: step 3929, loss 0.02803, acc 0.98
2016-09-07T20:49:57.601369: step 3930, loss 0.0422357, acc 1
2016-09-07T20:49:58.273065: step 3931, loss 0.032385, acc 1
2016-09-07T20:49:58.934772: step 3932, loss 0.0398599, acc 0.98
2016-09-07T20:49:59.616599: step 3933, loss 0.0127224, acc 1
2016-09-07T20:50:00.312810: step 3934, loss 0.0532708, acc 0.98
2016-09-07T20:50:00.969240: step 3935, loss 0.0583293, acc 0.98
2016-09-07T20:50:01.629110: step 3936, loss 0.0431228, acc 0.98
2016-09-07T20:50:02.311451: step 3937, loss 0.0236933, acc 1
2016-09-07T20:50:02.986295: step 3938, loss 0.00357649, acc 1
2016-09-07T20:50:03.654412: step 3939, loss 0.0733146, acc 0.98
2016-09-07T20:50:04.331151: step 3940, loss 0.00915071, acc 1
2016-09-07T20:50:04.995218: step 3941, loss 0.0406947, acc 1
2016-09-07T20:50:05.658196: step 3942, loss 0.0195454, acc 1
2016-09-07T20:50:06.320024: step 3943, loss 0.0411532, acc 0.98
2016-09-07T20:50:06.982319: step 3944, loss 0.0359705, acc 0.98
2016-09-07T20:50:07.641500: step 3945, loss 0.0564157, acc 0.98
2016-09-07T20:50:08.325517: step 3946, loss 0.0608986, acc 0.96
2016-09-07T20:50:08.991304: step 3947, loss 0.0154925, acc 1
2016-09-07T20:50:09.657383: step 3948, loss 0.046783, acc 0.98
2016-09-07T20:50:10.324531: step 3949, loss 0.0403695, acc 0.98
2016-09-07T20:50:10.997526: step 3950, loss 0.0626714, acc 0.98
2016-09-07T20:50:11.679107: step 3951, loss 0.0443243, acc 0.98
2016-09-07T20:50:12.375711: step 3952, loss 0.0451136, acc 0.98
2016-09-07T20:50:13.046924: step 3953, loss 0.160966, acc 0.96
2016-09-07T20:50:13.723412: step 3954, loss 0.0779947, acc 0.98
2016-09-07T20:50:14.386674: step 3955, loss 0.0335992, acc 0.98
2016-09-07T20:50:15.048451: step 3956, loss 0.0634521, acc 0.96
2016-09-07T20:50:15.714882: step 3957, loss 0.205072, acc 0.96
2016-09-07T20:50:16.387000: step 3958, loss 0.0324604, acc 0.98
2016-09-07T20:50:17.045628: step 3959, loss 0.0359992, acc 1
2016-09-07T20:50:17.735009: step 3960, loss 0.0244233, acc 1
2016-09-07T20:50:18.390877: step 3961, loss 0.0244733, acc 1
2016-09-07T20:50:19.067050: step 3962, loss 0.0102023, acc 1
2016-09-07T20:50:19.747934: step 3963, loss 0.00972735, acc 1
2016-09-07T20:50:20.404564: step 3964, loss 0.024492, acc 1
2016-09-07T20:50:21.076402: step 3965, loss 0.14966, acc 0.98
2016-09-07T20:50:21.750621: step 3966, loss 0.0674099, acc 0.96
2016-09-07T20:50:22.408847: step 3967, loss 0.0129634, acc 1
2016-09-07T20:50:23.089325: step 3968, loss 0.021015, acc 1
2016-09-07T20:50:23.770555: step 3969, loss 0.113948, acc 0.96
2016-09-07T20:50:24.442420: step 3970, loss 0.0873017, acc 0.96
2016-09-07T20:50:25.118396: step 3971, loss 0.0410966, acc 0.98
2016-09-07T20:50:25.790081: step 3972, loss 0.0789605, acc 0.98
2016-09-07T20:50:26.465572: step 3973, loss 0.00495854, acc 1
2016-09-07T20:50:27.117219: step 3974, loss 0.040779, acc 0.96
2016-09-07T20:50:27.780986: step 3975, loss 0.0198239, acc 1
2016-09-07T20:50:28.464024: step 3976, loss 0.103336, acc 0.96
2016-09-07T20:50:29.113047: step 3977, loss 0.00542466, acc 1
2016-09-07T20:50:29.774604: step 3978, loss 0.102108, acc 0.98
2016-09-07T20:50:30.444493: step 3979, loss 0.0373379, acc 0.98
2016-09-07T20:50:31.106297: step 3980, loss 0.0467159, acc 0.98
2016-09-07T20:50:31.762585: step 3981, loss 0.0221881, acc 1
2016-09-07T20:50:32.435253: step 3982, loss 0.0352727, acc 0.98
2016-09-07T20:50:33.095914: step 3983, loss 0.0309867, acc 0.98
2016-09-07T20:50:33.793378: step 3984, loss 0.00983983, acc 1
2016-09-07T20:50:34.461959: step 3985, loss 0.0718849, acc 0.98
2016-09-07T20:50:35.137954: step 3986, loss 0.0666146, acc 0.98
2016-09-07T20:50:35.831500: step 3987, loss 0.0299848, acc 1
2016-09-07T20:50:36.498779: step 3988, loss 0.099473, acc 0.94
2016-09-07T20:50:37.162938: step 3989, loss 0.0605202, acc 0.98
2016-09-07T20:50:37.833125: step 3990, loss 0.0458146, acc 0.96
2016-09-07T20:50:38.492963: step 3991, loss 0.0694027, acc 0.98
2016-09-07T20:50:39.174559: step 3992, loss 0.0120499, acc 1
2016-09-07T20:50:39.862289: step 3993, loss 0.0562523, acc 0.98
2016-09-07T20:50:40.528862: step 3994, loss 0.0418436, acc 0.98
2016-09-07T20:50:41.196388: step 3995, loss 0.010371, acc 1
2016-09-07T20:50:41.883917: step 3996, loss 0.0182602, acc 1
2016-09-07T20:50:42.549763: step 3997, loss 0.0235659, acc 1
2016-09-07T20:50:43.229719: step 3998, loss 0.0220903, acc 1
2016-09-07T20:50:43.894434: step 3999, loss 0.0740712, acc 0.98
2016-09-07T20:50:44.570398: step 4000, loss 0.0784708, acc 0.96

Evaluation:
2016-09-07T20:50:47.496449: step 4000, loss 1.641, acc 0.726

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-4000

2016-09-07T20:50:49.164082: step 4001, loss 0.0704121, acc 0.96
2016-09-07T20:50:49.847729: step 4002, loss 0.102705, acc 0.94
2016-09-07T20:50:50.514244: step 4003, loss 0.0216304, acc 1
2016-09-07T20:50:51.182222: step 4004, loss 0.00661803, acc 1
2016-09-07T20:50:51.860076: step 4005, loss 0.0451269, acc 0.98
2016-09-07T20:50:52.532759: step 4006, loss 0.0131629, acc 1
2016-09-07T20:50:53.196793: step 4007, loss 0.0455962, acc 0.98
2016-09-07T20:50:53.908296: step 4008, loss 0.0494352, acc 0.96
2016-09-07T20:50:54.582695: step 4009, loss 0.00766965, acc 1
2016-09-07T20:50:55.263337: step 4010, loss 0.0284772, acc 1
2016-09-07T20:50:55.936207: step 4011, loss 0.0442693, acc 0.98
2016-09-07T20:50:56.605987: step 4012, loss 0.0527975, acc 0.98
2016-09-07T20:50:57.285011: step 4013, loss 0.00617164, acc 1
2016-09-07T20:50:57.954460: step 4014, loss 0.106045, acc 0.96
2016-09-07T20:50:58.643752: step 4015, loss 0.0513135, acc 1
2016-09-07T20:50:59.297164: step 4016, loss 0.0460962, acc 0.98
2016-09-07T20:50:59.967552: step 4017, loss 0.00781527, acc 1
2016-09-07T20:51:00.690332: step 4018, loss 0.00414772, acc 1
2016-09-07T20:51:01.377837: step 4019, loss 0.0975915, acc 0.98
2016-09-07T20:51:02.052565: step 4020, loss 0.0185004, acc 1
2016-09-07T20:51:02.735275: step 4021, loss 0.0455652, acc 0.98
2016-09-07T20:51:03.391654: step 4022, loss 0.12841, acc 0.96
2016-09-07T20:51:04.068994: step 4023, loss 0.0407315, acc 0.98
2016-09-07T20:51:04.731099: step 4024, loss 0.0986249, acc 0.98
2016-09-07T20:51:05.392904: step 4025, loss 0.0601379, acc 0.96
2016-09-07T20:51:06.054186: step 4026, loss 0.0135845, acc 1
2016-09-07T20:51:06.721737: step 4027, loss 0.0503324, acc 1
2016-09-07T20:51:07.376956: step 4028, loss 0.0286362, acc 1
2016-09-07T20:51:08.044252: step 4029, loss 0.050576, acc 0.96
2016-09-07T20:51:08.716017: step 4030, loss 0.0054693, acc 1
2016-09-07T20:51:09.393549: step 4031, loss 0.0187952, acc 1
2016-09-07T20:51:10.050870: step 4032, loss 0.0534822, acc 0.98
2016-09-07T20:51:10.727133: step 4033, loss 0.0702937, acc 0.98
2016-09-07T20:51:11.403234: step 4034, loss 0.0368468, acc 0.98
2016-09-07T20:51:12.068766: step 4035, loss 0.0356774, acc 0.98
2016-09-07T20:51:12.743961: step 4036, loss 0.0511204, acc 0.98
2016-09-07T20:51:13.392821: step 4037, loss 0.090955, acc 0.92
2016-09-07T20:51:14.071734: step 4038, loss 0.0267528, acc 0.98
2016-09-07T20:51:14.740356: step 4039, loss 0.030285, acc 1
2016-09-07T20:51:15.408943: step 4040, loss 0.01997, acc 0.98
2016-09-07T20:51:16.080408: step 4041, loss 0.0226477, acc 1
2016-09-07T20:51:16.752028: step 4042, loss 0.07252, acc 0.96
2016-09-07T20:51:17.429753: step 4043, loss 0.00881636, acc 1
2016-09-07T20:51:18.114106: step 4044, loss 0.0299936, acc 0.98
2016-09-07T20:51:18.785576: step 4045, loss 0.0212649, acc 1
2016-09-07T20:51:19.445321: step 4046, loss 0.0286707, acc 0.98
2016-09-07T20:51:20.120188: step 4047, loss 0.0218231, acc 1
2016-09-07T20:51:20.806273: step 4048, loss 0.0193041, acc 1
2016-09-07T20:51:21.487692: step 4049, loss 0.0519021, acc 0.98
2016-09-07T20:51:22.175336: step 4050, loss 0.042441, acc 0.98
2016-09-07T20:51:22.857870: step 4051, loss 0.0289969, acc 0.98
2016-09-07T20:51:23.519535: step 4052, loss 0.0373042, acc 0.98
2016-09-07T20:51:24.185113: step 4053, loss 0.107772, acc 0.96
2016-09-07T20:51:24.869315: step 4054, loss 0.00421171, acc 1
2016-09-07T20:51:25.535281: step 4055, loss 0.0201888, acc 1
2016-09-07T20:51:26.226201: step 4056, loss 0.076869, acc 0.96
2016-09-07T20:51:26.902295: step 4057, loss 0.0265249, acc 0.98
2016-09-07T20:51:27.568820: step 4058, loss 0.0430891, acc 0.98
2016-09-07T20:51:28.236992: step 4059, loss 0.017043, acc 1
2016-09-07T20:51:28.911441: step 4060, loss 0.0207604, acc 0.98
2016-09-07T20:51:29.600087: step 4061, loss 0.040297, acc 0.98
2016-09-07T20:51:30.282126: step 4062, loss 0.0568074, acc 0.98
2016-09-07T20:51:30.967297: step 4063, loss 0.0394169, acc 0.96
2016-09-07T20:51:31.635732: step 4064, loss 0.00423745, acc 1
2016-09-07T20:51:32.315802: step 4065, loss 0.0113621, acc 1
2016-09-07T20:51:32.991696: step 4066, loss 0.00575856, acc 1
2016-09-07T20:51:33.667641: step 4067, loss 0.0370456, acc 0.98
2016-09-07T20:51:34.331407: step 4068, loss 0.00503413, acc 1
2016-09-07T20:51:35.011024: step 4069, loss 0.00659316, acc 1
2016-09-07T20:51:35.693424: step 4070, loss 0.0677282, acc 0.96
2016-09-07T20:51:36.367043: step 4071, loss 0.0436596, acc 0.96
2016-09-07T20:51:37.032897: step 4072, loss 0.0301, acc 0.98
2016-09-07T20:51:37.719812: step 4073, loss 0.0341778, acc 0.98
2016-09-07T20:51:38.111896: step 4074, loss 0.121426, acc 0.916667
2016-09-07T20:51:38.774617: step 4075, loss 0.0382735, acc 1
2016-09-07T20:51:39.461178: step 4076, loss 0.055435, acc 0.98
2016-09-07T20:51:40.105339: step 4077, loss 0.0312298, acc 1
2016-09-07T20:51:40.763884: step 4078, loss 0.00745763, acc 1
2016-09-07T20:51:41.443688: step 4079, loss 0.063039, acc 0.96
2016-09-07T20:51:42.113112: step 4080, loss 0.048868, acc 0.96
2016-09-07T20:51:42.786041: step 4081, loss 0.00447937, acc 1
2016-09-07T20:51:43.466543: step 4082, loss 0.0167457, acc 1
2016-09-07T20:51:44.147538: step 4083, loss 0.0215191, acc 0.98
2016-09-07T20:51:44.799647: step 4084, loss 0.0409076, acc 0.98
2016-09-07T20:51:45.466432: step 4085, loss 0.0162763, acc 1
2016-09-07T20:51:46.136507: step 4086, loss 0.0404865, acc 0.96
2016-09-07T20:51:46.785699: step 4087, loss 0.0384225, acc 1
2016-09-07T20:51:47.463274: step 4088, loss 0.0354554, acc 0.98
2016-09-07T20:51:48.133110: step 4089, loss 0.0126882, acc 1
2016-09-07T20:51:48.798594: step 4090, loss 0.00547895, acc 1
2016-09-07T20:51:49.460141: step 4091, loss 0.0282928, acc 1
2016-09-07T20:51:50.131832: step 4092, loss 0.0449909, acc 0.98
2016-09-07T20:51:50.798926: step 4093, loss 0.0158201, acc 1
2016-09-07T20:51:51.459104: step 4094, loss 0.0396461, acc 0.98
2016-09-07T20:51:52.128362: step 4095, loss 0.0362012, acc 0.98
2016-09-07T20:51:52.798107: step 4096, loss 0.0158899, acc 1
2016-09-07T20:51:53.475926: step 4097, loss 0.0622073, acc 0.98
2016-09-07T20:51:54.123574: step 4098, loss 0.00952308, acc 1
2016-09-07T20:51:54.777986: step 4099, loss 0.01737, acc 1
2016-09-07T20:51:55.473075: step 4100, loss 0.0167984, acc 1

Evaluation:
2016-09-07T20:51:58.418017: step 4100, loss 1.78726, acc 0.732

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-4100

2016-09-07T20:52:00.086586: step 4101, loss 0.0210536, acc 0.98
2016-09-07T20:52:00.792024: step 4102, loss 0.00463399, acc 1
2016-09-07T20:52:01.468907: step 4103, loss 0.0293667, acc 0.98
2016-09-07T20:52:02.132323: step 4104, loss 0.0249233, acc 0.98
2016-09-07T20:52:02.790133: step 4105, loss 0.0209107, acc 0.98
2016-09-07T20:52:03.458638: step 4106, loss 0.0692678, acc 0.96
2016-09-07T20:52:04.137954: step 4107, loss 0.0202831, acc 1
2016-09-07T20:52:04.803146: step 4108, loss 0.00719662, acc 1
2016-09-07T20:52:05.475294: step 4109, loss 0.00362723, acc 1
2016-09-07T20:52:06.153253: step 4110, loss 0.0209475, acc 0.98
2016-09-07T20:52:06.823507: step 4111, loss 0.00608058, acc 1
2016-09-07T20:52:07.493012: step 4112, loss 0.00457158, acc 1
2016-09-07T20:52:08.155113: step 4113, loss 0.0203532, acc 0.98
2016-09-07T20:52:08.808266: step 4114, loss 0.0421615, acc 0.98
2016-09-07T20:52:09.484528: step 4115, loss 0.0193142, acc 1
2016-09-07T20:52:10.180906: step 4116, loss 0.0278323, acc 0.98
2016-09-07T20:52:10.866055: step 4117, loss 0.00471487, acc 1
2016-09-07T20:52:11.533321: step 4118, loss 0.0174197, acc 1
2016-09-07T20:52:12.213383: step 4119, loss 0.0413602, acc 0.98
2016-09-07T20:52:12.880227: step 4120, loss 0.0379829, acc 1
2016-09-07T20:52:13.574953: step 4121, loss 0.0160702, acc 1
2016-09-07T20:52:14.238194: step 4122, loss 0.0689219, acc 0.96
2016-09-07T20:52:14.918928: step 4123, loss 0.0176999, acc 0.98
2016-09-07T20:52:15.585961: step 4124, loss 0.0150885, acc 1
2016-09-07T20:52:16.270601: step 4125, loss 0.0333165, acc 0.98
2016-09-07T20:52:16.936326: step 4126, loss 0.0148074, acc 1
2016-09-07T20:52:17.612222: step 4127, loss 0.0181674, acc 0.98
2016-09-07T20:52:18.295691: step 4128, loss 0.0209386, acc 0.98
2016-09-07T20:52:18.975393: step 4129, loss 0.0373054, acc 1
2016-09-07T20:52:19.634515: step 4130, loss 0.059096, acc 0.98
2016-09-07T20:52:20.312186: step 4131, loss 0.041114, acc 0.96
2016-09-07T20:52:20.980769: step 4132, loss 0.0230659, acc 1
2016-09-07T20:52:21.649909: step 4133, loss 0.00775516, acc 1
2016-09-07T20:52:22.311694: step 4134, loss 0.0305366, acc 0.98
2016-09-07T20:52:22.988204: step 4135, loss 0.0401154, acc 0.98
2016-09-07T20:52:23.675428: step 4136, loss 0.0404341, acc 0.98
2016-09-07T20:52:24.328511: step 4137, loss 0.0506101, acc 0.96
2016-09-07T20:52:24.978323: step 4138, loss 0.0101198, acc 1
2016-09-07T20:52:25.642806: step 4139, loss 0.0347432, acc 0.96
2016-09-07T20:52:26.316447: step 4140, loss 0.0387028, acc 0.98
2016-09-07T20:52:26.985575: step 4141, loss 0.0122564, acc 1
2016-09-07T20:52:27.652681: step 4142, loss 0.00956306, acc 1
2016-09-07T20:52:28.330309: step 4143, loss 0.0472146, acc 0.98
2016-09-07T20:52:29.013909: step 4144, loss 0.0146068, acc 1
2016-09-07T20:52:29.675486: step 4145, loss 0.0122769, acc 1
2016-09-07T20:52:30.355085: step 4146, loss 0.0226528, acc 0.98
2016-09-07T20:52:31.036801: step 4147, loss 0.0302313, acc 1
2016-09-07T20:52:31.702149: step 4148, loss 0.0134348, acc 1
2016-09-07T20:52:32.373007: step 4149, loss 0.00834004, acc 1
2016-09-07T20:52:33.040297: step 4150, loss 0.0316695, acc 0.98
2016-09-07T20:52:33.725239: step 4151, loss 0.137537, acc 0.96
2016-09-07T20:52:34.413958: step 4152, loss 0.0316559, acc 0.98
2016-09-07T20:52:35.109399: step 4153, loss 0.0304701, acc 1
2016-09-07T20:52:35.785434: step 4154, loss 0.0309002, acc 1
2016-09-07T20:52:36.475457: step 4155, loss 0.0765115, acc 0.96
2016-09-07T20:52:37.153160: step 4156, loss 0.00512568, acc 1
2016-09-07T20:52:37.836883: step 4157, loss 0.0245031, acc 1
2016-09-07T20:52:38.509589: step 4158, loss 0.010732, acc 1
2016-09-07T20:52:39.195945: step 4159, loss 0.00308939, acc 1
2016-09-07T20:52:39.866547: step 4160, loss 0.00515141, acc 1
2016-09-07T20:52:40.539624: step 4161, loss 0.048735, acc 0.98
2016-09-07T20:52:41.219984: step 4162, loss 0.0227553, acc 0.98
2016-09-07T20:52:41.871446: step 4163, loss 0.0114882, acc 1
2016-09-07T20:52:42.539635: step 4164, loss 0.0390184, acc 1
2016-09-07T20:52:43.216046: step 4165, loss 0.0335406, acc 0.98
2016-09-07T20:52:43.898530: step 4166, loss 0.060811, acc 0.98
2016-09-07T20:52:44.583370: step 4167, loss 0.0665374, acc 0.98
2016-09-07T20:52:45.255355: step 4168, loss 0.00556848, acc 1
2016-09-07T20:52:45.914904: step 4169, loss 0.0583222, acc 0.98
2016-09-07T20:52:46.596425: step 4170, loss 0.00655395, acc 1
2016-09-07T20:52:47.269247: step 4171, loss 0.00347489, acc 1
2016-09-07T20:52:47.935253: step 4172, loss 0.0456469, acc 0.98
2016-09-07T20:52:48.617626: step 4173, loss 0.0783472, acc 0.94
2016-09-07T20:52:49.299697: step 4174, loss 0.00742587, acc 1
2016-09-07T20:52:49.968829: step 4175, loss 0.0112439, acc 1
2016-09-07T20:52:50.634049: step 4176, loss 0.0329854, acc 0.98
2016-09-07T20:52:51.308276: step 4177, loss 0.0252342, acc 1
2016-09-07T20:52:51.982610: step 4178, loss 0.0551832, acc 0.96
2016-09-07T20:52:52.675019: step 4179, loss 0.0179944, acc 1
2016-09-07T20:52:53.352408: step 4180, loss 0.0150757, acc 1
2016-09-07T20:52:54.027678: step 4181, loss 0.0192608, acc 0.98
2016-09-07T20:52:54.686318: step 4182, loss 0.0440518, acc 0.96
2016-09-07T20:52:55.364441: step 4183, loss 0.0178735, acc 1
2016-09-07T20:52:56.068316: step 4184, loss 0.00365907, acc 1
2016-09-07T20:52:56.726895: step 4185, loss 0.0158784, acc 1
2016-09-07T20:52:57.392730: step 4186, loss 0.00713028, acc 1
2016-09-07T20:52:58.071820: step 4187, loss 0.0378798, acc 0.98
2016-09-07T20:52:58.760109: step 4188, loss 0.0211267, acc 1
2016-09-07T20:52:59.456121: step 4189, loss 0.0144982, acc 1
2016-09-07T20:53:00.143651: step 4190, loss 0.0599761, acc 0.94
2016-09-07T20:53:00.848338: step 4191, loss 0.0947077, acc 0.94
2016-09-07T20:53:01.520279: step 4192, loss 0.0177769, acc 1
2016-09-07T20:53:02.205819: step 4193, loss 0.0357106, acc 0.98
2016-09-07T20:53:02.863825: step 4194, loss 0.0365196, acc 1
2016-09-07T20:53:03.538114: step 4195, loss 0.0373098, acc 0.98
2016-09-07T20:53:04.211991: step 4196, loss 0.00585132, acc 1
2016-09-07T20:53:04.890208: step 4197, loss 0.0213723, acc 0.98
2016-09-07T20:53:05.578909: step 4198, loss 0.0322861, acc 0.98
2016-09-07T20:53:06.249799: step 4199, loss 0.0292514, acc 0.98
2016-09-07T20:53:06.925404: step 4200, loss 0.0333512, acc 0.98

Evaluation:
2016-09-07T20:53:09.857908: step 4200, loss 1.75646, acc 0.743

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-4200

2016-09-07T20:53:11.562675: step 4201, loss 0.104853, acc 0.98
2016-09-07T20:53:12.257609: step 4202, loss 0.00423845, acc 1
2016-09-07T20:53:12.944005: step 4203, loss 0.0166074, acc 1
2016-09-07T20:53:13.618151: step 4204, loss 0.0801728, acc 0.96
2016-09-07T20:53:14.318959: step 4205, loss 0.0708269, acc 0.96
2016-09-07T20:53:14.989536: step 4206, loss 0.0214636, acc 1
2016-09-07T20:53:15.651277: step 4207, loss 0.0140089, acc 1
2016-09-07T20:53:16.327857: step 4208, loss 0.0361633, acc 0.98
2016-09-07T20:53:17.003671: step 4209, loss 0.0366442, acc 0.98
2016-09-07T20:53:17.678401: step 4210, loss 0.133014, acc 0.94
2016-09-07T20:53:18.338833: step 4211, loss 0.026297, acc 1
2016-09-07T20:53:19.022624: step 4212, loss 0.00276434, acc 1
2016-09-07T20:53:19.689570: step 4213, loss 0.066423, acc 0.98
2016-09-07T20:53:20.357498: step 4214, loss 0.0245236, acc 1
2016-09-07T20:53:21.003256: step 4215, loss 0.00483597, acc 1
2016-09-07T20:53:21.662921: step 4216, loss 0.024491, acc 1
2016-09-07T20:53:22.323336: step 4217, loss 0.0793882, acc 0.96
2016-09-07T20:53:22.983640: step 4218, loss 0.0664527, acc 0.98
2016-09-07T20:53:23.673969: step 4219, loss 0.0220875, acc 1
2016-09-07T20:53:24.329082: step 4220, loss 0.011564, acc 1
2016-09-07T20:53:25.006255: step 4221, loss 0.0870595, acc 0.96
2016-09-07T20:53:25.675698: step 4222, loss 0.0376803, acc 0.98
2016-09-07T20:53:26.344654: step 4223, loss 0.123233, acc 0.96
2016-09-07T20:53:27.003686: step 4224, loss 0.0208547, acc 1
2016-09-07T20:53:27.686502: step 4225, loss 0.123442, acc 0.96
2016-09-07T20:53:28.378979: step 4226, loss 0.0608805, acc 0.98
2016-09-07T20:53:29.069620: step 4227, loss 0.0256373, acc 1
2016-09-07T20:53:29.730691: step 4228, loss 0.0893282, acc 0.96
2016-09-07T20:53:30.397501: step 4229, loss 0.0294903, acc 0.98
2016-09-07T20:53:31.053937: step 4230, loss 0.108193, acc 0.98
2016-09-07T20:53:31.708182: step 4231, loss 0.0157893, acc 1
2016-09-07T20:53:32.350472: step 4232, loss 0.021325, acc 0.98
2016-09-07T20:53:33.034141: step 4233, loss 0.0065759, acc 1
2016-09-07T20:53:33.714123: step 4234, loss 0.0136682, acc 1
2016-09-07T20:53:34.390807: step 4235, loss 0.0452842, acc 1
2016-09-07T20:53:35.052149: step 4236, loss 0.0328383, acc 0.98
2016-09-07T20:53:35.712973: step 4237, loss 0.0286932, acc 1
2016-09-07T20:53:36.382829: step 4238, loss 0.0197056, acc 1
2016-09-07T20:53:37.060272: step 4239, loss 0.0392624, acc 0.98
2016-09-07T20:53:37.735064: step 4240, loss 0.0298976, acc 1
2016-09-07T20:53:38.400150: step 4241, loss 0.0325928, acc 1
2016-09-07T20:53:39.075162: step 4242, loss 0.0211562, acc 0.98
2016-09-07T20:53:39.729198: step 4243, loss 0.0295472, acc 0.98
2016-09-07T20:53:40.396123: step 4244, loss 0.111848, acc 0.98
2016-09-07T20:53:41.072048: step 4245, loss 0.0553028, acc 0.98
2016-09-07T20:53:41.754874: step 4246, loss 0.0368579, acc 0.98
2016-09-07T20:53:42.460579: step 4247, loss 0.010171, acc 1
2016-09-07T20:53:43.132358: step 4248, loss 0.0173195, acc 1
2016-09-07T20:53:43.811128: step 4249, loss 0.0835971, acc 0.96
2016-09-07T20:53:44.485411: step 4250, loss 0.0384687, acc 1
2016-09-07T20:53:45.143156: step 4251, loss 0.0334352, acc 0.98
2016-09-07T20:53:45.818819: step 4252, loss 0.00369071, acc 1
2016-09-07T20:53:46.486054: step 4253, loss 0.00371018, acc 1
2016-09-07T20:53:47.153304: step 4254, loss 0.0211292, acc 0.98
2016-09-07T20:53:47.819468: step 4255, loss 0.0223381, acc 1
2016-09-07T20:53:48.491047: step 4256, loss 0.00498124, acc 1
2016-09-07T20:53:49.166165: step 4257, loss 0.0210091, acc 0.98
2016-09-07T20:53:49.840014: step 4258, loss 0.0773077, acc 0.96
2016-09-07T20:53:50.529950: step 4259, loss 0.070454, acc 0.96
2016-09-07T20:53:51.208389: step 4260, loss 0.0250043, acc 1
2016-09-07T20:53:51.885308: step 4261, loss 0.0282097, acc 0.98
2016-09-07T20:53:52.551390: step 4262, loss 0.00372186, acc 1
2016-09-07T20:53:53.210388: step 4263, loss 0.0824344, acc 0.98
2016-09-07T20:53:53.869032: step 4264, loss 0.0143013, acc 1
2016-09-07T20:53:54.561610: step 4265, loss 0.113337, acc 0.98
2016-09-07T20:53:55.213989: step 4266, loss 0.00702733, acc 1
2016-09-07T20:53:55.868867: step 4267, loss 0.00386136, acc 1
2016-09-07T20:53:56.245472: step 4268, loss 0.0239911, acc 1
2016-09-07T20:53:56.930980: step 4269, loss 0.0210032, acc 1
2016-09-07T20:53:57.583356: step 4270, loss 0.0086609, acc 1
2016-09-07T20:53:58.242416: step 4271, loss 0.0633, acc 0.96
2016-09-07T20:53:58.916065: step 4272, loss 0.00561001, acc 1
2016-09-07T20:53:59.583276: step 4273, loss 0.0204601, acc 0.98
2016-09-07T20:54:00.259616: step 4274, loss 0.0494378, acc 0.98
2016-09-07T20:54:00.917858: step 4275, loss 0.0184258, acc 0.98
2016-09-07T20:54:01.569254: step 4276, loss 0.0554034, acc 0.98
2016-09-07T20:54:02.240476: step 4277, loss 0.0111911, acc 1
2016-09-07T20:54:02.941167: step 4278, loss 0.0732585, acc 0.96
2016-09-07T20:54:03.625930: step 4279, loss 0.0662529, acc 0.98
2016-09-07T20:54:04.289229: step 4280, loss 0.0696512, acc 0.98
2016-09-07T20:54:04.953897: step 4281, loss 0.0396111, acc 0.98
2016-09-07T20:54:05.630453: step 4282, loss 0.0128025, acc 1
2016-09-07T20:54:06.308648: step 4283, loss 0.030675, acc 0.98
2016-09-07T20:54:06.988358: step 4284, loss 0.00416859, acc 1
2016-09-07T20:54:07.633385: step 4285, loss 0.0887102, acc 0.96
2016-09-07T20:54:08.285608: step 4286, loss 0.0103872, acc 1
2016-09-07T20:54:08.957546: step 4287, loss 0.0706572, acc 0.96
2016-09-07T20:54:09.634687: step 4288, loss 0.021095, acc 1
2016-09-07T20:54:10.299821: step 4289, loss 0.0356902, acc 0.98
2016-09-07T20:54:10.952492: step 4290, loss 0.0273061, acc 1
2016-09-07T20:54:11.626925: step 4291, loss 0.0312976, acc 0.98
2016-09-07T20:54:12.296649: step 4292, loss 0.01199, acc 1
2016-09-07T20:54:12.960261: step 4293, loss 0.0671147, acc 0.96
2016-09-07T20:54:13.633285: step 4294, loss 0.0339987, acc 0.98
2016-09-07T20:54:14.296238: step 4295, loss 0.00784221, acc 1
2016-09-07T20:54:14.956034: step 4296, loss 0.0292481, acc 0.98
2016-09-07T20:54:15.634681: step 4297, loss 0.0218824, acc 0.98
2016-09-07T20:54:16.320526: step 4298, loss 0.00473031, acc 1
2016-09-07T20:54:17.000839: step 4299, loss 0.0323675, acc 0.98
2016-09-07T20:54:17.670501: step 4300, loss 0.0776821, acc 0.96

Evaluation:
2016-09-07T20:54:20.579054: step 4300, loss 1.66068, acc 0.743

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-4300

2016-09-07T20:54:22.194861: step 4301, loss 0.0663246, acc 0.98
2016-09-07T20:54:22.871683: step 4302, loss 0.0166054, acc 1
2016-09-07T20:54:23.544741: step 4303, loss 0.00283505, acc 1
2016-09-07T20:54:24.222221: step 4304, loss 0.0349956, acc 1
2016-09-07T20:54:24.900498: step 4305, loss 0.00793304, acc 1
2016-09-07T20:54:25.560661: step 4306, loss 0.0396968, acc 0.98
2016-09-07T20:54:26.238159: step 4307, loss 0.0041176, acc 1
2016-09-07T20:54:26.917959: step 4308, loss 0.00333917, acc 1
2016-09-07T20:54:27.573411: step 4309, loss 0.0311664, acc 1
2016-09-07T20:54:28.240513: step 4310, loss 0.00859112, acc 1
2016-09-07T20:54:28.908540: step 4311, loss 0.0417994, acc 0.98
2016-09-07T20:54:29.578533: step 4312, loss 0.0117224, acc 1
2016-09-07T20:54:30.248890: step 4313, loss 0.00322446, acc 1
2016-09-07T20:54:30.903478: step 4314, loss 0.0595705, acc 0.96
2016-09-07T20:54:31.566196: step 4315, loss 0.00894761, acc 1
2016-09-07T20:54:32.247496: step 4316, loss 0.0236711, acc 0.98
2016-09-07T20:54:32.912373: step 4317, loss 0.00802372, acc 1
2016-09-07T20:54:33.589912: step 4318, loss 0.0294466, acc 0.98
2016-09-07T20:54:34.256093: step 4319, loss 0.0120558, acc 1
2016-09-07T20:54:34.944263: step 4320, loss 0.0132893, acc 1
2016-09-07T20:54:35.610958: step 4321, loss 0.0162012, acc 1
2016-09-07T20:54:36.280008: step 4322, loss 0.00496244, acc 1
2016-09-07T20:54:36.940295: step 4323, loss 0.0209856, acc 1
2016-09-07T20:54:37.618692: step 4324, loss 0.0182063, acc 1
2016-09-07T20:54:38.293350: step 4325, loss 0.00438644, acc 1
2016-09-07T20:54:38.967673: step 4326, loss 0.00407861, acc 1
2016-09-07T20:54:39.661019: step 4327, loss 0.00574672, acc 1
2016-09-07T20:54:40.322750: step 4328, loss 0.0310934, acc 0.98
2016-09-07T20:54:40.981749: step 4329, loss 0.110392, acc 0.96
2016-09-07T20:54:41.665076: step 4330, loss 0.0651478, acc 0.98
2016-09-07T20:54:42.336667: step 4331, loss 0.0430084, acc 0.98
2016-09-07T20:54:43.009629: step 4332, loss 0.0545471, acc 0.98
2016-09-07T20:54:43.664422: step 4333, loss 0.0330425, acc 1
2016-09-07T20:54:44.342475: step 4334, loss 0.0515991, acc 0.98
2016-09-07T20:54:45.031348: step 4335, loss 0.0215221, acc 1
2016-09-07T20:54:45.695150: step 4336, loss 0.0191529, acc 1
2016-09-07T20:54:46.353067: step 4337, loss 0.00357147, acc 1
2016-09-07T20:54:47.026905: step 4338, loss 0.0539038, acc 0.98
2016-09-07T20:54:47.708878: step 4339, loss 0.0109096, acc 1
2016-09-07T20:54:48.398334: step 4340, loss 0.0193457, acc 1
2016-09-07T20:54:49.073741: step 4341, loss 0.00278389, acc 1
2016-09-07T20:54:49.733499: step 4342, loss 0.0394392, acc 0.98
2016-09-07T20:54:50.406048: step 4343, loss 0.060896, acc 0.98
2016-09-07T20:54:51.083722: step 4344, loss 0.0511388, acc 0.98
2016-09-07T20:54:51.752100: step 4345, loss 0.0207308, acc 1
2016-09-07T20:54:52.418228: step 4346, loss 0.062545, acc 0.96
2016-09-07T20:54:53.083196: step 4347, loss 0.0136867, acc 1
2016-09-07T20:54:53.751529: step 4348, loss 0.0205847, acc 0.98
2016-09-07T20:54:54.400962: step 4349, loss 0.0684815, acc 0.98
2016-09-07T20:54:55.072076: step 4350, loss 0.00616459, acc 1
2016-09-07T20:54:55.729247: step 4351, loss 0.0831002, acc 0.96
2016-09-07T20:54:56.421322: step 4352, loss 0.005325, acc 1
2016-09-07T20:54:57.081753: step 4353, loss 0.00264398, acc 1
2016-09-07T20:54:57.758628: step 4354, loss 0.0203182, acc 0.98
2016-09-07T20:54:58.432293: step 4355, loss 0.0296544, acc 0.98
2016-09-07T20:54:59.096693: step 4356, loss 0.0121035, acc 1
2016-09-07T20:54:59.763100: step 4357, loss 0.0206188, acc 1
2016-09-07T20:55:00.470185: step 4358, loss 0.0579663, acc 0.98
2016-09-07T20:55:01.140744: step 4359, loss 0.0741612, acc 0.98
2016-09-07T20:55:01.828426: step 4360, loss 0.0060226, acc 1
2016-09-07T20:55:02.481502: step 4361, loss 0.0196272, acc 0.98
2016-09-07T20:55:03.139875: step 4362, loss 0.0496456, acc 0.98
2016-09-07T20:55:03.800915: step 4363, loss 0.0269934, acc 0.98
2016-09-07T20:55:04.477268: step 4364, loss 0.0124421, acc 1
2016-09-07T20:55:05.145181: step 4365, loss 0.00360326, acc 1
2016-09-07T20:55:05.795571: step 4366, loss 0.0283386, acc 1
2016-09-07T20:55:06.466753: step 4367, loss 0.00243592, acc 1
2016-09-07T20:55:07.146335: step 4368, loss 0.0202487, acc 1
2016-09-07T20:55:07.818282: step 4369, loss 0.00704959, acc 1
2016-09-07T20:55:08.489251: step 4370, loss 0.0417952, acc 0.98
2016-09-07T20:55:09.146151: step 4371, loss 0.0835235, acc 0.96
2016-09-07T20:55:09.822425: step 4372, loss 0.101023, acc 0.94
2016-09-07T20:55:10.498360: step 4373, loss 0.00334644, acc 1
2016-09-07T20:55:11.179825: step 4374, loss 0.00561662, acc 1
2016-09-07T20:55:11.854763: step 4375, loss 0.0790023, acc 0.98
2016-09-07T20:55:12.547024: step 4376, loss 0.0433705, acc 0.98
2016-09-07T20:55:13.217887: step 4377, loss 0.0457434, acc 1
2016-09-07T20:55:13.891473: step 4378, loss 0.00642018, acc 1
2016-09-07T20:55:14.562710: step 4379, loss 0.0508952, acc 0.96
2016-09-07T20:55:15.252190: step 4380, loss 0.00614035, acc 1
2016-09-07T20:55:15.942737: step 4381, loss 0.00658637, acc 1
2016-09-07T20:55:16.621744: step 4382, loss 0.0179527, acc 1
2016-09-07T20:55:17.315765: step 4383, loss 0.0129202, acc 1
2016-09-07T20:55:17.978483: step 4384, loss 0.0589873, acc 0.96
2016-09-07T20:55:18.678683: step 4385, loss 0.0524825, acc 0.96
2016-09-07T20:55:19.352000: step 4386, loss 0.080463, acc 0.96
2016-09-07T20:55:20.019911: step 4387, loss 0.0137833, acc 1
2016-09-07T20:55:20.678641: step 4388, loss 0.0357791, acc 1
2016-09-07T20:55:21.345924: step 4389, loss 0.0183671, acc 1
2016-09-07T20:55:22.011606: step 4390, loss 0.0325507, acc 1
2016-09-07T20:55:22.691930: step 4391, loss 0.0256536, acc 0.98
2016-09-07T20:55:23.363824: step 4392, loss 0.0154588, acc 1
2016-09-07T20:55:24.078864: step 4393, loss 0.0534407, acc 0.98
2016-09-07T20:55:24.748530: step 4394, loss 0.0245241, acc 1
2016-09-07T20:55:25.427793: step 4395, loss 0.0375423, acc 0.98
2016-09-07T20:55:26.084195: step 4396, loss 0.040815, acc 0.98
2016-09-07T20:55:26.746871: step 4397, loss 0.00588369, acc 1
2016-09-07T20:55:27.423003: step 4398, loss 0.0371552, acc 1
2016-09-07T20:55:28.109635: step 4399, loss 0.0345254, acc 1
2016-09-07T20:55:28.784420: step 4400, loss 0.13843, acc 0.96

Evaluation:
2016-09-07T20:55:31.727362: step 4400, loss 1.82016, acc 0.747

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-4400

2016-09-07T20:55:33.334693: step 4401, loss 0.021461, acc 0.98
2016-09-07T20:55:34.000543: step 4402, loss 0.0291516, acc 1
2016-09-07T20:55:34.677498: step 4403, loss 0.0293711, acc 0.98
2016-09-07T20:55:35.371108: step 4404, loss 0.0127253, acc 1
2016-09-07T20:55:36.035418: step 4405, loss 0.0446785, acc 0.98
2016-09-07T20:55:36.699602: step 4406, loss 0.0117233, acc 1
2016-09-07T20:55:37.348576: step 4407, loss 0.00512071, acc 1
2016-09-07T20:55:38.015863: step 4408, loss 0.0342892, acc 0.98
2016-09-07T20:55:38.693257: step 4409, loss 0.0047591, acc 1
2016-09-07T20:55:39.364124: step 4410, loss 0.0174913, acc 1
2016-09-07T20:55:40.069619: step 4411, loss 0.0121847, acc 1
2016-09-07T20:55:40.745098: step 4412, loss 0.0151675, acc 1
2016-09-07T20:55:41.411872: step 4413, loss 0.0178101, acc 1
2016-09-07T20:55:42.095632: step 4414, loss 0.0692073, acc 0.98
2016-09-07T20:55:42.752457: step 4415, loss 0.00504207, acc 1
2016-09-07T20:55:43.427450: step 4416, loss 0.120805, acc 0.94
2016-09-07T20:55:44.113018: step 4417, loss 0.0248589, acc 0.98
2016-09-07T20:55:44.787107: step 4418, loss 0.0166382, acc 1
2016-09-07T20:55:45.474108: step 4419, loss 0.0234819, acc 0.98
2016-09-07T20:55:46.159219: step 4420, loss 0.0638804, acc 0.96
2016-09-07T20:55:46.828684: step 4421, loss 0.0550389, acc 0.96
2016-09-07T20:55:47.499064: step 4422, loss 0.0331707, acc 0.98
2016-09-07T20:55:48.202160: step 4423, loss 0.00495528, acc 1
2016-09-07T20:55:48.879004: step 4424, loss 0.02007, acc 0.98
2016-09-07T20:55:49.561649: step 4425, loss 0.00417408, acc 1
2016-09-07T20:55:50.228444: step 4426, loss 0.043714, acc 0.98
2016-09-07T20:55:50.905221: step 4427, loss 0.0200993, acc 1
2016-09-07T20:55:51.594890: step 4428, loss 0.0146186, acc 1
2016-09-07T20:55:52.267574: step 4429, loss 0.0452752, acc 1
2016-09-07T20:55:52.928361: step 4430, loss 0.0304194, acc 0.98
2016-09-07T20:55:53.585162: step 4431, loss 0.00564928, acc 1
2016-09-07T20:55:54.254161: step 4432, loss 0.0639518, acc 0.98
2016-09-07T20:55:54.914297: step 4433, loss 0.00474595, acc 1
2016-09-07T20:55:55.578554: step 4434, loss 0.0405815, acc 0.98
2016-09-07T20:55:56.247109: step 4435, loss 0.066657, acc 0.98
2016-09-07T20:55:56.907961: step 4436, loss 0.0146634, acc 1
2016-09-07T20:55:57.573694: step 4437, loss 0.0104951, acc 1
2016-09-07T20:55:58.251425: step 4438, loss 0.0817311, acc 0.96
2016-09-07T20:55:58.910152: step 4439, loss 0.0247488, acc 1
2016-09-07T20:55:59.586014: step 4440, loss 0.0151043, acc 1
2016-09-07T20:56:00.299171: step 4441, loss 0.0331619, acc 0.98
2016-09-07T20:56:00.959090: step 4442, loss 0.0031037, acc 1
2016-09-07T20:56:01.629044: step 4443, loss 0.0530878, acc 0.98
2016-09-07T20:56:02.316850: step 4444, loss 0.0601986, acc 0.98
2016-09-07T20:56:02.988830: step 4445, loss 0.0233667, acc 1
2016-09-07T20:56:03.657259: step 4446, loss 0.00903148, acc 1
2016-09-07T20:56:04.318586: step 4447, loss 0.0324814, acc 0.98
2016-09-07T20:56:05.002061: step 4448, loss 0.0287537, acc 1
2016-09-07T20:56:05.691756: step 4449, loss 0.0327208, acc 1
2016-09-07T20:56:06.349863: step 4450, loss 0.0415737, acc 0.98
2016-09-07T20:56:07.021570: step 4451, loss 0.0182974, acc 0.98
2016-09-07T20:56:07.713938: step 4452, loss 0.0209518, acc 1
2016-09-07T20:56:08.384764: step 4453, loss 0.0182756, acc 1
2016-09-07T20:56:09.054746: step 4454, loss 0.0176051, acc 1
2016-09-07T20:56:09.708866: step 4455, loss 0.0290914, acc 1
2016-09-07T20:56:10.404196: step 4456, loss 0.0390851, acc 0.98
2016-09-07T20:56:11.077674: step 4457, loss 0.0129563, acc 1
2016-09-07T20:56:11.750142: step 4458, loss 0.0226233, acc 1
2016-09-07T20:56:12.399892: step 4459, loss 0.0665494, acc 0.94
2016-09-07T20:56:13.075953: step 4460, loss 0.00858903, acc 1
2016-09-07T20:56:13.742573: step 4461, loss 0.00326259, acc 1
2016-09-07T20:56:14.103763: step 4462, loss 0.0398924, acc 1
2016-09-07T20:56:14.771954: step 4463, loss 0.0152844, acc 1
2016-09-07T20:56:15.465524: step 4464, loss 0.0209605, acc 0.98
2016-09-07T20:56:16.127446: step 4465, loss 0.00599221, acc 1
2016-09-07T20:56:16.790179: step 4466, loss 0.00702463, acc 1
2016-09-07T20:56:17.466493: step 4467, loss 0.0131395, acc 1
2016-09-07T20:56:18.128324: step 4468, loss 0.0342435, acc 1
2016-09-07T20:56:18.798768: step 4469, loss 0.00935073, acc 1
2016-09-07T20:56:19.471112: step 4470, loss 0.0191761, acc 1
2016-09-07T20:56:20.136858: step 4471, loss 0.00945051, acc 1
2016-09-07T20:56:20.822462: step 4472, loss 0.0153429, acc 1
2016-09-07T20:56:21.497274: step 4473, loss 0.0735726, acc 0.94
2016-09-07T20:56:22.167603: step 4474, loss 0.0209671, acc 0.98
2016-09-07T20:56:22.836753: step 4475, loss 0.140402, acc 0.92
2016-09-07T20:56:23.482930: step 4476, loss 0.0103478, acc 1
2016-09-07T20:56:24.155838: step 4477, loss 0.0384321, acc 0.98
2016-09-07T20:56:24.855403: step 4478, loss 0.00651945, acc 1
2016-09-07T20:56:25.538673: step 4479, loss 0.00816272, acc 1
2016-09-07T20:56:26.195273: step 4480, loss 0.0155469, acc 1
2016-09-07T20:56:26.869963: step 4481, loss 0.0108715, acc 1
2016-09-07T20:56:27.537713: step 4482, loss 0.0158722, acc 1
2016-09-07T20:56:28.197504: step 4483, loss 0.0305849, acc 1
2016-09-07T20:56:28.875387: step 4484, loss 0.0473064, acc 0.98
2016-09-07T20:56:29.538467: step 4485, loss 0.0618276, acc 0.98
2016-09-07T20:56:30.206016: step 4486, loss 0.00922288, acc 1
2016-09-07T20:56:30.873763: step 4487, loss 0.0353075, acc 0.98
2016-09-07T20:56:31.534191: step 4488, loss 0.00449556, acc 1
2016-09-07T20:56:32.200151: step 4489, loss 0.00627187, acc 1
2016-09-07T20:56:32.872964: step 4490, loss 0.0117474, acc 1
2016-09-07T20:56:33.543859: step 4491, loss 0.00510273, acc 1
2016-09-07T20:56:34.210183: step 4492, loss 0.00483664, acc 1
2016-09-07T20:56:34.881213: step 4493, loss 0.14794, acc 0.96
2016-09-07T20:56:35.549675: step 4494, loss 0.0503109, acc 0.96
2016-09-07T20:56:36.214709: step 4495, loss 0.00990563, acc 1
2016-09-07T20:56:36.882167: step 4496, loss 0.0192379, acc 1
2016-09-07T20:56:37.542371: step 4497, loss 0.0130864, acc 1
2016-09-07T20:56:38.195852: step 4498, loss 0.126563, acc 0.98
2016-09-07T20:56:38.858922: step 4499, loss 0.011896, acc 1
2016-09-07T20:56:39.520208: step 4500, loss 0.0324374, acc 1

Evaluation:
2016-09-07T20:56:42.474357: step 4500, loss 1.87018, acc 0.724

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-4500

2016-09-07T20:56:44.209954: step 4501, loss 0.01764, acc 1
2016-09-07T20:56:44.880054: step 4502, loss 0.00307703, acc 1
2016-09-07T20:56:45.548932: step 4503, loss 0.0365721, acc 1
2016-09-07T20:56:46.225202: step 4504, loss 0.0154807, acc 1
2016-09-07T20:56:46.881309: step 4505, loss 0.0170838, acc 1
2016-09-07T20:56:47.553670: step 4506, loss 0.0163473, acc 1
2016-09-07T20:56:48.224797: step 4507, loss 0.0889824, acc 0.94
2016-09-07T20:56:48.896912: step 4508, loss 0.0249892, acc 1
2016-09-07T20:56:49.559550: step 4509, loss 0.0328931, acc 0.98
2016-09-07T20:56:50.242142: step 4510, loss 0.0315373, acc 0.98
2016-09-07T20:56:50.896501: step 4511, loss 0.0199315, acc 1
2016-09-07T20:56:51.579064: step 4512, loss 0.0399857, acc 0.98
2016-09-07T20:56:52.270282: step 4513, loss 0.0998498, acc 0.9
2016-09-07T20:56:52.960477: step 4514, loss 0.0881762, acc 0.94
2016-09-07T20:56:53.642696: step 4515, loss 0.0122352, acc 1
2016-09-07T20:56:54.323660: step 4516, loss 0.0354455, acc 0.98
2016-09-07T20:56:54.984880: step 4517, loss 0.00957501, acc 1
2016-09-07T20:56:55.662816: step 4518, loss 0.00951926, acc 1
2016-09-07T20:56:56.370151: step 4519, loss 0.0388385, acc 0.96
2016-09-07T20:56:57.045208: step 4520, loss 0.0192664, acc 0.98
2016-09-07T20:56:57.714311: step 4521, loss 0.072096, acc 0.96
2016-09-07T20:56:58.368592: step 4522, loss 0.0355494, acc 0.98
2016-09-07T20:56:59.031343: step 4523, loss 0.0379157, acc 0.98
2016-09-07T20:56:59.720973: step 4524, loss 0.00648636, acc 1
2016-09-07T20:57:00.455717: step 4525, loss 0.0250737, acc 0.98
2016-09-07T20:57:01.142253: step 4526, loss 0.0898776, acc 0.96
2016-09-07T20:57:01.839012: step 4527, loss 0.0452838, acc 1
2016-09-07T20:57:02.507218: step 4528, loss 0.132657, acc 0.94
2016-09-07T20:57:03.188025: step 4529, loss 0.0367515, acc 0.98
2016-09-07T20:57:03.868378: step 4530, loss 0.00298934, acc 1
2016-09-07T20:57:04.531015: step 4531, loss 0.135685, acc 0.94
2016-09-07T20:57:05.208249: step 4532, loss 0.0413712, acc 0.98
2016-09-07T20:57:05.894090: step 4533, loss 0.0436369, acc 0.96
2016-09-07T20:57:06.569665: step 4534, loss 0.0145957, acc 1
2016-09-07T20:57:07.239357: step 4535, loss 0.00674442, acc 1
2016-09-07T20:57:07.919683: step 4536, loss 0.0230629, acc 1
2016-09-07T20:57:08.593507: step 4537, loss 0.00622971, acc 1
2016-09-07T20:57:09.261792: step 4538, loss 0.0768917, acc 0.94
2016-09-07T20:57:09.937575: step 4539, loss 0.0519152, acc 0.98
2016-09-07T20:57:10.610483: step 4540, loss 0.0153932, acc 1
2016-09-07T20:57:11.281563: step 4541, loss 0.0567989, acc 0.98
2016-09-07T20:57:11.967675: step 4542, loss 0.0430938, acc 0.98
2016-09-07T20:57:12.626252: step 4543, loss 0.0183489, acc 1
2016-09-07T20:57:13.298727: step 4544, loss 0.0382509, acc 0.96
2016-09-07T20:57:13.968804: step 4545, loss 0.00816214, acc 1
2016-09-07T20:57:14.644122: step 4546, loss 0.0220295, acc 0.98
2016-09-07T20:57:15.329231: step 4547, loss 0.0150113, acc 1
2016-09-07T20:57:16.003060: step 4548, loss 0.0128657, acc 1
2016-09-07T20:57:16.678836: step 4549, loss 0.029457, acc 0.98
2016-09-07T20:57:17.341485: step 4550, loss 0.0334449, acc 0.98
2016-09-07T20:57:18.004640: step 4551, loss 0.0984879, acc 0.96
2016-09-07T20:57:18.695943: step 4552, loss 0.0180827, acc 1
2016-09-07T20:57:19.350525: step 4553, loss 0.0180142, acc 1
2016-09-07T20:57:20.029211: step 4554, loss 0.0475792, acc 0.98
2016-09-07T20:57:20.743332: step 4555, loss 0.0625472, acc 0.98
2016-09-07T20:57:21.423571: step 4556, loss 0.03408, acc 0.98
2016-09-07T20:57:22.097292: step 4557, loss 0.045137, acc 0.98
2016-09-07T20:57:22.763449: step 4558, loss 0.014452, acc 1
2016-09-07T20:57:23.442354: step 4559, loss 0.0332482, acc 0.98
2016-09-07T20:57:24.121417: step 4560, loss 0.0334422, acc 1
2016-09-07T20:57:24.789085: step 4561, loss 0.0361, acc 1
2016-09-07T20:57:25.460399: step 4562, loss 0.0792551, acc 0.94
2016-09-07T20:57:26.146193: step 4563, loss 0.0282069, acc 0.98
2016-09-07T20:57:26.800219: step 4564, loss 0.0210791, acc 0.98
2016-09-07T20:57:27.476327: step 4565, loss 0.0199778, acc 0.98
2016-09-07T20:57:28.130332: step 4566, loss 0.0324067, acc 0.98
2016-09-07T20:57:28.814139: step 4567, loss 0.0509351, acc 0.98
2016-09-07T20:57:29.478683: step 4568, loss 0.0122815, acc 1
2016-09-07T20:57:30.138641: step 4569, loss 0.0295019, acc 0.98
2016-09-07T20:57:30.795249: step 4570, loss 0.0316521, acc 0.98
2016-09-07T20:57:31.467386: step 4571, loss 0.00368778, acc 1
2016-09-07T20:57:32.127877: step 4572, loss 0.0170796, acc 1
2016-09-07T20:57:32.787793: step 4573, loss 0.0408881, acc 0.98
2016-09-07T20:57:33.447500: step 4574, loss 0.0152976, acc 1
2016-09-07T20:57:34.107163: step 4575, loss 0.024557, acc 1
2016-09-07T20:57:34.774598: step 4576, loss 0.0254104, acc 0.98
2016-09-07T20:57:35.446726: step 4577, loss 0.0127349, acc 1
2016-09-07T20:57:36.128284: step 4578, loss 0.00807796, acc 1
2016-09-07T20:57:36.795753: step 4579, loss 0.0150409, acc 1
2016-09-07T20:57:37.463080: step 4580, loss 0.0149295, acc 1
2016-09-07T20:57:38.123468: step 4581, loss 0.0339971, acc 0.98
2016-09-07T20:57:38.797811: step 4582, loss 0.0927655, acc 0.94
2016-09-07T20:57:39.458677: step 4583, loss 0.0356022, acc 0.98
2016-09-07T20:57:40.148320: step 4584, loss 0.0562513, acc 0.94
2016-09-07T20:57:40.831347: step 4585, loss 0.00298692, acc 1
2016-09-07T20:57:41.493012: step 4586, loss 0.00769496, acc 1
2016-09-07T20:57:42.165273: step 4587, loss 0.0585797, acc 0.96
2016-09-07T20:57:42.826264: step 4588, loss 0.0292179, acc 0.98
2016-09-07T20:57:43.493303: step 4589, loss 0.0222609, acc 0.98
2016-09-07T20:57:44.148137: step 4590, loss 0.0154249, acc 1
2016-09-07T20:57:44.823938: step 4591, loss 0.00313917, acc 1
2016-09-07T20:57:45.502576: step 4592, loss 0.0312325, acc 0.98
2016-09-07T20:57:46.175522: step 4593, loss 0.0284205, acc 0.98
2016-09-07T20:57:46.839229: step 4594, loss 0.0246683, acc 1
2016-09-07T20:57:47.512435: step 4595, loss 0.00307723, acc 1
2016-09-07T20:57:48.169106: step 4596, loss 0.0343899, acc 0.98
2016-09-07T20:57:48.855582: step 4597, loss 0.0486828, acc 1
2016-09-07T20:57:49.520918: step 4598, loss 0.0183923, acc 1
2016-09-07T20:57:50.178615: step 4599, loss 0.00560416, acc 1
2016-09-07T20:57:50.874687: step 4600, loss 0.00757755, acc 1

Evaluation:
2016-09-07T20:57:53.845044: step 4600, loss 2.70796, acc 0.729

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-4600

2016-09-07T20:57:55.438623: step 4601, loss 0.030631, acc 0.98
2016-09-07T20:57:56.095618: step 4602, loss 0.085089, acc 0.92
2016-09-07T20:57:56.753598: step 4603, loss 0.0203786, acc 0.98
2016-09-07T20:57:57.434132: step 4604, loss 0.00439199, acc 1
2016-09-07T20:57:58.100762: step 4605, loss 0.0210912, acc 1
2016-09-07T20:57:58.759201: step 4606, loss 0.0683772, acc 0.98
2016-09-07T20:57:59.412958: step 4607, loss 0.145132, acc 0.94
2016-09-07T20:58:00.072449: step 4608, loss 0.0431827, acc 0.98
2016-09-07T20:58:00.801430: step 4609, loss 0.0832546, acc 0.96
2016-09-07T20:58:01.484911: step 4610, loss 0.0249983, acc 1
2016-09-07T20:58:02.138684: step 4611, loss 0.0141116, acc 1
2016-09-07T20:58:02.814317: step 4612, loss 0.0294376, acc 1
2016-09-07T20:58:03.478165: step 4613, loss 0.0111909, acc 1
2016-09-07T20:58:04.160932: step 4614, loss 0.0240573, acc 1
2016-09-07T20:58:04.838781: step 4615, loss 0.0253012, acc 0.98
2016-09-07T20:58:05.500333: step 4616, loss 0.0359135, acc 0.98
2016-09-07T20:58:06.169901: step 4617, loss 0.131497, acc 0.96
2016-09-07T20:58:06.834398: step 4618, loss 0.0140015, acc 1
2016-09-07T20:58:07.504661: step 4619, loss 0.0119738, acc 1
2016-09-07T20:58:08.175646: step 4620, loss 0.0541823, acc 0.96
2016-09-07T20:58:08.831987: step 4621, loss 0.00654602, acc 1
2016-09-07T20:58:09.492871: step 4622, loss 0.0114394, acc 1
2016-09-07T20:58:10.151202: step 4623, loss 0.0472126, acc 0.96
2016-09-07T20:58:10.825946: step 4624, loss 0.0753831, acc 0.96
2016-09-07T20:58:11.506543: step 4625, loss 0.0152536, acc 1
2016-09-07T20:58:12.156349: step 4626, loss 0.0756672, acc 0.98
2016-09-07T20:58:12.803099: step 4627, loss 0.0181894, acc 1
2016-09-07T20:58:13.443343: step 4628, loss 0.0237273, acc 1
2016-09-07T20:58:14.117399: step 4629, loss 0.0112323, acc 1
2016-09-07T20:58:14.781771: step 4630, loss 0.0313988, acc 0.98
2016-09-07T20:58:15.475383: step 4631, loss 0.0310579, acc 1
2016-09-07T20:58:16.140829: step 4632, loss 0.0292228, acc 1
2016-09-07T20:58:16.816861: step 4633, loss 0.00326152, acc 1
2016-09-07T20:58:17.486470: step 4634, loss 0.0292087, acc 0.98
2016-09-07T20:58:18.165534: step 4635, loss 0.0327282, acc 0.98
2016-09-07T20:58:18.822500: step 4636, loss 0.0863024, acc 0.96
2016-09-07T20:58:19.519993: step 4637, loss 0.0689678, acc 0.98
2016-09-07T20:58:20.202476: step 4638, loss 0.0152488, acc 1
2016-09-07T20:58:20.869018: step 4639, loss 0.0266916, acc 0.98
2016-09-07T20:58:21.518280: step 4640, loss 0.0297306, acc 1
2016-09-07T20:58:22.179232: step 4641, loss 0.0618526, acc 0.98
2016-09-07T20:58:22.854934: step 4642, loss 0.00595692, acc 1
2016-09-07T20:58:23.523276: step 4643, loss 0.0352755, acc 0.98
2016-09-07T20:58:24.190823: step 4644, loss 0.108247, acc 0.94
2016-09-07T20:58:24.850008: step 4645, loss 0.0033313, acc 1
2016-09-07T20:58:25.516147: step 4646, loss 0.00595855, acc 1
2016-09-07T20:58:26.211396: step 4647, loss 0.0110719, acc 1
2016-09-07T20:58:26.876390: step 4648, loss 0.0464996, acc 0.96
2016-09-07T20:58:27.557367: step 4649, loss 0.0433499, acc 0.98
2016-09-07T20:58:28.223667: step 4650, loss 0.0182795, acc 1
2016-09-07T20:58:28.896160: step 4651, loss 0.0234296, acc 1
2016-09-07T20:58:29.552768: step 4652, loss 0.0402698, acc 1
2016-09-07T20:58:30.217645: step 4653, loss 0.0200999, acc 1
2016-09-07T20:58:30.885069: step 4654, loss 0.0483706, acc 0.98
2016-09-07T20:58:31.543070: step 4655, loss 0.0244026, acc 1
2016-09-07T20:58:31.892509: step 4656, loss 0.0240424, acc 1
2016-09-07T20:58:32.562150: step 4657, loss 0.0151381, acc 1
2016-09-07T20:58:33.235840: step 4658, loss 0.0560155, acc 0.96
2016-09-07T20:58:33.923637: step 4659, loss 0.00721547, acc 1
2016-09-07T20:58:34.599745: step 4660, loss 0.0842856, acc 0.96
2016-09-07T20:58:35.269972: step 4661, loss 0.0305467, acc 1
2016-09-07T20:58:35.952401: step 4662, loss 0.0178207, acc 1
2016-09-07T20:58:36.625244: step 4663, loss 0.0447846, acc 0.98
2016-09-07T20:58:37.295100: step 4664, loss 0.00749743, acc 1
2016-09-07T20:58:37.977546: step 4665, loss 0.00642532, acc 1
2016-09-07T20:58:38.638952: step 4666, loss 0.0263711, acc 0.98
2016-09-07T20:58:39.317723: step 4667, loss 0.00740192, acc 1
2016-09-07T20:58:39.999889: step 4668, loss 0.0295341, acc 0.98
2016-09-07T20:58:40.675291: step 4669, loss 0.0587525, acc 0.96
2016-09-07T20:58:41.353267: step 4670, loss 0.0420618, acc 0.96
2016-09-07T20:58:42.024390: step 4671, loss 0.0366508, acc 0.98
2016-09-07T20:58:42.717466: step 4672, loss 0.0338406, acc 0.98
2016-09-07T20:58:43.385306: step 4673, loss 0.0394746, acc 0.98
2016-09-07T20:58:44.049606: step 4674, loss 0.0109187, acc 1
2016-09-07T20:58:44.704126: step 4675, loss 0.0209772, acc 1
2016-09-07T20:58:45.371297: step 4676, loss 0.0293023, acc 0.98
2016-09-07T20:58:46.048203: step 4677, loss 0.00637288, acc 1
2016-09-07T20:58:46.722892: step 4678, loss 0.0209773, acc 1
2016-09-07T20:58:47.388843: step 4679, loss 0.0046806, acc 1
2016-09-07T20:58:48.064384: step 4680, loss 0.022527, acc 0.98
2016-09-07T20:58:48.729576: step 4681, loss 0.0164082, acc 1
2016-09-07T20:58:49.387263: step 4682, loss 0.0172673, acc 1
2016-09-07T20:58:50.047159: step 4683, loss 0.00389113, acc 1
2016-09-07T20:58:50.711502: step 4684, loss 0.00476742, acc 1
2016-09-07T20:58:51.380559: step 4685, loss 0.0470896, acc 0.96
2016-09-07T20:58:52.058481: step 4686, loss 0.00364465, acc 1
2016-09-07T20:58:52.727593: step 4687, loss 0.0569071, acc 0.96
2016-09-07T20:58:53.408166: step 4688, loss 0.108644, acc 0.98
2016-09-07T20:58:54.073358: step 4689, loss 0.0039102, acc 1
2016-09-07T20:58:54.736834: step 4690, loss 0.0436506, acc 0.98
2016-09-07T20:58:55.389648: step 4691, loss 0.00309208, acc 1
2016-09-07T20:58:56.049946: step 4692, loss 0.00465489, acc 1
2016-09-07T20:58:56.716260: step 4693, loss 0.0244447, acc 0.98
2016-09-07T20:58:57.385654: step 4694, loss 0.0105747, acc 1
2016-09-07T20:58:58.075783: step 4695, loss 0.00483725, acc 1
2016-09-07T20:58:58.753330: step 4696, loss 0.00285286, acc 1
2016-09-07T20:58:59.427578: step 4697, loss 0.0351792, acc 1
2016-09-07T20:59:00.094261: step 4698, loss 0.0104265, acc 1
2016-09-07T20:59:00.790732: step 4699, loss 0.0386633, acc 0.98
2016-09-07T20:59:01.480078: step 4700, loss 0.018664, acc 1

Evaluation:
2016-09-07T20:59:04.468877: step 4700, loss 2.28398, acc 0.743

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-4700

2016-09-07T20:59:06.141113: step 4701, loss 0.0684412, acc 0.98
2016-09-07T20:59:06.823628: step 4702, loss 0.0183485, acc 1
2016-09-07T20:59:07.500054: step 4703, loss 0.0644622, acc 0.96
2016-09-07T20:59:08.184855: step 4704, loss 0.0321815, acc 0.98
2016-09-07T20:59:08.852153: step 4705, loss 0.0025176, acc 1
2016-09-07T20:59:09.530482: step 4706, loss 0.0371217, acc 0.98
2016-09-07T20:59:10.191189: step 4707, loss 0.0528075, acc 0.98
2016-09-07T20:59:10.852686: step 4708, loss 0.0324733, acc 0.98
2016-09-07T20:59:11.531892: step 4709, loss 0.00994179, acc 1
2016-09-07T20:59:12.230767: step 4710, loss 0.00439388, acc 1
2016-09-07T20:59:12.910694: step 4711, loss 0.00298041, acc 1
2016-09-07T20:59:13.579779: step 4712, loss 0.00741618, acc 1
2016-09-07T20:59:14.252874: step 4713, loss 0.0959837, acc 0.94
2016-09-07T20:59:14.919684: step 4714, loss 0.00257872, acc 1
2016-09-07T20:59:15.595439: step 4715, loss 0.00349595, acc 1
2016-09-07T20:59:16.264197: step 4716, loss 0.0440445, acc 0.98
2016-09-07T20:59:16.937790: step 4717, loss 0.0344769, acc 0.98
2016-09-07T20:59:17.605690: step 4718, loss 0.0244307, acc 0.98
2016-09-07T20:59:18.278110: step 4719, loss 0.0676287, acc 0.96
2016-09-07T20:59:18.966969: step 4720, loss 0.00712499, acc 1
2016-09-07T20:59:19.649587: step 4721, loss 0.0593036, acc 0.96
2016-09-07T20:59:20.332287: step 4722, loss 0.0168056, acc 1
2016-09-07T20:59:21.015627: step 4723, loss 0.0786195, acc 0.96
2016-09-07T20:59:21.709872: step 4724, loss 0.0322826, acc 1
2016-09-07T20:59:22.391706: step 4725, loss 0.0150005, acc 1
2016-09-07T20:59:23.083976: step 4726, loss 0.0260086, acc 0.98
2016-09-07T20:59:23.762977: step 4727, loss 0.0141948, acc 1
2016-09-07T20:59:24.434800: step 4728, loss 0.0843412, acc 0.96
2016-09-07T20:59:25.101403: step 4729, loss 0.00504874, acc 1
2016-09-07T20:59:25.773319: step 4730, loss 0.00277459, acc 1
2016-09-07T20:59:26.458177: step 4731, loss 0.0245913, acc 0.98
2016-09-07T20:59:27.110403: step 4732, loss 0.053054, acc 0.94
2016-09-07T20:59:27.794185: step 4733, loss 0.00728288, acc 1
2016-09-07T20:59:28.471983: step 4734, loss 0.00863873, acc 1
2016-09-07T20:59:29.148647: step 4735, loss 0.0297864, acc 0.98
2016-09-07T20:59:29.818845: step 4736, loss 0.103345, acc 0.96
2016-09-07T20:59:30.467644: step 4737, loss 0.0264424, acc 1
2016-09-07T20:59:31.120153: step 4738, loss 0.0323104, acc 0.98
2016-09-07T20:59:31.804845: step 4739, loss 0.0495102, acc 0.96
2016-09-07T20:59:32.493840: step 4740, loss 0.00867771, acc 1
2016-09-07T20:59:33.157354: step 4741, loss 0.0118422, acc 1
2016-09-07T20:59:33.823204: step 4742, loss 0.00507113, acc 1
2016-09-07T20:59:34.491007: step 4743, loss 0.00802132, acc 1
2016-09-07T20:59:35.171459: step 4744, loss 0.0232856, acc 1
2016-09-07T20:59:35.837279: step 4745, loss 0.0493415, acc 0.98
2016-09-07T20:59:36.505508: step 4746, loss 0.0216762, acc 1
2016-09-07T20:59:37.179785: step 4747, loss 0.0257326, acc 1
2016-09-07T20:59:37.847627: step 4748, loss 0.0473167, acc 0.98
2016-09-07T20:59:38.506284: step 4749, loss 0.0129493, acc 1
2016-09-07T20:59:39.181194: step 4750, loss 0.00411653, acc 1
2016-09-07T20:59:39.851470: step 4751, loss 0.0768841, acc 0.96
2016-09-07T20:59:40.548974: step 4752, loss 0.0239643, acc 1
2016-09-07T20:59:41.215684: step 4753, loss 0.0254515, acc 0.98
2016-09-07T20:59:41.882721: step 4754, loss 0.0165651, acc 1
2016-09-07T20:59:42.539166: step 4755, loss 0.00966073, acc 1
2016-09-07T20:59:43.204588: step 4756, loss 0.0528725, acc 0.98
2016-09-07T20:59:43.870753: step 4757, loss 0.0170321, acc 0.98
2016-09-07T20:59:44.558600: step 4758, loss 0.0514593, acc 0.96
2016-09-07T20:59:45.208613: step 4759, loss 0.032455, acc 0.98
2016-09-07T20:59:45.881021: step 4760, loss 0.0180836, acc 0.98
2016-09-07T20:59:46.530967: step 4761, loss 0.00685273, acc 1
2016-09-07T20:59:47.193162: step 4762, loss 0.00721926, acc 1
2016-09-07T20:59:47.871173: step 4763, loss 0.0363825, acc 1
2016-09-07T20:59:48.552154: step 4764, loss 0.0236957, acc 0.98
2016-09-07T20:59:49.222663: step 4765, loss 0.00579593, acc 1
2016-09-07T20:59:49.887199: step 4766, loss 0.0224558, acc 1
2016-09-07T20:59:50.550211: step 4767, loss 0.0519274, acc 0.98
2016-09-07T20:59:51.231112: step 4768, loss 0.00863793, acc 1
2016-09-07T20:59:51.901564: step 4769, loss 0.034192, acc 0.98
2016-09-07T20:59:52.565807: step 4770, loss 0.0343902, acc 0.98
2016-09-07T20:59:53.215726: step 4771, loss 0.00777043, acc 1
2016-09-07T20:59:53.872385: step 4772, loss 0.0846069, acc 0.96
2016-09-07T20:59:54.526068: step 4773, loss 0.0377453, acc 0.98
2016-09-07T20:59:55.209795: step 4774, loss 0.0239112, acc 0.98
2016-09-07T20:59:55.910503: step 4775, loss 0.0231047, acc 0.98
2016-09-07T20:59:56.583084: step 4776, loss 0.0210909, acc 0.98
2016-09-07T20:59:57.255352: step 4777, loss 0.0684201, acc 0.94
2016-09-07T20:59:57.927849: step 4778, loss 0.00421202, acc 1
2016-09-07T20:59:58.587861: step 4779, loss 0.0184331, acc 1
2016-09-07T20:59:59.234695: step 4780, loss 0.0335421, acc 0.98
2016-09-07T20:59:59.901476: step 4781, loss 0.0173755, acc 1
2016-09-07T21:00:00.600725: step 4782, loss 0.0153506, acc 1
2016-09-07T21:00:01.271946: step 4783, loss 0.0528825, acc 0.98
2016-09-07T21:00:01.949930: step 4784, loss 0.0029118, acc 1
2016-09-07T21:00:02.608886: step 4785, loss 0.0234281, acc 0.98
2016-09-07T21:00:03.280176: step 4786, loss 0.0230869, acc 1
2016-09-07T21:00:03.950297: step 4787, loss 0.0309889, acc 0.98
2016-09-07T21:00:04.606994: step 4788, loss 0.259877, acc 0.9
2016-09-07T21:00:05.278848: step 4789, loss 0.145792, acc 0.98
2016-09-07T21:00:05.932849: step 4790, loss 0.0421807, acc 0.96
2016-09-07T21:00:06.582754: step 4791, loss 0.0945941, acc 0.96
2016-09-07T21:00:07.256373: step 4792, loss 0.00920212, acc 1
2016-09-07T21:00:07.923736: step 4793, loss 0.103033, acc 0.96
2016-09-07T21:00:08.580092: step 4794, loss 0.03204, acc 0.98
2016-09-07T21:00:09.247470: step 4795, loss 0.00782202, acc 1
2016-09-07T21:00:09.912059: step 4796, loss 0.0640646, acc 0.96
2016-09-07T21:00:10.565103: step 4797, loss 0.0322836, acc 0.98
2016-09-07T21:00:11.237165: step 4798, loss 0.0343127, acc 0.98
2016-09-07T21:00:11.920754: step 4799, loss 0.0809309, acc 0.94
2016-09-07T21:00:12.610949: step 4800, loss 0.005288, acc 1

Evaluation:
2016-09-07T21:00:15.574836: step 4800, loss 1.23892, acc 0.741

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-4800

2016-09-07T21:00:17.179409: step 4801, loss 0.0313711, acc 0.98
2016-09-07T21:00:17.864915: step 4802, loss 0.0557973, acc 0.96
2016-09-07T21:00:18.550200: step 4803, loss 0.0215249, acc 0.98
2016-09-07T21:00:19.230404: step 4804, loss 0.0247136, acc 1
2016-09-07T21:00:19.904447: step 4805, loss 0.00695394, acc 1
2016-09-07T21:00:20.585635: step 4806, loss 0.0220135, acc 1
2016-09-07T21:00:21.254738: step 4807, loss 0.0399338, acc 0.98
2016-09-07T21:00:21.917507: step 4808, loss 0.0508654, acc 0.98
2016-09-07T21:00:22.589200: step 4809, loss 0.0354682, acc 1
2016-09-07T21:00:23.238684: step 4810, loss 0.141318, acc 0.92
2016-09-07T21:00:23.917083: step 4811, loss 0.0274685, acc 1
2016-09-07T21:00:24.602911: step 4812, loss 0.0190263, acc 0.98
2016-09-07T21:00:25.273888: step 4813, loss 0.0145775, acc 1
2016-09-07T21:00:25.943325: step 4814, loss 0.0260769, acc 0.98
2016-09-07T21:00:26.595153: step 4815, loss 0.0207725, acc 1
2016-09-07T21:00:27.258687: step 4816, loss 0.0195268, acc 1
2016-09-07T21:00:27.913401: step 4817, loss 0.0297604, acc 1
2016-09-07T21:00:28.576491: step 4818, loss 0.0648294, acc 0.96
2016-09-07T21:00:29.239127: step 4819, loss 0.0505154, acc 0.98
2016-09-07T21:00:29.921966: step 4820, loss 0.0463211, acc 0.98
2016-09-07T21:00:30.595353: step 4821, loss 0.00394794, acc 1
2016-09-07T21:00:31.273386: step 4822, loss 0.0456948, acc 0.98
2016-09-07T21:00:31.940502: step 4823, loss 0.056954, acc 0.96
2016-09-07T21:00:32.621613: step 4824, loss 0.018125, acc 1
2016-09-07T21:00:33.305310: step 4825, loss 0.0222353, acc 0.98
2016-09-07T21:00:33.952738: step 4826, loss 0.0686845, acc 0.94
2016-09-07T21:00:34.629412: step 4827, loss 0.0824057, acc 0.96
2016-09-07T21:00:35.296022: step 4828, loss 0.0146132, acc 1
2016-09-07T21:00:35.969780: step 4829, loss 0.158383, acc 0.98
2016-09-07T21:00:36.626352: step 4830, loss 0.00661804, acc 1
2016-09-07T21:00:37.289401: step 4831, loss 0.0422634, acc 0.98
2016-09-07T21:00:37.945391: step 4832, loss 0.046831, acc 0.96
2016-09-07T21:00:38.616180: step 4833, loss 0.0443219, acc 0.98
2016-09-07T21:00:39.292129: step 4834, loss 0.00968071, acc 1
2016-09-07T21:00:39.949626: step 4835, loss 0.0226666, acc 0.98
2016-09-07T21:00:40.635346: step 4836, loss 0.0481349, acc 0.96
2016-09-07T21:00:41.339219: step 4837, loss 0.0541383, acc 0.94
2016-09-07T21:00:42.030022: step 4838, loss 0.0296067, acc 1
2016-09-07T21:00:42.699867: step 4839, loss 0.13861, acc 0.96
2016-09-07T21:00:43.372157: step 4840, loss 0.0180332, acc 1
2016-09-07T21:00:44.027899: step 4841, loss 0.0170435, acc 1
2016-09-07T21:00:44.683783: step 4842, loss 0.037349, acc 0.98
2016-09-07T21:00:45.355923: step 4843, loss 0.0750792, acc 0.98
2016-09-07T21:00:46.022262: step 4844, loss 0.0174602, acc 1
2016-09-07T21:00:46.686293: step 4845, loss 0.139919, acc 0.98
2016-09-07T21:00:47.356222: step 4846, loss 0.144736, acc 0.98
2016-09-07T21:00:48.023713: step 4847, loss 0.0520126, acc 1
2016-09-07T21:00:48.704688: step 4848, loss 0.0175944, acc 1
2016-09-07T21:00:49.392396: step 4849, loss 0.0139174, acc 1
2016-09-07T21:00:49.764933: step 4850, loss 0.08719, acc 0.916667
2016-09-07T21:00:50.444997: step 4851, loss 0.075985, acc 0.96
2016-09-07T21:00:51.117374: step 4852, loss 0.0565588, acc 0.96
2016-09-07T21:00:51.776081: step 4853, loss 0.0635903, acc 0.94
2016-09-07T21:00:52.448314: step 4854, loss 0.0581625, acc 0.96
2016-09-07T21:00:53.109765: step 4855, loss 0.0342983, acc 1
2016-09-07T21:00:53.761607: step 4856, loss 0.0358172, acc 1
2016-09-07T21:00:54.430406: step 4857, loss 0.0100896, acc 1
2016-09-07T21:00:55.098081: step 4858, loss 0.0327871, acc 0.98
2016-09-07T21:00:55.748930: step 4859, loss 0.0565037, acc 0.96
2016-09-07T21:00:56.421045: step 4860, loss 0.0190306, acc 1
2016-09-07T21:00:57.088692: step 4861, loss 0.0211583, acc 1
2016-09-07T21:00:57.751000: step 4862, loss 0.033229, acc 0.98
2016-09-07T21:00:58.455835: step 4863, loss 0.0492916, acc 0.94
2016-09-07T21:00:59.121479: step 4864, loss 0.0338988, acc 1
2016-09-07T21:00:59.786883: step 4865, loss 0.014852, acc 1
2016-09-07T21:01:00.499108: step 4866, loss 0.0433491, acc 0.96
2016-09-07T21:01:01.138306: step 4867, loss 0.00660323, acc 1
2016-09-07T21:01:01.792759: step 4868, loss 0.0560331, acc 0.98
2016-09-07T21:01:02.473930: step 4869, loss 0.0393918, acc 0.96
2016-09-07T21:01:03.137921: step 4870, loss 0.0168739, acc 1
2016-09-07T21:01:03.803527: step 4871, loss 0.0652661, acc 0.98
2016-09-07T21:01:04.484523: step 4872, loss 0.0188622, acc 1
2016-09-07T21:01:05.147322: step 4873, loss 0.00642058, acc 1
2016-09-07T21:01:05.825657: step 4874, loss 0.0201515, acc 1
2016-09-07T21:01:06.497348: step 4875, loss 0.0391286, acc 1
2016-09-07T21:01:07.176372: step 4876, loss 0.00620883, acc 1
2016-09-07T21:01:07.876691: step 4877, loss 0.00586794, acc 1
2016-09-07T21:01:08.540138: step 4878, loss 0.0644031, acc 0.96
2016-09-07T21:01:09.207184: step 4879, loss 0.0169881, acc 1
2016-09-07T21:01:09.877906: step 4880, loss 0.0344127, acc 0.98
2016-09-07T21:01:10.554257: step 4881, loss 0.0849174, acc 0.96
2016-09-07T21:01:11.227142: step 4882, loss 0.0320999, acc 1
2016-09-07T21:01:11.891670: step 4883, loss 0.0157499, acc 1
2016-09-07T21:01:12.549337: step 4884, loss 0.0321011, acc 1
2016-09-07T21:01:13.205241: step 4885, loss 0.203781, acc 0.94
2016-09-07T21:01:13.882359: step 4886, loss 0.0232891, acc 0.98
2016-09-07T21:01:14.553159: step 4887, loss 0.0160315, acc 1
2016-09-07T21:01:15.224437: step 4888, loss 0.074283, acc 0.98
2016-09-07T21:01:15.882013: step 4889, loss 0.00694389, acc 1
2016-09-07T21:01:16.537261: step 4890, loss 0.0331968, acc 0.98
2016-09-07T21:01:17.201834: step 4891, loss 0.0332073, acc 1
2016-09-07T21:01:17.870228: step 4892, loss 0.0157349, acc 1
2016-09-07T21:01:18.534078: step 4893, loss 0.0156284, acc 1
2016-09-07T21:01:19.203802: step 4894, loss 0.0164689, acc 1
2016-09-07T21:01:19.860786: step 4895, loss 0.0429736, acc 0.98
2016-09-07T21:01:20.513855: step 4896, loss 0.0305507, acc 1
2016-09-07T21:01:21.167184: step 4897, loss 0.031659, acc 0.98
2016-09-07T21:01:21.844011: step 4898, loss 0.0783421, acc 0.98
2016-09-07T21:01:22.530408: step 4899, loss 0.038826, acc 0.98
2016-09-07T21:01:23.223937: step 4900, loss 0.0409539, acc 0.98

Evaluation:
2016-09-07T21:01:26.240469: step 4900, loss 1.94964, acc 0.743

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-4900

2016-09-07T21:01:27.948031: step 4901, loss 0.030116, acc 1
2016-09-07T21:01:28.621499: step 4902, loss 0.0838455, acc 0.98
2016-09-07T21:01:29.294832: step 4903, loss 0.137796, acc 0.98
2016-09-07T21:01:29.959156: step 4904, loss 0.0171752, acc 1
2016-09-07T21:01:30.650993: step 4905, loss 0.0291961, acc 0.98
2016-09-07T21:01:31.321707: step 4906, loss 0.134962, acc 0.94
2016-09-07T21:01:32.001058: step 4907, loss 0.015283, acc 1
2016-09-07T21:01:32.661139: step 4908, loss 0.00467116, acc 1
2016-09-07T21:01:33.335484: step 4909, loss 0.00418204, acc 1
2016-09-07T21:01:34.002393: step 4910, loss 0.0096565, acc 1
2016-09-07T21:01:34.685209: step 4911, loss 0.0263692, acc 1
2016-09-07T21:01:35.371642: step 4912, loss 0.0200446, acc 0.98
2016-09-07T21:01:36.033334: step 4913, loss 0.0585547, acc 0.98
2016-09-07T21:01:36.695796: step 4914, loss 0.0291724, acc 1
2016-09-07T21:01:37.358488: step 4915, loss 0.0477908, acc 0.98
2016-09-07T21:01:38.025750: step 4916, loss 0.0188371, acc 1
2016-09-07T21:01:38.675577: step 4917, loss 0.0260777, acc 1
2016-09-07T21:01:39.331021: step 4918, loss 0.0300808, acc 1
2016-09-07T21:01:40.000735: step 4919, loss 0.0159228, acc 1
2016-09-07T21:01:40.665587: step 4920, loss 0.130002, acc 0.96
2016-09-07T21:01:41.327327: step 4921, loss 0.0470738, acc 0.98
2016-09-07T21:01:41.985332: step 4922, loss 0.0319905, acc 1
2016-09-07T21:01:42.648767: step 4923, loss 0.00673785, acc 1
2016-09-07T21:01:43.305695: step 4924, loss 0.0365767, acc 0.98
2016-09-07T21:01:43.976558: step 4925, loss 0.00798104, acc 1
2016-09-07T21:01:44.657295: step 4926, loss 0.0679443, acc 0.98
2016-09-07T21:01:45.328988: step 4927, loss 0.0560067, acc 0.96
2016-09-07T21:01:46.001517: step 4928, loss 0.0147813, acc 1
2016-09-07T21:01:46.675168: step 4929, loss 0.0277177, acc 1
2016-09-07T21:01:47.354927: step 4930, loss 0.026613, acc 0.98
2016-09-07T21:01:48.042853: step 4931, loss 0.0535865, acc 0.98
2016-09-07T21:01:48.717087: step 4932, loss 0.00496416, acc 1
2016-09-07T21:01:49.394426: step 4933, loss 0.022004, acc 1
2016-09-07T21:01:50.074364: step 4934, loss 0.015679, acc 1
2016-09-07T21:01:50.750880: step 4935, loss 0.0216193, acc 1
2016-09-07T21:01:51.436682: step 4936, loss 0.0790857, acc 0.98
2016-09-07T21:01:52.116324: step 4937, loss 0.0218309, acc 1
2016-09-07T21:01:52.791316: step 4938, loss 0.025365, acc 0.98
2016-09-07T21:01:53.464865: step 4939, loss 0.015048, acc 1
2016-09-07T21:01:54.130069: step 4940, loss 0.045109, acc 0.98
2016-09-07T21:01:54.777375: step 4941, loss 0.00692006, acc 1
2016-09-07T21:01:55.459483: step 4942, loss 0.0118007, acc 1
2016-09-07T21:01:56.132163: step 4943, loss 0.00566, acc 1
2016-09-07T21:01:56.802789: step 4944, loss 0.0436059, acc 0.98
2016-09-07T21:01:57.467116: step 4945, loss 0.00713527, acc 1
2016-09-07T21:01:58.134056: step 4946, loss 0.0192525, acc 0.98
2016-09-07T21:01:58.793035: step 4947, loss 0.0142189, acc 1
2016-09-07T21:01:59.482750: step 4948, loss 0.0546489, acc 0.98
2016-09-07T21:02:00.148861: step 4949, loss 0.0392749, acc 0.96
2016-09-07T21:02:00.878317: step 4950, loss 0.0294556, acc 0.98
2016-09-07T21:02:01.560163: step 4951, loss 0.00924848, acc 1
2016-09-07T21:02:02.229052: step 4952, loss 0.00762361, acc 1
2016-09-07T21:02:02.902078: step 4953, loss 0.0132156, acc 1
2016-09-07T21:02:03.573950: step 4954, loss 0.0360041, acc 0.98
2016-09-07T21:02:04.255331: step 4955, loss 0.0301078, acc 0.98
2016-09-07T21:02:04.926137: step 4956, loss 0.131221, acc 0.98
2016-09-07T21:02:05.609695: step 4957, loss 0.0158088, acc 1
2016-09-07T21:02:06.289178: step 4958, loss 0.00911228, acc 1
2016-09-07T21:02:06.959756: step 4959, loss 0.0199019, acc 0.98
2016-09-07T21:02:07.628774: step 4960, loss 0.017916, acc 1
2016-09-07T21:02:08.326112: step 4961, loss 0.0456521, acc 0.98
2016-09-07T21:02:08.995621: step 4962, loss 0.00397397, acc 1
2016-09-07T21:02:09.674465: step 4963, loss 0.00555858, acc 1
2016-09-07T21:02:10.361652: step 4964, loss 0.0173818, acc 1
2016-09-07T21:02:11.011020: step 4965, loss 0.00576059, acc 1
2016-09-07T21:02:11.718480: step 4966, loss 0.0223365, acc 1
2016-09-07T21:02:12.396866: step 4967, loss 0.0514256, acc 0.98
2016-09-07T21:02:13.059994: step 4968, loss 0.0340269, acc 0.98
2016-09-07T21:02:13.740266: step 4969, loss 0.00362671, acc 1
2016-09-07T21:02:14.391827: step 4970, loss 0.0437852, acc 0.98
2016-09-07T21:02:15.044266: step 4971, loss 0.0392332, acc 0.98
2016-09-07T21:02:15.710357: step 4972, loss 0.0256609, acc 1
2016-09-07T21:02:16.374422: step 4973, loss 0.0134465, acc 1
2016-09-07T21:02:17.045956: step 4974, loss 0.0251683, acc 0.98
2016-09-07T21:02:17.747803: step 4975, loss 0.0299283, acc 1
2016-09-07T21:02:18.404413: step 4976, loss 0.0567034, acc 0.98
2016-09-07T21:02:19.101140: step 4977, loss 0.029506, acc 1
2016-09-07T21:02:19.764312: step 4978, loss 0.0123413, acc 1
2016-09-07T21:02:20.426091: step 4979, loss 0.00660923, acc 1
2016-09-07T21:02:21.099553: step 4980, loss 0.0601943, acc 0.98
2016-09-07T21:02:21.739414: step 4981, loss 0.0380584, acc 0.98
2016-09-07T21:02:22.414542: step 4982, loss 0.0109508, acc 1
2016-09-07T21:02:23.092095: step 4983, loss 0.032978, acc 0.98
2016-09-07T21:02:23.750814: step 4984, loss 0.0156535, acc 1
2016-09-07T21:02:24.423553: step 4985, loss 0.00510193, acc 1
2016-09-07T21:02:25.087017: step 4986, loss 0.00526405, acc 1
2016-09-07T21:02:25.753289: step 4987, loss 0.0251705, acc 0.98
2016-09-07T21:02:26.439300: step 4988, loss 0.00368246, acc 1
2016-09-07T21:02:27.110987: step 4989, loss 0.0161598, acc 1
2016-09-07T21:02:27.780725: step 4990, loss 0.093356, acc 0.98
2016-09-07T21:02:28.442045: step 4991, loss 0.0288323, acc 0.98
2016-09-07T21:02:29.108566: step 4992, loss 0.0197949, acc 0.98
2016-09-07T21:02:29.778707: step 4993, loss 0.0493635, acc 0.98
2016-09-07T21:02:30.469087: step 4994, loss 0.0430868, acc 0.98
2016-09-07T21:02:31.143127: step 4995, loss 0.021048, acc 1
2016-09-07T21:02:31.817084: step 4996, loss 0.0091194, acc 1
2016-09-07T21:02:32.480997: step 4997, loss 0.00408082, acc 1
2016-09-07T21:02:33.158651: step 4998, loss 0.0365338, acc 0.98
2016-09-07T21:02:33.824345: step 4999, loss 0.0954009, acc 0.96
2016-09-07T21:02:34.506356: step 5000, loss 0.00384914, acc 1

Evaluation:
2016-09-07T21:02:37.536995: step 5000, loss 2.34918, acc 0.736

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-5000

2016-09-07T21:02:39.159259: step 5001, loss 0.0354696, acc 0.98
2016-09-07T21:02:39.822419: step 5002, loss 0.0870935, acc 0.96
2016-09-07T21:02:40.530454: step 5003, loss 0.039722, acc 0.96
2016-09-07T21:02:41.236126: step 5004, loss 0.0395542, acc 0.98
2016-09-07T21:02:41.925491: step 5005, loss 0.0636519, acc 0.98
2016-09-07T21:02:42.615727: step 5006, loss 0.00394492, acc 1
2016-09-07T21:02:43.292766: step 5007, loss 0.00681358, acc 1
2016-09-07T21:02:43.954286: step 5008, loss 0.0513106, acc 0.98
2016-09-07T21:02:44.627604: step 5009, loss 0.0232769, acc 0.98
2016-09-07T21:02:45.329822: step 5010, loss 0.0186066, acc 1
2016-09-07T21:02:46.015657: step 5011, loss 0.0574194, acc 0.96
2016-09-07T21:02:46.704495: step 5012, loss 0.0251673, acc 0.98
2016-09-07T21:02:47.371688: step 5013, loss 0.0260665, acc 0.98
2016-09-07T21:02:48.034823: step 5014, loss 0.194567, acc 0.94
2016-09-07T21:02:48.704744: step 5015, loss 0.080144, acc 0.96
2016-09-07T21:02:49.380092: step 5016, loss 0.0760625, acc 0.98
2016-09-07T21:02:50.054807: step 5017, loss 0.046656, acc 0.96
2016-09-07T21:02:50.719880: step 5018, loss 0.0217251, acc 1
2016-09-07T21:02:51.383730: step 5019, loss 0.0480154, acc 0.98
2016-09-07T21:02:52.054681: step 5020, loss 0.053989, acc 0.98
2016-09-07T21:02:52.731359: step 5021, loss 0.0220524, acc 1
2016-09-07T21:02:53.392984: step 5022, loss 0.0198316, acc 1
2016-09-07T21:02:54.070139: step 5023, loss 0.0140475, acc 1
2016-09-07T21:02:54.744827: step 5024, loss 0.00492269, acc 1
2016-09-07T21:02:55.424172: step 5025, loss 0.0625053, acc 0.98
2016-09-07T21:02:56.089940: step 5026, loss 0.041739, acc 0.98
2016-09-07T21:02:56.753607: step 5027, loss 0.0093218, acc 1
2016-09-07T21:02:57.457045: step 5028, loss 0.00587117, acc 1
2016-09-07T21:02:58.128015: step 5029, loss 0.0416918, acc 1
2016-09-07T21:02:58.815279: step 5030, loss 0.0192897, acc 1
2016-09-07T21:02:59.499654: step 5031, loss 0.0181643, acc 1
2016-09-07T21:03:00.174327: step 5032, loss 0.012264, acc 1
2016-09-07T21:03:00.891149: step 5033, loss 0.0322549, acc 0.98
2016-09-07T21:03:01.579601: step 5034, loss 0.0662417, acc 0.98
2016-09-07T21:03:02.267197: step 5035, loss 0.0222558, acc 1
2016-09-07T21:03:02.919476: step 5036, loss 0.0273486, acc 1
2016-09-07T21:03:03.589835: step 5037, loss 0.0546259, acc 0.98
2016-09-07T21:03:04.258556: step 5038, loss 0.111014, acc 0.98
2016-09-07T21:03:04.928955: step 5039, loss 0.0751878, acc 0.96
2016-09-07T21:03:05.620338: step 5040, loss 0.0179189, acc 1
2016-09-07T21:03:06.289110: step 5041, loss 0.0160534, acc 1
2016-09-07T21:03:06.969780: step 5042, loss 0.121662, acc 0.96
2016-09-07T21:03:07.645280: step 5043, loss 0.0241615, acc 0.98
2016-09-07T21:03:08.018544: step 5044, loss 0.019416, acc 1
2016-09-07T21:03:08.695648: step 5045, loss 0.0336354, acc 0.98
2016-09-07T21:03:09.373069: step 5046, loss 0.0295581, acc 1
2016-09-07T21:03:10.054158: step 5047, loss 0.141178, acc 0.96
2016-09-07T21:03:10.738509: step 5048, loss 0.0577056, acc 0.98
2016-09-07T21:03:11.427816: step 5049, loss 0.00816538, acc 1
2016-09-07T21:03:12.116394: step 5050, loss 0.0624266, acc 0.96
2016-09-07T21:03:12.786478: step 5051, loss 0.0198375, acc 1
2016-09-07T21:03:13.460602: step 5052, loss 0.0222644, acc 1
2016-09-07T21:03:14.150279: step 5053, loss 0.0289909, acc 0.98
2016-09-07T21:03:14.828858: step 5054, loss 0.0499237, acc 0.96
2016-09-07T21:03:15.504677: step 5055, loss 0.0297941, acc 0.98
2016-09-07T21:03:16.179232: step 5056, loss 0.0387224, acc 0.96
2016-09-07T21:03:16.853241: step 5057, loss 0.0286362, acc 1
2016-09-07T21:03:17.518293: step 5058, loss 0.0269404, acc 0.98
2016-09-07T21:03:18.182690: step 5059, loss 0.0204343, acc 0.98
2016-09-07T21:03:18.860216: step 5060, loss 0.0309702, acc 0.98
2016-09-07T21:03:19.538732: step 5061, loss 0.0242519, acc 0.98
2016-09-07T21:03:20.221811: step 5062, loss 0.0149666, acc 1
2016-09-07T21:03:20.908369: step 5063, loss 0.024161, acc 1
2016-09-07T21:03:21.571182: step 5064, loss 0.00943927, acc 1
2016-09-07T21:03:22.235690: step 5065, loss 0.0294285, acc 1
2016-09-07T21:03:22.912946: step 5066, loss 0.15592, acc 0.96
2016-09-07T21:03:23.578530: step 5067, loss 0.0176691, acc 1
2016-09-07T21:03:24.257560: step 5068, loss 0.0120364, acc 1
2016-09-07T21:03:24.928835: step 5069, loss 0.00498893, acc 1
2016-09-07T21:03:25.589915: step 5070, loss 0.00453666, acc 1
2016-09-07T21:03:26.273997: step 5071, loss 0.0398443, acc 0.98
2016-09-07T21:03:26.965870: step 5072, loss 0.00556172, acc 1
2016-09-07T21:03:27.620937: step 5073, loss 0.00582472, acc 1
2016-09-07T21:03:28.292027: step 5074, loss 0.0536152, acc 0.96
2016-09-07T21:03:28.958588: step 5075, loss 0.0111907, acc 1
2016-09-07T21:03:29.628064: step 5076, loss 0.0982742, acc 0.94
2016-09-07T21:03:30.301905: step 5077, loss 0.0508287, acc 0.96
2016-09-07T21:03:30.968091: step 5078, loss 0.071121, acc 0.96
2016-09-07T21:03:31.638648: step 5079, loss 0.00841003, acc 1
2016-09-07T21:03:32.301351: step 5080, loss 0.0202878, acc 0.98
2016-09-07T21:03:32.961005: step 5081, loss 0.0119573, acc 1
2016-09-07T21:03:33.634056: step 5082, loss 0.00556953, acc 1
2016-09-07T21:03:34.319055: step 5083, loss 0.0184804, acc 0.98
2016-09-07T21:03:34.986760: step 5084, loss 0.0359297, acc 0.98
2016-09-07T21:03:35.647870: step 5085, loss 0.0201665, acc 0.98
2016-09-07T21:03:36.325564: step 5086, loss 0.0580352, acc 0.98
2016-09-07T21:03:37.010578: step 5087, loss 0.00550584, acc 1
2016-09-07T21:03:37.677371: step 5088, loss 0.0286778, acc 1
2016-09-07T21:03:38.335948: step 5089, loss 0.0331844, acc 0.98
2016-09-07T21:03:39.003785: step 5090, loss 0.00805325, acc 1
2016-09-07T21:03:39.663986: step 5091, loss 0.0179396, acc 0.98
2016-09-07T21:03:40.326934: step 5092, loss 0.0664059, acc 0.94
2016-09-07T21:03:41.016193: step 5093, loss 0.0190801, acc 0.98
2016-09-07T21:03:41.718581: step 5094, loss 0.0288536, acc 1
2016-09-07T21:03:42.378811: step 5095, loss 0.022944, acc 0.98
2016-09-07T21:03:43.042737: step 5096, loss 0.0391535, acc 0.98
2016-09-07T21:03:43.703334: step 5097, loss 0.0467647, acc 0.98
2016-09-07T21:03:44.380337: step 5098, loss 0.180669, acc 0.98
2016-09-07T21:03:45.070563: step 5099, loss 0.0313211, acc 0.98
2016-09-07T21:03:45.762653: step 5100, loss 0.0183354, acc 1

Evaluation:
2016-09-07T21:03:48.777183: step 5100, loss 2.03716, acc 0.73

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-5100

2016-09-07T21:03:50.342983: step 5101, loss 0.259063, acc 0.96
2016-09-07T21:03:51.014347: step 5102, loss 0.0208262, acc 0.98
2016-09-07T21:03:51.677466: step 5103, loss 0.0260345, acc 0.98
2016-09-07T21:03:52.329680: step 5104, loss 0.0308055, acc 0.98
2016-09-07T21:03:53.010487: step 5105, loss 0.0699338, acc 0.96
2016-09-07T21:03:53.667751: step 5106, loss 0.0453315, acc 0.98
2016-09-07T21:03:54.347635: step 5107, loss 0.112648, acc 0.94
2016-09-07T21:03:55.022697: step 5108, loss 0.0390052, acc 1
2016-09-07T21:03:55.675032: step 5109, loss 0.0625751, acc 0.96
2016-09-07T21:03:56.330312: step 5110, loss 0.117028, acc 0.98
2016-09-07T21:03:57.013082: step 5111, loss 0.0458967, acc 0.98
2016-09-07T21:03:57.689676: step 5112, loss 0.0169025, acc 1
2016-09-07T21:03:58.376134: step 5113, loss 0.0122198, acc 1
2016-09-07T21:03:59.060707: step 5114, loss 0.0234108, acc 1
2016-09-07T21:03:59.737824: step 5115, loss 0.0391124, acc 0.96
2016-09-07T21:04:00.435152: step 5116, loss 0.0182086, acc 1
2016-09-07T21:04:01.104910: step 5117, loss 0.00900355, acc 1
2016-09-07T21:04:01.772418: step 5118, loss 0.0225512, acc 0.98
2016-09-07T21:04:02.429873: step 5119, loss 0.0205051, acc 0.98
2016-09-07T21:04:03.096416: step 5120, loss 0.049329, acc 0.98
2016-09-07T21:04:03.775693: step 5121, loss 0.0232489, acc 1
2016-09-07T21:04:04.472407: step 5122, loss 0.047388, acc 0.98
2016-09-07T21:04:05.142446: step 5123, loss 0.0292589, acc 0.98
2016-09-07T21:04:05.800712: step 5124, loss 0.0279705, acc 1
2016-09-07T21:04:06.486214: step 5125, loss 0.00968819, acc 1
2016-09-07T21:04:07.166380: step 5126, loss 0.0094504, acc 1
2016-09-07T21:04:07.829643: step 5127, loss 0.039414, acc 0.98
2016-09-07T21:04:08.504160: step 5128, loss 0.0445364, acc 0.98
2016-09-07T21:04:09.199052: step 5129, loss 0.037553, acc 0.98
2016-09-07T21:04:09.881300: step 5130, loss 0.0269064, acc 1
2016-09-07T21:04:10.528160: step 5131, loss 0.00710762, acc 1
2016-09-07T21:04:11.208111: step 5132, loss 0.122648, acc 0.96
2016-09-07T21:04:11.860033: step 5133, loss 0.06263, acc 0.96
2016-09-07T21:04:12.534432: step 5134, loss 0.0323339, acc 1
2016-09-07T21:04:13.197396: step 5135, loss 0.0462977, acc 0.98
2016-09-07T21:04:13.869481: step 5136, loss 0.0457274, acc 1
2016-09-07T21:04:14.553686: step 5137, loss 0.0476102, acc 0.98
2016-09-07T21:04:15.213084: step 5138, loss 0.00427748, acc 1
2016-09-07T21:04:15.889054: step 5139, loss 0.0352856, acc 0.98
2016-09-07T21:04:16.575146: step 5140, loss 0.0148771, acc 1
2016-09-07T21:04:17.256621: step 5141, loss 0.0193146, acc 0.98
2016-09-07T21:04:17.922874: step 5142, loss 0.0587795, acc 0.96
2016-09-07T21:04:18.590516: step 5143, loss 0.0222223, acc 0.98
2016-09-07T21:04:19.262396: step 5144, loss 0.0366853, acc 0.98
2016-09-07T21:04:19.940862: step 5145, loss 0.053085, acc 0.96
2016-09-07T21:04:20.625742: step 5146, loss 0.00565328, acc 1
2016-09-07T21:04:21.318324: step 5147, loss 0.0150785, acc 1
2016-09-07T21:04:21.988069: step 5148, loss 0.0373614, acc 1
2016-09-07T21:04:22.656944: step 5149, loss 0.0524483, acc 0.98
2016-09-07T21:04:23.337957: step 5150, loss 0.0326745, acc 0.98
2016-09-07T21:04:24.012610: step 5151, loss 0.0145983, acc 1
2016-09-07T21:04:24.674735: step 5152, loss 0.00772279, acc 1
2016-09-07T21:04:25.361159: step 5153, loss 0.0991861, acc 0.96
2016-09-07T21:04:26.038040: step 5154, loss 0.0593214, acc 0.98
2016-09-07T21:04:26.707470: step 5155, loss 0.232682, acc 0.96
2016-09-07T21:04:27.388504: step 5156, loss 0.0164962, acc 1
2016-09-07T21:04:28.071033: step 5157, loss 0.0125113, acc 1
2016-09-07T21:04:28.743131: step 5158, loss 0.0281, acc 0.98
2016-09-07T21:04:29.414514: step 5159, loss 0.0242707, acc 1
2016-09-07T21:04:30.101277: step 5160, loss 0.0227944, acc 1
2016-09-07T21:04:30.752652: step 5161, loss 0.0268377, acc 1
2016-09-07T21:04:31.426400: step 5162, loss 0.0214047, acc 1
2016-09-07T21:04:32.102089: step 5163, loss 0.0411588, acc 0.98
2016-09-07T21:04:32.765992: step 5164, loss 0.100289, acc 0.96
2016-09-07T21:04:33.411034: step 5165, loss 0.0238466, acc 0.98
2016-09-07T21:04:34.077214: step 5166, loss 0.138775, acc 0.96
2016-09-07T21:04:34.734220: step 5167, loss 0.036404, acc 0.96
2016-09-07T21:04:35.402284: step 5168, loss 0.0239676, acc 0.98
2016-09-07T21:04:36.068671: step 5169, loss 0.0638838, acc 0.98
2016-09-07T21:04:36.742339: step 5170, loss 0.0271081, acc 1
2016-09-07T21:04:37.414112: step 5171, loss 0.0522718, acc 0.96
2016-09-07T21:04:38.096543: step 5172, loss 0.0441943, acc 0.96
2016-09-07T21:04:38.760370: step 5173, loss 0.00946002, acc 1
2016-09-07T21:04:39.427326: step 5174, loss 0.0239365, acc 0.98
2016-09-07T21:04:40.094869: step 5175, loss 0.034074, acc 0.98
2016-09-07T21:04:40.762767: step 5176, loss 0.0311341, acc 1
2016-09-07T21:04:41.445758: step 5177, loss 0.0176289, acc 1
2016-09-07T21:04:42.120430: step 5178, loss 0.0629994, acc 0.96
2016-09-07T21:04:42.788865: step 5179, loss 0.048045, acc 1
2016-09-07T21:04:43.458658: step 5180, loss 0.0561985, acc 0.98
2016-09-07T21:04:44.137404: step 5181, loss 0.0086925, acc 1
2016-09-07T21:04:44.802335: step 5182, loss 0.012041, acc 1
2016-09-07T21:04:45.473699: step 5183, loss 0.0145063, acc 1
2016-09-07T21:04:46.162830: step 5184, loss 0.0352359, acc 1
2016-09-07T21:04:46.820247: step 5185, loss 0.0415471, acc 0.98
2016-09-07T21:04:47.502626: step 5186, loss 0.0234638, acc 1
2016-09-07T21:04:48.182671: step 5187, loss 0.0204535, acc 1
2016-09-07T21:04:48.863533: step 5188, loss 0.0757342, acc 0.98
2016-09-07T21:04:49.553773: step 5189, loss 0.0145811, acc 1
2016-09-07T21:04:50.227376: step 5190, loss 0.0119573, acc 1
2016-09-07T21:04:50.916846: step 5191, loss 0.00951718, acc 1
2016-09-07T21:04:51.600831: step 5192, loss 0.0413534, acc 0.98
2016-09-07T21:04:52.273023: step 5193, loss 0.00931917, acc 1
2016-09-07T21:04:52.906479: step 5194, loss 0.0209215, acc 0.98
2016-09-07T21:04:53.596004: step 5195, loss 0.0476305, acc 0.98
2016-09-07T21:04:54.282726: step 5196, loss 0.0498668, acc 0.98
2016-09-07T21:04:54.958695: step 5197, loss 0.0272584, acc 1
2016-09-07T21:04:55.627973: step 5198, loss 0.01555, acc 1
2016-09-07T21:04:56.302401: step 5199, loss 0.0439221, acc 0.98
2016-09-07T21:04:56.977758: step 5200, loss 0.0289647, acc 0.98

Evaluation:
2016-09-07T21:05:00.041080: step 5200, loss 2.06428, acc 0.741

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-5200

2016-09-07T21:05:01.757280: step 5201, loss 0.0256436, acc 0.98
2016-09-07T21:05:02.428584: step 5202, loss 0.0332105, acc 0.98
2016-09-07T21:05:03.115004: step 5203, loss 0.0375327, acc 0.98
2016-09-07T21:05:03.781691: step 5204, loss 0.00408137, acc 1
2016-09-07T21:05:04.438139: step 5205, loss 0.0249863, acc 1
2016-09-07T21:05:05.104273: step 5206, loss 0.00633632, acc 1
2016-09-07T21:05:05.769647: step 5207, loss 0.00349092, acc 1
2016-09-07T21:05:06.441298: step 5208, loss 0.00920476, acc 1
2016-09-07T21:05:07.102186: step 5209, loss 0.0173256, acc 0.98
2016-09-07T21:05:07.774142: step 5210, loss 0.0187196, acc 1
2016-09-07T21:05:08.446688: step 5211, loss 0.0691602, acc 0.96
2016-09-07T21:05:09.113347: step 5212, loss 0.0224125, acc 1
2016-09-07T21:05:09.790696: step 5213, loss 0.007704, acc 1
2016-09-07T21:05:10.461612: step 5214, loss 0.0540956, acc 0.98
2016-09-07T21:05:11.133260: step 5215, loss 0.030532, acc 0.98
2016-09-07T21:05:11.807322: step 5216, loss 0.0241129, acc 0.98
2016-09-07T21:05:12.487540: step 5217, loss 0.0125601, acc 1
2016-09-07T21:05:13.163415: step 5218, loss 0.01132, acc 1
2016-09-07T21:05:13.869999: step 5219, loss 0.00475656, acc 1
2016-09-07T21:05:14.528733: step 5220, loss 0.00900611, acc 1
2016-09-07T21:05:15.173187: step 5221, loss 0.0755608, acc 0.98
2016-09-07T21:05:15.842851: step 5222, loss 0.0115766, acc 1
2016-09-07T21:05:16.518399: step 5223, loss 0.0195419, acc 1
2016-09-07T21:05:17.193102: step 5224, loss 0.00747907, acc 1
2016-09-07T21:05:17.863530: step 5225, loss 0.0600487, acc 0.96
2016-09-07T21:05:18.540466: step 5226, loss 0.0302854, acc 0.98
2016-09-07T21:05:19.218958: step 5227, loss 0.021429, acc 1
2016-09-07T21:05:19.910378: step 5228, loss 0.0038783, acc 1
2016-09-07T21:05:20.597840: step 5229, loss 0.0204358, acc 1
2016-09-07T21:05:21.272978: step 5230, loss 0.0412189, acc 0.98
2016-09-07T21:05:21.955193: step 5231, loss 0.0372797, acc 1
2016-09-07T21:05:22.636471: step 5232, loss 0.0487836, acc 0.98
2016-09-07T21:05:23.306327: step 5233, loss 0.00466519, acc 1
2016-09-07T21:05:23.994568: step 5234, loss 0.0433283, acc 0.98
2016-09-07T21:05:24.660472: step 5235, loss 0.089554, acc 0.94
2016-09-07T21:05:25.322978: step 5236, loss 0.018466, acc 0.98
2016-09-07T21:05:25.986949: step 5237, loss 0.0262089, acc 0.98
2016-09-07T21:05:26.342175: step 5238, loss 0.00312943, acc 1
2016-09-07T21:05:27.017488: step 5239, loss 0.00819115, acc 1
2016-09-07T21:05:27.710624: step 5240, loss 0.0176873, acc 1
2016-09-07T21:05:28.389609: step 5241, loss 0.0486277, acc 0.98
2016-09-07T21:05:29.068469: step 5242, loss 0.0211139, acc 1
2016-09-07T21:05:29.758912: step 5243, loss 0.0927116, acc 0.96
2016-09-07T21:05:30.438185: step 5244, loss 0.0374702, acc 0.98
2016-09-07T21:05:31.114802: step 5245, loss 0.0145517, acc 1
2016-09-07T21:05:31.788361: step 5246, loss 0.0223638, acc 1
2016-09-07T21:05:32.457855: step 5247, loss 0.0486784, acc 0.98
2016-09-07T21:05:33.134251: step 5248, loss 0.00491882, acc 1
2016-09-07T21:05:33.807247: step 5249, loss 0.0339752, acc 0.96
2016-09-07T21:05:34.487878: step 5250, loss 0.0592117, acc 0.98
2016-09-07T21:05:35.175009: step 5251, loss 0.0255725, acc 1
2016-09-07T21:05:35.836489: step 5252, loss 0.0325676, acc 0.98
2016-09-07T21:05:36.511432: step 5253, loss 0.0179733, acc 1
2016-09-07T21:05:37.176863: step 5254, loss 0.0025754, acc 1
2016-09-07T21:05:37.841702: step 5255, loss 0.006777, acc 1
2016-09-07T21:05:38.521046: step 5256, loss 0.0444865, acc 0.98
2016-09-07T21:05:39.199578: step 5257, loss 0.0277776, acc 0.98
2016-09-07T21:05:39.847967: step 5258, loss 0.0200483, acc 0.98
2016-09-07T21:05:40.507311: step 5259, loss 0.0286016, acc 0.98
2016-09-07T21:05:41.198268: step 5260, loss 0.0386667, acc 0.98
2016-09-07T21:05:41.877706: step 5261, loss 0.00535655, acc 1
2016-09-07T21:05:42.532240: step 5262, loss 0.00819166, acc 1
2016-09-07T21:05:43.203801: step 5263, loss 0.0214333, acc 0.98
2016-09-07T21:05:43.876886: step 5264, loss 0.00427101, acc 1
2016-09-07T21:05:44.555671: step 5265, loss 0.0231959, acc 1
2016-09-07T21:05:45.211533: step 5266, loss 0.0330792, acc 0.96
2016-09-07T21:05:45.893300: step 5267, loss 0.00966708, acc 1
2016-09-07T21:05:46.580120: step 5268, loss 0.0657437, acc 0.98
2016-09-07T21:05:47.237842: step 5269, loss 0.00793075, acc 1
2016-09-07T21:05:47.923340: step 5270, loss 0.0339401, acc 0.98
2016-09-07T21:05:48.600439: step 5271, loss 0.0214101, acc 1
2016-09-07T21:05:49.280034: step 5272, loss 0.0756508, acc 0.96
2016-09-07T21:05:49.948900: step 5273, loss 0.0310362, acc 0.98
2016-09-07T21:05:50.606501: step 5274, loss 0.0144632, acc 1
2016-09-07T21:05:51.281992: step 5275, loss 0.0855846, acc 0.98
2016-09-07T21:05:51.957211: step 5276, loss 0.0387324, acc 0.96
2016-09-07T21:05:52.637323: step 5277, loss 0.00245462, acc 1
2016-09-07T21:05:53.312474: step 5278, loss 0.0296178, acc 0.98
2016-09-07T21:05:53.968399: step 5279, loss 0.0578787, acc 0.98
2016-09-07T21:05:54.649973: step 5280, loss 0.0379439, acc 0.98
2016-09-07T21:05:55.318512: step 5281, loss 0.0285172, acc 0.98
2016-09-07T21:05:55.991712: step 5282, loss 0.00385155, acc 1
2016-09-07T21:05:56.636201: step 5283, loss 0.0379008, acc 0.98
2016-09-07T21:05:57.305733: step 5284, loss 0.130666, acc 0.96
2016-09-07T21:05:58.005374: step 5285, loss 0.0744518, acc 0.98
2016-09-07T21:05:58.657800: step 5286, loss 0.0261479, acc 1
2016-09-07T21:05:59.332572: step 5287, loss 0.00930862, acc 1
2016-09-07T21:06:00.015887: step 5288, loss 0.0431225, acc 0.98
2016-09-07T21:06:00.723797: step 5289, loss 0.0190899, acc 0.98
2016-09-07T21:06:01.395400: step 5290, loss 0.0106846, acc 1
2016-09-07T21:06:02.066426: step 5291, loss 0.0418875, acc 0.98
2016-09-07T21:06:02.746468: step 5292, loss 0.0739242, acc 0.98
2016-09-07T21:06:03.403098: step 5293, loss 0.0112272, acc 1
2016-09-07T21:06:04.066010: step 5294, loss 0.0120778, acc 1
2016-09-07T21:06:04.733365: step 5295, loss 0.0176268, acc 1
2016-09-07T21:06:05.398432: step 5296, loss 0.00473704, acc 1
2016-09-07T21:06:06.086830: step 5297, loss 0.0295832, acc 1
2016-09-07T21:06:06.765050: step 5298, loss 0.0462077, acc 0.98
2016-09-07T21:06:07.431309: step 5299, loss 0.0499882, acc 0.98
2016-09-07T21:06:08.085759: step 5300, loss 0.0156521, acc 1

Evaluation:
2016-09-07T21:06:11.130217: step 5300, loss 1.62425, acc 0.737

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-5300

2016-09-07T21:06:12.773909: step 5301, loss 0.0257293, acc 1
2016-09-07T21:06:13.458641: step 5302, loss 0.0221018, acc 1
2016-09-07T21:06:14.140932: step 5303, loss 0.0388502, acc 0.98
2016-09-07T21:06:14.799434: step 5304, loss 0.0170739, acc 1
2016-09-07T21:06:15.462062: step 5305, loss 0.0556556, acc 0.96
2016-09-07T21:06:16.135161: step 5306, loss 0.020482, acc 1
2016-09-07T21:06:16.821130: step 5307, loss 0.0905318, acc 0.98
2016-09-07T21:06:17.509101: step 5308, loss 0.0314236, acc 0.96
2016-09-07T21:06:18.203801: step 5309, loss 0.0376486, acc 0.98
2016-09-07T21:06:18.894192: step 5310, loss 0.0188006, acc 1
2016-09-07T21:06:19.588613: step 5311, loss 0.00934302, acc 1
2016-09-07T21:06:20.257986: step 5312, loss 0.0201993, acc 1
2016-09-07T21:06:20.939936: step 5313, loss 0.00625928, acc 1
2016-09-07T21:06:21.603304: step 5314, loss 0.0843607, acc 0.96
2016-09-07T21:06:22.274205: step 5315, loss 0.018932, acc 1
2016-09-07T21:06:22.948449: step 5316, loss 0.00686359, acc 1
2016-09-07T21:06:23.615082: step 5317, loss 0.00496316, acc 1
2016-09-07T21:06:24.284559: step 5318, loss 0.104483, acc 0.96
2016-09-07T21:06:24.963937: step 5319, loss 0.056073, acc 0.96
2016-09-07T21:06:25.636873: step 5320, loss 0.0210963, acc 1
2016-09-07T21:06:26.319730: step 5321, loss 0.0152055, acc 1
2016-09-07T21:06:26.985412: step 5322, loss 0.0314611, acc 0.98
2016-09-07T21:06:27.662441: step 5323, loss 0.0146039, acc 1
2016-09-07T21:06:28.325809: step 5324, loss 0.00607092, acc 1
2016-09-07T21:06:28.998265: step 5325, loss 0.0127335, acc 1
2016-09-07T21:06:29.658148: step 5326, loss 0.0129356, acc 1
2016-09-07T21:06:30.327683: step 5327, loss 0.00653477, acc 1
2016-09-07T21:06:31.005215: step 5328, loss 0.0296059, acc 1
2016-09-07T21:06:31.681616: step 5329, loss 0.0327899, acc 0.98
2016-09-07T21:06:32.343845: step 5330, loss 0.00759504, acc 1
2016-09-07T21:06:33.023093: step 5331, loss 0.0281899, acc 0.98
2016-09-07T21:06:33.687806: step 5332, loss 0.0863507, acc 0.98
2016-09-07T21:06:34.375391: step 5333, loss 0.100971, acc 0.96
2016-09-07T21:06:35.030110: step 5334, loss 0.090689, acc 0.98
2016-09-07T21:06:35.702962: step 5335, loss 0.0160395, acc 1
2016-09-07T21:06:36.362597: step 5336, loss 0.0662792, acc 0.94
2016-09-07T21:06:37.042767: step 5337, loss 0.0242994, acc 1
2016-09-07T21:06:37.716315: step 5338, loss 0.0569861, acc 0.98
2016-09-07T21:06:38.392289: step 5339, loss 0.00774593, acc 1
2016-09-07T21:06:39.062194: step 5340, loss 0.0511163, acc 0.98
2016-09-07T21:06:39.712404: step 5341, loss 0.0533472, acc 0.98
2016-09-07T21:06:40.385089: step 5342, loss 0.0056353, acc 1
2016-09-07T21:06:41.060259: step 5343, loss 0.0575212, acc 0.96
2016-09-07T21:06:41.755587: step 5344, loss 0.0210627, acc 0.98
2016-09-07T21:06:42.429767: step 5345, loss 0.049404, acc 0.96
2016-09-07T21:06:43.095314: step 5346, loss 0.0271025, acc 1
2016-09-07T21:06:43.810598: step 5347, loss 0.0416033, acc 1
2016-09-07T21:06:44.493791: step 5348, loss 0.0699514, acc 0.96
2016-09-07T21:06:45.203158: step 5349, loss 0.0979396, acc 0.94
2016-09-07T21:06:45.872447: step 5350, loss 0.0397028, acc 0.98
2016-09-07T21:06:46.538412: step 5351, loss 0.0207831, acc 1
2016-09-07T21:06:47.190938: step 5352, loss 0.0203995, acc 1
2016-09-07T21:06:47.859961: step 5353, loss 0.0449461, acc 0.98
2016-09-07T21:06:48.530828: step 5354, loss 0.0441425, acc 0.98
2016-09-07T21:06:49.195926: step 5355, loss 0.0399126, acc 0.98
2016-09-07T21:06:49.840754: step 5356, loss 0.028172, acc 1
2016-09-07T21:06:50.508178: step 5357, loss 0.156804, acc 0.96
2016-09-07T21:06:51.179331: step 5358, loss 0.0498295, acc 0.96
2016-09-07T21:06:51.857588: step 5359, loss 0.0140564, acc 1
2016-09-07T21:06:52.511064: step 5360, loss 0.0417233, acc 0.98
2016-09-07T21:06:53.178750: step 5361, loss 0.0846827, acc 0.96
2016-09-07T21:06:53.854295: step 5362, loss 0.00487904, acc 1
2016-09-07T21:06:54.533469: step 5363, loss 0.0288876, acc 0.98
2016-09-07T21:06:55.206621: step 5364, loss 0.0168319, acc 1
2016-09-07T21:06:55.878288: step 5365, loss 0.0556574, acc 0.96
2016-09-07T21:06:56.536359: step 5366, loss 0.00740036, acc 1
2016-09-07T21:06:57.211656: step 5367, loss 0.0102757, acc 1
2016-09-07T21:06:57.893933: step 5368, loss 0.0115367, acc 1
2016-09-07T21:06:58.573502: step 5369, loss 0.0806238, acc 0.94
2016-09-07T21:06:59.253345: step 5370, loss 0.0234901, acc 0.98
2016-09-07T21:06:59.920581: step 5371, loss 0.0361619, acc 0.98
2016-09-07T21:07:00.638321: step 5372, loss 0.0553559, acc 0.98
2016-09-07T21:07:01.285279: step 5373, loss 0.0469253, acc 1
2016-09-07T21:07:01.944153: step 5374, loss 0.0122034, acc 1
2016-09-07T21:07:02.611894: step 5375, loss 0.00507591, acc 1
2016-09-07T21:07:03.284967: step 5376, loss 0.00351259, acc 1
2016-09-07T21:07:03.937250: step 5377, loss 0.00573019, acc 1
2016-09-07T21:07:04.634614: step 5378, loss 0.0454102, acc 0.96
2016-09-07T21:07:05.314390: step 5379, loss 0.044262, acc 0.98
2016-09-07T21:07:05.963581: step 5380, loss 0.0296378, acc 1
2016-09-07T21:07:06.627512: step 5381, loss 0.122698, acc 0.98
2016-09-07T21:07:07.294759: step 5382, loss 0.013421, acc 1
2016-09-07T21:07:07.967466: step 5383, loss 0.0121131, acc 1
2016-09-07T21:07:08.634140: step 5384, loss 0.00327985, acc 1
2016-09-07T21:07:09.311622: step 5385, loss 0.0466342, acc 1
2016-09-07T21:07:09.987092: step 5386, loss 0.133546, acc 0.98
2016-09-07T21:07:10.663555: step 5387, loss 0.0947835, acc 0.96
2016-09-07T21:07:11.338405: step 5388, loss 0.0051206, acc 1
2016-09-07T21:07:12.005482: step 5389, loss 0.0540351, acc 0.98
2016-09-07T21:07:12.671601: step 5390, loss 0.0361266, acc 0.98
2016-09-07T21:07:13.360057: step 5391, loss 0.0446732, acc 0.96
2016-09-07T21:07:14.043791: step 5392, loss 0.0438832, acc 0.96
2016-09-07T21:07:14.708384: step 5393, loss 0.0412636, acc 0.98
2016-09-07T21:07:15.366704: step 5394, loss 0.042455, acc 0.98
2016-09-07T21:07:16.026400: step 5395, loss 0.00491532, acc 1
2016-09-07T21:07:16.708970: step 5396, loss 0.0112421, acc 1
2016-09-07T21:07:17.394848: step 5397, loss 0.005009, acc 1
2016-09-07T21:07:18.058721: step 5398, loss 0.0223602, acc 0.98
2016-09-07T21:07:18.727853: step 5399, loss 0.0287595, acc 0.98
2016-09-07T21:07:19.389692: step 5400, loss 0.023929, acc 1

Evaluation:
2016-09-07T21:07:22.452435: step 5400, loss 1.90223, acc 0.733

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-5400

2016-09-07T21:07:24.085307: step 5401, loss 0.00395232, acc 1
2016-09-07T21:07:24.770555: step 5402, loss 0.0516759, acc 0.96
2016-09-07T21:07:25.428681: step 5403, loss 0.0144961, acc 1
2016-09-07T21:07:26.090567: step 5404, loss 0.0454881, acc 0.96
2016-09-07T21:07:26.752702: step 5405, loss 0.0209095, acc 1
2016-09-07T21:07:27.409930: step 5406, loss 0.0267448, acc 0.98
2016-09-07T21:07:28.065382: step 5407, loss 0.040798, acc 0.98
2016-09-07T21:07:28.736371: step 5408, loss 0.108439, acc 0.92
2016-09-07T21:07:29.412239: step 5409, loss 0.0417254, acc 0.96
2016-09-07T21:07:30.106905: step 5410, loss 0.0399629, acc 0.98
2016-09-07T21:07:30.776979: step 5411, loss 0.0221073, acc 0.98
2016-09-07T21:07:31.442286: step 5412, loss 0.00448919, acc 1
2016-09-07T21:07:32.130503: step 5413, loss 0.0143831, acc 1
2016-09-07T21:07:32.793148: step 5414, loss 0.00405466, acc 1
2016-09-07T21:07:33.460115: step 5415, loss 0.067389, acc 0.98
2016-09-07T21:07:34.131703: step 5416, loss 0.0112006, acc 1
2016-09-07T21:07:34.802259: step 5417, loss 0.0208584, acc 1
2016-09-07T21:07:35.484683: step 5418, loss 0.0685034, acc 0.98
2016-09-07T21:07:36.167852: step 5419, loss 0.0284768, acc 0.98
2016-09-07T21:07:36.846270: step 5420, loss 0.0174827, acc 1
2016-09-07T21:07:37.520697: step 5421, loss 0.00522446, acc 1
2016-09-07T21:07:38.192306: step 5422, loss 0.0204205, acc 0.98
2016-09-07T21:07:38.868453: step 5423, loss 0.106949, acc 0.96
2016-09-07T21:07:39.542819: step 5424, loss 0.0253319, acc 0.98
2016-09-07T21:07:40.208543: step 5425, loss 0.0624324, acc 0.98
2016-09-07T21:07:40.900088: step 5426, loss 0.00549471, acc 1
2016-09-07T21:07:41.591503: step 5427, loss 0.0139842, acc 1
2016-09-07T21:07:42.261793: step 5428, loss 0.066832, acc 0.94
2016-09-07T21:07:42.943742: step 5429, loss 0.0207507, acc 1
2016-09-07T21:07:43.628558: step 5430, loss 0.0209276, acc 1
2016-09-07T21:07:44.295622: step 5431, loss 0.0281825, acc 1
2016-09-07T21:07:44.653571: step 5432, loss 0.0706878, acc 0.916667
2016-09-07T21:07:45.301944: step 5433, loss 0.0201355, acc 1
2016-09-07T21:07:45.966651: step 5434, loss 0.0513802, acc 0.98
2016-09-07T21:07:46.639235: step 5435, loss 0.0251499, acc 1
2016-09-07T21:07:47.328082: step 5436, loss 0.0268173, acc 1
2016-09-07T21:07:48.006562: step 5437, loss 0.0392963, acc 0.98
2016-09-07T21:07:48.680073: step 5438, loss 0.0294876, acc 1
2016-09-07T21:07:49.354218: step 5439, loss 0.0206921, acc 0.98
2016-09-07T21:07:50.037311: step 5440, loss 0.010538, acc 1
2016-09-07T21:07:50.696200: step 5441, loss 0.00345402, acc 1
2016-09-07T21:07:51.363013: step 5442, loss 0.0121026, acc 1
2016-09-07T21:07:52.051089: step 5443, loss 0.0521875, acc 0.98
2016-09-07T21:07:52.728124: step 5444, loss 0.0329117, acc 0.98
2016-09-07T21:07:53.403370: step 5445, loss 0.0527462, acc 0.98
2016-09-07T21:07:54.082086: step 5446, loss 0.0725814, acc 0.96
2016-09-07T21:07:54.760720: step 5447, loss 0.016406, acc 1
2016-09-07T21:07:55.427211: step 5448, loss 0.0529502, acc 0.98
2016-09-07T21:07:56.097100: step 5449, loss 0.0209381, acc 0.98
2016-09-07T21:07:56.774458: step 5450, loss 0.0534589, acc 0.96
2016-09-07T21:07:57.457850: step 5451, loss 0.0246221, acc 1
2016-09-07T21:07:58.126044: step 5452, loss 0.0302801, acc 0.98
2016-09-07T21:07:58.794273: step 5453, loss 0.114047, acc 0.92
2016-09-07T21:07:59.469721: step 5454, loss 0.0187212, acc 1
2016-09-07T21:08:00.144383: step 5455, loss 0.0255265, acc 0.98
2016-09-07T21:08:00.868023: step 5456, loss 0.0038161, acc 1
2016-09-07T21:08:01.533551: step 5457, loss 0.0171698, acc 1
2016-09-07T21:08:02.202897: step 5458, loss 0.0587645, acc 0.96
2016-09-07T21:08:02.886989: step 5459, loss 0.0104261, acc 1
2016-09-07T21:08:03.547223: step 5460, loss 0.0944183, acc 0.98
2016-09-07T21:08:04.224313: step 5461, loss 0.0645821, acc 0.98
2016-09-07T21:08:04.904543: step 5462, loss 0.0216916, acc 1
2016-09-07T21:08:05.580060: step 5463, loss 0.017042, acc 1
2016-09-07T21:08:06.252528: step 5464, loss 0.111621, acc 0.96
2016-09-07T21:08:06.934942: step 5465, loss 0.0187026, acc 1
2016-09-07T21:08:07.614329: step 5466, loss 0.0122158, acc 1
2016-09-07T21:08:08.297271: step 5467, loss 0.00601136, acc 1
2016-09-07T21:08:08.954939: step 5468, loss 0.0279619, acc 1
2016-09-07T21:08:09.620613: step 5469, loss 0.0169011, acc 1
2016-09-07T21:08:10.279370: step 5470, loss 0.020912, acc 1
2016-09-07T21:08:10.949480: step 5471, loss 0.00670317, acc 1
2016-09-07T21:08:11.611615: step 5472, loss 0.0029604, acc 1
2016-09-07T21:08:12.288315: step 5473, loss 0.0293732, acc 1
2016-09-07T21:08:12.956004: step 5474, loss 0.00329745, acc 1
2016-09-07T21:08:13.630802: step 5475, loss 0.0287989, acc 0.98
2016-09-07T21:08:14.326703: step 5476, loss 0.00764281, acc 1
2016-09-07T21:08:15.007569: step 5477, loss 0.0295451, acc 0.98
2016-09-07T21:08:15.688624: step 5478, loss 0.0389782, acc 0.96
2016-09-07T21:08:16.366723: step 5479, loss 0.0729055, acc 0.96
2016-09-07T21:08:17.019161: step 5480, loss 0.0182783, acc 1
2016-09-07T21:08:17.680755: step 5481, loss 0.0126652, acc 1
2016-09-07T21:08:18.334308: step 5482, loss 0.0279007, acc 0.98
2016-09-07T21:08:19.016541: step 5483, loss 0.0367613, acc 0.98
2016-09-07T21:08:19.687061: step 5484, loss 0.0407651, acc 0.98
2016-09-07T21:08:20.354854: step 5485, loss 0.00628404, acc 1
2016-09-07T21:08:21.007201: step 5486, loss 0.104762, acc 0.98
2016-09-07T21:08:21.672348: step 5487, loss 0.0104691, acc 1
2016-09-07T21:08:22.339969: step 5488, loss 0.033531, acc 0.98
2016-09-07T21:08:23.026020: step 5489, loss 0.0188093, acc 1
2016-09-07T21:08:23.688856: step 5490, loss 0.00298493, acc 1
2016-09-07T21:08:24.358873: step 5491, loss 0.0194794, acc 1
2016-09-07T21:08:25.021729: step 5492, loss 0.00515839, acc 1
2016-09-07T21:08:25.682894: step 5493, loss 0.0660121, acc 0.94
2016-09-07T21:08:26.350653: step 5494, loss 0.0123567, acc 1
2016-09-07T21:08:27.044942: step 5495, loss 0.0348922, acc 0.98
2016-09-07T21:08:27.723115: step 5496, loss 0.0511187, acc 0.96
2016-09-07T21:08:28.398225: step 5497, loss 0.039593, acc 1
2016-09-07T21:08:29.063562: step 5498, loss 0.0307718, acc 0.98
2016-09-07T21:08:29.732075: step 5499, loss 0.0105748, acc 1
2016-09-07T21:08:30.405161: step 5500, loss 0.0122734, acc 1

Evaluation:
2016-09-07T21:08:33.521392: step 5500, loss 1.99728, acc 0.749

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-5500

2016-09-07T21:08:35.250692: step 5501, loss 0.0191814, acc 1
2016-09-07T21:08:35.938304: step 5502, loss 0.0952378, acc 0.94
2016-09-07T21:08:36.614570: step 5503, loss 0.0609126, acc 0.96
2016-09-07T21:08:37.285250: step 5504, loss 0.0125832, acc 1
2016-09-07T21:08:37.954067: step 5505, loss 0.00618186, acc 1
2016-09-07T21:08:38.631234: step 5506, loss 0.00545475, acc 1
2016-09-07T21:08:39.305692: step 5507, loss 0.0137634, acc 1
2016-09-07T21:08:39.989961: step 5508, loss 0.0188175, acc 0.98
2016-09-07T21:08:40.675654: step 5509, loss 0.129936, acc 0.94
2016-09-07T21:08:41.359708: step 5510, loss 0.00569921, acc 1
2016-09-07T21:08:42.054004: step 5511, loss 0.00753105, acc 1
2016-09-07T21:08:42.708737: step 5512, loss 0.0598197, acc 0.98
2016-09-07T21:08:43.386930: step 5513, loss 0.0925291, acc 0.98
2016-09-07T21:08:44.069749: step 5514, loss 0.0390706, acc 0.98
2016-09-07T21:08:44.734874: step 5515, loss 0.0373528, acc 0.96
2016-09-07T21:08:45.412035: step 5516, loss 0.00786582, acc 1
2016-09-07T21:08:46.097222: step 5517, loss 0.0267009, acc 0.98
2016-09-07T21:08:46.761565: step 5518, loss 0.0158273, acc 1
2016-09-07T21:08:47.423881: step 5519, loss 0.00285182, acc 1
2016-09-07T21:08:48.094999: step 5520, loss 0.0585932, acc 0.96
2016-09-07T21:08:48.770096: step 5521, loss 0.0796509, acc 0.98
2016-09-07T21:08:49.440716: step 5522, loss 0.0138284, acc 1
2016-09-07T21:08:50.094293: step 5523, loss 0.0429543, acc 1
2016-09-07T21:08:50.761925: step 5524, loss 0.0203604, acc 1
2016-09-07T21:08:51.422722: step 5525, loss 0.00695295, acc 1
2016-09-07T21:08:52.078118: step 5526, loss 0.0304316, acc 1
2016-09-07T21:08:52.724895: step 5527, loss 0.0971021, acc 0.96
2016-09-07T21:08:53.395790: step 5528, loss 0.0181397, acc 1
2016-09-07T21:08:54.077254: step 5529, loss 0.0043772, acc 1
2016-09-07T21:08:54.748033: step 5530, loss 0.112765, acc 0.94
2016-09-07T21:08:55.401904: step 5531, loss 0.0272629, acc 0.98
2016-09-07T21:08:56.084297: step 5532, loss 0.0469206, acc 0.96
2016-09-07T21:08:56.737044: step 5533, loss 0.0204937, acc 1
2016-09-07T21:08:57.418200: step 5534, loss 0.0257063, acc 1
2016-09-07T21:08:58.074900: step 5535, loss 0.0304907, acc 0.98
2016-09-07T21:08:58.752879: step 5536, loss 0.0266461, acc 1
2016-09-07T21:08:59.419507: step 5537, loss 0.00257743, acc 1
2016-09-07T21:09:00.125079: step 5538, loss 0.00795296, acc 1
2016-09-07T21:09:00.817461: step 5539, loss 0.0134971, acc 1
2016-09-07T21:09:01.484091: step 5540, loss 0.0265536, acc 1
2016-09-07T21:09:02.171450: step 5541, loss 0.0219025, acc 1
2016-09-07T21:09:02.832203: step 5542, loss 0.00581608, acc 1
2016-09-07T21:09:03.516099: step 5543, loss 0.038774, acc 1
2016-09-07T21:09:04.172443: step 5544, loss 0.0117184, acc 1
2016-09-07T21:09:04.821017: step 5545, loss 0.0107457, acc 1
2016-09-07T21:09:05.490587: step 5546, loss 0.0307491, acc 1
2016-09-07T21:09:06.167642: step 5547, loss 0.0228362, acc 0.98
2016-09-07T21:09:06.846361: step 5548, loss 0.0120688, acc 1
2016-09-07T21:09:07.508038: step 5549, loss 0.0583004, acc 0.98
2016-09-07T21:09:08.202009: step 5550, loss 0.0299637, acc 1
2016-09-07T21:09:08.885187: step 5551, loss 0.00873106, acc 1
2016-09-07T21:09:09.556221: step 5552, loss 0.0728042, acc 0.98
2016-09-07T21:09:10.219984: step 5553, loss 0.0196989, acc 0.98
2016-09-07T21:09:10.880610: step 5554, loss 0.0138958, acc 1
2016-09-07T21:09:11.545646: step 5555, loss 0.00571239, acc 1
2016-09-07T21:09:12.216321: step 5556, loss 0.00331368, acc 1
2016-09-07T21:09:12.883973: step 5557, loss 0.0180645, acc 1
2016-09-07T21:09:13.561564: step 5558, loss 0.0207128, acc 1
2016-09-07T21:09:14.228811: step 5559, loss 0.0735215, acc 0.98
2016-09-07T21:09:14.894176: step 5560, loss 0.0163997, acc 1
2016-09-07T21:09:15.571075: step 5561, loss 0.016427, acc 1
2016-09-07T21:09:16.235947: step 5562, loss 0.00739494, acc 1
2016-09-07T21:09:16.892316: step 5563, loss 0.0451086, acc 0.98
2016-09-07T21:09:17.578977: step 5564, loss 0.0348036, acc 1
2016-09-07T21:09:18.255046: step 5565, loss 0.0035577, acc 1
2016-09-07T21:09:18.926879: step 5566, loss 0.0429541, acc 0.98
2016-09-07T21:09:19.581288: step 5567, loss 0.0800339, acc 0.98
2016-09-07T21:09:20.256776: step 5568, loss 0.0240608, acc 0.98
2016-09-07T21:09:20.921485: step 5569, loss 0.0317317, acc 0.98
2016-09-07T21:09:21.582857: step 5570, loss 0.0031155, acc 1
2016-09-07T21:09:22.253368: step 5571, loss 0.0037566, acc 1
2016-09-07T21:09:22.932668: step 5572, loss 0.0561833, acc 0.96
2016-09-07T21:09:23.609387: step 5573, loss 0.00792898, acc 1
2016-09-07T21:09:24.289064: step 5574, loss 0.00483318, acc 1
2016-09-07T21:09:24.929013: step 5575, loss 0.0282325, acc 1
2016-09-07T21:09:25.591124: step 5576, loss 0.0309739, acc 0.98
2016-09-07T21:09:26.280293: step 5577, loss 0.0222864, acc 0.98
2016-09-07T21:09:26.949434: step 5578, loss 0.0222323, acc 0.98
2016-09-07T21:09:27.621231: step 5579, loss 0.0326675, acc 0.98
2016-09-07T21:09:28.300574: step 5580, loss 0.0301343, acc 0.98
2016-09-07T21:09:28.986693: step 5581, loss 0.0253293, acc 0.98
2016-09-07T21:09:29.682926: step 5582, loss 0.0548703, acc 0.98
2016-09-07T21:09:30.353931: step 5583, loss 0.045591, acc 0.98
2016-09-07T21:09:31.028443: step 5584, loss 0.022519, acc 0.98
2016-09-07T21:09:31.694960: step 5585, loss 0.105928, acc 0.94
2016-09-07T21:09:32.369102: step 5586, loss 0.0216451, acc 1
2016-09-07T21:09:33.045162: step 5587, loss 0.0175342, acc 0.98
2016-09-07T21:09:33.710397: step 5588, loss 0.0218353, acc 0.98
2016-09-07T21:09:34.389524: step 5589, loss 0.00652985, acc 1
2016-09-07T21:09:35.061646: step 5590, loss 0.0112658, acc 1
2016-09-07T21:09:35.720656: step 5591, loss 0.0277447, acc 0.98
2016-09-07T21:09:36.381249: step 5592, loss 0.0846163, acc 0.98
2016-09-07T21:09:37.040239: step 5593, loss 0.0323468, acc 0.98
2016-09-07T21:09:37.716317: step 5594, loss 0.0192424, acc 1
2016-09-07T21:09:38.389783: step 5595, loss 0.00328329, acc 1
2016-09-07T21:09:39.063633: step 5596, loss 0.0523786, acc 0.98
2016-09-07T21:09:39.729790: step 5597, loss 0.116261, acc 0.96
2016-09-07T21:09:40.405588: step 5598, loss 0.0166047, acc 1
2016-09-07T21:09:41.079643: step 5599, loss 0.00387718, acc 1
2016-09-07T21:09:41.741049: step 5600, loss 0.0362319, acc 0.96

Evaluation:
2016-09-07T21:09:44.823871: step 5600, loss 1.96371, acc 0.726

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-5600

2016-09-07T21:09:46.480369: step 5601, loss 0.0217568, acc 1
2016-09-07T21:09:47.147006: step 5602, loss 0.069742, acc 0.96
2016-09-07T21:09:47.816046: step 5603, loss 0.024528, acc 1
2016-09-07T21:09:48.470154: step 5604, loss 0.00721377, acc 1
2016-09-07T21:09:49.138912: step 5605, loss 0.0655344, acc 0.96
2016-09-07T21:09:49.799570: step 5606, loss 0.0378011, acc 0.98
2016-09-07T21:09:50.491212: step 5607, loss 0.0165592, acc 1
2016-09-07T21:09:51.163407: step 5608, loss 0.0543602, acc 0.98
2016-09-07T21:09:51.826556: step 5609, loss 0.0154241, acc 1
2016-09-07T21:09:52.496502: step 5610, loss 0.0232538, acc 0.98
2016-09-07T21:09:53.169387: step 5611, loss 0.0251816, acc 1
2016-09-07T21:09:53.853971: step 5612, loss 0.0233119, acc 0.98
2016-09-07T21:09:54.536477: step 5613, loss 0.00869813, acc 1
2016-09-07T21:09:55.218330: step 5614, loss 0.0536132, acc 0.98
2016-09-07T21:09:55.912938: step 5615, loss 0.0155979, acc 1
2016-09-07T21:09:56.582504: step 5616, loss 0.00877645, acc 1
2016-09-07T21:09:57.251134: step 5617, loss 0.00895987, acc 1
2016-09-07T21:09:57.904696: step 5618, loss 0.00484452, acc 1
2016-09-07T21:09:58.563225: step 5619, loss 0.014756, acc 1
2016-09-07T21:09:59.244125: step 5620, loss 0.0138701, acc 1
2016-09-07T21:09:59.916494: step 5621, loss 0.0251592, acc 0.98
2016-09-07T21:10:00.641219: step 5622, loss 0.040191, acc 0.96
2016-09-07T21:10:01.305363: step 5623, loss 0.0283501, acc 0.98
2016-09-07T21:10:01.972140: step 5624, loss 0.0184294, acc 1
2016-09-07T21:10:02.638102: step 5625, loss 0.00247348, acc 1
2016-09-07T21:10:03.004797: step 5626, loss 0.0474394, acc 1
2016-09-07T21:10:03.662753: step 5627, loss 0.0258552, acc 1
2016-09-07T21:10:04.326892: step 5628, loss 0.0158956, acc 1
2016-09-07T21:10:05.010063: step 5629, loss 0.0493076, acc 0.98
2016-09-07T21:10:05.680112: step 5630, loss 0.0492109, acc 0.98
2016-09-07T21:10:06.354470: step 5631, loss 0.0381971, acc 0.98
2016-09-07T21:10:07.001535: step 5632, loss 0.00962946, acc 1
2016-09-07T21:10:07.659745: step 5633, loss 0.02129, acc 0.98
2016-09-07T21:10:08.339090: step 5634, loss 0.0100464, acc 1
2016-09-07T21:10:09.027056: step 5635, loss 0.00539139, acc 1
2016-09-07T21:10:09.689126: step 5636, loss 0.0056996, acc 1
2016-09-07T21:10:10.360258: step 5637, loss 0.0667753, acc 0.98
2016-09-07T21:10:11.029762: step 5638, loss 0.0178904, acc 0.98
2016-09-07T21:10:11.698332: step 5639, loss 0.0340629, acc 1
2016-09-07T21:10:12.356915: step 5640, loss 0.0405675, acc 0.98
2016-09-07T21:10:13.029192: step 5641, loss 0.0062345, acc 1
2016-09-07T21:10:13.685638: step 5642, loss 0.0048245, acc 1
2016-09-07T21:10:14.365751: step 5643, loss 0.0891094, acc 0.96
2016-09-07T21:10:15.026656: step 5644, loss 0.0114093, acc 1
2016-09-07T21:10:15.696762: step 5645, loss 0.0355232, acc 0.98
2016-09-07T21:10:16.380949: step 5646, loss 0.00292804, acc 1
2016-09-07T21:10:17.042562: step 5647, loss 0.0949008, acc 0.94
2016-09-07T21:10:17.706892: step 5648, loss 0.00293294, acc 1
2016-09-07T21:10:18.372608: step 5649, loss 0.0142671, acc 1
2016-09-07T21:10:19.041381: step 5650, loss 0.0156728, acc 1
2016-09-07T21:10:19.702541: step 5651, loss 0.0187879, acc 1
2016-09-07T21:10:20.365997: step 5652, loss 0.00573141, acc 1
2016-09-07T21:10:21.067609: step 5653, loss 0.00550692, acc 1
2016-09-07T21:10:21.744639: step 5654, loss 0.0429839, acc 0.98
2016-09-07T21:10:22.422168: step 5655, loss 0.0426921, acc 0.98
2016-09-07T21:10:23.107660: step 5656, loss 0.0398584, acc 0.98
2016-09-07T21:10:23.778844: step 5657, loss 0.0151824, acc 1
2016-09-07T21:10:24.436125: step 5658, loss 0.048849, acc 1
2016-09-07T21:10:25.080163: step 5659, loss 0.0280762, acc 0.98
2016-09-07T21:10:25.747403: step 5660, loss 0.0602495, acc 0.96
2016-09-07T21:10:26.432690: step 5661, loss 0.027807, acc 0.98
2016-09-07T21:10:27.088908: step 5662, loss 0.0210235, acc 0.98
2016-09-07T21:10:27.765676: step 5663, loss 0.0471865, acc 0.98
2016-09-07T21:10:28.427773: step 5664, loss 0.0549428, acc 0.96
2016-09-07T21:10:29.088623: step 5665, loss 0.0119475, acc 1
2016-09-07T21:10:29.758692: step 5666, loss 0.0127103, acc 1
2016-09-07T21:10:30.456880: step 5667, loss 0.0205286, acc 1
2016-09-07T21:10:31.134355: step 5668, loss 0.00869636, acc 1
2016-09-07T21:10:31.799045: step 5669, loss 0.0364598, acc 0.98
2016-09-07T21:10:32.471778: step 5670, loss 0.0918042, acc 0.96
2016-09-07T21:10:33.120086: step 5671, loss 0.0406489, acc 0.98
2016-09-07T21:10:33.770698: step 5672, loss 0.0676675, acc 0.98
2016-09-07T21:10:34.429943: step 5673, loss 0.102741, acc 0.96
2016-09-07T21:10:35.089678: step 5674, loss 0.0571647, acc 0.98
2016-09-07T21:10:35.770193: step 5675, loss 0.00450418, acc 1
2016-09-07T21:10:36.439642: step 5676, loss 0.0219328, acc 1
2016-09-07T21:10:37.103327: step 5677, loss 0.00655347, acc 1
2016-09-07T21:10:37.769108: step 5678, loss 0.0134002, acc 1
2016-09-07T21:10:38.456160: step 5679, loss 0.0332028, acc 0.98
2016-09-07T21:10:39.136696: step 5680, loss 0.0159585, acc 1
2016-09-07T21:10:39.783398: step 5681, loss 0.0170009, acc 1
2016-09-07T21:10:40.457756: step 5682, loss 0.0354559, acc 1
2016-09-07T21:10:41.126058: step 5683, loss 0.0476689, acc 0.98
2016-09-07T21:10:41.816275: step 5684, loss 0.00541171, acc 1
2016-09-07T21:10:42.481476: step 5685, loss 0.00863927, acc 1
2016-09-07T21:10:43.153024: step 5686, loss 0.00530966, acc 1
2016-09-07T21:10:43.822623: step 5687, loss 0.0231532, acc 0.98
2016-09-07T21:10:44.494573: step 5688, loss 0.0454298, acc 0.98
2016-09-07T21:10:45.154266: step 5689, loss 0.00699973, acc 1
2016-09-07T21:10:45.810945: step 5690, loss 0.0394372, acc 0.98
2016-09-07T21:10:46.472105: step 5691, loss 0.0159693, acc 1
2016-09-07T21:10:47.143075: step 5692, loss 0.0287821, acc 1
2016-09-07T21:10:47.798511: step 5693, loss 0.0585382, acc 0.98
2016-09-07T21:10:48.488883: step 5694, loss 0.0160787, acc 1
2016-09-07T21:10:49.172227: step 5695, loss 0.0350362, acc 0.98
2016-09-07T21:10:49.865718: step 5696, loss 0.00649937, acc 1
2016-09-07T21:10:50.546104: step 5697, loss 0.0160715, acc 1
2016-09-07T21:10:51.220722: step 5698, loss 0.0206444, acc 1
2016-09-07T21:10:51.889343: step 5699, loss 0.00408832, acc 1
2016-09-07T21:10:52.568701: step 5700, loss 0.0307313, acc 0.98

Evaluation:
2016-09-07T21:10:55.678750: step 5700, loss 2.46618, acc 0.737

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-5700

2016-09-07T21:10:57.274145: step 5701, loss 0.0160045, acc 1
2016-09-07T21:10:57.951650: step 5702, loss 0.0627673, acc 0.96
2016-09-07T21:10:58.626174: step 5703, loss 0.0411584, acc 0.98
2016-09-07T21:10:59.309180: step 5704, loss 0.00356106, acc 1
2016-09-07T21:10:59.992214: step 5705, loss 0.0219373, acc 0.98
2016-09-07T21:11:00.714679: step 5706, loss 0.019503, acc 1
2016-09-07T21:11:01.398611: step 5707, loss 0.0307688, acc 0.98
2016-09-07T21:11:02.098070: step 5708, loss 0.197319, acc 0.96
2016-09-07T21:11:02.757595: step 5709, loss 0.0129422, acc 1
2016-09-07T21:11:03.408364: step 5710, loss 0.00372838, acc 1
2016-09-07T21:11:04.068497: step 5711, loss 0.0299099, acc 0.98
2016-09-07T21:11:04.745280: step 5712, loss 0.0316172, acc 0.98
2016-09-07T21:11:05.432148: step 5713, loss 0.00918339, acc 1
2016-09-07T21:11:06.121436: step 5714, loss 0.0448025, acc 0.98
2016-09-07T21:11:06.794376: step 5715, loss 0.0254663, acc 1
2016-09-07T21:11:07.467529: step 5716, loss 0.0366969, acc 0.98
2016-09-07T21:11:08.126996: step 5717, loss 0.00343486, acc 1
2016-09-07T21:11:08.796191: step 5718, loss 0.0847524, acc 0.98
2016-09-07T21:11:09.479138: step 5719, loss 0.0258214, acc 0.98
2016-09-07T21:11:10.160565: step 5720, loss 0.049852, acc 0.98
2016-09-07T21:11:10.842997: step 5721, loss 0.164956, acc 0.94
2016-09-07T21:11:11.515005: step 5722, loss 0.00664004, acc 1
2016-09-07T21:11:12.185873: step 5723, loss 0.00779181, acc 1
2016-09-07T21:11:12.860261: step 5724, loss 0.080466, acc 0.96
2016-09-07T21:11:13.534593: step 5725, loss 0.064664, acc 0.98
2016-09-07T21:11:14.198033: step 5726, loss 0.0182712, acc 1
2016-09-07T21:11:14.876705: step 5727, loss 0.0262002, acc 1
2016-09-07T21:11:15.550066: step 5728, loss 0.0266435, acc 0.98
2016-09-07T21:11:16.257278: step 5729, loss 0.0203627, acc 1
2016-09-07T21:11:16.924965: step 5730, loss 0.0574624, acc 0.98
2016-09-07T21:11:17.588852: step 5731, loss 0.0614603, acc 0.98
2016-09-07T21:11:18.257822: step 5732, loss 0.0111749, acc 1
2016-09-07T21:11:18.930996: step 5733, loss 0.0286891, acc 0.98
2016-09-07T21:11:19.593969: step 5734, loss 0.0306305, acc 0.98
2016-09-07T21:11:20.255002: step 5735, loss 0.0260041, acc 1
2016-09-07T21:11:20.919382: step 5736, loss 0.0231782, acc 1
2016-09-07T21:11:21.598127: step 5737, loss 0.0465234, acc 0.98
2016-09-07T21:11:22.268191: step 5738, loss 0.00537444, acc 1
2016-09-07T21:11:22.951911: step 5739, loss 0.020829, acc 1
2016-09-07T21:11:23.614860: step 5740, loss 0.0201663, acc 1
2016-09-07T21:11:24.284612: step 5741, loss 0.0426434, acc 0.96
2016-09-07T21:11:24.963891: step 5742, loss 0.0464062, acc 0.98
2016-09-07T21:11:25.631491: step 5743, loss 0.0175078, acc 1
2016-09-07T21:11:26.319352: step 5744, loss 0.0227561, acc 0.98
2016-09-07T21:11:26.988988: step 5745, loss 0.00470973, acc 1
2016-09-07T21:11:27.657032: step 5746, loss 0.0127129, acc 1
2016-09-07T21:11:28.337217: step 5747, loss 0.0266723, acc 1
2016-09-07T21:11:29.014026: step 5748, loss 0.0390926, acc 0.96
2016-09-07T21:11:29.662291: step 5749, loss 0.00285758, acc 1
2016-09-07T21:11:30.323040: step 5750, loss 0.0108242, acc 1
2016-09-07T21:11:30.985982: step 5751, loss 0.117328, acc 0.94
2016-09-07T21:11:31.649119: step 5752, loss 0.0512145, acc 1
2016-09-07T21:11:32.310487: step 5753, loss 0.0414357, acc 0.98
2016-09-07T21:11:32.992068: step 5754, loss 0.0113703, acc 1
2016-09-07T21:11:33.662492: step 5755, loss 0.0545047, acc 0.98
2016-09-07T21:11:34.339868: step 5756, loss 0.0519015, acc 0.98
2016-09-07T21:11:35.021766: step 5757, loss 0.0313544, acc 1
2016-09-07T21:11:35.697707: step 5758, loss 0.0341786, acc 0.98
2016-09-07T21:11:36.384266: step 5759, loss 0.019248, acc 1
2016-09-07T21:11:37.052141: step 5760, loss 0.0487079, acc 0.98
2016-09-07T21:11:37.717536: step 5761, loss 0.0203629, acc 1
2016-09-07T21:11:38.385392: step 5762, loss 0.0244509, acc 0.98
2016-09-07T21:11:39.044458: step 5763, loss 0.0299185, acc 1
2016-09-07T21:11:39.708625: step 5764, loss 0.00418326, acc 1
2016-09-07T21:11:40.393345: step 5765, loss 0.0965181, acc 0.98
2016-09-07T21:11:41.063688: step 5766, loss 0.00681433, acc 1
2016-09-07T21:11:41.740675: step 5767, loss 0.00521727, acc 1
2016-09-07T21:11:42.410856: step 5768, loss 0.00573519, acc 1
2016-09-07T21:11:43.077057: step 5769, loss 0.0252214, acc 1
2016-09-07T21:11:43.742164: step 5770, loss 0.0350086, acc 0.98
2016-09-07T21:11:44.398606: step 5771, loss 0.011172, acc 1
2016-09-07T21:11:45.071427: step 5772, loss 0.0520543, acc 0.98
2016-09-07T21:11:45.735837: step 5773, loss 0.0109523, acc 1
2016-09-07T21:11:46.406674: step 5774, loss 0.0408512, acc 0.98
2016-09-07T21:11:47.055595: step 5775, loss 0.0109977, acc 1
2016-09-07T21:11:47.722279: step 5776, loss 0.00511942, acc 1
2016-09-07T21:11:48.381506: step 5777, loss 0.0169883, acc 1
2016-09-07T21:11:49.057836: step 5778, loss 0.0408652, acc 0.96
2016-09-07T21:11:49.726486: step 5779, loss 0.052286, acc 0.96
2016-09-07T21:11:50.387965: step 5780, loss 0.0518753, acc 0.98
2016-09-07T21:11:51.061322: step 5781, loss 0.012976, acc 1
2016-09-07T21:11:51.745255: step 5782, loss 0.00329582, acc 1
2016-09-07T21:11:52.401073: step 5783, loss 0.00524317, acc 1
2016-09-07T21:11:53.075915: step 5784, loss 0.0352143, acc 0.98
2016-09-07T21:11:53.745715: step 5785, loss 0.0871749, acc 0.94
2016-09-07T21:11:54.420210: step 5786, loss 0.0188101, acc 1
2016-09-07T21:11:55.091349: step 5787, loss 0.048753, acc 0.98
2016-09-07T21:11:55.771073: step 5788, loss 0.0147762, acc 1
2016-09-07T21:11:56.436230: step 5789, loss 0.00995168, acc 1
2016-09-07T21:11:57.109693: step 5790, loss 0.00561865, acc 1
2016-09-07T21:11:57.764807: step 5791, loss 0.00415601, acc 1
2016-09-07T21:11:58.431183: step 5792, loss 0.0136792, acc 1
2016-09-07T21:11:59.101836: step 5793, loss 0.0113414, acc 1
2016-09-07T21:11:59.759126: step 5794, loss 0.0291296, acc 0.98
2016-09-07T21:12:00.507592: step 5795, loss 0.0888759, acc 0.94
2016-09-07T21:12:01.184065: step 5796, loss 0.0607006, acc 0.98
2016-09-07T21:12:01.849220: step 5797, loss 0.0320024, acc 0.98
2016-09-07T21:12:02.520965: step 5798, loss 0.0630674, acc 0.98
2016-09-07T21:12:03.194292: step 5799, loss 0.0513682, acc 0.96
2016-09-07T21:12:03.872407: step 5800, loss 0.125604, acc 0.98

Evaluation:
2016-09-07T21:12:06.963576: step 5800, loss 1.97333, acc 0.749

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473249784/checkpoints/model-5800

2016-09-07T21:12:08.631843: step 5801, loss 0.0255346, acc 0.98
2016-09-07T21:12:09.317750: step 5802, loss 0.0181767, acc 1
2016-09-07T21:12:09.972508: step 5803, loss 0.0069265, acc 1
2016-09-07T21:12:10.641751: step 5804, loss 0.015952, acc 1
2016-09-07T21:12:11.330630: step 5805, loss 0.016714, acc 1
2016-09-07T21:12:12.009267: step 5806, loss 0.0196035, acc 0.98
2016-09-07T21:12:12.690981: step 5807, loss 0.0597963, acc 0.96
2016-09-07T21:12:13.353432: step 5808, loss 0.004122, acc 1
2016-09-07T21:12:14.022586: step 5809, loss 0.0285771, acc 0.98
2016-09-07T21:12:14.698031: step 5810, loss 0.0337227, acc 0.98
2016-09-07T21:12:15.371665: step 5811, loss 0.0164021, acc 1
2016-09-07T21:12:16.041410: step 5812, loss 0.0396933, acc 0.96
2016-09-07T21:12:16.693519: step 5813, loss 0.0450313, acc 0.96
2016-09-07T21:12:17.361945: step 5814, loss 0.00580408, acc 1
2016-09-07T21:12:18.047404: step 5815, loss 0.0386293, acc 0.96
2016-09-07T21:12:18.703820: step 5816, loss 0.0284182, acc 0.98
2016-09-07T21:12:19.370140: step 5817, loss 0.0229962, acc 1
2016-09-07T21:12:20.034384: step 5818, loss 0.0156516, acc 1
2016-09-07T21:12:20.697462: step 5819, loss 0.00489356, acc 1
2016-09-07T21:12:21.067806: step 5820, loss 0.0141129, acc 1

WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f2261551e90>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.
WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f2261551e50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.

Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=50
CHECKPOINT_EVERY=100
CV_INDEX=0
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=300
EVALUATE_EVERY=100
FILTER_SIZES=3,4,5
HIDDEN_DIM=300
L2_REG_LAMBDA=0
LOG_DEVICE_PLACEMENT=False
NUM_EPOCHS=30
NUM_FILTERS=100
WORD2VEC=GoogleNews-vectors-negative300.bin

Loading data...
Vocabulary Size: 18758
Train/Dev split: 9594/1066
Writing to /home/cil/lstm-context-embeddings/runs/1473156846

Load word2vec file GoogleNews-vectors-negative300.bin

2016-09-06T18:14:25.751841: step 1, loss 0.693147, acc 0.3
2016-09-06T18:14:26.432769: step 2, loss 0.679301, acc 0.6
2016-09-06T18:14:27.125982: step 3, loss 0.735834, acc 0.52
2016-09-06T18:14:27.794697: step 4, loss 0.749811, acc 0.44
2016-09-06T18:14:28.472991: step 5, loss 0.710243, acc 0.48
2016-09-06T18:14:29.134676: step 6, loss 0.684222, acc 0.58
2016-09-06T18:14:29.810699: step 7, loss 0.687959, acc 0.56
2016-09-06T18:14:30.482746: step 8, loss 0.689523, acc 0.56
2016-09-06T18:14:31.162480: step 9, loss 0.696058, acc 0.5
2016-09-06T18:14:31.834469: step 10, loss 0.695151, acc 0.54
2016-09-06T18:14:32.505066: step 11, loss 0.698853, acc 0.36
2016-09-06T18:14:33.182522: step 12, loss 0.696366, acc 0.48
2016-09-06T18:14:33.843852: step 13, loss 0.689536, acc 0.56
2016-09-06T18:14:34.518723: step 14, loss 0.696672, acc 0.5
2016-09-06T18:14:35.184501: step 15, loss 0.696835, acc 0.48
2016-09-06T18:14:35.868534: step 16, loss 0.698318, acc 0.48
2016-09-06T18:14:36.535883: step 17, loss 0.700037, acc 0.42
2016-09-06T18:14:37.206923: step 18, loss 0.695197, acc 0.5
2016-09-06T18:14:37.888236: step 19, loss 0.678191, acc 0.56
2016-09-06T18:14:38.572019: step 20, loss 0.683006, acc 0.52
2016-09-06T18:14:39.239355: step 21, loss 0.702203, acc 0.48
2016-09-06T18:14:39.906158: step 22, loss 0.671686, acc 0.66
2016-09-06T18:14:40.577951: step 23, loss 0.686511, acc 0.52
2016-09-06T18:14:41.249209: step 24, loss 0.672182, acc 0.64
2016-09-06T18:14:41.900745: step 25, loss 0.65406, acc 0.64
2016-09-06T18:14:42.558265: step 26, loss 0.697968, acc 0.46
2016-09-06T18:14:43.238199: step 27, loss 0.686731, acc 0.6
2016-09-06T18:14:43.922530: step 28, loss 0.690431, acc 0.6
2016-09-06T18:14:44.591087: step 29, loss 0.649884, acc 0.56
2016-09-06T18:14:45.254746: step 30, loss 0.665444, acc 0.58
2016-09-06T18:14:45.909777: step 31, loss 0.666975, acc 0.62
2016-09-06T18:14:46.574432: step 32, loss 0.686039, acc 0.62
2016-09-06T18:14:47.256889: step 33, loss 0.617011, acc 0.66
2016-09-06T18:14:47.946135: step 34, loss 0.661, acc 0.66
2016-09-06T18:14:48.622061: step 35, loss 0.621556, acc 0.6
2016-09-06T18:14:49.293994: step 36, loss 0.64807, acc 0.56
2016-09-06T18:14:49.956317: step 37, loss 0.689918, acc 0.5
2016-09-06T18:14:50.604755: step 38, loss 0.603284, acc 0.7
2016-09-06T18:14:51.286290: step 39, loss 0.690796, acc 0.58
2016-09-06T18:14:51.964265: step 40, loss 0.620519, acc 0.62
2016-09-06T18:14:52.646832: step 41, loss 0.610886, acc 0.68
2016-09-06T18:14:53.340800: step 42, loss 0.606554, acc 0.7
2016-09-06T18:14:54.021321: step 43, loss 0.638156, acc 0.68
2016-09-06T18:14:54.681840: step 44, loss 0.629249, acc 0.62
2016-09-06T18:14:55.362727: step 45, loss 0.582467, acc 0.64
2016-09-06T18:14:56.034465: step 46, loss 0.768352, acc 0.66
2016-09-06T18:14:56.706074: step 47, loss 0.605834, acc 0.62
2016-09-06T18:14:57.390138: step 48, loss 0.636739, acc 0.6
2016-09-06T18:14:58.075282: step 49, loss 0.627127, acc 0.66
2016-09-06T18:14:58.769161: step 50, loss 0.708985, acc 0.54
2016-09-06T18:14:59.459496: step 51, loss 0.733144, acc 0.62
2016-09-06T18:15:00.159085: step 52, loss 0.682541, acc 0.58
2016-09-06T18:15:00.873378: step 53, loss 0.546373, acc 0.78
2016-09-06T18:15:01.537642: step 54, loss 0.617103, acc 0.76
2016-09-06T18:15:02.204875: step 55, loss 0.642059, acc 0.6
2016-09-06T18:15:02.859117: step 56, loss 0.566885, acc 0.72
2016-09-06T18:15:03.540430: step 57, loss 0.547586, acc 0.74
2016-09-06T18:15:04.211169: step 58, loss 0.635933, acc 0.6
2016-09-06T18:15:04.871905: step 59, loss 0.592233, acc 0.66
2016-09-06T18:15:05.534982: step 60, loss 0.58224, acc 0.74
2016-09-06T18:15:06.240609: step 61, loss 0.629677, acc 0.68
2016-09-06T18:15:06.911251: step 62, loss 0.719496, acc 0.66
2016-09-06T18:15:07.574813: step 63, loss 0.683571, acc 0.7
2016-09-06T18:15:08.281000: step 64, loss 0.748226, acc 0.46
2016-09-06T18:15:08.993356: step 65, loss 0.619023, acc 0.68
2016-09-06T18:15:09.744764: step 66, loss 0.636328, acc 0.64
2016-09-06T18:15:10.438582: step 67, loss 0.571611, acc 0.68
2016-09-06T18:15:11.092851: step 68, loss 0.569747, acc 0.78
2016-09-06T18:15:11.774591: step 69, loss 0.563133, acc 0.82
2016-09-06T18:15:12.465774: step 70, loss 0.630399, acc 0.62
2016-09-06T18:15:13.119938: step 71, loss 0.561611, acc 0.76
2016-09-06T18:15:13.796781: step 72, loss 0.574332, acc 0.74
2016-09-06T18:15:14.490522: step 73, loss 0.595738, acc 0.68
2016-09-06T18:15:15.169110: step 74, loss 0.535737, acc 0.78
2016-09-06T18:15:15.847416: step 75, loss 0.522128, acc 0.82
2016-09-06T18:15:16.566729: step 76, loss 0.516082, acc 0.78
2016-09-06T18:15:17.259910: step 77, loss 0.51936, acc 0.72
2016-09-06T18:15:17.939058: step 78, loss 0.568629, acc 0.74
2016-09-06T18:15:18.630362: step 79, loss 0.404755, acc 0.88
2016-09-06T18:15:19.295902: step 80, loss 0.398905, acc 0.84
2016-09-06T18:15:19.981716: step 81, loss 0.517287, acc 0.8
2016-09-06T18:15:20.652157: step 82, loss 0.565302, acc 0.66
2016-09-06T18:15:21.326828: step 83, loss 0.571802, acc 0.7
2016-09-06T18:15:22.005133: step 84, loss 0.753334, acc 0.58
2016-09-06T18:15:22.677445: step 85, loss 0.502145, acc 0.76
2016-09-06T18:15:23.340477: step 86, loss 0.587922, acc 0.64
2016-09-06T18:15:24.008114: step 87, loss 0.525002, acc 0.74
2016-09-06T18:15:24.693712: step 88, loss 0.486769, acc 0.78
2016-09-06T18:15:25.353286: step 89, loss 0.463521, acc 0.78
2016-09-06T18:15:26.032219: step 90, loss 0.628788, acc 0.68
2016-09-06T18:15:26.712228: step 91, loss 0.508426, acc 0.7
2016-09-06T18:15:27.376728: step 92, loss 0.507533, acc 0.7
2016-09-06T18:15:28.051612: step 93, loss 0.64712, acc 0.62
2016-09-06T18:15:28.734146: step 94, loss 0.457232, acc 0.82
2016-09-06T18:15:29.400360: step 95, loss 0.590758, acc 0.66
2016-09-06T18:15:30.051538: step 96, loss 0.48956, acc 0.74
2016-09-06T18:15:30.728858: step 97, loss 0.529452, acc 0.78
2016-09-06T18:15:31.408616: step 98, loss 0.611387, acc 0.64
2016-09-06T18:15:32.068222: step 99, loss 0.503899, acc 0.76
2016-09-06T18:15:32.743255: step 100, loss 0.46999, acc 0.76

Evaluation:
2016-09-06T18:15:35.820456: step 100, loss 0.502683, acc 0.742026

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-100

2016-09-06T18:15:37.506573: step 101, loss 0.511939, acc 0.7
2016-09-06T18:15:38.187887: step 102, loss 0.505671, acc 0.68
2016-09-06T18:15:38.843677: step 103, loss 0.416791, acc 0.8
2016-09-06T18:15:39.524786: step 104, loss 0.559309, acc 0.72
2016-09-06T18:15:40.200207: step 105, loss 0.509666, acc 0.76
2016-09-06T18:15:40.878418: step 106, loss 0.499434, acc 0.76
2016-09-06T18:15:41.549394: step 107, loss 0.491162, acc 0.72
2016-09-06T18:15:42.191418: step 108, loss 0.488842, acc 0.76
2016-09-06T18:15:42.856360: step 109, loss 0.742597, acc 0.64
2016-09-06T18:15:43.539509: step 110, loss 0.55006, acc 0.7
2016-09-06T18:15:44.210425: step 111, loss 0.448861, acc 0.78
2016-09-06T18:15:44.881109: step 112, loss 0.455301, acc 0.88
2016-09-06T18:15:45.542422: step 113, loss 0.445054, acc 0.78
2016-09-06T18:15:46.193820: step 114, loss 0.54915, acc 0.7
2016-09-06T18:15:46.882412: step 115, loss 0.454909, acc 0.8
2016-09-06T18:15:47.554770: step 116, loss 0.600368, acc 0.66
2016-09-06T18:15:48.232267: step 117, loss 0.589979, acc 0.8
2016-09-06T18:15:48.921043: step 118, loss 0.488261, acc 0.72
2016-09-06T18:15:49.590853: step 119, loss 0.494572, acc 0.78
2016-09-06T18:15:50.282454: step 120, loss 0.466481, acc 0.8
2016-09-06T18:15:50.955865: step 121, loss 0.533447, acc 0.72
2016-09-06T18:15:51.624906: step 122, loss 0.536282, acc 0.74
2016-09-06T18:15:52.317506: step 123, loss 0.465093, acc 0.8
2016-09-06T18:15:52.984673: step 124, loss 0.376478, acc 0.86
2016-09-06T18:15:53.660172: step 125, loss 0.525213, acc 0.68
2016-09-06T18:15:54.344162: step 126, loss 0.438605, acc 0.76
2016-09-06T18:15:55.016338: step 127, loss 0.503389, acc 0.8
2016-09-06T18:15:55.705213: step 128, loss 0.658571, acc 0.66
2016-09-06T18:15:56.393729: step 129, loss 0.508736, acc 0.72
2016-09-06T18:15:57.055275: step 130, loss 0.695785, acc 0.64
2016-09-06T18:15:57.742841: step 131, loss 0.466585, acc 0.72
2016-09-06T18:15:58.429107: step 132, loss 0.397902, acc 0.84
2016-09-06T18:15:59.110181: step 133, loss 0.456443, acc 0.8
2016-09-06T18:15:59.785716: step 134, loss 0.613719, acc 0.7
2016-09-06T18:16:00.472360: step 135, loss 0.390264, acc 0.82
2016-09-06T18:16:01.137088: step 136, loss 0.620724, acc 0.6
2016-09-06T18:16:01.808603: step 137, loss 0.587419, acc 0.7
2016-09-06T18:16:02.480694: step 138, loss 0.593919, acc 0.72
2016-09-06T18:16:03.151156: step 139, loss 0.516431, acc 0.82
2016-09-06T18:16:03.829090: step 140, loss 0.555427, acc 0.66
2016-09-06T18:16:04.514722: step 141, loss 0.530284, acc 0.78
2016-09-06T18:16:05.203654: step 142, loss 0.483818, acc 0.76
2016-09-06T18:16:05.864469: step 143, loss 0.451417, acc 0.82
2016-09-06T18:16:06.534817: step 144, loss 0.449481, acc 0.82
2016-09-06T18:16:07.215492: step 145, loss 0.538956, acc 0.68
2016-09-06T18:16:07.878472: step 146, loss 0.551963, acc 0.72
2016-09-06T18:16:08.558664: step 147, loss 0.54472, acc 0.7
2016-09-06T18:16:09.223404: step 148, loss 0.518143, acc 0.7
2016-09-06T18:16:09.892787: step 149, loss 0.518022, acc 0.8
2016-09-06T18:16:10.569469: step 150, loss 0.413052, acc 0.84
2016-09-06T18:16:11.232780: step 151, loss 0.481272, acc 0.76
2016-09-06T18:16:11.911789: step 152, loss 0.392373, acc 0.84
2016-09-06T18:16:12.601657: step 153, loss 0.631403, acc 0.74
2016-09-06T18:16:13.266920: step 154, loss 0.560672, acc 0.68
2016-09-06T18:16:13.954664: step 155, loss 0.57349, acc 0.7
2016-09-06T18:16:14.623212: step 156, loss 0.372685, acc 0.88
2016-09-06T18:16:15.310604: step 157, loss 0.613343, acc 0.66
2016-09-06T18:16:15.986009: step 158, loss 0.486916, acc 0.78
2016-09-06T18:16:16.655328: step 159, loss 0.575231, acc 0.72
2016-09-06T18:16:17.309246: step 160, loss 0.531874, acc 0.76
2016-09-06T18:16:17.992579: step 161, loss 0.470968, acc 0.86
2016-09-06T18:16:18.672767: step 162, loss 0.481847, acc 0.76
2016-09-06T18:16:19.336543: step 163, loss 0.430289, acc 0.78
2016-09-06T18:16:20.007112: step 164, loss 0.467755, acc 0.8
2016-09-06T18:16:20.686876: step 165, loss 0.544111, acc 0.64
2016-09-06T18:16:21.360228: step 166, loss 0.441219, acc 0.78
2016-09-06T18:16:22.018565: step 167, loss 0.477725, acc 0.82
2016-09-06T18:16:22.704221: step 168, loss 0.471586, acc 0.74
2016-09-06T18:16:23.377720: step 169, loss 0.42506, acc 0.78
2016-09-06T18:16:24.060158: step 170, loss 0.515692, acc 0.66
2016-09-06T18:16:24.733834: step 171, loss 0.394852, acc 0.88
2016-09-06T18:16:25.403585: step 172, loss 0.494626, acc 0.74
2016-09-06T18:16:26.095253: step 173, loss 0.405205, acc 0.82
2016-09-06T18:16:26.785197: step 174, loss 0.421033, acc 0.82
2016-09-06T18:16:27.470894: step 175, loss 0.522169, acc 0.78
2016-09-06T18:16:28.132724: step 176, loss 0.546748, acc 0.68
2016-09-06T18:16:28.793840: step 177, loss 0.615275, acc 0.68
2016-09-06T18:16:29.452982: step 178, loss 0.639713, acc 0.66
2016-09-06T18:16:30.140251: step 179, loss 0.503741, acc 0.74
2016-09-06T18:16:30.808159: step 180, loss 0.35749, acc 0.82
2016-09-06T18:16:31.472301: step 181, loss 0.468925, acc 0.7
2016-09-06T18:16:32.148208: step 182, loss 0.417858, acc 0.8
2016-09-06T18:16:32.819160: step 183, loss 0.415437, acc 0.78
2016-09-06T18:16:33.492748: step 184, loss 0.551552, acc 0.76
2016-09-06T18:16:34.177017: step 185, loss 0.468841, acc 0.76
2016-09-06T18:16:34.883323: step 186, loss 0.506564, acc 0.78
2016-09-06T18:16:35.575136: step 187, loss 0.362109, acc 0.86
2016-09-06T18:16:36.248024: step 188, loss 0.38005, acc 0.84
2016-09-06T18:16:36.906211: step 189, loss 0.37819, acc 0.88
2016-09-06T18:16:37.557856: step 190, loss 0.613454, acc 0.66
2016-09-06T18:16:38.238824: step 191, loss 0.548788, acc 0.78
2016-09-06T18:16:38.863187: step 192, loss 0.394394, acc 0.818182
2016-09-06T18:16:39.553722: step 193, loss 0.436592, acc 0.78
2016-09-06T18:16:40.245200: step 194, loss 0.394519, acc 0.86
2016-09-06T18:16:40.913431: step 195, loss 0.377764, acc 0.8
2016-09-06T18:16:41.588630: step 196, loss 0.377879, acc 0.86
2016-09-06T18:16:42.264013: step 197, loss 0.445177, acc 0.84
2016-09-06T18:16:42.947179: step 198, loss 0.374244, acc 0.9
2016-09-06T18:16:43.636535: step 199, loss 0.275679, acc 0.9
2016-09-06T18:16:44.308791: step 200, loss 0.393327, acc 0.84

Evaluation:
2016-09-06T18:16:47.380024: step 200, loss 0.430014, acc 0.787054

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-200

2016-09-06T18:16:49.098801: step 201, loss 0.396367, acc 0.78
2016-09-06T18:16:49.751842: step 202, loss 0.426549, acc 0.76
2016-09-06T18:16:50.415291: step 203, loss 0.467871, acc 0.78
2016-09-06T18:16:51.090003: step 204, loss 0.404407, acc 0.82
2016-09-06T18:16:51.763861: step 205, loss 0.325756, acc 0.88
2016-09-06T18:16:52.469560: step 206, loss 0.391715, acc 0.8
2016-09-06T18:16:53.143446: step 207, loss 0.29359, acc 0.84
2016-09-06T18:16:53.813612: step 208, loss 0.304048, acc 0.84
2016-09-06T18:16:54.472613: step 209, loss 0.286825, acc 0.9
2016-09-06T18:16:55.143699: step 210, loss 0.315801, acc 0.84
2016-09-06T18:16:55.808953: step 211, loss 0.293073, acc 0.86
2016-09-06T18:16:56.454578: step 212, loss 0.286769, acc 0.84
2016-09-06T18:16:57.120278: step 213, loss 0.398343, acc 0.82
2016-09-06T18:16:57.782392: step 214, loss 0.495883, acc 0.78
2016-09-06T18:16:58.467033: step 215, loss 0.242607, acc 0.92
2016-09-06T18:16:59.151429: step 216, loss 0.430718, acc 0.82
2016-09-06T18:16:59.827092: step 217, loss 0.285901, acc 0.88
2016-09-06T18:17:00.506516: step 218, loss 0.299987, acc 0.88
2016-09-06T18:17:01.160370: step 219, loss 0.293436, acc 0.88
2016-09-06T18:17:01.835837: step 220, loss 0.239513, acc 0.96
2016-09-06T18:17:02.494189: step 221, loss 0.388825, acc 0.74
2016-09-06T18:17:03.156684: step 222, loss 0.358251, acc 0.84
2016-09-06T18:17:03.836337: step 223, loss 0.469183, acc 0.76
2016-09-06T18:17:04.516644: step 224, loss 0.447229, acc 0.78
2016-09-06T18:17:05.175431: step 225, loss 0.280087, acc 0.88
2016-09-06T18:17:05.858382: step 226, loss 0.309927, acc 0.86
2016-09-06T18:17:06.548470: step 227, loss 0.532404, acc 0.78
2016-09-06T18:17:07.235452: step 228, loss 0.309401, acc 0.86
2016-09-06T18:17:07.908450: step 229, loss 0.380643, acc 0.82
2016-09-06T18:17:08.573944: step 230, loss 0.307147, acc 0.9
2016-09-06T18:17:09.251670: step 231, loss 0.366581, acc 0.86
2016-09-06T18:17:09.918486: step 232, loss 0.334296, acc 0.88
2016-09-06T18:17:10.589944: step 233, loss 0.340391, acc 0.86
2016-09-06T18:17:11.260123: step 234, loss 0.507041, acc 0.78
2016-09-06T18:17:11.940968: step 235, loss 0.379426, acc 0.88
2016-09-06T18:17:12.624605: step 236, loss 0.410709, acc 0.84
2016-09-06T18:17:13.309339: step 237, loss 0.439695, acc 0.76
2016-09-06T18:17:13.993799: step 238, loss 0.332305, acc 0.86
2016-09-06T18:17:14.665649: step 239, loss 0.428337, acc 0.76
2016-09-06T18:17:15.328107: step 240, loss 0.406976, acc 0.84
2016-09-06T18:17:15.997343: step 241, loss 0.369626, acc 0.84
2016-09-06T18:17:16.660795: step 242, loss 0.414279, acc 0.82
2016-09-06T18:17:17.316215: step 243, loss 0.318469, acc 0.86
2016-09-06T18:17:17.997867: step 244, loss 0.376282, acc 0.84
2016-09-06T18:17:18.661641: step 245, loss 0.55417, acc 0.74
2016-09-06T18:17:19.329731: step 246, loss 0.395452, acc 0.76
2016-09-06T18:17:20.017939: step 247, loss 0.409627, acc 0.82
2016-09-06T18:17:20.700341: step 248, loss 0.524026, acc 0.8
2016-09-06T18:17:21.370193: step 249, loss 0.468791, acc 0.82
2016-09-06T18:17:22.052502: step 250, loss 0.354613, acc 0.78
2016-09-06T18:17:22.720467: step 251, loss 0.337061, acc 0.84
2016-09-06T18:17:23.371184: step 252, loss 0.318734, acc 0.82
2016-09-06T18:17:24.027914: step 253, loss 0.425283, acc 0.82
2016-09-06T18:17:24.725120: step 254, loss 0.283803, acc 0.92
2016-09-06T18:17:25.398634: step 255, loss 0.392324, acc 0.84
2016-09-06T18:17:26.065371: step 256, loss 0.389919, acc 0.78
2016-09-06T18:17:26.727358: step 257, loss 0.358201, acc 0.84
2016-09-06T18:17:27.398784: step 258, loss 0.491259, acc 0.82
2016-09-06T18:17:28.089321: step 259, loss 0.314402, acc 0.84
2016-09-06T18:17:28.775510: step 260, loss 0.45413, acc 0.8
2016-09-06T18:17:29.450989: step 261, loss 0.292469, acc 0.88
2016-09-06T18:17:30.125640: step 262, loss 0.334559, acc 0.84
2016-09-06T18:17:30.793884: step 263, loss 0.463861, acc 0.84
2016-09-06T18:17:31.465185: step 264, loss 0.322254, acc 0.86
2016-09-06T18:17:32.146729: step 265, loss 0.320681, acc 0.86
2016-09-06T18:17:32.834142: step 266, loss 0.238653, acc 0.92
2016-09-06T18:17:33.516164: step 267, loss 0.375042, acc 0.82
2016-09-06T18:17:34.223835: step 268, loss 0.401046, acc 0.78
2016-09-06T18:17:34.923122: step 269, loss 0.300163, acc 0.86
2016-09-06T18:17:35.627323: step 270, loss 0.536478, acc 0.74
2016-09-06T18:17:36.301284: step 271, loss 0.20946, acc 0.9
2016-09-06T18:17:36.980506: step 272, loss 0.346945, acc 0.86
2016-09-06T18:17:37.651411: step 273, loss 0.35578, acc 0.8
2016-09-06T18:17:38.320983: step 274, loss 0.317038, acc 0.86
2016-09-06T18:17:38.989951: step 275, loss 0.320505, acc 0.88
2016-09-06T18:17:39.663173: step 276, loss 0.355593, acc 0.8
2016-09-06T18:17:40.339460: step 277, loss 0.265857, acc 0.9
2016-09-06T18:17:41.016117: step 278, loss 0.769741, acc 0.64
2016-09-06T18:17:41.678651: step 279, loss 0.44175, acc 0.88
2016-09-06T18:17:42.337128: step 280, loss 0.335407, acc 0.86
2016-09-06T18:17:42.998259: step 281, loss 0.365046, acc 0.86
2016-09-06T18:17:43.684737: step 282, loss 0.330988, acc 0.86
2016-09-06T18:17:44.362127: step 283, loss 0.468962, acc 0.78
2016-09-06T18:17:45.063311: step 284, loss 0.395092, acc 0.8
2016-09-06T18:17:45.756401: step 285, loss 0.371272, acc 0.9
2016-09-06T18:17:46.438012: step 286, loss 0.328895, acc 0.88
2016-09-06T18:17:47.110879: step 287, loss 0.431399, acc 0.74
2016-09-06T18:17:47.775944: step 288, loss 0.284569, acc 0.92
2016-09-06T18:17:48.439737: step 289, loss 0.391933, acc 0.84
2016-09-06T18:17:49.123844: step 290, loss 0.337367, acc 0.84
2016-09-06T18:17:49.803941: step 291, loss 0.328574, acc 0.84
2016-09-06T18:17:50.472982: step 292, loss 0.402265, acc 0.82
2016-09-06T18:17:51.137991: step 293, loss 0.51561, acc 0.74
2016-09-06T18:17:51.822870: step 294, loss 0.276234, acc 0.88
2016-09-06T18:17:52.507784: step 295, loss 0.300573, acc 0.9
2016-09-06T18:17:53.164291: step 296, loss 0.475957, acc 0.76
2016-09-06T18:17:53.841846: step 297, loss 0.274774, acc 0.9
2016-09-06T18:17:54.514961: step 298, loss 0.447217, acc 0.8
2016-09-06T18:17:55.195273: step 299, loss 0.427463, acc 0.86
2016-09-06T18:17:55.867975: step 300, loss 0.438853, acc 0.78

Evaluation:
2016-09-06T18:17:58.935230: step 300, loss 0.409465, acc 0.807692

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-300

2016-09-06T18:18:00.555929: step 301, loss 0.531895, acc 0.82
2016-09-06T18:18:01.229220: step 302, loss 0.273933, acc 0.92
2016-09-06T18:18:01.895175: step 303, loss 0.389508, acc 0.86
2016-09-06T18:18:02.587765: step 304, loss 0.27422, acc 0.9
2016-09-06T18:18:03.271980: step 305, loss 0.473376, acc 0.78
2016-09-06T18:18:03.933538: step 306, loss 0.201113, acc 0.96
2016-09-06T18:18:04.597253: step 307, loss 0.414961, acc 0.82
2016-09-06T18:18:05.259086: step 308, loss 0.395356, acc 0.82
2016-09-06T18:18:05.908842: step 309, loss 0.483391, acc 0.84
2016-09-06T18:18:06.562923: step 310, loss 0.465702, acc 0.8
2016-09-06T18:18:07.217781: step 311, loss 0.304857, acc 0.86
2016-09-06T18:18:07.886038: step 312, loss 0.418863, acc 0.82
2016-09-06T18:18:08.571788: step 313, loss 0.32869, acc 0.88
2016-09-06T18:18:09.235820: step 314, loss 0.499491, acc 0.72
2016-09-06T18:18:09.892058: step 315, loss 0.383537, acc 0.82
2016-09-06T18:18:10.549439: step 316, loss 0.382817, acc 0.82
2016-09-06T18:18:11.209785: step 317, loss 0.337866, acc 0.82
2016-09-06T18:18:11.908337: step 318, loss 0.467737, acc 0.78
2016-09-06T18:18:12.573784: step 319, loss 0.332362, acc 0.8
2016-09-06T18:18:13.258790: step 320, loss 0.502796, acc 0.78
2016-09-06T18:18:13.921410: step 321, loss 0.26207, acc 0.88
2016-09-06T18:18:14.605636: step 322, loss 0.389954, acc 0.8
2016-09-06T18:18:15.277657: step 323, loss 0.348053, acc 0.82
2016-09-06T18:18:15.954662: step 324, loss 0.295882, acc 0.88
2016-09-06T18:18:16.629340: step 325, loss 0.236032, acc 0.92
2016-09-06T18:18:17.332387: step 326, loss 0.471275, acc 0.8
2016-09-06T18:18:17.997769: step 327, loss 0.182821, acc 0.94
2016-09-06T18:18:18.672542: step 328, loss 0.323863, acc 0.94
2016-09-06T18:18:19.328064: step 329, loss 0.383278, acc 0.8
2016-09-06T18:18:20.003413: step 330, loss 0.419214, acc 0.76
2016-09-06T18:18:20.675834: step 331, loss 0.394478, acc 0.86
2016-09-06T18:18:21.359321: step 332, loss 0.272996, acc 0.88
2016-09-06T18:18:22.035070: step 333, loss 0.393347, acc 0.84
2016-09-06T18:18:22.711528: step 334, loss 0.33432, acc 0.86
2016-09-06T18:18:23.411980: step 335, loss 0.344918, acc 0.86
2016-09-06T18:18:24.093569: step 336, loss 0.365764, acc 0.86
2016-09-06T18:18:24.776599: step 337, loss 0.291418, acc 0.88
2016-09-06T18:18:25.434291: step 338, loss 0.389526, acc 0.86
2016-09-06T18:18:26.095404: step 339, loss 0.294771, acc 0.86
2016-09-06T18:18:26.763932: step 340, loss 0.460355, acc 0.76
2016-09-06T18:18:27.425345: step 341, loss 0.314204, acc 0.82
2016-09-06T18:18:28.065272: step 342, loss 0.355438, acc 0.86
2016-09-06T18:18:28.726153: step 343, loss 0.242815, acc 0.92
2016-09-06T18:18:29.409421: step 344, loss 0.394541, acc 0.8
2016-09-06T18:18:30.080976: step 345, loss 0.349156, acc 0.84
2016-09-06T18:18:30.752509: step 346, loss 0.339919, acc 0.82
2016-09-06T18:18:31.420339: step 347, loss 0.291177, acc 0.86
2016-09-06T18:18:32.098564: step 348, loss 0.482782, acc 0.78
2016-09-06T18:18:32.772621: step 349, loss 0.449036, acc 0.8
2016-09-06T18:18:33.445079: step 350, loss 0.462654, acc 0.8
2016-09-06T18:18:34.116887: step 351, loss 0.391549, acc 0.76
2016-09-06T18:18:34.783014: step 352, loss 0.476277, acc 0.78
2016-09-06T18:18:35.454323: step 353, loss 0.425535, acc 0.8
2016-09-06T18:18:36.128266: step 354, loss 0.386522, acc 0.82
2016-09-06T18:18:36.809554: step 355, loss 0.32808, acc 0.9
2016-09-06T18:18:37.477558: step 356, loss 0.341674, acc 0.88
2016-09-06T18:18:38.138634: step 357, loss 0.370028, acc 0.88
2016-09-06T18:18:38.808570: step 358, loss 0.483365, acc 0.78
2016-09-06T18:18:39.495512: step 359, loss 0.373376, acc 0.82
2016-09-06T18:18:40.165991: step 360, loss 0.482775, acc 0.74
2016-09-06T18:18:40.837320: step 361, loss 0.375388, acc 0.84
2016-09-06T18:18:41.514495: step 362, loss 0.419715, acc 0.8
2016-09-06T18:18:42.195393: step 363, loss 0.302701, acc 0.86
2016-09-06T18:18:42.873582: step 364, loss 0.351897, acc 0.82
2016-09-06T18:18:43.549188: step 365, loss 0.394801, acc 0.84
2016-09-06T18:18:44.222353: step 366, loss 0.538326, acc 0.82
2016-09-06T18:18:44.892878: step 367, loss 0.317255, acc 0.88
2016-09-06T18:18:45.575841: step 368, loss 0.432526, acc 0.8
2016-09-06T18:18:46.250438: step 369, loss 0.337263, acc 0.82
2016-09-06T18:18:46.932918: step 370, loss 0.258917, acc 0.92
2016-09-06T18:18:47.597826: step 371, loss 0.598642, acc 0.72
2016-09-06T18:18:48.266192: step 372, loss 0.420291, acc 0.78
2016-09-06T18:18:48.923824: step 373, loss 0.213539, acc 0.94
2016-09-06T18:18:49.604602: step 374, loss 0.304626, acc 0.84
2016-09-06T18:18:50.276395: step 375, loss 0.398597, acc 0.8
2016-09-06T18:18:50.975518: step 376, loss 0.426896, acc 0.8
2016-09-06T18:18:51.647380: step 377, loss 0.511091, acc 0.7
2016-09-06T18:18:52.326641: step 378, loss 0.373029, acc 0.82
2016-09-06T18:18:52.995636: step 379, loss 0.372542, acc 0.82
2016-09-06T18:18:53.654255: step 380, loss 0.22076, acc 0.88
2016-09-06T18:18:54.328950: step 381, loss 0.344605, acc 0.82
2016-09-06T18:18:54.993809: step 382, loss 0.442798, acc 0.82
2016-09-06T18:18:55.649568: step 383, loss 0.337208, acc 0.88
2016-09-06T18:18:56.264837: step 384, loss 0.405022, acc 0.863636
2016-09-06T18:18:56.967655: step 385, loss 0.241733, acc 0.94
2016-09-06T18:18:57.646599: step 386, loss 0.254748, acc 0.94
2016-09-06T18:18:58.322592: step 387, loss 0.194669, acc 0.96
2016-09-06T18:18:59.014223: step 388, loss 0.202196, acc 0.94
2016-09-06T18:18:59.701312: step 389, loss 0.221122, acc 0.92
2016-09-06T18:19:00.394340: step 390, loss 0.155037, acc 0.98
2016-09-06T18:19:01.064836: step 391, loss 0.225542, acc 0.96
2016-09-06T18:19:01.749335: step 392, loss 0.226469, acc 0.92
2016-09-06T18:19:02.406800: step 393, loss 0.17274, acc 0.94
2016-09-06T18:19:03.063187: step 394, loss 0.221948, acc 0.9
2016-09-06T18:19:03.734146: step 395, loss 0.105707, acc 0.98
2016-09-06T18:19:04.397326: step 396, loss 0.221064, acc 0.9
2016-09-06T18:19:05.098836: step 397, loss 0.110487, acc 0.92
2016-09-06T18:19:05.769940: step 398, loss 0.185373, acc 0.9
2016-09-06T18:19:06.442348: step 399, loss 0.128824, acc 0.98
2016-09-06T18:19:07.132888: step 400, loss 0.302171, acc 0.88

Evaluation:
2016-09-06T18:19:10.200520: step 400, loss 0.544774, acc 0.803002

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-400

2016-09-06T18:19:11.827245: step 401, loss 0.154041, acc 0.94
2016-09-06T18:19:12.530842: step 402, loss 0.261439, acc 0.9
2016-09-06T18:19:13.201893: step 403, loss 0.175617, acc 0.94
2016-09-06T18:19:13.877689: step 404, loss 0.392606, acc 0.82
2016-09-06T18:19:14.529267: step 405, loss 0.173345, acc 0.92
2016-09-06T18:19:15.205727: step 406, loss 0.227492, acc 0.9
2016-09-06T18:19:15.869316: step 407, loss 0.235375, acc 0.88
2016-09-06T18:19:16.556109: step 408, loss 0.227676, acc 0.9
2016-09-06T18:19:17.216244: step 409, loss 0.344373, acc 0.88
2016-09-06T18:19:17.874554: step 410, loss 0.236263, acc 0.92
2016-09-06T18:19:18.536032: step 411, loss 0.116102, acc 0.96
2016-09-06T18:19:19.235135: step 412, loss 0.250632, acc 0.9
2016-09-06T18:19:19.931098: step 413, loss 0.444705, acc 0.76
2016-09-06T18:19:20.582053: step 414, loss 0.410807, acc 0.82
2016-09-06T18:19:21.259832: step 415, loss 0.177269, acc 0.94
2016-09-06T18:19:21.924448: step 416, loss 0.258851, acc 0.9
2016-09-06T18:19:22.578237: step 417, loss 0.288524, acc 0.88
2016-09-06T18:19:23.245011: step 418, loss 0.302702, acc 0.86
2016-09-06T18:19:23.912063: step 419, loss 0.30092, acc 0.82
2016-09-06T18:19:24.580494: step 420, loss 0.25678, acc 0.9
2016-09-06T18:19:25.236620: step 421, loss 0.292988, acc 0.88
2016-09-06T18:19:25.896488: step 422, loss 0.200713, acc 0.94
2016-09-06T18:19:26.576467: step 423, loss 0.187054, acc 0.94
2016-09-06T18:19:27.242390: step 424, loss 0.226866, acc 0.88
2016-09-06T18:19:27.909258: step 425, loss 0.306004, acc 0.86
2016-09-06T18:19:28.595996: step 426, loss 0.282696, acc 0.88
2016-09-06T18:19:29.267623: step 427, loss 0.31877, acc 0.88
2016-09-06T18:19:29.942627: step 428, loss 0.131294, acc 0.94
2016-09-06T18:19:30.616185: step 429, loss 0.328205, acc 0.9
2016-09-06T18:19:31.291911: step 430, loss 0.328348, acc 0.86
2016-09-06T18:19:31.957134: step 431, loss 0.274823, acc 0.9
2016-09-06T18:19:32.608911: step 432, loss 0.165968, acc 0.96
2016-09-06T18:19:33.296507: step 433, loss 0.342567, acc 0.8
2016-09-06T18:19:33.978358: step 434, loss 0.33895, acc 0.88
2016-09-06T18:19:34.660256: step 435, loss 0.599913, acc 0.84
2016-09-06T18:19:35.316853: step 436, loss 0.375922, acc 0.84
2016-09-06T18:19:36.004906: step 437, loss 0.212109, acc 0.92
2016-09-06T18:19:36.689692: step 438, loss 0.28209, acc 0.92
2016-09-06T18:19:37.367237: step 439, loss 0.286088, acc 0.88
2016-09-06T18:19:38.038079: step 440, loss 0.205895, acc 0.94
2016-09-06T18:19:38.701905: step 441, loss 0.208895, acc 0.94
2016-09-06T18:19:39.375766: step 442, loss 0.27003, acc 0.88
2016-09-06T18:19:40.040316: step 443, loss 0.230316, acc 0.9
2016-09-06T18:19:40.690800: step 444, loss 0.307817, acc 0.82
2016-09-06T18:19:41.357816: step 445, loss 0.317351, acc 0.88
2016-09-06T18:19:42.006620: step 446, loss 0.238255, acc 0.94
2016-09-06T18:19:42.678146: step 447, loss 0.41438, acc 0.8
2016-09-06T18:19:43.373329: step 448, loss 0.301354, acc 0.86
2016-09-06T18:19:44.047682: step 449, loss 0.246411, acc 0.92
2016-09-06T18:19:44.727956: step 450, loss 0.275536, acc 0.9
2016-09-06T18:19:45.387644: step 451, loss 0.230782, acc 0.86
2016-09-06T18:19:46.062455: step 452, loss 0.212619, acc 0.92
2016-09-06T18:19:46.751876: step 453, loss 0.409868, acc 0.84
2016-09-06T18:19:47.426230: step 454, loss 0.221439, acc 0.92
2016-09-06T18:19:48.097601: step 455, loss 0.210752, acc 0.94
2016-09-06T18:19:48.765507: step 456, loss 0.24199, acc 0.92
2016-09-06T18:19:49.451393: step 457, loss 0.363254, acc 0.88
2016-09-06T18:19:50.122799: step 458, loss 0.29705, acc 0.84
2016-09-06T18:19:50.810418: step 459, loss 0.248812, acc 0.9
2016-09-06T18:19:51.505300: step 460, loss 0.27993, acc 0.86
2016-09-06T18:19:52.190034: step 461, loss 0.233962, acc 0.9
2016-09-06T18:19:52.851089: step 462, loss 0.289111, acc 0.88
2016-09-06T18:19:53.512602: step 463, loss 0.226866, acc 0.9
2016-09-06T18:19:54.174082: step 464, loss 0.167042, acc 0.88
2016-09-06T18:19:54.863328: step 465, loss 0.278663, acc 0.9
2016-09-06T18:19:55.536115: step 466, loss 0.220612, acc 0.9
2016-09-06T18:19:56.221576: step 467, loss 0.249181, acc 0.9
2016-09-06T18:19:56.886761: step 468, loss 0.278473, acc 0.9
2016-09-06T18:19:57.560247: step 469, loss 0.225098, acc 0.92
2016-09-06T18:19:58.213719: step 470, loss 0.126625, acc 0.92
2016-09-06T18:19:58.873076: step 471, loss 0.137316, acc 0.98
2016-09-06T18:19:59.528865: step 472, loss 0.305055, acc 0.9
2016-09-06T18:20:00.206744: step 473, loss 0.252186, acc 0.92
2016-09-06T18:20:00.901745: step 474, loss 0.271624, acc 0.88
2016-09-06T18:20:01.544446: step 475, loss 0.306345, acc 0.88
2016-09-06T18:20:02.214528: step 476, loss 0.189968, acc 0.92
2016-09-06T18:20:02.892828: step 477, loss 0.375279, acc 0.84
2016-09-06T18:20:03.566435: step 478, loss 0.396564, acc 0.88
2016-09-06T18:20:04.252659: step 479, loss 0.2985, acc 0.82
2016-09-06T18:20:04.918148: step 480, loss 0.235412, acc 0.86
2016-09-06T18:20:05.592336: step 481, loss 0.287537, acc 0.9
2016-09-06T18:20:06.275200: step 482, loss 0.323071, acc 0.9
2016-09-06T18:20:06.937512: step 483, loss 0.211606, acc 0.92
2016-09-06T18:20:07.602674: step 484, loss 0.281664, acc 0.86
2016-09-06T18:20:08.270168: step 485, loss 0.257693, acc 0.84
2016-09-06T18:20:08.943829: step 486, loss 0.228607, acc 0.9
2016-09-06T18:20:09.595488: step 487, loss 0.246622, acc 0.94
2016-09-06T18:20:10.278375: step 488, loss 0.215756, acc 0.9
2016-09-06T18:20:10.939832: step 489, loss 0.38341, acc 0.82
2016-09-06T18:20:11.612716: step 490, loss 0.167773, acc 0.94
2016-09-06T18:20:12.288899: step 491, loss 0.321977, acc 0.88
2016-09-06T18:20:12.958033: step 492, loss 0.241724, acc 0.92
2016-09-06T18:20:13.648512: step 493, loss 0.205059, acc 0.94
2016-09-06T18:20:14.325672: step 494, loss 0.320233, acc 0.82
2016-09-06T18:20:15.002520: step 495, loss 0.119329, acc 0.98
2016-09-06T18:20:15.655141: step 496, loss 0.391555, acc 0.78
2016-09-06T18:20:16.323848: step 497, loss 0.261989, acc 0.9
2016-09-06T18:20:16.983696: step 498, loss 0.22155, acc 0.94
2016-09-06T18:20:17.994192: step 499, loss 0.308918, acc 0.82
2016-09-06T18:20:18.947015: step 500, loss 0.208418, acc 0.92

Evaluation:
2016-09-06T18:20:22.733890: step 500, loss 0.452397, acc 0.803002

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-500

2016-09-06T18:20:24.914208: step 501, loss 0.35608, acc 0.84
2016-09-06T18:20:25.869402: step 502, loss 0.277298, acc 0.84
2016-09-06T18:20:26.825449: step 503, loss 0.39125, acc 0.8
2016-09-06T18:20:27.775191: step 504, loss 0.305087, acc 0.86
2016-09-06T18:20:28.540017: step 505, loss 0.23444, acc 0.9
2016-09-06T18:20:29.342475: step 506, loss 0.221588, acc 0.94
2016-09-06T18:20:30.005710: step 507, loss 0.299602, acc 0.88
2016-09-06T18:20:30.719013: step 508, loss 0.333529, acc 0.82
2016-09-06T18:20:31.401886: step 509, loss 0.217368, acc 0.9
2016-09-06T18:20:32.078652: step 510, loss 0.2921, acc 0.88
2016-09-06T18:20:32.745609: step 511, loss 0.225499, acc 0.92
2016-09-06T18:20:33.461884: step 512, loss 0.211908, acc 0.9
2016-09-06T18:20:34.158588: step 513, loss 0.251115, acc 0.96
2016-09-06T18:20:34.821758: step 514, loss 0.188731, acc 0.94
2016-09-06T18:20:35.516682: step 515, loss 0.298729, acc 0.92
2016-09-06T18:20:36.202170: step 516, loss 0.209054, acc 0.92
2016-09-06T18:20:36.896999: step 517, loss 0.252277, acc 0.92
2016-09-06T18:20:37.583414: step 518, loss 0.189304, acc 0.92
2016-09-06T18:20:38.266421: step 519, loss 0.273226, acc 0.84
2016-09-06T18:20:38.969056: step 520, loss 0.194667, acc 0.92
2016-09-06T18:20:39.619698: step 521, loss 0.327821, acc 0.86
2016-09-06T18:20:40.314817: step 522, loss 0.20914, acc 0.88
2016-09-06T18:20:40.980085: step 523, loss 0.274102, acc 0.88
2016-09-06T18:20:41.698972: step 524, loss 0.228477, acc 0.96
2016-09-06T18:20:42.371198: step 525, loss 0.264724, acc 0.9
2016-09-06T18:20:43.044635: step 526, loss 0.224549, acc 0.86
2016-09-06T18:20:43.723884: step 527, loss 0.265615, acc 0.94
2016-09-06T18:20:44.421043: step 528, loss 0.206563, acc 0.92
2016-09-06T18:20:45.119010: step 529, loss 0.410429, acc 0.8
2016-09-06T18:20:45.801941: step 530, loss 0.191574, acc 0.9
2016-09-06T18:20:46.492533: step 531, loss 0.206961, acc 0.92
2016-09-06T18:20:47.183801: step 532, loss 0.274804, acc 0.9
2016-09-06T18:20:47.877074: step 533, loss 0.2652, acc 0.86
2016-09-06T18:20:48.564598: step 534, loss 0.200885, acc 0.94
2016-09-06T18:20:49.246951: step 535, loss 0.11859, acc 0.96
2016-09-06T18:20:49.947165: step 536, loss 0.223992, acc 0.9
2016-09-06T18:20:50.631932: step 537, loss 0.203191, acc 0.9
2016-09-06T18:20:51.321503: step 538, loss 0.37872, acc 0.84
2016-09-06T18:20:52.026840: step 539, loss 0.264633, acc 0.88
2016-09-06T18:20:52.709345: step 540, loss 0.163488, acc 0.9
2016-09-06T18:20:53.388189: step 541, loss 0.263138, acc 0.84
2016-09-06T18:20:54.042274: step 542, loss 0.23493, acc 0.92
2016-09-06T18:20:54.741156: step 543, loss 0.249402, acc 0.88
2016-09-06T18:20:55.411673: step 544, loss 0.27427, acc 0.9
2016-09-06T18:20:56.114685: step 545, loss 0.118841, acc 0.96
2016-09-06T18:20:56.792152: step 546, loss 0.202281, acc 0.86
2016-09-06T18:20:57.478951: step 547, loss 0.323961, acc 0.8
2016-09-06T18:20:58.172155: step 548, loss 0.29966, acc 0.88
2016-09-06T18:20:58.841224: step 549, loss 0.206286, acc 0.86
2016-09-06T18:20:59.537748: step 550, loss 0.304969, acc 0.82
2016-09-06T18:21:00.233661: step 551, loss 0.110661, acc 0.94
2016-09-06T18:21:00.919782: step 552, loss 0.212893, acc 0.88
2016-09-06T18:21:01.599930: step 553, loss 0.265031, acc 0.92
2016-09-06T18:21:02.296918: step 554, loss 0.205134, acc 0.92
2016-09-06T18:21:03.013288: step 555, loss 0.176553, acc 0.92
2016-09-06T18:21:03.675117: step 556, loss 0.280474, acc 0.86
2016-09-06T18:21:04.377585: step 557, loss 0.30688, acc 0.88
2016-09-06T18:21:05.056099: step 558, loss 0.136815, acc 0.98
2016-09-06T18:21:05.758099: step 559, loss 0.167997, acc 0.94
2016-09-06T18:21:06.452194: step 560, loss 0.227843, acc 0.92
2016-09-06T18:21:07.142366: step 561, loss 0.401194, acc 0.88
2016-09-06T18:21:07.837149: step 562, loss 0.221046, acc 0.92
2016-09-06T18:21:08.509255: step 563, loss 0.320154, acc 0.88
2016-09-06T18:21:09.202221: step 564, loss 0.145699, acc 0.94
2016-09-06T18:21:09.907302: step 565, loss 0.29221, acc 0.88
2016-09-06T18:21:10.584254: step 566, loss 0.20785, acc 0.92
2016-09-06T18:21:11.280105: step 567, loss 0.324118, acc 0.9
2016-09-06T18:21:11.963556: step 568, loss 0.269858, acc 0.86
2016-09-06T18:21:12.655846: step 569, loss 0.203947, acc 0.92
2016-09-06T18:21:13.320163: step 570, loss 0.413651, acc 0.84
2016-09-06T18:21:14.037558: step 571, loss 0.366827, acc 0.8
2016-09-06T18:21:14.730671: step 572, loss 0.313539, acc 0.9
2016-09-06T18:21:15.418927: step 573, loss 0.321837, acc 0.8
2016-09-06T18:21:16.093198: step 574, loss 0.347868, acc 0.78
2016-09-06T18:21:16.778830: step 575, loss 0.254165, acc 0.9
2016-09-06T18:21:17.421143: step 576, loss 0.240921, acc 0.931818
2016-09-06T18:21:18.117321: step 577, loss 0.169288, acc 0.94
2016-09-06T18:21:18.822683: step 578, loss 0.349466, acc 0.84
2016-09-06T18:21:19.512341: step 579, loss 0.198011, acc 0.94
2016-09-06T18:21:20.193146: step 580, loss 0.170994, acc 0.94
2016-09-06T18:21:20.880107: step 581, loss 0.205377, acc 0.92
2016-09-06T18:21:21.574188: step 582, loss 0.291211, acc 0.9
2016-09-06T18:21:22.269868: step 583, loss 0.173069, acc 0.92
2016-09-06T18:21:22.969140: step 584, loss 0.118702, acc 0.96
2016-09-06T18:21:23.651271: step 585, loss 0.0806718, acc 1
2016-09-06T18:21:24.344308: step 586, loss 0.199525, acc 0.9
2016-09-06T18:21:25.026390: step 587, loss 0.162382, acc 0.92
2016-09-06T18:21:25.689785: step 588, loss 0.155626, acc 0.92
2016-09-06T18:21:26.393472: step 589, loss 0.106498, acc 0.96
2016-09-06T18:21:27.099438: step 590, loss 0.134954, acc 0.98
2016-09-06T18:21:27.783333: step 591, loss 0.0489757, acc 1
2016-09-06T18:21:28.508259: step 592, loss 0.122756, acc 0.92
2016-09-06T18:21:29.190081: step 593, loss 0.250499, acc 0.94
2016-09-06T18:21:29.883768: step 594, loss 0.166148, acc 0.94
2016-09-06T18:21:30.582363: step 595, loss 0.104596, acc 0.96
2016-09-06T18:21:31.241358: step 596, loss 0.191558, acc 0.92
2016-09-06T18:21:31.947665: step 597, loss 0.0971074, acc 0.96
2016-09-06T18:21:32.609565: step 598, loss 0.101476, acc 0.96
2016-09-06T18:21:33.279455: step 599, loss 0.121845, acc 0.92
2016-09-06T18:21:33.944391: step 600, loss 0.0967914, acc 0.96

Evaluation:
2016-09-06T18:21:37.067721: step 600, loss 0.635132, acc 0.793621

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-600

2016-09-06T18:21:38.719142: step 601, loss 0.183727, acc 0.96
2016-09-06T18:21:39.374745: step 602, loss 0.269165, acc 0.9
2016-09-06T18:21:40.074081: step 603, loss 0.236204, acc 0.9
2016-09-06T18:21:40.759222: step 604, loss 0.127086, acc 0.96
2016-09-06T18:21:41.444186: step 605, loss 0.261992, acc 0.88
2016-09-06T18:21:42.115454: step 606, loss 0.20591, acc 0.92
2016-09-06T18:21:42.829607: step 607, loss 0.105725, acc 0.98
2016-09-06T18:21:43.520422: step 608, loss 0.123295, acc 0.94
2016-09-06T18:21:44.216614: step 609, loss 0.211632, acc 0.92
2016-09-06T18:21:44.901141: step 610, loss 0.202142, acc 0.92
2016-09-06T18:21:45.587113: step 611, loss 0.246815, acc 0.9
2016-09-06T18:21:46.298787: step 612, loss 0.249884, acc 0.88
2016-09-06T18:21:46.971525: step 613, loss 0.14383, acc 0.94
2016-09-06T18:21:47.655307: step 614, loss 0.26117, acc 0.86
2016-09-06T18:21:48.347395: step 615, loss 0.256383, acc 0.86
2016-09-06T18:21:49.031773: step 616, loss 0.0912798, acc 0.96
2016-09-06T18:21:49.718525: step 617, loss 0.107247, acc 0.96
2016-09-06T18:21:50.398244: step 618, loss 0.196816, acc 0.94
2016-09-06T18:21:51.092402: step 619, loss 0.201381, acc 0.92
2016-09-06T18:21:51.750470: step 620, loss 0.163841, acc 0.96
2016-09-06T18:21:52.443634: step 621, loss 0.216187, acc 0.9
2016-09-06T18:21:53.117972: step 622, loss 0.182923, acc 0.9
2016-09-06T18:21:53.802225: step 623, loss 0.146764, acc 0.94
2016-09-06T18:21:54.479352: step 624, loss 0.269646, acc 0.9
2016-09-06T18:21:55.147009: step 625, loss 0.209631, acc 0.92
2016-09-06T18:21:55.827060: step 626, loss 0.184847, acc 0.9
2016-09-06T18:21:56.468898: step 627, loss 0.143677, acc 0.94
2016-09-06T18:21:57.180439: step 628, loss 0.103676, acc 0.94
2016-09-06T18:21:57.859017: step 629, loss 0.0647093, acc 0.98
2016-09-06T18:21:58.541888: step 630, loss 0.126593, acc 0.96
2016-09-06T18:21:59.228518: step 631, loss 0.174253, acc 0.94
2016-09-06T18:21:59.916099: step 632, loss 0.124304, acc 0.94
2016-09-06T18:22:00.630330: step 633, loss 0.127465, acc 0.92
2016-09-06T18:22:01.313314: step 634, loss 0.0951298, acc 0.96
2016-09-06T18:22:02.016458: step 635, loss 0.172212, acc 0.92
2016-09-06T18:22:02.699999: step 636, loss 0.27408, acc 0.92
2016-09-06T18:22:03.383699: step 637, loss 0.211331, acc 0.92
2016-09-06T18:22:04.068660: step 638, loss 0.206915, acc 0.92
2016-09-06T18:22:04.758308: step 639, loss 0.131943, acc 0.96
2016-09-06T18:22:05.456184: step 640, loss 0.0908515, acc 0.96
2016-09-06T18:22:06.122185: step 641, loss 0.0909522, acc 0.96
2016-09-06T18:22:06.864532: step 642, loss 0.245225, acc 0.9
2016-09-06T18:22:07.555472: step 643, loss 0.125315, acc 0.94
2016-09-06T18:22:08.240068: step 644, loss 0.173422, acc 0.94
2016-09-06T18:22:08.954423: step 645, loss 0.219211, acc 0.92
2016-09-06T18:22:09.637037: step 646, loss 0.303082, acc 0.88
2016-09-06T18:22:10.326795: step 647, loss 0.200913, acc 0.86
2016-09-06T18:22:11.009096: step 648, loss 0.232477, acc 0.94
2016-09-06T18:22:11.702564: step 649, loss 0.185937, acc 0.92
2016-09-06T18:22:12.387443: step 650, loss 0.21184, acc 0.94
2016-09-06T18:22:13.070544: step 651, loss 0.0600805, acc 0.98
2016-09-06T18:22:13.759171: step 652, loss 0.173622, acc 0.92
2016-09-06T18:22:14.435161: step 653, loss 0.243555, acc 0.88
2016-09-06T18:22:15.150337: step 654, loss 0.246168, acc 0.9
2016-09-06T18:22:15.813031: step 655, loss 0.153371, acc 0.92
2016-09-06T18:22:16.512584: step 656, loss 0.260542, acc 0.9
2016-09-06T18:22:17.212828: step 657, loss 0.140119, acc 0.98
2016-09-06T18:22:17.889161: step 658, loss 0.280658, acc 0.88
2016-09-06T18:22:18.594111: step 659, loss 0.218499, acc 0.96
2016-09-06T18:22:19.270244: step 660, loss 0.193858, acc 0.9
2016-09-06T18:22:19.952980: step 661, loss 0.156682, acc 0.94
2016-09-06T18:22:20.620682: step 662, loss 0.180873, acc 0.94
2016-09-06T18:22:21.306777: step 663, loss 0.201033, acc 0.88
2016-09-06T18:22:21.990434: step 664, loss 0.187306, acc 0.92
2016-09-06T18:22:22.697371: step 665, loss 0.202128, acc 0.88
2016-09-06T18:22:23.378314: step 666, loss 0.118403, acc 0.92
2016-09-06T18:22:24.068009: step 667, loss 0.118879, acc 0.92
2016-09-06T18:22:24.776801: step 668, loss 0.12801, acc 0.96
2016-09-06T18:22:25.444232: step 669, loss 0.228393, acc 0.9
2016-09-06T18:22:26.145539: step 670, loss 0.197927, acc 0.9
2016-09-06T18:22:26.824000: step 671, loss 0.143667, acc 0.92
2016-09-06T18:22:27.509590: step 672, loss 0.119731, acc 0.96
2016-09-06T18:22:28.193872: step 673, loss 0.121036, acc 0.94
2016-09-06T18:22:28.888695: step 674, loss 0.192262, acc 0.92
2016-09-06T18:22:29.628982: step 675, loss 0.113064, acc 0.96
2016-09-06T18:22:30.307601: step 676, loss 0.274891, acc 0.86
2016-09-06T18:22:30.994072: step 677, loss 0.230326, acc 0.92
2016-09-06T18:22:31.685628: step 678, loss 0.154117, acc 0.94
2016-09-06T18:22:32.376789: step 679, loss 0.117269, acc 0.94
2016-09-06T18:22:33.077350: step 680, loss 0.119299, acc 0.92
2016-09-06T18:22:33.740666: step 681, loss 0.122767, acc 0.96
2016-09-06T18:22:34.447070: step 682, loss 0.256608, acc 0.88
2016-09-06T18:22:35.130604: step 683, loss 0.183942, acc 0.9
2016-09-06T18:22:35.815416: step 684, loss 0.107083, acc 0.96
2016-09-06T18:22:36.512834: step 685, loss 0.179741, acc 0.94
2016-09-06T18:22:37.200408: step 686, loss 0.37375, acc 0.86
2016-09-06T18:22:37.886136: step 687, loss 0.256285, acc 0.86
2016-09-06T18:22:38.564252: step 688, loss 0.0617706, acc 0.96
2016-09-06T18:22:39.275180: step 689, loss 0.411487, acc 0.82
2016-09-06T18:22:39.946312: step 690, loss 0.145285, acc 0.94
2016-09-06T18:22:40.647830: step 691, loss 0.211787, acc 0.9
2016-09-06T18:22:41.335454: step 692, loss 0.132548, acc 0.96
2016-09-06T18:22:42.024517: step 693, loss 0.16716, acc 0.94
2016-09-06T18:22:42.724836: step 694, loss 0.150886, acc 0.94
2016-09-06T18:22:43.384351: step 695, loss 0.209437, acc 0.9
2016-09-06T18:22:44.079031: step 696, loss 0.160143, acc 0.92
2016-09-06T18:22:44.771510: step 697, loss 0.153486, acc 0.96
2016-09-06T18:22:45.472179: step 698, loss 0.241947, acc 0.92
2016-09-06T18:22:46.181638: step 699, loss 0.226832, acc 0.88
2016-09-06T18:22:46.873405: step 700, loss 0.17352, acc 0.96

Evaluation:
2016-09-06T18:22:50.046980: step 700, loss 0.509611, acc 0.778612

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-700

2016-09-06T18:22:51.675653: step 701, loss 0.0836503, acc 1
2016-09-06T18:22:52.361454: step 702, loss 0.220204, acc 0.9
2016-09-06T18:22:53.035263: step 703, loss 0.329, acc 0.86
2016-09-06T18:22:53.727851: step 704, loss 0.163392, acc 0.94
2016-09-06T18:22:54.443694: step 705, loss 0.201295, acc 0.92
2016-09-06T18:22:55.137886: step 706, loss 0.205525, acc 0.9
2016-09-06T18:22:55.822639: step 707, loss 0.193574, acc 0.92
2016-09-06T18:22:56.511659: step 708, loss 0.140727, acc 0.96
2016-09-06T18:22:57.206063: step 709, loss 0.138929, acc 0.92
2016-09-06T18:22:57.873686: step 710, loss 0.287603, acc 0.92
2016-09-06T18:22:58.563293: step 711, loss 0.0940621, acc 0.98
2016-09-06T18:22:59.247400: step 712, loss 0.140726, acc 0.96
2016-09-06T18:22:59.934247: step 713, loss 0.120893, acc 0.94
2016-09-06T18:23:00.643382: step 714, loss 0.198038, acc 0.92
2016-09-06T18:23:01.333108: step 715, loss 0.131099, acc 0.96
2016-09-06T18:23:02.032379: step 716, loss 0.259015, acc 0.86
2016-09-06T18:23:02.701403: step 717, loss 0.258496, acc 0.82
2016-09-06T18:23:03.412754: step 718, loss 0.118774, acc 0.92
2016-09-06T18:23:04.110615: step 719, loss 0.207745, acc 0.88
2016-09-06T18:23:04.794244: step 720, loss 0.271227, acc 0.9
2016-09-06T18:23:05.492201: step 721, loss 0.300091, acc 0.86
2016-09-06T18:23:06.166642: step 722, loss 0.178807, acc 0.94
2016-09-06T18:23:06.873940: step 723, loss 0.190659, acc 0.96
2016-09-06T18:23:07.536384: step 724, loss 0.203352, acc 0.88
2016-09-06T18:23:08.234624: step 725, loss 0.197991, acc 0.92
2016-09-06T18:23:08.934770: step 726, loss 0.210677, acc 0.94
2016-09-06T18:23:09.615761: step 727, loss 0.15989, acc 0.92
2016-09-06T18:23:10.299250: step 728, loss 0.20843, acc 0.92
2016-09-06T18:23:10.990294: step 729, loss 0.170651, acc 0.92
2016-09-06T18:23:11.693832: step 730, loss 0.134272, acc 0.94
2016-09-06T18:23:12.363676: step 731, loss 0.138071, acc 0.94
2016-09-06T18:23:13.024519: step 732, loss 0.101887, acc 0.96
2016-09-06T18:23:13.715289: step 733, loss 0.100742, acc 0.98
2016-09-06T18:23:14.396443: step 734, loss 0.143891, acc 0.94
2016-09-06T18:23:15.067036: step 735, loss 0.156486, acc 0.94
2016-09-06T18:23:15.760115: step 736, loss 0.150178, acc 0.92
2016-09-06T18:23:16.471548: step 737, loss 0.136918, acc 0.96
2016-09-06T18:23:17.149434: step 738, loss 0.138994, acc 0.9
2016-09-06T18:23:17.839377: step 739, loss 0.13633, acc 0.94
2016-09-06T18:23:18.526936: step 740, loss 0.11963, acc 0.96
2016-09-06T18:23:19.210663: step 741, loss 0.158503, acc 0.94
2016-09-06T18:23:19.908031: step 742, loss 0.129606, acc 0.9
2016-09-06T18:23:20.591642: step 743, loss 0.165985, acc 0.92
2016-09-06T18:23:21.287487: step 744, loss 0.19509, acc 0.9
2016-09-06T18:23:21.949977: step 745, loss 0.363768, acc 0.88
2016-09-06T18:23:22.641677: step 746, loss 0.193868, acc 0.86
2016-09-06T18:23:23.322651: step 747, loss 0.0981048, acc 0.96
2016-09-06T18:23:24.010405: step 748, loss 0.19305, acc 0.92
2016-09-06T18:23:24.709326: step 749, loss 0.184751, acc 0.88
2016-09-06T18:23:25.395984: step 750, loss 0.0747958, acc 0.96
2016-09-06T18:23:26.087890: step 751, loss 0.186384, acc 0.86
2016-09-06T18:23:26.742809: step 752, loss 0.209366, acc 0.92
2016-09-06T18:23:27.436315: step 753, loss 0.153466, acc 0.94
2016-09-06T18:23:28.117283: step 754, loss 0.122506, acc 0.98
2016-09-06T18:23:28.795159: step 755, loss 0.208418, acc 0.9
2016-09-06T18:23:29.477653: step 756, loss 0.207295, acc 0.9
2016-09-06T18:23:30.158481: step 757, loss 0.09208, acc 0.98
2016-09-06T18:23:30.829754: step 758, loss 0.231915, acc 0.9
2016-09-06T18:23:31.490042: step 759, loss 0.139951, acc 0.94
2016-09-06T18:23:32.183421: step 760, loss 0.193871, acc 0.9
2016-09-06T18:23:32.880876: step 761, loss 0.206863, acc 0.9
2016-09-06T18:23:33.572709: step 762, loss 0.303739, acc 0.88
2016-09-06T18:23:34.261987: step 763, loss 0.311883, acc 0.92
2016-09-06T18:23:34.952542: step 764, loss 0.169507, acc 0.9
2016-09-06T18:23:35.671982: step 765, loss 0.250878, acc 0.92
2016-09-06T18:23:36.358494: step 766, loss 0.174556, acc 0.92
2016-09-06T18:23:37.072597: step 767, loss 0.276428, acc 0.88
2016-09-06T18:23:37.725389: step 768, loss 0.058961, acc 0.977273
2016-09-06T18:23:38.423815: step 769, loss 0.127827, acc 0.96
2016-09-06T18:23:39.110719: step 770, loss 0.154493, acc 0.92
2016-09-06T18:23:39.813190: step 771, loss 0.105964, acc 0.98
2016-09-06T18:23:40.513252: step 772, loss 0.156557, acc 0.92
2016-09-06T18:23:41.194036: step 773, loss 0.0573642, acc 0.98
2016-09-06T18:23:41.896978: step 774, loss 0.204931, acc 0.9
2016-09-06T18:23:42.586128: step 775, loss 0.0827587, acc 0.96
2016-09-06T18:23:43.256996: step 776, loss 0.116284, acc 0.96
2016-09-06T18:23:43.941698: step 777, loss 0.102381, acc 0.96
2016-09-06T18:23:44.613380: step 778, loss 0.137678, acc 0.96
2016-09-06T18:23:45.315227: step 779, loss 0.140991, acc 0.94
2016-09-06T18:23:46.001308: step 780, loss 0.153709, acc 0.92
2016-09-06T18:23:46.685549: step 781, loss 0.0939771, acc 0.98
2016-09-06T18:23:47.359655: step 782, loss 0.184017, acc 0.92
2016-09-06T18:23:48.042882: step 783, loss 0.0735543, acc 0.98
2016-09-06T18:23:48.740296: step 784, loss 0.110295, acc 0.96
2016-09-06T18:23:49.428621: step 785, loss 0.108962, acc 0.92
2016-09-06T18:23:50.136142: step 786, loss 0.148692, acc 0.94
2016-09-06T18:23:50.793708: step 787, loss 0.102836, acc 0.96
2016-09-06T18:23:51.506258: step 788, loss 0.075388, acc 0.96
2016-09-06T18:23:52.206057: step 789, loss 0.0606697, acc 0.98
2016-09-06T18:23:52.884169: step 790, loss 0.0741981, acc 0.96
2016-09-06T18:23:53.561691: step 791, loss 0.0934005, acc 0.92
2016-09-06T18:23:54.236495: step 792, loss 0.0204168, acc 1
2016-09-06T18:23:54.943318: step 793, loss 0.0793937, acc 0.96
2016-09-06T18:23:55.636152: step 794, loss 0.213775, acc 0.94
2016-09-06T18:23:56.320108: step 795, loss 0.201364, acc 0.92
2016-09-06T18:23:56.989961: step 796, loss 0.254114, acc 0.9
2016-09-06T18:23:57.663350: step 797, loss 0.104132, acc 0.94
2016-09-06T18:23:58.386824: step 798, loss 0.0619296, acc 1
2016-09-06T18:23:59.075862: step 799, loss 0.0797613, acc 0.98
2016-09-06T18:23:59.780002: step 800, loss 0.185863, acc 0.98

Evaluation:
2016-09-06T18:24:02.955195: step 800, loss 0.686475, acc 0.797373

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-800

2016-09-06T18:24:04.675724: step 801, loss 0.152959, acc 0.92
2016-09-06T18:24:05.370635: step 802, loss 0.1857, acc 0.94
2016-09-06T18:24:06.046824: step 803, loss 0.0955961, acc 0.96
2016-09-06T18:24:06.718638: step 804, loss 0.163874, acc 0.96
2016-09-06T18:24:07.393200: step 805, loss 0.092322, acc 0.98
2016-09-06T18:24:08.063190: step 806, loss 0.24503, acc 0.9
2016-09-06T18:24:08.730683: step 807, loss 0.124182, acc 0.94
2016-09-06T18:24:09.426552: step 808, loss 0.0875208, acc 0.96
2016-09-06T18:24:10.099247: step 809, loss 0.0924038, acc 0.94
2016-09-06T18:24:10.786999: step 810, loss 0.0997138, acc 0.98
2016-09-06T18:24:11.477130: step 811, loss 0.131706, acc 0.9
2016-09-06T18:24:12.176738: step 812, loss 0.0999542, acc 0.94
2016-09-06T18:24:12.867867: step 813, loss 0.208869, acc 0.88
2016-09-06T18:24:13.538012: step 814, loss 0.141534, acc 0.92
2016-09-06T18:24:14.241467: step 815, loss 0.0394774, acc 1
2016-09-06T18:24:14.911700: step 816, loss 0.173786, acc 0.92
2016-09-06T18:24:15.595170: step 817, loss 0.0964082, acc 0.94
2016-09-06T18:24:16.284885: step 818, loss 0.0815979, acc 0.96
2016-09-06T18:24:16.973921: step 819, loss 0.079765, acc 0.96
2016-09-06T18:24:17.648817: step 820, loss 0.0538423, acc 0.98
2016-09-06T18:24:18.333111: step 821, loss 0.107178, acc 0.92
2016-09-06T18:24:19.029097: step 822, loss 0.248196, acc 0.9
2016-09-06T18:24:19.692092: step 823, loss 0.177723, acc 0.9
2016-09-06T18:24:20.367550: step 824, loss 0.0661562, acc 0.96
2016-09-06T18:24:21.041976: step 825, loss 0.0728137, acc 0.98
2016-09-06T18:24:21.735517: step 826, loss 0.0908121, acc 0.92
2016-09-06T18:24:22.440077: step 827, loss 0.0546805, acc 0.98
2016-09-06T18:24:23.139687: step 828, loss 0.123746, acc 0.96
2016-09-06T18:24:23.843460: step 829, loss 0.213413, acc 0.94
2016-09-06T18:24:24.521201: step 830, loss 0.11178, acc 0.98
2016-09-06T18:24:25.194626: step 831, loss 0.115673, acc 0.92
2016-09-06T18:24:25.899303: step 832, loss 0.101966, acc 0.96
2016-09-06T18:24:26.593625: step 833, loss 0.147692, acc 0.94
2016-09-06T18:24:27.278534: step 834, loss 0.118999, acc 0.94
2016-09-06T18:24:27.988203: step 835, loss 0.0984773, acc 0.96
2016-09-06T18:24:28.679202: step 836, loss 0.202672, acc 0.92
2016-09-06T18:24:29.346634: step 837, loss 0.127845, acc 0.96
2016-09-06T18:24:30.034062: step 838, loss 0.0836973, acc 0.94
2016-09-06T18:24:30.718786: step 839, loss 0.082716, acc 0.98
2016-09-06T18:24:31.406733: step 840, loss 0.0839285, acc 0.98
2016-09-06T18:24:32.102003: step 841, loss 0.119407, acc 0.96
2016-09-06T18:24:32.773437: step 842, loss 0.165059, acc 0.92
2016-09-06T18:24:33.464644: step 843, loss 0.106481, acc 0.94
2016-09-06T18:24:34.148101: step 844, loss 0.173191, acc 0.94
2016-09-06T18:24:34.830649: step 845, loss 0.16585, acc 0.9
2016-09-06T18:24:35.522056: step 846, loss 0.193166, acc 0.92
2016-09-06T18:24:36.198355: step 847, loss 0.187754, acc 0.9
2016-09-06T18:24:36.905336: step 848, loss 0.207463, acc 0.9
2016-09-06T18:24:37.590304: step 849, loss 0.0899687, acc 0.96
2016-09-06T18:24:38.315608: step 850, loss 0.194923, acc 0.92
2016-09-06T18:24:38.990750: step 851, loss 0.158222, acc 0.94
2016-09-06T18:24:39.678570: step 852, loss 0.0890845, acc 0.96
2016-09-06T18:24:40.377448: step 853, loss 0.163814, acc 0.96
2016-09-06T18:24:41.062437: step 854, loss 0.0717105, acc 1
2016-09-06T18:24:41.762443: step 855, loss 0.217222, acc 0.88
2016-09-06T18:24:42.430545: step 856, loss 0.10528, acc 0.98
2016-09-06T18:24:43.125459: step 857, loss 0.0724789, acc 1
2016-09-06T18:24:43.818157: step 858, loss 0.154012, acc 0.92
2016-09-06T18:24:44.486273: step 859, loss 0.0435403, acc 1
2016-09-06T18:24:45.173499: step 860, loss 0.166401, acc 0.9
2016-09-06T18:24:45.850601: step 861, loss 0.139948, acc 0.94
2016-09-06T18:24:46.530991: step 862, loss 0.146584, acc 0.96
2016-09-06T18:24:47.198985: step 863, loss 0.11241, acc 0.94
2016-09-06T18:24:47.911801: step 864, loss 0.131261, acc 0.9
2016-09-06T18:24:48.577227: step 865, loss 0.123744, acc 0.92
2016-09-06T18:24:49.273575: step 866, loss 0.0787186, acc 0.96
2016-09-06T18:24:49.953881: step 867, loss 0.154844, acc 0.96
2016-09-06T18:24:50.640185: step 868, loss 0.0676064, acc 0.98
2016-09-06T18:24:51.328149: step 869, loss 0.0659727, acc 0.98
2016-09-06T18:24:51.984703: step 870, loss 0.0789548, acc 0.96
2016-09-06T18:24:52.685928: step 871, loss 0.137901, acc 0.94
2016-09-06T18:24:53.369387: step 872, loss 0.114844, acc 0.94
2016-09-06T18:24:54.053981: step 873, loss 0.125531, acc 0.94
2016-09-06T18:24:54.747574: step 874, loss 0.159284, acc 0.94
2016-09-06T18:24:55.429642: step 875, loss 0.220313, acc 0.9
2016-09-06T18:24:56.114071: step 876, loss 0.185757, acc 0.94
2016-09-06T18:24:56.800553: step 877, loss 0.586836, acc 0.84
2016-09-06T18:24:57.502197: step 878, loss 0.151369, acc 0.92
2016-09-06T18:24:58.198467: step 879, loss 0.140894, acc 0.94
2016-09-06T18:24:58.893541: step 880, loss 0.0248172, acc 1
2016-09-06T18:24:59.608063: step 881, loss 0.203925, acc 0.9
2016-09-06T18:25:00.308022: step 882, loss 0.12077, acc 0.96
2016-09-06T18:25:00.982143: step 883, loss 0.163519, acc 0.92
2016-09-06T18:25:01.670779: step 884, loss 0.109771, acc 0.96
2016-09-06T18:25:02.378336: step 885, loss 0.205051, acc 0.9
2016-09-06T18:25:03.057826: step 886, loss 0.230387, acc 0.9
2016-09-06T18:25:03.740745: step 887, loss 0.0795478, acc 0.98
2016-09-06T18:25:04.427144: step 888, loss 0.0877411, acc 0.96
2016-09-06T18:25:05.106641: step 889, loss 0.196935, acc 0.92
2016-09-06T18:25:05.797836: step 890, loss 0.102623, acc 0.96
2016-09-06T18:25:06.446229: step 891, loss 0.123684, acc 0.92
2016-09-06T18:25:07.169617: step 892, loss 0.202997, acc 0.88
2016-09-06T18:25:07.861801: step 893, loss 0.0815328, acc 0.98
2016-09-06T18:25:08.547004: step 894, loss 0.246087, acc 0.94
2016-09-06T18:25:09.242738: step 895, loss 0.260301, acc 0.88
2016-09-06T18:25:09.930409: step 896, loss 0.134163, acc 0.92
2016-09-06T18:25:10.604005: step 897, loss 0.0768687, acc 0.98
2016-09-06T18:25:11.276316: step 898, loss 0.17204, acc 0.9
2016-09-06T18:25:11.975040: step 899, loss 0.148388, acc 0.94
2016-09-06T18:25:12.677433: step 900, loss 0.128816, acc 0.92

Evaluation:
2016-09-06T18:25:15.858324: step 900, loss 0.726965, acc 0.767355

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-900

2016-09-06T18:25:17.511694: step 901, loss 0.276251, acc 0.84
2016-09-06T18:25:18.189147: step 902, loss 0.105734, acc 0.92
2016-09-06T18:25:18.890714: step 903, loss 0.106515, acc 0.96
2016-09-06T18:25:19.565763: step 904, loss 0.146878, acc 0.96
2016-09-06T18:25:20.253454: step 905, loss 0.0365901, acc 1
2016-09-06T18:25:20.910802: step 906, loss 0.166472, acc 0.94
2016-09-06T18:25:21.623564: step 907, loss 0.0865572, acc 0.96
2016-09-06T18:25:22.310451: step 908, loss 0.0837219, acc 0.98
2016-09-06T18:25:22.980711: step 909, loss 0.120197, acc 0.96
2016-09-06T18:25:23.678289: step 910, loss 0.0966004, acc 0.98
2016-09-06T18:25:24.354982: step 911, loss 0.0469164, acc 1
2016-09-06T18:25:25.032718: step 912, loss 0.108243, acc 0.96
2016-09-06T18:25:25.690313: step 913, loss 0.232633, acc 0.9
2016-09-06T18:25:26.383946: step 914, loss 0.0346851, acc 1
2016-09-06T18:25:27.045652: step 915, loss 0.149827, acc 0.96
2016-09-06T18:25:27.715100: step 916, loss 0.141286, acc 0.9
2016-09-06T18:25:28.389962: step 917, loss 0.153826, acc 0.9
2016-09-06T18:25:29.089937: step 918, loss 0.126622, acc 0.96
2016-09-06T18:25:29.792794: step 919, loss 0.0966974, acc 0.96
2016-09-06T18:25:30.461989: step 920, loss 0.124196, acc 0.94
2016-09-06T18:25:31.152132: step 921, loss 0.172823, acc 0.94
2016-09-06T18:25:31.828135: step 922, loss 0.259738, acc 0.88
2016-09-06T18:25:32.517184: step 923, loss 0.0787713, acc 0.98
2016-09-06T18:25:33.199714: step 924, loss 0.125475, acc 0.94
2016-09-06T18:25:33.888818: step 925, loss 0.0986533, acc 0.94
2016-09-06T18:25:34.584184: step 926, loss 0.238929, acc 0.86
2016-09-06T18:25:35.275082: step 927, loss 0.160112, acc 0.9
2016-09-06T18:25:35.994743: step 928, loss 0.21691, acc 0.86
2016-09-06T18:25:36.679581: step 929, loss 0.0876417, acc 0.96
2016-09-06T18:25:37.355430: step 930, loss 0.152073, acc 0.92
2016-09-06T18:25:38.035186: step 931, loss 0.204137, acc 0.92
2016-09-06T18:25:38.721584: step 932, loss 0.0768987, acc 0.96
2016-09-06T18:25:39.395571: step 933, loss 0.0907339, acc 0.96
2016-09-06T18:25:40.098321: step 934, loss 0.0555414, acc 0.98
2016-09-06T18:25:40.808388: step 935, loss 0.207293, acc 0.94
2016-09-06T18:25:41.495678: step 936, loss 0.179017, acc 0.92
2016-09-06T18:25:42.179277: step 937, loss 0.0955262, acc 0.96
2016-09-06T18:25:42.873549: step 938, loss 0.113791, acc 0.94
2016-09-06T18:25:43.564345: step 939, loss 0.0980712, acc 0.94
2016-09-06T18:25:44.241594: step 940, loss 0.0920302, acc 1
2016-09-06T18:25:44.911309: step 941, loss 0.193707, acc 0.88
2016-09-06T18:25:45.606225: step 942, loss 0.12734, acc 0.92
2016-09-06T18:25:46.297881: step 943, loss 0.160105, acc 0.94
2016-09-06T18:25:46.961970: step 944, loss 0.132718, acc 0.94
2016-09-06T18:25:47.661511: step 945, loss 0.0939062, acc 0.94
2016-09-06T18:25:48.356341: step 946, loss 0.112299, acc 0.94
2016-09-06T18:25:49.028131: step 947, loss 0.150549, acc 0.96
2016-09-06T18:25:49.703297: step 948, loss 0.0966384, acc 0.94
2016-09-06T18:25:50.404028: step 949, loss 0.177277, acc 0.9
2016-09-06T18:25:51.078442: step 950, loss 0.0854943, acc 0.96
2016-09-06T18:25:51.766334: step 951, loss 0.144592, acc 0.94
2016-09-06T18:25:52.484753: step 952, loss 0.286299, acc 0.84
2016-09-06T18:25:53.190604: step 953, loss 0.0996769, acc 0.94
2016-09-06T18:25:53.878203: step 954, loss 0.177016, acc 0.94
2016-09-06T18:25:54.557080: step 955, loss 0.0599451, acc 1
2016-09-06T18:25:55.264716: step 956, loss 0.0602039, acc 0.98
2016-09-06T18:25:55.959121: step 957, loss 0.209677, acc 0.88
2016-09-06T18:25:56.655233: step 958, loss 0.170327, acc 0.94
2016-09-06T18:25:57.338171: step 959, loss 0.130251, acc 0.96
2016-09-06T18:25:57.971225: step 960, loss 0.080237, acc 0.977273
2016-09-06T18:25:58.680449: step 961, loss 0.0611861, acc 0.98
2016-09-06T18:25:59.354261: step 962, loss 0.0361754, acc 0.98
2016-09-06T18:26:00.061937: step 963, loss 0.0531227, acc 0.98
2016-09-06T18:26:00.799216: step 964, loss 0.115525, acc 0.98
2016-09-06T18:26:01.470455: step 965, loss 0.0870197, acc 0.96
2016-09-06T18:26:02.152542: step 966, loss 0.125156, acc 0.94
2016-09-06T18:26:02.839609: step 967, loss 0.037081, acc 0.98
2016-09-06T18:26:03.568378: step 968, loss 0.0797435, acc 0.96
2016-09-06T18:26:04.240559: step 969, loss 0.037289, acc 1
2016-09-06T18:26:04.939664: step 970, loss 0.116233, acc 0.96
2016-09-06T18:26:05.636344: step 971, loss 0.148775, acc 0.9
2016-09-06T18:26:06.324872: step 972, loss 0.216169, acc 0.92
2016-09-06T18:26:06.993119: step 973, loss 0.103008, acc 0.98
2016-09-06T18:26:07.742937: step 974, loss 0.0995076, acc 0.96
2016-09-06T18:26:08.465131: step 975, loss 0.0855754, acc 0.96
2016-09-06T18:26:09.142051: step 976, loss 0.0957358, acc 0.94
2016-09-06T18:26:09.819223: step 977, loss 0.111652, acc 0.92
2016-09-06T18:26:10.494547: step 978, loss 0.184896, acc 0.92
2016-09-06T18:26:11.177970: step 979, loss 0.0632736, acc 0.98
2016-09-06T18:26:11.882678: step 980, loss 0.15265, acc 0.96
2016-09-06T18:26:12.549522: step 981, loss 0.0807669, acc 0.96
2016-09-06T18:26:13.308809: step 982, loss 0.123919, acc 0.96
2016-09-06T18:26:13.995838: step 983, loss 0.0477164, acc 0.98
2016-09-06T18:26:14.702238: step 984, loss 0.06004, acc 0.96
2016-09-06T18:26:15.390888: step 985, loss 0.0612085, acc 0.98
2016-09-06T18:26:16.076954: step 986, loss 0.099182, acc 0.96
2016-09-06T18:26:16.763572: step 987, loss 0.0902804, acc 0.98
2016-09-06T18:26:17.434375: step 988, loss 0.0989497, acc 0.96
2016-09-06T18:26:18.137296: step 989, loss 0.135537, acc 0.92
2016-09-06T18:26:18.859934: step 990, loss 0.0746546, acc 0.96
2016-09-06T18:26:19.534675: step 991, loss 0.114591, acc 0.94
2016-09-06T18:26:20.229705: step 992, loss 0.0827391, acc 0.96
2016-09-06T18:26:20.905758: step 993, loss 0.089439, acc 0.94
2016-09-06T18:26:21.589767: step 994, loss 0.0868587, acc 0.96
2016-09-06T18:26:22.251639: step 995, loss 0.109054, acc 0.92
2016-09-06T18:26:22.974037: step 996, loss 0.10406, acc 0.96
2016-09-06T18:26:23.649898: step 997, loss 0.0281435, acc 1
2016-09-06T18:26:24.338277: step 998, loss 0.101874, acc 0.96
2016-09-06T18:26:25.030470: step 999, loss 0.0390554, acc 1
2016-09-06T18:26:25.736912: step 1000, loss 0.106003, acc 0.96

Evaluation:
2016-09-06T18:26:28.883830: step 1000, loss 0.802573, acc 0.762664

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-1000

2016-09-06T18:26:30.584500: step 1001, loss 0.0457556, acc 1
2016-09-06T18:26:31.287204: step 1002, loss 0.0513939, acc 0.96
2016-09-06T18:26:31.960930: step 1003, loss 0.151798, acc 0.9
2016-09-06T18:26:32.641299: step 1004, loss 0.104312, acc 0.98
2016-09-06T18:26:33.329999: step 1005, loss 0.0996672, acc 0.94
2016-09-06T18:26:33.995550: step 1006, loss 0.0906488, acc 0.96
2016-09-06T18:26:34.673071: step 1007, loss 0.0881429, acc 0.94
2016-09-06T18:26:35.355925: step 1008, loss 0.0758531, acc 0.96
2016-09-06T18:26:36.068019: step 1009, loss 0.0896599, acc 0.98
2016-09-06T18:26:36.747610: step 1010, loss 0.0575216, acc 0.98
2016-09-06T18:26:37.431670: step 1011, loss 0.0922607, acc 0.96
2016-09-06T18:26:38.105400: step 1012, loss 0.0601779, acc 0.98
2016-09-06T18:26:38.785655: step 1013, loss 0.0380149, acc 0.98
2016-09-06T18:26:39.465423: step 1014, loss 0.118998, acc 0.94
2016-09-06T18:26:40.165339: step 1015, loss 0.0478672, acc 0.98
2016-09-06T18:26:40.881075: step 1016, loss 0.0492739, acc 0.96
2016-09-06T18:26:41.558501: step 1017, loss 0.0850987, acc 0.94
2016-09-06T18:26:42.260213: step 1018, loss 0.0918065, acc 0.94
2016-09-06T18:26:42.931402: step 1019, loss 0.121307, acc 0.96
2016-09-06T18:26:43.622563: step 1020, loss 0.0507177, acc 1
2016-09-06T18:26:44.318115: step 1021, loss 0.0925772, acc 0.98
2016-09-06T18:26:45.005966: step 1022, loss 0.0317788, acc 1
2016-09-06T18:26:45.692790: step 1023, loss 0.0618804, acc 0.96
2016-09-06T18:26:46.373934: step 1024, loss 0.0848944, acc 0.94
2016-09-06T18:26:47.051922: step 1025, loss 0.0229225, acc 0.98
2016-09-06T18:26:47.758037: step 1026, loss 0.192988, acc 0.9
2016-09-06T18:26:48.436437: step 1027, loss 0.163365, acc 0.96
2016-09-06T18:26:49.135988: step 1028, loss 0.0416992, acc 0.98
2016-09-06T18:26:49.836674: step 1029, loss 0.113516, acc 0.94
2016-09-06T18:26:50.535532: step 1030, loss 0.0955751, acc 0.96
2016-09-06T18:26:51.208431: step 1031, loss 0.0263443, acc 0.98
2016-09-06T18:26:51.900270: step 1032, loss 0.0699393, acc 0.96
2016-09-06T18:26:52.579007: step 1033, loss 0.062491, acc 0.96
2016-09-06T18:26:53.263344: step 1034, loss 0.124319, acc 0.98
2016-09-06T18:26:53.953560: step 1035, loss 0.0698866, acc 0.98
2016-09-06T18:26:54.644703: step 1036, loss 0.111725, acc 0.92
2016-09-06T18:26:55.351954: step 1037, loss 0.111087, acc 0.94
2016-09-06T18:26:56.024599: step 1038, loss 0.0615572, acc 0.98
2016-09-06T18:26:56.717431: step 1039, loss 0.181895, acc 0.94
2016-09-06T18:26:57.392802: step 1040, loss 0.0515674, acc 1
2016-09-06T18:26:58.057185: step 1041, loss 0.103746, acc 0.92
2016-09-06T18:26:58.743555: step 1042, loss 0.244118, acc 0.94
2016-09-06T18:26:59.402151: step 1043, loss 0.0583845, acc 0.98
2016-09-06T18:27:00.068675: step 1044, loss 0.097223, acc 0.94
2016-09-06T18:27:00.810490: step 1045, loss 0.101013, acc 0.94
2016-09-06T18:27:01.495257: step 1046, loss 0.0356095, acc 0.98
2016-09-06T18:27:02.190482: step 1047, loss 0.120743, acc 0.98
2016-09-06T18:27:02.883023: step 1048, loss 0.0868926, acc 0.94
2016-09-06T18:27:03.565407: step 1049, loss 0.164972, acc 0.96
2016-09-06T18:27:04.239644: step 1050, loss 0.123568, acc 0.96
2016-09-06T18:27:04.934323: step 1051, loss 0.0336036, acc 1
2016-09-06T18:27:05.623455: step 1052, loss 0.0663267, acc 0.96
2016-09-06T18:27:06.301883: step 1053, loss 0.102486, acc 0.94
2016-09-06T18:27:06.983039: step 1054, loss 0.18563, acc 0.94
2016-09-06T18:27:07.668483: step 1055, loss 0.0952719, acc 0.96
2016-09-06T18:27:08.344431: step 1056, loss 0.0846639, acc 0.96
2016-09-06T18:27:09.027863: step 1057, loss 0.129868, acc 0.92
2016-09-06T18:27:09.730927: step 1058, loss 0.128862, acc 0.92
2016-09-06T18:27:10.395726: step 1059, loss 0.108867, acc 0.94
2016-09-06T18:27:11.100252: step 1060, loss 0.194116, acc 0.9
2016-09-06T18:27:11.801357: step 1061, loss 0.0681104, acc 1
2016-09-06T18:27:12.487593: step 1062, loss 0.0426672, acc 0.98
2016-09-06T18:27:13.197088: step 1063, loss 0.18143, acc 0.94
2016-09-06T18:27:13.887155: step 1064, loss 0.0627569, acc 0.98
2016-09-06T18:27:14.585338: step 1065, loss 0.109966, acc 0.98
2016-09-06T18:27:15.262996: step 1066, loss 0.0657612, acc 0.98
2016-09-06T18:27:15.955445: step 1067, loss 0.0824543, acc 0.98
2016-09-06T18:27:16.635665: step 1068, loss 0.0373373, acc 1
2016-09-06T18:27:17.326289: step 1069, loss 0.159027, acc 0.9
2016-09-06T18:27:18.021535: step 1070, loss 0.168171, acc 0.92
2016-09-06T18:27:18.702731: step 1071, loss 0.0424796, acc 0.98
2016-09-06T18:27:19.407146: step 1072, loss 0.120218, acc 0.94
2016-09-06T18:27:20.067216: step 1073, loss 0.0821297, acc 0.96
2016-09-06T18:27:20.742530: step 1074, loss 0.119726, acc 0.96
2016-09-06T18:27:21.443135: step 1075, loss 0.0630365, acc 1
2016-09-06T18:27:22.137325: step 1076, loss 0.0196073, acc 1
2016-09-06T18:27:22.817629: step 1077, loss 0.0649043, acc 0.96
2016-09-06T18:27:23.500242: step 1078, loss 0.186073, acc 0.94
2016-09-06T18:27:24.203310: step 1079, loss 0.210262, acc 0.94
2016-09-06T18:27:24.881358: step 1080, loss 0.0371293, acc 0.98
2016-09-06T18:27:25.570341: step 1081, loss 0.0830559, acc 0.94
2016-09-06T18:27:26.243216: step 1082, loss 0.0173854, acc 1
2016-09-06T18:27:26.929393: step 1083, loss 0.101395, acc 0.98
2016-09-06T18:27:27.612132: step 1084, loss 0.0476663, acc 0.96
2016-09-06T18:27:28.279062: step 1085, loss 0.138551, acc 0.94
2016-09-06T18:27:28.991611: step 1086, loss 0.041163, acc 0.98
2016-09-06T18:27:29.668511: step 1087, loss 0.160389, acc 0.94
2016-09-06T18:27:30.346450: step 1088, loss 0.0586141, acc 0.98
2016-09-06T18:27:31.025776: step 1089, loss 0.0993722, acc 0.96
2016-09-06T18:27:31.705070: step 1090, loss 0.227609, acc 0.92
2016-09-06T18:27:32.421688: step 1091, loss 0.356874, acc 0.86
2016-09-06T18:27:33.108003: step 1092, loss 0.102992, acc 0.94
2016-09-06T18:27:33.807049: step 1093, loss 0.0862255, acc 0.94
2016-09-06T18:27:34.493587: step 1094, loss 0.0463051, acc 0.98
2016-09-06T18:27:35.192847: step 1095, loss 0.030308, acc 1
2016-09-06T18:27:35.903477: step 1096, loss 0.0215034, acc 1
2016-09-06T18:27:36.591297: step 1097, loss 0.0585783, acc 0.98
2016-09-06T18:27:37.273474: step 1098, loss 0.038228, acc 0.98
2016-09-06T18:27:37.950566: step 1099, loss 0.15443, acc 0.94
2016-09-06T18:27:38.655726: step 1100, loss 0.106202, acc 0.94

Evaluation:
2016-09-06T18:27:41.807891: step 1100, loss 0.883833, acc 0.777674

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-1100

2016-09-06T18:27:43.615965: step 1101, loss 0.0531094, acc 0.98
2016-09-06T18:27:44.302267: step 1102, loss 0.0198983, acc 1
2016-09-06T18:27:44.993399: step 1103, loss 0.157756, acc 0.94
2016-09-06T18:27:45.678833: step 1104, loss 0.0716736, acc 0.94
2016-09-06T18:27:46.349556: step 1105, loss 0.0772965, acc 0.96
2016-09-06T18:27:47.032065: step 1106, loss 0.052817, acc 0.98
2016-09-06T18:27:47.702375: step 1107, loss 0.0849126, acc 0.98
2016-09-06T18:27:48.399571: step 1108, loss 0.170439, acc 0.94
2016-09-06T18:27:49.096395: step 1109, loss 0.173453, acc 0.92
2016-09-06T18:27:49.776437: step 1110, loss 0.164102, acc 0.94
2016-09-06T18:27:50.445676: step 1111, loss 0.119194, acc 0.94
2016-09-06T18:27:51.125867: step 1112, loss 0.0374172, acc 0.98
2016-09-06T18:27:51.807243: step 1113, loss 0.129528, acc 0.96
2016-09-06T18:27:52.469421: step 1114, loss 0.189878, acc 0.92
2016-09-06T18:27:53.164232: step 1115, loss 0.0488684, acc 0.98
2016-09-06T18:27:53.853536: step 1116, loss 0.161519, acc 0.94
2016-09-06T18:27:54.529311: step 1117, loss 0.200837, acc 0.9
2016-09-06T18:27:55.235160: step 1118, loss 0.109484, acc 0.94
2016-09-06T18:27:55.918061: step 1119, loss 0.0904803, acc 0.96
2016-09-06T18:27:56.599452: step 1120, loss 0.139424, acc 0.94
2016-09-06T18:27:57.268794: step 1121, loss 0.116032, acc 0.94
2016-09-06T18:27:57.981878: step 1122, loss 0.115599, acc 0.98
2016-09-06T18:27:58.683952: step 1123, loss 0.197673, acc 0.92
2016-09-06T18:27:59.383379: step 1124, loss 0.0403393, acc 1
2016-09-06T18:28:00.079194: step 1125, loss 0.119957, acc 0.94
2016-09-06T18:28:00.813468: step 1126, loss 0.150909, acc 0.94
2016-09-06T18:28:01.523808: step 1127, loss 0.0404992, acc 1
2016-09-06T18:28:02.202528: step 1128, loss 0.142529, acc 0.94
2016-09-06T18:28:02.881342: step 1129, loss 0.107886, acc 0.92
2016-09-06T18:28:03.560171: step 1130, loss 0.0928758, acc 0.96
2016-09-06T18:28:04.254405: step 1131, loss 0.0280507, acc 1
2016-09-06T18:28:04.949706: step 1132, loss 0.110505, acc 0.94
2016-09-06T18:28:05.623729: step 1133, loss 0.0783877, acc 0.96
2016-09-06T18:28:06.319477: step 1134, loss 0.130781, acc 0.94
2016-09-06T18:28:07.003188: step 1135, loss 0.165659, acc 0.88
2016-09-06T18:28:07.702438: step 1136, loss 0.10039, acc 0.96
2016-09-06T18:28:08.375337: step 1137, loss 0.0797499, acc 0.96
2016-09-06T18:28:09.053267: step 1138, loss 0.249361, acc 0.94
2016-09-06T18:28:09.738735: step 1139, loss 0.0416398, acc 1
2016-09-06T18:28:10.404261: step 1140, loss 0.0972034, acc 0.96
2016-09-06T18:28:11.123435: step 1141, loss 0.0777663, acc 0.96
2016-09-06T18:28:11.780028: step 1142, loss 0.0893333, acc 0.98
2016-09-06T18:28:12.438699: step 1143, loss 0.0322706, acc 1
2016-09-06T18:28:13.115976: step 1144, loss 0.102806, acc 0.96
2016-09-06T18:28:13.808424: step 1145, loss 0.0782204, acc 0.96
2016-09-06T18:28:14.480497: step 1146, loss 0.0585151, acc 1
2016-09-06T18:28:15.208917: step 1147, loss 0.141102, acc 0.96
2016-09-06T18:28:15.923643: step 1148, loss 0.0366761, acc 0.98
2016-09-06T18:28:16.599509: step 1149, loss 0.0649087, acc 1
2016-09-06T18:28:17.288291: step 1150, loss 0.207614, acc 0.92
2016-09-06T18:28:17.975820: step 1151, loss 0.148122, acc 0.92
2016-09-06T18:28:18.605763: step 1152, loss 0.170296, acc 0.931818
2016-09-06T18:28:19.320269: step 1153, loss 0.116265, acc 0.96
2016-09-06T18:28:19.998228: step 1154, loss 0.107174, acc 0.92
2016-09-06T18:28:20.716816: step 1155, loss 0.0565138, acc 1
2016-09-06T18:28:21.400985: step 1156, loss 0.217512, acc 0.94
2016-09-06T18:28:22.074863: step 1157, loss 0.0130253, acc 1
2016-09-06T18:28:22.768206: step 1158, loss 0.00270422, acc 1
2016-09-06T18:28:23.440936: step 1159, loss 0.0704934, acc 0.96
2016-09-06T18:28:24.136172: step 1160, loss 0.129729, acc 0.92
2016-09-06T18:28:24.845393: step 1161, loss 0.0313809, acc 1
2016-09-06T18:28:25.567320: step 1162, loss 0.112869, acc 0.96
2016-09-06T18:28:26.233090: step 1163, loss 0.0920212, acc 0.98
2016-09-06T18:28:26.912130: step 1164, loss 0.0439977, acc 1
2016-09-06T18:28:27.602774: step 1165, loss 0.0926566, acc 0.96
2016-09-06T18:28:28.284727: step 1166, loss 0.102782, acc 0.92
2016-09-06T18:28:28.981443: step 1167, loss 0.171378, acc 0.92
2016-09-06T18:28:29.653491: step 1168, loss 0.0397119, acc 0.98
2016-09-06T18:28:30.343935: step 1169, loss 0.148474, acc 0.94
2016-09-06T18:28:31.027555: step 1170, loss 0.110532, acc 0.96
2016-09-06T18:28:31.711033: step 1171, loss 0.113228, acc 0.98
2016-09-06T18:28:32.402895: step 1172, loss 0.0929475, acc 0.96
2016-09-06T18:28:33.098097: step 1173, loss 0.104818, acc 0.96
2016-09-06T18:28:33.792445: step 1174, loss 0.0604681, acc 0.98
2016-09-06T18:28:34.439638: step 1175, loss 0.108463, acc 0.96
2016-09-06T18:28:35.176136: step 1176, loss 0.0560581, acc 0.98
2016-09-06T18:28:35.865353: step 1177, loss 0.0462902, acc 0.98
2016-09-06T18:28:36.546204: step 1178, loss 0.225856, acc 0.86
2016-09-06T18:28:37.234738: step 1179, loss 0.0587224, acc 0.98
2016-09-06T18:28:37.910858: step 1180, loss 0.0338292, acc 1
2016-09-06T18:28:38.595698: step 1181, loss 0.20458, acc 0.92
2016-09-06T18:28:39.256804: step 1182, loss 0.0871695, acc 0.96
2016-09-06T18:28:39.977179: step 1183, loss 0.0227274, acc 1
2016-09-06T18:28:40.652376: step 1184, loss 0.0985785, acc 0.94
2016-09-06T18:28:41.321819: step 1185, loss 0.0170969, acc 1
2016-09-06T18:28:42.020640: step 1186, loss 0.138261, acc 0.96
2016-09-06T18:28:42.706370: step 1187, loss 0.139965, acc 0.94
2016-09-06T18:28:43.390375: step 1188, loss 0.0470444, acc 0.98
2016-09-06T18:28:44.081436: step 1189, loss 0.0330962, acc 1
2016-09-06T18:28:44.793588: step 1190, loss 0.13298, acc 0.92
2016-09-06T18:28:45.473661: step 1191, loss 0.0240841, acc 1
2016-09-06T18:28:46.164769: step 1192, loss 0.15993, acc 0.94
2016-09-06T18:28:46.862731: step 1193, loss 0.158669, acc 0.9
2016-09-06T18:28:47.543320: step 1194, loss 0.0569389, acc 1
2016-09-06T18:28:48.237689: step 1195, loss 0.0787479, acc 0.98
2016-09-06T18:28:48.936896: step 1196, loss 0.0715363, acc 0.94
2016-09-06T18:28:49.648362: step 1197, loss 0.132667, acc 0.92
2016-09-06T18:28:50.339020: step 1198, loss 0.0777566, acc 0.96
2016-09-06T18:28:51.023181: step 1199, loss 0.138353, acc 0.96
2016-09-06T18:28:51.719254: step 1200, loss 0.0669071, acc 0.96

Evaluation:
2016-09-06T18:28:54.839658: step 1200, loss 0.809155, acc 0.761726

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-1200

2016-09-06T18:28:56.526721: step 1201, loss 0.0629326, acc 0.98
2016-09-06T18:28:57.245204: step 1202, loss 0.0425224, acc 0.98
2016-09-06T18:28:57.961045: step 1203, loss 0.0194181, acc 1
2016-09-06T18:28:58.640638: step 1204, loss 0.0865194, acc 0.98
2016-09-06T18:28:59.328321: step 1205, loss 0.0692919, acc 0.96
2016-09-06T18:29:00.009960: step 1206, loss 0.0300729, acc 0.98
2016-09-06T18:29:00.718017: step 1207, loss 0.124365, acc 0.98
2016-09-06T18:29:01.382157: step 1208, loss 0.072511, acc 0.96
2016-09-06T18:29:02.057325: step 1209, loss 0.0999475, acc 0.98
2016-09-06T18:29:02.751983: step 1210, loss 0.0610895, acc 0.96
2016-09-06T18:29:03.416131: step 1211, loss 0.0289555, acc 1
2016-09-06T18:29:04.129536: step 1212, loss 0.0344228, acc 0.98
2016-09-06T18:29:04.782651: step 1213, loss 0.0601962, acc 0.98
2016-09-06T18:29:05.467157: step 1214, loss 0.0379035, acc 0.98
2016-09-06T18:29:06.150926: step 1215, loss 0.0434807, acc 0.98
2016-09-06T18:29:06.887582: step 1216, loss 0.136555, acc 0.96
2016-09-06T18:29:07.578390: step 1217, loss 0.0612214, acc 0.98
2016-09-06T18:29:08.236389: step 1218, loss 0.0836285, acc 0.96
2016-09-06T18:29:08.937254: step 1219, loss 0.0362019, acc 1
2016-09-06T18:29:09.606172: step 1220, loss 0.133227, acc 0.98
2016-09-06T18:29:10.288504: step 1221, loss 0.091432, acc 0.96
2016-09-06T18:29:10.981810: step 1222, loss 0.0382955, acc 1
2016-09-06T18:29:11.677659: step 1223, loss 0.0627924, acc 0.98
2016-09-06T18:29:12.357758: step 1224, loss 0.0872412, acc 0.98
2016-09-06T18:29:13.008705: step 1225, loss 0.103394, acc 0.96
2016-09-06T18:29:13.723051: step 1226, loss 0.057038, acc 0.96
2016-09-06T18:29:14.415054: step 1227, loss 0.0705848, acc 0.96
2016-09-06T18:29:15.097192: step 1228, loss 0.0403002, acc 0.98
2016-09-06T18:29:15.776009: step 1229, loss 0.238694, acc 0.94
2016-09-06T18:29:16.453609: step 1230, loss 0.0615434, acc 0.96
2016-09-06T18:29:17.158807: step 1231, loss 0.0394706, acc 0.98
2016-09-06T18:29:17.835984: step 1232, loss 0.110943, acc 0.94
2016-09-06T18:29:18.542935: step 1233, loss 0.0746979, acc 0.96
2016-09-06T18:29:19.214131: step 1234, loss 0.053201, acc 0.96
2016-09-06T18:29:19.906941: step 1235, loss 0.0726923, acc 0.96
2016-09-06T18:29:20.604830: step 1236, loss 0.0836266, acc 0.98
2016-09-06T18:29:21.290030: step 1237, loss 0.0200824, acc 1
2016-09-06T18:29:21.979418: step 1238, loss 0.197938, acc 0.96
2016-09-06T18:29:22.651052: step 1239, loss 0.0223019, acc 1
2016-09-06T18:29:23.344734: step 1240, loss 0.0890559, acc 0.96
2016-09-06T18:29:24.021457: step 1241, loss 0.132506, acc 0.94
2016-09-06T18:29:24.693063: step 1242, loss 0.0797703, acc 0.98
2016-09-06T18:29:25.366256: step 1243, loss 0.201922, acc 0.92
2016-09-06T18:29:26.065464: step 1244, loss 0.105118, acc 0.96
2016-09-06T18:29:26.738891: step 1245, loss 0.0754396, acc 0.98
2016-09-06T18:29:27.429332: step 1246, loss 0.0787741, acc 0.96
2016-09-06T18:29:28.143553: step 1247, loss 0.0737263, acc 0.98
2016-09-06T18:29:28.806144: step 1248, loss 0.0422978, acc 1
2016-09-06T18:29:29.500915: step 1249, loss 0.0561807, acc 0.98
2016-09-06T18:29:30.199684: step 1250, loss 0.0349561, acc 0.98
2016-09-06T18:29:30.897165: step 1251, loss 0.0607609, acc 0.98
2016-09-06T18:29:31.605083: step 1252, loss 0.0841357, acc 0.94
2016-09-06T18:29:32.288673: step 1253, loss 0.144473, acc 0.92
2016-09-06T18:29:32.984993: step 1254, loss 0.0179864, acc 1
2016-09-06T18:29:33.694510: step 1255, loss 0.0985106, acc 0.94
2016-09-06T18:29:34.390450: step 1256, loss 0.0863441, acc 0.92
2016-09-06T18:29:35.070052: step 1257, loss 0.0817521, acc 0.96
2016-09-06T18:29:35.756114: step 1258, loss 0.0894355, acc 0.96
2016-09-06T18:29:36.432361: step 1259, loss 0.0489405, acc 0.98
2016-09-06T18:29:37.109235: step 1260, loss 0.0984634, acc 0.96
2016-09-06T18:29:37.816860: step 1261, loss 0.137356, acc 0.94
2016-09-06T18:29:38.510599: step 1262, loss 0.0307017, acc 1
2016-09-06T18:29:39.201300: step 1263, loss 0.210065, acc 0.94
2016-09-06T18:29:39.894470: step 1264, loss 0.0555086, acc 1
2016-09-06T18:29:40.585486: step 1265, loss 0.0899035, acc 0.98
2016-09-06T18:29:41.287095: step 1266, loss 0.0618546, acc 0.98
2016-09-06T18:29:41.967550: step 1267, loss 0.227244, acc 0.92
2016-09-06T18:29:42.703800: step 1268, loss 0.139084, acc 0.92
2016-09-06T18:29:43.390212: step 1269, loss 0.213801, acc 0.9
2016-09-06T18:29:44.106465: step 1270, loss 0.0557115, acc 0.96
2016-09-06T18:29:44.815033: step 1271, loss 0.122513, acc 0.92
2016-09-06T18:29:45.480777: step 1272, loss 0.0823823, acc 0.96
2016-09-06T18:29:46.173238: step 1273, loss 0.11593, acc 0.96
2016-09-06T18:29:46.839492: step 1274, loss 0.0468056, acc 0.96
2016-09-06T18:29:47.542674: step 1275, loss 0.0731113, acc 0.94
2016-09-06T18:29:48.228272: step 1276, loss 0.159725, acc 0.94
2016-09-06T18:29:48.926134: step 1277, loss 0.0662381, acc 0.96
2016-09-06T18:29:49.618525: step 1278, loss 0.0799594, acc 0.96
2016-09-06T18:29:50.284841: step 1279, loss 0.0601311, acc 0.98
2016-09-06T18:29:51.022495: step 1280, loss 0.0653487, acc 0.96
2016-09-06T18:29:51.738256: step 1281, loss 0.117823, acc 0.94
2016-09-06T18:29:52.418044: step 1282, loss 0.0519276, acc 0.98
2016-09-06T18:29:53.100762: step 1283, loss 0.152283, acc 0.94
2016-09-06T18:29:53.795939: step 1284, loss 0.0570743, acc 0.98
2016-09-06T18:29:54.495684: step 1285, loss 0.0969411, acc 0.92
2016-09-06T18:29:55.159499: step 1286, loss 0.0161235, acc 1
2016-09-06T18:29:55.843134: step 1287, loss 0.0347127, acc 1
2016-09-06T18:29:56.514754: step 1288, loss 0.0681317, acc 0.96
2016-09-06T18:29:57.229161: step 1289, loss 0.102918, acc 0.94
2016-09-06T18:29:57.920001: step 1290, loss 0.0321335, acc 1
2016-09-06T18:29:58.610950: step 1291, loss 0.0515114, acc 0.98
2016-09-06T18:29:59.312607: step 1292, loss 0.0644287, acc 0.98
2016-09-06T18:29:59.980577: step 1293, loss 0.0606999, acc 0.98
2016-09-06T18:30:00.734837: step 1294, loss 0.0657426, acc 0.98
2016-09-06T18:30:01.420140: step 1295, loss 0.0392205, acc 0.98
2016-09-06T18:30:02.110968: step 1296, loss 0.234851, acc 0.94
2016-09-06T18:30:02.807866: step 1297, loss 0.0607394, acc 0.98
2016-09-06T18:30:03.501563: step 1298, loss 0.0394662, acc 0.98
2016-09-06T18:30:04.205046: step 1299, loss 0.0902868, acc 0.96
2016-09-06T18:30:04.869142: step 1300, loss 0.119857, acc 0.94

Evaluation:
2016-09-06T18:30:08.010512: step 1300, loss 1.0466, acc 0.770169

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-1300

2016-09-06T18:30:09.672633: step 1301, loss 0.102241, acc 0.94
2016-09-06T18:30:10.373850: step 1302, loss 0.00579034, acc 1
2016-09-06T18:30:11.075141: step 1303, loss 0.04359, acc 0.98
2016-09-06T18:30:11.769813: step 1304, loss 0.0959517, acc 0.98
2016-09-06T18:30:12.452539: step 1305, loss 0.0201121, acc 1
2016-09-06T18:30:13.129350: step 1306, loss 0.0535457, acc 0.96
2016-09-06T18:30:13.827764: step 1307, loss 0.178118, acc 0.96
2016-09-06T18:30:14.530388: step 1308, loss 0.0476091, acc 0.98
2016-09-06T18:30:15.215918: step 1309, loss 0.115184, acc 0.94
2016-09-06T18:30:15.919157: step 1310, loss 0.118671, acc 0.94
2016-09-06T18:30:16.611461: step 1311, loss 0.0889741, acc 0.96
2016-09-06T18:30:17.305624: step 1312, loss 0.0919766, acc 0.94
2016-09-06T18:30:17.971100: step 1313, loss 0.11532, acc 0.94
2016-09-06T18:30:18.679856: step 1314, loss 0.19265, acc 0.94
2016-09-06T18:30:19.385567: step 1315, loss 0.0871303, acc 0.96
2016-09-06T18:30:20.075154: step 1316, loss 0.088822, acc 0.96
2016-09-06T18:30:20.755395: step 1317, loss 0.0806491, acc 0.98
2016-09-06T18:30:21.422451: step 1318, loss 0.134003, acc 0.88
2016-09-06T18:30:22.119738: step 1319, loss 0.0339212, acc 1
2016-09-06T18:30:22.785526: step 1320, loss 0.0765314, acc 0.94
2016-09-06T18:30:23.485438: step 1321, loss 0.0671604, acc 0.96
2016-09-06T18:30:24.166534: step 1322, loss 0.0909065, acc 0.96
2016-09-06T18:30:24.830808: step 1323, loss 0.102959, acc 0.94
2016-09-06T18:30:25.527413: step 1324, loss 0.0600067, acc 0.98
2016-09-06T18:30:26.228907: step 1325, loss 0.166412, acc 0.92
2016-09-06T18:30:26.917580: step 1326, loss 0.0366051, acc 1
2016-09-06T18:30:27.603037: step 1327, loss 0.083398, acc 0.96
2016-09-06T18:30:28.304701: step 1328, loss 0.119979, acc 0.94
2016-09-06T18:30:29.002764: step 1329, loss 0.0605537, acc 0.98
2016-09-06T18:30:29.689561: step 1330, loss 0.0651063, acc 1
2016-09-06T18:30:30.380194: step 1331, loss 0.0558677, acc 1
2016-09-06T18:30:31.071589: step 1332, loss 0.0264775, acc 1
2016-09-06T18:30:31.769082: step 1333, loss 0.0564133, acc 0.98
2016-09-06T18:30:32.451265: step 1334, loss 0.0907939, acc 0.94
2016-09-06T18:30:33.142897: step 1335, loss 0.0478281, acc 0.98
2016-09-06T18:30:33.825482: step 1336, loss 0.0549659, acc 0.96
2016-09-06T18:30:34.511597: step 1337, loss 0.0575379, acc 0.98
2016-09-06T18:30:35.184841: step 1338, loss 0.0811583, acc 0.96
2016-09-06T18:30:35.879782: step 1339, loss 0.0468604, acc 1
2016-09-06T18:30:36.558425: step 1340, loss 0.0540088, acc 0.98
2016-09-06T18:30:37.213134: step 1341, loss 0.0792884, acc 0.96
2016-09-06T18:30:37.933708: step 1342, loss 0.154747, acc 0.94
2016-09-06T18:30:38.591872: step 1343, loss 0.162158, acc 0.9
2016-09-06T18:30:39.227286: step 1344, loss 0.0487479, acc 0.977273
2016-09-06T18:30:39.915563: step 1345, loss 0.0638243, acc 0.96
2016-09-06T18:30:40.613583: step 1346, loss 0.111786, acc 0.92
2016-09-06T18:30:41.324071: step 1347, loss 0.0273696, acc 1
2016-09-06T18:30:42.025523: step 1348, loss 0.20196, acc 0.9
2016-09-06T18:30:42.718124: step 1349, loss 0.0591256, acc 0.98
2016-09-06T18:30:43.400918: step 1350, loss 0.0683238, acc 0.98
2016-09-06T18:30:44.086564: step 1351, loss 0.113029, acc 0.94
2016-09-06T18:30:44.763634: step 1352, loss 0.193289, acc 0.9
2016-09-06T18:30:45.449872: step 1353, loss 0.0596849, acc 0.98
2016-09-06T18:30:46.133733: step 1354, loss 0.0576752, acc 0.98
2016-09-06T18:30:46.814724: step 1355, loss 0.128727, acc 0.94
2016-09-06T18:30:47.528110: step 1356, loss 0.118837, acc 0.96
2016-09-06T18:30:48.206865: step 1357, loss 0.0725169, acc 0.98
2016-09-06T18:30:48.897377: step 1358, loss 0.0220732, acc 1
2016-09-06T18:30:49.576478: step 1359, loss 0.0359617, acc 0.98
2016-09-06T18:30:50.264752: step 1360, loss 0.0437238, acc 1
2016-09-06T18:30:50.967415: step 1361, loss 0.0780971, acc 0.98
2016-09-06T18:30:51.648151: step 1362, loss 0.00341465, acc 1
2016-09-06T18:30:52.341844: step 1363, loss 0.127303, acc 0.96
2016-09-06T18:30:53.031678: step 1364, loss 0.0586997, acc 0.96
2016-09-06T18:30:53.728958: step 1365, loss 0.0742315, acc 0.98
2016-09-06T18:30:54.407179: step 1366, loss 0.0569029, acc 0.98
2016-09-06T18:30:55.094036: step 1367, loss 0.0275769, acc 1
2016-09-06T18:30:55.786822: step 1368, loss 0.10218, acc 0.92
2016-09-06T18:30:56.452904: step 1369, loss 0.192713, acc 0.92
2016-09-06T18:30:57.151996: step 1370, loss 0.0906021, acc 0.94
2016-09-06T18:30:57.825413: step 1371, loss 0.0212768, acc 1
2016-09-06T18:30:58.505076: step 1372, loss 0.0734103, acc 0.96
2016-09-06T18:30:59.212067: step 1373, loss 0.0400102, acc 1
2016-09-06T18:30:59.898306: step 1374, loss 0.0611095, acc 0.96
2016-09-06T18:31:00.635785: step 1375, loss 0.0369724, acc 0.98
2016-09-06T18:31:01.302423: step 1376, loss 0.172936, acc 0.94
2016-09-06T18:31:01.982448: step 1377, loss 0.192176, acc 0.94
2016-09-06T18:31:02.665549: step 1378, loss 0.0496249, acc 1
2016-09-06T18:31:03.353603: step 1379, loss 0.0626416, acc 0.98
2016-09-06T18:31:04.042309: step 1380, loss 0.0525558, acc 0.96
2016-09-06T18:31:04.719939: step 1381, loss 0.0214511, acc 1
2016-09-06T18:31:05.404811: step 1382, loss 0.151854, acc 0.94
2016-09-06T18:31:06.058222: step 1383, loss 0.0139146, acc 1
2016-09-06T18:31:06.756821: step 1384, loss 0.159051, acc 0.92
2016-09-06T18:31:07.431550: step 1385, loss 0.117569, acc 0.96
2016-09-06T18:31:08.111629: step 1386, loss 0.0573972, acc 0.98
2016-09-06T18:31:08.782567: step 1387, loss 0.0998356, acc 0.96
2016-09-06T18:31:09.470386: step 1388, loss 0.0970836, acc 0.96
2016-09-06T18:31:10.155738: step 1389, loss 0.128771, acc 0.94
2016-09-06T18:31:10.838679: step 1390, loss 0.0991528, acc 0.96
2016-09-06T18:31:11.552974: step 1391, loss 0.0449751, acc 0.98
2016-09-06T18:31:12.218855: step 1392, loss 0.0522579, acc 0.98
2016-09-06T18:31:12.902550: step 1393, loss 0.0904308, acc 0.98
2016-09-06T18:31:13.566778: step 1394, loss 0.102256, acc 0.96
2016-09-06T18:31:14.234831: step 1395, loss 0.0486984, acc 1
2016-09-06T18:31:14.927221: step 1396, loss 0.101549, acc 0.96
2016-09-06T18:31:15.627282: step 1397, loss 0.101811, acc 0.94
2016-09-06T18:31:16.327224: step 1398, loss 0.0512505, acc 1
2016-09-06T18:31:17.008576: step 1399, loss 0.110437, acc 0.92
2016-09-06T18:31:17.689330: step 1400, loss 0.0492104, acc 0.98

Evaluation:
2016-09-06T18:31:20.831259: step 1400, loss 0.874464, acc 0.768293

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-1400

2016-09-06T18:31:22.466211: step 1401, loss 0.0220164, acc 1
2016-09-06T18:31:23.135202: step 1402, loss 0.0297212, acc 0.98
2016-09-06T18:31:23.806675: step 1403, loss 0.0896154, acc 0.94
2016-09-06T18:31:24.521565: step 1404, loss 0.0439472, acc 0.96
2016-09-06T18:31:25.214768: step 1405, loss 0.0323633, acc 0.98
2016-09-06T18:31:25.927847: step 1406, loss 0.049395, acc 0.98
2016-09-06T18:31:26.595817: step 1407, loss 0.0633127, acc 0.96
2016-09-06T18:31:27.301770: step 1408, loss 0.077251, acc 0.96
2016-09-06T18:31:27.991760: step 1409, loss 0.0476909, acc 0.98
2016-09-06T18:31:28.669904: step 1410, loss 0.0270703, acc 0.98
2016-09-06T18:31:29.347928: step 1411, loss 0.0126857, acc 1
2016-09-06T18:31:30.046562: step 1412, loss 0.103543, acc 0.96
2016-09-06T18:31:30.732401: step 1413, loss 0.113003, acc 0.94
2016-09-06T18:31:31.394016: step 1414, loss 0.119672, acc 0.94
2016-09-06T18:31:32.115467: step 1415, loss 0.0775248, acc 0.94
2016-09-06T18:31:32.791621: step 1416, loss 0.0643277, acc 0.96
2016-09-06T18:31:33.495320: step 1417, loss 0.0698297, acc 0.98
2016-09-06T18:31:34.186294: step 1418, loss 0.0805779, acc 0.98
2016-09-06T18:31:34.852837: step 1419, loss 0.0183001, acc 1
2016-09-06T18:31:35.562001: step 1420, loss 0.103867, acc 0.98
2016-09-06T18:31:36.239127: step 1421, loss 0.0788965, acc 0.98
2016-09-06T18:31:36.932814: step 1422, loss 0.0013135, acc 1
2016-09-06T18:31:37.636181: step 1423, loss 0.0303145, acc 0.98
2016-09-06T18:31:38.340267: step 1424, loss 0.096201, acc 0.92
2016-09-06T18:31:39.046841: step 1425, loss 0.0547452, acc 0.98
2016-09-06T18:31:39.708209: step 1426, loss 0.0222857, acc 0.98
2016-09-06T18:31:40.409283: step 1427, loss 0.0800402, acc 0.96
2016-09-06T18:31:41.101136: step 1428, loss 0.0537773, acc 0.96
2016-09-06T18:31:41.788576: step 1429, loss 0.0339667, acc 1
2016-09-06T18:31:42.495354: step 1430, loss 0.0750164, acc 0.94
2016-09-06T18:31:43.169013: step 1431, loss 0.0575115, acc 0.98
2016-09-06T18:31:43.876338: step 1432, loss 0.0560994, acc 0.98
2016-09-06T18:31:44.566265: step 1433, loss 0.144971, acc 0.92
2016-09-06T18:31:45.260369: step 1434, loss 0.0559882, acc 0.96
2016-09-06T18:31:45.943741: step 1435, loss 0.00489617, acc 1
2016-09-06T18:31:46.649511: step 1436, loss 0.0701104, acc 0.96
2016-09-06T18:31:47.347922: step 1437, loss 0.0517056, acc 0.98
2016-09-06T18:31:48.068058: step 1438, loss 0.0799456, acc 0.96
2016-09-06T18:31:48.754358: step 1439, loss 0.0976537, acc 0.96
2016-09-06T18:31:49.432155: step 1440, loss 0.0858499, acc 0.94
2016-09-06T18:31:50.157479: step 1441, loss 0.0350985, acc 0.98
2016-09-06T18:31:50.839886: step 1442, loss 0.0791945, acc 0.96
2016-09-06T18:31:51.549535: step 1443, loss 0.145294, acc 0.98
2016-09-06T18:31:52.228227: step 1444, loss 0.014277, acc 1
2016-09-06T18:31:52.905923: step 1445, loss 0.0165957, acc 1
2016-09-06T18:31:53.624395: step 1446, loss 0.0560967, acc 0.96
2016-09-06T18:31:54.290535: step 1447, loss 0.143169, acc 0.94
2016-09-06T18:31:54.965995: step 1448, loss 0.0174813, acc 1
2016-09-06T18:31:55.660926: step 1449, loss 0.170127, acc 0.92
2016-09-06T18:31:56.362713: step 1450, loss 0.074588, acc 0.94
2016-09-06T18:31:57.056023: step 1451, loss 0.103512, acc 0.96
2016-09-06T18:31:57.745483: step 1452, loss 0.0188615, acc 1
2016-09-06T18:31:58.449468: step 1453, loss 0.0648073, acc 0.98
2016-09-06T18:31:59.126104: step 1454, loss 0.127118, acc 0.94
2016-09-06T18:31:59.855517: step 1455, loss 0.0575653, acc 0.98
2016-09-06T18:32:00.596747: step 1456, loss 0.0540841, acc 0.96
2016-09-06T18:32:01.281937: step 1457, loss 0.0431974, acc 0.96
2016-09-06T18:32:01.996581: step 1458, loss 0.11057, acc 0.92
2016-09-06T18:32:02.665385: step 1459, loss 0.0886096, acc 0.94
2016-09-06T18:32:03.377022: step 1460, loss 0.106491, acc 0.94
2016-09-06T18:32:04.071496: step 1461, loss 0.0390465, acc 0.98
2016-09-06T18:32:04.742206: step 1462, loss 0.040379, acc 0.96
2016-09-06T18:32:05.428593: step 1463, loss 0.071035, acc 0.96
2016-09-06T18:32:06.111983: step 1464, loss 0.0766173, acc 0.98
2016-09-06T18:32:06.795049: step 1465, loss 0.0718502, acc 0.98
2016-09-06T18:32:07.452479: step 1466, loss 0.0181808, acc 1
2016-09-06T18:32:08.175154: step 1467, loss 0.0386537, acc 0.98
2016-09-06T18:32:08.855426: step 1468, loss 0.108556, acc 0.94
2016-09-06T18:32:09.582352: step 1469, loss 0.0780065, acc 0.94
2016-09-06T18:32:10.267462: step 1470, loss 0.145004, acc 0.92
2016-09-06T18:32:10.959428: step 1471, loss 0.16245, acc 0.94
2016-09-06T18:32:11.678599: step 1472, loss 0.0927741, acc 0.96
2016-09-06T18:32:12.365204: step 1473, loss 0.0654502, acc 0.98
2016-09-06T18:32:13.094196: step 1474, loss 0.0406291, acc 0.98
2016-09-06T18:32:13.774047: step 1475, loss 0.0725693, acc 0.96
2016-09-06T18:32:14.482698: step 1476, loss 0.0626588, acc 0.94
2016-09-06T18:32:15.173558: step 1477, loss 0.059607, acc 0.96
2016-09-06T18:32:15.846833: step 1478, loss 0.0404449, acc 0.96
2016-09-06T18:32:16.544252: step 1479, loss 0.0329056, acc 0.98
2016-09-06T18:32:17.246600: step 1480, loss 0.0525842, acc 0.98
2016-09-06T18:32:17.940167: step 1481, loss 0.0614648, acc 0.98
2016-09-06T18:32:18.631945: step 1482, loss 0.163669, acc 0.94
2016-09-06T18:32:19.323123: step 1483, loss 0.0321806, acc 1
2016-09-06T18:32:20.036876: step 1484, loss 0.0826087, acc 0.96
2016-09-06T18:32:20.744629: step 1485, loss 0.105015, acc 0.96
2016-09-06T18:32:21.434012: step 1486, loss 0.11909, acc 0.92
2016-09-06T18:32:22.130303: step 1487, loss 0.0443329, acc 0.98
2016-09-06T18:32:22.818911: step 1488, loss 0.081658, acc 0.94
2016-09-06T18:32:23.518649: step 1489, loss 0.0341645, acc 0.98
2016-09-06T18:32:24.202107: step 1490, loss 0.0692202, acc 0.98
2016-09-06T18:32:24.901742: step 1491, loss 0.271809, acc 0.92
2016-09-06T18:32:25.579432: step 1492, loss 0.0748, acc 0.98
2016-09-06T18:32:26.270206: step 1493, loss 0.0590239, acc 0.96
2016-09-06T18:32:26.958796: step 1494, loss 0.0576772, acc 0.98
2016-09-06T18:32:27.623521: step 1495, loss 0.135754, acc 0.98
2016-09-06T18:32:28.296074: step 1496, loss 0.0421911, acc 1
2016-09-06T18:32:28.982057: step 1497, loss 0.0236837, acc 1
2016-09-06T18:32:29.721851: step 1498, loss 0.0654296, acc 0.96
2016-09-06T18:32:30.391067: step 1499, loss 0.136158, acc 0.94
2016-09-06T18:32:31.065591: step 1500, loss 0.0546908, acc 0.96

Evaluation:
2016-09-06T18:32:34.200319: step 1500, loss 1.00374, acc 0.77955

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-1500

2016-09-06T18:32:35.926411: step 1501, loss 0.110836, acc 0.96
2016-09-06T18:32:36.627111: step 1502, loss 0.118027, acc 0.94
2016-09-06T18:32:37.302247: step 1503, loss 0.0608323, acc 0.96
2016-09-06T18:32:37.987986: step 1504, loss 0.0804197, acc 0.96
2016-09-06T18:32:38.651217: step 1505, loss 0.00361298, acc 1
2016-09-06T18:32:39.368570: step 1506, loss 0.0890244, acc 0.94
2016-09-06T18:32:40.058265: step 1507, loss 0.0795329, acc 0.92
2016-09-06T18:32:40.745627: step 1508, loss 0.0172983, acc 1
2016-09-06T18:32:41.425713: step 1509, loss 0.0108394, acc 1
2016-09-06T18:32:42.102615: step 1510, loss 0.0266142, acc 0.98
2016-09-06T18:32:42.786320: step 1511, loss 0.021567, acc 1
2016-09-06T18:32:43.464468: step 1512, loss 0.113868, acc 0.9
2016-09-06T18:32:44.175367: step 1513, loss 0.197193, acc 0.96
2016-09-06T18:32:44.858915: step 1514, loss 0.0553731, acc 0.96
2016-09-06T18:32:45.562619: step 1515, loss 0.0821514, acc 0.96
2016-09-06T18:32:46.264967: step 1516, loss 0.141094, acc 0.96
2016-09-06T18:32:46.961683: step 1517, loss 0.0314528, acc 0.98
2016-09-06T18:32:47.649516: step 1518, loss 0.228465, acc 0.9
2016-09-06T18:32:48.333107: step 1519, loss 0.133789, acc 0.96
2016-09-06T18:32:49.042939: step 1520, loss 0.0985524, acc 0.96
2016-09-06T18:32:49.725532: step 1521, loss 0.080901, acc 0.94
2016-09-06T18:32:50.391916: step 1522, loss 0.10159, acc 0.94
2016-09-06T18:32:51.087149: step 1523, loss 0.0829787, acc 0.98
2016-09-06T18:32:51.773644: step 1524, loss 0.0422971, acc 0.98
2016-09-06T18:32:52.471120: step 1525, loss 0.038317, acc 1
2016-09-06T18:32:53.147152: step 1526, loss 0.0473082, acc 0.96
2016-09-06T18:32:53.863526: step 1527, loss 0.0342745, acc 1
2016-09-06T18:32:54.548347: step 1528, loss 0.137262, acc 0.92
2016-09-06T18:32:55.242652: step 1529, loss 0.0368279, acc 0.98
2016-09-06T18:32:55.914750: step 1530, loss 0.161623, acc 0.92
2016-09-06T18:32:56.615847: step 1531, loss 0.0743057, acc 0.96
2016-09-06T18:32:57.313997: step 1532, loss 0.0563878, acc 0.98
2016-09-06T18:32:57.989009: step 1533, loss 0.084792, acc 0.96
2016-09-06T18:32:58.694392: step 1534, loss 0.107388, acc 0.92
2016-09-06T18:32:59.391676: step 1535, loss 0.0326117, acc 1
2016-09-06T18:33:00.023062: step 1536, loss 0.137962, acc 0.931818
2016-09-06T18:33:00.745922: step 1537, loss 0.130481, acc 0.94
2016-09-06T18:33:01.425902: step 1538, loss 0.0361886, acc 0.98
2016-09-06T18:33:02.116098: step 1539, loss 0.0832152, acc 0.96
2016-09-06T18:33:02.789575: step 1540, loss 0.0569317, acc 0.98
2016-09-06T18:33:03.505219: step 1541, loss 0.0875894, acc 0.96
2016-09-06T18:33:04.191909: step 1542, loss 0.00839265, acc 1
2016-09-06T18:33:04.893764: step 1543, loss 0.0320458, acc 1
2016-09-06T18:33:05.581314: step 1544, loss 0.21362, acc 0.92
2016-09-06T18:33:06.269143: step 1545, loss 0.124669, acc 0.94
2016-09-06T18:33:06.970038: step 1546, loss 0.0397726, acc 0.98
2016-09-06T18:33:07.655149: step 1547, loss 0.0930813, acc 0.94
2016-09-06T18:33:08.334285: step 1548, loss 0.0773666, acc 0.96
2016-09-06T18:33:09.035487: step 1549, loss 0.0270224, acc 0.98
2016-09-06T18:33:09.730205: step 1550, loss 0.026393, acc 1
2016-09-06T18:33:10.411023: step 1551, loss 0.150859, acc 0.96
2016-09-06T18:33:11.100613: step 1552, loss 0.0130488, acc 1
2016-09-06T18:33:11.806167: step 1553, loss 0.0886985, acc 0.94
2016-09-06T18:33:12.477518: step 1554, loss 0.134044, acc 0.96
2016-09-06T18:33:13.153633: step 1555, loss 0.0561164, acc 0.96
2016-09-06T18:33:13.850613: step 1556, loss 0.0756288, acc 0.96
2016-09-06T18:33:14.546035: step 1557, loss 0.0381169, acc 1
2016-09-06T18:33:15.235115: step 1558, loss 0.034521, acc 0.98
2016-09-06T18:33:15.923539: step 1559, loss 0.0522849, acc 0.96
2016-09-06T18:33:16.613697: step 1560, loss 0.105683, acc 0.96
2016-09-06T18:33:17.273594: step 1561, loss 0.0591997, acc 0.98
2016-09-06T18:33:17.939546: step 1562, loss 0.132663, acc 0.9
2016-09-06T18:33:18.630257: step 1563, loss 0.0581998, acc 1
2016-09-06T18:33:19.311605: step 1564, loss 0.0372113, acc 0.98
2016-09-06T18:33:20.013339: step 1565, loss 0.076838, acc 0.96
2016-09-06T18:33:20.692672: step 1566, loss 0.104321, acc 0.98
2016-09-06T18:33:21.394491: step 1567, loss 0.0290877, acc 0.98
2016-09-06T18:33:22.083606: step 1568, loss 0.0150668, acc 1
2016-09-06T18:33:22.788692: step 1569, loss 0.0258955, acc 1
2016-09-06T18:33:23.479599: step 1570, loss 0.0910453, acc 0.94
2016-09-06T18:33:24.160418: step 1571, loss 0.0451425, acc 0.96
2016-09-06T18:33:24.851066: step 1572, loss 0.090794, acc 0.94
2016-09-06T18:33:25.527994: step 1573, loss 0.00491994, acc 1
2016-09-06T18:33:26.223484: step 1574, loss 0.0310732, acc 1
2016-09-06T18:33:26.888175: step 1575, loss 0.0235489, acc 1
2016-09-06T18:33:27.561891: step 1576, loss 0.0762139, acc 0.96
2016-09-06T18:33:28.245956: step 1577, loss 0.100886, acc 0.96
2016-09-06T18:33:28.934750: step 1578, loss 0.0256942, acc 0.98
2016-09-06T18:33:29.635508: step 1579, loss 0.0795476, acc 0.96
2016-09-06T18:33:30.330979: step 1580, loss 0.00990156, acc 1
2016-09-06T18:33:31.033901: step 1581, loss 0.0909281, acc 0.98
2016-09-06T18:33:31.719873: step 1582, loss 0.0150609, acc 1
2016-09-06T18:33:32.412003: step 1583, loss 0.0302222, acc 0.98
2016-09-06T18:33:33.101504: step 1584, loss 0.050288, acc 0.98
2016-09-06T18:33:33.783684: step 1585, loss 0.0608827, acc 0.98
2016-09-06T18:33:34.467980: step 1586, loss 0.14329, acc 0.96
2016-09-06T18:33:35.159361: step 1587, loss 0.200529, acc 0.94
2016-09-06T18:33:35.867386: step 1588, loss 0.0584584, acc 0.96
2016-09-06T18:33:36.553115: step 1589, loss 0.00723303, acc 1
2016-09-06T18:33:37.252458: step 1590, loss 0.151332, acc 0.94
2016-09-06T18:33:37.925305: step 1591, loss 0.064967, acc 0.96
2016-09-06T18:33:38.602849: step 1592, loss 0.0456921, acc 0.96
2016-09-06T18:33:39.302929: step 1593, loss 0.0400623, acc 0.98
2016-09-06T18:33:39.999385: step 1594, loss 0.101612, acc 0.96
2016-09-06T18:33:40.715444: step 1595, loss 0.056638, acc 0.96
2016-09-06T18:33:41.414437: step 1596, loss 0.00841595, acc 1
2016-09-06T18:33:42.102045: step 1597, loss 0.0372142, acc 0.98
2016-09-06T18:33:42.777479: step 1598, loss 0.0320332, acc 0.98
2016-09-06T18:33:43.442491: step 1599, loss 0.115115, acc 0.94
2016-09-06T18:33:44.144013: step 1600, loss 0.0480408, acc 0.98

Evaluation:
2016-09-06T18:33:47.251189: step 1600, loss 1.053, acc 0.762664

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-1600

2016-09-06T18:33:48.952383: step 1601, loss 0.0648328, acc 0.98
2016-09-06T18:33:49.623812: step 1602, loss 0.0538496, acc 0.96
2016-09-06T18:33:50.334862: step 1603, loss 0.0403386, acc 0.98
2016-09-06T18:33:51.038897: step 1604, loss 0.191744, acc 0.94
2016-09-06T18:33:51.724142: step 1605, loss 0.115173, acc 0.96
2016-09-06T18:33:52.403040: step 1606, loss 0.0429317, acc 0.98
2016-09-06T18:33:53.079754: step 1607, loss 0.0875823, acc 0.98
2016-09-06T18:33:53.761086: step 1608, loss 0.0665766, acc 0.98
2016-09-06T18:33:54.460971: step 1609, loss 0.0276213, acc 1
2016-09-06T18:33:55.168050: step 1610, loss 0.0421083, acc 0.98
2016-09-06T18:33:55.867515: step 1611, loss 0.0542251, acc 0.96
2016-09-06T18:33:56.551025: step 1612, loss 0.0278054, acc 0.98
2016-09-06T18:33:57.243260: step 1613, loss 0.0765456, acc 0.96
2016-09-06T18:33:57.923317: step 1614, loss 0.0343001, acc 1
2016-09-06T18:33:58.624547: step 1615, loss 0.0411288, acc 0.98
2016-09-06T18:33:59.278360: step 1616, loss 0.138393, acc 0.96
2016-09-06T18:33:59.973854: step 1617, loss 0.063902, acc 0.98
2016-09-06T18:34:00.721852: step 1618, loss 0.0847314, acc 0.96
2016-09-06T18:34:01.435551: step 1619, loss 0.0470483, acc 0.98
2016-09-06T18:34:02.117870: step 1620, loss 0.0175072, acc 1
2016-09-06T18:34:02.796582: step 1621, loss 0.0405962, acc 0.98
2016-09-06T18:34:03.497356: step 1622, loss 0.0528503, acc 0.98
2016-09-06T18:34:04.172933: step 1623, loss 0.103025, acc 0.96
2016-09-06T18:34:04.850364: step 1624, loss 0.074557, acc 0.94
2016-09-06T18:34:05.539143: step 1625, loss 0.114928, acc 0.9
2016-09-06T18:34:06.213487: step 1626, loss 0.0889859, acc 0.96
2016-09-06T18:34:06.903852: step 1627, loss 0.0913623, acc 0.98
2016-09-06T18:34:07.587643: step 1628, loss 0.0412873, acc 0.98
2016-09-06T18:34:08.292945: step 1629, loss 0.0229781, acc 0.98
2016-09-06T18:34:08.963209: step 1630, loss 0.0538478, acc 0.96
2016-09-06T18:34:09.648306: step 1631, loss 0.0791947, acc 0.98
2016-09-06T18:34:10.325099: step 1632, loss 0.0620941, acc 0.98
2016-09-06T18:34:11.022884: step 1633, loss 0.159818, acc 0.94
2016-09-06T18:34:11.710165: step 1634, loss 0.218696, acc 0.94
2016-09-06T18:34:12.412648: step 1635, loss 0.0633039, acc 0.98
2016-09-06T18:34:13.119056: step 1636, loss 0.0937224, acc 0.98
2016-09-06T18:34:13.799252: step 1637, loss 0.0407202, acc 0.98
2016-09-06T18:34:14.472835: step 1638, loss 0.0211864, acc 1
2016-09-06T18:34:15.156809: step 1639, loss 0.0387617, acc 0.98
2016-09-06T18:34:15.843133: step 1640, loss 0.0711716, acc 0.96
2016-09-06T18:34:16.538694: step 1641, loss 0.05322, acc 0.98
2016-09-06T18:34:17.235326: step 1642, loss 0.100942, acc 0.96
2016-09-06T18:34:17.967430: step 1643, loss 0.0190353, acc 1
2016-09-06T18:34:18.676397: step 1644, loss 0.016402, acc 1
2016-09-06T18:34:19.368437: step 1645, loss 0.0618966, acc 0.96
2016-09-06T18:34:20.053827: step 1646, loss 0.137167, acc 0.94
2016-09-06T18:34:20.745202: step 1647, loss 0.0521875, acc 0.96
2016-09-06T18:34:21.451570: step 1648, loss 0.125105, acc 0.96
2016-09-06T18:34:22.110182: step 1649, loss 0.0229722, acc 0.98
2016-09-06T18:34:22.804613: step 1650, loss 0.110704, acc 0.96
2016-09-06T18:34:23.489494: step 1651, loss 0.102367, acc 0.94
2016-09-06T18:34:24.177879: step 1652, loss 0.024915, acc 1
2016-09-06T18:34:24.883522: step 1653, loss 0.0207389, acc 0.98
2016-09-06T18:34:25.571480: step 1654, loss 0.0792462, acc 0.96
2016-09-06T18:34:26.263794: step 1655, loss 0.0742097, acc 0.96
2016-09-06T18:34:26.926025: step 1656, loss 0.0538129, acc 0.96
2016-09-06T18:34:27.649228: step 1657, loss 0.078593, acc 0.96
2016-09-06T18:34:28.369237: step 1658, loss 0.053319, acc 0.94
2016-09-06T18:34:29.058162: step 1659, loss 0.0106817, acc 1
2016-09-06T18:34:29.742597: step 1660, loss 0.0543167, acc 0.98
2016-09-06T18:34:30.423351: step 1661, loss 0.0438561, acc 0.98
2016-09-06T18:34:31.118308: step 1662, loss 0.0854311, acc 0.94
2016-09-06T18:34:31.780421: step 1663, loss 0.083651, acc 0.98
2016-09-06T18:34:32.486617: step 1664, loss 0.0164523, acc 1
2016-09-06T18:34:33.170637: step 1665, loss 0.0750663, acc 0.98
2016-09-06T18:34:33.863024: step 1666, loss 0.125389, acc 0.94
2016-09-06T18:34:34.557437: step 1667, loss 0.119439, acc 0.9
2016-09-06T18:34:35.235188: step 1668, loss 0.0977615, acc 0.92
2016-09-06T18:34:35.954038: step 1669, loss 0.0247051, acc 1
2016-09-06T18:34:36.629492: step 1670, loss 0.00779922, acc 1
2016-09-06T18:34:37.318099: step 1671, loss 0.0174274, acc 1
2016-09-06T18:34:38.024313: step 1672, loss 0.0339239, acc 0.98
2016-09-06T18:34:38.721725: step 1673, loss 0.0346036, acc 1
2016-09-06T18:34:39.421413: step 1674, loss 0.0381506, acc 0.98
2016-09-06T18:34:40.104934: step 1675, loss 0.0707714, acc 0.94
2016-09-06T18:34:40.808565: step 1676, loss 0.0581881, acc 0.96
2016-09-06T18:34:41.481679: step 1677, loss 0.0397085, acc 0.98
2016-09-06T18:34:42.154701: step 1678, loss 0.0157976, acc 1
2016-09-06T18:34:42.846180: step 1679, loss 0.0298352, acc 1
2016-09-06T18:34:43.524828: step 1680, loss 0.0283038, acc 1
2016-09-06T18:34:44.197116: step 1681, loss 0.0362565, acc 0.98
2016-09-06T18:34:44.876497: step 1682, loss 0.0083742, acc 1
2016-09-06T18:34:45.569105: step 1683, loss 0.0383738, acc 0.98
2016-09-06T18:34:46.252809: step 1684, loss 0.0515393, acc 0.98
2016-09-06T18:34:46.925102: step 1685, loss 0.0746474, acc 0.92
2016-09-06T18:34:47.615981: step 1686, loss 0.0187023, acc 1
2016-09-06T18:34:48.332232: step 1687, loss 0.0746758, acc 0.98
2016-09-06T18:34:49.033622: step 1688, loss 0.160667, acc 0.96
2016-09-06T18:34:49.726533: step 1689, loss 0.00783167, acc 1
2016-09-06T18:34:50.435134: step 1690, loss 0.094804, acc 0.94
2016-09-06T18:34:51.101736: step 1691, loss 0.151782, acc 0.98
2016-09-06T18:34:51.786590: step 1692, loss 0.0321493, acc 0.98
2016-09-06T18:34:52.473646: step 1693, loss 0.11205, acc 0.96
2016-09-06T18:34:53.147058: step 1694, loss 0.11402, acc 0.9
2016-09-06T18:34:53.843490: step 1695, loss 0.116606, acc 0.94
2016-09-06T18:34:54.515554: step 1696, loss 0.0688388, acc 0.96
2016-09-06T18:34:55.222529: step 1697, loss 0.0399891, acc 0.98
2016-09-06T18:34:55.877966: step 1698, loss 0.0276721, acc 1
2016-09-06T18:34:56.557250: step 1699, loss 0.0622245, acc 0.96
2016-09-06T18:34:57.270471: step 1700, loss 0.0626334, acc 0.98

Evaluation:
2016-09-06T18:35:00.473876: step 1700, loss 0.956861, acc 0.76454

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-1700

2016-09-06T18:35:02.117390: step 1701, loss 0.0443919, acc 1
2016-09-06T18:35:02.803282: step 1702, loss 0.0942114, acc 0.92
2016-09-06T18:35:03.477079: step 1703, loss 0.0795871, acc 1
2016-09-06T18:35:04.183894: step 1704, loss 0.00829393, acc 1
2016-09-06T18:35:04.909825: step 1705, loss 0.0794497, acc 0.96
2016-09-06T18:35:05.575734: step 1706, loss 0.0901419, acc 0.96
2016-09-06T18:35:06.255413: step 1707, loss 0.0273188, acc 1
2016-09-06T18:35:06.940923: step 1708, loss 0.126574, acc 0.98
2016-09-06T18:35:07.633899: step 1709, loss 0.113193, acc 0.92
2016-09-06T18:35:08.307061: step 1710, loss 0.11256, acc 0.94
2016-09-06T18:35:08.982575: step 1711, loss 0.0488777, acc 0.98
2016-09-06T18:35:09.680237: step 1712, loss 0.040303, acc 0.98
2016-09-06T18:35:10.338419: step 1713, loss 0.057251, acc 0.96
2016-09-06T18:35:11.020736: step 1714, loss 0.0484914, acc 0.98
2016-09-06T18:35:11.689361: step 1715, loss 0.152202, acc 0.94
2016-09-06T18:35:12.383840: step 1716, loss 0.100335, acc 0.98
2016-09-06T18:35:13.058675: step 1717, loss 0.0140963, acc 1
2016-09-06T18:35:13.721669: step 1718, loss 0.0265235, acc 1
2016-09-06T18:35:14.408784: step 1719, loss 0.0092718, acc 1
2016-09-06T18:35:15.074985: step 1720, loss 0.139724, acc 0.94
2016-09-06T18:35:15.807725: step 1721, loss 0.152302, acc 0.9
2016-09-06T18:35:16.495866: step 1722, loss 0.144469, acc 0.98
2016-09-06T18:35:17.170782: step 1723, loss 0.0750201, acc 0.96
2016-09-06T18:35:17.858773: step 1724, loss 0.0832267, acc 0.98
2016-09-06T18:35:18.544144: step 1725, loss 0.0361279, acc 0.98
2016-09-06T18:35:19.231394: step 1726, loss 0.0103871, acc 1
2016-09-06T18:35:19.886593: step 1727, loss 0.053046, acc 1
2016-09-06T18:35:20.530930: step 1728, loss 0.00393685, acc 1
2016-09-06T18:35:21.200795: step 1729, loss 0.0775494, acc 0.96
2016-09-06T18:35:21.881765: step 1730, loss 0.0577207, acc 0.96
2016-09-06T18:35:22.580142: step 1731, loss 0.0343165, acc 1
2016-09-06T18:35:23.283861: step 1732, loss 0.0210456, acc 1
2016-09-06T18:35:23.966853: step 1733, loss 0.0537322, acc 0.96
2016-09-06T18:35:24.651080: step 1734, loss 0.0614836, acc 0.98
2016-09-06T18:35:25.345836: step 1735, loss 0.0345671, acc 0.98
2016-09-06T18:35:26.039495: step 1736, loss 0.0441649, acc 0.98
2016-09-06T18:35:26.754039: step 1737, loss 0.02196, acc 1
2016-09-06T18:35:27.429476: step 1738, loss 0.0277997, acc 1
2016-09-06T18:35:28.118952: step 1739, loss 0.101393, acc 0.94
2016-09-06T18:35:28.828476: step 1740, loss 0.064355, acc 0.96
2016-09-06T18:35:29.509506: step 1741, loss 0.0218966, acc 1
2016-09-06T18:35:30.204251: step 1742, loss 0.0306198, acc 0.98
2016-09-06T18:35:30.879169: step 1743, loss 0.120737, acc 0.96
2016-09-06T18:35:31.562144: step 1744, loss 0.00578256, acc 1
2016-09-06T18:35:32.256492: step 1745, loss 0.00860833, acc 1
2016-09-06T18:35:32.935813: step 1746, loss 0.0543484, acc 0.98
2016-09-06T18:35:33.622128: step 1747, loss 0.0295277, acc 0.98
2016-09-06T18:35:34.294714: step 1748, loss 0.0709439, acc 0.96
2016-09-06T18:35:34.995685: step 1749, loss 0.0915728, acc 0.94
2016-09-06T18:35:35.680106: step 1750, loss 0.0450285, acc 0.96
2016-09-06T18:35:36.365946: step 1751, loss 0.0381128, acc 0.98
2016-09-06T18:35:37.071730: step 1752, loss 0.0212332, acc 0.98
2016-09-06T18:35:37.781061: step 1753, loss 0.0521972, acc 0.98
2016-09-06T18:35:38.483533: step 1754, loss 0.0179785, acc 0.98
2016-09-06T18:35:39.140138: step 1755, loss 0.0540193, acc 0.98
2016-09-06T18:35:39.848465: step 1756, loss 0.00924812, acc 1
2016-09-06T18:35:40.539821: step 1757, loss 0.036858, acc 0.98
2016-09-06T18:35:41.207409: step 1758, loss 0.095583, acc 0.98
2016-09-06T18:35:41.890065: step 1759, loss 0.048587, acc 0.98
2016-09-06T18:35:42.576361: step 1760, loss 0.00161729, acc 1
2016-09-06T18:35:43.300678: step 1761, loss 0.152072, acc 0.96
2016-09-06T18:35:43.979346: step 1762, loss 0.00564238, acc 1
2016-09-06T18:35:44.668631: step 1763, loss 0.0401148, acc 1
2016-09-06T18:35:45.347077: step 1764, loss 0.0466403, acc 0.96
2016-09-06T18:35:46.020824: step 1765, loss 0.027272, acc 0.98
2016-09-06T18:35:46.732090: step 1766, loss 0.16524, acc 0.92
2016-09-06T18:35:47.424996: step 1767, loss 0.0642144, acc 0.96
2016-09-06T18:35:48.113232: step 1768, loss 0.209699, acc 0.9
2016-09-06T18:35:48.789438: step 1769, loss 0.0906479, acc 0.96
2016-09-06T18:35:49.494870: step 1770, loss 0.040574, acc 0.98
2016-09-06T18:35:50.179600: step 1771, loss 0.0448739, acc 0.98
2016-09-06T18:35:50.865765: step 1772, loss 0.0696989, acc 0.96
2016-09-06T18:35:51.548592: step 1773, loss 0.0916946, acc 0.94
2016-09-06T18:35:52.227380: step 1774, loss 0.0408404, acc 1
2016-09-06T18:35:52.954588: step 1775, loss 0.0524976, acc 0.98
2016-09-06T18:35:53.631756: step 1776, loss 0.0243593, acc 1
2016-09-06T18:35:54.329237: step 1777, loss 0.0957744, acc 0.96
2016-09-06T18:35:55.021734: step 1778, loss 0.0355272, acc 1
2016-09-06T18:35:55.721929: step 1779, loss 0.0231049, acc 1
2016-09-06T18:35:56.402322: step 1780, loss 0.0106584, acc 1
2016-09-06T18:35:57.083735: step 1781, loss 0.00374719, acc 1
2016-09-06T18:35:57.779706: step 1782, loss 0.116077, acc 0.94
2016-09-06T18:35:58.451099: step 1783, loss 0.0614988, acc 1
2016-09-06T18:35:59.141944: step 1784, loss 0.0566515, acc 0.98
2016-09-06T18:35:59.821864: step 1785, loss 0.0758113, acc 0.98
2016-09-06T18:36:00.578727: step 1786, loss 0.0270698, acc 1
2016-09-06T18:36:01.259386: step 1787, loss 0.0758679, acc 0.96
2016-09-06T18:36:01.947195: step 1788, loss 0.0140856, acc 1
2016-09-06T18:36:02.660310: step 1789, loss 0.0788613, acc 0.96
2016-09-06T18:36:03.324340: step 1790, loss 0.135759, acc 0.96
2016-09-06T18:36:04.013102: step 1791, loss 0.0222185, acc 0.98
2016-09-06T18:36:04.693258: step 1792, loss 0.0656551, acc 0.96
2016-09-06T18:36:05.395110: step 1793, loss 0.0981314, acc 0.98
2016-09-06T18:36:06.068181: step 1794, loss 0.0488861, acc 1
2016-09-06T18:36:06.751117: step 1795, loss 0.00223311, acc 1
2016-09-06T18:36:07.480968: step 1796, loss 0.0678176, acc 0.96
2016-09-06T18:36:08.161347: step 1797, loss 0.0371123, acc 0.98
2016-09-06T18:36:08.855145: step 1798, loss 0.000365729, acc 1
2016-09-06T18:36:09.524175: step 1799, loss 0.0308871, acc 1
2016-09-06T18:36:10.204313: step 1800, loss 0.00759283, acc 1

Evaluation:
2016-09-06T18:36:13.334331: step 1800, loss 1.10815, acc 0.774859

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-1800

2016-09-06T18:36:14.942320: step 1801, loss 0.0352871, acc 0.98
2016-09-06T18:36:15.635811: step 1802, loss 0.122055, acc 0.9
2016-09-06T18:36:16.341480: step 1803, loss 0.0509014, acc 0.98
2016-09-06T18:36:17.034181: step 1804, loss 0.0382186, acc 0.98
2016-09-06T18:36:17.727917: step 1805, loss 0.0733202, acc 0.98
2016-09-06T18:36:18.410600: step 1806, loss 0.0328222, acc 1
2016-09-06T18:36:19.109833: step 1807, loss 0.00716954, acc 1
2016-09-06T18:36:19.788119: step 1808, loss 0.153046, acc 0.96
2016-09-06T18:36:20.466333: step 1809, loss 0.011948, acc 1
2016-09-06T18:36:21.155066: step 1810, loss 0.0275244, acc 0.98
2016-09-06T18:36:21.858995: step 1811, loss 0.0408486, acc 0.98
2016-09-06T18:36:22.543666: step 1812, loss 0.013225, acc 1
2016-09-06T18:36:23.221638: step 1813, loss 0.0802913, acc 0.94
2016-09-06T18:36:23.910829: step 1814, loss 0.0169491, acc 1
2016-09-06T18:36:24.587027: step 1815, loss 0.0501043, acc 0.98
2016-09-06T18:36:25.284408: step 1816, loss 0.0333459, acc 0.98
2016-09-06T18:36:25.969256: step 1817, loss 0.0785072, acc 0.96
2016-09-06T18:36:26.671135: step 1818, loss 0.0372945, acc 1
2016-09-06T18:36:27.344908: step 1819, loss 0.0780263, acc 0.98
2016-09-06T18:36:28.022992: step 1820, loss 0.048883, acc 0.98
2016-09-06T18:36:28.710751: step 1821, loss 0.0402179, acc 0.98
2016-09-06T18:36:29.391432: step 1822, loss 0.114093, acc 0.98
2016-09-06T18:36:30.090910: step 1823, loss 0.0524658, acc 0.96
2016-09-06T18:36:30.784700: step 1824, loss 0.0286591, acc 0.98
2016-09-06T18:36:31.493048: step 1825, loss 0.00341444, acc 1
2016-09-06T18:36:32.185674: step 1826, loss 0.0790715, acc 0.96
2016-09-06T18:36:32.880241: step 1827, loss 0.0555916, acc 0.98
2016-09-06T18:36:33.581385: step 1828, loss 0.0419064, acc 0.98
2016-09-06T18:36:34.250937: step 1829, loss 0.0477758, acc 0.98
2016-09-06T18:36:34.956930: step 1830, loss 0.035792, acc 0.98
2016-09-06T18:36:35.651747: step 1831, loss 0.217152, acc 0.9
2016-09-06T18:36:36.363100: step 1832, loss 0.0420456, acc 0.98
2016-09-06T18:36:37.052389: step 1833, loss 0.109099, acc 0.98
2016-09-06T18:36:37.719893: step 1834, loss 0.0867767, acc 0.96
2016-09-06T18:36:38.427192: step 1835, loss 0.154966, acc 0.92
2016-09-06T18:36:39.118930: step 1836, loss 0.0217439, acc 0.98
2016-09-06T18:36:39.809337: step 1837, loss 0.0624507, acc 0.98
2016-09-06T18:36:40.469967: step 1838, loss 0.109497, acc 0.96
2016-09-06T18:36:41.175928: step 1839, loss 0.0408058, acc 0.98
2016-09-06T18:36:41.827021: step 1840, loss 0.0984463, acc 0.96
2016-09-06T18:36:42.515046: step 1841, loss 0.0931075, acc 0.94
2016-09-06T18:36:43.179463: step 1842, loss 0.0483887, acc 0.98
2016-09-06T18:36:43.855961: step 1843, loss 0.0404535, acc 0.98
2016-09-06T18:36:44.540292: step 1844, loss 0.0036844, acc 1
2016-09-06T18:36:45.244985: step 1845, loss 0.0502845, acc 0.98
2016-09-06T18:36:45.943972: step 1846, loss 0.0328795, acc 1
2016-09-06T18:36:46.627684: step 1847, loss 0.117904, acc 0.92
2016-09-06T18:36:47.340123: step 1848, loss 0.0646885, acc 0.96
2016-09-06T18:36:48.039071: step 1849, loss 0.0533976, acc 0.96
2016-09-06T18:36:48.729878: step 1850, loss 0.0340771, acc 1
2016-09-06T18:36:49.422441: step 1851, loss 0.0296419, acc 1
2016-09-06T18:36:50.091706: step 1852, loss 0.115139, acc 0.94
2016-09-06T18:36:50.815954: step 1853, loss 0.110102, acc 0.94
2016-09-06T18:36:51.525312: step 1854, loss 0.070571, acc 0.98
2016-09-06T18:36:52.220914: step 1855, loss 0.0300023, acc 1
2016-09-06T18:36:52.913316: step 1856, loss 0.0771927, acc 0.94
2016-09-06T18:36:53.612961: step 1857, loss 0.00884187, acc 1
2016-09-06T18:36:54.328689: step 1858, loss 0.176117, acc 0.96
2016-09-06T18:36:54.992200: step 1859, loss 0.0947061, acc 0.96
2016-09-06T18:36:55.675945: step 1860, loss 0.172858, acc 0.94
2016-09-06T18:36:56.369868: step 1861, loss 0.0905197, acc 0.94
2016-09-06T18:36:57.043174: step 1862, loss 0.0392844, acc 1
2016-09-06T18:36:57.735448: step 1863, loss 0.00889244, acc 1
2016-09-06T18:36:58.419921: step 1864, loss 0.120316, acc 0.96
2016-09-06T18:36:59.120058: step 1865, loss 0.0539843, acc 0.98
2016-09-06T18:36:59.793422: step 1866, loss 0.0119383, acc 1
2016-09-06T18:37:00.516066: step 1867, loss 0.0497032, acc 0.98
2016-09-06T18:37:01.195940: step 1868, loss 0.0636724, acc 0.96
2016-09-06T18:37:01.880623: step 1869, loss 0.0445774, acc 0.98
2016-09-06T18:37:02.574666: step 1870, loss 0.00557901, acc 1
2016-09-06T18:37:03.244899: step 1871, loss 0.0135315, acc 1
2016-09-06T18:37:03.954466: step 1872, loss 0.047306, acc 0.96
2016-09-06T18:37:04.634032: step 1873, loss 0.133558, acc 0.92
2016-09-06T18:37:05.366218: step 1874, loss 0.0174436, acc 1
2016-09-06T18:37:06.061189: step 1875, loss 0.0956258, acc 0.94
2016-09-06T18:37:06.758243: step 1876, loss 0.108547, acc 0.96
2016-09-06T18:37:07.443207: step 1877, loss 0.0299746, acc 1
2016-09-06T18:37:08.120480: step 1878, loss 0.0845102, acc 0.94
2016-09-06T18:37:08.819927: step 1879, loss 0.040268, acc 0.98
2016-09-06T18:37:09.507559: step 1880, loss 0.0401208, acc 0.98
2016-09-06T18:37:10.193829: step 1881, loss 0.0302554, acc 0.98
2016-09-06T18:37:10.876224: step 1882, loss 0.0707587, acc 0.96
2016-09-06T18:37:11.579011: step 1883, loss 0.0933503, acc 0.94
2016-09-06T18:37:12.295687: step 1884, loss 0.00565174, acc 1
2016-09-06T18:37:12.975188: step 1885, loss 0.0648441, acc 0.94
2016-09-06T18:37:13.675638: step 1886, loss 0.0523266, acc 0.98
2016-09-06T18:37:14.366280: step 1887, loss 0.0331593, acc 1
2016-09-06T18:37:15.049238: step 1888, loss 0.0147866, acc 1
2016-09-06T18:37:15.718660: step 1889, loss 0.0250923, acc 0.98
2016-09-06T18:37:16.414936: step 1890, loss 0.044915, acc 0.98
2016-09-06T18:37:17.113464: step 1891, loss 0.0441941, acc 0.98
2016-09-06T18:37:17.776778: step 1892, loss 0.0625166, acc 0.98
2016-09-06T18:37:18.489329: step 1893, loss 0.04009, acc 0.98
2016-09-06T18:37:19.181212: step 1894, loss 0.107569, acc 0.94
2016-09-06T18:37:19.860803: step 1895, loss 0.0242198, acc 1
2016-09-06T18:37:20.557520: step 1896, loss 0.0853527, acc 0.98
2016-09-06T18:37:21.242613: step 1897, loss 0.0403712, acc 0.96
2016-09-06T18:37:21.916144: step 1898, loss 0.0252299, acc 0.98
2016-09-06T18:37:22.600716: step 1899, loss 0.0419733, acc 0.98
2016-09-06T18:37:23.307279: step 1900, loss 0.00833855, acc 1

Evaluation:
2016-09-06T18:37:26.455789: step 1900, loss 1.43291, acc 0.772983

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-1900

2016-09-06T18:37:28.183698: step 1901, loss 0.178515, acc 0.94
2016-09-06T18:37:28.880609: step 1902, loss 0.131544, acc 0.96
2016-09-06T18:37:29.575638: step 1903, loss 0.047903, acc 0.96
2016-09-06T18:37:30.279277: step 1904, loss 0.0206044, acc 1
2016-09-06T18:37:30.964082: step 1905, loss 0.0753004, acc 0.98
2016-09-06T18:37:31.650319: step 1906, loss 0.0677217, acc 0.94
2016-09-06T18:37:32.305057: step 1907, loss 0.0613457, acc 0.98
2016-09-06T18:37:32.991354: step 1908, loss 0.198247, acc 0.94
2016-09-06T18:37:33.682512: step 1909, loss 0.174724, acc 0.9
2016-09-06T18:37:34.394672: step 1910, loss 0.0223501, acc 1
2016-09-06T18:37:35.100299: step 1911, loss 0.0819292, acc 0.98
2016-09-06T18:37:35.797647: step 1912, loss 0.0781413, acc 0.96
2016-09-06T18:37:36.501665: step 1913, loss 0.0923252, acc 0.94
2016-09-06T18:37:37.193873: step 1914, loss 0.0880153, acc 0.96
2016-09-06T18:37:37.884689: step 1915, loss 0.081164, acc 0.92
2016-09-06T18:37:38.565001: step 1916, loss 0.0207724, acc 1
2016-09-06T18:37:39.249440: step 1917, loss 0.0217136, acc 1
2016-09-06T18:37:39.922161: step 1918, loss 0.132045, acc 0.96
2016-09-06T18:37:40.578082: step 1919, loss 0.0946695, acc 0.96
2016-09-06T18:37:41.231323: step 1920, loss 0.0440119, acc 1
2016-09-06T18:37:41.913272: step 1921, loss 0.0643235, acc 0.98
2016-09-06T18:37:42.617987: step 1922, loss 0.0114621, acc 1
2016-09-06T18:37:43.299854: step 1923, loss 0.0421026, acc 0.98
2016-09-06T18:37:43.988852: step 1924, loss 0.112666, acc 0.96
2016-09-06T18:37:44.654325: step 1925, loss 0.0061309, acc 1
2016-09-06T18:37:45.340936: step 1926, loss 0.0546159, acc 0.98
2016-09-06T18:37:46.065779: step 1927, loss 0.0536777, acc 0.98
2016-09-06T18:37:46.721415: step 1928, loss 0.00686316, acc 1
2016-09-06T18:37:47.402045: step 1929, loss 0.0582571, acc 0.96
2016-09-06T18:37:48.092238: step 1930, loss 0.0425019, acc 0.98
2016-09-06T18:37:48.764523: step 1931, loss 0.155097, acc 0.96
2016-09-06T18:37:49.442409: step 1932, loss 0.0487132, acc 1
2016-09-06T18:37:50.112393: step 1933, loss 0.0325497, acc 1
2016-09-06T18:37:50.812704: step 1934, loss 0.0684881, acc 0.96
2016-09-06T18:37:51.480454: step 1935, loss 0.0426452, acc 0.98
2016-09-06T18:37:52.174286: step 1936, loss 0.0441548, acc 0.96
2016-09-06T18:37:52.871845: step 1937, loss 0.0295829, acc 0.98
2016-09-06T18:37:53.557304: step 1938, loss 0.0279629, acc 1
2016-09-06T18:37:54.228643: step 1939, loss 0.0251348, acc 1
2016-09-06T18:37:54.920953: step 1940, loss 0.0366397, acc 0.98
2016-09-06T18:37:55.632362: step 1941, loss 0.090198, acc 0.96
2016-09-06T18:37:56.301669: step 1942, loss 0.0053105, acc 1
2016-09-06T18:37:56.997266: step 1943, loss 0.034406, acc 0.98
2016-09-06T18:37:57.677964: step 1944, loss 0.0770656, acc 0.96
2016-09-06T18:37:58.360825: step 1945, loss 0.000640291, acc 1
2016-09-06T18:37:59.054648: step 1946, loss 0.0426151, acc 0.98
2016-09-06T18:37:59.754863: step 1947, loss 0.0880316, acc 0.98
2016-09-06T18:38:00.498722: step 1948, loss 0.0086969, acc 1
2016-09-06T18:38:01.182186: step 1949, loss 0.0782883, acc 0.96
2016-09-06T18:38:01.856468: step 1950, loss 0.0975638, acc 0.96
2016-09-06T18:38:02.530505: step 1951, loss 0.103492, acc 0.96
2016-09-06T18:38:03.229435: step 1952, loss 0.0978335, acc 0.98
2016-09-06T18:38:03.920713: step 1953, loss 0.0663047, acc 0.96
2016-09-06T18:38:04.595898: step 1954, loss 0.0230391, acc 1
2016-09-06T18:38:05.307340: step 1955, loss 0.245454, acc 0.92
2016-09-06T18:38:05.997188: step 1956, loss 0.025117, acc 0.98
2016-09-06T18:38:06.671695: step 1957, loss 0.00997521, acc 1
2016-09-06T18:38:07.366520: step 1958, loss 0.00676047, acc 1
2016-09-06T18:38:08.038356: step 1959, loss 0.0667783, acc 0.98
2016-09-06T18:38:08.738021: step 1960, loss 0.0284566, acc 0.98
2016-09-06T18:38:09.404681: step 1961, loss 0.0734117, acc 0.92
2016-09-06T18:38:10.091797: step 1962, loss 0.0715397, acc 0.96
2016-09-06T18:38:10.758990: step 1963, loss 0.0688428, acc 0.98
2016-09-06T18:38:11.449776: step 1964, loss 0.0589159, acc 0.96
2016-09-06T18:38:12.124358: step 1965, loss 0.0675544, acc 0.94
2016-09-06T18:38:12.807177: step 1966, loss 0.0401658, acc 0.98
2016-09-06T18:38:13.510094: step 1967, loss 0.0854556, acc 0.96
2016-09-06T18:38:14.200019: step 1968, loss 0.0856275, acc 0.98
2016-09-06T18:38:14.900127: step 1969, loss 0.0734521, acc 0.96
2016-09-06T18:38:15.576122: step 1970, loss 0.116214, acc 0.96
2016-09-06T18:38:16.263014: step 1971, loss 0.0152966, acc 1
2016-09-06T18:38:16.951857: step 1972, loss 0.00948382, acc 1
2016-09-06T18:38:17.627913: step 1973, loss 0.0187254, acc 1
2016-09-06T18:38:18.311731: step 1974, loss 0.0182372, acc 0.98
2016-09-06T18:38:19.011402: step 1975, loss 0.0571624, acc 0.96
2016-09-06T18:38:19.719728: step 1976, loss 0.0993125, acc 0.94
2016-09-06T18:38:20.375041: step 1977, loss 0.0557792, acc 1
2016-09-06T18:38:21.080590: step 1978, loss 0.0332646, acc 0.98
2016-09-06T18:38:21.764557: step 1979, loss 0.054534, acc 0.96
2016-09-06T18:38:22.452872: step 1980, loss 0.0719044, acc 0.96
2016-09-06T18:38:23.136250: step 1981, loss 0.015452, acc 1
2016-09-06T18:38:23.817181: step 1982, loss 0.0541599, acc 0.98
2016-09-06T18:38:24.538743: step 1983, loss 0.0407305, acc 0.98
2016-09-06T18:38:25.218415: step 1984, loss 0.131041, acc 0.96
2016-09-06T18:38:25.893463: step 1985, loss 0.0408657, acc 0.98
2016-09-06T18:38:26.571308: step 1986, loss 0.0528274, acc 1
2016-09-06T18:38:27.262239: step 1987, loss 0.17185, acc 0.98
2016-09-06T18:38:27.935669: step 1988, loss 0.0253121, acc 1
2016-09-06T18:38:28.614820: step 1989, loss 0.0766128, acc 0.96
2016-09-06T18:38:29.318344: step 1990, loss 0.00851289, acc 1
2016-09-06T18:38:29.993315: step 1991, loss 0.0553336, acc 0.98
2016-09-06T18:38:30.683356: step 1992, loss 0.0142086, acc 1
2016-09-06T18:38:31.362711: step 1993, loss 0.0325177, acc 0.98
2016-09-06T18:38:32.057009: step 1994, loss 0.0220268, acc 0.98
2016-09-06T18:38:32.738710: step 1995, loss 0.0221666, acc 1
2016-09-06T18:38:33.412457: step 1996, loss 0.0345469, acc 0.98
2016-09-06T18:38:34.127274: step 1997, loss 0.0465228, acc 0.98
2016-09-06T18:38:34.787555: step 1998, loss 0.118464, acc 0.98
2016-09-06T18:38:35.502210: step 1999, loss 0.0374153, acc 0.98
2016-09-06T18:38:36.183928: step 2000, loss 0.0238733, acc 1

Evaluation:
2016-09-06T18:38:39.319633: step 2000, loss 1.20515, acc 0.768293

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-2000

2016-09-06T18:38:40.979304: step 2001, loss 0.0615713, acc 0.98
2016-09-06T18:38:41.661388: step 2002, loss 0.0297609, acc 1
2016-09-06T18:38:42.337865: step 2003, loss 0.159831, acc 0.92
2016-09-06T18:38:43.012756: step 2004, loss 0.0515424, acc 0.98
2016-09-06T18:38:43.710705: step 2005, loss 0.157892, acc 0.94
2016-09-06T18:38:44.364752: step 2006, loss 0.0201303, acc 0.98
2016-09-06T18:38:45.048654: step 2007, loss 0.10453, acc 0.98
2016-09-06T18:38:45.729665: step 2008, loss 0.0110138, acc 1
2016-09-06T18:38:46.396509: step 2009, loss 0.0478982, acc 0.96
2016-09-06T18:38:47.075974: step 2010, loss 0.0361669, acc 0.98
2016-09-06T18:38:47.759291: step 2011, loss 0.00383156, acc 1
2016-09-06T18:38:48.444221: step 2012, loss 0.0568298, acc 0.96
2016-09-06T18:38:49.115623: step 2013, loss 0.0142529, acc 1
2016-09-06T18:38:49.804688: step 2014, loss 0.0438333, acc 0.98
2016-09-06T18:38:50.475740: step 2015, loss 0.0459877, acc 0.98
2016-09-06T18:38:51.166012: step 2016, loss 0.091827, acc 0.96
2016-09-06T18:38:51.845183: step 2017, loss 0.0370796, acc 0.98
2016-09-06T18:38:52.540042: step 2018, loss 0.0292449, acc 0.98
2016-09-06T18:38:53.232703: step 2019, loss 0.127976, acc 0.96
2016-09-06T18:38:53.917325: step 2020, loss 0.0694047, acc 0.96
2016-09-06T18:38:54.649788: step 2021, loss 0.0514066, acc 0.96
2016-09-06T18:38:55.304494: step 2022, loss 0.0430629, acc 0.98
2016-09-06T18:38:55.983860: step 2023, loss 0.057829, acc 0.94
2016-09-06T18:38:56.665134: step 2024, loss 0.108138, acc 0.96
2016-09-06T18:38:57.359557: step 2025, loss 0.08985, acc 0.96
2016-09-06T18:38:58.050910: step 2026, loss 0.0725657, acc 0.96
2016-09-06T18:38:58.735898: step 2027, loss 0.056289, acc 0.98
2016-09-06T18:38:59.436931: step 2028, loss 0.0382665, acc 0.98
2016-09-06T18:39:00.102730: step 2029, loss 0.0777315, acc 0.96
2016-09-06T18:39:00.832283: step 2030, loss 0.0512793, acc 0.98
2016-09-06T18:39:01.517249: step 2031, loss 0.0229233, acc 1
2016-09-06T18:39:02.203232: step 2032, loss 0.0636708, acc 0.98
2016-09-06T18:39:02.885739: step 2033, loss 0.0120733, acc 1
2016-09-06T18:39:03.555602: step 2034, loss 0.105772, acc 0.98
2016-09-06T18:39:04.271159: step 2035, loss 0.0743088, acc 0.98
2016-09-06T18:39:04.952379: step 2036, loss 0.0243503, acc 1
2016-09-06T18:39:05.659355: step 2037, loss 0.0219569, acc 0.98
2016-09-06T18:39:06.348017: step 2038, loss 0.0508845, acc 0.98
2016-09-06T18:39:07.037414: step 2039, loss 0.0295326, acc 0.98
2016-09-06T18:39:07.727027: step 2040, loss 0.0246916, acc 0.98
2016-09-06T18:39:08.394448: step 2041, loss 0.0617171, acc 0.96
2016-09-06T18:39:09.114633: step 2042, loss 0.0793899, acc 0.96
2016-09-06T18:39:09.817916: step 2043, loss 0.0277274, acc 0.98
2016-09-06T18:39:10.516217: step 2044, loss 0.0374588, acc 0.98
2016-09-06T18:39:11.203058: step 2045, loss 0.0142699, acc 1
2016-09-06T18:39:11.876838: step 2046, loss 0.0454542, acc 0.98
2016-09-06T18:39:12.581822: step 2047, loss 0.121646, acc 0.94
2016-09-06T18:39:13.252943: step 2048, loss 0.0481827, acc 0.96
2016-09-06T18:39:13.958383: step 2049, loss 0.0221009, acc 1
2016-09-06T18:39:14.655445: step 2050, loss 0.110636, acc 0.94
2016-09-06T18:39:15.342354: step 2051, loss 0.0671813, acc 0.98
2016-09-06T18:39:16.051073: step 2052, loss 0.102126, acc 0.98
2016-09-06T18:39:16.752614: step 2053, loss 0.0436998, acc 0.96
2016-09-06T18:39:17.468991: step 2054, loss 0.021273, acc 1
2016-09-06T18:39:18.155729: step 2055, loss 0.0133683, acc 1
2016-09-06T18:39:18.845868: step 2056, loss 0.0192175, acc 1
2016-09-06T18:39:19.525649: step 2057, loss 0.0214219, acc 1
2016-09-06T18:39:20.234792: step 2058, loss 0.020903, acc 0.98
2016-09-06T18:39:20.942822: step 2059, loss 0.00387554, acc 1
2016-09-06T18:39:21.633121: step 2060, loss 0.00668469, acc 1
2016-09-06T18:39:22.337305: step 2061, loss 0.0289276, acc 0.98
2016-09-06T18:39:23.032295: step 2062, loss 0.0516013, acc 0.98
2016-09-06T18:39:23.725382: step 2063, loss 0.0490115, acc 0.98
2016-09-06T18:39:24.426010: step 2064, loss 0.0169889, acc 1
2016-09-06T18:39:25.101344: step 2065, loss 0.034925, acc 0.98
2016-09-06T18:39:25.803268: step 2066, loss 0.049971, acc 0.98
2016-09-06T18:39:26.487220: step 2067, loss 0.0482123, acc 0.96
2016-09-06T18:39:27.190879: step 2068, loss 0.0330713, acc 0.98
2016-09-06T18:39:27.897721: step 2069, loss 0.039734, acc 1
2016-09-06T18:39:28.600464: step 2070, loss 0.049565, acc 0.98
2016-09-06T18:39:29.297087: step 2071, loss 0.0442747, acc 0.98
2016-09-06T18:39:30.003653: step 2072, loss 0.0161329, acc 1
2016-09-06T18:39:30.704143: step 2073, loss 0.0938399, acc 0.96
2016-09-06T18:39:31.395313: step 2074, loss 0.00190375, acc 1
2016-09-06T18:39:32.069494: step 2075, loss 0.0304761, acc 0.98
2016-09-06T18:39:32.770958: step 2076, loss 0.0409026, acc 1
2016-09-06T18:39:33.461139: step 2077, loss 0.00678816, acc 1
2016-09-06T18:39:34.167767: step 2078, loss 0.189164, acc 0.92
2016-09-06T18:39:34.843088: step 2079, loss 0.0389002, acc 0.98
2016-09-06T18:39:35.549661: step 2080, loss 0.153216, acc 0.94
2016-09-06T18:39:36.231234: step 2081, loss 0.0313299, acc 0.98
2016-09-06T18:39:36.912295: step 2082, loss 0.0503619, acc 0.96
2016-09-06T18:39:37.616795: step 2083, loss 0.124668, acc 0.96
2016-09-06T18:39:38.324163: step 2084, loss 0.039199, acc 0.98
2016-09-06T18:39:39.014787: step 2085, loss 0.0639578, acc 0.98
2016-09-06T18:39:39.693114: step 2086, loss 0.0870584, acc 0.94
2016-09-06T18:39:40.397941: step 2087, loss 0.0420799, acc 0.98
2016-09-06T18:39:41.066833: step 2088, loss 0.0266653, acc 0.98
2016-09-06T18:39:41.751979: step 2089, loss 0.0230648, acc 0.98
2016-09-06T18:39:42.457488: step 2090, loss 0.19103, acc 0.96
2016-09-06T18:39:43.133280: step 2091, loss 0.0295314, acc 0.98
2016-09-06T18:39:43.837403: step 2092, loss 0.18537, acc 0.96
2016-09-06T18:39:44.495498: step 2093, loss 0.0156016, acc 1
2016-09-06T18:39:45.189152: step 2094, loss 0.108546, acc 0.94
2016-09-06T18:39:45.882920: step 2095, loss 0.0381805, acc 0.98
2016-09-06T18:39:46.562729: step 2096, loss 0.0412448, acc 0.96
2016-09-06T18:39:47.231601: step 2097, loss 0.0459429, acc 0.98
2016-09-06T18:39:47.912739: step 2098, loss 0.0710854, acc 0.94
2016-09-06T18:39:48.593998: step 2099, loss 0.0311588, acc 1
2016-09-06T18:39:49.253938: step 2100, loss 0.048236, acc 0.98

Evaluation:
2016-09-06T18:39:52.414079: step 2100, loss 1.22219, acc 0.757974

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-2100

2016-09-06T18:39:54.071514: step 2101, loss 0.0291061, acc 1
2016-09-06T18:39:54.780636: step 2102, loss 0.0265504, acc 1
2016-09-06T18:39:55.468722: step 2103, loss 0.0490683, acc 0.98
2016-09-06T18:39:56.151954: step 2104, loss 0.0201234, acc 1
2016-09-06T18:39:56.824224: step 2105, loss 0.0461938, acc 0.98
2016-09-06T18:39:57.517406: step 2106, loss 0.0222976, acc 1
2016-09-06T18:39:58.233260: step 2107, loss 0.0339167, acc 0.98
2016-09-06T18:39:58.909955: step 2108, loss 0.0138382, acc 1
2016-09-06T18:39:59.615284: step 2109, loss 0.0205535, acc 0.98
2016-09-06T18:40:00.323109: step 2110, loss 0.121183, acc 0.94
2016-09-06T18:40:00.997119: step 2111, loss 0.0507674, acc 0.96
2016-09-06T18:40:01.638730: step 2112, loss 0.0785951, acc 0.977273
2016-09-06T18:40:02.315411: step 2113, loss 0.00895612, acc 1
2016-09-06T18:40:03.014554: step 2114, loss 0.0290658, acc 1
2016-09-06T18:40:03.694008: step 2115, loss 0.0452277, acc 0.98
2016-09-06T18:40:04.390787: step 2116, loss 0.0248965, acc 1
2016-09-06T18:40:05.060312: step 2117, loss 0.0407655, acc 0.98
2016-09-06T18:40:05.734479: step 2118, loss 0.036496, acc 0.96
2016-09-06T18:40:06.401208: step 2119, loss 0.013367, acc 1
2016-09-06T18:40:07.088780: step 2120, loss 0.0040671, acc 1
2016-09-06T18:40:07.763104: step 2121, loss 0.0212942, acc 1
2016-09-06T18:40:08.456749: step 2122, loss 0.0469441, acc 0.96
2016-09-06T18:40:09.151197: step 2123, loss 0.0480425, acc 0.98
2016-09-06T18:40:09.825109: step 2124, loss 0.140552, acc 0.96
2016-09-06T18:40:10.521941: step 2125, loss 0.0173138, acc 1
2016-09-06T18:40:11.195125: step 2126, loss 0.00524694, acc 1
2016-09-06T18:40:11.892986: step 2127, loss 0.00917521, acc 1
2016-09-06T18:40:12.573907: step 2128, loss 0.0639614, acc 0.96
2016-09-06T18:40:13.273657: step 2129, loss 0.0891639, acc 0.96
2016-09-06T18:40:13.980977: step 2130, loss 0.0126318, acc 1
2016-09-06T18:40:14.661720: step 2131, loss 0.0508372, acc 0.98
2016-09-06T18:40:15.354063: step 2132, loss 0.0255685, acc 1
2016-09-06T18:40:16.053159: step 2133, loss 0.0469164, acc 0.98
2016-09-06T18:40:16.752030: step 2134, loss 0.00529438, acc 1
2016-09-06T18:40:17.439159: step 2135, loss 0.0909204, acc 0.94
2016-09-06T18:40:18.106457: step 2136, loss 0.00456266, acc 1
2016-09-06T18:40:18.810322: step 2137, loss 0.0241296, acc 0.98
2016-09-06T18:40:19.464462: step 2138, loss 0.0292934, acc 0.98
2016-09-06T18:40:20.147247: step 2139, loss 0.00978659, acc 1
2016-09-06T18:40:20.823610: step 2140, loss 0.0338731, acc 0.98
2016-09-06T18:40:21.503355: step 2141, loss 0.0652999, acc 0.98
2016-09-06T18:40:22.192415: step 2142, loss 0.010777, acc 1
2016-09-06T18:40:22.857887: step 2143, loss 0.113334, acc 0.98
2016-09-06T18:40:23.563839: step 2144, loss 0.022064, acc 0.98
2016-09-06T18:40:24.248137: step 2145, loss 0.0362947, acc 0.98
2016-09-06T18:40:24.912714: step 2146, loss 0.0417075, acc 0.98
2016-09-06T18:40:25.586035: step 2147, loss 0.0135456, acc 1
2016-09-06T18:40:26.257535: step 2148, loss 0.0441219, acc 0.98
2016-09-06T18:40:26.951010: step 2149, loss 0.00995761, acc 1
2016-09-06T18:40:27.646243: step 2150, loss 0.118862, acc 0.96
2016-09-06T18:40:28.350886: step 2151, loss 0.0205069, acc 0.98
2016-09-06T18:40:29.040989: step 2152, loss 0.0654046, acc 0.96
2016-09-06T18:40:29.730788: step 2153, loss 0.0580156, acc 0.98
2016-09-06T18:40:30.423135: step 2154, loss 0.112103, acc 0.94
2016-09-06T18:40:31.122217: step 2155, loss 0.126979, acc 0.98
2016-09-06T18:40:31.819282: step 2156, loss 0.0225647, acc 1
2016-09-06T18:40:32.526107: step 2157, loss 0.0404847, acc 0.96
2016-09-06T18:40:33.250327: step 2158, loss 0.0391719, acc 0.98
2016-09-06T18:40:33.938659: step 2159, loss 0.0369307, acc 0.96
2016-09-06T18:40:34.626821: step 2160, loss 0.0194153, acc 0.98
2016-09-06T18:40:35.296624: step 2161, loss 0.0905384, acc 0.94
2016-09-06T18:40:36.009427: step 2162, loss 0.0374163, acc 0.98
2016-09-06T18:40:36.702689: step 2163, loss 0.0212417, acc 1
2016-09-06T18:40:37.345401: step 2164, loss 0.0394968, acc 0.98
2016-09-06T18:40:38.075746: step 2165, loss 0.000539407, acc 1
2016-09-06T18:40:38.750802: step 2166, loss 0.0542965, acc 0.96
2016-09-06T18:40:39.422414: step 2167, loss 0.0484456, acc 0.98
2016-09-06T18:40:40.101885: step 2168, loss 0.00726484, acc 1
2016-09-06T18:40:40.775025: step 2169, loss 0.0502606, acc 0.96
2016-09-06T18:40:41.468713: step 2170, loss 0.0546478, acc 0.98
2016-09-06T18:40:42.127831: step 2171, loss 0.0286933, acc 1
2016-09-06T18:40:42.837138: step 2172, loss 0.0611838, acc 0.98
2016-09-06T18:40:43.520202: step 2173, loss 0.0107447, acc 1
2016-09-06T18:40:44.203274: step 2174, loss 0.0544081, acc 0.96
2016-09-06T18:40:44.883472: step 2175, loss 0.056809, acc 0.98
2016-09-06T18:40:45.573272: step 2176, loss 0.0234039, acc 0.98
2016-09-06T18:40:46.262847: step 2177, loss 0.0499825, acc 0.98
2016-09-06T18:40:46.944960: step 2178, loss 0.0206557, acc 1
2016-09-06T18:40:47.652847: step 2179, loss 0.0911019, acc 0.94
2016-09-06T18:40:48.329970: step 2180, loss 0.0303629, acc 1
2016-09-06T18:40:48.995952: step 2181, loss 0.12428, acc 0.94
2016-09-06T18:40:49.682915: step 2182, loss 0.0609273, acc 0.96
2016-09-06T18:40:50.368740: step 2183, loss 0.160544, acc 0.9
2016-09-06T18:40:51.044360: step 2184, loss 0.00320195, acc 1
2016-09-06T18:40:51.732105: step 2185, loss 0.155086, acc 0.98
2016-09-06T18:40:52.441314: step 2186, loss 0.0659808, acc 0.98
2016-09-06T18:40:53.119471: step 2187, loss 0.0425763, acc 0.98
2016-09-06T18:40:53.828955: step 2188, loss 0.0561251, acc 0.98
2016-09-06T18:40:54.511406: step 2189, loss 0.0870021, acc 0.96
2016-09-06T18:40:55.197644: step 2190, loss 0.080276, acc 0.96
2016-09-06T18:40:55.886292: step 2191, loss 0.0649213, acc 0.96
2016-09-06T18:40:56.563180: step 2192, loss 0.0541706, acc 0.98
2016-09-06T18:40:57.263711: step 2193, loss 0.0167674, acc 1
2016-09-06T18:40:57.952935: step 2194, loss 0.0379444, acc 0.98
2016-09-06T18:40:58.663327: step 2195, loss 0.119826, acc 0.94
2016-09-06T18:40:59.374415: step 2196, loss 0.0714493, acc 0.98
2016-09-06T18:41:00.083561: step 2197, loss 0.0114044, acc 1
2016-09-06T18:41:00.837285: step 2198, loss 0.0380484, acc 0.98
2016-09-06T18:41:01.518181: step 2199, loss 0.0421776, acc 0.96
2016-09-06T18:41:02.194068: step 2200, loss 0.0630874, acc 0.96

Evaluation:
2016-09-06T18:41:05.332616: step 2200, loss 1.37862, acc 0.765478

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-2200

2016-09-06T18:41:07.077439: step 2201, loss 0.198298, acc 0.9
2016-09-06T18:41:07.764919: step 2202, loss 0.103337, acc 0.98
2016-09-06T18:41:08.453537: step 2203, loss 0.00835533, acc 1
2016-09-06T18:41:09.139112: step 2204, loss 0.014994, acc 1
2016-09-06T18:41:09.812313: step 2205, loss 0.0154273, acc 1
2016-09-06T18:41:10.538428: step 2206, loss 0.027149, acc 1
2016-09-06T18:41:11.200078: step 2207, loss 0.0680946, acc 0.96
2016-09-06T18:41:11.871165: step 2208, loss 0.0207371, acc 1
2016-09-06T18:41:12.550037: step 2209, loss 0.0389464, acc 1
2016-09-06T18:41:13.221776: step 2210, loss 0.0446228, acc 1
2016-09-06T18:41:13.901076: step 2211, loss 0.0813156, acc 0.92
2016-09-06T18:41:14.603056: step 2212, loss 0.00718362, acc 1
2016-09-06T18:41:15.310129: step 2213, loss 0.0554964, acc 0.98
2016-09-06T18:41:15.976188: step 2214, loss 0.06838, acc 0.98
2016-09-06T18:41:16.668058: step 2215, loss 0.0676583, acc 0.98
2016-09-06T18:41:17.367101: step 2216, loss 0.0984524, acc 0.98
2016-09-06T18:41:18.042582: step 2217, loss 0.118737, acc 0.98
2016-09-06T18:41:18.727304: step 2218, loss 0.0255198, acc 0.98
2016-09-06T18:41:19.412975: step 2219, loss 0.0582083, acc 0.98
2016-09-06T18:41:20.114151: step 2220, loss 0.0339849, acc 0.98
2016-09-06T18:41:20.797241: step 2221, loss 0.0675693, acc 0.94
2016-09-06T18:41:21.482202: step 2222, loss 0.0583597, acc 0.94
2016-09-06T18:41:22.165976: step 2223, loss 0.0496582, acc 0.96
2016-09-06T18:41:22.855247: step 2224, loss 0.0595734, acc 0.98
2016-09-06T18:41:23.560908: step 2225, loss 0.0207811, acc 0.98
2016-09-06T18:41:24.225563: step 2226, loss 0.0641667, acc 0.96
2016-09-06T18:41:24.961197: step 2227, loss 0.00947845, acc 1
2016-09-06T18:41:25.638699: step 2228, loss 0.00238814, acc 1
2016-09-06T18:41:26.339810: step 2229, loss 0.0279543, acc 1
2016-09-06T18:41:27.027827: step 2230, loss 0.0581682, acc 0.98
2016-09-06T18:41:27.725074: step 2231, loss 0.0524389, acc 0.98
2016-09-06T18:41:28.423861: step 2232, loss 0.0431852, acc 0.98
2016-09-06T18:41:29.075996: step 2233, loss 0.0486452, acc 0.96
2016-09-06T18:41:29.771440: step 2234, loss 0.0116696, acc 1
2016-09-06T18:41:30.459434: step 2235, loss 0.0364699, acc 0.98
2016-09-06T18:41:31.166363: step 2236, loss 0.0234405, acc 1
2016-09-06T18:41:31.856845: step 2237, loss 0.0332681, acc 0.98
2016-09-06T18:41:32.543894: step 2238, loss 0.00862244, acc 1
2016-09-06T18:41:33.238308: step 2239, loss 0.0793972, acc 0.98
2016-09-06T18:41:33.918704: step 2240, loss 0.0958309, acc 0.98
2016-09-06T18:41:34.636180: step 2241, loss 0.0558027, acc 0.98
2016-09-06T18:41:35.327715: step 2242, loss 0.0586931, acc 0.98
2016-09-06T18:41:36.042361: step 2243, loss 0.0556279, acc 1
2016-09-06T18:41:36.721265: step 2244, loss 0.153297, acc 0.94
2016-09-06T18:41:37.419785: step 2245, loss 0.0239046, acc 0.98
2016-09-06T18:41:38.113140: step 2246, loss 0.0239832, acc 0.98
2016-09-06T18:41:38.804311: step 2247, loss 0.0403856, acc 0.98
2016-09-06T18:41:39.493640: step 2248, loss 0.01336, acc 1
2016-09-06T18:41:40.178442: step 2249, loss 0.0892403, acc 0.98
2016-09-06T18:41:40.857116: step 2250, loss 0.015818, acc 1
2016-09-06T18:41:41.531934: step 2251, loss 0.00583631, acc 1
2016-09-06T18:41:42.216865: step 2252, loss 0.0604425, acc 0.96
2016-09-06T18:41:42.906710: step 2253, loss 0.0338718, acc 0.98
2016-09-06T18:41:43.579908: step 2254, loss 0.0905976, acc 0.98
2016-09-06T18:41:44.261000: step 2255, loss 0.0989473, acc 0.96
2016-09-06T18:41:44.947522: step 2256, loss 0.0116771, acc 1
2016-09-06T18:41:45.628560: step 2257, loss 0.0560393, acc 0.96
2016-09-06T18:41:46.339246: step 2258, loss 0.0390185, acc 0.98
2016-09-06T18:41:47.026858: step 2259, loss 0.0479195, acc 0.98
2016-09-06T18:41:47.746569: step 2260, loss 0.0553578, acc 0.98
2016-09-06T18:41:48.420679: step 2261, loss 0.0612972, acc 0.96
2016-09-06T18:41:49.092340: step 2262, loss 0.0455814, acc 0.98
2016-09-06T18:41:49.787335: step 2263, loss 0.141102, acc 0.92
2016-09-06T18:41:50.500170: step 2264, loss 0.0957206, acc 0.96
2016-09-06T18:41:51.201477: step 2265, loss 0.0343587, acc 1
2016-09-06T18:41:51.888198: step 2266, loss 0.126366, acc 0.94
2016-09-06T18:41:52.621665: step 2267, loss 0.08321, acc 0.94
2016-09-06T18:41:53.299223: step 2268, loss 0.0407162, acc 1
2016-09-06T18:41:53.974988: step 2269, loss 0.0676854, acc 0.98
2016-09-06T18:41:54.664214: step 2270, loss 0.0836417, acc 0.94
2016-09-06T18:41:55.353935: step 2271, loss 0.0178244, acc 1
2016-09-06T18:41:56.030163: step 2272, loss 0.0720163, acc 0.98
2016-09-06T18:41:56.705788: step 2273, loss 0.0446481, acc 0.98
2016-09-06T18:41:57.389117: step 2274, loss 0.0579482, acc 0.96
2016-09-06T18:41:58.059746: step 2275, loss 0.0225151, acc 0.98
2016-09-06T18:41:58.739467: step 2276, loss 0.0388764, acc 0.98
2016-09-06T18:41:59.439555: step 2277, loss 0.203466, acc 0.94
2016-09-06T18:42:00.137972: step 2278, loss 0.0806491, acc 0.96
2016-09-06T18:42:00.843030: step 2279, loss 0.0288135, acc 1
2016-09-06T18:42:01.502676: step 2280, loss 0.138473, acc 0.96
2016-09-06T18:42:02.192594: step 2281, loss 0.041189, acc 0.98
2016-09-06T18:42:02.867930: step 2282, loss 0.0726081, acc 0.94
2016-09-06T18:42:03.566905: step 2283, loss 0.0398807, acc 0.96
2016-09-06T18:42:04.253021: step 2284, loss 0.0399458, acc 0.98
2016-09-06T18:42:04.934916: step 2285, loss 0.0476905, acc 0.98
2016-09-06T18:42:05.622527: step 2286, loss 0.133262, acc 0.92
2016-09-06T18:42:06.306034: step 2287, loss 0.0582536, acc 0.96
2016-09-06T18:42:07.010692: step 2288, loss 0.0497443, acc 0.96
2016-09-06T18:42:07.684391: step 2289, loss 0.0728678, acc 0.96
2016-09-06T18:42:08.361980: step 2290, loss 0.0340148, acc 1
2016-09-06T18:42:09.046676: step 2291, loss 0.0365365, acc 0.98
2016-09-06T18:42:09.785352: step 2292, loss 0.0131688, acc 1
2016-09-06T18:42:10.468611: step 2293, loss 0.0311324, acc 0.98
2016-09-06T18:42:11.135569: step 2294, loss 0.0183422, acc 1
2016-09-06T18:42:11.838539: step 2295, loss 0.0498473, acc 0.98
2016-09-06T18:42:12.506295: step 2296, loss 0.029698, acc 0.98
2016-09-06T18:42:13.198058: step 2297, loss 0.122054, acc 0.96
2016-09-06T18:42:13.904697: step 2298, loss 0.00764661, acc 1
2016-09-06T18:42:14.587833: step 2299, loss 0.0303027, acc 1
2016-09-06T18:42:15.279880: step 2300, loss 0.0342482, acc 0.98

Evaluation:
2016-09-06T18:42:18.393740: step 2300, loss 1.32177, acc 0.756098

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-2300

2016-09-06T18:42:20.079999: step 2301, loss 0.0666402, acc 0.98
2016-09-06T18:42:20.747895: step 2302, loss 0.0537275, acc 0.98
2016-09-06T18:42:21.451030: step 2303, loss 0.0933987, acc 0.98
2016-09-06T18:42:22.126055: step 2304, loss 0.0318701, acc 0.977273
2016-09-06T18:42:22.793809: step 2305, loss 0.0102755, acc 1
2016-09-06T18:42:23.462824: step 2306, loss 0.0815112, acc 0.98
2016-09-06T18:42:24.145373: step 2307, loss 0.110563, acc 0.96
2016-09-06T18:42:24.828463: step 2308, loss 0.0553967, acc 1
2016-09-06T18:42:25.527247: step 2309, loss 0.0305771, acc 0.98
2016-09-06T18:42:26.243270: step 2310, loss 0.00349254, acc 1
2016-09-06T18:42:26.913811: step 2311, loss 0.00377871, acc 1
2016-09-06T18:42:27.601655: step 2312, loss 0.0123282, acc 1
2016-09-06T18:42:28.296756: step 2313, loss 0.016227, acc 1
2016-09-06T18:42:28.982058: step 2314, loss 0.0174605, acc 1
2016-09-06T18:42:29.659375: step 2315, loss 0.0471803, acc 0.98
2016-09-06T18:42:30.343481: step 2316, loss 0.0299845, acc 1
2016-09-06T18:42:31.051289: step 2317, loss 0.0367596, acc 0.98
2016-09-06T18:42:31.723790: step 2318, loss 0.0360494, acc 0.98
2016-09-06T18:42:32.426845: step 2319, loss 0.231457, acc 0.94
2016-09-06T18:42:33.124753: step 2320, loss 0.0499776, acc 0.96
2016-09-06T18:42:33.822625: step 2321, loss 0.0517878, acc 0.98
2016-09-06T18:42:34.523335: step 2322, loss 0.0275896, acc 0.98
2016-09-06T18:42:35.197701: step 2323, loss 0.00679889, acc 1
2016-09-06T18:42:35.904173: step 2324, loss 0.0488392, acc 0.98
2016-09-06T18:42:36.583044: step 2325, loss 0.00792271, acc 1
2016-09-06T18:42:37.294325: step 2326, loss 0.031822, acc 1
2016-09-06T18:42:37.965675: step 2327, loss 0.00190952, acc 1
2016-09-06T18:42:38.659423: step 2328, loss 0.0164026, acc 1
2016-09-06T18:42:39.355256: step 2329, loss 0.071429, acc 0.96
2016-09-06T18:42:40.007579: step 2330, loss 0.12306, acc 0.98
2016-09-06T18:42:40.722750: step 2331, loss 0.0183737, acc 1
2016-09-06T18:42:41.380691: step 2332, loss 0.0446623, acc 0.98
2016-09-06T18:42:42.054156: step 2333, loss 0.139886, acc 0.96
2016-09-06T18:42:42.765710: step 2334, loss 0.0313755, acc 0.98
2016-09-06T18:42:43.453456: step 2335, loss 0.020491, acc 1
2016-09-06T18:42:44.156093: step 2336, loss 0.310869, acc 0.92
2016-09-06T18:42:44.815557: step 2337, loss 0.0244945, acc 1
2016-09-06T18:42:45.521114: step 2338, loss 0.088855, acc 0.96
2016-09-06T18:42:46.190116: step 2339, loss 0.052296, acc 0.98
2016-09-06T18:42:46.879459: step 2340, loss 0.0624041, acc 0.98
2016-09-06T18:42:47.574565: step 2341, loss 0.00352672, acc 1
2016-09-06T18:42:48.257917: step 2342, loss 0.0127325, acc 1
2016-09-06T18:42:48.948410: step 2343, loss 0.0415713, acc 0.96
2016-09-06T18:42:49.624344: step 2344, loss 0.0568168, acc 0.96
2016-09-06T18:42:50.326971: step 2345, loss 0.0986431, acc 0.96
2016-09-06T18:42:51.014609: step 2346, loss 0.0279778, acc 1
2016-09-06T18:42:51.710355: step 2347, loss 0.0879313, acc 0.96
2016-09-06T18:42:52.409177: step 2348, loss 0.0635239, acc 0.96
2016-09-06T18:42:53.088951: step 2349, loss 0.0305753, acc 0.98
2016-09-06T18:42:53.773473: step 2350, loss 0.0310975, acc 1
2016-09-06T18:42:54.432128: step 2351, loss 0.0191292, acc 1
2016-09-06T18:42:55.135682: step 2352, loss 0.118251, acc 0.92
2016-09-06T18:42:55.831816: step 2353, loss 0.085813, acc 0.96
2016-09-06T18:42:56.541314: step 2354, loss 0.0114555, acc 1
2016-09-06T18:42:57.258056: step 2355, loss 0.0285297, acc 1
2016-09-06T18:42:57.975116: step 2356, loss 0.0195608, acc 0.98
2016-09-06T18:42:58.696862: step 2357, loss 0.0737092, acc 0.96
2016-09-06T18:42:59.372364: step 2358, loss 0.0129468, acc 1
2016-09-06T18:43:00.048993: step 2359, loss 0.0621539, acc 0.98
2016-09-06T18:43:00.778077: step 2360, loss 0.0194879, acc 1
2016-09-06T18:43:01.481796: step 2361, loss 0.0151678, acc 1
2016-09-06T18:43:02.186295: step 2362, loss 0.0229615, acc 1
2016-09-06T18:43:02.845328: step 2363, loss 0.014248, acc 1
2016-09-06T18:43:03.541301: step 2364, loss 0.0174251, acc 1
2016-09-06T18:43:04.221094: step 2365, loss 0.0749753, acc 0.96
2016-09-06T18:43:04.940591: step 2366, loss 0.0346824, acc 0.98
2016-09-06T18:43:05.602398: step 2367, loss 0.0700374, acc 0.98
2016-09-06T18:43:06.279716: step 2368, loss 0.0535667, acc 0.98
2016-09-06T18:43:06.993447: step 2369, loss 0.0271465, acc 0.98
2016-09-06T18:43:07.662296: step 2370, loss 0.113077, acc 0.98
2016-09-06T18:43:08.365012: step 2371, loss 0.0106932, acc 1
2016-09-06T18:43:09.053651: step 2372, loss 0.0150002, acc 1
2016-09-06T18:43:09.753989: step 2373, loss 0.0695304, acc 0.94
2016-09-06T18:43:10.481790: step 2374, loss 0.0573562, acc 0.98
2016-09-06T18:43:11.165643: step 2375, loss 0.0390379, acc 0.96
2016-09-06T18:43:11.877675: step 2376, loss 0.0296756, acc 1
2016-09-06T18:43:12.550094: step 2377, loss 0.045063, acc 1
2016-09-06T18:43:13.249199: step 2378, loss 0.0204924, acc 0.98
2016-09-06T18:43:13.937065: step 2379, loss 0.190278, acc 0.92
2016-09-06T18:43:14.623457: step 2380, loss 0.0504457, acc 0.98
2016-09-06T18:43:15.313116: step 2381, loss 0.0310891, acc 1
2016-09-06T18:43:16.006749: step 2382, loss 0.0669797, acc 0.96
2016-09-06T18:43:16.734336: step 2383, loss 0.0121639, acc 1
2016-09-06T18:43:17.407339: step 2384, loss 0.0268209, acc 1
2016-09-06T18:43:18.091384: step 2385, loss 0.0831384, acc 0.94
2016-09-06T18:43:18.797668: step 2386, loss 0.0234283, acc 0.98
2016-09-06T18:43:19.503331: step 2387, loss 0.0241197, acc 1
2016-09-06T18:43:20.194878: step 2388, loss 0.0418371, acc 0.98
2016-09-06T18:43:20.844014: step 2389, loss 0.0117082, acc 1
2016-09-06T18:43:21.561946: step 2390, loss 0.0387457, acc 0.98
2016-09-06T18:43:22.223402: step 2391, loss 0.0518098, acc 0.98
2016-09-06T18:43:22.914010: step 2392, loss 0.0294574, acc 1
2016-09-06T18:43:23.606466: step 2393, loss 0.0279505, acc 1
2016-09-06T18:43:24.285535: step 2394, loss 0.105017, acc 0.96
2016-09-06T18:43:24.971144: step 2395, loss 0.0011687, acc 1
2016-09-06T18:43:25.670884: step 2396, loss 0.0986109, acc 0.96
2016-09-06T18:43:26.380842: step 2397, loss 0.0245134, acc 0.98
2016-09-06T18:43:27.064460: step 2398, loss 0.0296972, acc 1
2016-09-06T18:43:27.750329: step 2399, loss 0.0709687, acc 0.96
2016-09-06T18:43:28.441867: step 2400, loss 0.0289505, acc 1

Evaluation:
2016-09-06T18:43:31.606383: step 2400, loss 1.5044, acc 0.772045

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-2400

2016-09-06T18:43:33.235387: step 2401, loss 0.030241, acc 0.98
2016-09-06T18:43:33.929220: step 2402, loss 0.0237701, acc 1
2016-09-06T18:43:34.622627: step 2403, loss 0.0477251, acc 0.98
2016-09-06T18:43:35.280097: step 2404, loss 0.0560497, acc 0.96
2016-09-06T18:43:35.996114: step 2405, loss 0.175974, acc 0.96
2016-09-06T18:43:36.658352: step 2406, loss 0.0723693, acc 0.96
2016-09-06T18:43:37.347092: step 2407, loss 0.0908879, acc 0.94
2016-09-06T18:43:38.023817: step 2408, loss 0.0534719, acc 0.96
2016-09-06T18:43:38.722942: step 2409, loss 0.054087, acc 0.98
2016-09-06T18:43:39.421080: step 2410, loss 0.0579123, acc 0.96
2016-09-06T18:43:40.091156: step 2411, loss 0.13766, acc 0.96
2016-09-06T18:43:40.793004: step 2412, loss 0.0135973, acc 1
2016-09-06T18:43:41.470299: step 2413, loss 0.0275577, acc 0.98
2016-09-06T18:43:42.168809: step 2414, loss 0.0456886, acc 0.98
2016-09-06T18:43:42.875513: step 2415, loss 0.181742, acc 0.98
2016-09-06T18:43:43.555807: step 2416, loss 0.0293397, acc 0.98
2016-09-06T18:43:44.289933: step 2417, loss 0.086041, acc 0.94
2016-09-06T18:43:44.961887: step 2418, loss 0.0883384, acc 0.96
2016-09-06T18:43:45.659548: step 2419, loss 0.0601655, acc 0.96
2016-09-06T18:43:46.350390: step 2420, loss 0.0323913, acc 0.98
2016-09-06T18:43:47.049850: step 2421, loss 0.0332414, acc 0.98
2016-09-06T18:43:47.742561: step 2422, loss 0.0425742, acc 0.98
2016-09-06T18:43:48.426126: step 2423, loss 0.0877884, acc 0.98
2016-09-06T18:43:49.128137: step 2424, loss 0.0356335, acc 0.96
2016-09-06T18:43:49.799103: step 2425, loss 0.0374927, acc 0.98
2016-09-06T18:43:50.511027: step 2426, loss 0.0568452, acc 0.96
2016-09-06T18:43:51.216178: step 2427, loss 0.0293019, acc 0.98
2016-09-06T18:43:51.906983: step 2428, loss 0.0435733, acc 0.96
2016-09-06T18:43:52.580367: step 2429, loss 0.0331021, acc 0.98
2016-09-06T18:43:53.249761: step 2430, loss 0.0133636, acc 1
2016-09-06T18:43:53.929186: step 2431, loss 0.0403771, acc 0.98
2016-09-06T18:43:54.601642: step 2432, loss 0.0280223, acc 0.98
2016-09-06T18:43:55.287397: step 2433, loss 0.062574, acc 0.98
2016-09-06T18:43:56.004353: step 2434, loss 0.0172793, acc 1
2016-09-06T18:43:56.700595: step 2435, loss 0.0529242, acc 0.98
2016-09-06T18:43:57.391170: step 2436, loss 0.118708, acc 0.92
2016-09-06T18:43:58.092444: step 2437, loss 0.117191, acc 0.92
2016-09-06T18:43:58.797345: step 2438, loss 0.0700752, acc 0.96
2016-09-06T18:43:59.499004: step 2439, loss 0.0159255, acc 1
2016-09-06T18:44:00.189465: step 2440, loss 0.0689814, acc 0.94
2016-09-06T18:44:00.895571: step 2441, loss 0.0461386, acc 0.98
2016-09-06T18:44:01.575129: step 2442, loss 0.0119887, acc 1
2016-09-06T18:44:02.254249: step 2443, loss 0.0291255, acc 1
2016-09-06T18:44:02.933423: step 2444, loss 0.0786295, acc 0.98
2016-09-06T18:44:03.655480: step 2445, loss 0.0161816, acc 1
2016-09-06T18:44:04.351211: step 2446, loss 0.0293341, acc 0.98
2016-09-06T18:44:05.052613: step 2447, loss 0.0743255, acc 0.98
2016-09-06T18:44:05.740939: step 2448, loss 0.0201228, acc 1
2016-09-06T18:44:06.424226: step 2449, loss 0.070978, acc 0.96
2016-09-06T18:44:07.091306: step 2450, loss 0.128244, acc 0.94
2016-09-06T18:44:07.763214: step 2451, loss 0.0427333, acc 0.96
2016-09-06T18:44:08.454038: step 2452, loss 0.113711, acc 0.96
2016-09-06T18:44:09.154422: step 2453, loss 0.0506131, acc 0.96
2016-09-06T18:44:09.839435: step 2454, loss 0.0153301, acc 1
2016-09-06T18:44:10.538557: step 2455, loss 0.0585332, acc 0.96
2016-09-06T18:44:11.231382: step 2456, loss 0.121244, acc 0.94
2016-09-06T18:44:11.914455: step 2457, loss 0.0608178, acc 0.98
2016-09-06T18:44:12.576259: step 2458, loss 0.0392682, acc 1
2016-09-06T18:44:13.285380: step 2459, loss 0.0349334, acc 1
2016-09-06T18:44:13.962961: step 2460, loss 0.0185023, acc 1
2016-09-06T18:44:14.640900: step 2461, loss 0.0572147, acc 0.96
2016-09-06T18:44:15.324448: step 2462, loss 0.039561, acc 0.98
2016-09-06T18:44:16.017782: step 2463, loss 0.000761545, acc 1
2016-09-06T18:44:16.703609: step 2464, loss 0.045176, acc 1
2016-09-06T18:44:17.358103: step 2465, loss 0.0191817, acc 1
2016-09-06T18:44:18.057761: step 2466, loss 0.0966344, acc 0.94
2016-09-06T18:44:18.729828: step 2467, loss 0.141417, acc 0.96
2016-09-06T18:44:19.409078: step 2468, loss 0.150105, acc 0.94
2016-09-06T18:44:20.103399: step 2469, loss 0.383831, acc 0.98
2016-09-06T18:44:20.792077: step 2470, loss 0.063867, acc 0.96
2016-09-06T18:44:21.501002: step 2471, loss 0.0394733, acc 0.98
2016-09-06T18:44:22.190573: step 2472, loss 0.0157298, acc 1
2016-09-06T18:44:22.911016: step 2473, loss 0.0815651, acc 0.96
2016-09-06T18:44:23.595844: step 2474, loss 0.0822747, acc 0.94
2016-09-06T18:44:24.272398: step 2475, loss 0.0113572, acc 1
2016-09-06T18:44:24.961392: step 2476, loss 0.04575, acc 0.98
2016-09-06T18:44:25.641824: step 2477, loss 0.0479854, acc 0.98
2016-09-06T18:44:26.340046: step 2478, loss 0.0442978, acc 0.98
2016-09-06T18:44:27.007799: step 2479, loss 0.0593487, acc 0.98
2016-09-06T18:44:27.717571: step 2480, loss 0.113032, acc 0.96
2016-09-06T18:44:28.383809: step 2481, loss 0.0370963, acc 0.98
2016-09-06T18:44:29.099332: step 2482, loss 0.0807345, acc 0.94
2016-09-06T18:44:29.812097: step 2483, loss 0.0443034, acc 0.98
2016-09-06T18:44:30.511711: step 2484, loss 0.0099796, acc 1
2016-09-06T18:44:31.210949: step 2485, loss 0.00716156, acc 1
2016-09-06T18:44:31.879814: step 2486, loss 0.0236116, acc 1
2016-09-06T18:44:32.602198: step 2487, loss 0.0111557, acc 1
2016-09-06T18:44:33.305824: step 2488, loss 0.0299111, acc 0.98
2016-09-06T18:44:33.994101: step 2489, loss 0.0411257, acc 0.96
2016-09-06T18:44:34.687254: step 2490, loss 0.045252, acc 0.96
2016-09-06T18:44:35.359079: step 2491, loss 0.0894082, acc 0.96
2016-09-06T18:44:36.065821: step 2492, loss 0.054928, acc 0.96
2016-09-06T18:44:36.746070: step 2493, loss 0.0173558, acc 1
2016-09-06T18:44:37.437380: step 2494, loss 0.0335953, acc 0.98
2016-09-06T18:44:38.132800: step 2495, loss 0.082513, acc 0.94
2016-09-06T18:44:38.770609: step 2496, loss 0.00544038, acc 1
2016-09-06T18:44:39.451555: step 2497, loss 0.0413767, acc 0.98
2016-09-06T18:44:40.114108: step 2498, loss 0.0247473, acc 1
2016-09-06T18:44:40.811602: step 2499, loss 0.075599, acc 0.98
2016-09-06T18:44:41.465949: step 2500, loss 0.0275005, acc 0.98

Evaluation:
2016-09-06T18:44:44.605715: step 2500, loss 1.25587, acc 0.758912

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-2500

2016-09-06T18:44:46.231126: step 2501, loss 0.0644223, acc 0.98
2016-09-06T18:44:46.968003: step 2502, loss 0.0164549, acc 1
2016-09-06T18:44:47.656966: step 2503, loss 0.0113154, acc 1
2016-09-06T18:44:48.348464: step 2504, loss 0.00701117, acc 1
2016-09-06T18:44:49.017272: step 2505, loss 0.090802, acc 0.96
2016-09-06T18:44:49.695482: step 2506, loss 0.0448538, acc 1
2016-09-06T18:44:50.378891: step 2507, loss 0.172971, acc 0.92
2016-09-06T18:44:51.038352: step 2508, loss 0.0190141, acc 1
2016-09-06T18:44:51.747357: step 2509, loss 0.0137668, acc 1
2016-09-06T18:44:52.396602: step 2510, loss 0.0468918, acc 0.96
2016-09-06T18:44:53.066379: step 2511, loss 0.0551737, acc 0.96
2016-09-06T18:44:53.736292: step 2512, loss 0.0124541, acc 1
2016-09-06T18:44:54.458681: step 2513, loss 0.0449853, acc 0.98
2016-09-06T18:44:55.148340: step 2514, loss 0.00411723, acc 1
2016-09-06T18:44:55.841472: step 2515, loss 0.0503576, acc 0.96
2016-09-06T18:44:56.537796: step 2516, loss 0.057305, acc 0.96
2016-09-06T18:44:57.218746: step 2517, loss 0.120871, acc 0.96
2016-09-06T18:44:57.890847: step 2518, loss 0.00500117, acc 1
2016-09-06T18:44:58.562242: step 2519, loss 0.0666796, acc 0.96
2016-09-06T18:44:59.263682: step 2520, loss 0.0671314, acc 0.98
2016-09-06T18:44:59.956956: step 2521, loss 0.173962, acc 0.98
2016-09-06T18:45:00.654954: step 2522, loss 0.113112, acc 0.94
2016-09-06T18:45:01.347983: step 2523, loss 0.143663, acc 0.96
2016-09-06T18:45:02.040215: step 2524, loss 0.0437742, acc 0.98
2016-09-06T18:45:02.721835: step 2525, loss 0.0510604, acc 0.98
2016-09-06T18:45:03.388424: step 2526, loss 0.0356702, acc 0.98
2016-09-06T18:45:04.096746: step 2527, loss 0.0147542, acc 1
2016-09-06T18:45:04.787717: step 2528, loss 0.0418135, acc 0.98
2016-09-06T18:45:05.458387: step 2529, loss 0.0509643, acc 0.98
2016-09-06T18:45:06.164054: step 2530, loss 0.016463, acc 1
2016-09-06T18:45:06.833325: step 2531, loss 0.0398467, acc 0.98
2016-09-06T18:45:07.516024: step 2532, loss 0.0245048, acc 1
2016-09-06T18:45:08.209540: step 2533, loss 0.00505041, acc 1
2016-09-06T18:45:08.889425: step 2534, loss 0.0375267, acc 0.98
2016-09-06T18:45:09.608940: step 2535, loss 0.108459, acc 0.96
2016-09-06T18:45:10.308728: step 2536, loss 0.0417095, acc 1
2016-09-06T18:45:11.028193: step 2537, loss 0.0484284, acc 0.98
2016-09-06T18:45:11.705211: step 2538, loss 0.099619, acc 0.96
2016-09-06T18:45:12.397282: step 2539, loss 0.0126923, acc 1
2016-09-06T18:45:13.069614: step 2540, loss 0.051464, acc 0.98
2016-09-06T18:45:13.765027: step 2541, loss 0.0174337, acc 1
2016-09-06T18:45:14.475685: step 2542, loss 0.0581433, acc 0.98
2016-09-06T18:45:15.179733: step 2543, loss 0.0322053, acc 0.98
2016-09-06T18:45:15.903831: step 2544, loss 0.118667, acc 0.98
2016-09-06T18:45:16.577855: step 2545, loss 0.0283471, acc 1
2016-09-06T18:45:17.279285: step 2546, loss 0.00425354, acc 1
2016-09-06T18:45:17.967847: step 2547, loss 0.149823, acc 0.96
2016-09-06T18:45:18.680621: step 2548, loss 0.0175437, acc 1
2016-09-06T18:45:19.407456: step 2549, loss 0.0406701, acc 0.98
2016-09-06T18:45:20.105635: step 2550, loss 0.133306, acc 0.96
2016-09-06T18:45:20.791224: step 2551, loss 0.0535988, acc 0.94
2016-09-06T18:45:21.478070: step 2552, loss 0.0289849, acc 0.98
2016-09-06T18:45:22.160153: step 2553, loss 0.0431515, acc 0.98
2016-09-06T18:45:22.849847: step 2554, loss 0.0490707, acc 0.98
2016-09-06T18:45:23.505799: step 2555, loss 0.0297743, acc 0.98
2016-09-06T18:45:24.207911: step 2556, loss 0.0311964, acc 0.98
2016-09-06T18:45:24.901136: step 2557, loss 0.0206698, acc 0.98
2016-09-06T18:45:25.597375: step 2558, loss 0.0317068, acc 0.98
2016-09-06T18:45:26.280202: step 2559, loss 0.0476999, acc 0.98
2016-09-06T18:45:26.966505: step 2560, loss 0.00861409, acc 1
2016-09-06T18:45:27.639549: step 2561, loss 0.0298181, acc 0.98
2016-09-06T18:45:28.294707: step 2562, loss 0.00770189, acc 1
2016-09-06T18:45:28.987470: step 2563, loss 0.0335509, acc 0.98
2016-09-06T18:45:29.653005: step 2564, loss 0.133493, acc 0.96
2016-09-06T18:45:30.356647: step 2565, loss 0.063932, acc 0.98
2016-09-06T18:45:31.053092: step 2566, loss 0.00899291, acc 1
2016-09-06T18:45:31.738065: step 2567, loss 0.013113, acc 1
2016-09-06T18:45:32.416047: step 2568, loss 0.00414554, acc 1
2016-09-06T18:45:33.094145: step 2569, loss 0.0289026, acc 0.98
2016-09-06T18:45:33.806026: step 2570, loss 0.0375578, acc 0.98
2016-09-06T18:45:34.470127: step 2571, loss 0.0262871, acc 0.98
2016-09-06T18:45:35.165408: step 2572, loss 0.0468663, acc 0.96
2016-09-06T18:45:35.860640: step 2573, loss 0.048599, acc 0.98
2016-09-06T18:45:36.564776: step 2574, loss 0.0152393, acc 1
2016-09-06T18:45:37.261074: step 2575, loss 0.0121095, acc 1
2016-09-06T18:45:37.926998: step 2576, loss 0.0249298, acc 1
2016-09-06T18:45:38.634467: step 2577, loss 0.0253025, acc 0.98
2016-09-06T18:45:39.306022: step 2578, loss 0.0784567, acc 0.98
2016-09-06T18:45:40.009028: step 2579, loss 0.139577, acc 0.96
2016-09-06T18:45:40.686397: step 2580, loss 0.0335793, acc 0.98
2016-09-06T18:45:41.366113: step 2581, loss 0.0389599, acc 0.98
2016-09-06T18:45:42.041535: step 2582, loss 0.0735145, acc 0.96
2016-09-06T18:45:42.709145: step 2583, loss 0.0212369, acc 0.98
2016-09-06T18:45:43.404938: step 2584, loss 0.018323, acc 1
2016-09-06T18:45:44.074613: step 2585, loss 0.142432, acc 0.98
2016-09-06T18:45:44.753085: step 2586, loss 0.0391487, acc 0.98
2016-09-06T18:45:45.442831: step 2587, loss 0.0109392, acc 1
2016-09-06T18:45:46.123151: step 2588, loss 0.0871252, acc 0.96
2016-09-06T18:45:46.827727: step 2589, loss 0.00458213, acc 1
2016-09-06T18:45:47.510723: step 2590, loss 0.0344559, acc 0.98
2016-09-06T18:45:48.216480: step 2591, loss 0.0702568, acc 0.96
2016-09-06T18:45:48.895850: step 2592, loss 0.0506907, acc 0.98
2016-09-06T18:45:49.586711: step 2593, loss 0.0652717, acc 0.98
2016-09-06T18:45:50.278465: step 2594, loss 0.0334226, acc 0.98
2016-09-06T18:45:50.968386: step 2595, loss 0.0405241, acc 0.98
2016-09-06T18:45:51.640850: step 2596, loss 0.0139275, acc 1
2016-09-06T18:45:52.339388: step 2597, loss 0.0244421, acc 0.98
2016-09-06T18:45:53.056243: step 2598, loss 0.0525059, acc 0.96
2016-09-06T18:45:53.756101: step 2599, loss 0.052126, acc 0.96
2016-09-06T18:45:54.432661: step 2600, loss 0.01577, acc 1

Evaluation:
2016-09-06T18:45:57.585644: step 2600, loss 1.46257, acc 0.746717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-2600

2016-09-06T18:45:59.291529: step 2601, loss 0.0639335, acc 0.96
2016-09-06T18:45:59.980950: step 2602, loss 0.0470537, acc 0.98
2016-09-06T18:46:00.691555: step 2603, loss 0.0213754, acc 1
2016-09-06T18:46:01.396030: step 2604, loss 0.109655, acc 0.94
2016-09-06T18:46:02.085445: step 2605, loss 0.113189, acc 0.94
2016-09-06T18:46:02.809074: step 2606, loss 0.137404, acc 0.92
2016-09-06T18:46:03.508922: step 2607, loss 0.0394244, acc 0.98
2016-09-06T18:46:04.185882: step 2608, loss 0.0019699, acc 1
2016-09-06T18:46:04.858879: step 2609, loss 0.0208536, acc 1
2016-09-06T18:46:05.533227: step 2610, loss 0.0124872, acc 1
2016-09-06T18:46:06.231663: step 2611, loss 0.0223965, acc 1
2016-09-06T18:46:06.889663: step 2612, loss 0.0293787, acc 1
2016-09-06T18:46:07.589378: step 2613, loss 0.0156335, acc 0.98
2016-09-06T18:46:08.279330: step 2614, loss 0.105198, acc 0.94
2016-09-06T18:46:08.980050: step 2615, loss 0.0112307, acc 1
2016-09-06T18:46:09.677665: step 2616, loss 0.0328765, acc 0.98
2016-09-06T18:46:10.384712: step 2617, loss 0.0748641, acc 0.96
2016-09-06T18:46:11.080530: step 2618, loss 0.0262688, acc 1
2016-09-06T18:46:11.768715: step 2619, loss 0.00158833, acc 1
2016-09-06T18:46:12.447950: step 2620, loss 0.020455, acc 1
2016-09-06T18:46:13.133733: step 2621, loss 0.0102957, acc 1
2016-09-06T18:46:13.810334: step 2622, loss 0.0113604, acc 1
2016-09-06T18:46:14.502160: step 2623, loss 0.0222951, acc 0.98
2016-09-06T18:46:15.183046: step 2624, loss 0.058585, acc 0.98
2016-09-06T18:46:15.877988: step 2625, loss 0.0572641, acc 0.96
2016-09-06T18:46:16.555935: step 2626, loss 0.0538026, acc 0.98
2016-09-06T18:46:17.240764: step 2627, loss 0.0396615, acc 0.98
2016-09-06T18:46:17.935366: step 2628, loss 0.0663242, acc 0.96
2016-09-06T18:46:18.618285: step 2629, loss 0.0302747, acc 0.98
2016-09-06T18:46:19.296832: step 2630, loss 0.00531369, acc 1
2016-09-06T18:46:19.987179: step 2631, loss 0.0588118, acc 0.96
2016-09-06T18:46:20.714635: step 2632, loss 0.0266454, acc 1
2016-09-06T18:46:21.391861: step 2633, loss 0.0562342, acc 0.98
2016-09-06T18:46:22.104329: step 2634, loss 0.0224615, acc 0.98
2016-09-06T18:46:22.798559: step 2635, loss 0.0236038, acc 1
2016-09-06T18:46:23.479509: step 2636, loss 0.0574003, acc 0.96
2016-09-06T18:46:24.174982: step 2637, loss 0.0705923, acc 0.98
2016-09-06T18:46:24.862011: step 2638, loss 0.029514, acc 1
2016-09-06T18:46:25.590366: step 2639, loss 0.0283774, acc 0.98
2016-09-06T18:46:26.265678: step 2640, loss 0.0371972, acc 0.98
2016-09-06T18:46:26.950525: step 2641, loss 0.00621142, acc 1
2016-09-06T18:46:27.645932: step 2642, loss 0.0175576, acc 1
2016-09-06T18:46:28.336612: step 2643, loss 0.0136563, acc 1
2016-09-06T18:46:29.043705: step 2644, loss 0.0319996, acc 0.98
2016-09-06T18:46:29.727051: step 2645, loss 0.0521184, acc 0.98
2016-09-06T18:46:30.432336: step 2646, loss 0.00482263, acc 1
2016-09-06T18:46:31.103514: step 2647, loss 0.0314515, acc 0.98
2016-09-06T18:46:31.786986: step 2648, loss 0.0577416, acc 0.96
2016-09-06T18:46:32.466523: step 2649, loss 0.013526, acc 1
2016-09-06T18:46:33.141711: step 2650, loss 0.060027, acc 0.96
2016-09-06T18:46:33.839900: step 2651, loss 0.0629937, acc 0.96
2016-09-06T18:46:34.519151: step 2652, loss 0.0102393, acc 1
2016-09-06T18:46:35.233271: step 2653, loss 0.0989784, acc 0.98
2016-09-06T18:46:35.921811: step 2654, loss 0.0242493, acc 0.98
2016-09-06T18:46:36.627824: step 2655, loss 0.0209555, acc 0.98
2016-09-06T18:46:37.301124: step 2656, loss 0.0303589, acc 0.98
2016-09-06T18:46:38.000660: step 2657, loss 0.0407337, acc 0.96
2016-09-06T18:46:38.703189: step 2658, loss 0.0758504, acc 0.98
2016-09-06T18:46:39.397987: step 2659, loss 0.0271373, acc 1
2016-09-06T18:46:40.076144: step 2660, loss 0.0395372, acc 0.98
2016-09-06T18:46:40.771035: step 2661, loss 0.0324379, acc 0.98
2016-09-06T18:46:41.451091: step 2662, loss 0.0196727, acc 1
2016-09-06T18:46:42.132612: step 2663, loss 0.0081412, acc 1
2016-09-06T18:46:42.807338: step 2664, loss 0.100783, acc 0.96
2016-09-06T18:46:43.511035: step 2665, loss 0.203387, acc 0.92
2016-09-06T18:46:44.186404: step 2666, loss 0.073659, acc 0.96
2016-09-06T18:46:44.899795: step 2667, loss 0.00147519, acc 1
2016-09-06T18:46:45.587186: step 2668, loss 0.00817845, acc 1
2016-09-06T18:46:46.265201: step 2669, loss 0.0159158, acc 1
2016-09-06T18:46:46.975498: step 2670, loss 0.0433367, acc 0.98
2016-09-06T18:46:47.675298: step 2671, loss 0.0231681, acc 0.98
2016-09-06T18:46:48.374530: step 2672, loss 0.0277356, acc 0.98
2016-09-06T18:46:49.051306: step 2673, loss 0.152019, acc 0.94
2016-09-06T18:46:49.737946: step 2674, loss 0.0112102, acc 1
2016-09-06T18:46:50.416187: step 2675, loss 0.0138464, acc 1
2016-09-06T18:46:51.113212: step 2676, loss 0.0606269, acc 0.98
2016-09-06T18:46:51.806471: step 2677, loss 0.076048, acc 0.96
2016-09-06T18:46:52.503428: step 2678, loss 0.00388856, acc 1
2016-09-06T18:46:53.218099: step 2679, loss 0.0148369, acc 1
2016-09-06T18:46:53.902580: step 2680, loss 0.196415, acc 0.96
2016-09-06T18:46:54.593205: step 2681, loss 0.143227, acc 0.92
2016-09-06T18:46:55.268993: step 2682, loss 0.0502374, acc 0.98
2016-09-06T18:46:55.951172: step 2683, loss 0.0383286, acc 0.96
2016-09-06T18:46:56.622981: step 2684, loss 0.0443845, acc 0.96
2016-09-06T18:46:57.279150: step 2685, loss 0.0124733, acc 1
2016-09-06T18:46:57.991361: step 2686, loss 0.0968019, acc 0.96
2016-09-06T18:46:58.675742: step 2687, loss 0.168892, acc 0.96
2016-09-06T18:46:59.325713: step 2688, loss 0.0561913, acc 0.977273
2016-09-06T18:47:00.024031: step 2689, loss 0.0220298, acc 1
2016-09-06T18:47:00.758684: step 2690, loss 0.0459144, acc 0.98
2016-09-06T18:47:01.448828: step 2691, loss 0.0464547, acc 0.98
2016-09-06T18:47:02.123262: step 2692, loss 0.0213225, acc 0.98
2016-09-06T18:47:02.816883: step 2693, loss 0.0139793, acc 1
2016-09-06T18:47:03.490137: step 2694, loss 0.108315, acc 0.94
2016-09-06T18:47:04.174255: step 2695, loss 0.00291059, acc 1
2016-09-06T18:47:04.864833: step 2696, loss 0.0420332, acc 0.98
2016-09-06T18:47:05.570673: step 2697, loss 0.0559164, acc 0.98
2016-09-06T18:47:06.270756: step 2698, loss 0.0144224, acc 1
2016-09-06T18:47:06.949985: step 2699, loss 0.0919774, acc 0.94
2016-09-06T18:47:07.643063: step 2700, loss 0.0296126, acc 0.98

Evaluation:
2016-09-06T18:47:10.808492: step 2700, loss 1.27485, acc 0.749531

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-2700

2016-09-06T18:47:12.480184: step 2701, loss 0.0202846, acc 1
2016-09-06T18:47:13.174809: step 2702, loss 0.0217252, acc 1
2016-09-06T18:47:13.875425: step 2703, loss 0.0171964, acc 1
2016-09-06T18:47:14.564300: step 2704, loss 0.029794, acc 0.98
2016-09-06T18:47:15.254934: step 2705, loss 0.0733974, acc 0.98
2016-09-06T18:47:15.955652: step 2706, loss 0.0303285, acc 0.98
2016-09-06T18:47:16.616089: step 2707, loss 0.00230292, acc 1
2016-09-06T18:47:17.301459: step 2708, loss 0.0280809, acc 1
2016-09-06T18:47:18.021978: step 2709, loss 0.0486158, acc 0.96
2016-09-06T18:47:18.714064: step 2710, loss 0.0168611, acc 0.98
2016-09-06T18:47:19.390239: step 2711, loss 0.0134048, acc 1
2016-09-06T18:47:20.097770: step 2712, loss 0.0355338, acc 0.98
2016-09-06T18:47:20.812948: step 2713, loss 0.0153289, acc 1
2016-09-06T18:47:21.504309: step 2714, loss 0.00706262, acc 1
2016-09-06T18:47:22.205457: step 2715, loss 0.0387482, acc 0.98
2016-09-06T18:47:22.884842: step 2716, loss 0.0132054, acc 1
2016-09-06T18:47:23.578151: step 2717, loss 0.0558248, acc 0.96
2016-09-06T18:47:24.273354: step 2718, loss 0.0558766, acc 0.98
2016-09-06T18:47:24.946452: step 2719, loss 0.02521, acc 1
2016-09-06T18:47:25.649070: step 2720, loss 0.0347152, acc 0.98
2016-09-06T18:47:26.316368: step 2721, loss 0.0308296, acc 0.98
2016-09-06T18:47:26.991509: step 2722, loss 0.0824026, acc 0.94
2016-09-06T18:47:27.666576: step 2723, loss 0.0869017, acc 0.94
2016-09-06T18:47:28.371250: step 2724, loss 0.0264116, acc 1
2016-09-06T18:47:29.086303: step 2725, loss 0.00369171, acc 1
2016-09-06T18:47:29.743173: step 2726, loss 0.0703743, acc 0.96
2016-09-06T18:47:30.455831: step 2727, loss 0.0188049, acc 1
2016-09-06T18:47:31.146292: step 2728, loss 0.0354055, acc 0.98
2016-09-06T18:47:31.840595: step 2729, loss 0.0450093, acc 0.96
2016-09-06T18:47:32.528251: step 2730, loss 0.0116721, acc 1
2016-09-06T18:47:33.208540: step 2731, loss 0.0265031, acc 0.98
2016-09-06T18:47:33.919389: step 2732, loss 0.0381799, acc 0.98
2016-09-06T18:47:34.590471: step 2733, loss 0.0287475, acc 0.98
2016-09-06T18:47:35.313509: step 2734, loss 0.0798166, acc 0.96
2016-09-06T18:47:35.989187: step 2735, loss 0.0287866, acc 0.98
2016-09-06T18:47:36.683541: step 2736, loss 0.0449497, acc 0.98
2016-09-06T18:47:37.380353: step 2737, loss 0.0140131, acc 1
2016-09-06T18:47:38.057225: step 2738, loss 0.0210533, acc 1
2016-09-06T18:47:38.753154: step 2739, loss 0.0257277, acc 1
2016-09-06T18:47:39.430451: step 2740, loss 0.0132138, acc 1
2016-09-06T18:47:40.127683: step 2741, loss 0.00263965, acc 1
2016-09-06T18:47:40.822186: step 2742, loss 0.0345428, acc 0.98
2016-09-06T18:47:41.507374: step 2743, loss 0.0881838, acc 0.94
2016-09-06T18:47:42.175236: step 2744, loss 0.0158591, acc 1
2016-09-06T18:47:42.879332: step 2745, loss 0.0109027, acc 1
2016-09-06T18:47:43.598671: step 2746, loss 0.037158, acc 1
2016-09-06T18:47:44.252228: step 2747, loss 0.0361997, acc 0.98
2016-09-06T18:47:44.957557: step 2748, loss 0.0280607, acc 0.98
2016-09-06T18:47:45.673245: step 2749, loss 0.0145067, acc 1
2016-09-06T18:47:46.351907: step 2750, loss 0.0940479, acc 0.96
2016-09-06T18:47:47.031964: step 2751, loss 0.086401, acc 0.98
2016-09-06T18:47:47.724676: step 2752, loss 0.108706, acc 0.94
2016-09-06T18:47:48.436643: step 2753, loss 0.0255954, acc 1
2016-09-06T18:47:49.121570: step 2754, loss 0.00614895, acc 1
2016-09-06T18:47:49.827911: step 2755, loss 0.00359673, acc 1
2016-09-06T18:47:50.510955: step 2756, loss 0.0250624, acc 1
2016-09-06T18:47:51.213755: step 2757, loss 0.0361813, acc 0.98
2016-09-06T18:47:51.910382: step 2758, loss 0.0696071, acc 0.94
2016-09-06T18:47:52.570645: step 2759, loss 0.0259491, acc 1
2016-09-06T18:47:53.283101: step 2760, loss 0.0118558, acc 1
2016-09-06T18:47:53.957677: step 2761, loss 0.0371323, acc 0.98
2016-09-06T18:47:54.651130: step 2762, loss 0.0594559, acc 0.96
2016-09-06T18:47:55.355248: step 2763, loss 0.0447758, acc 0.98
2016-09-06T18:47:56.051028: step 2764, loss 0.115685, acc 0.94
2016-09-06T18:47:56.740818: step 2765, loss 0.0850347, acc 0.94
2016-09-06T18:47:57.398390: step 2766, loss 0.0578969, acc 0.96
2016-09-06T18:47:58.108153: step 2767, loss 0.0666673, acc 0.96
2016-09-06T18:47:58.788111: step 2768, loss 0.00913865, acc 1
2016-09-06T18:47:59.461973: step 2769, loss 0.013196, acc 1
2016-09-06T18:48:00.151882: step 2770, loss 0.0177131, acc 0.98
2016-09-06T18:48:00.894814: step 2771, loss 0.0424784, acc 0.96
2016-09-06T18:48:01.607577: step 2772, loss 0.0558465, acc 0.98
2016-09-06T18:48:02.291309: step 2773, loss 0.0472306, acc 0.98
2016-09-06T18:48:03.016171: step 2774, loss 0.0714162, acc 0.98
2016-09-06T18:48:03.693113: step 2775, loss 0.0990975, acc 0.98
2016-09-06T18:48:04.417231: step 2776, loss 0.0247898, acc 0.98
2016-09-06T18:48:05.102273: step 2777, loss 0.0367074, acc 1
2016-09-06T18:48:05.795408: step 2778, loss 0.0482212, acc 0.98
2016-09-06T18:48:06.504687: step 2779, loss 0.0177405, acc 1
2016-09-06T18:48:07.194183: step 2780, loss 0.0557806, acc 0.94
2016-09-06T18:48:07.875670: step 2781, loss 0.0418553, acc 0.98
2016-09-06T18:48:08.576541: step 2782, loss 0.0706158, acc 0.96
2016-09-06T18:48:09.267975: step 2783, loss 0.0634946, acc 0.98
2016-09-06T18:48:09.966471: step 2784, loss 0.0791387, acc 0.98
2016-09-06T18:48:10.629775: step 2785, loss 0.0398079, acc 0.96
2016-09-06T18:48:11.352094: step 2786, loss 0.0755028, acc 0.96
2016-09-06T18:48:12.038497: step 2787, loss 0.0318139, acc 1
2016-09-06T18:48:12.737865: step 2788, loss 0.0982396, acc 0.98
2016-09-06T18:48:13.409643: step 2789, loss 0.0507585, acc 0.96
2016-09-06T18:48:14.098168: step 2790, loss 0.00170937, acc 1
2016-09-06T18:48:14.800707: step 2791, loss 0.0862352, acc 0.92
2016-09-06T18:48:15.488608: step 2792, loss 0.0523036, acc 0.98
2016-09-06T18:48:16.205159: step 2793, loss 0.0225361, acc 1
2016-09-06T18:48:16.885216: step 2794, loss 0.00475306, acc 1
2016-09-06T18:48:17.564063: step 2795, loss 0.0378282, acc 0.96
2016-09-06T18:48:18.251633: step 2796, loss 0.0407144, acc 0.98
2016-09-06T18:48:18.932897: step 2797, loss 0.00728918, acc 1
2016-09-06T18:48:19.652462: step 2798, loss 0.0160632, acc 1
2016-09-06T18:48:20.356677: step 2799, loss 0.0642975, acc 0.96
2016-09-06T18:48:21.039849: step 2800, loss 0.0702098, acc 0.98

Evaluation:
2016-09-06T18:48:24.157681: step 2800, loss 1.473, acc 0.756098

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-2800

2016-09-06T18:48:25.814252: step 2801, loss 0.0107211, acc 1
2016-09-06T18:48:26.496650: step 2802, loss 0.0698389, acc 0.96
2016-09-06T18:48:27.169971: step 2803, loss 0.0641405, acc 0.96
2016-09-06T18:48:27.843317: step 2804, loss 0.00813259, acc 1
2016-09-06T18:48:28.537046: step 2805, loss 0.00279704, acc 1
2016-09-06T18:48:29.222759: step 2806, loss 0.0315511, acc 1
2016-09-06T18:48:29.899947: step 2807, loss 0.0848678, acc 0.96
2016-09-06T18:48:30.608124: step 2808, loss 0.044826, acc 0.96
2016-09-06T18:48:31.282678: step 2809, loss 0.0439054, acc 1
2016-09-06T18:48:31.958926: step 2810, loss 0.0430508, acc 0.96
2016-09-06T18:48:32.639910: step 2811, loss 0.0168822, acc 1
2016-09-06T18:48:33.321007: step 2812, loss 0.0566746, acc 0.96
2016-09-06T18:48:34.009893: step 2813, loss 0.0449869, acc 0.98
2016-09-06T18:48:34.673482: step 2814, loss 0.0206924, acc 1
2016-09-06T18:48:35.375633: step 2815, loss 0.0688947, acc 0.96
2016-09-06T18:48:36.048227: step 2816, loss 0.0740175, acc 0.94
2016-09-06T18:48:36.732852: step 2817, loss 0.0246994, acc 0.98
2016-09-06T18:48:37.398476: step 2818, loss 0.0658985, acc 0.96
2016-09-06T18:48:38.081121: step 2819, loss 0.0634772, acc 0.98
2016-09-06T18:48:38.766379: step 2820, loss 0.068172, acc 0.96
2016-09-06T18:48:39.442312: step 2821, loss 0.0624023, acc 0.96
2016-09-06T18:48:40.144418: step 2822, loss 0.03454, acc 0.96
2016-09-06T18:48:40.829292: step 2823, loss 0.0278213, acc 0.98
2016-09-06T18:48:41.510919: step 2824, loss 0.0233602, acc 1
2016-09-06T18:48:42.197074: step 2825, loss 0.0736492, acc 0.96
2016-09-06T18:48:42.894942: step 2826, loss 0.013233, acc 1
2016-09-06T18:48:43.562302: step 2827, loss 0.0195318, acc 1
2016-09-06T18:48:44.258918: step 2828, loss 0.00148738, acc 1
2016-09-06T18:48:44.959465: step 2829, loss 0.081197, acc 0.96
2016-09-06T18:48:45.636636: step 2830, loss 0.0339124, acc 1
2016-09-06T18:48:46.323057: step 2831, loss 0.0595879, acc 0.98
2016-09-06T18:48:46.996630: step 2832, loss 0.0628314, acc 0.98
2016-09-06T18:48:47.676291: step 2833, loss 0.0351145, acc 1
2016-09-06T18:48:48.376651: step 2834, loss 0.0877855, acc 0.96
2016-09-06T18:48:49.068382: step 2835, loss 0.195092, acc 0.96
2016-09-06T18:48:49.766558: step 2836, loss 0.0715003, acc 0.98
2016-09-06T18:48:50.458580: step 2837, loss 0.0646318, acc 0.98
2016-09-06T18:48:51.139213: step 2838, loss 0.0114876, acc 1
2016-09-06T18:48:51.805412: step 2839, loss 0.138311, acc 0.98
2016-09-06T18:48:52.504835: step 2840, loss 0.0419341, acc 0.96
2016-09-06T18:48:53.209406: step 2841, loss 0.0124104, acc 1
2016-09-06T18:48:53.872952: step 2842, loss 0.0359613, acc 0.98
2016-09-06T18:48:54.573692: step 2843, loss 0.0783178, acc 0.96
2016-09-06T18:48:55.264286: step 2844, loss 0.0369382, acc 0.98
2016-09-06T18:48:55.946067: step 2845, loss 0.0761898, acc 0.96
2016-09-06T18:48:56.642552: step 2846, loss 0.10842, acc 0.94
2016-09-06T18:48:57.334841: step 2847, loss 0.0295035, acc 1
2016-09-06T18:48:58.024211: step 2848, loss 0.0305323, acc 1
2016-09-06T18:48:58.687942: step 2849, loss 0.109203, acc 0.96
2016-09-06T18:48:59.393052: step 2850, loss 0.0272513, acc 1
2016-09-06T18:49:00.052438: step 2851, loss 0.0187366, acc 1
2016-09-06T18:49:00.771239: step 2852, loss 0.00693095, acc 1
2016-09-06T18:49:01.450258: step 2853, loss 0.064552, acc 0.96
2016-09-06T18:49:02.116810: step 2854, loss 0.0142854, acc 1
2016-09-06T18:49:02.806548: step 2855, loss 0.0415744, acc 0.98
2016-09-06T18:49:03.491520: step 2856, loss 0.0756651, acc 0.94
2016-09-06T18:49:04.193923: step 2857, loss 0.0404226, acc 0.98
2016-09-06T18:49:04.880266: step 2858, loss 0.105209, acc 0.96
2016-09-06T18:49:05.561821: step 2859, loss 0.0392332, acc 1
2016-09-06T18:49:06.220554: step 2860, loss 0.0495067, acc 0.98
2016-09-06T18:49:06.950157: step 2861, loss 0.0512175, acc 0.96
2016-09-06T18:49:07.639826: step 2862, loss 0.056212, acc 0.98
2016-09-06T18:49:08.315943: step 2863, loss 0.00623639, acc 1
2016-09-06T18:49:08.996192: step 2864, loss 0.0246917, acc 0.98
2016-09-06T18:49:09.663476: step 2865, loss 0.0135019, acc 1
2016-09-06T18:49:10.346484: step 2866, loss 0.0382921, acc 0.98
2016-09-06T18:49:11.051500: step 2867, loss 0.0346937, acc 0.98
2016-09-06T18:49:11.744107: step 2868, loss 0.0999573, acc 0.98
2016-09-06T18:49:12.429237: step 2869, loss 0.0210364, acc 0.98
2016-09-06T18:49:13.107005: step 2870, loss 0.0441277, acc 0.98
2016-09-06T18:49:13.817073: step 2871, loss 0.00193475, acc 1
2016-09-06T18:49:14.504754: step 2872, loss 0.139879, acc 0.92
2016-09-06T18:49:15.208414: step 2873, loss 0.0207108, acc 1
2016-09-06T18:49:15.903723: step 2874, loss 0.0333046, acc 0.98
2016-09-06T18:49:16.592546: step 2875, loss 0.0284636, acc 0.98
2016-09-06T18:49:17.274614: step 2876, loss 0.0153638, acc 1
2016-09-06T18:49:17.939718: step 2877, loss 0.032039, acc 0.98
2016-09-06T18:49:18.627903: step 2878, loss 0.0838035, acc 0.96
2016-09-06T18:49:19.303008: step 2879, loss 0.0498978, acc 0.96
2016-09-06T18:49:19.937628: step 2880, loss 0.00134909, acc 1
2016-09-06T18:49:20.635084: step 2881, loss 0.0246664, acc 0.98
2016-09-06T18:49:21.319286: step 2882, loss 0.171239, acc 0.94
2016-09-06T18:49:22.002102: step 2883, loss 0.068179, acc 0.96
2016-09-06T18:49:22.684671: step 2884, loss 0.00543582, acc 1
2016-09-06T18:49:23.373711: step 2885, loss 0.0354462, acc 1
2016-09-06T18:49:24.066678: step 2886, loss 0.0547479, acc 0.96
2016-09-06T18:49:24.751842: step 2887, loss 0.0244922, acc 1
2016-09-06T18:49:25.431940: step 2888, loss 0.0361706, acc 0.98
2016-09-06T18:49:26.122839: step 2889, loss 0.0277436, acc 0.98
2016-09-06T18:49:26.806092: step 2890, loss 0.119301, acc 0.96
2016-09-06T18:49:27.489059: step 2891, loss 0.0283894, acc 1
2016-09-06T18:49:28.165318: step 2892, loss 0.00552977, acc 1
2016-09-06T18:49:28.836260: step 2893, loss 0.081102, acc 0.96
2016-09-06T18:49:29.552115: step 2894, loss 0.0270251, acc 1
2016-09-06T18:49:30.236405: step 2895, loss 0.0269593, acc 0.98
2016-09-06T18:49:30.932016: step 2896, loss 0.0254819, acc 0.98
2016-09-06T18:49:31.618665: step 2897, loss 0.015256, acc 1
2016-09-06T18:49:32.310696: step 2898, loss 0.00444218, acc 1
2016-09-06T18:49:33.014453: step 2899, loss 0.0204565, acc 0.98
2016-09-06T18:49:33.703898: step 2900, loss 0.0171769, acc 1

Evaluation:
2016-09-06T18:49:36.833304: step 2900, loss 1.40044, acc 0.762664

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-2900

2016-09-06T18:49:38.646429: step 2901, loss 0.0245928, acc 1
2016-09-06T18:49:39.338898: step 2902, loss 0.0198941, acc 0.98
2016-09-06T18:49:40.034209: step 2903, loss 0.0806701, acc 0.94
2016-09-06T18:49:40.722459: step 2904, loss 0.0473468, acc 0.98
2016-09-06T18:49:41.433234: step 2905, loss 0.0403563, acc 0.96
2016-09-06T18:49:42.095769: step 2906, loss 0.0582001, acc 0.96
2016-09-06T18:49:42.783107: step 2907, loss 0.0827097, acc 0.96
2016-09-06T18:49:43.479238: step 2908, loss 0.01947, acc 1
2016-09-06T18:49:44.167703: step 2909, loss 0.0293523, acc 0.98
2016-09-06T18:49:44.863807: step 2910, loss 0.152633, acc 0.92
2016-09-06T18:49:45.564356: step 2911, loss 0.0562878, acc 0.96
2016-09-06T18:49:46.272541: step 2912, loss 0.0485464, acc 0.98
2016-09-06T18:49:46.967955: step 2913, loss 0.00766439, acc 1
2016-09-06T18:49:47.633838: step 2914, loss 0.0365062, acc 0.98
2016-09-06T18:49:48.323167: step 2915, loss 0.0300811, acc 0.98
2016-09-06T18:49:49.028224: step 2916, loss 0.0441104, acc 0.98
2016-09-06T18:49:49.708749: step 2917, loss 0.0264819, acc 0.98
2016-09-06T18:49:50.395035: step 2918, loss 0.0354728, acc 0.98
2016-09-06T18:49:51.104330: step 2919, loss 0.0240742, acc 0.98
2016-09-06T18:49:51.802129: step 2920, loss 0.0135202, acc 1
2016-09-06T18:49:52.482101: step 2921, loss 0.045933, acc 1
2016-09-06T18:49:53.171594: step 2922, loss 0.0728861, acc 0.94
2016-09-06T18:49:53.877683: step 2923, loss 0.039577, acc 1
2016-09-06T18:49:54.589115: step 2924, loss 0.0200273, acc 1
2016-09-06T18:49:55.255651: step 2925, loss 0.0435153, acc 0.98
2016-09-06T18:49:55.965770: step 2926, loss 0.0477076, acc 0.98
2016-09-06T18:49:56.656617: step 2927, loss 0.0492791, acc 0.98
2016-09-06T18:49:57.327850: step 2928, loss 0.0383342, acc 0.98
2016-09-06T18:49:58.031064: step 2929, loss 0.0429409, acc 1
2016-09-06T18:49:58.727900: step 2930, loss 0.0912667, acc 0.96
2016-09-06T18:49:59.450617: step 2931, loss 0.026726, acc 0.98
2016-09-06T18:50:00.127775: step 2932, loss 0.00314081, acc 1
2016-09-06T18:50:00.853992: step 2933, loss 0.00388924, acc 1
2016-09-06T18:50:01.527244: step 2934, loss 0.00508301, acc 1
2016-09-06T18:50:02.196608: step 2935, loss 0.0739565, acc 0.94
2016-09-06T18:50:02.881048: step 2936, loss 0.023137, acc 1
2016-09-06T18:50:03.560167: step 2937, loss 0.00920133, acc 1
2016-09-06T18:50:04.274586: step 2938, loss 0.0942731, acc 0.98
2016-09-06T18:50:04.959482: step 2939, loss 0.00536963, acc 1
2016-09-06T18:50:05.633121: step 2940, loss 0.0303988, acc 0.98
2016-09-06T18:50:06.323535: step 2941, loss 0.00270548, acc 1
2016-09-06T18:50:07.002169: step 2942, loss 0.0157569, acc 1
2016-09-06T18:50:07.703128: step 2943, loss 0.0494053, acc 1
2016-09-06T18:50:08.392267: step 2944, loss 0.00560105, acc 1
2016-09-06T18:50:09.101533: step 2945, loss 0.00283646, acc 1
2016-09-06T18:50:09.776180: step 2946, loss 0.100015, acc 0.98
2016-09-06T18:50:10.444741: step 2947, loss 0.0577004, acc 0.96
2016-09-06T18:50:11.130109: step 2948, loss 0.0328542, acc 0.98
2016-09-06T18:50:11.821091: step 2949, loss 0.0209299, acc 1
2016-09-06T18:50:12.515261: step 2950, loss 0.0182539, acc 1
2016-09-06T18:50:13.197501: step 2951, loss 0.0776936, acc 0.98
2016-09-06T18:50:13.904031: step 2952, loss 0.0157598, acc 1
2016-09-06T18:50:14.570776: step 2953, loss 0.0427276, acc 0.96
2016-09-06T18:50:15.268444: step 2954, loss 0.0999423, acc 0.96
2016-09-06T18:50:15.962458: step 2955, loss 0.0305364, acc 0.98
2016-09-06T18:50:16.659617: step 2956, loss 0.0147131, acc 1
2016-09-06T18:50:17.327387: step 2957, loss 0.0186977, acc 1
2016-09-06T18:50:18.025255: step 2958, loss 0.0273334, acc 0.98
2016-09-06T18:50:18.729724: step 2959, loss 0.0216035, acc 1
2016-09-06T18:50:19.419615: step 2960, loss 0.0580052, acc 0.96
2016-09-06T18:50:20.105318: step 2961, loss 0.0703756, acc 0.96
2016-09-06T18:50:20.794639: step 2962, loss 0.0408015, acc 0.98
2016-09-06T18:50:21.478290: step 2963, loss 0.0714822, acc 0.98
2016-09-06T18:50:22.182942: step 2964, loss 0.00190345, acc 1
2016-09-06T18:50:22.848939: step 2965, loss 0.149848, acc 0.96
2016-09-06T18:50:23.570562: step 2966, loss 0.0226936, acc 0.98
2016-09-06T18:50:24.244268: step 2967, loss 0.0164967, acc 1
2016-09-06T18:50:24.919747: step 2968, loss 0.0224723, acc 1
2016-09-06T18:50:25.605516: step 2969, loss 0.0364733, acc 0.98
2016-09-06T18:50:26.287890: step 2970, loss 0.0256067, acc 1
2016-09-06T18:50:26.956036: step 2971, loss 0.00613625, acc 1
2016-09-06T18:50:27.638288: step 2972, loss 0.0245205, acc 1
2016-09-06T18:50:28.318883: step 2973, loss 0.0268765, acc 1
2016-09-06T18:50:28.985419: step 2974, loss 0.0594784, acc 0.98
2016-09-06T18:50:29.667227: step 2975, loss 0.0564929, acc 0.98
2016-09-06T18:50:30.354636: step 2976, loss 0.04972, acc 0.98
2016-09-06T18:50:31.033028: step 2977, loss 0.00652184, acc 1
2016-09-06T18:50:31.713563: step 2978, loss 0.138725, acc 0.94
2016-09-06T18:50:32.397756: step 2979, loss 0.027113, acc 1
2016-09-06T18:50:33.102974: step 2980, loss 0.0299599, acc 1
2016-09-06T18:50:33.766633: step 2981, loss 0.00739116, acc 1
2016-09-06T18:50:34.457707: step 2982, loss 0.0104959, acc 1
2016-09-06T18:50:35.136462: step 2983, loss 0.027388, acc 1
2016-09-06T18:50:35.841568: step 2984, loss 0.0709197, acc 0.98
2016-09-06T18:50:36.535462: step 2985, loss 0.0159909, acc 1
2016-09-06T18:50:37.203397: step 2986, loss 0.0602657, acc 0.98
2016-09-06T18:50:37.920919: step 2987, loss 0.0420795, acc 0.98
2016-09-06T18:50:38.583604: step 2988, loss 0.00426227, acc 1
2016-09-06T18:50:39.299267: step 2989, loss 0.0128086, acc 1
2016-09-06T18:50:39.967385: step 2990, loss 0.0992985, acc 0.94
2016-09-06T18:50:40.644299: step 2991, loss 0.0298133, acc 0.98
2016-09-06T18:50:41.336730: step 2992, loss 0.0160549, acc 1
2016-09-06T18:50:42.021200: step 2993, loss 0.01908, acc 0.98
2016-09-06T18:50:42.717620: step 2994, loss 0.0451098, acc 0.96
2016-09-06T18:50:43.400929: step 2995, loss 0.0030193, acc 1
2016-09-06T18:50:44.097359: step 2996, loss 0.00654136, acc 1
2016-09-06T18:50:44.770800: step 2997, loss 0.00625356, acc 1
2016-09-06T18:50:45.449007: step 2998, loss 0.0393813, acc 0.96
2016-09-06T18:50:46.134960: step 2999, loss 0.0414212, acc 0.98
2016-09-06T18:50:46.826558: step 3000, loss 0.0132129, acc 1

Evaluation:
2016-09-06T18:50:49.995118: step 3000, loss 1.62081, acc 0.754221

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-3000

2016-09-06T18:50:51.702454: step 3001, loss 0.0831083, acc 0.94
2016-09-06T18:50:52.413384: step 3002, loss 0.0218664, acc 0.98
2016-09-06T18:50:53.080801: step 3003, loss 0.0155909, acc 1
2016-09-06T18:50:53.759173: step 3004, loss 0.0134503, acc 1
2016-09-06T18:50:54.438856: step 3005, loss 0.0628548, acc 0.96
2016-09-06T18:50:55.135883: step 3006, loss 0.00573233, acc 1
2016-09-06T18:50:55.832478: step 3007, loss 0.0333179, acc 0.98
2016-09-06T18:50:56.519442: step 3008, loss 0.0143532, acc 1
2016-09-06T18:50:57.236652: step 3009, loss 0.00802755, acc 1
2016-09-06T18:50:57.934725: step 3010, loss 0.0574766, acc 0.94
2016-09-06T18:50:58.620349: step 3011, loss 0.0342839, acc 1
2016-09-06T18:50:59.300515: step 3012, loss 0.24404, acc 0.96
2016-09-06T18:50:59.995353: step 3013, loss 0.0999447, acc 0.94
2016-09-06T18:51:00.720118: step 3014, loss 0.0456085, acc 0.96
2016-09-06T18:51:01.372730: step 3015, loss 0.0299706, acc 0.98
2016-09-06T18:51:02.074612: step 3016, loss 0.0101945, acc 1
2016-09-06T18:51:02.752444: step 3017, loss 0.00573266, acc 1
2016-09-06T18:51:03.419984: step 3018, loss 0.00165095, acc 1
2016-09-06T18:51:04.098751: step 3019, loss 0.0523841, acc 0.96
2016-09-06T18:51:04.780779: step 3020, loss 0.00401423, acc 1
2016-09-06T18:51:05.476364: step 3021, loss 0.0217097, acc 0.98
2016-09-06T18:51:06.161846: step 3022, loss 0.0049374, acc 1
2016-09-06T18:51:06.885407: step 3023, loss 0.0293747, acc 1
2016-09-06T18:51:07.555347: step 3024, loss 0.0592611, acc 0.96
2016-09-06T18:51:08.234392: step 3025, loss 0.0312563, acc 0.98
2016-09-06T18:51:08.918874: step 3026, loss 0.0268641, acc 0.98
2016-09-06T18:51:09.591576: step 3027, loss 0.0237548, acc 1
2016-09-06T18:51:10.251875: step 3028, loss 0.0396239, acc 0.98
2016-09-06T18:51:10.928625: step 3029, loss 0.148314, acc 0.96
2016-09-06T18:51:11.627874: step 3030, loss 0.0445641, acc 0.98
2016-09-06T18:51:12.322772: step 3031, loss 0.0144587, acc 1
2016-09-06T18:51:12.990694: step 3032, loss 0.0263434, acc 1
2016-09-06T18:51:13.673166: step 3033, loss 0.00343167, acc 1
2016-09-06T18:51:14.368708: step 3034, loss 0.119837, acc 0.92
2016-09-06T18:51:15.047777: step 3035, loss 0.0607784, acc 0.98
2016-09-06T18:51:15.747605: step 3036, loss 0.0232472, acc 1
2016-09-06T18:51:16.439024: step 3037, loss 0.0381478, acc 0.98
2016-09-06T18:51:17.083237: step 3038, loss 0.0346656, acc 1
2016-09-06T18:51:17.789842: step 3039, loss 0.0485979, acc 0.96
2016-09-06T18:51:18.476788: step 3040, loss 0.058125, acc 0.98
2016-09-06T18:51:19.195737: step 3041, loss 0.0274843, acc 0.98
2016-09-06T18:51:19.900599: step 3042, loss 0.0275464, acc 0.98
2016-09-06T18:51:20.601720: step 3043, loss 0.0150124, acc 1
2016-09-06T18:51:21.305272: step 3044, loss 0.0252912, acc 1
2016-09-06T18:51:21.980295: step 3045, loss 0.0347442, acc 0.98
2016-09-06T18:51:22.659999: step 3046, loss 0.0207207, acc 0.98
2016-09-06T18:51:23.359194: step 3047, loss 0.129234, acc 0.96
2016-09-06T18:51:24.059997: step 3048, loss 0.0353392, acc 0.98
2016-09-06T18:51:24.757851: step 3049, loss 0.0498757, acc 0.96
2016-09-06T18:51:25.424198: step 3050, loss 0.042225, acc 1
2016-09-06T18:51:26.157864: step 3051, loss 0.0875751, acc 0.92
2016-09-06T18:51:26.832533: step 3052, loss 0.0777523, acc 0.98
2016-09-06T18:51:27.531083: step 3053, loss 0.0170609, acc 1
2016-09-06T18:51:28.226163: step 3054, loss 0.016532, acc 1
2016-09-06T18:51:28.961731: step 3055, loss 0.012102, acc 1
2016-09-06T18:51:29.690166: step 3056, loss 0.0847099, acc 0.98
2016-09-06T18:51:30.363604: step 3057, loss 0.0199834, acc 1
2016-09-06T18:51:31.037449: step 3058, loss 0.0518534, acc 0.96
2016-09-06T18:51:31.733441: step 3059, loss 0.0522294, acc 0.98
2016-09-06T18:51:32.450530: step 3060, loss 0.00997862, acc 1
2016-09-06T18:51:33.145965: step 3061, loss 0.0345085, acc 0.98
2016-09-06T18:51:33.848003: step 3062, loss 0.0255, acc 0.98
2016-09-06T18:51:34.542939: step 3063, loss 0.000290459, acc 1
2016-09-06T18:51:35.222748: step 3064, loss 0.233213, acc 0.96
2016-09-06T18:51:35.909843: step 3065, loss 0.130485, acc 0.98
2016-09-06T18:51:36.607794: step 3066, loss 0.0266225, acc 0.98
2016-09-06T18:51:37.308326: step 3067, loss 0.0633857, acc 0.96
2016-09-06T18:51:38.028363: step 3068, loss 0.0411965, acc 0.98
2016-09-06T18:51:38.697512: step 3069, loss 0.0146464, acc 1
2016-09-06T18:51:39.408679: step 3070, loss 0.0155669, acc 1
2016-09-06T18:51:40.098447: step 3071, loss 0.00924482, acc 1
2016-09-06T18:51:40.739916: step 3072, loss 0.0272018, acc 1
2016-09-06T18:51:41.427828: step 3073, loss 0.0321231, acc 0.98
2016-09-06T18:51:42.118114: step 3074, loss 0.0195852, acc 0.98
2016-09-06T18:51:42.799341: step 3075, loss 0.0190003, acc 1
2016-09-06T18:51:43.460516: step 3076, loss 0.0342086, acc 0.98
2016-09-06T18:51:44.168240: step 3077, loss 0.00063292, acc 1
2016-09-06T18:51:44.843325: step 3078, loss 0.0666813, acc 0.98
2016-09-06T18:51:45.512627: step 3079, loss 0.00468202, acc 1
2016-09-06T18:51:46.201294: step 3080, loss 0.0246618, acc 1
2016-09-06T18:51:46.888169: step 3081, loss 0.0321336, acc 0.98
2016-09-06T18:51:47.554500: step 3082, loss 0.0977594, acc 0.98
2016-09-06T18:51:48.256869: step 3083, loss 0.045012, acc 1
2016-09-06T18:51:48.955377: step 3084, loss 0.043779, acc 0.98
2016-09-06T18:51:49.628350: step 3085, loss 0.0349294, acc 0.98
2016-09-06T18:51:50.312093: step 3086, loss 0.00200213, acc 1
2016-09-06T18:51:51.006492: step 3087, loss 0.0306102, acc 0.98
2016-09-06T18:51:51.686602: step 3088, loss 0.015827, acc 0.98
2016-09-06T18:51:52.389826: step 3089, loss 0.0370495, acc 0.98
2016-09-06T18:51:53.061507: step 3090, loss 0.0388217, acc 0.98
2016-09-06T18:51:53.771731: step 3091, loss 0.138982, acc 0.96
2016-09-06T18:51:54.451220: step 3092, loss 0.0753413, acc 0.96
2016-09-06T18:51:55.134732: step 3093, loss 0.0276315, acc 1
2016-09-06T18:51:55.809962: step 3094, loss 0.00780896, acc 1
2016-09-06T18:51:56.523135: step 3095, loss 0.00937313, acc 1
2016-09-06T18:51:57.216842: step 3096, loss 0.0397203, acc 1
2016-09-06T18:51:57.923218: step 3097, loss 0.00785929, acc 1
2016-09-06T18:51:58.617736: step 3098, loss 0.0354811, acc 0.98
2016-09-06T18:51:59.303698: step 3099, loss 0.0476306, acc 0.96
2016-09-06T18:51:59.982179: step 3100, loss 0.15641, acc 0.96

Evaluation:
2016-09-06T18:52:03.133900: step 3100, loss 1.6191, acc 0.747655

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-3100

2016-09-06T18:52:04.902402: step 3101, loss 0.00651135, acc 1
2016-09-06T18:52:05.583292: step 3102, loss 0.0506784, acc 0.98
2016-09-06T18:52:06.263035: step 3103, loss 0.0182352, acc 1
2016-09-06T18:52:06.966916: step 3104, loss 0.021984, acc 0.98
2016-09-06T18:52:07.646297: step 3105, loss 0.0509538, acc 0.96
2016-09-06T18:52:08.321084: step 3106, loss 0.0168276, acc 1
2016-09-06T18:52:09.018158: step 3107, loss 0.0360967, acc 0.98
2016-09-06T18:52:09.701302: step 3108, loss 0.0129385, acc 1
2016-09-06T18:52:10.389250: step 3109, loss 0.00852752, acc 1
2016-09-06T18:52:11.049558: step 3110, loss 0.028127, acc 1
2016-09-06T18:52:11.771556: step 3111, loss 0.0401027, acc 0.98
2016-09-06T18:52:12.471086: step 3112, loss 0.0584608, acc 0.96
2016-09-06T18:52:13.165447: step 3113, loss 0.051473, acc 0.98
2016-09-06T18:52:13.875973: step 3114, loss 0.00313181, acc 1
2016-09-06T18:52:14.564302: step 3115, loss 0.00234559, acc 1
2016-09-06T18:52:15.254824: step 3116, loss 0.0550655, acc 0.96
2016-09-06T18:52:15.925884: step 3117, loss 0.0533033, acc 0.98
2016-09-06T18:52:16.625691: step 3118, loss 0.0758979, acc 0.96
2016-09-06T18:52:17.303437: step 3119, loss 0.0440748, acc 0.98
2016-09-06T18:52:17.998602: step 3120, loss 0.0338004, acc 1
2016-09-06T18:52:18.729802: step 3121, loss 0.0804083, acc 0.98
2016-09-06T18:52:19.443823: step 3122, loss 0.0230751, acc 1
2016-09-06T18:52:20.169406: step 3123, loss 0.0401024, acc 0.98
2016-09-06T18:52:20.854634: step 3124, loss 0.000473281, acc 1
2016-09-06T18:52:21.551328: step 3125, loss 0.00348104, acc 1
2016-09-06T18:52:22.233671: step 3126, loss 0.0129757, acc 1
2016-09-06T18:52:22.906147: step 3127, loss 0.0815846, acc 0.94
2016-09-06T18:52:23.571470: step 3128, loss 0.0160176, acc 0.98
2016-09-06T18:52:24.257552: step 3129, loss 0.0224673, acc 1
2016-09-06T18:52:24.948475: step 3130, loss 0.0672041, acc 0.96
2016-09-06T18:52:25.624561: step 3131, loss 0.0807892, acc 0.94
2016-09-06T18:52:26.326995: step 3132, loss 0.019797, acc 1
2016-09-06T18:52:27.019960: step 3133, loss 0.0124445, acc 1
2016-09-06T18:52:27.701971: step 3134, loss 0.0451292, acc 0.96
2016-09-06T18:52:28.390791: step 3135, loss 0.0273966, acc 0.98
2016-09-06T18:52:29.093802: step 3136, loss 0.139877, acc 0.96
2016-09-06T18:52:29.817258: step 3137, loss 0.000137011, acc 1
2016-09-06T18:52:30.489809: step 3138, loss 0.0256477, acc 1
2016-09-06T18:52:31.180373: step 3139, loss 0.00266186, acc 1
2016-09-06T18:52:31.862304: step 3140, loss 0.0596618, acc 0.96
2016-09-06T18:52:32.543003: step 3141, loss 0.0827223, acc 0.96
2016-09-06T18:52:33.231367: step 3142, loss 0.00217189, acc 1
2016-09-06T18:52:33.914633: step 3143, loss 0.0731927, acc 0.94
2016-09-06T18:52:34.633268: step 3144, loss 0.0359784, acc 0.98
2016-09-06T18:52:35.297716: step 3145, loss 0.0252394, acc 1
2016-09-06T18:52:35.990232: step 3146, loss 0.139985, acc 0.92
2016-09-06T18:52:36.671349: step 3147, loss 0.0666397, acc 0.96
2016-09-06T18:52:37.351855: step 3148, loss 0.0440821, acc 0.98
2016-09-06T18:52:38.042785: step 3149, loss 0.0204873, acc 0.98
2016-09-06T18:52:38.706565: step 3150, loss 0.0446092, acc 0.98
2016-09-06T18:52:39.414229: step 3151, loss 0.0174892, acc 1
2016-09-06T18:52:40.084547: step 3152, loss 0.0355261, acc 1
2016-09-06T18:52:40.781927: step 3153, loss 0.0329279, acc 0.98
2016-09-06T18:52:41.497291: step 3154, loss 0.0209604, acc 1
2016-09-06T18:52:42.187645: step 3155, loss 0.0216454, acc 1
2016-09-06T18:52:42.891513: step 3156, loss 0.0124604, acc 1
2016-09-06T18:52:43.561489: step 3157, loss 0.0237668, acc 0.98
2016-09-06T18:52:44.274817: step 3158, loss 0.031366, acc 0.96
2016-09-06T18:52:44.944573: step 3159, loss 0.0711585, acc 0.96
2016-09-06T18:52:45.629979: step 3160, loss 0.0474854, acc 0.98
2016-09-06T18:52:46.326076: step 3161, loss 0.0949287, acc 0.96
2016-09-06T18:52:47.015761: step 3162, loss 0.0151305, acc 1
2016-09-06T18:52:47.709356: step 3163, loss 0.043486, acc 0.98
2016-09-06T18:52:48.372280: step 3164, loss 0.0469622, acc 0.96
2016-09-06T18:52:49.070550: step 3165, loss 0.0207397, acc 0.98
2016-09-06T18:52:49.733213: step 3166, loss 0.0703217, acc 0.96
2016-09-06T18:52:50.408199: step 3167, loss 0.0298029, acc 1
2016-09-06T18:52:51.094218: step 3168, loss 0.00183441, acc 1
2016-09-06T18:52:51.769883: step 3169, loss 0.0385925, acc 0.98
2016-09-06T18:52:52.467430: step 3170, loss 0.0177601, acc 0.98
2016-09-06T18:52:53.151678: step 3171, loss 0.00680444, acc 1
2016-09-06T18:52:53.857055: step 3172, loss 0.0428976, acc 0.98
2016-09-06T18:52:54.547069: step 3173, loss 0.12066, acc 0.96
2016-09-06T18:52:55.225221: step 3174, loss 0.00206726, acc 1
2016-09-06T18:52:55.912344: step 3175, loss 0.0264433, acc 1
2016-09-06T18:52:56.591297: step 3176, loss 0.0865996, acc 0.94
2016-09-06T18:52:57.277857: step 3177, loss 0.0345909, acc 1
2016-09-06T18:52:57.971194: step 3178, loss 0.0232604, acc 0.98
2016-09-06T18:52:58.704349: step 3179, loss 0.0101561, acc 1
2016-09-06T18:52:59.388464: step 3180, loss 0.000711589, acc 1
2016-09-06T18:53:00.069756: step 3181, loss 0.0920071, acc 0.96
2016-09-06T18:53:00.790173: step 3182, loss 0.0372107, acc 0.98
2016-09-06T18:53:01.492101: step 3183, loss 0.0137553, acc 1
2016-09-06T18:53:02.178840: step 3184, loss 0.0286277, acc 0.98
2016-09-06T18:53:02.890705: step 3185, loss 0.0348601, acc 0.98
2016-09-06T18:53:03.618633: step 3186, loss 0.0298172, acc 0.98
2016-09-06T18:53:04.313876: step 3187, loss 0.037223, acc 0.98
2016-09-06T18:53:05.009877: step 3188, loss 0.000616978, acc 1
2016-09-06T18:53:05.707763: step 3189, loss 0.0338126, acc 0.98
2016-09-06T18:53:06.391197: step 3190, loss 0.00403123, acc 1
2016-09-06T18:53:07.086973: step 3191, loss 0.0882017, acc 0.94
2016-09-06T18:53:07.756409: step 3192, loss 0.105208, acc 0.96
2016-09-06T18:53:08.441535: step 3193, loss 0.0164038, acc 1
2016-09-06T18:53:09.153732: step 3194, loss 0.0622648, acc 0.94
2016-09-06T18:53:09.845317: step 3195, loss 0.0826266, acc 0.98
2016-09-06T18:53:10.548283: step 3196, loss 0.0518967, acc 0.96
2016-09-06T18:53:11.209726: step 3197, loss 0.0313631, acc 0.98
2016-09-06T18:53:11.903407: step 3198, loss 0.0336766, acc 0.98
2016-09-06T18:53:12.594784: step 3199, loss 0.00563631, acc 1
2016-09-06T18:53:13.326125: step 3200, loss 0.0504519, acc 0.96

Evaluation:
2016-09-06T18:53:16.480159: step 3200, loss 1.50685, acc 0.745779

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-3200

2016-09-06T18:53:18.171806: step 3201, loss 0.0229792, acc 0.98
2016-09-06T18:53:18.877235: step 3202, loss 0.0238123, acc 1
2016-09-06T18:53:19.581257: step 3203, loss 0.0520811, acc 0.96
2016-09-06T18:53:20.261962: step 3204, loss 0.0268921, acc 1
2016-09-06T18:53:20.993227: step 3205, loss 0.00676895, acc 1
2016-09-06T18:53:21.697222: step 3206, loss 0.0418073, acc 0.98
2016-09-06T18:53:22.377442: step 3207, loss 0.00751399, acc 1
2016-09-06T18:53:23.086540: step 3208, loss 0.0149265, acc 1
2016-09-06T18:53:23.764386: step 3209, loss 0.0479889, acc 0.98
2016-09-06T18:53:24.431659: step 3210, loss 0.00676022, acc 1
2016-09-06T18:53:25.124177: step 3211, loss 0.0343146, acc 0.98
2016-09-06T18:53:25.794250: step 3212, loss 0.00932196, acc 1
2016-09-06T18:53:26.472213: step 3213, loss 0.0516468, acc 0.96
2016-09-06T18:53:27.150447: step 3214, loss 0.00237221, acc 1
2016-09-06T18:53:27.847297: step 3215, loss 0.0394523, acc 0.96
2016-09-06T18:53:28.547892: step 3216, loss 0.0391576, acc 0.98
2016-09-06T18:53:29.238751: step 3217, loss 0.045811, acc 0.98
2016-09-06T18:53:29.934663: step 3218, loss 0.00328913, acc 1
2016-09-06T18:53:30.593922: step 3219, loss 0.0281558, acc 1
2016-09-06T18:53:31.289473: step 3220, loss 0.108344, acc 0.94
2016-09-06T18:53:31.986218: step 3221, loss 0.0237438, acc 1
2016-09-06T18:53:32.664331: step 3222, loss 0.0260264, acc 0.98
2016-09-06T18:53:33.340800: step 3223, loss 0.0250394, acc 0.98
2016-09-06T18:53:34.020374: step 3224, loss 0.0980822, acc 0.98
2016-09-06T18:53:34.742581: step 3225, loss 0.0257965, acc 0.98
2016-09-06T18:53:35.424893: step 3226, loss 0.0185001, acc 1
2016-09-06T18:53:36.103662: step 3227, loss 0.174224, acc 0.98
2016-09-06T18:53:36.799424: step 3228, loss 0.0533411, acc 0.96
2016-09-06T18:53:37.500734: step 3229, loss 0.0257881, acc 0.98
2016-09-06T18:53:38.190688: step 3230, loss 0.0346513, acc 0.98
2016-09-06T18:53:38.884300: step 3231, loss 0.0182303, acc 1
2016-09-06T18:53:39.605268: step 3232, loss 0.0278795, acc 0.98
2016-09-06T18:53:40.284718: step 3233, loss 0.000320695, acc 1
2016-09-06T18:53:40.967088: step 3234, loss 0.0151656, acc 1
2016-09-06T18:53:41.654686: step 3235, loss 0.0418597, acc 0.96
2016-09-06T18:53:42.322911: step 3236, loss 0.0640231, acc 0.98
2016-09-06T18:53:43.023590: step 3237, loss 0.0327268, acc 0.98
2016-09-06T18:53:43.699881: step 3238, loss 0.0183715, acc 1
2016-09-06T18:53:44.408119: step 3239, loss 0.0307369, acc 0.98
2016-09-06T18:53:45.090072: step 3240, loss 0.0242301, acc 1
2016-09-06T18:53:45.790787: step 3241, loss 0.0440561, acc 0.98
2016-09-06T18:53:46.482636: step 3242, loss 0.0779273, acc 0.96
2016-09-06T18:53:47.159811: step 3243, loss 0.0122163, acc 1
2016-09-06T18:53:47.854069: step 3244, loss 0.0169447, acc 1
2016-09-06T18:53:48.520260: step 3245, loss 0.00798423, acc 1
2016-09-06T18:53:49.239784: step 3246, loss 0.103139, acc 0.94
2016-09-06T18:53:49.926225: step 3247, loss 0.0187244, acc 1
2016-09-06T18:53:50.617813: step 3248, loss 0.0786657, acc 0.94
2016-09-06T18:53:51.310009: step 3249, loss 0.0688005, acc 0.98
2016-09-06T18:53:51.979865: step 3250, loss 0.043842, acc 1
2016-09-06T18:53:52.659082: step 3251, loss 0.0158995, acc 1
2016-09-06T18:53:53.313818: step 3252, loss 0.0673757, acc 0.98
2016-09-06T18:53:54.013284: step 3253, loss 0.0138242, acc 1
2016-09-06T18:53:54.704085: step 3254, loss 0.0514676, acc 0.98
2016-09-06T18:53:55.397409: step 3255, loss 0.00447767, acc 1
2016-09-06T18:53:56.071137: step 3256, loss 0.0205041, acc 1
2016-09-06T18:53:56.752024: step 3257, loss 0.0793748, acc 0.98
2016-09-06T18:53:57.447326: step 3258, loss 0.0253899, acc 0.98
2016-09-06T18:53:58.138504: step 3259, loss 0.0377499, acc 0.98
2016-09-06T18:53:58.847982: step 3260, loss 0.0795115, acc 0.98
2016-09-06T18:53:59.524259: step 3261, loss 0.0669721, acc 0.94
2016-09-06T18:54:00.230116: step 3262, loss 0.0276016, acc 0.98
2016-09-06T18:54:00.894177: step 3263, loss 0.0243479, acc 0.98
2016-09-06T18:54:01.536592: step 3264, loss 0.00511926, acc 1
2016-09-06T18:54:02.230491: step 3265, loss 0.00737308, acc 1
2016-09-06T18:54:02.900151: step 3266, loss 0.0108997, acc 1
2016-09-06T18:54:03.606439: step 3267, loss 0.036556, acc 0.98
2016-09-06T18:54:04.284530: step 3268, loss 0.0579945, acc 0.98
2016-09-06T18:54:04.976092: step 3269, loss 0.00179823, acc 1
2016-09-06T18:54:05.683251: step 3270, loss 0.00500651, acc 1
2016-09-06T18:54:06.370529: step 3271, loss 0.0273533, acc 0.98
2016-09-06T18:54:07.054772: step 3272, loss 0.0547639, acc 0.98
2016-09-06T18:54:07.743068: step 3273, loss 0.0652857, acc 0.98
2016-09-06T18:54:08.442682: step 3274, loss 0.0138345, acc 1
2016-09-06T18:54:09.156612: step 3275, loss 0.154803, acc 0.92
2016-09-06T18:54:09.834439: step 3276, loss 0.0922399, acc 0.96
2016-09-06T18:54:10.510927: step 3277, loss 0.102082, acc 0.96
2016-09-06T18:54:11.188305: step 3278, loss 0.000446695, acc 1
2016-09-06T18:54:11.864380: step 3279, loss 0.0117922, acc 1
2016-09-06T18:54:12.560237: step 3280, loss 0.000812507, acc 1
2016-09-06T18:54:13.270727: step 3281, loss 0.0309802, acc 1
2016-09-06T18:54:13.941017: step 3282, loss 0.00069778, acc 1
2016-09-06T18:54:14.610610: step 3283, loss 0.00695076, acc 1
2016-09-06T18:54:15.290885: step 3284, loss 0.132617, acc 0.96
2016-09-06T18:54:15.984770: step 3285, loss 0.0199453, acc 1
2016-09-06T18:54:16.676315: step 3286, loss 0.0416149, acc 0.98
2016-09-06T18:54:17.351482: step 3287, loss 0.0289979, acc 0.98
2016-09-06T18:54:18.062800: step 3288, loss 0.142454, acc 0.96
2016-09-06T18:54:18.744665: step 3289, loss 0.0213391, acc 1
2016-09-06T18:54:19.429763: step 3290, loss 0.0820357, acc 0.98
2016-09-06T18:54:20.129514: step 3291, loss 0.00327282, acc 1
2016-09-06T18:54:20.814835: step 3292, loss 0.0177662, acc 1
2016-09-06T18:54:21.513292: step 3293, loss 0.0546805, acc 0.96
2016-09-06T18:54:22.160620: step 3294, loss 0.0637141, acc 0.98
2016-09-06T18:54:22.901630: step 3295, loss 0.00721632, acc 1
2016-09-06T18:54:23.566051: step 3296, loss 0.0225035, acc 0.98
2016-09-06T18:54:24.250078: step 3297, loss 0.074758, acc 0.96
2016-09-06T18:54:24.959561: step 3298, loss 0.00763151, acc 1
2016-09-06T18:54:25.630140: step 3299, loss 0.0526469, acc 0.98
2016-09-06T18:54:26.312212: step 3300, loss 0.0223738, acc 1

Evaluation:
2016-09-06T18:54:29.436894: step 3300, loss 1.40353, acc 0.761726

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-3300

2016-09-06T18:54:31.197107: step 3301, loss 0.0029433, acc 1
2016-09-06T18:54:31.869680: step 3302, loss 0.0609459, acc 0.98
2016-09-06T18:54:32.603353: step 3303, loss 0.00855648, acc 1
2016-09-06T18:54:33.321261: step 3304, loss 0.0525581, acc 0.98
2016-09-06T18:54:34.027004: step 3305, loss 0.0597031, acc 0.96
2016-09-06T18:54:34.719870: step 3306, loss 0.0340731, acc 0.96
2016-09-06T18:54:35.382747: step 3307, loss 0.0569706, acc 0.98
2016-09-06T18:54:36.094966: step 3308, loss 0.0144834, acc 1
2016-09-06T18:54:36.770617: step 3309, loss 0.0925673, acc 0.96
2016-09-06T18:54:37.459539: step 3310, loss 0.00610673, acc 1
2016-09-06T18:54:38.139167: step 3311, loss 0.0253618, acc 0.98
2016-09-06T18:54:38.829649: step 3312, loss 0.0242535, acc 0.98
2016-09-06T18:54:39.522399: step 3313, loss 0.145105, acc 0.96
2016-09-06T18:54:40.198007: step 3314, loss 0.0188897, acc 1
2016-09-06T18:54:40.915506: step 3315, loss 0.0212007, acc 0.98
2016-09-06T18:54:41.598217: step 3316, loss 0.0189951, acc 1
2016-09-06T18:54:42.294028: step 3317, loss 0.00450184, acc 1
2016-09-06T18:54:42.972185: step 3318, loss 0.0167327, acc 1
2016-09-06T18:54:43.650424: step 3319, loss 0.0690295, acc 0.94
2016-09-06T18:54:44.322103: step 3320, loss 0.0155568, acc 1
2016-09-06T18:54:45.014797: step 3321, loss 0.0209779, acc 1
2016-09-06T18:54:45.707560: step 3322, loss 0.0128899, acc 1
2016-09-06T18:54:46.388891: step 3323, loss 0.0249615, acc 0.98
2016-09-06T18:54:47.059898: step 3324, loss 0.0677175, acc 0.94
2016-09-06T18:54:47.755000: step 3325, loss 0.0176, acc 1
2016-09-06T18:54:48.446605: step 3326, loss 0.0199487, acc 0.98
2016-09-06T18:54:49.131613: step 3327, loss 0.0372999, acc 0.98
2016-09-06T18:54:49.814619: step 3328, loss 0.0486772, acc 0.96
2016-09-06T18:54:50.519285: step 3329, loss 0.0247262, acc 1
2016-09-06T18:54:51.205600: step 3330, loss 0.119693, acc 0.96
2016-09-06T18:54:51.902400: step 3331, loss 0.00075608, acc 1
2016-09-06T18:54:52.588972: step 3332, loss 0.037084, acc 0.98
2016-09-06T18:54:53.274155: step 3333, loss 0.0507743, acc 0.98
2016-09-06T18:54:53.981279: step 3334, loss 0.0578795, acc 0.98
2016-09-06T18:54:54.639219: step 3335, loss 0.0560435, acc 0.96
2016-09-06T18:54:55.351354: step 3336, loss 0.0295445, acc 0.98
2016-09-06T18:54:56.047248: step 3337, loss 0.011603, acc 1
2016-09-06T18:54:56.738485: step 3338, loss 0.0598673, acc 0.94
2016-09-06T18:54:57.445722: step 3339, loss 0.0116826, acc 1
2016-09-06T18:54:58.145951: step 3340, loss 0.0558225, acc 0.98
2016-09-06T18:54:58.848426: step 3341, loss 0.0329576, acc 1
2016-09-06T18:54:59.508142: step 3342, loss 0.0871441, acc 0.96
2016-09-06T18:55:00.201078: step 3343, loss 0.0546518, acc 0.98
2016-09-06T18:55:00.908730: step 3344, loss 0.0410376, acc 0.98
2016-09-06T18:55:01.606494: step 3345, loss 0.136371, acc 0.94
2016-09-06T18:55:02.283182: step 3346, loss 0.0890654, acc 0.94
2016-09-06T18:55:02.980631: step 3347, loss 0.0221153, acc 1
2016-09-06T18:55:03.686383: step 3348, loss 0.0135161, acc 1
2016-09-06T18:55:04.367003: step 3349, loss 0.0799589, acc 0.96
2016-09-06T18:55:05.073142: step 3350, loss 0.0275937, acc 0.98
2016-09-06T18:55:05.756052: step 3351, loss 0.0684859, acc 0.94
2016-09-06T18:55:06.439933: step 3352, loss 0.0319466, acc 0.98
2016-09-06T18:55:07.146954: step 3353, loss 0.0113572, acc 1
2016-09-06T18:55:07.830436: step 3354, loss 0.0196902, acc 1
2016-09-06T18:55:08.548757: step 3355, loss 0.0743257, acc 0.98
2016-09-06T18:55:09.231185: step 3356, loss 0.0438901, acc 1
2016-09-06T18:55:09.907681: step 3357, loss 0.0170224, acc 1
2016-09-06T18:55:10.606403: step 3358, loss 0.0116092, acc 1
2016-09-06T18:55:11.301884: step 3359, loss 0.0219435, acc 0.98
2016-09-06T18:55:11.984679: step 3360, loss 0.0118802, acc 1
2016-09-06T18:55:12.678524: step 3361, loss 0.0100461, acc 1
2016-09-06T18:55:13.383983: step 3362, loss 0.0277247, acc 0.98
2016-09-06T18:55:14.067097: step 3363, loss 0.0408574, acc 0.96
2016-09-06T18:55:14.744730: step 3364, loss 0.00257729, acc 1
2016-09-06T18:55:15.435665: step 3365, loss 0.066269, acc 0.98
2016-09-06T18:55:16.124457: step 3366, loss 6.88429e-05, acc 1
2016-09-06T18:55:16.833790: step 3367, loss 0.0568544, acc 0.96
2016-09-06T18:55:17.494753: step 3368, loss 0.0162164, acc 1
2016-09-06T18:55:18.243684: step 3369, loss 0.00650875, acc 1
2016-09-06T18:55:18.955635: step 3370, loss 0.118835, acc 0.98
2016-09-06T18:55:19.636200: step 3371, loss 0.066213, acc 0.96
2016-09-06T18:55:20.325115: step 3372, loss 0.115403, acc 0.98
2016-09-06T18:55:20.999968: step 3373, loss 0.0967385, acc 0.98
2016-09-06T18:55:21.717905: step 3374, loss 0.0277545, acc 1
2016-09-06T18:55:22.369079: step 3375, loss 0.0616739, acc 0.96
2016-09-06T18:55:23.066786: step 3376, loss 0.000743239, acc 1
2016-09-06T18:55:23.747723: step 3377, loss 0.0737446, acc 0.98
2016-09-06T18:55:24.455878: step 3378, loss 0.0513514, acc 0.98
2016-09-06T18:55:25.143470: step 3379, loss 0.0244966, acc 1
2016-09-06T18:55:25.818155: step 3380, loss 0.0247277, acc 0.98
2016-09-06T18:55:26.513154: step 3381, loss 0.0452688, acc 0.98
2016-09-06T18:55:27.174061: step 3382, loss 0.0008743, acc 1
2016-09-06T18:55:27.881040: step 3383, loss 0.0326576, acc 0.98
2016-09-06T18:55:28.550950: step 3384, loss 0.0525888, acc 0.94
2016-09-06T18:55:29.230362: step 3385, loss 0.0241814, acc 0.98
2016-09-06T18:55:29.933841: step 3386, loss 0.00260608, acc 1
2016-09-06T18:55:30.648057: step 3387, loss 0.0238852, acc 1
2016-09-06T18:55:31.355677: step 3388, loss 0.0467132, acc 0.98
2016-09-06T18:55:32.012715: step 3389, loss 0.0139529, acc 1
2016-09-06T18:55:32.732793: step 3390, loss 0.0701321, acc 0.96
2016-09-06T18:55:33.430265: step 3391, loss 0.0712875, acc 0.98
2016-09-06T18:55:34.132531: step 3392, loss 0.00883111, acc 1
2016-09-06T18:55:34.811563: step 3393, loss 0.00408056, acc 1
2016-09-06T18:55:35.500074: step 3394, loss 0.0519408, acc 0.98
2016-09-06T18:55:36.212140: step 3395, loss 0.0493733, acc 0.98
2016-09-06T18:55:36.884164: step 3396, loss 0.00612987, acc 1
2016-09-06T18:55:37.550916: step 3397, loss 0.131425, acc 0.96
2016-09-06T18:55:38.244116: step 3398, loss 0.00176428, acc 1
2016-09-06T18:55:38.939851: step 3399, loss 0.000413518, acc 1
2016-09-06T18:55:39.627685: step 3400, loss 0.176515, acc 0.98

Evaluation:
2016-09-06T18:55:42.762649: step 3400, loss 1.63128, acc 0.758912

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-3400

2016-09-06T18:55:44.476411: step 3401, loss 0.0577669, acc 0.96
2016-09-06T18:55:45.151228: step 3402, loss 0.000686833, acc 1
2016-09-06T18:55:45.846369: step 3403, loss 0.0148047, acc 1
2016-09-06T18:55:46.528048: step 3404, loss 0.036855, acc 0.96
2016-09-06T18:55:47.211025: step 3405, loss 0.00795556, acc 1
2016-09-06T18:55:47.900495: step 3406, loss 0.012874, acc 1
2016-09-06T18:55:48.596637: step 3407, loss 0.00192429, acc 1
2016-09-06T18:55:49.285004: step 3408, loss 0.0167996, acc 1
2016-09-06T18:55:49.957753: step 3409, loss 0.0798371, acc 0.96
2016-09-06T18:55:50.659234: step 3410, loss 0.0616729, acc 0.98
2016-09-06T18:55:51.336363: step 3411, loss 0.0044016, acc 1
2016-09-06T18:55:52.018953: step 3412, loss 0.0456245, acc 1
2016-09-06T18:55:52.707041: step 3413, loss 0.0505144, acc 0.98
2016-09-06T18:55:53.385518: step 3414, loss 0.0186665, acc 0.98
2016-09-06T18:55:54.071599: step 3415, loss 0.0095401, acc 1
2016-09-06T18:55:54.730220: step 3416, loss 0.0482195, acc 0.96
2016-09-06T18:55:55.434238: step 3417, loss 0.000754943, acc 1
2016-09-06T18:55:56.106420: step 3418, loss 0.00564591, acc 1
2016-09-06T18:55:56.770531: step 3419, loss 0.0459409, acc 0.98
2016-09-06T18:55:57.461715: step 3420, loss 0.0198066, acc 0.98
2016-09-06T18:55:58.141017: step 3421, loss 0.0166251, acc 0.98
2016-09-06T18:55:58.831252: step 3422, loss 0.0443839, acc 0.98
2016-09-06T18:55:59.513857: step 3423, loss 0.00481428, acc 1
2016-09-06T18:56:00.224626: step 3424, loss 0.0394847, acc 0.98
2016-09-06T18:56:00.912427: step 3425, loss 0.00941795, acc 1
2016-09-06T18:56:01.618598: step 3426, loss 0.0232541, acc 0.98
2016-09-06T18:56:02.316285: step 3427, loss 0.0219644, acc 1
2016-09-06T18:56:03.014290: step 3428, loss 0.0181402, acc 0.98
2016-09-06T18:56:03.703578: step 3429, loss 0.0201959, acc 1
2016-09-06T18:56:04.380310: step 3430, loss 0.0183008, acc 1
2016-09-06T18:56:05.128791: step 3431, loss 0.0663318, acc 0.96
2016-09-06T18:56:05.844618: step 3432, loss 0.0514031, acc 0.96
2016-09-06T18:56:06.532827: step 3433, loss 0.0321521, acc 0.98
2016-09-06T18:56:07.233497: step 3434, loss 0.019473, acc 1
2016-09-06T18:56:07.918924: step 3435, loss 0.0178071, acc 1
2016-09-06T18:56:08.588657: step 3436, loss 0.00720525, acc 1
2016-09-06T18:56:09.234511: step 3437, loss 0.0107825, acc 1
2016-09-06T18:56:09.952309: step 3438, loss 0.126869, acc 0.94
2016-09-06T18:56:10.657564: step 3439, loss 0.0328336, acc 0.98
2016-09-06T18:56:11.324251: step 3440, loss 0.0169342, acc 0.98
2016-09-06T18:56:12.016878: step 3441, loss 0.0179872, acc 0.98
2016-09-06T18:56:12.691334: step 3442, loss 0.0447432, acc 0.98
2016-09-06T18:56:13.366226: step 3443, loss 0.00277365, acc 1
2016-09-06T18:56:14.028904: step 3444, loss 0.0968935, acc 0.96
2016-09-06T18:56:14.727198: step 3445, loss 0.00123665, acc 1
2016-09-06T18:56:15.407518: step 3446, loss 0.0128676, acc 1
2016-09-06T18:56:16.091124: step 3447, loss 0.00395873, acc 1
2016-09-06T18:56:16.782614: step 3448, loss 0.0413873, acc 0.98
2016-09-06T18:56:17.480235: step 3449, loss 0.050724, acc 0.96
2016-09-06T18:56:18.177836: step 3450, loss 0.0235764, acc 0.98
2016-09-06T18:56:18.838804: step 3451, loss 0.0591795, acc 0.96
2016-09-06T18:56:19.531580: step 3452, loss 0.0267028, acc 0.98
2016-09-06T18:56:20.210376: step 3453, loss 0.0274038, acc 1
2016-09-06T18:56:20.892386: step 3454, loss 0.142861, acc 0.98
2016-09-06T18:56:21.580712: step 3455, loss 0.0489589, acc 0.96
2016-09-06T18:56:22.229633: step 3456, loss 0.000538322, acc 1
2016-09-06T18:56:22.900156: step 3457, loss 0.0395886, acc 1
2016-09-06T18:56:23.574344: step 3458, loss 0.0109648, acc 1
2016-09-06T18:56:24.291248: step 3459, loss 0.0333671, acc 0.98
2016-09-06T18:56:24.979306: step 3460, loss 0.069889, acc 0.96
2016-09-06T18:56:25.647558: step 3461, loss 0.00978919, acc 1
2016-09-06T18:56:26.330013: step 3462, loss 0.0162628, acc 0.98
2016-09-06T18:56:27.030213: step 3463, loss 0.00027449, acc 1
2016-09-06T18:56:27.749773: step 3464, loss 0.0713517, acc 0.98
2016-09-06T18:56:28.406196: step 3465, loss 0.0532996, acc 0.96
2016-09-06T18:56:29.109792: step 3466, loss 0.0219466, acc 0.98
2016-09-06T18:56:29.786734: step 3467, loss 0.0680121, acc 0.96
2016-09-06T18:56:30.478424: step 3468, loss 0.00349838, acc 1
2016-09-06T18:56:31.177829: step 3469, loss 0.070768, acc 0.98
2016-09-06T18:56:31.874688: step 3470, loss 0.00636748, acc 1
2016-09-06T18:56:32.572403: step 3471, loss 0.0202518, acc 1
2016-09-06T18:56:33.227833: step 3472, loss 0.0324215, acc 0.98
2016-09-06T18:56:33.918184: step 3473, loss 0.0249913, acc 0.98
2016-09-06T18:56:34.578598: step 3474, loss 0.0007112, acc 1
2016-09-06T18:56:35.246641: step 3475, loss 0.0216475, acc 0.98
2016-09-06T18:56:35.953564: step 3476, loss 0.024324, acc 1
2016-09-06T18:56:36.637903: step 3477, loss 0.0790236, acc 0.96
2016-09-06T18:56:37.328599: step 3478, loss 0.0129745, acc 1
2016-09-06T18:56:38.034175: step 3479, loss 0.0159561, acc 1
2016-09-06T18:56:38.735968: step 3480, loss 0.00210219, acc 1
2016-09-06T18:56:39.409273: step 3481, loss 0.0203616, acc 0.98
2016-09-06T18:56:40.093977: step 3482, loss 0.0142525, acc 1
2016-09-06T18:56:40.782616: step 3483, loss 0.00973256, acc 1
2016-09-06T18:56:41.479781: step 3484, loss 0.005057, acc 1
2016-09-06T18:56:42.169287: step 3485, loss 0.0314478, acc 1
2016-09-06T18:56:42.831414: step 3486, loss 0.0411117, acc 0.98
2016-09-06T18:56:43.518458: step 3487, loss 0.00651977, acc 1
2016-09-06T18:56:44.173726: step 3488, loss 0.0158044, acc 1
2016-09-06T18:56:44.848316: step 3489, loss 0.0340116, acc 1
2016-09-06T18:56:45.543342: step 3490, loss 0.0157252, acc 1
2016-09-06T18:56:46.230676: step 3491, loss 0.021183, acc 0.98
2016-09-06T18:56:46.908517: step 3492, loss 0.0193424, acc 1
2016-09-06T18:56:47.582795: step 3493, loss 0.00281144, acc 1
2016-09-06T18:56:48.282545: step 3494, loss 0.0257748, acc 0.98
2016-09-06T18:56:48.955019: step 3495, loss 0.00339434, acc 1
2016-09-06T18:56:49.651488: step 3496, loss 0.074536, acc 0.98
2016-09-06T18:56:50.377637: step 3497, loss 0.0318644, acc 0.98
2016-09-06T18:56:51.046959: step 3498, loss 0.0236563, acc 0.98
2016-09-06T18:56:51.726910: step 3499, loss 0.0561425, acc 0.96
2016-09-06T18:56:52.409881: step 3500, loss 0.124343, acc 0.96

Evaluation:
2016-09-06T18:56:55.551806: step 3500, loss 1.81949, acc 0.744841

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-3500

2016-09-06T18:56:57.223339: step 3501, loss 0.128708, acc 0.96
2016-09-06T18:56:57.908895: step 3502, loss 0.0408302, acc 0.98
2016-09-06T18:56:58.581660: step 3503, loss 0.0342606, acc 0.98
2016-09-06T18:56:59.270837: step 3504, loss 0.0703478, acc 0.98
2016-09-06T18:56:59.952243: step 3505, loss 0.0662314, acc 0.96
2016-09-06T18:57:00.679103: step 3506, loss 0.0198522, acc 1
2016-09-06T18:57:01.365300: step 3507, loss 0.0343116, acc 0.98
2016-09-06T18:57:02.051706: step 3508, loss 0.00748671, acc 1
2016-09-06T18:57:02.749487: step 3509, loss 0.00395137, acc 1
2016-09-06T18:57:03.437006: step 3510, loss 0.060187, acc 0.96
2016-09-06T18:57:04.142734: step 3511, loss 0.0404053, acc 1
2016-09-06T18:57:04.831922: step 3512, loss 0.0475167, acc 0.96
2016-09-06T18:57:05.525558: step 3513, loss 0.053321, acc 0.98
2016-09-06T18:57:06.238845: step 3514, loss 0.060797, acc 0.98
2016-09-06T18:57:06.916168: step 3515, loss 0.0433932, acc 0.98
2016-09-06T18:57:07.616823: step 3516, loss 0.0590395, acc 0.98
2016-09-06T18:57:08.286295: step 3517, loss 0.00305411, acc 1
2016-09-06T18:57:08.963104: step 3518, loss 0.055544, acc 0.98
2016-09-06T18:57:09.648145: step 3519, loss 0.00379598, acc 1
2016-09-06T18:57:10.350499: step 3520, loss 0.0973918, acc 0.92
2016-09-06T18:57:11.046355: step 3521, loss 0.0136761, acc 1
2016-09-06T18:57:11.707529: step 3522, loss 0.0441519, acc 0.96
2016-09-06T18:57:12.422613: step 3523, loss 0.0213052, acc 1
2016-09-06T18:57:13.099848: step 3524, loss 0.0636176, acc 0.98
2016-09-06T18:57:13.788885: step 3525, loss 0.0197956, acc 1
2016-09-06T18:57:14.501689: step 3526, loss 0.0185256, acc 1
2016-09-06T18:57:15.194157: step 3527, loss 0.00954393, acc 1
2016-09-06T18:57:15.899913: step 3528, loss 0.0154545, acc 1
2016-09-06T18:57:16.578244: step 3529, loss 0.0640581, acc 0.96
2016-09-06T18:57:17.290207: step 3530, loss 0.0228016, acc 0.98
2016-09-06T18:57:17.975998: step 3531, loss 0.024268, acc 0.98
2016-09-06T18:57:18.653380: step 3532, loss 0.0187584, acc 1
2016-09-06T18:57:19.343586: step 3533, loss 0.0374208, acc 0.98
2016-09-06T18:57:20.026619: step 3534, loss 0.0372147, acc 0.96
2016-09-06T18:57:20.711473: step 3535, loss 0.0145187, acc 1
2016-09-06T18:57:21.356041: step 3536, loss 0.0454379, acc 0.96
2016-09-06T18:57:22.069462: step 3537, loss 0.0268924, acc 1
2016-09-06T18:57:22.742957: step 3538, loss 0.0320444, acc 0.98
2016-09-06T18:57:23.425862: step 3539, loss 0.00466379, acc 1
2016-09-06T18:57:24.112131: step 3540, loss 0.0203786, acc 1
2016-09-06T18:57:24.794263: step 3541, loss 0.0191751, acc 1
2016-09-06T18:57:25.480207: step 3542, loss 0.0206029, acc 1
2016-09-06T18:57:26.137434: step 3543, loss 0.017594, acc 1
2016-09-06T18:57:26.830744: step 3544, loss 0.0455628, acc 1
2016-09-06T18:57:27.535566: step 3545, loss 0.00310158, acc 1
2016-09-06T18:57:28.227724: step 3546, loss 0.0359142, acc 0.98
2016-09-06T18:57:28.928377: step 3547, loss 0.0621723, acc 0.98
2016-09-06T18:57:29.623391: step 3548, loss 0.0336744, acc 1
2016-09-06T18:57:30.331921: step 3549, loss 0.000385892, acc 1
2016-09-06T18:57:31.005956: step 3550, loss 0.0118146, acc 1
2016-09-06T18:57:31.710794: step 3551, loss 0.0133073, acc 1
2016-09-06T18:57:32.387359: step 3552, loss 0.00699, acc 1
2016-09-06T18:57:33.074475: step 3553, loss 0.024093, acc 1
2016-09-06T18:57:33.777512: step 3554, loss 0.0443144, acc 1
2016-09-06T18:57:34.452368: step 3555, loss 0.00197652, acc 1
2016-09-06T18:57:35.149510: step 3556, loss 0.00384012, acc 1
2016-09-06T18:57:35.818251: step 3557, loss 0.0832089, acc 0.98
2016-09-06T18:57:36.523663: step 3558, loss 0.00237491, acc 1
2016-09-06T18:57:37.210677: step 3559, loss 0.0512041, acc 0.96
2016-09-06T18:57:37.929606: step 3560, loss 0.0341559, acc 0.98
2016-09-06T18:57:38.621381: step 3561, loss 0.0367822, acc 0.98
2016-09-06T18:57:39.315363: step 3562, loss 0.0188912, acc 0.98
2016-09-06T18:57:40.054145: step 3563, loss 0.0232753, acc 0.98
2016-09-06T18:57:40.713143: step 3564, loss 0.0214424, acc 0.98
2016-09-06T18:57:41.409514: step 3565, loss 0.0634261, acc 0.96
2016-09-06T18:57:42.110912: step 3566, loss 0.0386513, acc 0.96
2016-09-06T18:57:42.793139: step 3567, loss 0.0180128, acc 1
2016-09-06T18:57:43.476452: step 3568, loss 0.0270692, acc 0.98
2016-09-06T18:57:44.151599: step 3569, loss 0.144629, acc 0.98
2016-09-06T18:57:44.852371: step 3570, loss 0.00309039, acc 1
2016-09-06T18:57:45.534564: step 3571, loss 0.0704371, acc 0.98
2016-09-06T18:57:46.234497: step 3572, loss 0.0484266, acc 0.96
2016-09-06T18:57:46.924811: step 3573, loss 0.0197005, acc 0.98
2016-09-06T18:57:47.593299: step 3574, loss 0.036289, acc 0.98
2016-09-06T18:57:48.289967: step 3575, loss 0.0199354, acc 1
2016-09-06T18:57:48.956964: step 3576, loss 0.0619996, acc 0.96
2016-09-06T18:57:49.655851: step 3577, loss 0.0208331, acc 1
2016-09-06T18:57:50.333877: step 3578, loss 0.0408114, acc 0.96
2016-09-06T18:57:51.028697: step 3579, loss 0.0369846, acc 0.98
2016-09-06T18:57:51.710241: step 3580, loss 0.00695371, acc 1
2016-09-06T18:57:52.404136: step 3581, loss 0.00379663, acc 1
2016-09-06T18:57:53.102287: step 3582, loss 0.0355258, acc 1
2016-09-06T18:57:53.770308: step 3583, loss 0.0450532, acc 0.98
2016-09-06T18:57:54.478158: step 3584, loss 0.045506, acc 1
2016-09-06T18:57:55.156607: step 3585, loss 0.00872967, acc 1
2016-09-06T18:57:55.838705: step 3586, loss 0.0119994, acc 1
2016-09-06T18:57:56.535538: step 3587, loss 0.0563265, acc 0.98
2016-09-06T18:57:57.238454: step 3588, loss 0.0211189, acc 0.98
2016-09-06T18:57:57.928461: step 3589, loss 0.045247, acc 0.98
2016-09-06T18:57:58.610484: step 3590, loss 0.0187086, acc 0.98
2016-09-06T18:57:59.318852: step 3591, loss 0.0156686, acc 1
2016-09-06T18:57:59.999786: step 3592, loss 0.0183959, acc 0.98
2016-09-06T18:58:00.720638: step 3593, loss 0.01584, acc 1
2016-09-06T18:58:01.407290: step 3594, loss 0.0022915, acc 1
2016-09-06T18:58:02.096289: step 3595, loss 0.0598522, acc 0.96
2016-09-06T18:58:02.757673: step 3596, loss 0.0262357, acc 0.98
2016-09-06T18:58:03.407794: step 3597, loss 0.0122161, acc 1
2016-09-06T18:58:04.106095: step 3598, loss 0.0617555, acc 0.98
2016-09-06T18:58:04.782163: step 3599, loss 0.00253984, acc 1
2016-09-06T18:58:05.465928: step 3600, loss 0.000709632, acc 1

Evaluation:
2016-09-06T18:58:08.630586: step 3600, loss 1.85658, acc 0.746717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-3600

2016-09-06T18:58:10.263577: step 3601, loss 0.0355092, acc 0.98
2016-09-06T18:58:10.949988: step 3602, loss 0.012843, acc 1
2016-09-06T18:58:11.617702: step 3603, loss 0.0388085, acc 0.98
2016-09-06T18:58:12.299209: step 3604, loss 0.088291, acc 0.92
2016-09-06T18:58:12.998764: step 3605, loss 0.078628, acc 0.94
2016-09-06T18:58:13.692108: step 3606, loss 0.0252521, acc 0.98
2016-09-06T18:58:14.369836: step 3607, loss 0.053309, acc 0.98
2016-09-06T18:58:15.049943: step 3608, loss 0.0452623, acc 0.96
2016-09-06T18:58:15.728825: step 3609, loss 0.0108643, acc 1
2016-09-06T18:58:16.409009: step 3610, loss 0.0448967, acc 1
2016-09-06T18:58:17.076778: step 3611, loss 0.0940696, acc 0.98
2016-09-06T18:58:17.746481: step 3612, loss 0.00065725, acc 1
2016-09-06T18:58:18.459575: step 3613, loss 0.0157317, acc 1
2016-09-06T18:58:19.140354: step 3614, loss 0.0223929, acc 1
2016-09-06T18:58:19.845034: step 3615, loss 0.00789216, acc 1
2016-09-06T18:58:20.537331: step 3616, loss 0.10706, acc 0.92
2016-09-06T18:58:21.245858: step 3617, loss 0.00185605, acc 1
2016-09-06T18:58:21.956308: step 3618, loss 0.0171618, acc 0.98
2016-09-06T18:58:22.640494: step 3619, loss 0.00841576, acc 1
2016-09-06T18:58:23.371240: step 3620, loss 0.0192937, acc 0.98
2016-09-06T18:58:24.052435: step 3621, loss 0.0452188, acc 0.96
2016-09-06T18:58:24.769273: step 3622, loss 0.192349, acc 0.92
2016-09-06T18:58:25.449359: step 3623, loss 0.104341, acc 0.96
2016-09-06T18:58:26.144180: step 3624, loss 0.0177682, acc 0.98
2016-09-06T18:58:26.809489: step 3625, loss 0.0182117, acc 1
2016-09-06T18:58:27.470800: step 3626, loss 0.0669744, acc 0.96
2016-09-06T18:58:28.178954: step 3627, loss 0.000688532, acc 1
2016-09-06T18:58:28.858905: step 3628, loss 0.0626685, acc 0.98
2016-09-06T18:58:29.536025: step 3629, loss 0.00835254, acc 1
2016-09-06T18:58:30.227166: step 3630, loss 0.159791, acc 0.98
2016-09-06T18:58:30.915881: step 3631, loss 0.0902798, acc 0.96
2016-09-06T18:58:31.621766: step 3632, loss 0.0184423, acc 0.98
2016-09-06T18:58:32.301515: step 3633, loss 0.0506534, acc 0.98
2016-09-06T18:58:33.012831: step 3634, loss 0.0449067, acc 0.96
2016-09-06T18:58:33.687788: step 3635, loss 0.00468715, acc 1
2016-09-06T18:58:34.376876: step 3636, loss 0.0368332, acc 0.98
2016-09-06T18:58:35.057939: step 3637, loss 0.0344306, acc 0.96
2016-09-06T18:58:35.726210: step 3638, loss 0.0293381, acc 0.98
2016-09-06T18:58:36.415818: step 3639, loss 0.0403279, acc 0.98
2016-09-06T18:58:37.066817: step 3640, loss 0.0489387, acc 0.96
2016-09-06T18:58:37.781330: step 3641, loss 0.0337929, acc 0.98
2016-09-06T18:58:38.441734: step 3642, loss 0.0259635, acc 0.98
2016-09-06T18:58:39.112716: step 3643, loss 0.0834795, acc 0.96
2016-09-06T18:58:39.785517: step 3644, loss 0.0153272, acc 1
2016-09-06T18:58:40.461263: step 3645, loss 0.0265622, acc 0.98
2016-09-06T18:58:41.134188: step 3646, loss 0.0327589, acc 0.98
2016-09-06T18:58:41.813417: step 3647, loss 0.0274373, acc 1
2016-09-06T18:58:42.440553: step 3648, loss 0.0172665, acc 1
2016-09-06T18:58:43.093381: step 3649, loss 0.0536053, acc 0.98
2016-09-06T18:58:43.787398: step 3650, loss 0.0253011, acc 0.98
2016-09-06T18:58:44.453710: step 3651, loss 0.0106033, acc 1
2016-09-06T18:58:45.143723: step 3652, loss 0.0127705, acc 1
2016-09-06T18:58:45.828027: step 3653, loss 0.0302777, acc 0.98
2016-09-06T18:58:46.504045: step 3654, loss 0.0779059, acc 0.96
2016-09-06T18:58:47.180891: step 3655, loss 0.0097546, acc 1
2016-09-06T18:58:47.863074: step 3656, loss 0.0124546, acc 1
2016-09-06T18:58:48.548673: step 3657, loss 0.0626818, acc 0.98
2016-09-06T18:58:49.222608: step 3658, loss 0.0172264, acc 1
2016-09-06T18:58:49.918872: step 3659, loss 0.0255905, acc 0.98
2016-09-06T18:58:50.634559: step 3660, loss 0.0446628, acc 0.96
2016-09-06T18:58:51.317184: step 3661, loss 0.0399328, acc 0.98
2016-09-06T18:58:51.989123: step 3662, loss 0.0149981, acc 1
2016-09-06T18:58:52.671784: step 3663, loss 0.0441607, acc 0.96
2016-09-06T18:58:53.382069: step 3664, loss 0.0360616, acc 0.98
2016-09-06T18:58:54.066348: step 3665, loss 0.0114979, acc 1
2016-09-06T18:58:54.731639: step 3666, loss 0.0154265, acc 1
2016-09-06T18:58:55.418957: step 3667, loss 0.0211688, acc 0.98
2016-09-06T18:58:56.113094: step 3668, loss 0.0442367, acc 1
2016-09-06T18:58:56.811391: step 3669, loss 0.0452808, acc 0.98
2016-09-06T18:58:57.490575: step 3670, loss 0.00240074, acc 1
2016-09-06T18:58:58.200678: step 3671, loss 0.0152852, acc 1
2016-09-06T18:58:58.921929: step 3672, loss 0.0837621, acc 0.96
2016-09-06T18:58:59.609108: step 3673, loss 0.019137, acc 0.98
2016-09-06T18:59:00.315814: step 3674, loss 0.0400142, acc 0.98
2016-09-06T18:59:01.007051: step 3675, loss 0.00853045, acc 1
2016-09-06T18:59:01.696375: step 3676, loss 0.109303, acc 0.96
2016-09-06T18:59:02.355673: step 3677, loss 0.0397161, acc 0.98
2016-09-06T18:59:03.067053: step 3678, loss 0.0608901, acc 0.98
2016-09-06T18:59:03.739468: step 3679, loss 0.00273859, acc 1
2016-09-06T18:59:04.433032: step 3680, loss 0.0324639, acc 0.98
2016-09-06T18:59:05.127438: step 3681, loss 0.0123532, acc 1
2016-09-06T18:59:05.813713: step 3682, loss 0.0244535, acc 0.98
2016-09-06T18:59:06.494560: step 3683, loss 0.0482767, acc 0.98
2016-09-06T18:59:07.177586: step 3684, loss 0.0189326, acc 0.98
2016-09-06T18:59:07.859485: step 3685, loss 0.00666134, acc 1
2016-09-06T18:59:08.518737: step 3686, loss 0.0376056, acc 0.98
2016-09-06T18:59:09.238142: step 3687, loss 0.000649127, acc 1
2016-09-06T18:59:09.915656: step 3688, loss 0.00775977, acc 1
2016-09-06T18:59:10.595190: step 3689, loss 0.0248804, acc 0.98
2016-09-06T18:59:11.258957: step 3690, loss 0.0628406, acc 0.98
2016-09-06T18:59:11.948885: step 3691, loss 0.0698136, acc 0.96
2016-09-06T18:59:12.642928: step 3692, loss 0.0288627, acc 0.98
2016-09-06T18:59:13.305903: step 3693, loss 0.0557875, acc 0.96
2016-09-06T18:59:13.995846: step 3694, loss 0.0317136, acc 0.98
2016-09-06T18:59:14.702025: step 3695, loss 0.0146466, acc 1
2016-09-06T18:59:15.394811: step 3696, loss 0.0288889, acc 0.98
2016-09-06T18:59:16.072819: step 3697, loss 0.041381, acc 0.98
2016-09-06T18:59:16.741291: step 3698, loss 0.0365901, acc 0.98
2016-09-06T18:59:17.462914: step 3699, loss 0.129771, acc 0.96
2016-09-06T18:59:18.176485: step 3700, loss 0.0127918, acc 1

Evaluation:
2016-09-06T18:59:21.327284: step 3700, loss 1.77937, acc 0.751407

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-3700

2016-09-06T18:59:23.043636: step 3701, loss 0.164351, acc 0.98
2016-09-06T18:59:23.730296: step 3702, loss 0.0197885, acc 0.98
2016-09-06T18:59:24.411192: step 3703, loss 0.0155576, acc 1
2016-09-06T18:59:25.101075: step 3704, loss 0.0265392, acc 0.98
2016-09-06T18:59:25.801277: step 3705, loss 0.0303627, acc 0.96
2016-09-06T18:59:26.479731: step 3706, loss 0.153315, acc 0.94
2016-09-06T18:59:27.177803: step 3707, loss 0.0247137, acc 1
2016-09-06T18:59:27.876911: step 3708, loss 0.000824184, acc 1
2016-09-06T18:59:28.560796: step 3709, loss 0.0469358, acc 0.98
2016-09-06T18:59:29.252504: step 3710, loss 0.00448656, acc 1
2016-09-06T18:59:29.956946: step 3711, loss 0.0218163, acc 1
2016-09-06T18:59:30.663470: step 3712, loss 0.0686531, acc 0.98
2016-09-06T18:59:31.350349: step 3713, loss 0.0257825, acc 1
2016-09-06T18:59:32.054952: step 3714, loss 0.0157578, acc 1
2016-09-06T18:59:32.749205: step 3715, loss 0.000891229, acc 1
2016-09-06T18:59:33.438394: step 3716, loss 0.0240885, acc 1
2016-09-06T18:59:34.126879: step 3717, loss 0.05942, acc 0.96
2016-09-06T18:59:34.783821: step 3718, loss 0.0119003, acc 1
2016-09-06T18:59:35.468389: step 3719, loss 0.0298809, acc 0.98
2016-09-06T18:59:36.134736: step 3720, loss 0.011779, acc 1
2016-09-06T18:59:36.813872: step 3721, loss 0.121386, acc 0.96
2016-09-06T18:59:37.505459: step 3722, loss 0.0301368, acc 0.98
2016-09-06T18:59:38.186152: step 3723, loss 0.151529, acc 0.94
2016-09-06T18:59:38.868262: step 3724, loss 0.0235577, acc 1
2016-09-06T18:59:39.570303: step 3725, loss 0.0414416, acc 1
2016-09-06T18:59:40.286155: step 3726, loss 0.00765889, acc 1
2016-09-06T18:59:40.958742: step 3727, loss 0.0514689, acc 0.98
2016-09-06T18:59:41.630360: step 3728, loss 0.0358441, acc 0.98
2016-09-06T18:59:42.315943: step 3729, loss 0.00631529, acc 1
2016-09-06T18:59:43.019939: step 3730, loss 0.136402, acc 0.96
2016-09-06T18:59:43.706581: step 3731, loss 0.110281, acc 0.94
2016-09-06T18:59:44.375785: step 3732, loss 0.0303826, acc 0.98
2016-09-06T18:59:45.080448: step 3733, loss 0.0139192, acc 1
2016-09-06T18:59:45.758637: step 3734, loss 0.0203161, acc 0.98
2016-09-06T18:59:46.438574: step 3735, loss 0.209036, acc 0.92
2016-09-06T18:59:47.124785: step 3736, loss 0.0996573, acc 0.96
2016-09-06T18:59:47.836230: step 3737, loss 0.082096, acc 0.92
2016-09-06T18:59:48.519461: step 3738, loss 0.0318804, acc 0.98
2016-09-06T18:59:49.212546: step 3739, loss 0.0428387, acc 0.98
2016-09-06T18:59:49.906351: step 3740, loss 0.0324682, acc 0.98
2016-09-06T18:59:50.581205: step 3741, loss 0.0145446, acc 1
2016-09-06T18:59:51.266234: step 3742, loss 0.0131005, acc 1
2016-09-06T18:59:51.963646: step 3743, loss 0.052323, acc 0.98
2016-09-06T18:59:52.661506: step 3744, loss 0.0392917, acc 0.98
2016-09-06T18:59:53.362302: step 3745, loss 0.0459277, acc 0.98
2016-09-06T18:59:54.020608: step 3746, loss 0.0206567, acc 1
2016-09-06T18:59:54.741298: step 3747, loss 0.0135863, acc 1
2016-09-06T18:59:55.412575: step 3748, loss 0.05895, acc 0.98
2016-09-06T18:59:56.125640: step 3749, loss 0.0268291, acc 0.98
2016-09-06T18:59:56.815989: step 3750, loss 0.00669731, acc 1
2016-09-06T18:59:57.519076: step 3751, loss 0.0356279, acc 0.98
2016-09-06T18:59:58.216877: step 3752, loss 0.0237878, acc 1
2016-09-06T18:59:58.889683: step 3753, loss 0.0243479, acc 1
2016-09-06T18:59:59.575761: step 3754, loss 0.0256947, acc 0.98
2016-09-06T19:00:00.288699: step 3755, loss 0.0182638, acc 1
2016-09-06T19:00:00.952539: step 3756, loss 0.109416, acc 0.94
2016-09-06T19:00:01.656859: step 3757, loss 0.0630581, acc 0.98
2016-09-06T19:00:02.357974: step 3758, loss 0.0392438, acc 0.98
2016-09-06T19:00:03.071802: step 3759, loss 0.00194375, acc 1
2016-09-06T19:00:03.737180: step 3760, loss 0.0501298, acc 0.98
2016-09-06T19:00:04.431377: step 3761, loss 0.139379, acc 0.98
2016-09-06T19:00:05.105405: step 3762, loss 0.0249683, acc 1
2016-09-06T19:00:05.803211: step 3763, loss 0.0116459, acc 1
2016-09-06T19:00:06.488425: step 3764, loss 0.00119927, acc 1
2016-09-06T19:00:07.182818: step 3765, loss 0.0024532, acc 1
2016-09-06T19:00:07.890193: step 3766, loss 0.0650982, acc 0.96
2016-09-06T19:00:08.535973: step 3767, loss 0.0254753, acc 0.98
2016-09-06T19:00:09.246295: step 3768, loss 0.00910052, acc 1
2016-09-06T19:00:09.949333: step 3769, loss 0.0168934, acc 0.98
2016-09-06T19:00:10.637969: step 3770, loss 0.00469848, acc 1
2016-09-06T19:00:11.331479: step 3771, loss 0.113323, acc 0.96
2016-09-06T19:00:12.017232: step 3772, loss 0.0128299, acc 1
2016-09-06T19:00:12.726413: step 3773, loss 0.0396166, acc 0.98
2016-09-06T19:00:13.410133: step 3774, loss 0.0486054, acc 0.96
2016-09-06T19:00:14.136403: step 3775, loss 0.0186264, acc 0.98
2016-09-06T19:00:14.823275: step 3776, loss 0.0589517, acc 0.94
2016-09-06T19:00:15.507435: step 3777, loss 0.00317651, acc 1
2016-09-06T19:00:16.221122: step 3778, loss 0.0144695, acc 1
2016-09-06T19:00:16.895901: step 3779, loss 0.0994798, acc 0.96
2016-09-06T19:00:17.596226: step 3780, loss 0.0224722, acc 0.98
2016-09-06T19:00:18.297962: step 3781, loss 0.0293631, acc 1
2016-09-06T19:00:18.990173: step 3782, loss 0.00688563, acc 1
2016-09-06T19:00:19.686105: step 3783, loss 0.0126547, acc 1
2016-09-06T19:00:20.370718: step 3784, loss 0.0201478, acc 0.98
2016-09-06T19:00:21.042700: step 3785, loss 0.0737041, acc 0.94
2016-09-06T19:00:21.703031: step 3786, loss 0.0085899, acc 1
2016-09-06T19:00:22.411166: step 3787, loss 0.00373339, acc 1
2016-09-06T19:00:23.088813: step 3788, loss 0.0079221, acc 1
2016-09-06T19:00:23.784971: step 3789, loss 0.0804979, acc 0.98
2016-09-06T19:00:24.495298: step 3790, loss 0.0467068, acc 0.98
2016-09-06T19:00:25.185575: step 3791, loss 0.0302384, acc 0.98
2016-09-06T19:00:25.896150: step 3792, loss 0.00626755, acc 1
2016-09-06T19:00:26.567745: step 3793, loss 0.0428725, acc 0.98
2016-09-06T19:00:27.257814: step 3794, loss 0.0302732, acc 1
2016-09-06T19:00:27.953331: step 3795, loss 0.00215392, acc 1
2016-09-06T19:00:28.650778: step 3796, loss 0.0849088, acc 0.94
2016-09-06T19:00:29.351560: step 3797, loss 0.0408333, acc 1
2016-09-06T19:00:30.036044: step 3798, loss 0.0342252, acc 0.98
2016-09-06T19:00:30.724147: step 3799, loss 0.025316, acc 1
2016-09-06T19:00:31.383386: step 3800, loss 0.011973, acc 1

Evaluation:
2016-09-06T19:00:34.506966: step 3800, loss 1.53953, acc 0.743902

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-3800

2016-09-06T19:00:36.196447: step 3801, loss 0.00323485, acc 1
2016-09-06T19:00:36.908847: step 3802, loss 0.051229, acc 0.98
2016-09-06T19:00:37.624313: step 3803, loss 0.0543964, acc 0.98
2016-09-06T19:00:38.306319: step 3804, loss 0.0311734, acc 0.98
2016-09-06T19:00:38.994253: step 3805, loss 0.0122331, acc 1
2016-09-06T19:00:39.705952: step 3806, loss 0.0665757, acc 0.96
2016-09-06T19:00:40.444582: step 3807, loss 0.0238109, acc 1
2016-09-06T19:00:41.131073: step 3808, loss 0.0207757, acc 1
2016-09-06T19:00:41.861122: step 3809, loss 0.0220179, acc 0.98
2016-09-06T19:00:42.538129: step 3810, loss 0.0121182, acc 1
2016-09-06T19:00:43.228184: step 3811, loss 0.0816381, acc 0.94
2016-09-06T19:00:43.950331: step 3812, loss 0.135837, acc 0.96
2016-09-06T19:00:44.630990: step 3813, loss 0.0111724, acc 1
2016-09-06T19:00:45.320669: step 3814, loss 0.10257, acc 0.96
2016-09-06T19:00:46.028370: step 3815, loss 0.029817, acc 0.98
2016-09-06T19:00:46.718151: step 3816, loss 0.104774, acc 0.94
2016-09-06T19:00:47.397396: step 3817, loss 0.0275641, acc 1
2016-09-06T19:00:48.074359: step 3818, loss 0.0211268, acc 1
2016-09-06T19:00:48.769362: step 3819, loss 0.0169262, acc 0.98
2016-09-06T19:00:49.437479: step 3820, loss 0.063412, acc 0.96
2016-09-06T19:00:50.115768: step 3821, loss 0.00382975, acc 1
2016-09-06T19:00:50.814639: step 3822, loss 0.0254477, acc 1
2016-09-06T19:00:51.493362: step 3823, loss 0.00891198, acc 1
2016-09-06T19:00:52.191851: step 3824, loss 0.0263505, acc 1
2016-09-06T19:00:52.881639: step 3825, loss 0.00431273, acc 1
2016-09-06T19:00:53.578588: step 3826, loss 0.0384566, acc 1
2016-09-06T19:00:54.251061: step 3827, loss 0.00256545, acc 1
2016-09-06T19:00:54.958329: step 3828, loss 0.0780568, acc 0.96
2016-09-06T19:00:55.654633: step 3829, loss 0.2111, acc 0.98
2016-09-06T19:00:56.343385: step 3830, loss 0.14063, acc 0.98
2016-09-06T19:00:57.068744: step 3831, loss 0.0184829, acc 1
2016-09-06T19:00:57.729009: step 3832, loss 0.0494113, acc 0.98
2016-09-06T19:00:58.430218: step 3833, loss 0.0375839, acc 0.98
2016-09-06T19:00:59.119990: step 3834, loss 0.0323718, acc 1
2016-09-06T19:00:59.797201: step 3835, loss 0.114594, acc 0.94
2016-09-06T19:01:00.502086: step 3836, loss 0.00717848, acc 1
2016-09-06T19:01:01.205356: step 3837, loss 0.0230614, acc 1
2016-09-06T19:01:01.903739: step 3838, loss 0.0235063, acc 1
2016-09-06T19:01:02.582495: step 3839, loss 0.0617582, acc 0.98
2016-09-06T19:01:03.244857: step 3840, loss 0.0348793, acc 0.977273
2016-09-06T19:01:03.915632: step 3841, loss 0.024443, acc 0.98
2016-09-06T19:01:04.586906: step 3842, loss 0.00591877, acc 1
2016-09-06T19:01:05.266638: step 3843, loss 0.0231472, acc 1
2016-09-06T19:01:05.947492: step 3844, loss 0.0313628, acc 0.98
2016-09-06T19:01:06.654900: step 3845, loss 0.0437714, acc 0.98
2016-09-06T19:01:07.356760: step 3846, loss 0.018948, acc 0.98
2016-09-06T19:01:08.063910: step 3847, loss 0.00440833, acc 1
2016-09-06T19:01:08.724741: step 3848, loss 0.0199785, acc 1
2016-09-06T19:01:09.433333: step 3849, loss 0.0210061, acc 1
2016-09-06T19:01:10.142624: step 3850, loss 0.00970642, acc 1
2016-09-06T19:01:10.846048: step 3851, loss 0.0133056, acc 1
2016-09-06T19:01:11.528500: step 3852, loss 0.0456322, acc 0.98
2016-09-06T19:01:12.242282: step 3853, loss 0.0178503, acc 0.98
2016-09-06T19:01:12.933119: step 3854, loss 0.00931292, acc 1
2016-09-06T19:01:13.636762: step 3855, loss 0.0598008, acc 0.98
2016-09-06T19:01:14.325932: step 3856, loss 0.0462573, acc 0.98
2016-09-06T19:01:15.017302: step 3857, loss 0.0127366, acc 1
2016-09-06T19:01:15.706027: step 3858, loss 0.00298589, acc 1
2016-09-06T19:01:16.417160: step 3859, loss 0.0314563, acc 0.98
2016-09-06T19:01:17.069592: step 3860, loss 0.0495349, acc 0.98
2016-09-06T19:01:17.759573: step 3861, loss 0.00342324, acc 1
2016-09-06T19:01:18.448028: step 3862, loss 0.0362628, acc 0.98
2016-09-06T19:01:19.134242: step 3863, loss 0.021113, acc 1
2016-09-06T19:01:19.832844: step 3864, loss 0.0720971, acc 0.94
2016-09-06T19:01:20.515244: step 3865, loss 0.0270096, acc 0.98
2016-09-06T19:01:21.244098: step 3866, loss 0.0195516, acc 1
2016-09-06T19:01:21.924211: step 3867, loss 0.0110427, acc 1
2016-09-06T19:01:22.609611: step 3868, loss 0.0193578, acc 1
2016-09-06T19:01:23.294018: step 3869, loss 0.0262366, acc 0.98
2016-09-06T19:01:23.978472: step 3870, loss 0.00203203, acc 1
2016-09-06T19:01:24.669218: step 3871, loss 0.0165694, acc 0.98
2016-09-06T19:01:25.371248: step 3872, loss 0.01999, acc 1
2016-09-06T19:01:26.088606: step 3873, loss 0.0382978, acc 1
2016-09-06T19:01:26.771589: step 3874, loss 0.0880696, acc 0.96
2016-09-06T19:01:27.459103: step 3875, loss 0.0399011, acc 0.98
2016-09-06T19:01:28.151415: step 3876, loss 0.0118622, acc 1
2016-09-06T19:01:28.819734: step 3877, loss 0.00223493, acc 1
2016-09-06T19:01:29.551760: step 3878, loss 0.0099774, acc 1
2016-09-06T19:01:30.218414: step 3879, loss 0.0340805, acc 0.98
2016-09-06T19:01:30.924058: step 3880, loss 0.0579843, acc 0.98
2016-09-06T19:01:31.604719: step 3881, loss 0.0246562, acc 1
2016-09-06T19:01:32.282545: step 3882, loss 0.0432918, acc 0.98
2016-09-06T19:01:32.971221: step 3883, loss 0.0506564, acc 0.98
2016-09-06T19:01:33.658959: step 3884, loss 0.0155708, acc 1
2016-09-06T19:01:34.357464: step 3885, loss 0.0196093, acc 0.98
2016-09-06T19:01:35.016487: step 3886, loss 0.00343341, acc 1
2016-09-06T19:01:35.725634: step 3887, loss 0.0212058, acc 0.98
2016-09-06T19:01:36.409439: step 3888, loss 0.046362, acc 0.98
2016-09-06T19:01:37.098083: step 3889, loss 0.102688, acc 0.94
2016-09-06T19:01:37.791685: step 3890, loss 0.0453125, acc 0.98
2016-09-06T19:01:38.463060: step 3891, loss 0.0150879, acc 1
2016-09-06T19:01:39.129662: step 3892, loss 0.00441316, acc 1
2016-09-06T19:01:39.837918: step 3893, loss 0.0221194, acc 0.98
2016-09-06T19:01:40.538427: step 3894, loss 0.09031, acc 0.98
2016-09-06T19:01:41.216483: step 3895, loss 0.0557861, acc 0.98
2016-09-06T19:01:41.898817: step 3896, loss 0.00705005, acc 1
2016-09-06T19:01:42.596528: step 3897, loss 0.0303498, acc 0.98
2016-09-06T19:01:43.292757: step 3898, loss 0.0372547, acc 0.98
2016-09-06T19:01:43.990391: step 3899, loss 0.0906134, acc 0.96
2016-09-06T19:01:44.666636: step 3900, loss 0.00581056, acc 1

Evaluation:
2016-09-06T19:01:47.812670: step 3900, loss 1.4599, acc 0.765478

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-3900

2016-09-06T19:01:49.458609: step 3901, loss 0.032658, acc 0.98
2016-09-06T19:01:50.155100: step 3902, loss 0.0365498, acc 0.98
2016-09-06T19:01:50.835603: step 3903, loss 0.00392697, acc 1
2016-09-06T19:01:51.526804: step 3904, loss 0.0234392, acc 0.98
2016-09-06T19:01:52.222488: step 3905, loss 0.0313504, acc 0.98
2016-09-06T19:01:52.921975: step 3906, loss 0.0183606, acc 0.98
2016-09-06T19:01:53.604154: step 3907, loss 0.00178071, acc 1
2016-09-06T19:01:54.283330: step 3908, loss 0.04603, acc 0.98
2016-09-06T19:01:54.999956: step 3909, loss 0.0318568, acc 0.98
2016-09-06T19:01:55.656640: step 3910, loss 0.0315704, acc 1
2016-09-06T19:01:56.340642: step 3911, loss 0.199276, acc 0.94
2016-09-06T19:01:57.026810: step 3912, loss 0.0422133, acc 0.98
2016-09-06T19:01:57.730860: step 3913, loss 0.00937187, acc 1
2016-09-06T19:01:58.434084: step 3914, loss 0.0405604, acc 0.98
2016-09-06T19:01:59.102621: step 3915, loss 0.00404879, acc 1
2016-09-06T19:01:59.821242: step 3916, loss 0.0246369, acc 1
2016-09-06T19:02:00.545323: step 3917, loss 0.0455432, acc 0.96
2016-09-06T19:02:01.238299: step 3918, loss 0.153876, acc 0.92
2016-09-06T19:02:01.958590: step 3919, loss 0.0218304, acc 0.98
2016-09-06T19:02:02.638763: step 3920, loss 0.0602269, acc 0.98
2016-09-06T19:02:03.340955: step 3921, loss 0.0378078, acc 0.98
2016-09-06T19:02:04.016017: step 3922, loss 0.00411396, acc 1
2016-09-06T19:02:04.692201: step 3923, loss 0.0140441, acc 1
2016-09-06T19:02:05.376244: step 3924, loss 0.0005445, acc 1
2016-09-06T19:02:06.066909: step 3925, loss 0.0112464, acc 1
2016-09-06T19:02:06.761994: step 3926, loss 0.0155903, acc 1
2016-09-06T19:02:07.453855: step 3927, loss 0.0157662, acc 1
2016-09-06T19:02:08.166660: step 3928, loss 0.0104252, acc 1
2016-09-06T19:02:08.850638: step 3929, loss 0.0174896, acc 1
2016-09-06T19:02:09.544474: step 3930, loss 0.0441317, acc 0.98
2016-09-06T19:02:10.254447: step 3931, loss 0.0629122, acc 0.98
2016-09-06T19:02:10.968457: step 3932, loss 0.155972, acc 0.94
2016-09-06T19:02:11.649745: step 3933, loss 0.0186136, acc 0.98
2016-09-06T19:02:12.308466: step 3934, loss 0.0472276, acc 0.98
2016-09-06T19:02:13.018359: step 3935, loss 0.14509, acc 0.94
2016-09-06T19:02:13.681915: step 3936, loss 0.050678, acc 0.98
2016-09-06T19:02:14.381849: step 3937, loss 0.022377, acc 1
2016-09-06T19:02:15.076686: step 3938, loss 0.0125056, acc 1
2016-09-06T19:02:15.762496: step 3939, loss 0.0323249, acc 0.96
2016-09-06T19:02:16.489637: step 3940, loss 0.0793255, acc 0.96
2016-09-06T19:02:17.159733: step 3941, loss 0.0981295, acc 0.98
2016-09-06T19:02:17.873999: step 3942, loss 0.00118047, acc 1
2016-09-06T19:02:18.580849: step 3943, loss 0.0213502, acc 0.98
2016-09-06T19:02:19.279126: step 3944, loss 0.0312785, acc 0.96
2016-09-06T19:02:19.956834: step 3945, loss 0.178297, acc 0.9
2016-09-06T19:02:20.652403: step 3946, loss 0.0627899, acc 0.98
2016-09-06T19:02:21.358534: step 3947, loss 0.0246902, acc 1
2016-09-06T19:02:22.049699: step 3948, loss 0.0711642, acc 0.96
2016-09-06T19:02:22.736818: step 3949, loss 0.0210535, acc 1
2016-09-06T19:02:23.436618: step 3950, loss 0.0382751, acc 0.96
2016-09-06T19:02:24.142099: step 3951, loss 0.027337, acc 1
2016-09-06T19:02:24.814978: step 3952, loss 0.0079772, acc 1
2016-09-06T19:02:25.510542: step 3953, loss 0.029012, acc 0.98
2016-09-06T19:02:26.222580: step 3954, loss 0.0221466, acc 1
2016-09-06T19:02:26.906468: step 3955, loss 0.0226687, acc 1
2016-09-06T19:02:27.593907: step 3956, loss 0.0464679, acc 0.98
2016-09-06T19:02:28.278981: step 3957, loss 0.00347118, acc 1
2016-09-06T19:02:28.971386: step 3958, loss 0.0122842, acc 1
2016-09-06T19:02:29.661507: step 3959, loss 0.0121522, acc 1
2016-09-06T19:02:30.321352: step 3960, loss 0.0317411, acc 0.98
2016-09-06T19:02:31.015550: step 3961, loss 0.0249457, acc 1
2016-09-06T19:02:31.706537: step 3962, loss 0.015235, acc 1
2016-09-06T19:02:32.390422: step 3963, loss 0.0663302, acc 0.98
2016-09-06T19:02:33.098056: step 3964, loss 0.0407522, acc 0.96
2016-09-06T19:02:33.779435: step 3965, loss 0.0176564, acc 1
2016-09-06T19:02:34.473620: step 3966, loss 0.0275014, acc 1
2016-09-06T19:02:35.134293: step 3967, loss 0.0262163, acc 1
2016-09-06T19:02:35.837953: step 3968, loss 0.00313827, acc 1
2016-09-06T19:02:36.514450: step 3969, loss 0.0199806, acc 1
2016-09-06T19:02:37.193854: step 3970, loss 0.0447208, acc 0.98
2016-09-06T19:02:37.888300: step 3971, loss 0.0559634, acc 0.98
2016-09-06T19:02:38.581946: step 3972, loss 0.00387533, acc 1
2016-09-06T19:02:39.290213: step 3973, loss 0.0107671, acc 1
2016-09-06T19:02:39.961342: step 3974, loss 0.018904, acc 1
2016-09-06T19:02:40.668165: step 3975, loss 0.000698376, acc 1
2016-09-06T19:02:41.378475: step 3976, loss 0.00217989, acc 1
2016-09-06T19:02:42.062499: step 3977, loss 0.088811, acc 0.94
2016-09-06T19:02:42.755247: step 3978, loss 0.0177747, acc 0.98
2016-09-06T19:02:43.435176: step 3979, loss 0.17092, acc 0.98
2016-09-06T19:02:44.137342: step 3980, loss 0.0146781, acc 1
2016-09-06T19:02:44.805466: step 3981, loss 0.00703195, acc 1
2016-09-06T19:02:45.518122: step 3982, loss 0.00518896, acc 1
2016-09-06T19:02:46.203211: step 3983, loss 0.0318737, acc 0.98
2016-09-06T19:02:46.890764: step 3984, loss 0.0224908, acc 0.98
2016-09-06T19:02:47.592828: step 3985, loss 0.00433309, acc 1
2016-09-06T19:02:48.311392: step 3986, loss 0.234994, acc 0.96
2016-09-06T19:02:49.059124: step 3987, loss 0.0214683, acc 1
2016-09-06T19:02:49.727430: step 3988, loss 0.0222592, acc 0.98
2016-09-06T19:02:50.417530: step 3989, loss 0.0395139, acc 0.98
2016-09-06T19:02:51.106694: step 3990, loss 0.0437796, acc 0.98
2016-09-06T19:02:51.797457: step 3991, loss 0.0127424, acc 1
2016-09-06T19:02:52.472502: step 3992, loss 0.0206527, acc 1
2016-09-06T19:02:53.174570: step 3993, loss 0.00210209, acc 1
2016-09-06T19:02:53.884297: step 3994, loss 0.0599067, acc 0.94
2016-09-06T19:02:54.572506: step 3995, loss 0.016585, acc 1
2016-09-06T19:02:55.263991: step 3996, loss 0.0118907, acc 1
2016-09-06T19:02:55.940324: step 3997, loss 0.0116967, acc 1
2016-09-06T19:02:56.642165: step 3998, loss 0.0150828, acc 1
2016-09-06T19:02:57.344494: step 3999, loss 0.000456777, acc 1
2016-09-06T19:02:58.017108: step 4000, loss 0.0232095, acc 0.98

Evaluation:
2016-09-06T19:03:01.207170: step 4000, loss 1.64333, acc 0.762664

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-4000

2016-09-06T19:03:02.868099: step 4001, loss 0.0111618, acc 1
2016-09-06T19:03:03.559456: step 4002, loss 0.00123144, acc 1
2016-09-06T19:03:04.276435: step 4003, loss 0.0197073, acc 0.98
2016-09-06T19:03:04.952829: step 4004, loss 0.0105099, acc 1
2016-09-06T19:03:05.634754: step 4005, loss 0.0633295, acc 0.98
2016-09-06T19:03:06.312893: step 4006, loss 0.0416022, acc 1
2016-09-06T19:03:07.004976: step 4007, loss 0.0223412, acc 0.98
2016-09-06T19:03:07.680013: step 4008, loss 0.0511246, acc 0.98
2016-09-06T19:03:08.388222: step 4009, loss 0.0109284, acc 1
2016-09-06T19:03:09.068852: step 4010, loss 0.0277639, acc 0.98
2016-09-06T19:03:09.757509: step 4011, loss 0.0157996, acc 1
2016-09-06T19:03:10.454965: step 4012, loss 0.0340974, acc 1
2016-09-06T19:03:11.162261: step 4013, loss 0.01573, acc 0.98
2016-09-06T19:03:11.880306: step 4014, loss 0.0515923, acc 0.96
2016-09-06T19:03:12.556993: step 4015, loss 0.00293526, acc 1
2016-09-06T19:03:13.247347: step 4016, loss 0.0653948, acc 0.98
2016-09-06T19:03:13.913181: step 4017, loss 0.0204489, acc 0.98
2016-09-06T19:03:14.608971: step 4018, loss 0.0114392, acc 1
2016-09-06T19:03:15.306535: step 4019, loss 0.0229886, acc 1
2016-09-06T19:03:16.005362: step 4020, loss 0.0308067, acc 0.98
2016-09-06T19:03:16.697081: step 4021, loss 0.0285206, acc 0.98
2016-09-06T19:03:17.370906: step 4022, loss 0.0512592, acc 0.96
2016-09-06T19:03:18.053060: step 4023, loss 0.000671408, acc 1
2016-09-06T19:03:18.742095: step 4024, loss 0.0944723, acc 0.94
2016-09-06T19:03:19.457199: step 4025, loss 0.0409986, acc 0.98
2016-09-06T19:03:20.133660: step 4026, loss 0.00259325, acc 1
2016-09-06T19:03:20.811754: step 4027, loss 0.0937424, acc 0.98
2016-09-06T19:03:21.494560: step 4028, loss 0.00430642, acc 1
2016-09-06T19:03:22.153573: step 4029, loss 0.0636879, acc 0.96
2016-09-06T19:03:22.846858: step 4030, loss 0.013337, acc 1
2016-09-06T19:03:23.542592: step 4031, loss 0.00648831, acc 1
2016-09-06T19:03:24.174199: step 4032, loss 0.000192706, acc 1
2016-09-06T19:03:24.862621: step 4033, loss 0.00711863, acc 1
2016-09-06T19:03:25.553056: step 4034, loss 0.108919, acc 0.98
2016-09-06T19:03:26.264586: step 4035, loss 0.0426027, acc 0.96
2016-09-06T19:03:26.938641: step 4036, loss 0.0145132, acc 1
2016-09-06T19:03:27.636092: step 4037, loss 0.0266725, acc 0.98
2016-09-06T19:03:28.324831: step 4038, loss 0.000866109, acc 1
2016-09-06T19:03:29.014221: step 4039, loss 0.0117799, acc 1
2016-09-06T19:03:29.716845: step 4040, loss 0.00954244, acc 1
2016-09-06T19:03:30.398573: step 4041, loss 0.0162405, acc 1
2016-09-06T19:03:31.119076: step 4042, loss 0.0360841, acc 0.98
2016-09-06T19:03:31.797654: step 4043, loss 0.0430375, acc 0.98
2016-09-06T19:03:32.481245: step 4044, loss 0.0581293, acc 0.98
2016-09-06T19:03:33.172943: step 4045, loss 0.168701, acc 0.96
2016-09-06T19:03:33.844159: step 4046, loss 0.0848002, acc 0.96
2016-09-06T19:03:34.542496: step 4047, loss 0.0677757, acc 0.98
2016-09-06T19:03:35.238896: step 4048, loss 0.0346696, acc 0.96
2016-09-06T19:03:35.944117: step 4049, loss 0.0535451, acc 0.96
2016-09-06T19:03:36.615585: step 4050, loss 0.00662371, acc 1
2016-09-06T19:03:37.299246: step 4051, loss 0.0775217, acc 0.98
2016-09-06T19:03:37.977054: step 4052, loss 0.0207086, acc 0.98
2016-09-06T19:03:38.669204: step 4053, loss 0.0910833, acc 0.96
2016-09-06T19:03:39.366040: step 4054, loss 0.0273679, acc 0.98
2016-09-06T19:03:40.053691: step 4055, loss 0.0691424, acc 0.96
2016-09-06T19:03:40.751805: step 4056, loss 0.0187725, acc 1
2016-09-06T19:03:41.432042: step 4057, loss 0.000601684, acc 1
2016-09-06T19:03:42.109268: step 4058, loss 0.0550055, acc 0.98
2016-09-06T19:03:42.784443: step 4059, loss 0.00607572, acc 1
2016-09-06T19:03:43.457852: step 4060, loss 0.0187179, acc 0.98
2016-09-06T19:03:44.153349: step 4061, loss 0.00700317, acc 1
2016-09-06T19:03:44.850811: step 4062, loss 0.169018, acc 0.9
2016-09-06T19:03:45.555845: step 4063, loss 0.129696, acc 0.94
2016-09-06T19:03:46.240940: step 4064, loss 0.038236, acc 0.96
2016-09-06T19:03:46.946569: step 4065, loss 0.00149301, acc 1
2016-09-06T19:03:47.632347: step 4066, loss 0.0310015, acc 0.98
2016-09-06T19:03:48.330637: step 4067, loss 0.0161616, acc 1
2016-09-06T19:03:49.056750: step 4068, loss 0.06397, acc 0.96
2016-09-06T19:03:49.730520: step 4069, loss 0.00434859, acc 1
2016-09-06T19:03:50.428993: step 4070, loss 0.046338, acc 0.96
2016-09-06T19:03:51.122832: step 4071, loss 0.0136608, acc 1
2016-09-06T19:03:51.820131: step 4072, loss 0.0861495, acc 0.98
2016-09-06T19:03:52.523907: step 4073, loss 0.0422984, acc 0.98
2016-09-06T19:03:53.205404: step 4074, loss 0.0161394, acc 1
2016-09-06T19:03:53.914148: step 4075, loss 0.021382, acc 0.98
2016-09-06T19:03:54.610054: step 4076, loss 0.151378, acc 0.94
2016-09-06T19:03:55.286797: step 4077, loss 0.15643, acc 0.96
2016-09-06T19:03:55.971289: step 4078, loss 0.0905481, acc 0.94
2016-09-06T19:03:56.652264: step 4079, loss 0.0125076, acc 1
2016-09-06T19:03:57.346515: step 4080, loss 0.0164744, acc 1
2016-09-06T19:03:58.032776: step 4081, loss 0.0311692, acc 1
2016-09-06T19:03:58.726586: step 4082, loss 0.00745824, acc 1
2016-09-06T19:03:59.409544: step 4083, loss 0.0132567, acc 1
2016-09-06T19:04:00.106913: step 4084, loss 0.000288354, acc 1
2016-09-06T19:04:00.830941: step 4085, loss 0.024403, acc 0.98
2016-09-06T19:04:01.535570: step 4086, loss 0.0684421, acc 0.96
2016-09-06T19:04:02.228169: step 4087, loss 0.15201, acc 0.94
2016-09-06T19:04:02.922645: step 4088, loss 0.0454078, acc 0.98
2016-09-06T19:04:03.642760: step 4089, loss 0.023259, acc 0.98
2016-09-06T19:04:04.328085: step 4090, loss 0.0225316, acc 0.98
2016-09-06T19:04:05.012816: step 4091, loss 0.0531502, acc 0.98
2016-09-06T19:04:05.699183: step 4092, loss 0.0118991, acc 1
2016-09-06T19:04:06.381066: step 4093, loss 0.0140291, acc 1
2016-09-06T19:04:07.075720: step 4094, loss 0.0365717, acc 0.98
2016-09-06T19:04:07.760318: step 4095, loss 0.0598023, acc 0.96
2016-09-06T19:04:08.473267: step 4096, loss 0.0121167, acc 1
2016-09-06T19:04:09.149620: step 4097, loss 0.0934767, acc 0.98
2016-09-06T19:04:09.827037: step 4098, loss 0.00962382, acc 1
2016-09-06T19:04:10.508595: step 4099, loss 0.0637587, acc 0.96
2016-09-06T19:04:11.189396: step 4100, loss 0.00826009, acc 1

Evaluation:
2016-09-06T19:04:14.361971: step 4100, loss 1.59373, acc 0.749531

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-4100

2016-09-06T19:04:16.105860: step 4101, loss 0.0500896, acc 0.96
2016-09-06T19:04:16.821324: step 4102, loss 0.111219, acc 0.96
2016-09-06T19:04:17.526413: step 4103, loss 0.0204442, acc 1
2016-09-06T19:04:18.207048: step 4104, loss 0.00590662, acc 1
2016-09-06T19:04:18.904584: step 4105, loss 0.136897, acc 0.96
2016-09-06T19:04:19.600900: step 4106, loss 0.0332478, acc 1
2016-09-06T19:04:20.285565: step 4107, loss 0.0214394, acc 0.98
2016-09-06T19:04:20.992926: step 4108, loss 0.0368974, acc 0.98
2016-09-06T19:04:21.713955: step 4109, loss 0.0770093, acc 0.96
2016-09-06T19:04:22.396366: step 4110, loss 0.033, acc 0.98
2016-09-06T19:04:23.089020: step 4111, loss 0.0382214, acc 0.96
2016-09-06T19:04:23.765822: step 4112, loss 0.0311996, acc 1
2016-09-06T19:04:24.449396: step 4113, loss 0.0550327, acc 0.98
2016-09-06T19:04:25.144483: step 4114, loss 0.0280223, acc 1
2016-09-06T19:04:25.833001: step 4115, loss 0.0409073, acc 0.98
2016-09-06T19:04:26.533370: step 4116, loss 0.0544648, acc 0.96
2016-09-06T19:04:27.218811: step 4117, loss 0.0366746, acc 0.98
2016-09-06T19:04:27.898951: step 4118, loss 0.0276995, acc 0.98
2016-09-06T19:04:28.593566: step 4119, loss 0.031068, acc 0.98
2016-09-06T19:04:29.257243: step 4120, loss 0.0288057, acc 1
2016-09-06T19:04:29.945284: step 4121, loss 0.121004, acc 0.96
2016-09-06T19:04:30.611240: step 4122, loss 0.0438146, acc 0.98
2016-09-06T19:04:31.332615: step 4123, loss 0.0238056, acc 0.98
2016-09-06T19:04:32.029769: step 4124, loss 0.0246953, acc 1
2016-09-06T19:04:32.706474: step 4125, loss 0.0150795, acc 0.98
2016-09-06T19:04:33.381909: step 4126, loss 0.0225188, acc 1
2016-09-06T19:04:34.057742: step 4127, loss 0.0325038, acc 1
2016-09-06T19:04:34.758522: step 4128, loss 0.00143172, acc 1
2016-09-06T19:04:35.425582: step 4129, loss 0.02443, acc 0.98
2016-09-06T19:04:36.127624: step 4130, loss 0.000994496, acc 1
2016-09-06T19:04:36.872964: step 4131, loss 0.0239601, acc 1
2016-09-06T19:04:37.562550: step 4132, loss 0.0252248, acc 0.98
2016-09-06T19:04:38.265882: step 4133, loss 0.0297734, acc 0.98
2016-09-06T19:04:38.942120: step 4134, loss 0.0149617, acc 0.98
2016-09-06T19:04:39.627165: step 4135, loss 0.0262741, acc 0.98
2016-09-06T19:04:40.297686: step 4136, loss 0.0445122, acc 0.96
2016-09-06T19:04:41.017402: step 4137, loss 0.0604605, acc 0.96
2016-09-06T19:04:41.699569: step 4138, loss 0.0382472, acc 0.98
2016-09-06T19:04:42.401863: step 4139, loss 0.0391421, acc 0.98
2016-09-06T19:04:43.090575: step 4140, loss 0.00674969, acc 1
2016-09-06T19:04:43.781322: step 4141, loss 0.0132645, acc 1
2016-09-06T19:04:44.472888: step 4142, loss 0.0547526, acc 0.98
2016-09-06T19:04:45.148145: step 4143, loss 0.085659, acc 0.94
2016-09-06T19:04:45.829740: step 4144, loss 0.0580153, acc 0.94
2016-09-06T19:04:46.511465: step 4145, loss 0.00996003, acc 1
2016-09-06T19:04:47.189571: step 4146, loss 0.05257, acc 0.98
2016-09-06T19:04:47.880701: step 4147, loss 0.158871, acc 0.94
2016-09-06T19:04:48.565646: step 4148, loss 0.0235247, acc 0.98
2016-09-06T19:04:49.247610: step 4149, loss 0.00339145, acc 1
2016-09-06T19:04:49.920855: step 4150, loss 0.00876143, acc 1
2016-09-06T19:04:50.627529: step 4151, loss 0.0142161, acc 1
2016-09-06T19:04:51.315611: step 4152, loss 0.131508, acc 0.92
2016-09-06T19:04:52.007136: step 4153, loss 0.0353739, acc 0.98
2016-09-06T19:04:52.692787: step 4154, loss 0.0243577, acc 1
2016-09-06T19:04:53.394482: step 4155, loss 0.00482533, acc 1
2016-09-06T19:04:54.101649: step 4156, loss 0.0123259, acc 1
2016-09-06T19:04:54.789444: step 4157, loss 0.0247983, acc 0.98
2016-09-06T19:04:55.485328: step 4158, loss 0.0307217, acc 0.98
2016-09-06T19:04:56.173325: step 4159, loss 0.0595009, acc 0.98
2016-09-06T19:04:56.858294: step 4160, loss 0.022908, acc 1
2016-09-06T19:04:57.534400: step 4161, loss 0.000739089, acc 1
2016-09-06T19:04:58.228166: step 4162, loss 0.114452, acc 0.96
2016-09-06T19:04:58.926136: step 4163, loss 0.117361, acc 0.94
2016-09-06T19:04:59.606643: step 4164, loss 0.0281971, acc 1
2016-09-06T19:05:00.306188: step 4165, loss 0.0575602, acc 0.96
2016-09-06T19:05:00.990419: step 4166, loss 0.024677, acc 0.98
2016-09-06T19:05:01.670417: step 4167, loss 0.000402325, acc 1
2016-09-06T19:05:02.368688: step 4168, loss 0.0136117, acc 1
2016-09-06T19:05:03.065108: step 4169, loss 0.0112295, acc 1
2016-09-06T19:05:03.781363: step 4170, loss 0.00337888, acc 1
2016-09-06T19:05:04.455269: step 4171, loss 0.0150752, acc 1
2016-09-06T19:05:05.138110: step 4172, loss 0.065054, acc 0.94
2016-09-06T19:05:05.841595: step 4173, loss 0.151091, acc 0.94
2016-09-06T19:05:06.537358: step 4174, loss 0.0451095, acc 0.96
2016-09-06T19:05:07.235066: step 4175, loss 0.0268161, acc 0.98
2016-09-06T19:05:07.903865: step 4176, loss 0.0107771, acc 1
2016-09-06T19:05:08.605476: step 4177, loss 0.00916124, acc 1
2016-09-06T19:05:09.282303: step 4178, loss 0.0166453, acc 1
2016-09-06T19:05:09.958981: step 4179, loss 0.039541, acc 0.98
2016-09-06T19:05:10.649838: step 4180, loss 0.0532727, acc 0.98
2016-09-06T19:05:11.330218: step 4181, loss 0.0403963, acc 0.96
2016-09-06T19:05:12.030200: step 4182, loss 0.024818, acc 0.98
2016-09-06T19:05:12.698135: step 4183, loss 0.0312742, acc 0.98
2016-09-06T19:05:13.391566: step 4184, loss 0.0365147, acc 0.96
2016-09-06T19:05:14.081052: step 4185, loss 0.00331606, acc 1
2016-09-06T19:05:14.757257: step 4186, loss 0.0455726, acc 0.98
2016-09-06T19:05:15.457227: step 4187, loss 0.0230116, acc 1
2016-09-06T19:05:16.139343: step 4188, loss 0.226116, acc 0.98
2016-09-06T19:05:16.825406: step 4189, loss 0.0205047, acc 0.98
2016-09-06T19:05:17.485886: step 4190, loss 0.0144744, acc 1
2016-09-06T19:05:18.188079: step 4191, loss 0.0220267, acc 0.98
2016-09-06T19:05:18.858960: step 4192, loss 0.0971313, acc 0.94
2016-09-06T19:05:19.536975: step 4193, loss 0.00672487, acc 1
2016-09-06T19:05:20.239493: step 4194, loss 0.0408689, acc 0.98
2016-09-06T19:05:20.922099: step 4195, loss 0.0338835, acc 0.98
2016-09-06T19:05:21.644569: step 4196, loss 0.0147899, acc 0.98
2016-09-06T19:05:22.334243: step 4197, loss 0.0214415, acc 1
2016-09-06T19:05:23.042944: step 4198, loss 0.0239194, acc 1
2016-09-06T19:05:23.725565: step 4199, loss 0.0150687, acc 1
2016-09-06T19:05:24.422024: step 4200, loss 0.0329299, acc 0.98

Evaluation:
2016-09-06T19:05:27.552389: step 4200, loss 1.6199, acc 0.746717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-4200

2016-09-06T19:05:29.228317: step 4201, loss 0.0122852, acc 1
2016-09-06T19:05:29.915040: step 4202, loss 0.117268, acc 0.98
2016-09-06T19:05:30.607828: step 4203, loss 0.0724298, acc 0.96
2016-09-06T19:05:31.303907: step 4204, loss 0.0958402, acc 0.96
2016-09-06T19:05:31.953567: step 4205, loss 0.00812639, acc 1
2016-09-06T19:05:32.661077: step 4206, loss 0.0211502, acc 1
2016-09-06T19:05:33.341244: step 4207, loss 0.00437498, acc 1
2016-09-06T19:05:34.015161: step 4208, loss 0.132364, acc 0.92
2016-09-06T19:05:34.727168: step 4209, loss 0.0508816, acc 0.98
2016-09-06T19:05:35.412495: step 4210, loss 0.051879, acc 0.98
2016-09-06T19:05:36.101404: step 4211, loss 0.0157303, acc 1
2016-09-06T19:05:36.768599: step 4212, loss 0.0261872, acc 1
2016-09-06T19:05:37.477574: step 4213, loss 0.0119749, acc 1
2016-09-06T19:05:38.143160: step 4214, loss 0.071617, acc 0.98
2016-09-06T19:05:38.833583: step 4215, loss 0.0197331, acc 1
2016-09-06T19:05:39.538363: step 4216, loss 0.0633468, acc 0.96
2016-09-06T19:05:40.234841: step 4217, loss 0.017514, acc 1
2016-09-06T19:05:40.917360: step 4218, loss 0.0141304, acc 1
2016-09-06T19:05:41.576954: step 4219, loss 0.00841656, acc 1
2016-09-06T19:05:42.262292: step 4220, loss 0.0626583, acc 0.98
2016-09-06T19:05:42.933983: step 4221, loss 0.0800106, acc 0.96
2016-09-06T19:05:43.605967: step 4222, loss 0.0790181, acc 0.94
2016-09-06T19:05:44.301645: step 4223, loss 0.0231903, acc 0.98
2016-09-06T19:05:44.950150: step 4224, loss 0.163294, acc 0.977273
2016-09-06T19:05:45.631733: step 4225, loss 0.0338942, acc 0.98
2016-09-06T19:05:46.317610: step 4226, loss 0.00621427, acc 1
2016-09-06T19:05:47.036334: step 4227, loss 0.047827, acc 0.98
2016-09-06T19:05:47.712254: step 4228, loss 0.0880749, acc 0.98
2016-09-06T19:05:48.410342: step 4229, loss 0.0615009, acc 0.98
2016-09-06T19:05:49.099574: step 4230, loss 0.0224228, acc 1
2016-09-06T19:05:49.775420: step 4231, loss 0.00227579, acc 1
2016-09-06T19:05:50.462260: step 4232, loss 0.0125341, acc 1
2016-09-06T19:05:51.150756: step 4233, loss 0.0279218, acc 0.98
2016-09-06T19:05:51.860523: step 4234, loss 0.0178055, acc 1
2016-09-06T19:05:52.553361: step 4235, loss 0.033155, acc 0.98
2016-09-06T19:05:53.245678: step 4236, loss 0.0170715, acc 1
2016-09-06T19:05:53.933813: step 4237, loss 0.00993496, acc 1
2016-09-06T19:05:54.623973: step 4238, loss 0.0116024, acc 1
2016-09-06T19:05:55.321439: step 4239, loss 0.0292935, acc 0.98
2016-09-06T19:05:56.004418: step 4240, loss 0.00363726, acc 1
2016-09-06T19:05:56.701917: step 4241, loss 0.054794, acc 0.98
2016-09-06T19:05:57.377981: step 4242, loss 0.0124423, acc 1
2016-09-06T19:05:58.046386: step 4243, loss 0.0232112, acc 0.98
2016-09-06T19:05:58.740778: step 4244, loss 0.0576426, acc 1
2016-09-06T19:05:59.413072: step 4245, loss 0.0286239, acc 0.98
2016-09-06T19:06:00.099982: step 4246, loss 0.00106884, acc 1
2016-09-06T19:06:00.812618: step 4247, loss 0.0565983, acc 0.98
2016-09-06T19:06:01.513678: step 4248, loss 0.0557527, acc 0.98
2016-09-06T19:06:02.206423: step 4249, loss 0.0204844, acc 1
2016-09-06T19:06:02.888798: step 4250, loss 0.0532369, acc 0.98
2016-09-06T19:06:03.585411: step 4251, loss 0.0119837, acc 1
2016-09-06T19:06:04.278220: step 4252, loss 0.0453073, acc 0.98
2016-09-06T19:06:04.984025: step 4253, loss 0.0142256, acc 1
2016-09-06T19:06:05.673707: step 4254, loss 0.0254553, acc 1
2016-09-06T19:06:06.380105: step 4255, loss 0.00261284, acc 1
2016-09-06T19:06:07.077155: step 4256, loss 0.0149413, acc 1
2016-09-06T19:06:07.754426: step 4257, loss 0.00187916, acc 1
2016-09-06T19:06:08.475118: step 4258, loss 0.023103, acc 1
2016-09-06T19:06:09.152288: step 4259, loss 0.00174219, acc 1
2016-09-06T19:06:09.858602: step 4260, loss 0.00880928, acc 1
2016-09-06T19:06:10.542820: step 4261, loss 0.0049241, acc 1
2016-09-06T19:06:11.229820: step 4262, loss 0.0429611, acc 0.96
2016-09-06T19:06:11.929739: step 4263, loss 0.0309679, acc 1
2016-09-06T19:06:12.605684: step 4264, loss 0.0446959, acc 0.96
2016-09-06T19:06:13.303220: step 4265, loss 0.0296487, acc 0.98
2016-09-06T19:06:13.994836: step 4266, loss 0.0271326, acc 0.98
2016-09-06T19:06:14.697318: step 4267, loss 0.0354914, acc 0.98
2016-09-06T19:06:15.370713: step 4268, loss 0.0207684, acc 1
2016-09-06T19:06:16.052508: step 4269, loss 0.0337508, acc 0.98
2016-09-06T19:06:16.737285: step 4270, loss 0.052731, acc 0.96
2016-09-06T19:06:17.409304: step 4271, loss 0.0695343, acc 0.98
2016-09-06T19:06:18.092797: step 4272, loss 0.0107234, acc 1
2016-09-06T19:06:18.789668: step 4273, loss 0.0140984, acc 1
2016-09-06T19:06:19.488092: step 4274, loss 0.128071, acc 0.98
2016-09-06T19:06:20.163317: step 4275, loss 0.0626185, acc 0.98
2016-09-06T19:06:20.843051: step 4276, loss 0.00668921, acc 1
2016-09-06T19:06:21.518500: step 4277, loss 0.0234673, acc 0.98
2016-09-06T19:06:22.209603: step 4278, loss 0.00599779, acc 1
2016-09-06T19:06:22.913233: step 4279, loss 0.0561445, acc 0.96
2016-09-06T19:06:23.589565: step 4280, loss 0.0105712, acc 1
2016-09-06T19:06:24.293358: step 4281, loss 0.0253666, acc 1
2016-09-06T19:06:24.957406: step 4282, loss 0.0354449, acc 1
2016-09-06T19:06:25.620344: step 4283, loss 0.0463144, acc 0.98
2016-09-06T19:06:26.313492: step 4284, loss 0.00631273, acc 1
2016-09-06T19:06:26.985942: step 4285, loss 0.0345175, acc 0.98
2016-09-06T19:06:27.654086: step 4286, loss 0.0802942, acc 0.96
2016-09-06T19:06:28.335758: step 4287, loss 0.114271, acc 0.96
2016-09-06T19:06:29.042587: step 4288, loss 0.0273304, acc 0.98
2016-09-06T19:06:29.706949: step 4289, loss 0.00377257, acc 1
2016-09-06T19:06:30.396486: step 4290, loss 0.00823308, acc 1
2016-09-06T19:06:31.082908: step 4291, loss 0.0359959, acc 1
2016-09-06T19:06:31.763329: step 4292, loss 0.123654, acc 0.98
2016-09-06T19:06:32.452238: step 4293, loss 0.0429306, acc 0.96
2016-09-06T19:06:33.153471: step 4294, loss 0.0016031, acc 1
2016-09-06T19:06:33.849002: step 4295, loss 0.191762, acc 0.9
2016-09-06T19:06:34.506731: step 4296, loss 0.00478264, acc 1
2016-09-06T19:06:35.202036: step 4297, loss 0.033784, acc 0.98
2016-09-06T19:06:35.895974: step 4298, loss 0.0113191, acc 1
2016-09-06T19:06:36.580694: step 4299, loss 0.0244496, acc 0.98
2016-09-06T19:06:37.259895: step 4300, loss 0.027278, acc 0.98

Evaluation:
2016-09-06T19:06:40.415674: step 4300, loss 1.53148, acc 0.750469

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-4300

2016-09-06T19:06:42.178182: step 4301, loss 0.126478, acc 0.94
2016-09-06T19:06:42.849210: step 4302, loss 0.0341155, acc 0.98
2016-09-06T19:06:43.546706: step 4303, loss 0.00971727, acc 1
2016-09-06T19:06:44.221613: step 4304, loss 0.0645724, acc 0.94
2016-09-06T19:06:44.902770: step 4305, loss 0.0433461, acc 0.96
2016-09-06T19:06:45.586454: step 4306, loss 0.0217038, acc 1
2016-09-06T19:06:46.288232: step 4307, loss 0.0204624, acc 1
2016-09-06T19:06:46.987250: step 4308, loss 0.0408286, acc 0.98
2016-09-06T19:06:47.675966: step 4309, loss 0.0347249, acc 0.98
2016-09-06T19:06:48.379830: step 4310, loss 0.0156382, acc 1
2016-09-06T19:06:49.059532: step 4311, loss 0.0367409, acc 1
2016-09-06T19:06:49.742135: step 4312, loss 0.0122795, acc 1
2016-09-06T19:06:50.431218: step 4313, loss 0.113468, acc 0.94
2016-09-06T19:06:51.120922: step 4314, loss 0.0108952, acc 1
2016-09-06T19:06:51.812404: step 4315, loss 0.00879498, acc 1
2016-09-06T19:06:52.490815: step 4316, loss 0.044222, acc 0.98
2016-09-06T19:06:53.208496: step 4317, loss 0.0109056, acc 1
2016-09-06T19:06:53.889308: step 4318, loss 0.0338127, acc 0.98
2016-09-06T19:06:54.571545: step 4319, loss 0.0156063, acc 0.98
2016-09-06T19:06:55.281697: step 4320, loss 0.001394, acc 1
2016-09-06T19:06:56.006619: step 4321, loss 0.0155286, acc 1
2016-09-06T19:06:56.699981: step 4322, loss 0.0158347, acc 1
2016-09-06T19:06:57.364504: step 4323, loss 0.00198417, acc 1
2016-09-06T19:06:58.079389: step 4324, loss 0.000775439, acc 1
2016-09-06T19:06:58.747243: step 4325, loss 0.0143506, acc 1
2016-09-06T19:06:59.430952: step 4326, loss 7.8914e-05, acc 1
2016-09-06T19:07:00.137090: step 4327, loss 0.00599976, acc 1
2016-09-06T19:07:00.860256: step 4328, loss 0.0241556, acc 0.98
2016-09-06T19:07:01.588166: step 4329, loss 0.0367172, acc 0.98
2016-09-06T19:07:02.251133: step 4330, loss 0.0427693, acc 0.98
2016-09-06T19:07:02.951755: step 4331, loss 0.0762434, acc 0.96
2016-09-06T19:07:03.648161: step 4332, loss 0.0545701, acc 0.96
2016-09-06T19:07:04.335874: step 4333, loss 0.114409, acc 0.98
2016-09-06T19:07:05.018736: step 4334, loss 0.0368258, acc 0.98
2016-09-06T19:07:05.703177: step 4335, loss 0.0886751, acc 0.98
2016-09-06T19:07:06.408605: step 4336, loss 0.0491499, acc 0.96
2016-09-06T19:07:07.094673: step 4337, loss 0.00454705, acc 1
2016-09-06T19:07:07.784035: step 4338, loss 0.0459239, acc 0.96
2016-09-06T19:07:08.479488: step 4339, loss 0.0177903, acc 1
2016-09-06T19:07:09.172892: step 4340, loss 0.00520656, acc 1
2016-09-06T19:07:09.871123: step 4341, loss 0.0186925, acc 1
2016-09-06T19:07:10.554711: step 4342, loss 0.00556154, acc 1
2016-09-06T19:07:11.235096: step 4343, loss 0.0394713, acc 0.98
2016-09-06T19:07:11.897639: step 4344, loss 0.00442627, acc 1
2016-09-06T19:07:12.591892: step 4345, loss 0.0429778, acc 0.98
2016-09-06T19:07:13.260075: step 4346, loss 0.0211231, acc 1
2016-09-06T19:07:13.952193: step 4347, loss 0.0381533, acc 0.98
2016-09-06T19:07:14.638258: step 4348, loss 0.0253297, acc 1
2016-09-06T19:07:15.349864: step 4349, loss 0.0194039, acc 0.98
2016-09-06T19:07:16.069553: step 4350, loss 0.0396934, acc 1
2016-09-06T19:07:16.736401: step 4351, loss 0.00358695, acc 1
2016-09-06T19:07:17.446493: step 4352, loss 0.00449148, acc 1
2016-09-06T19:07:18.140760: step 4353, loss 0.00213871, acc 1
2016-09-06T19:07:18.847661: step 4354, loss 0.0387843, acc 1
2016-09-06T19:07:19.535489: step 4355, loss 0.122436, acc 0.98
2016-09-06T19:07:20.201367: step 4356, loss 0.0233779, acc 1
2016-09-06T19:07:20.919278: step 4357, loss 0.0409098, acc 1
2016-09-06T19:07:21.623518: step 4358, loss 0.0153295, acc 1
2016-09-06T19:07:22.306694: step 4359, loss 0.0186457, acc 0.98
2016-09-06T19:07:22.995612: step 4360, loss 0.0460608, acc 0.96
2016-09-06T19:07:23.680154: step 4361, loss 0.0644452, acc 0.98
2016-09-06T19:07:24.354884: step 4362, loss 0.00603201, acc 1
2016-09-06T19:07:25.015669: step 4363, loss 0.0139365, acc 1
2016-09-06T19:07:25.715586: step 4364, loss 0.00292058, acc 1
2016-09-06T19:07:26.375440: step 4365, loss 0.0191437, acc 1
2016-09-06T19:07:27.045297: step 4366, loss 0.00522989, acc 1
2016-09-06T19:07:27.730023: step 4367, loss 0.015, acc 0.98
2016-09-06T19:07:28.416261: step 4368, loss 0.0734885, acc 0.96
2016-09-06T19:07:29.102241: step 4369, loss 0.0665323, acc 0.94
2016-09-06T19:07:29.776653: step 4370, loss 0.0136284, acc 1
2016-09-06T19:07:30.499451: step 4371, loss 0.00809609, acc 1
2016-09-06T19:07:31.171496: step 4372, loss 0.02505, acc 0.98
2016-09-06T19:07:31.864615: step 4373, loss 0.0211981, acc 1
2016-09-06T19:07:32.552515: step 4374, loss 0.0160451, acc 0.98
2016-09-06T19:07:33.219052: step 4375, loss 0.0525081, acc 0.96
2016-09-06T19:07:33.906349: step 4376, loss 0.00177055, acc 1
2016-09-06T19:07:34.578939: step 4377, loss 0.0204455, acc 0.98
2016-09-06T19:07:35.285242: step 4378, loss 0.012102, acc 1
2016-09-06T19:07:35.975731: step 4379, loss 0.0625654, acc 0.96
2016-09-06T19:07:36.669338: step 4380, loss 0.000854682, acc 1
2016-09-06T19:07:37.362967: step 4381, loss 0.0111212, acc 1
2016-09-06T19:07:38.073559: step 4382, loss 0.0591353, acc 0.98
2016-09-06T19:07:38.756960: step 4383, loss 0.0156723, acc 1
2016-09-06T19:07:39.420853: step 4384, loss 0.0182305, acc 1
2016-09-06T19:07:40.132851: step 4385, loss 0.000187493, acc 1
2016-09-06T19:07:40.827754: step 4386, loss 0.00502457, acc 1
2016-09-06T19:07:41.529035: step 4387, loss 0.0277228, acc 0.98
2016-09-06T19:07:42.237009: step 4388, loss 0.0403768, acc 0.98
2016-09-06T19:07:42.920246: step 4389, loss 0.0370163, acc 0.98
2016-09-06T19:07:43.602917: step 4390, loss 0.0121601, acc 1
2016-09-06T19:07:44.255272: step 4391, loss 0.0321052, acc 0.98
2016-09-06T19:07:44.975318: step 4392, loss 0.0583324, acc 0.98
2016-09-06T19:07:45.672274: step 4393, loss 0.055238, acc 0.94
2016-09-06T19:07:46.365319: step 4394, loss 0.0193626, acc 1
2016-09-06T19:07:47.067624: step 4395, loss 0.00261527, acc 1
2016-09-06T19:07:47.774440: step 4396, loss 0.00749917, acc 1
2016-09-06T19:07:48.491075: step 4397, loss 0.00921612, acc 1
2016-09-06T19:07:49.180158: step 4398, loss 0.0316863, acc 1
2016-09-06T19:07:49.881374: step 4399, loss 0.00493106, acc 1
2016-09-06T19:07:50.567860: step 4400, loss 0.0212731, acc 1

Evaluation:
2016-09-06T19:07:53.696878: step 4400, loss 1.77147, acc 0.748593

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-4400

2016-09-06T19:07:55.334839: step 4401, loss 0.00464693, acc 1
2016-09-06T19:07:56.031808: step 4402, loss 0.0267481, acc 1
2016-09-06T19:07:56.717122: step 4403, loss 0.000618601, acc 1
2016-09-06T19:07:57.389816: step 4404, loss 0.0120791, acc 1
2016-09-06T19:07:58.096858: step 4405, loss 0.00648013, acc 1
2016-09-06T19:07:58.757847: step 4406, loss 0.0292645, acc 0.98
2016-09-06T19:07:59.455630: step 4407, loss 0.000391976, acc 1
2016-09-06T19:08:00.148807: step 4408, loss 0.125513, acc 0.98
2016-09-06T19:08:00.854151: step 4409, loss 0.125221, acc 0.94
2016-09-06T19:08:01.563616: step 4410, loss 0.0143373, acc 1
2016-09-06T19:08:02.251351: step 4411, loss 0.0414566, acc 0.96
2016-09-06T19:08:02.950337: step 4412, loss 0.198372, acc 0.98
2016-09-06T19:08:03.637090: step 4413, loss 0.0363268, acc 0.96
2016-09-06T19:08:04.330762: step 4414, loss 0.0223158, acc 0.98
2016-09-06T19:08:05.033653: step 4415, loss 0.0908531, acc 0.98
2016-09-06T19:08:05.674704: step 4416, loss 0.00159763, acc 1
2016-09-06T19:08:06.360367: step 4417, loss 0.0225161, acc 0.98
2016-09-06T19:08:07.043424: step 4418, loss 0.00100049, acc 1
2016-09-06T19:08:07.746708: step 4419, loss 0.00339299, acc 1
2016-09-06T19:08:08.418468: step 4420, loss 0.0177311, acc 1
2016-09-06T19:08:09.113876: step 4421, loss 0.00642809, acc 1
2016-09-06T19:08:09.801550: step 4422, loss 0.0275324, acc 0.98
2016-09-06T19:08:10.487998: step 4423, loss 0.00225551, acc 1
2016-09-06T19:08:11.175922: step 4424, loss 0.00827521, acc 1
2016-09-06T19:08:11.852523: step 4425, loss 0.0282291, acc 1
2016-09-06T19:08:12.570992: step 4426, loss 0.0431881, acc 0.98
2016-09-06T19:08:13.241995: step 4427, loss 0.241417, acc 0.94
2016-09-06T19:08:13.946856: step 4428, loss 0.0293428, acc 0.98
2016-09-06T19:08:14.640403: step 4429, loss 0.0116226, acc 1
2016-09-06T19:08:15.346616: step 4430, loss 0.0155593, acc 1
2016-09-06T19:08:16.026455: step 4431, loss 0.0251922, acc 0.98
2016-09-06T19:08:16.695993: step 4432, loss 0.000174285, acc 1
2016-09-06T19:08:17.392447: step 4433, loss 0.00543451, acc 1
2016-09-06T19:08:18.076842: step 4434, loss 0.00718076, acc 1
2016-09-06T19:08:18.780922: step 4435, loss 0.00782978, acc 1
2016-09-06T19:08:19.476593: step 4436, loss 0.0501047, acc 0.96
2016-09-06T19:08:20.151420: step 4437, loss 0.0293605, acc 0.98
2016-09-06T19:08:20.834785: step 4438, loss 0.0172106, acc 0.98
2016-09-06T19:08:21.494092: step 4439, loss 0.0246813, acc 0.98
2016-09-06T19:08:22.190941: step 4440, loss 0.0265442, acc 0.98
2016-09-06T19:08:22.866194: step 4441, loss 0.024387, acc 0.98
2016-09-06T19:08:23.539403: step 4442, loss 0.0558071, acc 0.96
2016-09-06T19:08:24.226254: step 4443, loss 0.142549, acc 0.98
2016-09-06T19:08:24.923308: step 4444, loss 0.0354, acc 0.98
2016-09-06T19:08:25.611466: step 4445, loss 0.0281698, acc 0.98
2016-09-06T19:08:26.286068: step 4446, loss 0.0163969, acc 0.98
2016-09-06T19:08:26.988521: step 4447, loss 0.0167615, acc 0.98
2016-09-06T19:08:27.663572: step 4448, loss 0.0479916, acc 0.94
2016-09-06T19:08:28.383243: step 4449, loss 0.0183732, acc 0.98
2016-09-06T19:08:29.108837: step 4450, loss 0.154463, acc 0.98
2016-09-06T19:08:29.795776: step 4451, loss 0.021412, acc 1
2016-09-06T19:08:30.502447: step 4452, loss 0.0412075, acc 0.98
2016-09-06T19:08:31.167828: step 4453, loss 0.000562671, acc 1
2016-09-06T19:08:31.882799: step 4454, loss 0.0246118, acc 1
2016-09-06T19:08:32.567655: step 4455, loss 0.0171512, acc 1
2016-09-06T19:08:33.256818: step 4456, loss 0.0100923, acc 1
2016-09-06T19:08:33.941321: step 4457, loss 0.0159391, acc 1
2016-09-06T19:08:34.632594: step 4458, loss 0.0250673, acc 1
2016-09-06T19:08:35.326428: step 4459, loss 0.0289469, acc 1
2016-09-06T19:08:35.986533: step 4460, loss 0.00576953, acc 1
2016-09-06T19:08:36.684745: step 4461, loss 0.0483605, acc 0.98
2016-09-06T19:08:37.341035: step 4462, loss 0.0462767, acc 0.98
2016-09-06T19:08:38.022719: step 4463, loss 0.00456224, acc 1
2016-09-06T19:08:38.716544: step 4464, loss 0.0170634, acc 1
2016-09-06T19:08:39.412297: step 4465, loss 0.0284728, acc 0.98
2016-09-06T19:08:40.094121: step 4466, loss 0.0391077, acc 0.98
2016-09-06T19:08:40.745396: step 4467, loss 0.0157246, acc 1
2016-09-06T19:08:41.427182: step 4468, loss 0.00533545, acc 1
2016-09-06T19:08:42.107855: step 4469, loss 0.0170015, acc 1
2016-09-06T19:08:42.805682: step 4470, loss 0.0152925, acc 0.98
2016-09-06T19:08:43.495436: step 4471, loss 0.0544851, acc 0.98
2016-09-06T19:08:44.202285: step 4472, loss 0.0259483, acc 0.98
2016-09-06T19:08:44.908098: step 4473, loss 0.00131311, acc 1
2016-09-06T19:08:45.585684: step 4474, loss 0.0873026, acc 0.96
2016-09-06T19:08:46.285491: step 4475, loss 0.00412068, acc 1
2016-09-06T19:08:46.985504: step 4476, loss 0.0169566, acc 1
2016-09-06T19:08:47.678798: step 4477, loss 0.0279911, acc 0.98
2016-09-06T19:08:48.358298: step 4478, loss 0.0102926, acc 1
2016-09-06T19:08:49.068306: step 4479, loss 0.0219899, acc 0.98
2016-09-06T19:08:49.755585: step 4480, loss 0.0361413, acc 0.98
2016-09-06T19:08:50.438482: step 4481, loss 0.10209, acc 0.96
2016-09-06T19:08:51.153195: step 4482, loss 0.0017604, acc 1
2016-09-06T19:08:51.833913: step 4483, loss 0.00386058, acc 1
2016-09-06T19:08:52.529397: step 4484, loss 0.13379, acc 0.96
2016-09-06T19:08:53.212504: step 4485, loss 0.0513629, acc 0.96
2016-09-06T19:08:53.918424: step 4486, loss 0.00626872, acc 1
2016-09-06T19:08:54.640401: step 4487, loss 0.0690294, acc 0.96
2016-09-06T19:08:55.315751: step 4488, loss 0.0127056, acc 1
2016-09-06T19:08:55.990743: step 4489, loss 0.0359356, acc 0.98
2016-09-06T19:08:56.677027: step 4490, loss 0.0587632, acc 0.96
2016-09-06T19:08:57.349015: step 4491, loss 0.00414916, acc 1
2016-09-06T19:08:58.055276: step 4492, loss 0.123291, acc 0.96
2016-09-06T19:08:58.759480: step 4493, loss 0.0941264, acc 0.98
2016-09-06T19:08:59.492939: step 4494, loss 0.0314658, acc 1
2016-09-06T19:09:00.160676: step 4495, loss 0.0264731, acc 0.98
2016-09-06T19:09:00.880247: step 4496, loss 0.0109575, acc 1
2016-09-06T19:09:01.585138: step 4497, loss 0.0663053, acc 0.96
2016-09-06T19:09:02.255069: step 4498, loss 0.0359342, acc 0.96
2016-09-06T19:09:02.933988: step 4499, loss 0.0446682, acc 0.98
2016-09-06T19:09:03.604557: step 4500, loss 0.045894, acc 0.96

Evaluation:
2016-09-06T19:09:06.777698: step 4500, loss 1.4477, acc 0.738274

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-4500

2016-09-06T19:09:08.431094: step 4501, loss 0.0737725, acc 0.96
2016-09-06T19:09:09.135468: step 4502, loss 0.00587516, acc 1
2016-09-06T19:09:09.815553: step 4503, loss 0.0335713, acc 1
2016-09-06T19:09:10.500286: step 4504, loss 0.037699, acc 0.96
2016-09-06T19:09:11.196757: step 4505, loss 0.060316, acc 0.96
2016-09-06T19:09:11.881433: step 4506, loss 0.0123941, acc 1
2016-09-06T19:09:12.600969: step 4507, loss 0.0536717, acc 0.98
2016-09-06T19:09:13.268145: step 4508, loss 0.0351279, acc 0.98
2016-09-06T19:09:13.979833: step 4509, loss 0.0759181, acc 0.96
2016-09-06T19:09:14.671862: step 4510, loss 0.0662408, acc 0.94
2016-09-06T19:09:15.374779: step 4511, loss 0.112874, acc 0.96
2016-09-06T19:09:16.097996: step 4512, loss 0.0329542, acc 0.98
2016-09-06T19:09:16.786042: step 4513, loss 0.0250021, acc 1
2016-09-06T19:09:17.478405: step 4514, loss 0.00645969, acc 1
2016-09-06T19:09:18.146430: step 4515, loss 0.0338833, acc 0.98
2016-09-06T19:09:18.833251: step 4516, loss 0.0189892, acc 0.98
2016-09-06T19:09:19.522242: step 4517, loss 0.0148502, acc 1
2016-09-06T19:09:20.226947: step 4518, loss 0.00625099, acc 1
2016-09-06T19:09:20.924672: step 4519, loss 0.0193701, acc 1
2016-09-06T19:09:21.608952: step 4520, loss 0.0518476, acc 0.98
2016-09-06T19:09:22.302459: step 4521, loss 0.1163, acc 0.98
2016-09-06T19:09:22.980250: step 4522, loss 0.0149321, acc 1
2016-09-06T19:09:23.685633: step 4523, loss 0.00169849, acc 1
2016-09-06T19:09:24.371672: step 4524, loss 0.0337468, acc 0.98
2016-09-06T19:09:25.053669: step 4525, loss 0.0837407, acc 0.98
2016-09-06T19:09:25.717583: step 4526, loss 0.0150202, acc 1
2016-09-06T19:09:26.403575: step 4527, loss 0.0212656, acc 0.98
2016-09-06T19:09:27.114697: step 4528, loss 0.0202725, acc 1
2016-09-06T19:09:27.776457: step 4529, loss 0.00139758, acc 1
2016-09-06T19:09:28.480325: step 4530, loss 0.017678, acc 1
2016-09-06T19:09:29.182431: step 4531, loss 0.00111532, acc 1
2016-09-06T19:09:29.870419: step 4532, loss 0.045553, acc 0.98
2016-09-06T19:09:30.571178: step 4533, loss 0.00310395, acc 1
2016-09-06T19:09:31.238206: step 4534, loss 0.024305, acc 1
2016-09-06T19:09:31.968798: step 4535, loss 0.0137661, acc 1
2016-09-06T19:09:32.665582: step 4536, loss 0.0067352, acc 1
2016-09-06T19:09:33.370144: step 4537, loss 0.0352, acc 0.98
2016-09-06T19:09:34.044565: step 4538, loss 0.00413211, acc 1
2016-09-06T19:09:34.734421: step 4539, loss 0.0122053, acc 1
2016-09-06T19:09:35.420526: step 4540, loss 0.0191395, acc 1
2016-09-06T19:09:36.096970: step 4541, loss 0.000501679, acc 1
2016-09-06T19:09:36.793509: step 4542, loss 0.049909, acc 0.98
2016-09-06T19:09:37.463870: step 4543, loss 0.00762065, acc 1
2016-09-06T19:09:38.151448: step 4544, loss 0.0273613, acc 0.98
2016-09-06T19:09:38.849489: step 4545, loss 0.129486, acc 0.94
2016-09-06T19:09:39.517436: step 4546, loss 0.00750422, acc 1
2016-09-06T19:09:40.218898: step 4547, loss 0.0524103, acc 0.96
2016-09-06T19:09:40.899462: step 4548, loss 0.00278357, acc 1
2016-09-06T19:09:41.610341: step 4549, loss 0.0215864, acc 1
2016-09-06T19:09:42.307079: step 4550, loss 0.134342, acc 0.96
2016-09-06T19:09:43.003612: step 4551, loss 0.000175566, acc 1
2016-09-06T19:09:43.707181: step 4552, loss 0.0298254, acc 1
2016-09-06T19:09:44.395017: step 4553, loss 0.00914357, acc 1
2016-09-06T19:09:45.098800: step 4554, loss 0.0375583, acc 0.96
2016-09-06T19:09:45.746764: step 4555, loss 0.000495059, acc 1
2016-09-06T19:09:46.449830: step 4556, loss 0.0147268, acc 1
2016-09-06T19:09:47.141912: step 4557, loss 0.0216559, acc 1
2016-09-06T19:09:47.837311: step 4558, loss 0.00604706, acc 1
2016-09-06T19:09:48.509885: step 4559, loss 0.0252334, acc 1
2016-09-06T19:09:49.198489: step 4560, loss 0.000733978, acc 1
2016-09-06T19:09:49.895036: step 4561, loss 0.0279982, acc 1
2016-09-06T19:09:50.581242: step 4562, loss 0.00926257, acc 1
2016-09-06T19:09:51.292359: step 4563, loss 0.0200385, acc 0.98
2016-09-06T19:09:51.971858: step 4564, loss 0.0473451, acc 0.98
2016-09-06T19:09:52.660424: step 4565, loss 0.0380699, acc 0.98
2016-09-06T19:09:53.336427: step 4566, loss 0.022849, acc 0.98
2016-09-06T19:09:54.041327: step 4567, loss 0.00137415, acc 1
2016-09-06T19:09:54.789715: step 4568, loss 0.080175, acc 0.96
2016-09-06T19:09:55.458260: step 4569, loss 0.018734, acc 0.98
2016-09-06T19:09:56.148203: step 4570, loss 0.0326379, acc 0.98
2016-09-06T19:09:56.840490: step 4571, loss 0.0117946, acc 1
2016-09-06T19:09:57.528004: step 4572, loss 0.00310474, acc 1
2016-09-06T19:09:58.217326: step 4573, loss 0.0198504, acc 0.98
2016-09-06T19:09:58.887924: step 4574, loss 0.00705376, acc 1
2016-09-06T19:09:59.599180: step 4575, loss 0.0186061, acc 1
2016-09-06T19:10:00.275975: step 4576, loss 0.0194559, acc 1
2016-09-06T19:10:00.972027: step 4577, loss 0.000160693, acc 1
2016-09-06T19:10:01.666092: step 4578, loss 0.00155661, acc 1
2016-09-06T19:10:02.356328: step 4579, loss 0.0259871, acc 0.98
2016-09-06T19:10:03.032543: step 4580, loss 0.00112947, acc 1
2016-09-06T19:10:03.727054: step 4581, loss 0.0171352, acc 0.98
2016-09-06T19:10:04.438514: step 4582, loss 0.0212028, acc 1
2016-09-06T19:10:05.096602: step 4583, loss 0.0167673, acc 0.98
2016-09-06T19:10:05.784100: step 4584, loss 0.0967344, acc 0.96
2016-09-06T19:10:06.468187: step 4585, loss 0.0174842, acc 0.98
2016-09-06T19:10:07.175209: step 4586, loss 0.0161034, acc 0.98
2016-09-06T19:10:07.862210: step 4587, loss 0.00537592, acc 1
2016-09-06T19:10:08.536346: step 4588, loss 0.00011028, acc 1
2016-09-06T19:10:09.289403: step 4589, loss 0.00629818, acc 1
2016-09-06T19:10:09.986722: step 4590, loss 0.0434248, acc 0.96
2016-09-06T19:10:10.666227: step 4591, loss 0.00838352, acc 1
2016-09-06T19:10:11.350279: step 4592, loss 0.0537002, acc 0.96
2016-09-06T19:10:12.050608: step 4593, loss 0.0593443, acc 0.98
2016-09-06T19:10:12.753628: step 4594, loss 0.036792, acc 0.98
2016-09-06T19:10:13.430786: step 4595, loss 0.011497, acc 1
2016-09-06T19:10:14.179059: step 4596, loss 0.0262411, acc 0.98
2016-09-06T19:10:14.879062: step 4597, loss 0.013072, acc 1
2016-09-06T19:10:15.588785: step 4598, loss 0.00209013, acc 1
2016-09-06T19:10:16.280430: step 4599, loss 0.00019258, acc 1
2016-09-06T19:10:16.959481: step 4600, loss 0.0368597, acc 0.98

Evaluation:
2016-09-06T19:10:20.087926: step 4600, loss 2.18973, acc 0.734522

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-4600

2016-09-06T19:10:21.762982: step 4601, loss 0.132983, acc 0.94
2016-09-06T19:10:22.450557: step 4602, loss 0.144423, acc 0.96
2016-09-06T19:10:23.107353: step 4603, loss 0.0341332, acc 0.98
2016-09-06T19:10:23.780277: step 4604, loss 0.00062322, acc 1
2016-09-06T19:10:24.460323: step 4605, loss 0.012719, acc 1
2016-09-06T19:10:25.149879: step 4606, loss 0.0229136, acc 0.98
2016-09-06T19:10:25.847700: step 4607, loss 0.0133413, acc 1
2016-09-06T19:10:26.480243: step 4608, loss 0.00219159, acc 1
2016-09-06T19:10:27.220679: step 4609, loss 0.0219747, acc 1
2016-09-06T19:10:27.896377: step 4610, loss 0.00747092, acc 1
2016-09-06T19:10:28.604827: step 4611, loss 0.00745866, acc 1
2016-09-06T19:10:29.285796: step 4612, loss 0.0151871, acc 1
2016-09-06T19:10:29.995226: step 4613, loss 0.0274017, acc 0.98
2016-09-06T19:10:30.681822: step 4614, loss 0.0175691, acc 0.98
2016-09-06T19:10:31.365033: step 4615, loss 0.000646482, acc 1
2016-09-06T19:10:32.082511: step 4616, loss 0.0232479, acc 0.98
2016-09-06T19:10:32.789465: step 4617, loss 0.0236963, acc 1
2016-09-06T19:10:33.456724: step 4618, loss 0.0228707, acc 1
2016-09-06T19:10:34.150707: step 4619, loss 0.0694583, acc 0.96
2016-09-06T19:10:34.842319: step 4620, loss 0.0189444, acc 0.98
2016-09-06T19:10:35.539611: step 4621, loss 0.0207992, acc 1
2016-09-06T19:10:36.231748: step 4622, loss 0.0056306, acc 1
2016-09-06T19:10:36.936646: step 4623, loss 0.184644, acc 0.98
2016-09-06T19:10:37.613978: step 4624, loss 0.00189785, acc 1
2016-09-06T19:10:38.296167: step 4625, loss 0.0122529, acc 1
2016-09-06T19:10:38.983072: step 4626, loss 0.00675347, acc 1
2016-09-06T19:10:39.674957: step 4627, loss 0.146843, acc 0.92
2016-09-06T19:10:40.369904: step 4628, loss 0.016284, acc 1
2016-09-06T19:10:41.038126: step 4629, loss 0.0229362, acc 1
2016-09-06T19:10:41.746853: step 4630, loss 0.0594311, acc 0.98
2016-09-06T19:10:42.426813: step 4631, loss 0.0142096, acc 0.98
2016-09-06T19:10:43.111649: step 4632, loss 0.064175, acc 0.96
2016-09-06T19:10:43.795055: step 4633, loss 0.00283168, acc 1
2016-09-06T19:10:44.487205: step 4634, loss 0.00692844, acc 1
2016-09-06T19:10:45.179347: step 4635, loss 0.00440362, acc 1
2016-09-06T19:10:45.845447: step 4636, loss 0.00654819, acc 1
2016-09-06T19:10:46.570670: step 4637, loss 0.0244418, acc 1
2016-09-06T19:10:47.251497: step 4638, loss 0.0109842, acc 1
2016-09-06T19:10:47.927529: step 4639, loss 0.00472934, acc 1
2016-09-06T19:10:48.638301: step 4640, loss 0.00587267, acc 1
2016-09-06T19:10:49.334326: step 4641, loss 0.0456414, acc 0.98
2016-09-06T19:10:50.038191: step 4642, loss 0.0503591, acc 0.96
2016-09-06T19:10:50.700152: step 4643, loss 0.0175902, acc 1
2016-09-06T19:10:51.386092: step 4644, loss 0.0188492, acc 1
2016-09-06T19:10:52.061737: step 4645, loss 0.0013854, acc 1
2016-09-06T19:10:52.750547: step 4646, loss 0.00186048, acc 1
2016-09-06T19:10:53.442273: step 4647, loss 0.0303229, acc 0.98
2016-09-06T19:10:54.147810: step 4648, loss 0.042551, acc 0.98
2016-09-06T19:10:54.865163: step 4649, loss 0.073323, acc 0.96
2016-09-06T19:10:55.537579: step 4650, loss 0.00381327, acc 1
2016-09-06T19:10:56.236233: step 4651, loss 0.0193553, acc 1
2016-09-06T19:10:56.933082: step 4652, loss 0.0670914, acc 0.96
2016-09-06T19:10:57.620298: step 4653, loss 0.018887, acc 0.98
2016-09-06T19:10:58.297486: step 4654, loss 0.00142053, acc 1
2016-09-06T19:10:58.990529: step 4655, loss 0.00382085, acc 1
2016-09-06T19:10:59.684637: step 4656, loss 0.0548013, acc 0.96
2016-09-06T19:11:00.393411: step 4657, loss 0.0448571, acc 0.96
2016-09-06T19:11:01.100370: step 4658, loss 0.0185649, acc 1
2016-09-06T19:11:01.800882: step 4659, loss 0.0106155, acc 1
2016-09-06T19:11:02.488981: step 4660, loss 0.000420789, acc 1
2016-09-06T19:11:03.163347: step 4661, loss 0.00428584, acc 1
2016-09-06T19:11:03.847202: step 4662, loss 0.0402142, acc 0.98
2016-09-06T19:11:04.593309: step 4663, loss 0.0943648, acc 0.94
2016-09-06T19:11:05.304624: step 4664, loss 0.0791464, acc 0.98
2016-09-06T19:11:05.992079: step 4665, loss 0.0187277, acc 0.98
2016-09-06T19:11:06.668518: step 4666, loss 0.00837923, acc 1
2016-09-06T19:11:07.354445: step 4667, loss 0.0730136, acc 0.96
2016-09-06T19:11:08.053039: step 4668, loss 0.0260025, acc 1
2016-09-06T19:11:08.728854: step 4669, loss 0.0221644, acc 0.98
2016-09-06T19:11:09.412818: step 4670, loss 0.0454865, acc 0.96
2016-09-06T19:11:10.107496: step 4671, loss 0.00172208, acc 1
2016-09-06T19:11:10.781163: step 4672, loss 0.00435936, acc 1
2016-09-06T19:11:11.471327: step 4673, loss 0.0129927, acc 1
2016-09-06T19:11:12.161111: step 4674, loss 0.0051072, acc 1
2016-09-06T19:11:12.878949: step 4675, loss 0.000147971, acc 1
2016-09-06T19:11:13.567797: step 4676, loss 0.151995, acc 0.96
2016-09-06T19:11:14.300501: step 4677, loss 0.0468288, acc 0.96
2016-09-06T19:11:15.015401: step 4678, loss 0.0316855, acc 0.98
2016-09-06T19:11:15.693820: step 4679, loss 0.0571868, acc 0.96
2016-09-06T19:11:16.380722: step 4680, loss 0.0357672, acc 0.98
2016-09-06T19:11:17.073012: step 4681, loss 0.0145608, acc 1
2016-09-06T19:11:17.778982: step 4682, loss 0.0364978, acc 0.98
2016-09-06T19:11:18.452778: step 4683, loss 0.0300532, acc 0.98
2016-09-06T19:11:19.136284: step 4684, loss 0.0252686, acc 0.98
2016-09-06T19:11:19.838597: step 4685, loss 0.0431891, acc 0.98
2016-09-06T19:11:20.525413: step 4686, loss 0.0337616, acc 0.98
2016-09-06T19:11:21.222742: step 4687, loss 0.0487377, acc 0.98
2016-09-06T19:11:21.886774: step 4688, loss 0.0284951, acc 0.98
2016-09-06T19:11:22.598405: step 4689, loss 0.0397441, acc 1
2016-09-06T19:11:23.279146: step 4690, loss 0.0501482, acc 0.96
2016-09-06T19:11:23.965033: step 4691, loss 0.030434, acc 0.98
2016-09-06T19:11:24.644145: step 4692, loss 0.0532048, acc 0.96
2016-09-06T19:11:25.339773: step 4693, loss 0.0609825, acc 0.98
2016-09-06T19:11:26.037021: step 4694, loss 0.00186657, acc 1
2016-09-06T19:11:26.707528: step 4695, loss 0.0437646, acc 0.98
2016-09-06T19:11:27.392308: step 4696, loss 0.0129958, acc 1
2016-09-06T19:11:28.060900: step 4697, loss 0.0256773, acc 0.98
2016-09-06T19:11:28.758137: step 4698, loss 0.0415331, acc 0.98
2016-09-06T19:11:29.456672: step 4699, loss 0.0275412, acc 0.98
2016-09-06T19:11:30.149721: step 4700, loss 0.0165518, acc 1

Evaluation:
2016-09-06T19:11:33.309962: step 4700, loss 1.73517, acc 0.752345

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-4700

2016-09-06T19:11:35.078550: step 4701, loss 0.000717112, acc 1
2016-09-06T19:11:35.810121: step 4702, loss 0.00796812, acc 1
2016-09-06T19:11:36.481227: step 4703, loss 0.0422061, acc 0.96
2016-09-06T19:11:37.153268: step 4704, loss 0.034235, acc 0.98
2016-09-06T19:11:37.855212: step 4705, loss 0.00252581, acc 1
2016-09-06T19:11:38.539303: step 4706, loss 0.0151344, acc 1
2016-09-06T19:11:39.252341: step 4707, loss 0.0311958, acc 1
2016-09-06T19:11:39.927511: step 4708, loss 0.0439611, acc 0.98
2016-09-06T19:11:40.623598: step 4709, loss 0.0397761, acc 0.96
2016-09-06T19:11:41.302167: step 4710, loss 0.000373006, acc 1
2016-09-06T19:11:41.992251: step 4711, loss 0.00762972, acc 1
2016-09-06T19:11:42.673605: step 4712, loss 0.0359012, acc 0.98
2016-09-06T19:11:43.357936: step 4713, loss 0.00822776, acc 1
2016-09-06T19:11:44.049460: step 4714, loss 0.00601959, acc 1
2016-09-06T19:11:44.726495: step 4715, loss 0.0268331, acc 0.98
2016-09-06T19:11:45.413929: step 4716, loss 0.00769845, acc 1
2016-09-06T19:11:46.096554: step 4717, loss 0.00814613, acc 1
2016-09-06T19:11:46.783331: step 4718, loss 0.035118, acc 0.98
2016-09-06T19:11:47.482563: step 4719, loss 0.00704857, acc 1
2016-09-06T19:11:48.173953: step 4720, loss 0.0135402, acc 1
2016-09-06T19:11:48.864639: step 4721, loss 0.0175939, acc 1
2016-09-06T19:11:49.532230: step 4722, loss 0.028091, acc 1
2016-09-06T19:11:50.237053: step 4723, loss 0.00291933, acc 1
2016-09-06T19:11:50.907707: step 4724, loss 0.0177919, acc 1
2016-09-06T19:11:51.576532: step 4725, loss 0.000104318, acc 1
2016-09-06T19:11:52.258168: step 4726, loss 0.000367662, acc 1
2016-09-06T19:11:52.959145: step 4727, loss 0.0799473, acc 0.94
2016-09-06T19:11:53.637888: step 4728, loss 0.000871321, acc 1
2016-09-06T19:11:54.347738: step 4729, loss 0.0136067, acc 1
2016-09-06T19:11:55.061637: step 4730, loss 0.0145781, acc 1
2016-09-06T19:11:55.735766: step 4731, loss 0.000156866, acc 1
2016-09-06T19:11:56.420213: step 4732, loss 0.00970682, acc 1
2016-09-06T19:11:57.081571: step 4733, loss 0.0302495, acc 0.98
2016-09-06T19:11:57.773100: step 4734, loss 0.000861119, acc 1
2016-09-06T19:11:58.461537: step 4735, loss 0.0141882, acc 1
2016-09-06T19:11:59.147982: step 4736, loss 0.000141911, acc 1
2016-09-06T19:11:59.849439: step 4737, loss 0.00410002, acc 1
2016-09-06T19:12:00.585481: step 4738, loss 0.00533167, acc 1
2016-09-06T19:12:01.261872: step 4739, loss 0.0824438, acc 0.96
2016-09-06T19:12:01.932893: step 4740, loss 4.71082e-05, acc 1
2016-09-06T19:12:02.622853: step 4741, loss 0.0234209, acc 0.98
2016-09-06T19:12:03.318765: step 4742, loss 0.00182733, acc 1
2016-09-06T19:12:04.000274: step 4743, loss 0.000164189, acc 1
2016-09-06T19:12:04.718464: step 4744, loss 0.00642805, acc 1
2016-09-06T19:12:05.408107: step 4745, loss 0.00306083, acc 1
2016-09-06T19:12:06.100194: step 4746, loss 0.00961689, acc 1
2016-09-06T19:12:06.783747: step 4747, loss 0.00780175, acc 1
2016-09-06T19:12:07.466211: step 4748, loss 0.0292866, acc 0.98
2016-09-06T19:12:08.166886: step 4749, loss 0.0179811, acc 1
2016-09-06T19:12:08.834276: step 4750, loss 0.0119465, acc 1
2016-09-06T19:12:09.524606: step 4751, loss 0.00401561, acc 1
2016-09-06T19:12:10.212031: step 4752, loss 0.0281967, acc 0.98
2016-09-06T19:12:10.904421: step 4753, loss 0.0133211, acc 1
2016-09-06T19:12:11.594727: step 4754, loss 0.00217603, acc 1
2016-09-06T19:12:12.286986: step 4755, loss 0.00010678, acc 1
2016-09-06T19:12:12.976312: step 4756, loss 0.026731, acc 0.98
2016-09-06T19:12:13.614203: step 4757, loss 0.042674, acc 0.98
2016-09-06T19:12:14.316140: step 4758, loss 4.21645e-05, acc 1
2016-09-06T19:12:14.993960: step 4759, loss 0.118429, acc 0.96
2016-09-06T19:12:15.687477: step 4760, loss 0.0228896, acc 0.98
2016-09-06T19:12:16.374605: step 4761, loss 0.0154676, acc 0.98
2016-09-06T19:12:17.075515: step 4762, loss 0.0147103, acc 1
2016-09-06T19:12:17.760951: step 4763, loss 0.0026641, acc 1
2016-09-06T19:12:18.418194: step 4764, loss 0.0025539, acc 1
2016-09-06T19:12:19.142177: step 4765, loss 0.00800863, acc 1
2016-09-06T19:12:19.825568: step 4766, loss 0.0124907, acc 1
2016-09-06T19:12:20.518742: step 4767, loss 0.0143978, acc 0.98
2016-09-06T19:12:21.236045: step 4768, loss 0.0450186, acc 0.98
2016-09-06T19:12:21.920849: step 4769, loss 0.0865049, acc 0.98
2016-09-06T19:12:22.628558: step 4770, loss 0.00966943, acc 1
2016-09-06T19:12:23.314243: step 4771, loss 0.00551658, acc 1
2016-09-06T19:12:24.016632: step 4772, loss 0.000642926, acc 1
2016-09-06T19:12:24.696621: step 4773, loss 0.0501371, acc 0.96
2016-09-06T19:12:25.380355: step 4774, loss 0.0271933, acc 0.98
2016-09-06T19:12:26.068725: step 4775, loss 0.00751214, acc 1
2016-09-06T19:12:26.766333: step 4776, loss 0.0115798, acc 1
2016-09-06T19:12:27.468192: step 4777, loss 0.0260377, acc 0.98
2016-09-06T19:12:28.156856: step 4778, loss 0.0740976, acc 0.98
2016-09-06T19:12:28.828098: step 4779, loss 0.0141104, acc 1
2016-09-06T19:12:29.509092: step 4780, loss 0.0107346, acc 1
2016-09-06T19:12:30.210341: step 4781, loss 0.000453297, acc 1
2016-09-06T19:12:30.894181: step 4782, loss 0.0273093, acc 0.98
2016-09-06T19:12:31.581196: step 4783, loss 0.00401082, acc 1
2016-09-06T19:12:32.279284: step 4784, loss 0.039854, acc 0.96
2016-09-06T19:12:32.954205: step 4785, loss 0.13101, acc 0.94
2016-09-06T19:12:33.654278: step 4786, loss 0.0359274, acc 1
2016-09-06T19:12:34.337989: step 4787, loss 0.143116, acc 0.92
2016-09-06T19:12:35.031208: step 4788, loss 0.0448504, acc 0.98
2016-09-06T19:12:35.715164: step 4789, loss 0.0441476, acc 0.96
2016-09-06T19:12:36.397416: step 4790, loss 1.37805e-06, acc 1
2016-09-06T19:12:37.101924: step 4791, loss 0.00817088, acc 1
2016-09-06T19:12:37.768352: step 4792, loss 0.0557733, acc 0.98
2016-09-06T19:12:38.490588: step 4793, loss 0.0213088, acc 0.98
2016-09-06T19:12:39.200870: step 4794, loss 0.0147571, acc 1
2016-09-06T19:12:39.907105: step 4795, loss 0.0412944, acc 0.96
2016-09-06T19:12:40.616228: step 4796, loss 0.000613639, acc 1
2016-09-06T19:12:41.269221: step 4797, loss 0.0069226, acc 1
2016-09-06T19:12:41.968362: step 4798, loss 0.111078, acc 0.96
2016-09-06T19:12:42.649469: step 4799, loss 0.0452759, acc 0.98
2016-09-06T19:12:43.297399: step 4800, loss 0.000105603, acc 1

Evaluation:
2016-09-06T19:12:46.419223: step 4800, loss 2.05946, acc 0.753283

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-4800

2016-09-06T19:12:48.121700: step 4801, loss 0.0273769, acc 0.98
2016-09-06T19:12:48.801060: step 4802, loss 0.00919855, acc 1
2016-09-06T19:12:49.501505: step 4803, loss 0.0338151, acc 0.98
2016-09-06T19:12:50.191759: step 4804, loss 0.0122742, acc 1
2016-09-06T19:12:50.841124: step 4805, loss 0.0173333, acc 1
2016-09-06T19:12:51.558727: step 4806, loss 0.018182, acc 0.98
2016-09-06T19:12:52.237590: step 4807, loss 0.0144806, acc 1
2016-09-06T19:12:52.923867: step 4808, loss 0.0186017, acc 1
2016-09-06T19:12:53.606025: step 4809, loss 0.0138378, acc 1
2016-09-06T19:12:54.277686: step 4810, loss 0.00762281, acc 1
2016-09-06T19:12:54.959264: step 4811, loss 0.0577795, acc 0.96
2016-09-06T19:12:55.648587: step 4812, loss 0.0132212, acc 1
2016-09-06T19:12:56.362212: step 4813, loss 0.0104222, acc 1
2016-09-06T19:12:57.043641: step 4814, loss 0.00507349, acc 1
2016-09-06T19:12:57.718271: step 4815, loss 0.00989521, acc 1
2016-09-06T19:12:58.407072: step 4816, loss 0.0180456, acc 0.98
2016-09-06T19:12:59.126502: step 4817, loss 0.0943474, acc 0.98
2016-09-06T19:12:59.816942: step 4818, loss 0.00175266, acc 1
2016-09-06T19:13:00.545237: step 4819, loss 0.00281638, acc 1
2016-09-06T19:13:01.259514: step 4820, loss 0.0485398, acc 0.98
2016-09-06T19:13:01.944600: step 4821, loss 0.0117404, acc 1
2016-09-06T19:13:02.663470: step 4822, loss 5.3642e-05, acc 1
2016-09-06T19:13:03.375467: step 4823, loss 0.00153861, acc 1
2016-09-06T19:13:04.098545: step 4824, loss 0.0920499, acc 0.98
2016-09-06T19:13:04.825620: step 4825, loss 0.00747036, acc 1
2016-09-06T19:13:05.520529: step 4826, loss 0.0381832, acc 0.98
2016-09-06T19:13:06.207347: step 4827, loss 0.00193449, acc 1
2016-09-06T19:13:06.913679: step 4828, loss 0.00373434, acc 1
2016-09-06T19:13:07.604521: step 4829, loss 0.0221753, acc 0.98
2016-09-06T19:13:08.346180: step 4830, loss 0.0824265, acc 0.96
2016-09-06T19:13:08.993663: step 4831, loss 0.095701, acc 0.98
2016-09-06T19:13:09.662049: step 4832, loss 0.0143343, acc 1
2016-09-06T19:13:10.348785: step 4833, loss 0.00798738, acc 1
2016-09-06T19:13:11.038943: step 4834, loss 0.0862829, acc 0.98
2016-09-06T19:13:11.714190: step 4835, loss 0.064809, acc 0.96
2016-09-06T19:13:12.395241: step 4836, loss 0.0599033, acc 0.96
2016-09-06T19:13:13.113881: step 4837, loss 0.000871665, acc 1
2016-09-06T19:13:13.778000: step 4838, loss 0.0174932, acc 0.98
2016-09-06T19:13:14.477151: step 4839, loss 0.0150504, acc 1
2016-09-06T19:13:15.151129: step 4840, loss 0.0183355, acc 1
2016-09-06T19:13:15.833688: step 4841, loss 0.0679064, acc 0.96
2016-09-06T19:13:16.523433: step 4842, loss 0.0347194, acc 0.98
2016-09-06T19:13:17.191699: step 4843, loss 0.0171728, acc 1
2016-09-06T19:13:17.897515: step 4844, loss 0.0240159, acc 0.98
2016-09-06T19:13:18.575991: step 4845, loss 0.0440571, acc 0.98
2016-09-06T19:13:19.283330: step 4846, loss 0.0298874, acc 0.98
2016-09-06T19:13:19.978785: step 4847, loss 0.0165897, acc 1
2016-09-06T19:13:20.657368: step 4848, loss 0.00317748, acc 1
2016-09-06T19:13:21.352260: step 4849, loss 0.0265681, acc 0.98
2016-09-06T19:13:22.035257: step 4850, loss 0.00206845, acc 1
2016-09-06T19:13:22.732914: step 4851, loss 0.0129704, acc 1
2016-09-06T19:13:23.391642: step 4852, loss 0.000323996, acc 1
2016-09-06T19:13:24.096067: step 4853, loss 0.0105498, acc 1
2016-09-06T19:13:24.789080: step 4854, loss 0.0173544, acc 0.98
2016-09-06T19:13:25.468528: step 4855, loss 0.0593962, acc 0.98
2016-09-06T19:13:26.158877: step 4856, loss 0.0992206, acc 0.98
2016-09-06T19:13:26.847394: step 4857, loss 0.000241897, acc 1
2016-09-06T19:13:27.531697: step 4858, loss 0.00163288, acc 1
2016-09-06T19:13:28.195318: step 4859, loss 0.00172633, acc 1
2016-09-06T19:13:28.897886: step 4860, loss 0.00198498, acc 1
2016-09-06T19:13:29.579188: step 4861, loss 0.0180813, acc 0.98
2016-09-06T19:13:30.285795: step 4862, loss 0.0128359, acc 1
2016-09-06T19:13:30.974067: step 4863, loss 0.0155956, acc 1
2016-09-06T19:13:31.676358: step 4864, loss 0.0280315, acc 0.98
2016-09-06T19:13:32.368851: step 4865, loss 0.017704, acc 0.98
2016-09-06T19:13:33.031542: step 4866, loss 0.00557677, acc 1
2016-09-06T19:13:33.732438: step 4867, loss 0.00433295, acc 1
2016-09-06T19:13:34.423186: step 4868, loss 0.0213336, acc 1
2016-09-06T19:13:35.106591: step 4869, loss 0.054164, acc 0.98
2016-09-06T19:13:35.779663: step 4870, loss 0.101939, acc 0.98
2016-09-06T19:13:36.456272: step 4871, loss 0.0866437, acc 0.96
2016-09-06T19:13:37.156710: step 4872, loss 0.0801325, acc 0.98
2016-09-06T19:13:37.824700: step 4873, loss 0.0258396, acc 0.98
2016-09-06T19:13:38.521195: step 4874, loss 0.0415892, acc 0.98
2016-09-06T19:13:39.209796: step 4875, loss 0.0270837, acc 0.98
2016-09-06T19:13:39.886650: step 4876, loss 0.0016311, acc 1
2016-09-06T19:13:40.582565: step 4877, loss 0.0732259, acc 0.98
2016-09-06T19:13:41.260981: step 4878, loss 0.00252935, acc 1
2016-09-06T19:13:41.975890: step 4879, loss 0.0322037, acc 0.98
2016-09-06T19:13:42.643546: step 4880, loss 0.000851049, acc 1
2016-09-06T19:13:43.358006: step 4881, loss 0.0661098, acc 0.98
2016-09-06T19:13:44.060674: step 4882, loss 0.029073, acc 1
2016-09-06T19:13:44.763196: step 4883, loss 0.0311695, acc 0.98
2016-09-06T19:13:45.437359: step 4884, loss 0.0106948, acc 1
2016-09-06T19:13:46.122919: step 4885, loss 0.00996736, acc 1
2016-09-06T19:13:46.808932: step 4886, loss 0.0424307, acc 0.96
2016-09-06T19:13:47.469663: step 4887, loss 0.0127648, acc 1
2016-09-06T19:13:48.157213: step 4888, loss 0.0051884, acc 1
2016-09-06T19:13:48.832983: step 4889, loss 0.0523483, acc 0.98
2016-09-06T19:13:49.530003: step 4890, loss 0.0059911, acc 1
2016-09-06T19:13:50.216664: step 4891, loss 0.0631478, acc 0.98
2016-09-06T19:13:50.918269: step 4892, loss 0.134041, acc 0.96
2016-09-06T19:13:51.622000: step 4893, loss 0.0269582, acc 0.98
2016-09-06T19:13:52.297222: step 4894, loss 0.00444814, acc 1
2016-09-06T19:13:52.982985: step 4895, loss 0.0331626, acc 0.98
2016-09-06T19:13:53.691617: step 4896, loss 0.0231443, acc 1
2016-09-06T19:13:54.384327: step 4897, loss 0.0223352, acc 0.98
2016-09-06T19:13:55.062819: step 4898, loss 0.0665117, acc 0.98
2016-09-06T19:13:55.753457: step 4899, loss 0.0356595, acc 0.98
2016-09-06T19:13:56.459209: step 4900, loss 0.0165673, acc 1

Evaluation:
2016-09-06T19:13:59.600910: step 4900, loss 2.01562, acc 0.753283

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-4900

2016-09-06T19:14:01.258848: step 4901, loss 0.0226832, acc 1
2016-09-06T19:14:01.930211: step 4902, loss 0.00225941, acc 1
2016-09-06T19:14:02.618190: step 4903, loss 0.0577936, acc 0.96
2016-09-06T19:14:03.312682: step 4904, loss 0.0392413, acc 0.98
2016-09-06T19:14:03.996951: step 4905, loss 0.0170921, acc 1
2016-09-06T19:14:04.695700: step 4906, loss 0.0155647, acc 1
2016-09-06T19:14:05.378366: step 4907, loss 0.0108891, acc 1
2016-09-06T19:14:06.106044: step 4908, loss 0.00266587, acc 1
2016-09-06T19:14:06.777139: step 4909, loss 0.0364517, acc 0.98
2016-09-06T19:14:07.456824: step 4910, loss 0.0373581, acc 0.96
2016-09-06T19:14:08.149349: step 4911, loss 0.00785515, acc 1
2016-09-06T19:14:08.857587: step 4912, loss 0.011654, acc 1
2016-09-06T19:14:09.567528: step 4913, loss 0.000152397, acc 1
2016-09-06T19:14:10.236358: step 4914, loss 0.0143013, acc 0.98
2016-09-06T19:14:10.944711: step 4915, loss 0.0282386, acc 1
2016-09-06T19:14:11.606712: step 4916, loss 0.000438461, acc 1
2016-09-06T19:14:12.290299: step 4917, loss 5.88423e-05, acc 1
2016-09-06T19:14:12.973992: step 4918, loss 0.00149864, acc 1
2016-09-06T19:14:13.660987: step 4919, loss 0.0344345, acc 1
2016-09-06T19:14:14.350067: step 4920, loss 0.00621842, acc 1
2016-09-06T19:14:15.016293: step 4921, loss 0.0407571, acc 0.98
2016-09-06T19:14:15.715608: step 4922, loss 0.0157715, acc 1
2016-09-06T19:14:16.394335: step 4923, loss 0.0492658, acc 0.98
2016-09-06T19:14:17.072048: step 4924, loss 0.0246446, acc 0.98
2016-09-06T19:14:17.770367: step 4925, loss 0.020927, acc 0.98
2016-09-06T19:14:18.448659: step 4926, loss 0.0298693, acc 0.98
2016-09-06T19:14:19.166596: step 4927, loss 0.0173014, acc 0.98
2016-09-06T19:14:19.825280: step 4928, loss 0.00336192, acc 1
2016-09-06T19:14:20.522305: step 4929, loss 0.000188526, acc 1
2016-09-06T19:14:21.198241: step 4930, loss 0.0167982, acc 0.98
2016-09-06T19:14:21.872591: step 4931, loss 0.0660875, acc 0.98
2016-09-06T19:14:22.558010: step 4932, loss 0.029695, acc 0.98
2016-09-06T19:14:23.243848: step 4933, loss 0.0498859, acc 0.96
2016-09-06T19:14:23.913786: step 4934, loss 0.0434981, acc 0.96
2016-09-06T19:14:24.609473: step 4935, loss 0.00176461, acc 1
2016-09-06T19:14:25.317364: step 4936, loss 0.00761039, acc 1
2016-09-06T19:14:25.989513: step 4937, loss 0.00318654, acc 1
2016-09-06T19:14:26.674773: step 4938, loss 0.00211894, acc 1
2016-09-06T19:14:27.361626: step 4939, loss 0.179243, acc 0.98
2016-09-06T19:14:28.048746: step 4940, loss 0.011498, acc 1
2016-09-06T19:14:28.729116: step 4941, loss 0.0325283, acc 0.98
2016-09-06T19:14:29.421285: step 4942, loss 0.0217039, acc 0.98
2016-09-06T19:14:30.138492: step 4943, loss 0.0360106, acc 0.96
2016-09-06T19:14:30.806529: step 4944, loss 0.00357932, acc 1
2016-09-06T19:14:31.493043: step 4945, loss 0.048594, acc 0.96
2016-09-06T19:14:32.183377: step 4946, loss 0.0109924, acc 1
2016-09-06T19:14:32.869489: step 4947, loss 0.156252, acc 0.96
2016-09-06T19:14:33.573979: step 4948, loss 0.0175746, acc 0.98
2016-09-06T19:14:34.259361: step 4949, loss 0.059197, acc 0.96
2016-09-06T19:14:34.965741: step 4950, loss 0.0344106, acc 0.98
2016-09-06T19:14:35.643559: step 4951, loss 0.0547554, acc 0.98
2016-09-06T19:14:36.336300: step 4952, loss 0.0285934, acc 0.98
2016-09-06T19:14:37.031376: step 4953, loss 0.0138659, acc 1
2016-09-06T19:14:37.727252: step 4954, loss 0.0514677, acc 0.96
2016-09-06T19:14:38.421077: step 4955, loss 0.0708275, acc 0.96
2016-09-06T19:14:39.093061: step 4956, loss 0.000297512, acc 1
2016-09-06T19:14:39.785321: step 4957, loss 0.0476882, acc 0.98
2016-09-06T19:14:40.497047: step 4958, loss 0.0216589, acc 0.98
2016-09-06T19:14:41.149830: step 4959, loss 0.0123424, acc 1
2016-09-06T19:14:41.830188: step 4960, loss 0.0355305, acc 0.98
2016-09-06T19:14:42.511926: step 4961, loss 0.0133759, acc 1
2016-09-06T19:14:43.209510: step 4962, loss 0.145305, acc 0.98
2016-09-06T19:14:43.888138: step 4963, loss 0.00445785, acc 1
2016-09-06T19:14:44.613537: step 4964, loss 0.00146105, acc 1
2016-09-06T19:14:45.295035: step 4965, loss 0.0133613, acc 1
2016-09-06T19:14:45.976864: step 4966, loss 0.0367057, acc 0.96
2016-09-06T19:14:46.668725: step 4967, loss 0.0256694, acc 1
2016-09-06T19:14:47.363266: step 4968, loss 0.0287249, acc 1
2016-09-06T19:14:48.072968: step 4969, loss 0.0232853, acc 1
2016-09-06T19:14:48.748107: step 4970, loss 0.0364147, acc 0.98
2016-09-06T19:14:49.448117: step 4971, loss 0.00503842, acc 1
2016-09-06T19:14:50.137904: step 4972, loss 0.0146224, acc 1
2016-09-06T19:14:50.849988: step 4973, loss 0.0350868, acc 1
2016-09-06T19:14:51.543222: step 4974, loss 0.0332423, acc 1
2016-09-06T19:14:52.241425: step 4975, loss 0.00765252, acc 1
2016-09-06T19:14:52.949274: step 4976, loss 0.025101, acc 1
2016-09-06T19:14:53.649067: step 4977, loss 0.0236625, acc 1
2016-09-06T19:14:54.330881: step 4978, loss 0.0137504, acc 1
2016-09-06T19:14:55.019324: step 4979, loss 0.0326403, acc 0.98
2016-09-06T19:14:55.713347: step 4980, loss 0.0254043, acc 0.98
2016-09-06T19:14:56.433912: step 4981, loss 0.0322961, acc 0.98
2016-09-06T19:14:57.128736: step 4982, loss 0.000115521, acc 1
2016-09-06T19:14:57.834010: step 4983, loss 0.000279338, acc 1
2016-09-06T19:14:58.509519: step 4984, loss 0.00456744, acc 1
2016-09-06T19:14:59.193067: step 4985, loss 0.010208, acc 1
2016-09-06T19:14:59.892612: step 4986, loss 0.0105959, acc 1
2016-09-06T19:15:00.622620: step 4987, loss 0.0146365, acc 1
2016-09-06T19:15:01.334457: step 4988, loss 0.000173231, acc 1
2016-09-06T19:15:02.032458: step 4989, loss 0.00254211, acc 1
2016-09-06T19:15:02.717799: step 4990, loss 0.0355139, acc 0.98
2016-09-06T19:15:03.409865: step 4991, loss 0.0660669, acc 0.98
2016-09-06T19:15:04.044083: step 4992, loss 0.000112465, acc 1
2016-09-06T19:15:04.722928: step 4993, loss 0.0279875, acc 1
2016-09-06T19:15:05.392826: step 4994, loss 0.00958776, acc 1
2016-09-06T19:15:06.082985: step 4995, loss 0.00273621, acc 1
2016-09-06T19:15:06.750283: step 4996, loss 0.036662, acc 0.98
2016-09-06T19:15:07.453928: step 4997, loss 0.00179914, acc 1
2016-09-06T19:15:08.164042: step 4998, loss 0.000958785, acc 1
2016-09-06T19:15:08.865713: step 4999, loss 0.0266082, acc 0.98
2016-09-06T19:15:09.556257: step 5000, loss 0.00674615, acc 1

Evaluation:
2016-09-06T19:15:12.701792: step 5000, loss 1.87872, acc 0.762664

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-5000

2016-09-06T19:15:14.416869: step 5001, loss 0.0266564, acc 0.98
2016-09-06T19:15:15.112810: step 5002, loss 0.0383184, acc 0.98
2016-09-06T19:15:15.844146: step 5003, loss 0.0489692, acc 0.96
2016-09-06T19:15:16.531135: step 5004, loss 0.016901, acc 1
2016-09-06T19:15:17.222325: step 5005, loss 0.0180045, acc 0.98
2016-09-06T19:15:17.911454: step 5006, loss 0.0045333, acc 1
2016-09-06T19:15:18.604953: step 5007, loss 0.0113092, acc 1
2016-09-06T19:15:19.292322: step 5008, loss 0.0118269, acc 1
2016-09-06T19:15:19.946466: step 5009, loss 0.0160198, acc 1
2016-09-06T19:15:20.655600: step 5010, loss 0.0382935, acc 0.98
2016-09-06T19:15:21.332034: step 5011, loss 0.00687756, acc 1
2016-09-06T19:15:22.021313: step 5012, loss 0.0123343, acc 1
2016-09-06T19:15:22.722717: step 5013, loss 0.00720147, acc 1
2016-09-06T19:15:23.393957: step 5014, loss 0.00405427, acc 1
2016-09-06T19:15:24.074327: step 5015, loss 0.00732812, acc 1
2016-09-06T19:15:24.741912: step 5016, loss 0.0139047, acc 1
2016-09-06T19:15:25.431854: step 5017, loss 0.0391377, acc 0.98
2016-09-06T19:15:26.109215: step 5018, loss 0.00258771, acc 1
2016-09-06T19:15:26.795402: step 5019, loss 0.00133112, acc 1
2016-09-06T19:15:27.488012: step 5020, loss 0.00124967, acc 1
2016-09-06T19:15:28.192776: step 5021, loss 0.403873, acc 0.96
2016-09-06T19:15:28.896963: step 5022, loss 0.056933, acc 0.98
2016-09-06T19:15:29.560366: step 5023, loss 0.000904941, acc 1
2016-09-06T19:15:30.268682: step 5024, loss 0.00163817, acc 1
2016-09-06T19:15:30.991942: step 5025, loss 0.0403621, acc 0.96
2016-09-06T19:15:31.677067: step 5026, loss 0.0386172, acc 0.98
2016-09-06T19:15:32.366786: step 5027, loss 0.0463505, acc 0.98
2016-09-06T19:15:33.052973: step 5028, loss 0.0206579, acc 0.98
2016-09-06T19:15:33.771652: step 5029, loss 2.41417e-05, acc 1
2016-09-06T19:15:34.444644: step 5030, loss 0.043119, acc 0.98
2016-09-06T19:15:35.155133: step 5031, loss 0.0631238, acc 0.98
2016-09-06T19:15:35.824749: step 5032, loss 0.031781, acc 0.98
2016-09-06T19:15:36.494495: step 5033, loss 0.0760325, acc 0.96
2016-09-06T19:15:37.166922: step 5034, loss 0.000629695, acc 1
2016-09-06T19:15:37.843608: step 5035, loss 0.0554391, acc 0.94
2016-09-06T19:15:38.555874: step 5036, loss 0.134125, acc 0.96
2016-09-06T19:15:39.232392: step 5037, loss 0.0421877, acc 0.98
2016-09-06T19:15:39.928925: step 5038, loss 0.0354974, acc 0.98
2016-09-06T19:15:40.601896: step 5039, loss 0.0144744, acc 1
2016-09-06T19:15:41.291510: step 5040, loss 0.0728347, acc 0.96
2016-09-06T19:15:41.986211: step 5041, loss 0.0301641, acc 1
2016-09-06T19:15:42.681474: step 5042, loss 0.0219298, acc 1
2016-09-06T19:15:43.384346: step 5043, loss 0.0199185, acc 0.98
2016-09-06T19:15:44.055496: step 5044, loss 0.0070359, acc 1
2016-09-06T19:15:44.752402: step 5045, loss 0.0113824, acc 1
2016-09-06T19:15:45.444332: step 5046, loss 0.0186272, acc 1
2016-09-06T19:15:46.124953: step 5047, loss 0.00789997, acc 1
2016-09-06T19:15:46.807188: step 5048, loss 0.00592163, acc 1
2016-09-06T19:15:47.488841: step 5049, loss 0.0117145, acc 1
2016-09-06T19:15:48.166314: step 5050, loss 0.0492759, acc 0.98
2016-09-06T19:15:48.819663: step 5051, loss 0.0762608, acc 0.96
2016-09-06T19:15:49.530380: step 5052, loss 0.00574177, acc 1
2016-09-06T19:15:50.224527: step 5053, loss 0.00660393, acc 1
2016-09-06T19:15:50.913294: step 5054, loss 0.0256708, acc 0.98
2016-09-06T19:15:51.615420: step 5055, loss 0.0177622, acc 1
2016-09-06T19:15:52.305967: step 5056, loss 0.0326159, acc 0.98
2016-09-06T19:15:53.000219: step 5057, loss 0.0176927, acc 0.98
2016-09-06T19:15:53.665947: step 5058, loss 0.00191874, acc 1
2016-09-06T19:15:54.379419: step 5059, loss 0.00362422, acc 1
2016-09-06T19:15:55.063947: step 5060, loss 0.0308039, acc 1
2016-09-06T19:15:55.740217: step 5061, loss 0.050177, acc 0.98
2016-09-06T19:15:56.414310: step 5062, loss 0.0270691, acc 0.98
2016-09-06T19:15:57.098249: step 5063, loss 0.0224, acc 0.98
2016-09-06T19:15:57.804310: step 5064, loss 0.0224821, acc 0.98
2016-09-06T19:15:58.469798: step 5065, loss 0.0133104, acc 1
2016-09-06T19:15:59.171362: step 5066, loss 0.00774044, acc 1
2016-09-06T19:15:59.860138: step 5067, loss 0.0401132, acc 0.96
2016-09-06T19:16:00.587533: step 5068, loss 0.000426509, acc 1
2016-09-06T19:16:01.275753: step 5069, loss 0.0280252, acc 0.98
2016-09-06T19:16:01.959311: step 5070, loss 0.0125293, acc 1
2016-09-06T19:16:02.651693: step 5071, loss 0.0843296, acc 0.98
2016-09-06T19:16:03.347794: step 5072, loss 0.00726549, acc 1
2016-09-06T19:16:04.048297: step 5073, loss 0.0131464, acc 1
2016-09-06T19:16:04.733851: step 5074, loss 0.0166561, acc 1
2016-09-06T19:16:05.419674: step 5075, loss 0.0137478, acc 1
2016-09-06T19:16:06.120183: step 5076, loss 0.0283039, acc 0.98
2016-09-06T19:16:06.793613: step 5077, loss 0.0253734, acc 0.98
2016-09-06T19:16:07.515939: step 5078, loss 0.0213002, acc 0.98
2016-09-06T19:16:08.195693: step 5079, loss 0.0368163, acc 1
2016-09-06T19:16:08.920862: step 5080, loss 0.000729376, acc 1
2016-09-06T19:16:09.614927: step 5081, loss 0.0160697, acc 0.98
2016-09-06T19:16:10.310483: step 5082, loss 0.0528464, acc 0.96
2016-09-06T19:16:10.988501: step 5083, loss 0.000893651, acc 1
2016-09-06T19:16:11.648543: step 5084, loss 0.0326491, acc 0.98
2016-09-06T19:16:12.351499: step 5085, loss 0.0306382, acc 0.98
2016-09-06T19:16:13.087544: step 5086, loss 0.054034, acc 0.98
2016-09-06T19:16:13.772656: step 5087, loss 0.015346, acc 1
2016-09-06T19:16:14.463183: step 5088, loss 0.0298528, acc 0.98
2016-09-06T19:16:15.143230: step 5089, loss 0.0371494, acc 0.98
2016-09-06T19:16:15.832188: step 5090, loss 0.00165192, acc 1
2016-09-06T19:16:16.496980: step 5091, loss 0.0769866, acc 0.98
2016-09-06T19:16:17.195701: step 5092, loss 0.0102804, acc 1
2016-09-06T19:16:17.877218: step 5093, loss 0.106803, acc 0.96
2016-09-06T19:16:18.585113: step 5094, loss 0.0607096, acc 0.98
2016-09-06T19:16:19.267150: step 5095, loss 0.0128962, acc 1
2016-09-06T19:16:19.976147: step 5096, loss 0.0704227, acc 0.96
2016-09-06T19:16:20.665121: step 5097, loss 0.0106889, acc 1
2016-09-06T19:16:21.345370: step 5098, loss 0.0185608, acc 0.98
2016-09-06T19:16:22.073523: step 5099, loss 0.00669762, acc 1
2016-09-06T19:16:22.759484: step 5100, loss 0.0164929, acc 0.98

Evaluation:
2016-09-06T19:16:25.881554: step 5100, loss 1.92341, acc 0.757974

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-5100

2016-09-06T19:16:27.531011: step 5101, loss 0.0223129, acc 0.98
2016-09-06T19:16:28.214363: step 5102, loss 0.0183702, acc 1
2016-09-06T19:16:28.904400: step 5103, loss 0.0585125, acc 0.96
2016-09-06T19:16:29.601314: step 5104, loss 0.0293834, acc 0.98
2016-09-06T19:16:30.318951: step 5105, loss 0.00822396, acc 1
2016-09-06T19:16:31.006597: step 5106, loss 0.00952796, acc 1
2016-09-06T19:16:31.679209: step 5107, loss 0.237395, acc 0.94
2016-09-06T19:16:32.373216: step 5108, loss 0.0358824, acc 0.98
2016-09-06T19:16:33.061516: step 5109, loss 0.0142472, acc 1
2016-09-06T19:16:33.745007: step 5110, loss 0.00392255, acc 1
2016-09-06T19:16:34.435948: step 5111, loss 0.0549142, acc 0.98
2016-09-06T19:16:35.120683: step 5112, loss 0.0461136, acc 0.98
2016-09-06T19:16:35.805355: step 5113, loss 0.042678, acc 0.96
2016-09-06T19:16:36.509309: step 5114, loss 0.0181277, acc 1
2016-09-06T19:16:37.210337: step 5115, loss 0.0686545, acc 0.98
2016-09-06T19:16:37.905027: step 5116, loss 0.00970088, acc 1
2016-09-06T19:16:38.604333: step 5117, loss 0.00683288, acc 1
2016-09-06T19:16:39.295474: step 5118, loss 0.0230749, acc 1
2016-09-06T19:16:40.022703: step 5119, loss 0.0171956, acc 1
2016-09-06T19:16:40.699646: step 5120, loss 0.00620548, acc 1
2016-09-06T19:16:41.380766: step 5121, loss 0.00642127, acc 1
2016-09-06T19:16:42.058030: step 5122, loss 0.0129867, acc 1
2016-09-06T19:16:42.746680: step 5123, loss 0.0566853, acc 0.96
2016-09-06T19:16:43.452281: step 5124, loss 0.00258653, acc 1
2016-09-06T19:16:44.101575: step 5125, loss 0.0405603, acc 0.98
2016-09-06T19:16:44.801991: step 5126, loss 0.0827806, acc 0.98
2016-09-06T19:16:45.481134: step 5127, loss 0.0168558, acc 0.98
2016-09-06T19:16:46.188699: step 5128, loss 0.0143301, acc 1
2016-09-06T19:16:46.879071: step 5129, loss 0.0434521, acc 0.98
2016-09-06T19:16:47.562862: step 5130, loss 0.00301541, acc 1
2016-09-06T19:16:48.261496: step 5131, loss 0.00286077, acc 1
2016-09-06T19:16:48.928149: step 5132, loss 0.00117811, acc 1
2016-09-06T19:16:49.620395: step 5133, loss 0.0528377, acc 0.96
2016-09-06T19:16:50.309303: step 5134, loss 2.82669e-05, acc 1
2016-09-06T19:16:50.997141: step 5135, loss 0.0108792, acc 1
2016-09-06T19:16:51.690048: step 5136, loss 0.0422971, acc 0.96
2016-09-06T19:16:52.374186: step 5137, loss 0.122398, acc 0.96
2016-09-06T19:16:53.089616: step 5138, loss 0.0107706, acc 1
2016-09-06T19:16:53.763210: step 5139, loss 0.0249169, acc 0.98
2016-09-06T19:16:54.455605: step 5140, loss 0.0844498, acc 0.96
2016-09-06T19:16:55.129752: step 5141, loss 0.0564689, acc 0.96
2016-09-06T19:16:55.806329: step 5142, loss 0.0251267, acc 0.98
2016-09-06T19:16:56.495456: step 5143, loss 0.0531943, acc 0.98
2016-09-06T19:16:57.182699: step 5144, loss 0.0349632, acc 0.98
2016-09-06T19:16:57.886855: step 5145, loss 0.00125343, acc 1
2016-09-06T19:16:58.566702: step 5146, loss 0.0521166, acc 0.98
2016-09-06T19:16:59.282929: step 5147, loss 0.00976445, acc 1
2016-09-06T19:17:00.010637: step 5148, loss 0.0365926, acc 0.98
2016-09-06T19:17:00.754873: step 5149, loss 0.000762265, acc 1
2016-09-06T19:17:01.440094: step 5150, loss 0.0369661, acc 0.96
2016-09-06T19:17:02.120863: step 5151, loss 0.0255326, acc 0.98
2016-09-06T19:17:02.836296: step 5152, loss 0.0194659, acc 1
2016-09-06T19:17:03.525541: step 5153, loss 0.0111547, acc 1
2016-09-06T19:17:04.224162: step 5154, loss 0.0170876, acc 1
2016-09-06T19:17:04.928875: step 5155, loss 0.0245936, acc 0.98
2016-09-06T19:17:05.623706: step 5156, loss 0.0833413, acc 0.96
2016-09-06T19:17:06.319255: step 5157, loss 0.0165843, acc 1
2016-09-06T19:17:07.003170: step 5158, loss 0.0499838, acc 0.98
2016-09-06T19:17:07.713500: step 5159, loss 0.0275519, acc 0.98
2016-09-06T19:17:08.397601: step 5160, loss 0.0994319, acc 0.96
2016-09-06T19:17:09.102610: step 5161, loss 0.0846808, acc 0.98
2016-09-06T19:17:09.783213: step 5162, loss 0.0562829, acc 0.98
2016-09-06T19:17:10.452656: step 5163, loss 0.0182647, acc 0.98
2016-09-06T19:17:11.139898: step 5164, loss 0.0981127, acc 0.98
2016-09-06T19:17:11.804816: step 5165, loss 0.00126989, acc 1
2016-09-06T19:17:12.522027: step 5166, loss 0.00458886, acc 1
2016-09-06T19:17:13.218625: step 5167, loss 0.0582569, acc 0.98
2016-09-06T19:17:13.902539: step 5168, loss 0.0282965, acc 0.98
2016-09-06T19:17:14.582823: step 5169, loss 0.0392846, acc 0.98
2016-09-06T19:17:15.279164: step 5170, loss 0.031615, acc 0.98
2016-09-06T19:17:15.961585: step 5171, loss 0.0150861, acc 1
2016-09-06T19:17:16.640838: step 5172, loss 0.0163146, acc 1
2016-09-06T19:17:17.348416: step 5173, loss 0.0262335, acc 0.98
2016-09-06T19:17:18.029060: step 5174, loss 0.0303774, acc 0.98
2016-09-06T19:17:18.711742: step 5175, loss 0.000168133, acc 1
2016-09-06T19:17:19.389048: step 5176, loss 0.0201866, acc 1
2016-09-06T19:17:20.056125: step 5177, loss 0.020947, acc 1
2016-09-06T19:17:20.741995: step 5178, loss 0.0126389, acc 1
2016-09-06T19:17:21.401397: step 5179, loss 0.0846594, acc 0.98
2016-09-06T19:17:22.106429: step 5180, loss 0.000768833, acc 1
2016-09-06T19:17:22.783409: step 5181, loss 0.0341457, acc 0.98
2016-09-06T19:17:23.517537: step 5182, loss 0.0199456, acc 0.98
2016-09-06T19:17:24.237258: step 5183, loss 0.109726, acc 0.92
2016-09-06T19:17:24.890600: step 5184, loss 0.00071107, acc 1
2016-09-06T19:17:25.581435: step 5185, loss 0.00954749, acc 1
2016-09-06T19:17:26.239074: step 5186, loss 0.0136547, acc 1
2016-09-06T19:17:26.940442: step 5187, loss 0.028582, acc 0.98
2016-09-06T19:17:27.629295: step 5188, loss 0.0507288, acc 0.98
2016-09-06T19:17:28.327503: step 5189, loss 0.000111153, acc 1
2016-09-06T19:17:28.993036: step 5190, loss 0.000266663, acc 1
2016-09-06T19:17:29.675562: step 5191, loss 0.000831132, acc 1
2016-09-06T19:17:30.405937: step 5192, loss 0.00209749, acc 1
2016-09-06T19:17:31.079347: step 5193, loss 0.00871182, acc 1
2016-09-06T19:17:31.781500: step 5194, loss 0.0242954, acc 0.98
2016-09-06T19:17:32.448849: step 5195, loss 0.0541299, acc 0.96
2016-09-06T19:17:33.131092: step 5196, loss 0.0319672, acc 1
2016-09-06T19:17:33.827315: step 5197, loss 0.0301004, acc 0.98
2016-09-06T19:17:34.519659: step 5198, loss 0.00590793, acc 1
2016-09-06T19:17:35.196175: step 5199, loss 0.0800031, acc 0.96
2016-09-06T19:17:35.888594: step 5200, loss 0.00153814, acc 1

Evaluation:
2016-09-06T19:17:39.056537: step 5200, loss 1.73352, acc 0.757036

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-5200

2016-09-06T19:17:40.834236: step 5201, loss 0.0208239, acc 0.98
2016-09-06T19:17:41.538125: step 5202, loss 0.00210396, acc 1
2016-09-06T19:17:42.252635: step 5203, loss 0.000265268, acc 1
2016-09-06T19:17:42.942591: step 5204, loss 0.0321378, acc 0.98
2016-09-06T19:17:43.618626: step 5205, loss 0.0197035, acc 1
2016-09-06T19:17:44.282540: step 5206, loss 0.000497253, acc 1
2016-09-06T19:17:44.987319: step 5207, loss 0.0175193, acc 0.98
2016-09-06T19:17:45.663163: step 5208, loss 0.0458103, acc 0.96
2016-09-06T19:17:46.352249: step 5209, loss 0.00180096, acc 1
2016-09-06T19:17:47.043031: step 5210, loss 0.00304411, acc 1
2016-09-06T19:17:47.759968: step 5211, loss 0.00686608, acc 1
2016-09-06T19:17:48.449950: step 5212, loss 0.000227692, acc 1
2016-09-06T19:17:49.118437: step 5213, loss 0.0287363, acc 0.98
2016-09-06T19:17:49.840458: step 5214, loss 0.0286662, acc 0.98
2016-09-06T19:17:50.557959: step 5215, loss 0.00606406, acc 1
2016-09-06T19:17:51.258115: step 5216, loss 0.0368386, acc 0.98
2016-09-06T19:17:51.930594: step 5217, loss 0.010903, acc 1
2016-09-06T19:17:52.620144: step 5218, loss 0.0474941, acc 0.98
2016-09-06T19:17:53.323077: step 5219, loss 0.000836903, acc 1
2016-09-06T19:17:53.981118: step 5220, loss 0.0182986, acc 1
2016-09-06T19:17:54.654906: step 5221, loss 0.000265965, acc 1
2016-09-06T19:17:55.346396: step 5222, loss 0.0400342, acc 1
2016-09-06T19:17:56.020871: step 5223, loss 0.0268983, acc 0.98
2016-09-06T19:17:56.710843: step 5224, loss 0.08752, acc 0.98
2016-09-06T19:17:57.392347: step 5225, loss 0.0241517, acc 0.98
2016-09-06T19:17:58.099163: step 5226, loss 0.013669, acc 1
2016-09-06T19:17:58.793422: step 5227, loss 0.00170769, acc 1
2016-09-06T19:17:59.503214: step 5228, loss 0.051422, acc 0.98
2016-09-06T19:18:00.233917: step 5229, loss 0.0100484, acc 1
2016-09-06T19:18:00.939818: step 5230, loss 0.0348317, acc 1
2016-09-06T19:18:01.636321: step 5231, loss 0.00266416, acc 1
2016-09-06T19:18:02.296741: step 5232, loss 0.00359782, acc 1
2016-09-06T19:18:02.994249: step 5233, loss 0.022861, acc 1
2016-09-06T19:18:03.658726: step 5234, loss 0.0350291, acc 0.98
2016-09-06T19:18:04.342265: step 5235, loss 0.0301888, acc 0.98
2016-09-06T19:18:05.026318: step 5236, loss 0.00113609, acc 1
2016-09-06T19:18:05.715811: step 5237, loss 0.000808233, acc 1
2016-09-06T19:18:06.383460: step 5238, loss 0.000192666, acc 1
2016-09-06T19:18:07.056983: step 5239, loss 0.0550981, acc 0.98
2016-09-06T19:18:07.766516: step 5240, loss 0.00637182, acc 1
2016-09-06T19:18:08.434073: step 5241, loss 0.269597, acc 0.96
2016-09-06T19:18:09.108214: step 5242, loss 0.00672232, acc 1
2016-09-06T19:18:09.806061: step 5243, loss 0.0267674, acc 0.98
2016-09-06T19:18:10.489674: step 5244, loss 0.0470999, acc 0.98
2016-09-06T19:18:11.189215: step 5245, loss 0.00372852, acc 1
2016-09-06T19:18:11.877150: step 5246, loss 0.0336129, acc 0.98
2016-09-06T19:18:12.586338: step 5247, loss 0.00142517, acc 1
2016-09-06T19:18:13.255691: step 5248, loss 0.0168434, acc 1
2016-09-06T19:18:13.945132: step 5249, loss 0.000168032, acc 1
2016-09-06T19:18:14.638025: step 5250, loss 0.028199, acc 0.98
2016-09-06T19:18:15.325491: step 5251, loss 8.93956e-05, acc 1
2016-09-06T19:18:15.997271: step 5252, loss 0.0207076, acc 0.98
2016-09-06T19:18:16.684825: step 5253, loss 0.035166, acc 1
2016-09-06T19:18:17.388121: step 5254, loss 0.00807005, acc 1
2016-09-06T19:18:18.061042: step 5255, loss 0.149696, acc 0.94
2016-09-06T19:18:18.730402: step 5256, loss 0.0660347, acc 0.96
2016-09-06T19:18:19.408456: step 5257, loss 0.0457579, acc 0.98
2016-09-06T19:18:20.079471: step 5258, loss 0.0124339, acc 1
2016-09-06T19:18:20.786649: step 5259, loss 0.0036624, acc 1
2016-09-06T19:18:21.481417: step 5260, loss 0.0235062, acc 0.98
2016-09-06T19:18:22.197330: step 5261, loss 0.113622, acc 0.94
2016-09-06T19:18:22.886322: step 5262, loss 0.117562, acc 0.96
2016-09-06T19:18:23.574075: step 5263, loss 0.0236001, acc 1
2016-09-06T19:18:24.274510: step 5264, loss 0.0550111, acc 0.98
2016-09-06T19:18:24.967793: step 5265, loss 0.0822041, acc 0.96
2016-09-06T19:18:25.683819: step 5266, loss 0.0253782, acc 1
2016-09-06T19:18:26.333061: step 5267, loss 0.0187359, acc 1
2016-09-06T19:18:27.035484: step 5268, loss 0.00698982, acc 1
2016-09-06T19:18:27.719375: step 5269, loss 0.0203402, acc 0.98
2016-09-06T19:18:28.393702: step 5270, loss 0.00218455, acc 1
2016-09-06T19:18:29.075881: step 5271, loss 0.0503661, acc 0.98
2016-09-06T19:18:29.780549: step 5272, loss 0.059854, acc 0.98
2016-09-06T19:18:30.488980: step 5273, loss 0.0572976, acc 0.94
2016-09-06T19:18:31.165743: step 5274, loss 0.0102388, acc 1
2016-09-06T19:18:31.867087: step 5275, loss 0.00136608, acc 1
2016-09-06T19:18:32.590719: step 5276, loss 0.0366621, acc 1
2016-09-06T19:18:33.263571: step 5277, loss 0.0836501, acc 0.96
2016-09-06T19:18:33.942933: step 5278, loss 0.09086, acc 0.98
2016-09-06T19:18:34.627922: step 5279, loss 0.0194331, acc 1
2016-09-06T19:18:35.317141: step 5280, loss 0.0347792, acc 0.98
2016-09-06T19:18:35.987573: step 5281, loss 0.026869, acc 0.98
2016-09-06T19:18:36.689791: step 5282, loss 0.0351324, acc 0.98
2016-09-06T19:18:37.383524: step 5283, loss 0.0390329, acc 0.98
2016-09-06T19:18:38.066379: step 5284, loss 0.0248292, acc 1
2016-09-06T19:18:38.755841: step 5285, loss 0.0179615, acc 1
2016-09-06T19:18:39.481989: step 5286, loss 0.0421517, acc 0.98
2016-09-06T19:18:40.202965: step 5287, loss 0.0874309, acc 0.96
2016-09-06T19:18:40.869699: step 5288, loss 0.018966, acc 1
2016-09-06T19:18:41.558306: step 5289, loss 0.0239262, acc 1
2016-09-06T19:18:42.230968: step 5290, loss 0.0211766, acc 1
2016-09-06T19:18:42.924787: step 5291, loss 0.00529848, acc 1
2016-09-06T19:18:43.607196: step 5292, loss 0.0164653, acc 0.98
2016-09-06T19:18:44.292285: step 5293, loss 0.0215782, acc 1
2016-09-06T19:18:45.006751: step 5294, loss 0.0318497, acc 0.98
2016-09-06T19:18:45.666059: step 5295, loss 0.0597208, acc 0.98
2016-09-06T19:18:46.363427: step 5296, loss 0.0459287, acc 0.98
2016-09-06T19:18:47.072842: step 5297, loss 0.0242007, acc 0.98
2016-09-06T19:18:47.777485: step 5298, loss 0.0263608, acc 0.98
2016-09-06T19:18:48.468395: step 5299, loss 0.032937, acc 0.98
2016-09-06T19:18:49.152271: step 5300, loss 0.0458975, acc 0.96

Evaluation:
2016-09-06T19:18:52.316660: step 5300, loss 1.74527, acc 0.752345

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-5300

2016-09-06T19:18:54.093441: step 5301, loss 0.0407733, acc 0.98
2016-09-06T19:18:54.797301: step 5302, loss 0.0237865, acc 0.98
2016-09-06T19:18:55.484231: step 5303, loss 0.0215743, acc 0.98
2016-09-06T19:18:56.156563: step 5304, loss 0.0413046, acc 0.96
2016-09-06T19:18:56.866967: step 5305, loss 0.0188208, acc 1
2016-09-06T19:18:57.549120: step 5306, loss 0.00691435, acc 1
2016-09-06T19:18:58.257371: step 5307, loss 0.0395513, acc 0.98
2016-09-06T19:18:58.931440: step 5308, loss 0.021537, acc 0.98
2016-09-06T19:18:59.601942: step 5309, loss 0.00264218, acc 1
2016-09-06T19:19:00.298750: step 5310, loss 0.00608641, acc 1
2016-09-06T19:19:00.989163: step 5311, loss 0.0239258, acc 0.98
2016-09-06T19:19:01.662285: step 5312, loss 0.0136486, acc 1
2016-09-06T19:19:02.366815: step 5313, loss 0.0127227, acc 1
2016-09-06T19:19:03.073560: step 5314, loss 0.0303784, acc 0.98
2016-09-06T19:19:03.748085: step 5315, loss 0.0620774, acc 0.98
2016-09-06T19:19:04.425847: step 5316, loss 0.0133173, acc 1
2016-09-06T19:19:05.111036: step 5317, loss 0.028554, acc 0.98
2016-09-06T19:19:05.805752: step 5318, loss 0.0158415, acc 1
2016-09-06T19:19:06.501788: step 5319, loss 0.0140131, acc 1
2016-09-06T19:19:07.165075: step 5320, loss 0.0164053, acc 0.98
2016-09-06T19:19:07.852575: step 5321, loss 0.006402, acc 1
2016-09-06T19:19:08.517189: step 5322, loss 0.00245539, acc 1
2016-09-06T19:19:09.222383: step 5323, loss 0.0227051, acc 1
2016-09-06T19:19:09.912988: step 5324, loss 0.104012, acc 0.96
2016-09-06T19:19:10.578685: step 5325, loss 0.00591866, acc 1
2016-09-06T19:19:11.249413: step 5326, loss 0.00625938, acc 1
2016-09-06T19:19:11.951029: step 5327, loss 0.0385388, acc 0.98
2016-09-06T19:19:12.685677: step 5328, loss 0.0217775, acc 0.98
2016-09-06T19:19:13.366885: step 5329, loss 0.00920278, acc 1
2016-09-06T19:19:14.051835: step 5330, loss 0.0216822, acc 0.98
2016-09-06T19:19:14.711969: step 5331, loss 0.00172204, acc 1
2016-09-06T19:19:15.384565: step 5332, loss 0.0119969, acc 1
2016-09-06T19:19:16.074192: step 5333, loss 0.0837444, acc 0.98
2016-09-06T19:19:16.768375: step 5334, loss 2.38522e-05, acc 1
2016-09-06T19:19:17.474851: step 5335, loss 0.0304207, acc 0.98
2016-09-06T19:19:18.130245: step 5336, loss 0.0277518, acc 0.98
2016-09-06T19:19:18.823974: step 5337, loss 0.0272455, acc 0.98
2016-09-06T19:19:19.517506: step 5338, loss 0.0482812, acc 1
2016-09-06T19:19:20.221572: step 5339, loss 0.00236135, acc 1
2016-09-06T19:19:20.912820: step 5340, loss 0.00255206, acc 1
2016-09-06T19:19:21.597626: step 5341, loss 0.0229311, acc 0.98
2016-09-06T19:19:22.293322: step 5342, loss 0.0218696, acc 0.98
2016-09-06T19:19:22.963023: step 5343, loss 0.00966463, acc 1
2016-09-06T19:19:23.657541: step 5344, loss 0.0403223, acc 0.96
2016-09-06T19:19:24.344702: step 5345, loss 0.000481681, acc 1
2016-09-06T19:19:25.015967: step 5346, loss 0.0234798, acc 1
2016-09-06T19:19:25.700753: step 5347, loss 0.101563, acc 0.98
2016-09-06T19:19:26.394197: step 5348, loss 0.0081005, acc 1
2016-09-06T19:19:27.115935: step 5349, loss 0.000961921, acc 1
2016-09-06T19:19:27.793437: step 5350, loss 0.0327372, acc 0.96
2016-09-06T19:19:28.505334: step 5351, loss 0.0232302, acc 0.98
2016-09-06T19:19:29.204300: step 5352, loss 0.0479196, acc 0.96
2016-09-06T19:19:29.906076: step 5353, loss 0.00371722, acc 1
2016-09-06T19:19:30.595267: step 5354, loss 0.000372967, acc 1
2016-09-06T19:19:31.248755: step 5355, loss 0.00415151, acc 1
2016-09-06T19:19:31.942546: step 5356, loss 0.169421, acc 0.96
2016-09-06T19:19:32.614546: step 5357, loss 0.0277772, acc 0.98
2016-09-06T19:19:33.300283: step 5358, loss 0.0276076, acc 0.98
2016-09-06T19:19:33.992233: step 5359, loss 5.3599e-05, acc 1
2016-09-06T19:19:34.692627: step 5360, loss 0.0114915, acc 1
2016-09-06T19:19:35.403850: step 5361, loss 0.000838685, acc 1
2016-09-06T19:19:36.089665: step 5362, loss 0.0548825, acc 0.96
2016-09-06T19:19:36.816424: step 5363, loss 0.0308288, acc 0.98
2016-09-06T19:19:37.515317: step 5364, loss 0.00638462, acc 1
2016-09-06T19:19:38.213644: step 5365, loss 0.0192664, acc 1
2016-09-06T19:19:38.909908: step 5366, loss 7.99506e-05, acc 1
2016-09-06T19:19:39.597004: step 5367, loss 0.000207863, acc 1
2016-09-06T19:19:40.302312: step 5368, loss 0.00684156, acc 1
2016-09-06T19:19:40.990291: step 5369, loss 0.0887187, acc 0.98
2016-09-06T19:19:41.675631: step 5370, loss 0.229742, acc 0.92
2016-09-06T19:19:42.336964: step 5371, loss 0.0157481, acc 0.98
2016-09-06T19:19:43.021739: step 5372, loss 0.0125302, acc 1
2016-09-06T19:19:43.718585: step 5373, loss 0.0876741, acc 0.94
2016-09-06T19:19:44.399398: step 5374, loss 0.0296847, acc 0.98
2016-09-06T19:19:45.106964: step 5375, loss 0.0361692, acc 0.98
2016-09-06T19:19:45.710745: step 5376, loss 0.0208453, acc 0.977273
2016-09-06T19:19:46.408281: step 5377, loss 0.046471, acc 0.96
2016-09-06T19:19:47.087008: step 5378, loss 0.003848, acc 1
2016-09-06T19:19:47.755095: step 5379, loss 0.000900244, acc 1
2016-09-06T19:19:48.468517: step 5380, loss 0.0184606, acc 1
2016-09-06T19:19:49.149328: step 5381, loss 0.00124451, acc 1
2016-09-06T19:19:49.852394: step 5382, loss 0.0345293, acc 0.98
2016-09-06T19:19:50.531597: step 5383, loss 0.0114066, acc 1
2016-09-06T19:19:51.266108: step 5384, loss 0.0218104, acc 1
2016-09-06T19:19:51.958218: step 5385, loss 0.0184733, acc 1
2016-09-06T19:19:52.648598: step 5386, loss 0.0534136, acc 0.98
2016-09-06T19:19:53.339771: step 5387, loss 0.094055, acc 0.94
2016-09-06T19:19:54.034170: step 5388, loss 0.0230544, acc 0.98
2016-09-06T19:19:54.757834: step 5389, loss 0.037674, acc 0.98
2016-09-06T19:19:55.430389: step 5390, loss 0.0319915, acc 0.98
2016-09-06T19:19:56.113709: step 5391, loss 0.0443005, acc 0.98
2016-09-06T19:19:56.801448: step 5392, loss 0.0162771, acc 1
2016-09-06T19:19:57.492054: step 5393, loss 0.0160393, acc 0.98
2016-09-06T19:19:58.176990: step 5394, loss 2.71681e-05, acc 1
2016-09-06T19:19:58.858071: step 5395, loss 0.00514635, acc 1
2016-09-06T19:19:59.566391: step 5396, loss 0.0444202, acc 0.96
2016-09-06T19:20:00.247038: step 5397, loss 0.0041694, acc 1
2016-09-06T19:20:00.904863: step 5398, loss 0.127063, acc 0.98
2016-09-06T19:20:01.588187: step 5399, loss 0.0500851, acc 0.96
2016-09-06T19:20:02.271141: step 5400, loss 0.0587664, acc 0.98

Evaluation:
2016-09-06T19:20:05.413364: step 5400, loss 1.7936, acc 0.750469

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-5400

2016-09-06T19:20:07.125609: step 5401, loss 0.00312264, acc 1
2016-09-06T19:20:07.826372: step 5402, loss 0.048034, acc 0.98
2016-09-06T19:20:08.511550: step 5403, loss 0.0249247, acc 1
2016-09-06T19:20:09.217840: step 5404, loss 0.0046028, acc 1
2016-09-06T19:20:09.893791: step 5405, loss 0.122701, acc 0.94
2016-09-06T19:20:10.564784: step 5406, loss 0.0223697, acc 1
2016-09-06T19:20:11.255903: step 5407, loss 0.0141783, acc 1
2016-09-06T19:20:11.934559: step 5408, loss 0.0447773, acc 0.98
2016-09-06T19:20:12.642672: step 5409, loss 0.0145322, acc 1
2016-09-06T19:20:13.335680: step 5410, loss 0.00492743, acc 1
2016-09-06T19:20:14.030454: step 5411, loss 0.00323378, acc 1
2016-09-06T19:20:14.719871: step 5412, loss 0.273497, acc 0.92
2016-09-06T19:20:15.417078: step 5413, loss 4.09114e-05, acc 1
2016-09-06T19:20:16.094597: step 5414, loss 0.0571497, acc 0.96
2016-09-06T19:20:16.800829: step 5415, loss 0.0278772, acc 0.98
2016-09-06T19:20:17.492825: step 5416, loss 0.0100725, acc 1
2016-09-06T19:20:18.168961: step 5417, loss 0.0282739, acc 1
2016-09-06T19:20:18.893258: step 5418, loss 0.0111402, acc 1
2016-09-06T19:20:19.582355: step 5419, loss 0.0567414, acc 0.96
2016-09-06T19:20:20.261916: step 5420, loss 0.0267467, acc 0.98
2016-09-06T19:20:20.947562: step 5421, loss 0.00637174, acc 1
2016-09-06T19:20:21.654979: step 5422, loss 0.0175844, acc 1
2016-09-06T19:20:22.347092: step 5423, loss 0.0189099, acc 0.98
2016-09-06T19:20:23.032601: step 5424, loss 0.0520002, acc 0.98
2016-09-06T19:20:23.732658: step 5425, loss 0.04649, acc 0.98
2016-09-06T19:20:24.416280: step 5426, loss 0.0140039, acc 1
2016-09-06T19:20:25.104210: step 5427, loss 0.0266316, acc 0.98
2016-09-06T19:20:25.791752: step 5428, loss 0.00039411, acc 1
2016-09-06T19:20:26.473113: step 5429, loss 0.0137763, acc 1
2016-09-06T19:20:27.183402: step 5430, loss 0.0168166, acc 1
2016-09-06T19:20:27.836900: step 5431, loss 0.08245, acc 0.98
2016-09-06T19:20:28.551510: step 5432, loss 0.00436316, acc 1
2016-09-06T19:20:29.227165: step 5433, loss 0.0510287, acc 0.98
2016-09-06T19:20:29.916750: step 5434, loss 0.0524778, acc 0.98
2016-09-06T19:20:30.605670: step 5435, loss 0.0541092, acc 0.96
2016-09-06T19:20:31.303998: step 5436, loss 0.00817688, acc 1
2016-09-06T19:20:31.974502: step 5437, loss 0.0133925, acc 1
2016-09-06T19:20:32.642851: step 5438, loss 0.0295958, acc 0.98
2016-09-06T19:20:33.353461: step 5439, loss 0.103524, acc 0.96
2016-09-06T19:20:34.015670: step 5440, loss 0.0144632, acc 0.98
2016-09-06T19:20:34.699286: step 5441, loss 0.0372804, acc 1
2016-09-06T19:20:35.399894: step 5442, loss 0.0145485, acc 1
2016-09-06T19:20:36.084871: step 5443, loss 0.0646876, acc 0.98
2016-09-06T19:20:36.777265: step 5444, loss 0.0391985, acc 0.96
2016-09-06T19:20:37.446156: step 5445, loss 0.0675784, acc 0.94
2016-09-06T19:20:38.155510: step 5446, loss 0.122802, acc 0.96
2016-09-06T19:20:38.849105: step 5447, loss 0.0337227, acc 0.98
2016-09-06T19:20:39.528656: step 5448, loss 0.0135684, acc 1
2016-09-06T19:20:40.208508: step 5449, loss 0.0583237, acc 0.96
2016-09-06T19:20:40.924979: step 5450, loss 0.0622407, acc 0.96
2016-09-06T19:20:41.627257: step 5451, loss 0.0354216, acc 0.98
2016-09-06T19:20:42.310656: step 5452, loss 0.00902848, acc 1
2016-09-06T19:20:43.017695: step 5453, loss 0.101622, acc 0.94
2016-09-06T19:20:43.694256: step 5454, loss 0.0223301, acc 1
2016-09-06T19:20:44.380936: step 5455, loss 0.0382538, acc 0.96
2016-09-06T19:20:45.073288: step 5456, loss 0.073028, acc 0.96
2016-09-06T19:20:45.761102: step 5457, loss 0.0292174, acc 0.98
2016-09-06T19:20:46.465438: step 5458, loss 0.0386392, acc 0.98
2016-09-06T19:20:47.144735: step 5459, loss 0.0359439, acc 0.96
2016-09-06T19:20:47.815623: step 5460, loss 0.0254984, acc 1
2016-09-06T19:20:48.491593: step 5461, loss 0.0284479, acc 0.98
2016-09-06T19:20:49.169186: step 5462, loss 0.0013623, acc 1
2016-09-06T19:20:49.840524: step 5463, loss 0.0310946, acc 1
2016-09-06T19:20:50.527688: step 5464, loss 0.0155668, acc 1
2016-09-06T19:20:51.248561: step 5465, loss 0.0065948, acc 1
2016-09-06T19:20:51.929258: step 5466, loss 0.0450893, acc 0.98
2016-09-06T19:20:52.651794: step 5467, loss 0.0267812, acc 0.98
2016-09-06T19:20:53.337457: step 5468, loss 0.00231273, acc 1
2016-09-06T19:20:54.029225: step 5469, loss 0.0564195, acc 0.98
2016-09-06T19:20:54.703808: step 5470, loss 0.0158344, acc 1
2016-09-06T19:20:55.383407: step 5471, loss 0.077277, acc 0.96
2016-09-06T19:20:56.077735: step 5472, loss 0.0136193, acc 1
2016-09-06T19:20:56.766767: step 5473, loss 0.0466609, acc 0.98
2016-09-06T19:20:57.452723: step 5474, loss 0.0124552, acc 1
2016-09-06T19:20:58.141880: step 5475, loss 0.0207037, acc 0.98
2016-09-06T19:20:58.809252: step 5476, loss 0.0406437, acc 0.98
2016-09-06T19:20:59.508283: step 5477, loss 0.0260104, acc 0.98
2016-09-06T19:21:00.213408: step 5478, loss 0.000123395, acc 1
2016-09-06T19:21:00.925962: step 5479, loss 0.0428636, acc 0.98
2016-09-06T19:21:01.582484: step 5480, loss 0.018104, acc 1
2016-09-06T19:21:02.283701: step 5481, loss 0.00496634, acc 1
2016-09-06T19:21:02.972696: step 5482, loss 0.0889593, acc 0.96
2016-09-06T19:21:03.676844: step 5483, loss 0.00555582, acc 1
2016-09-06T19:21:04.360043: step 5484, loss 0.0275232, acc 0.98
2016-09-06T19:21:05.034380: step 5485, loss 0.0940195, acc 0.96
2016-09-06T19:21:05.767703: step 5486, loss 0.0478753, acc 1
2016-09-06T19:21:06.474582: step 5487, loss 0.016187, acc 1
2016-09-06T19:21:07.166432: step 5488, loss 0.00664387, acc 1
2016-09-06T19:21:07.846378: step 5489, loss 0.0182881, acc 1
2016-09-06T19:21:08.521442: step 5490, loss 0.00756509, acc 1
2016-09-06T19:21:09.203840: step 5491, loss 0.0163235, acc 1
2016-09-06T19:21:09.871851: step 5492, loss 0.000499128, acc 1
2016-09-06T19:21:10.564646: step 5493, loss 0.00584759, acc 1
2016-09-06T19:21:11.228761: step 5494, loss 0.00651413, acc 1
2016-09-06T19:21:11.897369: step 5495, loss 0.027986, acc 1
2016-09-06T19:21:12.587587: step 5496, loss 0.0240763, acc 1
2016-09-06T19:21:13.259666: step 5497, loss 0.0180412, acc 1
2016-09-06T19:21:13.951208: step 5498, loss 0.00738942, acc 1
2016-09-06T19:21:14.645295: step 5499, loss 0.0387695, acc 0.98
2016-09-06T19:21:15.362555: step 5500, loss 0.0460149, acc 0.96

Evaluation:
2016-09-06T19:21:18.510023: step 5500, loss 1.86503, acc 0.747655

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-5500

2016-09-06T19:21:20.178861: step 5501, loss 0.00121956, acc 1
2016-09-06T19:21:20.849332: step 5502, loss 0.0782005, acc 0.98
2016-09-06T19:21:21.541411: step 5503, loss 0.0153667, acc 1
2016-09-06T19:21:22.231101: step 5504, loss 0.0247529, acc 1
2016-09-06T19:21:22.927148: step 5505, loss 0.0372509, acc 0.98
2016-09-06T19:21:23.606331: step 5506, loss 0.0481855, acc 0.98
2016-09-06T19:21:24.285632: step 5507, loss 0.0548096, acc 0.98
2016-09-06T19:21:25.002779: step 5508, loss 0.00958568, acc 1
2016-09-06T19:21:25.702445: step 5509, loss 0.0816067, acc 0.96
2016-09-06T19:21:26.387902: step 5510, loss 0.0399519, acc 0.98
2016-09-06T19:21:27.078231: step 5511, loss 0.00367825, acc 1
2016-09-06T19:21:27.782824: step 5512, loss 0.02694, acc 0.98
2016-09-06T19:21:28.502724: step 5513, loss 0.00844673, acc 1
2016-09-06T19:21:29.181413: step 5514, loss 0.00368077, acc 1
2016-09-06T19:21:29.889774: step 5515, loss 0.00612882, acc 1
2016-09-06T19:21:30.569518: step 5516, loss 0.0749912, acc 0.96
2016-09-06T19:21:31.257210: step 5517, loss 0.044427, acc 0.98
2016-09-06T19:21:31.933246: step 5518, loss 0.000277324, acc 1
2016-09-06T19:21:32.625746: step 5519, loss 0.000788891, acc 1
2016-09-06T19:21:33.332333: step 5520, loss 0.00566721, acc 1
2016-09-06T19:21:33.987559: step 5521, loss 0.0087416, acc 1
2016-09-06T19:21:34.698409: step 5522, loss 0.00404457, acc 1
2016-09-06T19:21:35.401554: step 5523, loss 0.000689753, acc 1
2016-09-06T19:21:36.068505: step 5524, loss 0.00187591, acc 1
2016-09-06T19:21:36.770702: step 5525, loss 0.0204011, acc 0.98
2016-09-06T19:21:37.465326: step 5526, loss 0.0774084, acc 0.96
2016-09-06T19:21:38.181456: step 5527, loss 0.0405603, acc 0.98
2016-09-06T19:21:38.849989: step 5528, loss 0.0160371, acc 1
2016-09-06T19:21:39.560448: step 5529, loss 0.0121559, acc 1
2016-09-06T19:21:40.237478: step 5530, loss 0.0180478, acc 0.98
2016-09-06T19:21:40.946279: step 5531, loss 0.0117899, acc 1
2016-09-06T19:21:41.613395: step 5532, loss 6.23958e-05, acc 1
2016-09-06T19:21:42.293726: step 5533, loss 0.0112942, acc 1
2016-09-06T19:21:43.003483: step 5534, loss 0.00446773, acc 1
2016-09-06T19:21:43.695799: step 5535, loss 0.0444224, acc 0.98
2016-09-06T19:21:44.365602: step 5536, loss 0.174697, acc 0.94
2016-09-06T19:21:45.070582: step 5537, loss 0.00808513, acc 1
2016-09-06T19:21:45.782180: step 5538, loss 0.00656155, acc 1
2016-09-06T19:21:46.472428: step 5539, loss 0.063206, acc 0.98
2016-09-06T19:21:47.148087: step 5540, loss 9.03311e-06, acc 1
2016-09-06T19:21:47.852179: step 5541, loss 0.0192588, acc 1
2016-09-06T19:21:48.542767: step 5542, loss 0.00965862, acc 1
2016-09-06T19:21:49.219529: step 5543, loss 0.0269439, acc 0.98
2016-09-06T19:21:49.907374: step 5544, loss 0.0627939, acc 0.96
2016-09-06T19:21:50.586505: step 5545, loss 0.0205679, acc 0.98
2016-09-06T19:21:51.267834: step 5546, loss 0.024996, acc 0.98
2016-09-06T19:21:51.925767: step 5547, loss 0.000923547, acc 1
2016-09-06T19:21:52.625925: step 5548, loss 0.0128723, acc 1
2016-09-06T19:21:53.280362: step 5549, loss 0.0135667, acc 1
2016-09-06T19:21:53.977339: step 5550, loss 0.0305269, acc 0.98
2016-09-06T19:21:54.656387: step 5551, loss 0.0149336, acc 1
2016-09-06T19:21:55.339622: step 5552, loss 0.0138107, acc 1
2016-09-06T19:21:56.037266: step 5553, loss 0.0182645, acc 1
2016-09-06T19:21:56.720828: step 5554, loss 0.0978313, acc 0.94
2016-09-06T19:21:57.428067: step 5555, loss 0.00363926, acc 1
2016-09-06T19:21:58.083857: step 5556, loss 0.0280866, acc 0.98
2016-09-06T19:21:58.759471: step 5557, loss 0.0100981, acc 1
2016-09-06T19:21:59.443933: step 5558, loss 0.0254615, acc 0.98
2016-09-06T19:22:00.142720: step 5559, loss 0.00564698, acc 1
2016-09-06T19:22:00.873552: step 5560, loss 0.0125538, acc 1
2016-09-06T19:22:01.526965: step 5561, loss 0.0286983, acc 0.98
2016-09-06T19:22:02.220540: step 5562, loss 0.00366443, acc 1
2016-09-06T19:22:02.902928: step 5563, loss 0.0595718, acc 0.96
2016-09-06T19:22:03.585280: step 5564, loss 0.00152485, acc 1
2016-09-06T19:22:04.270581: step 5565, loss 0.0108772, acc 1
2016-09-06T19:22:04.956149: step 5566, loss 0.0386601, acc 0.96
2016-09-06T19:22:05.640663: step 5567, loss 0.0560063, acc 0.98
2016-09-06T19:22:06.270023: step 5568, loss 0.0275182, acc 0.977273
2016-09-06T19:22:06.986973: step 5569, loss 0.0188042, acc 0.98
2016-09-06T19:22:07.662563: step 5570, loss 0.00356385, acc 1
2016-09-06T19:22:08.359937: step 5571, loss 0.0211171, acc 1
2016-09-06T19:22:09.064859: step 5572, loss 0.00230512, acc 1
2016-09-06T19:22:09.769779: step 5573, loss 0.000154799, acc 1
2016-09-06T19:22:10.461926: step 5574, loss 0.00649012, acc 1
2016-09-06T19:22:11.144513: step 5575, loss 0.0177986, acc 0.98
2016-09-06T19:22:11.854884: step 5576, loss 0.00173963, acc 1
2016-09-06T19:22:12.533871: step 5577, loss 0.0411828, acc 0.98
2016-09-06T19:22:13.231748: step 5578, loss 0.0690815, acc 0.96
2016-09-06T19:22:13.904876: step 5579, loss 0.0398057, acc 0.98
2016-09-06T19:22:14.616953: step 5580, loss 0.0426363, acc 0.98
2016-09-06T19:22:15.316759: step 5581, loss 0.0181616, acc 1
2016-09-06T19:22:15.998875: step 5582, loss 0.00245761, acc 1
2016-09-06T19:22:16.717748: step 5583, loss 0.0176593, acc 1
2016-09-06T19:22:17.397345: step 5584, loss 0.000450331, acc 1
2016-09-06T19:22:18.097289: step 5585, loss 0.0329282, acc 0.98
2016-09-06T19:22:18.799819: step 5586, loss 0.000395889, acc 1
2016-09-06T19:22:19.473728: step 5587, loss 0.000567263, acc 1
2016-09-06T19:22:20.154023: step 5588, loss 0.0175558, acc 0.98
2016-09-06T19:22:20.820529: step 5589, loss 0.00252986, acc 1
2016-09-06T19:22:21.526196: step 5590, loss 0.0141761, acc 1
2016-09-06T19:22:22.208852: step 5591, loss 0.0369518, acc 0.98
2016-09-06T19:22:22.915685: step 5592, loss 0.0479862, acc 0.98
2016-09-06T19:22:23.596425: step 5593, loss 0.0203731, acc 0.98
2016-09-06T19:22:24.302699: step 5594, loss 0.0242929, acc 0.98
2016-09-06T19:22:24.987042: step 5595, loss 0.0185655, acc 1
2016-09-06T19:22:25.649270: step 5596, loss 0.00391946, acc 1
2016-09-06T19:22:26.362485: step 5597, loss 0.00345318, acc 1
2016-09-06T19:22:27.054440: step 5598, loss 0.000929539, acc 1
2016-09-06T19:22:27.723317: step 5599, loss 0.000845972, acc 1
2016-09-06T19:22:28.400006: step 5600, loss 0.0144758, acc 1

Evaluation:
2016-09-06T19:22:31.552165: step 5600, loss 1.99637, acc 0.757036

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-5600

2016-09-06T19:22:33.282563: step 5601, loss 0.0107112, acc 1
2016-09-06T19:22:33.963960: step 5602, loss 0.01533, acc 1
2016-09-06T19:22:34.660851: step 5603, loss 0.0237621, acc 1
2016-09-06T19:22:35.324675: step 5604, loss 0.0228833, acc 0.98
2016-09-06T19:22:36.038582: step 5605, loss 0.00102956, acc 1
2016-09-06T19:22:36.751957: step 5606, loss 0.121129, acc 0.96
2016-09-06T19:22:37.450657: step 5607, loss 0.0358084, acc 0.98
2016-09-06T19:22:38.163923: step 5608, loss 0.03745, acc 1
2016-09-06T19:22:38.848861: step 5609, loss 0.0363538, acc 0.98
2016-09-06T19:22:39.578193: step 5610, loss 0.0061809, acc 1
2016-09-06T19:22:40.255494: step 5611, loss 0.034047, acc 0.98
2016-09-06T19:22:40.949985: step 5612, loss 0.0182396, acc 1
2016-09-06T19:22:41.631563: step 5613, loss 0.0245081, acc 1
2016-09-06T19:22:42.312045: step 5614, loss 0.0102994, acc 1
2016-09-06T19:22:42.990223: step 5615, loss 8.51249e-05, acc 1
2016-09-06T19:22:43.637515: step 5616, loss 0.0384797, acc 0.96
2016-09-06T19:22:44.339584: step 5617, loss 0.00677271, acc 1
2016-09-06T19:22:45.010869: step 5618, loss 0.025859, acc 0.98
2016-09-06T19:22:45.706082: step 5619, loss 0.0177358, acc 1
2016-09-06T19:22:46.389418: step 5620, loss 0.000932053, acc 1
2016-09-06T19:22:47.101512: step 5621, loss 0.0251851, acc 0.98
2016-09-06T19:22:47.788316: step 5622, loss 0.00323362, acc 1
2016-09-06T19:22:48.443070: step 5623, loss 0.066198, acc 0.96
2016-09-06T19:22:49.145473: step 5624, loss 0.00347792, acc 1
2016-09-06T19:22:49.814455: step 5625, loss 0.0449581, acc 0.98
2016-09-06T19:22:50.489689: step 5626, loss 0.0377504, acc 1
2016-09-06T19:22:51.174485: step 5627, loss 0.0274881, acc 0.98
2016-09-06T19:22:51.871966: step 5628, loss 0.00960521, acc 1
2016-09-06T19:22:52.569855: step 5629, loss 0.0015404, acc 1
2016-09-06T19:22:53.246379: step 5630, loss 0.0383388, acc 0.98
2016-09-06T19:22:53.965013: step 5631, loss 0.0160249, acc 1
2016-09-06T19:22:54.638340: step 5632, loss 0.000199152, acc 1
2016-09-06T19:22:55.319915: step 5633, loss 0.00114741, acc 1
2016-09-06T19:22:56.010995: step 5634, loss 0.000187186, acc 1
2016-09-06T19:22:56.696309: step 5635, loss 0.0060889, acc 1
2016-09-06T19:22:57.397722: step 5636, loss 0.0325021, acc 0.98
2016-09-06T19:22:58.071473: step 5637, loss 0.0219329, acc 1
2016-09-06T19:22:58.764559: step 5638, loss 0.000922785, acc 1
2016-09-06T19:22:59.446060: step 5639, loss 0.0362878, acc 0.98
2016-09-06T19:23:00.112379: step 5640, loss 0.000102477, acc 1
2016-09-06T19:23:00.833078: step 5641, loss 0.00496262, acc 1
2016-09-06T19:23:01.529119: step 5642, loss 0.0496909, acc 0.98
2016-09-06T19:23:02.232736: step 5643, loss 0.0176555, acc 1
2016-09-06T19:23:02.893934: step 5644, loss 0.000275369, acc 1
2016-09-06T19:23:03.594580: step 5645, loss 0.0307706, acc 1
2016-09-06T19:23:04.278976: step 5646, loss 0.022634, acc 0.98
2016-09-06T19:23:04.959461: step 5647, loss 0.00879939, acc 1
2016-09-06T19:23:05.641149: step 5648, loss 0.0183586, acc 1
2016-09-06T19:23:06.340568: step 5649, loss 0.00126544, acc 1
2016-09-06T19:23:07.032203: step 5650, loss 0.018803, acc 0.98
2016-09-06T19:23:07.690974: step 5651, loss 0.00212273, acc 1
2016-09-06T19:23:08.379391: step 5652, loss 0.000150431, acc 1
2016-09-06T19:23:09.057512: step 5653, loss 0.0277633, acc 0.98
2016-09-06T19:23:09.749033: step 5654, loss 0.00322291, acc 1
2016-09-06T19:23:10.435738: step 5655, loss 0.0367331, acc 0.98
2016-09-06T19:23:11.121009: step 5656, loss 0.000665766, acc 1
2016-09-06T19:23:11.826789: step 5657, loss 0.0520499, acc 0.98
2016-09-06T19:23:12.495748: step 5658, loss 0.0435044, acc 0.98
2016-09-06T19:23:13.205589: step 5659, loss 0.0020276, acc 1
2016-09-06T19:23:13.895272: step 5660, loss 0.0128767, acc 1
2016-09-06T19:23:14.579889: step 5661, loss 7.79098e-05, acc 1
2016-09-06T19:23:15.268315: step 5662, loss 0.00241666, acc 1
2016-09-06T19:23:15.951487: step 5663, loss 0.0706367, acc 0.94
2016-09-06T19:23:16.647801: step 5664, loss 0.00283497, acc 1
2016-09-06T19:23:17.312445: step 5665, loss 0.0648675, acc 0.98
2016-09-06T19:23:18.035580: step 5666, loss 0.0532692, acc 0.94
2016-09-06T19:23:18.720015: step 5667, loss 0.0285947, acc 0.98
2016-09-06T19:23:19.449899: step 5668, loss 0.0166284, acc 1
2016-09-06T19:23:20.147357: step 5669, loss 2.50093e-05, acc 1
2016-09-06T19:23:20.847218: step 5670, loss 0.00191675, acc 1
2016-09-06T19:23:21.552952: step 5671, loss 0.0548208, acc 0.96
2016-09-06T19:23:22.228147: step 5672, loss 0.0178166, acc 0.98
2016-09-06T19:23:22.934182: step 5673, loss 0.0406707, acc 0.98
2016-09-06T19:23:23.621707: step 5674, loss 0.0221778, acc 0.98
2016-09-06T19:23:24.293807: step 5675, loss 0.0172523, acc 0.98
2016-09-06T19:23:24.982068: step 5676, loss 0.0225352, acc 0.98
2016-09-06T19:23:25.663640: step 5677, loss 0.0241105, acc 1
2016-09-06T19:23:26.351677: step 5678, loss 0.00133333, acc 1
2016-09-06T19:23:27.031246: step 5679, loss 0.0227497, acc 0.98
2016-09-06T19:23:27.731347: step 5680, loss 0.0669652, acc 0.96
2016-09-06T19:23:28.430102: step 5681, loss 0.0191483, acc 1
2016-09-06T19:23:29.115606: step 5682, loss 0.0104063, acc 1
2016-09-06T19:23:29.823603: step 5683, loss 0.0537172, acc 0.96
2016-09-06T19:23:30.500431: step 5684, loss 0.0032077, acc 1
2016-09-06T19:23:31.217317: step 5685, loss 0.000594732, acc 1
2016-09-06T19:23:31.888501: step 5686, loss 0.0490565, acc 0.98
2016-09-06T19:23:32.568338: step 5687, loss 0.00857541, acc 1
2016-09-06T19:23:33.253585: step 5688, loss 0.0268922, acc 0.98
2016-09-06T19:23:33.938400: step 5689, loss 0.0116539, acc 1
2016-09-06T19:23:34.619785: step 5690, loss 0.12742, acc 0.98
2016-09-06T19:23:35.321827: step 5691, loss 0.0115205, acc 1
2016-09-06T19:23:36.046488: step 5692, loss 0.0429485, acc 0.98
2016-09-06T19:23:36.744783: step 5693, loss 0.0244161, acc 1
2016-09-06T19:23:37.411403: step 5694, loss 0.0171363, acc 0.98
2016-09-06T19:23:38.094733: step 5695, loss 0.00154902, acc 1
2016-09-06T19:23:38.776009: step 5696, loss 0.0573504, acc 0.96
2016-09-06T19:23:39.494306: step 5697, loss 0.0127021, acc 1
2016-09-06T19:23:40.164373: step 5698, loss 0.00101549, acc 1
2016-09-06T19:23:40.871483: step 5699, loss 0.0714571, acc 0.96
2016-09-06T19:23:41.540089: step 5700, loss 0.0192255, acc 1

Evaluation:
2016-09-06T19:23:44.637083: step 5700, loss 2.28198, acc 0.756098

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473156846/checkpoints/model-5700

2016-09-06T19:23:46.406914: step 5701, loss 0.0666653, acc 0.98
2016-09-06T19:23:47.088213: step 5702, loss 0.00563532, acc 1
2016-09-06T19:23:47.767964: step 5703, loss 0.000862778, acc 1
2016-09-06T19:23:48.453814: step 5704, loss 0.103167, acc 0.98
2016-09-06T19:23:49.155519: step 5705, loss 0.0117673, acc 1
2016-09-06T19:23:49.821044: step 5706, loss 0.0221769, acc 0.98
2016-09-06T19:23:50.525210: step 5707, loss 0.00487206, acc 1
2016-09-06T19:23:51.216852: step 5708, loss 0.0345411, acc 0.98
2016-09-06T19:23:51.898648: step 5709, loss 0.0181501, acc 1
2016-09-06T19:23:52.578872: step 5710, loss 0.000871286, acc 1
2016-09-06T19:23:53.279230: step 5711, loss 0.0446462, acc 0.98
2016-09-06T19:23:53.975430: step 5712, loss 0.0150133, acc 1
2016-09-06T19:23:54.637838: step 5713, loss 0.0191705, acc 1
2016-09-06T19:23:55.337021: step 5714, loss 0.00715844, acc 1
2016-09-06T19:23:56.025107: step 5715, loss 0.0128984, acc 1
2016-09-06T19:23:56.698521: step 5716, loss 0.0419697, acc 0.96
2016-09-06T19:23:57.383717: step 5717, loss 0.0384613, acc 0.98
2016-09-06T19:23:58.049498: step 5718, loss 0.0178583, acc 0.98
2016-09-06T19:23:58.744118: step 5719, loss 0.0193642, acc 0.98
2016-09-06T19:23:59.471220: step 5720, loss 0.000133254, acc 1
2016-09-06T19:24:00.195403: step 5721, loss 0.00697597, acc 1
2016-09-06T19:24:00.909909: step 5722, loss 0.0158825, acc 1
2016-09-06T19:24:01.581850: step 5723, loss 0.0259346, acc 0.98
2016-09-06T19:24:02.284916: step 5724, loss 0.0216245, acc 0.98
2016-09-06T19:24:02.975997: step 5725, loss 0.0113783, acc 1
2016-09-06T19:24:03.687307: step 5726, loss 0.0741069, acc 0.96
2016-09-06T19:24:04.369756: step 5727, loss 0.050303, acc 0.96
2016-09-06T19:24:05.043224: step 5728, loss 0.00743859, acc 1
2016-09-06T19:24:05.739587: step 5729, loss 0.00133402, acc 1
2016-09-06T19:24:06.426212: step 5730, loss 0.0176525, acc 0.98
2016-09-06T19:24:07.103072: step 5731, loss 0.0105201, acc 1
2016-09-06T19:24:07.772319: step 5732, loss 0.0226046, acc 0.98
2016-09-06T19:24:08.478035: step 5733, loss 0.00190313, acc 1
2016-09-06T19:24:09.149406: step 5734, loss 0.0485001, acc 0.96
2016-09-06T19:24:09.871090: step 5735, loss 0.000147569, acc 1
2016-09-06T19:24:10.545589: step 5736, loss 0.0368439, acc 0.98
2016-09-06T19:24:11.221918: step 5737, loss 0.0227324, acc 0.98
2016-09-06T19:24:11.925428: step 5738, loss 0.0128828, acc 1
2016-09-06T19:24:12.607205: step 5739, loss 0.0100485, acc 1
2016-09-06T19:24:13.316339: step 5740, loss 0.00691214, acc 1
2016-09-06T19:24:13.988192: step 5741, loss 0.00716352, acc 1
2016-09-06T19:24:14.683290: step 5742, loss 0.0727351, acc 0.98
2016-09-06T19:24:15.373518: step 5743, loss 0.0321914, acc 0.98
2016-09-06T19:24:16.073903: step 5744, loss 0.0317778, acc 0.96
2016-09-06T19:24:16.755062: step 5745, loss 0.0096209, acc 1
2016-09-06T19:24:17.429215: step 5746, loss 0.0742944, acc 0.96
2016-09-06T19:24:18.127045: step 5747, loss 0.0204329, acc 0.98
2016-09-06T19:24:18.802391: step 5748, loss 0.0394615, acc 0.98
2016-09-06T19:24:19.491554: step 5749, loss 0.00832246, acc 1
2016-09-06T19:24:20.165455: step 5750, loss 0.0108575, acc 1
2016-09-06T19:24:20.843032: step 5751, loss 0.000996044, acc 1
2016-09-06T19:24:21.531490: step 5752, loss 0.0409343, acc 0.98
2016-09-06T19:24:22.232907: step 5753, loss 0.0167802, acc 0.98
2016-09-06T19:24:22.934210: step 5754, loss 0.0135902, acc 1
2016-09-06T19:24:23.634949: step 5755, loss 0.0931924, acc 0.96
2016-09-06T19:24:24.334487: step 5756, loss 0.0220187, acc 0.98
2016-09-06T19:24:25.019330: step 5757, loss 0.0347888, acc 1
2016-09-06T19:24:25.699284: step 5758, loss 0.0158556, acc 1
2016-09-06T19:24:26.391172: step 5759, loss 0.00734406, acc 1
2016-09-06T19:24:27.015344: step 5760, loss 0.05732, acc 0.977273

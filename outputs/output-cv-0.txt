WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f94ad595e90>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.
WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f94ad595e50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.

Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=50
CHECKPOINT_EVERY=100
CV_INDEX=0
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=300
EVALUATE_EVERY=100
FILTER_SIZES=3,4,5
HIDDEN_DIM=300
L2_REG_LAMBDA=0.15
LOG_DEVICE_PLACEMENT=False
NUM_EPOCHS=100
NUM_FILTERS=100
WORD2VEC=GoogleNews-vectors-negative300.bin

Loading data...
Vocabulary Size: 18758
Train/Dev split: 9594/1066
Writing to /home/cil/lstm-context-embeddings/runs/1473089861

Load word2vec file GoogleNews-vectors-negative300.bin

2016-09-05T23:38:00.763792: step 1, loss 0.693147, acc 0.58
2016-09-05T23:38:01.545093: step 2, loss 0.714802, acc 0.44
2016-09-05T23:38:02.345443: step 3, loss 0.694734, acc 0.5
2016-09-05T23:38:03.194617: step 4, loss 0.676513, acc 0.58
2016-09-05T23:38:03.996130: step 5, loss 0.684654, acc 0.56
2016-09-05T23:38:04.812352: step 6, loss 0.700545, acc 0.52
2016-09-05T23:38:05.622022: step 7, loss 0.727826, acc 0.46
2016-09-05T23:38:06.437398: step 8, loss 0.688128, acc 0.5
2016-09-05T23:38:07.258109: step 9, loss 0.695601, acc 0.5
2016-09-05T23:38:08.075193: step 10, loss 0.702639, acc 0.48
2016-09-05T23:38:08.911183: step 11, loss 0.689235, acc 0.52
2016-09-05T23:38:09.687454: step 12, loss 0.656439, acc 0.58
2016-09-05T23:38:10.481019: step 13, loss 0.699943, acc 0.56
2016-09-05T23:38:11.287172: step 14, loss 0.760875, acc 0.46
2016-09-05T23:38:12.101762: step 15, loss 0.656139, acc 0.6
2016-09-05T23:38:12.908679: step 16, loss 0.705867, acc 0.48
2016-09-05T23:38:13.701405: step 17, loss 0.689575, acc 0.5
2016-09-05T23:38:14.473369: step 18, loss 0.686302, acc 0.58
2016-09-05T23:38:15.282319: step 19, loss 0.681486, acc 0.54
2016-09-05T23:38:16.065624: step 20, loss 0.709692, acc 0.56
2016-09-05T23:38:16.874540: step 21, loss 0.685, acc 0.6
2016-09-05T23:38:17.689562: step 22, loss 0.668578, acc 0.56
2016-09-05T23:38:18.487047: step 23, loss 0.690521, acc 0.58
2016-09-05T23:38:19.302561: step 24, loss 0.685112, acc 0.52
2016-09-05T23:38:20.118064: step 25, loss 0.669727, acc 0.64
2016-09-05T23:38:20.891945: step 26, loss 0.645695, acc 0.56
2016-09-05T23:38:21.687176: step 27, loss 0.682079, acc 0.54
2016-09-05T23:38:22.532270: step 28, loss 0.753856, acc 0.5
2016-09-05T23:38:23.335345: step 29, loss 0.67193, acc 0.64
2016-09-05T23:38:24.137412: step 30, loss 0.659637, acc 0.54
2016-09-05T23:38:24.970837: step 31, loss 0.742585, acc 0.42
2016-09-05T23:38:25.793870: step 32, loss 0.630458, acc 0.58
2016-09-05T23:38:26.573681: step 33, loss 0.65759, acc 0.6
2016-09-05T23:38:27.389885: step 34, loss 0.670635, acc 0.56
2016-09-05T23:38:28.194814: step 35, loss 0.657535, acc 0.64
2016-09-05T23:38:28.966284: step 36, loss 0.619548, acc 0.7
2016-09-05T23:38:29.789301: step 37, loss 0.641976, acc 0.58
2016-09-05T23:38:30.566289: step 38, loss 0.694675, acc 0.52
2016-09-05T23:38:31.388878: step 39, loss 0.730139, acc 0.36
2016-09-05T23:38:32.212450: step 40, loss 0.661577, acc 0.62
2016-09-05T23:38:33.063352: step 41, loss 0.620466, acc 0.68
2016-09-05T23:38:33.863026: step 42, loss 0.607695, acc 0.76
2016-09-05T23:38:34.665601: step 43, loss 0.561574, acc 0.72
2016-09-05T23:38:35.476073: step 44, loss 0.56698, acc 0.68
2016-09-05T23:38:36.250929: step 45, loss 0.716052, acc 0.64
2016-09-05T23:38:37.067463: step 46, loss 0.580881, acc 0.74
2016-09-05T23:38:37.890876: step 47, loss 0.597668, acc 0.68
2016-09-05T23:38:38.669406: step 48, loss 0.748852, acc 0.5
2016-09-05T23:38:39.470784: step 49, loss 0.543407, acc 0.76
2016-09-05T23:38:40.285579: step 50, loss 0.725357, acc 0.52
2016-09-05T23:38:41.115448: step 51, loss 0.555928, acc 0.7
2016-09-05T23:38:41.899095: step 52, loss 0.573135, acc 0.72
2016-09-05T23:38:42.700231: step 53, loss 0.58118, acc 0.76
2016-09-05T23:38:43.542337: step 54, loss 0.670801, acc 0.62
2016-09-05T23:38:44.336744: step 55, loss 0.562349, acc 0.68
2016-09-05T23:38:45.184626: step 56, loss 0.544382, acc 0.76
2016-09-05T23:38:45.993361: step 57, loss 0.615532, acc 0.72
2016-09-05T23:38:46.774885: step 58, loss 0.558227, acc 0.72
2016-09-05T23:38:47.585104: step 59, loss 0.590448, acc 0.68
2016-09-05T23:38:48.393979: step 60, loss 0.533199, acc 0.76
2016-09-05T23:38:49.240809: step 61, loss 0.546761, acc 0.76
2016-09-05T23:38:50.080146: step 62, loss 0.615047, acc 0.7
2016-09-05T23:38:50.880932: step 63, loss 0.587591, acc 0.7
2016-09-05T23:38:51.696943: step 64, loss 0.553212, acc 0.72
2016-09-05T23:38:52.546474: step 65, loss 0.569694, acc 0.7
2016-09-05T23:38:53.357770: step 66, loss 0.628593, acc 0.76
2016-09-05T23:38:54.159381: step 67, loss 0.542847, acc 0.7
2016-09-05T23:38:54.978295: step 68, loss 0.529963, acc 0.76
2016-09-05T23:38:55.784422: step 69, loss 0.58487, acc 0.68
2016-09-05T23:38:56.592085: step 70, loss 0.55261, acc 0.74
2016-09-05T23:38:57.404628: step 71, loss 0.520058, acc 0.78
2016-09-05T23:38:58.206607: step 72, loss 0.486957, acc 0.76
2016-09-05T23:38:59.020139: step 73, loss 0.601082, acc 0.72
2016-09-05T23:38:59.859959: step 74, loss 0.563107, acc 0.7
2016-09-05T23:39:00.732284: step 75, loss 0.738544, acc 0.6
2016-09-05T23:39:01.599077: step 76, loss 0.510456, acc 0.78
2016-09-05T23:39:02.421882: step 77, loss 0.541292, acc 0.72
2016-09-05T23:39:03.239220: step 78, loss 0.589117, acc 0.76
2016-09-05T23:39:04.048171: step 79, loss 0.609937, acc 0.64
2016-09-05T23:39:04.863094: step 80, loss 0.549358, acc 0.72
2016-09-05T23:39:05.677481: step 81, loss 0.546855, acc 0.7
2016-09-05T23:39:06.467709: step 82, loss 0.48206, acc 0.72
2016-09-05T23:39:07.252293: step 83, loss 0.671195, acc 0.62
2016-09-05T23:39:08.058177: step 84, loss 0.512553, acc 0.76
2016-09-05T23:39:08.890357: step 85, loss 0.383873, acc 0.92
2016-09-05T23:39:09.694859: step 86, loss 0.438623, acc 0.76
2016-09-05T23:39:10.482684: step 87, loss 0.581982, acc 0.7
2016-09-05T23:39:11.283792: step 88, loss 0.338183, acc 0.86
2016-09-05T23:39:12.098214: step 89, loss 0.652393, acc 0.66
2016-09-05T23:39:12.899511: step 90, loss 0.577341, acc 0.7
2016-09-05T23:39:13.725396: step 91, loss 0.567823, acc 0.64
2016-09-05T23:39:14.522999: step 92, loss 0.650767, acc 0.7
2016-09-05T23:39:15.325317: step 93, loss 0.489423, acc 0.7
2016-09-05T23:39:16.127586: step 94, loss 0.471721, acc 0.72
2016-09-05T23:39:16.943513: step 95, loss 0.584629, acc 0.7
2016-09-05T23:39:17.767818: step 96, loss 0.546731, acc 0.68
2016-09-05T23:39:18.569491: step 97, loss 0.50616, acc 0.78
2016-09-05T23:39:19.409519: step 98, loss 0.567942, acc 0.66
2016-09-05T23:39:20.225512: step 99, loss 0.557075, acc 0.7
2016-09-05T23:39:21.015222: step 100, loss 0.642241, acc 0.62

Evaluation:
2016-09-05T23:39:24.738985: step 100, loss 0.549099, acc 0.739212

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-100

2016-09-05T23:39:26.727571: step 101, loss 0.447522, acc 0.84
2016-09-05T23:39:27.546956: step 102, loss 0.562683, acc 0.78
2016-09-05T23:39:28.358217: step 103, loss 0.559144, acc 0.72
2016-09-05T23:39:29.153338: step 104, loss 0.443629, acc 0.8
2016-09-05T23:39:29.969693: step 105, loss 0.496387, acc 0.78
2016-09-05T23:39:30.768900: step 106, loss 0.470192, acc 0.82
2016-09-05T23:39:31.586928: step 107, loss 0.569147, acc 0.8
2016-09-05T23:39:32.414585: step 108, loss 0.515104, acc 0.82
2016-09-05T23:39:33.216012: step 109, loss 0.684994, acc 0.66
2016-09-05T23:39:34.013307: step 110, loss 0.453762, acc 0.78
2016-09-05T23:39:34.861399: step 111, loss 0.522229, acc 0.8
2016-09-05T23:39:35.641360: step 112, loss 0.490443, acc 0.76
2016-09-05T23:39:36.455104: step 113, loss 0.556858, acc 0.74
2016-09-05T23:39:37.287251: step 114, loss 0.537734, acc 0.68
2016-09-05T23:39:38.081475: step 115, loss 0.526645, acc 0.7
2016-09-05T23:39:38.896317: step 116, loss 0.560525, acc 0.68
2016-09-05T23:39:39.728172: step 117, loss 0.529119, acc 0.74
2016-09-05T23:39:40.549470: step 118, loss 0.468286, acc 0.84
2016-09-05T23:39:41.361424: step 119, loss 0.478024, acc 0.76
2016-09-05T23:39:42.172067: step 120, loss 0.592771, acc 0.66
2016-09-05T23:39:42.970287: step 121, loss 0.591676, acc 0.62
2016-09-05T23:39:43.773872: step 122, loss 0.522568, acc 0.7
2016-09-05T23:39:44.573879: step 123, loss 0.617348, acc 0.7
2016-09-05T23:39:45.369541: step 124, loss 0.553291, acc 0.66
2016-09-05T23:39:46.188748: step 125, loss 0.493702, acc 0.76
2016-09-05T23:39:47.007790: step 126, loss 0.431836, acc 0.82
2016-09-05T23:39:47.822147: step 127, loss 0.384288, acc 0.84
2016-09-05T23:39:48.616463: step 128, loss 0.606657, acc 0.72
2016-09-05T23:39:49.453531: step 129, loss 0.522348, acc 0.78
2016-09-05T23:39:50.280110: step 130, loss 0.523248, acc 0.72
2016-09-05T23:39:51.100746: step 131, loss 0.655361, acc 0.68
2016-09-05T23:39:51.925735: step 132, loss 0.588511, acc 0.8
2016-09-05T23:39:52.747224: step 133, loss 0.576935, acc 0.72
2016-09-05T23:39:53.549084: step 134, loss 0.439724, acc 0.82
2016-09-05T23:39:54.373195: step 135, loss 0.500851, acc 0.78
2016-09-05T23:39:55.160857: step 136, loss 0.378045, acc 0.82
2016-09-05T23:39:55.964112: step 137, loss 0.409988, acc 0.78
2016-09-05T23:39:56.819039: step 138, loss 0.485114, acc 0.74
2016-09-05T23:39:57.647248: step 139, loss 0.513611, acc 0.74
2016-09-05T23:39:58.449308: step 140, loss 0.609107, acc 0.64
2016-09-05T23:39:59.280271: step 141, loss 0.7066, acc 0.72
2016-09-05T23:40:00.129988: step 142, loss 0.633672, acc 0.66
2016-09-05T23:40:00.982573: step 143, loss 0.437749, acc 0.8
2016-09-05T23:40:01.782346: step 144, loss 0.483711, acc 0.84
2016-09-05T23:40:02.581656: step 145, loss 0.623152, acc 0.6
2016-09-05T23:40:03.358399: step 146, loss 0.427483, acc 0.8
2016-09-05T23:40:04.168825: step 147, loss 0.538985, acc 0.76
2016-09-05T23:40:05.009167: step 148, loss 0.437639, acc 0.84
2016-09-05T23:40:05.802008: step 149, loss 0.573088, acc 0.74
2016-09-05T23:40:06.609273: step 150, loss 0.43544, acc 0.72
2016-09-05T23:40:07.397478: step 151, loss 0.606267, acc 0.66
2016-09-05T23:40:08.182733: step 152, loss 0.532526, acc 0.72
2016-09-05T23:40:08.964701: step 153, loss 0.550167, acc 0.76
2016-09-05T23:40:09.798552: step 154, loss 0.5094, acc 0.72
2016-09-05T23:40:10.577385: step 155, loss 0.425992, acc 0.8
2016-09-05T23:40:11.391785: step 156, loss 0.427097, acc 0.82
2016-09-05T23:40:12.206386: step 157, loss 0.482153, acc 0.74
2016-09-05T23:40:13.014847: step 158, loss 0.575582, acc 0.7
2016-09-05T23:40:13.828904: step 159, loss 0.495614, acc 0.74
2016-09-05T23:40:14.656753: step 160, loss 0.428349, acc 0.9
2016-09-05T23:40:15.427297: step 161, loss 0.543754, acc 0.76
2016-09-05T23:40:16.282642: step 162, loss 0.49867, acc 0.78
2016-09-05T23:40:17.094114: step 163, loss 0.594128, acc 0.76
2016-09-05T23:40:17.881252: step 164, loss 0.70453, acc 0.58
2016-09-05T23:40:18.660206: step 165, loss 0.433625, acc 0.86
2016-09-05T23:40:19.513138: step 166, loss 0.361535, acc 0.84
2016-09-05T23:40:20.271364: step 167, loss 0.528019, acc 0.78
2016-09-05T23:40:21.057635: step 168, loss 0.428223, acc 0.76
2016-09-05T23:40:21.872076: step 169, loss 0.425411, acc 0.74
2016-09-05T23:40:22.709438: step 170, loss 0.427094, acc 0.8
2016-09-05T23:40:23.519003: step 171, loss 0.496371, acc 0.7
2016-09-05T23:40:24.331258: step 172, loss 0.402671, acc 0.8
2016-09-05T23:40:25.135349: step 173, loss 0.493711, acc 0.72
2016-09-05T23:40:25.938088: step 174, loss 0.612053, acc 0.62
2016-09-05T23:40:26.756341: step 175, loss 0.496905, acc 0.8
2016-09-05T23:40:27.525419: step 176, loss 0.516305, acc 0.78
2016-09-05T23:40:28.319154: step 177, loss 0.517516, acc 0.78
2016-09-05T23:40:29.151992: step 178, loss 0.532997, acc 0.68
2016-09-05T23:40:29.927079: step 179, loss 0.627994, acc 0.62
2016-09-05T23:40:30.736647: step 180, loss 0.478048, acc 0.78
2016-09-05T23:40:31.536667: step 181, loss 0.478932, acc 0.76
2016-09-05T23:40:32.331700: step 182, loss 0.679928, acc 0.56
2016-09-05T23:40:33.125538: step 183, loss 0.539747, acc 0.74
2016-09-05T23:40:33.945406: step 184, loss 0.434835, acc 0.82
2016-09-05T23:40:34.753070: step 185, loss 0.41594, acc 0.84
2016-09-05T23:40:35.551448: step 186, loss 0.479637, acc 0.78
2016-09-05T23:40:36.371424: step 187, loss 0.543167, acc 0.76
2016-09-05T23:40:37.168428: step 188, loss 0.46367, acc 0.78
2016-09-05T23:40:37.969652: step 189, loss 0.468604, acc 0.72
2016-09-05T23:40:38.813417: step 190, loss 0.348157, acc 0.84
2016-09-05T23:40:39.602763: step 191, loss 0.52643, acc 0.78
2016-09-05T23:40:40.378113: step 192, loss 0.416707, acc 0.840909
2016-09-05T23:40:41.197402: step 193, loss 0.270934, acc 0.9
2016-09-05T23:40:42.006710: step 194, loss 0.235121, acc 0.9
2016-09-05T23:40:42.827549: step 195, loss 0.392994, acc 0.82
2016-09-05T23:40:43.646853: step 196, loss 0.357672, acc 0.84
2016-09-05T23:40:44.449445: step 197, loss 0.32112, acc 0.84
2016-09-05T23:40:45.270691: step 198, loss 0.366552, acc 0.88
2016-09-05T23:40:46.089233: step 199, loss 0.301467, acc 0.92
2016-09-05T23:40:46.883241: step 200, loss 0.599709, acc 0.76

Evaluation:
2016-09-05T23:40:50.590430: step 200, loss 0.476764, acc 0.794559

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-200

2016-09-05T23:40:52.449461: step 201, loss 0.440363, acc 0.8
2016-09-05T23:40:53.270430: step 202, loss 0.43731, acc 0.86
2016-09-05T23:40:54.104795: step 203, loss 0.553473, acc 0.7
2016-09-05T23:40:54.939749: step 204, loss 0.336034, acc 0.88
2016-09-05T23:40:55.765428: step 205, loss 0.348508, acc 0.82
2016-09-05T23:40:56.556086: step 206, loss 0.424835, acc 0.82
2016-09-05T23:40:57.378105: step 207, loss 0.305689, acc 0.88
2016-09-05T23:40:58.198904: step 208, loss 0.393442, acc 0.84
2016-09-05T23:40:58.997206: step 209, loss 0.262371, acc 0.9
2016-09-05T23:40:59.825966: step 210, loss 0.53138, acc 0.8
2016-09-05T23:41:00.723873: step 211, loss 0.300163, acc 0.88
2016-09-05T23:41:01.501444: step 212, loss 0.290069, acc 0.92
2016-09-05T23:41:02.300921: step 213, loss 0.297165, acc 0.9
2016-09-05T23:41:03.133136: step 214, loss 0.594656, acc 0.78
2016-09-05T23:41:03.900050: step 215, loss 0.376992, acc 0.86
2016-09-05T23:41:04.710964: step 216, loss 0.320041, acc 0.86
2016-09-05T23:41:05.539226: step 217, loss 0.262795, acc 0.9
2016-09-05T23:41:06.313372: step 218, loss 0.378278, acc 0.82
2016-09-05T23:41:07.174887: step 219, loss 0.389892, acc 0.84
2016-09-05T23:41:08.011955: step 220, loss 0.18227, acc 0.96
2016-09-05T23:41:08.822164: step 221, loss 0.327682, acc 0.84
2016-09-05T23:41:09.626743: step 222, loss 0.396669, acc 0.84
2016-09-05T23:41:10.454937: step 223, loss 0.458578, acc 0.82
2016-09-05T23:41:11.257477: step 224, loss 0.349869, acc 0.9
2016-09-05T23:41:12.090953: step 225, loss 0.371666, acc 0.86
2016-09-05T23:41:12.912987: step 226, loss 0.373265, acc 0.86
2016-09-05T23:41:13.721303: step 227, loss 0.378731, acc 0.8
2016-09-05T23:41:14.513462: step 228, loss 0.424361, acc 0.78
2016-09-05T23:41:15.347526: step 229, loss 0.405627, acc 0.84
2016-09-05T23:41:16.197728: step 230, loss 0.2908, acc 0.88
2016-09-05T23:41:17.015527: step 231, loss 0.337269, acc 0.8
2016-09-05T23:41:17.835193: step 232, loss 0.446699, acc 0.8
2016-09-05T23:41:18.658622: step 233, loss 0.31257, acc 0.84
2016-09-05T23:41:19.477344: step 234, loss 0.155946, acc 0.94
2016-09-05T23:41:20.291790: step 235, loss 0.479509, acc 0.7
2016-09-05T23:41:21.084170: step 236, loss 0.460435, acc 0.8
2016-09-05T23:41:21.895491: step 237, loss 0.35565, acc 0.82
2016-09-05T23:41:22.726142: step 238, loss 0.39718, acc 0.8
2016-09-05T23:41:23.534347: step 239, loss 0.33998, acc 0.86
2016-09-05T23:41:24.387200: step 240, loss 0.283579, acc 0.86
2016-09-05T23:41:25.199072: step 241, loss 0.37005, acc 0.84
2016-09-05T23:41:25.997996: step 242, loss 0.273429, acc 0.88
2016-09-05T23:41:26.801143: step 243, loss 0.358, acc 0.82
2016-09-05T23:41:27.615352: step 244, loss 0.312678, acc 0.88
2016-09-05T23:41:28.423521: step 245, loss 0.417, acc 0.78
2016-09-05T23:41:29.210942: step 246, loss 0.30905, acc 0.86
2016-09-05T23:41:30.046559: step 247, loss 0.300295, acc 0.84
2016-09-05T23:41:30.837404: step 248, loss 0.251647, acc 0.84
2016-09-05T23:41:31.622401: step 249, loss 0.47538, acc 0.82
2016-09-05T23:41:32.443670: step 250, loss 0.299096, acc 0.88
2016-09-05T23:41:33.219282: step 251, loss 0.470413, acc 0.76
2016-09-05T23:41:34.012712: step 252, loss 0.439424, acc 0.82
2016-09-05T23:41:34.848672: step 253, loss 0.411683, acc 0.86
2016-09-05T23:41:35.679525: step 254, loss 0.295789, acc 0.9
2016-09-05T23:41:36.496196: step 255, loss 0.302762, acc 0.88
2016-09-05T23:41:37.313448: step 256, loss 0.296898, acc 0.86
2016-09-05T23:41:38.134850: step 257, loss 0.345616, acc 0.84
2016-09-05T23:41:38.940167: step 258, loss 0.464844, acc 0.8
2016-09-05T23:41:39.747746: step 259, loss 0.341615, acc 0.88
2016-09-05T23:41:40.542029: step 260, loss 0.447722, acc 0.8
2016-09-05T23:41:41.339492: step 261, loss 0.362408, acc 0.84
2016-09-05T23:41:42.143255: step 262, loss 0.350872, acc 0.76
2016-09-05T23:41:42.987272: step 263, loss 0.521554, acc 0.74
2016-09-05T23:41:43.767597: step 264, loss 0.340909, acc 0.84
2016-09-05T23:41:44.553679: step 265, loss 0.447525, acc 0.82
2016-09-05T23:41:45.326574: step 266, loss 0.410893, acc 0.84
2016-09-05T23:41:46.130246: step 267, loss 0.267682, acc 0.88
2016-09-05T23:41:46.993497: step 268, loss 0.185453, acc 0.98
2016-09-05T23:41:47.817401: step 269, loss 0.278833, acc 0.9
2016-09-05T23:41:48.626559: step 270, loss 0.410106, acc 0.72
2016-09-05T23:41:49.417039: step 271, loss 0.272429, acc 0.84
2016-09-05T23:41:50.225557: step 272, loss 0.288984, acc 0.88
2016-09-05T23:41:51.054161: step 273, loss 0.251482, acc 0.9
2016-09-05T23:41:51.892238: step 274, loss 0.441448, acc 0.78
2016-09-05T23:41:52.741397: step 275, loss 0.5139, acc 0.82
2016-09-05T23:41:53.549214: step 276, loss 0.331066, acc 0.8
2016-09-05T23:41:54.354151: step 277, loss 0.252283, acc 0.92
2016-09-05T23:41:55.164947: step 278, loss 0.206645, acc 0.9
2016-09-05T23:41:55.969652: step 279, loss 0.48566, acc 0.8
2016-09-05T23:41:56.763779: step 280, loss 0.51809, acc 0.8
2016-09-05T23:41:57.620091: step 281, loss 0.415203, acc 0.86
2016-09-05T23:41:58.422202: step 282, loss 0.522818, acc 0.82
2016-09-05T23:41:59.222070: step 283, loss 0.33186, acc 0.88
2016-09-05T23:42:00.034645: step 284, loss 0.299977, acc 0.86
2016-09-05T23:42:00.861310: step 285, loss 0.440267, acc 0.82
2016-09-05T23:42:01.690525: step 286, loss 0.279254, acc 0.88
2016-09-05T23:42:02.524592: step 287, loss 0.230481, acc 0.9
2016-09-05T23:42:03.325106: step 288, loss 0.278466, acc 0.88
2016-09-05T23:42:04.111675: step 289, loss 0.421886, acc 0.84
2016-09-05T23:42:04.932759: step 290, loss 0.421169, acc 0.82
2016-09-05T23:42:05.746640: step 291, loss 0.420359, acc 0.8
2016-09-05T23:42:06.544750: step 292, loss 0.456659, acc 0.78
2016-09-05T23:42:07.404711: step 293, loss 0.293765, acc 0.88
2016-09-05T23:42:08.217412: step 294, loss 0.478207, acc 0.76
2016-09-05T23:42:09.002541: step 295, loss 0.359229, acc 0.86
2016-09-05T23:42:09.853922: step 296, loss 0.25182, acc 0.92
2016-09-05T23:42:10.672277: step 297, loss 0.549429, acc 0.72
2016-09-05T23:42:11.472643: step 298, loss 0.353088, acc 0.78
2016-09-05T23:42:12.291002: step 299, loss 0.40979, acc 0.84
2016-09-05T23:42:13.103922: step 300, loss 0.433301, acc 0.82

Evaluation:
2016-09-05T23:42:16.833082: step 300, loss 0.469253, acc 0.776735

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-300

2016-09-05T23:42:18.635827: step 301, loss 0.44937, acc 0.78
2016-09-05T23:42:19.458587: step 302, loss 0.365461, acc 0.82
2016-09-05T23:42:20.254031: step 303, loss 0.303253, acc 0.88
2016-09-05T23:42:21.079618: step 304, loss 0.468741, acc 0.74
2016-09-05T23:42:21.900289: step 305, loss 0.469256, acc 0.76
2016-09-05T23:42:22.718380: step 306, loss 0.269229, acc 0.92
2016-09-05T23:42:23.499571: step 307, loss 0.337783, acc 0.86
2016-09-05T23:42:24.335736: step 308, loss 0.456248, acc 0.82
2016-09-05T23:42:25.147141: step 309, loss 0.227346, acc 0.96
2016-09-05T23:42:25.948573: step 310, loss 0.354328, acc 0.82
2016-09-05T23:42:26.772465: step 311, loss 0.448212, acc 0.84
2016-09-05T23:42:27.581984: step 312, loss 0.430508, acc 0.78
2016-09-05T23:42:28.397420: step 313, loss 0.410064, acc 0.8
2016-09-05T23:42:29.229118: step 314, loss 0.357559, acc 0.88
2016-09-05T23:42:30.028725: step 315, loss 0.300367, acc 0.84
2016-09-05T23:42:30.821659: step 316, loss 0.253454, acc 0.88
2016-09-05T23:42:31.636694: step 317, loss 0.308571, acc 0.9
2016-09-05T23:42:32.440932: step 318, loss 0.508625, acc 0.8
2016-09-05T23:42:33.261368: step 319, loss 0.354033, acc 0.84
2016-09-05T23:42:34.095098: step 320, loss 0.479883, acc 0.84
2016-09-05T23:42:34.888515: step 321, loss 0.347956, acc 0.82
2016-09-05T23:42:35.686522: step 322, loss 0.417998, acc 0.76
2016-09-05T23:42:36.500175: step 323, loss 0.259094, acc 0.9
2016-09-05T23:42:37.309078: step 324, loss 0.284806, acc 0.9
2016-09-05T23:42:38.128558: step 325, loss 0.326613, acc 0.86
2016-09-05T23:42:38.966259: step 326, loss 0.249083, acc 0.84
2016-09-05T23:42:39.780956: step 327, loss 0.468583, acc 0.78
2016-09-05T23:42:40.556225: step 328, loss 0.384936, acc 0.86
2016-09-05T23:42:41.365148: step 329, loss 0.389831, acc 0.82
2016-09-05T23:42:42.169971: step 330, loss 0.383558, acc 0.82
2016-09-05T23:42:42.976016: step 331, loss 0.30419, acc 0.88
2016-09-05T23:42:43.803285: step 332, loss 0.260496, acc 0.9
2016-09-05T23:42:44.598399: step 333, loss 0.378123, acc 0.82
2016-09-05T23:42:45.386577: step 334, loss 0.280388, acc 0.88
2016-09-05T23:42:46.224635: step 335, loss 0.297379, acc 0.9
2016-09-05T23:42:47.038970: step 336, loss 0.351991, acc 0.8
2016-09-05T23:42:47.850127: step 337, loss 0.315935, acc 0.84
2016-09-05T23:42:48.669599: step 338, loss 0.416853, acc 0.82
2016-09-05T23:42:49.476458: step 339, loss 0.451046, acc 0.78
2016-09-05T23:42:50.262957: step 340, loss 0.295586, acc 0.82
2016-09-05T23:42:51.093304: step 341, loss 0.242559, acc 0.9
2016-09-05T23:42:51.901337: step 342, loss 0.380814, acc 0.8
2016-09-05T23:42:52.678789: step 343, loss 0.329383, acc 0.86
2016-09-05T23:42:53.488400: step 344, loss 0.279204, acc 0.94
2016-09-05T23:42:54.328215: step 345, loss 0.321112, acc 0.88
2016-09-05T23:42:55.109331: step 346, loss 0.370463, acc 0.8
2016-09-05T23:42:55.904100: step 347, loss 0.401349, acc 0.86
2016-09-05T23:42:56.725320: step 348, loss 0.319448, acc 0.86
2016-09-05T23:42:57.506056: step 349, loss 0.510502, acc 0.76
2016-09-05T23:42:58.337366: step 350, loss 0.271804, acc 0.86
2016-09-05T23:42:59.171676: step 351, loss 0.378506, acc 0.78
2016-09-05T23:42:59.979471: step 352, loss 0.354026, acc 0.8
2016-09-05T23:43:00.823978: step 353, loss 0.430392, acc 0.78
2016-09-05T23:43:01.621605: step 354, loss 0.414605, acc 0.82
2016-09-05T23:43:02.404917: step 355, loss 0.34675, acc 0.88
2016-09-05T23:43:03.201741: step 356, loss 0.405685, acc 0.8
2016-09-05T23:43:04.045159: step 357, loss 0.370758, acc 0.86
2016-09-05T23:43:04.823372: step 358, loss 0.355825, acc 0.88
2016-09-05T23:43:05.652894: step 359, loss 0.336063, acc 0.84
2016-09-05T23:43:06.460374: step 360, loss 0.400782, acc 0.78
2016-09-05T23:43:07.254062: step 361, loss 0.309545, acc 0.88
2016-09-05T23:43:08.061576: step 362, loss 0.424434, acc 0.78
2016-09-05T23:43:08.873194: step 363, loss 0.339117, acc 0.84
2016-09-05T23:43:09.631773: step 364, loss 0.350911, acc 0.8
2016-09-05T23:43:10.421910: step 365, loss 0.310042, acc 0.84
2016-09-05T23:43:11.251833: step 366, loss 0.377927, acc 0.86
2016-09-05T23:43:12.092366: step 367, loss 0.333992, acc 0.84
2016-09-05T23:43:12.900465: step 368, loss 0.277674, acc 0.88
2016-09-05T23:43:13.701542: step 369, loss 0.348908, acc 0.8
2016-09-05T23:43:14.490782: step 370, loss 0.404388, acc 0.82
2016-09-05T23:43:15.306363: step 371, loss 0.366361, acc 0.86
2016-09-05T23:43:16.113489: step 372, loss 0.57313, acc 0.8
2016-09-05T23:43:16.900321: step 373, loss 0.414132, acc 0.8
2016-09-05T23:43:17.733462: step 374, loss 0.295018, acc 0.88
2016-09-05T23:43:18.537270: step 375, loss 0.367298, acc 0.8
2016-09-05T23:43:19.318057: step 376, loss 0.390482, acc 0.84
2016-09-05T23:43:20.172071: step 377, loss 0.27219, acc 0.9
2016-09-05T23:43:21.005446: step 378, loss 0.445054, acc 0.8
2016-09-05T23:43:21.783835: step 379, loss 0.400881, acc 0.8
2016-09-05T23:43:22.572898: step 380, loss 0.388028, acc 0.8
2016-09-05T23:43:23.367631: step 381, loss 0.402777, acc 0.78
2016-09-05T23:43:24.136836: step 382, loss 0.333611, acc 0.88
2016-09-05T23:43:24.925048: step 383, loss 0.346689, acc 0.88
2016-09-05T23:43:25.684852: step 384, loss 0.567533, acc 0.75
2016-09-05T23:43:26.509685: step 385, loss 0.172155, acc 1
2016-09-05T23:43:27.355944: step 386, loss 0.280724, acc 0.88
2016-09-05T23:43:28.174008: step 387, loss 0.321044, acc 0.88
2016-09-05T23:43:28.983125: step 388, loss 0.231211, acc 0.94
2016-09-05T23:43:29.821495: step 389, loss 0.239401, acc 0.94
2016-09-05T23:43:30.640073: step 390, loss 0.317007, acc 0.84
2016-09-05T23:43:31.454870: step 391, loss 0.223759, acc 0.96
2016-09-05T23:43:32.245618: step 392, loss 0.163856, acc 0.96
2016-09-05T23:43:33.045782: step 393, loss 0.175283, acc 0.94
2016-09-05T23:43:33.807824: step 394, loss 0.152338, acc 0.94
2016-09-05T23:43:34.614039: step 395, loss 0.176657, acc 0.92
2016-09-05T23:43:35.423206: step 396, loss 0.198383, acc 0.92
2016-09-05T23:43:36.209459: step 397, loss 0.207614, acc 0.92
2016-09-05T23:43:37.026645: step 398, loss 0.217824, acc 0.9
2016-09-05T23:43:37.837758: step 399, loss 0.319562, acc 0.84
2016-09-05T23:43:38.635193: step 400, loss 0.499263, acc 0.88

Evaluation:
2016-09-05T23:43:42.358826: step 400, loss 0.641886, acc 0.767355

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-400

2016-09-05T23:43:44.280607: step 401, loss 0.317943, acc 0.9
2016-09-05T23:43:45.080408: step 402, loss 0.118979, acc 0.96
2016-09-05T23:43:45.888302: step 403, loss 0.132694, acc 0.96
2016-09-05T23:43:46.790717: step 404, loss 0.2992, acc 0.86
2016-09-05T23:43:47.612515: step 405, loss 0.321098, acc 0.88
2016-09-05T23:43:48.439046: step 406, loss 0.300631, acc 0.88
2016-09-05T23:43:49.253620: step 407, loss 0.256766, acc 0.88
2016-09-05T23:43:50.079412: step 408, loss 0.198533, acc 0.92
2016-09-05T23:43:50.842110: step 409, loss 0.116107, acc 0.94
2016-09-05T23:43:51.636796: step 410, loss 0.299189, acc 0.84
2016-09-05T23:43:52.449391: step 411, loss 0.300366, acc 0.86
2016-09-05T23:43:53.226972: step 412, loss 0.228973, acc 0.88
2016-09-05T23:43:54.022158: step 413, loss 0.220048, acc 0.94
2016-09-05T23:43:54.826538: step 414, loss 0.262244, acc 0.92
2016-09-05T23:43:55.618603: step 415, loss 0.262677, acc 0.92
2016-09-05T23:43:56.426425: step 416, loss 0.220531, acc 0.9
2016-09-05T23:43:57.239587: step 417, loss 0.263509, acc 0.9
2016-09-05T23:43:58.040446: step 418, loss 0.251253, acc 0.88
2016-09-05T23:43:58.851535: step 419, loss 0.308755, acc 0.86
2016-09-05T23:43:59.660757: step 420, loss 0.283938, acc 0.86
2016-09-05T23:44:00.483725: step 421, loss 0.144555, acc 0.96
2016-09-05T23:44:01.295513: step 422, loss 0.309502, acc 0.92
2016-09-05T23:44:02.127518: step 423, loss 0.167382, acc 0.98
2016-09-05T23:44:02.907606: step 424, loss 0.309545, acc 0.86
2016-09-05T23:44:03.710026: step 425, loss 0.228941, acc 0.9
2016-09-05T23:44:04.515234: step 426, loss 0.22402, acc 0.9
2016-09-05T23:44:05.297266: step 427, loss 0.268973, acc 0.9
2016-09-05T23:44:06.126715: step 428, loss 0.21086, acc 0.88
2016-09-05T23:44:06.983758: step 429, loss 0.311748, acc 0.86
2016-09-05T23:44:07.769492: step 430, loss 0.359785, acc 0.88
2016-09-05T23:44:08.567594: step 431, loss 0.139511, acc 0.96
2016-09-05T23:44:09.361028: step 432, loss 0.126564, acc 0.94
2016-09-05T23:44:10.155016: step 433, loss 0.224452, acc 0.9
2016-09-05T23:44:10.968422: step 434, loss 0.326374, acc 0.88
2016-09-05T23:44:11.838942: step 435, loss 0.164607, acc 0.94
2016-09-05T23:44:12.608990: step 436, loss 0.19692, acc 0.9
2016-09-05T23:44:13.388374: step 437, loss 0.291841, acc 0.86
2016-09-05T23:44:14.191215: step 438, loss 0.178859, acc 0.94
2016-09-05T23:44:14.981367: step 439, loss 0.166377, acc 0.94
2016-09-05T23:44:15.820867: step 440, loss 0.132402, acc 0.96
2016-09-05T23:44:16.663790: step 441, loss 0.127334, acc 0.94
2016-09-05T23:44:17.430476: step 442, loss 0.17943, acc 0.9
2016-09-05T23:44:18.214958: step 443, loss 0.454332, acc 0.86
2016-09-05T23:44:19.018259: step 444, loss 0.318148, acc 0.86
2016-09-05T23:44:19.800536: step 445, loss 0.289759, acc 0.86
2016-09-05T23:44:20.619457: step 446, loss 0.210325, acc 0.92
2016-09-05T23:44:21.443249: step 447, loss 0.196836, acc 0.92
2016-09-05T23:44:22.223993: step 448, loss 0.281199, acc 0.88
2016-09-05T23:44:23.079995: step 449, loss 0.127526, acc 0.94
2016-09-05T23:44:23.896237: step 450, loss 0.232776, acc 0.9
2016-09-05T23:44:24.690694: step 451, loss 0.188727, acc 0.92
2016-09-05T23:44:25.481581: step 452, loss 0.255095, acc 0.9
2016-09-05T23:44:26.305774: step 453, loss 0.267351, acc 0.92
2016-09-05T23:44:27.111567: step 454, loss 0.24619, acc 0.94
2016-09-05T23:44:27.943692: step 455, loss 0.379133, acc 0.78
2016-09-05T23:44:28.762918: step 456, loss 0.151608, acc 0.94
2016-09-05T23:44:29.544545: step 457, loss 0.207856, acc 0.9
2016-09-05T23:44:30.335188: step 458, loss 0.218235, acc 0.92
2016-09-05T23:44:31.134415: step 459, loss 0.351133, acc 0.92
2016-09-05T23:44:31.920621: step 460, loss 0.265263, acc 0.88
2016-09-05T23:44:32.714311: step 461, loss 0.277158, acc 0.88
2016-09-05T23:44:33.535579: step 462, loss 0.277705, acc 0.88
2016-09-05T23:44:34.326993: step 463, loss 0.162325, acc 0.94
2016-09-05T23:44:35.124965: step 464, loss 0.202528, acc 0.94
2016-09-05T23:44:35.940738: step 465, loss 0.268689, acc 0.88
2016-09-05T23:44:36.718424: step 466, loss 0.28067, acc 0.9
2016-09-05T23:44:37.553201: step 467, loss 0.276151, acc 0.86
2016-09-05T23:44:38.354885: step 468, loss 0.263445, acc 0.92
2016-09-05T23:44:39.148591: step 469, loss 0.203493, acc 0.9
2016-09-05T23:44:39.979882: step 470, loss 0.306027, acc 0.86
2016-09-05T23:44:40.833476: step 471, loss 0.215282, acc 0.92
2016-09-05T23:44:41.598979: step 472, loss 0.272043, acc 0.88
2016-09-05T23:44:42.429485: step 473, loss 0.297747, acc 0.86
2016-09-05T23:44:43.254358: step 474, loss 0.329749, acc 0.86
2016-09-05T23:44:44.031400: step 475, loss 0.274614, acc 0.88
2016-09-05T23:44:44.826613: step 476, loss 0.162581, acc 0.94
2016-09-05T23:44:45.639994: step 477, loss 0.332509, acc 0.9
2016-09-05T23:44:46.417560: step 478, loss 0.250113, acc 0.88
2016-09-05T23:44:47.243045: step 479, loss 0.257474, acc 0.88
2016-09-05T23:44:48.045443: step 480, loss 0.199971, acc 0.92
2016-09-05T23:44:48.843492: step 481, loss 0.13973, acc 0.94
2016-09-05T23:44:49.657898: step 482, loss 0.2209, acc 0.9
2016-09-05T23:44:50.477693: step 483, loss 0.418401, acc 0.8
2016-09-05T23:44:51.254772: step 484, loss 0.223812, acc 0.92
2016-09-05T23:44:52.047997: step 485, loss 0.207553, acc 0.94
2016-09-05T23:44:52.848911: step 486, loss 0.41782, acc 0.78
2016-09-05T23:44:53.654488: step 487, loss 0.284118, acc 0.86
2016-09-05T23:44:54.446993: step 488, loss 0.333732, acc 0.84
2016-09-05T23:44:55.270633: step 489, loss 0.432164, acc 0.82
2016-09-05T23:44:56.060104: step 490, loss 0.0937776, acc 1
2016-09-05T23:44:56.895881: step 491, loss 0.177291, acc 0.94
2016-09-05T23:44:57.720267: step 492, loss 0.21991, acc 0.9
2016-09-05T23:44:58.506093: step 493, loss 0.244521, acc 0.84
2016-09-05T23:44:59.313830: step 494, loss 0.34547, acc 0.84
2016-09-05T23:45:00.138533: step 495, loss 0.154379, acc 0.92
2016-09-05T23:45:00.960223: step 496, loss 0.162669, acc 0.94
2016-09-05T23:45:01.753492: step 497, loss 0.268304, acc 0.88
2016-09-05T23:45:02.538485: step 498, loss 0.158983, acc 0.94
2016-09-05T23:45:03.359009: step 499, loss 0.324683, acc 0.84
2016-09-05T23:45:04.155890: step 500, loss 0.465212, acc 0.82

Evaluation:
2016-09-05T23:45:07.887211: step 500, loss 0.573185, acc 0.774859

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-500

2016-09-05T23:45:09.809139: step 501, loss 0.270018, acc 0.9
2016-09-05T23:45:10.606578: step 502, loss 0.223781, acc 0.92
2016-09-05T23:45:11.428615: step 503, loss 0.358117, acc 0.9
2016-09-05T23:45:12.249930: step 504, loss 0.251536, acc 0.9
2016-09-05T23:45:13.045433: step 505, loss 0.205166, acc 0.92
2016-09-05T23:45:13.857678: step 506, loss 0.28185, acc 0.9
2016-09-05T23:45:14.664836: step 507, loss 0.318441, acc 0.86
2016-09-05T23:45:15.436063: step 508, loss 0.348517, acc 0.82
2016-09-05T23:45:16.248304: step 509, loss 0.376474, acc 0.84
2016-09-05T23:45:17.104657: step 510, loss 0.230711, acc 0.92
2016-09-05T23:45:17.896252: step 511, loss 0.242633, acc 0.9
2016-09-05T23:45:18.750739: step 512, loss 0.260934, acc 0.9
2016-09-05T23:45:19.558124: step 513, loss 0.359958, acc 0.82
2016-09-05T23:45:20.345489: step 514, loss 0.30422, acc 0.86
2016-09-05T23:45:21.133697: step 515, loss 0.281433, acc 0.86
2016-09-05T23:45:21.935449: step 516, loss 0.263102, acc 0.86
2016-09-05T23:45:22.733670: step 517, loss 0.286256, acc 0.88
2016-09-05T23:45:23.535860: step 518, loss 0.182057, acc 0.92
2016-09-05T23:45:24.390054: step 519, loss 0.253052, acc 0.92
2016-09-05T23:45:25.173929: step 520, loss 0.233474, acc 0.92
2016-09-05T23:45:25.942746: step 521, loss 0.469062, acc 0.8
2016-09-05T23:45:26.771079: step 522, loss 0.234016, acc 0.92
2016-09-05T23:45:27.579427: step 523, loss 0.227155, acc 0.88
2016-09-05T23:45:28.407524: step 524, loss 0.219229, acc 0.9
2016-09-05T23:45:29.219638: step 525, loss 0.280777, acc 0.86
2016-09-05T23:45:29.992016: step 526, loss 0.220723, acc 0.9
2016-09-05T23:45:30.779163: step 527, loss 0.200444, acc 0.86
2016-09-05T23:45:31.595546: step 528, loss 0.272754, acc 0.88
2016-09-05T23:45:32.369206: step 529, loss 0.259765, acc 0.88
2016-09-05T23:45:33.196317: step 530, loss 0.504082, acc 0.82
2016-09-05T23:45:34.005339: step 531, loss 0.25118, acc 0.88
2016-09-05T23:45:34.778566: step 532, loss 0.304236, acc 0.88
2016-09-05T23:45:35.584015: step 533, loss 0.331733, acc 0.88
2016-09-05T23:45:36.377075: step 534, loss 0.166403, acc 0.94
2016-09-05T23:45:37.195887: step 535, loss 0.211514, acc 0.88
2016-09-05T23:45:37.998604: step 536, loss 0.216066, acc 0.88
2016-09-05T23:45:38.788845: step 537, loss 0.302859, acc 0.86
2016-09-05T23:45:39.586307: step 538, loss 0.381185, acc 0.86
2016-09-05T23:45:40.421811: step 539, loss 0.145016, acc 0.98
2016-09-05T23:45:41.214803: step 540, loss 0.291403, acc 0.86
2016-09-05T23:45:42.001390: step 541, loss 0.269654, acc 0.92
2016-09-05T23:45:42.820085: step 542, loss 0.397251, acc 0.82
2016-09-05T23:45:43.609830: step 543, loss 0.227519, acc 0.9
2016-09-05T23:45:44.432768: step 544, loss 0.326201, acc 0.86
2016-09-05T23:45:45.264681: step 545, loss 0.274344, acc 0.88
2016-09-05T23:45:46.060352: step 546, loss 0.284588, acc 0.9
2016-09-05T23:45:46.861466: step 547, loss 0.228738, acc 0.9
2016-09-05T23:45:47.682245: step 548, loss 0.328143, acc 0.84
2016-09-05T23:45:48.483727: step 549, loss 0.220215, acc 0.92
2016-09-05T23:45:49.285769: step 550, loss 0.203929, acc 0.9
2016-09-05T23:45:50.138797: step 551, loss 0.341379, acc 0.84
2016-09-05T23:45:50.988789: step 552, loss 0.218372, acc 0.88
2016-09-05T23:45:51.751002: step 553, loss 0.251153, acc 0.92
2016-09-05T23:45:52.554624: step 554, loss 0.354717, acc 0.86
2016-09-05T23:45:53.383056: step 555, loss 0.251374, acc 0.9
2016-09-05T23:45:54.166906: step 556, loss 0.424423, acc 0.88
2016-09-05T23:45:54.993475: step 557, loss 0.165911, acc 0.92
2016-09-05T23:45:55.800091: step 558, loss 0.247411, acc 0.92
2016-09-05T23:45:56.601822: step 559, loss 0.261589, acc 0.86
2016-09-05T23:45:57.423343: step 560, loss 0.291393, acc 0.88
2016-09-05T23:45:58.223611: step 561, loss 0.181299, acc 0.94
2016-09-05T23:45:58.976508: step 562, loss 0.166097, acc 0.94
2016-09-05T23:45:59.794106: step 563, loss 0.200494, acc 0.88
2016-09-05T23:46:00.670227: step 564, loss 0.244052, acc 0.9
2016-09-05T23:46:01.483974: step 565, loss 0.284986, acc 0.9
2016-09-05T23:46:02.303944: step 566, loss 0.257789, acc 0.86
2016-09-05T23:46:03.113922: step 567, loss 0.274032, acc 0.88
2016-09-05T23:46:03.912419: step 568, loss 0.0993295, acc 0.98
2016-09-05T23:46:04.708248: step 569, loss 0.192124, acc 0.94
2016-09-05T23:46:05.584268: step 570, loss 0.338625, acc 0.84
2016-09-05T23:46:06.440107: step 571, loss 0.160982, acc 0.9
2016-09-05T23:46:07.265514: step 572, loss 0.284292, acc 0.92
2016-09-05T23:46:08.106240: step 573, loss 0.316045, acc 0.9
2016-09-05T23:46:08.918907: step 574, loss 0.26646, acc 0.82
2016-09-05T23:46:09.713674: step 575, loss 0.260018, acc 0.88
2016-09-05T23:46:10.459820: step 576, loss 0.216213, acc 0.931818
2016-09-05T23:46:11.274440: step 577, loss 0.157895, acc 0.92
2016-09-05T23:46:12.093969: step 578, loss 0.166896, acc 0.88
2016-09-05T23:46:12.937722: step 579, loss 0.130328, acc 0.96
2016-09-05T23:46:13.745738: step 580, loss 0.186141, acc 0.92
2016-09-05T23:46:14.545617: step 581, loss 0.195951, acc 0.88
2016-09-05T23:46:15.367566: step 582, loss 0.197967, acc 0.94
2016-09-05T23:46:16.204550: step 583, loss 0.0685928, acc 0.98
2016-09-05T23:46:17.028953: step 584, loss 0.147845, acc 0.96
2016-09-05T23:46:17.846830: step 585, loss 0.178564, acc 0.94
2016-09-05T23:46:18.668929: step 586, loss 0.114538, acc 0.96
2016-09-05T23:46:19.479636: step 587, loss 0.152264, acc 0.98
2016-09-05T23:46:20.321696: step 588, loss 0.164417, acc 0.92
2016-09-05T23:46:21.121350: step 589, loss 0.301687, acc 0.86
2016-09-05T23:46:21.900787: step 590, loss 0.225441, acc 0.92
2016-09-05T23:46:22.729165: step 591, loss 0.170269, acc 0.92
2016-09-05T23:46:23.553215: step 592, loss 0.144285, acc 0.94
2016-09-05T23:46:24.378804: step 593, loss 0.259224, acc 0.86
2016-09-05T23:46:25.193836: step 594, loss 0.0693398, acc 0.96
2016-09-05T23:46:26.028356: step 595, loss 0.136741, acc 0.92
2016-09-05T23:46:26.795000: step 596, loss 0.213395, acc 0.9
2016-09-05T23:46:27.578680: step 597, loss 0.24655, acc 0.88
2016-09-05T23:46:28.393876: step 598, loss 0.115375, acc 0.94
2016-09-05T23:46:29.179826: step 599, loss 0.0959506, acc 0.96
2016-09-05T23:46:29.985597: step 600, loss 0.241911, acc 0.86

Evaluation:
2016-09-05T23:46:33.711105: step 600, loss 0.69297, acc 0.752345

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-600

2016-09-05T23:46:35.528703: step 601, loss 0.111153, acc 0.94
2016-09-05T23:46:36.341584: step 602, loss 0.214376, acc 0.92
2016-09-05T23:46:37.155819: step 603, loss 0.0981111, acc 1
2016-09-05T23:46:37.962035: step 604, loss 0.207474, acc 0.94
2016-09-05T23:46:38.779825: step 605, loss 0.200604, acc 0.92
2016-09-05T23:46:39.604397: step 606, loss 0.22131, acc 0.92
2016-09-05T23:46:40.406390: step 607, loss 0.260109, acc 0.88
2016-09-05T23:46:41.210113: step 608, loss 0.132981, acc 0.96
2016-09-05T23:46:42.036160: step 609, loss 0.242019, acc 0.9
2016-09-05T23:46:42.840491: step 610, loss 0.199773, acc 0.9
2016-09-05T23:46:43.648889: step 611, loss 0.184944, acc 0.94
2016-09-05T23:46:44.472388: step 612, loss 0.281599, acc 0.88
2016-09-05T23:46:45.271146: step 613, loss 0.183808, acc 0.92
2016-09-05T23:46:46.105922: step 614, loss 0.103119, acc 0.96
2016-09-05T23:46:46.930762: step 615, loss 0.415884, acc 0.86
2016-09-05T23:46:47.733803: step 616, loss 0.0989302, acc 0.94
2016-09-05T23:46:48.541352: step 617, loss 0.149109, acc 0.9
2016-09-05T23:46:49.414210: step 618, loss 0.0939027, acc 0.98
2016-09-05T23:46:50.254155: step 619, loss 0.153528, acc 0.94
2016-09-05T23:46:51.050923: step 620, loss 0.124509, acc 0.94
2016-09-05T23:46:51.887790: step 621, loss 0.163469, acc 0.92
2016-09-05T23:46:52.707656: step 622, loss 0.0551407, acc 0.98
2016-09-05T23:46:53.518057: step 623, loss 0.111538, acc 0.94
2016-09-05T23:46:54.337605: step 624, loss 0.311951, acc 0.9
2016-09-05T23:46:55.150785: step 625, loss 0.056245, acc 1
2016-09-05T23:46:55.942593: step 626, loss 0.157697, acc 0.92
2016-09-05T23:46:56.742458: step 627, loss 0.0915009, acc 0.94
2016-09-05T23:46:57.548918: step 628, loss 0.195929, acc 0.9
2016-09-05T23:46:58.331850: step 629, loss 0.123468, acc 0.94
2016-09-05T23:46:59.132952: step 630, loss 0.202371, acc 0.9
2016-09-05T23:46:59.954903: step 631, loss 0.268105, acc 0.88
2016-09-05T23:47:00.752601: step 632, loss 0.105781, acc 0.96
2016-09-05T23:47:01.561119: step 633, loss 0.0629667, acc 0.98
2016-09-05T23:47:02.388105: step 634, loss 0.164316, acc 0.92
2016-09-05T23:47:03.172771: step 635, loss 0.0547057, acc 1
2016-09-05T23:47:03.975718: step 636, loss 0.105181, acc 0.96
2016-09-05T23:47:04.775196: step 637, loss 0.137015, acc 0.94
2016-09-05T23:47:05.576058: step 638, loss 0.12839, acc 0.96
2016-09-05T23:47:06.406113: step 639, loss 0.14659, acc 0.92
2016-09-05T23:47:07.246606: step 640, loss 0.151286, acc 0.92
2016-09-05T23:47:08.043669: step 641, loss 0.176153, acc 0.92
2016-09-05T23:47:08.863326: step 642, loss 0.0902819, acc 0.96
2016-09-05T23:47:09.659909: step 643, loss 0.0771256, acc 0.96
2016-09-05T23:47:10.414876: step 644, loss 0.162925, acc 0.94
2016-09-05T23:47:11.243548: step 645, loss 0.148399, acc 0.96
2016-09-05T23:47:12.045462: step 646, loss 0.155422, acc 0.92
2016-09-05T23:47:12.844037: step 647, loss 0.170948, acc 0.92
2016-09-05T23:47:13.660948: step 648, loss 0.14167, acc 0.92
2016-09-05T23:47:14.474844: step 649, loss 0.250313, acc 0.88
2016-09-05T23:47:15.265455: step 650, loss 0.0955701, acc 0.98
2016-09-05T23:47:16.076014: step 651, loss 0.237526, acc 0.88
2016-09-05T23:47:16.893677: step 652, loss 0.155346, acc 0.92
2016-09-05T23:47:17.688218: step 653, loss 0.162263, acc 0.94
2016-09-05T23:47:18.479037: step 654, loss 0.227172, acc 0.9
2016-09-05T23:47:19.273607: step 655, loss 0.154963, acc 0.96
2016-09-05T23:47:20.105918: step 656, loss 0.140387, acc 0.92
2016-09-05T23:47:20.911270: step 657, loss 0.0528314, acc 0.98
2016-09-05T23:47:21.709462: step 658, loss 0.136788, acc 0.96
2016-09-05T23:47:22.511160: step 659, loss 0.292683, acc 0.86
2016-09-05T23:47:23.337933: step 660, loss 0.0831468, acc 0.98
2016-09-05T23:47:24.167644: step 661, loss 0.20777, acc 0.96
2016-09-05T23:47:24.976148: step 662, loss 0.168801, acc 0.92
2016-09-05T23:47:25.794447: step 663, loss 0.102735, acc 0.92
2016-09-05T23:47:26.604595: step 664, loss 0.170061, acc 0.92
2016-09-05T23:47:27.372116: step 665, loss 0.271595, acc 0.92
2016-09-05T23:47:28.153800: step 666, loss 0.108658, acc 0.96
2016-09-05T23:47:28.966588: step 667, loss 0.196663, acc 0.96
2016-09-05T23:47:29.755823: step 668, loss 0.162653, acc 0.94
2016-09-05T23:47:30.562545: step 669, loss 0.139474, acc 0.94
2016-09-05T23:47:31.374793: step 670, loss 0.144039, acc 0.92
2016-09-05T23:47:32.161404: step 671, loss 0.151603, acc 0.92
2016-09-05T23:47:32.977578: step 672, loss 0.197981, acc 0.94
2016-09-05T23:47:33.793981: step 673, loss 0.152055, acc 0.94
2016-09-05T23:47:34.570078: step 674, loss 0.133615, acc 0.94
2016-09-05T23:47:35.391507: step 675, loss 0.1267, acc 0.94
2016-09-05T23:47:36.197559: step 676, loss 0.118792, acc 0.94
2016-09-05T23:47:37.007715: step 677, loss 0.206973, acc 0.92
2016-09-05T23:47:37.821206: step 678, loss 0.282927, acc 0.9
2016-09-05T23:47:38.641194: step 679, loss 0.161494, acc 0.92
2016-09-05T23:47:39.431084: step 680, loss 0.0677337, acc 0.98
2016-09-05T23:47:40.233918: step 681, loss 0.200849, acc 0.9
2016-09-05T23:47:41.053527: step 682, loss 0.126203, acc 0.94
2016-09-05T23:47:41.829958: step 683, loss 0.26657, acc 0.92
2016-09-05T23:47:42.614644: step 684, loss 0.0847915, acc 0.98
2016-09-05T23:47:43.408914: step 685, loss 0.227711, acc 0.86
2016-09-05T23:47:44.220472: step 686, loss 0.173013, acc 0.94
2016-09-05T23:47:45.021759: step 687, loss 0.192272, acc 0.96
2016-09-05T23:47:45.825399: step 688, loss 0.207112, acc 0.94
2016-09-05T23:47:46.629617: step 689, loss 0.17353, acc 0.92
2016-09-05T23:47:47.470877: step 690, loss 0.128823, acc 0.94
2016-09-05T23:47:48.257954: step 691, loss 0.134429, acc 0.94
2016-09-05T23:47:49.073069: step 692, loss 0.0983251, acc 0.96
2016-09-05T23:47:49.892332: step 693, loss 0.180262, acc 0.94
2016-09-05T23:47:50.723833: step 694, loss 0.0356908, acc 1
2016-09-05T23:47:51.521893: step 695, loss 0.11771, acc 0.96
2016-09-05T23:47:52.327538: step 696, loss 0.158249, acc 0.96
2016-09-05T23:47:53.139558: step 697, loss 0.0547311, acc 0.98
2016-09-05T23:47:53.936623: step 698, loss 0.185383, acc 0.94
2016-09-05T23:47:54.732071: step 699, loss 0.095185, acc 0.98
2016-09-05T23:47:55.553493: step 700, loss 0.15868, acc 0.94

Evaluation:
2016-09-05T23:47:59.270512: step 700, loss 0.804961, acc 0.754221

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-700

2016-09-05T23:48:01.131746: step 701, loss 0.11575, acc 0.98
2016-09-05T23:48:01.969380: step 702, loss 0.146441, acc 0.92
2016-09-05T23:48:02.781812: step 703, loss 0.165165, acc 0.94
2016-09-05T23:48:03.579337: step 704, loss 0.193062, acc 0.94
2016-09-05T23:48:04.416889: step 705, loss 0.154281, acc 0.94
2016-09-05T23:48:05.214090: step 706, loss 0.363689, acc 0.86
2016-09-05T23:48:06.071324: step 707, loss 0.134011, acc 0.96
2016-09-05T23:48:06.906393: step 708, loss 0.150383, acc 0.9
2016-09-05T23:48:07.716780: step 709, loss 0.161594, acc 0.96
2016-09-05T23:48:08.497331: step 710, loss 0.290711, acc 0.92
2016-09-05T23:48:09.291226: step 711, loss 0.401173, acc 0.86
2016-09-05T23:48:10.139961: step 712, loss 0.201549, acc 0.9
2016-09-05T23:48:10.958470: step 713, loss 0.102969, acc 0.94
2016-09-05T23:48:11.790988: step 714, loss 0.0798994, acc 0.98
2016-09-05T23:48:12.656787: step 715, loss 0.227623, acc 0.9
2016-09-05T23:48:13.478541: step 716, loss 0.205577, acc 0.84
2016-09-05T23:48:14.270134: step 717, loss 0.189947, acc 0.92
2016-09-05T23:48:15.107104: step 718, loss 0.282844, acc 0.86
2016-09-05T23:48:15.928901: step 719, loss 0.10131, acc 0.98
2016-09-05T23:48:16.729066: step 720, loss 0.157892, acc 0.92
2016-09-05T23:48:17.575475: step 721, loss 0.136037, acc 0.94
2016-09-05T23:48:18.387057: step 722, loss 0.246998, acc 0.92
2016-09-05T23:48:19.190561: step 723, loss 0.234761, acc 0.86
2016-09-05T23:48:20.010921: step 724, loss 0.208998, acc 0.92
2016-09-05T23:48:20.807757: step 725, loss 0.162568, acc 0.94
2016-09-05T23:48:21.611431: step 726, loss 0.0998564, acc 0.98
2016-09-05T23:48:22.422825: step 727, loss 0.215352, acc 0.92
2016-09-05T23:48:23.228062: step 728, loss 0.17888, acc 0.94
2016-09-05T23:48:24.009439: step 729, loss 0.0866415, acc 0.98
2016-09-05T23:48:24.833942: step 730, loss 0.213153, acc 0.9
2016-09-05T23:48:25.635309: step 731, loss 0.224306, acc 0.88
2016-09-05T23:48:26.450424: step 732, loss 0.256685, acc 0.88
2016-09-05T23:48:27.259851: step 733, loss 0.193834, acc 0.86
2016-09-05T23:48:28.059586: step 734, loss 0.141753, acc 0.92
2016-09-05T23:48:28.857501: step 735, loss 0.055435, acc 0.98
2016-09-05T23:48:29.668021: step 736, loss 0.316774, acc 0.9
2016-09-05T23:48:30.465288: step 737, loss 0.152959, acc 0.94
2016-09-05T23:48:31.257336: step 738, loss 0.195393, acc 0.84
2016-09-05T23:48:32.065260: step 739, loss 0.144538, acc 0.96
2016-09-05T23:48:32.883956: step 740, loss 0.211513, acc 0.94
2016-09-05T23:48:33.698461: step 741, loss 0.0527754, acc 1
2016-09-05T23:48:34.549758: step 742, loss 0.235872, acc 0.96
2016-09-05T23:48:35.361071: step 743, loss 0.1368, acc 0.94
2016-09-05T23:48:36.170203: step 744, loss 0.181058, acc 0.92
2016-09-05T23:48:36.993480: step 745, loss 0.300076, acc 0.82
2016-09-05T23:48:37.801712: step 746, loss 0.135431, acc 0.92
2016-09-05T23:48:38.592203: step 747, loss 0.220242, acc 0.9
2016-09-05T23:48:39.422891: step 748, loss 0.194308, acc 0.88
2016-09-05T23:48:40.230621: step 749, loss 0.180982, acc 0.92
2016-09-05T23:48:41.033677: step 750, loss 0.208017, acc 0.94
2016-09-05T23:48:41.862500: step 751, loss 0.14203, acc 0.96
2016-09-05T23:48:42.661403: step 752, loss 0.289112, acc 0.86
2016-09-05T23:48:43.454645: step 753, loss 0.196251, acc 0.92
2016-09-05T23:48:44.268915: step 754, loss 0.208023, acc 0.86
2016-09-05T23:48:45.088972: step 755, loss 0.0760526, acc 1
2016-09-05T23:48:45.900843: step 756, loss 0.201794, acc 0.82
2016-09-05T23:48:46.712304: step 757, loss 0.140217, acc 0.94
2016-09-05T23:48:47.510831: step 758, loss 0.197955, acc 0.92
2016-09-05T23:48:48.308773: step 759, loss 0.171213, acc 0.94
2016-09-05T23:48:49.121906: step 760, loss 0.184405, acc 0.96
2016-09-05T23:48:49.915224: step 761, loss 0.249964, acc 0.9
2016-09-05T23:48:50.704424: step 762, loss 0.218492, acc 0.94
2016-09-05T23:48:51.572300: step 763, loss 0.174627, acc 0.9
2016-09-05T23:48:52.389415: step 764, loss 0.236858, acc 0.92
2016-09-05T23:48:53.169737: step 765, loss 0.338508, acc 0.86
2016-09-05T23:48:53.965460: step 766, loss 0.338017, acc 0.84
2016-09-05T23:48:54.775250: step 767, loss 0.263846, acc 0.88
2016-09-05T23:48:55.497578: step 768, loss 0.12628, acc 0.954545
2016-09-05T23:48:56.336946: step 769, loss 0.0802279, acc 0.98
2016-09-05T23:48:57.165057: step 770, loss 0.111696, acc 1
2016-09-05T23:48:57.961943: step 771, loss 0.0924811, acc 0.96
2016-09-05T23:48:58.841512: step 772, loss 0.122127, acc 0.94
2016-09-05T23:48:59.680977: step 773, loss 0.104508, acc 0.98
2016-09-05T23:49:00.504255: step 774, loss 0.0992922, acc 0.96
2016-09-05T23:49:01.310157: step 775, loss 0.149155, acc 0.92
2016-09-05T23:49:02.114174: step 776, loss 0.181084, acc 0.9
2016-09-05T23:49:02.913521: step 777, loss 0.126187, acc 0.92
2016-09-05T23:49:03.725299: step 778, loss 0.0546539, acc 1
2016-09-05T23:49:04.548866: step 779, loss 0.166101, acc 0.94
2016-09-05T23:49:05.353011: step 780, loss 0.300896, acc 0.86
2016-09-05T23:49:06.170202: step 781, loss 0.0598094, acc 0.96
2016-09-05T23:49:07.007892: step 782, loss 0.228684, acc 0.92
2016-09-05T23:49:07.811319: step 783, loss 0.0715846, acc 0.98
2016-09-05T23:49:08.609153: step 784, loss 0.0811926, acc 0.96
2016-09-05T23:49:09.426423: step 785, loss 0.198186, acc 0.92
2016-09-05T23:49:10.233211: step 786, loss 0.125022, acc 0.96
2016-09-05T23:49:11.025425: step 787, loss 0.135778, acc 0.96
2016-09-05T23:49:11.864880: step 788, loss 0.235973, acc 0.88
2016-09-05T23:49:12.681815: step 789, loss 0.270744, acc 0.86
2016-09-05T23:49:13.491778: step 790, loss 0.152349, acc 0.9
2016-09-05T23:49:14.324427: step 791, loss 0.118235, acc 0.94
2016-09-05T23:49:15.138694: step 792, loss 0.289325, acc 0.88
2016-09-05T23:49:15.992088: step 793, loss 0.0489205, acc 1
2016-09-05T23:49:16.846442: step 794, loss 0.171623, acc 0.88
2016-09-05T23:49:17.677221: step 795, loss 0.128389, acc 0.94
2016-09-05T23:49:18.475982: step 796, loss 0.140492, acc 0.94
2016-09-05T23:49:19.297719: step 797, loss 0.218652, acc 0.9
2016-09-05T23:49:20.097978: step 798, loss 0.186407, acc 0.92
2016-09-05T23:49:20.901273: step 799, loss 0.163941, acc 0.92
2016-09-05T23:49:21.720690: step 800, loss 0.0655012, acc 1

Evaluation:
2016-09-05T23:49:25.458740: step 800, loss 0.810506, acc 0.757974

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-800

2016-09-05T23:49:27.394820: step 801, loss 0.100195, acc 0.96
2016-09-05T23:49:28.209345: step 802, loss 0.101147, acc 0.98
2016-09-05T23:49:29.043021: step 803, loss 0.0637988, acc 1
2016-09-05T23:49:29.871576: step 804, loss 0.214825, acc 0.92
2016-09-05T23:49:30.694049: step 805, loss 0.0566442, acc 1
2016-09-05T23:49:31.522175: step 806, loss 0.148194, acc 0.92
2016-09-05T23:49:32.362068: step 807, loss 0.107064, acc 0.94
2016-09-05T23:49:33.149581: step 808, loss 0.103979, acc 0.96
2016-09-05T23:49:33.972722: step 809, loss 0.200212, acc 0.96
2016-09-05T23:49:34.776849: step 810, loss 0.0889166, acc 0.94
2016-09-05T23:49:35.567287: step 811, loss 0.0923585, acc 0.96
2016-09-05T23:49:36.384095: step 812, loss 0.130241, acc 0.96
2016-09-05T23:49:37.204922: step 813, loss 0.108927, acc 0.96
2016-09-05T23:49:38.005756: step 814, loss 0.16214, acc 0.88
2016-09-05T23:49:38.797615: step 815, loss 0.0496624, acc 0.98
2016-09-05T23:49:39.611092: step 816, loss 0.145447, acc 0.92
2016-09-05T23:49:40.393322: step 817, loss 0.0927749, acc 0.96
2016-09-05T23:49:41.192466: step 818, loss 0.022975, acc 1
2016-09-05T23:49:41.997472: step 819, loss 0.0920543, acc 0.98
2016-09-05T23:49:42.775751: step 820, loss 0.186676, acc 0.92
2016-09-05T23:49:43.598591: step 821, loss 0.0476297, acc 1
2016-09-05T23:49:44.391803: step 822, loss 0.0896244, acc 0.98
2016-09-05T23:49:45.201069: step 823, loss 0.127665, acc 0.94
2016-09-05T23:49:46.037799: step 824, loss 0.137447, acc 0.96
2016-09-05T23:49:46.859804: step 825, loss 0.106659, acc 0.98
2016-09-05T23:49:47.631892: step 826, loss 0.0799345, acc 0.98
2016-09-05T23:49:48.455452: step 827, loss 0.0688204, acc 0.98
2016-09-05T23:49:49.258064: step 828, loss 0.202653, acc 0.94
2016-09-05T23:49:50.061395: step 829, loss 0.0963099, acc 0.94
2016-09-05T23:49:50.862536: step 830, loss 0.181138, acc 0.9
2016-09-05T23:49:51.662392: step 831, loss 0.0867903, acc 0.96
2016-09-05T23:49:52.455219: step 832, loss 0.142404, acc 0.96
2016-09-05T23:49:53.277346: step 833, loss 0.228883, acc 0.94
2016-09-05T23:49:54.089017: step 834, loss 0.0799363, acc 0.96
2016-09-05T23:49:54.962291: step 835, loss 0.373729, acc 0.92
2016-09-05T23:49:55.778776: step 836, loss 0.0796706, acc 0.98
2016-09-05T23:49:56.582175: step 837, loss 0.116824, acc 0.96
2016-09-05T23:49:57.379170: step 838, loss 0.0751819, acc 0.98
2016-09-05T23:49:58.155124: step 839, loss 0.176813, acc 0.88
2016-09-05T23:49:58.986537: step 840, loss 0.119615, acc 0.92
2016-09-05T23:49:59.760241: step 841, loss 0.0547342, acc 0.98
2016-09-05T23:50:00.579443: step 842, loss 0.0744032, acc 0.98
2016-09-05T23:50:01.379313: step 843, loss 0.221205, acc 0.92
2016-09-05T23:50:02.193587: step 844, loss 0.158603, acc 0.94
2016-09-05T23:50:03.002776: step 845, loss 0.152202, acc 0.92
2016-09-05T23:50:03.830752: step 846, loss 0.146589, acc 0.9
2016-09-05T23:50:04.600344: step 847, loss 0.138167, acc 0.94
2016-09-05T23:50:05.409293: step 848, loss 0.14618, acc 0.94
2016-09-05T23:50:06.229416: step 849, loss 0.161763, acc 0.92
2016-09-05T23:50:06.995238: step 850, loss 0.183464, acc 0.9
2016-09-05T23:50:07.799095: step 851, loss 0.327558, acc 0.88
2016-09-05T23:50:08.613820: step 852, loss 0.0883737, acc 0.98
2016-09-05T23:50:09.437004: step 853, loss 0.250933, acc 0.92
2016-09-05T23:50:10.270366: step 854, loss 0.0553337, acc 1
2016-09-05T23:50:11.078483: step 855, loss 0.0970646, acc 0.96
2016-09-05T23:50:11.844060: step 856, loss 0.135165, acc 0.9
2016-09-05T23:50:12.661836: step 857, loss 0.152491, acc 0.94
2016-09-05T23:50:13.474725: step 858, loss 0.111194, acc 0.96
2016-09-05T23:50:14.258679: step 859, loss 0.173434, acc 0.88
2016-09-05T23:50:15.056846: step 860, loss 0.0850153, acc 0.94
2016-09-05T23:50:15.876215: step 861, loss 0.10559, acc 0.96
2016-09-05T23:50:16.657125: step 862, loss 0.0884438, acc 0.94
2016-09-05T23:50:17.469153: step 863, loss 0.0757412, acc 0.98
2016-09-05T23:50:18.276895: step 864, loss 0.0772057, acc 0.96
2016-09-05T23:50:19.079157: step 865, loss 0.100663, acc 0.96
2016-09-05T23:50:19.882589: step 866, loss 0.180789, acc 0.94
2016-09-05T23:50:20.697607: step 867, loss 0.111732, acc 0.94
2016-09-05T23:50:21.502401: step 868, loss 0.0852999, acc 0.96
2016-09-05T23:50:22.298291: step 869, loss 0.245428, acc 0.94
2016-09-05T23:50:23.118897: step 870, loss 0.14386, acc 0.9
2016-09-05T23:50:23.891224: step 871, loss 0.0790612, acc 0.96
2016-09-05T23:50:24.717406: step 872, loss 0.246348, acc 0.88
2016-09-05T23:50:25.519741: step 873, loss 0.158321, acc 0.96
2016-09-05T23:50:26.328034: step 874, loss 0.0919603, acc 0.96
2016-09-05T23:50:27.139665: step 875, loss 0.0995321, acc 0.96
2016-09-05T23:50:27.957485: step 876, loss 0.15262, acc 0.94
2016-09-05T23:50:28.731460: step 877, loss 0.184082, acc 0.92
2016-09-05T23:50:29.523829: step 878, loss 0.136313, acc 0.94
2016-09-05T23:50:30.342374: step 879, loss 0.119037, acc 0.96
2016-09-05T23:50:31.137906: step 880, loss 0.13322, acc 0.94
2016-09-05T23:50:31.983341: step 881, loss 0.208549, acc 0.94
2016-09-05T23:50:32.791823: step 882, loss 0.119691, acc 0.94
2016-09-05T23:50:33.594878: step 883, loss 0.135939, acc 0.94
2016-09-05T23:50:34.394379: step 884, loss 0.0601114, acc 0.98
2016-09-05T23:50:35.205155: step 885, loss 0.0768076, acc 1
2016-09-05T23:50:35.992553: step 886, loss 0.0674071, acc 0.98
2016-09-05T23:50:36.774920: step 887, loss 0.136372, acc 0.96
2016-09-05T23:50:37.577428: step 888, loss 0.137699, acc 0.96
2016-09-05T23:50:38.404347: step 889, loss 0.102119, acc 0.96
2016-09-05T23:50:39.225738: step 890, loss 0.167095, acc 0.92
2016-09-05T23:50:40.055599: step 891, loss 0.0965519, acc 0.96
2016-09-05T23:50:40.825115: step 892, loss 0.0857058, acc 0.98
2016-09-05T23:50:41.641535: step 893, loss 0.102997, acc 0.96
2016-09-05T23:50:42.491898: step 894, loss 0.0757521, acc 0.96
2016-09-05T23:50:43.284937: step 895, loss 0.115167, acc 0.98
2016-09-05T23:50:44.052611: step 896, loss 0.177922, acc 0.92
2016-09-05T23:50:44.897195: step 897, loss 0.121319, acc 0.96
2016-09-05T23:50:45.679605: step 898, loss 0.280163, acc 0.94
2016-09-05T23:50:46.497529: step 899, loss 0.109042, acc 0.96
2016-09-05T23:50:47.329541: step 900, loss 0.100707, acc 0.94

Evaluation:
2016-09-05T23:50:51.089140: step 900, loss 0.869974, acc 0.763602

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-900

2016-09-05T23:50:52.921645: step 901, loss 0.170095, acc 0.94
2016-09-05T23:50:53.721160: step 902, loss 0.148013, acc 0.92
2016-09-05T23:50:54.554940: step 903, loss 0.113074, acc 0.94
2016-09-05T23:50:55.349660: step 904, loss 0.151691, acc 0.94
2016-09-05T23:50:56.155681: step 905, loss 0.246846, acc 0.88
2016-09-05T23:50:56.985404: step 906, loss 0.197154, acc 0.92
2016-09-05T23:50:57.754566: step 907, loss 0.177765, acc 0.94
2016-09-05T23:50:58.578611: step 908, loss 0.136384, acc 0.92
2016-09-05T23:50:59.385218: step 909, loss 0.112541, acc 0.94
2016-09-05T23:51:00.178936: step 910, loss 0.180848, acc 0.9
2016-09-05T23:51:01.018582: step 911, loss 0.068223, acc 0.98
2016-09-05T23:51:01.828306: step 912, loss 0.109141, acc 0.96
2016-09-05T23:51:02.602746: step 913, loss 0.157497, acc 0.94
2016-09-05T23:51:03.418316: step 914, loss 0.178318, acc 0.96
2016-09-05T23:51:04.256493: step 915, loss 0.187081, acc 0.94
2016-09-05T23:51:05.089979: step 916, loss 0.125619, acc 0.96
2016-09-05T23:51:05.910520: step 917, loss 0.114499, acc 0.98
2016-09-05T23:51:06.773149: step 918, loss 0.110162, acc 0.94
2016-09-05T23:51:07.576998: step 919, loss 0.133881, acc 0.96
2016-09-05T23:51:08.362252: step 920, loss 0.210123, acc 0.94
2016-09-05T23:51:09.204290: step 921, loss 0.0936341, acc 0.96
2016-09-05T23:51:10.001675: step 922, loss 0.0672829, acc 0.98
2016-09-05T23:51:10.818235: step 923, loss 0.13684, acc 0.92
2016-09-05T23:51:11.631046: step 924, loss 0.113435, acc 0.96
2016-09-05T23:51:12.478511: step 925, loss 0.171547, acc 0.94
2016-09-05T23:51:13.298566: step 926, loss 0.230487, acc 0.94
2016-09-05T23:51:14.154257: step 927, loss 0.165141, acc 0.92
2016-09-05T23:51:14.963817: step 928, loss 0.197944, acc 0.96
2016-09-05T23:51:15.774149: step 929, loss 0.11946, acc 0.96
2016-09-05T23:51:16.598479: step 930, loss 0.174765, acc 0.88
2016-09-05T23:51:17.422270: step 931, loss 0.0655402, acc 0.98
2016-09-05T23:51:18.228785: step 932, loss 0.189316, acc 0.92
2016-09-05T23:51:19.072652: step 933, loss 0.189236, acc 0.92
2016-09-05T23:51:19.883962: step 934, loss 0.14682, acc 0.92
2016-09-05T23:51:20.658459: step 935, loss 0.331548, acc 0.88
2016-09-05T23:51:21.506360: step 936, loss 0.212578, acc 0.88
2016-09-05T23:51:22.304939: step 937, loss 0.23099, acc 0.9
2016-09-05T23:51:23.144742: step 938, loss 0.122974, acc 0.94
2016-09-05T23:51:23.967342: step 939, loss 0.24248, acc 0.92
2016-09-05T23:51:24.777901: step 940, loss 0.221537, acc 0.88
2016-09-05T23:51:25.550553: step 941, loss 0.201423, acc 0.92
2016-09-05T23:51:26.383457: step 942, loss 0.147863, acc 0.96
2016-09-05T23:51:27.194939: step 943, loss 0.227722, acc 0.92
2016-09-05T23:51:27.955664: step 944, loss 0.15491, acc 0.94
2016-09-05T23:51:28.761816: step 945, loss 0.115269, acc 0.96
2016-09-05T23:51:29.561054: step 946, loss 0.18365, acc 0.92
2016-09-05T23:51:30.353483: step 947, loss 0.149942, acc 0.96
2016-09-05T23:51:31.187484: step 948, loss 0.198194, acc 0.9
2016-09-05T23:51:31.990725: step 949, loss 0.175888, acc 0.92
2016-09-05T23:51:32.772600: step 950, loss 0.136384, acc 0.96
2016-09-05T23:51:33.573166: step 951, loss 0.160734, acc 0.94
2016-09-05T23:51:34.368637: step 952, loss 0.0664462, acc 1
2016-09-05T23:51:35.165454: step 953, loss 0.29676, acc 0.86
2016-09-05T23:51:35.973844: step 954, loss 0.121964, acc 0.92
2016-09-05T23:51:36.788906: step 955, loss 0.0734558, acc 0.98
2016-09-05T23:51:37.609503: step 956, loss 0.0893969, acc 0.96
2016-09-05T23:51:38.446382: step 957, loss 0.306222, acc 0.9
2016-09-05T23:51:39.256315: step 958, loss 0.106764, acc 0.94
2016-09-05T23:51:40.042024: step 959, loss 0.0974042, acc 0.96
2016-09-05T23:51:40.766588: step 960, loss 0.0969834, acc 0.954545
2016-09-05T23:51:41.583810: step 961, loss 0.0896916, acc 1
2016-09-05T23:51:42.418625: step 962, loss 0.0721551, acc 0.98
2016-09-05T23:51:43.269820: step 963, loss 0.0492269, acc 0.98
2016-09-05T23:51:44.125401: step 964, loss 0.0401233, acc 1
2016-09-05T23:51:44.897040: step 965, loss 0.0562844, acc 0.98
2016-09-05T23:51:45.709339: step 966, loss 0.0794691, acc 1
2016-09-05T23:51:46.527847: step 967, loss 0.219282, acc 0.88
2016-09-05T23:51:47.324584: step 968, loss 0.138125, acc 0.94
2016-09-05T23:51:48.115808: step 969, loss 0.118207, acc 0.9
2016-09-05T23:51:48.926872: step 970, loss 0.204, acc 0.9
2016-09-05T23:51:49.716554: step 971, loss 0.170571, acc 0.92
2016-09-05T23:51:50.524350: step 972, loss 0.141305, acc 0.92
2016-09-05T23:51:51.327409: step 973, loss 0.0272151, acc 1
2016-09-05T23:51:52.123487: step 974, loss 0.0330045, acc 1
2016-09-05T23:51:53.003122: step 975, loss 0.147015, acc 0.92
2016-09-05T23:51:53.835410: step 976, loss 0.110834, acc 0.98
2016-09-05T23:51:54.616435: step 977, loss 0.0505432, acc 0.98
2016-09-05T23:51:55.402328: step 978, loss 0.0888656, acc 0.96
2016-09-05T23:51:56.207705: step 979, loss 0.0342628, acc 1
2016-09-05T23:51:56.976692: step 980, loss 0.1046, acc 0.98
2016-09-05T23:51:57.793987: step 981, loss 0.0815187, acc 0.96
2016-09-05T23:51:58.604929: step 982, loss 0.0738695, acc 0.98
2016-09-05T23:51:59.408664: step 983, loss 0.071794, acc 0.98
2016-09-05T23:52:00.240895: step 984, loss 0.0857736, acc 0.94
2016-09-05T23:52:01.074082: step 985, loss 0.0448088, acc 0.98
2016-09-05T23:52:01.842837: step 986, loss 0.11157, acc 0.98
2016-09-05T23:52:02.677905: step 987, loss 0.0512331, acc 0.96
2016-09-05T23:52:03.514029: step 988, loss 0.237136, acc 0.94
2016-09-05T23:52:04.278669: step 989, loss 0.056132, acc 0.96
2016-09-05T23:52:05.079231: step 990, loss 0.0443036, acc 0.98
2016-09-05T23:52:05.907169: step 991, loss 0.156841, acc 0.94
2016-09-05T23:52:06.684295: step 992, loss 0.122163, acc 0.9
2016-09-05T23:52:07.494276: step 993, loss 0.0781853, acc 0.98
2016-09-05T23:52:08.319135: step 994, loss 0.14352, acc 0.94
2016-09-05T23:52:09.100164: step 995, loss 0.12407, acc 0.94
2016-09-05T23:52:09.891660: step 996, loss 0.0627287, acc 0.98
2016-09-05T23:52:10.720408: step 997, loss 0.103562, acc 0.98
2016-09-05T23:52:11.536415: step 998, loss 0.157637, acc 0.92
2016-09-05T23:52:12.343517: step 999, loss 0.178413, acc 0.94
2016-09-05T23:52:13.175768: step 1000, loss 0.104855, acc 0.96

Evaluation:
2016-09-05T23:52:16.886593: step 1000, loss 0.858065, acc 0.767355

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-1000

2016-09-05T23:52:18.702093: step 1001, loss 0.107836, acc 0.92
2016-09-05T23:52:19.521761: step 1002, loss 0.0859022, acc 0.96
2016-09-05T23:52:20.328763: step 1003, loss 0.087763, acc 0.96
2016-09-05T23:52:21.157480: step 1004, loss 0.211933, acc 0.92
2016-09-05T23:52:21.985464: step 1005, loss 0.0648274, acc 0.98
2016-09-05T23:52:22.794325: step 1006, loss 0.0864545, acc 0.96
2016-09-05T23:52:23.640479: step 1007, loss 0.0719765, acc 0.98
2016-09-05T23:52:24.484255: step 1008, loss 0.0783488, acc 0.98
2016-09-05T23:52:25.290044: step 1009, loss 0.0557392, acc 0.98
2016-09-05T23:52:26.051430: step 1010, loss 0.121044, acc 0.96
2016-09-05T23:52:26.881680: step 1011, loss 0.0818598, acc 0.98
2016-09-05T23:52:27.692650: step 1012, loss 0.123874, acc 0.96
2016-09-05T23:52:28.481351: step 1013, loss 0.177936, acc 0.9
2016-09-05T23:52:29.288761: step 1014, loss 0.0350009, acc 0.98
2016-09-05T23:52:30.101389: step 1015, loss 0.0912329, acc 0.96
2016-09-05T23:52:30.889344: step 1016, loss 0.253132, acc 0.96
2016-09-05T23:52:31.709388: step 1017, loss 0.0538132, acc 0.98
2016-09-05T23:52:32.527158: step 1018, loss 0.149389, acc 0.9
2016-09-05T23:52:33.308418: step 1019, loss 0.0965983, acc 0.96
2016-09-05T23:52:34.105925: step 1020, loss 0.141517, acc 0.94
2016-09-05T23:52:34.914876: step 1021, loss 0.20751, acc 0.92
2016-09-05T23:52:35.689441: step 1022, loss 0.0709302, acc 0.96
2016-09-05T23:52:36.482226: step 1023, loss 0.0829195, acc 0.98
2016-09-05T23:52:37.264469: step 1024, loss 0.0847724, acc 0.96
2016-09-05T23:52:38.069419: step 1025, loss 0.0968464, acc 0.98
2016-09-05T23:52:38.919279: step 1026, loss 0.112446, acc 0.94
2016-09-05T23:52:39.755818: step 1027, loss 0.0870502, acc 0.94
2016-09-05T23:52:40.562398: step 1028, loss 0.0891838, acc 0.94
2016-09-05T23:52:41.348037: step 1029, loss 0.147916, acc 0.92
2016-09-05T23:52:42.158658: step 1030, loss 0.145206, acc 0.96
2016-09-05T23:52:42.956664: step 1031, loss 0.104071, acc 0.94
2016-09-05T23:52:43.745283: step 1032, loss 0.0976794, acc 0.94
2016-09-05T23:52:44.552581: step 1033, loss 0.111347, acc 0.94
2016-09-05T23:52:45.353465: step 1034, loss 0.174068, acc 0.92
2016-09-05T23:52:46.150622: step 1035, loss 0.107778, acc 0.96
2016-09-05T23:52:46.965876: step 1036, loss 0.0794662, acc 0.98
2016-09-05T23:52:47.815791: step 1037, loss 0.243028, acc 0.9
2016-09-05T23:52:48.645676: step 1038, loss 0.218988, acc 0.9
2016-09-05T23:52:49.461459: step 1039, loss 0.0836268, acc 0.98
2016-09-05T23:52:50.262191: step 1040, loss 0.121222, acc 0.98
2016-09-05T23:52:51.031611: step 1041, loss 0.0535186, acc 1
2016-09-05T23:52:51.863003: step 1042, loss 0.112629, acc 0.96
2016-09-05T23:52:52.628634: step 1043, loss 0.116706, acc 0.96
2016-09-05T23:52:53.434004: step 1044, loss 0.099142, acc 0.94
2016-09-05T23:52:54.265851: step 1045, loss 0.0896884, acc 0.96
2016-09-05T23:52:55.059749: step 1046, loss 0.157081, acc 0.92
2016-09-05T23:52:55.858772: step 1047, loss 0.103297, acc 0.96
2016-09-05T23:52:56.676978: step 1048, loss 0.073013, acc 0.96
2016-09-05T23:52:57.481228: step 1049, loss 0.0854488, acc 0.96
2016-09-05T23:52:58.285469: step 1050, loss 0.0567937, acc 0.98
2016-09-05T23:52:59.101400: step 1051, loss 0.172239, acc 0.94
2016-09-05T23:52:59.888192: step 1052, loss 0.127226, acc 0.96
2016-09-05T23:53:00.680585: step 1053, loss 0.188723, acc 0.94
2016-09-05T23:53:01.503283: step 1054, loss 0.0482795, acc 0.98
2016-09-05T23:53:02.290774: step 1055, loss 0.0255275, acc 1
2016-09-05T23:53:03.082555: step 1056, loss 0.0620006, acc 0.98
2016-09-05T23:53:03.894597: step 1057, loss 0.0205483, acc 1
2016-09-05T23:53:04.693661: step 1058, loss 0.082145, acc 0.96
2016-09-05T23:53:05.499403: step 1059, loss 0.158729, acc 0.94
2016-09-05T23:53:06.340046: step 1060, loss 0.0984632, acc 0.96
2016-09-05T23:53:07.157395: step 1061, loss 0.0650238, acc 0.98
2016-09-05T23:53:07.969496: step 1062, loss 0.0715195, acc 0.98
2016-09-05T23:53:08.784438: step 1063, loss 0.0298309, acc 1
2016-09-05T23:53:09.557456: step 1064, loss 0.0735043, acc 0.98
2016-09-05T23:53:10.358766: step 1065, loss 0.10104, acc 0.92
2016-09-05T23:53:11.170194: step 1066, loss 0.144662, acc 0.92
2016-09-05T23:53:11.979297: step 1067, loss 0.112297, acc 0.96
2016-09-05T23:53:12.778928: step 1068, loss 0.106659, acc 0.94
2016-09-05T23:53:13.605414: step 1069, loss 0.176728, acc 0.9
2016-09-05T23:53:14.369473: step 1070, loss 0.0678625, acc 0.98
2016-09-05T23:53:15.176332: step 1071, loss 0.0178472, acc 1
2016-09-05T23:53:15.997418: step 1072, loss 0.0850042, acc 0.96
2016-09-05T23:53:16.776709: step 1073, loss 0.0696368, acc 0.98
2016-09-05T23:53:17.600000: step 1074, loss 0.0354536, acc 0.98
2016-09-05T23:53:18.421235: step 1075, loss 0.0993912, acc 0.94
2016-09-05T23:53:19.207183: step 1076, loss 0.0419783, acc 0.98
2016-09-05T23:53:20.027060: step 1077, loss 0.03833, acc 0.98
2016-09-05T23:53:20.865582: step 1078, loss 0.0463614, acc 0.98
2016-09-05T23:53:21.641977: step 1079, loss 0.083699, acc 0.98
2016-09-05T23:53:22.477790: step 1080, loss 0.132701, acc 0.92
2016-09-05T23:53:23.286990: step 1081, loss 0.108247, acc 0.96
2016-09-05T23:53:24.088115: step 1082, loss 0.0420969, acc 0.98
2016-09-05T23:53:24.878316: step 1083, loss 0.0566747, acc 0.98
2016-09-05T23:53:25.670216: step 1084, loss 0.167076, acc 0.96
2016-09-05T23:53:26.435298: step 1085, loss 0.192863, acc 0.92
2016-09-05T23:53:27.234398: step 1086, loss 0.0821066, acc 0.94
2016-09-05T23:53:28.045455: step 1087, loss 0.225991, acc 0.96
2016-09-05T23:53:28.861945: step 1088, loss 0.0284169, acc 1
2016-09-05T23:53:29.661106: step 1089, loss 0.114199, acc 0.96
2016-09-05T23:53:30.483325: step 1090, loss 0.151618, acc 0.94
2016-09-05T23:53:31.288054: step 1091, loss 0.0682818, acc 0.98
2016-09-05T23:53:32.102067: step 1092, loss 0.142234, acc 0.94
2016-09-05T23:53:32.915912: step 1093, loss 0.132234, acc 0.96
2016-09-05T23:53:33.692700: step 1094, loss 0.179162, acc 0.94
2016-09-05T23:53:34.505815: step 1095, loss 0.133377, acc 0.94
2016-09-05T23:53:35.335338: step 1096, loss 0.103498, acc 0.94
2016-09-05T23:53:36.141051: step 1097, loss 0.104767, acc 0.96
2016-09-05T23:53:36.965776: step 1098, loss 0.134338, acc 0.92
2016-09-05T23:53:37.786776: step 1099, loss 0.0868503, acc 0.96
2016-09-05T23:53:38.564710: step 1100, loss 0.143993, acc 0.94

Evaluation:
2016-09-05T23:53:42.299100: step 1100, loss 0.821391, acc 0.755159

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-1100

2016-09-05T23:53:44.189893: step 1101, loss 0.0652613, acc 0.98
2016-09-05T23:53:45.045699: step 1102, loss 0.142883, acc 0.94
2016-09-05T23:53:45.840275: step 1103, loss 0.104713, acc 0.96
2016-09-05T23:53:46.648143: step 1104, loss 0.112287, acc 0.96
2016-09-05T23:53:47.498033: step 1105, loss 0.0907259, acc 0.98
2016-09-05T23:53:48.289443: step 1106, loss 0.0759079, acc 0.98
2016-09-05T23:53:49.061603: step 1107, loss 0.143433, acc 0.92
2016-09-05T23:53:49.877350: step 1108, loss 0.192311, acc 0.92
2016-09-05T23:53:50.662297: step 1109, loss 0.0952679, acc 0.94
2016-09-05T23:53:51.449548: step 1110, loss 0.0727189, acc 1
2016-09-05T23:53:52.241149: step 1111, loss 0.0727951, acc 0.96
2016-09-05T23:53:53.041988: step 1112, loss 0.0586475, acc 0.98
2016-09-05T23:53:53.852019: step 1113, loss 0.106773, acc 0.96
2016-09-05T23:53:54.685756: step 1114, loss 0.148191, acc 0.94
2016-09-05T23:53:55.473547: step 1115, loss 0.219363, acc 0.88
2016-09-05T23:53:56.297406: step 1116, loss 0.152448, acc 0.9
2016-09-05T23:53:57.112306: step 1117, loss 0.0728103, acc 0.94
2016-09-05T23:53:57.912699: step 1118, loss 0.104918, acc 0.98
2016-09-05T23:53:58.701965: step 1119, loss 0.0674846, acc 0.98
2016-09-05T23:53:59.490349: step 1120, loss 0.0644151, acc 1
2016-09-05T23:54:00.343818: step 1121, loss 0.0895212, acc 0.94
2016-09-05T23:54:01.153697: step 1122, loss 0.0504825, acc 0.98
2016-09-05T23:54:01.965859: step 1123, loss 0.171988, acc 0.94
2016-09-05T23:54:02.752048: step 1124, loss 0.205292, acc 0.9
2016-09-05T23:54:03.554554: step 1125, loss 0.0141355, acc 1
2016-09-05T23:54:04.387300: step 1126, loss 0.0801594, acc 0.96
2016-09-05T23:54:05.165705: step 1127, loss 0.178715, acc 0.92
2016-09-05T23:54:05.963987: step 1128, loss 0.214841, acc 0.94
2016-09-05T23:54:06.791266: step 1129, loss 0.180389, acc 0.94
2016-09-05T23:54:07.600521: step 1130, loss 0.241148, acc 0.86
2016-09-05T23:54:08.397111: step 1131, loss 0.0357542, acc 0.98
2016-09-05T23:54:09.227955: step 1132, loss 0.093786, acc 0.96
2016-09-05T23:54:10.017071: step 1133, loss 0.172277, acc 0.92
2016-09-05T23:54:10.811007: step 1134, loss 0.0554281, acc 0.98
2016-09-05T23:54:11.635847: step 1135, loss 0.108463, acc 0.96
2016-09-05T23:54:12.414841: step 1136, loss 0.106046, acc 0.96
2016-09-05T23:54:13.199966: step 1137, loss 0.0794051, acc 0.96
2016-09-05T23:54:14.003000: step 1138, loss 0.131773, acc 0.92
2016-09-05T23:54:14.812274: step 1139, loss 0.134385, acc 0.96
2016-09-05T23:54:15.641223: step 1140, loss 0.0686123, acc 0.96
2016-09-05T23:54:16.476922: step 1141, loss 0.0752973, acc 0.96
2016-09-05T23:54:17.250749: step 1142, loss 0.0742353, acc 0.98
2016-09-05T23:54:18.052792: step 1143, loss 0.145143, acc 0.9
2016-09-05T23:54:18.878772: step 1144, loss 0.0798764, acc 0.98
2016-09-05T23:54:19.678632: step 1145, loss 0.165157, acc 0.96
2016-09-05T23:54:20.489909: step 1146, loss 0.244995, acc 0.94
2016-09-05T23:54:21.298606: step 1147, loss 0.165308, acc 0.92
2016-09-05T23:54:22.077757: step 1148, loss 0.0648914, acc 0.98
2016-09-05T23:54:22.885724: step 1149, loss 0.0727054, acc 0.98
2016-09-05T23:54:23.710741: step 1150, loss 0.112345, acc 0.94
2016-09-05T23:54:24.516434: step 1151, loss 0.0716843, acc 0.98
2016-09-05T23:54:25.263241: step 1152, loss 0.163522, acc 0.977273
2016-09-05T23:54:26.097270: step 1153, loss 0.246272, acc 0.9
2016-09-05T23:54:26.885459: step 1154, loss 0.0973227, acc 0.96
2016-09-05T23:54:27.683179: step 1155, loss 0.112832, acc 0.94
2016-09-05T23:54:28.492097: step 1156, loss 0.0801916, acc 1
2016-09-05T23:54:29.280459: step 1157, loss 0.0337855, acc 1
2016-09-05T23:54:30.107486: step 1158, loss 0.0567664, acc 0.98
2016-09-05T23:54:30.905419: step 1159, loss 0.136739, acc 0.92
2016-09-05T23:54:31.695665: step 1160, loss 0.049597, acc 1
2016-09-05T23:54:32.514955: step 1161, loss 0.0912869, acc 0.96
2016-09-05T23:54:33.309116: step 1162, loss 0.0820026, acc 0.96
2016-09-05T23:54:34.100282: step 1163, loss 0.0567184, acc 0.98
2016-09-05T23:54:34.902769: step 1164, loss 0.118522, acc 0.96
2016-09-05T23:54:35.712480: step 1165, loss 0.0503732, acc 0.98
2016-09-05T23:54:36.524750: step 1166, loss 0.0661293, acc 0.98
2016-09-05T23:54:37.336253: step 1167, loss 0.0528052, acc 0.98
2016-09-05T23:54:38.163352: step 1168, loss 0.0422567, acc 0.98
2016-09-05T23:54:38.983460: step 1169, loss 0.0518171, acc 0.98
2016-09-05T23:54:39.796166: step 1170, loss 0.0652852, acc 0.96
2016-09-05T23:54:40.602327: step 1171, loss 0.0785296, acc 0.96
2016-09-05T23:54:41.388327: step 1172, loss 0.042207, acc 0.98
2016-09-05T23:54:42.198447: step 1173, loss 0.0566997, acc 0.98
2016-09-05T23:54:43.036782: step 1174, loss 0.104503, acc 0.94
2016-09-05T23:54:43.821546: step 1175, loss 0.0314859, acc 1
2016-09-05T23:54:44.649418: step 1176, loss 0.0619831, acc 0.98
2016-09-05T23:54:45.471373: step 1177, loss 0.0983651, acc 0.98
2016-09-05T23:54:46.268771: step 1178, loss 0.112233, acc 0.96
2016-09-05T23:54:47.068967: step 1179, loss 0.200584, acc 0.94
2016-09-05T23:54:47.877186: step 1180, loss 0.0557301, acc 0.98
2016-09-05T23:54:48.642776: step 1181, loss 0.0845982, acc 0.96
2016-09-05T23:54:49.433223: step 1182, loss 0.0152894, acc 1
2016-09-05T23:54:50.243142: step 1183, loss 0.149273, acc 0.98
2016-09-05T23:54:51.033408: step 1184, loss 0.0957112, acc 0.96
2016-09-05T23:54:51.833165: step 1185, loss 0.0504489, acc 0.98
2016-09-05T23:54:52.648485: step 1186, loss 0.0490409, acc 0.98
2016-09-05T23:54:53.418898: step 1187, loss 0.0324379, acc 0.98
2016-09-05T23:54:54.228235: step 1188, loss 0.0624413, acc 0.98
2016-09-05T23:54:55.041816: step 1189, loss 0.138643, acc 0.94
2016-09-05T23:54:55.845270: step 1190, loss 0.0664512, acc 0.98
2016-09-05T23:54:56.663085: step 1191, loss 0.0486307, acc 0.96
2016-09-05T23:54:57.480992: step 1192, loss 0.0585462, acc 0.98
2016-09-05T23:54:58.323418: step 1193, loss 0.180029, acc 0.94
2016-09-05T23:54:59.155480: step 1194, loss 0.0653254, acc 0.98
2016-09-05T23:54:59.987734: step 1195, loss 0.0953743, acc 0.94
2016-09-05T23:55:00.811242: step 1196, loss 0.054561, acc 0.98
2016-09-05T23:55:01.602728: step 1197, loss 0.0851571, acc 0.98
2016-09-05T23:55:02.434686: step 1198, loss 0.085848, acc 0.96
2016-09-05T23:55:03.247212: step 1199, loss 0.164368, acc 0.92
2016-09-05T23:55:04.062192: step 1200, loss 0.051539, acc 1

Evaluation:
2016-09-05T23:55:07.814013: step 1200, loss 1.18267, acc 0.752345

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-1200

2016-09-05T23:55:09.679386: step 1201, loss 0.0788709, acc 0.98
2016-09-05T23:55:10.456353: step 1202, loss 0.174845, acc 0.94
2016-09-05T23:55:11.229606: step 1203, loss 0.107773, acc 0.94
2016-09-05T23:55:12.033508: step 1204, loss 0.100304, acc 0.96
2016-09-05T23:55:12.859244: step 1205, loss 0.0983928, acc 0.92
2016-09-05T23:55:13.669245: step 1206, loss 0.074254, acc 0.96
2016-09-05T23:55:14.507617: step 1207, loss 0.0838785, acc 0.96
2016-09-05T23:55:15.320627: step 1208, loss 0.0282736, acc 0.98
2016-09-05T23:55:16.117565: step 1209, loss 0.159859, acc 0.94
2016-09-05T23:55:16.936901: step 1210, loss 0.0675894, acc 0.96
2016-09-05T23:55:17.726265: step 1211, loss 0.0182545, acc 1
2016-09-05T23:55:18.497393: step 1212, loss 0.137786, acc 0.96
2016-09-05T23:55:19.306972: step 1213, loss 0.190511, acc 0.92
2016-09-05T23:55:20.081450: step 1214, loss 0.0902606, acc 0.96
2016-09-05T23:55:20.889322: step 1215, loss 0.0526566, acc 0.96
2016-09-05T23:55:21.686332: step 1216, loss 0.113675, acc 0.96
2016-09-05T23:55:22.482856: step 1217, loss 0.0578075, acc 0.98
2016-09-05T23:55:23.347809: step 1218, loss 0.0639629, acc 0.96
2016-09-05T23:55:24.178081: step 1219, loss 0.058994, acc 0.98
2016-09-05T23:55:24.941365: step 1220, loss 0.0743346, acc 1
2016-09-05T23:55:25.729807: step 1221, loss 0.0638453, acc 1
2016-09-05T23:55:26.561347: step 1222, loss 0.0498507, acc 0.98
2016-09-05T23:55:27.375868: step 1223, loss 0.0840221, acc 0.96
2016-09-05T23:55:28.163791: step 1224, loss 0.0389285, acc 0.98
2016-09-05T23:55:28.978163: step 1225, loss 0.0978689, acc 0.94
2016-09-05T23:55:29.817528: step 1226, loss 0.0829798, acc 0.96
2016-09-05T23:55:30.626695: step 1227, loss 0.0987776, acc 0.98
2016-09-05T23:55:31.438395: step 1228, loss 0.141658, acc 0.94
2016-09-05T23:55:32.203858: step 1229, loss 0.304657, acc 0.92
2016-09-05T23:55:33.014804: step 1230, loss 0.0730302, acc 0.98
2016-09-05T23:55:33.830282: step 1231, loss 0.078122, acc 1
2016-09-05T23:55:34.615228: step 1232, loss 0.117448, acc 0.98
2016-09-05T23:55:35.429652: step 1233, loss 0.0689168, acc 0.96
2016-09-05T23:55:36.235986: step 1234, loss 0.0273629, acc 1
2016-09-05T23:55:37.028259: step 1235, loss 0.181506, acc 0.96
2016-09-05T23:55:37.833263: step 1236, loss 0.0520535, acc 0.98
2016-09-05T23:55:38.682763: step 1237, loss 0.128179, acc 0.94
2016-09-05T23:55:39.483116: step 1238, loss 0.0417736, acc 0.98
2016-09-05T23:55:40.267909: step 1239, loss 0.0419225, acc 0.98
2016-09-05T23:55:41.073129: step 1240, loss 0.147304, acc 0.94
2016-09-05T23:55:41.880739: step 1241, loss 0.0275998, acc 1
2016-09-05T23:55:42.696936: step 1242, loss 0.0465835, acc 0.98
2016-09-05T23:55:43.514372: step 1243, loss 0.110562, acc 0.94
2016-09-05T23:55:44.279007: step 1244, loss 0.0596698, acc 0.98
2016-09-05T23:55:45.081718: step 1245, loss 0.211707, acc 0.92
2016-09-05T23:55:45.932865: step 1246, loss 0.0327916, acc 0.98
2016-09-05T23:55:46.704374: step 1247, loss 0.0165072, acc 1
2016-09-05T23:55:47.505226: step 1248, loss 0.103585, acc 0.96
2016-09-05T23:55:48.310728: step 1249, loss 0.0430912, acc 0.98
2016-09-05T23:55:49.108769: step 1250, loss 0.064677, acc 0.96
2016-09-05T23:55:49.908690: step 1251, loss 0.0986021, acc 0.96
2016-09-05T23:55:50.717690: step 1252, loss 0.277105, acc 0.88
2016-09-05T23:55:51.492102: step 1253, loss 0.0627427, acc 0.98
2016-09-05T23:55:52.317809: step 1254, loss 0.0473302, acc 0.96
2016-09-05T23:55:53.100481: step 1255, loss 0.0769109, acc 0.96
2016-09-05T23:55:53.893720: step 1256, loss 0.107111, acc 0.96
2016-09-05T23:55:54.724303: step 1257, loss 0.0975157, acc 0.96
2016-09-05T23:55:55.528643: step 1258, loss 0.126236, acc 0.94
2016-09-05T23:55:56.332107: step 1259, loss 0.0556686, acc 0.98
2016-09-05T23:55:57.147548: step 1260, loss 0.0830484, acc 0.94
2016-09-05T23:55:57.966633: step 1261, loss 0.121381, acc 0.9
2016-09-05T23:55:58.738141: step 1262, loss 0.138242, acc 0.96
2016-09-05T23:55:59.549590: step 1263, loss 0.0432461, acc 1
2016-09-05T23:56:00.410390: step 1264, loss 0.163901, acc 0.92
2016-09-05T23:56:01.199177: step 1265, loss 0.172245, acc 0.92
2016-09-05T23:56:01.990096: step 1266, loss 0.0310084, acc 0.98
2016-09-05T23:56:02.809293: step 1267, loss 0.0884474, acc 0.94
2016-09-05T23:56:03.597793: step 1268, loss 0.0885804, acc 0.94
2016-09-05T23:56:04.392342: step 1269, loss 0.211833, acc 0.9
2016-09-05T23:56:05.244840: step 1270, loss 0.0577663, acc 0.98
2016-09-05T23:56:06.059533: step 1271, loss 0.125326, acc 0.94
2016-09-05T23:56:06.883755: step 1272, loss 0.0431873, acc 1
2016-09-05T23:56:07.691771: step 1273, loss 0.0289559, acc 1
2016-09-05T23:56:08.466742: step 1274, loss 0.15024, acc 0.96
2016-09-05T23:56:09.267846: step 1275, loss 0.147046, acc 0.94
2016-09-05T23:56:10.101004: step 1276, loss 0.153943, acc 0.92
2016-09-05T23:56:10.879620: step 1277, loss 0.0350261, acc 1
2016-09-05T23:56:11.678206: step 1278, loss 0.0359651, acc 1
2016-09-05T23:56:12.508115: step 1279, loss 0.111574, acc 0.96
2016-09-05T23:56:13.314538: step 1280, loss 0.136547, acc 0.92
2016-09-05T23:56:14.088211: step 1281, loss 0.149084, acc 0.94
2016-09-05T23:56:14.910363: step 1282, loss 0.0462121, acc 1
2016-09-05T23:56:15.699486: step 1283, loss 0.0726378, acc 0.96
2016-09-05T23:56:16.480356: step 1284, loss 0.0554568, acc 1
2016-09-05T23:56:17.296523: step 1285, loss 0.0512535, acc 0.98
2016-09-05T23:56:18.099926: step 1286, loss 0.0590668, acc 0.98
2016-09-05T23:56:18.900474: step 1287, loss 0.118753, acc 0.94
2016-09-05T23:56:19.711598: step 1288, loss 0.14354, acc 0.96
2016-09-05T23:56:20.497815: step 1289, loss 0.166534, acc 0.94
2016-09-05T23:56:21.313592: step 1290, loss 0.101691, acc 0.94
2016-09-05T23:56:22.121112: step 1291, loss 0.0343542, acc 1
2016-09-05T23:56:22.928236: step 1292, loss 0.034319, acc 1
2016-09-05T23:56:23.744277: step 1293, loss 0.203219, acc 0.92
2016-09-05T23:56:24.570802: step 1294, loss 0.0563808, acc 0.98
2016-09-05T23:56:25.361126: step 1295, loss 0.0306019, acc 1
2016-09-05T23:56:26.138931: step 1296, loss 0.134582, acc 0.96
2016-09-05T23:56:26.948479: step 1297, loss 0.105075, acc 0.94
2016-09-05T23:56:27.727631: step 1298, loss 0.0376819, acc 1
2016-09-05T23:56:28.536092: step 1299, loss 0.122239, acc 0.98
2016-09-05T23:56:29.342182: step 1300, loss 0.102845, acc 0.98

Evaluation:
2016-09-05T23:56:33.096624: step 1300, loss 1.13493, acc 0.746717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-1300

2016-09-05T23:56:34.948507: step 1301, loss 0.0894446, acc 0.98
2016-09-05T23:56:35.808175: step 1302, loss 0.0867095, acc 0.98
2016-09-05T23:56:36.626635: step 1303, loss 0.0465014, acc 0.98
2016-09-05T23:56:37.433116: step 1304, loss 0.0463416, acc 0.98
2016-09-05T23:56:38.251963: step 1305, loss 0.136444, acc 0.94
2016-09-05T23:56:39.057445: step 1306, loss 0.109558, acc 0.96
2016-09-05T23:56:39.863793: step 1307, loss 0.0756214, acc 0.98
2016-09-05T23:56:40.678548: step 1308, loss 0.196325, acc 0.94
2016-09-05T23:56:41.472053: step 1309, loss 0.0579869, acc 0.96
2016-09-05T23:56:42.273851: step 1310, loss 0.0931603, acc 0.96
2016-09-05T23:56:43.096429: step 1311, loss 0.124815, acc 0.96
2016-09-05T23:56:43.898835: step 1312, loss 0.090919, acc 0.96
2016-09-05T23:56:44.693815: step 1313, loss 0.152709, acc 0.94
2016-09-05T23:56:45.521862: step 1314, loss 0.0696974, acc 0.98
2016-09-05T23:56:46.319290: step 1315, loss 0.0808433, acc 0.98
2016-09-05T23:56:47.134887: step 1316, loss 0.0382167, acc 1
2016-09-05T23:56:47.964202: step 1317, loss 0.0202273, acc 1
2016-09-05T23:56:48.785405: step 1318, loss 0.0698976, acc 0.98
2016-09-05T23:56:49.582244: step 1319, loss 0.0620552, acc 0.98
2016-09-05T23:56:50.397875: step 1320, loss 0.0788058, acc 0.98
2016-09-05T23:56:51.169092: step 1321, loss 0.0606323, acc 1
2016-09-05T23:56:51.977669: step 1322, loss 0.0488738, acc 0.98
2016-09-05T23:56:52.812810: step 1323, loss 0.0720603, acc 0.98
2016-09-05T23:56:53.621232: step 1324, loss 0.130266, acc 0.96
2016-09-05T23:56:54.436465: step 1325, loss 0.103546, acc 0.94
2016-09-05T23:56:55.247009: step 1326, loss 0.0437142, acc 1
2016-09-05T23:56:56.041614: step 1327, loss 0.199028, acc 0.88
2016-09-05T23:56:56.851936: step 1328, loss 0.136718, acc 0.96
2016-09-05T23:56:57.660455: step 1329, loss 0.116162, acc 0.94
2016-09-05T23:56:58.461597: step 1330, loss 0.115604, acc 0.94
2016-09-05T23:56:59.242510: step 1331, loss 0.0668796, acc 0.96
2016-09-05T23:57:00.107222: step 1332, loss 0.153572, acc 0.96
2016-09-05T23:57:00.940480: step 1333, loss 0.0512288, acc 0.98
2016-09-05T23:57:01.752610: step 1334, loss 0.0688765, acc 0.96
2016-09-05T23:57:02.548262: step 1335, loss 0.0354072, acc 1
2016-09-05T23:57:03.393860: step 1336, loss 0.0869832, acc 0.96
2016-09-05T23:57:04.166649: step 1337, loss 0.0884417, acc 0.98
2016-09-05T23:57:04.956078: step 1338, loss 0.0760549, acc 0.96
2016-09-05T23:57:05.815504: step 1339, loss 0.148406, acc 0.92
2016-09-05T23:57:06.612888: step 1340, loss 0.0307197, acc 1
2016-09-05T23:57:07.413425: step 1341, loss 0.0892324, acc 0.96
2016-09-05T23:57:08.247055: step 1342, loss 0.13629, acc 0.94
2016-09-05T23:57:09.050987: step 1343, loss 0.1131, acc 0.96
2016-09-05T23:57:09.796800: step 1344, loss 0.16217, acc 0.954545
2016-09-05T23:57:10.606988: step 1345, loss 0.0535748, acc 0.98
2016-09-05T23:57:11.384524: step 1346, loss 0.0834158, acc 0.96
2016-09-05T23:57:12.206254: step 1347, loss 0.035754, acc 1
2016-09-05T23:57:13.009335: step 1348, loss 0.0831036, acc 0.94
2016-09-05T23:57:13.794004: step 1349, loss 0.0816851, acc 0.94
2016-09-05T23:57:14.627070: step 1350, loss 0.0982493, acc 0.96
2016-09-05T23:57:15.447043: step 1351, loss 0.0363668, acc 0.98
2016-09-05T23:57:16.243263: step 1352, loss 0.0686226, acc 0.96
2016-09-05T23:57:17.059681: step 1353, loss 0.0649746, acc 0.96
2016-09-05T23:57:17.895133: step 1354, loss 0.0417361, acc 0.98
2016-09-05T23:57:18.714103: step 1355, loss 0.0933325, acc 0.94
2016-09-05T23:57:19.560577: step 1356, loss 0.115494, acc 0.96
2016-09-05T23:57:20.380229: step 1357, loss 0.038234, acc 0.98
2016-09-05T23:57:21.199245: step 1358, loss 0.0190046, acc 1
2016-09-05T23:57:22.000481: step 1359, loss 0.0595896, acc 1
2016-09-05T23:57:22.857010: step 1360, loss 0.0900989, acc 0.98
2016-09-05T23:57:23.654945: step 1361, loss 0.0122336, acc 1
2016-09-05T23:57:24.454122: step 1362, loss 0.0233258, acc 1
2016-09-05T23:57:25.266135: step 1363, loss 0.0810165, acc 0.96
2016-09-05T23:57:26.049070: step 1364, loss 0.0672941, acc 0.96
2016-09-05T23:57:26.886694: step 1365, loss 0.0277377, acc 0.98
2016-09-05T23:57:27.706052: step 1366, loss 0.0449431, acc 0.98
2016-09-05T23:57:28.575723: step 1367, loss 0.0831799, acc 0.94
2016-09-05T23:57:29.365380: step 1368, loss 0.115726, acc 0.94
2016-09-05T23:57:30.205949: step 1369, loss 0.0653968, acc 0.98
2016-09-05T23:57:31.017933: step 1370, loss 0.166673, acc 0.94
2016-09-05T23:57:31.831510: step 1371, loss 0.0261876, acc 0.98
2016-09-05T23:57:32.647669: step 1372, loss 0.0750664, acc 0.98
2016-09-05T23:57:33.456623: step 1373, loss 0.154321, acc 0.94
2016-09-05T23:57:34.279676: step 1374, loss 0.0874573, acc 0.96
2016-09-05T23:57:35.125423: step 1375, loss 0.0273447, acc 1
2016-09-05T23:57:35.935707: step 1376, loss 0.0312843, acc 1
2016-09-05T23:57:36.735868: step 1377, loss 0.146111, acc 0.96
2016-09-05T23:57:37.558633: step 1378, loss 0.0869975, acc 0.94
2016-09-05T23:57:38.409692: step 1379, loss 0.0338804, acc 1
2016-09-05T23:57:39.195283: step 1380, loss 0.0441965, acc 0.98
2016-09-05T23:57:39.989703: step 1381, loss 0.00946892, acc 1
2016-09-05T23:57:40.797430: step 1382, loss 0.0554962, acc 0.98
2016-09-05T23:57:41.597322: step 1383, loss 0.0525755, acc 0.98
2016-09-05T23:57:42.391817: step 1384, loss 0.202301, acc 0.96
2016-09-05T23:57:43.234745: step 1385, loss 0.0787125, acc 0.94
2016-09-05T23:57:44.041347: step 1386, loss 0.0472995, acc 0.98
2016-09-05T23:57:44.828593: step 1387, loss 0.0712479, acc 0.98
2016-09-05T23:57:45.638203: step 1388, loss 0.0618392, acc 0.96
2016-09-05T23:57:46.426011: step 1389, loss 0.0440587, acc 0.98
2016-09-05T23:57:47.248278: step 1390, loss 0.202655, acc 0.9
2016-09-05T23:57:48.073940: step 1391, loss 0.135625, acc 0.92
2016-09-05T23:57:48.844777: step 1392, loss 0.0554985, acc 0.98
2016-09-05T23:57:49.653619: step 1393, loss 0.087876, acc 0.94
2016-09-05T23:57:50.458242: step 1394, loss 0.0615147, acc 0.98
2016-09-05T23:57:51.244780: step 1395, loss 0.119822, acc 0.94
2016-09-05T23:57:52.064431: step 1396, loss 0.0698421, acc 0.98
2016-09-05T23:57:52.866868: step 1397, loss 0.111239, acc 0.96
2016-09-05T23:57:53.715168: step 1398, loss 0.0422681, acc 1
2016-09-05T23:57:54.515201: step 1399, loss 0.0825264, acc 0.96
2016-09-05T23:57:55.332312: step 1400, loss 0.062243, acc 0.98

Evaluation:
2016-09-05T23:57:59.063949: step 1400, loss 1.12222, acc 0.745779

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-1400

2016-09-05T23:58:00.890880: step 1401, loss 0.127333, acc 0.96
2016-09-05T23:58:01.733410: step 1402, loss 0.0689717, acc 0.96
2016-09-05T23:58:02.547805: step 1403, loss 0.105482, acc 0.94
2016-09-05T23:58:03.394084: step 1404, loss 0.041787, acc 0.98
2016-09-05T23:58:04.246245: step 1405, loss 0.0889878, acc 0.96
2016-09-05T23:58:05.076271: step 1406, loss 0.0424716, acc 1
2016-09-05T23:58:05.880835: step 1407, loss 0.0255263, acc 1
2016-09-05T23:58:06.700340: step 1408, loss 0.0681204, acc 0.96
2016-09-05T23:58:07.508293: step 1409, loss 0.062063, acc 0.98
2016-09-05T23:58:08.315500: step 1410, loss 0.1027, acc 0.96
2016-09-05T23:58:09.155829: step 1411, loss 0.0495901, acc 0.98
2016-09-05T23:58:09.968327: step 1412, loss 0.0863102, acc 0.96
2016-09-05T23:58:10.786785: step 1413, loss 0.0662395, acc 0.98
2016-09-05T23:58:11.593399: step 1414, loss 0.0415226, acc 0.98
2016-09-05T23:58:12.382920: step 1415, loss 0.141481, acc 0.96
2016-09-05T23:58:13.174818: step 1416, loss 0.078657, acc 0.98
2016-09-05T23:58:13.974809: step 1417, loss 0.0675027, acc 0.96
2016-09-05T23:58:14.825047: step 1418, loss 0.0630932, acc 0.96
2016-09-05T23:58:15.601798: step 1419, loss 0.101759, acc 0.94
2016-09-05T23:58:16.437410: step 1420, loss 0.0363982, acc 0.98
2016-09-05T23:58:17.257456: step 1421, loss 0.0794716, acc 0.98
2016-09-05T23:58:18.027732: step 1422, loss 0.0527444, acc 0.96
2016-09-05T23:58:18.818566: step 1423, loss 0.115793, acc 0.94
2016-09-05T23:58:19.687897: step 1424, loss 0.0138762, acc 1
2016-09-05T23:58:20.484478: step 1425, loss 0.0257535, acc 1
2016-09-05T23:58:21.289552: step 1426, loss 0.0268086, acc 1
2016-09-05T23:58:22.095360: step 1427, loss 0.0756021, acc 0.96
2016-09-05T23:58:22.858599: step 1428, loss 0.0296079, acc 1
2016-09-05T23:58:23.654799: step 1429, loss 0.072977, acc 0.96
2016-09-05T23:58:24.473549: step 1430, loss 0.04144, acc 0.98
2016-09-05T23:58:25.266163: step 1431, loss 0.00941491, acc 1
2016-09-05T23:58:26.059569: step 1432, loss 0.151859, acc 0.92
2016-09-05T23:58:26.874931: step 1433, loss 0.0133717, acc 1
2016-09-05T23:58:27.645333: step 1434, loss 0.0909107, acc 0.98
2016-09-05T23:58:28.457639: step 1435, loss 0.0550556, acc 0.96
2016-09-05T23:58:29.267368: step 1436, loss 0.0274509, acc 0.98
2016-09-05T23:58:30.057018: step 1437, loss 0.062193, acc 0.96
2016-09-05T23:58:30.872156: step 1438, loss 0.0254342, acc 1
2016-09-05T23:58:31.674618: step 1439, loss 0.0575417, acc 0.98
2016-09-05T23:58:32.454639: step 1440, loss 0.0685955, acc 0.98
2016-09-05T23:58:33.276815: step 1441, loss 0.123376, acc 0.96
2016-09-05T23:58:34.078313: step 1442, loss 0.024055, acc 1
2016-09-05T23:58:34.902743: step 1443, loss 0.138466, acc 0.96
2016-09-05T23:58:35.723076: step 1444, loss 0.0734797, acc 0.98
2016-09-05T23:58:36.555171: step 1445, loss 0.0757153, acc 0.98
2016-09-05T23:58:37.349630: step 1446, loss 0.053976, acc 0.98
2016-09-05T23:58:38.164851: step 1447, loss 0.00909306, acc 1
2016-09-05T23:58:39.011379: step 1448, loss 0.0202471, acc 1
2016-09-05T23:58:39.803848: step 1449, loss 0.0858003, acc 0.96
2016-09-05T23:58:40.587078: step 1450, loss 0.0794342, acc 0.96
2016-09-05T23:58:41.400865: step 1451, loss 0.139498, acc 0.94
2016-09-05T23:58:42.172760: step 1452, loss 0.0848269, acc 0.98
2016-09-05T23:58:42.990448: step 1453, loss 0.083018, acc 0.94
2016-09-05T23:58:43.791395: step 1454, loss 0.113972, acc 0.94
2016-09-05T23:58:44.555062: step 1455, loss 0.055601, acc 0.98
2016-09-05T23:58:45.380237: step 1456, loss 0.0649031, acc 0.98
2016-09-05T23:58:46.189471: step 1457, loss 0.0314797, acc 1
2016-09-05T23:58:46.984444: step 1458, loss 0.161931, acc 0.92
2016-09-05T23:58:47.788364: step 1459, loss 0.0666017, acc 0.96
2016-09-05T23:58:48.601520: step 1460, loss 0.0231689, acc 1
2016-09-05T23:58:49.390784: step 1461, loss 0.130619, acc 0.94
2016-09-05T23:58:50.207375: step 1462, loss 0.0776414, acc 0.96
2016-09-05T23:58:51.014091: step 1463, loss 0.0767819, acc 0.98
2016-09-05T23:58:51.803577: step 1464, loss 0.0887566, acc 0.94
2016-09-05T23:58:52.637907: step 1465, loss 0.100783, acc 0.94
2016-09-05T23:58:53.473021: step 1466, loss 0.0200916, acc 1
2016-09-05T23:58:54.276340: step 1467, loss 0.0225929, acc 1
2016-09-05T23:58:55.097401: step 1468, loss 0.235459, acc 0.92
2016-09-05T23:58:55.911671: step 1469, loss 0.0707378, acc 0.98
2016-09-05T23:58:56.718924: step 1470, loss 0.0904879, acc 0.98
2016-09-05T23:58:57.533853: step 1471, loss 0.0619238, acc 0.96
2016-09-05T23:58:58.334115: step 1472, loss 0.105328, acc 0.96
2016-09-05T23:58:59.089767: step 1473, loss 0.0969638, acc 0.94
2016-09-05T23:58:59.874526: step 1474, loss 0.0451382, acc 0.98
2016-09-05T23:59:00.697211: step 1475, loss 0.0323232, acc 1
2016-09-05T23:59:01.524324: step 1476, loss 0.0645968, acc 0.96
2016-09-05T23:59:02.359986: step 1477, loss 0.0610128, acc 0.96
2016-09-05T23:59:03.174272: step 1478, loss 0.0405718, acc 1
2016-09-05T23:59:03.962248: step 1479, loss 0.0115376, acc 1
2016-09-05T23:59:04.764165: step 1480, loss 0.115945, acc 0.96
2016-09-05T23:59:05.568383: step 1481, loss 0.0399912, acc 0.98
2016-09-05T23:59:06.334731: step 1482, loss 0.0858655, acc 0.96
2016-09-05T23:59:07.134503: step 1483, loss 0.0333827, acc 0.98
2016-09-05T23:59:07.944417: step 1484, loss 0.0662867, acc 0.98
2016-09-05T23:59:08.732596: step 1485, loss 0.184812, acc 0.94
2016-09-05T23:59:09.553275: step 1486, loss 0.113154, acc 0.96
2016-09-05T23:59:10.358149: step 1487, loss 0.0673571, acc 0.98
2016-09-05T23:59:11.170695: step 1488, loss 0.246583, acc 0.94
2016-09-05T23:59:11.976033: step 1489, loss 0.0620208, acc 0.98
2016-09-05T23:59:12.789838: step 1490, loss 0.0964183, acc 0.98
2016-09-05T23:59:13.600240: step 1491, loss 0.152763, acc 0.98
2016-09-05T23:59:14.394202: step 1492, loss 0.0587574, acc 0.98
2016-09-05T23:59:15.192848: step 1493, loss 0.0643146, acc 0.96
2016-09-05T23:59:15.984362: step 1494, loss 0.0554913, acc 0.98
2016-09-05T23:59:16.772109: step 1495, loss 0.104622, acc 0.92
2016-09-05T23:59:17.569430: step 1496, loss 0.0989481, acc 0.96
2016-09-05T23:59:18.368266: step 1497, loss 0.0424416, acc 1
2016-09-05T23:59:19.199513: step 1498, loss 0.0488329, acc 0.96
2016-09-05T23:59:20.036593: step 1499, loss 0.101036, acc 0.94
2016-09-05T23:59:20.816423: step 1500, loss 0.101419, acc 0.96

Evaluation:
2016-09-05T23:59:24.531483: step 1500, loss 0.991931, acc 0.763602

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-1500

2016-09-05T23:59:26.483428: step 1501, loss 0.0798295, acc 0.94
2016-09-05T23:59:27.320501: step 1502, loss 0.0832167, acc 0.96
2016-09-05T23:59:28.154538: step 1503, loss 0.079037, acc 0.98
2016-09-05T23:59:28.979642: step 1504, loss 0.154476, acc 0.94
2016-09-05T23:59:29.799366: step 1505, loss 0.0321821, acc 1
2016-09-05T23:59:30.597642: step 1506, loss 0.040715, acc 0.96
2016-09-05T23:59:31.400035: step 1507, loss 0.115576, acc 0.94
2016-09-05T23:59:32.228746: step 1508, loss 0.0820231, acc 0.94
2016-09-05T23:59:33.020820: step 1509, loss 0.0353079, acc 1
2016-09-05T23:59:33.808185: step 1510, loss 0.0889091, acc 0.94
2016-09-05T23:59:34.602247: step 1511, loss 0.0845289, acc 0.98
2016-09-05T23:59:35.391468: step 1512, loss 0.0424118, acc 1
2016-09-05T23:59:36.226486: step 1513, loss 0.111768, acc 0.96
2016-09-05T23:59:37.041812: step 1514, loss 0.0611806, acc 0.98
2016-09-05T23:59:37.813286: step 1515, loss 0.0719297, acc 0.96
2016-09-05T23:59:38.627870: step 1516, loss 0.130135, acc 0.94
2016-09-05T23:59:39.436640: step 1517, loss 0.0546444, acc 0.98
2016-09-05T23:59:40.225201: step 1518, loss 0.0245934, acc 1
2016-09-05T23:59:41.028011: step 1519, loss 0.20425, acc 0.94
2016-09-05T23:59:41.826085: step 1520, loss 0.0448048, acc 0.98
2016-09-05T23:59:42.621372: step 1521, loss 0.109582, acc 0.94
2016-09-05T23:59:43.471918: step 1522, loss 0.0567639, acc 0.98
2016-09-05T23:59:44.286464: step 1523, loss 0.154493, acc 0.92
2016-09-05T23:59:45.106335: step 1524, loss 0.0546127, acc 0.98
2016-09-05T23:59:45.911267: step 1525, loss 0.0734331, acc 0.96
2016-09-05T23:59:46.728358: step 1526, loss 0.201286, acc 0.92
2016-09-05T23:59:47.513675: step 1527, loss 0.122458, acc 0.94
2016-09-05T23:59:48.307065: step 1528, loss 0.031305, acc 0.98
2016-09-05T23:59:49.138538: step 1529, loss 0.0540022, acc 1
2016-09-05T23:59:49.943631: step 1530, loss 0.0606595, acc 0.98
2016-09-05T23:59:50.757543: step 1531, loss 0.0254325, acc 0.98
2016-09-05T23:59:51.605354: step 1532, loss 0.0895694, acc 0.96
2016-09-05T23:59:52.375546: step 1533, loss 0.0384426, acc 1
2016-09-05T23:59:53.162084: step 1534, loss 0.0256956, acc 1
2016-09-05T23:59:54.004893: step 1535, loss 0.0745521, acc 0.98
2016-09-05T23:59:54.733678: step 1536, loss 0.0820789, acc 0.954545
2016-09-05T23:59:55.539813: step 1537, loss 0.0326123, acc 1
2016-09-05T23:59:56.343491: step 1538, loss 0.0581051, acc 0.96
2016-09-05T23:59:57.154837: step 1539, loss 0.0183145, acc 1
2016-09-05T23:59:58.002901: step 1540, loss 0.0922528, acc 0.96
2016-09-05T23:59:58.808568: step 1541, loss 0.0481466, acc 0.98
2016-09-05T23:59:59.578343: step 1542, loss 0.0977037, acc 0.98
2016-09-06T00:00:00.437220: step 1543, loss 0.0136079, acc 1
2016-09-06T00:00:01.267694: step 1544, loss 0.0778356, acc 0.98
2016-09-06T00:00:02.041526: step 1545, loss 0.140008, acc 0.92
2016-09-06T00:00:02.834706: step 1546, loss 0.0629806, acc 0.96
2016-09-06T00:00:03.640898: step 1547, loss 0.0628292, acc 0.96
2016-09-06T00:00:04.429154: step 1548, loss 0.14754, acc 0.98
2016-09-06T00:00:05.227765: step 1549, loss 0.0455022, acc 0.98
2016-09-06T00:00:06.056855: step 1550, loss 0.0317075, acc 0.98
2016-09-06T00:00:06.845411: step 1551, loss 0.0331734, acc 0.98
2016-09-06T00:00:07.634116: step 1552, loss 0.0718572, acc 0.96
2016-09-06T00:00:08.443604: step 1553, loss 0.0737605, acc 0.96
2016-09-06T00:00:09.266631: step 1554, loss 0.02218, acc 0.98
2016-09-06T00:00:10.092829: step 1555, loss 0.102128, acc 0.92
2016-09-06T00:00:10.940414: step 1556, loss 0.0815473, acc 0.94
2016-09-06T00:00:11.747044: step 1557, loss 0.0603845, acc 0.96
2016-09-06T00:00:12.524388: step 1558, loss 0.0197817, acc 1
2016-09-06T00:00:13.358199: step 1559, loss 0.0738794, acc 0.98
2016-09-06T00:00:14.128787: step 1560, loss 0.0426738, acc 0.98
2016-09-06T00:00:14.981963: step 1561, loss 0.05533, acc 0.98
2016-09-06T00:00:15.794606: step 1562, loss 0.0480289, acc 1
2016-09-06T00:00:16.550036: step 1563, loss 0.0257847, acc 1
2016-09-06T00:00:17.349532: step 1564, loss 0.0258687, acc 0.98
2016-09-06T00:00:18.175079: step 1565, loss 0.0707433, acc 0.98
2016-09-06T00:00:18.960823: step 1566, loss 0.040492, acc 0.98
2016-09-06T00:00:19.843950: step 1567, loss 0.0744547, acc 0.96
2016-09-06T00:00:20.655419: step 1568, loss 0.0720509, acc 0.98
2016-09-06T00:00:21.450941: step 1569, loss 0.0829048, acc 0.98
2016-09-06T00:00:22.261389: step 1570, loss 0.103223, acc 0.96
2016-09-06T00:00:23.069604: step 1571, loss 0.018005, acc 1
2016-09-06T00:00:23.849572: step 1572, loss 0.0161011, acc 1
2016-09-06T00:00:24.650241: step 1573, loss 0.0194623, acc 0.98
2016-09-06T00:00:25.509852: step 1574, loss 0.112698, acc 0.96
2016-09-06T00:00:26.317637: step 1575, loss 0.0514681, acc 0.98
2016-09-06T00:00:27.141210: step 1576, loss 0.00898714, acc 1
2016-09-06T00:00:27.991299: step 1577, loss 0.304909, acc 0.94
2016-09-06T00:00:28.868806: step 1578, loss 0.161022, acc 0.94
2016-09-06T00:00:29.683093: step 1579, loss 0.0746176, acc 0.96
2016-09-06T00:00:30.503157: step 1580, loss 0.0840231, acc 0.96
2016-09-06T00:00:31.330436: step 1581, loss 0.0594376, acc 0.98
2016-09-06T00:00:32.167174: step 1582, loss 0.0638519, acc 0.96
2016-09-06T00:00:32.999638: step 1583, loss 0.0163642, acc 1
2016-09-06T00:00:33.782452: step 1584, loss 0.0738691, acc 0.96
2016-09-06T00:00:34.617335: step 1585, loss 0.0919333, acc 0.94
2016-09-06T00:00:35.450957: step 1586, loss 0.0991325, acc 0.96
2016-09-06T00:00:36.270100: step 1587, loss 0.0219196, acc 1
2016-09-06T00:00:37.061976: step 1588, loss 0.120681, acc 0.96
2016-09-06T00:00:37.851807: step 1589, loss 0.120463, acc 0.96
2016-09-06T00:00:38.663142: step 1590, loss 0.0606418, acc 0.96
2016-09-06T00:00:39.462310: step 1591, loss 0.126913, acc 0.92
2016-09-06T00:00:40.274943: step 1592, loss 0.17504, acc 0.92
2016-09-06T00:00:41.081446: step 1593, loss 0.0333791, acc 1
2016-09-06T00:00:41.889120: step 1594, loss 0.0807188, acc 0.98
2016-09-06T00:00:42.707887: step 1595, loss 0.0566261, acc 0.98
2016-09-06T00:00:43.573825: step 1596, loss 0.0586908, acc 0.98
2016-09-06T00:00:44.340022: step 1597, loss 0.103916, acc 0.96
2016-09-06T00:00:45.148562: step 1598, loss 0.0355083, acc 0.98
2016-09-06T00:00:45.974808: step 1599, loss 0.0213951, acc 0.98
2016-09-06T00:00:46.765764: step 1600, loss 0.0336089, acc 1

Evaluation:
2016-09-06T00:00:50.499843: step 1600, loss 1.11421, acc 0.751407

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-1600

2016-09-06T00:00:52.385590: step 1601, loss 0.123227, acc 0.94
2016-09-06T00:00:53.189999: step 1602, loss 0.0944737, acc 0.96
2016-09-06T00:00:54.012659: step 1603, loss 0.0501968, acc 0.98
2016-09-06T00:00:54.842250: step 1604, loss 0.0455447, acc 0.98
2016-09-06T00:00:55.680926: step 1605, loss 0.153373, acc 0.92
2016-09-06T00:00:56.496644: step 1606, loss 0.108437, acc 0.96
2016-09-06T00:00:57.327331: step 1607, loss 0.0673985, acc 0.96
2016-09-06T00:00:58.130459: step 1608, loss 0.043069, acc 0.98
2016-09-06T00:00:58.938711: step 1609, loss 0.0572877, acc 0.96
2016-09-06T00:00:59.757030: step 1610, loss 0.0620891, acc 0.96
2016-09-06T00:01:00.584288: step 1611, loss 0.0674888, acc 0.98
2016-09-06T00:01:01.373707: step 1612, loss 0.113101, acc 0.94
2016-09-06T00:01:02.179892: step 1613, loss 0.0405594, acc 0.98
2016-09-06T00:01:02.988120: step 1614, loss 0.0761957, acc 0.96
2016-09-06T00:01:03.782306: step 1615, loss 0.0701183, acc 1
2016-09-06T00:01:04.575832: step 1616, loss 0.0570688, acc 0.98
2016-09-06T00:01:05.379121: step 1617, loss 0.119794, acc 0.94
2016-09-06T00:01:06.202540: step 1618, loss 0.0268757, acc 0.98
2016-09-06T00:01:07.012618: step 1619, loss 0.123852, acc 0.96
2016-09-06T00:01:07.838911: step 1620, loss 0.0206276, acc 1
2016-09-06T00:01:08.622331: step 1621, loss 0.0607741, acc 0.96
2016-09-06T00:01:09.405102: step 1622, loss 0.051543, acc 1
2016-09-06T00:01:10.223590: step 1623, loss 0.0373862, acc 0.98
2016-09-06T00:01:11.030683: step 1624, loss 0.0290897, acc 1
2016-09-06T00:01:11.839758: step 1625, loss 0.0566108, acc 0.96
2016-09-06T00:01:12.678221: step 1626, loss 0.0176329, acc 1
2016-09-06T00:01:13.467611: step 1627, loss 0.103156, acc 0.96
2016-09-06T00:01:14.316703: step 1628, loss 0.129314, acc 0.96
2016-09-06T00:01:15.142187: step 1629, loss 0.1161, acc 0.94
2016-09-06T00:01:15.952858: step 1630, loss 0.105686, acc 0.96
2016-09-06T00:01:16.754737: step 1631, loss 0.0987087, acc 0.96
2016-09-06T00:01:17.566413: step 1632, loss 0.0206111, acc 1
2016-09-06T00:01:18.336115: step 1633, loss 0.0130975, acc 1
2016-09-06T00:01:19.107836: step 1634, loss 0.0851866, acc 0.96
2016-09-06T00:01:19.908504: step 1635, loss 0.101124, acc 0.94
2016-09-06T00:01:20.693036: step 1636, loss 0.14948, acc 0.94
2016-09-06T00:01:21.487383: step 1637, loss 0.0672641, acc 0.98
2016-09-06T00:01:22.298795: step 1638, loss 0.0478869, acc 0.98
2016-09-06T00:01:23.104526: step 1639, loss 0.0189744, acc 1
2016-09-06T00:01:23.907212: step 1640, loss 0.0384414, acc 0.98
2016-09-06T00:01:24.707783: step 1641, loss 0.0925641, acc 0.96
2016-09-06T00:01:25.499068: step 1642, loss 0.02484, acc 1
2016-09-06T00:01:26.350522: step 1643, loss 0.0701362, acc 0.96
2016-09-06T00:01:27.181848: step 1644, loss 0.066969, acc 0.96
2016-09-06T00:01:27.963731: step 1645, loss 0.0635914, acc 0.94
2016-09-06T00:01:28.797037: step 1646, loss 0.091061, acc 0.98
2016-09-06T00:01:29.577039: step 1647, loss 0.0196993, acc 1
2016-09-06T00:01:30.358630: step 1648, loss 0.100012, acc 0.96
2016-09-06T00:01:31.159949: step 1649, loss 0.104878, acc 0.94
2016-09-06T00:01:31.983616: step 1650, loss 0.0170997, acc 1
2016-09-06T00:01:32.769824: step 1651, loss 0.0554722, acc 0.98
2016-09-06T00:01:33.570848: step 1652, loss 0.0585914, acc 0.96
2016-09-06T00:01:34.386675: step 1653, loss 0.0268416, acc 1
2016-09-06T00:01:35.170494: step 1654, loss 0.0809809, acc 0.96
2016-09-06T00:01:35.968647: step 1655, loss 0.0732489, acc 0.98
2016-09-06T00:01:36.781282: step 1656, loss 0.0429625, acc 0.98
2016-09-06T00:01:37.619408: step 1657, loss 0.0130592, acc 1
2016-09-06T00:01:38.427405: step 1658, loss 0.0169398, acc 1
2016-09-06T00:01:39.262854: step 1659, loss 0.110164, acc 0.96
2016-09-06T00:01:40.054104: step 1660, loss 0.0257489, acc 1
2016-09-06T00:01:40.911652: step 1661, loss 0.0927814, acc 0.94
2016-09-06T00:01:41.729861: step 1662, loss 0.0174752, acc 1
2016-09-06T00:01:42.496264: step 1663, loss 0.0525933, acc 0.98
2016-09-06T00:01:43.264050: step 1664, loss 0.11125, acc 0.96
2016-09-06T00:01:44.100967: step 1665, loss 0.0515381, acc 0.96
2016-09-06T00:01:44.877669: step 1666, loss 0.0558821, acc 0.98
2016-09-06T00:01:45.685842: step 1667, loss 0.0146703, acc 1
2016-09-06T00:01:46.499043: step 1668, loss 0.0759054, acc 0.98
2016-09-06T00:01:47.274644: step 1669, loss 0.0529135, acc 0.96
2016-09-06T00:01:48.063159: step 1670, loss 0.142474, acc 0.94
2016-09-06T00:01:48.870289: step 1671, loss 0.0924944, acc 0.96
2016-09-06T00:01:49.729356: step 1672, loss 0.00902417, acc 1
2016-09-06T00:01:50.523863: step 1673, loss 0.0324852, acc 1
2016-09-06T00:01:51.336500: step 1674, loss 0.0652208, acc 0.96
2016-09-06T00:01:52.136511: step 1675, loss 0.0877078, acc 0.94
2016-09-06T00:01:52.927363: step 1676, loss 0.00581476, acc 1
2016-09-06T00:01:53.767961: step 1677, loss 0.16085, acc 0.96
2016-09-06T00:01:54.546923: step 1678, loss 0.116579, acc 0.98
2016-09-06T00:01:55.332723: step 1679, loss 0.0428451, acc 1
2016-09-06T00:01:56.134353: step 1680, loss 0.151263, acc 0.96
2016-09-06T00:01:56.921408: step 1681, loss 0.116751, acc 0.96
2016-09-06T00:01:57.737721: step 1682, loss 0.113234, acc 0.96
2016-09-06T00:01:58.557483: step 1683, loss 0.102688, acc 0.96
2016-09-06T00:01:59.348092: step 1684, loss 0.15009, acc 0.9
2016-09-06T00:02:00.152707: step 1685, loss 0.0738232, acc 0.96
2016-09-06T00:02:00.980623: step 1686, loss 0.0590694, acc 0.98
2016-09-06T00:02:01.780178: step 1687, loss 0.0406625, acc 0.98
2016-09-06T00:02:02.571841: step 1688, loss 0.0261161, acc 1
2016-09-06T00:02:03.415650: step 1689, loss 0.0315863, acc 1
2016-09-06T00:02:04.195663: step 1690, loss 0.0166509, acc 1
2016-09-06T00:02:04.998150: step 1691, loss 0.167947, acc 0.9
2016-09-06T00:02:05.813411: step 1692, loss 0.103615, acc 0.96
2016-09-06T00:02:06.606557: step 1693, loss 0.0648114, acc 0.94
2016-09-06T00:02:07.411090: step 1694, loss 0.0302713, acc 0.98
2016-09-06T00:02:08.226336: step 1695, loss 0.077799, acc 0.98
2016-09-06T00:02:09.036597: step 1696, loss 0.0822047, acc 0.96
2016-09-06T00:02:09.834289: step 1697, loss 0.0785263, acc 0.94
2016-09-06T00:02:10.656928: step 1698, loss 0.110969, acc 0.92
2016-09-06T00:02:11.442880: step 1699, loss 0.104529, acc 0.96
2016-09-06T00:02:12.233310: step 1700, loss 0.0307005, acc 1

Evaluation:
2016-09-06T00:02:15.982010: step 1700, loss 1.29971, acc 0.751407

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-1700

2016-09-06T00:02:17.884625: step 1701, loss 0.11075, acc 0.96
2016-09-06T00:02:18.720863: step 1702, loss 0.0670638, acc 0.98
2016-09-06T00:02:19.566505: step 1703, loss 0.0505325, acc 0.96
2016-09-06T00:02:20.393281: step 1704, loss 0.0139021, acc 1
2016-09-06T00:02:21.188766: step 1705, loss 0.117514, acc 0.94
2016-09-06T00:02:21.970967: step 1706, loss 0.153101, acc 0.9
2016-09-06T00:02:22.793419: step 1707, loss 0.0782632, acc 0.96
2016-09-06T00:02:23.576859: step 1708, loss 0.0661537, acc 0.96
2016-09-06T00:02:24.370949: step 1709, loss 0.195709, acc 0.96
2016-09-06T00:02:25.193426: step 1710, loss 0.0749006, acc 0.94
2016-09-06T00:02:25.981329: step 1711, loss 0.108202, acc 0.96
2016-09-06T00:02:26.786497: step 1712, loss 0.0804849, acc 0.98
2016-09-06T00:02:27.600678: step 1713, loss 0.133506, acc 0.94
2016-09-06T00:02:28.385434: step 1714, loss 0.117821, acc 0.98
2016-09-06T00:02:29.191368: step 1715, loss 0.0622662, acc 0.98
2016-09-06T00:02:30.000387: step 1716, loss 0.0865076, acc 0.94
2016-09-06T00:02:30.811398: step 1717, loss 0.0510126, acc 0.98
2016-09-06T00:02:31.620106: step 1718, loss 0.122091, acc 0.96
2016-09-06T00:02:32.453437: step 1719, loss 0.129708, acc 0.96
2016-09-06T00:02:33.252675: step 1720, loss 0.0528887, acc 0.96
2016-09-06T00:02:34.078943: step 1721, loss 0.0239589, acc 1
2016-09-06T00:02:34.890997: step 1722, loss 0.059227, acc 1
2016-09-06T00:02:35.660326: step 1723, loss 0.222624, acc 0.94
2016-09-06T00:02:36.473745: step 1724, loss 0.177509, acc 0.92
2016-09-06T00:02:37.304170: step 1725, loss 0.0828298, acc 0.96
2016-09-06T00:02:38.059795: step 1726, loss 0.0999115, acc 0.94
2016-09-06T00:02:38.842666: step 1727, loss 0.1373, acc 0.92
2016-09-06T00:02:39.587739: step 1728, loss 0.260054, acc 0.931818
2016-09-06T00:02:40.390771: step 1729, loss 0.0936971, acc 0.96
2016-09-06T00:02:41.223115: step 1730, loss 0.0461814, acc 1
2016-09-06T00:02:42.028848: step 1731, loss 0.0561677, acc 0.98
2016-09-06T00:02:42.804694: step 1732, loss 0.0836151, acc 0.98
2016-09-06T00:02:43.638959: step 1733, loss 0.0888865, acc 0.96
2016-09-06T00:02:44.448940: step 1734, loss 0.0538398, acc 0.98
2016-09-06T00:02:45.255754: step 1735, loss 0.0354771, acc 0.98
2016-09-06T00:02:46.078137: step 1736, loss 0.0402258, acc 0.98
2016-09-06T00:02:46.890514: step 1737, loss 0.0741504, acc 0.96
2016-09-06T00:02:47.682660: step 1738, loss 0.0474667, acc 0.98
2016-09-06T00:02:48.485560: step 1739, loss 0.0458661, acc 1
2016-09-06T00:02:49.271069: step 1740, loss 0.0768197, acc 0.96
2016-09-06T00:02:50.072984: step 1741, loss 0.0610795, acc 0.98
2016-09-06T00:02:50.892617: step 1742, loss 0.0633465, acc 0.96
2016-09-06T00:02:51.710785: step 1743, loss 0.0743133, acc 0.96
2016-09-06T00:02:52.484823: step 1744, loss 0.177289, acc 0.88
2016-09-06T00:02:53.303781: step 1745, loss 0.0219472, acc 1
2016-09-06T00:02:54.104321: step 1746, loss 0.061466, acc 0.98
2016-09-06T00:02:54.907887: step 1747, loss 0.0866806, acc 0.96
2016-09-06T00:02:55.724505: step 1748, loss 0.0398798, acc 1
2016-09-06T00:02:56.537370: step 1749, loss 0.0863035, acc 0.96
2016-09-06T00:02:57.369687: step 1750, loss 0.047688, acc 0.98
2016-09-06T00:02:58.185235: step 1751, loss 0.101436, acc 0.96
2016-09-06T00:02:59.006857: step 1752, loss 0.0308579, acc 0.98
2016-09-06T00:02:59.784494: step 1753, loss 0.0309856, acc 0.98
2016-09-06T00:03:00.604471: step 1754, loss 0.0797106, acc 0.98
2016-09-06T00:03:01.437585: step 1755, loss 0.0151159, acc 1
2016-09-06T00:03:02.222956: step 1756, loss 0.0263554, acc 0.98
2016-09-06T00:03:03.045414: step 1757, loss 0.107608, acc 0.96
2016-09-06T00:03:03.868282: step 1758, loss 0.0452559, acc 0.98
2016-09-06T00:03:04.640979: step 1759, loss 0.0556512, acc 0.98
2016-09-06T00:03:05.430213: step 1760, loss 0.0730892, acc 0.98
2016-09-06T00:03:06.241566: step 1761, loss 0.0804597, acc 0.98
2016-09-06T00:03:07.016245: step 1762, loss 0.0194644, acc 1
2016-09-06T00:03:07.817388: step 1763, loss 0.050008, acc 0.96
2016-09-06T00:03:08.648331: step 1764, loss 0.138734, acc 0.96
2016-09-06T00:03:09.434220: step 1765, loss 0.0528167, acc 0.98
2016-09-06T00:03:10.245137: step 1766, loss 0.100972, acc 0.94
2016-09-06T00:03:11.062393: step 1767, loss 0.133048, acc 0.92
2016-09-06T00:03:11.866712: step 1768, loss 0.0304887, acc 1
2016-09-06T00:03:12.653531: step 1769, loss 0.0674544, acc 0.98
2016-09-06T00:03:13.469337: step 1770, loss 0.0243712, acc 0.98
2016-09-06T00:03:14.277355: step 1771, loss 0.0326578, acc 1
2016-09-06T00:03:15.092133: step 1772, loss 0.0564453, acc 0.98
2016-09-06T00:03:15.891367: step 1773, loss 0.0187055, acc 1
2016-09-06T00:03:16.683723: step 1774, loss 0.0569808, acc 0.98
2016-09-06T00:03:17.496647: step 1775, loss 0.0495942, acc 0.96
2016-09-06T00:03:18.309037: step 1776, loss 0.170964, acc 0.9
2016-09-06T00:03:19.104995: step 1777, loss 0.0718439, acc 0.98
2016-09-06T00:03:19.894672: step 1778, loss 0.00515037, acc 1
2016-09-06T00:03:20.742659: step 1779, loss 0.10352, acc 0.96
2016-09-06T00:03:21.510651: step 1780, loss 0.0841857, acc 0.96
2016-09-06T00:03:22.332916: step 1781, loss 0.0320168, acc 1
2016-09-06T00:03:23.129350: step 1782, loss 0.0445282, acc 0.96
2016-09-06T00:03:23.917982: step 1783, loss 0.0526035, acc 0.98
2016-09-06T00:03:24.747755: step 1784, loss 0.106387, acc 0.96
2016-09-06T00:03:25.560298: step 1785, loss 0.0279493, acc 1
2016-09-06T00:03:26.348392: step 1786, loss 0.0489749, acc 0.98
2016-09-06T00:03:27.161271: step 1787, loss 0.0829586, acc 0.96
2016-09-06T00:03:27.976465: step 1788, loss 0.0152591, acc 1
2016-09-06T00:03:28.781403: step 1789, loss 0.0488317, acc 0.98
2016-09-06T00:03:29.597150: step 1790, loss 0.033187, acc 0.98
2016-09-06T00:03:30.394356: step 1791, loss 0.191721, acc 0.96
2016-09-06T00:03:31.212184: step 1792, loss 0.117494, acc 0.94
2016-09-06T00:03:32.019601: step 1793, loss 0.0366249, acc 1
2016-09-06T00:03:32.840159: step 1794, loss 0.139977, acc 0.94
2016-09-06T00:03:33.664857: step 1795, loss 0.0339177, acc 0.98
2016-09-06T00:03:34.454202: step 1796, loss 0.086184, acc 0.96
2016-09-06T00:03:35.293330: step 1797, loss 0.135464, acc 0.94
2016-09-06T00:03:36.070035: step 1798, loss 0.113446, acc 0.96
2016-09-06T00:03:36.849255: step 1799, loss 0.116837, acc 0.96
2016-09-06T00:03:37.669863: step 1800, loss 0.015487, acc 1

Evaluation:
2016-09-06T00:03:41.429419: step 1800, loss 1.22672, acc 0.746717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-1800

2016-09-06T00:03:43.294484: step 1801, loss 0.0180833, acc 1
2016-09-06T00:03:44.152540: step 1802, loss 0.0417335, acc 0.98
2016-09-06T00:03:44.991196: step 1803, loss 0.0495202, acc 0.98
2016-09-06T00:03:45.763318: step 1804, loss 0.0432854, acc 1
2016-09-06T00:03:46.598541: step 1805, loss 0.0528339, acc 1
2016-09-06T00:03:47.412983: step 1806, loss 0.0759259, acc 0.98
2016-09-06T00:03:48.233356: step 1807, loss 0.065984, acc 0.96
2016-09-06T00:03:49.047326: step 1808, loss 0.0457512, acc 0.96
2016-09-06T00:03:49.881648: step 1809, loss 0.0381585, acc 0.96
2016-09-06T00:03:50.709917: step 1810, loss 0.0484668, acc 0.96
2016-09-06T00:03:51.514379: step 1811, loss 0.0771761, acc 0.96
2016-09-06T00:03:52.364826: step 1812, loss 0.0163323, acc 1
2016-09-06T00:03:53.219133: step 1813, loss 0.109173, acc 0.96
2016-09-06T00:03:54.020415: step 1814, loss 0.169076, acc 0.94
2016-09-06T00:03:54.851575: step 1815, loss 0.00614299, acc 1
2016-09-06T00:03:55.670238: step 1816, loss 0.063521, acc 0.98
2016-09-06T00:03:56.511923: step 1817, loss 0.0696445, acc 0.98
2016-09-06T00:03:57.329724: step 1818, loss 0.0240749, acc 1
2016-09-06T00:03:58.121975: step 1819, loss 0.033278, acc 0.98
2016-09-06T00:03:58.936420: step 1820, loss 0.104058, acc 0.98
2016-09-06T00:03:59.786769: step 1821, loss 0.122401, acc 0.98
2016-09-06T00:04:00.600075: step 1822, loss 0.0243621, acc 1
2016-09-06T00:04:01.402595: step 1823, loss 0.0146583, acc 1
2016-09-06T00:04:02.217736: step 1824, loss 0.00874425, acc 1
2016-09-06T00:04:03.017941: step 1825, loss 0.0520984, acc 0.96
2016-09-06T00:04:03.822614: step 1826, loss 0.0672083, acc 0.98
2016-09-06T00:04:04.636897: step 1827, loss 0.00910031, acc 1
2016-09-06T00:04:05.434040: step 1828, loss 0.111895, acc 0.96
2016-09-06T00:04:06.227123: step 1829, loss 0.11535, acc 0.96
2016-09-06T00:04:07.058285: step 1830, loss 0.0265952, acc 0.98
2016-09-06T00:04:07.870161: step 1831, loss 0.0608249, acc 0.98
2016-09-06T00:04:08.685083: step 1832, loss 0.0767613, acc 0.98
2016-09-06T00:04:09.556352: step 1833, loss 0.0561362, acc 1
2016-09-06T00:04:10.363989: step 1834, loss 0.0787676, acc 0.96
2016-09-06T00:04:11.167232: step 1835, loss 0.068853, acc 0.98
2016-09-06T00:04:11.976293: step 1836, loss 0.0364302, acc 1
2016-09-06T00:04:12.771421: step 1837, loss 0.0291577, acc 1
2016-09-06T00:04:13.561810: step 1838, loss 0.109345, acc 0.94
2016-09-06T00:04:14.390668: step 1839, loss 0.124831, acc 0.96
2016-09-06T00:04:15.171466: step 1840, loss 0.0551525, acc 0.98
2016-09-06T00:04:15.966173: step 1841, loss 0.0335703, acc 0.98
2016-09-06T00:04:16.793880: step 1842, loss 0.0434518, acc 1
2016-09-06T00:04:17.617663: step 1843, loss 0.0704027, acc 0.98
2016-09-06T00:04:18.435764: step 1844, loss 0.0509818, acc 0.96
2016-09-06T00:04:19.246085: step 1845, loss 0.0575198, acc 0.98
2016-09-06T00:04:20.048911: step 1846, loss 0.0657839, acc 0.98
2016-09-06T00:04:20.879981: step 1847, loss 0.0732368, acc 0.96
2016-09-06T00:04:21.687817: step 1848, loss 0.0745173, acc 0.96
2016-09-06T00:04:22.480989: step 1849, loss 0.0472551, acc 0.98
2016-09-06T00:04:23.267835: step 1850, loss 0.00878081, acc 1
2016-09-06T00:04:24.092206: step 1851, loss 0.00799329, acc 1
2016-09-06T00:04:24.920720: step 1852, loss 0.198761, acc 0.9
2016-09-06T00:04:25.733316: step 1853, loss 0.0567858, acc 1
2016-09-06T00:04:26.535845: step 1854, loss 0.0329674, acc 1
2016-09-06T00:04:27.343887: step 1855, loss 0.159194, acc 0.94
2016-09-06T00:04:28.105759: step 1856, loss 0.128687, acc 0.96
2016-09-06T00:04:28.914166: step 1857, loss 0.104313, acc 0.98
2016-09-06T00:04:29.721031: step 1858, loss 0.115515, acc 0.98
2016-09-06T00:04:30.527358: step 1859, loss 0.100087, acc 0.96
2016-09-06T00:04:31.341535: step 1860, loss 0.0781729, acc 0.98
2016-09-06T00:04:32.163547: step 1861, loss 0.0495176, acc 0.96
2016-09-06T00:04:32.946866: step 1862, loss 0.117794, acc 0.96
2016-09-06T00:04:33.786386: step 1863, loss 0.0776717, acc 0.98
2016-09-06T00:04:34.646500: step 1864, loss 0.0598702, acc 0.96
2016-09-06T00:04:35.445251: step 1865, loss 0.0256949, acc 1
2016-09-06T00:04:36.256866: step 1866, loss 0.0768793, acc 0.96
2016-09-06T00:04:37.096594: step 1867, loss 0.0325482, acc 0.98
2016-09-06T00:04:37.902678: step 1868, loss 0.0684816, acc 0.98
2016-09-06T00:04:38.713253: step 1869, loss 0.121573, acc 0.96
2016-09-06T00:04:39.553457: step 1870, loss 0.0690903, acc 0.98
2016-09-06T00:04:40.338726: step 1871, loss 0.0250191, acc 1
2016-09-06T00:04:41.173910: step 1872, loss 0.126097, acc 0.92
2016-09-06T00:04:42.002485: step 1873, loss 0.108406, acc 0.96
2016-09-06T00:04:42.818724: step 1874, loss 0.0878082, acc 0.96
2016-09-06T00:04:43.620775: step 1875, loss 0.0611761, acc 0.98
2016-09-06T00:04:44.439876: step 1876, loss 0.0147309, acc 1
2016-09-06T00:04:45.276571: step 1877, loss 0.0429808, acc 1
2016-09-06T00:04:46.073675: step 1878, loss 0.0443211, acc 1
2016-09-06T00:04:46.924110: step 1879, loss 0.0640482, acc 0.96
2016-09-06T00:04:47.712564: step 1880, loss 0.0708632, acc 0.96
2016-09-06T00:04:48.539575: step 1881, loss 0.0377343, acc 0.98
2016-09-06T00:04:49.361283: step 1882, loss 0.00716981, acc 1
2016-09-06T00:04:50.159569: step 1883, loss 0.0875318, acc 0.94
2016-09-06T00:04:50.982075: step 1884, loss 0.13185, acc 0.96
2016-09-06T00:04:51.788485: step 1885, loss 0.0667346, acc 0.96
2016-09-06T00:04:52.601508: step 1886, loss 0.0618326, acc 0.96
2016-09-06T00:04:53.405468: step 1887, loss 0.0297807, acc 1
2016-09-06T00:04:54.220856: step 1888, loss 0.0319765, acc 1
2016-09-06T00:04:55.073405: step 1889, loss 0.045141, acc 0.98
2016-09-06T00:04:55.885115: step 1890, loss 0.0251077, acc 1
2016-09-06T00:04:56.706915: step 1891, loss 0.0123503, acc 1
2016-09-06T00:04:57.526758: step 1892, loss 0.0866788, acc 0.98
2016-09-06T00:04:58.352619: step 1893, loss 0.104956, acc 0.98
2016-09-06T00:04:59.138184: step 1894, loss 0.0467321, acc 0.98
2016-09-06T00:04:59.949656: step 1895, loss 0.0329199, acc 0.98
2016-09-06T00:05:00.757471: step 1896, loss 0.0710456, acc 0.98
2016-09-06T00:05:01.581412: step 1897, loss 0.0521362, acc 1
2016-09-06T00:05:02.378861: step 1898, loss 0.0767006, acc 0.98
2016-09-06T00:05:03.159923: step 1899, loss 0.0495022, acc 0.98
2016-09-06T00:05:03.991625: step 1900, loss 0.040699, acc 0.98

Evaluation:
2016-09-06T00:05:07.731135: step 1900, loss 1.58938, acc 0.738274

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-1900

2016-09-06T00:05:09.683982: step 1901, loss 0.0279263, acc 1
2016-09-06T00:05:10.477417: step 1902, loss 0.0431091, acc 0.98
2016-09-06T00:05:11.313608: step 1903, loss 0.0990693, acc 0.94
2016-09-06T00:05:12.126036: step 1904, loss 0.0252662, acc 1
2016-09-06T00:05:12.907435: step 1905, loss 0.0224327, acc 1
2016-09-06T00:05:13.763702: step 1906, loss 0.190509, acc 0.96
2016-09-06T00:05:14.605543: step 1907, loss 0.10375, acc 0.94
2016-09-06T00:05:15.417291: step 1908, loss 0.0600799, acc 0.98
2016-09-06T00:05:16.209458: step 1909, loss 0.0345814, acc 0.98
2016-09-06T00:05:17.061444: step 1910, loss 0.0332081, acc 1
2016-09-06T00:05:17.864189: step 1911, loss 0.00602139, acc 1
2016-09-06T00:05:18.675217: step 1912, loss 0.101515, acc 0.98
2016-09-06T00:05:19.488659: step 1913, loss 0.109688, acc 0.94
2016-09-06T00:05:20.248039: step 1914, loss 0.0272226, acc 1
2016-09-06T00:05:21.044366: step 1915, loss 0.211129, acc 0.86
2016-09-06T00:05:21.838777: step 1916, loss 0.0511255, acc 0.98
2016-09-06T00:05:22.647972: step 1917, loss 0.1886, acc 0.92
2016-09-06T00:05:23.436250: step 1918, loss 0.0574174, acc 0.98
2016-09-06T00:05:24.253264: step 1919, loss 0.0348736, acc 0.98
2016-09-06T00:05:25.026431: step 1920, loss 0.0192789, acc 1
2016-09-06T00:05:25.870073: step 1921, loss 0.0146175, acc 1
2016-09-06T00:05:26.672167: step 1922, loss 0.0440031, acc 1
2016-09-06T00:05:27.482620: step 1923, loss 0.101139, acc 0.96
2016-09-06T00:05:28.288470: step 1924, loss 0.0539525, acc 0.98
2016-09-06T00:05:29.110871: step 1925, loss 0.0240893, acc 0.98
2016-09-06T00:05:29.893454: step 1926, loss 0.00968801, acc 1
2016-09-06T00:05:30.697505: step 1927, loss 0.0427585, acc 0.98
2016-09-06T00:05:31.491476: step 1928, loss 0.0541237, acc 0.98
2016-09-06T00:05:32.298545: step 1929, loss 0.012086, acc 1
2016-09-06T00:05:33.103628: step 1930, loss 0.0255722, acc 0.98
2016-09-06T00:05:33.911609: step 1931, loss 0.0497848, acc 0.98
2016-09-06T00:05:34.696512: step 1932, loss 0.0195579, acc 1
2016-09-06T00:05:35.521599: step 1933, loss 0.0666466, acc 0.96
2016-09-06T00:05:36.395730: step 1934, loss 0.0182843, acc 1
2016-09-06T00:05:37.185023: step 1935, loss 0.00775006, acc 1
2016-09-06T00:05:38.000529: step 1936, loss 0.0445903, acc 0.98
2016-09-06T00:05:38.796748: step 1937, loss 0.0385487, acc 0.98
2016-09-06T00:05:39.594290: step 1938, loss 0.111069, acc 0.94
2016-09-06T00:05:40.401507: step 1939, loss 0.109643, acc 0.96
2016-09-06T00:05:41.262252: step 1940, loss 0.0449185, acc 0.98
2016-09-06T00:05:42.065163: step 1941, loss 0.0414521, acc 0.98
2016-09-06T00:05:42.876550: step 1942, loss 0.105684, acc 0.96
2016-09-06T00:05:43.694299: step 1943, loss 0.068199, acc 0.94
2016-09-06T00:05:44.491096: step 1944, loss 0.0319752, acc 0.98
2016-09-06T00:05:45.290962: step 1945, loss 0.0135746, acc 1
2016-09-06T00:05:46.109422: step 1946, loss 0.129145, acc 0.94
2016-09-06T00:05:46.910899: step 1947, loss 0.0359822, acc 0.98
2016-09-06T00:05:47.713984: step 1948, loss 0.106202, acc 0.94
2016-09-06T00:05:48.534515: step 1949, loss 0.153874, acc 0.96
2016-09-06T00:05:49.334925: step 1950, loss 0.0124512, acc 1
2016-09-06T00:05:50.147687: step 1951, loss 0.0421949, acc 0.98
2016-09-06T00:05:51.004068: step 1952, loss 0.0791547, acc 0.98
2016-09-06T00:05:51.819150: step 1953, loss 0.131544, acc 0.94
2016-09-06T00:05:52.635619: step 1954, loss 0.0401913, acc 1
2016-09-06T00:05:53.487955: step 1955, loss 0.00800713, acc 1
2016-09-06T00:05:54.288337: step 1956, loss 0.0732066, acc 0.96
2016-09-06T00:05:55.129212: step 1957, loss 0.0284845, acc 1
2016-09-06T00:05:55.968312: step 1958, loss 0.0791136, acc 0.96
2016-09-06T00:05:56.802587: step 1959, loss 0.0379523, acc 1
2016-09-06T00:05:57.614092: step 1960, loss 0.019056, acc 1
2016-09-06T00:05:58.467390: step 1961, loss 0.0345565, acc 0.98
2016-09-06T00:05:59.267673: step 1962, loss 0.017277, acc 1
2016-09-06T00:06:00.079141: step 1963, loss 0.109582, acc 0.92
2016-09-06T00:06:00.892565: step 1964, loss 0.0485027, acc 0.98
2016-09-06T00:06:01.696512: step 1965, loss 0.0197246, acc 1
2016-09-06T00:06:02.498988: step 1966, loss 0.0450484, acc 0.98
2016-09-06T00:06:03.336874: step 1967, loss 0.034142, acc 0.98
2016-09-06T00:06:04.155472: step 1968, loss 0.0417509, acc 1
2016-09-06T00:06:04.947489: step 1969, loss 0.1209, acc 0.96
2016-09-06T00:06:05.741226: step 1970, loss 0.048214, acc 0.98
2016-09-06T00:06:06.547784: step 1971, loss 0.0387039, acc 1
2016-09-06T00:06:07.356139: step 1972, loss 0.028906, acc 1
2016-09-06T00:06:08.170980: step 1973, loss 0.0485526, acc 0.96
2016-09-06T00:06:08.993727: step 1974, loss 0.0429039, acc 0.98
2016-09-06T00:06:09.793000: step 1975, loss 0.0636724, acc 0.98
2016-09-06T00:06:10.602117: step 1976, loss 0.0262955, acc 1
2016-09-06T00:06:11.443343: step 1977, loss 0.0179244, acc 1
2016-09-06T00:06:12.224609: step 1978, loss 0.0286723, acc 1
2016-09-06T00:06:13.060643: step 1979, loss 0.0374233, acc 0.98
2016-09-06T00:06:13.863942: step 1980, loss 0.0132611, acc 1
2016-09-06T00:06:14.636814: step 1981, loss 0.0265521, acc 1
2016-09-06T00:06:15.450566: step 1982, loss 0.0453744, acc 0.98
2016-09-06T00:06:16.282440: step 1983, loss 0.00761848, acc 1
2016-09-06T00:06:17.085655: step 1984, loss 0.043107, acc 0.98
2016-09-06T00:06:17.863779: step 1985, loss 0.0520306, acc 0.98
2016-09-06T00:06:18.678376: step 1986, loss 0.0106958, acc 1
2016-09-06T00:06:19.496916: step 1987, loss 0.0598959, acc 0.96
2016-09-06T00:06:20.278523: step 1988, loss 0.0282955, acc 1
2016-09-06T00:06:21.076131: step 1989, loss 0.0246499, acc 1
2016-09-06T00:06:21.855854: step 1990, loss 0.00461156, acc 1
2016-09-06T00:06:22.660155: step 1991, loss 0.0510107, acc 0.96
2016-09-06T00:06:23.489684: step 1992, loss 0.0658787, acc 0.96
2016-09-06T00:06:24.244302: step 1993, loss 0.0744729, acc 0.96
2016-09-06T00:06:25.070034: step 1994, loss 0.0379793, acc 0.98
2016-09-06T00:06:25.885946: step 1995, loss 0.0201764, acc 1
2016-09-06T00:06:26.663519: step 1996, loss 0.0557565, acc 0.98
2016-09-06T00:06:27.470389: step 1997, loss 0.0594078, acc 0.96
2016-09-06T00:06:28.307475: step 1998, loss 0.0709421, acc 0.98
2016-09-06T00:06:29.117699: step 1999, loss 0.0710158, acc 0.94
2016-09-06T00:06:29.943475: step 2000, loss 0.0377646, acc 1

Evaluation:
2016-09-06T00:06:33.660817: step 2000, loss 1.93014, acc 0.742964

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-2000

2016-09-06T00:06:35.635689: step 2001, loss 0.0967539, acc 0.96
2016-09-06T00:06:36.452172: step 2002, loss 0.013715, acc 1
2016-09-06T00:06:37.238586: step 2003, loss 0.145926, acc 0.96
2016-09-06T00:06:38.063498: step 2004, loss 0.128235, acc 0.96
2016-09-06T00:06:38.820049: step 2005, loss 0.19488, acc 0.9
2016-09-06T00:06:39.611816: step 2006, loss 0.0222948, acc 0.98
2016-09-06T00:06:40.435195: step 2007, loss 0.0141021, acc 1
2016-09-06T00:06:41.235563: step 2008, loss 0.0577694, acc 0.98
2016-09-06T00:06:42.081425: step 2009, loss 0.0481098, acc 1
2016-09-06T00:06:42.910316: step 2010, loss 0.0967142, acc 0.98
2016-09-06T00:06:43.703780: step 2011, loss 0.0212313, acc 1
2016-09-06T00:06:44.461154: step 2012, loss 0.0307412, acc 1
2016-09-06T00:06:45.277125: step 2013, loss 0.1514, acc 0.96
2016-09-06T00:06:46.091575: step 2014, loss 0.0364077, acc 0.98
2016-09-06T00:06:46.890140: step 2015, loss 0.0549304, acc 1
2016-09-06T00:06:47.730110: step 2016, loss 0.0470931, acc 0.98
2016-09-06T00:06:48.488814: step 2017, loss 0.0662338, acc 0.98
2016-09-06T00:06:49.322562: step 2018, loss 0.0772231, acc 0.94
2016-09-06T00:06:50.153285: step 2019, loss 0.0439067, acc 1
2016-09-06T00:06:50.941402: step 2020, loss 0.0586317, acc 0.96
2016-09-06T00:06:51.749590: step 2021, loss 0.0280712, acc 1
2016-09-06T00:06:52.574297: step 2022, loss 0.0977655, acc 0.96
2016-09-06T00:06:53.356464: step 2023, loss 0.0444241, acc 0.96
2016-09-06T00:06:54.141353: step 2024, loss 0.018519, acc 1
2016-09-06T00:06:54.977550: step 2025, loss 0.0625064, acc 0.96
2016-09-06T00:06:55.748932: step 2026, loss 0.0382917, acc 0.98
2016-09-06T00:06:56.539027: step 2027, loss 0.0872019, acc 0.94
2016-09-06T00:06:57.344579: step 2028, loss 0.138031, acc 0.96
2016-09-06T00:06:58.141302: step 2029, loss 0.0739846, acc 0.94
2016-09-06T00:06:58.930911: step 2030, loss 0.0804944, acc 0.98
2016-09-06T00:06:59.726859: step 2031, loss 0.0324828, acc 0.98
2016-09-06T00:07:00.531579: step 2032, loss 0.106799, acc 0.94
2016-09-06T00:07:01.392741: step 2033, loss 0.0333122, acc 1
2016-09-06T00:07:02.189085: step 2034, loss 0.0658304, acc 0.98
2016-09-06T00:07:02.996355: step 2035, loss 0.103198, acc 0.96
2016-09-06T00:07:03.825495: step 2036, loss 0.0440559, acc 0.98
2016-09-06T00:07:04.649432: step 2037, loss 0.012159, acc 1
2016-09-06T00:07:05.474878: step 2038, loss 0.013561, acc 1
2016-09-06T00:07:06.272311: step 2039, loss 0.0787432, acc 0.96
2016-09-06T00:07:07.158827: step 2040, loss 0.0468236, acc 0.98
2016-09-06T00:07:07.954247: step 2041, loss 0.0106233, acc 1
2016-09-06T00:07:08.749514: step 2042, loss 0.0112966, acc 1
2016-09-06T00:07:09.562758: step 2043, loss 0.0670771, acc 0.96
2016-09-06T00:07:10.381364: step 2044, loss 0.122054, acc 0.94
2016-09-06T00:07:11.153366: step 2045, loss 0.120363, acc 0.92
2016-09-06T00:07:12.003525: step 2046, loss 0.0553529, acc 0.98
2016-09-06T00:07:12.817077: step 2047, loss 0.0228923, acc 1
2016-09-06T00:07:13.624693: step 2048, loss 0.0904219, acc 0.96
2016-09-06T00:07:14.438645: step 2049, loss 0.098091, acc 0.96
2016-09-06T00:07:15.251150: step 2050, loss 0.0910305, acc 0.94
2016-09-06T00:07:16.071847: step 2051, loss 0.139811, acc 0.96
2016-09-06T00:07:16.907372: step 2052, loss 0.0186491, acc 1
2016-09-06T00:07:17.720577: step 2053, loss 0.0191075, acc 1
2016-09-06T00:07:18.526909: step 2054, loss 0.0735917, acc 0.96
2016-09-06T00:07:19.388447: step 2055, loss 0.0634645, acc 0.98
2016-09-06T00:07:20.228674: step 2056, loss 0.144295, acc 0.96
2016-09-06T00:07:21.059121: step 2057, loss 0.0339162, acc 0.98
2016-09-06T00:07:21.887498: step 2058, loss 0.0919798, acc 0.98
2016-09-06T00:07:22.740524: step 2059, loss 0.0493956, acc 0.98
2016-09-06T00:07:23.559273: step 2060, loss 0.0576838, acc 0.96
2016-09-06T00:07:24.364390: step 2061, loss 0.105504, acc 0.96
2016-09-06T00:07:25.181451: step 2062, loss 0.217061, acc 0.98
2016-09-06T00:07:25.968762: step 2063, loss 0.246174, acc 0.92
2016-09-06T00:07:26.767112: step 2064, loss 0.150923, acc 0.94
2016-09-06T00:07:27.583579: step 2065, loss 0.153537, acc 0.96
2016-09-06T00:07:28.371473: step 2066, loss 0.0843288, acc 0.98
2016-09-06T00:07:29.201234: step 2067, loss 0.241214, acc 0.92
2016-09-06T00:07:30.011715: step 2068, loss 0.084585, acc 0.96
2016-09-06T00:07:30.805071: step 2069, loss 0.0738455, acc 0.98
2016-09-06T00:07:31.600822: step 2070, loss 0.0911906, acc 0.98
2016-09-06T00:07:32.449150: step 2071, loss 0.0362014, acc 1
2016-09-06T00:07:33.210324: step 2072, loss 0.0669683, acc 0.98
2016-09-06T00:07:34.037503: step 2073, loss 0.0870657, acc 0.96
2016-09-06T00:07:34.839374: step 2074, loss 0.0472668, acc 1
2016-09-06T00:07:35.633974: step 2075, loss 0.0570919, acc 1
2016-09-06T00:07:36.427547: step 2076, loss 0.0530818, acc 1
2016-09-06T00:07:37.231972: step 2077, loss 0.0748386, acc 0.96
2016-09-06T00:07:38.027359: step 2078, loss 0.0432934, acc 1
2016-09-06T00:07:38.828634: step 2079, loss 0.0470854, acc 0.96
2016-09-06T00:07:39.646982: step 2080, loss 0.0837942, acc 0.96
2016-09-06T00:07:40.427543: step 2081, loss 0.0896788, acc 0.94
2016-09-06T00:07:41.245215: step 2082, loss 0.056257, acc 1
2016-09-06T00:07:42.054738: step 2083, loss 0.0286842, acc 1
2016-09-06T00:07:42.832934: step 2084, loss 0.0152572, acc 1
2016-09-06T00:07:43.661876: step 2085, loss 0.0446881, acc 0.98
2016-09-06T00:07:44.491820: step 2086, loss 0.0454974, acc 0.96
2016-09-06T00:07:45.297311: step 2087, loss 0.023095, acc 1
2016-09-06T00:07:46.087815: step 2088, loss 0.0572785, acc 0.96
2016-09-06T00:07:46.918512: step 2089, loss 0.0853854, acc 0.96
2016-09-06T00:07:47.704230: step 2090, loss 0.0378653, acc 0.98
2016-09-06T00:07:48.500279: step 2091, loss 0.0283972, acc 1
2016-09-06T00:07:49.323459: step 2092, loss 0.0655049, acc 0.98
2016-09-06T00:07:50.120596: step 2093, loss 0.0517867, acc 0.98
2016-09-06T00:07:50.919249: step 2094, loss 0.0406125, acc 0.98
2016-09-06T00:07:51.736085: step 2095, loss 0.0707303, acc 0.98
2016-09-06T00:07:52.518368: step 2096, loss 0.0261247, acc 0.98
2016-09-06T00:07:53.315549: step 2097, loss 0.103709, acc 0.96
2016-09-06T00:07:54.143600: step 2098, loss 0.0856513, acc 0.94
2016-09-06T00:07:54.922067: step 2099, loss 0.122695, acc 0.96
2016-09-06T00:07:55.735727: step 2100, loss 0.12399, acc 0.94

Evaluation:
2016-09-06T00:07:59.488331: step 2100, loss 1.95948, acc 0.746717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-2100

2016-09-06T00:08:01.323230: step 2101, loss 0.232041, acc 0.96
2016-09-06T00:08:02.138640: step 2102, loss 0.0269294, acc 0.98
2016-09-06T00:08:02.961714: step 2103, loss 0.122371, acc 0.96
2016-09-06T00:08:03.761916: step 2104, loss 0.00589905, acc 1
2016-09-06T00:08:04.599974: step 2105, loss 0.0222422, acc 1
2016-09-06T00:08:05.439815: step 2106, loss 0.144831, acc 0.98
2016-09-06T00:08:06.246043: step 2107, loss 0.0173932, acc 1
2016-09-06T00:08:07.076342: step 2108, loss 0.0513702, acc 0.98
2016-09-06T00:08:07.867372: step 2109, loss 0.0445167, acc 0.98
2016-09-06T00:08:08.679904: step 2110, loss 0.0614522, acc 0.96
2016-09-06T00:08:09.467431: step 2111, loss 0.0191209, acc 1
2016-09-06T00:08:10.217996: step 2112, loss 0.054621, acc 0.954545
2016-09-06T00:08:11.027048: step 2113, loss 0.0418792, acc 0.98
2016-09-06T00:08:11.868539: step 2114, loss 0.0277765, acc 1
2016-09-06T00:08:12.700446: step 2115, loss 0.0920579, acc 0.96
2016-09-06T00:08:13.518513: step 2116, loss 0.0985018, acc 0.94
2016-09-06T00:08:14.321995: step 2117, loss 0.0609398, acc 0.94
2016-09-06T00:08:15.102300: step 2118, loss 0.00612171, acc 1
2016-09-06T00:08:15.911228: step 2119, loss 0.0506122, acc 0.96
2016-09-06T00:08:16.730719: step 2120, loss 0.076191, acc 0.98
2016-09-06T00:08:17.533791: step 2121, loss 0.0324986, acc 0.98
2016-09-06T00:08:18.392113: step 2122, loss 0.0303924, acc 0.98
2016-09-06T00:08:19.181499: step 2123, loss 0.104746, acc 0.94
2016-09-06T00:08:19.996114: step 2124, loss 0.0437506, acc 1
2016-09-06T00:08:20.823773: step 2125, loss 0.0289004, acc 1
2016-09-06T00:08:21.625264: step 2126, loss 0.0164675, acc 1
2016-09-06T00:08:22.426146: step 2127, loss 0.0390753, acc 0.98
2016-09-06T00:08:23.249191: step 2128, loss 0.102753, acc 0.94
2016-09-06T00:08:24.040476: step 2129, loss 0.0666031, acc 0.96
2016-09-06T00:08:24.858052: step 2130, loss 0.0568176, acc 0.96
2016-09-06T00:08:25.678650: step 2131, loss 0.048642, acc 0.98
2016-09-06T00:08:26.468837: step 2132, loss 0.049094, acc 0.96
2016-09-06T00:08:27.279488: step 2133, loss 0.0273179, acc 0.98
2016-09-06T00:08:28.099890: step 2134, loss 0.106441, acc 0.98
2016-09-06T00:08:28.905266: step 2135, loss 0.0439795, acc 0.98
2016-09-06T00:08:29.731151: step 2136, loss 0.106533, acc 0.98
2016-09-06T00:08:30.571798: step 2137, loss 0.025575, acc 1
2016-09-06T00:08:31.397662: step 2138, loss 0.117122, acc 0.94
2016-09-06T00:08:32.201793: step 2139, loss 0.0500163, acc 0.98
2016-09-06T00:08:33.021493: step 2140, loss 0.0346259, acc 0.98
2016-09-06T00:08:33.820310: step 2141, loss 0.0442097, acc 0.98
2016-09-06T00:08:34.707962: step 2142, loss 0.0732299, acc 0.98
2016-09-06T00:08:35.535259: step 2143, loss 0.070148, acc 0.98
2016-09-06T00:08:36.348894: step 2144, loss 0.159026, acc 0.94
2016-09-06T00:08:37.138069: step 2145, loss 0.0641014, acc 0.98
2016-09-06T00:08:37.944596: step 2146, loss 0.0378378, acc 0.98
2016-09-06T00:08:38.761355: step 2147, loss 0.0383478, acc 0.98
2016-09-06T00:08:39.585802: step 2148, loss 0.11821, acc 0.96
2016-09-06T00:08:40.444427: step 2149, loss 0.0444313, acc 0.98
2016-09-06T00:08:41.262577: step 2150, loss 0.0209613, acc 1
2016-09-06T00:08:42.057326: step 2151, loss 0.099655, acc 0.94
2016-09-06T00:08:42.880285: step 2152, loss 0.0595462, acc 0.96
2016-09-06T00:08:43.679646: step 2153, loss 0.0470451, acc 0.98
2016-09-06T00:08:44.476693: step 2154, loss 0.0176627, acc 1
2016-09-06T00:08:45.304776: step 2155, loss 0.168358, acc 0.94
2016-09-06T00:08:46.123420: step 2156, loss 0.0367201, acc 0.98
2016-09-06T00:08:46.909719: step 2157, loss 0.0075244, acc 1
2016-09-06T00:08:47.750532: step 2158, loss 0.0679859, acc 0.96
2016-09-06T00:08:48.547337: step 2159, loss 0.0585788, acc 0.98
2016-09-06T00:08:49.340897: step 2160, loss 0.078904, acc 0.96
2016-09-06T00:08:50.155807: step 2161, loss 0.0572707, acc 0.98
2016-09-06T00:08:50.977747: step 2162, loss 0.0116136, acc 1
2016-09-06T00:08:51.768863: step 2163, loss 0.0300284, acc 0.98
2016-09-06T00:08:52.570266: step 2164, loss 0.0478071, acc 0.98
2016-09-06T00:08:53.400101: step 2165, loss 0.0951329, acc 0.96
2016-09-06T00:08:54.212139: step 2166, loss 0.0234529, acc 1
2016-09-06T00:08:55.019626: step 2167, loss 0.0860054, acc 0.94
2016-09-06T00:08:55.822410: step 2168, loss 0.0537761, acc 0.96
2016-09-06T00:08:56.620132: step 2169, loss 0.0133925, acc 1
2016-09-06T00:08:57.435308: step 2170, loss 0.12511, acc 0.94
2016-09-06T00:08:58.259117: step 2171, loss 0.01491, acc 1
2016-09-06T00:08:59.050258: step 2172, loss 0.0224095, acc 1
2016-09-06T00:08:59.847159: step 2173, loss 0.100988, acc 0.94
2016-09-06T00:09:00.677409: step 2174, loss 0.0224827, acc 1
2016-09-06T00:09:01.487985: step 2175, loss 0.015888, acc 1
2016-09-06T00:09:02.322435: step 2176, loss 0.0434584, acc 0.98
2016-09-06T00:09:03.151772: step 2177, loss 0.0160529, acc 1
2016-09-06T00:09:03.956260: step 2178, loss 0.0283073, acc 1
2016-09-06T00:09:04.757517: step 2179, loss 0.0260443, acc 1
2016-09-06T00:09:05.601761: step 2180, loss 0.0634058, acc 0.98
2016-09-06T00:09:06.379243: step 2181, loss 0.0669764, acc 0.96
2016-09-06T00:09:07.162539: step 2182, loss 0.0207034, acc 1
2016-09-06T00:09:07.970287: step 2183, loss 0.170458, acc 0.96
2016-09-06T00:09:08.763023: step 2184, loss 0.0938739, acc 0.98
2016-09-06T00:09:09.565210: step 2185, loss 0.0376811, acc 0.98
2016-09-06T00:09:10.404083: step 2186, loss 0.0333111, acc 0.98
2016-09-06T00:09:11.189610: step 2187, loss 0.117693, acc 0.96
2016-09-06T00:09:11.990349: step 2188, loss 0.0159223, acc 1
2016-09-06T00:09:12.821655: step 2189, loss 0.0156496, acc 1
2016-09-06T00:09:13.604285: step 2190, loss 0.0833242, acc 0.98
2016-09-06T00:09:14.410384: step 2191, loss 0.0631824, acc 0.96
2016-09-06T00:09:15.220840: step 2192, loss 0.035428, acc 0.98
2016-09-06T00:09:16.021592: step 2193, loss 0.0107254, acc 1
2016-09-06T00:09:16.861786: step 2194, loss 0.144811, acc 0.96
2016-09-06T00:09:17.694204: step 2195, loss 0.251418, acc 0.96
2016-09-06T00:09:18.473676: step 2196, loss 0.0178016, acc 1
2016-09-06T00:09:19.269900: step 2197, loss 0.0772007, acc 0.96
2016-09-06T00:09:20.076244: step 2198, loss 0.0746607, acc 0.94
2016-09-06T00:09:20.865027: step 2199, loss 0.0487952, acc 0.96
2016-09-06T00:09:21.674300: step 2200, loss 0.0439054, acc 1

Evaluation:
2016-09-06T00:09:25.410649: step 2200, loss 1.21535, acc 0.746717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-2200

2016-09-06T00:09:27.314323: step 2201, loss 0.0817563, acc 0.96
2016-09-06T00:09:28.104672: step 2202, loss 0.0196668, acc 1
2016-09-06T00:09:28.933400: step 2203, loss 0.107099, acc 0.94
2016-09-06T00:09:29.734960: step 2204, loss 0.0313768, acc 0.98
2016-09-06T00:09:30.565202: step 2205, loss 0.100234, acc 0.94
2016-09-06T00:09:31.369537: step 2206, loss 0.0259358, acc 1
2016-09-06T00:09:32.194274: step 2207, loss 0.0821646, acc 0.94
2016-09-06T00:09:32.973952: step 2208, loss 0.00573673, acc 1
2016-09-06T00:09:33.777156: step 2209, loss 0.022009, acc 1
2016-09-06T00:09:34.575477: step 2210, loss 0.0430963, acc 0.98
2016-09-06T00:09:35.369554: step 2211, loss 0.0266216, acc 1
2016-09-06T00:09:36.184486: step 2212, loss 0.085564, acc 0.94
2016-09-06T00:09:37.007072: step 2213, loss 0.0732879, acc 0.96
2016-09-06T00:09:37.787736: step 2214, loss 0.026587, acc 1
2016-09-06T00:09:38.611058: step 2215, loss 0.0428246, acc 0.98
2016-09-06T00:09:39.432361: step 2216, loss 0.0249903, acc 1
2016-09-06T00:09:40.213372: step 2217, loss 0.0449497, acc 0.98
2016-09-06T00:09:41.027620: step 2218, loss 0.103469, acc 0.96
2016-09-06T00:09:41.843788: step 2219, loss 0.049077, acc 1
2016-09-06T00:09:42.630467: step 2220, loss 0.0947924, acc 0.94
2016-09-06T00:09:43.433503: step 2221, loss 0.0457901, acc 0.98
2016-09-06T00:09:44.245147: step 2222, loss 0.0242018, acc 1
2016-09-06T00:09:45.069786: step 2223, loss 0.0540606, acc 0.98
2016-09-06T00:09:45.882524: step 2224, loss 0.0820799, acc 0.96
2016-09-06T00:09:46.686772: step 2225, loss 0.0239035, acc 1
2016-09-06T00:09:47.480041: step 2226, loss 0.0575283, acc 0.96
2016-09-06T00:09:48.260734: step 2227, loss 0.203136, acc 0.92
2016-09-06T00:09:49.103254: step 2228, loss 0.0351576, acc 1
2016-09-06T00:09:49.910811: step 2229, loss 0.0375729, acc 0.98
2016-09-06T00:09:50.728815: step 2230, loss 0.0192565, acc 1
2016-09-06T00:09:51.538126: step 2231, loss 0.0300587, acc 0.98
2016-09-06T00:09:52.340219: step 2232, loss 0.0881243, acc 0.96
2016-09-06T00:09:53.157533: step 2233, loss 0.093567, acc 0.96
2016-09-06T00:09:53.951466: step 2234, loss 0.0247768, acc 1
2016-09-06T00:09:54.748656: step 2235, loss 0.0358764, acc 0.98
2016-09-06T00:09:55.558111: step 2236, loss 0.152144, acc 0.9
2016-09-06T00:09:56.386055: step 2237, loss 0.0659772, acc 0.98
2016-09-06T00:09:57.190656: step 2238, loss 0.0769253, acc 0.96
2016-09-06T00:09:57.992558: step 2239, loss 0.0289429, acc 0.98
2016-09-06T00:09:58.801152: step 2240, loss 0.00543915, acc 1
2016-09-06T00:09:59.580746: step 2241, loss 0.091921, acc 0.96
2016-09-06T00:10:00.439960: step 2242, loss 0.101515, acc 0.94
2016-09-06T00:10:01.279649: step 2243, loss 0.048361, acc 0.96
2016-09-06T00:10:02.100584: step 2244, loss 0.06376, acc 0.96
2016-09-06T00:10:02.944147: step 2245, loss 0.0525884, acc 0.98
2016-09-06T00:10:03.775183: step 2246, loss 0.139259, acc 0.94
2016-09-06T00:10:04.578134: step 2247, loss 0.0799104, acc 0.96
2016-09-06T00:10:05.400907: step 2248, loss 0.0754483, acc 0.96
2016-09-06T00:10:06.242305: step 2249, loss 0.0100287, acc 1
2016-09-06T00:10:07.040204: step 2250, loss 0.0375797, acc 0.98
2016-09-06T00:10:07.879628: step 2251, loss 0.0847634, acc 0.94
2016-09-06T00:10:08.726840: step 2252, loss 0.0247974, acc 1
2016-09-06T00:10:09.556681: step 2253, loss 0.109535, acc 0.9
2016-09-06T00:10:10.384907: step 2254, loss 0.0205435, acc 1
2016-09-06T00:10:11.207009: step 2255, loss 0.0461953, acc 1
2016-09-06T00:10:12.040155: step 2256, loss 0.0460224, acc 0.96
2016-09-06T00:10:12.822434: step 2257, loss 0.0278091, acc 1
2016-09-06T00:10:13.638749: step 2258, loss 0.127636, acc 0.94
2016-09-06T00:10:14.451810: step 2259, loss 0.0977466, acc 0.96
2016-09-06T00:10:15.275094: step 2260, loss 0.0517526, acc 0.96
2016-09-06T00:10:16.071249: step 2261, loss 0.0347824, acc 0.98
2016-09-06T00:10:16.891097: step 2262, loss 0.0823631, acc 0.94
2016-09-06T00:10:17.689734: step 2263, loss 0.0207978, acc 1
2016-09-06T00:10:18.485758: step 2264, loss 0.00583251, acc 1
2016-09-06T00:10:19.280437: step 2265, loss 0.0635894, acc 0.98
2016-09-06T00:10:20.099251: step 2266, loss 0.0548558, acc 0.98
2016-09-06T00:10:20.913382: step 2267, loss 0.0523392, acc 1
2016-09-06T00:10:21.725515: step 2268, loss 0.0536965, acc 0.98
2016-09-06T00:10:22.500511: step 2269, loss 0.0242656, acc 0.98
2016-09-06T00:10:23.315999: step 2270, loss 0.0796705, acc 0.98
2016-09-06T00:10:24.168471: step 2271, loss 0.0335714, acc 0.98
2016-09-06T00:10:24.971285: step 2272, loss 0.0578101, acc 0.96
2016-09-06T00:10:25.763978: step 2273, loss 0.0723567, acc 0.96
2016-09-06T00:10:26.567673: step 2274, loss 0.0219639, acc 1
2016-09-06T00:10:27.365194: step 2275, loss 0.0135881, acc 1
2016-09-06T00:10:28.183677: step 2276, loss 0.05264, acc 0.98
2016-09-06T00:10:28.985139: step 2277, loss 0.0240545, acc 1
2016-09-06T00:10:29.777756: step 2278, loss 0.0284356, acc 0.98
2016-09-06T00:10:30.573887: step 2279, loss 0.133768, acc 0.96
2016-09-06T00:10:31.398492: step 2280, loss 0.0362692, acc 1
2016-09-06T00:10:32.192987: step 2281, loss 0.187619, acc 0.94
2016-09-06T00:10:33.008561: step 2282, loss 0.018219, acc 1
2016-09-06T00:10:33.863700: step 2283, loss 0.0197733, acc 0.98
2016-09-06T00:10:34.643510: step 2284, loss 0.0368284, acc 0.98
2016-09-06T00:10:35.461408: step 2285, loss 0.0120167, acc 1
2016-09-06T00:10:36.295043: step 2286, loss 0.0518584, acc 0.96
2016-09-06T00:10:37.068010: step 2287, loss 0.0764292, acc 0.96
2016-09-06T00:10:37.886593: step 2288, loss 0.0404208, acc 0.98
2016-09-06T00:10:38.696199: step 2289, loss 0.0464801, acc 1
2016-09-06T00:10:39.501293: step 2290, loss 0.0674606, acc 0.96
2016-09-06T00:10:40.297386: step 2291, loss 0.0404888, acc 0.96
2016-09-06T00:10:41.114146: step 2292, loss 0.0578122, acc 0.98
2016-09-06T00:10:41.967186: step 2293, loss 0.0704453, acc 0.98
2016-09-06T00:10:42.773560: step 2294, loss 0.0106428, acc 1
2016-09-06T00:10:43.592052: step 2295, loss 0.00646885, acc 1
2016-09-06T00:10:44.410705: step 2296, loss 0.0110828, acc 1
2016-09-06T00:10:45.208307: step 2297, loss 0.08327, acc 0.96
2016-09-06T00:10:46.028106: step 2298, loss 0.0311697, acc 0.98
2016-09-06T00:10:46.825137: step 2299, loss 0.0455202, acc 0.98
2016-09-06T00:10:47.644113: step 2300, loss 0.0220281, acc 1

Evaluation:
2016-09-06T00:10:51.395742: step 2300, loss 1.54845, acc 0.745779

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-2300

2016-09-06T00:10:53.337297: step 2301, loss 0.0679762, acc 0.94
2016-09-06T00:10:54.113981: step 2302, loss 0.0812031, acc 0.96
2016-09-06T00:10:54.940584: step 2303, loss 0.0475414, acc 0.98
2016-09-06T00:10:55.688353: step 2304, loss 0.125537, acc 0.977273
2016-09-06T00:10:56.512113: step 2305, loss 0.0148672, acc 1
2016-09-06T00:10:57.318187: step 2306, loss 0.0507534, acc 0.96
2016-09-06T00:10:58.126548: step 2307, loss 0.022616, acc 1
2016-09-06T00:10:58.972635: step 2308, loss 0.046988, acc 0.98
2016-09-06T00:10:59.751648: step 2309, loss 0.0321812, acc 0.98
2016-09-06T00:11:00.599250: step 2310, loss 0.0092953, acc 1
2016-09-06T00:11:01.366424: step 2311, loss 0.0174054, acc 1
2016-09-06T00:11:02.151352: step 2312, loss 0.0200241, acc 0.98
2016-09-06T00:11:03.000053: step 2313, loss 0.0295067, acc 0.98
2016-09-06T00:11:03.793404: step 2314, loss 0.0105483, acc 1
2016-09-06T00:11:04.602784: step 2315, loss 0.0310276, acc 0.98
2016-09-06T00:11:05.395879: step 2316, loss 0.0248402, acc 0.98
2016-09-06T00:11:06.182730: step 2317, loss 0.0479172, acc 0.98
2016-09-06T00:11:06.975276: step 2318, loss 0.0138596, acc 1
2016-09-06T00:11:07.803072: step 2319, loss 0.0331028, acc 0.98
2016-09-06T00:11:08.624554: step 2320, loss 0.0328359, acc 1
2016-09-06T00:11:09.445459: step 2321, loss 0.0388338, acc 0.96
2016-09-06T00:11:10.283700: step 2322, loss 0.0320057, acc 0.98
2016-09-06T00:11:11.143678: step 2323, loss 0.0309923, acc 1
2016-09-06T00:11:11.974935: step 2324, loss 0.0824276, acc 0.96
2016-09-06T00:11:12.838693: step 2325, loss 0.0490797, acc 0.96
2016-09-06T00:11:13.648261: step 2326, loss 0.0739296, acc 0.94
2016-09-06T00:11:14.470159: step 2327, loss 0.069766, acc 0.94
2016-09-06T00:11:15.273456: step 2328, loss 0.0558974, acc 0.98
2016-09-06T00:11:16.102660: step 2329, loss 0.0776505, acc 0.96
2016-09-06T00:11:16.900600: step 2330, loss 0.0102885, acc 1
2016-09-06T00:11:17.718412: step 2331, loss 0.00529026, acc 1
2016-09-06T00:11:18.542008: step 2332, loss 0.0674756, acc 0.98
2016-09-06T00:11:19.354368: step 2333, loss 0.0420742, acc 0.98
2016-09-06T00:11:20.184411: step 2334, loss 0.0411252, acc 0.98
2016-09-06T00:11:21.003616: step 2335, loss 0.0850159, acc 0.94
2016-09-06T00:11:21.826726: step 2336, loss 0.014121, acc 1
2016-09-06T00:11:22.659436: step 2337, loss 0.0591885, acc 0.98
2016-09-06T00:11:23.466479: step 2338, loss 0.0372049, acc 0.98
2016-09-06T00:11:24.282299: step 2339, loss 0.051124, acc 0.98
2016-09-06T00:11:25.076849: step 2340, loss 0.0170224, acc 1
2016-09-06T00:11:25.870235: step 2341, loss 0.0158309, acc 1
2016-09-06T00:11:26.660631: step 2342, loss 0.0708195, acc 0.98
2016-09-06T00:11:27.498179: step 2343, loss 0.0421653, acc 0.96
2016-09-06T00:11:28.339476: step 2344, loss 0.146844, acc 0.96
2016-09-06T00:11:29.160334: step 2345, loss 0.028313, acc 1
2016-09-06T00:11:29.963080: step 2346, loss 0.197177, acc 0.94
2016-09-06T00:11:30.754260: step 2347, loss 0.115916, acc 0.96
2016-09-06T00:11:31.545583: step 2348, loss 0.0999659, acc 0.96
2016-09-06T00:11:32.336508: step 2349, loss 0.00646596, acc 1
2016-09-06T00:11:33.152614: step 2350, loss 0.0528838, acc 0.98
2016-09-06T00:11:33.939392: step 2351, loss 0.0215622, acc 1
2016-09-06T00:11:34.741125: step 2352, loss 0.0990356, acc 0.98
2016-09-06T00:11:35.550858: step 2353, loss 0.058714, acc 0.96
2016-09-06T00:11:36.423757: step 2354, loss 0.029192, acc 1
2016-09-06T00:11:37.255046: step 2355, loss 0.0493259, acc 0.98
2016-09-06T00:11:38.100940: step 2356, loss 0.0972439, acc 0.94
2016-09-06T00:11:38.919602: step 2357, loss 0.0528253, acc 0.96
2016-09-06T00:11:39.738363: step 2358, loss 0.0145283, acc 1
2016-09-06T00:11:40.576667: step 2359, loss 0.0636868, acc 0.96
2016-09-06T00:11:41.388135: step 2360, loss 0.0884057, acc 0.98
2016-09-06T00:11:42.191761: step 2361, loss 0.0238325, acc 1
2016-09-06T00:11:43.045615: step 2362, loss 0.109488, acc 0.94
2016-09-06T00:11:43.861963: step 2363, loss 0.020694, acc 1
2016-09-06T00:11:44.686638: step 2364, loss 0.0335644, acc 1
2016-09-06T00:11:45.505446: step 2365, loss 0.0485651, acc 0.96
2016-09-06T00:11:46.330965: step 2366, loss 0.0126963, acc 1
2016-09-06T00:11:47.149679: step 2367, loss 0.00813916, acc 1
2016-09-06T00:11:47.994412: step 2368, loss 0.0539187, acc 0.96
2016-09-06T00:11:48.805994: step 2369, loss 0.0174365, acc 1
2016-09-06T00:11:49.598933: step 2370, loss 0.106303, acc 0.96
2016-09-06T00:11:50.422624: step 2371, loss 0.0815615, acc 0.98
2016-09-06T00:11:51.240630: step 2372, loss 0.112879, acc 0.94
2016-09-06T00:11:52.044217: step 2373, loss 0.0406855, acc 1
2016-09-06T00:11:52.878121: step 2374, loss 0.0656281, acc 0.96
2016-09-06T00:11:53.678626: step 2375, loss 0.0204066, acc 1
2016-09-06T00:11:54.473662: step 2376, loss 0.0310561, acc 0.98
2016-09-06T00:11:55.280716: step 2377, loss 0.0419462, acc 1
2016-09-06T00:11:56.076486: step 2378, loss 0.03441, acc 0.98
2016-09-06T00:11:56.910893: step 2379, loss 0.0328377, acc 0.98
2016-09-06T00:11:57.721338: step 2380, loss 0.047541, acc 1
2016-09-06T00:11:58.546044: step 2381, loss 0.100923, acc 0.98
2016-09-06T00:11:59.321719: step 2382, loss 0.0316224, acc 1
2016-09-06T00:12:00.144582: step 2383, loss 0.0241345, acc 1
2016-09-06T00:12:01.028007: step 2384, loss 0.0267942, acc 1
2016-09-06T00:12:01.807904: step 2385, loss 0.090748, acc 0.96
2016-09-06T00:12:02.585236: step 2386, loss 0.0882716, acc 0.96
2016-09-06T00:12:03.411170: step 2387, loss 0.0692041, acc 0.96
2016-09-06T00:12:04.184953: step 2388, loss 0.0562044, acc 0.96
2016-09-06T00:12:04.985183: step 2389, loss 0.0911201, acc 0.96
2016-09-06T00:12:05.797116: step 2390, loss 0.0198043, acc 0.98
2016-09-06T00:12:06.600403: step 2391, loss 0.0156677, acc 1
2016-09-06T00:12:07.417100: step 2392, loss 0.116634, acc 0.98
2016-09-06T00:12:08.255878: step 2393, loss 0.0337667, acc 1
2016-09-06T00:12:09.044600: step 2394, loss 0.0139021, acc 1
2016-09-06T00:12:09.852382: step 2395, loss 0.0100614, acc 1
2016-09-06T00:12:10.667895: step 2396, loss 0.0113883, acc 1
2016-09-06T00:12:11.470590: step 2397, loss 0.00941658, acc 1
2016-09-06T00:12:12.269464: step 2398, loss 0.04143, acc 0.98
2016-09-06T00:12:13.100442: step 2399, loss 0.0149773, acc 1
2016-09-06T00:12:13.903663: step 2400, loss 0.0104401, acc 1

Evaluation:
2016-09-06T00:12:17.619046: step 2400, loss 1.58061, acc 0.755159

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-2400

2016-09-06T00:12:19.495197: step 2401, loss 0.0083959, acc 1
2016-09-06T00:12:20.330503: step 2402, loss 0.0218351, acc 0.98
2016-09-06T00:12:21.170056: step 2403, loss 0.173544, acc 0.98
2016-09-06T00:12:22.004943: step 2404, loss 0.0545524, acc 0.96
2016-09-06T00:12:22.798633: step 2405, loss 0.100651, acc 0.94
2016-09-06T00:12:23.613818: step 2406, loss 0.0503848, acc 0.96
2016-09-06T00:12:24.415996: step 2407, loss 0.0585774, acc 0.98
2016-09-06T00:12:25.208999: step 2408, loss 0.019005, acc 1
2016-09-06T00:12:26.023148: step 2409, loss 0.0211227, acc 1
2016-09-06T00:12:26.879341: step 2410, loss 0.0795941, acc 0.98
2016-09-06T00:12:27.693052: step 2411, loss 0.0938342, acc 0.96
2016-09-06T00:12:28.509430: step 2412, loss 0.0474033, acc 0.98
2016-09-06T00:12:29.321021: step 2413, loss 0.0394499, acc 0.98
2016-09-06T00:12:30.143176: step 2414, loss 0.069578, acc 0.96
2016-09-06T00:12:30.945164: step 2415, loss 0.00530051, acc 1
2016-09-06T00:12:31.763757: step 2416, loss 0.0998681, acc 0.94
2016-09-06T00:12:32.584290: step 2417, loss 0.0334826, acc 1
2016-09-06T00:12:33.396763: step 2418, loss 0.024459, acc 0.98
2016-09-06T00:12:34.213838: step 2419, loss 0.079849, acc 0.94
2016-09-06T00:12:35.032144: step 2420, loss 0.0682068, acc 0.96
2016-09-06T00:12:35.829081: step 2421, loss 0.027303, acc 0.98
2016-09-06T00:12:36.666448: step 2422, loss 0.0196329, acc 0.98
2016-09-06T00:12:37.507446: step 2423, loss 0.0585643, acc 0.96
2016-09-06T00:12:38.315832: step 2424, loss 0.0710374, acc 0.96
2016-09-06T00:12:39.124770: step 2425, loss 0.0055517, acc 1
2016-09-06T00:12:39.964628: step 2426, loss 0.00696774, acc 1
2016-09-06T00:12:40.782270: step 2427, loss 0.0223671, acc 0.98
2016-09-06T00:12:41.593604: step 2428, loss 0.108773, acc 0.96
2016-09-06T00:12:42.439798: step 2429, loss 0.0483145, acc 0.98
2016-09-06T00:12:43.246873: step 2430, loss 0.0736239, acc 1
2016-09-06T00:12:44.044536: step 2431, loss 0.0212163, acc 1
2016-09-06T00:12:44.865956: step 2432, loss 0.0422453, acc 0.98
2016-09-06T00:12:45.684411: step 2433, loss 0.0141512, acc 1
2016-09-06T00:12:46.509038: step 2434, loss 0.145764, acc 0.92
2016-09-06T00:12:47.387531: step 2435, loss 0.0987485, acc 0.94
2016-09-06T00:12:48.200156: step 2436, loss 0.0768161, acc 0.96
2016-09-06T00:12:49.017553: step 2437, loss 0.0470339, acc 0.98
2016-09-06T00:12:49.821962: step 2438, loss 0.239618, acc 0.96
2016-09-06T00:12:50.647246: step 2439, loss 0.0707094, acc 0.96
2016-09-06T00:12:51.433816: step 2440, loss 0.0252243, acc 1
2016-09-06T00:12:52.238803: step 2441, loss 0.0466705, acc 0.98
2016-09-06T00:12:53.062813: step 2442, loss 0.113635, acc 0.96
2016-09-06T00:12:53.848963: step 2443, loss 0.0463512, acc 0.98
2016-09-06T00:12:54.664379: step 2444, loss 0.0209961, acc 1
2016-09-06T00:12:55.491230: step 2445, loss 0.167373, acc 0.94
2016-09-06T00:12:56.294789: step 2446, loss 0.0326201, acc 0.98
2016-09-06T00:12:57.100474: step 2447, loss 0.0057005, acc 1
2016-09-06T00:12:57.927487: step 2448, loss 0.0576223, acc 0.98
2016-09-06T00:12:58.699073: step 2449, loss 0.0419565, acc 0.98
2016-09-06T00:12:59.494693: step 2450, loss 0.0197607, acc 1
2016-09-06T00:13:00.325428: step 2451, loss 0.0849665, acc 0.94
2016-09-06T00:13:01.077131: step 2452, loss 0.0482383, acc 0.98
2016-09-06T00:13:01.908861: step 2453, loss 0.0472328, acc 0.98
2016-09-06T00:13:02.719660: step 2454, loss 0.0274509, acc 1
2016-09-06T00:13:03.506399: step 2455, loss 0.0589816, acc 0.98
2016-09-06T00:13:04.331719: step 2456, loss 0.105804, acc 0.96
2016-09-06T00:13:05.138027: step 2457, loss 0.0299411, acc 1
2016-09-06T00:13:05.924213: step 2458, loss 0.0166207, acc 1
2016-09-06T00:13:06.720881: step 2459, loss 0.0514364, acc 0.98
2016-09-06T00:13:07.540956: step 2460, loss 0.0690098, acc 0.98
2016-09-06T00:13:08.340351: step 2461, loss 0.0480921, acc 0.96
2016-09-06T00:13:09.152477: step 2462, loss 0.0454822, acc 0.98
2016-09-06T00:13:09.970928: step 2463, loss 0.0241774, acc 1
2016-09-06T00:13:10.767395: step 2464, loss 0.0280464, acc 1
2016-09-06T00:13:11.593580: step 2465, loss 0.0515542, acc 0.98
2016-09-06T00:13:12.379786: step 2466, loss 0.0698417, acc 0.96
2016-09-06T00:13:13.166004: step 2467, loss 0.025849, acc 1
2016-09-06T00:13:13.971372: step 2468, loss 0.0596182, acc 0.98
2016-09-06T00:13:14.790140: step 2469, loss 0.0595108, acc 0.96
2016-09-06T00:13:15.587229: step 2470, loss 0.0391269, acc 0.98
2016-09-06T00:13:16.386507: step 2471, loss 0.0303123, acc 0.98
2016-09-06T00:13:17.220358: step 2472, loss 0.0571192, acc 0.96
2016-09-06T00:13:18.014438: step 2473, loss 0.115817, acc 0.96
2016-09-06T00:13:18.859288: step 2474, loss 0.0608259, acc 0.96
2016-09-06T00:13:19.663880: step 2475, loss 0.155577, acc 0.92
2016-09-06T00:13:20.428031: step 2476, loss 0.0113755, acc 1
2016-09-06T00:13:21.239892: step 2477, loss 0.0168772, acc 1
2016-09-06T00:13:22.082456: step 2478, loss 0.0392289, acc 0.96
2016-09-06T00:13:22.876968: step 2479, loss 0.0747288, acc 0.98
2016-09-06T00:13:23.655676: step 2480, loss 0.160807, acc 0.98
2016-09-06T00:13:24.478369: step 2481, loss 0.0552438, acc 0.96
2016-09-06T00:13:25.271010: step 2482, loss 0.1139, acc 0.92
2016-09-06T00:13:26.071532: step 2483, loss 0.098143, acc 0.96
2016-09-06T00:13:26.904618: step 2484, loss 0.0529441, acc 0.96
2016-09-06T00:13:27.687843: step 2485, loss 0.0344726, acc 1
2016-09-06T00:13:28.517264: step 2486, loss 0.057421, acc 0.96
2016-09-06T00:13:29.345429: step 2487, loss 0.0724777, acc 0.96
2016-09-06T00:13:30.140342: step 2488, loss 0.0395594, acc 1
2016-09-06T00:13:30.951822: step 2489, loss 0.0222027, acc 1
2016-09-06T00:13:31.758964: step 2490, loss 0.0506074, acc 0.98
2016-09-06T00:13:32.542024: step 2491, loss 0.0499744, acc 0.98
2016-09-06T00:13:33.328352: step 2492, loss 0.0341486, acc 1
2016-09-06T00:13:34.132198: step 2493, loss 0.0725844, acc 0.96
2016-09-06T00:13:34.910779: step 2494, loss 0.0705099, acc 0.96
2016-09-06T00:13:35.716505: step 2495, loss 0.0440469, acc 0.98
2016-09-06T00:13:36.444867: step 2496, loss 0.0490746, acc 0.977273
2016-09-06T00:13:37.277414: step 2497, loss 0.0122257, acc 1
2016-09-06T00:13:38.110592: step 2498, loss 0.0224332, acc 1
2016-09-06T00:13:38.959888: step 2499, loss 0.0139491, acc 1
2016-09-06T00:13:39.762719: step 2500, loss 0.083911, acc 0.96

Evaluation:
2016-09-06T00:13:43.518326: step 2500, loss 1.40813, acc 0.749531

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-2500

2016-09-06T00:13:45.390725: step 2501, loss 0.0594543, acc 0.96
2016-09-06T00:13:46.201397: step 2502, loss 0.0628806, acc 0.98
2016-09-06T00:13:47.024659: step 2503, loss 0.186923, acc 0.94
2016-09-06T00:13:47.834311: step 2504, loss 0.0471355, acc 0.98
2016-09-06T00:13:48.660304: step 2505, loss 0.0521429, acc 0.96
2016-09-06T00:13:49.447760: step 2506, loss 0.0173506, acc 1
2016-09-06T00:13:50.251347: step 2507, loss 0.0303286, acc 1
2016-09-06T00:13:51.071823: step 2508, loss 0.0336798, acc 0.98
2016-09-06T00:13:51.858944: step 2509, loss 0.0196659, acc 1
2016-09-06T00:13:52.649320: step 2510, loss 0.0711081, acc 0.96
2016-09-06T00:13:53.469687: step 2511, loss 0.0122356, acc 1
2016-09-06T00:13:54.255087: step 2512, loss 0.0570881, acc 0.96
2016-09-06T00:13:55.075154: step 2513, loss 0.0692039, acc 0.98
2016-09-06T00:13:55.876160: step 2514, loss 0.0397066, acc 0.98
2016-09-06T00:13:56.691365: step 2515, loss 0.0688104, acc 0.98
2016-09-06T00:13:57.507353: step 2516, loss 0.0616796, acc 0.96
2016-09-06T00:13:58.320268: step 2517, loss 0.00827896, acc 1
2016-09-06T00:13:59.117156: step 2518, loss 0.0591008, acc 0.98
2016-09-06T00:13:59.932539: step 2519, loss 0.0524422, acc 0.98
2016-09-06T00:14:00.758570: step 2520, loss 0.0738049, acc 0.96
2016-09-06T00:14:01.556296: step 2521, loss 0.0210767, acc 1
2016-09-06T00:14:02.367358: step 2522, loss 0.0338156, acc 1
2016-09-06T00:14:03.164562: step 2523, loss 0.0642498, acc 0.98
2016-09-06T00:14:03.965314: step 2524, loss 0.0337086, acc 1
2016-09-06T00:14:04.807448: step 2525, loss 0.0836707, acc 0.96
2016-09-06T00:14:05.621592: step 2526, loss 0.0258385, acc 1
2016-09-06T00:14:06.391073: step 2527, loss 0.0708189, acc 0.96
2016-09-06T00:14:07.183731: step 2528, loss 0.0514458, acc 0.96
2016-09-06T00:14:07.989960: step 2529, loss 0.0238966, acc 1
2016-09-06T00:14:08.775916: step 2530, loss 0.0532692, acc 0.98
2016-09-06T00:14:09.571629: step 2531, loss 0.00587228, acc 1
2016-09-06T00:14:10.401869: step 2532, loss 0.0414293, acc 0.98
2016-09-06T00:14:11.186726: step 2533, loss 0.00923804, acc 1
2016-09-06T00:14:11.992879: step 2534, loss 0.0182037, acc 1
2016-09-06T00:14:12.816461: step 2535, loss 0.0195987, acc 1
2016-09-06T00:14:13.596622: step 2536, loss 0.00505188, acc 1
2016-09-06T00:14:14.397487: step 2537, loss 0.0477043, acc 0.96
2016-09-06T00:14:15.209216: step 2538, loss 0.0358503, acc 0.98
2016-09-06T00:14:16.009044: step 2539, loss 0.0477182, acc 0.96
2016-09-06T00:14:16.824185: step 2540, loss 0.00624732, acc 1
2016-09-06T00:14:17.646508: step 2541, loss 0.0083905, acc 1
2016-09-06T00:14:18.479416: step 2542, loss 0.0717459, acc 0.98
2016-09-06T00:14:19.290208: step 2543, loss 0.0712643, acc 0.98
2016-09-06T00:14:20.076272: step 2544, loss 0.0619029, acc 0.98
2016-09-06T00:14:20.897662: step 2545, loss 0.033026, acc 1
2016-09-06T00:14:21.674209: step 2546, loss 0.0291517, acc 0.98
2016-09-06T00:14:22.501418: step 2547, loss 0.0111557, acc 1
2016-09-06T00:14:23.273226: step 2548, loss 0.118843, acc 0.96
2016-09-06T00:14:24.068664: step 2549, loss 0.0263129, acc 1
2016-09-06T00:14:24.866666: step 2550, loss 0.164169, acc 0.96
2016-09-06T00:14:25.690841: step 2551, loss 0.0810025, acc 0.94
2016-09-06T00:14:26.471721: step 2552, loss 0.00762475, acc 1
2016-09-06T00:14:27.276683: step 2553, loss 0.0559517, acc 0.96
2016-09-06T00:14:28.070571: step 2554, loss 0.0226297, acc 1
2016-09-06T00:14:28.886939: step 2555, loss 0.0584495, acc 0.98
2016-09-06T00:14:29.687490: step 2556, loss 0.0354024, acc 1
2016-09-06T00:14:30.529325: step 2557, loss 0.135567, acc 0.96
2016-09-06T00:14:31.308889: step 2558, loss 0.0118902, acc 1
2016-09-06T00:14:32.117453: step 2559, loss 0.0172245, acc 1
2016-09-06T00:14:32.916285: step 2560, loss 0.035248, acc 0.98
2016-09-06T00:14:33.728916: step 2561, loss 0.0287388, acc 0.98
2016-09-06T00:14:34.519851: step 2562, loss 0.0543148, acc 0.98
2016-09-06T00:14:35.352346: step 2563, loss 0.0590793, acc 0.98
2016-09-06T00:14:36.188936: step 2564, loss 0.0470165, acc 0.98
2016-09-06T00:14:37.020592: step 2565, loss 0.0471548, acc 0.98
2016-09-06T00:14:37.792742: step 2566, loss 0.0210323, acc 1
2016-09-06T00:14:38.597451: step 2567, loss 0.0442182, acc 0.98
2016-09-06T00:14:39.427801: step 2568, loss 0.025743, acc 0.98
2016-09-06T00:14:40.207196: step 2569, loss 0.00960485, acc 1
2016-09-06T00:14:41.012939: step 2570, loss 0.133557, acc 0.98
2016-09-06T00:14:41.862566: step 2571, loss 0.102853, acc 0.98
2016-09-06T00:14:42.657627: step 2572, loss 0.0936057, acc 0.96
2016-09-06T00:14:43.463936: step 2573, loss 0.142623, acc 0.92
2016-09-06T00:14:44.238405: step 2574, loss 0.0276817, acc 1
2016-09-06T00:14:45.006119: step 2575, loss 0.0423183, acc 0.96
2016-09-06T00:14:45.811329: step 2576, loss 0.0131415, acc 1
2016-09-06T00:14:46.644225: step 2577, loss 0.045714, acc 0.98
2016-09-06T00:14:47.446541: step 2578, loss 0.020216, acc 0.98
2016-09-06T00:14:48.257785: step 2579, loss 0.120943, acc 0.98
2016-09-06T00:14:49.093925: step 2580, loss 0.0840045, acc 0.98
2016-09-06T00:14:49.866639: step 2581, loss 0.073167, acc 0.96
2016-09-06T00:14:50.676737: step 2582, loss 0.0404965, acc 1
2016-09-06T00:14:51.484434: step 2583, loss 0.0777185, acc 0.96
2016-09-06T00:14:52.276500: step 2584, loss 0.129699, acc 0.94
2016-09-06T00:14:53.080256: step 2585, loss 0.0364463, acc 1
2016-09-06T00:14:53.943790: step 2586, loss 0.0780697, acc 0.98
2016-09-06T00:14:54.728891: step 2587, loss 0.0372704, acc 0.98
2016-09-06T00:14:55.510543: step 2588, loss 0.0596458, acc 0.98
2016-09-06T00:14:56.335698: step 2589, loss 0.15359, acc 0.94
2016-09-06T00:14:57.115608: step 2590, loss 0.0798159, acc 0.96
2016-09-06T00:14:57.909349: step 2591, loss 0.0759227, acc 0.96
2016-09-06T00:14:58.735410: step 2592, loss 0.0366333, acc 0.98
2016-09-06T00:14:59.524587: step 2593, loss 0.0485096, acc 0.98
2016-09-06T00:15:00.345394: step 2594, loss 0.107354, acc 0.96
2016-09-06T00:15:01.161921: step 2595, loss 0.0911524, acc 0.98
2016-09-06T00:15:01.957057: step 2596, loss 0.0166503, acc 1
2016-09-06T00:15:02.779033: step 2597, loss 0.0485388, acc 0.98
2016-09-06T00:15:03.597435: step 2598, loss 0.0535769, acc 0.96
2016-09-06T00:15:04.371158: step 2599, loss 0.0147234, acc 1
2016-09-06T00:15:05.158002: step 2600, loss 0.102456, acc 0.94

Evaluation:
2016-09-06T00:15:08.902767: step 2600, loss 1.25657, acc 0.748593

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-2600

2016-09-06T00:15:10.886926: step 2601, loss 0.104376, acc 0.98
2016-09-06T00:15:11.704030: step 2602, loss 0.0444023, acc 0.98
2016-09-06T00:15:12.502372: step 2603, loss 0.030028, acc 0.98
2016-09-06T00:15:13.302949: step 2604, loss 0.0510358, acc 0.96
2016-09-06T00:15:14.091045: step 2605, loss 0.0553922, acc 0.96
2016-09-06T00:15:14.881843: step 2606, loss 0.0961289, acc 0.96
2016-09-06T00:15:15.677414: step 2607, loss 0.0647407, acc 0.98
2016-09-06T00:15:16.491675: step 2608, loss 0.068038, acc 0.98
2016-09-06T00:15:17.294464: step 2609, loss 0.0416903, acc 1
2016-09-06T00:15:18.096286: step 2610, loss 0.0330109, acc 1
2016-09-06T00:15:18.874030: step 2611, loss 0.0288758, acc 1
2016-09-06T00:15:19.666664: step 2612, loss 0.0352162, acc 1
2016-09-06T00:15:20.501626: step 2613, loss 0.0194123, acc 1
2016-09-06T00:15:21.321107: step 2614, loss 0.0333937, acc 0.98
2016-09-06T00:15:22.118203: step 2615, loss 0.0369826, acc 1
2016-09-06T00:15:22.950378: step 2616, loss 0.0457592, acc 0.98
2016-09-06T00:15:23.754103: step 2617, loss 0.00934664, acc 1
2016-09-06T00:15:24.560092: step 2618, loss 0.0597339, acc 0.98
2016-09-06T00:15:25.365432: step 2619, loss 0.00871195, acc 1
2016-09-06T00:15:26.165044: step 2620, loss 0.056809, acc 0.96
2016-09-06T00:15:26.965576: step 2621, loss 0.0801356, acc 0.96
2016-09-06T00:15:27.774425: step 2622, loss 0.0257113, acc 1
2016-09-06T00:15:28.557386: step 2623, loss 0.137679, acc 0.92
2016-09-06T00:15:29.364515: step 2624, loss 0.0568175, acc 0.96
2016-09-06T00:15:30.205295: step 2625, loss 0.097447, acc 0.98
2016-09-06T00:15:30.988684: step 2626, loss 0.0312252, acc 0.98
2016-09-06T00:15:31.785034: step 2627, loss 0.0320821, acc 0.98
2016-09-06T00:15:32.562948: step 2628, loss 0.0595735, acc 0.98
2016-09-06T00:15:33.351238: step 2629, loss 0.0734865, acc 0.96
2016-09-06T00:15:34.162287: step 2630, loss 0.0223258, acc 1
2016-09-06T00:15:34.971955: step 2631, loss 0.103235, acc 0.96
2016-09-06T00:15:35.769266: step 2632, loss 0.0258262, acc 0.98
2016-09-06T00:15:36.572753: step 2633, loss 0.0184658, acc 1
2016-09-06T00:15:37.413673: step 2634, loss 0.111431, acc 0.94
2016-09-06T00:15:38.215898: step 2635, loss 0.0462571, acc 0.98
2016-09-06T00:15:39.029219: step 2636, loss 0.0268392, acc 0.98
2016-09-06T00:15:39.850320: step 2637, loss 0.0581706, acc 0.98
2016-09-06T00:15:40.620910: step 2638, loss 0.0390893, acc 0.98
2016-09-06T00:15:41.409359: step 2639, loss 0.0427534, acc 0.98
2016-09-06T00:15:42.243972: step 2640, loss 0.0608114, acc 0.96
2016-09-06T00:15:43.027791: step 2641, loss 0.0400522, acc 1
2016-09-06T00:15:43.826246: step 2642, loss 0.0191103, acc 1
2016-09-06T00:15:44.652929: step 2643, loss 0.0396208, acc 0.98
2016-09-06T00:15:45.450538: step 2644, loss 0.0492178, acc 0.98
2016-09-06T00:15:46.248218: step 2645, loss 0.0209672, acc 1
2016-09-06T00:15:47.050904: step 2646, loss 0.0136327, acc 1
2016-09-06T00:15:47.829540: step 2647, loss 0.0351824, acc 0.98
2016-09-06T00:15:48.662538: step 2648, loss 0.0473817, acc 0.98
2016-09-06T00:15:49.493475: step 2649, loss 0.0254451, acc 0.98
2016-09-06T00:15:50.301965: step 2650, loss 0.0329149, acc 1
2016-09-06T00:15:51.119044: step 2651, loss 0.0123782, acc 1
2016-09-06T00:15:51.933480: step 2652, loss 0.0237973, acc 0.98
2016-09-06T00:15:52.719253: step 2653, loss 0.104768, acc 0.96
2016-09-06T00:15:53.536000: step 2654, loss 0.103735, acc 0.98
2016-09-06T00:15:54.380450: step 2655, loss 0.155514, acc 0.96
2016-09-06T00:15:55.174683: step 2656, loss 0.104436, acc 0.98
2016-09-06T00:15:55.965121: step 2657, loss 0.108948, acc 0.94
2016-09-06T00:15:56.758391: step 2658, loss 0.024411, acc 0.98
2016-09-06T00:15:57.530752: step 2659, loss 0.100267, acc 0.94
2016-09-06T00:15:58.341947: step 2660, loss 0.00812865, acc 1
2016-09-06T00:15:59.165979: step 2661, loss 0.00538498, acc 1
2016-09-06T00:15:59.958866: step 2662, loss 0.0507201, acc 1
2016-09-06T00:16:00.769784: step 2663, loss 0.0512018, acc 0.96
2016-09-06T00:16:01.579142: step 2664, loss 0.218341, acc 0.94
2016-09-06T00:16:02.377502: step 2665, loss 0.047875, acc 0.98
2016-09-06T00:16:03.182435: step 2666, loss 0.0345473, acc 0.98
2016-09-06T00:16:04.042687: step 2667, loss 0.0500559, acc 0.98
2016-09-06T00:16:04.830868: step 2668, loss 0.159361, acc 0.94
2016-09-06T00:16:05.614319: step 2669, loss 0.0240712, acc 1
2016-09-06T00:16:06.434951: step 2670, loss 0.0986427, acc 0.94
2016-09-06T00:16:07.206617: step 2671, loss 0.0700532, acc 0.96
2016-09-06T00:16:08.014028: step 2672, loss 0.0168997, acc 1
2016-09-06T00:16:08.860468: step 2673, loss 0.071577, acc 0.94
2016-09-06T00:16:09.644814: step 2674, loss 0.0583522, acc 0.98
2016-09-06T00:16:10.432973: step 2675, loss 0.0427812, acc 1
2016-09-06T00:16:11.253678: step 2676, loss 0.0967165, acc 0.96
2016-09-06T00:16:12.069397: step 2677, loss 0.0361843, acc 0.98
2016-09-06T00:16:12.874418: step 2678, loss 0.0257958, acc 1
2016-09-06T00:16:13.665163: step 2679, loss 0.00750897, acc 1
2016-09-06T00:16:14.473788: step 2680, loss 0.0421865, acc 0.98
2016-09-06T00:16:15.290451: step 2681, loss 0.0241845, acc 1
2016-09-06T00:16:16.122567: step 2682, loss 0.0915554, acc 0.98
2016-09-06T00:16:16.909109: step 2683, loss 0.0405889, acc 0.98
2016-09-06T00:16:17.691194: step 2684, loss 0.0333303, acc 0.98
2016-09-06T00:16:18.495561: step 2685, loss 0.051794, acc 1
2016-09-06T00:16:19.285406: step 2686, loss 0.227976, acc 0.94
2016-09-06T00:16:20.091106: step 2687, loss 0.0938875, acc 0.96
2016-09-06T00:16:20.823456: step 2688, loss 0.186449, acc 0.977273
2016-09-06T00:16:21.663135: step 2689, loss 0.0198159, acc 0.98
2016-09-06T00:16:22.471754: step 2690, loss 0.0827118, acc 0.96
2016-09-06T00:16:23.280819: step 2691, loss 0.0129368, acc 1
2016-09-06T00:16:24.075799: step 2692, loss 0.0799969, acc 0.98
2016-09-06T00:16:24.893147: step 2693, loss 0.0912738, acc 0.98
2016-09-06T00:16:25.698849: step 2694, loss 0.0362933, acc 0.98
2016-09-06T00:16:26.521533: step 2695, loss 0.0325042, acc 0.98
2016-09-06T00:16:27.335088: step 2696, loss 0.0259446, acc 1
2016-09-06T00:16:28.126448: step 2697, loss 0.018481, acc 1
2016-09-06T00:16:28.937407: step 2698, loss 0.0084432, acc 1
2016-09-06T00:16:29.757891: step 2699, loss 0.0405335, acc 0.96
2016-09-06T00:16:30.570772: step 2700, loss 0.0222003, acc 0.98

Evaluation:
2016-09-06T00:16:34.282479: step 2700, loss 1.32692, acc 0.742964

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-2700

2016-09-06T00:16:36.114554: step 2701, loss 0.00536287, acc 1
2016-09-06T00:16:36.949186: step 2702, loss 0.00902903, acc 1
2016-09-06T00:16:37.799935: step 2703, loss 0.0228034, acc 1
2016-09-06T00:16:38.594031: step 2704, loss 0.0274459, acc 1
2016-09-06T00:16:39.429014: step 2705, loss 0.0382771, acc 0.98
2016-09-06T00:16:40.266645: step 2706, loss 0.025062, acc 0.98
2016-09-06T00:16:41.107555: step 2707, loss 0.0322516, acc 0.98
2016-09-06T00:16:41.942423: step 2708, loss 0.022514, acc 1
2016-09-06T00:16:42.744787: step 2709, loss 0.0454118, acc 0.98
2016-09-06T00:16:43.536583: step 2710, loss 0.103395, acc 0.9
2016-09-06T00:16:44.367677: step 2711, loss 0.0203531, acc 1
2016-09-06T00:16:45.160925: step 2712, loss 0.0417045, acc 1
2016-09-06T00:16:45.969447: step 2713, loss 0.0555787, acc 1
2016-09-06T00:16:46.790822: step 2714, loss 0.0308162, acc 1
2016-09-06T00:16:47.600566: step 2715, loss 0.00582224, acc 1
2016-09-06T00:16:48.360037: step 2716, loss 0.00715865, acc 1
2016-09-06T00:16:49.177371: step 2717, loss 0.0465007, acc 0.98
2016-09-06T00:16:49.973432: step 2718, loss 0.00722793, acc 1
2016-09-06T00:16:50.773347: step 2719, loss 0.034105, acc 0.98
2016-09-06T00:16:51.572135: step 2720, loss 0.0368983, acc 0.98
2016-09-06T00:16:52.401825: step 2721, loss 0.0717506, acc 0.94
2016-09-06T00:16:53.203214: step 2722, loss 0.0816592, acc 0.92
2016-09-06T00:16:54.015988: step 2723, loss 0.0151546, acc 1
2016-09-06T00:16:54.843127: step 2724, loss 0.023433, acc 1
2016-09-06T00:16:55.641563: step 2725, loss 0.0098768, acc 1
2016-09-06T00:16:56.435149: step 2726, loss 0.0196804, acc 1
2016-09-06T00:16:57.260625: step 2727, loss 0.0356154, acc 0.98
2016-09-06T00:16:58.077830: step 2728, loss 0.00788977, acc 1
2016-09-06T00:16:58.876251: step 2729, loss 0.0604141, acc 0.96
2016-09-06T00:16:59.677433: step 2730, loss 0.0224619, acc 1
2016-09-06T00:17:00.495786: step 2731, loss 0.0732522, acc 0.98
2016-09-06T00:17:01.301226: step 2732, loss 0.0166381, acc 1
2016-09-06T00:17:02.115169: step 2733, loss 0.0287117, acc 0.98
2016-09-06T00:17:02.914738: step 2734, loss 0.0784707, acc 0.98
2016-09-06T00:17:03.729989: step 2735, loss 0.016676, acc 1
2016-09-06T00:17:04.567703: step 2736, loss 0.0331092, acc 0.98
2016-09-06T00:17:05.337575: step 2737, loss 0.0471701, acc 0.98
2016-09-06T00:17:06.135484: step 2738, loss 0.00484967, acc 1
2016-09-06T00:17:06.947423: step 2739, loss 0.124141, acc 0.92
2016-09-06T00:17:07.722700: step 2740, loss 0.0366021, acc 0.98
2016-09-06T00:17:08.532500: step 2741, loss 0.0541936, acc 0.96
2016-09-06T00:17:09.322143: step 2742, loss 0.0164124, acc 1
2016-09-06T00:17:10.129130: step 2743, loss 0.0360229, acc 0.98
2016-09-06T00:17:10.957478: step 2744, loss 0.0210177, acc 0.98
2016-09-06T00:17:11.781361: step 2745, loss 0.129588, acc 0.98
2016-09-06T00:17:12.563583: step 2746, loss 0.0389306, acc 0.98
2016-09-06T00:17:13.369824: step 2747, loss 0.0279868, acc 0.98
2016-09-06T00:17:14.184361: step 2748, loss 0.0645378, acc 0.98
2016-09-06T00:17:14.973382: step 2749, loss 0.0342098, acc 0.98
2016-09-06T00:17:15.790451: step 2750, loss 0.0200121, acc 1
2016-09-06T00:17:16.601966: step 2751, loss 0.0104157, acc 1
2016-09-06T00:17:17.422487: step 2752, loss 0.0677001, acc 0.98
2016-09-06T00:17:18.224641: step 2753, loss 0.016918, acc 1
2016-09-06T00:17:19.054333: step 2754, loss 0.0480633, acc 0.98
2016-09-06T00:17:19.836397: step 2755, loss 0.0241047, acc 0.98
2016-09-06T00:17:20.637793: step 2756, loss 0.0614109, acc 0.94
2016-09-06T00:17:21.468279: step 2757, loss 0.0728685, acc 0.96
2016-09-06T00:17:22.278161: step 2758, loss 0.0553786, acc 0.98
2016-09-06T00:17:23.093248: step 2759, loss 0.0826399, acc 0.94
2016-09-06T00:17:23.933523: step 2760, loss 0.0304945, acc 0.98
2016-09-06T00:17:24.720595: step 2761, loss 0.136432, acc 0.98
2016-09-06T00:17:25.519794: step 2762, loss 0.0153414, acc 1
2016-09-06T00:17:26.338075: step 2763, loss 0.114826, acc 0.9
2016-09-06T00:17:27.114457: step 2764, loss 0.0654849, acc 0.98
2016-09-06T00:17:27.897683: step 2765, loss 0.0263152, acc 1
2016-09-06T00:17:28.696788: step 2766, loss 0.048157, acc 0.98
2016-09-06T00:17:29.480442: step 2767, loss 0.0481049, acc 0.98
2016-09-06T00:17:30.308254: step 2768, loss 0.0344096, acc 0.98
2016-09-06T00:17:31.130649: step 2769, loss 0.0776427, acc 0.98
2016-09-06T00:17:31.923356: step 2770, loss 0.0358978, acc 0.98
2016-09-06T00:17:32.714593: step 2771, loss 0.0938853, acc 0.96
2016-09-06T00:17:33.570828: step 2772, loss 0.0569236, acc 0.98
2016-09-06T00:17:34.350517: step 2773, loss 0.053622, acc 0.98
2016-09-06T00:17:35.163146: step 2774, loss 0.0987548, acc 0.94
2016-09-06T00:17:35.992267: step 2775, loss 0.0268191, acc 0.98
2016-09-06T00:17:36.769536: step 2776, loss 0.0309989, acc 1
2016-09-06T00:17:37.575716: step 2777, loss 0.0527376, acc 0.98
2016-09-06T00:17:38.371443: step 2778, loss 0.0478611, acc 0.98
2016-09-06T00:17:39.175869: step 2779, loss 0.0496295, acc 0.98
2016-09-06T00:17:40.051970: step 2780, loss 0.02218, acc 1
2016-09-06T00:17:40.863711: step 2781, loss 0.0362435, acc 0.98
2016-09-06T00:17:41.655037: step 2782, loss 0.0186618, acc 1
2016-09-06T00:17:42.457235: step 2783, loss 0.0133973, acc 1
2016-09-06T00:17:43.272088: step 2784, loss 0.0120932, acc 1
2016-09-06T00:17:44.070401: step 2785, loss 0.0117809, acc 1
2016-09-06T00:17:44.869136: step 2786, loss 0.0353832, acc 1
2016-09-06T00:17:45.677093: step 2787, loss 0.139319, acc 0.98
2016-09-06T00:17:46.448988: step 2788, loss 0.010856, acc 1
2016-09-06T00:17:47.252121: step 2789, loss 0.0790771, acc 0.96
2016-09-06T00:17:48.075982: step 2790, loss 0.0214348, acc 1
2016-09-06T00:17:48.845505: step 2791, loss 0.0607468, acc 0.98
2016-09-06T00:17:49.669434: step 2792, loss 0.0325826, acc 0.98
2016-09-06T00:17:50.472900: step 2793, loss 0.0514321, acc 0.98
2016-09-06T00:17:51.281957: step 2794, loss 0.0100891, acc 1
2016-09-06T00:17:52.063940: step 2795, loss 0.0446965, acc 0.98
2016-09-06T00:17:52.871406: step 2796, loss 0.0340256, acc 1
2016-09-06T00:17:53.682614: step 2797, loss 0.093867, acc 0.98
2016-09-06T00:17:54.483957: step 2798, loss 0.0488334, acc 0.98
2016-09-06T00:17:55.302728: step 2799, loss 0.0328764, acc 0.98
2016-09-06T00:17:56.107170: step 2800, loss 0.0704465, acc 0.96

Evaluation:
2016-09-06T00:17:59.806234: step 2800, loss 1.83064, acc 0.736398

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-2800

2016-09-06T00:18:01.736773: step 2801, loss 0.0711886, acc 0.98
2016-09-06T00:18:02.563902: step 2802, loss 0.114311, acc 0.96
2016-09-06T00:18:03.381448: step 2803, loss 0.126286, acc 0.96
2016-09-06T00:18:04.229396: step 2804, loss 0.00828319, acc 1
2016-09-06T00:18:05.047313: step 2805, loss 0.0415147, acc 0.98
2016-09-06T00:18:05.833400: step 2806, loss 0.0175854, acc 1
2016-09-06T00:18:06.611252: step 2807, loss 0.192482, acc 0.92
2016-09-06T00:18:07.441397: step 2808, loss 0.0105753, acc 1
2016-09-06T00:18:08.225229: step 2809, loss 0.0425947, acc 1
2016-09-06T00:18:09.049406: step 2810, loss 0.0184222, acc 1
2016-09-06T00:18:09.849070: step 2811, loss 0.0537896, acc 0.98
2016-09-06T00:18:10.664541: step 2812, loss 0.0109222, acc 1
2016-09-06T00:18:11.448623: step 2813, loss 0.051472, acc 0.98
2016-09-06T00:18:12.284217: step 2814, loss 0.0270375, acc 1
2016-09-06T00:18:13.058070: step 2815, loss 0.0650392, acc 0.94
2016-09-06T00:18:13.883207: step 2816, loss 0.072315, acc 0.98
2016-09-06T00:18:14.689409: step 2817, loss 0.0173841, acc 1
2016-09-06T00:18:15.463946: step 2818, loss 0.0152106, acc 1
2016-09-06T00:18:16.261266: step 2819, loss 0.0324264, acc 0.98
2016-09-06T00:18:17.101067: step 2820, loss 0.11389, acc 0.94
2016-09-06T00:18:17.913626: step 2821, loss 0.0757519, acc 0.98
2016-09-06T00:18:18.727767: step 2822, loss 0.0410078, acc 0.98
2016-09-06T00:18:19.534168: step 2823, loss 0.128808, acc 0.94
2016-09-06T00:18:20.335629: step 2824, loss 0.0735231, acc 0.96
2016-09-06T00:18:21.110488: step 2825, loss 0.0296132, acc 1
2016-09-06T00:18:21.919392: step 2826, loss 0.0547022, acc 0.98
2016-09-06T00:18:22.695148: step 2827, loss 0.0160536, acc 1
2016-09-06T00:18:23.511750: step 2828, loss 0.0338812, acc 1
2016-09-06T00:18:24.322470: step 2829, loss 0.0885606, acc 0.94
2016-09-06T00:18:25.120249: step 2830, loss 0.0362014, acc 1
2016-09-06T00:18:25.928817: step 2831, loss 0.0159671, acc 1
2016-09-06T00:18:26.761411: step 2832, loss 0.049585, acc 0.96
2016-09-06T00:18:27.556208: step 2833, loss 0.0275996, acc 1
2016-09-06T00:18:28.379849: step 2834, loss 0.0287646, acc 1
2016-09-06T00:18:29.182654: step 2835, loss 0.0519552, acc 0.98
2016-09-06T00:18:29.979278: step 2836, loss 0.0208077, acc 1
2016-09-06T00:18:30.776915: step 2837, loss 0.0171348, acc 1
2016-09-06T00:18:31.588227: step 2838, loss 0.0303616, acc 1
2016-09-06T00:18:32.385474: step 2839, loss 0.0127511, acc 1
2016-09-06T00:18:33.198642: step 2840, loss 0.0336685, acc 0.98
2016-09-06T00:18:34.032399: step 2841, loss 0.0585574, acc 0.98
2016-09-06T00:18:34.809034: step 2842, loss 0.0176599, acc 1
2016-09-06T00:18:35.613238: step 2843, loss 0.0128918, acc 1
2016-09-06T00:18:36.450192: step 2844, loss 0.0592591, acc 0.96
2016-09-06T00:18:37.224852: step 2845, loss 0.169081, acc 0.96
2016-09-06T00:18:38.066506: step 2846, loss 0.0277581, acc 0.98
2016-09-06T00:18:38.865953: step 2847, loss 0.0212081, acc 1
2016-09-06T00:18:39.629356: step 2848, loss 0.16343, acc 0.96
2016-09-06T00:18:40.429669: step 2849, loss 0.0239387, acc 1
2016-09-06T00:18:41.233931: step 2850, loss 0.0128222, acc 1
2016-09-06T00:18:42.011161: step 2851, loss 0.0717052, acc 0.96
2016-09-06T00:18:42.841800: step 2852, loss 0.0634186, acc 0.96
2016-09-06T00:18:43.658854: step 2853, loss 0.0460554, acc 0.98
2016-09-06T00:18:44.454929: step 2854, loss 0.0705546, acc 0.98
2016-09-06T00:18:45.266362: step 2855, loss 0.122142, acc 0.98
2016-09-06T00:18:46.095117: step 2856, loss 0.045753, acc 0.98
2016-09-06T00:18:46.879127: step 2857, loss 0.0593188, acc 0.96
2016-09-06T00:18:47.681683: step 2858, loss 0.0185426, acc 1
2016-09-06T00:18:48.470835: step 2859, loss 0.025319, acc 1
2016-09-06T00:18:49.269096: step 2860, loss 0.055782, acc 0.98
2016-09-06T00:18:50.092919: step 2861, loss 0.093612, acc 0.98
2016-09-06T00:18:50.930163: step 2862, loss 0.00827868, acc 1
2016-09-06T00:18:51.759250: step 2863, loss 0.0379455, acc 0.98
2016-09-06T00:18:52.577367: step 2864, loss 0.0191511, acc 1
2016-09-06T00:18:53.397089: step 2865, loss 0.0337759, acc 1
2016-09-06T00:18:54.187548: step 2866, loss 0.187307, acc 0.88
2016-09-06T00:18:55.004324: step 2867, loss 0.0349388, acc 0.98
2016-09-06T00:18:55.806754: step 2868, loss 0.0445263, acc 0.98
2016-09-06T00:18:56.610605: step 2869, loss 0.0345112, acc 1
2016-09-06T00:18:57.428437: step 2870, loss 0.0423024, acc 0.98
2016-09-06T00:18:58.306384: step 2871, loss 0.041742, acc 0.96
2016-09-06T00:18:59.114656: step 2872, loss 0.0127648, acc 1
2016-09-06T00:18:59.896443: step 2873, loss 0.064626, acc 0.96
2016-09-06T00:19:00.725529: step 2874, loss 0.0432657, acc 0.98
2016-09-06T00:19:01.535732: step 2875, loss 0.0964929, acc 0.96
2016-09-06T00:19:02.341816: step 2876, loss 0.0352499, acc 0.96
2016-09-06T00:19:03.145914: step 2877, loss 0.163879, acc 0.98
2016-09-06T00:19:03.950670: step 2878, loss 0.103074, acc 0.96
2016-09-06T00:19:04.758215: step 2879, loss 0.0437352, acc 0.98
2016-09-06T00:19:05.519025: step 2880, loss 0.0302457, acc 1
2016-09-06T00:19:06.331686: step 2881, loss 0.034216, acc 0.98
2016-09-06T00:19:07.178772: step 2882, loss 0.0524265, acc 0.98
2016-09-06T00:19:08.006690: step 2883, loss 0.0261384, acc 1
2016-09-06T00:19:08.836371: step 2884, loss 0.0766223, acc 0.96
2016-09-06T00:19:09.662681: step 2885, loss 0.0132121, acc 1
2016-09-06T00:19:10.497377: step 2886, loss 0.0508629, acc 0.98
2016-09-06T00:19:11.296274: step 2887, loss 0.00443544, acc 1
2016-09-06T00:19:12.101481: step 2888, loss 0.0299707, acc 0.98
2016-09-06T00:19:12.914818: step 2889, loss 0.0433006, acc 0.98
2016-09-06T00:19:13.693435: step 2890, loss 0.100434, acc 0.96
2016-09-06T00:19:14.493128: step 2891, loss 0.075474, acc 0.96
2016-09-06T00:19:15.298151: step 2892, loss 0.0182732, acc 1
2016-09-06T00:19:16.142999: step 2893, loss 0.0383656, acc 0.98
2016-09-06T00:19:16.952544: step 2894, loss 0.0331681, acc 1
2016-09-06T00:19:17.773247: step 2895, loss 0.0100316, acc 1
2016-09-06T00:19:18.603925: step 2896, loss 0.0695039, acc 0.94
2016-09-06T00:19:19.430270: step 2897, loss 0.0924487, acc 0.98
2016-09-06T00:19:20.269017: step 2898, loss 0.0157898, acc 1
2016-09-06T00:19:21.090011: step 2899, loss 0.0497344, acc 0.98
2016-09-06T00:19:21.879293: step 2900, loss 0.0085234, acc 1

Evaluation:
2016-09-06T00:19:25.619408: step 2900, loss 1.57376, acc 0.734522

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-2900

2016-09-06T00:19:27.506773: step 2901, loss 0.0328492, acc 0.98
2016-09-06T00:19:28.314563: step 2902, loss 0.0552734, acc 0.98
2016-09-06T00:19:29.133422: step 2903, loss 0.057616, acc 0.98
2016-09-06T00:19:29.968740: step 2904, loss 0.0302832, acc 0.98
2016-09-06T00:19:30.785449: step 2905, loss 0.030934, acc 0.98
2016-09-06T00:19:31.587253: step 2906, loss 0.0284697, acc 1
2016-09-06T00:19:32.393936: step 2907, loss 0.00999064, acc 1
2016-09-06T00:19:33.213485: step 2908, loss 0.0135965, acc 1
2016-09-06T00:19:34.041772: step 2909, loss 0.0202777, acc 1
2016-09-06T00:19:34.869464: step 2910, loss 0.0167357, acc 1
2016-09-06T00:19:35.687984: step 2911, loss 0.0455673, acc 0.98
2016-09-06T00:19:36.470714: step 2912, loss 0.0220611, acc 0.98
2016-09-06T00:19:37.276754: step 2913, loss 0.0268719, acc 1
2016-09-06T00:19:38.113591: step 2914, loss 0.111334, acc 0.92
2016-09-06T00:19:38.901342: step 2915, loss 0.0374887, acc 0.98
2016-09-06T00:19:39.676126: step 2916, loss 0.0577095, acc 0.98
2016-09-06T00:19:40.484869: step 2917, loss 0.0473555, acc 0.98
2016-09-06T00:19:41.282420: step 2918, loss 0.0223108, acc 1
2016-09-06T00:19:42.066738: step 2919, loss 0.0217779, acc 0.98
2016-09-06T00:19:42.886813: step 2920, loss 0.0237381, acc 0.98
2016-09-06T00:19:43.701701: step 2921, loss 0.0142468, acc 1
2016-09-06T00:19:44.513243: step 2922, loss 0.0201068, acc 0.98
2016-09-06T00:19:45.323855: step 2923, loss 0.0643229, acc 0.96
2016-09-06T00:19:46.115760: step 2924, loss 0.00670842, acc 1
2016-09-06T00:19:46.926145: step 2925, loss 0.0129235, acc 1
2016-09-06T00:19:47.733030: step 2926, loss 0.028598, acc 1
2016-09-06T00:19:48.518133: step 2927, loss 0.0100278, acc 1
2016-09-06T00:19:49.311515: step 2928, loss 0.0246221, acc 1
2016-09-06T00:19:50.128127: step 2929, loss 0.0488605, acc 0.98
2016-09-06T00:19:50.930181: step 2930, loss 0.0456042, acc 0.98
2016-09-06T00:19:51.743453: step 2931, loss 0.0244988, acc 0.98
2016-09-06T00:19:52.550413: step 2932, loss 0.0408463, acc 0.98
2016-09-06T00:19:53.343773: step 2933, loss 0.00543752, acc 1
2016-09-06T00:19:54.165868: step 2934, loss 0.0203039, acc 1
2016-09-06T00:19:54.983262: step 2935, loss 0.108269, acc 0.94
2016-09-06T00:19:55.783251: step 2936, loss 0.00819593, acc 1
2016-09-06T00:19:56.584711: step 2937, loss 0.0172234, acc 1
2016-09-06T00:19:57.419622: step 2938, loss 0.120496, acc 0.96
2016-09-06T00:19:58.216068: step 2939, loss 0.105321, acc 0.96
2016-09-06T00:19:59.012634: step 2940, loss 0.00539822, acc 1
2016-09-06T00:19:59.845958: step 2941, loss 0.0683286, acc 0.96
2016-09-06T00:20:00.661404: step 2942, loss 0.00831891, acc 1
2016-09-06T00:20:01.492219: step 2943, loss 0.0118919, acc 1
2016-09-06T00:20:02.293778: step 2944, loss 0.00439318, acc 1
2016-09-06T00:20:03.077442: step 2945, loss 0.0282971, acc 1
2016-09-06T00:20:03.872506: step 2946, loss 0.0743678, acc 0.98
2016-09-06T00:20:04.701786: step 2947, loss 0.0302075, acc 1
2016-09-06T00:20:05.497356: step 2948, loss 0.0724528, acc 0.98
2016-09-06T00:20:06.289035: step 2949, loss 0.0210848, acc 1
2016-09-06T00:20:07.126518: step 2950, loss 0.0434634, acc 0.98
2016-09-06T00:20:07.890240: step 2951, loss 0.0693626, acc 0.98
2016-09-06T00:20:08.692316: step 2952, loss 0.0520763, acc 0.98
2016-09-06T00:20:09.508971: step 2953, loss 0.0387008, acc 0.98
2016-09-06T00:20:10.296286: step 2954, loss 0.0593883, acc 0.98
2016-09-06T00:20:11.101912: step 2955, loss 0.00755068, acc 1
2016-09-06T00:20:11.927036: step 2956, loss 0.030581, acc 1
2016-09-06T00:20:12.707070: step 2957, loss 0.0155076, acc 1
2016-09-06T00:20:13.506439: step 2958, loss 0.0692489, acc 0.94
2016-09-06T00:20:14.330326: step 2959, loss 0.0112797, acc 1
2016-09-06T00:20:15.110508: step 2960, loss 0.180022, acc 0.96
2016-09-06T00:20:15.912112: step 2961, loss 0.0183836, acc 1
2016-09-06T00:20:16.738956: step 2962, loss 0.024471, acc 0.98
2016-09-06T00:20:17.512682: step 2963, loss 0.0518162, acc 0.98
2016-09-06T00:20:18.343922: step 2964, loss 0.0282722, acc 1
2016-09-06T00:20:19.159980: step 2965, loss 0.0429425, acc 0.98
2016-09-06T00:20:19.953422: step 2966, loss 0.034113, acc 1
2016-09-06T00:20:20.746787: step 2967, loss 0.0172332, acc 1
2016-09-06T00:20:21.570623: step 2968, loss 0.0313839, acc 0.98
2016-09-06T00:20:22.369477: step 2969, loss 0.0201309, acc 1
2016-09-06T00:20:23.191129: step 2970, loss 0.00526129, acc 1
2016-09-06T00:20:24.037297: step 2971, loss 0.051197, acc 0.98
2016-09-06T00:20:24.817406: step 2972, loss 0.0469805, acc 0.98
2016-09-06T00:20:25.624865: step 2973, loss 0.0206557, acc 1
2016-09-06T00:20:26.458294: step 2974, loss 0.0130478, acc 1
2016-09-06T00:20:27.237217: step 2975, loss 0.0192108, acc 1
2016-09-06T00:20:28.050334: step 2976, loss 0.0115017, acc 1
2016-09-06T00:20:28.853218: step 2977, loss 0.0526282, acc 0.98
2016-09-06T00:20:29.657557: step 2978, loss 0.0334293, acc 0.98
2016-09-06T00:20:30.466494: step 2979, loss 0.00505929, acc 1
2016-09-06T00:20:31.281284: step 2980, loss 0.00728365, acc 1
2016-09-06T00:20:32.068606: step 2981, loss 0.0218002, acc 0.98
2016-09-06T00:20:32.868220: step 2982, loss 0.0389443, acc 0.98
2016-09-06T00:20:33.704008: step 2983, loss 0.0111998, acc 1
2016-09-06T00:20:34.466174: step 2984, loss 0.0237294, acc 1
2016-09-06T00:20:35.270002: step 2985, loss 0.0354253, acc 1
2016-09-06T00:20:36.084240: step 2986, loss 0.00538398, acc 1
2016-09-06T00:20:36.877330: step 2987, loss 0.0952818, acc 0.94
2016-09-06T00:20:37.665130: step 2988, loss 0.0356396, acc 1
2016-09-06T00:20:38.488866: step 2989, loss 0.104425, acc 0.94
2016-09-06T00:20:39.271332: step 2990, loss 0.0479309, acc 0.98
2016-09-06T00:20:40.087124: step 2991, loss 0.0180123, acc 1
2016-09-06T00:20:40.895188: step 2992, loss 0.0366239, acc 0.98
2016-09-06T00:20:41.693441: step 2993, loss 0.0305456, acc 1
2016-09-06T00:20:42.514067: step 2994, loss 0.0297843, acc 0.98
2016-09-06T00:20:43.316266: step 2995, loss 0.0266282, acc 1
2016-09-06T00:20:44.108320: step 2996, loss 0.0141619, acc 1
2016-09-06T00:20:44.920259: step 2997, loss 0.0679976, acc 0.96
2016-09-06T00:20:45.751115: step 2998, loss 0.0291178, acc 0.98
2016-09-06T00:20:46.552760: step 2999, loss 0.0889608, acc 0.96
2016-09-06T00:20:47.343886: step 3000, loss 0.0774658, acc 0.98

Evaluation:
2016-09-06T00:20:51.049459: step 3000, loss 1.88154, acc 0.747655

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-3000

2016-09-06T00:20:52.969837: step 3001, loss 0.00435858, acc 1
2016-09-06T00:20:53.772575: step 3002, loss 0.00822606, acc 1
2016-09-06T00:20:54.580362: step 3003, loss 0.056137, acc 0.98
2016-09-06T00:20:55.369365: step 3004, loss 0.0368121, acc 1
2016-09-06T00:20:56.180001: step 3005, loss 0.0444113, acc 0.98
2016-09-06T00:20:57.014787: step 3006, loss 0.0324059, acc 1
2016-09-06T00:20:57.824828: step 3007, loss 0.0226789, acc 1
2016-09-06T00:20:58.645498: step 3008, loss 0.0903955, acc 0.96
2016-09-06T00:20:59.474774: step 3009, loss 0.063275, acc 0.98
2016-09-06T00:21:00.314752: step 3010, loss 0.0350033, acc 1
2016-09-06T00:21:01.113292: step 3011, loss 0.0147536, acc 1
2016-09-06T00:21:01.931044: step 3012, loss 0.0403143, acc 0.98
2016-09-06T00:21:02.759710: step 3013, loss 0.117488, acc 0.96
2016-09-06T00:21:03.547120: step 3014, loss 0.0978706, acc 0.94
2016-09-06T00:21:04.376726: step 3015, loss 0.0849097, acc 0.96
2016-09-06T00:21:05.227532: step 3016, loss 0.0472848, acc 0.98
2016-09-06T00:21:06.025405: step 3017, loss 0.100555, acc 0.96
2016-09-06T00:21:06.825737: step 3018, loss 0.0781132, acc 0.96
2016-09-06T00:21:07.626577: step 3019, loss 0.0128868, acc 1
2016-09-06T00:21:08.418160: step 3020, loss 0.0213397, acc 0.98
2016-09-06T00:21:09.218429: step 3021, loss 0.0258463, acc 1
2016-09-06T00:21:10.049466: step 3022, loss 0.0871345, acc 0.96
2016-09-06T00:21:10.841189: step 3023, loss 0.00903294, acc 1
2016-09-06T00:21:11.643051: step 3024, loss 0.0329894, acc 0.98
2016-09-06T00:21:12.488978: step 3025, loss 0.0362108, acc 0.98
2016-09-06T00:21:13.265944: step 3026, loss 0.0289249, acc 0.98
2016-09-06T00:21:14.092847: step 3027, loss 0.0875675, acc 0.96
2016-09-06T00:21:14.933112: step 3028, loss 0.0445053, acc 0.96
2016-09-06T00:21:15.734377: step 3029, loss 0.016153, acc 1
2016-09-06T00:21:16.544650: step 3030, loss 0.0458603, acc 0.96
2016-09-06T00:21:17.378187: step 3031, loss 0.0284991, acc 0.98
2016-09-06T00:21:18.212340: step 3032, loss 0.019946, acc 1
2016-09-06T00:21:19.033079: step 3033, loss 0.0158315, acc 1
2016-09-06T00:21:19.861006: step 3034, loss 0.0592993, acc 0.96
2016-09-06T00:21:20.690165: step 3035, loss 0.00440893, acc 1
2016-09-06T00:21:21.500812: step 3036, loss 0.0350372, acc 0.98
2016-09-06T00:21:22.355542: step 3037, loss 0.0172169, acc 1
2016-09-06T00:21:23.165277: step 3038, loss 0.0654824, acc 0.96
2016-09-06T00:21:23.975382: step 3039, loss 0.0340937, acc 0.98
2016-09-06T00:21:24.784460: step 3040, loss 0.0625414, acc 0.98
2016-09-06T00:21:25.626611: step 3041, loss 0.158998, acc 0.92
2016-09-06T00:21:26.438083: step 3042, loss 0.0846445, acc 0.98
2016-09-06T00:21:27.286087: step 3043, loss 0.10676, acc 0.98
2016-09-06T00:21:28.100350: step 3044, loss 0.0465938, acc 0.96
2016-09-06T00:21:28.877378: step 3045, loss 0.0328727, acc 0.98
2016-09-06T00:21:29.717053: step 3046, loss 0.0555205, acc 0.96
2016-09-06T00:21:30.501409: step 3047, loss 0.0270963, acc 0.98
2016-09-06T00:21:31.318105: step 3048, loss 0.0184385, acc 1
2016-09-06T00:21:32.132128: step 3049, loss 0.07601, acc 0.98
2016-09-06T00:21:32.947711: step 3050, loss 0.0268182, acc 1
2016-09-06T00:21:33.793501: step 3051, loss 0.0262029, acc 1
2016-09-06T00:21:34.592196: step 3052, loss 0.0494243, acc 0.98
2016-09-06T00:21:35.449462: step 3053, loss 0.0581082, acc 0.98
2016-09-06T00:21:36.225333: step 3054, loss 0.0308148, acc 1
2016-09-06T00:21:37.023631: step 3055, loss 0.0641197, acc 0.98
2016-09-06T00:21:37.879478: step 3056, loss 0.0175923, acc 1
2016-09-06T00:21:38.675344: step 3057, loss 0.0263066, acc 1
2016-09-06T00:21:39.476194: step 3058, loss 0.0678033, acc 0.96
2016-09-06T00:21:40.278030: step 3059, loss 0.0262688, acc 0.98
2016-09-06T00:21:41.051824: step 3060, loss 0.0436766, acc 1
2016-09-06T00:21:41.878921: step 3061, loss 0.0481788, acc 0.98
2016-09-06T00:21:42.682397: step 3062, loss 0.0361397, acc 1
2016-09-06T00:21:43.458069: step 3063, loss 0.0698215, acc 0.98
2016-09-06T00:21:44.285400: step 3064, loss 0.00892851, acc 1
2016-09-06T00:21:45.081412: step 3065, loss 0.0110432, acc 1
2016-09-06T00:21:45.860908: step 3066, loss 0.187681, acc 0.96
2016-09-06T00:21:46.672897: step 3067, loss 0.14945, acc 0.96
2016-09-06T00:21:47.492957: step 3068, loss 0.0354235, acc 0.98
2016-09-06T00:21:48.267013: step 3069, loss 0.062388, acc 0.98
2016-09-06T00:21:49.089472: step 3070, loss 0.0159249, acc 1
2016-09-06T00:21:49.915269: step 3071, loss 0.0337329, acc 1
2016-09-06T00:21:50.637297: step 3072, loss 0.157598, acc 0.954545
2016-09-06T00:21:51.468819: step 3073, loss 0.00425679, acc 1
2016-09-06T00:21:52.300367: step 3074, loss 0.0447695, acc 0.96
2016-09-06T00:21:53.137447: step 3075, loss 0.0303067, acc 0.98
2016-09-06T00:21:53.966478: step 3076, loss 0.0343227, acc 1
2016-09-06T00:21:54.769496: step 3077, loss 0.0129363, acc 1
2016-09-06T00:21:55.562679: step 3078, loss 0.0425322, acc 0.96
2016-09-06T00:21:56.381737: step 3079, loss 0.015142, acc 1
2016-09-06T00:21:57.203318: step 3080, loss 0.0228604, acc 1
2016-09-06T00:21:57.989965: step 3081, loss 0.027111, acc 1
2016-09-06T00:21:58.798159: step 3082, loss 0.00772957, acc 1
2016-09-06T00:21:59.615832: step 3083, loss 0.040659, acc 0.98
2016-09-06T00:22:00.462886: step 3084, loss 0.0338767, acc 0.98
2016-09-06T00:22:01.265521: step 3085, loss 0.0737402, acc 0.94
2016-09-06T00:22:02.124572: step 3086, loss 0.013002, acc 1
2016-09-06T00:22:02.917392: step 3087, loss 0.0164933, acc 1
2016-09-06T00:22:03.745575: step 3088, loss 0.167562, acc 0.92
2016-09-06T00:22:04.565451: step 3089, loss 0.051264, acc 0.96
2016-09-06T00:22:05.356829: step 3090, loss 0.0972134, acc 0.98
2016-09-06T00:22:06.154976: step 3091, loss 0.0360492, acc 1
2016-09-06T00:22:06.990616: step 3092, loss 0.0192117, acc 1
2016-09-06T00:22:07.867728: step 3093, loss 0.030823, acc 1
2016-09-06T00:22:08.683919: step 3094, loss 0.128864, acc 0.94
2016-09-06T00:22:09.525569: step 3095, loss 0.0442426, acc 1
2016-09-06T00:22:10.355691: step 3096, loss 0.0939545, acc 0.96
2016-09-06T00:22:11.185327: step 3097, loss 0.032971, acc 0.98
2016-09-06T00:22:12.010486: step 3098, loss 0.0423553, acc 1
2016-09-06T00:22:12.826792: step 3099, loss 0.0194352, acc 1
2016-09-06T00:22:13.618319: step 3100, loss 0.0347647, acc 0.98

Evaluation:
2016-09-06T00:22:17.352036: step 3100, loss 1.35811, acc 0.73546

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-3100

2016-09-06T00:22:19.283665: step 3101, loss 0.0537451, acc 0.98
2016-09-06T00:22:20.093535: step 3102, loss 0.00810022, acc 1
2016-09-06T00:22:20.900726: step 3103, loss 0.0324462, acc 0.98
2016-09-06T00:22:21.723108: step 3104, loss 0.0197549, acc 1
2016-09-06T00:22:22.530843: step 3105, loss 0.0518633, acc 0.98
2016-09-06T00:22:23.350023: step 3106, loss 0.0144927, acc 1
2016-09-06T00:22:24.190795: step 3107, loss 0.0112232, acc 1
2016-09-06T00:22:25.001662: step 3108, loss 0.0550262, acc 0.98
2016-09-06T00:22:25.804399: step 3109, loss 0.0310693, acc 0.98
2016-09-06T00:22:26.621080: step 3110, loss 0.0865639, acc 0.96
2016-09-06T00:22:27.434646: step 3111, loss 0.0214496, acc 1
2016-09-06T00:22:28.254439: step 3112, loss 0.121051, acc 0.94
2016-09-06T00:22:29.078527: step 3113, loss 0.162886, acc 0.96
2016-09-06T00:22:29.886651: step 3114, loss 0.0179756, acc 1
2016-09-06T00:22:30.687245: step 3115, loss 0.032621, acc 0.98
2016-09-06T00:22:31.508822: step 3116, loss 0.0119191, acc 1
2016-09-06T00:22:32.333366: step 3117, loss 0.00951253, acc 1
2016-09-06T00:22:33.113803: step 3118, loss 0.030684, acc 1
2016-09-06T00:22:33.911800: step 3119, loss 0.00687455, acc 1
2016-09-06T00:22:34.725113: step 3120, loss 0.0735359, acc 0.98
2016-09-06T00:22:35.510563: step 3121, loss 0.0448547, acc 0.98
2016-09-06T00:22:36.303237: step 3122, loss 0.0296087, acc 1
2016-09-06T00:22:37.108141: step 3123, loss 0.0106228, acc 1
2016-09-06T00:22:37.903194: step 3124, loss 0.0768159, acc 0.96
2016-09-06T00:22:38.720489: step 3125, loss 0.0414072, acc 1
2016-09-06T00:22:39.524036: step 3126, loss 0.0343312, acc 0.98
2016-09-06T00:22:40.329454: step 3127, loss 0.0321739, acc 0.98
2016-09-06T00:22:41.144661: step 3128, loss 0.011353, acc 1
2016-09-06T00:22:41.962011: step 3129, loss 0.0251629, acc 1
2016-09-06T00:22:42.743749: step 3130, loss 0.0221929, acc 1
2016-09-06T00:22:43.574977: step 3131, loss 0.0437277, acc 0.98
2016-09-06T00:22:44.375419: step 3132, loss 0.0103174, acc 1
2016-09-06T00:22:45.175955: step 3133, loss 0.071436, acc 0.98
2016-09-06T00:22:45.970824: step 3134, loss 0.0245224, acc 1
2016-09-06T00:22:46.764415: step 3135, loss 0.0125423, acc 1
2016-09-06T00:22:47.556566: step 3136, loss 0.00644883, acc 1
2016-09-06T00:22:48.365404: step 3137, loss 0.0370643, acc 0.98
2016-09-06T00:22:49.175776: step 3138, loss 0.0886777, acc 0.98
2016-09-06T00:22:49.973452: step 3139, loss 0.0440687, acc 0.98
2016-09-06T00:22:50.809476: step 3140, loss 0.118376, acc 0.98
2016-09-06T00:22:51.662517: step 3141, loss 0.0430308, acc 0.98
2016-09-06T00:22:52.448814: step 3142, loss 0.0245768, acc 0.98
2016-09-06T00:22:53.285814: step 3143, loss 0.0325344, acc 1
2016-09-06T00:22:54.108563: step 3144, loss 0.114537, acc 0.96
2016-09-06T00:22:54.903751: step 3145, loss 0.0220946, acc 1
2016-09-06T00:22:55.702534: step 3146, loss 0.0203392, acc 1
2016-09-06T00:22:56.534891: step 3147, loss 0.0944467, acc 0.96
2016-09-06T00:22:57.339261: step 3148, loss 0.0209552, acc 1
2016-09-06T00:22:58.147101: step 3149, loss 0.0888157, acc 0.98
2016-09-06T00:22:59.037729: step 3150, loss 0.0321167, acc 0.98
2016-09-06T00:22:59.833265: step 3151, loss 0.0406284, acc 1
2016-09-06T00:23:00.669110: step 3152, loss 0.017448, acc 1
2016-09-06T00:23:01.497074: step 3153, loss 0.0142585, acc 1
2016-09-06T00:23:02.309752: step 3154, loss 0.0365408, acc 0.98
2016-09-06T00:23:03.127555: step 3155, loss 0.109803, acc 0.96
2016-09-06T00:23:03.979191: step 3156, loss 0.0534016, acc 0.98
2016-09-06T00:23:04.766098: step 3157, loss 0.048462, acc 0.96
2016-09-06T00:23:05.589708: step 3158, loss 0.0151651, acc 1
2016-09-06T00:23:06.390180: step 3159, loss 0.00576818, acc 1
2016-09-06T00:23:07.187287: step 3160, loss 0.0103752, acc 1
2016-09-06T00:23:07.991707: step 3161, loss 0.0198172, acc 1
2016-09-06T00:23:08.814230: step 3162, loss 0.0217823, acc 1
2016-09-06T00:23:09.614456: step 3163, loss 0.0205999, acc 1
2016-09-06T00:23:10.422625: step 3164, loss 0.00522697, acc 1
2016-09-06T00:23:11.271913: step 3165, loss 0.0437609, acc 0.98
2016-09-06T00:23:12.100194: step 3166, loss 0.00326307, acc 1
2016-09-06T00:23:12.912130: step 3167, loss 0.0404731, acc 0.98
2016-09-06T00:23:13.753092: step 3168, loss 0.00299801, acc 1
2016-09-06T00:23:14.541404: step 3169, loss 0.0226525, acc 0.98
2016-09-06T00:23:15.324624: step 3170, loss 0.0294015, acc 1
2016-09-06T00:23:16.165985: step 3171, loss 0.0185456, acc 1
2016-09-06T00:23:16.977511: step 3172, loss 0.0726396, acc 0.98
2016-09-06T00:23:17.817354: step 3173, loss 0.0378472, acc 0.96
2016-09-06T00:23:18.617824: step 3174, loss 0.00387648, acc 1
2016-09-06T00:23:19.427829: step 3175, loss 0.0282674, acc 0.98
2016-09-06T00:23:20.238644: step 3176, loss 0.0206592, acc 1
2016-09-06T00:23:21.037353: step 3177, loss 0.0644524, acc 0.96
2016-09-06T00:23:21.864186: step 3178, loss 0.00837585, acc 1
2016-09-06T00:23:22.655369: step 3179, loss 0.0391942, acc 0.98
2016-09-06T00:23:23.466956: step 3180, loss 0.0457645, acc 0.96
2016-09-06T00:23:24.310588: step 3181, loss 0.0259722, acc 1
2016-09-06T00:23:25.138016: step 3182, loss 0.0543537, acc 0.96
2016-09-06T00:23:25.934223: step 3183, loss 0.0266786, acc 0.98
2016-09-06T00:23:26.758872: step 3184, loss 0.0358287, acc 0.98
2016-09-06T00:23:27.570491: step 3185, loss 0.0573506, acc 0.98
2016-09-06T00:23:28.373525: step 3186, loss 0.107401, acc 0.98
2016-09-06T00:23:29.188186: step 3187, loss 0.0337359, acc 1
2016-09-06T00:23:30.001692: step 3188, loss 0.0346708, acc 1
2016-09-06T00:23:30.837918: step 3189, loss 0.0273938, acc 0.98
2016-09-06T00:23:31.679172: step 3190, loss 0.0219681, acc 1
2016-09-06T00:23:32.482754: step 3191, loss 0.0777449, acc 0.98
2016-09-06T00:23:33.293763: step 3192, loss 0.0174475, acc 1
2016-09-06T00:23:34.114290: step 3193, loss 0.0134529, acc 1
2016-09-06T00:23:34.941869: step 3194, loss 0.00341483, acc 1
2016-09-06T00:23:35.748284: step 3195, loss 0.00487989, acc 1
2016-09-06T00:23:36.569826: step 3196, loss 0.0834408, acc 0.98
2016-09-06T00:23:37.396872: step 3197, loss 0.191168, acc 0.92
2016-09-06T00:23:38.228365: step 3198, loss 0.0214256, acc 1
2016-09-06T00:23:39.045779: step 3199, loss 0.0125031, acc 1
2016-09-06T00:23:39.870666: step 3200, loss 0.0148674, acc 1

Evaluation:
2016-09-06T00:23:43.612335: step 3200, loss 1.57124, acc 0.744841

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-3200

2016-09-06T00:23:45.505212: step 3201, loss 0.0483443, acc 0.96
2016-09-06T00:23:46.332254: step 3202, loss 0.0396036, acc 1
2016-09-06T00:23:47.163307: step 3203, loss 0.0193858, acc 1
2016-09-06T00:23:47.972474: step 3204, loss 0.0448249, acc 0.98
2016-09-06T00:23:48.805407: step 3205, loss 0.0911725, acc 0.94
2016-09-06T00:23:49.613425: step 3206, loss 0.0607909, acc 0.96
2016-09-06T00:23:50.403311: step 3207, loss 0.0407635, acc 0.98
2016-09-06T00:23:51.243647: step 3208, loss 0.0139307, acc 1
2016-09-06T00:23:52.093664: step 3209, loss 0.0534489, acc 0.96
2016-09-06T00:23:52.887821: step 3210, loss 0.121797, acc 0.94
2016-09-06T00:23:53.703995: step 3211, loss 0.0353255, acc 0.98
2016-09-06T00:23:54.522491: step 3212, loss 0.0123894, acc 1
2016-09-06T00:23:55.319145: step 3213, loss 0.0810917, acc 0.98
2016-09-06T00:23:56.105287: step 3214, loss 0.00799247, acc 1
2016-09-06T00:23:56.910581: step 3215, loss 0.0187656, acc 1
2016-09-06T00:23:57.702584: step 3216, loss 0.0324407, acc 1
2016-09-06T00:23:58.549997: step 3217, loss 0.0738374, acc 0.98
2016-09-06T00:23:59.398836: step 3218, loss 0.03872, acc 0.98
2016-09-06T00:24:00.210705: step 3219, loss 0.0534767, acc 0.98
2016-09-06T00:24:01.038228: step 3220, loss 0.0302392, acc 0.98
2016-09-06T00:24:01.831448: step 3221, loss 0.0339099, acc 0.98
2016-09-06T00:24:02.630818: step 3222, loss 0.0414837, acc 0.98
2016-09-06T00:24:03.425409: step 3223, loss 0.0473294, acc 0.98
2016-09-06T00:24:04.213596: step 3224, loss 0.00626493, acc 1
2016-09-06T00:24:04.998072: step 3225, loss 0.00974195, acc 1
2016-09-06T00:24:05.796274: step 3226, loss 0.00624225, acc 1
2016-09-06T00:24:06.622323: step 3227, loss 0.0274117, acc 1
2016-09-06T00:24:07.412820: step 3228, loss 0.0339324, acc 1
2016-09-06T00:24:08.204917: step 3229, loss 0.0294738, acc 1
2016-09-06T00:24:09.033359: step 3230, loss 0.0225875, acc 1
2016-09-06T00:24:09.828858: step 3231, loss 0.0223364, acc 0.98
2016-09-06T00:24:10.651226: step 3232, loss 0.0555015, acc 0.98
2016-09-06T00:24:11.477884: step 3233, loss 0.0683588, acc 0.96
2016-09-06T00:24:12.239334: step 3234, loss 0.0354834, acc 0.98
2016-09-06T00:24:13.041325: step 3235, loss 0.0115881, acc 1
2016-09-06T00:24:13.887779: step 3236, loss 0.0254184, acc 0.98
2016-09-06T00:24:14.666942: step 3237, loss 0.00337874, acc 1
2016-09-06T00:24:15.486394: step 3238, loss 0.0427386, acc 0.98
2016-09-06T00:24:16.315094: step 3239, loss 0.00991885, acc 1
2016-09-06T00:24:17.121827: step 3240, loss 0.0611263, acc 0.98
2016-09-06T00:24:17.939612: step 3241, loss 0.0316435, acc 0.98
2016-09-06T00:24:18.736453: step 3242, loss 0.0984883, acc 0.98
2016-09-06T00:24:19.527373: step 3243, loss 0.016979, acc 1
2016-09-06T00:24:20.373188: step 3244, loss 0.0353918, acc 0.98
2016-09-06T00:24:21.156001: step 3245, loss 0.0306553, acc 1
2016-09-06T00:24:21.965518: step 3246, loss 0.0177259, acc 1
2016-09-06T00:24:22.773596: step 3247, loss 0.0470311, acc 1
2016-09-06T00:24:23.645458: step 3248, loss 0.0349781, acc 0.98
2016-09-06T00:24:24.471659: step 3249, loss 0.0545599, acc 0.98
2016-09-06T00:24:25.265796: step 3250, loss 0.0515156, acc 0.96
2016-09-06T00:24:26.091213: step 3251, loss 0.0195017, acc 0.98
2016-09-06T00:24:26.908371: step 3252, loss 0.0212016, acc 1
2016-09-06T00:24:27.707188: step 3253, loss 0.00503509, acc 1
2016-09-06T00:24:28.528477: step 3254, loss 0.0623379, acc 0.98
2016-09-06T00:24:29.331961: step 3255, loss 0.00390676, acc 1
2016-09-06T00:24:30.129919: step 3256, loss 0.153665, acc 0.96
2016-09-06T00:24:30.948751: step 3257, loss 0.136032, acc 0.94
2016-09-06T00:24:31.745780: step 3258, loss 0.0305411, acc 1
2016-09-06T00:24:32.598459: step 3259, loss 0.0309681, acc 1
2016-09-06T00:24:33.436703: step 3260, loss 0.0159693, acc 1
2016-09-06T00:24:34.246054: step 3261, loss 0.0561244, acc 0.96
2016-09-06T00:24:35.035125: step 3262, loss 0.0156675, acc 1
2016-09-06T00:24:35.874277: step 3263, loss 0.0294448, acc 0.98
2016-09-06T00:24:36.631308: step 3264, loss 0.0251561, acc 0.977273
2016-09-06T00:24:37.424131: step 3265, loss 0.0164867, acc 1
2016-09-06T00:24:38.265837: step 3266, loss 0.0497877, acc 0.96
2016-09-06T00:24:39.075039: step 3267, loss 0.131094, acc 0.96
2016-09-06T00:24:39.881326: step 3268, loss 0.0399908, acc 0.96
2016-09-06T00:24:40.698004: step 3269, loss 0.0162973, acc 1
2016-09-06T00:24:41.504046: step 3270, loss 0.032619, acc 0.98
2016-09-06T00:24:42.309399: step 3271, loss 0.0229623, acc 1
2016-09-06T00:24:43.161011: step 3272, loss 0.0286155, acc 0.98
2016-09-06T00:24:43.975516: step 3273, loss 0.0169123, acc 1
2016-09-06T00:24:44.776621: step 3274, loss 0.00447139, acc 1
2016-09-06T00:24:45.612274: step 3275, loss 0.032275, acc 0.98
2016-09-06T00:24:46.431980: step 3276, loss 0.0420161, acc 0.98
2016-09-06T00:24:47.255628: step 3277, loss 0.0389626, acc 0.96
2016-09-06T00:24:48.066630: step 3278, loss 0.0118446, acc 1
2016-09-06T00:24:48.892860: step 3279, loss 0.0108209, acc 1
2016-09-06T00:24:49.674130: step 3280, loss 0.0507302, acc 0.98
2016-09-06T00:24:50.473719: step 3281, loss 0.0208228, acc 0.98
2016-09-06T00:24:51.273356: step 3282, loss 0.00450342, acc 1
2016-09-06T00:24:52.043001: step 3283, loss 0.0150891, acc 1
2016-09-06T00:24:52.871161: step 3284, loss 0.0490121, acc 0.98
2016-09-06T00:24:53.679062: step 3285, loss 0.066844, acc 0.98
2016-09-06T00:24:54.469329: step 3286, loss 0.0194294, acc 1
2016-09-06T00:24:55.284316: step 3287, loss 0.0343431, acc 0.98
2016-09-06T00:24:56.117747: step 3288, loss 0.0266361, acc 1
2016-09-06T00:24:56.886595: step 3289, loss 0.0269658, acc 0.98
2016-09-06T00:24:57.692132: step 3290, loss 0.0640483, acc 0.96
2016-09-06T00:24:58.494367: step 3291, loss 0.0612194, acc 0.96
2016-09-06T00:24:59.303363: step 3292, loss 0.037545, acc 0.98
2016-09-06T00:25:00.148466: step 3293, loss 0.0540799, acc 0.98
2016-09-06T00:25:00.982301: step 3294, loss 0.0345082, acc 0.98
2016-09-06T00:25:01.767215: step 3295, loss 0.120795, acc 0.96
2016-09-06T00:25:02.556932: step 3296, loss 0.0190023, acc 1
2016-09-06T00:25:03.394765: step 3297, loss 0.0258927, acc 1
2016-09-06T00:25:04.193384: step 3298, loss 0.0555433, acc 0.98
2016-09-06T00:25:05.010886: step 3299, loss 0.00492965, acc 1
2016-09-06T00:25:05.825931: step 3300, loss 0.0333497, acc 0.98

Evaluation:
2016-09-06T00:25:09.568066: step 3300, loss 1.7282, acc 0.748593

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-3300

2016-09-06T00:25:11.533029: step 3301, loss 0.0222559, acc 0.98
2016-09-06T00:25:12.349851: step 3302, loss 0.0450568, acc 0.98
2016-09-06T00:25:13.168908: step 3303, loss 0.00649482, acc 1
2016-09-06T00:25:13.941837: step 3304, loss 0.00682755, acc 1
2016-09-06T00:25:14.760452: step 3305, loss 0.0347574, acc 0.98
2016-09-06T00:25:15.552005: step 3306, loss 0.00560044, acc 1
2016-09-06T00:25:16.365185: step 3307, loss 0.0330513, acc 0.98
2016-09-06T00:25:17.204867: step 3308, loss 0.0323892, acc 1
2016-09-06T00:25:18.025313: step 3309, loss 0.00376692, acc 1
2016-09-06T00:25:18.842549: step 3310, loss 0.0605451, acc 0.98
2016-09-06T00:25:19.665643: step 3311, loss 0.0675827, acc 0.96
2016-09-06T00:25:20.487926: step 3312, loss 0.028537, acc 0.98
2016-09-06T00:25:21.252746: step 3313, loss 0.0651605, acc 0.98
2016-09-06T00:25:22.051476: step 3314, loss 0.00865659, acc 1
2016-09-06T00:25:22.889408: step 3315, loss 0.0105903, acc 1
2016-09-06T00:25:23.685787: step 3316, loss 0.0273353, acc 0.98
2016-09-06T00:25:24.488288: step 3317, loss 0.0951793, acc 0.96
2016-09-06T00:25:25.286774: step 3318, loss 0.020018, acc 1
2016-09-06T00:25:26.070738: step 3319, loss 0.0474611, acc 0.98
2016-09-06T00:25:26.880424: step 3320, loss 0.067148, acc 0.96
2016-09-06T00:25:27.687822: step 3321, loss 0.00386514, acc 1
2016-09-06T00:25:28.482724: step 3322, loss 0.0558373, acc 0.98
2016-09-06T00:25:29.313954: step 3323, loss 0.0113542, acc 1
2016-09-06T00:25:30.150906: step 3324, loss 0.0144511, acc 1
2016-09-06T00:25:30.906004: step 3325, loss 0.0571015, acc 0.98
2016-09-06T00:25:31.734219: step 3326, loss 0.00709802, acc 1
2016-09-06T00:25:32.539457: step 3327, loss 0.00672231, acc 1
2016-09-06T00:25:33.318562: step 3328, loss 0.0494243, acc 0.96
2016-09-06T00:25:34.122875: step 3329, loss 0.0552816, acc 0.98
2016-09-06T00:25:34.919651: step 3330, loss 0.139994, acc 0.96
2016-09-06T00:25:35.699025: step 3331, loss 0.0309064, acc 1
2016-09-06T00:25:36.532833: step 3332, loss 0.0286617, acc 1
2016-09-06T00:25:37.333069: step 3333, loss 0.0413899, acc 0.96
2016-09-06T00:25:38.133489: step 3334, loss 0.0135956, acc 1
2016-09-06T00:25:38.957784: step 3335, loss 0.0161519, acc 1
2016-09-06T00:25:39.798829: step 3336, loss 0.0173216, acc 1
2016-09-06T00:25:40.616072: step 3337, loss 0.0296824, acc 1
2016-09-06T00:25:41.404853: step 3338, loss 0.0468488, acc 0.96
2016-09-06T00:25:42.248317: step 3339, loss 0.030722, acc 1
2016-09-06T00:25:43.030423: step 3340, loss 0.0195761, acc 0.98
2016-09-06T00:25:43.818681: step 3341, loss 0.007713, acc 1
2016-09-06T00:25:44.628894: step 3342, loss 0.029834, acc 1
2016-09-06T00:25:45.420605: step 3343, loss 0.102337, acc 0.94
2016-09-06T00:25:46.227368: step 3344, loss 0.0764418, acc 0.96
2016-09-06T00:25:47.075551: step 3345, loss 0.0230382, acc 0.98
2016-09-06T00:25:47.854403: step 3346, loss 0.0132779, acc 1
2016-09-06T00:25:48.661799: step 3347, loss 0.0312499, acc 0.98
2016-09-06T00:25:49.508843: step 3348, loss 0.0598299, acc 0.98
2016-09-06T00:25:50.271186: step 3349, loss 0.0145827, acc 1
2016-09-06T00:25:51.082232: step 3350, loss 0.0223856, acc 0.98
2016-09-06T00:25:51.910509: step 3351, loss 0.020889, acc 0.98
2016-09-06T00:25:52.672165: step 3352, loss 0.0245185, acc 1
2016-09-06T00:25:53.496767: step 3353, loss 0.041919, acc 0.98
2016-09-06T00:25:54.326595: step 3354, loss 0.0305701, acc 1
2016-09-06T00:25:55.079094: step 3355, loss 0.0741355, acc 0.98
2016-09-06T00:25:55.880675: step 3356, loss 0.00439138, acc 1
2016-09-06T00:25:56.726196: step 3357, loss 0.0361928, acc 1
2016-09-06T00:25:57.517550: step 3358, loss 0.0305076, acc 1
2016-09-06T00:25:58.323063: step 3359, loss 0.00512131, acc 1
2016-09-06T00:25:59.157810: step 3360, loss 0.065637, acc 0.94
2016-09-06T00:25:59.969998: step 3361, loss 0.0131308, acc 1
2016-09-06T00:26:00.821289: step 3362, loss 0.0335135, acc 0.98
2016-09-06T00:26:01.647095: step 3363, loss 0.00621407, acc 1
2016-09-06T00:26:02.462330: step 3364, loss 0.0166597, acc 1
2016-09-06T00:26:03.246681: step 3365, loss 0.0429034, acc 0.98
2016-09-06T00:26:04.110934: step 3366, loss 0.0367863, acc 0.98
2016-09-06T00:26:05.012846: step 3367, loss 0.0319247, acc 0.98
2016-09-06T00:26:05.831937: step 3368, loss 0.0665827, acc 0.98
2016-09-06T00:26:06.647661: step 3369, loss 0.00777841, acc 1
2016-09-06T00:26:07.462521: step 3370, loss 0.00370953, acc 1
2016-09-06T00:26:08.268373: step 3371, loss 0.0365277, acc 0.98
2016-09-06T00:26:09.128534: step 3372, loss 0.0349177, acc 0.98
2016-09-06T00:26:09.951095: step 3373, loss 0.0411054, acc 0.96
2016-09-06T00:26:10.762235: step 3374, loss 0.114864, acc 0.92
2016-09-06T00:26:11.608111: step 3375, loss 0.0114241, acc 1
2016-09-06T00:26:12.430692: step 3376, loss 0.00942671, acc 1
2016-09-06T00:26:13.213301: step 3377, loss 0.0448755, acc 0.98
2016-09-06T00:26:14.013160: step 3378, loss 0.0151999, acc 1
2016-09-06T00:26:14.835067: step 3379, loss 0.114351, acc 0.94
2016-09-06T00:26:15.653978: step 3380, loss 0.0511583, acc 0.98
2016-09-06T00:26:16.456320: step 3381, loss 0.0504824, acc 0.98
2016-09-06T00:26:17.281449: step 3382, loss 0.00353789, acc 1
2016-09-06T00:26:18.095341: step 3383, loss 0.0484852, acc 1
2016-09-06T00:26:18.913999: step 3384, loss 0.0277179, acc 0.98
2016-09-06T00:26:19.768963: step 3385, loss 0.0578112, acc 0.96
2016-09-06T00:26:20.573304: step 3386, loss 0.0178884, acc 1
2016-09-06T00:26:21.389379: step 3387, loss 0.0114687, acc 1
2016-09-06T00:26:22.239750: step 3388, loss 0.0492099, acc 1
2016-09-06T00:26:23.039343: step 3389, loss 0.0401729, acc 0.98
2016-09-06T00:26:23.864095: step 3390, loss 0.0295876, acc 1
2016-09-06T00:26:24.712772: step 3391, loss 0.0256976, acc 0.98
2016-09-06T00:26:25.547662: step 3392, loss 0.0640904, acc 0.98
2016-09-06T00:26:26.356090: step 3393, loss 0.0363171, acc 1
2016-09-06T00:26:27.202971: step 3394, loss 0.0515038, acc 0.98
2016-09-06T00:26:28.016932: step 3395, loss 0.0361353, acc 0.98
2016-09-06T00:26:28.841541: step 3396, loss 0.0636292, acc 0.94
2016-09-06T00:26:29.681862: step 3397, loss 0.0325147, acc 0.98
2016-09-06T00:26:30.452972: step 3398, loss 0.0511693, acc 0.96
2016-09-06T00:26:31.257979: step 3399, loss 0.0255, acc 1
2016-09-06T00:26:32.081829: step 3400, loss 0.0218704, acc 1

Evaluation:
2016-09-06T00:26:35.844595: step 3400, loss 1.96061, acc 0.730769

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-3400

2016-09-06T00:26:37.629235: step 3401, loss 0.0228303, acc 1
2016-09-06T00:26:38.449077: step 3402, loss 0.0339216, acc 0.98
2016-09-06T00:26:39.270105: step 3403, loss 0.0205878, acc 1
2016-09-06T00:26:40.089421: step 3404, loss 0.0118077, acc 1
2016-09-06T00:26:40.876122: step 3405, loss 0.0402256, acc 0.96
2016-09-06T00:26:41.718447: step 3406, loss 0.00461755, acc 1
2016-09-06T00:26:42.517776: step 3407, loss 0.00399815, acc 1
2016-09-06T00:26:43.331388: step 3408, loss 0.0391058, acc 0.98
2016-09-06T00:26:44.166184: step 3409, loss 0.133234, acc 0.94
2016-09-06T00:26:44.980386: step 3410, loss 0.0447361, acc 0.98
2016-09-06T00:26:45.806849: step 3411, loss 0.0280424, acc 1
2016-09-06T00:26:46.640642: step 3412, loss 0.0988512, acc 0.96
2016-09-06T00:26:47.453142: step 3413, loss 0.0135182, acc 1
2016-09-06T00:26:48.264651: step 3414, loss 0.0310029, acc 0.98
2016-09-06T00:26:49.070937: step 3415, loss 0.0141949, acc 1
2016-09-06T00:26:49.862628: step 3416, loss 0.019005, acc 1
2016-09-06T00:26:50.672392: step 3417, loss 0.097003, acc 0.98
2016-09-06T00:26:51.497521: step 3418, loss 0.00647114, acc 1
2016-09-06T00:26:52.350166: step 3419, loss 0.0731829, acc 0.98
2016-09-06T00:26:53.164410: step 3420, loss 0.0689037, acc 0.96
2016-09-06T00:26:53.961747: step 3421, loss 0.0904187, acc 0.98
2016-09-06T00:26:54.808562: step 3422, loss 0.0210031, acc 0.98
2016-09-06T00:26:55.602652: step 3423, loss 0.025485, acc 0.98
2016-09-06T00:26:56.425727: step 3424, loss 0.0499102, acc 0.96
2016-09-06T00:26:57.241985: step 3425, loss 0.16312, acc 0.96
2016-09-06T00:26:58.043302: step 3426, loss 0.0416271, acc 0.98
2016-09-06T00:26:58.839045: step 3427, loss 0.0242529, acc 0.98
2016-09-06T00:26:59.665181: step 3428, loss 0.0422023, acc 0.98
2016-09-06T00:27:00.472605: step 3429, loss 0.0191441, acc 1
2016-09-06T00:27:01.276423: step 3430, loss 0.0358524, acc 0.98
2016-09-06T00:27:02.114918: step 3431, loss 0.0167336, acc 1
2016-09-06T00:27:02.923368: step 3432, loss 0.0360555, acc 0.98
2016-09-06T00:27:03.730306: step 3433, loss 0.0676006, acc 0.96
2016-09-06T00:27:04.578526: step 3434, loss 0.0570361, acc 0.98
2016-09-06T00:27:05.422870: step 3435, loss 0.0206635, acc 1
2016-09-06T00:27:06.250325: step 3436, loss 0.00928365, acc 1
2016-09-06T00:27:07.093065: step 3437, loss 0.0245066, acc 1
2016-09-06T00:27:07.913883: step 3438, loss 0.0339532, acc 0.98
2016-09-06T00:27:08.707517: step 3439, loss 0.04213, acc 1
2016-09-06T00:27:09.524131: step 3440, loss 0.00700293, acc 1
2016-09-06T00:27:10.324524: step 3441, loss 0.043272, acc 1
2016-09-06T00:27:11.172456: step 3442, loss 0.0495662, acc 1
2016-09-06T00:27:12.047266: step 3443, loss 0.189705, acc 0.96
2016-09-06T00:27:12.860554: step 3444, loss 0.0429665, acc 0.98
2016-09-06T00:27:13.631807: step 3445, loss 0.00679884, acc 1
2016-09-06T00:27:14.482153: step 3446, loss 0.0145318, acc 1
2016-09-06T00:27:15.306122: step 3447, loss 0.0395791, acc 0.98
2016-09-06T00:27:16.086900: step 3448, loss 0.0343258, acc 0.98
2016-09-06T00:27:16.875975: step 3449, loss 0.0130925, acc 1
2016-09-06T00:27:17.709805: step 3450, loss 0.0390426, acc 0.98
2016-09-06T00:27:18.496222: step 3451, loss 0.0191997, acc 1
2016-09-06T00:27:19.281401: step 3452, loss 0.0740339, acc 0.98
2016-09-06T00:27:20.076886: step 3453, loss 0.058581, acc 0.98
2016-09-06T00:27:20.878286: step 3454, loss 0.0898707, acc 0.96
2016-09-06T00:27:21.702990: step 3455, loss 0.0191521, acc 1
2016-09-06T00:27:22.479492: step 3456, loss 0.0155262, acc 1
2016-09-06T00:27:23.262563: step 3457, loss 0.0175116, acc 1
2016-09-06T00:27:24.095634: step 3458, loss 0.0165854, acc 1
2016-09-06T00:27:24.880442: step 3459, loss 0.00856862, acc 1
2016-09-06T00:27:25.673732: step 3460, loss 0.0400205, acc 0.96
2016-09-06T00:27:26.479154: step 3461, loss 0.00359629, acc 1
2016-09-06T00:27:27.280675: step 3462, loss 0.0073869, acc 1
2016-09-06T00:27:28.069242: step 3463, loss 0.0453026, acc 0.98
2016-09-06T00:27:28.902302: step 3464, loss 0.0635037, acc 0.96
2016-09-06T00:27:29.712050: step 3465, loss 0.0296045, acc 0.98
2016-09-06T00:27:30.502785: step 3466, loss 0.0210024, acc 0.98
2016-09-06T00:27:31.354698: step 3467, loss 0.143086, acc 0.94
2016-09-06T00:27:32.173769: step 3468, loss 0.0935592, acc 0.96
2016-09-06T00:27:32.968999: step 3469, loss 0.0614013, acc 0.98
2016-09-06T00:27:33.784208: step 3470, loss 0.00681814, acc 1
2016-09-06T00:27:34.613413: step 3471, loss 0.00748839, acc 1
2016-09-06T00:27:35.385016: step 3472, loss 0.0241084, acc 1
2016-09-06T00:27:36.206478: step 3473, loss 0.0134817, acc 1
2016-09-06T00:27:37.034290: step 3474, loss 0.0157652, acc 1
2016-09-06T00:27:37.832694: step 3475, loss 0.0109809, acc 1
2016-09-06T00:27:38.634697: step 3476, loss 0.0216804, acc 1
2016-09-06T00:27:39.445432: step 3477, loss 0.0288226, acc 1
2016-09-06T00:27:40.230503: step 3478, loss 0.0493153, acc 0.98
2016-09-06T00:27:41.064517: step 3479, loss 0.0207686, acc 1
2016-09-06T00:27:41.885440: step 3480, loss 0.161192, acc 0.98
2016-09-06T00:27:42.682777: step 3481, loss 0.0638372, acc 0.94
2016-09-06T00:27:43.482419: step 3482, loss 0.0214541, acc 0.98
2016-09-06T00:27:44.291052: step 3483, loss 0.0953271, acc 0.94
2016-09-06T00:27:45.090309: step 3484, loss 0.0427042, acc 0.98
2016-09-06T00:27:45.898203: step 3485, loss 0.0370241, acc 0.98
2016-09-06T00:27:46.723629: step 3486, loss 0.052179, acc 0.98
2016-09-06T00:27:47.509941: step 3487, loss 0.0258339, acc 0.98
2016-09-06T00:27:48.310645: step 3488, loss 0.0258427, acc 0.98
2016-09-06T00:27:49.122948: step 3489, loss 0.0369302, acc 1
2016-09-06T00:27:49.913699: step 3490, loss 0.063476, acc 0.98
2016-09-06T00:27:50.720711: step 3491, loss 0.024158, acc 0.98
2016-09-06T00:27:51.532975: step 3492, loss 0.0533543, acc 0.96
2016-09-06T00:27:52.323806: step 3493, loss 0.0156195, acc 1
2016-09-06T00:27:53.134603: step 3494, loss 0.0584718, acc 0.96
2016-09-06T00:27:53.962299: step 3495, loss 0.0423694, acc 0.98
2016-09-06T00:27:54.785806: step 3496, loss 0.00340476, acc 1
2016-09-06T00:27:55.612981: step 3497, loss 0.0196061, acc 1
2016-09-06T00:27:56.417650: step 3498, loss 0.0354281, acc 0.98
2016-09-06T00:27:57.202775: step 3499, loss 0.0736553, acc 0.96
2016-09-06T00:27:58.036028: step 3500, loss 0.0196101, acc 1

Evaluation:
2016-09-06T00:28:01.798410: step 3500, loss 1.67572, acc 0.727017

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-3500

2016-09-06T00:28:03.657637: step 3501, loss 0.0106464, acc 1
2016-09-06T00:28:04.496591: step 3502, loss 0.0496153, acc 0.96
2016-09-06T00:28:05.331085: step 3503, loss 0.0444457, acc 0.98
2016-09-06T00:28:06.171456: step 3504, loss 0.014208, acc 1
2016-09-06T00:28:06.986473: step 3505, loss 0.0563744, acc 0.96
2016-09-06T00:28:07.822023: step 3506, loss 0.00393932, acc 1
2016-09-06T00:28:08.628007: step 3507, loss 0.0365229, acc 1
2016-09-06T00:28:09.401231: step 3508, loss 0.181031, acc 0.92
2016-09-06T00:28:10.199924: step 3509, loss 0.0235468, acc 1
2016-09-06T00:28:11.020695: step 3510, loss 0.0204923, acc 1
2016-09-06T00:28:11.798863: step 3511, loss 0.0218767, acc 1
2016-09-06T00:28:12.594394: step 3512, loss 0.00940639, acc 1
2016-09-06T00:28:13.410639: step 3513, loss 0.0598223, acc 0.98
2016-09-06T00:28:14.202722: step 3514, loss 0.0191599, acc 1
2016-09-06T00:28:15.026144: step 3515, loss 0.00837946, acc 1
2016-09-06T00:28:15.831934: step 3516, loss 0.00417541, acc 1
2016-09-06T00:28:16.634227: step 3517, loss 0.0716217, acc 0.96
2016-09-06T00:28:17.429532: step 3518, loss 0.211551, acc 0.96
2016-09-06T00:28:18.265028: step 3519, loss 0.0315063, acc 0.98
2016-09-06T00:28:19.069272: step 3520, loss 0.0334183, acc 0.98
2016-09-06T00:28:19.880239: step 3521, loss 0.0188404, acc 1
2016-09-06T00:28:20.671269: step 3522, loss 0.165208, acc 0.94
2016-09-06T00:28:21.472609: step 3523, loss 0.0159002, acc 1
2016-09-06T00:28:22.264525: step 3524, loss 0.0799006, acc 0.92
2016-09-06T00:28:23.078228: step 3525, loss 0.0598497, acc 0.96
2016-09-06T00:28:23.856388: step 3526, loss 0.0662785, acc 0.98
2016-09-06T00:28:24.654934: step 3527, loss 0.0287399, acc 1
2016-09-06T00:28:25.462832: step 3528, loss 0.0128887, acc 1
2016-09-06T00:28:26.255494: step 3529, loss 0.0601422, acc 0.98
2016-09-06T00:28:27.057943: step 3530, loss 0.0438524, acc 0.98
2016-09-06T00:28:27.884462: step 3531, loss 0.0346358, acc 0.98
2016-09-06T00:28:28.689277: step 3532, loss 0.117831, acc 0.94
2016-09-06T00:28:29.505488: step 3533, loss 0.073973, acc 0.94
2016-09-06T00:28:30.327228: step 3534, loss 0.0327694, acc 0.98
2016-09-06T00:28:31.115507: step 3535, loss 0.047638, acc 0.96
2016-09-06T00:28:31.933832: step 3536, loss 0.0326347, acc 1
2016-09-06T00:28:32.764026: step 3537, loss 0.0327271, acc 0.98
2016-09-06T00:28:33.544735: step 3538, loss 0.0351187, acc 1
2016-09-06T00:28:34.341560: step 3539, loss 0.0421877, acc 0.98
2016-09-06T00:28:35.153372: step 3540, loss 0.0163728, acc 1
2016-09-06T00:28:35.964197: step 3541, loss 0.0178444, acc 1
2016-09-06T00:28:36.791980: step 3542, loss 0.04323, acc 0.98
2016-09-06T00:28:37.600952: step 3543, loss 0.0185653, acc 0.98
2016-09-06T00:28:38.385650: step 3544, loss 0.0173411, acc 1
2016-09-06T00:28:39.182921: step 3545, loss 0.0477707, acc 0.98
2016-09-06T00:28:39.980220: step 3546, loss 0.00711218, acc 1
2016-09-06T00:28:40.794670: step 3547, loss 0.056526, acc 0.96
2016-09-06T00:28:41.604917: step 3548, loss 0.0147107, acc 1
2016-09-06T00:28:42.400519: step 3549, loss 0.0482653, acc 0.98
2016-09-06T00:28:43.225446: step 3550, loss 0.0363728, acc 1
2016-09-06T00:28:44.065246: step 3551, loss 0.047628, acc 0.98
2016-09-06T00:28:44.863998: step 3552, loss 0.0335968, acc 0.98
2016-09-06T00:28:45.654817: step 3553, loss 0.00760852, acc 1
2016-09-06T00:28:46.452741: step 3554, loss 0.0771846, acc 0.94
2016-09-06T00:28:47.271462: step 3555, loss 0.0695378, acc 0.96
2016-09-06T00:28:48.057619: step 3556, loss 0.00447884, acc 1
2016-09-06T00:28:48.876113: step 3557, loss 0.0194459, acc 0.98
2016-09-06T00:28:49.704870: step 3558, loss 0.0992419, acc 0.98
2016-09-06T00:28:50.485581: step 3559, loss 0.0121792, acc 1
2016-09-06T00:28:51.272050: step 3560, loss 0.132812, acc 0.96
2016-09-06T00:28:52.083704: step 3561, loss 0.0435573, acc 0.98
2016-09-06T00:28:52.873433: step 3562, loss 0.156243, acc 0.96
2016-09-06T00:28:53.690830: step 3563, loss 0.0588219, acc 0.98
2016-09-06T00:28:54.496149: step 3564, loss 0.060269, acc 0.96
2016-09-06T00:28:55.312931: step 3565, loss 0.0329599, acc 1
2016-09-06T00:28:56.143690: step 3566, loss 0.166865, acc 0.92
2016-09-06T00:28:56.966991: step 3567, loss 0.00964008, acc 1
2016-09-06T00:28:57.754471: step 3568, loss 0.056695, acc 0.98
2016-09-06T00:28:58.563378: step 3569, loss 0.0396343, acc 1
2016-09-06T00:28:59.387499: step 3570, loss 0.0637042, acc 0.98
2016-09-06T00:29:00.190401: step 3571, loss 0.0351762, acc 0.98
2016-09-06T00:29:01.029995: step 3572, loss 0.0629845, acc 0.96
2016-09-06T00:29:01.836310: step 3573, loss 0.040217, acc 1
2016-09-06T00:29:02.618780: step 3574, loss 0.0154156, acc 1
2016-09-06T00:29:03.415082: step 3575, loss 0.0581508, acc 0.98
2016-09-06T00:29:04.250843: step 3576, loss 0.0202003, acc 1
2016-09-06T00:29:05.015354: step 3577, loss 0.0247478, acc 1
2016-09-06T00:29:05.826293: step 3578, loss 0.0163639, acc 1
2016-09-06T00:29:06.647916: step 3579, loss 0.0862609, acc 0.98
2016-09-06T00:29:07.432153: step 3580, loss 0.0610223, acc 0.94
2016-09-06T00:29:08.235554: step 3581, loss 0.0397234, acc 0.96
2016-09-06T00:29:09.074305: step 3582, loss 0.0414261, acc 0.98
2016-09-06T00:29:09.858967: step 3583, loss 0.00710397, acc 1
2016-09-06T00:29:10.686949: step 3584, loss 0.0726686, acc 0.98
2016-09-06T00:29:11.509739: step 3585, loss 0.0234681, acc 1
2016-09-06T00:29:12.290158: step 3586, loss 0.0543563, acc 0.96
2016-09-06T00:29:13.068087: step 3587, loss 0.0810473, acc 0.94
2016-09-06T00:29:13.905430: step 3588, loss 0.0567976, acc 0.98
2016-09-06T00:29:14.693403: step 3589, loss 0.0398087, acc 0.98
2016-09-06T00:29:15.476885: step 3590, loss 0.0541148, acc 0.98
2016-09-06T00:29:16.312928: step 3591, loss 0.0215721, acc 1
2016-09-06T00:29:17.080213: step 3592, loss 0.00899806, acc 1
2016-09-06T00:29:17.892685: step 3593, loss 0.0210525, acc 1
2016-09-06T00:29:18.702574: step 3594, loss 0.018122, acc 1
2016-09-06T00:29:19.474906: step 3595, loss 0.0112207, acc 1
2016-09-06T00:29:20.291406: step 3596, loss 0.146122, acc 0.98
2016-09-06T00:29:21.099292: step 3597, loss 0.0312137, acc 0.98
2016-09-06T00:29:21.888546: step 3598, loss 0.0421898, acc 0.98
2016-09-06T00:29:22.676122: step 3599, loss 0.0145914, acc 1
2016-09-06T00:29:23.502656: step 3600, loss 0.0669163, acc 0.96

Evaluation:
2016-09-06T00:29:27.235173: step 3600, loss 1.68586, acc 0.734522

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-3600

2016-09-06T00:29:29.063779: step 3601, loss 0.140311, acc 0.94
2016-09-06T00:29:29.889187: step 3602, loss 0.0564201, acc 0.98
2016-09-06T00:29:30.702211: step 3603, loss 0.0273551, acc 1
2016-09-06T00:29:31.509612: step 3604, loss 0.0411939, acc 0.98
2016-09-06T00:29:32.362358: step 3605, loss 0.0281229, acc 1
2016-09-06T00:29:33.164748: step 3606, loss 0.0409413, acc 0.96
2016-09-06T00:29:33.949196: step 3607, loss 0.0138084, acc 1
2016-09-06T00:29:34.794999: step 3608, loss 0.00815288, acc 1
2016-09-06T00:29:35.634517: step 3609, loss 0.0361894, acc 0.98
2016-09-06T00:29:36.441013: step 3610, loss 0.0121612, acc 1
2016-09-06T00:29:37.242616: step 3611, loss 0.0763021, acc 0.98
2016-09-06T00:29:38.041333: step 3612, loss 0.0860135, acc 0.98
2016-09-06T00:29:38.839008: step 3613, loss 0.0863039, acc 0.96
2016-09-06T00:29:39.674264: step 3614, loss 0.0134106, acc 1
2016-09-06T00:29:40.471129: step 3615, loss 0.0910858, acc 0.98
2016-09-06T00:29:41.246419: step 3616, loss 0.048893, acc 0.98
2016-09-06T00:29:42.052660: step 3617, loss 0.0452365, acc 0.98
2016-09-06T00:29:42.858158: step 3618, loss 0.04754, acc 0.96
2016-09-06T00:29:43.667830: step 3619, loss 0.0624858, acc 0.98
2016-09-06T00:29:44.495983: step 3620, loss 0.00704334, acc 1
2016-09-06T00:29:45.320939: step 3621, loss 0.0220132, acc 1
2016-09-06T00:29:46.119698: step 3622, loss 0.0280804, acc 0.98
2016-09-06T00:29:46.939002: step 3623, loss 0.054797, acc 0.98
2016-09-06T00:29:47.769443: step 3624, loss 0.0568362, acc 0.98
2016-09-06T00:29:48.616885: step 3625, loss 0.0780466, acc 0.94
2016-09-06T00:29:49.434320: step 3626, loss 0.029193, acc 1
2016-09-06T00:29:50.253932: step 3627, loss 0.0989473, acc 0.98
2016-09-06T00:29:51.058621: step 3628, loss 0.0219142, acc 1
2016-09-06T00:29:51.863814: step 3629, loss 0.0151742, acc 1
2016-09-06T00:29:52.694059: step 3630, loss 0.030906, acc 1
2016-09-06T00:29:53.487485: step 3631, loss 0.0245081, acc 0.98
2016-09-06T00:29:54.277636: step 3632, loss 0.00999304, acc 1
2016-09-06T00:29:55.104942: step 3633, loss 0.023655, acc 1
2016-09-06T00:29:55.930466: step 3634, loss 0.033951, acc 0.98
2016-09-06T00:29:56.726417: step 3635, loss 0.0513609, acc 0.96
2016-09-06T00:29:57.549464: step 3636, loss 0.0348405, acc 1
2016-09-06T00:29:58.384313: step 3637, loss 0.0500152, acc 0.98
2016-09-06T00:29:59.211839: step 3638, loss 0.00583094, acc 1
2016-09-06T00:30:00.055791: step 3639, loss 0.0251642, acc 0.98
2016-09-06T00:30:00.932956: step 3640, loss 0.00958634, acc 1
2016-09-06T00:30:01.764612: step 3641, loss 0.0514077, acc 0.96
2016-09-06T00:30:02.606075: step 3642, loss 0.0342897, acc 0.96
2016-09-06T00:30:03.438341: step 3643, loss 0.0182784, acc 1
2016-09-06T00:30:04.238741: step 3644, loss 0.0141655, acc 1
2016-09-06T00:30:05.054434: step 3645, loss 0.0689394, acc 0.96
2016-09-06T00:30:05.880937: step 3646, loss 0.0151728, acc 1
2016-09-06T00:30:06.697837: step 3647, loss 0.00618335, acc 1
2016-09-06T00:30:07.453554: step 3648, loss 0.0293906, acc 0.977273
2016-09-06T00:30:08.279992: step 3649, loss 0.0254727, acc 1
2016-09-06T00:30:09.094713: step 3650, loss 0.021844, acc 1
2016-09-06T00:30:09.894757: step 3651, loss 0.00516733, acc 1
2016-09-06T00:30:10.711048: step 3652, loss 0.00946948, acc 1
2016-09-06T00:30:11.522730: step 3653, loss 0.0813031, acc 0.98
2016-09-06T00:30:12.330586: step 3654, loss 0.043707, acc 0.98
2016-09-06T00:30:13.154926: step 3655, loss 0.0101214, acc 1
2016-09-06T00:30:13.936630: step 3656, loss 0.0160207, acc 1
2016-09-06T00:30:14.736152: step 3657, loss 0.013686, acc 1
2016-09-06T00:30:15.556604: step 3658, loss 0.00464623, acc 1
2016-09-06T00:30:16.354610: step 3659, loss 0.103484, acc 0.96
2016-09-06T00:30:17.166321: step 3660, loss 0.00817588, acc 1
2016-09-06T00:30:17.992734: step 3661, loss 0.0344053, acc 0.98
2016-09-06T00:30:18.786417: step 3662, loss 0.0230028, acc 0.98
2016-09-06T00:30:19.578576: step 3663, loss 0.025614, acc 0.98
2016-09-06T00:30:20.384750: step 3664, loss 0.0316122, acc 1
2016-09-06T00:30:21.153743: step 3665, loss 0.033855, acc 0.98
2016-09-06T00:30:21.962337: step 3666, loss 0.00396252, acc 1
2016-09-06T00:30:22.780625: step 3667, loss 0.0534947, acc 0.98
2016-09-06T00:30:23.565959: step 3668, loss 0.140506, acc 0.96
2016-09-06T00:30:24.388253: step 3669, loss 0.10718, acc 0.96
2016-09-06T00:30:25.226702: step 3670, loss 0.0546926, acc 0.96
2016-09-06T00:30:26.026144: step 3671, loss 0.0633647, acc 0.98
2016-09-06T00:30:26.833909: step 3672, loss 0.136897, acc 0.98
2016-09-06T00:30:27.630001: step 3673, loss 0.0388325, acc 0.98
2016-09-06T00:30:28.422116: step 3674, loss 0.0362417, acc 0.98
2016-09-06T00:30:29.233656: step 3675, loss 0.0241544, acc 0.98
2016-09-06T00:30:30.047242: step 3676, loss 0.00881218, acc 1
2016-09-06T00:30:30.819537: step 3677, loss 0.105537, acc 0.98
2016-09-06T00:30:31.641528: step 3678, loss 0.0117907, acc 1
2016-09-06T00:30:32.490777: step 3679, loss 0.029927, acc 1
2016-09-06T00:30:33.288675: step 3680, loss 0.0834744, acc 0.96
2016-09-06T00:30:34.118788: step 3681, loss 0.1714, acc 0.96
2016-09-06T00:30:34.946411: step 3682, loss 0.0505613, acc 1
2016-09-06T00:30:35.726655: step 3683, loss 0.0165938, acc 1
2016-09-06T00:30:36.544786: step 3684, loss 0.0182735, acc 1
2016-09-06T00:30:37.365158: step 3685, loss 0.0124891, acc 1
2016-09-06T00:30:38.147526: step 3686, loss 0.0215999, acc 0.98
2016-09-06T00:30:38.923978: step 3687, loss 0.0124949, acc 1
2016-09-06T00:30:39.777312: step 3688, loss 0.0700343, acc 0.98
2016-09-06T00:30:40.564143: step 3689, loss 0.0620871, acc 0.96
2016-09-06T00:30:41.349856: step 3690, loss 0.0363364, acc 0.98
2016-09-06T00:30:42.208941: step 3691, loss 0.162248, acc 0.92
2016-09-06T00:30:43.011788: step 3692, loss 0.0195941, acc 1
2016-09-06T00:30:43.845526: step 3693, loss 0.0297138, acc 0.98
2016-09-06T00:30:44.652148: step 3694, loss 0.0538674, acc 0.98
2016-09-06T00:30:45.460992: step 3695, loss 0.0182291, acc 1
2016-09-06T00:30:46.264630: step 3696, loss 0.0503268, acc 0.98
2016-09-06T00:30:47.114976: step 3697, loss 0.0434922, acc 0.98
2016-09-06T00:30:47.905614: step 3698, loss 0.0155922, acc 1
2016-09-06T00:30:48.691453: step 3699, loss 0.0125954, acc 1
2016-09-06T00:30:49.508696: step 3700, loss 0.0306219, acc 1

Evaluation:
2016-09-06T00:30:53.264592: step 3700, loss 1.80259, acc 0.734522

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-3700

2016-09-06T00:30:55.190907: step 3701, loss 0.0297552, acc 1
2016-09-06T00:30:56.020328: step 3702, loss 0.00545094, acc 1
2016-09-06T00:30:56.842324: step 3703, loss 0.051902, acc 0.96
2016-09-06T00:30:57.657782: step 3704, loss 0.0444669, acc 0.98
2016-09-06T00:30:58.466791: step 3705, loss 0.0238232, acc 0.98
2016-09-06T00:30:59.337090: step 3706, loss 0.0445247, acc 0.98
2016-09-06T00:31:00.139320: step 3707, loss 0.0130024, acc 1
2016-09-06T00:31:00.977703: step 3708, loss 0.00643615, acc 1
2016-09-06T00:31:01.821712: step 3709, loss 0.0402755, acc 0.98
2016-09-06T00:31:02.656399: step 3710, loss 0.0470778, acc 0.98
2016-09-06T00:31:03.443764: step 3711, loss 0.0164931, acc 1
2016-09-06T00:31:04.261991: step 3712, loss 0.0634906, acc 0.96
2016-09-06T00:31:05.070230: step 3713, loss 0.0140664, acc 1
2016-09-06T00:31:05.859058: step 3714, loss 0.00514843, acc 1
2016-09-06T00:31:06.699740: step 3715, loss 0.132214, acc 0.96
2016-09-06T00:31:07.462725: step 3716, loss 0.0502738, acc 0.98
2016-09-06T00:31:08.283333: step 3717, loss 0.0455946, acc 0.98
2016-09-06T00:31:09.117073: step 3718, loss 0.0048197, acc 1
2016-09-06T00:31:09.923401: step 3719, loss 0.0487784, acc 0.98
2016-09-06T00:31:10.772003: step 3720, loss 0.0242371, acc 0.98
2016-09-06T00:31:11.571027: step 3721, loss 0.0333494, acc 0.98
2016-09-06T00:31:12.383818: step 3722, loss 0.0431676, acc 0.98
2016-09-06T00:31:13.212201: step 3723, loss 0.0443544, acc 0.98
2016-09-06T00:31:14.053320: step 3724, loss 0.020218, acc 1
2016-09-06T00:31:14.865182: step 3725, loss 0.0301884, acc 0.98
2016-09-06T00:31:15.637395: step 3726, loss 0.113526, acc 0.98
2016-09-06T00:31:16.434740: step 3727, loss 0.0122748, acc 1
2016-09-06T00:31:17.249399: step 3728, loss 0.0163278, acc 1
2016-09-06T00:31:18.029838: step 3729, loss 0.0172468, acc 1
2016-09-06T00:31:18.822089: step 3730, loss 0.0489317, acc 0.96
2016-09-06T00:31:19.633095: step 3731, loss 0.0833986, acc 0.96
2016-09-06T00:31:20.439577: step 3732, loss 0.139644, acc 0.96
2016-09-06T00:31:21.257211: step 3733, loss 0.0819794, acc 0.96
2016-09-06T00:31:22.108631: step 3734, loss 0.0239516, acc 1
2016-09-06T00:31:22.896129: step 3735, loss 0.0065354, acc 1
2016-09-06T00:31:23.679634: step 3736, loss 0.0157952, acc 1
2016-09-06T00:31:24.514466: step 3737, loss 0.0181328, acc 0.98
2016-09-06T00:31:25.275508: step 3738, loss 0.0828709, acc 0.98
2016-09-06T00:31:26.096524: step 3739, loss 0.0366291, acc 0.98
2016-09-06T00:31:26.905009: step 3740, loss 0.00359396, acc 1
2016-09-06T00:31:27.700318: step 3741, loss 0.0612703, acc 0.96
2016-09-06T00:31:28.495617: step 3742, loss 0.0403604, acc 0.98
2016-09-06T00:31:29.332283: step 3743, loss 0.016895, acc 1
2016-09-06T00:31:30.108616: step 3744, loss 0.0563297, acc 0.94
2016-09-06T00:31:30.933532: step 3745, loss 0.0410972, acc 0.98
2016-09-06T00:31:31.760635: step 3746, loss 0.0489077, acc 0.96
2016-09-06T00:31:32.574064: step 3747, loss 0.0160665, acc 1
2016-09-06T00:31:33.372435: step 3748, loss 0.059822, acc 0.96
2016-09-06T00:31:34.180612: step 3749, loss 0.0235073, acc 1
2016-09-06T00:31:34.956636: step 3750, loss 0.1021, acc 0.92
2016-09-06T00:31:35.750198: step 3751, loss 0.023141, acc 1
2016-09-06T00:31:36.571289: step 3752, loss 0.0944791, acc 0.96
2016-09-06T00:31:37.399120: step 3753, loss 0.0347183, acc 0.98
2016-09-06T00:31:38.188382: step 3754, loss 0.0547539, acc 0.98
2016-09-06T00:31:39.005490: step 3755, loss 0.0325516, acc 1
2016-09-06T00:31:39.799220: step 3756, loss 0.0223013, acc 1
2016-09-06T00:31:40.608964: step 3757, loss 0.0250234, acc 0.98
2016-09-06T00:31:41.411870: step 3758, loss 0.0874684, acc 0.94
2016-09-06T00:31:42.201438: step 3759, loss 0.0142144, acc 1
2016-09-06T00:31:42.982451: step 3760, loss 0.0326184, acc 0.98
2016-09-06T00:31:43.793080: step 3761, loss 0.103608, acc 0.94
2016-09-06T00:31:44.591945: step 3762, loss 0.00586652, acc 1
2016-09-06T00:31:45.381285: step 3763, loss 0.0330509, acc 0.98
2016-09-06T00:31:46.202478: step 3764, loss 0.0536164, acc 0.96
2016-09-06T00:31:46.999172: step 3765, loss 0.0182943, acc 1
2016-09-06T00:31:47.806466: step 3766, loss 0.00730949, acc 1
2016-09-06T00:31:48.622310: step 3767, loss 0.0405775, acc 0.98
2016-09-06T00:31:49.412957: step 3768, loss 0.0222629, acc 1
2016-09-06T00:31:50.205139: step 3769, loss 0.00740688, acc 1
2016-09-06T00:31:51.016829: step 3770, loss 0.0405315, acc 0.96
2016-09-06T00:31:51.791974: step 3771, loss 0.00792625, acc 1
2016-09-06T00:31:52.579005: step 3772, loss 0.033045, acc 0.98
2016-09-06T00:31:53.412249: step 3773, loss 0.116248, acc 0.96
2016-09-06T00:31:54.244734: step 3774, loss 0.0357553, acc 0.98
2016-09-06T00:31:55.052074: step 3775, loss 0.04111, acc 1
2016-09-06T00:31:55.881924: step 3776, loss 0.0615256, acc 0.96
2016-09-06T00:31:56.651899: step 3777, loss 0.0294183, acc 0.98
2016-09-06T00:31:57.475935: step 3778, loss 0.0976816, acc 0.98
2016-09-06T00:31:58.308545: step 3779, loss 0.0210848, acc 1
2016-09-06T00:31:59.096371: step 3780, loss 0.0411605, acc 1
2016-09-06T00:31:59.896916: step 3781, loss 0.0191932, acc 1
2016-09-06T00:32:00.748425: step 3782, loss 0.0308275, acc 1
2016-09-06T00:32:01.521447: step 3783, loss 0.0233472, acc 1
2016-09-06T00:32:02.346742: step 3784, loss 0.012148, acc 1
2016-09-06T00:32:03.162755: step 3785, loss 0.0522831, acc 0.96
2016-09-06T00:32:03.950885: step 3786, loss 0.145411, acc 0.96
2016-09-06T00:32:04.755104: step 3787, loss 0.00912569, acc 1
2016-09-06T00:32:05.579532: step 3788, loss 0.00389102, acc 1
2016-09-06T00:32:06.341409: step 3789, loss 0.0341519, acc 1
2016-09-06T00:32:07.132961: step 3790, loss 0.0384332, acc 0.98
2016-09-06T00:32:07.938657: step 3791, loss 0.0970598, acc 0.96
2016-09-06T00:32:08.732817: step 3792, loss 0.00975672, acc 1
2016-09-06T00:32:09.534393: step 3793, loss 0.0303558, acc 1
2016-09-06T00:32:10.354837: step 3794, loss 0.0442906, acc 0.98
2016-09-06T00:32:11.129855: step 3795, loss 0.0362796, acc 1
2016-09-06T00:32:11.940000: step 3796, loss 0.0238591, acc 1
2016-09-06T00:32:12.740636: step 3797, loss 0.0694096, acc 0.98
2016-09-06T00:32:13.544148: step 3798, loss 0.031373, acc 0.98
2016-09-06T00:32:14.361194: step 3799, loss 0.0313923, acc 0.98
2016-09-06T00:32:15.178751: step 3800, loss 0.0472191, acc 0.98

Evaluation:
2016-09-06T00:32:18.884226: step 3800, loss 1.7836, acc 0.738274

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-3800

2016-09-06T00:32:20.750201: step 3801, loss 0.00417861, acc 1
2016-09-06T00:32:21.570878: step 3802, loss 0.00871543, acc 1
2016-09-06T00:32:22.386626: step 3803, loss 0.0245743, acc 1
2016-09-06T00:32:23.228414: step 3804, loss 0.0247384, acc 1
2016-09-06T00:32:24.041315: step 3805, loss 0.0417616, acc 0.98
2016-09-06T00:32:24.837937: step 3806, loss 0.0125971, acc 1
2016-09-06T00:32:25.645338: step 3807, loss 0.0278929, acc 1
2016-09-06T00:32:26.472800: step 3808, loss 0.059084, acc 0.98
2016-09-06T00:32:27.300173: step 3809, loss 0.0555982, acc 0.98
2016-09-06T00:32:28.104768: step 3810, loss 0.00528769, acc 1
2016-09-06T00:32:28.925660: step 3811, loss 0.131948, acc 0.98
2016-09-06T00:32:29.746141: step 3812, loss 0.0574698, acc 0.98
2016-09-06T00:32:30.570898: step 3813, loss 0.00379336, acc 1
2016-09-06T00:32:31.394531: step 3814, loss 0.0305063, acc 0.98
2016-09-06T00:32:32.201306: step 3815, loss 0.0260603, acc 0.98
2016-09-06T00:32:33.025642: step 3816, loss 0.0310466, acc 1
2016-09-06T00:32:33.838364: step 3817, loss 0.0935665, acc 0.96
2016-09-06T00:32:34.665671: step 3818, loss 0.00889295, acc 1
2016-09-06T00:32:35.496402: step 3819, loss 0.0155675, acc 1
2016-09-06T00:32:36.334348: step 3820, loss 0.0291672, acc 0.98
2016-09-06T00:32:37.158076: step 3821, loss 0.0345463, acc 0.98
2016-09-06T00:32:37.941997: step 3822, loss 0.00659236, acc 1
2016-09-06T00:32:38.739341: step 3823, loss 0.125877, acc 0.92
2016-09-06T00:32:39.556842: step 3824, loss 0.0161224, acc 1
2016-09-06T00:32:40.352645: step 3825, loss 0.0493151, acc 0.98
2016-09-06T00:32:41.161332: step 3826, loss 0.104257, acc 0.96
2016-09-06T00:32:41.977694: step 3827, loss 0.0623907, acc 0.98
2016-09-06T00:32:42.775828: step 3828, loss 0.0549207, acc 0.96
2016-09-06T00:32:43.582253: step 3829, loss 0.00561096, acc 1
2016-09-06T00:32:44.391875: step 3830, loss 0.0520241, acc 0.96
2016-09-06T00:32:45.154043: step 3831, loss 0.032036, acc 0.98
2016-09-06T00:32:45.942954: step 3832, loss 0.0607927, acc 0.98
2016-09-06T00:32:46.745594: step 3833, loss 0.00339965, acc 1
2016-09-06T00:32:47.529453: step 3834, loss 0.0257832, acc 1
2016-09-06T00:32:48.351701: step 3835, loss 0.0565591, acc 0.96
2016-09-06T00:32:49.190571: step 3836, loss 0.0216644, acc 0.98
2016-09-06T00:32:49.991967: step 3837, loss 0.0563929, acc 0.98
2016-09-06T00:32:50.784329: step 3838, loss 0.0253542, acc 1
2016-09-06T00:32:51.597118: step 3839, loss 0.0413036, acc 0.96
2016-09-06T00:32:52.334464: step 3840, loss 0.0278721, acc 0.977273
2016-09-06T00:32:53.164535: step 3841, loss 0.0117663, acc 1
2016-09-06T00:32:53.980766: step 3842, loss 0.0488446, acc 0.98
2016-09-06T00:32:54.774219: step 3843, loss 0.0439252, acc 0.98
2016-09-06T00:32:55.591480: step 3844, loss 0.132584, acc 0.96
2016-09-06T00:32:56.405721: step 3845, loss 0.0109374, acc 1
2016-09-06T00:32:57.210340: step 3846, loss 0.0210802, acc 0.98
2016-09-06T00:32:58.018571: step 3847, loss 0.0375482, acc 0.98
2016-09-06T00:32:58.827899: step 3848, loss 0.0727688, acc 0.98
2016-09-06T00:32:59.605665: step 3849, loss 0.058361, acc 0.98
2016-09-06T00:33:00.496570: step 3850, loss 0.0224009, acc 1
2016-09-06T00:33:01.320760: step 3851, loss 0.00612979, acc 1
2016-09-06T00:33:02.116935: step 3852, loss 0.00860251, acc 1
2016-09-06T00:33:02.914967: step 3853, loss 0.0491048, acc 0.98
2016-09-06T00:33:03.736682: step 3854, loss 0.00895383, acc 1
2016-09-06T00:33:04.520736: step 3855, loss 0.0519602, acc 0.96
2016-09-06T00:33:05.283665: step 3856, loss 0.0378345, acc 0.98
2016-09-06T00:33:06.101925: step 3857, loss 0.0731743, acc 0.96
2016-09-06T00:33:06.884498: step 3858, loss 0.0389847, acc 0.98
2016-09-06T00:33:07.704460: step 3859, loss 0.00681509, acc 1
2016-09-06T00:33:08.508466: step 3860, loss 0.0357875, acc 1
2016-09-06T00:33:09.290725: step 3861, loss 0.0170926, acc 1
2016-09-06T00:33:10.089360: step 3862, loss 0.0986819, acc 0.98
2016-09-06T00:33:10.913140: step 3863, loss 0.0515598, acc 0.98
2016-09-06T00:33:11.699727: step 3864, loss 0.037509, acc 0.98
2016-09-06T00:33:12.505077: step 3865, loss 0.0211684, acc 0.98
2016-09-06T00:33:13.314047: step 3866, loss 0.0367943, acc 1
2016-09-06T00:33:14.090652: step 3867, loss 0.0517517, acc 0.98
2016-09-06T00:33:14.929586: step 3868, loss 0.0441905, acc 0.98
2016-09-06T00:33:15.738041: step 3869, loss 0.0413026, acc 0.98
2016-09-06T00:33:16.540906: step 3870, loss 0.00933544, acc 1
2016-09-06T00:33:17.335914: step 3871, loss 0.0045129, acc 1
2016-09-06T00:33:18.144870: step 3872, loss 0.0152053, acc 1
2016-09-06T00:33:18.937621: step 3873, loss 0.0119875, acc 1
2016-09-06T00:33:19.763541: step 3874, loss 0.0185306, acc 1
2016-09-06T00:33:20.595971: step 3875, loss 0.0549556, acc 0.96
2016-09-06T00:33:21.375679: step 3876, loss 0.0462942, acc 0.98
2016-09-06T00:33:22.170074: step 3877, loss 0.0129464, acc 1
2016-09-06T00:33:22.970827: step 3878, loss 0.01865, acc 0.98
2016-09-06T00:33:23.770813: step 3879, loss 0.0233524, acc 1
2016-09-06T00:33:24.572775: step 3880, loss 0.00424263, acc 1
2016-09-06T00:33:25.391346: step 3881, loss 0.105176, acc 0.94
2016-09-06T00:33:26.185385: step 3882, loss 0.021649, acc 1
2016-09-06T00:33:26.987252: step 3883, loss 0.072051, acc 0.94
2016-09-06T00:33:27.804554: step 3884, loss 0.0147803, acc 1
2016-09-06T00:33:28.582068: step 3885, loss 0.014959, acc 1
2016-09-06T00:33:29.407598: step 3886, loss 0.0548416, acc 0.98
2016-09-06T00:33:30.206079: step 3887, loss 0.198857, acc 0.98
2016-09-06T00:33:31.006156: step 3888, loss 0.082964, acc 0.98
2016-09-06T00:33:31.837717: step 3889, loss 0.0334977, acc 0.98
2016-09-06T00:33:32.664821: step 3890, loss 0.0714324, acc 0.98
2016-09-06T00:33:33.467572: step 3891, loss 0.128523, acc 0.96
2016-09-06T00:33:34.255921: step 3892, loss 0.0523949, acc 0.98
2016-09-06T00:33:35.065702: step 3893, loss 0.0236867, acc 0.98
2016-09-06T00:33:35.834761: step 3894, loss 0.0170001, acc 1
2016-09-06T00:33:36.660348: step 3895, loss 0.0208266, acc 1
2016-09-06T00:33:37.501161: step 3896, loss 0.0106331, acc 1
2016-09-06T00:33:38.293635: step 3897, loss 0.0119691, acc 1
2016-09-06T00:33:39.097862: step 3898, loss 0.118561, acc 0.92
2016-09-06T00:33:39.919706: step 3899, loss 0.212201, acc 0.92
2016-09-06T00:33:40.713932: step 3900, loss 0.00315495, acc 1

Evaluation:
2016-09-06T00:33:44.408958: step 3900, loss 1.5251, acc 0.734522

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-3900

2016-09-06T00:33:46.385943: step 3901, loss 0.0801249, acc 0.96
2016-09-06T00:33:47.195809: step 3902, loss 0.0221256, acc 0.98
2016-09-06T00:33:48.030560: step 3903, loss 0.00833723, acc 1
2016-09-06T00:33:48.863239: step 3904, loss 0.0258868, acc 0.98
2016-09-06T00:33:49.655239: step 3905, loss 0.0747295, acc 0.96
2016-09-06T00:33:50.467001: step 3906, loss 0.0469498, acc 0.98
2016-09-06T00:33:51.263666: step 3907, loss 0.0255631, acc 1
2016-09-06T00:33:52.084688: step 3908, loss 0.0302124, acc 0.98
2016-09-06T00:33:52.854097: step 3909, loss 0.0390789, acc 0.98
2016-09-06T00:33:53.634781: step 3910, loss 0.0431434, acc 0.98
2016-09-06T00:33:54.458428: step 3911, loss 0.0601609, acc 0.96
2016-09-06T00:33:55.247661: step 3912, loss 0.112272, acc 0.94
2016-09-06T00:33:56.052164: step 3913, loss 0.0474233, acc 0.98
2016-09-06T00:33:56.879632: step 3914, loss 0.0585228, acc 0.98
2016-09-06T00:33:57.676144: step 3915, loss 0.0250857, acc 0.98
2016-09-06T00:33:58.478517: step 3916, loss 0.0433855, acc 0.98
2016-09-06T00:33:59.292091: step 3917, loss 0.00805775, acc 1
2016-09-06T00:34:00.075761: step 3918, loss 0.0255894, acc 0.98
2016-09-06T00:34:00.885157: step 3919, loss 0.0358166, acc 0.98
2016-09-06T00:34:01.691907: step 3920, loss 0.028521, acc 1
2016-09-06T00:34:02.466287: step 3921, loss 0.0126694, acc 1
2016-09-06T00:34:03.282697: step 3922, loss 0.0204433, acc 1
2016-09-06T00:34:04.080124: step 3923, loss 0.0507332, acc 1
2016-09-06T00:34:04.870706: step 3924, loss 0.0475915, acc 0.98
2016-09-06T00:34:05.690732: step 3925, loss 0.0179313, acc 1
2016-09-06T00:34:06.489455: step 3926, loss 0.017652, acc 1
2016-09-06T00:34:07.288048: step 3927, loss 0.00543287, acc 1
2016-09-06T00:34:08.091218: step 3928, loss 0.0986969, acc 0.94
2016-09-06T00:34:08.892405: step 3929, loss 0.055335, acc 0.96
2016-09-06T00:34:09.695704: step 3930, loss 0.00737638, acc 1
2016-09-06T00:34:10.529927: step 3931, loss 0.0211197, acc 0.98
2016-09-06T00:34:11.334498: step 3932, loss 0.00450863, acc 1
2016-09-06T00:34:12.135540: step 3933, loss 0.0183686, acc 1
2016-09-06T00:34:12.972272: step 3934, loss 0.00728141, acc 1
2016-09-06T00:34:13.781811: step 3935, loss 0.0365985, acc 0.98
2016-09-06T00:34:14.587185: step 3936, loss 0.0308829, acc 1
2016-09-06T00:34:15.387317: step 3937, loss 0.00759782, acc 1
2016-09-06T00:34:16.202216: step 3938, loss 0.0466788, acc 0.98
2016-09-06T00:34:17.004691: step 3939, loss 0.0151659, acc 1
2016-09-06T00:34:17.816026: step 3940, loss 0.0571859, acc 0.96
2016-09-06T00:34:18.652891: step 3941, loss 0.127388, acc 0.96
2016-09-06T00:34:19.451825: step 3942, loss 0.00899427, acc 1
2016-09-06T00:34:20.276046: step 3943, loss 0.00554725, acc 1
2016-09-06T00:34:21.092716: step 3944, loss 0.0365659, acc 0.98
2016-09-06T00:34:21.887051: step 3945, loss 0.0200246, acc 0.98
2016-09-06T00:34:22.669659: step 3946, loss 0.0155325, acc 1
2016-09-06T00:34:23.485548: step 3947, loss 0.21623, acc 0.96
2016-09-06T00:34:24.278970: step 3948, loss 0.0295676, acc 0.98
2016-09-06T00:34:25.083690: step 3949, loss 0.130773, acc 0.96
2016-09-06T00:34:25.922820: step 3950, loss 0.0385414, acc 1
2016-09-06T00:34:26.713325: step 3951, loss 0.0192392, acc 1
2016-09-06T00:34:27.519241: step 3952, loss 0.0315954, acc 0.98
2016-09-06T00:34:28.348108: step 3953, loss 0.0159164, acc 1
2016-09-06T00:34:29.186605: step 3954, loss 0.0676439, acc 0.94
2016-09-06T00:34:29.994193: step 3955, loss 0.111799, acc 0.96
2016-09-06T00:34:30.855740: step 3956, loss 0.0123508, acc 1
2016-09-06T00:34:31.664322: step 3957, loss 0.00893013, acc 1
2016-09-06T00:34:32.466993: step 3958, loss 0.00865376, acc 1
2016-09-06T00:34:33.301669: step 3959, loss 0.0359337, acc 1
2016-09-06T00:34:34.099360: step 3960, loss 0.0447032, acc 0.96
2016-09-06T00:34:34.905012: step 3961, loss 0.037557, acc 0.98
2016-09-06T00:34:35.777455: step 3962, loss 0.0107561, acc 1
2016-09-06T00:34:36.603112: step 3963, loss 0.00545912, acc 1
2016-09-06T00:34:37.412614: step 3964, loss 0.053732, acc 0.98
2016-09-06T00:34:38.237257: step 3965, loss 0.0418877, acc 1
2016-09-06T00:34:39.057617: step 3966, loss 0.0283794, acc 1
2016-09-06T00:34:39.877552: step 3967, loss 0.0666143, acc 0.98
2016-09-06T00:34:40.691032: step 3968, loss 0.00827085, acc 1
2016-09-06T00:34:41.539769: step 3969, loss 0.0190487, acc 1
2016-09-06T00:34:42.383187: step 3970, loss 0.034455, acc 1
2016-09-06T00:34:43.200423: step 3971, loss 0.0396855, acc 0.98
2016-09-06T00:34:44.026681: step 3972, loss 0.0253095, acc 1
2016-09-06T00:34:44.818012: step 3973, loss 0.0310799, acc 0.98
2016-09-06T00:34:45.649357: step 3974, loss 0.115271, acc 0.9
2016-09-06T00:34:46.492708: step 3975, loss 0.0752213, acc 0.98
2016-09-06T00:34:47.284746: step 3976, loss 0.00643089, acc 1
2016-09-06T00:34:48.103993: step 3977, loss 0.014378, acc 1
2016-09-06T00:34:48.910665: step 3978, loss 0.0574615, acc 0.96
2016-09-06T00:34:49.697181: step 3979, loss 0.0591743, acc 0.98
2016-09-06T00:34:50.513474: step 3980, loss 0.0126522, acc 1
2016-09-06T00:34:51.316356: step 3981, loss 0.0792975, acc 0.98
2016-09-06T00:34:52.107279: step 3982, loss 0.0456445, acc 0.98
2016-09-06T00:34:52.911334: step 3983, loss 0.0322368, acc 1
2016-09-06T00:34:53.700655: step 3984, loss 0.0323343, acc 1
2016-09-06T00:34:54.481117: step 3985, loss 0.0239078, acc 0.98
2016-09-06T00:34:55.282282: step 3986, loss 0.00964241, acc 1
2016-09-06T00:34:56.100435: step 3987, loss 0.080295, acc 0.96
2016-09-06T00:34:56.873868: step 3988, loss 0.0130415, acc 1
2016-09-06T00:34:57.681895: step 3989, loss 0.0194031, acc 1
2016-09-06T00:34:58.521103: step 3990, loss 0.0365657, acc 0.98
2016-09-06T00:34:59.306450: step 3991, loss 0.0128743, acc 1
2016-09-06T00:35:00.120533: step 3992, loss 0.00639686, acc 1
2016-09-06T00:35:00.968420: step 3993, loss 0.0136924, acc 1
2016-09-06T00:35:01.731000: step 3994, loss 0.00741343, acc 1
2016-09-06T00:35:02.520335: step 3995, loss 0.0500915, acc 0.98
2016-09-06T00:35:03.366304: step 3996, loss 0.0460415, acc 0.98
2016-09-06T00:35:04.146215: step 3997, loss 0.0482777, acc 0.96
2016-09-06T00:35:04.965766: step 3998, loss 0.05645, acc 0.98
2016-09-06T00:35:05.801892: step 3999, loss 0.0356096, acc 0.98
2016-09-06T00:35:06.626078: step 4000, loss 0.0390368, acc 0.98

Evaluation:
2016-09-06T00:35:10.376776: step 4000, loss 1.81728, acc 0.73546

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-4000

2016-09-06T00:35:12.296434: step 4001, loss 0.0983539, acc 0.96
2016-09-06T00:35:13.103073: step 4002, loss 0.0578449, acc 0.96
2016-09-06T00:35:13.899607: step 4003, loss 0.00605711, acc 1
2016-09-06T00:35:14.716384: step 4004, loss 0.0648165, acc 0.98
2016-09-06T00:35:15.527574: step 4005, loss 0.015935, acc 1
2016-09-06T00:35:16.332363: step 4006, loss 0.0272889, acc 1
2016-09-06T00:35:17.142248: step 4007, loss 0.0670597, acc 0.98
2016-09-06T00:35:17.975212: step 4008, loss 0.0204516, acc 1
2016-09-06T00:35:18.786826: step 4009, loss 0.0612938, acc 0.96
2016-09-06T00:35:19.597717: step 4010, loss 0.0399407, acc 0.98
2016-09-06T00:35:20.444624: step 4011, loss 0.0302067, acc 0.98
2016-09-06T00:35:21.240778: step 4012, loss 0.117177, acc 0.96
2016-09-06T00:35:22.060122: step 4013, loss 0.0316259, acc 1
2016-09-06T00:35:22.892265: step 4014, loss 0.0206826, acc 1
2016-09-06T00:35:23.718847: step 4015, loss 0.0473139, acc 0.98
2016-09-06T00:35:24.549396: step 4016, loss 0.0442109, acc 1
2016-09-06T00:35:25.375205: step 4017, loss 0.0951761, acc 0.94
2016-09-06T00:35:26.186063: step 4018, loss 0.0900357, acc 0.94
2016-09-06T00:35:26.986306: step 4019, loss 0.162758, acc 0.94
2016-09-06T00:35:27.835945: step 4020, loss 0.110476, acc 0.94
2016-09-06T00:35:28.690412: step 4021, loss 0.0402993, acc 0.98
2016-09-06T00:35:29.488841: step 4022, loss 0.00775912, acc 1
2016-09-06T00:35:30.304271: step 4023, loss 0.194114, acc 0.96
2016-09-06T00:35:31.141703: step 4024, loss 0.00652321, acc 1
2016-09-06T00:35:31.940886: step 4025, loss 0.0514693, acc 0.98
2016-09-06T00:35:32.766820: step 4026, loss 0.145852, acc 0.96
2016-09-06T00:35:33.591329: step 4027, loss 0.0297904, acc 1
2016-09-06T00:35:34.389659: step 4028, loss 0.129489, acc 0.96
2016-09-06T00:35:35.181383: step 4029, loss 0.0205705, acc 1
2016-09-06T00:35:35.999230: step 4030, loss 0.12434, acc 0.96
2016-09-06T00:35:36.789728: step 4031, loss 0.0846707, acc 0.96
2016-09-06T00:35:37.555875: step 4032, loss 0.0554587, acc 0.954545
2016-09-06T00:35:38.374351: step 4033, loss 0.0154598, acc 1
2016-09-06T00:35:39.195922: step 4034, loss 0.0971781, acc 0.94
2016-09-06T00:35:40.007495: step 4035, loss 0.065619, acc 0.98
2016-09-06T00:35:40.826843: step 4036, loss 0.0620073, acc 0.98
2016-09-06T00:35:41.619390: step 4037, loss 0.0458162, acc 0.98
2016-09-06T00:35:42.434261: step 4038, loss 0.0682304, acc 0.98
2016-09-06T00:35:43.283750: step 4039, loss 0.0194607, acc 1
2016-09-06T00:35:44.067148: step 4040, loss 0.155108, acc 0.94
2016-09-06T00:35:44.860938: step 4041, loss 0.0338188, acc 0.98
2016-09-06T00:35:45.678836: step 4042, loss 0.0116449, acc 1
2016-09-06T00:35:46.480221: step 4043, loss 0.0823096, acc 0.96
2016-09-06T00:35:47.283639: step 4044, loss 0.0710188, acc 0.96
2016-09-06T00:35:48.127823: step 4045, loss 0.0857349, acc 0.96
2016-09-06T00:35:48.918656: step 4046, loss 0.00922417, acc 1
2016-09-06T00:35:49.704687: step 4047, loss 0.0258902, acc 1
2016-09-06T00:35:50.541797: step 4048, loss 0.0534024, acc 0.98
2016-09-06T00:35:51.340827: step 4049, loss 0.0620003, acc 0.96
2016-09-06T00:35:52.144806: step 4050, loss 0.00580539, acc 1
2016-09-06T00:35:52.956831: step 4051, loss 0.0211951, acc 0.98
2016-09-06T00:35:53.726996: step 4052, loss 0.0425644, acc 0.96
2016-09-06T00:35:54.549650: step 4053, loss 0.0334724, acc 0.98
2016-09-06T00:35:55.357347: step 4054, loss 0.093826, acc 0.94
2016-09-06T00:35:56.163347: step 4055, loss 0.0569572, acc 0.96
2016-09-06T00:35:56.968618: step 4056, loss 0.112259, acc 0.96
2016-09-06T00:35:57.776877: step 4057, loss 0.0190939, acc 1
2016-09-06T00:35:58.570297: step 4058, loss 0.120192, acc 0.96
2016-09-06T00:35:59.361743: step 4059, loss 0.0100593, acc 1
2016-09-06T00:36:00.181452: step 4060, loss 0.0077168, acc 1
2016-09-06T00:36:01.021327: step 4061, loss 0.0566111, acc 0.98
2016-09-06T00:36:01.820682: step 4062, loss 0.110962, acc 0.96
2016-09-06T00:36:02.646043: step 4063, loss 0.0305836, acc 0.98
2016-09-06T00:36:03.444212: step 4064, loss 0.0771502, acc 0.94
2016-09-06T00:36:04.249861: step 4065, loss 0.0601607, acc 0.98
2016-09-06T00:36:05.072671: step 4066, loss 0.0469206, acc 0.98
2016-09-06T00:36:05.870445: step 4067, loss 0.0447508, acc 0.98
2016-09-06T00:36:06.673004: step 4068, loss 0.063379, acc 0.98
2016-09-06T00:36:07.475908: step 4069, loss 0.00967022, acc 1
2016-09-06T00:36:08.286186: step 4070, loss 0.0425203, acc 1
2016-09-06T00:36:09.109337: step 4071, loss 0.00709559, acc 1
2016-09-06T00:36:09.930875: step 4072, loss 0.038457, acc 0.98
2016-09-06T00:36:10.753189: step 4073, loss 0.034419, acc 1
2016-09-06T00:36:11.587453: step 4074, loss 0.0380417, acc 0.98
2016-09-06T00:36:12.404715: step 4075, loss 0.0153453, acc 1
2016-09-06T00:36:13.212372: step 4076, loss 0.00792634, acc 1
2016-09-06T00:36:14.032890: step 4077, loss 0.0223885, acc 0.98
2016-09-06T00:36:14.868876: step 4078, loss 0.022298, acc 1
2016-09-06T00:36:15.708298: step 4079, loss 0.0234194, acc 0.98
2016-09-06T00:36:16.527934: step 4080, loss 0.0169639, acc 1
2016-09-06T00:36:17.353441: step 4081, loss 0.0485915, acc 0.98
2016-09-06T00:36:18.163325: step 4082, loss 0.134343, acc 0.98
2016-09-06T00:36:18.972305: step 4083, loss 0.0206374, acc 1
2016-09-06T00:36:19.821397: step 4084, loss 0.0509771, acc 0.96
2016-09-06T00:36:20.629411: step 4085, loss 0.0446714, acc 1
2016-09-06T00:36:21.417036: step 4086, loss 0.0274934, acc 0.98
2016-09-06T00:36:22.243965: step 4087, loss 0.0226957, acc 0.98
2016-09-06T00:36:23.052030: step 4088, loss 0.0289006, acc 0.98
2016-09-06T00:36:23.865792: step 4089, loss 0.0319876, acc 0.98
2016-09-06T00:36:24.682636: step 4090, loss 0.105326, acc 0.94
2016-09-06T00:36:25.482767: step 4091, loss 0.11871, acc 0.96
2016-09-06T00:36:26.285092: step 4092, loss 0.022344, acc 0.98
2016-09-06T00:36:27.118394: step 4093, loss 0.0419906, acc 1
2016-09-06T00:36:27.944690: step 4094, loss 0.040236, acc 0.98
2016-09-06T00:36:28.733371: step 4095, loss 0.0112883, acc 1
2016-09-06T00:36:29.552305: step 4096, loss 0.0281241, acc 0.98
2016-09-06T00:36:30.358970: step 4097, loss 0.0190888, acc 1
2016-09-06T00:36:31.148284: step 4098, loss 0.0870748, acc 0.96
2016-09-06T00:36:31.947052: step 4099, loss 0.0152506, acc 1
2016-09-06T00:36:32.755917: step 4100, loss 0.048266, acc 0.98

Evaluation:
2016-09-06T00:36:36.466776: step 4100, loss 1.64826, acc 0.727955

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-4100

2016-09-06T00:36:38.310216: step 4101, loss 0.0327844, acc 1
2016-09-06T00:36:39.125672: step 4102, loss 0.0123387, acc 1
2016-09-06T00:36:39.940530: step 4103, loss 0.0552652, acc 0.98
2016-09-06T00:36:40.746950: step 4104, loss 0.028536, acc 0.98
2016-09-06T00:36:41.590911: step 4105, loss 0.00704789, acc 1
2016-09-06T00:36:42.422762: step 4106, loss 0.0374629, acc 0.98
2016-09-06T00:36:43.242595: step 4107, loss 0.0311272, acc 1
2016-09-06T00:36:44.076590: step 4108, loss 0.0273298, acc 0.98
2016-09-06T00:36:44.901958: step 4109, loss 0.0124378, acc 1
2016-09-06T00:36:45.719929: step 4110, loss 0.0655055, acc 0.98
2016-09-06T00:36:46.557604: step 4111, loss 0.0119658, acc 1
2016-09-06T00:36:47.389423: step 4112, loss 0.0339632, acc 0.98
2016-09-06T00:36:48.185894: step 4113, loss 0.0774244, acc 0.98
2016-09-06T00:36:49.005632: step 4114, loss 0.0265804, acc 0.98
2016-09-06T00:36:49.835177: step 4115, loss 0.0489861, acc 0.98
2016-09-06T00:36:50.600594: step 4116, loss 0.0322783, acc 0.98
2016-09-06T00:36:51.422334: step 4117, loss 0.0386722, acc 0.98
2016-09-06T00:36:52.267017: step 4118, loss 0.0512458, acc 0.96
2016-09-06T00:36:53.099326: step 4119, loss 0.00635819, acc 1
2016-09-06T00:36:53.871935: step 4120, loss 0.0185751, acc 1
2016-09-06T00:36:54.728408: step 4121, loss 0.0252604, acc 0.98
2016-09-06T00:36:55.501705: step 4122, loss 0.130746, acc 0.96
2016-09-06T00:36:56.310835: step 4123, loss 0.0280959, acc 1
2016-09-06T00:36:57.140150: step 4124, loss 0.00421776, acc 1
2016-09-06T00:36:57.940834: step 4125, loss 0.0203795, acc 1
2016-09-06T00:36:58.745812: step 4126, loss 0.0240608, acc 0.98
2016-09-06T00:36:59.527532: step 4127, loss 0.0216615, acc 0.98
2016-09-06T00:37:00.326439: step 4128, loss 0.0120628, acc 1
2016-09-06T00:37:01.128862: step 4129, loss 0.0945111, acc 0.98
2016-09-06T00:37:01.941041: step 4130, loss 0.112339, acc 0.96
2016-09-06T00:37:02.745859: step 4131, loss 0.041061, acc 0.98
2016-09-06T00:37:03.532179: step 4132, loss 0.01004, acc 1
2016-09-06T00:37:04.380501: step 4133, loss 0.103537, acc 0.94
2016-09-06T00:37:05.223055: step 4134, loss 0.0220159, acc 1
2016-09-06T00:37:06.042093: step 4135, loss 0.00578169, acc 1
2016-09-06T00:37:06.848480: step 4136, loss 0.0232843, acc 0.98
2016-09-06T00:37:07.664082: step 4137, loss 0.048298, acc 0.96
2016-09-06T00:37:08.505302: step 4138, loss 0.00505035, acc 1
2016-09-06T00:37:09.333436: step 4139, loss 0.0243932, acc 1
2016-09-06T00:37:10.149042: step 4140, loss 0.0346152, acc 0.98
2016-09-06T00:37:10.954984: step 4141, loss 0.0362342, acc 0.98
2016-09-06T00:37:11.785403: step 4142, loss 0.0450341, acc 0.98
2016-09-06T00:37:12.604790: step 4143, loss 0.00481202, acc 1
2016-09-06T00:37:13.396953: step 4144, loss 0.0510783, acc 0.96
2016-09-06T00:37:14.210048: step 4145, loss 0.0809141, acc 0.98
2016-09-06T00:37:15.009062: step 4146, loss 0.00845231, acc 1
2016-09-06T00:37:15.810398: step 4147, loss 0.00873648, acc 1
2016-09-06T00:37:16.625823: step 4148, loss 0.0508679, acc 0.98
2016-09-06T00:37:17.418894: step 4149, loss 0.0353492, acc 1
2016-09-06T00:37:18.241500: step 4150, loss 0.00339432, acc 1
2016-09-06T00:37:19.105163: step 4151, loss 0.030021, acc 1
2016-09-06T00:37:19.917530: step 4152, loss 0.0210655, acc 1
2016-09-06T00:37:20.734937: step 4153, loss 0.00692771, acc 1
2016-09-06T00:37:21.585468: step 4154, loss 0.0152982, acc 1
2016-09-06T00:37:22.386787: step 4155, loss 0.0103003, acc 1
2016-09-06T00:37:23.189786: step 4156, loss 0.023655, acc 0.98
2016-09-06T00:37:24.023630: step 4157, loss 0.0317126, acc 0.98
2016-09-06T00:37:24.836769: step 4158, loss 0.00504212, acc 1
2016-09-06T00:37:25.647928: step 4159, loss 0.0241824, acc 1
2016-09-06T00:37:26.481436: step 4160, loss 0.0386641, acc 0.98
2016-09-06T00:37:27.284575: step 4161, loss 0.0327011, acc 1
2016-09-06T00:37:28.093911: step 4162, loss 0.0829019, acc 0.98
2016-09-06T00:37:28.912765: step 4163, loss 0.0397011, acc 0.96
2016-09-06T00:37:29.710091: step 4164, loss 0.0194681, acc 0.98
2016-09-06T00:37:30.519939: step 4165, loss 0.0528386, acc 0.98
2016-09-06T00:37:31.348841: step 4166, loss 0.0981058, acc 0.98
2016-09-06T00:37:32.178467: step 4167, loss 0.0557577, acc 0.98
2016-09-06T00:37:32.965451: step 4168, loss 0.0356676, acc 0.98
2016-09-06T00:37:33.771457: step 4169, loss 0.00597781, acc 1
2016-09-06T00:37:34.587602: step 4170, loss 0.0523521, acc 0.96
2016-09-06T00:37:35.380062: step 4171, loss 0.0119215, acc 1
2016-09-06T00:37:36.161416: step 4172, loss 0.023617, acc 1
2016-09-06T00:37:36.976800: step 4173, loss 0.0412476, acc 0.98
2016-09-06T00:37:37.752246: step 4174, loss 0.00769526, acc 1
2016-09-06T00:37:38.578070: step 4175, loss 0.14892, acc 0.96
2016-09-06T00:37:39.414630: step 4176, loss 0.0217636, acc 1
2016-09-06T00:37:40.212398: step 4177, loss 0.0927848, acc 0.96
2016-09-06T00:37:41.024167: step 4178, loss 0.0121117, acc 1
2016-09-06T00:37:41.840773: step 4179, loss 0.0258126, acc 0.98
2016-09-06T00:37:42.615353: step 4180, loss 0.13614, acc 0.98
2016-09-06T00:37:43.445284: step 4181, loss 0.0361758, acc 0.98
2016-09-06T00:37:44.261923: step 4182, loss 0.0300339, acc 0.98
2016-09-06T00:37:45.076250: step 4183, loss 0.0579237, acc 0.98
2016-09-06T00:37:45.883817: step 4184, loss 0.00315647, acc 1
2016-09-06T00:37:46.697251: step 4185, loss 0.0354894, acc 0.98
2016-09-06T00:37:47.481502: step 4186, loss 0.0334631, acc 0.98
2016-09-06T00:37:48.297876: step 4187, loss 0.10299, acc 0.96
2016-09-06T00:37:49.128486: step 4188, loss 0.0582473, acc 0.98
2016-09-06T00:37:49.921635: step 4189, loss 0.0510538, acc 1
2016-09-06T00:37:50.741408: step 4190, loss 0.0319375, acc 0.98
2016-09-06T00:37:51.559228: step 4191, loss 0.109444, acc 0.96
2016-09-06T00:37:52.397729: step 4192, loss 0.0385185, acc 1
2016-09-06T00:37:53.213115: step 4193, loss 0.108531, acc 0.9
2016-09-06T00:37:54.029549: step 4194, loss 0.0366258, acc 0.98
2016-09-06T00:37:54.836264: step 4195, loss 0.233634, acc 0.94
2016-09-06T00:37:55.636768: step 4196, loss 0.0167307, acc 1
2016-09-06T00:37:56.463598: step 4197, loss 0.0320529, acc 0.98
2016-09-06T00:37:57.294826: step 4198, loss 0.0389033, acc 0.98
2016-09-06T00:37:58.135492: step 4199, loss 0.0282532, acc 1
2016-09-06T00:37:58.959061: step 4200, loss 0.0561061, acc 0.96

Evaluation:
2016-09-06T00:38:02.716920: step 4200, loss 1.38301, acc 0.727955

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-4200

2016-09-06T00:38:04.523405: step 4201, loss 0.0616368, acc 0.96
2016-09-06T00:38:05.327618: step 4202, loss 0.116399, acc 0.98
2016-09-06T00:38:06.137867: step 4203, loss 0.0744466, acc 0.96
2016-09-06T00:38:06.932729: step 4204, loss 0.106838, acc 0.98
2016-09-06T00:38:07.731505: step 4205, loss 0.0158352, acc 1
2016-09-06T00:38:08.574823: step 4206, loss 0.0361288, acc 1
2016-09-06T00:38:09.418155: step 4207, loss 0.0651492, acc 0.98
2016-09-06T00:38:10.238013: step 4208, loss 0.0233065, acc 1
2016-09-06T00:38:11.051758: step 4209, loss 0.0104223, acc 1
2016-09-06T00:38:11.854706: step 4210, loss 0.0378896, acc 0.98
2016-09-06T00:38:12.636005: step 4211, loss 0.0584096, acc 0.98
2016-09-06T00:38:13.468304: step 4212, loss 0.0439453, acc 0.98
2016-09-06T00:38:14.275289: step 4213, loss 0.0216977, acc 1
2016-09-06T00:38:15.076640: step 4214, loss 0.0595091, acc 0.96
2016-09-06T00:38:15.883811: step 4215, loss 0.0111852, acc 1
2016-09-06T00:38:16.696537: step 4216, loss 0.0219419, acc 0.98
2016-09-06T00:38:17.526413: step 4217, loss 0.0421385, acc 0.98
2016-09-06T00:38:18.404607: step 4218, loss 0.112183, acc 0.96
2016-09-06T00:38:19.205195: step 4219, loss 0.0433296, acc 0.96
2016-09-06T00:38:20.034150: step 4220, loss 0.00480275, acc 1
2016-09-06T00:38:20.875358: step 4221, loss 0.0166252, acc 1
2016-09-06T00:38:21.697637: step 4222, loss 0.0463536, acc 0.98
2016-09-06T00:38:22.480422: step 4223, loss 0.00862814, acc 1
2016-09-06T00:38:23.267067: step 4224, loss 0.0267222, acc 0.977273
2016-09-06T00:38:24.086578: step 4225, loss 0.00685445, acc 1
2016-09-06T00:38:24.886773: step 4226, loss 0.0414515, acc 0.98
2016-09-06T00:38:25.727169: step 4227, loss 0.017638, acc 1
2016-09-06T00:38:26.536110: step 4228, loss 0.016876, acc 1
2016-09-06T00:38:27.367081: step 4229, loss 0.0491317, acc 0.98
2016-09-06T00:38:28.193129: step 4230, loss 0.0427662, acc 0.98
2016-09-06T00:38:29.049156: step 4231, loss 0.027471, acc 0.98
2016-09-06T00:38:29.870010: step 4232, loss 0.097908, acc 0.96
2016-09-06T00:38:30.651940: step 4233, loss 0.0076808, acc 1
2016-09-06T00:38:31.443196: step 4234, loss 0.0142883, acc 1
2016-09-06T00:38:32.242240: step 4235, loss 0.0551218, acc 0.98
2016-09-06T00:38:33.041897: step 4236, loss 0.0335012, acc 1
2016-09-06T00:38:33.865492: step 4237, loss 0.0102899, acc 1
2016-09-06T00:38:34.656242: step 4238, loss 0.00473191, acc 1
2016-09-06T00:38:35.450314: step 4239, loss 0.0218793, acc 0.98
2016-09-06T00:38:36.251999: step 4240, loss 0.0386419, acc 0.98
2016-09-06T00:38:37.041673: step 4241, loss 0.052554, acc 0.96
2016-09-06T00:38:37.864680: step 4242, loss 0.0560231, acc 0.96
2016-09-06T00:38:38.694721: step 4243, loss 0.0282281, acc 0.98
2016-09-06T00:38:39.495710: step 4244, loss 0.0117904, acc 1
2016-09-06T00:38:40.296466: step 4245, loss 0.092235, acc 0.96
2016-09-06T00:38:41.119256: step 4246, loss 0.036416, acc 0.98
2016-09-06T00:38:41.905369: step 4247, loss 0.0314661, acc 0.98
2016-09-06T00:38:42.716925: step 4248, loss 0.0261155, acc 0.98
2016-09-06T00:38:43.502823: step 4249, loss 0.0602863, acc 0.94
2016-09-06T00:38:44.322089: step 4250, loss 0.0239504, acc 1
2016-09-06T00:38:45.121789: step 4251, loss 0.0266878, acc 0.98
2016-09-06T00:38:45.911977: step 4252, loss 0.0104692, acc 1
2016-09-06T00:38:46.740330: step 4253, loss 0.0286514, acc 0.98
2016-09-06T00:38:47.574415: step 4254, loss 0.00745751, acc 1
2016-09-06T00:38:48.378554: step 4255, loss 0.0073997, acc 1
2016-09-06T00:38:49.166314: step 4256, loss 0.0186918, acc 1
2016-09-06T00:38:49.970604: step 4257, loss 0.0158178, acc 1
2016-09-06T00:38:50.779870: step 4258, loss 0.0107151, acc 1
2016-09-06T00:38:51.556741: step 4259, loss 0.00841265, acc 1
2016-09-06T00:38:52.355055: step 4260, loss 0.0796741, acc 0.96
2016-09-06T00:38:53.157249: step 4261, loss 0.00849455, acc 1
2016-09-06T00:38:53.975444: step 4262, loss 0.0396534, acc 1
2016-09-06T00:38:54.795569: step 4263, loss 0.0449019, acc 0.98
2016-09-06T00:38:55.624437: step 4264, loss 0.0125898, acc 1
2016-09-06T00:38:56.414669: step 4265, loss 0.0353922, acc 0.98
2016-09-06T00:38:57.218970: step 4266, loss 0.0623667, acc 0.98
2016-09-06T00:38:58.031317: step 4267, loss 0.0332684, acc 0.98
2016-09-06T00:38:58.801009: step 4268, loss 0.0203456, acc 1
2016-09-06T00:38:59.598834: step 4269, loss 0.0158126, acc 1
2016-09-06T00:39:00.442113: step 4270, loss 0.0159935, acc 1
2016-09-06T00:39:01.229364: step 4271, loss 0.0285914, acc 1
2016-09-06T00:39:02.049636: step 4272, loss 0.109502, acc 0.96
2016-09-06T00:39:02.873674: step 4273, loss 0.0334137, acc 1
2016-09-06T00:39:03.665801: step 4274, loss 0.00385884, acc 1
2016-09-06T00:39:04.455613: step 4275, loss 0.0714328, acc 0.96
2016-09-06T00:39:05.249404: step 4276, loss 0.0169475, acc 1
2016-09-06T00:39:06.034221: step 4277, loss 0.074721, acc 0.94
2016-09-06T00:39:06.868476: step 4278, loss 0.0124957, acc 1
2016-09-06T00:39:07.700110: step 4279, loss 0.107277, acc 0.96
2016-09-06T00:39:08.507431: step 4280, loss 0.0135474, acc 1
2016-09-06T00:39:09.309140: step 4281, loss 0.00442778, acc 1
2016-09-06T00:39:10.145557: step 4282, loss 0.0418932, acc 0.96
2016-09-06T00:39:10.951426: step 4283, loss 0.0144613, acc 1
2016-09-06T00:39:11.765409: step 4284, loss 0.0532151, acc 0.96
2016-09-06T00:39:12.585243: step 4285, loss 0.0440881, acc 0.98
2016-09-06T00:39:13.384593: step 4286, loss 0.116721, acc 0.98
2016-09-06T00:39:14.181557: step 4287, loss 0.0208525, acc 0.98
2016-09-06T00:39:14.981438: step 4288, loss 0.0376419, acc 0.98
2016-09-06T00:39:15.751539: step 4289, loss 0.0342075, acc 0.98
2016-09-06T00:39:16.561550: step 4290, loss 0.029644, acc 0.98
2016-09-06T00:39:17.397742: step 4291, loss 0.01296, acc 1
2016-09-06T00:39:18.188635: step 4292, loss 0.052971, acc 0.98
2016-09-06T00:39:18.993061: step 4293, loss 0.0269304, acc 1
2016-09-06T00:39:19.813460: step 4294, loss 0.0144286, acc 1
2016-09-06T00:39:20.589659: step 4295, loss 0.0413317, acc 0.96
2016-09-06T00:39:21.401266: step 4296, loss 0.0300849, acc 0.98
2016-09-06T00:39:22.198517: step 4297, loss 0.0171691, acc 1
2016-09-06T00:39:22.989423: step 4298, loss 0.00695117, acc 1
2016-09-06T00:39:23.796856: step 4299, loss 0.0222069, acc 1
2016-09-06T00:39:24.619026: step 4300, loss 0.0207263, acc 1

Evaluation:
2016-09-06T00:39:28.386781: step 4300, loss 1.68399, acc 0.732645

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-4300

2016-09-06T00:39:30.335422: step 4301, loss 0.0131278, acc 1
2016-09-06T00:39:31.189266: step 4302, loss 0.0342363, acc 0.98
2016-09-06T00:39:32.017566: step 4303, loss 0.0201333, acc 1
2016-09-06T00:39:32.817994: step 4304, loss 0.01669, acc 1
2016-09-06T00:39:33.645580: step 4305, loss 0.0136321, acc 1
2016-09-06T00:39:34.430268: step 4306, loss 0.0130045, acc 1
2016-09-06T00:39:35.215018: step 4307, loss 0.0446241, acc 0.96
2016-09-06T00:39:36.000218: step 4308, loss 0.0324606, acc 0.98
2016-09-06T00:39:36.808356: step 4309, loss 0.0193643, acc 1
2016-09-06T00:39:37.582033: step 4310, loss 0.00880691, acc 1
2016-09-06T00:39:38.393633: step 4311, loss 0.106709, acc 0.98
2016-09-06T00:39:39.205940: step 4312, loss 0.0816515, acc 0.94
2016-09-06T00:39:40.021186: step 4313, loss 0.0270135, acc 1
2016-09-06T00:39:40.851406: step 4314, loss 0.025122, acc 1
2016-09-06T00:39:41.663439: step 4315, loss 0.0396412, acc 1
2016-09-06T00:39:42.456904: step 4316, loss 0.00464707, acc 1
2016-09-06T00:39:43.266116: step 4317, loss 0.112217, acc 0.98
2016-09-06T00:39:44.071052: step 4318, loss 0.0030752, acc 1
2016-09-06T00:39:44.864493: step 4319, loss 0.0206077, acc 1
2016-09-06T00:39:45.650368: step 4320, loss 0.00549076, acc 1
2016-09-06T00:39:46.480103: step 4321, loss 0.0145774, acc 1
2016-09-06T00:39:47.305547: step 4322, loss 0.0540152, acc 0.98
2016-09-06T00:39:48.133403: step 4323, loss 0.0188803, acc 0.98
2016-09-06T00:39:48.976997: step 4324, loss 0.0148878, acc 1
2016-09-06T00:39:49.768415: step 4325, loss 0.0291829, acc 0.98
2016-09-06T00:39:50.557364: step 4326, loss 0.0131707, acc 1
2016-09-06T00:39:51.380903: step 4327, loss 0.0148342, acc 1
2016-09-06T00:39:52.140523: step 4328, loss 0.00930117, acc 1
2016-09-06T00:39:52.955737: step 4329, loss 0.0323811, acc 0.98
2016-09-06T00:39:53.755891: step 4330, loss 0.0218026, acc 0.98
2016-09-06T00:39:54.554774: step 4331, loss 0.0336847, acc 1
2016-09-06T00:39:55.376788: step 4332, loss 0.0503838, acc 0.96
2016-09-06T00:39:56.196589: step 4333, loss 0.0198468, acc 0.98
2016-09-06T00:39:56.981955: step 4334, loss 0.0753677, acc 0.98
2016-09-06T00:39:57.772248: step 4335, loss 0.0245245, acc 1
2016-09-06T00:39:58.601810: step 4336, loss 0.0515727, acc 1
2016-09-06T00:39:59.369674: step 4337, loss 0.0419811, acc 0.98
2016-09-06T00:40:00.171178: step 4338, loss 0.0213118, acc 0.98
2016-09-06T00:40:01.010239: step 4339, loss 0.0284163, acc 0.98
2016-09-06T00:40:01.777702: step 4340, loss 0.0779405, acc 0.96
2016-09-06T00:40:02.586631: step 4341, loss 0.0181529, acc 1
2016-09-06T00:40:03.410931: step 4342, loss 0.0110301, acc 1
2016-09-06T00:40:04.211729: step 4343, loss 0.0693387, acc 0.96
2016-09-06T00:40:05.038789: step 4344, loss 0.0321988, acc 0.98
2016-09-06T00:40:05.876725: step 4345, loss 0.137111, acc 0.98
2016-09-06T00:40:06.639487: step 4346, loss 0.0118959, acc 1
2016-09-06T00:40:07.429815: step 4347, loss 0.00360163, acc 1
2016-09-06T00:40:08.280305: step 4348, loss 0.0141621, acc 1
2016-09-06T00:40:09.068242: step 4349, loss 0.0643099, acc 0.96
2016-09-06T00:40:09.852888: step 4350, loss 0.0106822, acc 1
2016-09-06T00:40:10.658964: step 4351, loss 0.027827, acc 0.98
2016-09-06T00:40:11.458136: step 4352, loss 0.0193285, acc 1
2016-09-06T00:40:12.265648: step 4353, loss 0.0177512, acc 1
2016-09-06T00:40:13.074244: step 4354, loss 0.0994213, acc 0.94
2016-09-06T00:40:13.846402: step 4355, loss 0.0105495, acc 1
2016-09-06T00:40:14.688537: step 4356, loss 0.00609275, acc 1
2016-09-06T00:40:15.549149: step 4357, loss 0.0394924, acc 0.98
2016-09-06T00:40:16.342006: step 4358, loss 0.0296653, acc 0.98
2016-09-06T00:40:17.139345: step 4359, loss 0.0252364, acc 0.98
2016-09-06T00:40:17.959903: step 4360, loss 0.0976787, acc 0.98
2016-09-06T00:40:18.733436: step 4361, loss 0.00943692, acc 1
2016-09-06T00:40:19.524316: step 4362, loss 0.0302553, acc 0.98
2016-09-06T00:40:20.354001: step 4363, loss 0.0339749, acc 0.98
2016-09-06T00:40:21.141407: step 4364, loss 0.0112865, acc 1
2016-09-06T00:40:21.928057: step 4365, loss 0.0610219, acc 0.96
2016-09-06T00:40:22.739657: step 4366, loss 0.0798307, acc 0.98
2016-09-06T00:40:23.546126: step 4367, loss 0.00902867, acc 1
2016-09-06T00:40:24.328552: step 4368, loss 0.0310082, acc 1
2016-09-06T00:40:25.159863: step 4369, loss 0.0758315, acc 0.98
2016-09-06T00:40:25.932192: step 4370, loss 0.00658649, acc 1
2016-09-06T00:40:26.737678: step 4371, loss 0.030808, acc 1
2016-09-06T00:40:27.544302: step 4372, loss 0.0664991, acc 0.98
2016-09-06T00:40:28.344672: step 4373, loss 0.0136263, acc 1
2016-09-06T00:40:29.162058: step 4374, loss 0.0431734, acc 0.98
2016-09-06T00:40:29.966757: step 4375, loss 0.00631448, acc 1
2016-09-06T00:40:30.753083: step 4376, loss 0.00357728, acc 1
2016-09-06T00:40:31.575529: step 4377, loss 0.00933774, acc 1
2016-09-06T00:40:32.373131: step 4378, loss 0.0537333, acc 0.98
2016-09-06T00:40:33.174132: step 4379, loss 0.0506888, acc 0.98
2016-09-06T00:40:33.993754: step 4380, loss 0.0405784, acc 0.98
2016-09-06T00:40:34.792750: step 4381, loss 0.010077, acc 1
2016-09-06T00:40:35.585636: step 4382, loss 0.0127034, acc 1
2016-09-06T00:40:36.401730: step 4383, loss 0.00456551, acc 1
2016-09-06T00:40:37.207716: step 4384, loss 0.0617248, acc 0.98
2016-09-06T00:40:37.986819: step 4385, loss 0.0764582, acc 0.94
2016-09-06T00:40:38.808654: step 4386, loss 0.090955, acc 0.96
2016-09-06T00:40:39.627845: step 4387, loss 0.00894378, acc 1
2016-09-06T00:40:40.412300: step 4388, loss 0.0202127, acc 1
2016-09-06T00:40:41.243706: step 4389, loss 0.179191, acc 0.98
2016-09-06T00:40:42.106391: step 4390, loss 0.024229, acc 1
2016-09-06T00:40:42.879202: step 4391, loss 0.0260723, acc 1
2016-09-06T00:40:43.714003: step 4392, loss 0.0162771, acc 1
2016-09-06T00:40:44.529047: step 4393, loss 0.187051, acc 0.94
2016-09-06T00:40:45.331960: step 4394, loss 0.0754176, acc 0.96
2016-09-06T00:40:46.135906: step 4395, loss 0.05098, acc 0.96
2016-09-06T00:40:46.958736: step 4396, loss 0.0227808, acc 1
2016-09-06T00:40:47.752663: step 4397, loss 0.0171927, acc 1
2016-09-06T00:40:48.558296: step 4398, loss 0.0528381, acc 0.96
2016-09-06T00:40:49.374711: step 4399, loss 0.151679, acc 0.9
2016-09-06T00:40:50.163032: step 4400, loss 0.00575631, acc 1

Evaluation:
2016-09-06T00:40:53.874419: step 4400, loss 1.28451, acc 0.729831

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-4400

2016-09-06T00:40:55.751026: step 4401, loss 0.0512768, acc 0.98
2016-09-06T00:40:56.536109: step 4402, loss 0.0658184, acc 0.96
2016-09-06T00:40:57.345545: step 4403, loss 0.0455117, acc 0.98
2016-09-06T00:40:58.165860: step 4404, loss 0.026164, acc 1
2016-09-06T00:40:58.971460: step 4405, loss 0.00927652, acc 1
2016-09-06T00:40:59.782506: step 4406, loss 0.0244358, acc 1
2016-09-06T00:41:00.610178: step 4407, loss 0.0112965, acc 1
2016-09-06T00:41:01.397696: step 4408, loss 0.0204674, acc 0.98
2016-09-06T00:41:02.189621: step 4409, loss 0.0379162, acc 1
2016-09-06T00:41:02.995526: step 4410, loss 0.0197836, acc 1
2016-09-06T00:41:03.826939: step 4411, loss 0.0488595, acc 0.98
2016-09-06T00:41:04.650323: step 4412, loss 0.0113691, acc 1
2016-09-06T00:41:05.475862: step 4413, loss 0.0662305, acc 0.96
2016-09-06T00:41:06.298352: step 4414, loss 0.0169938, acc 1
2016-09-06T00:41:07.073899: step 4415, loss 0.017718, acc 1
2016-09-06T00:41:07.828288: step 4416, loss 0.0151886, acc 1
2016-09-06T00:41:08.665393: step 4417, loss 0.00499413, acc 1
2016-09-06T00:41:09.440576: step 4418, loss 0.0347537, acc 0.98
2016-09-06T00:41:10.264340: step 4419, loss 0.0620254, acc 0.96
2016-09-06T00:41:11.070378: step 4420, loss 0.163684, acc 0.94
2016-09-06T00:41:11.858591: step 4421, loss 0.00451262, acc 1
2016-09-06T00:41:12.690604: step 4422, loss 0.0115348, acc 1
2016-09-06T00:41:13.499050: step 4423, loss 0.00550237, acc 1
2016-09-06T00:41:14.299024: step 4424, loss 0.0245424, acc 0.98
2016-09-06T00:41:15.095528: step 4425, loss 0.003624, acc 1
2016-09-06T00:41:15.897408: step 4426, loss 0.00412092, acc 1
2016-09-06T00:41:16.682590: step 4427, loss 0.0258854, acc 0.98
2016-09-06T00:41:17.510089: step 4428, loss 0.00938381, acc 1
2016-09-06T00:41:18.354974: step 4429, loss 0.0296262, acc 0.98
2016-09-06T00:41:19.183898: step 4430, loss 0.00458828, acc 1
2016-09-06T00:41:19.995976: step 4431, loss 0.00413133, acc 1
2016-09-06T00:41:20.815536: step 4432, loss 0.0750735, acc 0.98
2016-09-06T00:41:21.618663: step 4433, loss 0.00507729, acc 1
2016-09-06T00:41:22.407361: step 4434, loss 0.139765, acc 0.98
2016-09-06T00:41:23.225518: step 4435, loss 0.00333223, acc 1
2016-09-06T00:41:24.015224: step 4436, loss 0.00842675, acc 1
2016-09-06T00:41:24.795725: step 4437, loss 0.00720068, acc 1
2016-09-06T00:41:25.607490: step 4438, loss 0.0686959, acc 0.98
2016-09-06T00:41:26.395096: step 4439, loss 0.0529719, acc 0.98
2016-09-06T00:41:27.188907: step 4440, loss 0.0103904, acc 1
2016-09-06T00:41:27.993437: step 4441, loss 0.00830621, acc 1
2016-09-06T00:41:28.789404: step 4442, loss 0.0348202, acc 0.98
2016-09-06T00:41:29.589060: step 4443, loss 0.0201624, acc 0.98
2016-09-06T00:41:30.422363: step 4444, loss 0.00899063, acc 1
2016-09-06T00:41:31.223312: step 4445, loss 0.0350449, acc 0.98
2016-09-06T00:41:32.031914: step 4446, loss 0.0140301, acc 1
2016-09-06T00:41:32.839515: step 4447, loss 0.0162291, acc 1
2016-09-06T00:41:33.621441: step 4448, loss 0.00671243, acc 1
2016-09-06T00:41:34.479913: step 4449, loss 0.0575151, acc 0.98
2016-09-06T00:41:35.284412: step 4450, loss 0.0174417, acc 0.98
2016-09-06T00:41:36.078675: step 4451, loss 0.0410046, acc 0.98
2016-09-06T00:41:36.887784: step 4452, loss 0.0542589, acc 0.96
2016-09-06T00:41:37.744116: step 4453, loss 0.00594392, acc 1
2016-09-06T00:41:38.543087: step 4454, loss 0.0237792, acc 1
2016-09-06T00:41:39.326190: step 4455, loss 0.125395, acc 0.96
2016-09-06T00:41:40.170228: step 4456, loss 0.0234546, acc 0.98
2016-09-06T00:41:40.962139: step 4457, loss 0.00579988, acc 1
2016-09-06T00:41:41.772113: step 4458, loss 0.0229957, acc 0.98
2016-09-06T00:41:42.585386: step 4459, loss 0.0351848, acc 0.98
2016-09-06T00:41:43.390453: step 4460, loss 0.0307797, acc 0.98
2016-09-06T00:41:44.191354: step 4461, loss 0.0414623, acc 0.98
2016-09-06T00:41:45.000954: step 4462, loss 0.0633066, acc 0.98
2016-09-06T00:41:45.813438: step 4463, loss 0.0846294, acc 0.96
2016-09-06T00:41:46.618126: step 4464, loss 0.0638226, acc 0.96
2016-09-06T00:41:47.420166: step 4465, loss 0.0301966, acc 0.98
2016-09-06T00:41:48.210972: step 4466, loss 0.0170208, acc 1
2016-09-06T00:41:49.019366: step 4467, loss 0.0186605, acc 1
2016-09-06T00:41:49.836009: step 4468, loss 0.044712, acc 0.98
2016-09-06T00:41:50.634347: step 4469, loss 0.0152195, acc 1
2016-09-06T00:41:51.431436: step 4470, loss 0.0204405, acc 1
2016-09-06T00:41:52.265727: step 4471, loss 0.0260136, acc 0.98
2016-09-06T00:41:53.071265: step 4472, loss 0.0777724, acc 0.98
2016-09-06T00:41:53.876813: step 4473, loss 0.0213526, acc 0.98
2016-09-06T00:41:54.697509: step 4474, loss 0.0346051, acc 1
2016-09-06T00:41:55.507400: step 4475, loss 0.0232655, acc 0.98
2016-09-06T00:41:56.325982: step 4476, loss 0.00359522, acc 1
2016-09-06T00:41:57.181714: step 4477, loss 0.101504, acc 0.98
2016-09-06T00:41:57.982550: step 4478, loss 0.0456744, acc 0.98
2016-09-06T00:41:58.799265: step 4479, loss 0.0223793, acc 1
2016-09-06T00:41:59.630861: step 4480, loss 0.00473689, acc 1
2016-09-06T00:42:00.440586: step 4481, loss 0.0153822, acc 1
2016-09-06T00:42:01.241279: step 4482, loss 0.0231243, acc 0.98
2016-09-06T00:42:02.050081: step 4483, loss 0.0222988, acc 1
2016-09-06T00:42:02.864071: step 4484, loss 0.0830401, acc 0.96
2016-09-06T00:42:03.676583: step 4485, loss 0.0489328, acc 0.98
2016-09-06T00:42:04.522814: step 4486, loss 0.00818324, acc 1
2016-09-06T00:42:05.336079: step 4487, loss 0.0878173, acc 0.98
2016-09-06T00:42:06.136696: step 4488, loss 0.0544913, acc 0.96
2016-09-06T00:42:06.932255: step 4489, loss 0.0149134, acc 1
2016-09-06T00:42:07.744206: step 4490, loss 0.00954203, acc 1
2016-09-06T00:42:08.524686: step 4491, loss 0.0317959, acc 0.98
2016-09-06T00:42:09.343931: step 4492, loss 0.0452109, acc 0.98
2016-09-06T00:42:10.174665: step 4493, loss 0.0337135, acc 0.98
2016-09-06T00:42:10.979934: step 4494, loss 0.0628687, acc 0.98
2016-09-06T00:42:11.800809: step 4495, loss 0.0143792, acc 1
2016-09-06T00:42:12.619898: step 4496, loss 0.0147694, acc 1
2016-09-06T00:42:13.432722: step 4497, loss 0.0182577, acc 1
2016-09-06T00:42:14.250861: step 4498, loss 0.00855208, acc 1
2016-09-06T00:42:15.077608: step 4499, loss 0.0702256, acc 0.96
2016-09-06T00:42:15.901403: step 4500, loss 0.00355143, acc 1

Evaluation:
2016-09-06T00:42:19.624365: step 4500, loss 1.67487, acc 0.729831

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-4500

2016-09-06T00:42:21.612855: step 4501, loss 0.0127374, acc 1
2016-09-06T00:42:22.415429: step 4502, loss 0.0280715, acc 0.98
2016-09-06T00:42:23.213832: step 4503, loss 0.005252, acc 1
2016-09-06T00:42:24.050510: step 4504, loss 0.0344932, acc 1
2016-09-06T00:42:24.865397: step 4505, loss 0.0306403, acc 1
2016-09-06T00:42:25.701365: step 4506, loss 0.0143228, acc 1
2016-09-06T00:42:26.546849: step 4507, loss 0.00591613, acc 1
2016-09-06T00:42:27.375152: step 4508, loss 0.00283316, acc 1
2016-09-06T00:42:28.153383: step 4509, loss 0.0290958, acc 1
2016-09-06T00:42:28.988479: step 4510, loss 0.0219059, acc 1
2016-09-06T00:42:29.816352: step 4511, loss 0.0170688, acc 1
2016-09-06T00:42:30.624552: step 4512, loss 0.00773066, acc 1
2016-09-06T00:42:31.453012: step 4513, loss 0.0286692, acc 0.98
2016-09-06T00:42:32.274572: step 4514, loss 0.0185351, acc 1
2016-09-06T00:42:33.118195: step 4515, loss 0.011425, acc 1
2016-09-06T00:42:33.915048: step 4516, loss 0.00431085, acc 1
2016-09-06T00:42:34.755778: step 4517, loss 0.0128574, acc 1
2016-09-06T00:42:35.557311: step 4518, loss 0.00402389, acc 1
2016-09-06T00:42:36.367165: step 4519, loss 0.0621334, acc 0.98
2016-09-06T00:42:37.181433: step 4520, loss 0.00317577, acc 1
2016-09-06T00:42:37.989414: step 4521, loss 0.0967694, acc 0.92
2016-09-06T00:42:38.804976: step 4522, loss 0.0991925, acc 0.96
2016-09-06T00:42:39.608936: step 4523, loss 0.0221751, acc 0.98
2016-09-06T00:42:40.389322: step 4524, loss 0.0499414, acc 0.96
2016-09-06T00:42:41.188174: step 4525, loss 0.00891535, acc 1
2016-09-06T00:42:42.018718: step 4526, loss 0.0402992, acc 0.98
2016-09-06T00:42:42.850439: step 4527, loss 0.00439539, acc 1
2016-09-06T00:42:43.658122: step 4528, loss 0.0229205, acc 1
2016-09-06T00:42:44.485047: step 4529, loss 0.0566373, acc 0.98
2016-09-06T00:42:45.303647: step 4530, loss 0.00402001, acc 1
2016-09-06T00:42:46.094565: step 4531, loss 0.0216242, acc 1
2016-09-06T00:42:46.921989: step 4532, loss 0.0197491, acc 1
2016-09-06T00:42:47.732274: step 4533, loss 0.0161805, acc 1
2016-09-06T00:42:48.533502: step 4534, loss 0.00377264, acc 1
2016-09-06T00:42:49.350111: step 4535, loss 0.105786, acc 0.94
2016-09-06T00:42:50.160517: step 4536, loss 0.0543034, acc 0.96
2016-09-06T00:42:50.967673: step 4537, loss 0.0545829, acc 0.96
2016-09-06T00:42:51.820853: step 4538, loss 0.0285801, acc 0.98
2016-09-06T00:42:52.683175: step 4539, loss 0.00965903, acc 1
2016-09-06T00:42:53.498881: step 4540, loss 0.0174892, acc 0.98
2016-09-06T00:42:54.322381: step 4541, loss 0.00844471, acc 1
2016-09-06T00:42:55.116790: step 4542, loss 0.0273979, acc 0.98
2016-09-06T00:42:55.934736: step 4543, loss 0.0311704, acc 0.98
2016-09-06T00:42:56.764318: step 4544, loss 0.0294409, acc 0.98
2016-09-06T00:42:57.565015: step 4545, loss 0.0434653, acc 0.98
2016-09-06T00:42:58.379757: step 4546, loss 0.0194944, acc 1
2016-09-06T00:42:59.184534: step 4547, loss 0.0118673, acc 1
2016-09-06T00:42:59.992186: step 4548, loss 0.0248856, acc 1
2016-09-06T00:43:00.820610: step 4549, loss 0.00828028, acc 1
2016-09-06T00:43:01.649600: step 4550, loss 0.0820037, acc 0.96
2016-09-06T00:43:02.485359: step 4551, loss 0.0703306, acc 0.96
2016-09-06T00:43:03.248580: step 4552, loss 0.0395909, acc 0.98
2016-09-06T00:43:04.045573: step 4553, loss 0.0035227, acc 1
2016-09-06T00:43:04.883718: step 4554, loss 0.0467239, acc 0.98
2016-09-06T00:43:05.654998: step 4555, loss 0.00871204, acc 1
2016-09-06T00:43:06.465064: step 4556, loss 0.0122762, acc 1
2016-09-06T00:43:07.289397: step 4557, loss 0.0615715, acc 0.96
2016-09-06T00:43:08.062720: step 4558, loss 0.0129216, acc 1
2016-09-06T00:43:08.842810: step 4559, loss 0.0969817, acc 0.98
2016-09-06T00:43:09.678293: step 4560, loss 0.0417043, acc 0.96
2016-09-06T00:43:10.452951: step 4561, loss 0.0155611, acc 1
2016-09-06T00:43:11.264072: step 4562, loss 0.0252141, acc 1
2016-09-06T00:43:12.095437: step 4563, loss 0.0379167, acc 0.98
2016-09-06T00:43:12.892094: step 4564, loss 0.0182319, acc 1
2016-09-06T00:43:13.695041: step 4565, loss 0.0534232, acc 0.96
2016-09-06T00:43:14.521159: step 4566, loss 0.0561012, acc 0.98
2016-09-06T00:43:15.311056: step 4567, loss 0.0205114, acc 1
2016-09-06T00:43:16.120768: step 4568, loss 0.0259846, acc 0.98
2016-09-06T00:43:16.933733: step 4569, loss 0.00421102, acc 1
2016-09-06T00:43:17.741307: step 4570, loss 0.00662109, acc 1
2016-09-06T00:43:18.562849: step 4571, loss 0.0530327, acc 0.98
2016-09-06T00:43:19.360580: step 4572, loss 0.0441992, acc 0.96
2016-09-06T00:43:20.138765: step 4573, loss 0.0247447, acc 0.98
2016-09-06T00:43:20.943893: step 4574, loss 0.0411736, acc 0.96
2016-09-06T00:43:21.782160: step 4575, loss 0.0145929, acc 1
2016-09-06T00:43:22.592629: step 4576, loss 0.0234832, acc 1
2016-09-06T00:43:23.379163: step 4577, loss 0.0248076, acc 0.98
2016-09-06T00:43:24.191351: step 4578, loss 0.0262776, acc 0.98
2016-09-06T00:43:24.993631: step 4579, loss 0.0442961, acc 0.98
2016-09-06T00:43:25.785215: step 4580, loss 0.0057693, acc 1
2016-09-06T00:43:26.638023: step 4581, loss 0.0045441, acc 1
2016-09-06T00:43:27.413598: step 4582, loss 0.0336195, acc 0.96
2016-09-06T00:43:28.196492: step 4583, loss 0.0201451, acc 1
2016-09-06T00:43:29.006131: step 4584, loss 0.0386497, acc 0.98
2016-09-06T00:43:29.796417: step 4585, loss 0.0766829, acc 0.98
2016-09-06T00:43:30.622988: step 4586, loss 0.0172706, acc 1
2016-09-06T00:43:31.447905: step 4587, loss 0.0344802, acc 1
2016-09-06T00:43:32.223905: step 4588, loss 0.104855, acc 0.98
2016-09-06T00:43:33.023075: step 4589, loss 0.019157, acc 0.98
2016-09-06T00:43:33.852746: step 4590, loss 0.101132, acc 0.96
2016-09-06T00:43:34.644940: step 4591, loss 0.0621469, acc 0.96
2016-09-06T00:43:35.451881: step 4592, loss 0.0210631, acc 0.98
2016-09-06T00:43:36.260550: step 4593, loss 0.0178613, acc 1
2016-09-06T00:43:37.054298: step 4594, loss 0.0281213, acc 1
2016-09-06T00:43:37.851563: step 4595, loss 0.0228948, acc 0.98
2016-09-06T00:43:38.671173: step 4596, loss 0.00849941, acc 1
2016-09-06T00:43:39.480935: step 4597, loss 0.0175228, acc 1
2016-09-06T00:43:40.318992: step 4598, loss 0.0161587, acc 1
2016-09-06T00:43:41.138146: step 4599, loss 0.0195328, acc 0.98
2016-09-06T00:43:41.909243: step 4600, loss 0.0383993, acc 0.98

Evaluation:
2016-09-06T00:43:45.634947: step 4600, loss 1.73598, acc 0.717636

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-4600

2016-09-06T00:43:47.486962: step 4601, loss 0.0745419, acc 0.98
2016-09-06T00:43:48.278490: step 4602, loss 0.0218549, acc 0.98
2016-09-06T00:43:49.101105: step 4603, loss 0.0636042, acc 0.96
2016-09-06T00:43:49.925422: step 4604, loss 0.0287467, acc 1
2016-09-06T00:43:50.741416: step 4605, loss 0.0170708, acc 1
2016-09-06T00:43:51.553090: step 4606, loss 0.0142145, acc 1
2016-09-06T00:43:52.405901: step 4607, loss 0.038184, acc 0.98
2016-09-06T00:43:53.161511: step 4608, loss 0.00624314, acc 1
2016-09-06T00:43:53.966809: step 4609, loss 0.00980414, acc 1
2016-09-06T00:43:54.783857: step 4610, loss 0.0643169, acc 0.98
2016-09-06T00:43:55.596175: step 4611, loss 0.0134708, acc 1
2016-09-06T00:43:56.455071: step 4612, loss 0.0736409, acc 0.96
2016-09-06T00:43:57.277960: step 4613, loss 0.00998225, acc 1
2016-09-06T00:43:58.100158: step 4614, loss 0.0741349, acc 0.98
2016-09-06T00:43:58.886602: step 4615, loss 0.011201, acc 1
2016-09-06T00:43:59.712350: step 4616, loss 0.0101804, acc 1
2016-09-06T00:44:00.571459: step 4617, loss 0.00827158, acc 1
2016-09-06T00:44:01.357227: step 4618, loss 0.00428902, acc 1
2016-09-06T00:44:02.142553: step 4619, loss 0.0211373, acc 0.98
2016-09-06T00:44:02.982535: step 4620, loss 0.00441453, acc 1
2016-09-06T00:44:03.768903: step 4621, loss 0.0623267, acc 0.96
2016-09-06T00:44:04.575813: step 4622, loss 0.0214294, acc 0.98
2016-09-06T00:44:05.424642: step 4623, loss 0.0293189, acc 0.98
2016-09-06T00:44:06.208467: step 4624, loss 0.0612575, acc 0.96
2016-09-06T00:44:07.040940: step 4625, loss 0.0375511, acc 0.98
2016-09-06T00:44:07.837480: step 4626, loss 0.0358777, acc 0.98
2016-09-06T00:44:08.617939: step 4627, loss 0.0130738, acc 1
2016-09-06T00:44:09.422302: step 4628, loss 0.0305259, acc 0.98
2016-09-06T00:44:10.260390: step 4629, loss 0.0890174, acc 0.94
2016-09-06T00:44:11.018336: step 4630, loss 0.0476332, acc 0.98
2016-09-06T00:44:11.821999: step 4631, loss 0.00641185, acc 1
2016-09-06T00:44:12.666168: step 4632, loss 0.0347458, acc 0.98
2016-09-06T00:44:13.484576: step 4633, loss 0.0317655, acc 1
2016-09-06T00:44:14.286144: step 4634, loss 0.0589369, acc 0.98
2016-09-06T00:44:15.094130: step 4635, loss 0.0160927, acc 1
2016-09-06T00:44:15.871719: step 4636, loss 0.0114281, acc 1
2016-09-06T00:44:16.650617: step 4637, loss 0.0849977, acc 0.96
2016-09-06T00:44:17.459251: step 4638, loss 0.0385183, acc 0.98
2016-09-06T00:44:18.264947: step 4639, loss 0.0322097, acc 0.98
2016-09-06T00:44:19.064966: step 4640, loss 0.0174352, acc 1
2016-09-06T00:44:19.871151: step 4641, loss 0.0393606, acc 0.96
2016-09-06T00:44:20.664755: step 4642, loss 0.0414965, acc 1
2016-09-06T00:44:21.539379: step 4643, loss 0.0184006, acc 1
2016-09-06T00:44:22.349210: step 4644, loss 0.0529468, acc 0.96
2016-09-06T00:44:23.134668: step 4645, loss 0.0166023, acc 1
2016-09-06T00:44:23.934877: step 4646, loss 0.0342131, acc 0.98
2016-09-06T00:44:24.742734: step 4647, loss 0.0919265, acc 0.96
2016-09-06T00:44:25.507734: step 4648, loss 0.0113809, acc 1
2016-09-06T00:44:26.282310: step 4649, loss 0.0202273, acc 1
2016-09-06T00:44:27.089228: step 4650, loss 0.00898529, acc 1
2016-09-06T00:44:27.886775: step 4651, loss 0.0366651, acc 0.98
2016-09-06T00:44:28.704226: step 4652, loss 0.00689609, acc 1
2016-09-06T00:44:29.510366: step 4653, loss 0.00419589, acc 1
2016-09-06T00:44:30.301158: step 4654, loss 0.0150089, acc 1
2016-09-06T00:44:31.122967: step 4655, loss 0.018405, acc 1
2016-09-06T00:44:31.984912: step 4656, loss 0.0142102, acc 1
2016-09-06T00:44:32.791876: step 4657, loss 0.143937, acc 0.96
2016-09-06T00:44:33.599157: step 4658, loss 0.0331293, acc 0.98
2016-09-06T00:44:34.426327: step 4659, loss 0.00706571, acc 1
2016-09-06T00:44:35.210227: step 4660, loss 0.0600609, acc 0.96
2016-09-06T00:44:36.014346: step 4661, loss 0.0302543, acc 1
2016-09-06T00:44:36.824437: step 4662, loss 0.0692465, acc 0.94
2016-09-06T00:44:37.631452: step 4663, loss 0.0186803, acc 0.98
2016-09-06T00:44:38.432158: step 4664, loss 0.0167352, acc 1
2016-09-06T00:44:39.269260: step 4665, loss 0.00312747, acc 1
2016-09-06T00:44:40.070414: step 4666, loss 0.0686995, acc 0.98
2016-09-06T00:44:40.865416: step 4667, loss 0.0086966, acc 1
2016-09-06T00:44:41.655726: step 4668, loss 0.0148781, acc 1
2016-09-06T00:44:42.438401: step 4669, loss 0.0611053, acc 0.98
2016-09-06T00:44:43.220910: step 4670, loss 0.0437468, acc 0.98
2016-09-06T00:44:44.036899: step 4671, loss 0.00325988, acc 1
2016-09-06T00:44:44.813874: step 4672, loss 0.0544852, acc 0.96
2016-09-06T00:44:45.637056: step 4673, loss 0.00595862, acc 1
2016-09-06T00:44:46.457862: step 4674, loss 0.100108, acc 0.96
2016-09-06T00:44:47.241700: step 4675, loss 0.0036029, acc 1
2016-09-06T00:44:48.049736: step 4676, loss 0.0441535, acc 0.96
2016-09-06T00:44:48.837481: step 4677, loss 0.0875685, acc 0.94
2016-09-06T00:44:49.629026: step 4678, loss 0.0170742, acc 1
2016-09-06T00:44:50.433423: step 4679, loss 0.0547243, acc 0.96
2016-09-06T00:44:51.215586: step 4680, loss 0.0109977, acc 1
2016-09-06T00:44:52.033585: step 4681, loss 0.0459209, acc 0.96
2016-09-06T00:44:52.862760: step 4682, loss 0.0260271, acc 1
2016-09-06T00:44:53.689403: step 4683, loss 0.0323187, acc 1
2016-09-06T00:44:54.506430: step 4684, loss 0.0176001, acc 1
2016-09-06T00:44:55.338046: step 4685, loss 0.00790954, acc 1
2016-09-06T00:44:56.181930: step 4686, loss 0.0485216, acc 0.98
2016-09-06T00:44:56.965423: step 4687, loss 0.023112, acc 1
2016-09-06T00:44:57.747236: step 4688, loss 0.00244432, acc 1
2016-09-06T00:44:58.561923: step 4689, loss 0.00424166, acc 1
2016-09-06T00:44:59.349815: step 4690, loss 0.0142091, acc 1
2016-09-06T00:45:00.189806: step 4691, loss 0.0621045, acc 0.98
2016-09-06T00:45:01.041891: step 4692, loss 0.00258105, acc 1
2016-09-06T00:45:01.834697: step 4693, loss 0.0506396, acc 0.96
2016-09-06T00:45:02.648933: step 4694, loss 0.0129228, acc 1
2016-09-06T00:45:03.464013: step 4695, loss 0.014848, acc 1
2016-09-06T00:45:04.266820: step 4696, loss 0.0115667, acc 1
2016-09-06T00:45:05.081668: step 4697, loss 0.00585336, acc 1
2016-09-06T00:45:05.910361: step 4698, loss 0.014509, acc 1
2016-09-06T00:45:06.728076: step 4699, loss 0.09481, acc 0.98
2016-09-06T00:45:07.525458: step 4700, loss 0.00398564, acc 1

Evaluation:
2016-09-06T00:45:11.271238: step 4700, loss 2.19291, acc 0.719512

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-4700

2016-09-06T00:45:13.173694: step 4701, loss 0.0102068, acc 1
2016-09-06T00:45:13.982661: step 4702, loss 0.00781428, acc 1
2016-09-06T00:45:14.777299: step 4703, loss 0.106033, acc 0.96
2016-09-06T00:45:15.588227: step 4704, loss 0.00972289, acc 1
2016-09-06T00:45:16.369234: step 4705, loss 0.00453095, acc 1
2016-09-06T00:45:17.191115: step 4706, loss 0.0491909, acc 0.98
2016-09-06T00:45:18.012088: step 4707, loss 0.0153117, acc 1
2016-09-06T00:45:18.829764: step 4708, loss 0.0217653, acc 0.98
2016-09-06T00:45:19.622386: step 4709, loss 0.0274564, acc 0.98
2016-09-06T00:45:20.430059: step 4710, loss 0.00924208, acc 1
2016-09-06T00:45:21.240799: step 4711, loss 0.00860237, acc 1
2016-09-06T00:45:22.036798: step 4712, loss 0.0271529, acc 0.98
2016-09-06T00:45:22.857912: step 4713, loss 0.0416142, acc 0.96
2016-09-06T00:45:23.660617: step 4714, loss 0.028745, acc 0.98
2016-09-06T00:45:24.457898: step 4715, loss 0.0966393, acc 0.96
2016-09-06T00:45:25.283190: step 4716, loss 0.0261336, acc 1
2016-09-06T00:45:26.111234: step 4717, loss 0.00838127, acc 1
2016-09-06T00:45:26.899856: step 4718, loss 0.0374382, acc 0.98
2016-09-06T00:45:27.734948: step 4719, loss 0.0175844, acc 1
2016-09-06T00:45:28.548264: step 4720, loss 0.035668, acc 0.98
2016-09-06T00:45:29.359979: step 4721, loss 0.00417997, acc 1
2016-09-06T00:45:30.199748: step 4722, loss 0.051063, acc 0.98
2016-09-06T00:45:31.026893: step 4723, loss 0.0507192, acc 0.96
2016-09-06T00:45:31.822787: step 4724, loss 0.00384357, acc 1
2016-09-06T00:45:32.626330: step 4725, loss 0.0205888, acc 0.98
2016-09-06T00:45:33.444936: step 4726, loss 0.0149788, acc 1
2016-09-06T00:45:34.251066: step 4727, loss 0.0594312, acc 0.96
2016-09-06T00:45:35.078020: step 4728, loss 0.0417482, acc 0.98
2016-09-06T00:45:35.912029: step 4729, loss 0.005925, acc 1
2016-09-06T00:45:36.743850: step 4730, loss 0.0139184, acc 1
2016-09-06T00:45:37.594478: step 4731, loss 0.0217019, acc 1
2016-09-06T00:45:38.418772: step 4732, loss 0.0195037, acc 0.98
2016-09-06T00:45:39.213405: step 4733, loss 0.0269187, acc 1
2016-09-06T00:45:40.066831: step 4734, loss 0.0251546, acc 1
2016-09-06T00:45:40.911029: step 4735, loss 0.0488343, acc 0.98
2016-09-06T00:45:41.699591: step 4736, loss 0.110254, acc 0.96
2016-09-06T00:45:42.491581: step 4737, loss 0.0561027, acc 0.96
2016-09-06T00:45:43.305415: step 4738, loss 0.0471928, acc 0.98
2016-09-06T00:45:44.090234: step 4739, loss 0.0348105, acc 0.96
2016-09-06T00:45:44.912987: step 4740, loss 0.0198686, acc 1
2016-09-06T00:45:45.728397: step 4741, loss 0.0201143, acc 0.98
2016-09-06T00:45:46.511065: step 4742, loss 0.0199826, acc 1
2016-09-06T00:45:47.329999: step 4743, loss 0.0251146, acc 1
2016-09-06T00:45:48.147700: step 4744, loss 0.00477997, acc 1
2016-09-06T00:45:48.950648: step 4745, loss 0.0375897, acc 0.98
2016-09-06T00:45:49.751226: step 4746, loss 0.0583316, acc 0.96
2016-09-06T00:45:50.567609: step 4747, loss 0.0118035, acc 1
2016-09-06T00:45:51.380752: step 4748, loss 0.00711569, acc 1
2016-09-06T00:45:52.174842: step 4749, loss 0.0342877, acc 0.98
2016-09-06T00:45:52.979017: step 4750, loss 0.0113353, acc 1
2016-09-06T00:45:53.752491: step 4751, loss 0.00273025, acc 1
2016-09-06T00:45:54.572223: step 4752, loss 0.0274323, acc 0.98
2016-09-06T00:45:55.395312: step 4753, loss 0.0231042, acc 1
2016-09-06T00:45:56.214786: step 4754, loss 0.0485872, acc 0.98
2016-09-06T00:45:57.013077: step 4755, loss 0.00670978, acc 1
2016-09-06T00:45:57.864140: step 4756, loss 0.00541138, acc 1
2016-09-06T00:45:58.629486: step 4757, loss 0.0167194, acc 1
2016-09-06T00:45:59.431270: step 4758, loss 0.0252587, acc 1
2016-09-06T00:46:00.260802: step 4759, loss 0.0286279, acc 0.98
2016-09-06T00:46:01.028210: step 4760, loss 0.00315408, acc 1
2016-09-06T00:46:01.840165: step 4761, loss 0.00743919, acc 1
2016-09-06T00:46:02.655385: step 4762, loss 0.00279958, acc 1
2016-09-06T00:46:03.445232: step 4763, loss 0.00292632, acc 1
2016-09-06T00:46:04.289101: step 4764, loss 0.161033, acc 0.96
2016-09-06T00:46:05.142657: step 4765, loss 0.0164282, acc 1
2016-09-06T00:46:05.944317: step 4766, loss 0.0116083, acc 1
2016-09-06T00:46:06.752218: step 4767, loss 0.111388, acc 0.98
2016-09-06T00:46:07.571269: step 4768, loss 0.0184285, acc 1
2016-09-06T00:46:08.379866: step 4769, loss 0.0320148, acc 0.98
2016-09-06T00:46:09.187957: step 4770, loss 0.00306738, acc 1
2016-09-06T00:46:10.000331: step 4771, loss 0.0254426, acc 1
2016-09-06T00:46:10.822870: step 4772, loss 0.024148, acc 0.98
2016-09-06T00:46:11.623916: step 4773, loss 0.0123013, acc 1
2016-09-06T00:46:12.473233: step 4774, loss 0.0429791, acc 0.96
2016-09-06T00:46:13.279866: step 4775, loss 0.0120561, acc 1
2016-09-06T00:46:14.083752: step 4776, loss 0.0101252, acc 1
2016-09-06T00:46:14.900209: step 4777, loss 0.0622385, acc 0.96
2016-09-06T00:46:15.729975: step 4778, loss 0.0271734, acc 0.98
2016-09-06T00:46:16.551718: step 4779, loss 0.0218374, acc 1
2016-09-06T00:46:17.405631: step 4780, loss 0.0523907, acc 0.96
2016-09-06T00:46:18.217058: step 4781, loss 0.00468953, acc 1
2016-09-06T00:46:19.029304: step 4782, loss 0.0278483, acc 0.98
2016-09-06T00:46:19.850906: step 4783, loss 0.0261231, acc 0.98
2016-09-06T00:46:20.733125: step 4784, loss 0.0158369, acc 1
2016-09-06T00:46:21.541937: step 4785, loss 0.0290795, acc 0.98
2016-09-06T00:46:22.377081: step 4786, loss 0.102334, acc 0.98
2016-09-06T00:46:23.208092: step 4787, loss 0.0609387, acc 0.98
2016-09-06T00:46:23.995028: step 4788, loss 0.0187007, acc 0.98
2016-09-06T00:46:24.793724: step 4789, loss 0.0199745, acc 0.98
2016-09-06T00:46:25.597616: step 4790, loss 0.0473134, acc 0.98
2016-09-06T00:46:26.381379: step 4791, loss 0.00386708, acc 1
2016-09-06T00:46:27.186116: step 4792, loss 0.00367808, acc 1
2016-09-06T00:46:27.960101: step 4793, loss 0.0286411, acc 0.98
2016-09-06T00:46:28.786600: step 4794, loss 0.0389447, acc 0.96
2016-09-06T00:46:29.613303: step 4795, loss 0.109825, acc 0.98
2016-09-06T00:46:30.469029: step 4796, loss 0.00257575, acc 1
2016-09-06T00:46:31.288008: step 4797, loss 0.0642406, acc 0.96
2016-09-06T00:46:32.083546: step 4798, loss 0.00525269, acc 1
2016-09-06T00:46:32.911355: step 4799, loss 0.00985568, acc 1
2016-09-06T00:46:33.626306: step 4800, loss 0.00657755, acc 1

Evaluation:
2016-09-06T00:46:37.386523: step 4800, loss 1.76997, acc 0.726079

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-4800

2016-09-06T00:46:39.302853: step 4801, loss 0.00666723, acc 1
2016-09-06T00:46:40.117840: step 4802, loss 0.0629544, acc 0.96
2016-09-06T00:46:40.921957: step 4803, loss 0.00370495, acc 1
2016-09-06T00:46:41.753249: step 4804, loss 0.0422311, acc 0.98
2016-09-06T00:46:42.595175: step 4805, loss 0.00443573, acc 1
2016-09-06T00:46:43.436664: step 4806, loss 0.0101077, acc 1
2016-09-06T00:46:44.243322: step 4807, loss 0.0100777, acc 1
2016-09-06T00:46:45.060274: step 4808, loss 0.019984, acc 0.98
2016-09-06T00:46:45.832998: step 4809, loss 0.0377558, acc 0.98
2016-09-06T00:46:46.634711: step 4810, loss 0.00266178, acc 1
2016-09-06T00:46:47.467510: step 4811, loss 0.0332923, acc 0.98
2016-09-06T00:46:48.239309: step 4812, loss 0.00287803, acc 1
2016-09-06T00:46:49.036364: step 4813, loss 0.0618259, acc 0.98
2016-09-06T00:46:49.877392: step 4814, loss 0.0069695, acc 1
2016-09-06T00:46:50.681530: step 4815, loss 0.0211563, acc 1
2016-09-06T00:46:51.477632: step 4816, loss 0.0395521, acc 1
2016-09-06T00:46:52.262112: step 4817, loss 0.0204707, acc 1
2016-09-06T00:46:53.048773: step 4818, loss 0.00268279, acc 1
2016-09-06T00:46:53.844765: step 4819, loss 0.0100601, acc 1
2016-09-06T00:46:54.659431: step 4820, loss 0.0108338, acc 1
2016-09-06T00:46:55.429557: step 4821, loss 0.0171647, acc 1
2016-09-06T00:46:56.251332: step 4822, loss 0.0477221, acc 0.98
2016-09-06T00:46:57.079040: step 4823, loss 0.0483826, acc 1
2016-09-06T00:46:57.873462: step 4824, loss 0.0470286, acc 0.96
2016-09-06T00:46:58.682138: step 4825, loss 0.0181157, acc 1
2016-09-06T00:46:59.493554: step 4826, loss 0.0440701, acc 0.96
2016-09-06T00:47:00.284994: step 4827, loss 0.0282573, acc 0.98
2016-09-06T00:47:01.085326: step 4828, loss 0.0182079, acc 0.98
2016-09-06T00:47:01.887108: step 4829, loss 0.0221121, acc 1
2016-09-06T00:47:02.687240: step 4830, loss 0.00461826, acc 1
2016-09-06T00:47:03.492598: step 4831, loss 0.0199911, acc 0.98
2016-09-06T00:47:04.288903: step 4832, loss 0.0246616, acc 1
2016-09-06T00:47:05.092511: step 4833, loss 0.0130737, acc 1
2016-09-06T00:47:05.924438: step 4834, loss 0.00326911, acc 1
2016-09-06T00:47:06.739289: step 4835, loss 0.00297361, acc 1
2016-09-06T00:47:07.550832: step 4836, loss 0.0302741, acc 0.98
2016-09-06T00:47:08.403200: step 4837, loss 0.00841976, acc 1
2016-09-06T00:47:09.195725: step 4838, loss 0.0342831, acc 0.96
2016-09-06T00:47:09.983889: step 4839, loss 0.0346578, acc 0.98
2016-09-06T00:47:10.767469: step 4840, loss 0.00323789, acc 1
2016-09-06T00:47:11.591308: step 4841, loss 0.0531365, acc 0.96
2016-09-06T00:47:12.385414: step 4842, loss 0.0383519, acc 0.98
2016-09-06T00:47:13.199010: step 4843, loss 0.0187714, acc 1
2016-09-06T00:47:14.012395: step 4844, loss 0.0250344, acc 0.98
2016-09-06T00:47:14.813931: step 4845, loss 0.00284881, acc 1
2016-09-06T00:47:15.622867: step 4846, loss 0.0226908, acc 0.98
2016-09-06T00:47:16.430118: step 4847, loss 0.00402308, acc 1
2016-09-06T00:47:17.219442: step 4848, loss 0.00826966, acc 1
2016-09-06T00:47:18.011229: step 4849, loss 0.0868623, acc 0.98
2016-09-06T00:47:18.825908: step 4850, loss 0.00322849, acc 1
2016-09-06T00:47:19.598042: step 4851, loss 0.00419803, acc 1
2016-09-06T00:47:20.408745: step 4852, loss 0.035659, acc 0.98
2016-09-06T00:47:21.221423: step 4853, loss 0.105281, acc 0.96
2016-09-06T00:47:22.011653: step 4854, loss 0.0159257, acc 1
2016-09-06T00:47:22.836717: step 4855, loss 0.00776765, acc 1
2016-09-06T00:47:23.654457: step 4856, loss 0.00450287, acc 1
2016-09-06T00:47:24.423286: step 4857, loss 0.00492302, acc 1
2016-09-06T00:47:25.225559: step 4858, loss 0.0440096, acc 0.98
2016-09-06T00:47:26.013371: step 4859, loss 0.00249061, acc 1
2016-09-06T00:47:26.815234: step 4860, loss 0.0120228, acc 1
2016-09-06T00:47:27.653682: step 4861, loss 0.0634972, acc 0.98
2016-09-06T00:47:28.461013: step 4862, loss 0.0055014, acc 1
2016-09-06T00:47:29.304605: step 4863, loss 0.00323993, acc 1
2016-09-06T00:47:30.132739: step 4864, loss 0.0142447, acc 1
2016-09-06T00:47:30.957394: step 4865, loss 0.00465009, acc 1
2016-09-06T00:47:31.735689: step 4866, loss 0.147273, acc 0.96
2016-09-06T00:47:32.545805: step 4867, loss 0.0125463, acc 1
2016-09-06T00:47:33.341003: step 4868, loss 0.0274506, acc 1
2016-09-06T00:47:34.140097: step 4869, loss 0.038992, acc 0.98
2016-09-06T00:47:34.954939: step 4870, loss 0.0293594, acc 1
2016-09-06T00:47:35.788650: step 4871, loss 0.0187393, acc 1
2016-09-06T00:47:36.576630: step 4872, loss 0.00584277, acc 1
2016-09-06T00:47:37.359383: step 4873, loss 0.0126401, acc 1
2016-09-06T00:47:38.182929: step 4874, loss 0.0137607, acc 1
2016-09-06T00:47:39.033424: step 4875, loss 0.0254363, acc 0.98
2016-09-06T00:47:39.817633: step 4876, loss 0.0243524, acc 1
2016-09-06T00:47:40.630965: step 4877, loss 0.00771741, acc 1
2016-09-06T00:47:41.418234: step 4878, loss 0.0528539, acc 0.98
2016-09-06T00:47:42.247754: step 4879, loss 0.0374208, acc 0.98
2016-09-06T00:47:43.039985: step 4880, loss 0.0114148, acc 1
2016-09-06T00:47:43.845448: step 4881, loss 0.0571704, acc 0.98
2016-09-06T00:47:44.661702: step 4882, loss 0.016505, acc 1
2016-09-06T00:47:45.474991: step 4883, loss 0.0465049, acc 0.98
2016-09-06T00:47:46.270307: step 4884, loss 0.0210336, acc 1
2016-09-06T00:47:47.038799: step 4885, loss 0.0049865, acc 1
2016-09-06T00:47:47.858205: step 4886, loss 0.0726279, acc 0.94
2016-09-06T00:47:48.642534: step 4887, loss 0.0290343, acc 0.98
2016-09-06T00:47:49.449040: step 4888, loss 0.0145511, acc 1
2016-09-06T00:47:50.261509: step 4889, loss 0.0436677, acc 0.96
2016-09-06T00:47:51.051004: step 4890, loss 0.0139082, acc 1
2016-09-06T00:47:51.839252: step 4891, loss 0.0124958, acc 1
2016-09-06T00:47:52.653448: step 4892, loss 0.0110541, acc 1
2016-09-06T00:47:53.459553: step 4893, loss 0.00610059, acc 1
2016-09-06T00:47:54.258416: step 4894, loss 0.00303266, acc 1
2016-09-06T00:47:55.080677: step 4895, loss 0.0328893, acc 1
2016-09-06T00:47:55.863126: step 4896, loss 0.0298883, acc 0.98
2016-09-06T00:47:56.703401: step 4897, loss 0.00889735, acc 1
2016-09-06T00:47:57.505117: step 4898, loss 0.0586459, acc 0.98
2016-09-06T00:47:58.327687: step 4899, loss 0.0130701, acc 1
2016-09-06T00:47:59.179292: step 4900, loss 0.0345936, acc 0.98

Evaluation:
2016-09-06T00:48:02.948442: step 4900, loss 2.30205, acc 0.724203

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-4900

2016-09-06T00:48:04.822985: step 4901, loss 0.0343456, acc 0.98
2016-09-06T00:48:05.637140: step 4902, loss 0.00338891, acc 1
2016-09-06T00:48:06.462062: step 4903, loss 0.00554766, acc 1
2016-09-06T00:48:07.295387: step 4904, loss 0.0123136, acc 1
2016-09-06T00:48:08.087325: step 4905, loss 0.0618311, acc 0.96
2016-09-06T00:48:08.884705: step 4906, loss 0.125114, acc 0.98
2016-09-06T00:48:09.697781: step 4907, loss 0.0190185, acc 1
2016-09-06T00:48:10.465051: step 4908, loss 0.0204223, acc 0.98
2016-09-06T00:48:11.273082: step 4909, loss 0.0260037, acc 0.98
2016-09-06T00:48:12.068009: step 4910, loss 0.120595, acc 0.92
2016-09-06T00:48:12.872156: step 4911, loss 0.0391447, acc 0.98
2016-09-06T00:48:13.691010: step 4912, loss 0.0555375, acc 0.98
2016-09-06T00:48:14.531589: step 4913, loss 0.0294254, acc 0.98
2016-09-06T00:48:15.325814: step 4914, loss 0.0267377, acc 0.98
2016-09-06T00:48:16.120623: step 4915, loss 0.0848295, acc 0.98
2016-09-06T00:48:16.932032: step 4916, loss 0.0581054, acc 0.96
2016-09-06T00:48:17.744197: step 4917, loss 0.00700058, acc 1
2016-09-06T00:48:18.558524: step 4918, loss 0.0139203, acc 1
2016-09-06T00:48:19.372274: step 4919, loss 0.00930091, acc 1
2016-09-06T00:48:20.147778: step 4920, loss 0.0375265, acc 0.98
2016-09-06T00:48:20.960288: step 4921, loss 0.0252941, acc 1
2016-09-06T00:48:21.777936: step 4922, loss 0.0391032, acc 0.96
2016-09-06T00:48:22.561907: step 4923, loss 0.0143812, acc 1
2016-09-06T00:48:23.361036: step 4924, loss 0.0302202, acc 0.98
2016-09-06T00:48:24.158308: step 4925, loss 0.138912, acc 0.98
2016-09-06T00:48:24.968218: step 4926, loss 0.0159624, acc 1
2016-09-06T00:48:25.790877: step 4927, loss 0.0291453, acc 0.98
2016-09-06T00:48:26.611428: step 4928, loss 0.022801, acc 1
2016-09-06T00:48:27.381241: step 4929, loss 0.0544794, acc 0.96
2016-09-06T00:48:28.216123: step 4930, loss 0.0388042, acc 0.98
2016-09-06T00:48:29.020784: step 4931, loss 0.107054, acc 0.96
2016-09-06T00:48:29.804242: step 4932, loss 0.00326054, acc 1
2016-09-06T00:48:30.614645: step 4933, loss 0.0294156, acc 1
2016-09-06T00:48:31.410665: step 4934, loss 0.0320857, acc 1
2016-09-06T00:48:32.237447: step 4935, loss 0.0278631, acc 0.98
2016-09-06T00:48:33.057205: step 4936, loss 0.0193657, acc 0.98
2016-09-06T00:48:33.877434: step 4937, loss 0.0488266, acc 0.96
2016-09-06T00:48:34.674329: step 4938, loss 0.0912094, acc 0.98
2016-09-06T00:48:35.455722: step 4939, loss 0.00352926, acc 1
2016-09-06T00:48:36.277152: step 4940, loss 0.00787354, acc 1
2016-09-06T00:48:37.063316: step 4941, loss 0.0133556, acc 1
2016-09-06T00:48:37.891336: step 4942, loss 0.00395352, acc 1
2016-09-06T00:48:38.709908: step 4943, loss 0.0165146, acc 1
2016-09-06T00:48:39.511948: step 4944, loss 0.0737243, acc 0.98
2016-09-06T00:48:40.311996: step 4945, loss 0.0101281, acc 1
2016-09-06T00:48:41.122508: step 4946, loss 0.0128399, acc 1
2016-09-06T00:48:41.928972: step 4947, loss 0.0198545, acc 1
2016-09-06T00:48:42.717739: step 4948, loss 0.00876623, acc 1
2016-09-06T00:48:43.513403: step 4949, loss 0.00690526, acc 1
2016-09-06T00:48:44.337116: step 4950, loss 0.0258424, acc 1
2016-09-06T00:48:45.142323: step 4951, loss 0.0155846, acc 1
2016-09-06T00:48:45.950196: step 4952, loss 0.0109675, acc 1
2016-09-06T00:48:46.763383: step 4953, loss 0.00423332, acc 1
2016-09-06T00:48:47.560161: step 4954, loss 0.00529009, acc 1
2016-09-06T00:48:48.394689: step 4955, loss 0.0294935, acc 0.98
2016-09-06T00:48:49.186904: step 4956, loss 0.0203667, acc 1
2016-09-06T00:48:49.994871: step 4957, loss 0.0231553, acc 1
2016-09-06T00:48:50.834241: step 4958, loss 0.00599189, acc 1
2016-09-06T00:48:51.606444: step 4959, loss 0.030017, acc 1
2016-09-06T00:48:52.395024: step 4960, loss 0.0449562, acc 0.98
2016-09-06T00:48:53.208812: step 4961, loss 0.0524956, acc 0.96
2016-09-06T00:48:53.994132: step 4962, loss 0.0908266, acc 0.98
2016-09-06T00:48:54.810624: step 4963, loss 0.00330325, acc 1
2016-09-06T00:48:55.624266: step 4964, loss 0.00714311, acc 1
2016-09-06T00:48:56.408157: step 4965, loss 0.0223892, acc 0.98
2016-09-06T00:48:57.207003: step 4966, loss 0.0485032, acc 1
2016-09-06T00:48:58.010723: step 4967, loss 0.0226802, acc 1
2016-09-06T00:48:58.809836: step 4968, loss 0.00372011, acc 1
2016-09-06T00:48:59.634267: step 4969, loss 0.0324891, acc 0.98
2016-09-06T00:49:00.508045: step 4970, loss 0.13003, acc 0.96
2016-09-06T00:49:01.307546: step 4971, loss 0.0188131, acc 1
2016-09-06T00:49:02.088484: step 4972, loss 0.022695, acc 0.98
2016-09-06T00:49:02.884996: step 4973, loss 0.0103794, acc 1
2016-09-06T00:49:03.688380: step 4974, loss 0.0257989, acc 0.98
2016-09-06T00:49:04.476812: step 4975, loss 0.037666, acc 0.96
2016-09-06T00:49:05.291734: step 4976, loss 0.0172077, acc 1
2016-09-06T00:49:06.091536: step 4977, loss 0.00341936, acc 1
2016-09-06T00:49:06.889877: step 4978, loss 0.0563745, acc 0.98
2016-09-06T00:49:07.693762: step 4979, loss 0.0324111, acc 0.98
2016-09-06T00:49:08.492409: step 4980, loss 0.0316179, acc 0.98
2016-09-06T00:49:09.309193: step 4981, loss 0.00472369, acc 1
2016-09-06T00:49:10.133751: step 4982, loss 0.0388905, acc 0.98
2016-09-06T00:49:10.940889: step 4983, loss 0.00653462, acc 1
2016-09-06T00:49:11.756027: step 4984, loss 0.0586556, acc 0.98
2016-09-06T00:49:12.574473: step 4985, loss 0.051866, acc 0.96
2016-09-06T00:49:13.343987: step 4986, loss 0.0351531, acc 1
2016-09-06T00:49:14.159302: step 4987, loss 0.0163279, acc 1
2016-09-06T00:49:15.003385: step 4988, loss 0.0321025, acc 0.98
2016-09-06T00:49:15.782147: step 4989, loss 0.0450366, acc 0.96
2016-09-06T00:49:16.590557: step 4990, loss 0.102614, acc 0.98
2016-09-06T00:49:17.422790: step 4991, loss 0.0267091, acc 0.98
2016-09-06T00:49:18.144882: step 4992, loss 0.0469292, acc 0.954545
2016-09-06T00:49:19.003686: step 4993, loss 0.00515148, acc 1
2016-09-06T00:49:19.800923: step 4994, loss 0.0300233, acc 0.98
2016-09-06T00:49:20.601846: step 4995, loss 0.0180929, acc 1
2016-09-06T00:49:21.411679: step 4996, loss 0.0331949, acc 1
2016-09-06T00:49:22.253450: step 4997, loss 0.00681002, acc 1
2016-09-06T00:49:23.018664: step 4998, loss 0.0348771, acc 0.98
2016-09-06T00:49:23.822419: step 4999, loss 0.0107103, acc 1
2016-09-06T00:49:24.650871: step 5000, loss 0.0256561, acc 0.98

Evaluation:
2016-09-06T00:49:28.380102: step 5000, loss 2.08433, acc 0.725141

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-5000

2016-09-06T00:49:30.330829: step 5001, loss 0.052898, acc 0.98
2016-09-06T00:49:31.163364: step 5002, loss 0.104335, acc 0.98
2016-09-06T00:49:31.967308: step 5003, loss 0.0198834, acc 1
2016-09-06T00:49:32.747742: step 5004, loss 0.00851789, acc 1
2016-09-06T00:49:33.532879: step 5005, loss 0.0336098, acc 0.98
2016-09-06T00:49:34.368277: step 5006, loss 0.0204492, acc 0.98
2016-09-06T00:49:35.140595: step 5007, loss 0.00346177, acc 1
2016-09-06T00:49:35.926640: step 5008, loss 0.0454655, acc 0.98
2016-09-06T00:49:36.745896: step 5009, loss 0.00823346, acc 1
2016-09-06T00:49:37.547078: step 5010, loss 0.00917581, acc 1
2016-09-06T00:49:38.338667: step 5011, loss 0.00678359, acc 1
2016-09-06T00:49:39.184338: step 5012, loss 0.0508518, acc 0.98
2016-09-06T00:49:39.959842: step 5013, loss 0.0545213, acc 0.98
2016-09-06T00:49:40.763976: step 5014, loss 0.0433882, acc 0.98
2016-09-06T00:49:41.583434: step 5015, loss 0.0469142, acc 0.98
2016-09-06T00:49:42.382090: step 5016, loss 0.0258704, acc 0.98
2016-09-06T00:49:43.185861: step 5017, loss 0.0621146, acc 0.96
2016-09-06T00:49:43.995979: step 5018, loss 0.102419, acc 0.94
2016-09-06T00:49:44.759576: step 5019, loss 0.020404, acc 1
2016-09-06T00:49:45.574179: step 5020, loss 0.0204553, acc 1
2016-09-06T00:49:46.365785: step 5021, loss 0.106265, acc 0.98
2016-09-06T00:49:47.164634: step 5022, loss 0.0225465, acc 1
2016-09-06T00:49:47.984336: step 5023, loss 0.00770392, acc 1
2016-09-06T00:49:48.794855: step 5024, loss 0.0371674, acc 0.98
2016-09-06T00:49:49.591581: step 5025, loss 0.0231975, acc 0.98
2016-09-06T00:49:50.428065: step 5026, loss 0.017862, acc 1
2016-09-06T00:49:51.244881: step 5027, loss 0.00393541, acc 1
2016-09-06T00:49:52.055356: step 5028, loss 0.0259186, acc 1
2016-09-06T00:49:52.857880: step 5029, loss 0.00335455, acc 1
2016-09-06T00:49:53.665667: step 5030, loss 0.0288225, acc 0.98
2016-09-06T00:49:54.467511: step 5031, loss 0.0793167, acc 0.94
2016-09-06T00:49:55.329162: step 5032, loss 0.109127, acc 0.98
2016-09-06T00:49:56.185321: step 5033, loss 0.0297744, acc 0.98
2016-09-06T00:49:57.015880: step 5034, loss 0.0160152, acc 1
2016-09-06T00:49:57.819677: step 5035, loss 0.0595745, acc 0.98
2016-09-06T00:49:58.660669: step 5036, loss 0.0897593, acc 0.96
2016-09-06T00:49:59.476426: step 5037, loss 0.00539576, acc 1
2016-09-06T00:50:00.288403: step 5038, loss 0.096106, acc 0.94
2016-09-06T00:50:01.110755: step 5039, loss 0.042952, acc 0.98
2016-09-06T00:50:01.909472: step 5040, loss 0.00858543, acc 1
2016-09-06T00:50:02.718544: step 5041, loss 0.0692446, acc 0.96
2016-09-06T00:50:03.552724: step 5042, loss 0.0244137, acc 0.98
2016-09-06T00:50:04.389105: step 5043, loss 0.0263298, acc 1
2016-09-06T00:50:05.203361: step 5044, loss 0.0358214, acc 0.98
2016-09-06T00:50:06.023048: step 5045, loss 0.0311955, acc 1
2016-09-06T00:50:06.830914: step 5046, loss 0.0107019, acc 1
2016-09-06T00:50:07.632435: step 5047, loss 0.0263729, acc 1
2016-09-06T00:50:08.475967: step 5048, loss 0.0136795, acc 1
2016-09-06T00:50:09.287496: step 5049, loss 0.00453763, acc 1
2016-09-06T00:50:10.082413: step 5050, loss 0.0372279, acc 0.96
2016-09-06T00:50:10.947423: step 5051, loss 0.0360613, acc 0.98
2016-09-06T00:50:11.752052: step 5052, loss 0.0126926, acc 1
2016-09-06T00:50:12.556558: step 5053, loss 0.0046505, acc 1
2016-09-06T00:50:13.394574: step 5054, loss 0.0212218, acc 1
2016-09-06T00:50:14.229414: step 5055, loss 0.0169062, acc 1
2016-09-06T00:50:15.001622: step 5056, loss 0.0385095, acc 0.98
2016-09-06T00:50:15.788559: step 5057, loss 0.0254487, acc 1
2016-09-06T00:50:16.629530: step 5058, loss 0.0611822, acc 0.96
2016-09-06T00:50:17.435463: step 5059, loss 0.0211614, acc 0.98
2016-09-06T00:50:18.237748: step 5060, loss 0.084217, acc 0.96
2016-09-06T00:50:19.059078: step 5061, loss 0.0366564, acc 0.96
2016-09-06T00:50:19.870800: step 5062, loss 0.0219588, acc 1
2016-09-06T00:50:20.646269: step 5063, loss 0.00670596, acc 1
2016-09-06T00:50:21.502475: step 5064, loss 0.016187, acc 1
2016-09-06T00:50:22.262250: step 5065, loss 0.0163996, acc 1
2016-09-06T00:50:23.057699: step 5066, loss 0.0233952, acc 0.98
2016-09-06T00:50:23.877082: step 5067, loss 0.0522572, acc 0.96
2016-09-06T00:50:24.663799: step 5068, loss 0.0513344, acc 0.96
2016-09-06T00:50:25.451221: step 5069, loss 0.059008, acc 0.98
2016-09-06T00:50:26.255937: step 5070, loss 0.00670748, acc 1
2016-09-06T00:50:27.024246: step 5071, loss 0.0387054, acc 0.98
2016-09-06T00:50:27.839932: step 5072, loss 0.0324806, acc 0.98
2016-09-06T00:50:28.640951: step 5073, loss 0.00490518, acc 1
2016-09-06T00:50:29.439073: step 5074, loss 0.00444067, acc 1
2016-09-06T00:50:30.262980: step 5075, loss 0.0490465, acc 0.98
2016-09-06T00:50:31.073022: step 5076, loss 0.0047919, acc 1
2016-09-06T00:50:31.926939: step 5077, loss 0.0318245, acc 0.98
2016-09-06T00:50:32.727480: step 5078, loss 0.0384172, acc 0.98
2016-09-06T00:50:33.570932: step 5079, loss 0.00369102, acc 1
2016-09-06T00:50:34.342777: step 5080, loss 0.0246218, acc 1
2016-09-06T00:50:35.145293: step 5081, loss 0.00466401, acc 1
2016-09-06T00:50:35.954387: step 5082, loss 0.0199069, acc 1
2016-09-06T00:50:36.733551: step 5083, loss 0.0457823, acc 0.96
2016-09-06T00:50:37.550332: step 5084, loss 0.01173, acc 1
2016-09-06T00:50:38.365925: step 5085, loss 0.0140286, acc 1
2016-09-06T00:50:39.151411: step 5086, loss 0.00466631, acc 1
2016-09-06T00:50:39.990714: step 5087, loss 0.0138523, acc 1
2016-09-06T00:50:40.835729: step 5088, loss 0.00303777, acc 1
2016-09-06T00:50:41.638082: step 5089, loss 0.00493062, acc 1
2016-09-06T00:50:42.436363: step 5090, loss 0.00353235, acc 1
2016-09-06T00:50:43.291592: step 5091, loss 0.0213626, acc 1
2016-09-06T00:50:44.100014: step 5092, loss 0.0169085, acc 1
2016-09-06T00:50:44.898587: step 5093, loss 0.00661156, acc 1
2016-09-06T00:50:45.697109: step 5094, loss 0.0177119, acc 1
2016-09-06T00:50:46.482853: step 5095, loss 0.00979097, acc 1
2016-09-06T00:50:47.266331: step 5096, loss 0.00692304, acc 1
2016-09-06T00:50:48.088638: step 5097, loss 0.0242147, acc 1
2016-09-06T00:50:48.866723: step 5098, loss 0.096495, acc 0.94
2016-09-06T00:50:49.664693: step 5099, loss 0.0546986, acc 0.96
2016-09-06T00:50:50.510762: step 5100, loss 0.00358788, acc 1

Evaluation:
2016-09-06T00:50:54.180866: step 5100, loss 2.47332, acc 0.717636

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-5100

2016-09-06T00:50:56.056861: step 5101, loss 0.0362049, acc 0.98
2016-09-06T00:50:56.924488: step 5102, loss 0.0191578, acc 1
2016-09-06T00:50:57.761679: step 5103, loss 0.00282074, acc 1
2016-09-06T00:50:58.568168: step 5104, loss 0.0294919, acc 1
2016-09-06T00:50:59.410424: step 5105, loss 0.00317924, acc 1
2016-09-06T00:51:00.228526: step 5106, loss 0.00346364, acc 1
2016-09-06T00:51:01.030583: step 5107, loss 0.0325871, acc 0.98
2016-09-06T00:51:01.851602: step 5108, loss 0.0157185, acc 1
2016-09-06T00:51:02.692874: step 5109, loss 0.0142387, acc 1
2016-09-06T00:51:03.459005: step 5110, loss 0.00484141, acc 1
2016-09-06T00:51:04.266603: step 5111, loss 0.00831778, acc 1
2016-09-06T00:51:05.111277: step 5112, loss 0.00452307, acc 1
2016-09-06T00:51:05.877094: step 5113, loss 0.0233303, acc 0.98
2016-09-06T00:51:06.697619: step 5114, loss 0.00377084, acc 1
2016-09-06T00:51:07.520031: step 5115, loss 0.0256705, acc 0.98
2016-09-06T00:51:08.294382: step 5116, loss 0.0103628, acc 1
2016-09-06T00:51:09.107921: step 5117, loss 0.0155364, acc 1
2016-09-06T00:51:09.909656: step 5118, loss 0.0510015, acc 0.96
2016-09-06T00:51:10.735320: step 5119, loss 0.0208166, acc 0.98
2016-09-06T00:51:11.538051: step 5120, loss 0.00898349, acc 1
2016-09-06T00:51:12.360207: step 5121, loss 0.0212704, acc 1
2016-09-06T00:51:13.129974: step 5122, loss 0.0348824, acc 0.98
2016-09-06T00:51:13.927473: step 5123, loss 0.0184028, acc 1
2016-09-06T00:51:14.723783: step 5124, loss 0.0492278, acc 0.98
2016-09-06T00:51:15.505466: step 5125, loss 0.0450016, acc 0.98
2016-09-06T00:51:16.308314: step 5126, loss 0.0436303, acc 0.98
2016-09-06T00:51:17.111414: step 5127, loss 0.0348652, acc 0.98
2016-09-06T00:51:17.901666: step 5128, loss 0.0230767, acc 1
2016-09-06T00:51:18.733699: step 5129, loss 0.0226835, acc 0.98
2016-09-06T00:51:19.544421: step 5130, loss 0.0417452, acc 0.96
2016-09-06T00:51:20.340438: step 5131, loss 0.0064047, acc 1
2016-09-06T00:51:21.141860: step 5132, loss 0.00262886, acc 1
2016-09-06T00:51:21.961364: step 5133, loss 0.0638101, acc 0.96
2016-09-06T00:51:22.738739: step 5134, loss 0.0159719, acc 1
2016-09-06T00:51:23.559988: step 5135, loss 0.0561067, acc 0.98
2016-09-06T00:51:24.381813: step 5136, loss 0.0528186, acc 0.96
2016-09-06T00:51:25.194186: step 5137, loss 0.0506155, acc 0.96
2016-09-06T00:51:25.982314: step 5138, loss 0.0085185, acc 1
2016-09-06T00:51:26.818871: step 5139, loss 0.00629957, acc 1
2016-09-06T00:51:27.615982: step 5140, loss 0.00487647, acc 1
2016-09-06T00:51:28.441146: step 5141, loss 0.031911, acc 0.98
2016-09-06T00:51:29.244691: step 5142, loss 0.0919483, acc 0.96
2016-09-06T00:51:30.061455: step 5143, loss 0.0109338, acc 1
2016-09-06T00:51:30.869956: step 5144, loss 0.00593148, acc 1
2016-09-06T00:51:31.671818: step 5145, loss 0.00816671, acc 1
2016-09-06T00:51:32.445408: step 5146, loss 0.0700245, acc 0.98
2016-09-06T00:51:33.275121: step 5147, loss 0.00267115, acc 1
2016-09-06T00:51:34.113978: step 5148, loss 0.0841214, acc 0.98
2016-09-06T00:51:34.910717: step 5149, loss 0.146579, acc 0.98
2016-09-06T00:51:35.686740: step 5150, loss 0.032067, acc 0.98
2016-09-06T00:51:36.515055: step 5151, loss 0.0130063, acc 1
2016-09-06T00:51:37.292727: step 5152, loss 0.0525834, acc 0.96
2016-09-06T00:51:38.108346: step 5153, loss 0.0628088, acc 0.98
2016-09-06T00:51:38.899962: step 5154, loss 0.0107141, acc 1
2016-09-06T00:51:39.690434: step 5155, loss 0.0117683, acc 1
2016-09-06T00:51:40.490114: step 5156, loss 0.0391297, acc 1
2016-09-06T00:51:41.310736: step 5157, loss 0.00791507, acc 1
2016-09-06T00:51:42.103283: step 5158, loss 0.0250742, acc 1
2016-09-06T00:51:42.950124: step 5159, loss 0.0466131, acc 0.96
2016-09-06T00:51:43.768228: step 5160, loss 0.0225839, acc 0.98
2016-09-06T00:51:44.538889: step 5161, loss 0.120962, acc 0.94
2016-09-06T00:51:45.352775: step 5162, loss 0.00516031, acc 1
2016-09-06T00:51:46.179944: step 5163, loss 0.0678913, acc 0.98
2016-09-06T00:51:46.960494: step 5164, loss 0.0414263, acc 0.98
2016-09-06T00:51:47.773672: step 5165, loss 0.0213642, acc 0.98
2016-09-06T00:51:48.568469: step 5166, loss 0.0212995, acc 0.98
2016-09-06T00:51:49.376059: step 5167, loss 0.0362777, acc 0.98
2016-09-06T00:51:50.189705: step 5168, loss 0.0175839, acc 1
2016-09-06T00:51:51.001383: step 5169, loss 0.031367, acc 1
2016-09-06T00:51:51.768316: step 5170, loss 0.0602401, acc 0.98
2016-09-06T00:51:52.562085: step 5171, loss 0.00611312, acc 1
2016-09-06T00:51:53.374325: step 5172, loss 0.0503644, acc 0.98
2016-09-06T00:51:54.172580: step 5173, loss 0.00519623, acc 1
2016-09-06T00:51:55.015439: step 5174, loss 0.0226142, acc 1
2016-09-06T00:51:55.842754: step 5175, loss 0.00637842, acc 1
2016-09-06T00:51:56.627953: step 5176, loss 0.0196058, acc 0.98
2016-09-06T00:51:57.449452: step 5177, loss 0.0278383, acc 0.98
2016-09-06T00:51:58.249405: step 5178, loss 0.030658, acc 0.98
2016-09-06T00:51:59.041893: step 5179, loss 0.0101784, acc 1
2016-09-06T00:51:59.878476: step 5180, loss 0.00384564, acc 1
2016-09-06T00:52:00.712287: step 5181, loss 0.0435181, acc 0.98
2016-09-06T00:52:01.529874: step 5182, loss 0.0425122, acc 0.96
2016-09-06T00:52:02.348440: step 5183, loss 0.0211005, acc 1
2016-09-06T00:52:03.106143: step 5184, loss 0.024049, acc 0.977273
2016-09-06T00:52:03.880208: step 5185, loss 0.0116769, acc 1
2016-09-06T00:52:04.670189: step 5186, loss 0.0181817, acc 1
2016-09-06T00:52:05.522049: step 5187, loss 0.00790946, acc 1
2016-09-06T00:52:06.283979: step 5188, loss 0.0170429, acc 1
2016-09-06T00:52:07.072489: step 5189, loss 0.0040586, acc 1
2016-09-06T00:52:07.894631: step 5190, loss 0.00450392, acc 1
2016-09-06T00:52:08.682540: step 5191, loss 0.0178121, acc 1
2016-09-06T00:52:09.505128: step 5192, loss 0.0580229, acc 0.98
2016-09-06T00:52:10.323896: step 5193, loss 0.0333498, acc 0.98
2016-09-06T00:52:11.131366: step 5194, loss 0.0272784, acc 0.98
2016-09-06T00:52:11.923708: step 5195, loss 0.0288913, acc 0.98
2016-09-06T00:52:12.744228: step 5196, loss 0.0116233, acc 1
2016-09-06T00:52:13.548591: step 5197, loss 0.0040062, acc 1
2016-09-06T00:52:14.373812: step 5198, loss 0.0122391, acc 1
2016-09-06T00:52:15.193195: step 5199, loss 0.042906, acc 0.98
2016-09-06T00:52:15.959775: step 5200, loss 0.023859, acc 1

Evaluation:
2016-09-06T00:52:19.655753: step 5200, loss 2.80013, acc 0.742964

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-5200

2016-09-06T00:52:21.501687: step 5201, loss 0.00383917, acc 1
2016-09-06T00:52:22.331617: step 5202, loss 0.00378552, acc 1
2016-09-06T00:52:23.145400: step 5203, loss 0.0754917, acc 0.96
2016-09-06T00:52:23.979650: step 5204, loss 0.0261792, acc 0.98
2016-09-06T00:52:24.765816: step 5205, loss 0.014368, acc 1
2016-09-06T00:52:25.569453: step 5206, loss 0.0174493, acc 1
2016-09-06T00:52:26.398733: step 5207, loss 0.0354071, acc 0.98
2016-09-06T00:52:27.204923: step 5208, loss 0.0269342, acc 0.98
2016-09-06T00:52:28.032029: step 5209, loss 0.0110979, acc 1
2016-09-06T00:52:28.863907: step 5210, loss 0.0635157, acc 0.96
2016-09-06T00:52:29.681696: step 5211, loss 0.0214216, acc 1
2016-09-06T00:52:30.514789: step 5212, loss 0.0374367, acc 0.96
2016-09-06T00:52:31.333106: step 5213, loss 0.0522716, acc 0.98
2016-09-06T00:52:32.155135: step 5214, loss 0.0101489, acc 1
2016-09-06T00:52:33.024544: step 5215, loss 0.00887496, acc 1
2016-09-06T00:52:33.855958: step 5216, loss 0.00981636, acc 1
2016-09-06T00:52:34.683514: step 5217, loss 0.0628406, acc 0.96
2016-09-06T00:52:35.471541: step 5218, loss 0.0228892, acc 1
2016-09-06T00:52:36.265827: step 5219, loss 0.0128525, acc 1
2016-09-06T00:52:37.065640: step 5220, loss 0.0682838, acc 0.96
2016-09-06T00:52:37.867135: step 5221, loss 0.026219, acc 0.98
2016-09-06T00:52:38.682955: step 5222, loss 0.0264069, acc 0.98
2016-09-06T00:52:39.474364: step 5223, loss 0.0171736, acc 1
2016-09-06T00:52:40.235665: step 5224, loss 0.012467, acc 1
2016-09-06T00:52:41.046860: step 5225, loss 0.0205862, acc 0.98
2016-09-06T00:52:41.857485: step 5226, loss 0.0137857, acc 1
2016-09-06T00:52:42.648256: step 5227, loss 0.0222603, acc 1
2016-09-06T00:52:43.450000: step 5228, loss 0.00434332, acc 1
2016-09-06T00:52:44.281956: step 5229, loss 0.00509253, acc 1
2016-09-06T00:52:45.086641: step 5230, loss 0.014403, acc 1
2016-09-06T00:52:45.897405: step 5231, loss 0.00508819, acc 1
2016-09-06T00:52:46.727284: step 5232, loss 0.00533658, acc 1
2016-09-06T00:52:47.491030: step 5233, loss 0.0438132, acc 0.96
2016-09-06T00:52:48.273595: step 5234, loss 0.0124205, acc 1
2016-09-06T00:52:49.113851: step 5235, loss 0.0341947, acc 0.98
2016-09-06T00:52:49.946146: step 5236, loss 0.0356251, acc 0.98
2016-09-06T00:52:50.776335: step 5237, loss 0.0327137, acc 0.98
2016-09-06T00:52:51.600573: step 5238, loss 0.0356569, acc 0.98
2016-09-06T00:52:52.405668: step 5239, loss 0.171989, acc 0.98
2016-09-06T00:52:53.212319: step 5240, loss 0.0208894, acc 0.98
2016-09-06T00:52:54.023234: step 5241, loss 0.0117422, acc 1
2016-09-06T00:52:54.823680: step 5242, loss 0.0557339, acc 0.98
2016-09-06T00:52:55.626326: step 5243, loss 0.00329119, acc 1
2016-09-06T00:52:56.440226: step 5244, loss 0.122987, acc 0.98
2016-09-06T00:52:57.222712: step 5245, loss 0.0228701, acc 0.98
2016-09-06T00:52:58.007986: step 5246, loss 0.0157249, acc 1
2016-09-06T00:52:58.831355: step 5247, loss 0.00396084, acc 1
2016-09-06T00:52:59.606000: step 5248, loss 0.0820705, acc 0.98
2016-09-06T00:53:00.406109: step 5249, loss 0.00310887, acc 1
2016-09-06T00:53:01.261341: step 5250, loss 0.00639355, acc 1
2016-09-06T00:53:02.041587: step 5251, loss 0.0152475, acc 1
2016-09-06T00:53:02.846931: step 5252, loss 0.0502676, acc 0.98
2016-09-06T00:53:03.656659: step 5253, loss 0.0407002, acc 0.98
2016-09-06T00:53:04.420915: step 5254, loss 0.0176377, acc 1
2016-09-06T00:53:05.222485: step 5255, loss 0.0460148, acc 0.96
2016-09-06T00:53:06.035996: step 5256, loss 0.0162818, acc 1
2016-09-06T00:53:06.828367: step 5257, loss 0.047752, acc 0.96
2016-09-06T00:53:07.628259: step 5258, loss 0.00440396, acc 1
2016-09-06T00:53:08.449594: step 5259, loss 0.0184858, acc 1
2016-09-06T00:53:09.252828: step 5260, loss 0.0186077, acc 0.98
2016-09-06T00:53:10.064594: step 5261, loss 0.0198366, acc 0.98
2016-09-06T00:53:10.893398: step 5262, loss 0.0619542, acc 0.98
2016-09-06T00:53:11.666353: step 5263, loss 0.0635039, acc 0.98
2016-09-06T00:53:12.445292: step 5264, loss 0.198724, acc 0.98
2016-09-06T00:53:13.254363: step 5265, loss 0.0154935, acc 1
2016-09-06T00:53:14.054440: step 5266, loss 0.0118952, acc 1
2016-09-06T00:53:14.858596: step 5267, loss 0.0377225, acc 1
2016-09-06T00:53:15.646174: step 5268, loss 0.0743864, acc 0.98
2016-09-06T00:53:16.438967: step 5269, loss 0.0526279, acc 0.98
2016-09-06T00:53:17.269788: step 5270, loss 0.0221258, acc 0.98
2016-09-06T00:53:18.111420: step 5271, loss 0.0107084, acc 1
2016-09-06T00:53:18.908619: step 5272, loss 0.0255799, acc 1
2016-09-06T00:53:19.716357: step 5273, loss 0.0330449, acc 0.98
2016-09-06T00:53:20.520190: step 5274, loss 0.0153716, acc 1
2016-09-06T00:53:21.328496: step 5275, loss 0.0194361, acc 1
2016-09-06T00:53:22.154158: step 5276, loss 0.00825582, acc 1
2016-09-06T00:53:22.990121: step 5277, loss 0.0395695, acc 0.98
2016-09-06T00:53:23.768122: step 5278, loss 0.0608858, acc 0.98
2016-09-06T00:53:24.568081: step 5279, loss 0.0766311, acc 0.96
2016-09-06T00:53:25.379607: step 5280, loss 0.0110277, acc 1
2016-09-06T00:53:26.205636: step 5281, loss 0.00694998, acc 1
2016-09-06T00:53:26.989448: step 5282, loss 0.032672, acc 1
2016-09-06T00:53:27.815022: step 5283, loss 0.0166338, acc 1
2016-09-06T00:53:28.601338: step 5284, loss 0.0444224, acc 0.98
2016-09-06T00:53:29.414034: step 5285, loss 0.0147983, acc 1
2016-09-06T00:53:30.258408: step 5286, loss 0.0183987, acc 1
2016-09-06T00:53:31.034394: step 5287, loss 0.0204783, acc 1
2016-09-06T00:53:31.808448: step 5288, loss 0.00418505, acc 1
2016-09-06T00:53:32.654944: step 5289, loss 0.0603353, acc 0.98
2016-09-06T00:53:33.429070: step 5290, loss 0.0336709, acc 0.98
2016-09-06T00:53:34.234346: step 5291, loss 0.0685087, acc 0.96
2016-09-06T00:53:35.061333: step 5292, loss 0.0425938, acc 0.98
2016-09-06T00:53:35.830192: step 5293, loss 0.0471178, acc 0.98
2016-09-06T00:53:36.626852: step 5294, loss 0.056798, acc 0.98
2016-09-06T00:53:37.431688: step 5295, loss 0.0296222, acc 0.98
2016-09-06T00:53:38.217447: step 5296, loss 0.0745108, acc 0.98
2016-09-06T00:53:39.027887: step 5297, loss 0.0234067, acc 1
2016-09-06T00:53:39.842060: step 5298, loss 0.0546933, acc 0.96
2016-09-06T00:53:40.655267: step 5299, loss 0.0143699, acc 1
2016-09-06T00:53:41.464156: step 5300, loss 0.0184242, acc 1

Evaluation:
2016-09-06T00:53:45.199841: step 5300, loss 2.07752, acc 0.727955

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-5300

2016-09-06T00:53:47.182443: step 5301, loss 0.01757, acc 1
2016-09-06T00:53:47.968763: step 5302, loss 0.0114034, acc 1
2016-09-06T00:53:48.800464: step 5303, loss 0.0412587, acc 1
2016-09-06T00:53:49.595939: step 5304, loss 0.0479136, acc 0.98
2016-09-06T00:53:50.406354: step 5305, loss 0.00848963, acc 1
2016-09-06T00:53:51.223766: step 5306, loss 0.00550945, acc 1
2016-09-06T00:53:52.023890: step 5307, loss 0.00660882, acc 1
2016-09-06T00:53:52.845229: step 5308, loss 0.00653968, acc 1
2016-09-06T00:53:53.670533: step 5309, loss 0.0284034, acc 0.98
2016-09-06T00:53:54.507174: step 5310, loss 0.011264, acc 1
2016-09-06T00:53:55.307457: step 5311, loss 0.0175827, acc 1
2016-09-06T00:53:56.135438: step 5312, loss 0.0410492, acc 0.98
2016-09-06T00:53:56.952516: step 5313, loss 0.0300399, acc 0.98
2016-09-06T00:53:57.762753: step 5314, loss 0.0249727, acc 1
2016-09-06T00:53:58.560573: step 5315, loss 0.00985491, acc 1
2016-09-06T00:53:59.343146: step 5316, loss 0.00609025, acc 1
2016-09-06T00:54:00.113613: step 5317, loss 0.0139746, acc 1
2016-09-06T00:54:00.975231: step 5318, loss 0.00970131, acc 1
2016-09-06T00:54:01.780677: step 5319, loss 0.0532151, acc 0.98
2016-09-06T00:54:02.546530: step 5320, loss 0.00582707, acc 1
2016-09-06T00:54:03.344613: step 5321, loss 0.0245395, acc 0.98
2016-09-06T00:54:04.160944: step 5322, loss 0.0785225, acc 0.96
2016-09-06T00:54:04.968277: step 5323, loss 0.00460761, acc 1
2016-09-06T00:54:05.784725: step 5324, loss 0.00932815, acc 1
2016-09-06T00:54:06.624691: step 5325, loss 0.062499, acc 0.98
2016-09-06T00:54:07.413574: step 5326, loss 0.00679225, acc 1
2016-09-06T00:54:08.200210: step 5327, loss 0.00487352, acc 1
2016-09-06T00:54:09.037841: step 5328, loss 0.0377114, acc 0.98
2016-09-06T00:54:09.806502: step 5329, loss 0.0429279, acc 0.96
2016-09-06T00:54:10.642173: step 5330, loss 0.00681111, acc 1
2016-09-06T00:54:11.433242: step 5331, loss 0.0124941, acc 1
2016-09-06T00:54:12.208948: step 5332, loss 0.0126593, acc 1
2016-09-06T00:54:13.005712: step 5333, loss 0.029383, acc 0.98
2016-09-06T00:54:13.834968: step 5334, loss 0.0112672, acc 1
2016-09-06T00:54:14.614951: step 5335, loss 0.024816, acc 0.98
2016-09-06T00:54:15.431250: step 5336, loss 0.00369062, acc 1
2016-09-06T00:54:16.257810: step 5337, loss 0.0236719, acc 0.98
2016-09-06T00:54:17.049159: step 5338, loss 0.0134335, acc 1
2016-09-06T00:54:17.837424: step 5339, loss 0.0272554, acc 0.98
2016-09-06T00:54:18.652304: step 5340, loss 0.0332412, acc 0.98
2016-09-06T00:54:19.431133: step 5341, loss 0.0171382, acc 1
2016-09-06T00:54:20.244601: step 5342, loss 0.0338667, acc 0.98
2016-09-06T00:54:21.061694: step 5343, loss 0.0231691, acc 0.98
2016-09-06T00:54:21.852877: step 5344, loss 0.0151754, acc 1
2016-09-06T00:54:22.669282: step 5345, loss 0.0346787, acc 1
2016-09-06T00:54:23.485051: step 5346, loss 0.0295131, acc 0.98
2016-09-06T00:54:24.274031: step 5347, loss 0.00826651, acc 1
2016-09-06T00:54:25.092493: step 5348, loss 0.0115639, acc 1
2016-09-06T00:54:25.915842: step 5349, loss 0.00372708, acc 1
2016-09-06T00:54:26.720198: step 5350, loss 0.0179324, acc 1
2016-09-06T00:54:27.559274: step 5351, loss 0.0133935, acc 1
2016-09-06T00:54:28.367662: step 5352, loss 0.0140028, acc 1
2016-09-06T00:54:29.152112: step 5353, loss 0.0667453, acc 0.98
2016-09-06T00:54:29.985841: step 5354, loss 0.00419775, acc 1
2016-09-06T00:54:30.798620: step 5355, loss 0.00375407, acc 1
2016-09-06T00:54:31.560321: step 5356, loss 0.0573172, acc 0.98
2016-09-06T00:54:32.361639: step 5357, loss 0.00888836, acc 1
2016-09-06T00:54:33.171242: step 5358, loss 0.029779, acc 0.98
2016-09-06T00:54:33.965938: step 5359, loss 0.0297425, acc 0.98
2016-09-06T00:54:34.756813: step 5360, loss 0.00660876, acc 1
2016-09-06T00:54:35.570232: step 5361, loss 0.0146308, acc 1
2016-09-06T00:54:36.346474: step 5362, loss 0.0572171, acc 0.98
2016-09-06T00:54:37.159001: step 5363, loss 0.016527, acc 1
2016-09-06T00:54:37.978265: step 5364, loss 0.0177624, acc 1
2016-09-06T00:54:38.758239: step 5365, loss 0.0110323, acc 1
2016-09-06T00:54:39.594125: step 5366, loss 0.00550199, acc 1
2016-09-06T00:54:40.404570: step 5367, loss 0.0822342, acc 0.98
2016-09-06T00:54:41.222282: step 5368, loss 0.0121108, acc 1
2016-09-06T00:54:42.033899: step 5369, loss 0.0408663, acc 0.98
2016-09-06T00:54:42.853019: step 5370, loss 0.0162227, acc 1
2016-09-06T00:54:43.669293: step 5371, loss 0.153252, acc 0.96
2016-09-06T00:54:44.466167: step 5372, loss 0.0190475, acc 1
2016-09-06T00:54:45.278222: step 5373, loss 0.0296991, acc 0.98
2016-09-06T00:54:46.040193: step 5374, loss 0.00296949, acc 1
2016-09-06T00:54:46.863001: step 5375, loss 0.0598351, acc 0.96
2016-09-06T00:54:47.617652: step 5376, loss 0.00536197, acc 1
2016-09-06T00:54:48.409418: step 5377, loss 0.00463273, acc 1
2016-09-06T00:54:49.230851: step 5378, loss 0.0173884, acc 1
2016-09-06T00:54:50.032632: step 5379, loss 0.00643879, acc 1
2016-09-06T00:54:50.835051: step 5380, loss 0.0224925, acc 1
2016-09-06T00:54:51.673071: step 5381, loss 0.00556539, acc 1
2016-09-06T00:54:52.505503: step 5382, loss 0.0126286, acc 1
2016-09-06T00:54:53.297955: step 5383, loss 0.0045436, acc 1
2016-09-06T00:54:54.144931: step 5384, loss 0.0210829, acc 1
2016-09-06T00:54:54.959450: step 5385, loss 0.0488028, acc 0.96
2016-09-06T00:54:55.760414: step 5386, loss 0.00766631, acc 1
2016-09-06T00:54:56.541428: step 5387, loss 0.0273433, acc 0.98
2016-09-06T00:54:57.383826: step 5388, loss 0.00918765, acc 1
2016-09-06T00:54:58.177261: step 5389, loss 0.0246799, acc 1
2016-09-06T00:54:58.980137: step 5390, loss 0.00352976, acc 1
2016-09-06T00:54:59.770177: step 5391, loss 0.0146228, acc 1
2016-09-06T00:55:00.547626: step 5392, loss 0.0252875, acc 1
2016-09-06T00:55:01.367803: step 5393, loss 0.00919139, acc 1
2016-09-06T00:55:02.188683: step 5394, loss 0.0280975, acc 0.98
2016-09-06T00:55:02.950305: step 5395, loss 0.00707768, acc 1
2016-09-06T00:55:03.746022: step 5396, loss 0.00331537, acc 1
2016-09-06T00:55:04.554972: step 5397, loss 0.00313656, acc 1
2016-09-06T00:55:05.329412: step 5398, loss 0.0664301, acc 0.96
2016-09-06T00:55:06.138063: step 5399, loss 0.027211, acc 0.98
2016-09-06T00:55:06.947576: step 5400, loss 0.00463657, acc 1

Evaluation:
2016-09-06T00:55:10.668662: step 5400, loss 2.24114, acc 0.725141

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-5400

2016-09-06T00:55:12.533502: step 5401, loss 0.104566, acc 0.96
2016-09-06T00:55:13.373653: step 5402, loss 0.0192919, acc 1
2016-09-06T00:55:14.186146: step 5403, loss 0.046708, acc 0.98
2016-09-06T00:55:15.019857: step 5404, loss 0.0352476, acc 1
2016-09-06T00:55:15.870111: step 5405, loss 0.00879769, acc 1
2016-09-06T00:55:16.711942: step 5406, loss 0.0522756, acc 0.96
2016-09-06T00:55:17.521390: step 5407, loss 0.020217, acc 0.98
2016-09-06T00:55:18.294912: step 5408, loss 0.0277894, acc 0.98
2016-09-06T00:55:19.124139: step 5409, loss 0.0741371, acc 0.98
2016-09-06T00:55:19.908694: step 5410, loss 0.0685586, acc 0.98
2016-09-06T00:55:20.703552: step 5411, loss 0.0359486, acc 0.98
2016-09-06T00:55:21.516964: step 5412, loss 0.0302361, acc 1
2016-09-06T00:55:22.306536: step 5413, loss 0.0367717, acc 0.98
2016-09-06T00:55:23.125404: step 5414, loss 0.0200138, acc 0.98
2016-09-06T00:55:23.934966: step 5415, loss 0.0328359, acc 0.98
2016-09-06T00:55:24.741464: step 5416, loss 0.00674764, acc 1
2016-09-06T00:55:25.538313: step 5417, loss 0.0254017, acc 1
2016-09-06T00:55:26.339023: step 5418, loss 0.0437135, acc 0.96
2016-09-06T00:55:27.154216: step 5419, loss 0.106588, acc 0.96
2016-09-06T00:55:27.943887: step 5420, loss 0.00363234, acc 1
2016-09-06T00:55:28.760675: step 5421, loss 0.00329084, acc 1
2016-09-06T00:55:29.556558: step 5422, loss 0.00881079, acc 1
2016-09-06T00:55:30.355360: step 5423, loss 0.0169481, acc 1
2016-09-06T00:55:31.137624: step 5424, loss 0.0180094, acc 1
2016-09-06T00:55:31.938298: step 5425, loss 0.0531116, acc 0.96
2016-09-06T00:55:32.765935: step 5426, loss 0.151045, acc 0.98
2016-09-06T00:55:33.589443: step 5427, loss 0.00643495, acc 1
2016-09-06T00:55:34.386859: step 5428, loss 0.0119517, acc 1
2016-09-06T00:55:35.217796: step 5429, loss 0.0409065, acc 0.98
2016-09-06T00:55:36.069501: step 5430, loss 0.133064, acc 0.96
2016-09-06T00:55:36.888516: step 5431, loss 0.00431039, acc 1
2016-09-06T00:55:37.707113: step 5432, loss 0.0288612, acc 1
2016-09-06T00:55:38.523757: step 5433, loss 0.0227536, acc 0.98
2016-09-06T00:55:39.294894: step 5434, loss 0.0379163, acc 0.98
2016-09-06T00:55:40.081942: step 5435, loss 0.0161091, acc 1
2016-09-06T00:55:40.885039: step 5436, loss 0.0687156, acc 0.98
2016-09-06T00:55:41.659505: step 5437, loss 0.0247411, acc 1
2016-09-06T00:55:42.450592: step 5438, loss 0.0757146, acc 0.98
2016-09-06T00:55:43.288530: step 5439, loss 0.00524359, acc 1
2016-09-06T00:55:44.072004: step 5440, loss 0.114015, acc 0.98
2016-09-06T00:55:44.856106: step 5441, loss 0.0191176, acc 1
2016-09-06T00:55:45.684885: step 5442, loss 0.016366, acc 1
2016-09-06T00:55:46.487293: step 5443, loss 0.0202123, acc 1
2016-09-06T00:55:47.312674: step 5444, loss 0.0819973, acc 0.98
2016-09-06T00:55:48.117701: step 5445, loss 0.0100412, acc 1
2016-09-06T00:55:48.923960: step 5446, loss 0.00733641, acc 1
2016-09-06T00:55:49.736103: step 5447, loss 0.0109289, acc 1
2016-09-06T00:55:50.545513: step 5448, loss 0.0330116, acc 0.98
2016-09-06T00:55:51.330212: step 5449, loss 0.0736809, acc 0.96
2016-09-06T00:55:52.127252: step 5450, loss 0.0227363, acc 1
2016-09-06T00:55:52.942399: step 5451, loss 0.0365838, acc 0.98
2016-09-06T00:55:53.748870: step 5452, loss 0.0251701, acc 1
2016-09-06T00:55:54.555198: step 5453, loss 0.0236172, acc 0.98
2016-09-06T00:55:55.390019: step 5454, loss 0.0175092, acc 1
2016-09-06T00:55:56.180338: step 5455, loss 0.00518489, acc 1
2016-09-06T00:55:56.972938: step 5456, loss 0.0535171, acc 0.98
2016-09-06T00:55:57.799889: step 5457, loss 0.0123111, acc 1
2016-09-06T00:55:58.579715: step 5458, loss 0.0259623, acc 0.98
2016-09-06T00:55:59.359107: step 5459, loss 0.0571898, acc 0.98
2016-09-06T00:56:00.180733: step 5460, loss 0.00843061, acc 1
2016-09-06T00:56:00.966152: step 5461, loss 0.00684265, acc 1
2016-09-06T00:56:01.775278: step 5462, loss 0.0165725, acc 1
2016-09-06T00:56:02.563673: step 5463, loss 0.00515945, acc 1
2016-09-06T00:56:03.352581: step 5464, loss 0.0128671, acc 1
2016-09-06T00:56:04.173976: step 5465, loss 0.0375094, acc 0.98
2016-09-06T00:56:04.965470: step 5466, loss 0.0510307, acc 0.98
2016-09-06T00:56:05.784126: step 5467, loss 0.0380869, acc 0.98
2016-09-06T00:56:06.606983: step 5468, loss 0.0530276, acc 0.98
2016-09-06T00:56:07.440455: step 5469, loss 0.0876307, acc 0.96
2016-09-06T00:56:08.253804: step 5470, loss 0.0279284, acc 0.98
2016-09-06T00:56:09.045072: step 5471, loss 0.0289778, acc 1
2016-09-06T00:56:09.845005: step 5472, loss 0.0157787, acc 1
2016-09-06T00:56:10.630031: step 5473, loss 0.0215565, acc 1
2016-09-06T00:56:11.452633: step 5474, loss 0.0193587, acc 1
2016-09-06T00:56:12.278923: step 5475, loss 0.00493745, acc 1
2016-09-06T00:56:13.071087: step 5476, loss 0.00744221, acc 1
2016-09-06T00:56:13.864999: step 5477, loss 0.0498043, acc 1
2016-09-06T00:56:14.700008: step 5478, loss 0.0288011, acc 0.98
2016-09-06T00:56:15.501256: step 5479, loss 0.0626302, acc 0.96
2016-09-06T00:56:16.301431: step 5480, loss 0.0422, acc 0.98
2016-09-06T00:56:17.125752: step 5481, loss 0.0237174, acc 1
2016-09-06T00:56:17.910269: step 5482, loss 0.00395999, acc 1
2016-09-06T00:56:18.735059: step 5483, loss 0.059291, acc 0.98
2016-09-06T00:56:19.556429: step 5484, loss 0.0309668, acc 0.98
2016-09-06T00:56:20.352917: step 5485, loss 0.0402975, acc 0.96
2016-09-06T00:56:21.148375: step 5486, loss 0.0741227, acc 0.98
2016-09-06T00:56:21.968916: step 5487, loss 0.0127533, acc 1
2016-09-06T00:56:22.751399: step 5488, loss 0.0453083, acc 1
2016-09-06T00:56:23.544860: step 5489, loss 0.0402196, acc 0.98
2016-09-06T00:56:24.370562: step 5490, loss 0.0037589, acc 1
2016-09-06T00:56:25.148834: step 5491, loss 0.0533387, acc 0.98
2016-09-06T00:56:26.013423: step 5492, loss 0.080147, acc 0.96
2016-09-06T00:56:26.791709: step 5493, loss 0.0411056, acc 1
2016-09-06T00:56:27.589382: step 5494, loss 0.0041218, acc 1
2016-09-06T00:56:28.386977: step 5495, loss 0.0165514, acc 1
2016-09-06T00:56:29.217349: step 5496, loss 0.0362257, acc 0.98
2016-09-06T00:56:30.056782: step 5497, loss 0.00842391, acc 1
2016-09-06T00:56:30.847260: step 5498, loss 0.0043117, acc 1
2016-09-06T00:56:31.683903: step 5499, loss 0.00684186, acc 1
2016-09-06T00:56:32.447710: step 5500, loss 0.00599761, acc 1

Evaluation:
2016-09-06T00:56:36.188110: step 5500, loss 2.12755, acc 0.723265

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-5500

2016-09-06T00:56:38.101081: step 5501, loss 0.0549983, acc 0.96
2016-09-06T00:56:38.922741: step 5502, loss 0.0178812, acc 1
2016-09-06T00:56:39.731229: step 5503, loss 0.0582411, acc 0.96
2016-09-06T00:56:40.560104: step 5504, loss 0.0144537, acc 1
2016-09-06T00:56:41.370633: step 5505, loss 0.0095402, acc 1
2016-09-06T00:56:42.142400: step 5506, loss 0.00435999, acc 1
2016-09-06T00:56:42.946467: step 5507, loss 0.044083, acc 0.96
2016-09-06T00:56:43.760945: step 5508, loss 0.0257691, acc 0.98
2016-09-06T00:56:44.551382: step 5509, loss 0.0500123, acc 0.96
2016-09-06T00:56:45.363264: step 5510, loss 0.0347049, acc 0.98
2016-09-06T00:56:46.178771: step 5511, loss 0.00722264, acc 1
2016-09-06T00:56:46.958232: step 5512, loss 0.0164298, acc 1
2016-09-06T00:56:47.747988: step 5513, loss 0.0286059, acc 0.98
2016-09-06T00:56:48.565271: step 5514, loss 0.0459153, acc 0.98
2016-09-06T00:56:49.367077: step 5515, loss 0.00606442, acc 1
2016-09-06T00:56:50.158816: step 5516, loss 0.00681764, acc 1
2016-09-06T00:56:50.966846: step 5517, loss 0.0583697, acc 0.96
2016-09-06T00:56:51.749847: step 5518, loss 0.0112226, acc 1
2016-09-06T00:56:52.574009: step 5519, loss 0.0116259, acc 1
2016-09-06T00:56:53.409201: step 5520, loss 0.0195677, acc 0.98
2016-09-06T00:56:54.185447: step 5521, loss 0.0247503, acc 1
2016-09-06T00:56:55.014597: step 5522, loss 0.0407825, acc 0.98
2016-09-06T00:56:55.839989: step 5523, loss 0.0393287, acc 0.96
2016-09-06T00:56:56.627490: step 5524, loss 0.160374, acc 0.94
2016-09-06T00:56:57.437359: step 5525, loss 0.00351835, acc 1
2016-09-06T00:56:58.275539: step 5526, loss 0.0410738, acc 0.98
2016-09-06T00:56:59.070992: step 5527, loss 0.00722916, acc 1
2016-09-06T00:56:59.868326: step 5528, loss 0.0477295, acc 0.98
2016-09-06T00:57:00.724688: step 5529, loss 0.0582566, acc 0.96
2016-09-06T00:57:01.521854: step 5530, loss 0.0150685, acc 1
2016-09-06T00:57:02.336178: step 5531, loss 0.0197465, acc 0.98
2016-09-06T00:57:03.131200: step 5532, loss 0.0846086, acc 0.96
2016-09-06T00:57:03.910338: step 5533, loss 0.0206995, acc 0.98
2016-09-06T00:57:04.713784: step 5534, loss 0.0123311, acc 1
2016-09-06T00:57:05.546149: step 5535, loss 0.0163795, acc 1
2016-09-06T00:57:06.315933: step 5536, loss 0.0327686, acc 0.98
2016-09-06T00:57:07.123401: step 5537, loss 0.0481116, acc 0.98
2016-09-06T00:57:07.961066: step 5538, loss 0.0960385, acc 0.96
2016-09-06T00:57:08.737669: step 5539, loss 0.0145291, acc 1
2016-09-06T00:57:09.536545: step 5540, loss 0.00989276, acc 1
2016-09-06T00:57:10.356221: step 5541, loss 0.00522058, acc 1
2016-09-06T00:57:11.139760: step 5542, loss 0.0341128, acc 0.96
2016-09-06T00:57:11.961933: step 5543, loss 0.024332, acc 0.98
2016-09-06T00:57:12.797922: step 5544, loss 0.0160207, acc 1
2016-09-06T00:57:13.584712: step 5545, loss 0.020617, acc 1
2016-09-06T00:57:14.395274: step 5546, loss 0.0119098, acc 1
2016-09-06T00:57:15.214566: step 5547, loss 0.0472903, acc 0.98
2016-09-06T00:57:16.011889: step 5548, loss 0.00719337, acc 1
2016-09-06T00:57:16.784613: step 5549, loss 0.0669993, acc 0.96
2016-09-06T00:57:17.626874: step 5550, loss 0.0271877, acc 0.98
2016-09-06T00:57:18.430789: step 5551, loss 0.0454982, acc 1
2016-09-06T00:57:19.227934: step 5552, loss 0.0535324, acc 0.98
2016-09-06T00:57:20.040326: step 5553, loss 0.0130281, acc 1
2016-09-06T00:57:20.837147: step 5554, loss 0.0310705, acc 1
2016-09-06T00:57:21.643148: step 5555, loss 0.0371097, acc 0.96
2016-09-06T00:57:22.452060: step 5556, loss 0.0168457, acc 1
2016-09-06T00:57:23.203033: step 5557, loss 0.0155044, acc 1
2016-09-06T00:57:24.014578: step 5558, loss 0.0205767, acc 0.98
2016-09-06T00:57:24.834370: step 5559, loss 0.00688529, acc 1
2016-09-06T00:57:25.626198: step 5560, loss 0.00333548, acc 1
2016-09-06T00:57:26.429400: step 5561, loss 0.0301959, acc 1
2016-09-06T00:57:27.250210: step 5562, loss 0.0178008, acc 0.98
2016-09-06T00:57:28.045801: step 5563, loss 0.0597539, acc 0.98
2016-09-06T00:57:28.853809: step 5564, loss 0.0133515, acc 1
2016-09-06T00:57:29.717043: step 5565, loss 0.0227048, acc 0.98
2016-09-06T00:57:30.518236: step 5566, loss 0.0274139, acc 0.98
2016-09-06T00:57:31.315499: step 5567, loss 0.0347418, acc 0.98
2016-09-06T00:57:32.069941: step 5568, loss 0.0230387, acc 1
2016-09-06T00:57:32.882484: step 5569, loss 0.00595576, acc 1
2016-09-06T00:57:33.698398: step 5570, loss 0.0454657, acc 0.96
2016-09-06T00:57:34.492135: step 5571, loss 0.0901282, acc 0.98
2016-09-06T00:57:35.289656: step 5572, loss 0.120622, acc 0.96
2016-09-06T00:57:36.076691: step 5573, loss 0.0128318, acc 1
2016-09-06T00:57:36.873400: step 5574, loss 0.0122652, acc 1
2016-09-06T00:57:37.688714: step 5575, loss 0.0129815, acc 1
2016-09-06T00:57:38.492386: step 5576, loss 0.00862574, acc 1
2016-09-06T00:57:39.320620: step 5577, loss 0.00799553, acc 1
2016-09-06T00:57:40.107903: step 5578, loss 0.00625784, acc 1
2016-09-06T00:57:40.900711: step 5579, loss 0.067738, acc 0.98
2016-09-06T00:57:41.730028: step 5580, loss 0.0242519, acc 1
2016-09-06T00:57:42.541126: step 5581, loss 0.0363079, acc 1
2016-09-06T00:57:43.345162: step 5582, loss 0.00535695, acc 1
2016-09-06T00:57:44.163319: step 5583, loss 0.00643643, acc 1
2016-09-06T00:57:44.971849: step 5584, loss 0.0466902, acc 0.96
2016-09-06T00:57:45.818180: step 5585, loss 0.0866804, acc 0.98
2016-09-06T00:57:46.630326: step 5586, loss 0.0602018, acc 0.98
2016-09-06T00:57:47.399532: step 5587, loss 0.0382711, acc 0.98
2016-09-06T00:57:48.183758: step 5588, loss 0.0488412, acc 0.98
2016-09-06T00:57:49.005813: step 5589, loss 0.00818558, acc 1
2016-09-06T00:57:49.791871: step 5590, loss 0.0170577, acc 1
2016-09-06T00:57:50.604166: step 5591, loss 0.042971, acc 0.98
2016-09-06T00:57:51.411848: step 5592, loss 0.0672449, acc 0.96
2016-09-06T00:57:52.194506: step 5593, loss 0.273595, acc 0.96
2016-09-06T00:57:53.006303: step 5594, loss 0.0268054, acc 1
2016-09-06T00:57:53.794218: step 5595, loss 0.0186308, acc 0.98
2016-09-06T00:57:54.582629: step 5596, loss 0.0308535, acc 0.98
2016-09-06T00:57:55.410467: step 5597, loss 0.0506768, acc 0.98
2016-09-06T00:57:56.269547: step 5598, loss 0.00913909, acc 1
2016-09-06T00:57:57.076289: step 5599, loss 0.0769652, acc 0.96
2016-09-06T00:57:57.873097: step 5600, loss 0.0543553, acc 0.98

Evaluation:
2016-09-06T00:58:01.632309: step 5600, loss 1.80013, acc 0.719512

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-5600

2016-09-06T00:58:03.608240: step 5601, loss 0.00688226, acc 1
2016-09-06T00:58:04.407696: step 5602, loss 0.0364341, acc 1
2016-09-06T00:58:05.232834: step 5603, loss 0.0327417, acc 0.98
2016-09-06T00:58:06.061336: step 5604, loss 0.114165, acc 0.96
2016-09-06T00:58:06.886148: step 5605, loss 0.00589007, acc 1
2016-09-06T00:58:07.716603: step 5606, loss 0.0352522, acc 1
2016-09-06T00:58:08.541621: step 5607, loss 0.0445424, acc 0.98
2016-09-06T00:58:09.366779: step 5608, loss 0.0130505, acc 1
2016-09-06T00:58:10.172392: step 5609, loss 0.0378115, acc 0.98
2016-09-06T00:58:11.018626: step 5610, loss 0.0100867, acc 1
2016-09-06T00:58:11.825462: step 5611, loss 0.0663686, acc 0.94
2016-09-06T00:58:12.629572: step 5612, loss 0.0365544, acc 0.98
2016-09-06T00:58:13.467732: step 5613, loss 0.0188402, acc 1
2016-09-06T00:58:14.276718: step 5614, loss 0.0706209, acc 0.96
2016-09-06T00:58:15.087380: step 5615, loss 0.0313696, acc 1
2016-09-06T00:58:15.938899: step 5616, loss 0.0308001, acc 0.98
2016-09-06T00:58:16.753406: step 5617, loss 0.0260641, acc 0.98
2016-09-06T00:58:17.578479: step 5618, loss 0.0037789, acc 1
2016-09-06T00:58:18.393716: step 5619, loss 0.0221717, acc 1
2016-09-06T00:58:19.190009: step 5620, loss 0.021118, acc 0.98
2016-09-06T00:58:19.982687: step 5621, loss 0.0514224, acc 0.98
2016-09-06T00:58:20.817946: step 5622, loss 0.0330069, acc 0.98
2016-09-06T00:58:21.633448: step 5623, loss 0.0118032, acc 1
2016-09-06T00:58:22.424890: step 5624, loss 0.0134358, acc 1
2016-09-06T00:58:23.242709: step 5625, loss 0.00440234, acc 1
2016-09-06T00:58:24.040856: step 5626, loss 0.0203791, acc 1
2016-09-06T00:58:24.842289: step 5627, loss 0.0347349, acc 0.98
2016-09-06T00:58:25.681360: step 5628, loss 0.0146877, acc 1
2016-09-06T00:58:26.508030: step 5629, loss 0.00821572, acc 1
2016-09-06T00:58:27.299881: step 5630, loss 0.0492348, acc 0.98
2016-09-06T00:58:28.093827: step 5631, loss 0.00569253, acc 1
2016-09-06T00:58:28.907118: step 5632, loss 0.0844933, acc 0.94
2016-09-06T00:58:29.700974: step 5633, loss 0.00823611, acc 1
2016-09-06T00:58:30.489323: step 5634, loss 0.0360267, acc 0.96
2016-09-06T00:58:31.296971: step 5635, loss 0.0182049, acc 0.98
2016-09-06T00:58:32.143469: step 5636, loss 0.0606983, acc 0.98
2016-09-06T00:58:32.964996: step 5637, loss 0.030385, acc 1
2016-09-06T00:58:33.785924: step 5638, loss 0.0251036, acc 0.98
2016-09-06T00:58:34.590181: step 5639, loss 0.015453, acc 1
2016-09-06T00:58:35.359724: step 5640, loss 0.0155315, acc 1
2016-09-06T00:58:36.214720: step 5641, loss 0.0398945, acc 0.98
2016-09-06T00:58:37.006853: step 5642, loss 0.00397819, acc 1
2016-09-06T00:58:37.808719: step 5643, loss 0.0335628, acc 1
2016-09-06T00:58:38.635681: step 5644, loss 0.0207538, acc 0.98
2016-09-06T00:58:39.451053: step 5645, loss 0.00877717, acc 1
2016-09-06T00:58:40.254771: step 5646, loss 0.0603026, acc 0.98
2016-09-06T00:58:41.073361: step 5647, loss 0.0406197, acc 0.98
2016-09-06T00:58:41.852702: step 5648, loss 0.0171587, acc 1
2016-09-06T00:58:42.665056: step 5649, loss 0.00774071, acc 1
2016-09-06T00:58:43.504166: step 5650, loss 0.148708, acc 0.98
2016-09-06T00:58:44.299435: step 5651, loss 0.00745507, acc 1
2016-09-06T00:58:45.094448: step 5652, loss 0.0164801, acc 1
2016-09-06T00:58:45.937095: step 5653, loss 0.0187047, acc 1
2016-09-06T00:58:46.742282: step 5654, loss 0.0352372, acc 0.96
2016-09-06T00:58:47.534035: step 5655, loss 0.00409287, acc 1
2016-09-06T00:58:48.343826: step 5656, loss 0.0136151, acc 1
2016-09-06T00:58:49.167618: step 5657, loss 0.00646822, acc 1
2016-09-06T00:58:50.000536: step 5658, loss 0.0251985, acc 0.98
2016-09-06T00:58:50.822993: step 5659, loss 0.142526, acc 0.94
2016-09-06T00:58:51.609879: step 5660, loss 0.040967, acc 0.98
2016-09-06T00:58:52.425471: step 5661, loss 0.00529757, acc 1
2016-09-06T00:58:53.256130: step 5662, loss 0.00443458, acc 1
2016-09-06T00:58:54.079398: step 5663, loss 0.0263488, acc 0.98
2016-09-06T00:58:54.907827: step 5664, loss 0.00902639, acc 1
2016-09-06T00:58:55.764879: step 5665, loss 0.0106133, acc 1
2016-09-06T00:58:56.593804: step 5666, loss 0.0386623, acc 1
2016-09-06T00:58:57.414312: step 5667, loss 0.019217, acc 0.98
2016-09-06T00:58:58.236592: step 5668, loss 0.0116451, acc 1
2016-09-06T00:58:59.058565: step 5669, loss 0.0185317, acc 1
2016-09-06T00:58:59.867358: step 5670, loss 0.00482543, acc 1
2016-09-06T00:59:00.709270: step 5671, loss 0.0232625, acc 0.98
2016-09-06T00:59:01.530136: step 5672, loss 0.0296946, acc 0.98
2016-09-06T00:59:02.327163: step 5673, loss 0.0252485, acc 1
2016-09-06T00:59:03.142512: step 5674, loss 0.0101257, acc 1
2016-09-06T00:59:03.972063: step 5675, loss 0.0719078, acc 0.96
2016-09-06T00:59:04.789729: step 5676, loss 0.0251772, acc 0.98
2016-09-06T00:59:05.604158: step 5677, loss 0.0367525, acc 0.96
2016-09-06T00:59:06.408045: step 5678, loss 0.0576644, acc 0.96
2016-09-06T00:59:07.212601: step 5679, loss 0.0133735, acc 1
2016-09-06T00:59:07.998394: step 5680, loss 0.109184, acc 0.96
2016-09-06T00:59:08.824720: step 5681, loss 0.00733886, acc 1
2016-09-06T00:59:09.639717: step 5682, loss 0.00738995, acc 1
2016-09-06T00:59:10.448945: step 5683, loss 0.00459406, acc 1
2016-09-06T00:59:11.274999: step 5684, loss 0.0341187, acc 1
2016-09-06T00:59:12.049701: step 5685, loss 0.00657008, acc 1
2016-09-06T00:59:12.840022: step 5686, loss 0.00536509, acc 1
2016-09-06T00:59:13.654933: step 5687, loss 0.0295559, acc 0.98
2016-09-06T00:59:14.459293: step 5688, loss 0.0387498, acc 0.98
2016-09-06T00:59:15.251075: step 5689, loss 0.00361367, acc 1
2016-09-06T00:59:16.087862: step 5690, loss 0.00814752, acc 1
2016-09-06T00:59:16.886670: step 5691, loss 0.00936404, acc 1
2016-09-06T00:59:17.685765: step 5692, loss 0.00361252, acc 1
2016-09-06T00:59:18.505535: step 5693, loss 0.0321601, acc 1
2016-09-06T00:59:19.291898: step 5694, loss 0.120012, acc 0.96
2016-09-06T00:59:20.122617: step 5695, loss 0.0341811, acc 0.98
2016-09-06T00:59:20.923939: step 5696, loss 0.074581, acc 0.94
2016-09-06T00:59:21.703540: step 5697, loss 0.034957, acc 0.98
2016-09-06T00:59:22.494875: step 5698, loss 0.0972262, acc 0.98
2016-09-06T00:59:23.297429: step 5699, loss 0.0345519, acc 1
2016-09-06T00:59:24.092248: step 5700, loss 0.00407003, acc 1

Evaluation:
2016-09-06T00:59:27.868750: step 5700, loss 1.95229, acc 0.724203

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-5700

2016-09-06T00:59:29.817260: step 5701, loss 0.0434648, acc 0.98
2016-09-06T00:59:30.645137: step 5702, loss 0.00745706, acc 1
2016-09-06T00:59:31.437402: step 5703, loss 0.0259428, acc 1
2016-09-06T00:59:32.225643: step 5704, loss 0.0826349, acc 0.98
2016-09-06T00:59:33.031012: step 5705, loss 0.0289371, acc 1
2016-09-06T00:59:33.805296: step 5706, loss 0.0367951, acc 1
2016-09-06T00:59:34.613672: step 5707, loss 0.00663125, acc 1
2016-09-06T00:59:35.425161: step 5708, loss 0.0202361, acc 1
2016-09-06T00:59:36.227149: step 5709, loss 0.0422445, acc 0.98
2016-09-06T00:59:37.010746: step 5710, loss 0.0115353, acc 1
2016-09-06T00:59:37.824312: step 5711, loss 0.0180289, acc 1
2016-09-06T00:59:38.619473: step 5712, loss 0.0726013, acc 0.96
2016-09-06T00:59:39.418726: step 5713, loss 0.0125067, acc 1
2016-09-06T00:59:40.237933: step 5714, loss 0.0235795, acc 1
2016-09-06T00:59:41.034279: step 5715, loss 0.0291407, acc 0.98
2016-09-06T00:59:41.821994: step 5716, loss 0.0930302, acc 0.98
2016-09-06T00:59:42.627959: step 5717, loss 0.0290257, acc 1
2016-09-06T00:59:43.487210: step 5718, loss 0.0377845, acc 0.98
2016-09-06T00:59:44.344895: step 5719, loss 0.0534795, acc 0.98
2016-09-06T00:59:45.175235: step 5720, loss 0.00568575, acc 1
2016-09-06T00:59:45.970129: step 5721, loss 0.0208047, acc 1
2016-09-06T00:59:46.776320: step 5722, loss 0.00335625, acc 1
2016-09-06T00:59:47.614565: step 5723, loss 0.00740523, acc 1
2016-09-06T00:59:48.417957: step 5724, loss 0.0318359, acc 0.98
2016-09-06T00:59:49.233966: step 5725, loss 0.0362287, acc 0.98
2016-09-06T00:59:50.080437: step 5726, loss 0.0350764, acc 0.98
2016-09-06T00:59:50.868687: step 5727, loss 0.0099815, acc 1
2016-09-06T00:59:51.699608: step 5728, loss 0.0675849, acc 0.92
2016-09-06T00:59:52.520602: step 5729, loss 0.0378681, acc 1
2016-09-06T00:59:53.322534: step 5730, loss 0.0336346, acc 0.98
2016-09-06T00:59:54.142788: step 5731, loss 0.00351979, acc 1
2016-09-06T00:59:54.962785: step 5732, loss 0.0186981, acc 0.98
2016-09-06T00:59:55.774637: step 5733, loss 0.0074287, acc 1
2016-09-06T00:59:56.577392: step 5734, loss 0.0171464, acc 1
2016-09-06T00:59:57.441028: step 5735, loss 0.00781515, acc 1
2016-09-06T00:59:58.237334: step 5736, loss 0.0158537, acc 1
2016-09-06T00:59:59.026836: step 5737, loss 0.0564554, acc 0.94
2016-09-06T00:59:59.856576: step 5738, loss 0.0245327, acc 0.98
2016-09-06T01:00:00.682344: step 5739, loss 0.0042855, acc 1
2016-09-06T01:00:01.484523: step 5740, loss 0.0198835, acc 1
2016-09-06T01:00:02.300748: step 5741, loss 0.0309662, acc 1
2016-09-06T01:00:03.121369: step 5742, loss 0.0199987, acc 1
2016-09-06T01:00:03.939683: step 5743, loss 0.0798639, acc 0.96
2016-09-06T01:00:04.773494: step 5744, loss 0.00401054, acc 1
2016-09-06T01:00:05.599147: step 5745, loss 0.0530519, acc 0.96
2016-09-06T01:00:06.411303: step 5746, loss 0.076101, acc 0.98
2016-09-06T01:00:07.222841: step 5747, loss 0.0127461, acc 1
2016-09-06T01:00:08.027697: step 5748, loss 0.0242959, acc 1
2016-09-06T01:00:08.864256: step 5749, loss 0.00325543, acc 1
2016-09-06T01:00:09.693911: step 5750, loss 0.0212985, acc 1
2016-09-06T01:00:10.489259: step 5751, loss 0.00307919, acc 1
2016-09-06T01:00:11.278873: step 5752, loss 0.00821026, acc 1
2016-09-06T01:00:12.098105: step 5753, loss 0.0168399, acc 1
2016-09-06T01:00:12.907202: step 5754, loss 0.0172586, acc 0.98
2016-09-06T01:00:13.708415: step 5755, loss 0.025079, acc 1
2016-09-06T01:00:14.520593: step 5756, loss 0.076362, acc 0.96
2016-09-06T01:00:15.360799: step 5757, loss 0.0259546, acc 0.98
2016-09-06T01:00:16.141023: step 5758, loss 0.0165548, acc 1
2016-09-06T01:00:16.943940: step 5759, loss 0.00632509, acc 1
2016-09-06T01:00:17.709403: step 5760, loss 0.00409564, acc 1
2016-09-06T01:00:18.501635: step 5761, loss 0.00840951, acc 1
2016-09-06T01:00:19.311280: step 5762, loss 0.0265815, acc 0.98
2016-09-06T01:00:20.109291: step 5763, loss 0.056866, acc 0.98
2016-09-06T01:00:20.906671: step 5764, loss 0.0292615, acc 1
2016-09-06T01:00:21.708820: step 5765, loss 0.00392693, acc 1
2016-09-06T01:00:22.495862: step 5766, loss 0.00300843, acc 1
2016-09-06T01:00:23.283509: step 5767, loss 0.0112403, acc 1
2016-09-06T01:00:24.132620: step 5768, loss 0.0274316, acc 0.98
2016-09-06T01:00:24.933829: step 5769, loss 0.0188605, acc 0.98
2016-09-06T01:00:25.733399: step 5770, loss 0.00331359, acc 1
2016-09-06T01:00:26.562388: step 5771, loss 0.0301903, acc 0.98
2016-09-06T01:00:27.362159: step 5772, loss 0.00340078, acc 1
2016-09-06T01:00:28.140599: step 5773, loss 0.00349495, acc 1
2016-09-06T01:00:28.980261: step 5774, loss 0.0142901, acc 1
2016-09-06T01:00:29.806424: step 5775, loss 0.0118653, acc 1
2016-09-06T01:00:30.607102: step 5776, loss 0.0567726, acc 0.98
2016-09-06T01:00:31.432015: step 5777, loss 0.0200639, acc 1
2016-09-06T01:00:32.273261: step 5778, loss 0.00375501, acc 1
2016-09-06T01:00:33.050075: step 5779, loss 0.00331881, acc 1
2016-09-06T01:00:33.829743: step 5780, loss 0.0421452, acc 0.98
2016-09-06T01:00:34.659351: step 5781, loss 0.0037981, acc 1
2016-09-06T01:00:35.452671: step 5782, loss 0.013906, acc 1
2016-09-06T01:00:36.238769: step 5783, loss 0.0171428, acc 1
2016-09-06T01:00:37.075617: step 5784, loss 0.0104806, acc 1
2016-09-06T01:00:37.855396: step 5785, loss 0.0225247, acc 1
2016-09-06T01:00:38.672167: step 5786, loss 0.0579885, acc 0.94
2016-09-06T01:00:39.516007: step 5787, loss 0.00727495, acc 1
2016-09-06T01:00:40.311401: step 5788, loss 0.00305894, acc 1
2016-09-06T01:00:41.106272: step 5789, loss 0.0821682, acc 0.96
2016-09-06T01:00:41.959542: step 5790, loss 0.0130222, acc 1
2016-09-06T01:00:42.788675: step 5791, loss 0.141405, acc 0.96
2016-09-06T01:00:43.613968: step 5792, loss 0.0102035, acc 1
2016-09-06T01:00:44.428008: step 5793, loss 0.0076383, acc 1
2016-09-06T01:00:45.243954: step 5794, loss 0.0155537, acc 1
2016-09-06T01:00:46.062934: step 5795, loss 0.00274983, acc 1
2016-09-06T01:00:46.894858: step 5796, loss 0.0114487, acc 1
2016-09-06T01:00:47.701883: step 5797, loss 0.00441408, acc 1
2016-09-06T01:00:48.531876: step 5798, loss 0.0263495, acc 0.98
2016-09-06T01:00:49.364369: step 5799, loss 0.0632186, acc 0.98
2016-09-06T01:00:50.173900: step 5800, loss 0.00486822, acc 1

Evaluation:
2016-09-06T01:00:53.931456: step 5800, loss 2.31947, acc 0.726079

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-5800

2016-09-06T01:00:55.822248: step 5801, loss 0.0525946, acc 0.96
2016-09-06T01:00:56.667621: step 5802, loss 0.0028079, acc 1
2016-09-06T01:00:57.496260: step 5803, loss 0.0396451, acc 0.98
2016-09-06T01:00:58.300809: step 5804, loss 0.0564999, acc 0.96
2016-09-06T01:00:59.115566: step 5805, loss 0.0257392, acc 0.98
2016-09-06T01:00:59.930025: step 5806, loss 0.0930411, acc 0.98
2016-09-06T01:01:00.770167: step 5807, loss 0.0037915, acc 1
2016-09-06T01:01:01.624520: step 5808, loss 0.0196107, acc 1
2016-09-06T01:01:02.430257: step 5809, loss 0.00343909, acc 1
2016-09-06T01:01:03.235059: step 5810, loss 0.0304914, acc 0.98
2016-09-06T01:01:04.053178: step 5811, loss 0.0169845, acc 1
2016-09-06T01:01:04.890432: step 5812, loss 0.0406918, acc 1
2016-09-06T01:01:05.675647: step 5813, loss 0.0400196, acc 0.96
2016-09-06T01:01:06.528147: step 5814, loss 0.0656766, acc 0.96
2016-09-06T01:01:07.356205: step 5815, loss 0.0352717, acc 0.98
2016-09-06T01:01:08.127444: step 5816, loss 0.0213763, acc 1
2016-09-06T01:01:08.912864: step 5817, loss 0.0281969, acc 0.98
2016-09-06T01:01:09.724647: step 5818, loss 0.00342136, acc 1
2016-09-06T01:01:10.506575: step 5819, loss 0.010816, acc 1
2016-09-06T01:01:11.312167: step 5820, loss 0.00956919, acc 1
2016-09-06T01:01:12.125515: step 5821, loss 0.0511821, acc 0.96
2016-09-06T01:01:12.926607: step 5822, loss 0.0159263, acc 1
2016-09-06T01:01:13.737350: step 5823, loss 0.050598, acc 0.98
2016-09-06T01:01:14.544196: step 5824, loss 0.0243569, acc 1
2016-09-06T01:01:15.322915: step 5825, loss 0.030112, acc 0.98
2016-09-06T01:01:16.131044: step 5826, loss 0.0223467, acc 1
2016-09-06T01:01:16.951998: step 5827, loss 0.0143303, acc 1
2016-09-06T01:01:17.724162: step 5828, loss 0.0357794, acc 0.98
2016-09-06T01:01:18.537090: step 5829, loss 0.0147142, acc 1
2016-09-06T01:01:19.327376: step 5830, loss 0.00984552, acc 1
2016-09-06T01:01:20.137550: step 5831, loss 0.0324167, acc 0.98
2016-09-06T01:01:20.959199: step 5832, loss 0.00567093, acc 1
2016-09-06T01:01:21.763528: step 5833, loss 0.0089133, acc 1
2016-09-06T01:01:22.588279: step 5834, loss 0.0102473, acc 1
2016-09-06T01:01:23.402128: step 5835, loss 0.0266593, acc 0.98
2016-09-06T01:01:24.215805: step 5836, loss 0.0229691, acc 0.98
2016-09-06T01:01:25.038759: step 5837, loss 0.00253384, acc 1
2016-09-06T01:01:25.876825: step 5838, loss 0.0149905, acc 1
2016-09-06T01:01:26.684336: step 5839, loss 0.00539298, acc 1
2016-09-06T01:01:27.467884: step 5840, loss 0.0107806, acc 1
2016-09-06T01:01:28.263759: step 5841, loss 0.00732338, acc 1
2016-09-06T01:01:29.069481: step 5842, loss 0.0159019, acc 1
2016-09-06T01:01:29.870483: step 5843, loss 0.0587254, acc 0.94
2016-09-06T01:01:30.684075: step 5844, loss 0.00726273, acc 1
2016-09-06T01:01:31.524689: step 5845, loss 0.00877649, acc 1
2016-09-06T01:01:32.332722: step 5846, loss 0.00427005, acc 1
2016-09-06T01:01:33.131932: step 5847, loss 0.00272358, acc 1
2016-09-06T01:01:33.950582: step 5848, loss 0.0241341, acc 1
2016-09-06T01:01:34.737097: step 5849, loss 0.00399796, acc 1
2016-09-06T01:01:35.525887: step 5850, loss 0.00271467, acc 1
2016-09-06T01:01:36.356467: step 5851, loss 0.0403505, acc 0.98
2016-09-06T01:01:37.143929: step 5852, loss 0.00287105, acc 1
2016-09-06T01:01:37.967832: step 5853, loss 0.0252144, acc 0.98
2016-09-06T01:01:38.788380: step 5854, loss 0.00641114, acc 1
2016-09-06T01:01:39.617779: step 5855, loss 0.0189055, acc 0.98
2016-09-06T01:01:40.396467: step 5856, loss 0.00281183, acc 1
2016-09-06T01:01:41.216336: step 5857, loss 0.0055022, acc 1
2016-09-06T01:01:42.017164: step 5858, loss 0.107075, acc 0.98
2016-09-06T01:01:42.801739: step 5859, loss 0.00748981, acc 1
2016-09-06T01:01:43.605453: step 5860, loss 0.0244243, acc 0.98
2016-09-06T01:01:44.380276: step 5861, loss 0.0235165, acc 0.98
2016-09-06T01:01:45.184712: step 5862, loss 0.0253341, acc 0.98
2016-09-06T01:01:45.993878: step 5863, loss 0.0113876, acc 1
2016-09-06T01:01:46.790285: step 5864, loss 0.00271049, acc 1
2016-09-06T01:01:47.590190: step 5865, loss 0.0162968, acc 1
2016-09-06T01:01:48.413556: step 5866, loss 0.0198769, acc 1
2016-09-06T01:01:49.204313: step 5867, loss 0.0134643, acc 1
2016-09-06T01:01:50.021690: step 5868, loss 0.0264599, acc 0.98
2016-09-06T01:01:50.883886: step 5869, loss 0.0409801, acc 0.98
2016-09-06T01:01:51.720621: step 5870, loss 0.0617244, acc 0.98
2016-09-06T01:01:52.527523: step 5871, loss 0.00358051, acc 1
2016-09-06T01:01:53.335720: step 5872, loss 0.0786445, acc 0.96
2016-09-06T01:01:54.146074: step 5873, loss 0.00568586, acc 1
2016-09-06T01:01:54.956199: step 5874, loss 0.00620926, acc 1
2016-09-06T01:01:55.797001: step 5875, loss 0.0634609, acc 0.96
2016-09-06T01:01:56.588170: step 5876, loss 0.057831, acc 0.98
2016-09-06T01:01:57.451961: step 5877, loss 0.037408, acc 0.98
2016-09-06T01:01:58.368049: step 5878, loss 0.0723448, acc 0.98
2016-09-06T01:01:59.176075: step 5879, loss 0.00769656, acc 1
2016-09-06T01:01:59.995251: step 5880, loss 0.0169592, acc 1
2016-09-06T01:02:00.862686: step 5881, loss 0.00217646, acc 1
2016-09-06T01:02:01.691726: step 5882, loss 0.00278879, acc 1
2016-09-06T01:02:02.489443: step 5883, loss 0.0779862, acc 0.96
2016-09-06T01:02:03.320887: step 5884, loss 0.0205329, acc 1
2016-09-06T01:02:04.181558: step 5885, loss 0.0390919, acc 1
2016-09-06T01:02:04.950613: step 5886, loss 0.0173768, acc 1
2016-09-06T01:02:05.750519: step 5887, loss 0.0759504, acc 0.98
2016-09-06T01:02:06.559180: step 5888, loss 0.00359047, acc 1
2016-09-06T01:02:07.345193: step 5889, loss 0.00759787, acc 1
2016-09-06T01:02:08.159288: step 5890, loss 0.00644355, acc 1
2016-09-06T01:02:08.986145: step 5891, loss 0.0190002, acc 1
2016-09-06T01:02:09.777938: step 5892, loss 0.014868, acc 1
2016-09-06T01:02:10.579073: step 5893, loss 0.0934611, acc 0.98
2016-09-06T01:02:11.415106: step 5894, loss 0.0177414, acc 1
2016-09-06T01:02:12.212093: step 5895, loss 0.0266531, acc 0.98
2016-09-06T01:02:12.999052: step 5896, loss 0.0128632, acc 1
2016-09-06T01:02:13.817436: step 5897, loss 0.0244986, acc 0.98
2016-09-06T01:02:14.614392: step 5898, loss 0.0595842, acc 0.96
2016-09-06T01:02:15.436875: step 5899, loss 0.00319666, acc 1
2016-09-06T01:02:16.270547: step 5900, loss 0.0113581, acc 1

Evaluation:
2016-09-06T01:02:19.954804: step 5900, loss 2.97687, acc 0.726079

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-5900

2016-09-06T01:02:21.808500: step 5901, loss 0.0134822, acc 1
2016-09-06T01:02:22.647824: step 5902, loss 0.00576358, acc 1
2016-09-06T01:02:23.454743: step 5903, loss 0.00406779, acc 1
2016-09-06T01:02:24.255364: step 5904, loss 0.0388001, acc 0.98
2016-09-06T01:02:25.078764: step 5905, loss 0.00367176, acc 1
2016-09-06T01:02:25.879327: step 5906, loss 0.0199275, acc 1
2016-09-06T01:02:26.672105: step 5907, loss 0.0767809, acc 0.98
2016-09-06T01:02:27.491990: step 5908, loss 0.00425488, acc 1
2016-09-06T01:02:28.314597: step 5909, loss 0.00887925, acc 1
2016-09-06T01:02:29.141361: step 5910, loss 0.0226615, acc 1
2016-09-06T01:02:29.983958: step 5911, loss 0.0744148, acc 0.98
2016-09-06T01:02:30.828471: step 5912, loss 0.00792795, acc 1
2016-09-06T01:02:31.606876: step 5913, loss 0.0625466, acc 0.98
2016-09-06T01:02:32.425227: step 5914, loss 0.00463791, acc 1
2016-09-06T01:02:33.242975: step 5915, loss 0.0177319, acc 1
2016-09-06T01:02:34.049885: step 5916, loss 0.0132367, acc 1
2016-09-06T01:02:34.868043: step 5917, loss 0.0176719, acc 1
2016-09-06T01:02:35.712500: step 5918, loss 0.00457244, acc 1
2016-09-06T01:02:36.510250: step 5919, loss 0.0209485, acc 1
2016-09-06T01:02:37.310337: step 5920, loss 0.0410079, acc 0.98
2016-09-06T01:02:38.131693: step 5921, loss 0.0301245, acc 1
2016-09-06T01:02:38.925930: step 5922, loss 0.0438761, acc 0.98
2016-09-06T01:02:39.752349: step 5923, loss 0.0162727, acc 1
2016-09-06T01:02:40.598984: step 5924, loss 0.00991577, acc 1
2016-09-06T01:02:41.425399: step 5925, loss 0.00392239, acc 1
2016-09-06T01:02:42.217482: step 5926, loss 0.108534, acc 0.98
2016-09-06T01:02:43.068312: step 5927, loss 0.0140276, acc 1
2016-09-06T01:02:43.896994: step 5928, loss 0.0121741, acc 1
2016-09-06T01:02:44.704699: step 5929, loss 0.0499653, acc 0.98
2016-09-06T01:02:45.532780: step 5930, loss 0.0981741, acc 0.98
2016-09-06T01:02:46.345624: step 5931, loss 0.0169351, acc 1
2016-09-06T01:02:47.178584: step 5932, loss 0.260515, acc 0.94
2016-09-06T01:02:47.999221: step 5933, loss 0.00357013, acc 1
2016-09-06T01:02:48.860953: step 5934, loss 0.0244447, acc 1
2016-09-06T01:02:49.697987: step 5935, loss 0.0482069, acc 0.96
2016-09-06T01:02:50.583002: step 5936, loss 0.036658, acc 0.98
2016-09-06T01:02:51.406126: step 5937, loss 0.00564337, acc 1
2016-09-06T01:02:52.169220: step 5938, loss 0.0853111, acc 0.92
2016-09-06T01:02:52.972383: step 5939, loss 0.00644289, acc 1
2016-09-06T01:02:53.781217: step 5940, loss 0.124488, acc 0.96
2016-09-06T01:02:54.567619: step 5941, loss 0.00415475, acc 1
2016-09-06T01:02:55.378775: step 5942, loss 0.0238106, acc 1
2016-09-06T01:02:56.213475: step 5943, loss 0.0143398, acc 1
2016-09-06T01:02:56.994171: step 5944, loss 0.0109945, acc 1
2016-09-06T01:02:57.795679: step 5945, loss 0.0164876, acc 1
2016-09-06T01:02:58.607124: step 5946, loss 0.020946, acc 1
2016-09-06T01:02:59.417850: step 5947, loss 0.0158409, acc 1
2016-09-06T01:03:00.252097: step 5948, loss 0.0138954, acc 1
2016-09-06T01:03:01.059313: step 5949, loss 0.0392142, acc 1
2016-09-06T01:03:01.827470: step 5950, loss 0.0525754, acc 0.98
2016-09-06T01:03:02.638290: step 5951, loss 0.00932546, acc 1
2016-09-06T01:03:03.390860: step 5952, loss 0.0222025, acc 1
2016-09-06T01:03:04.223885: step 5953, loss 0.00384379, acc 1
2016-09-06T01:03:05.034629: step 5954, loss 0.0565562, acc 0.96
2016-09-06T01:03:05.869243: step 5955, loss 0.00852923, acc 1
2016-09-06T01:03:06.666495: step 5956, loss 0.0140601, acc 1
2016-09-06T01:03:07.464989: step 5957, loss 0.00356884, acc 1
2016-09-06T01:03:08.283153: step 5958, loss 0.00464472, acc 1
2016-09-06T01:03:09.078531: step 5959, loss 0.0113705, acc 1
2016-09-06T01:03:09.888519: step 5960, loss 0.0181468, acc 1
2016-09-06T01:03:10.715580: step 5961, loss 0.024633, acc 0.98
2016-09-06T01:03:11.517436: step 5962, loss 0.00583901, acc 1
2016-09-06T01:03:12.335563: step 5963, loss 0.0531214, acc 0.98
2016-09-06T01:03:13.114706: step 5964, loss 0.012561, acc 1
2016-09-06T01:03:13.896069: step 5965, loss 0.0109702, acc 1
2016-09-06T01:03:14.706643: step 5966, loss 0.0199689, acc 1
2016-09-06T01:03:15.528963: step 5967, loss 0.0196712, acc 1
2016-09-06T01:03:16.295780: step 5968, loss 0.0503866, acc 0.98
2016-09-06T01:03:17.100987: step 5969, loss 0.00558023, acc 1
2016-09-06T01:03:17.928258: step 5970, loss 0.0208593, acc 0.98
2016-09-06T01:03:18.733455: step 5971, loss 0.0116352, acc 1
2016-09-06T01:03:19.546757: step 5972, loss 0.0919081, acc 0.98
2016-09-06T01:03:20.387213: step 5973, loss 0.00647622, acc 1
2016-09-06T01:03:21.145247: step 5974, loss 0.00955011, acc 1
2016-09-06T01:03:21.952335: step 5975, loss 0.0271862, acc 0.98
2016-09-06T01:03:22.796403: step 5976, loss 0.119147, acc 0.96
2016-09-06T01:03:23.562614: step 5977, loss 0.0233334, acc 0.98
2016-09-06T01:03:24.369369: step 5978, loss 0.0954269, acc 0.98
2016-09-06T01:03:25.183482: step 5979, loss 0.105301, acc 0.98
2016-09-06T01:03:25.967594: step 5980, loss 0.0421266, acc 0.98
2016-09-06T01:03:26.764023: step 5981, loss 0.0213106, acc 1
2016-09-06T01:03:27.589906: step 5982, loss 0.00971245, acc 1
2016-09-06T01:03:28.407487: step 5983, loss 0.00908224, acc 1
2016-09-06T01:03:29.202153: step 5984, loss 0.0252984, acc 1
2016-09-06T01:03:30.048200: step 5985, loss 0.0291571, acc 0.98
2016-09-06T01:03:30.859448: step 5986, loss 0.0158475, acc 1
2016-09-06T01:03:31.657641: step 5987, loss 0.0073343, acc 1
2016-09-06T01:03:32.457416: step 5988, loss 0.0159236, acc 1
2016-09-06T01:03:33.219495: step 5989, loss 0.0240299, acc 0.98
2016-09-06T01:03:34.033663: step 5990, loss 0.0174143, acc 0.98
2016-09-06T01:03:34.855908: step 5991, loss 0.0334082, acc 1
2016-09-06T01:03:35.637149: step 5992, loss 0.0162665, acc 1
2016-09-06T01:03:36.431742: step 5993, loss 0.159433, acc 0.94
2016-09-06T01:03:37.236661: step 5994, loss 0.028917, acc 0.98
2016-09-06T01:03:38.038094: step 5995, loss 0.0590429, acc 0.94
2016-09-06T01:03:38.872552: step 5996, loss 0.0107108, acc 1
2016-09-06T01:03:39.691706: step 5997, loss 0.0698186, acc 0.98
2016-09-06T01:03:40.487167: step 5998, loss 0.0341067, acc 0.98
2016-09-06T01:03:41.287505: step 5999, loss 0.0510642, acc 0.98
2016-09-06T01:03:42.129102: step 6000, loss 0.0236037, acc 0.98

Evaluation:
2016-09-06T01:03:45.833634: step 6000, loss 1.70004, acc 0.731707

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-6000

2016-09-06T01:03:47.797435: step 6001, loss 0.0241185, acc 1
2016-09-06T01:03:48.619144: step 6002, loss 0.00609862, acc 1
2016-09-06T01:03:49.422344: step 6003, loss 0.00976861, acc 1
2016-09-06T01:03:50.219348: step 6004, loss 0.0305035, acc 1
2016-09-06T01:03:51.030781: step 6005, loss 0.0361926, acc 0.98
2016-09-06T01:03:51.825238: step 6006, loss 0.0198155, acc 1
2016-09-06T01:03:52.645480: step 6007, loss 0.00966228, acc 1
2016-09-06T01:03:53.475749: step 6008, loss 0.0073684, acc 1
2016-09-06T01:03:54.305331: step 6009, loss 0.0161542, acc 1
2016-09-06T01:03:55.100439: step 6010, loss 0.0459524, acc 1
2016-09-06T01:03:55.912906: step 6011, loss 0.0191956, acc 0.98
2016-09-06T01:03:56.718095: step 6012, loss 0.00549374, acc 1
2016-09-06T01:03:57.502313: step 6013, loss 0.01809, acc 1
2016-09-06T01:03:58.292588: step 6014, loss 0.00460825, acc 1
2016-09-06T01:03:59.106267: step 6015, loss 0.0236093, acc 0.98
2016-09-06T01:03:59.888728: step 6016, loss 0.114356, acc 0.94
2016-09-06T01:04:00.717992: step 6017, loss 0.0607217, acc 0.96
2016-09-06T01:04:01.523430: step 6018, loss 0.0057541, acc 1
2016-09-06T01:04:02.320886: step 6019, loss 0.00292352, acc 1
2016-09-06T01:04:03.118225: step 6020, loss 0.0696028, acc 0.94
2016-09-06T01:04:03.932035: step 6021, loss 0.0284141, acc 1
2016-09-06T01:04:04.774033: step 6022, loss 0.018258, acc 1
2016-09-06T01:04:05.590656: step 6023, loss 0.026887, acc 0.98
2016-09-06T01:04:06.448829: step 6024, loss 0.0401976, acc 0.98
2016-09-06T01:04:07.242676: step 6025, loss 0.0204766, acc 0.98
2016-09-06T01:04:08.057209: step 6026, loss 0.0953791, acc 0.98
2016-09-06T01:04:08.906190: step 6027, loss 0.0274003, acc 0.98
2016-09-06T01:04:09.713536: step 6028, loss 0.0329022, acc 0.98
2016-09-06T01:04:10.514361: step 6029, loss 0.0112398, acc 1
2016-09-06T01:04:11.344611: step 6030, loss 0.0326009, acc 0.98
2016-09-06T01:04:12.167620: step 6031, loss 0.103757, acc 0.96
2016-09-06T01:04:12.941216: step 6032, loss 0.00500849, acc 1
2016-09-06T01:04:13.779351: step 6033, loss 0.017891, acc 0.98
2016-09-06T01:04:14.589582: step 6034, loss 0.0081847, acc 1
2016-09-06T01:04:15.387388: step 6035, loss 0.0799773, acc 0.94
2016-09-06T01:04:16.189403: step 6036, loss 0.00457484, acc 1
2016-09-06T01:04:17.001442: step 6037, loss 0.0168887, acc 1
2016-09-06T01:04:17.803625: step 6038, loss 0.0290584, acc 0.98
2016-09-06T01:04:18.639975: step 6039, loss 0.0105476, acc 1
2016-09-06T01:04:19.445742: step 6040, loss 0.0586012, acc 0.96
2016-09-06T01:04:20.268281: step 6041, loss 0.0208643, acc 1
2016-09-06T01:04:21.114867: step 6042, loss 0.0138027, acc 1
2016-09-06T01:04:21.918292: step 6043, loss 0.0225109, acc 1
2016-09-06T01:04:22.752376: step 6044, loss 0.00982615, acc 1
2016-09-06T01:04:23.563449: step 6045, loss 0.11823, acc 0.92
2016-09-06T01:04:24.392903: step 6046, loss 0.0710939, acc 0.98
2016-09-06T01:04:25.263366: step 6047, loss 0.00602702, acc 1
2016-09-06T01:04:26.095417: step 6048, loss 0.0103939, acc 1
2016-09-06T01:04:26.957138: step 6049, loss 0.0159873, acc 1
2016-09-06T01:04:27.783616: step 6050, loss 0.0091162, acc 1
2016-09-06T01:04:28.587967: step 6051, loss 0.0616853, acc 0.96
2016-09-06T01:04:29.412793: step 6052, loss 0.0201787, acc 0.98
2016-09-06T01:04:30.205543: step 6053, loss 0.0312187, acc 1
2016-09-06T01:04:30.993734: step 6054, loss 0.0204635, acc 1
2016-09-06T01:04:31.773552: step 6055, loss 0.00724432, acc 1
2016-09-06T01:04:32.580836: step 6056, loss 0.0201205, acc 1
2016-09-06T01:04:33.413376: step 6057, loss 0.0351589, acc 1
2016-09-06T01:04:34.215448: step 6058, loss 0.0103479, acc 1
2016-09-06T01:04:34.999681: step 6059, loss 0.0464465, acc 0.98
2016-09-06T01:04:35.800755: step 6060, loss 0.0169869, acc 1
2016-09-06T01:04:36.607044: step 6061, loss 0.0548506, acc 0.98
2016-09-06T01:04:37.394627: step 6062, loss 0.0261784, acc 1
2016-09-06T01:04:38.179933: step 6063, loss 0.0214034, acc 0.98
2016-09-06T01:04:38.994961: step 6064, loss 0.0870749, acc 0.96
2016-09-06T01:04:39.787702: step 6065, loss 0.0142239, acc 1
2016-09-06T01:04:40.581325: step 6066, loss 0.015798, acc 1
2016-09-06T01:04:41.435719: step 6067, loss 0.0854683, acc 0.96
2016-09-06T01:04:42.251380: step 6068, loss 0.020111, acc 1
2016-09-06T01:04:43.037357: step 6069, loss 0.0157572, acc 1
2016-09-06T01:04:43.857223: step 6070, loss 0.00493555, acc 1
2016-09-06T01:04:44.641430: step 6071, loss 0.00471978, acc 1
2016-09-06T01:04:45.433416: step 6072, loss 0.0257852, acc 0.98
2016-09-06T01:04:46.237106: step 6073, loss 0.00326523, acc 1
2016-09-06T01:04:47.021675: step 6074, loss 0.00315509, acc 1
2016-09-06T01:04:47.846153: step 6075, loss 0.00324894, acc 1
2016-09-06T01:04:48.660227: step 6076, loss 0.0192383, acc 1
2016-09-06T01:04:49.435459: step 6077, loss 0.0149335, acc 1
2016-09-06T01:04:50.243697: step 6078, loss 0.0495588, acc 0.98
2016-09-06T01:04:51.063270: step 6079, loss 0.0246151, acc 1
2016-09-06T01:04:51.856958: step 6080, loss 0.0286454, acc 0.98
2016-09-06T01:04:52.683904: step 6081, loss 0.00578005, acc 1
2016-09-06T01:04:53.493148: step 6082, loss 0.0195775, acc 1
2016-09-06T01:04:54.273793: step 6083, loss 0.0349531, acc 0.98
2016-09-06T01:04:55.099763: step 6084, loss 0.121992, acc 0.98
2016-09-06T01:04:55.911464: step 6085, loss 0.0520069, acc 0.98
2016-09-06T01:04:56.699723: step 6086, loss 0.0243817, acc 1
2016-09-06T01:04:57.505129: step 6087, loss 0.0329883, acc 0.98
2016-09-06T01:04:58.399391: step 6088, loss 0.0352291, acc 0.98
2016-09-06T01:04:59.217101: step 6089, loss 0.00706088, acc 1
2016-09-06T01:05:00.036857: step 6090, loss 0.00372489, acc 1
2016-09-06T01:05:00.889452: step 6091, loss 0.00553148, acc 1
2016-09-06T01:05:01.682955: step 6092, loss 0.0124253, acc 1
2016-09-06T01:05:02.484007: step 6093, loss 0.0454451, acc 0.96
2016-09-06T01:05:03.320564: step 6094, loss 0.0258068, acc 0.98
2016-09-06T01:05:04.085534: step 6095, loss 0.0140848, acc 1
2016-09-06T01:05:04.904130: step 6096, loss 0.0495461, acc 0.96
2016-09-06T01:05:05.749662: step 6097, loss 0.0728288, acc 0.98
2016-09-06T01:05:06.558475: step 6098, loss 0.0201933, acc 1
2016-09-06T01:05:07.359107: step 6099, loss 0.0940354, acc 0.92
2016-09-06T01:05:08.181799: step 6100, loss 0.031039, acc 0.98

Evaluation:
2016-09-06T01:05:11.933383: step 6100, loss 1.99806, acc 0.725141

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-6100

2016-09-06T01:05:13.740826: step 6101, loss 0.0130681, acc 1
2016-09-06T01:05:14.541148: step 6102, loss 0.05053, acc 0.98
2016-09-06T01:05:15.388320: step 6103, loss 0.0954631, acc 0.98
2016-09-06T01:05:16.182605: step 6104, loss 0.00295208, acc 1
2016-09-06T01:05:16.993253: step 6105, loss 0.107934, acc 0.98
2016-09-06T01:05:17.806351: step 6106, loss 0.0119102, acc 1
2016-09-06T01:05:18.605632: step 6107, loss 0.0360072, acc 0.98
2016-09-06T01:05:19.420338: step 6108, loss 0.0236596, acc 1
2016-09-06T01:05:20.285257: step 6109, loss 0.0167399, acc 1
2016-09-06T01:05:21.103258: step 6110, loss 0.019966, acc 1
2016-09-06T01:05:21.922479: step 6111, loss 0.0230172, acc 0.98
2016-09-06T01:05:22.773400: step 6112, loss 0.124754, acc 0.96
2016-09-06T01:05:23.567696: step 6113, loss 0.0186812, acc 1
2016-09-06T01:05:24.366496: step 6114, loss 0.0209618, acc 1
2016-09-06T01:05:25.215360: step 6115, loss 0.029386, acc 0.98
2016-09-06T01:05:26.011130: step 6116, loss 0.0213434, acc 1
2016-09-06T01:05:26.827383: step 6117, loss 0.0128991, acc 1
2016-09-06T01:05:27.643935: step 6118, loss 0.0370804, acc 0.98
2016-09-06T01:05:28.448191: step 6119, loss 0.0286347, acc 0.98
2016-09-06T01:05:29.248217: step 6120, loss 0.0410808, acc 0.98
2016-09-06T01:05:30.076094: step 6121, loss 0.0380776, acc 0.98
2016-09-06T01:05:30.875121: step 6122, loss 0.00612737, acc 1
2016-09-06T01:05:31.695351: step 6123, loss 0.0358294, acc 0.98
2016-09-06T01:05:32.512350: step 6124, loss 0.016005, acc 1
2016-09-06T01:05:33.324701: step 6125, loss 0.0167908, acc 1
2016-09-06T01:05:34.137197: step 6126, loss 0.0454185, acc 0.98
2016-09-06T01:05:34.986861: step 6127, loss 0.0535602, acc 0.96
2016-09-06T01:05:35.837274: step 6128, loss 0.0108637, acc 1
2016-09-06T01:05:36.625227: step 6129, loss 0.0325571, acc 0.98
2016-09-06T01:05:37.413546: step 6130, loss 0.0448769, acc 0.96
2016-09-06T01:05:38.239471: step 6131, loss 0.0134964, acc 1
2016-09-06T01:05:39.056803: step 6132, loss 0.113723, acc 0.96
2016-09-06T01:05:39.889540: step 6133, loss 0.00301498, acc 1
2016-09-06T01:05:40.726505: step 6134, loss 0.0302787, acc 0.98
2016-09-06T01:05:41.515285: step 6135, loss 0.00681927, acc 1
2016-09-06T01:05:42.316039: step 6136, loss 0.0317608, acc 0.98
2016-09-06T01:05:43.113219: step 6137, loss 0.00984571, acc 1
2016-09-06T01:05:43.899898: step 6138, loss 0.0670583, acc 0.98
2016-09-06T01:05:44.718278: step 6139, loss 0.0404635, acc 0.98
2016-09-06T01:05:45.552846: step 6140, loss 0.00447527, acc 1
2016-09-06T01:05:46.374743: step 6141, loss 0.017305, acc 1
2016-09-06T01:05:47.171660: step 6142, loss 0.0629331, acc 0.98
2016-09-06T01:05:47.974843: step 6143, loss 0.00866519, acc 1
2016-09-06T01:05:48.701544: step 6144, loss 0.0325679, acc 0.977273
2016-09-06T01:05:49.541285: step 6145, loss 0.0223182, acc 1
2016-09-06T01:05:50.376851: step 6146, loss 0.0190064, acc 0.98
2016-09-06T01:05:51.137264: step 6147, loss 0.00351981, acc 1
2016-09-06T01:05:51.923538: step 6148, loss 0.0054897, acc 1
2016-09-06T01:05:52.758656: step 6149, loss 0.023687, acc 0.98
2016-09-06T01:05:53.535153: step 6150, loss 0.0939037, acc 0.96
2016-09-06T01:05:54.346351: step 6151, loss 0.00804979, acc 1
2016-09-06T01:05:55.152782: step 6152, loss 0.0259262, acc 1
2016-09-06T01:05:55.932357: step 6153, loss 0.00513376, acc 1
2016-09-06T01:05:56.758204: step 6154, loss 0.0554983, acc 0.98
2016-09-06T01:05:57.581696: step 6155, loss 0.0395638, acc 0.98
2016-09-06T01:05:58.383265: step 6156, loss 0.00416361, acc 1
2016-09-06T01:05:59.190678: step 6157, loss 0.00827619, acc 1
2016-09-06T01:06:00.004099: step 6158, loss 0.0119888, acc 1
2016-09-06T01:06:00.841420: step 6159, loss 0.00378649, acc 1
2016-09-06T01:06:01.626500: step 6160, loss 0.0485518, acc 0.98
2016-09-06T01:06:02.456074: step 6161, loss 0.0101402, acc 1
2016-09-06T01:06:03.239148: step 6162, loss 0.0145147, acc 1
2016-09-06T01:06:04.039938: step 6163, loss 0.0426536, acc 0.96
2016-09-06T01:06:04.840747: step 6164, loss 0.0570811, acc 0.98
2016-09-06T01:06:05.652365: step 6165, loss 0.0204485, acc 1
2016-09-06T01:06:06.477361: step 6166, loss 0.0232736, acc 0.98
2016-09-06T01:06:07.305031: step 6167, loss 0.0149236, acc 1
2016-09-06T01:06:08.105638: step 6168, loss 0.00370576, acc 1
2016-09-06T01:06:08.889685: step 6169, loss 0.00528017, acc 1
2016-09-06T01:06:09.723040: step 6170, loss 0.0241955, acc 1
2016-09-06T01:06:10.511151: step 6171, loss 0.0155907, acc 1
2016-09-06T01:06:11.311076: step 6172, loss 0.0151125, acc 1
2016-09-06T01:06:12.142191: step 6173, loss 0.0187246, acc 0.98
2016-09-06T01:06:12.962296: step 6174, loss 0.0481191, acc 0.96
2016-09-06T01:06:13.791593: step 6175, loss 0.0200028, acc 1
2016-09-06T01:06:14.622163: step 6176, loss 0.0132347, acc 1
2016-09-06T01:06:15.435521: step 6177, loss 0.0366145, acc 1
2016-09-06T01:06:16.271585: step 6178, loss 0.0258036, acc 0.98
2016-09-06T01:06:17.107349: step 6179, loss 0.00539343, acc 1
2016-09-06T01:06:17.935481: step 6180, loss 0.0200211, acc 0.98
2016-09-06T01:06:18.758572: step 6181, loss 0.0116956, acc 1
2016-09-06T01:06:19.606121: step 6182, loss 0.0104476, acc 1
2016-09-06T01:06:20.430352: step 6183, loss 0.0125618, acc 1
2016-09-06T01:06:21.233688: step 6184, loss 0.0177774, acc 0.98
2016-09-06T01:06:22.071017: step 6185, loss 0.0190083, acc 0.98
2016-09-06T01:06:22.893558: step 6186, loss 0.0673478, acc 0.96
2016-09-06T01:06:23.742254: step 6187, loss 0.0602628, acc 0.98
2016-09-06T01:06:24.569709: step 6188, loss 0.00446225, acc 1
2016-09-06T01:06:25.374595: step 6189, loss 0.0199892, acc 0.98
2016-09-06T01:06:26.198676: step 6190, loss 0.0114639, acc 1
2016-09-06T01:06:26.995038: step 6191, loss 0.0037126, acc 1
2016-09-06T01:06:27.814905: step 6192, loss 0.0244526, acc 0.98
2016-09-06T01:06:28.606204: step 6193, loss 0.0738101, acc 0.98
2016-09-06T01:06:29.410883: step 6194, loss 0.00347474, acc 1
2016-09-06T01:06:30.228007: step 6195, loss 0.029719, acc 0.98
2016-09-06T01:06:31.035259: step 6196, loss 0.0653168, acc 0.96
2016-09-06T01:06:31.837721: step 6197, loss 0.142316, acc 0.96
2016-09-06T01:06:32.651192: step 6198, loss 0.00797748, acc 1
2016-09-06T01:06:33.444858: step 6199, loss 0.00681401, acc 1
2016-09-06T01:06:34.246779: step 6200, loss 0.0167794, acc 1

Evaluation:
2016-09-06T01:06:37.973944: step 6200, loss 2.02856, acc 0.730769

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-6200

2016-09-06T01:06:39.888466: step 6201, loss 0.0216996, acc 0.98
2016-09-06T01:06:40.704615: step 6202, loss 0.0275497, acc 1
2016-09-06T01:06:41.535111: step 6203, loss 0.00569364, acc 1
2016-09-06T01:06:42.384902: step 6204, loss 0.0129774, acc 1
2016-09-06T01:06:43.145281: step 6205, loss 0.039695, acc 0.98
2016-09-06T01:06:43.947514: step 6206, loss 0.00817809, acc 1
2016-09-06T01:06:44.769586: step 6207, loss 0.00516719, acc 1
2016-09-06T01:06:45.559538: step 6208, loss 0.0293072, acc 0.98
2016-09-06T01:06:46.378064: step 6209, loss 0.00384275, acc 1
2016-09-06T01:06:47.180740: step 6210, loss 0.0091287, acc 1
2016-09-06T01:06:47.986367: step 6211, loss 0.0154458, acc 1
2016-09-06T01:06:48.801909: step 6212, loss 0.0319149, acc 0.98
2016-09-06T01:06:49.630067: step 6213, loss 0.0495027, acc 0.98
2016-09-06T01:06:50.436281: step 6214, loss 0.0426507, acc 0.96
2016-09-06T01:06:51.228586: step 6215, loss 0.0168447, acc 1
2016-09-06T01:06:52.040677: step 6216, loss 0.0328551, acc 0.96
2016-09-06T01:06:52.841424: step 6217, loss 0.00483793, acc 1
2016-09-06T01:06:53.615168: step 6218, loss 0.00335505, acc 1
2016-09-06T01:06:54.431116: step 6219, loss 0.00288415, acc 1
2016-09-06T01:06:55.200675: step 6220, loss 0.0175864, acc 1
2016-09-06T01:06:56.005985: step 6221, loss 0.0200172, acc 0.98
2016-09-06T01:06:56.812184: step 6222, loss 0.105847, acc 0.96
2016-09-06T01:06:57.598767: step 6223, loss 0.0235125, acc 1
2016-09-06T01:06:58.403577: step 6224, loss 0.0279125, acc 0.98
2016-09-06T01:06:59.201197: step 6225, loss 0.00347149, acc 1
2016-09-06T01:07:00.038445: step 6226, loss 0.0684114, acc 0.96
2016-09-06T01:07:00.874265: step 6227, loss 0.0759714, acc 0.94
2016-09-06T01:07:01.674919: step 6228, loss 0.00673715, acc 1
2016-09-06T01:07:02.501861: step 6229, loss 0.086258, acc 0.96
2016-09-06T01:07:03.333017: step 6230, loss 0.0341279, acc 0.98
2016-09-06T01:07:04.156031: step 6231, loss 0.00299679, acc 1
2016-09-06T01:07:04.956647: step 6232, loss 0.0220092, acc 1
2016-09-06T01:07:05.750224: step 6233, loss 0.0192417, acc 1
2016-09-06T01:07:06.553526: step 6234, loss 0.00298632, acc 1
2016-09-06T01:07:07.338212: step 6235, loss 0.0152047, acc 1
2016-09-06T01:07:08.123932: step 6236, loss 0.00279868, acc 1
2016-09-06T01:07:08.974064: step 6237, loss 0.0124476, acc 1
2016-09-06T01:07:09.772569: step 6238, loss 0.0301483, acc 0.98
2016-09-06T01:07:10.575866: step 6239, loss 0.00446685, acc 1
2016-09-06T01:07:11.380696: step 6240, loss 0.0282436, acc 0.98
2016-09-06T01:07:12.165398: step 6241, loss 0.00266385, acc 1
2016-09-06T01:07:12.956789: step 6242, loss 0.0295545, acc 1
2016-09-06T01:07:13.761036: step 6243, loss 0.00961197, acc 1
2016-09-06T01:07:14.550633: step 6244, loss 0.00829148, acc 1
2016-09-06T01:07:15.341390: step 6245, loss 0.00449295, acc 1
2016-09-06T01:07:16.161670: step 6246, loss 0.0433059, acc 0.98
2016-09-06T01:07:16.944895: step 6247, loss 0.0375412, acc 0.98
2016-09-06T01:07:17.730826: step 6248, loss 0.0138821, acc 1
2016-09-06T01:07:18.536976: step 6249, loss 0.016716, acc 0.98
2016-09-06T01:07:19.327884: step 6250, loss 0.0602654, acc 0.96
2016-09-06T01:07:20.155872: step 6251, loss 0.0841997, acc 0.96
2016-09-06T01:07:21.001604: step 6252, loss 0.015008, acc 1
2016-09-06T01:07:21.804937: step 6253, loss 0.0184929, acc 1
2016-09-06T01:07:22.591515: step 6254, loss 0.00937435, acc 1
2016-09-06T01:07:23.419755: step 6255, loss 0.0461757, acc 0.98
2016-09-06T01:07:24.218248: step 6256, loss 0.0159164, acc 1
2016-09-06T01:07:25.040547: step 6257, loss 0.065209, acc 0.96
2016-09-06T01:07:25.878130: step 6258, loss 0.0115135, acc 1
2016-09-06T01:07:26.670004: step 6259, loss 0.0203143, acc 0.98
2016-09-06T01:07:27.469035: step 6260, loss 0.0313136, acc 0.98
2016-09-06T01:07:28.266859: step 6261, loss 0.0889518, acc 0.96
2016-09-06T01:07:29.052496: step 6262, loss 0.00827796, acc 1
2016-09-06T01:07:29.859073: step 6263, loss 0.0110037, acc 1
2016-09-06T01:07:30.664745: step 6264, loss 0.031571, acc 0.98
2016-09-06T01:07:31.463823: step 6265, loss 0.0309256, acc 1
2016-09-06T01:07:32.259338: step 6266, loss 0.0033263, acc 1
2016-09-06T01:07:33.076285: step 6267, loss 0.0173032, acc 1
2016-09-06T01:07:33.854793: step 6268, loss 0.0382012, acc 0.98
2016-09-06T01:07:34.685293: step 6269, loss 0.0210052, acc 1
2016-09-06T01:07:35.516033: step 6270, loss 0.0344621, acc 0.98
2016-09-06T01:07:36.315202: step 6271, loss 0.0499239, acc 0.98
2016-09-06T01:07:37.165814: step 6272, loss 0.0818491, acc 0.98
2016-09-06T01:07:37.974786: step 6273, loss 0.182631, acc 0.98
2016-09-06T01:07:38.767894: step 6274, loss 0.0515173, acc 0.98
2016-09-06T01:07:39.590407: step 6275, loss 0.00394077, acc 1
2016-09-06T01:07:40.424044: step 6276, loss 0.005391, acc 1
2016-09-06T01:07:41.206354: step 6277, loss 0.0290789, acc 0.98
2016-09-06T01:07:41.996884: step 6278, loss 0.0356683, acc 1
2016-09-06T01:07:42.788762: step 6279, loss 0.0399983, acc 0.98
2016-09-06T01:07:43.567449: step 6280, loss 0.00573515, acc 1
2016-09-06T01:07:44.392935: step 6281, loss 0.0114849, acc 1
2016-09-06T01:07:45.180896: step 6282, loss 0.0294845, acc 0.98
2016-09-06T01:07:45.965770: step 6283, loss 0.00296107, acc 1
2016-09-06T01:07:46.780763: step 6284, loss 0.0356624, acc 1
2016-09-06T01:07:47.597726: step 6285, loss 0.0424033, acc 0.98
2016-09-06T01:07:48.401691: step 6286, loss 0.0067078, acc 1
2016-09-06T01:07:49.201918: step 6287, loss 0.00712163, acc 1
2016-09-06T01:07:50.033290: step 6288, loss 0.00731327, acc 1
2016-09-06T01:07:50.832008: step 6289, loss 0.0203591, acc 0.98
2016-09-06T01:07:51.632463: step 6290, loss 0.0361285, acc 0.98
2016-09-06T01:07:52.443688: step 6291, loss 0.00407251, acc 1
2016-09-06T01:07:53.216713: step 6292, loss 0.0660676, acc 0.98
2016-09-06T01:07:54.028694: step 6293, loss 0.030543, acc 1
2016-09-06T01:07:54.878662: step 6294, loss 0.0724991, acc 0.98
2016-09-06T01:07:55.684615: step 6295, loss 0.00717954, acc 1
2016-09-06T01:07:56.480861: step 6296, loss 0.00286605, acc 1
2016-09-06T01:07:57.290907: step 6297, loss 0.0330909, acc 0.98
2016-09-06T01:07:58.049521: step 6298, loss 0.00976727, acc 1
2016-09-06T01:07:58.855733: step 6299, loss 0.0765532, acc 0.96
2016-09-06T01:07:59.660061: step 6300, loss 0.0374573, acc 0.98

Evaluation:
2016-09-06T01:08:03.396383: step 6300, loss 1.83897, acc 0.729831

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-6300

2016-09-06T01:08:05.193399: step 6301, loss 0.0452391, acc 0.98
2016-09-06T01:08:06.034253: step 6302, loss 0.0328849, acc 0.98
2016-09-06T01:08:06.865699: step 6303, loss 0.00318359, acc 1
2016-09-06T01:08:07.681393: step 6304, loss 0.0197754, acc 1
2016-09-06T01:08:08.515281: step 6305, loss 0.0832707, acc 0.94
2016-09-06T01:08:09.347393: step 6306, loss 0.0246122, acc 1
2016-09-06T01:08:10.153685: step 6307, loss 0.022392, acc 1
2016-09-06T01:08:10.999086: step 6308, loss 0.0488198, acc 0.98
2016-09-06T01:08:11.814724: step 6309, loss 0.0170487, acc 1
2016-09-06T01:08:12.581801: step 6310, loss 0.0643179, acc 0.98
2016-09-06T01:08:13.402772: step 6311, loss 0.0107878, acc 1
2016-09-06T01:08:14.218822: step 6312, loss 0.0202583, acc 0.98
2016-09-06T01:08:15.021422: step 6313, loss 0.0520502, acc 0.96
2016-09-06T01:08:15.840268: step 6314, loss 0.0409216, acc 0.96
2016-09-06T01:08:16.652662: step 6315, loss 0.0303425, acc 0.98
2016-09-06T01:08:17.469657: step 6316, loss 0.011453, acc 1
2016-09-06T01:08:18.334484: step 6317, loss 0.0217565, acc 0.98
2016-09-06T01:08:19.166199: step 6318, loss 0.0262458, acc 0.98
2016-09-06T01:08:19.968561: step 6319, loss 0.0362232, acc 0.96
2016-09-06T01:08:20.758227: step 6320, loss 0.00384819, acc 1
2016-09-06T01:08:21.590589: step 6321, loss 0.0490437, acc 0.98
2016-09-06T01:08:22.386692: step 6322, loss 0.0591202, acc 0.98
2016-09-06T01:08:23.181210: step 6323, loss 0.0185877, acc 1
2016-09-06T01:08:24.005779: step 6324, loss 0.0188175, acc 0.98
2016-09-06T01:08:24.770480: step 6325, loss 0.0108821, acc 1
2016-09-06T01:08:25.593483: step 6326, loss 0.0740958, acc 0.96
2016-09-06T01:08:26.397404: step 6327, loss 0.0162436, acc 1
2016-09-06T01:08:27.176482: step 6328, loss 0.00323236, acc 1
2016-09-06T01:08:27.982792: step 6329, loss 0.00448041, acc 1
2016-09-06T01:08:28.804101: step 6330, loss 0.0443466, acc 0.96
2016-09-06T01:08:29.606881: step 6331, loss 0.07365, acc 0.98
2016-09-06T01:08:30.401562: step 6332, loss 0.0112766, acc 1
2016-09-06T01:08:31.228821: step 6333, loss 0.0312183, acc 1
2016-09-06T01:08:32.066203: step 6334, loss 0.100095, acc 0.96
2016-09-06T01:08:32.856431: step 6335, loss 0.0060324, acc 1
2016-09-06T01:08:33.604902: step 6336, loss 0.00294756, acc 1
2016-09-06T01:08:34.409401: step 6337, loss 0.0125863, acc 1
2016-09-06T01:08:35.222919: step 6338, loss 0.0231537, acc 0.98
2016-09-06T01:08:36.042596: step 6339, loss 0.00461451, acc 1
2016-09-06T01:08:36.826470: step 6340, loss 0.0383791, acc 0.98
2016-09-06T01:08:37.636220: step 6341, loss 0.0150757, acc 1
2016-09-06T01:08:38.423090: step 6342, loss 0.0189287, acc 1
2016-09-06T01:08:39.218330: step 6343, loss 0.0114292, acc 1
2016-09-06T01:08:40.048520: step 6344, loss 0.00739387, acc 1
2016-09-06T01:08:40.873078: step 6345, loss 0.0189265, acc 0.98
2016-09-06T01:08:41.659429: step 6346, loss 0.00426199, acc 1
2016-09-06T01:08:42.477489: step 6347, loss 0.0109109, acc 1
2016-09-06T01:08:43.289418: step 6348, loss 0.0190331, acc 0.98
2016-09-06T01:08:44.068283: step 6349, loss 0.0039947, acc 1
2016-09-06T01:08:44.888703: step 6350, loss 0.0207078, acc 0.98
2016-09-06T01:08:45.713911: step 6351, loss 0.00419829, acc 1
2016-09-06T01:08:46.489399: step 6352, loss 0.0361925, acc 1
2016-09-06T01:08:47.327369: step 6353, loss 0.0101504, acc 1
2016-09-06T01:08:48.140784: step 6354, loss 0.0265085, acc 1
2016-09-06T01:08:48.936620: step 6355, loss 0.00645165, acc 1
2016-09-06T01:08:49.732089: step 6356, loss 0.00400503, acc 1
2016-09-06T01:08:50.548998: step 6357, loss 0.0115051, acc 1
2016-09-06T01:08:51.321934: step 6358, loss 0.0206281, acc 1
2016-09-06T01:08:52.136138: step 6359, loss 0.0119549, acc 1
2016-09-06T01:08:52.954529: step 6360, loss 0.0692546, acc 0.98
2016-09-06T01:08:53.756352: step 6361, loss 0.0210592, acc 1
2016-09-06T01:08:54.553313: step 6362, loss 0.0926532, acc 0.98
2016-09-06T01:08:55.357709: step 6363, loss 0.0181624, acc 1
2016-09-06T01:08:56.160908: step 6364, loss 0.00815056, acc 1
2016-09-06T01:08:56.988089: step 6365, loss 0.0224578, acc 1
2016-09-06T01:08:57.827253: step 6366, loss 0.0184149, acc 1
2016-09-06T01:08:58.622735: step 6367, loss 0.0184174, acc 0.98
2016-09-06T01:08:59.432238: step 6368, loss 0.0106356, acc 1
2016-09-06T01:09:00.260380: step 6369, loss 0.0155385, acc 1
2016-09-06T01:09:01.031016: step 6370, loss 0.0285372, acc 0.98
2016-09-06T01:09:01.832766: step 6371, loss 0.00454675, acc 1
2016-09-06T01:09:02.676429: step 6372, loss 0.0422274, acc 1
2016-09-06T01:09:03.452916: step 6373, loss 0.00301303, acc 1
2016-09-06T01:09:04.280737: step 6374, loss 0.0687238, acc 0.98
2016-09-06T01:09:05.107552: step 6375, loss 0.0305156, acc 0.98
2016-09-06T01:09:05.937047: step 6376, loss 0.00576619, acc 1
2016-09-06T01:09:06.752621: step 6377, loss 0.111863, acc 0.96
2016-09-06T01:09:07.589991: step 6378, loss 0.00470098, acc 1
2016-09-06T01:09:08.385737: step 6379, loss 0.0594519, acc 0.96
2016-09-06T01:09:09.205963: step 6380, loss 0.00300532, acc 1
2016-09-06T01:09:10.042940: step 6381, loss 0.00256197, acc 1
2016-09-06T01:09:10.850609: step 6382, loss 0.142765, acc 0.96
2016-09-06T01:09:11.639209: step 6383, loss 0.00422794, acc 1
2016-09-06T01:09:12.484935: step 6384, loss 0.0328121, acc 0.98
2016-09-06T01:09:13.294552: step 6385, loss 0.0335082, acc 1
2016-09-06T01:09:14.107972: step 6386, loss 0.00459651, acc 1
2016-09-06T01:09:14.959203: step 6387, loss 0.00354061, acc 1
2016-09-06T01:09:15.741647: step 6388, loss 0.0543943, acc 0.96
2016-09-06T01:09:16.587300: step 6389, loss 0.0334427, acc 0.98
2016-09-06T01:09:17.398851: step 6390, loss 0.00968149, acc 1
2016-09-06T01:09:18.201727: step 6391, loss 0.0536738, acc 0.96
2016-09-06T01:09:19.013836: step 6392, loss 0.0280961, acc 1
2016-09-06T01:09:19.850550: step 6393, loss 0.00739676, acc 1
2016-09-06T01:09:20.688065: step 6394, loss 0.0309124, acc 1
2016-09-06T01:09:21.519164: step 6395, loss 0.0429459, acc 0.98
2016-09-06T01:09:22.334315: step 6396, loss 0.00447953, acc 1
2016-09-06T01:09:23.140692: step 6397, loss 0.0538229, acc 0.98
2016-09-06T01:09:23.954439: step 6398, loss 0.00395087, acc 1
2016-09-06T01:09:24.790375: step 6399, loss 0.00526688, acc 1
2016-09-06T01:09:25.630279: step 6400, loss 0.021373, acc 0.98

Evaluation:
2016-09-06T01:09:29.340823: step 6400, loss 1.85951, acc 0.725141

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-6400

2016-09-06T01:09:31.181370: step 6401, loss 0.00897703, acc 1
2016-09-06T01:09:32.015919: step 6402, loss 0.0517969, acc 0.96
2016-09-06T01:09:32.823117: step 6403, loss 0.00522872, acc 1
2016-09-06T01:09:33.625618: step 6404, loss 0.00458721, acc 1
2016-09-06T01:09:34.457046: step 6405, loss 0.0211345, acc 0.98
2016-09-06T01:09:35.264657: step 6406, loss 0.015572, acc 1
2016-09-06T01:09:36.091872: step 6407, loss 0.00321464, acc 1
2016-09-06T01:09:36.922739: step 6408, loss 0.0379643, acc 0.98
2016-09-06T01:09:37.728111: step 6409, loss 0.0150794, acc 1
2016-09-06T01:09:38.505362: step 6410, loss 0.0139334, acc 1
2016-09-06T01:09:39.319897: step 6411, loss 0.0359504, acc 0.98
2016-09-06T01:09:40.178515: step 6412, loss 0.0167495, acc 0.98
2016-09-06T01:09:40.949747: step 6413, loss 0.0389174, acc 0.98
2016-09-06T01:09:41.769491: step 6414, loss 0.0769545, acc 0.96
2016-09-06T01:09:42.593617: step 6415, loss 0.0392272, acc 0.98
2016-09-06T01:09:43.373074: step 6416, loss 0.00976578, acc 1
2016-09-06T01:09:44.161510: step 6417, loss 0.00230446, acc 1
2016-09-06T01:09:44.967759: step 6418, loss 0.0282081, acc 0.98
2016-09-06T01:09:45.763672: step 6419, loss 0.014399, acc 1
2016-09-06T01:09:46.558780: step 6420, loss 0.00274053, acc 1
2016-09-06T01:09:47.345481: step 6421, loss 0.0233571, acc 1
2016-09-06T01:09:48.143490: step 6422, loss 0.00381656, acc 1
2016-09-06T01:09:48.955462: step 6423, loss 0.00341295, acc 1
2016-09-06T01:09:49.753457: step 6424, loss 0.00875308, acc 1
2016-09-06T01:09:50.548695: step 6425, loss 0.00452071, acc 1
2016-09-06T01:09:51.372684: step 6426, loss 0.00242656, acc 1
2016-09-06T01:09:52.185256: step 6427, loss 0.0381289, acc 0.98
2016-09-06T01:09:53.010459: step 6428, loss 0.02169, acc 0.98
2016-09-06T01:09:53.814355: step 6429, loss 0.0127882, acc 1
2016-09-06T01:09:54.641654: step 6430, loss 0.00484232, acc 1
2016-09-06T01:09:55.456919: step 6431, loss 0.0115182, acc 1
2016-09-06T01:09:56.248606: step 6432, loss 0.0168917, acc 0.98
2016-09-06T01:09:57.074819: step 6433, loss 0.00236103, acc 1
2016-09-06T01:09:57.851083: step 6434, loss 0.0595812, acc 0.98
2016-09-06T01:09:58.664670: step 6435, loss 0.0125643, acc 1
2016-09-06T01:09:59.483352: step 6436, loss 0.00334213, acc 1
2016-09-06T01:10:00.276290: step 6437, loss 0.0146451, acc 1
2016-09-06T01:10:01.071023: step 6438, loss 0.00333909, acc 1
2016-09-06T01:10:01.886014: step 6439, loss 0.00369551, acc 1
2016-09-06T01:10:02.661979: step 6440, loss 0.180276, acc 0.98
2016-09-06T01:10:03.468011: step 6441, loss 0.0398535, acc 0.96
2016-09-06T01:10:04.293917: step 6442, loss 0.00407148, acc 1
2016-09-06T01:10:05.097282: step 6443, loss 0.0184165, acc 1
2016-09-06T01:10:05.883844: step 6444, loss 0.0586829, acc 0.96
2016-09-06T01:10:06.718831: step 6445, loss 0.0255619, acc 0.98
2016-09-06T01:10:07.507078: step 6446, loss 0.0336046, acc 0.98
2016-09-06T01:10:08.305356: step 6447, loss 0.00271524, acc 1
2016-09-06T01:10:09.117430: step 6448, loss 0.0260351, acc 1
2016-09-06T01:10:09.891872: step 6449, loss 0.00794275, acc 1
2016-09-06T01:10:10.709545: step 6450, loss 0.0186138, acc 0.98
2016-09-06T01:10:11.518702: step 6451, loss 0.017723, acc 0.98
2016-09-06T01:10:12.344727: step 6452, loss 0.00689853, acc 1
2016-09-06T01:10:13.140374: step 6453, loss 0.0486983, acc 0.98
2016-09-06T01:10:13.954023: step 6454, loss 0.015034, acc 1
2016-09-06T01:10:14.737320: step 6455, loss 0.00879302, acc 1
2016-09-06T01:10:15.548243: step 6456, loss 0.0264766, acc 0.98
2016-09-06T01:10:16.363248: step 6457, loss 0.0145019, acc 1
2016-09-06T01:10:17.156324: step 6458, loss 0.00512747, acc 1
2016-09-06T01:10:17.958254: step 6459, loss 0.00298499, acc 1
2016-09-06T01:10:18.777678: step 6460, loss 0.00873659, acc 1
2016-09-06T01:10:19.565428: step 6461, loss 0.0324597, acc 1
2016-09-06T01:10:20.368202: step 6462, loss 0.0312275, acc 1
2016-09-06T01:10:21.204698: step 6463, loss 0.0133392, acc 1
2016-09-06T01:10:22.005956: step 6464, loss 0.00277963, acc 1
2016-09-06T01:10:22.819036: step 6465, loss 0.00521786, acc 1
2016-09-06T01:10:23.645584: step 6466, loss 0.060124, acc 0.98
2016-09-06T01:10:24.440901: step 6467, loss 0.00532722, acc 1
2016-09-06T01:10:25.253987: step 6468, loss 0.00999714, acc 1
2016-09-06T01:10:26.056217: step 6469, loss 0.0112559, acc 1
2016-09-06T01:10:26.827227: step 6470, loss 0.0219144, acc 1
2016-09-06T01:10:27.638618: step 6471, loss 0.0272893, acc 1
2016-09-06T01:10:28.439718: step 6472, loss 0.00408772, acc 1
2016-09-06T01:10:29.246867: step 6473, loss 0.0121754, acc 1
2016-09-06T01:10:30.046821: step 6474, loss 0.119757, acc 0.94
2016-09-06T01:10:30.897378: step 6475, loss 0.00393934, acc 1
2016-09-06T01:10:31.694048: step 6476, loss 0.0221404, acc 1
2016-09-06T01:10:32.509423: step 6477, loss 0.0305974, acc 0.98
2016-09-06T01:10:33.333455: step 6478, loss 0.0147207, acc 1
2016-09-06T01:10:34.114701: step 6479, loss 0.0216026, acc 0.98
2016-09-06T01:10:34.911990: step 6480, loss 0.0315069, acc 0.98
2016-09-06T01:10:35.712532: step 6481, loss 0.0170661, acc 0.98
2016-09-06T01:10:36.510830: step 6482, loss 0.0255276, acc 0.98
2016-09-06T01:10:37.301018: step 6483, loss 0.015173, acc 1
2016-09-06T01:10:38.091150: step 6484, loss 0.00619103, acc 1
2016-09-06T01:10:38.907790: step 6485, loss 0.0181384, acc 1
2016-09-06T01:10:39.708982: step 6486, loss 0.0110928, acc 1
2016-09-06T01:10:40.516119: step 6487, loss 0.027247, acc 0.98
2016-09-06T01:10:41.334665: step 6488, loss 0.00369158, acc 1
2016-09-06T01:10:42.157447: step 6489, loss 0.062277, acc 0.98
2016-09-06T01:10:42.960682: step 6490, loss 0.0644006, acc 0.98
2016-09-06T01:10:43.741403: step 6491, loss 0.0247261, acc 1
2016-09-06T01:10:44.591355: step 6492, loss 0.0274465, acc 0.98
2016-09-06T01:10:45.402395: step 6493, loss 0.0148572, acc 1
2016-09-06T01:10:46.206554: step 6494, loss 0.00457977, acc 1
2016-09-06T01:10:47.013684: step 6495, loss 0.018093, acc 0.98
2016-09-06T01:10:47.849025: step 6496, loss 0.0245308, acc 0.98
2016-09-06T01:10:48.641441: step 6497, loss 0.0723564, acc 0.98
2016-09-06T01:10:49.431523: step 6498, loss 0.0046606, acc 1
2016-09-06T01:10:50.254564: step 6499, loss 0.00841625, acc 1
2016-09-06T01:10:51.017216: step 6500, loss 0.0168604, acc 1

Evaluation:
2016-09-06T01:10:54.748115: step 6500, loss 2.11836, acc 0.709193

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-6500

2016-09-06T01:10:56.646826: step 6501, loss 0.00278046, acc 1
2016-09-06T01:10:57.460398: step 6502, loss 0.0251858, acc 1
2016-09-06T01:10:58.273701: step 6503, loss 0.0284824, acc 0.98
2016-09-06T01:10:59.111491: step 6504, loss 0.0168105, acc 1
2016-09-06T01:10:59.962707: step 6505, loss 0.0260861, acc 1
2016-09-06T01:11:00.748001: step 6506, loss 0.131245, acc 0.98
2016-09-06T01:11:01.526358: step 6507, loss 0.00502804, acc 1
2016-09-06T01:11:02.330297: step 6508, loss 0.00536251, acc 1
2016-09-06T01:11:03.131281: step 6509, loss 0.017613, acc 0.98
2016-09-06T01:11:03.945714: step 6510, loss 0.0313868, acc 1
2016-09-06T01:11:04.775561: step 6511, loss 0.0187877, acc 1
2016-09-06T01:11:05.554162: step 6512, loss 0.0188779, acc 1
2016-09-06T01:11:06.347154: step 6513, loss 0.0243095, acc 1
2016-09-06T01:11:07.163786: step 6514, loss 0.0427633, acc 0.98
2016-09-06T01:11:07.962447: step 6515, loss 0.0196006, acc 0.98
2016-09-06T01:11:08.764614: step 6516, loss 0.0232179, acc 1
2016-09-06T01:11:09.594951: step 6517, loss 0.0251086, acc 1
2016-09-06T01:11:10.396083: step 6518, loss 0.124478, acc 0.96
2016-09-06T01:11:11.202145: step 6519, loss 0.00782546, acc 1
2016-09-06T01:11:12.012054: step 6520, loss 0.0165966, acc 1
2016-09-06T01:11:12.815433: step 6521, loss 0.0308169, acc 0.98
2016-09-06T01:11:13.614337: step 6522, loss 0.038, acc 0.98
2016-09-06T01:11:14.458826: step 6523, loss 0.0168451, acc 1
2016-09-06T01:11:15.255635: step 6524, loss 0.00344754, acc 1
2016-09-06T01:11:16.053412: step 6525, loss 0.028676, acc 0.98
2016-09-06T01:11:16.836644: step 6526, loss 0.0282562, acc 0.98
2016-09-06T01:11:17.640536: step 6527, loss 0.0238134, acc 0.98
2016-09-06T01:11:18.402060: step 6528, loss 0.00450213, acc 1
2016-09-06T01:11:19.242294: step 6529, loss 0.00321973, acc 1
2016-09-06T01:11:20.036448: step 6530, loss 0.0472845, acc 0.98
2016-09-06T01:11:20.874124: step 6531, loss 0.0026509, acc 1
2016-09-06T01:11:21.719169: step 6532, loss 0.0185322, acc 1
2016-09-06T01:11:22.514660: step 6533, loss 0.0310518, acc 0.98
2016-09-06T01:11:23.311985: step 6534, loss 0.00484694, acc 1
2016-09-06T01:11:24.142831: step 6535, loss 0.00334946, acc 1
2016-09-06T01:11:24.924338: step 6536, loss 0.0164397, acc 1
2016-09-06T01:11:25.721417: step 6537, loss 0.0229253, acc 1
2016-09-06T01:11:26.547366: step 6538, loss 0.00278896, acc 1
2016-09-06T01:11:27.358107: step 6539, loss 0.0252812, acc 0.98
2016-09-06T01:11:28.168926: step 6540, loss 0.0206339, acc 1
2016-09-06T01:11:28.985678: step 6541, loss 0.0327964, acc 0.98
2016-09-06T01:11:29.781064: step 6542, loss 0.00563546, acc 1
2016-09-06T01:11:30.587766: step 6543, loss 0.00922375, acc 1
2016-09-06T01:11:31.405529: step 6544, loss 0.0606746, acc 0.96
2016-09-06T01:11:32.210568: step 6545, loss 0.00791214, acc 1
2016-09-06T01:11:33.011645: step 6546, loss 0.112368, acc 0.98
2016-09-06T01:11:33.843118: step 6547, loss 0.0041806, acc 1
2016-09-06T01:11:34.691343: step 6548, loss 0.00803236, acc 1
2016-09-06T01:11:35.525339: step 6549, loss 0.00719993, acc 1
2016-09-06T01:11:36.341340: step 6550, loss 0.00929745, acc 1
2016-09-06T01:11:37.139831: step 6551, loss 0.0235351, acc 0.98
2016-09-06T01:11:37.928699: step 6552, loss 0.0511771, acc 0.98
2016-09-06T01:11:38.747857: step 6553, loss 0.0330874, acc 0.98
2016-09-06T01:11:39.546419: step 6554, loss 0.0290622, acc 0.98
2016-09-06T01:11:40.340947: step 6555, loss 0.0158407, acc 1
2016-09-06T01:11:41.153782: step 6556, loss 0.00455143, acc 1
2016-09-06T01:11:41.989028: step 6557, loss 0.0355355, acc 1
2016-09-06T01:11:42.797124: step 6558, loss 0.00738538, acc 1
2016-09-06T01:11:43.615762: step 6559, loss 0.0343582, acc 0.96
2016-09-06T01:11:44.429353: step 6560, loss 0.0103701, acc 1
2016-09-06T01:11:45.247124: step 6561, loss 0.0337561, acc 0.98
2016-09-06T01:11:46.090142: step 6562, loss 0.0277401, acc 1
2016-09-06T01:11:46.868179: step 6563, loss 0.00278878, acc 1
2016-09-06T01:11:47.658009: step 6564, loss 0.0918585, acc 0.96
2016-09-06T01:11:48.502022: step 6565, loss 0.00367846, acc 1
2016-09-06T01:11:49.320530: step 6566, loss 0.0254324, acc 1
2016-09-06T01:11:50.142297: step 6567, loss 0.0311342, acc 0.98
2016-09-06T01:11:50.964415: step 6568, loss 0.00381657, acc 1
2016-09-06T01:11:51.796987: step 6569, loss 0.013521, acc 1
2016-09-06T01:11:52.634679: step 6570, loss 0.0074599, acc 1
2016-09-06T01:11:53.460542: step 6571, loss 0.0672343, acc 0.94
2016-09-06T01:11:54.260291: step 6572, loss 0.0137472, acc 1
2016-09-06T01:11:55.076156: step 6573, loss 0.00338044, acc 1
2016-09-06T01:11:55.917091: step 6574, loss 0.00825353, acc 1
2016-09-06T01:11:56.761082: step 6575, loss 0.0104546, acc 1
2016-09-06T01:11:57.542161: step 6576, loss 0.0158615, acc 1
2016-09-06T01:11:58.344188: step 6577, loss 0.0257714, acc 0.98
2016-09-06T01:11:59.169359: step 6578, loss 0.00277235, acc 1
2016-09-06T01:11:59.949786: step 6579, loss 0.0208032, acc 0.98
2016-09-06T01:12:00.793272: step 6580, loss 0.0647912, acc 0.96
2016-09-06T01:12:01.621323: step 6581, loss 0.0150295, acc 1
2016-09-06T01:12:02.414995: step 6582, loss 0.0144321, acc 1
2016-09-06T01:12:03.214596: step 6583, loss 0.0336842, acc 0.98
2016-09-06T01:12:04.033614: step 6584, loss 0.00816329, acc 1
2016-09-06T01:12:04.846943: step 6585, loss 0.00286266, acc 1
2016-09-06T01:12:05.648035: step 6586, loss 0.0215287, acc 0.98
2016-09-06T01:12:06.493412: step 6587, loss 0.0585337, acc 0.96
2016-09-06T01:12:07.315260: step 6588, loss 0.00510486, acc 1
2016-09-06T01:12:08.144471: step 6589, loss 0.177569, acc 0.96
2016-09-06T01:12:08.988284: step 6590, loss 0.00545025, acc 1
2016-09-06T01:12:09.798709: step 6591, loss 0.00388125, acc 1
2016-09-06T01:12:10.608727: step 6592, loss 0.0268988, acc 0.98
2016-09-06T01:12:11.448830: step 6593, loss 0.0171902, acc 0.98
2016-09-06T01:12:12.270394: step 6594, loss 0.0281124, acc 1
2016-09-06T01:12:13.082989: step 6595, loss 0.0126991, acc 1
2016-09-06T01:12:13.918132: step 6596, loss 0.0407236, acc 1
2016-09-06T01:12:14.737912: step 6597, loss 0.0403175, acc 0.98
2016-09-06T01:12:15.544484: step 6598, loss 0.0280314, acc 0.98
2016-09-06T01:12:16.376447: step 6599, loss 0.0114611, acc 1
2016-09-06T01:12:17.201092: step 6600, loss 0.00265854, acc 1

Evaluation:
2016-09-06T01:12:20.951810: step 6600, loss 1.95876, acc 0.719512

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-6600

2016-09-06T01:12:22.804566: step 6601, loss 0.0136069, acc 1
2016-09-06T01:12:23.681034: step 6602, loss 0.00439581, acc 1
2016-09-06T01:12:24.483505: step 6603, loss 0.0264559, acc 1
2016-09-06T01:12:25.285348: step 6604, loss 0.0669189, acc 0.98
2016-09-06T01:12:26.128076: step 6605, loss 0.0840475, acc 0.98
2016-09-06T01:12:26.977441: step 6606, loss 0.0219085, acc 1
2016-09-06T01:12:27.791844: step 6607, loss 0.0289989, acc 0.98
2016-09-06T01:12:28.618731: step 6608, loss 0.0189259, acc 0.98
2016-09-06T01:12:29.448513: step 6609, loss 0.00394045, acc 1
2016-09-06T01:12:30.283557: step 6610, loss 0.00569666, acc 1
2016-09-06T01:12:31.130682: step 6611, loss 0.0182939, acc 1
2016-09-06T01:12:31.938599: step 6612, loss 0.00388149, acc 1
2016-09-06T01:12:32.730464: step 6613, loss 0.0191617, acc 0.98
2016-09-06T01:12:33.513123: step 6614, loss 0.0199034, acc 1
2016-09-06T01:12:34.382191: step 6615, loss 0.0115638, acc 1
2016-09-06T01:12:35.205121: step 6616, loss 0.0548461, acc 0.98
2016-09-06T01:12:36.005876: step 6617, loss 0.0153111, acc 1
2016-09-06T01:12:36.838952: step 6618, loss 0.0173107, acc 0.98
2016-09-06T01:12:37.666048: step 6619, loss 0.047053, acc 0.98
2016-09-06T01:12:38.497921: step 6620, loss 0.00476019, acc 1
2016-09-06T01:12:39.348243: step 6621, loss 0.0548883, acc 0.98
2016-09-06T01:12:40.192676: step 6622, loss 0.0600906, acc 0.98
2016-09-06T01:12:40.995642: step 6623, loss 0.0155002, acc 1
2016-09-06T01:12:41.808964: step 6624, loss 0.0368078, acc 0.98
2016-09-06T01:12:42.616925: step 6625, loss 0.0045609, acc 1
2016-09-06T01:12:43.418613: step 6626, loss 0.0698116, acc 0.98
2016-09-06T01:12:44.239786: step 6627, loss 0.0166558, acc 1
2016-09-06T01:12:45.084776: step 6628, loss 0.0469817, acc 0.98
2016-09-06T01:12:45.887749: step 6629, loss 0.0236796, acc 0.98
2016-09-06T01:12:46.710738: step 6630, loss 0.0106639, acc 1
2016-09-06T01:12:47.514767: step 6631, loss 0.0239269, acc 0.98
2016-09-06T01:12:48.317664: step 6632, loss 0.0192595, acc 1
2016-09-06T01:12:49.170346: step 6633, loss 0.0171171, acc 1
2016-09-06T01:12:49.981274: step 6634, loss 0.0222552, acc 1
2016-09-06T01:12:50.775393: step 6635, loss 0.0225354, acc 1
2016-09-06T01:12:51.575835: step 6636, loss 0.00411363, acc 1
2016-09-06T01:12:52.384818: step 6637, loss 0.0136397, acc 1
2016-09-06T01:12:53.149615: step 6638, loss 0.0395579, acc 1
2016-09-06T01:12:53.962361: step 6639, loss 0.00258547, acc 1
2016-09-06T01:12:54.812791: step 6640, loss 0.072813, acc 0.98
2016-09-06T01:12:55.597963: step 6641, loss 0.114055, acc 0.98
2016-09-06T01:12:56.383687: step 6642, loss 0.0106703, acc 1
2016-09-06T01:12:57.185910: step 6643, loss 0.109658, acc 0.98
2016-09-06T01:12:57.959376: step 6644, loss 0.0347894, acc 0.98
2016-09-06T01:12:58.756771: step 6645, loss 0.0139802, acc 1
2016-09-06T01:12:59.572399: step 6646, loss 0.00279664, acc 1
2016-09-06T01:13:00.393013: step 6647, loss 0.037046, acc 0.98
2016-09-06T01:13:01.176780: step 6648, loss 0.0287103, acc 0.98
2016-09-06T01:13:01.987649: step 6649, loss 0.00669379, acc 1
2016-09-06T01:13:02.787695: step 6650, loss 0.0378523, acc 0.98
2016-09-06T01:13:03.595875: step 6651, loss 0.00357616, acc 1
2016-09-06T01:13:04.408113: step 6652, loss 0.0923057, acc 0.96
2016-09-06T01:13:05.190331: step 6653, loss 0.0297952, acc 0.98
2016-09-06T01:13:06.001660: step 6654, loss 0.0730137, acc 0.96
2016-09-06T01:13:06.809859: step 6655, loss 0.0311515, acc 0.98
2016-09-06T01:13:07.596581: step 6656, loss 0.0489603, acc 0.98
2016-09-06T01:13:08.442806: step 6657, loss 0.0504442, acc 0.98
2016-09-06T01:13:09.254176: step 6658, loss 0.0420168, acc 0.98
2016-09-06T01:13:10.039979: step 6659, loss 0.0166753, acc 1
2016-09-06T01:13:10.845646: step 6660, loss 0.0721161, acc 0.96
2016-09-06T01:13:11.657124: step 6661, loss 0.0518639, acc 0.96
2016-09-06T01:13:12.441010: step 6662, loss 0.0313531, acc 0.98
2016-09-06T01:13:13.248109: step 6663, loss 0.0307485, acc 0.98
2016-09-06T01:13:14.069283: step 6664, loss 0.0193055, acc 1
2016-09-06T01:13:14.840268: step 6665, loss 0.0178447, acc 1
2016-09-06T01:13:15.653591: step 6666, loss 0.0177782, acc 1
2016-09-06T01:13:16.453137: step 6667, loss 0.00265659, acc 1
2016-09-06T01:13:17.261670: step 6668, loss 0.0334777, acc 0.98
2016-09-06T01:13:18.150543: step 6669, loss 0.0554931, acc 0.96
2016-09-06T01:13:18.967600: step 6670, loss 0.00487894, acc 1
2016-09-06T01:13:19.736724: step 6671, loss 0.0261975, acc 0.98
2016-09-06T01:13:20.530109: step 6672, loss 0.0167675, acc 1
2016-09-06T01:13:21.349556: step 6673, loss 0.0226083, acc 0.98
2016-09-06T01:13:22.131639: step 6674, loss 0.0166082, acc 1
2016-09-06T01:13:22.941198: step 6675, loss 0.00429183, acc 1
2016-09-06T01:13:23.748152: step 6676, loss 0.00468517, acc 1
2016-09-06T01:13:24.519080: step 6677, loss 0.00622921, acc 1
2016-09-06T01:13:25.328898: step 6678, loss 0.0815184, acc 0.96
2016-09-06T01:13:26.145312: step 6679, loss 0.00334452, acc 1
2016-09-06T01:13:26.921641: step 6680, loss 0.00366285, acc 1
2016-09-06T01:13:27.729747: step 6681, loss 0.0274339, acc 1
2016-09-06T01:13:28.531890: step 6682, loss 0.0103798, acc 1
2016-09-06T01:13:29.335621: step 6683, loss 0.0108137, acc 1
2016-09-06T01:13:30.163612: step 6684, loss 0.0346888, acc 0.98
2016-09-06T01:13:31.048001: step 6685, loss 0.0156529, acc 1
2016-09-06T01:13:31.835231: step 6686, loss 0.0229451, acc 0.98
2016-09-06T01:13:32.628265: step 6687, loss 0.0138824, acc 1
2016-09-06T01:13:33.469877: step 6688, loss 0.0293455, acc 1
2016-09-06T01:13:34.315375: step 6689, loss 0.125766, acc 0.94
2016-09-06T01:13:35.145226: step 6690, loss 0.027255, acc 1
2016-09-06T01:13:35.984842: step 6691, loss 0.00500477, acc 1
2016-09-06T01:13:36.773380: step 6692, loss 0.0630588, acc 0.98
2016-09-06T01:13:37.555693: step 6693, loss 0.00284231, acc 1
2016-09-06T01:13:38.408547: step 6694, loss 0.0184207, acc 1
2016-09-06T01:13:39.219584: step 6695, loss 0.00378272, acc 1
2016-09-06T01:13:40.045105: step 6696, loss 0.0471997, acc 0.98
2016-09-06T01:13:40.850874: step 6697, loss 0.041237, acc 1
2016-09-06T01:13:41.648292: step 6698, loss 0.036982, acc 0.98
2016-09-06T01:13:42.459259: step 6699, loss 0.025586, acc 0.98
2016-09-06T01:13:43.292544: step 6700, loss 0.00253307, acc 1

Evaluation:
2016-09-06T01:13:47.016188: step 6700, loss 2.28549, acc 0.712946

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-6700

2016-09-06T01:13:49.021950: step 6701, loss 0.00685052, acc 1
2016-09-06T01:13:49.817675: step 6702, loss 0.017045, acc 0.98
2016-09-06T01:13:50.655040: step 6703, loss 0.146459, acc 0.98
2016-09-06T01:13:51.482273: step 6704, loss 0.012899, acc 1
2016-09-06T01:13:52.294727: step 6705, loss 0.0334158, acc 0.98
2016-09-06T01:13:53.118633: step 6706, loss 0.0552414, acc 0.98
2016-09-06T01:13:53.939812: step 6707, loss 0.011055, acc 1
2016-09-06T01:13:54.756504: step 6708, loss 0.0203346, acc 1
2016-09-06T01:13:55.571240: step 6709, loss 0.00431245, acc 1
2016-09-06T01:13:56.402191: step 6710, loss 0.0216494, acc 1
2016-09-06T01:13:57.205367: step 6711, loss 0.00276088, acc 1
2016-09-06T01:13:58.023197: step 6712, loss 0.0214889, acc 0.98
2016-09-06T01:13:58.868186: step 6713, loss 0.0093475, acc 1
2016-09-06T01:13:59.655370: step 6714, loss 0.015351, acc 1
2016-09-06T01:14:00.506995: step 6715, loss 0.0305799, acc 1
2016-09-06T01:14:01.350455: step 6716, loss 0.0392347, acc 0.98
2016-09-06T01:14:02.157121: step 6717, loss 0.00968271, acc 1
2016-09-06T01:14:02.999784: step 6718, loss 0.0387594, acc 0.96
2016-09-06T01:14:03.833683: step 6719, loss 0.0323297, acc 0.98
2016-09-06T01:14:04.623678: step 6720, loss 0.0103997, acc 1
2016-09-06T01:14:05.441413: step 6721, loss 0.00630444, acc 1
2016-09-06T01:14:06.256057: step 6722, loss 0.0354625, acc 0.96
2016-09-06T01:14:07.072362: step 6723, loss 0.00363359, acc 1
2016-09-06T01:14:07.900448: step 6724, loss 0.0606927, acc 0.98
2016-09-06T01:14:08.735877: step 6725, loss 0.0139206, acc 1
2016-09-06T01:14:09.537374: step 6726, loss 0.018737, acc 1
2016-09-06T01:14:10.331380: step 6727, loss 0.015212, acc 1
2016-09-06T01:14:11.141892: step 6728, loss 0.01617, acc 1
2016-09-06T01:14:11.968434: step 6729, loss 0.00360053, acc 1
2016-09-06T01:14:12.771727: step 6730, loss 0.0156627, acc 1
2016-09-06T01:14:13.593633: step 6731, loss 0.012608, acc 1
2016-09-06T01:14:14.412636: step 6732, loss 0.0522995, acc 0.96
2016-09-06T01:14:15.207389: step 6733, loss 0.0313657, acc 0.98
2016-09-06T01:14:16.054193: step 6734, loss 0.0042012, acc 1
2016-09-06T01:14:16.896647: step 6735, loss 0.0026422, acc 1
2016-09-06T01:14:17.699642: step 6736, loss 0.0660648, acc 0.96
2016-09-06T01:14:18.511050: step 6737, loss 0.0277161, acc 1
2016-09-06T01:14:19.318372: step 6738, loss 0.0025299, acc 1
2016-09-06T01:14:20.157683: step 6739, loss 0.00388702, acc 1
2016-09-06T01:14:20.947925: step 6740, loss 0.00719536, acc 1
2016-09-06T01:14:21.770607: step 6741, loss 0.00891379, acc 1
2016-09-06T01:14:22.561806: step 6742, loss 0.00582916, acc 1
2016-09-06T01:14:23.364253: step 6743, loss 0.00239404, acc 1
2016-09-06T01:14:24.197871: step 6744, loss 0.00860805, acc 1
2016-09-06T01:14:25.013712: step 6745, loss 0.0168713, acc 0.98
2016-09-06T01:14:25.811216: step 6746, loss 0.0151439, acc 1
2016-09-06T01:14:26.628689: step 6747, loss 0.00759046, acc 1
2016-09-06T01:14:27.466597: step 6748, loss 0.0240102, acc 0.98
2016-09-06T01:14:28.241532: step 6749, loss 0.0332439, acc 1
2016-09-06T01:14:29.066942: step 6750, loss 0.00242789, acc 1
2016-09-06T01:14:29.852249: step 6751, loss 0.109927, acc 0.96
2016-09-06T01:14:30.680458: step 6752, loss 0.00649728, acc 1
2016-09-06T01:14:31.490940: step 6753, loss 0.00346768, acc 1
2016-09-06T01:14:32.259041: step 6754, loss 0.00906167, acc 1
2016-09-06T01:14:33.050323: step 6755, loss 0.010669, acc 1
2016-09-06T01:14:33.882170: step 6756, loss 0.00606305, acc 1
2016-09-06T01:14:34.667368: step 6757, loss 0.00317094, acc 1
2016-09-06T01:14:35.482811: step 6758, loss 0.0465701, acc 1
2016-09-06T01:14:36.320838: step 6759, loss 0.00778414, acc 1
2016-09-06T01:14:37.082082: step 6760, loss 0.00561426, acc 1
2016-09-06T01:14:37.902053: step 6761, loss 0.00288784, acc 1
2016-09-06T01:14:38.696922: step 6762, loss 0.0190892, acc 1
2016-09-06T01:14:39.475847: step 6763, loss 0.0598111, acc 0.96
2016-09-06T01:14:40.283369: step 6764, loss 0.0604078, acc 0.98
2016-09-06T01:14:41.100191: step 6765, loss 0.0124488, acc 1
2016-09-06T01:14:41.891737: step 6766, loss 0.0175793, acc 1
2016-09-06T01:14:42.695218: step 6767, loss 0.0109438, acc 1
2016-09-06T01:14:43.503936: step 6768, loss 0.0142936, acc 1
2016-09-06T01:14:44.310281: step 6769, loss 0.00649202, acc 1
2016-09-06T01:14:45.109712: step 6770, loss 0.00317969, acc 1
2016-09-06T01:14:45.915925: step 6771, loss 0.0170248, acc 1
2016-09-06T01:14:46.704325: step 6772, loss 0.0236274, acc 0.98
2016-09-06T01:14:47.515221: step 6773, loss 0.0415449, acc 0.98
2016-09-06T01:14:48.321772: step 6774, loss 0.00546838, acc 1
2016-09-06T01:14:49.138518: step 6775, loss 0.0509068, acc 0.98
2016-09-06T01:14:49.955993: step 6776, loss 0.0813404, acc 0.96
2016-09-06T01:14:50.779911: step 6777, loss 0.00373067, acc 1
2016-09-06T01:14:51.592385: step 6778, loss 0.0562265, acc 0.98
2016-09-06T01:14:52.393343: step 6779, loss 0.024154, acc 0.98
2016-09-06T01:14:53.201744: step 6780, loss 0.00335276, acc 1
2016-09-06T01:14:54.005407: step 6781, loss 0.0169717, acc 1
2016-09-06T01:14:54.815396: step 6782, loss 0.0136951, acc 1
2016-09-06T01:14:55.647788: step 6783, loss 0.00329526, acc 1
2016-09-06T01:14:56.466012: step 6784, loss 0.0156901, acc 1
2016-09-06T01:14:57.281909: step 6785, loss 0.0212088, acc 0.98
2016-09-06T01:14:58.121326: step 6786, loss 0.0893219, acc 0.96
2016-09-06T01:14:58.930554: step 6787, loss 0.00871314, acc 1
2016-09-06T01:14:59.761462: step 6788, loss 0.00544172, acc 1
2016-09-06T01:15:00.600993: step 6789, loss 0.0033284, acc 1
2016-09-06T01:15:01.412835: step 6790, loss 0.00583563, acc 1
2016-09-06T01:15:02.236863: step 6791, loss 0.0234494, acc 0.98
2016-09-06T01:15:03.102189: step 6792, loss 0.00623174, acc 1
2016-09-06T01:15:03.895029: step 6793, loss 0.00756167, acc 1
2016-09-06T01:15:04.696182: step 6794, loss 0.0702009, acc 0.98
2016-09-06T01:15:05.535120: step 6795, loss 0.071517, acc 0.98
2016-09-06T01:15:06.338299: step 6796, loss 0.0173666, acc 1
2016-09-06T01:15:07.151433: step 6797, loss 0.0175333, acc 1
2016-09-06T01:15:07.973361: step 6798, loss 0.0888712, acc 0.94
2016-09-06T01:15:08.787737: step 6799, loss 0.0315139, acc 0.98
2016-09-06T01:15:09.599378: step 6800, loss 0.0199449, acc 0.98

Evaluation:
2016-09-06T01:15:13.389349: step 6800, loss 2.34723, acc 0.716698

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-6800

2016-09-06T01:15:15.253499: step 6801, loss 0.00899088, acc 1
2016-09-06T01:15:16.073570: step 6802, loss 0.0109392, acc 1
2016-09-06T01:15:16.864134: step 6803, loss 0.0366939, acc 1
2016-09-06T01:15:17.699766: step 6804, loss 0.00610747, acc 1
2016-09-06T01:15:18.495148: step 6805, loss 0.00476098, acc 1
2016-09-06T01:15:19.328167: step 6806, loss 0.0088397, acc 1
2016-09-06T01:15:20.154761: step 6807, loss 0.0219196, acc 0.98
2016-09-06T01:15:20.945250: step 6808, loss 0.0146162, acc 1
2016-09-06T01:15:21.757100: step 6809, loss 0.00959562, acc 1
2016-09-06T01:15:22.578611: step 6810, loss 0.0174221, acc 1
2016-09-06T01:15:23.436161: step 6811, loss 0.033577, acc 0.98
2016-09-06T01:15:24.280581: step 6812, loss 0.00613771, acc 1
2016-09-06T01:15:25.089498: step 6813, loss 0.00460001, acc 1
2016-09-06T01:15:25.894694: step 6814, loss 0.0113054, acc 1
2016-09-06T01:15:26.698711: step 6815, loss 0.00515305, acc 1
2016-09-06T01:15:27.505769: step 6816, loss 0.00321475, acc 1
2016-09-06T01:15:28.293190: step 6817, loss 0.00671055, acc 1
2016-09-06T01:15:29.087178: step 6818, loss 0.0581492, acc 0.96
2016-09-06T01:15:29.932621: step 6819, loss 0.039238, acc 0.98
2016-09-06T01:15:30.731828: step 6820, loss 0.0151782, acc 1
2016-09-06T01:15:31.527413: step 6821, loss 0.024195, acc 0.98
2016-09-06T01:15:32.346200: step 6822, loss 0.129186, acc 0.96
2016-09-06T01:15:33.145823: step 6823, loss 0.00352166, acc 1
2016-09-06T01:15:33.945140: step 6824, loss 0.00414335, acc 1
2016-09-06T01:15:34.762763: step 6825, loss 0.0722421, acc 0.96
2016-09-06T01:15:35.576681: step 6826, loss 0.0371171, acc 0.98
2016-09-06T01:15:36.404869: step 6827, loss 0.0367763, acc 0.96
2016-09-06T01:15:37.208891: step 6828, loss 0.0324538, acc 0.98
2016-09-06T01:15:38.029660: step 6829, loss 0.0874657, acc 0.94
2016-09-06T01:15:38.832185: step 6830, loss 0.0114102, acc 1
2016-09-06T01:15:39.623304: step 6831, loss 0.038111, acc 0.98
2016-09-06T01:15:40.437476: step 6832, loss 0.0443279, acc 0.98
2016-09-06T01:15:41.212460: step 6833, loss 0.112183, acc 0.98
2016-09-06T01:15:42.008709: step 6834, loss 0.00724193, acc 1
2016-09-06T01:15:42.830896: step 6835, loss 0.0831691, acc 0.96
2016-09-06T01:15:43.623586: step 6836, loss 0.0110344, acc 1
2016-09-06T01:15:44.444466: step 6837, loss 0.0237597, acc 0.98
2016-09-06T01:15:45.256103: step 6838, loss 0.00946977, acc 1
2016-09-06T01:15:46.054576: step 6839, loss 0.0219167, acc 1
2016-09-06T01:15:46.890985: step 6840, loss 0.0436409, acc 0.98
2016-09-06T01:15:47.703537: step 6841, loss 0.0362428, acc 0.98
2016-09-06T01:15:48.529978: step 6842, loss 0.00386594, acc 1
2016-09-06T01:15:49.326641: step 6843, loss 0.0168851, acc 1
2016-09-06T01:15:50.172525: step 6844, loss 0.0186356, acc 1
2016-09-06T01:15:51.001468: step 6845, loss 0.0182678, acc 0.98
2016-09-06T01:15:51.824017: step 6846, loss 0.196692, acc 0.96
2016-09-06T01:15:52.656390: step 6847, loss 0.0734824, acc 0.94
2016-09-06T01:15:53.459415: step 6848, loss 0.00382465, acc 1
2016-09-06T01:15:54.262827: step 6849, loss 0.00952925, acc 1
2016-09-06T01:15:55.071798: step 6850, loss 0.0194652, acc 1
2016-09-06T01:15:55.893561: step 6851, loss 0.0196974, acc 0.98
2016-09-06T01:15:56.705935: step 6852, loss 0.0190167, acc 1
2016-09-06T01:15:57.553064: step 6853, loss 0.0128887, acc 1
2016-09-06T01:15:58.433757: step 6854, loss 0.00530613, acc 1
2016-09-06T01:15:59.225463: step 6855, loss 0.00962211, acc 1
2016-09-06T01:16:00.067021: step 6856, loss 0.0310743, acc 0.98
2016-09-06T01:16:00.897828: step 6857, loss 0.0061408, acc 1
2016-09-06T01:16:01.692916: step 6858, loss 0.0315256, acc 0.98
2016-09-06T01:16:02.524542: step 6859, loss 0.00571277, acc 1
2016-09-06T01:16:03.383789: step 6860, loss 0.00825231, acc 1
2016-09-06T01:16:04.185357: step 6861, loss 0.0785178, acc 0.96
2016-09-06T01:16:05.004873: step 6862, loss 0.0119448, acc 1
2016-09-06T01:16:05.826955: step 6863, loss 0.0554857, acc 0.98
2016-09-06T01:16:06.599976: step 6864, loss 0.0410635, acc 0.98
2016-09-06T01:16:07.428609: step 6865, loss 0.0253204, acc 0.98
2016-09-06T01:16:08.269363: step 6866, loss 0.0451, acc 0.98
2016-09-06T01:16:09.053134: step 6867, loss 0.0547645, acc 0.96
2016-09-06T01:16:09.869584: step 6868, loss 0.00666774, acc 1
2016-09-06T01:16:10.680363: step 6869, loss 0.0302459, acc 0.98
2016-09-06T01:16:11.476422: step 6870, loss 0.0390011, acc 0.98
2016-09-06T01:16:12.270228: step 6871, loss 0.013897, acc 1
2016-09-06T01:16:13.071482: step 6872, loss 0.0143076, acc 1
2016-09-06T01:16:13.873370: step 6873, loss 0.0152449, acc 1
2016-09-06T01:16:14.709022: step 6874, loss 0.00440104, acc 1
2016-09-06T01:16:15.554970: step 6875, loss 0.0495858, acc 0.98
2016-09-06T01:16:16.354578: step 6876, loss 0.0262103, acc 0.98
2016-09-06T01:16:17.206224: step 6877, loss 0.00544875, acc 1
2016-09-06T01:16:18.020196: step 6878, loss 0.0140941, acc 1
2016-09-06T01:16:18.824719: step 6879, loss 0.062183, acc 0.98
2016-09-06T01:16:19.628859: step 6880, loss 0.0545989, acc 0.96
2016-09-06T01:16:20.459820: step 6881, loss 0.0289469, acc 1
2016-09-06T01:16:21.271624: step 6882, loss 0.0476592, acc 0.98
2016-09-06T01:16:22.099088: step 6883, loss 0.124391, acc 0.98
2016-09-06T01:16:22.911767: step 6884, loss 0.0179514, acc 1
2016-09-06T01:16:23.697516: step 6885, loss 0.00453161, acc 1
2016-09-06T01:16:24.524892: step 6886, loss 0.031591, acc 1
2016-09-06T01:16:25.353517: step 6887, loss 0.0264867, acc 0.98
2016-09-06T01:16:26.164273: step 6888, loss 0.020042, acc 1
2016-09-06T01:16:26.983153: step 6889, loss 0.152873, acc 0.98
2016-09-06T01:16:27.826687: step 6890, loss 0.00799416, acc 1
2016-09-06T01:16:28.662400: step 6891, loss 0.037291, acc 1
2016-09-06T01:16:29.464545: step 6892, loss 0.00849828, acc 1
2016-09-06T01:16:30.293720: step 6893, loss 0.00370844, acc 1
2016-09-06T01:16:31.107931: step 6894, loss 0.0512951, acc 0.98
2016-09-06T01:16:31.921129: step 6895, loss 0.104474, acc 0.98
2016-09-06T01:16:32.732135: step 6896, loss 0.00946834, acc 1
2016-09-06T01:16:33.536379: step 6897, loss 0.0144236, acc 1
2016-09-06T01:16:34.338980: step 6898, loss 0.0517865, acc 0.98
2016-09-06T01:16:35.173293: step 6899, loss 0.0241376, acc 0.98
2016-09-06T01:16:35.985216: step 6900, loss 0.0229381, acc 0.98

Evaluation:
2016-09-06T01:16:39.707911: step 6900, loss 1.90789, acc 0.732645

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-6900

2016-09-06T01:16:41.605725: step 6901, loss 0.00435199, acc 1
2016-09-06T01:16:42.448876: step 6902, loss 0.00987534, acc 1
2016-09-06T01:16:43.245189: step 6903, loss 0.00321123, acc 1
2016-09-06T01:16:44.052681: step 6904, loss 0.0166621, acc 1
2016-09-06T01:16:44.883943: step 6905, loss 0.048443, acc 0.96
2016-09-06T01:16:45.703789: step 6906, loss 0.0450609, acc 0.96
2016-09-06T01:16:46.512533: step 6907, loss 0.0479025, acc 0.98
2016-09-06T01:16:47.338711: step 6908, loss 0.00365487, acc 1
2016-09-06T01:16:48.147892: step 6909, loss 0.0426244, acc 0.96
2016-09-06T01:16:48.943670: step 6910, loss 0.0115735, acc 1
2016-09-06T01:16:49.787056: step 6911, loss 0.058098, acc 0.96
2016-09-06T01:16:50.568623: step 6912, loss 0.00301172, acc 1
2016-09-06T01:16:51.343654: step 6913, loss 0.0107442, acc 1
2016-09-06T01:16:52.189094: step 6914, loss 0.0537585, acc 0.96
2016-09-06T01:16:53.000771: step 6915, loss 0.0300341, acc 0.98
2016-09-06T01:16:53.799265: step 6916, loss 0.00632137, acc 1
2016-09-06T01:16:54.603067: step 6917, loss 0.00428312, acc 1
2016-09-06T01:16:55.451036: step 6918, loss 0.0199472, acc 1
2016-09-06T01:16:56.248839: step 6919, loss 0.00957367, acc 1
2016-09-06T01:16:57.041790: step 6920, loss 0.0422954, acc 0.96
2016-09-06T01:16:57.868832: step 6921, loss 0.0145576, acc 1
2016-09-06T01:16:58.639306: step 6922, loss 0.00865176, acc 1
2016-09-06T01:16:59.450814: step 6923, loss 0.00645871, acc 1
2016-09-06T01:17:00.291397: step 6924, loss 0.0349649, acc 0.98
2016-09-06T01:17:01.054162: step 6925, loss 0.0291914, acc 0.98
2016-09-06T01:17:01.843898: step 6926, loss 0.0129987, acc 1
2016-09-06T01:17:02.659574: step 6927, loss 0.0193198, acc 1
2016-09-06T01:17:03.467815: step 6928, loss 0.0177792, acc 1
2016-09-06T01:17:04.264361: step 6929, loss 0.00965794, acc 1
2016-09-06T01:17:05.081927: step 6930, loss 0.0117483, acc 1
2016-09-06T01:17:05.869659: step 6931, loss 0.0247913, acc 0.98
2016-09-06T01:17:06.657031: step 6932, loss 0.0592131, acc 0.96
2016-09-06T01:17:07.489874: step 6933, loss 0.00869875, acc 1
2016-09-06T01:17:08.280751: step 6934, loss 0.0123241, acc 1
2016-09-06T01:17:09.092500: step 6935, loss 0.0312101, acc 0.98
2016-09-06T01:17:09.902893: step 6936, loss 0.00445301, acc 1
2016-09-06T01:17:10.684632: step 6937, loss 0.0238234, acc 0.98
2016-09-06T01:17:11.505527: step 6938, loss 0.0207836, acc 0.98
2016-09-06T01:17:12.351975: step 6939, loss 0.0181785, acc 1
2016-09-06T01:17:13.151596: step 6940, loss 0.0111878, acc 1
2016-09-06T01:17:14.003360: step 6941, loss 0.0184527, acc 1
2016-09-06T01:17:14.812026: step 6942, loss 0.00479331, acc 1
2016-09-06T01:17:15.606109: step 6943, loss 0.0182586, acc 1
2016-09-06T01:17:16.382422: step 6944, loss 0.0716368, acc 0.96
2016-09-06T01:17:17.229965: step 6945, loss 0.0124074, acc 1
2016-09-06T01:17:18.022590: step 6946, loss 0.0598431, acc 0.98
2016-09-06T01:17:18.837071: step 6947, loss 0.0243849, acc 0.98
2016-09-06T01:17:19.642966: step 6948, loss 0.0210059, acc 0.98
2016-09-06T01:17:20.432348: step 6949, loss 0.00332339, acc 1
2016-09-06T01:17:21.216030: step 6950, loss 0.0204899, acc 0.98
2016-09-06T01:17:22.049887: step 6951, loss 0.00615659, acc 1
2016-09-06T01:17:22.828199: step 6952, loss 0.0036305, acc 1
2016-09-06T01:17:23.637117: step 6953, loss 0.00672218, acc 1
2016-09-06T01:17:24.457650: step 6954, loss 0.00483921, acc 1
2016-09-06T01:17:25.242723: step 6955, loss 0.049074, acc 0.98
2016-09-06T01:17:26.022506: step 6956, loss 0.0091574, acc 1
2016-09-06T01:17:26.853238: step 6957, loss 0.00331605, acc 1
2016-09-06T01:17:27.680299: step 6958, loss 0.0161091, acc 1
2016-09-06T01:17:28.468030: step 6959, loss 0.016548, acc 1
2016-09-06T01:17:29.308017: step 6960, loss 0.0551319, acc 0.96
2016-09-06T01:17:30.091614: step 6961, loss 0.0850456, acc 0.98
2016-09-06T01:17:30.872835: step 6962, loss 0.00306944, acc 1
2016-09-06T01:17:31.704332: step 6963, loss 0.00581903, acc 1
2016-09-06T01:17:32.479914: step 6964, loss 0.0524908, acc 0.96
2016-09-06T01:17:33.286165: step 6965, loss 0.0443572, acc 0.98
2016-09-06T01:17:34.097557: step 6966, loss 0.0302564, acc 0.98
2016-09-06T01:17:34.902690: step 6967, loss 0.0365537, acc 0.98
2016-09-06T01:17:35.714494: step 6968, loss 0.018997, acc 0.98
2016-09-06T01:17:36.544514: step 6969, loss 0.00576511, acc 1
2016-09-06T01:17:37.314054: step 6970, loss 0.0265985, acc 1
2016-09-06T01:17:38.104440: step 6971, loss 0.00286668, acc 1
2016-09-06T01:17:38.909614: step 6972, loss 0.00290517, acc 1
2016-09-06T01:17:39.695423: step 6973, loss 0.0142712, acc 1
2016-09-06T01:17:40.518423: step 6974, loss 0.0378206, acc 0.98
2016-09-06T01:17:41.337143: step 6975, loss 0.0065386, acc 1
2016-09-06T01:17:42.122625: step 6976, loss 0.0776502, acc 0.96
2016-09-06T01:17:42.933703: step 6977, loss 0.0161424, acc 1
2016-09-06T01:17:43.725441: step 6978, loss 0.0685013, acc 0.98
2016-09-06T01:17:44.529663: step 6979, loss 0.031475, acc 0.98
2016-09-06T01:17:45.345985: step 6980, loss 0.0304573, acc 0.98
2016-09-06T01:17:46.154201: step 6981, loss 0.0264906, acc 0.98
2016-09-06T01:17:46.927730: step 6982, loss 0.0212717, acc 1
2016-09-06T01:17:47.734790: step 6983, loss 0.0148109, acc 1
2016-09-06T01:17:48.518504: step 6984, loss 0.00560904, acc 1
2016-09-06T01:17:49.330868: step 6985, loss 0.048626, acc 0.96
2016-09-06T01:17:50.171443: step 6986, loss 0.0586881, acc 0.98
2016-09-06T01:17:50.997333: step 6987, loss 0.00950292, acc 1
2016-09-06T01:17:51.778054: step 6988, loss 0.0180148, acc 1
2016-09-06T01:17:52.609623: step 6989, loss 0.0165579, acc 1
2016-09-06T01:17:53.416822: step 6990, loss 0.0470699, acc 0.98
2016-09-06T01:17:54.201659: step 6991, loss 0.0247386, acc 1
2016-09-06T01:17:55.007205: step 6992, loss 0.00857213, acc 1
2016-09-06T01:17:55.820133: step 6993, loss 0.00505378, acc 1
2016-09-06T01:17:56.622124: step 6994, loss 0.0169371, acc 1
2016-09-06T01:17:57.432938: step 6995, loss 0.0582854, acc 0.96
2016-09-06T01:17:58.279792: step 6996, loss 0.00354519, acc 1
2016-09-06T01:17:59.075153: step 6997, loss 0.0865193, acc 0.96
2016-09-06T01:17:59.897452: step 6998, loss 0.0227171, acc 0.98
2016-09-06T01:18:00.723428: step 6999, loss 0.00476865, acc 1
2016-09-06T01:18:01.477851: step 7000, loss 0.0389024, acc 0.98

Evaluation:
2016-09-06T01:18:05.210527: step 7000, loss 2.35098, acc 0.719512

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-7000

2016-09-06T01:18:07.118223: step 7001, loss 0.0376259, acc 0.98
2016-09-06T01:18:07.935426: step 7002, loss 0.00945864, acc 1
2016-09-06T01:18:08.739315: step 7003, loss 0.0683329, acc 0.98
2016-09-06T01:18:09.551282: step 7004, loss 0.00939902, acc 1
2016-09-06T01:18:10.356319: step 7005, loss 0.0200119, acc 1
2016-09-06T01:18:11.170931: step 7006, loss 0.0200199, acc 1
2016-09-06T01:18:11.991907: step 7007, loss 0.00625456, acc 1
2016-09-06T01:18:12.786794: step 7008, loss 0.00308565, acc 1
2016-09-06T01:18:13.588570: step 7009, loss 0.0352676, acc 0.96
2016-09-06T01:18:14.425554: step 7010, loss 0.0333217, acc 0.98
2016-09-06T01:18:15.256084: step 7011, loss 0.0100011, acc 1
2016-09-06T01:18:16.057060: step 7012, loss 0.0107735, acc 1
2016-09-06T01:18:16.866692: step 7013, loss 0.00773912, acc 1
2016-09-06T01:18:17.654120: step 7014, loss 0.0092157, acc 1
2016-09-06T01:18:18.469427: step 7015, loss 0.00683005, acc 1
2016-09-06T01:18:19.311583: step 7016, loss 0.00514813, acc 1
2016-09-06T01:18:20.105785: step 7017, loss 0.0217579, acc 0.98
2016-09-06T01:18:20.921455: step 7018, loss 0.0798707, acc 0.98
2016-09-06T01:18:21.736328: step 7019, loss 0.0172471, acc 1
2016-09-06T01:18:22.567685: step 7020, loss 0.0519886, acc 0.96
2016-09-06T01:18:23.369748: step 7021, loss 0.0285967, acc 0.98
2016-09-06T01:18:24.172394: step 7022, loss 0.0196505, acc 0.98
2016-09-06T01:18:25.001759: step 7023, loss 0.0136874, acc 1
2016-09-06T01:18:25.796809: step 7024, loss 0.0232691, acc 1
2016-09-06T01:18:26.575036: step 7025, loss 0.0158133, acc 1
2016-09-06T01:18:27.395442: step 7026, loss 0.0306513, acc 0.98
2016-09-06T01:18:28.203035: step 7027, loss 0.0151712, acc 1
2016-09-06T01:18:29.033592: step 7028, loss 0.0458563, acc 0.98
2016-09-06T01:18:29.846518: step 7029, loss 0.0381547, acc 0.98
2016-09-06T01:18:30.635577: step 7030, loss 0.00287859, acc 1
2016-09-06T01:18:31.418797: step 7031, loss 0.0197545, acc 1
2016-09-06T01:18:32.222450: step 7032, loss 0.0182681, acc 1
2016-09-06T01:18:33.022061: step 7033, loss 0.00287535, acc 1
2016-09-06T01:18:33.830605: step 7034, loss 0.00659096, acc 1
2016-09-06T01:18:34.639067: step 7035, loss 0.00342049, acc 1
2016-09-06T01:18:35.448278: step 7036, loss 0.0076128, acc 1
2016-09-06T01:18:36.266320: step 7037, loss 0.0115012, acc 1
2016-09-06T01:18:37.095506: step 7038, loss 0.0362585, acc 0.98
2016-09-06T01:18:37.879585: step 7039, loss 0.0380048, acc 1
2016-09-06T01:18:38.670097: step 7040, loss 0.0194157, acc 1
2016-09-06T01:18:39.475839: step 7041, loss 0.012307, acc 1
2016-09-06T01:18:40.263745: step 7042, loss 0.00446859, acc 1
2016-09-06T01:18:41.083915: step 7043, loss 0.0119085, acc 1
2016-09-06T01:18:41.903093: step 7044, loss 0.0371889, acc 0.96
2016-09-06T01:18:42.705981: step 7045, loss 0.022752, acc 0.98
2016-09-06T01:18:43.512150: step 7046, loss 0.0178868, acc 1
2016-09-06T01:18:44.316182: step 7047, loss 0.0398425, acc 0.98
2016-09-06T01:18:45.101582: step 7048, loss 0.00952239, acc 1
2016-09-06T01:18:45.913801: step 7049, loss 0.0624404, acc 0.98
2016-09-06T01:18:46.709646: step 7050, loss 0.0525005, acc 0.98
2016-09-06T01:18:47.518686: step 7051, loss 0.00273218, acc 1
2016-09-06T01:18:48.317339: step 7052, loss 0.0707689, acc 0.98
2016-09-06T01:18:49.157621: step 7053, loss 0.0571099, acc 0.96
2016-09-06T01:18:49.976198: step 7054, loss 0.00395933, acc 1
2016-09-06T01:18:50.780588: step 7055, loss 0.00304578, acc 1
2016-09-06T01:18:51.622071: step 7056, loss 0.00537295, acc 1
2016-09-06T01:18:52.404677: step 7057, loss 0.015726, acc 1
2016-09-06T01:18:53.208858: step 7058, loss 0.0262446, acc 1
2016-09-06T01:18:54.033391: step 7059, loss 0.0131964, acc 1
2016-09-06T01:18:54.807413: step 7060, loss 0.00875141, acc 1
2016-09-06T01:18:55.609321: step 7061, loss 0.028543, acc 1
2016-09-06T01:18:56.420400: step 7062, loss 0.0174999, acc 1
2016-09-06T01:18:57.241473: step 7063, loss 0.00649954, acc 1
2016-09-06T01:18:58.051409: step 7064, loss 0.00660365, acc 1
2016-09-06T01:18:58.855787: step 7065, loss 0.0613652, acc 0.98
2016-09-06T01:18:59.624239: step 7066, loss 0.00283806, acc 1
2016-09-06T01:19:00.449085: step 7067, loss 0.0373755, acc 0.98
2016-09-06T01:19:01.306657: step 7068, loss 0.0474574, acc 0.96
2016-09-06T01:19:02.100543: step 7069, loss 0.00373383, acc 1
2016-09-06T01:19:02.897448: step 7070, loss 0.00461583, acc 1
2016-09-06T01:19:03.701166: step 7071, loss 0.00621013, acc 1
2016-09-06T01:19:04.461746: step 7072, loss 0.0428826, acc 0.98
2016-09-06T01:19:05.270386: step 7073, loss 0.0648812, acc 0.94
2016-09-06T01:19:06.073260: step 7074, loss 0.003812, acc 1
2016-09-06T01:19:06.865842: step 7075, loss 0.00267769, acc 1
2016-09-06T01:19:07.660183: step 7076, loss 0.0105153, acc 1
2016-09-06T01:19:08.480942: step 7077, loss 0.00453321, acc 1
2016-09-06T01:19:09.277773: step 7078, loss 0.0168166, acc 0.98
2016-09-06T01:19:10.091006: step 7079, loss 0.0274806, acc 0.98
2016-09-06T01:19:10.921811: step 7080, loss 0.0193376, acc 1
2016-09-06T01:19:11.714586: step 7081, loss 0.0584104, acc 0.96
2016-09-06T01:19:12.522671: step 7082, loss 0.0633101, acc 0.96
2016-09-06T01:19:13.331349: step 7083, loss 0.00792771, acc 1
2016-09-06T01:19:14.129205: step 7084, loss 0.0202608, acc 0.98
2016-09-06T01:19:14.912427: step 7085, loss 0.0176644, acc 1
2016-09-06T01:19:15.747693: step 7086, loss 0.0218287, acc 0.98
2016-09-06T01:19:16.554871: step 7087, loss 0.0657353, acc 0.96
2016-09-06T01:19:17.361960: step 7088, loss 0.00265298, acc 1
2016-09-06T01:19:18.201623: step 7089, loss 0.032937, acc 1
2016-09-06T01:19:18.998084: step 7090, loss 0.0839, acc 0.98
2016-09-06T01:19:19.802707: step 7091, loss 0.00398091, acc 1
2016-09-06T01:19:20.596672: step 7092, loss 0.0158812, acc 1
2016-09-06T01:19:21.380032: step 7093, loss 0.0309277, acc 0.98
2016-09-06T01:19:22.180069: step 7094, loss 0.00291899, acc 1
2016-09-06T01:19:22.989016: step 7095, loss 0.00308757, acc 1
2016-09-06T01:19:23.802898: step 7096, loss 0.0176169, acc 0.98
2016-09-06T01:19:24.612917: step 7097, loss 0.0111819, acc 1
2016-09-06T01:19:25.433514: step 7098, loss 0.0165674, acc 0.98
2016-09-06T01:19:26.237395: step 7099, loss 0.00334896, acc 1
2016-09-06T01:19:27.024152: step 7100, loss 0.0157947, acc 1

Evaluation:
2016-09-06T01:19:30.790587: step 7100, loss 2.00805, acc 0.718574

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-7100

2016-09-06T01:19:32.639282: step 7101, loss 0.0494029, acc 0.98
2016-09-06T01:19:33.455218: step 7102, loss 0.0240743, acc 1
2016-09-06T01:19:34.274736: step 7103, loss 0.0489607, acc 0.98
2016-09-06T01:19:35.051417: step 7104, loss 0.00965693, acc 1
2016-09-06T01:19:35.849067: step 7105, loss 0.00244844, acc 1
2016-09-06T01:19:36.671990: step 7106, loss 0.0212863, acc 0.98
2016-09-06T01:19:37.475837: step 7107, loss 0.00357774, acc 1
2016-09-06T01:19:38.275508: step 7108, loss 0.00571106, acc 1
2016-09-06T01:19:39.111694: step 7109, loss 0.0169952, acc 0.98
2016-09-06T01:19:39.950529: step 7110, loss 0.0201946, acc 0.98
2016-09-06T01:19:40.741527: step 7111, loss 0.00356215, acc 1
2016-09-06T01:19:41.549504: step 7112, loss 0.00491471, acc 1
2016-09-06T01:19:42.377440: step 7113, loss 0.0150761, acc 1
2016-09-06T01:19:43.164876: step 7114, loss 0.0242513, acc 0.98
2016-09-06T01:19:43.934863: step 7115, loss 0.00631461, acc 1
2016-09-06T01:19:44.777870: step 7116, loss 0.0268119, acc 0.98
2016-09-06T01:19:45.560828: step 7117, loss 0.0178951, acc 0.98
2016-09-06T01:19:46.349359: step 7118, loss 0.00823398, acc 1
2016-09-06T01:19:47.200288: step 7119, loss 0.0118867, acc 1
2016-09-06T01:19:47.994934: step 7120, loss 0.0696318, acc 0.96
2016-09-06T01:19:48.772110: step 7121, loss 0.00433975, acc 1
2016-09-06T01:19:49.596222: step 7122, loss 0.00777618, acc 1
2016-09-06T01:19:50.376235: step 7123, loss 0.00259048, acc 1
2016-09-06T01:19:51.167352: step 7124, loss 0.0823684, acc 0.98
2016-09-06T01:19:51.973486: step 7125, loss 0.00542847, acc 1
2016-09-06T01:19:52.760458: step 7126, loss 0.0763151, acc 0.96
2016-09-06T01:19:53.602768: step 7127, loss 0.0045024, acc 1
2016-09-06T01:19:54.405249: step 7128, loss 0.0526462, acc 0.96
2016-09-06T01:19:55.191347: step 7129, loss 0.0471212, acc 0.98
2016-09-06T01:19:55.991096: step 7130, loss 0.0224005, acc 0.98
2016-09-06T01:19:56.816968: step 7131, loss 0.0177805, acc 1
2016-09-06T01:19:57.605404: step 7132, loss 0.00697641, acc 1
2016-09-06T01:19:58.410609: step 7133, loss 0.0291118, acc 1
2016-09-06T01:19:59.222036: step 7134, loss 0.00770615, acc 1
2016-09-06T01:19:59.992526: step 7135, loss 0.0034395, acc 1
2016-09-06T01:20:00.851279: step 7136, loss 0.0107521, acc 1
2016-09-06T01:20:01.675537: step 7137, loss 0.0153455, acc 1
2016-09-06T01:20:02.461391: step 7138, loss 0.030486, acc 0.98
2016-09-06T01:20:03.283377: step 7139, loss 0.00546974, acc 1
2016-09-06T01:20:04.095323: step 7140, loss 0.0420505, acc 0.98
2016-09-06T01:20:04.876724: step 7141, loss 0.00784745, acc 1
2016-09-06T01:20:05.661821: step 7142, loss 0.0178972, acc 0.98
2016-09-06T01:20:06.479670: step 7143, loss 0.0259476, acc 1
2016-09-06T01:20:07.272942: step 7144, loss 0.00297567, acc 1
2016-09-06T01:20:08.069314: step 7145, loss 0.0239858, acc 0.98
2016-09-06T01:20:08.908074: step 7146, loss 0.0499163, acc 0.96
2016-09-06T01:20:09.715812: step 7147, loss 0.00906413, acc 1
2016-09-06T01:20:10.529824: step 7148, loss 0.00811196, acc 1
2016-09-06T01:20:11.373950: step 7149, loss 0.00761514, acc 1
2016-09-06T01:20:12.153274: step 7150, loss 0.00926565, acc 1
2016-09-06T01:20:12.930116: step 7151, loss 0.0171286, acc 1
2016-09-06T01:20:13.754662: step 7152, loss 0.0605603, acc 0.96
2016-09-06T01:20:14.559247: step 7153, loss 0.0458993, acc 0.98
2016-09-06T01:20:15.372616: step 7154, loss 0.014589, acc 1
2016-09-06T01:20:16.164410: step 7155, loss 0.0218486, acc 0.98
2016-09-06T01:20:16.965902: step 7156, loss 0.0397176, acc 1
2016-09-06T01:20:17.760123: step 7157, loss 0.0540829, acc 0.96
2016-09-06T01:20:18.585932: step 7158, loss 0.0620963, acc 0.98
2016-09-06T01:20:19.370163: step 7159, loss 0.0586595, acc 0.94
2016-09-06T01:20:20.177436: step 7160, loss 0.0444978, acc 0.98
2016-09-06T01:20:21.001111: step 7161, loss 0.00547576, acc 1
2016-09-06T01:20:21.790444: step 7162, loss 0.0367593, acc 0.98
2016-09-06T01:20:22.594290: step 7163, loss 0.0148787, acc 1
2016-09-06T01:20:23.422407: step 7164, loss 0.00279981, acc 1
2016-09-06T01:20:24.210120: step 7165, loss 0.0134039, acc 1
2016-09-06T01:20:25.018993: step 7166, loss 0.0127662, acc 1
2016-09-06T01:20:25.826081: step 7167, loss 0.00415467, acc 1
2016-09-06T01:20:26.628452: step 7168, loss 0.0311363, acc 0.98
2016-09-06T01:20:27.444240: step 7169, loss 0.00575975, acc 1
2016-09-06T01:20:28.248308: step 7170, loss 0.0361466, acc 0.98
2016-09-06T01:20:29.024762: step 7171, loss 0.00785819, acc 1
2016-09-06T01:20:29.822525: step 7172, loss 0.059985, acc 0.96
2016-09-06T01:20:30.645092: step 7173, loss 0.0238052, acc 0.98
2016-09-06T01:20:31.452350: step 7174, loss 0.0221709, acc 1
2016-09-06T01:20:32.291521: step 7175, loss 0.00359087, acc 1
2016-09-06T01:20:33.130598: step 7176, loss 0.014597, acc 1
2016-09-06T01:20:33.914594: step 7177, loss 0.0403484, acc 0.98
2016-09-06T01:20:34.725531: step 7178, loss 0.0258912, acc 0.98
2016-09-06T01:20:35.542920: step 7179, loss 0.0125133, acc 1
2016-09-06T01:20:36.318476: step 7180, loss 0.0236919, acc 1
2016-09-06T01:20:37.112077: step 7181, loss 0.0214349, acc 0.98
2016-09-06T01:20:37.925348: step 7182, loss 0.024386, acc 1
2016-09-06T01:20:38.708237: step 7183, loss 0.00590134, acc 1
2016-09-06T01:20:39.515329: step 7184, loss 0.00912787, acc 1
2016-09-06T01:20:40.330921: step 7185, loss 0.00432125, acc 1
2016-09-06T01:20:41.124927: step 7186, loss 0.0154973, acc 1
2016-09-06T01:20:41.985146: step 7187, loss 0.0107029, acc 1
2016-09-06T01:20:42.808618: step 7188, loss 0.00305625, acc 1
2016-09-06T01:20:43.585597: step 7189, loss 0.00316546, acc 1
2016-09-06T01:20:44.371859: step 7190, loss 0.0180785, acc 0.98
2016-09-06T01:20:45.189131: step 7191, loss 0.0715359, acc 0.98
2016-09-06T01:20:45.975818: step 7192, loss 0.010891, acc 1
2016-09-06T01:20:46.763916: step 7193, loss 0.0214808, acc 0.98
2016-09-06T01:20:47.566823: step 7194, loss 0.00437008, acc 1
2016-09-06T01:20:48.371009: step 7195, loss 0.0279514, acc 0.98
2016-09-06T01:20:49.175374: step 7196, loss 0.00284013, acc 1
2016-09-06T01:20:49.982369: step 7197, loss 0.00690416, acc 1
2016-09-06T01:20:50.773956: step 7198, loss 0.0273729, acc 0.98
2016-09-06T01:20:51.566122: step 7199, loss 0.00709158, acc 1
2016-09-06T01:20:52.382000: step 7200, loss 0.00259906, acc 1

Evaluation:
2016-09-06T01:20:56.132172: step 7200, loss 2.37124, acc 0.72045

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-7200

2016-09-06T01:20:58.000926: step 7201, loss 0.0130088, acc 1
2016-09-06T01:20:58.836650: step 7202, loss 0.0211773, acc 0.98
2016-09-06T01:20:59.630281: step 7203, loss 0.0180221, acc 0.98
2016-09-06T01:21:00.461771: step 7204, loss 0.0467353, acc 0.98
2016-09-06T01:21:01.284726: step 7205, loss 0.0145045, acc 1
2016-09-06T01:21:02.093707: step 7206, loss 0.0181823, acc 0.98
2016-09-06T01:21:02.907788: step 7207, loss 0.0030145, acc 1
2016-09-06T01:21:03.702382: step 7208, loss 0.00960102, acc 1
2016-09-06T01:21:04.522761: step 7209, loss 0.0166186, acc 1
2016-09-06T01:21:05.331374: step 7210, loss 0.0301644, acc 0.98
2016-09-06T01:21:06.145434: step 7211, loss 0.0358307, acc 0.98
2016-09-06T01:21:06.971180: step 7212, loss 0.0270949, acc 0.98
2016-09-06T01:21:07.739620: step 7213, loss 0.027674, acc 1
2016-09-06T01:21:08.591597: step 7214, loss 0.0346894, acc 1
2016-09-06T01:21:09.386524: step 7215, loss 0.0138814, acc 1
2016-09-06T01:21:10.159331: step 7216, loss 0.0369927, acc 0.98
2016-09-06T01:21:10.954625: step 7217, loss 0.0151649, acc 1
2016-09-06T01:21:11.772299: step 7218, loss 0.0355873, acc 0.98
2016-09-06T01:21:12.575729: step 7219, loss 0.0712824, acc 0.96
2016-09-06T01:21:13.388442: step 7220, loss 0.0467066, acc 0.98
2016-09-06T01:21:14.207629: step 7221, loss 0.0307282, acc 1
2016-09-06T01:21:15.006613: step 7222, loss 0.00664547, acc 1
2016-09-06T01:21:15.806463: step 7223, loss 0.0210911, acc 0.98
2016-09-06T01:21:16.604320: step 7224, loss 0.0305476, acc 0.98
2016-09-06T01:21:17.405723: step 7225, loss 0.012502, acc 1
2016-09-06T01:21:18.196568: step 7226, loss 0.00986217, acc 1
2016-09-06T01:21:19.010661: step 7227, loss 0.0170554, acc 1
2016-09-06T01:21:19.803528: step 7228, loss 0.144693, acc 0.98
2016-09-06T01:21:20.602304: step 7229, loss 0.040043, acc 0.96
2016-09-06T01:21:21.416020: step 7230, loss 0.0325087, acc 0.98
2016-09-06T01:21:22.177459: step 7231, loss 0.017186, acc 1
2016-09-06T01:21:23.017241: step 7232, loss 0.0405134, acc 0.96
2016-09-06T01:21:23.838457: step 7233, loss 0.00300017, acc 1
2016-09-06T01:21:24.625702: step 7234, loss 0.0292028, acc 1
2016-09-06T01:21:25.418009: step 7235, loss 0.00427134, acc 1
2016-09-06T01:21:26.221081: step 7236, loss 0.0110017, acc 1
2016-09-06T01:21:27.033288: step 7237, loss 0.0119813, acc 1
2016-09-06T01:21:27.871051: step 7238, loss 0.0115759, acc 1
2016-09-06T01:21:28.677509: step 7239, loss 0.0162023, acc 1
2016-09-06T01:21:29.447766: step 7240, loss 0.0320577, acc 1
2016-09-06T01:21:30.281392: step 7241, loss 0.00398827, acc 1
2016-09-06T01:21:31.107097: step 7242, loss 0.0642468, acc 0.96
2016-09-06T01:21:31.899310: step 7243, loss 0.00331124, acc 1
2016-09-06T01:21:32.690051: step 7244, loss 0.00416164, acc 1
2016-09-06T01:21:33.526910: step 7245, loss 0.0351276, acc 0.98
2016-09-06T01:21:34.306588: step 7246, loss 0.0189947, acc 1
2016-09-06T01:21:35.113581: step 7247, loss 0.0146297, acc 1
2016-09-06T01:21:35.920696: step 7248, loss 0.016545, acc 1
2016-09-06T01:21:36.723577: step 7249, loss 0.00313319, acc 1
2016-09-06T01:21:37.527392: step 7250, loss 0.0182082, acc 1
2016-09-06T01:21:38.350041: step 7251, loss 0.026769, acc 0.98
2016-09-06T01:21:39.129793: step 7252, loss 0.0424009, acc 0.98
2016-09-06T01:21:39.949027: step 7253, loss 0.0118974, acc 1
2016-09-06T01:21:40.756316: step 7254, loss 0.0141025, acc 1
2016-09-06T01:21:41.539831: step 7255, loss 0.00266777, acc 1
2016-09-06T01:21:42.352573: step 7256, loss 0.00271255, acc 1
2016-09-06T01:21:43.153980: step 7257, loss 0.0248835, acc 0.98
2016-09-06T01:21:43.977516: step 7258, loss 0.00307893, acc 1
2016-09-06T01:21:44.784998: step 7259, loss 0.00334764, acc 1
2016-09-06T01:21:45.603716: step 7260, loss 0.0258276, acc 1
2016-09-06T01:21:46.402067: step 7261, loss 0.0332666, acc 0.98
2016-09-06T01:21:47.215534: step 7262, loss 0.00532171, acc 1
2016-09-06T01:21:48.017032: step 7263, loss 0.00393789, acc 1
2016-09-06T01:21:48.805025: step 7264, loss 0.0410172, acc 0.98
2016-09-06T01:21:49.596099: step 7265, loss 0.105322, acc 0.96
2016-09-06T01:21:50.422956: step 7266, loss 0.0136796, acc 1
2016-09-06T01:21:51.222895: step 7267, loss 0.00292527, acc 1
2016-09-06T01:21:52.019809: step 7268, loss 0.0438817, acc 0.98
2016-09-06T01:21:52.843493: step 7269, loss 0.016713, acc 1
2016-09-06T01:21:53.609994: step 7270, loss 0.0223873, acc 1
2016-09-06T01:21:54.448360: step 7271, loss 0.00287198, acc 1
2016-09-06T01:21:55.278577: step 7272, loss 0.0127328, acc 1
2016-09-06T01:21:56.068127: step 7273, loss 0.164361, acc 0.96
2016-09-06T01:21:56.860633: step 7274, loss 0.0293276, acc 0.98
2016-09-06T01:21:57.670280: step 7275, loss 0.204785, acc 0.96
2016-09-06T01:21:58.478049: step 7276, loss 0.0466507, acc 0.98
2016-09-06T01:21:59.281830: step 7277, loss 0.0168425, acc 1
2016-09-06T01:22:00.088030: step 7278, loss 0.0119418, acc 1
2016-09-06T01:22:00.901395: step 7279, loss 0.0613465, acc 0.98
2016-09-06T01:22:01.722843: step 7280, loss 0.00674439, acc 1
2016-09-06T01:22:02.522980: step 7281, loss 0.0118319, acc 1
2016-09-06T01:22:03.328778: step 7282, loss 0.0357723, acc 1
2016-09-06T01:22:04.146395: step 7283, loss 0.0273886, acc 1
2016-09-06T01:22:04.962928: step 7284, loss 0.0263986, acc 0.98
2016-09-06T01:22:05.756321: step 7285, loss 0.0106649, acc 1
2016-09-06T01:22:06.538145: step 7286, loss 0.0536636, acc 1
2016-09-06T01:22:07.334801: step 7287, loss 0.00361461, acc 1
2016-09-06T01:22:08.129299: step 7288, loss 0.0256581, acc 1
2016-09-06T01:22:08.932956: step 7289, loss 0.019926, acc 1
2016-09-06T01:22:09.763627: step 7290, loss 0.0182377, acc 0.98
2016-09-06T01:22:10.573371: step 7291, loss 0.0261028, acc 0.98
2016-09-06T01:22:11.394608: step 7292, loss 0.00480582, acc 1
2016-09-06T01:22:12.233623: step 7293, loss 0.017832, acc 1
2016-09-06T01:22:13.039051: step 7294, loss 0.0210229, acc 0.98
2016-09-06T01:22:13.898784: step 7295, loss 0.00908548, acc 1
2016-09-06T01:22:14.669967: step 7296, loss 0.00378222, acc 1
2016-09-06T01:22:15.460551: step 7297, loss 0.00375328, acc 1
2016-09-06T01:22:16.251973: step 7298, loss 0.0321, acc 0.98
2016-09-06T01:22:17.050569: step 7299, loss 0.00289289, acc 1
2016-09-06T01:22:17.818702: step 7300, loss 0.0227609, acc 1

Evaluation:
2016-09-06T01:22:21.554558: step 7300, loss 2.4654, acc 0.714822

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-7300

2016-09-06T01:22:23.453927: step 7301, loss 0.0546832, acc 0.98
2016-09-06T01:22:24.249244: step 7302, loss 0.00370073, acc 1
2016-09-06T01:22:25.060924: step 7303, loss 0.00292554, acc 1
2016-09-06T01:22:25.907188: step 7304, loss 0.0553088, acc 0.96
2016-09-06T01:22:26.725465: step 7305, loss 0.00378414, acc 1
2016-09-06T01:22:27.533620: step 7306, loss 0.00301069, acc 1
2016-09-06T01:22:28.357423: step 7307, loss 0.0185705, acc 1
2016-09-06T01:22:29.177267: step 7308, loss 0.0105952, acc 1
2016-09-06T01:22:29.946935: step 7309, loss 0.0111957, acc 1
2016-09-06T01:22:30.737534: step 7310, loss 0.00453969, acc 1
2016-09-06T01:22:31.573396: step 7311, loss 0.018528, acc 1
2016-09-06T01:22:32.333410: step 7312, loss 0.0427194, acc 0.96
2016-09-06T01:22:33.141418: step 7313, loss 0.0117364, acc 1
2016-09-06T01:22:33.953655: step 7314, loss 0.00402612, acc 1
2016-09-06T01:22:34.764322: step 7315, loss 0.0715784, acc 0.94
2016-09-06T01:22:35.564615: step 7316, loss 0.0299823, acc 0.98
2016-09-06T01:22:36.372706: step 7317, loss 0.00371141, acc 1
2016-09-06T01:22:37.166044: step 7318, loss 0.121662, acc 0.98
2016-09-06T01:22:37.970646: step 7319, loss 0.0324211, acc 0.98
2016-09-06T01:22:38.792120: step 7320, loss 0.0305174, acc 0.98
2016-09-06T01:22:39.577279: step 7321, loss 0.0210938, acc 1
2016-09-06T01:22:40.399759: step 7322, loss 0.0127883, acc 1
2016-09-06T01:22:41.216373: step 7323, loss 0.00476573, acc 1
2016-09-06T01:22:41.991166: step 7324, loss 0.00360276, acc 1
2016-09-06T01:22:42.826616: step 7325, loss 0.0453863, acc 0.98
2016-09-06T01:22:43.651458: step 7326, loss 0.00989247, acc 1
2016-09-06T01:22:44.435786: step 7327, loss 0.0410142, acc 0.98
2016-09-06T01:22:45.241526: step 7328, loss 0.0574785, acc 0.98
2016-09-06T01:22:46.054111: step 7329, loss 0.0173128, acc 1
2016-09-06T01:22:46.849494: step 7330, loss 0.0217562, acc 1
2016-09-06T01:22:47.686957: step 7331, loss 0.00362344, acc 1
2016-09-06T01:22:48.512905: step 7332, loss 0.00755405, acc 1
2016-09-06T01:22:49.300788: step 7333, loss 0.0065671, acc 1
2016-09-06T01:22:50.097533: step 7334, loss 0.0193623, acc 1
2016-09-06T01:22:50.940521: step 7335, loss 0.0226579, acc 1
2016-09-06T01:22:51.725396: step 7336, loss 0.0242181, acc 0.98
2016-09-06T01:22:52.506883: step 7337, loss 0.0363727, acc 0.98
2016-09-06T01:22:53.345597: step 7338, loss 0.027251, acc 0.98
2016-09-06T01:22:54.124840: step 7339, loss 0.058345, acc 0.96
2016-09-06T01:22:54.907201: step 7340, loss 0.00255372, acc 1
2016-09-06T01:22:55.777692: step 7341, loss 0.0506121, acc 0.98
2016-09-06T01:22:56.623154: step 7342, loss 0.0101062, acc 1
2016-09-06T01:22:57.443032: step 7343, loss 0.0291658, acc 0.98
2016-09-06T01:22:58.280618: step 7344, loss 0.029592, acc 0.98
2016-09-06T01:22:59.077379: step 7345, loss 0.0287379, acc 1
2016-09-06T01:22:59.877239: step 7346, loss 0.0172497, acc 1
2016-09-06T01:23:00.739584: step 7347, loss 0.0268139, acc 0.98
2016-09-06T01:23:01.552726: step 7348, loss 0.0175723, acc 1
2016-09-06T01:23:02.357752: step 7349, loss 0.0628126, acc 0.96
2016-09-06T01:23:03.181642: step 7350, loss 0.0615427, acc 0.98
2016-09-06T01:23:03.991348: step 7351, loss 0.0183864, acc 1
2016-09-06T01:23:04.805721: step 7352, loss 0.0132493, acc 1
2016-09-06T01:23:05.657426: step 7353, loss 0.00740235, acc 1
2016-09-06T01:23:06.473953: step 7354, loss 0.0346316, acc 0.96
2016-09-06T01:23:07.276325: step 7355, loss 0.0357909, acc 0.98
2016-09-06T01:23:08.083174: step 7356, loss 0.00251439, acc 1
2016-09-06T01:23:08.906251: step 7357, loss 0.0205096, acc 1
2016-09-06T01:23:09.710344: step 7358, loss 0.0126992, acc 1
2016-09-06T01:23:10.540893: step 7359, loss 0.00380039, acc 1
2016-09-06T01:23:11.362436: step 7360, loss 0.00261313, acc 1
2016-09-06T01:23:12.151256: step 7361, loss 0.0184396, acc 1
2016-09-06T01:23:13.012151: step 7362, loss 0.0918159, acc 0.94
2016-09-06T01:23:13.817011: step 7363, loss 0.028539, acc 0.98
2016-09-06T01:23:14.584576: step 7364, loss 0.1406, acc 0.92
2016-09-06T01:23:15.431262: step 7365, loss 0.0408392, acc 0.98
2016-09-06T01:23:16.249705: step 7366, loss 0.0608252, acc 0.96
2016-09-06T01:23:17.031302: step 7367, loss 0.0160372, acc 1
2016-09-06T01:23:17.846929: step 7368, loss 0.0639092, acc 0.98
2016-09-06T01:23:18.703336: step 7369, loss 0.02041, acc 0.98
2016-09-06T01:23:19.491574: step 7370, loss 0.0119314, acc 1
2016-09-06T01:23:20.291973: step 7371, loss 0.0374388, acc 1
2016-09-06T01:23:21.100005: step 7372, loss 0.0204199, acc 1
2016-09-06T01:23:21.893456: step 7373, loss 0.102287, acc 0.98
2016-09-06T01:23:22.693784: step 7374, loss 0.0725976, acc 0.98
2016-09-06T01:23:23.519858: step 7375, loss 0.0135094, acc 1
2016-09-06T01:23:24.303352: step 7376, loss 0.0228717, acc 1
2016-09-06T01:23:25.088946: step 7377, loss 0.00351556, acc 1
2016-09-06T01:23:25.917362: step 7378, loss 0.0167131, acc 1
2016-09-06T01:23:26.696032: step 7379, loss 0.0102894, acc 1
2016-09-06T01:23:27.514240: step 7380, loss 0.00517065, acc 1
2016-09-06T01:23:28.360963: step 7381, loss 0.0207626, acc 1
2016-09-06T01:23:29.157366: step 7382, loss 0.0350258, acc 0.98
2016-09-06T01:23:29.965775: step 7383, loss 0.0354551, acc 0.98
2016-09-06T01:23:30.777966: step 7384, loss 0.0255357, acc 1
2016-09-06T01:23:31.558609: step 7385, loss 0.0101021, acc 1
2016-09-06T01:23:32.373133: step 7386, loss 0.0353639, acc 0.98
2016-09-06T01:23:33.183218: step 7387, loss 0.0128403, acc 1
2016-09-06T01:23:34.022327: step 7388, loss 0.00620881, acc 1
2016-09-06T01:23:34.829795: step 7389, loss 0.00908169, acc 1
2016-09-06T01:23:35.661210: step 7390, loss 0.00493844, acc 1
2016-09-06T01:23:36.440852: step 7391, loss 0.0233605, acc 1
2016-09-06T01:23:37.231989: step 7392, loss 0.00883157, acc 1
2016-09-06T01:23:38.044097: step 7393, loss 0.00663896, acc 1
2016-09-06T01:23:38.817746: step 7394, loss 0.109705, acc 0.98
2016-09-06T01:23:39.640616: step 7395, loss 0.0300558, acc 1
2016-09-06T01:23:40.466212: step 7396, loss 0.0229145, acc 0.98
2016-09-06T01:23:41.257282: step 7397, loss 0.0107433, acc 1
2016-09-06T01:23:42.060443: step 7398, loss 0.0517372, acc 0.98
2016-09-06T01:23:42.901305: step 7399, loss 0.0302183, acc 0.98
2016-09-06T01:23:43.682494: step 7400, loss 0.00481827, acc 1

Evaluation:
2016-09-06T01:23:47.380639: step 7400, loss 2.7189, acc 0.721388

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-7400

2016-09-06T01:23:49.320817: step 7401, loss 0.0415968, acc 0.98
2016-09-06T01:23:50.104445: step 7402, loss 0.0057508, acc 1
2016-09-06T01:23:50.923648: step 7403, loss 0.0197992, acc 1
2016-09-06T01:23:51.752318: step 7404, loss 0.0461322, acc 0.98
2016-09-06T01:23:52.549213: step 7405, loss 0.0589593, acc 0.94
2016-09-06T01:23:53.376192: step 7406, loss 0.0182226, acc 1
2016-09-06T01:23:54.175981: step 7407, loss 0.00441546, acc 1
2016-09-06T01:23:54.991738: step 7408, loss 0.0303718, acc 1
2016-09-06T01:23:55.777156: step 7409, loss 0.0290756, acc 0.98
2016-09-06T01:23:56.576577: step 7410, loss 0.0135984, acc 1
2016-09-06T01:23:57.378889: step 7411, loss 0.0161044, acc 1
2016-09-06T01:23:58.159668: step 7412, loss 0.00842464, acc 1
2016-09-06T01:23:58.981130: step 7413, loss 0.0194352, acc 0.98
2016-09-06T01:23:59.778381: step 7414, loss 0.00736827, acc 1
2016-09-06T01:24:00.586059: step 7415, loss 0.00476149, acc 1
2016-09-06T01:24:01.419594: step 7416, loss 0.0175477, acc 1
2016-09-06T01:24:02.207127: step 7417, loss 0.0415662, acc 0.98
2016-09-06T01:24:03.015286: step 7418, loss 0.0056694, acc 1
2016-09-06T01:24:03.822298: step 7419, loss 0.0118358, acc 1
2016-09-06T01:24:04.621517: step 7420, loss 0.00883669, acc 1
2016-09-06T01:24:05.439938: step 7421, loss 0.0339626, acc 1
2016-09-06T01:24:06.275467: step 7422, loss 0.00925463, acc 1
2016-09-06T01:24:07.096313: step 7423, loss 0.00368295, acc 1
2016-09-06T01:24:07.904608: step 7424, loss 0.0127043, acc 1
2016-09-06T01:24:08.716148: step 7425, loss 0.013663, acc 1
2016-09-06T01:24:09.537184: step 7426, loss 0.0213195, acc 1
2016-09-06T01:24:10.344343: step 7427, loss 0.00840147, acc 1
2016-09-06T01:24:11.155564: step 7428, loss 0.0302325, acc 0.98
2016-09-06T01:24:11.979172: step 7429, loss 0.0154553, acc 1
2016-09-06T01:24:12.767581: step 7430, loss 0.0800225, acc 0.96
2016-09-06T01:24:13.557786: step 7431, loss 0.0306584, acc 0.98
2016-09-06T01:24:14.386413: step 7432, loss 0.00594204, acc 1
2016-09-06T01:24:15.220174: step 7433, loss 0.0791171, acc 0.98
2016-09-06T01:24:16.018968: step 7434, loss 0.020108, acc 0.98
2016-09-06T01:24:16.843046: step 7435, loss 0.00329822, acc 1
2016-09-06T01:24:17.647174: step 7436, loss 0.00837626, acc 1
2016-09-06T01:24:18.451033: step 7437, loss 0.0214387, acc 1
2016-09-06T01:24:19.271390: step 7438, loss 0.00948067, acc 1
2016-09-06T01:24:20.046033: step 7439, loss 0.0140735, acc 1
2016-09-06T01:24:20.858845: step 7440, loss 0.0354956, acc 0.98
2016-09-06T01:24:21.673933: step 7441, loss 0.0219306, acc 1
2016-09-06T01:24:22.471271: step 7442, loss 0.0284853, acc 0.98
2016-09-06T01:24:23.283344: step 7443, loss 0.0259546, acc 0.98
2016-09-06T01:24:24.113158: step 7444, loss 0.032706, acc 0.98
2016-09-06T01:24:24.895963: step 7445, loss 0.0254429, acc 0.98
2016-09-06T01:24:25.700323: step 7446, loss 0.017997, acc 1
2016-09-06T01:24:26.508104: step 7447, loss 0.0283681, acc 0.98
2016-09-06T01:24:27.295326: step 7448, loss 0.00310741, acc 1
2016-09-06T01:24:28.097896: step 7449, loss 0.0587289, acc 0.98
2016-09-06T01:24:28.928063: step 7450, loss 0.00317003, acc 1
2016-09-06T01:24:29.755146: step 7451, loss 0.0124963, acc 1
2016-09-06T01:24:30.559463: step 7452, loss 0.0574967, acc 0.94
2016-09-06T01:24:31.370396: step 7453, loss 0.0206383, acc 1
2016-09-06T01:24:32.139575: step 7454, loss 0.0100905, acc 1
2016-09-06T01:24:32.944442: step 7455, loss 0.00473166, acc 1
2016-09-06T01:24:33.784790: step 7456, loss 0.0992383, acc 0.96
2016-09-06T01:24:34.569413: step 7457, loss 0.0245781, acc 1
2016-09-06T01:24:35.367761: step 7458, loss 0.0137151, acc 1
2016-09-06T01:24:36.190916: step 7459, loss 0.0609552, acc 0.96
2016-09-06T01:24:36.978555: step 7460, loss 0.00370215, acc 1
2016-09-06T01:24:37.808810: step 7461, loss 0.00766789, acc 1
2016-09-06T01:24:38.629943: step 7462, loss 0.108927, acc 0.96
2016-09-06T01:24:39.423299: step 7463, loss 0.00284734, acc 1
2016-09-06T01:24:40.186644: step 7464, loss 0.0417512, acc 1
2016-09-06T01:24:40.992698: step 7465, loss 0.0485552, acc 0.98
2016-09-06T01:24:41.774555: step 7466, loss 0.040678, acc 0.98
2016-09-06T01:24:42.599359: step 7467, loss 0.00731275, acc 1
2016-09-06T01:24:43.400799: step 7468, loss 0.0772414, acc 0.96
2016-09-06T01:24:44.168827: step 7469, loss 0.0410707, acc 0.98
2016-09-06T01:24:44.976670: step 7470, loss 0.00401681, acc 1
2016-09-06T01:24:45.798394: step 7471, loss 0.0440657, acc 0.98
2016-09-06T01:24:46.575907: step 7472, loss 0.00400343, acc 1
2016-09-06T01:24:47.392790: step 7473, loss 0.00850549, acc 1
2016-09-06T01:24:48.207056: step 7474, loss 0.0575332, acc 0.96
2016-09-06T01:24:49.046893: step 7475, loss 0.00616453, acc 1
2016-09-06T01:24:49.870253: step 7476, loss 0.0211159, acc 0.98
2016-09-06T01:24:50.695855: step 7477, loss 0.00652596, acc 1
2016-09-06T01:24:51.483333: step 7478, loss 0.0251136, acc 1
2016-09-06T01:24:52.283915: step 7479, loss 0.0116007, acc 1
2016-09-06T01:24:53.083389: step 7480, loss 0.00464031, acc 1
2016-09-06T01:24:53.893752: step 7481, loss 0.017219, acc 1
2016-09-06T01:24:54.685502: step 7482, loss 0.0461122, acc 0.98
2016-09-06T01:24:55.495487: step 7483, loss 0.0557844, acc 0.96
2016-09-06T01:24:56.293345: step 7484, loss 0.0199071, acc 0.98
2016-09-06T01:24:57.082008: step 7485, loss 0.0133729, acc 1
2016-09-06T01:24:57.876753: step 7486, loss 0.030674, acc 0.98
2016-09-06T01:24:58.662833: step 7487, loss 0.008836, acc 1
2016-09-06T01:24:59.433650: step 7488, loss 0.0186893, acc 1
2016-09-06T01:25:00.293545: step 7489, loss 0.00292079, acc 1
2016-09-06T01:25:01.070680: step 7490, loss 0.00673765, acc 1
2016-09-06T01:25:01.888625: step 7491, loss 0.00640832, acc 1
2016-09-06T01:25:02.687620: step 7492, loss 0.0123831, acc 1
2016-09-06T01:25:03.487004: step 7493, loss 0.012237, acc 1
2016-09-06T01:25:04.307487: step 7494, loss 0.00372398, acc 1
2016-09-06T01:25:05.134958: step 7495, loss 0.0147044, acc 1
2016-09-06T01:25:05.938769: step 7496, loss 0.0066507, acc 1
2016-09-06T01:25:06.776226: step 7497, loss 0.0668614, acc 0.98
2016-09-06T01:25:07.597753: step 7498, loss 0.00361485, acc 1
2016-09-06T01:25:08.369499: step 7499, loss 0.0273461, acc 0.98
2016-09-06T01:25:09.191819: step 7500, loss 0.0239705, acc 1

Evaluation:
2016-09-06T01:25:12.960310: step 7500, loss 2.54737, acc 0.714822

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-7500

2016-09-06T01:25:14.854068: step 7501, loss 0.00322515, acc 1
2016-09-06T01:25:15.677412: step 7502, loss 0.0343662, acc 0.98
2016-09-06T01:25:16.496847: step 7503, loss 0.00558196, acc 1
2016-09-06T01:25:17.298728: step 7504, loss 0.0246824, acc 1
2016-09-06T01:25:18.102907: step 7505, loss 0.0284497, acc 0.98
2016-09-06T01:25:18.925913: step 7506, loss 0.0108922, acc 1
2016-09-06T01:25:19.737364: step 7507, loss 0.00360052, acc 1
2016-09-06T01:25:20.520361: step 7508, loss 0.0144455, acc 1
2016-09-06T01:25:21.347471: step 7509, loss 0.00426869, acc 1
2016-09-06T01:25:22.113810: step 7510, loss 0.0494482, acc 0.98
2016-09-06T01:25:22.915198: step 7511, loss 0.00914357, acc 1
2016-09-06T01:25:23.754349: step 7512, loss 0.115052, acc 0.98
2016-09-06T01:25:24.545945: step 7513, loss 0.00490062, acc 1
2016-09-06T01:25:25.369094: step 7514, loss 0.0547175, acc 0.96
2016-09-06T01:25:26.189532: step 7515, loss 0.0102382, acc 1
2016-09-06T01:25:27.005492: step 7516, loss 0.00268146, acc 1
2016-09-06T01:25:27.802165: step 7517, loss 0.00765475, acc 1
2016-09-06T01:25:28.608165: step 7518, loss 0.00680274, acc 1
2016-09-06T01:25:29.395311: step 7519, loss 0.0160656, acc 1
2016-09-06T01:25:30.189442: step 7520, loss 0.00345286, acc 1
2016-09-06T01:25:31.029843: step 7521, loss 0.00361498, acc 1
2016-09-06T01:25:31.827379: step 7522, loss 0.00613398, acc 1
2016-09-06T01:25:32.612605: step 7523, loss 0.00336021, acc 1
2016-09-06T01:25:33.413453: step 7524, loss 0.0255851, acc 0.98
2016-09-06T01:25:34.218014: step 7525, loss 0.00316358, acc 1
2016-09-06T01:25:35.000048: step 7526, loss 0.0208021, acc 0.98
2016-09-06T01:25:35.841841: step 7527, loss 0.0344115, acc 0.98
2016-09-06T01:25:36.650127: step 7528, loss 0.00257636, acc 1
2016-09-06T01:25:37.459279: step 7529, loss 0.00499051, acc 1
2016-09-06T01:25:38.275918: step 7530, loss 0.00312344, acc 1
2016-09-06T01:25:39.111804: step 7531, loss 0.0524319, acc 0.96
2016-09-06T01:25:39.894769: step 7532, loss 0.0143376, acc 1
2016-09-06T01:25:40.665432: step 7533, loss 0.00266204, acc 1
2016-09-06T01:25:41.449208: step 7534, loss 0.0396928, acc 0.96
2016-09-06T01:25:42.262961: step 7535, loss 0.00587077, acc 1
2016-09-06T01:25:43.090880: step 7536, loss 0.0320554, acc 1
2016-09-06T01:25:43.875733: step 7537, loss 0.100759, acc 0.96
2016-09-06T01:25:44.674271: step 7538, loss 0.003262, acc 1
2016-09-06T01:25:45.519957: step 7539, loss 0.00995381, acc 1
2016-09-06T01:25:46.330890: step 7540, loss 0.0271416, acc 1
2016-09-06T01:25:47.137512: step 7541, loss 0.0838566, acc 0.94
2016-09-06T01:25:47.959097: step 7542, loss 0.0032246, acc 1
2016-09-06T01:25:48.776677: step 7543, loss 0.0462731, acc 0.98
2016-09-06T01:25:49.588553: step 7544, loss 0.0127063, acc 1
2016-09-06T01:25:50.386679: step 7545, loss 0.00284034, acc 1
2016-09-06T01:25:51.197301: step 7546, loss 0.028286, acc 0.98
2016-09-06T01:25:51.968859: step 7547, loss 0.0195054, acc 0.98
2016-09-06T01:25:52.780764: step 7548, loss 0.0293871, acc 0.98
2016-09-06T01:25:53.604609: step 7549, loss 0.0276842, acc 1
2016-09-06T01:25:54.385342: step 7550, loss 0.00327884, acc 1
2016-09-06T01:25:55.224868: step 7551, loss 0.00555259, acc 1
2016-09-06T01:25:56.061786: step 7552, loss 0.032162, acc 0.98
2016-09-06T01:25:56.860621: step 7553, loss 0.0124898, acc 1
2016-09-06T01:25:57.665208: step 7554, loss 0.0361016, acc 0.98
2016-09-06T01:25:58.504785: step 7555, loss 0.0280517, acc 1
2016-09-06T01:25:59.296737: step 7556, loss 0.033306, acc 1
2016-09-06T01:26:00.061733: step 7557, loss 0.00324105, acc 1
2016-09-06T01:26:00.926784: step 7558, loss 0.00526448, acc 1
2016-09-06T01:26:01.716096: step 7559, loss 0.0171859, acc 1
2016-09-06T01:26:02.521485: step 7560, loss 0.00397299, acc 1
2016-09-06T01:26:03.343002: step 7561, loss 0.0198955, acc 1
2016-09-06T01:26:04.123522: step 7562, loss 0.0184609, acc 0.98
2016-09-06T01:26:04.910851: step 7563, loss 0.0358877, acc 0.98
2016-09-06T01:26:05.723998: step 7564, loss 0.0192929, acc 0.98
2016-09-06T01:26:06.488529: step 7565, loss 0.00396945, acc 1
2016-09-06T01:26:07.295633: step 7566, loss 0.0233437, acc 1
2016-09-06T01:26:08.115956: step 7567, loss 0.0918502, acc 0.96
2016-09-06T01:26:08.909857: step 7568, loss 0.0157907, acc 1
2016-09-06T01:26:09.715167: step 7569, loss 0.00528521, acc 1
2016-09-06T01:26:10.535087: step 7570, loss 0.00518346, acc 1
2016-09-06T01:26:11.314077: step 7571, loss 0.0444419, acc 0.96
2016-09-06T01:26:12.163135: step 7572, loss 0.00351721, acc 1
2016-09-06T01:26:13.007228: step 7573, loss 0.0171229, acc 1
2016-09-06T01:26:13.777676: step 7574, loss 0.0374945, acc 0.98
2016-09-06T01:26:14.586012: step 7575, loss 0.00262928, acc 1
2016-09-06T01:26:15.425083: step 7576, loss 0.00270221, acc 1
2016-09-06T01:26:16.224310: step 7577, loss 0.00337345, acc 1
2016-09-06T01:26:16.992350: step 7578, loss 0.0248969, acc 0.98
2016-09-06T01:26:17.840181: step 7579, loss 0.0292196, acc 1
2016-09-06T01:26:18.636187: step 7580, loss 0.00319014, acc 1
2016-09-06T01:26:19.420246: step 7581, loss 0.0337565, acc 0.98
2016-09-06T01:26:20.241355: step 7582, loss 0.00267398, acc 1
2016-09-06T01:26:21.009095: step 7583, loss 0.012637, acc 1
2016-09-06T01:26:21.815661: step 7584, loss 0.00662016, acc 1
2016-09-06T01:26:22.619344: step 7585, loss 0.0435244, acc 0.98
2016-09-06T01:26:23.412795: step 7586, loss 0.0396334, acc 0.98
2016-09-06T01:26:24.203764: step 7587, loss 0.0239297, acc 0.98
2016-09-06T01:26:25.036767: step 7588, loss 0.042314, acc 0.98
2016-09-06T01:26:25.833060: step 7589, loss 0.00277111, acc 1
2016-09-06T01:26:26.628484: step 7590, loss 0.00628822, acc 1
2016-09-06T01:26:27.439598: step 7591, loss 0.0444574, acc 0.98
2016-09-06T01:26:28.237151: step 7592, loss 0.0401057, acc 0.98
2016-09-06T01:26:29.036616: step 7593, loss 0.0187394, acc 1
2016-09-06T01:26:29.875261: step 7594, loss 0.0168024, acc 1
2016-09-06T01:26:30.662639: step 7595, loss 0.0367167, acc 0.96
2016-09-06T01:26:31.473600: step 7596, loss 0.0313825, acc 1
2016-09-06T01:26:32.284871: step 7597, loss 0.0468952, acc 0.96
2016-09-06T01:26:33.095230: step 7598, loss 0.042116, acc 0.96
2016-09-06T01:26:33.915878: step 7599, loss 0.00792633, acc 1
2016-09-06T01:26:34.728638: step 7600, loss 0.0160732, acc 1

Evaluation:
2016-09-06T01:26:38.445211: step 7600, loss 2.55498, acc 0.71576

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-7600

2016-09-06T01:26:40.327571: step 7601, loss 0.0218952, acc 1
2016-09-06T01:26:41.152979: step 7602, loss 0.0363718, acc 0.98
2016-09-06T01:26:41.964413: step 7603, loss 0.0264498, acc 0.98
2016-09-06T01:26:42.768644: step 7604, loss 0.0163882, acc 1
2016-09-06T01:26:43.586298: step 7605, loss 0.00432795, acc 1
2016-09-06T01:26:44.386853: step 7606, loss 0.00690239, acc 1
2016-09-06T01:26:45.184058: step 7607, loss 0.0293676, acc 1
2016-09-06T01:26:45.987929: step 7608, loss 0.0436202, acc 0.98
2016-09-06T01:26:46.818529: step 7609, loss 0.00252619, acc 1
2016-09-06T01:26:47.621303: step 7610, loss 0.0184147, acc 0.98
2016-09-06T01:26:48.408964: step 7611, loss 0.0167904, acc 0.98
2016-09-06T01:26:49.207252: step 7612, loss 0.0103901, acc 1
2016-09-06T01:26:50.004823: step 7613, loss 0.07902, acc 0.96
2016-09-06T01:26:50.873982: step 7614, loss 0.00438793, acc 1
2016-09-06T01:26:51.699693: step 7615, loss 0.00499439, acc 1
2016-09-06T01:26:52.474927: step 7616, loss 0.0198745, acc 1
2016-09-06T01:26:53.270032: step 7617, loss 0.0365292, acc 0.98
2016-09-06T01:26:54.069206: step 7618, loss 0.0327938, acc 0.98
2016-09-06T01:26:54.866827: step 7619, loss 0.0221404, acc 0.98
2016-09-06T01:26:55.669680: step 7620, loss 0.0186577, acc 0.98
2016-09-06T01:26:56.480116: step 7621, loss 0.00794736, acc 1
2016-09-06T01:26:57.263057: step 7622, loss 0.0397422, acc 0.98
2016-09-06T01:26:58.080924: step 7623, loss 0.014659, acc 1
2016-09-06T01:26:58.876506: step 7624, loss 0.00508429, acc 1
2016-09-06T01:26:59.678178: step 7625, loss 0.00373775, acc 1
2016-09-06T01:27:00.502040: step 7626, loss 0.0254304, acc 1
2016-09-06T01:27:01.315629: step 7627, loss 0.00280729, acc 1
2016-09-06T01:27:02.111799: step 7628, loss 0.00304383, acc 1
2016-09-06T01:27:02.936478: step 7629, loss 0.0196156, acc 1
2016-09-06T01:27:03.757902: step 7630, loss 0.0337525, acc 0.98
2016-09-06T01:27:04.534043: step 7631, loss 0.015774, acc 1
2016-09-06T01:27:05.321371: step 7632, loss 0.0103988, acc 1
2016-09-06T01:27:06.145963: step 7633, loss 0.0520622, acc 0.98
2016-09-06T01:27:06.936936: step 7634, loss 0.0932857, acc 0.98
2016-09-06T01:27:07.773060: step 7635, loss 0.200166, acc 0.96
2016-09-06T01:27:08.617945: step 7636, loss 0.00416094, acc 1
2016-09-06T01:27:09.395550: step 7637, loss 0.00425723, acc 1
2016-09-06T01:27:10.206991: step 7638, loss 0.0165865, acc 0.98
2016-09-06T01:27:11.014140: step 7639, loss 0.0100855, acc 1
2016-09-06T01:27:11.798148: step 7640, loss 0.0202425, acc 0.98
2016-09-06T01:27:12.619271: step 7641, loss 0.0029385, acc 1
2016-09-06T01:27:13.482697: step 7642, loss 0.0104896, acc 1
2016-09-06T01:27:14.301378: step 7643, loss 0.0033751, acc 1
2016-09-06T01:27:15.100329: step 7644, loss 0.029737, acc 0.98
2016-09-06T01:27:15.912152: step 7645, loss 0.0287335, acc 0.98
2016-09-06T01:27:16.699001: step 7646, loss 0.0137624, acc 1
2016-09-06T01:27:17.499544: step 7647, loss 0.0268784, acc 0.98
2016-09-06T01:27:18.297921: step 7648, loss 0.0445937, acc 0.96
2016-09-06T01:27:19.059320: step 7649, loss 0.1025, acc 0.94
2016-09-06T01:27:19.867304: step 7650, loss 0.0496662, acc 0.96
2016-09-06T01:27:20.669642: step 7651, loss 0.00818177, acc 1
2016-09-06T01:27:21.444685: step 7652, loss 0.0120299, acc 1
2016-09-06T01:27:22.274826: step 7653, loss 0.0167971, acc 1
2016-09-06T01:27:23.072541: step 7654, loss 0.0443842, acc 0.96
2016-09-06T01:27:23.858024: step 7655, loss 0.00465632, acc 1
2016-09-06T01:27:24.668115: step 7656, loss 0.0121937, acc 1
2016-09-06T01:27:25.479287: step 7657, loss 0.0815062, acc 0.94
2016-09-06T01:27:26.253374: step 7658, loss 0.00354993, acc 1
2016-09-06T01:27:27.052808: step 7659, loss 0.02928, acc 0.98
2016-09-06T01:27:27.870041: step 7660, loss 0.0327511, acc 0.98
2016-09-06T01:27:28.702091: step 7661, loss 0.00406686, acc 1
2016-09-06T01:27:29.511238: step 7662, loss 0.0054084, acc 1
2016-09-06T01:27:30.345048: step 7663, loss 0.0811863, acc 0.96
2016-09-06T01:27:31.135716: step 7664, loss 0.125312, acc 0.96
2016-09-06T01:27:31.953987: step 7665, loss 0.0153984, acc 1
2016-09-06T01:27:32.779000: step 7666, loss 0.0586983, acc 0.96
2016-09-06T01:27:33.560428: step 7667, loss 0.0234385, acc 1
2016-09-06T01:27:34.352057: step 7668, loss 0.0504831, acc 0.96
2016-09-06T01:27:35.160885: step 7669, loss 0.00769588, acc 1
2016-09-06T01:27:35.950572: step 7670, loss 0.00275186, acc 1
2016-09-06T01:27:36.748766: step 7671, loss 0.00294515, acc 1
2016-09-06T01:27:37.570286: step 7672, loss 0.00684404, acc 1
2016-09-06T01:27:38.407815: step 7673, loss 0.00705052, acc 1
2016-09-06T01:27:39.194044: step 7674, loss 0.0201333, acc 1
2016-09-06T01:27:40.023729: step 7675, loss 0.019582, acc 0.98
2016-09-06T01:27:40.792486: step 7676, loss 0.00310461, acc 1
2016-09-06T01:27:41.585751: step 7677, loss 0.0533933, acc 0.96
2016-09-06T01:27:42.402569: step 7678, loss 0.01444, acc 1
2016-09-06T01:27:43.183678: step 7679, loss 0.00686867, acc 1
2016-09-06T01:27:43.914646: step 7680, loss 0.00352222, acc 1
2016-09-06T01:27:44.735762: step 7681, loss 0.00412991, acc 1
2016-09-06T01:27:45.539738: step 7682, loss 0.00850121, acc 1
2016-09-06T01:27:46.346380: step 7683, loss 0.00898205, acc 1
2016-09-06T01:27:47.144375: step 7684, loss 0.0333801, acc 0.98
2016-09-06T01:27:47.977097: step 7685, loss 0.00405773, acc 1
2016-09-06T01:27:48.810006: step 7686, loss 0.00490965, acc 1
2016-09-06T01:27:49.635400: step 7687, loss 0.0162169, acc 1
2016-09-06T01:27:50.423874: step 7688, loss 0.00335935, acc 1
2016-09-06T01:27:51.248814: step 7689, loss 0.0127471, acc 1
2016-09-06T01:27:52.064694: step 7690, loss 0.135975, acc 0.98
2016-09-06T01:27:52.840712: step 7691, loss 0.0253867, acc 0.98
2016-09-06T01:27:53.655793: step 7692, loss 0.00301057, acc 1
2016-09-06T01:27:54.481403: step 7693, loss 0.0222302, acc 1
2016-09-06T01:27:55.255977: step 7694, loss 0.0372142, acc 0.98
2016-09-06T01:27:56.066961: step 7695, loss 0.00258091, acc 1
2016-09-06T01:27:56.881352: step 7696, loss 0.026697, acc 1
2016-09-06T01:27:57.672424: step 7697, loss 0.00393259, acc 1
2016-09-06T01:27:58.539978: step 7698, loss 0.00307411, acc 1
2016-09-06T01:27:59.347244: step 7699, loss 0.00260299, acc 1
2016-09-06T01:28:00.127735: step 7700, loss 0.0473579, acc 0.96

Evaluation:
2016-09-06T01:28:03.840829: step 7700, loss 2.22886, acc 0.716698

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-7700

2016-09-06T01:28:05.749206: step 7701, loss 0.00730642, acc 1
2016-09-06T01:28:06.555287: step 7702, loss 0.00475892, acc 1
2016-09-06T01:28:07.395655: step 7703, loss 0.0246383, acc 1
2016-09-06T01:28:08.217534: step 7704, loss 0.0039646, acc 1
2016-09-06T01:28:09.063758: step 7705, loss 0.00499508, acc 1
2016-09-06T01:28:09.830181: step 7706, loss 0.0362009, acc 0.98
2016-09-06T01:28:10.635434: step 7707, loss 0.0044714, acc 1
2016-09-06T01:28:11.447598: step 7708, loss 0.057851, acc 0.96
2016-09-06T01:28:12.246345: step 7709, loss 0.0227308, acc 1
2016-09-06T01:28:13.054732: step 7710, loss 0.0218055, acc 0.98
2016-09-06T01:28:13.853806: step 7711, loss 0.0250837, acc 0.98
2016-09-06T01:28:14.653893: step 7712, loss 0.035112, acc 0.98
2016-09-06T01:28:15.461883: step 7713, loss 0.00520387, acc 1
2016-09-06T01:28:16.276397: step 7714, loss 0.00835694, acc 1
2016-09-06T01:28:17.072224: step 7715, loss 0.0225098, acc 0.98
2016-09-06T01:28:17.882911: step 7716, loss 0.0267079, acc 0.98
2016-09-06T01:28:18.716835: step 7717, loss 0.00270889, acc 1
2016-09-06T01:28:19.489706: step 7718, loss 0.0260993, acc 1
2016-09-06T01:28:20.313418: step 7719, loss 0.0050473, acc 1
2016-09-06T01:28:21.136704: step 7720, loss 0.00259639, acc 1
2016-09-06T01:28:21.903150: step 7721, loss 0.0462896, acc 0.96
2016-09-06T01:28:22.717849: step 7722, loss 0.0170576, acc 1
2016-09-06T01:28:23.557101: step 7723, loss 0.0577385, acc 0.98
2016-09-06T01:28:24.345111: step 7724, loss 0.00252038, acc 1
2016-09-06T01:28:25.130203: step 7725, loss 0.0395647, acc 0.98
2016-09-06T01:28:25.965061: step 7726, loss 0.00256598, acc 1
2016-09-06T01:28:26.731207: step 7727, loss 0.0047682, acc 1
2016-09-06T01:28:27.540519: step 7728, loss 0.0397437, acc 0.96
2016-09-06T01:28:28.344236: step 7729, loss 0.0407086, acc 0.98
2016-09-06T01:28:29.126330: step 7730, loss 0.00859521, acc 1
2016-09-06T01:28:29.916636: step 7731, loss 0.0059794, acc 1
2016-09-06T01:28:30.726813: step 7732, loss 0.0363293, acc 0.98
2016-09-06T01:28:31.532166: step 7733, loss 0.026617, acc 0.98
2016-09-06T01:28:32.358554: step 7734, loss 0.0136105, acc 1
2016-09-06T01:28:33.180197: step 7735, loss 0.00841022, acc 1
2016-09-06T01:28:33.971223: step 7736, loss 0.00430483, acc 1
2016-09-06T01:28:34.790723: step 7737, loss 0.00334846, acc 1
2016-09-06T01:28:35.630348: step 7738, loss 0.0251447, acc 1
2016-09-06T01:28:36.399322: step 7739, loss 0.00782824, acc 1
2016-09-06T01:28:37.206909: step 7740, loss 0.00226663, acc 1
2016-09-06T01:28:38.023847: step 7741, loss 0.0300152, acc 0.98
2016-09-06T01:28:38.794030: step 7742, loss 0.00644549, acc 1
2016-09-06T01:28:39.595140: step 7743, loss 0.0104189, acc 1
2016-09-06T01:28:40.420656: step 7744, loss 0.0564158, acc 0.96
2016-09-06T01:28:41.260186: step 7745, loss 0.00375008, acc 1
2016-09-06T01:28:42.068284: step 7746, loss 0.0103554, acc 1
2016-09-06T01:28:42.879682: step 7747, loss 0.0213868, acc 1
2016-09-06T01:28:43.665392: step 7748, loss 0.0443379, acc 0.98
2016-09-06T01:28:44.452531: step 7749, loss 0.0326137, acc 0.98
2016-09-06T01:28:45.304828: step 7750, loss 0.0211701, acc 0.98
2016-09-06T01:28:46.102624: step 7751, loss 0.0508826, acc 0.98
2016-09-06T01:28:46.919746: step 7752, loss 0.00286786, acc 1
2016-09-06T01:28:47.747484: step 7753, loss 0.0131475, acc 1
2016-09-06T01:28:48.553661: step 7754, loss 0.00317798, acc 1
2016-09-06T01:28:49.351453: step 7755, loss 0.0129458, acc 1
2016-09-06T01:28:50.200909: step 7756, loss 0.0128957, acc 1
2016-09-06T01:28:51.002686: step 7757, loss 0.00299726, acc 1
2016-09-06T01:28:51.792466: step 7758, loss 0.0341437, acc 0.98
2016-09-06T01:28:52.613462: step 7759, loss 0.00234224, acc 1
2016-09-06T01:28:53.432095: step 7760, loss 0.00875081, acc 1
2016-09-06T01:28:54.252046: step 7761, loss 0.0052289, acc 1
2016-09-06T01:28:55.052363: step 7762, loss 0.0157899, acc 1
2016-09-06T01:28:55.868335: step 7763, loss 0.0255024, acc 1
2016-09-06T01:28:56.688281: step 7764, loss 0.00646066, acc 1
2016-09-06T01:28:57.506991: step 7765, loss 0.0391598, acc 0.96
2016-09-06T01:28:58.324685: step 7766, loss 0.0274712, acc 1
2016-09-06T01:28:59.151067: step 7767, loss 0.0146832, acc 1
2016-09-06T01:29:00.019500: step 7768, loss 0.044445, acc 0.98
2016-09-06T01:29:00.868768: step 7769, loss 0.0100583, acc 1
2016-09-06T01:29:01.664108: step 7770, loss 0.00245446, acc 1
2016-09-06T01:29:02.503989: step 7771, loss 0.125398, acc 0.96
2016-09-06T01:29:03.313335: step 7772, loss 0.00267583, acc 1
2016-09-06T01:29:04.114828: step 7773, loss 0.0710944, acc 0.96
2016-09-06T01:29:04.973281: step 7774, loss 0.0029237, acc 1
2016-09-06T01:29:05.816456: step 7775, loss 0.084751, acc 0.98
2016-09-06T01:29:06.619693: step 7776, loss 0.00276243, acc 1
2016-09-06T01:29:07.413180: step 7777, loss 0.00245043, acc 1
2016-09-06T01:29:08.218715: step 7778, loss 0.0219053, acc 1
2016-09-06T01:29:09.023212: step 7779, loss 0.00242423, acc 1
2016-09-06T01:29:09.839445: step 7780, loss 0.0141962, acc 1
2016-09-06T01:29:10.645128: step 7781, loss 0.00332808, acc 1
2016-09-06T01:29:11.425357: step 7782, loss 0.00336562, acc 1
2016-09-06T01:29:12.237959: step 7783, loss 0.00530306, acc 1
2016-09-06T01:29:13.052853: step 7784, loss 0.0162748, acc 0.98
2016-09-06T01:29:13.849983: step 7785, loss 0.0164657, acc 0.98
2016-09-06T01:29:14.653679: step 7786, loss 0.00767038, acc 1
2016-09-06T01:29:15.459792: step 7787, loss 0.028527, acc 0.98
2016-09-06T01:29:16.236403: step 7788, loss 0.0417929, acc 0.98
2016-09-06T01:29:17.057209: step 7789, loss 0.0609281, acc 0.98
2016-09-06T01:29:17.858653: step 7790, loss 0.0420981, acc 1
2016-09-06T01:29:18.644995: step 7791, loss 0.0249575, acc 0.98
2016-09-06T01:29:19.472973: step 7792, loss 0.0243515, acc 0.98
2016-09-06T01:29:20.283053: step 7793, loss 0.00407349, acc 1
2016-09-06T01:29:21.063095: step 7794, loss 0.00805782, acc 1
2016-09-06T01:29:21.876845: step 7795, loss 0.025496, acc 1
2016-09-06T01:29:22.689409: step 7796, loss 0.00406659, acc 1
2016-09-06T01:29:23.469718: step 7797, loss 0.00951068, acc 1
2016-09-06T01:29:24.312544: step 7798, loss 0.00264144, acc 1
2016-09-06T01:29:25.118600: step 7799, loss 0.0227186, acc 1
2016-09-06T01:29:25.913604: step 7800, loss 0.0176228, acc 0.98

Evaluation:
2016-09-06T01:29:29.671378: step 7800, loss 2.46677, acc 0.718574

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-7800

2016-09-06T01:29:31.623726: step 7801, loss 0.00461946, acc 1
2016-09-06T01:29:32.462515: step 7802, loss 0.0197796, acc 0.98
2016-09-06T01:29:33.238790: step 7803, loss 0.0024677, acc 1
2016-09-06T01:29:34.067586: step 7804, loss 0.0297355, acc 0.98
2016-09-06T01:29:34.893364: step 7805, loss 0.0416555, acc 0.98
2016-09-06T01:29:35.674481: step 7806, loss 0.0296253, acc 0.98
2016-09-06T01:29:36.453863: step 7807, loss 0.117514, acc 0.98
2016-09-06T01:29:37.284345: step 7808, loss 0.00630127, acc 1
2016-09-06T01:29:38.046882: step 7809, loss 0.0714854, acc 0.94
2016-09-06T01:29:38.863128: step 7810, loss 0.0327809, acc 0.98
2016-09-06T01:29:39.661808: step 7811, loss 0.0429035, acc 0.98
2016-09-06T01:29:40.430762: step 7812, loss 0.0201755, acc 1
2016-09-06T01:29:41.260526: step 7813, loss 0.0337129, acc 0.98
2016-09-06T01:29:42.067902: step 7814, loss 0.00747624, acc 1
2016-09-06T01:29:42.855504: step 7815, loss 0.0073334, acc 1
2016-09-06T01:29:43.674836: step 7816, loss 0.0083905, acc 1
2016-09-06T01:29:44.487921: step 7817, loss 0.0071804, acc 1
2016-09-06T01:29:45.284175: step 7818, loss 0.075585, acc 0.98
2016-09-06T01:29:46.111772: step 7819, loss 0.0106734, acc 1
2016-09-06T01:29:46.919079: step 7820, loss 0.00213118, acc 1
2016-09-06T01:29:47.727494: step 7821, loss 0.0367686, acc 0.98
2016-09-06T01:29:48.528662: step 7822, loss 0.0189886, acc 0.98
2016-09-06T01:29:49.335857: step 7823, loss 0.00635008, acc 1
2016-09-06T01:29:50.109854: step 7824, loss 0.0217964, acc 1
2016-09-06T01:29:50.920402: step 7825, loss 0.00296373, acc 1
2016-09-06T01:29:51.755183: step 7826, loss 0.00752463, acc 1
2016-09-06T01:29:52.519525: step 7827, loss 0.0383671, acc 0.98
2016-09-06T01:29:53.320862: step 7828, loss 0.0195753, acc 1
2016-09-06T01:29:54.157992: step 7829, loss 0.0178968, acc 0.98
2016-09-06T01:29:54.965716: step 7830, loss 0.0166357, acc 1
2016-09-06T01:29:55.769940: step 7831, loss 0.00290206, acc 1
2016-09-06T01:29:56.615065: step 7832, loss 0.0168517, acc 1
2016-09-06T01:29:57.402115: step 7833, loss 0.00567357, acc 1
2016-09-06T01:29:58.249843: step 7834, loss 0.00383653, acc 1
2016-09-06T01:29:59.083089: step 7835, loss 0.00630714, acc 1
2016-09-06T01:29:59.885356: step 7836, loss 0.0198831, acc 1
2016-09-06T01:30:00.709341: step 7837, loss 0.00261027, acc 1
2016-09-06T01:30:01.538194: step 7838, loss 0.0238279, acc 0.98
2016-09-06T01:30:02.340047: step 7839, loss 0.0033082, acc 1
2016-09-06T01:30:03.149598: step 7840, loss 0.0347369, acc 0.98
2016-09-06T01:30:03.991089: step 7841, loss 0.0149411, acc 1
2016-09-06T01:30:04.831287: step 7842, loss 0.0220994, acc 0.98
2016-09-06T01:30:05.628217: step 7843, loss 0.00274629, acc 1
2016-09-06T01:30:06.473311: step 7844, loss 0.0119578, acc 1
2016-09-06T01:30:07.298246: step 7845, loss 0.00227971, acc 1
2016-09-06T01:30:08.101409: step 7846, loss 0.0173633, acc 1
2016-09-06T01:30:08.920883: step 7847, loss 0.053395, acc 0.98
2016-09-06T01:30:09.722410: step 7848, loss 0.0128671, acc 1
2016-09-06T01:30:10.508136: step 7849, loss 0.0432467, acc 0.96
2016-09-06T01:30:11.337415: step 7850, loss 0.00292094, acc 1
2016-09-06T01:30:12.150372: step 7851, loss 0.0181987, acc 1
2016-09-06T01:30:12.953954: step 7852, loss 0.00618803, acc 1
2016-09-06T01:30:13.801772: step 7853, loss 0.0131889, acc 1
2016-09-06T01:30:14.644932: step 7854, loss 0.0185539, acc 1
2016-09-06T01:30:15.461542: step 7855, loss 0.151109, acc 0.92
2016-09-06T01:30:16.283381: step 7856, loss 0.00277197, acc 1
2016-09-06T01:30:17.081721: step 7857, loss 0.00836958, acc 1
2016-09-06T01:30:17.889960: step 7858, loss 0.0496429, acc 0.96
2016-09-06T01:30:18.711536: step 7859, loss 0.03115, acc 1
2016-09-06T01:30:19.527830: step 7860, loss 0.037903, acc 0.98
2016-09-06T01:30:20.310684: step 7861, loss 0.00253856, acc 1
2016-09-06T01:30:21.129455: step 7862, loss 0.0555459, acc 0.98
2016-09-06T01:30:21.971344: step 7863, loss 0.0026037, acc 1
2016-09-06T01:30:22.749347: step 7864, loss 0.0216429, acc 1
2016-09-06T01:30:23.565205: step 7865, loss 0.0526429, acc 0.98
2016-09-06T01:30:24.374593: step 7866, loss 0.0523092, acc 0.98
2016-09-06T01:30:25.164316: step 7867, loss 0.0263032, acc 1
2016-09-06T01:30:26.002214: step 7868, loss 0.0107727, acc 1
2016-09-06T01:30:26.816811: step 7869, loss 0.0253003, acc 0.98
2016-09-06T01:30:27.617837: step 7870, loss 0.0989982, acc 0.94
2016-09-06T01:30:28.439886: step 7871, loss 0.0301548, acc 1
2016-09-06T01:30:29.191143: step 7872, loss 0.0161152, acc 1
2016-09-06T01:30:30.026975: step 7873, loss 0.00934296, acc 1
2016-09-06T01:30:30.826966: step 7874, loss 0.0236949, acc 1
2016-09-06T01:30:31.659361: step 7875, loss 0.0321995, acc 0.98
2016-09-06T01:30:32.447416: step 7876, loss 0.0273502, acc 0.98
2016-09-06T01:30:33.258184: step 7877, loss 0.0205956, acc 0.98
2016-09-06T01:30:34.082282: step 7878, loss 0.00311827, acc 1
2016-09-06T01:30:34.881474: step 7879, loss 0.00418373, acc 1
2016-09-06T01:30:35.684451: step 7880, loss 0.00591441, acc 1
2016-09-06T01:30:36.485376: step 7881, loss 0.00792895, acc 1
2016-09-06T01:30:37.272601: step 7882, loss 0.00213902, acc 1
2016-09-06T01:30:38.060764: step 7883, loss 0.0363332, acc 0.98
2016-09-06T01:30:38.879342: step 7884, loss 0.0217569, acc 0.98
2016-09-06T01:30:39.722454: step 7885, loss 0.0108338, acc 1
2016-09-06T01:30:40.539660: step 7886, loss 0.00218819, acc 1
2016-09-06T01:30:41.356706: step 7887, loss 0.00352025, acc 1
2016-09-06T01:30:42.146845: step 7888, loss 0.0366889, acc 0.98
2016-09-06T01:30:42.968042: step 7889, loss 0.0126445, acc 1
2016-09-06T01:30:43.763074: step 7890, loss 0.0200006, acc 1
2016-09-06T01:30:44.556868: step 7891, loss 0.0043838, acc 1
2016-09-06T01:30:45.352532: step 7892, loss 0.00279608, acc 1
2016-09-06T01:30:46.175974: step 7893, loss 0.00523423, acc 1
2016-09-06T01:30:46.964732: step 7894, loss 0.0530766, acc 0.96
2016-09-06T01:30:47.767394: step 7895, loss 0.002231, acc 1
2016-09-06T01:30:48.570816: step 7896, loss 0.00320633, acc 1
2016-09-06T01:30:49.362199: step 7897, loss 0.0474175, acc 0.98
2016-09-06T01:30:50.154452: step 7898, loss 0.0719706, acc 0.96
2016-09-06T01:30:50.960080: step 7899, loss 0.0268022, acc 0.98
2016-09-06T01:30:51.726438: step 7900, loss 0.0464885, acc 0.98

Evaluation:
2016-09-06T01:30:55.460076: step 7900, loss 2.49205, acc 0.718574

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-7900

2016-09-06T01:30:57.376128: step 7901, loss 0.0489806, acc 0.96
2016-09-06T01:30:58.180945: step 7902, loss 0.017051, acc 1
2016-09-06T01:30:58.978250: step 7903, loss 0.0121703, acc 1
2016-09-06T01:30:59.861252: step 7904, loss 0.00288289, acc 1
2016-09-06T01:31:00.668216: step 7905, loss 0.00621867, acc 1
2016-09-06T01:31:01.477434: step 7906, loss 0.00972033, acc 1
2016-09-06T01:31:02.304387: step 7907, loss 0.0101931, acc 1
2016-09-06T01:31:03.132789: step 7908, loss 0.0379379, acc 0.98
2016-09-06T01:31:03.924576: step 7909, loss 0.00341902, acc 1
2016-09-06T01:31:04.716583: step 7910, loss 0.0194777, acc 1
2016-09-06T01:31:05.515798: step 7911, loss 0.0061638, acc 1
2016-09-06T01:31:06.329450: step 7912, loss 0.00727595, acc 1
2016-09-06T01:31:07.161898: step 7913, loss 0.0233906, acc 1
2016-09-06T01:31:07.970676: step 7914, loss 0.00283331, acc 1
2016-09-06T01:31:08.738771: step 7915, loss 0.0383905, acc 1
2016-09-06T01:31:09.549566: step 7916, loss 0.00752906, acc 1
2016-09-06T01:31:10.400558: step 7917, loss 0.00272414, acc 1
2016-09-06T01:31:11.185854: step 7918, loss 0.0144469, acc 1
2016-09-06T01:31:11.998390: step 7919, loss 0.0175966, acc 0.98
2016-09-06T01:31:12.833401: step 7920, loss 0.0461795, acc 1
2016-09-06T01:31:13.642090: step 7921, loss 0.0512154, acc 0.98
2016-09-06T01:31:14.420982: step 7922, loss 0.00896086, acc 1
2016-09-06T01:31:15.225538: step 7923, loss 0.0124021, acc 1
2016-09-06T01:31:16.020992: step 7924, loss 0.0284646, acc 0.98
2016-09-06T01:31:16.813620: step 7925, loss 0.0134358, acc 1
2016-09-06T01:31:17.643700: step 7926, loss 0.00323743, acc 1
2016-09-06T01:31:18.453041: step 7927, loss 0.00413447, acc 1
2016-09-06T01:31:19.277272: step 7928, loss 0.0116718, acc 1
2016-09-06T01:31:20.117025: step 7929, loss 0.047796, acc 0.98
2016-09-06T01:31:20.886132: step 7930, loss 0.0195225, acc 0.98
2016-09-06T01:31:21.674878: step 7931, loss 0.0259868, acc 1
2016-09-06T01:31:22.483512: step 7932, loss 0.00804858, acc 1
2016-09-06T01:31:23.261257: step 7933, loss 0.0733926, acc 0.96
2016-09-06T01:31:24.067302: step 7934, loss 0.00325019, acc 1
2016-09-06T01:31:24.892631: step 7935, loss 0.0200983, acc 0.98
2016-09-06T01:31:25.686495: step 7936, loss 0.0188598, acc 0.98
2016-09-06T01:31:26.491840: step 7937, loss 0.0300563, acc 0.98
2016-09-06T01:31:27.296597: step 7938, loss 0.00419854, acc 1
2016-09-06T01:31:28.071965: step 7939, loss 0.0294936, acc 0.98
2016-09-06T01:31:28.891040: step 7940, loss 0.0572784, acc 0.96
2016-09-06T01:31:29.657444: step 7941, loss 0.0648373, acc 0.98
2016-09-06T01:31:30.455810: step 7942, loss 0.00590798, acc 1
2016-09-06T01:31:31.285482: step 7943, loss 0.0143135, acc 1
2016-09-06T01:31:32.074001: step 7944, loss 0.0033371, acc 1
2016-09-06T01:31:32.891559: step 7945, loss 0.021024, acc 1
2016-09-06T01:31:33.722218: step 7946, loss 0.00311172, acc 1
2016-09-06T01:31:34.543012: step 7947, loss 0.0173423, acc 1
2016-09-06T01:31:35.325932: step 7948, loss 0.0144549, acc 1
2016-09-06T01:31:36.159484: step 7949, loss 0.0248601, acc 0.98
2016-09-06T01:31:36.991660: step 7950, loss 0.00729964, acc 1
2016-09-06T01:31:37.798326: step 7951, loss 0.00347677, acc 1
2016-09-06T01:31:38.596382: step 7952, loss 0.0299992, acc 0.98
2016-09-06T01:31:39.419684: step 7953, loss 0.0055144, acc 1
2016-09-06T01:31:40.199970: step 7954, loss 0.0259941, acc 0.98
2016-09-06T01:31:40.994349: step 7955, loss 0.0119577, acc 1
2016-09-06T01:31:41.804164: step 7956, loss 0.00376662, acc 1
2016-09-06T01:31:42.600485: step 7957, loss 0.00608209, acc 1
2016-09-06T01:31:43.400668: step 7958, loss 0.0167718, acc 1
2016-09-06T01:31:44.228753: step 7959, loss 0.00571307, acc 1
2016-09-06T01:31:45.024225: step 7960, loss 0.0275677, acc 0.98
2016-09-06T01:31:45.837729: step 7961, loss 0.0144847, acc 1
2016-09-06T01:31:46.707352: step 7962, loss 0.00631866, acc 1
2016-09-06T01:31:47.524980: step 7963, loss 0.0163582, acc 1
2016-09-06T01:31:48.354938: step 7964, loss 0.00332885, acc 1
2016-09-06T01:31:49.216056: step 7965, loss 0.00691517, acc 1
2016-09-06T01:31:50.028009: step 7966, loss 0.00309293, acc 1
2016-09-06T01:31:50.847427: step 7967, loss 0.0180257, acc 1
2016-09-06T01:31:51.684598: step 7968, loss 0.00437188, acc 1
2016-09-06T01:31:52.517434: step 7969, loss 0.00327849, acc 1
2016-09-06T01:31:53.296276: step 7970, loss 0.0496627, acc 0.96
2016-09-06T01:31:54.141901: step 7971, loss 0.00677202, acc 1
2016-09-06T01:31:54.977497: step 7972, loss 0.0222766, acc 1
2016-09-06T01:31:55.797481: step 7973, loss 0.0564293, acc 0.98
2016-09-06T01:31:56.636959: step 7974, loss 0.00365892, acc 1
2016-09-06T01:31:57.437397: step 7975, loss 0.046889, acc 0.98
2016-09-06T01:31:58.245369: step 7976, loss 0.0530321, acc 0.98
2016-09-06T01:31:59.085453: step 7977, loss 0.0321465, acc 0.96
2016-09-06T01:31:59.878253: step 7978, loss 0.0132256, acc 1
2016-09-06T01:32:00.715510: step 7979, loss 0.0331577, acc 0.98
2016-09-06T01:32:01.531363: step 7980, loss 0.0164646, acc 1
2016-09-06T01:32:02.356722: step 7981, loss 0.0298312, acc 0.98
2016-09-06T01:32:03.173386: step 7982, loss 0.15791, acc 0.96
2016-09-06T01:32:03.970606: step 7983, loss 0.00305748, acc 1
2016-09-06T01:32:04.787513: step 7984, loss 0.0269407, acc 0.98
2016-09-06T01:32:05.578753: step 7985, loss 0.0135908, acc 1
2016-09-06T01:32:06.387575: step 7986, loss 0.0345202, acc 0.98
2016-09-06T01:32:07.206636: step 7987, loss 0.0147294, acc 1
2016-09-06T01:32:07.991344: step 7988, loss 0.00771063, acc 1
2016-09-06T01:32:08.782492: step 7989, loss 0.0026926, acc 1
2016-09-06T01:32:09.601983: step 7990, loss 0.00244818, acc 1
2016-09-06T01:32:10.389477: step 7991, loss 0.00811848, acc 1
2016-09-06T01:32:11.189180: step 7992, loss 0.00290917, acc 1
2016-09-06T01:32:12.001492: step 7993, loss 0.0038699, acc 1
2016-09-06T01:32:12.805504: step 7994, loss 0.00709922, acc 1
2016-09-06T01:32:13.609266: step 7995, loss 0.0544771, acc 0.98
2016-09-06T01:32:14.433772: step 7996, loss 0.026733, acc 1
2016-09-06T01:32:15.217669: step 7997, loss 0.0163843, acc 1
2016-09-06T01:32:16.041636: step 7998, loss 0.00361598, acc 1
2016-09-06T01:32:16.875376: step 7999, loss 0.0190018, acc 1
2016-09-06T01:32:17.693464: step 8000, loss 0.0164205, acc 0.98

Evaluation:
2016-09-06T01:32:21.394382: step 8000, loss 2.36125, acc 0.712008

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-8000

2016-09-06T01:32:23.315193: step 8001, loss 0.00471588, acc 1
2016-09-06T01:32:24.152942: step 8002, loss 0.0672015, acc 0.96
2016-09-06T01:32:24.962958: step 8003, loss 0.00258262, acc 1
2016-09-06T01:32:25.764531: step 8004, loss 0.00227602, acc 1
2016-09-06T01:32:26.570510: step 8005, loss 0.0253623, acc 1
2016-09-06T01:32:27.371870: step 8006, loss 0.0108119, acc 1
2016-09-06T01:32:28.174144: step 8007, loss 0.0488651, acc 0.98
2016-09-06T01:32:28.982897: step 8008, loss 0.038596, acc 0.98
2016-09-06T01:32:29.784098: step 8009, loss 0.0118828, acc 1
2016-09-06T01:32:30.601355: step 8010, loss 0.0277256, acc 1
2016-09-06T01:32:31.418885: step 8011, loss 0.00259484, acc 1
2016-09-06T01:32:32.195478: step 8012, loss 0.0267777, acc 0.98
2016-09-06T01:32:33.004077: step 8013, loss 0.01883, acc 1
2016-09-06T01:32:33.829427: step 8014, loss 0.00334106, acc 1
2016-09-06T01:32:34.613785: step 8015, loss 0.0237037, acc 0.98
2016-09-06T01:32:35.403779: step 8016, loss 0.0187437, acc 1
2016-09-06T01:32:36.206958: step 8017, loss 0.0142421, acc 1
2016-09-06T01:32:36.991839: step 8018, loss 0.0130443, acc 1
2016-09-06T01:32:37.796018: step 8019, loss 0.0699776, acc 0.98
2016-09-06T01:32:38.604801: step 8020, loss 0.00546235, acc 1
2016-09-06T01:32:39.410131: step 8021, loss 0.0042581, acc 1
2016-09-06T01:32:40.228298: step 8022, loss 0.0413038, acc 0.96
2016-09-06T01:32:41.031335: step 8023, loss 0.00222835, acc 1
2016-09-06T01:32:41.830117: step 8024, loss 0.00301389, acc 1
2016-09-06T01:32:42.640278: step 8025, loss 0.0037711, acc 1
2016-09-06T01:32:43.452778: step 8026, loss 0.00506475, acc 1
2016-09-06T01:32:44.252427: step 8027, loss 0.116178, acc 0.94
2016-09-06T01:32:45.057439: step 8028, loss 0.00492231, acc 1
2016-09-06T01:32:45.866075: step 8029, loss 0.0592697, acc 0.98
2016-09-06T01:32:46.653406: step 8030, loss 0.00709983, acc 1
2016-09-06T01:32:47.452752: step 8031, loss 0.0194766, acc 0.98
2016-09-06T01:32:48.267068: step 8032, loss 0.0646168, acc 0.94
2016-09-06T01:32:49.067972: step 8033, loss 0.0311645, acc 1
2016-09-06T01:32:49.885769: step 8034, loss 0.0299703, acc 1
2016-09-06T01:32:50.719364: step 8035, loss 0.0242208, acc 0.98
2016-09-06T01:32:51.531871: step 8036, loss 0.0169744, acc 1
2016-09-06T01:32:52.327611: step 8037, loss 0.00259762, acc 1
2016-09-06T01:32:53.142130: step 8038, loss 0.0170607, acc 1
2016-09-06T01:32:53.919484: step 8039, loss 0.0175252, acc 1
2016-09-06T01:32:54.727668: step 8040, loss 0.0105789, acc 1
2016-09-06T01:32:55.535128: step 8041, loss 0.0786543, acc 0.92
2016-09-06T01:32:56.337991: step 8042, loss 0.00369744, acc 1
2016-09-06T01:32:57.124530: step 8043, loss 0.00811746, acc 1
2016-09-06T01:32:57.922109: step 8044, loss 0.00403104, acc 1
2016-09-06T01:32:58.726226: step 8045, loss 0.0407503, acc 0.96
2016-09-06T01:32:59.558173: step 8046, loss 0.0243663, acc 0.98
2016-09-06T01:33:00.396436: step 8047, loss 0.0995365, acc 0.98
2016-09-06T01:33:01.168344: step 8048, loss 0.0022382, acc 1
2016-09-06T01:33:01.985873: step 8049, loss 0.00540062, acc 1
2016-09-06T01:33:02.788915: step 8050, loss 0.0347955, acc 1
2016-09-06T01:33:03.587048: step 8051, loss 0.0548222, acc 0.98
2016-09-06T01:33:04.445079: step 8052, loss 0.0100541, acc 1
2016-09-06T01:33:05.271420: step 8053, loss 0.0753035, acc 0.98
2016-09-06T01:33:06.072760: step 8054, loss 0.00930954, acc 1
2016-09-06T01:33:06.864187: step 8055, loss 0.00861558, acc 1
2016-09-06T01:33:07.654556: step 8056, loss 0.00903422, acc 1
2016-09-06T01:33:08.435830: step 8057, loss 0.00496467, acc 1
2016-09-06T01:33:09.233272: step 8058, loss 0.0684429, acc 0.96
2016-09-06T01:33:10.036227: step 8059, loss 0.0658928, acc 0.96
2016-09-06T01:33:10.854521: step 8060, loss 0.0193994, acc 1
2016-09-06T01:33:11.708805: step 8061, loss 0.0323522, acc 0.98
2016-09-06T01:33:12.532411: step 8062, loss 0.0215227, acc 0.98
2016-09-06T01:33:13.341921: step 8063, loss 0.0444262, acc 0.98
2016-09-06T01:33:14.100415: step 8064, loss 0.0307584, acc 1
2016-09-06T01:33:14.920778: step 8065, loss 0.0303473, acc 0.98
2016-09-06T01:33:15.687559: step 8066, loss 0.00929405, acc 1
2016-09-06T01:33:16.484666: step 8067, loss 0.00550779, acc 1
2016-09-06T01:33:17.307484: step 8068, loss 0.0208137, acc 1
2016-09-06T01:33:18.108107: step 8069, loss 0.00539509, acc 1
2016-09-06T01:33:18.928606: step 8070, loss 0.0743265, acc 0.98
2016-09-06T01:33:19.740375: step 8071, loss 0.00308636, acc 1
2016-09-06T01:33:20.543720: step 8072, loss 0.0136582, acc 1
2016-09-06T01:33:21.332136: step 8073, loss 0.0143374, acc 1
2016-09-06T01:33:22.156988: step 8074, loss 0.00306849, acc 1
2016-09-06T01:33:22.919718: step 8075, loss 0.0250451, acc 1
2016-09-06T01:33:23.729008: step 8076, loss 0.00977489, acc 1
2016-09-06T01:33:24.547107: step 8077, loss 0.00866685, acc 1
2016-09-06T01:33:25.329807: step 8078, loss 0.00323364, acc 1
2016-09-06T01:33:26.136011: step 8079, loss 0.0158651, acc 1
2016-09-06T01:33:26.938313: step 8080, loss 0.0286126, acc 1
2016-09-06T01:33:27.740363: step 8081, loss 0.0189734, acc 1
2016-09-06T01:33:28.562797: step 8082, loss 0.0283651, acc 0.98
2016-09-06T01:33:29.362521: step 8083, loss 0.00934541, acc 1
2016-09-06T01:33:30.158099: step 8084, loss 0.0197752, acc 1
2016-09-06T01:33:30.952701: step 8085, loss 0.0549329, acc 0.98
2016-09-06T01:33:31.753635: step 8086, loss 0.0359766, acc 0.98
2016-09-06T01:33:32.565562: step 8087, loss 0.0641684, acc 0.98
2016-09-06T01:33:33.405320: step 8088, loss 0.0260334, acc 0.98
2016-09-06T01:33:34.223587: step 8089, loss 0.0140205, acc 1
2016-09-06T01:33:35.021985: step 8090, loss 0.0470195, acc 0.98
2016-09-06T01:33:35.855109: step 8091, loss 0.0419063, acc 0.96
2016-09-06T01:33:36.678212: step 8092, loss 0.0210024, acc 1
2016-09-06T01:33:37.458549: step 8093, loss 0.0485609, acc 0.98
2016-09-06T01:33:38.253266: step 8094, loss 0.0242721, acc 1
2016-09-06T01:33:39.068958: step 8095, loss 0.0166669, acc 1
2016-09-06T01:33:39.857023: step 8096, loss 0.00337737, acc 1
2016-09-06T01:33:40.661430: step 8097, loss 0.00733697, acc 1
2016-09-06T01:33:41.466508: step 8098, loss 0.00864753, acc 1
2016-09-06T01:33:42.229361: step 8099, loss 0.0032132, acc 1
2016-09-06T01:33:43.044678: step 8100, loss 0.0472872, acc 0.98

Evaluation:
2016-09-06T01:33:46.757361: step 8100, loss 2.64217, acc 0.726079

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-8100

2016-09-06T01:33:48.713032: step 8101, loss 0.00705643, acc 1
2016-09-06T01:33:49.530907: step 8102, loss 0.0368302, acc 0.98
2016-09-06T01:33:50.360614: step 8103, loss 0.00832137, acc 1
2016-09-06T01:33:51.164648: step 8104, loss 0.0077558, acc 1
2016-09-06T01:33:51.962963: step 8105, loss 0.00403237, acc 1
2016-09-06T01:33:52.808490: step 8106, loss 0.00318333, acc 1
2016-09-06T01:33:53.646659: step 8107, loss 0.0230941, acc 1
2016-09-06T01:33:54.425403: step 8108, loss 0.00515423, acc 1
2016-09-06T01:33:55.224692: step 8109, loss 0.00301779, acc 1
2016-09-06T01:33:56.034699: step 8110, loss 0.0116132, acc 1
2016-09-06T01:33:56.821505: step 8111, loss 0.00684964, acc 1
2016-09-06T01:33:57.637597: step 8112, loss 0.02061, acc 1
2016-09-06T01:33:58.446355: step 8113, loss 0.0200824, acc 1
2016-09-06T01:33:59.234068: step 8114, loss 0.00332879, acc 1
2016-09-06T01:34:00.033752: step 8115, loss 0.00373934, acc 1
2016-09-06T01:34:00.903852: step 8116, loss 0.0368135, acc 0.98
2016-09-06T01:34:01.689252: step 8117, loss 0.00772757, acc 1
2016-09-06T01:34:02.503348: step 8118, loss 0.0141788, acc 1
2016-09-06T01:34:03.319595: step 8119, loss 0.0041845, acc 1
2016-09-06T01:34:04.091736: step 8120, loss 0.0112624, acc 1
2016-09-06T01:34:04.889085: step 8121, loss 0.0049045, acc 1
2016-09-06T01:34:05.709476: step 8122, loss 0.0469761, acc 0.98
2016-09-06T01:34:06.472069: step 8123, loss 0.00455333, acc 1
2016-09-06T01:34:07.283673: step 8124, loss 0.029072, acc 0.98
2016-09-06T01:34:08.133978: step 8125, loss 0.0232232, acc 0.98
2016-09-06T01:34:08.909079: step 8126, loss 0.00992845, acc 1
2016-09-06T01:34:09.696184: step 8127, loss 0.00298938, acc 1
2016-09-06T01:34:10.503038: step 8128, loss 0.0435795, acc 0.98
2016-09-06T01:34:11.311890: step 8129, loss 0.0190459, acc 1
2016-09-06T01:34:12.110564: step 8130, loss 0.0651901, acc 0.96
2016-09-06T01:34:12.922044: step 8131, loss 0.0268938, acc 0.98
2016-09-06T01:34:13.691182: step 8132, loss 0.01056, acc 1
2016-09-06T01:34:14.501684: step 8133, loss 0.0459453, acc 0.98
2016-09-06T01:34:15.309407: step 8134, loss 0.0165185, acc 1
2016-09-06T01:34:16.079733: step 8135, loss 0.00464836, acc 1
2016-09-06T01:34:16.930369: step 8136, loss 0.00614245, acc 1
2016-09-06T01:34:17.766467: step 8137, loss 0.0213006, acc 0.98
2016-09-06T01:34:18.566061: step 8138, loss 0.00360641, acc 1
2016-09-06T01:34:19.370170: step 8139, loss 0.0214475, acc 1
2016-09-06T01:34:20.215103: step 8140, loss 0.0132816, acc 1
2016-09-06T01:34:21.020267: step 8141, loss 0.00692925, acc 1
2016-09-06T01:34:21.834163: step 8142, loss 0.0229493, acc 0.98
2016-09-06T01:34:22.689547: step 8143, loss 0.0117843, acc 1
2016-09-06T01:34:23.496953: step 8144, loss 0.0143362, acc 1
2016-09-06T01:34:24.312848: step 8145, loss 0.00297582, acc 1
2016-09-06T01:34:25.140160: step 8146, loss 0.0173781, acc 0.98
2016-09-06T01:34:25.937445: step 8147, loss 0.0138591, acc 1
2016-09-06T01:34:26.747442: step 8148, loss 0.0034058, acc 1
2016-09-06T01:34:27.560246: step 8149, loss 0.00494301, acc 1
2016-09-06T01:34:28.356791: step 8150, loss 0.014576, acc 1
2016-09-06T01:34:29.153404: step 8151, loss 0.00276979, acc 1
2016-09-06T01:34:29.988809: step 8152, loss 0.0179776, acc 0.98
2016-09-06T01:34:30.816678: step 8153, loss 0.0213696, acc 1
2016-09-06T01:34:31.686336: step 8154, loss 0.00308561, acc 1
2016-09-06T01:34:32.497230: step 8155, loss 0.0104839, acc 1
2016-09-06T01:34:33.274063: step 8156, loss 0.00618747, acc 1
2016-09-06T01:34:34.081978: step 8157, loss 0.00673566, acc 1
2016-09-06T01:34:34.927122: step 8158, loss 0.0365308, acc 0.98
2016-09-06T01:34:35.742560: step 8159, loss 0.0213313, acc 0.98
2016-09-06T01:34:36.538200: step 8160, loss 0.0971396, acc 0.96
2016-09-06T01:34:37.361275: step 8161, loss 0.00269407, acc 1
2016-09-06T01:34:38.151588: step 8162, loss 0.0129053, acc 1
2016-09-06T01:34:38.959781: step 8163, loss 0.015311, acc 1
2016-09-06T01:34:39.784766: step 8164, loss 0.00687412, acc 1
2016-09-06T01:34:40.602251: step 8165, loss 0.0173804, acc 0.98
2016-09-06T01:34:41.399109: step 8166, loss 0.0235244, acc 0.98
2016-09-06T01:34:42.229933: step 8167, loss 0.00245278, acc 1
2016-09-06T01:34:43.053441: step 8168, loss 0.0131071, acc 1
2016-09-06T01:34:43.865306: step 8169, loss 0.0150049, acc 1
2016-09-06T01:34:44.684844: step 8170, loss 0.00516342, acc 1
2016-09-06T01:34:45.485972: step 8171, loss 0.0204, acc 1
2016-09-06T01:34:46.305290: step 8172, loss 0.0460467, acc 0.96
2016-09-06T01:34:47.130135: step 8173, loss 0.0198407, acc 0.98
2016-09-06T01:34:47.955173: step 8174, loss 0.0112435, acc 1
2016-09-06T01:34:48.741987: step 8175, loss 0.00234295, acc 1
2016-09-06T01:34:49.546820: step 8176, loss 0.0368602, acc 0.98
2016-09-06T01:34:50.357680: step 8177, loss 0.0173198, acc 0.98
2016-09-06T01:34:51.155130: step 8178, loss 0.0295238, acc 0.98
2016-09-06T01:34:51.945082: step 8179, loss 0.0282733, acc 0.98
2016-09-06T01:34:52.748983: step 8180, loss 0.00849459, acc 1
2016-09-06T01:34:53.540754: step 8181, loss 0.0270855, acc 1
2016-09-06T01:34:54.366357: step 8182, loss 0.00229385, acc 1
2016-09-06T01:34:55.172718: step 8183, loss 0.00953223, acc 1
2016-09-06T01:34:56.001021: step 8184, loss 0.0229373, acc 1
2016-09-06T01:34:56.815041: step 8185, loss 0.00368622, acc 1
2016-09-06T01:34:57.649426: step 8186, loss 0.00225133, acc 1
2016-09-06T01:34:58.440012: step 8187, loss 0.0108862, acc 1
2016-09-06T01:34:59.239235: step 8188, loss 0.0185709, acc 0.98
2016-09-06T01:35:00.058215: step 8189, loss 0.0109897, acc 1
2016-09-06T01:35:00.898901: step 8190, loss 0.0539668, acc 0.98
2016-09-06T01:35:01.672020: step 8191, loss 0.0137774, acc 1
2016-09-06T01:35:02.511036: step 8192, loss 0.0211125, acc 0.98
2016-09-06T01:35:03.303285: step 8193, loss 0.00296801, acc 1
2016-09-06T01:35:04.088990: step 8194, loss 0.00375476, acc 1
2016-09-06T01:35:04.889719: step 8195, loss 0.00640277, acc 1
2016-09-06T01:35:05.669807: step 8196, loss 0.00296697, acc 1
2016-09-06T01:35:06.474885: step 8197, loss 0.0158714, acc 1
2016-09-06T01:35:07.294297: step 8198, loss 0.0149847, acc 1
2016-09-06T01:35:08.081427: step 8199, loss 0.00539841, acc 1
2016-09-06T01:35:08.895734: step 8200, loss 0.00281473, acc 1

Evaluation:
2016-09-06T01:35:12.631484: step 8200, loss 3.13885, acc 0.72045

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-8200

2016-09-06T01:35:14.541858: step 8201, loss 0.183341, acc 0.96
2016-09-06T01:35:15.375417: step 8202, loss 0.0417252, acc 0.98
2016-09-06T01:35:16.233572: step 8203, loss 0.0428719, acc 0.98
2016-09-06T01:35:17.062255: step 8204, loss 0.0305985, acc 0.98
2016-09-06T01:35:17.893197: step 8205, loss 0.0522683, acc 0.96
2016-09-06T01:35:18.700565: step 8206, loss 0.0278649, acc 0.98
2016-09-06T01:35:19.537064: step 8207, loss 0.00553798, acc 1
2016-09-06T01:35:20.307880: step 8208, loss 0.0315312, acc 0.98
2016-09-06T01:35:21.100412: step 8209, loss 0.002213, acc 1
2016-09-06T01:35:21.940202: step 8210, loss 0.0228862, acc 0.98
2016-09-06T01:35:22.745158: step 8211, loss 0.0411771, acc 0.98
2016-09-06T01:35:23.555354: step 8212, loss 0.00354792, acc 1
2016-09-06T01:35:24.386974: step 8213, loss 0.0159346, acc 1
2016-09-06T01:35:25.201347: step 8214, loss 0.03195, acc 1
2016-09-06T01:35:26.014545: step 8215, loss 0.00541858, acc 1
2016-09-06T01:35:26.826051: step 8216, loss 0.00484027, acc 1
2016-09-06T01:35:27.669643: step 8217, loss 0.00254493, acc 1
2016-09-06T01:35:28.493591: step 8218, loss 0.00240091, acc 1
2016-09-06T01:35:29.321200: step 8219, loss 0.0984561, acc 0.98
2016-09-06T01:35:30.123363: step 8220, loss 0.0161603, acc 1
2016-09-06T01:35:30.947068: step 8221, loss 0.0142697, acc 1
2016-09-06T01:35:31.813356: step 8222, loss 0.0113185, acc 1
2016-09-06T01:35:32.655132: step 8223, loss 0.00730371, acc 1
2016-09-06T01:35:33.486828: step 8224, loss 0.0216618, acc 1
2016-09-06T01:35:34.300000: step 8225, loss 0.0180303, acc 1
2016-09-06T01:35:35.130742: step 8226, loss 0.0256618, acc 0.98
2016-09-06T01:35:35.945402: step 8227, loss 0.0212346, acc 1
2016-09-06T01:35:36.772836: step 8228, loss 0.0454598, acc 0.98
2016-09-06T01:35:37.553469: step 8229, loss 0.00206973, acc 1
2016-09-06T01:35:38.354221: step 8230, loss 0.06878, acc 0.96
2016-09-06T01:35:39.186550: step 8231, loss 0.119755, acc 0.98
2016-09-06T01:35:39.989617: step 8232, loss 0.00255294, acc 1
2016-09-06T01:35:40.809098: step 8233, loss 0.0510942, acc 1
2016-09-06T01:35:41.671631: step 8234, loss 0.00302128, acc 1
2016-09-06T01:35:42.506499: step 8235, loss 0.011622, acc 1
2016-09-06T01:35:43.306569: step 8236, loss 0.0043599, acc 1
2016-09-06T01:35:44.115056: step 8237, loss 0.00345193, acc 1
2016-09-06T01:35:44.928165: step 8238, loss 0.0128855, acc 1
2016-09-06T01:35:45.687484: step 8239, loss 0.0548464, acc 0.98
2016-09-06T01:35:46.478078: step 8240, loss 0.00419261, acc 1
2016-09-06T01:35:47.281958: step 8241, loss 0.00515757, acc 1
2016-09-06T01:35:48.073397: step 8242, loss 0.0298401, acc 1
2016-09-06T01:35:48.897020: step 8243, loss 0.0424201, acc 0.96
2016-09-06T01:35:49.715740: step 8244, loss 0.0126428, acc 1
2016-09-06T01:35:50.486455: step 8245, loss 0.00386204, acc 1
2016-09-06T01:35:51.322800: step 8246, loss 0.0736359, acc 0.98
2016-09-06T01:35:52.161003: step 8247, loss 0.018188, acc 0.98
2016-09-06T01:35:52.927526: step 8248, loss 0.0252318, acc 0.98
2016-09-06T01:35:53.721969: step 8249, loss 0.00284885, acc 1
2016-09-06T01:35:54.521395: step 8250, loss 0.00978504, acc 1
2016-09-06T01:35:55.296813: step 8251, loss 0.0155334, acc 1
2016-09-06T01:35:56.099225: step 8252, loss 0.00242889, acc 1
2016-09-06T01:35:56.917041: step 8253, loss 0.0109801, acc 1
2016-09-06T01:35:57.713186: step 8254, loss 0.00384541, acc 1
2016-09-06T01:35:58.524815: step 8255, loss 0.0448252, acc 0.98
2016-09-06T01:35:59.251120: step 8256, loss 0.0821954, acc 0.977273
2016-09-06T01:36:00.072594: step 8257, loss 0.0417435, acc 0.98
2016-09-06T01:36:00.928487: step 8258, loss 0.010511, acc 1
2016-09-06T01:36:01.740530: step 8259, loss 0.00468355, acc 1
2016-09-06T01:36:02.563311: step 8260, loss 0.0125811, acc 1
2016-09-06T01:36:03.409339: step 8261, loss 0.00269947, acc 1
2016-09-06T01:36:04.240876: step 8262, loss 0.161725, acc 0.98
2016-09-06T01:36:05.023454: step 8263, loss 0.00493378, acc 1
2016-09-06T01:36:05.824995: step 8264, loss 0.00535124, acc 1
2016-09-06T01:36:06.645970: step 8265, loss 0.0330773, acc 0.98
2016-09-06T01:36:07.434348: step 8266, loss 0.00246581, acc 1
2016-09-06T01:36:08.279600: step 8267, loss 0.0259109, acc 1
2016-09-06T01:36:09.093906: step 8268, loss 0.0241025, acc 0.98
2016-09-06T01:36:09.873429: step 8269, loss 0.0127784, acc 1
2016-09-06T01:36:10.691968: step 8270, loss 0.0160397, acc 1
2016-09-06T01:36:11.501541: step 8271, loss 0.00252319, acc 1
2016-09-06T01:36:12.265630: step 8272, loss 0.0320125, acc 0.98
2016-09-06T01:36:13.061408: step 8273, loss 0.0168136, acc 1
2016-09-06T01:36:13.884747: step 8274, loss 0.0070747, acc 1
2016-09-06T01:36:14.662623: step 8275, loss 0.00387606, acc 1
2016-09-06T01:36:15.492319: step 8276, loss 0.0226114, acc 0.98
2016-09-06T01:36:16.330130: step 8277, loss 0.00360856, acc 1
2016-09-06T01:36:17.107709: step 8278, loss 0.0746299, acc 0.94
2016-09-06T01:36:17.908833: step 8279, loss 0.0214556, acc 0.98
2016-09-06T01:36:18.756655: step 8280, loss 0.0299464, acc 0.98
2016-09-06T01:36:19.558701: step 8281, loss 0.00482979, acc 1
2016-09-06T01:36:20.349412: step 8282, loss 0.0163397, acc 0.98
2016-09-06T01:36:21.190274: step 8283, loss 0.00980768, acc 1
2016-09-06T01:36:21.950292: step 8284, loss 0.0287831, acc 0.98
2016-09-06T01:36:22.743863: step 8285, loss 0.0208347, acc 0.98
2016-09-06T01:36:23.582988: step 8286, loss 0.00931241, acc 1
2016-09-06T01:36:24.366739: step 8287, loss 0.00345267, acc 1
2016-09-06T01:36:25.148962: step 8288, loss 0.057049, acc 0.98
2016-09-06T01:36:25.945817: step 8289, loss 0.0359029, acc 0.96
2016-09-06T01:36:26.718791: step 8290, loss 0.0242387, acc 1
2016-09-06T01:36:27.547711: step 8291, loss 0.00796716, acc 1
2016-09-06T01:36:28.362408: step 8292, loss 0.0512049, acc 0.98
2016-09-06T01:36:29.138780: step 8293, loss 0.00881664, acc 1
2016-09-06T01:36:29.942688: step 8294, loss 0.0187582, acc 1
2016-09-06T01:36:30.789207: step 8295, loss 0.0361695, acc 0.98
2016-09-06T01:36:31.603390: step 8296, loss 0.0028007, acc 1
2016-09-06T01:36:32.408531: step 8297, loss 0.048281, acc 0.98
2016-09-06T01:36:33.239483: step 8298, loss 0.01778, acc 1
2016-09-06T01:36:34.006434: step 8299, loss 0.0262673, acc 0.98
2016-09-06T01:36:34.794827: step 8300, loss 0.00217473, acc 1

Evaluation:
2016-09-06T01:36:38.561629: step 8300, loss 2.14672, acc 0.73546

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-8300

2016-09-06T01:36:40.420395: step 8301, loss 0.0112577, acc 1
2016-09-06T01:36:41.231438: step 8302, loss 0.039558, acc 0.96
2016-09-06T01:36:42.120653: step 8303, loss 0.0304687, acc 0.98
2016-09-06T01:36:42.947045: step 8304, loss 0.0106699, acc 1
2016-09-06T01:36:43.759276: step 8305, loss 0.0260824, acc 1
2016-09-06T01:36:44.593580: step 8306, loss 0.00292616, acc 1
2016-09-06T01:36:45.411585: step 8307, loss 0.00259447, acc 1
2016-09-06T01:36:46.218492: step 8308, loss 0.0447372, acc 0.98
2016-09-06T01:36:47.034128: step 8309, loss 0.0396983, acc 0.96
2016-09-06T01:36:47.849390: step 8310, loss 0.0369423, acc 0.98
2016-09-06T01:36:48.619082: step 8311, loss 0.0104563, acc 1
2016-09-06T01:36:49.420343: step 8312, loss 0.00704525, acc 1
2016-09-06T01:36:50.265599: step 8313, loss 0.00273402, acc 1
2016-09-06T01:36:51.084473: step 8314, loss 0.0231328, acc 1
2016-09-06T01:36:51.863954: step 8315, loss 0.02119, acc 0.98
2016-09-06T01:36:52.667774: step 8316, loss 0.00513972, acc 1
2016-09-06T01:36:53.455328: step 8317, loss 0.0663596, acc 0.96
2016-09-06T01:36:54.268063: step 8318, loss 0.0185457, acc 0.98
2016-09-06T01:36:55.081971: step 8319, loss 0.00303474, acc 1
2016-09-06T01:36:55.868552: step 8320, loss 0.0129423, acc 1
2016-09-06T01:36:56.681088: step 8321, loss 0.00396007, acc 1
2016-09-06T01:36:57.514494: step 8322, loss 0.00959111, acc 1
2016-09-06T01:36:58.308939: step 8323, loss 0.00637203, acc 1
2016-09-06T01:36:59.133949: step 8324, loss 0.0570685, acc 0.96
2016-09-06T01:36:59.943936: step 8325, loss 0.0819739, acc 0.96
2016-09-06T01:37:00.738472: step 8326, loss 0.00219856, acc 1
2016-09-06T01:37:01.529256: step 8327, loss 0.00675885, acc 1
2016-09-06T01:37:02.361644: step 8328, loss 0.00800735, acc 1
2016-09-06T01:37:03.126943: step 8329, loss 0.00329822, acc 1
2016-09-06T01:37:03.919409: step 8330, loss 0.0518888, acc 0.98
2016-09-06T01:37:04.739444: step 8331, loss 0.0188327, acc 0.98
2016-09-06T01:37:05.533876: step 8332, loss 0.0124611, acc 1
2016-09-06T01:37:06.390393: step 8333, loss 0.00256611, acc 1
2016-09-06T01:37:07.206738: step 8334, loss 0.0037202, acc 1
2016-09-06T01:37:08.033493: step 8335, loss 0.0426585, acc 0.98
2016-09-06T01:37:08.843081: step 8336, loss 0.0240191, acc 0.98
2016-09-06T01:37:09.681157: step 8337, loss 0.00906075, acc 1
2016-09-06T01:37:10.486281: step 8338, loss 0.00183285, acc 1
2016-09-06T01:37:11.307800: step 8339, loss 0.0189156, acc 1
2016-09-06T01:37:12.140612: step 8340, loss 0.0402316, acc 0.98
2016-09-06T01:37:12.917410: step 8341, loss 0.0143321, acc 1
2016-09-06T01:37:13.729968: step 8342, loss 0.00390847, acc 1
2016-09-06T01:37:14.517444: step 8343, loss 0.00182748, acc 1
2016-09-06T01:37:15.321413: step 8344, loss 0.00310058, acc 1
2016-09-06T01:37:16.162242: step 8345, loss 0.0438004, acc 0.96
2016-09-06T01:37:16.998762: step 8346, loss 0.0127141, acc 1
2016-09-06T01:37:17.828200: step 8347, loss 0.0276566, acc 0.98
2016-09-06T01:37:18.640285: step 8348, loss 0.00175474, acc 1
2016-09-06T01:37:19.474103: step 8349, loss 0.00633596, acc 1
2016-09-06T01:37:20.287557: step 8350, loss 0.0130941, acc 1
2016-09-06T01:37:21.106406: step 8351, loss 0.0169428, acc 1
2016-09-06T01:37:21.916786: step 8352, loss 0.0630005, acc 0.98
2016-09-06T01:37:22.718434: step 8353, loss 0.0115371, acc 1
2016-09-06T01:37:23.520465: step 8354, loss 0.0186512, acc 1
2016-09-06T01:37:24.366093: step 8355, loss 0.00922969, acc 1
2016-09-06T01:37:25.168272: step 8356, loss 0.00273855, acc 1
2016-09-06T01:37:26.000816: step 8357, loss 0.00538516, acc 1
2016-09-06T01:37:26.827593: step 8358, loss 0.00667978, acc 1
2016-09-06T01:37:27.663043: step 8359, loss 0.0133163, acc 1
2016-09-06T01:37:28.481570: step 8360, loss 0.00711137, acc 1
2016-09-06T01:37:29.285001: step 8361, loss 0.01718, acc 1
2016-09-06T01:37:30.108557: step 8362, loss 0.00523863, acc 1
2016-09-06T01:37:30.894955: step 8363, loss 0.0418234, acc 0.98
2016-09-06T01:37:31.697248: step 8364, loss 0.043358, acc 0.96
2016-09-06T01:37:32.510492: step 8365, loss 0.0228487, acc 1
2016-09-06T01:37:33.321510: step 8366, loss 0.0724101, acc 0.96
2016-09-06T01:37:34.138148: step 8367, loss 0.0113148, acc 1
2016-09-06T01:37:34.949317: step 8368, loss 0.0770789, acc 0.94
2016-09-06T01:37:35.744638: step 8369, loss 0.00681018, acc 1
2016-09-06T01:37:36.538215: step 8370, loss 0.0756881, acc 0.96
2016-09-06T01:37:37.356600: step 8371, loss 0.0046525, acc 1
2016-09-06T01:37:38.157650: step 8372, loss 0.0150573, acc 1
2016-09-06T01:37:38.948983: step 8373, loss 0.033713, acc 0.98
2016-09-06T01:37:39.760127: step 8374, loss 0.00275713, acc 1
2016-09-06T01:37:40.560751: step 8375, loss 0.00693444, acc 1
2016-09-06T01:37:41.348964: step 8376, loss 0.0112485, acc 1
2016-09-06T01:37:42.146083: step 8377, loss 0.0200823, acc 0.98
2016-09-06T01:37:42.947513: step 8378, loss 0.0043222, acc 1
2016-09-06T01:37:43.770977: step 8379, loss 0.00455517, acc 1
2016-09-06T01:37:44.569363: step 8380, loss 0.0265667, acc 0.98
2016-09-06T01:37:45.377190: step 8381, loss 0.0187855, acc 1
2016-09-06T01:37:46.185043: step 8382, loss 0.00275927, acc 1
2016-09-06T01:37:46.979229: step 8383, loss 0.0168463, acc 0.98
2016-09-06T01:37:47.774652: step 8384, loss 0.00260858, acc 1
2016-09-06T01:37:48.575308: step 8385, loss 0.00701039, acc 1
2016-09-06T01:37:49.378007: step 8386, loss 0.00283188, acc 1
2016-09-06T01:37:50.188021: step 8387, loss 0.0638112, acc 0.98
2016-09-06T01:37:50.997598: step 8388, loss 0.0258142, acc 0.98
2016-09-06T01:37:51.862528: step 8389, loss 0.0807169, acc 0.96
2016-09-06T01:37:52.648839: step 8390, loss 0.0426833, acc 0.98
2016-09-06T01:37:53.449160: step 8391, loss 0.0215713, acc 1
2016-09-06T01:37:54.270515: step 8392, loss 0.0175053, acc 0.98
2016-09-06T01:37:55.059333: step 8393, loss 0.0157573, acc 0.98
2016-09-06T01:37:55.856996: step 8394, loss 0.0391844, acc 0.98
2016-09-06T01:37:56.672142: step 8395, loss 0.00202188, acc 1
2016-09-06T01:37:57.459493: step 8396, loss 0.0128637, acc 1
2016-09-06T01:37:58.286305: step 8397, loss 0.0209443, acc 1
2016-09-06T01:37:59.137762: step 8398, loss 0.00936115, acc 1
2016-09-06T01:37:59.924975: step 8399, loss 0.00530244, acc 1
2016-09-06T01:38:00.757684: step 8400, loss 0.0135379, acc 1

Evaluation:
2016-09-06T01:38:04.469200: step 8400, loss 2.26515, acc 0.727017

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-8400

2016-09-06T01:38:06.372307: step 8401, loss 0.0115607, acc 1
2016-09-06T01:38:07.205260: step 8402, loss 0.00216941, acc 1
2016-09-06T01:38:08.007227: step 8403, loss 0.00422845, acc 1
2016-09-06T01:38:08.826232: step 8404, loss 0.0146513, acc 1
2016-09-06T01:38:09.610900: step 8405, loss 0.013923, acc 1
2016-09-06T01:38:10.428439: step 8406, loss 0.113322, acc 0.96
2016-09-06T01:38:11.265771: step 8407, loss 0.00593865, acc 1
2016-09-06T01:38:12.079977: step 8408, loss 0.00324841, acc 1
2016-09-06T01:38:12.884152: step 8409, loss 0.0143191, acc 1
2016-09-06T01:38:13.687131: step 8410, loss 0.00478034, acc 1
2016-09-06T01:38:14.470922: step 8411, loss 0.00941793, acc 1
2016-09-06T01:38:15.280118: step 8412, loss 0.0660106, acc 0.94
2016-09-06T01:38:16.085398: step 8413, loss 0.0408178, acc 0.98
2016-09-06T01:38:16.858345: step 8414, loss 0.0141166, acc 1
2016-09-06T01:38:17.668532: step 8415, loss 0.0501049, acc 0.98
2016-09-06T01:38:18.475887: step 8416, loss 0.0131406, acc 1
2016-09-06T01:38:19.272600: step 8417, loss 0.0847653, acc 0.96
2016-09-06T01:38:20.109658: step 8418, loss 0.0848052, acc 0.98
2016-09-06T01:38:20.947760: step 8419, loss 0.00338842, acc 1
2016-09-06T01:38:21.721452: step 8420, loss 0.00351042, acc 1
2016-09-06T01:38:22.543280: step 8421, loss 0.00482972, acc 1
2016-09-06T01:38:23.351114: step 8422, loss 0.0397037, acc 0.98
2016-09-06T01:38:24.126450: step 8423, loss 0.0092562, acc 1
2016-09-06T01:38:24.935234: step 8424, loss 0.00629819, acc 1
2016-09-06T01:38:25.750832: step 8425, loss 0.0355621, acc 1
2016-09-06T01:38:26.527477: step 8426, loss 0.0261891, acc 0.98
2016-09-06T01:38:27.331878: step 8427, loss 0.0251172, acc 0.98
2016-09-06T01:38:28.165405: step 8428, loss 0.0269464, acc 1
2016-09-06T01:38:28.985535: step 8429, loss 0.0457577, acc 0.96
2016-09-06T01:38:29.784849: step 8430, loss 0.0107404, acc 1
2016-09-06T01:38:30.597457: step 8431, loss 0.0599433, acc 0.96
2016-09-06T01:38:31.374383: step 8432, loss 0.00425375, acc 1
2016-09-06T01:38:32.203547: step 8433, loss 0.0220825, acc 0.98
2016-09-06T01:38:32.995908: step 8434, loss 0.00697136, acc 1
2016-09-06T01:38:33.749549: step 8435, loss 0.0224358, acc 0.98
2016-09-06T01:38:34.560058: step 8436, loss 0.00537613, acc 1
2016-09-06T01:38:35.391919: step 8437, loss 0.00677243, acc 1
2016-09-06T01:38:36.188594: step 8438, loss 0.0240622, acc 0.98
2016-09-06T01:38:36.999472: step 8439, loss 0.00361655, acc 1
2016-09-06T01:38:37.813731: step 8440, loss 0.00342803, acc 1
2016-09-06T01:38:38.594605: step 8441, loss 0.00346868, acc 1
2016-09-06T01:38:39.383457: step 8442, loss 0.0315244, acc 0.98
2016-09-06T01:38:40.181164: step 8443, loss 0.0168349, acc 1
2016-09-06T01:38:41.008655: step 8444, loss 0.011187, acc 1
2016-09-06T01:38:41.817216: step 8445, loss 0.0108442, acc 1
2016-09-06T01:38:42.660158: step 8446, loss 0.0668735, acc 0.96
2016-09-06T01:38:43.457148: step 8447, loss 0.0118124, acc 1
2016-09-06T01:38:44.247053: step 8448, loss 0.00448603, acc 1
2016-09-06T01:38:45.071141: step 8449, loss 0.0130422, acc 1
2016-09-06T01:38:45.860064: step 8450, loss 0.0222036, acc 1
2016-09-06T01:38:46.665349: step 8451, loss 0.0185523, acc 1
2016-09-06T01:38:47.487337: step 8452, loss 0.039504, acc 0.98
2016-09-06T01:38:48.285443: step 8453, loss 0.058076, acc 0.98
2016-09-06T01:38:49.088380: step 8454, loss 0.00928454, acc 1
2016-09-06T01:38:49.921829: step 8455, loss 0.0220965, acc 1
2016-09-06T01:38:50.706894: step 8456, loss 0.00335081, acc 1
2016-09-06T01:38:51.491241: step 8457, loss 0.0264315, acc 0.98
2016-09-06T01:38:52.321868: step 8458, loss 0.00431354, acc 1
2016-09-06T01:38:53.099306: step 8459, loss 0.0169288, acc 1
2016-09-06T01:38:53.900758: step 8460, loss 0.0287806, acc 0.98
2016-09-06T01:38:54.726461: step 8461, loss 0.100915, acc 0.98
2016-09-06T01:38:55.522479: step 8462, loss 0.00302547, acc 1
2016-09-06T01:38:56.338964: step 8463, loss 0.00878843, acc 1
2016-09-06T01:38:57.144874: step 8464, loss 0.0214028, acc 1
2016-09-06T01:38:57.938911: step 8465, loss 0.0275366, acc 0.98
2016-09-06T01:38:58.743509: step 8466, loss 0.0663838, acc 0.98
2016-09-06T01:38:59.538108: step 8467, loss 0.00482847, acc 1
2016-09-06T01:39:00.339022: step 8468, loss 0.0180523, acc 0.98
2016-09-06T01:39:01.131868: step 8469, loss 0.00437411, acc 1
2016-09-06T01:39:01.950868: step 8470, loss 0.00630266, acc 1
2016-09-06T01:39:02.736615: step 8471, loss 0.0302069, acc 1
2016-09-06T01:39:03.542693: step 8472, loss 0.0370948, acc 1
2016-09-06T01:39:04.344883: step 8473, loss 0.0173501, acc 1
2016-09-06T01:39:05.132147: step 8474, loss 0.0196233, acc 0.98
2016-09-06T01:39:05.963148: step 8475, loss 0.0308628, acc 0.98
2016-09-06T01:39:06.787149: step 8476, loss 0.00570498, acc 1
2016-09-06T01:39:07.581441: step 8477, loss 0.0298407, acc 1
2016-09-06T01:39:08.424833: step 8478, loss 0.00415742, acc 1
2016-09-06T01:39:09.273941: step 8479, loss 0.00947127, acc 1
2016-09-06T01:39:10.086220: step 8480, loss 0.00473356, acc 1
2016-09-06T01:39:10.882344: step 8481, loss 0.00443549, acc 1
2016-09-06T01:39:11.691274: step 8482, loss 0.0184077, acc 0.98
2016-09-06T01:39:12.473398: step 8483, loss 0.0168151, acc 1
2016-09-06T01:39:13.246758: step 8484, loss 0.00416234, acc 1
2016-09-06T01:39:14.069081: step 8485, loss 0.00287979, acc 1
2016-09-06T01:39:14.859098: step 8486, loss 0.0309581, acc 0.98
2016-09-06T01:39:15.658504: step 8487, loss 0.0347569, acc 0.96
2016-09-06T01:39:16.478861: step 8488, loss 0.00347318, acc 1
2016-09-06T01:39:17.259369: step 8489, loss 0.00848455, acc 1
2016-09-06T01:39:18.077866: step 8490, loss 0.00303874, acc 1
2016-09-06T01:39:18.872737: step 8491, loss 0.0512595, acc 0.94
2016-09-06T01:39:19.676254: step 8492, loss 0.00343841, acc 1
2016-09-06T01:39:20.472732: step 8493, loss 0.00287308, acc 1
2016-09-06T01:39:21.289391: step 8494, loss 0.0267773, acc 1
2016-09-06T01:39:22.127867: step 8495, loss 0.0360888, acc 0.98
2016-09-06T01:39:22.926067: step 8496, loss 0.0176985, acc 0.98
2016-09-06T01:39:23.771536: step 8497, loss 0.0356411, acc 0.98
2016-09-06T01:39:24.556108: step 8498, loss 0.00679693, acc 1
2016-09-06T01:39:25.354383: step 8499, loss 0.013698, acc 1
2016-09-06T01:39:26.171027: step 8500, loss 0.0233099, acc 1

Evaluation:
2016-09-06T01:39:29.886442: step 8500, loss 2.53118, acc 0.714822

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-8500

2016-09-06T01:39:31.870493: step 8501, loss 0.0475048, acc 0.94
2016-09-06T01:39:32.674408: step 8502, loss 0.00353119, acc 1
2016-09-06T01:39:33.523506: step 8503, loss 0.034661, acc 0.98
2016-09-06T01:39:34.341959: step 8504, loss 0.00341847, acc 1
2016-09-06T01:39:35.183863: step 8505, loss 0.00605837, acc 1
2016-09-06T01:39:36.025443: step 8506, loss 0.00878183, acc 1
2016-09-06T01:39:36.833251: step 8507, loss 0.00918236, acc 1
2016-09-06T01:39:37.644264: step 8508, loss 0.00299453, acc 1
2016-09-06T01:39:38.465036: step 8509, loss 0.0266828, acc 0.98
2016-09-06T01:39:39.281102: step 8510, loss 0.00314538, acc 1
2016-09-06T01:39:40.087782: step 8511, loss 0.0265051, acc 0.98
2016-09-06T01:39:40.930334: step 8512, loss 0.0028602, acc 1
2016-09-06T01:39:41.757549: step 8513, loss 0.00396122, acc 1
2016-09-06T01:39:42.561637: step 8514, loss 0.0031069, acc 1
2016-09-06T01:39:43.405232: step 8515, loss 0.143325, acc 0.98
2016-09-06T01:39:44.252901: step 8516, loss 0.0428511, acc 0.96
2016-09-06T01:39:45.046966: step 8517, loss 0.0221982, acc 0.98
2016-09-06T01:39:45.876727: step 8518, loss 0.0491627, acc 0.96
2016-09-06T01:39:46.673573: step 8519, loss 0.00523792, acc 1
2016-09-06T01:39:47.480564: step 8520, loss 0.00410655, acc 1
2016-09-06T01:39:48.298454: step 8521, loss 0.0308048, acc 0.98
2016-09-06T01:39:49.108154: step 8522, loss 0.00242529, acc 1
2016-09-06T01:39:49.899359: step 8523, loss 0.0165968, acc 1
2016-09-06T01:39:50.722814: step 8524, loss 0.0149176, acc 1
2016-09-06T01:39:51.569749: step 8525, loss 0.0240477, acc 0.98
2016-09-06T01:39:52.353655: step 8526, loss 0.0149831, acc 1
2016-09-06T01:39:53.145481: step 8527, loss 0.0510631, acc 0.96
2016-09-06T01:39:53.945480: step 8528, loss 0.00847844, acc 1
2016-09-06T01:39:54.756222: step 8529, loss 0.00277689, acc 1
2016-09-06T01:39:55.565542: step 8530, loss 0.00239805, acc 1
2016-09-06T01:39:56.386225: step 8531, loss 0.0217557, acc 0.98
2016-09-06T01:39:57.207209: step 8532, loss 0.00223158, acc 1
2016-09-06T01:39:57.995041: step 8533, loss 0.0637224, acc 0.98
2016-09-06T01:39:58.788660: step 8534, loss 0.0263364, acc 0.98
2016-09-06T01:39:59.577339: step 8535, loss 0.00524858, acc 1
2016-09-06T01:40:00.416987: step 8536, loss 0.0280755, acc 0.98
2016-09-06T01:40:01.278532: step 8537, loss 0.00338188, acc 1
2016-09-06T01:40:02.088571: step 8538, loss 0.00201646, acc 1
2016-09-06T01:40:02.892246: step 8539, loss 0.0135578, acc 1
2016-09-06T01:40:03.706000: step 8540, loss 0.00197426, acc 1
2016-09-06T01:40:04.471205: step 8541, loss 0.00481273, acc 1
2016-09-06T01:40:05.301394: step 8542, loss 0.0140548, acc 1
2016-09-06T01:40:06.128707: step 8543, loss 0.0193621, acc 1
2016-09-06T01:40:06.932149: step 8544, loss 0.00197835, acc 1
2016-09-06T01:40:07.742578: step 8545, loss 0.00246816, acc 1
2016-09-06T01:40:08.558681: step 8546, loss 0.00852022, acc 1
2016-09-06T01:40:09.373577: step 8547, loss 0.0267253, acc 0.98
2016-09-06T01:40:10.181470: step 8548, loss 0.00558488, acc 1
2016-09-06T01:40:11.016417: step 8549, loss 0.02212, acc 1
2016-09-06T01:40:11.811798: step 8550, loss 0.0156806, acc 1
2016-09-06T01:40:12.617450: step 8551, loss 0.00247343, acc 1
2016-09-06T01:40:13.465626: step 8552, loss 0.00261076, acc 1
2016-09-06T01:40:14.269627: step 8553, loss 0.0177694, acc 0.98
2016-09-06T01:40:15.081145: step 8554, loss 0.0160051, acc 1
2016-09-06T01:40:15.898997: step 8555, loss 0.0764363, acc 0.94
2016-09-06T01:40:16.703429: step 8556, loss 0.0623127, acc 0.96
2016-09-06T01:40:17.511659: step 8557, loss 0.0438056, acc 0.98
2016-09-06T01:40:18.346932: step 8558, loss 0.0272141, acc 1
2016-09-06T01:40:19.157771: step 8559, loss 0.00244987, acc 1
2016-09-06T01:40:19.966936: step 8560, loss 0.0483847, acc 0.98
2016-09-06T01:40:20.807356: step 8561, loss 0.0341347, acc 0.98
2016-09-06T01:40:21.619625: step 8562, loss 0.00281703, acc 1
2016-09-06T01:40:22.411012: step 8563, loss 0.0881343, acc 0.94
2016-09-06T01:40:23.265268: step 8564, loss 0.0067054, acc 1
2016-09-06T01:40:24.107734: step 8565, loss 0.00709336, acc 1
2016-09-06T01:40:24.926975: step 8566, loss 0.00216641, acc 1
2016-09-06T01:40:25.761187: step 8567, loss 0.0162017, acc 1
2016-09-06T01:40:26.571622: step 8568, loss 0.00215058, acc 1
2016-09-06T01:40:27.378322: step 8569, loss 0.00200544, acc 1
2016-09-06T01:40:28.201847: step 8570, loss 0.00613204, acc 1
2016-09-06T01:40:29.031612: step 8571, loss 0.00206521, acc 1
2016-09-06T01:40:29.827903: step 8572, loss 0.0119833, acc 1
2016-09-06T01:40:30.659101: step 8573, loss 0.0232547, acc 0.98
2016-09-06T01:40:31.473133: step 8574, loss 0.0872951, acc 0.98
2016-09-06T01:40:32.245735: step 8575, loss 0.0165392, acc 0.98
2016-09-06T01:40:33.060293: step 8576, loss 0.00276824, acc 1
2016-09-06T01:40:33.902321: step 8577, loss 0.00753518, acc 1
2016-09-06T01:40:34.713470: step 8578, loss 0.0362186, acc 0.98
2016-09-06T01:40:35.534918: step 8579, loss 0.0177216, acc 1
2016-09-06T01:40:36.355988: step 8580, loss 0.0142674, acc 1
2016-09-06T01:40:37.169270: step 8581, loss 0.0223323, acc 1
2016-09-06T01:40:37.968313: step 8582, loss 0.0344431, acc 0.98
2016-09-06T01:40:38.792608: step 8583, loss 0.00777827, acc 1
2016-09-06T01:40:39.592203: step 8584, loss 0.00259873, acc 1
2016-09-06T01:40:40.399421: step 8585, loss 0.0180089, acc 0.98
2016-09-06T01:40:41.278313: step 8586, loss 0.0401849, acc 0.98
2016-09-06T01:40:42.082691: step 8587, loss 0.00215868, acc 1
2016-09-06T01:40:42.899952: step 8588, loss 0.0093561, acc 1
2016-09-06T01:40:43.706166: step 8589, loss 0.0263176, acc 1
2016-09-06T01:40:44.531101: step 8590, loss 0.00339394, acc 1
2016-09-06T01:40:45.348561: step 8591, loss 0.0149307, acc 1
2016-09-06T01:40:46.201797: step 8592, loss 0.0151281, acc 1
2016-09-06T01:40:47.029141: step 8593, loss 0.0132513, acc 1
2016-09-06T01:40:47.849403: step 8594, loss 0.00270534, acc 1
2016-09-06T01:40:48.683002: step 8595, loss 0.0119748, acc 1
2016-09-06T01:40:49.494569: step 8596, loss 0.0188322, acc 0.98
2016-09-06T01:40:50.307651: step 8597, loss 0.0553537, acc 0.98
2016-09-06T01:40:51.140263: step 8598, loss 0.00338498, acc 1
2016-09-06T01:40:51.921313: step 8599, loss 0.00559782, acc 1
2016-09-06T01:40:52.744101: step 8600, loss 0.00545278, acc 1

Evaluation:
2016-09-06T01:40:56.493631: step 8600, loss 2.67328, acc 0.712946

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-8600

2016-09-06T01:40:58.368508: step 8601, loss 0.0348413, acc 0.98
2016-09-06T01:40:59.177190: step 8602, loss 0.0023605, acc 1
2016-09-06T01:40:59.994774: step 8603, loss 0.00266715, acc 1
2016-09-06T01:41:00.857306: step 8604, loss 0.0151772, acc 1
2016-09-06T01:41:01.701291: step 8605, loss 0.00301622, acc 1
2016-09-06T01:41:02.479179: step 8606, loss 0.0154378, acc 1
2016-09-06T01:41:03.298614: step 8607, loss 0.0128345, acc 1
2016-09-06T01:41:04.118103: step 8608, loss 0.0346241, acc 1
2016-09-06T01:41:04.909994: step 8609, loss 0.0130293, acc 1
2016-09-06T01:41:05.704965: step 8610, loss 0.0120039, acc 1
2016-09-06T01:41:06.528499: step 8611, loss 0.0348807, acc 0.98
2016-09-06T01:41:07.314172: step 8612, loss 0.00390801, acc 1
2016-09-06T01:41:08.125227: step 8613, loss 0.00422064, acc 1
2016-09-06T01:41:08.946066: step 8614, loss 0.0310877, acc 0.98
2016-09-06T01:41:09.739666: step 8615, loss 0.165301, acc 0.98
2016-09-06T01:41:10.565491: step 8616, loss 0.00339132, acc 1
2016-09-06T01:41:11.378561: step 8617, loss 0.015073, acc 1
2016-09-06T01:41:12.154887: step 8618, loss 0.0233636, acc 0.98
2016-09-06T01:41:12.982861: step 8619, loss 0.00388275, acc 1
2016-09-06T01:41:13.787967: step 8620, loss 0.0270611, acc 0.98
2016-09-06T01:41:14.560708: step 8621, loss 0.00290414, acc 1
2016-09-06T01:41:15.375113: step 8622, loss 0.0337903, acc 0.98
2016-09-06T01:41:16.179300: step 8623, loss 0.0566354, acc 0.94
2016-09-06T01:41:17.009504: step 8624, loss 0.00694282, acc 1
2016-09-06T01:41:17.805673: step 8625, loss 0.0065302, acc 1
2016-09-06T01:41:18.620222: step 8626, loss 0.068928, acc 0.96
2016-09-06T01:41:19.424804: step 8627, loss 0.012472, acc 1
2016-09-06T01:41:20.252133: step 8628, loss 0.0363737, acc 1
2016-09-06T01:41:21.092642: step 8629, loss 0.00388791, acc 1
2016-09-06T01:41:21.868080: step 8630, loss 0.00519326, acc 1
2016-09-06T01:41:22.646136: step 8631, loss 0.00286806, acc 1
2016-09-06T01:41:23.450906: step 8632, loss 0.00480085, acc 1
2016-09-06T01:41:24.239713: step 8633, loss 0.0450045, acc 0.98
2016-09-06T01:41:25.060476: step 8634, loss 0.0382424, acc 0.98
2016-09-06T01:41:25.880054: step 8635, loss 0.0136819, acc 1
2016-09-06T01:41:26.666532: step 8636, loss 0.00272611, acc 1
2016-09-06T01:41:27.480301: step 8637, loss 0.0284053, acc 1
2016-09-06T01:41:28.281788: step 8638, loss 0.0254169, acc 0.98
2016-09-06T01:41:29.125222: step 8639, loss 0.0277717, acc 1
2016-09-06T01:41:29.908193: step 8640, loss 0.0113186, acc 1
2016-09-06T01:41:30.736258: step 8641, loss 0.01841, acc 0.98
2016-09-06T01:41:31.522893: step 8642, loss 0.00786438, acc 1
2016-09-06T01:41:32.320990: step 8643, loss 0.00483616, acc 1
2016-09-06T01:41:33.128548: step 8644, loss 0.00596526, acc 1
2016-09-06T01:41:33.939151: step 8645, loss 0.00670667, acc 1
2016-09-06T01:41:34.728177: step 8646, loss 0.0089014, acc 1
2016-09-06T01:41:35.548139: step 8647, loss 0.00217957, acc 1
2016-09-06T01:41:36.339981: step 8648, loss 0.0479342, acc 0.98
2016-09-06T01:41:37.134258: step 8649, loss 0.021154, acc 1
2016-09-06T01:41:37.915727: step 8650, loss 0.0371234, acc 0.98
2016-09-06T01:41:38.692610: step 8651, loss 0.0156824, acc 1
2016-09-06T01:41:39.561005: step 8652, loss 0.0242471, acc 0.98
2016-09-06T01:41:40.363399: step 8653, loss 0.00227177, acc 1
2016-09-06T01:41:41.164132: step 8654, loss 0.00231004, acc 1
2016-09-06T01:41:41.981573: step 8655, loss 0.0376765, acc 0.98
2016-09-06T01:41:42.778867: step 8656, loss 0.0460028, acc 0.96
2016-09-06T01:41:43.565989: step 8657, loss 0.00238035, acc 1
2016-09-06T01:41:44.389567: step 8658, loss 0.00645162, acc 1
2016-09-06T01:41:45.198575: step 8659, loss 0.00436333, acc 1
2016-09-06T01:41:45.969191: step 8660, loss 0.0287151, acc 0.98
2016-09-06T01:41:46.789488: step 8661, loss 0.0246861, acc 1
2016-09-06T01:41:47.597682: step 8662, loss 0.0317288, acc 0.98
2016-09-06T01:41:48.397630: step 8663, loss 0.0244049, acc 0.98
2016-09-06T01:41:49.212854: step 8664, loss 0.00669941, acc 1
2016-09-06T01:41:50.042553: step 8665, loss 0.0305238, acc 0.98
2016-09-06T01:41:50.865630: step 8666, loss 0.0164511, acc 0.98
2016-09-06T01:41:51.668781: step 8667, loss 0.0123643, acc 1
2016-09-06T01:41:52.477732: step 8668, loss 0.00478767, acc 1
2016-09-06T01:41:53.255620: step 8669, loss 0.0870398, acc 0.96
2016-09-06T01:41:54.079042: step 8670, loss 0.00240276, acc 1
2016-09-06T01:41:54.883514: step 8671, loss 0.027148, acc 0.98
2016-09-06T01:41:55.658943: step 8672, loss 0.245019, acc 0.96
2016-09-06T01:41:56.464465: step 8673, loss 0.0402496, acc 0.98
2016-09-06T01:41:57.264599: step 8674, loss 0.0223597, acc 0.98
2016-09-06T01:41:58.071954: step 8675, loss 0.00242794, acc 1
2016-09-06T01:41:58.870893: step 8676, loss 0.0945856, acc 0.96
2016-09-06T01:41:59.668316: step 8677, loss 0.0181456, acc 0.98
2016-09-06T01:42:00.483638: step 8678, loss 0.0176191, acc 1
2016-09-06T01:42:01.271362: step 8679, loss 0.0219928, acc 1
2016-09-06T01:42:02.079819: step 8680, loss 0.00521049, acc 1
2016-09-06T01:42:02.866279: step 8681, loss 0.0103878, acc 1
2016-09-06T01:42:03.696305: step 8682, loss 0.0383273, acc 0.98
2016-09-06T01:42:04.482343: step 8683, loss 0.0417211, acc 0.98
2016-09-06T01:42:05.300139: step 8684, loss 0.00514499, acc 1
2016-09-06T01:42:06.146669: step 8685, loss 0.00305375, acc 1
2016-09-06T01:42:06.981082: step 8686, loss 0.00428305, acc 1
2016-09-06T01:42:07.774855: step 8687, loss 0.014878, acc 1
2016-09-06T01:42:08.592706: step 8688, loss 0.0204972, acc 0.98
2016-09-06T01:42:09.438939: step 8689, loss 0.0387627, acc 0.98
2016-09-06T01:42:10.225423: step 8690, loss 0.00274856, acc 1
2016-09-06T01:42:11.029675: step 8691, loss 0.0250258, acc 0.98
2016-09-06T01:42:11.828228: step 8692, loss 0.0631139, acc 0.96
2016-09-06T01:42:12.607190: step 8693, loss 0.0315294, acc 1
2016-09-06T01:42:13.405872: step 8694, loss 0.00834777, acc 1
2016-09-06T01:42:14.213405: step 8695, loss 0.00443862, acc 1
2016-09-06T01:42:15.023681: step 8696, loss 0.00307361, acc 1
2016-09-06T01:42:15.832652: step 8697, loss 0.0175999, acc 1
2016-09-06T01:42:16.657757: step 8698, loss 0.00875816, acc 1
2016-09-06T01:42:17.451126: step 8699, loss 0.0104834, acc 1
2016-09-06T01:42:18.257066: step 8700, loss 0.0174469, acc 0.98

Evaluation:
2016-09-06T01:42:21.991436: step 8700, loss 2.24718, acc 0.716698

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-8700

2016-09-06T01:42:23.827519: step 8701, loss 0.0312846, acc 1
2016-09-06T01:42:24.643281: step 8702, loss 0.0296913, acc 1
2016-09-06T01:42:25.500896: step 8703, loss 0.00403416, acc 1
2016-09-06T01:42:26.314621: step 8704, loss 0.0648287, acc 0.98
2016-09-06T01:42:27.105815: step 8705, loss 0.017421, acc 1
2016-09-06T01:42:27.925477: step 8706, loss 0.0974409, acc 0.98
2016-09-06T01:42:28.760303: step 8707, loss 0.00349256, acc 1
2016-09-06T01:42:29.545459: step 8708, loss 0.0965522, acc 0.94
2016-09-06T01:42:30.361050: step 8709, loss 0.0172603, acc 1
2016-09-06T01:42:31.170051: step 8710, loss 0.00495142, acc 1
2016-09-06T01:42:31.973342: step 8711, loss 0.0192053, acc 0.98
2016-09-06T01:42:32.767841: step 8712, loss 0.00271166, acc 1
2016-09-06T01:42:33.577565: step 8713, loss 0.0148963, acc 1
2016-09-06T01:42:34.326564: step 8714, loss 0.0110514, acc 1
2016-09-06T01:42:35.165856: step 8715, loss 0.0169456, acc 1
2016-09-06T01:42:35.947668: step 8716, loss 0.00353199, acc 1
2016-09-06T01:42:36.770604: step 8717, loss 0.00561716, acc 1
2016-09-06T01:42:37.601812: step 8718, loss 0.0079216, acc 1
2016-09-06T01:42:38.442047: step 8719, loss 0.00270209, acc 1
2016-09-06T01:42:39.241015: step 8720, loss 0.0191364, acc 0.98
2016-09-06T01:42:40.051007: step 8721, loss 0.00522218, acc 1
2016-09-06T01:42:40.880702: step 8722, loss 0.00449192, acc 1
2016-09-06T01:42:41.660219: step 8723, loss 0.0091901, acc 1
2016-09-06T01:42:42.447152: step 8724, loss 0.00335656, acc 1
2016-09-06T01:42:43.258260: step 8725, loss 0.108849, acc 0.96
2016-09-06T01:42:44.040904: step 8726, loss 0.041248, acc 0.98
2016-09-06T01:42:44.906016: step 8727, loss 0.00580112, acc 1
2016-09-06T01:42:45.754603: step 8728, loss 0.00971354, acc 1
2016-09-06T01:42:46.540464: step 8729, loss 0.0219515, acc 0.98
2016-09-06T01:42:47.357054: step 8730, loss 0.00273604, acc 1
2016-09-06T01:42:48.159819: step 8731, loss 0.0153009, acc 1
2016-09-06T01:42:48.949398: step 8732, loss 0.00233082, acc 1
2016-09-06T01:42:49.718664: step 8733, loss 0.0372298, acc 0.98
2016-09-06T01:42:50.509859: step 8734, loss 0.0114823, acc 1
2016-09-06T01:42:51.298985: step 8735, loss 0.0108272, acc 1
2016-09-06T01:42:52.132569: step 8736, loss 0.0124427, acc 1
2016-09-06T01:42:52.942790: step 8737, loss 0.00509662, acc 1
2016-09-06T01:42:53.736692: step 8738, loss 0.0898442, acc 0.96
2016-09-06T01:42:54.550587: step 8739, loss 0.00282691, acc 1
2016-09-06T01:42:55.397151: step 8740, loss 0.036463, acc 0.98
2016-09-06T01:42:56.196606: step 8741, loss 0.0122842, acc 1
2016-09-06T01:42:56.980636: step 8742, loss 0.00528516, acc 1
2016-09-06T01:42:57.792687: step 8743, loss 0.0840048, acc 0.96
2016-09-06T01:42:58.587771: step 8744, loss 0.00214731, acc 1
2016-09-06T01:42:59.387863: step 8745, loss 0.0319332, acc 0.98
2016-09-06T01:43:00.216021: step 8746, loss 0.0166033, acc 1
2016-09-06T01:43:01.016025: step 8747, loss 0.00759973, acc 1
2016-09-06T01:43:01.830376: step 8748, loss 0.0264435, acc 0.98
2016-09-06T01:43:02.654967: step 8749, loss 0.0256696, acc 1
2016-09-06T01:43:03.432467: step 8750, loss 0.167792, acc 0.96
2016-09-06T01:43:04.242110: step 8751, loss 0.00638327, acc 1
2016-09-06T01:43:05.089731: step 8752, loss 0.0204317, acc 1
2016-09-06T01:43:05.887637: step 8753, loss 0.0186446, acc 0.98
2016-09-06T01:43:06.655002: step 8754, loss 0.0327501, acc 0.98
2016-09-06T01:43:07.489493: step 8755, loss 0.0166207, acc 1
2016-09-06T01:43:08.265122: step 8756, loss 0.00250839, acc 1
2016-09-06T01:43:09.069744: step 8757, loss 0.00594785, acc 1
2016-09-06T01:43:09.880199: step 8758, loss 0.00647835, acc 1
2016-09-06T01:43:10.683552: step 8759, loss 0.00982231, acc 1
2016-09-06T01:43:11.492787: step 8760, loss 0.0616396, acc 0.98
2016-09-06T01:43:12.330619: step 8761, loss 0.0103273, acc 1
2016-09-06T01:43:13.131683: step 8762, loss 0.00422706, acc 1
2016-09-06T01:43:13.936221: step 8763, loss 0.0576146, acc 0.98
2016-09-06T01:43:14.723506: step 8764, loss 0.0148703, acc 1
2016-09-06T01:43:15.503120: step 8765, loss 0.0390198, acc 0.98
2016-09-06T01:43:16.291717: step 8766, loss 0.016015, acc 1
2016-09-06T01:43:17.100376: step 8767, loss 0.0433924, acc 0.96
2016-09-06T01:43:17.893389: step 8768, loss 0.0270955, acc 0.98
2016-09-06T01:43:18.705804: step 8769, loss 0.0071719, acc 1
2016-09-06T01:43:19.527123: step 8770, loss 0.0525494, acc 0.98
2016-09-06T01:43:20.325748: step 8771, loss 0.0124508, acc 1
2016-09-06T01:43:21.135490: step 8772, loss 0.0104853, acc 1
2016-09-06T01:43:21.949992: step 8773, loss 0.0145934, acc 1
2016-09-06T01:43:22.730685: step 8774, loss 0.0391221, acc 0.98
2016-09-06T01:43:23.545383: step 8775, loss 0.0563738, acc 0.98
2016-09-06T01:43:24.345455: step 8776, loss 0.0168199, acc 0.98
2016-09-06T01:43:25.136771: step 8777, loss 0.0133504, acc 1
2016-09-06T01:43:25.964322: step 8778, loss 0.019636, acc 0.98
2016-09-06T01:43:26.776798: step 8779, loss 0.00257868, acc 1
2016-09-06T01:43:27.574551: step 8780, loss 0.00663694, acc 1
2016-09-06T01:43:28.396037: step 8781, loss 0.0708515, acc 0.96
2016-09-06T01:43:29.211905: step 8782, loss 0.0194058, acc 0.98
2016-09-06T01:43:30.023845: step 8783, loss 0.00527959, acc 1
2016-09-06T01:43:30.843750: step 8784, loss 0.0381432, acc 1
2016-09-06T01:43:31.682990: step 8785, loss 0.0108987, acc 1
2016-09-06T01:43:32.478120: step 8786, loss 0.0217169, acc 0.98
2016-09-06T01:43:33.254687: step 8787, loss 0.0148535, acc 1
2016-09-06T01:43:34.063326: step 8788, loss 0.016934, acc 1
2016-09-06T01:43:34.831705: step 8789, loss 0.00442437, acc 1
2016-09-06T01:43:35.651375: step 8790, loss 0.0322824, acc 1
2016-09-06T01:43:36.467614: step 8791, loss 0.00648533, acc 1
2016-09-06T01:43:37.253369: step 8792, loss 0.00721282, acc 1
2016-09-06T01:43:38.048404: step 8793, loss 0.0304307, acc 0.98
2016-09-06T01:43:38.901840: step 8794, loss 0.0058001, acc 1
2016-09-06T01:43:39.697328: step 8795, loss 0.0178494, acc 1
2016-09-06T01:43:40.517528: step 8796, loss 0.0114217, acc 1
2016-09-06T01:43:41.347297: step 8797, loss 0.00327023, acc 1
2016-09-06T01:43:42.117745: step 8798, loss 0.0181819, acc 0.98
2016-09-06T01:43:42.933931: step 8799, loss 0.00322374, acc 1
2016-09-06T01:43:43.744635: step 8800, loss 0.0309106, acc 0.98

Evaluation:
2016-09-06T01:43:47.466657: step 8800, loss 3.07655, acc 0.714822

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-8800

2016-09-06T01:43:49.446708: step 8801, loss 0.00824567, acc 1
2016-09-06T01:43:50.235471: step 8802, loss 0.172214, acc 0.96
2016-09-06T01:43:51.053907: step 8803, loss 0.0341967, acc 0.98
2016-09-06T01:43:51.864837: step 8804, loss 0.00303969, acc 1
2016-09-06T01:43:52.689979: step 8805, loss 0.128515, acc 0.98
2016-09-06T01:43:53.528433: step 8806, loss 0.0558259, acc 0.98
2016-09-06T01:43:54.356577: step 8807, loss 0.00696734, acc 1
2016-09-06T01:43:55.167488: step 8808, loss 0.00318715, acc 1
2016-09-06T01:43:55.978091: step 8809, loss 0.0241154, acc 1
2016-09-06T01:43:56.764430: step 8810, loss 0.00436501, acc 1
2016-09-06T01:43:57.556460: step 8811, loss 0.0455878, acc 0.96
2016-09-06T01:43:58.402896: step 8812, loss 0.0296074, acc 1
2016-09-06T01:43:59.215045: step 8813, loss 0.00317241, acc 1
2016-09-06T01:44:00.017070: step 8814, loss 0.00604605, acc 1
2016-09-06T01:44:00.830363: step 8815, loss 0.127856, acc 0.96
2016-09-06T01:44:01.647801: step 8816, loss 0.00227798, acc 1
2016-09-06T01:44:02.450280: step 8817, loss 0.0291871, acc 0.98
2016-09-06T01:44:03.264035: step 8818, loss 0.0371869, acc 1
2016-09-06T01:44:04.078394: step 8819, loss 0.018493, acc 0.98
2016-09-06T01:44:04.888122: step 8820, loss 0.0282963, acc 0.98
2016-09-06T01:44:05.719187: step 8821, loss 0.0147429, acc 1
2016-09-06T01:44:06.534086: step 8822, loss 0.025655, acc 0.98
2016-09-06T01:44:07.344425: step 8823, loss 0.0138465, acc 1
2016-09-06T01:44:08.186841: step 8824, loss 0.00330356, acc 1
2016-09-06T01:44:08.986712: step 8825, loss 0.0167019, acc 1
2016-09-06T01:44:09.774869: step 8826, loss 0.0235472, acc 0.98
2016-09-06T01:44:10.623463: step 8827, loss 0.0319673, acc 0.98
2016-09-06T01:44:11.510018: step 8828, loss 0.00383689, acc 1
2016-09-06T01:44:12.292106: step 8829, loss 0.0309666, acc 1
2016-09-06T01:44:13.105862: step 8830, loss 0.00864805, acc 1
2016-09-06T01:44:13.935788: step 8831, loss 0.0180273, acc 0.98
2016-09-06T01:44:14.673689: step 8832, loss 0.0031607, acc 1
2016-09-06T01:44:15.506061: step 8833, loss 0.00370462, acc 1
2016-09-06T01:44:16.316027: step 8834, loss 0.0298799, acc 1
2016-09-06T01:44:17.127840: step 8835, loss 0.00246733, acc 1
2016-09-06T01:44:17.952113: step 8836, loss 0.0513765, acc 0.98
2016-09-06T01:44:18.767756: step 8837, loss 0.00294604, acc 1
2016-09-06T01:44:19.592852: step 8838, loss 0.0034333, acc 1
2016-09-06T01:44:20.429032: step 8839, loss 0.0135109, acc 1
2016-09-06T01:44:21.242333: step 8840, loss 0.0122886, acc 1
2016-09-06T01:44:22.056484: step 8841, loss 0.0154285, acc 1
2016-09-06T01:44:22.846413: step 8842, loss 0.00247992, acc 1
2016-09-06T01:44:23.662191: step 8843, loss 0.022034, acc 1
2016-09-06T01:44:24.480798: step 8844, loss 0.0409912, acc 0.98
2016-09-06T01:44:25.265748: step 8845, loss 0.00281267, acc 1
2016-09-06T01:44:26.090244: step 8846, loss 0.00307895, acc 1
2016-09-06T01:44:26.910155: step 8847, loss 0.00396672, acc 1
2016-09-06T01:44:27.708269: step 8848, loss 0.023606, acc 0.98
2016-09-06T01:44:28.536194: step 8849, loss 0.00874632, acc 1
2016-09-06T01:44:29.315714: step 8850, loss 0.00494894, acc 1
2016-09-06T01:44:30.160030: step 8851, loss 0.00277649, acc 1
2016-09-06T01:44:31.001044: step 8852, loss 0.0636951, acc 0.98
2016-09-06T01:44:31.824619: step 8853, loss 0.0111794, acc 1
2016-09-06T01:44:32.653583: step 8854, loss 0.0172774, acc 1
2016-09-06T01:44:33.492814: step 8855, loss 0.0132784, acc 1
2016-09-06T01:44:34.283698: step 8856, loss 0.0304492, acc 1
2016-09-06T01:44:35.122280: step 8857, loss 0.0284247, acc 0.98
2016-09-06T01:44:35.950371: step 8858, loss 0.0284832, acc 0.98
2016-09-06T01:44:36.785478: step 8859, loss 0.0126073, acc 1
2016-09-06T01:44:37.605128: step 8860, loss 0.00311923, acc 1
2016-09-06T01:44:38.430556: step 8861, loss 0.0139006, acc 1
2016-09-06T01:44:39.258349: step 8862, loss 0.00252352, acc 1
2016-09-06T01:44:40.077539: step 8863, loss 0.0713022, acc 0.98
2016-09-06T01:44:40.926019: step 8864, loss 0.00245851, acc 1
2016-09-06T01:44:41.749253: step 8865, loss 0.0344213, acc 0.98
2016-09-06T01:44:42.573521: step 8866, loss 0.0239913, acc 0.98
2016-09-06T01:44:43.372688: step 8867, loss 0.00642964, acc 1
2016-09-06T01:44:44.186531: step 8868, loss 0.0257762, acc 1
2016-09-06T01:44:44.972331: step 8869, loss 0.00701133, acc 1
2016-09-06T01:44:45.778749: step 8870, loss 0.038529, acc 0.98
2016-09-06T01:44:46.600801: step 8871, loss 0.0373168, acc 0.98
2016-09-06T01:44:47.376165: step 8872, loss 0.0360651, acc 0.98
2016-09-06T01:44:48.182484: step 8873, loss 0.0165668, acc 1
2016-09-06T01:44:49.048921: step 8874, loss 0.00287746, acc 1
2016-09-06T01:44:49.845056: step 8875, loss 0.0296768, acc 1
2016-09-06T01:44:50.656014: step 8876, loss 0.013085, acc 1
2016-09-06T01:44:51.487035: step 8877, loss 0.0154975, acc 1
2016-09-06T01:44:52.269653: step 8878, loss 0.00412597, acc 1
2016-09-06T01:44:53.098007: step 8879, loss 0.00939332, acc 1
2016-09-06T01:44:53.930883: step 8880, loss 0.0353308, acc 0.98
2016-09-06T01:44:54.739610: step 8881, loss 0.0110211, acc 1
2016-09-06T01:44:55.538760: step 8882, loss 0.00240481, acc 1
2016-09-06T01:44:56.344564: step 8883, loss 0.00571575, acc 1
2016-09-06T01:44:57.130738: step 8884, loss 0.0253838, acc 0.98
2016-09-06T01:44:57.891198: step 8885, loss 0.0248975, acc 0.98
2016-09-06T01:44:58.679824: step 8886, loss 0.0364102, acc 1
2016-09-06T01:44:59.475408: step 8887, loss 0.0129991, acc 1
2016-09-06T01:45:00.307587: step 8888, loss 0.00446491, acc 1
2016-09-06T01:45:01.115468: step 8889, loss 0.00383539, acc 1
2016-09-06T01:45:01.915521: step 8890, loss 0.00342606, acc 1
2016-09-06T01:45:02.755595: step 8891, loss 0.0135927, acc 1
2016-09-06T01:45:03.569428: step 8892, loss 0.00321848, acc 1
2016-09-06T01:45:04.350872: step 8893, loss 0.0251808, acc 1
2016-09-06T01:45:05.164843: step 8894, loss 0.00293472, acc 1
2016-09-06T01:45:05.997633: step 8895, loss 0.0102602, acc 1
2016-09-06T01:45:06.791975: step 8896, loss 0.116671, acc 0.96
2016-09-06T01:45:07.617738: step 8897, loss 0.112885, acc 0.98
2016-09-06T01:45:08.428860: step 8898, loss 0.0230009, acc 1
2016-09-06T01:45:09.243023: step 8899, loss 0.00288171, acc 1
2016-09-06T01:45:10.054815: step 8900, loss 0.100068, acc 0.94

Evaluation:
2016-09-06T01:45:13.820736: step 8900, loss 2.46244, acc 0.713884

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-8900

2016-09-06T01:45:15.690532: step 8901, loss 0.0289936, acc 0.98
2016-09-06T01:45:16.502667: step 8902, loss 0.0229392, acc 0.98
2016-09-06T01:45:17.305694: step 8903, loss 0.00369149, acc 1
2016-09-06T01:45:18.116611: step 8904, loss 0.00293649, acc 1
2016-09-06T01:45:18.931485: step 8905, loss 0.00278014, acc 1
2016-09-06T01:45:19.775381: step 8906, loss 0.0698021, acc 0.98
2016-09-06T01:45:20.598778: step 8907, loss 0.0158087, acc 1
2016-09-06T01:45:21.390380: step 8908, loss 0.0214874, acc 0.98
2016-09-06T01:45:22.196135: step 8909, loss 0.00235601, acc 1
2016-09-06T01:45:23.030678: step 8910, loss 0.015152, acc 1
2016-09-06T01:45:23.833919: step 8911, loss 0.0138679, acc 1
2016-09-06T01:45:24.637770: step 8912, loss 0.0163086, acc 0.98
2016-09-06T01:45:25.468272: step 8913, loss 0.00442128, acc 1
2016-09-06T01:45:26.253420: step 8914, loss 0.0162821, acc 1
2016-09-06T01:45:27.036201: step 8915, loss 0.0448427, acc 0.96
2016-09-06T01:45:27.852242: step 8916, loss 0.00886544, acc 1
2016-09-06T01:45:28.644706: step 8917, loss 0.0337159, acc 0.98
2016-09-06T01:45:29.460662: step 8918, loss 0.0382101, acc 0.98
2016-09-06T01:45:30.310446: step 8919, loss 0.0137546, acc 1
2016-09-06T01:45:31.128560: step 8920, loss 0.012685, acc 1
2016-09-06T01:45:31.930746: step 8921, loss 0.0556796, acc 0.98
2016-09-06T01:45:32.758409: step 8922, loss 0.0178661, acc 0.98
2016-09-06T01:45:33.577331: step 8923, loss 0.0255522, acc 0.98
2016-09-06T01:45:34.386972: step 8924, loss 0.0135753, acc 1
2016-09-06T01:45:35.211367: step 8925, loss 0.0167445, acc 1
2016-09-06T01:45:36.046744: step 8926, loss 0.00297288, acc 1
2016-09-06T01:45:36.873544: step 8927, loss 0.0163411, acc 1
2016-09-06T01:45:37.695911: step 8928, loss 0.00252453, acc 1
2016-09-06T01:45:38.538501: step 8929, loss 0.0139612, acc 1
2016-09-06T01:45:39.364278: step 8930, loss 0.138594, acc 0.96
2016-09-06T01:45:40.202353: step 8931, loss 0.0382623, acc 0.98
2016-09-06T01:45:41.018391: step 8932, loss 0.014084, acc 1
2016-09-06T01:45:41.838865: step 8933, loss 0.0209749, acc 0.98
2016-09-06T01:45:42.685434: step 8934, loss 0.0128118, acc 1
2016-09-06T01:45:43.515725: step 8935, loss 0.00755016, acc 1
2016-09-06T01:45:44.355494: step 8936, loss 0.010542, acc 1
2016-09-06T01:45:45.173344: step 8937, loss 0.0179323, acc 0.98
2016-09-06T01:45:46.019249: step 8938, loss 0.00925026, acc 1
2016-09-06T01:45:46.762805: step 8939, loss 0.0543413, acc 0.94
2016-09-06T01:45:47.563286: step 8940, loss 0.0547206, acc 0.96
2016-09-06T01:45:48.390777: step 8941, loss 0.0217507, acc 1
2016-09-06T01:45:49.192911: step 8942, loss 0.108486, acc 0.98
2016-09-06T01:45:49.989415: step 8943, loss 0.00568521, acc 1
2016-09-06T01:45:50.801775: step 8944, loss 0.0387675, acc 0.98
2016-09-06T01:45:51.596037: step 8945, loss 0.0281687, acc 0.98
2016-09-06T01:45:52.385394: step 8946, loss 0.00437869, acc 1
2016-09-06T01:45:53.178911: step 8947, loss 0.0572792, acc 0.96
2016-09-06T01:45:53.973407: step 8948, loss 0.0181033, acc 0.98
2016-09-06T01:45:54.803160: step 8949, loss 0.0274963, acc 0.98
2016-09-06T01:45:55.604191: step 8950, loss 0.00351417, acc 1
2016-09-06T01:45:56.423844: step 8951, loss 0.0567623, acc 0.98
2016-09-06T01:45:57.257249: step 8952, loss 0.0277537, acc 1
2016-09-06T01:45:58.080492: step 8953, loss 0.0030346, acc 1
2016-09-06T01:45:58.861748: step 8954, loss 0.00336455, acc 1
2016-09-06T01:45:59.662815: step 8955, loss 0.00785499, acc 1
2016-09-06T01:46:00.518840: step 8956, loss 0.0191436, acc 1
2016-09-06T01:46:01.292977: step 8957, loss 0.0102221, acc 1
2016-09-06T01:46:02.085428: step 8958, loss 0.0088801, acc 1
2016-09-06T01:46:02.897106: step 8959, loss 0.0215949, acc 0.98
2016-09-06T01:46:03.690116: step 8960, loss 0.038316, acc 0.98
2016-09-06T01:46:04.493459: step 8961, loss 0.0225499, acc 1
2016-09-06T01:46:05.348345: step 8962, loss 0.145488, acc 0.98
2016-09-06T01:46:06.124795: step 8963, loss 0.0268218, acc 0.98
2016-09-06T01:46:06.931325: step 8964, loss 0.0237314, acc 0.98
2016-09-06T01:46:07.786268: step 8965, loss 0.040541, acc 0.98
2016-09-06T01:46:08.595983: step 8966, loss 0.00890838, acc 1
2016-09-06T01:46:09.417730: step 8967, loss 0.0332474, acc 1
2016-09-06T01:46:10.230003: step 8968, loss 0.00327364, acc 1
2016-09-06T01:46:11.026050: step 8969, loss 0.0233896, acc 0.98
2016-09-06T01:46:11.827335: step 8970, loss 0.1386, acc 0.98
2016-09-06T01:46:12.663131: step 8971, loss 0.0043017, acc 1
2016-09-06T01:46:13.506863: step 8972, loss 0.0313804, acc 0.98
2016-09-06T01:46:14.305900: step 8973, loss 0.0294539, acc 1
2016-09-06T01:46:15.129317: step 8974, loss 0.00354371, acc 1
2016-09-06T01:46:15.945117: step 8975, loss 0.00266724, acc 1
2016-09-06T01:46:16.757859: step 8976, loss 0.0254593, acc 1
2016-09-06T01:46:17.573488: step 8977, loss 0.0217167, acc 0.98
2016-09-06T01:46:18.413314: step 8978, loss 0.0221909, acc 0.98
2016-09-06T01:46:19.248907: step 8979, loss 0.0256395, acc 1
2016-09-06T01:46:20.068688: step 8980, loss 0.0117379, acc 1
2016-09-06T01:46:20.870071: step 8981, loss 0.0386074, acc 0.96
2016-09-06T01:46:21.675466: step 8982, loss 0.041206, acc 0.98
2016-09-06T01:46:22.484481: step 8983, loss 0.00394176, acc 1
2016-09-06T01:46:23.303292: step 8984, loss 0.00258864, acc 1
2016-09-06T01:46:24.113322: step 8985, loss 0.079222, acc 0.94
2016-09-06T01:46:25.010123: step 8986, loss 0.0726474, acc 0.98
2016-09-06T01:46:25.835750: step 8987, loss 0.0129291, acc 1
2016-09-06T01:46:26.667866: step 8988, loss 0.00604058, acc 1
2016-09-06T01:46:27.457329: step 8989, loss 0.0117247, acc 1
2016-09-06T01:46:28.297035: step 8990, loss 0.00478303, acc 1
2016-09-06T01:46:29.088934: step 8991, loss 0.00512559, acc 1
2016-09-06T01:46:29.896014: step 8992, loss 0.0298842, acc 1
2016-09-06T01:46:30.699563: step 8993, loss 0.0386299, acc 1
2016-09-06T01:46:31.476765: step 8994, loss 0.0112806, acc 1
2016-09-06T01:46:32.284075: step 8995, loss 0.0087216, acc 1
2016-09-06T01:46:33.098420: step 8996, loss 0.00901574, acc 1
2016-09-06T01:46:33.885817: step 8997, loss 0.0981198, acc 0.98
2016-09-06T01:46:34.738885: step 8998, loss 0.0305906, acc 1
2016-09-06T01:46:35.566945: step 8999, loss 0.00470652, acc 1
2016-09-06T01:46:36.342635: step 9000, loss 0.0122705, acc 1

Evaluation:
2016-09-06T01:46:40.026785: step 9000, loss 2.13446, acc 0.712946

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-9000

2016-09-06T01:46:41.937300: step 9001, loss 0.0862705, acc 0.96
2016-09-06T01:46:42.798440: step 9002, loss 0.0164773, acc 1
2016-09-06T01:46:43.590775: step 9003, loss 0.0216059, acc 1
2016-09-06T01:46:44.386915: step 9004, loss 0.0195673, acc 0.98
2016-09-06T01:46:45.229404: step 9005, loss 0.00699602, acc 1
2016-09-06T01:46:46.045393: step 9006, loss 0.0192716, acc 1
2016-09-06T01:46:46.828165: step 9007, loss 0.0342315, acc 0.96
2016-09-06T01:46:47.659568: step 9008, loss 0.00836788, acc 1
2016-09-06T01:46:48.448700: step 9009, loss 0.0139914, acc 1
2016-09-06T01:46:49.261437: step 9010, loss 0.0231064, acc 1
2016-09-06T01:46:50.079285: step 9011, loss 0.0236827, acc 1
2016-09-06T01:46:50.865390: step 9012, loss 0.054159, acc 0.98
2016-09-06T01:46:51.638120: step 9013, loss 0.00411955, acc 1
2016-09-06T01:46:52.434811: step 9014, loss 0.00333979, acc 1
2016-09-06T01:46:53.223395: step 9015, loss 0.0140757, acc 1
2016-09-06T01:46:54.037434: step 9016, loss 0.00303345, acc 1
2016-09-06T01:46:54.832701: step 9017, loss 0.0116592, acc 1
2016-09-06T01:46:55.670066: step 9018, loss 0.0120934, acc 1
2016-09-06T01:46:56.480600: step 9019, loss 0.0211614, acc 0.98
2016-09-06T01:46:57.314018: step 9020, loss 0.0194842, acc 0.98
2016-09-06T01:46:58.102223: step 9021, loss 0.0229056, acc 0.98
2016-09-06T01:46:58.916290: step 9022, loss 0.0413987, acc 0.98
2016-09-06T01:46:59.763501: step 9023, loss 0.0253821, acc 0.98
2016-09-06T01:47:00.514916: step 9024, loss 0.0191505, acc 0.977273
2016-09-06T01:47:01.326754: step 9025, loss 0.0664833, acc 0.98
2016-09-06T01:47:02.139273: step 9026, loss 0.009015, acc 1
2016-09-06T01:47:02.933300: step 9027, loss 0.0070923, acc 1
2016-09-06T01:47:03.741862: step 9028, loss 0.0141451, acc 1
2016-09-06T01:47:04.540912: step 9029, loss 0.0177233, acc 0.98
2016-09-06T01:47:05.331738: step 9030, loss 0.0202958, acc 0.98
2016-09-06T01:47:06.126454: step 9031, loss 0.00654299, acc 1
2016-09-06T01:47:06.948738: step 9032, loss 0.00523921, acc 1
2016-09-06T01:47:07.750444: step 9033, loss 0.00849724, acc 1
2016-09-06T01:47:08.539813: step 9034, loss 0.00289337, acc 1
2016-09-06T01:47:09.358042: step 9035, loss 0.0296125, acc 0.98
2016-09-06T01:47:10.157912: step 9036, loss 0.0386285, acc 0.98
2016-09-06T01:47:10.953455: step 9037, loss 0.00621982, acc 1
2016-09-06T01:47:11.756956: step 9038, loss 0.0027437, acc 1
2016-09-06T01:47:12.583281: step 9039, loss 0.00271487, acc 1
2016-09-06T01:47:13.403207: step 9040, loss 0.017221, acc 1
2016-09-06T01:47:14.203177: step 9041, loss 0.0354938, acc 0.98
2016-09-06T01:47:14.992903: step 9042, loss 0.00277828, acc 1
2016-09-06T01:47:15.776992: step 9043, loss 0.00321833, acc 1
2016-09-06T01:47:16.593965: step 9044, loss 0.00938504, acc 1
2016-09-06T01:47:17.383597: step 9045, loss 0.00377002, acc 1
2016-09-06T01:47:18.228071: step 9046, loss 0.00305176, acc 1
2016-09-06T01:47:19.057305: step 9047, loss 0.0061058, acc 1
2016-09-06T01:47:19.893937: step 9048, loss 0.0135327, acc 1
2016-09-06T01:47:20.704136: step 9049, loss 0.0187736, acc 0.98
2016-09-06T01:47:21.511313: step 9050, loss 0.0576594, acc 0.94
2016-09-06T01:47:22.303929: step 9051, loss 0.00325864, acc 1
2016-09-06T01:47:23.110408: step 9052, loss 0.00311493, acc 1
2016-09-06T01:47:23.928585: step 9053, loss 0.0269542, acc 0.98
2016-09-06T01:47:24.753186: step 9054, loss 0.00305224, acc 1
2016-09-06T01:47:25.566028: step 9055, loss 0.0110288, acc 1
2016-09-06T01:47:26.371484: step 9056, loss 0.011612, acc 1
2016-09-06T01:47:27.196113: step 9057, loss 0.00299385, acc 1
2016-09-06T01:47:28.009380: step 9058, loss 0.003312, acc 1
2016-09-06T01:47:28.829486: step 9059, loss 0.00383087, acc 1
2016-09-06T01:47:29.637840: step 9060, loss 0.0173128, acc 0.98
2016-09-06T01:47:30.437414: step 9061, loss 0.0324918, acc 0.98
2016-09-06T01:47:31.260176: step 9062, loss 0.00805603, acc 1
2016-09-06T01:47:32.069902: step 9063, loss 0.00665291, acc 1
2016-09-06T01:47:32.873398: step 9064, loss 0.00261845, acc 1
2016-09-06T01:47:33.695975: step 9065, loss 0.0173324, acc 1
2016-09-06T01:47:34.510387: step 9066, loss 0.00258672, acc 1
2016-09-06T01:47:35.333496: step 9067, loss 0.0291814, acc 0.98
2016-09-06T01:47:36.202807: step 9068, loss 0.0168073, acc 0.98
2016-09-06T01:47:37.003613: step 9069, loss 0.00758589, acc 1
2016-09-06T01:47:37.811863: step 9070, loss 0.00255029, acc 1
2016-09-06T01:47:38.643927: step 9071, loss 0.00281386, acc 1
2016-09-06T01:47:39.454811: step 9072, loss 0.0150014, acc 1
2016-09-06T01:47:40.236615: step 9073, loss 0.0162484, acc 1
2016-09-06T01:47:41.057697: step 9074, loss 0.00406342, acc 1
2016-09-06T01:47:41.857568: step 9075, loss 0.0063052, acc 1
2016-09-06T01:47:42.668691: step 9076, loss 0.0493166, acc 0.98
2016-09-06T01:47:43.488056: step 9077, loss 0.0133343, acc 1
2016-09-06T01:47:44.330961: step 9078, loss 0.0119865, acc 1
2016-09-06T01:47:45.160004: step 9079, loss 0.0155765, acc 1
2016-09-06T01:47:45.977277: step 9080, loss 0.00253918, acc 1
2016-09-06T01:47:46.816857: step 9081, loss 0.00364276, acc 1
2016-09-06T01:47:47.595992: step 9082, loss 0.0260493, acc 0.98
2016-09-06T01:47:48.402062: step 9083, loss 0.00874358, acc 1
2016-09-06T01:47:49.211206: step 9084, loss 0.00256348, acc 1
2016-09-06T01:47:50.028619: step 9085, loss 0.0175122, acc 1
2016-09-06T01:47:50.853987: step 9086, loss 0.0164506, acc 1
2016-09-06T01:47:51.681394: step 9087, loss 0.00233546, acc 1
2016-09-06T01:47:52.459542: step 9088, loss 0.0455864, acc 0.96
2016-09-06T01:47:53.273682: step 9089, loss 0.00620146, acc 1
2016-09-06T01:47:54.089990: step 9090, loss 0.00339171, acc 1
2016-09-06T01:47:54.868104: step 9091, loss 0.00287948, acc 1
2016-09-06T01:47:55.660789: step 9092, loss 0.051461, acc 0.98
2016-09-06T01:47:56.513375: step 9093, loss 0.0178538, acc 0.98
2016-09-06T01:47:57.292678: step 9094, loss 0.00264042, acc 1
2016-09-06T01:47:58.092397: step 9095, loss 0.0212069, acc 1
2016-09-06T01:47:58.936092: step 9096, loss 0.00310666, acc 1
2016-09-06T01:47:59.722690: step 9097, loss 0.0303385, acc 1
2016-09-06T01:48:00.560758: step 9098, loss 0.00368544, acc 1
2016-09-06T01:48:01.363476: step 9099, loss 0.116003, acc 0.98
2016-09-06T01:48:02.130100: step 9100, loss 0.0127745, acc 1

Evaluation:
2016-09-06T01:48:05.853297: step 9100, loss 2.62668, acc 0.725141

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-9100

2016-09-06T01:48:07.734803: step 9101, loss 0.00224654, acc 1
2016-09-06T01:48:08.540858: step 9102, loss 0.00328198, acc 1
2016-09-06T01:48:09.351736: step 9103, loss 0.0407806, acc 0.98
2016-09-06T01:48:10.177464: step 9104, loss 0.00225716, acc 1
2016-09-06T01:48:11.003911: step 9105, loss 0.0106884, acc 1
2016-09-06T01:48:11.800040: step 9106, loss 0.00323834, acc 1
2016-09-06T01:48:12.633968: step 9107, loss 0.021046, acc 1
2016-09-06T01:48:13.439855: step 9108, loss 0.00502414, acc 1
2016-09-06T01:48:14.243793: step 9109, loss 0.0125531, acc 1
2016-09-06T01:48:15.053375: step 9110, loss 0.016054, acc 0.98
2016-09-06T01:48:15.865404: step 9111, loss 0.010947, acc 1
2016-09-06T01:48:16.680310: step 9112, loss 0.00290074, acc 1
2016-09-06T01:48:17.489144: step 9113, loss 0.0520741, acc 0.98
2016-09-06T01:48:18.297785: step 9114, loss 0.00190883, acc 1
2016-09-06T01:48:19.065840: step 9115, loss 0.00504296, acc 1
2016-09-06T01:48:19.897314: step 9116, loss 0.00219776, acc 1
2016-09-06T01:48:20.718891: step 9117, loss 0.0139018, acc 1
2016-09-06T01:48:21.534136: step 9118, loss 0.00617807, acc 1
2016-09-06T01:48:22.344814: step 9119, loss 0.0259246, acc 0.98
2016-09-06T01:48:23.149078: step 9120, loss 0.00443941, acc 1
2016-09-06T01:48:23.937611: step 9121, loss 0.00836502, acc 1
2016-09-06T01:48:24.722892: step 9122, loss 0.0194757, acc 1
2016-09-06T01:48:25.531561: step 9123, loss 0.00631965, acc 1
2016-09-06T01:48:26.305612: step 9124, loss 0.003962, acc 1
2016-09-06T01:48:27.103616: step 9125, loss 0.034211, acc 0.96
2016-09-06T01:48:27.937680: step 9126, loss 0.00249793, acc 1
2016-09-06T01:48:28.722234: step 9127, loss 0.00218706, acc 1
2016-09-06T01:48:29.520461: step 9128, loss 0.00198276, acc 1
2016-09-06T01:48:30.336453: step 9129, loss 0.0726356, acc 0.96
2016-09-06T01:48:31.124761: step 9130, loss 0.00213516, acc 1
2016-09-06T01:48:31.934224: step 9131, loss 0.0743066, acc 0.94
2016-09-06T01:48:32.759815: step 9132, loss 0.00850498, acc 1
2016-09-06T01:48:33.577623: step 9133, loss 0.125795, acc 0.96
2016-09-06T01:48:34.402646: step 9134, loss 0.030103, acc 0.98
2016-09-06T01:48:35.237065: step 9135, loss 0.0149618, acc 1
2016-09-06T01:48:36.045706: step 9136, loss 0.00910636, acc 1
2016-09-06T01:48:36.858571: step 9137, loss 0.0623092, acc 0.98
2016-09-06T01:48:37.684286: step 9138, loss 0.0329107, acc 0.98
2016-09-06T01:48:38.492604: step 9139, loss 0.0385299, acc 0.96
2016-09-06T01:48:39.323713: step 9140, loss 0.00789805, acc 1
2016-09-06T01:48:40.101234: step 9141, loss 0.00210447, acc 1
2016-09-06T01:48:40.901868: step 9142, loss 0.0020418, acc 1
2016-09-06T01:48:41.722684: step 9143, loss 0.0064517, acc 1
2016-09-06T01:48:42.549909: step 9144, loss 0.0120491, acc 1
2016-09-06T01:48:43.365730: step 9145, loss 0.00401025, acc 1
2016-09-06T01:48:44.156980: step 9146, loss 0.00225325, acc 1
2016-09-06T01:48:45.010621: step 9147, loss 0.002298, acc 1
2016-09-06T01:48:45.808920: step 9148, loss 0.0225022, acc 0.98
2016-09-06T01:48:46.607602: step 9149, loss 0.0178769, acc 1
2016-09-06T01:48:47.420020: step 9150, loss 0.0136581, acc 1
2016-09-06T01:48:48.217516: step 9151, loss 0.0147781, acc 1
2016-09-06T01:48:48.998705: step 9152, loss 0.0343498, acc 0.98
2016-09-06T01:48:49.849442: step 9153, loss 0.0031418, acc 1
2016-09-06T01:48:50.679931: step 9154, loss 0.00497117, acc 1
2016-09-06T01:48:51.476226: step 9155, loss 0.00626188, acc 1
2016-09-06T01:48:52.323721: step 9156, loss 0.0370776, acc 0.98
2016-09-06T01:48:53.132530: step 9157, loss 0.012135, acc 1
2016-09-06T01:48:53.928902: step 9158, loss 0.0121581, acc 1
2016-09-06T01:48:54.800636: step 9159, loss 0.0608604, acc 0.98
2016-09-06T01:48:55.605813: step 9160, loss 0.00381113, acc 1
2016-09-06T01:48:56.410284: step 9161, loss 0.00606506, acc 1
2016-09-06T01:48:57.227525: step 9162, loss 0.0310672, acc 1
2016-09-06T01:48:58.028489: step 9163, loss 0.0040136, acc 1
2016-09-06T01:48:58.825329: step 9164, loss 0.00197278, acc 1
2016-09-06T01:48:59.643176: step 9165, loss 0.0330295, acc 0.98
2016-09-06T01:49:00.506439: step 9166, loss 0.00210381, acc 1
2016-09-06T01:49:01.309721: step 9167, loss 0.013727, acc 1
2016-09-06T01:49:02.120110: step 9168, loss 0.0462356, acc 0.96
2016-09-06T01:49:02.926970: step 9169, loss 0.032685, acc 1
2016-09-06T01:49:03.729963: step 9170, loss 0.00967475, acc 1
2016-09-06T01:49:04.549872: step 9171, loss 0.0632252, acc 0.96
2016-09-06T01:49:05.346344: step 9172, loss 0.00712756, acc 1
2016-09-06T01:49:06.170105: step 9173, loss 0.00302345, acc 1
2016-09-06T01:49:07.040244: step 9174, loss 0.0247829, acc 0.98
2016-09-06T01:49:07.860258: step 9175, loss 0.00273401, acc 1
2016-09-06T01:49:08.656901: step 9176, loss 0.0130027, acc 1
2016-09-06T01:49:09.501090: step 9177, loss 0.0311227, acc 0.98
2016-09-06T01:49:10.314878: step 9178, loss 0.00700941, acc 1
2016-09-06T01:49:11.132464: step 9179, loss 0.0461098, acc 0.98
2016-09-06T01:49:11.942833: step 9180, loss 0.0559075, acc 0.96
2016-09-06T01:49:12.741423: step 9181, loss 0.0336994, acc 0.98
2016-09-06T01:49:13.561058: step 9182, loss 0.00898759, acc 1
2016-09-06T01:49:14.397349: step 9183, loss 0.017796, acc 1
2016-09-06T01:49:15.216457: step 9184, loss 0.0189125, acc 1
2016-09-06T01:49:15.996009: step 9185, loss 0.0493758, acc 0.96
2016-09-06T01:49:16.795366: step 9186, loss 0.0416825, acc 0.98
2016-09-06T01:49:17.611448: step 9187, loss 0.00241131, acc 1
2016-09-06T01:49:18.424039: step 9188, loss 0.018467, acc 1
2016-09-06T01:49:19.242504: step 9189, loss 0.017822, acc 0.98
2016-09-06T01:49:20.060052: step 9190, loss 0.0277856, acc 0.98
2016-09-06T01:49:20.875262: step 9191, loss 0.00235268, acc 1
2016-09-06T01:49:21.692414: step 9192, loss 0.215252, acc 0.96
2016-09-06T01:49:22.521969: step 9193, loss 0.0729583, acc 0.96
2016-09-06T01:49:23.345121: step 9194, loss 0.00212809, acc 1
2016-09-06T01:49:24.177884: step 9195, loss 0.00881173, acc 1
2016-09-06T01:49:25.021837: step 9196, loss 0.00468147, acc 1
2016-09-06T01:49:25.845374: step 9197, loss 0.00686233, acc 1
2016-09-06T01:49:26.648371: step 9198, loss 0.0152885, acc 1
2016-09-06T01:49:27.479781: step 9199, loss 0.0262631, acc 0.98
2016-09-06T01:49:28.294557: step 9200, loss 0.00306612, acc 1

Evaluation:
2016-09-06T01:49:32.009716: step 9200, loss 1.98338, acc 0.71576

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-9200

2016-09-06T01:49:33.939948: step 9201, loss 0.0611112, acc 0.96
2016-09-06T01:49:34.794841: step 9202, loss 0.0472974, acc 0.98
2016-09-06T01:49:35.606853: step 9203, loss 0.0210404, acc 0.98
2016-09-06T01:49:36.427972: step 9204, loss 0.0227119, acc 1
2016-09-06T01:49:37.252311: step 9205, loss 0.00913321, acc 1
2016-09-06T01:49:38.080787: step 9206, loss 0.0039955, acc 1
2016-09-06T01:49:38.881065: step 9207, loss 0.0321201, acc 0.98
2016-09-06T01:49:39.681665: step 9208, loss 0.00561829, acc 1
2016-09-06T01:49:40.490429: step 9209, loss 0.0213998, acc 1
2016-09-06T01:49:41.297439: step 9210, loss 0.0105181, acc 1
2016-09-06T01:49:42.097334: step 9211, loss 0.0418917, acc 0.98
2016-09-06T01:49:42.944667: step 9212, loss 0.00680101, acc 1
2016-09-06T01:49:43.710782: step 9213, loss 0.0215155, acc 1
2016-09-06T01:49:44.519896: step 9214, loss 0.0410488, acc 0.98
2016-09-06T01:49:45.320276: step 9215, loss 0.018629, acc 0.98
2016-09-06T01:49:46.073020: step 9216, loss 0.00661855, acc 1
2016-09-06T01:49:46.899230: step 9217, loss 0.00372864, acc 1
2016-09-06T01:49:47.708188: step 9218, loss 0.0244517, acc 1
2016-09-06T01:49:48.486902: step 9219, loss 0.0197438, acc 0.98
2016-09-06T01:49:49.295264: step 9220, loss 0.00806285, acc 1
2016-09-06T01:49:50.131126: step 9221, loss 0.0057263, acc 1
2016-09-06T01:49:50.919108: step 9222, loss 0.00354753, acc 1
2016-09-06T01:49:51.720001: step 9223, loss 0.00969597, acc 1
2016-09-06T01:49:52.498678: step 9224, loss 0.00459391, acc 1
2016-09-06T01:49:53.308778: step 9225, loss 0.00314097, acc 1
2016-09-06T01:49:54.142811: step 9226, loss 0.00304283, acc 1
2016-09-06T01:49:54.974446: step 9227, loss 0.0134378, acc 1
2016-09-06T01:49:55.756533: step 9228, loss 0.0371105, acc 0.98
2016-09-06T01:49:56.565623: step 9229, loss 0.00324243, acc 1
2016-09-06T01:49:57.382036: step 9230, loss 0.00310895, acc 1
2016-09-06T01:49:58.151526: step 9231, loss 0.0254406, acc 0.98
2016-09-06T01:49:59.002928: step 9232, loss 0.00538055, acc 1
2016-09-06T01:49:59.825400: step 9233, loss 0.00309448, acc 1
2016-09-06T01:50:00.625452: step 9234, loss 0.00404822, acc 1
2016-09-06T01:50:01.400958: step 9235, loss 0.0257097, acc 0.98
2016-09-06T01:50:02.251467: step 9236, loss 0.0321871, acc 0.98
2016-09-06T01:50:03.067151: step 9237, loss 0.00303096, acc 1
2016-09-06T01:50:03.880627: step 9238, loss 0.0158471, acc 1
2016-09-06T01:50:04.699302: step 9239, loss 0.0114586, acc 1
2016-09-06T01:50:05.518794: step 9240, loss 0.0188093, acc 0.98
2016-09-06T01:50:06.325003: step 9241, loss 0.00327954, acc 1
2016-09-06T01:50:07.138843: step 9242, loss 0.040323, acc 0.98
2016-09-06T01:50:07.940719: step 9243, loss 0.0033751, acc 1
2016-09-06T01:50:08.728694: step 9244, loss 0.00348926, acc 1
2016-09-06T01:50:09.521896: step 9245, loss 0.0112097, acc 1
2016-09-06T01:50:10.316367: step 9246, loss 0.00407937, acc 1
2016-09-06T01:50:11.107218: step 9247, loss 0.019487, acc 0.98
2016-09-06T01:50:11.922276: step 9248, loss 0.0308251, acc 0.98
2016-09-06T01:50:12.707170: step 9249, loss 0.0175791, acc 1
2016-09-06T01:50:13.501985: step 9250, loss 0.0480139, acc 0.98
2016-09-06T01:50:14.314022: step 9251, loss 0.0355875, acc 0.98
2016-09-06T01:50:15.144032: step 9252, loss 0.0443933, acc 0.98
2016-09-06T01:50:15.960144: step 9253, loss 0.00277038, acc 1
2016-09-06T01:50:16.779278: step 9254, loss 0.0235972, acc 0.98
2016-09-06T01:50:17.567326: step 9255, loss 0.0277084, acc 0.98
2016-09-06T01:50:18.381488: step 9256, loss 0.00273795, acc 1
2016-09-06T01:50:19.207983: step 9257, loss 0.0179927, acc 0.98
2016-09-06T01:50:20.003199: step 9258, loss 0.0202756, acc 0.98
2016-09-06T01:50:20.816560: step 9259, loss 0.087196, acc 0.98
2016-09-06T01:50:21.647675: step 9260, loss 0.0134588, acc 1
2016-09-06T01:50:22.491526: step 9261, loss 0.00272382, acc 1
2016-09-06T01:50:23.283403: step 9262, loss 0.0606099, acc 0.98
2016-09-06T01:50:24.105141: step 9263, loss 0.0234089, acc 0.98
2016-09-06T01:50:24.911809: step 9264, loss 0.0111794, acc 1
2016-09-06T01:50:25.719177: step 9265, loss 0.00425062, acc 1
2016-09-06T01:50:26.522703: step 9266, loss 0.0187947, acc 0.98
2016-09-06T01:50:27.329835: step 9267, loss 0.00518778, acc 1
2016-09-06T01:50:28.135007: step 9268, loss 0.0791027, acc 0.96
2016-09-06T01:50:28.977694: step 9269, loss 0.0258682, acc 1
2016-09-06T01:50:29.772006: step 9270, loss 0.00728416, acc 1
2016-09-06T01:50:30.572322: step 9271, loss 0.0255167, acc 0.98
2016-09-06T01:50:31.391065: step 9272, loss 0.00576427, acc 1
2016-09-06T01:50:32.182932: step 9273, loss 0.00268415, acc 1
2016-09-06T01:50:32.985097: step 9274, loss 0.0525926, acc 0.98
2016-09-06T01:50:33.832813: step 9275, loss 0.00338305, acc 1
2016-09-06T01:50:34.655568: step 9276, loss 0.00660295, acc 1
2016-09-06T01:50:35.490276: step 9277, loss 0.0287653, acc 1
2016-09-06T01:50:36.367595: step 9278, loss 0.00432307, acc 1
2016-09-06T01:50:37.188743: step 9279, loss 0.00279035, acc 1
2016-09-06T01:50:37.988979: step 9280, loss 0.00469431, acc 1
2016-09-06T01:50:38.811741: step 9281, loss 0.0235807, acc 0.98
2016-09-06T01:50:39.615020: step 9282, loss 0.0103709, acc 1
2016-09-06T01:50:40.419961: step 9283, loss 0.0319637, acc 0.98
2016-09-06T01:50:41.231891: step 9284, loss 0.0430271, acc 0.96
2016-09-06T01:50:42.049321: step 9285, loss 0.00498299, acc 1
2016-09-06T01:50:42.836323: step 9286, loss 0.00817576, acc 1
2016-09-06T01:50:43.659864: step 9287, loss 0.0172967, acc 0.98
2016-09-06T01:50:44.484933: step 9288, loss 0.00797625, acc 1
2016-09-06T01:50:45.292679: step 9289, loss 0.00349718, acc 1
2016-09-06T01:50:46.095226: step 9290, loss 0.00554373, acc 1
2016-09-06T01:50:46.901851: step 9291, loss 0.0212373, acc 0.98
2016-09-06T01:50:47.712903: step 9292, loss 0.0268394, acc 0.98
2016-09-06T01:50:48.507191: step 9293, loss 0.00391617, acc 1
2016-09-06T01:50:49.324047: step 9294, loss 0.0174914, acc 1
2016-09-06T01:50:50.137006: step 9295, loss 0.00624836, acc 1
2016-09-06T01:50:50.941023: step 9296, loss 0.00777805, acc 1
2016-09-06T01:50:51.744379: step 9297, loss 0.00310026, acc 1
2016-09-06T01:50:52.520429: step 9298, loss 0.00881096, acc 1
2016-09-06T01:50:53.306381: step 9299, loss 0.0812123, acc 0.98
2016-09-06T01:50:54.103795: step 9300, loss 0.00299291, acc 1

Evaluation:
2016-09-06T01:50:57.804519: step 9300, loss 3.27146, acc 0.713884

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-9300

2016-09-06T01:50:59.695220: step 9301, loss 0.00658728, acc 1
2016-09-06T01:51:00.512055: step 9302, loss 0.0217728, acc 0.98
2016-09-06T01:51:01.366953: step 9303, loss 0.220407, acc 0.98
2016-09-06T01:51:02.203156: step 9304, loss 0.0153407, acc 1
2016-09-06T01:51:03.028729: step 9305, loss 0.0197193, acc 0.98
2016-09-06T01:51:03.834311: step 9306, loss 0.086106, acc 0.96
2016-09-06T01:51:04.677636: step 9307, loss 0.0073016, acc 1
2016-09-06T01:51:05.523287: step 9308, loss 0.132952, acc 0.98
2016-09-06T01:51:06.321474: step 9309, loss 0.00706363, acc 1
2016-09-06T01:51:07.104348: step 9310, loss 0.0113772, acc 1
2016-09-06T01:51:07.940051: step 9311, loss 0.0188127, acc 0.98
2016-09-06T01:51:08.726772: step 9312, loss 0.00632349, acc 1
2016-09-06T01:51:09.533419: step 9313, loss 0.00543058, acc 1
2016-09-06T01:51:10.379911: step 9314, loss 0.0297464, acc 0.98
2016-09-06T01:51:11.209396: step 9315, loss 0.0557544, acc 0.96
2016-09-06T01:51:11.997646: step 9316, loss 0.0444595, acc 0.98
2016-09-06T01:51:12.800357: step 9317, loss 0.00795927, acc 1
2016-09-06T01:51:13.631256: step 9318, loss 0.030201, acc 0.98
2016-09-06T01:51:14.422001: step 9319, loss 0.0267037, acc 0.98
2016-09-06T01:51:15.224547: step 9320, loss 0.02768, acc 0.98
2016-09-06T01:51:16.020373: step 9321, loss 0.0302228, acc 1
2016-09-06T01:51:16.823354: step 9322, loss 0.00896616, acc 1
2016-09-06T01:51:17.641484: step 9323, loss 0.0366616, acc 0.98
2016-09-06T01:51:18.476454: step 9324, loss 0.072844, acc 0.98
2016-09-06T01:51:19.272991: step 9325, loss 0.039221, acc 1
2016-09-06T01:51:20.114639: step 9326, loss 0.0213934, acc 1
2016-09-06T01:51:20.932590: step 9327, loss 0.0249422, acc 0.98
2016-09-06T01:51:21.739791: step 9328, loss 0.0177607, acc 1
2016-09-06T01:51:22.520353: step 9329, loss 0.00600187, acc 1
2016-09-06T01:51:23.325479: step 9330, loss 0.0419066, acc 0.98
2016-09-06T01:51:24.119298: step 9331, loss 0.088952, acc 0.96
2016-09-06T01:51:24.909682: step 9332, loss 0.00703163, acc 1
2016-09-06T01:51:25.730599: step 9333, loss 0.0212798, acc 0.98
2016-09-06T01:51:26.525903: step 9334, loss 0.00515813, acc 1
2016-09-06T01:51:27.318954: step 9335, loss 0.00834417, acc 1
2016-09-06T01:51:28.139065: step 9336, loss 0.0177431, acc 1
2016-09-06T01:51:28.916107: step 9337, loss 0.00506968, acc 1
2016-09-06T01:51:29.719432: step 9338, loss 0.017879, acc 1
2016-09-06T01:51:30.525330: step 9339, loss 0.00494512, acc 1
2016-09-06T01:51:31.321342: step 9340, loss 0.0404079, acc 0.96
2016-09-06T01:51:32.151239: step 9341, loss 0.0316267, acc 1
2016-09-06T01:51:32.967029: step 9342, loss 0.00507318, acc 1
2016-09-06T01:51:33.762883: step 9343, loss 0.0209687, acc 1
2016-09-06T01:51:34.575131: step 9344, loss 0.00546689, acc 1
2016-09-06T01:51:35.367924: step 9345, loss 0.0500284, acc 0.98
2016-09-06T01:51:36.154484: step 9346, loss 0.0174345, acc 1
2016-09-06T01:51:36.968803: step 9347, loss 0.0191431, acc 0.98
2016-09-06T01:51:37.788531: step 9348, loss 0.00471494, acc 1
2016-09-06T01:51:38.604277: step 9349, loss 0.140297, acc 0.96
2016-09-06T01:51:39.395664: step 9350, loss 0.0197262, acc 0.98
2016-09-06T01:51:40.206320: step 9351, loss 0.023762, acc 0.98
2016-09-06T01:51:41.020204: step 9352, loss 0.0422616, acc 0.98
2016-09-06T01:51:41.813598: step 9353, loss 0.0077217, acc 1
2016-09-06T01:51:42.609765: step 9354, loss 0.0197778, acc 1
2016-09-06T01:51:43.436449: step 9355, loss 0.00774703, acc 1
2016-09-06T01:51:44.244517: step 9356, loss 0.00529871, acc 1
2016-09-06T01:51:45.059219: step 9357, loss 0.0415879, acc 0.98
2016-09-06T01:51:45.849062: step 9358, loss 0.0042265, acc 1
2016-09-06T01:51:46.648500: step 9359, loss 0.00674044, acc 1
2016-09-06T01:51:47.465387: step 9360, loss 0.0285268, acc 0.98
2016-09-06T01:51:48.256225: step 9361, loss 0.00552157, acc 1
2016-09-06T01:51:49.050517: step 9362, loss 0.0387675, acc 0.96
2016-09-06T01:51:49.867316: step 9363, loss 0.223158, acc 0.98
2016-09-06T01:51:50.641710: step 9364, loss 0.0153341, acc 1
2016-09-06T01:51:51.449625: step 9365, loss 0.0215839, acc 1
2016-09-06T01:51:52.253906: step 9366, loss 0.0349235, acc 0.98
2016-09-06T01:51:53.061532: step 9367, loss 0.0218388, acc 0.98
2016-09-06T01:51:53.901938: step 9368, loss 0.0102805, acc 1
2016-09-06T01:51:54.718108: step 9369, loss 0.00518441, acc 1
2016-09-06T01:51:55.541414: step 9370, loss 0.00995747, acc 1
2016-09-06T01:51:56.350021: step 9371, loss 0.00471615, acc 1
2016-09-06T01:51:57.161987: step 9372, loss 0.0120153, acc 1
2016-09-06T01:51:57.958425: step 9373, loss 0.00599625, acc 1
2016-09-06T01:51:58.754675: step 9374, loss 0.00524738, acc 1
2016-09-06T01:51:59.572455: step 9375, loss 0.0225933, acc 0.98
2016-09-06T01:52:00.414763: step 9376, loss 0.0169519, acc 1
2016-09-06T01:52:01.209432: step 9377, loss 0.0366935, acc 0.98
2016-09-06T01:52:02.031124: step 9378, loss 0.0194585, acc 1
2016-09-06T01:52:02.830748: step 9379, loss 0.00913908, acc 1
2016-09-06T01:52:03.617404: step 9380, loss 0.0048591, acc 1
2016-09-06T01:52:04.408111: step 9381, loss 0.00505074, acc 1
2016-09-06T01:52:05.252102: step 9382, loss 0.0128363, acc 1
2016-09-06T01:52:06.042685: step 9383, loss 0.0240937, acc 0.98
2016-09-06T01:52:06.837446: step 9384, loss 0.00519196, acc 1
2016-09-06T01:52:07.601436: step 9385, loss 0.0466931, acc 0.98
2016-09-06T01:52:08.406900: step 9386, loss 0.0364551, acc 0.98
2016-09-06T01:52:09.231256: step 9387, loss 0.00535892, acc 1
2016-09-06T01:52:10.017359: step 9388, loss 0.00482846, acc 1
2016-09-06T01:52:10.813859: step 9389, loss 0.00514709, acc 1
2016-09-06T01:52:11.703987: step 9390, loss 0.0236381, acc 0.98
2016-09-06T01:52:12.493085: step 9391, loss 0.0661822, acc 0.96
2016-09-06T01:52:13.305845: step 9392, loss 0.00998369, acc 1
2016-09-06T01:52:14.128368: step 9393, loss 0.036971, acc 0.98
2016-09-06T01:52:14.932171: step 9394, loss 0.0371908, acc 0.98
2016-09-06T01:52:15.725523: step 9395, loss 0.00960887, acc 1
2016-09-06T01:52:16.511058: step 9396, loss 0.0222996, acc 1
2016-09-06T01:52:17.297131: step 9397, loss 0.0167156, acc 1
2016-09-06T01:52:18.096347: step 9398, loss 0.0092028, acc 1
2016-09-06T01:52:18.933684: step 9399, loss 0.00441184, acc 1
2016-09-06T01:52:19.741853: step 9400, loss 0.0142334, acc 1

Evaluation:
2016-09-06T01:52:23.503320: step 9400, loss 2.59061, acc 0.730769

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-9400

2016-09-06T01:52:25.382455: step 9401, loss 0.00621558, acc 1
2016-09-06T01:52:26.232797: step 9402, loss 0.133938, acc 0.96
2016-09-06T01:52:26.993777: step 9403, loss 0.0563104, acc 0.96
2016-09-06T01:52:27.857813: step 9404, loss 0.0131734, acc 1
2016-09-06T01:52:28.747220: step 9405, loss 0.0223803, acc 1
2016-09-06T01:52:29.549479: step 9406, loss 0.0151898, acc 1
2016-09-06T01:52:30.337589: step 9407, loss 0.00512078, acc 1
2016-09-06T01:52:31.101760: step 9408, loss 0.0116944, acc 1
2016-09-06T01:52:31.919551: step 9409, loss 0.0177439, acc 1
2016-09-06T01:52:32.733147: step 9410, loss 0.00537382, acc 1
2016-09-06T01:52:33.554715: step 9411, loss 0.00611881, acc 1
2016-09-06T01:52:34.361576: step 9412, loss 0.0172205, acc 1
2016-09-06T01:52:35.189405: step 9413, loss 0.0124429, acc 1
2016-09-06T01:52:36.022766: step 9414, loss 0.00572967, acc 1
2016-09-06T01:52:36.832408: step 9415, loss 0.00562275, acc 1
2016-09-06T01:52:37.630389: step 9416, loss 0.0305644, acc 0.98
2016-09-06T01:52:38.440571: step 9417, loss 0.0443383, acc 0.98
2016-09-06T01:52:39.261449: step 9418, loss 0.00563906, acc 1
2016-09-06T01:52:40.078029: step 9419, loss 0.0734914, acc 0.96
2016-09-06T01:52:40.913144: step 9420, loss 0.0390708, acc 0.98
2016-09-06T01:52:41.733472: step 9421, loss 0.0355646, acc 0.98
2016-09-06T01:52:42.541278: step 9422, loss 0.0127206, acc 1
2016-09-06T01:52:43.371594: step 9423, loss 0.00851586, acc 1
2016-09-06T01:52:44.198581: step 9424, loss 0.0509946, acc 0.98
2016-09-06T01:52:45.018089: step 9425, loss 0.00486282, acc 1
2016-09-06T01:52:45.837920: step 9426, loss 0.0209289, acc 1
2016-09-06T01:52:46.680289: step 9427, loss 0.00433008, acc 1
2016-09-06T01:52:47.488598: step 9428, loss 0.0188165, acc 0.98
2016-09-06T01:52:48.299386: step 9429, loss 0.00941242, acc 1
2016-09-06T01:52:49.119949: step 9430, loss 0.0309108, acc 1
2016-09-06T01:52:49.925094: step 9431, loss 0.0245906, acc 1
2016-09-06T01:52:50.742710: step 9432, loss 0.00472513, acc 1
2016-09-06T01:52:51.565564: step 9433, loss 0.018173, acc 1
2016-09-06T01:52:52.390404: step 9434, loss 0.0290046, acc 1
2016-09-06T01:52:53.209757: step 9435, loss 0.0120932, acc 1
2016-09-06T01:52:54.025336: step 9436, loss 0.0142802, acc 1
2016-09-06T01:52:54.797286: step 9437, loss 0.0275084, acc 1
2016-09-06T01:52:55.612786: step 9438, loss 0.00420923, acc 1
2016-09-06T01:52:56.436694: step 9439, loss 0.0150968, acc 1
2016-09-06T01:52:57.239279: step 9440, loss 0.00402651, acc 1
2016-09-06T01:52:58.025946: step 9441, loss 0.00502335, acc 1
2016-09-06T01:52:58.879888: step 9442, loss 0.0231432, acc 0.98
2016-09-06T01:52:59.687178: step 9443, loss 0.00968274, acc 1
2016-09-06T01:53:00.525900: step 9444, loss 0.0277248, acc 1
2016-09-06T01:53:01.371677: step 9445, loss 0.00397402, acc 1
2016-09-06T01:53:02.176908: step 9446, loss 0.015592, acc 1
2016-09-06T01:53:03.009073: step 9447, loss 0.0125856, acc 1
2016-09-06T01:53:03.827893: step 9448, loss 0.00389606, acc 1
2016-09-06T01:53:04.661582: step 9449, loss 0.029212, acc 0.98
2016-09-06T01:53:05.470680: step 9450, loss 0.00507945, acc 1
2016-09-06T01:53:06.329009: step 9451, loss 0.0236114, acc 0.98
2016-09-06T01:53:07.155418: step 9452, loss 0.0156456, acc 1
2016-09-06T01:53:07.982543: step 9453, loss 0.00411793, acc 1
2016-09-06T01:53:08.827508: step 9454, loss 0.0620846, acc 0.96
2016-09-06T01:53:09.621543: step 9455, loss 0.00733483, acc 1
2016-09-06T01:53:10.415540: step 9456, loss 0.00482104, acc 1
2016-09-06T01:53:11.232912: step 9457, loss 0.175117, acc 0.98
2016-09-06T01:53:12.052863: step 9458, loss 0.00362925, acc 1
2016-09-06T01:53:12.828047: step 9459, loss 0.00360186, acc 1
2016-09-06T01:53:13.673963: step 9460, loss 0.0513627, acc 0.96
2016-09-06T01:53:14.475612: step 9461, loss 0.00521542, acc 1
2016-09-06T01:53:15.282446: step 9462, loss 0.0032812, acc 1
2016-09-06T01:53:16.106137: step 9463, loss 0.00380251, acc 1
2016-09-06T01:53:16.932437: step 9464, loss 0.0145291, acc 1
2016-09-06T01:53:17.741068: step 9465, loss 0.038967, acc 0.98
2016-09-06T01:53:18.561850: step 9466, loss 0.0210415, acc 1
2016-09-06T01:53:19.393635: step 9467, loss 0.0856091, acc 0.96
2016-09-06T01:53:20.184860: step 9468, loss 0.00300702, acc 1
2016-09-06T01:53:20.981122: step 9469, loss 0.0344646, acc 0.98
2016-09-06T01:53:21.778192: step 9470, loss 0.0569962, acc 0.96
2016-09-06T01:53:22.582820: step 9471, loss 0.00361923, acc 1
2016-09-06T01:53:23.384225: step 9472, loss 0.0346925, acc 0.96
2016-09-06T01:53:24.207367: step 9473, loss 0.0175723, acc 0.98
2016-09-06T01:53:24.996674: step 9474, loss 0.0558061, acc 0.96
2016-09-06T01:53:25.814628: step 9475, loss 0.0295532, acc 0.98
2016-09-06T01:53:26.662365: step 9476, loss 0.00744847, acc 1
2016-09-06T01:53:27.436052: step 9477, loss 0.0276706, acc 0.98
2016-09-06T01:53:28.262124: step 9478, loss 0.0175161, acc 1
2016-09-06T01:53:29.065853: step 9479, loss 0.00914078, acc 1
2016-09-06T01:53:29.848926: step 9480, loss 0.0032514, acc 1
2016-09-06T01:53:30.664139: step 9481, loss 0.00313069, acc 1
2016-09-06T01:53:31.463403: step 9482, loss 0.00309545, acc 1
2016-09-06T01:53:32.285938: step 9483, loss 0.0166027, acc 1
2016-09-06T01:53:33.101232: step 9484, loss 0.0171541, acc 1
2016-09-06T01:53:33.916082: step 9485, loss 0.0162718, acc 1
2016-09-06T01:53:34.683999: step 9486, loss 0.0332864, acc 0.98
2016-09-06T01:53:35.505587: step 9487, loss 0.00866631, acc 1
2016-09-06T01:53:36.309453: step 9488, loss 0.00406469, acc 1
2016-09-06T01:53:37.087808: step 9489, loss 0.0128326, acc 1
2016-09-06T01:53:37.883129: step 9490, loss 0.0481497, acc 0.98
2016-09-06T01:53:38.705924: step 9491, loss 0.0124413, acc 1
2016-09-06T01:53:39.496964: step 9492, loss 0.00482825, acc 1
2016-09-06T01:53:40.302407: step 9493, loss 0.0211153, acc 0.98
2016-09-06T01:53:41.104017: step 9494, loss 0.00437711, acc 1
2016-09-06T01:53:41.884375: step 9495, loss 0.00553993, acc 1
2016-09-06T01:53:42.696191: step 9496, loss 0.054756, acc 0.98
2016-09-06T01:53:43.531513: step 9497, loss 0.033395, acc 0.98
2016-09-06T01:53:44.335512: step 9498, loss 0.00369236, acc 1
2016-09-06T01:53:45.138671: step 9499, loss 0.0400307, acc 0.98
2016-09-06T01:53:45.961855: step 9500, loss 0.0728254, acc 0.98

Evaluation:
2016-09-06T01:53:49.687063: step 9500, loss 3.24318, acc 0.71576

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-9500

2016-09-06T01:53:51.564902: step 9501, loss 0.0222671, acc 0.98
2016-09-06T01:53:52.387210: step 9502, loss 0.00541083, acc 1
2016-09-06T01:53:53.194991: step 9503, loss 0.0165371, acc 1
2016-09-06T01:53:54.055220: step 9504, loss 0.00433677, acc 1
2016-09-06T01:53:54.830139: step 9505, loss 0.0168021, acc 1
2016-09-06T01:53:55.645780: step 9506, loss 0.0897416, acc 0.96
2016-09-06T01:53:56.438352: step 9507, loss 0.018681, acc 0.98
2016-09-06T01:53:57.282237: step 9508, loss 0.024844, acc 0.98
2016-09-06T01:53:58.076058: step 9509, loss 0.0244625, acc 1
2016-09-06T01:53:58.883561: step 9510, loss 0.0787052, acc 0.98
2016-09-06T01:53:59.690591: step 9511, loss 0.0087176, acc 1
2016-09-06T01:54:00.548014: step 9512, loss 0.00457978, acc 1
2016-09-06T01:54:01.370739: step 9513, loss 0.0625241, acc 0.98
2016-09-06T01:54:02.185106: step 9514, loss 0.00429769, acc 1
2016-09-06T01:54:02.998223: step 9515, loss 0.0235779, acc 1
2016-09-06T01:54:03.835164: step 9516, loss 0.00474321, acc 1
2016-09-06T01:54:04.641980: step 9517, loss 0.0294605, acc 1
2016-09-06T01:54:05.465785: step 9518, loss 0.132121, acc 0.92
2016-09-06T01:54:06.279669: step 9519, loss 0.00427226, acc 1
2016-09-06T01:54:07.090194: step 9520, loss 0.0259455, acc 1
2016-09-06T01:54:07.916852: step 9521, loss 0.00457557, acc 1
2016-09-06T01:54:08.708283: step 9522, loss 0.00423074, acc 1
2016-09-06T01:54:09.498001: step 9523, loss 0.0311373, acc 0.98
2016-09-06T01:54:10.320080: step 9524, loss 0.0109345, acc 1
2016-09-06T01:54:11.142904: step 9525, loss 0.0438572, acc 0.98
2016-09-06T01:54:11.958677: step 9526, loss 0.00420239, acc 1
2016-09-06T01:54:12.796469: step 9527, loss 0.0118984, acc 1
2016-09-06T01:54:13.616947: step 9528, loss 0.0675536, acc 0.96
2016-09-06T01:54:14.462317: step 9529, loss 0.00433023, acc 1
2016-09-06T01:54:15.286988: step 9530, loss 0.00478726, acc 1
2016-09-06T01:54:16.116962: step 9531, loss 0.0237293, acc 1
2016-09-06T01:54:16.924905: step 9532, loss 0.0139558, acc 1
2016-09-06T01:54:17.740390: step 9533, loss 0.00443324, acc 1
2016-09-06T01:54:18.561724: step 9534, loss 0.0118722, acc 1
2016-09-06T01:54:19.353162: step 9535, loss 0.0105091, acc 1
2016-09-06T01:54:20.169540: step 9536, loss 0.0276309, acc 0.98
2016-09-06T01:54:20.977738: step 9537, loss 0.00430868, acc 1
2016-09-06T01:54:21.777086: step 9538, loss 0.0167225, acc 1
2016-09-06T01:54:22.602500: step 9539, loss 0.0183821, acc 1
2016-09-06T01:54:23.405306: step 9540, loss 0.131155, acc 0.96
2016-09-06T01:54:24.245112: step 9541, loss 0.0165411, acc 1
2016-09-06T01:54:25.084083: step 9542, loss 0.0162509, acc 1
2016-09-06T01:54:25.900749: step 9543, loss 0.166055, acc 0.96
2016-09-06T01:54:26.687027: step 9544, loss 0.0969315, acc 0.98
2016-09-06T01:54:27.504048: step 9545, loss 0.00376222, acc 1
2016-09-06T01:54:28.327359: step 9546, loss 0.0628626, acc 0.98
2016-09-06T01:54:29.145867: step 9547, loss 0.00818654, acc 1
2016-09-06T01:54:29.938711: step 9548, loss 0.00357265, acc 1
2016-09-06T01:54:30.776026: step 9549, loss 0.0352909, acc 1
2016-09-06T01:54:31.597475: step 9550, loss 0.00613648, acc 1
2016-09-06T01:54:32.427676: step 9551, loss 0.0113769, acc 1
2016-09-06T01:54:33.259222: step 9552, loss 0.0470445, acc 0.98
2016-09-06T01:54:34.105112: step 9553, loss 0.0277127, acc 0.98
2016-09-06T01:54:34.947972: step 9554, loss 0.0121225, acc 1
2016-09-06T01:54:35.778343: step 9555, loss 0.0387096, acc 0.98
2016-09-06T01:54:36.580455: step 9556, loss 0.0343519, acc 0.98
2016-09-06T01:54:37.377569: step 9557, loss 0.0192439, acc 0.98
2016-09-06T01:54:38.208953: step 9558, loss 0.102919, acc 0.96
2016-09-06T01:54:39.008381: step 9559, loss 0.0133785, acc 1
2016-09-06T01:54:39.818160: step 9560, loss 0.00353479, acc 1
2016-09-06T01:54:40.656267: step 9561, loss 0.00372909, acc 1
2016-09-06T01:54:41.504289: step 9562, loss 0.00599082, acc 1
2016-09-06T01:54:42.325878: step 9563, loss 0.0140495, acc 1
2016-09-06T01:54:43.142101: step 9564, loss 0.0452685, acc 0.96
2016-09-06T01:54:43.961341: step 9565, loss 0.00326628, acc 1
2016-09-06T01:54:44.754737: step 9566, loss 0.0243385, acc 0.98
2016-09-06T01:54:45.550710: step 9567, loss 0.00673877, acc 1
2016-09-06T01:54:46.375446: step 9568, loss 0.0504967, acc 0.98
2016-09-06T01:54:47.162105: step 9569, loss 0.0467893, acc 1
2016-09-06T01:54:47.979070: step 9570, loss 0.0198837, acc 0.98
2016-09-06T01:54:48.810372: step 9571, loss 0.00843813, acc 1
2016-09-06T01:54:49.622279: step 9572, loss 0.0153898, acc 1
2016-09-06T01:54:50.448649: step 9573, loss 0.0233668, acc 0.98
2016-09-06T01:54:51.259863: step 9574, loss 0.0266618, acc 1
2016-09-06T01:54:52.024168: step 9575, loss 0.0148546, acc 1
2016-09-06T01:54:52.822101: step 9576, loss 0.0050664, acc 1
2016-09-06T01:54:53.681622: step 9577, loss 0.0250676, acc 0.98
2016-09-06T01:54:54.455674: step 9578, loss 0.0190266, acc 1
2016-09-06T01:54:55.289787: step 9579, loss 0.00873919, acc 1
2016-09-06T01:54:56.082927: step 9580, loss 0.0334336, acc 0.98
2016-09-06T01:54:56.858489: step 9581, loss 0.0036341, acc 1
2016-09-06T01:54:57.664019: step 9582, loss 0.00623748, acc 1
2016-09-06T01:54:58.501068: step 9583, loss 0.020773, acc 0.98
2016-09-06T01:54:59.265193: step 9584, loss 0.00607129, acc 1
2016-09-06T01:55:00.065689: step 9585, loss 0.0075124, acc 1
2016-09-06T01:55:00.938188: step 9586, loss 0.0505399, acc 0.98
2016-09-06T01:55:01.734826: step 9587, loss 0.0198528, acc 1
2016-09-06T01:55:02.540766: step 9588, loss 0.0340612, acc 1
2016-09-06T01:55:03.381717: step 9589, loss 0.0244665, acc 0.98
2016-09-06T01:55:04.177908: step 9590, loss 0.00814483, acc 1
2016-09-06T01:55:04.985327: step 9591, loss 0.00346173, acc 1
2016-09-06T01:55:05.841463: step 9592, loss 0.0660974, acc 0.98
2016-09-06T01:55:06.660768: step 9593, loss 0.0349397, acc 0.96
2016-09-06T01:55:07.473319: step 9594, loss 0.173487, acc 0.94
2016-09-06T01:55:08.324881: step 9595, loss 0.0333328, acc 0.98
2016-09-06T01:55:09.146161: step 9596, loss 0.00709187, acc 1
2016-09-06T01:55:09.966847: step 9597, loss 0.0479195, acc 0.96
2016-09-06T01:55:10.798596: step 9598, loss 0.00578548, acc 1
2016-09-06T01:55:11.612179: step 9599, loss 0.00548242, acc 1
2016-09-06T01:55:12.350847: step 9600, loss 0.0046269, acc 1

Evaluation:
2016-09-06T01:55:16.146012: step 9600, loss 3.36427, acc 0.704503

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-9600

2016-09-06T01:55:18.107994: step 9601, loss 0.00981461, acc 1
2016-09-06T01:55:18.930212: step 9602, loss 0.0911986, acc 0.96
2016-09-06T01:55:19.776315: step 9603, loss 0.00534922, acc 1
2016-09-06T01:55:20.599112: step 9604, loss 0.0184131, acc 0.98
2016-09-06T01:55:21.461224: step 9605, loss 0.0144987, acc 1
2016-09-06T01:55:22.307051: step 9606, loss 0.0387587, acc 0.98
2016-09-06T01:55:23.132088: step 9607, loss 0.0105836, acc 1
2016-09-06T01:55:23.931738: step 9608, loss 0.012698, acc 1
2016-09-06T01:55:24.744329: step 9609, loss 0.00520693, acc 1
2016-09-06T01:55:25.558129: step 9610, loss 0.00387539, acc 1
2016-09-06T01:55:26.384595: step 9611, loss 0.0267829, acc 0.98
2016-09-06T01:55:27.185884: step 9612, loss 0.0122908, acc 1
2016-09-06T01:55:27.985288: step 9613, loss 0.00404556, acc 1
2016-09-06T01:55:28.785554: step 9614, loss 0.00467639, acc 1
2016-09-06T01:55:29.555760: step 9615, loss 0.0193203, acc 0.98
2016-09-06T01:55:30.373039: step 9616, loss 0.0430429, acc 0.98
2016-09-06T01:55:31.190218: step 9617, loss 0.00551956, acc 1
2016-09-06T01:55:31.972227: step 9618, loss 0.0145146, acc 1
2016-09-06T01:55:32.798521: step 9619, loss 0.00445017, acc 1
2016-09-06T01:55:33.623594: step 9620, loss 0.0136659, acc 1
2016-09-06T01:55:34.444820: step 9621, loss 0.00572137, acc 1
2016-09-06T01:55:35.246947: step 9622, loss 0.292732, acc 0.96
2016-09-06T01:55:36.112623: step 9623, loss 0.102517, acc 0.98
2016-09-06T01:55:36.886396: step 9624, loss 0.00581191, acc 1
2016-09-06T01:55:37.714302: step 9625, loss 0.0154646, acc 1
2016-09-06T01:55:38.527690: step 9626, loss 0.00520457, acc 1
2016-09-06T01:55:39.318661: step 9627, loss 0.00557958, acc 1
2016-09-06T01:55:40.110595: step 9628, loss 0.0214446, acc 1
2016-09-06T01:55:40.939382: step 9629, loss 0.038813, acc 0.98
2016-09-06T01:55:41.740482: step 9630, loss 0.132509, acc 0.98
2016-09-06T01:55:42.525354: step 9631, loss 0.0256793, acc 0.98
2016-09-06T01:55:43.361814: step 9632, loss 0.0104605, acc 1
2016-09-06T01:55:44.159863: step 9633, loss 0.0343297, acc 0.98
2016-09-06T01:55:44.960554: step 9634, loss 0.0166995, acc 1
2016-09-06T01:55:45.783029: step 9635, loss 0.00452596, acc 1
2016-09-06T01:55:46.542053: step 9636, loss 0.0126236, acc 1
2016-09-06T01:55:47.338814: step 9637, loss 0.00323308, acc 1
2016-09-06T01:55:48.157976: step 9638, loss 0.104729, acc 0.96
2016-09-06T01:55:48.967019: step 9639, loss 0.0245399, acc 1
2016-09-06T01:55:49.759441: step 9640, loss 0.0105713, acc 1
2016-09-06T01:55:50.582401: step 9641, loss 0.0200164, acc 0.98
2016-09-06T01:55:51.392172: step 9642, loss 0.0147437, acc 1
2016-09-06T01:55:52.197873: step 9643, loss 0.0551159, acc 0.96
2016-09-06T01:55:53.016139: step 9644, loss 0.0129818, acc 1
2016-09-06T01:55:53.821529: step 9645, loss 0.00683215, acc 1
2016-09-06T01:55:54.599887: step 9646, loss 0.0192619, acc 1
2016-09-06T01:55:55.398440: step 9647, loss 0.0204076, acc 0.98
2016-09-06T01:55:56.181357: step 9648, loss 0.0209752, acc 0.98
2016-09-06T01:55:56.992288: step 9649, loss 0.0128753, acc 1
2016-09-06T01:55:57.823442: step 9650, loss 0.00883047, acc 1
2016-09-06T01:55:58.605465: step 9651, loss 0.0248906, acc 0.98
2016-09-06T01:55:59.407504: step 9652, loss 0.0241396, acc 1
2016-09-06T01:56:00.236784: step 9653, loss 0.0247204, acc 1
2016-09-06T01:56:01.038406: step 9654, loss 0.00566312, acc 1
2016-09-06T01:56:01.852398: step 9655, loss 0.0203254, acc 0.98
2016-09-06T01:56:02.694148: step 9656, loss 0.0170196, acc 1
2016-09-06T01:56:03.487049: step 9657, loss 0.00310929, acc 1
2016-09-06T01:56:04.253451: step 9658, loss 0.00440235, acc 1
2016-09-06T01:56:05.064402: step 9659, loss 0.00551844, acc 1
2016-09-06T01:56:05.862854: step 9660, loss 0.00497587, acc 1
2016-09-06T01:56:06.666151: step 9661, loss 0.0286625, acc 0.98
2016-09-06T01:56:07.489443: step 9662, loss 0.00357212, acc 1
2016-09-06T01:56:08.281807: step 9663, loss 0.00704545, acc 1
2016-09-06T01:56:09.097226: step 9664, loss 0.00748781, acc 1
2016-09-06T01:56:09.909523: step 9665, loss 0.0124126, acc 1
2016-09-06T01:56:10.686276: step 9666, loss 0.0227264, acc 1
2016-09-06T01:56:11.525456: step 9667, loss 0.00326732, acc 1
2016-09-06T01:56:12.349632: step 9668, loss 0.0303472, acc 1
2016-09-06T01:56:13.120921: step 9669, loss 0.0598913, acc 0.96
2016-09-06T01:56:13.927872: step 9670, loss 0.0041206, acc 1
2016-09-06T01:56:14.742180: step 9671, loss 0.00679391, acc 1
2016-09-06T01:56:15.540642: step 9672, loss 0.00370804, acc 1
2016-09-06T01:56:16.347676: step 9673, loss 0.0331549, acc 0.98
2016-09-06T01:56:17.153542: step 9674, loss 0.00324939, acc 1
2016-09-06T01:56:17.936653: step 9675, loss 0.0505631, acc 0.98
2016-09-06T01:56:18.741137: step 9676, loss 0.0153992, acc 1
2016-09-06T01:56:19.537083: step 9677, loss 0.0120155, acc 1
2016-09-06T01:56:20.312746: step 9678, loss 0.175896, acc 0.96
2016-09-06T01:56:21.123387: step 9679, loss 0.0153299, acc 1
2016-09-06T01:56:21.915785: step 9680, loss 0.0182747, acc 0.98
2016-09-06T01:56:22.690897: step 9681, loss 0.00315173, acc 1
2016-09-06T01:56:23.522479: step 9682, loss 0.00526336, acc 1
2016-09-06T01:56:24.279585: step 9683, loss 0.0931092, acc 0.98
2016-09-06T01:56:25.079412: step 9684, loss 0.00276645, acc 1
2016-09-06T01:56:25.879246: step 9685, loss 0.00388089, acc 1
2016-09-06T01:56:26.674155: step 9686, loss 0.0107662, acc 1
2016-09-06T01:56:27.485449: step 9687, loss 0.0151686, acc 1
2016-09-06T01:56:28.308547: step 9688, loss 0.01743, acc 1
2016-09-06T01:56:29.109203: step 9689, loss 0.0518451, acc 0.98
2016-09-06T01:56:29.936900: step 9690, loss 0.0387383, acc 0.98
2016-09-06T01:56:30.781334: step 9691, loss 0.03944, acc 0.98
2016-09-06T01:56:31.592153: step 9692, loss 0.0026628, acc 1
2016-09-06T01:56:32.406484: step 9693, loss 0.0121358, acc 1
2016-09-06T01:56:33.225413: step 9694, loss 0.00246593, acc 1
2016-09-06T01:56:34.044867: step 9695, loss 0.00375364, acc 1
2016-09-06T01:56:34.816509: step 9696, loss 0.00560782, acc 1
2016-09-06T01:56:35.640095: step 9697, loss 0.0116217, acc 1
2016-09-06T01:56:36.441441: step 9698, loss 0.0320272, acc 1
2016-09-06T01:56:37.245500: step 9699, loss 0.0074752, acc 1
2016-09-06T01:56:38.062468: step 9700, loss 0.0172323, acc 1

Evaluation:
2016-09-06T01:56:41.788621: step 9700, loss 2.0715, acc 0.718574

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-9700

2016-09-06T01:56:43.689716: step 9701, loss 0.0106784, acc 1
2016-09-06T01:56:44.525417: step 9702, loss 0.00245644, acc 1
2016-09-06T01:56:45.352293: step 9703, loss 0.0045866, acc 1
2016-09-06T01:56:46.177120: step 9704, loss 0.00352453, acc 1
2016-09-06T01:56:46.973409: step 9705, loss 0.0695227, acc 0.94
2016-09-06T01:56:47.801961: step 9706, loss 0.00964667, acc 1
2016-09-06T01:56:48.569350: step 9707, loss 0.0332218, acc 0.96
2016-09-06T01:56:49.387212: step 9708, loss 0.0173881, acc 0.98
2016-09-06T01:56:50.213954: step 9709, loss 0.0221366, acc 1
2016-09-06T01:56:51.028998: step 9710, loss 0.0434402, acc 0.96
2016-09-06T01:56:51.843417: step 9711, loss 0.0651053, acc 0.98
2016-09-06T01:56:52.680676: step 9712, loss 0.0740274, acc 0.96
2016-09-06T01:56:53.489518: step 9713, loss 0.00496657, acc 1
2016-09-06T01:56:54.279794: step 9714, loss 0.00731803, acc 1
2016-09-06T01:56:55.087935: step 9715, loss 0.151424, acc 0.94
2016-09-06T01:56:55.893947: step 9716, loss 0.00577521, acc 1
2016-09-06T01:56:56.666839: step 9717, loss 0.0250935, acc 1
2016-09-06T01:56:57.486521: step 9718, loss 0.00768816, acc 1
2016-09-06T01:56:58.343012: step 9719, loss 0.0116057, acc 1
2016-09-06T01:56:59.153778: step 9720, loss 0.0682469, acc 0.98
2016-09-06T01:56:59.941371: step 9721, loss 0.00245735, acc 1
2016-09-06T01:57:00.762789: step 9722, loss 0.00227546, acc 1
2016-09-06T01:57:01.564194: step 9723, loss 0.0150455, acc 1
2016-09-06T01:57:02.404659: step 9724, loss 0.0197509, acc 1
2016-09-06T01:57:03.238796: step 9725, loss 0.0496849, acc 0.96
2016-09-06T01:57:04.038472: step 9726, loss 0.0210031, acc 0.98
2016-09-06T01:57:04.849456: step 9727, loss 0.0112343, acc 1
2016-09-06T01:57:05.718773: step 9728, loss 0.00955423, acc 1
2016-09-06T01:57:06.521960: step 9729, loss 0.00900974, acc 1
2016-09-06T01:57:07.319521: step 9730, loss 0.008316, acc 1
2016-09-06T01:57:08.128987: step 9731, loss 0.041996, acc 0.98
2016-09-06T01:57:08.910522: step 9732, loss 0.0385878, acc 0.98
2016-09-06T01:57:09.704432: step 9733, loss 0.0164885, acc 1
2016-09-06T01:57:10.550001: step 9734, loss 0.0294746, acc 0.98
2016-09-06T01:57:11.353386: step 9735, loss 0.0136055, acc 1
2016-09-06T01:57:12.165016: step 9736, loss 0.0412754, acc 0.98
2016-09-06T01:57:13.003364: step 9737, loss 0.0170109, acc 1
2016-09-06T01:57:13.798093: step 9738, loss 0.0168186, acc 0.98
2016-09-06T01:57:14.602327: step 9739, loss 0.00272416, acc 1
2016-09-06T01:57:15.406701: step 9740, loss 0.00708806, acc 1
2016-09-06T01:57:16.214914: step 9741, loss 0.0220149, acc 1
2016-09-06T01:57:17.041231: step 9742, loss 0.00258096, acc 1
2016-09-06T01:57:17.854750: step 9743, loss 0.00264656, acc 1
2016-09-06T01:57:18.670495: step 9744, loss 0.0117139, acc 1
2016-09-06T01:57:19.503663: step 9745, loss 0.00501194, acc 1
2016-09-06T01:57:20.347983: step 9746, loss 0.0127616, acc 1
2016-09-06T01:57:21.168166: step 9747, loss 0.018284, acc 1
2016-09-06T01:57:22.008649: step 9748, loss 0.0245776, acc 0.98
2016-09-06T01:57:22.849216: step 9749, loss 0.00444238, acc 1
2016-09-06T01:57:23.657294: step 9750, loss 0.0242043, acc 0.98
2016-09-06T01:57:24.473950: step 9751, loss 0.00292139, acc 1
2016-09-06T01:57:25.302151: step 9752, loss 0.0215559, acc 0.98
2016-09-06T01:57:26.108084: step 9753, loss 0.00334304, acc 1
2016-09-06T01:57:26.888042: step 9754, loss 0.00330291, acc 1
2016-09-06T01:57:27.697433: step 9755, loss 0.0137804, acc 1
2016-09-06T01:57:28.527260: step 9756, loss 0.0888168, acc 0.96
2016-09-06T01:57:29.332359: step 9757, loss 0.0623351, acc 0.96
2016-09-06T01:57:30.165643: step 9758, loss 0.00716855, acc 1
2016-09-06T01:57:30.962305: step 9759, loss 0.00268292, acc 1
2016-09-06T01:57:31.755695: step 9760, loss 0.0517804, acc 0.98
2016-09-06T01:57:32.557494: step 9761, loss 0.0838319, acc 0.96
2016-09-06T01:57:33.378108: step 9762, loss 0.013859, acc 1
2016-09-06T01:57:34.158929: step 9763, loss 0.0123956, acc 1
2016-09-06T01:57:34.970236: step 9764, loss 0.0101351, acc 1
2016-09-06T01:57:35.788082: step 9765, loss 0.0176322, acc 0.98
2016-09-06T01:57:36.572633: step 9766, loss 0.0173187, acc 0.98
2016-09-06T01:57:37.384457: step 9767, loss 0.00830064, acc 1
2016-09-06T01:57:38.189723: step 9768, loss 0.00309656, acc 1
2016-09-06T01:57:38.987514: step 9769, loss 0.0392132, acc 0.98
2016-09-06T01:57:39.795504: step 9770, loss 0.0360572, acc 0.98
2016-09-06T01:57:40.638012: step 9771, loss 0.0586798, acc 0.98
2016-09-06T01:57:41.439664: step 9772, loss 0.0481795, acc 0.98
2016-09-06T01:57:42.231089: step 9773, loss 0.0138395, acc 1
2016-09-06T01:57:43.047074: step 9774, loss 0.0434987, acc 0.98
2016-09-06T01:57:43.823401: step 9775, loss 0.00527136, acc 1
2016-09-06T01:57:44.647094: step 9776, loss 0.0220313, acc 0.98
2016-09-06T01:57:45.489212: step 9777, loss 0.0135051, acc 1
2016-09-06T01:57:46.287492: step 9778, loss 0.0192993, acc 0.98
2016-09-06T01:57:47.095180: step 9779, loss 0.0283142, acc 0.98
2016-09-06T01:57:47.907645: step 9780, loss 0.0186701, acc 1
2016-09-06T01:57:48.708440: step 9781, loss 0.00275789, acc 1
2016-09-06T01:57:49.498701: step 9782, loss 0.0197585, acc 1
2016-09-06T01:57:50.329903: step 9783, loss 0.00276335, acc 1
2016-09-06T01:57:51.097743: step 9784, loss 0.00630573, acc 1
2016-09-06T01:57:51.879926: step 9785, loss 0.00505243, acc 1
2016-09-06T01:57:52.685420: step 9786, loss 0.0170777, acc 1
2016-09-06T01:57:53.468438: step 9787, loss 0.0492442, acc 0.98
2016-09-06T01:57:54.283449: step 9788, loss 0.00247946, acc 1
2016-09-06T01:57:55.104890: step 9789, loss 0.0420162, acc 0.98
2016-09-06T01:57:55.913770: step 9790, loss 0.00663794, acc 1
2016-09-06T01:57:56.725054: step 9791, loss 0.00810142, acc 1
2016-09-06T01:57:57.494997: step 9792, loss 0.0148031, acc 1
2016-09-06T01:57:58.272058: step 9793, loss 0.0289981, acc 0.98
2016-09-06T01:57:59.125566: step 9794, loss 0.00871441, acc 1
2016-09-06T01:57:59.934279: step 9795, loss 0.00673943, acc 1
2016-09-06T01:58:00.785457: step 9796, loss 0.0944311, acc 0.96
2016-09-06T01:58:01.604971: step 9797, loss 0.012472, acc 1
2016-09-06T01:58:02.452699: step 9798, loss 0.0202069, acc 1
2016-09-06T01:58:03.244740: step 9799, loss 0.0137315, acc 1
2016-09-06T01:58:04.056753: step 9800, loss 0.0034579, acc 1

Evaluation:
2016-09-06T01:58:07.801796: step 9800, loss 2.08234, acc 0.723265

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-9800

2016-09-06T01:58:09.623614: step 9801, loss 0.00400006, acc 1
2016-09-06T01:58:10.436283: step 9802, loss 0.00805541, acc 1
2016-09-06T01:58:11.301476: step 9803, loss 0.0875471, acc 0.98
2016-09-06T01:58:12.112548: step 9804, loss 0.017072, acc 1
2016-09-06T01:58:12.901312: step 9805, loss 0.00229249, acc 1
2016-09-06T01:58:13.733439: step 9806, loss 0.00299536, acc 1
2016-09-06T01:58:14.537662: step 9807, loss 0.00395199, acc 1
2016-09-06T01:58:15.348247: step 9808, loss 0.00517775, acc 1
2016-09-06T01:58:16.173923: step 9809, loss 0.00234203, acc 1
2016-09-06T01:58:16.982408: step 9810, loss 0.00933139, acc 1
2016-09-06T01:58:17.830040: step 9811, loss 0.0034753, acc 1
2016-09-06T01:58:18.667839: step 9812, loss 0.00366739, acc 1
2016-09-06T01:58:19.489150: step 9813, loss 0.0158258, acc 1
2016-09-06T01:58:20.285073: step 9814, loss 0.0470697, acc 0.98
2016-09-06T01:58:21.108103: step 9815, loss 0.00243155, acc 1
2016-09-06T01:58:21.917340: step 9816, loss 0.0501365, acc 0.96
2016-09-06T01:58:22.696041: step 9817, loss 0.0215585, acc 1
2016-09-06T01:58:23.504957: step 9818, loss 0.0670525, acc 0.96
2016-09-06T01:58:24.332509: step 9819, loss 0.0190341, acc 0.98
2016-09-06T01:58:25.131142: step 9820, loss 0.00608267, acc 1
2016-09-06T01:58:25.930959: step 9821, loss 0.0366947, acc 0.98
2016-09-06T01:58:26.741316: step 9822, loss 0.00246617, acc 1
2016-09-06T01:58:27.528037: step 9823, loss 0.0814589, acc 0.98
2016-09-06T01:58:28.309732: step 9824, loss 0.0458254, acc 0.98
2016-09-06T01:58:29.129398: step 9825, loss 0.0280095, acc 0.98
2016-09-06T01:58:29.904035: step 9826, loss 0.016421, acc 0.98
2016-09-06T01:58:30.717313: step 9827, loss 0.00742038, acc 1
2016-09-06T01:58:31.550228: step 9828, loss 0.0697715, acc 0.98
2016-09-06T01:58:32.335769: step 9829, loss 0.018818, acc 1
2016-09-06T01:58:33.163435: step 9830, loss 0.0219426, acc 1
2016-09-06T01:58:33.954874: step 9831, loss 0.00431226, acc 1
2016-09-06T01:58:34.745790: step 9832, loss 0.00252624, acc 1
2016-09-06T01:58:35.525501: step 9833, loss 0.0158994, acc 1
2016-09-06T01:58:36.337723: step 9834, loss 0.00243457, acc 1
2016-09-06T01:58:37.134453: step 9835, loss 0.0217103, acc 1
2016-09-06T01:58:37.928784: step 9836, loss 0.00334234, acc 1
2016-09-06T01:58:38.752642: step 9837, loss 0.0127655, acc 1
2016-09-06T01:58:39.574006: step 9838, loss 0.0219762, acc 1
2016-09-06T01:58:40.391589: step 9839, loss 0.040831, acc 0.98
2016-09-06T01:58:41.218543: step 9840, loss 0.0122238, acc 1
2016-09-06T01:58:42.022192: step 9841, loss 0.0232074, acc 0.98
2016-09-06T01:58:42.843336: step 9842, loss 0.00224901, acc 1
2016-09-06T01:58:43.652196: step 9843, loss 0.0295367, acc 0.98
2016-09-06T01:58:44.449425: step 9844, loss 0.0974676, acc 0.98
2016-09-06T01:58:45.253667: step 9845, loss 0.105011, acc 0.94
2016-09-06T01:58:46.079237: step 9846, loss 0.00984899, acc 1
2016-09-06T01:58:46.867525: step 9847, loss 0.0122114, acc 1
2016-09-06T01:58:47.658447: step 9848, loss 0.0182251, acc 1
2016-09-06T01:58:48.481006: step 9849, loss 0.00252128, acc 1
2016-09-06T01:58:49.272916: step 9850, loss 0.0478933, acc 0.98
2016-09-06T01:58:50.065274: step 9851, loss 0.00590595, acc 1
2016-09-06T01:58:50.877457: step 9852, loss 0.0188988, acc 1
2016-09-06T01:58:51.641059: step 9853, loss 0.0137752, acc 1
2016-09-06T01:58:52.430443: step 9854, loss 0.00668432, acc 1
2016-09-06T01:58:53.248775: step 9855, loss 0.00333363, acc 1
2016-09-06T01:58:54.045855: step 9856, loss 0.0492986, acc 0.98
2016-09-06T01:58:54.861510: step 9857, loss 0.0222468, acc 0.98
2016-09-06T01:58:55.683374: step 9858, loss 0.0861929, acc 0.98
2016-09-06T01:58:56.459823: step 9859, loss 0.0106495, acc 1
2016-09-06T01:58:57.272010: step 9860, loss 0.0529196, acc 0.98
2016-09-06T01:58:58.109223: step 9861, loss 0.03834, acc 0.98
2016-09-06T01:58:58.886581: step 9862, loss 0.0422988, acc 0.98
2016-09-06T01:58:59.689531: step 9863, loss 0.0571122, acc 0.98
2016-09-06T01:59:00.545369: step 9864, loss 0.027118, acc 0.98
2016-09-06T01:59:01.347573: step 9865, loss 0.0128356, acc 1
2016-09-06T01:59:02.156304: step 9866, loss 0.0124279, acc 1
2016-09-06T01:59:02.964511: step 9867, loss 0.0283089, acc 0.98
2016-09-06T01:59:03.734937: step 9868, loss 0.0158757, acc 1
2016-09-06T01:59:04.541908: step 9869, loss 0.0102325, acc 1
2016-09-06T01:59:05.368830: step 9870, loss 0.0465048, acc 0.98
2016-09-06T01:59:06.129456: step 9871, loss 0.00302107, acc 1
2016-09-06T01:59:06.938360: step 9872, loss 0.0225739, acc 0.98
2016-09-06T01:59:07.751951: step 9873, loss 0.00309062, acc 1
2016-09-06T01:59:08.543341: step 9874, loss 0.00419235, acc 1
2016-09-06T01:59:09.358284: step 9875, loss 0.077035, acc 0.98
2016-09-06T01:59:10.172310: step 9876, loss 0.00752609, acc 1
2016-09-06T01:59:10.966341: step 9877, loss 0.00540373, acc 1
2016-09-06T01:59:11.802438: step 9878, loss 0.0204041, acc 0.98
2016-09-06T01:59:12.649145: step 9879, loss 0.00318107, acc 1
2016-09-06T01:59:13.449285: step 9880, loss 0.0369402, acc 0.96
2016-09-06T01:59:14.258678: step 9881, loss 0.0866208, acc 0.98
2016-09-06T01:59:15.059697: step 9882, loss 0.00262443, acc 1
2016-09-06T01:59:15.844790: step 9883, loss 0.019409, acc 0.98
2016-09-06T01:59:16.676435: step 9884, loss 0.0274581, acc 0.98
2016-09-06T01:59:17.504919: step 9885, loss 0.0237726, acc 0.98
2016-09-06T01:59:18.313164: step 9886, loss 0.00916095, acc 1
2016-09-06T01:59:19.130725: step 9887, loss 0.0222293, acc 1
2016-09-06T01:59:19.980993: step 9888, loss 0.0136036, acc 1
2016-09-06T01:59:20.790400: step 9889, loss 0.00687615, acc 1
2016-09-06T01:59:21.595891: step 9890, loss 0.0158416, acc 1
2016-09-06T01:59:22.418986: step 9891, loss 0.00854573, acc 1
2016-09-06T01:59:23.227561: step 9892, loss 0.0272759, acc 0.98
2016-09-06T01:59:24.050512: step 9893, loss 0.00590158, acc 1
2016-09-06T01:59:24.890944: step 9894, loss 0.00592264, acc 1
2016-09-06T01:59:25.741811: step 9895, loss 0.0679234, acc 0.98
2016-09-06T01:59:26.535945: step 9896, loss 0.00223772, acc 1
2016-09-06T01:59:27.390025: step 9897, loss 0.0323418, acc 0.98
2016-09-06T01:59:28.186495: step 9898, loss 0.00199197, acc 1
2016-09-06T01:59:28.996129: step 9899, loss 0.0428958, acc 0.96
2016-09-06T01:59:29.841580: step 9900, loss 0.0645531, acc 0.98

Evaluation:
2016-09-06T01:59:33.575268: step 9900, loss 2.18804, acc 0.718574

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-9900

2016-09-06T01:59:35.505363: step 9901, loss 0.0276529, acc 1
2016-09-06T01:59:36.335015: step 9902, loss 0.0577778, acc 0.98
2016-09-06T01:59:37.154959: step 9903, loss 0.00206657, acc 1
2016-09-06T01:59:37.941975: step 9904, loss 0.0509675, acc 0.96
2016-09-06T01:59:38.735636: step 9905, loss 0.0139314, acc 1
2016-09-06T01:59:39.562746: step 9906, loss 0.0113598, acc 1
2016-09-06T01:59:40.392890: step 9907, loss 0.0538534, acc 0.96
2016-09-06T01:59:41.197410: step 9908, loss 0.0202381, acc 1
2016-09-06T01:59:42.041583: step 9909, loss 0.0301973, acc 0.98
2016-09-06T01:59:42.835312: step 9910, loss 0.00686243, acc 1
2016-09-06T01:59:43.634855: step 9911, loss 0.0486611, acc 0.96
2016-09-06T01:59:44.439938: step 9912, loss 0.0187009, acc 1
2016-09-06T01:59:45.257407: step 9913, loss 0.00292053, acc 1
2016-09-06T01:59:46.058430: step 9914, loss 0.00400311, acc 1
2016-09-06T01:59:46.848788: step 9915, loss 0.0102927, acc 1
2016-09-06T01:59:47.666948: step 9916, loss 0.0342873, acc 0.98
2016-09-06T01:59:48.461153: step 9917, loss 0.0601591, acc 0.96
2016-09-06T01:59:49.242785: step 9918, loss 0.0176224, acc 0.98
2016-09-06T01:59:50.071905: step 9919, loss 0.0209322, acc 0.98
2016-09-06T01:59:50.858670: step 9920, loss 0.0176248, acc 0.98
2016-09-06T01:59:51.679033: step 9921, loss 0.0153542, acc 1
2016-09-06T01:59:52.493036: step 9922, loss 0.0296113, acc 1
2016-09-06T01:59:53.312848: step 9923, loss 0.0192106, acc 0.98
2016-09-06T01:59:54.128196: step 9924, loss 0.0909352, acc 0.96
2016-09-06T01:59:54.950775: step 9925, loss 0.0144708, acc 1
2016-09-06T01:59:55.745763: step 9926, loss 0.0186793, acc 0.98
2016-09-06T01:59:56.545664: step 9927, loss 0.0513187, acc 0.98
2016-09-06T01:59:57.324822: step 9928, loss 0.0452239, acc 0.98
2016-09-06T01:59:58.118823: step 9929, loss 0.0173563, acc 0.98
2016-09-06T01:59:58.956587: step 9930, loss 0.0107345, acc 1
2016-09-06T01:59:59.774187: step 9931, loss 0.00299856, acc 1
2016-09-06T02:00:00.590577: step 9932, loss 0.00864401, acc 1
2016-09-06T02:00:01.401591: step 9933, loss 0.0231974, acc 0.98
2016-09-06T02:00:02.203992: step 9934, loss 0.0522158, acc 0.98
2016-09-06T02:00:02.982203: step 9935, loss 0.0119655, acc 1
2016-09-06T02:00:03.771381: step 9936, loss 0.0695182, acc 0.96
2016-09-06T02:00:04.568964: step 9937, loss 0.00199799, acc 1
2016-09-06T02:00:05.363200: step 9938, loss 0.011288, acc 1
2016-09-06T02:00:06.160879: step 9939, loss 0.0366161, acc 1
2016-09-06T02:00:07.004507: step 9940, loss 0.0114396, acc 1
2016-09-06T02:00:07.785403: step 9941, loss 0.0265639, acc 0.98
2016-09-06T02:00:08.594462: step 9942, loss 0.00501003, acc 1
2016-09-06T02:00:09.477181: step 9943, loss 0.00558839, acc 1
2016-09-06T02:00:10.297614: step 9944, loss 0.00192752, acc 1
2016-09-06T02:00:11.139602: step 9945, loss 0.0137128, acc 1
2016-09-06T02:00:11.969007: step 9946, loss 0.03071, acc 0.98
2016-09-06T02:00:12.791911: step 9947, loss 0.00173957, acc 1
2016-09-06T02:00:13.623056: step 9948, loss 0.023832, acc 1
2016-09-06T02:00:14.436712: step 9949, loss 0.0145098, acc 1
2016-09-06T02:00:15.229751: step 9950, loss 0.00644572, acc 1
2016-09-06T02:00:16.030815: step 9951, loss 0.0580751, acc 0.98
2016-09-06T02:00:16.863927: step 9952, loss 0.0503298, acc 0.96
2016-09-06T02:00:17.679946: step 9953, loss 0.0511889, acc 0.96
2016-09-06T02:00:18.480223: step 9954, loss 0.0180915, acc 1
2016-09-06T02:00:19.289464: step 9955, loss 0.0193522, acc 0.98
2016-09-06T02:00:20.096542: step 9956, loss 0.0906825, acc 0.98
2016-09-06T02:00:20.887064: step 9957, loss 0.00174806, acc 1
2016-09-06T02:00:21.746183: step 9958, loss 0.0297815, acc 0.98
2016-09-06T02:00:22.569347: step 9959, loss 0.0048248, acc 1
2016-09-06T02:00:23.386039: step 9960, loss 0.0170725, acc 0.98
2016-09-06T02:00:24.253215: step 9961, loss 0.0321628, acc 1
2016-09-06T02:00:25.099412: step 9962, loss 0.0456707, acc 0.98
2016-09-06T02:00:25.937320: step 9963, loss 0.0415379, acc 0.98
2016-09-06T02:00:26.754200: step 9964, loss 0.00287752, acc 1
2016-09-06T02:00:27.559986: step 9965, loss 0.00720846, acc 1
2016-09-06T02:00:28.323245: step 9966, loss 0.024429, acc 0.98
2016-09-06T02:00:29.121299: step 9967, loss 0.0160386, acc 1
2016-09-06T02:00:29.948231: step 9968, loss 0.00992647, acc 1
2016-09-06T02:00:30.737061: step 9969, loss 0.0418143, acc 0.98
2016-09-06T02:00:31.554303: step 9970, loss 0.0482957, acc 0.98
2016-09-06T02:00:32.394812: step 9971, loss 0.0250172, acc 0.98
2016-09-06T02:00:33.189442: step 9972, loss 0.0309311, acc 1
2016-09-06T02:00:33.994963: step 9973, loss 0.0188334, acc 0.98
2016-09-06T02:00:34.815300: step 9974, loss 0.190004, acc 0.98
2016-09-06T02:00:35.585385: step 9975, loss 0.00237505, acc 1
2016-09-06T02:00:36.375284: step 9976, loss 0.00265601, acc 1
2016-09-06T02:00:37.200652: step 9977, loss 0.031908, acc 0.98
2016-09-06T02:00:38.004655: step 9978, loss 0.0639743, acc 0.98
2016-09-06T02:00:38.801017: step 9979, loss 0.0364249, acc 0.98
2016-09-06T02:00:39.652700: step 9980, loss 0.0024004, acc 1
2016-09-06T02:00:40.431752: step 9981, loss 0.035501, acc 0.98
2016-09-06T02:00:41.238952: step 9982, loss 0.0242153, acc 0.98
2016-09-06T02:00:42.078806: step 9983, loss 0.0865174, acc 0.98
2016-09-06T02:00:42.790293: step 9984, loss 0.00590608, acc 1
2016-09-06T02:00:43.619844: step 9985, loss 0.00357634, acc 1
2016-09-06T02:00:44.427094: step 9986, loss 0.0275508, acc 0.98
2016-09-06T02:00:45.208307: step 9987, loss 0.0175645, acc 1
2016-09-06T02:00:46.057668: step 9988, loss 0.0639186, acc 0.98
2016-09-06T02:00:46.926849: step 9989, loss 0.0528392, acc 0.96
2016-09-06T02:00:47.735357: step 9990, loss 0.00235515, acc 1
2016-09-06T02:00:48.514891: step 9991, loss 0.0248126, acc 0.98
2016-09-06T02:00:49.324516: step 9992, loss 0.0348465, acc 0.98
2016-09-06T02:00:50.095538: step 9993, loss 0.00787684, acc 1
2016-09-06T02:00:50.884658: step 9994, loss 0.00269386, acc 1
2016-09-06T02:00:51.713439: step 9995, loss 0.0182617, acc 1
2016-09-06T02:00:52.486379: step 9996, loss 0.00988416, acc 1
2016-09-06T02:00:53.307079: step 9997, loss 0.00567338, acc 1
2016-09-06T02:00:54.131489: step 9998, loss 0.00294275, acc 1
2016-09-06T02:00:54.920428: step 9999, loss 0.0103389, acc 1
2016-09-06T02:00:55.757406: step 10000, loss 0.0426668, acc 0.98

Evaluation:
2016-09-06T02:00:59.505583: step 10000, loss 2.01378, acc 0.721388

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-10000

2016-09-06T02:01:01.344065: step 10001, loss 0.00377784, acc 1
2016-09-06T02:01:02.135485: step 10002, loss 0.0224737, acc 1
2016-09-06T02:01:02.972337: step 10003, loss 0.0239437, acc 0.98
2016-09-06T02:01:03.792789: step 10004, loss 0.0330908, acc 0.98
2016-09-06T02:01:04.613473: step 10005, loss 0.0148718, acc 1
2016-09-06T02:01:05.434521: step 10006, loss 0.0786235, acc 0.96
2016-09-06T02:01:06.257501: step 10007, loss 0.00703977, acc 1
2016-09-06T02:01:07.074531: step 10008, loss 0.00484086, acc 1
2016-09-06T02:01:07.904132: step 10009, loss 0.0656354, acc 0.96
2016-09-06T02:01:08.704496: step 10010, loss 0.0324377, acc 0.98
2016-09-06T02:01:09.552332: step 10011, loss 0.033594, acc 0.98
2016-09-06T02:01:10.359513: step 10012, loss 0.0581671, acc 0.98
2016-09-06T02:01:11.165912: step 10013, loss 0.0194725, acc 1
2016-09-06T02:01:11.978454: step 10014, loss 0.0293193, acc 0.98
2016-09-06T02:01:12.806763: step 10015, loss 0.0106612, acc 1
2016-09-06T02:01:13.630561: step 10016, loss 0.00229666, acc 1
2016-09-06T02:01:14.441044: step 10017, loss 0.0287005, acc 0.98
2016-09-06T02:01:15.236157: step 10018, loss 0.0279875, acc 0.98
2016-09-06T02:01:16.088035: step 10019, loss 0.0256724, acc 0.98
2016-09-06T02:01:16.866437: step 10020, loss 0.00752164, acc 1
2016-09-06T02:01:17.665980: step 10021, loss 0.00221725, acc 1
2016-09-06T02:01:18.504092: step 10022, loss 0.0173888, acc 1
2016-09-06T02:01:19.306236: step 10023, loss 0.0510415, acc 0.96
2016-09-06T02:01:20.115300: step 10024, loss 0.0118122, acc 1
2016-09-06T02:01:20.950662: step 10025, loss 0.00782949, acc 1
2016-09-06T02:01:21.752153: step 10026, loss 0.0377444, acc 0.98
2016-09-06T02:01:22.553322: step 10027, loss 0.0364744, acc 0.98
2016-09-06T02:01:23.400806: step 10028, loss 0.0748172, acc 0.98
2016-09-06T02:01:24.203938: step 10029, loss 0.00210049, acc 1
2016-09-06T02:01:24.989464: step 10030, loss 0.0770425, acc 0.98
2016-09-06T02:01:25.813364: step 10031, loss 0.0176026, acc 1
2016-09-06T02:01:26.598827: step 10032, loss 0.0165063, acc 1
2016-09-06T02:01:27.394245: step 10033, loss 0.0144987, acc 1
2016-09-06T02:01:28.194317: step 10034, loss 0.00399875, acc 1
2016-09-06T02:01:28.995342: step 10035, loss 0.0238245, acc 1
2016-09-06T02:01:29.824860: step 10036, loss 0.038704, acc 0.98
2016-09-06T02:01:30.649783: step 10037, loss 0.00879656, acc 1
2016-09-06T02:01:31.442563: step 10038, loss 0.0200605, acc 1
2016-09-06T02:01:32.265337: step 10039, loss 0.00782698, acc 1
2016-09-06T02:01:33.097928: step 10040, loss 0.00479654, acc 1
2016-09-06T02:01:33.922204: step 10041, loss 0.0437935, acc 0.98
2016-09-06T02:01:34.746276: step 10042, loss 0.0307398, acc 1
2016-09-06T02:01:35.594710: step 10043, loss 0.00268594, acc 1
2016-09-06T02:01:36.402149: step 10044, loss 0.0119087, acc 1
2016-09-06T02:01:37.210398: step 10045, loss 0.0437301, acc 1
2016-09-06T02:01:38.027848: step 10046, loss 0.0021703, acc 1
2016-09-06T02:01:38.860041: step 10047, loss 0.021946, acc 0.98
2016-09-06T02:01:39.686798: step 10048, loss 0.0283981, acc 1
2016-09-06T02:01:40.535595: step 10049, loss 0.0221002, acc 0.98
2016-09-06T02:01:41.369259: step 10050, loss 0.0342714, acc 0.98
2016-09-06T02:01:42.188824: step 10051, loss 0.00250669, acc 1
2016-09-06T02:01:42.997858: step 10052, loss 0.059173, acc 0.96
2016-09-06T02:01:43.810576: step 10053, loss 0.0410629, acc 0.96
2016-09-06T02:01:44.605260: step 10054, loss 0.0343066, acc 0.98
2016-09-06T02:01:45.445407: step 10055, loss 0.0641135, acc 0.98
2016-09-06T02:01:46.286961: step 10056, loss 0.00387987, acc 1
2016-09-06T02:01:47.076857: step 10057, loss 0.00723707, acc 1
2016-09-06T02:01:47.885449: step 10058, loss 0.00532914, acc 1
2016-09-06T02:01:48.699922: step 10059, loss 0.0159361, acc 1
2016-09-06T02:01:49.494962: step 10060, loss 0.0660406, acc 0.98
2016-09-06T02:01:50.283861: step 10061, loss 0.0130462, acc 1
2016-09-06T02:01:51.103182: step 10062, loss 0.0186658, acc 0.98
2016-09-06T02:01:51.868530: step 10063, loss 0.0142309, acc 1
2016-09-06T02:01:52.734661: step 10064, loss 0.00381601, acc 1
2016-09-06T02:01:53.557298: step 10065, loss 0.018372, acc 0.98
2016-09-06T02:01:54.361828: step 10066, loss 0.0126907, acc 1
2016-09-06T02:01:55.153221: step 10067, loss 0.0231249, acc 0.98
2016-09-06T02:01:55.970556: step 10068, loss 0.0203407, acc 0.98
2016-09-06T02:01:56.769628: step 10069, loss 0.00890436, acc 1
2016-09-06T02:01:57.606955: step 10070, loss 0.011979, acc 1
2016-09-06T02:01:58.435845: step 10071, loss 0.00280603, acc 1
2016-09-06T02:01:59.261837: step 10072, loss 0.0279719, acc 1
2016-09-06T02:02:00.091740: step 10073, loss 0.0136005, acc 1
2016-09-06T02:02:00.923597: step 10074, loss 0.00320981, acc 1
2016-09-06T02:02:01.744821: step 10075, loss 0.0182978, acc 0.98
2016-09-06T02:02:02.551912: step 10076, loss 0.00244267, acc 1
2016-09-06T02:02:03.374947: step 10077, loss 0.00252714, acc 1
2016-09-06T02:02:04.209977: step 10078, loss 0.00278288, acc 1
2016-09-06T02:02:05.018726: step 10079, loss 0.019652, acc 0.98
2016-09-06T02:02:05.838616: step 10080, loss 0.00238609, acc 1
2016-09-06T02:02:06.642727: step 10081, loss 0.0355067, acc 0.98
2016-09-06T02:02:07.456730: step 10082, loss 0.0217322, acc 1
2016-09-06T02:02:08.266828: step 10083, loss 0.00438975, acc 1
2016-09-06T02:02:09.065775: step 10084, loss 0.004832, acc 1
2016-09-06T02:02:09.856540: step 10085, loss 0.00737702, acc 1
2016-09-06T02:02:10.675942: step 10086, loss 0.0195925, acc 0.98
2016-09-06T02:02:11.522207: step 10087, loss 0.0150146, acc 1
2016-09-06T02:02:12.316136: step 10088, loss 0.00239836, acc 1
2016-09-06T02:02:13.165006: step 10089, loss 0.0795036, acc 0.96
2016-09-06T02:02:13.976271: step 10090, loss 0.0159891, acc 1
2016-09-06T02:02:14.780523: step 10091, loss 0.0153845, acc 1
2016-09-06T02:02:15.596850: step 10092, loss 0.00316044, acc 1
2016-09-06T02:02:16.404087: step 10093, loss 0.019359, acc 1
2016-09-06T02:02:17.204639: step 10094, loss 0.0348635, acc 0.98
2016-09-06T02:02:18.035115: step 10095, loss 0.00261373, acc 1
2016-09-06T02:02:18.858576: step 10096, loss 0.0295361, acc 0.98
2016-09-06T02:02:19.661146: step 10097, loss 0.0164221, acc 1
2016-09-06T02:02:20.468750: step 10098, loss 0.0208985, acc 0.98
2016-09-06T02:02:21.298729: step 10099, loss 0.0220506, acc 1
2016-09-06T02:02:22.133933: step 10100, loss 0.00252445, acc 1

Evaluation:
2016-09-06T02:02:25.893731: step 10100, loss 2.51816, acc 0.724203

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-10100

2016-09-06T02:02:27.790998: step 10101, loss 0.00861373, acc 1
2016-09-06T02:02:28.626008: step 10102, loss 0.00371062, acc 1
2016-09-06T02:02:29.458245: step 10103, loss 0.110848, acc 0.98
2016-09-06T02:02:30.263255: step 10104, loss 0.0133652, acc 1
2016-09-06T02:02:31.132060: step 10105, loss 0.00927757, acc 1
2016-09-06T02:02:31.960782: step 10106, loss 0.0120075, acc 1
2016-09-06T02:02:32.748138: step 10107, loss 0.00222153, acc 1
2016-09-06T02:02:33.573416: step 10108, loss 0.0173242, acc 1
2016-09-06T02:02:34.402177: step 10109, loss 0.0157317, acc 1
2016-09-06T02:02:35.209229: step 10110, loss 0.0189006, acc 1
2016-09-06T02:02:36.046809: step 10111, loss 0.0335702, acc 1
2016-09-06T02:02:36.859980: step 10112, loss 0.0183773, acc 0.98
2016-09-06T02:02:37.691534: step 10113, loss 0.0065855, acc 1
2016-09-06T02:02:38.527379: step 10114, loss 0.0200864, acc 0.98
2016-09-06T02:02:39.329376: step 10115, loss 0.0241856, acc 0.98
2016-09-06T02:02:40.173427: step 10116, loss 0.0316728, acc 0.98
2016-09-06T02:02:41.006001: step 10117, loss 0.130588, acc 0.96
2016-09-06T02:02:41.840481: step 10118, loss 0.00463336, acc 1
2016-09-06T02:02:42.643749: step 10119, loss 0.0199323, acc 1
2016-09-06T02:02:43.441603: step 10120, loss 0.00308168, acc 1
2016-09-06T02:02:44.261229: step 10121, loss 0.019814, acc 1
2016-09-06T02:02:45.049318: step 10122, loss 0.0563562, acc 0.96
2016-09-06T02:02:45.833278: step 10123, loss 0.0351701, acc 0.98
2016-09-06T02:02:46.650932: step 10124, loss 0.00185557, acc 1
2016-09-06T02:02:47.459755: step 10125, loss 0.0287942, acc 1
2016-09-06T02:02:48.317362: step 10126, loss 0.0017663, acc 1
2016-09-06T02:02:49.129950: step 10127, loss 0.00223999, acc 1
2016-09-06T02:02:49.930832: step 10128, loss 0.0150818, acc 1
2016-09-06T02:02:50.741424: step 10129, loss 0.0380842, acc 0.98
2016-09-06T02:02:51.551030: step 10130, loss 0.0766565, acc 0.96
2016-09-06T02:02:52.349057: step 10131, loss 0.0281518, acc 1
2016-09-06T02:02:53.197067: step 10132, loss 0.00432485, acc 1
2016-09-06T02:02:54.013275: step 10133, loss 0.0320743, acc 0.98
2016-09-06T02:02:54.824829: step 10134, loss 0.0214856, acc 0.98
2016-09-06T02:02:55.646518: step 10135, loss 0.0100113, acc 1
2016-09-06T02:02:56.451152: step 10136, loss 0.00289084, acc 1
2016-09-06T02:02:57.292292: step 10137, loss 0.00714543, acc 1
2016-09-06T02:02:58.131420: step 10138, loss 0.00212692, acc 1
2016-09-06T02:02:58.952957: step 10139, loss 0.0292141, acc 1
2016-09-06T02:02:59.773407: step 10140, loss 0.0120374, acc 1
2016-09-06T02:03:00.641644: step 10141, loss 0.0219332, acc 1
2016-09-06T02:03:01.462131: step 10142, loss 0.00293147, acc 1
2016-09-06T02:03:02.258380: step 10143, loss 0.00301179, acc 1
2016-09-06T02:03:03.044983: step 10144, loss 0.0382826, acc 0.98
2016-09-06T02:03:03.848960: step 10145, loss 0.0408961, acc 1
2016-09-06T02:03:04.662989: step 10146, loss 0.016241, acc 1
2016-09-06T02:03:05.504509: step 10147, loss 0.00291111, acc 1
2016-09-06T02:03:06.366741: step 10148, loss 0.00269648, acc 1
2016-09-06T02:03:07.175308: step 10149, loss 0.00669455, acc 1
2016-09-06T02:03:07.964575: step 10150, loss 0.00447959, acc 1
2016-09-06T02:03:08.793372: step 10151, loss 0.0194756, acc 0.98
2016-09-06T02:03:09.616036: step 10152, loss 0.00217534, acc 1
2016-09-06T02:03:10.429004: step 10153, loss 0.031974, acc 0.98
2016-09-06T02:03:11.256965: step 10154, loss 0.00472416, acc 1
2016-09-06T02:03:12.102687: step 10155, loss 0.00409832, acc 1
2016-09-06T02:03:12.917595: step 10156, loss 0.0164297, acc 1
2016-09-06T02:03:13.728282: step 10157, loss 0.00681718, acc 1
2016-09-06T02:03:14.556611: step 10158, loss 0.0276138, acc 1
2016-09-06T02:03:15.346678: step 10159, loss 0.0209332, acc 1
2016-09-06T02:03:16.144095: step 10160, loss 0.00385179, acc 1
2016-09-06T02:03:16.956078: step 10161, loss 0.00792216, acc 1
2016-09-06T02:03:17.747889: step 10162, loss 0.054495, acc 0.98
2016-09-06T02:03:18.532965: step 10163, loss 0.0167857, acc 0.98
2016-09-06T02:03:19.339822: step 10164, loss 0.0188807, acc 1
2016-09-06T02:03:20.104943: step 10165, loss 0.00779036, acc 1
2016-09-06T02:03:20.917884: step 10166, loss 0.0296479, acc 0.98
2016-09-06T02:03:21.740780: step 10167, loss 0.00242253, acc 1
2016-09-06T02:03:22.560986: step 10168, loss 0.00360373, acc 1
2016-09-06T02:03:23.346564: step 10169, loss 0.00681812, acc 1
2016-09-06T02:03:24.188253: step 10170, loss 0.0449747, acc 0.98
2016-09-06T02:03:24.992240: step 10171, loss 0.00297079, acc 1
2016-09-06T02:03:25.797190: step 10172, loss 0.00220248, acc 1
2016-09-06T02:03:26.599200: step 10173, loss 0.00222139, acc 1
2016-09-06T02:03:27.366443: step 10174, loss 0.0139681, acc 1
2016-09-06T02:03:28.168884: step 10175, loss 0.00417321, acc 1
2016-09-06T02:03:28.922852: step 10176, loss 0.00913674, acc 1
2016-09-06T02:03:29.724533: step 10177, loss 0.022511, acc 0.98
2016-09-06T02:03:30.548452: step 10178, loss 0.0178326, acc 1
2016-09-06T02:03:31.354758: step 10179, loss 0.0234651, acc 0.98
2016-09-06T02:03:32.147832: step 10180, loss 0.0186217, acc 1
2016-09-06T02:03:32.954834: step 10181, loss 0.00216524, acc 1
2016-09-06T02:03:33.780200: step 10182, loss 0.009426, acc 1
2016-09-06T02:03:34.562840: step 10183, loss 0.00218462, acc 1
2016-09-06T02:03:35.387140: step 10184, loss 0.0482739, acc 0.98
2016-09-06T02:03:36.215803: step 10185, loss 0.00442052, acc 1
2016-09-06T02:03:37.024560: step 10186, loss 0.00218266, acc 1
2016-09-06T02:03:37.828810: step 10187, loss 0.0163153, acc 1
2016-09-06T02:03:38.641814: step 10188, loss 0.0288898, acc 0.98
2016-09-06T02:03:39.442984: step 10189, loss 0.00213713, acc 1
2016-09-06T02:03:40.226995: step 10190, loss 0.00216103, acc 1
2016-09-06T02:03:41.071984: step 10191, loss 0.00292449, acc 1
2016-09-06T02:03:41.884015: step 10192, loss 0.047114, acc 1
2016-09-06T02:03:42.706346: step 10193, loss 0.00207398, acc 1
2016-09-06T02:03:43.528870: step 10194, loss 0.00238607, acc 1
2016-09-06T02:03:44.308867: step 10195, loss 0.050292, acc 0.98
2016-09-06T02:03:45.150354: step 10196, loss 0.00772063, acc 1
2016-09-06T02:03:45.986148: step 10197, loss 0.00228525, acc 1
2016-09-06T02:03:46.838334: step 10198, loss 0.0086986, acc 1
2016-09-06T02:03:47.610577: step 10199, loss 0.026857, acc 0.98
2016-09-06T02:03:48.461448: step 10200, loss 0.0454226, acc 0.96

Evaluation:
2016-09-06T02:03:52.257192: step 10200, loss 2.59017, acc 0.718574

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-10200

2016-09-06T02:03:54.220270: step 10201, loss 0.0509856, acc 0.98
2016-09-06T02:03:55.005014: step 10202, loss 0.0277864, acc 0.98
2016-09-06T02:03:55.857324: step 10203, loss 0.00899431, acc 1
2016-09-06T02:03:56.676174: step 10204, loss 0.00569969, acc 1
2016-09-06T02:03:57.482325: step 10205, loss 0.0354758, acc 0.96
2016-09-06T02:03:58.327709: step 10206, loss 0.00204289, acc 1
2016-09-06T02:03:59.129495: step 10207, loss 0.017423, acc 1
2016-09-06T02:03:59.966684: step 10208, loss 0.0352125, acc 0.98
2016-09-06T02:04:00.812691: step 10209, loss 0.0237573, acc 0.98
2016-09-06T02:04:01.634086: step 10210, loss 0.0147281, acc 1
2016-09-06T02:04:02.480133: step 10211, loss 0.0139591, acc 1
2016-09-06T02:04:03.303041: step 10212, loss 0.0124531, acc 1
2016-09-06T02:04:04.124700: step 10213, loss 0.00203996, acc 1
2016-09-06T02:04:04.905641: step 10214, loss 0.0128407, acc 1
2016-09-06T02:04:05.717975: step 10215, loss 0.0143299, acc 1
2016-09-06T02:04:06.534897: step 10216, loss 0.0117839, acc 1
2016-09-06T02:04:07.368880: step 10217, loss 0.0245739, acc 0.98
2016-09-06T02:04:08.157835: step 10218, loss 0.00778304, acc 1
2016-09-06T02:04:08.989472: step 10219, loss 0.0264516, acc 0.98
2016-09-06T02:04:09.817929: step 10220, loss 0.00222413, acc 1
2016-09-06T02:04:10.647483: step 10221, loss 0.00227606, acc 1
2016-09-06T02:04:11.469229: step 10222, loss 0.0390545, acc 0.96
2016-09-06T02:04:12.310798: step 10223, loss 0.0131802, acc 1
2016-09-06T02:04:13.131439: step 10224, loss 0.0350565, acc 0.98
2016-09-06T02:04:13.967049: step 10225, loss 0.00262123, acc 1
2016-09-06T02:04:14.770222: step 10226, loss 0.00219211, acc 1
2016-09-06T02:04:15.578240: step 10227, loss 0.0269963, acc 0.98
2016-09-06T02:04:16.407612: step 10228, loss 0.125572, acc 0.98
2016-09-06T02:04:17.223626: step 10229, loss 0.0131313, acc 1
2016-09-06T02:04:18.044442: step 10230, loss 0.014512, acc 1
2016-09-06T02:04:18.878593: step 10231, loss 0.0074706, acc 1
2016-09-06T02:04:19.713923: step 10232, loss 0.00671128, acc 1
2016-09-06T02:04:20.521048: step 10233, loss 0.00937409, acc 1
2016-09-06T02:04:21.356312: step 10234, loss 0.00662137, acc 1
2016-09-06T02:04:22.163456: step 10235, loss 0.0129168, acc 1
2016-09-06T02:04:22.970872: step 10236, loss 0.00244107, acc 1
2016-09-06T02:04:23.760570: step 10237, loss 0.0317335, acc 0.98
2016-09-06T02:04:24.572246: step 10238, loss 0.00303678, acc 1
2016-09-06T02:04:25.347459: step 10239, loss 0.00336687, acc 1
2016-09-06T02:04:26.160555: step 10240, loss 0.0237628, acc 0.98
2016-09-06T02:04:26.987449: step 10241, loss 0.00373573, acc 1
2016-09-06T02:04:27.770482: step 10242, loss 0.0140879, acc 1
2016-09-06T02:04:28.632064: step 10243, loss 0.0196659, acc 0.98
2016-09-06T02:04:29.465107: step 10244, loss 0.0236813, acc 0.98
2016-09-06T02:04:30.247642: step 10245, loss 0.0456883, acc 0.96
2016-09-06T02:04:31.070703: step 10246, loss 0.0189579, acc 1
2016-09-06T02:04:31.885024: step 10247, loss 0.00304891, acc 1
2016-09-06T02:04:32.684688: step 10248, loss 0.00250048, acc 1
2016-09-06T02:04:33.475974: step 10249, loss 0.00585508, acc 1
2016-09-06T02:04:34.310928: step 10250, loss 0.00252884, acc 1
2016-09-06T02:04:35.098242: step 10251, loss 0.0135676, acc 1
2016-09-06T02:04:35.870784: step 10252, loss 0.0200622, acc 0.98
2016-09-06T02:04:36.701432: step 10253, loss 0.0190658, acc 0.98
2016-09-06T02:04:37.482017: step 10254, loss 0.034596, acc 0.98
2016-09-06T02:04:38.274089: step 10255, loss 0.00870332, acc 1
2016-09-06T02:04:39.084494: step 10256, loss 0.00712721, acc 1
2016-09-06T02:04:39.888519: step 10257, loss 0.00363867, acc 1
2016-09-06T02:04:40.739300: step 10258, loss 0.00334264, acc 1
2016-09-06T02:04:41.533403: step 10259, loss 0.043807, acc 0.98
2016-09-06T02:04:42.322049: step 10260, loss 0.00940952, acc 1
2016-09-06T02:04:43.121971: step 10261, loss 0.0030902, acc 1
2016-09-06T02:04:43.930887: step 10262, loss 0.00465214, acc 1
2016-09-06T02:04:44.724462: step 10263, loss 0.00265925, acc 1
2016-09-06T02:04:45.538184: step 10264, loss 0.00443568, acc 1
2016-09-06T02:04:46.373644: step 10265, loss 0.00408372, acc 1
2016-09-06T02:04:47.179986: step 10266, loss 0.00442389, acc 1
2016-09-06T02:04:47.972859: step 10267, loss 0.0100442, acc 1
2016-09-06T02:04:48.768437: step 10268, loss 0.00287151, acc 1
2016-09-06T02:04:49.568032: step 10269, loss 0.0200648, acc 1
2016-09-06T02:04:50.370493: step 10270, loss 0.00433615, acc 1
2016-09-06T02:04:51.205985: step 10271, loss 0.0158626, acc 1
2016-09-06T02:04:51.984766: step 10272, loss 0.00272234, acc 1
2016-09-06T02:04:52.786385: step 10273, loss 0.00316813, acc 1
2016-09-06T02:04:53.621388: step 10274, loss 0.0181995, acc 1
2016-09-06T02:04:54.421410: step 10275, loss 0.0664569, acc 0.98
2016-09-06T02:04:55.202134: step 10276, loss 0.0248407, acc 0.98
2016-09-06T02:04:55.992212: step 10277, loss 0.0042621, acc 1
2016-09-06T02:04:56.785451: step 10278, loss 0.00274754, acc 1
2016-09-06T02:04:57.592022: step 10279, loss 0.0175237, acc 0.98
2016-09-06T02:04:58.410487: step 10280, loss 0.00291537, acc 1
2016-09-06T02:04:59.186351: step 10281, loss 0.0147953, acc 1
2016-09-06T02:05:00.013469: step 10282, loss 0.0031893, acc 1
2016-09-06T02:05:00.899813: step 10283, loss 0.00759592, acc 1
2016-09-06T02:05:01.708555: step 10284, loss 0.039809, acc 0.98
2016-09-06T02:05:02.515611: step 10285, loss 0.0336353, acc 0.98
2016-09-06T02:05:03.348443: step 10286, loss 0.138885, acc 0.98
2016-09-06T02:05:04.151145: step 10287, loss 0.00225161, acc 1
2016-09-06T02:05:04.959204: step 10288, loss 0.0199904, acc 1
2016-09-06T02:05:05.773110: step 10289, loss 0.00862767, acc 1
2016-09-06T02:05:06.581812: step 10290, loss 0.00244628, acc 1
2016-09-06T02:05:07.426043: step 10291, loss 0.00819559, acc 1
2016-09-06T02:05:08.240167: step 10292, loss 0.018305, acc 0.98
2016-09-06T02:05:09.053371: step 10293, loss 0.0171766, acc 1
2016-09-06T02:05:09.875017: step 10294, loss 0.00207551, acc 1
2016-09-06T02:05:10.710132: step 10295, loss 0.0193333, acc 0.98
2016-09-06T02:05:11.542311: step 10296, loss 0.0141873, acc 1
2016-09-06T02:05:12.321317: step 10297, loss 0.00558589, acc 1
2016-09-06T02:05:13.150900: step 10298, loss 0.00201034, acc 1
2016-09-06T02:05:13.964917: step 10299, loss 0.00438847, acc 1
2016-09-06T02:05:14.779417: step 10300, loss 0.0193881, acc 1

Evaluation:
2016-09-06T02:05:18.570190: step 10300, loss 2.48708, acc 0.706379

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-10300

2016-09-06T02:05:20.495694: step 10301, loss 0.00217315, acc 1
2016-09-06T02:05:21.300206: step 10302, loss 0.00460091, acc 1
2016-09-06T02:05:22.106412: step 10303, loss 0.0307984, acc 0.98
2016-09-06T02:05:22.925535: step 10304, loss 0.00286887, acc 1
2016-09-06T02:05:23.742093: step 10305, loss 0.0033486, acc 1
2016-09-06T02:05:24.583980: step 10306, loss 0.0180472, acc 1
2016-09-06T02:05:25.403630: step 10307, loss 0.00566112, acc 1
2016-09-06T02:05:26.245018: step 10308, loss 0.00278779, acc 1
2016-09-06T02:05:27.058052: step 10309, loss 0.0447155, acc 0.98
2016-09-06T02:05:27.897430: step 10310, loss 0.0158262, acc 1
2016-09-06T02:05:28.701411: step 10311, loss 0.0332504, acc 0.98
2016-09-06T02:05:29.497090: step 10312, loss 0.00792225, acc 1
2016-09-06T02:05:30.313405: step 10313, loss 0.0169165, acc 0.98
2016-09-06T02:05:31.143909: step 10314, loss 0.111669, acc 0.98
2016-09-06T02:05:31.929827: step 10315, loss 0.00202246, acc 1
2016-09-06T02:05:32.701541: step 10316, loss 0.116126, acc 0.98
2016-09-06T02:05:33.497344: step 10317, loss 0.0298763, acc 0.98
2016-09-06T02:05:34.276203: step 10318, loss 0.0647504, acc 0.98
2016-09-06T02:05:35.131305: step 10319, loss 0.00586107, acc 1
2016-09-06T02:05:35.992552: step 10320, loss 0.0299168, acc 0.98
2016-09-06T02:05:36.796852: step 10321, loss 0.0084201, acc 1
2016-09-06T02:05:37.584450: step 10322, loss 0.00630354, acc 1
2016-09-06T02:05:38.391172: step 10323, loss 0.0868171, acc 0.98
2016-09-06T02:05:39.184363: step 10324, loss 0.00543315, acc 1
2016-09-06T02:05:39.994494: step 10325, loss 0.0164416, acc 0.98
2016-09-06T02:05:40.814049: step 10326, loss 0.00216864, acc 1
2016-09-06T02:05:41.603669: step 10327, loss 0.00326145, acc 1
2016-09-06T02:05:42.405302: step 10328, loss 0.00639767, acc 1
2016-09-06T02:05:43.196763: step 10329, loss 0.0176496, acc 1
2016-09-06T02:05:43.975870: step 10330, loss 0.0113558, acc 1
2016-09-06T02:05:44.780831: step 10331, loss 0.0126542, acc 1
2016-09-06T02:05:45.625533: step 10332, loss 0.0219588, acc 1
2016-09-06T02:05:46.420299: step 10333, loss 0.00594674, acc 1
2016-09-06T02:05:47.205029: step 10334, loss 0.00533499, acc 1
2016-09-06T02:05:48.042224: step 10335, loss 0.00316048, acc 1
2016-09-06T02:05:48.842127: step 10336, loss 0.0208192, acc 1
2016-09-06T02:05:49.670110: step 10337, loss 0.0395208, acc 0.98
2016-09-06T02:05:50.475017: step 10338, loss 0.0170712, acc 0.98
2016-09-06T02:05:51.269770: step 10339, loss 0.0190962, acc 0.98
2016-09-06T02:05:52.084263: step 10340, loss 0.00947628, acc 1
2016-09-06T02:05:52.900063: step 10341, loss 0.00189733, acc 1
2016-09-06T02:05:53.698713: step 10342, loss 0.0299465, acc 0.98
2016-09-06T02:05:54.513436: step 10343, loss 0.00674168, acc 1
2016-09-06T02:05:55.318187: step 10344, loss 0.00517516, acc 1
2016-09-06T02:05:56.126546: step 10345, loss 0.0621283, acc 0.96
2016-09-06T02:05:56.927330: step 10346, loss 0.012992, acc 1
2016-09-06T02:05:57.724695: step 10347, loss 0.0120929, acc 1
2016-09-06T02:05:58.518031: step 10348, loss 0.0151514, acc 1
2016-09-06T02:05:59.315417: step 10349, loss 0.00240681, acc 1
2016-09-06T02:06:00.114070: step 10350, loss 0.0419241, acc 0.96
2016-09-06T02:06:00.915850: step 10351, loss 0.141479, acc 0.96
2016-09-06T02:06:01.685097: step 10352, loss 0.0028264, acc 1
2016-09-06T02:06:02.525371: step 10353, loss 0.0193057, acc 1
2016-09-06T02:06:03.334995: step 10354, loss 0.0506005, acc 0.96
2016-09-06T02:06:04.144048: step 10355, loss 0.025121, acc 0.98
2016-09-06T02:06:04.952794: step 10356, loss 0.0330479, acc 0.98
2016-09-06T02:06:05.737442: step 10357, loss 0.00465741, acc 1
2016-09-06T02:06:06.540581: step 10358, loss 0.00845188, acc 1
2016-09-06T02:06:07.343335: step 10359, loss 0.00895628, acc 1
2016-09-06T02:06:08.127595: step 10360, loss 0.00348234, acc 1
2016-09-06T02:06:08.921164: step 10361, loss 0.0192842, acc 0.98
2016-09-06T02:06:09.721722: step 10362, loss 0.0421474, acc 0.96
2016-09-06T02:06:10.495275: step 10363, loss 0.0205303, acc 0.98
2016-09-06T02:06:11.305335: step 10364, loss 0.00546291, acc 1
2016-09-06T02:06:12.095558: step 10365, loss 0.0353724, acc 0.98
2016-09-06T02:06:12.912048: step 10366, loss 0.0697799, acc 0.98
2016-09-06T02:06:13.729936: step 10367, loss 0.0635114, acc 0.96
2016-09-06T02:06:14.503655: step 10368, loss 0.00244907, acc 1
2016-09-06T02:06:15.315824: step 10369, loss 0.0371615, acc 0.98
2016-09-06T02:06:16.136581: step 10370, loss 0.0708084, acc 0.94
2016-09-06T02:06:16.942677: step 10371, loss 0.00371451, acc 1
2016-09-06T02:06:17.755827: step 10372, loss 0.0356754, acc 0.98
2016-09-06T02:06:18.588619: step 10373, loss 0.0043653, acc 1
2016-09-06T02:06:19.411631: step 10374, loss 0.00943014, acc 1
2016-09-06T02:06:20.307946: step 10375, loss 0.00439252, acc 1
2016-09-06T02:06:21.127760: step 10376, loss 0.0179006, acc 1
2016-09-06T02:06:21.947828: step 10377, loss 0.0193887, acc 1
2016-09-06T02:06:22.774084: step 10378, loss 0.00316796, acc 1
2016-09-06T02:06:23.570028: step 10379, loss 0.00744254, acc 1
2016-09-06T02:06:24.405999: step 10380, loss 0.053805, acc 0.96
2016-09-06T02:06:25.236803: step 10381, loss 0.0216334, acc 0.98
2016-09-06T02:06:26.090150: step 10382, loss 0.0344365, acc 0.98
2016-09-06T02:06:27.036161: step 10383, loss 0.00313692, acc 1
2016-09-06T02:06:27.857697: step 10384, loss 0.0378817, acc 0.98
2016-09-06T02:06:28.667072: step 10385, loss 0.0212221, acc 1
2016-09-06T02:06:29.472509: step 10386, loss 0.0059648, acc 1
2016-09-06T02:06:30.302614: step 10387, loss 0.0166135, acc 1
2016-09-06T02:06:31.116990: step 10388, loss 0.0216261, acc 0.98
2016-09-06T02:06:31.943299: step 10389, loss 0.0323325, acc 0.98
2016-09-06T02:06:32.788336: step 10390, loss 0.0034732, acc 1
2016-09-06T02:06:33.595765: step 10391, loss 0.0214724, acc 1
2016-09-06T02:06:34.403173: step 10392, loss 0.108596, acc 0.98
2016-09-06T02:06:35.261086: step 10393, loss 0.0146556, acc 1
2016-09-06T02:06:36.094074: step 10394, loss 0.0474266, acc 0.96
2016-09-06T02:06:36.932866: step 10395, loss 0.030512, acc 0.98
2016-09-06T02:06:37.757055: step 10396, loss 0.00501928, acc 1
2016-09-06T02:06:38.589683: step 10397, loss 0.0299771, acc 1
2016-09-06T02:06:39.393827: step 10398, loss 0.00371894, acc 1
2016-09-06T02:06:40.238106: step 10399, loss 0.00492849, acc 1
2016-09-06T02:06:41.055784: step 10400, loss 0.0339871, acc 0.98

Evaluation:
2016-09-06T02:06:44.810096: step 10400, loss 3.30896, acc 0.708255

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-10400

2016-09-06T02:06:46.725311: step 10401, loss 0.018208, acc 1
2016-09-06T02:06:47.605942: step 10402, loss 0.0230605, acc 0.98
2016-09-06T02:06:48.431488: step 10403, loss 0.00346198, acc 1
2016-09-06T02:06:49.301413: step 10404, loss 0.0196833, acc 0.98
2016-09-06T02:06:50.136440: step 10405, loss 0.00410174, acc 1
2016-09-06T02:06:50.984962: step 10406, loss 0.0321472, acc 1
2016-09-06T02:06:51.774697: step 10407, loss 0.0235365, acc 0.98
2016-09-06T02:06:52.574601: step 10408, loss 0.00848663, acc 1
2016-09-06T02:06:53.488599: step 10409, loss 0.0222895, acc 1
2016-09-06T02:06:54.363551: step 10410, loss 0.00383707, acc 1
2016-09-06T02:06:55.171842: step 10411, loss 0.00452528, acc 1
2016-09-06T02:06:55.989132: step 10412, loss 0.0400447, acc 0.98
2016-09-06T02:06:56.798718: step 10413, loss 0.00673628, acc 1
2016-09-06T02:06:57.610774: step 10414, loss 0.00932441, acc 1
2016-09-06T02:06:58.449278: step 10415, loss 0.0559449, acc 0.96
2016-09-06T02:06:59.248001: step 10416, loss 0.0125597, acc 1
2016-09-06T02:07:00.056221: step 10417, loss 0.0779886, acc 0.96
2016-09-06T02:07:00.936589: step 10418, loss 0.00968308, acc 1
2016-09-06T02:07:01.745880: step 10419, loss 0.00354223, acc 1
2016-09-06T02:07:02.541403: step 10420, loss 0.0256376, acc 0.98
2016-09-06T02:07:03.378744: step 10421, loss 0.0262801, acc 0.98
2016-09-06T02:07:04.191255: step 10422, loss 0.0176799, acc 1
2016-09-06T02:07:04.978903: step 10423, loss 0.00295152, acc 1
2016-09-06T02:07:05.788515: step 10424, loss 0.00353456, acc 1
2016-09-06T02:07:06.651223: step 10425, loss 0.00669468, acc 1
2016-09-06T02:07:07.426675: step 10426, loss 0.029152, acc 0.98
2016-09-06T02:07:08.212623: step 10427, loss 0.00649628, acc 1
2016-09-06T02:07:09.047735: step 10428, loss 0.0390845, acc 0.98
2016-09-06T02:07:09.833964: step 10429, loss 0.0471302, acc 0.98
2016-09-06T02:07:10.611594: step 10430, loss 0.0031372, acc 1
2016-09-06T02:07:11.433762: step 10431, loss 0.00263006, acc 1
2016-09-06T02:07:12.238303: step 10432, loss 0.0172751, acc 1
2016-09-06T02:07:13.041599: step 10433, loss 0.0128109, acc 1
2016-09-06T02:07:13.879291: step 10434, loss 0.00822241, acc 1
2016-09-06T02:07:14.651826: step 10435, loss 0.0363936, acc 0.96
2016-09-06T02:07:15.467572: step 10436, loss 0.0554868, acc 0.94
2016-09-06T02:07:16.280302: step 10437, loss 0.0611554, acc 0.96
2016-09-06T02:07:17.065095: step 10438, loss 0.00275101, acc 1
2016-09-06T02:07:17.895836: step 10439, loss 0.017109, acc 1
2016-09-06T02:07:18.698739: step 10440, loss 0.0104399, acc 1
2016-09-06T02:07:19.475682: step 10441, loss 0.0159557, acc 1
2016-09-06T02:07:20.282894: step 10442, loss 0.0330422, acc 0.98
2016-09-06T02:07:21.125358: step 10443, loss 0.0382633, acc 0.96
2016-09-06T02:07:21.928124: step 10444, loss 0.0175905, acc 0.98
2016-09-06T02:07:22.731597: step 10445, loss 0.0126675, acc 1
2016-09-06T02:07:23.576731: step 10446, loss 0.165927, acc 0.96
2016-09-06T02:07:24.389838: step 10447, loss 0.00431034, acc 1
2016-09-06T02:07:25.196762: step 10448, loss 0.00600401, acc 1
2016-09-06T02:07:26.029246: step 10449, loss 0.00235075, acc 1
2016-09-06T02:07:26.836986: step 10450, loss 0.0148681, acc 1
2016-09-06T02:07:27.675323: step 10451, loss 0.032793, acc 0.98
2016-09-06T02:07:28.509538: step 10452, loss 0.0153055, acc 1
2016-09-06T02:07:29.366719: step 10453, loss 0.016282, acc 1
2016-09-06T02:07:30.170309: step 10454, loss 0.0165662, acc 1
2016-09-06T02:07:31.014895: step 10455, loss 0.00296494, acc 1
2016-09-06T02:07:31.819175: step 10456, loss 0.0123242, acc 1
2016-09-06T02:07:32.623891: step 10457, loss 0.0241607, acc 1
2016-09-06T02:07:33.474934: step 10458, loss 0.0321293, acc 0.98
2016-09-06T02:07:34.300435: step 10459, loss 0.0141512, acc 1
2016-09-06T02:07:35.125142: step 10460, loss 0.0250836, acc 0.98
2016-09-06T02:07:35.947973: step 10461, loss 0.0510101, acc 0.98
2016-09-06T02:07:36.766093: step 10462, loss 0.00369173, acc 1
2016-09-06T02:07:37.576694: step 10463, loss 0.0159154, acc 1
2016-09-06T02:07:38.410277: step 10464, loss 0.0026383, acc 1
2016-09-06T02:07:39.244719: step 10465, loss 0.00542857, acc 1
2016-09-06T02:07:40.063275: step 10466, loss 0.0497212, acc 0.98
2016-09-06T02:07:40.863610: step 10467, loss 0.00875819, acc 1
2016-09-06T02:07:41.685536: step 10468, loss 0.0293863, acc 0.98
2016-09-06T02:07:42.458029: step 10469, loss 0.0216131, acc 1
2016-09-06T02:07:43.240512: step 10470, loss 0.117584, acc 0.96
2016-09-06T02:07:44.035160: step 10471, loss 0.00292821, acc 1
2016-09-06T02:07:44.813955: step 10472, loss 0.00459645, acc 1
2016-09-06T02:07:45.629107: step 10473, loss 0.0261706, acc 0.98
2016-09-06T02:07:46.441462: step 10474, loss 0.00444433, acc 1
2016-09-06T02:07:47.218330: step 10475, loss 0.0502626, acc 0.98
2016-09-06T02:07:48.035473: step 10476, loss 0.0278501, acc 0.98
2016-09-06T02:07:48.834277: step 10477, loss 0.0291392, acc 1
2016-09-06T02:07:49.632237: step 10478, loss 0.047367, acc 0.98
2016-09-06T02:07:50.499614: step 10479, loss 0.0276602, acc 0.98
2016-09-06T02:07:51.337727: step 10480, loss 0.0363254, acc 0.98
2016-09-06T02:07:52.133126: step 10481, loss 0.00320387, acc 1
2016-09-06T02:07:52.914242: step 10482, loss 0.0125129, acc 1
2016-09-06T02:07:53.745412: step 10483, loss 0.0222426, acc 1
2016-09-06T02:07:54.541403: step 10484, loss 0.0257006, acc 0.98
2016-09-06T02:07:55.320594: step 10485, loss 0.012881, acc 1
2016-09-06T02:07:56.134601: step 10486, loss 0.00315693, acc 1
2016-09-06T02:07:56.944575: step 10487, loss 0.0707486, acc 0.96
2016-09-06T02:07:57.762453: step 10488, loss 0.00635759, acc 1
2016-09-06T02:07:58.571882: step 10489, loss 0.00312959, acc 1
2016-09-06T02:07:59.337112: step 10490, loss 0.0273314, acc 0.98
2016-09-06T02:08:00.165129: step 10491, loss 0.0252018, acc 0.98
2016-09-06T02:08:01.023794: step 10492, loss 0.00508838, acc 1
2016-09-06T02:08:01.814253: step 10493, loss 0.0171823, acc 1
2016-09-06T02:08:02.615276: step 10494, loss 0.00353947, acc 1
2016-09-06T02:08:03.430257: step 10495, loss 0.00619932, acc 1
2016-09-06T02:08:04.225628: step 10496, loss 0.0126767, acc 1
2016-09-06T02:08:05.025430: step 10497, loss 0.0283547, acc 0.98
2016-09-06T02:08:05.831865: step 10498, loss 0.0107184, acc 1
2016-09-06T02:08:06.609631: step 10499, loss 0.00374423, acc 1
2016-09-06T02:08:07.441518: step 10500, loss 0.0024494, acc 1

Evaluation:
2016-09-06T02:08:11.197054: step 10500, loss 2.75887, acc 0.717636

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-10500

2016-09-06T02:08:13.060857: step 10501, loss 0.00287911, acc 1
2016-09-06T02:08:13.853879: step 10502, loss 0.0266083, acc 0.98
2016-09-06T02:08:14.664423: step 10503, loss 0.0179295, acc 0.98
2016-09-06T02:08:15.462264: step 10504, loss 0.00570942, acc 1
2016-09-06T02:08:16.260885: step 10505, loss 0.0157735, acc 1
2016-09-06T02:08:17.086948: step 10506, loss 0.033614, acc 0.96
2016-09-06T02:08:17.899963: step 10507, loss 0.00257883, acc 1
2016-09-06T02:08:18.704055: step 10508, loss 0.00248062, acc 1
2016-09-06T02:08:19.486424: step 10509, loss 0.0062893, acc 1
2016-09-06T02:08:20.299670: step 10510, loss 0.00276036, acc 1
2016-09-06T02:08:21.082975: step 10511, loss 0.0462006, acc 0.96
2016-09-06T02:08:21.895421: step 10512, loss 0.0199994, acc 1
2016-09-06T02:08:22.734624: step 10513, loss 0.0130838, acc 1
2016-09-06T02:08:23.560524: step 10514, loss 0.0128456, acc 1
2016-09-06T02:08:24.373321: step 10515, loss 0.0173645, acc 1
2016-09-06T02:08:25.201777: step 10516, loss 0.0159874, acc 1
2016-09-06T02:08:26.002134: step 10517, loss 0.0189758, acc 0.98
2016-09-06T02:08:26.806568: step 10518, loss 0.0497081, acc 0.98
2016-09-06T02:08:27.614166: step 10519, loss 0.00275048, acc 1
2016-09-06T02:08:28.403167: step 10520, loss 0.00276023, acc 1
2016-09-06T02:08:29.186751: step 10521, loss 0.0101175, acc 1
2016-09-06T02:08:29.995204: step 10522, loss 0.00983828, acc 1
2016-09-06T02:08:30.760449: step 10523, loss 0.00631193, acc 1
2016-09-06T02:08:31.588590: step 10524, loss 0.0262878, acc 1
2016-09-06T02:08:32.413461: step 10525, loss 0.0129477, acc 1
2016-09-06T02:08:33.203649: step 10526, loss 0.0231054, acc 1
2016-09-06T02:08:33.993792: step 10527, loss 0.022328, acc 0.98
2016-09-06T02:08:34.802732: step 10528, loss 0.0300513, acc 0.98
2016-09-06T02:08:35.583005: step 10529, loss 0.021089, acc 1
2016-09-06T02:08:36.391029: step 10530, loss 0.00496512, acc 1
2016-09-06T02:08:37.205343: step 10531, loss 0.00463792, acc 1
2016-09-06T02:08:38.042503: step 10532, loss 0.00513708, acc 1
2016-09-06T02:08:38.834216: step 10533, loss 0.00246292, acc 1
2016-09-06T02:08:39.655437: step 10534, loss 0.0525213, acc 0.98
2016-09-06T02:08:40.461508: step 10535, loss 0.0172645, acc 1
2016-09-06T02:08:41.280604: step 10536, loss 0.00402858, acc 1
2016-09-06T02:08:42.111654: step 10537, loss 0.0339898, acc 0.98
2016-09-06T02:08:42.878062: step 10538, loss 0.00238158, acc 1
2016-09-06T02:08:43.670638: step 10539, loss 0.017784, acc 1
2016-09-06T02:08:44.483934: step 10540, loss 0.0122307, acc 1
2016-09-06T02:08:45.270269: step 10541, loss 0.0179698, acc 1
2016-09-06T02:08:46.097924: step 10542, loss 0.0312191, acc 0.98
2016-09-06T02:08:46.913764: step 10543, loss 0.00613088, acc 1
2016-09-06T02:08:47.704876: step 10544, loss 0.00233638, acc 1
2016-09-06T02:08:48.496470: step 10545, loss 0.0455448, acc 0.98
2016-09-06T02:08:49.320446: step 10546, loss 0.0293071, acc 1
2016-09-06T02:08:50.102906: step 10547, loss 0.0161904, acc 1
2016-09-06T02:08:50.916069: step 10548, loss 0.021459, acc 0.98
2016-09-06T02:08:51.759064: step 10549, loss 0.0331126, acc 0.98
2016-09-06T02:08:52.537244: step 10550, loss 0.0127762, acc 1
2016-09-06T02:08:53.347447: step 10551, loss 0.00238438, acc 1
2016-09-06T02:08:54.151606: step 10552, loss 0.00241948, acc 1
2016-09-06T02:08:54.954189: step 10553, loss 0.00247618, acc 1
2016-09-06T02:08:55.742189: step 10554, loss 0.0138806, acc 1
2016-09-06T02:08:56.555838: step 10555, loss 0.0409203, acc 0.98
2016-09-06T02:08:57.377467: step 10556, loss 0.0105381, acc 1
2016-09-06T02:08:58.228484: step 10557, loss 0.00483606, acc 1
2016-09-06T02:08:59.054932: step 10558, loss 0.00509851, acc 1
2016-09-06T02:08:59.852399: step 10559, loss 0.0242602, acc 1
2016-09-06T02:09:00.606600: step 10560, loss 0.00318849, acc 1
2016-09-06T02:09:01.425858: step 10561, loss 0.0297492, acc 0.98
2016-09-06T02:09:02.232986: step 10562, loss 0.0551732, acc 0.98
2016-09-06T02:09:03.029674: step 10563, loss 0.00647713, acc 1
2016-09-06T02:09:03.853114: step 10564, loss 0.0214459, acc 0.98
2016-09-06T02:09:04.641007: step 10565, loss 0.00227026, acc 1
2016-09-06T02:09:05.449845: step 10566, loss 0.00226446, acc 1
2016-09-06T02:09:06.269486: step 10567, loss 0.0528314, acc 0.98
2016-09-06T02:09:07.043286: step 10568, loss 0.0225654, acc 0.98
2016-09-06T02:09:07.837469: step 10569, loss 0.0278824, acc 0.98
2016-09-06T02:09:08.644960: step 10570, loss 0.00221284, acc 1
2016-09-06T02:09:09.445452: step 10571, loss 0.0376787, acc 0.96
2016-09-06T02:09:10.284669: step 10572, loss 0.0736271, acc 0.96
2016-09-06T02:09:11.127882: step 10573, loss 0.00346825, acc 1
2016-09-06T02:09:11.931053: step 10574, loss 0.00183795, acc 1
2016-09-06T02:09:12.721685: step 10575, loss 0.00186245, acc 1
2016-09-06T02:09:13.552953: step 10576, loss 0.0262521, acc 1
2016-09-06T02:09:14.343534: step 10577, loss 0.0237678, acc 1
2016-09-06T02:09:15.141265: step 10578, loss 0.00520335, acc 1
2016-09-06T02:09:15.952209: step 10579, loss 0.00191562, acc 1
2016-09-06T02:09:16.724143: step 10580, loss 0.0163609, acc 1
2016-09-06T02:09:17.557479: step 10581, loss 0.00450223, acc 1
2016-09-06T02:09:18.377741: step 10582, loss 0.0333433, acc 0.98
2016-09-06T02:09:19.172154: step 10583, loss 0.00181605, acc 1
2016-09-06T02:09:19.970097: step 10584, loss 0.0163072, acc 0.98
2016-09-06T02:09:20.807031: step 10585, loss 0.0146981, acc 1
2016-09-06T02:09:21.601097: step 10586, loss 0.00408605, acc 1
2016-09-06T02:09:22.389843: step 10587, loss 0.00239344, acc 1
2016-09-06T02:09:23.200611: step 10588, loss 0.00441557, acc 1
2016-09-06T02:09:23.999748: step 10589, loss 0.052094, acc 0.96
2016-09-06T02:09:24.780326: step 10590, loss 0.00196939, acc 1
2016-09-06T02:09:25.576445: step 10591, loss 0.00679205, acc 1
2016-09-06T02:09:26.369243: step 10592, loss 0.00180102, acc 1
2016-09-06T02:09:27.178696: step 10593, loss 0.0164826, acc 1
2016-09-06T02:09:27.988512: step 10594, loss 0.00464771, acc 1
2016-09-06T02:09:28.799392: step 10595, loss 0.00473557, acc 1
2016-09-06T02:09:29.618979: step 10596, loss 0.00285697, acc 1
2016-09-06T02:09:30.445981: step 10597, loss 0.00171326, acc 1
2016-09-06T02:09:31.232988: step 10598, loss 0.0396623, acc 1
2016-09-06T02:09:32.024444: step 10599, loss 0.00801032, acc 1
2016-09-06T02:09:32.833874: step 10600, loss 0.00185753, acc 1

Evaluation:
2016-09-06T02:09:36.549763: step 10600, loss 2.9641, acc 0.708255

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-10600

2016-09-06T02:09:38.578744: step 10601, loss 0.0235967, acc 1
2016-09-06T02:09:39.413011: step 10602, loss 0.00852147, acc 1
2016-09-06T02:09:40.256434: step 10603, loss 0.0175499, acc 1
2016-09-06T02:09:41.054831: step 10604, loss 0.0601273, acc 0.98
2016-09-06T02:09:41.849438: step 10605, loss 0.00180244, acc 1
2016-09-06T02:09:42.665650: step 10606, loss 0.00827605, acc 1
2016-09-06T02:09:43.448861: step 10607, loss 0.0203781, acc 0.98
2016-09-06T02:09:44.278879: step 10608, loss 0.0267755, acc 0.98
2016-09-06T02:09:45.098811: step 10609, loss 0.025958, acc 1
2016-09-06T02:09:45.902546: step 10610, loss 0.011192, acc 1
2016-09-06T02:09:46.704358: step 10611, loss 0.056386, acc 0.98
2016-09-06T02:09:47.535445: step 10612, loss 0.0435622, acc 0.98
2016-09-06T02:09:48.384080: step 10613, loss 0.0352323, acc 0.98
2016-09-06T02:09:49.181312: step 10614, loss 0.0139575, acc 1
2016-09-06T02:09:50.031264: step 10615, loss 0.0018163, acc 1
2016-09-06T02:09:50.828060: step 10616, loss 0.00285746, acc 1
2016-09-06T02:09:51.638871: step 10617, loss 0.0093851, acc 1
2016-09-06T02:09:52.494564: step 10618, loss 0.0190292, acc 1
2016-09-06T02:09:53.324048: step 10619, loss 0.0160558, acc 1
2016-09-06T02:09:54.128004: step 10620, loss 0.00187332, acc 1
2016-09-06T02:09:54.952286: step 10621, loss 0.0416511, acc 0.98
2016-09-06T02:09:55.757539: step 10622, loss 0.00207455, acc 1
2016-09-06T02:09:56.567707: step 10623, loss 0.00195103, acc 1
2016-09-06T02:09:57.381565: step 10624, loss 0.0193741, acc 1
2016-09-06T02:09:58.180913: step 10625, loss 0.0113064, acc 1
2016-09-06T02:09:58.958273: step 10626, loss 0.00412156, acc 1
2016-09-06T02:09:59.794348: step 10627, loss 0.00339614, acc 1
2016-09-06T02:10:00.630475: step 10628, loss 0.0383422, acc 0.98
2016-09-06T02:10:01.425218: step 10629, loss 0.0408416, acc 0.98
2016-09-06T02:10:02.244305: step 10630, loss 0.120648, acc 0.98
2016-09-06T02:10:03.067656: step 10631, loss 0.0112995, acc 1
2016-09-06T02:10:03.858840: step 10632, loss 0.00873264, acc 1
2016-09-06T02:10:04.647561: step 10633, loss 0.0234637, acc 0.98
2016-09-06T02:10:05.443888: step 10634, loss 0.00249114, acc 1
2016-09-06T02:10:06.260930: step 10635, loss 0.0246366, acc 0.98
2016-09-06T02:10:07.068799: step 10636, loss 0.0181515, acc 1
2016-09-06T02:10:07.867411: step 10637, loss 0.0425372, acc 0.98
2016-09-06T02:10:08.648068: step 10638, loss 0.029299, acc 1
2016-09-06T02:10:09.488174: step 10639, loss 0.00345305, acc 1
2016-09-06T02:10:10.299337: step 10640, loss 0.024202, acc 0.98
2016-09-06T02:10:11.081512: step 10641, loss 0.00463574, acc 1
2016-09-06T02:10:11.890302: step 10642, loss 0.00561277, acc 1
2016-09-06T02:10:12.736733: step 10643, loss 0.0421563, acc 0.98
2016-09-06T02:10:13.530544: step 10644, loss 0.00988028, acc 1
2016-09-06T02:10:14.325708: step 10645, loss 0.0183617, acc 0.98
2016-09-06T02:10:15.173447: step 10646, loss 0.00324765, acc 1
2016-09-06T02:10:15.960257: step 10647, loss 0.0025813, acc 1
2016-09-06T02:10:16.798746: step 10648, loss 0.00556401, acc 1
2016-09-06T02:10:17.602250: step 10649, loss 0.012599, acc 1
2016-09-06T02:10:18.381635: step 10650, loss 0.00239587, acc 1
2016-09-06T02:10:19.182458: step 10651, loss 0.0308076, acc 0.98
2016-09-06T02:10:20.015184: step 10652, loss 0.00242609, acc 1
2016-09-06T02:10:20.788270: step 10653, loss 0.00359724, acc 1
2016-09-06T02:10:21.594761: step 10654, loss 0.0103542, acc 1
2016-09-06T02:10:22.392556: step 10655, loss 0.0134819, acc 1
2016-09-06T02:10:23.176090: step 10656, loss 0.00379771, acc 1
2016-09-06T02:10:23.975873: step 10657, loss 0.00256402, acc 1
2016-09-06T02:10:24.798063: step 10658, loss 0.00799926, acc 1
2016-09-06T02:10:25.590221: step 10659, loss 0.00483741, acc 1
2016-09-06T02:10:26.395873: step 10660, loss 0.0200963, acc 0.98
2016-09-06T02:10:27.205784: step 10661, loss 0.0340319, acc 0.96
2016-09-06T02:10:28.017437: step 10662, loss 0.00501872, acc 1
2016-09-06T02:10:28.813251: step 10663, loss 0.00281558, acc 1
2016-09-06T02:10:29.633336: step 10664, loss 0.00253278, acc 1
2016-09-06T02:10:30.417294: step 10665, loss 0.00475251, acc 1
2016-09-06T02:10:31.216024: step 10666, loss 0.0105072, acc 1
2016-09-06T02:10:32.039901: step 10667, loss 0.0210245, acc 1
2016-09-06T02:10:32.822856: step 10668, loss 0.0694319, acc 0.98
2016-09-06T02:10:33.630216: step 10669, loss 0.0221516, acc 1
2016-09-06T02:10:34.450297: step 10670, loss 0.0542211, acc 0.96
2016-09-06T02:10:35.241209: step 10671, loss 0.00325826, acc 1
2016-09-06T02:10:36.064277: step 10672, loss 0.0295883, acc 0.98
2016-09-06T02:10:36.879974: step 10673, loss 0.00262516, acc 1
2016-09-06T02:10:37.692160: step 10674, loss 0.012503, acc 1
2016-09-06T02:10:38.498944: step 10675, loss 0.0551397, acc 0.98
2016-09-06T02:10:39.342010: step 10676, loss 0.0406224, acc 0.98
2016-09-06T02:10:40.129399: step 10677, loss 0.00438264, acc 1
2016-09-06T02:10:40.921028: step 10678, loss 0.00235437, acc 1
2016-09-06T02:10:41.767230: step 10679, loss 0.0159994, acc 1
2016-09-06T02:10:42.532309: step 10680, loss 0.00509376, acc 1
2016-09-06T02:10:43.357278: step 10681, loss 0.00382136, acc 1
2016-09-06T02:10:44.157440: step 10682, loss 0.0026591, acc 1
2016-09-06T02:10:44.941184: step 10683, loss 0.0160624, acc 1
2016-09-06T02:10:45.752601: step 10684, loss 0.0252479, acc 0.98
2016-09-06T02:10:46.547823: step 10685, loss 0.0332674, acc 0.98
2016-09-06T02:10:47.357093: step 10686, loss 0.00219873, acc 1
2016-09-06T02:10:48.185822: step 10687, loss 0.0181347, acc 1
2016-09-06T02:10:48.984349: step 10688, loss 0.00518332, acc 1
2016-09-06T02:10:49.787480: step 10689, loss 0.00840565, acc 1
2016-09-06T02:10:50.608290: step 10690, loss 0.0282615, acc 0.98
2016-09-06T02:10:51.435301: step 10691, loss 0.0079578, acc 1
2016-09-06T02:10:52.215239: step 10692, loss 0.0563126, acc 0.98
2016-09-06T02:10:53.034068: step 10693, loss 0.00227081, acc 1
2016-09-06T02:10:53.824471: step 10694, loss 0.025513, acc 0.98
2016-09-06T02:10:54.609859: step 10695, loss 0.00622476, acc 1
2016-09-06T02:10:55.419442: step 10696, loss 0.0022904, acc 1
2016-09-06T02:10:56.240459: step 10697, loss 0.017365, acc 0.98
2016-09-06T02:10:57.029800: step 10698, loss 0.00346886, acc 1
2016-09-06T02:10:57.841115: step 10699, loss 0.00212458, acc 1
2016-09-06T02:10:58.675690: step 10700, loss 0.00272417, acc 1

Evaluation:
2016-09-06T02:11:02.389022: step 10700, loss 2.64349, acc 0.712946

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-10700

2016-09-06T02:11:04.270350: step 10701, loss 0.0367731, acc 0.96
2016-09-06T02:11:05.095715: step 10702, loss 0.016277, acc 1
2016-09-06T02:11:05.925426: step 10703, loss 0.0342205, acc 0.98
2016-09-06T02:11:06.719022: step 10704, loss 0.0371353, acc 0.96
2016-09-06T02:11:07.550930: step 10705, loss 0.00351468, acc 1
2016-09-06T02:11:08.358020: step 10706, loss 0.0287505, acc 0.98
2016-09-06T02:11:09.136089: step 10707, loss 0.0351209, acc 1
2016-09-06T02:11:09.970848: step 10708, loss 0.0175671, acc 0.98
2016-09-06T02:11:10.776306: step 10709, loss 0.016019, acc 1
2016-09-06T02:11:11.568299: step 10710, loss 0.020077, acc 0.98
2016-09-06T02:11:12.379066: step 10711, loss 0.00205784, acc 1
2016-09-06T02:11:13.212016: step 10712, loss 0.00210314, acc 1
2016-09-06T02:11:14.009946: step 10713, loss 0.0251948, acc 1
2016-09-06T02:11:14.817073: step 10714, loss 0.00780304, acc 1
2016-09-06T02:11:15.665240: step 10715, loss 0.0807233, acc 0.98
2016-09-06T02:11:16.453595: step 10716, loss 0.0174484, acc 1
2016-09-06T02:11:17.267949: step 10717, loss 0.0321372, acc 0.96
2016-09-06T02:11:18.080428: step 10718, loss 0.0038219, acc 1
2016-09-06T02:11:18.870496: step 10719, loss 0.0215657, acc 0.98
2016-09-06T02:11:19.670425: step 10720, loss 0.0266315, acc 1
2016-09-06T02:11:20.471361: step 10721, loss 0.0291144, acc 0.98
2016-09-06T02:11:21.244205: step 10722, loss 0.00535814, acc 1
2016-09-06T02:11:22.075013: step 10723, loss 0.0196855, acc 0.98
2016-09-06T02:11:22.887796: step 10724, loss 0.00201419, acc 1
2016-09-06T02:11:23.684176: step 10725, loss 0.0144892, acc 1
2016-09-06T02:11:24.483896: step 10726, loss 0.0222698, acc 0.98
2016-09-06T02:11:25.287508: step 10727, loss 0.0221423, acc 0.98
2016-09-06T02:11:26.085409: step 10728, loss 0.0625785, acc 0.98
2016-09-06T02:11:26.857376: step 10729, loss 0.0275775, acc 0.98
2016-09-06T02:11:27.673601: step 10730, loss 0.0142781, acc 1
2016-09-06T02:11:28.454907: step 10731, loss 0.0345882, acc 0.98
2016-09-06T02:11:29.270129: step 10732, loss 0.00340614, acc 1
2016-09-06T02:11:30.091103: step 10733, loss 0.0196311, acc 1
2016-09-06T02:11:30.887188: step 10734, loss 0.0158288, acc 1
2016-09-06T02:11:31.682763: step 10735, loss 0.0106878, acc 1
2016-09-06T02:11:32.498458: step 10736, loss 0.0294903, acc 0.98
2016-09-06T02:11:33.286440: step 10737, loss 0.0286175, acc 1
2016-09-06T02:11:34.093086: step 10738, loss 0.0331663, acc 1
2016-09-06T02:11:34.905506: step 10739, loss 0.00672058, acc 1
2016-09-06T02:11:35.691446: step 10740, loss 0.0584412, acc 0.96
2016-09-06T02:11:36.496258: step 10741, loss 0.0320567, acc 0.98
2016-09-06T02:11:37.320249: step 10742, loss 0.0114604, acc 1
2016-09-06T02:11:38.122069: step 10743, loss 0.00241186, acc 1
2016-09-06T02:11:38.918342: step 10744, loss 0.0433807, acc 0.98
2016-09-06T02:11:39.718729: step 10745, loss 0.00207665, acc 1
2016-09-06T02:11:40.507231: step 10746, loss 0.0211781, acc 1
2016-09-06T02:11:41.335131: step 10747, loss 0.00226962, acc 1
2016-09-06T02:11:42.148242: step 10748, loss 0.00709027, acc 1
2016-09-06T02:11:42.945459: step 10749, loss 0.0521369, acc 0.98
2016-09-06T02:11:43.740593: step 10750, loss 0.0163423, acc 1
2016-09-06T02:11:44.505700: step 10751, loss 0.0290587, acc 0.98
2016-09-06T02:11:45.253314: step 10752, loss 0.00338136, acc 1
2016-09-06T02:11:46.088100: step 10753, loss 0.0392572, acc 0.98
2016-09-06T02:11:46.917311: step 10754, loss 0.0428225, acc 0.98
2016-09-06T02:11:47.717400: step 10755, loss 0.0228129, acc 0.98
2016-09-06T02:11:48.534834: step 10756, loss 0.00207836, acc 1
2016-09-06T02:11:49.333880: step 10757, loss 0.00240445, acc 1
2016-09-06T02:11:50.144024: step 10758, loss 0.0120541, acc 1
2016-09-06T02:11:50.982762: step 10759, loss 0.00389328, acc 1
2016-09-06T02:11:51.796012: step 10760, loss 0.0188821, acc 1
2016-09-06T02:11:52.593566: step 10761, loss 0.0279638, acc 0.98
2016-09-06T02:11:53.401547: step 10762, loss 0.00203351, acc 1
2016-09-06T02:11:54.184991: step 10763, loss 0.00633932, acc 1
2016-09-06T02:11:54.985282: step 10764, loss 0.00208142, acc 1
2016-09-06T02:11:55.825121: step 10765, loss 0.0496904, acc 0.98
2016-09-06T02:11:56.660419: step 10766, loss 0.00247225, acc 1
2016-09-06T02:11:57.440529: step 10767, loss 0.0162886, acc 1
2016-09-06T02:11:58.259324: step 10768, loss 0.0332894, acc 0.98
2016-09-06T02:11:59.117122: step 10769, loss 0.0167943, acc 1
2016-09-06T02:11:59.910860: step 10770, loss 0.00798538, acc 1
2016-09-06T02:12:00.753416: step 10771, loss 0.00231535, acc 1
2016-09-06T02:12:01.573748: step 10772, loss 0.00406855, acc 1
2016-09-06T02:12:02.375197: step 10773, loss 0.0256357, acc 0.98
2016-09-06T02:12:03.172883: step 10774, loss 0.00482261, acc 1
2016-09-06T02:12:03.977855: step 10775, loss 0.00853983, acc 1
2016-09-06T02:12:04.765404: step 10776, loss 0.00236292, acc 1
2016-09-06T02:12:05.556577: step 10777, loss 0.00223955, acc 1
2016-09-06T02:12:06.385902: step 10778, loss 0.025976, acc 1
2016-09-06T02:12:07.166396: step 10779, loss 0.00477489, acc 1
2016-09-06T02:12:07.994892: step 10780, loss 0.0206588, acc 0.98
2016-09-06T02:12:08.781376: step 10781, loss 0.0306877, acc 0.98
2016-09-06T02:12:09.568674: step 10782, loss 0.023037, acc 1
2016-09-06T02:12:10.364121: step 10783, loss 0.00830142, acc 1
2016-09-06T02:12:11.203638: step 10784, loss 0.00193394, acc 1
2016-09-06T02:12:12.021877: step 10785, loss 0.0492209, acc 0.96
2016-09-06T02:12:12.826111: step 10786, loss 0.0413132, acc 0.96
2016-09-06T02:12:13.621019: step 10787, loss 0.00214728, acc 1
2016-09-06T02:12:14.410321: step 10788, loss 0.0254299, acc 0.98
2016-09-06T02:12:15.212430: step 10789, loss 0.00214789, acc 1
2016-09-06T02:12:16.016144: step 10790, loss 0.0453599, acc 0.98
2016-09-06T02:12:16.818878: step 10791, loss 0.0188997, acc 0.98
2016-09-06T02:12:17.603638: step 10792, loss 0.0023083, acc 1
2016-09-06T02:12:18.397726: step 10793, loss 0.0358638, acc 0.98
2016-09-06T02:12:19.241386: step 10794, loss 0.00988625, acc 1
2016-09-06T02:12:20.063749: step 10795, loss 0.0112743, acc 1
2016-09-06T02:12:20.908126: step 10796, loss 0.00183614, acc 1
2016-09-06T02:12:21.689394: step 10797, loss 0.00194867, acc 1
2016-09-06T02:12:22.467498: step 10798, loss 0.0332147, acc 0.98
2016-09-06T02:12:23.294450: step 10799, loss 0.0102289, acc 1
2016-09-06T02:12:24.082153: step 10800, loss 0.00965401, acc 1

Evaluation:
2016-09-06T02:12:27.821439: step 10800, loss 2.97219, acc 0.706379

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-10800

2016-09-06T02:12:29.688102: step 10801, loss 0.0111167, acc 1
2016-09-06T02:12:30.492974: step 10802, loss 0.00195989, acc 1
2016-09-06T02:12:31.297907: step 10803, loss 0.00198965, acc 1
2016-09-06T02:12:32.124879: step 10804, loss 0.017228, acc 1
2016-09-06T02:12:32.951102: step 10805, loss 0.0364445, acc 0.98
2016-09-06T02:12:33.764015: step 10806, loss 0.0160432, acc 0.98
2016-09-06T02:12:34.615347: step 10807, loss 0.0430508, acc 0.96
2016-09-06T02:12:35.422326: step 10808, loss 0.0400906, acc 0.98
2016-09-06T02:12:36.236061: step 10809, loss 0.013994, acc 1
2016-09-06T02:12:37.050325: step 10810, loss 0.0191922, acc 0.98
2016-09-06T02:12:37.837640: step 10811, loss 0.0160567, acc 1
2016-09-06T02:12:38.619806: step 10812, loss 0.0142661, acc 1
2016-09-06T02:12:39.466606: step 10813, loss 0.0227858, acc 1
2016-09-06T02:12:40.289456: step 10814, loss 0.00470914, acc 1
2016-09-06T02:12:41.088503: step 10815, loss 0.00254339, acc 1
2016-09-06T02:12:41.884364: step 10816, loss 0.0390063, acc 0.96
2016-09-06T02:12:42.697463: step 10817, loss 0.00177496, acc 1
2016-09-06T02:12:43.515722: step 10818, loss 0.0377151, acc 0.98
2016-09-06T02:12:44.337016: step 10819, loss 0.0352338, acc 0.96
2016-09-06T02:12:45.186388: step 10820, loss 0.027151, acc 0.98
2016-09-06T02:12:45.969675: step 10821, loss 0.0199114, acc 1
2016-09-06T02:12:46.784252: step 10822, loss 0.00176985, acc 1
2016-09-06T02:12:47.606698: step 10823, loss 0.00363916, acc 1
2016-09-06T02:12:48.399483: step 10824, loss 0.00239987, acc 1
2016-09-06T02:12:49.191233: step 10825, loss 0.0214498, acc 1
2016-09-06T02:12:50.010011: step 10826, loss 0.00336317, acc 1
2016-09-06T02:12:50.794048: step 10827, loss 0.0157792, acc 1
2016-09-06T02:12:51.648045: step 10828, loss 0.0154732, acc 1
2016-09-06T02:12:52.477755: step 10829, loss 0.00346506, acc 1
2016-09-06T02:12:53.280817: step 10830, loss 0.0193799, acc 1
2016-09-06T02:12:54.088787: step 10831, loss 0.0254123, acc 0.98
2016-09-06T02:12:54.924140: step 10832, loss 0.0106494, acc 1
2016-09-06T02:12:55.719409: step 10833, loss 0.00175957, acc 1
2016-09-06T02:12:56.509985: step 10834, loss 0.0133926, acc 1
2016-09-06T02:12:57.318370: step 10835, loss 0.0595408, acc 0.96
2016-09-06T02:12:58.132042: step 10836, loss 0.00326542, acc 1
2016-09-06T02:12:58.926833: step 10837, loss 0.00803356, acc 1
2016-09-06T02:12:59.764829: step 10838, loss 0.00497261, acc 1
2016-09-06T02:13:00.585550: step 10839, loss 0.00165806, acc 1
2016-09-06T02:13:01.388370: step 10840, loss 0.0412275, acc 0.98
2016-09-06T02:13:02.219515: step 10841, loss 0.00171405, acc 1
2016-09-06T02:13:03.021454: step 10842, loss 0.00167505, acc 1
2016-09-06T02:13:03.834666: step 10843, loss 0.0240209, acc 1
2016-09-06T02:13:04.642094: step 10844, loss 0.00235055, acc 1
2016-09-06T02:13:05.447806: step 10845, loss 0.0202389, acc 1
2016-09-06T02:13:06.240539: step 10846, loss 0.00231372, acc 1
2016-09-06T02:13:07.055694: step 10847, loss 0.00362304, acc 1
2016-09-06T02:13:07.879799: step 10848, loss 0.00538696, acc 1
2016-09-06T02:13:08.695062: step 10849, loss 0.00179938, acc 1
2016-09-06T02:13:09.515901: step 10850, loss 0.00962755, acc 1
2016-09-06T02:13:10.325480: step 10851, loss 0.0022762, acc 1
2016-09-06T02:13:11.123143: step 10852, loss 0.032462, acc 0.98
2016-09-06T02:13:11.956465: step 10853, loss 0.00475213, acc 1
2016-09-06T02:13:12.772743: step 10854, loss 0.0161105, acc 0.98
2016-09-06T02:13:13.597452: step 10855, loss 0.00204394, acc 1
2016-09-06T02:13:14.429486: step 10856, loss 0.00710043, acc 1
2016-09-06T02:13:15.266666: step 10857, loss 0.0185945, acc 0.98
2016-09-06T02:13:16.067710: step 10858, loss 0.00569269, acc 1
2016-09-06T02:13:16.918873: step 10859, loss 0.0127842, acc 1
2016-09-06T02:13:17.738385: step 10860, loss 0.0195909, acc 0.98
2016-09-06T02:13:18.559066: step 10861, loss 0.0358221, acc 0.96
2016-09-06T02:13:19.405373: step 10862, loss 0.00273307, acc 1
2016-09-06T02:13:20.222088: step 10863, loss 0.0026724, acc 1
2016-09-06T02:13:21.018607: step 10864, loss 0.0399049, acc 0.98
2016-09-06T02:13:21.803087: step 10865, loss 0.0116336, acc 1
2016-09-06T02:13:22.618262: step 10866, loss 0.010671, acc 1
2016-09-06T02:13:23.401654: step 10867, loss 0.0114271, acc 1
2016-09-06T02:13:24.201813: step 10868, loss 0.00511517, acc 1
2016-09-06T02:13:24.998357: step 10869, loss 0.00233102, acc 1
2016-09-06T02:13:25.848926: step 10870, loss 0.0106508, acc 1
2016-09-06T02:13:26.658989: step 10871, loss 0.0063157, acc 1
2016-09-06T02:13:27.464320: step 10872, loss 0.00807033, acc 1
2016-09-06T02:13:28.287244: step 10873, loss 0.00236303, acc 1
2016-09-06T02:13:29.080288: step 10874, loss 0.00646465, acc 1
2016-09-06T02:13:29.912370: step 10875, loss 0.0139086, acc 1
2016-09-06T02:13:30.708251: step 10876, loss 0.0113864, acc 1
2016-09-06T02:13:31.548696: step 10877, loss 0.00744734, acc 1
2016-09-06T02:13:32.354456: step 10878, loss 0.0152354, acc 1
2016-09-06T02:13:33.160769: step 10879, loss 0.0221444, acc 1
2016-09-06T02:13:33.981807: step 10880, loss 0.0117677, acc 1
2016-09-06T02:13:34.848679: step 10881, loss 0.00575838, acc 1
2016-09-06T02:13:35.657036: step 10882, loss 0.00264793, acc 1
2016-09-06T02:13:36.465200: step 10883, loss 0.00225704, acc 1
2016-09-06T02:13:37.296138: step 10884, loss 0.00301412, acc 1
2016-09-06T02:13:38.131399: step 10885, loss 0.035649, acc 0.98
2016-09-06T02:13:38.950036: step 10886, loss 0.00420843, acc 1
2016-09-06T02:13:39.804442: step 10887, loss 0.094896, acc 0.98
2016-09-06T02:13:40.619791: step 10888, loss 0.0141683, acc 1
2016-09-06T02:13:41.428923: step 10889, loss 0.0324253, acc 0.98
2016-09-06T02:13:42.252153: step 10890, loss 0.0076353, acc 1
2016-09-06T02:13:43.051257: step 10891, loss 0.00206289, acc 1
2016-09-06T02:13:43.852004: step 10892, loss 0.00207905, acc 1
2016-09-06T02:13:44.700458: step 10893, loss 0.0387752, acc 0.98
2016-09-06T02:13:45.500419: step 10894, loss 0.00799391, acc 1
2016-09-06T02:13:46.327393: step 10895, loss 0.00206457, acc 1
2016-09-06T02:13:47.178089: step 10896, loss 0.0249492, acc 0.98
2016-09-06T02:13:47.998147: step 10897, loss 0.00208181, acc 1
2016-09-06T02:13:48.770820: step 10898, loss 0.00809765, acc 1
2016-09-06T02:13:49.582728: step 10899, loss 0.0332661, acc 0.98
2016-09-06T02:13:50.422561: step 10900, loss 0.0142591, acc 1

Evaluation:
2016-09-06T02:13:54.179263: step 10900, loss 3.14391, acc 0.705441

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-10900

2016-09-06T02:13:56.113402: step 10901, loss 0.0207716, acc 1
2016-09-06T02:13:56.952080: step 10902, loss 0.047133, acc 0.96
2016-09-06T02:13:57.751049: step 10903, loss 0.00272778, acc 1
2016-09-06T02:13:58.537409: step 10904, loss 0.00707563, acc 1
2016-09-06T02:13:59.354430: step 10905, loss 0.00588582, acc 1
2016-09-06T02:14:00.165351: step 10906, loss 0.00392982, acc 1
2016-09-06T02:14:01.019256: step 10907, loss 0.00268465, acc 1
2016-09-06T02:14:01.804973: step 10908, loss 0.0514041, acc 0.98
2016-09-06T02:14:02.619750: step 10909, loss 0.0210462, acc 0.98
2016-09-06T02:14:03.434577: step 10910, loss 0.00183485, acc 1
2016-09-06T02:14:04.274318: step 10911, loss 0.00751706, acc 1
2016-09-06T02:14:05.091988: step 10912, loss 0.12752, acc 0.98
2016-09-06T02:14:05.882775: step 10913, loss 0.0475482, acc 0.96
2016-09-06T02:14:06.690353: step 10914, loss 0.04033, acc 0.98
2016-09-06T02:14:07.528825: step 10915, loss 0.025972, acc 0.98
2016-09-06T02:14:08.320694: step 10916, loss 0.00218443, acc 1
2016-09-06T02:14:09.125234: step 10917, loss 0.0028829, acc 1
2016-09-06T02:14:09.957122: step 10918, loss 0.0280751, acc 0.98
2016-09-06T02:14:10.773816: step 10919, loss 0.00158184, acc 1
2016-09-06T02:14:11.582956: step 10920, loss 0.0108795, acc 1
2016-09-06T02:14:12.389687: step 10921, loss 0.0574703, acc 0.96
2016-09-06T02:14:13.185341: step 10922, loss 0.00222969, acc 1
2016-09-06T02:14:13.994601: step 10923, loss 0.00551447, acc 1
2016-09-06T02:14:14.805266: step 10924, loss 0.00997672, acc 1
2016-09-06T02:14:15.601638: step 10925, loss 0.00630163, acc 1
2016-09-06T02:14:16.401373: step 10926, loss 0.0154587, acc 1
2016-09-06T02:14:17.264952: step 10927, loss 0.0343328, acc 0.96
2016-09-06T02:14:18.070632: step 10928, loss 0.00926902, acc 1
2016-09-06T02:14:18.885080: step 10929, loss 0.011269, acc 1
2016-09-06T02:14:19.709227: step 10930, loss 0.0248191, acc 1
2016-09-06T02:14:20.520609: step 10931, loss 0.0353552, acc 0.98
2016-09-06T02:14:21.308831: step 10932, loss 0.123255, acc 0.94
2016-09-06T02:14:22.143999: step 10933, loss 0.00335865, acc 1
2016-09-06T02:14:22.957132: step 10934, loss 0.17222, acc 0.98
2016-09-06T02:14:23.771742: step 10935, loss 0.00225729, acc 1
2016-09-06T02:14:24.608587: step 10936, loss 0.00246969, acc 1
2016-09-06T02:14:25.451710: step 10937, loss 0.0131265, acc 1
2016-09-06T02:14:26.271669: step 10938, loss 0.0350687, acc 0.98
2016-09-06T02:14:27.088902: step 10939, loss 0.0234811, acc 1
2016-09-06T02:14:27.908408: step 10940, loss 0.0112361, acc 1
2016-09-06T02:14:28.713782: step 10941, loss 0.01673, acc 1
2016-09-06T02:14:29.530345: step 10942, loss 0.00643131, acc 1
2016-09-06T02:14:30.327905: step 10943, loss 0.00811968, acc 1
2016-09-06T02:14:31.076721: step 10944, loss 0.00271381, acc 1
2016-09-06T02:14:31.911020: step 10945, loss 0.0109982, acc 1
2016-09-06T02:14:32.724160: step 10946, loss 0.0223266, acc 1
2016-09-06T02:14:33.559298: step 10947, loss 0.00489063, acc 1
2016-09-06T02:14:34.374552: step 10948, loss 0.0223669, acc 1
2016-09-06T02:14:35.211043: step 10949, loss 0.0206398, acc 0.98
2016-09-06T02:14:36.063168: step 10950, loss 0.00509065, acc 1
2016-09-06T02:14:36.892844: step 10951, loss 0.00465529, acc 1
2016-09-06T02:14:37.713485: step 10952, loss 0.00267179, acc 1
2016-09-06T02:14:38.495988: step 10953, loss 0.02109, acc 0.98
2016-09-06T02:14:39.311180: step 10954, loss 0.0032226, acc 1
2016-09-06T02:14:40.127197: step 10955, loss 0.0318434, acc 1
2016-09-06T02:14:40.910830: step 10956, loss 0.0162884, acc 1
2016-09-06T02:14:41.734369: step 10957, loss 0.00283618, acc 1
2016-09-06T02:14:42.544645: step 10958, loss 0.00304961, acc 1
2016-09-06T02:14:43.337124: step 10959, loss 0.0127188, acc 1
2016-09-06T02:14:44.146234: step 10960, loss 0.0447804, acc 0.96
2016-09-06T02:14:44.990457: step 10961, loss 0.0029063, acc 1
2016-09-06T02:14:45.782606: step 10962, loss 0.013428, acc 1
2016-09-06T02:14:46.613424: step 10963, loss 0.00356597, acc 1
2016-09-06T02:14:47.401650: step 10964, loss 0.0110219, acc 1
2016-09-06T02:14:48.202877: step 10965, loss 0.00446081, acc 1
2016-09-06T02:14:49.029128: step 10966, loss 0.0393434, acc 0.98
2016-09-06T02:14:49.845059: step 10967, loss 0.0225905, acc 1
2016-09-06T02:14:50.619707: step 10968, loss 0.00375868, acc 1
2016-09-06T02:14:51.441825: step 10969, loss 0.003686, acc 1
2016-09-06T02:14:52.280021: step 10970, loss 0.0318654, acc 0.98
2016-09-06T02:14:53.080451: step 10971, loss 0.0726599, acc 0.98
2016-09-06T02:14:53.902026: step 10972, loss 0.0030901, acc 1
2016-09-06T02:14:54.729397: step 10973, loss 0.00437432, acc 1
2016-09-06T02:14:55.519827: step 10974, loss 0.0036908, acc 1
2016-09-06T02:14:56.319137: step 10975, loss 0.00325008, acc 1
2016-09-06T02:14:57.130462: step 10976, loss 0.0104439, acc 1
2016-09-06T02:14:57.943123: step 10977, loss 0.0154157, acc 1
2016-09-06T02:14:58.749974: step 10978, loss 0.0139035, acc 1
2016-09-06T02:14:59.579019: step 10979, loss 0.00625508, acc 1
2016-09-06T02:15:00.374638: step 10980, loss 0.00470489, acc 1
2016-09-06T02:15:01.199806: step 10981, loss 0.00272174, acc 1
2016-09-06T02:15:02.025385: step 10982, loss 0.0180179, acc 0.98
2016-09-06T02:15:02.812229: step 10983, loss 0.0136565, acc 1
2016-09-06T02:15:03.654542: step 10984, loss 0.00309064, acc 1
2016-09-06T02:15:04.495300: step 10985, loss 0.0251284, acc 0.98
2016-09-06T02:15:05.284949: step 10986, loss 0.0188977, acc 0.98
2016-09-06T02:15:06.104399: step 10987, loss 0.0730413, acc 0.98
2016-09-06T02:15:06.909655: step 10988, loss 0.0875434, acc 0.96
2016-09-06T02:15:07.720453: step 10989, loss 0.0165945, acc 0.98
2016-09-06T02:15:08.536114: step 10990, loss 0.0154957, acc 1
2016-09-06T02:15:09.354648: step 10991, loss 0.0993617, acc 0.98
2016-09-06T02:15:10.164545: step 10992, loss 0.00610629, acc 1
2016-09-06T02:15:10.982255: step 10993, loss 0.0225214, acc 0.98
2016-09-06T02:15:11.813437: step 10994, loss 0.00230673, acc 1
2016-09-06T02:15:12.620296: step 10995, loss 0.00277547, acc 1
2016-09-06T02:15:13.420838: step 10996, loss 0.0329435, acc 0.98
2016-09-06T02:15:14.252780: step 10997, loss 0.0148078, acc 1
2016-09-06T02:15:15.142308: step 10998, loss 0.00515427, acc 1
2016-09-06T02:15:15.964301: step 10999, loss 0.0143735, acc 1
2016-09-06T02:15:16.800656: step 11000, loss 0.0120709, acc 1

Evaluation:
2016-09-06T02:15:20.569308: step 11000, loss 2.371, acc 0.709193

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-11000

2016-09-06T02:15:22.412630: step 11001, loss 0.00614232, acc 1
2016-09-06T02:15:23.215406: step 11002, loss 0.0574623, acc 0.98
2016-09-06T02:15:24.043741: step 11003, loss 0.00612188, acc 1
2016-09-06T02:15:24.857828: step 11004, loss 0.0028968, acc 1
2016-09-06T02:15:25.675441: step 11005, loss 0.00295834, acc 1
2016-09-06T02:15:26.505840: step 11006, loss 0.00718854, acc 1
2016-09-06T02:15:27.333405: step 11007, loss 0.0290218, acc 0.98
2016-09-06T02:15:28.127865: step 11008, loss 0.0639962, acc 0.96
2016-09-06T02:15:28.968754: step 11009, loss 0.032049, acc 0.98
2016-09-06T02:15:29.778573: step 11010, loss 0.00945277, acc 1
2016-09-06T02:15:30.581389: step 11011, loss 0.0279463, acc 0.98
2016-09-06T02:15:31.418072: step 11012, loss 0.00863868, acc 1
2016-09-06T02:15:32.247353: step 11013, loss 0.00271086, acc 1
2016-09-06T02:15:33.020835: step 11014, loss 0.00285718, acc 1
2016-09-06T02:15:33.836864: step 11015, loss 0.00677713, acc 1
2016-09-06T02:15:34.660277: step 11016, loss 0.00294177, acc 1
2016-09-06T02:15:35.432218: step 11017, loss 0.0057565, acc 1
2016-09-06T02:15:36.247698: step 11018, loss 0.0465592, acc 0.98
2016-09-06T02:15:37.038078: step 11019, loss 0.0192033, acc 0.98
2016-09-06T02:15:37.839220: step 11020, loss 0.0151183, acc 1
2016-09-06T02:15:38.669817: step 11021, loss 0.00898308, acc 1
2016-09-06T02:15:39.517907: step 11022, loss 0.0357413, acc 0.98
2016-09-06T02:15:40.324108: step 11023, loss 0.00792013, acc 1
2016-09-06T02:15:41.176522: step 11024, loss 0.00309036, acc 1
2016-09-06T02:15:42.011773: step 11025, loss 0.00299581, acc 1
2016-09-06T02:15:42.835650: step 11026, loss 0.019004, acc 0.98
2016-09-06T02:15:43.640207: step 11027, loss 0.0265591, acc 0.98
2016-09-06T02:15:44.482200: step 11028, loss 0.00343851, acc 1
2016-09-06T02:15:45.287368: step 11029, loss 0.00453021, acc 1
2016-09-06T02:15:46.084144: step 11030, loss 0.0177657, acc 1
2016-09-06T02:15:46.902753: step 11031, loss 0.0111648, acc 1
2016-09-06T02:15:47.714702: step 11032, loss 0.00927645, acc 1
2016-09-06T02:15:48.550803: step 11033, loss 0.0296104, acc 0.98
2016-09-06T02:15:49.401779: step 11034, loss 0.0029927, acc 1
2016-09-06T02:15:50.188917: step 11035, loss 0.00754606, acc 1
2016-09-06T02:15:51.005480: step 11036, loss 0.00290916, acc 1
2016-09-06T02:15:51.832550: step 11037, loss 0.0193883, acc 0.98
2016-09-06T02:15:52.636641: step 11038, loss 0.0035181, acc 1
2016-09-06T02:15:53.439743: step 11039, loss 0.00372166, acc 1
2016-09-06T02:15:54.270214: step 11040, loss 0.00258033, acc 1
2016-09-06T02:15:55.075637: step 11041, loss 0.0112635, acc 1
2016-09-06T02:15:55.865321: step 11042, loss 0.0356077, acc 0.98
2016-09-06T02:15:56.681134: step 11043, loss 0.0225648, acc 0.98
2016-09-06T02:15:57.499748: step 11044, loss 0.0159471, acc 1
2016-09-06T02:15:58.312882: step 11045, loss 0.0170401, acc 0.98
2016-09-06T02:15:59.165400: step 11046, loss 0.0436738, acc 0.98
2016-09-06T02:15:59.963121: step 11047, loss 0.00242117, acc 1
2016-09-06T02:16:00.837393: step 11048, loss 0.0023849, acc 1
2016-09-06T02:16:01.634937: step 11049, loss 0.109275, acc 0.94
2016-09-06T02:16:02.422421: step 11050, loss 0.0300719, acc 0.98
2016-09-06T02:16:03.244732: step 11051, loss 0.0294481, acc 0.98
2016-09-06T02:16:04.050814: step 11052, loss 0.01085, acc 1
2016-09-06T02:16:04.864459: step 11053, loss 0.0346446, acc 0.98
2016-09-06T02:16:05.649112: step 11054, loss 0.0444878, acc 0.98
2016-09-06T02:16:06.451297: step 11055, loss 0.00319731, acc 1
2016-09-06T02:16:07.251819: step 11056, loss 0.031085, acc 1
2016-09-06T02:16:08.065814: step 11057, loss 0.00569106, acc 1
2016-09-06T02:16:08.875774: step 11058, loss 0.00726821, acc 1
2016-09-06T02:16:09.729405: step 11059, loss 0.0374964, acc 0.98
2016-09-06T02:16:10.496377: step 11060, loss 0.00230881, acc 1
2016-09-06T02:16:11.284825: step 11061, loss 0.03259, acc 0.98
2016-09-06T02:16:12.081613: step 11062, loss 0.00222542, acc 1
2016-09-06T02:16:12.882581: step 11063, loss 0.0171087, acc 1
2016-09-06T02:16:13.771564: step 11064, loss 0.0106358, acc 1
2016-09-06T02:16:14.591122: step 11065, loss 0.00220672, acc 1
2016-09-06T02:16:15.378027: step 11066, loss 0.00202486, acc 1
2016-09-06T02:16:16.173565: step 11067, loss 0.00398035, acc 1
2016-09-06T02:16:16.966823: step 11068, loss 0.0725247, acc 0.98
2016-09-06T02:16:17.752258: step 11069, loss 0.00831211, acc 1
2016-09-06T02:16:18.559737: step 11070, loss 0.00446555, acc 1
2016-09-06T02:16:19.373469: step 11071, loss 0.0041204, acc 1
2016-09-06T02:16:20.157010: step 11072, loss 0.0159787, acc 1
2016-09-06T02:16:20.967311: step 11073, loss 0.0891514, acc 0.98
2016-09-06T02:16:21.785084: step 11074, loss 0.0174189, acc 1
2016-09-06T02:16:22.574103: step 11075, loss 0.0019046, acc 1
2016-09-06T02:16:23.366492: step 11076, loss 0.00228334, acc 1
2016-09-06T02:16:24.210228: step 11077, loss 0.013477, acc 1
2016-09-06T02:16:25.027407: step 11078, loss 0.0077937, acc 1
2016-09-06T02:16:25.813362: step 11079, loss 0.0158707, acc 0.98
2016-09-06T02:16:26.638435: step 11080, loss 0.0172918, acc 0.98
2016-09-06T02:16:27.412630: step 11081, loss 0.00833183, acc 1
2016-09-06T02:16:28.203089: step 11082, loss 0.0391537, acc 0.96
2016-09-06T02:16:29.021228: step 11083, loss 0.0131038, acc 1
2016-09-06T02:16:29.813022: step 11084, loss 0.00328328, acc 1
2016-09-06T02:16:30.633937: step 11085, loss 0.0463479, acc 0.96
2016-09-06T02:16:31.458740: step 11086, loss 0.00391662, acc 1
2016-09-06T02:16:32.253275: step 11087, loss 0.0138713, acc 1
2016-09-06T02:16:33.056956: step 11088, loss 0.00438333, acc 1
2016-09-06T02:16:33.895185: step 11089, loss 0.0487905, acc 0.98
2016-09-06T02:16:34.676013: step 11090, loss 0.0102885, acc 1
2016-09-06T02:16:35.475169: step 11091, loss 0.00849115, acc 1
2016-09-06T02:16:36.292759: step 11092, loss 0.0205514, acc 0.98
2016-09-06T02:16:37.085131: step 11093, loss 0.0303639, acc 0.98
2016-09-06T02:16:37.928871: step 11094, loss 0.0442247, acc 0.98
2016-09-06T02:16:38.736975: step 11095, loss 0.0230949, acc 0.98
2016-09-06T02:16:39.520049: step 11096, loss 0.00465597, acc 1
2016-09-06T02:16:40.311195: step 11097, loss 0.0134186, acc 1
2016-09-06T02:16:41.153846: step 11098, loss 0.00927563, acc 1
2016-09-06T02:16:41.918928: step 11099, loss 0.00514054, acc 1
2016-09-06T02:16:42.705797: step 11100, loss 0.099769, acc 0.96

Evaluation:
2016-09-06T02:16:46.391099: step 11100, loss 2.30054, acc 0.712008

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-11100

2016-09-06T02:16:48.343231: step 11101, loss 0.00861053, acc 1
2016-09-06T02:16:49.140996: step 11102, loss 0.00471827, acc 1
2016-09-06T02:16:49.970450: step 11103, loss 0.0104925, acc 1
2016-09-06T02:16:50.780328: step 11104, loss 0.0429029, acc 0.98
2016-09-06T02:16:51.598165: step 11105, loss 0.0331784, acc 0.98
2016-09-06T02:16:52.433822: step 11106, loss 0.00757191, acc 1
2016-09-06T02:16:53.280868: step 11107, loss 0.0155709, acc 1
2016-09-06T02:16:54.082390: step 11108, loss 0.00276711, acc 1
2016-09-06T02:16:54.887520: step 11109, loss 0.00249361, acc 1
2016-09-06T02:16:55.700864: step 11110, loss 0.0285764, acc 0.98
2016-09-06T02:16:56.525759: step 11111, loss 0.0233591, acc 0.98
2016-09-06T02:16:57.320360: step 11112, loss 0.00719291, acc 1
2016-09-06T02:16:58.146937: step 11113, loss 0.0129274, acc 1
2016-09-06T02:16:58.913683: step 11114, loss 0.00232894, acc 1
2016-09-06T02:16:59.728106: step 11115, loss 0.016101, acc 0.98
2016-09-06T02:17:00.588551: step 11116, loss 0.0127809, acc 1
2016-09-06T02:17:01.378080: step 11117, loss 0.0148331, acc 1
2016-09-06T02:17:02.175331: step 11118, loss 0.0206574, acc 0.98
2016-09-06T02:17:02.997681: step 11119, loss 0.00691136, acc 1
2016-09-06T02:17:03.779795: step 11120, loss 0.0693945, acc 0.98
2016-09-06T02:17:04.594345: step 11121, loss 0.0274466, acc 0.98
2016-09-06T02:17:05.408930: step 11122, loss 0.0885901, acc 0.96
2016-09-06T02:17:06.193454: step 11123, loss 0.0597251, acc 0.96
2016-09-06T02:17:07.018660: step 11124, loss 0.0691632, acc 0.96
2016-09-06T02:17:07.853402: step 11125, loss 0.0021165, acc 1
2016-09-06T02:17:08.621838: step 11126, loss 0.00682017, acc 1
2016-09-06T02:17:09.410687: step 11127, loss 0.00238591, acc 1
2016-09-06T02:17:10.238294: step 11128, loss 0.00215265, acc 1
2016-09-06T02:17:11.024141: step 11129, loss 0.00292751, acc 1
2016-09-06T02:17:11.844506: step 11130, loss 0.0190536, acc 0.98
2016-09-06T02:17:12.665415: step 11131, loss 0.0727039, acc 0.96
2016-09-06T02:17:13.439616: step 11132, loss 0.0168312, acc 1
2016-09-06T02:17:14.256362: step 11133, loss 0.0161523, acc 1
2016-09-06T02:17:15.078934: step 11134, loss 0.00318635, acc 1
2016-09-06T02:17:15.881732: step 11135, loss 0.0237907, acc 0.98
2016-09-06T02:17:16.643222: step 11136, loss 0.0673901, acc 0.977273
2016-09-06T02:17:17.434180: step 11137, loss 0.00265203, acc 1
2016-09-06T02:17:18.256830: step 11138, loss 0.0274701, acc 1
2016-09-06T02:17:19.084507: step 11139, loss 0.00417355, acc 1
2016-09-06T02:17:19.901899: step 11140, loss 0.0489698, acc 0.98
2016-09-06T02:17:20.712016: step 11141, loss 0.00482414, acc 1
2016-09-06T02:17:21.513563: step 11142, loss 0.00291046, acc 1
2016-09-06T02:17:22.337185: step 11143, loss 0.00599312, acc 1
2016-09-06T02:17:23.122710: step 11144, loss 0.0153915, acc 1
2016-09-06T02:17:23.924303: step 11145, loss 0.0226323, acc 0.98
2016-09-06T02:17:24.707164: step 11146, loss 0.00634473, acc 1
2016-09-06T02:17:25.497782: step 11147, loss 0.0170685, acc 1
2016-09-06T02:17:26.321935: step 11148, loss 0.0617085, acc 0.98
2016-09-06T02:17:27.143355: step 11149, loss 0.00474029, acc 1
2016-09-06T02:17:27.923455: step 11150, loss 0.00243302, acc 1
2016-09-06T02:17:28.725995: step 11151, loss 0.0361743, acc 0.98
2016-09-06T02:17:29.524308: step 11152, loss 0.0201921, acc 1
2016-09-06T02:17:30.330252: step 11153, loss 0.0038828, acc 1
2016-09-06T02:17:31.158498: step 11154, loss 0.00510447, acc 1
2016-09-06T02:17:31.999229: step 11155, loss 0.0673508, acc 0.98
2016-09-06T02:17:32.804659: step 11156, loss 0.0170699, acc 0.98
2016-09-06T02:17:33.639111: step 11157, loss 0.043121, acc 0.98
2016-09-06T02:17:34.460343: step 11158, loss 0.00493246, acc 1
2016-09-06T02:17:35.272583: step 11159, loss 0.0221141, acc 0.98
2016-09-06T02:17:36.057801: step 11160, loss 0.00834953, acc 1
2016-09-06T02:17:36.881680: step 11161, loss 0.00847192, acc 1
2016-09-06T02:17:37.676644: step 11162, loss 0.0339099, acc 0.98
2016-09-06T02:17:38.478426: step 11163, loss 0.023265, acc 0.98
2016-09-06T02:17:39.256386: step 11164, loss 0.00315579, acc 1
2016-09-06T02:17:40.050096: step 11165, loss 0.0675071, acc 0.96
2016-09-06T02:17:40.858692: step 11166, loss 0.0034128, acc 1
2016-09-06T02:17:41.655533: step 11167, loss 0.0292168, acc 0.98
2016-09-06T02:17:42.484075: step 11168, loss 0.0209867, acc 1
2016-09-06T02:17:43.268693: step 11169, loss 0.0276109, acc 0.98
2016-09-06T02:17:44.102812: step 11170, loss 0.00311333, acc 1
2016-09-06T02:17:44.887986: step 11171, loss 0.00264658, acc 1
2016-09-06T02:17:45.695140: step 11172, loss 0.00330895, acc 1
2016-09-06T02:17:46.503394: step 11173, loss 0.00512762, acc 1
2016-09-06T02:17:47.271557: step 11174, loss 0.0148167, acc 1
2016-09-06T02:17:48.064710: step 11175, loss 0.0116522, acc 1
2016-09-06T02:17:48.875225: step 11176, loss 0.00361628, acc 1
2016-09-06T02:17:49.697883: step 11177, loss 0.00698386, acc 1
2016-09-06T02:17:50.488058: step 11178, loss 0.0113328, acc 1
2016-09-06T02:17:51.292861: step 11179, loss 0.101276, acc 0.98
2016-09-06T02:17:52.094638: step 11180, loss 0.00471961, acc 1
2016-09-06T02:17:52.965721: step 11181, loss 0.00232321, acc 1
2016-09-06T02:17:53.782977: step 11182, loss 0.0154028, acc 1
2016-09-06T02:17:54.585265: step 11183, loss 0.00252828, acc 1
2016-09-06T02:17:55.375963: step 11184, loss 0.0035473, acc 1
2016-09-06T02:17:56.176512: step 11185, loss 0.0113373, acc 1
2016-09-06T02:17:56.964705: step 11186, loss 0.00223298, acc 1
2016-09-06T02:17:57.757612: step 11187, loss 0.0188956, acc 0.98
2016-09-06T02:17:58.564066: step 11188, loss 0.0285404, acc 0.98
2016-09-06T02:17:59.369356: step 11189, loss 0.0220704, acc 1
2016-09-06T02:18:00.155911: step 11190, loss 0.00236933, acc 1
2016-09-06T02:18:01.031691: step 11191, loss 0.0275631, acc 0.98
2016-09-06T02:18:01.818838: step 11192, loss 0.00396762, acc 1
2016-09-06T02:18:02.589660: step 11193, loss 0.00419439, acc 1
2016-09-06T02:18:03.403470: step 11194, loss 0.0311513, acc 1
2016-09-06T02:18:04.209454: step 11195, loss 0.00236924, acc 1
2016-09-06T02:18:05.013833: step 11196, loss 0.00233052, acc 1
2016-09-06T02:18:05.830534: step 11197, loss 0.0462797, acc 0.96
2016-09-06T02:18:06.641127: step 11198, loss 0.00229063, acc 1
2016-09-06T02:18:07.440677: step 11199, loss 0.0024982, acc 1
2016-09-06T02:18:08.248585: step 11200, loss 0.0180122, acc 1

Evaluation:
2016-09-06T02:18:11.983465: step 11200, loss 3.39696, acc 0.706379

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-11200

2016-09-06T02:18:13.958707: step 11201, loss 0.0558371, acc 0.98
2016-09-06T02:18:14.816462: step 11202, loss 0.0716969, acc 0.96
2016-09-06T02:18:15.632182: step 11203, loss 0.00756231, acc 1
2016-09-06T02:18:16.419741: step 11204, loss 0.0829009, acc 0.98
2016-09-06T02:18:17.220205: step 11205, loss 0.00336491, acc 1
2016-09-06T02:18:18.045109: step 11206, loss 0.00318006, acc 1
2016-09-06T02:18:18.852381: step 11207, loss 0.00475873, acc 1
2016-09-06T02:18:19.666984: step 11208, loss 0.0644529, acc 0.98
2016-09-06T02:18:20.491058: step 11209, loss 0.0212271, acc 0.98
2016-09-06T02:18:21.278894: step 11210, loss 0.00258543, acc 1
2016-09-06T02:18:22.058968: step 11211, loss 0.00927314, acc 1
2016-09-06T02:18:22.876626: step 11212, loss 0.0169831, acc 0.98
2016-09-06T02:18:23.663488: step 11213, loss 0.00473719, acc 1
2016-09-06T02:18:24.472091: step 11214, loss 0.00323611, acc 1
2016-09-06T02:18:25.301404: step 11215, loss 0.051358, acc 0.98
2016-09-06T02:18:26.087324: step 11216, loss 0.00356158, acc 1
2016-09-06T02:18:26.892043: step 11217, loss 0.00330435, acc 1
2016-09-06T02:18:27.682046: step 11218, loss 0.00467813, acc 1
2016-09-06T02:18:28.471099: step 11219, loss 0.0275269, acc 1
2016-09-06T02:18:29.293013: step 11220, loss 0.00271341, acc 1
2016-09-06T02:18:30.140845: step 11221, loss 0.0181963, acc 1
2016-09-06T02:18:30.912037: step 11222, loss 0.0826596, acc 0.98
2016-09-06T02:18:31.715318: step 11223, loss 0.00191379, acc 1
2016-09-06T02:18:32.532828: step 11224, loss 0.0223289, acc 0.98
2016-09-06T02:18:33.335509: step 11225, loss 0.0127061, acc 1
2016-09-06T02:18:34.133778: step 11226, loss 0.0167613, acc 0.98
2016-09-06T02:18:34.949589: step 11227, loss 0.0154251, acc 1
2016-09-06T02:18:35.748374: step 11228, loss 0.00224362, acc 1
2016-09-06T02:18:36.560591: step 11229, loss 0.019689, acc 1
2016-09-06T02:18:37.412111: step 11230, loss 0.00278953, acc 1
2016-09-06T02:18:38.183089: step 11231, loss 0.00276139, acc 1
2016-09-06T02:18:38.992125: step 11232, loss 0.00218016, acc 1
2016-09-06T02:18:39.832223: step 11233, loss 0.00289904, acc 1
2016-09-06T02:18:40.630918: step 11234, loss 0.00645458, acc 1
2016-09-06T02:18:41.437579: step 11235, loss 0.00838814, acc 1
2016-09-06T02:18:42.234047: step 11236, loss 0.00239568, acc 1
2016-09-06T02:18:43.018092: step 11237, loss 0.0159285, acc 1
2016-09-06T02:18:43.825414: step 11238, loss 0.00248036, acc 1
2016-09-06T02:18:44.648927: step 11239, loss 0.0333398, acc 0.98
2016-09-06T02:18:45.457029: step 11240, loss 0.00254344, acc 1
2016-09-06T02:18:46.279354: step 11241, loss 0.0317875, acc 0.98
2016-09-06T02:18:47.126794: step 11242, loss 0.00368493, acc 1
2016-09-06T02:18:47.918875: step 11243, loss 0.0361873, acc 0.96
2016-09-06T02:18:48.728992: step 11244, loss 0.018586, acc 1
2016-09-06T02:18:49.536906: step 11245, loss 0.0315205, acc 0.98
2016-09-06T02:18:50.369997: step 11246, loss 0.0374699, acc 0.98
2016-09-06T02:18:51.177608: step 11247, loss 0.00366214, acc 1
2016-09-06T02:18:52.050706: step 11248, loss 0.00569577, acc 1
2016-09-06T02:18:52.859976: step 11249, loss 0.00840961, acc 1
2016-09-06T02:18:53.662783: step 11250, loss 0.014961, acc 1
2016-09-06T02:18:54.484134: step 11251, loss 0.0232746, acc 0.98
2016-09-06T02:18:55.286911: step 11252, loss 0.00396983, acc 1
2016-09-06T02:18:56.077986: step 11253, loss 0.00590408, acc 1
2016-09-06T02:18:56.900202: step 11254, loss 0.00523791, acc 1
2016-09-06T02:18:57.714681: step 11255, loss 0.00712329, acc 1
2016-09-06T02:18:58.535401: step 11256, loss 0.00242502, acc 1
2016-09-06T02:18:59.342485: step 11257, loss 0.00959655, acc 1
2016-09-06T02:19:00.154794: step 11258, loss 0.00336405, acc 1
2016-09-06T02:19:01.003973: step 11259, loss 0.0100246, acc 1
2016-09-06T02:19:01.826480: step 11260, loss 0.0240966, acc 0.98
2016-09-06T02:19:02.629406: step 11261, loss 0.00443733, acc 1
2016-09-06T02:19:03.430907: step 11262, loss 0.0181119, acc 1
2016-09-06T02:19:04.256201: step 11263, loss 0.0291663, acc 0.98
2016-09-06T02:19:05.048673: step 11264, loss 0.0817641, acc 0.96
2016-09-06T02:19:05.874763: step 11265, loss 0.066916, acc 0.98
2016-09-06T02:19:06.694259: step 11266, loss 0.00250671, acc 1
2016-09-06T02:19:07.503295: step 11267, loss 0.00419699, acc 1
2016-09-06T02:19:08.301123: step 11268, loss 0.0231951, acc 0.98
2016-09-06T02:19:09.148876: step 11269, loss 0.0178564, acc 1
2016-09-06T02:19:09.979400: step 11270, loss 0.00304113, acc 1
2016-09-06T02:19:10.795833: step 11271, loss 0.0170247, acc 1
2016-09-06T02:19:11.618440: step 11272, loss 0.00363981, acc 1
2016-09-06T02:19:12.441453: step 11273, loss 0.016897, acc 0.98
2016-09-06T02:19:13.212082: step 11274, loss 0.00315027, acc 1
2016-09-06T02:19:14.007587: step 11275, loss 0.0028454, acc 1
2016-09-06T02:19:14.824929: step 11276, loss 0.00218816, acc 1
2016-09-06T02:19:15.646206: step 11277, loss 0.0295115, acc 1
2016-09-06T02:19:16.453110: step 11278, loss 0.00495739, acc 1
2016-09-06T02:19:17.282270: step 11279, loss 0.00307815, acc 1
2016-09-06T02:19:18.067075: step 11280, loss 0.0195975, acc 1
2016-09-06T02:19:18.871705: step 11281, loss 0.00332793, acc 1
2016-09-06T02:19:19.712433: step 11282, loss 0.024441, acc 0.98
2016-09-06T02:19:20.494418: step 11283, loss 0.0041024, acc 1
2016-09-06T02:19:21.315860: step 11284, loss 0.00602316, acc 1
2016-09-06T02:19:22.123252: step 11285, loss 0.00232611, acc 1
2016-09-06T02:19:22.899307: step 11286, loss 0.0243008, acc 0.98
2016-09-06T02:19:23.703820: step 11287, loss 0.00211281, acc 1
2016-09-06T02:19:24.514021: step 11288, loss 0.00505132, acc 1
2016-09-06T02:19:25.305685: step 11289, loss 0.00239673, acc 1
2016-09-06T02:19:26.165135: step 11290, loss 0.00828409, acc 1
2016-09-06T02:19:27.017078: step 11291, loss 0.00475817, acc 1
2016-09-06T02:19:27.835705: step 11292, loss 0.0146395, acc 1
2016-09-06T02:19:28.627270: step 11293, loss 0.002673, acc 1
2016-09-06T02:19:29.443665: step 11294, loss 0.0470594, acc 0.98
2016-09-06T02:19:30.261263: step 11295, loss 0.00908584, acc 1
2016-09-06T02:19:31.060969: step 11296, loss 0.00328608, acc 1
2016-09-06T02:19:31.902346: step 11297, loss 0.0629215, acc 0.96
2016-09-06T02:19:32.670396: step 11298, loss 0.0685608, acc 0.98
2016-09-06T02:19:33.495221: step 11299, loss 0.00209547, acc 1
2016-09-06T02:19:34.331030: step 11300, loss 0.00819774, acc 1

Evaluation:
2016-09-06T02:19:38.076032: step 11300, loss 3.11964, acc 0.72045

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-11300

2016-09-06T02:19:40.017277: step 11301, loss 0.0182052, acc 0.98
2016-09-06T02:19:40.846368: step 11302, loss 0.0249838, acc 1
2016-09-06T02:19:41.716855: step 11303, loss 0.0296363, acc 0.98
2016-09-06T02:19:42.534166: step 11304, loss 0.00194358, acc 1
2016-09-06T02:19:43.335374: step 11305, loss 0.053934, acc 0.98
2016-09-06T02:19:44.145859: step 11306, loss 0.00397051, acc 1
2016-09-06T02:19:44.953295: step 11307, loss 0.0033745, acc 1
2016-09-06T02:19:45.774114: step 11308, loss 0.0117547, acc 1
2016-09-06T02:19:46.599904: step 11309, loss 0.0023686, acc 1
2016-09-06T02:19:47.422236: step 11310, loss 0.0258506, acc 0.98
2016-09-06T02:19:48.211135: step 11311, loss 0.00454002, acc 1
2016-09-06T02:19:49.061020: step 11312, loss 0.0161428, acc 1
2016-09-06T02:19:49.864227: step 11313, loss 0.228256, acc 0.96
2016-09-06T02:19:50.663411: step 11314, loss 0.0116675, acc 1
2016-09-06T02:19:51.475889: step 11315, loss 0.00189544, acc 1
2016-09-06T02:19:52.289279: step 11316, loss 0.0267429, acc 1
2016-09-06T02:19:53.105473: step 11317, loss 0.0870176, acc 0.94
2016-09-06T02:19:53.930917: step 11318, loss 0.0348077, acc 0.98
2016-09-06T02:19:54.728894: step 11319, loss 0.00448144, acc 1
2016-09-06T02:19:55.543079: step 11320, loss 0.00439884, acc 1
2016-09-06T02:19:56.381690: step 11321, loss 0.0286136, acc 0.98
2016-09-06T02:19:57.170728: step 11322, loss 0.0242851, acc 0.98
2016-09-06T02:19:58.158863: step 11323, loss 0.0117072, acc 1
2016-09-06T02:19:58.984619: step 11324, loss 0.00484459, acc 1
2016-09-06T02:19:59.829581: step 11325, loss 0.0390741, acc 0.98
2016-09-06T02:20:00.680586: step 11326, loss 0.129149, acc 0.98
2016-09-06T02:20:01.589182: step 11327, loss 0.0133651, acc 1
2016-09-06T02:20:02.375210: step 11328, loss 0.00598564, acc 1
2016-09-06T02:20:03.187042: step 11329, loss 0.00669587, acc 1
2016-09-06T02:20:03.991731: step 11330, loss 0.0459906, acc 0.98
2016-09-06T02:20:04.833787: step 11331, loss 0.14276, acc 0.98
2016-09-06T02:20:05.682237: step 11332, loss 0.0410922, acc 0.98
2016-09-06T02:20:06.528987: step 11333, loss 0.00666354, acc 1
2016-09-06T02:20:07.333613: step 11334, loss 0.0768957, acc 0.98
2016-09-06T02:20:08.142455: step 11335, loss 0.00767363, acc 1
2016-09-06T02:20:08.917320: step 11336, loss 0.0196167, acc 1
2016-09-06T02:20:09.728905: step 11337, loss 0.0218347, acc 0.98
2016-09-06T02:20:10.566073: step 11338, loss 0.00687719, acc 1
2016-09-06T02:20:11.352503: step 11339, loss 0.0198431, acc 1
2016-09-06T02:20:12.145471: step 11340, loss 0.0171479, acc 1
2016-09-06T02:20:13.002162: step 11341, loss 0.0191834, acc 1
2016-09-06T02:20:13.792759: step 11342, loss 0.00680227, acc 1
2016-09-06T02:20:14.605402: step 11343, loss 0.029619, acc 0.98
2016-09-06T02:20:15.432079: step 11344, loss 0.0619268, acc 0.94
2016-09-06T02:20:16.209872: step 11345, loss 0.0157215, acc 1
2016-09-06T02:20:16.994706: step 11346, loss 0.00656467, acc 1
2016-09-06T02:20:17.809099: step 11347, loss 0.00659823, acc 1
2016-09-06T02:20:18.604275: step 11348, loss 0.0107462, acc 1
2016-09-06T02:20:19.401634: step 11349, loss 0.0116125, acc 1
2016-09-06T02:20:20.218129: step 11350, loss 0.0276935, acc 0.98
2016-09-06T02:20:20.990448: step 11351, loss 0.0113428, acc 1
2016-09-06T02:20:21.795631: step 11352, loss 0.0555536, acc 0.98
2016-09-06T02:20:22.597521: step 11353, loss 0.0552053, acc 0.96
2016-09-06T02:20:23.399231: step 11354, loss 0.0403545, acc 0.98
2016-09-06T02:20:24.216570: step 11355, loss 0.0597433, acc 0.98
2016-09-06T02:20:25.031831: step 11356, loss 0.0238754, acc 0.98
2016-09-06T02:20:25.847521: step 11357, loss 0.0170029, acc 1
2016-09-06T02:20:26.669237: step 11358, loss 0.00866185, acc 1
2016-09-06T02:20:27.486992: step 11359, loss 0.0221687, acc 1
2016-09-06T02:20:28.309077: step 11360, loss 0.0353704, acc 0.98
2016-09-06T02:20:29.095914: step 11361, loss 0.0175773, acc 1
2016-09-06T02:20:29.956240: step 11362, loss 0.0122749, acc 1
2016-09-06T02:20:30.781621: step 11363, loss 0.00589547, acc 1
2016-09-06T02:20:31.603368: step 11364, loss 0.006196, acc 1
2016-09-06T02:20:32.449917: step 11365, loss 0.0118956, acc 1
2016-09-06T02:20:33.267361: step 11366, loss 0.0199584, acc 0.98
2016-09-06T02:20:34.061723: step 11367, loss 0.0208427, acc 0.98
2016-09-06T02:20:34.868343: step 11368, loss 0.147338, acc 0.98
2016-09-06T02:20:35.666154: step 11369, loss 0.0327438, acc 0.98
2016-09-06T02:20:36.462980: step 11370, loss 0.0180161, acc 1
2016-09-06T02:20:37.281747: step 11371, loss 0.0420747, acc 0.98
2016-09-06T02:20:38.105899: step 11372, loss 0.157762, acc 0.96
2016-09-06T02:20:38.905535: step 11373, loss 0.00808884, acc 1
2016-09-06T02:20:39.720690: step 11374, loss 0.0155949, acc 1
2016-09-06T02:20:40.503045: step 11375, loss 0.0389726, acc 0.98
2016-09-06T02:20:41.351679: step 11376, loss 0.012106, acc 1
2016-09-06T02:20:42.190863: step 11377, loss 0.00602521, acc 1
2016-09-06T02:20:42.994508: step 11378, loss 0.00900343, acc 1
2016-09-06T02:20:43.813670: step 11379, loss 0.0115772, acc 1
2016-09-06T02:20:44.659058: step 11380, loss 0.0248567, acc 1
2016-09-06T02:20:45.448408: step 11381, loss 0.055407, acc 0.98
2016-09-06T02:20:46.261877: step 11382, loss 0.0339956, acc 0.98
2016-09-06T02:20:47.092836: step 11383, loss 0.0153121, acc 1
2016-09-06T02:20:47.923319: step 11384, loss 0.0304483, acc 1
2016-09-06T02:20:48.739782: step 11385, loss 0.0109743, acc 1
2016-09-06T02:20:49.576987: step 11386, loss 0.0256544, acc 1
2016-09-06T02:20:50.402850: step 11387, loss 0.00804715, acc 1
2016-09-06T02:20:51.166899: step 11388, loss 0.00589566, acc 1
2016-09-06T02:20:52.011998: step 11389, loss 0.0431219, acc 0.98
2016-09-06T02:20:52.813434: step 11390, loss 0.0100799, acc 1
2016-09-06T02:20:53.616461: step 11391, loss 0.00481036, acc 1
2016-09-06T02:20:54.444074: step 11392, loss 0.0432912, acc 0.96
2016-09-06T02:20:55.254064: step 11393, loss 0.0092416, acc 1
2016-09-06T02:20:56.066980: step 11394, loss 0.0692931, acc 0.96
2016-09-06T02:20:56.888457: step 11395, loss 0.0151908, acc 1
2016-09-06T02:20:57.704711: step 11396, loss 0.027853, acc 0.98
2016-09-06T02:20:58.512338: step 11397, loss 0.0273062, acc 1
2016-09-06T02:20:59.336314: step 11398, loss 0.00508394, acc 1
2016-09-06T02:21:00.165312: step 11399, loss 0.00632735, acc 1
2016-09-06T02:21:00.991685: step 11400, loss 0.00494464, acc 1

Evaluation:
2016-09-06T02:21:04.717479: step 11400, loss 2.7308, acc 0.718574

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-11400

2016-09-06T02:21:06.696051: step 11401, loss 0.00882057, acc 1
2016-09-06T02:21:07.516725: step 11402, loss 0.0205051, acc 0.98
2016-09-06T02:21:08.328923: step 11403, loss 0.0242908, acc 0.98
2016-09-06T02:21:09.120982: step 11404, loss 0.0196763, acc 0.98
2016-09-06T02:21:09.943612: step 11405, loss 0.00485618, acc 1
2016-09-06T02:21:10.774191: step 11406, loss 0.0322609, acc 0.98
2016-09-06T02:21:11.590019: step 11407, loss 0.00488579, acc 1
2016-09-06T02:21:12.405315: step 11408, loss 0.00505781, acc 1
2016-09-06T02:21:13.242615: step 11409, loss 0.0047617, acc 1
2016-09-06T02:21:14.051131: step 11410, loss 0.0531717, acc 0.98
2016-09-06T02:21:14.889395: step 11411, loss 0.08645, acc 0.98
2016-09-06T02:21:15.694004: step 11412, loss 0.00844161, acc 1
2016-09-06T02:21:16.473905: step 11413, loss 0.0645568, acc 0.96
2016-09-06T02:21:17.300238: step 11414, loss 0.033433, acc 1
2016-09-06T02:21:18.126544: step 11415, loss 0.00673603, acc 1
2016-09-06T02:21:18.939815: step 11416, loss 0.00627151, acc 1
2016-09-06T02:21:19.790852: step 11417, loss 0.00666743, acc 1
2016-09-06T02:21:20.617408: step 11418, loss 0.0330903, acc 0.98
2016-09-06T02:21:21.412258: step 11419, loss 0.0211156, acc 0.98
2016-09-06T02:21:22.239858: step 11420, loss 0.0628788, acc 0.96
2016-09-06T02:21:23.054767: step 11421, loss 0.00408019, acc 1
2016-09-06T02:21:23.852108: step 11422, loss 0.0201079, acc 0.98
2016-09-06T02:21:24.679616: step 11423, loss 0.0187848, acc 0.98
2016-09-06T02:21:25.496157: step 11424, loss 0.00399362, acc 1
2016-09-06T02:21:26.299031: step 11425, loss 0.00878708, acc 1
2016-09-06T02:21:27.153494: step 11426, loss 0.0257187, acc 1
2016-09-06T02:21:27.989075: step 11427, loss 0.004709, acc 1
2016-09-06T02:21:28.759179: step 11428, loss 0.00999742, acc 1
2016-09-06T02:21:29.568839: step 11429, loss 0.0124882, acc 1
2016-09-06T02:21:30.415094: step 11430, loss 0.00395849, acc 1
2016-09-06T02:21:31.192090: step 11431, loss 0.0158801, acc 1
2016-09-06T02:21:31.985371: step 11432, loss 0.00503257, acc 1
2016-09-06T02:21:32.808070: step 11433, loss 0.0301421, acc 0.98
2016-09-06T02:21:33.585137: step 11434, loss 0.0152884, acc 1
2016-09-06T02:21:34.384797: step 11435, loss 0.0405634, acc 0.98
2016-09-06T02:21:35.198207: step 11436, loss 0.0694653, acc 0.96
2016-09-06T02:21:35.995934: step 11437, loss 0.0421198, acc 0.98
2016-09-06T02:21:36.798775: step 11438, loss 0.0118685, acc 1
2016-09-06T02:21:37.625859: step 11439, loss 0.00369192, acc 1
2016-09-06T02:21:38.410187: step 11440, loss 0.046561, acc 0.98
2016-09-06T02:21:39.222757: step 11441, loss 0.0162172, acc 1
2016-09-06T02:21:40.049401: step 11442, loss 0.00346965, acc 1
2016-09-06T02:21:40.874159: step 11443, loss 0.0548821, acc 0.96
2016-09-06T02:21:41.658542: step 11444, loss 0.00932515, acc 1
2016-09-06T02:21:42.461155: step 11445, loss 0.0175199, acc 1
2016-09-06T02:21:43.269888: step 11446, loss 0.00308704, acc 1
2016-09-06T02:21:44.079725: step 11447, loss 0.0221073, acc 0.98
2016-09-06T02:21:44.898129: step 11448, loss 0.0258246, acc 1
2016-09-06T02:21:45.678326: step 11449, loss 0.00426152, acc 1
2016-09-06T02:21:46.460915: step 11450, loss 0.00326074, acc 1
2016-09-06T02:21:47.272650: step 11451, loss 0.010543, acc 1
2016-09-06T02:21:48.071949: step 11452, loss 0.104221, acc 0.96
2016-09-06T02:21:48.888897: step 11453, loss 0.0289762, acc 0.98
2016-09-06T02:21:49.736824: step 11454, loss 0.00341894, acc 1
2016-09-06T02:21:50.511445: step 11455, loss 0.010967, acc 1
2016-09-06T02:21:51.341587: step 11456, loss 0.0630473, acc 0.98
2016-09-06T02:21:52.171393: step 11457, loss 0.0624263, acc 0.98
2016-09-06T02:21:52.973860: step 11458, loss 0.0155768, acc 1
2016-09-06T02:21:53.763064: step 11459, loss 0.0182307, acc 1
2016-09-06T02:21:54.571673: step 11460, loss 0.0107678, acc 1
2016-09-06T02:21:55.349510: step 11461, loss 0.0226365, acc 0.98
2016-09-06T02:21:56.152726: step 11462, loss 0.0181675, acc 1
2016-09-06T02:21:56.942245: step 11463, loss 0.0234314, acc 0.98
2016-09-06T02:21:57.741428: step 11464, loss 0.0106966, acc 1
2016-09-06T02:21:58.546816: step 11465, loss 0.00315932, acc 1
2016-09-06T02:21:59.368311: step 11466, loss 0.0463268, acc 0.98
2016-09-06T02:22:00.171735: step 11467, loss 0.00286799, acc 1
2016-09-06T02:22:00.981356: step 11468, loss 0.026072, acc 0.98
2016-09-06T02:22:01.813420: step 11469, loss 0.0167205, acc 1
2016-09-06T02:22:02.621254: step 11470, loss 0.00478178, acc 1
2016-09-06T02:22:03.419375: step 11471, loss 0.0247668, acc 0.98
2016-09-06T02:22:04.250733: step 11472, loss 0.0262493, acc 0.98
2016-09-06T02:22:05.052754: step 11473, loss 0.0456259, acc 0.98
2016-09-06T02:22:05.840466: step 11474, loss 0.00318877, acc 1
2016-09-06T02:22:06.662648: step 11475, loss 0.0271311, acc 0.98
2016-09-06T02:22:07.420792: step 11476, loss 0.00298655, acc 1
2016-09-06T02:22:08.201126: step 11477, loss 0.00621597, acc 1
2016-09-06T02:22:09.011400: step 11478, loss 0.0510852, acc 0.98
2016-09-06T02:22:09.788520: step 11479, loss 0.00239676, acc 1
2016-09-06T02:22:10.590184: step 11480, loss 0.00292579, acc 1
2016-09-06T02:22:11.400238: step 11481, loss 0.0185792, acc 1
2016-09-06T02:22:12.211074: step 11482, loss 0.00311134, acc 1
2016-09-06T02:22:13.016545: step 11483, loss 0.00628262, acc 1
2016-09-06T02:22:13.825596: step 11484, loss 0.0426983, acc 0.98
2016-09-06T02:22:14.633245: step 11485, loss 0.013869, acc 1
2016-09-06T02:22:15.440966: step 11486, loss 0.00918644, acc 1
2016-09-06T02:22:16.262232: step 11487, loss 0.00330845, acc 1
2016-09-06T02:22:17.055676: step 11488, loss 0.0400455, acc 0.96
2016-09-06T02:22:17.858951: step 11489, loss 0.0276117, acc 1
2016-09-06T02:22:18.690970: step 11490, loss 0.0061676, acc 1
2016-09-06T02:22:19.494129: step 11491, loss 0.00313483, acc 1
2016-09-06T02:22:20.316050: step 11492, loss 0.0024262, acc 1
2016-09-06T02:22:21.121442: step 11493, loss 0.0133275, acc 1
2016-09-06T02:22:21.919721: step 11494, loss 0.022576, acc 1
2016-09-06T02:22:22.741785: step 11495, loss 0.00431267, acc 1
2016-09-06T02:22:23.578012: step 11496, loss 0.00453576, acc 1
2016-09-06T02:22:24.388451: step 11497, loss 0.0422476, acc 0.98
2016-09-06T02:22:25.222465: step 11498, loss 0.00419438, acc 1
2016-09-06T02:22:26.061441: step 11499, loss 0.179925, acc 0.98
2016-09-06T02:22:26.890945: step 11500, loss 0.0688127, acc 0.98

Evaluation:
2016-09-06T02:22:30.629383: step 11500, loss 2.20503, acc 0.717636

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-11500

2016-09-06T02:22:32.649961: step 11501, loss 0.00360359, acc 1
2016-09-06T02:22:33.468092: step 11502, loss 0.00457714, acc 1
2016-09-06T02:22:34.267606: step 11503, loss 0.0519393, acc 0.94
2016-09-06T02:22:35.090659: step 11504, loss 0.0156852, acc 1
2016-09-06T02:22:35.945594: step 11505, loss 0.0378924, acc 0.98
2016-09-06T02:22:36.742071: step 11506, loss 0.0272899, acc 1
2016-09-06T02:22:37.565416: step 11507, loss 0.0343497, acc 0.98
2016-09-06T02:22:38.390965: step 11508, loss 0.0241941, acc 1
2016-09-06T02:22:39.194015: step 11509, loss 0.00224462, acc 1
2016-09-06T02:22:39.966088: step 11510, loss 0.00415346, acc 1
2016-09-06T02:22:40.811374: step 11511, loss 0.00265356, acc 1
2016-09-06T02:22:41.607588: step 11512, loss 0.0035905, acc 1
2016-09-06T02:22:42.415225: step 11513, loss 0.0118497, acc 1
2016-09-06T02:22:43.224100: step 11514, loss 0.0288079, acc 0.98
2016-09-06T02:22:44.103909: step 11515, loss 0.0113876, acc 1
2016-09-06T02:22:44.877777: step 11516, loss 0.00265971, acc 1
2016-09-06T02:22:45.675572: step 11517, loss 0.0266561, acc 0.98
2016-09-06T02:22:46.481898: step 11518, loss 0.0358953, acc 0.98
2016-09-06T02:22:47.276411: step 11519, loss 0.00472108, acc 1
2016-09-06T02:22:47.997292: step 11520, loss 0.0133526, acc 1
2016-09-06T02:22:48.803431: step 11521, loss 0.00299486, acc 1
2016-09-06T02:22:49.620324: step 11522, loss 0.078517, acc 0.96
2016-09-06T02:22:50.458065: step 11523, loss 0.00255086, acc 1
2016-09-06T02:22:51.273115: step 11524, loss 0.0118882, acc 1
2016-09-06T02:22:52.080026: step 11525, loss 0.00457826, acc 1
2016-09-06T02:22:52.904065: step 11526, loss 0.0174132, acc 1
2016-09-06T02:22:53.749214: step 11527, loss 0.0162675, acc 1
2016-09-06T02:22:54.543110: step 11528, loss 0.0434807, acc 0.98
2016-09-06T02:22:55.346314: step 11529, loss 0.0311963, acc 1
2016-09-06T02:22:56.157136: step 11530, loss 0.020062, acc 1
2016-09-06T02:22:56.935891: step 11531, loss 0.0652664, acc 0.96
2016-09-06T02:22:57.754769: step 11532, loss 0.0291892, acc 0.98
2016-09-06T02:22:58.579654: step 11533, loss 0.00940974, acc 1
2016-09-06T02:22:59.390601: step 11534, loss 0.0211116, acc 0.98
2016-09-06T02:23:00.237403: step 11535, loss 0.00245448, acc 1
2016-09-06T02:23:01.044642: step 11536, loss 0.0242682, acc 0.98
2016-09-06T02:23:01.838379: step 11537, loss 0.0402502, acc 0.98
2016-09-06T02:23:02.638505: step 11538, loss 0.00291719, acc 1
2016-09-06T02:23:03.444855: step 11539, loss 0.00436515, acc 1
2016-09-06T02:23:04.215968: step 11540, loss 0.0159618, acc 1
2016-09-06T02:23:05.038521: step 11541, loss 0.00317133, acc 1
2016-09-06T02:23:05.870091: step 11542, loss 0.0147696, acc 1
2016-09-06T02:23:06.644356: step 11543, loss 0.00224532, acc 1
2016-09-06T02:23:07.479333: step 11544, loss 0.00889362, acc 1
2016-09-06T02:23:08.299876: step 11545, loss 0.0141871, acc 1
2016-09-06T02:23:09.072388: step 11546, loss 0.0449356, acc 0.98
2016-09-06T02:23:09.908425: step 11547, loss 0.00801501, acc 1
2016-09-06T02:23:10.721995: step 11548, loss 0.012233, acc 1
2016-09-06T02:23:11.492567: step 11549, loss 0.00242231, acc 1
2016-09-06T02:23:12.325188: step 11550, loss 0.00224665, acc 1
2016-09-06T02:23:13.107062: step 11551, loss 0.00256771, acc 1
2016-09-06T02:23:13.891224: step 11552, loss 0.00240986, acc 1
2016-09-06T02:23:14.690986: step 11553, loss 0.00344197, acc 1
2016-09-06T02:23:15.510373: step 11554, loss 0.0217302, acc 1
2016-09-06T02:23:16.309062: step 11555, loss 0.0032591, acc 1
2016-09-06T02:23:17.169605: step 11556, loss 0.0459835, acc 0.96
2016-09-06T02:23:18.018915: step 11557, loss 0.00220934, acc 1
2016-09-06T02:23:18.845619: step 11558, loss 0.0312317, acc 1
2016-09-06T02:23:19.681991: step 11559, loss 0.0196534, acc 1
2016-09-06T02:23:20.543279: step 11560, loss 0.0022991, acc 1
2016-09-06T02:23:21.379119: step 11561, loss 0.0385884, acc 0.98
2016-09-06T02:23:22.194091: step 11562, loss 0.0119448, acc 1
2016-09-06T02:23:23.030175: step 11563, loss 0.022008, acc 0.98
2016-09-06T02:23:23.933420: step 11564, loss 0.0181044, acc 0.98
2016-09-06T02:23:24.806734: step 11565, loss 0.00237127, acc 1
2016-09-06T02:23:25.604601: step 11566, loss 0.00985196, acc 1
2016-09-06T02:23:26.389491: step 11567, loss 0.0156903, acc 1
2016-09-06T02:23:27.193599: step 11568, loss 0.00271136, acc 1
2016-09-06T02:23:28.021429: step 11569, loss 0.0149546, acc 1
2016-09-06T02:23:28.825995: step 11570, loss 0.00566839, acc 1
2016-09-06T02:23:29.617308: step 11571, loss 0.00245741, acc 1
2016-09-06T02:23:30.405957: step 11572, loss 0.0217673, acc 1
2016-09-06T02:23:31.230394: step 11573, loss 0.00239349, acc 1
2016-09-06T02:23:32.002613: step 11574, loss 0.0484531, acc 0.98
2016-09-06T02:23:32.800303: step 11575, loss 0.00262078, acc 1
2016-09-06T02:23:33.625043: step 11576, loss 0.00230516, acc 1
2016-09-06T02:23:34.419950: step 11577, loss 0.0558576, acc 0.98
2016-09-06T02:23:35.227002: step 11578, loss 0.0302382, acc 1
2016-09-06T02:23:36.051927: step 11579, loss 0.00263167, acc 1
2016-09-06T02:23:36.890837: step 11580, loss 0.00919626, acc 1
2016-09-06T02:23:37.681462: step 11581, loss 0.0140923, acc 1
2016-09-06T02:23:38.489459: step 11582, loss 0.00520332, acc 1
2016-09-06T02:23:39.286034: step 11583, loss 0.00283576, acc 1
2016-09-06T02:23:40.125889: step 11584, loss 0.0829953, acc 0.96
2016-09-06T02:23:40.939302: step 11585, loss 0.00303783, acc 1
2016-09-06T02:23:41.738536: step 11586, loss 0.0102065, acc 1
2016-09-06T02:23:42.517704: step 11587, loss 0.00920497, acc 1
2016-09-06T02:23:43.323622: step 11588, loss 0.0121569, acc 1
2016-09-06T02:23:44.125412: step 11589, loss 0.0139849, acc 1
2016-09-06T02:23:44.921110: step 11590, loss 0.00448938, acc 1
2016-09-06T02:23:45.747261: step 11591, loss 0.0731191, acc 0.96
2016-09-06T02:23:46.545466: step 11592, loss 0.0088149, acc 1
2016-09-06T02:23:47.355186: step 11593, loss 0.0125922, acc 1
2016-09-06T02:23:48.163104: step 11594, loss 0.00629583, acc 1
2016-09-06T02:23:48.920967: step 11595, loss 0.0239384, acc 0.98
2016-09-06T02:23:49.713826: step 11596, loss 0.0158835, acc 1
2016-09-06T02:23:50.528238: step 11597, loss 0.00733849, acc 1
2016-09-06T02:23:51.309394: step 11598, loss 0.0120566, acc 1
2016-09-06T02:23:52.109423: step 11599, loss 0.0248316, acc 0.98
2016-09-06T02:23:52.923178: step 11600, loss 0.00960411, acc 1

Evaluation:
2016-09-06T02:23:56.659499: step 11600, loss 2.45835, acc 0.722326

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-11600

2016-09-06T02:23:58.561461: step 11601, loss 0.00748826, acc 1
2016-09-06T02:23:59.389001: step 11602, loss 0.0029421, acc 1
2016-09-06T02:24:00.199934: step 11603, loss 0.0304186, acc 0.98
2016-09-06T02:24:01.029394: step 11604, loss 0.00259492, acc 1
2016-09-06T02:24:01.845232: step 11605, loss 0.0427684, acc 0.98
2016-09-06T02:24:02.657753: step 11606, loss 0.0310524, acc 0.98
2016-09-06T02:24:03.463418: step 11607, loss 0.00261361, acc 1
2016-09-06T02:24:04.293526: step 11608, loss 0.0650867, acc 0.96
2016-09-06T02:24:05.134600: step 11609, loss 0.012143, acc 1
2016-09-06T02:24:05.918721: step 11610, loss 0.00709052, acc 1
2016-09-06T02:24:06.696464: step 11611, loss 0.0308867, acc 0.98
2016-09-06T02:24:07.519354: step 11612, loss 0.00361914, acc 1
2016-09-06T02:24:08.322461: step 11613, loss 0.0173415, acc 1
2016-09-06T02:24:09.123047: step 11614, loss 0.0048451, acc 1
2016-09-06T02:24:09.927911: step 11615, loss 0.0249083, acc 0.98
2016-09-06T02:24:10.730959: step 11616, loss 0.00281261, acc 1
2016-09-06T02:24:11.543469: step 11617, loss 0.00265636, acc 1
2016-09-06T02:24:12.347815: step 11618, loss 0.0194477, acc 0.98
2016-09-06T02:24:13.131431: step 11619, loss 0.00460051, acc 1
2016-09-06T02:24:13.926167: step 11620, loss 0.00237001, acc 1
2016-09-06T02:24:14.748959: step 11621, loss 0.0393008, acc 0.96
2016-09-06T02:24:15.569312: step 11622, loss 0.0158557, acc 1
2016-09-06T02:24:16.405565: step 11623, loss 0.00248403, acc 1
2016-09-06T02:24:17.218122: step 11624, loss 0.00257955, acc 1
2016-09-06T02:24:17.988323: step 11625, loss 0.0207102, acc 0.98
2016-09-06T02:24:18.793071: step 11626, loss 0.00254933, acc 1
2016-09-06T02:24:19.613443: step 11627, loss 0.0224051, acc 1
2016-09-06T02:24:20.393978: step 11628, loss 0.0193493, acc 0.98
2016-09-06T02:24:21.199583: step 11629, loss 0.018885, acc 1
2016-09-06T02:24:22.014352: step 11630, loss 0.00519227, acc 1
2016-09-06T02:24:22.780956: step 11631, loss 0.00252113, acc 1
2016-09-06T02:24:23.606035: step 11632, loss 0.0784657, acc 0.96
2016-09-06T02:24:24.425531: step 11633, loss 0.00370828, acc 1
2016-09-06T02:24:25.230392: step 11634, loss 0.0114749, acc 1
2016-09-06T02:24:26.032677: step 11635, loss 0.0188453, acc 1
2016-09-06T02:24:26.889107: step 11636, loss 0.0021725, acc 1
2016-09-06T02:24:27.696461: step 11637, loss 0.0103397, acc 1
2016-09-06T02:24:28.502178: step 11638, loss 0.00207552, acc 1
2016-09-06T02:24:29.348918: step 11639, loss 0.0144197, acc 1
2016-09-06T02:24:30.163716: step 11640, loss 0.0033581, acc 1
2016-09-06T02:24:30.988908: step 11641, loss 0.002216, acc 1
2016-09-06T02:24:31.855551: step 11642, loss 0.00437434, acc 1
2016-09-06T02:24:32.685808: step 11643, loss 0.00501877, acc 1
2016-09-06T02:24:33.491284: step 11644, loss 0.00651789, acc 1
2016-09-06T02:24:34.297559: step 11645, loss 0.0465091, acc 0.98
2016-09-06T02:24:35.113332: step 11646, loss 0.00272193, acc 1
2016-09-06T02:24:35.937492: step 11647, loss 0.00577004, acc 1
2016-09-06T02:24:36.775056: step 11648, loss 0.00938549, acc 1
2016-09-06T02:24:37.572302: step 11649, loss 0.00258137, acc 1
2016-09-06T02:24:38.378903: step 11650, loss 0.0266944, acc 1
2016-09-06T02:24:39.192097: step 11651, loss 0.00243189, acc 1
2016-09-06T02:24:40.025240: step 11652, loss 0.0044768, acc 1
2016-09-06T02:24:40.830993: step 11653, loss 0.093865, acc 0.96
2016-09-06T02:24:41.667516: step 11654, loss 0.0464825, acc 0.96
2016-09-06T02:24:42.475058: step 11655, loss 0.0236992, acc 0.98
2016-09-06T02:24:43.312445: step 11656, loss 0.0109305, acc 1
2016-09-06T02:24:44.127131: step 11657, loss 0.0284907, acc 0.98
2016-09-06T02:24:44.933022: step 11658, loss 0.0370725, acc 0.98
2016-09-06T02:24:45.728875: step 11659, loss 0.00196183, acc 1
2016-09-06T02:24:46.545968: step 11660, loss 0.00203352, acc 1
2016-09-06T02:24:47.379406: step 11661, loss 0.0724531, acc 0.96
2016-09-06T02:24:48.155652: step 11662, loss 0.00195099, acc 1
2016-09-06T02:24:48.965353: step 11663, loss 0.00978385, acc 1
2016-09-06T02:24:49.803073: step 11664, loss 0.029374, acc 0.98
2016-09-06T02:24:50.612831: step 11665, loss 0.00317534, acc 1
2016-09-06T02:24:51.424012: step 11666, loss 0.0326833, acc 0.98
2016-09-06T02:24:52.237470: step 11667, loss 0.00284355, acc 1
2016-09-06T02:24:53.040043: step 11668, loss 0.00425409, acc 1
2016-09-06T02:24:53.869462: step 11669, loss 0.0281667, acc 0.98
2016-09-06T02:24:54.675834: step 11670, loss 0.0458165, acc 0.96
2016-09-06T02:24:55.451693: step 11671, loss 0.00607891, acc 1
2016-09-06T02:24:56.269899: step 11672, loss 0.00870059, acc 1
2016-09-06T02:24:57.106623: step 11673, loss 0.00535103, acc 1
2016-09-06T02:24:57.881664: step 11674, loss 0.00474086, acc 1
2016-09-06T02:24:58.684834: step 11675, loss 0.139385, acc 0.96
2016-09-06T02:24:59.505564: step 11676, loss 0.0692792, acc 0.96
2016-09-06T02:25:00.301405: step 11677, loss 0.0174808, acc 1
2016-09-06T02:25:01.097295: step 11678, loss 0.00915163, acc 1
2016-09-06T02:25:01.916728: step 11679, loss 0.00447117, acc 1
2016-09-06T02:25:02.702002: step 11680, loss 0.0329749, acc 0.98
2016-09-06T02:25:03.502112: step 11681, loss 0.0660492, acc 0.96
2016-09-06T02:25:04.345946: step 11682, loss 0.0152573, acc 1
2016-09-06T02:25:05.114391: step 11683, loss 0.00333371, acc 1
2016-09-06T02:25:05.910591: step 11684, loss 0.00541048, acc 1
2016-09-06T02:25:06.725994: step 11685, loss 0.00207549, acc 1
2016-09-06T02:25:07.514923: step 11686, loss 0.00536725, acc 1
2016-09-06T02:25:08.306425: step 11687, loss 0.00838058, acc 1
2016-09-06T02:25:09.128679: step 11688, loss 0.00429211, acc 1
2016-09-06T02:25:09.921277: step 11689, loss 0.0550804, acc 0.98
2016-09-06T02:25:10.720493: step 11690, loss 0.00844167, acc 1
2016-09-06T02:25:11.538540: step 11691, loss 0.030732, acc 1
2016-09-06T02:25:12.338197: step 11692, loss 0.0901311, acc 0.98
2016-09-06T02:25:13.138069: step 11693, loss 0.00493497, acc 1
2016-09-06T02:25:13.934282: step 11694, loss 0.0203715, acc 1
2016-09-06T02:25:14.713690: step 11695, loss 0.0166236, acc 1
2016-09-06T02:25:15.535778: step 11696, loss 0.00357201, acc 1
2016-09-06T02:25:16.399664: step 11697, loss 0.0747247, acc 0.94
2016-09-06T02:25:17.204970: step 11698, loss 0.105709, acc 0.96
2016-09-06T02:25:18.023758: step 11699, loss 0.0171261, acc 0.98
2016-09-06T02:25:18.842803: step 11700, loss 0.0420303, acc 0.98

Evaluation:
2016-09-06T02:25:22.549582: step 11700, loss 2.07229, acc 0.723265

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-11700

2016-09-06T02:25:24.439915: step 11701, loss 0.00207952, acc 1
2016-09-06T02:25:25.271520: step 11702, loss 0.00698615, acc 1
2016-09-06T02:25:26.095656: step 11703, loss 0.0125385, acc 1
2016-09-06T02:25:26.928052: step 11704, loss 0.0114184, acc 1
2016-09-06T02:25:27.705640: step 11705, loss 0.00208162, acc 1
2016-09-06T02:25:28.527864: step 11706, loss 0.040208, acc 0.98
2016-09-06T02:25:29.308310: step 11707, loss 0.0191857, acc 1
2016-09-06T02:25:30.120402: step 11708, loss 0.00268615, acc 1
2016-09-06T02:25:30.951646: step 11709, loss 0.0260292, acc 0.98
2016-09-06T02:25:31.744966: step 11710, loss 0.00393469, acc 1
2016-09-06T02:25:32.539668: step 11711, loss 0.0138788, acc 1
2016-09-06T02:25:33.310382: step 11712, loss 0.00198131, acc 1
2016-09-06T02:25:34.115555: step 11713, loss 0.00714804, acc 1
2016-09-06T02:25:34.910214: step 11714, loss 0.0531323, acc 0.98
2016-09-06T02:25:35.729802: step 11715, loss 0.0111069, acc 1
2016-09-06T02:25:36.524553: step 11716, loss 0.0215981, acc 1
2016-09-06T02:25:37.339546: step 11717, loss 0.0660579, acc 0.98
2016-09-06T02:25:38.141564: step 11718, loss 0.024883, acc 0.98
2016-09-06T02:25:38.934375: step 11719, loss 0.0116443, acc 1
2016-09-06T02:25:39.729079: step 11720, loss 0.023166, acc 0.98
2016-09-06T02:25:40.545355: step 11721, loss 0.00502972, acc 1
2016-09-06T02:25:41.364660: step 11722, loss 0.00241075, acc 1
2016-09-06T02:25:42.158716: step 11723, loss 0.0431504, acc 0.98
2016-09-06T02:25:42.957405: step 11724, loss 0.00645527, acc 1
2016-09-06T02:25:43.743209: step 11725, loss 0.00447105, acc 1
2016-09-06T02:25:44.564629: step 11726, loss 0.0101355, acc 1
2016-09-06T02:25:45.368081: step 11727, loss 0.00228351, acc 1
2016-09-06T02:25:46.169625: step 11728, loss 0.0328502, acc 1
2016-09-06T02:25:47.008539: step 11729, loss 0.001919, acc 1
2016-09-06T02:25:47.825872: step 11730, loss 0.00380204, acc 1
2016-09-06T02:25:48.606204: step 11731, loss 0.00194149, acc 1
2016-09-06T02:25:49.407649: step 11732, loss 0.0158649, acc 1
2016-09-06T02:25:50.224196: step 11733, loss 0.00385689, acc 1
2016-09-06T02:25:51.035258: step 11734, loss 0.00198139, acc 1
2016-09-06T02:25:51.830727: step 11735, loss 0.00737522, acc 1
2016-09-06T02:25:52.649162: step 11736, loss 0.00471212, acc 1
2016-09-06T02:25:53.448945: step 11737, loss 0.00218286, acc 1
2016-09-06T02:25:54.251063: step 11738, loss 0.0144889, acc 1
2016-09-06T02:25:55.058053: step 11739, loss 0.0149186, acc 1
2016-09-06T02:25:55.853413: step 11740, loss 0.0126488, acc 1
2016-09-06T02:25:56.641673: step 11741, loss 0.00260391, acc 1
2016-09-06T02:25:57.475264: step 11742, loss 0.00840934, acc 1
2016-09-06T02:25:58.263576: step 11743, loss 0.00201253, acc 1
2016-09-06T02:25:59.067983: step 11744, loss 0.0621251, acc 0.96
2016-09-06T02:25:59.882033: step 11745, loss 0.00375542, acc 1
2016-09-06T02:26:00.680015: step 11746, loss 0.0133085, acc 1
2016-09-06T02:26:01.495745: step 11747, loss 0.0026401, acc 1
2016-09-06T02:26:02.294290: step 11748, loss 0.00360161, acc 1
2016-09-06T02:26:03.081271: step 11749, loss 0.00186761, acc 1
2016-09-06T02:26:03.887595: step 11750, loss 0.0510741, acc 0.96
2016-09-06T02:26:04.744905: step 11751, loss 0.0205976, acc 0.98
2016-09-06T02:26:05.573394: step 11752, loss 0.00206169, acc 1
2016-09-06T02:26:06.350121: step 11753, loss 0.0168406, acc 1
2016-09-06T02:26:07.182085: step 11754, loss 0.00200241, acc 1
2016-09-06T02:26:07.969810: step 11755, loss 0.0209685, acc 1
2016-09-06T02:26:08.818978: step 11756, loss 0.00189553, acc 1
2016-09-06T02:26:09.650151: step 11757, loss 0.00694625, acc 1
2016-09-06T02:26:10.436471: step 11758, loss 0.0177347, acc 0.98
2016-09-06T02:26:11.254306: step 11759, loss 0.00360215, acc 1
2016-09-06T02:26:12.095837: step 11760, loss 0.0212516, acc 0.98
2016-09-06T02:26:12.897596: step 11761, loss 0.00260514, acc 1
2016-09-06T02:26:13.711591: step 11762, loss 0.00192713, acc 1
2016-09-06T02:26:14.519267: step 11763, loss 0.0107484, acc 1
2016-09-06T02:26:15.327740: step 11764, loss 0.0247478, acc 1
2016-09-06T02:26:16.172928: step 11765, loss 0.0267037, acc 0.98
2016-09-06T02:26:17.011040: step 11766, loss 0.00989415, acc 1
2016-09-06T02:26:17.829148: step 11767, loss 0.0132206, acc 1
2016-09-06T02:26:18.648136: step 11768, loss 0.00185307, acc 1
2016-09-06T02:26:19.522839: step 11769, loss 0.00192137, acc 1
2016-09-06T02:26:20.324201: step 11770, loss 0.0240695, acc 0.98
2016-09-06T02:26:21.131684: step 11771, loss 0.00234206, acc 1
2016-09-06T02:26:21.969978: step 11772, loss 0.00334922, acc 1
2016-09-06T02:26:22.786355: step 11773, loss 0.0078821, acc 1
2016-09-06T02:26:23.620586: step 11774, loss 0.00283721, acc 1
2016-09-06T02:26:24.472712: step 11775, loss 0.00208984, acc 1
2016-09-06T02:26:25.325107: step 11776, loss 0.0108813, acc 1
2016-09-06T02:26:26.126990: step 11777, loss 0.0018798, acc 1
2016-09-06T02:26:26.938409: step 11778, loss 0.00210757, acc 1
2016-09-06T02:26:27.753973: step 11779, loss 0.0891449, acc 0.96
2016-09-06T02:26:28.542902: step 11780, loss 0.0455462, acc 0.96
2016-09-06T02:26:29.326834: step 11781, loss 0.004175, acc 1
2016-09-06T02:26:30.152348: step 11782, loss 0.00443678, acc 1
2016-09-06T02:26:30.952045: step 11783, loss 0.04636, acc 0.98
2016-09-06T02:26:31.773546: step 11784, loss 0.00184232, acc 1
2016-09-06T02:26:32.591711: step 11785, loss 0.0224031, acc 1
2016-09-06T02:26:33.375236: step 11786, loss 0.00183997, acc 1
2016-09-06T02:26:34.163573: step 11787, loss 0.0117586, acc 1
2016-09-06T02:26:34.971142: step 11788, loss 0.0347971, acc 0.98
2016-09-06T02:26:35.757216: step 11789, loss 0.00298318, acc 1
2016-09-06T02:26:36.553409: step 11790, loss 0.0390528, acc 0.98
2016-09-06T02:26:37.448172: step 11791, loss 0.0926708, acc 0.96
2016-09-06T02:26:38.229848: step 11792, loss 0.00437477, acc 1
2016-09-06T02:26:39.042517: step 11793, loss 0.00855248, acc 1
2016-09-06T02:26:39.843107: step 11794, loss 0.0193613, acc 0.98
2016-09-06T02:26:40.613490: step 11795, loss 0.0385545, acc 0.98
2016-09-06T02:26:41.398797: step 11796, loss 0.0025082, acc 1
2016-09-06T02:26:42.197195: step 11797, loss 0.00174699, acc 1
2016-09-06T02:26:42.993419: step 11798, loss 0.0483197, acc 0.96
2016-09-06T02:26:43.804233: step 11799, loss 0.00191989, acc 1
2016-09-06T02:26:44.629782: step 11800, loss 0.0669122, acc 0.98

Evaluation:
2016-09-06T02:26:48.326574: step 11800, loss 2.26493, acc 0.722326

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-11800

2016-09-06T02:26:50.245994: step 11801, loss 0.00313592, acc 1
2016-09-06T02:26:51.081230: step 11802, loss 0.00186856, acc 1
2016-09-06T02:26:51.897074: step 11803, loss 0.0161923, acc 0.98
2016-09-06T02:26:52.721991: step 11804, loss 0.00620576, acc 1
2016-09-06T02:26:53.516720: step 11805, loss 0.0115201, acc 1
2016-09-06T02:26:54.340418: step 11806, loss 0.00927721, acc 1
2016-09-06T02:26:55.136750: step 11807, loss 0.019122, acc 0.98
2016-09-06T02:26:55.920096: step 11808, loss 0.0199351, acc 0.98
2016-09-06T02:26:56.743773: step 11809, loss 0.00258413, acc 1
2016-09-06T02:26:57.513015: step 11810, loss 0.0290202, acc 0.98
2016-09-06T02:26:58.364764: step 11811, loss 0.0166679, acc 0.98
2016-09-06T02:26:59.222210: step 11812, loss 0.0137189, acc 1
2016-09-06T02:27:00.028319: step 11813, loss 0.0342877, acc 0.98
2016-09-06T02:27:00.858398: step 11814, loss 0.00212546, acc 1
2016-09-06T02:27:01.690089: step 11815, loss 0.00778386, acc 1
2016-09-06T02:27:02.480120: step 11816, loss 0.00454244, acc 1
2016-09-06T02:27:03.319936: step 11817, loss 0.0240408, acc 1
2016-09-06T02:27:04.140261: step 11818, loss 0.0856463, acc 0.98
2016-09-06T02:27:04.962927: step 11819, loss 0.034594, acc 1
2016-09-06T02:27:05.755811: step 11820, loss 0.0169618, acc 1
2016-09-06T02:27:06.571303: step 11821, loss 0.0132159, acc 1
2016-09-06T02:27:07.370638: step 11822, loss 0.0174866, acc 1
2016-09-06T02:27:08.175609: step 11823, loss 0.00839586, acc 1
2016-09-06T02:27:08.997639: step 11824, loss 0.168787, acc 0.96
2016-09-06T02:27:09.807619: step 11825, loss 0.0104376, acc 1
2016-09-06T02:27:10.628049: step 11826, loss 0.0217818, acc 1
2016-09-06T02:27:11.486918: step 11827, loss 0.015981, acc 1
2016-09-06T02:27:12.295511: step 11828, loss 0.0132216, acc 1
2016-09-06T02:27:13.090041: step 11829, loss 0.0519759, acc 0.98
2016-09-06T02:27:13.929381: step 11830, loss 0.00241169, acc 1
2016-09-06T02:27:14.780816: step 11831, loss 0.0418169, acc 0.98
2016-09-06T02:27:15.606207: step 11832, loss 0.00885913, acc 1
2016-09-06T02:27:16.446774: step 11833, loss 0.00198185, acc 1
2016-09-06T02:27:17.289061: step 11834, loss 0.00190403, acc 1
2016-09-06T02:27:18.101096: step 11835, loss 0.0132819, acc 1
2016-09-06T02:27:18.939337: step 11836, loss 0.00867438, acc 1
2016-09-06T02:27:19.779240: step 11837, loss 0.0385858, acc 0.98
2016-09-06T02:27:20.579410: step 11838, loss 0.00865516, acc 1
2016-09-06T02:27:21.378568: step 11839, loss 0.00450026, acc 1
2016-09-06T02:27:22.181692: step 11840, loss 0.0531594, acc 0.98
2016-09-06T02:27:22.966118: step 11841, loss 0.00300726, acc 1
2016-09-06T02:27:23.788360: step 11842, loss 0.00252524, acc 1
2016-09-06T02:27:24.592760: step 11843, loss 0.0383943, acc 0.96
2016-09-06T02:27:25.412953: step 11844, loss 0.00535765, acc 1
2016-09-06T02:27:26.226853: step 11845, loss 0.0196156, acc 1
2016-09-06T02:27:27.036049: step 11846, loss 0.0333408, acc 1
2016-09-06T02:27:27.807407: step 11847, loss 0.0186959, acc 1
2016-09-06T02:27:28.626881: step 11848, loss 0.0532606, acc 0.98
2016-09-06T02:27:29.433206: step 11849, loss 0.00211574, acc 1
2016-09-06T02:27:30.233086: step 11850, loss 0.019687, acc 1
2016-09-06T02:27:31.068716: step 11851, loss 0.00407786, acc 1
2016-09-06T02:27:31.895886: step 11852, loss 0.00240772, acc 1
2016-09-06T02:27:32.696142: step 11853, loss 0.0151006, acc 1
2016-09-06T02:27:33.552700: step 11854, loss 0.00436929, acc 1
2016-09-06T02:27:34.444961: step 11855, loss 0.0303639, acc 0.98
2016-09-06T02:27:35.261072: step 11856, loss 0.0148826, acc 1
2016-09-06T02:27:36.035914: step 11857, loss 0.00322168, acc 1
2016-09-06T02:27:36.869441: step 11858, loss 0.0374867, acc 0.98
2016-09-06T02:27:37.665942: step 11859, loss 0.0178852, acc 1
2016-09-06T02:27:38.486529: step 11860, loss 0.00539245, acc 1
2016-09-06T02:27:39.306552: step 11861, loss 0.018644, acc 0.98
2016-09-06T02:27:40.120745: step 11862, loss 0.013959, acc 1
2016-09-06T02:27:40.924616: step 11863, loss 0.00275938, acc 1
2016-09-06T02:27:41.742377: step 11864, loss 0.0373498, acc 0.98
2016-09-06T02:27:42.553216: step 11865, loss 0.00229384, acc 1
2016-09-06T02:27:43.359299: step 11866, loss 0.00257277, acc 1
2016-09-06T02:27:44.190988: step 11867, loss 0.00229987, acc 1
2016-09-06T02:27:45.018984: step 11868, loss 0.0199388, acc 0.98
2016-09-06T02:27:45.848424: step 11869, loss 0.0171058, acc 0.98
2016-09-06T02:27:46.661913: step 11870, loss 0.00245006, acc 1
2016-09-06T02:27:47.475865: step 11871, loss 0.00838568, acc 1
2016-09-06T02:27:48.289666: step 11872, loss 0.076987, acc 0.98
2016-09-06T02:27:49.116443: step 11873, loss 0.0123846, acc 1
2016-09-06T02:27:49.960854: step 11874, loss 0.00330704, acc 1
2016-09-06T02:27:50.746990: step 11875, loss 0.00755806, acc 1
2016-09-06T02:27:51.571006: step 11876, loss 0.0250349, acc 0.98
2016-09-06T02:27:52.380687: step 11877, loss 0.00602974, acc 1
2016-09-06T02:27:53.199825: step 11878, loss 0.009139, acc 1
2016-09-06T02:27:54.018611: step 11879, loss 0.00260212, acc 1
2016-09-06T02:27:54.820413: step 11880, loss 0.036688, acc 0.98
2016-09-06T02:27:55.617714: step 11881, loss 0.129412, acc 0.96
2016-09-06T02:27:56.412244: step 11882, loss 0.00409733, acc 1
2016-09-06T02:27:57.267077: step 11883, loss 0.0967444, acc 0.96
2016-09-06T02:27:58.051648: step 11884, loss 0.0025054, acc 1
2016-09-06T02:27:58.846506: step 11885, loss 0.00516744, acc 1
2016-09-06T02:27:59.677679: step 11886, loss 0.0207988, acc 1
2016-09-06T02:28:00.496837: step 11887, loss 0.0250753, acc 1
2016-09-06T02:28:01.287135: step 11888, loss 0.00902321, acc 1
2016-09-06T02:28:02.106642: step 11889, loss 0.00784604, acc 1
2016-09-06T02:28:02.908631: step 11890, loss 0.0233591, acc 1
2016-09-06T02:28:03.724016: step 11891, loss 0.0255508, acc 1
2016-09-06T02:28:04.525742: step 11892, loss 0.0235393, acc 1
2016-09-06T02:28:05.365687: step 11893, loss 0.0183282, acc 1
2016-09-06T02:28:06.156524: step 11894, loss 0.003232, acc 1
2016-09-06T02:28:06.985243: step 11895, loss 0.00309117, acc 1
2016-09-06T02:28:07.825555: step 11896, loss 0.0437872, acc 0.98
2016-09-06T02:28:08.624286: step 11897, loss 0.00357321, acc 1
2016-09-06T02:28:09.458525: step 11898, loss 0.0206325, acc 0.98
2016-09-06T02:28:10.257804: step 11899, loss 0.0285884, acc 0.98
2016-09-06T02:28:11.057894: step 11900, loss 0.00323437, acc 1

Evaluation:
2016-09-06T02:28:14.824083: step 11900, loss 2.55454, acc 0.712946

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-11900

2016-09-06T02:28:16.840739: step 11901, loss 0.0554452, acc 0.96
2016-09-06T02:28:17.661979: step 11902, loss 0.00661877, acc 1
2016-09-06T02:28:18.498514: step 11903, loss 0.015477, acc 1
2016-09-06T02:28:19.240628: step 11904, loss 0.00328158, acc 1
2016-09-06T02:28:20.063607: step 11905, loss 0.0130207, acc 1
2016-09-06T02:28:20.885539: step 11906, loss 0.0249711, acc 0.98
2016-09-06T02:28:21.721060: step 11907, loss 0.003961, acc 1
2016-09-06T02:28:22.525048: step 11908, loss 0.00646304, acc 1
2016-09-06T02:28:23.328662: step 11909, loss 0.00359976, acc 1
2016-09-06T02:28:24.150022: step 11910, loss 0.00354537, acc 1
2016-09-06T02:28:24.943597: step 11911, loss 0.00352885, acc 1
2016-09-06T02:28:25.747152: step 11912, loss 0.0471484, acc 0.98
2016-09-06T02:28:26.573814: step 11913, loss 0.0120035, acc 1
2016-09-06T02:28:27.400420: step 11914, loss 0.00366686, acc 1
2016-09-06T02:28:28.198239: step 11915, loss 0.0031641, acc 1
2016-09-06T02:28:29.028767: step 11916, loss 0.00838421, acc 1
2016-09-06T02:28:29.868493: step 11917, loss 0.00371364, acc 1
2016-09-06T02:28:30.679118: step 11918, loss 0.00329813, acc 1
2016-09-06T02:28:31.521295: step 11919, loss 0.0113679, acc 1
2016-09-06T02:28:32.351683: step 11920, loss 0.0226937, acc 1
2016-09-06T02:28:33.132103: step 11921, loss 0.0468388, acc 0.96
2016-09-06T02:28:33.929839: step 11922, loss 0.00695103, acc 1
2016-09-06T02:28:34.746215: step 11923, loss 0.00502288, acc 1
2016-09-06T02:28:35.526648: step 11924, loss 0.0169185, acc 1
2016-09-06T02:28:36.327834: step 11925, loss 0.00335351, acc 1
2016-09-06T02:28:37.125444: step 11926, loss 0.0032099, acc 1
2016-09-06T02:28:37.925761: step 11927, loss 0.00292494, acc 1
2016-09-06T02:28:38.763177: step 11928, loss 0.00816841, acc 1
2016-09-06T02:28:39.597155: step 11929, loss 0.0200304, acc 1
2016-09-06T02:28:40.379619: step 11930, loss 0.0548486, acc 0.98
2016-09-06T02:28:41.196178: step 11931, loss 0.080888, acc 0.98
2016-09-06T02:28:42.018843: step 11932, loss 0.00633789, acc 1
2016-09-06T02:28:42.828104: step 11933, loss 0.0211026, acc 0.98
2016-09-06T02:28:43.665777: step 11934, loss 0.0193446, acc 0.98
2016-09-06T02:28:44.478377: step 11935, loss 0.0156301, acc 1
2016-09-06T02:28:45.283903: step 11936, loss 0.00766635, acc 1
2016-09-06T02:28:46.090754: step 11937, loss 0.00637203, acc 1
2016-09-06T02:28:46.909758: step 11938, loss 0.0382636, acc 0.98
2016-09-06T02:28:47.699206: step 11939, loss 0.00471968, acc 1
2016-09-06T02:28:48.471266: step 11940, loss 0.0300764, acc 1
2016-09-06T02:28:49.309210: step 11941, loss 0.0545325, acc 0.98
2016-09-06T02:28:50.085125: step 11942, loss 0.0275133, acc 1
2016-09-06T02:28:50.876886: step 11943, loss 0.172379, acc 0.94
2016-09-06T02:28:51.706111: step 11944, loss 0.00257705, acc 1
2016-09-06T02:28:52.480141: step 11945, loss 0.0247219, acc 1
2016-09-06T02:28:53.275752: step 11946, loss 0.0327069, acc 0.98
2016-09-06T02:28:54.108483: step 11947, loss 0.0479877, acc 0.98
2016-09-06T02:28:54.889327: step 11948, loss 0.00295605, acc 1
2016-09-06T02:28:55.687054: step 11949, loss 0.00329835, acc 1
2016-09-06T02:28:56.567531: step 11950, loss 0.0185183, acc 1
2016-09-06T02:28:57.370458: step 11951, loss 0.0276276, acc 0.98
2016-09-06T02:28:58.149448: step 11952, loss 0.00687841, acc 1
2016-09-06T02:28:58.957424: step 11953, loss 0.0480872, acc 0.98
2016-09-06T02:28:59.745715: step 11954, loss 0.00799123, acc 1
2016-09-06T02:29:00.597010: step 11955, loss 0.0216925, acc 0.98
2016-09-06T02:29:01.404408: step 11956, loss 0.0102239, acc 1
2016-09-06T02:29:02.169379: step 11957, loss 0.0369163, acc 0.98
2016-09-06T02:29:03.002424: step 11958, loss 0.0180726, acc 1
2016-09-06T02:29:03.814310: step 11959, loss 0.00420002, acc 1
2016-09-06T02:29:04.587933: step 11960, loss 0.0152223, acc 1
2016-09-06T02:29:05.420722: step 11961, loss 0.0360553, acc 0.98
2016-09-06T02:29:06.241500: step 11962, loss 0.0474296, acc 0.98
2016-09-06T02:29:07.031303: step 11963, loss 0.00359513, acc 1
2016-09-06T02:29:07.817695: step 11964, loss 0.0031547, acc 1
2016-09-06T02:29:08.610360: step 11965, loss 0.0386095, acc 0.98
2016-09-06T02:29:09.387681: step 11966, loss 0.0216002, acc 0.98
2016-09-06T02:29:10.225397: step 11967, loss 0.0119759, acc 1
2016-09-06T02:29:11.023262: step 11968, loss 0.026807, acc 0.98
2016-09-06T02:29:11.820356: step 11969, loss 0.0195508, acc 0.98
2016-09-06T02:29:12.618277: step 11970, loss 0.0213852, acc 0.98
2016-09-06T02:29:13.430841: step 11971, loss 0.04297, acc 0.96
2016-09-06T02:29:14.216786: step 11972, loss 0.0450974, acc 0.98
2016-09-06T02:29:15.064592: step 11973, loss 0.00806855, acc 1
2016-09-06T02:29:15.911901: step 11974, loss 0.0166377, acc 1
2016-09-06T02:29:16.692016: step 11975, loss 0.0205215, acc 0.98
2016-09-06T02:29:17.487394: step 11976, loss 0.00311336, acc 1
2016-09-06T02:29:18.324516: step 11977, loss 0.0199166, acc 0.98
2016-09-06T02:29:19.110875: step 11978, loss 0.0123278, acc 1
2016-09-06T02:29:19.925446: step 11979, loss 0.00992726, acc 1
2016-09-06T02:29:20.712828: step 11980, loss 0.0173841, acc 0.98
2016-09-06T02:29:21.493459: step 11981, loss 0.00276184, acc 1
2016-09-06T02:29:22.324358: step 11982, loss 0.0835688, acc 0.94
2016-09-06T02:29:23.161887: step 11983, loss 0.042596, acc 0.96
2016-09-06T02:29:23.936430: step 11984, loss 0.0287542, acc 0.98
2016-09-06T02:29:24.769273: step 11985, loss 0.00270718, acc 1
2016-09-06T02:29:25.582714: step 11986, loss 0.0151074, acc 1
2016-09-06T02:29:26.356514: step 11987, loss 0.00501013, acc 1
2016-09-06T02:29:27.120738: step 11988, loss 0.00308355, acc 1
2016-09-06T02:29:27.937034: step 11989, loss 0.00322829, acc 1
2016-09-06T02:29:28.742549: step 11990, loss 0.0464662, acc 0.96
2016-09-06T02:29:29.547636: step 11991, loss 0.00282884, acc 1
2016-09-06T02:29:30.370556: step 11992, loss 0.00265364, acc 1
2016-09-06T02:29:31.170811: step 11993, loss 0.0152718, acc 1
2016-09-06T02:29:31.973311: step 11994, loss 0.00252219, acc 1
2016-09-06T02:29:32.769607: step 11995, loss 0.031919, acc 0.98
2016-09-06T02:29:33.574797: step 11996, loss 0.00373137, acc 1
2016-09-06T02:29:34.376551: step 11997, loss 0.00718579, acc 1
2016-09-06T02:29:35.191402: step 11998, loss 0.00483773, acc 1
2016-09-06T02:29:35.967190: step 11999, loss 0.0149941, acc 1
2016-09-06T02:29:36.776915: step 12000, loss 0.0168757, acc 0.98

Evaluation:
2016-09-06T02:29:40.458548: step 12000, loss 2.55625, acc 0.721388

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-12000

2016-09-06T02:29:42.361360: step 12001, loss 0.00236568, acc 1
2016-09-06T02:29:43.177627: step 12002, loss 0.0161644, acc 1
2016-09-06T02:29:44.003862: step 12003, loss 0.00916112, acc 1
2016-09-06T02:29:44.823687: step 12004, loss 0.0387865, acc 0.98
2016-09-06T02:29:45.649158: step 12005, loss 0.0110891, acc 1
2016-09-06T02:29:46.468824: step 12006, loss 0.00310357, acc 1
2016-09-06T02:29:47.264476: step 12007, loss 0.0132878, acc 1
2016-09-06T02:29:48.052061: step 12008, loss 0.0023212, acc 1
2016-09-06T02:29:48.880921: step 12009, loss 0.0230422, acc 0.98
2016-09-06T02:29:49.702938: step 12010, loss 0.0023275, acc 1
2016-09-06T02:29:50.528649: step 12011, loss 0.0228611, acc 0.98
2016-09-06T02:29:51.340663: step 12012, loss 0.0181665, acc 0.98
2016-09-06T02:29:52.145563: step 12013, loss 0.0296672, acc 0.98
2016-09-06T02:29:52.929443: step 12014, loss 0.00701881, acc 1
2016-09-06T02:29:53.745129: step 12015, loss 0.0021409, acc 1
2016-09-06T02:29:54.561618: step 12016, loss 0.0276221, acc 0.98
2016-09-06T02:29:55.344033: step 12017, loss 0.0160626, acc 1
2016-09-06T02:29:56.135454: step 12018, loss 0.00217397, acc 1
2016-09-06T02:29:56.972798: step 12019, loss 0.0149524, acc 1
2016-09-06T02:29:57.755538: step 12020, loss 0.0285781, acc 0.98
2016-09-06T02:29:58.594832: step 12021, loss 0.00214578, acc 1
2016-09-06T02:29:59.425330: step 12022, loss 0.0944932, acc 0.98
2016-09-06T02:30:00.220146: step 12023, loss 0.0252741, acc 0.98
2016-09-06T02:30:01.025325: step 12024, loss 0.00778122, acc 1
2016-09-06T02:30:01.835763: step 12025, loss 0.00222846, acc 1
2016-09-06T02:30:02.638205: step 12026, loss 0.00204121, acc 1
2016-09-06T02:30:03.438391: step 12027, loss 0.00195861, acc 1
2016-09-06T02:30:04.249328: step 12028, loss 0.00280746, acc 1
2016-09-06T02:30:05.041237: step 12029, loss 0.0213749, acc 0.98
2016-09-06T02:30:05.864541: step 12030, loss 0.0223536, acc 0.98
2016-09-06T02:30:06.708789: step 12031, loss 0.0335586, acc 0.96
2016-09-06T02:30:07.481376: step 12032, loss 0.0553541, acc 0.98
2016-09-06T02:30:08.275101: step 12033, loss 0.017605, acc 1
2016-09-06T02:30:09.115026: step 12034, loss 0.0201011, acc 0.98
2016-09-06T02:30:09.890101: step 12035, loss 0.0472555, acc 0.96
2016-09-06T02:30:10.717640: step 12036, loss 0.00422354, acc 1
2016-09-06T02:30:11.545325: step 12037, loss 0.036254, acc 0.98
2016-09-06T02:30:12.334936: step 12038, loss 0.0689415, acc 0.96
2016-09-06T02:30:13.145958: step 12039, loss 0.012753, acc 1
2016-09-06T02:30:13.968977: step 12040, loss 0.00956694, acc 1
2016-09-06T02:30:14.755060: step 12041, loss 0.00497876, acc 1
2016-09-06T02:30:15.517277: step 12042, loss 0.0320467, acc 0.98
2016-09-06T02:30:16.354142: step 12043, loss 0.001836, acc 1
2016-09-06T02:30:17.120079: step 12044, loss 0.00243586, acc 1
2016-09-06T02:30:17.911051: step 12045, loss 0.0682978, acc 0.96
2016-09-06T02:30:18.727565: step 12046, loss 0.00254949, acc 1
2016-09-06T02:30:19.520214: step 12047, loss 0.00257616, acc 1
2016-09-06T02:30:20.312709: step 12048, loss 0.0266165, acc 0.98
2016-09-06T02:30:21.131760: step 12049, loss 0.00187021, acc 1
2016-09-06T02:30:21.907118: step 12050, loss 0.0691478, acc 0.98
2016-09-06T02:30:22.736155: step 12051, loss 0.00257442, acc 1
2016-09-06T02:30:23.565355: step 12052, loss 0.00660729, acc 1
2016-09-06T02:30:24.336025: step 12053, loss 0.0170633, acc 0.98
2016-09-06T02:30:25.139784: step 12054, loss 0.0102355, acc 1
2016-09-06T02:30:25.939334: step 12055, loss 0.00275794, acc 1
2016-09-06T02:30:26.758806: step 12056, loss 0.00193192, acc 1
2016-09-06T02:30:27.609102: step 12057, loss 0.0079046, acc 1
2016-09-06T02:30:28.445454: step 12058, loss 0.00210875, acc 1
2016-09-06T02:30:29.230544: step 12059, loss 0.00384334, acc 1
2016-09-06T02:30:30.019844: step 12060, loss 0.0264245, acc 0.98
2016-09-06T02:30:30.828216: step 12061, loss 0.0246816, acc 0.98
2016-09-06T02:30:31.655126: step 12062, loss 0.0231749, acc 1
2016-09-06T02:30:32.489244: step 12063, loss 0.00305002, acc 1
2016-09-06T02:30:33.302763: step 12064, loss 0.0185062, acc 0.98
2016-09-06T02:30:34.072218: step 12065, loss 0.0163357, acc 1
2016-09-06T02:30:34.865555: step 12066, loss 0.00529817, acc 1
2016-09-06T02:30:35.702070: step 12067, loss 0.00808759, acc 1
2016-09-06T02:30:36.490032: step 12068, loss 0.00210821, acc 1
2016-09-06T02:30:37.286044: step 12069, loss 0.00201125, acc 1
2016-09-06T02:30:38.105443: step 12070, loss 0.0287241, acc 0.98
2016-09-06T02:30:38.901459: step 12071, loss 0.0190478, acc 0.98
2016-09-06T02:30:39.719031: step 12072, loss 0.0123169, acc 1
2016-09-06T02:30:40.518347: step 12073, loss 0.0192645, acc 1
2016-09-06T02:30:41.285847: step 12074, loss 0.0397657, acc 0.98
2016-09-06T02:30:42.085379: step 12075, loss 0.0172613, acc 0.98
2016-09-06T02:30:42.914574: step 12076, loss 0.00378535, acc 1
2016-09-06T02:30:43.727436: step 12077, loss 0.00236325, acc 1
2016-09-06T02:30:44.524583: step 12078, loss 0.0352457, acc 0.98
2016-09-06T02:30:45.324162: step 12079, loss 0.0127913, acc 1
2016-09-06T02:30:46.147371: step 12080, loss 0.00248177, acc 1
2016-09-06T02:30:46.949279: step 12081, loss 0.0468419, acc 0.98
2016-09-06T02:30:47.774957: step 12082, loss 0.047714, acc 0.98
2016-09-06T02:30:48.531238: step 12083, loss 0.00605162, acc 1
2016-09-06T02:30:49.343776: step 12084, loss 0.00299869, acc 1
2016-09-06T02:30:50.139537: step 12085, loss 0.00250704, acc 1
2016-09-06T02:30:50.914006: step 12086, loss 0.00544306, acc 1
2016-09-06T02:30:51.729995: step 12087, loss 0.00237486, acc 1
2016-09-06T02:30:52.547356: step 12088, loss 0.0434072, acc 0.98
2016-09-06T02:30:53.337667: step 12089, loss 0.0282662, acc 0.98
2016-09-06T02:30:54.146884: step 12090, loss 0.0101169, acc 1
2016-09-06T02:30:54.981200: step 12091, loss 0.0212625, acc 1
2016-09-06T02:30:55.743305: step 12092, loss 0.0155981, acc 1
2016-09-06T02:30:56.589746: step 12093, loss 0.0334786, acc 0.98
2016-09-06T02:30:57.425989: step 12094, loss 0.00868344, acc 1
2016-09-06T02:30:58.206114: step 12095, loss 0.00881424, acc 1
2016-09-06T02:30:58.946961: step 12096, loss 0.0647527, acc 0.977273
2016-09-06T02:30:59.737983: step 12097, loss 0.00184618, acc 1
2016-09-06T02:31:00.577197: step 12098, loss 0.0340129, acc 1
2016-09-06T02:31:01.408945: step 12099, loss 0.0293486, acc 0.98
2016-09-06T02:31:02.213247: step 12100, loss 0.00295866, acc 1

Evaluation:
2016-09-06T02:31:05.966330: step 12100, loss 2.59015, acc 0.717636

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-12100

2016-09-06T02:31:07.861667: step 12101, loss 0.00179442, acc 1
2016-09-06T02:31:08.680230: step 12102, loss 0.0033362, acc 1
2016-09-06T02:31:09.498709: step 12103, loss 0.00175377, acc 1
2016-09-06T02:31:10.328732: step 12104, loss 0.00181945, acc 1
2016-09-06T02:31:11.146143: step 12105, loss 0.0208037, acc 0.98
2016-09-06T02:31:11.945213: step 12106, loss 0.00172989, acc 1
2016-09-06T02:31:12.713463: step 12107, loss 0.0347759, acc 0.98
2016-09-06T02:31:13.523640: step 12108, loss 0.00881746, acc 1
2016-09-06T02:31:14.361427: step 12109, loss 0.00176071, acc 1
2016-09-06T02:31:15.156580: step 12110, loss 0.00182911, acc 1
2016-09-06T02:31:15.944301: step 12111, loss 0.00167763, acc 1
2016-09-06T02:31:16.789813: step 12112, loss 0.0330627, acc 1
2016-09-06T02:31:17.551570: step 12113, loss 0.0016921, acc 1
2016-09-06T02:31:18.333008: step 12114, loss 0.00706717, acc 1
2016-09-06T02:31:19.154072: step 12115, loss 0.0273813, acc 0.98
2016-09-06T02:31:19.920532: step 12116, loss 0.0167558, acc 0.98
2016-09-06T02:31:20.736730: step 12117, loss 0.0115905, acc 1
2016-09-06T02:31:21.537725: step 12118, loss 0.0605903, acc 0.96
2016-09-06T02:31:22.360770: step 12119, loss 0.00483076, acc 1
2016-09-06T02:31:23.185180: step 12120, loss 0.0223078, acc 1
2016-09-06T02:31:24.003465: step 12121, loss 0.0494274, acc 0.98
2016-09-06T02:31:24.801552: step 12122, loss 0.04191, acc 0.96
2016-09-06T02:31:25.599271: step 12123, loss 0.00174785, acc 1
2016-09-06T02:31:26.418072: step 12124, loss 0.00177606, acc 1
2016-09-06T02:31:27.216186: step 12125, loss 0.0167221, acc 0.98
2016-09-06T02:31:28.021500: step 12126, loss 0.022455, acc 0.98
2016-09-06T02:31:28.869634: step 12127, loss 0.0223817, acc 0.98
2016-09-06T02:31:29.660721: step 12128, loss 0.00206652, acc 1
2016-09-06T02:31:30.485726: step 12129, loss 0.00230722, acc 1
2016-09-06T02:31:31.311416: step 12130, loss 0.00886666, acc 1
2016-09-06T02:31:32.105189: step 12131, loss 0.00150316, acc 1
2016-09-06T02:31:32.919632: step 12132, loss 0.00730613, acc 1
2016-09-06T02:31:33.730263: step 12133, loss 0.00182242, acc 1
2016-09-06T02:31:34.537327: step 12134, loss 0.013914, acc 1
2016-09-06T02:31:35.329295: step 12135, loss 0.0103499, acc 1
2016-09-06T02:31:36.139483: step 12136, loss 0.00150161, acc 1
2016-09-06T02:31:36.946535: step 12137, loss 0.0244183, acc 1
2016-09-06T02:31:37.801936: step 12138, loss 0.00172649, acc 1
2016-09-06T02:31:38.643592: step 12139, loss 0.0291481, acc 0.98
2016-09-06T02:31:39.472852: step 12140, loss 0.0035878, acc 1
2016-09-06T02:31:40.272660: step 12141, loss 0.00273614, acc 1
2016-09-06T02:31:41.101103: step 12142, loss 0.0453342, acc 0.98
2016-09-06T02:31:41.932021: step 12143, loss 0.0589635, acc 0.96
2016-09-06T02:31:42.766218: step 12144, loss 0.00368035, acc 1
2016-09-06T02:31:43.592104: step 12145, loss 0.0399966, acc 0.96
2016-09-06T02:31:44.408246: step 12146, loss 0.00152669, acc 1
2016-09-06T02:31:45.215617: step 12147, loss 0.05049, acc 0.98
2016-09-06T02:31:46.031445: step 12148, loss 0.0425587, acc 0.98
2016-09-06T02:31:46.852493: step 12149, loss 0.00992623, acc 1
2016-09-06T02:31:47.668937: step 12150, loss 0.0504462, acc 0.98
2016-09-06T02:31:48.513414: step 12151, loss 0.0361807, acc 0.98
2016-09-06T02:31:49.318046: step 12152, loss 0.0065801, acc 1
2016-09-06T02:31:50.127917: step 12153, loss 0.00157657, acc 1
2016-09-06T02:31:50.953262: step 12154, loss 0.0304743, acc 0.98
2016-09-06T02:31:51.786374: step 12155, loss 0.00165748, acc 1
2016-09-06T02:31:52.576192: step 12156, loss 0.00171682, acc 1
2016-09-06T02:31:53.375873: step 12157, loss 0.0142695, acc 1
2016-09-06T02:31:54.190259: step 12158, loss 0.00431647, acc 1
2016-09-06T02:31:54.992638: step 12159, loss 0.0043096, acc 1
2016-09-06T02:31:55.802113: step 12160, loss 0.00798573, acc 1
2016-09-06T02:31:56.613718: step 12161, loss 0.00248241, acc 1
2016-09-06T02:31:57.404656: step 12162, loss 0.0348279, acc 0.96
2016-09-06T02:31:58.194670: step 12163, loss 0.00731812, acc 1
2016-09-06T02:31:59.020323: step 12164, loss 0.0427166, acc 0.98
2016-09-06T02:31:59.817457: step 12165, loss 0.0103347, acc 1
2016-09-06T02:32:00.669500: step 12166, loss 0.00229681, acc 1
2016-09-06T02:32:01.492452: step 12167, loss 0.0364247, acc 0.98
2016-09-06T02:32:02.257264: step 12168, loss 0.0188176, acc 0.98
2016-09-06T02:32:03.058891: step 12169, loss 0.02308, acc 0.98
2016-09-06T02:32:03.890964: step 12170, loss 0.0208451, acc 0.98
2016-09-06T02:32:04.700984: step 12171, loss 0.0329938, acc 0.98
2016-09-06T02:32:05.484080: step 12172, loss 0.0167725, acc 1
2016-09-06T02:32:06.309445: step 12173, loss 0.00396673, acc 1
2016-09-06T02:32:07.063989: step 12174, loss 0.0611997, acc 0.98
2016-09-06T02:32:07.875594: step 12175, loss 0.00456394, acc 1
2016-09-06T02:32:08.707029: step 12176, loss 0.00541408, acc 1
2016-09-06T02:32:09.512668: step 12177, loss 0.00182884, acc 1
2016-09-06T02:32:10.304165: step 12178, loss 0.0421135, acc 0.98
2016-09-06T02:32:11.138272: step 12179, loss 0.0350156, acc 0.96
2016-09-06T02:32:11.929252: step 12180, loss 0.00152403, acc 1
2016-09-06T02:32:12.765800: step 12181, loss 0.0146053, acc 1
2016-09-06T02:32:13.577630: step 12182, loss 0.0173362, acc 0.98
2016-09-06T02:32:14.352168: step 12183, loss 0.00154604, acc 1
2016-09-06T02:32:15.179465: step 12184, loss 0.0080888, acc 1
2016-09-06T02:32:15.992672: step 12185, loss 0.0253263, acc 0.98
2016-09-06T02:32:16.750221: step 12186, loss 0.00182232, acc 1
2016-09-06T02:32:17.558609: step 12187, loss 0.0155273, acc 1
2016-09-06T02:32:18.360386: step 12188, loss 0.0018817, acc 1
2016-09-06T02:32:19.143711: step 12189, loss 0.0142306, acc 1
2016-09-06T02:32:19.949558: step 12190, loss 0.0155141, acc 1
2016-09-06T02:32:20.773188: step 12191, loss 0.0082684, acc 1
2016-09-06T02:32:21.581667: step 12192, loss 0.00816926, acc 1
2016-09-06T02:32:22.376691: step 12193, loss 0.00170713, acc 1
2016-09-06T02:32:23.189393: step 12194, loss 0.0243551, acc 1
2016-09-06T02:32:23.969609: step 12195, loss 0.00232951, acc 1
2016-09-06T02:32:24.787441: step 12196, loss 0.00787499, acc 1
2016-09-06T02:32:25.602399: step 12197, loss 0.0277573, acc 0.98
2016-09-06T02:32:26.410032: step 12198, loss 0.00241773, acc 1
2016-09-06T02:32:27.205912: step 12199, loss 0.00179472, acc 1
2016-09-06T02:32:28.005104: step 12200, loss 0.00719289, acc 1

Evaluation:
2016-09-06T02:32:31.707025: step 12200, loss 2.78364, acc 0.71576

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-12200

2016-09-06T02:32:33.537469: step 12201, loss 0.015161, acc 1
2016-09-06T02:32:34.363673: step 12202, loss 0.0356788, acc 0.98
2016-09-06T02:32:35.165102: step 12203, loss 0.00270042, acc 1
2016-09-06T02:32:35.973709: step 12204, loss 0.0198496, acc 1
2016-09-06T02:32:36.785986: step 12205, loss 0.0198808, acc 1
2016-09-06T02:32:37.587032: step 12206, loss 0.00251872, acc 1
2016-09-06T02:32:38.419085: step 12207, loss 0.00211831, acc 1
2016-09-06T02:32:39.273426: step 12208, loss 0.0249986, acc 1
2016-09-06T02:32:40.078135: step 12209, loss 0.0447629, acc 0.96
2016-09-06T02:32:40.881404: step 12210, loss 0.0127566, acc 1
2016-09-06T02:32:41.692759: step 12211, loss 0.0364095, acc 0.98
2016-09-06T02:32:42.490657: step 12212, loss 0.0023005, acc 1
2016-09-06T02:32:43.287479: step 12213, loss 0.0618403, acc 0.98
2016-09-06T02:32:44.110738: step 12214, loss 0.00221482, acc 1
2016-09-06T02:32:44.925245: step 12215, loss 0.00281124, acc 1
2016-09-06T02:32:45.730900: step 12216, loss 0.0244921, acc 0.98
2016-09-06T02:32:46.533235: step 12217, loss 0.00276206, acc 1
2016-09-06T02:32:47.324666: step 12218, loss 0.00565829, acc 1
2016-09-06T02:32:48.139786: step 12219, loss 0.0115497, acc 1
2016-09-06T02:32:48.978770: step 12220, loss 0.00279892, acc 1
2016-09-06T02:32:49.828196: step 12221, loss 0.0199978, acc 1
2016-09-06T02:32:50.648970: step 12222, loss 0.00215567, acc 1
2016-09-06T02:32:51.424315: step 12223, loss 0.00784039, acc 1
2016-09-06T02:32:52.271909: step 12224, loss 0.0127461, acc 1
2016-09-06T02:32:53.048988: step 12225, loss 0.00683412, acc 1
2016-09-06T02:32:53.827009: step 12226, loss 0.0159915, acc 1
2016-09-06T02:32:54.635536: step 12227, loss 0.00569111, acc 1
2016-09-06T02:32:55.408594: step 12228, loss 0.00434083, acc 1
2016-09-06T02:32:56.244999: step 12229, loss 0.0233297, acc 0.98
2016-09-06T02:32:57.062062: step 12230, loss 0.0862727, acc 0.98
2016-09-06T02:32:57.885406: step 12231, loss 0.00976201, acc 1
2016-09-06T02:32:58.674566: step 12232, loss 0.107746, acc 0.98
2016-09-06T02:32:59.476404: step 12233, loss 0.005737, acc 1
2016-09-06T02:33:00.305401: step 12234, loss 0.0103131, acc 1
2016-09-06T02:33:01.113447: step 12235, loss 0.00197436, acc 1
2016-09-06T02:33:01.939776: step 12236, loss 0.0025989, acc 1
2016-09-06T02:33:02.753794: step 12237, loss 0.00293323, acc 1
2016-09-06T02:33:03.563512: step 12238, loss 0.00393736, acc 1
2016-09-06T02:33:04.378867: step 12239, loss 0.070146, acc 0.98
2016-09-06T02:33:05.181592: step 12240, loss 0.0327652, acc 0.98
2016-09-06T02:33:06.009065: step 12241, loss 0.0029927, acc 1
2016-09-06T02:33:06.816979: step 12242, loss 0.0183715, acc 0.98
2016-09-06T02:33:07.602329: step 12243, loss 0.00828295, acc 1
2016-09-06T02:33:08.418909: step 12244, loss 0.0200426, acc 1
2016-09-06T02:33:09.247947: step 12245, loss 0.00987198, acc 1
2016-09-06T02:33:10.059199: step 12246, loss 0.0548949, acc 0.98
2016-09-06T02:33:10.887444: step 12247, loss 0.00366411, acc 1
2016-09-06T02:33:11.711178: step 12248, loss 0.010182, acc 1
2016-09-06T02:33:12.549431: step 12249, loss 0.00990556, acc 1
2016-09-06T02:33:13.389196: step 12250, loss 0.0533716, acc 0.98
2016-09-06T02:33:14.219089: step 12251, loss 0.00394249, acc 1
2016-09-06T02:33:15.041362: step 12252, loss 0.0135483, acc 1
2016-09-06T02:33:15.862172: step 12253, loss 0.0126756, acc 1
2016-09-06T02:33:16.730659: step 12254, loss 0.00266677, acc 1
2016-09-06T02:33:17.534759: step 12255, loss 0.0161199, acc 1
2016-09-06T02:33:18.346337: step 12256, loss 0.0160534, acc 1
2016-09-06T02:33:19.165038: step 12257, loss 0.0340031, acc 0.98
2016-09-06T02:33:19.963857: step 12258, loss 0.00224642, acc 1
2016-09-06T02:33:20.760696: step 12259, loss 0.00364672, acc 1
2016-09-06T02:33:21.548867: step 12260, loss 0.00402283, acc 1
2016-09-06T02:33:22.372529: step 12261, loss 0.00276867, acc 1
2016-09-06T02:33:23.181600: step 12262, loss 0.0175268, acc 1
2016-09-06T02:33:23.990776: step 12263, loss 0.0215794, acc 0.98
2016-09-06T02:33:24.825053: step 12264, loss 0.00718951, acc 1
2016-09-06T02:33:25.613997: step 12265, loss 0.0659632, acc 0.96
2016-09-06T02:33:26.435305: step 12266, loss 0.0306049, acc 0.98
2016-09-06T02:33:27.247173: step 12267, loss 0.0540645, acc 0.98
2016-09-06T02:33:28.044905: step 12268, loss 0.0032469, acc 1
2016-09-06T02:33:28.852948: step 12269, loss 0.00238226, acc 1
2016-09-06T02:33:29.657364: step 12270, loss 0.038019, acc 0.98
2016-09-06T02:33:30.450030: step 12271, loss 0.0518317, acc 0.96
2016-09-06T02:33:31.269280: step 12272, loss 0.00229282, acc 1
2016-09-06T02:33:32.088007: step 12273, loss 0.111066, acc 0.96
2016-09-06T02:33:32.887050: step 12274, loss 0.0241842, acc 0.98
2016-09-06T02:33:33.691033: step 12275, loss 0.0103701, acc 1
2016-09-06T02:33:34.503692: step 12276, loss 0.00590327, acc 1
2016-09-06T02:33:35.320040: step 12277, loss 0.00246969, acc 1
2016-09-06T02:33:36.128781: step 12278, loss 0.0195029, acc 1
2016-09-06T02:33:36.945887: step 12279, loss 0.00276036, acc 1
2016-09-06T02:33:37.732113: step 12280, loss 0.0028282, acc 1
2016-09-06T02:33:38.524971: step 12281, loss 0.128084, acc 0.96
2016-09-06T02:33:39.335573: step 12282, loss 0.0406155, acc 0.96
2016-09-06T02:33:40.159179: step 12283, loss 0.05956, acc 0.94
2016-09-06T02:33:40.965589: step 12284, loss 0.00769456, acc 1
2016-09-06T02:33:41.785074: step 12285, loss 0.0146312, acc 1
2016-09-06T02:33:42.549034: step 12286, loss 0.0320404, acc 0.98
2016-09-06T02:33:43.351412: step 12287, loss 0.00277397, acc 1
2016-09-06T02:33:44.128028: step 12288, loss 0.0156004, acc 1
2016-09-06T02:33:44.930012: step 12289, loss 0.00454984, acc 1
2016-09-06T02:33:45.742218: step 12290, loss 0.021919, acc 1
2016-09-06T02:33:46.554745: step 12291, loss 0.00236957, acc 1
2016-09-06T02:33:47.346145: step 12292, loss 0.0279111, acc 0.98
2016-09-06T02:33:48.152583: step 12293, loss 0.0133807, acc 1
2016-09-06T02:33:48.971488: step 12294, loss 0.00728414, acc 1
2016-09-06T02:33:49.782614: step 12295, loss 0.00272885, acc 1
2016-09-06T02:33:50.591382: step 12296, loss 0.00323027, acc 1
2016-09-06T02:33:51.388536: step 12297, loss 0.00470113, acc 1
2016-09-06T02:33:52.187967: step 12298, loss 0.00229893, acc 1
2016-09-06T02:33:53.010552: step 12299, loss 0.0406535, acc 0.96
2016-09-06T02:33:53.830322: step 12300, loss 0.0197132, acc 0.98

Evaluation:
2016-09-06T02:33:57.559333: step 12300, loss 2.14438, acc 0.711069

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-12300

2016-09-06T02:33:59.472291: step 12301, loss 0.0192156, acc 1
2016-09-06T02:34:00.318267: step 12302, loss 0.0135427, acc 1
2016-09-06T02:34:01.156151: step 12303, loss 0.00426656, acc 1
2016-09-06T02:34:01.965940: step 12304, loss 0.0604699, acc 0.96
2016-09-06T02:34:02.783831: step 12305, loss 0.0253851, acc 0.98
2016-09-06T02:34:03.605026: step 12306, loss 0.00251522, acc 1
2016-09-06T02:34:04.388346: step 12307, loss 0.0135309, acc 1
2016-09-06T02:34:05.193417: step 12308, loss 0.00558675, acc 1
2016-09-06T02:34:05.979702: step 12309, loss 0.00242624, acc 1
2016-09-06T02:34:06.759974: step 12310, loss 0.00402722, acc 1
2016-09-06T02:34:07.558916: step 12311, loss 0.0159389, acc 1
2016-09-06T02:34:08.382355: step 12312, loss 0.0137152, acc 1
2016-09-06T02:34:09.181196: step 12313, loss 0.0102781, acc 1
2016-09-06T02:34:09.963923: step 12314, loss 0.0181241, acc 1
2016-09-06T02:34:10.794877: step 12315, loss 0.0221993, acc 1
2016-09-06T02:34:11.592153: step 12316, loss 0.00948629, acc 1
2016-09-06T02:34:12.418883: step 12317, loss 0.0102251, acc 1
2016-09-06T02:34:13.233280: step 12318, loss 0.00238949, acc 1
2016-09-06T02:34:14.016945: step 12319, loss 0.0208687, acc 1
2016-09-06T02:34:14.843399: step 12320, loss 0.0118853, acc 1
2016-09-06T02:34:15.649041: step 12321, loss 0.015665, acc 1
2016-09-06T02:34:16.411918: step 12322, loss 0.00606211, acc 1
2016-09-06T02:34:17.222517: step 12323, loss 0.00528116, acc 1
2016-09-06T02:34:18.035060: step 12324, loss 0.0240129, acc 1
2016-09-06T02:34:18.813408: step 12325, loss 0.0185431, acc 0.98
2016-09-06T02:34:19.640145: step 12326, loss 0.017244, acc 1
2016-09-06T02:34:20.472516: step 12327, loss 0.026311, acc 0.98
2016-09-06T02:34:21.241314: step 12328, loss 0.0029416, acc 1
2016-09-06T02:34:22.095821: step 12329, loss 0.0282388, acc 0.98
2016-09-06T02:34:22.908348: step 12330, loss 0.0309366, acc 0.98
2016-09-06T02:34:23.707719: step 12331, loss 0.0216072, acc 1
2016-09-06T02:34:24.509280: step 12332, loss 0.00254283, acc 1
2016-09-06T02:34:25.316037: step 12333, loss 0.00267049, acc 1
2016-09-06T02:34:26.129038: step 12334, loss 0.0149507, acc 1
2016-09-06T02:34:26.929793: step 12335, loss 0.00286165, acc 1
2016-09-06T02:34:27.796911: step 12336, loss 0.0118398, acc 1
2016-09-06T02:34:28.605617: step 12337, loss 0.116158, acc 0.98
2016-09-06T02:34:29.419913: step 12338, loss 0.00240458, acc 1
2016-09-06T02:34:30.284384: step 12339, loss 0.013189, acc 1
2016-09-06T02:34:31.079394: step 12340, loss 0.0197047, acc 1
2016-09-06T02:34:31.878619: step 12341, loss 0.00695235, acc 1
2016-09-06T02:34:32.690114: step 12342, loss 0.031633, acc 0.98
2016-09-06T02:34:33.511994: step 12343, loss 0.0131394, acc 1
2016-09-06T02:34:34.313913: step 12344, loss 0.00486237, acc 1
2016-09-06T02:34:35.134712: step 12345, loss 0.00219272, acc 1
2016-09-06T02:34:35.914962: step 12346, loss 0.0262812, acc 0.98
2016-09-06T02:34:36.738667: step 12347, loss 0.0124912, acc 1
2016-09-06T02:34:37.551094: step 12348, loss 0.00221475, acc 1
2016-09-06T02:34:38.370424: step 12349, loss 0.177058, acc 0.98
2016-09-06T02:34:39.188222: step 12350, loss 0.00458668, acc 1
2016-09-06T02:34:40.020467: step 12351, loss 0.00221351, acc 1
2016-09-06T02:34:40.808471: step 12352, loss 0.00207646, acc 1
2016-09-06T02:34:41.619314: step 12353, loss 0.0576959, acc 0.98
2016-09-06T02:34:42.439740: step 12354, loss 0.00866732, acc 1
2016-09-06T02:34:43.246669: step 12355, loss 0.00274263, acc 1
2016-09-06T02:34:44.064415: step 12356, loss 0.0257207, acc 1
2016-09-06T02:34:44.901238: step 12357, loss 0.0152419, acc 1
2016-09-06T02:34:45.717405: step 12358, loss 0.0183345, acc 1
2016-09-06T02:34:46.549717: step 12359, loss 0.0327814, acc 0.98
2016-09-06T02:34:47.383461: step 12360, loss 0.00781232, acc 1
2016-09-06T02:34:48.193189: step 12361, loss 0.00375603, acc 1
2016-09-06T02:34:49.016230: step 12362, loss 0.00208622, acc 1
2016-09-06T02:34:49.830026: step 12363, loss 0.00480326, acc 1
2016-09-06T02:34:50.652612: step 12364, loss 0.0188267, acc 0.98
2016-09-06T02:34:51.429368: step 12365, loss 0.0161953, acc 0.98
2016-09-06T02:34:52.238527: step 12366, loss 0.00753824, acc 1
2016-09-06T02:34:53.048153: step 12367, loss 0.0252457, acc 1
2016-09-06T02:34:53.812410: step 12368, loss 0.00239546, acc 1
2016-09-06T02:34:54.628368: step 12369, loss 0.00387148, acc 1
2016-09-06T02:34:55.419225: step 12370, loss 0.0975309, acc 0.96
2016-09-06T02:34:56.229601: step 12371, loss 0.00906107, acc 1
2016-09-06T02:34:57.056498: step 12372, loss 0.00237558, acc 1
2016-09-06T02:34:57.854578: step 12373, loss 0.0131477, acc 1
2016-09-06T02:34:58.627624: step 12374, loss 0.0280951, acc 0.98
2016-09-06T02:34:59.466743: step 12375, loss 0.00895736, acc 1
2016-09-06T02:35:00.350193: step 12376, loss 0.0148478, acc 1
2016-09-06T02:35:01.136732: step 12377, loss 0.00395953, acc 1
2016-09-06T02:35:01.937064: step 12378, loss 0.0024605, acc 1
2016-09-06T02:35:02.749724: step 12379, loss 0.0182923, acc 0.98
2016-09-06T02:35:03.547303: step 12380, loss 0.00243264, acc 1
2016-09-06T02:35:04.344964: step 12381, loss 0.00727972, acc 1
2016-09-06T02:35:05.159721: step 12382, loss 0.00238419, acc 1
2016-09-06T02:35:05.957262: step 12383, loss 0.0312595, acc 0.98
2016-09-06T02:35:06.776723: step 12384, loss 0.00210325, acc 1
2016-09-06T02:35:07.608323: step 12385, loss 0.00211307, acc 1
2016-09-06T02:35:08.391118: step 12386, loss 0.109561, acc 0.96
2016-09-06T02:35:09.187361: step 12387, loss 0.00261642, acc 1
2016-09-06T02:35:10.032658: step 12388, loss 0.00260203, acc 1
2016-09-06T02:35:10.803373: step 12389, loss 0.00455704, acc 1
2016-09-06T02:35:11.647426: step 12390, loss 0.00209286, acc 1
2016-09-06T02:35:12.453756: step 12391, loss 0.00225355, acc 1
2016-09-06T02:35:13.225553: step 12392, loss 0.00205175, acc 1
2016-09-06T02:35:14.041478: step 12393, loss 0.00764693, acc 1
2016-09-06T02:35:14.850675: step 12394, loss 0.00979986, acc 1
2016-09-06T02:35:15.609485: step 12395, loss 0.0125484, acc 1
2016-09-06T02:35:16.397001: step 12396, loss 0.00491715, acc 1
2016-09-06T02:35:17.219136: step 12397, loss 0.0235175, acc 1
2016-09-06T02:35:18.011103: step 12398, loss 0.0177573, acc 1
2016-09-06T02:35:18.829298: step 12399, loss 0.00251236, acc 1
2016-09-06T02:35:19.631198: step 12400, loss 0.0153576, acc 1

Evaluation:
2016-09-06T02:35:23.368996: step 12400, loss 2.4573, acc 0.719512

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-12400

2016-09-06T02:35:25.167174: step 12401, loss 0.00337235, acc 1
2016-09-06T02:35:25.992839: step 12402, loss 0.00300446, acc 1
2016-09-06T02:35:26.803936: step 12403, loss 0.0044103, acc 1
2016-09-06T02:35:27.621524: step 12404, loss 0.00335256, acc 1
2016-09-06T02:35:28.438308: step 12405, loss 0.00363432, acc 1
2016-09-06T02:35:29.269809: step 12406, loss 0.0168281, acc 0.98
2016-09-06T02:35:30.086039: step 12407, loss 0.00342244, acc 1
2016-09-06T02:35:30.942358: step 12408, loss 0.0170176, acc 0.98
2016-09-06T02:35:31.762255: step 12409, loss 0.00252863, acc 1
2016-09-06T02:35:32.555525: step 12410, loss 0.00243376, acc 1
2016-09-06T02:35:33.393164: step 12411, loss 0.00394656, acc 1
2016-09-06T02:35:34.196103: step 12412, loss 0.0209967, acc 0.98
2016-09-06T02:35:34.972714: step 12413, loss 0.0312608, acc 0.98
2016-09-06T02:35:35.761339: step 12414, loss 0.00250016, acc 1
2016-09-06T02:35:36.594897: step 12415, loss 0.0264192, acc 0.98
2016-09-06T02:35:37.376595: step 12416, loss 0.0230839, acc 0.98
2016-09-06T02:35:38.171784: step 12417, loss 0.0147016, acc 1
2016-09-06T02:35:38.977022: step 12418, loss 0.00335694, acc 1
2016-09-06T02:35:39.783441: step 12419, loss 0.0160372, acc 1
2016-09-06T02:35:40.608689: step 12420, loss 0.0155027, acc 1
2016-09-06T02:35:41.407140: step 12421, loss 0.0201631, acc 1
2016-09-06T02:35:42.183873: step 12422, loss 0.0209882, acc 0.98
2016-09-06T02:35:42.995780: step 12423, loss 0.0160731, acc 1
2016-09-06T02:35:43.809876: step 12424, loss 0.0021718, acc 1
2016-09-06T02:35:44.601346: step 12425, loss 0.0119549, acc 1
2016-09-06T02:35:45.441012: step 12426, loss 0.0223929, acc 0.98
2016-09-06T02:35:46.274836: step 12427, loss 0.0135107, acc 1
2016-09-06T02:35:47.035924: step 12428, loss 0.00226191, acc 1
2016-09-06T02:35:47.815978: step 12429, loss 0.0174176, acc 0.98
2016-09-06T02:35:48.645280: step 12430, loss 0.0117677, acc 1
2016-09-06T02:35:49.419209: step 12431, loss 0.00260608, acc 1
2016-09-06T02:35:50.227713: step 12432, loss 0.00965114, acc 1
2016-09-06T02:35:51.015997: step 12433, loss 0.00392847, acc 1
2016-09-06T02:35:51.832239: step 12434, loss 0.0160793, acc 1
2016-09-06T02:35:52.661598: step 12435, loss 0.00806383, acc 1
2016-09-06T02:35:53.496798: step 12436, loss 0.00241833, acc 1
2016-09-06T02:35:54.296968: step 12437, loss 0.00260354, acc 1
2016-09-06T02:35:55.107564: step 12438, loss 0.0140387, acc 1
2016-09-06T02:35:55.932394: step 12439, loss 0.00247135, acc 1
2016-09-06T02:35:56.705725: step 12440, loss 0.00289678, acc 1
2016-09-06T02:35:57.521090: step 12441, loss 0.00248935, acc 1
2016-09-06T02:35:58.368609: step 12442, loss 0.00260772, acc 1
2016-09-06T02:35:59.163724: step 12443, loss 0.00249669, acc 1
2016-09-06T02:35:59.949985: step 12444, loss 0.0332662, acc 0.98
2016-09-06T02:36:00.792893: step 12445, loss 0.00277851, acc 1
2016-09-06T02:36:01.570659: step 12446, loss 0.0105802, acc 1
2016-09-06T02:36:02.403415: step 12447, loss 0.0026049, acc 1
2016-09-06T02:36:03.229463: step 12448, loss 0.00720465, acc 1
2016-09-06T02:36:04.029464: step 12449, loss 0.136706, acc 0.96
2016-09-06T02:36:04.856044: step 12450, loss 0.00341023, acc 1
2016-09-06T02:36:05.719509: step 12451, loss 0.00992527, acc 1
2016-09-06T02:36:06.509423: step 12452, loss 0.00333091, acc 1
2016-09-06T02:36:07.312633: step 12453, loss 0.00223076, acc 1
2016-09-06T02:36:08.137139: step 12454, loss 0.00873371, acc 1
2016-09-06T02:36:08.923879: step 12455, loss 0.00508767, acc 1
2016-09-06T02:36:09.692474: step 12456, loss 0.0443311, acc 0.98
2016-09-06T02:36:10.550094: step 12457, loss 0.0816744, acc 0.92
2016-09-06T02:36:11.349603: step 12458, loss 0.00318403, acc 1
2016-09-06T02:36:12.125741: step 12459, loss 0.0218587, acc 1
2016-09-06T02:36:12.948270: step 12460, loss 0.057731, acc 0.98
2016-09-06T02:36:13.746212: step 12461, loss 0.00614222, acc 1
2016-09-06T02:36:14.551743: step 12462, loss 0.0206515, acc 1
2016-09-06T02:36:15.376930: step 12463, loss 0.0317317, acc 0.98
2016-09-06T02:36:16.130816: step 12464, loss 0.00191036, acc 1
2016-09-06T02:36:16.940163: step 12465, loss 0.0575021, acc 0.98
2016-09-06T02:36:17.762602: step 12466, loss 0.0399194, acc 0.96
2016-09-06T02:36:18.541100: step 12467, loss 0.0175433, acc 0.98
2016-09-06T02:36:19.350747: step 12468, loss 0.0348578, acc 0.96
2016-09-06T02:36:20.166531: step 12469, loss 0.00191215, acc 1
2016-09-06T02:36:20.953135: step 12470, loss 0.011768, acc 1
2016-09-06T02:36:21.754754: step 12471, loss 0.00317423, acc 1
2016-09-06T02:36:22.575881: step 12472, loss 0.00235176, acc 1
2016-09-06T02:36:23.346925: step 12473, loss 0.00965971, acc 1
2016-09-06T02:36:24.173687: step 12474, loss 0.015744, acc 0.98
2016-09-06T02:36:24.997566: step 12475, loss 0.0324129, acc 0.98
2016-09-06T02:36:25.801064: step 12476, loss 0.0302643, acc 0.98
2016-09-06T02:36:26.605354: step 12477, loss 0.042674, acc 0.98
2016-09-06T02:36:27.421911: step 12478, loss 0.0134662, acc 1
2016-09-06T02:36:28.218284: step 12479, loss 0.0595067, acc 0.96
2016-09-06T02:36:28.978688: step 12480, loss 0.00308497, acc 1
2016-09-06T02:36:29.778680: step 12481, loss 0.00761883, acc 1
2016-09-06T02:36:30.590087: step 12482, loss 0.029649, acc 0.98
2016-09-06T02:36:31.402196: step 12483, loss 0.00158015, acc 1
2016-09-06T02:36:32.239684: step 12484, loss 0.0351414, acc 0.98
2016-09-06T02:36:33.059385: step 12485, loss 0.00193973, acc 1
2016-09-06T02:36:33.858552: step 12486, loss 0.00233996, acc 1
2016-09-06T02:36:34.682839: step 12487, loss 0.00197309, acc 1
2016-09-06T02:36:35.482688: step 12488, loss 0.0195373, acc 0.98
2016-09-06T02:36:36.316415: step 12489, loss 0.0140746, acc 1
2016-09-06T02:36:37.138398: step 12490, loss 0.0117876, acc 1
2016-09-06T02:36:37.939964: step 12491, loss 0.0222178, acc 1
2016-09-06T02:36:38.759158: step 12492, loss 0.0638386, acc 0.98
2016-09-06T02:36:39.582101: step 12493, loss 0.00199267, acc 1
2016-09-06T02:36:40.388872: step 12494, loss 0.00318378, acc 1
2016-09-06T02:36:41.191934: step 12495, loss 0.00204026, acc 1
2016-09-06T02:36:41.995795: step 12496, loss 0.0145326, acc 1
2016-09-06T02:36:42.808689: step 12497, loss 0.00164462, acc 1
2016-09-06T02:36:43.623857: step 12498, loss 0.00249405, acc 1
2016-09-06T02:36:44.481949: step 12499, loss 0.0043874, acc 1
2016-09-06T02:36:45.308261: step 12500, loss 0.010578, acc 1

Evaluation:
2016-09-06T02:36:49.100677: step 12500, loss 2.18771, acc 0.718574

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-12500

2016-09-06T02:36:50.917856: step 12501, loss 0.0170309, acc 0.98
2016-09-06T02:36:51.756458: step 12502, loss 0.0104305, acc 1
2016-09-06T02:36:52.560285: step 12503, loss 0.00910616, acc 1
2016-09-06T02:36:53.332829: step 12504, loss 0.0554029, acc 0.98
2016-09-06T02:36:54.144317: step 12505, loss 0.0135182, acc 1
2016-09-06T02:36:54.940130: step 12506, loss 0.0235349, acc 1
2016-09-06T02:36:55.726104: step 12507, loss 0.0148716, acc 1
2016-09-06T02:36:56.542538: step 12508, loss 0.00159348, acc 1
2016-09-06T02:36:57.343439: step 12509, loss 0.011315, acc 1
2016-09-06T02:36:58.144798: step 12510, loss 0.0173244, acc 0.98
2016-09-06T02:36:58.960744: step 12511, loss 0.00282227, acc 1
2016-09-06T02:36:59.767771: step 12512, loss 0.00912405, acc 1
2016-09-06T02:37:00.606941: step 12513, loss 0.00693601, acc 1
2016-09-06T02:37:01.412654: step 12514, loss 0.00368783, acc 1
2016-09-06T02:37:02.202824: step 12515, loss 0.0114476, acc 1
2016-09-06T02:37:03.042367: step 12516, loss 0.0123518, acc 1
2016-09-06T02:37:03.852781: step 12517, loss 0.00168425, acc 1
2016-09-06T02:37:04.616647: step 12518, loss 0.0172567, acc 1
2016-09-06T02:37:05.444448: step 12519, loss 0.0407226, acc 0.98
2016-09-06T02:37:06.253115: step 12520, loss 0.002031, acc 1
2016-09-06T02:37:07.020000: step 12521, loss 0.0277816, acc 1
2016-09-06T02:37:07.832716: step 12522, loss 0.00201273, acc 1
2016-09-06T02:37:08.699518: step 12523, loss 0.129105, acc 0.98
2016-09-06T02:37:09.488325: step 12524, loss 0.00191438, acc 1
2016-09-06T02:37:10.290959: step 12525, loss 0.0027028, acc 1
2016-09-06T02:37:11.109719: step 12526, loss 0.0129878, acc 1
2016-09-06T02:37:11.911916: step 12527, loss 0.0346595, acc 0.98
2016-09-06T02:37:12.724081: step 12528, loss 0.0207572, acc 0.98
2016-09-06T02:37:13.528967: step 12529, loss 0.00204805, acc 1
2016-09-06T02:37:14.318046: step 12530, loss 0.00190802, acc 1
2016-09-06T02:37:15.125103: step 12531, loss 0.00324027, acc 1
2016-09-06T02:37:15.928607: step 12532, loss 0.0133267, acc 1
2016-09-06T02:37:16.688574: step 12533, loss 0.0127524, acc 1
2016-09-06T02:37:17.475947: step 12534, loss 0.00476714, acc 1
2016-09-06T02:37:18.289018: step 12535, loss 0.0327918, acc 1
2016-09-06T02:37:19.101282: step 12536, loss 0.0224514, acc 0.98
2016-09-06T02:37:19.894374: step 12537, loss 0.00244964, acc 1
2016-09-06T02:37:20.711971: step 12538, loss 0.0305433, acc 0.98
2016-09-06T02:37:21.493875: step 12539, loss 0.00207651, acc 1
2016-09-06T02:37:22.319632: step 12540, loss 0.00243269, acc 1
2016-09-06T02:37:23.117145: step 12541, loss 0.022927, acc 1
2016-09-06T02:37:23.924403: step 12542, loss 0.00936488, acc 1
2016-09-06T02:37:24.745754: step 12543, loss 0.00220807, acc 1
2016-09-06T02:37:25.570116: step 12544, loss 0.0277638, acc 0.98
2016-09-06T02:37:26.354898: step 12545, loss 0.00819583, acc 1
2016-09-06T02:37:27.148121: step 12546, loss 0.0170356, acc 1
2016-09-06T02:37:27.965968: step 12547, loss 0.00331617, acc 1
2016-09-06T02:37:28.750409: step 12548, loss 0.0403777, acc 0.98
2016-09-06T02:37:29.550890: step 12549, loss 0.0210274, acc 1
2016-09-06T02:37:30.370211: step 12550, loss 0.022756, acc 1
2016-09-06T02:37:31.124433: step 12551, loss 0.00754765, acc 1
2016-09-06T02:37:31.954197: step 12552, loss 0.00219794, acc 1
2016-09-06T02:37:32.804643: step 12553, loss 0.0205001, acc 0.98
2016-09-06T02:37:33.618080: step 12554, loss 0.00242416, acc 1
2016-09-06T02:37:34.425423: step 12555, loss 0.0109734, acc 1
2016-09-06T02:37:35.241759: step 12556, loss 0.015828, acc 1
2016-09-06T02:37:36.033477: step 12557, loss 0.00227811, acc 1
2016-09-06T02:37:36.830079: step 12558, loss 0.0358771, acc 0.96
2016-09-06T02:37:37.650561: step 12559, loss 0.00854531, acc 1
2016-09-06T02:37:38.458763: step 12560, loss 0.0579251, acc 0.98
2016-09-06T02:37:39.257963: step 12561, loss 0.00214754, acc 1
2016-09-06T02:37:40.086342: step 12562, loss 0.00402941, acc 1
2016-09-06T02:37:40.870820: step 12563, loss 0.0151051, acc 1
2016-09-06T02:37:41.713736: step 12564, loss 0.00232098, acc 1
2016-09-06T02:37:42.532821: step 12565, loss 0.00262229, acc 1
2016-09-06T02:37:43.349444: step 12566, loss 0.0222777, acc 0.98
2016-09-06T02:37:44.145034: step 12567, loss 0.0115633, acc 1
2016-09-06T02:37:44.952602: step 12568, loss 0.0292498, acc 0.98
2016-09-06T02:37:45.764135: step 12569, loss 0.0362069, acc 0.98
2016-09-06T02:37:46.573474: step 12570, loss 0.00400435, acc 1
2016-09-06T02:37:47.401447: step 12571, loss 0.00448475, acc 1
2016-09-06T02:37:48.197536: step 12572, loss 0.00207815, acc 1
2016-09-06T02:37:48.992633: step 12573, loss 0.00196605, acc 1
2016-09-06T02:37:49.794571: step 12574, loss 0.0057485, acc 1
2016-09-06T02:37:50.607908: step 12575, loss 0.0119877, acc 1
2016-09-06T02:37:51.424449: step 12576, loss 0.00404635, acc 1
2016-09-06T02:37:52.246237: step 12577, loss 0.00187763, acc 1
2016-09-06T02:37:53.052933: step 12578, loss 0.143324, acc 0.96
2016-09-06T02:37:53.871855: step 12579, loss 0.00193369, acc 1
2016-09-06T02:37:54.715667: step 12580, loss 0.00206915, acc 1
2016-09-06T02:37:55.509465: step 12581, loss 0.0428475, acc 0.98
2016-09-06T02:37:56.308231: step 12582, loss 0.00179798, acc 1
2016-09-06T02:37:57.140906: step 12583, loss 0.0101119, acc 1
2016-09-06T02:37:57.959601: step 12584, loss 0.00318204, acc 1
2016-09-06T02:37:58.771833: step 12585, loss 0.00763176, acc 1
2016-09-06T02:37:59.585693: step 12586, loss 0.0036473, acc 1
2016-09-06T02:38:00.419912: step 12587, loss 0.0544559, acc 0.98
2016-09-06T02:38:01.260748: step 12588, loss 0.006292, acc 1
2016-09-06T02:38:02.090563: step 12589, loss 0.0348409, acc 1
2016-09-06T02:38:02.913291: step 12590, loss 0.0310098, acc 0.98
2016-09-06T02:38:03.710680: step 12591, loss 0.010485, acc 1
2016-09-06T02:38:04.531421: step 12592, loss 0.0229671, acc 1
2016-09-06T02:38:05.338547: step 12593, loss 0.0124674, acc 1
2016-09-06T02:38:06.151817: step 12594, loss 0.014813, acc 1
2016-09-06T02:38:07.014870: step 12595, loss 0.0202732, acc 1
2016-09-06T02:38:07.819800: step 12596, loss 0.00213948, acc 1
2016-09-06T02:38:08.630966: step 12597, loss 0.0112517, acc 1
2016-09-06T02:38:09.460893: step 12598, loss 0.00208616, acc 1
2016-09-06T02:38:10.274598: step 12599, loss 0.00232596, acc 1
2016-09-06T02:38:11.060962: step 12600, loss 0.0316785, acc 0.98

Evaluation:
2016-09-06T02:38:14.793394: step 12600, loss 2.82867, acc 0.711069

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-12600

2016-09-06T02:38:16.771804: step 12601, loss 0.0256044, acc 0.98
2016-09-06T02:38:17.575058: step 12602, loss 0.00758681, acc 1
2016-09-06T02:38:18.382738: step 12603, loss 0.00233352, acc 1
2016-09-06T02:38:19.215874: step 12604, loss 0.0125845, acc 1
2016-09-06T02:38:20.025469: step 12605, loss 0.0161813, acc 0.98
2016-09-06T02:38:20.830685: step 12606, loss 0.00291819, acc 1
2016-09-06T02:38:21.653755: step 12607, loss 0.0143999, acc 1
2016-09-06T02:38:22.484545: step 12608, loss 0.00358284, acc 1
2016-09-06T02:38:23.277943: step 12609, loss 0.0202163, acc 0.98
2016-09-06T02:38:24.100019: step 12610, loss 0.00687528, acc 1
2016-09-06T02:38:24.909323: step 12611, loss 0.083677, acc 0.96
2016-09-06T02:38:25.687230: step 12612, loss 0.029102, acc 0.98
2016-09-06T02:38:26.507797: step 12613, loss 0.00386751, acc 1
2016-09-06T02:38:27.325943: step 12614, loss 0.0041637, acc 1
2016-09-06T02:38:28.125507: step 12615, loss 0.00520036, acc 1
2016-09-06T02:38:28.927500: step 12616, loss 0.00696791, acc 1
2016-09-06T02:38:29.738350: step 12617, loss 0.0174864, acc 0.98
2016-09-06T02:38:30.513767: step 12618, loss 0.0241261, acc 1
2016-09-06T02:38:31.350937: step 12619, loss 0.0340598, acc 0.98
2016-09-06T02:38:32.177862: step 12620, loss 0.00338517, acc 1
2016-09-06T02:38:32.983983: step 12621, loss 0.162441, acc 0.94
2016-09-06T02:38:33.828284: step 12622, loss 0.0030065, acc 1
2016-09-06T02:38:34.641209: step 12623, loss 0.0153747, acc 1
2016-09-06T02:38:35.447609: step 12624, loss 0.0279654, acc 1
2016-09-06T02:38:36.256436: step 12625, loss 0.00354408, acc 1
2016-09-06T02:38:37.063730: step 12626, loss 0.00983242, acc 1
2016-09-06T02:38:37.859676: step 12627, loss 0.0148367, acc 1
2016-09-06T02:38:38.642463: step 12628, loss 0.0061727, acc 1
2016-09-06T02:38:39.451573: step 12629, loss 0.0123893, acc 1
2016-09-06T02:38:40.214766: step 12630, loss 0.00727562, acc 1
2016-09-06T02:38:41.010484: step 12631, loss 0.00289902, acc 1
2016-09-06T02:38:41.788910: step 12632, loss 0.00295294, acc 1
2016-09-06T02:38:42.576860: step 12633, loss 0.00574056, acc 1
2016-09-06T02:38:43.391098: step 12634, loss 0.0032307, acc 1
2016-09-06T02:38:44.200786: step 12635, loss 0.00302369, acc 1
2016-09-06T02:38:45.007684: step 12636, loss 0.052573, acc 0.96
2016-09-06T02:38:45.823748: step 12637, loss 0.00304282, acc 1
2016-09-06T02:38:46.620443: step 12638, loss 0.00459227, acc 1
2016-09-06T02:38:47.445507: step 12639, loss 0.00457435, acc 1
2016-09-06T02:38:48.230860: step 12640, loss 0.0199831, acc 0.98
2016-09-06T02:38:49.042349: step 12641, loss 0.025337, acc 0.98
2016-09-06T02:38:49.840793: step 12642, loss 0.00307983, acc 1
2016-09-06T02:38:50.650973: step 12643, loss 0.044396, acc 0.98
2016-09-06T02:38:51.464390: step 12644, loss 0.00816316, acc 1
2016-09-06T02:38:52.262419: step 12645, loss 0.00436376, acc 1
2016-09-06T02:38:53.090942: step 12646, loss 0.0119367, acc 1
2016-09-06T02:38:53.905067: step 12647, loss 0.0375733, acc 0.98
2016-09-06T02:38:54.687869: step 12648, loss 0.0082017, acc 1
2016-09-06T02:38:55.526902: step 12649, loss 0.114322, acc 0.96
2016-09-06T02:38:56.365490: step 12650, loss 0.00771499, acc 1
2016-09-06T02:38:57.157323: step 12651, loss 0.0311138, acc 0.98
2016-09-06T02:38:57.996696: step 12652, loss 0.0037261, acc 1
2016-09-06T02:38:58.804441: step 12653, loss 0.0347604, acc 0.98
2016-09-06T02:38:59.622887: step 12654, loss 0.0283788, acc 0.98
2016-09-06T02:39:00.456031: step 12655, loss 0.0605956, acc 0.96
2016-09-06T02:39:01.305969: step 12656, loss 0.00582532, acc 1
2016-09-06T02:39:02.101695: step 12657, loss 0.00637056, acc 1
2016-09-06T02:39:02.919685: step 12658, loss 0.0171874, acc 1
2016-09-06T02:39:03.749261: step 12659, loss 0.00383269, acc 1
2016-09-06T02:39:04.541970: step 12660, loss 0.00659432, acc 1
2016-09-06T02:39:05.332372: step 12661, loss 0.00290988, acc 1
2016-09-06T02:39:06.142460: step 12662, loss 0.0139518, acc 1
2016-09-06T02:39:06.949598: step 12663, loss 0.00236308, acc 1
2016-09-06T02:39:07.771030: step 12664, loss 0.00333678, acc 1
2016-09-06T02:39:08.580867: step 12665, loss 0.0192789, acc 0.98
2016-09-06T02:39:09.369410: step 12666, loss 0.042452, acc 0.98
2016-09-06T02:39:10.165875: step 12667, loss 0.024348, acc 0.98
2016-09-06T02:39:11.001675: step 12668, loss 0.0023877, acc 1
2016-09-06T02:39:11.792857: step 12669, loss 0.193267, acc 0.98
2016-09-06T02:39:12.585676: step 12670, loss 0.0025464, acc 1
2016-09-06T02:39:13.427812: step 12671, loss 0.00953008, acc 1
2016-09-06T02:39:14.143027: step 12672, loss 0.00221406, acc 1
2016-09-06T02:39:14.953146: step 12673, loss 0.00875539, acc 1
2016-09-06T02:39:15.772460: step 12674, loss 0.0209406, acc 0.98
2016-09-06T02:39:16.557580: step 12675, loss 0.00766753, acc 1
2016-09-06T02:39:17.358807: step 12676, loss 0.118843, acc 0.98
2016-09-06T02:39:18.172978: step 12677, loss 0.004083, acc 1
2016-09-06T02:39:18.989994: step 12678, loss 0.013824, acc 1
2016-09-06T02:39:19.816507: step 12679, loss 0.00789035, acc 1
2016-09-06T02:39:20.635584: step 12680, loss 0.0168369, acc 1
2016-09-06T02:39:21.431136: step 12681, loss 0.00745208, acc 1
2016-09-06T02:39:22.243584: step 12682, loss 0.0545933, acc 0.98
2016-09-06T02:39:23.087103: step 12683, loss 0.0652236, acc 0.94
2016-09-06T02:39:23.861857: step 12684, loss 0.0341741, acc 0.98
2016-09-06T02:39:24.681767: step 12685, loss 0.00710805, acc 1
2016-09-06T02:39:25.535676: step 12686, loss 0.00332661, acc 1
2016-09-06T02:39:26.372466: step 12687, loss 0.00350379, acc 1
2016-09-06T02:39:27.213959: step 12688, loss 0.0367208, acc 0.96
2016-09-06T02:39:28.031386: step 12689, loss 0.00424907, acc 1
2016-09-06T02:39:28.808527: step 12690, loss 0.00345245, acc 1
2016-09-06T02:39:29.609811: step 12691, loss 0.0044717, acc 1
2016-09-06T02:39:30.456133: step 12692, loss 0.016712, acc 1
2016-09-06T02:39:31.298116: step 12693, loss 0.032223, acc 0.98
2016-09-06T02:39:32.110420: step 12694, loss 0.0152991, acc 1
2016-09-06T02:39:32.932497: step 12695, loss 0.0056514, acc 1
2016-09-06T02:39:33.748933: step 12696, loss 0.00342485, acc 1
2016-09-06T02:39:34.537563: step 12697, loss 0.00610967, acc 1
2016-09-06T02:39:35.371991: step 12698, loss 0.0194618, acc 0.98
2016-09-06T02:39:36.147774: step 12699, loss 0.01681, acc 1
2016-09-06T02:39:36.946916: step 12700, loss 0.0157104, acc 1

Evaluation:
2016-09-06T02:39:40.693568: step 12700, loss 2.89101, acc 0.71576

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-12700

2016-09-06T02:39:42.604352: step 12701, loss 0.0257998, acc 0.98
2016-09-06T02:39:43.407624: step 12702, loss 0.0179675, acc 0.98
2016-09-06T02:39:44.242365: step 12703, loss 0.0559808, acc 0.98
2016-09-06T02:39:45.090449: step 12704, loss 0.00545119, acc 1
2016-09-06T02:39:45.909520: step 12705, loss 0.0038487, acc 1
2016-09-06T02:39:46.773444: step 12706, loss 0.0156537, acc 1
2016-09-06T02:39:47.603124: step 12707, loss 0.00366526, acc 1
2016-09-06T02:39:48.414587: step 12708, loss 0.0215367, acc 0.98
2016-09-06T02:39:49.210854: step 12709, loss 0.0168139, acc 1
2016-09-06T02:39:50.026736: step 12710, loss 0.0204182, acc 1
2016-09-06T02:39:50.846322: step 12711, loss 0.0120516, acc 1
2016-09-06T02:39:51.640122: step 12712, loss 0.0421654, acc 0.98
2016-09-06T02:39:52.414237: step 12713, loss 0.00591106, acc 1
2016-09-06T02:39:53.221717: step 12714, loss 0.00898695, acc 1
2016-09-06T02:39:54.080905: step 12715, loss 0.0469951, acc 0.98
2016-09-06T02:39:54.887163: step 12716, loss 0.0186386, acc 1
2016-09-06T02:39:55.727383: step 12717, loss 0.0199213, acc 1
2016-09-06T02:39:56.509226: step 12718, loss 0.0131724, acc 1
2016-09-06T02:39:57.294457: step 12719, loss 0.00614609, acc 1
2016-09-06T02:39:58.102327: step 12720, loss 0.00384121, acc 1
2016-09-06T02:39:58.903817: step 12721, loss 0.00820899, acc 1
2016-09-06T02:39:59.697394: step 12722, loss 0.0034645, acc 1
2016-09-06T02:40:00.523066: step 12723, loss 0.115816, acc 0.98
2016-09-06T02:40:01.330282: step 12724, loss 0.0197865, acc 0.98
2016-09-06T02:40:02.117793: step 12725, loss 0.0205741, acc 1
2016-09-06T02:40:02.923982: step 12726, loss 0.00541427, acc 1
2016-09-06T02:40:03.715551: step 12727, loss 0.00857903, acc 1
2016-09-06T02:40:04.506985: step 12728, loss 0.0176757, acc 0.98
2016-09-06T02:40:05.282277: step 12729, loss 0.00428781, acc 1
2016-09-06T02:40:06.082615: step 12730, loss 0.039396, acc 0.98
2016-09-06T02:40:06.887193: step 12731, loss 0.00488246, acc 1
2016-09-06T02:40:07.710471: step 12732, loss 0.00386669, acc 1
2016-09-06T02:40:08.508616: step 12733, loss 0.0477719, acc 0.98
2016-09-06T02:40:09.319801: step 12734, loss 0.00464268, acc 1
2016-09-06T02:40:10.166347: step 12735, loss 0.0046655, acc 1
2016-09-06T02:40:10.942530: step 12736, loss 0.0213436, acc 1
2016-09-06T02:40:11.899216: step 12737, loss 0.00426404, acc 1
2016-09-06T02:40:12.808202: step 12738, loss 0.226958, acc 0.96
2016-09-06T02:40:13.662600: step 12739, loss 0.0193171, acc 0.98
2016-09-06T02:40:14.487213: step 12740, loss 0.143811, acc 0.96
2016-09-06T02:40:15.321515: step 12741, loss 0.0159249, acc 1
2016-09-06T02:40:16.207886: step 12742, loss 0.0728894, acc 0.98
2016-09-06T02:40:17.000003: step 12743, loss 0.0151155, acc 1
2016-09-06T02:40:17.807420: step 12744, loss 0.00456904, acc 1
2016-09-06T02:40:18.642689: step 12745, loss 0.00961356, acc 1
2016-09-06T02:40:19.466169: step 12746, loss 0.00355661, acc 1
2016-09-06T02:40:20.268062: step 12747, loss 0.00384727, acc 1
2016-09-06T02:40:21.097824: step 12748, loss 0.0149947, acc 1
2016-09-06T02:40:21.907891: step 12749, loss 0.0201469, acc 0.98
2016-09-06T02:40:22.731614: step 12750, loss 0.0222683, acc 0.98
2016-09-06T02:40:23.561692: step 12751, loss 0.00365222, acc 1
2016-09-06T02:40:24.369311: step 12752, loss 0.0139573, acc 1
2016-09-06T02:40:25.164089: step 12753, loss 0.00449564, acc 1
2016-09-06T02:40:25.974766: step 12754, loss 0.00664502, acc 1
2016-09-06T02:40:26.786212: step 12755, loss 0.150355, acc 0.96
2016-09-06T02:40:27.573504: step 12756, loss 0.00645493, acc 1
2016-09-06T02:40:28.408976: step 12757, loss 0.0107287, acc 1
2016-09-06T02:40:29.263283: step 12758, loss 0.00958237, acc 1
2016-09-06T02:40:30.105489: step 12759, loss 0.00587896, acc 1
2016-09-06T02:40:30.925881: step 12760, loss 0.0201371, acc 1
2016-09-06T02:40:31.780255: step 12761, loss 0.0269047, acc 0.98
2016-09-06T02:40:32.591855: step 12762, loss 0.00458853, acc 1
2016-09-06T02:40:33.432631: step 12763, loss 0.0219955, acc 1
2016-09-06T02:40:34.258075: step 12764, loss 0.00363227, acc 1
2016-09-06T02:40:35.052444: step 12765, loss 0.011842, acc 1
2016-09-06T02:40:35.882883: step 12766, loss 0.00716715, acc 1
2016-09-06T02:40:36.685495: step 12767, loss 0.0210902, acc 0.98
2016-09-06T02:40:37.467065: step 12768, loss 0.0142622, acc 1
2016-09-06T02:40:38.291400: step 12769, loss 0.0109525, acc 1
2016-09-06T02:40:39.097352: step 12770, loss 0.0190958, acc 1
2016-09-06T02:40:39.861534: step 12771, loss 0.0167339, acc 1
2016-09-06T02:40:40.680844: step 12772, loss 0.0145658, acc 1
2016-09-06T02:40:41.508587: step 12773, loss 0.0112875, acc 1
2016-09-06T02:40:42.337557: step 12774, loss 0.00367206, acc 1
2016-09-06T02:40:43.163062: step 12775, loss 0.00716742, acc 1
2016-09-06T02:40:43.970015: step 12776, loss 0.021522, acc 0.98
2016-09-06T02:40:44.770265: step 12777, loss 0.0250097, acc 1
2016-09-06T02:40:45.570851: step 12778, loss 0.00386078, acc 1
2016-09-06T02:40:46.383451: step 12779, loss 0.00386273, acc 1
2016-09-06T02:40:47.157414: step 12780, loss 0.114041, acc 0.96
2016-09-06T02:40:47.951698: step 12781, loss 0.00456065, acc 1
2016-09-06T02:40:48.739083: step 12782, loss 0.0304632, acc 0.98
2016-09-06T02:40:49.549457: step 12783, loss 0.0036944, acc 1
2016-09-06T02:40:50.394681: step 12784, loss 0.0269667, acc 1
2016-09-06T02:40:51.229393: step 12785, loss 0.019291, acc 1
2016-09-06T02:40:52.023841: step 12786, loss 0.00371764, acc 1
2016-09-06T02:40:52.842305: step 12787, loss 0.0751202, acc 0.98
2016-09-06T02:40:53.670649: step 12788, loss 0.0165549, acc 1
2016-09-06T02:40:54.476212: step 12789, loss 0.00470427, acc 1
2016-09-06T02:40:55.262911: step 12790, loss 0.00346695, acc 1
2016-09-06T02:40:56.097201: step 12791, loss 0.0238496, acc 0.98
2016-09-06T02:40:56.890184: step 12792, loss 0.0231188, acc 1
2016-09-06T02:40:57.699449: step 12793, loss 0.00421014, acc 1
2016-09-06T02:40:58.510230: step 12794, loss 0.00333524, acc 1
2016-09-06T02:40:59.301254: step 12795, loss 0.00459509, acc 1
2016-09-06T02:41:00.121157: step 12796, loss 0.00463974, acc 1
2016-09-06T02:41:01.032582: step 12797, loss 0.0646793, acc 0.96
2016-09-06T02:41:01.844830: step 12798, loss 0.0341574, acc 0.98
2016-09-06T02:41:02.641033: step 12799, loss 0.0159874, acc 1
2016-09-06T02:41:03.445227: step 12800, loss 0.0742161, acc 0.98

Evaluation:
2016-09-06T02:41:07.180759: step 12800, loss 2.59497, acc 0.725141

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-12800

2016-09-06T02:41:09.000644: step 12801, loss 0.00854691, acc 1
2016-09-06T02:41:09.812050: step 12802, loss 0.0190775, acc 0.98
2016-09-06T02:41:10.627914: step 12803, loss 0.0037511, acc 1
2016-09-06T02:41:11.414815: step 12804, loss 0.00398695, acc 1
2016-09-06T02:41:12.216978: step 12805, loss 0.00927245, acc 1
2016-09-06T02:41:13.005376: step 12806, loss 0.0133804, acc 1
2016-09-06T02:41:13.824558: step 12807, loss 0.0168575, acc 1
2016-09-06T02:41:14.654096: step 12808, loss 0.0027217, acc 1
2016-09-06T02:41:15.468253: step 12809, loss 0.0318904, acc 0.98
2016-09-06T02:41:16.256611: step 12810, loss 0.0031976, acc 1
2016-09-06T02:41:17.068094: step 12811, loss 0.00263918, acc 1
2016-09-06T02:41:17.891750: step 12812, loss 0.00780725, acc 1
2016-09-06T02:41:18.678757: step 12813, loss 0.0672878, acc 0.96
2016-09-06T02:41:19.480211: step 12814, loss 0.00260104, acc 1
2016-09-06T02:41:20.282925: step 12815, loss 0.00649656, acc 1
2016-09-06T02:41:21.084189: step 12816, loss 0.0111431, acc 1
2016-09-06T02:41:21.890678: step 12817, loss 0.046601, acc 0.98
2016-09-06T02:41:22.726409: step 12818, loss 0.0335215, acc 0.98
2016-09-06T02:41:23.506215: step 12819, loss 0.00329359, acc 1
2016-09-06T02:41:24.323979: step 12820, loss 0.00990629, acc 1
2016-09-06T02:41:25.142887: step 12821, loss 0.00759563, acc 1
2016-09-06T02:41:25.926876: step 12822, loss 0.00520522, acc 1
2016-09-06T02:41:26.739058: step 12823, loss 0.00286144, acc 1
2016-09-06T02:41:27.546742: step 12824, loss 0.0137155, acc 1
2016-09-06T02:41:28.357208: step 12825, loss 0.0120132, acc 1
2016-09-06T02:41:29.180930: step 12826, loss 0.00240769, acc 1
2016-09-06T02:41:30.013310: step 12827, loss 0.0321044, acc 0.98
2016-09-06T02:41:30.821018: step 12828, loss 0.0273476, acc 0.98
2016-09-06T02:41:31.641262: step 12829, loss 0.0025298, acc 1
2016-09-06T02:41:32.470397: step 12830, loss 0.0125618, acc 1
2016-09-06T02:41:33.283080: step 12831, loss 0.00232055, acc 1
2016-09-06T02:41:34.084499: step 12832, loss 0.076164, acc 0.98
2016-09-06T02:41:34.947609: step 12833, loss 0.0167859, acc 1
2016-09-06T02:41:35.773373: step 12834, loss 0.017791, acc 0.98
2016-09-06T02:41:36.596906: step 12835, loss 0.0231948, acc 0.98
2016-09-06T02:41:37.432710: step 12836, loss 0.0314113, acc 0.98
2016-09-06T02:41:38.269758: step 12837, loss 0.00378254, acc 1
2016-09-06T02:41:39.070483: step 12838, loss 0.0255235, acc 0.98
2016-09-06T02:41:39.923737: step 12839, loss 0.0101055, acc 1
2016-09-06T02:41:40.762179: step 12840, loss 0.00563722, acc 1
2016-09-06T02:41:41.559115: step 12841, loss 0.0084987, acc 1
2016-09-06T02:41:42.381990: step 12842, loss 0.00995624, acc 1
2016-09-06T02:41:43.177015: step 12843, loss 0.00846876, acc 1
2016-09-06T02:41:43.998404: step 12844, loss 0.00455992, acc 1
2016-09-06T02:41:44.848419: step 12845, loss 0.00340164, acc 1
2016-09-06T02:41:45.671696: step 12846, loss 0.0116399, acc 1
2016-09-06T02:41:46.527348: step 12847, loss 0.00351117, acc 1
2016-09-06T02:41:47.354675: step 12848, loss 0.003544, acc 1
2016-09-06T02:41:48.162352: step 12849, loss 0.00564252, acc 1
2016-09-06T02:41:48.935413: step 12850, loss 0.0119656, acc 1
2016-09-06T02:41:49.735720: step 12851, loss 0.0484683, acc 0.98
2016-09-06T02:41:50.542248: step 12852, loss 0.0138817, acc 1
2016-09-06T02:41:51.333023: step 12853, loss 0.0103534, acc 1
2016-09-06T02:41:52.112491: step 12854, loss 0.0152752, acc 1
2016-09-06T02:41:52.929775: step 12855, loss 0.00462251, acc 1
2016-09-06T02:41:53.717198: step 12856, loss 0.0080415, acc 1
2016-09-06T02:41:54.534670: step 12857, loss 0.00446816, acc 1
2016-09-06T02:41:55.336954: step 12858, loss 0.016019, acc 1
2016-09-06T02:41:56.122639: step 12859, loss 0.00284229, acc 1
2016-09-06T02:41:56.940903: step 12860, loss 0.00299112, acc 1
2016-09-06T02:41:57.771326: step 12861, loss 0.00986133, acc 1
2016-09-06T02:41:58.581484: step 12862, loss 0.0450122, acc 0.98
2016-09-06T02:41:59.377351: step 12863, loss 0.0673401, acc 0.98
2016-09-06T02:42:00.111340: step 12864, loss 0.0608667, acc 0.977273
2016-09-06T02:42:00.924703: step 12865, loss 0.0025365, acc 1
2016-09-06T02:42:01.762796: step 12866, loss 0.0172268, acc 0.98
2016-09-06T02:42:02.552708: step 12867, loss 0.00255825, acc 1
2016-09-06T02:42:03.344224: step 12868, loss 0.00652938, acc 1
2016-09-06T02:42:04.179520: step 12869, loss 0.00355516, acc 1
2016-09-06T02:42:04.983319: step 12870, loss 0.0577029, acc 0.98
2016-09-06T02:42:05.800760: step 12871, loss 0.00475822, acc 1
2016-09-06T02:42:06.621826: step 12872, loss 0.00501436, acc 1
2016-09-06T02:42:07.421421: step 12873, loss 0.00250335, acc 1
2016-09-06T02:42:08.234767: step 12874, loss 0.00517096, acc 1
2016-09-06T02:42:09.055718: step 12875, loss 0.00603678, acc 1
2016-09-06T02:42:09.914258: step 12876, loss 0.00701447, acc 1
2016-09-06T02:42:10.699948: step 12877, loss 0.0652756, acc 0.96
2016-09-06T02:42:11.495970: step 12878, loss 0.0917447, acc 0.98
2016-09-06T02:42:12.312344: step 12879, loss 0.00269258, acc 1
2016-09-06T02:42:13.086741: step 12880, loss 0.0434741, acc 0.98
2016-09-06T02:42:13.887825: step 12881, loss 0.00843989, acc 1
2016-09-06T02:42:14.700508: step 12882, loss 0.00254256, acc 1
2016-09-06T02:42:15.505643: step 12883, loss 0.00263087, acc 1
2016-09-06T02:42:16.293128: step 12884, loss 0.00358799, acc 1
2016-09-06T02:42:17.112562: step 12885, loss 0.00477537, acc 1
2016-09-06T02:42:17.879427: step 12886, loss 0.00375741, acc 1
2016-09-06T02:42:18.724019: step 12887, loss 0.00532127, acc 1
2016-09-06T02:42:19.533901: step 12888, loss 0.0211652, acc 1
2016-09-06T02:42:20.309386: step 12889, loss 0.0106916, acc 1
2016-09-06T02:42:21.122956: step 12890, loss 0.0382851, acc 0.96
2016-09-06T02:42:21.969529: step 12891, loss 0.0158162, acc 1
2016-09-06T02:42:22.772077: step 12892, loss 0.00928342, acc 1
2016-09-06T02:42:23.584325: step 12893, loss 0.00407401, acc 1
2016-09-06T02:42:24.448130: step 12894, loss 0.00367343, acc 1
2016-09-06T02:42:25.284239: step 12895, loss 0.0522484, acc 0.98
2016-09-06T02:42:26.082501: step 12896, loss 0.00729591, acc 1
2016-09-06T02:42:26.943982: step 12897, loss 0.00357646, acc 1
2016-09-06T02:42:27.748594: step 12898, loss 0.00343452, acc 1
2016-09-06T02:42:28.541422: step 12899, loss 0.0035648, acc 1
2016-09-06T02:42:29.366788: step 12900, loss 0.00995429, acc 1

Evaluation:
2016-09-06T02:42:33.074880: step 12900, loss 2.76009, acc 0.724203

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-12900

2016-09-06T02:42:35.071547: step 12901, loss 0.00345244, acc 1
2016-09-06T02:42:35.919525: step 12902, loss 0.0184753, acc 0.98
2016-09-06T02:42:36.757923: step 12903, loss 0.00458617, acc 1
2016-09-06T02:42:37.568427: step 12904, loss 0.00350893, acc 1
2016-09-06T02:42:38.360484: step 12905, loss 0.0179929, acc 0.98
2016-09-06T02:42:39.194552: step 12906, loss 0.0106658, acc 1
2016-09-06T02:42:39.994561: step 12907, loss 0.052236, acc 0.96
2016-09-06T02:42:40.803621: step 12908, loss 0.00387071, acc 1
2016-09-06T02:42:41.654189: step 12909, loss 0.003546, acc 1
2016-09-06T02:42:42.451224: step 12910, loss 0.0055372, acc 1
2016-09-06T02:42:43.273963: step 12911, loss 0.00351634, acc 1
2016-09-06T02:42:44.093450: step 12912, loss 0.00492566, acc 1
2016-09-06T02:42:44.880118: step 12913, loss 0.00347947, acc 1
2016-09-06T02:42:45.686164: step 12914, loss 0.00350303, acc 1
2016-09-06T02:42:46.516819: step 12915, loss 0.00344693, acc 1
2016-09-06T02:42:47.336463: step 12916, loss 0.00575609, acc 1
2016-09-06T02:42:48.145109: step 12917, loss 0.02593, acc 0.98
2016-09-06T02:42:49.042635: step 12918, loss 0.0248055, acc 0.98
2016-09-06T02:42:49.858476: step 12919, loss 0.019141, acc 1
2016-09-06T02:42:50.640475: step 12920, loss 0.00339521, acc 1
2016-09-06T02:42:51.453078: step 12921, loss 0.00378186, acc 1
2016-09-06T02:42:52.284696: step 12922, loss 0.0517607, acc 0.98
2016-09-06T02:42:53.083694: step 12923, loss 0.0488046, acc 0.98
2016-09-06T02:42:53.895876: step 12924, loss 0.0117016, acc 1
2016-09-06T02:42:54.729450: step 12925, loss 0.00434911, acc 1
2016-09-06T02:42:55.523195: step 12926, loss 0.00767719, acc 1
2016-09-06T02:42:56.329371: step 12927, loss 0.00312324, acc 1
2016-09-06T02:42:57.158623: step 12928, loss 0.0095815, acc 1
2016-09-06T02:42:57.938164: step 12929, loss 0.00830701, acc 1
2016-09-06T02:42:58.748452: step 12930, loss 0.0031889, acc 1
2016-09-06T02:42:59.580279: step 12931, loss 0.011547, acc 1
2016-09-06T02:43:00.397513: step 12932, loss 0.003509, acc 1
2016-09-06T02:43:01.182067: step 12933, loss 0.00404875, acc 1
2016-09-06T02:43:01.994205: step 12934, loss 0.00297685, acc 1
2016-09-06T02:43:02.775886: step 12935, loss 0.0134381, acc 1
2016-09-06T02:43:03.554591: step 12936, loss 0.00335151, acc 1
2016-09-06T02:43:04.371419: step 12937, loss 0.00398347, acc 1
2016-09-06T02:43:05.173773: step 12938, loss 0.00284784, acc 1
2016-09-06T02:43:05.970927: step 12939, loss 0.0383046, acc 0.96
2016-09-06T02:43:06.795581: step 12940, loss 0.0111359, acc 1
2016-09-06T02:43:07.602618: step 12941, loss 0.019452, acc 0.98
2016-09-06T02:43:08.395677: step 12942, loss 0.00559356, acc 1
2016-09-06T02:43:09.215229: step 12943, loss 0.0116938, acc 1
2016-09-06T02:43:10.009819: step 12944, loss 0.1919, acc 0.96
2016-09-06T02:43:10.822483: step 12945, loss 0.0209445, acc 0.98
2016-09-06T02:43:11.637114: step 12946, loss 0.0111228, acc 1
2016-09-06T02:43:12.464035: step 12947, loss 0.0377039, acc 0.96
2016-09-06T02:43:13.263078: step 12948, loss 0.00240256, acc 1
2016-09-06T02:43:14.073370: step 12949, loss 0.00538728, acc 1
2016-09-06T02:43:14.856383: step 12950, loss 0.0189159, acc 1
2016-09-06T02:43:15.659088: step 12951, loss 0.0110993, acc 1
2016-09-06T02:43:16.459528: step 12952, loss 0.00235244, acc 1
2016-09-06T02:43:17.248595: step 12953, loss 0.0363421, acc 0.98
2016-09-06T02:43:18.064286: step 12954, loss 0.00274673, acc 1
2016-09-06T02:43:18.882473: step 12955, loss 0.00772828, acc 1
2016-09-06T02:43:19.666111: step 12956, loss 0.00803786, acc 1
2016-09-06T02:43:20.464145: step 12957, loss 0.0291708, acc 0.98
2016-09-06T02:43:21.293524: step 12958, loss 0.00221909, acc 1
2016-09-06T02:43:22.101049: step 12959, loss 0.00473897, acc 1
2016-09-06T02:43:22.899355: step 12960, loss 0.00277702, acc 1
2016-09-06T02:43:23.748093: step 12961, loss 0.00242055, acc 1
2016-09-06T02:43:24.521116: step 12962, loss 0.0101163, acc 1
2016-09-06T02:43:25.323980: step 12963, loss 0.0134549, acc 1
2016-09-06T02:43:26.145403: step 12964, loss 0.00701552, acc 1
2016-09-06T02:43:26.928197: step 12965, loss 0.00229714, acc 1
2016-09-06T02:43:27.786615: step 12966, loss 0.00205902, acc 1
2016-09-06T02:43:28.604877: step 12967, loss 0.00210776, acc 1
2016-09-06T02:43:29.409975: step 12968, loss 0.00717857, acc 1
2016-09-06T02:43:30.222772: step 12969, loss 0.0283385, acc 0.98
2016-09-06T02:43:31.043513: step 12970, loss 0.00323065, acc 1
2016-09-06T02:43:31.840581: step 12971, loss 0.0464657, acc 0.98
2016-09-06T02:43:32.646939: step 12972, loss 0.00597507, acc 1
2016-09-06T02:43:33.454511: step 12973, loss 0.0264282, acc 1
2016-09-06T02:43:34.285643: step 12974, loss 0.0340983, acc 0.98
2016-09-06T02:43:35.088877: step 12975, loss 0.00203061, acc 1
2016-09-06T02:43:35.893726: step 12976, loss 0.0445563, acc 1
2016-09-06T02:43:36.690275: step 12977, loss 0.00291552, acc 1
2016-09-06T02:43:37.505323: step 12978, loss 0.00202665, acc 1
2016-09-06T02:43:38.337461: step 12979, loss 0.0407079, acc 0.98
2016-09-06T02:43:39.124214: step 12980, loss 0.0116791, acc 1
2016-09-06T02:43:39.945423: step 12981, loss 0.00658302, acc 1
2016-09-06T02:43:40.788112: step 12982, loss 0.0021049, acc 1
2016-09-06T02:43:41.600654: step 12983, loss 0.0439993, acc 0.98
2016-09-06T02:43:42.402747: step 12984, loss 0.0121779, acc 1
2016-09-06T02:43:43.228999: step 12985, loss 0.00219941, acc 1
2016-09-06T02:43:44.025491: step 12986, loss 0.00213797, acc 1
2016-09-06T02:43:44.845649: step 12987, loss 0.00216383, acc 1
2016-09-06T02:43:45.687419: step 12988, loss 0.00355724, acc 1
2016-09-06T02:43:46.480878: step 12989, loss 0.00832931, acc 1
2016-09-06T02:43:47.315269: step 12990, loss 0.00474344, acc 1
2016-09-06T02:43:48.136477: step 12991, loss 0.0177911, acc 0.98
2016-09-06T02:43:48.943395: step 12992, loss 0.00212541, acc 1
2016-09-06T02:43:49.757771: step 12993, loss 0.0198905, acc 0.98
2016-09-06T02:43:50.643997: step 12994, loss 0.0101022, acc 1
2016-09-06T02:43:51.432563: step 12995, loss 0.00341324, acc 1
2016-09-06T02:43:52.228309: step 12996, loss 0.00847878, acc 1
2016-09-06T02:43:53.065660: step 12997, loss 0.00991007, acc 1
2016-09-06T02:43:53.879714: step 12998, loss 0.00697078, acc 1
2016-09-06T02:43:54.657554: step 12999, loss 0.00229901, acc 1
2016-09-06T02:43:55.487333: step 13000, loss 0.00201483, acc 1

Evaluation:
2016-09-06T02:43:59.195322: step 13000, loss 2.3566, acc 0.727017

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-13000

2016-09-06T02:44:01.113669: step 13001, loss 0.00807078, acc 1
2016-09-06T02:44:01.921505: step 13002, loss 0.0117269, acc 1
2016-09-06T02:44:02.762609: step 13003, loss 0.00233322, acc 1
2016-09-06T02:44:03.586231: step 13004, loss 0.00202534, acc 1
2016-09-06T02:44:04.396685: step 13005, loss 0.0278146, acc 0.98
2016-09-06T02:44:05.206842: step 13006, loss 0.00204064, acc 1
2016-09-06T02:44:06.022671: step 13007, loss 0.00256403, acc 1
2016-09-06T02:44:06.838711: step 13008, loss 0.0554448, acc 0.96
2016-09-06T02:44:07.681987: step 13009, loss 0.00212828, acc 1
2016-09-06T02:44:08.462505: step 13010, loss 0.0131824, acc 1
2016-09-06T02:44:09.269456: step 13011, loss 0.0132489, acc 1
2016-09-06T02:44:10.131401: step 13012, loss 0.0131786, acc 1
2016-09-06T02:44:10.932534: step 13013, loss 0.00201628, acc 1
2016-09-06T02:44:11.747557: step 13014, loss 0.0195085, acc 1
2016-09-06T02:44:12.550445: step 13015, loss 0.00209216, acc 1
2016-09-06T02:44:13.390046: step 13016, loss 0.00643016, acc 1
2016-09-06T02:44:14.197748: step 13017, loss 0.00302111, acc 1
2016-09-06T02:44:15.023455: step 13018, loss 0.00218338, acc 1
2016-09-06T02:44:15.863835: step 13019, loss 0.00209414, acc 1
2016-09-06T02:44:16.646122: step 13020, loss 0.0210568, acc 1
2016-09-06T02:44:17.440392: step 13021, loss 0.00194265, acc 1
2016-09-06T02:44:18.281511: step 13022, loss 0.00420906, acc 1
2016-09-06T02:44:19.069076: step 13023, loss 0.00382566, acc 1
2016-09-06T02:44:19.883024: step 13024, loss 0.0502205, acc 0.98
2016-09-06T02:44:20.718903: step 13025, loss 0.0340184, acc 0.98
2016-09-06T02:44:21.519887: step 13026, loss 0.0886576, acc 0.98
2016-09-06T02:44:22.317309: step 13027, loss 0.00554788, acc 1
2016-09-06T02:44:23.151736: step 13028, loss 0.00186123, acc 1
2016-09-06T02:44:23.977584: step 13029, loss 0.00181393, acc 1
2016-09-06T02:44:24.786875: step 13030, loss 0.0377443, acc 0.98
2016-09-06T02:44:25.636930: step 13031, loss 0.0110127, acc 1
2016-09-06T02:44:26.440180: step 13032, loss 0.0138389, acc 1
2016-09-06T02:44:27.263738: step 13033, loss 0.0193341, acc 1
2016-09-06T02:44:28.087426: step 13034, loss 0.0884007, acc 0.98
2016-09-06T02:44:28.906057: step 13035, loss 0.0419077, acc 0.98
2016-09-06T02:44:29.709074: step 13036, loss 0.00274599, acc 1
2016-09-06T02:44:30.571306: step 13037, loss 0.0648546, acc 0.98
2016-09-06T02:44:31.371308: step 13038, loss 0.00927605, acc 1
2016-09-06T02:44:32.171411: step 13039, loss 0.0451701, acc 0.98
2016-09-06T02:44:33.000074: step 13040, loss 0.00265547, acc 1
2016-09-06T02:44:33.813296: step 13041, loss 0.00871551, acc 1
2016-09-06T02:44:34.614038: step 13042, loss 0.0223127, acc 1
2016-09-06T02:44:35.461786: step 13043, loss 0.00706028, acc 1
2016-09-06T02:44:36.281301: step 13044, loss 0.0164333, acc 1
2016-09-06T02:44:37.124575: step 13045, loss 0.0044287, acc 1
2016-09-06T02:44:37.961608: step 13046, loss 0.011858, acc 1
2016-09-06T02:44:38.776704: step 13047, loss 0.00224642, acc 1
2016-09-06T02:44:39.568996: step 13048, loss 0.00257424, acc 1
2016-09-06T02:44:40.369957: step 13049, loss 0.00292536, acc 1
2016-09-06T02:44:41.175994: step 13050, loss 0.0376666, acc 0.98
2016-09-06T02:44:41.969424: step 13051, loss 0.0584846, acc 0.98
2016-09-06T02:44:42.770865: step 13052, loss 0.00476492, acc 1
2016-09-06T02:44:43.568844: step 13053, loss 0.00829235, acc 1
2016-09-06T02:44:44.410045: step 13054, loss 0.0378006, acc 0.98
2016-09-06T02:44:45.248728: step 13055, loss 0.00642863, acc 1
2016-09-06T02:44:45.976819: step 13056, loss 0.0503411, acc 0.977273
2016-09-06T02:44:46.784881: step 13057, loss 0.00222735, acc 1
2016-09-06T02:44:47.651770: step 13058, loss 0.0783008, acc 0.96
2016-09-06T02:44:48.497092: step 13059, loss 0.001945, acc 1
2016-09-06T02:44:49.291929: step 13060, loss 0.0215347, acc 0.98
2016-09-06T02:44:50.090625: step 13061, loss 0.00428149, acc 1
2016-09-06T02:44:50.920919: step 13062, loss 0.00202234, acc 1
2016-09-06T02:44:51.707741: step 13063, loss 0.00470499, acc 1
2016-09-06T02:44:52.487677: step 13064, loss 0.0052238, acc 1
2016-09-06T02:44:53.315806: step 13065, loss 0.00329473, acc 1
2016-09-06T02:44:54.096374: step 13066, loss 0.00201149, acc 1
2016-09-06T02:44:54.918815: step 13067, loss 0.0134742, acc 1
2016-09-06T02:44:55.740178: step 13068, loss 0.0388985, acc 0.96
2016-09-06T02:44:56.546747: step 13069, loss 0.0028484, acc 1
2016-09-06T02:44:57.376196: step 13070, loss 0.00453298, acc 1
2016-09-06T02:44:58.210786: step 13071, loss 0.0160093, acc 0.98
2016-09-06T02:44:59.010239: step 13072, loss 0.044072, acc 0.96
2016-09-06T02:44:59.846817: step 13073, loss 0.0250891, acc 0.98
2016-09-06T02:45:00.683349: step 13074, loss 0.00539233, acc 1
2016-09-06T02:45:01.517877: step 13075, loss 0.0103429, acc 1
2016-09-06T02:45:02.305419: step 13076, loss 0.0127501, acc 1
2016-09-06T02:45:03.123522: step 13077, loss 0.010323, acc 1
2016-09-06T02:45:03.919094: step 13078, loss 0.0306467, acc 0.98
2016-09-06T02:45:04.732012: step 13079, loss 0.019071, acc 0.98
2016-09-06T02:45:05.566568: step 13080, loss 0.101588, acc 0.98
2016-09-06T02:45:06.401947: step 13081, loss 0.00277057, acc 1
2016-09-06T02:45:07.231855: step 13082, loss 0.0870774, acc 0.98
2016-09-06T02:45:08.116243: step 13083, loss 0.0154476, acc 1
2016-09-06T02:45:08.913327: step 13084, loss 0.0272963, acc 1
2016-09-06T02:45:09.725419: step 13085, loss 0.00203853, acc 1
2016-09-06T02:45:10.537487: step 13086, loss 0.00896826, acc 1
2016-09-06T02:45:11.356459: step 13087, loss 0.00174963, acc 1
2016-09-06T02:45:12.128685: step 13088, loss 0.00532938, acc 1
2016-09-06T02:45:12.958711: step 13089, loss 0.0117643, acc 1
2016-09-06T02:45:13.762644: step 13090, loss 0.0144315, acc 1
2016-09-06T02:45:14.561078: step 13091, loss 0.00269044, acc 1
2016-09-06T02:45:15.384081: step 13092, loss 0.0258916, acc 1
2016-09-06T02:45:16.230572: step 13093, loss 0.00456171, acc 1
2016-09-06T02:45:17.013403: step 13094, loss 0.0472683, acc 0.98
2016-09-06T02:45:17.832808: step 13095, loss 0.0156111, acc 1
2016-09-06T02:45:18.656549: step 13096, loss 0.00302892, acc 1
2016-09-06T02:45:19.449421: step 13097, loss 0.0270409, acc 0.98
2016-09-06T02:45:20.267142: step 13098, loss 0.0151588, acc 1
2016-09-06T02:45:21.095440: step 13099, loss 0.031746, acc 1
2016-09-06T02:45:21.906934: step 13100, loss 0.00253648, acc 1

Evaluation:
2016-09-06T02:45:25.624844: step 13100, loss 2.00685, acc 0.727955

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-13100

2016-09-06T02:45:27.527678: step 13101, loss 0.00672362, acc 1
2016-09-06T02:45:28.339939: step 13102, loss 0.0367609, acc 0.98
2016-09-06T02:45:29.175411: step 13103, loss 0.00204801, acc 1
2016-09-06T02:45:30.008319: step 13104, loss 0.00551705, acc 1
2016-09-06T02:45:30.817856: step 13105, loss 0.0222596, acc 0.98
2016-09-06T02:45:31.579297: step 13106, loss 0.00182945, acc 1
2016-09-06T02:45:32.378343: step 13107, loss 0.00254783, acc 1
2016-09-06T02:45:33.193068: step 13108, loss 0.0116327, acc 1
2016-09-06T02:45:33.981693: step 13109, loss 0.0342381, acc 0.98
2016-09-06T02:45:34.797908: step 13110, loss 0.00390603, acc 1
2016-09-06T02:45:35.630189: step 13111, loss 0.0491131, acc 0.98
2016-09-06T02:45:36.417270: step 13112, loss 0.00189913, acc 1
2016-09-06T02:45:37.211295: step 13113, loss 0.00187125, acc 1
2016-09-06T02:45:38.011742: step 13114, loss 0.103833, acc 0.96
2016-09-06T02:45:38.781870: step 13115, loss 0.00535582, acc 1
2016-09-06T02:45:39.596807: step 13116, loss 0.0018329, acc 1
2016-09-06T02:45:40.393788: step 13117, loss 0.0139579, acc 1
2016-09-06T02:45:41.172792: step 13118, loss 0.0222099, acc 0.98
2016-09-06T02:45:41.983764: step 13119, loss 0.00512277, acc 1
2016-09-06T02:45:42.787772: step 13120, loss 0.0395856, acc 0.98
2016-09-06T02:45:43.598543: step 13121, loss 0.00244853, acc 1
2016-09-06T02:45:44.383571: step 13122, loss 0.0729933, acc 0.96
2016-09-06T02:45:45.193725: step 13123, loss 0.0862895, acc 0.98
2016-09-06T02:45:45.998196: step 13124, loss 0.062422, acc 0.96
2016-09-06T02:45:46.832277: step 13125, loss 0.0209307, acc 0.98
2016-09-06T02:45:47.670123: step 13126, loss 0.0236044, acc 0.98
2016-09-06T02:45:48.471371: step 13127, loss 0.00415105, acc 1
2016-09-06T02:45:49.285717: step 13128, loss 0.0192864, acc 0.98
2016-09-06T02:45:50.095009: step 13129, loss 0.0167816, acc 1
2016-09-06T02:45:50.885255: step 13130, loss 0.00790735, acc 1
2016-09-06T02:45:51.664841: step 13131, loss 0.0379935, acc 0.98
2016-09-06T02:45:52.468342: step 13132, loss 0.0152961, acc 1
2016-09-06T02:45:53.284196: step 13133, loss 0.0290936, acc 0.98
2016-09-06T02:45:54.117003: step 13134, loss 0.0341034, acc 1
2016-09-06T02:45:54.940622: step 13135, loss 0.00334039, acc 1
2016-09-06T02:45:55.737525: step 13136, loss 0.00895018, acc 1
2016-09-06T02:45:56.558828: step 13137, loss 0.0241168, acc 0.98
2016-09-06T02:45:57.394589: step 13138, loss 0.0136236, acc 1
2016-09-06T02:45:58.199083: step 13139, loss 0.0334562, acc 1
2016-09-06T02:45:59.008722: step 13140, loss 0.00603245, acc 1
2016-09-06T02:45:59.837077: step 13141, loss 0.0355339, acc 0.98
2016-09-06T02:46:00.680600: step 13142, loss 0.0335695, acc 0.98
2016-09-06T02:46:01.488487: step 13143, loss 0.00508712, acc 1
2016-09-06T02:46:02.331723: step 13144, loss 0.0107724, acc 1
2016-09-06T02:46:03.111488: step 13145, loss 0.0376235, acc 0.98
2016-09-06T02:46:03.945962: step 13146, loss 0.00178851, acc 1
2016-09-06T02:46:04.765449: step 13147, loss 0.0128867, acc 1
2016-09-06T02:46:05.576045: step 13148, loss 0.00182957, acc 1
2016-09-06T02:46:06.420237: step 13149, loss 0.00228858, acc 1
2016-09-06T02:46:07.267494: step 13150, loss 0.0101495, acc 1
2016-09-06T02:46:08.053358: step 13151, loss 0.00284839, acc 1
2016-09-06T02:46:08.884531: step 13152, loss 0.0040674, acc 1
2016-09-06T02:46:09.747629: step 13153, loss 0.00208274, acc 1
2016-09-06T02:46:10.587916: step 13154, loss 0.0415266, acc 0.98
2016-09-06T02:46:11.380769: step 13155, loss 0.00892241, acc 1
2016-09-06T02:46:12.207625: step 13156, loss 0.0266567, acc 1
2016-09-06T02:46:13.014349: step 13157, loss 0.00284251, acc 1
2016-09-06T02:46:13.831900: step 13158, loss 0.0124517, acc 1
2016-09-06T02:46:14.660904: step 13159, loss 0.00938713, acc 1
2016-09-06T02:46:15.489449: step 13160, loss 0.00204343, acc 1
2016-09-06T02:46:16.303838: step 13161, loss 0.0148729, acc 1
2016-09-06T02:46:17.103837: step 13162, loss 0.0561988, acc 0.98
2016-09-06T02:46:17.905401: step 13163, loss 0.0175264, acc 1
2016-09-06T02:46:18.691735: step 13164, loss 0.0284376, acc 0.98
2016-09-06T02:46:19.523316: step 13165, loss 0.0159692, acc 1
2016-09-06T02:46:20.345669: step 13166, loss 0.00446054, acc 1
2016-09-06T02:46:21.140477: step 13167, loss 0.00196727, acc 1
2016-09-06T02:46:21.929979: step 13168, loss 0.0628707, acc 0.98
2016-09-06T02:46:22.754409: step 13169, loss 0.0049506, acc 1
2016-09-06T02:46:23.564925: step 13170, loss 0.0179535, acc 0.98
2016-09-06T02:46:24.342303: step 13171, loss 0.00573837, acc 1
2016-09-06T02:46:25.151934: step 13172, loss 0.0292695, acc 0.98
2016-09-06T02:46:25.950530: step 13173, loss 0.00535657, acc 1
2016-09-06T02:46:26.768847: step 13174, loss 0.00228104, acc 1
2016-09-06T02:46:27.596588: step 13175, loss 0.121713, acc 0.98
2016-09-06T02:46:28.376028: step 13176, loss 0.0152014, acc 1
2016-09-06T02:46:29.189377: step 13177, loss 0.00175236, acc 1
2016-09-06T02:46:29.972657: step 13178, loss 0.00190316, acc 1
2016-09-06T02:46:30.834056: step 13179, loss 0.0268895, acc 0.98
2016-09-06T02:46:31.641091: step 13180, loss 0.0257728, acc 1
2016-09-06T02:46:32.476172: step 13181, loss 0.0237411, acc 1
2016-09-06T02:46:33.274109: step 13182, loss 0.0174819, acc 1
2016-09-06T02:46:34.099385: step 13183, loss 0.0098517, acc 1
2016-09-06T02:46:34.956153: step 13184, loss 0.00168009, acc 1
2016-09-06T02:46:35.756276: step 13185, loss 0.00223013, acc 1
2016-09-06T02:46:36.597848: step 13186, loss 0.00243241, acc 1
2016-09-06T02:46:37.430351: step 13187, loss 0.0227273, acc 0.98
2016-09-06T02:46:38.253402: step 13188, loss 0.00195826, acc 1
2016-09-06T02:46:39.071728: step 13189, loss 0.0141663, acc 1
2016-09-06T02:46:39.886833: step 13190, loss 0.0290179, acc 1
2016-09-06T02:46:40.714428: step 13191, loss 0.00700715, acc 1
2016-09-06T02:46:41.521727: step 13192, loss 0.00171517, acc 1
2016-09-06T02:46:42.345307: step 13193, loss 0.0149652, acc 1
2016-09-06T02:46:43.157107: step 13194, loss 0.0115112, acc 1
2016-09-06T02:46:43.990616: step 13195, loss 0.00354396, acc 1
2016-09-06T02:46:44.841108: step 13196, loss 0.00177487, acc 1
2016-09-06T02:46:45.634983: step 13197, loss 0.0190943, acc 1
2016-09-06T02:46:46.468111: step 13198, loss 0.00173984, acc 1
2016-09-06T02:46:47.284312: step 13199, loss 0.00289674, acc 1
2016-09-06T02:46:48.100106: step 13200, loss 0.0380189, acc 0.96

Evaluation:
2016-09-06T02:46:51.801725: step 13200, loss 2.15314, acc 0.719512

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-13200

2016-09-06T02:46:53.617902: step 13201, loss 0.0096745, acc 1
2016-09-06T02:46:54.464072: step 13202, loss 0.00744583, acc 1
2016-09-06T02:46:55.294624: step 13203, loss 0.00169586, acc 1
2016-09-06T02:46:56.128190: step 13204, loss 0.00935107, acc 1
2016-09-06T02:46:56.947962: step 13205, loss 0.0154942, acc 1
2016-09-06T02:46:57.750588: step 13206, loss 0.0400141, acc 0.98
2016-09-06T02:46:58.561419: step 13207, loss 0.00430165, acc 1
2016-09-06T02:46:59.379456: step 13208, loss 0.0524778, acc 0.98
2016-09-06T02:47:00.183805: step 13209, loss 0.0018651, acc 1
2016-09-06T02:47:01.016180: step 13210, loss 0.00180828, acc 1
2016-09-06T02:47:01.841595: step 13211, loss 0.00626782, acc 1
2016-09-06T02:47:02.676060: step 13212, loss 0.0131004, acc 1
2016-09-06T02:47:03.493924: step 13213, loss 0.0157595, acc 1
2016-09-06T02:47:04.344746: step 13214, loss 0.00164216, acc 1
2016-09-06T02:47:05.145530: step 13215, loss 0.00622409, acc 1
2016-09-06T02:47:05.933365: step 13216, loss 0.0169883, acc 0.98
2016-09-06T02:47:06.749360: step 13217, loss 0.0173812, acc 1
2016-09-06T02:47:07.556995: step 13218, loss 0.0270097, acc 0.98
2016-09-06T02:47:08.362625: step 13219, loss 0.0109716, acc 1
2016-09-06T02:47:09.153446: step 13220, loss 0.00220225, acc 1
2016-09-06T02:47:09.981685: step 13221, loss 0.00171329, acc 1
2016-09-06T02:47:10.756047: step 13222, loss 0.0186621, acc 1
2016-09-06T02:47:11.593862: step 13223, loss 0.0299493, acc 0.98
2016-09-06T02:47:12.389083: step 13224, loss 0.00465281, acc 1
2016-09-06T02:47:13.165281: step 13225, loss 0.0439564, acc 0.96
2016-09-06T02:47:14.007115: step 13226, loss 0.0192527, acc 1
2016-09-06T02:47:14.809249: step 13227, loss 0.00571926, acc 1
2016-09-06T02:47:15.625717: step 13228, loss 0.0210275, acc 1
2016-09-06T02:47:16.428913: step 13229, loss 0.0297276, acc 0.98
2016-09-06T02:47:17.259980: step 13230, loss 0.00430514, acc 1
2016-09-06T02:47:18.057422: step 13231, loss 0.0674962, acc 0.96
2016-09-06T02:47:18.882754: step 13232, loss 0.00262819, acc 1
2016-09-06T02:47:19.679928: step 13233, loss 0.0157263, acc 0.98
2016-09-06T02:47:20.468729: step 13234, loss 0.0693474, acc 0.96
2016-09-06T02:47:21.283240: step 13235, loss 0.0139523, acc 1
2016-09-06T02:47:22.092030: step 13236, loss 0.0564174, acc 0.98
2016-09-06T02:47:22.878205: step 13237, loss 0.199815, acc 0.98
2016-09-06T02:47:23.674157: step 13238, loss 0.00568052, acc 1
2016-09-06T02:47:24.493117: step 13239, loss 0.0028806, acc 1
2016-09-06T02:47:25.263417: step 13240, loss 0.0265622, acc 0.98
2016-09-06T02:47:26.075728: step 13241, loss 0.00619938, acc 1
2016-09-06T02:47:26.868019: step 13242, loss 0.0223226, acc 1
2016-09-06T02:47:27.663132: step 13243, loss 0.0244685, acc 0.98
2016-09-06T02:47:28.489681: step 13244, loss 0.0451485, acc 0.98
2016-09-06T02:47:29.325554: step 13245, loss 0.011152, acc 1
2016-09-06T02:47:30.140830: step 13246, loss 0.0211334, acc 0.98
2016-09-06T02:47:30.964070: step 13247, loss 0.0167749, acc 1
2016-09-06T02:47:31.720785: step 13248, loss 0.00635336, acc 1
2016-09-06T02:47:32.515254: step 13249, loss 0.0057535, acc 1
2016-09-06T02:47:33.331420: step 13250, loss 0.0305777, acc 0.98
2016-09-06T02:47:34.140064: step 13251, loss 0.0093745, acc 1
2016-09-06T02:47:34.914943: step 13252, loss 0.00480447, acc 1
2016-09-06T02:47:35.713634: step 13253, loss 0.0469464, acc 0.98
2016-09-06T02:47:36.520363: step 13254, loss 0.00315093, acc 1
2016-09-06T02:47:37.301424: step 13255, loss 0.00289662, acc 1
2016-09-06T02:47:38.129095: step 13256, loss 0.0112884, acc 1
2016-09-06T02:47:38.957542: step 13257, loss 0.032728, acc 1
2016-09-06T02:47:39.748206: step 13258, loss 0.00246628, acc 1
2016-09-06T02:47:40.555375: step 13259, loss 0.0350538, acc 1
2016-09-06T02:47:41.391322: step 13260, loss 0.00725655, acc 1
2016-09-06T02:47:42.186529: step 13261, loss 0.00360794, acc 1
2016-09-06T02:47:43.009209: step 13262, loss 0.00262102, acc 1
2016-09-06T02:47:43.812499: step 13263, loss 0.00264264, acc 1
2016-09-06T02:47:44.587110: step 13264, loss 0.00271609, acc 1
2016-09-06T02:47:45.396327: step 13265, loss 0.0032704, acc 1
2016-09-06T02:47:46.221981: step 13266, loss 0.00271196, acc 1
2016-09-06T02:47:46.987507: step 13267, loss 0.0458265, acc 0.98
2016-09-06T02:47:47.792149: step 13268, loss 0.00808511, acc 1
2016-09-06T02:47:48.582518: step 13269, loss 0.00271887, acc 1
2016-09-06T02:47:49.389461: step 13270, loss 0.00452677, acc 1
2016-09-06T02:47:50.217852: step 13271, loss 0.0171244, acc 1
2016-09-06T02:47:51.024161: step 13272, loss 0.00872416, acc 1
2016-09-06T02:47:51.829876: step 13273, loss 0.0223981, acc 1
2016-09-06T02:47:52.659521: step 13274, loss 0.0455059, acc 0.96
2016-09-06T02:47:53.472437: step 13275, loss 0.0172706, acc 0.98
2016-09-06T02:47:54.272663: step 13276, loss 0.0036495, acc 1
2016-09-06T02:47:55.077918: step 13277, loss 0.00366004, acc 1
2016-09-06T02:47:55.903776: step 13278, loss 0.014935, acc 1
2016-09-06T02:47:56.699827: step 13279, loss 0.0530159, acc 0.96
2016-09-06T02:47:57.512987: step 13280, loss 0.00298162, acc 1
2016-09-06T02:47:58.344812: step 13281, loss 0.0837083, acc 0.98
2016-09-06T02:47:59.125491: step 13282, loss 0.0132435, acc 1
2016-09-06T02:47:59.930902: step 13283, loss 0.00247224, acc 1
2016-09-06T02:48:00.776676: step 13284, loss 0.00917208, acc 1
2016-09-06T02:48:01.542901: step 13285, loss 0.00279155, acc 1
2016-09-06T02:48:02.322365: step 13286, loss 0.0298565, acc 1
2016-09-06T02:48:03.135189: step 13287, loss 0.0107765, acc 1
2016-09-06T02:48:03.915552: step 13288, loss 0.00236786, acc 1
2016-09-06T02:48:04.716952: step 13289, loss 0.0246145, acc 0.98
2016-09-06T02:48:05.564231: step 13290, loss 0.0118992, acc 1
2016-09-06T02:48:06.354815: step 13291, loss 0.0539419, acc 0.96
2016-09-06T02:48:07.170505: step 13292, loss 0.00339441, acc 1
2016-09-06T02:48:07.985354: step 13293, loss 0.0027225, acc 1
2016-09-06T02:48:08.808471: step 13294, loss 0.0293702, acc 0.98
2016-09-06T02:48:09.638027: step 13295, loss 0.00234443, acc 1
2016-09-06T02:48:10.422748: step 13296, loss 0.0106133, acc 1
2016-09-06T02:48:11.210400: step 13297, loss 0.0468124, acc 0.96
2016-09-06T02:48:12.031752: step 13298, loss 0.00287012, acc 1
2016-09-06T02:48:12.861425: step 13299, loss 0.00232213, acc 1
2016-09-06T02:48:13.633902: step 13300, loss 0.0420488, acc 0.98

Evaluation:
2016-09-06T02:48:17.399664: step 13300, loss 2.25696, acc 0.718574

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-13300

2016-09-06T02:48:19.359584: step 13301, loss 0.0138285, acc 1
2016-09-06T02:48:20.198611: step 13302, loss 0.00429987, acc 1
2016-09-06T02:48:21.009660: step 13303, loss 0.0143907, acc 1
2016-09-06T02:48:21.834957: step 13304, loss 0.00296436, acc 1
2016-09-06T02:48:22.671230: step 13305, loss 0.00219814, acc 1
2016-09-06T02:48:23.458682: step 13306, loss 0.0629535, acc 0.98
2016-09-06T02:48:24.275026: step 13307, loss 0.00400961, acc 1
2016-09-06T02:48:25.112497: step 13308, loss 0.00391152, acc 1
2016-09-06T02:48:25.972641: step 13309, loss 0.0288969, acc 0.98
2016-09-06T02:48:26.811042: step 13310, loss 0.00356548, acc 1
2016-09-06T02:48:27.643460: step 13311, loss 0.00300904, acc 1
2016-09-06T02:48:28.470015: step 13312, loss 0.00297332, acc 1
2016-09-06T02:48:29.276891: step 13313, loss 0.0143973, acc 1
2016-09-06T02:48:30.100847: step 13314, loss 0.0238418, acc 0.98
2016-09-06T02:48:30.905084: step 13315, loss 0.0736308, acc 0.98
2016-09-06T02:48:31.713525: step 13316, loss 0.0399452, acc 0.96
2016-09-06T02:48:32.549139: step 13317, loss 0.0338213, acc 0.98
2016-09-06T02:48:33.348666: step 13318, loss 0.014374, acc 1
2016-09-06T02:48:34.182150: step 13319, loss 0.0163513, acc 0.98
2016-09-06T02:48:34.985921: step 13320, loss 0.0165718, acc 0.98
2016-09-06T02:48:35.800853: step 13321, loss 0.0233044, acc 0.98
2016-09-06T02:48:36.581908: step 13322, loss 0.0021954, acc 1
2016-09-06T02:48:37.403872: step 13323, loss 0.0345064, acc 0.98
2016-09-06T02:48:38.196870: step 13324, loss 0.0175924, acc 1
2016-09-06T02:48:38.994415: step 13325, loss 0.0127406, acc 1
2016-09-06T02:48:39.814493: step 13326, loss 0.0375196, acc 0.98
2016-09-06T02:48:40.621591: step 13327, loss 0.0285408, acc 1
2016-09-06T02:48:41.409214: step 13328, loss 0.00503384, acc 1
2016-09-06T02:48:42.242152: step 13329, loss 0.0097028, acc 1
2016-09-06T02:48:43.077963: step 13330, loss 0.0140411, acc 1
2016-09-06T02:48:43.865041: step 13331, loss 0.107652, acc 0.98
2016-09-06T02:48:44.686285: step 13332, loss 0.00199553, acc 1
2016-09-06T02:48:45.513722: step 13333, loss 0.00631896, acc 1
2016-09-06T02:48:46.305552: step 13334, loss 0.032668, acc 0.98
2016-09-06T02:48:47.107640: step 13335, loss 0.0106362, acc 1
2016-09-06T02:48:47.933415: step 13336, loss 0.010331, acc 1
2016-09-06T02:48:48.721469: step 13337, loss 0.0125281, acc 1
2016-09-06T02:48:49.525185: step 13338, loss 0.00211888, acc 1
2016-09-06T02:48:50.325988: step 13339, loss 0.0326066, acc 0.98
2016-09-06T02:48:51.140851: step 13340, loss 0.0019451, acc 1
2016-09-06T02:48:51.955141: step 13341, loss 0.0173098, acc 1
2016-09-06T02:48:52.769202: step 13342, loss 0.0460635, acc 0.98
2016-09-06T02:48:53.566376: step 13343, loss 0.0426884, acc 0.98
2016-09-06T02:48:54.376071: step 13344, loss 0.024172, acc 0.98
2016-09-06T02:48:55.187135: step 13345, loss 0.00202504, acc 1
2016-09-06T02:48:55.986425: step 13346, loss 0.0023437, acc 1
2016-09-06T02:48:56.784244: step 13347, loss 0.00386024, acc 1
2016-09-06T02:48:57.600955: step 13348, loss 0.0048329, acc 1
2016-09-06T02:48:58.400410: step 13349, loss 0.00684262, acc 1
2016-09-06T02:48:59.223883: step 13350, loss 0.148025, acc 0.96
2016-09-06T02:49:00.055547: step 13351, loss 0.00285615, acc 1
2016-09-06T02:49:00.872407: step 13352, loss 0.00699758, acc 1
2016-09-06T02:49:01.681918: step 13353, loss 0.0199029, acc 1
2016-09-06T02:49:02.489933: step 13354, loss 0.00184709, acc 1
2016-09-06T02:49:03.279487: step 13355, loss 0.0298328, acc 0.98
2016-09-06T02:49:04.086484: step 13356, loss 0.039591, acc 0.96
2016-09-06T02:49:04.920870: step 13357, loss 0.0216966, acc 1
2016-09-06T02:49:05.725098: step 13358, loss 0.0199773, acc 1
2016-09-06T02:49:06.514650: step 13359, loss 0.0034493, acc 1
2016-09-06T02:49:07.320179: step 13360, loss 0.0578493, acc 0.98
2016-09-06T02:49:08.111675: step 13361, loss 0.00390502, acc 1
2016-09-06T02:49:08.891313: step 13362, loss 0.00242337, acc 1
2016-09-06T02:49:09.718514: step 13363, loss 0.0139721, acc 1
2016-09-06T02:49:10.499210: step 13364, loss 0.00565277, acc 1
2016-09-06T02:49:11.362974: step 13365, loss 0.0571243, acc 0.94
2016-09-06T02:49:12.196236: step 13366, loss 0.00165794, acc 1
2016-09-06T02:49:12.992412: step 13367, loss 0.00228164, acc 1
2016-09-06T02:49:13.785913: step 13368, loss 0.00459446, acc 1
2016-09-06T02:49:14.602820: step 13369, loss 0.00215984, acc 1
2016-09-06T02:49:15.417415: step 13370, loss 0.00979058, acc 1
2016-09-06T02:49:16.223395: step 13371, loss 0.0018711, acc 1
2016-09-06T02:49:17.055662: step 13372, loss 0.0426201, acc 0.98
2016-09-06T02:49:17.866307: step 13373, loss 0.0180292, acc 0.98
2016-09-06T02:49:18.671046: step 13374, loss 0.00176052, acc 1
2016-09-06T02:49:19.505764: step 13375, loss 0.0620104, acc 0.98
2016-09-06T02:49:20.294131: step 13376, loss 0.00286501, acc 1
2016-09-06T02:49:21.124832: step 13377, loss 0.00360828, acc 1
2016-09-06T02:49:21.955199: step 13378, loss 0.0269104, acc 0.98
2016-09-06T02:49:22.762925: step 13379, loss 0.0213128, acc 1
2016-09-06T02:49:23.586815: step 13380, loss 0.00264331, acc 1
2016-09-06T02:49:24.416984: step 13381, loss 0.015856, acc 1
2016-09-06T02:49:25.223993: step 13382, loss 0.0139588, acc 1
2016-09-06T02:49:26.026131: step 13383, loss 0.0218236, acc 0.98
2016-09-06T02:49:26.852540: step 13384, loss 0.0081533, acc 1
2016-09-06T02:49:27.687704: step 13385, loss 0.0666906, acc 0.98
2016-09-06T02:49:28.528955: step 13386, loss 0.0151831, acc 1
2016-09-06T02:49:29.369759: step 13387, loss 0.00622709, acc 1
2016-09-06T02:49:30.151888: step 13388, loss 0.00169394, acc 1
2016-09-06T02:49:30.976709: step 13389, loss 0.0447784, acc 0.96
2016-09-06T02:49:31.804479: step 13390, loss 0.0179861, acc 0.98
2016-09-06T02:49:32.605123: step 13391, loss 0.00619168, acc 1
2016-09-06T02:49:33.369604: step 13392, loss 0.0361728, acc 0.98
2016-09-06T02:49:34.203120: step 13393, loss 0.0103254, acc 1
2016-09-06T02:49:35.002139: step 13394, loss 0.0105966, acc 1
2016-09-06T02:49:35.795343: step 13395, loss 0.0386098, acc 0.98
2016-09-06T02:49:36.641669: step 13396, loss 0.0301676, acc 0.98
2016-09-06T02:49:37.455614: step 13397, loss 0.00853367, acc 1
2016-09-06T02:49:38.245967: step 13398, loss 0.00495198, acc 1
2016-09-06T02:49:39.076675: step 13399, loss 0.00175446, acc 1
2016-09-06T02:49:39.910774: step 13400, loss 0.00173596, acc 1

Evaluation:
2016-09-06T02:49:43.621751: step 13400, loss 1.9469, acc 0.712946

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-13400

2016-09-06T02:49:45.519497: step 13401, loss 0.00195693, acc 1
2016-09-06T02:49:46.330197: step 13402, loss 0.00162025, acc 1
2016-09-06T02:49:47.141194: step 13403, loss 0.0100044, acc 1
2016-09-06T02:49:47.978119: step 13404, loss 0.00705438, acc 1
2016-09-06T02:49:48.836761: step 13405, loss 0.0025314, acc 1
2016-09-06T02:49:49.636172: step 13406, loss 0.00633703, acc 1
2016-09-06T02:49:50.435135: step 13407, loss 0.0133644, acc 1
2016-09-06T02:49:51.251629: step 13408, loss 0.0256814, acc 0.98
2016-09-06T02:49:52.053989: step 13409, loss 0.0734335, acc 0.98
2016-09-06T02:49:52.868874: step 13410, loss 0.00943404, acc 1
2016-09-06T02:49:53.690590: step 13411, loss 0.00840964, acc 1
2016-09-06T02:49:54.493050: step 13412, loss 0.013831, acc 1
2016-09-06T02:49:55.301237: step 13413, loss 0.00190624, acc 1
2016-09-06T02:49:56.126311: step 13414, loss 0.0218769, acc 0.98
2016-09-06T02:49:56.950025: step 13415, loss 0.0298405, acc 0.98
2016-09-06T02:49:57.729450: step 13416, loss 0.00419215, acc 1
2016-09-06T02:49:58.534801: step 13417, loss 0.0869932, acc 0.96
2016-09-06T02:49:59.322533: step 13418, loss 0.0855235, acc 0.98
2016-09-06T02:50:00.112407: step 13419, loss 0.00733169, acc 1
2016-09-06T02:50:00.982243: step 13420, loss 0.00422699, acc 1
2016-09-06T02:50:01.788540: step 13421, loss 0.0213954, acc 0.98
2016-09-06T02:50:02.591513: step 13422, loss 0.0113922, acc 1
2016-09-06T02:50:03.399702: step 13423, loss 0.0100375, acc 1
2016-09-06T02:50:04.242574: step 13424, loss 0.0028427, acc 1
2016-09-06T02:50:05.027715: step 13425, loss 0.0070733, acc 1
2016-09-06T02:50:05.848322: step 13426, loss 0.0521836, acc 0.96
2016-09-06T02:50:06.668934: step 13427, loss 0.00341389, acc 1
2016-09-06T02:50:07.456403: step 13428, loss 0.0202258, acc 1
2016-09-06T02:50:08.275790: step 13429, loss 0.00212571, acc 1
2016-09-06T02:50:09.078797: step 13430, loss 0.0042198, acc 1
2016-09-06T02:50:09.877775: step 13431, loss 0.00209655, acc 1
2016-09-06T02:50:10.653153: step 13432, loss 0.00377583, acc 1
2016-09-06T02:50:11.477454: step 13433, loss 0.0137463, acc 1
2016-09-06T02:50:12.258101: step 13434, loss 0.0224218, acc 1
2016-09-06T02:50:13.052105: step 13435, loss 0.0123292, acc 1
2016-09-06T02:50:13.853373: step 13436, loss 0.0131219, acc 1
2016-09-06T02:50:14.640155: step 13437, loss 0.0173289, acc 1
2016-09-06T02:50:15.438132: step 13438, loss 0.0560492, acc 0.98
2016-09-06T02:50:16.244688: step 13439, loss 0.00227271, acc 1
2016-09-06T02:50:17.008716: step 13440, loss 0.00392156, acc 1
2016-09-06T02:50:17.857567: step 13441, loss 0.00208484, acc 1
2016-09-06T02:50:18.663477: step 13442, loss 0.018929, acc 0.98
2016-09-06T02:50:19.458045: step 13443, loss 0.00247668, acc 1
2016-09-06T02:50:20.271457: step 13444, loss 0.0281048, acc 0.98
2016-09-06T02:50:21.107253: step 13445, loss 0.00206115, acc 1
2016-09-06T02:50:21.909441: step 13446, loss 0.00410293, acc 1
2016-09-06T02:50:22.734014: step 13447, loss 0.00200711, acc 1
2016-09-06T02:50:23.549571: step 13448, loss 0.002222, acc 1
2016-09-06T02:50:24.334158: step 13449, loss 0.018343, acc 1
2016-09-06T02:50:25.131953: step 13450, loss 0.00205113, acc 1
2016-09-06T02:50:25.957908: step 13451, loss 0.0603995, acc 0.96
2016-09-06T02:50:26.744073: step 13452, loss 0.00285766, acc 1
2016-09-06T02:50:27.551986: step 13453, loss 0.00422157, acc 1
2016-09-06T02:50:28.349417: step 13454, loss 0.00324723, acc 1
2016-09-06T02:50:29.135789: step 13455, loss 0.00208527, acc 1
2016-09-06T02:50:29.958688: step 13456, loss 0.0438649, acc 0.96
2016-09-06T02:50:30.767002: step 13457, loss 0.00248265, acc 1
2016-09-06T02:50:31.581136: step 13458, loss 0.00201941, acc 1
2016-09-06T02:50:32.372134: step 13459, loss 0.00198906, acc 1
2016-09-06T02:50:33.169300: step 13460, loss 0.00863178, acc 1
2016-09-06T02:50:33.989905: step 13461, loss 0.0717219, acc 0.98
2016-09-06T02:50:34.815442: step 13462, loss 0.0259402, acc 1
2016-09-06T02:50:35.636661: step 13463, loss 0.00601873, acc 1
2016-09-06T02:50:36.430539: step 13464, loss 0.0101525, acc 1
2016-09-06T02:50:37.232249: step 13465, loss 0.0156186, acc 1
2016-09-06T02:50:38.077779: step 13466, loss 0.024299, acc 0.98
2016-09-06T02:50:38.877452: step 13467, loss 0.00825875, acc 1
2016-09-06T02:50:39.690565: step 13468, loss 0.00250505, acc 1
2016-09-06T02:50:40.488992: step 13469, loss 0.0189034, acc 0.98
2016-09-06T02:50:41.318336: step 13470, loss 0.00294523, acc 1
2016-09-06T02:50:42.127929: step 13471, loss 0.017447, acc 1
2016-09-06T02:50:42.941626: step 13472, loss 0.0474503, acc 0.98
2016-09-06T02:50:43.728874: step 13473, loss 0.00389271, acc 1
2016-09-06T02:50:44.528079: step 13474, loss 0.00355098, acc 1
2016-09-06T02:50:45.342320: step 13475, loss 0.00980429, acc 1
2016-09-06T02:50:46.121424: step 13476, loss 0.0129861, acc 1
2016-09-06T02:50:46.929423: step 13477, loss 0.00322254, acc 1
2016-09-06T02:50:47.748170: step 13478, loss 0.0394964, acc 1
2016-09-06T02:50:48.505646: step 13479, loss 0.0159845, acc 0.98
2016-09-06T02:50:49.303534: step 13480, loss 0.00204675, acc 1
2016-09-06T02:50:50.121160: step 13481, loss 0.00537174, acc 1
2016-09-06T02:50:50.950805: step 13482, loss 0.0219574, acc 0.98
2016-09-06T02:50:51.752239: step 13483, loss 0.0464665, acc 0.96
2016-09-06T02:50:52.588608: step 13484, loss 0.00212629, acc 1
2016-09-06T02:50:53.361057: step 13485, loss 0.00214016, acc 1
2016-09-06T02:50:54.161071: step 13486, loss 0.00212458, acc 1
2016-09-06T02:50:54.989457: step 13487, loss 0.0105126, acc 1
2016-09-06T02:50:55.768540: step 13488, loss 0.0091981, acc 1
2016-09-06T02:50:56.560236: step 13489, loss 0.0127934, acc 1
2016-09-06T02:50:57.386591: step 13490, loss 0.00228682, acc 1
2016-09-06T02:50:58.227003: step 13491, loss 0.0802166, acc 0.96
2016-09-06T02:50:59.002909: step 13492, loss 0.00240358, acc 1
2016-09-06T02:50:59.832493: step 13493, loss 0.0311681, acc 0.98
2016-09-06T02:51:00.648649: step 13494, loss 0.00194727, acc 1
2016-09-06T02:51:01.457047: step 13495, loss 0.00202332, acc 1
2016-09-06T02:51:02.231622: step 13496, loss 0.0283806, acc 0.98
2016-09-06T02:51:03.005490: step 13497, loss 0.00202009, acc 1
2016-09-06T02:51:03.820656: step 13498, loss 0.0477361, acc 0.98
2016-09-06T02:51:04.627439: step 13499, loss 0.00346674, acc 1
2016-09-06T02:51:05.424479: step 13500, loss 0.00227086, acc 1

Evaluation:
2016-09-06T02:51:09.166383: step 13500, loss 2.39402, acc 0.727955

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-13500

2016-09-06T02:51:11.017939: step 13501, loss 0.0230313, acc 0.98
2016-09-06T02:51:11.817609: step 13502, loss 0.0292136, acc 0.98
2016-09-06T02:51:12.628113: step 13503, loss 0.0021486, acc 1
2016-09-06T02:51:13.476141: step 13504, loss 0.0239879, acc 0.98
2016-09-06T02:51:14.292477: step 13505, loss 0.0437423, acc 0.98
2016-09-06T02:51:15.105854: step 13506, loss 0.00411511, acc 1
2016-09-06T02:51:15.928318: step 13507, loss 0.014936, acc 1
2016-09-06T02:51:16.742213: step 13508, loss 0.0543891, acc 0.96
2016-09-06T02:51:17.531958: step 13509, loss 0.00602397, acc 1
2016-09-06T02:51:18.354088: step 13510, loss 0.0294013, acc 0.98
2016-09-06T02:51:19.177025: step 13511, loss 0.0180361, acc 1
2016-09-06T02:51:19.968073: step 13512, loss 0.00679815, acc 1
2016-09-06T02:51:20.797987: step 13513, loss 0.00277415, acc 1
2016-09-06T02:51:21.608337: step 13514, loss 0.00855755, acc 1
2016-09-06T02:51:22.394034: step 13515, loss 0.0128032, acc 1
2016-09-06T02:51:23.246608: step 13516, loss 0.101562, acc 0.96
2016-09-06T02:51:24.057129: step 13517, loss 0.0101507, acc 1
2016-09-06T02:51:24.866934: step 13518, loss 0.036811, acc 0.98
2016-09-06T02:51:25.658072: step 13519, loss 0.00259265, acc 1
2016-09-06T02:51:26.462139: step 13520, loss 0.00901733, acc 1
2016-09-06T02:51:27.253405: step 13521, loss 0.00274743, acc 1
2016-09-06T02:51:28.082244: step 13522, loss 0.00793595, acc 1
2016-09-06T02:51:28.905210: step 13523, loss 0.00270088, acc 1
2016-09-06T02:51:29.736347: step 13524, loss 0.00530889, acc 1
2016-09-06T02:51:30.551090: step 13525, loss 0.0333475, acc 0.98
2016-09-06T02:51:31.387456: step 13526, loss 0.0258034, acc 0.98
2016-09-06T02:51:32.165025: step 13527, loss 0.00787308, acc 1
2016-09-06T02:51:32.978563: step 13528, loss 0.010616, acc 1
2016-09-06T02:51:33.811886: step 13529, loss 0.00776091, acc 1
2016-09-06T02:51:34.582355: step 13530, loss 0.00352815, acc 1
2016-09-06T02:51:35.403092: step 13531, loss 0.00515785, acc 1
2016-09-06T02:51:36.248633: step 13532, loss 0.00229091, acc 1
2016-09-06T02:51:37.037660: step 13533, loss 0.0212983, acc 0.98
2016-09-06T02:51:37.847655: step 13534, loss 0.00244751, acc 1
2016-09-06T02:51:38.647868: step 13535, loss 0.0169447, acc 0.98
2016-09-06T02:51:39.419773: step 13536, loss 0.00243275, acc 1
2016-09-06T02:51:40.216805: step 13537, loss 0.00224858, acc 1
2016-09-06T02:51:41.033533: step 13538, loss 0.0291747, acc 0.98
2016-09-06T02:51:41.829703: step 13539, loss 0.0231684, acc 1
2016-09-06T02:51:42.613271: step 13540, loss 0.00582877, acc 1
2016-09-06T02:51:43.423922: step 13541, loss 0.00228226, acc 1
2016-09-06T02:51:44.216155: step 13542, loss 0.00483349, acc 1
2016-09-06T02:51:45.017054: step 13543, loss 0.00241374, acc 1
2016-09-06T02:51:45.851552: step 13544, loss 0.00270138, acc 1
2016-09-06T02:51:46.647523: step 13545, loss 0.00998659, acc 1
2016-09-06T02:51:47.456701: step 13546, loss 0.0636379, acc 0.98
2016-09-06T02:51:48.257768: step 13547, loss 0.0165129, acc 0.98
2016-09-06T02:51:49.038970: step 13548, loss 0.0148828, acc 1
2016-09-06T02:51:49.850328: step 13549, loss 0.0481529, acc 0.98
2016-09-06T02:51:50.649351: step 13550, loss 0.00831823, acc 1
2016-09-06T02:51:51.467311: step 13551, loss 0.0433046, acc 0.98
2016-09-06T02:51:52.256359: step 13552, loss 0.0205478, acc 0.98
2016-09-06T02:51:53.013944: step 13553, loss 0.00428523, acc 1
2016-09-06T02:51:53.809094: step 13554, loss 0.0277792, acc 0.98
2016-09-06T02:51:54.666668: step 13555, loss 0.120799, acc 0.98
2016-09-06T02:51:55.460721: step 13556, loss 0.0263054, acc 0.98
2016-09-06T02:51:56.258825: step 13557, loss 0.00954596, acc 1
2016-09-06T02:51:57.093566: step 13558, loss 0.00196256, acc 1
2016-09-06T02:51:57.899858: step 13559, loss 0.011931, acc 1
2016-09-06T02:51:58.702010: step 13560, loss 0.0278026, acc 0.98
2016-09-06T02:51:59.520870: step 13561, loss 0.00224486, acc 1
2016-09-06T02:52:00.348905: step 13562, loss 0.00179437, acc 1
2016-09-06T02:52:01.140861: step 13563, loss 0.0107809, acc 1
2016-09-06T02:52:01.983552: step 13564, loss 0.0060366, acc 1
2016-09-06T02:52:02.795190: step 13565, loss 0.0191371, acc 1
2016-09-06T02:52:03.579153: step 13566, loss 0.0344326, acc 0.98
2016-09-06T02:52:04.371444: step 13567, loss 0.042329, acc 0.98
2016-09-06T02:52:05.192663: step 13568, loss 0.0197424, acc 1
2016-09-06T02:52:05.996060: step 13569, loss 0.0313991, acc 0.98
2016-09-06T02:52:06.786626: step 13570, loss 0.00525357, acc 1
2016-09-06T02:52:07.600559: step 13571, loss 0.0338742, acc 0.98
2016-09-06T02:52:08.364988: step 13572, loss 0.00673604, acc 1
2016-09-06T02:52:09.171501: step 13573, loss 0.00374645, acc 1
2016-09-06T02:52:09.971598: step 13574, loss 0.00411835, acc 1
2016-09-06T02:52:10.779316: step 13575, loss 0.00336479, acc 1
2016-09-06T02:52:11.596677: step 13576, loss 0.0350367, acc 0.98
2016-09-06T02:52:12.390144: step 13577, loss 0.0198672, acc 0.98
2016-09-06T02:52:13.185528: step 13578, loss 0.0161827, acc 1
2016-09-06T02:52:14.004666: step 13579, loss 0.00199435, acc 1
2016-09-06T02:52:14.780981: step 13580, loss 0.00252368, acc 1
2016-09-06T02:52:15.583190: step 13581, loss 0.0174526, acc 0.98
2016-09-06T02:52:16.422154: step 13582, loss 0.0241979, acc 0.98
2016-09-06T02:52:17.212203: step 13583, loss 0.0124928, acc 1
2016-09-06T02:52:18.040277: step 13584, loss 0.0244351, acc 1
2016-09-06T02:52:18.859047: step 13585, loss 0.0199964, acc 0.98
2016-09-06T02:52:19.687095: step 13586, loss 0.00434888, acc 1
2016-09-06T02:52:20.489658: step 13587, loss 0.108122, acc 0.96
2016-09-06T02:52:21.322287: step 13588, loss 0.0065862, acc 1
2016-09-06T02:52:22.173352: step 13589, loss 0.00226237, acc 1
2016-09-06T02:52:22.989266: step 13590, loss 0.0248477, acc 1
2016-09-06T02:52:23.804743: step 13591, loss 0.00197353, acc 1
2016-09-06T02:52:24.609659: step 13592, loss 0.0019466, acc 1
2016-09-06T02:52:25.412128: step 13593, loss 0.00237189, acc 1
2016-09-06T02:52:26.208636: step 13594, loss 0.00340388, acc 1
2016-09-06T02:52:27.060450: step 13595, loss 0.00343775, acc 1
2016-09-06T02:52:27.909052: step 13596, loss 0.0524009, acc 0.96
2016-09-06T02:52:28.732024: step 13597, loss 0.00284003, acc 1
2016-09-06T02:52:29.579989: step 13598, loss 0.0179414, acc 1
2016-09-06T02:52:30.380839: step 13599, loss 0.0130706, acc 1
2016-09-06T02:52:31.199227: step 13600, loss 0.0396603, acc 0.98

Evaluation:
2016-09-06T02:52:34.942416: step 13600, loss 2.258, acc 0.713884

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-13600

2016-09-06T02:52:36.729369: step 13601, loss 0.0166476, acc 1
2016-09-06T02:52:37.544107: step 13602, loss 0.0175728, acc 1
2016-09-06T02:52:38.342575: step 13603, loss 0.00205807, acc 1
2016-09-06T02:52:39.174447: step 13604, loss 0.00452256, acc 1
2016-09-06T02:52:39.997071: step 13605, loss 0.0153369, acc 1
2016-09-06T02:52:40.802933: step 13606, loss 0.026905, acc 1
2016-09-06T02:52:41.642648: step 13607, loss 0.0123082, acc 1
2016-09-06T02:52:42.456085: step 13608, loss 0.00602517, acc 1
2016-09-06T02:52:43.266974: step 13609, loss 0.0273972, acc 0.98
2016-09-06T02:52:44.091965: step 13610, loss 0.00409888, acc 1
2016-09-06T02:52:44.915572: step 13611, loss 0.0194756, acc 1
2016-09-06T02:52:45.723129: step 13612, loss 0.00241703, acc 1
2016-09-06T02:52:46.567262: step 13613, loss 0.0269846, acc 0.98
2016-09-06T02:52:47.378745: step 13614, loss 0.0166406, acc 1
2016-09-06T02:52:48.185113: step 13615, loss 0.00787368, acc 1
2016-09-06T02:52:49.022061: step 13616, loss 0.00296263, acc 1
2016-09-06T02:52:49.840573: step 13617, loss 0.0117637, acc 1
2016-09-06T02:52:50.666723: step 13618, loss 0.0171863, acc 1
2016-09-06T02:52:51.506670: step 13619, loss 0.00401323, acc 1
2016-09-06T02:52:52.314121: step 13620, loss 0.0130029, acc 1
2016-09-06T02:52:53.110133: step 13621, loss 0.00274353, acc 1
2016-09-06T02:52:53.938107: step 13622, loss 0.00615845, acc 1
2016-09-06T02:52:54.778508: step 13623, loss 0.00369112, acc 1
2016-09-06T02:52:55.552287: step 13624, loss 0.116946, acc 0.98
2016-09-06T02:52:56.356887: step 13625, loss 0.00948976, acc 1
2016-09-06T02:52:57.150530: step 13626, loss 0.0171103, acc 0.98
2016-09-06T02:52:57.946851: step 13627, loss 0.00481654, acc 1
2016-09-06T02:52:58.778551: step 13628, loss 0.00219516, acc 1
2016-09-06T02:52:59.587551: step 13629, loss 0.0133055, acc 1
2016-09-06T02:53:00.413435: step 13630, loss 0.00327301, acc 1
2016-09-06T02:53:01.210991: step 13631, loss 0.132248, acc 0.98
2016-09-06T02:53:01.978819: step 13632, loss 0.00519988, acc 1
2016-09-06T02:53:02.783302: step 13633, loss 0.00394229, acc 1
2016-09-06T02:53:03.596268: step 13634, loss 0.0261877, acc 0.98
2016-09-06T02:53:04.394500: step 13635, loss 0.0198597, acc 0.98
2016-09-06T02:53:05.214658: step 13636, loss 0.00466111, acc 1
2016-09-06T02:53:06.027666: step 13637, loss 0.00490517, acc 1
2016-09-06T02:53:06.847351: step 13638, loss 0.0234764, acc 0.98
2016-09-06T02:53:07.646033: step 13639, loss 0.00530554, acc 1
2016-09-06T02:53:08.433216: step 13640, loss 0.0185178, acc 1
2016-09-06T02:53:09.263691: step 13641, loss 0.00831233, acc 1
2016-09-06T02:53:10.056721: step 13642, loss 0.00339649, acc 1
2016-09-06T02:53:10.843507: step 13643, loss 0.00273047, acc 1
2016-09-06T02:53:11.669614: step 13644, loss 0.0189968, acc 1
2016-09-06T02:53:12.498766: step 13645, loss 0.00259768, acc 1
2016-09-06T02:53:13.313800: step 13646, loss 0.00262256, acc 1
2016-09-06T02:53:14.161644: step 13647, loss 0.00379619, acc 1
2016-09-06T02:53:14.980199: step 13648, loss 0.0304006, acc 0.98
2016-09-06T02:53:15.765526: step 13649, loss 0.00338575, acc 1
2016-09-06T02:53:16.567518: step 13650, loss 0.00326936, acc 1
2016-09-06T02:53:17.327416: step 13651, loss 0.00523503, acc 1
2016-09-06T02:53:18.155872: step 13652, loss 0.0164263, acc 1
2016-09-06T02:53:18.997121: step 13653, loss 0.00283538, acc 1
2016-09-06T02:53:19.789409: step 13654, loss 0.0107058, acc 1
2016-09-06T02:53:20.585402: step 13655, loss 0.00393392, acc 1
2016-09-06T02:53:21.400572: step 13656, loss 0.00323631, acc 1
2016-09-06T02:53:22.163872: step 13657, loss 0.0138819, acc 1
2016-09-06T02:53:22.976559: step 13658, loss 0.00948804, acc 1
2016-09-06T02:53:23.817633: step 13659, loss 0.0140762, acc 1
2016-09-06T02:53:24.602492: step 13660, loss 0.00325109, acc 1
2016-09-06T02:53:25.370032: step 13661, loss 0.00324262, acc 1
2016-09-06T02:53:26.166385: step 13662, loss 0.00269882, acc 1
2016-09-06T02:53:26.958464: step 13663, loss 0.00274687, acc 1
2016-09-06T02:53:27.792861: step 13664, loss 0.00267991, acc 1
2016-09-06T02:53:28.586349: step 13665, loss 0.00511261, acc 1
2016-09-06T02:53:29.360181: step 13666, loss 0.0216053, acc 1
2016-09-06T02:53:30.182250: step 13667, loss 0.00266553, acc 1
2016-09-06T02:53:30.995421: step 13668, loss 0.00411526, acc 1
2016-09-06T02:53:31.780206: step 13669, loss 0.00293022, acc 1
2016-09-06T02:53:32.602982: step 13670, loss 0.0321558, acc 0.98
2016-09-06T02:53:33.417778: step 13671, loss 0.0324113, acc 0.98
2016-09-06T02:53:34.213406: step 13672, loss 0.00261958, acc 1
2016-09-06T02:53:35.023965: step 13673, loss 0.0035714, acc 1
2016-09-06T02:53:35.815019: step 13674, loss 0.00377264, acc 1
2016-09-06T02:53:36.618705: step 13675, loss 0.0907704, acc 0.98
2016-09-06T02:53:37.462278: step 13676, loss 0.00246829, acc 1
2016-09-06T02:53:38.274888: step 13677, loss 0.00243632, acc 1
2016-09-06T02:53:39.083636: step 13678, loss 0.00241329, acc 1
2016-09-06T02:53:39.887617: step 13679, loss 0.0157445, acc 1
2016-09-06T02:53:40.683741: step 13680, loss 0.0371136, acc 0.98
2016-09-06T02:53:41.471017: step 13681, loss 0.017141, acc 0.98
2016-09-06T02:53:42.292025: step 13682, loss 0.00294196, acc 1
2016-09-06T02:53:43.108502: step 13683, loss 0.0107917, acc 1
2016-09-06T02:53:43.898269: step 13684, loss 0.0268368, acc 0.98
2016-09-06T02:53:44.699817: step 13685, loss 0.0529048, acc 0.96
2016-09-06T02:53:45.508047: step 13686, loss 0.00309283, acc 1
2016-09-06T02:53:46.302581: step 13687, loss 0.0128146, acc 1
2016-09-06T02:53:47.079762: step 13688, loss 0.0042527, acc 1
2016-09-06T02:53:47.886712: step 13689, loss 0.00493869, acc 1
2016-09-06T02:53:48.686809: step 13690, loss 0.0157043, acc 1
2016-09-06T02:53:49.522812: step 13691, loss 0.0051376, acc 1
2016-09-06T02:53:50.345212: step 13692, loss 0.00520252, acc 1
2016-09-06T02:53:51.127958: step 13693, loss 0.0297944, acc 0.98
2016-09-06T02:53:51.987422: step 13694, loss 0.00210679, acc 1
2016-09-06T02:53:52.821559: step 13695, loss 0.00201704, acc 1
2016-09-06T02:53:53.637010: step 13696, loss 0.00647873, acc 1
2016-09-06T02:53:54.458638: step 13697, loss 0.00204731, acc 1
2016-09-06T02:53:55.286066: step 13698, loss 0.0198448, acc 1
2016-09-06T02:53:56.083606: step 13699, loss 0.00224215, acc 1
2016-09-06T02:53:56.892343: step 13700, loss 0.0377891, acc 0.98

Evaluation:
2016-09-06T02:54:00.629920: step 13700, loss 2.87765, acc 0.721388

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-13700

2016-09-06T02:54:02.632668: step 13701, loss 0.00462492, acc 1
2016-09-06T02:54:03.478791: step 13702, loss 0.0152103, acc 1
2016-09-06T02:54:04.309907: step 13703, loss 0.0407502, acc 0.98
2016-09-06T02:54:05.145204: step 13704, loss 0.0021309, acc 1
2016-09-06T02:54:05.953222: step 13705, loss 0.045659, acc 0.96
2016-09-06T02:54:06.741937: step 13706, loss 0.0020907, acc 1
2016-09-06T02:54:07.552150: step 13707, loss 0.0155652, acc 1
2016-09-06T02:54:08.363048: step 13708, loss 0.020064, acc 0.98
2016-09-06T02:54:09.168482: step 13709, loss 0.00366289, acc 1
2016-09-06T02:54:10.005647: step 13710, loss 0.0333382, acc 0.98
2016-09-06T02:54:10.807436: step 13711, loss 0.0189928, acc 0.98
2016-09-06T02:54:11.612336: step 13712, loss 0.00201059, acc 1
2016-09-06T02:54:12.436799: step 13713, loss 0.00200766, acc 1
2016-09-06T02:54:13.235015: step 13714, loss 0.00202685, acc 1
2016-09-06T02:54:14.046430: step 13715, loss 0.00460234, acc 1
2016-09-06T02:54:14.889300: step 13716, loss 0.00354719, acc 1
2016-09-06T02:54:15.712048: step 13717, loss 0.0048354, acc 1
2016-09-06T02:54:16.518527: step 13718, loss 0.0163341, acc 1
2016-09-06T02:54:17.365285: step 13719, loss 0.00507342, acc 1
2016-09-06T02:54:18.196279: step 13720, loss 0.0513633, acc 0.98
2016-09-06T02:54:18.965212: step 13721, loss 0.00200107, acc 1
2016-09-06T02:54:19.774045: step 13722, loss 0.0022358, acc 1
2016-09-06T02:54:20.577833: step 13723, loss 0.0211783, acc 0.98
2016-09-06T02:54:21.367586: step 13724, loss 0.00934687, acc 1
2016-09-06T02:54:22.164427: step 13725, loss 0.00198776, acc 1
2016-09-06T02:54:22.980656: step 13726, loss 0.00193933, acc 1
2016-09-06T02:54:23.750874: step 13727, loss 0.0266416, acc 0.98
2016-09-06T02:54:24.566865: step 13728, loss 0.0144011, acc 1
2016-09-06T02:54:25.376477: step 13729, loss 0.00190711, acc 1
2016-09-06T02:54:26.175434: step 13730, loss 0.0104232, acc 1
2016-09-06T02:54:26.977116: step 13731, loss 0.0165016, acc 0.98
2016-09-06T02:54:27.784338: step 13732, loss 0.00375826, acc 1
2016-09-06T02:54:28.564834: step 13733, loss 0.00195067, acc 1
2016-09-06T02:54:29.399541: step 13734, loss 0.00189226, acc 1
2016-09-06T02:54:30.240293: step 13735, loss 0.00690841, acc 1
2016-09-06T02:54:31.043178: step 13736, loss 0.00199908, acc 1
2016-09-06T02:54:31.838524: step 13737, loss 0.0279776, acc 0.98
2016-09-06T02:54:32.664607: step 13738, loss 0.0120808, acc 1
2016-09-06T02:54:33.438545: step 13739, loss 0.066803, acc 0.98
2016-09-06T02:54:34.270711: step 13740, loss 0.00435704, acc 1
2016-09-06T02:54:35.095953: step 13741, loss 0.0422824, acc 0.98
2016-09-06T02:54:35.882487: step 13742, loss 0.00283698, acc 1
2016-09-06T02:54:36.666362: step 13743, loss 0.00175303, acc 1
2016-09-06T02:54:37.477451: step 13744, loss 0.0185218, acc 1
2016-09-06T02:54:38.278071: step 13745, loss 0.0205339, acc 0.98
2016-09-06T02:54:39.102810: step 13746, loss 0.0552005, acc 0.96
2016-09-06T02:54:39.903259: step 13747, loss 0.0169848, acc 1
2016-09-06T02:54:40.668202: step 13748, loss 0.0141519, acc 1
2016-09-06T02:54:41.481542: step 13749, loss 0.0286856, acc 0.98
2016-09-06T02:54:42.317707: step 13750, loss 0.00267354, acc 1
2016-09-06T02:54:43.093159: step 13751, loss 0.0502019, acc 0.98
2016-09-06T02:54:43.901749: step 13752, loss 0.00305309, acc 1
2016-09-06T02:54:44.709201: step 13753, loss 0.00202457, acc 1
2016-09-06T02:54:45.491517: step 13754, loss 0.0020372, acc 1
2016-09-06T02:54:46.308308: step 13755, loss 0.0230169, acc 0.98
2016-09-06T02:54:47.079405: step 13756, loss 0.0649791, acc 0.98
2016-09-06T02:54:47.905286: step 13757, loss 0.00946262, acc 1
2016-09-06T02:54:48.736129: step 13758, loss 0.00221986, acc 1
2016-09-06T02:54:49.552165: step 13759, loss 0.0157407, acc 1
2016-09-06T02:54:50.342717: step 13760, loss 0.0193349, acc 0.98
2016-09-06T02:54:51.180546: step 13761, loss 0.00370116, acc 1
2016-09-06T02:54:52.012491: step 13762, loss 0.012904, acc 1
2016-09-06T02:54:52.829437: step 13763, loss 0.00676518, acc 1
2016-09-06T02:54:53.653716: step 13764, loss 0.0140552, acc 1
2016-09-06T02:54:54.501488: step 13765, loss 0.0309813, acc 0.98
2016-09-06T02:54:55.356360: step 13766, loss 0.0147466, acc 1
2016-09-06T02:54:56.182359: step 13767, loss 0.0148252, acc 1
2016-09-06T02:54:57.007882: step 13768, loss 0.0163747, acc 0.98
2016-09-06T02:54:57.808844: step 13769, loss 0.0162976, acc 1
2016-09-06T02:54:58.621543: step 13770, loss 0.0190982, acc 0.98
2016-09-06T02:54:59.452228: step 13771, loss 0.0124022, acc 1
2016-09-06T02:55:00.287135: step 13772, loss 0.0048455, acc 1
2016-09-06T02:55:01.082693: step 13773, loss 0.0218457, acc 1
2016-09-06T02:55:01.928978: step 13774, loss 0.00595215, acc 1
2016-09-06T02:55:02.739994: step 13775, loss 0.0287022, acc 0.98
2016-09-06T02:55:03.553891: step 13776, loss 0.0144418, acc 1
2016-09-06T02:55:04.390747: step 13777, loss 0.005864, acc 1
2016-09-06T02:55:05.209453: step 13778, loss 0.0284097, acc 1
2016-09-06T02:55:06.004530: step 13779, loss 0.0174546, acc 0.98
2016-09-06T02:55:06.831859: step 13780, loss 0.00278102, acc 1
2016-09-06T02:55:07.641609: step 13781, loss 0.0147737, acc 1
2016-09-06T02:55:08.459256: step 13782, loss 0.02385, acc 1
2016-09-06T02:55:09.287786: step 13783, loss 0.0049315, acc 1
2016-09-06T02:55:10.087237: step 13784, loss 0.00236824, acc 1
2016-09-06T02:55:10.892351: step 13785, loss 0.118339, acc 0.98
2016-09-06T02:55:11.695688: step 13786, loss 0.00246912, acc 1
2016-09-06T02:55:12.496279: step 13787, loss 0.0102056, acc 1
2016-09-06T02:55:13.314420: step 13788, loss 0.00670719, acc 1
2016-09-06T02:55:14.126713: step 13789, loss 0.00740174, acc 1
2016-09-06T02:55:14.942891: step 13790, loss 0.00241655, acc 1
2016-09-06T02:55:15.740469: step 13791, loss 0.00245743, acc 1
2016-09-06T02:55:16.572219: step 13792, loss 0.019396, acc 1
2016-09-06T02:55:17.390358: step 13793, loss 0.0841447, acc 0.98
2016-09-06T02:55:18.181986: step 13794, loss 0.00309363, acc 1
2016-09-06T02:55:19.007241: step 13795, loss 0.00258374, acc 1
2016-09-06T02:55:19.815637: step 13796, loss 0.00275001, acc 1
2016-09-06T02:55:20.593335: step 13797, loss 0.00324272, acc 1
2016-09-06T02:55:21.383048: step 13798, loss 0.0111028, acc 1
2016-09-06T02:55:22.315060: step 13799, loss 0.0264635, acc 0.98
2016-09-06T02:55:23.113770: step 13800, loss 0.00230393, acc 1

Evaluation:
2016-09-06T02:55:26.839527: step 13800, loss 3.17186, acc 0.72045

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-13800

2016-09-06T02:55:28.739456: step 13801, loss 0.0808786, acc 0.96
2016-09-06T02:55:29.529205: step 13802, loss 0.00275209, acc 1
2016-09-06T02:55:30.304111: step 13803, loss 0.00243592, acc 1
2016-09-06T02:55:31.149991: step 13804, loss 0.00256078, acc 1
2016-09-06T02:55:31.957850: step 13805, loss 0.0028654, acc 1
2016-09-06T02:55:32.762932: step 13806, loss 0.010181, acc 1
2016-09-06T02:55:33.605048: step 13807, loss 0.0758266, acc 0.96
2016-09-06T02:55:34.414629: step 13808, loss 0.0037206, acc 1
2016-09-06T02:55:35.207786: step 13809, loss 0.00218342, acc 1
2016-09-06T02:55:36.065185: step 13810, loss 0.176057, acc 0.94
2016-09-06T02:55:36.895116: step 13811, loss 0.0152459, acc 1
2016-09-06T02:55:37.698115: step 13812, loss 0.00282196, acc 1
2016-09-06T02:55:38.506948: step 13813, loss 0.00181824, acc 1
2016-09-06T02:55:39.315862: step 13814, loss 0.00301739, acc 1
2016-09-06T02:55:40.119119: step 13815, loss 0.00178629, acc 1
2016-09-06T02:55:40.912012: step 13816, loss 0.00261756, acc 1
2016-09-06T02:55:41.731444: step 13817, loss 0.0173082, acc 1
2016-09-06T02:55:42.502434: step 13818, loss 0.0605424, acc 0.96
2016-09-06T02:55:43.332887: step 13819, loss 0.00527409, acc 1
2016-09-06T02:55:44.148289: step 13820, loss 0.00248571, acc 1
2016-09-06T02:55:44.926968: step 13821, loss 0.00831023, acc 1
2016-09-06T02:55:45.717274: step 13822, loss 0.00603619, acc 1
2016-09-06T02:55:46.529782: step 13823, loss 0.017441, acc 0.98
2016-09-06T02:55:47.264706: step 13824, loss 0.00237978, acc 1
2016-09-06T02:55:48.085103: step 13825, loss 0.00193327, acc 1
2016-09-06T02:55:48.887529: step 13826, loss 0.0111204, acc 1
2016-09-06T02:55:49.716028: step 13827, loss 0.00244807, acc 1
2016-09-06T02:55:50.526565: step 13828, loss 0.0285163, acc 0.98
2016-09-06T02:55:51.333558: step 13829, loss 0.0118974, acc 1
2016-09-06T02:55:52.111482: step 13830, loss 0.00541107, acc 1
2016-09-06T02:55:52.925091: step 13831, loss 0.0020224, acc 1
2016-09-06T02:55:53.715222: step 13832, loss 0.015381, acc 1
2016-09-06T02:55:54.511571: step 13833, loss 0.00280196, acc 1
2016-09-06T02:55:55.338757: step 13834, loss 0.00228302, acc 1
2016-09-06T02:55:56.143666: step 13835, loss 0.0236765, acc 1
2016-09-06T02:55:56.953416: step 13836, loss 0.00340378, acc 1
2016-09-06T02:55:57.766182: step 13837, loss 0.00228317, acc 1
2016-09-06T02:55:58.597614: step 13838, loss 0.00234777, acc 1
2016-09-06T02:55:59.393313: step 13839, loss 0.0021966, acc 1
2016-09-06T02:56:00.201855: step 13840, loss 0.00607193, acc 1
2016-09-06T02:56:01.029538: step 13841, loss 0.00477125, acc 1
2016-09-06T02:56:01.827303: step 13842, loss 0.0582832, acc 0.98
2016-09-06T02:56:02.627896: step 13843, loss 0.00202398, acc 1
2016-09-06T02:56:03.447005: step 13844, loss 0.0249391, acc 0.98
2016-09-06T02:56:04.262563: step 13845, loss 0.0193747, acc 0.98
2016-09-06T02:56:05.098957: step 13846, loss 0.00935092, acc 1
2016-09-06T02:56:05.952652: step 13847, loss 0.00768033, acc 1
2016-09-06T02:56:06.779110: step 13848, loss 0.031206, acc 0.98
2016-09-06T02:56:07.578823: step 13849, loss 0.026629, acc 0.98
2016-09-06T02:56:08.429093: step 13850, loss 0.017643, acc 1
2016-09-06T02:56:09.228571: step 13851, loss 0.00488202, acc 1
2016-09-06T02:56:10.033319: step 13852, loss 0.0585992, acc 0.96
2016-09-06T02:56:10.859786: step 13853, loss 0.00228048, acc 1
2016-09-06T02:56:11.675288: step 13854, loss 0.00193643, acc 1
2016-09-06T02:56:12.503097: step 13855, loss 0.0448689, acc 0.98
2016-09-06T02:56:13.344622: step 13856, loss 0.00194593, acc 1
2016-09-06T02:56:14.163317: step 13857, loss 0.00224747, acc 1
2016-09-06T02:56:14.970421: step 13858, loss 0.00642643, acc 1
2016-09-06T02:56:15.785195: step 13859, loss 0.00182962, acc 1
2016-09-06T02:56:16.586367: step 13860, loss 0.00837107, acc 1
2016-09-06T02:56:17.408971: step 13861, loss 0.00179541, acc 1
2016-09-06T02:56:18.237006: step 13862, loss 0.0277236, acc 1
2016-09-06T02:56:19.095697: step 13863, loss 0.00196813, acc 1
2016-09-06T02:56:19.904441: step 13864, loss 0.00185343, acc 1
2016-09-06T02:56:20.744605: step 13865, loss 0.0876753, acc 0.96
2016-09-06T02:56:21.582906: step 13866, loss 0.00429446, acc 1
2016-09-06T02:56:22.382756: step 13867, loss 0.00835834, acc 1
2016-09-06T02:56:23.229264: step 13868, loss 0.00461154, acc 1
2016-09-06T02:56:24.043019: step 13869, loss 0.00165567, acc 1
2016-09-06T02:56:24.811985: step 13870, loss 0.0135587, acc 1
2016-09-06T02:56:25.612810: step 13871, loss 0.00166752, acc 1
2016-09-06T02:56:26.455785: step 13872, loss 0.00850326, acc 1
2016-09-06T02:56:27.247232: step 13873, loss 0.0138891, acc 1
2016-09-06T02:56:28.003671: step 13874, loss 0.00175194, acc 1
2016-09-06T02:56:28.823576: step 13875, loss 0.0130134, acc 1
2016-09-06T02:56:29.601405: step 13876, loss 0.014132, acc 1
2016-09-06T02:56:30.419548: step 13877, loss 0.0253195, acc 1
2016-09-06T02:56:31.227829: step 13878, loss 0.0123962, acc 1
2016-09-06T02:56:32.030146: step 13879, loss 0.0172742, acc 0.98
2016-09-06T02:56:32.816691: step 13880, loss 0.00466196, acc 1
2016-09-06T02:56:33.643366: step 13881, loss 0.00294381, acc 1
2016-09-06T02:56:34.432614: step 13882, loss 0.0170081, acc 1
2016-09-06T02:56:35.242004: step 13883, loss 0.00173343, acc 1
2016-09-06T02:56:36.039259: step 13884, loss 0.012636, acc 1
2016-09-06T02:56:36.856014: step 13885, loss 0.0438544, acc 0.96
2016-09-06T02:56:37.680404: step 13886, loss 0.0253971, acc 0.98
2016-09-06T02:56:38.513401: step 13887, loss 0.00182274, acc 1
2016-09-06T02:56:39.329713: step 13888, loss 0.0227328, acc 1
2016-09-06T02:56:40.116432: step 13889, loss 0.0116872, acc 1
2016-09-06T02:56:40.906253: step 13890, loss 0.00936896, acc 1
2016-09-06T02:56:41.701706: step 13891, loss 0.00724735, acc 1
2016-09-06T02:56:42.545482: step 13892, loss 0.125735, acc 0.98
2016-09-06T02:56:43.373375: step 13893, loss 0.0256605, acc 0.98
2016-09-06T02:56:44.153694: step 13894, loss 0.00864671, acc 1
2016-09-06T02:56:44.979559: step 13895, loss 0.00178128, acc 1
2016-09-06T02:56:45.787366: step 13896, loss 0.00246455, acc 1
2016-09-06T02:56:46.570995: step 13897, loss 0.02769, acc 0.98
2016-09-06T02:56:47.377431: step 13898, loss 0.00634974, acc 1
2016-09-06T02:56:48.209951: step 13899, loss 0.0310651, acc 1
2016-09-06T02:56:49.006454: step 13900, loss 0.0132869, acc 1

Evaluation:
2016-09-06T02:56:52.734289: step 13900, loss 2.21026, acc 0.711069

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-13900

2016-09-06T02:56:54.583378: step 13901, loss 0.0370453, acc 0.98
2016-09-06T02:56:55.373149: step 13902, loss 0.0438172, acc 0.96
2016-09-06T02:56:56.181270: step 13903, loss 0.00436875, acc 1
2016-09-06T02:56:56.994384: step 13904, loss 0.00588322, acc 1
2016-09-06T02:56:57.812669: step 13905, loss 0.00326236, acc 1
2016-09-06T02:56:58.624690: step 13906, loss 0.00417016, acc 1
2016-09-06T02:56:59.443718: step 13907, loss 0.00336644, acc 1
2016-09-06T02:57:00.263104: step 13908, loss 0.0127227, acc 1
2016-09-06T02:57:01.069912: step 13909, loss 0.00258879, acc 1
2016-09-06T02:57:01.877651: step 13910, loss 0.0211071, acc 1
2016-09-06T02:57:02.682375: step 13911, loss 0.00358364, acc 1
2016-09-06T02:57:03.479391: step 13912, loss 0.00567046, acc 1
2016-09-06T02:57:04.279986: step 13913, loss 0.00406796, acc 1
2016-09-06T02:57:05.095390: step 13914, loss 0.00241616, acc 1
2016-09-06T02:57:05.863723: step 13915, loss 0.0349824, acc 0.98
2016-09-06T02:57:06.730693: step 13916, loss 0.00197859, acc 1
2016-09-06T02:57:07.535403: step 13917, loss 0.00204345, acc 1
2016-09-06T02:57:08.345406: step 13918, loss 0.00228002, acc 1
2016-09-06T02:57:09.147296: step 13919, loss 0.0021087, acc 1
2016-09-06T02:57:09.994879: step 13920, loss 0.0244423, acc 0.98
2016-09-06T02:57:10.783124: step 13921, loss 0.00206791, acc 1
2016-09-06T02:57:11.555903: step 13922, loss 0.0256344, acc 0.98
2016-09-06T02:57:12.378220: step 13923, loss 0.027538, acc 1
2016-09-06T02:57:13.156261: step 13924, loss 0.00949325, acc 1
2016-09-06T02:57:13.969157: step 13925, loss 0.0200642, acc 0.98
2016-09-06T02:57:14.781399: step 13926, loss 0.00210756, acc 1
2016-09-06T02:57:15.585135: step 13927, loss 0.0107978, acc 1
2016-09-06T02:57:16.401850: step 13928, loss 0.00213741, acc 1
2016-09-06T02:57:17.187536: step 13929, loss 0.0170246, acc 0.98
2016-09-06T02:57:17.986292: step 13930, loss 0.00212725, acc 1
2016-09-06T02:57:18.780951: step 13931, loss 0.00783519, acc 1
2016-09-06T02:57:19.607807: step 13932, loss 0.0281185, acc 0.98
2016-09-06T02:57:20.382712: step 13933, loss 0.00264341, acc 1
2016-09-06T02:57:21.198658: step 13934, loss 0.0236433, acc 1
2016-09-06T02:57:22.017298: step 13935, loss 0.00293181, acc 1
2016-09-06T02:57:22.856977: step 13936, loss 0.0203169, acc 0.98
2016-09-06T02:57:23.665658: step 13937, loss 0.0638213, acc 0.96
2016-09-06T02:57:24.478650: step 13938, loss 0.00235265, acc 1
2016-09-06T02:57:25.255568: step 13939, loss 0.00370203, acc 1
2016-09-06T02:57:26.046185: step 13940, loss 0.0142539, acc 1
2016-09-06T02:57:26.864906: step 13941, loss 0.0221343, acc 0.98
2016-09-06T02:57:27.654448: step 13942, loss 0.00227993, acc 1
2016-09-06T02:57:28.447955: step 13943, loss 0.0146447, acc 1
2016-09-06T02:57:29.289406: step 13944, loss 0.00245283, acc 1
2016-09-06T02:57:30.054446: step 13945, loss 0.00245611, acc 1
2016-09-06T02:57:30.859673: step 13946, loss 0.0268581, acc 0.98
2016-09-06T02:57:31.680238: step 13947, loss 0.0022733, acc 1
2016-09-06T02:57:32.475030: step 13948, loss 0.00707758, acc 1
2016-09-06T02:57:33.299606: step 13949, loss 0.0311809, acc 0.98
2016-09-06T02:57:34.125866: step 13950, loss 0.0019819, acc 1
2016-09-06T02:57:34.895810: step 13951, loss 0.00276722, acc 1
2016-09-06T02:57:35.692463: step 13952, loss 0.0257876, acc 0.98
2016-09-06T02:57:36.513204: step 13953, loss 0.00809493, acc 1
2016-09-06T02:57:37.319399: step 13954, loss 0.0466631, acc 0.98
2016-09-06T02:57:38.145870: step 13955, loss 0.0108763, acc 1
2016-09-06T02:57:38.961542: step 13956, loss 0.0464934, acc 0.98
2016-09-06T02:57:39.735487: step 13957, loss 0.0426286, acc 0.98
2016-09-06T02:57:40.525596: step 13958, loss 0.00636172, acc 1
2016-09-06T02:57:41.332813: step 13959, loss 0.0241076, acc 1
2016-09-06T02:57:42.116428: step 13960, loss 0.0190908, acc 0.98
2016-09-06T02:57:42.908624: step 13961, loss 0.00241634, acc 1
2016-09-06T02:57:43.714894: step 13962, loss 0.084968, acc 0.98
2016-09-06T02:57:44.531822: step 13963, loss 0.00312589, acc 1
2016-09-06T02:57:45.352758: step 13964, loss 0.00384059, acc 1
2016-09-06T02:57:46.165453: step 13965, loss 0.042618, acc 0.98
2016-09-06T02:57:46.978325: step 13966, loss 0.00641867, acc 1
2016-09-06T02:57:47.797196: step 13967, loss 0.00259091, acc 1
2016-09-06T02:57:48.630620: step 13968, loss 0.00471292, acc 1
2016-09-06T02:57:49.410501: step 13969, loss 0.0165437, acc 1
2016-09-06T02:57:50.207736: step 13970, loss 0.0157829, acc 0.98
2016-09-06T02:57:51.036256: step 13971, loss 0.0212751, acc 1
2016-09-06T02:57:51.822600: step 13972, loss 0.0296859, acc 0.98
2016-09-06T02:57:52.638210: step 13973, loss 0.0041505, acc 1
2016-09-06T02:57:53.506424: step 13974, loss 0.00218061, acc 1
2016-09-06T02:57:54.314825: step 13975, loss 0.00301452, acc 1
2016-09-06T02:57:55.109896: step 13976, loss 0.00224074, acc 1
2016-09-06T02:57:55.946053: step 13977, loss 0.00754251, acc 1
2016-09-06T02:57:56.764917: step 13978, loss 0.00187017, acc 1
2016-09-06T02:57:57.589215: step 13979, loss 0.00757353, acc 1
2016-09-06T02:57:58.410490: step 13980, loss 0.0335747, acc 0.98
2016-09-06T02:57:59.219270: step 13981, loss 0.0107036, acc 1
2016-09-06T02:58:00.025982: step 13982, loss 0.00886873, acc 1
2016-09-06T02:58:00.868921: step 13983, loss 0.00208835, acc 1
2016-09-06T02:58:01.675618: step 13984, loss 0.0949698, acc 0.96
2016-09-06T02:58:02.496115: step 13985, loss 0.0579475, acc 0.98
2016-09-06T02:58:03.376437: step 13986, loss 0.00436277, acc 1
2016-09-06T02:58:04.195295: step 13987, loss 0.0407085, acc 0.98
2016-09-06T02:58:05.005977: step 13988, loss 0.00468125, acc 1
2016-09-06T02:58:05.841308: step 13989, loss 0.00167649, acc 1
2016-09-06T02:58:06.654806: step 13990, loss 0.0164069, acc 0.98
2016-09-06T02:58:07.498868: step 13991, loss 0.00591826, acc 1
2016-09-06T02:58:08.322118: step 13992, loss 0.00224285, acc 1
2016-09-06T02:58:09.128264: step 13993, loss 0.0374329, acc 0.98
2016-09-06T02:58:09.937572: step 13994, loss 0.0179973, acc 0.98
2016-09-06T02:58:10.764025: step 13995, loss 0.00662584, acc 1
2016-09-06T02:58:11.582203: step 13996, loss 0.0274918, acc 0.98
2016-09-06T02:58:12.365241: step 13997, loss 0.0118882, acc 1
2016-09-06T02:58:13.171588: step 13998, loss 0.0015986, acc 1
2016-09-06T02:58:13.986772: step 13999, loss 0.0429832, acc 0.98
2016-09-06T02:58:14.791771: step 14000, loss 0.00225262, acc 1

Evaluation:
2016-09-06T02:58:18.519454: step 14000, loss 2.45934, acc 0.713884

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-14000

2016-09-06T02:58:20.507530: step 14001, loss 0.00866573, acc 1
2016-09-06T02:58:21.347595: step 14002, loss 0.0355424, acc 0.98
2016-09-06T02:58:22.171227: step 14003, loss 0.00520543, acc 1
2016-09-06T02:58:22.973607: step 14004, loss 0.0170362, acc 1
2016-09-06T02:58:23.803993: step 14005, loss 0.0074305, acc 1
2016-09-06T02:58:24.584509: step 14006, loss 0.00160691, acc 1
2016-09-06T02:58:25.400138: step 14007, loss 0.00161421, acc 1
2016-09-06T02:58:26.247432: step 14008, loss 0.00177875, acc 1
2016-09-06T02:58:27.040864: step 14009, loss 0.0195971, acc 0.98
2016-09-06T02:58:27.853142: step 14010, loss 0.0158232, acc 0.98
2016-09-06T02:58:28.713379: step 14011, loss 0.0463318, acc 0.98
2016-09-06T02:58:29.530681: step 14012, loss 0.00259039, acc 1
2016-09-06T02:58:30.330425: step 14013, loss 0.00296522, acc 1
2016-09-06T02:58:31.169941: step 14014, loss 0.0373951, acc 0.98
2016-09-06T02:58:31.975540: step 14015, loss 0.0668202, acc 0.98
2016-09-06T02:58:32.722693: step 14016, loss 0.00457744, acc 1
2016-09-06T02:58:33.552979: step 14017, loss 0.00149762, acc 1
2016-09-06T02:58:34.353703: step 14018, loss 0.0335639, acc 0.98
2016-09-06T02:58:35.198780: step 14019, loss 0.00143385, acc 1
2016-09-06T02:58:36.017445: step 14020, loss 0.0378269, acc 0.96
2016-09-06T02:58:36.832252: step 14021, loss 0.00158232, acc 1
2016-09-06T02:58:37.673839: step 14022, loss 0.014685, acc 1
2016-09-06T02:58:38.523743: step 14023, loss 0.00816231, acc 1
2016-09-06T02:58:39.329360: step 14024, loss 0.0406184, acc 0.98
2016-09-06T02:58:40.125699: step 14025, loss 0.0091509, acc 1
2016-09-06T02:58:40.958938: step 14026, loss 0.00140049, acc 1
2016-09-06T02:58:41.756969: step 14027, loss 0.00544807, acc 1
2016-09-06T02:58:42.580675: step 14028, loss 0.0326885, acc 0.98
2016-09-06T02:58:43.372877: step 14029, loss 0.00142886, acc 1
2016-09-06T02:58:44.191749: step 14030, loss 0.00218478, acc 1
2016-09-06T02:58:44.948611: step 14031, loss 0.0249371, acc 0.98
2016-09-06T02:58:45.775144: step 14032, loss 0.0463032, acc 0.98
2016-09-06T02:58:46.572866: step 14033, loss 0.0030012, acc 1
2016-09-06T02:58:47.368827: step 14034, loss 0.00466583, acc 1
2016-09-06T02:58:48.181631: step 14035, loss 0.0193609, acc 1
2016-09-06T02:58:49.004104: step 14036, loss 0.018716, acc 1
2016-09-06T02:58:49.795580: step 14037, loss 0.00553806, acc 1
2016-09-06T02:58:50.616900: step 14038, loss 0.018352, acc 1
2016-09-06T02:58:51.440812: step 14039, loss 0.00142583, acc 1
2016-09-06T02:58:52.214118: step 14040, loss 0.0050758, acc 1
2016-09-06T02:58:53.018943: step 14041, loss 0.00175089, acc 1
2016-09-06T02:58:53.809796: step 14042, loss 0.0149316, acc 1
2016-09-06T02:58:54.608470: step 14043, loss 0.0132217, acc 1
2016-09-06T02:58:55.440497: step 14044, loss 0.00636348, acc 1
2016-09-06T02:58:56.236771: step 14045, loss 0.0202757, acc 0.98
2016-09-06T02:58:57.032732: step 14046, loss 0.0198506, acc 0.98
2016-09-06T02:58:57.860922: step 14047, loss 0.00156527, acc 1
2016-09-06T02:58:58.673471: step 14048, loss 0.00154588, acc 1
2016-09-06T02:58:59.482389: step 14049, loss 0.00155015, acc 1
2016-09-06T02:59:00.297875: step 14050, loss 0.0173876, acc 0.98
2016-09-06T02:59:01.113046: step 14051, loss 0.00246832, acc 1
2016-09-06T02:59:01.897719: step 14052, loss 0.0066534, acc 1
2016-09-06T02:59:02.697510: step 14053, loss 0.00483071, acc 1
2016-09-06T02:59:03.508986: step 14054, loss 0.0075764, acc 1
2016-09-06T02:59:04.301405: step 14055, loss 0.0181713, acc 0.98
2016-09-06T02:59:05.104087: step 14056, loss 0.00164534, acc 1
2016-09-06T02:59:05.915299: step 14057, loss 0.0494202, acc 0.98
2016-09-06T02:59:06.743446: step 14058, loss 0.00336093, acc 1
2016-09-06T02:59:07.530754: step 14059, loss 0.0205476, acc 0.98
2016-09-06T02:59:08.343566: step 14060, loss 0.00161581, acc 1
2016-09-06T02:59:09.091196: step 14061, loss 0.00156112, acc 1
2016-09-06T02:59:09.928991: step 14062, loss 0.0752569, acc 0.98
2016-09-06T02:59:10.776842: step 14063, loss 0.0219908, acc 0.98
2016-09-06T02:59:11.586295: step 14064, loss 0.0123031, acc 1
2016-09-06T02:59:12.421427: step 14065, loss 0.00147555, acc 1
2016-09-06T02:59:13.217404: step 14066, loss 0.00144148, acc 1
2016-09-06T02:59:14.012724: step 14067, loss 0.016543, acc 0.98
2016-09-06T02:59:14.860579: step 14068, loss 0.0222131, acc 0.98
2016-09-06T02:59:15.675425: step 14069, loss 0.0153362, acc 1
2016-09-06T02:59:16.484119: step 14070, loss 0.00261338, acc 1
2016-09-06T02:59:17.289770: step 14071, loss 0.0373537, acc 0.98
2016-09-06T02:59:18.100121: step 14072, loss 0.00164721, acc 1
2016-09-06T02:59:18.900304: step 14073, loss 0.0281873, acc 0.98
2016-09-06T02:59:19.699496: step 14074, loss 0.0182228, acc 1
2016-09-06T02:59:20.517511: step 14075, loss 0.0422167, acc 0.98
2016-09-06T02:59:21.311061: step 14076, loss 0.00137601, acc 1
2016-09-06T02:59:22.110225: step 14077, loss 0.0289632, acc 0.98
2016-09-06T02:59:22.917389: step 14078, loss 0.00267677, acc 1
2016-09-06T02:59:23.703263: step 14079, loss 0.00127432, acc 1
2016-09-06T02:59:24.483624: step 14080, loss 0.0283557, acc 1
2016-09-06T02:59:25.314827: step 14081, loss 0.0118265, acc 1
2016-09-06T02:59:26.070195: step 14082, loss 0.0066833, acc 1
2016-09-06T02:59:26.877575: step 14083, loss 0.0185981, acc 1
2016-09-06T02:59:27.687397: step 14084, loss 0.00864607, acc 1
2016-09-06T02:59:28.481900: step 14085, loss 0.0282744, acc 0.98
2016-09-06T02:59:29.308922: step 14086, loss 0.00173504, acc 1
2016-09-06T02:59:30.128063: step 14087, loss 0.0113627, acc 1
2016-09-06T02:59:30.923632: step 14088, loss 0.00130913, acc 1
2016-09-06T02:59:31.739357: step 14089, loss 0.00288322, acc 1
2016-09-06T02:59:32.539888: step 14090, loss 0.00155373, acc 1
2016-09-06T02:59:33.335285: step 14091, loss 0.0716526, acc 0.96
2016-09-06T02:59:34.126311: step 14092, loss 0.00978232, acc 1
2016-09-06T02:59:34.928609: step 14093, loss 0.00262527, acc 1
2016-09-06T02:59:35.717487: step 14094, loss 0.0196655, acc 1
2016-09-06T02:59:36.515467: step 14095, loss 0.0181773, acc 0.98
2016-09-06T02:59:37.331200: step 14096, loss 0.0506396, acc 0.98
2016-09-06T02:59:38.112211: step 14097, loss 0.00132546, acc 1
2016-09-06T02:59:38.904593: step 14098, loss 0.0192287, acc 0.98
2016-09-06T02:59:39.709490: step 14099, loss 0.0410393, acc 0.96
2016-09-06T02:59:40.544273: step 14100, loss 0.00131529, acc 1

Evaluation:
2016-09-06T02:59:44.338409: step 14100, loss 2.21527, acc 0.708255

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-14100

2016-09-06T02:59:46.214476: step 14101, loss 0.0359699, acc 0.98
2016-09-06T02:59:47.036614: step 14102, loss 0.00165304, acc 1
2016-09-06T02:59:47.836108: step 14103, loss 0.00161432, acc 1
2016-09-06T02:59:48.659928: step 14104, loss 0.00277311, acc 1
2016-09-06T02:59:49.484470: step 14105, loss 0.00370896, acc 1
2016-09-06T02:59:50.286603: step 14106, loss 0.00277742, acc 1
2016-09-06T02:59:51.084615: step 14107, loss 0.0140874, acc 1
2016-09-06T02:59:51.896860: step 14108, loss 0.00143269, acc 1
2016-09-06T02:59:52.724921: step 14109, loss 0.020674, acc 0.98
2016-09-06T02:59:53.536769: step 14110, loss 0.00144493, acc 1
2016-09-06T02:59:54.350997: step 14111, loss 0.00168926, acc 1
2016-09-06T02:59:55.129788: step 14112, loss 0.00152265, acc 1
2016-09-06T02:59:55.936901: step 14113, loss 0.00288449, acc 1
2016-09-06T02:59:56.786449: step 14114, loss 0.0522904, acc 0.98
2016-09-06T02:59:57.571365: step 14115, loss 0.0180101, acc 0.98
2016-09-06T02:59:58.354365: step 14116, loss 0.0034079, acc 1
2016-09-06T02:59:59.175657: step 14117, loss 0.012838, acc 1
2016-09-06T02:59:59.959029: step 14118, loss 0.0031725, acc 1
2016-09-06T03:00:00.802355: step 14119, loss 0.00813158, acc 1
2016-09-06T03:00:01.622070: step 14120, loss 0.00171301, acc 1
2016-09-06T03:00:02.419436: step 14121, loss 0.0136102, acc 1
2016-09-06T03:00:03.212007: step 14122, loss 0.00488804, acc 1
2016-09-06T03:00:04.015903: step 14123, loss 0.0342741, acc 0.98
2016-09-06T03:00:04.786706: step 14124, loss 0.0154197, acc 1
2016-09-06T03:00:05.572734: step 14125, loss 0.0249585, acc 1
2016-09-06T03:00:06.394506: step 14126, loss 0.103824, acc 0.96
2016-09-06T03:00:07.185657: step 14127, loss 0.00164019, acc 1
2016-09-06T03:00:08.030482: step 14128, loss 0.0290885, acc 0.98
2016-09-06T03:00:08.855483: step 14129, loss 0.0105613, acc 1
2016-09-06T03:00:09.634195: step 14130, loss 0.00374111, acc 1
2016-09-06T03:00:10.421977: step 14131, loss 0.0317751, acc 0.98
2016-09-06T03:00:11.229657: step 14132, loss 0.00187941, acc 1
2016-09-06T03:00:12.016955: step 14133, loss 0.0239257, acc 1
2016-09-06T03:00:12.843695: step 14134, loss 0.00153562, acc 1
2016-09-06T03:00:13.681722: step 14135, loss 0.0207191, acc 0.98
2016-09-06T03:00:14.447256: step 14136, loss 0.00674261, acc 1
2016-09-06T03:00:15.276697: step 14137, loss 0.00213189, acc 1
2016-09-06T03:00:16.098770: step 14138, loss 0.00177452, acc 1
2016-09-06T03:00:16.865366: step 14139, loss 0.00619036, acc 1
2016-09-06T03:00:17.700133: step 14140, loss 0.00528174, acc 1
2016-09-06T03:00:18.522843: step 14141, loss 0.038044, acc 0.98
2016-09-06T03:00:19.325498: step 14142, loss 0.00554104, acc 1
2016-09-06T03:00:20.130637: step 14143, loss 0.036262, acc 0.96
2016-09-06T03:00:20.926449: step 14144, loss 0.0163796, acc 1
2016-09-06T03:00:21.743226: step 14145, loss 0.0324471, acc 0.98
2016-09-06T03:00:22.542112: step 14146, loss 0.0245494, acc 1
2016-09-06T03:00:23.377587: step 14147, loss 0.0224844, acc 0.98
2016-09-06T03:00:24.171936: step 14148, loss 0.0161952, acc 1
2016-09-06T03:00:24.975027: step 14149, loss 0.0105613, acc 1
2016-09-06T03:00:25.814891: step 14150, loss 0.0141267, acc 1
2016-09-06T03:00:26.617220: step 14151, loss 0.0162944, acc 1
2016-09-06T03:00:27.409225: step 14152, loss 0.00194063, acc 1
2016-09-06T03:00:28.237287: step 14153, loss 0.0194776, acc 0.98
2016-09-06T03:00:29.064952: step 14154, loss 0.00988796, acc 1
2016-09-06T03:00:29.847607: step 14155, loss 0.00187863, acc 1
2016-09-06T03:00:30.680281: step 14156, loss 0.00193188, acc 1
2016-09-06T03:00:31.520153: step 14157, loss 0.0179325, acc 1
2016-09-06T03:00:32.348471: step 14158, loss 0.0155591, acc 1
2016-09-06T03:00:33.173546: step 14159, loss 0.00345759, acc 1
2016-09-06T03:00:33.981157: step 14160, loss 0.0172396, acc 0.98
2016-09-06T03:00:34.797793: step 14161, loss 0.0139449, acc 1
2016-09-06T03:00:35.657084: step 14162, loss 0.00681841, acc 1
2016-09-06T03:00:36.469710: step 14163, loss 0.0137924, acc 1
2016-09-06T03:00:37.290076: step 14164, loss 0.0046678, acc 1
2016-09-06T03:00:38.102716: step 14165, loss 0.00765288, acc 1
2016-09-06T03:00:38.913627: step 14166, loss 0.0050501, acc 1
2016-09-06T03:00:39.715659: step 14167, loss 0.00198213, acc 1
2016-09-06T03:00:40.563344: step 14168, loss 0.00201746, acc 1
2016-09-06T03:00:41.383538: step 14169, loss 0.0051597, acc 1
2016-09-06T03:00:42.205647: step 14170, loss 0.00195641, acc 1
2016-09-06T03:00:43.015004: step 14171, loss 0.00195045, acc 1
2016-09-06T03:00:43.847499: step 14172, loss 0.0311582, acc 0.98
2016-09-06T03:00:44.642835: step 14173, loss 0.0472388, acc 0.98
2016-09-06T03:00:45.489161: step 14174, loss 0.0244705, acc 0.98
2016-09-06T03:00:46.308753: step 14175, loss 0.0019617, acc 1
2016-09-06T03:00:47.071617: step 14176, loss 0.0234306, acc 0.98
2016-09-06T03:00:47.880919: step 14177, loss 0.0277512, acc 1
2016-09-06T03:00:48.700691: step 14178, loss 0.0159642, acc 0.98
2016-09-06T03:00:49.498307: step 14179, loss 0.0167935, acc 1
2016-09-06T03:00:50.298245: step 14180, loss 0.0128332, acc 1
2016-09-06T03:00:51.094809: step 14181, loss 0.0078143, acc 1
2016-09-06T03:00:51.899472: step 14182, loss 0.0160422, acc 0.98
2016-09-06T03:00:52.719673: step 14183, loss 0.0350136, acc 0.96
2016-09-06T03:00:53.537269: step 14184, loss 0.014213, acc 1
2016-09-06T03:00:54.318329: step 14185, loss 0.0292237, acc 0.98
2016-09-06T03:00:55.139786: step 14186, loss 0.00171768, acc 1
2016-09-06T03:00:55.952902: step 14187, loss 0.0106286, acc 1
2016-09-06T03:00:56.758203: step 14188, loss 0.00281115, acc 1
2016-09-06T03:00:57.566584: step 14189, loss 0.00428241, acc 1
2016-09-06T03:00:58.401854: step 14190, loss 0.0212742, acc 0.98
2016-09-06T03:00:59.188145: step 14191, loss 0.0165351, acc 0.98
2016-09-06T03:00:59.987898: step 14192, loss 0.00215941, acc 1
2016-09-06T03:01:00.856969: step 14193, loss 0.00177234, acc 1
2016-09-06T03:01:01.645251: step 14194, loss 0.0490884, acc 0.98
2016-09-06T03:01:02.432108: step 14195, loss 0.0983189, acc 0.98
2016-09-06T03:01:03.252291: step 14196, loss 0.0955842, acc 0.98
2016-09-06T03:01:04.045412: step 14197, loss 0.00173015, acc 1
2016-09-06T03:01:04.828377: step 14198, loss 0.025844, acc 0.98
2016-09-06T03:01:05.653441: step 14199, loss 0.00150284, acc 1
2016-09-06T03:01:06.447330: step 14200, loss 0.00130129, acc 1

Evaluation:
2016-09-06T03:01:10.201661: step 14200, loss 2.56652, acc 0.707317

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-14200

2016-09-06T03:01:12.153809: step 14201, loss 0.00358171, acc 1
2016-09-06T03:01:12.955926: step 14202, loss 0.0106644, acc 1
2016-09-06T03:01:13.750465: step 14203, loss 0.0201393, acc 1
2016-09-06T03:01:14.568680: step 14204, loss 0.00130312, acc 1
2016-09-06T03:01:15.413908: step 14205, loss 0.0303296, acc 1
2016-09-06T03:01:16.197914: step 14206, loss 0.00139687, acc 1
2016-09-06T03:01:17.016578: step 14207, loss 0.00310298, acc 1
2016-09-06T03:01:17.791494: step 14208, loss 0.00554658, acc 1
2016-09-06T03:01:18.646254: step 14209, loss 0.0017669, acc 1
2016-09-06T03:01:19.457604: step 14210, loss 0.0266851, acc 1
2016-09-06T03:01:20.266012: step 14211, loss 0.00139483, acc 1
2016-09-06T03:01:21.060773: step 14212, loss 0.0258824, acc 0.98
2016-09-06T03:01:21.860973: step 14213, loss 0.00694377, acc 1
2016-09-06T03:01:22.673179: step 14214, loss 0.00771846, acc 1
2016-09-06T03:01:23.470387: step 14215, loss 0.00165218, acc 1
2016-09-06T03:01:24.283770: step 14216, loss 0.00288129, acc 1
2016-09-06T03:01:25.101409: step 14217, loss 0.0682126, acc 0.98
2016-09-06T03:01:25.899119: step 14218, loss 0.00158779, acc 1
2016-09-06T03:01:26.693873: step 14219, loss 0.0297918, acc 0.98
2016-09-06T03:01:27.505066: step 14220, loss 0.0738206, acc 0.98
2016-09-06T03:01:28.304549: step 14221, loss 0.00157887, acc 1
2016-09-06T03:01:29.132802: step 14222, loss 0.00159629, acc 1
2016-09-06T03:01:29.955521: step 14223, loss 0.0017049, acc 1
2016-09-06T03:01:30.768427: step 14224, loss 0.0291344, acc 0.98
2016-09-06T03:01:31.553810: step 14225, loss 0.0191377, acc 0.98
2016-09-06T03:01:32.372747: step 14226, loss 0.00264828, acc 1
2016-09-06T03:01:33.144080: step 14227, loss 0.00220471, acc 1
2016-09-06T03:01:33.948997: step 14228, loss 0.0214123, acc 0.98
2016-09-06T03:01:34.753430: step 14229, loss 0.0025095, acc 1
2016-09-06T03:01:35.538454: step 14230, loss 0.0245693, acc 1
2016-09-06T03:01:36.336056: step 14231, loss 0.0431052, acc 0.98
2016-09-06T03:01:37.154823: step 14232, loss 0.0209886, acc 0.98
2016-09-06T03:01:37.928708: step 14233, loss 0.00602217, acc 1
2016-09-06T03:01:38.756764: step 14234, loss 0.0162098, acc 0.98
2016-09-06T03:01:39.573058: step 14235, loss 0.021125, acc 0.98
2016-09-06T03:01:40.389622: step 14236, loss 0.0154943, acc 1
2016-09-06T03:01:41.193282: step 14237, loss 0.0181112, acc 0.98
2016-09-06T03:01:42.017451: step 14238, loss 0.00280839, acc 1
2016-09-06T03:01:42.802004: step 14239, loss 0.00183066, acc 1
2016-09-06T03:01:43.599289: step 14240, loss 0.00143796, acc 1
2016-09-06T03:01:44.430213: step 14241, loss 0.0015917, acc 1
2016-09-06T03:01:45.215343: step 14242, loss 0.0221241, acc 1
2016-09-06T03:01:46.033271: step 14243, loss 0.00534974, acc 1
2016-09-06T03:01:46.864336: step 14244, loss 0.00460352, acc 1
2016-09-06T03:01:47.654160: step 14245, loss 0.00167437, acc 1
2016-09-06T03:01:48.446615: step 14246, loss 0.0469907, acc 0.96
2016-09-06T03:01:49.265438: step 14247, loss 0.0324094, acc 0.98
2016-09-06T03:01:50.080257: step 14248, loss 0.00376381, acc 1
2016-09-06T03:01:50.900707: step 14249, loss 0.0030482, acc 1
2016-09-06T03:01:51.765369: step 14250, loss 0.0017782, acc 1
2016-09-06T03:01:52.587058: step 14251, loss 0.0185691, acc 1
2016-09-06T03:01:53.408826: step 14252, loss 0.00466253, acc 1
2016-09-06T03:01:54.239185: step 14253, loss 0.00175527, acc 1
2016-09-06T03:01:55.075136: step 14254, loss 0.00991584, acc 1
2016-09-06T03:01:55.904510: step 14255, loss 0.0193604, acc 0.98
2016-09-06T03:01:56.728442: step 14256, loss 0.00329607, acc 1
2016-09-06T03:01:57.529090: step 14257, loss 0.00291437, acc 1
2016-09-06T03:01:58.342885: step 14258, loss 0.00178671, acc 1
2016-09-06T03:01:59.170547: step 14259, loss 0.0063752, acc 1
2016-09-06T03:01:59.991023: step 14260, loss 0.0175727, acc 0.98
2016-09-06T03:02:00.850410: step 14261, loss 0.0358975, acc 1
2016-09-06T03:02:01.670951: step 14262, loss 0.02569, acc 0.98
2016-09-06T03:02:02.472528: step 14263, loss 0.0770263, acc 0.98
2016-09-06T03:02:03.286445: step 14264, loss 0.00251776, acc 1
2016-09-06T03:02:04.107515: step 14265, loss 0.00332626, acc 1
2016-09-06T03:02:04.913404: step 14266, loss 0.0358363, acc 0.98
2016-09-06T03:02:05.713448: step 14267, loss 0.0109352, acc 1
2016-09-06T03:02:06.521293: step 14268, loss 0.00187783, acc 1
2016-09-06T03:02:07.356498: step 14269, loss 0.0404275, acc 0.98
2016-09-06T03:02:08.187848: step 14270, loss 0.0667197, acc 0.98
2016-09-06T03:02:09.006718: step 14271, loss 0.00176229, acc 1
2016-09-06T03:02:09.833567: step 14272, loss 0.00642327, acc 1
2016-09-06T03:02:10.612811: step 14273, loss 0.0017213, acc 1
2016-09-06T03:02:11.433338: step 14274, loss 0.0108802, acc 1
2016-09-06T03:02:12.263375: step 14275, loss 0.0016438, acc 1
2016-09-06T03:02:13.061112: step 14276, loss 0.0268789, acc 0.98
2016-09-06T03:02:13.869648: step 14277, loss 0.0046834, acc 1
2016-09-06T03:02:14.703335: step 14278, loss 0.00225185, acc 1
2016-09-06T03:02:15.511324: step 14279, loss 0.00188261, acc 1
2016-09-06T03:02:16.328155: step 14280, loss 0.00177599, acc 1
2016-09-06T03:02:17.180630: step 14281, loss 0.0157803, acc 1
2016-09-06T03:02:17.994314: step 14282, loss 0.00195822, acc 1
2016-09-06T03:02:18.801960: step 14283, loss 0.00178392, acc 1
2016-09-06T03:02:19.626015: step 14284, loss 0.0166929, acc 1
2016-09-06T03:02:20.456411: step 14285, loss 0.00166664, acc 1
2016-09-06T03:02:21.276817: step 14286, loss 0.0354555, acc 0.98
2016-09-06T03:02:22.137239: step 14287, loss 0.0112508, acc 1
2016-09-06T03:02:22.962365: step 14288, loss 0.00177401, acc 1
2016-09-06T03:02:23.787939: step 14289, loss 0.00156907, acc 1
2016-09-06T03:02:24.624764: step 14290, loss 0.015739, acc 0.98
2016-09-06T03:02:25.421406: step 14291, loss 0.0398969, acc 0.98
2016-09-06T03:02:26.209619: step 14292, loss 0.00153989, acc 1
2016-09-06T03:02:27.055708: step 14293, loss 0.00157656, acc 1
2016-09-06T03:02:27.859177: step 14294, loss 0.0157153, acc 0.98
2016-09-06T03:02:28.693204: step 14295, loss 0.00151863, acc 1
2016-09-06T03:02:29.508337: step 14296, loss 0.0153572, acc 1
2016-09-06T03:02:30.316626: step 14297, loss 0.0041404, acc 1
2016-09-06T03:02:31.109374: step 14298, loss 0.00254023, acc 1
2016-09-06T03:02:31.907088: step 14299, loss 0.0169581, acc 1
2016-09-06T03:02:32.717214: step 14300, loss 0.00158524, acc 1

Evaluation:
2016-09-06T03:02:36.444847: step 14300, loss 2.57447, acc 0.72045

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-14300

2016-09-06T03:02:38.388724: step 14301, loss 0.00687998, acc 1
2016-09-06T03:02:39.225361: step 14302, loss 0.00271311, acc 1
2016-09-06T03:02:40.042976: step 14303, loss 0.007394, acc 1
2016-09-06T03:02:40.823411: step 14304, loss 0.00497376, acc 1
2016-09-06T03:02:41.634650: step 14305, loss 0.00637558, acc 1
2016-09-06T03:02:42.460079: step 14306, loss 0.0126982, acc 1
2016-09-06T03:02:43.229465: step 14307, loss 0.0143689, acc 1
2016-09-06T03:02:44.011559: step 14308, loss 0.0123461, acc 1
2016-09-06T03:02:44.821417: step 14309, loss 0.0827604, acc 0.98
2016-09-06T03:02:45.610336: step 14310, loss 0.00158661, acc 1
2016-09-06T03:02:46.425439: step 14311, loss 0.00931551, acc 1
2016-09-06T03:02:47.247822: step 14312, loss 0.00214031, acc 1
2016-09-06T03:02:48.044154: step 14313, loss 0.0161929, acc 0.98
2016-09-06T03:02:48.850303: step 14314, loss 0.00619707, acc 1
2016-09-06T03:02:49.665769: step 14315, loss 0.0204783, acc 0.98
2016-09-06T03:02:50.480089: step 14316, loss 0.0171953, acc 0.98
2016-09-06T03:02:51.286086: step 14317, loss 0.0379379, acc 0.98
2016-09-06T03:02:52.107334: step 14318, loss 0.0240348, acc 1
2016-09-06T03:02:52.909940: step 14319, loss 0.00205048, acc 1
2016-09-06T03:02:53.701295: step 14320, loss 0.0533823, acc 0.96
2016-09-06T03:02:54.504406: step 14321, loss 0.0174604, acc 0.98
2016-09-06T03:02:55.296183: step 14322, loss 0.0604906, acc 0.98
2016-09-06T03:02:56.097086: step 14323, loss 0.0082122, acc 1
2016-09-06T03:02:56.914078: step 14324, loss 0.00177804, acc 1
2016-09-06T03:02:57.697442: step 14325, loss 0.00171681, acc 1
2016-09-06T03:02:58.510057: step 14326, loss 0.00204056, acc 1
2016-09-06T03:02:59.290700: step 14327, loss 0.0149604, acc 1
2016-09-06T03:03:00.085723: step 14328, loss 0.0223159, acc 0.98
2016-09-06T03:03:00.935097: step 14329, loss 0.00219826, acc 1
2016-09-06T03:03:01.747043: step 14330, loss 0.00197308, acc 1
2016-09-06T03:03:02.511682: step 14331, loss 0.0146578, acc 1
2016-09-06T03:03:03.333866: step 14332, loss 0.00206348, acc 1
2016-09-06T03:03:04.141564: step 14333, loss 0.0254074, acc 1
2016-09-06T03:03:04.940066: step 14334, loss 0.00642966, acc 1
2016-09-06T03:03:05.766490: step 14335, loss 0.0296342, acc 1
2016-09-06T03:03:06.596057: step 14336, loss 0.0288551, acc 0.98
2016-09-06T03:03:07.402384: step 14337, loss 0.00711241, acc 1
2016-09-06T03:03:08.208783: step 14338, loss 0.0158824, acc 1
2016-09-06T03:03:09.045506: step 14339, loss 0.00243727, acc 1
2016-09-06T03:03:09.805877: step 14340, loss 0.013725, acc 1
2016-09-06T03:03:10.599803: step 14341, loss 0.0436778, acc 0.98
2016-09-06T03:03:11.426245: step 14342, loss 0.0632564, acc 0.98
2016-09-06T03:03:12.230623: step 14343, loss 0.0196423, acc 0.98
2016-09-06T03:03:13.024333: step 14344, loss 0.0100987, acc 1
2016-09-06T03:03:13.836478: step 14345, loss 0.00255632, acc 1
2016-09-06T03:03:14.636021: step 14346, loss 0.0186933, acc 1
2016-09-06T03:03:15.444141: step 14347, loss 0.00308387, acc 1
2016-09-06T03:03:16.269773: step 14348, loss 0.0027847, acc 1
2016-09-06T03:03:17.082201: step 14349, loss 0.0340504, acc 0.98
2016-09-06T03:03:17.945475: step 14350, loss 0.00350518, acc 1
2016-09-06T03:03:19.074935: step 14351, loss 0.0111959, acc 1
2016-09-06T03:03:20.074427: step 14352, loss 0.0162499, acc 1
2016-09-06T03:03:20.904917: step 14353, loss 0.026406, acc 1
2016-09-06T03:03:21.853017: step 14354, loss 0.262562, acc 0.98
2016-09-06T03:03:22.682769: step 14355, loss 0.0123743, acc 1
2016-09-06T03:03:23.491918: step 14356, loss 0.00499266, acc 1
2016-09-06T03:03:24.303913: step 14357, loss 0.0276724, acc 0.98
2016-09-06T03:03:25.100023: step 14358, loss 0.0405812, acc 0.98
2016-09-06T03:03:25.905405: step 14359, loss 0.00335784, acc 1
2016-09-06T03:03:26.693789: step 14360, loss 0.00354984, acc 1
2016-09-06T03:03:27.530941: step 14361, loss 0.00400716, acc 1
2016-09-06T03:03:28.353305: step 14362, loss 0.00359404, acc 1
2016-09-06T03:03:29.125775: step 14363, loss 0.0259082, acc 0.98
2016-09-06T03:03:29.938393: step 14364, loss 0.0294691, acc 0.98
2016-09-06T03:03:30.748973: step 14365, loss 0.00397841, acc 1
2016-09-06T03:03:31.542253: step 14366, loss 0.0037689, acc 1
2016-09-06T03:03:32.346489: step 14367, loss 0.00442495, acc 1
2016-09-06T03:03:33.167673: step 14368, loss 0.0283869, acc 0.98
2016-09-06T03:03:33.955010: step 14369, loss 0.0415775, acc 0.98
2016-09-06T03:03:34.764043: step 14370, loss 0.0164799, acc 1
2016-09-06T03:03:35.591467: step 14371, loss 0.0111004, acc 1
2016-09-06T03:03:36.368694: step 14372, loss 0.00712303, acc 1
2016-09-06T03:03:37.256928: step 14373, loss 0.00410566, acc 1
2016-09-06T03:03:38.077300: step 14374, loss 0.00613668, acc 1
2016-09-06T03:03:38.901729: step 14375, loss 0.0042411, acc 1
2016-09-06T03:03:39.739138: step 14376, loss 0.00945514, acc 1
2016-09-06T03:03:40.599477: step 14377, loss 0.0850982, acc 0.96
2016-09-06T03:03:41.416099: step 14378, loss 0.0042633, acc 1
2016-09-06T03:03:42.215785: step 14379, loss 0.00506419, acc 1
2016-09-06T03:03:43.079294: step 14380, loss 0.00431882, acc 1
2016-09-06T03:03:43.919250: step 14381, loss 0.0264301, acc 0.98
2016-09-06T03:03:44.731876: step 14382, loss 0.0213824, acc 0.98
2016-09-06T03:03:45.614033: step 14383, loss 0.0382368, acc 0.98
2016-09-06T03:03:46.420419: step 14384, loss 0.0104175, acc 1
2016-09-06T03:03:47.201895: step 14385, loss 0.00837862, acc 1
2016-09-06T03:03:48.039953: step 14386, loss 0.0288368, acc 0.98
2016-09-06T03:03:48.838804: step 14387, loss 0.00426546, acc 1
2016-09-06T03:03:49.662241: step 14388, loss 0.0213034, acc 1
2016-09-06T03:03:50.473753: step 14389, loss 0.00477998, acc 1
2016-09-06T03:03:51.288400: step 14390, loss 0.00986122, acc 1
2016-09-06T03:03:52.054944: step 14391, loss 0.00420115, acc 1
2016-09-06T03:03:52.878379: step 14392, loss 0.00603044, acc 1
2016-09-06T03:03:53.688750: step 14393, loss 0.0101587, acc 1
2016-09-06T03:03:54.485211: step 14394, loss 0.00544796, acc 1
2016-09-06T03:03:55.306998: step 14395, loss 0.0649761, acc 0.96
2016-09-06T03:03:56.136602: step 14396, loss 0.0402952, acc 0.98
2016-09-06T03:03:56.909963: step 14397, loss 0.0185914, acc 1
2016-09-06T03:03:57.741096: step 14398, loss 0.011377, acc 1
2016-09-06T03:03:58.601868: step 14399, loss 0.124502, acc 0.96
2016-09-06T03:03:59.316456: step 14400, loss 0.00370754, acc 1

Evaluation:
2016-09-06T03:04:03.116402: step 14400, loss 2.74688, acc 0.71576

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-14400

2016-09-06T03:04:05.031577: step 14401, loss 0.00402493, acc 1
2016-09-06T03:04:05.847505: step 14402, loss 0.00706616, acc 1
2016-09-06T03:04:06.651013: step 14403, loss 0.00565604, acc 1
2016-09-06T03:04:07.465251: step 14404, loss 0.0152901, acc 1
2016-09-06T03:04:08.335682: step 14405, loss 0.00386869, acc 1
2016-09-06T03:04:09.119992: step 14406, loss 0.00380322, acc 1
2016-09-06T03:04:09.918155: step 14407, loss 0.00353579, acc 1
2016-09-06T03:04:10.716979: step 14408, loss 0.00338589, acc 1
2016-09-06T03:04:11.500141: step 14409, loss 0.014413, acc 1
2016-09-06T03:04:12.354949: step 14410, loss 0.00330324, acc 1
2016-09-06T03:04:13.167918: step 14411, loss 0.018053, acc 1
2016-09-06T03:04:13.980222: step 14412, loss 0.00502214, acc 1
2016-09-06T03:04:14.798094: step 14413, loss 0.00574738, acc 1
2016-09-06T03:04:15.616424: step 14414, loss 0.00324498, acc 1
2016-09-06T03:04:16.411265: step 14415, loss 0.00943689, acc 1
2016-09-06T03:04:17.197763: step 14416, loss 0.12909, acc 0.94
2016-09-06T03:04:18.034040: step 14417, loss 0.144033, acc 0.98
2016-09-06T03:04:18.812539: step 14418, loss 0.00625138, acc 1
2016-09-06T03:04:19.616720: step 14419, loss 0.0030928, acc 1
2016-09-06T03:04:20.421720: step 14420, loss 0.0160141, acc 1
2016-09-06T03:04:21.217337: step 14421, loss 0.002895, acc 1
2016-09-06T03:04:22.011844: step 14422, loss 0.0220633, acc 1
2016-09-06T03:04:22.798094: step 14423, loss 0.00622436, acc 1
2016-09-06T03:04:23.577156: step 14424, loss 0.026199, acc 0.98
2016-09-06T03:04:24.406276: step 14425, loss 0.00381116, acc 1
2016-09-06T03:04:25.222684: step 14426, loss 0.0166069, acc 1
2016-09-06T03:04:26.005590: step 14427, loss 0.0133413, acc 1
2016-09-06T03:04:26.853236: step 14428, loss 0.00480392, acc 1
2016-09-06T03:04:27.681783: step 14429, loss 0.0827209, acc 0.98
2016-09-06T03:04:28.485723: step 14430, loss 0.00457029, acc 1
2016-09-06T03:04:29.291742: step 14431, loss 0.0404596, acc 0.96
2016-09-06T03:04:30.124580: step 14432, loss 0.0100812, acc 1
2016-09-06T03:04:30.921680: step 14433, loss 0.0157503, acc 1
2016-09-06T03:04:31.709002: step 14434, loss 0.00351531, acc 1
2016-09-06T03:04:32.541905: step 14435, loss 0.00282872, acc 1
2016-09-06T03:04:33.368876: step 14436, loss 0.025932, acc 0.98
2016-09-06T03:04:34.152163: step 14437, loss 0.00250725, acc 1
2016-09-06T03:04:34.987902: step 14438, loss 0.0248574, acc 0.98
2016-09-06T03:04:35.749264: step 14439, loss 0.0162496, acc 1
2016-09-06T03:04:36.545965: step 14440, loss 0.00248009, acc 1
2016-09-06T03:04:37.366227: step 14441, loss 0.013903, acc 1
2016-09-06T03:04:38.160667: step 14442, loss 0.00464322, acc 1
2016-09-06T03:04:38.968290: step 14443, loss 0.0368934, acc 1
2016-09-06T03:04:39.788196: step 14444, loss 0.00285729, acc 1
2016-09-06T03:04:40.571992: step 14445, loss 0.00586068, acc 1
2016-09-06T03:04:41.351044: step 14446, loss 0.0473604, acc 0.98
2016-09-06T03:04:42.171091: step 14447, loss 0.00553057, acc 1
2016-09-06T03:04:42.970198: step 14448, loss 0.0172684, acc 1
2016-09-06T03:04:43.841409: step 14449, loss 0.0290785, acc 1
2016-09-06T03:04:44.671129: step 14450, loss 0.00429526, acc 1
2016-09-06T03:04:45.456186: step 14451, loss 0.0340936, acc 0.96
2016-09-06T03:04:46.242546: step 14452, loss 0.0109133, acc 1
2016-09-06T03:04:47.069152: step 14453, loss 0.00910406, acc 1
2016-09-06T03:04:47.849906: step 14454, loss 0.0256612, acc 0.98
2016-09-06T03:04:48.675577: step 14455, loss 0.0358554, acc 0.98
2016-09-06T03:04:49.495644: step 14456, loss 0.0125673, acc 1
2016-09-06T03:04:50.270284: step 14457, loss 0.00607534, acc 1
2016-09-06T03:04:51.054146: step 14458, loss 0.0256712, acc 0.98
2016-09-06T03:04:51.859960: step 14459, loss 0.00341263, acc 1
2016-09-06T03:04:52.643686: step 14460, loss 0.0327642, acc 0.98
2016-09-06T03:04:53.436944: step 14461, loss 0.0127508, acc 1
2016-09-06T03:04:54.242756: step 14462, loss 0.00952946, acc 1
2016-09-06T03:04:55.048686: step 14463, loss 0.00941681, acc 1
2016-09-06T03:04:55.877015: step 14464, loss 0.023392, acc 1
2016-09-06T03:04:56.673232: step 14465, loss 0.00273602, acc 1
2016-09-06T03:04:57.488117: step 14466, loss 0.0480258, acc 0.98
2016-09-06T03:04:58.300585: step 14467, loss 0.0127239, acc 1
2016-09-06T03:04:59.108785: step 14468, loss 0.0976212, acc 0.98
2016-09-06T03:04:59.935509: step 14469, loss 0.0478562, acc 0.98
2016-09-06T03:05:00.800494: step 14470, loss 0.017053, acc 1
2016-09-06T03:05:01.618074: step 14471, loss 0.0034873, acc 1
2016-09-06T03:05:02.393448: step 14472, loss 0.00308911, acc 1
2016-09-06T03:05:03.199781: step 14473, loss 0.0564149, acc 0.94
2016-09-06T03:05:04.010125: step 14474, loss 0.00529052, acc 1
2016-09-06T03:05:04.808887: step 14475, loss 0.0117584, acc 1
2016-09-06T03:05:05.601908: step 14476, loss 0.246303, acc 0.96
2016-09-06T03:05:06.417214: step 14477, loss 0.0224596, acc 0.98
2016-09-06T03:05:07.235141: step 14478, loss 0.0123293, acc 1
2016-09-06T03:05:08.046445: step 14479, loss 0.00697691, acc 1
2016-09-06T03:05:08.819009: step 14480, loss 0.0388142, acc 0.98
2016-09-06T03:05:09.596461: step 14481, loss 0.0245974, acc 0.98
2016-09-06T03:05:10.398067: step 14482, loss 0.0111346, acc 1
2016-09-06T03:05:11.205902: step 14483, loss 0.0157249, acc 1
2016-09-06T03:05:11.978604: step 14484, loss 0.0073884, acc 1
2016-09-06T03:05:12.783114: step 14485, loss 0.0102282, acc 1
2016-09-06T03:05:13.722679: step 14486, loss 0.0168122, acc 1
2016-09-06T03:05:14.607549: step 14487, loss 0.00465895, acc 1
2016-09-06T03:05:15.440142: step 14488, loss 0.0159598, acc 1
2016-09-06T03:05:16.302397: step 14489, loss 0.0367833, acc 0.98
2016-09-06T03:05:17.123409: step 14490, loss 0.00549067, acc 1
2016-09-06T03:05:17.934084: step 14491, loss 0.0638463, acc 0.98
2016-09-06T03:05:18.872601: step 14492, loss 0.00466268, acc 1
2016-09-06T03:05:19.707956: step 14493, loss 0.0493153, acc 0.96
2016-09-06T03:05:20.539846: step 14494, loss 0.0215816, acc 0.98
2016-09-06T03:05:21.330307: step 14495, loss 0.018684, acc 1
2016-09-06T03:05:22.142055: step 14496, loss 0.00502775, acc 1
2016-09-06T03:05:22.955187: step 14497, loss 0.00707996, acc 1
2016-09-06T03:05:23.760839: step 14498, loss 0.00982631, acc 1
2016-09-06T03:05:24.601403: step 14499, loss 0.00533095, acc 1
2016-09-06T03:05:25.492404: step 14500, loss 0.0116697, acc 1

Evaluation:
2016-09-06T03:05:29.249622: step 14500, loss 3.51069, acc 0.711069

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-14500

2016-09-06T03:05:31.134848: step 14501, loss 0.0610648, acc 0.96
2016-09-06T03:05:32.082047: step 14502, loss 0.00909459, acc 1
2016-09-06T03:05:32.879640: step 14503, loss 0.00527111, acc 1
2016-09-06T03:05:33.697884: step 14504, loss 0.005368, acc 1
2016-09-06T03:05:34.547034: step 14505, loss 0.0108659, acc 1
2016-09-06T03:05:35.362316: step 14506, loss 0.00553423, acc 1
2016-09-06T03:05:36.210534: step 14507, loss 0.0306234, acc 1
2016-09-06T03:05:37.084049: step 14508, loss 0.101157, acc 0.98
2016-09-06T03:05:38.023718: step 14509, loss 0.0183251, acc 1
2016-09-06T03:05:38.920654: step 14510, loss 0.0845723, acc 0.96
2016-09-06T03:05:39.727108: step 14511, loss 0.00503356, acc 1
2016-09-06T03:05:40.549930: step 14512, loss 0.168303, acc 0.96
2016-09-06T03:05:41.368473: step 14513, loss 0.00991748, acc 1
2016-09-06T03:05:42.198166: step 14514, loss 0.0113704, acc 1
2016-09-06T03:05:43.061471: step 14515, loss 0.0365464, acc 0.98
2016-09-06T03:05:43.882938: step 14516, loss 0.0054256, acc 1
2016-09-06T03:05:44.688922: step 14517, loss 0.00480056, acc 1
2016-09-06T03:05:45.541105: step 14518, loss 0.0121249, acc 1
2016-09-06T03:05:46.359020: step 14519, loss 0.0156078, acc 1
2016-09-06T03:05:47.172101: step 14520, loss 0.201904, acc 0.96
2016-09-06T03:05:47.983467: step 14521, loss 0.0949628, acc 0.98
2016-09-06T03:05:48.778603: step 14522, loss 0.00586171, acc 1
2016-09-06T03:05:49.575213: step 14523, loss 0.0140383, acc 1
2016-09-06T03:05:50.369997: step 14524, loss 0.00798893, acc 1
2016-09-06T03:05:51.173011: step 14525, loss 0.0484972, acc 0.98
2016-09-06T03:05:51.962370: step 14526, loss 0.0831257, acc 0.98
2016-09-06T03:05:52.742612: step 14527, loss 0.0328838, acc 0.98
2016-09-06T03:05:53.562801: step 14528, loss 0.0233161, acc 0.98
2016-09-06T03:05:54.379933: step 14529, loss 0.0261419, acc 0.98
2016-09-06T03:05:55.155054: step 14530, loss 0.0136906, acc 1
2016-09-06T03:05:55.964294: step 14531, loss 0.00936776, acc 1
2016-09-06T03:05:56.724900: step 14532, loss 0.0148537, acc 1
2016-09-06T03:05:57.535313: step 14533, loss 0.0282822, acc 0.98
2016-09-06T03:05:58.349695: step 14534, loss 0.0884248, acc 0.98
2016-09-06T03:05:59.133403: step 14535, loss 0.0242011, acc 1
2016-09-06T03:05:59.958921: step 14536, loss 0.0360485, acc 0.98
2016-09-06T03:06:00.775975: step 14537, loss 0.00525578, acc 1
2016-09-06T03:06:01.571121: step 14538, loss 0.0363433, acc 0.98
2016-09-06T03:06:02.383277: step 14539, loss 0.0136509, acc 1
2016-09-06T03:06:03.232135: step 14540, loss 0.022965, acc 0.98
2016-09-06T03:06:04.006800: step 14541, loss 0.0198907, acc 0.98
2016-09-06T03:06:04.797306: step 14542, loss 0.00655068, acc 1
2016-09-06T03:06:05.600625: step 14543, loss 0.00509887, acc 1
2016-09-06T03:06:06.382973: step 14544, loss 0.137646, acc 0.98
2016-09-06T03:06:07.209378: step 14545, loss 0.128074, acc 0.98
2016-09-06T03:06:08.037892: step 14546, loss 0.0344013, acc 0.98
2016-09-06T03:06:08.848615: step 14547, loss 0.0175559, acc 1
2016-09-06T03:06:09.662666: step 14548, loss 0.00381779, acc 1
2016-09-06T03:06:10.470991: step 14549, loss 0.0352824, acc 0.98
2016-09-06T03:06:11.284321: step 14550, loss 0.0214882, acc 0.98
2016-09-06T03:06:12.087085: step 14551, loss 0.00754818, acc 1
2016-09-06T03:06:12.891380: step 14552, loss 0.00814672, acc 1
2016-09-06T03:06:13.691152: step 14553, loss 0.081581, acc 0.98
2016-09-06T03:06:14.491743: step 14554, loss 0.00862981, acc 1
2016-09-06T03:06:15.294129: step 14555, loss 0.0697955, acc 0.98
2016-09-06T03:06:16.086024: step 14556, loss 0.077012, acc 0.98
2016-09-06T03:06:16.941885: step 14557, loss 0.0260193, acc 0.98
2016-09-06T03:06:17.777146: step 14558, loss 0.0959785, acc 0.96
2016-09-06T03:06:18.554293: step 14559, loss 0.00696043, acc 1
2016-09-06T03:06:19.368512: step 14560, loss 0.0248767, acc 0.98
2016-09-06T03:06:20.176657: step 14561, loss 0.041944, acc 0.98
2016-09-06T03:06:20.953743: step 14562, loss 0.00358495, acc 1
2016-09-06T03:06:21.753396: step 14563, loss 0.00698918, acc 1
2016-09-06T03:06:22.588862: step 14564, loss 0.0888664, acc 0.98
2016-09-06T03:06:23.366431: step 14565, loss 0.00683729, acc 1
2016-09-06T03:06:24.159715: step 14566, loss 0.0311561, acc 0.98
2016-09-06T03:06:24.996922: step 14567, loss 0.010226, acc 1
2016-09-06T03:06:25.769977: step 14568, loss 0.00671291, acc 1
2016-09-06T03:06:26.551254: step 14569, loss 0.0531645, acc 0.98
2016-09-06T03:06:27.364105: step 14570, loss 0.00350247, acc 1
2016-09-06T03:06:28.151975: step 14571, loss 0.0194032, acc 1
2016-09-06T03:06:28.969125: step 14572, loss 0.00355472, acc 1
2016-09-06T03:06:29.807572: step 14573, loss 0.00333971, acc 1
2016-09-06T03:06:30.621337: step 14574, loss 0.0170893, acc 1
2016-09-06T03:06:31.451222: step 14575, loss 0.0188349, acc 1
2016-09-06T03:06:32.266628: step 14576, loss 0.00421717, acc 1
2016-09-06T03:06:33.077198: step 14577, loss 0.0106764, acc 1
2016-09-06T03:06:33.874913: step 14578, loss 0.0368387, acc 0.98
2016-09-06T03:06:34.697777: step 14579, loss 0.00891522, acc 1
2016-09-06T03:06:35.498262: step 14580, loss 0.020247, acc 0.98
2016-09-06T03:06:36.302969: step 14581, loss 0.00741383, acc 1
2016-09-06T03:06:37.114463: step 14582, loss 0.00855108, acc 1
2016-09-06T03:06:37.938520: step 14583, loss 0.00392129, acc 1
2016-09-06T03:06:38.749378: step 14584, loss 0.00402415, acc 1
2016-09-06T03:06:39.598915: step 14585, loss 0.00675749, acc 1
2016-09-06T03:06:40.386191: step 14586, loss 0.0303469, acc 1
2016-09-06T03:06:41.183374: step 14587, loss 0.0333072, acc 0.98
2016-09-06T03:06:42.020599: step 14588, loss 0.00407352, acc 1
2016-09-06T03:06:42.833607: step 14589, loss 0.0551984, acc 0.96
2016-09-06T03:06:43.656424: step 14590, loss 0.00453884, acc 1
2016-09-06T03:06:44.521372: step 14591, loss 0.127181, acc 0.94
2016-09-06T03:06:45.272246: step 14592, loss 0.00524741, acc 1
2016-09-06T03:06:46.085524: step 14593, loss 0.00316592, acc 1
2016-09-06T03:06:46.901429: step 14594, loss 0.0101913, acc 1
2016-09-06T03:06:47.719515: step 14595, loss 0.0126699, acc 1
2016-09-06T03:06:48.534844: step 14596, loss 0.0163041, acc 1
2016-09-06T03:06:49.386215: step 14597, loss 0.00771154, acc 1
2016-09-06T03:06:50.224092: step 14598, loss 0.0338699, acc 0.98
2016-09-06T03:06:51.052355: step 14599, loss 0.0300313, acc 0.98
2016-09-06T03:06:51.881317: step 14600, loss 0.0157335, acc 1

Evaluation:
2016-09-06T03:06:55.668862: step 14600, loss 2.58551, acc 0.71576

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-14600

2016-09-06T03:06:57.457154: step 14601, loss 0.00957985, acc 1
2016-09-06T03:06:58.256787: step 14602, loss 0.00323255, acc 1
2016-09-06T03:06:59.073463: step 14603, loss 0.010208, acc 1
2016-09-06T03:06:59.865396: step 14604, loss 0.0235366, acc 0.98
2016-09-06T03:07:00.690459: step 14605, loss 0.00302694, acc 1
2016-09-06T03:07:01.516882: step 14606, loss 0.00467911, acc 1
2016-09-06T03:07:02.332057: step 14607, loss 0.00303962, acc 1
2016-09-06T03:07:03.140167: step 14608, loss 0.00710462, acc 1
2016-09-06T03:07:03.965445: step 14609, loss 0.01802, acc 1
2016-09-06T03:07:04.787374: step 14610, loss 0.0032046, acc 1
2016-09-06T03:07:05.595143: step 14611, loss 0.0118031, acc 1
2016-09-06T03:07:06.435774: step 14612, loss 0.287789, acc 0.96
2016-09-06T03:07:07.237511: step 14613, loss 0.00852051, acc 1
2016-09-06T03:07:08.078199: step 14614, loss 0.0216852, acc 1
2016-09-06T03:07:08.917810: step 14615, loss 0.0214875, acc 0.98
2016-09-06T03:07:09.741443: step 14616, loss 0.0186996, acc 0.98
2016-09-06T03:07:10.570720: step 14617, loss 0.00787849, acc 1
2016-09-06T03:07:11.388436: step 14618, loss 0.0334576, acc 1
2016-09-06T03:07:12.193223: step 14619, loss 0.0153664, acc 1
2016-09-06T03:07:13.027515: step 14620, loss 0.00767978, acc 1
2016-09-06T03:07:13.844523: step 14621, loss 0.0116215, acc 1
2016-09-06T03:07:14.659344: step 14622, loss 0.0064735, acc 1
2016-09-06T03:07:15.472089: step 14623, loss 0.0210171, acc 0.98
2016-09-06T03:07:16.273299: step 14624, loss 0.00641678, acc 1
2016-09-06T03:07:17.093404: step 14625, loss 0.0599404, acc 0.98
2016-09-06T03:07:17.865249: step 14626, loss 0.0129078, acc 1
2016-09-06T03:07:18.685165: step 14627, loss 0.00364764, acc 1
2016-09-06T03:07:19.513397: step 14628, loss 0.0197548, acc 0.98
2016-09-06T03:07:20.293771: step 14629, loss 0.00676468, acc 1
2016-09-06T03:07:21.140942: step 14630, loss 0.0252761, acc 0.98
2016-09-06T03:07:21.962316: step 14631, loss 0.023056, acc 0.98
2016-09-06T03:07:22.769862: step 14632, loss 0.00397459, acc 1
2016-09-06T03:07:23.567096: step 14633, loss 0.0173277, acc 0.98
2016-09-06T03:07:24.362514: step 14634, loss 0.0102529, acc 1
2016-09-06T03:07:25.120875: step 14635, loss 0.0190637, acc 0.98
2016-09-06T03:07:25.937909: step 14636, loss 0.00572754, acc 1
2016-09-06T03:07:26.756101: step 14637, loss 0.00314122, acc 1
2016-09-06T03:07:27.535733: step 14638, loss 0.0206571, acc 1
2016-09-06T03:07:28.341542: step 14639, loss 0.0134485, acc 1
2016-09-06T03:07:29.163280: step 14640, loss 0.013857, acc 1
2016-09-06T03:07:29.965952: step 14641, loss 0.0192473, acc 1
2016-09-06T03:07:30.752052: step 14642, loss 0.0035323, acc 1
2016-09-06T03:07:31.550020: step 14643, loss 0.0452797, acc 0.98
2016-09-06T03:07:32.347757: step 14644, loss 0.0151993, acc 1
2016-09-06T03:07:33.181978: step 14645, loss 0.00474052, acc 1
2016-09-06T03:07:34.029694: step 14646, loss 0.0194448, acc 0.98
2016-09-06T03:07:34.801920: step 14647, loss 0.0659536, acc 0.98
2016-09-06T03:07:35.605490: step 14648, loss 0.021228, acc 0.98
2016-09-06T03:07:36.427967: step 14649, loss 0.00307076, acc 1
2016-09-06T03:07:37.212463: step 14650, loss 0.00330145, acc 1
2016-09-06T03:07:38.009451: step 14651, loss 0.0102001, acc 1
2016-09-06T03:07:38.843442: step 14652, loss 0.00292762, acc 1
2016-09-06T03:07:39.645115: step 14653, loss 0.00892814, acc 1
2016-09-06T03:07:40.457230: step 14654, loss 0.0036687, acc 1
2016-09-06T03:07:41.280984: step 14655, loss 0.00301896, acc 1
2016-09-06T03:07:42.055047: step 14656, loss 0.0131655, acc 1
2016-09-06T03:07:42.839101: step 14657, loss 0.0403232, acc 0.98
2016-09-06T03:07:43.654655: step 14658, loss 0.0087745, acc 1
2016-09-06T03:07:44.450557: step 14659, loss 0.00292431, acc 1
2016-09-06T03:07:45.237014: step 14660, loss 0.0257294, acc 1
2016-09-06T03:07:46.057066: step 14661, loss 0.00657916, acc 1
2016-09-06T03:07:46.860095: step 14662, loss 0.00360656, acc 1
2016-09-06T03:07:47.653636: step 14663, loss 0.00464059, acc 1
2016-09-06T03:07:48.484837: step 14664, loss 0.00321842, acc 1
2016-09-06T03:07:49.276705: step 14665, loss 0.00708868, acc 1
2016-09-06T03:07:50.080894: step 14666, loss 0.0034695, acc 1
2016-09-06T03:07:50.915511: step 14667, loss 0.00684066, acc 1
2016-09-06T03:07:51.705415: step 14668, loss 0.025015, acc 1
2016-09-06T03:07:52.506316: step 14669, loss 0.0509214, acc 0.96
2016-09-06T03:07:53.332225: step 14670, loss 0.00443221, acc 1
2016-09-06T03:07:54.110415: step 14671, loss 0.0121066, acc 1
2016-09-06T03:07:54.923543: step 14672, loss 0.00298075, acc 1
2016-09-06T03:07:55.717194: step 14673, loss 0.00803777, acc 1
2016-09-06T03:07:56.506702: step 14674, loss 0.0027473, acc 1
2016-09-06T03:07:57.326799: step 14675, loss 0.0213809, acc 0.98
2016-09-06T03:07:58.153125: step 14676, loss 0.114564, acc 0.98
2016-09-06T03:07:58.971140: step 14677, loss 0.0157237, acc 1
2016-09-06T03:07:59.778580: step 14678, loss 0.0336218, acc 0.98
2016-09-06T03:08:00.616830: step 14679, loss 0.00252297, acc 1
2016-09-06T03:08:01.402863: step 14680, loss 0.00518998, acc 1
2016-09-06T03:08:02.213684: step 14681, loss 0.0106125, acc 1
2016-09-06T03:08:03.063259: step 14682, loss 0.054258, acc 0.98
2016-09-06T03:08:03.879596: step 14683, loss 0.0154571, acc 1
2016-09-06T03:08:04.691170: step 14684, loss 0.00742101, acc 1
2016-09-06T03:08:05.497273: step 14685, loss 0.00379617, acc 1
2016-09-06T03:08:06.285119: step 14686, loss 0.0180745, acc 0.98
2016-09-06T03:08:07.092793: step 14687, loss 0.00320391, acc 1
2016-09-06T03:08:07.911835: step 14688, loss 0.00313572, acc 1
2016-09-06T03:08:08.686679: step 14689, loss 0.00236351, acc 1
2016-09-06T03:08:09.500815: step 14690, loss 0.0301384, acc 0.98
2016-09-06T03:08:10.320176: step 14691, loss 0.00514269, acc 1
2016-09-06T03:08:11.115563: step 14692, loss 0.0293848, acc 0.98
2016-09-06T03:08:11.915995: step 14693, loss 0.00528007, acc 1
2016-09-06T03:08:12.739326: step 14694, loss 0.0151639, acc 1
2016-09-06T03:08:13.523559: step 14695, loss 0.00752589, acc 1
2016-09-06T03:08:14.297498: step 14696, loss 0.00217829, acc 1
2016-09-06T03:08:15.113517: step 14697, loss 0.0402971, acc 0.98
2016-09-06T03:08:15.889351: step 14698, loss 0.00918389, acc 1
2016-09-06T03:08:16.691001: step 14699, loss 0.0356066, acc 0.98
2016-09-06T03:08:17.513558: step 14700, loss 0.0172883, acc 0.98

Evaluation:
2016-09-06T03:08:21.252296: step 14700, loss 2.34728, acc 0.709193

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-14700

2016-09-06T03:08:23.209808: step 14701, loss 0.0177858, acc 1
2016-09-06T03:08:24.032338: step 14702, loss 0.0134745, acc 1
2016-09-06T03:08:24.848571: step 14703, loss 0.0023249, acc 1
2016-09-06T03:08:25.632193: step 14704, loss 0.0699679, acc 0.96
2016-09-06T03:08:26.429148: step 14705, loss 0.0263052, acc 1
2016-09-06T03:08:27.258730: step 14706, loss 0.00503466, acc 1
2016-09-06T03:08:28.091572: step 14707, loss 0.0179053, acc 1
2016-09-06T03:08:28.891255: step 14708, loss 0.00292135, acc 1
2016-09-06T03:08:29.705420: step 14709, loss 0.0214035, acc 1
2016-09-06T03:08:30.491277: step 14710, loss 0.00203125, acc 1
2016-09-06T03:08:31.291943: step 14711, loss 0.054393, acc 0.98
2016-09-06T03:08:32.126196: step 14712, loss 0.0222582, acc 1
2016-09-06T03:08:32.939969: step 14713, loss 0.00199894, acc 1
2016-09-06T03:08:33.761653: step 14714, loss 0.00341149, acc 1
2016-09-06T03:08:34.596198: step 14715, loss 0.0134934, acc 1
2016-09-06T03:08:35.369587: step 14716, loss 0.0106177, acc 1
2016-09-06T03:08:36.181531: step 14717, loss 0.0230417, acc 1
2016-09-06T03:08:37.002269: step 14718, loss 0.00523024, acc 1
2016-09-06T03:08:37.756737: step 14719, loss 0.0322362, acc 1
2016-09-06T03:08:38.559941: step 14720, loss 0.0188207, acc 1
2016-09-06T03:08:39.385507: step 14721, loss 0.00653975, acc 1
2016-09-06T03:08:40.169737: step 14722, loss 0.00484648, acc 1
2016-09-06T03:08:40.965246: step 14723, loss 0.0169594, acc 0.98
2016-09-06T03:08:41.785190: step 14724, loss 0.0156148, acc 1
2016-09-06T03:08:42.574774: step 14725, loss 0.00609462, acc 1
2016-09-06T03:08:43.393661: step 14726, loss 0.00382995, acc 1
2016-09-06T03:08:44.193564: step 14727, loss 0.00413546, acc 1
2016-09-06T03:08:45.036509: step 14728, loss 0.0108835, acc 1
2016-09-06T03:08:45.857955: step 14729, loss 0.006073, acc 1
2016-09-06T03:08:46.655748: step 14730, loss 0.0318456, acc 1
2016-09-06T03:08:47.424291: step 14731, loss 0.0121783, acc 1
2016-09-06T03:08:48.208570: step 14732, loss 0.00206814, acc 1
2016-09-06T03:08:49.041550: step 14733, loss 0.0432216, acc 0.96
2016-09-06T03:08:49.852382: step 14734, loss 0.00216032, acc 1
2016-09-06T03:08:50.635770: step 14735, loss 0.00221754, acc 1
2016-09-06T03:08:51.454880: step 14736, loss 0.0199701, acc 0.98
2016-09-06T03:08:52.242540: step 14737, loss 0.00863537, acc 1
2016-09-06T03:08:53.059469: step 14738, loss 0.0300408, acc 0.98
2016-09-06T03:08:53.893737: step 14739, loss 0.0558711, acc 0.98
2016-09-06T03:08:54.665264: step 14740, loss 0.00445957, acc 1
2016-09-06T03:08:55.481204: step 14741, loss 0.0265084, acc 0.98
2016-09-06T03:08:56.287390: step 14742, loss 0.01407, acc 1
2016-09-06T03:08:57.085136: step 14743, loss 0.00211751, acc 1
2016-09-06T03:08:57.877446: step 14744, loss 0.00209081, acc 1
2016-09-06T03:08:58.686574: step 14745, loss 0.0140525, acc 1
2016-09-06T03:08:59.474342: step 14746, loss 0.0023634, acc 1
2016-09-06T03:09:00.305389: step 14747, loss 0.0144387, acc 1
2016-09-06T03:09:01.118395: step 14748, loss 0.00367966, acc 1
2016-09-06T03:09:01.906177: step 14749, loss 0.00568584, acc 1
2016-09-06T03:09:02.718481: step 14750, loss 0.0152208, acc 1
2016-09-06T03:09:03.531533: step 14751, loss 0.0205009, acc 0.98
2016-09-06T03:09:04.352861: step 14752, loss 0.0345123, acc 1
2016-09-06T03:09:05.171025: step 14753, loss 0.0285036, acc 0.98
2016-09-06T03:09:05.989594: step 14754, loss 0.0199248, acc 0.98
2016-09-06T03:09:06.781296: step 14755, loss 0.00239495, acc 1
2016-09-06T03:09:07.548960: step 14756, loss 0.00188182, acc 1
2016-09-06T03:09:08.373453: step 14757, loss 0.00307276, acc 1
2016-09-06T03:09:09.185875: step 14758, loss 0.0154092, acc 1
2016-09-06T03:09:09.976406: step 14759, loss 0.00196991, acc 1
2016-09-06T03:09:10.801551: step 14760, loss 0.0149188, acc 1
2016-09-06T03:09:11.578460: step 14761, loss 0.071576, acc 0.96
2016-09-06T03:09:12.435078: step 14762, loss 0.00251299, acc 1
2016-09-06T03:09:13.227626: step 14763, loss 0.0092048, acc 1
2016-09-06T03:09:14.016457: step 14764, loss 0.00217916, acc 1
2016-09-06T03:09:14.818974: step 14765, loss 0.0043905, acc 1
2016-09-06T03:09:15.628371: step 14766, loss 0.001976, acc 1
2016-09-06T03:09:16.445738: step 14767, loss 0.031674, acc 0.98
2016-09-06T03:09:17.262268: step 14768, loss 0.0140632, acc 1
2016-09-06T03:09:18.089520: step 14769, loss 0.0197111, acc 1
2016-09-06T03:09:18.877030: step 14770, loss 0.0211532, acc 0.98
2016-09-06T03:09:19.672140: step 14771, loss 0.00262187, acc 1
2016-09-06T03:09:20.510126: step 14772, loss 0.0249855, acc 1
2016-09-06T03:09:21.282498: step 14773, loss 0.0105684, acc 1
2016-09-06T03:09:22.087927: step 14774, loss 0.0115123, acc 1
2016-09-06T03:09:22.919831: step 14775, loss 0.00206336, acc 1
2016-09-06T03:09:23.668389: step 14776, loss 0.00244875, acc 1
2016-09-06T03:09:24.502352: step 14777, loss 0.0155754, acc 1
2016-09-06T03:09:25.322033: step 14778, loss 0.0386023, acc 0.96
2016-09-06T03:09:26.098618: step 14779, loss 0.0102118, acc 1
2016-09-06T03:09:26.886744: step 14780, loss 0.00210801, acc 1
2016-09-06T03:09:27.700428: step 14781, loss 0.00668327, acc 1
2016-09-06T03:09:28.497261: step 14782, loss 0.00296584, acc 1
2016-09-06T03:09:29.289035: step 14783, loss 0.00208862, acc 1
2016-09-06T03:09:30.076641: step 14784, loss 0.023403, acc 0.977273
2016-09-06T03:09:30.883722: step 14785, loss 0.0101823, acc 1
2016-09-06T03:09:31.724840: step 14786, loss 0.0296618, acc 0.98
2016-09-06T03:09:32.536067: step 14787, loss 0.00207912, acc 1
2016-09-06T03:09:33.318899: step 14788, loss 0.00207643, acc 1
2016-09-06T03:09:34.130749: step 14789, loss 0.0020682, acc 1
2016-09-06T03:09:34.930954: step 14790, loss 0.00655382, acc 1
2016-09-06T03:09:35.731116: step 14791, loss 0.00255904, acc 1
2016-09-06T03:09:36.569639: step 14792, loss 0.00215072, acc 1
2016-09-06T03:09:37.385930: step 14793, loss 0.020761, acc 0.98
2016-09-06T03:09:38.200706: step 14794, loss 0.00650794, acc 1
2016-09-06T03:09:38.997484: step 14795, loss 0.00398808, acc 1
2016-09-06T03:09:39.839617: step 14796, loss 0.00556023, acc 1
2016-09-06T03:09:40.629426: step 14797, loss 0.00222257, acc 1
2016-09-06T03:09:41.449348: step 14798, loss 0.00211335, acc 1
2016-09-06T03:09:42.265062: step 14799, loss 0.00332579, acc 1
2016-09-06T03:09:43.048317: step 14800, loss 0.033257, acc 0.98

Evaluation:
2016-09-06T03:09:46.837105: step 14800, loss 2.86013, acc 0.717636

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-14800

2016-09-06T03:09:48.641013: step 14801, loss 0.0473107, acc 0.96
2016-09-06T03:09:49.434108: step 14802, loss 0.00203108, acc 1
2016-09-06T03:09:50.231078: step 14803, loss 0.00401597, acc 1
2016-09-06T03:09:51.064980: step 14804, loss 0.0554279, acc 0.96
2016-09-06T03:09:51.897345: step 14805, loss 0.00430118, acc 1
2016-09-06T03:09:52.668189: step 14806, loss 0.00914392, acc 1
2016-09-06T03:09:53.497011: step 14807, loss 0.0038195, acc 1
2016-09-06T03:09:54.329499: step 14808, loss 0.0210916, acc 0.98
2016-09-06T03:09:55.134522: step 14809, loss 0.0105213, acc 1
2016-09-06T03:09:55.944099: step 14810, loss 0.00827946, acc 1
2016-09-06T03:09:56.788880: step 14811, loss 0.00390572, acc 1
2016-09-06T03:09:57.536859: step 14812, loss 0.00294938, acc 1
2016-09-06T03:09:58.330038: step 14813, loss 0.00347898, acc 1
2016-09-06T03:09:59.140872: step 14814, loss 0.0249688, acc 0.98
2016-09-06T03:09:59.928909: step 14815, loss 0.00213207, acc 1
2016-09-06T03:10:00.761337: step 14816, loss 0.00319996, acc 1
2016-09-06T03:10:01.592594: step 14817, loss 0.0203566, acc 0.98
2016-09-06T03:10:02.367787: step 14818, loss 0.00259243, acc 1
2016-09-06T03:10:03.194152: step 14819, loss 0.00192644, acc 1
2016-09-06T03:10:04.013724: step 14820, loss 0.00718727, acc 1
2016-09-06T03:10:04.779013: step 14821, loss 0.00194244, acc 1
2016-09-06T03:10:05.590715: step 14822, loss 0.0124804, acc 1
2016-09-06T03:10:06.411371: step 14823, loss 0.00453492, acc 1
2016-09-06T03:10:07.206006: step 14824, loss 0.002111, acc 1
2016-09-06T03:10:08.060938: step 14825, loss 0.0276345, acc 0.98
2016-09-06T03:10:08.884621: step 14826, loss 0.0185325, acc 0.98
2016-09-06T03:10:09.689918: step 14827, loss 0.0852222, acc 0.94
2016-09-06T03:10:10.499024: step 14828, loss 0.00515546, acc 1
2016-09-06T03:10:11.298984: step 14829, loss 0.00384737, acc 1
2016-09-06T03:10:12.077489: step 14830, loss 0.0339941, acc 0.98
2016-09-06T03:10:12.902145: step 14831, loss 0.00175305, acc 1
2016-09-06T03:10:13.690495: step 14832, loss 0.0308793, acc 0.98
2016-09-06T03:10:14.463049: step 14833, loss 0.00564103, acc 1
2016-09-06T03:10:15.288837: step 14834, loss 0.00171213, acc 1
2016-09-06T03:10:16.107440: step 14835, loss 0.00297947, acc 1
2016-09-06T03:10:16.889919: step 14836, loss 0.0205625, acc 1
2016-09-06T03:10:17.685226: step 14837, loss 0.0863019, acc 0.98
2016-09-06T03:10:18.529072: step 14838, loss 0.00925291, acc 1
2016-09-06T03:10:19.325613: step 14839, loss 0.0389199, acc 0.98
2016-09-06T03:10:20.130353: step 14840, loss 0.00677064, acc 1
2016-09-06T03:10:20.939669: step 14841, loss 0.00649447, acc 1
2016-09-06T03:10:21.739767: step 14842, loss 0.0214831, acc 0.98
2016-09-06T03:10:22.551684: step 14843, loss 0.00197815, acc 1
2016-09-06T03:10:23.361481: step 14844, loss 0.00154647, acc 1
2016-09-06T03:10:24.130675: step 14845, loss 0.0232452, acc 1
2016-09-06T03:10:24.920808: step 14846, loss 0.00174422, acc 1
2016-09-06T03:10:25.724829: step 14847, loss 0.00325373, acc 1
2016-09-06T03:10:26.523565: step 14848, loss 0.0096358, acc 1
2016-09-06T03:10:27.324291: step 14849, loss 0.0214025, acc 0.98
2016-09-06T03:10:28.118298: step 14850, loss 0.0479429, acc 0.98
2016-09-06T03:10:28.918327: step 14851, loss 0.00206203, acc 1
2016-09-06T03:10:29.700149: step 14852, loss 0.0216352, acc 0.98
2016-09-06T03:10:30.535838: step 14853, loss 0.00324601, acc 1
2016-09-06T03:10:31.342418: step 14854, loss 0.00529716, acc 1
2016-09-06T03:10:32.149417: step 14855, loss 0.0220642, acc 0.98
2016-09-06T03:10:32.954755: step 14856, loss 0.00175431, acc 1
2016-09-06T03:10:33.753490: step 14857, loss 0.0274858, acc 1
2016-09-06T03:10:34.561632: step 14858, loss 0.00269068, acc 1
2016-09-06T03:10:35.377492: step 14859, loss 0.00846187, acc 1
2016-09-06T03:10:36.148031: step 14860, loss 0.0175457, acc 0.98
2016-09-06T03:10:36.967416: step 14861, loss 0.00386333, acc 1
2016-09-06T03:10:37.811370: step 14862, loss 0.0348605, acc 0.98
2016-09-06T03:10:38.632867: step 14863, loss 0.0148733, acc 1
2016-09-06T03:10:39.442407: step 14864, loss 0.0543456, acc 0.98
2016-09-06T03:10:40.250855: step 14865, loss 0.00383828, acc 1
2016-09-06T03:10:41.007861: step 14866, loss 0.0151313, acc 1
2016-09-06T03:10:41.856579: step 14867, loss 0.0452327, acc 0.98
2016-09-06T03:10:42.662452: step 14868, loss 0.00585276, acc 1
2016-09-06T03:10:43.432013: step 14869, loss 0.0222488, acc 0.98
2016-09-06T03:10:44.237774: step 14870, loss 0.00813982, acc 1
2016-09-06T03:10:45.062232: step 14871, loss 0.00271977, acc 1
2016-09-06T03:10:45.844783: step 14872, loss 0.0215242, acc 1
2016-09-06T03:10:46.631287: step 14873, loss 0.010279, acc 1
2016-09-06T03:10:47.430014: step 14874, loss 0.00165313, acc 1
2016-09-06T03:10:48.230646: step 14875, loss 0.00557666, acc 1
2016-09-06T03:10:49.040849: step 14876, loss 0.00206305, acc 1
2016-09-06T03:10:49.854645: step 14877, loss 0.0325308, acc 0.98
2016-09-06T03:10:50.653464: step 14878, loss 0.00166985, acc 1
2016-09-06T03:10:51.450550: step 14879, loss 0.0315715, acc 0.98
2016-09-06T03:10:52.330930: step 14880, loss 0.00222773, acc 1
2016-09-06T03:10:53.131848: step 14881, loss 0.00193696, acc 1
2016-09-06T03:10:53.935517: step 14882, loss 0.0352531, acc 0.98
2016-09-06T03:10:54.735676: step 14883, loss 0.0130812, acc 1
2016-09-06T03:10:55.520721: step 14884, loss 0.0151658, acc 1
2016-09-06T03:10:56.349262: step 14885, loss 0.00402491, acc 1
2016-09-06T03:10:57.165388: step 14886, loss 0.00197054, acc 1
2016-09-06T03:10:57.919491: step 14887, loss 0.00213968, acc 1
2016-09-06T03:10:58.708786: step 14888, loss 0.00177819, acc 1
2016-09-06T03:10:59.509404: step 14889, loss 0.0226513, acc 0.98
2016-09-06T03:11:00.307007: step 14890, loss 0.001709, acc 1
2016-09-06T03:11:01.117067: step 14891, loss 0.0294977, acc 0.98
2016-09-06T03:11:01.938915: step 14892, loss 0.0177838, acc 0.98
2016-09-06T03:11:02.735797: step 14893, loss 0.0135563, acc 1
2016-09-06T03:11:03.543001: step 14894, loss 0.0465086, acc 0.98
2016-09-06T03:11:04.340109: step 14895, loss 0.00172083, acc 1
2016-09-06T03:11:05.159975: step 14896, loss 0.0444588, acc 0.96
2016-09-06T03:11:05.970142: step 14897, loss 0.0389071, acc 0.98
2016-09-06T03:11:06.780408: step 14898, loss 0.00746972, acc 1
2016-09-06T03:11:07.562169: step 14899, loss 0.00482354, acc 1
2016-09-06T03:11:08.363853: step 14900, loss 0.00387243, acc 1

Evaluation:
2016-09-06T03:11:12.101375: step 14900, loss 2.27901, acc 0.708255

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-14900

2016-09-06T03:11:13.919580: step 14901, loss 0.00684784, acc 1
2016-09-06T03:11:14.722486: step 14902, loss 0.00232199, acc 1
2016-09-06T03:11:15.531946: step 14903, loss 0.00566532, acc 1
2016-09-06T03:11:16.355647: step 14904, loss 0.00603184, acc 1
2016-09-06T03:11:17.191960: step 14905, loss 0.00216309, acc 1
2016-09-06T03:11:18.013945: step 14906, loss 0.00152358, acc 1
2016-09-06T03:11:18.813416: step 14907, loss 0.00228616, acc 1
2016-09-06T03:11:19.628895: step 14908, loss 0.016203, acc 0.98
2016-09-06T03:11:20.467906: step 14909, loss 0.015231, acc 1
2016-09-06T03:11:21.288293: step 14910, loss 0.00150134, acc 1
2016-09-06T03:11:22.072477: step 14911, loss 0.0290436, acc 0.98
2016-09-06T03:11:22.884895: step 14912, loss 0.0157518, acc 0.98
2016-09-06T03:11:23.672327: step 14913, loss 0.00302611, acc 1
2016-09-06T03:11:24.453810: step 14914, loss 0.0273657, acc 0.98
2016-09-06T03:11:25.268095: step 14915, loss 0.0189205, acc 0.98
2016-09-06T03:11:26.089462: step 14916, loss 0.00252038, acc 1
2016-09-06T03:11:26.936611: step 14917, loss 0.00908986, acc 1
2016-09-06T03:11:27.763152: step 14918, loss 0.0195081, acc 0.98
2016-09-06T03:11:28.577533: step 14919, loss 0.00325863, acc 1
2016-09-06T03:11:29.390455: step 14920, loss 0.0124836, acc 1
2016-09-06T03:11:30.223881: step 14921, loss 0.00177468, acc 1
2016-09-06T03:11:31.047670: step 14922, loss 0.013837, acc 1
2016-09-06T03:11:31.871215: step 14923, loss 0.00174724, acc 1
2016-09-06T03:11:32.677294: step 14924, loss 0.0288333, acc 0.98
2016-09-06T03:11:33.565692: step 14925, loss 0.0154869, acc 1
2016-09-06T03:11:34.386310: step 14926, loss 0.0095359, acc 1
2016-09-06T03:11:35.215570: step 14927, loss 0.00179299, acc 1
2016-09-06T03:11:36.037116: step 14928, loss 0.0324878, acc 0.98
2016-09-06T03:11:36.849581: step 14929, loss 0.0100171, acc 1
2016-09-06T03:11:37.660803: step 14930, loss 0.0192387, acc 0.98
2016-09-06T03:11:38.505803: step 14931, loss 0.00821852, acc 1
2016-09-06T03:11:39.331160: step 14932, loss 0.00678656, acc 1
2016-09-06T03:11:40.148697: step 14933, loss 0.0169042, acc 0.98
2016-09-06T03:11:40.953145: step 14934, loss 0.0074353, acc 1
2016-09-06T03:11:41.778249: step 14935, loss 0.00154893, acc 1
2016-09-06T03:11:42.575745: step 14936, loss 0.00297722, acc 1
2016-09-06T03:11:43.413912: step 14937, loss 0.0900626, acc 0.98
2016-09-06T03:11:44.230020: step 14938, loss 0.00181288, acc 1
2016-09-06T03:11:45.052261: step 14939, loss 0.00184738, acc 1
2016-09-06T03:11:45.872804: step 14940, loss 0.0251469, acc 0.98
2016-09-06T03:11:46.698312: step 14941, loss 0.0239389, acc 0.98
2016-09-06T03:11:47.471987: step 14942, loss 0.00236753, acc 1
2016-09-06T03:11:48.320846: step 14943, loss 0.00189313, acc 1
2016-09-06T03:11:49.149445: step 14944, loss 0.04308, acc 0.98
2016-09-06T03:11:49.929947: step 14945, loss 0.0276709, acc 0.98
2016-09-06T03:11:50.748754: step 14946, loss 0.00314872, acc 1
2016-09-06T03:11:51.559300: step 14947, loss 0.00803916, acc 1
2016-09-06T03:11:52.342930: step 14948, loss 0.00772633, acc 1
2016-09-06T03:11:53.164440: step 14949, loss 0.00145318, acc 1
2016-09-06T03:11:53.973183: step 14950, loss 0.017377, acc 1
2016-09-06T03:11:54.785406: step 14951, loss 0.0195128, acc 1
2016-09-06T03:11:55.578491: step 14952, loss 0.00449529, acc 1
2016-09-06T03:11:56.411263: step 14953, loss 0.0309506, acc 1
2016-09-06T03:11:57.161497: step 14954, loss 0.00150331, acc 1
2016-09-06T03:11:57.969608: step 14955, loss 0.0319064, acc 0.98
2016-09-06T03:11:58.772204: step 14956, loss 0.00156387, acc 1
2016-09-06T03:11:59.568518: step 14957, loss 0.0132177, acc 1
2016-09-06T03:12:00.412191: step 14958, loss 0.0157031, acc 1
2016-09-06T03:12:01.221422: step 14959, loss 0.0689583, acc 0.96
2016-09-06T03:12:01.997524: step 14960, loss 0.00164734, acc 1
2016-09-06T03:12:02.789762: step 14961, loss 0.107282, acc 0.96
2016-09-06T03:12:03.601827: step 14962, loss 0.0214684, acc 0.98
2016-09-06T03:12:04.405060: step 14963, loss 0.0159829, acc 0.98
2016-09-06T03:12:05.201500: step 14964, loss 0.0901807, acc 0.98
2016-09-06T03:12:06.051916: step 14965, loss 0.00224538, acc 1
2016-09-06T03:12:06.841042: step 14966, loss 0.00285557, acc 1
2016-09-06T03:12:07.647627: step 14967, loss 0.00264264, acc 1
2016-09-06T03:12:08.489366: step 14968, loss 0.00155179, acc 1
2016-09-06T03:12:09.274295: step 14969, loss 0.0188617, acc 1
2016-09-06T03:12:10.091430: step 14970, loss 0.0247388, acc 0.98
2016-09-06T03:12:10.889418: step 14971, loss 0.0176078, acc 0.98
2016-09-06T03:12:11.672682: step 14972, loss 0.00180033, acc 1
2016-09-06T03:12:12.481500: step 14973, loss 0.0391309, acc 0.98
2016-09-06T03:12:13.320922: step 14974, loss 0.00374685, acc 1
2016-09-06T03:12:14.082042: step 14975, loss 0.00904674, acc 1
2016-09-06T03:12:14.823811: step 14976, loss 0.00577533, acc 1
2016-09-06T03:12:15.625251: step 14977, loss 0.00286617, acc 1
2016-09-06T03:12:16.449934: step 14978, loss 0.0298179, acc 0.98
2016-09-06T03:12:17.267449: step 14979, loss 0.00292205, acc 1
2016-09-06T03:12:18.068669: step 14980, loss 0.00745852, acc 1
2016-09-06T03:12:18.861666: step 14981, loss 0.0183395, acc 0.98
2016-09-06T03:12:19.700848: step 14982, loss 0.00623813, acc 1
2016-09-06T03:12:20.536088: step 14983, loss 0.0109025, acc 1
2016-09-06T03:12:21.313840: step 14984, loss 0.0125102, acc 1
2016-09-06T03:12:22.115668: step 14985, loss 0.00397611, acc 1
2016-09-06T03:12:22.942353: step 14986, loss 0.00328324, acc 1
2016-09-06T03:12:23.732626: step 14987, loss 0.018382, acc 0.98
2016-09-06T03:12:24.518433: step 14988, loss 0.0182418, acc 1
2016-09-06T03:12:25.358666: step 14989, loss 0.00325449, acc 1
2016-09-06T03:12:26.175239: step 14990, loss 0.0434712, acc 0.98
2016-09-06T03:12:26.974794: step 14991, loss 0.0112143, acc 1
2016-09-06T03:12:27.810341: step 14992, loss 0.00918147, acc 1
2016-09-06T03:12:28.575769: step 14993, loss 0.00174461, acc 1
2016-09-06T03:12:29.395230: step 14994, loss 0.00195201, acc 1
2016-09-06T03:12:30.235306: step 14995, loss 0.0022858, acc 1
2016-09-06T03:12:30.999755: step 14996, loss 0.00184424, acc 1
2016-09-06T03:12:31.789738: step 14997, loss 0.00182934, acc 1
2016-09-06T03:12:32.608130: step 14998, loss 0.0470073, acc 0.98
2016-09-06T03:12:33.441474: step 14999, loss 0.0159572, acc 1
2016-09-06T03:12:34.230814: step 15000, loss 0.00187407, acc 1

Evaluation:
2016-09-06T03:12:37.967642: step 15000, loss 2.44076, acc 0.717636

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-15000

2016-09-06T03:12:39.876541: step 15001, loss 0.00659391, acc 1
2016-09-06T03:12:40.689015: step 15002, loss 0.0202381, acc 1
2016-09-06T03:12:41.502151: step 15003, loss 0.0232892, acc 0.98
2016-09-06T03:12:42.326282: step 15004, loss 0.0396104, acc 0.98
2016-09-06T03:12:43.096315: step 15005, loss 0.00275009, acc 1
2016-09-06T03:12:43.916181: step 15006, loss 0.0032397, acc 1
2016-09-06T03:12:44.746262: step 15007, loss 0.00611648, acc 1
2016-09-06T03:12:45.556887: step 15008, loss 0.0638431, acc 0.96
2016-09-06T03:12:46.340992: step 15009, loss 0.00479188, acc 1
2016-09-06T03:12:47.153278: step 15010, loss 0.0123734, acc 1
2016-09-06T03:12:47.924947: step 15011, loss 0.00193159, acc 1
2016-09-06T03:12:48.725318: step 15012, loss 0.00326994, acc 1
2016-09-06T03:12:49.522945: step 15013, loss 0.0019742, acc 1
2016-09-06T03:12:50.360145: step 15014, loss 0.0166555, acc 0.98
2016-09-06T03:12:51.163741: step 15015, loss 0.00442843, acc 1
2016-09-06T03:12:51.961363: step 15016, loss 0.00345525, acc 1
2016-09-06T03:12:52.765218: step 15017, loss 0.0453255, acc 0.98
2016-09-06T03:12:53.585504: step 15018, loss 0.00301966, acc 1
2016-09-06T03:12:54.408264: step 15019, loss 0.0123372, acc 1
2016-09-06T03:12:55.193268: step 15020, loss 0.00527451, acc 1
2016-09-06T03:12:56.004809: step 15021, loss 0.00217925, acc 1
2016-09-06T03:12:56.843209: step 15022, loss 0.003809, acc 1
2016-09-06T03:12:57.621663: step 15023, loss 0.0138826, acc 1
2016-09-06T03:12:58.446158: step 15024, loss 0.00557685, acc 1
2016-09-06T03:12:59.253275: step 15025, loss 0.0180937, acc 0.98
2016-09-06T03:13:00.037369: step 15026, loss 0.00383007, acc 1
2016-09-06T03:13:00.861106: step 15027, loss 0.00853598, acc 1
2016-09-06T03:13:01.662482: step 15028, loss 0.00886364, acc 1
2016-09-06T03:13:02.453623: step 15029, loss 0.0299024, acc 0.98
2016-09-06T03:13:03.259235: step 15030, loss 0.0191204, acc 0.98
2016-09-06T03:13:04.072234: step 15031, loss 0.0201874, acc 0.98
2016-09-06T03:13:04.849589: step 15032, loss 0.00183214, acc 1
2016-09-06T03:13:05.642686: step 15033, loss 0.00182751, acc 1
2016-09-06T03:13:06.457035: step 15034, loss 0.0184143, acc 0.98
2016-09-06T03:13:07.270639: step 15035, loss 0.00229326, acc 1
2016-09-06T03:13:08.052910: step 15036, loss 0.00184616, acc 1
2016-09-06T03:13:08.843288: step 15037, loss 0.0160345, acc 1
2016-09-06T03:13:09.659522: step 15038, loss 0.0135285, acc 1
2016-09-06T03:13:10.478604: step 15039, loss 0.00187549, acc 1
2016-09-06T03:13:11.296011: step 15040, loss 0.0139324, acc 1
2016-09-06T03:13:12.098997: step 15041, loss 0.0186438, acc 0.98
2016-09-06T03:13:12.896919: step 15042, loss 0.036773, acc 0.98
2016-09-06T03:13:13.719220: step 15043, loss 0.0314515, acc 0.98
2016-09-06T03:13:14.546115: step 15044, loss 0.0172998, acc 1
2016-09-06T03:13:15.372432: step 15045, loss 0.0337908, acc 0.98
2016-09-06T03:13:16.162666: step 15046, loss 0.00299743, acc 1
2016-09-06T03:13:16.941562: step 15047, loss 0.0568263, acc 0.98
2016-09-06T03:13:17.746216: step 15048, loss 0.00349253, acc 1
2016-09-06T03:13:18.585550: step 15049, loss 0.0438974, acc 0.98
2016-09-06T03:13:19.407036: step 15050, loss 0.00169979, acc 1
2016-09-06T03:13:20.189136: step 15051, loss 0.0099802, acc 1
2016-09-06T03:13:20.999845: step 15052, loss 0.0071381, acc 1
2016-09-06T03:13:21.821613: step 15053, loss 0.0236491, acc 0.98
2016-09-06T03:13:22.619046: step 15054, loss 0.00919452, acc 1
2016-09-06T03:13:23.409367: step 15055, loss 0.0124547, acc 1
2016-09-06T03:13:24.200414: step 15056, loss 0.00460265, acc 1
2016-09-06T03:13:24.993412: step 15057, loss 0.00166204, acc 1
2016-09-06T03:13:25.808419: step 15058, loss 0.0201256, acc 0.98
2016-09-06T03:13:26.585981: step 15059, loss 0.00193966, acc 1
2016-09-06T03:13:27.384528: step 15060, loss 0.00310506, acc 1
2016-09-06T03:13:28.188682: step 15061, loss 0.00160924, acc 1
2016-09-06T03:13:28.981588: step 15062, loss 0.0267566, acc 1
2016-09-06T03:13:29.787552: step 15063, loss 0.00152898, acc 1
2016-09-06T03:13:30.593401: step 15064, loss 0.0271183, acc 0.98
2016-09-06T03:13:31.384646: step 15065, loss 0.005108, acc 1
2016-09-06T03:13:32.202628: step 15066, loss 0.00149479, acc 1
2016-09-06T03:13:33.014418: step 15067, loss 0.0120011, acc 1
2016-09-06T03:13:33.844222: step 15068, loss 0.00149442, acc 1
2016-09-06T03:13:34.639668: step 15069, loss 0.00150741, acc 1
2016-09-06T03:13:35.466836: step 15070, loss 0.00353056, acc 1
2016-09-06T03:13:36.245626: step 15071, loss 0.0154073, acc 1
2016-09-06T03:13:37.040971: step 15072, loss 0.00192101, acc 1
2016-09-06T03:13:37.863674: step 15073, loss 0.00140526, acc 1
2016-09-06T03:13:38.641816: step 15074, loss 0.132096, acc 0.94
2016-09-06T03:13:39.443837: step 15075, loss 0.00141239, acc 1
2016-09-06T03:13:40.239662: step 15076, loss 0.00357722, acc 1
2016-09-06T03:13:41.052920: step 15077, loss 0.0340191, acc 0.98
2016-09-06T03:13:41.840847: step 15078, loss 0.00131281, acc 1
2016-09-06T03:13:42.652116: step 15079, loss 0.00432425, acc 1
2016-09-06T03:13:43.491570: step 15080, loss 0.0165115, acc 1
2016-09-06T03:13:44.298803: step 15081, loss 0.03274, acc 0.96
2016-09-06T03:13:45.089397: step 15082, loss 0.0385423, acc 0.98
2016-09-06T03:13:45.890689: step 15083, loss 0.0130776, acc 1
2016-09-06T03:13:46.702235: step 15084, loss 0.015416, acc 1
2016-09-06T03:13:47.526866: step 15085, loss 0.0196544, acc 1
2016-09-06T03:13:48.356318: step 15086, loss 0.150201, acc 0.98
2016-09-06T03:13:49.150006: step 15087, loss 0.00192488, acc 1
2016-09-06T03:13:49.973403: step 15088, loss 0.0279016, acc 1
2016-09-06T03:13:50.772791: step 15089, loss 0.0145982, acc 1
2016-09-06T03:13:51.617148: step 15090, loss 0.00366848, acc 1
2016-09-06T03:13:52.508586: step 15091, loss 0.021175, acc 1
2016-09-06T03:13:53.326734: step 15092, loss 0.00440749, acc 1
2016-09-06T03:13:54.167851: step 15093, loss 0.0232044, acc 0.98
2016-09-06T03:13:54.992243: step 15094, loss 0.00251314, acc 1
2016-09-06T03:13:55.838233: step 15095, loss 0.0271456, acc 0.98
2016-09-06T03:13:56.642863: step 15096, loss 0.029675, acc 0.98
2016-09-06T03:13:57.460110: step 15097, loss 0.00195231, acc 1
2016-09-06T03:13:58.267561: step 15098, loss 0.00239093, acc 1
2016-09-06T03:13:59.060944: step 15099, loss 0.0116629, acc 1
2016-09-06T03:13:59.881914: step 15100, loss 0.00462214, acc 1

Evaluation:
2016-09-06T03:14:03.619110: step 15100, loss 1.93184, acc 0.708255

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-15100

2016-09-06T03:14:05.532906: step 15101, loss 0.00868394, acc 1
2016-09-06T03:14:06.347333: step 15102, loss 0.02043, acc 1
2016-09-06T03:14:07.172127: step 15103, loss 0.026917, acc 0.98
2016-09-06T03:14:07.967819: step 15104, loss 0.00566888, acc 1
2016-09-06T03:14:08.777726: step 15105, loss 0.00503161, acc 1
2016-09-06T03:14:09.636589: step 15106, loss 0.0172399, acc 0.98
2016-09-06T03:14:10.443716: step 15107, loss 0.00436672, acc 1
2016-09-06T03:14:11.246911: step 15108, loss 0.00193518, acc 1
2016-09-06T03:14:12.069811: step 15109, loss 0.00277634, acc 1
2016-09-06T03:14:12.861740: step 15110, loss 0.00593999, acc 1
2016-09-06T03:14:13.680618: step 15111, loss 0.0192378, acc 1
2016-09-06T03:14:14.517544: step 15112, loss 0.00221335, acc 1
2016-09-06T03:14:15.303949: step 15113, loss 0.0114306, acc 1
2016-09-06T03:14:16.080795: step 15114, loss 0.0167177, acc 0.98
2016-09-06T03:14:16.913230: step 15115, loss 0.00200541, acc 1
2016-09-06T03:14:17.721778: step 15116, loss 0.00203492, acc 1
2016-09-06T03:14:18.531915: step 15117, loss 0.0187195, acc 0.98
2016-09-06T03:14:19.343979: step 15118, loss 0.00202612, acc 1
2016-09-06T03:14:20.159749: step 15119, loss 0.00227472, acc 1
2016-09-06T03:14:20.968696: step 15120, loss 0.0157722, acc 1
2016-09-06T03:14:21.801353: step 15121, loss 0.00199862, acc 1
2016-09-06T03:14:22.630456: step 15122, loss 0.0108196, acc 1
2016-09-06T03:14:23.469306: step 15123, loss 0.00256576, acc 1
2016-09-06T03:14:24.295005: step 15124, loss 0.00579957, acc 1
2016-09-06T03:14:25.136499: step 15125, loss 0.0267915, acc 0.98
2016-09-06T03:14:25.908277: step 15126, loss 0.0116925, acc 1
2016-09-06T03:14:26.728836: step 15127, loss 0.00197423, acc 1
2016-09-06T03:14:27.538428: step 15128, loss 0.0373366, acc 0.98
2016-09-06T03:14:28.317334: step 15129, loss 0.0163552, acc 1
2016-09-06T03:14:29.158354: step 15130, loss 0.00202153, acc 1
2016-09-06T03:14:29.982559: step 15131, loss 0.00235873, acc 1
2016-09-06T03:14:30.827784: step 15132, loss 0.020007, acc 0.98
2016-09-06T03:14:31.626262: step 15133, loss 0.0205967, acc 0.98
2016-09-06T03:14:32.431038: step 15134, loss 0.00233391, acc 1
2016-09-06T03:14:33.209697: step 15135, loss 0.00261273, acc 1
2016-09-06T03:14:34.031676: step 15136, loss 0.0747308, acc 0.98
2016-09-06T03:14:34.833531: step 15137, loss 0.0550703, acc 0.98
2016-09-06T03:14:35.614019: step 15138, loss 0.00546301, acc 1
2016-09-06T03:14:36.429479: step 15139, loss 0.00230371, acc 1
2016-09-06T03:14:37.282877: step 15140, loss 0.00172292, acc 1
2016-09-06T03:14:38.088271: step 15141, loss 0.039208, acc 0.98
2016-09-06T03:14:38.895673: step 15142, loss 0.0243297, acc 0.98
2016-09-06T03:14:39.728880: step 15143, loss 0.00231811, acc 1
2016-09-06T03:14:40.524931: step 15144, loss 0.00286285, acc 1
2016-09-06T03:14:41.359626: step 15145, loss 0.00524245, acc 1
2016-09-06T03:14:42.192831: step 15146, loss 0.00212805, acc 1
2016-09-06T03:14:42.993512: step 15147, loss 0.0412062, acc 0.98
2016-09-06T03:14:43.800082: step 15148, loss 0.0178311, acc 0.98
2016-09-06T03:14:44.641449: step 15149, loss 0.00187255, acc 1
2016-09-06T03:14:45.447578: step 15150, loss 0.00214943, acc 1
2016-09-06T03:14:46.248655: step 15151, loss 0.0438011, acc 0.98
2016-09-06T03:14:47.091295: step 15152, loss 0.00849024, acc 1
2016-09-06T03:14:47.934628: step 15153, loss 0.0535605, acc 0.96
2016-09-06T03:14:48.742035: step 15154, loss 0.0272816, acc 0.98
2016-09-06T03:14:49.562265: step 15155, loss 0.0174229, acc 0.98
2016-09-06T03:14:50.391525: step 15156, loss 0.0569764, acc 0.96
2016-09-06T03:14:51.185490: step 15157, loss 0.00162591, acc 1
2016-09-06T03:14:52.000485: step 15158, loss 0.00243018, acc 1
2016-09-06T03:14:52.813710: step 15159, loss 0.00159467, acc 1
2016-09-06T03:14:53.618834: step 15160, loss 0.00187949, acc 1
2016-09-06T03:14:54.429069: step 15161, loss 0.0112275, acc 1
2016-09-06T03:14:55.245095: step 15162, loss 0.0192473, acc 0.98
2016-09-06T03:14:56.057147: step 15163, loss 0.0121336, acc 1
2016-09-06T03:14:56.887359: step 15164, loss 0.0126849, acc 1
2016-09-06T03:14:57.678438: step 15165, loss 0.0168426, acc 1
2016-09-06T03:14:58.494584: step 15166, loss 0.00407115, acc 1
2016-09-06T03:14:59.314222: step 15167, loss 0.0105455, acc 1
2016-09-06T03:15:00.054749: step 15168, loss 0.024745, acc 0.977273
2016-09-06T03:15:00.933933: step 15169, loss 0.00835617, acc 1
2016-09-06T03:15:01.764752: step 15170, loss 0.0249931, acc 0.98
2016-09-06T03:15:02.592336: step 15171, loss 0.00184858, acc 1
2016-09-06T03:15:03.400117: step 15172, loss 0.0610082, acc 0.94
2016-09-06T03:15:04.201489: step 15173, loss 0.00188595, acc 1
2016-09-06T03:15:05.003325: step 15174, loss 0.00764546, acc 1
2016-09-06T03:15:05.780159: step 15175, loss 0.00197776, acc 1
2016-09-06T03:15:06.598824: step 15176, loss 0.0159998, acc 1
2016-09-06T03:15:07.402364: step 15177, loss 0.00227651, acc 1
2016-09-06T03:15:08.204565: step 15178, loss 0.00171263, acc 1
2016-09-06T03:15:09.017091: step 15179, loss 0.0223077, acc 0.98
2016-09-06T03:15:09.841710: step 15180, loss 0.00478706, acc 1
2016-09-06T03:15:10.638816: step 15181, loss 0.00203045, acc 1
2016-09-06T03:15:11.483521: step 15182, loss 0.00217797, acc 1
2016-09-06T03:15:12.289993: step 15183, loss 0.00181683, acc 1
2016-09-06T03:15:13.077370: step 15184, loss 0.0344829, acc 0.98
2016-09-06T03:15:13.885212: step 15185, loss 0.0112585, acc 1
2016-09-06T03:15:14.699785: step 15186, loss 0.00177049, acc 1
2016-09-06T03:15:15.488557: step 15187, loss 0.00231885, acc 1
2016-09-06T03:15:16.297928: step 15188, loss 0.0180089, acc 0.98
2016-09-06T03:15:17.120219: step 15189, loss 0.0045587, acc 1
2016-09-06T03:15:17.909654: step 15190, loss 0.036179, acc 0.98
2016-09-06T03:15:18.718616: step 15191, loss 0.00352649, acc 1
2016-09-06T03:15:19.524597: step 15192, loss 0.014885, acc 1
2016-09-06T03:15:20.309355: step 15193, loss 0.00211666, acc 1
2016-09-06T03:15:21.120732: step 15194, loss 0.0237594, acc 0.98
2016-09-06T03:15:21.944139: step 15195, loss 0.0281464, acc 0.98
2016-09-06T03:15:22.743198: step 15196, loss 0.00171356, acc 1
2016-09-06T03:15:23.549010: step 15197, loss 0.0592209, acc 0.98
2016-09-06T03:15:24.376251: step 15198, loss 0.00184298, acc 1
2016-09-06T03:15:25.133400: step 15199, loss 0.00282149, acc 1
2016-09-06T03:15:25.933417: step 15200, loss 0.0393889, acc 0.96

Evaluation:
2016-09-06T03:15:29.630938: step 15200, loss 2.79769, acc 0.695122

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-15200

2016-09-06T03:15:31.484985: step 15201, loss 0.00470113, acc 1
2016-09-06T03:15:32.273448: step 15202, loss 0.00339949, acc 1
2016-09-06T03:15:33.114599: step 15203, loss 0.00177793, acc 1
2016-09-06T03:15:33.916011: step 15204, loss 0.00791119, acc 1
2016-09-06T03:15:34.716356: step 15205, loss 0.00282502, acc 1
2016-09-06T03:15:35.525461: step 15206, loss 0.0135619, acc 1
2016-09-06T03:15:36.360701: step 15207, loss 0.0193789, acc 0.98
2016-09-06T03:15:37.159847: step 15208, loss 0.00157919, acc 1
2016-09-06T03:15:37.993678: step 15209, loss 0.00179923, acc 1
2016-09-06T03:15:38.798017: step 15210, loss 0.0109028, acc 1
2016-09-06T03:15:39.602127: step 15211, loss 0.0369621, acc 0.96
2016-09-06T03:15:40.444118: step 15212, loss 0.00641585, acc 1
2016-09-06T03:15:41.243533: step 15213, loss 0.00162695, acc 1
2016-09-06T03:15:42.051869: step 15214, loss 0.00364596, acc 1
2016-09-06T03:15:42.872871: step 15215, loss 0.0773254, acc 0.96
2016-09-06T03:15:43.699442: step 15216, loss 0.0017143, acc 1
2016-09-06T03:15:44.496490: step 15217, loss 0.0287776, acc 1
2016-09-06T03:15:45.319387: step 15218, loss 0.00168255, acc 1
2016-09-06T03:15:46.122289: step 15219, loss 0.00872878, acc 1
2016-09-06T03:15:46.936814: step 15220, loss 0.00677977, acc 1
2016-09-06T03:15:47.779144: step 15221, loss 0.0243631, acc 1
2016-09-06T03:15:48.584201: step 15222, loss 0.00795174, acc 1
2016-09-06T03:15:49.368490: step 15223, loss 0.00194131, acc 1
2016-09-06T03:15:50.220078: step 15224, loss 0.0108871, acc 1
2016-09-06T03:15:51.011542: step 15225, loss 0.00195141, acc 1
2016-09-06T03:15:51.795900: step 15226, loss 0.00319821, acc 1
2016-09-06T03:15:52.634770: step 15227, loss 0.00314368, acc 1
2016-09-06T03:15:53.434441: step 15228, loss 0.00209378, acc 1
2016-09-06T03:15:54.225461: step 15229, loss 0.0315602, acc 0.98
2016-09-06T03:15:55.069568: step 15230, loss 0.0267488, acc 0.98
2016-09-06T03:15:55.881458: step 15231, loss 0.00973138, acc 1
2016-09-06T03:15:56.681413: step 15232, loss 0.0463154, acc 0.98
2016-09-06T03:15:57.490201: step 15233, loss 0.016462, acc 0.98
2016-09-06T03:15:58.295095: step 15234, loss 0.00926683, acc 1
2016-09-06T03:15:59.087175: step 15235, loss 0.0200278, acc 0.98
2016-09-06T03:15:59.893026: step 15236, loss 0.0372186, acc 0.96
2016-09-06T03:16:00.766980: step 15237, loss 0.00798096, acc 1
2016-09-06T03:16:01.578866: step 15238, loss 0.00778024, acc 1
2016-09-06T03:16:02.389882: step 15239, loss 0.0085975, acc 1
2016-09-06T03:16:03.192762: step 15240, loss 0.00453633, acc 1
2016-09-06T03:16:03.955916: step 15241, loss 0.00304688, acc 1
2016-09-06T03:16:04.788139: step 15242, loss 0.00242915, acc 1
2016-09-06T03:16:05.621557: step 15243, loss 0.0480905, acc 0.96
2016-09-06T03:16:06.386264: step 15244, loss 0.0163152, acc 0.98
2016-09-06T03:16:07.189999: step 15245, loss 0.00834915, acc 1
2016-09-06T03:16:08.000065: step 15246, loss 0.00342647, acc 1
2016-09-06T03:16:08.800675: step 15247, loss 0.0145757, acc 1
2016-09-06T03:16:09.629504: step 15248, loss 0.0175079, acc 0.98
2016-09-06T03:16:10.458238: step 15249, loss 0.00224781, acc 1
2016-09-06T03:16:11.212881: step 15250, loss 0.00530878, acc 1
2016-09-06T03:16:12.016836: step 15251, loss 0.00300057, acc 1
2016-09-06T03:16:12.838456: step 15252, loss 0.00303198, acc 1
2016-09-06T03:16:13.642791: step 15253, loss 0.00309523, acc 1
2016-09-06T03:16:14.503827: step 15254, loss 0.00224979, acc 1
2016-09-06T03:16:15.327998: step 15255, loss 0.00277659, acc 1
2016-09-06T03:16:16.139867: step 15256, loss 0.00225607, acc 1
2016-09-06T03:16:16.937161: step 15257, loss 0.00700247, acc 1
2016-09-06T03:16:17.790204: step 15258, loss 0.00219934, acc 1
2016-09-06T03:16:18.587018: step 15259, loss 0.00869035, acc 1
2016-09-06T03:16:19.372237: step 15260, loss 0.0126981, acc 1
2016-09-06T03:16:20.205311: step 15261, loss 0.0114086, acc 1
2016-09-06T03:16:21.030191: step 15262, loss 0.00219877, acc 1
2016-09-06T03:16:21.790362: step 15263, loss 0.00506231, acc 1
2016-09-06T03:16:22.620369: step 15264, loss 0.00241658, acc 1
2016-09-06T03:16:23.406869: step 15265, loss 0.00231965, acc 1
2016-09-06T03:16:24.259331: step 15266, loss 0.00973453, acc 1
2016-09-06T03:16:25.128308: step 15267, loss 0.0124582, acc 1
2016-09-06T03:16:25.967632: step 15268, loss 0.00305581, acc 1
2016-09-06T03:16:26.800439: step 15269, loss 0.00230314, acc 1
2016-09-06T03:16:27.623466: step 15270, loss 0.00233282, acc 1
2016-09-06T03:16:28.446925: step 15271, loss 0.00784424, acc 1
2016-09-06T03:16:29.282090: step 15272, loss 0.00222622, acc 1
2016-09-06T03:16:30.124827: step 15273, loss 0.0141218, acc 1
2016-09-06T03:16:30.944657: step 15274, loss 0.00229595, acc 1
2016-09-06T03:16:31.756400: step 15275, loss 0.0208737, acc 1
2016-09-06T03:16:32.555483: step 15276, loss 0.00219435, acc 1
2016-09-06T03:16:33.367281: step 15277, loss 0.0168794, acc 1
2016-09-06T03:16:34.149004: step 15278, loss 0.0918226, acc 0.96
2016-09-06T03:16:34.947308: step 15279, loss 0.00212546, acc 1
2016-09-06T03:16:35.743872: step 15280, loss 0.0230206, acc 1
2016-09-06T03:16:36.543054: step 15281, loss 0.00209512, acc 1
2016-09-06T03:16:37.362718: step 15282, loss 0.046335, acc 0.98
2016-09-06T03:16:38.162724: step 15283, loss 0.0418994, acc 0.98
2016-09-06T03:16:38.949760: step 15284, loss 0.00198047, acc 1
2016-09-06T03:16:39.787637: step 15285, loss 0.00986446, acc 1
2016-09-06T03:16:40.628534: step 15286, loss 0.00193638, acc 1
2016-09-06T03:16:41.412437: step 15287, loss 0.0133033, acc 1
2016-09-06T03:16:42.236295: step 15288, loss 0.00822888, acc 1
2016-09-06T03:16:43.054072: step 15289, loss 0.00278408, acc 1
2016-09-06T03:16:43.858636: step 15290, loss 0.00189087, acc 1
2016-09-06T03:16:44.660153: step 15291, loss 0.0118798, acc 1
2016-09-06T03:16:45.500796: step 15292, loss 0.0166891, acc 0.98
2016-09-06T03:16:46.282261: step 15293, loss 0.0814269, acc 0.98
2016-09-06T03:16:47.092692: step 15294, loss 0.00182028, acc 1
2016-09-06T03:16:47.926781: step 15295, loss 0.0150214, acc 1
2016-09-06T03:16:48.728439: step 15296, loss 0.0558703, acc 0.96
2016-09-06T03:16:49.546817: step 15297, loss 0.00192748, acc 1
2016-09-06T03:16:50.353915: step 15298, loss 0.00336112, acc 1
2016-09-06T03:16:51.164229: step 15299, loss 0.00772025, acc 1
2016-09-06T03:16:51.970415: step 15300, loss 0.00330164, acc 1

Evaluation:
2016-09-06T03:16:55.737563: step 15300, loss 2.53283, acc 0.699812

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-15300

2016-09-06T03:16:57.657977: step 15301, loss 0.00277645, acc 1
2016-09-06T03:16:58.456333: step 15302, loss 0.0310674, acc 0.98
2016-09-06T03:16:59.257582: step 15303, loss 0.0125924, acc 1
2016-09-06T03:17:00.046156: step 15304, loss 0.0361167, acc 0.98
2016-09-06T03:17:00.884284: step 15305, loss 0.00572586, acc 1
2016-09-06T03:17:01.687745: step 15306, loss 0.00721735, acc 1
2016-09-06T03:17:02.537276: step 15307, loss 0.00168033, acc 1
2016-09-06T03:17:03.342844: step 15308, loss 0.00279389, acc 1
2016-09-06T03:17:04.145110: step 15309, loss 0.0272166, acc 1
2016-09-06T03:17:04.956132: step 15310, loss 0.00223814, acc 1
2016-09-06T03:17:05.728543: step 15311, loss 0.00167207, acc 1
2016-09-06T03:17:06.543852: step 15312, loss 0.00622894, acc 1
2016-09-06T03:17:07.350328: step 15313, loss 0.00304533, acc 1
2016-09-06T03:17:08.136247: step 15314, loss 0.0125042, acc 1
2016-09-06T03:17:08.933773: step 15315, loss 0.0208785, acc 0.98
2016-09-06T03:17:09.749921: step 15316, loss 0.00304325, acc 1
2016-09-06T03:17:10.527980: step 15317, loss 0.0247434, acc 0.98
2016-09-06T03:17:11.370820: step 15318, loss 0.0153825, acc 1
2016-09-06T03:17:12.172224: step 15319, loss 0.00172369, acc 1
2016-09-06T03:17:12.939260: step 15320, loss 0.00158749, acc 1
2016-09-06T03:17:13.762622: step 15321, loss 0.00422944, acc 1
2016-09-06T03:17:14.597446: step 15322, loss 0.00152685, acc 1
2016-09-06T03:17:15.383473: step 15323, loss 0.00153754, acc 1
2016-09-06T03:17:16.196647: step 15324, loss 0.00857309, acc 1
2016-09-06T03:17:17.030981: step 15325, loss 0.0147749, acc 1
2016-09-06T03:17:17.860927: step 15326, loss 0.00543071, acc 1
2016-09-06T03:17:18.686666: step 15327, loss 0.00899763, acc 1
2016-09-06T03:17:19.512062: step 15328, loss 0.0376756, acc 0.98
2016-09-06T03:17:20.310593: step 15329, loss 0.0442733, acc 0.96
2016-09-06T03:17:21.134926: step 15330, loss 0.00821712, acc 1
2016-09-06T03:17:21.947619: step 15331, loss 0.00918692, acc 1
2016-09-06T03:17:22.734881: step 15332, loss 0.00922196, acc 1
2016-09-06T03:17:23.529865: step 15333, loss 0.00241963, acc 1
2016-09-06T03:17:24.369907: step 15334, loss 0.019277, acc 1
2016-09-06T03:17:25.208184: step 15335, loss 0.0780081, acc 0.98
2016-09-06T03:17:26.018338: step 15336, loss 0.0258702, acc 0.98
2016-09-06T03:17:26.823005: step 15337, loss 0.0201901, acc 0.98
2016-09-06T03:17:27.645400: step 15338, loss 0.00489034, acc 1
2016-09-06T03:17:28.460012: step 15339, loss 0.0108214, acc 1
2016-09-06T03:17:29.280872: step 15340, loss 0.00209882, acc 1
2016-09-06T03:17:30.084177: step 15341, loss 0.0146514, acc 1
2016-09-06T03:17:30.907270: step 15342, loss 0.00660912, acc 1
2016-09-06T03:17:31.756531: step 15343, loss 0.0104834, acc 1
2016-09-06T03:17:32.573064: step 15344, loss 0.00180618, acc 1
2016-09-06T03:17:33.370829: step 15345, loss 0.00160964, acc 1
2016-09-06T03:17:34.204609: step 15346, loss 0.0124718, acc 1
2016-09-06T03:17:35.044809: step 15347, loss 0.0157866, acc 0.98
2016-09-06T03:17:35.866984: step 15348, loss 0.0133127, acc 1
2016-09-06T03:17:36.700855: step 15349, loss 0.00186926, acc 1
2016-09-06T03:17:37.548361: step 15350, loss 0.00621742, acc 1
2016-09-06T03:17:38.345706: step 15351, loss 0.00159048, acc 1
2016-09-06T03:17:39.187458: step 15352, loss 0.00177169, acc 1
2016-09-06T03:17:39.984248: step 15353, loss 0.0140732, acc 1
2016-09-06T03:17:40.746475: step 15354, loss 0.0197586, acc 0.98
2016-09-06T03:17:41.549223: step 15355, loss 0.0167264, acc 0.98
2016-09-06T03:17:42.370878: step 15356, loss 0.00511151, acc 1
2016-09-06T03:17:43.168333: step 15357, loss 0.0153894, acc 1
2016-09-06T03:17:44.021514: step 15358, loss 0.136426, acc 0.98
2016-09-06T03:17:44.842326: step 15359, loss 0.00267112, acc 1
2016-09-06T03:17:45.570174: step 15360, loss 0.0681107, acc 0.977273
2016-09-06T03:17:46.394682: step 15361, loss 0.0122405, acc 1
2016-09-06T03:17:47.233411: step 15362, loss 0.00621058, acc 1
2016-09-06T03:17:48.003143: step 15363, loss 0.00342795, acc 1
2016-09-06T03:17:48.800020: step 15364, loss 0.0207865, acc 1
2016-09-06T03:17:49.611087: step 15365, loss 0.0137604, acc 1
2016-09-06T03:17:50.412154: step 15366, loss 0.00329503, acc 1
2016-09-06T03:17:51.252672: step 15367, loss 0.00466508, acc 1
2016-09-06T03:17:52.054342: step 15368, loss 0.0113592, acc 1
2016-09-06T03:17:52.846807: step 15369, loss 0.0309904, acc 0.98
2016-09-06T03:17:53.641416: step 15370, loss 0.00201991, acc 1
2016-09-06T03:17:54.434576: step 15371, loss 0.0134496, acc 1
2016-09-06T03:17:55.235885: step 15372, loss 0.00472861, acc 1
2016-09-06T03:17:56.034141: step 15373, loss 0.00824358, acc 1
2016-09-06T03:17:56.838641: step 15374, loss 0.00361392, acc 1
2016-09-06T03:17:57.645526: step 15375, loss 0.00271628, acc 1
2016-09-06T03:17:58.462721: step 15376, loss 0.0275065, acc 0.98
2016-09-06T03:17:59.282269: step 15377, loss 0.00242361, acc 1
2016-09-06T03:18:00.056517: step 15378, loss 0.00238827, acc 1
2016-09-06T03:18:00.881300: step 15379, loss 0.00233636, acc 1
2016-09-06T03:18:01.693333: step 15380, loss 0.0231021, acc 0.98
2016-09-06T03:18:02.476303: step 15381, loss 0.0597826, acc 0.98
2016-09-06T03:18:03.285354: step 15382, loss 0.0240774, acc 1
2016-09-06T03:18:04.101402: step 15383, loss 0.00314273, acc 1
2016-09-06T03:18:04.875984: step 15384, loss 0.0053155, acc 1
2016-09-06T03:18:05.717917: step 15385, loss 0.0102425, acc 1
2016-09-06T03:18:06.536313: step 15386, loss 0.00874998, acc 1
2016-09-06T03:18:07.335402: step 15387, loss 0.0397326, acc 0.98
2016-09-06T03:18:08.138413: step 15388, loss 0.00661723, acc 1
2016-09-06T03:18:08.956981: step 15389, loss 0.00368742, acc 1
2016-09-06T03:18:09.756038: step 15390, loss 0.0787216, acc 0.98
2016-09-06T03:18:10.567306: step 15391, loss 0.00540128, acc 1
2016-09-06T03:18:11.362146: step 15392, loss 0.00251635, acc 1
2016-09-06T03:18:12.179075: step 15393, loss 0.00243866, acc 1
2016-09-06T03:18:12.971567: step 15394, loss 0.00265018, acc 1
2016-09-06T03:18:13.774585: step 15395, loss 0.00310545, acc 1
2016-09-06T03:18:14.594427: step 15396, loss 0.0318699, acc 0.98
2016-09-06T03:18:15.401126: step 15397, loss 0.0021603, acc 1
2016-09-06T03:18:16.217289: step 15398, loss 0.0173206, acc 1
2016-09-06T03:18:17.000875: step 15399, loss 0.0253038, acc 0.98
2016-09-06T03:18:17.830735: step 15400, loss 0.00231287, acc 1

Evaluation:
2016-09-06T03:18:21.526274: step 15400, loss 2.70411, acc 0.703565

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-15400

2016-09-06T03:18:23.374511: step 15401, loss 0.0113628, acc 1
2016-09-06T03:18:24.179564: step 15402, loss 0.027872, acc 0.98
2016-09-06T03:18:25.015815: step 15403, loss 0.0257192, acc 0.98
2016-09-06T03:18:25.830408: step 15404, loss 0.002085, acc 1
2016-09-06T03:18:26.652019: step 15405, loss 0.00208774, acc 1
2016-09-06T03:18:27.467685: step 15406, loss 0.0669566, acc 0.98
2016-09-06T03:18:28.287652: step 15407, loss 0.00203395, acc 1
2016-09-06T03:18:29.088845: step 15408, loss 0.0115567, acc 1
2016-09-06T03:18:29.905803: step 15409, loss 0.0216783, acc 0.98
2016-09-06T03:18:30.743321: step 15410, loss 0.00196089, acc 1
2016-09-06T03:18:31.559369: step 15411, loss 0.00319628, acc 1
2016-09-06T03:18:32.399088: step 15412, loss 0.0176817, acc 0.98
2016-09-06T03:18:33.215426: step 15413, loss 0.0238202, acc 0.98
2016-09-06T03:18:34.019806: step 15414, loss 0.003012, acc 1
2016-09-06T03:18:34.861112: step 15415, loss 0.031804, acc 0.98
2016-09-06T03:18:35.673036: step 15416, loss 0.0168109, acc 0.98
2016-09-06T03:18:36.475396: step 15417, loss 0.00208222, acc 1
2016-09-06T03:18:37.265179: step 15418, loss 0.0409069, acc 0.98
2016-09-06T03:18:38.084134: step 15419, loss 0.0177673, acc 1
2016-09-06T03:18:38.876749: step 15420, loss 0.0117672, acc 1
2016-09-06T03:18:39.689898: step 15421, loss 0.00530676, acc 1
2016-09-06T03:18:40.525233: step 15422, loss 0.00462248, acc 1
2016-09-06T03:18:41.300052: step 15423, loss 0.00200171, acc 1
2016-09-06T03:18:42.097429: step 15424, loss 0.0565905, acc 0.98
2016-09-06T03:18:42.920230: step 15425, loss 0.00202504, acc 1
2016-09-06T03:18:43.704622: step 15426, loss 0.0102658, acc 1
2016-09-06T03:18:44.502841: step 15427, loss 0.00319729, acc 1
2016-09-06T03:18:45.316932: step 15428, loss 0.0385304, acc 0.98
2016-09-06T03:18:46.101466: step 15429, loss 0.0130582, acc 1
2016-09-06T03:18:46.907431: step 15430, loss 0.00223935, acc 1
2016-09-06T03:18:47.708825: step 15431, loss 0.00358579, acc 1
2016-09-06T03:18:48.499111: step 15432, loss 0.00406252, acc 1
2016-09-06T03:18:49.332814: step 15433, loss 0.0285264, acc 0.98
2016-09-06T03:18:50.129693: step 15434, loss 0.00294191, acc 1
2016-09-06T03:18:50.944571: step 15435, loss 0.00801702, acc 1
2016-09-06T03:18:51.757314: step 15436, loss 0.00347006, acc 1
2016-09-06T03:18:52.567744: step 15437, loss 0.00363355, acc 1
2016-09-06T03:18:53.368497: step 15438, loss 0.0126357, acc 1
2016-09-06T03:18:54.177271: step 15439, loss 0.0691925, acc 0.96
2016-09-06T03:18:54.988459: step 15440, loss 0.0621466, acc 0.96
2016-09-06T03:18:55.772304: step 15441, loss 0.00202802, acc 1
2016-09-06T03:18:56.591115: step 15442, loss 0.0160741, acc 0.98
2016-09-06T03:18:57.415634: step 15443, loss 0.0163553, acc 1
2016-09-06T03:18:58.214299: step 15444, loss 0.00220657, acc 1
2016-09-06T03:18:59.047250: step 15445, loss 0.00393892, acc 1
2016-09-06T03:18:59.856068: step 15446, loss 0.0309053, acc 0.98
2016-09-06T03:19:00.677407: step 15447, loss 0.0301722, acc 0.98
2016-09-06T03:19:01.492800: step 15448, loss 0.0179541, acc 0.98
2016-09-06T03:19:02.306223: step 15449, loss 0.0336735, acc 0.98
2016-09-06T03:19:03.060138: step 15450, loss 0.00189762, acc 1
2016-09-06T03:19:03.851520: step 15451, loss 0.0345398, acc 0.98
2016-09-06T03:19:04.638779: step 15452, loss 0.00188675, acc 1
2016-09-06T03:19:05.428036: step 15453, loss 0.00300495, acc 1
2016-09-06T03:19:06.261112: step 15454, loss 0.00212419, acc 1
2016-09-06T03:19:07.085709: step 15455, loss 0.0130742, acc 1
2016-09-06T03:19:07.883261: step 15456, loss 0.00206556, acc 1
2016-09-06T03:19:08.676426: step 15457, loss 0.00195813, acc 1
2016-09-06T03:19:09.485652: step 15458, loss 0.0143389, acc 1
2016-09-06T03:19:10.257756: step 15459, loss 0.00190597, acc 1
2016-09-06T03:19:11.072962: step 15460, loss 0.0212825, acc 1
2016-09-06T03:19:11.872817: step 15461, loss 0.0470682, acc 0.98
2016-09-06T03:19:12.658684: step 15462, loss 0.0144801, acc 1
2016-09-06T03:19:13.478655: step 15463, loss 0.00276243, acc 1
2016-09-06T03:19:14.287042: step 15464, loss 0.00874362, acc 1
2016-09-06T03:19:15.062778: step 15465, loss 0.00869903, acc 1
2016-09-06T03:19:15.915065: step 15466, loss 0.0137871, acc 1
2016-09-06T03:19:16.769617: step 15467, loss 0.0606153, acc 0.96
2016-09-06T03:19:17.565159: step 15468, loss 0.0206305, acc 0.98
2016-09-06T03:19:18.369797: step 15469, loss 0.0344832, acc 0.96
2016-09-06T03:19:19.178098: step 15470, loss 0.00806108, acc 1
2016-09-06T03:19:19.983019: step 15471, loss 0.00178809, acc 1
2016-09-06T03:19:20.785559: step 15472, loss 0.0520295, acc 0.98
2016-09-06T03:19:21.600474: step 15473, loss 0.0130709, acc 1
2016-09-06T03:19:22.396482: step 15474, loss 0.00312326, acc 1
2016-09-06T03:19:23.221133: step 15475, loss 0.0061393, acc 1
2016-09-06T03:19:24.007879: step 15476, loss 0.0127332, acc 1
2016-09-06T03:19:24.791719: step 15477, loss 0.00197668, acc 1
2016-09-06T03:19:25.575382: step 15478, loss 0.0018145, acc 1
2016-09-06T03:19:26.417058: step 15479, loss 0.00735874, acc 1
2016-09-06T03:19:27.221416: step 15480, loss 0.0307189, acc 0.98
2016-09-06T03:19:28.012142: step 15481, loss 0.00239783, acc 1
2016-09-06T03:19:28.827858: step 15482, loss 0.00192314, acc 1
2016-09-06T03:19:29.691923: step 15483, loss 0.00341165, acc 1
2016-09-06T03:19:30.511639: step 15484, loss 0.00217448, acc 1
2016-09-06T03:19:31.354042: step 15485, loss 0.0083625, acc 1
2016-09-06T03:19:32.184175: step 15486, loss 0.00227791, acc 1
2016-09-06T03:19:33.023738: step 15487, loss 0.0228792, acc 0.98
2016-09-06T03:19:33.836485: step 15488, loss 0.00192014, acc 1
2016-09-06T03:19:34.638781: step 15489, loss 0.00556906, acc 1
2016-09-06T03:19:35.467921: step 15490, loss 0.00884252, acc 1
2016-09-06T03:19:36.306767: step 15491, loss 0.00236271, acc 1
2016-09-06T03:19:37.098639: step 15492, loss 0.00302567, acc 1
2016-09-06T03:19:37.911990: step 15493, loss 0.00201925, acc 1
2016-09-06T03:19:38.769717: step 15494, loss 0.0267676, acc 0.98
2016-09-06T03:19:39.639924: step 15495, loss 0.00768078, acc 1
2016-09-06T03:19:40.446299: step 15496, loss 0.00391922, acc 1
2016-09-06T03:19:41.266657: step 15497, loss 0.0537217, acc 0.98
2016-09-06T03:19:42.056609: step 15498, loss 0.0248333, acc 1
2016-09-06T03:19:42.864862: step 15499, loss 0.00186019, acc 1
2016-09-06T03:19:43.679301: step 15500, loss 0.00186755, acc 1

Evaluation:
2016-09-06T03:19:47.411811: step 15500, loss 2.95487, acc 0.699812

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-15500

2016-09-06T03:19:49.419241: step 15501, loss 0.00491458, acc 1
2016-09-06T03:19:50.204216: step 15502, loss 0.00285159, acc 1
2016-09-06T03:19:51.033638: step 15503, loss 0.00189571, acc 1
2016-09-06T03:19:51.870484: step 15504, loss 0.0136183, acc 1
2016-09-06T03:19:52.631463: step 15505, loss 0.00221681, acc 1
2016-09-06T03:19:53.426590: step 15506, loss 0.0131475, acc 1
2016-09-06T03:19:54.260010: step 15507, loss 0.0139884, acc 1
2016-09-06T03:19:55.043198: step 15508, loss 0.00486537, acc 1
2016-09-06T03:19:55.832062: step 15509, loss 0.220876, acc 0.94
2016-09-06T03:19:56.654334: step 15510, loss 0.00182797, acc 1
2016-09-06T03:19:57.451774: step 15511, loss 0.00163649, acc 1
2016-09-06T03:19:58.247493: step 15512, loss 0.00201735, acc 1
2016-09-06T03:19:59.052715: step 15513, loss 0.00739828, acc 1
2016-09-06T03:19:59.843604: step 15514, loss 0.00350181, acc 1
2016-09-06T03:20:00.653464: step 15515, loss 0.0797783, acc 0.96
2016-09-06T03:20:01.472454: step 15516, loss 0.0147545, acc 1
2016-09-06T03:20:02.281159: step 15517, loss 0.00461669, acc 1
2016-09-06T03:20:03.121255: step 15518, loss 0.0207729, acc 1
2016-09-06T03:20:03.931120: step 15519, loss 0.0163407, acc 1
2016-09-06T03:20:04.714092: step 15520, loss 0.0371215, acc 0.98
2016-09-06T03:20:05.524421: step 15521, loss 0.0220099, acc 1
2016-09-06T03:20:06.376838: step 15522, loss 0.0156986, acc 1
2016-09-06T03:20:07.174246: step 15523, loss 0.0211362, acc 1
2016-09-06T03:20:07.950885: step 15524, loss 0.00618547, acc 1
2016-09-06T03:20:08.777960: step 15525, loss 0.00209603, acc 1
2016-09-06T03:20:09.552080: step 15526, loss 0.0156839, acc 1
2016-09-06T03:20:10.363942: step 15527, loss 0.0159562, acc 1
2016-09-06T03:20:11.167947: step 15528, loss 0.0078215, acc 1
2016-09-06T03:20:11.975250: step 15529, loss 0.0236788, acc 1
2016-09-06T03:20:12.792425: step 15530, loss 0.109905, acc 0.98
2016-09-06T03:20:13.618809: step 15531, loss 0.00739595, acc 1
2016-09-06T03:20:14.413219: step 15532, loss 0.00188586, acc 1
2016-09-06T03:20:15.182314: step 15533, loss 0.0102796, acc 1
2016-09-06T03:20:15.988917: step 15534, loss 0.0164493, acc 1
2016-09-06T03:20:16.796049: step 15535, loss 0.0334008, acc 0.98
2016-09-06T03:20:17.600744: step 15536, loss 0.0128283, acc 1
2016-09-06T03:20:18.452463: step 15537, loss 0.00882705, acc 1
2016-09-06T03:20:19.282828: step 15538, loss 0.0368084, acc 0.98
2016-09-06T03:20:20.078399: step 15539, loss 0.0202287, acc 1
2016-09-06T03:20:21.076399: step 15540, loss 0.0348247, acc 0.98
2016-09-06T03:20:21.979645: step 15541, loss 0.00349725, acc 1
2016-09-06T03:20:22.814789: step 15542, loss 0.00351139, acc 1
2016-09-06T03:20:23.723503: step 15543, loss 0.0204017, acc 1
2016-09-06T03:20:24.624812: step 15544, loss 0.0277145, acc 0.98
2016-09-06T03:20:25.558412: step 15545, loss 0.00389071, acc 1
2016-09-06T03:20:26.543098: step 15546, loss 0.0445025, acc 0.98
2016-09-06T03:20:27.486398: step 15547, loss 0.0499699, acc 0.96
2016-09-06T03:20:28.325059: step 15548, loss 0.00411672, acc 1
2016-09-06T03:20:29.147188: step 15549, loss 0.0159866, acc 1
2016-09-06T03:20:29.974675: step 15550, loss 0.0243822, acc 0.98
2016-09-06T03:20:30.805619: step 15551, loss 0.00807355, acc 1
2016-09-06T03:20:31.574372: step 15552, loss 0.0474098, acc 0.954545
2016-09-06T03:20:32.549460: step 15553, loss 0.00449036, acc 1
2016-09-06T03:20:33.390684: step 15554, loss 0.130299, acc 0.96
2016-09-06T03:20:34.197797: step 15555, loss 0.00457245, acc 1
2016-09-06T03:20:35.100612: step 15556, loss 0.0291314, acc 0.98
2016-09-06T03:20:35.974554: step 15557, loss 0.00565247, acc 1
2016-09-06T03:20:36.828390: step 15558, loss 0.00449468, acc 1
2016-09-06T03:20:37.741417: step 15559, loss 0.0044854, acc 1
2016-09-06T03:20:38.557990: step 15560, loss 0.00447026, acc 1
2016-09-06T03:20:39.414182: step 15561, loss 0.00718995, acc 1
2016-09-06T03:20:40.381444: step 15562, loss 0.00530055, acc 1
2016-09-06T03:20:41.230759: step 15563, loss 0.446224, acc 0.98
2016-09-06T03:20:42.075126: step 15564, loss 0.0114204, acc 1
2016-09-06T03:20:42.961948: step 15565, loss 0.0039817, acc 1
2016-09-06T03:20:43.776854: step 15566, loss 0.00381469, acc 1
2016-09-06T03:20:44.565480: step 15567, loss 0.00417375, acc 1
2016-09-06T03:20:45.373480: step 15568, loss 0.0245765, acc 0.98
2016-09-06T03:20:46.197538: step 15569, loss 0.00531677, acc 1
2016-09-06T03:20:47.000629: step 15570, loss 0.00337648, acc 1
2016-09-06T03:20:47.801035: step 15571, loss 0.00456223, acc 1
2016-09-06T03:20:48.631217: step 15572, loss 0.190153, acc 0.96
2016-09-06T03:20:49.407217: step 15573, loss 0.00363314, acc 1
2016-09-06T03:20:50.192186: step 15574, loss 0.0157315, acc 1
2016-09-06T03:20:51.003091: step 15575, loss 0.0320104, acc 0.98
2016-09-06T03:20:51.807606: step 15576, loss 0.0549439, acc 0.98
2016-09-06T03:20:52.615418: step 15577, loss 0.00659569, acc 1
2016-09-06T03:20:53.444665: step 15578, loss 0.0288225, acc 1
2016-09-06T03:20:54.228232: step 15579, loss 0.0272257, acc 1
2016-09-06T03:20:55.022793: step 15580, loss 0.0226702, acc 1
2016-09-06T03:20:55.836544: step 15581, loss 0.0389418, acc 0.98
2016-09-06T03:20:56.610848: step 15582, loss 0.00771856, acc 1
2016-09-06T03:20:57.439274: step 15583, loss 0.00936272, acc 1
2016-09-06T03:20:58.241308: step 15584, loss 0.00986503, acc 1
2016-09-06T03:20:59.045493: step 15585, loss 0.0192824, acc 0.98
2016-09-06T03:20:59.863512: step 15586, loss 0.00821155, acc 1
2016-09-06T03:21:00.694767: step 15587, loss 0.00351887, acc 1
2016-09-06T03:21:01.480931: step 15588, loss 0.0116813, acc 1
2016-09-06T03:21:02.287686: step 15589, loss 0.0034535, acc 1
2016-09-06T03:21:03.141488: step 15590, loss 0.0499401, acc 0.96
2016-09-06T03:21:03.925831: step 15591, loss 0.0508354, acc 0.98
2016-09-06T03:21:04.717945: step 15592, loss 0.00534782, acc 1
2016-09-06T03:21:05.541132: step 15593, loss 0.0235386, acc 0.98
2016-09-06T03:21:06.369456: step 15594, loss 0.00346386, acc 1
2016-09-06T03:21:07.158102: step 15595, loss 0.0289656, acc 0.98
2016-09-06T03:21:07.980231: step 15596, loss 0.00349063, acc 1
2016-09-06T03:21:08.774942: step 15597, loss 0.0396909, acc 0.98
2016-09-06T03:21:09.584940: step 15598, loss 0.00648672, acc 1
2016-09-06T03:21:10.417756: step 15599, loss 0.0207412, acc 0.98
2016-09-06T03:21:11.226310: step 15600, loss 0.00770517, acc 1

Evaluation:
2016-09-06T03:21:14.948860: step 15600, loss 3.1276, acc 0.702627

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-15600

2016-09-06T03:21:16.899977: step 15601, loss 0.0127097, acc 1
2016-09-06T03:21:17.729093: step 15602, loss 0.00345203, acc 1
2016-09-06T03:21:18.522686: step 15603, loss 0.00351771, acc 1
2016-09-06T03:21:19.327002: step 15604, loss 0.00388505, acc 1
2016-09-06T03:21:20.146882: step 15605, loss 0.0216454, acc 1
2016-09-06T03:21:20.934541: step 15606, loss 0.00335148, acc 1
2016-09-06T03:21:21.732474: step 15607, loss 0.035556, acc 0.98
2016-09-06T03:21:22.547725: step 15608, loss 0.00416637, acc 1
2016-09-06T03:21:23.353768: step 15609, loss 0.00355665, acc 1
2016-09-06T03:21:24.148481: step 15610, loss 0.00441966, acc 1
2016-09-06T03:21:25.010402: step 15611, loss 0.00375371, acc 1
2016-09-06T03:21:25.809443: step 15612, loss 0.00310284, acc 1
2016-09-06T03:21:26.605820: step 15613, loss 0.00473702, acc 1
2016-09-06T03:21:27.438778: step 15614, loss 0.00786052, acc 1
2016-09-06T03:21:28.202045: step 15615, loss 0.00300342, acc 1
2016-09-06T03:21:29.003634: step 15616, loss 0.10272, acc 0.98
2016-09-06T03:21:29.817058: step 15617, loss 0.00656455, acc 1
2016-09-06T03:21:30.613921: step 15618, loss 0.0316433, acc 0.98
2016-09-06T03:21:31.478140: step 15619, loss 0.0355413, acc 0.96
2016-09-06T03:21:32.306406: step 15620, loss 0.00922377, acc 1
2016-09-06T03:21:33.115781: step 15621, loss 0.0245438, acc 0.98
2016-09-06T03:21:33.926360: step 15622, loss 0.0489572, acc 0.98
2016-09-06T03:21:34.747848: step 15623, loss 0.0242641, acc 1
2016-09-06T03:21:35.535384: step 15624, loss 0.0296022, acc 0.98
2016-09-06T03:21:36.351107: step 15625, loss 0.0147058, acc 1
2016-09-06T03:21:37.188506: step 15626, loss 0.00503794, acc 1
2016-09-06T03:21:38.039824: step 15627, loss 0.0177609, acc 0.98
2016-09-06T03:21:38.875799: step 15628, loss 0.0310426, acc 0.98
2016-09-06T03:21:39.723987: step 15629, loss 0.00629446, acc 1
2016-09-06T03:21:40.494214: step 15630, loss 0.142278, acc 0.98
2016-09-06T03:21:41.305985: step 15631, loss 0.120792, acc 0.98
2016-09-06T03:21:42.152465: step 15632, loss 0.00265004, acc 1
2016-09-06T03:21:42.966028: step 15633, loss 0.00283837, acc 1
2016-09-06T03:21:43.780677: step 15634, loss 0.00247684, acc 1
2016-09-06T03:21:44.602044: step 15635, loss 0.0624008, acc 0.98
2016-09-06T03:21:45.414030: step 15636, loss 0.0076728, acc 1
2016-09-06T03:21:46.238980: step 15637, loss 0.0160885, acc 1
2016-09-06T03:21:47.096210: step 15638, loss 0.0476803, acc 0.98
2016-09-06T03:21:47.920987: step 15639, loss 0.00589576, acc 1
2016-09-06T03:21:48.710031: step 15640, loss 0.00836776, acc 1
2016-09-06T03:21:49.518082: step 15641, loss 0.0146229, acc 1
2016-09-06T03:21:50.398743: step 15642, loss 0.00288156, acc 1
2016-09-06T03:21:51.164314: step 15643, loss 0.0312707, acc 0.98
2016-09-06T03:21:51.966412: step 15644, loss 0.00701486, acc 1
2016-09-06T03:21:52.801421: step 15645, loss 0.0209496, acc 1
2016-09-06T03:21:53.591858: step 15646, loss 0.00626766, acc 1
2016-09-06T03:21:54.392526: step 15647, loss 0.0282416, acc 0.98
2016-09-06T03:21:55.206450: step 15648, loss 0.00241107, acc 1
2016-09-06T03:21:55.993976: step 15649, loss 0.00385703, acc 1
2016-09-06T03:21:56.849592: step 15650, loss 0.0489644, acc 0.96
2016-09-06T03:21:57.659227: step 15651, loss 0.00981738, acc 1
2016-09-06T03:21:58.442376: step 15652, loss 0.00830289, acc 1
2016-09-06T03:21:59.269185: step 15653, loss 0.0395223, acc 0.96
2016-09-06T03:22:00.083026: step 15654, loss 0.0104522, acc 1
2016-09-06T03:22:00.899038: step 15655, loss 0.00770833, acc 1
2016-09-06T03:22:01.716427: step 15656, loss 0.00314024, acc 1
2016-09-06T03:22:02.535429: step 15657, loss 0.0167231, acc 1
2016-09-06T03:22:03.334853: step 15658, loss 0.00855014, acc 1
2016-09-06T03:22:04.169288: step 15659, loss 0.0215412, acc 0.98
2016-09-06T03:22:05.005841: step 15660, loss 0.02342, acc 1
2016-09-06T03:22:05.798591: step 15661, loss 0.0178759, acc 1
2016-09-06T03:22:06.571599: step 15662, loss 0.0557299, acc 0.96
2016-09-06T03:22:07.400033: step 15663, loss 0.00367527, acc 1
2016-09-06T03:22:08.204962: step 15664, loss 0.0510887, acc 0.98
2016-09-06T03:22:09.040891: step 15665, loss 0.0439344, acc 0.96
2016-09-06T03:22:09.861924: step 15666, loss 0.00348978, acc 1
2016-09-06T03:22:10.682875: step 15667, loss 0.023845, acc 0.98
2016-09-06T03:22:11.484541: step 15668, loss 0.00339925, acc 1
2016-09-06T03:22:12.297149: step 15669, loss 0.0232137, acc 0.98
2016-09-06T03:22:13.111612: step 15670, loss 0.00323166, acc 1
2016-09-06T03:22:13.928799: step 15671, loss 0.019294, acc 1
2016-09-06T03:22:14.766341: step 15672, loss 0.0257777, acc 0.98
2016-09-06T03:22:15.546648: step 15673, loss 0.00328047, acc 1
2016-09-06T03:22:16.363393: step 15674, loss 0.00566138, acc 1
2016-09-06T03:22:17.217093: step 15675, loss 0.00380026, acc 1
2016-09-06T03:22:18.073708: step 15676, loss 0.0158812, acc 1
2016-09-06T03:22:18.876027: step 15677, loss 0.035147, acc 0.98
2016-09-06T03:22:19.736268: step 15678, loss 0.00342439, acc 1
2016-09-06T03:22:20.652412: step 15679, loss 0.0237199, acc 0.98
2016-09-06T03:22:21.461081: step 15680, loss 0.00459207, acc 1
2016-09-06T03:22:22.270031: step 15681, loss 0.0062398, acc 1
2016-09-06T03:22:23.092731: step 15682, loss 0.00618274, acc 1
2016-09-06T03:22:23.881970: step 15683, loss 0.00314504, acc 1
2016-09-06T03:22:24.687641: step 15684, loss 0.0174177, acc 0.98
2016-09-06T03:22:25.511875: step 15685, loss 0.0500905, acc 0.98
2016-09-06T03:22:26.331056: step 15686, loss 0.0218152, acc 1
2016-09-06T03:22:27.149815: step 15687, loss 0.00314613, acc 1
2016-09-06T03:22:28.004700: step 15688, loss 0.0131926, acc 1
2016-09-06T03:22:28.827367: step 15689, loss 0.0126579, acc 1
2016-09-06T03:22:29.637797: step 15690, loss 0.0451293, acc 0.98
2016-09-06T03:22:30.491837: step 15691, loss 0.00309399, acc 1
2016-09-06T03:22:31.323533: step 15692, loss 0.00296598, acc 1
2016-09-06T03:22:32.139931: step 15693, loss 0.0238548, acc 1
2016-09-06T03:22:32.986245: step 15694, loss 0.0394266, acc 0.98
2016-09-06T03:22:33.782855: step 15695, loss 0.00710122, acc 1
2016-09-06T03:22:34.603459: step 15696, loss 0.0824441, acc 0.94
2016-09-06T03:22:35.440736: step 15697, loss 0.00440113, acc 1
2016-09-06T03:22:36.253164: step 15698, loss 0.0155431, acc 1
2016-09-06T03:22:37.045067: step 15699, loss 0.013431, acc 1
2016-09-06T03:22:37.876088: step 15700, loss 0.0584907, acc 0.98

Evaluation:
2016-09-06T03:22:41.606307: step 15700, loss 2.80236, acc 0.699812

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-15700

2016-09-06T03:22:43.532512: step 15701, loss 0.0117704, acc 1
2016-09-06T03:22:44.349744: step 15702, loss 0.0275023, acc 0.98
2016-09-06T03:22:45.179447: step 15703, loss 0.00288772, acc 1
2016-09-06T03:22:46.003802: step 15704, loss 0.00622202, acc 1
2016-09-06T03:22:46.824811: step 15705, loss 0.00475779, acc 1
2016-09-06T03:22:47.652764: step 15706, loss 0.00352224, acc 1
2016-09-06T03:22:48.460653: step 15707, loss 0.0248988, acc 0.98
2016-09-06T03:22:49.284888: step 15708, loss 0.0241986, acc 1
2016-09-06T03:22:50.103953: step 15709, loss 0.00363566, acc 1
2016-09-06T03:22:50.906559: step 15710, loss 0.0184478, acc 1
2016-09-06T03:22:51.686748: step 15711, loss 0.0165296, acc 1
2016-09-06T03:22:52.508827: step 15712, loss 0.121699, acc 0.98
2016-09-06T03:22:53.296678: step 15713, loss 0.0376083, acc 0.98
2016-09-06T03:22:54.115880: step 15714, loss 0.0170818, acc 1
2016-09-06T03:22:54.944687: step 15715, loss 0.0390119, acc 0.96
2016-09-06T03:22:55.764750: step 15716, loss 0.00261864, acc 1
2016-09-06T03:22:56.550386: step 15717, loss 0.00587182, acc 1
2016-09-06T03:22:57.348937: step 15718, loss 0.0321312, acc 0.96
2016-09-06T03:22:58.167664: step 15719, loss 0.0038352, acc 1
2016-09-06T03:22:58.949368: step 15720, loss 0.0130533, acc 1
2016-09-06T03:22:59.769478: step 15721, loss 0.0500555, acc 1
2016-09-06T03:23:00.721071: step 15722, loss 0.0104244, acc 1
2016-09-06T03:23:01.602215: step 15723, loss 0.00542288, acc 1
2016-09-06T03:23:02.411947: step 15724, loss 0.00285921, acc 1
2016-09-06T03:23:03.232928: step 15725, loss 0.00501152, acc 1
2016-09-06T03:23:04.064711: step 15726, loss 0.00374777, acc 1
2016-09-06T03:23:04.926463: step 15727, loss 0.014435, acc 1
2016-09-06T03:23:05.808323: step 15728, loss 0.00301374, acc 1
2016-09-06T03:23:06.709931: step 15729, loss 0.0172548, acc 1
2016-09-06T03:23:07.588154: step 15730, loss 0.0650039, acc 0.96
2016-09-06T03:23:08.475379: step 15731, loss 0.115768, acc 0.96
2016-09-06T03:23:09.333540: step 15732, loss 0.00863604, acc 1
2016-09-06T03:23:10.261389: step 15733, loss 0.00290989, acc 1
2016-09-06T03:23:11.179830: step 15734, loss 0.0072598, acc 1
2016-09-06T03:23:12.005440: step 15735, loss 0.0029753, acc 1
2016-09-06T03:23:12.849925: step 15736, loss 0.00324011, acc 1
2016-09-06T03:23:13.658714: step 15737, loss 0.00378065, acc 1
2016-09-06T03:23:14.458946: step 15738, loss 0.0153952, acc 1
2016-09-06T03:23:15.295166: step 15739, loss 0.0149671, acc 1
2016-09-06T03:23:16.134172: step 15740, loss 0.00278929, acc 1
2016-09-06T03:23:16.942453: step 15741, loss 0.043392, acc 0.98
2016-09-06T03:23:17.789558: step 15742, loss 0.0250771, acc 0.98
2016-09-06T03:23:18.620966: step 15743, loss 0.0048312, acc 1
2016-09-06T03:23:19.408377: step 15744, loss 0.00273468, acc 1
2016-09-06T03:23:20.240974: step 15745, loss 0.0266761, acc 0.98
2016-09-06T03:23:21.062922: step 15746, loss 0.0122235, acc 1
2016-09-06T03:23:21.895433: step 15747, loss 0.00283005, acc 1
2016-09-06T03:23:22.723429: step 15748, loss 0.00846995, acc 1
2016-09-06T03:23:23.501448: step 15749, loss 0.0026665, acc 1
2016-09-06T03:23:24.278490: step 15750, loss 0.00616598, acc 1
2016-09-06T03:23:25.116404: step 15751, loss 0.0290468, acc 0.98
2016-09-06T03:23:25.941219: step 15752, loss 0.0178887, acc 0.98
2016-09-06T03:23:26.746769: step 15753, loss 0.0784254, acc 0.98
2016-09-06T03:23:27.554348: step 15754, loss 0.00272594, acc 1
2016-09-06T03:23:28.362969: step 15755, loss 0.00415772, acc 1
2016-09-06T03:23:29.161860: step 15756, loss 0.00430372, acc 1
2016-09-06T03:23:29.963704: step 15757, loss 0.0117728, acc 1
2016-09-06T03:23:30.765510: step 15758, loss 0.00239583, acc 1
2016-09-06T03:23:31.581899: step 15759, loss 0.00246885, acc 1
2016-09-06T03:23:32.423743: step 15760, loss 0.0307836, acc 0.98
2016-09-06T03:23:33.247263: step 15761, loss 0.00231243, acc 1
2016-09-06T03:23:34.019675: step 15762, loss 0.00229171, acc 1
2016-09-06T03:23:34.821875: step 15763, loss 0.00351076, acc 1
2016-09-06T03:23:35.644184: step 15764, loss 0.0203007, acc 0.98
2016-09-06T03:23:36.431550: step 15765, loss 0.00301205, acc 1
2016-09-06T03:23:37.231067: step 15766, loss 0.0160717, acc 1
2016-09-06T03:23:38.097096: step 15767, loss 0.00799742, acc 1
2016-09-06T03:23:38.899104: step 15768, loss 0.00378336, acc 1
2016-09-06T03:23:39.711080: step 15769, loss 0.00818625, acc 1
2016-09-06T03:23:40.513980: step 15770, loss 0.013173, acc 1
2016-09-06T03:23:41.300940: step 15771, loss 0.0113589, acc 1
2016-09-06T03:23:42.091719: step 15772, loss 0.00243982, acc 1
2016-09-06T03:23:42.928992: step 15773, loss 0.0125513, acc 1
2016-09-06T03:23:43.734615: step 15774, loss 0.00222162, acc 1
2016-09-06T03:23:44.525667: step 15775, loss 0.0060735, acc 1
2016-09-06T03:23:45.317129: step 15776, loss 0.0265602, acc 0.98
2016-09-06T03:23:46.097318: step 15777, loss 0.00887334, acc 1
2016-09-06T03:23:46.897630: step 15778, loss 0.0600274, acc 0.96
2016-09-06T03:23:47.721597: step 15779, loss 0.00216807, acc 1
2016-09-06T03:23:48.509541: step 15780, loss 0.00277573, acc 1
2016-09-06T03:23:49.324679: step 15781, loss 0.00236601, acc 1
2016-09-06T03:23:50.126565: step 15782, loss 0.0352463, acc 0.98
2016-09-06T03:23:50.924568: step 15783, loss 0.0242538, acc 0.98
2016-09-06T03:23:51.745140: step 15784, loss 0.00231047, acc 1
2016-09-06T03:23:52.548931: step 15785, loss 0.00870587, acc 1
2016-09-06T03:23:53.348670: step 15786, loss 0.0142876, acc 1
2016-09-06T03:23:54.154740: step 15787, loss 0.0145475, acc 1
2016-09-06T03:23:54.972505: step 15788, loss 0.0209155, acc 0.98
2016-09-06T03:23:55.792371: step 15789, loss 0.00430913, acc 1
2016-09-06T03:23:56.590161: step 15790, loss 0.00209881, acc 1
2016-09-06T03:23:57.414628: step 15791, loss 0.00364654, acc 1
2016-09-06T03:23:58.220891: step 15792, loss 0.00682304, acc 1
2016-09-06T03:23:59.020427: step 15793, loss 0.00244842, acc 1
2016-09-06T03:23:59.810988: step 15794, loss 0.00217908, acc 1
2016-09-06T03:24:00.622270: step 15795, loss 0.00192218, acc 1
2016-09-06T03:24:01.435632: step 15796, loss 0.00877903, acc 1
2016-09-06T03:24:02.269444: step 15797, loss 0.0313869, acc 0.98
2016-09-06T03:24:03.055340: step 15798, loss 0.0325173, acc 0.98
2016-09-06T03:24:03.874626: step 15799, loss 0.00186886, acc 1
2016-09-06T03:24:04.706230: step 15800, loss 0.0160303, acc 1

Evaluation:
2016-09-06T03:24:08.403693: step 15800, loss 2.78563, acc 0.704503

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-15800

2016-09-06T03:24:10.232380: step 15801, loss 0.00699664, acc 1
2016-09-06T03:24:11.067778: step 15802, loss 0.0302754, acc 0.98
2016-09-06T03:24:11.881284: step 15803, loss 0.00370391, acc 1
2016-09-06T03:24:12.715189: step 15804, loss 0.00291173, acc 1
2016-09-06T03:24:13.545061: step 15805, loss 0.0039743, acc 1
2016-09-06T03:24:14.366299: step 15806, loss 0.00179005, acc 1
2016-09-06T03:24:15.147668: step 15807, loss 0.00177433, acc 1
2016-09-06T03:24:15.969194: step 15808, loss 0.0913838, acc 0.96
2016-09-06T03:24:16.780073: step 15809, loss 0.00344284, acc 1
2016-09-06T03:24:17.538794: step 15810, loss 0.00567437, acc 1
2016-09-06T03:24:18.325546: step 15811, loss 0.0209637, acc 0.98
2016-09-06T03:24:19.182923: step 15812, loss 0.0309515, acc 0.98
2016-09-06T03:24:19.993487: step 15813, loss 0.0287483, acc 0.98
2016-09-06T03:24:20.833858: step 15814, loss 0.00375423, acc 1
2016-09-06T03:24:21.676667: step 15815, loss 0.0192808, acc 0.98
2016-09-06T03:24:22.478268: step 15816, loss 0.00344287, acc 1
2016-09-06T03:24:23.282807: step 15817, loss 0.0103035, acc 1
2016-09-06T03:24:24.095551: step 15818, loss 0.0153498, acc 1
2016-09-06T03:24:24.957586: step 15819, loss 0.029887, acc 1
2016-09-06T03:24:25.796262: step 15820, loss 0.0245641, acc 0.98
2016-09-06T03:24:26.670991: step 15821, loss 0.00299304, acc 1
2016-09-06T03:24:27.564976: step 15822, loss 0.0101621, acc 1
2016-09-06T03:24:28.394669: step 15823, loss 0.0145283, acc 1
2016-09-06T03:24:29.251619: step 15824, loss 0.0970036, acc 0.98
2016-09-06T03:24:30.083696: step 15825, loss 0.00554254, acc 1
2016-09-06T03:24:30.911540: step 15826, loss 0.00417006, acc 1
2016-09-06T03:24:31.857076: step 15827, loss 0.0481005, acc 0.98
2016-09-06T03:24:32.719982: step 15828, loss 0.00339924, acc 1
2016-09-06T03:24:33.560837: step 15829, loss 0.00518989, acc 1
2016-09-06T03:24:34.481203: step 15830, loss 0.00349833, acc 1
2016-09-06T03:24:35.301873: step 15831, loss 0.00334377, acc 1
2016-09-06T03:24:36.132808: step 15832, loss 0.00356817, acc 1
2016-09-06T03:24:36.900609: step 15833, loss 0.0129366, acc 1
2016-09-06T03:24:37.760884: step 15834, loss 0.00334654, acc 1
2016-09-06T03:24:38.598343: step 15835, loss 0.00615762, acc 1
2016-09-06T03:24:39.399751: step 15836, loss 0.00336519, acc 1
2016-09-06T03:24:40.211446: step 15837, loss 0.127236, acc 0.98
2016-09-06T03:24:41.022066: step 15838, loss 0.0158852, acc 1
2016-09-06T03:24:41.837590: step 15839, loss 0.00325423, acc 1
2016-09-06T03:24:42.654329: step 15840, loss 0.00644821, acc 1
2016-09-06T03:24:43.502009: step 15841, loss 0.00317781, acc 1
2016-09-06T03:24:44.325673: step 15842, loss 0.0357394, acc 0.96
2016-09-06T03:24:45.126225: step 15843, loss 0.00621664, acc 1
2016-09-06T03:24:45.947761: step 15844, loss 0.0175482, acc 0.98
2016-09-06T03:24:46.763249: step 15845, loss 0.0135425, acc 1
2016-09-06T03:24:47.588553: step 15846, loss 0.00747674, acc 1
2016-09-06T03:24:48.447984: step 15847, loss 0.00307183, acc 1
2016-09-06T03:24:49.280297: step 15848, loss 0.00565684, acc 1
2016-09-06T03:24:50.084391: step 15849, loss 0.0216586, acc 1
2016-09-06T03:24:50.935461: step 15850, loss 0.00353373, acc 1
2016-09-06T03:24:51.733989: step 15851, loss 0.0196397, acc 1
2016-09-06T03:24:52.549507: step 15852, loss 0.00709108, acc 1
2016-09-06T03:24:53.369384: step 15853, loss 0.108602, acc 0.96
2016-09-06T03:24:54.208735: step 15854, loss 0.00903869, acc 1
2016-09-06T03:24:55.021157: step 15855, loss 0.00315207, acc 1
2016-09-06T03:24:55.872538: step 15856, loss 0.0277115, acc 0.98
2016-09-06T03:24:56.686358: step 15857, loss 0.0535459, acc 0.98
2016-09-06T03:24:57.450239: step 15858, loss 0.00273369, acc 1
2016-09-06T03:24:58.256046: step 15859, loss 0.0291881, acc 0.98
2016-09-06T03:24:59.052035: step 15860, loss 0.124419, acc 0.98
2016-09-06T03:24:59.849441: step 15861, loss 0.0138009, acc 1
2016-09-06T03:25:00.715196: step 15862, loss 0.00379214, acc 1
2016-09-06T03:25:01.539950: step 15863, loss 0.00344357, acc 1
2016-09-06T03:25:02.330218: step 15864, loss 0.0506343, acc 0.98
2016-09-06T03:25:03.123157: step 15865, loss 0.0123533, acc 1
2016-09-06T03:25:03.947039: step 15866, loss 0.00246492, acc 1
2016-09-06T03:25:04.753317: step 15867, loss 0.00400944, acc 1
2016-09-06T03:25:05.566358: step 15868, loss 0.00739449, acc 1
2016-09-06T03:25:06.395545: step 15869, loss 0.0502908, acc 0.96
2016-09-06T03:25:07.183884: step 15870, loss 0.00534937, acc 1
2016-09-06T03:25:07.989032: step 15871, loss 0.0146473, acc 1
2016-09-06T03:25:08.795701: step 15872, loss 0.0420331, acc 0.98
2016-09-06T03:25:09.589290: step 15873, loss 0.0144701, acc 1
2016-09-06T03:25:10.396646: step 15874, loss 0.015355, acc 1
2016-09-06T03:25:11.201085: step 15875, loss 0.0306199, acc 0.98
2016-09-06T03:25:12.003074: step 15876, loss 0.0087272, acc 1
2016-09-06T03:25:12.811925: step 15877, loss 0.00521776, acc 1
2016-09-06T03:25:13.603934: step 15878, loss 0.0253424, acc 0.98
2016-09-06T03:25:14.375837: step 15879, loss 0.0246082, acc 1
2016-09-06T03:25:15.140642: step 15880, loss 0.018507, acc 0.98
2016-09-06T03:25:15.963509: step 15881, loss 0.00662998, acc 1
2016-09-06T03:25:16.763378: step 15882, loss 0.0273124, acc 0.98
2016-09-06T03:25:17.568885: step 15883, loss 0.00205025, acc 1
2016-09-06T03:25:18.402358: step 15884, loss 0.0135663, acc 1
2016-09-06T03:25:19.205485: step 15885, loss 0.0179559, acc 1
2016-09-06T03:25:20.002817: step 15886, loss 0.00640933, acc 1
2016-09-06T03:25:20.838984: step 15887, loss 0.0049353, acc 1
2016-09-06T03:25:21.613483: step 15888, loss 0.040167, acc 0.98
2016-09-06T03:25:22.419830: step 15889, loss 0.016461, acc 0.98
2016-09-06T03:25:23.252749: step 15890, loss 0.0284313, acc 1
2016-09-06T03:25:24.045439: step 15891, loss 0.00350188, acc 1
2016-09-06T03:25:24.868914: step 15892, loss 0.00213526, acc 1
2016-09-06T03:25:25.689123: step 15893, loss 0.00820257, acc 1
2016-09-06T03:25:26.479836: step 15894, loss 0.0361258, acc 0.98
2016-09-06T03:25:27.273407: step 15895, loss 0.00202566, acc 1
2016-09-06T03:25:28.104919: step 15896, loss 0.00536185, acc 1
2016-09-06T03:25:28.881431: step 15897, loss 0.00867189, acc 1
2016-09-06T03:25:29.693912: step 15898, loss 0.00952009, acc 1
2016-09-06T03:25:30.527810: step 15899, loss 0.00245783, acc 1
2016-09-06T03:25:31.325327: step 15900, loss 0.0678378, acc 0.98

Evaluation:
2016-09-06T03:25:35.045798: step 15900, loss 2.64908, acc 0.70075

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-15900

2016-09-06T03:25:36.956798: step 15901, loss 0.0333078, acc 0.98
2016-09-06T03:25:37.782835: step 15902, loss 0.0147439, acc 1
2016-09-06T03:25:38.542444: step 15903, loss 0.0218643, acc 0.98
2016-09-06T03:25:39.366524: step 15904, loss 0.0931026, acc 0.96
2016-09-06T03:25:40.187769: step 15905, loss 0.0336774, acc 0.96
2016-09-06T03:25:40.995680: step 15906, loss 0.0152206, acc 1
2016-09-06T03:25:41.805398: step 15907, loss 0.00255319, acc 1
2016-09-06T03:25:42.604684: step 15908, loss 0.00208305, acc 1
2016-09-06T03:25:43.390183: step 15909, loss 0.00203139, acc 1
2016-09-06T03:25:44.199447: step 15910, loss 0.0161548, acc 1
2016-09-06T03:25:45.044675: step 15911, loss 0.00209856, acc 1
2016-09-06T03:25:45.840954: step 15912, loss 0.00478933, acc 1
2016-09-06T03:25:46.645490: step 15913, loss 0.0446047, acc 0.98
2016-09-06T03:25:47.443041: step 15914, loss 0.00279015, acc 1
2016-09-06T03:25:48.224506: step 15915, loss 0.0298625, acc 0.98
2016-09-06T03:25:49.038216: step 15916, loss 0.0187058, acc 1
2016-09-06T03:25:49.837080: step 15917, loss 0.00185729, acc 1
2016-09-06T03:25:50.624753: step 15918, loss 0.00809432, acc 1
2016-09-06T03:25:51.432164: step 15919, loss 0.0364268, acc 0.98
2016-09-06T03:25:52.253913: step 15920, loss 0.00219076, acc 1
2016-09-06T03:25:53.077151: step 15921, loss 0.00452082, acc 1
2016-09-06T03:25:53.881400: step 15922, loss 0.0296812, acc 0.98
2016-09-06T03:25:54.709629: step 15923, loss 0.0144676, acc 1
2016-09-06T03:25:55.510641: step 15924, loss 0.0360563, acc 0.98
2016-09-06T03:25:56.307105: step 15925, loss 0.00261736, acc 1
2016-09-06T03:25:57.133830: step 15926, loss 0.00204524, acc 1
2016-09-06T03:25:57.920427: step 15927, loss 0.00409932, acc 1
2016-09-06T03:25:58.733289: step 15928, loss 0.00220082, acc 1
2016-09-06T03:25:59.545724: step 15929, loss 0.0153244, acc 1
2016-09-06T03:26:00.340226: step 15930, loss 0.0837197, acc 0.98
2016-09-06T03:26:01.104892: step 15931, loss 0.0227391, acc 1
2016-09-06T03:26:01.918284: step 15932, loss 0.00447281, acc 1
2016-09-06T03:26:02.698318: step 15933, loss 0.0140078, acc 1
2016-09-06T03:26:03.508812: step 15934, loss 0.00346133, acc 1
2016-09-06T03:26:04.348348: step 15935, loss 0.00815293, acc 1
2016-09-06T03:26:05.067467: step 15936, loss 0.00208278, acc 1
2016-09-06T03:26:05.886601: step 15937, loss 0.00211241, acc 1
2016-09-06T03:26:06.729802: step 15938, loss 0.0046777, acc 1
2016-09-06T03:26:07.525148: step 15939, loss 0.00396059, acc 1
2016-09-06T03:26:08.325178: step 15940, loss 0.0838765, acc 0.98
2016-09-06T03:26:09.126440: step 15941, loss 0.00640002, acc 1
2016-09-06T03:26:09.904323: step 15942, loss 0.0205383, acc 0.98
2016-09-06T03:26:10.721182: step 15943, loss 0.00200103, acc 1
2016-09-06T03:26:11.545086: step 15944, loss 0.0281815, acc 0.98
2016-09-06T03:26:12.356771: step 15945, loss 0.00695349, acc 1
2016-09-06T03:26:13.158897: step 15946, loss 0.00176926, acc 1
2016-09-06T03:26:13.957507: step 15947, loss 0.00930639, acc 1
2016-09-06T03:26:14.769891: step 15948, loss 0.0139176, acc 1
2016-09-06T03:26:15.565631: step 15949, loss 0.00224509, acc 1
2016-09-06T03:26:16.409581: step 15950, loss 0.00626835, acc 1
2016-09-06T03:26:17.219132: step 15951, loss 0.00170517, acc 1
2016-09-06T03:26:18.019490: step 15952, loss 0.0319378, acc 1
2016-09-06T03:26:18.839744: step 15953, loss 0.00664849, acc 1
2016-09-06T03:26:19.636142: step 15954, loss 0.00896205, acc 1
2016-09-06T03:26:20.433609: step 15955, loss 0.00188457, acc 1
2016-09-06T03:26:21.261786: step 15956, loss 0.0107889, acc 1
2016-09-06T03:26:22.049364: step 15957, loss 0.00172151, acc 1
2016-09-06T03:26:22.869259: step 15958, loss 0.00373921, acc 1
2016-09-06T03:26:23.668501: step 15959, loss 0.0113622, acc 1
2016-09-06T03:26:24.440654: step 15960, loss 0.00314409, acc 1
2016-09-06T03:26:25.230372: step 15961, loss 0.0449743, acc 1
2016-09-06T03:26:26.039223: step 15962, loss 0.0154263, acc 1
2016-09-06T03:26:26.854999: step 15963, loss 0.00875528, acc 1
2016-09-06T03:26:27.692704: step 15964, loss 0.00213142, acc 1
2016-09-06T03:26:28.525550: step 15965, loss 0.00863578, acc 1
2016-09-06T03:26:29.305148: step 15966, loss 0.00251781, acc 1
2016-09-06T03:26:30.111516: step 15967, loss 0.0024682, acc 1
2016-09-06T03:26:30.923054: step 15968, loss 0.00227368, acc 1
2016-09-06T03:26:31.743009: step 15969, loss 0.0399682, acc 0.98
2016-09-06T03:26:32.576474: step 15970, loss 0.0300653, acc 0.98
2016-09-06T03:26:33.468815: step 15971, loss 0.00192708, acc 1
2016-09-06T03:26:34.285560: step 15972, loss 0.0218579, acc 1
2016-09-06T03:26:35.120125: step 15973, loss 0.00205542, acc 1
2016-09-06T03:26:35.943716: step 15974, loss 0.0155468, acc 1
2016-09-06T03:26:36.742851: step 15975, loss 0.0178706, acc 1
2016-09-06T03:26:37.532035: step 15976, loss 0.00188086, acc 1
2016-09-06T03:26:38.372752: step 15977, loss 0.00301481, acc 1
2016-09-06T03:26:39.183717: step 15978, loss 0.0191925, acc 1
2016-09-06T03:26:40.016114: step 15979, loss 0.0353461, acc 0.98
2016-09-06T03:26:40.842688: step 15980, loss 0.00238226, acc 1
2016-09-06T03:26:41.648756: step 15981, loss 0.0189457, acc 0.98
2016-09-06T03:26:42.472661: step 15982, loss 0.0129179, acc 1
2016-09-06T03:26:43.300766: step 15983, loss 0.0067479, acc 1
2016-09-06T03:26:44.126328: step 15984, loss 0.00405192, acc 1
2016-09-06T03:26:44.912224: step 15985, loss 0.0151914, acc 1
2016-09-06T03:26:45.776824: step 15986, loss 0.00196016, acc 1
2016-09-06T03:26:46.597597: step 15987, loss 0.00188827, acc 1
2016-09-06T03:26:47.369608: step 15988, loss 0.079401, acc 0.98
2016-09-06T03:26:48.227792: step 15989, loss 0.0220727, acc 1
2016-09-06T03:26:49.064821: step 15990, loss 0.00889567, acc 1
2016-09-06T03:26:49.885890: step 15991, loss 0.00187975, acc 1
2016-09-06T03:26:50.691372: step 15992, loss 0.001854, acc 1
2016-09-06T03:26:51.471069: step 15993, loss 0.0020426, acc 1
2016-09-06T03:26:52.245399: step 15994, loss 0.0811285, acc 0.98
2016-09-06T03:26:53.061819: step 15995, loss 0.0282739, acc 0.98
2016-09-06T03:26:53.876501: step 15996, loss 0.00173396, acc 1
2016-09-06T03:26:54.671520: step 15997, loss 0.0420785, acc 0.98
2016-09-06T03:26:55.474766: step 15998, loss 0.0391628, acc 0.98
2016-09-06T03:26:56.309445: step 15999, loss 0.00167668, acc 1
2016-09-06T03:26:57.098291: step 16000, loss 0.0164783, acc 0.98

Evaluation:
2016-09-06T03:27:00.843169: step 16000, loss 2.35711, acc 0.692308

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-16000

2016-09-06T03:27:02.727507: step 16001, loss 0.00325179, acc 1
2016-09-06T03:27:03.531598: step 16002, loss 0.0061027, acc 1
2016-09-06T03:27:04.337384: step 16003, loss 0.00153801, acc 1
2016-09-06T03:27:05.164510: step 16004, loss 0.00453291, acc 1
2016-09-06T03:27:06.012478: step 16005, loss 0.0133519, acc 1
2016-09-06T03:27:06.807991: step 16006, loss 0.00824906, acc 1
2016-09-06T03:27:07.643577: step 16007, loss 0.00214906, acc 1
2016-09-06T03:27:08.446683: step 16008, loss 0.0132102, acc 1
2016-09-06T03:27:09.294230: step 16009, loss 0.0246511, acc 1
2016-09-06T03:27:10.114030: step 16010, loss 0.0017897, acc 1
2016-09-06T03:27:10.961483: step 16011, loss 0.00748825, acc 1
2016-09-06T03:27:11.760782: step 16012, loss 0.0152878, acc 1
2016-09-06T03:27:12.556503: step 16013, loss 0.0342601, acc 0.98
2016-09-06T03:27:13.349435: step 16014, loss 0.0703624, acc 0.94
2016-09-06T03:27:14.162560: step 16015, loss 0.0202274, acc 0.98
2016-09-06T03:27:14.977333: step 16016, loss 0.0172447, acc 1
2016-09-06T03:27:15.782561: step 16017, loss 0.0021608, acc 1
2016-09-06T03:27:16.560463: step 16018, loss 0.0302862, acc 0.98
2016-09-06T03:27:17.363398: step 16019, loss 0.0124358, acc 1
2016-09-06T03:27:18.155488: step 16020, loss 0.00419527, acc 1
2016-09-06T03:27:18.952188: step 16021, loss 0.0063834, acc 1
2016-09-06T03:27:19.757092: step 16022, loss 0.00293745, acc 1
2016-09-06T03:27:20.596272: step 16023, loss 0.00158038, acc 1
2016-09-06T03:27:21.379678: step 16024, loss 0.00684739, acc 1
2016-09-06T03:27:22.186262: step 16025, loss 0.00257229, acc 1
2016-09-06T03:27:22.994599: step 16026, loss 0.00158563, acc 1
2016-09-06T03:27:23.788428: step 16027, loss 0.00164111, acc 1
2016-09-06T03:27:24.595445: step 16028, loss 0.00167028, acc 1
2016-09-06T03:27:25.405432: step 16029, loss 0.00340798, acc 1
2016-09-06T03:27:26.203262: step 16030, loss 0.00218776, acc 1
2016-09-06T03:27:27.007025: step 16031, loss 0.00174404, acc 1
2016-09-06T03:27:27.829912: step 16032, loss 0.00223689, acc 1
2016-09-06T03:27:28.641269: step 16033, loss 0.00166244, acc 1
2016-09-06T03:27:29.460138: step 16034, loss 0.00784604, acc 1
2016-09-06T03:27:30.268145: step 16035, loss 0.0101961, acc 1
2016-09-06T03:27:31.060910: step 16036, loss 0.00160632, acc 1
2016-09-06T03:27:31.890801: step 16037, loss 0.00863044, acc 1
2016-09-06T03:27:32.700787: step 16038, loss 0.0250514, acc 0.98
2016-09-06T03:27:33.495130: step 16039, loss 0.00334897, acc 1
2016-09-06T03:27:34.284800: step 16040, loss 0.00162541, acc 1
2016-09-06T03:27:35.111352: step 16041, loss 0.0366249, acc 0.98
2016-09-06T03:27:35.909464: step 16042, loss 0.00381116, acc 1
2016-09-06T03:27:36.715124: step 16043, loss 0.0337692, acc 0.98
2016-09-06T03:27:37.534985: step 16044, loss 0.00163485, acc 1
2016-09-06T03:27:38.303566: step 16045, loss 0.0376919, acc 0.98
2016-09-06T03:27:39.107326: step 16046, loss 0.0450173, acc 0.98
2016-09-06T03:27:39.918962: step 16047, loss 0.01627, acc 1
2016-09-06T03:27:40.697277: step 16048, loss 0.0258896, acc 1
2016-09-06T03:27:41.492139: step 16049, loss 0.00480918, acc 1
2016-09-06T03:27:42.324115: step 16050, loss 0.00616726, acc 1
2016-09-06T03:27:43.120252: step 16051, loss 0.0324417, acc 0.98
2016-09-06T03:27:43.917929: step 16052, loss 0.00376679, acc 1
2016-09-06T03:27:44.726971: step 16053, loss 0.00358921, acc 1
2016-09-06T03:27:45.535447: step 16054, loss 0.00162849, acc 1
2016-09-06T03:27:46.339160: step 16055, loss 0.0166685, acc 0.98
2016-09-06T03:27:47.160938: step 16056, loss 0.00170134, acc 1
2016-09-06T03:27:47.969818: step 16057, loss 0.00177555, acc 1
2016-09-06T03:27:48.759415: step 16058, loss 0.00186202, acc 1
2016-09-06T03:27:49.567213: step 16059, loss 0.00311782, acc 1
2016-09-06T03:27:50.358871: step 16060, loss 0.0213703, acc 1
2016-09-06T03:27:51.157946: step 16061, loss 0.0128572, acc 1
2016-09-06T03:27:51.951400: step 16062, loss 0.00244622, acc 1
2016-09-06T03:27:52.744970: step 16063, loss 0.0200884, acc 1
2016-09-06T03:27:53.541990: step 16064, loss 0.0263199, acc 1
2016-09-06T03:27:54.358713: step 16065, loss 0.018186, acc 1
2016-09-06T03:27:55.178970: step 16066, loss 0.0264429, acc 1
2016-09-06T03:27:56.001547: step 16067, loss 0.00223087, acc 1
2016-09-06T03:27:56.827145: step 16068, loss 0.00586236, acc 1
2016-09-06T03:27:57.623686: step 16069, loss 0.0132203, acc 1
2016-09-06T03:27:58.420989: step 16070, loss 0.0342332, acc 0.98
2016-09-06T03:27:59.238578: step 16071, loss 0.0328983, acc 0.98
2016-09-06T03:28:00.012503: step 16072, loss 0.00334517, acc 1
2016-09-06T03:28:00.848988: step 16073, loss 0.00183437, acc 1
2016-09-06T03:28:01.668998: step 16074, loss 0.0187972, acc 0.98
2016-09-06T03:28:02.464673: step 16075, loss 0.0190077, acc 0.98
2016-09-06T03:28:03.260472: step 16076, loss 0.00575658, acc 1
2016-09-06T03:28:04.092978: step 16077, loss 0.0286795, acc 0.98
2016-09-06T03:28:04.900689: step 16078, loss 0.00410214, acc 1
2016-09-06T03:28:05.703617: step 16079, loss 0.00319933, acc 1
2016-09-06T03:28:06.545724: step 16080, loss 0.0296927, acc 0.98
2016-09-06T03:28:07.335947: step 16081, loss 0.0312983, acc 0.98
2016-09-06T03:28:08.128194: step 16082, loss 0.00198187, acc 1
2016-09-06T03:28:08.930832: step 16083, loss 0.0622108, acc 0.94
2016-09-06T03:28:09.724479: step 16084, loss 0.00195675, acc 1
2016-09-06T03:28:10.530552: step 16085, loss 0.0158688, acc 0.98
2016-09-06T03:28:11.338600: step 16086, loss 0.0160477, acc 1
2016-09-06T03:28:12.156605: step 16087, loss 0.00195755, acc 1
2016-09-06T03:28:12.975665: step 16088, loss 0.00191403, acc 1
2016-09-06T03:28:13.777451: step 16089, loss 0.120011, acc 0.98
2016-09-06T03:28:14.539350: step 16090, loss 0.0405437, acc 0.98
2016-09-06T03:28:15.370921: step 16091, loss 0.0142766, acc 1
2016-09-06T03:28:16.175818: step 16092, loss 0.0162046, acc 1
2016-09-06T03:28:16.963416: step 16093, loss 0.0016352, acc 1
2016-09-06T03:28:17.761126: step 16094, loss 0.00461721, acc 1
2016-09-06T03:28:18.596546: step 16095, loss 0.00209338, acc 1
2016-09-06T03:28:19.390990: step 16096, loss 0.0115773, acc 1
2016-09-06T03:28:20.221832: step 16097, loss 0.0405016, acc 0.98
2016-09-06T03:28:21.039592: step 16098, loss 0.00174898, acc 1
2016-09-06T03:28:21.843841: step 16099, loss 0.00249504, acc 1
2016-09-06T03:28:22.613635: step 16100, loss 0.00801843, acc 1

Evaluation:
2016-09-06T03:28:26.391435: step 16100, loss 2.46215, acc 0.698874

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-16100

2016-09-06T03:28:28.387434: step 16101, loss 0.00653687, acc 1
2016-09-06T03:28:29.200127: step 16102, loss 0.028328, acc 0.98
2016-09-06T03:28:30.027928: step 16103, loss 0.00818524, acc 1
2016-09-06T03:28:30.823116: step 16104, loss 0.00691625, acc 1
2016-09-06T03:28:31.631719: step 16105, loss 0.0222931, acc 1
2016-09-06T03:28:32.443113: step 16106, loss 0.00447633, acc 1
2016-09-06T03:28:33.241670: step 16107, loss 0.0142239, acc 1
2016-09-06T03:28:34.029673: step 16108, loss 0.00703799, acc 1
2016-09-06T03:28:34.836688: step 16109, loss 0.00181878, acc 1
2016-09-06T03:28:35.671535: step 16110, loss 0.0224594, acc 1
2016-09-06T03:28:36.473788: step 16111, loss 0.00778972, acc 1
2016-09-06T03:28:37.255440: step 16112, loss 0.00243449, acc 1
2016-09-06T03:28:38.110502: step 16113, loss 0.00189999, acc 1
2016-09-06T03:28:38.908260: step 16114, loss 0.0500111, acc 0.96
2016-09-06T03:28:39.681779: step 16115, loss 0.0152416, acc 1
2016-09-06T03:28:40.506988: step 16116, loss 0.0134002, acc 1
2016-09-06T03:28:41.327684: step 16117, loss 0.00245884, acc 1
2016-09-06T03:28:42.130328: step 16118, loss 0.00207274, acc 1
2016-09-06T03:28:42.956619: step 16119, loss 0.00235356, acc 1
2016-09-06T03:28:43.754191: step 16120, loss 0.00217168, acc 1
2016-09-06T03:28:44.556296: step 16121, loss 0.00814362, acc 1
2016-09-06T03:28:45.388819: step 16122, loss 0.017915, acc 0.98
2016-09-06T03:28:46.181954: step 16123, loss 0.0182111, acc 1
2016-09-06T03:28:46.987565: step 16124, loss 0.0272771, acc 0.98
2016-09-06T03:28:47.809564: step 16125, loss 0.0333147, acc 0.98
2016-09-06T03:28:48.627672: step 16126, loss 0.0551727, acc 0.98
2016-09-06T03:28:49.427603: step 16127, loss 0.0108947, acc 1
2016-09-06T03:28:50.188142: step 16128, loss 0.00173446, acc 1
2016-09-06T03:28:50.992376: step 16129, loss 0.00171481, acc 1
2016-09-06T03:28:51.786298: step 16130, loss 0.0195267, acc 0.98
2016-09-06T03:28:52.603643: step 16131, loss 0.042338, acc 0.98
2016-09-06T03:28:53.421194: step 16132, loss 0.00537348, acc 1
2016-09-06T03:28:54.223436: step 16133, loss 0.00170823, acc 1
2016-09-06T03:28:55.069790: step 16134, loss 0.00167446, acc 1
2016-09-06T03:28:55.888967: step 16135, loss 0.026842, acc 0.98
2016-09-06T03:28:56.705678: step 16136, loss 0.0100161, acc 1
2016-09-06T03:28:57.562406: step 16137, loss 0.118692, acc 0.96
2016-09-06T03:28:58.422342: step 16138, loss 0.00165701, acc 1
2016-09-06T03:28:59.237523: step 16139, loss 0.0298421, acc 1
2016-09-06T03:29:00.079146: step 16140, loss 0.0286341, acc 0.98
2016-09-06T03:29:00.978636: step 16141, loss 0.00215657, acc 1
2016-09-06T03:29:01.757396: step 16142, loss 0.00180195, acc 1
2016-09-06T03:29:02.588948: step 16143, loss 0.0120697, acc 1
2016-09-06T03:29:03.399498: step 16144, loss 0.0117668, acc 1
2016-09-06T03:29:04.187732: step 16145, loss 0.00189263, acc 1
2016-09-06T03:29:04.980788: step 16146, loss 0.0037048, acc 1
2016-09-06T03:29:05.769441: step 16147, loss 0.00169183, acc 1
2016-09-06T03:29:06.565187: step 16148, loss 0.0165249, acc 1
2016-09-06T03:29:07.370182: step 16149, loss 0.0145989, acc 1
2016-09-06T03:29:08.183033: step 16150, loss 0.0143564, acc 1
2016-09-06T03:29:08.970337: step 16151, loss 0.0141672, acc 1
2016-09-06T03:29:09.779420: step 16152, loss 0.006368, acc 1
2016-09-06T03:29:10.618668: step 16153, loss 0.00977978, acc 1
2016-09-06T03:29:11.410713: step 16154, loss 0.0187661, acc 0.98
2016-09-06T03:29:12.214693: step 16155, loss 0.0135725, acc 1
2016-09-06T03:29:13.061100: step 16156, loss 0.00489318, acc 1
2016-09-06T03:29:13.854861: step 16157, loss 0.0110502, acc 1
2016-09-06T03:29:14.669576: step 16158, loss 0.00177812, acc 1
2016-09-06T03:29:15.474453: step 16159, loss 0.00249594, acc 1
2016-09-06T03:29:16.270719: step 16160, loss 0.00957409, acc 1
2016-09-06T03:29:17.046395: step 16161, loss 0.00251253, acc 1
2016-09-06T03:29:17.876559: step 16162, loss 0.0184362, acc 1
2016-09-06T03:29:18.637412: step 16163, loss 0.00240638, acc 1
2016-09-06T03:29:19.450559: step 16164, loss 0.00737437, acc 1
2016-09-06T03:29:20.271752: step 16165, loss 0.0024363, acc 1
2016-09-06T03:29:21.068983: step 16166, loss 0.0260584, acc 1
2016-09-06T03:29:21.862296: step 16167, loss 0.0018381, acc 1
2016-09-06T03:29:22.669266: step 16168, loss 0.00183717, acc 1
2016-09-06T03:29:23.471206: step 16169, loss 0.0378311, acc 0.96
2016-09-06T03:29:24.273195: step 16170, loss 0.00187021, acc 1
2016-09-06T03:29:25.084922: step 16171, loss 0.0239446, acc 0.98
2016-09-06T03:29:25.872835: step 16172, loss 0.00945458, acc 1
2016-09-06T03:29:26.674476: step 16173, loss 0.00242047, acc 1
2016-09-06T03:29:27.481536: step 16174, loss 0.00229081, acc 1
2016-09-06T03:29:28.257054: step 16175, loss 0.0181244, acc 0.98
2016-09-06T03:29:29.082986: step 16176, loss 0.00200178, acc 1
2016-09-06T03:29:29.877558: step 16177, loss 0.00181656, acc 1
2016-09-06T03:29:30.688808: step 16178, loss 0.00227264, acc 1
2016-09-06T03:29:31.510621: step 16179, loss 0.0196208, acc 0.98
2016-09-06T03:29:32.343197: step 16180, loss 0.0552467, acc 0.96
2016-09-06T03:29:33.157230: step 16181, loss 0.0130048, acc 1
2016-09-06T03:29:33.997099: step 16182, loss 0.00934899, acc 1
2016-09-06T03:29:34.776200: step 16183, loss 0.0100047, acc 1
2016-09-06T03:29:35.537541: step 16184, loss 0.00309434, acc 1
2016-09-06T03:29:36.355394: step 16185, loss 0.00169046, acc 1
2016-09-06T03:29:37.169273: step 16186, loss 0.0410996, acc 0.98
2016-09-06T03:29:37.973379: step 16187, loss 0.00168294, acc 1
2016-09-06T03:29:38.790439: step 16188, loss 0.00181413, acc 1
2016-09-06T03:29:39.649114: step 16189, loss 0.0168253, acc 0.98
2016-09-06T03:29:40.475431: step 16190, loss 0.00168153, acc 1
2016-09-06T03:29:41.272825: step 16191, loss 0.00247085, acc 1
2016-09-06T03:29:42.063881: step 16192, loss 0.022356, acc 0.98
2016-09-06T03:29:42.855073: step 16193, loss 0.00348285, acc 1
2016-09-06T03:29:43.674908: step 16194, loss 0.015052, acc 1
2016-09-06T03:29:44.524793: step 16195, loss 0.00683075, acc 1
2016-09-06T03:29:45.331763: step 16196, loss 0.0235097, acc 0.98
2016-09-06T03:29:46.126944: step 16197, loss 0.0263028, acc 1
2016-09-06T03:29:46.957073: step 16198, loss 0.00157367, acc 1
2016-09-06T03:29:47.755958: step 16199, loss 0.00262205, acc 1
2016-09-06T03:29:48.587620: step 16200, loss 0.00171713, acc 1

Evaluation:
2016-09-06T03:29:52.326768: step 16200, loss 2.9032, acc 0.689493

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-16200

2016-09-06T03:29:54.170973: step 16201, loss 0.00230722, acc 1
2016-09-06T03:29:54.952567: step 16202, loss 0.0019546, acc 1
2016-09-06T03:29:55.774929: step 16203, loss 0.011737, acc 1
2016-09-06T03:29:56.591494: step 16204, loss 0.0484404, acc 0.98
2016-09-06T03:29:57.384033: step 16205, loss 0.00477422, acc 1
2016-09-06T03:29:58.184693: step 16206, loss 0.0238866, acc 0.98
2016-09-06T03:29:58.988154: step 16207, loss 0.00982295, acc 1
2016-09-06T03:29:59.785982: step 16208, loss 0.00214805, acc 1
2016-09-06T03:30:00.616987: step 16209, loss 0.00630839, acc 1
2016-09-06T03:30:01.426082: step 16210, loss 0.00158069, acc 1
2016-09-06T03:30:02.222180: step 16211, loss 0.00638576, acc 1
2016-09-06T03:30:03.026785: step 16212, loss 0.00405762, acc 1
2016-09-06T03:30:03.823836: step 16213, loss 0.00556189, acc 1
2016-09-06T03:30:04.587533: step 16214, loss 0.0367731, acc 0.98
2016-09-06T03:30:05.387269: step 16215, loss 0.00154973, acc 1
2016-09-06T03:30:06.211590: step 16216, loss 0.00310143, acc 1
2016-09-06T03:30:06.996731: step 16217, loss 0.00161541, acc 1
2016-09-06T03:30:07.804090: step 16218, loss 0.00215053, acc 1
2016-09-06T03:30:08.625494: step 16219, loss 0.0264494, acc 0.98
2016-09-06T03:30:09.433235: step 16220, loss 0.00154499, acc 1
2016-09-06T03:30:10.230119: step 16221, loss 0.00154698, acc 1
2016-09-06T03:30:11.031955: step 16222, loss 0.00191014, acc 1
2016-09-06T03:30:11.811982: step 16223, loss 0.0142312, acc 1
2016-09-06T03:30:12.646785: step 16224, loss 0.00157788, acc 1
2016-09-06T03:30:13.466726: step 16225, loss 0.00170105, acc 1
2016-09-06T03:30:14.251909: step 16226, loss 0.0270754, acc 1
2016-09-06T03:30:15.066091: step 16227, loss 0.00915025, acc 1
2016-09-06T03:30:15.905558: step 16228, loss 0.0121395, acc 1
2016-09-06T03:30:16.685583: step 16229, loss 0.00925196, acc 1
2016-09-06T03:30:17.477549: step 16230, loss 0.00159392, acc 1
2016-09-06T03:30:18.309473: step 16231, loss 0.0131573, acc 1
2016-09-06T03:30:19.096094: step 16232, loss 0.00162765, acc 1
2016-09-06T03:30:19.909960: step 16233, loss 0.0126271, acc 1
2016-09-06T03:30:20.740531: step 16234, loss 0.00261262, acc 1
2016-09-06T03:30:21.516770: step 16235, loss 0.0172143, acc 0.98
2016-09-06T03:30:22.315702: step 16236, loss 0.00246127, acc 1
2016-09-06T03:30:23.120520: step 16237, loss 0.0163889, acc 0.98
2016-09-06T03:30:23.906545: step 16238, loss 0.0213639, acc 1
2016-09-06T03:30:24.696970: step 16239, loss 0.00169577, acc 1
2016-09-06T03:30:25.532793: step 16240, loss 0.0343616, acc 1
2016-09-06T03:30:26.335359: step 16241, loss 0.0101444, acc 1
2016-09-06T03:30:27.145374: step 16242, loss 0.0025822, acc 1
2016-09-06T03:30:27.990738: step 16243, loss 0.0121376, acc 1
2016-09-06T03:30:28.755260: step 16244, loss 0.00175536, acc 1
2016-09-06T03:30:29.536847: step 16245, loss 0.00194086, acc 1
2016-09-06T03:30:30.364534: step 16246, loss 0.0029798, acc 1
2016-09-06T03:30:31.180659: step 16247, loss 0.00415292, acc 1
2016-09-06T03:30:32.005453: step 16248, loss 0.015173, acc 1
2016-09-06T03:30:32.828670: step 16249, loss 0.00179538, acc 1
2016-09-06T03:30:33.622135: step 16250, loss 0.0023603, acc 1
2016-09-06T03:30:34.401725: step 16251, loss 0.0353954, acc 0.98
2016-09-06T03:30:35.217410: step 16252, loss 0.0190809, acc 1
2016-09-06T03:30:36.013487: step 16253, loss 0.00839691, acc 1
2016-09-06T03:30:36.833515: step 16254, loss 0.00176331, acc 1
2016-09-06T03:30:37.655778: step 16255, loss 0.0243925, acc 0.98
2016-09-06T03:30:38.488806: step 16256, loss 0.00277766, acc 1
2016-09-06T03:30:39.317022: step 16257, loss 0.00847851, acc 1
2016-09-06T03:30:40.163711: step 16258, loss 0.00180163, acc 1
2016-09-06T03:30:40.954009: step 16259, loss 0.00180866, acc 1
2016-09-06T03:30:41.807834: step 16260, loss 0.00184134, acc 1
2016-09-06T03:30:42.634320: step 16261, loss 0.00294256, acc 1
2016-09-06T03:30:43.450291: step 16262, loss 0.00436395, acc 1
2016-09-06T03:30:44.257910: step 16263, loss 0.0192732, acc 1
2016-09-06T03:30:45.118801: step 16264, loss 0.0139338, acc 1
2016-09-06T03:30:45.936000: step 16265, loss 0.00794129, acc 1
2016-09-06T03:30:46.759944: step 16266, loss 0.0118025, acc 1
2016-09-06T03:30:47.598193: step 16267, loss 0.00198429, acc 1
2016-09-06T03:30:48.416700: step 16268, loss 0.00187721, acc 1
2016-09-06T03:30:49.241371: step 16269, loss 0.00974668, acc 1
2016-09-06T03:30:50.067343: step 16270, loss 0.00341808, acc 1
2016-09-06T03:30:50.893591: step 16271, loss 0.0064231, acc 1
2016-09-06T03:30:51.678701: step 16272, loss 0.0322798, acc 0.98
2016-09-06T03:30:52.513686: step 16273, loss 0.00198447, acc 1
2016-09-06T03:30:53.376589: step 16274, loss 0.0136496, acc 1
2016-09-06T03:30:54.152646: step 16275, loss 0.018792, acc 1
2016-09-06T03:30:54.956055: step 16276, loss 0.0020574, acc 1
2016-09-06T03:30:55.775859: step 16277, loss 0.0135366, acc 1
2016-09-06T03:30:56.577343: step 16278, loss 0.0114649, acc 1
2016-09-06T03:30:57.402235: step 16279, loss 0.00196918, acc 1
2016-09-06T03:30:58.228918: step 16280, loss 0.00192115, acc 1
2016-09-06T03:30:59.025040: step 16281, loss 0.00948963, acc 1
2016-09-06T03:30:59.831704: step 16282, loss 0.00192349, acc 1
2016-09-06T03:31:00.663173: step 16283, loss 0.00207573, acc 1
2016-09-06T03:31:01.454327: step 16284, loss 0.028022, acc 0.98
2016-09-06T03:31:02.249977: step 16285, loss 0.0101844, acc 1
2016-09-06T03:31:03.045340: step 16286, loss 0.00192312, acc 1
2016-09-06T03:31:03.854591: step 16287, loss 0.00192585, acc 1
2016-09-06T03:31:04.663558: step 16288, loss 0.0331617, acc 0.98
2016-09-06T03:31:05.486398: step 16289, loss 0.0459148, acc 0.98
2016-09-06T03:31:06.286210: step 16290, loss 0.0126949, acc 1
2016-09-06T03:31:07.080187: step 16291, loss 0.0018809, acc 1
2016-09-06T03:31:07.949415: step 16292, loss 0.00185854, acc 1
2016-09-06T03:31:08.744707: step 16293, loss 0.036123, acc 0.98
2016-09-06T03:31:09.561360: step 16294, loss 0.00728614, acc 1
2016-09-06T03:31:10.395341: step 16295, loss 0.00226781, acc 1
2016-09-06T03:31:11.196356: step 16296, loss 0.00234672, acc 1
2016-09-06T03:31:12.004183: step 16297, loss 0.0235284, acc 1
2016-09-06T03:31:12.849774: step 16298, loss 0.00635577, acc 1
2016-09-06T03:31:13.651657: step 16299, loss 0.157297, acc 0.96
2016-09-06T03:31:14.458459: step 16300, loss 0.00179975, acc 1

Evaluation:
2016-09-06T03:31:18.229502: step 16300, loss 3.07858, acc 0.708255

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-16300

2016-09-06T03:31:20.164638: step 16301, loss 0.0283126, acc 0.98
2016-09-06T03:31:20.986618: step 16302, loss 0.0293138, acc 0.98
2016-09-06T03:31:21.780718: step 16303, loss 0.0291283, acc 0.98
2016-09-06T03:31:22.617722: step 16304, loss 0.0398083, acc 0.98
2016-09-06T03:31:23.411121: step 16305, loss 0.0133888, acc 1
2016-09-06T03:31:24.226690: step 16306, loss 0.050348, acc 0.96
2016-09-06T03:31:25.064163: step 16307, loss 0.0270445, acc 0.98
2016-09-06T03:31:25.873915: step 16308, loss 0.0311308, acc 0.96
2016-09-06T03:31:26.714278: step 16309, loss 0.0210145, acc 0.98
2016-09-06T03:31:27.524294: step 16310, loss 0.00192048, acc 1
2016-09-06T03:31:28.322882: step 16311, loss 0.00152437, acc 1
2016-09-06T03:31:29.112765: step 16312, loss 0.0488091, acc 0.98
2016-09-06T03:31:29.950139: step 16313, loss 0.00155189, acc 1
2016-09-06T03:31:30.746398: step 16314, loss 0.0160912, acc 1
2016-09-06T03:31:31.553928: step 16315, loss 0.0237008, acc 1
2016-09-06T03:31:32.377820: step 16316, loss 0.00297104, acc 1
2016-09-06T03:31:33.204073: step 16317, loss 0.0215697, acc 1
2016-09-06T03:31:34.030664: step 16318, loss 0.00985685, acc 1
2016-09-06T03:31:34.875337: step 16319, loss 0.0211871, acc 0.98
2016-09-06T03:31:35.648721: step 16320, loss 0.00177609, acc 1
2016-09-06T03:31:36.499556: step 16321, loss 0.00834928, acc 1
2016-09-06T03:31:37.332995: step 16322, loss 0.00944417, acc 1
2016-09-06T03:31:38.150053: step 16323, loss 0.0019725, acc 1
2016-09-06T03:31:38.954088: step 16324, loss 0.0331158, acc 0.98
2016-09-06T03:31:39.787421: step 16325, loss 0.00328746, acc 1
2016-09-06T03:31:40.609664: step 16326, loss 0.00385606, acc 1
2016-09-06T03:31:41.393882: step 16327, loss 0.00396959, acc 1
2016-09-06T03:31:42.231468: step 16328, loss 0.00368132, acc 1
2016-09-06T03:31:43.025400: step 16329, loss 0.0451247, acc 0.98
2016-09-06T03:31:43.821693: step 16330, loss 0.00267824, acc 1
2016-09-06T03:31:44.661591: step 16331, loss 0.0466146, acc 0.98
2016-09-06T03:31:45.467287: step 16332, loss 0.0293957, acc 0.98
2016-09-06T03:31:46.310095: step 16333, loss 0.00175096, acc 1
2016-09-06T03:31:47.110234: step 16334, loss 0.0022778, acc 1
2016-09-06T03:31:47.935394: step 16335, loss 0.00744567, acc 1
2016-09-06T03:31:48.737752: step 16336, loss 0.0534802, acc 0.98
2016-09-06T03:31:49.551581: step 16337, loss 0.00318691, acc 1
2016-09-06T03:31:50.373408: step 16338, loss 0.0135488, acc 1
2016-09-06T03:31:51.146151: step 16339, loss 0.0144805, acc 1
2016-09-06T03:31:51.970037: step 16340, loss 0.0133843, acc 1
2016-09-06T03:31:52.754530: step 16341, loss 0.144343, acc 0.98
2016-09-06T03:31:53.529009: step 16342, loss 0.0165464, acc 1
2016-09-06T03:31:54.336633: step 16343, loss 0.00228923, acc 1
2016-09-06T03:31:55.177377: step 16344, loss 0.00184857, acc 1
2016-09-06T03:31:55.982477: step 16345, loss 0.0034557, acc 1
2016-09-06T03:31:56.797417: step 16346, loss 0.0179925, acc 1
2016-09-06T03:31:57.631087: step 16347, loss 0.0144748, acc 1
2016-09-06T03:31:58.414091: step 16348, loss 0.00157702, acc 1
2016-09-06T03:31:59.219767: step 16349, loss 0.117993, acc 0.98
2016-09-06T03:32:00.058158: step 16350, loss 0.00162577, acc 1
2016-09-06T03:32:00.871117: step 16351, loss 0.00143694, acc 1
2016-09-06T03:32:01.661268: step 16352, loss 0.00386542, acc 1
2016-09-06T03:32:02.446895: step 16353, loss 0.0145916, acc 1
2016-09-06T03:32:03.238808: step 16354, loss 0.00958238, acc 1
2016-09-06T03:32:04.027146: step 16355, loss 0.00199201, acc 1
2016-09-06T03:32:04.823370: step 16356, loss 0.0200825, acc 1
2016-09-06T03:32:05.591647: step 16357, loss 0.00295374, acc 1
2016-09-06T03:32:06.416567: step 16358, loss 0.0311063, acc 0.98
2016-09-06T03:32:07.248567: step 16359, loss 0.0137912, acc 1
2016-09-06T03:32:08.027882: step 16360, loss 0.00255049, acc 1
2016-09-06T03:32:08.851815: step 16361, loss 0.00194346, acc 1
2016-09-06T03:32:09.656415: step 16362, loss 0.00302876, acc 1
2016-09-06T03:32:10.442717: step 16363, loss 0.0116299, acc 1
2016-09-06T03:32:11.266940: step 16364, loss 0.00220994, acc 1
2016-09-06T03:32:12.056076: step 16365, loss 0.00161834, acc 1
2016-09-06T03:32:12.854923: step 16366, loss 0.0480965, acc 0.96
2016-09-06T03:32:13.687967: step 16367, loss 0.0124365, acc 1
2016-09-06T03:32:14.489117: step 16368, loss 0.0162274, acc 1
2016-09-06T03:32:15.290594: step 16369, loss 0.0181496, acc 1
2016-09-06T03:32:16.111735: step 16370, loss 0.00183676, acc 1
2016-09-06T03:32:16.911212: step 16371, loss 0.00215575, acc 1
2016-09-06T03:32:17.729027: step 16372, loss 0.0439658, acc 0.98
2016-09-06T03:32:18.541805: step 16373, loss 0.0307505, acc 0.98
2016-09-06T03:32:19.372841: step 16374, loss 0.00230753, acc 1
2016-09-06T03:32:20.160540: step 16375, loss 0.0054475, acc 1
2016-09-06T03:32:20.958371: step 16376, loss 0.00391603, acc 1
2016-09-06T03:32:21.799909: step 16377, loss 0.00174554, acc 1
2016-09-06T03:32:22.574541: step 16378, loss 0.0232169, acc 0.98
2016-09-06T03:32:23.357349: step 16379, loss 0.00337767, acc 1
2016-09-06T03:32:24.153410: step 16380, loss 0.00381709, acc 1
2016-09-06T03:32:24.940900: step 16381, loss 0.00453621, acc 1
2016-09-06T03:32:25.764429: step 16382, loss 0.00808116, acc 1
2016-09-06T03:32:26.582100: step 16383, loss 0.00190829, acc 1
2016-09-06T03:32:27.347037: step 16384, loss 0.0346648, acc 0.98
2016-09-06T03:32:28.177932: step 16385, loss 0.00475505, acc 1
2016-09-06T03:32:28.984468: step 16386, loss 0.0298328, acc 0.98
2016-09-06T03:32:29.809960: step 16387, loss 0.0130391, acc 1
2016-09-06T03:32:30.637395: step 16388, loss 0.00644835, acc 1
2016-09-06T03:32:31.435916: step 16389, loss 0.00187948, acc 1
2016-09-06T03:32:32.214683: step 16390, loss 0.00215713, acc 1
2016-09-06T03:32:33.025166: step 16391, loss 0.00290136, acc 1
2016-09-06T03:32:33.858835: step 16392, loss 0.00176548, acc 1
2016-09-06T03:32:34.652401: step 16393, loss 0.0040089, acc 1
2016-09-06T03:32:35.468620: step 16394, loss 0.00241705, acc 1
2016-09-06T03:32:36.309879: step 16395, loss 0.013004, acc 1
2016-09-06T03:32:37.114763: step 16396, loss 0.0160241, acc 1
2016-09-06T03:32:37.892089: step 16397, loss 0.00590351, acc 1
2016-09-06T03:32:38.725759: step 16398, loss 0.0691006, acc 0.96
2016-09-06T03:32:39.495153: step 16399, loss 0.0423941, acc 0.98
2016-09-06T03:32:40.308199: step 16400, loss 0.0150661, acc 1

Evaluation:
2016-09-06T03:32:44.068045: step 16400, loss 2.69178, acc 0.702627

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-16400

2016-09-06T03:32:45.961093: step 16401, loss 0.00165699, acc 1
2016-09-06T03:32:46.784707: step 16402, loss 0.00237337, acc 1
2016-09-06T03:32:47.628600: step 16403, loss 0.0779674, acc 0.98
2016-09-06T03:32:48.457596: step 16404, loss 0.01461, acc 1
2016-09-06T03:32:49.239499: step 16405, loss 0.00338079, acc 1
2016-09-06T03:32:50.051969: step 16406, loss 0.0261942, acc 1
2016-09-06T03:32:50.848236: step 16407, loss 0.00186869, acc 1
2016-09-06T03:32:51.673050: step 16408, loss 0.00182472, acc 1
2016-09-06T03:32:52.507756: step 16409, loss 0.0091835, acc 1
2016-09-06T03:32:53.337771: step 16410, loss 0.00177081, acc 1
2016-09-06T03:32:54.121273: step 16411, loss 0.00266115, acc 1
2016-09-06T03:32:54.940850: step 16412, loss 0.00591882, acc 1
2016-09-06T03:32:55.783918: step 16413, loss 0.00278021, acc 1
2016-09-06T03:32:56.563065: step 16414, loss 0.00786833, acc 1
2016-09-06T03:32:57.351222: step 16415, loss 0.00719471, acc 1
2016-09-06T03:32:58.177334: step 16416, loss 0.00158056, acc 1
2016-09-06T03:32:58.973397: step 16417, loss 0.0289129, acc 0.98
2016-09-06T03:32:59.775768: step 16418, loss 0.024272, acc 0.98
2016-09-06T03:33:00.629992: step 16419, loss 0.0142085, acc 1
2016-09-06T03:33:01.434309: step 16420, loss 0.0443514, acc 0.96
2016-09-06T03:33:02.252799: step 16421, loss 0.00141068, acc 1
2016-09-06T03:33:03.058511: step 16422, loss 0.00217572, acc 1
2016-09-06T03:33:03.866426: step 16423, loss 0.00150427, acc 1
2016-09-06T03:33:04.661573: step 16424, loss 0.00139204, acc 1
2016-09-06T03:33:05.469687: step 16425, loss 0.0177595, acc 0.98
2016-09-06T03:33:06.266794: step 16426, loss 0.00141217, acc 1
2016-09-06T03:33:07.072613: step 16427, loss 0.0123376, acc 1
2016-09-06T03:33:07.918742: step 16428, loss 0.014862, acc 1
2016-09-06T03:33:08.772262: step 16429, loss 0.024152, acc 0.98
2016-09-06T03:33:09.577917: step 16430, loss 0.0156611, acc 1
2016-09-06T03:33:10.403178: step 16431, loss 0.0014293, acc 1
2016-09-06T03:33:11.220256: step 16432, loss 0.0328343, acc 0.98
2016-09-06T03:33:12.015452: step 16433, loss 0.0014152, acc 1
2016-09-06T03:33:12.874184: step 16434, loss 0.00146804, acc 1
2016-09-06T03:33:13.686795: step 16435, loss 0.0640425, acc 0.96
2016-09-06T03:33:14.504087: step 16436, loss 0.00145068, acc 1
2016-09-06T03:33:15.339762: step 16437, loss 0.014796, acc 1
2016-09-06T03:33:16.182938: step 16438, loss 0.00141511, acc 1
2016-09-06T03:33:17.013432: step 16439, loss 0.0165226, acc 0.98
2016-09-06T03:33:17.840361: step 16440, loss 0.00642779, acc 1
2016-09-06T03:33:18.648236: step 16441, loss 0.00154827, acc 1
2016-09-06T03:33:19.487028: step 16442, loss 0.00211711, acc 1
2016-09-06T03:33:20.289216: step 16443, loss 0.00569864, acc 1
2016-09-06T03:33:21.105384: step 16444, loss 0.0183024, acc 0.98
2016-09-06T03:33:21.920038: step 16445, loss 0.0672147, acc 0.98
2016-09-06T03:33:22.729645: step 16446, loss 0.00555932, acc 1
2016-09-06T03:33:23.576174: step 16447, loss 0.0168816, acc 1
2016-09-06T03:33:24.364988: step 16448, loss 0.00173548, acc 1
2016-09-06T03:33:25.148755: step 16449, loss 0.00147878, acc 1
2016-09-06T03:33:25.955814: step 16450, loss 0.0156936, acc 1
2016-09-06T03:33:26.742524: step 16451, loss 0.00141094, acc 1
2016-09-06T03:33:27.538437: step 16452, loss 0.00197779, acc 1
2016-09-06T03:33:28.349969: step 16453, loss 0.0159618, acc 0.98
2016-09-06T03:33:29.141795: step 16454, loss 0.0139128, acc 1
2016-09-06T03:33:29.973454: step 16455, loss 0.031806, acc 0.98
2016-09-06T03:33:30.775785: step 16456, loss 0.00139119, acc 1
2016-09-06T03:33:31.557421: step 16457, loss 0.00151501, acc 1
2016-09-06T03:33:32.350202: step 16458, loss 0.00943967, acc 1
2016-09-06T03:33:33.182203: step 16459, loss 0.00135555, acc 1
2016-09-06T03:33:33.965651: step 16460, loss 0.0042499, acc 1
2016-09-06T03:33:34.788913: step 16461, loss 0.0289558, acc 0.98
2016-09-06T03:33:35.595628: step 16462, loss 0.00850951, acc 1
2016-09-06T03:33:36.387641: step 16463, loss 0.00353085, acc 1
2016-09-06T03:33:37.186419: step 16464, loss 0.0162136, acc 0.98
2016-09-06T03:33:38.013685: step 16465, loss 0.00163053, acc 1
2016-09-06T03:33:38.813647: step 16466, loss 0.0122172, acc 1
2016-09-06T03:33:39.685843: step 16467, loss 0.0103945, acc 1
2016-09-06T03:33:40.499377: step 16468, loss 0.00197608, acc 1
2016-09-06T03:33:41.297778: step 16469, loss 0.00466345, acc 1
2016-09-06T03:33:42.118235: step 16470, loss 0.0109082, acc 1
2016-09-06T03:33:42.931121: step 16471, loss 0.00135428, acc 1
2016-09-06T03:33:43.726071: step 16472, loss 0.00189268, acc 1
2016-09-06T03:33:44.513462: step 16473, loss 0.00602228, acc 1
2016-09-06T03:33:45.335344: step 16474, loss 0.0180419, acc 0.98
2016-09-06T03:33:46.125403: step 16475, loss 0.00185279, acc 1
2016-09-06T03:33:46.940547: step 16476, loss 0.0110119, acc 1
2016-09-06T03:33:47.777437: step 16477, loss 0.00530744, acc 1
2016-09-06T03:33:48.576873: step 16478, loss 0.00142883, acc 1
2016-09-06T03:33:49.386239: step 16479, loss 0.00505783, acc 1
2016-09-06T03:33:50.218134: step 16480, loss 0.0135547, acc 1
2016-09-06T03:33:51.022977: step 16481, loss 0.02532, acc 0.98
2016-09-06T03:33:51.844941: step 16482, loss 0.0234742, acc 0.98
2016-09-06T03:33:52.640883: step 16483, loss 0.014767, acc 1
2016-09-06T03:33:53.443925: step 16484, loss 0.00153494, acc 1
2016-09-06T03:33:54.223904: step 16485, loss 0.0701299, acc 0.98
2016-09-06T03:33:55.039855: step 16486, loss 0.0158359, acc 0.98
2016-09-06T03:33:55.821411: step 16487, loss 0.00385106, acc 1
2016-09-06T03:33:56.631520: step 16488, loss 0.0028404, acc 1
2016-09-06T03:33:57.432876: step 16489, loss 0.00466271, acc 1
2016-09-06T03:33:58.213332: step 16490, loss 0.00149748, acc 1
2016-09-06T03:33:59.039445: step 16491, loss 0.023045, acc 0.98
2016-09-06T03:33:59.876987: step 16492, loss 0.00263244, acc 1
2016-09-06T03:34:00.696301: step 16493, loss 0.00153668, acc 1
2016-09-06T03:34:01.489296: step 16494, loss 0.0177434, acc 0.98
2016-09-06T03:34:02.303507: step 16495, loss 0.0210956, acc 0.98
2016-09-06T03:34:03.144031: step 16496, loss 0.00177169, acc 1
2016-09-06T03:34:03.958205: step 16497, loss 0.0153035, acc 1
2016-09-06T03:34:04.774555: step 16498, loss 0.0232656, acc 0.98
2016-09-06T03:34:05.560540: step 16499, loss 0.00354815, acc 1
2016-09-06T03:34:06.348791: step 16500, loss 0.00187076, acc 1

Evaluation:
2016-09-06T03:34:10.078388: step 16500, loss 2.71378, acc 0.703565

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-16500

2016-09-06T03:34:12.034435: step 16501, loss 0.00208044, acc 1
2016-09-06T03:34:12.845645: step 16502, loss 0.0356402, acc 0.98
2016-09-06T03:34:13.645316: step 16503, loss 0.00157289, acc 1
2016-09-06T03:34:14.456009: step 16504, loss 0.00154646, acc 1
2016-09-06T03:34:15.229412: step 16505, loss 0.00151685, acc 1
2016-09-06T03:34:16.019390: step 16506, loss 0.0155548, acc 1
2016-09-06T03:34:16.857344: step 16507, loss 0.0150266, acc 1
2016-09-06T03:34:17.605438: step 16508, loss 0.00149717, acc 1
2016-09-06T03:34:18.416357: step 16509, loss 0.0109697, acc 1
2016-09-06T03:34:19.228816: step 16510, loss 0.0020481, acc 1
2016-09-06T03:34:20.030240: step 16511, loss 0.0341605, acc 0.96
2016-09-06T03:34:20.791889: step 16512, loss 0.00162077, acc 1
2016-09-06T03:34:21.609210: step 16513, loss 0.00180087, acc 1
2016-09-06T03:34:22.409908: step 16514, loss 0.00536025, acc 1
2016-09-06T03:34:23.224253: step 16515, loss 0.00157969, acc 1
2016-09-06T03:34:24.048178: step 16516, loss 0.0515224, acc 0.96
2016-09-06T03:34:24.852165: step 16517, loss 0.00153317, acc 1
2016-09-06T03:34:25.689136: step 16518, loss 0.00151611, acc 1
2016-09-06T03:34:26.472465: step 16519, loss 0.011846, acc 1
2016-09-06T03:34:27.259975: step 16520, loss 0.0543763, acc 0.98
2016-09-06T03:34:28.068442: step 16521, loss 0.0828386, acc 0.96
2016-09-06T03:34:28.919000: step 16522, loss 0.00131286, acc 1
2016-09-06T03:34:29.721339: step 16523, loss 0.0418988, acc 0.98
2016-09-06T03:34:30.577247: step 16524, loss 0.00148799, acc 1
2016-09-06T03:34:31.381158: step 16525, loss 0.00304601, acc 1
2016-09-06T03:34:32.167646: step 16526, loss 0.00220564, acc 1
2016-09-06T03:34:32.981457: step 16527, loss 0.00159552, acc 1
2016-09-06T03:34:33.809073: step 16528, loss 0.0403914, acc 0.96
2016-09-06T03:34:34.598106: step 16529, loss 0.00139081, acc 1
2016-09-06T03:34:35.410129: step 16530, loss 0.00252201, acc 1
2016-09-06T03:34:36.223594: step 16531, loss 0.00152828, acc 1
2016-09-06T03:34:37.026877: step 16532, loss 0.0333916, acc 0.96
2016-09-06T03:34:37.836489: step 16533, loss 0.00773565, acc 1
2016-09-06T03:34:38.643642: step 16534, loss 0.0125651, acc 1
2016-09-06T03:34:39.451946: step 16535, loss 0.0171892, acc 1
2016-09-06T03:34:40.251348: step 16536, loss 0.012038, acc 1
2016-09-06T03:34:41.080318: step 16537, loss 0.0164007, acc 1
2016-09-06T03:34:41.906764: step 16538, loss 0.0239126, acc 0.98
2016-09-06T03:34:42.732789: step 16539, loss 0.00282331, acc 1
2016-09-06T03:34:43.602589: step 16540, loss 0.00873672, acc 1
2016-09-06T03:34:44.398492: step 16541, loss 0.0159993, acc 0.98
2016-09-06T03:34:45.251690: step 16542, loss 0.00754552, acc 1
2016-09-06T03:34:46.049014: step 16543, loss 0.00146211, acc 1
2016-09-06T03:34:46.874575: step 16544, loss 0.00172017, acc 1
2016-09-06T03:34:47.692935: step 16545, loss 0.0279678, acc 0.98
2016-09-06T03:34:48.516301: step 16546, loss 0.0099739, acc 1
2016-09-06T03:34:49.308610: step 16547, loss 0.00184028, acc 1
2016-09-06T03:34:50.120603: step 16548, loss 0.00707637, acc 1
2016-09-06T03:34:50.947975: step 16549, loss 0.00155342, acc 1
2016-09-06T03:34:51.753829: step 16550, loss 0.0949472, acc 0.98
2016-09-06T03:34:52.554345: step 16551, loss 0.00827336, acc 1
2016-09-06T03:34:53.374380: step 16552, loss 0.00158413, acc 1
2016-09-06T03:34:54.189074: step 16553, loss 0.029377, acc 0.98
2016-09-06T03:34:55.019328: step 16554, loss 0.00296958, acc 1
2016-09-06T03:34:55.835248: step 16555, loss 0.0149205, acc 1
2016-09-06T03:34:56.699298: step 16556, loss 0.00203521, acc 1
2016-09-06T03:34:57.509365: step 16557, loss 0.00229532, acc 1
2016-09-06T03:34:58.335767: step 16558, loss 0.00278639, acc 1
2016-09-06T03:34:59.154389: step 16559, loss 0.0123819, acc 1
2016-09-06T03:34:59.960762: step 16560, loss 0.0567799, acc 0.98
2016-09-06T03:35:00.788729: step 16561, loss 0.0862998, acc 0.98
2016-09-06T03:35:01.620193: step 16562, loss 0.00171287, acc 1
2016-09-06T03:35:02.425142: step 16563, loss 0.00753359, acc 1
2016-09-06T03:35:03.220262: step 16564, loss 0.00447302, acc 1
2016-09-06T03:35:04.095560: step 16565, loss 0.0756241, acc 0.94
2016-09-06T03:35:04.927460: step 16566, loss 0.00799719, acc 1
2016-09-06T03:35:05.739302: step 16567, loss 0.0132202, acc 1
2016-09-06T03:35:06.578613: step 16568, loss 0.00630253, acc 1
2016-09-06T03:35:07.378609: step 16569, loss 0.0170123, acc 1
2016-09-06T03:35:08.188718: step 16570, loss 0.0383702, acc 0.96
2016-09-06T03:35:09.041700: step 16571, loss 0.00683968, acc 1
2016-09-06T03:35:09.856664: step 16572, loss 0.00823372, acc 1
2016-09-06T03:35:10.671287: step 16573, loss 0.00357544, acc 1
2016-09-06T03:35:11.484272: step 16574, loss 0.00192654, acc 1
2016-09-06T03:35:12.319023: step 16575, loss 0.00240275, acc 1
2016-09-06T03:35:13.143114: step 16576, loss 0.0552369, acc 0.98
2016-09-06T03:35:13.987428: step 16577, loss 0.00239924, acc 1
2016-09-06T03:35:14.766651: step 16578, loss 0.00193558, acc 1
2016-09-06T03:35:15.601470: step 16579, loss 0.0170709, acc 0.98
2016-09-06T03:35:16.424174: step 16580, loss 0.0454695, acc 0.98
2016-09-06T03:35:17.235960: step 16581, loss 0.017361, acc 0.98
2016-09-06T03:35:18.049573: step 16582, loss 0.0019913, acc 1
2016-09-06T03:35:18.898667: step 16583, loss 0.0291446, acc 1
2016-09-06T03:35:19.727865: step 16584, loss 0.00461493, acc 1
2016-09-06T03:35:20.505648: step 16585, loss 0.00912238, acc 1
2016-09-06T03:35:21.302493: step 16586, loss 0.0481664, acc 0.98
2016-09-06T03:35:22.151535: step 16587, loss 0.0104012, acc 1
2016-09-06T03:35:22.928089: step 16588, loss 0.00652266, acc 1
2016-09-06T03:35:23.720117: step 16589, loss 0.00220713, acc 1
2016-09-06T03:35:24.537623: step 16590, loss 0.0155592, acc 1
2016-09-06T03:35:25.324297: step 16591, loss 0.0025943, acc 1
2016-09-06T03:35:26.120568: step 16592, loss 0.00217505, acc 1
2016-09-06T03:35:26.937676: step 16593, loss 0.00687731, acc 1
2016-09-06T03:35:27.714016: step 16594, loss 0.0223947, acc 0.98
2016-09-06T03:35:28.546443: step 16595, loss 0.00249402, acc 1
2016-09-06T03:35:29.381517: step 16596, loss 0.0036238, acc 1
2016-09-06T03:35:30.167136: step 16597, loss 0.0216023, acc 1
2016-09-06T03:35:30.971727: step 16598, loss 0.166898, acc 0.98
2016-09-06T03:35:31.804754: step 16599, loss 0.00704299, acc 1
2016-09-06T03:35:32.602190: step 16600, loss 0.00325501, acc 1

Evaluation:
2016-09-06T03:35:36.355160: step 16600, loss 3.19649, acc 0.707317

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-16600

2016-09-06T03:35:38.267042: step 16601, loss 0.00273492, acc 1
2016-09-06T03:35:39.081352: step 16602, loss 0.00393417, acc 1
2016-09-06T03:35:39.874289: step 16603, loss 0.0153541, acc 1
2016-09-06T03:35:40.708884: step 16604, loss 0.00245674, acc 1
2016-09-06T03:35:41.538993: step 16605, loss 0.0119295, acc 1
2016-09-06T03:35:42.352932: step 16606, loss 0.0157441, acc 1
2016-09-06T03:35:43.160442: step 16607, loss 0.00529803, acc 1
2016-09-06T03:35:43.977143: step 16608, loss 0.00258647, acc 1
2016-09-06T03:35:44.780184: step 16609, loss 0.0119413, acc 1
2016-09-06T03:35:45.590431: step 16610, loss 0.0286936, acc 1
2016-09-06T03:35:46.390812: step 16611, loss 0.00275027, acc 1
2016-09-06T03:35:47.181313: step 16612, loss 0.0108015, acc 1
2016-09-06T03:35:47.982798: step 16613, loss 0.00428053, acc 1
2016-09-06T03:35:48.832647: step 16614, loss 0.00259682, acc 1
2016-09-06T03:35:49.652660: step 16615, loss 0.00260332, acc 1
2016-09-06T03:35:50.467839: step 16616, loss 0.00283333, acc 1
2016-09-06T03:35:51.284497: step 16617, loss 0.0155265, acc 1
2016-09-06T03:35:52.121377: step 16618, loss 0.028288, acc 0.98
2016-09-06T03:35:52.895147: step 16619, loss 0.016157, acc 1
2016-09-06T03:35:53.744158: step 16620, loss 0.0452042, acc 0.96
2016-09-06T03:35:54.549499: step 16621, loss 0.031017, acc 0.98
2016-09-06T03:35:55.378546: step 16622, loss 0.00830799, acc 1
2016-09-06T03:35:56.203908: step 16623, loss 0.00257016, acc 1
2016-09-06T03:35:57.027357: step 16624, loss 0.0567505, acc 0.98
2016-09-06T03:35:57.857277: step 16625, loss 0.0192352, acc 0.98
2016-09-06T03:35:58.690186: step 16626, loss 0.00263465, acc 1
2016-09-06T03:35:59.503892: step 16627, loss 0.00376434, acc 1
2016-09-06T03:36:00.328111: step 16628, loss 0.00494482, acc 1
2016-09-06T03:36:01.162305: step 16629, loss 0.00836678, acc 1
2016-09-06T03:36:02.003279: step 16630, loss 0.00237732, acc 1
2016-09-06T03:36:02.812429: step 16631, loss 0.00357154, acc 1
2016-09-06T03:36:03.632871: step 16632, loss 0.00840837, acc 1
2016-09-06T03:36:04.440145: step 16633, loss 0.00234023, acc 1
2016-09-06T03:36:05.217945: step 16634, loss 0.0024045, acc 1
2016-09-06T03:36:06.036270: step 16635, loss 0.0208511, acc 0.98
2016-09-06T03:36:06.867417: step 16636, loss 0.0242702, acc 0.98
2016-09-06T03:36:07.667605: step 16637, loss 0.0144863, acc 1
2016-09-06T03:36:08.461235: step 16638, loss 0.00320449, acc 1
2016-09-06T03:36:09.286708: step 16639, loss 0.0273969, acc 0.98
2016-09-06T03:36:10.089464: step 16640, loss 0.00973668, acc 1
2016-09-06T03:36:10.896941: step 16641, loss 0.00260869, acc 1
2016-09-06T03:36:11.714194: step 16642, loss 0.00711035, acc 1
2016-09-06T03:36:12.483308: step 16643, loss 0.00642448, acc 1
2016-09-06T03:36:13.308805: step 16644, loss 0.00305173, acc 1
2016-09-06T03:36:14.127918: step 16645, loss 0.0186499, acc 0.98
2016-09-06T03:36:14.918308: step 16646, loss 0.00496538, acc 1
2016-09-06T03:36:15.726282: step 16647, loss 0.0170014, acc 0.98
2016-09-06T03:36:16.543080: step 16648, loss 0.00214335, acc 1
2016-09-06T03:36:17.348792: step 16649, loss 0.00280474, acc 1
2016-09-06T03:36:18.168020: step 16650, loss 0.0154125, acc 1
2016-09-06T03:36:18.972567: step 16651, loss 0.0025581, acc 1
2016-09-06T03:36:19.775937: step 16652, loss 0.00225855, acc 1
2016-09-06T03:36:20.567832: step 16653, loss 0.0153918, acc 1
2016-09-06T03:36:21.434224: step 16654, loss 0.00235454, acc 1
2016-09-06T03:36:22.228648: step 16655, loss 0.00584102, acc 1
2016-09-06T03:36:23.017796: step 16656, loss 0.0131048, acc 1
2016-09-06T03:36:23.829786: step 16657, loss 0.0021026, acc 1
2016-09-06T03:36:24.595844: step 16658, loss 0.0237494, acc 1
2016-09-06T03:36:25.391433: step 16659, loss 0.0191661, acc 0.98
2016-09-06T03:36:26.194150: step 16660, loss 0.00703877, acc 1
2016-09-06T03:36:26.977401: step 16661, loss 0.00216888, acc 1
2016-09-06T03:36:27.796175: step 16662, loss 0.0246283, acc 1
2016-09-06T03:36:28.627682: step 16663, loss 0.00279718, acc 1
2016-09-06T03:36:29.416406: step 16664, loss 0.00213437, acc 1
2016-09-06T03:36:30.236015: step 16665, loss 0.0360534, acc 0.98
2016-09-06T03:36:31.069618: step 16666, loss 0.00210995, acc 1
2016-09-06T03:36:31.830711: step 16667, loss 0.0176782, acc 0.98
2016-09-06T03:36:32.641566: step 16668, loss 0.00937652, acc 1
2016-09-06T03:36:33.450809: step 16669, loss 0.0211539, acc 0.98
2016-09-06T03:36:34.233435: step 16670, loss 0.00204604, acc 1
2016-09-06T03:36:35.055547: step 16671, loss 0.00202955, acc 1
2016-09-06T03:36:35.867196: step 16672, loss 0.028059, acc 0.98
2016-09-06T03:36:36.645424: step 16673, loss 0.0381046, acc 0.98
2016-09-06T03:36:37.456181: step 16674, loss 0.00956962, acc 1
2016-09-06T03:36:38.292517: step 16675, loss 0.00463018, acc 1
2016-09-06T03:36:39.084079: step 16676, loss 0.00601733, acc 1
2016-09-06T03:36:39.883666: step 16677, loss 0.00238492, acc 1
2016-09-06T03:36:40.703568: step 16678, loss 0.00375024, acc 1
2016-09-06T03:36:41.522301: step 16679, loss 0.00208028, acc 1
2016-09-06T03:36:42.330118: step 16680, loss 0.00196115, acc 1
2016-09-06T03:36:43.128334: step 16681, loss 0.0273367, acc 1
2016-09-06T03:36:43.925348: step 16682, loss 0.00269466, acc 1
2016-09-06T03:36:44.725549: step 16683, loss 0.00493191, acc 1
2016-09-06T03:36:45.524664: step 16684, loss 0.00741666, acc 1
2016-09-06T03:36:46.315890: step 16685, loss 0.00199368, acc 1
2016-09-06T03:36:47.125392: step 16686, loss 0.0300811, acc 0.98
2016-09-06T03:36:47.928281: step 16687, loss 0.0782606, acc 0.96
2016-09-06T03:36:48.738208: step 16688, loss 0.00192061, acc 1
2016-09-06T03:36:49.543422: step 16689, loss 0.00199005, acc 1
2016-09-06T03:36:50.350162: step 16690, loss 0.0195253, acc 0.98
2016-09-06T03:36:51.171745: step 16691, loss 0.00206257, acc 1
2016-09-06T03:36:51.968552: step 16692, loss 0.098294, acc 0.94
2016-09-06T03:36:52.760903: step 16693, loss 0.00600879, acc 1
2016-09-06T03:36:53.570650: step 16694, loss 0.00250753, acc 1
2016-09-06T03:36:54.395570: step 16695, loss 0.00444866, acc 1
2016-09-06T03:36:55.196490: step 16696, loss 0.0443117, acc 0.96
2016-09-06T03:36:55.970317: step 16697, loss 0.00404723, acc 1
2016-09-06T03:36:56.772541: step 16698, loss 0.0171096, acc 0.98
2016-09-06T03:36:57.576286: step 16699, loss 0.00262088, acc 1
2016-09-06T03:36:58.381052: step 16700, loss 0.00181147, acc 1

Evaluation:
2016-09-06T03:37:02.147240: step 16700, loss 2.96183, acc 0.704503

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-16700

2016-09-06T03:37:04.004163: step 16701, loss 0.0496894, acc 0.96
2016-09-06T03:37:04.816340: step 16702, loss 0.0104822, acc 1
2016-09-06T03:37:05.651889: step 16703, loss 0.0174939, acc 0.98
2016-09-06T03:37:06.426718: step 16704, loss 0.00158926, acc 1
2016-09-06T03:37:07.246221: step 16705, loss 0.00557003, acc 1
2016-09-06T03:37:08.058862: step 16706, loss 0.018038, acc 0.98
2016-09-06T03:37:08.864175: step 16707, loss 0.00534999, acc 1
2016-09-06T03:37:09.685506: step 16708, loss 0.00565385, acc 1
2016-09-06T03:37:10.504878: step 16709, loss 0.00169268, acc 1
2016-09-06T03:37:11.322512: step 16710, loss 0.00199351, acc 1
2016-09-06T03:37:12.123502: step 16711, loss 0.00162921, acc 1
2016-09-06T03:37:12.940799: step 16712, loss 0.0159038, acc 0.98
2016-09-06T03:37:13.801720: step 16713, loss 0.0262813, acc 0.98
2016-09-06T03:37:14.638464: step 16714, loss 0.00164221, acc 1
2016-09-06T03:37:15.419107: step 16715, loss 0.0170992, acc 1
2016-09-06T03:37:16.225525: step 16716, loss 0.0167834, acc 0.98
2016-09-06T03:37:17.069029: step 16717, loss 0.0017712, acc 1
2016-09-06T03:37:17.853762: step 16718, loss 0.0199561, acc 0.98
2016-09-06T03:37:18.679539: step 16719, loss 0.00178201, acc 1
2016-09-06T03:37:19.496470: step 16720, loss 0.0214249, acc 1
2016-09-06T03:37:20.294385: step 16721, loss 0.00952163, acc 1
2016-09-06T03:37:21.099783: step 16722, loss 0.00162029, acc 1
2016-09-06T03:37:21.907186: step 16723, loss 0.00161611, acc 1
2016-09-06T03:37:22.690566: step 16724, loss 0.0138438, acc 1
2016-09-06T03:37:23.485620: step 16725, loss 0.00360186, acc 1
2016-09-06T03:37:24.302848: step 16726, loss 0.0017272, acc 1
2016-09-06T03:37:25.068482: step 16727, loss 0.00183837, acc 1
2016-09-06T03:37:25.882447: step 16728, loss 0.0165778, acc 1
2016-09-06T03:37:26.713478: step 16729, loss 0.0450105, acc 0.98
2016-09-06T03:37:27.513602: step 16730, loss 0.0481975, acc 0.98
2016-09-06T03:37:28.320028: step 16731, loss 0.0118551, acc 1
2016-09-06T03:37:29.135733: step 16732, loss 0.00442023, acc 1
2016-09-06T03:37:29.895999: step 16733, loss 0.0186938, acc 0.98
2016-09-06T03:37:30.725039: step 16734, loss 0.00149941, acc 1
2016-09-06T03:37:31.556351: step 16735, loss 0.00155408, acc 1
2016-09-06T03:37:32.324025: step 16736, loss 0.0239958, acc 0.98
2016-09-06T03:37:33.140444: step 16737, loss 0.0300366, acc 0.98
2016-09-06T03:37:33.983784: step 16738, loss 0.00693063, acc 1
2016-09-06T03:37:34.801365: step 16739, loss 0.00468834, acc 1
2016-09-06T03:37:35.604311: step 16740, loss 0.0137765, acc 1
2016-09-06T03:37:36.424911: step 16741, loss 0.00140146, acc 1
2016-09-06T03:37:37.213157: step 16742, loss 0.0100103, acc 1
2016-09-06T03:37:38.008631: step 16743, loss 0.0132268, acc 1
2016-09-06T03:37:38.839056: step 16744, loss 0.00196012, acc 1
2016-09-06T03:37:39.663434: step 16745, loss 0.00146173, acc 1
2016-09-06T03:37:40.455887: step 16746, loss 0.00211662, acc 1
2016-09-06T03:37:41.291118: step 16747, loss 0.033637, acc 1
2016-09-06T03:37:42.090700: step 16748, loss 0.00152647, acc 1
2016-09-06T03:37:42.906761: step 16749, loss 0.00147832, acc 1
2016-09-06T03:37:43.733361: step 16750, loss 0.00164658, acc 1
2016-09-06T03:37:44.537488: step 16751, loss 0.0627577, acc 0.96
2016-09-06T03:37:45.361376: step 16752, loss 0.0143694, acc 1
2016-09-06T03:37:46.181364: step 16753, loss 0.0251697, acc 1
2016-09-06T03:37:46.980054: step 16754, loss 0.00147553, acc 1
2016-09-06T03:37:47.782917: step 16755, loss 0.0342073, acc 0.98
2016-09-06T03:37:48.611882: step 16756, loss 0.0210598, acc 1
2016-09-06T03:37:49.426358: step 16757, loss 0.022078, acc 0.98
2016-09-06T03:37:50.249704: step 16758, loss 0.00433487, acc 1
2016-09-06T03:37:51.092021: step 16759, loss 0.00277457, acc 1
2016-09-06T03:37:51.917964: step 16760, loss 0.00454974, acc 1
2016-09-06T03:37:52.728759: step 16761, loss 0.00257419, acc 1
2016-09-06T03:37:53.566002: step 16762, loss 0.0289088, acc 0.98
2016-09-06T03:37:54.375257: step 16763, loss 0.00165618, acc 1
2016-09-06T03:37:55.198434: step 16764, loss 0.00174262, acc 1
2016-09-06T03:37:56.041899: step 16765, loss 0.027366, acc 0.98
2016-09-06T03:37:56.847943: step 16766, loss 0.00192886, acc 1
2016-09-06T03:37:57.634849: step 16767, loss 0.00172405, acc 1
2016-09-06T03:37:58.457563: step 16768, loss 0.026965, acc 0.98
2016-09-06T03:37:59.289415: step 16769, loss 0.00197741, acc 1
2016-09-06T03:38:00.080144: step 16770, loss 0.00171303, acc 1
2016-09-06T03:38:00.923517: step 16771, loss 0.0110533, acc 1
2016-09-06T03:38:01.762007: step 16772, loss 0.0737229, acc 0.96
2016-09-06T03:38:02.553306: step 16773, loss 0.00210486, acc 1
2016-09-06T03:38:03.356606: step 16774, loss 0.015229, acc 1
2016-09-06T03:38:04.201842: step 16775, loss 0.0159968, acc 0.98
2016-09-06T03:38:04.972942: step 16776, loss 0.00173801, acc 1
2016-09-06T03:38:05.774964: step 16777, loss 0.00851289, acc 1
2016-09-06T03:38:06.610306: step 16778, loss 0.0015281, acc 1
2016-09-06T03:38:07.420251: step 16779, loss 0.00196465, acc 1
2016-09-06T03:38:08.233691: step 16780, loss 0.0186461, acc 1
2016-09-06T03:38:09.054896: step 16781, loss 0.0136456, acc 1
2016-09-06T03:38:09.846395: step 16782, loss 0.0248362, acc 0.98
2016-09-06T03:38:10.641352: step 16783, loss 0.0197265, acc 0.98
2016-09-06T03:38:11.429335: step 16784, loss 0.00237113, acc 1
2016-09-06T03:38:12.221142: step 16785, loss 0.00271108, acc 1
2016-09-06T03:38:13.006299: step 16786, loss 0.00149906, acc 1
2016-09-06T03:38:13.811817: step 16787, loss 0.0179886, acc 0.98
2016-09-06T03:38:14.598019: step 16788, loss 0.00168229, acc 1
2016-09-06T03:38:15.410645: step 16789, loss 0.00662854, acc 1
2016-09-06T03:38:16.231301: step 16790, loss 0.0168275, acc 1
2016-09-06T03:38:17.017800: step 16791, loss 0.00192325, acc 1
2016-09-06T03:38:17.834401: step 16792, loss 0.00442915, acc 1
2016-09-06T03:38:18.660814: step 16793, loss 0.00841923, acc 1
2016-09-06T03:38:19.477422: step 16794, loss 0.00150655, acc 1
2016-09-06T03:38:20.273380: step 16795, loss 0.00242762, acc 1
2016-09-06T03:38:21.091043: step 16796, loss 0.00193085, acc 1
2016-09-06T03:38:21.861875: step 16797, loss 0.00260794, acc 1
2016-09-06T03:38:22.668049: step 16798, loss 0.00249174, acc 1
2016-09-06T03:38:23.486981: step 16799, loss 0.00625466, acc 1
2016-09-06T03:38:24.276581: step 16800, loss 0.00158794, acc 1

Evaluation:
2016-09-06T03:38:27.956136: step 16800, loss 2.84414, acc 0.701689

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-16800

2016-09-06T03:38:29.958520: step 16801, loss 0.0019392, acc 1
2016-09-06T03:38:30.764525: step 16802, loss 0.0111533, acc 1
2016-09-06T03:38:31.584199: step 16803, loss 0.0116232, acc 1
2016-09-06T03:38:32.417423: step 16804, loss 0.0265444, acc 0.98
2016-09-06T03:38:33.215183: step 16805, loss 0.0062747, acc 1
2016-09-06T03:38:34.049396: step 16806, loss 0.00242857, acc 1
2016-09-06T03:38:34.853412: step 16807, loss 0.00279575, acc 1
2016-09-06T03:38:35.657738: step 16808, loss 0.00178605, acc 1
2016-09-06T03:38:36.490783: step 16809, loss 0.0275928, acc 1
2016-09-06T03:38:37.299590: step 16810, loss 0.00162355, acc 1
2016-09-06T03:38:38.108241: step 16811, loss 0.0184842, acc 0.98
2016-09-06T03:38:38.922516: step 16812, loss 0.0459537, acc 0.96
2016-09-06T03:38:39.747805: step 16813, loss 0.0336137, acc 0.96
2016-09-06T03:38:40.588669: step 16814, loss 0.00900609, acc 1
2016-09-06T03:38:41.365200: step 16815, loss 0.00159592, acc 1
2016-09-06T03:38:42.188148: step 16816, loss 0.00512697, acc 1
2016-09-06T03:38:43.000406: step 16817, loss 0.00464245, acc 1
2016-09-06T03:38:43.790300: step 16818, loss 0.00162786, acc 1
2016-09-06T03:38:44.583834: step 16819, loss 0.00472723, acc 1
2016-09-06T03:38:45.398191: step 16820, loss 0.0153224, acc 1
2016-09-06T03:38:46.181712: step 16821, loss 0.00323086, acc 1
2016-09-06T03:38:46.955976: step 16822, loss 0.00160136, acc 1
2016-09-06T03:38:47.745949: step 16823, loss 0.0105729, acc 1
2016-09-06T03:38:48.553436: step 16824, loss 0.00199624, acc 1
2016-09-06T03:38:49.371255: step 16825, loss 0.00165774, acc 1
2016-09-06T03:38:50.153754: step 16826, loss 0.00241902, acc 1
2016-09-06T03:38:50.936148: step 16827, loss 0.0257756, acc 0.98
2016-09-06T03:38:51.781772: step 16828, loss 0.0029745, acc 1
2016-09-06T03:38:52.590693: step 16829, loss 0.033689, acc 0.98
2016-09-06T03:38:53.398944: step 16830, loss 0.00199801, acc 1
2016-09-06T03:38:54.221637: step 16831, loss 0.030033, acc 0.98
2016-09-06T03:38:55.049201: step 16832, loss 0.00188013, acc 1
2016-09-06T03:38:55.833015: step 16833, loss 0.0134776, acc 1
2016-09-06T03:38:56.635011: step 16834, loss 0.00266561, acc 1
2016-09-06T03:38:57.437446: step 16835, loss 0.0184331, acc 0.98
2016-09-06T03:38:58.260594: step 16836, loss 0.00157141, acc 1
2016-09-06T03:38:59.058357: step 16837, loss 0.0120856, acc 1
2016-09-06T03:38:59.867222: step 16838, loss 0.00420242, acc 1
2016-09-06T03:39:00.680849: step 16839, loss 0.0351452, acc 0.98
2016-09-06T03:39:01.499625: step 16840, loss 0.00600525, acc 1
2016-09-06T03:39:02.342444: step 16841, loss 0.00481871, acc 1
2016-09-06T03:39:03.120312: step 16842, loss 0.0162188, acc 0.98
2016-09-06T03:39:03.921672: step 16843, loss 0.00154433, acc 1
2016-09-06T03:39:04.775578: step 16844, loss 0.0014564, acc 1
2016-09-06T03:39:05.554235: step 16845, loss 0.00158254, acc 1
2016-09-06T03:39:06.324108: step 16846, loss 0.0154938, acc 0.98
2016-09-06T03:39:07.156822: step 16847, loss 0.0018209, acc 1
2016-09-06T03:39:07.955966: step 16848, loss 0.011548, acc 1
2016-09-06T03:39:08.747666: step 16849, loss 0.00142897, acc 1
2016-09-06T03:39:09.559354: step 16850, loss 0.00214053, acc 1
2016-09-06T03:39:10.355378: step 16851, loss 0.00145626, acc 1
2016-09-06T03:39:11.159368: step 16852, loss 0.00145787, acc 1
2016-09-06T03:39:11.959548: step 16853, loss 0.00514115, acc 1
2016-09-06T03:39:12.747872: step 16854, loss 0.00177238, acc 1
2016-09-06T03:39:13.534838: step 16855, loss 0.00141194, acc 1
2016-09-06T03:39:14.320508: step 16856, loss 0.00137932, acc 1
2016-09-06T03:39:15.106225: step 16857, loss 0.0286402, acc 0.98
2016-09-06T03:39:15.937437: step 16858, loss 0.0013587, acc 1
2016-09-06T03:39:16.742807: step 16859, loss 0.00138727, acc 1
2016-09-06T03:39:17.561401: step 16860, loss 0.0236929, acc 0.98
2016-09-06T03:39:18.364293: step 16861, loss 0.00129131, acc 1
2016-09-06T03:39:19.158659: step 16862, loss 0.00160847, acc 1
2016-09-06T03:39:19.966312: step 16863, loss 0.00761716, acc 1
2016-09-06T03:39:20.775888: step 16864, loss 0.0348508, acc 0.98
2016-09-06T03:39:21.599925: step 16865, loss 0.0241798, acc 1
2016-09-06T03:39:22.413692: step 16866, loss 0.00278801, acc 1
2016-09-06T03:39:23.217996: step 16867, loss 0.00123303, acc 1
2016-09-06T03:39:24.023381: step 16868, loss 0.0489587, acc 0.98
2016-09-06T03:39:24.816704: step 16869, loss 0.00223299, acc 1
2016-09-06T03:39:25.622408: step 16870, loss 0.0295459, acc 0.98
2016-09-06T03:39:26.456615: step 16871, loss 0.00121288, acc 1
2016-09-06T03:39:27.238060: step 16872, loss 0.00408665, acc 1
2016-09-06T03:39:28.043866: step 16873, loss 0.0325181, acc 0.98
2016-09-06T03:39:28.862134: step 16874, loss 0.0504623, acc 0.98
2016-09-06T03:39:29.661058: step 16875, loss 0.00126706, acc 1
2016-09-06T03:39:30.467254: step 16876, loss 0.0042811, acc 1
2016-09-06T03:39:31.263296: step 16877, loss 0.00397097, acc 1
2016-09-06T03:39:32.048125: step 16878, loss 0.00921095, acc 1
2016-09-06T03:39:32.852277: step 16879, loss 0.0188026, acc 1
2016-09-06T03:39:33.662828: step 16880, loss 0.0012803, acc 1
2016-09-06T03:39:34.460379: step 16881, loss 0.00945521, acc 1
2016-09-06T03:39:35.279292: step 16882, loss 0.0127595, acc 1
2016-09-06T03:39:36.084580: step 16883, loss 0.0171196, acc 0.98
2016-09-06T03:39:36.897369: step 16884, loss 0.00736854, acc 1
2016-09-06T03:39:37.710077: step 16885, loss 0.00161585, acc 1
2016-09-06T03:39:38.501911: step 16886, loss 0.00147491, acc 1
2016-09-06T03:39:39.294806: step 16887, loss 0.00142925, acc 1
2016-09-06T03:39:40.121497: step 16888, loss 0.0128166, acc 1
2016-09-06T03:39:40.912437: step 16889, loss 0.0136017, acc 1
2016-09-06T03:39:41.713892: step 16890, loss 0.0164106, acc 1
2016-09-06T03:39:42.536670: step 16891, loss 0.0089822, acc 1
2016-09-06T03:39:43.337824: step 16892, loss 0.0466995, acc 0.98
2016-09-06T03:39:44.157274: step 16893, loss 0.0455387, acc 0.98
2016-09-06T03:39:44.956782: step 16894, loss 0.0572667, acc 0.98
2016-09-06T03:39:45.740098: step 16895, loss 0.0225821, acc 1
2016-09-06T03:39:46.482112: step 16896, loss 0.00299318, acc 1
2016-09-06T03:39:47.303475: step 16897, loss 0.00909522, acc 1
2016-09-06T03:39:48.109307: step 16898, loss 0.0193072, acc 0.98
2016-09-06T03:39:48.930028: step 16899, loss 0.00274357, acc 1
2016-09-06T03:39:49.782267: step 16900, loss 0.00315577, acc 1

Evaluation:
2016-09-06T03:39:53.579377: step 16900, loss 2.70914, acc 0.705441

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-16900

2016-09-06T03:39:55.527612: step 16901, loss 0.00146348, acc 1
2016-09-06T03:39:56.291457: step 16902, loss 0.00400298, acc 1
2016-09-06T03:39:57.115296: step 16903, loss 0.00159078, acc 1
2016-09-06T03:39:57.940308: step 16904, loss 0.00172774, acc 1
2016-09-06T03:39:58.709752: step 16905, loss 0.00877026, acc 1
2016-09-06T03:39:59.516996: step 16906, loss 0.00147392, acc 1
2016-09-06T03:40:00.353409: step 16907, loss 0.0223112, acc 0.98
2016-09-06T03:40:01.112958: step 16908, loss 0.00245868, acc 1
2016-09-06T03:40:01.918100: step 16909, loss 0.0018512, acc 1
2016-09-06T03:40:02.721749: step 16910, loss 0.00167356, acc 1
2016-09-06T03:40:03.502034: step 16911, loss 0.00148982, acc 1
2016-09-06T03:40:04.330290: step 16912, loss 0.0311746, acc 1
2016-09-06T03:40:05.210157: step 16913, loss 0.116267, acc 0.98
2016-09-06T03:40:05.991275: step 16914, loss 0.0101686, acc 1
2016-09-06T03:40:06.770707: step 16915, loss 0.00145113, acc 1
2016-09-06T03:40:07.607392: step 16916, loss 0.00231816, acc 1
2016-09-06T03:40:08.377184: step 16917, loss 0.0159688, acc 0.98
2016-09-06T03:40:09.183998: step 16918, loss 0.00243402, acc 1
2016-09-06T03:40:10.036834: step 16919, loss 0.0337754, acc 0.98
2016-09-06T03:40:10.809642: step 16920, loss 0.00219408, acc 1
2016-09-06T03:40:11.629633: step 16921, loss 0.0102487, acc 1
2016-09-06T03:40:12.456126: step 16922, loss 0.0264516, acc 1
2016-09-06T03:40:13.226225: step 16923, loss 0.0149094, acc 1
2016-09-06T03:40:14.013330: step 16924, loss 0.00235511, acc 1
2016-09-06T03:40:14.830486: step 16925, loss 0.022349, acc 0.98
2016-09-06T03:40:15.645870: step 16926, loss 0.00192345, acc 1
2016-09-06T03:40:16.460964: step 16927, loss 0.00580152, acc 1
2016-09-06T03:40:17.245100: step 16928, loss 0.00285495, acc 1
2016-09-06T03:40:18.024039: step 16929, loss 0.0226507, acc 0.98
2016-09-06T03:40:18.830001: step 16930, loss 0.00743863, acc 1
2016-09-06T03:40:19.648004: step 16931, loss 0.00177906, acc 1
2016-09-06T03:40:20.444674: step 16932, loss 0.0615059, acc 0.98
2016-09-06T03:40:21.254965: step 16933, loss 0.00553958, acc 1
2016-09-06T03:40:22.053887: step 16934, loss 0.00536365, acc 1
2016-09-06T03:40:22.828331: step 16935, loss 0.00999052, acc 1
2016-09-06T03:40:23.643654: step 16936, loss 0.00194134, acc 1
2016-09-06T03:40:24.436193: step 16937, loss 0.0423099, acc 0.98
2016-09-06T03:40:25.224168: step 16938, loss 0.00185611, acc 1
2016-09-06T03:40:26.022051: step 16939, loss 0.0953275, acc 0.96
2016-09-06T03:40:26.825647: step 16940, loss 0.00214319, acc 1
2016-09-06T03:40:27.624444: step 16941, loss 0.0018053, acc 1
2016-09-06T03:40:28.446106: step 16942, loss 0.0156856, acc 1
2016-09-06T03:40:29.260292: step 16943, loss 0.00286583, acc 1
2016-09-06T03:40:30.034095: step 16944, loss 0.00841938, acc 1
2016-09-06T03:40:30.871613: step 16945, loss 0.00259658, acc 1
2016-09-06T03:40:31.650649: step 16946, loss 0.00197685, acc 1
2016-09-06T03:40:32.462582: step 16947, loss 0.00295105, acc 1
2016-09-06T03:40:33.293646: step 16948, loss 0.0125418, acc 1
2016-09-06T03:40:34.106149: step 16949, loss 0.0407409, acc 0.96
2016-09-06T03:40:34.921137: step 16950, loss 0.00291566, acc 1
2016-09-06T03:40:35.754958: step 16951, loss 0.00185173, acc 1
2016-09-06T03:40:36.561999: step 16952, loss 0.00202458, acc 1
2016-09-06T03:40:37.347831: step 16953, loss 0.00810348, acc 1
2016-09-06T03:40:38.169312: step 16954, loss 0.0314487, acc 0.98
2016-09-06T03:40:39.007293: step 16955, loss 0.00484448, acc 1
2016-09-06T03:40:39.812907: step 16956, loss 0.00185833, acc 1
2016-09-06T03:40:40.620268: step 16957, loss 0.0813923, acc 0.96
2016-09-06T03:40:41.458104: step 16958, loss 0.00773036, acc 1
2016-09-06T03:40:42.222989: step 16959, loss 0.00182858, acc 1
2016-09-06T03:40:43.024361: step 16960, loss 0.00640862, acc 1
2016-09-06T03:40:43.854414: step 16961, loss 0.00813244, acc 1
2016-09-06T03:40:44.658985: step 16962, loss 0.0127212, acc 1
2016-09-06T03:40:45.442201: step 16963, loss 0.01869, acc 0.98
2016-09-06T03:40:46.247411: step 16964, loss 0.0359493, acc 0.98
2016-09-06T03:40:47.044852: step 16965, loss 0.00253786, acc 1
2016-09-06T03:40:47.841693: step 16966, loss 0.0088727, acc 1
2016-09-06T03:40:48.643362: step 16967, loss 0.0136453, acc 1
2016-09-06T03:40:49.414427: step 16968, loss 0.00498046, acc 1
2016-09-06T03:40:50.256319: step 16969, loss 0.00357525, acc 1
2016-09-06T03:40:51.101448: step 16970, loss 0.00543011, acc 1
2016-09-06T03:40:51.907031: step 16971, loss 0.0208088, acc 0.98
2016-09-06T03:40:52.702674: step 16972, loss 0.010445, acc 1
2016-09-06T03:40:53.526264: step 16973, loss 0.0111369, acc 1
2016-09-06T03:40:54.291289: step 16974, loss 0.0174718, acc 1
2016-09-06T03:40:55.090920: step 16975, loss 0.0528805, acc 0.98
2016-09-06T03:40:55.941165: step 16976, loss 0.0161038, acc 1
2016-09-06T03:40:56.722370: step 16977, loss 0.00219109, acc 1
2016-09-06T03:40:57.509907: step 16978, loss 0.0243522, acc 0.98
2016-09-06T03:40:58.333853: step 16979, loss 0.0179265, acc 1
2016-09-06T03:40:59.129698: step 16980, loss 0.00223691, acc 1
2016-09-06T03:40:59.927418: step 16981, loss 0.00226676, acc 1
2016-09-06T03:41:00.795800: step 16982, loss 0.036942, acc 0.96
2016-09-06T03:41:01.589842: step 16983, loss 0.0023535, acc 1
2016-09-06T03:41:02.394321: step 16984, loss 0.00909465, acc 1
2016-09-06T03:41:03.193420: step 16985, loss 0.0423724, acc 0.98
2016-09-06T03:41:03.981303: step 16986, loss 0.00306775, acc 1
2016-09-06T03:41:04.779587: step 16987, loss 0.0221921, acc 0.98
2016-09-06T03:41:05.612633: step 16988, loss 0.15071, acc 0.98
2016-09-06T03:41:06.405404: step 16989, loss 0.0373781, acc 0.96
2016-09-06T03:41:07.181760: step 16990, loss 0.00227733, acc 1
2016-09-06T03:41:07.999650: step 16991, loss 0.0543089, acc 0.98
2016-09-06T03:41:08.796342: step 16992, loss 0.00207245, acc 1
2016-09-06T03:41:09.602590: step 16993, loss 0.00263375, acc 1
2016-09-06T03:41:10.427983: step 16994, loss 0.00734007, acc 1
2016-09-06T03:41:11.209577: step 16995, loss 0.00230577, acc 1
2016-09-06T03:41:11.997593: step 16996, loss 0.0163948, acc 1
2016-09-06T03:41:12.789422: step 16997, loss 0.0106638, acc 1
2016-09-06T03:41:13.604211: step 16998, loss 0.0433842, acc 0.98
2016-09-06T03:41:14.444004: step 16999, loss 0.012459, acc 1
2016-09-06T03:41:15.258703: step 17000, loss 0.00364925, acc 1

Evaluation:
2016-09-06T03:41:19.013621: step 17000, loss 3.16868, acc 0.699812

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-17000

2016-09-06T03:41:20.796991: step 17001, loss 0.0239368, acc 0.98
2016-09-06T03:41:21.647940: step 17002, loss 0.00320749, acc 1
2016-09-06T03:41:22.475423: step 17003, loss 0.00453974, acc 1
2016-09-06T03:41:23.254202: step 17004, loss 0.0225122, acc 0.98
2016-09-06T03:41:24.088239: step 17005, loss 0.0343153, acc 0.96
2016-09-06T03:41:24.922443: step 17006, loss 0.0350652, acc 1
2016-09-06T03:41:25.725101: step 17007, loss 0.00310526, acc 1
2016-09-06T03:41:26.560220: step 17008, loss 0.0193727, acc 0.98
2016-09-06T03:41:27.410887: step 17009, loss 0.00361708, acc 1
2016-09-06T03:41:28.235227: step 17010, loss 0.00343152, acc 1
2016-09-06T03:41:29.060693: step 17011, loss 0.0379571, acc 0.98
2016-09-06T03:41:29.911391: step 17012, loss 0.00582365, acc 1
2016-09-06T03:41:30.795859: step 17013, loss 0.00831711, acc 1
2016-09-06T03:41:31.595565: step 17014, loss 0.00355024, acc 1
2016-09-06T03:41:32.444949: step 17015, loss 0.00357923, acc 1
2016-09-06T03:41:33.349847: step 17016, loss 0.0195345, acc 0.98
2016-09-06T03:41:34.217263: step 17017, loss 0.0036167, acc 1
2016-09-06T03:41:35.024766: step 17018, loss 0.00362298, acc 1
2016-09-06T03:41:36.036781: step 17019, loss 0.00362518, acc 1
2016-09-06T03:41:36.970029: step 17020, loss 0.0309934, acc 0.98
2016-09-06T03:41:37.778793: step 17021, loss 0.0206957, acc 0.98
2016-09-06T03:41:38.594611: step 17022, loss 0.0035941, acc 1
2016-09-06T03:41:39.425820: step 17023, loss 0.0173591, acc 1
2016-09-06T03:41:40.257327: step 17024, loss 0.0215547, acc 0.98
2016-09-06T03:41:41.294150: step 17025, loss 0.00349144, acc 1
2016-09-06T03:41:42.178224: step 17026, loss 0.00351678, acc 1
2016-09-06T03:41:43.078684: step 17027, loss 0.00343692, acc 1
2016-09-06T03:41:43.957484: step 17028, loss 0.156222, acc 0.94
2016-09-06T03:41:44.934600: step 17029, loss 0.00337657, acc 1
2016-09-06T03:41:45.897158: step 17030, loss 0.0211544, acc 0.98
2016-09-06T03:41:46.857567: step 17031, loss 0.0130545, acc 1
2016-09-06T03:41:47.763400: step 17032, loss 0.00324538, acc 1
2016-09-06T03:41:48.609912: step 17033, loss 0.00301149, acc 1
2016-09-06T03:41:49.486973: step 17034, loss 0.00367106, acc 1
2016-09-06T03:41:50.429421: step 17035, loss 0.00852707, acc 1
2016-09-06T03:41:51.256377: step 17036, loss 0.00318199, acc 1
2016-09-06T03:41:52.135906: step 17037, loss 0.0143483, acc 1
2016-09-06T03:41:52.985161: step 17038, loss 0.00349109, acc 1
2016-09-06T03:41:53.803780: step 17039, loss 0.00547107, acc 1
2016-09-06T03:41:54.608057: step 17040, loss 0.0260946, acc 1
2016-09-06T03:41:55.433610: step 17041, loss 0.00292286, acc 1
2016-09-06T03:41:56.296165: step 17042, loss 0.00270171, acc 1
2016-09-06T03:41:57.120574: step 17043, loss 0.00619684, acc 1
2016-09-06T03:41:57.957536: step 17044, loss 0.00290729, acc 1
2016-09-06T03:41:58.782036: step 17045, loss 0.0147007, acc 1
2016-09-06T03:41:59.575295: step 17046, loss 0.0737613, acc 0.96
2016-09-06T03:42:00.424917: step 17047, loss 0.00257976, acc 1
2016-09-06T03:42:01.220183: step 17048, loss 0.0113169, acc 1
2016-09-06T03:42:01.993837: step 17049, loss 0.00403996, acc 1
2016-09-06T03:42:02.845063: step 17050, loss 0.00273585, acc 1
2016-09-06T03:42:03.708199: step 17051, loss 0.00358295, acc 1
2016-09-06T03:42:04.522199: step 17052, loss 0.00691298, acc 1
2016-09-06T03:42:05.324098: step 17053, loss 0.00270646, acc 1
2016-09-06T03:42:06.188967: step 17054, loss 0.00259439, acc 1
2016-09-06T03:42:07.050979: step 17055, loss 0.00613873, acc 1
2016-09-06T03:42:07.846812: step 17056, loss 0.0133597, acc 1
2016-09-06T03:42:08.692480: step 17057, loss 0.0288228, acc 1
2016-09-06T03:42:09.500881: step 17058, loss 0.0182195, acc 0.98
2016-09-06T03:42:10.307385: step 17059, loss 0.00272478, acc 1
2016-09-06T03:42:11.131057: step 17060, loss 0.00245393, acc 1
2016-09-06T03:42:11.953554: step 17061, loss 0.0387181, acc 0.98
2016-09-06T03:42:12.759526: step 17062, loss 0.0452885, acc 0.96
2016-09-06T03:42:13.576931: step 17063, loss 0.00829807, acc 1
2016-09-06T03:42:14.397704: step 17064, loss 0.0212174, acc 0.98
2016-09-06T03:42:15.229924: step 17065, loss 0.0514916, acc 0.96
2016-09-06T03:42:16.061049: step 17066, loss 0.00279426, acc 1
2016-09-06T03:42:16.906438: step 17067, loss 0.051077, acc 0.98
2016-09-06T03:42:17.698926: step 17068, loss 0.0263188, acc 0.98
2016-09-06T03:42:18.494581: step 17069, loss 0.002406, acc 1
2016-09-06T03:42:19.297491: step 17070, loss 0.0176251, acc 1
2016-09-06T03:42:20.074125: step 17071, loss 0.0314084, acc 0.98
2016-09-06T03:42:20.922000: step 17072, loss 0.00200556, acc 1
2016-09-06T03:42:21.775959: step 17073, loss 0.0138672, acc 1
2016-09-06T03:42:22.576270: step 17074, loss 0.0935469, acc 0.94
2016-09-06T03:42:23.398690: step 17075, loss 0.00447298, acc 1
2016-09-06T03:42:24.207639: step 17076, loss 0.00978046, acc 1
2016-09-06T03:42:24.968669: step 17077, loss 0.0107011, acc 1
2016-09-06T03:42:25.774874: step 17078, loss 0.00204702, acc 1
2016-09-06T03:42:26.601397: step 17079, loss 0.0209972, acc 0.98
2016-09-06T03:42:27.372480: step 17080, loss 0.00367257, acc 1
2016-09-06T03:42:28.181823: step 17081, loss 0.00976853, acc 1
2016-09-06T03:42:29.027905: step 17082, loss 0.0180696, acc 1
2016-09-06T03:42:29.798573: step 17083, loss 0.00365148, acc 1
2016-09-06T03:42:30.597853: step 17084, loss 0.00198099, acc 1
2016-09-06T03:42:31.441150: step 17085, loss 0.0215771, acc 0.98
2016-09-06T03:42:32.220781: step 17086, loss 0.00208817, acc 1
2016-09-06T03:42:33.026184: step 17087, loss 0.00417409, acc 1
2016-09-06T03:42:33.813118: step 17088, loss 0.00204238, acc 1
2016-09-06T03:42:34.608314: step 17089, loss 0.00342602, acc 1
2016-09-06T03:42:35.393501: step 17090, loss 0.0174015, acc 0.98
2016-09-06T03:42:36.201413: step 17091, loss 0.00182825, acc 1
2016-09-06T03:42:37.001477: step 17092, loss 0.0588401, acc 0.98
2016-09-06T03:42:37.816199: step 17093, loss 0.00706087, acc 1
2016-09-06T03:42:38.635820: step 17094, loss 0.00213849, acc 1
2016-09-06T03:42:39.408878: step 17095, loss 0.00207333, acc 1
2016-09-06T03:42:40.225187: step 17096, loss 0.00295055, acc 1
2016-09-06T03:42:41.029656: step 17097, loss 0.00451598, acc 1
2016-09-06T03:42:41.879675: step 17098, loss 0.0017458, acc 1
2016-09-06T03:42:42.700542: step 17099, loss 0.00429773, acc 1
2016-09-06T03:42:43.564745: step 17100, loss 0.00432497, acc 1

Evaluation:
2016-09-06T03:42:47.277337: step 17100, loss 2.37311, acc 0.701689

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-17100

2016-09-06T03:42:49.262064: step 17101, loss 0.00601455, acc 1
2016-09-06T03:42:50.082389: step 17102, loss 0.00163904, acc 1
2016-09-06T03:42:50.923144: step 17103, loss 0.00170604, acc 1
2016-09-06T03:42:51.719182: step 17104, loss 0.0276834, acc 0.98
2016-09-06T03:42:52.528996: step 17105, loss 0.00418371, acc 1
2016-09-06T03:42:53.336051: step 17106, loss 0.00544578, acc 1
2016-09-06T03:42:54.107065: step 17107, loss 0.00189871, acc 1
2016-09-06T03:42:54.899294: step 17108, loss 0.0120762, acc 1
2016-09-06T03:42:55.748498: step 17109, loss 0.00239943, acc 1
2016-09-06T03:42:56.571470: step 17110, loss 0.0018994, acc 1
2016-09-06T03:42:57.371079: step 17111, loss 0.00359087, acc 1
2016-09-06T03:42:58.172821: step 17112, loss 0.0360802, acc 0.98
2016-09-06T03:42:58.981548: step 17113, loss 0.0110306, acc 1
2016-09-06T03:42:59.780287: step 17114, loss 0.0469205, acc 0.98
2016-09-06T03:43:00.616627: step 17115, loss 0.00951104, acc 1
2016-09-06T03:43:01.464147: step 17116, loss 0.00923998, acc 1
2016-09-06T03:43:02.286385: step 17117, loss 0.0176387, acc 0.98
2016-09-06T03:43:03.121672: step 17118, loss 0.0019711, acc 1
2016-09-06T03:43:03.908761: step 17119, loss 0.0123127, acc 1
2016-09-06T03:43:04.705213: step 17120, loss 0.00173248, acc 1
2016-09-06T03:43:05.545233: step 17121, loss 0.00233358, acc 1
2016-09-06T03:43:06.366390: step 17122, loss 0.076658, acc 0.94
2016-09-06T03:43:07.162764: step 17123, loss 0.00266012, acc 1
2016-09-06T03:43:08.010007: step 17124, loss 0.0196547, acc 0.98
2016-09-06T03:43:08.822076: step 17125, loss 0.00186378, acc 1
2016-09-06T03:43:09.615273: step 17126, loss 0.0469463, acc 0.96
2016-09-06T03:43:10.449813: step 17127, loss 0.00426535, acc 1
2016-09-06T03:43:11.260712: step 17128, loss 0.00161232, acc 1
2016-09-06T03:43:12.067561: step 17129, loss 0.0344391, acc 1
2016-09-06T03:43:12.916114: step 17130, loss 0.00167477, acc 1
2016-09-06T03:43:13.744736: step 17131, loss 0.0256572, acc 0.98
2016-09-06T03:43:14.518770: step 17132, loss 0.00474649, acc 1
2016-09-06T03:43:15.328469: step 17133, loss 0.00373617, acc 1
2016-09-06T03:43:16.145572: step 17134, loss 0.0020919, acc 1
2016-09-06T03:43:16.959677: step 17135, loss 0.00171363, acc 1
2016-09-06T03:43:17.749714: step 17136, loss 0.00160004, acc 1
2016-09-06T03:43:18.573633: step 17137, loss 0.0535954, acc 0.98
2016-09-06T03:43:19.367167: step 17138, loss 0.00822154, acc 1
2016-09-06T03:43:20.155655: step 17139, loss 0.0122195, acc 1
2016-09-06T03:43:20.961346: step 17140, loss 0.0133577, acc 1
2016-09-06T03:43:21.763064: step 17141, loss 0.0299228, acc 0.98
2016-09-06T03:43:22.598253: step 17142, loss 0.0180262, acc 0.98
2016-09-06T03:43:23.416429: step 17143, loss 0.0037828, acc 1
2016-09-06T03:43:24.197575: step 17144, loss 0.00606146, acc 1
2016-09-06T03:43:25.013781: step 17145, loss 0.00160996, acc 1
2016-09-06T03:43:25.809950: step 17146, loss 0.00157765, acc 1
2016-09-06T03:43:26.594916: step 17147, loss 0.00168088, acc 1
2016-09-06T03:43:27.405190: step 17148, loss 0.0102441, acc 1
2016-09-06T03:43:28.211819: step 17149, loss 0.030308, acc 0.98
2016-09-06T03:43:28.987000: step 17150, loss 0.0015825, acc 1
2016-09-06T03:43:29.828120: step 17151, loss 0.00160112, acc 1
2016-09-06T03:43:30.628787: step 17152, loss 0.0414227, acc 0.96
2016-09-06T03:43:31.420045: step 17153, loss 0.0178126, acc 1
2016-09-06T03:43:32.246498: step 17154, loss 0.00871752, acc 1
2016-09-06T03:43:33.049886: step 17155, loss 0.00238472, acc 1
2016-09-06T03:43:33.822095: step 17156, loss 0.0718955, acc 0.96
2016-09-06T03:43:34.683265: step 17157, loss 0.0277268, acc 0.98
2016-09-06T03:43:35.515499: step 17158, loss 0.00553444, acc 1
2016-09-06T03:43:36.317924: step 17159, loss 0.00363686, acc 1
2016-09-06T03:43:37.148772: step 17160, loss 0.00884377, acc 1
2016-09-06T03:43:37.955783: step 17161, loss 0.0151066, acc 1
2016-09-06T03:43:38.738653: step 17162, loss 0.0329553, acc 0.98
2016-09-06T03:43:39.550670: step 17163, loss 0.0146387, acc 1
2016-09-06T03:43:40.372746: step 17164, loss 0.0178742, acc 0.98
2016-09-06T03:43:41.151816: step 17165, loss 0.00170023, acc 1
2016-09-06T03:43:42.011177: step 17166, loss 0.0205055, acc 0.98
2016-09-06T03:43:42.829925: step 17167, loss 0.00214029, acc 1
2016-09-06T03:43:43.631086: step 17168, loss 0.0139011, acc 1
2016-09-06T03:43:44.426158: step 17169, loss 0.00169743, acc 1
2016-09-06T03:43:45.245228: step 17170, loss 0.0147835, acc 1
2016-09-06T03:43:46.033655: step 17171, loss 0.0180267, acc 0.98
2016-09-06T03:43:46.810393: step 17172, loss 0.00333666, acc 1
2016-09-06T03:43:47.611663: step 17173, loss 0.00200025, acc 1
2016-09-06T03:43:48.384103: step 17174, loss 0.00882967, acc 1
2016-09-06T03:43:49.196019: step 17175, loss 0.00173787, acc 1
2016-09-06T03:43:50.030322: step 17176, loss 0.00369895, acc 1
2016-09-06T03:43:50.811501: step 17177, loss 0.00569268, acc 1
2016-09-06T03:43:51.614899: step 17178, loss 0.0277679, acc 0.98
2016-09-06T03:43:52.419534: step 17179, loss 0.0105475, acc 1
2016-09-06T03:43:53.204315: step 17180, loss 0.00222907, acc 1
2016-09-06T03:43:54.031837: step 17181, loss 0.0131092, acc 1
2016-09-06T03:43:54.856430: step 17182, loss 0.00159585, acc 1
2016-09-06T03:43:55.648551: step 17183, loss 0.015414, acc 1
2016-09-06T03:43:56.461702: step 17184, loss 0.00157848, acc 1
2016-09-06T03:43:57.249151: step 17185, loss 0.00163245, acc 1
2016-09-06T03:43:58.033424: step 17186, loss 0.0394504, acc 0.98
2016-09-06T03:43:58.847252: step 17187, loss 0.0125157, acc 1
2016-09-06T03:43:59.644882: step 17188, loss 0.00945447, acc 1
2016-09-06T03:44:00.480504: step 17189, loss 0.00635657, acc 1
2016-09-06T03:44:01.270086: step 17190, loss 0.0274706, acc 0.98
2016-09-06T03:44:02.078248: step 17191, loss 0.00189517, acc 1
2016-09-06T03:44:02.856711: step 17192, loss 0.0789653, acc 0.98
2016-09-06T03:44:03.682871: step 17193, loss 0.0167261, acc 1
2016-09-06T03:44:04.527238: step 17194, loss 0.00307322, acc 1
2016-09-06T03:44:05.314635: step 17195, loss 0.0173696, acc 0.98
2016-09-06T03:44:06.124127: step 17196, loss 0.070494, acc 0.96
2016-09-06T03:44:06.947092: step 17197, loss 0.0155695, acc 1
2016-09-06T03:44:07.748548: step 17198, loss 0.0633551, acc 0.96
2016-09-06T03:44:08.589119: step 17199, loss 0.00169094, acc 1
2016-09-06T03:44:09.395179: step 17200, loss 0.0391821, acc 0.98

Evaluation:
2016-09-06T03:44:13.144746: step 17200, loss 2.14235, acc 0.702627

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-17200

2016-09-06T03:44:15.055790: step 17201, loss 0.00961719, acc 1
2016-09-06T03:44:15.870312: step 17202, loss 0.00708856, acc 1
2016-09-06T03:44:16.681132: step 17203, loss 0.0292827, acc 0.98
2016-09-06T03:44:17.521952: step 17204, loss 0.00206927, acc 1
2016-09-06T03:44:18.316616: step 17205, loss 0.00979133, acc 1
2016-09-06T03:44:19.144889: step 17206, loss 0.00168456, acc 1
2016-09-06T03:44:19.975745: step 17207, loss 0.0233495, acc 1
2016-09-06T03:44:20.782170: step 17208, loss 0.0192304, acc 0.98
2016-09-06T03:44:21.605137: step 17209, loss 0.00138979, acc 1
2016-09-06T03:44:22.371938: step 17210, loss 0.00253021, acc 1
2016-09-06T03:44:23.175042: step 17211, loss 0.00950952, acc 1
2016-09-06T03:44:24.018650: step 17212, loss 0.0151377, acc 1
2016-09-06T03:44:24.788241: step 17213, loss 0.0264786, acc 0.98
2016-09-06T03:44:25.630818: step 17214, loss 0.0028417, acc 1
2016-09-06T03:44:26.441729: step 17215, loss 0.0201268, acc 0.98
2016-09-06T03:44:27.239609: step 17216, loss 0.0142223, acc 1
2016-09-06T03:44:28.049411: step 17217, loss 0.0033376, acc 1
2016-09-06T03:44:28.911074: step 17218, loss 0.00895703, acc 1
2016-09-06T03:44:29.745110: step 17219, loss 0.00853717, acc 1
2016-09-06T03:44:30.550132: step 17220, loss 0.0044731, acc 1
2016-09-06T03:44:31.381849: step 17221, loss 0.00186667, acc 1
2016-09-06T03:44:32.218174: step 17222, loss 0.0232603, acc 0.98
2016-09-06T03:44:33.024588: step 17223, loss 0.0157466, acc 1
2016-09-06T03:44:33.869721: step 17224, loss 0.00940741, acc 1
2016-09-06T03:44:34.672143: step 17225, loss 0.00167085, acc 1
2016-09-06T03:44:35.462143: step 17226, loss 0.0016543, acc 1
2016-09-06T03:44:36.289346: step 17227, loss 0.00166, acc 1
2016-09-06T03:44:37.065841: step 17228, loss 0.00167307, acc 1
2016-09-06T03:44:37.887393: step 17229, loss 0.0246301, acc 1
2016-09-06T03:44:38.726462: step 17230, loss 0.00185724, acc 1
2016-09-06T03:44:39.537373: step 17231, loss 0.01438, acc 1
2016-09-06T03:44:40.361140: step 17232, loss 0.0439714, acc 0.98
2016-09-06T03:44:41.211054: step 17233, loss 0.00171714, acc 1
2016-09-06T03:44:42.009637: step 17234, loss 0.0146135, acc 1
2016-09-06T03:44:42.823398: step 17235, loss 0.00204888, acc 1
2016-09-06T03:44:43.658816: step 17236, loss 0.00247564, acc 1
2016-09-06T03:44:44.462658: step 17237, loss 0.0125421, acc 1
2016-09-06T03:44:45.258592: step 17238, loss 0.00175336, acc 1
2016-09-06T03:44:46.082424: step 17239, loss 0.00166404, acc 1
2016-09-06T03:44:46.882367: step 17240, loss 0.00169291, acc 1
2016-09-06T03:44:47.692608: step 17241, loss 0.0181478, acc 0.98
2016-09-06T03:44:48.530808: step 17242, loss 0.0100004, acc 1
2016-09-06T03:44:49.345680: step 17243, loss 0.00873985, acc 1
2016-09-06T03:44:50.129408: step 17244, loss 0.0650332, acc 0.96
2016-09-06T03:44:50.948041: step 17245, loss 0.00224483, acc 1
2016-09-06T03:44:51.758893: step 17246, loss 0.00716526, acc 1
2016-09-06T03:44:52.555681: step 17247, loss 0.00176878, acc 1
2016-09-06T03:44:53.348013: step 17248, loss 0.0838873, acc 0.94
2016-09-06T03:44:54.193745: step 17249, loss 0.0165594, acc 1
2016-09-06T03:44:54.984398: step 17250, loss 0.0656822, acc 0.98
2016-09-06T03:44:55.829099: step 17251, loss 0.00154739, acc 1
2016-09-06T03:44:56.660387: step 17252, loss 0.00142292, acc 1
2016-09-06T03:44:57.433680: step 17253, loss 0.00141546, acc 1
2016-09-06T03:44:58.230534: step 17254, loss 0.0325822, acc 0.96
2016-09-06T03:44:59.062497: step 17255, loss 0.00170385, acc 1
2016-09-06T03:44:59.863926: step 17256, loss 0.0061978, acc 1
2016-09-06T03:45:00.705675: step 17257, loss 0.0576801, acc 0.94
2016-09-06T03:45:01.549957: step 17258, loss 0.0441542, acc 0.98
2016-09-06T03:45:02.355618: step 17259, loss 0.0181916, acc 0.98
2016-09-06T03:45:03.182681: step 17260, loss 0.00334355, acc 1
2016-09-06T03:45:04.007063: step 17261, loss 0.00147789, acc 1
2016-09-06T03:45:04.806750: step 17262, loss 0.0256586, acc 1
2016-09-06T03:45:05.619338: step 17263, loss 0.0723605, acc 0.98
2016-09-06T03:45:06.430863: step 17264, loss 0.00191388, acc 1
2016-09-06T03:45:07.238897: step 17265, loss 0.00745513, acc 1
2016-09-06T03:45:08.039379: step 17266, loss 0.0240462, acc 1
2016-09-06T03:45:08.887792: step 17267, loss 0.0219383, acc 1
2016-09-06T03:45:09.718487: step 17268, loss 0.0481712, acc 0.98
2016-09-06T03:45:10.510840: step 17269, loss 0.00258462, acc 1
2016-09-06T03:45:11.375283: step 17270, loss 0.00192965, acc 1
2016-09-06T03:45:12.196973: step 17271, loss 0.00841979, acc 1
2016-09-06T03:45:13.010431: step 17272, loss 0.00287569, acc 1
2016-09-06T03:45:13.835902: step 17273, loss 0.0316172, acc 0.98
2016-09-06T03:45:14.665804: step 17274, loss 0.0230374, acc 1
2016-09-06T03:45:15.480069: step 17275, loss 0.00253285, acc 1
2016-09-06T03:45:16.298848: step 17276, loss 0.0159126, acc 0.98
2016-09-06T03:45:17.097833: step 17277, loss 0.0640906, acc 0.98
2016-09-06T03:45:17.881740: step 17278, loss 0.00936677, acc 1
2016-09-06T03:45:18.704047: step 17279, loss 0.00991467, acc 1
2016-09-06T03:45:19.457715: step 17280, loss 0.00220568, acc 1
2016-09-06T03:45:20.257440: step 17281, loss 0.00239221, acc 1
2016-09-06T03:45:21.088736: step 17282, loss 0.0472741, acc 0.94
2016-09-06T03:45:21.905452: step 17283, loss 0.00168119, acc 1
2016-09-06T03:45:22.709135: step 17284, loss 0.0229025, acc 0.98
2016-09-06T03:45:23.536317: step 17285, loss 0.00647293, acc 1
2016-09-06T03:45:24.326252: step 17286, loss 0.00362916, acc 1
2016-09-06T03:45:25.143542: step 17287, loss 0.0108883, acc 1
2016-09-06T03:45:25.983031: step 17288, loss 0.00583621, acc 1
2016-09-06T03:45:26.811716: step 17289, loss 0.0147559, acc 1
2016-09-06T03:45:27.599144: step 17290, loss 0.00172816, acc 1
2016-09-06T03:45:28.388079: step 17291, loss 0.0444128, acc 0.98
2016-09-06T03:45:29.187412: step 17292, loss 0.00355273, acc 1
2016-09-06T03:45:29.986407: step 17293, loss 0.00177549, acc 1
2016-09-06T03:45:30.806777: step 17294, loss 0.00350976, acc 1
2016-09-06T03:45:31.613715: step 17295, loss 0.00250719, acc 1
2016-09-06T03:45:32.413496: step 17296, loss 0.0230167, acc 0.98
2016-09-06T03:45:33.215329: step 17297, loss 0.00792114, acc 1
2016-09-06T03:45:34.009572: step 17298, loss 0.0208906, acc 0.98
2016-09-06T03:45:34.823821: step 17299, loss 0.0071107, acc 1
2016-09-06T03:45:35.667068: step 17300, loss 0.0207832, acc 0.98

Evaluation:
2016-09-06T03:45:39.411594: step 17300, loss 2.66187, acc 0.707317

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-17300

2016-09-06T03:45:41.286762: step 17301, loss 0.00185009, acc 1
2016-09-06T03:45:42.125909: step 17302, loss 0.0933078, acc 0.96
2016-09-06T03:45:42.992572: step 17303, loss 0.0192554, acc 1
2016-09-06T03:45:43.829630: step 17304, loss 0.0133418, acc 1
2016-09-06T03:45:44.632000: step 17305, loss 0.00287113, acc 1
2016-09-06T03:45:45.451459: step 17306, loss 0.105381, acc 0.98
2016-09-06T03:45:46.286382: step 17307, loss 0.0305969, acc 0.98
2016-09-06T03:45:47.093102: step 17308, loss 0.0074093, acc 1
2016-09-06T03:45:47.890530: step 17309, loss 0.0482128, acc 0.96
2016-09-06T03:45:48.735840: step 17310, loss 0.002224, acc 1
2016-09-06T03:45:49.535722: step 17311, loss 0.0473182, acc 0.96
2016-09-06T03:45:50.374448: step 17312, loss 0.0231174, acc 0.98
2016-09-06T03:45:51.186366: step 17313, loss 0.00239584, acc 1
2016-09-06T03:45:51.963841: step 17314, loss 0.00563821, acc 1
2016-09-06T03:45:52.743178: step 17315, loss 0.00943077, acc 1
2016-09-06T03:45:53.558779: step 17316, loss 0.0271379, acc 0.98
2016-09-06T03:45:54.352280: step 17317, loss 0.00987874, acc 1
2016-09-06T03:45:55.157641: step 17318, loss 0.0283722, acc 1
2016-09-06T03:45:56.008235: step 17319, loss 0.0261232, acc 0.98
2016-09-06T03:45:56.811611: step 17320, loss 0.00354992, acc 1
2016-09-06T03:45:57.602091: step 17321, loss 0.00317078, acc 1
2016-09-06T03:45:58.419077: step 17322, loss 0.00643596, acc 1
2016-09-06T03:45:59.223293: step 17323, loss 0.0771223, acc 0.98
2016-09-06T03:46:00.047955: step 17324, loss 0.00735489, acc 1
2016-09-06T03:46:00.889568: step 17325, loss 0.00239247, acc 1
2016-09-06T03:46:01.706281: step 17326, loss 0.0065512, acc 1
2016-09-06T03:46:02.523989: step 17327, loss 0.00210712, acc 1
2016-09-06T03:46:03.366259: step 17328, loss 0.0252127, acc 1
2016-09-06T03:46:04.168774: step 17329, loss 0.0276877, acc 0.98
2016-09-06T03:46:04.971550: step 17330, loss 0.00218113, acc 1
2016-09-06T03:46:05.816760: step 17331, loss 0.15221, acc 0.96
2016-09-06T03:46:06.638043: step 17332, loss 0.0167436, acc 1
2016-09-06T03:46:07.447534: step 17333, loss 0.0584008, acc 0.98
2016-09-06T03:46:08.310904: step 17334, loss 0.0141632, acc 1
2016-09-06T03:46:09.116377: step 17335, loss 0.0163343, acc 1
2016-09-06T03:46:09.910723: step 17336, loss 0.0439327, acc 0.98
2016-09-06T03:46:10.738525: step 17337, loss 0.00859392, acc 1
2016-09-06T03:46:11.520541: step 17338, loss 0.0201709, acc 0.98
2016-09-06T03:46:12.341889: step 17339, loss 0.0322776, acc 0.98
2016-09-06T03:46:13.160548: step 17340, loss 0.00318397, acc 1
2016-09-06T03:46:13.959915: step 17341, loss 0.0384521, acc 0.98
2016-09-06T03:46:14.819034: step 17342, loss 0.00325195, acc 1
2016-09-06T03:46:15.683055: step 17343, loss 0.0036203, acc 1
2016-09-06T03:46:16.519759: step 17344, loss 0.0277078, acc 0.98
2016-09-06T03:46:17.325804: step 17345, loss 0.00343658, acc 1
2016-09-06T03:46:18.125950: step 17346, loss 0.0176064, acc 1
2016-09-06T03:46:18.967341: step 17347, loss 0.0327284, acc 0.98
2016-09-06T03:46:19.756201: step 17348, loss 0.0344202, acc 0.98
2016-09-06T03:46:20.552475: step 17349, loss 0.00451803, acc 1
2016-09-06T03:46:21.385490: step 17350, loss 0.00364417, acc 1
2016-09-06T03:46:22.217588: step 17351, loss 0.0568028, acc 0.98
2016-09-06T03:46:23.024340: step 17352, loss 0.00358947, acc 1
2016-09-06T03:46:23.831557: step 17353, loss 0.0163565, acc 1
2016-09-06T03:46:24.608719: step 17354, loss 0.0417519, acc 0.98
2016-09-06T03:46:25.396490: step 17355, loss 0.00814288, acc 1
2016-09-06T03:46:26.221571: step 17356, loss 0.00565741, acc 1
2016-09-06T03:46:27.000305: step 17357, loss 0.0160024, acc 1
2016-09-06T03:46:27.825427: step 17358, loss 0.00803451, acc 1
2016-09-06T03:46:28.605774: step 17359, loss 0.0127461, acc 1
2016-09-06T03:46:29.370542: step 17360, loss 0.0113378, acc 1
2016-09-06T03:46:30.173844: step 17361, loss 0.00367051, acc 1
2016-09-06T03:46:30.985327: step 17362, loss 0.00636664, acc 1
2016-09-06T03:46:31.807084: step 17363, loss 0.0618118, acc 0.98
2016-09-06T03:46:32.633664: step 17364, loss 0.0470604, acc 0.98
2016-09-06T03:46:33.437270: step 17365, loss 0.0170771, acc 0.98
2016-09-06T03:46:34.229397: step 17366, loss 0.00983349, acc 1
2016-09-06T03:46:35.088631: step 17367, loss 0.0051744, acc 1
2016-09-06T03:46:35.903836: step 17368, loss 0.00333966, acc 1
2016-09-06T03:46:36.698938: step 17369, loss 0.00352136, acc 1
2016-09-06T03:46:37.516117: step 17370, loss 0.00296665, acc 1
2016-09-06T03:46:38.364179: step 17371, loss 0.0163808, acc 1
2016-09-06T03:46:39.187789: step 17372, loss 0.00283725, acc 1
2016-09-06T03:46:39.991138: step 17373, loss 0.00550862, acc 1
2016-09-06T03:46:40.828537: step 17374, loss 0.00294341, acc 1
2016-09-06T03:46:41.658544: step 17375, loss 0.00274947, acc 1
2016-09-06T03:46:42.478075: step 17376, loss 0.00308885, acc 1
2016-09-06T03:46:43.327400: step 17377, loss 0.00266331, acc 1
2016-09-06T03:46:44.137533: step 17378, loss 0.0211586, acc 0.98
2016-09-06T03:46:44.906208: step 17379, loss 0.0454003, acc 0.98
2016-09-06T03:46:45.741275: step 17380, loss 0.0526997, acc 0.98
2016-09-06T03:46:46.564279: step 17381, loss 0.00450601, acc 1
2016-09-06T03:46:47.378617: step 17382, loss 0.00657708, acc 1
2016-09-06T03:46:48.213339: step 17383, loss 0.00338008, acc 1
2016-09-06T03:46:49.031139: step 17384, loss 0.00231626, acc 1
2016-09-06T03:46:49.840026: step 17385, loss 0.0200763, acc 0.98
2016-09-06T03:46:50.659043: step 17386, loss 0.00373243, acc 1
2016-09-06T03:46:51.450491: step 17387, loss 0.023661, acc 0.98
2016-09-06T03:46:52.271883: step 17388, loss 0.0162079, acc 0.98
2016-09-06T03:46:53.104241: step 17389, loss 0.0486404, acc 0.98
2016-09-06T03:46:53.910061: step 17390, loss 0.0204278, acc 0.98
2016-09-06T03:46:54.717431: step 17391, loss 0.00527745, acc 1
2016-09-06T03:46:55.549466: step 17392, loss 0.0302362, acc 1
2016-09-06T03:46:56.355662: step 17393, loss 0.00816308, acc 1
2016-09-06T03:46:57.131526: step 17394, loss 0.0021655, acc 1
2016-09-06T03:46:57.964730: step 17395, loss 0.0171702, acc 0.98
2016-09-06T03:46:58.784976: step 17396, loss 0.00731125, acc 1
2016-09-06T03:46:59.596289: step 17397, loss 0.0263518, acc 1
2016-09-06T03:47:00.423948: step 17398, loss 0.00212957, acc 1
2016-09-06T03:47:01.238685: step 17399, loss 0.015404, acc 1
2016-09-06T03:47:02.029768: step 17400, loss 0.0220379, acc 0.98

Evaluation:
2016-09-06T03:47:05.757996: step 17400, loss 3.05103, acc 0.707317

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-17400

2016-09-06T03:47:07.807250: step 17401, loss 0.00212438, acc 1
2016-09-06T03:47:08.653628: step 17402, loss 0.00212105, acc 1
2016-09-06T03:47:09.421835: step 17403, loss 0.0195038, acc 1
2016-09-06T03:47:10.206602: step 17404, loss 0.00768864, acc 1
2016-09-06T03:47:11.020911: step 17405, loss 0.0187323, acc 1
2016-09-06T03:47:11.795339: step 17406, loss 0.00213281, acc 1
2016-09-06T03:47:12.595270: step 17407, loss 0.0216774, acc 0.98
2016-09-06T03:47:13.428677: step 17408, loss 0.0059983, acc 1
2016-09-06T03:47:14.214032: step 17409, loss 0.00222623, acc 1
2016-09-06T03:47:15.056719: step 17410, loss 0.0447451, acc 0.96
2016-09-06T03:47:15.885447: step 17411, loss 0.011625, acc 1
2016-09-06T03:47:16.703182: step 17412, loss 0.0036937, acc 1
2016-09-06T03:47:17.503627: step 17413, loss 0.0209136, acc 0.98
2016-09-06T03:47:18.323861: step 17414, loss 0.00662929, acc 1
2016-09-06T03:47:19.088799: step 17415, loss 0.00322328, acc 1
2016-09-06T03:47:19.891781: step 17416, loss 0.00220343, acc 1
2016-09-06T03:47:20.713445: step 17417, loss 0.0166375, acc 0.98
2016-09-06T03:47:21.506121: step 17418, loss 0.0142479, acc 1
2016-09-06T03:47:22.321230: step 17419, loss 0.00690168, acc 1
2016-09-06T03:47:23.141644: step 17420, loss 0.00219034, acc 1
2016-09-06T03:47:23.920942: step 17421, loss 0.028884, acc 0.98
2016-09-06T03:47:24.722418: step 17422, loss 0.00218651, acc 1
2016-09-06T03:47:25.545407: step 17423, loss 0.00217746, acc 1
2016-09-06T03:47:26.330736: step 17424, loss 0.00254289, acc 1
2016-09-06T03:47:27.128923: step 17425, loss 0.00215549, acc 1
2016-09-06T03:47:27.979583: step 17426, loss 0.0139174, acc 1
2016-09-06T03:47:28.835079: step 17427, loss 0.0198445, acc 1
2016-09-06T03:47:29.647358: step 17428, loss 0.00234685, acc 1
2016-09-06T03:47:30.494719: step 17429, loss 0.00551243, acc 1
2016-09-06T03:47:31.311698: step 17430, loss 0.00248322, acc 1
2016-09-06T03:47:32.117180: step 17431, loss 0.00900684, acc 1
2016-09-06T03:47:32.944932: step 17432, loss 0.00394662, acc 1
2016-09-06T03:47:33.758998: step 17433, loss 0.00258899, acc 1
2016-09-06T03:47:34.550841: step 17434, loss 0.0109581, acc 1
2016-09-06T03:47:35.389933: step 17435, loss 0.0544606, acc 0.98
2016-09-06T03:47:36.211262: step 17436, loss 0.0119293, acc 1
2016-09-06T03:47:37.070965: step 17437, loss 0.00571576, acc 1
2016-09-06T03:47:37.913755: step 17438, loss 0.00364465, acc 1
2016-09-06T03:47:38.760269: step 17439, loss 0.00198847, acc 1
2016-09-06T03:47:39.545724: step 17440, loss 0.060129, acc 0.96
2016-09-06T03:47:40.395794: step 17441, loss 0.0182851, acc 1
2016-09-06T03:47:41.195018: step 17442, loss 0.0105935, acc 1
2016-09-06T03:47:42.023208: step 17443, loss 0.0176348, acc 0.98
2016-09-06T03:47:42.829857: step 17444, loss 0.00188178, acc 1
2016-09-06T03:47:43.647709: step 17445, loss 0.00705779, acc 1
2016-09-06T03:47:44.452923: step 17446, loss 0.0175515, acc 1
2016-09-06T03:47:45.250415: step 17447, loss 0.0049126, acc 1
2016-09-06T03:47:46.049393: step 17448, loss 0.00613012, acc 1
2016-09-06T03:47:46.847558: step 17449, loss 0.0475951, acc 0.98
2016-09-06T03:47:47.663178: step 17450, loss 0.00460463, acc 1
2016-09-06T03:47:48.480975: step 17451, loss 0.0118788, acc 1
2016-09-06T03:47:49.245542: step 17452, loss 0.001927, acc 1
2016-09-06T03:47:50.076614: step 17453, loss 0.0059919, acc 1
2016-09-06T03:47:50.955228: step 17454, loss 0.0173083, acc 0.98
2016-09-06T03:47:51.775301: step 17455, loss 0.0224253, acc 0.98
2016-09-06T03:47:52.573403: step 17456, loss 0.00212469, acc 1
2016-09-06T03:47:53.388516: step 17457, loss 0.00277232, acc 1
2016-09-06T03:47:54.179684: step 17458, loss 0.00744134, acc 1
2016-09-06T03:47:54.988924: step 17459, loss 0.00415151, acc 1
2016-09-06T03:47:55.805884: step 17460, loss 0.0217923, acc 0.98
2016-09-06T03:47:56.606862: step 17461, loss 0.00204817, acc 1
2016-09-06T03:47:57.423184: step 17462, loss 0.00261352, acc 1
2016-09-06T03:47:58.262837: step 17463, loss 0.00185207, acc 1
2016-09-06T03:47:59.072319: step 17464, loss 0.00208833, acc 1
2016-09-06T03:47:59.875311: step 17465, loss 0.0125363, acc 1
2016-09-06T03:48:00.748215: step 17466, loss 0.0156553, acc 1
2016-09-06T03:48:01.568829: step 17467, loss 0.017926, acc 1
2016-09-06T03:48:02.382315: step 17468, loss 0.00185451, acc 1
2016-09-06T03:48:03.319548: step 17469, loss 0.0344116, acc 0.98
2016-09-06T03:48:04.146318: step 17470, loss 0.00730705, acc 1
2016-09-06T03:48:04.960764: step 17471, loss 0.00897991, acc 1
2016-09-06T03:48:05.749126: step 17472, loss 0.0405779, acc 0.977273
2016-09-06T03:48:06.552347: step 17473, loss 0.00181909, acc 1
2016-09-06T03:48:07.361803: step 17474, loss 0.00425467, acc 1
2016-09-06T03:48:08.211647: step 17475, loss 0.00189635, acc 1
2016-09-06T03:48:09.060700: step 17476, loss 0.0316996, acc 0.98
2016-09-06T03:48:09.846166: step 17477, loss 0.00181484, acc 1
2016-09-06T03:48:10.638793: step 17478, loss 0.00180697, acc 1
2016-09-06T03:48:11.452934: step 17479, loss 0.00180293, acc 1
2016-09-06T03:48:12.229464: step 17480, loss 0.0417369, acc 0.96
2016-09-06T03:48:13.035202: step 17481, loss 0.00335661, acc 1
2016-09-06T03:48:13.848909: step 17482, loss 0.00178344, acc 1
2016-09-06T03:48:14.647925: step 17483, loss 0.0167158, acc 1
2016-09-06T03:48:15.443651: step 17484, loss 0.0182837, acc 0.98
2016-09-06T03:48:16.238098: step 17485, loss 0.0107594, acc 1
2016-09-06T03:48:17.040548: step 17486, loss 0.0109609, acc 1
2016-09-06T03:48:17.857362: step 17487, loss 0.00270428, acc 1
2016-09-06T03:48:18.643755: step 17488, loss 0.036345, acc 1
2016-09-06T03:48:19.450623: step 17489, loss 0.00177714, acc 1
2016-09-06T03:48:20.256786: step 17490, loss 0.0145482, acc 1
2016-09-06T03:48:21.076447: step 17491, loss 0.00180251, acc 1
2016-09-06T03:48:21.861436: step 17492, loss 0.00184979, acc 1
2016-09-06T03:48:22.689281: step 17493, loss 0.0162655, acc 0.98
2016-09-06T03:48:23.517078: step 17494, loss 0.023705, acc 0.98
2016-09-06T03:48:24.326737: step 17495, loss 0.00183748, acc 1
2016-09-06T03:48:25.133936: step 17496, loss 0.00728847, acc 1
2016-09-06T03:48:25.961855: step 17497, loss 0.00675235, acc 1
2016-09-06T03:48:26.759290: step 17498, loss 0.0153164, acc 1
2016-09-06T03:48:27.565097: step 17499, loss 0.00185633, acc 1
2016-09-06T03:48:28.426951: step 17500, loss 0.00534547, acc 1

Evaluation:
2016-09-06T03:48:32.169455: step 17500, loss 3.43327, acc 0.702627

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-17500

2016-09-06T03:48:34.032349: step 17501, loss 0.0335015, acc 0.98
2016-09-06T03:48:34.830666: step 17502, loss 0.00271878, acc 1
2016-09-06T03:48:35.655448: step 17503, loss 0.00277142, acc 1
2016-09-06T03:48:36.461421: step 17504, loss 0.00222189, acc 1
2016-09-06T03:48:37.247450: step 17505, loss 0.0104618, acc 1
2016-09-06T03:48:38.094982: step 17506, loss 0.0150505, acc 1
2016-09-06T03:48:38.892912: step 17507, loss 0.00318747, acc 1
2016-09-06T03:48:39.660794: step 17508, loss 0.0727472, acc 0.98
2016-09-06T03:48:40.479338: step 17509, loss 0.00185797, acc 1
2016-09-06T03:48:41.277829: step 17510, loss 0.0164894, acc 0.98
2016-09-06T03:48:42.065379: step 17511, loss 0.00271109, acc 1
2016-09-06T03:48:42.874577: step 17512, loss 0.0018041, acc 1
2016-09-06T03:48:43.674396: step 17513, loss 0.0320838, acc 0.98
2016-09-06T03:48:44.482982: step 17514, loss 0.00184745, acc 1
2016-09-06T03:48:45.294746: step 17515, loss 0.0738706, acc 0.98
2016-09-06T03:48:46.078544: step 17516, loss 0.00161291, acc 1
2016-09-06T03:48:46.869339: step 17517, loss 0.00156883, acc 1
2016-09-06T03:48:47.685715: step 17518, loss 0.00525025, acc 1
2016-09-06T03:48:48.500669: step 17519, loss 0.0039793, acc 1
2016-09-06T03:48:49.354794: step 17520, loss 0.0104237, acc 1
2016-09-06T03:48:50.168108: step 17521, loss 0.0930968, acc 0.98
2016-09-06T03:48:50.980839: step 17522, loss 0.00612576, acc 1
2016-09-06T03:48:51.785063: step 17523, loss 0.0143613, acc 1
2016-09-06T03:48:52.587019: step 17524, loss 0.00202422, acc 1
2016-09-06T03:48:53.384608: step 17525, loss 0.0137998, acc 1
2016-09-06T03:48:54.184945: step 17526, loss 0.0204027, acc 0.98
2016-09-06T03:48:55.002338: step 17527, loss 0.0229225, acc 0.98
2016-09-06T03:48:55.777398: step 17528, loss 0.00191935, acc 1
2016-09-06T03:48:56.585782: step 17529, loss 0.00350302, acc 1
2016-09-06T03:48:57.383398: step 17530, loss 0.0207497, acc 0.98
2016-09-06T03:48:58.168224: step 17531, loss 0.0019981, acc 1
2016-09-06T03:48:58.980972: step 17532, loss 0.00189495, acc 1
2016-09-06T03:48:59.818823: step 17533, loss 0.017292, acc 1
2016-09-06T03:49:00.639856: step 17534, loss 0.00587698, acc 1
2016-09-06T03:49:01.448293: step 17535, loss 0.00282482, acc 1
2016-09-06T03:49:02.280942: step 17536, loss 0.00316802, acc 1
2016-09-06T03:49:03.082452: step 17537, loss 0.0199781, acc 1
2016-09-06T03:49:03.896062: step 17538, loss 0.0720994, acc 0.94
2016-09-06T03:49:04.756564: step 17539, loss 0.0125766, acc 1
2016-09-06T03:49:05.555110: step 17540, loss 0.0289071, acc 0.98
2016-09-06T03:49:06.348539: step 17541, loss 0.00694066, acc 1
2016-09-06T03:49:07.193726: step 17542, loss 0.0200394, acc 1
2016-09-06T03:49:08.012876: step 17543, loss 0.0293056, acc 0.98
2016-09-06T03:49:08.809957: step 17544, loss 0.0411768, acc 0.98
2016-09-06T03:49:09.629134: step 17545, loss 0.00405869, acc 1
2016-09-06T03:49:10.426870: step 17546, loss 0.00203184, acc 1
2016-09-06T03:49:11.239869: step 17547, loss 0.0249014, acc 0.98
2016-09-06T03:49:12.054641: step 17548, loss 0.0191405, acc 0.98
2016-09-06T03:49:12.854853: step 17549, loss 0.0155433, acc 1
2016-09-06T03:49:13.680162: step 17550, loss 0.0177665, acc 1
2016-09-06T03:49:14.525931: step 17551, loss 0.00358574, acc 1
2016-09-06T03:49:15.334669: step 17552, loss 0.0072392, acc 1
2016-09-06T03:49:16.148599: step 17553, loss 0.00286378, acc 1
2016-09-06T03:49:16.974484: step 17554, loss 0.0176242, acc 0.98
2016-09-06T03:49:17.787296: step 17555, loss 0.0433124, acc 0.98
2016-09-06T03:49:18.607658: step 17556, loss 0.00219668, acc 1
2016-09-06T03:49:19.453017: step 17557, loss 0.0109618, acc 1
2016-09-06T03:49:20.272597: step 17558, loss 0.0261995, acc 1
2016-09-06T03:49:21.088139: step 17559, loss 0.00213593, acc 1
2016-09-06T03:49:21.892930: step 17560, loss 0.0201224, acc 0.98
2016-09-06T03:49:22.703701: step 17561, loss 0.00634544, acc 1
2016-09-06T03:49:23.535366: step 17562, loss 0.00402717, acc 1
2016-09-06T03:49:24.353382: step 17563, loss 0.00210467, acc 1
2016-09-06T03:49:25.179962: step 17564, loss 0.00210102, acc 1
2016-09-06T03:49:25.961507: step 17565, loss 0.00270967, acc 1
2016-09-06T03:49:26.777407: step 17566, loss 0.00229933, acc 1
2016-09-06T03:49:27.625612: step 17567, loss 0.0021384, acc 1
2016-09-06T03:49:28.409843: step 17568, loss 0.0108351, acc 1
2016-09-06T03:49:29.196624: step 17569, loss 0.0020449, acc 1
2016-09-06T03:49:30.021469: step 17570, loss 0.00199912, acc 1
2016-09-06T03:49:30.834561: step 17571, loss 0.0140341, acc 1
2016-09-06T03:49:31.638647: step 17572, loss 0.00295218, acc 1
2016-09-06T03:49:32.446949: step 17573, loss 0.00422979, acc 1
2016-09-06T03:49:33.222466: step 17574, loss 0.00186916, acc 1
2016-09-06T03:49:34.009070: step 17575, loss 0.00483508, acc 1
2016-09-06T03:49:34.821421: step 17576, loss 0.00219608, acc 1
2016-09-06T03:49:35.633072: step 17577, loss 0.0183614, acc 1
2016-09-06T03:49:36.441297: step 17578, loss 0.00447195, acc 1
2016-09-06T03:49:37.264564: step 17579, loss 0.00224426, acc 1
2016-09-06T03:49:38.077948: step 17580, loss 0.0068008, acc 1
2016-09-06T03:49:38.891104: step 17581, loss 0.0198081, acc 1
2016-09-06T03:49:39.744225: step 17582, loss 0.0665865, acc 0.98
2016-09-06T03:49:40.545110: step 17583, loss 0.00182441, acc 1
2016-09-06T03:49:41.342253: step 17584, loss 0.0159805, acc 1
2016-09-06T03:49:42.188059: step 17585, loss 0.0956706, acc 0.98
2016-09-06T03:49:42.976510: step 17586, loss 0.00962913, acc 1
2016-09-06T03:49:43.778836: step 17587, loss 0.00264734, acc 1
2016-09-06T03:49:44.588717: step 17588, loss 0.0016533, acc 1
2016-09-06T03:49:45.395209: step 17589, loss 0.017618, acc 0.98
2016-09-06T03:49:46.196838: step 17590, loss 0.00215563, acc 1
2016-09-06T03:49:47.027469: step 17591, loss 0.0247448, acc 0.98
2016-09-06T03:49:47.843354: step 17592, loss 0.00189187, acc 1
2016-09-06T03:49:48.641063: step 17593, loss 0.00156479, acc 1
2016-09-06T03:49:49.477453: step 17594, loss 0.00184388, acc 1
2016-09-06T03:49:50.313291: step 17595, loss 0.00644344, acc 1
2016-09-06T03:49:51.125843: step 17596, loss 0.0122413, acc 1
2016-09-06T03:49:51.935691: step 17597, loss 0.0244145, acc 1
2016-09-06T03:49:52.753404: step 17598, loss 0.0052465, acc 1
2016-09-06T03:49:53.568888: step 17599, loss 0.00161365, acc 1
2016-09-06T03:49:54.389440: step 17600, loss 0.00190952, acc 1

Evaluation:
2016-09-06T03:49:58.111742: step 17600, loss 2.65455, acc 0.705441

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-17600

2016-09-06T03:49:59.907795: step 17601, loss 0.00233424, acc 1
2016-09-06T03:50:00.752344: step 17602, loss 0.0142132, acc 1
2016-09-06T03:50:01.590735: step 17603, loss 0.0203056, acc 1
2016-09-06T03:50:02.365158: step 17604, loss 0.00328831, acc 1
2016-09-06T03:50:03.182692: step 17605, loss 0.00836072, acc 1
2016-09-06T03:50:04.036087: step 17606, loss 0.00548488, acc 1
2016-09-06T03:50:04.820362: step 17607, loss 0.0287542, acc 1
2016-09-06T03:50:05.585816: step 17608, loss 0.00681492, acc 1
2016-09-06T03:50:06.400395: step 17609, loss 0.00292839, acc 1
2016-09-06T03:50:07.169015: step 17610, loss 0.0519703, acc 0.98
2016-09-06T03:50:07.988576: step 17611, loss 0.00177667, acc 1
2016-09-06T03:50:08.785519: step 17612, loss 0.0054847, acc 1
2016-09-06T03:50:09.593408: step 17613, loss 0.05133, acc 0.96
2016-09-06T03:50:10.426475: step 17614, loss 0.00581381, acc 1
2016-09-06T03:50:11.257452: step 17615, loss 0.00252844, acc 1
2016-09-06T03:50:12.060448: step 17616, loss 0.00336383, acc 1
2016-09-06T03:50:12.841610: step 17617, loss 0.0179653, acc 0.98
2016-09-06T03:50:13.669713: step 17618, loss 0.022045, acc 0.98
2016-09-06T03:50:14.468113: step 17619, loss 0.0127568, acc 1
2016-09-06T03:50:15.262586: step 17620, loss 0.0018386, acc 1
2016-09-06T03:50:16.070704: step 17621, loss 0.00200942, acc 1
2016-09-06T03:50:16.848663: step 17622, loss 0.0160323, acc 0.98
2016-09-06T03:50:17.652250: step 17623, loss 0.00173862, acc 1
2016-09-06T03:50:18.459061: step 17624, loss 0.0173267, acc 0.98
2016-09-06T03:50:19.257888: step 17625, loss 0.00181693, acc 1
2016-09-06T03:50:20.088788: step 17626, loss 0.00169989, acc 1
2016-09-06T03:50:20.899800: step 17627, loss 0.00220564, acc 1
2016-09-06T03:50:21.707796: step 17628, loss 0.00573161, acc 1
2016-09-06T03:50:22.507787: step 17629, loss 0.0137139, acc 1
2016-09-06T03:50:23.348639: step 17630, loss 0.00644923, acc 1
2016-09-06T03:50:24.165130: step 17631, loss 0.00170322, acc 1
2016-09-06T03:50:24.969005: step 17632, loss 0.0306417, acc 0.98
2016-09-06T03:50:25.786327: step 17633, loss 0.0713728, acc 0.94
2016-09-06T03:50:26.571833: step 17634, loss 0.0270909, acc 0.98
2016-09-06T03:50:27.383626: step 17635, loss 0.0164814, acc 0.98
2016-09-06T03:50:28.173666: step 17636, loss 0.00974708, acc 1
2016-09-06T03:50:28.952440: step 17637, loss 0.0016269, acc 1
2016-09-06T03:50:29.762126: step 17638, loss 0.0193272, acc 0.98
2016-09-06T03:50:30.611478: step 17639, loss 0.00271262, acc 1
2016-09-06T03:50:31.398911: step 17640, loss 0.0263329, acc 0.98
2016-09-06T03:50:32.174070: step 17641, loss 0.0203659, acc 1
2016-09-06T03:50:32.995763: step 17642, loss 0.0174641, acc 0.98
2016-09-06T03:50:33.816945: step 17643, loss 0.00504073, acc 1
2016-09-06T03:50:34.609489: step 17644, loss 0.00334582, acc 1
2016-09-06T03:50:35.437631: step 17645, loss 0.00161421, acc 1
2016-09-06T03:50:36.201073: step 17646, loss 0.0214787, acc 0.98
2016-09-06T03:50:36.981403: step 17647, loss 0.0153446, acc 1
2016-09-06T03:50:37.803133: step 17648, loss 0.00162122, acc 1
2016-09-06T03:50:38.598236: step 17649, loss 0.00168847, acc 1
2016-09-06T03:50:39.416183: step 17650, loss 0.0428417, acc 0.98
2016-09-06T03:50:40.247369: step 17651, loss 0.00213239, acc 1
2016-09-06T03:50:41.048826: step 17652, loss 0.0275024, acc 0.98
2016-09-06T03:50:41.856798: step 17653, loss 0.00161719, acc 1
2016-09-06T03:50:42.647248: step 17654, loss 0.0174519, acc 0.98
2016-09-06T03:50:43.435693: step 17655, loss 0.00161081, acc 1
2016-09-06T03:50:44.233314: step 17656, loss 0.00178035, acc 1
2016-09-06T03:50:45.065068: step 17657, loss 0.00169928, acc 1
2016-09-06T03:50:45.851330: step 17658, loss 0.00915532, acc 1
2016-09-06T03:50:46.676094: step 17659, loss 0.00569695, acc 1
2016-09-06T03:50:47.489455: step 17660, loss 0.00165013, acc 1
2016-09-06T03:50:48.265354: step 17661, loss 0.013118, acc 1
2016-09-06T03:50:49.066377: step 17662, loss 0.00195619, acc 1
2016-09-06T03:50:49.901434: step 17663, loss 0.0367821, acc 0.98
2016-09-06T03:50:50.632140: step 17664, loss 0.00357518, acc 1
2016-09-06T03:50:51.456217: step 17665, loss 0.0175975, acc 0.98
2016-09-06T03:50:52.294790: step 17666, loss 0.014755, acc 1
2016-09-06T03:50:53.065589: step 17667, loss 0.00952389, acc 1
2016-09-06T03:50:53.888850: step 17668, loss 0.0063086, acc 1
2016-09-06T03:50:54.689446: step 17669, loss 0.00218157, acc 1
2016-09-06T03:50:55.481315: step 17670, loss 0.00393817, acc 1
2016-09-06T03:50:56.299368: step 17671, loss 0.00199895, acc 1
2016-09-06T03:50:57.113703: step 17672, loss 0.0100627, acc 1
2016-09-06T03:50:57.893376: step 17673, loss 0.0382103, acc 0.98
2016-09-06T03:50:58.704012: step 17674, loss 0.00170757, acc 1
2016-09-06T03:50:59.504713: step 17675, loss 0.0464827, acc 0.96
2016-09-06T03:51:00.349128: step 17676, loss 0.00237673, acc 1
2016-09-06T03:51:01.147598: step 17677, loss 0.00276464, acc 1
2016-09-06T03:51:01.960154: step 17678, loss 0.00894754, acc 1
2016-09-06T03:51:02.741064: step 17679, loss 0.00174337, acc 1
2016-09-06T03:51:03.570203: step 17680, loss 0.0172988, acc 1
2016-09-06T03:51:04.396744: step 17681, loss 0.00167929, acc 1
2016-09-06T03:51:05.179905: step 17682, loss 0.0133489, acc 1
2016-09-06T03:51:05.991755: step 17683, loss 0.00165711, acc 1
2016-09-06T03:51:06.799861: step 17684, loss 0.00442434, acc 1
2016-09-06T03:51:07.592706: step 17685, loss 0.00171973, acc 1
2016-09-06T03:51:08.405080: step 17686, loss 0.065678, acc 0.98
2016-09-06T03:51:09.259010: step 17687, loss 0.0174085, acc 0.98
2016-09-06T03:51:10.064922: step 17688, loss 0.0195387, acc 1
2016-09-06T03:51:10.871800: step 17689, loss 0.0173823, acc 1
2016-09-06T03:51:11.722518: step 17690, loss 0.0364913, acc 0.98
2016-09-06T03:51:12.546825: step 17691, loss 0.0157008, acc 0.98
2016-09-06T03:51:13.364152: step 17692, loss 0.00500543, acc 1
2016-09-06T03:51:14.213193: step 17693, loss 0.0189186, acc 1
2016-09-06T03:51:15.041506: step 17694, loss 0.00158487, acc 1
2016-09-06T03:51:15.876885: step 17695, loss 0.00332518, acc 1
2016-09-06T03:51:16.721662: step 17696, loss 0.00168348, acc 1
2016-09-06T03:51:17.533736: step 17697, loss 0.00168711, acc 1
2016-09-06T03:51:18.326656: step 17698, loss 0.0102929, acc 1
2016-09-06T03:51:19.153338: step 17699, loss 0.00167154, acc 1
2016-09-06T03:51:19.956471: step 17700, loss 0.00168864, acc 1

Evaluation:
2016-09-06T03:51:23.692980: step 17700, loss 3.07809, acc 0.697936

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-17700

2016-09-06T03:51:25.500262: step 17701, loss 0.00246065, acc 1
2016-09-06T03:51:26.316139: step 17702, loss 0.0140444, acc 1
2016-09-06T03:51:27.118534: step 17703, loss 0.0020803, acc 1
2016-09-06T03:51:27.914378: step 17704, loss 0.00174035, acc 1
2016-09-06T03:51:28.722541: step 17705, loss 0.0180316, acc 1
2016-09-06T03:51:29.492528: step 17706, loss 0.0156252, acc 0.98
2016-09-06T03:51:30.299616: step 17707, loss 0.0174104, acc 0.98
2016-09-06T03:51:31.127144: step 17708, loss 0.00520737, acc 1
2016-09-06T03:51:31.916788: step 17709, loss 0.00176631, acc 1
2016-09-06T03:51:32.737816: step 17710, loss 0.00259072, acc 1
2016-09-06T03:51:33.532286: step 17711, loss 0.0118515, acc 1
2016-09-06T03:51:34.360563: step 17712, loss 0.0028434, acc 1
2016-09-06T03:51:35.145100: step 17713, loss 0.00676787, acc 1
2016-09-06T03:51:35.980116: step 17714, loss 0.00181351, acc 1
2016-09-06T03:51:36.772491: step 17715, loss 0.00170635, acc 1
2016-09-06T03:51:37.567850: step 17716, loss 0.0393478, acc 0.96
2016-09-06T03:51:38.384584: step 17717, loss 0.022677, acc 1
2016-09-06T03:51:39.190707: step 17718, loss 0.0157791, acc 0.98
2016-09-06T03:51:39.977374: step 17719, loss 0.0123378, acc 1
2016-09-06T03:51:40.800982: step 17720, loss 0.00165624, acc 1
2016-09-06T03:51:41.577574: step 17721, loss 0.00167696, acc 1
2016-09-06T03:51:42.369600: step 17722, loss 0.0155518, acc 1
2016-09-06T03:51:43.185612: step 17723, loss 0.012702, acc 1
2016-09-06T03:51:43.969300: step 17724, loss 0.00166278, acc 1
2016-09-06T03:51:44.779369: step 17725, loss 0.0480288, acc 0.96
2016-09-06T03:51:45.596014: step 17726, loss 0.00245563, acc 1
2016-09-06T03:51:46.385344: step 17727, loss 0.00159924, acc 1
2016-09-06T03:51:47.178282: step 17728, loss 0.01913, acc 0.98
2016-09-06T03:51:47.987581: step 17729, loss 0.00209379, acc 1
2016-09-06T03:51:48.772898: step 17730, loss 0.0103328, acc 1
2016-09-06T03:51:49.574948: step 17731, loss 0.00214633, acc 1
2016-09-06T03:51:50.387701: step 17732, loss 0.0459971, acc 0.98
2016-09-06T03:51:51.181546: step 17733, loss 0.00297625, acc 1
2016-09-06T03:51:51.994215: step 17734, loss 0.0222928, acc 0.98
2016-09-06T03:51:52.826384: step 17735, loss 0.0118867, acc 1
2016-09-06T03:51:53.627354: step 17736, loss 0.00330234, acc 1
2016-09-06T03:51:54.432591: step 17737, loss 0.00149792, acc 1
2016-09-06T03:51:55.248743: step 17738, loss 0.0140305, acc 1
2016-09-06T03:51:56.029783: step 17739, loss 0.0267779, acc 0.98
2016-09-06T03:51:56.857139: step 17740, loss 0.0182107, acc 1
2016-09-06T03:51:57.669258: step 17741, loss 0.00158467, acc 1
2016-09-06T03:51:58.453406: step 17742, loss 0.0160106, acc 0.98
2016-09-06T03:51:59.268744: step 17743, loss 0.00142929, acc 1
2016-09-06T03:52:00.092159: step 17744, loss 0.00799914, acc 1
2016-09-06T03:52:00.909728: step 17745, loss 0.00141065, acc 1
2016-09-06T03:52:01.719471: step 17746, loss 0.00499886, acc 1
2016-09-06T03:52:02.568839: step 17747, loss 0.0280697, acc 1
2016-09-06T03:52:03.332886: step 17748, loss 0.00140172, acc 1
2016-09-06T03:52:04.119033: step 17749, loss 0.0116708, acc 1
2016-09-06T03:52:04.947419: step 17750, loss 0.0174235, acc 0.98
2016-09-06T03:52:05.747301: step 17751, loss 0.0152859, acc 0.98
2016-09-06T03:52:06.560415: step 17752, loss 0.0136479, acc 1
2016-09-06T03:52:07.413414: step 17753, loss 0.0966509, acc 0.96
2016-09-06T03:52:08.203109: step 17754, loss 0.00144909, acc 1
2016-09-06T03:52:09.007059: step 17755, loss 0.0142237, acc 1
2016-09-06T03:52:09.814028: step 17756, loss 0.00129261, acc 1
2016-09-06T03:52:10.569514: step 17757, loss 0.0198496, acc 1
2016-09-06T03:52:11.351253: step 17758, loss 0.00130107, acc 1
2016-09-06T03:52:12.161417: step 17759, loss 0.00639965, acc 1
2016-09-06T03:52:12.945483: step 17760, loss 0.00198438, acc 1
2016-09-06T03:52:13.754877: step 17761, loss 0.00130732, acc 1
2016-09-06T03:52:14.556210: step 17762, loss 0.0191454, acc 1
2016-09-06T03:52:15.341415: step 17763, loss 0.00174664, acc 1
2016-09-06T03:52:16.162359: step 17764, loss 0.0123804, acc 1
2016-09-06T03:52:16.970132: step 17765, loss 0.00218065, acc 1
2016-09-06T03:52:17.761463: step 17766, loss 0.00131391, acc 1
2016-09-06T03:52:18.586443: step 17767, loss 0.00150001, acc 1
2016-09-06T03:52:19.395725: step 17768, loss 0.00136866, acc 1
2016-09-06T03:52:20.173477: step 17769, loss 0.0175901, acc 0.98
2016-09-06T03:52:21.020845: step 17770, loss 0.0447251, acc 0.96
2016-09-06T03:52:21.813545: step 17771, loss 0.0149744, acc 1
2016-09-06T03:52:22.627024: step 17772, loss 0.00307749, acc 1
2016-09-06T03:52:23.447864: step 17773, loss 0.0192264, acc 1
2016-09-06T03:52:24.276278: step 17774, loss 0.0295376, acc 0.98
2016-09-06T03:52:25.069933: step 17775, loss 0.0013756, acc 1
2016-09-06T03:52:25.877960: step 17776, loss 0.0182861, acc 0.98
2016-09-06T03:52:26.694124: step 17777, loss 0.00334376, acc 1
2016-09-06T03:52:27.472485: step 17778, loss 0.00418966, acc 1
2016-09-06T03:52:28.272134: step 17779, loss 0.00854739, acc 1
2016-09-06T03:52:29.090624: step 17780, loss 0.00144938, acc 1
2016-09-06T03:52:29.871422: step 17781, loss 0.00145743, acc 1
2016-09-06T03:52:30.691577: step 17782, loss 0.00152654, acc 1
2016-09-06T03:52:31.503614: step 17783, loss 0.00483039, acc 1
2016-09-06T03:52:32.287321: step 17784, loss 0.00196878, acc 1
2016-09-06T03:52:33.107102: step 17785, loss 0.00134997, acc 1
2016-09-06T03:52:33.915492: step 17786, loss 0.0013206, acc 1
2016-09-06T03:52:34.721897: step 17787, loss 0.00526004, acc 1
2016-09-06T03:52:35.568601: step 17788, loss 0.0159028, acc 0.98
2016-09-06T03:52:36.384169: step 17789, loss 0.0160846, acc 0.98
2016-09-06T03:52:37.174654: step 17790, loss 0.0178839, acc 0.98
2016-09-06T03:52:37.965513: step 17791, loss 0.00213084, acc 1
2016-09-06T03:52:38.796675: step 17792, loss 0.0107114, acc 1
2016-09-06T03:52:39.592957: step 17793, loss 0.0163047, acc 1
2016-09-06T03:52:40.419995: step 17794, loss 0.0127495, acc 1
2016-09-06T03:52:41.237187: step 17795, loss 0.107122, acc 0.96
2016-09-06T03:52:41.998702: step 17796, loss 0.00138198, acc 1
2016-09-06T03:52:42.792501: step 17797, loss 0.00367897, acc 1
2016-09-06T03:52:43.606295: step 17798, loss 0.0996197, acc 0.96
2016-09-06T03:52:44.399930: step 17799, loss 0.0229244, acc 0.98
2016-09-06T03:52:45.219433: step 17800, loss 0.0108166, acc 1

Evaluation:
2016-09-06T03:52:48.976816: step 17800, loss 2.97068, acc 0.708255

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-17800

2016-09-06T03:52:50.855316: step 17801, loss 0.00234624, acc 1
2016-09-06T03:52:51.661571: step 17802, loss 0.00433807, acc 1
2016-09-06T03:52:52.490298: step 17803, loss 0.00179848, acc 1
2016-09-06T03:52:53.318307: step 17804, loss 0.00316032, acc 1
2016-09-06T03:52:54.121462: step 17805, loss 0.00570892, acc 1
2016-09-06T03:52:54.949814: step 17806, loss 0.0019126, acc 1
2016-09-06T03:52:55.774730: step 17807, loss 0.00227976, acc 1
2016-09-06T03:52:56.540934: step 17808, loss 0.0457306, acc 0.98
2016-09-06T03:52:57.336939: step 17809, loss 0.0209734, acc 0.98
2016-09-06T03:52:58.160369: step 17810, loss 0.0153968, acc 1
2016-09-06T03:52:58.949157: step 17811, loss 0.00343778, acc 1
2016-09-06T03:52:59.756789: step 17812, loss 0.00233443, acc 1
2016-09-06T03:53:00.616704: step 17813, loss 0.0028278, acc 1
2016-09-06T03:53:01.460077: step 17814, loss 0.0146476, acc 1
2016-09-06T03:53:02.267851: step 17815, loss 0.00214175, acc 1
2016-09-06T03:53:03.084235: step 17816, loss 0.0205271, acc 0.98
2016-09-06T03:53:03.926722: step 17817, loss 0.00191436, acc 1
2016-09-06T03:53:04.729488: step 17818, loss 0.00192017, acc 1
2016-09-06T03:53:05.549557: step 17819, loss 0.00662079, acc 1
2016-09-06T03:53:06.352452: step 17820, loss 0.0140154, acc 1
2016-09-06T03:53:07.154680: step 17821, loss 0.00180937, acc 1
2016-09-06T03:53:07.988083: step 17822, loss 0.00378463, acc 1
2016-09-06T03:53:08.812939: step 17823, loss 0.0163298, acc 1
2016-09-06T03:53:09.626738: step 17824, loss 0.0336437, acc 0.98
2016-09-06T03:53:10.469412: step 17825, loss 0.0106169, acc 1
2016-09-06T03:53:11.293268: step 17826, loss 0.00223612, acc 1
2016-09-06T03:53:12.085751: step 17827, loss 0.0211499, acc 0.98
2016-09-06T03:53:12.897014: step 17828, loss 0.00180377, acc 1
2016-09-06T03:53:13.701458: step 17829, loss 0.00177774, acc 1
2016-09-06T03:53:14.554735: step 17830, loss 0.0251047, acc 0.98
2016-09-06T03:53:15.378895: step 17831, loss 0.00752049, acc 1
2016-09-06T03:53:16.226898: step 17832, loss 0.00197874, acc 1
2016-09-06T03:53:16.998710: step 17833, loss 0.00275362, acc 1
2016-09-06T03:53:17.858879: step 17834, loss 0.00310659, acc 1
2016-09-06T03:53:18.684474: step 17835, loss 0.0259074, acc 0.98
2016-09-06T03:53:19.462784: step 17836, loss 0.0116753, acc 1
2016-09-06T03:53:20.274583: step 17837, loss 0.0158999, acc 0.98
2016-09-06T03:53:21.078148: step 17838, loss 0.00525655, acc 1
2016-09-06T03:53:21.888581: step 17839, loss 0.051023, acc 0.98
2016-09-06T03:53:22.707613: step 17840, loss 0.0261136, acc 0.98
2016-09-06T03:53:23.530949: step 17841, loss 0.0186153, acc 1
2016-09-06T03:53:24.310617: step 17842, loss 0.0432779, acc 0.98
2016-09-06T03:53:25.131644: step 17843, loss 0.0342004, acc 0.98
2016-09-06T03:53:26.019212: step 17844, loss 0.0115959, acc 1
2016-09-06T03:53:26.853126: step 17845, loss 0.00208994, acc 1
2016-09-06T03:53:27.710747: step 17846, loss 0.00483003, acc 1
2016-09-06T03:53:28.536620: step 17847, loss 0.00339973, acc 1
2016-09-06T03:53:29.359585: step 17848, loss 0.01492, acc 1
2016-09-06T03:53:30.251673: step 17849, loss 0.00942927, acc 1
2016-09-06T03:53:31.145967: step 17850, loss 0.0146293, acc 1
2016-09-06T03:53:32.053648: step 17851, loss 0.0125455, acc 1
2016-09-06T03:53:32.949901: step 17852, loss 0.0416538, acc 0.98
2016-09-06T03:53:33.982146: step 17853, loss 0.103865, acc 0.98
2016-09-06T03:53:35.014325: step 17854, loss 0.0198934, acc 0.98
2016-09-06T03:53:35.855761: step 17855, loss 0.00285634, acc 1
2016-09-06T03:53:36.647597: step 17856, loss 0.00690922, acc 1
2016-09-06T03:53:37.479647: step 17857, loss 0.00301914, acc 1
2016-09-06T03:53:38.291906: step 17858, loss 0.0166167, acc 1
2016-09-06T03:53:39.110387: step 17859, loss 0.00293, acc 1
2016-09-06T03:53:39.971053: step 17860, loss 0.00288694, acc 1
2016-09-06T03:53:40.850587: step 17861, loss 0.00294827, acc 1
2016-09-06T03:53:41.670469: step 17862, loss 0.0032461, acc 1
2016-09-06T03:53:42.536063: step 17863, loss 0.00311316, acc 1
2016-09-06T03:53:43.367758: step 17864, loss 0.00316417, acc 1
2016-09-06T03:53:44.335095: step 17865, loss 0.0073729, acc 1
2016-09-06T03:53:45.265334: step 17866, loss 0.00320881, acc 1
2016-09-06T03:53:46.117986: step 17867, loss 0.0129479, acc 1
2016-09-06T03:53:46.995146: step 17868, loss 0.0633967, acc 0.96
2016-09-06T03:53:48.124367: step 17869, loss 0.0360791, acc 0.98
2016-09-06T03:53:49.140684: step 17870, loss 0.00356653, acc 1
2016-09-06T03:53:50.236406: step 17871, loss 0.00326605, acc 1
2016-09-06T03:53:51.106604: step 17872, loss 0.0299774, acc 0.98
2016-09-06T03:53:52.030354: step 17873, loss 0.0547898, acc 0.98
2016-09-06T03:53:53.042994: step 17874, loss 0.00334262, acc 1
2016-09-06T03:53:53.972343: step 17875, loss 0.00317579, acc 1
2016-09-06T03:53:55.048422: step 17876, loss 0.0038162, acc 1
2016-09-06T03:53:55.942085: step 17877, loss 0.00628328, acc 1
2016-09-06T03:53:56.775609: step 17878, loss 0.0536679, acc 0.96
2016-09-06T03:53:57.766482: step 17879, loss 0.0132576, acc 1
2016-09-06T03:53:58.614369: step 17880, loss 0.00304721, acc 1
2016-09-06T03:53:59.648036: step 17881, loss 0.00663712, acc 1
2016-09-06T03:54:00.487951: step 17882, loss 0.0186666, acc 0.98
2016-09-06T03:54:01.371973: step 17883, loss 0.00313792, acc 1
2016-09-06T03:54:02.179068: step 17884, loss 0.0396219, acc 0.98
2016-09-06T03:54:03.073747: step 17885, loss 0.126864, acc 0.96
2016-09-06T03:54:03.890901: step 17886, loss 0.00283201, acc 1
2016-09-06T03:54:04.692595: step 17887, loss 0.0127253, acc 1
2016-09-06T03:54:05.453068: step 17888, loss 0.0345766, acc 0.98
2016-09-06T03:54:06.273592: step 17889, loss 0.0208002, acc 1
2016-09-06T03:54:07.113773: step 17890, loss 0.0205306, acc 0.98
2016-09-06T03:54:07.901348: step 17891, loss 0.00266411, acc 1
2016-09-06T03:54:08.684360: step 17892, loss 0.101746, acc 0.98
2016-09-06T03:54:09.509397: step 17893, loss 0.00260736, acc 1
2016-09-06T03:54:10.296516: step 17894, loss 0.0762013, acc 0.94
2016-09-06T03:54:11.113818: step 17895, loss 0.0153541, acc 1
2016-09-06T03:54:11.920333: step 17896, loss 0.00475737, acc 1
2016-09-06T03:54:12.694564: step 17897, loss 0.0452578, acc 0.98
2016-09-06T03:54:13.492238: step 17898, loss 0.0078569, acc 1
2016-09-06T03:54:14.292575: step 17899, loss 0.0278504, acc 0.98
2016-09-06T03:54:15.088750: step 17900, loss 0.00306752, acc 1

Evaluation:
2016-09-06T03:54:18.823293: step 17900, loss 2.42104, acc 0.712946

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-17900

2016-09-06T03:54:20.672195: step 17901, loss 0.0033172, acc 1
2016-09-06T03:54:21.483413: step 17902, loss 0.00813351, acc 1
2016-09-06T03:54:22.321424: step 17903, loss 0.0532905, acc 0.96
2016-09-06T03:54:23.162330: step 17904, loss 0.0261507, acc 0.98
2016-09-06T03:54:23.968838: step 17905, loss 0.00714759, acc 1
2016-09-06T03:54:24.789968: step 17906, loss 0.00355018, acc 1
2016-09-06T03:54:25.625234: step 17907, loss 0.00415102, acc 1
2016-09-06T03:54:26.428661: step 17908, loss 0.0373703, acc 1
2016-09-06T03:54:27.222792: step 17909, loss 0.0298806, acc 1
2016-09-06T03:54:28.076992: step 17910, loss 0.0210259, acc 0.98
2016-09-06T03:54:28.898514: step 17911, loss 0.00445243, acc 1
2016-09-06T03:54:29.696557: step 17912, loss 0.0395414, acc 0.98
2016-09-06T03:54:30.514934: step 17913, loss 0.0055771, acc 1
2016-09-06T03:54:31.404331: step 17914, loss 0.0302017, acc 0.98
2016-09-06T03:54:32.163506: step 17915, loss 0.018141, acc 0.98
2016-09-06T03:54:32.963328: step 17916, loss 0.00327008, acc 1
2016-09-06T03:54:33.778724: step 17917, loss 0.038893, acc 1
2016-09-06T03:54:34.602826: step 17918, loss 0.00320201, acc 1
2016-09-06T03:54:35.418088: step 17919, loss 0.00332465, acc 1
2016-09-06T03:54:36.250313: step 17920, loss 0.0161701, acc 1
2016-09-06T03:54:37.045419: step 17921, loss 0.00754663, acc 1
2016-09-06T03:54:37.941355: step 17922, loss 0.00332816, acc 1
2016-09-06T03:54:38.765814: step 17923, loss 0.0175599, acc 0.98
2016-09-06T03:54:39.578395: step 17924, loss 0.00916054, acc 1
2016-09-06T03:54:40.401192: step 17925, loss 0.00323943, acc 1
2016-09-06T03:54:41.330488: step 17926, loss 0.00323766, acc 1
2016-09-06T03:54:42.156257: step 17927, loss 0.0292249, acc 0.98
2016-09-06T03:54:42.989770: step 17928, loss 0.00437843, acc 1
2016-09-06T03:54:43.808445: step 17929, loss 0.0272801, acc 0.98
2016-09-06T03:54:44.653998: step 17930, loss 0.00322545, acc 1
2016-09-06T03:54:45.447232: step 17931, loss 0.00335232, acc 1
2016-09-06T03:54:46.255225: step 17932, loss 0.00446991, acc 1
2016-09-06T03:54:47.102983: step 17933, loss 0.00335912, acc 1
2016-09-06T03:54:47.888854: step 17934, loss 0.00766452, acc 1
2016-09-06T03:54:48.744597: step 17935, loss 0.0352727, acc 0.98
2016-09-06T03:54:49.596548: step 17936, loss 0.00325411, acc 1
2016-09-06T03:54:50.455506: step 17937, loss 0.00321047, acc 1
2016-09-06T03:54:51.312853: step 17938, loss 0.0142162, acc 1
2016-09-06T03:54:52.173728: step 17939, loss 0.0600318, acc 0.98
2016-09-06T03:54:52.977764: step 17940, loss 0.00419216, acc 1
2016-09-06T03:54:53.827893: step 17941, loss 0.0125727, acc 1
2016-09-06T03:54:54.658287: step 17942, loss 0.0586101, acc 0.96
2016-09-06T03:54:55.479231: step 17943, loss 0.00309955, acc 1
2016-09-06T03:54:56.300482: step 17944, loss 0.00607016, acc 1
2016-09-06T03:54:57.157738: step 17945, loss 0.0109593, acc 1
2016-09-06T03:54:57.990382: step 17946, loss 0.0108068, acc 1
2016-09-06T03:54:58.764145: step 17947, loss 0.0817013, acc 0.96
2016-09-06T03:54:59.603734: step 17948, loss 0.0131875, acc 1
2016-09-06T03:55:00.452235: step 17949, loss 0.00806903, acc 1
2016-09-06T03:55:01.269399: step 17950, loss 0.00389658, acc 1
2016-09-06T03:55:02.096634: step 17951, loss 0.0260068, acc 0.98
2016-09-06T03:55:02.937112: step 17952, loss 0.0105633, acc 1
2016-09-06T03:55:03.749753: step 17953, loss 0.00259317, acc 1
2016-09-06T03:55:04.556157: step 17954, loss 0.0247911, acc 1
2016-09-06T03:55:05.374227: step 17955, loss 0.00949405, acc 1
2016-09-06T03:55:06.204274: step 17956, loss 0.00308125, acc 1
2016-09-06T03:55:07.029399: step 17957, loss 0.0388003, acc 0.98
2016-09-06T03:55:07.869778: step 17958, loss 0.00219336, acc 1
2016-09-06T03:55:08.670560: step 17959, loss 0.00217305, acc 1
2016-09-06T03:55:09.480743: step 17960, loss 0.00217376, acc 1
2016-09-06T03:55:10.303812: step 17961, loss 0.0570796, acc 0.98
2016-09-06T03:55:11.112581: step 17962, loss 0.00663638, acc 1
2016-09-06T03:55:11.914684: step 17963, loss 0.0187217, acc 1
2016-09-06T03:55:12.734685: step 17964, loss 0.0353353, acc 0.96
2016-09-06T03:55:13.543119: step 17965, loss 0.0374358, acc 0.96
2016-09-06T03:55:14.350875: step 17966, loss 0.0381249, acc 0.98
2016-09-06T03:55:15.216986: step 17967, loss 0.00192393, acc 1
2016-09-06T03:55:15.999477: step 17968, loss 0.0497293, acc 0.96
2016-09-06T03:55:16.828651: step 17969, loss 0.0542289, acc 0.98
2016-09-06T03:55:17.647660: step 17970, loss 0.0621138, acc 0.98
2016-09-06T03:55:18.490605: step 17971, loss 0.0122991, acc 1
2016-09-06T03:55:19.296812: step 17972, loss 0.00201853, acc 1
2016-09-06T03:55:20.095734: step 17973, loss 0.0117134, acc 1
2016-09-06T03:55:20.918881: step 17974, loss 0.00376341, acc 1
2016-09-06T03:55:21.714229: step 17975, loss 0.0212983, acc 1
2016-09-06T03:55:22.516028: step 17976, loss 0.0340656, acc 0.98
2016-09-06T03:55:23.330143: step 17977, loss 0.0049903, acc 1
2016-09-06T03:55:24.117914: step 17978, loss 0.00205521, acc 1
2016-09-06T03:55:24.945795: step 17979, loss 0.011635, acc 1
2016-09-06T03:55:25.756263: step 17980, loss 0.00938408, acc 1
2016-09-06T03:55:26.541559: step 17981, loss 0.0104372, acc 1
2016-09-06T03:55:27.356626: step 17982, loss 0.0022207, acc 1
2016-09-06T03:55:28.180161: step 17983, loss 0.0393588, acc 0.98
2016-09-06T03:55:28.954985: step 17984, loss 0.00480321, acc 1
2016-09-06T03:55:29.767865: step 17985, loss 0.0254116, acc 1
2016-09-06T03:55:30.603496: step 17986, loss 0.0356244, acc 0.98
2016-09-06T03:55:31.401326: step 17987, loss 0.00893648, acc 1
2016-09-06T03:55:32.204281: step 17988, loss 0.0034843, acc 1
2016-09-06T03:55:33.012508: step 17989, loss 0.0144718, acc 1
2016-09-06T03:55:33.817751: step 17990, loss 0.0114063, acc 1
2016-09-06T03:55:34.635909: step 17991, loss 0.00697577, acc 1
2016-09-06T03:55:35.466237: step 17992, loss 0.0107837, acc 1
2016-09-06T03:55:36.251303: step 17993, loss 0.0202521, acc 0.98
2016-09-06T03:55:37.085452: step 17994, loss 0.0496502, acc 0.96
2016-09-06T03:55:37.901146: step 17995, loss 0.00349188, acc 1
2016-09-06T03:55:38.685388: step 17996, loss 0.00188288, acc 1
2016-09-06T03:55:39.490309: step 17997, loss 0.0321656, acc 0.98
2016-09-06T03:55:40.312296: step 17998, loss 0.00935074, acc 1
2016-09-06T03:55:41.083691: step 17999, loss 0.00293193, acc 1
2016-09-06T03:55:41.869379: step 18000, loss 0.0518088, acc 0.96

Evaluation:
2016-09-06T03:55:45.625359: step 18000, loss 2.74328, acc 0.712008

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-18000

2016-09-06T03:55:47.478174: step 18001, loss 0.0119981, acc 1
2016-09-06T03:55:48.283254: step 18002, loss 0.00795003, acc 1
2016-09-06T03:55:49.132535: step 18003, loss 0.00470607, acc 1
2016-09-06T03:55:49.945309: step 18004, loss 0.00187119, acc 1
2016-09-06T03:55:50.781311: step 18005, loss 0.0168724, acc 0.98
2016-09-06T03:55:51.579591: step 18006, loss 0.0148121, acc 1
2016-09-06T03:55:52.386710: step 18007, loss 0.00182191, acc 1
2016-09-06T03:55:53.185419: step 18008, loss 0.00188983, acc 1
2016-09-06T03:55:54.035647: step 18009, loss 0.00188992, acc 1
2016-09-06T03:55:54.861146: step 18010, loss 0.00225509, acc 1
2016-09-06T03:55:55.680118: step 18011, loss 0.0173681, acc 1
2016-09-06T03:55:56.496412: step 18012, loss 0.0152656, acc 1
2016-09-06T03:55:57.352426: step 18013, loss 0.00182089, acc 1
2016-09-06T03:55:58.168559: step 18014, loss 0.0188421, acc 1
2016-09-06T03:55:58.996814: step 18015, loss 0.00308348, acc 1
2016-09-06T03:55:59.817607: step 18016, loss 0.0114066, acc 1
2016-09-06T03:56:00.665407: step 18017, loss 0.0135345, acc 1
2016-09-06T03:56:01.521044: step 18018, loss 0.00184863, acc 1
2016-09-06T03:56:02.345900: step 18019, loss 0.00183387, acc 1
2016-09-06T03:56:03.143172: step 18020, loss 0.00183412, acc 1
2016-09-06T03:56:04.000520: step 18021, loss 0.0106474, acc 1
2016-09-06T03:56:04.842228: step 18022, loss 0.0168119, acc 1
2016-09-06T03:56:05.663489: step 18023, loss 0.00541195, acc 1
2016-09-06T03:56:06.551704: step 18024, loss 0.0037735, acc 1
2016-09-06T03:56:07.358055: step 18025, loss 0.0196845, acc 1
2016-09-06T03:56:08.202332: step 18026, loss 0.0161513, acc 1
2016-09-06T03:56:09.051534: step 18027, loss 0.0141804, acc 1
2016-09-06T03:56:09.943003: step 18028, loss 0.00200218, acc 1
2016-09-06T03:56:10.775775: step 18029, loss 0.00189337, acc 1
2016-09-06T03:56:11.593405: step 18030, loss 0.0687725, acc 0.94
2016-09-06T03:56:12.421493: step 18031, loss 0.0163432, acc 0.98
2016-09-06T03:56:13.248807: step 18032, loss 0.00187953, acc 1
2016-09-06T03:56:14.233480: step 18033, loss 0.00753858, acc 1
2016-09-06T03:56:15.056208: step 18034, loss 0.0349589, acc 0.98
2016-09-06T03:56:15.894869: step 18035, loss 0.00919059, acc 1
2016-09-06T03:56:16.722778: step 18036, loss 0.0097601, acc 1
2016-09-06T03:56:17.575589: step 18037, loss 0.00336475, acc 1
2016-09-06T03:56:18.443991: step 18038, loss 0.0103292, acc 1
2016-09-06T03:56:19.302573: step 18039, loss 0.00240588, acc 1
2016-09-06T03:56:20.238339: step 18040, loss 0.00183963, acc 1
2016-09-06T03:56:21.046751: step 18041, loss 0.00187339, acc 1
2016-09-06T03:56:21.885124: step 18042, loss 0.0176212, acc 1
2016-09-06T03:56:22.751056: step 18043, loss 0.0226027, acc 1
2016-09-06T03:56:23.545098: step 18044, loss 0.00187732, acc 1
2016-09-06T03:56:24.392709: step 18045, loss 0.00693625, acc 1
2016-09-06T03:56:25.200150: step 18046, loss 0.00202023, acc 1
2016-09-06T03:56:26.019470: step 18047, loss 0.00276248, acc 1
2016-09-06T03:56:26.800512: step 18048, loss 0.00207384, acc 1
2016-09-06T03:56:27.622444: step 18049, loss 0.00191338, acc 1
2016-09-06T03:56:28.442739: step 18050, loss 0.0157481, acc 1
2016-09-06T03:56:29.258388: step 18051, loss 0.00190464, acc 1
2016-09-06T03:56:30.085132: step 18052, loss 0.00279854, acc 1
2016-09-06T03:56:30.909994: step 18053, loss 0.016888, acc 0.98
2016-09-06T03:56:31.700479: step 18054, loss 0.0022993, acc 1
2016-09-06T03:56:32.537187: step 18055, loss 0.00187088, acc 1
2016-09-06T03:56:33.345581: step 18056, loss 0.00191602, acc 1
2016-09-06T03:56:34.160843: step 18057, loss 0.0112382, acc 1
2016-09-06T03:56:35.001077: step 18058, loss 0.00183659, acc 1
2016-09-06T03:56:35.831970: step 18059, loss 0.0182757, acc 0.98
2016-09-06T03:56:36.648786: step 18060, loss 0.0180986, acc 1
2016-09-06T03:56:37.460477: step 18061, loss 0.0241996, acc 0.98
2016-09-06T03:56:38.292875: step 18062, loss 0.00674473, acc 1
2016-09-06T03:56:39.135898: step 18063, loss 0.00185757, acc 1
2016-09-06T03:56:39.941592: step 18064, loss 0.0262674, acc 0.98
2016-09-06T03:56:40.825636: step 18065, loss 0.0301747, acc 0.98
2016-09-06T03:56:41.611089: step 18066, loss 0.00959254, acc 1
2016-09-06T03:56:42.415712: step 18067, loss 0.00347445, acc 1
2016-09-06T03:56:43.262250: step 18068, loss 0.0326651, acc 0.98
2016-09-06T03:56:44.099767: step 18069, loss 0.00174812, acc 1
2016-09-06T03:56:44.935795: step 18070, loss 0.00738256, acc 1
2016-09-06T03:56:45.777298: step 18071, loss 0.00173751, acc 1
2016-09-06T03:56:46.605454: step 18072, loss 0.00230066, acc 1
2016-09-06T03:56:47.405434: step 18073, loss 0.00844508, acc 1
2016-09-06T03:56:48.246571: step 18074, loss 0.0405872, acc 0.98
2016-09-06T03:56:49.059481: step 18075, loss 0.0133133, acc 1
2016-09-06T03:56:49.860726: step 18076, loss 0.0274241, acc 0.98
2016-09-06T03:56:50.720757: step 18077, loss 0.0133651, acc 1
2016-09-06T03:56:51.522322: step 18078, loss 0.00554198, acc 1
2016-09-06T03:56:52.474919: step 18079, loss 0.00562828, acc 1
2016-09-06T03:56:53.270296: step 18080, loss 0.00186478, acc 1
2016-09-06T03:56:54.075722: step 18081, loss 0.00189807, acc 1
2016-09-06T03:56:54.889216: step 18082, loss 0.049406, acc 0.98
2016-09-06T03:56:55.714956: step 18083, loss 0.00335691, acc 1
2016-09-06T03:56:56.545924: step 18084, loss 0.0121185, acc 1
2016-09-06T03:56:57.386770: step 18085, loss 0.00192617, acc 1
2016-09-06T03:56:58.197069: step 18086, loss 0.0526283, acc 0.96
2016-09-06T03:56:59.030083: step 18087, loss 0.00203208, acc 1
2016-09-06T03:56:59.845239: step 18088, loss 0.00194619, acc 1
2016-09-06T03:57:00.793027: step 18089, loss 0.00395932, acc 1
2016-09-06T03:57:01.658002: step 18090, loss 0.00331304, acc 1
2016-09-06T03:57:02.505502: step 18091, loss 0.0659752, acc 0.96
2016-09-06T03:57:03.315929: step 18092, loss 0.00246674, acc 1
2016-09-06T03:57:04.173158: step 18093, loss 0.00527916, acc 1
2016-09-06T03:57:05.061962: step 18094, loss 0.00835607, acc 1
2016-09-06T03:57:06.036599: step 18095, loss 0.00211353, acc 1
2016-09-06T03:57:06.869366: step 18096, loss 0.00216138, acc 1
2016-09-06T03:57:07.703314: step 18097, loss 0.0219738, acc 0.98
2016-09-06T03:57:08.541453: step 18098, loss 0.00198951, acc 1
2016-09-06T03:57:09.331060: step 18099, loss 0.00247679, acc 1
2016-09-06T03:57:10.159382: step 18100, loss 0.00454132, acc 1

Evaluation:
2016-09-06T03:57:13.890314: step 18100, loss 3.34699, acc 0.71576

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-18100

2016-09-06T03:57:15.787306: step 18101, loss 0.0325253, acc 0.98
2016-09-06T03:57:16.716389: step 18102, loss 0.00244336, acc 1
2016-09-06T03:57:17.560563: step 18103, loss 0.00193585, acc 1
2016-09-06T03:57:18.419909: step 18104, loss 0.00232038, acc 1
2016-09-06T03:57:19.232265: step 18105, loss 0.0216833, acc 0.98
2016-09-06T03:57:20.123860: step 18106, loss 0.0147259, acc 1
2016-09-06T03:57:20.957928: step 18107, loss 0.0190507, acc 0.98
2016-09-06T03:57:21.852680: step 18108, loss 0.00191126, acc 1
2016-09-06T03:57:22.656095: step 18109, loss 0.0743284, acc 0.96
2016-09-06T03:57:23.456440: step 18110, loss 0.00192623, acc 1
2016-09-06T03:57:24.305061: step 18111, loss 0.00182304, acc 1
2016-09-06T03:57:25.131917: step 18112, loss 0.0096379, acc 1
2016-09-06T03:57:25.929138: step 18113, loss 0.00937884, acc 1
2016-09-06T03:57:26.743680: step 18114, loss 0.00532009, acc 1
2016-09-06T03:57:27.524274: step 18115, loss 0.00197109, acc 1
2016-09-06T03:57:28.321460: step 18116, loss 0.0347274, acc 0.98
2016-09-06T03:57:29.179178: step 18117, loss 0.0176612, acc 1
2016-09-06T03:57:29.991950: step 18118, loss 0.00440615, acc 1
2016-09-06T03:57:30.812497: step 18119, loss 0.00832574, acc 1
2016-09-06T03:57:31.649988: step 18120, loss 0.0105059, acc 1
2016-09-06T03:57:32.470422: step 18121, loss 0.00309772, acc 1
2016-09-06T03:57:33.294637: step 18122, loss 0.00416822, acc 1
2016-09-06T03:57:34.101724: step 18123, loss 0.00219137, acc 1
2016-09-06T03:57:34.910321: step 18124, loss 0.0018996, acc 1
2016-09-06T03:57:35.742758: step 18125, loss 0.00220096, acc 1
2016-09-06T03:57:36.609504: step 18126, loss 0.00181337, acc 1
2016-09-06T03:57:37.441302: step 18127, loss 0.0126272, acc 1
2016-09-06T03:57:38.297948: step 18128, loss 0.0177814, acc 1
2016-09-06T03:57:39.156222: step 18129, loss 0.0032035, acc 1
2016-09-06T03:57:40.029601: step 18130, loss 0.00241107, acc 1
2016-09-06T03:57:40.789851: step 18131, loss 0.0371302, acc 0.98
2016-09-06T03:57:41.605271: step 18132, loss 0.00177719, acc 1
2016-09-06T03:57:42.458700: step 18133, loss 0.165302, acc 0.96
2016-09-06T03:57:43.252366: step 18134, loss 0.0289142, acc 0.98
2016-09-06T03:57:44.021595: step 18135, loss 0.00185707, acc 1
2016-09-06T03:57:44.859084: step 18136, loss 0.00551007, acc 1
2016-09-06T03:57:45.643340: step 18137, loss 0.00961351, acc 1
2016-09-06T03:57:46.423838: step 18138, loss 0.00173628, acc 1
2016-09-06T03:57:47.270183: step 18139, loss 0.0168377, acc 1
2016-09-06T03:57:48.079417: step 18140, loss 0.0119644, acc 1
2016-09-06T03:57:48.868139: step 18141, loss 0.001478, acc 1
2016-09-06T03:57:49.698666: step 18142, loss 0.00158252, acc 1
2016-09-06T03:57:50.500779: step 18143, loss 0.0093631, acc 1
2016-09-06T03:57:51.304931: step 18144, loss 0.00146998, acc 1
2016-09-06T03:57:52.112079: step 18145, loss 0.00199039, acc 1
2016-09-06T03:57:52.882772: step 18146, loss 0.00899249, acc 1
2016-09-06T03:57:53.684923: step 18147, loss 0.00852553, acc 1
2016-09-06T03:57:54.500997: step 18148, loss 0.00364193, acc 1
2016-09-06T03:57:55.302974: step 18149, loss 0.0163145, acc 0.98
2016-09-06T03:57:56.127360: step 18150, loss 0.00396406, acc 1
2016-09-06T03:57:56.957109: step 18151, loss 0.00156256, acc 1
2016-09-06T03:57:57.722515: step 18152, loss 0.00151685, acc 1
2016-09-06T03:57:58.533495: step 18153, loss 0.00679109, acc 1
2016-09-06T03:57:59.365373: step 18154, loss 0.0136076, acc 1
2016-09-06T03:58:00.156251: step 18155, loss 0.0116511, acc 1
2016-09-06T03:58:00.979413: step 18156, loss 0.0330513, acc 0.98
2016-09-06T03:58:01.830728: step 18157, loss 0.0125282, acc 1
2016-09-06T03:58:02.640490: step 18158, loss 0.0319587, acc 0.98
2016-09-06T03:58:03.461575: step 18159, loss 0.00156695, acc 1
2016-09-06T03:58:04.297761: step 18160, loss 0.02957, acc 0.98
2016-09-06T03:58:05.128123: step 18161, loss 0.0143872, acc 1
2016-09-06T03:58:05.933186: step 18162, loss 0.0023586, acc 1
2016-09-06T03:58:06.804573: step 18163, loss 0.00953563, acc 1
2016-09-06T03:58:07.613450: step 18164, loss 0.00188854, acc 1
2016-09-06T03:58:08.461591: step 18165, loss 0.00652511, acc 1
2016-09-06T03:58:09.364677: step 18166, loss 0.00163163, acc 1
2016-09-06T03:58:10.175732: step 18167, loss 0.0052387, acc 1
2016-09-06T03:58:10.950129: step 18168, loss 0.00185456, acc 1
2016-09-06T03:58:11.786870: step 18169, loss 0.0016906, acc 1
2016-09-06T03:58:12.626197: step 18170, loss 0.00188663, acc 1
2016-09-06T03:58:13.423363: step 18171, loss 0.00170852, acc 1
2016-09-06T03:58:14.233140: step 18172, loss 0.015819, acc 0.98
2016-09-06T03:58:15.047666: step 18173, loss 0.0175995, acc 1
2016-09-06T03:58:15.844584: step 18174, loss 0.00169704, acc 1
2016-09-06T03:58:16.657679: step 18175, loss 0.0200181, acc 1
2016-09-06T03:58:17.477394: step 18176, loss 0.0185408, acc 1
2016-09-06T03:58:18.297494: step 18177, loss 0.00209814, acc 1
2016-09-06T03:58:19.106209: step 18178, loss 0.00497658, acc 1
2016-09-06T03:58:19.913012: step 18179, loss 0.057657, acc 0.96
2016-09-06T03:58:20.686515: step 18180, loss 0.00243138, acc 1
2016-09-06T03:58:21.517257: step 18181, loss 0.0208098, acc 0.98
2016-09-06T03:58:22.359736: step 18182, loss 0.0279155, acc 1
2016-09-06T03:58:23.163136: step 18183, loss 0.0190617, acc 1
2016-09-06T03:58:23.970595: step 18184, loss 0.0377153, acc 0.98
2016-09-06T03:58:24.795920: step 18185, loss 0.00174606, acc 1
2016-09-06T03:58:25.620674: step 18186, loss 0.00636166, acc 1
2016-09-06T03:58:26.444345: step 18187, loss 0.00180602, acc 1
2016-09-06T03:58:27.265598: step 18188, loss 0.00160285, acc 1
2016-09-06T03:58:28.077498: step 18189, loss 0.0726689, acc 0.98
2016-09-06T03:58:28.871569: step 18190, loss 0.0375977, acc 0.98
2016-09-06T03:58:29.683598: step 18191, loss 0.00258629, acc 1
2016-09-06T03:58:30.469473: step 18192, loss 0.0388565, acc 0.98
2016-09-06T03:58:31.278951: step 18193, loss 0.00428913, acc 1
2016-09-06T03:58:32.251500: step 18194, loss 0.0234502, acc 1
2016-09-06T03:58:33.082137: step 18195, loss 0.0224063, acc 1
2016-09-06T03:58:33.921441: step 18196, loss 0.00222163, acc 1
2016-09-06T03:58:34.756177: step 18197, loss 0.0326541, acc 0.98
2016-09-06T03:58:35.579181: step 18198, loss 0.0137676, acc 1
2016-09-06T03:58:36.505653: step 18199, loss 0.00506514, acc 1
2016-09-06T03:58:37.370548: step 18200, loss 0.00192521, acc 1

Evaluation:
2016-09-06T03:58:41.127054: step 18200, loss 3.27037, acc 0.716698

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-18200

2016-09-06T03:58:43.088146: step 18201, loss 0.00460924, acc 1
2016-09-06T03:58:43.927608: step 18202, loss 0.00728678, acc 1
2016-09-06T03:58:44.720057: step 18203, loss 0.00213177, acc 1
2016-09-06T03:58:45.559279: step 18204, loss 0.00271017, acc 1
2016-09-06T03:58:46.370881: step 18205, loss 0.00217185, acc 1
2016-09-06T03:58:47.257982: step 18206, loss 0.0190539, acc 1
2016-09-06T03:58:48.149944: step 18207, loss 0.00320693, acc 1
2016-09-06T03:58:48.963879: step 18208, loss 0.00595765, acc 1
2016-09-06T03:58:49.733610: step 18209, loss 0.0468495, acc 0.96
2016-09-06T03:58:50.516050: step 18210, loss 0.00885634, acc 1
2016-09-06T03:58:51.344572: step 18211, loss 0.00343852, acc 1
2016-09-06T03:58:52.128469: step 18212, loss 0.00236391, acc 1
2016-09-06T03:58:52.927673: step 18213, loss 0.00274391, acc 1
2016-09-06T03:58:53.757413: step 18214, loss 0.0332982, acc 0.96
2016-09-06T03:58:54.549569: step 18215, loss 0.00372404, acc 1
2016-09-06T03:58:55.364470: step 18216, loss 0.0104376, acc 1
2016-09-06T03:58:56.159855: step 18217, loss 0.0319689, acc 0.98
2016-09-06T03:58:56.956096: step 18218, loss 0.00388662, acc 1
2016-09-06T03:58:57.791663: step 18219, loss 0.00292518, acc 1
2016-09-06T03:58:58.617334: step 18220, loss 0.00399547, acc 1
2016-09-06T03:58:59.396129: step 18221, loss 0.00276762, acc 1
2016-09-06T03:59:00.190160: step 18222, loss 0.0345071, acc 0.98
2016-09-06T03:59:01.038811: step 18223, loss 0.00504846, acc 1
2016-09-06T03:59:01.827951: step 18224, loss 0.00231038, acc 1
2016-09-06T03:59:02.600665: step 18225, loss 0.0289856, acc 0.98
2016-09-06T03:59:03.457919: step 18226, loss 0.018916, acc 0.98
2016-09-06T03:59:04.223767: step 18227, loss 0.0338986, acc 0.98
2016-09-06T03:59:05.014758: step 18228, loss 0.00906129, acc 1
2016-09-06T03:59:05.867166: step 18229, loss 0.00241112, acc 1
2016-09-06T03:59:06.684030: step 18230, loss 0.0028975, acc 1
2016-09-06T03:59:07.489204: step 18231, loss 0.00224222, acc 1
2016-09-06T03:59:08.296426: step 18232, loss 0.00219395, acc 1
2016-09-06T03:59:09.091603: step 18233, loss 0.00466065, acc 1
2016-09-06T03:59:09.896547: step 18234, loss 0.0260086, acc 0.98
2016-09-06T03:59:10.711481: step 18235, loss 0.00726201, acc 1
2016-09-06T03:59:11.511179: step 18236, loss 0.138713, acc 0.98
2016-09-06T03:59:12.321423: step 18237, loss 0.0102811, acc 1
2016-09-06T03:59:13.121355: step 18238, loss 0.0678558, acc 0.96
2016-09-06T03:59:13.912027: step 18239, loss 0.00468633, acc 1
2016-09-06T03:59:14.641303: step 18240, loss 0.00200032, acc 1
2016-09-06T03:59:15.468812: step 18241, loss 0.00193992, acc 1
2016-09-06T03:59:16.282965: step 18242, loss 0.0362314, acc 0.98
2016-09-06T03:59:17.143376: step 18243, loss 0.00522117, acc 1
2016-09-06T03:59:18.005645: step 18244, loss 0.0197285, acc 0.98
2016-09-06T03:59:18.836045: step 18245, loss 0.00930125, acc 1
2016-09-06T03:59:19.654665: step 18246, loss 0.0183386, acc 0.98
2016-09-06T03:59:20.506727: step 18247, loss 0.00351441, acc 1
2016-09-06T03:59:21.345419: step 18248, loss 0.00627422, acc 1
2016-09-06T03:59:22.162805: step 18249, loss 0.0494062, acc 0.98
2016-09-06T03:59:22.974209: step 18250, loss 0.00209517, acc 1
2016-09-06T03:59:23.778598: step 18251, loss 0.00412584, acc 1
2016-09-06T03:59:24.578287: step 18252, loss 0.0167785, acc 0.98
2016-09-06T03:59:25.417115: step 18253, loss 0.00220161, acc 1
2016-09-06T03:59:26.236509: step 18254, loss 0.00331679, acc 1
2016-09-06T03:59:27.031782: step 18255, loss 0.00603176, acc 1
2016-09-06T03:59:27.861955: step 18256, loss 0.0211093, acc 0.98
2016-09-06T03:59:28.686259: step 18257, loss 0.00578319, acc 1
2016-09-06T03:59:29.479634: step 18258, loss 0.0023318, acc 1
2016-09-06T03:59:30.316881: step 18259, loss 0.00366396, acc 1
2016-09-06T03:59:31.110555: step 18260, loss 0.00294485, acc 1
2016-09-06T03:59:31.937607: step 18261, loss 0.00237699, acc 1
2016-09-06T03:59:32.784089: step 18262, loss 0.00246565, acc 1
2016-09-06T03:59:33.651561: step 18263, loss 0.00606136, acc 1
2016-09-06T03:59:34.444322: step 18264, loss 0.0353612, acc 0.98
2016-09-06T03:59:35.226710: step 18265, loss 0.0105485, acc 1
2016-09-06T03:59:36.027459: step 18266, loss 0.00754599, acc 1
2016-09-06T03:59:36.818561: step 18267, loss 0.0128764, acc 1
2016-09-06T03:59:37.608033: step 18268, loss 0.00465898, acc 1
2016-09-06T03:59:38.489401: step 18269, loss 0.0142115, acc 1
2016-09-06T03:59:39.355096: step 18270, loss 0.00258868, acc 1
2016-09-06T03:59:40.169402: step 18271, loss 0.00261862, acc 1
2016-09-06T03:59:41.079825: step 18272, loss 0.00575094, acc 1
2016-09-06T03:59:41.904929: step 18273, loss 0.00326535, acc 1
2016-09-06T03:59:42.734535: step 18274, loss 0.0117354, acc 1
2016-09-06T03:59:43.564024: step 18275, loss 0.00282389, acc 1
2016-09-06T03:59:44.371209: step 18276, loss 0.0222325, acc 0.98
2016-09-06T03:59:45.316677: step 18277, loss 0.00271482, acc 1
2016-09-06T03:59:46.135348: step 18278, loss 0.0165961, acc 0.98
2016-09-06T03:59:46.965068: step 18279, loss 0.0360964, acc 0.98
2016-09-06T03:59:47.844554: step 18280, loss 0.00266894, acc 1
2016-09-06T03:59:48.836839: step 18281, loss 0.00512204, acc 1
2016-09-06T03:59:49.676835: step 18282, loss 0.0369903, acc 0.98
2016-09-06T03:59:50.557060: step 18283, loss 0.0403702, acc 0.96
2016-09-06T03:59:51.351238: step 18284, loss 0.00268746, acc 1
2016-09-06T03:59:52.184617: step 18285, loss 0.00304597, acc 1
2016-09-06T03:59:53.018933: step 18286, loss 0.00257221, acc 1
2016-09-06T03:59:53.845572: step 18287, loss 0.0299373, acc 0.98
2016-09-06T03:59:54.762884: step 18288, loss 0.0332873, acc 0.98
2016-09-06T03:59:55.645714: step 18289, loss 0.0504596, acc 0.96
2016-09-06T03:59:56.481550: step 18290, loss 0.00230987, acc 1
2016-09-06T03:59:57.321435: step 18291, loss 0.0149916, acc 1
2016-09-06T03:59:58.148942: step 18292, loss 0.00664088, acc 1
2016-09-06T03:59:59.049880: step 18293, loss 0.00973181, acc 1
2016-09-06T03:59:59.858189: step 18294, loss 0.00852968, acc 1
2016-09-06T04:00:00.713598: step 18295, loss 0.00488339, acc 1
2016-09-06T04:00:01.543917: step 18296, loss 0.00218539, acc 1
2016-09-06T04:00:02.372864: step 18297, loss 0.00257563, acc 1
2016-09-06T04:00:03.190057: step 18298, loss 0.0202189, acc 0.98
2016-09-06T04:00:04.021538: step 18299, loss 0.00280476, acc 1
2016-09-06T04:00:04.915466: step 18300, loss 0.00251866, acc 1

Evaluation:
2016-09-06T04:00:08.652108: step 18300, loss 3.22867, acc 0.712008

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-18300

2016-09-06T04:00:10.555817: step 18301, loss 0.00737134, acc 1
2016-09-06T04:00:11.414083: step 18302, loss 0.00235658, acc 1
2016-09-06T04:00:12.218961: step 18303, loss 0.00211993, acc 1
2016-09-06T04:00:13.025328: step 18304, loss 0.00548913, acc 1
2016-09-06T04:00:13.845814: step 18305, loss 0.0172346, acc 1
2016-09-06T04:00:14.632761: step 18306, loss 0.00476108, acc 1
2016-09-06T04:00:15.437618: step 18307, loss 0.0229409, acc 1
2016-09-06T04:00:16.273681: step 18308, loss 0.0121413, acc 1
2016-09-06T04:00:17.097016: step 18309, loss 0.00879395, acc 1
2016-09-06T04:00:17.874293: step 18310, loss 0.00592717, acc 1
2016-09-06T04:00:18.697375: step 18311, loss 0.00239349, acc 1
2016-09-06T04:00:19.516267: step 18312, loss 0.00216639, acc 1
2016-09-06T04:00:20.318544: step 18313, loss 0.0179751, acc 1
2016-09-06T04:00:21.117271: step 18314, loss 0.00218871, acc 1
2016-09-06T04:00:21.908753: step 18315, loss 0.0174501, acc 0.98
2016-09-06T04:00:22.753080: step 18316, loss 0.00366178, acc 1
2016-09-06T04:00:23.557784: step 18317, loss 0.00216353, acc 1
2016-09-06T04:00:24.418038: step 18318, loss 0.0023724, acc 1
2016-09-06T04:00:25.250955: step 18319, loss 0.0126039, acc 1
2016-09-06T04:00:26.063935: step 18320, loss 0.0345597, acc 0.98
2016-09-06T04:00:26.921086: step 18321, loss 0.00292119, acc 1
2016-09-06T04:00:27.745589: step 18322, loss 0.012155, acc 1
2016-09-06T04:00:28.545590: step 18323, loss 0.00218617, acc 1
2016-09-06T04:00:29.361446: step 18324, loss 0.00210343, acc 1
2016-09-06T04:00:30.137564: step 18325, loss 0.0327942, acc 0.96
2016-09-06T04:00:30.949369: step 18326, loss 0.0379611, acc 0.96
2016-09-06T04:00:31.787590: step 18327, loss 0.00263619, acc 1
2016-09-06T04:00:32.611306: step 18328, loss 0.0695673, acc 0.96
2016-09-06T04:00:33.428544: step 18329, loss 0.0290007, acc 0.98
2016-09-06T04:00:34.253907: step 18330, loss 0.00200714, acc 1
2016-09-06T04:00:35.075335: step 18331, loss 0.0164298, acc 1
2016-09-06T04:00:35.890140: step 18332, loss 0.00230464, acc 1
2016-09-06T04:00:36.728001: step 18333, loss 0.0134909, acc 1
2016-09-06T04:00:37.549190: step 18334, loss 0.00215624, acc 1
2016-09-06T04:00:38.371554: step 18335, loss 0.00246563, acc 1
2016-09-06T04:00:39.220780: step 18336, loss 0.0104248, acc 1
2016-09-06T04:00:40.018122: step 18337, loss 0.00212147, acc 1
2016-09-06T04:00:40.870885: step 18338, loss 0.00554107, acc 1
2016-09-06T04:00:41.695293: step 18339, loss 0.0018893, acc 1
2016-09-06T04:00:42.520737: step 18340, loss 0.00356362, acc 1
2016-09-06T04:00:43.328502: step 18341, loss 0.0114049, acc 1
2016-09-06T04:00:44.129784: step 18342, loss 0.00587519, acc 1
2016-09-06T04:00:44.959356: step 18343, loss 0.00178148, acc 1
2016-09-06T04:00:45.772514: step 18344, loss 0.00177755, acc 1
2016-09-06T04:00:46.566488: step 18345, loss 0.0240182, acc 1
2016-09-06T04:00:47.383111: step 18346, loss 0.00183225, acc 1
2016-09-06T04:00:48.147783: step 18347, loss 0.0252337, acc 1
2016-09-06T04:00:48.944462: step 18348, loss 0.00232069, acc 1
2016-09-06T04:00:49.797523: step 18349, loss 0.0362942, acc 1
2016-09-06T04:00:50.573917: step 18350, loss 0.0031806, acc 1
2016-09-06T04:00:51.393424: step 18351, loss 0.00184902, acc 1
2016-09-06T04:00:52.211045: step 18352, loss 0.0292315, acc 0.98
2016-09-06T04:00:52.984340: step 18353, loss 0.0150966, acc 1
2016-09-06T04:00:53.777885: step 18354, loss 0.00258376, acc 1
2016-09-06T04:00:54.568218: step 18355, loss 0.0402716, acc 0.98
2016-09-06T04:00:55.383990: step 18356, loss 0.00194715, acc 1
2016-09-06T04:00:56.211906: step 18357, loss 0.00197527, acc 1
2016-09-06T04:00:57.010353: step 18358, loss 0.00194852, acc 1
2016-09-06T04:00:57.817484: step 18359, loss 0.0021999, acc 1
2016-09-06T04:00:58.627717: step 18360, loss 0.00197818, acc 1
2016-09-06T04:00:59.447652: step 18361, loss 0.00200442, acc 1
2016-09-06T04:01:00.247112: step 18362, loss 0.0120816, acc 1
2016-09-06T04:01:01.046994: step 18363, loss 0.00199021, acc 1
2016-09-06T04:01:01.889193: step 18364, loss 0.00201397, acc 1
2016-09-06T04:01:02.692578: step 18365, loss 0.00366507, acc 1
2016-09-06T04:01:03.503436: step 18366, loss 0.00657227, acc 1
2016-09-06T04:01:04.315867: step 18367, loss 0.0730776, acc 0.96
2016-09-06T04:01:05.127399: step 18368, loss 0.00240816, acc 1
2016-09-06T04:01:05.927962: step 18369, loss 0.00840414, acc 1
2016-09-06T04:01:06.773635: step 18370, loss 0.0235053, acc 0.98
2016-09-06T04:01:07.613857: step 18371, loss 0.0192667, acc 1
2016-09-06T04:01:08.405406: step 18372, loss 0.0138747, acc 1
2016-09-06T04:01:09.256676: step 18373, loss 0.00290062, acc 1
2016-09-06T04:01:10.081764: step 18374, loss 0.0313588, acc 0.98
2016-09-06T04:01:10.887059: step 18375, loss 0.0292871, acc 0.98
2016-09-06T04:01:11.722532: step 18376, loss 0.00230879, acc 1
2016-09-06T04:01:12.551163: step 18377, loss 0.0135283, acc 1
2016-09-06T04:01:13.370683: step 18378, loss 0.0261302, acc 0.98
2016-09-06T04:01:14.191468: step 18379, loss 0.00192725, acc 1
2016-09-06T04:01:15.033781: step 18380, loss 0.0025099, acc 1
2016-09-06T04:01:15.908936: step 18381, loss 0.00521824, acc 1
2016-09-06T04:01:16.728821: step 18382, loss 0.00197726, acc 1
2016-09-06T04:01:17.698156: step 18383, loss 0.00199382, acc 1
2016-09-06T04:01:18.527927: step 18384, loss 0.0226231, acc 1
2016-09-06T04:01:19.420611: step 18385, loss 0.00284467, acc 1
2016-09-06T04:01:20.277130: step 18386, loss 0.00574517, acc 1
2016-09-06T04:01:21.129956: step 18387, loss 0.00213087, acc 1
2016-09-06T04:01:22.095676: step 18388, loss 0.0241757, acc 0.98
2016-09-06T04:01:22.913439: step 18389, loss 0.0892787, acc 0.96
2016-09-06T04:01:23.760143: step 18390, loss 0.155685, acc 0.94
2016-09-06T04:01:24.581767: step 18391, loss 0.00191043, acc 1
2016-09-06T04:01:25.408292: step 18392, loss 0.00348288, acc 1
2016-09-06T04:01:26.246114: step 18393, loss 0.00195851, acc 1
2016-09-06T04:01:27.079655: step 18394, loss 0.00172324, acc 1
2016-09-06T04:01:27.896156: step 18395, loss 0.00274536, acc 1
2016-09-06T04:01:28.716110: step 18396, loss 0.0156891, acc 1
2016-09-06T04:01:29.518225: step 18397, loss 0.00356655, acc 1
2016-09-06T04:01:30.333840: step 18398, loss 0.00245612, acc 1
2016-09-06T04:01:31.146598: step 18399, loss 0.00197222, acc 1
2016-09-06T04:01:31.989247: step 18400, loss 0.0656057, acc 0.96

Evaluation:
2016-09-06T04:01:35.709980: step 18400, loss 2.82583, acc 0.703565

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-18400

2016-09-06T04:01:37.581050: step 18401, loss 0.0357462, acc 0.98
2016-09-06T04:01:38.406944: step 18402, loss 0.00183075, acc 1
2016-09-06T04:01:39.220281: step 18403, loss 0.0254693, acc 0.98
2016-09-06T04:01:40.022271: step 18404, loss 0.00451774, acc 1
2016-09-06T04:01:40.839065: step 18405, loss 0.00192959, acc 1
2016-09-06T04:01:41.645451: step 18406, loss 0.0654948, acc 0.96
2016-09-06T04:01:42.452664: step 18407, loss 0.00474771, acc 1
2016-09-06T04:01:43.285157: step 18408, loss 0.0133578, acc 1
2016-09-06T04:01:44.140304: step 18409, loss 0.0425159, acc 0.98
2016-09-06T04:01:45.021697: step 18410, loss 0.0334139, acc 0.98
2016-09-06T04:01:45.827882: step 18411, loss 0.0112853, acc 1
2016-09-06T04:01:46.646143: step 18412, loss 0.00227773, acc 1
2016-09-06T04:01:47.441491: step 18413, loss 0.0028873, acc 1
2016-09-06T04:01:48.248007: step 18414, loss 0.0102862, acc 1
2016-09-06T04:01:49.060329: step 18415, loss 0.0406, acc 0.96
2016-09-06T04:01:49.853846: step 18416, loss 0.00244876, acc 1
2016-09-06T04:01:50.638844: step 18417, loss 0.0138436, acc 1
2016-09-06T04:01:51.466706: step 18418, loss 0.0360916, acc 0.98
2016-09-06T04:01:52.243883: step 18419, loss 0.0176484, acc 0.98
2016-09-06T04:01:53.062296: step 18420, loss 0.0207458, acc 1
2016-09-06T04:01:53.889362: step 18421, loss 0.00281514, acc 1
2016-09-06T04:01:54.692764: step 18422, loss 0.00549036, acc 1
2016-09-06T04:01:55.515788: step 18423, loss 0.00313578, acc 1
2016-09-06T04:01:56.334397: step 18424, loss 0.00266033, acc 1
2016-09-06T04:01:57.143385: step 18425, loss 0.00259093, acc 1
2016-09-06T04:01:57.948988: step 18426, loss 0.0275619, acc 0.98
2016-09-06T04:01:58.797454: step 18427, loss 0.0181997, acc 1
2016-09-06T04:01:59.634542: step 18428, loss 0.00264898, acc 1
2016-09-06T04:02:00.513745: step 18429, loss 0.0284647, acc 0.98
2016-09-06T04:02:01.364975: step 18430, loss 0.00719308, acc 1
2016-09-06T04:02:02.169030: step 18431, loss 0.0171163, acc 0.98
2016-09-06T04:02:02.914206: step 18432, loss 0.00291992, acc 1
2016-09-06T04:02:03.776063: step 18433, loss 0.00262085, acc 1
2016-09-06T04:02:04.631623: step 18434, loss 0.029421, acc 0.98
2016-09-06T04:02:05.421111: step 18435, loss 0.00347847, acc 1
2016-09-06T04:02:06.367727: step 18436, loss 0.00511577, acc 1
2016-09-06T04:02:07.253745: step 18437, loss 0.011125, acc 1
2016-09-06T04:02:08.090534: step 18438, loss 0.00442672, acc 1
2016-09-06T04:02:08.954743: step 18439, loss 0.00807578, acc 1
2016-09-06T04:02:09.920350: step 18440, loss 0.00289834, acc 1
2016-09-06T04:02:10.736346: step 18441, loss 0.0125613, acc 1
2016-09-06T04:02:11.718462: step 18442, loss 0.158253, acc 0.96
2016-09-06T04:02:12.585676: step 18443, loss 0.00388808, acc 1
2016-09-06T04:02:13.430732: step 18444, loss 0.00293988, acc 1
2016-09-06T04:02:14.262879: step 18445, loss 0.00287104, acc 1
2016-09-06T04:02:15.070462: step 18446, loss 0.00297068, acc 1
2016-09-06T04:02:15.913459: step 18447, loss 0.00313224, acc 1
2016-09-06T04:02:16.741224: step 18448, loss 0.0490219, acc 0.98
2016-09-06T04:02:17.617297: step 18449, loss 0.00957795, acc 1
2016-09-06T04:02:18.403981: step 18450, loss 0.00363291, acc 1
2016-09-06T04:02:19.280287: step 18451, loss 0.00378988, acc 1
2016-09-06T04:02:20.241487: step 18452, loss 0.00411141, acc 1
2016-09-06T04:02:21.270663: step 18453, loss 0.00408315, acc 1
2016-09-06T04:02:22.171838: step 18454, loss 0.0123894, acc 1
2016-09-06T04:02:23.071373: step 18455, loss 0.0103703, acc 1
2016-09-06T04:02:24.154483: step 18456, loss 0.00513699, acc 1
2016-09-06T04:02:25.162358: step 18457, loss 0.00597624, acc 1
2016-09-06T04:02:26.185380: step 18458, loss 0.01758, acc 1
2016-09-06T04:02:27.230277: step 18459, loss 0.010822, acc 1
2016-09-06T04:02:28.421074: step 18460, loss 0.0132771, acc 1
2016-09-06T04:02:29.385301: step 18461, loss 0.0088591, acc 1
2016-09-06T04:02:30.440858: step 18462, loss 0.0150137, acc 1
2016-09-06T04:02:31.432124: step 18463, loss 0.0132136, acc 1
2016-09-06T04:02:32.403818: step 18464, loss 0.00698709, acc 1
2016-09-06T04:02:33.413110: step 18465, loss 0.050422, acc 0.98
2016-09-06T04:02:34.580054: step 18466, loss 0.0289638, acc 0.98
2016-09-06T04:02:35.704531: step 18467, loss 0.00506958, acc 1
2016-09-06T04:02:36.888464: step 18468, loss 0.00620565, acc 1
2016-09-06T04:02:38.146140: step 18469, loss 0.00524056, acc 1
2016-09-06T04:02:39.172595: step 18470, loss 0.0231596, acc 1
2016-09-06T04:02:40.060740: step 18471, loss 0.00514846, acc 1
2016-09-06T04:02:41.010815: step 18472, loss 0.00509884, acc 1
2016-09-06T04:02:42.165272: step 18473, loss 0.0072046, acc 1
2016-09-06T04:02:43.209553: step 18474, loss 0.00510341, acc 1
2016-09-06T04:02:44.208196: step 18475, loss 0.0254556, acc 1
2016-09-06T04:02:45.112520: step 18476, loss 0.010693, acc 1
2016-09-06T04:02:46.183044: step 18477, loss 0.00506963, acc 1
2016-09-06T04:02:47.186067: step 18478, loss 0.0843095, acc 0.98
2016-09-06T04:02:48.230738: step 18479, loss 0.00519637, acc 1
2016-09-06T04:02:49.128196: step 18480, loss 0.005204, acc 1
2016-09-06T04:02:50.087891: step 18481, loss 0.00496372, acc 1
2016-09-06T04:02:51.052031: step 18482, loss 0.0217001, acc 0.98
2016-09-06T04:02:52.082475: step 18483, loss 0.0225346, acc 0.98
2016-09-06T04:02:53.049642: step 18484, loss 0.00472875, acc 1
2016-09-06T04:02:53.867779: step 18485, loss 0.0323154, acc 0.98
2016-09-06T04:02:54.830534: step 18486, loss 0.0914869, acc 0.98
2016-09-06T04:02:55.848294: step 18487, loss 0.0048686, acc 1
2016-09-06T04:02:56.697758: step 18488, loss 0.00480672, acc 1
2016-09-06T04:02:57.491412: step 18489, loss 0.0044056, acc 1
2016-09-06T04:02:58.467442: step 18490, loss 0.0137738, acc 1
2016-09-06T04:02:59.578776: step 18491, loss 0.00428717, acc 1
2016-09-06T04:03:00.429694: step 18492, loss 0.00422346, acc 1
2016-09-06T04:03:01.214631: step 18493, loss 0.35938, acc 0.96
2016-09-06T04:03:02.092441: step 18494, loss 0.00426129, acc 1
2016-09-06T04:03:02.962065: step 18495, loss 0.00391244, acc 1
2016-09-06T04:03:03.796401: step 18496, loss 0.016306, acc 1
2016-09-06T04:03:04.607367: step 18497, loss 0.00380047, acc 1
2016-09-06T04:03:05.422683: step 18498, loss 0.00510076, acc 1
2016-09-06T04:03:06.282073: step 18499, loss 0.00573659, acc 1
2016-09-06T04:03:07.106078: step 18500, loss 0.00875975, acc 1

Evaluation:
2016-09-06T04:03:10.826473: step 18500, loss 3.20575, acc 0.690432

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-18500

2016-09-06T04:03:12.777201: step 18501, loss 0.131076, acc 0.94
2016-09-06T04:03:13.556222: step 18502, loss 0.0178118, acc 1
2016-09-06T04:03:14.366559: step 18503, loss 0.00441138, acc 1
2016-09-06T04:03:15.206569: step 18504, loss 0.0279826, acc 0.98
2016-09-06T04:03:16.036602: step 18505, loss 0.0375057, acc 0.98
2016-09-06T04:03:16.857328: step 18506, loss 0.00659794, acc 1
2016-09-06T04:03:17.684440: step 18507, loss 0.0132797, acc 1
2016-09-06T04:03:18.524933: step 18508, loss 0.02082, acc 1
2016-09-06T04:03:19.303209: step 18509, loss 0.00342854, acc 1
2016-09-06T04:03:20.118221: step 18510, loss 0.00360526, acc 1
2016-09-06T04:03:20.960436: step 18511, loss 0.00377443, acc 1
2016-09-06T04:03:21.769736: step 18512, loss 0.0493074, acc 0.96
2016-09-06T04:03:22.570222: step 18513, loss 0.00695978, acc 1
2016-09-06T04:03:23.402392: step 18514, loss 0.00526296, acc 1
2016-09-06T04:03:24.223736: step 18515, loss 0.0434056, acc 0.98
2016-09-06T04:03:25.022469: step 18516, loss 0.0150287, acc 1
2016-09-06T04:03:25.871604: step 18517, loss 0.00433162, acc 1
2016-09-06T04:03:26.678148: step 18518, loss 0.0362665, acc 0.98
2016-09-06T04:03:27.449720: step 18519, loss 0.00377022, acc 1
2016-09-06T04:03:28.285126: step 18520, loss 0.111039, acc 0.98
2016-09-06T04:03:29.093630: step 18521, loss 0.0129936, acc 1
2016-09-06T04:03:29.876877: step 18522, loss 0.00287861, acc 1
2016-09-06T04:03:30.718755: step 18523, loss 0.0716381, acc 0.98
2016-09-06T04:03:31.537532: step 18524, loss 0.00305728, acc 1
2016-09-06T04:03:32.377549: step 18525, loss 0.0159262, acc 1
2016-09-06T04:03:33.223100: step 18526, loss 0.00323925, acc 1
2016-09-06T04:03:34.036305: step 18527, loss 0.0118719, acc 1
2016-09-06T04:03:34.837556: step 18528, loss 0.00677157, acc 1
2016-09-06T04:03:35.668376: step 18529, loss 0.00406485, acc 1
2016-09-06T04:03:36.471045: step 18530, loss 0.0156287, acc 1
2016-09-06T04:03:37.273018: step 18531, loss 0.0146045, acc 1
2016-09-06T04:03:38.091374: step 18532, loss 0.0122973, acc 1
2016-09-06T04:03:38.895887: step 18533, loss 0.00980333, acc 1
2016-09-06T04:03:39.713675: step 18534, loss 0.00343141, acc 1
2016-09-06T04:03:40.554041: step 18535, loss 0.00366627, acc 1
2016-09-06T04:03:41.377486: step 18536, loss 0.00596452, acc 1
2016-09-06T04:03:42.196559: step 18537, loss 0.00434387, acc 1
2016-09-06T04:03:43.036657: step 18538, loss 0.00377201, acc 1
2016-09-06T04:03:43.881253: step 18539, loss 0.0268133, acc 0.98
2016-09-06T04:03:44.714392: step 18540, loss 0.0455778, acc 0.96
2016-09-06T04:03:45.547260: step 18541, loss 0.0602078, acc 0.96
2016-09-06T04:03:46.515876: step 18542, loss 0.00672545, acc 1
2016-09-06T04:03:47.334826: step 18543, loss 0.00351522, acc 1
2016-09-06T04:03:48.351185: step 18544, loss 0.050715, acc 0.98
2016-09-06T04:03:49.166763: step 18545, loss 0.0130321, acc 1
2016-09-06T04:03:49.990337: step 18546, loss 0.00346178, acc 1
2016-09-06T04:03:50.777610: step 18547, loss 0.0553384, acc 0.98
2016-09-06T04:03:51.587113: step 18548, loss 0.00427243, acc 1
2016-09-06T04:03:52.426622: step 18549, loss 0.00512522, acc 1
2016-09-06T04:03:53.316885: step 18550, loss 0.00332166, acc 1
2016-09-06T04:03:54.147440: step 18551, loss 0.0105511, acc 1
2016-09-06T04:03:54.993942: step 18552, loss 0.00593703, acc 1
2016-09-06T04:03:55.819911: step 18553, loss 0.00325055, acc 1
2016-09-06T04:03:56.670595: step 18554, loss 0.00817386, acc 1
2016-09-06T04:03:57.516829: step 18555, loss 0.00332952, acc 1
2016-09-06T04:03:58.323265: step 18556, loss 0.00771843, acc 1
2016-09-06T04:03:59.124769: step 18557, loss 0.0392614, acc 0.98
2016-09-06T04:03:59.941974: step 18558, loss 0.0161204, acc 1
2016-09-06T04:04:00.802338: step 18559, loss 0.0115353, acc 1
2016-09-06T04:04:01.601845: step 18560, loss 0.0174654, acc 0.98
2016-09-06T04:04:02.392555: step 18561, loss 0.00416722, acc 1
2016-09-06T04:04:03.226513: step 18562, loss 0.0889464, acc 0.98
2016-09-06T04:04:04.024307: step 18563, loss 0.0229117, acc 1
2016-09-06T04:04:04.813764: step 18564, loss 0.00295822, acc 1
2016-09-06T04:04:05.618229: step 18565, loss 0.00822176, acc 1
2016-09-06T04:04:06.397602: step 18566, loss 0.049897, acc 0.98
2016-09-06T04:04:07.202713: step 18567, loss 0.0046012, acc 1
2016-09-06T04:04:08.039546: step 18568, loss 0.00377482, acc 1
2016-09-06T04:04:08.838310: step 18569, loss 0.0153965, acc 1
2016-09-06T04:04:09.653017: step 18570, loss 0.0221772, acc 1
2016-09-06T04:04:10.453669: step 18571, loss 0.00295022, acc 1
2016-09-06T04:04:11.255015: step 18572, loss 0.00287142, acc 1
2016-09-06T04:04:12.061636: step 18573, loss 0.0353912, acc 0.98
2016-09-06T04:04:12.913472: step 18574, loss 0.00464704, acc 1
2016-09-06T04:04:13.691984: step 18575, loss 0.0423473, acc 0.98
2016-09-06T04:04:14.522525: step 18576, loss 0.0186472, acc 0.98
2016-09-06T04:04:15.335292: step 18577, loss 0.0877009, acc 0.98
2016-09-06T04:04:16.126994: step 18578, loss 0.00697973, acc 1
2016-09-06T04:04:16.912748: step 18579, loss 0.0446114, acc 0.96
2016-09-06T04:04:17.751096: step 18580, loss 0.00654496, acc 1
2016-09-06T04:04:18.519430: step 18581, loss 0.00286362, acc 1
2016-09-06T04:04:19.301677: step 18582, loss 0.0312027, acc 0.98
2016-09-06T04:04:20.186143: step 18583, loss 0.00318234, acc 1
2016-09-06T04:04:20.964810: step 18584, loss 0.00299605, acc 1
2016-09-06T04:04:21.737088: step 18585, loss 0.0183328, acc 0.98
2016-09-06T04:04:22.555739: step 18586, loss 0.0183848, acc 1
2016-09-06T04:04:23.344052: step 18587, loss 0.00341156, acc 1
2016-09-06T04:04:24.138097: step 18588, loss 0.0279811, acc 1
2016-09-06T04:04:24.953351: step 18589, loss 0.0314305, acc 0.98
2016-09-06T04:04:25.747144: step 18590, loss 0.00532962, acc 1
2016-09-06T04:04:26.546176: step 18591, loss 0.00317518, acc 1
2016-09-06T04:04:27.363201: step 18592, loss 0.0297786, acc 0.98
2016-09-06T04:04:28.146418: step 18593, loss 0.0763265, acc 0.94
2016-09-06T04:04:28.948411: step 18594, loss 0.00417182, acc 1
2016-09-06T04:04:29.752599: step 18595, loss 0.011031, acc 1
2016-09-06T04:04:30.553270: step 18596, loss 0.0568497, acc 0.98
2016-09-06T04:04:31.378278: step 18597, loss 0.00400911, acc 1
2016-09-06T04:04:32.220133: step 18598, loss 0.0118501, acc 1
2016-09-06T04:04:33.041865: step 18599, loss 0.00305422, acc 1
2016-09-06T04:04:33.830475: step 18600, loss 0.00540023, acc 1

Evaluation:
2016-09-06T04:04:37.563617: step 18600, loss 3.00629, acc 0.699812

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-18600

2016-09-06T04:04:39.439486: step 18601, loss 0.0638598, acc 0.96
2016-09-06T04:04:40.258534: step 18602, loss 0.00702173, acc 1
2016-09-06T04:04:41.093867: step 18603, loss 0.0124586, acc 1
2016-09-06T04:04:41.930404: step 18604, loss 0.00332303, acc 1
2016-09-06T04:04:42.751282: step 18605, loss 0.0110595, acc 1
2016-09-06T04:04:43.587262: step 18606, loss 0.00667704, acc 1
2016-09-06T04:04:44.392406: step 18607, loss 0.0124339, acc 1
2016-09-06T04:04:45.237330: step 18608, loss 0.00304363, acc 1
2016-09-06T04:04:46.056415: step 18609, loss 0.0107084, acc 1
2016-09-06T04:04:46.924602: step 18610, loss 0.0242489, acc 0.98
2016-09-06T04:04:47.724575: step 18611, loss 0.010397, acc 1
2016-09-06T04:04:48.515808: step 18612, loss 0.01327, acc 1
2016-09-06T04:04:49.331623: step 18613, loss 0.0235921, acc 1
2016-09-06T04:04:50.124928: step 18614, loss 0.00418085, acc 1
2016-09-06T04:04:50.930695: step 18615, loss 0.00295372, acc 1
2016-09-06T04:04:51.762722: step 18616, loss 0.00679807, acc 1
2016-09-06T04:04:52.591415: step 18617, loss 0.00304949, acc 1
2016-09-06T04:04:53.410960: step 18618, loss 0.0179082, acc 0.98
2016-09-06T04:04:54.244617: step 18619, loss 0.00536364, acc 1
2016-09-06T04:04:55.041450: step 18620, loss 0.00287424, acc 1
2016-09-06T04:04:55.870652: step 18621, loss 0.0397353, acc 0.96
2016-09-06T04:04:56.752669: step 18622, loss 0.00286178, acc 1
2016-09-06T04:04:57.591683: step 18623, loss 0.00316841, acc 1
2016-09-06T04:04:58.334985: step 18624, loss 0.00280093, acc 1
2016-09-06T04:04:59.162414: step 18625, loss 0.0486628, acc 0.98
2016-09-06T04:04:59.991310: step 18626, loss 0.0155489, acc 1
2016-09-06T04:05:00.828611: step 18627, loss 0.00270175, acc 1
2016-09-06T04:05:01.655171: step 18628, loss 0.00884353, acc 1
2016-09-06T04:05:02.450577: step 18629, loss 0.00265888, acc 1
2016-09-06T04:05:03.257802: step 18630, loss 0.00267481, acc 1
2016-09-06T04:05:04.094238: step 18631, loss 0.00269587, acc 1
2016-09-06T04:05:04.895933: step 18632, loss 0.00262956, acc 1
2016-09-06T04:05:05.714272: step 18633, loss 0.0107312, acc 1
2016-09-06T04:05:06.561500: step 18634, loss 0.00256201, acc 1
2016-09-06T04:05:07.377506: step 18635, loss 0.0383296, acc 0.96
2016-09-06T04:05:08.197156: step 18636, loss 0.0168121, acc 1
2016-09-06T04:05:09.037374: step 18637, loss 0.0025134, acc 1
2016-09-06T04:05:09.840647: step 18638, loss 0.00247675, acc 1
2016-09-06T04:05:10.618450: step 18639, loss 0.017173, acc 1
2016-09-06T04:05:11.456179: step 18640, loss 0.00541483, acc 1
2016-09-06T04:05:12.279021: step 18641, loss 0.0252219, acc 0.98
2016-09-06T04:05:13.057445: step 18642, loss 0.216522, acc 0.98
2016-09-06T04:05:13.840537: step 18643, loss 0.0202565, acc 0.98
2016-09-06T04:05:14.662167: step 18644, loss 0.0124471, acc 1
2016-09-06T04:05:15.476668: step 18645, loss 0.0108883, acc 1
2016-09-06T04:05:16.295244: step 18646, loss 0.0171953, acc 1
2016-09-06T04:05:17.120746: step 18647, loss 0.0731394, acc 0.98
2016-09-06T04:05:18.016357: step 18648, loss 0.0215041, acc 1
2016-09-06T04:05:18.925820: step 18649, loss 0.0445421, acc 0.96
2016-09-06T04:05:19.830033: step 18650, loss 0.0210785, acc 0.98
2016-09-06T04:05:20.841877: step 18651, loss 0.0148346, acc 1
2016-09-06T04:05:21.949886: step 18652, loss 0.00506351, acc 1
2016-09-06T04:05:23.109368: step 18653, loss 0.0183322, acc 1
2016-09-06T04:05:24.311388: step 18654, loss 0.00973029, acc 1
2016-09-06T04:05:25.224619: step 18655, loss 0.00624994, acc 1
2016-09-06T04:05:26.340857: step 18656, loss 0.00605981, acc 1
2016-09-06T04:05:27.699876: step 18657, loss 0.00622627, acc 1
2016-09-06T04:05:29.185709: step 18658, loss 0.0145951, acc 1
2016-09-06T04:05:30.524414: step 18659, loss 0.00660957, acc 1
2016-09-06T04:05:31.598469: step 18660, loss 0.00857188, acc 1
2016-09-06T04:05:32.951134: step 18661, loss 0.00691083, acc 1
2016-09-06T04:05:34.283981: step 18662, loss 0.0278745, acc 1
2016-09-06T04:05:35.480231: step 18663, loss 0.101922, acc 0.98
2016-09-06T04:05:36.660533: step 18664, loss 0.00716173, acc 1
2016-09-06T04:05:37.850417: step 18665, loss 0.007287, acc 1
2016-09-06T04:05:39.227684: step 18666, loss 0.00720839, acc 1
2016-09-06T04:05:40.251268: step 18667, loss 0.0488159, acc 0.98
2016-09-06T04:05:41.613335: step 18668, loss 0.00787805, acc 1
2016-09-06T04:05:42.965678: step 18669, loss 0.009338, acc 1
2016-09-06T04:05:44.103642: step 18670, loss 0.0257371, acc 0.98
2016-09-06T04:05:44.991864: step 18671, loss 0.166281, acc 0.98
2016-09-06T04:05:46.041900: step 18672, loss 0.00698681, acc 1
2016-09-06T04:05:47.152450: step 18673, loss 0.00791122, acc 1
2016-09-06T04:05:48.357322: step 18674, loss 0.00677946, acc 1
2016-09-06T04:05:49.311937: step 18675, loss 0.00864504, acc 1
2016-09-06T04:05:50.383923: step 18676, loss 0.0403707, acc 0.98
2016-09-06T04:05:51.249926: step 18677, loss 0.0251112, acc 1
2016-09-06T04:05:52.357345: step 18678, loss 0.0401551, acc 0.98
2016-09-06T04:05:53.294063: step 18679, loss 0.00819433, acc 1
2016-09-06T04:05:54.169780: step 18680, loss 0.00623576, acc 1
2016-09-06T04:05:55.153409: step 18681, loss 0.00886957, acc 1
2016-09-06T04:05:56.223735: step 18682, loss 0.00934887, acc 1
2016-09-06T04:05:57.153675: step 18683, loss 0.0181843, acc 1
2016-09-06T04:05:58.044092: step 18684, loss 0.00592707, acc 1
2016-09-06T04:05:58.902817: step 18685, loss 0.0198979, acc 1
2016-09-06T04:05:59.739627: step 18686, loss 0.00579974, acc 1
2016-09-06T04:06:00.622036: step 18687, loss 0.00575182, acc 1
2016-09-06T04:06:01.422913: step 18688, loss 0.0395298, acc 0.98
2016-09-06T04:06:02.350120: step 18689, loss 0.116569, acc 0.98
2016-09-06T04:06:03.292563: step 18690, loss 0.00546433, acc 1
2016-09-06T04:06:04.147001: step 18691, loss 0.00541182, acc 1
2016-09-06T04:06:05.061123: step 18692, loss 0.0351605, acc 0.98
2016-09-06T04:06:05.939409: step 18693, loss 0.0713979, acc 0.96
2016-09-06T04:06:06.751022: step 18694, loss 0.0172685, acc 1
2016-09-06T04:06:07.522733: step 18695, loss 0.00868765, acc 1
2016-09-06T04:06:08.367839: step 18696, loss 0.00572208, acc 1
2016-09-06T04:06:09.253560: step 18697, loss 0.0676512, acc 0.98
2016-09-06T04:06:10.072204: step 18698, loss 0.0115891, acc 1
2016-09-06T04:06:10.994424: step 18699, loss 0.0197819, acc 0.98
2016-09-06T04:06:11.860390: step 18700, loss 0.0184507, acc 0.98

Evaluation:
2016-09-06T04:06:15.583261: step 18700, loss 3.6218, acc 0.699812

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-18700

2016-09-06T04:06:17.461812: step 18701, loss 0.0042709, acc 1
2016-09-06T04:06:18.257090: step 18702, loss 0.0233818, acc 0.98
2016-09-06T04:06:19.154403: step 18703, loss 0.0402179, acc 0.96
2016-09-06T04:06:20.085895: step 18704, loss 0.00923938, acc 1
2016-09-06T04:06:20.882581: step 18705, loss 0.00396439, acc 1
2016-09-06T04:06:21.698142: step 18706, loss 0.017502, acc 1
2016-09-06T04:06:22.530949: step 18707, loss 0.00784954, acc 1
2016-09-06T04:06:23.383522: step 18708, loss 0.00606237, acc 1
2016-09-06T04:06:24.205350: step 18709, loss 0.00608122, acc 1
2016-09-06T04:06:25.051997: step 18710, loss 0.0164347, acc 1
2016-09-06T04:06:25.906520: step 18711, loss 0.00373006, acc 1
2016-09-06T04:06:26.730677: step 18712, loss 0.0166293, acc 1
2016-09-06T04:06:27.569887: step 18713, loss 0.00785232, acc 1
2016-09-06T04:06:28.455571: step 18714, loss 0.00365649, acc 1
2016-09-06T04:06:29.264072: step 18715, loss 0.0180058, acc 0.98
2016-09-06T04:06:30.127213: step 18716, loss 0.0041727, acc 1
2016-09-06T04:06:30.973583: step 18717, loss 0.00704069, acc 1
2016-09-06T04:06:31.813615: step 18718, loss 0.00390778, acc 1
2016-09-06T04:06:32.653788: step 18719, loss 0.0049144, acc 1
2016-09-06T04:06:33.569844: step 18720, loss 0.00341892, acc 1
2016-09-06T04:06:34.443301: step 18721, loss 0.00344562, acc 1
2016-09-06T04:06:35.286310: step 18722, loss 0.0140405, acc 1
2016-09-06T04:06:36.108817: step 18723, loss 0.00374715, acc 1
2016-09-06T04:06:36.956101: step 18724, loss 0.0133344, acc 1
2016-09-06T04:06:37.761813: step 18725, loss 0.00874906, acc 1
2016-09-06T04:06:38.609627: step 18726, loss 0.00330173, acc 1
2016-09-06T04:06:39.517115: step 18727, loss 0.00780904, acc 1
2016-09-06T04:06:40.374775: step 18728, loss 0.0032591, acc 1
2016-09-06T04:06:41.243630: step 18729, loss 0.0156586, acc 1
2016-09-06T04:06:42.116852: step 18730, loss 0.0032954, acc 1
2016-09-06T04:06:42.915526: step 18731, loss 0.0230632, acc 0.98
2016-09-06T04:06:43.702815: step 18732, loss 0.0236125, acc 1
2016-09-06T04:06:44.532507: step 18733, loss 0.0249415, acc 0.98
2016-09-06T04:06:45.384912: step 18734, loss 0.0193804, acc 0.98
2016-09-06T04:06:46.164250: step 18735, loss 0.00357402, acc 1
2016-09-06T04:06:46.965533: step 18736, loss 0.024971, acc 0.98
2016-09-06T04:06:47.784456: step 18737, loss 0.0194619, acc 0.98
2016-09-06T04:06:48.572275: step 18738, loss 0.00307673, acc 1
2016-09-06T04:06:49.371910: step 18739, loss 0.0199006, acc 0.98
2016-09-06T04:06:50.189881: step 18740, loss 0.00304857, acc 1
2016-09-06T04:06:51.004972: step 18741, loss 0.00354194, acc 1
2016-09-06T04:06:51.881789: step 18742, loss 0.00309838, acc 1
2016-09-06T04:06:52.770239: step 18743, loss 0.00326595, acc 1
2016-09-06T04:06:53.595310: step 18744, loss 0.00305669, acc 1
2016-09-06T04:06:54.474201: step 18745, loss 0.00671681, acc 1
2016-09-06T04:06:55.326850: step 18746, loss 0.00293751, acc 1
2016-09-06T04:06:56.193826: step 18747, loss 0.00449821, acc 1
2016-09-06T04:06:57.041467: step 18748, loss 0.0106505, acc 1
2016-09-06T04:06:57.861850: step 18749, loss 0.0288248, acc 0.98
2016-09-06T04:06:58.703732: step 18750, loss 0.00284095, acc 1
2016-09-06T04:06:59.463849: step 18751, loss 0.0349796, acc 0.98
2016-09-06T04:07:00.260935: step 18752, loss 0.131778, acc 0.96
2016-09-06T04:07:01.075380: step 18753, loss 0.00713512, acc 1
2016-09-06T04:07:01.904112: step 18754, loss 0.0128683, acc 1
2016-09-06T04:07:02.713862: step 18755, loss 0.0401648, acc 0.96
2016-09-06T04:07:03.539586: step 18756, loss 0.00317303, acc 1
2016-09-06T04:07:04.376926: step 18757, loss 0.0147309, acc 1
2016-09-06T04:07:05.214794: step 18758, loss 0.0328699, acc 1
2016-09-06T04:07:06.088383: step 18759, loss 0.0181806, acc 0.98
2016-09-06T04:07:07.042277: step 18760, loss 0.0026072, acc 1
2016-09-06T04:07:07.998794: step 18761, loss 0.0156027, acc 1
2016-09-06T04:07:08.821735: step 18762, loss 0.00907222, acc 1
2016-09-06T04:07:09.764028: step 18763, loss 0.00268876, acc 1
2016-09-06T04:07:10.591681: step 18764, loss 0.00374837, acc 1
2016-09-06T04:07:11.417008: step 18765, loss 0.00803167, acc 1
2016-09-06T04:07:12.246569: step 18766, loss 0.00413575, acc 1
2016-09-06T04:07:13.040906: step 18767, loss 0.00263681, acc 1
2016-09-06T04:07:13.833877: step 18768, loss 0.0374945, acc 0.96
2016-09-06T04:07:14.640537: step 18769, loss 0.00386685, acc 1
2016-09-06T04:07:15.481266: step 18770, loss 0.00629184, acc 1
2016-09-06T04:07:16.291018: step 18771, loss 0.0102664, acc 1
2016-09-06T04:07:17.108613: step 18772, loss 0.0105935, acc 1
2016-09-06T04:07:17.921366: step 18773, loss 0.0289122, acc 0.98
2016-09-06T04:07:18.771618: step 18774, loss 0.0439245, acc 0.96
2016-09-06T04:07:19.597923: step 18775, loss 0.00250578, acc 1
2016-09-06T04:07:20.502749: step 18776, loss 0.00814044, acc 1
2016-09-06T04:07:21.333282: step 18777, loss 0.00281676, acc 1
2016-09-06T04:07:22.125878: step 18778, loss 0.00247924, acc 1
2016-09-06T04:07:22.973436: step 18779, loss 0.0419193, acc 0.98
2016-09-06T04:07:23.793596: step 18780, loss 0.076256, acc 0.96
2016-09-06T04:07:24.667345: step 18781, loss 0.00279649, acc 1
2016-09-06T04:07:25.510557: step 18782, loss 0.00236908, acc 1
2016-09-06T04:07:26.346622: step 18783, loss 0.0112825, acc 1
2016-09-06T04:07:27.148393: step 18784, loss 0.0210485, acc 1
2016-09-06T04:07:27.984540: step 18785, loss 0.0300185, acc 0.98
2016-09-06T04:07:28.810544: step 18786, loss 0.0182175, acc 1
2016-09-06T04:07:29.627740: step 18787, loss 0.003091, acc 1
2016-09-06T04:07:30.443945: step 18788, loss 0.00220576, acc 1
2016-09-06T04:07:31.254732: step 18789, loss 0.00277756, acc 1
2016-09-06T04:07:32.052590: step 18790, loss 0.0230581, acc 0.98
2016-09-06T04:07:32.876737: step 18791, loss 0.0027847, acc 1
2016-09-06T04:07:33.737323: step 18792, loss 0.0104966, acc 1
2016-09-06T04:07:34.505799: step 18793, loss 0.0174804, acc 1
2016-09-06T04:07:35.308102: step 18794, loss 0.0681118, acc 0.98
2016-09-06T04:07:36.132598: step 18795, loss 0.00218546, acc 1
2016-09-06T04:07:36.913065: step 18796, loss 0.00353448, acc 1
2016-09-06T04:07:37.740515: step 18797, loss 0.00267161, acc 1
2016-09-06T04:07:38.601033: step 18798, loss 0.0184555, acc 0.98
2016-09-06T04:07:39.417692: step 18799, loss 0.00306112, acc 1
2016-09-06T04:07:40.225760: step 18800, loss 0.00204173, acc 1

Evaluation:
2016-09-06T04:07:43.993121: step 18800, loss 3.27068, acc 0.70075

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-18800

2016-09-06T04:07:45.991883: step 18801, loss 0.0307788, acc 0.98
2016-09-06T04:07:46.805222: step 18802, loss 0.0398428, acc 0.96
2016-09-06T04:07:47.604182: step 18803, loss 0.0184521, acc 0.98
2016-09-06T04:07:48.449443: step 18804, loss 0.0862194, acc 0.98
2016-09-06T04:07:49.271470: step 18805, loss 0.0163458, acc 0.98
2016-09-06T04:07:50.094926: step 18806, loss 0.0309618, acc 0.98
2016-09-06T04:07:50.934550: step 18807, loss 0.00197294, acc 1
2016-09-06T04:07:51.736142: step 18808, loss 0.00372201, acc 1
2016-09-06T04:07:52.553693: step 18809, loss 0.0167345, acc 1
2016-09-06T04:07:53.393428: step 18810, loss 0.0142427, acc 1
2016-09-06T04:07:54.218125: step 18811, loss 0.0150889, acc 1
2016-09-06T04:07:55.049151: step 18812, loss 0.00178548, acc 1
2016-09-06T04:07:55.865181: step 18813, loss 0.128588, acc 0.98
2016-09-06T04:07:56.671971: step 18814, loss 0.00186922, acc 1
2016-09-06T04:07:57.487446: step 18815, loss 0.00589084, acc 1
2016-09-06T04:07:58.207453: step 18816, loss 0.00299569, acc 1
2016-09-06T04:07:59.007983: step 18817, loss 0.00185609, acc 1
2016-09-06T04:07:59.810453: step 18818, loss 0.0206325, acc 0.98
2016-09-06T04:08:00.650043: step 18819, loss 0.0392875, acc 0.96
2016-09-06T04:08:01.489451: step 18820, loss 0.0183479, acc 1
2016-09-06T04:08:02.281481: step 18821, loss 0.00155453, acc 1
2016-09-06T04:08:03.099888: step 18822, loss 0.00506024, acc 1
2016-09-06T04:08:03.935909: step 18823, loss 0.00401476, acc 1
2016-09-06T04:08:04.739497: step 18824, loss 0.0184066, acc 0.98
2016-09-06T04:08:05.588530: step 18825, loss 0.0104729, acc 1
2016-09-06T04:08:06.399992: step 18826, loss 0.00160346, acc 1
2016-09-06T04:08:07.175534: step 18827, loss 0.00906808, acc 1
2016-09-06T04:08:07.994549: step 18828, loss 0.0203878, acc 0.98
2016-09-06T04:08:08.821445: step 18829, loss 0.00743948, acc 1
2016-09-06T04:08:09.606197: step 18830, loss 0.0147846, acc 1
2016-09-06T04:08:10.410288: step 18831, loss 0.0025833, acc 1
2016-09-06T04:08:11.223470: step 18832, loss 0.0140113, acc 1
2016-09-06T04:08:12.009595: step 18833, loss 0.00565144, acc 1
2016-09-06T04:08:12.832744: step 18834, loss 0.00252831, acc 1
2016-09-06T04:08:13.642335: step 18835, loss 0.0124396, acc 1
2016-09-06T04:08:14.470918: step 18836, loss 0.0175791, acc 0.98
2016-09-06T04:08:15.259039: step 18837, loss 0.0032174, acc 1
2016-09-06T04:08:16.102145: step 18838, loss 0.0021513, acc 1
2016-09-06T04:08:16.898518: step 18839, loss 0.0209885, acc 1
2016-09-06T04:08:17.696587: step 18840, loss 0.0225074, acc 0.98
2016-09-06T04:08:18.517639: step 18841, loss 0.0198477, acc 0.98
2016-09-06T04:08:19.312658: step 18842, loss 0.0357526, acc 1
2016-09-06T04:08:20.123142: step 18843, loss 0.0205282, acc 0.98
2016-09-06T04:08:20.960431: step 18844, loss 0.0197902, acc 0.98
2016-09-06T04:08:21.761251: step 18845, loss 0.0287967, acc 0.98
2016-09-06T04:08:22.566135: step 18846, loss 0.00203526, acc 1
2016-09-06T04:08:23.385476: step 18847, loss 0.0245903, acc 0.98
2016-09-06T04:08:24.236442: step 18848, loss 0.0180415, acc 0.98
2016-09-06T04:08:25.046771: step 18849, loss 0.0236748, acc 0.98
2016-09-06T04:08:25.878655: step 18850, loss 0.018247, acc 0.98
2016-09-06T04:08:26.689765: step 18851, loss 0.00218029, acc 1
2016-09-06T04:08:27.514353: step 18852, loss 0.0321782, acc 0.98
2016-09-06T04:08:28.354858: step 18853, loss 0.00197418, acc 1
2016-09-06T04:08:29.211059: step 18854, loss 0.0226376, acc 0.98
2016-09-06T04:08:30.012273: step 18855, loss 0.00295863, acc 1
2016-09-06T04:08:30.848704: step 18856, loss 0.00191429, acc 1
2016-09-06T04:08:31.661757: step 18857, loss 0.0100789, acc 1
2016-09-06T04:08:32.480866: step 18858, loss 0.00514843, acc 1
2016-09-06T04:08:33.318659: step 18859, loss 0.0699012, acc 0.96
2016-09-06T04:08:34.089704: step 18860, loss 0.00302491, acc 1
2016-09-06T04:08:34.898420: step 18861, loss 0.00212134, acc 1
2016-09-06T04:08:35.767059: step 18862, loss 0.0434842, acc 0.98
2016-09-06T04:08:36.611667: step 18863, loss 0.00201352, acc 1
2016-09-06T04:08:37.376725: step 18864, loss 0.00173154, acc 1
2016-09-06T04:08:38.192530: step 18865, loss 0.015315, acc 1
2016-09-06T04:08:39.020767: step 18866, loss 0.00169242, acc 1
2016-09-06T04:08:39.815880: step 18867, loss 0.00166435, acc 1
2016-09-06T04:08:40.603381: step 18868, loss 0.0203225, acc 1
2016-09-06T04:08:41.436524: step 18869, loss 0.0167785, acc 0.98
2016-09-06T04:08:42.234727: step 18870, loss 0.00713694, acc 1
2016-09-06T04:08:43.025429: step 18871, loss 0.00755436, acc 1
2016-09-06T04:08:43.840231: step 18872, loss 0.00223695, acc 1
2016-09-06T04:08:44.624735: step 18873, loss 0.00194205, acc 1
2016-09-06T04:08:45.462788: step 18874, loss 0.0410723, acc 0.96
2016-09-06T04:08:46.284824: step 18875, loss 0.0163692, acc 0.98
2016-09-06T04:08:47.062828: step 18876, loss 0.00161482, acc 1
2016-09-06T04:08:47.863049: step 18877, loss 0.0591444, acc 0.96
2016-09-06T04:08:48.675293: step 18878, loss 0.00165062, acc 1
2016-09-06T04:08:49.459988: step 18879, loss 0.00160906, acc 1
2016-09-06T04:08:50.263029: step 18880, loss 0.0729409, acc 0.96
2016-09-06T04:08:51.124287: step 18881, loss 0.0133207, acc 1
2016-09-06T04:08:51.925088: step 18882, loss 0.0178717, acc 0.98
2016-09-06T04:08:52.715351: step 18883, loss 0.00643923, acc 1
2016-09-06T04:08:53.530089: step 18884, loss 0.0225552, acc 0.98
2016-09-06T04:08:54.336024: step 18885, loss 0.00476354, acc 1
2016-09-06T04:08:55.137518: step 18886, loss 0.00398961, acc 1
2016-09-06T04:08:55.980553: step 18887, loss 0.0272671, acc 0.98
2016-09-06T04:08:56.781239: step 18888, loss 0.00415035, acc 1
2016-09-06T04:08:57.597949: step 18889, loss 0.0211331, acc 1
2016-09-06T04:08:58.435691: step 18890, loss 0.00209976, acc 1
2016-09-06T04:08:59.213217: step 18891, loss 0.0162555, acc 1
2016-09-06T04:09:00.010561: step 18892, loss 0.0347045, acc 0.98
2016-09-06T04:09:00.841978: step 18893, loss 0.00245015, acc 1
2016-09-06T04:09:01.637935: step 18894, loss 0.0362847, acc 0.96
2016-09-06T04:09:02.442130: step 18895, loss 0.00670123, acc 1
2016-09-06T04:09:03.255735: step 18896, loss 0.0111709, acc 1
2016-09-06T04:09:04.047121: step 18897, loss 0.00198597, acc 1
2016-09-06T04:09:04.859742: step 18898, loss 0.016236, acc 0.98
2016-09-06T04:09:05.662246: step 18899, loss 0.00207059, acc 1
2016-09-06T04:09:06.454876: step 18900, loss 0.00210005, acc 1

Evaluation:
2016-09-06T04:09:10.179163: step 18900, loss 3.63405, acc 0.695122

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-18900

2016-09-06T04:09:12.101474: step 18901, loss 0.0284995, acc 0.98
2016-09-06T04:09:12.947389: step 18902, loss 0.0111486, acc 1
2016-09-06T04:09:13.798407: step 18903, loss 0.0102043, acc 1
2016-09-06T04:09:14.618864: step 18904, loss 0.021751, acc 1
2016-09-06T04:09:15.469699: step 18905, loss 0.00460702, acc 1
2016-09-06T04:09:16.273862: step 18906, loss 0.00233871, acc 1
2016-09-06T04:09:17.088697: step 18907, loss 0.0325529, acc 0.98
2016-09-06T04:09:17.945045: step 18908, loss 0.00247956, acc 1
2016-09-06T04:09:18.771732: step 18909, loss 0.0185844, acc 1
2016-09-06T04:09:19.632961: step 18910, loss 0.0190181, acc 0.98
2016-09-06T04:09:20.482034: step 18911, loss 0.00262567, acc 1
2016-09-06T04:09:21.315559: step 18912, loss 0.00256514, acc 1
2016-09-06T04:09:22.108961: step 18913, loss 0.00254003, acc 1
2016-09-06T04:09:23.032535: step 18914, loss 0.0269027, acc 1
2016-09-06T04:09:23.883393: step 18915, loss 0.0149177, acc 1
2016-09-06T04:09:24.727963: step 18916, loss 0.0139351, acc 1
2016-09-06T04:09:25.555288: step 18917, loss 0.0166743, acc 1
2016-09-06T04:09:26.410147: step 18918, loss 0.0066947, acc 1
2016-09-06T04:09:27.232207: step 18919, loss 0.00631325, acc 1
2016-09-06T04:09:28.047346: step 18920, loss 0.00263448, acc 1
2016-09-06T04:09:28.968204: step 18921, loss 0.0163002, acc 1
2016-09-06T04:09:29.776279: step 18922, loss 0.00265837, acc 1
2016-09-06T04:09:30.583411: step 18923, loss 0.0308657, acc 0.98
2016-09-06T04:09:31.397154: step 18924, loss 0.0321772, acc 0.98
2016-09-06T04:09:32.235438: step 18925, loss 0.00381456, acc 1
2016-09-06T04:09:33.016067: step 18926, loss 0.0299749, acc 0.98
2016-09-06T04:09:33.835479: step 18927, loss 0.00264392, acc 1
2016-09-06T04:09:34.682462: step 18928, loss 0.00495737, acc 1
2016-09-06T04:09:35.571723: step 18929, loss 0.0379073, acc 0.96
2016-09-06T04:09:36.396050: step 18930, loss 0.00274336, acc 1
2016-09-06T04:09:37.251441: step 18931, loss 0.118236, acc 0.96
2016-09-06T04:09:38.112726: step 18932, loss 0.00258446, acc 1
2016-09-06T04:09:38.940259: step 18933, loss 0.0843228, acc 0.98
2016-09-06T04:09:39.781458: step 18934, loss 0.00869559, acc 1
2016-09-06T04:09:40.646756: step 18935, loss 0.014484, acc 1
2016-09-06T04:09:41.439587: step 18936, loss 0.00869788, acc 1
2016-09-06T04:09:42.235462: step 18937, loss 0.00239498, acc 1
2016-09-06T04:09:43.058527: step 18938, loss 0.00348256, acc 1
2016-09-06T04:09:43.845206: step 18939, loss 0.00305277, acc 1
2016-09-06T04:09:44.663067: step 18940, loss 0.0111417, acc 1
2016-09-06T04:09:45.495718: step 18941, loss 0.0271097, acc 1
2016-09-06T04:09:46.301483: step 18942, loss 0.00302755, acc 1
2016-09-06T04:09:47.121676: step 18943, loss 0.00213064, acc 1
2016-09-06T04:09:47.948445: step 18944, loss 0.00242952, acc 1
2016-09-06T04:09:48.771379: step 18945, loss 0.00636483, acc 1
2016-09-06T04:09:49.602529: step 18946, loss 0.00707937, acc 1
2016-09-06T04:09:50.426666: step 18947, loss 0.0239123, acc 1
2016-09-06T04:09:51.237650: step 18948, loss 0.00214932, acc 1
2016-09-06T04:09:52.078116: step 18949, loss 0.075466, acc 0.98
2016-09-06T04:09:52.936165: step 18950, loss 0.0584731, acc 0.96
2016-09-06T04:09:53.763816: step 18951, loss 0.0166164, acc 1
2016-09-06T04:09:54.564726: step 18952, loss 0.00273333, acc 1
2016-09-06T04:09:55.398833: step 18953, loss 0.00935181, acc 1
2016-09-06T04:09:56.208812: step 18954, loss 0.0214252, acc 1
2016-09-06T04:09:57.020826: step 18955, loss 0.00181083, acc 1
2016-09-06T04:09:57.848233: step 18956, loss 0.00805587, acc 1
2016-09-06T04:09:58.682728: step 18957, loss 0.0355693, acc 0.98
2016-09-06T04:09:59.481266: step 18958, loss 0.00241448, acc 1
2016-09-06T04:10:00.330174: step 18959, loss 0.00580234, acc 1
2016-09-06T04:10:01.141494: step 18960, loss 0.0481628, acc 0.98
2016-09-06T04:10:01.959526: step 18961, loss 0.00177922, acc 1
2016-09-06T04:10:02.793244: step 18962, loss 0.00186837, acc 1
2016-09-06T04:10:03.678548: step 18963, loss 0.0265916, acc 1
2016-09-06T04:10:04.494261: step 18964, loss 0.0170403, acc 0.98
2016-09-06T04:10:05.306874: step 18965, loss 0.0221997, acc 0.98
2016-09-06T04:10:06.111811: step 18966, loss 0.0134049, acc 1
2016-09-06T04:10:06.915078: step 18967, loss 0.00173082, acc 1
2016-09-06T04:10:07.718712: step 18968, loss 0.00212308, acc 1
2016-09-06T04:10:08.541746: step 18969, loss 0.00378783, acc 1
2016-09-06T04:10:09.341114: step 18970, loss 0.00200084, acc 1
2016-09-06T04:10:10.155906: step 18971, loss 0.00175028, acc 1
2016-09-06T04:10:10.981701: step 18972, loss 0.0017498, acc 1
2016-09-06T04:10:11.806965: step 18973, loss 0.00171871, acc 1
2016-09-06T04:10:12.636891: step 18974, loss 0.00189307, acc 1
2016-09-06T04:10:13.455978: step 18975, loss 0.0280059, acc 0.98
2016-09-06T04:10:14.221873: step 18976, loss 0.00238453, acc 1
2016-09-06T04:10:15.059319: step 18977, loss 0.13035, acc 0.98
2016-09-06T04:10:15.888734: step 18978, loss 0.0186045, acc 0.98
2016-09-06T04:10:16.735379: step 18979, loss 0.00217966, acc 1
2016-09-06T04:10:17.557044: step 18980, loss 0.00654938, acc 1
2016-09-06T04:10:18.394779: step 18981, loss 0.00354072, acc 1
2016-09-06T04:10:19.277680: step 18982, loss 0.0157412, acc 1
2016-09-06T04:10:20.100908: step 18983, loss 0.0137041, acc 1
2016-09-06T04:10:20.906419: step 18984, loss 0.0150941, acc 1
2016-09-06T04:10:21.726359: step 18985, loss 0.0404774, acc 0.98
2016-09-06T04:10:22.602463: step 18986, loss 0.0186335, acc 0.98
2016-09-06T04:10:23.405405: step 18987, loss 0.0016459, acc 1
2016-09-06T04:10:24.208840: step 18988, loss 0.00196583, acc 1
2016-09-06T04:10:24.998684: step 18989, loss 0.00420786, acc 1
2016-09-06T04:10:25.800234: step 18990, loss 0.0160969, acc 0.98
2016-09-06T04:10:26.615501: step 18991, loss 0.0128148, acc 1
2016-09-06T04:10:27.409397: step 18992, loss 0.00158553, acc 1
2016-09-06T04:10:28.225013: step 18993, loss 0.0166486, acc 0.98
2016-09-06T04:10:29.049695: step 18994, loss 0.0120169, acc 1
2016-09-06T04:10:29.830126: step 18995, loss 0.00185064, acc 1
2016-09-06T04:10:30.631060: step 18996, loss 0.0321515, acc 0.96
2016-09-06T04:10:31.467856: step 18997, loss 0.0027487, acc 1
2016-09-06T04:10:32.221457: step 18998, loss 0.0544249, acc 0.98
2016-09-06T04:10:33.034498: step 18999, loss 0.00272822, acc 1
2016-09-06T04:10:33.901235: step 19000, loss 0.00168596, acc 1

Evaluation:
2016-09-06T04:10:37.588002: step 19000, loss 3.06836, acc 0.69606

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-19000

2016-09-06T04:10:39.518960: step 19001, loss 0.0380254, acc 0.98
2016-09-06T04:10:40.368154: step 19002, loss 0.0168805, acc 1
2016-09-06T04:10:41.180086: step 19003, loss 0.0168039, acc 0.98
2016-09-06T04:10:41.996548: step 19004, loss 0.00268754, acc 1
2016-09-06T04:10:42.848914: step 19005, loss 0.00688547, acc 1
2016-09-06T04:10:43.694062: step 19006, loss 0.0279172, acc 0.98
2016-09-06T04:10:44.455595: step 19007, loss 0.0133421, acc 1
2016-09-06T04:10:45.203251: step 19008, loss 0.00173883, acc 1
2016-09-06T04:10:45.999275: step 19009, loss 0.00329041, acc 1
2016-09-06T04:10:46.792880: step 19010, loss 0.00609049, acc 1
2016-09-06T04:10:47.632371: step 19011, loss 0.00185795, acc 1
2016-09-06T04:10:48.450688: step 19012, loss 0.0467227, acc 0.98
2016-09-06T04:10:49.250685: step 19013, loss 0.0363955, acc 0.96
2016-09-06T04:10:50.088301: step 19014, loss 0.0110233, acc 1
2016-09-06T04:10:50.901650: step 19015, loss 0.0124125, acc 1
2016-09-06T04:10:51.704095: step 19016, loss 0.00174643, acc 1
2016-09-06T04:10:52.522869: step 19017, loss 0.00176743, acc 1
2016-09-06T04:10:53.331297: step 19018, loss 0.00172433, acc 1
2016-09-06T04:10:54.122518: step 19019, loss 0.0149247, acc 1
2016-09-06T04:10:54.920731: step 19020, loss 0.00210903, acc 1
2016-09-06T04:10:55.723078: step 19021, loss 0.0029096, acc 1
2016-09-06T04:10:56.523326: step 19022, loss 0.00570061, acc 1
2016-09-06T04:10:57.325700: step 19023, loss 0.0018274, acc 1
2016-09-06T04:10:58.144694: step 19024, loss 0.0266337, acc 0.98
2016-09-06T04:10:58.948019: step 19025, loss 0.0131256, acc 1
2016-09-06T04:10:59.763541: step 19026, loss 0.00173987, acc 1
2016-09-06T04:11:00.618373: step 19027, loss 0.00192534, acc 1
2016-09-06T04:11:01.405406: step 19028, loss 0.00381486, acc 1
2016-09-06T04:11:02.169239: step 19029, loss 0.00168843, acc 1
2016-09-06T04:11:02.994371: step 19030, loss 0.00914199, acc 1
2016-09-06T04:11:03.773416: step 19031, loss 0.0330918, acc 0.98
2016-09-06T04:11:04.578650: step 19032, loss 0.00498941, acc 1
2016-09-06T04:11:05.391919: step 19033, loss 0.0186211, acc 0.98
2016-09-06T04:11:06.168853: step 19034, loss 0.0168688, acc 1
2016-09-06T04:11:06.974830: step 19035, loss 0.0277466, acc 0.98
2016-09-06T04:11:07.772731: step 19036, loss 0.00167903, acc 1
2016-09-06T04:11:08.573272: step 19037, loss 0.0419725, acc 0.98
2016-09-06T04:11:09.407235: step 19038, loss 0.00161763, acc 1
2016-09-06T04:11:10.224800: step 19039, loss 0.0104366, acc 1
2016-09-06T04:11:11.000688: step 19040, loss 0.00295141, acc 1
2016-09-06T04:11:11.812955: step 19041, loss 0.00913437, acc 1
2016-09-06T04:11:12.662635: step 19042, loss 0.0470807, acc 0.96
2016-09-06T04:11:13.460842: step 19043, loss 0.00255278, acc 1
2016-09-06T04:11:14.246523: step 19044, loss 0.00768496, acc 1
2016-09-06T04:11:15.051416: step 19045, loss 0.00189289, acc 1
2016-09-06T04:11:15.845478: step 19046, loss 0.0228241, acc 0.98
2016-09-06T04:11:16.668633: step 19047, loss 0.0193093, acc 0.98
2016-09-06T04:11:17.492513: step 19048, loss 0.0020571, acc 1
2016-09-06T04:11:18.265438: step 19049, loss 0.0103128, acc 1
2016-09-06T04:11:19.082306: step 19050, loss 0.020967, acc 0.98
2016-09-06T04:11:19.911032: step 19051, loss 0.0147713, acc 1
2016-09-06T04:11:20.704485: step 19052, loss 0.00218516, acc 1
2016-09-06T04:11:21.492965: step 19053, loss 0.00235472, acc 1
2016-09-06T04:11:22.321411: step 19054, loss 0.0176668, acc 1
2016-09-06T04:11:23.109084: step 19055, loss 0.0912524, acc 0.96
2016-09-06T04:11:23.910120: step 19056, loss 0.00482076, acc 1
2016-09-06T04:11:24.712584: step 19057, loss 0.00265495, acc 1
2016-09-06T04:11:25.568139: step 19058, loss 0.00268534, acc 1
2016-09-06T04:11:26.395679: step 19059, loss 0.00730278, acc 1
2016-09-06T04:11:27.234845: step 19060, loss 0.0264113, acc 0.98
2016-09-06T04:11:28.033545: step 19061, loss 0.0638813, acc 0.96
2016-09-06T04:11:28.823750: step 19062, loss 0.0156779, acc 1
2016-09-06T04:11:29.631978: step 19063, loss 0.0097134, acc 1
2016-09-06T04:11:30.404091: step 19064, loss 0.0126215, acc 1
2016-09-06T04:11:31.215126: step 19065, loss 0.0120249, acc 1
2016-09-06T04:11:32.044011: step 19066, loss 0.00980881, acc 1
2016-09-06T04:11:32.849221: step 19067, loss 0.00519365, acc 1
2016-09-06T04:11:33.633405: step 19068, loss 0.00475703, acc 1
2016-09-06T04:11:34.456800: step 19069, loss 0.0146275, acc 1
2016-09-06T04:11:35.259226: step 19070, loss 0.005586, acc 1
2016-09-06T04:11:36.061533: step 19071, loss 0.00336719, acc 1
2016-09-06T04:11:36.877574: step 19072, loss 0.0435568, acc 0.96
2016-09-06T04:11:37.669671: step 19073, loss 0.00760002, acc 1
2016-09-06T04:11:38.483545: step 19074, loss 0.022374, acc 0.98
2016-09-06T04:11:39.312898: step 19075, loss 0.0177247, acc 0.98
2016-09-06T04:11:40.185507: step 19076, loss 0.0218307, acc 0.98
2016-09-06T04:11:41.004137: step 19077, loss 0.0192508, acc 0.98
2016-09-06T04:11:41.830666: step 19078, loss 0.0154927, acc 1
2016-09-06T04:11:42.681902: step 19079, loss 0.0132255, acc 1
2016-09-06T04:11:43.530411: step 19080, loss 0.00379351, acc 1
2016-09-06T04:11:44.378844: step 19081, loss 0.0335931, acc 0.98
2016-09-06T04:11:45.242867: step 19082, loss 0.0032376, acc 1
2016-09-06T04:11:46.066393: step 19083, loss 0.0198051, acc 1
2016-09-06T04:11:46.906589: step 19084, loss 0.01412, acc 1
2016-09-06T04:11:47.729458: step 19085, loss 0.0218997, acc 0.98
2016-09-06T04:11:48.548821: step 19086, loss 0.0315773, acc 0.98
2016-09-06T04:11:49.334746: step 19087, loss 0.00339945, acc 1
2016-09-06T04:11:50.262058: step 19088, loss 0.00341513, acc 1
2016-09-06T04:11:51.122987: step 19089, loss 0.00390989, acc 1
2016-09-06T04:11:52.149921: step 19090, loss 0.00317724, acc 1
2016-09-06T04:11:53.093135: step 19091, loss 0.00981203, acc 1
2016-09-06T04:11:53.938518: step 19092, loss 0.00929912, acc 1
2016-09-06T04:11:54.765454: step 19093, loss 0.00536635, acc 1
2016-09-06T04:11:55.718160: step 19094, loss 0.00334277, acc 1
2016-09-06T04:11:56.534822: step 19095, loss 0.0155076, acc 1
2016-09-06T04:11:57.377007: step 19096, loss 0.00387107, acc 1
2016-09-06T04:11:58.179901: step 19097, loss 0.0238709, acc 1
2016-09-06T04:11:59.367081: step 19098, loss 0.00302932, acc 1
2016-09-06T04:12:00.322018: step 19099, loss 0.0238937, acc 0.98
2016-09-06T04:12:01.295127: step 19100, loss 0.00303333, acc 1

Evaluation:
2016-09-06T04:12:05.071528: step 19100, loss 4.24544, acc 0.695122

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-19100

2016-09-06T04:12:07.034976: step 19101, loss 0.00397487, acc 1
2016-09-06T04:12:07.955928: step 19102, loss 0.0029535, acc 1
2016-09-06T04:12:08.802568: step 19103, loss 0.00293534, acc 1
2016-09-06T04:12:09.744165: step 19104, loss 0.0482282, acc 0.98
2016-09-06T04:12:10.605782: step 19105, loss 0.00298384, acc 1
2016-09-06T04:12:11.499673: step 19106, loss 0.00288152, acc 1
2016-09-06T04:12:12.414463: step 19107, loss 0.018088, acc 0.98
2016-09-06T04:12:13.266042: step 19108, loss 0.0206202, acc 0.98
2016-09-06T04:12:14.059568: step 19109, loss 0.0576243, acc 0.98
2016-09-06T04:12:14.889015: step 19110, loss 0.00294676, acc 1
2016-09-06T04:12:15.703986: step 19111, loss 0.00283001, acc 1
2016-09-06T04:12:16.592258: step 19112, loss 0.0026395, acc 1
2016-09-06T04:12:17.403788: step 19113, loss 0.0183588, acc 0.98
2016-09-06T04:12:18.240716: step 19114, loss 0.00256506, acc 1
2016-09-06T04:12:19.092384: step 19115, loss 0.021037, acc 1
2016-09-06T04:12:20.093594: step 19116, loss 0.00302721, acc 1
2016-09-06T04:12:20.923569: step 19117, loss 0.0982351, acc 0.98
2016-09-06T04:12:21.830087: step 19118, loss 0.00796862, acc 1
2016-09-06T04:12:22.655963: step 19119, loss 0.00614754, acc 1
2016-09-06T04:12:23.490366: step 19120, loss 0.019043, acc 1
2016-09-06T04:12:24.327276: step 19121, loss 0.0517055, acc 0.98
2016-09-06T04:12:25.143280: step 19122, loss 0.00238839, acc 1
2016-09-06T04:12:25.945929: step 19123, loss 0.0841034, acc 0.98
2016-09-06T04:12:26.780786: step 19124, loss 0.0025481, acc 1
2016-09-06T04:12:27.604756: step 19125, loss 0.00296015, acc 1
2016-09-06T04:12:28.465220: step 19126, loss 0.00662348, acc 1
2016-09-06T04:12:29.286425: step 19127, loss 0.0303657, acc 0.98
2016-09-06T04:12:30.105450: step 19128, loss 0.0023049, acc 1
2016-09-06T04:12:30.922149: step 19129, loss 0.00226926, acc 1
2016-09-06T04:12:31.742913: step 19130, loss 0.0156362, acc 1
2016-09-06T04:12:32.584018: step 19131, loss 0.0134691, acc 1
2016-09-06T04:12:33.348776: step 19132, loss 0.0249389, acc 0.98
2016-09-06T04:12:34.161099: step 19133, loss 0.0330644, acc 1
2016-09-06T04:12:34.978038: step 19134, loss 0.00212279, acc 1
2016-09-06T04:12:35.785662: step 19135, loss 0.00302475, acc 1
2016-09-06T04:12:36.603534: step 19136, loss 0.0238549, acc 0.98
2016-09-06T04:12:37.413909: step 19137, loss 0.00488069, acc 1
2016-09-06T04:12:38.241567: step 19138, loss 0.0371678, acc 0.96
2016-09-06T04:12:39.056000: step 19139, loss 0.0060468, acc 1
2016-09-06T04:12:39.948369: step 19140, loss 0.0027235, acc 1
2016-09-06T04:12:40.761983: step 19141, loss 0.00208451, acc 1
2016-09-06T04:12:41.571617: step 19142, loss 0.00882745, acc 1
2016-09-06T04:12:42.384834: step 19143, loss 0.0188649, acc 0.98
2016-09-06T04:12:43.183270: step 19144, loss 0.0177577, acc 1
2016-09-06T04:12:43.987448: step 19145, loss 0.00381502, acc 1
2016-09-06T04:12:44.817836: step 19146, loss 0.0162311, acc 0.98
2016-09-06T04:12:45.612004: step 19147, loss 0.00209947, acc 1
2016-09-06T04:12:46.419444: step 19148, loss 0.00215315, acc 1
2016-09-06T04:12:47.266608: step 19149, loss 0.02363, acc 0.98
2016-09-06T04:12:48.086095: step 19150, loss 0.00210844, acc 1
2016-09-06T04:12:48.907250: step 19151, loss 0.00209515, acc 1
2016-09-06T04:12:49.724247: step 19152, loss 0.0147368, acc 1
2016-09-06T04:12:50.547308: step 19153, loss 0.00454621, acc 1
2016-09-06T04:12:51.338412: step 19154, loss 0.011859, acc 1
2016-09-06T04:12:52.162018: step 19155, loss 0.00404559, acc 1
2016-09-06T04:12:53.006526: step 19156, loss 0.00225999, acc 1
2016-09-06T04:12:53.823630: step 19157, loss 0.00649499, acc 1
2016-09-06T04:12:54.660643: step 19158, loss 0.00531669, acc 1
2016-09-06T04:12:55.460745: step 19159, loss 0.00305947, acc 1
2016-09-06T04:12:56.282732: step 19160, loss 0.00536389, acc 1
2016-09-06T04:12:57.136268: step 19161, loss 0.00204995, acc 1
2016-09-06T04:12:57.972358: step 19162, loss 0.0112171, acc 1
2016-09-06T04:12:58.759341: step 19163, loss 0.00204694, acc 1
2016-09-06T04:12:59.581806: step 19164, loss 0.0203055, acc 1
2016-09-06T04:13:00.465811: step 19165, loss 0.00429091, acc 1
2016-09-06T04:13:01.303022: step 19166, loss 0.0890479, acc 0.98
2016-09-06T04:13:02.117473: step 19167, loss 0.00214678, acc 1
2016-09-06T04:13:02.909415: step 19168, loss 0.0947355, acc 0.98
2016-09-06T04:13:03.722943: step 19169, loss 0.0332835, acc 0.98
2016-09-06T04:13:04.500746: step 19170, loss 0.00791424, acc 1
2016-09-06T04:13:05.334620: step 19171, loss 0.00280887, acc 1
2016-09-06T04:13:06.145790: step 19172, loss 0.00256401, acc 1
2016-09-06T04:13:06.958147: step 19173, loss 0.0165411, acc 1
2016-09-06T04:13:07.789199: step 19174, loss 0.0172777, acc 0.98
2016-09-06T04:13:08.625437: step 19175, loss 0.00170213, acc 1
2016-09-06T04:13:09.415762: step 19176, loss 0.00718835, acc 1
2016-09-06T04:13:10.237029: step 19177, loss 0.0339574, acc 0.98
2016-09-06T04:13:11.040348: step 19178, loss 0.00326061, acc 1
2016-09-06T04:13:11.865415: step 19179, loss 0.0316568, acc 0.98
2016-09-06T04:13:12.701588: step 19180, loss 0.00173693, acc 1
2016-09-06T04:13:13.555055: step 19181, loss 0.0067344, acc 1
2016-09-06T04:13:14.373370: step 19182, loss 0.0193894, acc 0.98
2016-09-06T04:13:15.204419: step 19183, loss 0.0310262, acc 0.98
2016-09-06T04:13:16.005681: step 19184, loss 0.00165366, acc 1
2016-09-06T04:13:16.789944: step 19185, loss 0.00204052, acc 1
2016-09-06T04:13:17.619821: step 19186, loss 0.0137631, acc 1
2016-09-06T04:13:18.456639: step 19187, loss 0.00659748, acc 1
2016-09-06T04:13:19.273429: step 19188, loss 0.0297634, acc 1
2016-09-06T04:13:20.079508: step 19189, loss 0.00168371, acc 1
2016-09-06T04:13:20.899043: step 19190, loss 0.00195107, acc 1
2016-09-06T04:13:21.685737: step 19191, loss 0.00167822, acc 1
2016-09-06T04:13:22.506390: step 19192, loss 0.00181582, acc 1
2016-09-06T04:13:23.311850: step 19193, loss 0.00332726, acc 1
2016-09-06T04:13:24.106735: step 19194, loss 0.00499074, acc 1
2016-09-06T04:13:24.901696: step 19195, loss 0.00337399, acc 1
2016-09-06T04:13:25.737567: step 19196, loss 0.00162502, acc 1
2016-09-06T04:13:26.541246: step 19197, loss 0.00856191, acc 1
2016-09-06T04:13:27.339193: step 19198, loss 0.00223952, acc 1
2016-09-06T04:13:28.148274: step 19199, loss 0.00169404, acc 1
2016-09-06T04:13:28.847858: step 19200, loss 0.00271122, acc 1

Evaluation:
2016-09-06T04:13:32.560510: step 19200, loss 2.77668, acc 0.706379

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473089861/checkpoints/model-19200


WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7fc3d760be90>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.
WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7fc3d760be50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.

Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=50
CHECKPOINT_EVERY=100
CV_INDEX=9
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=300
EVALUATE_EVERY=100
FILTER_SIZES=3,4,5
HIDDEN_DIM=300
L2_REG_LAMBDA=0
LOG_DEVICE_PLACEMENT=False
NUM_EPOCHS=30
NUM_FILTERS=100
WORD2VEC=GoogleNews-vectors-negative300.bin

Loading data...
Vocabulary Size: 18758
Train/Dev split: 9594/1066
Writing to /home/cil/lstm-context-embeddings/runs/1473194978

Load word2vec file GoogleNews-vectors-negative300.bin

2016-09-07T04:49:58.578745: step 1, loss 0.693147, acc 0.38
2016-09-07T04:49:59.252728: step 2, loss 0.650807, acc 0.68
2016-09-07T04:49:59.955792: step 3, loss 0.780677, acc 0.44
2016-09-07T04:50:00.674673: step 4, loss 0.70422, acc 0.52
2016-09-07T04:50:01.359892: step 5, loss 0.715505, acc 0.5
2016-09-07T04:50:02.011674: step 6, loss 0.71648, acc 0.44
2016-09-07T04:50:02.716562: step 7, loss 0.690537, acc 0.5
2016-09-07T04:50:03.395681: step 8, loss 0.689841, acc 0.54
2016-09-07T04:50:04.061121: step 9, loss 0.693443, acc 0.48
2016-09-07T04:50:04.717614: step 10, loss 0.6938, acc 0.56
2016-09-07T04:50:05.390280: step 11, loss 0.733944, acc 0.38
2016-09-07T04:50:06.085506: step 12, loss 0.734094, acc 0.38
2016-09-07T04:50:06.773765: step 13, loss 0.689661, acc 0.52
2016-09-07T04:50:07.481067: step 14, loss 0.69315, acc 0.44
2016-09-07T04:50:08.148462: step 15, loss 0.686605, acc 0.6
2016-09-07T04:50:08.852109: step 16, loss 0.704576, acc 0.42
2016-09-07T04:50:09.549872: step 17, loss 0.68584, acc 0.54
2016-09-07T04:50:10.229244: step 18, loss 0.709798, acc 0.42
2016-09-07T04:50:10.898702: step 19, loss 0.706211, acc 0.44
2016-09-07T04:50:11.577619: step 20, loss 0.687053, acc 0.54
2016-09-07T04:50:12.273826: step 21, loss 0.688535, acc 0.52
2016-09-07T04:50:12.942825: step 22, loss 0.67965, acc 0.66
2016-09-07T04:50:13.640873: step 23, loss 0.68179, acc 0.56
2016-09-07T04:50:14.322613: step 24, loss 0.67925, acc 0.62
2016-09-07T04:50:14.995502: step 25, loss 0.703621, acc 0.46
2016-09-07T04:50:15.682120: step 26, loss 0.670444, acc 0.64
2016-09-07T04:50:16.366245: step 27, loss 0.687702, acc 0.6
2016-09-07T04:50:17.074776: step 28, loss 0.643291, acc 0.78
2016-09-07T04:50:17.767700: step 29, loss 0.681348, acc 0.56
2016-09-07T04:50:18.432229: step 30, loss 0.672661, acc 0.54
2016-09-07T04:50:19.132001: step 31, loss 0.645319, acc 0.62
2016-09-07T04:50:19.815687: step 32, loss 0.626133, acc 0.6
2016-09-07T04:50:20.505031: step 33, loss 0.772634, acc 0.44
2016-09-07T04:50:21.215632: step 34, loss 0.635482, acc 0.62
2016-09-07T04:50:21.915104: step 35, loss 0.68534, acc 0.48
2016-09-07T04:50:22.609571: step 36, loss 0.588628, acc 0.68
2016-09-07T04:50:23.287296: step 37, loss 0.678188, acc 0.56
2016-09-07T04:50:23.981669: step 38, loss 0.561372, acc 0.74
2016-09-07T04:50:24.660161: step 39, loss 0.625938, acc 0.66
2016-09-07T04:50:25.355405: step 40, loss 0.651901, acc 0.66
2016-09-07T04:50:26.039802: step 41, loss 0.731134, acc 0.58
2016-09-07T04:50:26.754323: step 42, loss 0.604146, acc 0.66
2016-09-07T04:50:27.443575: step 43, loss 0.591282, acc 0.72
2016-09-07T04:50:28.106698: step 44, loss 0.595491, acc 0.72
2016-09-07T04:50:28.804943: step 45, loss 0.540894, acc 0.74
2016-09-07T04:50:29.479466: step 46, loss 0.589472, acc 0.64
2016-09-07T04:50:30.183321: step 47, loss 0.689432, acc 0.76
2016-09-07T04:50:30.851589: step 48, loss 0.669086, acc 0.72
2016-09-07T04:50:31.555521: step 49, loss 0.635333, acc 0.58
2016-09-07T04:50:32.230624: step 50, loss 0.663022, acc 0.66
2016-09-07T04:50:32.917036: step 51, loss 0.786283, acc 0.48
2016-09-07T04:50:33.598715: step 52, loss 0.58627, acc 0.66
2016-09-07T04:50:34.292338: step 53, loss 0.621712, acc 0.68
2016-09-07T04:50:34.994019: step 54, loss 0.569108, acc 0.72
2016-09-07T04:50:35.667491: step 55, loss 0.718692, acc 0.5
2016-09-07T04:50:36.371648: step 56, loss 0.70422, acc 0.54
2016-09-07T04:50:37.051764: step 57, loss 0.715658, acc 0.52
2016-09-07T04:50:37.750781: step 58, loss 0.621594, acc 0.68
2016-09-07T04:50:38.444385: step 59, loss 0.554551, acc 0.8
2016-09-07T04:50:39.127633: step 60, loss 0.589148, acc 0.74
2016-09-07T04:50:39.829761: step 61, loss 0.556898, acc 0.8
2016-09-07T04:50:40.507115: step 62, loss 0.567943, acc 0.72
2016-09-07T04:50:41.225820: step 63, loss 0.525705, acc 0.76
2016-09-07T04:50:41.949160: step 64, loss 0.479981, acc 0.76
2016-09-07T04:50:42.641958: step 65, loss 0.635149, acc 0.62
2016-09-07T04:50:43.329573: step 66, loss 0.595923, acc 0.72
2016-09-07T04:50:44.017334: step 67, loss 0.481482, acc 0.82
2016-09-07T04:50:44.727200: step 68, loss 0.601569, acc 0.66
2016-09-07T04:50:45.407765: step 69, loss 0.71849, acc 0.62
2016-09-07T04:50:46.092660: step 70, loss 0.569488, acc 0.58
2016-09-07T04:50:46.769585: step 71, loss 0.608872, acc 0.68
2016-09-07T04:50:47.438011: step 72, loss 0.541264, acc 0.68
2016-09-07T04:50:48.135745: step 73, loss 0.468079, acc 0.84
2016-09-07T04:50:48.814386: step 74, loss 0.628087, acc 0.68
2016-09-07T04:50:49.513932: step 75, loss 0.658859, acc 0.66
2016-09-07T04:50:50.179136: step 76, loss 0.716992, acc 0.6
2016-09-07T04:50:50.864029: step 77, loss 0.510015, acc 0.72
2016-09-07T04:50:51.553739: step 78, loss 0.558388, acc 0.74
2016-09-07T04:50:52.239590: step 79, loss 0.590237, acc 0.6
2016-09-07T04:50:52.923712: step 80, loss 0.609713, acc 0.62
2016-09-07T04:50:53.610668: step 81, loss 0.578839, acc 0.74
2016-09-07T04:50:54.302474: step 82, loss 0.586003, acc 0.7
2016-09-07T04:50:54.968411: step 83, loss 0.513993, acc 0.7
2016-09-07T04:50:55.667929: step 84, loss 0.597173, acc 0.68
2016-09-07T04:50:56.346966: step 85, loss 0.52641, acc 0.76
2016-09-07T04:50:57.026395: step 86, loss 0.547232, acc 0.7
2016-09-07T04:50:57.703085: step 87, loss 0.556889, acc 0.7
2016-09-07T04:50:58.384543: step 88, loss 0.499466, acc 0.76
2016-09-07T04:50:59.074020: step 89, loss 0.506919, acc 0.74
2016-09-07T04:50:59.725257: step 90, loss 0.499892, acc 0.74
2016-09-07T04:51:00.450132: step 91, loss 0.709644, acc 0.66
2016-09-07T04:51:01.123548: step 92, loss 0.39366, acc 0.78
2016-09-07T04:51:01.788444: step 93, loss 0.382203, acc 0.8
2016-09-07T04:51:02.461117: step 94, loss 0.474609, acc 0.78
2016-09-07T04:51:03.136919: step 95, loss 0.498264, acc 0.78
2016-09-07T04:51:03.831438: step 96, loss 0.485179, acc 0.72
2016-09-07T04:51:04.518189: step 97, loss 0.340546, acc 0.9
2016-09-07T04:51:05.224847: step 98, loss 0.61675, acc 0.64
2016-09-07T04:51:05.881420: step 99, loss 0.553561, acc 0.66
2016-09-07T04:51:06.590051: step 100, loss 0.537392, acc 0.66

Evaluation:
2016-09-07T04:51:09.875052: step 100, loss 0.612066, acc 0.705441

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-100

2016-09-07T04:51:11.621673: step 101, loss 0.537252, acc 0.72
2016-09-07T04:51:12.299238: step 102, loss 0.549469, acc 0.7
2016-09-07T04:51:12.969132: step 103, loss 0.675749, acc 0.7
2016-09-07T04:51:13.656198: step 104, loss 0.482837, acc 0.78
2016-09-07T04:51:14.331067: step 105, loss 0.446901, acc 0.84
2016-09-07T04:51:15.011635: step 106, loss 0.495639, acc 0.84
2016-09-07T04:51:15.690668: step 107, loss 0.497114, acc 0.7
2016-09-07T04:51:16.375134: step 108, loss 0.493853, acc 0.8
2016-09-07T04:51:17.047431: step 109, loss 0.558503, acc 0.68
2016-09-07T04:51:17.754966: step 110, loss 0.491466, acc 0.8
2016-09-07T04:51:18.443260: step 111, loss 0.57779, acc 0.68
2016-09-07T04:51:19.126448: step 112, loss 0.591901, acc 0.7
2016-09-07T04:51:19.807049: step 113, loss 0.491235, acc 0.76
2016-09-07T04:51:20.483955: step 114, loss 0.489701, acc 0.74
2016-09-07T04:51:21.161243: step 115, loss 0.629541, acc 0.62
2016-09-07T04:51:21.842470: step 116, loss 0.545918, acc 0.68
2016-09-07T04:51:22.506978: step 117, loss 0.497543, acc 0.76
2016-09-07T04:51:23.192858: step 118, loss 0.41555, acc 0.82
2016-09-07T04:51:23.868522: step 119, loss 0.522069, acc 0.84
2016-09-07T04:51:24.583945: step 120, loss 0.37389, acc 0.84
2016-09-07T04:51:25.290224: step 121, loss 0.434496, acc 0.78
2016-09-07T04:51:25.978736: step 122, loss 0.496942, acc 0.82
2016-09-07T04:51:26.669746: step 123, loss 0.826177, acc 0.5
2016-09-07T04:51:27.359863: step 124, loss 0.567881, acc 0.66
2016-09-07T04:51:28.095845: step 125, loss 0.484839, acc 0.74
2016-09-07T04:51:28.769525: step 126, loss 0.407397, acc 0.82
2016-09-07T04:51:29.462000: step 127, loss 0.482551, acc 0.78
2016-09-07T04:51:30.152160: step 128, loss 0.498736, acc 0.76
2016-09-07T04:51:30.835298: step 129, loss 0.474038, acc 0.78
2016-09-07T04:51:31.518511: step 130, loss 0.592585, acc 0.62
2016-09-07T04:51:32.198034: step 131, loss 0.617038, acc 0.74
2016-09-07T04:51:32.879571: step 132, loss 0.384732, acc 0.88
2016-09-07T04:51:33.559353: step 133, loss 0.396266, acc 0.78
2016-09-07T04:51:34.229701: step 134, loss 0.502597, acc 0.8
2016-09-07T04:51:34.895233: step 135, loss 0.46281, acc 0.78
2016-09-07T04:51:35.589062: step 136, loss 0.370899, acc 0.86
2016-09-07T04:51:36.274545: step 137, loss 0.451915, acc 0.78
2016-09-07T04:51:36.967574: step 138, loss 0.459638, acc 0.78
2016-09-07T04:51:37.675603: step 139, loss 0.584368, acc 0.68
2016-09-07T04:51:38.354405: step 140, loss 0.536808, acc 0.76
2016-09-07T04:51:39.052653: step 141, loss 0.634866, acc 0.7
2016-09-07T04:51:39.737363: step 142, loss 0.569364, acc 0.76
2016-09-07T04:51:40.419268: step 143, loss 0.466942, acc 0.74
2016-09-07T04:51:41.105515: step 144, loss 0.549809, acc 0.66
2016-09-07T04:51:41.800287: step 145, loss 0.399716, acc 0.84
2016-09-07T04:51:42.507520: step 146, loss 0.421111, acc 0.86
2016-09-07T04:51:43.195574: step 147, loss 0.541277, acc 0.68
2016-09-07T04:51:43.896250: step 148, loss 0.504854, acc 0.74
2016-09-07T04:51:44.590562: step 149, loss 0.462988, acc 0.82
2016-09-07T04:51:45.306986: step 150, loss 0.497014, acc 0.78
2016-09-07T04:51:45.998665: step 151, loss 0.541613, acc 0.68
2016-09-07T04:51:46.674670: step 152, loss 0.435571, acc 0.78
2016-09-07T04:51:47.398545: step 153, loss 0.554376, acc 0.66
2016-09-07T04:51:48.086621: step 154, loss 0.576702, acc 0.68
2016-09-07T04:51:48.773905: step 155, loss 0.554355, acc 0.68
2016-09-07T04:51:49.447923: step 156, loss 0.551799, acc 0.7
2016-09-07T04:51:50.141394: step 157, loss 0.405979, acc 0.84
2016-09-07T04:51:50.830483: step 158, loss 0.473099, acc 0.74
2016-09-07T04:51:51.481790: step 159, loss 0.511191, acc 0.72
2016-09-07T04:51:52.176876: step 160, loss 0.433404, acc 0.8
2016-09-07T04:51:52.845778: step 161, loss 0.448372, acc 0.76
2016-09-07T04:51:53.518145: step 162, loss 0.469316, acc 0.78
2016-09-07T04:51:54.201033: step 163, loss 0.475817, acc 0.74
2016-09-07T04:51:54.875119: step 164, loss 0.643712, acc 0.66
2016-09-07T04:51:55.573150: step 165, loss 0.483915, acc 0.72
2016-09-07T04:51:56.261941: step 166, loss 0.402077, acc 0.86
2016-09-07T04:51:56.983083: step 167, loss 0.471524, acc 0.84
2016-09-07T04:51:57.657264: step 168, loss 0.517047, acc 0.78
2016-09-07T04:51:58.333510: step 169, loss 0.427229, acc 0.78
2016-09-07T04:51:59.017739: step 170, loss 0.513517, acc 0.7
2016-09-07T04:51:59.699804: step 171, loss 0.535727, acc 0.7
2016-09-07T04:52:00.431639: step 172, loss 0.527236, acc 0.68
2016-09-07T04:52:01.089322: step 173, loss 0.391556, acc 0.82
2016-09-07T04:52:01.792605: step 174, loss 0.564855, acc 0.8
2016-09-07T04:52:02.437309: step 175, loss 0.635051, acc 0.74
2016-09-07T04:52:03.119228: step 176, loss 0.697936, acc 0.6
2016-09-07T04:52:03.805226: step 177, loss 0.430762, acc 0.84
2016-09-07T04:52:04.472575: step 178, loss 0.440612, acc 0.76
2016-09-07T04:52:05.145124: step 179, loss 0.542841, acc 0.78
2016-09-07T04:52:05.823465: step 180, loss 0.417419, acc 0.82
2016-09-07T04:52:06.534954: step 181, loss 0.505097, acc 0.74
2016-09-07T04:52:07.190977: step 182, loss 0.722808, acc 0.46
2016-09-07T04:52:07.899922: step 183, loss 0.441937, acc 0.8
2016-09-07T04:52:08.603744: step 184, loss 0.466156, acc 0.82
2016-09-07T04:52:09.291877: step 185, loss 0.537614, acc 0.76
2016-09-07T04:52:09.984051: step 186, loss 0.537291, acc 0.7
2016-09-07T04:52:10.670191: step 187, loss 0.519441, acc 0.76
2016-09-07T04:52:11.370994: step 188, loss 0.436564, acc 0.84
2016-09-07T04:52:12.053563: step 189, loss 0.354819, acc 0.8
2016-09-07T04:52:12.728608: step 190, loss 0.401832, acc 0.82
2016-09-07T04:52:13.399003: step 191, loss 0.637079, acc 0.74
2016-09-07T04:52:14.074225: step 192, loss 0.402719, acc 0.840909
2016-09-07T04:52:14.762747: step 193, loss 0.383996, acc 0.86
2016-09-07T04:52:15.436504: step 194, loss 0.345742, acc 0.88
2016-09-07T04:52:16.147042: step 195, loss 0.393514, acc 0.82
2016-09-07T04:52:16.811067: step 196, loss 0.30651, acc 0.86
2016-09-07T04:52:17.503448: step 197, loss 0.465525, acc 0.78
2016-09-07T04:52:18.182601: step 198, loss 0.485123, acc 0.78
2016-09-07T04:52:18.874315: step 199, loss 0.310037, acc 0.86
2016-09-07T04:52:19.570346: step 200, loss 0.267883, acc 0.92

Evaluation:
2016-09-07T04:52:22.897370: step 200, loss 0.482775, acc 0.774859

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-200

2016-09-07T04:52:24.589944: step 201, loss 0.474522, acc 0.84
2016-09-07T04:52:25.268708: step 202, loss 0.487435, acc 0.8
2016-09-07T04:52:25.974018: step 203, loss 0.365639, acc 0.86
2016-09-07T04:52:26.664053: step 204, loss 0.407255, acc 0.82
2016-09-07T04:52:27.359020: step 205, loss 0.386744, acc 0.84
2016-09-07T04:52:28.053104: step 206, loss 0.372857, acc 0.84
2016-09-07T04:52:28.715990: step 207, loss 0.279036, acc 0.88
2016-09-07T04:52:29.398759: step 208, loss 0.378904, acc 0.84
2016-09-07T04:52:30.067631: step 209, loss 0.26715, acc 0.9
2016-09-07T04:52:30.746107: step 210, loss 0.302555, acc 0.86
2016-09-07T04:52:31.421498: step 211, loss 0.337091, acc 0.78
2016-09-07T04:52:32.094448: step 212, loss 0.252169, acc 0.92
2016-09-07T04:52:32.781368: step 213, loss 0.464623, acc 0.78
2016-09-07T04:52:33.467566: step 214, loss 0.342514, acc 0.82
2016-09-07T04:52:34.166816: step 215, loss 0.276983, acc 0.92
2016-09-07T04:52:34.829959: step 216, loss 0.200348, acc 0.94
2016-09-07T04:52:35.530900: step 217, loss 0.517771, acc 0.8
2016-09-07T04:52:36.217126: step 218, loss 0.414385, acc 0.78
2016-09-07T04:52:36.886859: step 219, loss 0.404858, acc 0.9
2016-09-07T04:52:37.582337: step 220, loss 0.315925, acc 0.84
2016-09-07T04:52:38.280069: step 221, loss 0.399477, acc 0.82
2016-09-07T04:52:38.947963: step 222, loss 0.291, acc 0.88
2016-09-07T04:52:39.607143: step 223, loss 0.325597, acc 0.84
2016-09-07T04:52:40.297345: step 224, loss 0.34466, acc 0.84
2016-09-07T04:52:40.986997: step 225, loss 0.292942, acc 0.84
2016-09-07T04:52:41.685159: step 226, loss 0.593397, acc 0.72
2016-09-07T04:52:42.380709: step 227, loss 0.387286, acc 0.8
2016-09-07T04:52:43.081156: step 228, loss 0.334502, acc 0.8
2016-09-07T04:52:43.785997: step 229, loss 0.345276, acc 0.84
2016-09-07T04:52:44.448065: step 230, loss 0.465057, acc 0.84
2016-09-07T04:52:45.164301: step 231, loss 0.487635, acc 0.7
2016-09-07T04:52:45.847673: step 232, loss 0.338464, acc 0.84
2016-09-07T04:52:46.543880: step 233, loss 0.520027, acc 0.78
2016-09-07T04:52:47.226117: step 234, loss 0.362587, acc 0.84
2016-09-07T04:52:47.911153: step 235, loss 0.324924, acc 0.86
2016-09-07T04:52:48.615788: step 236, loss 0.367222, acc 0.82
2016-09-07T04:52:49.341570: step 237, loss 0.442289, acc 0.78
2016-09-07T04:52:50.037130: step 238, loss 0.302708, acc 0.88
2016-09-07T04:52:50.751882: step 239, loss 0.27028, acc 0.9
2016-09-07T04:52:51.439954: step 240, loss 0.286636, acc 0.82
2016-09-07T04:52:52.134045: step 241, loss 0.344093, acc 0.82
2016-09-07T04:52:52.808603: step 242, loss 0.479519, acc 0.78
2016-09-07T04:52:53.521588: step 243, loss 0.320201, acc 0.8
2016-09-07T04:52:54.214710: step 244, loss 0.46789, acc 0.82
2016-09-07T04:52:54.911582: step 245, loss 0.340676, acc 0.84
2016-09-07T04:52:55.587059: step 246, loss 0.376772, acc 0.82
2016-09-07T04:52:56.265707: step 247, loss 0.539957, acc 0.74
2016-09-07T04:52:56.961928: step 248, loss 0.341951, acc 0.88
2016-09-07T04:52:57.615872: step 249, loss 0.367308, acc 0.86
2016-09-07T04:52:58.327163: step 250, loss 0.343248, acc 0.82
2016-09-07T04:52:59.026410: step 251, loss 0.287602, acc 0.9
2016-09-07T04:52:59.703169: step 252, loss 0.352484, acc 0.86
2016-09-07T04:53:00.428352: step 253, loss 0.310823, acc 0.82
2016-09-07T04:53:01.115365: step 254, loss 0.483046, acc 0.7
2016-09-07T04:53:01.855686: step 255, loss 0.273937, acc 0.88
2016-09-07T04:53:02.512822: step 256, loss 0.368039, acc 0.86
2016-09-07T04:53:03.192329: step 257, loss 0.411186, acc 0.78
2016-09-07T04:53:03.869791: step 258, loss 0.275071, acc 0.92
2016-09-07T04:53:04.554960: step 259, loss 0.349191, acc 0.8
2016-09-07T04:53:05.241027: step 260, loss 0.349144, acc 0.8
2016-09-07T04:53:05.923802: step 261, loss 0.259135, acc 0.88
2016-09-07T04:53:06.622465: step 262, loss 0.62279, acc 0.72
2016-09-07T04:53:07.308598: step 263, loss 0.388664, acc 0.8
2016-09-07T04:53:08.001098: step 264, loss 0.362379, acc 0.82
2016-09-07T04:53:08.677606: step 265, loss 0.553164, acc 0.74
2016-09-07T04:53:09.358015: step 266, loss 0.551909, acc 0.72
2016-09-07T04:53:10.037412: step 267, loss 0.456692, acc 0.82
2016-09-07T04:53:10.726856: step 268, loss 0.327095, acc 0.9
2016-09-07T04:53:11.408162: step 269, loss 0.266859, acc 0.9
2016-09-07T04:53:12.075360: step 270, loss 0.328838, acc 0.88
2016-09-07T04:53:12.800550: step 271, loss 0.401807, acc 0.82
2016-09-07T04:53:13.498445: step 272, loss 0.441934, acc 0.82
2016-09-07T04:53:14.189442: step 273, loss 0.40226, acc 0.82
2016-09-07T04:53:14.876131: step 274, loss 0.265975, acc 0.88
2016-09-07T04:53:15.556339: step 275, loss 0.372185, acc 0.82
2016-09-07T04:53:16.239815: step 276, loss 0.490609, acc 0.74
2016-09-07T04:53:16.892385: step 277, loss 0.301428, acc 0.88
2016-09-07T04:53:17.602254: step 278, loss 0.353302, acc 0.82
2016-09-07T04:53:18.284864: step 279, loss 0.35844, acc 0.8
2016-09-07T04:53:18.962821: step 280, loss 0.427773, acc 0.8
2016-09-07T04:53:19.629991: step 281, loss 0.340128, acc 0.88
2016-09-07T04:53:20.309730: step 282, loss 0.537079, acc 0.76
2016-09-07T04:53:20.993729: step 283, loss 0.436669, acc 0.76
2016-09-07T04:53:21.662978: step 284, loss 0.391383, acc 0.82
2016-09-07T04:53:22.360389: step 285, loss 0.372811, acc 0.84
2016-09-07T04:53:23.029776: step 286, loss 0.354465, acc 0.84
2016-09-07T04:53:23.725984: step 287, loss 0.346091, acc 0.82
2016-09-07T04:53:24.415729: step 288, loss 0.235576, acc 0.94
2016-09-07T04:53:25.102040: step 289, loss 0.539705, acc 0.82
2016-09-07T04:53:25.794619: step 290, loss 0.330049, acc 0.82
2016-09-07T04:53:26.504925: step 291, loss 0.371504, acc 0.82
2016-09-07T04:53:27.218605: step 292, loss 0.379904, acc 0.8
2016-09-07T04:53:27.909772: step 293, loss 0.24708, acc 0.92
2016-09-07T04:53:28.596821: step 294, loss 0.48637, acc 0.74
2016-09-07T04:53:29.274309: step 295, loss 0.470483, acc 0.82
2016-09-07T04:53:29.961526: step 296, loss 0.237683, acc 0.9
2016-09-07T04:53:30.657084: step 297, loss 0.436053, acc 0.86
2016-09-07T04:53:31.322457: step 298, loss 0.403958, acc 0.8
2016-09-07T04:53:32.041665: step 299, loss 0.434395, acc 0.82
2016-09-07T04:53:32.738014: step 300, loss 0.439313, acc 0.78

Evaluation:
2016-09-07T04:53:36.150812: step 300, loss 0.456622, acc 0.787992

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-300

2016-09-07T04:53:37.808429: step 301, loss 0.465952, acc 0.76
2016-09-07T04:53:38.479324: step 302, loss 0.324599, acc 0.82
2016-09-07T04:53:39.200858: step 303, loss 0.527673, acc 0.74
2016-09-07T04:53:39.873932: step 304, loss 0.437897, acc 0.78
2016-09-07T04:53:40.551988: step 305, loss 0.308526, acc 0.86
2016-09-07T04:53:41.257384: step 306, loss 0.358482, acc 0.88
2016-09-07T04:53:41.963482: step 307, loss 0.327307, acc 0.88
2016-09-07T04:53:42.664452: step 308, loss 0.420548, acc 0.76
2016-09-07T04:53:43.331888: step 309, loss 0.391458, acc 0.82
2016-09-07T04:53:44.029358: step 310, loss 0.398627, acc 0.8
2016-09-07T04:53:44.712449: step 311, loss 0.28259, acc 0.9
2016-09-07T04:53:45.392153: step 312, loss 0.435245, acc 0.8
2016-09-07T04:53:46.095462: step 313, loss 0.395259, acc 0.82
2016-09-07T04:53:46.782806: step 314, loss 0.357982, acc 0.86
2016-09-07T04:53:47.470129: step 315, loss 0.434183, acc 0.82
2016-09-07T04:53:48.142013: step 316, loss 0.327194, acc 0.84
2016-09-07T04:53:48.837667: step 317, loss 0.403344, acc 0.82
2016-09-07T04:53:49.514891: step 318, loss 0.245867, acc 0.92
2016-09-07T04:53:50.237800: step 319, loss 0.431275, acc 0.82
2016-09-07T04:53:50.910394: step 320, loss 0.349718, acc 0.84
2016-09-07T04:53:51.618015: step 321, loss 0.413088, acc 0.86
2016-09-07T04:53:52.331688: step 322, loss 0.399178, acc 0.82
2016-09-07T04:53:52.997798: step 323, loss 0.509947, acc 0.76
2016-09-07T04:53:53.707806: step 324, loss 0.442684, acc 0.82
2016-09-07T04:53:54.400937: step 325, loss 0.343029, acc 0.88
2016-09-07T04:53:55.093258: step 326, loss 0.37001, acc 0.76
2016-09-07T04:53:55.782226: step 327, loss 0.494414, acc 0.8
2016-09-07T04:53:56.461211: step 328, loss 0.223876, acc 0.88
2016-09-07T04:53:57.148222: step 329, loss 0.329089, acc 0.82
2016-09-07T04:53:57.826821: step 330, loss 0.270079, acc 0.9
2016-09-07T04:53:58.500579: step 331, loss 0.521898, acc 0.8
2016-09-07T04:53:59.195126: step 332, loss 0.437706, acc 0.74
2016-09-07T04:53:59.869676: step 333, loss 0.552445, acc 0.74
2016-09-07T04:54:00.566493: step 334, loss 0.386434, acc 0.78
2016-09-07T04:54:01.266150: step 335, loss 0.345713, acc 0.9
2016-09-07T04:54:01.959844: step 336, loss 0.268736, acc 0.86
2016-09-07T04:54:02.626612: step 337, loss 0.212213, acc 0.96
2016-09-07T04:54:03.337633: step 338, loss 0.391617, acc 0.8
2016-09-07T04:54:04.054720: step 339, loss 0.295313, acc 0.9
2016-09-07T04:54:04.751677: step 340, loss 0.386324, acc 0.82
2016-09-07T04:54:05.436024: step 341, loss 0.402841, acc 0.78
2016-09-07T04:54:06.132277: step 342, loss 0.352589, acc 0.82
2016-09-07T04:54:06.847166: step 343, loss 0.258889, acc 0.9
2016-09-07T04:54:07.523725: step 344, loss 0.326236, acc 0.78
2016-09-07T04:54:08.217620: step 345, loss 0.370575, acc 0.86
2016-09-07T04:54:08.879671: step 346, loss 0.306753, acc 0.84
2016-09-07T04:54:09.543011: step 347, loss 0.331596, acc 0.8
2016-09-07T04:54:10.231792: step 348, loss 0.383165, acc 0.86
2016-09-07T04:54:10.907685: step 349, loss 0.367291, acc 0.84
2016-09-07T04:54:11.617294: step 350, loss 0.442261, acc 0.76
2016-09-07T04:54:12.288825: step 351, loss 0.406655, acc 0.78
2016-09-07T04:54:12.988089: step 352, loss 0.144142, acc 1
2016-09-07T04:54:13.690213: step 353, loss 0.297998, acc 0.86
2016-09-07T04:54:14.367080: step 354, loss 0.30557, acc 0.88
2016-09-07T04:54:15.041095: step 355, loss 0.367534, acc 0.82
2016-09-07T04:54:15.719849: step 356, loss 0.332718, acc 0.86
2016-09-07T04:54:16.409868: step 357, loss 0.519078, acc 0.74
2016-09-07T04:54:17.076671: step 358, loss 0.246545, acc 0.92
2016-09-07T04:54:17.776230: step 359, loss 0.37672, acc 0.82
2016-09-07T04:54:18.454442: step 360, loss 0.208369, acc 0.94
2016-09-07T04:54:19.132972: step 361, loss 0.244017, acc 0.96
2016-09-07T04:54:19.825213: step 362, loss 0.47151, acc 0.8
2016-09-07T04:54:20.521364: step 363, loss 0.271566, acc 0.88
2016-09-07T04:54:21.245537: step 364, loss 0.309005, acc 0.82
2016-09-07T04:54:21.933284: step 365, loss 0.489109, acc 0.82
2016-09-07T04:54:22.617907: step 366, loss 0.668339, acc 0.7
2016-09-07T04:54:23.285581: step 367, loss 0.353957, acc 0.84
2016-09-07T04:54:23.967278: step 368, loss 0.260112, acc 0.86
2016-09-07T04:54:24.667048: step 369, loss 0.408256, acc 0.8
2016-09-07T04:54:25.379349: step 370, loss 0.277292, acc 0.84
2016-09-07T04:54:26.094398: step 371, loss 0.517868, acc 0.74
2016-09-07T04:54:26.761560: step 372, loss 0.369636, acc 0.8
2016-09-07T04:54:27.431459: step 373, loss 0.364508, acc 0.82
2016-09-07T04:54:28.139130: step 374, loss 0.474497, acc 0.8
2016-09-07T04:54:28.862379: step 375, loss 0.341972, acc 0.88
2016-09-07T04:54:29.544514: step 376, loss 0.42151, acc 0.78
2016-09-07T04:54:30.214130: step 377, loss 0.383338, acc 0.82
2016-09-07T04:54:30.920108: step 378, loss 0.438254, acc 0.82
2016-09-07T04:54:31.596966: step 379, loss 0.455409, acc 0.82
2016-09-07T04:54:32.287772: step 380, loss 0.339178, acc 0.84
2016-09-07T04:54:32.974513: step 381, loss 0.471844, acc 0.76
2016-09-07T04:54:33.639424: step 382, loss 0.308466, acc 0.9
2016-09-07T04:54:34.337420: step 383, loss 0.375682, acc 0.8
2016-09-07T04:54:34.971553: step 384, loss 0.347088, acc 0.818182
2016-09-07T04:54:35.710490: step 385, loss 0.144292, acc 0.96
2016-09-07T04:54:36.379169: step 386, loss 0.275868, acc 0.92
2016-09-07T04:54:37.066633: step 387, loss 0.31209, acc 0.88
2016-09-07T04:54:37.769570: step 388, loss 0.272377, acc 0.9
2016-09-07T04:54:38.458417: step 389, loss 0.239541, acc 0.9
2016-09-07T04:54:39.132649: step 390, loss 0.238013, acc 0.82
2016-09-07T04:54:39.810765: step 391, loss 0.224839, acc 0.96
2016-09-07T04:54:40.510670: step 392, loss 0.304926, acc 0.84
2016-09-07T04:54:41.195177: step 393, loss 0.111728, acc 0.94
2016-09-07T04:54:41.874277: step 394, loss 0.154851, acc 0.92
2016-09-07T04:54:42.569016: step 395, loss 0.29795, acc 0.84
2016-09-07T04:54:43.283626: step 396, loss 0.11996, acc 0.98
2016-09-07T04:54:43.982314: step 397, loss 0.320377, acc 0.84
2016-09-07T04:54:44.649784: step 398, loss 0.303051, acc 0.86
2016-09-07T04:54:45.361631: step 399, loss 0.245644, acc 0.9
2016-09-07T04:54:46.048101: step 400, loss 0.159941, acc 0.94

Evaluation:
2016-09-07T04:54:49.521762: step 400, loss 0.58094, acc 0.794559

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-400

2016-09-07T04:54:51.212685: step 401, loss 0.293549, acc 0.9
2016-09-07T04:54:51.872370: step 402, loss 0.460259, acc 0.84
2016-09-07T04:54:52.576192: step 403, loss 0.18541, acc 0.92
2016-09-07T04:54:53.265418: step 404, loss 0.250087, acc 0.88
2016-09-07T04:54:53.949526: step 405, loss 0.164589, acc 0.94
2016-09-07T04:54:54.640437: step 406, loss 0.173769, acc 0.92
2016-09-07T04:54:55.321143: step 407, loss 0.199245, acc 0.94
2016-09-07T04:54:56.019007: step 408, loss 0.473644, acc 0.8
2016-09-07T04:54:56.699091: step 409, loss 0.244198, acc 0.88
2016-09-07T04:54:57.405727: step 410, loss 0.168937, acc 0.9
2016-09-07T04:54:58.094586: step 411, loss 0.261257, acc 0.88
2016-09-07T04:54:58.781784: step 412, loss 0.302071, acc 0.86
2016-09-07T04:54:59.473537: step 413, loss 0.207775, acc 0.9
2016-09-07T04:55:00.154488: step 414, loss 0.19085, acc 0.96
2016-09-07T04:55:00.885564: step 415, loss 0.404062, acc 0.8
2016-09-07T04:55:01.550688: step 416, loss 0.29186, acc 0.84
2016-09-07T04:55:02.271002: step 417, loss 0.172581, acc 0.92
2016-09-07T04:55:02.971777: step 418, loss 0.29261, acc 0.82
2016-09-07T04:55:03.658057: step 419, loss 0.203552, acc 0.96
2016-09-07T04:55:04.335368: step 420, loss 0.188573, acc 0.94
2016-09-07T04:55:05.005963: step 421, loss 0.213108, acc 0.9
2016-09-07T04:55:05.705869: step 422, loss 0.144941, acc 0.94
2016-09-07T04:55:06.369826: step 423, loss 0.159099, acc 0.92
2016-09-07T04:55:07.068017: step 424, loss 0.227014, acc 0.92
2016-09-07T04:55:07.742510: step 425, loss 0.129598, acc 0.94
2016-09-07T04:55:08.418516: step 426, loss 0.213704, acc 0.9
2016-09-07T04:55:09.104412: step 427, loss 0.0811725, acc 0.98
2016-09-07T04:55:09.795345: step 428, loss 0.047484, acc 1
2016-09-07T04:55:10.490821: step 429, loss 0.177339, acc 0.94
2016-09-07T04:55:11.146309: step 430, loss 0.304563, acc 0.9
2016-09-07T04:55:11.835455: step 431, loss 0.189751, acc 0.9
2016-09-07T04:55:12.531900: step 432, loss 0.213545, acc 0.92
2016-09-07T04:55:13.221866: step 433, loss 0.170061, acc 0.94
2016-09-07T04:55:13.900501: step 434, loss 0.156701, acc 0.9
2016-09-07T04:55:14.578576: step 435, loss 0.284077, acc 0.86
2016-09-07T04:55:15.268602: step 436, loss 0.270689, acc 0.84
2016-09-07T04:55:15.932687: step 437, loss 0.0866182, acc 0.96
2016-09-07T04:55:16.639764: step 438, loss 0.249741, acc 0.86
2016-09-07T04:55:17.300278: step 439, loss 0.364774, acc 0.82
2016-09-07T04:55:17.970526: step 440, loss 0.145253, acc 0.92
2016-09-07T04:55:18.670666: step 441, loss 0.154631, acc 0.92
2016-09-07T04:55:19.370268: step 442, loss 0.282755, acc 0.9
2016-09-07T04:55:20.070483: step 443, loss 0.275449, acc 0.86
2016-09-07T04:55:20.745815: step 444, loss 0.218537, acc 0.92
2016-09-07T04:55:21.441875: step 445, loss 0.258318, acc 0.88
2016-09-07T04:55:22.119493: step 446, loss 0.23386, acc 0.9
2016-09-07T04:55:22.782587: step 447, loss 0.233969, acc 0.9
2016-09-07T04:55:23.465682: step 448, loss 0.394109, acc 0.82
2016-09-07T04:55:24.163390: step 449, loss 0.257933, acc 0.82
2016-09-07T04:55:24.858541: step 450, loss 0.242193, acc 0.92
2016-09-07T04:55:25.525033: step 451, loss 0.223816, acc 0.9
2016-09-07T04:55:26.242379: step 452, loss 0.275817, acc 0.84
2016-09-07T04:55:26.912030: step 453, loss 0.2282, acc 0.88
2016-09-07T04:55:27.605804: step 454, loss 0.277191, acc 0.9
2016-09-07T04:55:28.286350: step 455, loss 0.228635, acc 0.94
2016-09-07T04:55:29.003970: step 456, loss 0.269568, acc 0.86
2016-09-07T04:55:29.688401: step 457, loss 0.266479, acc 0.92
2016-09-07T04:55:30.337715: step 458, loss 0.355204, acc 0.9
2016-09-07T04:55:31.052378: step 459, loss 0.24922, acc 0.9
2016-09-07T04:55:31.703838: step 460, loss 0.263079, acc 0.88
2016-09-07T04:55:32.374934: step 461, loss 0.295517, acc 0.92
2016-09-07T04:55:33.073489: step 462, loss 0.22883, acc 0.92
2016-09-07T04:55:33.770538: step 463, loss 0.344939, acc 0.84
2016-09-07T04:55:34.449380: step 464, loss 0.296835, acc 0.88
2016-09-07T04:55:35.114105: step 465, loss 0.32158, acc 0.86
2016-09-07T04:55:35.816600: step 466, loss 0.213882, acc 0.9
2016-09-07T04:55:36.483575: step 467, loss 0.289869, acc 0.86
2016-09-07T04:55:37.155853: step 468, loss 0.244948, acc 0.88
2016-09-07T04:55:37.837450: step 469, loss 0.302401, acc 0.86
2016-09-07T04:55:38.520953: step 470, loss 0.302264, acc 0.84
2016-09-07T04:55:39.206737: step 471, loss 0.136143, acc 0.94
2016-09-07T04:55:39.889587: step 472, loss 0.395356, acc 0.78
2016-09-07T04:55:40.601793: step 473, loss 0.199912, acc 0.88
2016-09-07T04:55:41.264674: step 474, loss 0.248095, acc 0.9
2016-09-07T04:55:41.974016: step 475, loss 0.191849, acc 0.88
2016-09-07T04:55:42.661773: step 476, loss 0.195689, acc 0.94
2016-09-07T04:55:43.343673: step 477, loss 0.139134, acc 0.94
2016-09-07T04:55:44.006836: step 478, loss 0.135992, acc 0.94
2016-09-07T04:55:44.678904: step 479, loss 0.182622, acc 0.96
2016-09-07T04:55:45.376695: step 480, loss 0.156442, acc 0.96
2016-09-07T04:55:46.045053: step 481, loss 0.194397, acc 0.9
2016-09-07T04:55:46.744429: step 482, loss 0.301527, acc 0.88
2016-09-07T04:55:47.454778: step 483, loss 0.241776, acc 0.88
2016-09-07T04:55:48.142019: step 484, loss 0.333201, acc 0.9
2016-09-07T04:55:48.825995: step 485, loss 0.352387, acc 0.84
2016-09-07T04:55:49.530257: step 486, loss 0.184966, acc 0.92
2016-09-07T04:55:50.231704: step 487, loss 0.202726, acc 0.94
2016-09-07T04:55:50.910719: step 488, loss 0.364257, acc 0.9
2016-09-07T04:55:51.568211: step 489, loss 0.216683, acc 0.88
2016-09-07T04:55:52.267339: step 490, loss 0.421759, acc 0.84
2016-09-07T04:55:52.950506: step 491, loss 0.111724, acc 0.96
2016-09-07T04:55:53.636635: step 492, loss 0.298525, acc 0.88
2016-09-07T04:55:54.298425: step 493, loss 0.410998, acc 0.84
2016-09-07T04:55:55.005848: step 494, loss 0.330392, acc 0.86
2016-09-07T04:55:55.661014: step 495, loss 0.241908, acc 0.86
2016-09-07T04:55:56.353426: step 496, loss 0.250896, acc 0.92
2016-09-07T04:55:57.026111: step 497, loss 0.228181, acc 0.9
2016-09-07T04:55:57.712223: step 498, loss 0.386799, acc 0.84
2016-09-07T04:55:58.410839: step 499, loss 0.274103, acc 0.9
2016-09-07T04:55:59.098924: step 500, loss 0.352144, acc 0.84

Evaluation:
2016-09-07T04:56:02.667273: step 500, loss 0.482255, acc 0.783302

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-500

2016-09-07T04:56:04.425322: step 501, loss 0.273115, acc 0.9
2016-09-07T04:56:05.113296: step 502, loss 0.373653, acc 0.88
2016-09-07T04:56:05.790613: step 503, loss 0.41688, acc 0.88
2016-09-07T04:56:06.458349: step 504, loss 0.228484, acc 0.92
2016-09-07T04:56:07.150159: step 505, loss 0.318865, acc 0.88
2016-09-07T04:56:07.822579: step 506, loss 0.246831, acc 0.9
2016-09-07T04:56:08.498230: step 507, loss 0.171864, acc 0.92
2016-09-07T04:56:09.191869: step 508, loss 0.296441, acc 0.9
2016-09-07T04:56:09.875521: step 509, loss 0.232167, acc 0.88
2016-09-07T04:56:10.563810: step 510, loss 0.214871, acc 0.92
2016-09-07T04:56:11.253589: step 511, loss 0.321163, acc 0.84
2016-09-07T04:56:11.979467: step 512, loss 0.288618, acc 0.9
2016-09-07T04:56:12.671396: step 513, loss 0.266568, acc 0.9
2016-09-07T04:56:13.348935: step 514, loss 0.206407, acc 0.88
2016-09-07T04:56:14.040811: step 515, loss 0.234074, acc 0.88
2016-09-07T04:56:14.730287: step 516, loss 0.164266, acc 0.96
2016-09-07T04:56:15.419735: step 517, loss 0.131882, acc 0.96
2016-09-07T04:56:16.092770: step 518, loss 0.171649, acc 0.9
2016-09-07T04:56:16.813100: step 519, loss 0.314333, acc 0.9
2016-09-07T04:56:17.494105: step 520, loss 0.222747, acc 0.86
2016-09-07T04:56:18.172650: step 521, loss 0.122857, acc 0.94
2016-09-07T04:56:18.860443: step 522, loss 0.323765, acc 0.88
2016-09-07T04:56:19.556197: step 523, loss 0.25507, acc 0.92
2016-09-07T04:56:20.277347: step 524, loss 0.210697, acc 0.88
2016-09-07T04:56:20.936464: step 525, loss 0.690205, acc 0.7
2016-09-07T04:56:21.637128: step 526, loss 0.320529, acc 0.86
2016-09-07T04:56:22.320970: step 527, loss 0.437753, acc 0.74
2016-09-07T04:56:23.017778: step 528, loss 0.260231, acc 0.84
2016-09-07T04:56:23.705530: step 529, loss 0.294092, acc 0.88
2016-09-07T04:56:24.393637: step 530, loss 0.219803, acc 0.88
2016-09-07T04:56:25.097375: step 531, loss 0.252765, acc 0.88
2016-09-07T04:56:25.773743: step 532, loss 0.184508, acc 0.92
2016-09-07T04:56:26.453082: step 533, loss 0.2433, acc 0.88
2016-09-07T04:56:27.142030: step 534, loss 0.400313, acc 0.82
2016-09-07T04:56:27.818946: step 535, loss 0.130444, acc 0.96
2016-09-07T04:56:28.506758: step 536, loss 0.230061, acc 0.9
2016-09-07T04:56:29.199330: step 537, loss 0.206442, acc 0.9
2016-09-07T04:56:29.905288: step 538, loss 0.270794, acc 0.84
2016-09-07T04:56:30.586441: step 539, loss 0.360611, acc 0.88
2016-09-07T04:56:31.265277: step 540, loss 0.167137, acc 0.92
2016-09-07T04:56:31.953799: step 541, loss 0.271841, acc 0.86
2016-09-07T04:56:32.634148: step 542, loss 0.263844, acc 0.86
2016-09-07T04:56:33.332068: step 543, loss 0.271536, acc 0.88
2016-09-07T04:56:34.030924: step 544, loss 0.294393, acc 0.94
2016-09-07T04:56:34.733041: step 545, loss 0.333852, acc 0.9
2016-09-07T04:56:35.411091: step 546, loss 0.229438, acc 0.9
2016-09-07T04:56:36.103733: step 547, loss 0.212685, acc 0.86
2016-09-07T04:56:36.795443: step 548, loss 0.234846, acc 0.88
2016-09-07T04:56:37.472820: step 549, loss 0.235094, acc 0.92
2016-09-07T04:56:38.144551: step 550, loss 0.158417, acc 0.92
2016-09-07T04:56:38.814618: step 551, loss 0.342619, acc 0.9
2016-09-07T04:56:39.522344: step 552, loss 0.213454, acc 0.88
2016-09-07T04:56:40.200597: step 553, loss 0.383907, acc 0.82
2016-09-07T04:56:40.893839: step 554, loss 0.343469, acc 0.84
2016-09-07T04:56:41.575344: step 555, loss 0.179594, acc 0.9
2016-09-07T04:56:42.259692: step 556, loss 0.359795, acc 0.82
2016-09-07T04:56:42.943090: step 557, loss 0.233852, acc 0.92
2016-09-07T04:56:43.613779: step 558, loss 0.316415, acc 0.84
2016-09-07T04:56:44.323136: step 559, loss 0.116591, acc 0.98
2016-09-07T04:56:45.002643: step 560, loss 0.288352, acc 0.88
2016-09-07T04:56:45.690032: step 561, loss 0.206564, acc 0.9
2016-09-07T04:56:46.380393: step 562, loss 0.399288, acc 0.84
2016-09-07T04:56:47.059657: step 563, loss 0.193865, acc 0.94
2016-09-07T04:56:47.774225: step 564, loss 0.285886, acc 0.82
2016-09-07T04:56:48.474757: step 565, loss 0.283328, acc 0.88
2016-09-07T04:56:49.173162: step 566, loss 0.142877, acc 0.92
2016-09-07T04:56:49.867400: step 567, loss 0.203613, acc 0.88
2016-09-07T04:56:50.550353: step 568, loss 0.196079, acc 0.92
2016-09-07T04:56:51.242352: step 569, loss 0.30226, acc 0.86
2016-09-07T04:56:51.932425: step 570, loss 0.297201, acc 0.86
2016-09-07T04:56:52.604846: step 571, loss 0.341073, acc 0.88
2016-09-07T04:56:53.286088: step 572, loss 0.239841, acc 0.9
2016-09-07T04:56:53.975567: step 573, loss 0.254198, acc 0.94
2016-09-07T04:56:54.655500: step 574, loss 0.213314, acc 0.92
2016-09-07T04:56:55.348400: step 575, loss 0.230767, acc 0.86
2016-09-07T04:56:55.990620: step 576, loss 0.265491, acc 0.886364
2016-09-07T04:56:56.673879: step 577, loss 0.230784, acc 0.88
2016-09-07T04:56:57.395981: step 578, loss 0.166436, acc 0.94
2016-09-07T04:56:58.101478: step 579, loss 0.208532, acc 0.9
2016-09-07T04:56:58.817834: step 580, loss 0.222545, acc 0.88
2016-09-07T04:56:59.520426: step 581, loss 0.176529, acc 0.92
2016-09-07T04:57:00.233553: step 582, loss 0.206879, acc 0.9
2016-09-07T04:57:00.911729: step 583, loss 0.137216, acc 0.96
2016-09-07T04:57:01.570860: step 584, loss 0.14228, acc 0.94
2016-09-07T04:57:02.254154: step 585, loss 0.129265, acc 0.94
2016-09-07T04:57:02.902573: step 586, loss 0.181167, acc 0.92
2016-09-07T04:57:03.597563: step 587, loss 0.078573, acc 0.98
2016-09-07T04:57:04.268027: step 588, loss 0.279263, acc 0.88
2016-09-07T04:57:04.936861: step 589, loss 0.109611, acc 0.94
2016-09-07T04:57:05.613112: step 590, loss 0.0669208, acc 0.98
2016-09-07T04:57:06.293072: step 591, loss 0.127025, acc 0.94
2016-09-07T04:57:06.988698: step 592, loss 0.237552, acc 0.92
2016-09-07T04:57:07.688989: step 593, loss 0.231161, acc 0.9
2016-09-07T04:57:08.410947: step 594, loss 0.110432, acc 0.94
2016-09-07T04:57:09.098287: step 595, loss 0.058429, acc 0.98
2016-09-07T04:57:09.768272: step 596, loss 0.18178, acc 0.96
2016-09-07T04:57:10.447632: step 597, loss 0.102187, acc 0.96
2016-09-07T04:57:11.144917: step 598, loss 0.115183, acc 0.96
2016-09-07T04:57:11.832179: step 599, loss 0.119235, acc 0.92
2016-09-07T04:57:12.516028: step 600, loss 0.198181, acc 0.96

Evaluation:
2016-09-07T04:57:16.043102: step 600, loss 0.620999, acc 0.794559

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-600

2016-09-07T04:57:17.770264: step 601, loss 0.233452, acc 0.9
2016-09-07T04:57:18.459772: step 602, loss 0.142623, acc 0.96
2016-09-07T04:57:19.145329: step 603, loss 0.0456112, acc 1
2016-09-07T04:57:19.821420: step 604, loss 0.0616078, acc 0.98
2016-09-07T04:57:20.536035: step 605, loss 0.0817173, acc 0.96
2016-09-07T04:57:21.206897: step 606, loss 0.162054, acc 0.92
2016-09-07T04:57:21.885027: step 607, loss 0.167279, acc 0.94
2016-09-07T04:57:22.553334: step 608, loss 0.133015, acc 0.92
2016-09-07T04:57:23.234115: step 609, loss 0.260083, acc 0.88
2016-09-07T04:57:23.912144: step 610, loss 0.317978, acc 0.88
2016-09-07T04:57:24.571001: step 611, loss 0.0769759, acc 0.96
2016-09-07T04:57:25.278731: step 612, loss 0.167715, acc 0.94
2016-09-07T04:57:25.947917: step 613, loss 0.103771, acc 0.94
2016-09-07T04:57:26.606161: step 614, loss 0.230214, acc 0.9
2016-09-07T04:57:27.298279: step 615, loss 0.145412, acc 0.92
2016-09-07T04:57:27.971765: step 616, loss 0.12738, acc 0.96
2016-09-07T04:57:28.661479: step 617, loss 0.285764, acc 0.86
2016-09-07T04:57:29.330892: step 618, loss 0.169842, acc 0.96
2016-09-07T04:57:30.023193: step 619, loss 0.163248, acc 0.94
2016-09-07T04:57:30.693454: step 620, loss 0.112377, acc 0.96
2016-09-07T04:57:31.380875: step 621, loss 0.240423, acc 0.86
2016-09-07T04:57:32.057795: step 622, loss 0.248479, acc 0.88
2016-09-07T04:57:32.754074: step 623, loss 0.229707, acc 0.92
2016-09-07T04:57:33.440864: step 624, loss 0.148265, acc 0.94
2016-09-07T04:57:34.134211: step 625, loss 0.139149, acc 0.92
2016-09-07T04:57:34.837538: step 626, loss 0.257495, acc 0.86
2016-09-07T04:57:35.512456: step 627, loss 0.263871, acc 0.84
2016-09-07T04:57:36.200475: step 628, loss 0.132286, acc 0.96
2016-09-07T04:57:36.886653: step 629, loss 0.0841532, acc 0.98
2016-09-07T04:57:37.548673: step 630, loss 0.160616, acc 0.96
2016-09-07T04:57:38.240229: step 631, loss 0.175443, acc 0.92
2016-09-07T04:57:38.915055: step 632, loss 0.187302, acc 0.9
2016-09-07T04:57:39.629765: step 633, loss 0.0750517, acc 0.98
2016-09-07T04:57:40.291464: step 634, loss 0.257838, acc 0.88
2016-09-07T04:57:41.005359: step 635, loss 0.205597, acc 0.88
2016-09-07T04:57:41.690534: step 636, loss 0.0977051, acc 0.96
2016-09-07T04:57:42.367675: step 637, loss 0.285382, acc 0.88
2016-09-07T04:57:43.041046: step 638, loss 0.115285, acc 0.94
2016-09-07T04:57:43.710732: step 639, loss 0.279555, acc 0.88
2016-09-07T04:57:44.402887: step 640, loss 0.134074, acc 0.94
2016-09-07T04:57:45.059350: step 641, loss 0.234629, acc 0.92
2016-09-07T04:57:45.767931: step 642, loss 0.280477, acc 0.9
2016-09-07T04:57:46.467217: step 643, loss 0.206137, acc 0.94
2016-09-07T04:57:47.145827: step 644, loss 0.0656663, acc 0.98
2016-09-07T04:57:47.833760: step 645, loss 0.0867579, acc 0.96
2016-09-07T04:57:48.528806: step 646, loss 0.269873, acc 0.9
2016-09-07T04:57:49.208297: step 647, loss 0.339034, acc 0.9
2016-09-07T04:57:49.886081: step 648, loss 0.209979, acc 0.92
2016-09-07T04:57:50.583375: step 649, loss 0.0449006, acc 1
2016-09-07T04:57:51.263624: step 650, loss 0.205237, acc 0.9
2016-09-07T04:57:51.955784: step 651, loss 0.160322, acc 0.9
2016-09-07T04:57:52.637756: step 652, loss 0.125583, acc 0.94
2016-09-07T04:57:53.329382: step 653, loss 0.22806, acc 0.9
2016-09-07T04:57:54.033789: step 654, loss 0.17785, acc 0.9
2016-09-07T04:57:54.688510: step 655, loss 0.162943, acc 0.94
2016-09-07T04:57:55.383561: step 656, loss 0.202835, acc 0.94
2016-09-07T04:57:56.072972: step 657, loss 0.19695, acc 0.92
2016-09-07T04:57:56.755108: step 658, loss 0.26641, acc 0.9
2016-09-07T04:57:57.437161: step 659, loss 0.228872, acc 0.88
2016-09-07T04:57:58.140092: step 660, loss 0.192541, acc 0.88
2016-09-07T04:57:58.832182: step 661, loss 0.105748, acc 0.96
2016-09-07T04:57:59.519557: step 662, loss 0.207659, acc 0.94
2016-09-07T04:58:00.243288: step 663, loss 0.148471, acc 0.9
2016-09-07T04:58:00.943515: step 664, loss 0.226993, acc 0.9
2016-09-07T04:58:01.623791: step 665, loss 0.172404, acc 0.96
2016-09-07T04:58:02.302234: step 666, loss 0.133902, acc 0.96
2016-09-07T04:58:02.985736: step 667, loss 0.160798, acc 0.9
2016-09-07T04:58:03.677785: step 668, loss 0.141478, acc 0.92
2016-09-07T04:58:04.353861: step 669, loss 0.127013, acc 0.94
2016-09-07T04:58:05.046331: step 670, loss 0.128809, acc 0.96
2016-09-07T04:58:05.740757: step 671, loss 0.143149, acc 0.92
2016-09-07T04:58:06.448109: step 672, loss 0.090312, acc 0.98
2016-09-07T04:58:07.128351: step 673, loss 0.185576, acc 0.92
2016-09-07T04:58:07.810678: step 674, loss 0.267853, acc 0.88
2016-09-07T04:58:08.514491: step 675, loss 0.26563, acc 0.88
2016-09-07T04:58:09.183700: step 676, loss 0.243305, acc 0.9
2016-09-07T04:58:09.866628: step 677, loss 0.079239, acc 0.96
2016-09-07T04:58:10.556431: step 678, loss 0.241004, acc 0.9
2016-09-07T04:58:11.234811: step 679, loss 0.216736, acc 0.88
2016-09-07T04:58:11.936849: step 680, loss 0.235229, acc 0.84
2016-09-07T04:58:12.639196: step 681, loss 0.169633, acc 0.94
2016-09-07T04:58:13.345374: step 682, loss 0.147209, acc 0.96
2016-09-07T04:58:14.013716: step 683, loss 0.109548, acc 0.94
2016-09-07T04:58:14.700148: step 684, loss 0.258978, acc 0.88
2016-09-07T04:58:15.397379: step 685, loss 0.212842, acc 0.9
2016-09-07T04:58:16.090077: step 686, loss 0.21995, acc 0.92
2016-09-07T04:58:16.772088: step 687, loss 0.192616, acc 0.9
2016-09-07T04:58:17.455796: step 688, loss 0.0776875, acc 0.96
2016-09-07T04:58:18.149351: step 689, loss 0.140285, acc 0.92
2016-09-07T04:58:18.830449: step 690, loss 0.18696, acc 0.9
2016-09-07T04:58:19.507697: step 691, loss 0.392964, acc 0.88
2016-09-07T04:58:20.196027: step 692, loss 0.272894, acc 0.94
2016-09-07T04:58:20.882881: step 693, loss 0.161954, acc 0.94
2016-09-07T04:58:21.580494: step 694, loss 0.146454, acc 0.94
2016-09-07T04:58:22.258709: step 695, loss 0.15564, acc 0.94
2016-09-07T04:58:22.977280: step 696, loss 0.167039, acc 0.92
2016-09-07T04:58:23.657145: step 697, loss 0.133625, acc 0.98
2016-09-07T04:58:24.342243: step 698, loss 0.285845, acc 0.88
2016-09-07T04:58:25.042908: step 699, loss 0.0493502, acc 1
2016-09-07T04:58:25.710110: step 700, loss 0.0918903, acc 0.96

Evaluation:
2016-09-07T04:58:29.244738: step 700, loss 0.693298, acc 0.767355

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-700

2016-09-07T04:58:31.004620: step 701, loss 0.225049, acc 0.92
2016-09-07T04:58:31.697276: step 702, loss 0.138086, acc 0.88
2016-09-07T04:58:32.389216: step 703, loss 0.224032, acc 0.86
2016-09-07T04:58:33.096742: step 704, loss 0.0570987, acc 1
2016-09-07T04:58:33.830201: step 705, loss 0.290412, acc 0.9
2016-09-07T04:58:34.488919: step 706, loss 0.256587, acc 0.86
2016-09-07T04:58:35.173379: step 707, loss 0.144604, acc 0.9
2016-09-07T04:58:35.869087: step 708, loss 0.198563, acc 0.92
2016-09-07T04:58:36.533290: step 709, loss 0.127979, acc 0.92
2016-09-07T04:58:37.218978: step 710, loss 0.152037, acc 0.94
2016-09-07T04:58:37.884185: step 711, loss 0.108954, acc 0.94
2016-09-07T04:58:38.592356: step 712, loss 0.160232, acc 0.96
2016-09-07T04:58:39.254736: step 713, loss 0.270715, acc 0.9
2016-09-07T04:58:39.926096: step 714, loss 0.125353, acc 0.92
2016-09-07T04:58:40.613198: step 715, loss 0.297698, acc 0.86
2016-09-07T04:58:41.310122: step 716, loss 0.139751, acc 0.94
2016-09-07T04:58:42.022874: step 717, loss 0.29224, acc 0.88
2016-09-07T04:58:42.715391: step 718, loss 0.200212, acc 0.88
2016-09-07T04:58:43.471553: step 719, loss 0.138379, acc 0.9
2016-09-07T04:58:44.214250: step 720, loss 0.0771128, acc 0.98
2016-09-07T04:58:44.900506: step 721, loss 0.0884721, acc 0.96
2016-09-07T04:58:45.579950: step 722, loss 0.211949, acc 0.9
2016-09-07T04:58:46.279725: step 723, loss 0.209377, acc 0.9
2016-09-07T04:58:46.985712: step 724, loss 0.196756, acc 0.92
2016-09-07T04:58:47.681968: step 725, loss 0.233256, acc 0.88
2016-09-07T04:58:48.367164: step 726, loss 0.156741, acc 0.92
2016-09-07T04:58:49.056831: step 727, loss 0.270652, acc 0.88
2016-09-07T04:58:49.737828: step 728, loss 0.197535, acc 0.94
2016-09-07T04:58:50.425929: step 729, loss 0.2454, acc 0.88
2016-09-07T04:58:51.124727: step 730, loss 0.174645, acc 0.9
2016-09-07T04:58:51.827072: step 731, loss 0.112833, acc 0.96
2016-09-07T04:58:52.526466: step 732, loss 0.180146, acc 0.9
2016-09-07T04:58:53.221679: step 733, loss 0.10981, acc 0.96
2016-09-07T04:58:53.915608: step 734, loss 0.167054, acc 0.88
2016-09-07T04:58:54.606232: step 735, loss 0.155101, acc 0.94
2016-09-07T04:58:55.287512: step 736, loss 0.105043, acc 0.98
2016-09-07T04:58:55.954380: step 737, loss 0.0948451, acc 0.96
2016-09-07T04:58:56.651728: step 738, loss 0.165897, acc 0.96
2016-09-07T04:58:57.305442: step 739, loss 0.157006, acc 0.96
2016-09-07T04:58:57.996922: step 740, loss 0.249878, acc 0.88
2016-09-07T04:58:58.683767: step 741, loss 0.159403, acc 0.92
2016-09-07T04:58:59.363409: step 742, loss 0.201495, acc 0.92
2016-09-07T04:59:00.053475: step 743, loss 0.261816, acc 0.84
2016-09-07T04:59:00.782892: step 744, loss 0.0876074, acc 0.96
2016-09-07T04:59:01.502440: step 745, loss 0.253063, acc 0.92
2016-09-07T04:59:02.174871: step 746, loss 0.168906, acc 0.92
2016-09-07T04:59:02.866434: step 747, loss 0.349849, acc 0.94
2016-09-07T04:59:03.546101: step 748, loss 0.277981, acc 0.84
2016-09-07T04:59:04.240163: step 749, loss 0.131893, acc 0.94
2016-09-07T04:59:04.945879: step 750, loss 0.182534, acc 0.96
2016-09-07T04:59:05.620289: step 751, loss 0.18439, acc 0.9
2016-09-07T04:59:06.314119: step 752, loss 0.257282, acc 0.88
2016-09-07T04:59:07.012755: step 753, loss 0.234019, acc 0.86
2016-09-07T04:59:07.690076: step 754, loss 0.247823, acc 0.84
2016-09-07T04:59:08.356289: step 755, loss 0.130674, acc 0.96
2016-09-07T04:59:09.028060: step 756, loss 0.079052, acc 0.98
2016-09-07T04:59:09.717718: step 757, loss 0.190912, acc 0.94
2016-09-07T04:59:10.423062: step 758, loss 0.208899, acc 0.96
2016-09-07T04:59:11.122205: step 759, loss 0.167052, acc 0.94
2016-09-07T04:59:11.822342: step 760, loss 0.105231, acc 0.96
2016-09-07T04:59:12.513180: step 761, loss 0.133021, acc 0.92
2016-09-07T04:59:13.192449: step 762, loss 0.238028, acc 0.9
2016-09-07T04:59:13.887838: step 763, loss 0.16406, acc 0.92
2016-09-07T04:59:14.580699: step 764, loss 0.187548, acc 0.9
2016-09-07T04:59:15.250226: step 765, loss 0.137041, acc 0.94
2016-09-07T04:59:15.958418: step 766, loss 0.0822166, acc 0.96
2016-09-07T04:59:16.646805: step 767, loss 0.20778, acc 0.88
2016-09-07T04:59:17.297897: step 768, loss 0.10508, acc 0.954545
2016-09-07T04:59:18.014540: step 769, loss 0.063272, acc 0.98
2016-09-07T04:59:18.720491: step 770, loss 0.106855, acc 0.96
2016-09-07T04:59:19.452699: step 771, loss 0.0959, acc 0.94
2016-09-07T04:59:20.121606: step 772, loss 0.101663, acc 0.96
2016-09-07T04:59:20.813500: step 773, loss 0.0761538, acc 0.98
2016-09-07T04:59:21.505477: step 774, loss 0.139558, acc 0.92
2016-09-07T04:59:22.185698: step 775, loss 0.0302086, acc 1
2016-09-07T04:59:22.865708: step 776, loss 0.109146, acc 0.98
2016-09-07T04:59:23.542546: step 777, loss 0.0881496, acc 0.96
2016-09-07T04:59:24.252771: step 778, loss 0.0995038, acc 0.96
2016-09-07T04:59:24.916502: step 779, loss 0.054549, acc 0.98
2016-09-07T04:59:25.606910: step 780, loss 0.0946678, acc 0.96
2016-09-07T04:59:26.286178: step 781, loss 0.0648409, acc 0.98
2016-09-07T04:59:26.987863: step 782, loss 0.155285, acc 0.92
2016-09-07T04:59:27.664866: step 783, loss 0.122508, acc 0.94
2016-09-07T04:59:28.359651: step 784, loss 0.162777, acc 0.94
2016-09-07T04:59:29.088539: step 785, loss 0.391471, acc 0.88
2016-09-07T04:59:29.780130: step 786, loss 0.0508385, acc 0.98
2016-09-07T04:59:30.466900: step 787, loss 0.0465837, acc 0.98
2016-09-07T04:59:31.168939: step 788, loss 0.174289, acc 0.92
2016-09-07T04:59:31.860814: step 789, loss 0.321273, acc 0.94
2016-09-07T04:59:32.552827: step 790, loss 0.100977, acc 0.96
2016-09-07T04:59:33.225509: step 791, loss 0.1609, acc 0.88
2016-09-07T04:59:33.926958: step 792, loss 0.0739855, acc 0.96
2016-09-07T04:59:34.619789: step 793, loss 0.100475, acc 0.94
2016-09-07T04:59:35.296423: step 794, loss 0.127722, acc 0.96
2016-09-07T04:59:35.976767: step 795, loss 0.176343, acc 0.94
2016-09-07T04:59:36.680503: step 796, loss 0.057076, acc 1
2016-09-07T04:59:37.394683: step 797, loss 0.143648, acc 0.96
2016-09-07T04:59:38.066262: step 798, loss 0.28383, acc 0.9
2016-09-07T04:59:38.770419: step 799, loss 0.13871, acc 0.96
2016-09-07T04:59:39.475007: step 800, loss 0.160036, acc 0.96

Evaluation:
2016-09-07T04:59:43.046347: step 800, loss 0.605664, acc 0.783302

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-800

2016-09-07T04:59:44.861355: step 801, loss 0.198203, acc 0.94
2016-09-07T04:59:45.555943: step 802, loss 0.244085, acc 0.9
2016-09-07T04:59:46.260611: step 803, loss 0.0925163, acc 0.98
2016-09-07T04:59:46.955414: step 804, loss 0.140715, acc 0.96
2016-09-07T04:59:47.652085: step 805, loss 0.0648078, acc 0.96
2016-09-07T04:59:48.350153: step 806, loss 0.118228, acc 0.94
2016-09-07T04:59:49.049232: step 807, loss 0.0758999, acc 0.96
2016-09-07T04:59:49.714582: step 808, loss 0.0500605, acc 0.98
2016-09-07T04:59:50.396260: step 809, loss 0.146504, acc 0.94
2016-09-07T04:59:51.103932: step 810, loss 0.0662949, acc 1
2016-09-07T04:59:51.802618: step 811, loss 0.024652, acc 0.98
2016-09-07T04:59:52.477038: step 812, loss 0.0944538, acc 0.96
2016-09-07T04:59:53.194243: step 813, loss 0.165739, acc 0.96
2016-09-07T04:59:53.895372: step 814, loss 0.122597, acc 0.96
2016-09-07T04:59:54.583637: step 815, loss 0.10715, acc 0.96
2016-09-07T04:59:55.286979: step 816, loss 0.0962941, acc 0.96
2016-09-07T04:59:55.977519: step 817, loss 0.0567788, acc 0.98
2016-09-07T04:59:56.674596: step 818, loss 0.14366, acc 0.94
2016-09-07T04:59:57.348445: step 819, loss 0.111797, acc 0.96
2016-09-07T04:59:58.044054: step 820, loss 0.173355, acc 0.92
2016-09-07T04:59:58.731781: step 821, loss 0.120424, acc 0.94
2016-09-07T04:59:59.416009: step 822, loss 0.133546, acc 0.92
2016-09-07T05:00:00.110088: step 823, loss 0.0654535, acc 0.98
2016-09-07T05:00:00.844163: step 824, loss 0.0657006, acc 0.98
2016-09-07T05:00:01.550380: step 825, loss 0.0453316, acc 1
2016-09-07T05:00:02.223839: step 826, loss 0.11565, acc 0.96
2016-09-07T05:00:02.914172: step 827, loss 0.199261, acc 0.96
2016-09-07T05:00:03.592367: step 828, loss 0.114441, acc 0.96
2016-09-07T05:00:04.277928: step 829, loss 0.127929, acc 0.96
2016-09-07T05:00:04.962422: step 830, loss 0.0689466, acc 0.98
2016-09-07T05:00:05.654114: step 831, loss 0.151851, acc 0.92
2016-09-07T05:00:06.364949: step 832, loss 0.113182, acc 0.96
2016-09-07T05:00:07.045359: step 833, loss 0.048321, acc 0.98
2016-09-07T05:00:07.739219: step 834, loss 0.22946, acc 0.86
2016-09-07T05:00:08.445997: step 835, loss 0.110018, acc 0.96
2016-09-07T05:00:09.131258: step 836, loss 0.0807284, acc 0.96
2016-09-07T05:00:09.834071: step 837, loss 0.154438, acc 0.96
2016-09-07T05:00:10.488771: step 838, loss 0.147948, acc 0.9
2016-09-07T05:00:11.183239: step 839, loss 0.180468, acc 0.94
2016-09-07T05:00:11.851207: step 840, loss 0.0531013, acc 0.96
2016-09-07T05:00:12.521865: step 841, loss 0.0280118, acc 1
2016-09-07T05:00:13.211183: step 842, loss 0.039561, acc 0.98
2016-09-07T05:00:13.901682: step 843, loss 0.111488, acc 0.96
2016-09-07T05:00:14.578249: step 844, loss 0.140492, acc 0.92
2016-09-07T05:00:15.270225: step 845, loss 0.129272, acc 0.94
2016-09-07T05:00:15.998898: step 846, loss 0.0923465, acc 0.94
2016-09-07T05:00:16.679756: step 847, loss 0.0727898, acc 1
2016-09-07T05:00:17.367191: step 848, loss 0.126011, acc 0.96
2016-09-07T05:00:18.057383: step 849, loss 0.140426, acc 0.96
2016-09-07T05:00:18.751546: step 850, loss 0.100832, acc 0.94
2016-09-07T05:00:19.423184: step 851, loss 0.0811031, acc 0.96
2016-09-07T05:00:20.127035: step 852, loss 0.160861, acc 0.88
2016-09-07T05:00:20.846966: step 853, loss 0.175003, acc 0.92
2016-09-07T05:00:21.542562: step 854, loss 0.0584418, acc 0.98
2016-09-07T05:00:22.233355: step 855, loss 0.145673, acc 0.94
2016-09-07T05:00:22.925367: step 856, loss 0.173752, acc 0.94
2016-09-07T05:00:23.617288: step 857, loss 0.0982711, acc 0.94
2016-09-07T05:00:24.329932: step 858, loss 0.154826, acc 0.9
2016-09-07T05:00:25.014577: step 859, loss 0.080036, acc 0.98
2016-09-07T05:00:25.742389: step 860, loss 0.149847, acc 0.96
2016-09-07T05:00:26.428084: step 861, loss 0.0874839, acc 0.94
2016-09-07T05:00:27.121709: step 862, loss 0.087463, acc 0.94
2016-09-07T05:00:27.816365: step 863, loss 0.225995, acc 0.94
2016-09-07T05:00:28.521416: step 864, loss 0.127716, acc 0.96
2016-09-07T05:00:29.237265: step 865, loss 0.115072, acc 0.92
2016-09-07T05:00:29.922018: step 866, loss 0.119163, acc 0.94
2016-09-07T05:00:30.610928: step 867, loss 0.0502319, acc 0.98
2016-09-07T05:00:31.297110: step 868, loss 0.0804019, acc 0.98
2016-09-07T05:00:31.986608: step 869, loss 0.0464954, acc 0.98
2016-09-07T05:00:32.693107: step 870, loss 0.120833, acc 0.94
2016-09-07T05:00:33.386559: step 871, loss 0.124732, acc 0.94
2016-09-07T05:00:34.097047: step 872, loss 0.155118, acc 0.92
2016-09-07T05:00:34.799815: step 873, loss 0.0988925, acc 0.96
2016-09-07T05:00:35.502201: step 874, loss 0.176511, acc 0.9
2016-09-07T05:00:36.188671: step 875, loss 0.271123, acc 0.9
2016-09-07T05:00:36.904556: step 876, loss 0.238513, acc 0.92
2016-09-07T05:00:37.616706: step 877, loss 0.166824, acc 0.92
2016-09-07T05:00:38.373749: step 878, loss 0.110877, acc 0.94
2016-09-07T05:00:39.069124: step 879, loss 0.108101, acc 0.96
2016-09-07T05:00:39.738477: step 880, loss 0.228632, acc 0.94
2016-09-07T05:00:40.448007: step 881, loss 0.18271, acc 0.96
2016-09-07T05:00:41.146611: step 882, loss 0.118599, acc 0.98
2016-09-07T05:00:41.843483: step 883, loss 0.102308, acc 0.98
2016-09-07T05:00:42.554508: step 884, loss 0.174928, acc 0.94
2016-09-07T05:00:43.241278: step 885, loss 0.131232, acc 0.92
2016-09-07T05:00:43.923364: step 886, loss 0.147348, acc 0.96
2016-09-07T05:00:44.609017: step 887, loss 0.12716, acc 0.92
2016-09-07T05:00:45.305165: step 888, loss 0.096243, acc 0.98
2016-09-07T05:00:45.996307: step 889, loss 0.146229, acc 0.96
2016-09-07T05:00:46.674450: step 890, loss 0.0489431, acc 0.98
2016-09-07T05:00:47.372864: step 891, loss 0.14473, acc 0.9
2016-09-07T05:00:48.068565: step 892, loss 0.136306, acc 0.92
2016-09-07T05:00:48.761602: step 893, loss 0.0931172, acc 0.94
2016-09-07T05:00:49.458598: step 894, loss 0.142322, acc 0.96
2016-09-07T05:00:50.151335: step 895, loss 0.190047, acc 0.94
2016-09-07T05:00:50.869259: step 896, loss 0.105059, acc 0.96
2016-09-07T05:00:51.566899: step 897, loss 0.0821876, acc 0.98
2016-09-07T05:00:52.265211: step 898, loss 0.150704, acc 0.92
2016-09-07T05:00:52.955286: step 899, loss 0.203369, acc 0.92
2016-09-07T05:00:53.640227: step 900, loss 0.0666038, acc 0.98

Evaluation:
2016-09-07T05:00:57.223654: step 900, loss 0.681813, acc 0.778612

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-900

2016-09-07T05:00:59.008437: step 901, loss 0.0673684, acc 0.96
2016-09-07T05:00:59.698919: step 902, loss 0.256473, acc 0.86
2016-09-07T05:01:00.443365: step 903, loss 0.100151, acc 0.96
2016-09-07T05:01:01.123741: step 904, loss 0.119488, acc 0.9
2016-09-07T05:01:01.810044: step 905, loss 0.186863, acc 0.9
2016-09-07T05:01:02.487066: step 906, loss 0.128182, acc 0.96
2016-09-07T05:01:03.168451: step 907, loss 0.0719348, acc 0.96
2016-09-07T05:01:03.855826: step 908, loss 0.198867, acc 0.94
2016-09-07T05:01:04.542585: step 909, loss 0.103256, acc 0.92
2016-09-07T05:01:05.241928: step 910, loss 0.148377, acc 0.96
2016-09-07T05:01:05.918974: step 911, loss 0.114551, acc 0.94
2016-09-07T05:01:06.609812: step 912, loss 0.0245689, acc 1
2016-09-07T05:01:07.289560: step 913, loss 0.114981, acc 0.92
2016-09-07T05:01:07.975496: step 914, loss 0.259117, acc 0.9
2016-09-07T05:01:08.656913: step 915, loss 0.18106, acc 0.9
2016-09-07T05:01:09.338255: step 916, loss 0.108724, acc 0.94
2016-09-07T05:01:10.047894: step 917, loss 0.0849081, acc 0.96
2016-09-07T05:01:10.716201: step 918, loss 0.0726463, acc 0.98
2016-09-07T05:01:11.409535: step 919, loss 0.157932, acc 0.9
2016-09-07T05:01:12.092432: step 920, loss 0.204644, acc 0.88
2016-09-07T05:01:12.774717: step 921, loss 0.1093, acc 0.96
2016-09-07T05:01:13.472749: step 922, loss 0.13908, acc 0.98
2016-09-07T05:01:14.152057: step 923, loss 0.0973446, acc 0.94
2016-09-07T05:01:14.874740: step 924, loss 0.0805456, acc 0.96
2016-09-07T05:01:15.566684: step 925, loss 0.0239732, acc 1
2016-09-07T05:01:16.267027: step 926, loss 0.110583, acc 0.98
2016-09-07T05:01:16.954440: step 927, loss 0.170984, acc 0.9
2016-09-07T05:01:17.634634: step 928, loss 0.21424, acc 0.92
2016-09-07T05:01:18.318936: step 929, loss 0.175952, acc 0.92
2016-09-07T05:01:19.020202: step 930, loss 0.132353, acc 0.94
2016-09-07T05:01:19.752371: step 931, loss 0.21684, acc 0.94
2016-09-07T05:01:20.448263: step 932, loss 0.20635, acc 0.92
2016-09-07T05:01:21.136344: step 933, loss 0.166411, acc 0.94
2016-09-07T05:01:21.837514: step 934, loss 0.402391, acc 0.88
2016-09-07T05:01:22.537025: step 935, loss 0.0785383, acc 0.98
2016-09-07T05:01:23.236212: step 936, loss 0.129836, acc 0.94
2016-09-07T05:01:23.906297: step 937, loss 0.138848, acc 0.96
2016-09-07T05:01:24.625788: step 938, loss 0.162886, acc 0.92
2016-09-07T05:01:25.321048: step 939, loss 0.153021, acc 0.92
2016-09-07T05:01:26.018559: step 940, loss 0.166256, acc 0.86
2016-09-07T05:01:26.704372: step 941, loss 0.171391, acc 0.94
2016-09-07T05:01:27.388975: step 942, loss 0.220052, acc 0.9
2016-09-07T05:01:28.096585: step 943, loss 0.231975, acc 0.86
2016-09-07T05:01:28.796748: step 944, loss 0.147564, acc 0.96
2016-09-07T05:01:29.513638: step 945, loss 0.25233, acc 0.9
2016-09-07T05:01:30.206523: step 946, loss 0.0637867, acc 1
2016-09-07T05:01:30.895060: step 947, loss 0.209955, acc 0.96
2016-09-07T05:01:31.577371: step 948, loss 0.161587, acc 0.92
2016-09-07T05:01:32.272805: step 949, loss 0.123482, acc 0.94
2016-09-07T05:01:32.996176: step 950, loss 0.0815203, acc 1
2016-09-07T05:01:33.670109: step 951, loss 0.0613768, acc 0.98
2016-09-07T05:01:34.366439: step 952, loss 0.202185, acc 0.92
2016-09-07T05:01:35.054686: step 953, loss 0.116823, acc 0.98
2016-09-07T05:01:35.746466: step 954, loss 0.250283, acc 0.86
2016-09-07T05:01:36.421027: step 955, loss 0.160057, acc 0.92
2016-09-07T05:01:37.093402: step 956, loss 0.0853898, acc 0.94
2016-09-07T05:01:37.820342: step 957, loss 0.0572232, acc 0.98
2016-09-07T05:01:38.534472: step 958, loss 0.138718, acc 0.94
2016-09-07T05:01:39.227320: step 959, loss 0.132914, acc 0.94
2016-09-07T05:01:39.861379: step 960, loss 0.120522, acc 0.954545
2016-09-07T05:01:40.563944: step 961, loss 0.0791767, acc 0.96
2016-09-07T05:01:41.268891: step 962, loss 0.130754, acc 0.94
2016-09-07T05:01:41.947586: step 963, loss 0.151191, acc 0.88
2016-09-07T05:01:42.691224: step 964, loss 0.0441061, acc 0.98
2016-09-07T05:01:43.400549: step 965, loss 0.122881, acc 0.94
2016-09-07T05:01:44.106564: step 966, loss 0.0877519, acc 0.96
2016-09-07T05:01:44.790552: step 967, loss 0.121485, acc 0.94
2016-09-07T05:01:45.504320: step 968, loss 0.0831588, acc 0.94
2016-09-07T05:01:46.214855: step 969, loss 0.0819864, acc 0.98
2016-09-07T05:01:46.904753: step 970, loss 0.134398, acc 0.92
2016-09-07T05:01:47.627036: step 971, loss 0.04041, acc 1
2016-09-07T05:01:48.334841: step 972, loss 0.110465, acc 0.94
2016-09-07T05:01:49.039823: step 973, loss 0.0629352, acc 0.98
2016-09-07T05:01:49.736981: step 974, loss 0.0397495, acc 0.98
2016-09-07T05:01:50.433506: step 975, loss 0.0533258, acc 0.98
2016-09-07T05:01:51.138544: step 976, loss 0.0412031, acc 0.98
2016-09-07T05:01:51.831980: step 977, loss 0.158961, acc 0.94
2016-09-07T05:01:52.553819: step 978, loss 0.0579073, acc 0.98
2016-09-07T05:01:53.273440: step 979, loss 0.07298, acc 0.96
2016-09-07T05:01:53.994460: step 980, loss 0.088295, acc 0.98
2016-09-07T05:01:54.715691: step 981, loss 0.0498625, acc 1
2016-09-07T05:01:55.404750: step 982, loss 0.119635, acc 0.96
2016-09-07T05:01:56.078302: step 983, loss 0.064306, acc 0.96
2016-09-07T05:01:56.763001: step 984, loss 0.0613068, acc 0.98
2016-09-07T05:01:57.469845: step 985, loss 0.0719038, acc 0.98
2016-09-07T05:01:58.177747: step 986, loss 0.0603977, acc 0.96
2016-09-07T05:01:58.861356: step 987, loss 0.11662, acc 0.96
2016-09-07T05:01:59.548951: step 988, loss 0.063677, acc 0.98
2016-09-07T05:02:00.243264: step 989, loss 0.0384068, acc 0.96
2016-09-07T05:02:00.919737: step 990, loss 0.115584, acc 0.96
2016-09-07T05:02:01.615344: step 991, loss 0.142561, acc 0.96
2016-09-07T05:02:02.290291: step 992, loss 0.241184, acc 0.88
2016-09-07T05:02:03.009973: step 993, loss 0.0588901, acc 0.98
2016-09-07T05:02:03.686882: step 994, loss 0.0554131, acc 0.96
2016-09-07T05:02:04.368022: step 995, loss 0.00919656, acc 1
2016-09-07T05:02:05.063880: step 996, loss 0.0369051, acc 0.98
2016-09-07T05:02:05.753805: step 997, loss 0.225726, acc 0.92
2016-09-07T05:02:06.432121: step 998, loss 0.0144013, acc 1
2016-09-07T05:02:07.148151: step 999, loss 0.0832322, acc 0.94
2016-09-07T05:02:07.871432: step 1000, loss 0.0941099, acc 0.98

Evaluation:
2016-09-07T05:02:11.497484: step 1000, loss 1.06565, acc 0.75985

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-1000

2016-09-07T05:02:13.252077: step 1001, loss 0.0987731, acc 0.92
2016-09-07T05:02:13.963610: step 1002, loss 0.0981813, acc 0.94
2016-09-07T05:02:14.671982: step 1003, loss 0.0391103, acc 0.98
2016-09-07T05:02:15.386343: step 1004, loss 0.0318056, acc 1
2016-09-07T05:02:16.090545: step 1005, loss 0.125175, acc 0.96
2016-09-07T05:02:16.773603: step 1006, loss 0.0437836, acc 1
2016-09-07T05:02:17.470824: step 1007, loss 0.17131, acc 0.92
2016-09-07T05:02:18.168928: step 1008, loss 0.147881, acc 0.96
2016-09-07T05:02:18.847364: step 1009, loss 0.162542, acc 0.96
2016-09-07T05:02:19.522021: step 1010, loss 0.230892, acc 0.92
2016-09-07T05:02:20.212640: step 1011, loss 0.0304793, acc 1
2016-09-07T05:02:20.887312: step 1012, loss 0.0466971, acc 0.98
2016-09-07T05:02:21.574677: step 1013, loss 0.0357309, acc 0.98
2016-09-07T05:02:22.268303: step 1014, loss 0.0839397, acc 0.94
2016-09-07T05:02:22.937258: step 1015, loss 0.0514844, acc 1
2016-09-07T05:02:23.608148: step 1016, loss 0.103896, acc 0.94
2016-09-07T05:02:24.297403: step 1017, loss 0.076694, acc 0.96
2016-09-07T05:02:24.973839: step 1018, loss 0.0254361, acc 1
2016-09-07T05:02:25.676119: step 1019, loss 0.0989966, acc 0.98
2016-09-07T05:02:26.366664: step 1020, loss 0.0685971, acc 0.96
2016-09-07T05:02:27.071625: step 1021, loss 0.117554, acc 0.94
2016-09-07T05:02:27.767311: step 1022, loss 0.142824, acc 0.94
2016-09-07T05:02:28.454055: step 1023, loss 0.131238, acc 0.94
2016-09-07T05:02:29.161132: step 1024, loss 0.0687005, acc 0.94
2016-09-07T05:02:29.842837: step 1025, loss 0.105409, acc 0.94
2016-09-07T05:02:30.548724: step 1026, loss 0.0540974, acc 0.96
2016-09-07T05:02:31.235894: step 1027, loss 0.211163, acc 0.92
2016-09-07T05:02:31.945885: step 1028, loss 0.208899, acc 0.92
2016-09-07T05:02:32.614070: step 1029, loss 0.0961752, acc 0.96
2016-09-07T05:02:33.302182: step 1030, loss 0.158902, acc 0.94
2016-09-07T05:02:33.991875: step 1031, loss 0.0956236, acc 0.96
2016-09-07T05:02:34.678970: step 1032, loss 0.0896071, acc 0.94
2016-09-07T05:02:35.369393: step 1033, loss 0.0868067, acc 0.94
2016-09-07T05:02:36.037272: step 1034, loss 0.120893, acc 0.94
2016-09-07T05:02:36.743546: step 1035, loss 0.0915499, acc 0.98
2016-09-07T05:02:37.422496: step 1036, loss 0.0997215, acc 0.94
2016-09-07T05:02:38.109940: step 1037, loss 0.168417, acc 0.92
2016-09-07T05:02:38.801458: step 1038, loss 0.0713509, acc 0.96
2016-09-07T05:02:39.493230: step 1039, loss 0.0496983, acc 1
2016-09-07T05:02:40.197115: step 1040, loss 0.0865203, acc 1
2016-09-07T05:02:40.875355: step 1041, loss 0.0837154, acc 0.96
2016-09-07T05:02:41.585893: step 1042, loss 0.0785718, acc 0.94
2016-09-07T05:02:42.269833: step 1043, loss 0.0385093, acc 0.98
2016-09-07T05:02:42.944407: step 1044, loss 0.0827999, acc 0.96
2016-09-07T05:02:43.643854: step 1045, loss 0.0475693, acc 0.98
2016-09-07T05:02:44.330019: step 1046, loss 0.0570794, acc 0.98
2016-09-07T05:02:45.044410: step 1047, loss 0.114133, acc 0.96
2016-09-07T05:02:45.735253: step 1048, loss 0.0476966, acc 0.96
2016-09-07T05:02:46.440351: step 1049, loss 0.125393, acc 0.96
2016-09-07T05:02:47.144979: step 1050, loss 0.0889185, acc 0.96
2016-09-07T05:02:47.822610: step 1051, loss 0.101988, acc 0.92
2016-09-07T05:02:48.518000: step 1052, loss 0.0522879, acc 0.98
2016-09-07T05:02:49.206501: step 1053, loss 0.206522, acc 0.94
2016-09-07T05:02:49.932745: step 1054, loss 0.0792045, acc 0.96
2016-09-07T05:02:50.630671: step 1055, loss 0.171858, acc 0.94
2016-09-07T05:02:51.313394: step 1056, loss 0.194397, acc 0.92
2016-09-07T05:02:52.024570: step 1057, loss 0.0783187, acc 0.94
2016-09-07T05:02:52.726846: step 1058, loss 0.205746, acc 0.9
2016-09-07T05:02:53.423689: step 1059, loss 0.0462901, acc 0.96
2016-09-07T05:02:54.088276: step 1060, loss 0.0685878, acc 0.96
2016-09-07T05:02:54.805782: step 1061, loss 0.0967397, acc 0.92
2016-09-07T05:02:55.499057: step 1062, loss 0.252377, acc 0.86
2016-09-07T05:02:56.193309: step 1063, loss 0.108308, acc 0.96
2016-09-07T05:02:56.873773: step 1064, loss 0.0873533, acc 0.96
2016-09-07T05:02:57.557962: step 1065, loss 0.057679, acc 0.98
2016-09-07T05:02:58.262493: step 1066, loss 0.170964, acc 0.92
2016-09-07T05:02:58.947450: step 1067, loss 0.115535, acc 0.96
2016-09-07T05:02:59.651977: step 1068, loss 0.199149, acc 0.96
2016-09-07T05:03:00.368071: step 1069, loss 0.160449, acc 0.94
2016-09-07T05:03:01.048694: step 1070, loss 0.115079, acc 0.94
2016-09-07T05:03:01.724583: step 1071, loss 0.214474, acc 0.86
2016-09-07T05:03:02.421155: step 1072, loss 0.116041, acc 0.96
2016-09-07T05:03:03.142012: step 1073, loss 0.260773, acc 0.92
2016-09-07T05:03:03.833663: step 1074, loss 0.108744, acc 0.94
2016-09-07T05:03:04.532163: step 1075, loss 0.0828856, acc 0.96
2016-09-07T05:03:05.226393: step 1076, loss 0.0984168, acc 0.92
2016-09-07T05:03:05.928095: step 1077, loss 0.125248, acc 0.92
2016-09-07T05:03:06.635719: step 1078, loss 0.0759916, acc 0.94
2016-09-07T05:03:07.308832: step 1079, loss 0.26381, acc 0.9
2016-09-07T05:03:08.014626: step 1080, loss 0.0899056, acc 0.98
2016-09-07T05:03:08.719108: step 1081, loss 0.0685916, acc 0.96
2016-09-07T05:03:09.407297: step 1082, loss 0.184948, acc 0.96
2016-09-07T05:03:10.081909: step 1083, loss 0.197635, acc 0.88
2016-09-07T05:03:10.779796: step 1084, loss 0.112375, acc 0.96
2016-09-07T05:03:11.469245: step 1085, loss 0.167789, acc 0.92
2016-09-07T05:03:12.130412: step 1086, loss 0.0667178, acc 1
2016-09-07T05:03:12.824494: step 1087, loss 0.0978071, acc 0.92
2016-09-07T05:03:13.533141: step 1088, loss 0.167618, acc 0.98
2016-09-07T05:03:14.230543: step 1089, loss 0.0225665, acc 1
2016-09-07T05:03:14.921821: step 1090, loss 0.0929625, acc 0.98
2016-09-07T05:03:15.659005: step 1091, loss 0.0864157, acc 0.92
2016-09-07T05:03:16.381760: step 1092, loss 0.0779903, acc 0.98
2016-09-07T05:03:17.059205: step 1093, loss 0.0536872, acc 1
2016-09-07T05:03:17.735277: step 1094, loss 0.246308, acc 0.96
2016-09-07T05:03:18.414223: step 1095, loss 0.153253, acc 0.94
2016-09-07T05:03:19.102512: step 1096, loss 0.0820818, acc 0.96
2016-09-07T05:03:19.770322: step 1097, loss 0.0618853, acc 0.98
2016-09-07T05:03:20.462898: step 1098, loss 0.149372, acc 0.96
2016-09-07T05:03:21.165050: step 1099, loss 0.0764693, acc 0.96
2016-09-07T05:03:21.864555: step 1100, loss 0.0845534, acc 0.94

Evaluation:
2016-09-07T05:03:25.415276: step 1100, loss 0.859395, acc 0.771107

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-1100

2016-09-07T05:03:27.271092: step 1101, loss 0.113391, acc 0.96
2016-09-07T05:03:27.965791: step 1102, loss 0.0794004, acc 0.96
2016-09-07T05:03:28.649474: step 1103, loss 0.233859, acc 0.94
2016-09-07T05:03:29.341180: step 1104, loss 0.159042, acc 0.94
2016-09-07T05:03:30.038080: step 1105, loss 0.0605864, acc 0.98
2016-09-07T05:03:30.715760: step 1106, loss 0.186125, acc 0.94
2016-09-07T05:03:31.386906: step 1107, loss 0.148174, acc 0.94
2016-09-07T05:03:32.107833: step 1108, loss 0.146815, acc 0.92
2016-09-07T05:03:32.793972: step 1109, loss 0.167085, acc 0.94
2016-09-07T05:03:33.470358: step 1110, loss 0.098673, acc 0.98
2016-09-07T05:03:34.152322: step 1111, loss 0.0575837, acc 0.98
2016-09-07T05:03:34.857622: step 1112, loss 0.153729, acc 0.92
2016-09-07T05:03:35.553733: step 1113, loss 0.0636924, acc 0.96
2016-09-07T05:03:36.233756: step 1114, loss 0.12573, acc 0.94
2016-09-07T05:03:36.935043: step 1115, loss 0.0344809, acc 1
2016-09-07T05:03:37.637565: step 1116, loss 0.0848664, acc 0.96
2016-09-07T05:03:38.327000: step 1117, loss 0.139334, acc 0.92
2016-09-07T05:03:39.024691: step 1118, loss 0.0640164, acc 1
2016-09-07T05:03:39.712129: step 1119, loss 0.040886, acc 0.98
2016-09-07T05:03:40.450225: step 1120, loss 0.102771, acc 0.94
2016-09-07T05:03:41.121443: step 1121, loss 0.0523165, acc 1
2016-09-07T05:03:41.819503: step 1122, loss 0.138695, acc 0.94
2016-09-07T05:03:42.494632: step 1123, loss 0.114351, acc 0.96
2016-09-07T05:03:43.178364: step 1124, loss 0.107757, acc 0.98
2016-09-07T05:03:43.885245: step 1125, loss 0.084871, acc 0.98
2016-09-07T05:03:44.555063: step 1126, loss 0.0708695, acc 0.98
2016-09-07T05:03:45.265796: step 1127, loss 0.0461091, acc 0.96
2016-09-07T05:03:45.942619: step 1128, loss 0.15355, acc 0.94
2016-09-07T05:03:46.619308: step 1129, loss 0.158629, acc 0.9
2016-09-07T05:03:47.301810: step 1130, loss 0.0914434, acc 0.94
2016-09-07T05:03:48.005131: step 1131, loss 0.117132, acc 0.96
2016-09-07T05:03:48.692817: step 1132, loss 0.122007, acc 0.92
2016-09-07T05:03:49.399689: step 1133, loss 0.0893223, acc 0.96
2016-09-07T05:03:50.139087: step 1134, loss 0.0395522, acc 0.98
2016-09-07T05:03:50.840334: step 1135, loss 0.0687602, acc 0.98
2016-09-07T05:03:51.524593: step 1136, loss 0.0867301, acc 0.98
2016-09-07T05:03:52.230902: step 1137, loss 0.0744841, acc 0.98
2016-09-07T05:03:52.915552: step 1138, loss 0.0567975, acc 0.98
2016-09-07T05:03:53.641428: step 1139, loss 0.0706969, acc 0.98
2016-09-07T05:03:54.343277: step 1140, loss 0.0854869, acc 0.96
2016-09-07T05:03:55.030529: step 1141, loss 0.12207, acc 0.88
2016-09-07T05:03:55.711958: step 1142, loss 0.119406, acc 0.94
2016-09-07T05:03:56.430572: step 1143, loss 0.211476, acc 0.92
2016-09-07T05:03:57.149263: step 1144, loss 0.0887185, acc 0.98
2016-09-07T05:03:57.821510: step 1145, loss 0.0885149, acc 0.98
2016-09-07T05:03:58.511949: step 1146, loss 0.027266, acc 1
2016-09-07T05:03:59.207102: step 1147, loss 0.0805171, acc 0.98
2016-09-07T05:03:59.902534: step 1148, loss 0.0918739, acc 0.96
2016-09-07T05:04:00.627109: step 1149, loss 0.119376, acc 0.94
2016-09-07T05:04:01.311893: step 1150, loss 0.071485, acc 0.98
2016-09-07T05:04:02.007825: step 1151, loss 0.0699037, acc 0.96
2016-09-07T05:04:02.629297: step 1152, loss 0.0814423, acc 0.977273
2016-09-07T05:04:03.345338: step 1153, loss 0.0958594, acc 0.98
2016-09-07T05:04:04.034372: step 1154, loss 0.0455213, acc 0.96
2016-09-07T05:04:04.727410: step 1155, loss 0.039989, acc 0.98
2016-09-07T05:04:05.405978: step 1156, loss 0.131424, acc 0.96
2016-09-07T05:04:06.114666: step 1157, loss 0.120333, acc 0.98
2016-09-07T05:04:06.809917: step 1158, loss 0.0561389, acc 0.98
2016-09-07T05:04:07.486094: step 1159, loss 0.0237827, acc 1
2016-09-07T05:04:08.184123: step 1160, loss 0.060094, acc 0.96
2016-09-07T05:04:08.872129: step 1161, loss 0.0799658, acc 0.96
2016-09-07T05:04:09.549526: step 1162, loss 0.0378596, acc 0.98
2016-09-07T05:04:10.245014: step 1163, loss 0.0868045, acc 0.96
2016-09-07T05:04:10.928224: step 1164, loss 0.044545, acc 0.98
2016-09-07T05:04:11.626468: step 1165, loss 0.0305069, acc 1
2016-09-07T05:04:12.294518: step 1166, loss 0.0113704, acc 1
2016-09-07T05:04:12.995254: step 1167, loss 0.0769976, acc 0.96
2016-09-07T05:04:13.679497: step 1168, loss 0.117256, acc 0.96
2016-09-07T05:04:14.367433: step 1169, loss 0.104759, acc 0.94
2016-09-07T05:04:15.053525: step 1170, loss 0.0670442, acc 0.98
2016-09-07T05:04:15.762233: step 1171, loss 0.0881669, acc 0.96
2016-09-07T05:04:16.485125: step 1172, loss 0.0299186, acc 0.98
2016-09-07T05:04:17.171169: step 1173, loss 0.0783668, acc 0.98
2016-09-07T05:04:17.831255: step 1174, loss 0.22815, acc 0.92
2016-09-07T05:04:18.527391: step 1175, loss 0.166266, acc 0.96
2016-09-07T05:04:19.228593: step 1176, loss 0.0148612, acc 1
2016-09-07T05:04:19.945348: step 1177, loss 0.0185271, acc 1
2016-09-07T05:04:20.626174: step 1178, loss 0.036472, acc 0.98
2016-09-07T05:04:21.322935: step 1179, loss 0.205587, acc 0.92
2016-09-07T05:04:22.013774: step 1180, loss 0.0289739, acc 1
2016-09-07T05:04:22.714450: step 1181, loss 0.0455489, acc 1
2016-09-07T05:04:23.384603: step 1182, loss 0.130163, acc 0.94
2016-09-07T05:04:24.083599: step 1183, loss 0.0576311, acc 0.98
2016-09-07T05:04:24.806846: step 1184, loss 0.11663, acc 0.94
2016-09-07T05:04:25.474833: step 1185, loss 0.11123, acc 0.94
2016-09-07T05:04:26.181023: step 1186, loss 0.184862, acc 0.9
2016-09-07T05:04:26.871909: step 1187, loss 0.0485124, acc 0.96
2016-09-07T05:04:27.540188: step 1188, loss 0.160671, acc 0.94
2016-09-07T05:04:28.225010: step 1189, loss 0.0735637, acc 0.96
2016-09-07T05:04:28.904215: step 1190, loss 0.0839846, acc 0.98
2016-09-07T05:04:29.618553: step 1191, loss 0.142579, acc 0.98
2016-09-07T05:04:30.282766: step 1192, loss 0.0675426, acc 0.96
2016-09-07T05:04:30.995478: step 1193, loss 0.132715, acc 0.94
2016-09-07T05:04:31.685312: step 1194, loss 0.0518304, acc 0.98
2016-09-07T05:04:32.375067: step 1195, loss 0.0400396, acc 0.98
2016-09-07T05:04:33.066651: step 1196, loss 0.0890717, acc 0.94
2016-09-07T05:04:33.769518: step 1197, loss 0.0427485, acc 1
2016-09-07T05:04:34.462404: step 1198, loss 0.0749881, acc 0.98
2016-09-07T05:04:35.146918: step 1199, loss 0.165676, acc 0.92
2016-09-07T05:04:35.840171: step 1200, loss 0.129046, acc 0.94

Evaluation:
2016-09-07T05:04:39.436504: step 1200, loss 0.861234, acc 0.772983

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-1200

2016-09-07T05:04:41.296235: step 1201, loss 0.124324, acc 0.96
2016-09-07T05:04:41.981333: step 1202, loss 0.0559374, acc 1
2016-09-07T05:04:42.679776: step 1203, loss 0.0353048, acc 0.98
2016-09-07T05:04:43.363477: step 1204, loss 0.0730549, acc 0.98
2016-09-07T05:04:44.074210: step 1205, loss 0.0922372, acc 0.96
2016-09-07T05:04:44.762873: step 1206, loss 0.0631816, acc 0.94
2016-09-07T05:04:45.466549: step 1207, loss 0.0971119, acc 0.94
2016-09-07T05:04:46.132650: step 1208, loss 0.113637, acc 0.96
2016-09-07T05:04:46.835038: step 1209, loss 0.0469546, acc 1
2016-09-07T05:04:47.536722: step 1210, loss 0.0601646, acc 0.98
2016-09-07T05:04:48.219119: step 1211, loss 0.0836149, acc 0.96
2016-09-07T05:04:48.912805: step 1212, loss 0.0686251, acc 0.96
2016-09-07T05:04:49.586580: step 1213, loss 0.0322738, acc 1
2016-09-07T05:04:50.301685: step 1214, loss 0.0609773, acc 0.98
2016-09-07T05:04:51.018913: step 1215, loss 0.0932543, acc 0.96
2016-09-07T05:04:51.709848: step 1216, loss 0.143173, acc 0.94
2016-09-07T05:04:52.386850: step 1217, loss 0.0631805, acc 0.96
2016-09-07T05:04:53.058071: step 1218, loss 0.0832871, acc 0.94
2016-09-07T05:04:53.771569: step 1219, loss 0.0647663, acc 0.96
2016-09-07T05:04:54.442337: step 1220, loss 0.0543869, acc 0.98
2016-09-07T05:04:55.128629: step 1221, loss 0.0513261, acc 0.98
2016-09-07T05:04:55.833381: step 1222, loss 0.0773744, acc 0.98
2016-09-07T05:04:56.533645: step 1223, loss 0.0486601, acc 0.96
2016-09-07T05:04:57.228449: step 1224, loss 0.107571, acc 0.94
2016-09-07T05:04:57.927048: step 1225, loss 0.0888878, acc 0.98
2016-09-07T05:04:58.632735: step 1226, loss 0.0361401, acc 0.98
2016-09-07T05:04:59.324111: step 1227, loss 0.0575456, acc 0.98
2016-09-07T05:05:00.000165: step 1228, loss 0.102065, acc 0.96
2016-09-07T05:05:00.721271: step 1229, loss 0.101079, acc 0.94
2016-09-07T05:05:01.427299: step 1230, loss 0.0981952, acc 0.96
2016-09-07T05:05:02.131384: step 1231, loss 0.0701281, acc 0.96
2016-09-07T05:05:02.809155: step 1232, loss 0.0318318, acc 1
2016-09-07T05:05:03.513577: step 1233, loss 0.0584585, acc 1
2016-09-07T05:05:04.209989: step 1234, loss 0.0344735, acc 0.98
2016-09-07T05:05:04.916936: step 1235, loss 0.0838885, acc 0.94
2016-09-07T05:05:05.613879: step 1236, loss 0.0645102, acc 0.96
2016-09-07T05:05:06.298088: step 1237, loss 0.131983, acc 0.94
2016-09-07T05:05:07.007085: step 1238, loss 0.0501735, acc 0.98
2016-09-07T05:05:07.691188: step 1239, loss 0.0146168, acc 1
2016-09-07T05:05:08.387686: step 1240, loss 0.0878367, acc 0.96
2016-09-07T05:05:09.088021: step 1241, loss 0.0649827, acc 0.96
2016-09-07T05:05:09.787253: step 1242, loss 0.0669132, acc 0.98
2016-09-07T05:05:10.476543: step 1243, loss 0.120205, acc 0.96
2016-09-07T05:05:11.164851: step 1244, loss 0.0731349, acc 0.98
2016-09-07T05:05:11.878607: step 1245, loss 0.0648913, acc 0.98
2016-09-07T05:05:12.576525: step 1246, loss 0.0949469, acc 0.96
2016-09-07T05:05:13.276933: step 1247, loss 0.0492745, acc 0.98
2016-09-07T05:05:13.955536: step 1248, loss 0.0754377, acc 0.96
2016-09-07T05:05:14.663390: step 1249, loss 0.0134147, acc 1
2016-09-07T05:05:15.394558: step 1250, loss 0.222816, acc 0.88
2016-09-07T05:05:16.088370: step 1251, loss 0.093218, acc 0.92
2016-09-07T05:05:16.775396: step 1252, loss 0.0371151, acc 1
2016-09-07T05:05:17.493871: step 1253, loss 0.0789993, acc 0.98
2016-09-07T05:05:18.202151: step 1254, loss 0.075133, acc 0.96
2016-09-07T05:05:18.917264: step 1255, loss 0.0939853, acc 0.96
2016-09-07T05:05:19.599911: step 1256, loss 0.0810981, acc 0.96
2016-09-07T05:05:20.301252: step 1257, loss 0.0587924, acc 0.98
2016-09-07T05:05:20.976390: step 1258, loss 0.121053, acc 0.94
2016-09-07T05:05:21.666789: step 1259, loss 0.0470144, acc 0.98
2016-09-07T05:05:22.359204: step 1260, loss 0.0881683, acc 0.98
2016-09-07T05:05:23.056993: step 1261, loss 0.0705959, acc 0.96
2016-09-07T05:05:23.755956: step 1262, loss 0.0679003, acc 0.94
2016-09-07T05:05:24.423966: step 1263, loss 0.0830832, acc 0.96
2016-09-07T05:05:25.118959: step 1264, loss 0.0656789, acc 0.96
2016-09-07T05:05:25.813433: step 1265, loss 0.0452484, acc 0.98
2016-09-07T05:05:26.517803: step 1266, loss 0.0264558, acc 1
2016-09-07T05:05:27.228435: step 1267, loss 0.0800705, acc 0.98
2016-09-07T05:05:27.926279: step 1268, loss 0.127456, acc 0.96
2016-09-07T05:05:28.646272: step 1269, loss 0.10059, acc 0.96
2016-09-07T05:05:29.336131: step 1270, loss 0.0915364, acc 0.94
2016-09-07T05:05:30.014823: step 1271, loss 0.064561, acc 0.96
2016-09-07T05:05:30.700356: step 1272, loss 0.0672123, acc 0.98
2016-09-07T05:05:31.384475: step 1273, loss 0.0378959, acc 0.98
2016-09-07T05:05:32.063940: step 1274, loss 0.184008, acc 0.9
2016-09-07T05:05:32.764233: step 1275, loss 0.224673, acc 0.94
2016-09-07T05:05:33.482655: step 1276, loss 0.0375891, acc 0.96
2016-09-07T05:05:34.157418: step 1277, loss 0.107351, acc 0.94
2016-09-07T05:05:34.843901: step 1278, loss 0.0718506, acc 0.96
2016-09-07T05:05:35.543356: step 1279, loss 0.0668573, acc 0.96
2016-09-07T05:05:36.263494: step 1280, loss 0.0456268, acc 0.98
2016-09-07T05:05:36.954235: step 1281, loss 0.223965, acc 0.92
2016-09-07T05:05:37.628167: step 1282, loss 0.170939, acc 0.92
2016-09-07T05:05:38.339889: step 1283, loss 0.0828443, acc 0.98
2016-09-07T05:05:39.022323: step 1284, loss 0.0795551, acc 0.96
2016-09-07T05:05:39.711919: step 1285, loss 0.0514248, acc 1
2016-09-07T05:05:40.418149: step 1286, loss 0.0733128, acc 0.98
2016-09-07T05:05:41.101617: step 1287, loss 0.0981613, acc 0.92
2016-09-07T05:05:41.810500: step 1288, loss 0.126967, acc 0.92
2016-09-07T05:05:42.473448: step 1289, loss 0.171075, acc 0.9
2016-09-07T05:05:43.203297: step 1290, loss 0.040168, acc 0.98
2016-09-07T05:05:43.884185: step 1291, loss 0.0387096, acc 1
2016-09-07T05:05:44.571675: step 1292, loss 0.298552, acc 0.92
2016-09-07T05:05:45.253038: step 1293, loss 0.0376459, acc 0.98
2016-09-07T05:05:45.958751: step 1294, loss 0.102403, acc 0.92
2016-09-07T05:05:46.660232: step 1295, loss 0.0414378, acc 1
2016-09-07T05:05:47.345700: step 1296, loss 0.0784783, acc 0.96
2016-09-07T05:05:48.026549: step 1297, loss 0.135451, acc 0.94
2016-09-07T05:05:48.704012: step 1298, loss 0.0979074, acc 0.96
2016-09-07T05:05:49.401002: step 1299, loss 0.0984158, acc 0.94
2016-09-07T05:05:50.085504: step 1300, loss 0.0878083, acc 0.94

Evaluation:
2016-09-07T05:05:53.669561: step 1300, loss 1.11842, acc 0.742026

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-1300

2016-09-07T05:05:55.482912: step 1301, loss 0.173585, acc 0.98
2016-09-07T05:05:56.192653: step 1302, loss 0.053301, acc 0.98
2016-09-07T05:05:56.861528: step 1303, loss 0.216109, acc 0.9
2016-09-07T05:05:57.576181: step 1304, loss 0.100809, acc 0.94
2016-09-07T05:05:58.277952: step 1305, loss 0.0652726, acc 0.98
2016-09-07T05:05:58.967329: step 1306, loss 0.124977, acc 0.94
2016-09-07T05:05:59.647524: step 1307, loss 0.0604868, acc 0.98
2016-09-07T05:06:00.381423: step 1308, loss 0.0355747, acc 0.98
2016-09-07T05:06:01.105918: step 1309, loss 0.0242401, acc 0.98
2016-09-07T05:06:01.790871: step 1310, loss 0.0614904, acc 0.98
2016-09-07T05:06:02.478400: step 1311, loss 0.0462143, acc 0.98
2016-09-07T05:06:03.177072: step 1312, loss 0.0218614, acc 0.98
2016-09-07T05:06:03.888145: step 1313, loss 0.044858, acc 0.98
2016-09-07T05:06:04.584833: step 1314, loss 0.128152, acc 0.94
2016-09-07T05:06:05.241936: step 1315, loss 0.160093, acc 0.96
2016-09-07T05:06:05.949751: step 1316, loss 0.0872372, acc 0.96
2016-09-07T05:06:06.626022: step 1317, loss 0.00684989, acc 1
2016-09-07T05:06:07.301017: step 1318, loss 0.142638, acc 0.94
2016-09-07T05:06:08.006865: step 1319, loss 0.100469, acc 0.96
2016-09-07T05:06:08.694459: step 1320, loss 0.0232608, acc 1
2016-09-07T05:06:09.379908: step 1321, loss 0.1462, acc 0.98
2016-09-07T05:06:10.056741: step 1322, loss 0.122123, acc 0.92
2016-09-07T05:06:10.753447: step 1323, loss 0.0598104, acc 1
2016-09-07T05:06:11.462043: step 1324, loss 0.0711738, acc 0.96
2016-09-07T05:06:12.145897: step 1325, loss 0.0760944, acc 0.96
2016-09-07T05:06:12.839826: step 1326, loss 0.0831369, acc 0.98
2016-09-07T05:06:13.523616: step 1327, loss 0.0666771, acc 0.98
2016-09-07T05:06:14.224423: step 1328, loss 0.0750954, acc 0.94
2016-09-07T05:06:14.896805: step 1329, loss 0.0586772, acc 0.96
2016-09-07T05:06:15.595070: step 1330, loss 0.0376228, acc 0.98
2016-09-07T05:06:16.293185: step 1331, loss 0.040423, acc 0.98
2016-09-07T05:06:16.996700: step 1332, loss 0.0394317, acc 1
2016-09-07T05:06:17.690039: step 1333, loss 0.149035, acc 0.96
2016-09-07T05:06:18.380476: step 1334, loss 0.124098, acc 0.96
2016-09-07T05:06:19.070555: step 1335, loss 0.165484, acc 0.92
2016-09-07T05:06:19.723056: step 1336, loss 0.0484204, acc 0.98
2016-09-07T05:06:20.418877: step 1337, loss 0.117184, acc 0.98
2016-09-07T05:06:21.090915: step 1338, loss 0.0512986, acc 0.96
2016-09-07T05:06:21.784150: step 1339, loss 0.095114, acc 0.94
2016-09-07T05:06:22.459529: step 1340, loss 0.0539313, acc 0.98
2016-09-07T05:06:23.139044: step 1341, loss 0.0495571, acc 0.98
2016-09-07T05:06:23.877937: step 1342, loss 0.106682, acc 0.96
2016-09-07T05:06:24.567192: step 1343, loss 0.128004, acc 0.9
2016-09-07T05:06:25.270466: step 1344, loss 0.0459729, acc 0.954545
2016-09-07T05:06:25.965146: step 1345, loss 0.0510491, acc 0.98
2016-09-07T05:06:26.668562: step 1346, loss 0.0729973, acc 0.98
2016-09-07T05:06:27.367100: step 1347, loss 0.076894, acc 0.96
2016-09-07T05:06:28.047794: step 1348, loss 0.0598864, acc 0.98
2016-09-07T05:06:28.769649: step 1349, loss 0.0194467, acc 1
2016-09-07T05:06:29.447421: step 1350, loss 0.0481896, acc 0.98
2016-09-07T05:06:30.138573: step 1351, loss 0.0240937, acc 1
2016-09-07T05:06:30.822917: step 1352, loss 0.105519, acc 0.98
2016-09-07T05:06:31.511398: step 1353, loss 0.143331, acc 0.98
2016-09-07T05:06:32.211346: step 1354, loss 0.0707088, acc 0.98
2016-09-07T05:06:32.900912: step 1355, loss 0.0255654, acc 0.98
2016-09-07T05:06:33.621648: step 1356, loss 0.182854, acc 0.92
2016-09-07T05:06:34.314495: step 1357, loss 0.0192674, acc 0.98
2016-09-07T05:06:35.006550: step 1358, loss 0.158035, acc 0.96
2016-09-07T05:06:35.684377: step 1359, loss 0.00909443, acc 1
2016-09-07T05:06:36.367124: step 1360, loss 0.0473532, acc 0.96
2016-09-07T05:06:37.075539: step 1361, loss 0.0504544, acc 0.96
2016-09-07T05:06:37.750990: step 1362, loss 0.219908, acc 0.94
2016-09-07T05:06:38.457302: step 1363, loss 0.0764362, acc 0.96
2016-09-07T05:06:39.137248: step 1364, loss 0.0341352, acc 0.98
2016-09-07T05:06:39.839307: step 1365, loss 0.0323605, acc 1
2016-09-07T05:06:40.559390: step 1366, loss 0.032117, acc 0.98
2016-09-07T05:06:41.268409: step 1367, loss 0.0503276, acc 0.98
2016-09-07T05:06:41.984502: step 1368, loss 0.134403, acc 0.96
2016-09-07T05:06:42.674313: step 1369, loss 0.0942669, acc 0.96
2016-09-07T05:06:43.369500: step 1370, loss 0.0313338, acc 0.98
2016-09-07T05:06:44.059919: step 1371, loss 0.132867, acc 0.94
2016-09-07T05:06:44.763710: step 1372, loss 0.0783791, acc 0.96
2016-09-07T05:06:45.450169: step 1373, loss 0.0654652, acc 0.98
2016-09-07T05:06:46.130982: step 1374, loss 0.0509152, acc 0.96
2016-09-07T05:06:46.842191: step 1375, loss 0.0189276, acc 1
2016-09-07T05:06:47.548704: step 1376, loss 0.0949321, acc 0.98
2016-09-07T05:06:48.227889: step 1377, loss 0.0433704, acc 0.96
2016-09-07T05:06:48.922255: step 1378, loss 0.150394, acc 0.96
2016-09-07T05:06:49.619459: step 1379, loss 0.07603, acc 0.98
2016-09-07T05:06:50.324086: step 1380, loss 0.0812244, acc 0.92
2016-09-07T05:06:51.022618: step 1381, loss 0.0658805, acc 0.98
2016-09-07T05:06:51.740123: step 1382, loss 0.0978434, acc 0.92
2016-09-07T05:06:52.438779: step 1383, loss 0.0211041, acc 1
2016-09-07T05:06:53.118738: step 1384, loss 0.0567135, acc 0.98
2016-09-07T05:06:53.817290: step 1385, loss 0.133701, acc 0.92
2016-09-07T05:06:54.494592: step 1386, loss 0.150888, acc 0.92
2016-09-07T05:06:55.210486: step 1387, loss 0.0351624, acc 0.98
2016-09-07T05:06:55.892686: step 1388, loss 0.0595527, acc 0.98
2016-09-07T05:06:56.589289: step 1389, loss 0.0698822, acc 1
2016-09-07T05:06:57.287692: step 1390, loss 0.148951, acc 0.96
2016-09-07T05:06:57.984839: step 1391, loss 0.0544693, acc 0.98
2016-09-07T05:06:58.690742: step 1392, loss 0.0767189, acc 0.98
2016-09-07T05:06:59.359439: step 1393, loss 0.0928231, acc 0.94
2016-09-07T05:07:00.081736: step 1394, loss 0.132818, acc 0.94
2016-09-07T05:07:00.814201: step 1395, loss 0.0511989, acc 1
2016-09-07T05:07:01.498490: step 1396, loss 0.0943261, acc 0.92
2016-09-07T05:07:02.203057: step 1397, loss 0.196581, acc 0.9
2016-09-07T05:07:02.899097: step 1398, loss 0.0228935, acc 1
2016-09-07T05:07:03.623449: step 1399, loss 0.0597326, acc 0.98
2016-09-07T05:07:04.300605: step 1400, loss 0.0206244, acc 1

Evaluation:
2016-09-07T05:07:08.004511: step 1400, loss 1.1743, acc 0.755159

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-1400

2016-09-07T05:07:09.775722: step 1401, loss 0.0615588, acc 1
2016-09-07T05:07:10.477767: step 1402, loss 0.0326129, acc 0.98
2016-09-07T05:07:11.168576: step 1403, loss 0.0492412, acc 0.98
2016-09-07T05:07:11.880900: step 1404, loss 0.0856467, acc 0.96
2016-09-07T05:07:12.564845: step 1405, loss 0.0658479, acc 0.96
2016-09-07T05:07:13.286527: step 1406, loss 0.0418858, acc 0.96
2016-09-07T05:07:13.956975: step 1407, loss 0.117911, acc 0.94
2016-09-07T05:07:14.629776: step 1408, loss 0.0839007, acc 0.92
2016-09-07T05:07:15.313846: step 1409, loss 0.0691879, acc 0.96
2016-09-07T05:07:16.012583: step 1410, loss 0.069441, acc 0.96
2016-09-07T05:07:16.709704: step 1411, loss 0.0319484, acc 1
2016-09-07T05:07:17.398716: step 1412, loss 0.0989093, acc 0.92
2016-09-07T05:07:18.108264: step 1413, loss 0.0333293, acc 1
2016-09-07T05:07:18.786694: step 1414, loss 0.149639, acc 0.9
2016-09-07T05:07:19.482615: step 1415, loss 0.106485, acc 0.98
2016-09-07T05:07:20.170977: step 1416, loss 0.0802163, acc 0.96
2016-09-07T05:07:20.863389: step 1417, loss 0.0729793, acc 0.94
2016-09-07T05:07:21.552108: step 1418, loss 0.0579893, acc 0.98
2016-09-07T05:07:22.218866: step 1419, loss 0.0378393, acc 0.98
2016-09-07T05:07:22.947903: step 1420, loss 0.0284372, acc 1
2016-09-07T05:07:23.638571: step 1421, loss 0.139883, acc 0.96
2016-09-07T05:07:24.358684: step 1422, loss 0.033082, acc 0.98
2016-09-07T05:07:25.072084: step 1423, loss 0.156557, acc 0.92
2016-09-07T05:07:25.766400: step 1424, loss 0.0707679, acc 0.96
2016-09-07T05:07:26.495923: step 1425, loss 0.0690294, acc 0.96
2016-09-07T05:07:27.198654: step 1426, loss 0.0546946, acc 0.98
2016-09-07T05:07:27.907562: step 1427, loss 0.0877419, acc 0.94
2016-09-07T05:07:28.613581: step 1428, loss 0.123972, acc 0.92
2016-09-07T05:07:29.320750: step 1429, loss 0.0894074, acc 0.92
2016-09-07T05:07:30.028529: step 1430, loss 0.152836, acc 0.96
2016-09-07T05:07:30.714288: step 1431, loss 0.0179124, acc 1
2016-09-07T05:07:31.401728: step 1432, loss 0.115095, acc 0.92
2016-09-07T05:07:32.094392: step 1433, loss 0.168093, acc 0.92
2016-09-07T05:07:32.783878: step 1434, loss 0.0722026, acc 0.98
2016-09-07T05:07:33.486812: step 1435, loss 0.0542409, acc 0.96
2016-09-07T05:07:34.190380: step 1436, loss 0.0669557, acc 0.98
2016-09-07T05:07:34.906110: step 1437, loss 0.0846477, acc 0.92
2016-09-07T05:07:35.598479: step 1438, loss 0.0980863, acc 0.96
2016-09-07T05:07:36.296925: step 1439, loss 0.0620318, acc 0.98
2016-09-07T05:07:36.997670: step 1440, loss 0.127337, acc 0.92
2016-09-07T05:07:37.712208: step 1441, loss 0.0946022, acc 0.92
2016-09-07T05:07:38.410163: step 1442, loss 0.126024, acc 0.92
2016-09-07T05:07:39.082924: step 1443, loss 0.0798769, acc 0.96
2016-09-07T05:07:39.791385: step 1444, loss 0.0726659, acc 0.94
2016-09-07T05:07:40.488524: step 1445, loss 0.0835075, acc 0.96
2016-09-07T05:07:41.183950: step 1446, loss 0.0796028, acc 0.92
2016-09-07T05:07:41.885830: step 1447, loss 0.0530736, acc 0.98
2016-09-07T05:07:42.573096: step 1448, loss 0.077593, acc 0.98
2016-09-07T05:07:43.278915: step 1449, loss 0.017061, acc 1
2016-09-07T05:07:43.963162: step 1450, loss 0.0599886, acc 0.96
2016-09-07T05:07:44.669220: step 1451, loss 0.0543061, acc 0.96
2016-09-07T05:07:45.347564: step 1452, loss 0.037466, acc 0.98
2016-09-07T05:07:46.039933: step 1453, loss 0.178273, acc 0.88
2016-09-07T05:07:46.731803: step 1454, loss 0.143877, acc 0.94
2016-09-07T05:07:47.426243: step 1455, loss 0.127106, acc 0.92
2016-09-07T05:07:48.148079: step 1456, loss 0.0388374, acc 0.98
2016-09-07T05:07:48.830673: step 1457, loss 0.0978091, acc 0.98
2016-09-07T05:07:49.524810: step 1458, loss 0.0947384, acc 0.96
2016-09-07T05:07:50.220827: step 1459, loss 0.0695816, acc 0.96
2016-09-07T05:07:50.916396: step 1460, loss 0.160495, acc 0.94
2016-09-07T05:07:51.614788: step 1461, loss 0.0431473, acc 0.98
2016-09-07T05:07:52.293575: step 1462, loss 0.0466566, acc 0.96
2016-09-07T05:07:53.006118: step 1463, loss 0.0733017, acc 0.96
2016-09-07T05:07:53.692691: step 1464, loss 0.105401, acc 0.96
2016-09-07T05:07:54.386546: step 1465, loss 0.055515, acc 0.98
2016-09-07T05:07:55.069746: step 1466, loss 0.0325265, acc 1
2016-09-07T05:07:55.751250: step 1467, loss 0.0452439, acc 0.96
2016-09-07T05:07:56.477731: step 1468, loss 0.0597055, acc 0.98
2016-09-07T05:07:57.163947: step 1469, loss 0.101075, acc 0.96
2016-09-07T05:07:57.871703: step 1470, loss 0.0505905, acc 0.98
2016-09-07T05:07:58.568440: step 1471, loss 0.160198, acc 0.92
2016-09-07T05:07:59.275418: step 1472, loss 0.0395379, acc 0.98
2016-09-07T05:07:59.969638: step 1473, loss 0.0781314, acc 0.94
2016-09-07T05:08:00.692318: step 1474, loss 0.0594424, acc 0.98
2016-09-07T05:08:01.423067: step 1475, loss 0.0532875, acc 0.98
2016-09-07T05:08:02.137449: step 1476, loss 0.125198, acc 0.94
2016-09-07T05:08:02.841312: step 1477, loss 0.0778875, acc 0.92
2016-09-07T05:08:03.545282: step 1478, loss 0.120384, acc 0.92
2016-09-07T05:08:04.231175: step 1479, loss 0.0969004, acc 0.94
2016-09-07T05:08:04.941230: step 1480, loss 0.0583428, acc 0.98
2016-09-07T05:08:05.605367: step 1481, loss 0.0710539, acc 0.98
2016-09-07T05:08:06.292269: step 1482, loss 0.0242965, acc 1
2016-09-07T05:08:06.991601: step 1483, loss 0.0396183, acc 1
2016-09-07T05:08:07.686353: step 1484, loss 0.207108, acc 0.92
2016-09-07T05:08:08.373664: step 1485, loss 0.0467937, acc 0.98
2016-09-07T05:08:09.052783: step 1486, loss 0.162882, acc 0.92
2016-09-07T05:08:09.778732: step 1487, loss 0.0399214, acc 0.98
2016-09-07T05:08:10.474580: step 1488, loss 0.0357447, acc 1
2016-09-07T05:08:11.165989: step 1489, loss 0.0491192, acc 1
2016-09-07T05:08:11.842303: step 1490, loss 0.0594541, acc 0.98
2016-09-07T05:08:12.530705: step 1491, loss 0.109008, acc 0.94
2016-09-07T05:08:13.239053: step 1492, loss 0.0412364, acc 0.98
2016-09-07T05:08:13.913960: step 1493, loss 0.0477731, acc 1
2016-09-07T05:08:14.634871: step 1494, loss 0.114377, acc 0.98
2016-09-07T05:08:15.335644: step 1495, loss 0.0625469, acc 0.98
2016-09-07T05:08:16.022341: step 1496, loss 0.135471, acc 0.96
2016-09-07T05:08:16.712444: step 1497, loss 0.107475, acc 0.94
2016-09-07T05:08:17.417233: step 1498, loss 0.154423, acc 0.92
2016-09-07T05:08:18.128788: step 1499, loss 0.0472295, acc 0.98
2016-09-07T05:08:18.806810: step 1500, loss 0.017401, acc 1

Evaluation:
2016-09-07T05:08:22.374788: step 1500, loss 1.14022, acc 0.757974

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-1500

2016-09-07T05:08:24.184261: step 1501, loss 0.0821693, acc 0.92
2016-09-07T05:08:24.862747: step 1502, loss 0.0408811, acc 0.98
2016-09-07T05:08:25.607799: step 1503, loss 0.0405971, acc 0.98
2016-09-07T05:08:26.315850: step 1504, loss 0.0826828, acc 0.98
2016-09-07T05:08:27.020635: step 1505, loss 0.0360728, acc 1
2016-09-07T05:08:27.719977: step 1506, loss 0.113402, acc 0.94
2016-09-07T05:08:28.403177: step 1507, loss 0.0846292, acc 0.94
2016-09-07T05:08:29.114342: step 1508, loss 0.0200712, acc 1
2016-09-07T05:08:29.808728: step 1509, loss 0.145223, acc 0.96
2016-09-07T05:08:30.503893: step 1510, loss 0.128456, acc 0.96
2016-09-07T05:08:31.193112: step 1511, loss 0.0505581, acc 0.96
2016-09-07T05:08:31.908212: step 1512, loss 0.0341552, acc 0.98
2016-09-07T05:08:32.662050: step 1513, loss 0.144236, acc 0.94
2016-09-07T05:08:33.360486: step 1514, loss 0.0724752, acc 0.96
2016-09-07T05:08:34.052066: step 1515, loss 0.0526363, acc 0.98
2016-09-07T05:08:34.751684: step 1516, loss 0.145095, acc 0.96
2016-09-07T05:08:35.477961: step 1517, loss 0.0281631, acc 1
2016-09-07T05:08:36.181609: step 1518, loss 0.0608454, acc 0.96
2016-09-07T05:08:36.870025: step 1519, loss 0.218565, acc 0.94
2016-09-07T05:08:37.560811: step 1520, loss 0.189647, acc 0.9
2016-09-07T05:08:38.254080: step 1521, loss 0.0487796, acc 1
2016-09-07T05:08:38.961184: step 1522, loss 0.0654203, acc 0.98
2016-09-07T05:08:39.680006: step 1523, loss 0.127349, acc 0.9
2016-09-07T05:08:40.364167: step 1524, loss 0.0928527, acc 0.98
2016-09-07T05:08:41.057258: step 1525, loss 0.134317, acc 0.94
2016-09-07T05:08:41.730448: step 1526, loss 0.0964093, acc 0.96
2016-09-07T05:08:42.424532: step 1527, loss 0.0957154, acc 0.94
2016-09-07T05:08:43.139464: step 1528, loss 0.137019, acc 0.96
2016-09-07T05:08:43.823430: step 1529, loss 0.0837747, acc 0.96
2016-09-07T05:08:44.515615: step 1530, loss 0.0782247, acc 0.96
2016-09-07T05:08:45.216122: step 1531, loss 0.0842039, acc 0.98
2016-09-07T05:08:45.935040: step 1532, loss 0.0522768, acc 0.96
2016-09-07T05:08:46.631138: step 1533, loss 0.051528, acc 0.98
2016-09-07T05:08:47.326312: step 1534, loss 0.109728, acc 0.96
2016-09-07T05:08:48.009751: step 1535, loss 0.0484301, acc 0.98
2016-09-07T05:08:48.664519: step 1536, loss 0.0648392, acc 0.954545
2016-09-07T05:08:49.381553: step 1537, loss 0.035134, acc 0.98
2016-09-07T05:08:50.102241: step 1538, loss 0.0439513, acc 0.98
2016-09-07T05:08:50.795117: step 1539, loss 0.160082, acc 0.92
2016-09-07T05:08:51.492832: step 1540, loss 0.0757042, acc 0.96
2016-09-07T05:08:52.173852: step 1541, loss 0.0874904, acc 0.96
2016-09-07T05:08:52.863955: step 1542, loss 0.110952, acc 0.96
2016-09-07T05:08:53.541830: step 1543, loss 0.0360901, acc 0.98
2016-09-07T05:08:54.249433: step 1544, loss 0.0722285, acc 0.94
2016-09-07T05:08:54.934369: step 1545, loss 0.0645355, acc 0.96
2016-09-07T05:08:55.619132: step 1546, loss 0.0407426, acc 0.98
2016-09-07T05:08:56.307665: step 1547, loss 0.00718928, acc 1
2016-09-07T05:08:56.996394: step 1548, loss 0.076227, acc 0.98
2016-09-07T05:08:57.693483: step 1549, loss 0.143314, acc 0.92
2016-09-07T05:08:58.384597: step 1550, loss 0.135951, acc 0.94
2016-09-07T05:08:59.081311: step 1551, loss 0.00723778, acc 1
2016-09-07T05:08:59.769891: step 1552, loss 0.133299, acc 0.96
2016-09-07T05:09:00.478525: step 1553, loss 0.0392082, acc 1
2016-09-07T05:09:01.167640: step 1554, loss 0.0421114, acc 0.98
2016-09-07T05:09:01.859046: step 1555, loss 0.176945, acc 0.9
2016-09-07T05:09:02.557940: step 1556, loss 0.0122971, acc 1
2016-09-07T05:09:03.233552: step 1557, loss 0.135893, acc 0.98
2016-09-07T05:09:03.920300: step 1558, loss 0.0586339, acc 1
2016-09-07T05:09:04.603839: step 1559, loss 0.0472339, acc 0.96
2016-09-07T05:09:05.287358: step 1560, loss 0.0239704, acc 1
2016-09-07T05:09:05.980962: step 1561, loss 0.0726338, acc 0.98
2016-09-07T05:09:06.681507: step 1562, loss 0.0253187, acc 0.98
2016-09-07T05:09:07.381595: step 1563, loss 0.0361493, acc 0.98
2016-09-07T05:09:08.065282: step 1564, loss 0.0468464, acc 0.96
2016-09-07T05:09:08.782431: step 1565, loss 0.071801, acc 0.98
2016-09-07T05:09:09.490033: step 1566, loss 0.0187049, acc 1
2016-09-07T05:09:10.172432: step 1567, loss 0.0336869, acc 0.98
2016-09-07T05:09:10.872069: step 1568, loss 0.0608827, acc 1
2016-09-07T05:09:11.553524: step 1569, loss 0.0377744, acc 0.98
2016-09-07T05:09:12.243156: step 1570, loss 0.222628, acc 0.9
2016-09-07T05:09:12.920824: step 1571, loss 0.0354048, acc 0.98
2016-09-07T05:09:13.604691: step 1572, loss 0.188864, acc 0.94
2016-09-07T05:09:14.284676: step 1573, loss 0.0970226, acc 0.96
2016-09-07T05:09:14.942628: step 1574, loss 0.0304117, acc 0.98
2016-09-07T05:09:15.628252: step 1575, loss 0.0208208, acc 1
2016-09-07T05:09:16.311708: step 1576, loss 0.00355006, acc 1
2016-09-07T05:09:17.021239: step 1577, loss 0.0559598, acc 0.96
2016-09-07T05:09:17.707112: step 1578, loss 0.0513776, acc 0.98
2016-09-07T05:09:18.414893: step 1579, loss 0.065209, acc 0.98
2016-09-07T05:09:19.101513: step 1580, loss 0.0474163, acc 0.96
2016-09-07T05:09:19.783315: step 1581, loss 0.0699048, acc 0.96
2016-09-07T05:09:20.484692: step 1582, loss 0.0313429, acc 1
2016-09-07T05:09:21.191318: step 1583, loss 0.0520596, acc 0.96
2016-09-07T05:09:21.904304: step 1584, loss 0.228575, acc 0.96
2016-09-07T05:09:22.587650: step 1585, loss 0.0218899, acc 1
2016-09-07T05:09:23.267297: step 1586, loss 0.0279915, acc 1
2016-09-07T05:09:23.942652: step 1587, loss 0.0998135, acc 0.96
2016-09-07T05:09:24.621342: step 1588, loss 0.0639919, acc 0.96
2016-09-07T05:09:25.298916: step 1589, loss 0.0143153, acc 1
2016-09-07T05:09:25.993289: step 1590, loss 0.0880523, acc 0.94
2016-09-07T05:09:26.718106: step 1591, loss 0.035055, acc 0.98
2016-09-07T05:09:27.403757: step 1592, loss 0.158566, acc 0.94
2016-09-07T05:09:28.083949: step 1593, loss 0.046125, acc 0.96
2016-09-07T05:09:28.805919: step 1594, loss 0.0907142, acc 0.96
2016-09-07T05:09:29.536016: step 1595, loss 0.142336, acc 0.96
2016-09-07T05:09:30.246125: step 1596, loss 0.093578, acc 0.94
2016-09-07T05:09:30.923104: step 1597, loss 0.0777742, acc 0.96
2016-09-07T05:09:31.615113: step 1598, loss 0.0374948, acc 0.98
2016-09-07T05:09:32.320429: step 1599, loss 0.0537078, acc 0.98
2016-09-07T05:09:32.986387: step 1600, loss 0.0306336, acc 1

Evaluation:
2016-09-07T05:09:36.583182: step 1600, loss 1.06322, acc 0.753283

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-1600

2016-09-07T05:09:38.376844: step 1601, loss 0.126326, acc 0.98
2016-09-07T05:09:39.059196: step 1602, loss 0.0903843, acc 0.96
2016-09-07T05:09:39.753961: step 1603, loss 0.107994, acc 0.92
2016-09-07T05:09:40.445568: step 1604, loss 0.029572, acc 0.98
2016-09-07T05:09:41.147908: step 1605, loss 0.0797687, acc 0.96
2016-09-07T05:09:41.824681: step 1606, loss 0.0443488, acc 0.98
2016-09-07T05:09:42.518479: step 1607, loss 0.0288628, acc 1
2016-09-07T05:09:43.234041: step 1608, loss 0.0724206, acc 0.98
2016-09-07T05:09:43.930440: step 1609, loss 0.0280672, acc 0.98
2016-09-07T05:09:44.635514: step 1610, loss 0.0661245, acc 0.96
2016-09-07T05:09:45.311904: step 1611, loss 0.0211124, acc 0.98
2016-09-07T05:09:46.016872: step 1612, loss 0.0486214, acc 0.98
2016-09-07T05:09:46.711526: step 1613, loss 0.160341, acc 0.96
2016-09-07T05:09:47.394802: step 1614, loss 0.0475841, acc 0.98
2016-09-07T05:09:48.086959: step 1615, loss 0.03247, acc 1
2016-09-07T05:09:48.789443: step 1616, loss 0.055757, acc 0.98
2016-09-07T05:09:49.483510: step 1617, loss 0.123655, acc 0.98
2016-09-07T05:09:50.166163: step 1618, loss 0.0597986, acc 0.96
2016-09-07T05:09:50.875524: step 1619, loss 0.0736714, acc 0.96
2016-09-07T05:09:51.561118: step 1620, loss 0.0972171, acc 0.98
2016-09-07T05:09:52.258163: step 1621, loss 0.024654, acc 0.98
2016-09-07T05:09:52.946639: step 1622, loss 0.0268898, acc 0.98
2016-09-07T05:09:53.641620: step 1623, loss 0.00910332, acc 1
2016-09-07T05:09:54.340486: step 1624, loss 0.0337462, acc 0.98
2016-09-07T05:09:55.016876: step 1625, loss 0.129674, acc 0.96
2016-09-07T05:09:55.717428: step 1626, loss 0.0201367, acc 1
2016-09-07T05:09:56.417932: step 1627, loss 0.0606405, acc 0.98
2016-09-07T05:09:57.134746: step 1628, loss 0.0683143, acc 0.94
2016-09-07T05:09:57.833786: step 1629, loss 0.0109831, acc 1
2016-09-07T05:09:58.508006: step 1630, loss 0.0732094, acc 0.96
2016-09-07T05:09:59.215025: step 1631, loss 0.0699584, acc 0.96
2016-09-07T05:09:59.891312: step 1632, loss 0.0369253, acc 0.98
2016-09-07T05:10:00.591381: step 1633, loss 0.0820287, acc 0.94
2016-09-07T05:10:01.284422: step 1634, loss 0.103094, acc 0.92
2016-09-07T05:10:01.978232: step 1635, loss 0.0447549, acc 0.96
2016-09-07T05:10:02.675530: step 1636, loss 0.0267349, acc 0.98
2016-09-07T05:10:03.358622: step 1637, loss 0.0362437, acc 0.98
2016-09-07T05:10:04.052434: step 1638, loss 0.0458544, acc 0.98
2016-09-07T05:10:04.739142: step 1639, loss 0.0447332, acc 0.98
2016-09-07T05:10:05.433643: step 1640, loss 0.0526502, acc 0.98
2016-09-07T05:10:06.114711: step 1641, loss 0.0589043, acc 0.96
2016-09-07T05:10:06.813590: step 1642, loss 0.0451338, acc 1
2016-09-07T05:10:07.502717: step 1643, loss 0.118741, acc 0.98
2016-09-07T05:10:08.182117: step 1644, loss 0.114686, acc 0.96
2016-09-07T05:10:08.897583: step 1645, loss 0.0512737, acc 0.98
2016-09-07T05:10:09.587225: step 1646, loss 0.0904374, acc 0.96
2016-09-07T05:10:10.273384: step 1647, loss 0.0378574, acc 1
2016-09-07T05:10:10.962164: step 1648, loss 0.105524, acc 0.92
2016-09-07T05:10:11.664510: step 1649, loss 0.124032, acc 0.94
2016-09-07T05:10:12.370638: step 1650, loss 0.134059, acc 0.94
2016-09-07T05:10:13.044423: step 1651, loss 0.057047, acc 0.96
2016-09-07T05:10:13.758351: step 1652, loss 0.123921, acc 0.98
2016-09-07T05:10:14.439781: step 1653, loss 0.0837255, acc 0.94
2016-09-07T05:10:15.122376: step 1654, loss 0.0694825, acc 0.96
2016-09-07T05:10:15.832546: step 1655, loss 0.0699428, acc 0.98
2016-09-07T05:10:16.517057: step 1656, loss 0.136288, acc 0.9
2016-09-07T05:10:17.253298: step 1657, loss 0.0703611, acc 0.92
2016-09-07T05:10:17.937280: step 1658, loss 0.0191988, acc 1
2016-09-07T05:10:18.628251: step 1659, loss 0.0493981, acc 0.98
2016-09-07T05:10:19.333164: step 1660, loss 0.0815803, acc 0.96
2016-09-07T05:10:20.011006: step 1661, loss 0.0269505, acc 1
2016-09-07T05:10:20.708644: step 1662, loss 0.0297669, acc 0.98
2016-09-07T05:10:21.376578: step 1663, loss 0.0326076, acc 0.98
2016-09-07T05:10:22.104648: step 1664, loss 0.0356221, acc 0.98
2016-09-07T05:10:22.779334: step 1665, loss 0.140923, acc 0.88
2016-09-07T05:10:23.471436: step 1666, loss 0.0556716, acc 0.98
2016-09-07T05:10:24.162008: step 1667, loss 0.1028, acc 0.94
2016-09-07T05:10:24.847383: step 1668, loss 0.0771158, acc 0.92
2016-09-07T05:10:25.548966: step 1669, loss 0.0526077, acc 1
2016-09-07T05:10:26.218803: step 1670, loss 0.0361684, acc 0.98
2016-09-07T05:10:26.941885: step 1671, loss 0.0636412, acc 0.96
2016-09-07T05:10:27.648423: step 1672, loss 0.157784, acc 0.92
2016-09-07T05:10:28.346064: step 1673, loss 0.113132, acc 0.94
2016-09-07T05:10:29.034707: step 1674, loss 0.0364377, acc 0.98
2016-09-07T05:10:29.734361: step 1675, loss 0.0370307, acc 1
2016-09-07T05:10:30.454662: step 1676, loss 0.0377149, acc 0.98
2016-09-07T05:10:31.120392: step 1677, loss 0.0702551, acc 0.98
2016-09-07T05:10:31.831798: step 1678, loss 0.0331057, acc 0.98
2016-09-07T05:10:32.528019: step 1679, loss 0.062735, acc 0.96
2016-09-07T05:10:33.215718: step 1680, loss 0.0284215, acc 0.98
2016-09-07T05:10:33.903015: step 1681, loss 0.0903563, acc 0.94
2016-09-07T05:10:34.592858: step 1682, loss 0.109461, acc 0.94
2016-09-07T05:10:35.293826: step 1683, loss 0.170243, acc 0.96
2016-09-07T05:10:35.960487: step 1684, loss 0.101177, acc 0.96
2016-09-07T05:10:36.678832: step 1685, loss 0.116852, acc 0.96
2016-09-07T05:10:37.379089: step 1686, loss 0.0409363, acc 0.98
2016-09-07T05:10:38.067817: step 1687, loss 0.0566721, acc 1
2016-09-07T05:10:38.779993: step 1688, loss 0.09141, acc 0.94
2016-09-07T05:10:39.456760: step 1689, loss 0.105742, acc 0.96
2016-09-07T05:10:40.175361: step 1690, loss 0.127453, acc 0.96
2016-09-07T05:10:40.879798: step 1691, loss 0.0769786, acc 0.96
2016-09-07T05:10:41.583797: step 1692, loss 0.00967018, acc 1
2016-09-07T05:10:42.289653: step 1693, loss 0.0115806, acc 1
2016-09-07T05:10:42.981732: step 1694, loss 0.074317, acc 0.96
2016-09-07T05:10:43.684527: step 1695, loss 0.0943391, acc 0.98
2016-09-07T05:10:44.371166: step 1696, loss 0.0296346, acc 0.98
2016-09-07T05:10:45.060446: step 1697, loss 0.0515367, acc 0.98
2016-09-07T05:10:45.738522: step 1698, loss 0.0311729, acc 1
2016-09-07T05:10:46.440431: step 1699, loss 0.0352668, acc 0.98
2016-09-07T05:10:47.130284: step 1700, loss 0.0711893, acc 0.96

Evaluation:
2016-09-07T05:10:50.828689: step 1700, loss 1.25471, acc 0.761726

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-1700

2016-09-07T05:10:52.570696: step 1701, loss 0.0105428, acc 1
2016-09-07T05:10:53.288529: step 1702, loss 0.0494025, acc 0.96
2016-09-07T05:10:53.985364: step 1703, loss 0.0335035, acc 0.96
2016-09-07T05:10:54.682029: step 1704, loss 0.110355, acc 0.96
2016-09-07T05:10:55.372181: step 1705, loss 0.0707265, acc 0.96
2016-09-07T05:10:56.056520: step 1706, loss 0.0482509, acc 0.98
2016-09-07T05:10:56.752124: step 1707, loss 0.0417923, acc 0.98
2016-09-07T05:10:57.427871: step 1708, loss 0.047457, acc 0.96
2016-09-07T05:10:58.142700: step 1709, loss 0.0730192, acc 0.96
2016-09-07T05:10:58.842647: step 1710, loss 0.050704, acc 0.98
2016-09-07T05:10:59.522779: step 1711, loss 0.0502549, acc 0.98
2016-09-07T05:11:00.239976: step 1712, loss 0.0723486, acc 0.96
2016-09-07T05:11:00.914832: step 1713, loss 0.0136545, acc 1
2016-09-07T05:11:01.646796: step 1714, loss 0.0248073, acc 1
2016-09-07T05:11:02.350201: step 1715, loss 0.039584, acc 0.98
2016-09-07T05:11:03.044058: step 1716, loss 0.121883, acc 0.96
2016-09-07T05:11:03.734950: step 1717, loss 0.0815788, acc 0.98
2016-09-07T05:11:04.431300: step 1718, loss 0.0460833, acc 0.98
2016-09-07T05:11:05.155958: step 1719, loss 0.0591444, acc 0.96
2016-09-07T05:11:05.846711: step 1720, loss 0.0589337, acc 0.98
2016-09-07T05:11:06.563022: step 1721, loss 0.0364502, acc 1
2016-09-07T05:11:07.245409: step 1722, loss 0.106454, acc 0.96
2016-09-07T05:11:07.952248: step 1723, loss 0.048174, acc 0.98
2016-09-07T05:11:08.657151: step 1724, loss 0.00910988, acc 1
2016-09-07T05:11:09.359663: step 1725, loss 0.11519, acc 0.96
2016-09-07T05:11:10.054573: step 1726, loss 0.102205, acc 0.98
2016-09-07T05:11:10.716691: step 1727, loss 0.101283, acc 0.92
2016-09-07T05:11:11.372242: step 1728, loss 0.0136125, acc 1
2016-09-07T05:11:12.052751: step 1729, loss 0.0241221, acc 1
2016-09-07T05:11:12.747364: step 1730, loss 0.0329119, acc 0.98
2016-09-07T05:11:13.442370: step 1731, loss 0.146649, acc 0.92
2016-09-07T05:11:14.125666: step 1732, loss 0.141431, acc 0.94
2016-09-07T05:11:14.824859: step 1733, loss 0.0735877, acc 0.96
2016-09-07T05:11:15.515506: step 1734, loss 0.0470095, acc 0.98
2016-09-07T05:11:16.248295: step 1735, loss 0.00565691, acc 1
2016-09-07T05:11:16.942927: step 1736, loss 0.0258933, acc 1
2016-09-07T05:11:17.641780: step 1737, loss 0.0110244, acc 1
2016-09-07T05:11:18.343018: step 1738, loss 0.0636508, acc 0.98
2016-09-07T05:11:19.042129: step 1739, loss 0.034824, acc 1
2016-09-07T05:11:19.755562: step 1740, loss 0.0516583, acc 0.98
2016-09-07T05:11:20.435986: step 1741, loss 0.18031, acc 0.9
2016-09-07T05:11:21.138769: step 1742, loss 0.0990887, acc 0.94
2016-09-07T05:11:21.847414: step 1743, loss 0.0253939, acc 0.98
2016-09-07T05:11:22.518482: step 1744, loss 0.0178449, acc 1
2016-09-07T05:11:23.206854: step 1745, loss 0.0656697, acc 0.98
2016-09-07T05:11:23.892814: step 1746, loss 0.0566226, acc 0.98
2016-09-07T05:11:24.604004: step 1747, loss 0.0659906, acc 0.96
2016-09-07T05:11:25.294851: step 1748, loss 0.0247118, acc 0.98
2016-09-07T05:11:25.988064: step 1749, loss 0.064852, acc 0.96
2016-09-07T05:11:26.700667: step 1750, loss 0.0776517, acc 0.96
2016-09-07T05:11:27.392617: step 1751, loss 0.057135, acc 0.98
2016-09-07T05:11:28.094910: step 1752, loss 0.0662341, acc 0.96
2016-09-07T05:11:28.774078: step 1753, loss 0.053994, acc 0.98
2016-09-07T05:11:29.490770: step 1754, loss 0.0190699, acc 1
2016-09-07T05:11:30.200990: step 1755, loss 0.00261749, acc 1
2016-09-07T05:11:30.893090: step 1756, loss 0.044846, acc 0.98
2016-09-07T05:11:31.576086: step 1757, loss 0.0425881, acc 1
2016-09-07T05:11:32.276777: step 1758, loss 0.0798178, acc 0.98
2016-09-07T05:11:32.988591: step 1759, loss 0.0593742, acc 0.98
2016-09-07T05:11:33.665444: step 1760, loss 0.156424, acc 0.96
2016-09-07T05:11:34.381858: step 1761, loss 0.0480601, acc 0.98
2016-09-07T05:11:35.070002: step 1762, loss 0.0440798, acc 0.98
2016-09-07T05:11:35.778593: step 1763, loss 0.0487122, acc 0.96
2016-09-07T05:11:36.467819: step 1764, loss 0.0686986, acc 0.98
2016-09-07T05:11:37.142205: step 1765, loss 0.128831, acc 0.98
2016-09-07T05:11:37.848461: step 1766, loss 0.0566361, acc 0.98
2016-09-07T05:11:38.541069: step 1767, loss 0.0427792, acc 0.98
2016-09-07T05:11:39.240683: step 1768, loss 0.036015, acc 0.98
2016-09-07T05:11:39.952195: step 1769, loss 0.0900702, acc 0.96
2016-09-07T05:11:40.632065: step 1770, loss 0.0347396, acc 1
2016-09-07T05:11:41.312307: step 1771, loss 0.134255, acc 0.94
2016-09-07T05:11:41.978051: step 1772, loss 0.029177, acc 0.98
2016-09-07T05:11:42.693674: step 1773, loss 0.0207877, acc 1
2016-09-07T05:11:43.387331: step 1774, loss 0.0648128, acc 0.96
2016-09-07T05:11:44.086190: step 1775, loss 0.0871468, acc 0.98
2016-09-07T05:11:44.780886: step 1776, loss 0.0558525, acc 0.98
2016-09-07T05:11:45.499487: step 1777, loss 0.0183607, acc 1
2016-09-07T05:11:46.201800: step 1778, loss 0.0155898, acc 1
2016-09-07T05:11:46.904854: step 1779, loss 0.0136435, acc 1
2016-09-07T05:11:47.595775: step 1780, loss 0.0811056, acc 0.92
2016-09-07T05:11:48.294142: step 1781, loss 0.00487679, acc 1
2016-09-07T05:11:48.985008: step 1782, loss 0.0356439, acc 0.98
2016-09-07T05:11:49.677947: step 1783, loss 0.017889, acc 0.98
2016-09-07T05:11:50.364802: step 1784, loss 0.130339, acc 0.98
2016-09-07T05:11:51.086163: step 1785, loss 0.089707, acc 0.96
2016-09-07T05:11:51.787769: step 1786, loss 0.0346097, acc 0.98
2016-09-07T05:11:52.461927: step 1787, loss 0.0919395, acc 0.98
2016-09-07T05:11:53.146882: step 1788, loss 0.0455001, acc 0.96
2016-09-07T05:11:53.833156: step 1789, loss 0.113079, acc 0.96
2016-09-07T05:11:54.543025: step 1790, loss 0.0220936, acc 0.98
2016-09-07T05:11:55.224721: step 1791, loss 0.080156, acc 0.92
2016-09-07T05:11:55.922576: step 1792, loss 0.0870802, acc 0.96
2016-09-07T05:11:56.620716: step 1793, loss 0.0822844, acc 0.94
2016-09-07T05:11:57.311501: step 1794, loss 0.112867, acc 0.94
2016-09-07T05:11:57.995628: step 1795, loss 0.0111132, acc 1
2016-09-07T05:11:58.676963: step 1796, loss 0.166851, acc 0.94
2016-09-07T05:11:59.385600: step 1797, loss 0.0217696, acc 0.98
2016-09-07T05:12:00.057060: step 1798, loss 0.175352, acc 0.9
2016-09-07T05:12:00.811665: step 1799, loss 0.0728994, acc 0.96
2016-09-07T05:12:01.480518: step 1800, loss 0.0462148, acc 0.98

Evaluation:
2016-09-07T05:12:05.051876: step 1800, loss 1.35883, acc 0.758912

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-1800

2016-09-07T05:12:06.818571: step 1801, loss 0.296408, acc 0.94
2016-09-07T05:12:07.521823: step 1802, loss 0.0562126, acc 0.96
2016-09-07T05:12:08.217748: step 1803, loss 0.0540952, acc 0.98
2016-09-07T05:12:08.922923: step 1804, loss 0.0752195, acc 0.94
2016-09-07T05:12:09.612538: step 1805, loss 0.0609889, acc 0.96
2016-09-07T05:12:10.316762: step 1806, loss 0.103006, acc 0.96
2016-09-07T05:12:11.017634: step 1807, loss 0.054022, acc 0.98
2016-09-07T05:12:11.712283: step 1808, loss 0.168187, acc 0.94
2016-09-07T05:12:12.413355: step 1809, loss 0.0333873, acc 1
2016-09-07T05:12:13.107739: step 1810, loss 0.0421307, acc 1
2016-09-07T05:12:13.812518: step 1811, loss 0.0377384, acc 0.98
2016-09-07T05:12:14.486195: step 1812, loss 0.0649096, acc 0.96
2016-09-07T05:12:15.209897: step 1813, loss 0.0560229, acc 0.98
2016-09-07T05:12:15.912011: step 1814, loss 0.208928, acc 0.92
2016-09-07T05:12:16.619240: step 1815, loss 0.0557651, acc 0.96
2016-09-07T05:12:17.312226: step 1816, loss 0.0641885, acc 0.98
2016-09-07T05:12:17.987218: step 1817, loss 0.0977778, acc 0.96
2016-09-07T05:12:18.712097: step 1818, loss 0.110835, acc 0.94
2016-09-07T05:12:19.404349: step 1819, loss 0.0709946, acc 0.94
2016-09-07T05:12:20.096170: step 1820, loss 0.0396536, acc 0.98
2016-09-07T05:12:20.791891: step 1821, loss 0.0808684, acc 0.94
2016-09-07T05:12:21.474829: step 1822, loss 0.0343304, acc 0.98
2016-09-07T05:12:22.161409: step 1823, loss 0.0642543, acc 0.94
2016-09-07T05:12:22.809450: step 1824, loss 0.0345742, acc 1
2016-09-07T05:12:23.511342: step 1825, loss 0.0478376, acc 0.98
2016-09-07T05:12:24.194558: step 1826, loss 0.0662988, acc 0.98
2016-09-07T05:12:24.886388: step 1827, loss 0.0461786, acc 1
2016-09-07T05:12:25.588695: step 1828, loss 0.0534773, acc 0.96
2016-09-07T05:12:26.287864: step 1829, loss 0.0526882, acc 0.98
2016-09-07T05:12:27.009138: step 1830, loss 0.0400715, acc 1
2016-09-07T05:12:27.687368: step 1831, loss 0.0748904, acc 0.98
2016-09-07T05:12:28.393551: step 1832, loss 0.14654, acc 0.98
2016-09-07T05:12:29.089159: step 1833, loss 0.0514358, acc 0.98
2016-09-07T05:12:29.780886: step 1834, loss 0.0688173, acc 0.96
2016-09-07T05:12:30.468610: step 1835, loss 0.0828036, acc 0.96
2016-09-07T05:12:31.160800: step 1836, loss 0.0464505, acc 0.98
2016-09-07T05:12:31.886319: step 1837, loss 0.073944, acc 0.96
2016-09-07T05:12:32.578392: step 1838, loss 0.03596, acc 0.98
2016-09-07T05:12:33.266157: step 1839, loss 0.177752, acc 0.88
2016-09-07T05:12:33.965973: step 1840, loss 0.0233444, acc 0.98
2016-09-07T05:12:34.685078: step 1841, loss 0.0826225, acc 0.96
2016-09-07T05:12:35.384410: step 1842, loss 0.041951, acc 0.98
2016-09-07T05:12:36.053752: step 1843, loss 0.048673, acc 0.98
2016-09-07T05:12:36.771065: step 1844, loss 0.0631303, acc 0.96
2016-09-07T05:12:37.482901: step 1845, loss 0.0389537, acc 0.98
2016-09-07T05:12:38.223151: step 1846, loss 0.0489241, acc 0.98
2016-09-07T05:12:38.923427: step 1847, loss 0.0276205, acc 0.98
2016-09-07T05:12:39.625982: step 1848, loss 0.130508, acc 0.96
2016-09-07T05:12:40.352616: step 1849, loss 0.0375715, acc 0.98
2016-09-07T05:12:41.038750: step 1850, loss 0.0496183, acc 0.98
2016-09-07T05:12:41.726303: step 1851, loss 0.0721255, acc 0.96
2016-09-07T05:12:42.430524: step 1852, loss 0.0279095, acc 0.98
2016-09-07T05:12:43.133567: step 1853, loss 0.0184649, acc 1
2016-09-07T05:12:43.841476: step 1854, loss 0.0278509, acc 1
2016-09-07T05:12:44.516205: step 1855, loss 0.0945631, acc 0.98
2016-09-07T05:12:45.232867: step 1856, loss 0.0486741, acc 0.98
2016-09-07T05:12:45.931146: step 1857, loss 0.0564886, acc 0.98
2016-09-07T05:12:46.620714: step 1858, loss 0.172743, acc 0.96
2016-09-07T05:12:47.306095: step 1859, loss 0.0538009, acc 0.98
2016-09-07T05:12:47.987798: step 1860, loss 0.0271986, acc 0.98
2016-09-07T05:12:48.677464: step 1861, loss 0.0346834, acc 0.98
2016-09-07T05:12:49.347976: step 1862, loss 0.0277875, acc 0.98
2016-09-07T05:12:50.070876: step 1863, loss 0.0344324, acc 0.98
2016-09-07T05:12:50.773340: step 1864, loss 0.102053, acc 0.94
2016-09-07T05:12:51.484540: step 1865, loss 0.0210728, acc 1
2016-09-07T05:12:52.195698: step 1866, loss 0.037428, acc 0.98
2016-09-07T05:12:52.859980: step 1867, loss 0.0118968, acc 1
2016-09-07T05:12:53.558914: step 1868, loss 0.149421, acc 0.92
2016-09-07T05:12:54.232172: step 1869, loss 0.0918913, acc 0.98
2016-09-07T05:12:54.923370: step 1870, loss 0.119141, acc 0.94
2016-09-07T05:12:55.614321: step 1871, loss 0.0440916, acc 0.98
2016-09-07T05:12:56.303048: step 1872, loss 0.0641747, acc 0.96
2016-09-07T05:12:56.987487: step 1873, loss 0.0215622, acc 0.98
2016-09-07T05:12:57.664500: step 1874, loss 0.124321, acc 0.96
2016-09-07T05:12:58.379792: step 1875, loss 0.0777095, acc 0.96
2016-09-07T05:12:59.062228: step 1876, loss 0.00885337, acc 1
2016-09-07T05:12:59.758288: step 1877, loss 0.0702241, acc 0.98
2016-09-07T05:13:00.494562: step 1878, loss 0.00489418, acc 1
2016-09-07T05:13:01.197115: step 1879, loss 0.042926, acc 0.96
2016-09-07T05:13:01.929670: step 1880, loss 0.0250115, acc 0.98
2016-09-07T05:13:02.621546: step 1881, loss 0.0694828, acc 0.98
2016-09-07T05:13:03.294763: step 1882, loss 0.0465863, acc 1
2016-09-07T05:13:03.985414: step 1883, loss 0.0217607, acc 0.98
2016-09-07T05:13:04.681541: step 1884, loss 0.000341617, acc 1
2016-09-07T05:13:05.379639: step 1885, loss 0.0537788, acc 0.98
2016-09-07T05:13:06.073973: step 1886, loss 0.0538085, acc 0.98
2016-09-07T05:13:06.786320: step 1887, loss 0.0452246, acc 0.98
2016-09-07T05:13:07.475508: step 1888, loss 0.0396295, acc 0.98
2016-09-07T05:13:08.174729: step 1889, loss 0.0176902, acc 1
2016-09-07T05:13:08.848515: step 1890, loss 0.0107898, acc 1
2016-09-07T05:13:09.544330: step 1891, loss 0.000528431, acc 1
2016-09-07T05:13:10.246778: step 1892, loss 0.0763353, acc 0.96
2016-09-07T05:13:10.920268: step 1893, loss 0.0141378, acc 1
2016-09-07T05:13:11.633074: step 1894, loss 0.144032, acc 0.94
2016-09-07T05:13:12.323720: step 1895, loss 0.0315271, acc 1
2016-09-07T05:13:13.010266: step 1896, loss 0.0980235, acc 0.96
2016-09-07T05:13:13.702210: step 1897, loss 0.0463194, acc 0.98
2016-09-07T05:13:14.388618: step 1898, loss 0.116703, acc 0.94
2016-09-07T05:13:15.068028: step 1899, loss 0.0649404, acc 0.98
2016-09-07T05:13:15.743271: step 1900, loss 0.0325994, acc 0.98

Evaluation:
2016-09-07T05:13:19.373002: step 1900, loss 1.32724, acc 0.767355

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-1900

2016-09-07T05:13:21.184209: step 1901, loss 0.0560064, acc 0.98
2016-09-07T05:13:21.864612: step 1902, loss 0.0288188, acc 1
2016-09-07T05:13:22.531557: step 1903, loss 0.0315199, acc 0.98
2016-09-07T05:13:23.224220: step 1904, loss 0.083358, acc 0.94
2016-09-07T05:13:23.928693: step 1905, loss 0.0330339, acc 1
2016-09-07T05:13:24.600362: step 1906, loss 0.0268443, acc 0.98
2016-09-07T05:13:25.304130: step 1907, loss 0.0856153, acc 0.98
2016-09-07T05:13:26.041203: step 1908, loss 0.00693963, acc 1
2016-09-07T05:13:26.717598: step 1909, loss 0.0236995, acc 1
2016-09-07T05:13:27.390630: step 1910, loss 0.0179259, acc 1
2016-09-07T05:13:28.097686: step 1911, loss 0.0297969, acc 0.98
2016-09-07T05:13:28.800003: step 1912, loss 0.0325557, acc 0.98
2016-09-07T05:13:29.498301: step 1913, loss 0.0565791, acc 0.98
2016-09-07T05:13:30.164735: step 1914, loss 0.0275854, acc 0.98
2016-09-07T05:13:30.867304: step 1915, loss 0.140682, acc 0.98
2016-09-07T05:13:31.561564: step 1916, loss 0.00358751, acc 1
2016-09-07T05:13:32.259118: step 1917, loss 0.0156311, acc 1
2016-09-07T05:13:32.949767: step 1918, loss 0.0326007, acc 0.98
2016-09-07T05:13:33.644452: step 1919, loss 0.0467206, acc 0.98
2016-09-07T05:13:34.292620: step 1920, loss 0.122573, acc 0.931818
2016-09-07T05:13:34.961570: step 1921, loss 0.0988735, acc 0.94
2016-09-07T05:13:35.679201: step 1922, loss 0.0252158, acc 0.98
2016-09-07T05:13:36.365308: step 1923, loss 0.01806, acc 0.98
2016-09-07T05:13:37.078530: step 1924, loss 0.0507537, acc 0.96
2016-09-07T05:13:37.766113: step 1925, loss 0.0124903, acc 1
2016-09-07T05:13:38.453033: step 1926, loss 0.0371696, acc 0.98
2016-09-07T05:13:39.150745: step 1927, loss 0.120083, acc 0.94
2016-09-07T05:13:39.851425: step 1928, loss 0.0201848, acc 1
2016-09-07T05:13:40.576771: step 1929, loss 0.0255952, acc 1
2016-09-07T05:13:41.285335: step 1930, loss 0.0159005, acc 1
2016-09-07T05:13:41.978309: step 1931, loss 0.0261258, acc 1
2016-09-07T05:13:42.693611: step 1932, loss 0.0251187, acc 0.98
2016-09-07T05:13:43.360880: step 1933, loss 0.0373145, acc 0.98
2016-09-07T05:13:44.076283: step 1934, loss 0.171671, acc 0.96
2016-09-07T05:13:44.756859: step 1935, loss 0.00240008, acc 1
2016-09-07T05:13:45.444442: step 1936, loss 0.0709362, acc 0.98
2016-09-07T05:13:46.136327: step 1937, loss 0.0479871, acc 0.98
2016-09-07T05:13:46.835505: step 1938, loss 0.0173911, acc 1
2016-09-07T05:13:47.531826: step 1939, loss 0.00848552, acc 1
2016-09-07T05:13:48.211068: step 1940, loss 0.17594, acc 0.94
2016-09-07T05:13:48.909367: step 1941, loss 0.0614741, acc 0.96
2016-09-07T05:13:49.581072: step 1942, loss 0.0375317, acc 0.98
2016-09-07T05:13:50.278142: step 1943, loss 0.044502, acc 0.96
2016-09-07T05:13:50.972434: step 1944, loss 0.138253, acc 0.96
2016-09-07T05:13:51.677186: step 1945, loss 0.0930021, acc 0.94
2016-09-07T05:13:52.380206: step 1946, loss 0.0138184, acc 1
2016-09-07T05:13:53.057380: step 1947, loss 0.0509596, acc 0.96
2016-09-07T05:13:53.806370: step 1948, loss 0.0154215, acc 1
2016-09-07T05:13:54.526268: step 1949, loss 0.0438669, acc 0.98
2016-09-07T05:13:55.218954: step 1950, loss 0.0485827, acc 0.98
2016-09-07T05:13:55.943405: step 1951, loss 0.0318774, acc 0.98
2016-09-07T05:13:56.612412: step 1952, loss 0.0685624, acc 0.96
2016-09-07T05:13:57.340405: step 1953, loss 0.0773041, acc 0.98
2016-09-07T05:13:58.044946: step 1954, loss 0.049801, acc 0.98
2016-09-07T05:13:58.730100: step 1955, loss 0.0387737, acc 0.98
2016-09-07T05:13:59.416209: step 1956, loss 0.0758583, acc 0.96
2016-09-07T05:14:00.116548: step 1957, loss 0.0554877, acc 0.98
2016-09-07T05:14:00.856850: step 1958, loss 0.0315742, acc 1
2016-09-07T05:14:01.534183: step 1959, loss 0.114816, acc 0.98
2016-09-07T05:14:02.207069: step 1960, loss 0.0226414, acc 0.98
2016-09-07T05:14:02.903459: step 1961, loss 0.052576, acc 1
2016-09-07T05:14:03.603231: step 1962, loss 0.0526341, acc 0.96
2016-09-07T05:14:04.293111: step 1963, loss 0.0212871, acc 1
2016-09-07T05:14:04.981102: step 1964, loss 0.0368013, acc 0.98
2016-09-07T05:14:05.685060: step 1965, loss 0.0610106, acc 0.96
2016-09-07T05:14:06.364975: step 1966, loss 0.0676474, acc 0.96
2016-09-07T05:14:07.047781: step 1967, loss 0.0298774, acc 0.98
2016-09-07T05:14:07.760065: step 1968, loss 0.15224, acc 0.96
2016-09-07T05:14:08.435968: step 1969, loss 0.0208764, acc 1
2016-09-07T05:14:09.141742: step 1970, loss 0.0658825, acc 0.98
2016-09-07T05:14:09.823279: step 1971, loss 0.117607, acc 0.94
2016-09-07T05:14:10.528291: step 1972, loss 0.0312616, acc 0.98
2016-09-07T05:14:11.250228: step 1973, loss 0.0343242, acc 0.98
2016-09-07T05:14:11.944774: step 1974, loss 0.0406743, acc 0.98
2016-09-07T05:14:12.629017: step 1975, loss 0.080128, acc 0.94
2016-09-07T05:14:13.341689: step 1976, loss 0.0538768, acc 0.96
2016-09-07T05:14:14.054178: step 1977, loss 0.0474806, acc 0.98
2016-09-07T05:14:14.734070: step 1978, loss 0.021137, acc 1
2016-09-07T05:14:15.435456: step 1979, loss 0.0687238, acc 0.98
2016-09-07T05:14:16.142945: step 1980, loss 0.020983, acc 1
2016-09-07T05:14:16.847613: step 1981, loss 0.152338, acc 0.9
2016-09-07T05:14:17.572817: step 1982, loss 0.0235669, acc 1
2016-09-07T05:14:18.258105: step 1983, loss 0.0579184, acc 0.96
2016-09-07T05:14:18.978900: step 1984, loss 0.0765076, acc 0.96
2016-09-07T05:14:19.674543: step 1985, loss 0.0876355, acc 0.98
2016-09-07T05:14:20.363600: step 1986, loss 0.0985311, acc 0.96
2016-09-07T05:14:21.061576: step 1987, loss 0.0293421, acc 1
2016-09-07T05:14:21.747853: step 1988, loss 0.0390206, acc 0.98
2016-09-07T05:14:22.459468: step 1989, loss 0.0576042, acc 0.98
2016-09-07T05:14:23.116948: step 1990, loss 0.072049, acc 0.96
2016-09-07T05:14:23.830059: step 1991, loss 0.0599854, acc 0.96
2016-09-07T05:14:24.506096: step 1992, loss 0.0148477, acc 1
2016-09-07T05:14:25.208685: step 1993, loss 0.0302147, acc 0.98
2016-09-07T05:14:25.924695: step 1994, loss 0.0765693, acc 0.96
2016-09-07T05:14:26.615068: step 1995, loss 0.0570536, acc 0.96
2016-09-07T05:14:27.315749: step 1996, loss 0.104985, acc 0.94
2016-09-07T05:14:27.990374: step 1997, loss 0.0556422, acc 0.96
2016-09-07T05:14:28.688079: step 1998, loss 0.0599942, acc 0.98
2016-09-07T05:14:29.381052: step 1999, loss 0.0574424, acc 0.98
2016-09-07T05:14:30.058745: step 2000, loss 0.157205, acc 0.94

Evaluation:
2016-09-07T05:14:33.671811: step 2000, loss 1.44372, acc 0.753283

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-2000

2016-09-07T05:14:35.444830: step 2001, loss 0.059621, acc 0.96
2016-09-07T05:14:36.143382: step 2002, loss 0.0197665, acc 1
2016-09-07T05:14:36.829905: step 2003, loss 0.0387999, acc 0.98
2016-09-07T05:14:37.517411: step 2004, loss 0.0511269, acc 0.98
2016-09-07T05:14:38.241972: step 2005, loss 0.0410562, acc 1
2016-09-07T05:14:38.951109: step 2006, loss 0.014126, acc 1
2016-09-07T05:14:39.630416: step 2007, loss 0.0349402, acc 0.98
2016-09-07T05:14:40.302830: step 2008, loss 0.0330594, acc 0.98
2016-09-07T05:14:41.009359: step 2009, loss 0.0816255, acc 0.96
2016-09-07T05:14:41.734583: step 2010, loss 0.0822767, acc 0.94
2016-09-07T05:14:42.400735: step 2011, loss 0.052903, acc 0.98
2016-09-07T05:14:43.084081: step 2012, loss 0.0395612, acc 0.98
2016-09-07T05:14:43.767713: step 2013, loss 0.0327938, acc 1
2016-09-07T05:14:44.444039: step 2014, loss 0.0599, acc 0.98
2016-09-07T05:14:45.135379: step 2015, loss 0.0332105, acc 1
2016-09-07T05:14:45.824882: step 2016, loss 0.0576941, acc 1
2016-09-07T05:14:46.514997: step 2017, loss 0.00192517, acc 1
2016-09-07T05:14:47.185071: step 2018, loss 0.0706235, acc 0.96
2016-09-07T05:14:47.911623: step 2019, loss 0.129514, acc 0.96
2016-09-07T05:14:48.616465: step 2020, loss 0.101343, acc 0.96
2016-09-07T05:14:49.294859: step 2021, loss 0.0120189, acc 1
2016-09-07T05:14:49.988033: step 2022, loss 0.0853672, acc 0.92
2016-09-07T05:14:50.677752: step 2023, loss 0.0633744, acc 0.94
2016-09-07T05:14:51.394706: step 2024, loss 0.154229, acc 0.94
2016-09-07T05:14:52.086261: step 2025, loss 0.0212811, acc 1
2016-09-07T05:14:52.771663: step 2026, loss 0.051722, acc 0.98
2016-09-07T05:14:53.484246: step 2027, loss 0.0910597, acc 0.94
2016-09-07T05:14:54.187778: step 2028, loss 0.0403371, acc 0.98
2016-09-07T05:14:54.897189: step 2029, loss 0.0423061, acc 0.96
2016-09-07T05:14:55.578225: step 2030, loss 0.0114259, acc 1
2016-09-07T05:14:56.303108: step 2031, loss 0.134963, acc 0.92
2016-09-07T05:14:57.005536: step 2032, loss 0.0133934, acc 1
2016-09-07T05:14:57.717333: step 2033, loss 0.118206, acc 0.92
2016-09-07T05:14:58.406022: step 2034, loss 0.118228, acc 0.96
2016-09-07T05:14:59.137457: step 2035, loss 0.0286282, acc 0.98
2016-09-07T05:14:59.846955: step 2036, loss 0.0391174, acc 0.98
2016-09-07T05:15:00.575083: step 2037, loss 0.0337817, acc 0.98
2016-09-07T05:15:01.265055: step 2038, loss 0.0737598, acc 0.96
2016-09-07T05:15:01.935242: step 2039, loss 0.0473155, acc 0.98
2016-09-07T05:15:02.635080: step 2040, loss 0.0512842, acc 0.96
2016-09-07T05:15:03.324818: step 2041, loss 0.0846038, acc 0.96
2016-09-07T05:15:03.990525: step 2042, loss 0.00977421, acc 1
2016-09-07T05:15:04.694399: step 2043, loss 0.224861, acc 0.94
2016-09-07T05:15:05.398344: step 2044, loss 0.0398402, acc 0.98
2016-09-07T05:15:06.105090: step 2045, loss 0.121534, acc 0.92
2016-09-07T05:15:06.786889: step 2046, loss 0.0590814, acc 0.98
2016-09-07T05:15:07.480488: step 2047, loss 0.17795, acc 0.98
2016-09-07T05:15:08.202567: step 2048, loss 0.0334632, acc 0.96
2016-09-07T05:15:08.879409: step 2049, loss 0.0142867, acc 1
2016-09-07T05:15:09.568173: step 2050, loss 0.0581628, acc 0.96
2016-09-07T05:15:10.273685: step 2051, loss 0.0233821, acc 1
2016-09-07T05:15:10.968995: step 2052, loss 0.0925464, acc 0.94
2016-09-07T05:15:11.665166: step 2053, loss 0.00361991, acc 1
2016-09-07T05:15:12.340147: step 2054, loss 0.0402372, acc 0.98
2016-09-07T05:15:13.049263: step 2055, loss 0.0838572, acc 0.96
2016-09-07T05:15:13.747749: step 2056, loss 0.0644486, acc 0.96
2016-09-07T05:15:14.433480: step 2057, loss 0.034686, acc 0.98
2016-09-07T05:15:15.117586: step 2058, loss 0.07692, acc 0.96
2016-09-07T05:15:15.809908: step 2059, loss 0.0365822, acc 0.98
2016-09-07T05:15:16.535547: step 2060, loss 0.127459, acc 0.92
2016-09-07T05:15:17.214379: step 2061, loss 0.050124, acc 0.98
2016-09-07T05:15:17.925861: step 2062, loss 0.0996179, acc 0.96
2016-09-07T05:15:18.617964: step 2063, loss 0.0472099, acc 0.96
2016-09-07T05:15:19.320375: step 2064, loss 0.0418495, acc 0.98
2016-09-07T05:15:20.013808: step 2065, loss 0.260484, acc 0.94
2016-09-07T05:15:20.702882: step 2066, loss 0.0381748, acc 1
2016-09-07T05:15:21.432982: step 2067, loss 0.0764189, acc 0.98
2016-09-07T05:15:22.146013: step 2068, loss 0.0685921, acc 0.96
2016-09-07T05:15:22.831682: step 2069, loss 0.0389892, acc 1
2016-09-07T05:15:23.547651: step 2070, loss 0.0820466, acc 0.98
2016-09-07T05:15:24.249124: step 2071, loss 0.146268, acc 0.94
2016-09-07T05:15:24.959729: step 2072, loss 0.0426212, acc 0.96
2016-09-07T05:15:25.640257: step 2073, loss 0.1283, acc 0.98
2016-09-07T05:15:26.358982: step 2074, loss 0.0729703, acc 0.96
2016-09-07T05:15:27.069829: step 2075, loss 0.0179417, acc 1
2016-09-07T05:15:27.756444: step 2076, loss 0.0416606, acc 0.98
2016-09-07T05:15:28.490040: step 2077, loss 0.0502578, acc 0.98
2016-09-07T05:15:29.153893: step 2078, loss 0.132107, acc 0.9
2016-09-07T05:15:29.872797: step 2079, loss 0.0460559, acc 0.98
2016-09-07T05:15:30.568649: step 2080, loss 0.0309699, acc 1
2016-09-07T05:15:31.265778: step 2081, loss 0.0517142, acc 0.98
2016-09-07T05:15:31.933953: step 2082, loss 0.0495729, acc 0.96
2016-09-07T05:15:32.644738: step 2083, loss 0.0381163, acc 0.98
2016-09-07T05:15:33.355661: step 2084, loss 0.0762801, acc 0.98
2016-09-07T05:15:34.053317: step 2085, loss 0.0357647, acc 0.98
2016-09-07T05:15:34.766600: step 2086, loss 0.0292395, acc 1
2016-09-07T05:15:35.456119: step 2087, loss 0.0403174, acc 0.98
2016-09-07T05:15:36.144728: step 2088, loss 0.0823587, acc 0.96
2016-09-07T05:15:36.833730: step 2089, loss 0.101013, acc 0.94
2016-09-07T05:15:37.515093: step 2090, loss 0.109297, acc 0.94
2016-09-07T05:15:38.212870: step 2091, loss 0.0678345, acc 0.96
2016-09-07T05:15:38.887048: step 2092, loss 0.0693317, acc 0.96
2016-09-07T05:15:39.576785: step 2093, loss 0.0625664, acc 0.98
2016-09-07T05:15:40.269303: step 2094, loss 0.0852604, acc 0.96
2016-09-07T05:15:40.962939: step 2095, loss 0.074753, acc 0.96
2016-09-07T05:15:41.655515: step 2096, loss 0.0583364, acc 0.96
2016-09-07T05:15:42.348586: step 2097, loss 0.0170181, acc 1
2016-09-07T05:15:43.050812: step 2098, loss 0.0469901, acc 0.98
2016-09-07T05:15:43.740297: step 2099, loss 0.0590689, acc 0.96
2016-09-07T05:15:44.458797: step 2100, loss 0.043925, acc 0.98

Evaluation:
2016-09-07T05:15:48.096681: step 2100, loss 1.35106, acc 0.761726

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-2100

2016-09-07T05:15:49.870981: step 2101, loss 0.0757998, acc 0.94
2016-09-07T05:15:50.575336: step 2102, loss 0.0281332, acc 0.98
2016-09-07T05:15:51.276816: step 2103, loss 0.0126542, acc 1
2016-09-07T05:15:51.956079: step 2104, loss 0.0417731, acc 0.98
2016-09-07T05:15:52.664045: step 2105, loss 0.123342, acc 0.98
2016-09-07T05:15:53.369575: step 2106, loss 0.114976, acc 0.94
2016-09-07T05:15:54.053478: step 2107, loss 0.0647569, acc 0.96
2016-09-07T05:15:54.760819: step 2108, loss 0.0397577, acc 0.96
2016-09-07T05:15:55.447309: step 2109, loss 0.0264519, acc 0.98
2016-09-07T05:15:56.111639: step 2110, loss 0.0493842, acc 0.98
2016-09-07T05:15:56.798174: step 2111, loss 0.146313, acc 0.94
2016-09-07T05:15:57.464012: step 2112, loss 0.0256563, acc 1
2016-09-07T05:15:58.160633: step 2113, loss 0.0637242, acc 0.98
2016-09-07T05:15:58.877417: step 2114, loss 0.0287425, acc 1
2016-09-07T05:15:59.577116: step 2115, loss 0.0607599, acc 0.96
2016-09-07T05:16:00.317480: step 2116, loss 0.0175902, acc 1
2016-09-07T05:16:01.017326: step 2117, loss 0.0609632, acc 0.94
2016-09-07T05:16:01.693427: step 2118, loss 0.0430062, acc 1
2016-09-07T05:16:02.393388: step 2119, loss 0.041209, acc 0.98
2016-09-07T05:16:03.071961: step 2120, loss 0.0181902, acc 1
2016-09-07T05:16:03.784669: step 2121, loss 0.0764992, acc 0.98
2016-09-07T05:16:04.486664: step 2122, loss 0.0365375, acc 0.98
2016-09-07T05:16:05.172961: step 2123, loss 0.049792, acc 0.98
2016-09-07T05:16:05.888071: step 2124, loss 0.0228812, acc 1
2016-09-07T05:16:06.573332: step 2125, loss 0.0309765, acc 0.98
2016-09-07T05:16:07.260616: step 2126, loss 0.0415963, acc 1
2016-09-07T05:16:07.962742: step 2127, loss 0.0372644, acc 0.98
2016-09-07T05:16:08.664997: step 2128, loss 0.0118057, acc 1
2016-09-07T05:16:09.359129: step 2129, loss 0.0347254, acc 0.98
2016-09-07T05:16:10.050943: step 2130, loss 0.0665809, acc 0.96
2016-09-07T05:16:10.758609: step 2131, loss 0.0293952, acc 0.98
2016-09-07T05:16:11.464968: step 2132, loss 0.0209996, acc 1
2016-09-07T05:16:12.160020: step 2133, loss 0.0487352, acc 0.98
2016-09-07T05:16:12.841358: step 2134, loss 0.0644846, acc 0.96
2016-09-07T05:16:13.530255: step 2135, loss 0.022718, acc 1
2016-09-07T05:16:14.222890: step 2136, loss 0.0186567, acc 0.98
2016-09-07T05:16:14.893937: step 2137, loss 0.0718063, acc 0.96
2016-09-07T05:16:15.616752: step 2138, loss 0.0747797, acc 0.94
2016-09-07T05:16:16.307861: step 2139, loss 0.00983766, acc 1
2016-09-07T05:16:17.001980: step 2140, loss 0.0208491, acc 1
2016-09-07T05:16:17.710247: step 2141, loss 0.045357, acc 0.98
2016-09-07T05:16:18.403710: step 2142, loss 0.00920613, acc 1
2016-09-07T05:16:19.110490: step 2143, loss 0.037904, acc 0.96
2016-09-07T05:16:19.788674: step 2144, loss 0.0281926, acc 1
2016-09-07T05:16:20.489436: step 2145, loss 0.0171785, acc 1
2016-09-07T05:16:21.191084: step 2146, loss 0.101058, acc 0.96
2016-09-07T05:16:21.876603: step 2147, loss 0.0123868, acc 1
2016-09-07T05:16:22.549462: step 2148, loss 0.0978489, acc 0.98
2016-09-07T05:16:23.262760: step 2149, loss 0.0379507, acc 0.98
2016-09-07T05:16:23.957580: step 2150, loss 0.032324, acc 0.98
2016-09-07T05:16:24.626402: step 2151, loss 0.0501522, acc 0.96
2016-09-07T05:16:25.311804: step 2152, loss 0.0068792, acc 1
2016-09-07T05:16:26.019123: step 2153, loss 0.0855546, acc 0.94
2016-09-07T05:16:26.720035: step 2154, loss 0.0448253, acc 0.98
2016-09-07T05:16:27.428716: step 2155, loss 0.0147247, acc 1
2016-09-07T05:16:28.102663: step 2156, loss 0.0831772, acc 0.98
2016-09-07T05:16:28.834249: step 2157, loss 0.0569806, acc 0.98
2016-09-07T05:16:29.528731: step 2158, loss 0.0496273, acc 0.98
2016-09-07T05:16:30.220524: step 2159, loss 0.0253152, acc 1
2016-09-07T05:16:30.919315: step 2160, loss 0.0953662, acc 0.92
2016-09-07T05:16:31.613121: step 2161, loss 0.00646503, acc 1
2016-09-07T05:16:32.358278: step 2162, loss 0.0685872, acc 0.98
2016-09-07T05:16:33.043490: step 2163, loss 0.0738172, acc 0.94
2016-09-07T05:16:33.727012: step 2164, loss 0.0324474, acc 0.98
2016-09-07T05:16:34.446753: step 2165, loss 0.0508066, acc 0.98
2016-09-07T05:16:35.156639: step 2166, loss 0.00714274, acc 1
2016-09-07T05:16:35.855113: step 2167, loss 0.157994, acc 0.98
2016-09-07T05:16:36.532189: step 2168, loss 0.0504655, acc 0.98
2016-09-07T05:16:37.253921: step 2169, loss 0.0124009, acc 1
2016-09-07T05:16:37.956234: step 2170, loss 0.0247199, acc 0.98
2016-09-07T05:16:38.658684: step 2171, loss 0.0257847, acc 1
2016-09-07T05:16:39.348177: step 2172, loss 0.121565, acc 0.96
2016-09-07T05:16:40.037905: step 2173, loss 0.0375092, acc 0.96
2016-09-07T05:16:40.742909: step 2174, loss 0.0201738, acc 1
2016-09-07T05:16:41.439713: step 2175, loss 0.066306, acc 0.98
2016-09-07T05:16:42.159002: step 2176, loss 0.0526908, acc 0.94
2016-09-07T05:16:42.847232: step 2177, loss 0.098313, acc 0.94
2016-09-07T05:16:43.547395: step 2178, loss 0.116067, acc 0.94
2016-09-07T05:16:44.239930: step 2179, loss 0.108325, acc 0.94
2016-09-07T05:16:44.934095: step 2180, loss 0.0584014, acc 0.96
2016-09-07T05:16:45.639901: step 2181, loss 0.00678463, acc 1
2016-09-07T05:16:46.327912: step 2182, loss 0.166707, acc 0.96
2016-09-07T05:16:47.022012: step 2183, loss 0.0157558, acc 0.98
2016-09-07T05:16:47.743894: step 2184, loss 0.0636647, acc 0.96
2016-09-07T05:16:48.435727: step 2185, loss 0.00579677, acc 1
2016-09-07T05:16:49.136402: step 2186, loss 0.0731571, acc 0.98
2016-09-07T05:16:49.863798: step 2187, loss 0.0331411, acc 1
2016-09-07T05:16:50.549449: step 2188, loss 0.0391605, acc 0.98
2016-09-07T05:16:51.254230: step 2189, loss 0.0497643, acc 0.96
2016-09-07T05:16:51.959345: step 2190, loss 0.062991, acc 0.98
2016-09-07T05:16:52.666477: step 2191, loss 0.0725646, acc 0.98
2016-09-07T05:16:53.349261: step 2192, loss 0.0350667, acc 0.98
2016-09-07T05:16:54.072917: step 2193, loss 0.03185, acc 1
2016-09-07T05:16:54.742333: step 2194, loss 0.0254625, acc 1
2016-09-07T05:16:55.430136: step 2195, loss 0.0745238, acc 0.96
2016-09-07T05:16:56.117477: step 2196, loss 0.157314, acc 0.92
2016-09-07T05:16:56.809395: step 2197, loss 0.0778902, acc 0.96
2016-09-07T05:16:57.496175: step 2198, loss 0.0748579, acc 0.94
2016-09-07T05:16:58.159312: step 2199, loss 0.178222, acc 0.96
2016-09-07T05:16:58.870289: step 2200, loss 0.0813177, acc 0.96

Evaluation:
2016-09-07T05:17:02.578450: step 2200, loss 1.43909, acc 0.760788

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-2200

2016-09-07T05:17:04.438799: step 2201, loss 0.058621, acc 0.98
2016-09-07T05:17:05.124224: step 2202, loss 0.072127, acc 0.94
2016-09-07T05:17:05.822219: step 2203, loss 0.028341, acc 0.98
2016-09-07T05:17:06.536865: step 2204, loss 0.0393324, acc 0.98
2016-09-07T05:17:07.249709: step 2205, loss 0.0617272, acc 0.96
2016-09-07T05:17:07.941619: step 2206, loss 0.0754372, acc 0.92
2016-09-07T05:17:08.642030: step 2207, loss 0.0409314, acc 1
2016-09-07T05:17:09.334254: step 2208, loss 0.0168119, acc 1
2016-09-07T05:17:10.026037: step 2209, loss 0.0138045, acc 1
2016-09-07T05:17:10.739462: step 2210, loss 0.12537, acc 0.96
2016-09-07T05:17:11.405480: step 2211, loss 0.0857547, acc 0.96
2016-09-07T05:17:12.105396: step 2212, loss 0.067671, acc 0.96
2016-09-07T05:17:12.787711: step 2213, loss 0.0326428, acc 1
2016-09-07T05:17:13.475619: step 2214, loss 0.0286787, acc 1
2016-09-07T05:17:14.154207: step 2215, loss 0.0562035, acc 0.98
2016-09-07T05:17:14.838086: step 2216, loss 0.101641, acc 0.96
2016-09-07T05:17:15.534254: step 2217, loss 0.0986906, acc 0.98
2016-09-07T05:17:16.229938: step 2218, loss 0.00289636, acc 1
2016-09-07T05:17:16.952155: step 2219, loss 0.00134443, acc 1
2016-09-07T05:17:17.625927: step 2220, loss 0.0578328, acc 0.98
2016-09-07T05:17:18.335424: step 2221, loss 0.0821814, acc 0.98
2016-09-07T05:17:19.038692: step 2222, loss 0.080278, acc 0.94
2016-09-07T05:17:19.739511: step 2223, loss 0.124879, acc 0.96
2016-09-07T05:17:20.431882: step 2224, loss 0.0385491, acc 0.96
2016-09-07T05:17:21.142628: step 2225, loss 0.106752, acc 0.96
2016-09-07T05:17:21.812402: step 2226, loss 0.111692, acc 0.94
2016-09-07T05:17:22.500616: step 2227, loss 0.0414741, acc 0.98
2016-09-07T05:17:23.207144: step 2228, loss 0.119241, acc 0.96
2016-09-07T05:17:23.891932: step 2229, loss 0.0366041, acc 0.98
2016-09-07T05:17:24.600293: step 2230, loss 0.0819728, acc 0.96
2016-09-07T05:17:25.314642: step 2231, loss 0.0265721, acc 1
2016-09-07T05:17:25.994042: step 2232, loss 0.0487155, acc 0.96
2016-09-07T05:17:26.673102: step 2233, loss 0.0335911, acc 0.98
2016-09-07T05:17:27.361449: step 2234, loss 0.23099, acc 0.92
2016-09-07T05:17:28.053527: step 2235, loss 0.0200155, acc 1
2016-09-07T05:17:28.734186: step 2236, loss 0.0128588, acc 1
2016-09-07T05:17:29.391865: step 2237, loss 0.0594503, acc 0.98
2016-09-07T05:17:30.101514: step 2238, loss 0.164182, acc 0.92
2016-09-07T05:17:30.809471: step 2239, loss 0.0531556, acc 0.98
2016-09-07T05:17:31.502052: step 2240, loss 0.0345897, acc 0.98
2016-09-07T05:17:32.209272: step 2241, loss 0.0203847, acc 1
2016-09-07T05:17:32.906327: step 2242, loss 0.0547224, acc 0.98
2016-09-07T05:17:33.605077: step 2243, loss 0.0445961, acc 1
2016-09-07T05:17:34.268429: step 2244, loss 0.066502, acc 0.96
2016-09-07T05:17:34.976931: step 2245, loss 0.0374857, acc 0.98
2016-09-07T05:17:35.683191: step 2246, loss 0.0581321, acc 0.96
2016-09-07T05:17:36.358192: step 2247, loss 0.0780964, acc 0.94
2016-09-07T05:17:37.068739: step 2248, loss 0.140971, acc 0.96
2016-09-07T05:17:37.785085: step 2249, loss 0.0618478, acc 0.98
2016-09-07T05:17:38.485466: step 2250, loss 0.0151059, acc 1
2016-09-07T05:17:39.185048: step 2251, loss 0.00923453, acc 1
2016-09-07T05:17:39.864855: step 2252, loss 0.0292371, acc 1
2016-09-07T05:17:40.547010: step 2253, loss 0.318532, acc 0.9
2016-09-07T05:17:41.247547: step 2254, loss 0.0675704, acc 0.96
2016-09-07T05:17:41.923104: step 2255, loss 0.043151, acc 0.96
2016-09-07T05:17:42.623784: step 2256, loss 0.00895084, acc 1
2016-09-07T05:17:43.328288: step 2257, loss 0.0587423, acc 0.98
2016-09-07T05:17:44.019274: step 2258, loss 0.0297195, acc 0.98
2016-09-07T05:17:44.722986: step 2259, loss 0.0839376, acc 0.98
2016-09-07T05:17:45.427263: step 2260, loss 0.0328303, acc 0.98
2016-09-07T05:17:46.131295: step 2261, loss 0.0583768, acc 0.96
2016-09-07T05:17:46.818033: step 2262, loss 0.0207476, acc 1
2016-09-07T05:17:47.498461: step 2263, loss 0.155865, acc 0.88
2016-09-07T05:17:48.196640: step 2264, loss 0.0328921, acc 1
2016-09-07T05:17:48.887231: step 2265, loss 0.078804, acc 0.94
2016-09-07T05:17:49.585846: step 2266, loss 0.152838, acc 0.94
2016-09-07T05:17:50.261258: step 2267, loss 0.05245, acc 0.96
2016-09-07T05:17:50.943234: step 2268, loss 0.0379649, acc 0.98
2016-09-07T05:17:51.654141: step 2269, loss 0.0200586, acc 1
2016-09-07T05:17:52.320065: step 2270, loss 0.0358637, acc 1
2016-09-07T05:17:53.042008: step 2271, loss 0.038687, acc 0.98
2016-09-07T05:17:53.736763: step 2272, loss 0.0161292, acc 1
2016-09-07T05:17:54.457724: step 2273, loss 0.0722863, acc 0.96
2016-09-07T05:17:55.141488: step 2274, loss 0.0468956, acc 0.96
2016-09-07T05:17:55.827976: step 2275, loss 0.10573, acc 0.98
2016-09-07T05:17:56.531453: step 2276, loss 0.0825654, acc 0.96
2016-09-07T05:17:57.222498: step 2277, loss 0.0282567, acc 1
2016-09-07T05:17:57.909922: step 2278, loss 0.10626, acc 0.92
2016-09-07T05:17:58.586203: step 2279, loss 0.0331477, acc 1
2016-09-07T05:17:59.269708: step 2280, loss 0.0406317, acc 0.98
2016-09-07T05:17:59.955876: step 2281, loss 0.0435686, acc 1
2016-09-07T05:18:00.692930: step 2282, loss 0.0465051, acc 0.98
2016-09-07T05:18:01.412519: step 2283, loss 0.0222856, acc 1
2016-09-07T05:18:02.097221: step 2284, loss 0.0932295, acc 0.96
2016-09-07T05:18:02.794138: step 2285, loss 0.0279725, acc 1
2016-09-07T05:18:03.498793: step 2286, loss 0.0277052, acc 1
2016-09-07T05:18:04.192078: step 2287, loss 0.0811412, acc 0.98
2016-09-07T05:18:04.908455: step 2288, loss 0.0475283, acc 0.98
2016-09-07T05:18:05.576555: step 2289, loss 0.0455886, acc 0.96
2016-09-07T05:18:06.292981: step 2290, loss 0.0168649, acc 1
2016-09-07T05:18:06.967618: step 2291, loss 0.0764643, acc 0.96
2016-09-07T05:18:07.660271: step 2292, loss 0.0825145, acc 0.94
2016-09-07T05:18:08.372885: step 2293, loss 0.04671, acc 0.98
2016-09-07T05:18:09.084408: step 2294, loss 0.0429926, acc 0.98
2016-09-07T05:18:09.803795: step 2295, loss 0.0687354, acc 0.98
2016-09-07T05:18:10.493140: step 2296, loss 0.048573, acc 0.98
2016-09-07T05:18:11.199201: step 2297, loss 0.044049, acc 0.98
2016-09-07T05:18:11.882898: step 2298, loss 0.034566, acc 0.98
2016-09-07T05:18:12.585707: step 2299, loss 0.0299859, acc 0.98
2016-09-07T05:18:13.289198: step 2300, loss 0.0274841, acc 0.98

Evaluation:
2016-09-07T05:18:16.861008: step 2300, loss 1.34956, acc 0.75985

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-2300

2016-09-07T05:18:18.682355: step 2301, loss 0.0761925, acc 0.94
2016-09-07T05:18:19.372245: step 2302, loss 0.0845009, acc 0.96
2016-09-07T05:18:20.059046: step 2303, loss 0.0422082, acc 0.98
2016-09-07T05:18:20.715095: step 2304, loss 0.00172176, acc 1
2016-09-07T05:18:21.429070: step 2305, loss 0.0245497, acc 0.98
2016-09-07T05:18:22.101233: step 2306, loss 0.0129684, acc 1
2016-09-07T05:18:22.782027: step 2307, loss 0.096967, acc 0.94
2016-09-07T05:18:23.477403: step 2308, loss 0.188639, acc 0.94
2016-09-07T05:18:24.184952: step 2309, loss 0.0409598, acc 0.98
2016-09-07T05:18:24.863184: step 2310, loss 0.0578496, acc 0.98
2016-09-07T05:18:25.581826: step 2311, loss 0.0132232, acc 1
2016-09-07T05:18:26.272608: step 2312, loss 0.0416562, acc 0.98
2016-09-07T05:18:26.968410: step 2313, loss 0.0370259, acc 0.98
2016-09-07T05:18:27.656592: step 2314, loss 0.0429603, acc 0.98
2016-09-07T05:18:28.328176: step 2315, loss 0.0230927, acc 1
2016-09-07T05:18:29.063534: step 2316, loss 0.0179523, acc 1
2016-09-07T05:18:29.767889: step 2317, loss 0.13521, acc 0.94
2016-09-07T05:18:30.460086: step 2318, loss 0.0245866, acc 0.98
2016-09-07T05:18:31.151565: step 2319, loss 0.0904374, acc 0.96
2016-09-07T05:18:31.841351: step 2320, loss 0.0128153, acc 1
2016-09-07T05:18:32.544229: step 2321, loss 0.0469767, acc 0.98
2016-09-07T05:18:33.207113: step 2322, loss 0.0136086, acc 1
2016-09-07T05:18:33.910209: step 2323, loss 0.0253715, acc 1
2016-09-07T05:18:34.613712: step 2324, loss 0.0321051, acc 1
2016-09-07T05:18:35.312459: step 2325, loss 0.0436252, acc 0.96
2016-09-07T05:18:36.025960: step 2326, loss 0.0992158, acc 0.96
2016-09-07T05:18:36.713638: step 2327, loss 0.0393829, acc 0.98
2016-09-07T05:18:37.408252: step 2328, loss 0.016154, acc 1
2016-09-07T05:18:38.082540: step 2329, loss 0.0234038, acc 1
2016-09-07T05:18:38.752581: step 2330, loss 0.00531766, acc 1
2016-09-07T05:18:39.447187: step 2331, loss 0.0613252, acc 0.98
2016-09-07T05:18:40.130626: step 2332, loss 0.00129307, acc 1
2016-09-07T05:18:40.831641: step 2333, loss 0.122953, acc 0.98
2016-09-07T05:18:41.504489: step 2334, loss 0.0194754, acc 1
2016-09-07T05:18:42.202785: step 2335, loss 0.0344589, acc 0.98
2016-09-07T05:18:42.872317: step 2336, loss 0.119553, acc 0.92
2016-09-07T05:18:43.561724: step 2337, loss 0.0250643, acc 1
2016-09-07T05:18:44.242650: step 2338, loss 0.108434, acc 0.92
2016-09-07T05:18:44.918428: step 2339, loss 0.069411, acc 0.98
2016-09-07T05:18:45.636448: step 2340, loss 0.156193, acc 0.96
2016-09-07T05:18:46.340757: step 2341, loss 0.045532, acc 0.98
2016-09-07T05:18:47.063044: step 2342, loss 0.0214026, acc 1
2016-09-07T05:18:47.756902: step 2343, loss 0.0256797, acc 1
2016-09-07T05:18:48.439115: step 2344, loss 0.0273502, acc 0.98
2016-09-07T05:18:49.132029: step 2345, loss 0.0439865, acc 0.96
2016-09-07T05:18:49.830337: step 2346, loss 0.0518314, acc 0.98
2016-09-07T05:18:50.539668: step 2347, loss 0.0541172, acc 0.98
2016-09-07T05:18:51.226005: step 2348, loss 0.0275751, acc 1
2016-09-07T05:18:51.932640: step 2349, loss 0.0176052, acc 1
2016-09-07T05:18:52.638149: step 2350, loss 0.0215611, acc 1
2016-09-07T05:18:53.324661: step 2351, loss 0.040071, acc 0.98
2016-09-07T05:18:54.025793: step 2352, loss 0.0951177, acc 0.96
2016-09-07T05:18:54.730049: step 2353, loss 0.182255, acc 0.96
2016-09-07T05:18:55.456748: step 2354, loss 0.0733723, acc 0.98
2016-09-07T05:18:56.130370: step 2355, loss 0.150289, acc 0.92
2016-09-07T05:18:56.819260: step 2356, loss 0.00744827, acc 1
2016-09-07T05:18:57.497108: step 2357, loss 0.0689517, acc 0.98
2016-09-07T05:18:58.164253: step 2358, loss 0.0150976, acc 0.98
2016-09-07T05:18:58.848285: step 2359, loss 0.020118, acc 0.98
2016-09-07T05:18:59.527713: step 2360, loss 0.0581573, acc 0.98
2016-09-07T05:19:00.270277: step 2361, loss 0.0467153, acc 0.98
2016-09-07T05:19:00.963168: step 2362, loss 0.0003505, acc 1
2016-09-07T05:19:01.647148: step 2363, loss 0.104182, acc 0.96
2016-09-07T05:19:02.345286: step 2364, loss 0.0688592, acc 0.96
2016-09-07T05:19:03.042214: step 2365, loss 0.112685, acc 0.94
2016-09-07T05:19:03.745522: step 2366, loss 0.0162059, acc 1
2016-09-07T05:19:04.405977: step 2367, loss 0.0499044, acc 0.98
2016-09-07T05:19:05.128427: step 2368, loss 0.0723118, acc 0.98
2016-09-07T05:19:05.821342: step 2369, loss 0.166795, acc 0.96
2016-09-07T05:19:06.511808: step 2370, loss 0.0594234, acc 0.94
2016-09-07T05:19:07.189619: step 2371, loss 0.0377593, acc 0.98
2016-09-07T05:19:07.883078: step 2372, loss 0.0380717, acc 0.98
2016-09-07T05:19:08.585861: step 2373, loss 0.0600286, acc 0.94
2016-09-07T05:19:09.271690: step 2374, loss 0.0994054, acc 0.92
2016-09-07T05:19:09.978062: step 2375, loss 0.0330525, acc 0.98
2016-09-07T05:19:10.681448: step 2376, loss 0.0390638, acc 0.98
2016-09-07T05:19:11.388896: step 2377, loss 0.0249514, acc 0.98
2016-09-07T05:19:12.088718: step 2378, loss 0.0500217, acc 0.98
2016-09-07T05:19:12.787345: step 2379, loss 0.0330385, acc 1
2016-09-07T05:19:13.502057: step 2380, loss 0.0415179, acc 1
2016-09-07T05:19:14.207908: step 2381, loss 0.0397929, acc 0.98
2016-09-07T05:19:14.892239: step 2382, loss 0.0703222, acc 0.98
2016-09-07T05:19:15.569505: step 2383, loss 0.0278796, acc 0.98
2016-09-07T05:19:16.274713: step 2384, loss 0.0780034, acc 0.92
2016-09-07T05:19:16.980836: step 2385, loss 0.0286728, acc 0.98
2016-09-07T05:19:17.661545: step 2386, loss 0.0175959, acc 0.98
2016-09-07T05:19:18.380005: step 2387, loss 0.0529816, acc 0.96
2016-09-07T05:19:19.063658: step 2388, loss 0.0376248, acc 0.98
2016-09-07T05:19:19.748937: step 2389, loss 0.0624369, acc 0.96
2016-09-07T05:19:20.443143: step 2390, loss 0.0264178, acc 1
2016-09-07T05:19:21.129601: step 2391, loss 0.0093799, acc 1
2016-09-07T05:19:21.810915: step 2392, loss 0.058181, acc 0.98
2016-09-07T05:19:22.481702: step 2393, loss 0.0835598, acc 0.96
2016-09-07T05:19:23.193202: step 2394, loss 0.0777747, acc 0.96
2016-09-07T05:19:23.886025: step 2395, loss 0.0291915, acc 1
2016-09-07T05:19:24.571634: step 2396, loss 0.0482984, acc 0.96
2016-09-07T05:19:25.253974: step 2397, loss 0.00315666, acc 1
2016-09-07T05:19:25.949238: step 2398, loss 0.0625309, acc 0.96
2016-09-07T05:19:26.636191: step 2399, loss 0.0540107, acc 0.98
2016-09-07T05:19:27.331278: step 2400, loss 0.0830277, acc 0.94

Evaluation:
2016-09-07T05:19:30.974733: step 2400, loss 1.5394, acc 0.741088

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-2400

2016-09-07T05:19:32.750020: step 2401, loss 0.00566165, acc 1
2016-09-07T05:19:33.429242: step 2402, loss 0.166033, acc 0.92
2016-09-07T05:19:34.102371: step 2403, loss 0.0444564, acc 0.98
2016-09-07T05:19:34.800540: step 2404, loss 0.0142017, acc 1
2016-09-07T05:19:35.486614: step 2405, loss 0.0618229, acc 0.96
2016-09-07T05:19:36.182625: step 2406, loss 0.0298029, acc 0.98
2016-09-07T05:19:36.857363: step 2407, loss 0.135033, acc 0.94
2016-09-07T05:19:37.557136: step 2408, loss 0.13666, acc 0.94
2016-09-07T05:19:38.264940: step 2409, loss 0.0970965, acc 0.94
2016-09-07T05:19:38.952496: step 2410, loss 0.0488625, acc 0.96
2016-09-07T05:19:39.643287: step 2411, loss 0.0158995, acc 1
2016-09-07T05:19:40.328106: step 2412, loss 0.0929003, acc 0.98
2016-09-07T05:19:41.031586: step 2413, loss 0.0368684, acc 0.98
2016-09-07T05:19:41.730618: step 2414, loss 0.0380072, acc 0.98
2016-09-07T05:19:42.438254: step 2415, loss 0.0636954, acc 0.98
2016-09-07T05:19:43.129857: step 2416, loss 0.0235088, acc 1
2016-09-07T05:19:43.812906: step 2417, loss 0.0569885, acc 0.96
2016-09-07T05:19:44.515437: step 2418, loss 0.0192878, acc 1
2016-09-07T05:19:45.202105: step 2419, loss 0.0335569, acc 0.98
2016-09-07T05:19:45.925409: step 2420, loss 0.0236506, acc 1
2016-09-07T05:19:46.604890: step 2421, loss 0.0535562, acc 0.98
2016-09-07T05:19:47.277703: step 2422, loss 0.0871696, acc 0.96
2016-09-07T05:19:47.981899: step 2423, loss 0.0619469, acc 0.98
2016-09-07T05:19:48.670229: step 2424, loss 0.0281964, acc 1
2016-09-07T05:19:49.364238: step 2425, loss 0.218398, acc 0.9
2016-09-07T05:19:50.034985: step 2426, loss 0.0652088, acc 0.98
2016-09-07T05:19:50.726780: step 2427, loss 0.0282666, acc 0.98
2016-09-07T05:19:51.410500: step 2428, loss 0.0449408, acc 0.98
2016-09-07T05:19:52.105549: step 2429, loss 0.0878985, acc 0.98
2016-09-07T05:19:52.797534: step 2430, loss 0.0318248, acc 0.98
2016-09-07T05:19:53.490362: step 2431, loss 0.0740602, acc 0.98
2016-09-07T05:19:54.180121: step 2432, loss 0.0412735, acc 0.96
2016-09-07T05:19:54.870165: step 2433, loss 0.067608, acc 0.98
2016-09-07T05:19:55.584350: step 2434, loss 0.0428173, acc 1
2016-09-07T05:19:56.263843: step 2435, loss 0.0689395, acc 0.98
2016-09-07T05:19:56.943824: step 2436, loss 0.0476909, acc 0.98
2016-09-07T05:19:57.638053: step 2437, loss 0.0347035, acc 0.98
2016-09-07T05:19:58.342838: step 2438, loss 0.0277092, acc 0.98
2016-09-07T05:19:59.031222: step 2439, loss 0.0598921, acc 0.96
2016-09-07T05:19:59.709672: step 2440, loss 0.14114, acc 0.96
2016-09-07T05:20:00.445365: step 2441, loss 0.014029, acc 1
2016-09-07T05:20:01.126426: step 2442, loss 0.0166726, acc 1
2016-09-07T05:20:01.825241: step 2443, loss 0.111599, acc 0.96
2016-09-07T05:20:02.511361: step 2444, loss 0.0332463, acc 1
2016-09-07T05:20:03.225270: step 2445, loss 0.0671211, acc 0.96
2016-09-07T05:20:03.955988: step 2446, loss 0.0540486, acc 0.94
2016-09-07T05:20:04.621645: step 2447, loss 0.0389318, acc 0.98
2016-09-07T05:20:05.315911: step 2448, loss 0.0126772, acc 1
2016-09-07T05:20:06.019840: step 2449, loss 0.0241068, acc 0.98
2016-09-07T05:20:06.713221: step 2450, loss 0.0253664, acc 1
2016-09-07T05:20:07.394519: step 2451, loss 0.0429666, acc 0.98
2016-09-07T05:20:08.096833: step 2452, loss 0.0771039, acc 0.94
2016-09-07T05:20:08.809193: step 2453, loss 0.0878988, acc 0.94
2016-09-07T05:20:09.486914: step 2454, loss 0.0110304, acc 1
2016-09-07T05:20:10.180878: step 2455, loss 0.0880088, acc 0.94
2016-09-07T05:20:10.875368: step 2456, loss 0.0846222, acc 0.98
2016-09-07T05:20:11.586159: step 2457, loss 0.0191427, acc 1
2016-09-07T05:20:12.298800: step 2458, loss 0.0680222, acc 0.96
2016-09-07T05:20:13.001607: step 2459, loss 0.006911, acc 1
2016-09-07T05:20:13.717234: step 2460, loss 0.001873, acc 1
2016-09-07T05:20:14.422404: step 2461, loss 0.00197472, acc 1
2016-09-07T05:20:15.096519: step 2462, loss 0.0466869, acc 0.96
2016-09-07T05:20:15.796298: step 2463, loss 0.124632, acc 0.96
2016-09-07T05:20:16.505777: step 2464, loss 0.0501552, acc 0.98
2016-09-07T05:20:17.241232: step 2465, loss 0.0652835, acc 0.98
2016-09-07T05:20:17.932405: step 2466, loss 0.0544867, acc 0.98
2016-09-07T05:20:18.608648: step 2467, loss 0.00306712, acc 1
2016-09-07T05:20:19.299533: step 2468, loss 0.0109612, acc 1
2016-09-07T05:20:19.993918: step 2469, loss 0.0633936, acc 0.98
2016-09-07T05:20:20.693470: step 2470, loss 0.0623623, acc 0.96
2016-09-07T05:20:21.369835: step 2471, loss 0.0271827, acc 1
2016-09-07T05:20:22.092400: step 2472, loss 0.189189, acc 0.96
2016-09-07T05:20:22.789259: step 2473, loss 0.046124, acc 1
2016-09-07T05:20:23.476354: step 2474, loss 0.148663, acc 0.94
2016-09-07T05:20:24.164515: step 2475, loss 0.0270642, acc 1
2016-09-07T05:20:24.863558: step 2476, loss 0.0480174, acc 0.98
2016-09-07T05:20:25.557880: step 2477, loss 0.0764544, acc 0.96
2016-09-07T05:20:26.232857: step 2478, loss 0.00779749, acc 1
2016-09-07T05:20:26.966403: step 2479, loss 0.0818429, acc 0.96
2016-09-07T05:20:27.669135: step 2480, loss 0.0652321, acc 0.94
2016-09-07T05:20:28.382012: step 2481, loss 0.0369376, acc 0.98
2016-09-07T05:20:29.078218: step 2482, loss 0.0677345, acc 0.96
2016-09-07T05:20:29.768559: step 2483, loss 0.0847427, acc 0.98
2016-09-07T05:20:30.487062: step 2484, loss 0.0472739, acc 0.98
2016-09-07T05:20:31.173948: step 2485, loss 0.0192562, acc 1
2016-09-07T05:20:31.863613: step 2486, loss 0.0244199, acc 1
2016-09-07T05:20:32.553134: step 2487, loss 0.0723677, acc 0.98
2016-09-07T05:20:33.237838: step 2488, loss 0.0157104, acc 1
2016-09-07T05:20:33.950288: step 2489, loss 0.0614029, acc 0.94
2016-09-07T05:20:34.616196: step 2490, loss 0.0161565, acc 1
2016-09-07T05:20:35.326992: step 2491, loss 0.183994, acc 0.94
2016-09-07T05:20:35.982307: step 2492, loss 0.0232371, acc 0.98
2016-09-07T05:20:36.667165: step 2493, loss 0.115021, acc 0.92
2016-09-07T05:20:37.368278: step 2494, loss 0.0717781, acc 0.96
2016-09-07T05:20:38.067409: step 2495, loss 0.0839048, acc 0.96
2016-09-07T05:20:38.709768: step 2496, loss 0.0314289, acc 1
2016-09-07T05:20:39.413191: step 2497, loss 0.0321455, acc 0.98
2016-09-07T05:20:40.113157: step 2498, loss 0.0736606, acc 0.98
2016-09-07T05:20:40.790561: step 2499, loss 0.197849, acc 0.94
2016-09-07T05:20:41.480386: step 2500, loss 0.0487039, acc 0.98

Evaluation:
2016-09-07T05:20:45.073715: step 2500, loss 1.17028, acc 0.748593

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-2500

2016-09-07T05:20:46.751215: step 2501, loss 0.0300483, acc 0.98
2016-09-07T05:20:47.459445: step 2502, loss 0.0422186, acc 0.98
2016-09-07T05:20:48.169220: step 2503, loss 0.010393, acc 1
2016-09-07T05:20:48.835561: step 2504, loss 0.0486303, acc 0.96
2016-09-07T05:20:49.529485: step 2505, loss 0.0469035, acc 0.96
2016-09-07T05:20:50.232298: step 2506, loss 0.0579999, acc 0.96
2016-09-07T05:20:50.948587: step 2507, loss 0.02408, acc 0.98
2016-09-07T05:20:51.653508: step 2508, loss 0.0633438, acc 0.98
2016-09-07T05:20:52.347997: step 2509, loss 0.0480091, acc 0.96
2016-09-07T05:20:53.018784: step 2510, loss 0.0788777, acc 0.96
2016-09-07T05:20:53.710023: step 2511, loss 0.0872479, acc 0.96
2016-09-07T05:20:54.400576: step 2512, loss 0.0462001, acc 0.96
2016-09-07T05:20:55.065761: step 2513, loss 0.0313766, acc 1
2016-09-07T05:20:55.777902: step 2514, loss 0.051559, acc 0.98
2016-09-07T05:20:56.464937: step 2515, loss 0.064923, acc 0.96
2016-09-07T05:20:57.142949: step 2516, loss 0.0235467, acc 1
2016-09-07T05:20:57.829156: step 2517, loss 0.106801, acc 0.98
2016-09-07T05:20:58.494270: step 2518, loss 0.064029, acc 0.96
2016-09-07T05:20:59.189406: step 2519, loss 0.0322005, acc 0.98
2016-09-07T05:20:59.882416: step 2520, loss 0.0317194, acc 0.98
2016-09-07T05:21:00.652713: step 2521, loss 0.0244709, acc 1
2016-09-07T05:21:01.350241: step 2522, loss 0.0358235, acc 0.98
2016-09-07T05:21:02.039924: step 2523, loss 0.0350011, acc 0.98
2016-09-07T05:21:02.748434: step 2524, loss 0.0227673, acc 0.98
2016-09-07T05:21:03.435701: step 2525, loss 0.0144892, acc 1
2016-09-07T05:21:04.138117: step 2526, loss 0.0608801, acc 0.96
2016-09-07T05:21:04.829104: step 2527, loss 0.0287358, acc 0.98
2016-09-07T05:21:05.506199: step 2528, loss 0.0674069, acc 0.98
2016-09-07T05:21:06.210067: step 2529, loss 0.0646613, acc 0.96
2016-09-07T05:21:06.922704: step 2530, loss 0.0344093, acc 0.98
2016-09-07T05:21:07.596882: step 2531, loss 0.0172505, acc 1
2016-09-07T05:21:08.281841: step 2532, loss 0.0577769, acc 0.96
2016-09-07T05:21:08.995011: step 2533, loss 0.0655515, acc 0.98
2016-09-07T05:21:09.662711: step 2534, loss 0.0573401, acc 0.98
2016-09-07T05:21:10.362232: step 2535, loss 0.00136288, acc 1
2016-09-07T05:21:11.042210: step 2536, loss 0.0733733, acc 0.98
2016-09-07T05:21:11.726403: step 2537, loss 0.0189595, acc 1
2016-09-07T05:21:12.406024: step 2538, loss 0.0932846, acc 0.96
2016-09-07T05:21:13.093041: step 2539, loss 0.000210467, acc 1
2016-09-07T05:21:13.804562: step 2540, loss 0.0767591, acc 0.98
2016-09-07T05:21:14.495094: step 2541, loss 0.026269, acc 1
2016-09-07T05:21:15.174396: step 2542, loss 0.0220784, acc 0.98
2016-09-07T05:21:15.886742: step 2543, loss 0.0604273, acc 0.96
2016-09-07T05:21:16.570315: step 2544, loss 0.075899, acc 0.94
2016-09-07T05:21:17.286225: step 2545, loss 0.00574619, acc 1
2016-09-07T05:21:17.954907: step 2546, loss 0.0490745, acc 0.96
2016-09-07T05:21:18.645980: step 2547, loss 0.117182, acc 0.94
2016-09-07T05:21:19.351836: step 2548, loss 0.18859, acc 0.94
2016-09-07T05:21:20.042936: step 2549, loss 0.0703911, acc 0.98
2016-09-07T05:21:20.749005: step 2550, loss 0.0536674, acc 0.98
2016-09-07T05:21:21.456634: step 2551, loss 0.0709703, acc 0.96
2016-09-07T05:21:22.167212: step 2552, loss 0.0201344, acc 0.98
2016-09-07T05:21:22.852454: step 2553, loss 0.0253578, acc 0.98
2016-09-07T05:21:23.515387: step 2554, loss 0.0249991, acc 0.98
2016-09-07T05:21:24.198760: step 2555, loss 0.0454501, acc 0.98
2016-09-07T05:21:24.889310: step 2556, loss 0.0607783, acc 0.96
2016-09-07T05:21:25.589704: step 2557, loss 0.0323172, acc 0.98
2016-09-07T05:21:26.286088: step 2558, loss 0.0855344, acc 0.94
2016-09-07T05:21:27.008103: step 2559, loss 0.0805653, acc 0.96
2016-09-07T05:21:27.700719: step 2560, loss 0.0280977, acc 1
2016-09-07T05:21:28.381196: step 2561, loss 0.00935783, acc 1
2016-09-07T05:21:29.077247: step 2562, loss 0.0408176, acc 0.96
2016-09-07T05:21:29.758883: step 2563, loss 0.0501136, acc 0.98
2016-09-07T05:21:30.445363: step 2564, loss 0.0142775, acc 1
2016-09-07T05:21:31.127217: step 2565, loss 0.0507433, acc 0.96
2016-09-07T05:21:31.845228: step 2566, loss 0.0214762, acc 0.98
2016-09-07T05:21:32.538795: step 2567, loss 0.00665627, acc 1
2016-09-07T05:21:33.221876: step 2568, loss 0.0681215, acc 0.98
2016-09-07T05:21:33.912977: step 2569, loss 0.0571358, acc 0.98
2016-09-07T05:21:34.620117: step 2570, loss 0.0123705, acc 1
2016-09-07T05:21:35.304322: step 2571, loss 0.0366459, acc 1
2016-09-07T05:21:35.966048: step 2572, loss 0.140075, acc 0.94
2016-09-07T05:21:36.691027: step 2573, loss 0.0140521, acc 1
2016-09-07T05:21:37.376891: step 2574, loss 0.0213558, acc 0.98
2016-09-07T05:21:38.093510: step 2575, loss 0.0998165, acc 0.94
2016-09-07T05:21:38.790157: step 2576, loss 0.0395148, acc 0.98
2016-09-07T05:21:39.488499: step 2577, loss 0.0365702, acc 0.98
2016-09-07T05:21:40.204891: step 2578, loss 0.0239099, acc 1
2016-09-07T05:21:40.894615: step 2579, loss 0.0248818, acc 1
2016-09-07T05:21:41.600532: step 2580, loss 0.0202774, acc 1
2016-09-07T05:21:42.292033: step 2581, loss 0.021512, acc 1
2016-09-07T05:21:42.990128: step 2582, loss 0.0679719, acc 0.96
2016-09-07T05:21:43.709772: step 2583, loss 0.00495329, acc 1
2016-09-07T05:21:44.412735: step 2584, loss 0.0219215, acc 0.98
2016-09-07T05:21:45.119758: step 2585, loss 0.0182686, acc 1
2016-09-07T05:21:45.807792: step 2586, loss 0.0198173, acc 0.98
2016-09-07T05:21:46.501519: step 2587, loss 0.137473, acc 0.96
2016-09-07T05:21:47.196795: step 2588, loss 0.0214092, acc 1
2016-09-07T05:21:47.894824: step 2589, loss 0.0615084, acc 0.96
2016-09-07T05:21:48.604337: step 2590, loss 0.00381128, acc 1
2016-09-07T05:21:49.262459: step 2591, loss 0.0852823, acc 0.94
2016-09-07T05:21:49.978875: step 2592, loss 0.0253187, acc 1
2016-09-07T05:21:50.669840: step 2593, loss 0.0182473, acc 1
2016-09-07T05:21:51.362178: step 2594, loss 0.0466353, acc 0.98
2016-09-07T05:21:52.055233: step 2595, loss 0.0900607, acc 0.96
2016-09-07T05:21:52.734564: step 2596, loss 0.0625375, acc 0.96
2016-09-07T05:21:53.433264: step 2597, loss 0.158071, acc 0.98
2016-09-07T05:21:54.123677: step 2598, loss 0.0441755, acc 0.98
2016-09-07T05:21:54.846736: step 2599, loss 0.0197474, acc 1
2016-09-07T05:21:55.547581: step 2600, loss 0.0399751, acc 0.98

Evaluation:
2016-09-07T05:21:59.111554: step 2600, loss 1.61034, acc 0.744841

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-2600

2016-09-07T05:22:00.962098: step 2601, loss 0.0613431, acc 0.98
2016-09-07T05:22:01.638815: step 2602, loss 0.0245085, acc 1
2016-09-07T05:22:02.331211: step 2603, loss 0.0518734, acc 0.96
2016-09-07T05:22:03.070920: step 2604, loss 0.103128, acc 0.94
2016-09-07T05:22:03.738627: step 2605, loss 0.0257006, acc 0.98
2016-09-07T05:22:04.406929: step 2606, loss 0.0462492, acc 0.96
2016-09-07T05:22:05.085476: step 2607, loss 0.0653507, acc 0.96
2016-09-07T05:22:05.784965: step 2608, loss 0.146505, acc 0.96
2016-09-07T05:22:06.489721: step 2609, loss 0.0642144, acc 0.96
2016-09-07T05:22:07.165006: step 2610, loss 0.0324452, acc 0.98
2016-09-07T05:22:07.872603: step 2611, loss 0.00796594, acc 1
2016-09-07T05:22:08.571095: step 2612, loss 0.0853101, acc 0.96
2016-09-07T05:22:09.277580: step 2613, loss 0.0545497, acc 0.98
2016-09-07T05:22:09.986610: step 2614, loss 0.00322919, acc 1
2016-09-07T05:22:10.667348: step 2615, loss 0.053189, acc 0.98
2016-09-07T05:22:11.355076: step 2616, loss 0.0607081, acc 0.98
2016-09-07T05:22:12.023748: step 2617, loss 0.0407693, acc 0.98
2016-09-07T05:22:12.727958: step 2618, loss 0.126132, acc 0.92
2016-09-07T05:22:13.425265: step 2619, loss 0.0217676, acc 1
2016-09-07T05:22:14.129960: step 2620, loss 0.0917769, acc 0.98
2016-09-07T05:22:14.828795: step 2621, loss 0.0538986, acc 0.96
2016-09-07T05:22:15.519517: step 2622, loss 0.0499094, acc 0.96
2016-09-07T05:22:16.239146: step 2623, loss 0.00548861, acc 1
2016-09-07T05:22:16.918318: step 2624, loss 0.0145669, acc 1
2016-09-07T05:22:17.610374: step 2625, loss 0.0714416, acc 0.94
2016-09-07T05:22:18.316695: step 2626, loss 0.0925252, acc 0.96
2016-09-07T05:22:19.009090: step 2627, loss 0.0593811, acc 0.96
2016-09-07T05:22:19.701881: step 2628, loss 0.0613747, acc 0.96
2016-09-07T05:22:20.400676: step 2629, loss 0.0590834, acc 0.94
2016-09-07T05:22:21.107174: step 2630, loss 0.0514573, acc 0.96
2016-09-07T05:22:21.782918: step 2631, loss 0.0918685, acc 0.92
2016-09-07T05:22:22.492873: step 2632, loss 0.0881538, acc 0.96
2016-09-07T05:22:23.177678: step 2633, loss 0.0119112, acc 1
2016-09-07T05:22:23.872110: step 2634, loss 0.00232075, acc 1
2016-09-07T05:22:24.556299: step 2635, loss 0.0412104, acc 0.98
2016-09-07T05:22:25.225327: step 2636, loss 0.0314991, acc 0.96
2016-09-07T05:22:25.935381: step 2637, loss 0.0109098, acc 1
2016-09-07T05:22:26.614660: step 2638, loss 0.0238165, acc 1
2016-09-07T05:22:27.291315: step 2639, loss 0.0674639, acc 0.96
2016-09-07T05:22:27.983744: step 2640, loss 0.0389177, acc 0.98
2016-09-07T05:22:28.676849: step 2641, loss 0.0532499, acc 0.98
2016-09-07T05:22:29.369178: step 2642, loss 0.00532962, acc 1
2016-09-07T05:22:30.049009: step 2643, loss 0.0882345, acc 0.96
2016-09-07T05:22:30.744778: step 2644, loss 0.0128337, acc 1
2016-09-07T05:22:31.432667: step 2645, loss 0.000901707, acc 1
2016-09-07T05:22:32.126571: step 2646, loss 0.0989183, acc 0.94
2016-09-07T05:22:32.820941: step 2647, loss 0.0490734, acc 0.98
2016-09-07T05:22:33.503767: step 2648, loss 0.0645954, acc 0.98
2016-09-07T05:22:34.198574: step 2649, loss 0.0348217, acc 0.98
2016-09-07T05:22:34.856442: step 2650, loss 0.0656895, acc 0.94
2016-09-07T05:22:35.566707: step 2651, loss 0.0228522, acc 1
2016-09-07T05:22:36.252231: step 2652, loss 0.00249061, acc 1
2016-09-07T05:22:36.945114: step 2653, loss 0.000723185, acc 1
2016-09-07T05:22:37.643875: step 2654, loss 0.0236821, acc 1
2016-09-07T05:22:38.328098: step 2655, loss 0.108541, acc 0.96
2016-09-07T05:22:39.012287: step 2656, loss 0.0276069, acc 0.98
2016-09-07T05:22:39.686264: step 2657, loss 0.0111141, acc 1
2016-09-07T05:22:40.387022: step 2658, loss 0.0333672, acc 0.98
2016-09-07T05:22:41.074683: step 2659, loss 0.0215151, acc 1
2016-09-07T05:22:41.787787: step 2660, loss 0.0224317, acc 0.98
2016-09-07T05:22:42.475855: step 2661, loss 0.0740331, acc 0.98
2016-09-07T05:22:43.176037: step 2662, loss 0.0449085, acc 0.96
2016-09-07T05:22:43.893731: step 2663, loss 0.058029, acc 0.96
2016-09-07T05:22:44.571462: step 2664, loss 0.0573954, acc 0.96
2016-09-07T05:22:45.295143: step 2665, loss 0.0470595, acc 0.98
2016-09-07T05:22:45.988053: step 2666, loss 0.122844, acc 0.98
2016-09-07T05:22:46.676472: step 2667, loss 0.0719986, acc 0.96
2016-09-07T05:22:47.419586: step 2668, loss 0.0213178, acc 0.98
2016-09-07T05:22:48.090293: step 2669, loss 0.0615416, acc 0.96
2016-09-07T05:22:48.832271: step 2670, loss 0.0633375, acc 0.96
2016-09-07T05:22:49.522624: step 2671, loss 0.0707693, acc 0.96
2016-09-07T05:22:50.221059: step 2672, loss 0.0558592, acc 0.94
2016-09-07T05:22:50.896298: step 2673, loss 0.0136229, acc 1
2016-09-07T05:22:51.564986: step 2674, loss 0.0287079, acc 0.98
2016-09-07T05:22:52.254417: step 2675, loss 0.0405437, acc 0.98
2016-09-07T05:22:52.925570: step 2676, loss 0.0318111, acc 0.98
2016-09-07T05:22:53.638216: step 2677, loss 0.0961713, acc 0.94
2016-09-07T05:22:54.335162: step 2678, loss 0.0391677, acc 0.98
2016-09-07T05:22:55.035370: step 2679, loss 0.0231219, acc 1
2016-09-07T05:22:55.726297: step 2680, loss 0.0183683, acc 1
2016-09-07T05:22:56.426820: step 2681, loss 0.0356523, acc 0.98
2016-09-07T05:22:57.123779: step 2682, loss 0.123293, acc 0.96
2016-09-07T05:22:57.793007: step 2683, loss 0.0377002, acc 0.98
2016-09-07T05:22:58.494072: step 2684, loss 0.0177055, acc 1
2016-09-07T05:22:59.191932: step 2685, loss 0.0390755, acc 0.96
2016-09-07T05:22:59.879363: step 2686, loss 0.0962029, acc 0.96
2016-09-07T05:23:00.613440: step 2687, loss 0.0630855, acc 0.96
2016-09-07T05:23:01.260651: step 2688, loss 0.0284841, acc 1
2016-09-07T05:23:01.959389: step 2689, loss 0.0153946, acc 0.98
2016-09-07T05:23:02.619981: step 2690, loss 0.0722831, acc 0.96
2016-09-07T05:23:03.326957: step 2691, loss 0.0988402, acc 0.98
2016-09-07T05:23:04.008829: step 2692, loss 0.086252, acc 0.98
2016-09-07T05:23:04.695449: step 2693, loss 0.148496, acc 0.96
2016-09-07T05:23:05.410404: step 2694, loss 0.0485904, acc 0.98
2016-09-07T05:23:06.100936: step 2695, loss 0.0262291, acc 0.98
2016-09-07T05:23:06.809242: step 2696, loss 0.115205, acc 0.92
2016-09-07T05:23:07.511041: step 2697, loss 0.0274251, acc 0.98
2016-09-07T05:23:08.193622: step 2698, loss 0.0589446, acc 0.96
2016-09-07T05:23:08.902094: step 2699, loss 0.00745623, acc 1
2016-09-07T05:23:09.602013: step 2700, loss 0.0211438, acc 1

Evaluation:
2016-09-07T05:23:13.250439: step 2700, loss 1.53673, acc 0.742964

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-2700

2016-09-07T05:23:14.990418: step 2701, loss 0.0164519, acc 1
2016-09-07T05:23:15.675020: step 2702, loss 0.0653892, acc 0.98
2016-09-07T05:23:16.389634: step 2703, loss 0.0117461, acc 1
2016-09-07T05:23:17.064579: step 2704, loss 0.24743, acc 0.96
2016-09-07T05:23:17.777895: step 2705, loss 0.0388046, acc 0.98
2016-09-07T05:23:18.470779: step 2706, loss 0.0684678, acc 0.98
2016-09-07T05:23:19.173490: step 2707, loss 0.0359195, acc 0.98
2016-09-07T05:23:19.863937: step 2708, loss 0.0116215, acc 1
2016-09-07T05:23:20.552346: step 2709, loss 0.0408729, acc 1
2016-09-07T05:23:21.262042: step 2710, loss 0.0164661, acc 1
2016-09-07T05:23:21.950455: step 2711, loss 0.0247212, acc 0.98
2016-09-07T05:23:22.650207: step 2712, loss 0.105412, acc 0.94
2016-09-07T05:23:23.337239: step 2713, loss 0.0236437, acc 0.98
2016-09-07T05:23:24.041740: step 2714, loss 0.0332932, acc 1
2016-09-07T05:23:24.732840: step 2715, loss 0.0188607, acc 1
2016-09-07T05:23:25.421701: step 2716, loss 0.023852, acc 1
2016-09-07T05:23:26.149761: step 2717, loss 0.103132, acc 0.96
2016-09-07T05:23:26.851192: step 2718, loss 0.0175419, acc 0.98
2016-09-07T05:23:27.525153: step 2719, loss 0.0193508, acc 0.98
2016-09-07T05:23:28.208955: step 2720, loss 0.0565019, acc 0.98
2016-09-07T05:23:28.919423: step 2721, loss 0.0254792, acc 1
2016-09-07T05:23:29.619070: step 2722, loss 0.0365136, acc 1
2016-09-07T05:23:30.298998: step 2723, loss 0.0597342, acc 0.98
2016-09-07T05:23:31.014289: step 2724, loss 0.0406465, acc 0.98
2016-09-07T05:23:31.719956: step 2725, loss 0.00350355, acc 1
2016-09-07T05:23:32.412103: step 2726, loss 0.0174922, acc 1
2016-09-07T05:23:33.092390: step 2727, loss 0.0223378, acc 0.98
2016-09-07T05:23:33.787834: step 2728, loss 0.0171854, acc 1
2016-09-07T05:23:34.479338: step 2729, loss 0.0343924, acc 1
2016-09-07T05:23:35.176517: step 2730, loss 0.0126505, acc 1
2016-09-07T05:23:35.874762: step 2731, loss 0.0310831, acc 0.98
2016-09-07T05:23:36.591000: step 2732, loss 0.120062, acc 0.98
2016-09-07T05:23:37.279664: step 2733, loss 0.0292594, acc 0.98
2016-09-07T05:23:37.983939: step 2734, loss 0.0167522, acc 1
2016-09-07T05:23:38.675264: step 2735, loss 0.070661, acc 0.98
2016-09-07T05:23:39.358725: step 2736, loss 0.0148661, acc 1
2016-09-07T05:23:40.053422: step 2737, loss 0.00813028, acc 1
2016-09-07T05:23:40.753686: step 2738, loss 0.0879624, acc 0.96
2016-09-07T05:23:41.457280: step 2739, loss 0.0192019, acc 1
2016-09-07T05:23:42.176410: step 2740, loss 0.0438416, acc 0.96
2016-09-07T05:23:42.901399: step 2741, loss 0.0201778, acc 0.98
2016-09-07T05:23:43.592486: step 2742, loss 0.00735959, acc 1
2016-09-07T05:23:44.286610: step 2743, loss 0.00297646, acc 1
2016-09-07T05:23:45.001236: step 2744, loss 0.0874644, acc 0.96
2016-09-07T05:23:45.710497: step 2745, loss 0.0116971, acc 1
2016-09-07T05:23:46.400024: step 2746, loss 0.0523439, acc 0.98
2016-09-07T05:23:47.074922: step 2747, loss 0.0580135, acc 0.96
2016-09-07T05:23:47.787232: step 2748, loss 0.0566945, acc 0.98
2016-09-07T05:23:48.472462: step 2749, loss 0.0711704, acc 0.94
2016-09-07T05:23:49.167692: step 2750, loss 0.0630999, acc 0.94
2016-09-07T05:23:49.858706: step 2751, loss 0.0244173, acc 1
2016-09-07T05:23:50.542084: step 2752, loss 0.0554611, acc 0.98
2016-09-07T05:23:51.248997: step 2753, loss 0.170598, acc 0.94
2016-09-07T05:23:51.905912: step 2754, loss 0.0318579, acc 0.98
2016-09-07T05:23:52.601457: step 2755, loss 0.0653704, acc 0.96
2016-09-07T05:23:53.317917: step 2756, loss 0.105508, acc 0.96
2016-09-07T05:23:53.995794: step 2757, loss 0.00131038, acc 1
2016-09-07T05:23:54.697313: step 2758, loss 0.192741, acc 0.9
2016-09-07T05:23:55.410883: step 2759, loss 0.0792027, acc 0.98
2016-09-07T05:23:56.133343: step 2760, loss 0.0347729, acc 1
2016-09-07T05:23:56.811169: step 2761, loss 0.0376158, acc 0.98
2016-09-07T05:23:57.505383: step 2762, loss 0.0122708, acc 1
2016-09-07T05:23:58.222555: step 2763, loss 0.0804983, acc 0.96
2016-09-07T05:23:58.946814: step 2764, loss 0.121914, acc 0.9
2016-09-07T05:23:59.632987: step 2765, loss 0.0352051, acc 1
2016-09-07T05:24:00.318767: step 2766, loss 0.01055, acc 1
2016-09-07T05:24:01.024634: step 2767, loss 0.133883, acc 0.94
2016-09-07T05:24:01.725640: step 2768, loss 0.0375617, acc 0.96
2016-09-07T05:24:02.426559: step 2769, loss 0.048124, acc 1
2016-09-07T05:24:03.149540: step 2770, loss 0.235653, acc 0.94
2016-09-07T05:24:03.856020: step 2771, loss 0.0191653, acc 0.98
2016-09-07T05:24:04.541876: step 2772, loss 0.0335944, acc 1
2016-09-07T05:24:05.211260: step 2773, loss 0.156236, acc 0.96
2016-09-07T05:24:05.893409: step 2774, loss 0.108358, acc 0.96
2016-09-07T05:24:06.582340: step 2775, loss 0.0491476, acc 0.98
2016-09-07T05:24:07.276068: step 2776, loss 0.0614912, acc 0.98
2016-09-07T05:24:07.981072: step 2777, loss 0.0201028, acc 1
2016-09-07T05:24:08.660252: step 2778, loss 0.0297266, acc 1
2016-09-07T05:24:09.357742: step 2779, loss 0.0321831, acc 0.98
2016-09-07T05:24:10.075325: step 2780, loss 0.0298447, acc 1
2016-09-07T05:24:10.755872: step 2781, loss 0.0614096, acc 0.98
2016-09-07T05:24:11.449319: step 2782, loss 0.029494, acc 1
2016-09-07T05:24:12.142031: step 2783, loss 0.0328043, acc 1
2016-09-07T05:24:12.871810: step 2784, loss 0.0222834, acc 0.98
2016-09-07T05:24:13.542517: step 2785, loss 0.0179922, acc 1
2016-09-07T05:24:14.247004: step 2786, loss 0.0969115, acc 0.96
2016-09-07T05:24:14.942821: step 2787, loss 0.0130284, acc 1
2016-09-07T05:24:15.642021: step 2788, loss 0.0354502, acc 0.98
2016-09-07T05:24:16.340008: step 2789, loss 0.0374155, acc 0.98
2016-09-07T05:24:17.035912: step 2790, loss 0.0241376, acc 1
2016-09-07T05:24:17.728462: step 2791, loss 0.0544716, acc 0.98
2016-09-07T05:24:18.418328: step 2792, loss 0.102264, acc 0.96
2016-09-07T05:24:19.116256: step 2793, loss 0.0747021, acc 0.96
2016-09-07T05:24:19.817313: step 2794, loss 0.063526, acc 0.94
2016-09-07T05:24:20.511431: step 2795, loss 0.0195892, acc 1
2016-09-07T05:24:21.247463: step 2796, loss 0.0629446, acc 0.98
2016-09-07T05:24:21.919891: step 2797, loss 0.0624471, acc 0.98
2016-09-07T05:24:22.623652: step 2798, loss 0.0331976, acc 0.98
2016-09-07T05:24:23.314204: step 2799, loss 0.113395, acc 0.92
2016-09-07T05:24:24.018499: step 2800, loss 0.00944552, acc 1

Evaluation:
2016-09-07T05:24:27.629797: step 2800, loss 1.51624, acc 0.742026

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-2800

2016-09-07T05:24:29.425917: step 2801, loss 0.0376081, acc 1
2016-09-07T05:24:30.116722: step 2802, loss 0.0865571, acc 0.94
2016-09-07T05:24:30.849424: step 2803, loss 0.0427453, acc 0.98
2016-09-07T05:24:31.558996: step 2804, loss 0.051227, acc 0.98
2016-09-07T05:24:32.280332: step 2805, loss 0.154519, acc 0.96
2016-09-07T05:24:32.980376: step 2806, loss 0.0288483, acc 1
2016-09-07T05:24:33.687608: step 2807, loss 0.0272731, acc 0.98
2016-09-07T05:24:34.381426: step 2808, loss 0.116051, acc 0.94
2016-09-07T05:24:35.091584: step 2809, loss 0.0617834, acc 0.98
2016-09-07T05:24:35.800151: step 2810, loss 0.0565357, acc 0.96
2016-09-07T05:24:36.488862: step 2811, loss 0.0295039, acc 1
2016-09-07T05:24:37.188962: step 2812, loss 0.0592512, acc 0.96
2016-09-07T05:24:37.872213: step 2813, loss 0.0322929, acc 0.98
2016-09-07T05:24:38.551772: step 2814, loss 0.00697832, acc 1
2016-09-07T05:24:39.234078: step 2815, loss 0.0397032, acc 1
2016-09-07T05:24:39.922536: step 2816, loss 0.0238831, acc 0.98
2016-09-07T05:24:40.630948: step 2817, loss 0.0708937, acc 0.96
2016-09-07T05:24:41.314175: step 2818, loss 0.0233436, acc 0.98
2016-09-07T05:24:42.004137: step 2819, loss 0.0782373, acc 0.98
2016-09-07T05:24:42.726890: step 2820, loss 0.0371693, acc 0.98
2016-09-07T05:24:43.425788: step 2821, loss 0.037424, acc 0.98
2016-09-07T05:24:44.154488: step 2822, loss 0.0738113, acc 0.96
2016-09-07T05:24:44.835863: step 2823, loss 0.161217, acc 0.96
2016-09-07T05:24:45.551348: step 2824, loss 0.079237, acc 0.96
2016-09-07T05:24:46.245208: step 2825, loss 0.0525123, acc 0.98
2016-09-07T05:24:46.937667: step 2826, loss 0.00106893, acc 1
2016-09-07T05:24:47.620138: step 2827, loss 0.0533382, acc 0.98
2016-09-07T05:24:48.294332: step 2828, loss 0.0497921, acc 0.96
2016-09-07T05:24:48.997852: step 2829, loss 0.0382566, acc 0.98
2016-09-07T05:24:49.687345: step 2830, loss 0.047722, acc 0.96
2016-09-07T05:24:50.383495: step 2831, loss 0.0419249, acc 0.98
2016-09-07T05:24:51.082170: step 2832, loss 0.00172596, acc 1
2016-09-07T05:24:51.806562: step 2833, loss 0.111455, acc 0.98
2016-09-07T05:24:52.514935: step 2834, loss 0.0451155, acc 0.98
2016-09-07T05:24:53.186146: step 2835, loss 0.0219642, acc 1
2016-09-07T05:24:53.904416: step 2836, loss 0.0884909, acc 0.92
2016-09-07T05:24:54.598135: step 2837, loss 0.0547, acc 0.96
2016-09-07T05:24:55.286047: step 2838, loss 0.015999, acc 1
2016-09-07T05:24:55.982865: step 2839, loss 0.0844518, acc 0.98
2016-09-07T05:24:56.680213: step 2840, loss 0.0695388, acc 0.98
2016-09-07T05:24:57.409538: step 2841, loss 0.0334489, acc 0.98
2016-09-07T05:24:58.098333: step 2842, loss 0.123874, acc 0.94
2016-09-07T05:24:58.800869: step 2843, loss 0.02164, acc 0.98
2016-09-07T05:24:59.497342: step 2844, loss 0.0529064, acc 0.98
2016-09-07T05:25:00.193694: step 2845, loss 0.00626598, acc 1
2016-09-07T05:25:00.924624: step 2846, loss 0.0170321, acc 1
2016-09-07T05:25:01.599635: step 2847, loss 0.0383075, acc 0.98
2016-09-07T05:25:02.297209: step 2848, loss 0.0243513, acc 1
2016-09-07T05:25:03.010652: step 2849, loss 0.0890561, acc 0.98
2016-09-07T05:25:03.705813: step 2850, loss 0.0791768, acc 0.96
2016-09-07T05:25:04.415272: step 2851, loss 0.0284544, acc 1
2016-09-07T05:25:05.100822: step 2852, loss 0.0456099, acc 0.98
2016-09-07T05:25:05.827135: step 2853, loss 0.0627144, acc 0.94
2016-09-07T05:25:06.504598: step 2854, loss 0.0533209, acc 0.96
2016-09-07T05:25:07.197675: step 2855, loss 0.0292126, acc 0.98
2016-09-07T05:25:07.890386: step 2856, loss 0.00163683, acc 1
2016-09-07T05:25:08.574424: step 2857, loss 0.105338, acc 0.96
2016-09-07T05:25:09.288451: step 2858, loss 0.0450239, acc 0.98
2016-09-07T05:25:09.974234: step 2859, loss 0.0289668, acc 1
2016-09-07T05:25:10.718378: step 2860, loss 0.0486626, acc 0.98
2016-09-07T05:25:11.435908: step 2861, loss 0.0235631, acc 1
2016-09-07T05:25:12.138514: step 2862, loss 0.0828687, acc 0.98
2016-09-07T05:25:12.849586: step 2863, loss 0.0640571, acc 0.96
2016-09-07T05:25:13.533704: step 2864, loss 0.0591345, acc 0.96
2016-09-07T05:25:14.263284: step 2865, loss 0.0132829, acc 1
2016-09-07T05:25:14.952191: step 2866, loss 0.0789743, acc 0.98
2016-09-07T05:25:15.649240: step 2867, loss 0.102224, acc 0.96
2016-09-07T05:25:16.339853: step 2868, loss 0.113738, acc 0.96
2016-09-07T05:25:17.040271: step 2869, loss 0.0284811, acc 1
2016-09-07T05:25:17.747796: step 2870, loss 0.0967995, acc 0.96
2016-09-07T05:25:18.409039: step 2871, loss 0.0640936, acc 0.98
2016-09-07T05:25:19.102603: step 2872, loss 0.0175957, acc 0.98
2016-09-07T05:25:19.782352: step 2873, loss 0.0716813, acc 0.96
2016-09-07T05:25:20.461993: step 2874, loss 0.0383636, acc 1
2016-09-07T05:25:21.165570: step 2875, loss 0.0389821, acc 0.98
2016-09-07T05:25:21.866251: step 2876, loss 0.0190415, acc 1
2016-09-07T05:25:22.560968: step 2877, loss 0.0556372, acc 0.96
2016-09-07T05:25:23.234290: step 2878, loss 0.057796, acc 0.96
2016-09-07T05:25:23.951853: step 2879, loss 0.0388879, acc 0.98
2016-09-07T05:25:24.604685: step 2880, loss 0.00623308, acc 1
2016-09-07T05:25:25.302372: step 2881, loss 0.0465504, acc 0.96
2016-09-07T05:25:25.993438: step 2882, loss 0.0129694, acc 1
2016-09-07T05:25:26.681634: step 2883, loss 0.0427475, acc 0.98
2016-09-07T05:25:27.371166: step 2884, loss 0.0314373, acc 1
2016-09-07T05:25:28.026451: step 2885, loss 0.0291143, acc 1
2016-09-07T05:25:28.746258: step 2886, loss 0.0342393, acc 1
2016-09-07T05:25:29.434792: step 2887, loss 0.00685879, acc 1
2016-09-07T05:25:30.133350: step 2888, loss 0.014168, acc 1
2016-09-07T05:25:30.847341: step 2889, loss 0.0334905, acc 0.98
2016-09-07T05:25:31.549862: step 2890, loss 0.00211089, acc 1
2016-09-07T05:25:32.279226: step 2891, loss 0.0583228, acc 0.96
2016-09-07T05:25:32.974919: step 2892, loss 0.0377242, acc 0.98
2016-09-07T05:25:33.663350: step 2893, loss 0.0411213, acc 1
2016-09-07T05:25:34.374603: step 2894, loss 0.135219, acc 0.92
2016-09-07T05:25:35.076302: step 2895, loss 0.0737837, acc 0.96
2016-09-07T05:25:35.776601: step 2896, loss 0.0309324, acc 0.98
2016-09-07T05:25:36.436235: step 2897, loss 0.0715968, acc 0.98
2016-09-07T05:25:37.152150: step 2898, loss 0.0121289, acc 1
2016-09-07T05:25:37.823971: step 2899, loss 0.108524, acc 0.98
2016-09-07T05:25:38.506599: step 2900, loss 0.0190293, acc 0.98

Evaluation:
2016-09-07T05:25:42.072997: step 2900, loss 1.46764, acc 0.739212

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-2900

2016-09-07T05:25:43.895015: step 2901, loss 0.0613798, acc 0.96
2016-09-07T05:25:44.579901: step 2902, loss 0.0611036, acc 0.96
2016-09-07T05:25:45.289668: step 2903, loss 0.015682, acc 1
2016-09-07T05:25:45.996093: step 2904, loss 0.0350217, acc 0.98
2016-09-07T05:25:46.693647: step 2905, loss 0.0361882, acc 0.98
2016-09-07T05:25:47.381607: step 2906, loss 0.0130553, acc 1
2016-09-07T05:25:48.076674: step 2907, loss 0.0361568, acc 0.98
2016-09-07T05:25:48.779851: step 2908, loss 0.0358069, acc 0.98
2016-09-07T05:25:49.461714: step 2909, loss 0.0552115, acc 0.98
2016-09-07T05:25:50.148726: step 2910, loss 0.0421311, acc 0.96
2016-09-07T05:25:50.833751: step 2911, loss 0.0520046, acc 0.96
2016-09-07T05:25:51.532525: step 2912, loss 0.0739541, acc 0.96
2016-09-07T05:25:52.218894: step 2913, loss 0.0304016, acc 1
2016-09-07T05:25:52.899231: step 2914, loss 0.0147285, acc 1
2016-09-07T05:25:53.595775: step 2915, loss 0.0160326, acc 0.98
2016-09-07T05:25:54.292410: step 2916, loss 0.0279447, acc 1
2016-09-07T05:25:54.998514: step 2917, loss 0.110566, acc 0.96
2016-09-07T05:25:55.656756: step 2918, loss 0.0208123, acc 0.98
2016-09-07T05:25:56.364532: step 2919, loss 0.0717757, acc 0.96
2016-09-07T05:25:57.076852: step 2920, loss 0.0184745, acc 1
2016-09-07T05:25:57.757494: step 2921, loss 0.051549, acc 0.98
2016-09-07T05:25:58.445584: step 2922, loss 0.00970425, acc 1
2016-09-07T05:25:59.155589: step 2923, loss 0.0376207, acc 0.98
2016-09-07T05:25:59.855602: step 2924, loss 0.0401212, acc 0.98
2016-09-07T05:26:00.572001: step 2925, loss 0.090391, acc 0.94
2016-09-07T05:26:01.269917: step 2926, loss 0.0444062, acc 0.98
2016-09-07T05:26:01.977506: step 2927, loss 0.00279397, acc 1
2016-09-07T05:26:02.685601: step 2928, loss 0.0827999, acc 0.96
2016-09-07T05:26:03.393811: step 2929, loss 0.0155427, acc 1
2016-09-07T05:26:04.061909: step 2930, loss 0.0142942, acc 1
2016-09-07T05:26:04.759350: step 2931, loss 0.0189849, acc 1
2016-09-07T05:26:05.445355: step 2932, loss 0.0246536, acc 0.98
2016-09-07T05:26:06.151657: step 2933, loss 0.0126149, acc 1
2016-09-07T05:26:06.840697: step 2934, loss 0.019643, acc 1
2016-09-07T05:26:07.519565: step 2935, loss 0.0279908, acc 1
2016-09-07T05:26:08.222388: step 2936, loss 0.0203368, acc 1
2016-09-07T05:26:08.886044: step 2937, loss 0.0805808, acc 0.96
2016-09-07T05:26:09.595499: step 2938, loss 0.00625962, acc 1
2016-09-07T05:26:10.262459: step 2939, loss 0.0930007, acc 0.98
2016-09-07T05:26:10.935493: step 2940, loss 0.0110702, acc 1
2016-09-07T05:26:11.623547: step 2941, loss 0.0470064, acc 0.96
2016-09-07T05:26:12.331161: step 2942, loss 0.0411358, acc 0.98
2016-09-07T05:26:13.021011: step 2943, loss 0.0277442, acc 0.98
2016-09-07T05:26:13.702965: step 2944, loss 0.0631405, acc 0.98
2016-09-07T05:26:14.408391: step 2945, loss 0.0849987, acc 0.96
2016-09-07T05:26:15.097142: step 2946, loss 0.0573071, acc 0.98
2016-09-07T05:26:15.805016: step 2947, loss 0.0340998, acc 1
2016-09-07T05:26:16.497698: step 2948, loss 0.00589115, acc 1
2016-09-07T05:26:17.175486: step 2949, loss 0.0771989, acc 0.98
2016-09-07T05:26:17.881731: step 2950, loss 0.0243799, acc 0.98
2016-09-07T05:26:18.561117: step 2951, loss 0.118316, acc 0.98
2016-09-07T05:26:19.298160: step 2952, loss 0.0539347, acc 0.96
2016-09-07T05:26:20.009229: step 2953, loss 0.00306262, acc 1
2016-09-07T05:26:20.718563: step 2954, loss 0.0120625, acc 1
2016-09-07T05:26:21.401202: step 2955, loss 0.00600147, acc 1
2016-09-07T05:26:22.087356: step 2956, loss 0.144537, acc 0.94
2016-09-07T05:26:22.801287: step 2957, loss 0.0231161, acc 1
2016-09-07T05:26:23.487788: step 2958, loss 0.0229, acc 0.98
2016-09-07T05:26:24.187904: step 2959, loss 0.0191456, acc 1
2016-09-07T05:26:24.880626: step 2960, loss 0.118754, acc 0.98
2016-09-07T05:26:25.590329: step 2961, loss 0.0618085, acc 0.94
2016-09-07T05:26:26.309050: step 2962, loss 0.0121241, acc 1
2016-09-07T05:26:27.036684: step 2963, loss 0.0354076, acc 0.98
2016-09-07T05:26:27.747174: step 2964, loss 0.0663513, acc 0.96
2016-09-07T05:26:28.451938: step 2965, loss 0.0585834, acc 0.96
2016-09-07T05:26:29.169123: step 2966, loss 0.042525, acc 0.98
2016-09-07T05:26:29.872172: step 2967, loss 0.00190108, acc 1
2016-09-07T05:26:30.538199: step 2968, loss 0.186562, acc 0.94
2016-09-07T05:26:31.221269: step 2969, loss 0.0301296, acc 0.98
2016-09-07T05:26:31.901824: step 2970, loss 0.0316688, acc 0.98
2016-09-07T05:26:32.598199: step 2971, loss 0.0772869, acc 0.96
2016-09-07T05:26:33.297879: step 2972, loss 0.0420782, acc 0.98
2016-09-07T05:26:33.979660: step 2973, loss 0.070857, acc 0.98
2016-09-07T05:26:34.684797: step 2974, loss 0.0682823, acc 0.96
2016-09-07T05:26:35.363702: step 2975, loss 0.0495027, acc 0.96
2016-09-07T05:26:36.062191: step 2976, loss 0.127107, acc 0.96
2016-09-07T05:26:36.736246: step 2977, loss 0.0803719, acc 0.96
2016-09-07T05:26:37.444552: step 2978, loss 0.0467015, acc 1
2016-09-07T05:26:38.144861: step 2979, loss 0.0090099, acc 1
2016-09-07T05:26:38.822478: step 2980, loss 0.0679582, acc 0.96
2016-09-07T05:26:39.513551: step 2981, loss 0.0325304, acc 0.98
2016-09-07T05:26:40.182694: step 2982, loss 0.0274971, acc 1
2016-09-07T05:26:40.907630: step 2983, loss 0.032197, acc 0.98
2016-09-07T05:26:41.600063: step 2984, loss 0.00845818, acc 1
2016-09-07T05:26:42.289890: step 2985, loss 0.024575, acc 1
2016-09-07T05:26:43.002254: step 2986, loss 0.00952694, acc 1
2016-09-07T05:26:43.690852: step 2987, loss 0.0303089, acc 0.98
2016-09-07T05:26:44.382654: step 2988, loss 0.036919, acc 1
2016-09-07T05:26:45.061355: step 2989, loss 0.0227151, acc 1
2016-09-07T05:26:45.767787: step 2990, loss 0.00903524, acc 1
2016-09-07T05:26:46.465059: step 2991, loss 0.0456114, acc 0.98
2016-09-07T05:26:47.149400: step 2992, loss 0.0848615, acc 0.98
2016-09-07T05:26:47.819667: step 2993, loss 0.0255598, acc 1
2016-09-07T05:26:48.508367: step 2994, loss 0.0214088, acc 1
2016-09-07T05:26:49.221555: step 2995, loss 0.0971019, acc 0.96
2016-09-07T05:26:49.901026: step 2996, loss 0.049414, acc 0.96
2016-09-07T05:26:50.591442: step 2997, loss 0.048821, acc 0.96
2016-09-07T05:26:51.297545: step 2998, loss 0.0457293, acc 0.98
2016-09-07T05:26:52.005553: step 2999, loss 0.0809546, acc 0.96
2016-09-07T05:26:52.706768: step 3000, loss 0.0656997, acc 0.96

Evaluation:
2016-09-07T05:26:56.313661: step 3000, loss 1.64017, acc 0.739212

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-3000

2016-09-07T05:26:58.045717: step 3001, loss 0.132027, acc 0.98
2016-09-07T05:26:58.760330: step 3002, loss 0.0311929, acc 0.98
2016-09-07T05:26:59.426095: step 3003, loss 0.00480271, acc 1
2016-09-07T05:27:00.125806: step 3004, loss 0.0311321, acc 1
2016-09-07T05:27:00.847850: step 3005, loss 0.0461722, acc 1
2016-09-07T05:27:01.554605: step 3006, loss 0.0230752, acc 1
2016-09-07T05:27:02.257934: step 3007, loss 0.0309706, acc 0.98
2016-09-07T05:27:02.974839: step 3008, loss 0.124761, acc 0.96
2016-09-07T05:27:03.702339: step 3009, loss 0.0467854, acc 0.98
2016-09-07T05:27:04.391636: step 3010, loss 0.0689954, acc 0.98
2016-09-07T05:27:05.073463: step 3011, loss 0.0523887, acc 0.98
2016-09-07T05:27:05.761081: step 3012, loss 0.0166814, acc 1
2016-09-07T05:27:06.460189: step 3013, loss 0.00889399, acc 1
2016-09-07T05:27:07.151690: step 3014, loss 0.0349648, acc 1
2016-09-07T05:27:07.804569: step 3015, loss 0.0285871, acc 0.98
2016-09-07T05:27:08.528170: step 3016, loss 0.0427219, acc 1
2016-09-07T05:27:09.212362: step 3017, loss 0.0207559, acc 0.98
2016-09-07T05:27:09.916705: step 3018, loss 7.79235e-05, acc 1
2016-09-07T05:27:10.597262: step 3019, loss 0.0580373, acc 0.96
2016-09-07T05:27:11.328793: step 3020, loss 0.0468327, acc 0.96
2016-09-07T05:27:12.049540: step 3021, loss 0.0120607, acc 1
2016-09-07T05:27:12.733415: step 3022, loss 0.0303232, acc 0.98
2016-09-07T05:27:13.425964: step 3023, loss 0.0310154, acc 0.98
2016-09-07T05:27:14.104167: step 3024, loss 0.0328824, acc 1
2016-09-07T05:27:14.809377: step 3025, loss 0.0219307, acc 1
2016-09-07T05:27:15.501532: step 3026, loss 0.0195053, acc 1
2016-09-07T05:27:16.203894: step 3027, loss 0.0105984, acc 1
2016-09-07T05:27:16.917112: step 3028, loss 0.0269854, acc 1
2016-09-07T05:27:17.601118: step 3029, loss 0.0538161, acc 0.98
2016-09-07T05:27:18.291429: step 3030, loss 0.0161668, acc 1
2016-09-07T05:27:19.033327: step 3031, loss 0.0277233, acc 0.98
2016-09-07T05:27:19.718844: step 3032, loss 0.0630651, acc 0.98
2016-09-07T05:27:20.432844: step 3033, loss 0.0568945, acc 0.98
2016-09-07T05:27:21.096668: step 3034, loss 0.0553232, acc 0.98
2016-09-07T05:27:21.800867: step 3035, loss 0.0342055, acc 0.98
2016-09-07T05:27:22.495950: step 3036, loss 0.145739, acc 0.98
2016-09-07T05:27:23.185690: step 3037, loss 0.00122199, acc 1
2016-09-07T05:27:23.871564: step 3038, loss 0.00262003, acc 1
2016-09-07T05:27:24.556819: step 3039, loss 0.0115159, acc 1
2016-09-07T05:27:25.251238: step 3040, loss 0.0128081, acc 1
2016-09-07T05:27:25.928375: step 3041, loss 0.0170956, acc 1
2016-09-07T05:27:26.652621: step 3042, loss 0.0433871, acc 0.98
2016-09-07T05:27:27.362060: step 3043, loss 0.0280944, acc 0.98
2016-09-07T05:27:28.049414: step 3044, loss 0.057172, acc 0.96
2016-09-07T05:27:28.764930: step 3045, loss 0.00138656, acc 1
2016-09-07T05:27:29.419648: step 3046, loss 0.0207602, acc 0.98
2016-09-07T05:27:30.117946: step 3047, loss 0.0618389, acc 0.98
2016-09-07T05:27:30.837495: step 3048, loss 0.0237547, acc 0.98
2016-09-07T05:27:31.522661: step 3049, loss 0.0418576, acc 0.96
2016-09-07T05:27:32.214627: step 3050, loss 0.0565325, acc 0.96
2016-09-07T05:27:32.911832: step 3051, loss 0.149908, acc 0.96
2016-09-07T05:27:33.617189: step 3052, loss 0.0283004, acc 0.98
2016-09-07T05:27:34.275034: step 3053, loss 0.0137349, acc 1
2016-09-07T05:27:34.970438: step 3054, loss 0.0683679, acc 0.98
2016-09-07T05:27:35.663809: step 3055, loss 0.0847094, acc 0.94
2016-09-07T05:27:36.357406: step 3056, loss 0.00486732, acc 1
2016-09-07T05:27:37.059554: step 3057, loss 0.0177756, acc 1
2016-09-07T05:27:37.762439: step 3058, loss 0.00923246, acc 1
2016-09-07T05:27:38.463100: step 3059, loss 0.0305694, acc 0.98
2016-09-07T05:27:39.128428: step 3060, loss 0.103625, acc 0.94
2016-09-07T05:27:39.840952: step 3061, loss 0.0780898, acc 0.98
2016-09-07T05:27:40.540493: step 3062, loss 0.12257, acc 0.94
2016-09-07T05:27:41.241884: step 3063, loss 0.0171857, acc 1
2016-09-07T05:27:41.936995: step 3064, loss 0.014026, acc 1
2016-09-07T05:27:42.629568: step 3065, loss 0.0355376, acc 0.98
2016-09-07T05:27:43.352433: step 3066, loss 0.0331621, acc 0.98
2016-09-07T05:27:44.046321: step 3067, loss 0.0955314, acc 0.96
2016-09-07T05:27:44.731945: step 3068, loss 0.0443177, acc 0.98
2016-09-07T05:27:45.424295: step 3069, loss 0.0013928, acc 1
2016-09-07T05:27:46.132353: step 3070, loss 0.0841586, acc 0.98
2016-09-07T05:27:46.822166: step 3071, loss 0.0711175, acc 0.98
2016-09-07T05:27:47.440957: step 3072, loss 0.0251086, acc 0.977273
2016-09-07T05:27:48.152846: step 3073, loss 0.0205561, acc 0.98
2016-09-07T05:27:48.830002: step 3074, loss 0.047627, acc 0.98
2016-09-07T05:27:49.489064: step 3075, loss 0.031433, acc 0.98
2016-09-07T05:27:50.185502: step 3076, loss 0.00240063, acc 1
2016-09-07T05:27:50.883956: step 3077, loss 0.256868, acc 0.92
2016-09-07T05:27:51.593187: step 3078, loss 0.0124587, acc 1
2016-09-07T05:27:52.287258: step 3079, loss 0.00864028, acc 1
2016-09-07T05:27:53.012642: step 3080, loss 0.0112154, acc 1
2016-09-07T05:27:53.688176: step 3081, loss 0.0140368, acc 1
2016-09-07T05:27:54.364769: step 3082, loss 0.0878154, acc 0.98
2016-09-07T05:27:55.061543: step 3083, loss 0.0413611, acc 0.96
2016-09-07T05:27:55.762122: step 3084, loss 0.0922228, acc 0.98
2016-09-07T05:27:56.465026: step 3085, loss 0.0376511, acc 0.96
2016-09-07T05:27:57.128397: step 3086, loss 0.0946379, acc 0.98
2016-09-07T05:27:57.867344: step 3087, loss 0.032074, acc 0.98
2016-09-07T05:27:58.558931: step 3088, loss 0.0135883, acc 1
2016-09-07T05:27:59.236515: step 3089, loss 0.0117316, acc 1
2016-09-07T05:27:59.939668: step 3090, loss 0.0302042, acc 0.98
2016-09-07T05:28:00.678000: step 3091, loss 0.04286, acc 0.98
2016-09-07T05:28:01.394724: step 3092, loss 0.0164511, acc 1
2016-09-07T05:28:02.088321: step 3093, loss 0.0243572, acc 0.98
2016-09-07T05:28:02.772307: step 3094, loss 0.0241918, acc 1
2016-09-07T05:28:03.462950: step 3095, loss 0.0617027, acc 0.98
2016-09-07T05:28:04.159370: step 3096, loss 0.0912647, acc 0.96
2016-09-07T05:28:04.844134: step 3097, loss 0.0285749, acc 0.98
2016-09-07T05:28:05.507725: step 3098, loss 0.0225605, acc 0.98
2016-09-07T05:28:06.229980: step 3099, loss 0.00289102, acc 1
2016-09-07T05:28:06.964590: step 3100, loss 0.035412, acc 0.98

Evaluation:
2016-09-07T05:28:10.569306: step 3100, loss 1.59756, acc 0.742026

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-3100

2016-09-07T05:28:12.322349: step 3101, loss 0.0285372, acc 1
2016-09-07T05:28:13.030955: step 3102, loss 0.00154069, acc 1
2016-09-07T05:28:13.729761: step 3103, loss 0.0623077, acc 0.98
2016-09-07T05:28:14.436836: step 3104, loss 0.136605, acc 0.92
2016-09-07T05:28:15.113659: step 3105, loss 0.0255599, acc 0.98
2016-09-07T05:28:15.820715: step 3106, loss 0.0324518, acc 0.98
2016-09-07T05:28:16.520280: step 3107, loss 0.0218818, acc 1
2016-09-07T05:28:17.203671: step 3108, loss 0.0465039, acc 0.98
2016-09-07T05:28:17.908293: step 3109, loss 0.01753, acc 1
2016-09-07T05:28:18.635008: step 3110, loss 0.0246103, acc 1
2016-09-07T05:28:19.363037: step 3111, loss 0.000399821, acc 1
2016-09-07T05:28:20.054330: step 3112, loss 0.00192688, acc 1
2016-09-07T05:28:20.771803: step 3113, loss 0.0665332, acc 0.96
2016-09-07T05:28:21.480995: step 3114, loss 0.0305952, acc 0.98
2016-09-07T05:28:22.176149: step 3115, loss 0.0342552, acc 0.98
2016-09-07T05:28:22.872959: step 3116, loss 0.0084793, acc 1
2016-09-07T05:28:23.547293: step 3117, loss 0.0408977, acc 0.98
2016-09-07T05:28:24.247735: step 3118, loss 0.0499208, acc 0.98
2016-09-07T05:28:24.935863: step 3119, loss 0.0243655, acc 0.98
2016-09-07T05:28:25.622093: step 3120, loss 0.0817939, acc 0.92
2016-09-07T05:28:26.322635: step 3121, loss 0.0053341, acc 1
2016-09-07T05:28:27.030295: step 3122, loss 0.0291542, acc 0.98
2016-09-07T05:28:27.727000: step 3123, loss 0.0338587, acc 0.98
2016-09-07T05:28:28.410650: step 3124, loss 0.085343, acc 0.98
2016-09-07T05:28:29.122227: step 3125, loss 0.0327299, acc 0.98
2016-09-07T05:28:29.804093: step 3126, loss 0.0727745, acc 0.98
2016-09-07T05:28:30.481443: step 3127, loss 0.0863188, acc 0.96
2016-09-07T05:28:31.167697: step 3128, loss 0.035744, acc 1
2016-09-07T05:28:31.870519: step 3129, loss 0.0361986, acc 0.98
2016-09-07T05:28:32.552979: step 3130, loss 0.0077245, acc 1
2016-09-07T05:28:33.233483: step 3131, loss 0.000116072, acc 1
2016-09-07T05:28:33.944164: step 3132, loss 0.00866597, acc 1
2016-09-07T05:28:34.631686: step 3133, loss 0.00476684, acc 1
2016-09-07T05:28:35.301715: step 3134, loss 0.0488513, acc 0.96
2016-09-07T05:28:36.002898: step 3135, loss 0.0371785, acc 0.98
2016-09-07T05:28:36.694933: step 3136, loss 0.0691265, acc 0.98
2016-09-07T05:28:37.398379: step 3137, loss 0.0189832, acc 1
2016-09-07T05:28:38.068484: step 3138, loss 0.0593371, acc 0.96
2016-09-07T05:28:38.774608: step 3139, loss 0.0105327, acc 1
2016-09-07T05:28:39.474785: step 3140, loss 0.00530814, acc 1
2016-09-07T05:28:40.166715: step 3141, loss 0.0195225, acc 0.98
2016-09-07T05:28:40.875039: step 3142, loss 0.0186037, acc 1
2016-09-07T05:28:41.572931: step 3143, loss 0.00807978, acc 1
2016-09-07T05:28:42.267992: step 3144, loss 0.0106198, acc 1
2016-09-07T05:28:42.947501: step 3145, loss 0.00608345, acc 1
2016-09-07T05:28:43.653347: step 3146, loss 0.00613731, acc 1
2016-09-07T05:28:44.330296: step 3147, loss 0.0150961, acc 1
2016-09-07T05:28:45.005747: step 3148, loss 0.0760098, acc 0.96
2016-09-07T05:28:45.672359: step 3149, loss 0.0170236, acc 1
2016-09-07T05:28:46.366981: step 3150, loss 0.00113894, acc 1
2016-09-07T05:28:47.089778: step 3151, loss 0.157745, acc 0.96
2016-09-07T05:28:47.788798: step 3152, loss 0.0323423, acc 0.98
2016-09-07T05:28:48.473363: step 3153, loss 0.00492564, acc 1
2016-09-07T05:28:49.170506: step 3154, loss 0.00247804, acc 1
2016-09-07T05:28:49.856474: step 3155, loss 0.153352, acc 0.94
2016-09-07T05:28:50.555384: step 3156, loss 0.0203662, acc 0.98
2016-09-07T05:28:51.242958: step 3157, loss 0.0314254, acc 0.98
2016-09-07T05:28:51.957434: step 3158, loss 0.0526049, acc 0.98
2016-09-07T05:28:52.664766: step 3159, loss 0.213362, acc 0.94
2016-09-07T05:28:53.350304: step 3160, loss 0.0544286, acc 0.98
2016-09-07T05:28:54.044895: step 3161, loss 0.0269989, acc 1
2016-09-07T05:28:54.731829: step 3162, loss 0.0558318, acc 0.96
2016-09-07T05:28:55.436295: step 3163, loss 0.0978075, acc 0.96
2016-09-07T05:28:56.096349: step 3164, loss 0.0302924, acc 0.98
2016-09-07T05:28:56.777233: step 3165, loss 0.00945477, acc 1
2016-09-07T05:28:57.457355: step 3166, loss 0.0365382, acc 1
2016-09-07T05:28:58.143290: step 3167, loss 0.00112274, acc 1
2016-09-07T05:28:58.843275: step 3168, loss 0.0292017, acc 0.98
2016-09-07T05:28:59.532236: step 3169, loss 0.0317238, acc 0.98
2016-09-07T05:29:00.238450: step 3170, loss 0.0798463, acc 0.94
2016-09-07T05:29:00.903749: step 3171, loss 0.0149409, acc 1
2016-09-07T05:29:01.617938: step 3172, loss 0.0749113, acc 0.96
2016-09-07T05:29:02.297587: step 3173, loss 0.0233709, acc 1
2016-09-07T05:29:02.987724: step 3174, loss 0.116997, acc 0.96
2016-09-07T05:29:03.666519: step 3175, loss 0.0169978, acc 1
2016-09-07T05:29:04.374201: step 3176, loss 0.0458497, acc 0.96
2016-09-07T05:29:05.078848: step 3177, loss 0.00606754, acc 1
2016-09-07T05:29:05.763618: step 3178, loss 0.0977543, acc 0.98
2016-09-07T05:29:06.444147: step 3179, loss 0.0831181, acc 0.94
2016-09-07T05:29:07.138536: step 3180, loss 0.0478591, acc 0.96
2016-09-07T05:29:07.829651: step 3181, loss 0.0335018, acc 0.96
2016-09-07T05:29:08.508669: step 3182, loss 0.0263402, acc 0.98
2016-09-07T05:29:09.191127: step 3183, loss 0.0193845, acc 1
2016-09-07T05:29:09.913360: step 3184, loss 0.00102824, acc 1
2016-09-07T05:29:10.608488: step 3185, loss 0.0229749, acc 1
2016-09-07T05:29:11.300607: step 3186, loss 0.0375427, acc 0.98
2016-09-07T05:29:11.979820: step 3187, loss 0.00510366, acc 1
2016-09-07T05:29:12.657973: step 3188, loss 0.0332827, acc 0.98
2016-09-07T05:29:13.338203: step 3189, loss 0.0639909, acc 0.98
2016-09-07T05:29:14.025239: step 3190, loss 0.0345592, acc 1
2016-09-07T05:29:14.730814: step 3191, loss 0.0416885, acc 0.98
2016-09-07T05:29:15.405845: step 3192, loss 0.0339602, acc 1
2016-09-07T05:29:16.072218: step 3193, loss 0.0100664, acc 1
2016-09-07T05:29:16.762568: step 3194, loss 0.0706086, acc 0.98
2016-09-07T05:29:17.453770: step 3195, loss 0.0203325, acc 1
2016-09-07T05:29:18.124312: step 3196, loss 0.0126795, acc 1
2016-09-07T05:29:18.804107: step 3197, loss 0.189516, acc 0.94
2016-09-07T05:29:19.512004: step 3198, loss 0.0351414, acc 1
2016-09-07T05:29:20.186967: step 3199, loss 0.0247439, acc 0.98
2016-09-07T05:29:20.861285: step 3200, loss 0.0274559, acc 0.98

Evaluation:
2016-09-07T05:29:24.014008: step 3200, loss 1.77673, acc 0.73546

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-3200

2016-09-07T05:29:25.668821: step 3201, loss 0.0279979, acc 0.98
2016-09-07T05:29:26.347907: step 3202, loss 0.043424, acc 0.98
2016-09-07T05:29:27.024872: step 3203, loss 0.133625, acc 0.98
2016-09-07T05:29:27.706996: step 3204, loss 0.069498, acc 0.96
2016-09-07T05:29:28.413345: step 3205, loss 0.0320885, acc 1
2016-09-07T05:29:29.141117: step 3206, loss 0.0517025, acc 0.96
2016-09-07T05:29:29.822311: step 3207, loss 0.03559, acc 0.98
2016-09-07T05:29:30.517752: step 3208, loss 0.0744145, acc 0.98
2016-09-07T05:29:31.208185: step 3209, loss 0.0267691, acc 0.98
2016-09-07T05:29:31.886050: step 3210, loss 0.0104827, acc 1
2016-09-07T05:29:32.572159: step 3211, loss 0.000574223, acc 1
2016-09-07T05:29:33.275155: step 3212, loss 0.00976951, acc 1
2016-09-07T05:29:33.996316: step 3213, loss 0.0301864, acc 0.98
2016-09-07T05:29:34.673408: step 3214, loss 0.0534741, acc 0.96
2016-09-07T05:29:35.356392: step 3215, loss 0.140983, acc 0.94
2016-09-07T05:29:36.037350: step 3216, loss 0.00936372, acc 1
2016-09-07T05:29:36.736944: step 3217, loss 0.0244427, acc 0.98
2016-09-07T05:29:37.442538: step 3218, loss 0.0398973, acc 0.98
2016-09-07T05:29:38.098338: step 3219, loss 0.0358747, acc 0.98
2016-09-07T05:29:38.807170: step 3220, loss 0.123458, acc 0.96
2016-09-07T05:29:39.465357: step 3221, loss 0.0985009, acc 0.98
2016-09-07T05:29:40.162702: step 3222, loss 0.00589922, acc 1
2016-09-07T05:29:40.854492: step 3223, loss 0.033768, acc 0.98
2016-09-07T05:29:41.535085: step 3224, loss 0.0706696, acc 0.96
2016-09-07T05:29:42.238337: step 3225, loss 0.0720377, acc 0.98
2016-09-07T05:29:42.922517: step 3226, loss 0.0922606, acc 0.96
2016-09-07T05:29:43.627125: step 3227, loss 0.0916889, acc 0.92
2016-09-07T05:29:44.306342: step 3228, loss 0.00759415, acc 1
2016-09-07T05:29:44.983312: step 3229, loss 0.0165473, acc 0.98
2016-09-07T05:29:45.676663: step 3230, loss 0.144308, acc 0.94
2016-09-07T05:29:46.363124: step 3231, loss 0.0399163, acc 0.98
2016-09-07T05:29:47.062879: step 3232, loss 0.0087786, acc 1
2016-09-07T05:29:47.723844: step 3233, loss 0.0525373, acc 0.98
2016-09-07T05:29:48.407271: step 3234, loss 0.0911037, acc 0.96
2016-09-07T05:29:49.076124: step 3235, loss 0.0267981, acc 1
2016-09-07T05:29:49.752792: step 3236, loss 0.0755184, acc 0.96
2016-09-07T05:29:50.463713: step 3237, loss 0.0322236, acc 0.98
2016-09-07T05:29:51.187587: step 3238, loss 0.0796011, acc 0.98
2016-09-07T05:29:51.891903: step 3239, loss 0.0520864, acc 0.98
2016-09-07T05:29:52.563331: step 3240, loss 0.0127239, acc 1
2016-09-07T05:29:53.268672: step 3241, loss 0.0319859, acc 0.98
2016-09-07T05:29:53.931567: step 3242, loss 0.0897546, acc 0.96
2016-09-07T05:29:54.618618: step 3243, loss 0.0316424, acc 0.98
2016-09-07T05:29:55.300662: step 3244, loss 0.0133751, acc 1
2016-09-07T05:29:56.012610: step 3245, loss 0.0626141, acc 0.96
2016-09-07T05:29:56.727339: step 3246, loss 0.0331942, acc 0.98
2016-09-07T05:29:57.380255: step 3247, loss 0.0314772, acc 0.98
2016-09-07T05:29:58.075027: step 3248, loss 0.0795079, acc 0.96
2016-09-07T05:29:58.768676: step 3249, loss 0.00427488, acc 1
2016-09-07T05:29:59.458298: step 3250, loss 0.00731333, acc 1
2016-09-07T05:30:00.144139: step 3251, loss 0.0325191, acc 0.98
2016-09-07T05:30:00.872391: step 3252, loss 0.0917884, acc 0.96
2016-09-07T05:30:01.590332: step 3253, loss 0.0339726, acc 1
2016-09-07T05:30:02.288321: step 3254, loss 0.02073, acc 0.98
2016-09-07T05:30:02.970356: step 3255, loss 0.0248462, acc 0.98
2016-09-07T05:30:03.660509: step 3256, loss 0.033419, acc 1
2016-09-07T05:30:04.365798: step 3257, loss 0.0886189, acc 0.96
2016-09-07T05:30:05.040302: step 3258, loss 0.0517354, acc 0.96
2016-09-07T05:30:05.741234: step 3259, loss 0.0650933, acc 0.96
2016-09-07T05:30:06.436625: step 3260, loss 0.00703122, acc 1
2016-09-07T05:30:07.114163: step 3261, loss 0.0187812, acc 1
2016-09-07T05:30:07.808483: step 3262, loss 0.0616235, acc 0.96
2016-09-07T05:30:08.537530: step 3263, loss 0.0434579, acc 0.98
2016-09-07T05:30:09.221319: step 3264, loss 0.112341, acc 0.977273
2016-09-07T05:30:09.926233: step 3265, loss 0.0390818, acc 1
2016-09-07T05:30:10.608197: step 3266, loss 0.0270203, acc 1
2016-09-07T05:30:11.322454: step 3267, loss 0.0961893, acc 0.94
2016-09-07T05:30:12.008319: step 3268, loss 0.0545276, acc 0.98
2016-09-07T05:30:12.706534: step 3269, loss 0.00993239, acc 1
2016-09-07T05:30:13.399813: step 3270, loss 0.0444085, acc 0.98
2016-09-07T05:30:14.087871: step 3271, loss 0.0272911, acc 0.98
2016-09-07T05:30:14.804023: step 3272, loss 0.0704187, acc 0.96
2016-09-07T05:30:15.479075: step 3273, loss 0.0108835, acc 1
2016-09-07T05:30:16.192242: step 3274, loss 0.0124787, acc 1
2016-09-07T05:30:16.885334: step 3275, loss 0.0168779, acc 1
2016-09-07T05:30:17.575269: step 3276, loss 0.0230268, acc 0.98
2016-09-07T05:30:18.268106: step 3277, loss 0.0132487, acc 1
2016-09-07T05:30:18.952243: step 3278, loss 0.0366565, acc 0.98
2016-09-07T05:30:19.655910: step 3279, loss 0.00171044, acc 1
2016-09-07T05:30:20.325664: step 3280, loss 0.0457686, acc 0.98
2016-09-07T05:30:20.998577: step 3281, loss 0.00287984, acc 1
2016-09-07T05:30:21.701576: step 3282, loss 0.0132076, acc 1
2016-09-07T05:30:22.383841: step 3283, loss 0.015068, acc 1
2016-09-07T05:30:23.047683: step 3284, loss 0.0439434, acc 0.98
2016-09-07T05:30:23.733020: step 3285, loss 0.0268276, acc 0.98
2016-09-07T05:30:24.432182: step 3286, loss 0.00908037, acc 1
2016-09-07T05:30:25.090014: step 3287, loss 0.0109069, acc 1
2016-09-07T05:30:25.796089: step 3288, loss 0.019199, acc 1
2016-09-07T05:30:26.474754: step 3289, loss 0.0634172, acc 0.96
2016-09-07T05:30:27.146554: step 3290, loss 0.00344796, acc 1
2016-09-07T05:30:27.821490: step 3291, loss 0.00960583, acc 1
2016-09-07T05:30:28.515577: step 3292, loss 0.00279925, acc 1
2016-09-07T05:30:29.222180: step 3293, loss 0.0253846, acc 0.98
2016-09-07T05:30:29.879801: step 3294, loss 0.0178096, acc 0.98
2016-09-07T05:30:30.586547: step 3295, loss 0.00360015, acc 1
2016-09-07T05:30:31.256495: step 3296, loss 0.0901045, acc 0.92
2016-09-07T05:30:31.943399: step 3297, loss 0.175578, acc 0.96
2016-09-07T05:30:32.627725: step 3298, loss 0.00172203, acc 1
2016-09-07T05:30:33.312652: step 3299, loss 0.0304557, acc 0.98
2016-09-07T05:30:34.002314: step 3300, loss 0.0465374, acc 0.96

Evaluation:
2016-09-07T05:30:37.138406: step 3300, loss 1.96463, acc 0.73546

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-3300

2016-09-07T05:30:38.917817: step 3301, loss 0.0327837, acc 0.98
2016-09-07T05:30:39.604180: step 3302, loss 0.0197689, acc 0.98
2016-09-07T05:30:40.293292: step 3303, loss 0.0415008, acc 0.98
2016-09-07T05:30:40.966852: step 3304, loss 0.000322099, acc 1
2016-09-07T05:30:41.668239: step 3305, loss 0.0195732, acc 0.98
2016-09-07T05:30:42.361941: step 3306, loss 0.0319153, acc 1
2016-09-07T05:30:43.025251: step 3307, loss 0.00483652, acc 1
2016-09-07T05:30:43.721818: step 3308, loss 0.0442703, acc 1
2016-09-07T05:30:44.397751: step 3309, loss 0.040419, acc 0.96
2016-09-07T05:30:45.090073: step 3310, loss 0.0453483, acc 0.98
2016-09-07T05:30:45.788041: step 3311, loss 0.0125544, acc 1
2016-09-07T05:30:46.477197: step 3312, loss 0.0617312, acc 0.96
2016-09-07T05:30:47.166951: step 3313, loss 0.0174982, acc 1
2016-09-07T05:30:47.838301: step 3314, loss 0.0260216, acc 0.98
2016-09-07T05:30:48.523209: step 3315, loss 0.0376273, acc 0.98
2016-09-07T05:30:49.184557: step 3316, loss 0.0212686, acc 0.98
2016-09-07T05:30:49.865885: step 3317, loss 0.00134755, acc 1
2016-09-07T05:30:50.548987: step 3318, loss 0.0108477, acc 1
2016-09-07T05:30:51.252679: step 3319, loss 0.00850876, acc 1
2016-09-07T05:30:51.937018: step 3320, loss 0.0186742, acc 1
2016-09-07T05:30:52.620065: step 3321, loss 0.0184695, acc 1
2016-09-07T05:30:53.335041: step 3322, loss 0.0119104, acc 1
2016-09-07T05:30:54.020778: step 3323, loss 0.00692021, acc 1
2016-09-07T05:30:54.694882: step 3324, loss 0.011382, acc 1
2016-09-07T05:30:55.377825: step 3325, loss 0.0242528, acc 1
2016-09-07T05:30:56.056351: step 3326, loss 0.0118427, acc 1
2016-09-07T05:30:56.747508: step 3327, loss 0.0380884, acc 0.98
2016-09-07T05:30:57.418595: step 3328, loss 0.107553, acc 0.96
2016-09-07T05:30:58.121098: step 3329, loss 0.0534735, acc 0.96
2016-09-07T05:30:58.778295: step 3330, loss 0.0841639, acc 0.94
2016-09-07T05:30:59.461902: step 3331, loss 0.00551797, acc 1
2016-09-07T05:31:00.146666: step 3332, loss 0.036796, acc 0.98
2016-09-07T05:31:00.863605: step 3333, loss 0.0244887, acc 0.98
2016-09-07T05:31:01.543881: step 3334, loss 0.0189284, acc 0.98
2016-09-07T05:31:02.221804: step 3335, loss 0.0431359, acc 0.98
2016-09-07T05:31:02.912272: step 3336, loss 0.0176773, acc 1
2016-09-07T05:31:03.578595: step 3337, loss 0.0276181, acc 0.98
2016-09-07T05:31:04.259405: step 3338, loss 0.0616264, acc 0.98
2016-09-07T05:31:04.932995: step 3339, loss 0.0333462, acc 0.98
2016-09-07T05:31:05.595741: step 3340, loss 0.098147, acc 0.96
2016-09-07T05:31:06.274090: step 3341, loss 0.0249236, acc 0.98
2016-09-07T05:31:06.940204: step 3342, loss 0.0140902, acc 1
2016-09-07T05:31:07.631454: step 3343, loss 0.00280735, acc 1
2016-09-07T05:31:08.278753: step 3344, loss 0.0223803, acc 0.98
2016-09-07T05:31:08.986361: step 3345, loss 0.00834162, acc 1
2016-09-07T05:31:09.672631: step 3346, loss 0.0301929, acc 0.98
2016-09-07T05:31:10.351159: step 3347, loss 0.025474, acc 0.98
2016-09-07T05:31:11.028333: step 3348, loss 0.0237833, acc 0.98
2016-09-07T05:31:11.705580: step 3349, loss 0.0208025, acc 1
2016-09-07T05:31:12.415874: step 3350, loss 0.0390151, acc 0.98
2016-09-07T05:31:13.119008: step 3351, loss 0.0277214, acc 0.98
2016-09-07T05:31:13.814030: step 3352, loss 0.0465426, acc 0.96
2016-09-07T05:31:14.473596: step 3353, loss 0.0157116, acc 0.98
2016-09-07T05:31:15.163684: step 3354, loss 0.0160375, acc 1
2016-09-07T05:31:15.853369: step 3355, loss 0.0147354, acc 1
2016-09-07T05:31:16.544455: step 3356, loss 0.0125228, acc 1
2016-09-07T05:31:17.229210: step 3357, loss 0.000588353, acc 1
2016-09-07T05:31:17.912354: step 3358, loss 0.0149421, acc 1
2016-09-07T05:31:18.624418: step 3359, loss 0.0518047, acc 0.96
2016-09-07T05:31:19.343742: step 3360, loss 0.0469104, acc 0.98
2016-09-07T05:31:20.046300: step 3361, loss 0.00635128, acc 1
2016-09-07T05:31:20.735472: step 3362, loss 0.0466647, acc 0.98
2016-09-07T05:31:21.427180: step 3363, loss 0.00543461, acc 1
2016-09-07T05:31:22.136950: step 3364, loss 0.0194543, acc 1
2016-09-07T05:31:22.835612: step 3365, loss 3.44265e-06, acc 1
2016-09-07T05:31:23.501800: step 3366, loss 0.0185663, acc 0.98
2016-09-07T05:31:24.197085: step 3367, loss 0.0167337, acc 1
2016-09-07T05:31:24.893006: step 3368, loss 0.0868771, acc 0.98
2016-09-07T05:31:25.575741: step 3369, loss 0.0234656, acc 0.98
2016-09-07T05:31:26.263743: step 3370, loss 0.0227139, acc 0.98
2016-09-07T05:31:26.987392: step 3371, loss 0.0307119, acc 0.98
2016-09-07T05:31:27.684474: step 3372, loss 0.0430277, acc 0.98
2016-09-07T05:31:28.371357: step 3373, loss 0.0417397, acc 0.98
2016-09-07T05:31:29.042208: step 3374, loss 0.0858624, acc 0.98
2016-09-07T05:31:29.720843: step 3375, loss 0.0175765, acc 0.98
2016-09-07T05:31:30.425433: step 3376, loss 0.0107629, acc 1
2016-09-07T05:31:31.090616: step 3377, loss 0.0159762, acc 0.98
2016-09-07T05:31:31.808102: step 3378, loss 0.0300342, acc 1
2016-09-07T05:31:32.475121: step 3379, loss 0.0786525, acc 0.98
2016-09-07T05:31:33.146665: step 3380, loss 0.0758419, acc 0.98
2016-09-07T05:31:33.826459: step 3381, loss 0.101878, acc 0.98
2016-09-07T05:31:34.507985: step 3382, loss 0.0162497, acc 1
2016-09-07T05:31:35.193365: step 3383, loss 0.0525583, acc 0.98
2016-09-07T05:31:35.875510: step 3384, loss 0.193926, acc 0.94
2016-09-07T05:31:36.586280: step 3385, loss 0.00198817, acc 1
2016-09-07T05:31:37.254449: step 3386, loss 0.294404, acc 0.98
2016-09-07T05:31:37.943260: step 3387, loss 0.00330239, acc 1
2016-09-07T05:31:38.624253: step 3388, loss 0.000719606, acc 1
2016-09-07T05:31:39.324015: step 3389, loss 0.0464244, acc 0.98
2016-09-07T05:31:39.983069: step 3390, loss 0.0249852, acc 0.98
2016-09-07T05:31:40.682544: step 3391, loss 0.0792876, acc 0.96
2016-09-07T05:31:41.382530: step 3392, loss 0.0351322, acc 0.98
2016-09-07T05:31:42.081974: step 3393, loss 0.00231751, acc 1
2016-09-07T05:31:42.797148: step 3394, loss 0.0402168, acc 0.98
2016-09-07T05:31:43.472653: step 3395, loss 0.0894719, acc 0.94
2016-09-07T05:31:44.161097: step 3396, loss 0.0154329, acc 1
2016-09-07T05:31:44.855971: step 3397, loss 0.00130466, acc 1
2016-09-07T05:31:45.552195: step 3398, loss 0.055955, acc 0.96
2016-09-07T05:31:46.247827: step 3399, loss 0.115479, acc 0.96
2016-09-07T05:31:46.910685: step 3400, loss 0.0720289, acc 0.96

Evaluation:
2016-09-07T05:31:50.053103: step 3400, loss 1.62013, acc 0.728893

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-3400

2016-09-07T05:31:51.719816: step 3401, loss 0.0636587, acc 0.98
2016-09-07T05:31:52.395815: step 3402, loss 0.000948621, acc 1
2016-09-07T05:31:53.080328: step 3403, loss 0.00447246, acc 1
2016-09-07T05:31:53.747185: step 3404, loss 0.0441939, acc 0.98
2016-09-07T05:31:54.442477: step 3405, loss 0.0701444, acc 0.96
2016-09-07T05:31:55.109124: step 3406, loss 0.0305203, acc 0.98
2016-09-07T05:31:55.825858: step 3407, loss 0.0631543, acc 0.96
2016-09-07T05:31:56.502453: step 3408, loss 0.0172248, acc 1
2016-09-07T05:31:57.175650: step 3409, loss 0.0706074, acc 0.98
2016-09-07T05:31:57.858370: step 3410, loss 0.141774, acc 0.94
2016-09-07T05:31:58.539162: step 3411, loss 0.0243081, acc 1
2016-09-07T05:31:59.243668: step 3412, loss 0.129475, acc 0.98
2016-09-07T05:31:59.928022: step 3413, loss 0.0624468, acc 0.96
2016-09-07T05:32:00.667539: step 3414, loss 0.0474602, acc 0.98
2016-09-07T05:32:01.353816: step 3415, loss 0.0649077, acc 0.98
2016-09-07T05:32:02.034585: step 3416, loss 0.035365, acc 1
2016-09-07T05:32:02.713696: step 3417, loss 0.0600996, acc 0.96
2016-09-07T05:32:03.389734: step 3418, loss 0.0807108, acc 0.96
2016-09-07T05:32:04.100718: step 3419, loss 0.0238224, acc 1
2016-09-07T05:32:04.770439: step 3420, loss 0.00043757, acc 1
2016-09-07T05:32:05.462249: step 3421, loss 0.0732631, acc 0.98
2016-09-07T05:32:06.127070: step 3422, loss 0.0469187, acc 1
2016-09-07T05:32:06.798284: step 3423, loss 0.00368554, acc 1
2016-09-07T05:32:07.478712: step 3424, loss 0.0726114, acc 0.96
2016-09-07T05:32:08.170870: step 3425, loss 0.0393079, acc 0.98
2016-09-07T05:32:08.848592: step 3426, loss 0.0607882, acc 0.98
2016-09-07T05:32:09.544764: step 3427, loss 0.0179033, acc 1
2016-09-07T05:32:10.248079: step 3428, loss 0.018938, acc 1
2016-09-07T05:32:10.921328: step 3429, loss 0.024657, acc 1
2016-09-07T05:32:11.625193: step 3430, loss 0.0402997, acc 0.98
2016-09-07T05:32:12.333074: step 3431, loss 0.0141999, acc 1
2016-09-07T05:32:13.030088: step 3432, loss 0.00538899, acc 1
2016-09-07T05:32:13.724007: step 3433, loss 0.0293444, acc 0.98
2016-09-07T05:32:14.406038: step 3434, loss 0.0532878, acc 0.96
2016-09-07T05:32:15.132860: step 3435, loss 0.0238906, acc 0.98
2016-09-07T05:32:15.837893: step 3436, loss 0.0270849, acc 0.98
2016-09-07T05:32:16.521206: step 3437, loss 0.0130478, acc 1
2016-09-07T05:32:17.198619: step 3438, loss 0.0275359, acc 0.98
2016-09-07T05:32:17.888454: step 3439, loss 0.0749815, acc 0.96
2016-09-07T05:32:18.584177: step 3440, loss 0.0232444, acc 1
2016-09-07T05:32:19.239696: step 3441, loss 0.127649, acc 0.92
2016-09-07T05:32:19.938275: step 3442, loss 0.0102555, acc 1
2016-09-07T05:32:20.609899: step 3443, loss 0.0404456, acc 0.98
2016-09-07T05:32:21.301815: step 3444, loss 0.0269926, acc 1
2016-09-07T05:32:21.992258: step 3445, loss 0.0157279, acc 1
2016-09-07T05:32:22.697391: step 3446, loss 0.0262848, acc 0.98
2016-09-07T05:32:23.395478: step 3447, loss 0.00206903, acc 1
2016-09-07T05:32:24.061986: step 3448, loss 0.0105633, acc 1
2016-09-07T05:32:24.757370: step 3449, loss 0.0570701, acc 0.96
2016-09-07T05:32:25.416634: step 3450, loss 0.0753256, acc 0.98
2016-09-07T05:32:26.096277: step 3451, loss 0.0439569, acc 0.98
2016-09-07T05:32:26.777488: step 3452, loss 0.0422625, acc 0.98
2016-09-07T05:32:27.449083: step 3453, loss 0.0161855, acc 1
2016-09-07T05:32:28.142302: step 3454, loss 0.0562821, acc 0.96
2016-09-07T05:32:28.845093: step 3455, loss 0.0885513, acc 0.94
2016-09-07T05:32:29.493325: step 3456, loss 0.0475465, acc 0.954545
2016-09-07T05:32:30.157795: step 3457, loss 0.108558, acc 0.96
2016-09-07T05:32:30.859040: step 3458, loss 0.0108836, acc 1
2016-09-07T05:32:31.563020: step 3459, loss 0.0313433, acc 0.98
2016-09-07T05:32:32.245269: step 3460, loss 0.0296602, acc 0.98
2016-09-07T05:32:32.920749: step 3461, loss 0.0565727, acc 0.98
2016-09-07T05:32:33.606614: step 3462, loss 0.0111773, acc 1
2016-09-07T05:32:34.317644: step 3463, loss 0.0242484, acc 0.98
2016-09-07T05:32:34.988545: step 3464, loss 0.0155594, acc 1
2016-09-07T05:32:35.657944: step 3465, loss 0.0440745, acc 0.96
2016-09-07T05:32:36.353931: step 3466, loss 0.103842, acc 0.96
2016-09-07T05:32:37.038956: step 3467, loss 0.0433708, acc 0.98
2016-09-07T05:32:37.719152: step 3468, loss 0.058422, acc 0.96
2016-09-07T05:32:38.388175: step 3469, loss 0.0241153, acc 1
2016-09-07T05:32:39.092032: step 3470, loss 0.0526272, acc 0.96
2016-09-07T05:32:39.747242: step 3471, loss 0.0102745, acc 1
2016-09-07T05:32:40.447548: step 3472, loss 0.000286048, acc 1
2016-09-07T05:32:41.135436: step 3473, loss 0.0366936, acc 0.98
2016-09-07T05:32:41.818070: step 3474, loss 0.0211994, acc 1
2016-09-07T05:32:42.489672: step 3475, loss 0.00549919, acc 1
2016-09-07T05:32:43.174795: step 3476, loss 0.0380825, acc 0.96
2016-09-07T05:32:43.866147: step 3477, loss 0.0105298, acc 1
2016-09-07T05:32:44.535525: step 3478, loss 0.0911977, acc 0.94
2016-09-07T05:32:45.240384: step 3479, loss 0.0187364, acc 0.98
2016-09-07T05:32:45.954881: step 3480, loss 0.0225043, acc 0.98
2016-09-07T05:32:46.643003: step 3481, loss 0.00317671, acc 1
2016-09-07T05:32:47.319822: step 3482, loss 0.0206175, acc 1
2016-09-07T05:32:47.994531: step 3483, loss 0.0181314, acc 0.98
2016-09-07T05:32:48.719316: step 3484, loss 0.000517054, acc 1
2016-09-07T05:32:49.383182: step 3485, loss 0.00850199, acc 1
2016-09-07T05:32:50.096154: step 3486, loss 0.00892159, acc 1
2016-09-07T05:32:50.769525: step 3487, loss 0.0133402, acc 1
2016-09-07T05:32:51.452657: step 3488, loss 0.0262172, acc 1
2016-09-07T05:32:52.116819: step 3489, loss 0.00661687, acc 1
2016-09-07T05:32:52.801068: step 3490, loss 0.00502486, acc 1
2016-09-07T05:32:53.502249: step 3491, loss 0.0150469, acc 1
2016-09-07T05:32:54.181634: step 3492, loss 0.0422871, acc 0.98
2016-09-07T05:32:54.910226: step 3493, loss 0.017361, acc 0.98
2016-09-07T05:32:55.592893: step 3494, loss 0.0399289, acc 0.98
2016-09-07T05:32:56.300840: step 3495, loss 0.0094711, acc 1
2016-09-07T05:32:56.989139: step 3496, loss 0.043927, acc 0.98
2016-09-07T05:32:57.682763: step 3497, loss 0.00571484, acc 1
2016-09-07T05:32:58.382284: step 3498, loss 0.0360817, acc 0.98
2016-09-07T05:32:59.067099: step 3499, loss 0.00150569, acc 1
2016-09-07T05:32:59.769991: step 3500, loss 0.0195297, acc 0.98

Evaluation:
2016-09-07T05:33:02.950041: step 3500, loss 2.12448, acc 0.736398

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-3500

2016-09-07T05:33:04.616886: step 3501, loss 0.0578125, acc 0.96
2016-09-07T05:33:05.290924: step 3502, loss 0.0797881, acc 0.98
2016-09-07T05:33:05.981200: step 3503, loss 0.0367133, acc 0.98
2016-09-07T05:33:06.660604: step 3504, loss 0.0184933, acc 0.98
2016-09-07T05:33:07.346805: step 3505, loss 0.00133664, acc 1
2016-09-07T05:33:08.040099: step 3506, loss 0.0298752, acc 1
2016-09-07T05:33:08.715823: step 3507, loss 0.0373982, acc 0.98
2016-09-07T05:33:09.394588: step 3508, loss 0.235124, acc 0.94
2016-09-07T05:33:10.083571: step 3509, loss 0.0166806, acc 0.98
2016-09-07T05:33:10.775473: step 3510, loss 0.0203103, acc 0.98
2016-09-07T05:33:11.463427: step 3511, loss 0.0361143, acc 0.98
2016-09-07T05:33:12.149151: step 3512, loss 0.0444642, acc 0.98
2016-09-07T05:33:12.861473: step 3513, loss 0.127225, acc 0.94
2016-09-07T05:33:13.561471: step 3514, loss 0.00994334, acc 1
2016-09-07T05:33:14.251413: step 3515, loss 0.00463881, acc 1
2016-09-07T05:33:14.953661: step 3516, loss 0.0307503, acc 0.98
2016-09-07T05:33:15.632941: step 3517, loss 0.0170883, acc 1
2016-09-07T05:33:16.318094: step 3518, loss 0.00718571, acc 1
2016-09-07T05:33:16.991485: step 3519, loss 0.0503449, acc 0.98
2016-09-07T05:33:17.679709: step 3520, loss 0.0412094, acc 0.98
2016-09-07T05:33:18.361615: step 3521, loss 0.0382071, acc 0.98
2016-09-07T05:33:19.031887: step 3522, loss 0.0461059, acc 0.98
2016-09-07T05:33:19.732710: step 3523, loss 0.0366084, acc 0.98
2016-09-07T05:33:20.408264: step 3524, loss 0.0479784, acc 0.98
2016-09-07T05:33:21.114531: step 3525, loss 0.0431327, acc 0.96
2016-09-07T05:33:21.772839: step 3526, loss 0.0274778, acc 1
2016-09-07T05:33:22.489234: step 3527, loss 0.00133703, acc 1
2016-09-07T05:33:23.164504: step 3528, loss 0.0287871, acc 0.98
2016-09-07T05:33:23.866544: step 3529, loss 0.00415995, acc 1
2016-09-07T05:33:24.554267: step 3530, loss 0.00571524, acc 1
2016-09-07T05:33:25.256901: step 3531, loss 0.0310124, acc 0.98
2016-09-07T05:33:25.949503: step 3532, loss 0.06282, acc 0.98
2016-09-07T05:33:26.614687: step 3533, loss 0.0201826, acc 1
2016-09-07T05:33:27.311545: step 3534, loss 0.00493757, acc 1
2016-09-07T05:33:27.973494: step 3535, loss 0.0295434, acc 0.98
2016-09-07T05:33:28.662922: step 3536, loss 0.0164279, acc 1
2016-09-07T05:33:29.342730: step 3537, loss 0.0344132, acc 0.98
2016-09-07T05:33:30.007698: step 3538, loss 0.00212977, acc 1
2016-09-07T05:33:30.688530: step 3539, loss 0.0778022, acc 0.98
2016-09-07T05:33:31.361062: step 3540, loss 0.11755, acc 0.96
2016-09-07T05:33:32.069888: step 3541, loss 0.0422077, acc 0.98
2016-09-07T05:33:32.742609: step 3542, loss 0.0225453, acc 1
2016-09-07T05:33:33.445407: step 3543, loss 0.00365198, acc 1
2016-09-07T05:33:34.122872: step 3544, loss 0.019698, acc 1
2016-09-07T05:33:34.806472: step 3545, loss 0.0659492, acc 0.98
2016-09-07T05:33:35.494959: step 3546, loss 0.0242611, acc 1
2016-09-07T05:33:36.179511: step 3547, loss 0.0149298, acc 1
2016-09-07T05:33:36.870118: step 3548, loss 0.0456236, acc 0.98
2016-09-07T05:33:37.544351: step 3549, loss 0.0574417, acc 0.98
2016-09-07T05:33:38.227986: step 3550, loss 0.0128819, acc 1
2016-09-07T05:33:38.909099: step 3551, loss 0.0843825, acc 0.96
2016-09-07T05:33:39.606395: step 3552, loss 0.0240417, acc 1
2016-09-07T05:33:40.276981: step 3553, loss 0.0113112, acc 1
2016-09-07T05:33:40.957187: step 3554, loss 0.101992, acc 0.94
2016-09-07T05:33:41.706754: step 3555, loss 0.202904, acc 0.94
2016-09-07T05:33:42.419499: step 3556, loss 0.0160708, acc 1
2016-09-07T05:33:43.111137: step 3557, loss 0.0368238, acc 0.98
2016-09-07T05:33:43.805921: step 3558, loss 0.011634, acc 1
2016-09-07T05:33:44.489362: step 3559, loss 0.0975236, acc 0.98
2016-09-07T05:33:45.167708: step 3560, loss 0.290733, acc 0.96
2016-09-07T05:33:45.826909: step 3561, loss 0.0188609, acc 0.98
2016-09-07T05:33:46.553567: step 3562, loss 0.040678, acc 0.98
2016-09-07T05:33:47.215654: step 3563, loss 0.0178152, acc 1
2016-09-07T05:33:47.913346: step 3564, loss 0.0107992, acc 1
2016-09-07T05:33:48.613031: step 3565, loss 0.0180342, acc 1
2016-09-07T05:33:49.303683: step 3566, loss 0.0404041, acc 0.98
2016-09-07T05:33:50.002010: step 3567, loss 0.153018, acc 0.94
2016-09-07T05:33:50.661684: step 3568, loss 0.0850275, acc 0.98
2016-09-07T05:33:51.359507: step 3569, loss 0.0937122, acc 0.94
2016-09-07T05:33:52.039301: step 3570, loss 0.0308889, acc 1
2016-09-07T05:33:52.717739: step 3571, loss 0.0324212, acc 1
2016-09-07T05:33:53.392155: step 3572, loss 0.0632146, acc 1
2016-09-07T05:33:54.059382: step 3573, loss 0.0378793, acc 0.98
2016-09-07T05:33:54.744622: step 3574, loss 0.11016, acc 0.96
2016-09-07T05:33:55.422828: step 3575, loss 0.041833, acc 0.98
2016-09-07T05:33:56.118610: step 3576, loss 0.0232856, acc 1
2016-09-07T05:33:56.797849: step 3577, loss 0.0360181, acc 0.96
2016-09-07T05:33:57.468107: step 3578, loss 0.0812927, acc 0.98
2016-09-07T05:33:58.155398: step 3579, loss 0.0293694, acc 0.98
2016-09-07T05:33:58.824484: step 3580, loss 0.0169392, acc 1
2016-09-07T05:33:59.509365: step 3581, loss 0.0721391, acc 0.94
2016-09-07T05:34:00.210952: step 3582, loss 0.0661508, acc 0.98
2016-09-07T05:34:00.925150: step 3583, loss 0.11545, acc 0.96
2016-09-07T05:34:01.610574: step 3584, loss 0.0157087, acc 0.98
2016-09-07T05:34:02.282338: step 3585, loss 0.00402317, acc 1
2016-09-07T05:34:02.965790: step 3586, loss 0.0275172, acc 0.98
2016-09-07T05:34:03.656495: step 3587, loss 0.0988072, acc 0.96
2016-09-07T05:34:04.346471: step 3588, loss 0.0581905, acc 0.98
2016-09-07T05:34:05.044950: step 3589, loss 0.00206364, acc 1
2016-09-07T05:34:05.749303: step 3590, loss 0.068653, acc 0.96
2016-09-07T05:34:06.423215: step 3591, loss 0.00415807, acc 1
2016-09-07T05:34:07.118240: step 3592, loss 0.0162494, acc 1
2016-09-07T05:34:07.798488: step 3593, loss 0.0248681, acc 1
2016-09-07T05:34:08.482485: step 3594, loss 0.12332, acc 0.98
2016-09-07T05:34:09.174500: step 3595, loss 0.0597872, acc 0.98
2016-09-07T05:34:09.872969: step 3596, loss 0.0397397, acc 0.98
2016-09-07T05:34:10.582491: step 3597, loss 0.0572381, acc 0.96
2016-09-07T05:34:11.243235: step 3598, loss 0.0534717, acc 0.96
2016-09-07T05:34:11.951909: step 3599, loss 0.042514, acc 0.96
2016-09-07T05:34:12.662192: step 3600, loss 0.00904395, acc 1

Evaluation:
2016-09-07T05:34:15.789448: step 3600, loss 1.76239, acc 0.730769

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-3600

2016-09-07T05:34:17.486658: step 3601, loss 0.050275, acc 0.96
2016-09-07T05:34:18.194972: step 3602, loss 0.00653309, acc 1
2016-09-07T05:34:18.918650: step 3603, loss 0.0343322, acc 0.98
2016-09-07T05:34:19.595486: step 3604, loss 0.0768546, acc 0.98
2016-09-07T05:34:20.308837: step 3605, loss 0.0433122, acc 0.98
2016-09-07T05:34:21.002808: step 3606, loss 0.0255709, acc 0.98
2016-09-07T05:34:21.695482: step 3607, loss 0.0107951, acc 1
2016-09-07T05:34:22.356809: step 3608, loss 0.0544619, acc 0.98
2016-09-07T05:34:23.032446: step 3609, loss 0.104008, acc 0.96
2016-09-07T05:34:23.749665: step 3610, loss 0.108375, acc 0.96
2016-09-07T05:34:24.427938: step 3611, loss 0.100384, acc 0.96
2016-09-07T05:34:25.110971: step 3612, loss 0.0304811, acc 0.98
2016-09-07T05:34:25.827981: step 3613, loss 0.0405142, acc 0.98
2016-09-07T05:34:26.515379: step 3614, loss 0.0510801, acc 0.98
2016-09-07T05:34:27.206099: step 3615, loss 0.00831772, acc 1
2016-09-07T05:34:27.895512: step 3616, loss 0.0446286, acc 0.98
2016-09-07T05:34:28.627201: step 3617, loss 0.00157591, acc 1
2016-09-07T05:34:29.306457: step 3618, loss 0.0181336, acc 1
2016-09-07T05:34:29.987130: step 3619, loss 0.109826, acc 0.98
2016-09-07T05:34:30.688473: step 3620, loss 0.0606042, acc 0.98
2016-09-07T05:34:31.388047: step 3621, loss 0.0644997, acc 0.96
2016-09-07T05:34:32.099883: step 3622, loss 0.0185803, acc 1
2016-09-07T05:34:32.756492: step 3623, loss 0.0131109, acc 1
2016-09-07T05:34:33.475281: step 3624, loss 0.0208477, acc 0.98
2016-09-07T05:34:34.160457: step 3625, loss 0.0550225, acc 0.96
2016-09-07T05:34:34.864149: step 3626, loss 0.0784231, acc 0.92
2016-09-07T05:34:35.543289: step 3627, loss 0.0234461, acc 1
2016-09-07T05:34:36.219646: step 3628, loss 0.0235596, acc 0.98
2016-09-07T05:34:36.923730: step 3629, loss 0.0524038, acc 0.98
2016-09-07T05:34:37.586170: step 3630, loss 0.000379946, acc 1
2016-09-07T05:34:38.304089: step 3631, loss 0.0705332, acc 0.98
2016-09-07T05:34:39.006083: step 3632, loss 0.0552461, acc 0.96
2016-09-07T05:34:39.708437: step 3633, loss 0.154693, acc 0.96
2016-09-07T05:34:40.398962: step 3634, loss 0.030002, acc 1
2016-09-07T05:34:41.074844: step 3635, loss 0.001533, acc 1
2016-09-07T05:34:41.774314: step 3636, loss 0.0595454, acc 0.98
2016-09-07T05:34:42.450209: step 3637, loss 0.0490878, acc 0.98
2016-09-07T05:34:43.137684: step 3638, loss 0.0420191, acc 0.98
2016-09-07T05:34:43.812482: step 3639, loss 0.0102983, acc 1
2016-09-07T05:34:44.506720: step 3640, loss 0.0154726, acc 1
2016-09-07T05:34:45.186348: step 3641, loss 0.0401437, acc 0.96
2016-09-07T05:34:45.884744: step 3642, loss 0.0437869, acc 1
2016-09-07T05:34:46.563860: step 3643, loss 0.0533658, acc 0.96
2016-09-07T05:34:47.227074: step 3644, loss 0.0295803, acc 0.98
2016-09-07T05:34:47.924518: step 3645, loss 0.108923, acc 0.98
2016-09-07T05:34:48.608963: step 3646, loss 0.137138, acc 0.96
2016-09-07T05:34:49.292913: step 3647, loss 0.0392787, acc 0.98
2016-09-07T05:34:49.930653: step 3648, loss 0.0327044, acc 1
2016-09-07T05:34:50.621759: step 3649, loss 0.137287, acc 0.96
2016-09-07T05:34:51.301166: step 3650, loss 0.0220054, acc 1
2016-09-07T05:34:51.950728: step 3651, loss 0.0569547, acc 0.98
2016-09-07T05:34:52.651127: step 3652, loss 0.0245965, acc 1
2016-09-07T05:34:53.323959: step 3653, loss 0.0947058, acc 0.94
2016-09-07T05:34:54.010139: step 3654, loss 0.0147378, acc 1
2016-09-07T05:34:54.680471: step 3655, loss 0.0246997, acc 1
2016-09-07T05:34:55.360594: step 3656, loss 0.0303814, acc 0.98
2016-09-07T05:34:56.057412: step 3657, loss 0.0493808, acc 0.98
2016-09-07T05:34:56.758332: step 3658, loss 0.0332423, acc 0.98
2016-09-07T05:34:57.466072: step 3659, loss 0.0326218, acc 0.98
2016-09-07T05:34:58.152393: step 3660, loss 0.00960694, acc 1
2016-09-07T05:34:58.837973: step 3661, loss 0.114261, acc 0.96
2016-09-07T05:34:59.532057: step 3662, loss 0.0230847, acc 0.98
2016-09-07T05:35:00.234888: step 3663, loss 0.000274286, acc 1
2016-09-07T05:35:00.931463: step 3664, loss 0.0257199, acc 0.98
2016-09-07T05:35:01.599612: step 3665, loss 0.0188456, acc 0.98
2016-09-07T05:35:02.306662: step 3666, loss 0.0137386, acc 1
2016-09-07T05:35:02.997300: step 3667, loss 0.165108, acc 0.94
2016-09-07T05:35:03.678293: step 3668, loss 0.00143913, acc 1
2016-09-07T05:35:04.363376: step 3669, loss 0.0450392, acc 0.96
2016-09-07T05:35:05.055725: step 3670, loss 0.134471, acc 0.98
2016-09-07T05:35:05.780079: step 3671, loss 0.00144901, acc 1
2016-09-07T05:35:06.480266: step 3672, loss 0.0626384, acc 0.98
2016-09-07T05:35:07.188165: step 3673, loss 0.000122749, acc 1
2016-09-07T05:35:07.888006: step 3674, loss 0.0360324, acc 0.98
2016-09-07T05:35:08.586041: step 3675, loss 0.027203, acc 1
2016-09-07T05:35:09.269993: step 3676, loss 0.0306566, acc 0.98
2016-09-07T05:35:09.956756: step 3677, loss 0.049902, acc 0.98
2016-09-07T05:35:10.655003: step 3678, loss 0.00192455, acc 1
2016-09-07T05:35:11.319800: step 3679, loss 0.0131997, acc 1
2016-09-07T05:35:12.014064: step 3680, loss 0.0530553, acc 1
2016-09-07T05:35:12.720248: step 3681, loss 0.0105187, acc 1
2016-09-07T05:35:13.418250: step 3682, loss 0.0328536, acc 0.96
2016-09-07T05:35:14.149369: step 3683, loss 0.0336348, acc 1
2016-09-07T05:35:14.841594: step 3684, loss 0.0565646, acc 0.98
2016-09-07T05:35:15.528651: step 3685, loss 0.0448972, acc 0.96
2016-09-07T05:35:16.231404: step 3686, loss 0.0144849, acc 1
2016-09-07T05:35:16.905152: step 3687, loss 0.00728587, acc 1
2016-09-07T05:35:17.632958: step 3688, loss 0.00377042, acc 1
2016-09-07T05:35:18.331984: step 3689, loss 0.0487678, acc 0.98
2016-09-07T05:35:19.039618: step 3690, loss 0.0179183, acc 1
2016-09-07T05:35:19.720486: step 3691, loss 0.0238078, acc 1
2016-09-07T05:35:20.441502: step 3692, loss 0.0445353, acc 0.98
2016-09-07T05:35:21.150634: step 3693, loss 0.0898177, acc 0.96
2016-09-07T05:35:21.840799: step 3694, loss 0.00527467, acc 1
2016-09-07T05:35:22.519822: step 3695, loss 0.0217178, acc 1
2016-09-07T05:35:23.196590: step 3696, loss 0.017942, acc 1
2016-09-07T05:35:23.897026: step 3697, loss 0.0254025, acc 0.98
2016-09-07T05:35:24.578964: step 3698, loss 0.0145188, acc 1
2016-09-07T05:35:25.281802: step 3699, loss 0.0497809, acc 0.98
2016-09-07T05:35:25.972309: step 3700, loss 0.0330818, acc 0.98

Evaluation:
2016-09-07T05:35:29.140495: step 3700, loss 1.80444, acc 0.737336

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-3700

2016-09-07T05:35:30.912272: step 3701, loss 0.0025942, acc 1
2016-09-07T05:35:31.621616: step 3702, loss 0.0255282, acc 1
2016-09-07T05:35:32.326259: step 3703, loss 0.0339147, acc 0.98
2016-09-07T05:35:32.995060: step 3704, loss 0.0212281, acc 0.98
2016-09-07T05:35:33.659833: step 3705, loss 0.0212429, acc 0.98
2016-09-07T05:35:34.343226: step 3706, loss 0.00253063, acc 1
2016-09-07T05:35:35.016813: step 3707, loss 0.00775159, acc 1
2016-09-07T05:35:35.715398: step 3708, loss 0.0208624, acc 1
2016-09-07T05:35:36.398318: step 3709, loss 0.0140482, acc 1
2016-09-07T05:35:37.115321: step 3710, loss 0.0145144, acc 1
2016-09-07T05:35:37.797066: step 3711, loss 0.0489767, acc 0.96
2016-09-07T05:35:38.489937: step 3712, loss 0.115761, acc 0.92
2016-09-07T05:35:39.161863: step 3713, loss 0.0501847, acc 0.98
2016-09-07T05:35:39.853404: step 3714, loss 0.0602187, acc 0.96
2016-09-07T05:35:40.554002: step 3715, loss 0.129418, acc 0.96
2016-09-07T05:35:41.231348: step 3716, loss 0.0372806, acc 0.98
2016-09-07T05:35:41.922140: step 3717, loss 0.0229729, acc 1
2016-09-07T05:35:42.584917: step 3718, loss 0.0229176, acc 1
2016-09-07T05:35:43.278783: step 3719, loss 0.0511897, acc 0.96
2016-09-07T05:35:43.971521: step 3720, loss 0.0159361, acc 1
2016-09-07T05:35:44.669582: step 3721, loss 0.0830052, acc 0.96
2016-09-07T05:35:45.360940: step 3722, loss 0.00337949, acc 1
2016-09-07T05:35:46.037201: step 3723, loss 0.00512162, acc 1
2016-09-07T05:35:46.751559: step 3724, loss 0.0914427, acc 0.96
2016-09-07T05:35:47.427800: step 3725, loss 0.105346, acc 0.96
2016-09-07T05:35:48.134274: step 3726, loss 0.00112165, acc 1
2016-09-07T05:35:48.826991: step 3727, loss 0.0241737, acc 0.98
2016-09-07T05:35:49.505480: step 3728, loss 0.0794014, acc 0.96
2016-09-07T05:35:50.194344: step 3729, loss 0.00466448, acc 1
2016-09-07T05:35:50.858108: step 3730, loss 0.0207564, acc 0.98
2016-09-07T05:35:51.565769: step 3731, loss 0.0270118, acc 0.98
2016-09-07T05:35:52.274424: step 3732, loss 0.00396042, acc 1
2016-09-07T05:35:52.958459: step 3733, loss 0.0119479, acc 1
2016-09-07T05:35:53.667884: step 3734, loss 0.0692491, acc 0.98
2016-09-07T05:35:54.340878: step 3735, loss 0.0387525, acc 0.98
2016-09-07T05:35:55.043489: step 3736, loss 0.0133252, acc 1
2016-09-07T05:35:55.712688: step 3737, loss 0.0504438, acc 0.96
2016-09-07T05:35:56.403044: step 3738, loss 0.0596677, acc 0.96
2016-09-07T05:35:57.096921: step 3739, loss 0.0184202, acc 1
2016-09-07T05:35:57.768284: step 3740, loss 0.0180458, acc 0.98
2016-09-07T05:35:58.444601: step 3741, loss 0.00820515, acc 1
2016-09-07T05:35:59.132281: step 3742, loss 0.059334, acc 0.98
2016-09-07T05:35:59.822034: step 3743, loss 0.0536588, acc 0.98
2016-09-07T05:36:00.541900: step 3744, loss 0.0304388, acc 0.98
2016-09-07T05:36:01.270145: step 3745, loss 0.0241839, acc 0.98
2016-09-07T05:36:01.975059: step 3746, loss 0.0438198, acc 0.98
2016-09-07T05:36:02.662431: step 3747, loss 0.0603918, acc 0.96
2016-09-07T05:36:03.341345: step 3748, loss 0.0547344, acc 0.98
2016-09-07T05:36:04.034524: step 3749, loss 0.0437315, acc 0.96
2016-09-07T05:36:04.723488: step 3750, loss 0.0512158, acc 0.96
2016-09-07T05:36:05.388838: step 3751, loss 0.0216968, acc 1
2016-09-07T05:36:06.081332: step 3752, loss 0.0755991, acc 0.96
2016-09-07T05:36:06.766792: step 3753, loss 0.0231627, acc 1
2016-09-07T05:36:07.443600: step 3754, loss 0.0134631, acc 1
2016-09-07T05:36:08.146439: step 3755, loss 0.00954193, acc 1
2016-09-07T05:36:08.834028: step 3756, loss 0.154101, acc 0.96
2016-09-07T05:36:09.514859: step 3757, loss 0.0205296, acc 1
2016-09-07T05:36:10.189978: step 3758, loss 0.0433595, acc 0.98
2016-09-07T05:36:10.874533: step 3759, loss 0.0289041, acc 0.98
2016-09-07T05:36:11.539603: step 3760, loss 0.100007, acc 0.98
2016-09-07T05:36:12.219873: step 3761, loss 0.0898309, acc 0.98
2016-09-07T05:36:12.915397: step 3762, loss 0.0523856, acc 0.98
2016-09-07T05:36:13.600703: step 3763, loss 0.0578407, acc 0.96
2016-09-07T05:36:14.298295: step 3764, loss 0.0447761, acc 0.98
2016-09-07T05:36:14.973267: step 3765, loss 0.0690503, acc 0.96
2016-09-07T05:36:15.676447: step 3766, loss 0.044556, acc 0.98
2016-09-07T05:36:16.380444: step 3767, loss 0.0426577, acc 0.98
2016-09-07T05:36:17.065132: step 3768, loss 0.0439849, acc 0.96
2016-09-07T05:36:17.754389: step 3769, loss 0.0960167, acc 0.96
2016-09-07T05:36:18.453429: step 3770, loss 0.0220587, acc 1
2016-09-07T05:36:19.158272: step 3771, loss 0.0451963, acc 0.98
2016-09-07T05:36:19.835991: step 3772, loss 0.077233, acc 0.98
2016-09-07T05:36:20.553554: step 3773, loss 0.0223203, acc 1
2016-09-07T05:36:21.242687: step 3774, loss 0.0654237, acc 0.96
2016-09-07T05:36:21.937072: step 3775, loss 0.126269, acc 0.94
2016-09-07T05:36:22.622600: step 3776, loss 0.0286264, acc 0.98
2016-09-07T05:36:23.309455: step 3777, loss 0.0178213, acc 1
2016-09-07T05:36:24.004556: step 3778, loss 0.0737746, acc 0.98
2016-09-07T05:36:24.684558: step 3779, loss 0.012959, acc 1
2016-09-07T05:36:25.358980: step 3780, loss 0.120489, acc 0.92
2016-09-07T05:36:26.039109: step 3781, loss 0.0646558, acc 0.98
2016-09-07T05:36:26.735285: step 3782, loss 0.0474906, acc 0.98
2016-09-07T05:36:27.415793: step 3783, loss 0.0146966, acc 1
2016-09-07T05:36:28.115790: step 3784, loss 0.0250409, acc 1
2016-09-07T05:36:28.815720: step 3785, loss 0.0107994, acc 1
2016-09-07T05:36:29.492050: step 3786, loss 0.00724115, acc 1
2016-09-07T05:36:30.154686: step 3787, loss 0.0245444, acc 0.98
2016-09-07T05:36:30.826308: step 3788, loss 0.0146586, acc 1
2016-09-07T05:36:31.522794: step 3789, loss 0.00883463, acc 1
2016-09-07T05:36:32.206561: step 3790, loss 0.0528351, acc 0.96
2016-09-07T05:36:32.891377: step 3791, loss 0.00996269, acc 1
2016-09-07T05:36:33.554227: step 3792, loss 0.0152659, acc 1
2016-09-07T05:36:34.237132: step 3793, loss 0.0196038, acc 1
2016-09-07T05:36:34.949249: step 3794, loss 0.00049755, acc 1
2016-09-07T05:36:35.638669: step 3795, loss 0.0311611, acc 0.98
2016-09-07T05:36:36.318659: step 3796, loss 0.078088, acc 0.98
2016-09-07T05:36:37.019334: step 3797, loss 0.0186054, acc 1
2016-09-07T05:36:37.702951: step 3798, loss 0.0489089, acc 0.98
2016-09-07T05:36:38.406682: step 3799, loss 0.0204422, acc 1
2016-09-07T05:36:39.061384: step 3800, loss 0.0415472, acc 0.98

Evaluation:
2016-09-07T05:36:42.210937: step 3800, loss 1.79388, acc 0.744841

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-3800

2016-09-07T05:36:43.846586: step 3801, loss 0.150435, acc 0.94
2016-09-07T05:36:44.542703: step 3802, loss 0.0262927, acc 0.98
2016-09-07T05:36:45.220563: step 3803, loss 0.0129307, acc 1
2016-09-07T05:36:45.911105: step 3804, loss 0.00541858, acc 1
2016-09-07T05:36:46.585544: step 3805, loss 0.0215574, acc 0.98
2016-09-07T05:36:47.254251: step 3806, loss 0.0138157, acc 1
2016-09-07T05:36:47.977128: step 3807, loss 0.00488133, acc 1
2016-09-07T05:36:48.644988: step 3808, loss 0.00695802, acc 1
2016-09-07T05:36:49.351683: step 3809, loss 0.0094193, acc 1
2016-09-07T05:36:50.038225: step 3810, loss 0.023568, acc 0.98
2016-09-07T05:36:50.727259: step 3811, loss 0.0060859, acc 1
2016-09-07T05:36:51.399927: step 3812, loss 0.0297976, acc 0.98
2016-09-07T05:36:52.089958: step 3813, loss 0.0216986, acc 1
2016-09-07T05:36:52.767250: step 3814, loss 0.0683403, acc 0.98
2016-09-07T05:36:53.441991: step 3815, loss 0.00922055, acc 1
2016-09-07T05:36:54.144768: step 3816, loss 0.0405458, acc 0.96
2016-09-07T05:36:54.828138: step 3817, loss 0.118088, acc 0.98
2016-09-07T05:36:55.516959: step 3818, loss 0.0415676, acc 0.96
2016-09-07T05:36:56.209353: step 3819, loss 0.0593764, acc 0.98
2016-09-07T05:36:56.899312: step 3820, loss 0.0708244, acc 0.96
2016-09-07T05:36:57.598906: step 3821, loss 0.00551914, acc 1
2016-09-07T05:36:58.277604: step 3822, loss 0.0567927, acc 0.96
2016-09-07T05:36:58.973745: step 3823, loss 0.0111046, acc 1
2016-09-07T05:36:59.666125: step 3824, loss 0.0514057, acc 0.98
2016-09-07T05:37:00.391426: step 3825, loss 0.0306452, acc 0.98
2016-09-07T05:37:01.074148: step 3826, loss 0.0182783, acc 1
2016-09-07T05:37:01.759846: step 3827, loss 0.0101475, acc 1
2016-09-07T05:37:02.482281: step 3828, loss 0.0299289, acc 0.98
2016-09-07T05:37:03.159261: step 3829, loss 0.0164308, acc 1
2016-09-07T05:37:03.842256: step 3830, loss 0.0199791, acc 0.98
2016-09-07T05:37:04.519658: step 3831, loss 0.0123292, acc 1
2016-09-07T05:37:05.205284: step 3832, loss 0.11888, acc 0.96
2016-09-07T05:37:05.889762: step 3833, loss 0.0296091, acc 0.98
2016-09-07T05:37:06.576512: step 3834, loss 0.0325038, acc 0.98
2016-09-07T05:37:07.288487: step 3835, loss 0.0556596, acc 0.96
2016-09-07T05:37:07.973091: step 3836, loss 0.0340348, acc 0.98
2016-09-07T05:37:08.640149: step 3837, loss 0.00668741, acc 1
2016-09-07T05:37:09.327337: step 3838, loss 0.0414287, acc 0.98
2016-09-07T05:37:10.007114: step 3839, loss 0.0177136, acc 0.98
2016-09-07T05:37:10.653335: step 3840, loss 0.0292234, acc 0.977273
2016-09-07T05:37:11.324412: step 3841, loss 0.0421912, acc 0.98
2016-09-07T05:37:12.017600: step 3842, loss 0.0182362, acc 1
2016-09-07T05:37:12.670547: step 3843, loss 0.0369457, acc 0.98
2016-09-07T05:37:13.388332: step 3844, loss 0.0173151, acc 1
2016-09-07T05:37:14.079683: step 3845, loss 0.0185622, acc 1
2016-09-07T05:37:14.765763: step 3846, loss 0.0380634, acc 0.98
2016-09-07T05:37:15.441331: step 3847, loss 0.0190814, acc 0.98
2016-09-07T05:37:16.118015: step 3848, loss 0.0130936, acc 1
2016-09-07T05:37:16.811667: step 3849, loss 0.0442936, acc 0.98
2016-09-07T05:37:17.461812: step 3850, loss 0.0233995, acc 1
2016-09-07T05:37:18.186581: step 3851, loss 0.00170444, acc 1
2016-09-07T05:37:18.867891: step 3852, loss 0.0016578, acc 1
2016-09-07T05:37:19.553137: step 3853, loss 0.0500828, acc 0.98
2016-09-07T05:37:20.250386: step 3854, loss 0.0148311, acc 1
2016-09-07T05:37:20.930856: step 3855, loss 0.00569116, acc 1
2016-09-07T05:37:21.606640: step 3856, loss 0.014291, acc 1
2016-09-07T05:37:22.270340: step 3857, loss 0.0153735, acc 1
2016-09-07T05:37:22.982879: step 3858, loss 0.000366542, acc 1
2016-09-07T05:37:23.656490: step 3859, loss 0.0409794, acc 0.98
2016-09-07T05:37:24.335723: step 3860, loss 0.0275548, acc 0.98
2016-09-07T05:37:25.019158: step 3861, loss 0.0470755, acc 0.98
2016-09-07T05:37:25.705685: step 3862, loss 0.012988, acc 1
2016-09-07T05:37:26.410433: step 3863, loss 0.0299176, acc 0.98
2016-09-07T05:37:27.118605: step 3864, loss 0.000194442, acc 1
2016-09-07T05:37:27.843563: step 3865, loss 0.0107656, acc 1
2016-09-07T05:37:28.530235: step 3866, loss 0.0196425, acc 0.98
2016-09-07T05:37:29.206318: step 3867, loss 0.0317219, acc 0.98
2016-09-07T05:37:29.891204: step 3868, loss 0.0190092, acc 1
2016-09-07T05:37:30.574904: step 3869, loss 0.0021713, acc 1
2016-09-07T05:37:31.270573: step 3870, loss 0.0622208, acc 0.98
2016-09-07T05:37:31.943364: step 3871, loss 0.0218547, acc 0.98
2016-09-07T05:37:32.653735: step 3872, loss 0.189649, acc 0.9
2016-09-07T05:37:33.366283: step 3873, loss 0.0814833, acc 0.96
2016-09-07T05:37:34.043230: step 3874, loss 0.123977, acc 0.98
2016-09-07T05:37:34.741283: step 3875, loss 0.00286718, acc 1
2016-09-07T05:37:35.423108: step 3876, loss 0.028241, acc 0.98
2016-09-07T05:37:36.125193: step 3877, loss 0.0152113, acc 0.98
2016-09-07T05:37:36.794212: step 3878, loss 0.0738015, acc 0.96
2016-09-07T05:37:37.502672: step 3879, loss 0.0167464, acc 1
2016-09-07T05:37:38.191085: step 3880, loss 0.00790514, acc 1
2016-09-07T05:37:38.888127: step 3881, loss 0.0997819, acc 0.96
2016-09-07T05:37:39.590791: step 3882, loss 0.0171323, acc 1
2016-09-07T05:37:40.264394: step 3883, loss 0.0122747, acc 1
2016-09-07T05:37:40.981629: step 3884, loss 0.00875868, acc 1
2016-09-07T05:37:41.668795: step 3885, loss 0.0108821, acc 1
2016-09-07T05:37:42.344102: step 3886, loss 0.0391654, acc 0.98
2016-09-07T05:37:43.023090: step 3887, loss 0.013948, acc 1
2016-09-07T05:37:43.715270: step 3888, loss 0.0291724, acc 1
2016-09-07T05:37:44.398236: step 3889, loss 0.0169048, acc 1
2016-09-07T05:37:45.079542: step 3890, loss 0.0642243, acc 0.96
2016-09-07T05:37:45.787702: step 3891, loss 0.0140439, acc 1
2016-09-07T05:37:46.470652: step 3892, loss 0.00580247, acc 1
2016-09-07T05:37:47.159044: step 3893, loss 0.0366305, acc 0.98
2016-09-07T05:37:47.859709: step 3894, loss 0.000851311, acc 1
2016-09-07T05:37:48.541531: step 3895, loss 0.00883788, acc 1
2016-09-07T05:37:49.234837: step 3896, loss 0.0319866, acc 1
2016-09-07T05:37:49.897080: step 3897, loss 0.0377419, acc 0.98
2016-09-07T05:37:50.598968: step 3898, loss 0.0838811, acc 0.96
2016-09-07T05:37:51.267800: step 3899, loss 0.0035901, acc 1
2016-09-07T05:37:51.953280: step 3900, loss 0.0496461, acc 0.98

Evaluation:
2016-09-07T05:37:55.091627: step 3900, loss 1.77601, acc 0.736398

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-3900

2016-09-07T05:37:56.820682: step 3901, loss 0.0311728, acc 1
2016-09-07T05:37:57.507501: step 3902, loss 0.00287963, acc 1
2016-09-07T05:37:58.204232: step 3903, loss 0.0152613, acc 1
2016-09-07T05:37:58.894908: step 3904, loss 0.0469525, acc 0.96
2016-09-07T05:37:59.559954: step 3905, loss 0.0124696, acc 1
2016-09-07T05:38:00.284453: step 3906, loss 0.0549526, acc 0.98
2016-09-07T05:38:00.985648: step 3907, loss 0.0172495, acc 1
2016-09-07T05:38:01.675854: step 3908, loss 0.0310929, acc 0.98
2016-09-07T05:38:02.362758: step 3909, loss 0.0120817, acc 1
2016-09-07T05:38:03.037740: step 3910, loss 0.0457323, acc 0.98
2016-09-07T05:38:03.749542: step 3911, loss 0.00390034, acc 1
2016-09-07T05:38:04.405474: step 3912, loss 0.0173791, acc 1
2016-09-07T05:38:05.111025: step 3913, loss 0.00078969, acc 1
2016-09-07T05:38:05.804829: step 3914, loss 0.0135011, acc 1
2016-09-07T05:38:06.477722: step 3915, loss 0.0287356, acc 0.98
2016-09-07T05:38:07.162376: step 3916, loss 0.139163, acc 0.92
2016-09-07T05:38:07.836074: step 3917, loss 0.01602, acc 1
2016-09-07T05:38:08.511853: step 3918, loss 0.00614484, acc 1
2016-09-07T05:38:09.170814: step 3919, loss 0.0190735, acc 0.98
2016-09-07T05:38:09.885682: step 3920, loss 0.076155, acc 0.96
2016-09-07T05:38:10.587212: step 3921, loss 0.024375, acc 0.98
2016-09-07T05:38:11.274078: step 3922, loss 0.0268305, acc 0.98
2016-09-07T05:38:11.972450: step 3923, loss 0.013856, acc 1
2016-09-07T05:38:12.644396: step 3924, loss 0.0292405, acc 0.98
2016-09-07T05:38:13.321768: step 3925, loss 0.0166454, acc 1
2016-09-07T05:38:13.987066: step 3926, loss 0.0771348, acc 0.98
2016-09-07T05:38:14.710325: step 3927, loss 0.0413152, acc 0.96
2016-09-07T05:38:15.392612: step 3928, loss 0.00779391, acc 1
2016-09-07T05:38:16.089052: step 3929, loss 0.0400925, acc 0.98
2016-09-07T05:38:16.794327: step 3930, loss 0.0413751, acc 0.98
2016-09-07T05:38:17.500108: step 3931, loss 0.0244502, acc 1
2016-09-07T05:38:18.222654: step 3932, loss 0.103619, acc 0.94
2016-09-07T05:38:18.907430: step 3933, loss 0.00844185, acc 1
2016-09-07T05:38:19.603704: step 3934, loss 0.0960905, acc 0.94
2016-09-07T05:38:20.316637: step 3935, loss 0.0300762, acc 0.98
2016-09-07T05:38:20.994228: step 3936, loss 0.0110479, acc 1
2016-09-07T05:38:21.686830: step 3937, loss 0.0368192, acc 0.98
2016-09-07T05:38:22.381840: step 3938, loss 0.0554165, acc 0.96
2016-09-07T05:38:23.091380: step 3939, loss 0.113335, acc 0.96
2016-09-07T05:38:23.785455: step 3940, loss 0.0193721, acc 1
2016-09-07T05:38:24.488905: step 3941, loss 0.00353514, acc 1
2016-09-07T05:38:25.152559: step 3942, loss 0.0143553, acc 0.98
2016-09-07T05:38:25.849026: step 3943, loss 0.0525667, acc 0.98
2016-09-07T05:38:26.534059: step 3944, loss 0.0174277, acc 1
2016-09-07T05:38:27.206676: step 3945, loss 0.0324091, acc 1
2016-09-07T05:38:27.921265: step 3946, loss 0.0132822, acc 1
2016-09-07T05:38:28.630500: step 3947, loss 0.0422787, acc 0.98
2016-09-07T05:38:29.319208: step 3948, loss 0.0426235, acc 0.98
2016-09-07T05:38:30.001062: step 3949, loss 0.00345714, acc 1
2016-09-07T05:38:30.686736: step 3950, loss 0.0747871, acc 0.98
2016-09-07T05:38:31.379053: step 3951, loss 0.0684403, acc 0.96
2016-09-07T05:38:32.070699: step 3952, loss 0.00781278, acc 1
2016-09-07T05:38:32.786912: step 3953, loss 0.0301007, acc 0.98
2016-09-07T05:38:33.482540: step 3954, loss 0.032107, acc 0.98
2016-09-07T05:38:34.164079: step 3955, loss 0.00210065, acc 1
2016-09-07T05:38:34.868626: step 3956, loss 0.0221325, acc 1
2016-09-07T05:38:35.553149: step 3957, loss 0.0201617, acc 0.98
2016-09-07T05:38:36.243152: step 3958, loss 0.00532949, acc 1
2016-09-07T05:38:36.922487: step 3959, loss 0.0101612, acc 1
2016-09-07T05:38:37.637959: step 3960, loss 0.0225293, acc 1
2016-09-07T05:38:38.323870: step 3961, loss 0.0208341, acc 1
2016-09-07T05:38:39.005891: step 3962, loss 0.0145511, acc 1
2016-09-07T05:38:39.677419: step 3963, loss 0.000722084, acc 1
2016-09-07T05:38:40.364127: step 3964, loss 0.0123766, acc 1
2016-09-07T05:38:41.066656: step 3965, loss 0.0834539, acc 0.98
2016-09-07T05:38:41.742623: step 3966, loss 0.00718413, acc 1
2016-09-07T05:38:42.444673: step 3967, loss 0.0166557, acc 0.98
2016-09-07T05:38:43.147306: step 3968, loss 0.000485799, acc 1
2016-09-07T05:38:43.840777: step 3969, loss 0.00950936, acc 1
2016-09-07T05:38:44.522280: step 3970, loss 0.0030932, acc 1
2016-09-07T05:38:45.225698: step 3971, loss 0.0184056, acc 1
2016-09-07T05:38:45.948918: step 3972, loss 0.034651, acc 1
2016-09-07T05:38:46.619817: step 3973, loss 0.090612, acc 0.98
2016-09-07T05:38:47.313524: step 3974, loss 0.00239731, acc 1
2016-09-07T05:38:47.995861: step 3975, loss 0.00356464, acc 1
2016-09-07T05:38:48.692678: step 3976, loss 0.0301507, acc 0.98
2016-09-07T05:38:49.368877: step 3977, loss 0.00511257, acc 1
2016-09-07T05:38:50.035850: step 3978, loss 8.25073e-05, acc 1
2016-09-07T05:38:50.734075: step 3979, loss 0.000842091, acc 1
2016-09-07T05:38:51.412519: step 3980, loss 0.0375531, acc 0.98
2016-09-07T05:38:52.083418: step 3981, loss 0.0323892, acc 1
2016-09-07T05:38:52.779592: step 3982, loss 0.119953, acc 0.96
2016-09-07T05:38:53.455762: step 3983, loss 0.0146097, acc 1
2016-09-07T05:38:54.141148: step 3984, loss 0.0285014, acc 0.98
2016-09-07T05:38:54.844571: step 3985, loss 0.0239255, acc 0.98
2016-09-07T05:38:55.558289: step 3986, loss 0.000131123, acc 1
2016-09-07T05:38:56.216634: step 3987, loss 0.0518592, acc 0.98
2016-09-07T05:38:56.927322: step 3988, loss 0.00388173, acc 1
2016-09-07T05:38:57.608518: step 3989, loss 0.264965, acc 0.94
2016-09-07T05:38:58.291741: step 3990, loss 0.00428634, acc 1
2016-09-07T05:38:58.967993: step 3991, loss 0.0449289, acc 0.98
2016-09-07T05:38:59.652515: step 3992, loss 0.0744578, acc 0.98
2016-09-07T05:39:00.386475: step 3993, loss 0.0368094, acc 1
2016-09-07T05:39:01.070454: step 3994, loss 0.0567827, acc 0.98
2016-09-07T05:39:01.750473: step 3995, loss 0.0397434, acc 0.96
2016-09-07T05:39:02.426552: step 3996, loss 0.0339449, acc 0.98
2016-09-07T05:39:03.110656: step 3997, loss 0.00283483, acc 1
2016-09-07T05:39:03.783828: step 3998, loss 0.0241034, acc 1
2016-09-07T05:39:04.457630: step 3999, loss 0.0030843, acc 1
2016-09-07T05:39:05.193907: step 4000, loss 0.0145261, acc 1

Evaluation:
2016-09-07T05:39:08.373971: step 4000, loss 1.75197, acc 0.743902

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-4000

2016-09-07T05:39:10.132051: step 4001, loss 0.123253, acc 0.96
2016-09-07T05:39:10.824284: step 4002, loss 0.00819666, acc 1
2016-09-07T05:39:11.497346: step 4003, loss 0.0145123, acc 1
2016-09-07T05:39:12.188582: step 4004, loss 0.0597366, acc 0.96
2016-09-07T05:39:12.887270: step 4005, loss 0.0306403, acc 0.98
2016-09-07T05:39:13.598106: step 4006, loss 0.0414906, acc 0.98
2016-09-07T05:39:14.262985: step 4007, loss 0.0207694, acc 0.98
2016-09-07T05:39:14.933273: step 4008, loss 0.0103518, acc 1
2016-09-07T05:39:15.605453: step 4009, loss 0.00848455, acc 1
2016-09-07T05:39:16.274606: step 4010, loss 0.0648575, acc 0.96
2016-09-07T05:39:16.954228: step 4011, loss 0.0110539, acc 1
2016-09-07T05:39:17.642343: step 4012, loss 0.0878029, acc 0.98
2016-09-07T05:39:18.333632: step 4013, loss 0.0279365, acc 0.98
2016-09-07T05:39:18.993114: step 4014, loss 0.0248045, acc 0.98
2016-09-07T05:39:19.699999: step 4015, loss 0.0805852, acc 0.98
2016-09-07T05:39:20.382534: step 4016, loss 0.0398984, acc 1
2016-09-07T05:39:21.056797: step 4017, loss 0.0322206, acc 0.98
2016-09-07T05:39:21.729614: step 4018, loss 0.0271742, acc 0.98
2016-09-07T05:39:22.420685: step 4019, loss 0.0106081, acc 1
2016-09-07T05:39:23.130733: step 4020, loss 0.00392057, acc 1
2016-09-07T05:39:23.797702: step 4021, loss 0.0591267, acc 0.94
2016-09-07T05:39:24.497553: step 4022, loss 0.0504311, acc 0.96
2016-09-07T05:39:25.195252: step 4023, loss 0.0270777, acc 0.98
2016-09-07T05:39:25.889312: step 4024, loss 0.0223988, acc 1
2016-09-07T05:39:26.581638: step 4025, loss 0.00472941, acc 1
2016-09-07T05:39:27.271262: step 4026, loss 0.140715, acc 0.96
2016-09-07T05:39:27.984405: step 4027, loss 0.0227178, acc 1
2016-09-07T05:39:28.659559: step 4028, loss 0.00010566, acc 1
2016-09-07T05:39:29.338824: step 4029, loss 0.0135182, acc 1
2016-09-07T05:39:30.020005: step 4030, loss 0.0502314, acc 0.94
2016-09-07T05:39:30.709654: step 4031, loss 0.0725485, acc 0.96
2016-09-07T05:39:31.329910: step 4032, loss 0.0490535, acc 0.977273
2016-09-07T05:39:32.009961: step 4033, loss 0.0278849, acc 0.98
2016-09-07T05:39:32.714396: step 4034, loss 0.0438941, acc 0.98
2016-09-07T05:39:33.380905: step 4035, loss 0.0439824, acc 1
2016-09-07T05:39:34.095616: step 4036, loss 0.240457, acc 0.94
2016-09-07T05:39:34.783836: step 4037, loss 0.00898631, acc 1
2016-09-07T05:39:35.470021: step 4038, loss 0.0406995, acc 0.98
2016-09-07T05:39:36.161145: step 4039, loss 0.0104808, acc 1
2016-09-07T05:39:36.844154: step 4040, loss 0.00742263, acc 1
2016-09-07T05:39:37.534414: step 4041, loss 0.0426612, acc 0.98
2016-09-07T05:39:38.201508: step 4042, loss 0.00384949, acc 1
2016-09-07T05:39:38.922392: step 4043, loss 0.0149555, acc 1
2016-09-07T05:39:39.621249: step 4044, loss 0.0297802, acc 0.98
2016-09-07T05:39:40.302772: step 4045, loss 0.00550201, acc 1
2016-09-07T05:39:40.986822: step 4046, loss 0.0288357, acc 0.98
2016-09-07T05:39:41.664842: step 4047, loss 0.0166901, acc 1
2016-09-07T05:39:42.378495: step 4048, loss 0.000760745, acc 1
2016-09-07T05:39:43.078395: step 4049, loss 0.0408645, acc 0.98
2016-09-07T05:39:43.772321: step 4050, loss 0.0411625, acc 0.98
2016-09-07T05:39:44.461260: step 4051, loss 0.0273885, acc 1
2016-09-07T05:39:45.133132: step 4052, loss 0.00510892, acc 1
2016-09-07T05:39:45.826911: step 4053, loss 0.00977479, acc 1
2016-09-07T05:39:46.510990: step 4054, loss 0.0277162, acc 1
2016-09-07T05:39:47.210980: step 4055, loss 0.00134657, acc 1
2016-09-07T05:39:47.868767: step 4056, loss 0.0473172, acc 0.96
2016-09-07T05:39:48.554421: step 4057, loss 0.0464666, acc 0.98
2016-09-07T05:39:49.246904: step 4058, loss 0.00174215, acc 1
2016-09-07T05:39:49.929817: step 4059, loss 0.00147382, acc 1
2016-09-07T05:39:50.627147: step 4060, loss 0.00167117, acc 1
2016-09-07T05:39:51.322109: step 4061, loss 0.0257864, acc 1
2016-09-07T05:39:52.028579: step 4062, loss 0.0356966, acc 0.98
2016-09-07T05:39:52.731321: step 4063, loss 0.0489678, acc 0.98
2016-09-07T05:39:53.412313: step 4064, loss 0.0753182, acc 0.96
2016-09-07T05:39:54.113453: step 4065, loss 0.02146, acc 0.98
2016-09-07T05:39:54.792366: step 4066, loss 0.00734391, acc 1
2016-09-07T05:39:55.470398: step 4067, loss 0.0498489, acc 0.98
2016-09-07T05:39:56.147600: step 4068, loss 0.0194157, acc 1
2016-09-07T05:39:56.854596: step 4069, loss 0.00136934, acc 1
2016-09-07T05:39:57.526498: step 4070, loss 0.0108126, acc 1
2016-09-07T05:39:58.233758: step 4071, loss 0.00256509, acc 1
2016-09-07T05:39:58.928889: step 4072, loss 0.00377283, acc 1
2016-09-07T05:39:59.606798: step 4073, loss 0.00736804, acc 1
2016-09-07T05:40:00.320417: step 4074, loss 0.000577604, acc 1
2016-09-07T05:40:00.965159: step 4075, loss 0.0127191, acc 1
2016-09-07T05:40:01.663645: step 4076, loss 0.0175133, acc 1
2016-09-07T05:40:02.332309: step 4077, loss 0.0261832, acc 0.98
2016-09-07T05:40:03.002538: step 4078, loss 0.0147085, acc 1
2016-09-07T05:40:03.681334: step 4079, loss 0.0277423, acc 0.98
2016-09-07T05:40:04.374846: step 4080, loss 0.00281707, acc 1
2016-09-07T05:40:05.058460: step 4081, loss 0.046059, acc 0.98
2016-09-07T05:40:05.735378: step 4082, loss 0.0209177, acc 1
2016-09-07T05:40:06.495540: step 4083, loss 0.0879406, acc 0.98
2016-09-07T05:40:07.209907: step 4084, loss 0.00520091, acc 1
2016-09-07T05:40:07.889387: step 4085, loss 0.0209598, acc 0.98
2016-09-07T05:40:08.585259: step 4086, loss 0.100202, acc 0.98
2016-09-07T05:40:09.264355: step 4087, loss 0.0460788, acc 0.98
2016-09-07T05:40:09.964248: step 4088, loss 0.0695883, acc 0.98
2016-09-07T05:40:10.635719: step 4089, loss 0.0056416, acc 1
2016-09-07T05:40:11.347840: step 4090, loss 0.00248475, acc 1
2016-09-07T05:40:12.038020: step 4091, loss 0.018081, acc 0.98
2016-09-07T05:40:12.740003: step 4092, loss 0.0105265, acc 1
2016-09-07T05:40:13.430414: step 4093, loss 0.0371545, acc 0.98
2016-09-07T05:40:14.109487: step 4094, loss 0.0227572, acc 0.98
2016-09-07T05:40:14.808401: step 4095, loss 0.135177, acc 0.9
2016-09-07T05:40:15.465787: step 4096, loss 0.0909969, acc 0.94
2016-09-07T05:40:16.183852: step 4097, loss 0.0474466, acc 0.96
2016-09-07T05:40:16.889909: step 4098, loss 0.05508, acc 0.98
2016-09-07T05:40:17.597746: step 4099, loss 0.0082665, acc 1
2016-09-07T05:40:18.314087: step 4100, loss 0.0422086, acc 0.98

Evaluation:
2016-09-07T05:40:21.479708: step 4100, loss 1.61624, acc 0.754221

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-4100

2016-09-07T05:40:23.186440: step 4101, loss 0.0307916, acc 0.98
2016-09-07T05:40:23.850555: step 4102, loss 0.0529164, acc 0.98
2016-09-07T05:40:24.553378: step 4103, loss 0.0457622, acc 0.96
2016-09-07T05:40:25.226096: step 4104, loss 0.0446284, acc 0.96
2016-09-07T05:40:25.909437: step 4105, loss 0.00567891, acc 1
2016-09-07T05:40:26.597615: step 4106, loss 0.0164079, acc 0.98
2016-09-07T05:40:27.303270: step 4107, loss 0.0563764, acc 0.98
2016-09-07T05:40:27.986829: step 4108, loss 0.0486768, acc 0.96
2016-09-07T05:40:28.648349: step 4109, loss 0.0519617, acc 0.96
2016-09-07T05:40:29.363686: step 4110, loss 0.00222891, acc 1
2016-09-07T05:40:30.047652: step 4111, loss 0.0209964, acc 1
2016-09-07T05:40:30.740470: step 4112, loss 0.0321564, acc 0.98
2016-09-07T05:40:31.427045: step 4113, loss 0.0366441, acc 0.98
2016-09-07T05:40:32.113950: step 4114, loss 0.0185033, acc 1
2016-09-07T05:40:32.811105: step 4115, loss 0.068786, acc 0.98
2016-09-07T05:40:33.471386: step 4116, loss 0.00319407, acc 1
2016-09-07T05:40:34.156669: step 4117, loss 0.0325488, acc 0.98
2016-09-07T05:40:34.831161: step 4118, loss 0.0172826, acc 0.98
2016-09-07T05:40:35.506874: step 4119, loss 0.0195759, acc 0.98
2016-09-07T05:40:36.191250: step 4120, loss 0.0190287, acc 1
2016-09-07T05:40:36.863493: step 4121, loss 0.03859, acc 0.96
2016-09-07T05:40:37.554749: step 4122, loss 0.0397216, acc 0.98
2016-09-07T05:40:38.235945: step 4123, loss 0.0486258, acc 0.96
2016-09-07T05:40:38.939105: step 4124, loss 0.0347539, acc 0.98
2016-09-07T05:40:39.630097: step 4125, loss 0.00335344, acc 1
2016-09-07T05:40:40.373111: step 4126, loss 0.00849454, acc 1
2016-09-07T05:40:41.088640: step 4127, loss 0.0408758, acc 0.98
2016-09-07T05:40:41.793209: step 4128, loss 0.0260608, acc 0.98
2016-09-07T05:40:42.501432: step 4129, loss 0.0119387, acc 1
2016-09-07T05:40:43.182451: step 4130, loss 0.0531079, acc 0.98
2016-09-07T05:40:43.891380: step 4131, loss 0.0184723, acc 1
2016-09-07T05:40:44.574801: step 4132, loss 0.0314761, acc 0.98
2016-09-07T05:40:45.262378: step 4133, loss 0.0295468, acc 0.98
2016-09-07T05:40:45.945003: step 4134, loss 0.0541701, acc 0.98
2016-09-07T05:40:46.635360: step 4135, loss 0.0147176, acc 1
2016-09-07T05:40:47.344412: step 4136, loss 0.015757, acc 0.98
2016-09-07T05:40:47.998541: step 4137, loss 0.0237175, acc 0.98
2016-09-07T05:40:48.693858: step 4138, loss 0.00943609, acc 1
2016-09-07T05:40:49.365840: step 4139, loss 0.0511613, acc 0.98
2016-09-07T05:40:50.047021: step 4140, loss 0.00760256, acc 1
2016-09-07T05:40:50.742532: step 4141, loss 0.0749261, acc 0.98
2016-09-07T05:40:51.437336: step 4142, loss 0.00129175, acc 1
2016-09-07T05:40:52.122010: step 4143, loss 0.0334955, acc 1
2016-09-07T05:40:52.805004: step 4144, loss 3.73396e-05, acc 1
2016-09-07T05:40:53.484332: step 4145, loss 0.018715, acc 1
2016-09-07T05:40:54.164881: step 4146, loss 0.0226426, acc 0.98
2016-09-07T05:40:54.866063: step 4147, loss 0.00312366, acc 1
2016-09-07T05:40:55.550841: step 4148, loss 0.0153768, acc 0.98
2016-09-07T05:40:56.231533: step 4149, loss 0.0402722, acc 0.98
2016-09-07T05:40:56.947474: step 4150, loss 0.0214109, acc 0.98
2016-09-07T05:40:57.639028: step 4151, loss 0.0814776, acc 0.96
2016-09-07T05:40:58.321150: step 4152, loss 0.0528784, acc 0.96
2016-09-07T05:40:59.024653: step 4153, loss 0.0362544, acc 0.98
2016-09-07T05:40:59.719176: step 4154, loss 0.0609678, acc 0.96
2016-09-07T05:41:00.459715: step 4155, loss 0.147607, acc 0.98
2016-09-07T05:41:01.124725: step 4156, loss 0.000594366, acc 1
2016-09-07T05:41:01.811174: step 4157, loss 0.140335, acc 0.98
2016-09-07T05:41:02.495826: step 4158, loss 0.0142819, acc 1
2016-09-07T05:41:03.181855: step 4159, loss 0.00516828, acc 1
2016-09-07T05:41:03.880229: step 4160, loss 0.0093744, acc 1
2016-09-07T05:41:04.560425: step 4161, loss 0.000413648, acc 1
2016-09-07T05:41:05.232441: step 4162, loss 0.00898329, acc 1
2016-09-07T05:41:05.906247: step 4163, loss 0.0212782, acc 0.98
2016-09-07T05:41:06.612545: step 4164, loss 0.0276104, acc 0.98
2016-09-07T05:41:07.287400: step 4165, loss 0.00973218, acc 1
2016-09-07T05:41:07.983330: step 4166, loss 0.0098937, acc 1
2016-09-07T05:41:08.658933: step 4167, loss 0.0298248, acc 0.98
2016-09-07T05:41:09.348574: step 4168, loss 0.01381, acc 1
2016-09-07T05:41:10.040588: step 4169, loss 0.00152306, acc 1
2016-09-07T05:41:10.708325: step 4170, loss 0.00171635, acc 1
2016-09-07T05:41:11.405730: step 4171, loss 0.0246316, acc 0.98
2016-09-07T05:41:12.081513: step 4172, loss 0.0089577, acc 1
2016-09-07T05:41:12.781468: step 4173, loss 0.103106, acc 0.98
2016-09-07T05:41:13.458566: step 4174, loss 0.0695281, acc 0.96
2016-09-07T05:41:14.133003: step 4175, loss 0.0749831, acc 0.94
2016-09-07T05:41:14.811853: step 4176, loss 0.000487041, acc 1
2016-09-07T05:41:15.500779: step 4177, loss 0.0284543, acc 1
2016-09-07T05:41:16.197334: step 4178, loss 0.00692769, acc 1
2016-09-07T05:41:16.861455: step 4179, loss 0.0959648, acc 0.96
2016-09-07T05:41:17.562266: step 4180, loss 0.0333772, acc 0.98
2016-09-07T05:41:18.238813: step 4181, loss 0.114558, acc 0.96
2016-09-07T05:41:18.921542: step 4182, loss 0.0144903, acc 1
2016-09-07T05:41:19.606806: step 4183, loss 0.131218, acc 0.9
2016-09-07T05:41:20.289579: step 4184, loss 0.027824, acc 0.98
2016-09-07T05:41:21.001326: step 4185, loss 0.0155891, acc 1
2016-09-07T05:41:21.691601: step 4186, loss 0.0373231, acc 0.98
2016-09-07T05:41:22.366302: step 4187, loss 0.0558149, acc 0.96
2016-09-07T05:41:23.047151: step 4188, loss 0.0482486, acc 0.98
2016-09-07T05:41:23.742642: step 4189, loss 0.00798433, acc 1
2016-09-07T05:41:24.437662: step 4190, loss 0.0106307, acc 1
2016-09-07T05:41:25.131308: step 4191, loss 0.0251068, acc 0.98
2016-09-07T05:41:25.840602: step 4192, loss 0.0430368, acc 0.96
2016-09-07T05:41:26.503684: step 4193, loss 0.0321445, acc 1
2016-09-07T05:41:27.194441: step 4194, loss 0.0467165, acc 0.98
2016-09-07T05:41:27.892442: step 4195, loss 0.00937036, acc 1
2016-09-07T05:41:28.573827: step 4196, loss 0.0264549, acc 0.98
2016-09-07T05:41:29.253578: step 4197, loss 0.0178003, acc 0.98
2016-09-07T05:41:29.945570: step 4198, loss 0.0781356, acc 0.96
2016-09-07T05:41:30.662653: step 4199, loss 0.00627936, acc 1
2016-09-07T05:41:31.335991: step 4200, loss 0.0126993, acc 1

Evaluation:
2016-09-07T05:41:34.501235: step 4200, loss 1.88996, acc 0.738274

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-4200

2016-09-07T05:41:36.130188: step 4201, loss 0.0278548, acc 0.98
2016-09-07T05:41:36.804247: step 4202, loss 0.0789415, acc 0.96
2016-09-07T05:41:37.477464: step 4203, loss 0.0450169, acc 0.98
2016-09-07T05:41:38.173241: step 4204, loss 0.00977567, acc 1
2016-09-07T05:41:38.877447: step 4205, loss 0.0312717, acc 0.98
2016-09-07T05:41:39.547047: step 4206, loss 0.06121, acc 0.98
2016-09-07T05:41:40.246056: step 4207, loss 0.017429, acc 1
2016-09-07T05:41:40.919935: step 4208, loss 0.0763566, acc 0.98
2016-09-07T05:41:41.654058: step 4209, loss 0.00526028, acc 1
2016-09-07T05:41:42.341948: step 4210, loss 0.00701131, acc 1
2016-09-07T05:41:43.017389: step 4211, loss 0.0630841, acc 0.96
2016-09-07T05:41:43.687765: step 4212, loss 0.0568179, acc 0.94
2016-09-07T05:41:44.382853: step 4213, loss 0.0342664, acc 1
2016-09-07T05:41:45.081196: step 4214, loss 0.0612731, acc 0.96
2016-09-07T05:41:45.763428: step 4215, loss 0.0227899, acc 0.98
2016-09-07T05:41:46.428947: step 4216, loss 0.0065112, acc 1
2016-09-07T05:41:47.103594: step 4217, loss 0.0166981, acc 0.98
2016-09-07T05:41:47.795275: step 4218, loss 0.0617192, acc 0.98
2016-09-07T05:41:48.479506: step 4219, loss 0.0538602, acc 0.96
2016-09-07T05:41:49.166349: step 4220, loss 0.046058, acc 0.98
2016-09-07T05:41:49.881154: step 4221, loss 0.0152325, acc 1
2016-09-07T05:41:50.583990: step 4222, loss 0.0502933, acc 0.98
2016-09-07T05:41:51.279315: step 4223, loss 0.0268565, acc 1
2016-09-07T05:41:51.939150: step 4224, loss 0.000295677, acc 1
2016-09-07T05:41:52.638176: step 4225, loss 0.00859856, acc 1
2016-09-07T05:41:53.323987: step 4226, loss 0.0484959, acc 0.96
2016-09-07T05:41:53.999857: step 4227, loss 0.0262688, acc 0.98
2016-09-07T05:41:54.713882: step 4228, loss 0.00887025, acc 1
2016-09-07T05:41:55.369888: step 4229, loss 0.0505817, acc 1
2016-09-07T05:41:56.060742: step 4230, loss 0.0505069, acc 0.98
2016-09-07T05:41:56.733992: step 4231, loss 0.0144616, acc 0.98
2016-09-07T05:41:57.419988: step 4232, loss 0.0182926, acc 1
2016-09-07T05:41:58.126715: step 4233, loss 0.00163048, acc 1
2016-09-07T05:41:58.791610: step 4234, loss 0.00110631, acc 1
2016-09-07T05:41:59.508019: step 4235, loss 0.0170881, acc 0.98
2016-09-07T05:42:00.209853: step 4236, loss 0.0339346, acc 0.98
2016-09-07T05:42:00.926550: step 4237, loss 0.0316979, acc 0.98
2016-09-07T05:42:01.626717: step 4238, loss 0.0174735, acc 1
2016-09-07T05:42:02.313459: step 4239, loss 0.00151533, acc 1
2016-09-07T05:42:03.008262: step 4240, loss 0.0114902, acc 1
2016-09-07T05:42:03.683591: step 4241, loss 0.0475731, acc 0.98
2016-09-07T05:42:04.375944: step 4242, loss 0.00538704, acc 1
2016-09-07T05:42:05.074210: step 4243, loss 0.0144411, acc 1
2016-09-07T05:42:05.750658: step 4244, loss 0.000326477, acc 1
2016-09-07T05:42:06.433434: step 4245, loss 0.00157371, acc 1
2016-09-07T05:42:07.117014: step 4246, loss 0.113168, acc 0.96
2016-09-07T05:42:07.818225: step 4247, loss 0.0841292, acc 0.94
2016-09-07T05:42:08.467884: step 4248, loss 0.202581, acc 0.98
2016-09-07T05:42:09.166132: step 4249, loss 0.00653703, acc 1
2016-09-07T05:42:09.853941: step 4250, loss 0.00616092, acc 1
2016-09-07T05:42:10.547158: step 4251, loss 0.026374, acc 0.98
2016-09-07T05:42:11.229983: step 4252, loss 0.0243066, acc 1
2016-09-07T05:42:11.946031: step 4253, loss 0.0474985, acc 0.96
2016-09-07T05:42:12.642441: step 4254, loss 0.00745728, acc 1
2016-09-07T05:42:13.304942: step 4255, loss 0.127357, acc 0.94
2016-09-07T05:42:14.004961: step 4256, loss 0.121668, acc 0.92
2016-09-07T05:42:14.726502: step 4257, loss 0.0196125, acc 1
2016-09-07T05:42:15.436329: step 4258, loss 0.00809382, acc 1
2016-09-07T05:42:16.119886: step 4259, loss 0.00136823, acc 1
2016-09-07T05:42:16.810934: step 4260, loss 0.039656, acc 0.96
2016-09-07T05:42:17.515763: step 4261, loss 0.00558696, acc 1
2016-09-07T05:42:18.168916: step 4262, loss 0.014089, acc 1
2016-09-07T05:42:18.842107: step 4263, loss 0.00293769, acc 1
2016-09-07T05:42:19.516055: step 4264, loss 0.0134336, acc 1
2016-09-07T05:42:20.183269: step 4265, loss 0.0265316, acc 1
2016-09-07T05:42:20.872406: step 4266, loss 0.028088, acc 0.98
2016-09-07T05:42:21.549533: step 4267, loss 0.0613456, acc 0.96
2016-09-07T05:42:22.278062: step 4268, loss 0.0211507, acc 1
2016-09-07T05:42:22.951593: step 4269, loss 0.0132843, acc 1
2016-09-07T05:42:23.653272: step 4270, loss 0.0155326, acc 0.98
2016-09-07T05:42:24.349729: step 4271, loss 0.0185411, acc 0.98
2016-09-07T05:42:25.040489: step 4272, loss 0.133345, acc 0.94
2016-09-07T05:42:25.725940: step 4273, loss 0.00156051, acc 1
2016-09-07T05:42:26.399224: step 4274, loss 0.0237537, acc 1
2016-09-07T05:42:27.091701: step 4275, loss 0.0304616, acc 0.98
2016-09-07T05:42:27.756352: step 4276, loss 0.0348884, acc 0.98
2016-09-07T05:42:28.440418: step 4277, loss 0.00277562, acc 1
2016-09-07T05:42:29.130423: step 4278, loss 0.0536713, acc 0.98
2016-09-07T05:42:29.816635: step 4279, loss 0.0272954, acc 0.98
2016-09-07T05:42:30.523918: step 4280, loss 0.0357587, acc 0.98
2016-09-07T05:42:31.223854: step 4281, loss 0.0149786, acc 1
2016-09-07T05:42:31.931015: step 4282, loss 0.00033617, acc 1
2016-09-07T05:42:32.604311: step 4283, loss 0.00331544, acc 1
2016-09-07T05:42:33.279445: step 4284, loss 0.03264, acc 0.98
2016-09-07T05:42:33.968742: step 4285, loss 0.0797799, acc 0.94
2016-09-07T05:42:34.642802: step 4286, loss 0.00294567, acc 1
2016-09-07T05:42:35.342460: step 4287, loss 0.121021, acc 0.98
2016-09-07T05:42:36.036684: step 4288, loss 0.0470442, acc 0.98
2016-09-07T05:42:36.758524: step 4289, loss 0.161233, acc 0.94
2016-09-07T05:42:37.451852: step 4290, loss 0.124779, acc 0.96
2016-09-07T05:42:38.131143: step 4291, loss 0.0208363, acc 1
2016-09-07T05:42:38.832867: step 4292, loss 0.0306897, acc 1
2016-09-07T05:42:39.519016: step 4293, loss 0.000890786, acc 1
2016-09-07T05:42:40.228366: step 4294, loss 0.0197381, acc 0.98
2016-09-07T05:42:40.930704: step 4295, loss 0.0115922, acc 1
2016-09-07T05:42:41.640561: step 4296, loss 0.0648379, acc 0.98
2016-09-07T05:42:42.349333: step 4297, loss 0.00291209, acc 1
2016-09-07T05:42:43.040687: step 4298, loss 0.00910922, acc 1
2016-09-07T05:42:43.730274: step 4299, loss 0.0260167, acc 1
2016-09-07T05:42:44.404381: step 4300, loss 0.0452097, acc 0.98

Evaluation:
2016-09-07T05:42:47.557419: step 4300, loss 2.09692, acc 0.732645

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-4300

2016-09-07T05:42:49.263198: step 4301, loss 0.0127391, acc 1
2016-09-07T05:42:49.975862: step 4302, loss 0.0138976, acc 1
2016-09-07T05:42:50.654239: step 4303, loss 0.0107918, acc 1
2016-09-07T05:42:51.330482: step 4304, loss 0.0840674, acc 0.96
2016-09-07T05:42:52.008262: step 4305, loss 0.00941104, acc 1
2016-09-07T05:42:52.708812: step 4306, loss 0.00179376, acc 1
2016-09-07T05:42:53.393262: step 4307, loss 0.0224827, acc 0.98
2016-09-07T05:42:54.077131: step 4308, loss 0.021298, acc 0.98
2016-09-07T05:42:54.797591: step 4309, loss 0.0184413, acc 1
2016-09-07T05:42:55.476771: step 4310, loss 0.0300017, acc 0.98
2016-09-07T05:42:56.162396: step 4311, loss 0.00371568, acc 1
2016-09-07T05:42:56.859978: step 4312, loss 0.0900791, acc 0.98
2016-09-07T05:42:57.541939: step 4313, loss 0.014356, acc 1
2016-09-07T05:42:58.238305: step 4314, loss 0.0105807, acc 1
2016-09-07T05:42:58.945200: step 4315, loss 0.01413, acc 1
2016-09-07T05:42:59.678610: step 4316, loss 0.178771, acc 0.96
2016-09-07T05:43:00.377654: step 4317, loss 0.0108195, acc 1
2016-09-07T05:43:01.058570: step 4318, loss 0.0107426, acc 1
2016-09-07T05:43:01.744988: step 4319, loss 0.141849, acc 0.98
2016-09-07T05:43:02.428695: step 4320, loss 0.00233878, acc 1
2016-09-07T05:43:03.108203: step 4321, loss 0.0255821, acc 1
2016-09-07T05:43:03.770734: step 4322, loss 0.0476405, acc 0.96
2016-09-07T05:43:04.471301: step 4323, loss 0.108166, acc 0.94
2016-09-07T05:43:05.143472: step 4324, loss 0.011805, acc 1
2016-09-07T05:43:05.824709: step 4325, loss 0.00584565, acc 1
2016-09-07T05:43:06.529445: step 4326, loss 0.0142657, acc 1
2016-09-07T05:43:07.208958: step 4327, loss 0.00353815, acc 1
2016-09-07T05:43:07.911597: step 4328, loss 0.020379, acc 1
2016-09-07T05:43:08.608526: step 4329, loss 0.00373099, acc 1
2016-09-07T05:43:09.305618: step 4330, loss 0.0415283, acc 0.98
2016-09-07T05:43:09.958760: step 4331, loss 0.206004, acc 0.92
2016-09-07T05:43:10.630266: step 4332, loss 0.167759, acc 0.96
2016-09-07T05:43:11.341254: step 4333, loss 0.0296411, acc 0.98
2016-09-07T05:43:12.045071: step 4334, loss 0.0628762, acc 0.96
2016-09-07T05:43:12.747010: step 4335, loss 0.0400501, acc 0.96
2016-09-07T05:43:13.413329: step 4336, loss 0.0181196, acc 0.98
2016-09-07T05:43:14.133147: step 4337, loss 0.0296779, acc 0.98
2016-09-07T05:43:14.825749: step 4338, loss 0.00140424, acc 1
2016-09-07T05:43:15.496891: step 4339, loss 0.0182654, acc 1
2016-09-07T05:43:16.176673: step 4340, loss 0.0268947, acc 1
2016-09-07T05:43:16.865823: step 4341, loss 0.0638965, acc 0.96
2016-09-07T05:43:17.554337: step 4342, loss 0.0211646, acc 0.98
2016-09-07T05:43:18.223161: step 4343, loss 0.0409551, acc 0.98
2016-09-07T05:43:18.923404: step 4344, loss 0.0402187, acc 0.98
2016-09-07T05:43:19.620165: step 4345, loss 0.00715683, acc 1
2016-09-07T05:43:20.297910: step 4346, loss 0.0339663, acc 0.98
2016-09-07T05:43:20.988874: step 4347, loss 0.00819968, acc 1
2016-09-07T05:43:21.675127: step 4348, loss 0.00713388, acc 1
2016-09-07T05:43:22.365763: step 4349, loss 0.0265637, acc 0.98
2016-09-07T05:43:23.043012: step 4350, loss 0.0170326, acc 1
2016-09-07T05:43:23.761121: step 4351, loss 0.0458151, acc 1
2016-09-07T05:43:24.450549: step 4352, loss 0.0432115, acc 0.98
2016-09-07T05:43:25.140383: step 4353, loss 0.0502075, acc 0.98
2016-09-07T05:43:25.828364: step 4354, loss 0.0218074, acc 0.98
2016-09-07T05:43:26.509668: step 4355, loss 0.0400564, acc 0.98
2016-09-07T05:43:27.208534: step 4356, loss 0.0356087, acc 0.98
2016-09-07T05:43:27.872440: step 4357, loss 0.0194954, acc 0.98
2016-09-07T05:43:28.587641: step 4358, loss 0.00539572, acc 1
2016-09-07T05:43:29.284042: step 4359, loss 0.009372, acc 1
2016-09-07T05:43:30.022864: step 4360, loss 0.0749114, acc 0.94
2016-09-07T05:43:30.720729: step 4361, loss 0.169332, acc 0.96
2016-09-07T05:43:31.394956: step 4362, loss 0.00028088, acc 1
2016-09-07T05:43:32.107850: step 4363, loss 0.0305388, acc 0.98
2016-09-07T05:43:32.780538: step 4364, loss 0.0140671, acc 1
2016-09-07T05:43:33.453664: step 4365, loss 0.00998195, acc 1
2016-09-07T05:43:34.140010: step 4366, loss 0.0429079, acc 0.98
2016-09-07T05:43:34.875114: step 4367, loss 0.0206751, acc 1
2016-09-07T05:43:35.567898: step 4368, loss 0.0272035, acc 0.98
2016-09-07T05:43:36.237662: step 4369, loss 0.0248253, acc 0.98
2016-09-07T05:43:36.953604: step 4370, loss 0.0248854, acc 0.98
2016-09-07T05:43:37.623155: step 4371, loss 0.00373827, acc 1
2016-09-07T05:43:38.305076: step 4372, loss 0.0250903, acc 0.98
2016-09-07T05:43:39.006098: step 4373, loss 0.00204019, acc 1
2016-09-07T05:43:39.668529: step 4374, loss 0.0460997, acc 0.98
2016-09-07T05:43:40.387171: step 4375, loss 0.0157476, acc 1
2016-09-07T05:43:41.078672: step 4376, loss 0.0405599, acc 0.96
2016-09-07T05:43:41.788836: step 4377, loss 0.0136438, acc 1
2016-09-07T05:43:42.482354: step 4378, loss 0.0257836, acc 0.98
2016-09-07T05:43:43.182079: step 4379, loss 0.0342382, acc 0.98
2016-09-07T05:43:43.877011: step 4380, loss 0.0472531, acc 0.98
2016-09-07T05:43:44.585958: step 4381, loss 0.00440966, acc 1
2016-09-07T05:43:45.285376: step 4382, loss 0.0191109, acc 0.98
2016-09-07T05:43:45.949540: step 4383, loss 7.66083e-05, acc 1
2016-09-07T05:43:46.658986: step 4384, loss 0.0139785, acc 1
2016-09-07T05:43:47.349704: step 4385, loss 0.0242724, acc 0.98
2016-09-07T05:43:48.043843: step 4386, loss 0.0115977, acc 1
2016-09-07T05:43:48.774869: step 4387, loss 0.0360916, acc 0.98
2016-09-07T05:43:49.469226: step 4388, loss 0.0216322, acc 0.98
2016-09-07T05:43:50.155351: step 4389, loss 0.00326541, acc 1
2016-09-07T05:43:50.825547: step 4390, loss 0.0173147, acc 0.98
2016-09-07T05:43:51.548408: step 4391, loss 0.0255056, acc 0.98
2016-09-07T05:43:52.253064: step 4392, loss 0.0895605, acc 0.96
2016-09-07T05:43:52.944083: step 4393, loss 0.0191951, acc 0.98
2016-09-07T05:43:53.634497: step 4394, loss 0.04507, acc 0.98
2016-09-07T05:43:54.318921: step 4395, loss 0.00746159, acc 1
2016-09-07T05:43:55.031911: step 4396, loss 0.0282061, acc 0.98
2016-09-07T05:43:55.722428: step 4397, loss 0.17635, acc 0.92
2016-09-07T05:43:56.399450: step 4398, loss 0.0160396, acc 1
2016-09-07T05:43:57.083182: step 4399, loss 0.0110839, acc 1
2016-09-07T05:43:57.767402: step 4400, loss 0.00754623, acc 1

Evaluation:
2016-09-07T05:44:00.964310: step 4400, loss 2.0483, acc 0.737336

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-4400

2016-09-07T05:44:02.624409: step 4401, loss 0.00131428, acc 1
2016-09-07T05:44:03.342987: step 4402, loss 0.0182707, acc 1
2016-09-07T05:44:04.021407: step 4403, loss 0.0170974, acc 1
2016-09-07T05:44:04.729774: step 4404, loss 0.0298036, acc 0.98
2016-09-07T05:44:05.421335: step 4405, loss 0.0262158, acc 0.98
2016-09-07T05:44:06.100604: step 4406, loss 0.0206191, acc 0.98
2016-09-07T05:44:06.791248: step 4407, loss 0.0302941, acc 0.98
2016-09-07T05:44:07.463052: step 4408, loss 0.00753444, acc 1
2016-09-07T05:44:08.165620: step 4409, loss 0.00224449, acc 1
2016-09-07T05:44:08.864151: step 4410, loss 0.0337375, acc 0.98
2016-09-07T05:44:09.566251: step 4411, loss 0.0328597, acc 1
2016-09-07T05:44:10.258954: step 4412, loss 0.0187105, acc 0.98
2016-09-07T05:44:10.947362: step 4413, loss 0.0180888, acc 1
2016-09-07T05:44:11.624249: step 4414, loss 0.0490797, acc 0.98
2016-09-07T05:44:12.299713: step 4415, loss 0.0381822, acc 0.98
2016-09-07T05:44:12.944978: step 4416, loss 0.00401764, acc 1
2016-09-07T05:44:13.638276: step 4417, loss 0.048683, acc 0.96
2016-09-07T05:44:14.341680: step 4418, loss 0.0235822, acc 0.98
2016-09-07T05:44:15.027142: step 4419, loss 0.037049, acc 0.98
2016-09-07T05:44:15.713281: step 4420, loss 0.0225336, acc 0.98
2016-09-07T05:44:16.406418: step 4421, loss 0.0388387, acc 0.98
2016-09-07T05:44:17.086707: step 4422, loss 0.0522064, acc 0.98
2016-09-07T05:44:17.787733: step 4423, loss 0.0314747, acc 0.98
2016-09-07T05:44:18.485920: step 4424, loss 0.0164793, acc 0.98
2016-09-07T05:44:19.188003: step 4425, loss 0.0355086, acc 0.98
2016-09-07T05:44:19.885113: step 4426, loss 0.0557608, acc 0.98
2016-09-07T05:44:20.564235: step 4427, loss 0.0134158, acc 1
2016-09-07T05:44:21.268019: step 4428, loss 0.0277013, acc 0.98
2016-09-07T05:44:21.938841: step 4429, loss 0.0408343, acc 0.98
2016-09-07T05:44:22.667891: step 4430, loss 0.0114115, acc 1
2016-09-07T05:44:23.370580: step 4431, loss 0.0304402, acc 0.98
2016-09-07T05:44:24.057356: step 4432, loss 0.0044336, acc 1
2016-09-07T05:44:24.745810: step 4433, loss 0.0825684, acc 0.96
2016-09-07T05:44:25.425506: step 4434, loss 0.00936968, acc 1
2016-09-07T05:44:26.119755: step 4435, loss 0.01648, acc 0.98
2016-09-07T05:44:26.799597: step 4436, loss 0.018416, acc 0.98
2016-09-07T05:44:27.518288: step 4437, loss 0.0297069, acc 1
2016-09-07T05:44:28.230735: step 4438, loss 0.0689096, acc 0.96
2016-09-07T05:44:28.909952: step 4439, loss 0.0466395, acc 0.98
2016-09-07T05:44:29.604485: step 4440, loss 0.0481074, acc 0.96
2016-09-07T05:44:30.289872: step 4441, loss 0.0591991, acc 0.94
2016-09-07T05:44:30.998143: step 4442, loss 0.00674494, acc 1
2016-09-07T05:44:31.666493: step 4443, loss 0.00885034, acc 1
2016-09-07T05:44:32.377931: step 4444, loss 0.0586087, acc 0.96
2016-09-07T05:44:33.059059: step 4445, loss 0.12103, acc 0.98
2016-09-07T05:44:33.744988: step 4446, loss 0.04685, acc 0.96
2016-09-07T05:44:34.434119: step 4447, loss 0.00235941, acc 1
2016-09-07T05:44:35.132773: step 4448, loss 0.0412809, acc 0.98
2016-09-07T05:44:35.848023: step 4449, loss 0.0347273, acc 0.98
2016-09-07T05:44:36.515294: step 4450, loss 0.0304316, acc 0.98
2016-09-07T05:44:37.193795: step 4451, loss 0.0207044, acc 1
2016-09-07T05:44:37.875107: step 4452, loss 0.0236389, acc 1
2016-09-07T05:44:38.567227: step 4453, loss 0.00707024, acc 1
2016-09-07T05:44:39.257123: step 4454, loss 0.00349694, acc 1
2016-09-07T05:44:39.948619: step 4455, loss 0.00483565, acc 1
2016-09-07T05:44:40.662625: step 4456, loss 0.000195761, acc 1
2016-09-07T05:44:41.365271: step 4457, loss 0.0128611, acc 1
2016-09-07T05:44:42.040769: step 4458, loss 0.065637, acc 0.98
2016-09-07T05:44:42.714372: step 4459, loss 0.0141464, acc 1
2016-09-07T05:44:43.407892: step 4460, loss 0.0662047, acc 0.96
2016-09-07T05:44:44.084743: step 4461, loss 0.0329367, acc 0.98
2016-09-07T05:44:44.765053: step 4462, loss 0.00211989, acc 1
2016-09-07T05:44:45.478968: step 4463, loss 0.0480544, acc 0.98
2016-09-07T05:44:46.153742: step 4464, loss 0.0165632, acc 0.98
2016-09-07T05:44:46.835754: step 4465, loss 0.00506129, acc 1
2016-09-07T05:44:47.528454: step 4466, loss 0.0445359, acc 0.98
2016-09-07T05:44:48.201623: step 4467, loss 0.0205569, acc 1
2016-09-07T05:44:48.890467: step 4468, loss 0.00770465, acc 1
2016-09-07T05:44:49.566356: step 4469, loss 0.0133503, acc 1
2016-09-07T05:44:50.274369: step 4470, loss 0.0588404, acc 0.96
2016-09-07T05:44:50.945431: step 4471, loss 0.00479918, acc 1
2016-09-07T05:44:51.625999: step 4472, loss 0.0423346, acc 0.98
2016-09-07T05:44:52.303024: step 4473, loss 0.0139207, acc 1
2016-09-07T05:44:52.980364: step 4474, loss 0.00525702, acc 1
2016-09-07T05:44:53.686816: step 4475, loss 0.158562, acc 0.94
2016-09-07T05:44:54.371652: step 4476, loss 0.00363292, acc 1
2016-09-07T05:44:55.074876: step 4477, loss 0.0592385, acc 0.98
2016-09-07T05:44:55.758284: step 4478, loss 0.0207165, acc 0.98
2016-09-07T05:44:56.433364: step 4479, loss 0.0479191, acc 0.98
2016-09-07T05:44:57.110761: step 4480, loss 0.0982096, acc 0.94
2016-09-07T05:44:57.797590: step 4481, loss 0.0118675, acc 1
2016-09-07T05:44:58.520232: step 4482, loss 0.0862479, acc 0.96
2016-09-07T05:44:59.199533: step 4483, loss 0.000318863, acc 1
2016-09-07T05:44:59.905637: step 4484, loss 0.0793259, acc 0.98
2016-09-07T05:45:00.653720: step 4485, loss 0.0181433, acc 0.98
2016-09-07T05:45:01.344042: step 4486, loss 0.0753787, acc 0.96
2016-09-07T05:45:02.032956: step 4487, loss 0.000749219, acc 1
2016-09-07T05:45:02.728929: step 4488, loss 0.0842546, acc 0.96
2016-09-07T05:45:03.410287: step 4489, loss 0.000997241, acc 1
2016-09-07T05:45:04.080705: step 4490, loss 0.0140001, acc 1
2016-09-07T05:45:04.799375: step 4491, loss 0.00193505, acc 1
2016-09-07T05:45:05.480190: step 4492, loss 0.0806036, acc 0.96
2016-09-07T05:45:06.160712: step 4493, loss 0.00272006, acc 1
2016-09-07T05:45:06.845529: step 4494, loss 0.000183814, acc 1
2016-09-07T05:45:07.544528: step 4495, loss 0.0131001, acc 1
2016-09-07T05:45:08.258972: step 4496, loss 0.0315545, acc 0.98
2016-09-07T05:45:08.955309: step 4497, loss 0.0103143, acc 1
2016-09-07T05:45:09.634406: step 4498, loss 0.00468773, acc 1
2016-09-07T05:45:10.308525: step 4499, loss 0.00761456, acc 1
2016-09-07T05:45:10.996595: step 4500, loss 0.00288157, acc 1

Evaluation:
2016-09-07T05:45:14.189472: step 4500, loss 2.14862, acc 0.743902

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-4500

2016-09-07T05:45:15.854731: step 4501, loss 0.00491115, acc 1
2016-09-07T05:45:16.569097: step 4502, loss 0.0398467, acc 0.98
2016-09-07T05:45:17.232909: step 4503, loss 0.00146339, acc 1
2016-09-07T05:45:17.936622: step 4504, loss 0.0376517, acc 0.98
2016-09-07T05:45:18.615209: step 4505, loss 0.00195595, acc 1
2016-09-07T05:45:19.303461: step 4506, loss 0.0228116, acc 0.98
2016-09-07T05:45:20.005593: step 4507, loss 0.00317955, acc 1
2016-09-07T05:45:20.696736: step 4508, loss 0.0186015, acc 0.98
2016-09-07T05:45:21.385948: step 4509, loss 0.0426797, acc 0.98
2016-09-07T05:45:22.063597: step 4510, loss 0.0146028, acc 1
2016-09-07T05:45:22.777869: step 4511, loss 0.0422396, acc 0.96
2016-09-07T05:45:23.461465: step 4512, loss 0.00680893, acc 1
2016-09-07T05:45:24.155791: step 4513, loss 0.0132996, acc 1
2016-09-07T05:45:24.829248: step 4514, loss 0.0390984, acc 0.98
2016-09-07T05:45:25.527372: step 4515, loss 0.00145939, acc 1
2016-09-07T05:45:26.213550: step 4516, loss 0.0454137, acc 0.98
2016-09-07T05:45:26.889247: step 4517, loss 0.00213252, acc 1
2016-09-07T05:45:27.580013: step 4518, loss 0.0483303, acc 0.98
2016-09-07T05:45:28.258374: step 4519, loss 0.06524, acc 0.96
2016-09-07T05:45:28.949409: step 4520, loss 0.00122821, acc 1
2016-09-07T05:45:29.651073: step 4521, loss 7.58916e-05, acc 1
2016-09-07T05:45:30.345862: step 4522, loss 0.0287471, acc 0.98
2016-09-07T05:45:31.026500: step 4523, loss 0.0163368, acc 1
2016-09-07T05:45:31.695062: step 4524, loss 0.0538805, acc 0.96
2016-09-07T05:45:32.396643: step 4525, loss 0.00519933, acc 1
2016-09-07T05:45:33.071964: step 4526, loss 0.0802233, acc 0.98
2016-09-07T05:45:33.754559: step 4527, loss 0.0925315, acc 0.94
2016-09-07T05:45:34.452158: step 4528, loss 0.00137393, acc 1
2016-09-07T05:45:35.144105: step 4529, loss 0.0107928, acc 1
2016-09-07T05:45:35.840556: step 4530, loss 0.0109948, acc 1
2016-09-07T05:45:36.504786: step 4531, loss 0.00616304, acc 1
2016-09-07T05:45:37.190447: step 4532, loss 0.0702668, acc 0.96
2016-09-07T05:45:37.882081: step 4533, loss 0.0397027, acc 0.98
2016-09-07T05:45:38.570277: step 4534, loss 0.0298948, acc 0.98
2016-09-07T05:45:39.248007: step 4535, loss 0.00269381, acc 1
2016-09-07T05:45:39.935372: step 4536, loss 0.0472752, acc 0.98
2016-09-07T05:45:40.641551: step 4537, loss 0.0722636, acc 0.96
2016-09-07T05:45:41.347018: step 4538, loss 0.00488221, acc 1
2016-09-07T05:45:42.072431: step 4539, loss 0.00829657, acc 1
2016-09-07T05:45:42.772691: step 4540, loss 0.00196387, acc 1
2016-09-07T05:45:43.459038: step 4541, loss 0.0571254, acc 0.94
2016-09-07T05:45:44.132759: step 4542, loss 0.0204563, acc 0.98
2016-09-07T05:45:44.804547: step 4543, loss 0.0244063, acc 0.98
2016-09-07T05:45:45.492609: step 4544, loss 0.00757415, acc 1
2016-09-07T05:45:46.173757: step 4545, loss 0.0107491, acc 1
2016-09-07T05:45:46.876986: step 4546, loss 0.0476994, acc 0.96
2016-09-07T05:45:47.555272: step 4547, loss 0.0012373, acc 1
2016-09-07T05:45:48.240998: step 4548, loss 0.0147922, acc 1
2016-09-07T05:45:48.924837: step 4549, loss 0.0151174, acc 1
2016-09-07T05:45:49.594868: step 4550, loss 0.0214661, acc 0.98
2016-09-07T05:45:50.287744: step 4551, loss 0.0315452, acc 0.98
2016-09-07T05:45:50.953781: step 4552, loss 0.0161933, acc 1
2016-09-07T05:45:51.662187: step 4553, loss 0.00377887, acc 1
2016-09-07T05:45:52.353124: step 4554, loss 4.77469e-05, acc 1
2016-09-07T05:45:53.040510: step 4555, loss 0.00956526, acc 1
2016-09-07T05:45:53.728692: step 4556, loss 0.0194976, acc 1
2016-09-07T05:45:54.418836: step 4557, loss 0.0162372, acc 1
2016-09-07T05:45:55.124981: step 4558, loss 0.00850926, acc 1
2016-09-07T05:45:55.801287: step 4559, loss 0.0507916, acc 0.96
2016-09-07T05:45:56.506249: step 4560, loss 0.00853376, acc 1
2016-09-07T05:45:57.212515: step 4561, loss 0.00799601, acc 1
2016-09-07T05:45:57.888722: step 4562, loss 0.0419918, acc 0.96
2016-09-07T05:45:58.586602: step 4563, loss 0.0548387, acc 0.98
2016-09-07T05:45:59.272624: step 4564, loss 0.0134482, acc 1
2016-09-07T05:45:59.984036: step 4565, loss 0.0500966, acc 0.98
2016-09-07T05:46:00.703082: step 4566, loss 0.0324179, acc 0.98
2016-09-07T05:46:01.373316: step 4567, loss 0.0372574, acc 1
2016-09-07T05:46:02.065564: step 4568, loss 0.0515556, acc 0.96
2016-09-07T05:46:02.760888: step 4569, loss 0.0194396, acc 1
2016-09-07T05:46:03.458636: step 4570, loss 0.0053156, acc 1
2016-09-07T05:46:04.146272: step 4571, loss 0.0522891, acc 0.98
2016-09-07T05:46:04.841268: step 4572, loss 0.006674, acc 1
2016-09-07T05:46:05.507991: step 4573, loss 0.020755, acc 0.98
2016-09-07T05:46:06.180125: step 4574, loss 0.0568371, acc 0.96
2016-09-07T05:46:06.875418: step 4575, loss 0.00310402, acc 1
2016-09-07T05:46:07.556151: step 4576, loss 0.0132268, acc 1
2016-09-07T05:46:08.246676: step 4577, loss 0.00543613, acc 1
2016-09-07T05:46:08.946499: step 4578, loss 0.0652732, acc 0.94
2016-09-07T05:46:09.671349: step 4579, loss 0.0258735, acc 0.98
2016-09-07T05:46:10.338898: step 4580, loss 0.0236965, acc 0.98
2016-09-07T05:46:11.006336: step 4581, loss 0.000265346, acc 1
2016-09-07T05:46:11.678113: step 4582, loss 0.1281, acc 0.94
2016-09-07T05:46:12.363992: step 4583, loss 0.0104035, acc 1
2016-09-07T05:46:13.069055: step 4584, loss 0.023493, acc 0.98
2016-09-07T05:46:13.764944: step 4585, loss 0.0177199, acc 1
2016-09-07T05:46:14.465147: step 4586, loss 0.11268, acc 0.96
2016-09-07T05:46:15.133097: step 4587, loss 0.0270827, acc 0.98
2016-09-07T05:46:15.821806: step 4588, loss 0.157299, acc 0.96
2016-09-07T05:46:16.498955: step 4589, loss 0.0149214, acc 1
2016-09-07T05:46:17.176103: step 4590, loss 0.0153261, acc 1
2016-09-07T05:46:17.870552: step 4591, loss 0.013631, acc 1
2016-09-07T05:46:18.540767: step 4592, loss 0.0501108, acc 0.98
2016-09-07T05:46:19.230736: step 4593, loss 0.00209293, acc 1
2016-09-07T05:46:19.904885: step 4594, loss 0.0274053, acc 0.98
2016-09-07T05:46:20.603792: step 4595, loss 0.0118339, acc 1
2016-09-07T05:46:21.318123: step 4596, loss 0.0384779, acc 0.98
2016-09-07T05:46:22.000265: step 4597, loss 0.0353626, acc 0.98
2016-09-07T05:46:22.689813: step 4598, loss 0.00302569, acc 1
2016-09-07T05:46:23.360022: step 4599, loss 0.00554352, acc 1
2016-09-07T05:46:24.056009: step 4600, loss 0.0335042, acc 0.98

Evaluation:
2016-09-07T05:46:27.220651: step 4600, loss 2.2267, acc 0.733584

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-4600

2016-09-07T05:46:28.909507: step 4601, loss 0.0578755, acc 0.94
2016-09-07T05:46:29.589901: step 4602, loss 0.0630776, acc 0.96
2016-09-07T05:46:30.292374: step 4603, loss 0.0437788, acc 0.98
2016-09-07T05:46:30.962580: step 4604, loss 0.0231754, acc 0.98
2016-09-07T05:46:31.653640: step 4605, loss 0.00763209, acc 1
2016-09-07T05:46:32.348521: step 4606, loss 0.0202719, acc 0.98
2016-09-07T05:46:33.032591: step 4607, loss 0.093554, acc 0.98
2016-09-07T05:46:33.703161: step 4608, loss 0.0152929, acc 1
2016-09-07T05:46:34.377025: step 4609, loss 0.0319043, acc 0.98
2016-09-07T05:46:35.065101: step 4610, loss 0.0713689, acc 0.98
2016-09-07T05:46:35.755465: step 4611, loss 0.0268384, acc 1
2016-09-07T05:46:36.460671: step 4612, loss 0.125086, acc 0.96
2016-09-07T05:46:37.162380: step 4613, loss 0.0203207, acc 0.98
2016-09-07T05:46:37.861153: step 4614, loss 0.0324085, acc 0.98
2016-09-07T05:46:38.560382: step 4615, loss 0.0389014, acc 0.98
2016-09-07T05:46:39.269644: step 4616, loss 0.0585505, acc 0.98
2016-09-07T05:46:39.951876: step 4617, loss 0.0272784, acc 0.98
2016-09-07T05:46:40.629417: step 4618, loss 0.0336714, acc 0.98
2016-09-07T05:46:41.339015: step 4619, loss 0.00975179, acc 1
2016-09-07T05:46:42.034228: step 4620, loss 0.0645822, acc 0.96
2016-09-07T05:46:42.697842: step 4621, loss 0.0390752, acc 1
2016-09-07T05:46:43.382754: step 4622, loss 0.0272052, acc 1
2016-09-07T05:46:44.073524: step 4623, loss 0.00184597, acc 1
2016-09-07T05:46:44.763073: step 4624, loss 0.0230095, acc 1
2016-09-07T05:46:45.444472: step 4625, loss 0.0270388, acc 0.98
2016-09-07T05:46:46.135196: step 4626, loss 0.00597371, acc 1
2016-09-07T05:46:46.822428: step 4627, loss 0.0390937, acc 0.98
2016-09-07T05:46:47.484522: step 4628, loss 0.00142239, acc 1
2016-09-07T05:46:48.188972: step 4629, loss 0.104921, acc 0.94
2016-09-07T05:46:48.880829: step 4630, loss 0.00288796, acc 1
2016-09-07T05:46:49.569898: step 4631, loss 0.0158419, acc 1
2016-09-07T05:46:50.251838: step 4632, loss 0.0279308, acc 0.98
2016-09-07T05:46:50.938168: step 4633, loss 0.0437948, acc 0.96
2016-09-07T05:46:51.685858: step 4634, loss 0.0187878, acc 1
2016-09-07T05:46:52.361397: step 4635, loss 0.00813997, acc 1
2016-09-07T05:46:53.072691: step 4636, loss 0.00353321, acc 1
2016-09-07T05:46:53.775096: step 4637, loss 0.120086, acc 0.98
2016-09-07T05:46:54.453622: step 4638, loss 0.0375636, acc 0.98
2016-09-07T05:46:55.167168: step 4639, loss 0.00224424, acc 1
2016-09-07T05:46:55.854336: step 4640, loss 0.0951065, acc 0.96
2016-09-07T05:46:56.570719: step 4641, loss 0.0179588, acc 1
2016-09-07T05:46:57.267696: step 4642, loss 0.0252974, acc 0.98
2016-09-07T05:46:57.962094: step 4643, loss 0.0141496, acc 1
2016-09-07T05:46:58.634406: step 4644, loss 0.0723122, acc 0.98
2016-09-07T05:46:59.335900: step 4645, loss 0.00162311, acc 1
2016-09-07T05:47:00.052759: step 4646, loss 0.0198172, acc 0.98
2016-09-07T05:47:00.751282: step 4647, loss 0.00554548, acc 1
2016-09-07T05:47:01.457067: step 4648, loss 0.00201307, acc 1
2016-09-07T05:47:02.170011: step 4649, loss 0.041509, acc 0.96
2016-09-07T05:47:02.874459: step 4650, loss 0.0447834, acc 0.98
2016-09-07T05:47:03.560528: step 4651, loss 0.0145982, acc 1
2016-09-07T05:47:04.252096: step 4652, loss 0.00133655, acc 1
2016-09-07T05:47:04.958561: step 4653, loss 0.111781, acc 0.98
2016-09-07T05:47:05.623755: step 4654, loss 0.0337772, acc 0.98
2016-09-07T05:47:06.319864: step 4655, loss 0.0185582, acc 1
2016-09-07T05:47:06.998377: step 4656, loss 0.0422541, acc 0.96
2016-09-07T05:47:07.689699: step 4657, loss 0.0200719, acc 1
2016-09-07T05:47:08.368513: step 4658, loss 0.155594, acc 0.92
2016-09-07T05:47:09.064737: step 4659, loss 0.0559983, acc 0.96
2016-09-07T05:47:09.758880: step 4660, loss 0.0355739, acc 0.98
2016-09-07T05:47:10.454594: step 4661, loss 0.0169675, acc 1
2016-09-07T05:47:11.130532: step 4662, loss 0.00192357, acc 1
2016-09-07T05:47:11.813914: step 4663, loss 0.00881917, acc 1
2016-09-07T05:47:12.489992: step 4664, loss 0.0020143, acc 1
2016-09-07T05:47:13.170058: step 4665, loss 0.0823431, acc 0.96
2016-09-07T05:47:13.846415: step 4666, loss 0.0343469, acc 0.98
2016-09-07T05:47:14.545760: step 4667, loss 0.0212497, acc 1
2016-09-07T05:47:15.199734: step 4668, loss 0.0262767, acc 0.98
2016-09-07T05:47:15.873825: step 4669, loss 0.0199952, acc 1
2016-09-07T05:47:16.556297: step 4670, loss 0.0246257, acc 1
2016-09-07T05:47:17.230735: step 4671, loss 0.0515564, acc 0.98
2016-09-07T05:47:17.925520: step 4672, loss 0.0297144, acc 0.98
2016-09-07T05:47:18.600942: step 4673, loss 0.0725877, acc 0.94
2016-09-07T05:47:19.318287: step 4674, loss 0.0401931, acc 0.98
2016-09-07T05:47:19.980455: step 4675, loss 0.018923, acc 1
2016-09-07T05:47:20.688809: step 4676, loss 0.0358212, acc 0.98
2016-09-07T05:47:21.351633: step 4677, loss 0.0271908, acc 0.98
2016-09-07T05:47:22.043181: step 4678, loss 0.0124775, acc 1
2016-09-07T05:47:22.752111: step 4679, loss 0.0301635, acc 0.98
2016-09-07T05:47:23.444281: step 4680, loss 0.0211962, acc 1
2016-09-07T05:47:24.142786: step 4681, loss 0.00250857, acc 1
2016-09-07T05:47:24.827987: step 4682, loss 0.0630831, acc 0.96
2016-09-07T05:47:25.507626: step 4683, loss 0.00266965, acc 1
2016-09-07T05:47:26.201414: step 4684, loss 0.0463638, acc 0.96
2016-09-07T05:47:26.881161: step 4685, loss 0.0221976, acc 0.98
2016-09-07T05:47:27.580725: step 4686, loss 0.0111062, acc 1
2016-09-07T05:47:28.280612: step 4687, loss 0.00934876, acc 1
2016-09-07T05:47:28.962650: step 4688, loss 0.106478, acc 0.98
2016-09-07T05:47:29.651126: step 4689, loss 0.159885, acc 0.94
2016-09-07T05:47:30.351173: step 4690, loss 0.0192165, acc 1
2016-09-07T05:47:31.043007: step 4691, loss 0.0300708, acc 1
2016-09-07T05:47:31.739943: step 4692, loss 0.00112427, acc 1
2016-09-07T05:47:32.436505: step 4693, loss 0.0213832, acc 1
2016-09-07T05:47:33.119696: step 4694, loss 0.0708192, acc 0.98
2016-09-07T05:47:33.848029: step 4695, loss 0.0210874, acc 0.98
2016-09-07T05:47:34.512058: step 4696, loss 0.045539, acc 0.98
2016-09-07T05:47:35.202998: step 4697, loss 0.0468416, acc 0.98
2016-09-07T05:47:35.892394: step 4698, loss 0.0369511, acc 1
2016-09-07T05:47:36.644440: step 4699, loss 0.0229391, acc 1
2016-09-07T05:47:37.398058: step 4700, loss 0.00630053, acc 1

Evaluation:
2016-09-07T05:47:40.553370: step 4700, loss 1.59949, acc 0.749531

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-4700

2016-09-07T05:47:42.303813: step 4701, loss 0.0111938, acc 1
2016-09-07T05:47:43.016236: step 4702, loss 0.0638209, acc 0.96
2016-09-07T05:47:43.706958: step 4703, loss 0.0533059, acc 0.96
2016-09-07T05:47:44.400813: step 4704, loss 0.0413536, acc 0.98
2016-09-07T05:47:45.090920: step 4705, loss 0.000407185, acc 1
2016-09-07T05:47:45.793256: step 4706, loss 0.0340369, acc 0.98
2016-09-07T05:47:46.445508: step 4707, loss 0.0184003, acc 1
2016-09-07T05:47:47.177647: step 4708, loss 0.0360111, acc 1
2016-09-07T05:47:47.864571: step 4709, loss 0.0159801, acc 1
2016-09-07T05:47:48.562238: step 4710, loss 0.00362839, acc 1
2016-09-07T05:47:49.270667: step 4711, loss 0.0654636, acc 0.98
2016-09-07T05:47:49.940577: step 4712, loss 0.0149524, acc 1
2016-09-07T05:47:50.640532: step 4713, loss 0.0426784, acc 0.98
2016-09-07T05:47:51.317345: step 4714, loss 0.0176413, acc 1
2016-09-07T05:47:51.998800: step 4715, loss 0.0121901, acc 1
2016-09-07T05:47:52.687308: step 4716, loss 0.0116182, acc 1
2016-09-07T05:47:53.376967: step 4717, loss 0.0154793, acc 0.98
2016-09-07T05:47:54.085270: step 4718, loss 0.00273144, acc 1
2016-09-07T05:47:54.738189: step 4719, loss 0.0335574, acc 0.98
2016-09-07T05:47:55.452807: step 4720, loss 0.000630559, acc 1
2016-09-07T05:47:56.152264: step 4721, loss 0.00976992, acc 1
2016-09-07T05:47:56.825791: step 4722, loss 0.0393801, acc 0.98
2016-09-07T05:47:57.510267: step 4723, loss 0.0388562, acc 0.98
2016-09-07T05:47:58.217418: step 4724, loss 0.0360144, acc 0.98
2016-09-07T05:47:58.920738: step 4725, loss 0.0236953, acc 1
2016-09-07T05:47:59.580096: step 4726, loss 0.0356043, acc 0.98
2016-09-07T05:48:00.275783: step 4727, loss 0.0184747, acc 0.98
2016-09-07T05:48:00.938201: step 4728, loss 0.0392858, acc 0.98
2016-09-07T05:48:01.616263: step 4729, loss 0.0600062, acc 0.98
2016-09-07T05:48:02.296394: step 4730, loss 0.0306454, acc 1
2016-09-07T05:48:02.973271: step 4731, loss 0.00343924, acc 1
2016-09-07T05:48:03.660372: step 4732, loss 0.0180986, acc 0.98
2016-09-07T05:48:04.356205: step 4733, loss 0.0319869, acc 0.98
2016-09-07T05:48:05.065425: step 4734, loss 0.0245708, acc 0.98
2016-09-07T05:48:05.716945: step 4735, loss 0.0196419, acc 0.98
2016-09-07T05:48:06.388391: step 4736, loss 6.43678e-05, acc 1
2016-09-07T05:48:07.056716: step 4737, loss 0.00407054, acc 1
2016-09-07T05:48:07.746140: step 4738, loss 0.0918461, acc 0.96
2016-09-07T05:48:08.438317: step 4739, loss 0.0331984, acc 0.98
2016-09-07T05:48:09.123079: step 4740, loss 0.0243861, acc 1
2016-09-07T05:48:09.823440: step 4741, loss 0.00851593, acc 1
2016-09-07T05:48:10.489042: step 4742, loss 0.00212169, acc 1
2016-09-07T05:48:11.179520: step 4743, loss 0.0185801, acc 1
2016-09-07T05:48:11.872199: step 4744, loss 0.0342711, acc 0.96
2016-09-07T05:48:12.570670: step 4745, loss 0.0458891, acc 0.98
2016-09-07T05:48:13.260908: step 4746, loss 0.00128664, acc 1
2016-09-07T05:48:13.949606: step 4747, loss 0.0158877, acc 1
2016-09-07T05:48:14.644607: step 4748, loss 0.115059, acc 0.94
2016-09-07T05:48:15.304091: step 4749, loss 0.0100918, acc 1
2016-09-07T05:48:16.014787: step 4750, loss 0.0177043, acc 1
2016-09-07T05:48:16.726061: step 4751, loss 0.0218358, acc 1
2016-09-07T05:48:17.418616: step 4752, loss 0.0126408, acc 1
2016-09-07T05:48:18.092467: step 4753, loss 0.0297391, acc 0.98
2016-09-07T05:48:18.797528: step 4754, loss 0.0308843, acc 0.98
2016-09-07T05:48:19.493084: step 4755, loss 0.00374622, acc 1
2016-09-07T05:48:20.161106: step 4756, loss 0.0456685, acc 0.98
2016-09-07T05:48:20.844449: step 4757, loss 0.0593814, acc 0.98
2016-09-07T05:48:21.544151: step 4758, loss 0.00274156, acc 1
2016-09-07T05:48:22.251438: step 4759, loss 0.0243132, acc 1
2016-09-07T05:48:22.956468: step 4760, loss 0.0253301, acc 0.98
2016-09-07T05:48:23.616927: step 4761, loss 0.0223705, acc 0.98
2016-09-07T05:48:24.358640: step 4762, loss 0.140981, acc 0.92
2016-09-07T05:48:25.040568: step 4763, loss 0.0133031, acc 1
2016-09-07T05:48:25.723044: step 4764, loss 0.000629126, acc 1
2016-09-07T05:48:26.411124: step 4765, loss 0.00103815, acc 1
2016-09-07T05:48:27.102059: step 4766, loss 0.0638726, acc 0.96
2016-09-07T05:48:27.786031: step 4767, loss 0.00116775, acc 1
2016-09-07T05:48:28.456481: step 4768, loss 0.0245613, acc 0.98
2016-09-07T05:48:29.161517: step 4769, loss 0.00151595, acc 1
2016-09-07T05:48:29.847195: step 4770, loss 0.0141892, acc 1
2016-09-07T05:48:30.519270: step 4771, loss 0.0305593, acc 0.98
2016-09-07T05:48:31.211726: step 4772, loss 0.0207894, acc 0.98
2016-09-07T05:48:31.906080: step 4773, loss 0.0273776, acc 0.98
2016-09-07T05:48:32.602094: step 4774, loss 0.0454975, acc 0.96
2016-09-07T05:48:33.279409: step 4775, loss 0.00833307, acc 1
2016-09-07T05:48:33.982415: step 4776, loss 0.0550581, acc 0.96
2016-09-07T05:48:34.654963: step 4777, loss 0.0412306, acc 0.96
2016-09-07T05:48:35.331816: step 4778, loss 0.034977, acc 0.98
2016-09-07T05:48:36.026873: step 4779, loss 0.047242, acc 0.96
2016-09-07T05:48:36.708996: step 4780, loss 0.034345, acc 0.98
2016-09-07T05:48:37.403866: step 4781, loss 0.138796, acc 0.94
2016-09-07T05:48:38.087806: step 4782, loss 0.0131962, acc 1
2016-09-07T05:48:38.792763: step 4783, loss 0.111609, acc 0.94
2016-09-07T05:48:39.472448: step 4784, loss 0.080272, acc 0.98
2016-09-07T05:48:40.148246: step 4785, loss 0.0442807, acc 0.98
2016-09-07T05:48:40.842340: step 4786, loss 0.0190638, acc 0.98
2016-09-07T05:48:41.528849: step 4787, loss 0.0351096, acc 0.98
2016-09-07T05:48:42.212952: step 4788, loss 0.111503, acc 0.98
2016-09-07T05:48:42.894463: step 4789, loss 0.0185699, acc 1
2016-09-07T05:48:43.607217: step 4790, loss 0.0791483, acc 0.98
2016-09-07T05:48:44.344940: step 4791, loss 0.0334126, acc 0.98
2016-09-07T05:48:45.036112: step 4792, loss 0.023021, acc 1
2016-09-07T05:48:45.732380: step 4793, loss 0.0252372, acc 0.98
2016-09-07T05:48:46.418546: step 4794, loss 0.0243456, acc 0.98
2016-09-07T05:48:47.138811: step 4795, loss 0.0438177, acc 0.98
2016-09-07T05:48:47.829519: step 4796, loss 0.000491012, acc 1
2016-09-07T05:48:48.517990: step 4797, loss 0.0231941, acc 1
2016-09-07T05:48:49.211015: step 4798, loss 0.0648081, acc 0.98
2016-09-07T05:48:49.898566: step 4799, loss 0.0243266, acc 1
2016-09-07T05:48:50.547778: step 4800, loss 0.00477856, acc 1

Evaluation:
2016-09-07T05:48:53.697953: step 4800, loss 1.78009, acc 0.739212

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-4800

2016-09-07T05:48:55.384212: step 4801, loss 0.00757969, acc 1
2016-09-07T05:48:56.081458: step 4802, loss 0.0775893, acc 0.98
2016-09-07T05:48:56.799807: step 4803, loss 0.0345542, acc 0.98
2016-09-07T05:48:57.465079: step 4804, loss 0.0335883, acc 0.98
2016-09-07T05:48:58.151326: step 4805, loss 0.0225379, acc 1
2016-09-07T05:48:58.844632: step 4806, loss 0.0195252, acc 1
2016-09-07T05:48:59.529011: step 4807, loss 0.0108308, acc 1
2016-09-07T05:49:00.241617: step 4808, loss 0.00551289, acc 1
2016-09-07T05:49:00.895158: step 4809, loss 0.0339923, acc 0.98
2016-09-07T05:49:01.611718: step 4810, loss 0.0225956, acc 0.98
2016-09-07T05:49:02.329973: step 4811, loss 0.00553261, acc 1
2016-09-07T05:49:03.014451: step 4812, loss 0.040595, acc 0.98
2016-09-07T05:49:03.721895: step 4813, loss 0.0397499, acc 0.98
2016-09-07T05:49:04.430293: step 4814, loss 0.0739703, acc 0.96
2016-09-07T05:49:05.135426: step 4815, loss 0.00460916, acc 1
2016-09-07T05:49:05.823766: step 4816, loss 0.0167651, acc 1
2016-09-07T05:49:06.502078: step 4817, loss 0.0155124, acc 1
2016-09-07T05:49:07.191235: step 4818, loss 0.0296866, acc 0.98
2016-09-07T05:49:07.887376: step 4819, loss 0.00449414, acc 1
2016-09-07T05:49:08.590018: step 4820, loss 0.0377682, acc 0.98
2016-09-07T05:49:09.284825: step 4821, loss 0.00719198, acc 1
2016-09-07T05:49:10.019531: step 4822, loss 0.017471, acc 1
2016-09-07T05:49:10.700294: step 4823, loss 0.00489537, acc 1
2016-09-07T05:49:11.427105: step 4824, loss 0.000215843, acc 1
2016-09-07T05:49:12.118887: step 4825, loss 0.0133251, acc 1
2016-09-07T05:49:12.808802: step 4826, loss 0.00359134, acc 1
2016-09-07T05:49:13.505325: step 4827, loss 0.0298476, acc 0.98
2016-09-07T05:49:14.176445: step 4828, loss 0.001001, acc 1
2016-09-07T05:49:14.894740: step 4829, loss 0.0588134, acc 0.98
2016-09-07T05:49:15.583472: step 4830, loss 0.00749564, acc 1
2016-09-07T05:49:16.266520: step 4831, loss 0.00554901, acc 1
2016-09-07T05:49:16.952639: step 4832, loss 0.110908, acc 0.94
2016-09-07T05:49:17.640709: step 4833, loss 0.0167875, acc 1
2016-09-07T05:49:18.354128: step 4834, loss 0.026189, acc 0.98
2016-09-07T05:49:19.036288: step 4835, loss 0.0256712, acc 0.98
2016-09-07T05:49:19.731429: step 4836, loss 0.00643598, acc 1
2016-09-07T05:49:20.429673: step 4837, loss 0.0256252, acc 0.98
2016-09-07T05:49:21.151822: step 4838, loss 0.0616816, acc 0.96
2016-09-07T05:49:21.855422: step 4839, loss 0.0361776, acc 0.98
2016-09-07T05:49:22.527606: step 4840, loss 0.0596377, acc 0.98
2016-09-07T05:49:23.242589: step 4841, loss 0.0189766, acc 1
2016-09-07T05:49:23.931222: step 4842, loss 0.00210374, acc 1
2016-09-07T05:49:24.602006: step 4843, loss 0.00292114, acc 1
2016-09-07T05:49:25.286844: step 4844, loss 0.0013698, acc 1
2016-09-07T05:49:25.974963: step 4845, loss 0.000955642, acc 1
2016-09-07T05:49:26.674554: step 4846, loss 0.00034849, acc 1
2016-09-07T05:49:27.352630: step 4847, loss 0.0133667, acc 1
2016-09-07T05:49:28.051574: step 4848, loss 0.000613981, acc 1
2016-09-07T05:49:28.710165: step 4849, loss 0.0206824, acc 0.98
2016-09-07T05:49:29.383643: step 4850, loss 0.0120156, acc 1
2016-09-07T05:49:30.081850: step 4851, loss 0.0495502, acc 0.96
2016-09-07T05:49:30.757979: step 4852, loss 0.00188491, acc 1
2016-09-07T05:49:31.457724: step 4853, loss 0.00271208, acc 1
2016-09-07T05:49:32.141032: step 4854, loss 0.00881401, acc 1
2016-09-07T05:49:32.840361: step 4855, loss 0.103708, acc 0.94
2016-09-07T05:49:33.528509: step 4856, loss 0.0366649, acc 0.96
2016-09-07T05:49:34.216671: step 4857, loss 0.128523, acc 0.98
2016-09-07T05:49:34.907812: step 4858, loss 0.000711427, acc 1
2016-09-07T05:49:35.582459: step 4859, loss 0.0178669, acc 0.98
2016-09-07T05:49:36.253949: step 4860, loss 0.0403536, acc 0.98
2016-09-07T05:49:36.941957: step 4861, loss 0.137758, acc 0.94
2016-09-07T05:49:37.669364: step 4862, loss 0.00181344, acc 1
2016-09-07T05:49:38.353016: step 4863, loss 0.0509808, acc 0.96
2016-09-07T05:49:39.038255: step 4864, loss 0.0855002, acc 0.96
2016-09-07T05:49:39.749469: step 4865, loss 0.00755336, acc 1
2016-09-07T05:49:40.436554: step 4866, loss 0.0310602, acc 0.98
2016-09-07T05:49:41.124959: step 4867, loss 0.00388738, acc 1
2016-09-07T05:49:41.802130: step 4868, loss 0.00846158, acc 1
2016-09-07T05:49:42.507499: step 4869, loss 0.0643402, acc 0.96
2016-09-07T05:49:43.181698: step 4870, loss 0.00750087, acc 1
2016-09-07T05:49:43.857228: step 4871, loss 0.00348083, acc 1
2016-09-07T05:49:44.532378: step 4872, loss 0.0295158, acc 0.98
2016-09-07T05:49:45.221490: step 4873, loss 0.00293252, acc 1
2016-09-07T05:49:45.913540: step 4874, loss 0.00313959, acc 1
2016-09-07T05:49:46.560253: step 4875, loss 0.000341731, acc 1
2016-09-07T05:49:47.257155: step 4876, loss 0.0352933, acc 1
2016-09-07T05:49:47.919147: step 4877, loss 0.0208709, acc 1
2016-09-07T05:49:48.597937: step 4878, loss 0.000279304, acc 1
2016-09-07T05:49:49.299655: step 4879, loss 0.0407263, acc 0.96
2016-09-07T05:49:50.010651: step 4880, loss 0.0193525, acc 1
2016-09-07T05:49:50.716895: step 4881, loss 0.0250593, acc 1
2016-09-07T05:49:51.381581: step 4882, loss 0.0266241, acc 0.98
2016-09-07T05:49:52.092153: step 4883, loss 0.101632, acc 0.96
2016-09-07T05:49:52.767700: step 4884, loss 0.0447049, acc 0.98
2016-09-07T05:49:53.441729: step 4885, loss 0.00957998, acc 1
2016-09-07T05:49:54.122566: step 4886, loss 0.012273, acc 1
2016-09-07T05:49:54.792766: step 4887, loss 0.0218774, acc 0.98
2016-09-07T05:49:55.481398: step 4888, loss 0.0193165, acc 1
2016-09-07T05:49:56.169542: step 4889, loss 0.00988995, acc 1
2016-09-07T05:49:56.910699: step 4890, loss 0.0621602, acc 0.94
2016-09-07T05:49:57.607700: step 4891, loss 0.00941326, acc 1
2016-09-07T05:49:58.284580: step 4892, loss 0.0459789, acc 0.98
2016-09-07T05:49:58.960308: step 4893, loss 0.0186045, acc 1
2016-09-07T05:49:59.652397: step 4894, loss 0.0329204, acc 0.98
2016-09-07T05:50:00.407575: step 4895, loss 0.0208811, acc 0.98
2016-09-07T05:50:01.080476: step 4896, loss 0.00955139, acc 1
2016-09-07T05:50:01.802525: step 4897, loss 0.017255, acc 1
2016-09-07T05:50:02.489956: step 4898, loss 0.0514398, acc 0.94
2016-09-07T05:50:03.169912: step 4899, loss 0.092458, acc 0.98
2016-09-07T05:50:03.868015: step 4900, loss 0.00661582, acc 1

Evaluation:
2016-09-07T05:50:07.034412: step 4900, loss 2.22737, acc 0.738274

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-4900

2016-09-07T05:50:08.788717: step 4901, loss 0.022972, acc 1
2016-09-07T05:50:09.447005: step 4902, loss 0.0508335, acc 0.98
2016-09-07T05:50:10.138898: step 4903, loss 0.0430585, acc 0.98
2016-09-07T05:50:10.800769: step 4904, loss 0.0215667, acc 0.98
2016-09-07T05:50:11.473252: step 4905, loss 0.00143141, acc 1
2016-09-07T05:50:12.147543: step 4906, loss 0.00668921, acc 1
2016-09-07T05:50:12.840985: step 4907, loss 0.0207172, acc 0.98
2016-09-07T05:50:13.547559: step 4908, loss 0.00159855, acc 1
2016-09-07T05:50:14.216230: step 4909, loss 0.0218341, acc 0.98
2016-09-07T05:50:14.929222: step 4910, loss 0.0361797, acc 0.98
2016-09-07T05:50:15.593857: step 4911, loss 0.0434813, acc 0.98
2016-09-07T05:50:16.282304: step 4912, loss 0.0025723, acc 1
2016-09-07T05:50:16.990404: step 4913, loss 0.029414, acc 0.98
2016-09-07T05:50:17.686789: step 4914, loss 0.0101961, acc 1
2016-09-07T05:50:18.383006: step 4915, loss 0.00309573, acc 1
2016-09-07T05:50:19.059802: step 4916, loss 0.0422379, acc 0.98
2016-09-07T05:50:19.761879: step 4917, loss 0.00410427, acc 1
2016-09-07T05:50:20.439165: step 4918, loss 0.0261586, acc 0.98
2016-09-07T05:50:21.127930: step 4919, loss 0.00902246, acc 1
2016-09-07T05:50:21.815591: step 4920, loss 0.0273025, acc 0.98
2016-09-07T05:50:22.493696: step 4921, loss 0.0132307, acc 1
2016-09-07T05:50:23.202346: step 4922, loss 0.0095522, acc 1
2016-09-07T05:50:23.861773: step 4923, loss 0.000399456, acc 1
2016-09-07T05:50:24.585468: step 4924, loss 0.00413671, acc 1
2016-09-07T05:50:25.259830: step 4925, loss 0.0262067, acc 1
2016-09-07T05:50:25.946647: step 4926, loss 0.00826057, acc 1
2016-09-07T05:50:26.634234: step 4927, loss 0.0041792, acc 1
2016-09-07T05:50:27.331537: step 4928, loss 0.00448128, acc 1
2016-09-07T05:50:28.039574: step 4929, loss 0.00729196, acc 1
2016-09-07T05:50:28.721491: step 4930, loss 0.00108986, acc 1
2016-09-07T05:50:29.406859: step 4931, loss 0.0493514, acc 0.98
2016-09-07T05:50:30.086930: step 4932, loss 0.0228099, acc 0.98
2016-09-07T05:50:30.792569: step 4933, loss 0.0101682, acc 1
2016-09-07T05:50:31.476933: step 4934, loss 0.172785, acc 0.98
2016-09-07T05:50:32.167284: step 4935, loss 0.0156821, acc 1
2016-09-07T05:50:32.885559: step 4936, loss 0.0251766, acc 0.98
2016-09-07T05:50:33.565314: step 4937, loss 0.0246226, acc 0.98
2016-09-07T05:50:34.286203: step 4938, loss 1.66948e-05, acc 1
2016-09-07T05:50:34.974155: step 4939, loss 0.00359004, acc 1
2016-09-07T05:50:35.664380: step 4940, loss 0.0124125, acc 1
2016-09-07T05:50:36.339592: step 4941, loss 0.00751, acc 1
2016-09-07T05:50:37.037936: step 4942, loss 0.0130294, acc 1
2016-09-07T05:50:37.750239: step 4943, loss 0.00409006, acc 1
2016-09-07T05:50:38.408884: step 4944, loss 0.0377445, acc 0.98
2016-09-07T05:50:39.084776: step 4945, loss 0.00854127, acc 1
2016-09-07T05:50:39.768770: step 4946, loss 0.0483824, acc 0.96
2016-09-07T05:50:40.447081: step 4947, loss 0.080922, acc 0.98
2016-09-07T05:50:41.130438: step 4948, loss 0.0127463, acc 1
2016-09-07T05:50:41.841842: step 4949, loss 0.0206936, acc 0.98
2016-09-07T05:50:42.529493: step 4950, loss 0.000861742, acc 1
2016-09-07T05:50:43.193236: step 4951, loss 0.079897, acc 0.96
2016-09-07T05:50:43.898552: step 4952, loss 0.140136, acc 0.96
2016-09-07T05:50:44.616659: step 4953, loss 0.145546, acc 0.96
2016-09-07T05:50:45.311673: step 4954, loss 0.0239752, acc 1
2016-09-07T05:50:46.016624: step 4955, loss 0.0292942, acc 0.98
2016-09-07T05:50:46.696685: step 4956, loss 7.08793e-05, acc 1
2016-09-07T05:50:47.383756: step 4957, loss 0.0170837, acc 1
2016-09-07T05:50:48.052607: step 4958, loss 0.0109066, acc 1
2016-09-07T05:50:48.724393: step 4959, loss 0.000294158, acc 1
2016-09-07T05:50:49.405531: step 4960, loss 0.0133132, acc 1
2016-09-07T05:50:50.072226: step 4961, loss 0.0135528, acc 1
2016-09-07T05:50:50.762020: step 4962, loss 0.0228704, acc 1
2016-09-07T05:50:51.442813: step 4963, loss 0.000651753, acc 1
2016-09-07T05:50:52.157457: step 4964, loss 0.0290258, acc 0.98
2016-09-07T05:50:52.835530: step 4965, loss 0.0356649, acc 0.96
2016-09-07T05:50:53.518793: step 4966, loss 0.0929435, acc 0.98
2016-09-07T05:50:54.191376: step 4967, loss 0.00388007, acc 1
2016-09-07T05:50:54.885187: step 4968, loss 0.078562, acc 0.98
2016-09-07T05:50:55.576500: step 4969, loss 0.0460454, acc 0.98
2016-09-07T05:50:56.283752: step 4970, loss 0.0740891, acc 0.94
2016-09-07T05:50:56.992057: step 4971, loss 0.0976279, acc 0.94
2016-09-07T05:50:57.708825: step 4972, loss 0.0118679, acc 1
2016-09-07T05:50:58.416618: step 4973, loss 0.0208773, acc 1
2016-09-07T05:50:59.118736: step 4974, loss 0.0189232, acc 1
2016-09-07T05:50:59.809647: step 4975, loss 0.0156512, acc 1
2016-09-07T05:51:00.535819: step 4976, loss 0.0271175, acc 0.98
2016-09-07T05:51:01.194383: step 4977, loss 0.0054208, acc 1
2016-09-07T05:51:01.894474: step 4978, loss 0.0370342, acc 0.98
2016-09-07T05:51:02.591024: step 4979, loss 0.00889746, acc 1
2016-09-07T05:51:03.277121: step 4980, loss 0.0285519, acc 0.98
2016-09-07T05:51:03.972586: step 4981, loss 0.0658414, acc 0.98
2016-09-07T05:51:04.657278: step 4982, loss 0.144848, acc 0.98
2016-09-07T05:51:05.341181: step 4983, loss 0.00460725, acc 1
2016-09-07T05:51:05.986321: step 4984, loss 0.00355523, acc 1
2016-09-07T05:51:06.689806: step 4985, loss 0.0158372, acc 1
2016-09-07T05:51:07.377734: step 4986, loss 0.0117338, acc 1
2016-09-07T05:51:08.056372: step 4987, loss 0.0189484, acc 0.98
2016-09-07T05:51:08.720259: step 4988, loss 0.0121536, acc 1
2016-09-07T05:51:09.418280: step 4989, loss 0.0978096, acc 0.96
2016-09-07T05:51:10.109527: step 4990, loss 0.116172, acc 0.96
2016-09-07T05:51:10.787418: step 4991, loss 0.035401, acc 0.98
2016-09-07T05:51:11.443620: step 4992, loss 0.0322458, acc 1
2016-09-07T05:51:12.121494: step 4993, loss 0.0160119, acc 1
2016-09-07T05:51:12.806722: step 4994, loss 0.0359996, acc 1
2016-09-07T05:51:13.479689: step 4995, loss 0.12199, acc 0.94
2016-09-07T05:51:14.150134: step 4996, loss 0.0178833, acc 1
2016-09-07T05:51:14.866207: step 4997, loss 0.0154339, acc 1
2016-09-07T05:51:15.537972: step 4998, loss 0.075694, acc 0.98
2016-09-07T05:51:16.243644: step 4999, loss 0.0703565, acc 0.96
2016-09-07T05:51:16.936146: step 5000, loss 0.012397, acc 1

Evaluation:
2016-09-07T05:51:20.084879: step 5000, loss 1.8003, acc 0.73546

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-5000

2016-09-07T05:51:21.810588: step 5001, loss 0.0229916, acc 0.98
2016-09-07T05:51:22.503108: step 5002, loss 0.00406464, acc 1
2016-09-07T05:51:23.178355: step 5003, loss 0.017178, acc 0.98
2016-09-07T05:51:23.862398: step 5004, loss 0.00270688, acc 1
2016-09-07T05:51:24.554355: step 5005, loss 0.0502926, acc 0.98
2016-09-07T05:51:25.214550: step 5006, loss 0.018016, acc 1
2016-09-07T05:51:25.910117: step 5007, loss 0.00592607, acc 1
2016-09-07T05:51:26.602236: step 5008, loss 0.0729271, acc 0.96
2016-09-07T05:51:27.293612: step 5009, loss 0.0509309, acc 0.96
2016-09-07T05:51:28.000120: step 5010, loss 0.0238914, acc 0.98
2016-09-07T05:51:28.684976: step 5011, loss 0.0213381, acc 1
2016-09-07T05:51:29.390212: step 5012, loss 0.0361308, acc 0.98
2016-09-07T05:51:30.057804: step 5013, loss 0.0124383, acc 1
2016-09-07T05:51:30.752934: step 5014, loss 0.0701287, acc 0.96
2016-09-07T05:51:31.465983: step 5015, loss 0.0224462, acc 0.98
2016-09-07T05:51:32.141728: step 5016, loss 0.00858669, acc 1
2016-09-07T05:51:32.828694: step 5017, loss 0.0652186, acc 0.96
2016-09-07T05:51:33.522477: step 5018, loss 0.00170455, acc 1
2016-09-07T05:51:34.222322: step 5019, loss 0.00601172, acc 1
2016-09-07T05:51:34.903360: step 5020, loss 0.00213239, acc 1
2016-09-07T05:51:35.574778: step 5021, loss 0.0172374, acc 0.98
2016-09-07T05:51:36.270124: step 5022, loss 0.0209541, acc 1
2016-09-07T05:51:36.964884: step 5023, loss 0.0262703, acc 0.98
2016-09-07T05:51:37.640562: step 5024, loss 0.0420856, acc 0.98
2016-09-07T05:51:38.311735: step 5025, loss 0.0196229, acc 0.98
2016-09-07T05:51:39.010472: step 5026, loss 0.0175909, acc 0.98
2016-09-07T05:51:39.669686: step 5027, loss 0.0220287, acc 0.98
2016-09-07T05:51:40.384140: step 5028, loss 0.0228026, acc 1
2016-09-07T05:51:41.080877: step 5029, loss 0.000736743, acc 1
2016-09-07T05:51:41.751345: step 5030, loss 0.0371032, acc 0.98
2016-09-07T05:51:42.436666: step 5031, loss 0.00254067, acc 1
2016-09-07T05:51:43.113797: step 5032, loss 0.000452299, acc 1
2016-09-07T05:51:43.818926: step 5033, loss 0.0441337, acc 0.96
2016-09-07T05:51:44.476940: step 5034, loss 0.00887443, acc 1
2016-09-07T05:51:45.170128: step 5035, loss 0.0383258, acc 0.98
2016-09-07T05:51:45.852006: step 5036, loss 0.00263437, acc 1
2016-09-07T05:51:46.540845: step 5037, loss 0.063461, acc 0.96
2016-09-07T05:51:47.236389: step 5038, loss 0.0332, acc 0.98
2016-09-07T05:51:47.925646: step 5039, loss 0.00039571, acc 1
2016-09-07T05:51:48.629735: step 5040, loss 0.000440021, acc 1
2016-09-07T05:51:49.300998: step 5041, loss 0.00820826, acc 1
2016-09-07T05:51:50.027475: step 5042, loss 0.0518638, acc 0.98
2016-09-07T05:51:50.692691: step 5043, loss 0.0923618, acc 0.96
2016-09-07T05:51:51.374791: step 5044, loss 0.0133726, acc 1
2016-09-07T05:51:52.082604: step 5045, loss 0.0175653, acc 1
2016-09-07T05:51:52.805092: step 5046, loss 0.00328168, acc 1
2016-09-07T05:51:53.518415: step 5047, loss 0.00794791, acc 1
2016-09-07T05:51:54.203441: step 5048, loss 0.0373079, acc 0.96
2016-09-07T05:51:54.894347: step 5049, loss 0.00533117, acc 1
2016-09-07T05:51:55.586230: step 5050, loss 0.00323605, acc 1
2016-09-07T05:51:56.283517: step 5051, loss 0.0354293, acc 0.96
2016-09-07T05:51:56.986589: step 5052, loss 0.00657824, acc 1
2016-09-07T05:51:57.656632: step 5053, loss 0.0283058, acc 0.98
2016-09-07T05:51:58.369344: step 5054, loss 0.0202504, acc 1
2016-09-07T05:51:59.077905: step 5055, loss 0.0229597, acc 0.98
2016-09-07T05:51:59.762768: step 5056, loss 0.0358312, acc 0.98
2016-09-07T05:52:00.511103: step 5057, loss 0.0986267, acc 0.96
2016-09-07T05:52:01.200254: step 5058, loss 0.0290676, acc 0.98
2016-09-07T05:52:01.907076: step 5059, loss 0.0160025, acc 1
2016-09-07T05:52:02.579935: step 5060, loss 0.00882698, acc 1
2016-09-07T05:52:03.275297: step 5061, loss 0.0197348, acc 1
2016-09-07T05:52:03.964033: step 5062, loss 0.0649757, acc 0.96
2016-09-07T05:52:04.652538: step 5063, loss 0.0378366, acc 0.98
2016-09-07T05:52:05.345386: step 5064, loss 0.00809431, acc 1
2016-09-07T05:52:06.018317: step 5065, loss 0.0170813, acc 1
2016-09-07T05:52:06.727068: step 5066, loss 0.0289609, acc 0.98
2016-09-07T05:52:07.404576: step 5067, loss 0.000231504, acc 1
2016-09-07T05:52:08.103233: step 5068, loss 0.0534804, acc 0.96
2016-09-07T05:52:08.807910: step 5069, loss 0.0144788, acc 1
2016-09-07T05:52:09.497875: step 5070, loss 0.00183186, acc 1
2016-09-07T05:52:10.197843: step 5071, loss 0.0863611, acc 0.98
2016-09-07T05:52:10.844793: step 5072, loss 0.0018879, acc 1
2016-09-07T05:52:11.552472: step 5073, loss 0.00788334, acc 1
2016-09-07T05:52:12.226191: step 5074, loss 0.038483, acc 0.96
2016-09-07T05:52:12.906481: step 5075, loss 0.0200294, acc 1
2016-09-07T05:52:13.605616: step 5076, loss 0.0395638, acc 0.98
2016-09-07T05:52:14.283532: step 5077, loss 0.0529611, acc 0.98
2016-09-07T05:52:14.993473: step 5078, loss 0.00731888, acc 1
2016-09-07T05:52:15.647370: step 5079, loss 3.69933e-05, acc 1
2016-09-07T05:52:16.378814: step 5080, loss 0.012703, acc 1
2016-09-07T05:52:17.065822: step 5081, loss 0.0121641, acc 1
2016-09-07T05:52:17.759399: step 5082, loss 0.0329545, acc 0.98
2016-09-07T05:52:18.442746: step 5083, loss 0.0244441, acc 0.98
2016-09-07T05:52:19.141012: step 5084, loss 0.0162788, acc 0.98
2016-09-07T05:52:19.851477: step 5085, loss 0.000713023, acc 1
2016-09-07T05:52:20.521508: step 5086, loss 0.000555362, acc 1
2016-09-07T05:52:21.221809: step 5087, loss 0.0444167, acc 0.98
2016-09-07T05:52:21.915225: step 5088, loss 0.0225843, acc 1
2016-09-07T05:52:22.616637: step 5089, loss 0.021401, acc 1
2016-09-07T05:52:23.296906: step 5090, loss 0.0752212, acc 0.96
2016-09-07T05:52:24.013627: step 5091, loss 0.00897557, acc 1
2016-09-07T05:52:24.747690: step 5092, loss 0.0354092, acc 0.98
2016-09-07T05:52:25.434664: step 5093, loss 0.0156352, acc 1
2016-09-07T05:52:26.116476: step 5094, loss 0.00378269, acc 1
2016-09-07T05:52:26.811142: step 5095, loss 0.0341183, acc 0.96
2016-09-07T05:52:27.503785: step 5096, loss 0.00660031, acc 1
2016-09-07T05:52:28.201495: step 5097, loss 0.00440335, acc 1
2016-09-07T05:52:28.878121: step 5098, loss 0.0178064, acc 0.98
2016-09-07T05:52:29.591263: step 5099, loss 0.0497565, acc 0.98
2016-09-07T05:52:30.273094: step 5100, loss 0.0338591, acc 0.98

Evaluation:
2016-09-07T05:52:33.445664: step 5100, loss 2.48423, acc 0.738274

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-5100

2016-09-07T05:52:35.118461: step 5101, loss 0.00117651, acc 1
2016-09-07T05:52:35.807584: step 5102, loss 0.0164631, acc 0.98
2016-09-07T05:52:36.492948: step 5103, loss 0.103444, acc 0.96
2016-09-07T05:52:37.170064: step 5104, loss 0.00126869, acc 1
2016-09-07T05:52:37.855787: step 5105, loss 0.0101144, acc 1
2016-09-07T05:52:38.519383: step 5106, loss 0.108648, acc 0.96
2016-09-07T05:52:39.234036: step 5107, loss 0.00140837, acc 1
2016-09-07T05:52:39.918167: step 5108, loss 0.0297431, acc 0.98
2016-09-07T05:52:40.622517: step 5109, loss 0.0290903, acc 0.98
2016-09-07T05:52:41.326565: step 5110, loss 0.0169087, acc 1
2016-09-07T05:52:42.010413: step 5111, loss 0.00244711, acc 1
2016-09-07T05:52:42.701252: step 5112, loss 0.0127231, acc 1
2016-09-07T05:52:43.373084: step 5113, loss 0.0295769, acc 0.98
2016-09-07T05:52:44.087710: step 5114, loss 0.0267933, acc 1
2016-09-07T05:52:44.766821: step 5115, loss 0.0815976, acc 0.98
2016-09-07T05:52:45.464458: step 5116, loss 0.00982227, acc 1
2016-09-07T05:52:46.150133: step 5117, loss 0.0220455, acc 1
2016-09-07T05:52:46.828459: step 5118, loss 0.0121394, acc 1
2016-09-07T05:52:47.509941: step 5119, loss 0.00542637, acc 1
2016-09-07T05:52:48.174098: step 5120, loss 0.0184952, acc 0.98
2016-09-07T05:52:48.882444: step 5121, loss 0.0906909, acc 0.98
2016-09-07T05:52:49.580997: step 5122, loss 0.0568235, acc 0.98
2016-09-07T05:52:50.286491: step 5123, loss 0.0367567, acc 0.98
2016-09-07T05:52:50.980754: step 5124, loss 0.0420312, acc 0.96
2016-09-07T05:52:51.690134: step 5125, loss 3.38722e-05, acc 1
2016-09-07T05:52:52.404178: step 5126, loss 0.0730879, acc 0.94
2016-09-07T05:52:53.116721: step 5127, loss 0.042308, acc 0.98
2016-09-07T05:52:53.798677: step 5128, loss 0.00942894, acc 1
2016-09-07T05:52:54.478090: step 5129, loss 0.117081, acc 0.96
2016-09-07T05:52:55.160528: step 5130, loss 0.00076889, acc 1
2016-09-07T05:52:55.846617: step 5131, loss 0.100822, acc 0.96
2016-09-07T05:52:56.509633: step 5132, loss 0.0363075, acc 0.96
2016-09-07T05:52:57.218220: step 5133, loss 0.00878408, acc 1
2016-09-07T05:52:57.910266: step 5134, loss 0.0277292, acc 0.98
2016-09-07T05:52:58.602943: step 5135, loss 0.0111898, acc 1
2016-09-07T05:52:59.304485: step 5136, loss 0.0140663, acc 1
2016-09-07T05:52:59.988645: step 5137, loss 0.00949648, acc 1
2016-09-07T05:53:00.692803: step 5138, loss 0.0107028, acc 1
2016-09-07T05:53:01.367345: step 5139, loss 0.0327221, acc 0.98
2016-09-07T05:53:02.074289: step 5140, loss 0.0296572, acc 0.98
2016-09-07T05:53:02.749484: step 5141, loss 0.106935, acc 0.96
2016-09-07T05:53:03.426170: step 5142, loss 0.136667, acc 0.96
2016-09-07T05:53:04.149523: step 5143, loss 0.0367448, acc 0.98
2016-09-07T05:53:04.826279: step 5144, loss 0.0304942, acc 0.98
2016-09-07T05:53:05.502384: step 5145, loss 0.0361097, acc 1
2016-09-07T05:53:06.200227: step 5146, loss 0.129921, acc 0.98
2016-09-07T05:53:06.907350: step 5147, loss 0.0317061, acc 0.98
2016-09-07T05:53:07.619589: step 5148, loss 0.000608497, acc 1
2016-09-07T05:53:08.324809: step 5149, loss 9.54961e-05, acc 1
2016-09-07T05:53:09.020945: step 5150, loss 0.0329149, acc 0.98
2016-09-07T05:53:09.691153: step 5151, loss 0.0180914, acc 1
2016-09-07T05:53:10.371742: step 5152, loss 0.0093976, acc 1
2016-09-07T05:53:11.073049: step 5153, loss 0.00847919, acc 1
2016-09-07T05:53:11.772205: step 5154, loss 0.0522224, acc 0.98
2016-09-07T05:53:12.468276: step 5155, loss 0.0147992, acc 0.98
2016-09-07T05:53:13.162580: step 5156, loss 0.0212783, acc 0.98
2016-09-07T05:53:13.841578: step 5157, loss 0.020072, acc 0.98
2016-09-07T05:53:14.517993: step 5158, loss 0.0571284, acc 0.98
2016-09-07T05:53:15.206413: step 5159, loss 0.0119181, acc 1
2016-09-07T05:53:15.860376: step 5160, loss 0.0586539, acc 0.98
2016-09-07T05:53:16.566903: step 5161, loss 0.0467882, acc 0.96
2016-09-07T05:53:17.291060: step 5162, loss 0.0232705, acc 1
2016-09-07T05:53:17.979850: step 5163, loss 0.0144444, acc 1
2016-09-07T05:53:18.650772: step 5164, loss 0.00917422, acc 1
2016-09-07T05:53:19.332491: step 5165, loss 0.014022, acc 1
2016-09-07T05:53:20.029575: step 5166, loss 0.031077, acc 0.98
2016-09-07T05:53:20.700031: step 5167, loss 0.0149667, acc 1
2016-09-07T05:53:21.416150: step 5168, loss 0.0569275, acc 0.98
2016-09-07T05:53:22.106435: step 5169, loss 0.00764251, acc 1
2016-09-07T05:53:22.813621: step 5170, loss 0.00718311, acc 1
2016-09-07T05:53:23.494098: step 5171, loss 0.0331245, acc 0.98
2016-09-07T05:53:24.183991: step 5172, loss 0.0217496, acc 0.98
2016-09-07T05:53:24.871210: step 5173, loss 0.065344, acc 0.98
2016-09-07T05:53:25.539464: step 5174, loss 0.00056856, acc 1
2016-09-07T05:53:26.230967: step 5175, loss 0.00933778, acc 1
2016-09-07T05:53:26.904473: step 5176, loss 0.0151218, acc 0.98
2016-09-07T05:53:27.614767: step 5177, loss 0.0864478, acc 0.94
2016-09-07T05:53:28.305542: step 5178, loss 0.0460086, acc 0.98
2016-09-07T05:53:28.980029: step 5179, loss 0.0521084, acc 0.98
2016-09-07T05:53:29.667503: step 5180, loss 0.000142464, acc 1
2016-09-07T05:53:30.324854: step 5181, loss 0.0226465, acc 0.98
2016-09-07T05:53:31.017750: step 5182, loss 0.0244683, acc 1
2016-09-07T05:53:31.715863: step 5183, loss 0.0298501, acc 0.98
2016-09-07T05:53:32.383859: step 5184, loss 0.0175296, acc 1
2016-09-07T05:53:33.058049: step 5185, loss 0.0180326, acc 1
2016-09-07T05:53:33.742077: step 5186, loss 0.0537388, acc 0.96
2016-09-07T05:53:34.429213: step 5187, loss 0.0652028, acc 0.96
2016-09-07T05:53:35.086703: step 5188, loss 0.0454842, acc 0.96
2016-09-07T05:53:35.792283: step 5189, loss 0.0044774, acc 1
2016-09-07T05:53:36.473228: step 5190, loss 0.142505, acc 0.92
2016-09-07T05:53:37.161826: step 5191, loss 0.0219247, acc 1
2016-09-07T05:53:37.832676: step 5192, loss 0.0246693, acc 0.98
2016-09-07T05:53:38.519037: step 5193, loss 0.00844478, acc 1
2016-09-07T05:53:39.208893: step 5194, loss 0.0838717, acc 0.98
2016-09-07T05:53:39.878291: step 5195, loss 0.0117798, acc 1
2016-09-07T05:53:40.596593: step 5196, loss 0.0156717, acc 0.98
2016-09-07T05:53:41.281794: step 5197, loss 0.071339, acc 0.96
2016-09-07T05:53:41.963434: step 5198, loss 0.0032266, acc 1
2016-09-07T05:53:42.654784: step 5199, loss 0.000724679, acc 1
2016-09-07T05:53:43.336606: step 5200, loss 0.00103555, acc 1

Evaluation:
2016-09-07T05:53:46.520976: step 5200, loss 2.27375, acc 0.743902

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-5200

2016-09-07T05:53:48.194259: step 5201, loss 0.0233834, acc 1
2016-09-07T05:53:48.889800: step 5202, loss 0.00746703, acc 1
2016-09-07T05:53:49.558564: step 5203, loss 0.0956482, acc 0.96
2016-09-07T05:53:50.248014: step 5204, loss 0.0298147, acc 0.98
2016-09-07T05:53:50.958686: step 5205, loss 0.0125009, acc 1
2016-09-07T05:53:51.632702: step 5206, loss 0.031007, acc 0.98
2016-09-07T05:53:52.314769: step 5207, loss 0.0159452, acc 1
2016-09-07T05:53:53.003505: step 5208, loss 0.00131047, acc 1
2016-09-07T05:53:53.666629: step 5209, loss 0.026848, acc 0.98
2016-09-07T05:53:54.323466: step 5210, loss 0.0127024, acc 1
2016-09-07T05:53:55.024297: step 5211, loss 0.0086916, acc 1
2016-09-07T05:53:55.715491: step 5212, loss 0.0100843, acc 1
2016-09-07T05:53:56.407076: step 5213, loss 0.00726907, acc 1
2016-09-07T05:53:57.085682: step 5214, loss 0.0357023, acc 0.98
2016-09-07T05:53:57.787324: step 5215, loss 0.0776496, acc 0.96
2016-09-07T05:53:58.502294: step 5216, loss 0.107272, acc 0.96
2016-09-07T05:53:59.169868: step 5217, loss 0.0137058, acc 1
2016-09-07T05:53:59.862093: step 5218, loss 0.0402022, acc 0.98
2016-09-07T05:54:00.570598: step 5219, loss 0.0325561, acc 0.98
2016-09-07T05:54:01.251257: step 5220, loss 0.0391209, acc 0.98
2016-09-07T05:54:01.946275: step 5221, loss 0.00425733, acc 1
2016-09-07T05:54:02.645728: step 5222, loss 0.0368653, acc 0.98
2016-09-07T05:54:03.356673: step 5223, loss 0.00330518, acc 1
2016-09-07T05:54:04.055451: step 5224, loss 0.0187697, acc 1
2016-09-07T05:54:04.739510: step 5225, loss 0.0399061, acc 0.98
2016-09-07T05:54:05.441402: step 5226, loss 0.0281926, acc 1
2016-09-07T05:54:06.132943: step 5227, loss 0.0454802, acc 0.98
2016-09-07T05:54:06.824816: step 5228, loss 0.0758698, acc 0.98
2016-09-07T05:54:07.517735: step 5229, loss 0.0551266, acc 0.96
2016-09-07T05:54:08.230722: step 5230, loss 0.0170871, acc 1
2016-09-07T05:54:08.936923: step 5231, loss 0.0373554, acc 0.98
2016-09-07T05:54:09.614232: step 5232, loss 0.0159397, acc 1
2016-09-07T05:54:10.294837: step 5233, loss 0.00359071, acc 1
2016-09-07T05:54:10.984381: step 5234, loss 0.00172736, acc 1
2016-09-07T05:54:11.688109: step 5235, loss 0.0367637, acc 0.98
2016-09-07T05:54:12.364047: step 5236, loss 0.0198166, acc 0.98
2016-09-07T05:54:13.062593: step 5237, loss 0.00846321, acc 1
2016-09-07T05:54:13.721036: step 5238, loss 0.0313413, acc 0.98
2016-09-07T05:54:14.394075: step 5239, loss 0.0235615, acc 1
2016-09-07T05:54:15.083462: step 5240, loss 0.0215484, acc 0.98
2016-09-07T05:54:15.758055: step 5241, loss 0.0266611, acc 0.98
2016-09-07T05:54:16.450862: step 5242, loss 0.0156626, acc 1
2016-09-07T05:54:17.135921: step 5243, loss 0.00215953, acc 1
2016-09-07T05:54:17.864535: step 5244, loss 0.000168661, acc 1
2016-09-07T05:54:18.562349: step 5245, loss 0.0237192, acc 0.98
2016-09-07T05:54:19.248319: step 5246, loss 0.0156376, acc 1
2016-09-07T05:54:19.946724: step 5247, loss 0.0283451, acc 1
2016-09-07T05:54:20.643563: step 5248, loss 0.0210207, acc 0.98
2016-09-07T05:54:21.329315: step 5249, loss 0.0462141, acc 0.98
2016-09-07T05:54:22.023810: step 5250, loss 0.0213248, acc 1
2016-09-07T05:54:22.730422: step 5251, loss 0.0439704, acc 0.96
2016-09-07T05:54:23.424923: step 5252, loss 0.00951428, acc 1
2016-09-07T05:54:24.116093: step 5253, loss 0.0148172, acc 1
2016-09-07T05:54:24.804439: step 5254, loss 0.0168599, acc 1
2016-09-07T05:54:25.495251: step 5255, loss 0.0121125, acc 1
2016-09-07T05:54:26.187186: step 5256, loss 0.0267337, acc 0.98
2016-09-07T05:54:26.860310: step 5257, loss 0.00300482, acc 1
2016-09-07T05:54:27.557704: step 5258, loss 0.0046937, acc 1
2016-09-07T05:54:28.244928: step 5259, loss 0.0124871, acc 1
2016-09-07T05:54:28.967184: step 5260, loss 0.0444381, acc 0.98
2016-09-07T05:54:29.650175: step 5261, loss 0.0134747, acc 1
2016-09-07T05:54:30.344898: step 5262, loss 0.0206664, acc 0.98
2016-09-07T05:54:31.064856: step 5263, loss 0.00766017, acc 1
2016-09-07T05:54:31.751771: step 5264, loss 0.00922622, acc 1
2016-09-07T05:54:32.418090: step 5265, loss 0.000452462, acc 1
2016-09-07T05:54:33.100426: step 5266, loss 0.00737108, acc 1
2016-09-07T05:54:33.786166: step 5267, loss 0.0694577, acc 0.98
2016-09-07T05:54:34.452214: step 5268, loss 0.00478342, acc 1
2016-09-07T05:54:35.155752: step 5269, loss 0.0326836, acc 0.96
2016-09-07T05:54:35.851763: step 5270, loss 0.148604, acc 0.96
2016-09-07T05:54:36.551755: step 5271, loss 0.00145093, acc 1
2016-09-07T05:54:37.231350: step 5272, loss 0.101363, acc 0.96
2016-09-07T05:54:37.908334: step 5273, loss 0.0210106, acc 1
2016-09-07T05:54:38.607985: step 5274, loss 0.041377, acc 0.96
2016-09-07T05:54:39.288956: step 5275, loss 0.0670591, acc 0.96
2016-09-07T05:54:39.967087: step 5276, loss 0.000501692, acc 1
2016-09-07T05:54:40.670346: step 5277, loss 0.015661, acc 1
2016-09-07T05:54:41.329951: step 5278, loss 0.00640927, acc 1
2016-09-07T05:54:42.009188: step 5279, loss 0.0536027, acc 0.96
2016-09-07T05:54:42.697363: step 5280, loss 0.0371374, acc 0.96
2016-09-07T05:54:43.385255: step 5281, loss 0.00513158, acc 1
2016-09-07T05:54:44.099129: step 5282, loss 0.029339, acc 1
2016-09-07T05:54:44.791195: step 5283, loss 0.046415, acc 0.98
2016-09-07T05:54:45.494709: step 5284, loss 0.0394335, acc 0.98
2016-09-07T05:54:46.167396: step 5285, loss 0.0231483, acc 0.98
2016-09-07T05:54:46.855000: step 5286, loss 0.0829053, acc 0.96
2016-09-07T05:54:47.534169: step 5287, loss 0.0224463, acc 0.98
2016-09-07T05:54:48.234390: step 5288, loss 0.0276213, acc 0.98
2016-09-07T05:54:48.933545: step 5289, loss 0.00732649, acc 1
2016-09-07T05:54:49.614513: step 5290, loss 0.0189716, acc 0.98
2016-09-07T05:54:50.331295: step 5291, loss 0.000504624, acc 1
2016-09-07T05:54:51.031116: step 5292, loss 0.0179359, acc 1
2016-09-07T05:54:51.704620: step 5293, loss 0.00100373, acc 1
2016-09-07T05:54:52.394048: step 5294, loss 0.0297831, acc 0.98
2016-09-07T05:54:53.081578: step 5295, loss 0.0125216, acc 1
2016-09-07T05:54:53.779721: step 5296, loss 0.0389072, acc 0.98
2016-09-07T05:54:54.471123: step 5297, loss 0.01885, acc 1
2016-09-07T05:54:55.186891: step 5298, loss 0.0148732, acc 0.98
2016-09-07T05:54:55.874189: step 5299, loss 0.00715484, acc 1
2016-09-07T05:54:56.563382: step 5300, loss 0.0192651, acc 1

Evaluation:
2016-09-07T05:54:59.773249: step 5300, loss 2.39779, acc 0.750469

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-5300

2016-09-07T05:55:01.519076: step 5301, loss 0.0131857, acc 1
2016-09-07T05:55:02.206833: step 5302, loss 0.0433268, acc 0.98
2016-09-07T05:55:02.870218: step 5303, loss 0.0205645, acc 0.98
2016-09-07T05:55:03.568187: step 5304, loss 0.0411147, acc 0.98
2016-09-07T05:55:04.239453: step 5305, loss 0.00688768, acc 1
2016-09-07T05:55:04.920036: step 5306, loss 0.00387233, acc 1
2016-09-07T05:55:05.615079: step 5307, loss 0.00254829, acc 1
2016-09-07T05:55:06.282482: step 5308, loss 0.000954367, acc 1
2016-09-07T05:55:06.963258: step 5309, loss 0.00270568, acc 1
2016-09-07T05:55:07.642634: step 5310, loss 0.00641275, acc 1
2016-09-07T05:55:08.357501: step 5311, loss 0.0166361, acc 0.98
2016-09-07T05:55:09.044600: step 5312, loss 0.00855583, acc 1
2016-09-07T05:55:09.738216: step 5313, loss 0.0371648, acc 0.98
2016-09-07T05:55:10.428300: step 5314, loss 0.023359, acc 1
2016-09-07T05:55:11.102841: step 5315, loss 0.043224, acc 0.98
2016-09-07T05:55:11.804819: step 5316, loss 0.0114698, acc 1
2016-09-07T05:55:12.477431: step 5317, loss 0.0134046, acc 1
2016-09-07T05:55:13.169007: step 5318, loss 0.0840511, acc 0.98
2016-09-07T05:55:13.834194: step 5319, loss 0.0695158, acc 0.96
2016-09-07T05:55:14.605239: step 5320, loss 0.087516, acc 0.92
2016-09-07T05:55:15.300336: step 5321, loss 0.00210315, acc 1
2016-09-07T05:55:15.998927: step 5322, loss 0.000477882, acc 1
2016-09-07T05:55:16.704448: step 5323, loss 0.0061956, acc 1
2016-09-07T05:55:17.364101: step 5324, loss 0.00845573, acc 1
2016-09-07T05:55:18.058138: step 5325, loss 0.0040509, acc 1
2016-09-07T05:55:18.756949: step 5326, loss 0.0311578, acc 0.98
2016-09-07T05:55:19.443876: step 5327, loss 0.163683, acc 0.96
2016-09-07T05:55:20.126724: step 5328, loss 0.0743491, acc 0.96
2016-09-07T05:55:20.827954: step 5329, loss 0.0342088, acc 0.96
2016-09-07T05:55:21.531899: step 5330, loss 0.00879497, acc 1
2016-09-07T05:55:22.218831: step 5331, loss 0.00824, acc 1
2016-09-07T05:55:22.885808: step 5332, loss 0.0052401, acc 1
2016-09-07T05:55:23.585053: step 5333, loss 0.306192, acc 0.96
2016-09-07T05:55:24.261865: step 5334, loss 0.0697579, acc 0.98
2016-09-07T05:55:24.931301: step 5335, loss 0.0106131, acc 1
2016-09-07T05:55:25.640760: step 5336, loss 0.0138092, acc 1
2016-09-07T05:55:26.349599: step 5337, loss 0.023041, acc 1
2016-09-07T05:55:27.024638: step 5338, loss 0.0332541, acc 1
2016-09-07T05:55:27.728668: step 5339, loss 0.00687228, acc 1
2016-09-07T05:55:28.442190: step 5340, loss 0.0250951, acc 1
2016-09-07T05:55:29.126240: step 5341, loss 0.0130746, acc 1
2016-09-07T05:55:29.819882: step 5342, loss 0.0359713, acc 0.98
2016-09-07T05:55:30.478629: step 5343, loss 0.0268928, acc 1
2016-09-07T05:55:31.184626: step 5344, loss 0.0672824, acc 0.96
2016-09-07T05:55:31.856519: step 5345, loss 0.0282478, acc 0.98
2016-09-07T05:55:32.524284: step 5346, loss 0.0875158, acc 0.98
2016-09-07T05:55:33.203157: step 5347, loss 0.0967616, acc 0.98
2016-09-07T05:55:33.901625: step 5348, loss 0.0283516, acc 1
2016-09-07T05:55:34.600074: step 5349, loss 0.00394395, acc 1
2016-09-07T05:55:35.302283: step 5350, loss 0.0184495, acc 0.98
2016-09-07T05:55:36.008454: step 5351, loss 0.0444874, acc 0.96
2016-09-07T05:55:36.682082: step 5352, loss 0.0653257, acc 0.98
2016-09-07T05:55:37.352572: step 5353, loss 0.0272446, acc 1
2016-09-07T05:55:38.038146: step 5354, loss 0.0297766, acc 1
2016-09-07T05:55:38.717352: step 5355, loss 0.0349534, acc 0.96
2016-09-07T05:55:39.421273: step 5356, loss 0.0219074, acc 0.98
2016-09-07T05:55:40.104155: step 5357, loss 0.117019, acc 0.98
2016-09-07T05:55:40.819130: step 5358, loss 0.0380136, acc 0.98
2016-09-07T05:55:41.481136: step 5359, loss 0.117791, acc 0.94
2016-09-07T05:55:42.189539: step 5360, loss 0.0490263, acc 0.98
2016-09-07T05:55:42.865644: step 5361, loss 0.027559, acc 0.98
2016-09-07T05:55:43.542825: step 5362, loss 0.0251817, acc 0.98
2016-09-07T05:55:44.223572: step 5363, loss 0.0128848, acc 1
2016-09-07T05:55:44.903384: step 5364, loss 0.082633, acc 0.94
2016-09-07T05:55:45.615171: step 5365, loss 0.033715, acc 0.98
2016-09-07T05:55:46.295339: step 5366, loss 0.0116195, acc 1
2016-09-07T05:55:46.975424: step 5367, loss 0.0448005, acc 0.96
2016-09-07T05:55:47.675508: step 5368, loss 0.0447024, acc 0.98
2016-09-07T05:55:48.362756: step 5369, loss 0.0619409, acc 0.96
2016-09-07T05:55:49.059422: step 5370, loss 0.0566922, acc 0.98
2016-09-07T05:55:49.713888: step 5371, loss 0.0394904, acc 0.98
2016-09-07T05:55:50.424007: step 5372, loss 0.0620884, acc 0.98
2016-09-07T05:55:51.101283: step 5373, loss 0.00757933, acc 1
2016-09-07T05:55:51.793336: step 5374, loss 0.0218131, acc 1
2016-09-07T05:55:52.472096: step 5375, loss 0.0274616, acc 0.98
2016-09-07T05:55:53.127737: step 5376, loss 0.0374931, acc 0.977273
2016-09-07T05:55:53.839988: step 5377, loss 0.0159153, acc 1
2016-09-07T05:55:54.503243: step 5378, loss 0.0129735, acc 1
2016-09-07T05:55:55.196597: step 5379, loss 0.0230444, acc 1
2016-09-07T05:55:55.859270: step 5380, loss 0.0139693, acc 1
2016-09-07T05:55:56.559653: step 5381, loss 0.00219066, acc 1
2016-09-07T05:55:57.263182: step 5382, loss 0.0283885, acc 0.98
2016-09-07T05:55:57.972324: step 5383, loss 0.0104304, acc 1
2016-09-07T05:55:58.648772: step 5384, loss 0.0131776, acc 1
2016-09-07T05:55:59.353768: step 5385, loss 0.00671522, acc 1
2016-09-07T05:56:00.067217: step 5386, loss 0.00102938, acc 1
2016-09-07T05:56:00.785578: step 5387, loss 0.00369072, acc 1
2016-09-07T05:56:01.475755: step 5388, loss 0.0337995, acc 0.98
2016-09-07T05:56:02.156978: step 5389, loss 0.046962, acc 0.98
2016-09-07T05:56:02.847296: step 5390, loss 0.0303219, acc 0.98
2016-09-07T05:56:03.539706: step 5391, loss 0.000560597, acc 1
2016-09-07T05:56:04.231990: step 5392, loss 0.0201915, acc 0.98
2016-09-07T05:56:04.940173: step 5393, loss 0.045216, acc 0.96
2016-09-07T05:56:05.651432: step 5394, loss 0.00114672, acc 1
2016-09-07T05:56:06.326138: step 5395, loss 0.033421, acc 1
2016-09-07T05:56:07.020695: step 5396, loss 0.0247822, acc 0.98
2016-09-07T05:56:07.706443: step 5397, loss 0.0514094, acc 0.96
2016-09-07T05:56:08.394889: step 5398, loss 0.0747912, acc 0.98
2016-09-07T05:56:09.086595: step 5399, loss 0.000828955, acc 1
2016-09-07T05:56:09.788439: step 5400, loss 0.0349907, acc 0.98

Evaluation:
2016-09-07T05:56:12.971227: step 5400, loss 1.93696, acc 0.737336

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-5400

2016-09-07T05:56:14.636137: step 5401, loss 0.00523149, acc 1
2016-09-07T05:56:15.340750: step 5402, loss 0.00117688, acc 1
2016-09-07T05:56:16.026901: step 5403, loss 0.00120881, acc 1
2016-09-07T05:56:16.709873: step 5404, loss 0.0350268, acc 0.98
2016-09-07T05:56:17.404721: step 5405, loss 0.0119935, acc 1
2016-09-07T05:56:18.091948: step 5406, loss 0.00803657, acc 1
2016-09-07T05:56:18.770132: step 5407, loss 0.00193349, acc 1
2016-09-07T05:56:19.448754: step 5408, loss 0.0340242, acc 1
2016-09-07T05:56:20.169719: step 5409, loss 0.0274119, acc 0.98
2016-09-07T05:56:20.847642: step 5410, loss 0.0182878, acc 0.98
2016-09-07T05:56:21.531253: step 5411, loss 0.0243892, acc 0.98
2016-09-07T05:56:22.188063: step 5412, loss 0.0491471, acc 0.98
2016-09-07T05:56:22.905900: step 5413, loss 0.015464, acc 0.98
2016-09-07T05:56:23.569359: step 5414, loss 0.0840559, acc 0.94
2016-09-07T05:56:24.242939: step 5415, loss 0.00790059, acc 1
2016-09-07T05:56:24.935189: step 5416, loss 0.000729687, acc 1
2016-09-07T05:56:25.650738: step 5417, loss 0.047819, acc 0.96
2016-09-07T05:56:26.337136: step 5418, loss 0.139293, acc 0.98
2016-09-07T05:56:27.007624: step 5419, loss 0.0281303, acc 1
2016-09-07T05:56:27.706304: step 5420, loss 0.0646908, acc 0.96
2016-09-07T05:56:28.368350: step 5421, loss 0.00477018, acc 1
2016-09-07T05:56:29.073178: step 5422, loss 0.0460192, acc 0.96
2016-09-07T05:56:29.757309: step 5423, loss 0.1497, acc 0.98
2016-09-07T05:56:30.444508: step 5424, loss 0.0446039, acc 0.98
2016-09-07T05:56:31.122905: step 5425, loss 0.0337408, acc 0.98
2016-09-07T05:56:31.811489: step 5426, loss 0.0256423, acc 1
2016-09-07T05:56:32.518877: step 5427, loss 0.00417918, acc 1
2016-09-07T05:56:33.193113: step 5428, loss 0.0392686, acc 0.98
2016-09-07T05:56:33.890065: step 5429, loss 0.000573346, acc 1
2016-09-07T05:56:34.572260: step 5430, loss 0.0305641, acc 0.98
2016-09-07T05:56:35.271542: step 5431, loss 0.00206901, acc 1
2016-09-07T05:56:35.950809: step 5432, loss 0.0358996, acc 0.98
2016-09-07T05:56:36.622990: step 5433, loss 0.0619041, acc 0.96
2016-09-07T05:56:37.338883: step 5434, loss 0.000601351, acc 1
2016-09-07T05:56:38.050136: step 5435, loss 0.00109599, acc 1
2016-09-07T05:56:38.745781: step 5436, loss 0.00961805, acc 1
2016-09-07T05:56:39.421719: step 5437, loss 0.0027392, acc 1
2016-09-07T05:56:40.105517: step 5438, loss 0.0321911, acc 0.96
2016-09-07T05:56:40.799279: step 5439, loss 0.0181948, acc 1
2016-09-07T05:56:41.482814: step 5440, loss 0.0067034, acc 1
2016-09-07T05:56:42.191505: step 5441, loss 0.0248527, acc 1
2016-09-07T05:56:42.876066: step 5442, loss 0.0988768, acc 0.94
2016-09-07T05:56:43.566138: step 5443, loss 0.0192493, acc 1
2016-09-07T05:56:44.259012: step 5444, loss 0.0146465, acc 1
2016-09-07T05:56:44.939171: step 5445, loss 0.17426, acc 0.98
2016-09-07T05:56:45.635218: step 5446, loss 0.0971747, acc 0.92
2016-09-07T05:56:46.300536: step 5447, loss 0.00689626, acc 1
2016-09-07T05:56:47.006407: step 5448, loss 0.0152314, acc 1
2016-09-07T05:56:47.674538: step 5449, loss 0.00201678, acc 1
2016-09-07T05:56:48.350849: step 5450, loss 0.0200024, acc 1
2016-09-07T05:56:49.032159: step 5451, loss 0.0586098, acc 0.96
2016-09-07T05:56:49.740164: step 5452, loss 0.0540399, acc 0.98
2016-09-07T05:56:50.410793: step 5453, loss 0.0224227, acc 0.98
2016-09-07T05:56:51.097804: step 5454, loss 0.012322, acc 1
2016-09-07T05:56:51.802213: step 5455, loss 0.0512403, acc 0.96
2016-09-07T05:56:52.474421: step 5456, loss 0.0929422, acc 0.96
2016-09-07T05:56:53.171595: step 5457, loss 0.000793682, acc 1
2016-09-07T05:56:53.856731: step 5458, loss 0.0192548, acc 0.98
2016-09-07T05:56:54.546777: step 5459, loss 0.0432039, acc 0.98
2016-09-07T05:56:55.235647: step 5460, loss 0.0019115, acc 1
2016-09-07T05:56:55.927541: step 5461, loss 0.00177444, acc 1
2016-09-07T05:56:56.628591: step 5462, loss 0.063472, acc 0.96
2016-09-07T05:56:57.315320: step 5463, loss 0.00242528, acc 1
2016-09-07T05:56:58.002387: step 5464, loss 0.0712162, acc 0.98
2016-09-07T05:56:58.689351: step 5465, loss 0.00692499, acc 1
2016-09-07T05:56:59.383896: step 5466, loss 0.0647112, acc 0.96
2016-09-07T05:57:00.098203: step 5467, loss 0.0255861, acc 0.98
2016-09-07T05:57:00.821012: step 5468, loss 0.0260418, acc 0.98
2016-09-07T05:57:01.519403: step 5469, loss 0.0491664, acc 0.98
2016-09-07T05:57:02.224818: step 5470, loss 0.027371, acc 0.98
2016-09-07T05:57:02.914196: step 5471, loss 0.0329731, acc 0.98
2016-09-07T05:57:03.598730: step 5472, loss 0.000833772, acc 1
2016-09-07T05:57:04.290187: step 5473, loss 0.0049229, acc 1
2016-09-07T05:57:04.994771: step 5474, loss 0.062264, acc 0.98
2016-09-07T05:57:05.683107: step 5475, loss 0.0451233, acc 0.98
2016-09-07T05:57:06.385554: step 5476, loss 0.00787901, acc 1
2016-09-07T05:57:07.073605: step 5477, loss 0.0174746, acc 0.98
2016-09-07T05:57:07.751971: step 5478, loss 0.0664042, acc 0.98
2016-09-07T05:57:08.438902: step 5479, loss 0.0725931, acc 0.96
2016-09-07T05:57:09.101394: step 5480, loss 0.0144272, acc 1
2016-09-07T05:57:09.821468: step 5481, loss 0.0110252, acc 1
2016-09-07T05:57:10.517529: step 5482, loss 0.0213468, acc 0.98
2016-09-07T05:57:11.193279: step 5483, loss 0.00422708, acc 1
2016-09-07T05:57:11.871100: step 5484, loss 0.0103095, acc 1
2016-09-07T05:57:12.576620: step 5485, loss 0.00376429, acc 1
2016-09-07T05:57:13.291043: step 5486, loss 0.0783456, acc 0.96
2016-09-07T05:57:13.971143: step 5487, loss 0.0620213, acc 0.98
2016-09-07T05:57:14.676100: step 5488, loss 0.035478, acc 0.98
2016-09-07T05:57:15.372737: step 5489, loss 0.0143419, acc 1
2016-09-07T05:57:16.049036: step 5490, loss 0.024395, acc 0.98
2016-09-07T05:57:16.733201: step 5491, loss 0.0414637, acc 0.98
2016-09-07T05:57:17.433865: step 5492, loss 0.0564777, acc 0.96
2016-09-07T05:57:18.126658: step 5493, loss 0.0152984, acc 0.98
2016-09-07T05:57:18.799783: step 5494, loss 0.0106029, acc 1
2016-09-07T05:57:19.524936: step 5495, loss 0.0237273, acc 0.98
2016-09-07T05:57:20.239482: step 5496, loss 0.0629781, acc 0.96
2016-09-07T05:57:20.927183: step 5497, loss 0.0228222, acc 0.98
2016-09-07T05:57:21.624340: step 5498, loss 0.018879, acc 1
2016-09-07T05:57:22.297465: step 5499, loss 0.01967, acc 1
2016-09-07T05:57:23.016616: step 5500, loss 0.00844609, acc 1

Evaluation:
2016-09-07T05:57:26.184274: step 5500, loss 1.95371, acc 0.73546

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-5500

2016-09-07T05:57:27.833697: step 5501, loss 0.0764197, acc 0.96
2016-09-07T05:57:28.519563: step 5502, loss 0.0171892, acc 0.98
2016-09-07T05:57:29.208570: step 5503, loss 0.0170549, acc 1
2016-09-07T05:57:29.895188: step 5504, loss 0.0845115, acc 0.98
2016-09-07T05:57:30.564664: step 5505, loss 0.00112801, acc 1
2016-09-07T05:57:31.251683: step 5506, loss 0.222771, acc 0.94
2016-09-07T05:57:31.947730: step 5507, loss 0.0224142, acc 1
2016-09-07T05:57:32.666110: step 5508, loss 0.021449, acc 1
2016-09-07T05:57:33.366866: step 5509, loss 0.00816555, acc 1
2016-09-07T05:57:34.060226: step 5510, loss 0.0277464, acc 0.98
2016-09-07T05:57:34.756393: step 5511, loss 0.0301656, acc 0.98
2016-09-07T05:57:35.445836: step 5512, loss 0.030621, acc 1
2016-09-07T05:57:36.133335: step 5513, loss 0.0108897, acc 1
2016-09-07T05:57:36.798581: step 5514, loss 0.0157526, acc 1
2016-09-07T05:57:37.482789: step 5515, loss 0.00171171, acc 1
2016-09-07T05:57:38.183955: step 5516, loss 0.0458355, acc 0.96
2016-09-07T05:57:38.890602: step 5517, loss 0.0153473, acc 1
2016-09-07T05:57:39.583210: step 5518, loss 0.00553438, acc 1
2016-09-07T05:57:40.279134: step 5519, loss 0.0660522, acc 0.98
2016-09-07T05:57:41.003028: step 5520, loss 0.0712876, acc 0.98
2016-09-07T05:57:41.693616: step 5521, loss 0.0257615, acc 1
2016-09-07T05:57:42.371742: step 5522, loss 0.0542528, acc 0.96
2016-09-07T05:57:43.085718: step 5523, loss 0.00331833, acc 1
2016-09-07T05:57:43.783377: step 5524, loss 0.00462269, acc 1
2016-09-07T05:57:44.456330: step 5525, loss 0.0554595, acc 0.98
2016-09-07T05:57:45.157454: step 5526, loss 0.00157025, acc 1
2016-09-07T05:57:45.879838: step 5527, loss 0.0326568, acc 1
2016-09-07T05:57:46.562483: step 5528, loss 0.0260077, acc 1
2016-09-07T05:57:47.273195: step 5529, loss 0.0279291, acc 0.98
2016-09-07T05:57:47.975986: step 5530, loss 0.0232413, acc 1
2016-09-07T05:57:48.667482: step 5531, loss 0.027795, acc 1
2016-09-07T05:57:49.377787: step 5532, loss 0.0128772, acc 1
2016-09-07T05:57:50.046219: step 5533, loss 0.0260811, acc 0.98
2016-09-07T05:57:50.753842: step 5534, loss 0.0344662, acc 1
2016-09-07T05:57:51.446957: step 5535, loss 0.0016228, acc 1
2016-09-07T05:57:52.132002: step 5536, loss 0.00663327, acc 1
2016-09-07T05:57:52.805859: step 5537, loss 0.0203647, acc 1
2016-09-07T05:57:53.490025: step 5538, loss 0.0437788, acc 0.96
2016-09-07T05:57:54.167868: step 5539, loss 0.0140246, acc 1
2016-09-07T05:57:54.838127: step 5540, loss 0.0853567, acc 0.96
2016-09-07T05:57:55.543653: step 5541, loss 0.0341726, acc 0.98
2016-09-07T05:57:56.214664: step 5542, loss 0.0617917, acc 0.98
2016-09-07T05:57:56.890206: step 5543, loss 0.00119151, acc 1
2016-09-07T05:57:57.586527: step 5544, loss 0.0335308, acc 1
2016-09-07T05:57:58.271757: step 5545, loss 0.0164446, acc 1
2016-09-07T05:57:58.962005: step 5546, loss 0.0291667, acc 1
2016-09-07T05:57:59.625853: step 5547, loss 0.0667354, acc 0.98
2016-09-07T05:58:00.371282: step 5548, loss 0.000670928, acc 1
2016-09-07T05:58:01.060802: step 5549, loss 0.00356049, acc 1
2016-09-07T05:58:01.746185: step 5550, loss 0.153881, acc 0.94
2016-09-07T05:58:02.440562: step 5551, loss 0.0240993, acc 1
2016-09-07T05:58:03.139612: step 5552, loss 0.0179909, acc 0.98
2016-09-07T05:58:03.842898: step 5553, loss 0.0179889, acc 1
2016-09-07T05:58:04.505546: step 5554, loss 0.0134041, acc 1
2016-09-07T05:58:05.214673: step 5555, loss 0.0454513, acc 0.98
2016-09-07T05:58:05.898445: step 5556, loss 0.0519936, acc 0.98
2016-09-07T05:58:06.592419: step 5557, loss 0.0293351, acc 1
2016-09-07T05:58:07.265480: step 5558, loss 0.00123929, acc 1
2016-09-07T05:58:07.952696: step 5559, loss 0.00249289, acc 1
2016-09-07T05:58:08.634149: step 5560, loss 0.0762588, acc 0.98
2016-09-07T05:58:09.306115: step 5561, loss 0.010498, acc 1
2016-09-07T05:58:10.019040: step 5562, loss 0.0308951, acc 0.98
2016-09-07T05:58:10.710768: step 5563, loss 0.0281656, acc 0.98
2016-09-07T05:58:11.385181: step 5564, loss 0.0637244, acc 0.96
2016-09-07T05:58:12.057856: step 5565, loss 0.0237682, acc 1
2016-09-07T05:58:12.728685: step 5566, loss 0.0219274, acc 0.98
2016-09-07T05:58:13.422181: step 5567, loss 0.0597289, acc 0.98
2016-09-07T05:58:14.052835: step 5568, loss 0.0639887, acc 0.977273
2016-09-07T05:58:14.758256: step 5569, loss 0.00761221, acc 1
2016-09-07T05:58:15.433247: step 5570, loss 0.0220275, acc 0.98
2016-09-07T05:58:16.113071: step 5571, loss 0.0325895, acc 0.98
2016-09-07T05:58:16.788384: step 5572, loss 0.0283371, acc 0.98
2016-09-07T05:58:17.475202: step 5573, loss 0.115895, acc 0.96
2016-09-07T05:58:18.171883: step 5574, loss 0.0134771, acc 1
2016-09-07T05:58:18.875517: step 5575, loss 0.00468779, acc 1
2016-09-07T05:58:19.570903: step 5576, loss 0.0349917, acc 0.98
2016-09-07T05:58:20.262406: step 5577, loss 0.0232939, acc 0.98
2016-09-07T05:58:20.962757: step 5578, loss 0.0251397, acc 1
2016-09-07T05:58:21.674557: step 5579, loss 0.0438245, acc 0.98
2016-09-07T05:58:22.389243: step 5580, loss 0.0126941, acc 1
2016-09-07T05:58:23.093705: step 5581, loss 0.0674656, acc 0.96
2016-09-07T05:58:23.772576: step 5582, loss 0.0197591, acc 1
2016-09-07T05:58:24.456524: step 5583, loss 0.0231388, acc 0.98
2016-09-07T05:58:25.133382: step 5584, loss 9.71218e-05, acc 1
2016-09-07T05:58:25.815059: step 5585, loss 0.0140148, acc 1
2016-09-07T05:58:26.495101: step 5586, loss 0.000718829, acc 1
2016-09-07T05:58:27.202455: step 5587, loss 0.0872437, acc 0.98
2016-09-07T05:58:27.911917: step 5588, loss 0.00913781, acc 1
2016-09-07T05:58:28.628610: step 5589, loss 0.0310826, acc 0.98
2016-09-07T05:58:29.323425: step 5590, loss 0.000888317, acc 1
2016-09-07T05:58:30.012716: step 5591, loss 0.00799894, acc 1
2016-09-07T05:58:30.686399: step 5592, loss 0.010706, acc 1
2016-09-07T05:58:31.374662: step 5593, loss 0.0125958, acc 1
2016-09-07T05:58:32.048685: step 5594, loss 0.013676, acc 1
2016-09-07T05:58:32.753073: step 5595, loss 0.0050708, acc 1
2016-09-07T05:58:33.453669: step 5596, loss 0.00119799, acc 1
2016-09-07T05:58:34.135813: step 5597, loss 0.0475445, acc 0.96
2016-09-07T05:58:34.843631: step 5598, loss 0.00597271, acc 1
2016-09-07T05:58:35.544738: step 5599, loss 0.0177049, acc 1
2016-09-07T05:58:36.223846: step 5600, loss 0.0438449, acc 0.96

Evaluation:
2016-09-07T05:58:39.402675: step 5600, loss 2.01865, acc 0.734522

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-5600

2016-09-07T05:58:41.166995: step 5601, loss 0.000512084, acc 1
2016-09-07T05:58:41.854942: step 5602, loss 0.0200372, acc 1
2016-09-07T05:58:42.565122: step 5603, loss 0.00234005, acc 1
2016-09-07T05:58:43.237076: step 5604, loss 0.0109078, acc 1
2016-09-07T05:58:43.927510: step 5605, loss 0.0127935, acc 1
2016-09-07T05:58:44.621403: step 5606, loss 0.0132228, acc 1
2016-09-07T05:58:45.282985: step 5607, loss 0.00502441, acc 1
2016-09-07T05:58:45.987891: step 5608, loss 0.000620617, acc 1
2016-09-07T05:58:46.671207: step 5609, loss 0.014175, acc 1
2016-09-07T05:58:47.349342: step 5610, loss 0.00689308, acc 1
2016-09-07T05:58:48.022404: step 5611, loss 0.0147252, acc 0.98
2016-09-07T05:58:48.710003: step 5612, loss 0.0223147, acc 0.98
2016-09-07T05:58:49.391636: step 5613, loss 0.0168361, acc 1
2016-09-07T05:58:50.071970: step 5614, loss 0.044704, acc 0.96
2016-09-07T05:58:50.777331: step 5615, loss 0.01601, acc 0.98
2016-09-07T05:58:51.455023: step 5616, loss 0.00101501, acc 1
2016-09-07T05:58:52.158954: step 5617, loss 0.00691201, acc 1
2016-09-07T05:58:52.853182: step 5618, loss 0.0716242, acc 0.94
2016-09-07T05:58:53.551462: step 5619, loss 0.00230109, acc 1
2016-09-07T05:58:54.232828: step 5620, loss 0.026921, acc 0.98
2016-09-07T05:58:54.909128: step 5621, loss 0.00195521, acc 1
2016-09-07T05:58:55.612107: step 5622, loss 0.0152567, acc 1
2016-09-07T05:58:56.291787: step 5623, loss 0.0117825, acc 1
2016-09-07T05:58:56.978837: step 5624, loss 0.0216062, acc 1
2016-09-07T05:58:57.663365: step 5625, loss 0.0422269, acc 0.98
2016-09-07T05:58:58.355704: step 5626, loss 6.29812e-05, acc 1
2016-09-07T05:58:59.050149: step 5627, loss 0.0108704, acc 1
2016-09-07T05:58:59.732939: step 5628, loss 0.0116936, acc 1
2016-09-07T05:59:00.464375: step 5629, loss 0.010931, acc 1
2016-09-07T05:59:01.149725: step 5630, loss 1.83954e-05, acc 1
2016-09-07T05:59:01.845816: step 5631, loss 0.0374939, acc 0.98
2016-09-07T05:59:02.536323: step 5632, loss 0.0110991, acc 1
2016-09-07T05:59:03.216838: step 5633, loss 0.0150427, acc 1
2016-09-07T05:59:03.901895: step 5634, loss 0.0786824, acc 0.96
2016-09-07T05:59:04.550440: step 5635, loss 0.00242461, acc 1
2016-09-07T05:59:05.247992: step 5636, loss 0.0487546, acc 0.98
2016-09-07T05:59:05.921456: step 5637, loss 0.0182301, acc 1
2016-09-07T05:59:06.618077: step 5638, loss 0.0519045, acc 0.96
2016-09-07T05:59:07.299501: step 5639, loss 0.0749827, acc 0.98
2016-09-07T05:59:07.991091: step 5640, loss 0.000606039, acc 1
2016-09-07T05:59:08.674780: step 5641, loss 0.00912175, acc 1
2016-09-07T05:59:09.360535: step 5642, loss 0.0113595, acc 1
2016-09-07T05:59:10.105544: step 5643, loss 0.0254293, acc 0.98
2016-09-07T05:59:10.820918: step 5644, loss 0.0157041, acc 1
2016-09-07T05:59:11.489669: step 5645, loss 0.122414, acc 0.96
2016-09-07T05:59:12.177661: step 5646, loss 0.0411892, acc 0.98
2016-09-07T05:59:12.849325: step 5647, loss 0.0755053, acc 0.98
2016-09-07T05:59:13.535800: step 5648, loss 0.0225558, acc 1
2016-09-07T05:59:14.195252: step 5649, loss 0.0012859, acc 1
2016-09-07T05:59:14.887789: step 5650, loss 0.0079102, acc 1
2016-09-07T05:59:15.575822: step 5651, loss 0.00875709, acc 1
2016-09-07T05:59:16.304239: step 5652, loss 0.00542988, acc 1
2016-09-07T05:59:17.019810: step 5653, loss 0.0124602, acc 1
2016-09-07T05:59:17.720543: step 5654, loss 0.0113681, acc 1
2016-09-07T05:59:18.428585: step 5655, loss 0.00130634, acc 1
2016-09-07T05:59:19.118501: step 5656, loss 0.0338742, acc 0.98
2016-09-07T05:59:19.804179: step 5657, loss 0.0106355, acc 1
2016-09-07T05:59:20.490489: step 5658, loss 0.036762, acc 0.98
2016-09-07T05:59:21.185230: step 5659, loss 0.0465136, acc 0.96
2016-09-07T05:59:21.876599: step 5660, loss 0.0420998, acc 0.96
2016-09-07T05:59:22.557094: step 5661, loss 0.00323078, acc 1
2016-09-07T05:59:23.258312: step 5662, loss 0.0248811, acc 0.98
2016-09-07T05:59:23.945477: step 5663, loss 0.152085, acc 0.96
2016-09-07T05:59:24.650198: step 5664, loss 0.00962777, acc 1
2016-09-07T05:59:25.342760: step 5665, loss 0.0140744, acc 1
2016-09-07T05:59:26.025021: step 5666, loss 0.0835632, acc 0.94
2016-09-07T05:59:26.704749: step 5667, loss 0.00316531, acc 1
2016-09-07T05:59:27.382926: step 5668, loss 0.0740628, acc 0.96
2016-09-07T05:59:28.072424: step 5669, loss 0.00622006, acc 1
2016-09-07T05:59:28.739599: step 5670, loss 0.00675006, acc 1
2016-09-07T05:59:29.425671: step 5671, loss 0.00150839, acc 1
2016-09-07T05:59:30.108527: step 5672, loss 0.0190038, acc 1
2016-09-07T05:59:30.793907: step 5673, loss 0.0192275, acc 0.98
2016-09-07T05:59:31.469908: step 5674, loss 0.0586568, acc 0.98
2016-09-07T05:59:32.160762: step 5675, loss 0.109002, acc 0.96
2016-09-07T05:59:32.856719: step 5676, loss 0.0155267, acc 1
2016-09-07T05:59:33.529861: step 5677, loss 0.0150049, acc 1
2016-09-07T05:59:34.238953: step 5678, loss 0.0128463, acc 1
2016-09-07T05:59:34.916005: step 5679, loss 0.0311327, acc 1
2016-09-07T05:59:35.641102: step 5680, loss 0.000282792, acc 1
2016-09-07T05:59:36.344506: step 5681, loss 0.0213601, acc 1
2016-09-07T05:59:37.012379: step 5682, loss 0.162081, acc 0.98
2016-09-07T05:59:37.713224: step 5683, loss 0.022655, acc 1
2016-09-07T05:59:38.383994: step 5684, loss 0.0812698, acc 0.94
2016-09-07T05:59:39.062865: step 5685, loss 0.0237795, acc 1
2016-09-07T05:59:39.744975: step 5686, loss 0.0546239, acc 0.98
2016-09-07T05:59:40.436682: step 5687, loss 0.0580275, acc 0.96
2016-09-07T05:59:41.122672: step 5688, loss 0.0303384, acc 0.96
2016-09-07T05:59:41.800737: step 5689, loss 0.0702996, acc 0.96
2016-09-07T05:59:42.512414: step 5690, loss 0.0315765, acc 0.98
2016-09-07T05:59:43.197594: step 5691, loss 0.00165577, acc 1
2016-09-07T05:59:43.883752: step 5692, loss 0.00052609, acc 1
2016-09-07T05:59:44.578325: step 5693, loss 0.00380057, acc 1
2016-09-07T05:59:45.270730: step 5694, loss 0.0622019, acc 0.96
2016-09-07T05:59:45.983286: step 5695, loss 0.055635, acc 0.96
2016-09-07T05:59:46.675619: step 5696, loss 0.0132257, acc 1
2016-09-07T05:59:47.373475: step 5697, loss 0.0209082, acc 0.98
2016-09-07T05:59:48.094520: step 5698, loss 0.0329136, acc 0.98
2016-09-07T05:59:48.779386: step 5699, loss 0.0288411, acc 0.98
2016-09-07T05:59:49.475579: step 5700, loss 0.00173538, acc 1

Evaluation:
2016-09-07T05:59:52.650388: step 5700, loss 1.92094, acc 0.750469

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473194978/checkpoints/model-5700

2016-09-07T05:59:54.350655: step 5701, loss 0.00135435, acc 1
2016-09-07T05:59:55.050196: step 5702, loss 0.0125877, acc 1
2016-09-07T05:59:55.741844: step 5703, loss 0.00459524, acc 1
2016-09-07T05:59:56.427364: step 5704, loss 0.00943833, acc 1
2016-09-07T05:59:57.122946: step 5705, loss 0.059166, acc 0.98
2016-09-07T05:59:57.801259: step 5706, loss 0.00195431, acc 1
2016-09-07T05:59:58.490954: step 5707, loss 0.0271721, acc 0.98
2016-09-07T05:59:59.191790: step 5708, loss 0.0249049, acc 0.98
2016-09-07T05:59:59.873379: step 5709, loss 0.00227561, acc 1
2016-09-07T06:00:00.610436: step 5710, loss 0.037098, acc 0.98
2016-09-07T06:00:01.299950: step 5711, loss 0.0716325, acc 0.98
2016-09-07T06:00:02.004985: step 5712, loss 0.0254548, acc 1
2016-09-07T06:00:02.692138: step 5713, loss 0.00490178, acc 1
2016-09-07T06:00:03.375086: step 5714, loss 0.070478, acc 0.96
2016-09-07T06:00:04.103026: step 5715, loss 0.0120141, acc 1
2016-09-07T06:00:04.765978: step 5716, loss 0.0139368, acc 1
2016-09-07T06:00:05.470371: step 5717, loss 0.00419275, acc 1
2016-09-07T06:00:06.149076: step 5718, loss 0.0315066, acc 0.98
2016-09-07T06:00:06.858229: step 5719, loss 0.0289225, acc 0.98
2016-09-07T06:00:07.540867: step 5720, loss 0.035107, acc 0.98
2016-09-07T06:00:08.231303: step 5721, loss 0.035868, acc 1
2016-09-07T06:00:08.949031: step 5722, loss 0.0164421, acc 1
2016-09-07T06:00:09.627361: step 5723, loss 0.0123591, acc 1
2016-09-07T06:00:10.344866: step 5724, loss 0.000217048, acc 1
2016-09-07T06:00:11.044048: step 5725, loss 0.000137277, acc 1
2016-09-07T06:00:11.737237: step 5726, loss 0.0225096, acc 1
2016-09-07T06:00:12.448094: step 5727, loss 0.00591777, acc 1
2016-09-07T06:00:13.127352: step 5728, loss 0.0267461, acc 0.98
2016-09-07T06:00:13.838646: step 5729, loss 0.0172653, acc 1
2016-09-07T06:00:14.520623: step 5730, loss 0.0150354, acc 0.98
2016-09-07T06:00:15.223983: step 5731, loss 0.0199223, acc 1
2016-09-07T06:00:15.910363: step 5732, loss 0.00973427, acc 1
2016-09-07T06:00:16.591853: step 5733, loss 0.032982, acc 0.98
2016-09-07T06:00:17.292108: step 5734, loss 0.0966062, acc 0.98
2016-09-07T06:00:17.957613: step 5735, loss 0.0116267, acc 1
2016-09-07T06:00:18.653271: step 5736, loss 0.00324322, acc 1
2016-09-07T06:00:19.341477: step 5737, loss 0.0990732, acc 0.96
2016-09-07T06:00:20.016798: step 5738, loss 0.0154515, acc 1
2016-09-07T06:00:20.703684: step 5739, loss 0.0178756, acc 0.98
2016-09-07T06:00:21.414217: step 5740, loss 0.0972748, acc 0.98
2016-09-07T06:00:22.115241: step 5741, loss 0.000938706, acc 1
2016-09-07T06:00:22.800900: step 5742, loss 0.000806157, acc 1
2016-09-07T06:00:23.495738: step 5743, loss 0.0121977, acc 1
2016-09-07T06:00:24.179335: step 5744, loss 0.0139704, acc 1
2016-09-07T06:00:24.873393: step 5745, loss 0.0037839, acc 1
2016-09-07T06:00:25.565604: step 5746, loss 0.030651, acc 0.98
2016-09-07T06:00:26.253758: step 5747, loss 0.00368834, acc 1
2016-09-07T06:00:26.940943: step 5748, loss 0.026348, acc 1
2016-09-07T06:00:27.610443: step 5749, loss 0.0390493, acc 0.96
2016-09-07T06:00:28.324045: step 5750, loss 0.0107699, acc 1
2016-09-07T06:00:29.004390: step 5751, loss 0.0228325, acc 1
2016-09-07T06:00:29.700521: step 5752, loss 0.154775, acc 0.96
2016-09-07T06:00:30.379303: step 5753, loss 0.025097, acc 0.98
2016-09-07T06:00:31.075086: step 5754, loss 0.0374434, acc 0.98
2016-09-07T06:00:31.785475: step 5755, loss 0.0533648, acc 0.98
2016-09-07T06:00:32.467825: step 5756, loss 0.0159177, acc 0.98
2016-09-07T06:00:33.175044: step 5757, loss 0.0544522, acc 0.98
2016-09-07T06:00:33.853790: step 5758, loss 0.031802, acc 1
2016-09-07T06:00:34.533071: step 5759, loss 0.0127924, acc 1
2016-09-07T06:00:35.171627: step 5760, loss 0.0567923, acc 0.977273

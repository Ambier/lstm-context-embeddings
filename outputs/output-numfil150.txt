WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f10c214c710>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.
WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f10c214c7d0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.

Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=50
CHECKPOINT_EVERY=100
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=300
EVALUATE_EVERY=100
FILTER_SIZES=3,4,5
HIDDEN_DIM=300
L2_REG_LAMBDA=0
LOG_DEVICE_PLACEMENT=False
NUM_EPOCHS=30
NUM_FILTERS=150
WORD2VEC=GoogleNews-vectors-negative300.bin

Loading data...
Vocabulary Size: 18758
Train/Dev split: 9662/1000
Writing to /home/cil/lstm-context-embeddings/runs/1473258106

Load word2vec file GoogleNews-vectors-negative300.bin

2016-09-07T22:22:05.166748: step 1, loss 0.693147, acc 0.44
2016-09-07T22:22:05.863758: step 2, loss 0.735133, acc 0.44
2016-09-07T22:22:06.555422: step 3, loss 0.700178, acc 0.4
2016-09-07T22:22:07.254495: step 4, loss 0.692633, acc 0.54
2016-09-07T22:22:07.947927: step 5, loss 0.663658, acc 0.64
2016-09-07T22:22:08.661561: step 6, loss 0.710566, acc 0.56
2016-09-07T22:22:09.362596: step 7, loss 0.770625, acc 0.46
2016-09-07T22:22:10.056382: step 8, loss 0.748395, acc 0.42
2016-09-07T22:22:10.756301: step 9, loss 0.671655, acc 0.6
2016-09-07T22:22:11.432836: step 10, loss 0.691683, acc 0.54
2016-09-07T22:22:12.122540: step 11, loss 0.688839, acc 0.64
2016-09-07T22:22:12.808902: step 12, loss 0.719798, acc 0.44
2016-09-07T22:22:13.504600: step 13, loss 0.690199, acc 0.56
2016-09-07T22:22:14.205057: step 14, loss 0.686972, acc 0.54
2016-09-07T22:22:14.906724: step 15, loss 0.685116, acc 0.54
2016-09-07T22:22:15.609229: step 16, loss 0.695611, acc 0.52
2016-09-07T22:22:16.292027: step 17, loss 0.689239, acc 0.48
2016-09-07T22:22:16.991987: step 18, loss 0.704317, acc 0.38
2016-09-07T22:22:17.690355: step 19, loss 0.672333, acc 0.54
2016-09-07T22:22:18.370530: step 20, loss 0.680024, acc 0.5
2016-09-07T22:22:19.068644: step 21, loss 0.69714, acc 0.54
2016-09-07T22:22:19.772348: step 22, loss 0.685972, acc 0.54
2016-09-07T22:22:20.458758: step 23, loss 0.71289, acc 0.52
2016-09-07T22:22:21.136241: step 24, loss 0.686403, acc 0.48
2016-09-07T22:22:21.835907: step 25, loss 0.67581, acc 0.66
2016-09-07T22:22:22.560987: step 26, loss 0.644021, acc 0.68
2016-09-07T22:22:23.267194: step 27, loss 0.641325, acc 0.64
2016-09-07T22:22:23.960016: step 28, loss 0.635224, acc 0.58
2016-09-07T22:22:24.657539: step 29, loss 0.65015, acc 0.56
2016-09-07T22:22:25.347752: step 30, loss 0.767308, acc 0.56
2016-09-07T22:22:26.034203: step 31, loss 0.569505, acc 0.72
2016-09-07T22:22:26.728864: step 32, loss 0.59519, acc 0.6
2016-09-07T22:22:27.418669: step 33, loss 0.681897, acc 0.54
2016-09-07T22:22:28.113687: step 34, loss 0.617557, acc 0.62
2016-09-07T22:22:28.818108: step 35, loss 0.647972, acc 0.56
2016-09-07T22:22:29.527040: step 36, loss 0.717312, acc 0.5
2016-09-07T22:22:30.205007: step 37, loss 0.614082, acc 0.68
2016-09-07T22:22:30.903884: step 38, loss 0.625487, acc 0.62
2016-09-07T22:22:31.589419: step 39, loss 0.607671, acc 0.66
2016-09-07T22:22:32.286237: step 40, loss 0.62056, acc 0.64
2016-09-07T22:22:32.985603: step 41, loss 0.621634, acc 0.6
2016-09-07T22:22:33.676970: step 42, loss 0.581532, acc 0.76
2016-09-07T22:22:34.388134: step 43, loss 0.585936, acc 0.62
2016-09-07T22:22:35.098761: step 44, loss 0.5759, acc 0.72
2016-09-07T22:22:35.800407: step 45, loss 0.690511, acc 0.62
2016-09-07T22:22:36.506369: step 46, loss 0.521303, acc 0.74
2016-09-07T22:22:37.223768: step 47, loss 0.564908, acc 0.72
2016-09-07T22:22:37.951619: step 48, loss 0.55197, acc 0.76
2016-09-07T22:22:38.658577: step 49, loss 0.684534, acc 0.72
2016-09-07T22:22:39.354890: step 50, loss 0.649013, acc 0.66
2016-09-07T22:22:40.049407: step 51, loss 0.688278, acc 0.68
2016-09-07T22:22:40.741179: step 52, loss 0.707115, acc 0.66
2016-09-07T22:22:41.428174: step 53, loss 0.528426, acc 0.68
2016-09-07T22:22:42.119132: step 54, loss 0.61231, acc 0.72
2016-09-07T22:22:42.833459: step 55, loss 0.592764, acc 0.74
2016-09-07T22:22:43.535136: step 56, loss 0.505852, acc 0.78
2016-09-07T22:22:44.232392: step 57, loss 0.630167, acc 0.64
2016-09-07T22:22:44.918519: step 58, loss 0.599921, acc 0.64
2016-09-07T22:22:45.610107: step 59, loss 0.590845, acc 0.72
2016-09-07T22:22:46.303584: step 60, loss 0.615398, acc 0.72
2016-09-07T22:22:47.024750: step 61, loss 0.675795, acc 0.58
2016-09-07T22:22:47.716123: step 62, loss 0.599268, acc 0.66
2016-09-07T22:22:48.426998: step 63, loss 0.580278, acc 0.72
2016-09-07T22:22:49.123569: step 64, loss 0.531769, acc 0.7
2016-09-07T22:22:49.814519: step 65, loss 0.44559, acc 0.82
2016-09-07T22:22:50.504366: step 66, loss 0.577767, acc 0.62
2016-09-07T22:22:51.182666: step 67, loss 0.528298, acc 0.66
2016-09-07T22:22:51.889988: step 68, loss 0.636023, acc 0.7
2016-09-07T22:22:52.582318: step 69, loss 0.440035, acc 0.82
2016-09-07T22:22:53.265593: step 70, loss 0.648318, acc 0.76
2016-09-07T22:22:53.968023: step 71, loss 0.590922, acc 0.74
2016-09-07T22:22:54.662082: step 72, loss 0.505684, acc 0.72
2016-09-07T22:22:55.353901: step 73, loss 0.451674, acc 0.76
2016-09-07T22:22:56.028666: step 74, loss 0.476351, acc 0.78
2016-09-07T22:22:56.737966: step 75, loss 0.541281, acc 0.74
2016-09-07T22:22:57.435500: step 76, loss 0.664128, acc 0.68
2016-09-07T22:22:58.137457: step 77, loss 0.520067, acc 0.76
2016-09-07T22:22:58.838403: step 78, loss 0.568582, acc 0.74
2016-09-07T22:22:59.537602: step 79, loss 0.524847, acc 0.66
2016-09-07T22:23:00.256600: step 80, loss 0.638352, acc 0.62
2016-09-07T22:23:00.937438: step 81, loss 0.508578, acc 0.76
2016-09-07T22:23:01.627226: step 82, loss 0.480402, acc 0.74
2016-09-07T22:23:02.329302: step 83, loss 0.515996, acc 0.78
2016-09-07T22:23:03.024997: step 84, loss 0.473154, acc 0.72
2016-09-07T22:23:03.720447: step 85, loss 0.572397, acc 0.74
2016-09-07T22:23:04.424616: step 86, loss 0.411518, acc 0.78
2016-09-07T22:23:05.131695: step 87, loss 0.569261, acc 0.68
2016-09-07T22:23:05.840124: step 88, loss 0.423321, acc 0.82
2016-09-07T22:23:06.546348: step 89, loss 0.413528, acc 0.78
2016-09-07T22:23:07.249873: step 90, loss 0.577222, acc 0.66
2016-09-07T22:23:07.947354: step 91, loss 0.55552, acc 0.72
2016-09-07T22:23:08.660134: step 92, loss 0.535903, acc 0.66
2016-09-07T22:23:09.350587: step 93, loss 0.488167, acc 0.74
2016-09-07T22:23:10.050073: step 94, loss 0.4361, acc 0.74
2016-09-07T22:23:10.743390: step 95, loss 0.421948, acc 0.76
2016-09-07T22:23:11.443458: step 96, loss 0.451655, acc 0.76
2016-09-07T22:23:12.159337: step 97, loss 0.576361, acc 0.74
2016-09-07T22:23:12.853455: step 98, loss 0.486329, acc 0.78
2016-09-07T22:23:13.547317: step 99, loss 0.598741, acc 0.68
2016-09-07T22:23:14.247626: step 100, loss 0.523268, acc 0.76

Evaluation:
2016-09-07T22:23:17.327449: step 100, loss 0.493537, acc 0.765

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-100

2016-09-07T22:23:19.051887: step 101, loss 0.53306, acc 0.78
2016-09-07T22:23:19.768803: step 102, loss 0.706699, acc 0.66
2016-09-07T22:23:20.471856: step 103, loss 0.470617, acc 0.78
2016-09-07T22:23:21.161561: step 104, loss 0.527092, acc 0.74
2016-09-07T22:23:21.847820: step 105, loss 0.473858, acc 0.76
2016-09-07T22:23:22.557276: step 106, loss 0.457059, acc 0.78
2016-09-07T22:23:23.241519: step 107, loss 0.48381, acc 0.78
2016-09-07T22:23:23.931291: step 108, loss 0.464732, acc 0.74
2016-09-07T22:23:24.622178: step 109, loss 0.469443, acc 0.82
2016-09-07T22:23:25.318603: step 110, loss 0.484092, acc 0.76
2016-09-07T22:23:26.001018: step 111, loss 0.529818, acc 0.76
2016-09-07T22:23:26.693449: step 112, loss 0.450092, acc 0.86
2016-09-07T22:23:27.386136: step 113, loss 0.531099, acc 0.66
2016-09-07T22:23:28.075317: step 114, loss 0.472889, acc 0.82
2016-09-07T22:23:28.767385: step 115, loss 0.453094, acc 0.76
2016-09-07T22:23:29.446562: step 116, loss 0.394167, acc 0.78
2016-09-07T22:23:30.151769: step 117, loss 0.696837, acc 0.64
2016-09-07T22:23:30.839905: step 118, loss 0.467623, acc 0.72
2016-09-07T22:23:31.586661: step 119, loss 0.56527, acc 0.78
2016-09-07T22:23:32.283888: step 120, loss 0.380452, acc 0.82
2016-09-07T22:23:32.986173: step 121, loss 0.568292, acc 0.74
2016-09-07T22:23:33.684525: step 122, loss 0.472105, acc 0.78
2016-09-07T22:23:34.375083: step 123, loss 0.59102, acc 0.74
2016-09-07T22:23:35.078246: step 124, loss 0.580423, acc 0.74
2016-09-07T22:23:35.816701: step 125, loss 0.694318, acc 0.66
2016-09-07T22:23:36.522524: step 126, loss 0.646073, acc 0.72
2016-09-07T22:23:37.230457: step 127, loss 0.679876, acc 0.66
2016-09-07T22:23:37.928645: step 128, loss 0.401539, acc 0.82
2016-09-07T22:23:38.613036: step 129, loss 0.429885, acc 0.78
2016-09-07T22:23:39.311800: step 130, loss 0.510128, acc 0.72
2016-09-07T22:23:39.989921: step 131, loss 0.528191, acc 0.78
2016-09-07T22:23:40.681875: step 132, loss 0.485699, acc 0.76
2016-09-07T22:23:41.367445: step 133, loss 0.570183, acc 0.7
2016-09-07T22:23:42.057406: step 134, loss 0.425347, acc 0.86
2016-09-07T22:23:42.744362: step 135, loss 0.588626, acc 0.68
2016-09-07T22:23:43.425026: step 136, loss 0.470489, acc 0.72
2016-09-07T22:23:44.111217: step 137, loss 0.567861, acc 0.66
2016-09-07T22:23:44.807775: step 138, loss 0.617618, acc 0.7
2016-09-07T22:23:45.508961: step 139, loss 0.475636, acc 0.76
2016-09-07T22:23:46.209394: step 140, loss 0.400707, acc 0.84
2016-09-07T22:23:46.919114: step 141, loss 0.562019, acc 0.7
2016-09-07T22:23:47.645909: step 142, loss 0.539173, acc 0.7
2016-09-07T22:23:48.354732: step 143, loss 0.575329, acc 0.68
2016-09-07T22:23:49.032321: step 144, loss 0.531857, acc 0.74
2016-09-07T22:23:49.731856: step 145, loss 0.475409, acc 0.72
2016-09-07T22:23:50.409402: step 146, loss 0.54819, acc 0.72
2016-09-07T22:23:51.117207: step 147, loss 0.539116, acc 0.68
2016-09-07T22:23:51.833734: step 148, loss 0.400194, acc 0.86
2016-09-07T22:23:52.546898: step 149, loss 0.537484, acc 0.7
2016-09-07T22:23:53.270404: step 150, loss 0.51963, acc 0.68
2016-09-07T22:23:53.970804: step 151, loss 0.555621, acc 0.68
2016-09-07T22:23:54.671009: step 152, loss 0.534193, acc 0.68
2016-09-07T22:23:55.381810: step 153, loss 0.553379, acc 0.76
2016-09-07T22:23:56.079058: step 154, loss 0.596497, acc 0.7
2016-09-07T22:23:56.792124: step 155, loss 0.45985, acc 0.76
2016-09-07T22:23:57.467018: step 156, loss 0.438288, acc 0.84
2016-09-07T22:23:58.149021: step 157, loss 0.441465, acc 0.76
2016-09-07T22:23:58.854356: step 158, loss 0.630376, acc 0.58
2016-09-07T22:23:59.546345: step 159, loss 0.501443, acc 0.76
2016-09-07T22:24:00.242000: step 160, loss 0.54921, acc 0.72
2016-09-07T22:24:00.919707: step 161, loss 0.572455, acc 0.74
2016-09-07T22:24:01.608328: step 162, loss 0.579398, acc 0.62
2016-09-07T22:24:02.314997: step 163, loss 0.430174, acc 0.8
2016-09-07T22:24:03.012738: step 164, loss 0.45423, acc 0.76
2016-09-07T22:24:03.707809: step 165, loss 0.462326, acc 0.82
2016-09-07T22:24:04.390220: step 166, loss 0.478716, acc 0.7
2016-09-07T22:24:05.077774: step 167, loss 0.507907, acc 0.72
2016-09-07T22:24:05.777205: step 168, loss 0.578851, acc 0.64
2016-09-07T22:24:06.475864: step 169, loss 0.539032, acc 0.68
2016-09-07T22:24:07.179021: step 170, loss 0.575899, acc 0.7
2016-09-07T22:24:07.863290: step 171, loss 0.599738, acc 0.66
2016-09-07T22:24:08.583085: step 172, loss 0.483446, acc 0.82
2016-09-07T22:24:09.286865: step 173, loss 0.555825, acc 0.74
2016-09-07T22:24:09.977128: step 174, loss 0.444435, acc 0.78
2016-09-07T22:24:10.684936: step 175, loss 0.449678, acc 0.78
2016-09-07T22:24:11.369688: step 176, loss 0.457903, acc 0.84
2016-09-07T22:24:12.063272: step 177, loss 0.399099, acc 0.82
2016-09-07T22:24:12.747608: step 178, loss 0.506479, acc 0.72
2016-09-07T22:24:13.438157: step 179, loss 0.428503, acc 0.8
2016-09-07T22:24:14.114560: step 180, loss 0.306443, acc 0.86
2016-09-07T22:24:14.808434: step 181, loss 0.537335, acc 0.68
2016-09-07T22:24:15.493957: step 182, loss 0.528863, acc 0.72
2016-09-07T22:24:16.183676: step 183, loss 0.451342, acc 0.78
2016-09-07T22:24:16.883762: step 184, loss 0.672983, acc 0.6
2016-09-07T22:24:17.577157: step 185, loss 0.595361, acc 0.74
2016-09-07T22:24:18.280075: step 186, loss 0.468811, acc 0.78
2016-09-07T22:24:18.993559: step 187, loss 0.483741, acc 0.7
2016-09-07T22:24:19.698613: step 188, loss 0.457389, acc 0.78
2016-09-07T22:24:20.404586: step 189, loss 0.398921, acc 0.84
2016-09-07T22:24:21.100595: step 190, loss 0.494091, acc 0.74
2016-09-07T22:24:21.807440: step 191, loss 0.573639, acc 0.74
2016-09-07T22:24:22.546543: step 192, loss 0.529287, acc 0.74
2016-09-07T22:24:23.244405: step 193, loss 0.399244, acc 0.8
2016-09-07T22:24:23.618081: step 194, loss 0.269192, acc 0.916667
2016-09-07T22:24:24.339397: step 195, loss 0.402979, acc 0.84
2016-09-07T22:24:25.022440: step 196, loss 0.369323, acc 0.86
2016-09-07T22:24:25.718491: step 197, loss 0.300066, acc 0.92
2016-09-07T22:24:26.417935: step 198, loss 0.372688, acc 0.86
2016-09-07T22:24:27.110190: step 199, loss 0.330368, acc 0.88
2016-09-07T22:24:27.802197: step 200, loss 0.462233, acc 0.76

Evaluation:
2016-09-07T22:24:30.860992: step 200, loss 0.453547, acc 0.789

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-200

2016-09-07T22:24:32.502468: step 201, loss 0.419292, acc 0.8
2016-09-07T22:24:33.206363: step 202, loss 0.366554, acc 0.84
2016-09-07T22:24:33.910712: step 203, loss 0.266084, acc 0.84
2016-09-07T22:24:34.610178: step 204, loss 0.520901, acc 0.82
2016-09-07T22:24:35.313733: step 205, loss 0.335858, acc 0.86
2016-09-07T22:24:35.997733: step 206, loss 0.2934, acc 0.84
2016-09-07T22:24:36.688607: step 207, loss 0.607594, acc 0.76
2016-09-07T22:24:37.382195: step 208, loss 0.375924, acc 0.82
2016-09-07T22:24:38.085597: step 209, loss 0.304431, acc 0.88
2016-09-07T22:24:38.778220: step 210, loss 0.351131, acc 0.84
2016-09-07T22:24:39.474859: step 211, loss 0.249842, acc 0.94
2016-09-07T22:24:40.184110: step 212, loss 0.535612, acc 0.78
2016-09-07T22:24:40.875181: step 213, loss 0.306904, acc 0.86
2016-09-07T22:24:41.550048: step 214, loss 0.531597, acc 0.76
2016-09-07T22:24:42.256387: step 215, loss 0.230136, acc 0.92
2016-09-07T22:24:42.960669: step 216, loss 0.477342, acc 0.78
2016-09-07T22:24:43.650311: step 217, loss 0.345086, acc 0.88
2016-09-07T22:24:44.345094: step 218, loss 0.488265, acc 0.76
2016-09-07T22:24:45.034186: step 219, loss 0.323884, acc 0.88
2016-09-07T22:24:45.731439: step 220, loss 0.280964, acc 0.92
2016-09-07T22:24:46.432892: step 221, loss 0.265127, acc 0.9
2016-09-07T22:24:47.122202: step 222, loss 0.307417, acc 0.8
2016-09-07T22:24:47.834042: step 223, loss 0.411807, acc 0.78
2016-09-07T22:24:48.530124: step 224, loss 0.370687, acc 0.84
2016-09-07T22:24:49.212430: step 225, loss 0.462798, acc 0.78
2016-09-07T22:24:49.892321: step 226, loss 0.421605, acc 0.8
2016-09-07T22:24:50.574102: step 227, loss 0.211747, acc 0.96
2016-09-07T22:24:51.245863: step 228, loss 0.391389, acc 0.84
2016-09-07T22:24:51.937563: step 229, loss 0.292183, acc 0.86
2016-09-07T22:24:52.626095: step 230, loss 0.392964, acc 0.76
2016-09-07T22:24:53.335948: step 231, loss 0.320088, acc 0.8
2016-09-07T22:24:54.041999: step 232, loss 0.288545, acc 0.92
2016-09-07T22:24:54.745406: step 233, loss 0.41657, acc 0.8
2016-09-07T22:24:55.422128: step 234, loss 0.413025, acc 0.82
2016-09-07T22:24:56.111288: step 235, loss 0.256012, acc 0.94
2016-09-07T22:24:56.806232: step 236, loss 0.276028, acc 0.9
2016-09-07T22:24:57.506588: step 237, loss 0.439943, acc 0.84
2016-09-07T22:24:58.211984: step 238, loss 0.354746, acc 0.84
2016-09-07T22:24:58.916885: step 239, loss 0.508028, acc 0.78
2016-09-07T22:24:59.617852: step 240, loss 0.338659, acc 0.84
2016-09-07T22:25:00.321595: step 241, loss 0.301168, acc 0.88
2016-09-07T22:25:00.995997: step 242, loss 0.36987, acc 0.86
2016-09-07T22:25:01.699755: step 243, loss 0.474386, acc 0.8
2016-09-07T22:25:02.395288: step 244, loss 0.279254, acc 0.88
2016-09-07T22:25:03.094466: step 245, loss 0.234474, acc 0.88
2016-09-07T22:25:03.789559: step 246, loss 0.334039, acc 0.84
2016-09-07T22:25:04.485784: step 247, loss 0.288047, acc 0.9
2016-09-07T22:25:05.167634: step 248, loss 0.294363, acc 0.88
2016-09-07T22:25:05.884430: step 249, loss 0.345259, acc 0.8
2016-09-07T22:25:06.601498: step 250, loss 0.377345, acc 0.84
2016-09-07T22:25:07.312139: step 251, loss 0.517455, acc 0.82
2016-09-07T22:25:08.014048: step 252, loss 0.311001, acc 0.86
2016-09-07T22:25:08.701773: step 253, loss 0.400945, acc 0.78
2016-09-07T22:25:09.427148: step 254, loss 0.613193, acc 0.78
2016-09-07T22:25:10.139477: step 255, loss 0.448036, acc 0.78
2016-09-07T22:25:10.828084: step 256, loss 0.271459, acc 0.92
2016-09-07T22:25:11.513415: step 257, loss 0.340928, acc 0.86
2016-09-07T22:25:12.210218: step 258, loss 0.526382, acc 0.66
2016-09-07T22:25:12.904870: step 259, loss 0.424167, acc 0.76
2016-09-07T22:25:13.607254: step 260, loss 0.349615, acc 0.84
2016-09-07T22:25:14.291605: step 261, loss 0.2943, acc 0.92
2016-09-07T22:25:14.979178: step 262, loss 0.356857, acc 0.84
2016-09-07T22:25:15.675821: step 263, loss 0.48856, acc 0.78
2016-09-07T22:25:16.356147: step 264, loss 0.31753, acc 0.88
2016-09-07T22:25:17.054721: step 265, loss 0.367739, acc 0.82
2016-09-07T22:25:17.749253: step 266, loss 0.297418, acc 0.88
2016-09-07T22:25:18.449975: step 267, loss 0.212087, acc 0.92
2016-09-07T22:25:19.137491: step 268, loss 0.329235, acc 0.86
2016-09-07T22:25:19.841329: step 269, loss 0.333784, acc 0.86
2016-09-07T22:25:20.525307: step 270, loss 0.262417, acc 0.9
2016-09-07T22:25:21.187322: step 271, loss 0.383512, acc 0.8
2016-09-07T22:25:21.863742: step 272, loss 0.388781, acc 0.8
2016-09-07T22:25:22.538196: step 273, loss 0.301728, acc 0.86
2016-09-07T22:25:23.251213: step 274, loss 0.405328, acc 0.86
2016-09-07T22:25:23.959488: step 275, loss 0.370225, acc 0.76
2016-09-07T22:25:24.653761: step 276, loss 0.501758, acc 0.8
2016-09-07T22:25:25.344743: step 277, loss 0.418105, acc 0.76
2016-09-07T22:25:26.039414: step 278, loss 0.321132, acc 0.86
2016-09-07T22:25:26.736170: step 279, loss 0.434608, acc 0.84
2016-09-07T22:25:27.424836: step 280, loss 0.486664, acc 0.78
2016-09-07T22:25:28.111135: step 281, loss 0.399993, acc 0.84
2016-09-07T22:25:28.806622: step 282, loss 0.274391, acc 0.92
2016-09-07T22:25:29.490233: step 283, loss 0.493568, acc 0.82
2016-09-07T22:25:30.165448: step 284, loss 0.438387, acc 0.8
2016-09-07T22:25:30.866952: step 285, loss 0.397934, acc 0.82
2016-09-07T22:25:31.559877: step 286, loss 0.417346, acc 0.78
2016-09-07T22:25:32.261220: step 287, loss 0.341577, acc 0.86
2016-09-07T22:25:32.951706: step 288, loss 0.393703, acc 0.76
2016-09-07T22:25:33.654412: step 289, loss 0.486742, acc 0.72
2016-09-07T22:25:34.346536: step 290, loss 0.29312, acc 0.82
2016-09-07T22:25:35.059676: step 291, loss 0.266614, acc 0.94
2016-09-07T22:25:35.760016: step 292, loss 0.257098, acc 0.92
2016-09-07T22:25:36.452623: step 293, loss 0.474383, acc 0.78
2016-09-07T22:25:37.136398: step 294, loss 0.310143, acc 0.86
2016-09-07T22:25:37.820511: step 295, loss 0.359315, acc 0.82
2016-09-07T22:25:38.512996: step 296, loss 0.250828, acc 0.9
2016-09-07T22:25:39.195213: step 297, loss 0.293508, acc 0.88
2016-09-07T22:25:39.880414: step 298, loss 0.280181, acc 0.88
2016-09-07T22:25:40.570351: step 299, loss 0.515321, acc 0.74
2016-09-07T22:25:41.264024: step 300, loss 0.244161, acc 0.9

Evaluation:
2016-09-07T22:25:44.356204: step 300, loss 0.472285, acc 0.792

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-300

2016-09-07T22:25:45.995235: step 301, loss 0.340207, acc 0.82
2016-09-07T22:25:46.688381: step 302, loss 0.305433, acc 0.84
2016-09-07T22:25:47.380654: step 303, loss 0.428041, acc 0.88
2016-09-07T22:25:48.070751: step 304, loss 0.292812, acc 0.88
2016-09-07T22:25:48.765104: step 305, loss 0.47408, acc 0.84
2016-09-07T22:25:49.457063: step 306, loss 0.268263, acc 0.88
2016-09-07T22:25:50.137251: step 307, loss 0.370073, acc 0.82
2016-09-07T22:25:50.832534: step 308, loss 0.387878, acc 0.82
2016-09-07T22:25:51.525096: step 309, loss 0.415751, acc 0.8
2016-09-07T22:25:52.233545: step 310, loss 0.287987, acc 0.86
2016-09-07T22:25:52.933369: step 311, loss 0.257857, acc 0.88
2016-09-07T22:25:53.643326: step 312, loss 0.298073, acc 0.86
2016-09-07T22:25:54.354220: step 313, loss 0.350557, acc 0.86
2016-09-07T22:25:55.058237: step 314, loss 0.532844, acc 0.8
2016-09-07T22:25:55.737890: step 315, loss 0.466116, acc 0.72
2016-09-07T22:25:56.439271: step 316, loss 0.389832, acc 0.82
2016-09-07T22:25:57.155020: step 317, loss 0.351936, acc 0.82
2016-09-07T22:25:57.850597: step 318, loss 0.239745, acc 0.94
2016-09-07T22:25:58.566289: step 319, loss 0.270653, acc 0.86
2016-09-07T22:25:59.256849: step 320, loss 0.402386, acc 0.78
2016-09-07T22:25:59.941859: step 321, loss 0.257824, acc 0.9
2016-09-07T22:26:00.669928: step 322, loss 0.409054, acc 0.8
2016-09-07T22:26:01.365909: step 323, loss 0.413561, acc 0.76
2016-09-07T22:26:02.095380: step 324, loss 0.297837, acc 0.88
2016-09-07T22:26:02.780151: step 325, loss 0.551151, acc 0.72
2016-09-07T22:26:03.477336: step 326, loss 0.408702, acc 0.8
2016-09-07T22:26:04.170295: step 327, loss 0.396547, acc 0.78
2016-09-07T22:26:04.854885: step 328, loss 0.429314, acc 0.8
2016-09-07T22:26:05.549018: step 329, loss 0.376068, acc 0.78
2016-09-07T22:26:06.252841: step 330, loss 0.449261, acc 0.82
2016-09-07T22:26:06.965384: step 331, loss 0.218712, acc 0.94
2016-09-07T22:26:07.641591: step 332, loss 0.289842, acc 0.88
2016-09-07T22:26:08.340567: step 333, loss 0.285906, acc 0.84
2016-09-07T22:26:09.033794: step 334, loss 0.265535, acc 0.88
2016-09-07T22:26:09.729031: step 335, loss 0.340666, acc 0.86
2016-09-07T22:26:10.413477: step 336, loss 0.35297, acc 0.88
2016-09-07T22:26:11.115723: step 337, loss 0.415637, acc 0.84
2016-09-07T22:26:11.804966: step 338, loss 0.410807, acc 0.76
2016-09-07T22:26:12.486532: step 339, loss 0.272287, acc 0.88
2016-09-07T22:26:13.168254: step 340, loss 0.315151, acc 0.88
2016-09-07T22:26:13.861247: step 341, loss 0.255271, acc 0.84
2016-09-07T22:26:14.555618: step 342, loss 0.319225, acc 0.9
2016-09-07T22:26:15.248250: step 343, loss 0.27611, acc 0.86
2016-09-07T22:26:15.948932: step 344, loss 0.426531, acc 0.82
2016-09-07T22:26:16.644812: step 345, loss 0.387442, acc 0.86
2016-09-07T22:26:17.355037: step 346, loss 0.515419, acc 0.76
2016-09-07T22:26:18.055164: step 347, loss 0.255057, acc 0.92
2016-09-07T22:26:18.755573: step 348, loss 0.256804, acc 0.92
2016-09-07T22:26:19.455511: step 349, loss 0.410188, acc 0.8
2016-09-07T22:26:20.148697: step 350, loss 0.305749, acc 0.84
2016-09-07T22:26:20.840147: step 351, loss 0.534054, acc 0.74
2016-09-07T22:26:21.534772: step 352, loss 0.385441, acc 0.78
2016-09-07T22:26:22.231521: step 353, loss 0.363016, acc 0.84
2016-09-07T22:26:22.909282: step 354, loss 0.373593, acc 0.84
2016-09-07T22:26:23.591556: step 355, loss 0.337431, acc 0.86
2016-09-07T22:26:24.300208: step 356, loss 0.311403, acc 0.84
2016-09-07T22:26:25.023244: step 357, loss 0.513022, acc 0.78
2016-09-07T22:26:25.714886: step 358, loss 0.332965, acc 0.78
2016-09-07T22:26:26.403102: step 359, loss 0.402861, acc 0.8
2016-09-07T22:26:27.121110: step 360, loss 0.540522, acc 0.8
2016-09-07T22:26:27.809476: step 361, loss 0.350626, acc 0.86
2016-09-07T22:26:28.507426: step 362, loss 0.367464, acc 0.84
2016-09-07T22:26:29.215058: step 363, loss 0.282773, acc 0.88
2016-09-07T22:26:29.914005: step 364, loss 0.344558, acc 0.84
2016-09-07T22:26:30.601627: step 365, loss 0.507055, acc 0.74
2016-09-07T22:26:31.293760: step 366, loss 0.360107, acc 0.82
2016-09-07T22:26:31.982541: step 367, loss 0.314474, acc 0.9
2016-09-07T22:26:32.677457: step 368, loss 0.37993, acc 0.86
2016-09-07T22:26:33.369506: step 369, loss 0.392906, acc 0.8
2016-09-07T22:26:34.069799: step 370, loss 0.395972, acc 0.8
2016-09-07T22:26:34.771037: step 371, loss 0.421981, acc 0.8
2016-09-07T22:26:35.471878: step 372, loss 0.368312, acc 0.84
2016-09-07T22:26:36.195689: step 373, loss 0.530201, acc 0.76
2016-09-07T22:26:36.882329: step 374, loss 0.452604, acc 0.86
2016-09-07T22:26:37.592935: step 375, loss 0.324107, acc 0.86
2016-09-07T22:26:38.295917: step 376, loss 0.308562, acc 0.86
2016-09-07T22:26:38.991016: step 377, loss 0.522985, acc 0.72
2016-09-07T22:26:39.707909: step 378, loss 0.318963, acc 0.88
2016-09-07T22:26:40.410733: step 379, loss 0.357341, acc 0.86
2016-09-07T22:26:41.104369: step 380, loss 0.509506, acc 0.78
2016-09-07T22:26:41.799958: step 381, loss 0.422336, acc 0.8
2016-09-07T22:26:42.500423: step 382, loss 0.375425, acc 0.78
2016-09-07T22:26:43.191158: step 383, loss 0.443882, acc 0.82
2016-09-07T22:26:43.898827: step 384, loss 0.447016, acc 0.76
2016-09-07T22:26:44.619046: step 385, loss 0.378326, acc 0.86
2016-09-07T22:26:45.314666: step 386, loss 0.361587, acc 0.82
2016-09-07T22:26:46.010374: step 387, loss 0.475082, acc 0.78
2016-09-07T22:26:46.382084: step 388, loss 0.399518, acc 0.833333
2016-09-07T22:26:47.082875: step 389, loss 0.327853, acc 0.88
2016-09-07T22:26:47.765800: step 390, loss 0.251135, acc 0.86
2016-09-07T22:26:48.453536: step 391, loss 0.279571, acc 0.9
2016-09-07T22:26:49.131211: step 392, loss 0.189937, acc 0.9
2016-09-07T22:26:49.824588: step 393, loss 0.230561, acc 0.9
2016-09-07T22:26:50.506916: step 394, loss 0.316643, acc 0.86
2016-09-07T22:26:51.210036: step 395, loss 0.224484, acc 0.94
2016-09-07T22:26:51.903651: step 396, loss 0.193129, acc 0.9
2016-09-07T22:26:52.597106: step 397, loss 0.20892, acc 0.9
2016-09-07T22:26:53.290733: step 398, loss 0.211998, acc 0.9
2016-09-07T22:26:53.980843: step 399, loss 0.215086, acc 0.9
2016-09-07T22:26:54.670318: step 400, loss 0.133951, acc 0.96

Evaluation:
2016-09-07T22:26:57.740761: step 400, loss 0.61562, acc 0.789

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-400

2016-09-07T22:26:59.346621: step 401, loss 0.191803, acc 0.94
2016-09-07T22:27:00.053726: step 402, loss 0.752689, acc 0.8
2016-09-07T22:27:00.793822: step 403, loss 0.257997, acc 0.94
2016-09-07T22:27:01.468595: step 404, loss 0.141649, acc 0.92
2016-09-07T22:27:02.156875: step 405, loss 0.24033, acc 0.9
2016-09-07T22:27:02.853241: step 406, loss 0.268681, acc 0.9
2016-09-07T22:27:03.540069: step 407, loss 0.0500349, acc 1
2016-09-07T22:27:04.231012: step 408, loss 0.269016, acc 0.9
2016-09-07T22:27:04.932802: step 409, loss 0.482209, acc 0.82
2016-09-07T22:27:05.631064: step 410, loss 0.182619, acc 0.92
2016-09-07T22:27:06.339963: step 411, loss 0.309077, acc 0.92
2016-09-07T22:27:07.057120: step 412, loss 0.121027, acc 0.94
2016-09-07T22:27:07.766666: step 413, loss 0.265086, acc 0.86
2016-09-07T22:27:08.464774: step 414, loss 0.391093, acc 0.82
2016-09-07T22:27:09.154057: step 415, loss 0.316432, acc 0.86
2016-09-07T22:27:09.867599: step 416, loss 0.186266, acc 0.94
2016-09-07T22:27:10.563863: step 417, loss 0.178532, acc 0.92
2016-09-07T22:27:11.287892: step 418, loss 0.278758, acc 0.84
2016-09-07T22:27:12.021718: step 419, loss 0.112921, acc 0.98
2016-09-07T22:27:12.714988: step 420, loss 0.225476, acc 0.88
2016-09-07T22:27:13.395795: step 421, loss 0.371137, acc 0.84
2016-09-07T22:27:14.086563: step 422, loss 0.221813, acc 0.9
2016-09-07T22:27:14.764897: step 423, loss 0.2848, acc 0.88
2016-09-07T22:27:15.455109: step 424, loss 0.331034, acc 0.82
2016-09-07T22:27:16.163312: step 425, loss 0.140971, acc 1
2016-09-07T22:27:16.868590: step 426, loss 0.169744, acc 0.96
2016-09-07T22:27:17.578757: step 427, loss 0.29947, acc 0.88
2016-09-07T22:27:18.290418: step 428, loss 0.339414, acc 0.8
2016-09-07T22:27:18.990925: step 429, loss 0.173368, acc 0.94
2016-09-07T22:27:19.669725: step 430, loss 0.142049, acc 0.98
2016-09-07T22:27:20.359089: step 431, loss 0.34861, acc 0.86
2016-09-07T22:27:21.074704: step 432, loss 0.232079, acc 0.92
2016-09-07T22:27:21.788237: step 433, loss 0.176093, acc 0.96
2016-09-07T22:27:22.490730: step 434, loss 0.363591, acc 0.9
2016-09-07T22:27:23.185988: step 435, loss 0.310123, acc 0.84
2016-09-07T22:27:23.878897: step 436, loss 0.34334, acc 0.88
2016-09-07T22:27:24.593789: step 437, loss 0.145183, acc 0.94
2016-09-07T22:27:25.282766: step 438, loss 0.229653, acc 0.9
2016-09-07T22:27:25.978470: step 439, loss 0.121902, acc 0.94
2016-09-07T22:27:26.677613: step 440, loss 0.246898, acc 0.9
2016-09-07T22:27:27.388074: step 441, loss 0.232467, acc 0.9
2016-09-07T22:27:28.119279: step 442, loss 0.146264, acc 0.96
2016-09-07T22:27:28.820559: step 443, loss 0.277182, acc 0.92
2016-09-07T22:27:29.519331: step 444, loss 0.291272, acc 0.84
2016-09-07T22:27:30.207616: step 445, loss 0.261263, acc 0.92
2016-09-07T22:27:30.887947: step 446, loss 0.220551, acc 0.92
2016-09-07T22:27:31.574879: step 447, loss 0.269787, acc 0.86
2016-09-07T22:27:32.277452: step 448, loss 0.117187, acc 0.98
2016-09-07T22:27:33.029969: step 449, loss 0.126203, acc 0.96
2016-09-07T22:27:33.724084: step 450, loss 0.292693, acc 0.88
2016-09-07T22:27:34.420017: step 451, loss 0.202878, acc 0.92
2016-09-07T22:27:35.122856: step 452, loss 0.271693, acc 0.9
2016-09-07T22:27:35.823029: step 453, loss 0.218285, acc 0.92
2016-09-07T22:27:36.549008: step 454, loss 0.241684, acc 0.88
2016-09-07T22:27:37.243393: step 455, loss 0.351596, acc 0.84
2016-09-07T22:27:37.953112: step 456, loss 0.164748, acc 0.96
2016-09-07T22:27:38.644996: step 457, loss 0.214527, acc 0.86
2016-09-07T22:27:39.343708: step 458, loss 0.284771, acc 0.9
2016-09-07T22:27:40.024869: step 459, loss 0.330419, acc 0.82
2016-09-07T22:27:40.729783: step 460, loss 0.241585, acc 0.9
2016-09-07T22:27:41.439239: step 461, loss 0.210671, acc 0.94
2016-09-07T22:27:42.140832: step 462, loss 0.330479, acc 0.84
2016-09-07T22:27:42.857577: step 463, loss 0.230846, acc 0.94
2016-09-07T22:27:43.548954: step 464, loss 0.351933, acc 0.8
2016-09-07T22:27:44.239421: step 465, loss 0.29814, acc 0.84
2016-09-07T22:27:44.952962: step 466, loss 0.166224, acc 0.94
2016-09-07T22:27:45.645162: step 467, loss 0.244831, acc 0.92
2016-09-07T22:27:46.358974: step 468, loss 0.173406, acc 0.92
2016-09-07T22:27:47.042768: step 469, loss 0.232074, acc 0.9
2016-09-07T22:27:47.735536: step 470, loss 0.316289, acc 0.86
2016-09-07T22:27:48.437626: step 471, loss 0.320272, acc 0.86
2016-09-07T22:27:49.110207: step 472, loss 0.30724, acc 0.9
2016-09-07T22:27:49.791511: step 473, loss 0.22582, acc 0.9
2016-09-07T22:27:50.498792: step 474, loss 0.333531, acc 0.82
2016-09-07T22:27:51.204193: step 475, loss 0.152164, acc 0.94
2016-09-07T22:27:51.912507: step 476, loss 0.163202, acc 0.9
2016-09-07T22:27:52.613106: step 477, loss 0.280777, acc 0.9
2016-09-07T22:27:53.304672: step 478, loss 0.243192, acc 0.94
2016-09-07T22:27:54.012309: step 479, loss 0.327852, acc 0.88
2016-09-07T22:27:54.699323: step 480, loss 0.215749, acc 0.9
2016-09-07T22:27:55.409531: step 481, loss 0.167521, acc 0.92
2016-09-07T22:27:56.108794: step 482, loss 0.165295, acc 0.96
2016-09-07T22:27:56.789695: step 483, loss 0.224924, acc 0.9
2016-09-07T22:27:57.467879: step 484, loss 0.175433, acc 0.92
2016-09-07T22:27:58.170180: step 485, loss 0.369458, acc 0.82
2016-09-07T22:27:58.870499: step 486, loss 0.236241, acc 0.9
2016-09-07T22:27:59.570368: step 487, loss 0.151133, acc 0.94
2016-09-07T22:28:00.259518: step 488, loss 0.369935, acc 0.84
2016-09-07T22:28:00.944786: step 489, loss 0.402697, acc 0.86
2016-09-07T22:28:01.627441: step 490, loss 0.26012, acc 0.9
2016-09-07T22:28:02.315389: step 491, loss 0.184737, acc 0.92
2016-09-07T22:28:03.019099: step 492, loss 0.165679, acc 0.96
2016-09-07T22:28:03.713231: step 493, loss 0.113897, acc 0.94
2016-09-07T22:28:04.402881: step 494, loss 0.302341, acc 0.94
2016-09-07T22:28:05.114236: step 495, loss 0.347821, acc 0.84
2016-09-07T22:28:05.815093: step 496, loss 0.218749, acc 0.9
2016-09-07T22:28:06.522363: step 497, loss 0.173437, acc 0.92
2016-09-07T22:28:07.227595: step 498, loss 0.377211, acc 0.84
2016-09-07T22:28:07.944128: step 499, loss 0.302168, acc 0.84
2016-09-07T22:28:08.636619: step 500, loss 0.238621, acc 0.9

Evaluation:
2016-09-07T22:28:11.714056: step 500, loss 0.499257, acc 0.793

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-500

2016-09-07T22:28:13.435565: step 501, loss 0.305626, acc 0.92
2016-09-07T22:28:14.127337: step 502, loss 0.246407, acc 0.88
2016-09-07T22:28:14.834588: step 503, loss 0.192048, acc 0.92
2016-09-07T22:28:15.555036: step 504, loss 0.25345, acc 0.84
2016-09-07T22:28:16.252262: step 505, loss 0.252572, acc 0.88
2016-09-07T22:28:16.961265: step 506, loss 0.213034, acc 0.9
2016-09-07T22:28:17.661164: step 507, loss 0.20031, acc 0.96
2016-09-07T22:28:18.354038: step 508, loss 0.1959, acc 0.9
2016-09-07T22:28:19.048682: step 509, loss 0.186296, acc 0.92
2016-09-07T22:28:19.751836: step 510, loss 0.366092, acc 0.82
2016-09-07T22:28:20.451399: step 511, loss 0.186875, acc 0.9
2016-09-07T22:28:21.169808: step 512, loss 0.211326, acc 0.94
2016-09-07T22:28:21.850584: step 513, loss 0.250418, acc 0.92
2016-09-07T22:28:22.554840: step 514, loss 0.393652, acc 0.8
2016-09-07T22:28:23.257215: step 515, loss 0.169747, acc 0.94
2016-09-07T22:28:23.957085: step 516, loss 0.234751, acc 0.9
2016-09-07T22:28:24.651813: step 517, loss 0.203377, acc 0.92
2016-09-07T22:28:25.357478: step 518, loss 0.356141, acc 0.9
2016-09-07T22:28:26.060540: step 519, loss 0.330619, acc 0.84
2016-09-07T22:28:26.762290: step 520, loss 0.400917, acc 0.82
2016-09-07T22:28:27.464301: step 521, loss 0.240193, acc 0.9
2016-09-07T22:28:28.164981: step 522, loss 0.211456, acc 0.88
2016-09-07T22:28:28.858316: step 523, loss 0.314944, acc 0.84
2016-09-07T22:28:29.559101: step 524, loss 0.365219, acc 0.84
2016-09-07T22:28:30.266606: step 525, loss 0.274609, acc 0.88
2016-09-07T22:28:30.957346: step 526, loss 0.317189, acc 0.86
2016-09-07T22:28:31.657949: step 527, loss 0.22447, acc 0.9
2016-09-07T22:28:32.359140: step 528, loss 0.248445, acc 0.92
2016-09-07T22:28:33.081049: step 529, loss 0.338872, acc 0.84
2016-09-07T22:28:33.783667: step 530, loss 0.201216, acc 0.92
2016-09-07T22:28:34.468234: step 531, loss 0.277374, acc 0.88
2016-09-07T22:28:35.168847: step 532, loss 0.235059, acc 0.9
2016-09-07T22:28:35.861052: step 533, loss 0.336558, acc 0.82
2016-09-07T22:28:36.570571: step 534, loss 0.324447, acc 0.82
2016-09-07T22:28:37.255888: step 535, loss 0.198776, acc 0.88
2016-09-07T22:28:37.953916: step 536, loss 0.193112, acc 0.92
2016-09-07T22:28:38.644831: step 537, loss 0.194434, acc 0.94
2016-09-07T22:28:39.340551: step 538, loss 0.318256, acc 0.86
2016-09-07T22:28:40.051826: step 539, loss 0.236318, acc 0.9
2016-09-07T22:28:40.769452: step 540, loss 0.262008, acc 0.9
2016-09-07T22:28:41.479363: step 541, loss 0.39541, acc 0.86
2016-09-07T22:28:42.175281: step 542, loss 0.269804, acc 0.84
2016-09-07T22:28:42.867460: step 543, loss 0.281233, acc 0.86
2016-09-07T22:28:43.559881: step 544, loss 0.222245, acc 0.94
2016-09-07T22:28:44.258654: step 545, loss 0.171321, acc 0.94
2016-09-07T22:28:44.945034: step 546, loss 0.195807, acc 0.92
2016-09-07T22:28:45.645838: step 547, loss 0.242584, acc 0.88
2016-09-07T22:28:46.348379: step 548, loss 0.310842, acc 0.9
2016-09-07T22:28:47.035046: step 549, loss 0.257916, acc 0.84
2016-09-07T22:28:47.716831: step 550, loss 0.164676, acc 0.94
2016-09-07T22:28:48.417692: step 551, loss 0.210545, acc 0.86
2016-09-07T22:28:49.114499: step 552, loss 0.318997, acc 0.88
2016-09-07T22:28:49.811568: step 553, loss 0.279696, acc 0.9
2016-09-07T22:28:50.494248: step 554, loss 0.170503, acc 0.94
2016-09-07T22:28:51.194822: step 555, loss 0.241306, acc 0.88
2016-09-07T22:28:51.881876: step 556, loss 0.225961, acc 0.88
2016-09-07T22:28:52.602056: step 557, loss 0.348365, acc 0.84
2016-09-07T22:28:53.298115: step 558, loss 0.298924, acc 0.82
2016-09-07T22:28:53.999039: step 559, loss 0.254734, acc 0.84
2016-09-07T22:28:54.691240: step 560, loss 0.280838, acc 0.9
2016-09-07T22:28:55.413134: step 561, loss 0.229597, acc 0.9
2016-09-07T22:28:56.129049: step 562, loss 0.126756, acc 0.96
2016-09-07T22:28:56.842993: step 563, loss 0.366751, acc 0.86
2016-09-07T22:28:57.539757: step 564, loss 0.410236, acc 0.82
2016-09-07T22:28:58.225914: step 565, loss 0.15421, acc 0.96
2016-09-07T22:28:58.925636: step 566, loss 0.526847, acc 0.74
2016-09-07T22:28:59.606927: step 567, loss 0.315929, acc 0.88
2016-09-07T22:29:00.320189: step 568, loss 0.18521, acc 0.92
2016-09-07T22:29:01.006805: step 569, loss 0.368678, acc 0.82
2016-09-07T22:29:01.693973: step 570, loss 0.209459, acc 0.96
2016-09-07T22:29:02.380510: step 571, loss 0.193948, acc 0.92
2016-09-07T22:29:03.069100: step 572, loss 0.323977, acc 0.86
2016-09-07T22:29:03.766792: step 573, loss 0.271511, acc 0.88
2016-09-07T22:29:04.478190: step 574, loss 0.200661, acc 0.94
2016-09-07T22:29:05.175416: step 575, loss 0.22498, acc 0.94
2016-09-07T22:29:05.864903: step 576, loss 0.211824, acc 0.92
2016-09-07T22:29:06.575434: step 577, loss 0.261669, acc 0.84
2016-09-07T22:29:07.267935: step 578, loss 0.364778, acc 0.84
2016-09-07T22:29:07.989718: step 579, loss 0.236277, acc 0.9
2016-09-07T22:29:08.686789: step 580, loss 0.191858, acc 0.88
2016-09-07T22:29:09.402538: step 581, loss 0.21631, acc 0.94
2016-09-07T22:29:09.780817: step 582, loss 0.270907, acc 0.833333
2016-09-07T22:29:10.494623: step 583, loss 0.075879, acc 1
2016-09-07T22:29:11.211325: step 584, loss 0.160612, acc 0.96
2016-09-07T22:29:11.923190: step 585, loss 0.32445, acc 0.86
2016-09-07T22:29:12.628959: step 586, loss 0.167186, acc 0.94
2016-09-07T22:29:13.345536: step 587, loss 0.160764, acc 0.94
2016-09-07T22:29:14.034379: step 588, loss 0.266132, acc 0.88
2016-09-07T22:29:14.729828: step 589, loss 0.106213, acc 0.94
2016-09-07T22:29:15.431319: step 590, loss 0.166815, acc 0.94
2016-09-07T22:29:16.125329: step 591, loss 0.063276, acc 0.96
2016-09-07T22:29:16.822058: step 592, loss 0.281958, acc 0.88
2016-09-07T22:29:17.532946: step 593, loss 0.0675866, acc 1
2016-09-07T22:29:18.252842: step 594, loss 0.0418702, acc 1
2016-09-07T22:29:18.970336: step 595, loss 0.261789, acc 0.88
2016-09-07T22:29:19.672648: step 596, loss 0.0789084, acc 0.98
2016-09-07T22:29:20.401096: step 597, loss 0.100982, acc 0.96
2016-09-07T22:29:21.076139: step 598, loss 0.290546, acc 0.9
2016-09-07T22:29:21.788660: step 599, loss 0.175986, acc 0.92
2016-09-07T22:29:22.498946: step 600, loss 0.022715, acc 1

Evaluation:
2016-09-07T22:29:25.631843: step 600, loss 0.672442, acc 0.784

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-600

2016-09-07T22:29:27.254752: step 601, loss 0.0810653, acc 0.98
2016-09-07T22:29:27.942571: step 602, loss 0.12752, acc 0.94
2016-09-07T22:29:28.633848: step 603, loss 0.100302, acc 0.96
2016-09-07T22:29:29.328469: step 604, loss 0.185475, acc 0.94
2016-09-07T22:29:30.025415: step 605, loss 0.231791, acc 0.94
2016-09-07T22:29:30.712630: step 606, loss 0.144615, acc 0.94
2016-09-07T22:29:31.415589: step 607, loss 0.160986, acc 0.92
2016-09-07T22:29:32.104095: step 608, loss 0.0803162, acc 0.98
2016-09-07T22:29:32.810200: step 609, loss 0.127534, acc 0.94
2016-09-07T22:29:33.508398: step 610, loss 0.0850108, acc 0.96
2016-09-07T22:29:34.195917: step 611, loss 0.131276, acc 0.96
2016-09-07T22:29:34.892534: step 612, loss 0.0663659, acc 0.98
2016-09-07T22:29:35.592430: step 613, loss 0.185152, acc 0.88
2016-09-07T22:29:36.283884: step 614, loss 0.153335, acc 0.92
2016-09-07T22:29:36.982950: step 615, loss 0.145465, acc 0.94
2016-09-07T22:29:37.684673: step 616, loss 0.124536, acc 0.96
2016-09-07T22:29:38.397986: step 617, loss 0.0549116, acc 0.98
2016-09-07T22:29:39.097962: step 618, loss 0.192291, acc 0.92
2016-09-07T22:29:39.794616: step 619, loss 0.130741, acc 0.92
2016-09-07T22:29:40.497380: step 620, loss 0.0762796, acc 0.96
2016-09-07T22:29:41.195829: step 621, loss 0.109737, acc 0.98
2016-09-07T22:29:41.922543: step 622, loss 0.0595855, acc 0.98
2016-09-07T22:29:42.620562: step 623, loss 0.152508, acc 0.92
2016-09-07T22:29:43.314020: step 624, loss 0.175388, acc 0.92
2016-09-07T22:29:43.995832: step 625, loss 0.0625813, acc 0.98
2016-09-07T22:29:44.677619: step 626, loss 0.329643, acc 0.9
2016-09-07T22:29:45.401565: step 627, loss 0.268016, acc 0.9
2016-09-07T22:29:46.105689: step 628, loss 0.158581, acc 0.94
2016-09-07T22:29:46.806992: step 629, loss 0.259155, acc 0.84
2016-09-07T22:29:47.502169: step 630, loss 0.116276, acc 0.92
2016-09-07T22:29:48.198110: step 631, loss 0.116543, acc 0.94
2016-09-07T22:29:48.919237: step 632, loss 0.147771, acc 0.92
2016-09-07T22:29:49.614628: step 633, loss 0.15567, acc 0.92
2016-09-07T22:29:50.308627: step 634, loss 0.144562, acc 0.92
2016-09-07T22:29:50.991354: step 635, loss 0.141975, acc 0.92
2016-09-07T22:29:51.688116: step 636, loss 0.401091, acc 0.84
2016-09-07T22:29:52.383511: step 637, loss 0.206965, acc 0.92
2016-09-07T22:29:53.056456: step 638, loss 0.188323, acc 0.9
2016-09-07T22:29:53.752583: step 639, loss 0.277947, acc 0.9
2016-09-07T22:29:54.438253: step 640, loss 0.188529, acc 0.94
2016-09-07T22:29:55.133021: step 641, loss 0.25972, acc 0.88
2016-09-07T22:29:55.851105: step 642, loss 0.202839, acc 0.94
2016-09-07T22:29:56.549044: step 643, loss 0.20014, acc 0.96
2016-09-07T22:29:57.267506: step 644, loss 0.153381, acc 0.92
2016-09-07T22:29:57.988288: step 645, loss 0.185284, acc 0.94
2016-09-07T22:29:58.695570: step 646, loss 0.265818, acc 0.9
2016-09-07T22:29:59.380514: step 647, loss 0.132234, acc 0.92
2016-09-07T22:30:00.087307: step 648, loss 0.208054, acc 0.96
2016-09-07T22:30:00.842086: step 649, loss 0.121689, acc 0.96
2016-09-07T22:30:01.552642: step 650, loss 0.110566, acc 0.96
2016-09-07T22:30:02.266260: step 651, loss 0.118874, acc 0.96
2016-09-07T22:30:02.953852: step 652, loss 0.0857909, acc 0.98
2016-09-07T22:30:03.669397: step 653, loss 0.155825, acc 0.94
2016-09-07T22:30:04.349357: step 654, loss 0.213305, acc 0.9
2016-09-07T22:30:05.045403: step 655, loss 0.175191, acc 0.92
2016-09-07T22:30:05.728395: step 656, loss 0.155601, acc 0.94
2016-09-07T22:30:06.438943: step 657, loss 0.176745, acc 0.9
2016-09-07T22:30:07.125876: step 658, loss 0.234812, acc 0.88
2016-09-07T22:30:07.837472: step 659, loss 0.203671, acc 0.92
2016-09-07T22:30:08.550457: step 660, loss 0.164143, acc 0.96
2016-09-07T22:30:09.252293: step 661, loss 0.228604, acc 0.88
2016-09-07T22:30:09.934239: step 662, loss 0.209533, acc 0.9
2016-09-07T22:30:10.618263: step 663, loss 0.157753, acc 0.92
2016-09-07T22:30:11.334498: step 664, loss 0.232519, acc 0.88
2016-09-07T22:30:12.024944: step 665, loss 0.176672, acc 0.94
2016-09-07T22:30:12.728688: step 666, loss 0.141336, acc 0.94
2016-09-07T22:30:13.421053: step 667, loss 0.14226, acc 0.92
2016-09-07T22:30:14.121796: step 668, loss 0.253552, acc 0.88
2016-09-07T22:30:14.814775: step 669, loss 0.237185, acc 0.92
2016-09-07T22:30:15.503594: step 670, loss 0.0415442, acc 1
2016-09-07T22:30:16.232678: step 671, loss 0.11068, acc 0.94
2016-09-07T22:30:16.955975: step 672, loss 0.190336, acc 0.86
2016-09-07T22:30:17.666452: step 673, loss 0.202177, acc 0.88
2016-09-07T22:30:18.385177: step 674, loss 0.278693, acc 0.9
2016-09-07T22:30:19.085106: step 675, loss 0.268327, acc 0.9
2016-09-07T22:30:19.805605: step 676, loss 0.114777, acc 0.94
2016-09-07T22:30:20.503793: step 677, loss 0.203069, acc 0.88
2016-09-07T22:30:21.185370: step 678, loss 0.22028, acc 0.88
2016-09-07T22:30:21.896882: step 679, loss 0.0953524, acc 0.98
2016-09-07T22:30:22.588741: step 680, loss 0.198258, acc 0.88
2016-09-07T22:30:23.287253: step 681, loss 0.123953, acc 0.94
2016-09-07T22:30:23.995766: step 682, loss 0.185952, acc 0.9
2016-09-07T22:30:24.682365: step 683, loss 0.199564, acc 0.88
2016-09-07T22:30:25.377016: step 684, loss 0.212576, acc 0.92
2016-09-07T22:30:26.071840: step 685, loss 0.213081, acc 0.92
2016-09-07T22:30:26.755428: step 686, loss 0.302149, acc 0.86
2016-09-07T22:30:27.454497: step 687, loss 0.150781, acc 0.92
2016-09-07T22:30:28.159441: step 688, loss 0.192792, acc 0.92
2016-09-07T22:30:28.861331: step 689, loss 0.211182, acc 0.86
2016-09-07T22:30:29.554701: step 690, loss 0.199532, acc 0.88
2016-09-07T22:30:30.262258: step 691, loss 0.284678, acc 0.82
2016-09-07T22:30:30.961519: step 692, loss 0.197288, acc 0.92
2016-09-07T22:30:31.663605: step 693, loss 0.195329, acc 0.92
2016-09-07T22:30:32.361132: step 694, loss 0.0934584, acc 0.98
2016-09-07T22:30:33.070524: step 695, loss 0.156326, acc 0.94
2016-09-07T22:30:33.771568: step 696, loss 0.267132, acc 0.92
2016-09-07T22:30:34.475712: step 697, loss 0.134585, acc 0.92
2016-09-07T22:30:35.180691: step 698, loss 0.250813, acc 0.9
2016-09-07T22:30:35.876833: step 699, loss 0.232024, acc 0.92
2016-09-07T22:30:36.568673: step 700, loss 0.216128, acc 0.9

Evaluation:
2016-09-07T22:30:39.647484: step 700, loss 0.602589, acc 0.783

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-700

2016-09-07T22:30:41.327280: step 701, loss 0.240152, acc 0.9
2016-09-07T22:30:42.024221: step 702, loss 0.145967, acc 0.94
2016-09-07T22:30:42.732496: step 703, loss 0.109495, acc 0.96
2016-09-07T22:30:43.436845: step 704, loss 0.162056, acc 0.94
2016-09-07T22:30:44.171235: step 705, loss 0.141541, acc 0.94
2016-09-07T22:30:44.873618: step 706, loss 0.16107, acc 0.92
2016-09-07T22:30:45.594617: step 707, loss 0.139045, acc 0.9
2016-09-07T22:30:46.312436: step 708, loss 0.135577, acc 0.98
2016-09-07T22:30:47.004722: step 709, loss 0.140122, acc 0.98
2016-09-07T22:30:47.701295: step 710, loss 0.0876598, acc 0.96
2016-09-07T22:30:48.374290: step 711, loss 0.1159, acc 0.96
2016-09-07T22:30:49.082507: step 712, loss 0.130835, acc 0.92
2016-09-07T22:30:49.774163: step 713, loss 0.122178, acc 0.98
2016-09-07T22:30:50.469767: step 714, loss 0.12453, acc 0.92
2016-09-07T22:30:51.170267: step 715, loss 0.0926029, acc 0.98
2016-09-07T22:30:51.863496: step 716, loss 0.216103, acc 0.9
2016-09-07T22:30:52.574933: step 717, loss 0.347708, acc 0.88
2016-09-07T22:30:53.276081: step 718, loss 0.16404, acc 0.94
2016-09-07T22:30:53.977669: step 719, loss 0.192566, acc 0.92
2016-09-07T22:30:54.668660: step 720, loss 0.184711, acc 0.9
2016-09-07T22:30:55.358670: step 721, loss 0.18963, acc 0.88
2016-09-07T22:30:56.051794: step 722, loss 0.33193, acc 0.88
2016-09-07T22:30:56.746235: step 723, loss 0.225568, acc 0.88
2016-09-07T22:30:57.446281: step 724, loss 0.148499, acc 0.94
2016-09-07T22:30:58.143847: step 725, loss 0.134398, acc 0.94
2016-09-07T22:30:58.842705: step 726, loss 0.0815391, acc 0.98
2016-09-07T22:30:59.544827: step 727, loss 0.189212, acc 0.94
2016-09-07T22:31:00.284210: step 728, loss 0.280982, acc 0.86
2016-09-07T22:31:00.984480: step 729, loss 0.144556, acc 0.94
2016-09-07T22:31:01.682050: step 730, loss 0.121673, acc 0.92
2016-09-07T22:31:02.407424: step 731, loss 0.0853268, acc 0.98
2016-09-07T22:31:03.122848: step 732, loss 0.222534, acc 0.94
2016-09-07T22:31:03.820567: step 733, loss 0.207032, acc 0.92
2016-09-07T22:31:04.545678: step 734, loss 0.156825, acc 0.94
2016-09-07T22:31:05.238982: step 735, loss 0.200883, acc 0.94
2016-09-07T22:31:05.942968: step 736, loss 0.382267, acc 0.86
2016-09-07T22:31:06.656408: step 737, loss 0.168251, acc 0.9
2016-09-07T22:31:07.355701: step 738, loss 0.166576, acc 0.94
2016-09-07T22:31:08.052187: step 739, loss 0.155742, acc 0.94
2016-09-07T22:31:08.748952: step 740, loss 0.191329, acc 0.94
2016-09-07T22:31:09.454048: step 741, loss 0.188011, acc 0.94
2016-09-07T22:31:10.175535: step 742, loss 0.190817, acc 0.9
2016-09-07T22:31:10.855947: step 743, loss 0.155334, acc 0.94
2016-09-07T22:31:11.534843: step 744, loss 0.389087, acc 0.86
2016-09-07T22:31:12.235093: step 745, loss 0.184719, acc 0.92
2016-09-07T22:31:12.934792: step 746, loss 0.162153, acc 0.9
2016-09-07T22:31:13.644115: step 747, loss 0.255513, acc 0.84
2016-09-07T22:31:14.356192: step 748, loss 0.176233, acc 0.9
2016-09-07T22:31:15.070058: step 749, loss 0.157009, acc 0.94
2016-09-07T22:31:15.777759: step 750, loss 0.115253, acc 0.96
2016-09-07T22:31:16.483114: step 751, loss 0.305475, acc 0.82
2016-09-07T22:31:17.178315: step 752, loss 0.232845, acc 0.88
2016-09-07T22:31:17.871698: step 753, loss 0.210561, acc 0.94
2016-09-07T22:31:18.579970: step 754, loss 0.310919, acc 0.84
2016-09-07T22:31:19.290096: step 755, loss 0.138839, acc 0.94
2016-09-07T22:31:19.988059: step 756, loss 0.131764, acc 0.96
2016-09-07T22:31:20.697210: step 757, loss 0.240304, acc 0.86
2016-09-07T22:31:21.405132: step 758, loss 0.115394, acc 0.96
2016-09-07T22:31:22.095592: step 759, loss 0.281899, acc 0.86
2016-09-07T22:31:22.787642: step 760, loss 0.142205, acc 0.94
2016-09-07T22:31:23.476007: step 761, loss 0.316393, acc 0.86
2016-09-07T22:31:24.182293: step 762, loss 0.217705, acc 0.88
2016-09-07T22:31:24.889220: step 763, loss 0.113649, acc 0.94
2016-09-07T22:31:25.580957: step 764, loss 0.070127, acc 1
2016-09-07T22:31:26.287804: step 765, loss 0.110215, acc 0.96
2016-09-07T22:31:26.978283: step 766, loss 0.14302, acc 0.88
2016-09-07T22:31:27.684724: step 767, loss 0.367733, acc 0.84
2016-09-07T22:31:28.397581: step 768, loss 0.183242, acc 0.9
2016-09-07T22:31:29.115465: step 769, loss 0.295253, acc 0.86
2016-09-07T22:31:29.815986: step 770, loss 0.155598, acc 0.92
2016-09-07T22:31:30.512239: step 771, loss 0.262535, acc 0.9
2016-09-07T22:31:31.234034: step 772, loss 0.0841351, acc 0.96
2016-09-07T22:31:31.923681: step 773, loss 0.209779, acc 0.88
2016-09-07T22:31:32.614221: step 774, loss 0.261569, acc 0.9
2016-09-07T22:31:33.322748: step 775, loss 0.219796, acc 0.94
2016-09-07T22:31:33.709429: step 776, loss 0.0970403, acc 0.916667
2016-09-07T22:31:34.419247: step 777, loss 0.0969269, acc 0.96
2016-09-07T22:31:35.129995: step 778, loss 0.121705, acc 0.94
2016-09-07T22:31:35.828112: step 779, loss 0.208153, acc 0.88
2016-09-07T22:31:36.533989: step 780, loss 0.0719037, acc 0.98
2016-09-07T22:31:37.234337: step 781, loss 0.102768, acc 0.98
2016-09-07T22:31:37.944690: step 782, loss 0.114096, acc 0.94
2016-09-07T22:31:38.652875: step 783, loss 0.125916, acc 0.98
2016-09-07T22:31:39.360802: step 784, loss 0.263147, acc 0.92
2016-09-07T22:31:40.085883: step 785, loss 0.196171, acc 0.9
2016-09-07T22:31:40.781537: step 786, loss 0.0818479, acc 0.98
2016-09-07T22:31:41.489298: step 787, loss 0.0904889, acc 0.96
2016-09-07T22:31:42.220035: step 788, loss 0.244446, acc 0.94
2016-09-07T22:31:42.914366: step 789, loss 0.0311186, acc 0.98
2016-09-07T22:31:43.611253: step 790, loss 0.056576, acc 1
2016-09-07T22:31:44.295732: step 791, loss 0.125567, acc 0.96
2016-09-07T22:31:45.001453: step 792, loss 0.104613, acc 0.96
2016-09-07T22:31:45.680884: step 793, loss 0.13385, acc 0.94
2016-09-07T22:31:46.368413: step 794, loss 0.138395, acc 0.98
2016-09-07T22:31:47.067231: step 795, loss 0.123956, acc 0.94
2016-09-07T22:31:47.760222: step 796, loss 0.124933, acc 0.96
2016-09-07T22:31:48.457509: step 797, loss 0.215065, acc 0.9
2016-09-07T22:31:49.164083: step 798, loss 0.0935505, acc 0.96
2016-09-07T22:31:49.862664: step 799, loss 0.124661, acc 0.96
2016-09-07T22:31:50.565163: step 800, loss 0.127205, acc 0.92

Evaluation:
2016-09-07T22:31:53.677713: step 800, loss 0.68061, acc 0.789

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-800

2016-09-07T22:31:55.384270: step 801, loss 0.081417, acc 0.98
2016-09-07T22:31:56.087047: step 802, loss 0.0595497, acc 0.96
2016-09-07T22:31:56.757917: step 803, loss 0.0644131, acc 0.98
2016-09-07T22:31:57.463867: step 804, loss 0.056176, acc 0.98
2016-09-07T22:31:58.166723: step 805, loss 0.121606, acc 0.92
2016-09-07T22:31:58.860502: step 806, loss 0.0802744, acc 0.94
2016-09-07T22:31:59.553539: step 807, loss 0.117228, acc 0.94
2016-09-07T22:32:00.287271: step 808, loss 0.125966, acc 0.9
2016-09-07T22:32:00.953553: step 809, loss 0.0894661, acc 0.94
2016-09-07T22:32:01.657865: step 810, loss 0.062444, acc 1
2016-09-07T22:32:02.359878: step 811, loss 0.160162, acc 0.9
2016-09-07T22:32:03.079025: step 812, loss 0.1073, acc 0.98
2016-09-07T22:32:03.782747: step 813, loss 0.14598, acc 0.9
2016-09-07T22:32:04.473776: step 814, loss 0.0956304, acc 0.96
2016-09-07T22:32:05.170558: step 815, loss 0.34207, acc 0.94
2016-09-07T22:32:05.855991: step 816, loss 0.0585225, acc 0.96
2016-09-07T22:32:06.553712: step 817, loss 0.31994, acc 0.82
2016-09-07T22:32:07.257644: step 818, loss 0.0978163, acc 0.96
2016-09-07T22:32:07.956374: step 819, loss 0.090386, acc 0.96
2016-09-07T22:32:08.649078: step 820, loss 0.085907, acc 0.98
2016-09-07T22:32:09.327462: step 821, loss 0.0895581, acc 0.96
2016-09-07T22:32:10.032461: step 822, loss 0.208666, acc 0.92
2016-09-07T22:32:10.708685: step 823, loss 0.149752, acc 0.94
2016-09-07T22:32:11.413861: step 824, loss 0.109614, acc 0.96
2016-09-07T22:32:12.108782: step 825, loss 0.239501, acc 0.94
2016-09-07T22:32:12.817105: step 826, loss 0.264263, acc 0.9
2016-09-07T22:32:13.510656: step 827, loss 0.134795, acc 0.96
2016-09-07T22:32:14.218823: step 828, loss 0.0968754, acc 0.96
2016-09-07T22:32:14.914364: step 829, loss 0.126119, acc 0.92
2016-09-07T22:32:15.599204: step 830, loss 0.0650518, acc 0.96
2016-09-07T22:32:16.289005: step 831, loss 0.131641, acc 0.94
2016-09-07T22:32:16.980817: step 832, loss 0.130374, acc 0.96
2016-09-07T22:32:17.671103: step 833, loss 0.200214, acc 0.92
2016-09-07T22:32:18.366383: step 834, loss 0.0851547, acc 0.98
2016-09-07T22:32:19.097847: step 835, loss 0.0575895, acc 0.98
2016-09-07T22:32:19.794125: step 836, loss 0.0570929, acc 1
2016-09-07T22:32:20.491801: step 837, loss 0.0927919, acc 0.98
2016-09-07T22:32:21.177505: step 838, loss 0.0737817, acc 0.98
2016-09-07T22:32:21.874386: step 839, loss 0.134795, acc 0.98
2016-09-07T22:32:22.564628: step 840, loss 0.199235, acc 0.9
2016-09-07T22:32:23.263690: step 841, loss 0.145178, acc 0.96
2016-09-07T22:32:23.979480: step 842, loss 0.18526, acc 0.94
2016-09-07T22:32:24.683188: step 843, loss 0.164645, acc 0.94
2016-09-07T22:32:25.392780: step 844, loss 0.25905, acc 0.92
2016-09-07T22:32:26.083958: step 845, loss 0.139206, acc 0.94
2016-09-07T22:32:26.763429: step 846, loss 0.163981, acc 0.94
2016-09-07T22:32:27.442573: step 847, loss 0.109999, acc 0.96
2016-09-07T22:32:28.144001: step 848, loss 0.148598, acc 0.98
2016-09-07T22:32:28.833959: step 849, loss 0.0934557, acc 0.96
2016-09-07T22:32:29.517847: step 850, loss 0.151693, acc 0.94
2016-09-07T22:32:30.218579: step 851, loss 0.104224, acc 0.94
2016-09-07T22:32:30.926556: step 852, loss 0.180018, acc 0.96
2016-09-07T22:32:31.604140: step 853, loss 0.195379, acc 0.92
2016-09-07T22:32:32.301318: step 854, loss 0.233674, acc 0.92
2016-09-07T22:32:32.995156: step 855, loss 0.194991, acc 0.92
2016-09-07T22:32:33.691584: step 856, loss 0.112423, acc 0.94
2016-09-07T22:32:34.410645: step 857, loss 0.0625301, acc 0.98
2016-09-07T22:32:35.101012: step 858, loss 0.0649128, acc 0.98
2016-09-07T22:32:35.806002: step 859, loss 0.157351, acc 0.96
2016-09-07T22:32:36.504353: step 860, loss 0.14685, acc 0.92
2016-09-07T22:32:37.206664: step 861, loss 0.0915098, acc 0.94
2016-09-07T22:32:37.904222: step 862, loss 0.229981, acc 0.88
2016-09-07T22:32:38.590287: step 863, loss 0.0635453, acc 0.98
2016-09-07T22:32:39.278966: step 864, loss 0.0528849, acc 0.98
2016-09-07T22:32:39.982751: step 865, loss 0.0624228, acc 0.98
2016-09-07T22:32:40.682413: step 866, loss 0.132634, acc 0.96
2016-09-07T22:32:41.369028: step 867, loss 0.162127, acc 0.92
2016-09-07T22:32:42.092298: step 868, loss 0.0717065, acc 1
2016-09-07T22:32:42.793451: step 869, loss 0.156791, acc 0.94
2016-09-07T22:32:43.491373: step 870, loss 0.190783, acc 0.92
2016-09-07T22:32:44.204276: step 871, loss 0.228418, acc 0.92
2016-09-07T22:32:44.907114: step 872, loss 0.088697, acc 0.98
2016-09-07T22:32:45.609647: step 873, loss 0.0842043, acc 0.94
2016-09-07T22:32:46.297038: step 874, loss 0.141649, acc 0.96
2016-09-07T22:32:46.986441: step 875, loss 0.115268, acc 0.96
2016-09-07T22:32:47.692768: step 876, loss 0.185596, acc 0.9
2016-09-07T22:32:48.412682: step 877, loss 0.140982, acc 0.94
2016-09-07T22:32:49.106326: step 878, loss 0.0771671, acc 0.96
2016-09-07T22:32:49.808276: step 879, loss 0.107225, acc 0.96
2016-09-07T22:32:50.517773: step 880, loss 0.164601, acc 0.94
2016-09-07T22:32:51.198155: step 881, loss 0.207206, acc 0.96
2016-09-07T22:32:51.928090: step 882, loss 0.100086, acc 0.98
2016-09-07T22:32:52.624319: step 883, loss 0.102506, acc 0.96
2016-09-07T22:32:53.339154: step 884, loss 0.163587, acc 0.92
2016-09-07T22:32:54.038440: step 885, loss 0.168978, acc 0.9
2016-09-07T22:32:54.740132: step 886, loss 0.0812331, acc 0.98
2016-09-07T22:32:55.432623: step 887, loss 0.172791, acc 0.94
2016-09-07T22:32:56.130027: step 888, loss 0.137319, acc 0.96
2016-09-07T22:32:56.836893: step 889, loss 0.198397, acc 0.9
2016-09-07T22:32:57.534948: step 890, loss 0.0859799, acc 0.96
2016-09-07T22:32:58.225089: step 891, loss 0.182511, acc 0.94
2016-09-07T22:32:58.922862: step 892, loss 0.128287, acc 0.94
2016-09-07T22:32:59.632766: step 893, loss 0.0520677, acc 0.98
2016-09-07T22:33:00.368725: step 894, loss 0.0992378, acc 0.94
2016-09-07T22:33:01.078294: step 895, loss 0.223893, acc 0.96
2016-09-07T22:33:01.801857: step 896, loss 0.153052, acc 0.96
2016-09-07T22:33:02.542175: step 897, loss 0.0614609, acc 0.96
2016-09-07T22:33:03.232053: step 898, loss 0.14155, acc 0.94
2016-09-07T22:33:03.939802: step 899, loss 0.190802, acc 0.92
2016-09-07T22:33:04.640986: step 900, loss 0.11419, acc 0.96

Evaluation:
2016-09-07T22:33:07.772112: step 900, loss 0.866138, acc 0.764

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-900

2016-09-07T22:33:09.391965: step 901, loss 0.173627, acc 0.9
2016-09-07T22:33:10.091508: step 902, loss 0.148388, acc 0.94
2016-09-07T22:33:10.792613: step 903, loss 0.0738733, acc 0.98
2016-09-07T22:33:11.489477: step 904, loss 0.0626853, acc 0.98
2016-09-07T22:33:12.187308: step 905, loss 0.153525, acc 0.96
2016-09-07T22:33:12.886846: step 906, loss 0.144673, acc 0.94
2016-09-07T22:33:13.572709: step 907, loss 0.355784, acc 0.92
2016-09-07T22:33:14.268271: step 908, loss 0.102667, acc 0.92
2016-09-07T22:33:14.973910: step 909, loss 0.153428, acc 0.94
2016-09-07T22:33:15.667184: step 910, loss 0.135166, acc 0.94
2016-09-07T22:33:16.368978: step 911, loss 0.10208, acc 0.94
2016-09-07T22:33:17.060840: step 912, loss 0.0627311, acc 1
2016-09-07T22:33:17.749748: step 913, loss 0.149399, acc 0.92
2016-09-07T22:33:18.466355: step 914, loss 0.159767, acc 0.92
2016-09-07T22:33:19.161828: step 915, loss 0.207033, acc 0.94
2016-09-07T22:33:19.852519: step 916, loss 0.113657, acc 0.96
2016-09-07T22:33:20.562930: step 917, loss 0.0659126, acc 0.98
2016-09-07T22:33:21.273272: step 918, loss 0.197637, acc 0.92
2016-09-07T22:33:21.983600: step 919, loss 0.108817, acc 0.96
2016-09-07T22:33:22.692162: step 920, loss 0.169012, acc 0.92
2016-09-07T22:33:23.379867: step 921, loss 0.0726557, acc 0.96
2016-09-07T22:33:24.087466: step 922, loss 0.227082, acc 0.92
2016-09-07T22:33:24.792366: step 923, loss 0.129405, acc 0.96
2016-09-07T22:33:25.480074: step 924, loss 0.100631, acc 0.94
2016-09-07T22:33:26.183838: step 925, loss 0.122261, acc 0.94
2016-09-07T22:33:26.869150: step 926, loss 0.201028, acc 0.92
2016-09-07T22:33:27.580523: step 927, loss 0.173297, acc 0.92
2016-09-07T22:33:28.271107: step 928, loss 0.119579, acc 0.96
2016-09-07T22:33:28.966126: step 929, loss 0.139889, acc 0.92
2016-09-07T22:33:29.655677: step 930, loss 0.054276, acc 0.98
2016-09-07T22:33:30.374387: step 931, loss 0.14788, acc 0.94
2016-09-07T22:33:31.082086: step 932, loss 0.195677, acc 0.92
2016-09-07T22:33:31.784014: step 933, loss 0.0310483, acc 1
2016-09-07T22:33:32.466803: step 934, loss 0.0815298, acc 0.96
2016-09-07T22:33:33.181188: step 935, loss 0.192827, acc 0.92
2016-09-07T22:33:33.894493: step 936, loss 0.0684628, acc 0.98
2016-09-07T22:33:34.583799: step 937, loss 0.163653, acc 0.92
2016-09-07T22:33:35.261531: step 938, loss 0.207946, acc 0.88
2016-09-07T22:33:35.964384: step 939, loss 0.122297, acc 0.94
2016-09-07T22:33:36.693602: step 940, loss 0.0570769, acc 0.98
2016-09-07T22:33:37.407233: step 941, loss 0.0756484, acc 0.98
2016-09-07T22:33:38.111973: step 942, loss 0.0935865, acc 0.94
2016-09-07T22:33:38.824270: step 943, loss 0.212942, acc 0.92
2016-09-07T22:33:39.521589: step 944, loss 0.162273, acc 0.94
2016-09-07T22:33:40.214956: step 945, loss 0.310723, acc 0.82
2016-09-07T22:33:40.914496: step 946, loss 0.189957, acc 0.88
2016-09-07T22:33:41.609630: step 947, loss 0.124805, acc 0.96
2016-09-07T22:33:42.305662: step 948, loss 0.185931, acc 0.86
2016-09-07T22:33:43.002729: step 949, loss 0.114812, acc 0.94
2016-09-07T22:33:43.702763: step 950, loss 0.0827611, acc 0.94
2016-09-07T22:33:44.396420: step 951, loss 0.0856825, acc 0.94
2016-09-07T22:33:45.095867: step 952, loss 0.16641, acc 0.92
2016-09-07T22:33:45.790708: step 953, loss 0.0772726, acc 0.96
2016-09-07T22:33:46.483252: step 954, loss 0.127497, acc 0.98
2016-09-07T22:33:47.190679: step 955, loss 0.251077, acc 0.9
2016-09-07T22:33:47.895554: step 956, loss 0.0343737, acc 1
2016-09-07T22:33:48.572801: step 957, loss 0.240041, acc 0.92
2016-09-07T22:33:49.262047: step 958, loss 0.0974491, acc 0.96
2016-09-07T22:33:49.957562: step 959, loss 0.0759246, acc 0.96
2016-09-07T22:33:50.640328: step 960, loss 0.0903853, acc 0.96
2016-09-07T22:33:51.350253: step 961, loss 0.126981, acc 0.94
2016-09-07T22:33:52.041075: step 962, loss 0.0862249, acc 0.96
2016-09-07T22:33:52.754214: step 963, loss 0.159045, acc 0.94
2016-09-07T22:33:53.486329: step 964, loss 0.0800134, acc 0.96
2016-09-07T22:33:54.192331: step 965, loss 0.0980352, acc 0.94
2016-09-07T22:33:54.893680: step 966, loss 0.0959765, acc 0.92
2016-09-07T22:33:55.590573: step 967, loss 0.187867, acc 0.92
2016-09-07T22:33:56.276754: step 968, loss 0.050536, acc 0.98
2016-09-07T22:33:57.005992: step 969, loss 0.192351, acc 0.92
2016-09-07T22:33:57.368623: step 970, loss 0.11232, acc 1
2016-09-07T22:33:58.057623: step 971, loss 0.0835313, acc 0.98
2016-09-07T22:33:58.760121: step 972, loss 0.0778141, acc 0.98
2016-09-07T22:33:59.443453: step 973, loss 0.140262, acc 0.92
2016-09-07T22:34:00.140774: step 974, loss 0.125392, acc 0.96
2016-09-07T22:34:00.870012: step 975, loss 0.0834721, acc 0.98
2016-09-07T22:34:01.576669: step 976, loss 0.108046, acc 0.96
2016-09-07T22:34:02.294478: step 977, loss 0.0787022, acc 0.98
2016-09-07T22:34:03.000419: step 978, loss 0.13946, acc 0.94
2016-09-07T22:34:03.682020: step 979, loss 0.0238668, acc 1
2016-09-07T22:34:04.392712: step 980, loss 0.260325, acc 0.88
2016-09-07T22:34:05.100727: step 981, loss 0.152109, acc 0.94
2016-09-07T22:34:05.792533: step 982, loss 0.122324, acc 0.96
2016-09-07T22:34:06.483234: step 983, loss 0.168967, acc 0.94
2016-09-07T22:34:07.187283: step 984, loss 0.100611, acc 0.94
2016-09-07T22:34:07.886013: step 985, loss 0.0588325, acc 0.98
2016-09-07T22:34:08.585237: step 986, loss 0.157303, acc 0.96
2016-09-07T22:34:09.286462: step 987, loss 0.100905, acc 0.96
2016-09-07T22:34:09.978102: step 988, loss 0.014588, acc 1
2016-09-07T22:34:10.695646: step 989, loss 0.138988, acc 0.92
2016-09-07T22:34:11.374641: step 990, loss 0.014758, acc 1
2016-09-07T22:34:12.066221: step 991, loss 0.128821, acc 0.94
2016-09-07T22:34:12.778086: step 992, loss 0.250666, acc 0.92
2016-09-07T22:34:13.476719: step 993, loss 0.051812, acc 0.98
2016-09-07T22:34:14.193062: step 994, loss 0.142597, acc 0.98
2016-09-07T22:34:14.899866: step 995, loss 0.0693217, acc 0.98
2016-09-07T22:34:15.595184: step 996, loss 0.0904725, acc 0.94
2016-09-07T22:34:16.305340: step 997, loss 0.0492782, acc 0.98
2016-09-07T22:34:17.008935: step 998, loss 0.0746037, acc 0.94
2016-09-07T22:34:17.708914: step 999, loss 0.10648, acc 0.96
2016-09-07T22:34:18.420642: step 1000, loss 0.144757, acc 0.94

Evaluation:
2016-09-07T22:34:21.578514: step 1000, loss 0.767455, acc 0.785

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-1000

2016-09-07T22:34:23.211191: step 1001, loss 0.060221, acc 1
2016-09-07T22:34:23.921465: step 1002, loss 0.145143, acc 0.92
2016-09-07T22:34:24.620471: step 1003, loss 0.124533, acc 0.94
2016-09-07T22:34:25.313914: step 1004, loss 0.104293, acc 0.96
2016-09-07T22:34:26.007301: step 1005, loss 0.228584, acc 0.92
2016-09-07T22:34:26.706785: step 1006, loss 0.0581738, acc 0.98
2016-09-07T22:34:27.406759: step 1007, loss 0.114362, acc 0.92
2016-09-07T22:34:28.118900: step 1008, loss 0.0575073, acc 0.98
2016-09-07T22:34:28.816961: step 1009, loss 0.0408814, acc 1
2016-09-07T22:34:29.522672: step 1010, loss 0.0900826, acc 0.96
2016-09-07T22:34:30.225061: step 1011, loss 0.11314, acc 0.94
2016-09-07T22:34:30.919745: step 1012, loss 0.0539499, acc 0.96
2016-09-07T22:34:31.612023: step 1013, loss 0.0286993, acc 0.98
2016-09-07T22:34:32.311991: step 1014, loss 0.0711887, acc 0.96
2016-09-07T22:34:33.016835: step 1015, loss 0.0272806, acc 0.98
2016-09-07T22:34:33.700264: step 1016, loss 0.0822227, acc 0.96
2016-09-07T22:34:34.409391: step 1017, loss 0.095108, acc 0.94
2016-09-07T22:34:35.109273: step 1018, loss 0.0710066, acc 0.96
2016-09-07T22:34:35.822622: step 1019, loss 0.0583856, acc 0.98
2016-09-07T22:34:36.514575: step 1020, loss 0.0944771, acc 0.94
2016-09-07T22:34:37.211431: step 1021, loss 0.145902, acc 0.94
2016-09-07T22:34:37.915834: step 1022, loss 0.0964454, acc 0.94
2016-09-07T22:34:38.618643: step 1023, loss 0.0537795, acc 0.98
2016-09-07T22:34:39.316669: step 1024, loss 0.103759, acc 0.96
2016-09-07T22:34:40.016624: step 1025, loss 0.143359, acc 0.94
2016-09-07T22:34:40.714378: step 1026, loss 0.144585, acc 0.92
2016-09-07T22:34:41.453790: step 1027, loss 0.13193, acc 0.92
2016-09-07T22:34:42.135073: step 1028, loss 0.0714918, acc 0.96
2016-09-07T22:34:42.829866: step 1029, loss 0.0760901, acc 0.96
2016-09-07T22:34:43.537746: step 1030, loss 0.0624109, acc 0.96
2016-09-07T22:34:44.227079: step 1031, loss 0.082886, acc 0.94
2016-09-07T22:34:44.908582: step 1032, loss 0.0487599, acc 0.98
2016-09-07T22:34:45.598332: step 1033, loss 0.190824, acc 0.9
2016-09-07T22:34:46.306715: step 1034, loss 0.113634, acc 0.96
2016-09-07T22:34:47.000178: step 1035, loss 0.162554, acc 0.9
2016-09-07T22:34:47.712489: step 1036, loss 0.0470674, acc 1
2016-09-07T22:34:48.409077: step 1037, loss 0.0107274, acc 1
2016-09-07T22:34:49.100225: step 1038, loss 0.100348, acc 0.94
2016-09-07T22:34:49.807560: step 1039, loss 0.147698, acc 0.98
2016-09-07T22:34:50.510832: step 1040, loss 0.0925641, acc 0.92
2016-09-07T22:34:51.201686: step 1041, loss 0.0361011, acc 0.98
2016-09-07T22:34:51.890088: step 1042, loss 0.0663932, acc 0.96
2016-09-07T22:34:52.585972: step 1043, loss 0.20547, acc 0.88
2016-09-07T22:34:53.287905: step 1044, loss 0.0113289, acc 1
2016-09-07T22:34:53.988025: step 1045, loss 0.0689609, acc 0.94
2016-09-07T22:34:54.697427: step 1046, loss 0.141363, acc 0.96
2016-09-07T22:34:55.388768: step 1047, loss 0.154766, acc 0.94
2016-09-07T22:34:56.101248: step 1048, loss 0.102437, acc 0.96
2016-09-07T22:34:56.787494: step 1049, loss 0.130787, acc 0.94
2016-09-07T22:34:57.486122: step 1050, loss 0.109077, acc 0.92
2016-09-07T22:34:58.174139: step 1051, loss 0.0752707, acc 0.98
2016-09-07T22:34:58.872003: step 1052, loss 0.17696, acc 0.94
2016-09-07T22:34:59.570957: step 1053, loss 0.0458691, acc 1
2016-09-07T22:35:00.267927: step 1054, loss 0.146483, acc 0.94
2016-09-07T22:35:00.956856: step 1055, loss 0.0648555, acc 0.96
2016-09-07T22:35:01.650868: step 1056, loss 0.134942, acc 0.96
2016-09-07T22:35:02.346655: step 1057, loss 0.0959415, acc 0.94
2016-09-07T22:35:03.054501: step 1058, loss 0.0978016, acc 0.96
2016-09-07T22:35:03.777966: step 1059, loss 0.19187, acc 0.9
2016-09-07T22:35:04.473514: step 1060, loss 0.127785, acc 0.92
2016-09-07T22:35:05.160992: step 1061, loss 0.0274241, acc 1
2016-09-07T22:35:05.838207: step 1062, loss 0.0825421, acc 0.98
2016-09-07T22:35:06.535473: step 1063, loss 0.230885, acc 0.9
2016-09-07T22:35:07.235246: step 1064, loss 0.081909, acc 0.96
2016-09-07T22:35:07.926974: step 1065, loss 0.167907, acc 0.96
2016-09-07T22:35:08.610994: step 1066, loss 0.208047, acc 0.88
2016-09-07T22:35:09.311588: step 1067, loss 0.12109, acc 0.96
2016-09-07T22:35:10.011911: step 1068, loss 0.0247559, acc 1
2016-09-07T22:35:10.699104: step 1069, loss 0.106036, acc 0.94
2016-09-07T22:35:11.389115: step 1070, loss 0.0733955, acc 0.96
2016-09-07T22:35:12.077443: step 1071, loss 0.108997, acc 0.98
2016-09-07T22:35:12.775980: step 1072, loss 0.0436558, acc 1
2016-09-07T22:35:13.471731: step 1073, loss 0.119807, acc 0.92
2016-09-07T22:35:14.178847: step 1074, loss 0.104211, acc 0.94
2016-09-07T22:35:14.875886: step 1075, loss 0.0882998, acc 0.98
2016-09-07T22:35:15.577151: step 1076, loss 0.08784, acc 0.94
2016-09-07T22:35:16.276357: step 1077, loss 0.137185, acc 0.94
2016-09-07T22:35:16.994508: step 1078, loss 0.215145, acc 0.92
2016-09-07T22:35:17.682749: step 1079, loss 0.123959, acc 0.9
2016-09-07T22:35:18.377648: step 1080, loss 0.105337, acc 0.94
2016-09-07T22:35:19.066428: step 1081, loss 0.0771174, acc 1
2016-09-07T22:35:19.760014: step 1082, loss 0.0179727, acc 1
2016-09-07T22:35:20.451081: step 1083, loss 0.132592, acc 0.94
2016-09-07T22:35:21.151507: step 1084, loss 0.0444341, acc 1
2016-09-07T22:35:21.830346: step 1085, loss 0.104734, acc 0.96
2016-09-07T22:35:22.513994: step 1086, loss 0.175964, acc 0.96
2016-09-07T22:35:23.221751: step 1087, loss 0.104027, acc 0.96
2016-09-07T22:35:23.922711: step 1088, loss 0.0740419, acc 0.96
2016-09-07T22:35:24.632950: step 1089, loss 0.106638, acc 0.94
2016-09-07T22:35:25.347432: step 1090, loss 0.243098, acc 0.94
2016-09-07T22:35:26.051598: step 1091, loss 0.147749, acc 0.92
2016-09-07T22:35:26.787360: step 1092, loss 0.0870283, acc 0.98
2016-09-07T22:35:27.495226: step 1093, loss 0.142, acc 0.94
2016-09-07T22:35:28.192504: step 1094, loss 0.0642842, acc 0.98
2016-09-07T22:35:28.881058: step 1095, loss 0.0458306, acc 0.98
2016-09-07T22:35:29.580391: step 1096, loss 0.147073, acc 0.88
2016-09-07T22:35:30.272230: step 1097, loss 0.237903, acc 0.9
2016-09-07T22:35:30.957672: step 1098, loss 0.103037, acc 0.96
2016-09-07T22:35:31.644118: step 1099, loss 0.037681, acc 0.98
2016-09-07T22:35:32.333226: step 1100, loss 0.0730558, acc 0.98

Evaluation:
2016-09-07T22:35:35.533943: step 1100, loss 0.894261, acc 0.766

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-1100

2016-09-07T22:35:37.273727: step 1101, loss 0.0806519, acc 0.98
2016-09-07T22:35:37.964413: step 1102, loss 0.228465, acc 0.9
2016-09-07T22:35:38.647514: step 1103, loss 0.0259761, acc 1
2016-09-07T22:35:39.349380: step 1104, loss 0.0824519, acc 0.96
2016-09-07T22:35:40.052043: step 1105, loss 0.178249, acc 0.94
2016-09-07T22:35:40.753855: step 1106, loss 0.211804, acc 0.94
2016-09-07T22:35:41.466567: step 1107, loss 0.0944511, acc 0.96
2016-09-07T22:35:42.168147: step 1108, loss 0.0212314, acc 1
2016-09-07T22:35:42.878057: step 1109, loss 0.0816958, acc 0.96
2016-09-07T22:35:43.593016: step 1110, loss 0.169565, acc 0.92
2016-09-07T22:35:44.304425: step 1111, loss 0.0984937, acc 0.94
2016-09-07T22:35:45.013718: step 1112, loss 0.170091, acc 0.92
2016-09-07T22:35:45.726445: step 1113, loss 0.0183254, acc 1
2016-09-07T22:35:46.423736: step 1114, loss 0.0612841, acc 0.98
2016-09-07T22:35:47.127026: step 1115, loss 0.0371099, acc 0.98
2016-09-07T22:35:47.825593: step 1116, loss 0.187357, acc 0.86
2016-09-07T22:35:48.517884: step 1117, loss 0.098559, acc 0.98
2016-09-07T22:35:49.212754: step 1118, loss 0.0465938, acc 0.98
2016-09-07T22:35:49.911302: step 1119, loss 0.142934, acc 0.98
2016-09-07T22:35:50.628426: step 1120, loss 0.142638, acc 0.94
2016-09-07T22:35:51.333828: step 1121, loss 0.135246, acc 0.92
2016-09-07T22:35:52.028007: step 1122, loss 0.19795, acc 0.86
2016-09-07T22:35:52.711835: step 1123, loss 0.0951559, acc 0.98
2016-09-07T22:35:53.390607: step 1124, loss 0.102943, acc 0.96
2016-09-07T22:35:54.076184: step 1125, loss 0.13994, acc 0.92
2016-09-07T22:35:54.777031: step 1126, loss 0.0925646, acc 0.98
2016-09-07T22:35:55.485297: step 1127, loss 0.0666949, acc 0.98
2016-09-07T22:35:56.194268: step 1128, loss 0.114386, acc 0.92
2016-09-07T22:35:56.895499: step 1129, loss 0.104473, acc 0.92
2016-09-07T22:35:57.600470: step 1130, loss 0.123715, acc 0.92
2016-09-07T22:35:58.321320: step 1131, loss 0.0661309, acc 0.96
2016-09-07T22:35:59.025680: step 1132, loss 0.105078, acc 0.94
2016-09-07T22:35:59.746396: step 1133, loss 0.0534883, acc 0.98
2016-09-07T22:36:00.472411: step 1134, loss 0.053331, acc 0.98
2016-09-07T22:36:01.178104: step 1135, loss 0.130412, acc 0.96
2016-09-07T22:36:01.890563: step 1136, loss 0.0482237, acc 0.98
2016-09-07T22:36:02.587626: step 1137, loss 0.106145, acc 0.96
2016-09-07T22:36:03.302810: step 1138, loss 0.109192, acc 0.94
2016-09-07T22:36:04.000156: step 1139, loss 0.0453383, acc 0.98
2016-09-07T22:36:04.690582: step 1140, loss 0.111828, acc 0.96
2016-09-07T22:36:05.397416: step 1141, loss 0.066083, acc 0.94
2016-09-07T22:36:06.102777: step 1142, loss 0.0627531, acc 0.94
2016-09-07T22:36:06.788108: step 1143, loss 0.0814698, acc 0.98
2016-09-07T22:36:07.471180: step 1144, loss 0.0269082, acc 1
2016-09-07T22:36:08.158825: step 1145, loss 0.181445, acc 0.96
2016-09-07T22:36:08.874846: step 1146, loss 0.149211, acc 0.94
2016-09-07T22:36:09.584021: step 1147, loss 0.113643, acc 0.92
2016-09-07T22:36:10.291946: step 1148, loss 0.107941, acc 0.96
2016-09-07T22:36:11.007657: step 1149, loss 0.0895163, acc 0.96
2016-09-07T22:36:11.710802: step 1150, loss 0.10233, acc 0.96
2016-09-07T22:36:12.409629: step 1151, loss 0.122117, acc 0.92
2016-09-07T22:36:13.095343: step 1152, loss 0.0605245, acc 0.96
2016-09-07T22:36:13.803898: step 1153, loss 0.0882351, acc 0.98
2016-09-07T22:36:14.536708: step 1154, loss 0.0286043, acc 1
2016-09-07T22:36:15.241935: step 1155, loss 0.0840546, acc 0.98
2016-09-07T22:36:15.939266: step 1156, loss 0.0743193, acc 0.96
2016-09-07T22:36:16.631894: step 1157, loss 0.128238, acc 0.94
2016-09-07T22:36:17.320324: step 1158, loss 0.29992, acc 0.92
2016-09-07T22:36:18.012927: step 1159, loss 0.0624643, acc 0.96
2016-09-07T22:36:18.705907: step 1160, loss 0.0476176, acc 1
2016-09-07T22:36:19.385028: step 1161, loss 0.111126, acc 0.96
2016-09-07T22:36:20.095680: step 1162, loss 0.0755846, acc 0.94
2016-09-07T22:36:20.785794: step 1163, loss 0.102594, acc 0.96
2016-09-07T22:36:21.161503: step 1164, loss 0.0841896, acc 1
2016-09-07T22:36:21.855722: step 1165, loss 0.173864, acc 0.96
2016-09-07T22:36:22.564955: step 1166, loss 0.0876931, acc 0.94
2016-09-07T22:36:23.256577: step 1167, loss 0.0493722, acc 1
2016-09-07T22:36:23.957941: step 1168, loss 0.0582096, acc 1
2016-09-07T22:36:24.666915: step 1169, loss 0.131797, acc 0.94
2016-09-07T22:36:25.357918: step 1170, loss 0.243637, acc 0.94
2016-09-07T22:36:26.078342: step 1171, loss 0.0866103, acc 0.96
2016-09-07T22:36:26.794167: step 1172, loss 0.0451992, acc 0.98
2016-09-07T22:36:27.502363: step 1173, loss 0.0426715, acc 1
2016-09-07T22:36:28.186595: step 1174, loss 0.0373838, acc 1
2016-09-07T22:36:28.870533: step 1175, loss 0.0755635, acc 0.94
2016-09-07T22:36:29.599228: step 1176, loss 0.27906, acc 0.92
2016-09-07T22:36:30.280202: step 1177, loss 0.0288932, acc 1
2016-09-07T22:36:30.981826: step 1178, loss 0.0192515, acc 1
2016-09-07T22:36:31.688546: step 1179, loss 0.0251372, acc 1
2016-09-07T22:36:32.392591: step 1180, loss 0.121735, acc 0.96
2016-09-07T22:36:33.107094: step 1181, loss 0.0402448, acc 0.98
2016-09-07T22:36:33.814166: step 1182, loss 0.0260642, acc 1
2016-09-07T22:36:34.533342: step 1183, loss 0.117253, acc 0.94
2016-09-07T22:36:35.247611: step 1184, loss 0.0500569, acc 0.98
2016-09-07T22:36:35.956353: step 1185, loss 0.0969988, acc 0.94
2016-09-07T22:36:36.660307: step 1186, loss 0.105227, acc 0.96
2016-09-07T22:36:37.374878: step 1187, loss 0.0508439, acc 0.98
2016-09-07T22:36:38.079418: step 1188, loss 0.0913804, acc 0.94
2016-09-07T22:36:38.770495: step 1189, loss 0.0406172, acc 0.98
2016-09-07T22:36:39.488467: step 1190, loss 0.0598133, acc 0.96
2016-09-07T22:36:40.182023: step 1191, loss 0.0824339, acc 0.96
2016-09-07T22:36:40.873392: step 1192, loss 0.0502484, acc 1
2016-09-07T22:36:41.569657: step 1193, loss 0.0526601, acc 0.98
2016-09-07T22:36:42.259089: step 1194, loss 0.0586132, acc 0.98
2016-09-07T22:36:42.952031: step 1195, loss 0.0363969, acc 0.98
2016-09-07T22:36:43.661332: step 1196, loss 0.0866519, acc 0.96
2016-09-07T22:36:44.369674: step 1197, loss 0.109476, acc 0.98
2016-09-07T22:36:45.064666: step 1198, loss 0.0494496, acc 0.98
2016-09-07T22:36:45.778580: step 1199, loss 0.135541, acc 0.94
2016-09-07T22:36:46.478415: step 1200, loss 0.124288, acc 0.96

Evaluation:
2016-09-07T22:36:49.618334: step 1200, loss 1.16027, acc 0.754

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-1200

2016-09-07T22:36:51.370650: step 1201, loss 0.0638562, acc 0.98
2016-09-07T22:36:52.062861: step 1202, loss 0.0573774, acc 0.96
2016-09-07T22:36:52.759598: step 1203, loss 0.0348215, acc 1
2016-09-07T22:36:53.458487: step 1204, loss 0.178241, acc 0.96
2016-09-07T22:36:54.157074: step 1205, loss 0.150738, acc 0.96
2016-09-07T22:36:54.860571: step 1206, loss 0.0671771, acc 0.98
2016-09-07T22:36:55.554326: step 1207, loss 0.1264, acc 0.98
2016-09-07T22:36:56.246911: step 1208, loss 0.0242013, acc 1
2016-09-07T22:36:56.928456: step 1209, loss 0.0430798, acc 0.96
2016-09-07T22:36:57.626273: step 1210, loss 0.300499, acc 0.9
2016-09-07T22:36:58.320609: step 1211, loss 0.0691483, acc 0.98
2016-09-07T22:36:59.008207: step 1212, loss 0.0862892, acc 0.96
2016-09-07T22:36:59.729190: step 1213, loss 0.0792054, acc 0.98
2016-09-07T22:37:00.467526: step 1214, loss 0.127121, acc 0.96
2016-09-07T22:37:01.157542: step 1215, loss 0.222046, acc 0.92
2016-09-07T22:37:01.888432: step 1216, loss 0.0289479, acc 1
2016-09-07T22:37:02.579046: step 1217, loss 0.048228, acc 1
2016-09-07T22:37:03.277264: step 1218, loss 0.0527202, acc 1
2016-09-07T22:37:03.964464: step 1219, loss 0.122164, acc 0.94
2016-09-07T22:37:04.670679: step 1220, loss 0.097129, acc 0.94
2016-09-07T22:37:05.367604: step 1221, loss 0.142521, acc 0.94
2016-09-07T22:37:06.086830: step 1222, loss 0.0793655, acc 0.96
2016-09-07T22:37:06.790948: step 1223, loss 0.0850845, acc 0.98
2016-09-07T22:37:07.483367: step 1224, loss 0.102019, acc 0.94
2016-09-07T22:37:08.189469: step 1225, loss 0.0709264, acc 0.94
2016-09-07T22:37:08.870190: step 1226, loss 0.0887998, acc 0.98
2016-09-07T22:37:09.578039: step 1227, loss 0.0678505, acc 0.96
2016-09-07T22:37:10.269617: step 1228, loss 0.0415618, acc 0.98
2016-09-07T22:37:10.979493: step 1229, loss 0.158274, acc 0.96
2016-09-07T22:37:11.686778: step 1230, loss 0.0954967, acc 0.96
2016-09-07T22:37:12.367439: step 1231, loss 0.0576693, acc 0.98
2016-09-07T22:37:13.066435: step 1232, loss 0.0443445, acc 0.98
2016-09-07T22:37:13.757630: step 1233, loss 0.0215148, acc 1
2016-09-07T22:37:14.470235: step 1234, loss 0.094606, acc 0.96
2016-09-07T22:37:15.172400: step 1235, loss 0.117425, acc 0.94
2016-09-07T22:37:15.871513: step 1236, loss 0.0733484, acc 0.98
2016-09-07T22:37:16.571756: step 1237, loss 0.0414706, acc 0.98
2016-09-07T22:37:17.249838: step 1238, loss 0.0133697, acc 1
2016-09-07T22:37:17.940665: step 1239, loss 0.030801, acc 1
2016-09-07T22:37:18.625765: step 1240, loss 0.230809, acc 0.9
2016-09-07T22:37:19.335620: step 1241, loss 0.172706, acc 0.92
2016-09-07T22:37:20.041242: step 1242, loss 0.0588849, acc 0.98
2016-09-07T22:37:20.746864: step 1243, loss 0.089746, acc 0.96
2016-09-07T22:37:21.450813: step 1244, loss 0.219617, acc 0.94
2016-09-07T22:37:22.144438: step 1245, loss 0.159618, acc 0.9
2016-09-07T22:37:22.844268: step 1246, loss 0.180619, acc 0.96
2016-09-07T22:37:23.559948: step 1247, loss 0.102022, acc 0.92
2016-09-07T22:37:24.257823: step 1248, loss 0.0658541, acc 0.96
2016-09-07T22:37:24.958500: step 1249, loss 0.166411, acc 0.92
2016-09-07T22:37:25.671300: step 1250, loss 0.0638623, acc 1
2016-09-07T22:37:26.391369: step 1251, loss 0.0424096, acc 1
2016-09-07T22:37:27.077656: step 1252, loss 0.0827446, acc 0.94
2016-09-07T22:37:27.778477: step 1253, loss 0.186691, acc 0.9
2016-09-07T22:37:28.479343: step 1254, loss 0.0521088, acc 0.98
2016-09-07T22:37:29.184680: step 1255, loss 0.143356, acc 0.98
2016-09-07T22:37:29.872738: step 1256, loss 0.0746471, acc 0.96
2016-09-07T22:37:30.563157: step 1257, loss 0.0708669, acc 0.98
2016-09-07T22:37:31.261371: step 1258, loss 0.105121, acc 0.94
2016-09-07T22:37:31.962170: step 1259, loss 0.0536037, acc 1
2016-09-07T22:37:32.659062: step 1260, loss 0.0640995, acc 0.98
2016-09-07T22:37:33.352588: step 1261, loss 0.0818781, acc 0.96
2016-09-07T22:37:34.042032: step 1262, loss 0.100957, acc 0.96
2016-09-07T22:37:34.755895: step 1263, loss 0.0676024, acc 1
2016-09-07T22:37:35.446407: step 1264, loss 0.121086, acc 0.92
2016-09-07T22:37:36.145437: step 1265, loss 0.0557837, acc 1
2016-09-07T22:37:36.838655: step 1266, loss 0.163297, acc 0.92
2016-09-07T22:37:37.542776: step 1267, loss 0.086503, acc 0.94
2016-09-07T22:37:38.238074: step 1268, loss 0.168175, acc 0.9
2016-09-07T22:37:38.939700: step 1269, loss 0.07124, acc 0.98
2016-09-07T22:37:39.630705: step 1270, loss 0.0532939, acc 1
2016-09-07T22:37:40.346580: step 1271, loss 0.0549078, acc 0.98
2016-09-07T22:37:41.056852: step 1272, loss 0.0709005, acc 0.96
2016-09-07T22:37:41.752550: step 1273, loss 0.0905425, acc 0.96
2016-09-07T22:37:42.436691: step 1274, loss 0.0645886, acc 0.98
2016-09-07T22:37:43.150932: step 1275, loss 0.143199, acc 0.94
2016-09-07T22:37:43.867462: step 1276, loss 0.0104632, acc 1
2016-09-07T22:37:44.568630: step 1277, loss 0.130865, acc 0.96
2016-09-07T22:37:45.277582: step 1278, loss 0.115123, acc 0.94
2016-09-07T22:37:45.975558: step 1279, loss 0.11775, acc 0.92
2016-09-07T22:37:46.692742: step 1280, loss 0.107905, acc 0.96
2016-09-07T22:37:47.424920: step 1281, loss 0.114202, acc 0.94
2016-09-07T22:37:48.121697: step 1282, loss 0.0554139, acc 0.96
2016-09-07T22:37:48.807098: step 1283, loss 0.0471536, acc 0.98
2016-09-07T22:37:49.489072: step 1284, loss 0.0834035, acc 0.96
2016-09-07T22:37:50.186319: step 1285, loss 0.165899, acc 0.94
2016-09-07T22:37:50.899450: step 1286, loss 0.0936273, acc 0.96
2016-09-07T22:37:51.600515: step 1287, loss 0.151626, acc 0.92
2016-09-07T22:37:52.293001: step 1288, loss 0.110613, acc 0.96
2016-09-07T22:37:52.989761: step 1289, loss 0.254673, acc 0.9
2016-09-07T22:37:53.683839: step 1290, loss 0.0602547, acc 0.98
2016-09-07T22:37:54.384054: step 1291, loss 0.123476, acc 0.92
2016-09-07T22:37:55.085574: step 1292, loss 0.075659, acc 0.98
2016-09-07T22:37:55.788341: step 1293, loss 0.14289, acc 0.94
2016-09-07T22:37:56.496019: step 1294, loss 0.144941, acc 0.94
2016-09-07T22:37:57.192800: step 1295, loss 0.169418, acc 0.94
2016-09-07T22:37:57.889941: step 1296, loss 0.15024, acc 0.94
2016-09-07T22:37:58.607103: step 1297, loss 0.0312705, acc 0.98
2016-09-07T22:37:59.324113: step 1298, loss 0.106061, acc 0.96
2016-09-07T22:38:00.035494: step 1299, loss 0.105544, acc 0.96
2016-09-07T22:38:00.772039: step 1300, loss 0.178027, acc 0.94

Evaluation:
2016-09-07T22:38:03.937304: step 1300, loss 0.796515, acc 0.758

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-1300

2016-09-07T22:38:05.576367: step 1301, loss 0.184259, acc 0.92
2016-09-07T22:38:06.287637: step 1302, loss 0.0359534, acc 1
2016-09-07T22:38:06.985904: step 1303, loss 0.0654796, acc 0.96
2016-09-07T22:38:07.677271: step 1304, loss 0.0861343, acc 0.96
2016-09-07T22:38:08.360848: step 1305, loss 0.12849, acc 0.96
2016-09-07T22:38:09.051455: step 1306, loss 0.106354, acc 0.98
2016-09-07T22:38:09.752979: step 1307, loss 0.0777959, acc 0.96
2016-09-07T22:38:10.457281: step 1308, loss 0.158285, acc 0.96
2016-09-07T22:38:11.156440: step 1309, loss 0.112123, acc 0.96
2016-09-07T22:38:11.882456: step 1310, loss 0.0426515, acc 1
2016-09-07T22:38:12.578475: step 1311, loss 0.141634, acc 0.94
2016-09-07T22:38:13.289877: step 1312, loss 0.0537773, acc 0.98
2016-09-07T22:38:13.988456: step 1313, loss 0.0888004, acc 0.96
2016-09-07T22:38:14.679817: step 1314, loss 0.0826058, acc 0.96
2016-09-07T22:38:15.379585: step 1315, loss 0.239405, acc 0.92
2016-09-07T22:38:16.088350: step 1316, loss 0.0499403, acc 1
2016-09-07T22:38:16.787600: step 1317, loss 0.128727, acc 0.96
2016-09-07T22:38:17.489964: step 1318, loss 0.195934, acc 0.92
2016-09-07T22:38:18.183747: step 1319, loss 0.0197961, acc 1
2016-09-07T22:38:18.878396: step 1320, loss 0.0721336, acc 0.98
2016-09-07T22:38:19.559067: step 1321, loss 0.0426968, acc 0.98
2016-09-07T22:38:20.262109: step 1322, loss 0.182096, acc 0.92
2016-09-07T22:38:20.970193: step 1323, loss 0.0633071, acc 0.98
2016-09-07T22:38:21.675695: step 1324, loss 0.0888326, acc 0.96
2016-09-07T22:38:22.388997: step 1325, loss 0.118101, acc 0.94
2016-09-07T22:38:23.088305: step 1326, loss 0.0497003, acc 0.98
2016-09-07T22:38:23.794564: step 1327, loss 0.0253552, acc 1
2016-09-07T22:38:24.489958: step 1328, loss 0.0430286, acc 0.98
2016-09-07T22:38:25.187657: step 1329, loss 0.0131224, acc 1
2016-09-07T22:38:25.867801: step 1330, loss 0.0688155, acc 0.96
2016-09-07T22:38:26.546450: step 1331, loss 0.052708, acc 0.98
2016-09-07T22:38:27.231249: step 1332, loss 0.0372174, acc 0.98
2016-09-07T22:38:27.924805: step 1333, loss 0.0707036, acc 0.98
2016-09-07T22:38:28.622563: step 1334, loss 0.0605192, acc 0.98
2016-09-07T22:38:29.319774: step 1335, loss 0.0507338, acc 0.98
2016-09-07T22:38:30.024406: step 1336, loss 0.19159, acc 0.94
2016-09-07T22:38:30.734394: step 1337, loss 0.167819, acc 0.96
2016-09-07T22:38:31.445227: step 1338, loss 0.00793347, acc 1
2016-09-07T22:38:32.133041: step 1339, loss 0.0727153, acc 0.98
2016-09-07T22:38:32.831567: step 1340, loss 0.0549185, acc 0.98
2016-09-07T22:38:33.521942: step 1341, loss 0.0231103, acc 1
2016-09-07T22:38:34.224461: step 1342, loss 0.0964661, acc 0.94
2016-09-07T22:38:34.920937: step 1343, loss 0.0792557, acc 0.96
2016-09-07T22:38:35.660538: step 1344, loss 0.0268736, acc 1
2016-09-07T22:38:36.341054: step 1345, loss 0.0788339, acc 0.96
2016-09-07T22:38:37.037203: step 1346, loss 0.0372834, acc 0.98
2016-09-07T22:38:37.730869: step 1347, loss 0.0395103, acc 1
2016-09-07T22:38:38.415991: step 1348, loss 0.131248, acc 0.94
2016-09-07T22:38:39.128037: step 1349, loss 0.0264421, acc 0.98
2016-09-07T22:38:39.835872: step 1350, loss 0.0308945, acc 0.98
2016-09-07T22:38:40.548180: step 1351, loss 0.201696, acc 0.92
2016-09-07T22:38:41.238688: step 1352, loss 0.121508, acc 0.98
2016-09-07T22:38:41.938723: step 1353, loss 0.0225603, acc 1
2016-09-07T22:38:42.619827: step 1354, loss 0.236915, acc 0.88
2016-09-07T22:38:43.316434: step 1355, loss 0.0572347, acc 0.98
2016-09-07T22:38:44.011008: step 1356, loss 0.0498097, acc 1
2016-09-07T22:38:44.700131: step 1357, loss 0.0363581, acc 1
2016-09-07T22:38:45.072171: step 1358, loss 0.25642, acc 0.833333
2016-09-07T22:38:45.790030: step 1359, loss 0.0336271, acc 0.98
2016-09-07T22:38:46.503293: step 1360, loss 0.144704, acc 0.96
2016-09-07T22:38:47.215027: step 1361, loss 0.22067, acc 0.92
2016-09-07T22:38:47.937814: step 1362, loss 0.0275369, acc 1
2016-09-07T22:38:48.651498: step 1363, loss 0.0606094, acc 0.98
2016-09-07T22:38:49.348568: step 1364, loss 0.0746672, acc 0.94
2016-09-07T22:38:50.047260: step 1365, loss 0.0574163, acc 1
2016-09-07T22:38:50.751595: step 1366, loss 0.0487833, acc 0.96
2016-09-07T22:38:51.470028: step 1367, loss 0.160672, acc 0.96
2016-09-07T22:38:52.180816: step 1368, loss 0.183067, acc 0.94
2016-09-07T22:38:52.893550: step 1369, loss 0.123285, acc 0.92
2016-09-07T22:38:53.599075: step 1370, loss 0.031229, acc 0.98
2016-09-07T22:38:54.300122: step 1371, loss 0.159332, acc 0.92
2016-09-07T22:38:55.026444: step 1372, loss 0.103072, acc 0.94
2016-09-07T22:38:55.725593: step 1373, loss 0.0547743, acc 0.98
2016-09-07T22:38:56.434219: step 1374, loss 0.0552425, acc 0.98
2016-09-07T22:38:57.128129: step 1375, loss 0.0472506, acc 0.98
2016-09-07T22:38:57.840306: step 1376, loss 0.0147697, acc 1
2016-09-07T22:38:58.511515: step 1377, loss 0.124896, acc 0.96
2016-09-07T22:38:59.187723: step 1378, loss 0.0416036, acc 0.98
2016-09-07T22:38:59.880339: step 1379, loss 0.145297, acc 0.94
2016-09-07T22:39:00.632343: step 1380, loss 0.0861769, acc 0.96
2016-09-07T22:39:01.351280: step 1381, loss 0.0337273, acc 1
2016-09-07T22:39:02.041407: step 1382, loss 0.0412043, acc 0.98
2016-09-07T22:39:02.730109: step 1383, loss 0.0331088, acc 1
2016-09-07T22:39:03.419891: step 1384, loss 0.0775331, acc 0.96
2016-09-07T22:39:04.113792: step 1385, loss 0.0682903, acc 0.98
2016-09-07T22:39:04.805315: step 1386, loss 0.103823, acc 0.92
2016-09-07T22:39:05.506185: step 1387, loss 0.142943, acc 0.96
2016-09-07T22:39:06.231679: step 1388, loss 0.0272836, acc 0.98
2016-09-07T22:39:06.942561: step 1389, loss 0.0651205, acc 0.96
2016-09-07T22:39:07.629890: step 1390, loss 0.0778146, acc 1
2016-09-07T22:39:08.337733: step 1391, loss 0.0466834, acc 0.98
2016-09-07T22:39:09.033515: step 1392, loss 0.138523, acc 0.96
2016-09-07T22:39:09.723107: step 1393, loss 0.0179811, acc 1
2016-09-07T22:39:10.448195: step 1394, loss 0.104313, acc 0.92
2016-09-07T22:39:11.162858: step 1395, loss 0.0519641, acc 0.98
2016-09-07T22:39:11.882071: step 1396, loss 0.119098, acc 0.96
2016-09-07T22:39:12.577214: step 1397, loss 0.0467448, acc 0.98
2016-09-07T22:39:13.276389: step 1398, loss 0.0528353, acc 0.98
2016-09-07T22:39:13.979587: step 1399, loss 0.073134, acc 0.98
2016-09-07T22:39:14.692040: step 1400, loss 0.136878, acc 0.92

Evaluation:
2016-09-07T22:39:17.872351: step 1400, loss 1.08675, acc 0.764

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-1400

2016-09-07T22:39:19.555265: step 1401, loss 0.0694233, acc 0.98
2016-09-07T22:39:20.264376: step 1402, loss 0.0128714, acc 1
2016-09-07T22:39:20.973465: step 1403, loss 0.0207899, acc 1
2016-09-07T22:39:21.670888: step 1404, loss 0.0339464, acc 0.98
2016-09-07T22:39:22.351427: step 1405, loss 0.160128, acc 0.92
2016-09-07T22:39:23.084691: step 1406, loss 0.202922, acc 0.92
2016-09-07T22:39:23.793747: step 1407, loss 0.120495, acc 0.98
2016-09-07T22:39:24.492716: step 1408, loss 0.0664662, acc 0.96
2016-09-07T22:39:25.182899: step 1409, loss 0.128547, acc 0.94
2016-09-07T22:39:25.867793: step 1410, loss 0.0578907, acc 0.98
2016-09-07T22:39:26.563516: step 1411, loss 0.0220721, acc 1
2016-09-07T22:39:27.258950: step 1412, loss 0.2557, acc 0.9
2016-09-07T22:39:27.953815: step 1413, loss 0.0271127, acc 1
2016-09-07T22:39:28.660742: step 1414, loss 0.0576324, acc 0.98
2016-09-07T22:39:29.349110: step 1415, loss 0.125597, acc 0.96
2016-09-07T22:39:30.044516: step 1416, loss 0.042196, acc 0.98
2016-09-07T22:39:30.731083: step 1417, loss 0.0238818, acc 1
2016-09-07T22:39:31.447276: step 1418, loss 0.04634, acc 1
2016-09-07T22:39:32.141063: step 1419, loss 0.10553, acc 0.96
2016-09-07T22:39:32.834272: step 1420, loss 0.137272, acc 0.96
2016-09-07T22:39:33.525936: step 1421, loss 0.0903841, acc 0.98
2016-09-07T22:39:34.215249: step 1422, loss 0.114599, acc 0.92
2016-09-07T22:39:34.919054: step 1423, loss 0.124778, acc 0.94
2016-09-07T22:39:35.638275: step 1424, loss 0.134383, acc 0.92
2016-09-07T22:39:36.348011: step 1425, loss 0.0665016, acc 0.96
2016-09-07T22:39:37.060008: step 1426, loss 0.0629337, acc 0.96
2016-09-07T22:39:37.779966: step 1427, loss 0.114653, acc 0.96
2016-09-07T22:39:38.482620: step 1428, loss 0.0616254, acc 1
2016-09-07T22:39:39.184773: step 1429, loss 0.177932, acc 0.94
2016-09-07T22:39:39.878089: step 1430, loss 0.0498281, acc 1
2016-09-07T22:39:40.578843: step 1431, loss 0.101438, acc 0.92
2016-09-07T22:39:41.267326: step 1432, loss 0.0496864, acc 0.98
2016-09-07T22:39:41.959067: step 1433, loss 0.0281439, acc 1
2016-09-07T22:39:42.640571: step 1434, loss 0.0635113, acc 0.98
2016-09-07T22:39:43.339146: step 1435, loss 0.0794912, acc 0.96
2016-09-07T22:39:44.050966: step 1436, loss 0.0703703, acc 0.96
2016-09-07T22:39:44.744291: step 1437, loss 0.138415, acc 0.96
2016-09-07T22:39:45.432165: step 1438, loss 0.0700589, acc 0.94
2016-09-07T22:39:46.124679: step 1439, loss 0.137342, acc 0.94
2016-09-07T22:39:46.843158: step 1440, loss 0.126433, acc 0.98
2016-09-07T22:39:47.528018: step 1441, loss 0.0694509, acc 0.96
2016-09-07T22:39:48.228150: step 1442, loss 0.0433137, acc 0.98
2016-09-07T22:39:48.920039: step 1443, loss 0.0515405, acc 0.96
2016-09-07T22:39:49.618059: step 1444, loss 0.0417769, acc 1
2016-09-07T22:39:50.308786: step 1445, loss 0.068429, acc 0.96
2016-09-07T22:39:51.002961: step 1446, loss 0.0304669, acc 0.98
2016-09-07T22:39:51.686337: step 1447, loss 0.0690804, acc 0.96
2016-09-07T22:39:52.389998: step 1448, loss 0.138424, acc 0.94
2016-09-07T22:39:53.090353: step 1449, loss 0.161952, acc 0.92
2016-09-07T22:39:53.812112: step 1450, loss 0.156242, acc 0.94
2016-09-07T22:39:54.504623: step 1451, loss 0.133842, acc 0.92
2016-09-07T22:39:55.207294: step 1452, loss 0.102985, acc 0.96
2016-09-07T22:39:55.898808: step 1453, loss 0.0808157, acc 0.98
2016-09-07T22:39:56.597891: step 1454, loss 0.0387249, acc 0.98
2016-09-07T22:39:57.299375: step 1455, loss 0.028722, acc 1
2016-09-07T22:39:57.985083: step 1456, loss 0.111118, acc 0.98
2016-09-07T22:39:58.689898: step 1457, loss 0.0815543, acc 0.96
2016-09-07T22:39:59.383539: step 1458, loss 0.0430422, acc 0.98
2016-09-07T22:40:00.094163: step 1459, loss 0.0669113, acc 0.98
2016-09-07T22:40:00.878823: step 1460, loss 0.0881698, acc 0.94
2016-09-07T22:40:01.579855: step 1461, loss 0.0544085, acc 1
2016-09-07T22:40:02.253024: step 1462, loss 0.0876426, acc 0.94
2016-09-07T22:40:02.927993: step 1463, loss 0.0418188, acc 0.98
2016-09-07T22:40:03.617908: step 1464, loss 0.169757, acc 0.92
2016-09-07T22:40:04.314541: step 1465, loss 0.0829149, acc 0.96
2016-09-07T22:40:05.011791: step 1466, loss 0.0453292, acc 0.98
2016-09-07T22:40:05.721597: step 1467, loss 0.10224, acc 0.94
2016-09-07T22:40:06.434050: step 1468, loss 0.107781, acc 0.96
2016-09-07T22:40:07.153269: step 1469, loss 0.13527, acc 0.96
2016-09-07T22:40:07.855652: step 1470, loss 0.0277895, acc 0.98
2016-09-07T22:40:08.586220: step 1471, loss 0.120798, acc 0.96
2016-09-07T22:40:09.275474: step 1472, loss 0.0946765, acc 0.96
2016-09-07T22:40:09.988236: step 1473, loss 0.0228982, acc 1
2016-09-07T22:40:10.676353: step 1474, loss 0.0532815, acc 0.98
2016-09-07T22:40:11.368287: step 1475, loss 0.0743201, acc 0.98
2016-09-07T22:40:12.066835: step 1476, loss 0.147511, acc 0.92
2016-09-07T22:40:12.784543: step 1477, loss 0.0422417, acc 0.96
2016-09-07T22:40:13.482424: step 1478, loss 0.129306, acc 0.92
2016-09-07T22:40:14.189677: step 1479, loss 0.0660448, acc 0.96
2016-09-07T22:40:14.883853: step 1480, loss 0.0360491, acc 1
2016-09-07T22:40:15.599582: step 1481, loss 0.0736099, acc 0.96
2016-09-07T22:40:16.302961: step 1482, loss 0.0598032, acc 0.98
2016-09-07T22:40:17.004700: step 1483, loss 0.184481, acc 0.9
2016-09-07T22:40:17.710282: step 1484, loss 0.0928044, acc 0.94
2016-09-07T22:40:18.433150: step 1485, loss 0.13184, acc 0.92
2016-09-07T22:40:19.122185: step 1486, loss 0.0489135, acc 0.96
2016-09-07T22:40:19.831987: step 1487, loss 0.0242065, acc 1
2016-09-07T22:40:20.506755: step 1488, loss 0.0161675, acc 0.98
2016-09-07T22:40:21.203633: step 1489, loss 0.052064, acc 0.96
2016-09-07T22:40:21.899192: step 1490, loss 0.0638605, acc 0.96
2016-09-07T22:40:22.598771: step 1491, loss 0.0362213, acc 1
2016-09-07T22:40:23.304165: step 1492, loss 0.0804371, acc 0.96
2016-09-07T22:40:24.016869: step 1493, loss 0.0871011, acc 0.98
2016-09-07T22:40:24.708707: step 1494, loss 0.0778161, acc 0.98
2016-09-07T22:40:25.403870: step 1495, loss 0.0518656, acc 0.96
2016-09-07T22:40:26.087809: step 1496, loss 0.0919554, acc 0.96
2016-09-07T22:40:26.776595: step 1497, loss 0.114089, acc 0.96
2016-09-07T22:40:27.488725: step 1498, loss 0.0411246, acc 0.98
2016-09-07T22:40:28.185539: step 1499, loss 0.067054, acc 0.96
2016-09-07T22:40:28.883474: step 1500, loss 0.0693764, acc 0.96

Evaluation:
2016-09-07T22:40:32.044844: step 1500, loss 1.02872, acc 0.765

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-1500

2016-09-07T22:40:33.768188: step 1501, loss 0.0713378, acc 0.98
2016-09-07T22:40:34.446357: step 1502, loss 0.029829, acc 1
2016-09-07T22:40:35.130146: step 1503, loss 0.0539144, acc 0.96
2016-09-07T22:40:35.831633: step 1504, loss 0.136224, acc 0.92
2016-09-07T22:40:36.538828: step 1505, loss 0.0871029, acc 0.94
2016-09-07T22:40:37.226081: step 1506, loss 0.0897949, acc 0.96
2016-09-07T22:40:37.927191: step 1507, loss 0.0227477, acc 1
2016-09-07T22:40:38.617553: step 1508, loss 0.0131965, acc 1
2016-09-07T22:40:39.325655: step 1509, loss 0.081414, acc 0.94
2016-09-07T22:40:40.012408: step 1510, loss 0.118653, acc 0.94
2016-09-07T22:40:40.713034: step 1511, loss 0.184643, acc 0.96
2016-09-07T22:40:41.405439: step 1512, loss 0.157796, acc 0.96
2016-09-07T22:40:42.114295: step 1513, loss 0.0724396, acc 0.94
2016-09-07T22:40:42.795038: step 1514, loss 0.0404001, acc 0.98
2016-09-07T22:40:43.481449: step 1515, loss 0.0567215, acc 0.98
2016-09-07T22:40:44.170942: step 1516, loss 0.108483, acc 0.98
2016-09-07T22:40:44.866412: step 1517, loss 0.0221739, acc 1
2016-09-07T22:40:45.560050: step 1518, loss 0.0554881, acc 0.98
2016-09-07T22:40:46.255470: step 1519, loss 0.0533658, acc 0.96
2016-09-07T22:40:46.956082: step 1520, loss 0.0369071, acc 0.98
2016-09-07T22:40:47.643095: step 1521, loss 0.0503309, acc 0.98
2016-09-07T22:40:48.340527: step 1522, loss 0.158222, acc 0.94
2016-09-07T22:40:49.046537: step 1523, loss 0.059891, acc 0.98
2016-09-07T22:40:49.735709: step 1524, loss 0.154219, acc 0.94
2016-09-07T22:40:50.432679: step 1525, loss 0.0810347, acc 0.96
2016-09-07T22:40:51.119131: step 1526, loss 0.0750438, acc 0.98
2016-09-07T22:40:51.806282: step 1527, loss 0.131743, acc 0.92
2016-09-07T22:40:52.506666: step 1528, loss 0.0812974, acc 0.96
2016-09-07T22:40:53.218113: step 1529, loss 0.172144, acc 0.92
2016-09-07T22:40:53.913494: step 1530, loss 0.0848705, acc 0.96
2016-09-07T22:40:54.624366: step 1531, loss 0.0484026, acc 0.96
2016-09-07T22:40:55.309317: step 1532, loss 0.0849757, acc 0.92
2016-09-07T22:40:56.054257: step 1533, loss 0.0597144, acc 1
2016-09-07T22:40:56.750180: step 1534, loss 0.0519295, acc 0.98
2016-09-07T22:40:57.445018: step 1535, loss 0.0511981, acc 1
2016-09-07T22:40:58.150161: step 1536, loss 0.0191546, acc 1
2016-09-07T22:40:58.841715: step 1537, loss 0.0643457, acc 0.98
2016-09-07T22:40:59.536525: step 1538, loss 0.105406, acc 0.96
2016-09-07T22:41:00.233396: step 1539, loss 0.0847804, acc 0.96
2016-09-07T22:41:00.928620: step 1540, loss 0.0458479, acc 0.96
2016-09-07T22:41:01.607507: step 1541, loss 0.0413086, acc 1
2016-09-07T22:41:02.319126: step 1542, loss 0.0513278, acc 0.98
2016-09-07T22:41:03.022644: step 1543, loss 0.0602378, acc 0.96
2016-09-07T22:41:03.712066: step 1544, loss 0.0259321, acc 1
2016-09-07T22:41:04.404712: step 1545, loss 0.149188, acc 0.94
2016-09-07T22:41:05.094130: step 1546, loss 0.0938215, acc 0.96
2016-09-07T22:41:05.799085: step 1547, loss 0.159203, acc 0.94
2016-09-07T22:41:06.480479: step 1548, loss 0.0608852, acc 0.98
2016-09-07T22:41:07.170023: step 1549, loss 0.0311706, acc 1
2016-09-07T22:41:07.882581: step 1550, loss 0.0620035, acc 0.98
2016-09-07T22:41:08.583199: step 1551, loss 0.0684176, acc 0.98
2016-09-07T22:41:08.962911: step 1552, loss 0.000246598, acc 1
2016-09-07T22:41:09.648593: step 1553, loss 0.0424222, acc 1
2016-09-07T22:41:10.355622: step 1554, loss 0.0344503, acc 0.98
2016-09-07T22:41:11.052059: step 1555, loss 0.0660737, acc 1
2016-09-07T22:41:11.751576: step 1556, loss 0.00369047, acc 1
2016-09-07T22:41:12.439401: step 1557, loss 0.0418124, acc 0.98
2016-09-07T22:41:13.131321: step 1558, loss 0.293265, acc 0.94
2016-09-07T22:41:13.807002: step 1559, loss 0.0304352, acc 0.98
2016-09-07T22:41:14.492971: step 1560, loss 0.072237, acc 0.94
2016-09-07T22:41:15.197263: step 1561, loss 0.0171066, acc 1
2016-09-07T22:41:15.897835: step 1562, loss 0.0954589, acc 0.98
2016-09-07T22:41:16.581364: step 1563, loss 0.0431103, acc 0.96
2016-09-07T22:41:17.272765: step 1564, loss 0.0370838, acc 0.98
2016-09-07T22:41:17.989960: step 1565, loss 0.161437, acc 0.9
2016-09-07T22:41:18.680589: step 1566, loss 0.0777898, acc 0.94
2016-09-07T22:41:19.364863: step 1567, loss 0.0117364, acc 1
2016-09-07T22:41:20.061633: step 1568, loss 0.0605198, acc 0.96
2016-09-07T22:41:20.778697: step 1569, loss 0.0927572, acc 0.96
2016-09-07T22:41:21.485364: step 1570, loss 0.066631, acc 0.98
2016-09-07T22:41:22.196143: step 1571, loss 0.0541321, acc 0.98
2016-09-07T22:41:22.927951: step 1572, loss 0.0408714, acc 0.98
2016-09-07T22:41:23.625336: step 1573, loss 0.0242849, acc 0.98
2016-09-07T22:41:24.317077: step 1574, loss 0.0567056, acc 0.98
2016-09-07T22:41:24.998004: step 1575, loss 0.0456244, acc 0.98
2016-09-07T22:41:25.696770: step 1576, loss 0.11644, acc 0.92
2016-09-07T22:41:26.384217: step 1577, loss 0.0204935, acc 0.98
2016-09-07T22:41:27.072999: step 1578, loss 0.0179946, acc 0.98
2016-09-07T22:41:27.762291: step 1579, loss 0.0350976, acc 0.98
2016-09-07T22:41:28.474227: step 1580, loss 0.15032, acc 0.96
2016-09-07T22:41:29.168443: step 1581, loss 0.0293069, acc 1
2016-09-07T22:41:29.867139: step 1582, loss 0.133688, acc 0.94
2016-09-07T22:41:30.578595: step 1583, loss 0.031393, acc 0.98
2016-09-07T22:41:31.280716: step 1584, loss 0.115859, acc 0.92
2016-09-07T22:41:31.979206: step 1585, loss 0.0551471, acc 0.98
2016-09-07T22:41:32.677824: step 1586, loss 0.0629711, acc 0.96
2016-09-07T22:41:33.360116: step 1587, loss 0.0337471, acc 0.98
2016-09-07T22:41:34.046574: step 1588, loss 0.0586993, acc 0.98
2016-09-07T22:41:34.743391: step 1589, loss 0.0400792, acc 0.98
2016-09-07T22:41:35.440392: step 1590, loss 0.04751, acc 0.98
2016-09-07T22:41:36.140780: step 1591, loss 0.0652183, acc 0.96
2016-09-07T22:41:36.849917: step 1592, loss 0.0654075, acc 0.96
2016-09-07T22:41:37.559190: step 1593, loss 0.197467, acc 0.9
2016-09-07T22:41:38.262694: step 1594, loss 0.13464, acc 0.94
2016-09-07T22:41:38.979791: step 1595, loss 0.0209275, acc 1
2016-09-07T22:41:39.683130: step 1596, loss 0.0299233, acc 0.98
2016-09-07T22:41:40.395438: step 1597, loss 0.115668, acc 0.96
2016-09-07T22:41:41.132813: step 1598, loss 0.06961, acc 0.94
2016-09-07T22:41:41.815626: step 1599, loss 0.0336763, acc 1
2016-09-07T22:41:42.515946: step 1600, loss 0.0847501, acc 0.96

Evaluation:
2016-09-07T22:41:45.693554: step 1600, loss 1.26012, acc 0.759

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-1600

2016-09-07T22:41:47.303214: step 1601, loss 0.0231938, acc 1
2016-09-07T22:41:47.993359: step 1602, loss 0.0943797, acc 0.96
2016-09-07T22:41:48.695620: step 1603, loss 0.0344679, acc 1
2016-09-07T22:41:49.387655: step 1604, loss 0.0643448, acc 0.98
2016-09-07T22:41:50.106523: step 1605, loss 0.023821, acc 0.98
2016-09-07T22:41:50.797101: step 1606, loss 0.0684726, acc 0.98
2016-09-07T22:41:51.505392: step 1607, loss 0.0713507, acc 0.96
2016-09-07T22:41:52.212566: step 1608, loss 0.103155, acc 0.96
2016-09-07T22:41:52.918646: step 1609, loss 0.133119, acc 0.96
2016-09-07T22:41:53.611460: step 1610, loss 0.00876027, acc 1
2016-09-07T22:41:54.319960: step 1611, loss 0.0426851, acc 0.96
2016-09-07T22:41:55.016506: step 1612, loss 0.0700969, acc 0.98
2016-09-07T22:41:55.695806: step 1613, loss 0.0701207, acc 0.96
2016-09-07T22:41:56.396650: step 1614, loss 0.0842792, acc 0.94
2016-09-07T22:41:57.089475: step 1615, loss 0.0712404, acc 0.98
2016-09-07T22:41:57.773237: step 1616, loss 0.107767, acc 0.98
2016-09-07T22:41:58.473458: step 1617, loss 0.0401563, acc 1
2016-09-07T22:41:59.154361: step 1618, loss 0.0891942, acc 0.96
2016-09-07T22:41:59.861864: step 1619, loss 0.136113, acc 0.92
2016-09-07T22:42:00.608058: step 1620, loss 0.0363477, acc 1
2016-09-07T22:42:01.310217: step 1621, loss 0.052573, acc 1
2016-09-07T22:42:01.994219: step 1622, loss 0.0625557, acc 0.96
2016-09-07T22:42:02.690169: step 1623, loss 0.0612572, acc 0.96
2016-09-07T22:42:03.384705: step 1624, loss 0.0872098, acc 0.96
2016-09-07T22:42:04.092798: step 1625, loss 0.0773175, acc 0.96
2016-09-07T22:42:04.786591: step 1626, loss 0.0443035, acc 0.98
2016-09-07T22:42:05.483664: step 1627, loss 0.0960096, acc 0.96
2016-09-07T22:42:06.179250: step 1628, loss 0.0538204, acc 1
2016-09-07T22:42:06.851598: step 1629, loss 0.0794038, acc 0.98
2016-09-07T22:42:07.553343: step 1630, loss 0.0570526, acc 1
2016-09-07T22:42:08.250299: step 1631, loss 0.0696476, acc 0.98
2016-09-07T22:42:08.981155: step 1632, loss 0.0615216, acc 0.96
2016-09-07T22:42:09.704645: step 1633, loss 0.0925884, acc 0.96
2016-09-07T22:42:10.405331: step 1634, loss 0.0853477, acc 0.96
2016-09-07T22:42:11.096344: step 1635, loss 0.115146, acc 0.94
2016-09-07T22:42:11.771782: step 1636, loss 0.0185364, acc 1
2016-09-07T22:42:12.478323: step 1637, loss 0.0822913, acc 0.96
2016-09-07T22:42:13.160289: step 1638, loss 0.0152409, acc 1
2016-09-07T22:42:13.840679: step 1639, loss 0.0410125, acc 0.98
2016-09-07T22:42:14.531741: step 1640, loss 0.0725982, acc 0.98
2016-09-07T22:42:15.230569: step 1641, loss 0.110337, acc 0.96
2016-09-07T22:42:15.920328: step 1642, loss 0.143422, acc 0.92
2016-09-07T22:42:16.607688: step 1643, loss 0.0774475, acc 0.96
2016-09-07T22:42:17.326590: step 1644, loss 0.0105114, acc 1
2016-09-07T22:42:18.039504: step 1645, loss 0.10346, acc 0.96
2016-09-07T22:42:18.743271: step 1646, loss 0.0967763, acc 0.94
2016-09-07T22:42:19.423472: step 1647, loss 0.0589977, acc 0.98
2016-09-07T22:42:20.108107: step 1648, loss 0.0151295, acc 1
2016-09-07T22:42:20.794405: step 1649, loss 0.0554207, acc 0.98
2016-09-07T22:42:21.471547: step 1650, loss 0.0824943, acc 0.94
2016-09-07T22:42:22.157075: step 1651, loss 0.0644692, acc 0.94
2016-09-07T22:42:22.850334: step 1652, loss 0.0265892, acc 0.98
2016-09-07T22:42:23.528635: step 1653, loss 0.00609922, acc 1
2016-09-07T22:42:24.226365: step 1654, loss 0.00998779, acc 1
2016-09-07T22:42:24.920372: step 1655, loss 0.0455995, acc 0.96
2016-09-07T22:42:25.624413: step 1656, loss 0.07044, acc 0.96
2016-09-07T22:42:26.324660: step 1657, loss 0.045495, acc 0.98
2016-09-07T22:42:27.025505: step 1658, loss 0.285368, acc 0.9
2016-09-07T22:42:27.733585: step 1659, loss 0.0615398, acc 0.96
2016-09-07T22:42:28.450272: step 1660, loss 0.0475712, acc 0.98
2016-09-07T22:42:29.144279: step 1661, loss 0.120142, acc 0.96
2016-09-07T22:42:29.853196: step 1662, loss 0.0157968, acc 1
2016-09-07T22:42:30.549304: step 1663, loss 0.0554052, acc 0.96
2016-09-07T22:42:31.250913: step 1664, loss 0.00762238, acc 1
2016-09-07T22:42:31.956013: step 1665, loss 0.0514225, acc 0.98
2016-09-07T22:42:32.663522: step 1666, loss 0.0596459, acc 0.98
2016-09-07T22:42:33.382112: step 1667, loss 0.149924, acc 0.96
2016-09-07T22:42:34.080186: step 1668, loss 0.0327257, acc 0.98
2016-09-07T22:42:34.765141: step 1669, loss 0.0831395, acc 0.96
2016-09-07T22:42:35.461575: step 1670, loss 0.0519266, acc 0.98
2016-09-07T22:42:36.179722: step 1671, loss 0.0523, acc 0.98
2016-09-07T22:42:36.878394: step 1672, loss 0.0750351, acc 0.98
2016-09-07T22:42:37.559044: step 1673, loss 0.07143, acc 0.98
2016-09-07T22:42:38.253105: step 1674, loss 0.0917704, acc 0.94
2016-09-07T22:42:38.957717: step 1675, loss 0.0166645, acc 1
2016-09-07T22:42:39.654756: step 1676, loss 0.0369032, acc 0.98
2016-09-07T22:42:40.360660: step 1677, loss 0.139146, acc 0.9
2016-09-07T22:42:41.066112: step 1678, loss 0.0402153, acc 1
2016-09-07T22:42:41.766458: step 1679, loss 0.213713, acc 0.94
2016-09-07T22:42:42.474016: step 1680, loss 0.0105576, acc 1
2016-09-07T22:42:43.215878: step 1681, loss 0.11968, acc 0.98
2016-09-07T22:42:43.963626: step 1682, loss 0.0987374, acc 0.98
2016-09-07T22:42:44.692848: step 1683, loss 0.0705897, acc 0.98
2016-09-07T22:42:45.396782: step 1684, loss 0.0628368, acc 1
2016-09-07T22:42:46.098310: step 1685, loss 0.0435117, acc 0.98
2016-09-07T22:42:46.809061: step 1686, loss 0.0578053, acc 0.98
2016-09-07T22:42:47.513798: step 1687, loss 0.108452, acc 0.92
2016-09-07T22:42:48.247744: step 1688, loss 0.0228998, acc 1
2016-09-07T22:42:48.944764: step 1689, loss 0.0752614, acc 0.98
2016-09-07T22:42:49.629554: step 1690, loss 0.081929, acc 0.94
2016-09-07T22:42:50.311085: step 1691, loss 0.0126556, acc 1
2016-09-07T22:42:50.984307: step 1692, loss 0.0736389, acc 0.96
2016-09-07T22:42:51.664371: step 1693, loss 0.0906865, acc 0.96
2016-09-07T22:42:52.360447: step 1694, loss 0.0639382, acc 0.98
2016-09-07T22:42:53.050334: step 1695, loss 0.0390268, acc 1
2016-09-07T22:42:53.743487: step 1696, loss 0.0223585, acc 1
2016-09-07T22:42:54.450789: step 1697, loss 0.0150421, acc 1
2016-09-07T22:42:55.157973: step 1698, loss 0.0786973, acc 0.96
2016-09-07T22:42:55.860466: step 1699, loss 0.039527, acc 0.98
2016-09-07T22:42:56.569844: step 1700, loss 0.0405882, acc 1

Evaluation:
2016-09-07T22:42:59.729441: step 1700, loss 1.23285, acc 0.764

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-1700

2016-09-07T22:43:01.409153: step 1701, loss 0.0699101, acc 0.94
2016-09-07T22:43:02.125188: step 1702, loss 0.0319071, acc 0.98
2016-09-07T22:43:02.838003: step 1703, loss 0.058558, acc 0.98
2016-09-07T22:43:03.527762: step 1704, loss 0.154807, acc 0.94
2016-09-07T22:43:04.232964: step 1705, loss 0.0379074, acc 0.98
2016-09-07T22:43:04.937358: step 1706, loss 0.313862, acc 0.9
2016-09-07T22:43:05.626856: step 1707, loss 0.0873392, acc 0.94
2016-09-07T22:43:06.330094: step 1708, loss 0.0144193, acc 1
2016-09-07T22:43:07.029281: step 1709, loss 0.0493984, acc 0.98
2016-09-07T22:43:07.710083: step 1710, loss 0.25367, acc 0.92
2016-09-07T22:43:08.433294: step 1711, loss 0.158231, acc 0.96
2016-09-07T22:43:09.157351: step 1712, loss 0.059223, acc 0.94
2016-09-07T22:43:09.859972: step 1713, loss 0.0812791, acc 0.94
2016-09-07T22:43:10.566219: step 1714, loss 0.0188205, acc 1
2016-09-07T22:43:11.273852: step 1715, loss 0.0297241, acc 1
2016-09-07T22:43:11.975301: step 1716, loss 0.0587529, acc 0.98
2016-09-07T22:43:12.681009: step 1717, loss 0.0578975, acc 0.96
2016-09-07T22:43:13.385871: step 1718, loss 0.0808638, acc 0.96
2016-09-07T22:43:14.087459: step 1719, loss 0.0186218, acc 1
2016-09-07T22:43:14.776905: step 1720, loss 0.0716723, acc 1
2016-09-07T22:43:15.478380: step 1721, loss 0.0719266, acc 0.98
2016-09-07T22:43:16.211507: step 1722, loss 0.0750685, acc 0.96
2016-09-07T22:43:16.908182: step 1723, loss 0.155935, acc 0.94
2016-09-07T22:43:17.601546: step 1724, loss 0.0961866, acc 0.96
2016-09-07T22:43:18.292340: step 1725, loss 0.108923, acc 0.94
2016-09-07T22:43:19.000780: step 1726, loss 0.0156417, acc 1
2016-09-07T22:43:19.690125: step 1727, loss 0.0413572, acc 1
2016-09-07T22:43:20.389412: step 1728, loss 0.0835028, acc 0.96
2016-09-07T22:43:21.084205: step 1729, loss 0.0308879, acc 0.98
2016-09-07T22:43:21.784599: step 1730, loss 0.0678053, acc 0.98
2016-09-07T22:43:22.481617: step 1731, loss 0.0432119, acc 0.98
2016-09-07T22:43:23.197288: step 1732, loss 0.0762406, acc 0.96
2016-09-07T22:43:23.897266: step 1733, loss 0.0635024, acc 0.96
2016-09-07T22:43:24.601257: step 1734, loss 0.0331418, acc 0.98
2016-09-07T22:43:25.294368: step 1735, loss 0.0499456, acc 0.98
2016-09-07T22:43:26.003504: step 1736, loss 0.082678, acc 0.98
2016-09-07T22:43:26.735915: step 1737, loss 0.0611649, acc 1
2016-09-07T22:43:27.419311: step 1738, loss 0.0277017, acc 1
2016-09-07T22:43:28.126667: step 1739, loss 0.13834, acc 0.92
2016-09-07T22:43:28.826770: step 1740, loss 0.0357578, acc 0.98
2016-09-07T22:43:29.530945: step 1741, loss 0.0270534, acc 1
2016-09-07T22:43:30.213920: step 1742, loss 0.073513, acc 0.98
2016-09-07T22:43:30.902792: step 1743, loss 0.223269, acc 0.9
2016-09-07T22:43:31.600566: step 1744, loss 0.111347, acc 0.96
2016-09-07T22:43:32.296074: step 1745, loss 0.0648467, acc 0.96
2016-09-07T22:43:32.656587: step 1746, loss 0.0022908, acc 1
2016-09-07T22:43:33.363197: step 1747, loss 0.0861876, acc 0.98
2016-09-07T22:43:34.061802: step 1748, loss 0.121074, acc 0.98
2016-09-07T22:43:34.757290: step 1749, loss 0.0601011, acc 0.98
2016-09-07T22:43:35.461711: step 1750, loss 0.00677948, acc 1
2016-09-07T22:43:36.178886: step 1751, loss 0.045966, acc 0.98
2016-09-07T22:43:36.873151: step 1752, loss 0.0238564, acc 1
2016-09-07T22:43:37.570117: step 1753, loss 0.0386352, acc 0.98
2016-09-07T22:43:38.279514: step 1754, loss 0.115576, acc 0.96
2016-09-07T22:43:38.996622: step 1755, loss 0.0261965, acc 1
2016-09-07T22:43:39.703422: step 1756, loss 0.017285, acc 1
2016-09-07T22:43:40.402774: step 1757, loss 0.0310431, acc 1
2016-09-07T22:43:41.104172: step 1758, loss 0.0582167, acc 0.98
2016-09-07T22:43:41.810078: step 1759, loss 0.0278223, acc 1
2016-09-07T22:43:42.499416: step 1760, loss 0.0866082, acc 0.92
2016-09-07T22:43:43.203416: step 1761, loss 0.00329869, acc 1
2016-09-07T22:43:43.907064: step 1762, loss 0.0519475, acc 0.98
2016-09-07T22:43:44.597487: step 1763, loss 0.0213548, acc 0.98
2016-09-07T22:43:45.296371: step 1764, loss 0.110115, acc 0.94
2016-09-07T22:43:46.027137: step 1765, loss 0.0623669, acc 0.98
2016-09-07T22:43:46.727786: step 1766, loss 0.208479, acc 0.96
2016-09-07T22:43:47.419295: step 1767, loss 0.0901339, acc 0.94
2016-09-07T22:43:48.113566: step 1768, loss 0.0422229, acc 0.96
2016-09-07T22:43:48.789997: step 1769, loss 0.0167991, acc 1
2016-09-07T22:43:49.486402: step 1770, loss 0.0509649, acc 0.96
2016-09-07T22:43:50.190310: step 1771, loss 0.0369578, acc 1
2016-09-07T22:43:50.884435: step 1772, loss 0.0117361, acc 1
2016-09-07T22:43:51.570445: step 1773, loss 0.0141779, acc 1
2016-09-07T22:43:52.263492: step 1774, loss 0.0736369, acc 0.98
2016-09-07T22:43:52.949240: step 1775, loss 0.0704797, acc 0.98
2016-09-07T22:43:53.656212: step 1776, loss 0.0222666, acc 1
2016-09-07T22:43:54.362854: step 1777, loss 0.0617746, acc 0.96
2016-09-07T22:43:55.064833: step 1778, loss 0.0429005, acc 0.98
2016-09-07T22:43:55.778560: step 1779, loss 0.0392873, acc 1
2016-09-07T22:43:56.457542: step 1780, loss 0.0386889, acc 0.98
2016-09-07T22:43:57.158407: step 1781, loss 0.0898356, acc 0.96
2016-09-07T22:43:57.867416: step 1782, loss 0.038945, acc 1
2016-09-07T22:43:58.573719: step 1783, loss 0.012063, acc 1
2016-09-07T22:43:59.271606: step 1784, loss 0.0787597, acc 0.96
2016-09-07T22:43:59.949448: step 1785, loss 0.00658201, acc 1
2016-09-07T22:44:00.677186: step 1786, loss 0.0100322, acc 1
2016-09-07T22:44:01.368691: step 1787, loss 0.0795713, acc 0.96
2016-09-07T22:44:02.094457: step 1788, loss 0.017964, acc 0.98
2016-09-07T22:44:02.786435: step 1789, loss 0.0107229, acc 1
2016-09-07T22:44:03.492862: step 1790, loss 0.0136121, acc 1
2016-09-07T22:44:04.193342: step 1791, loss 0.0145244, acc 1
2016-09-07T22:44:04.860571: step 1792, loss 0.130075, acc 0.92
2016-09-07T22:44:05.552866: step 1793, loss 0.100452, acc 0.92
2016-09-07T22:44:06.256615: step 1794, loss 0.0932306, acc 0.94
2016-09-07T22:44:06.948252: step 1795, loss 0.0246951, acc 1
2016-09-07T22:44:07.658578: step 1796, loss 0.0191913, acc 1
2016-09-07T22:44:08.342166: step 1797, loss 0.0191308, acc 1
2016-09-07T22:44:09.050600: step 1798, loss 0.0429577, acc 0.98
2016-09-07T22:44:09.740014: step 1799, loss 0.0439625, acc 0.96
2016-09-07T22:44:10.443265: step 1800, loss 0.141856, acc 0.98

Evaluation:
2016-09-07T22:44:13.651790: step 1800, loss 1.52312, acc 0.755

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-1800

2016-09-07T22:44:15.355188: step 1801, loss 0.0297154, acc 0.98
2016-09-07T22:44:16.065742: step 1802, loss 0.114251, acc 0.94
2016-09-07T22:44:16.756381: step 1803, loss 0.0607036, acc 0.96
2016-09-07T22:44:17.454954: step 1804, loss 0.0559179, acc 0.98
2016-09-07T22:44:18.145158: step 1805, loss 0.0319051, acc 0.98
2016-09-07T22:44:18.833008: step 1806, loss 0.0443612, acc 0.98
2016-09-07T22:44:19.526723: step 1807, loss 0.0933798, acc 0.96
2016-09-07T22:44:20.218661: step 1808, loss 0.249668, acc 0.94
2016-09-07T22:44:20.914871: step 1809, loss 0.12715, acc 0.94
2016-09-07T22:44:21.607669: step 1810, loss 0.0338404, acc 0.98
2016-09-07T22:44:22.310540: step 1811, loss 0.0460962, acc 1
2016-09-07T22:44:23.014553: step 1812, loss 0.122305, acc 0.96
2016-09-07T22:44:23.692754: step 1813, loss 0.0210736, acc 1
2016-09-07T22:44:24.397230: step 1814, loss 0.0766588, acc 0.96
2016-09-07T22:44:25.110450: step 1815, loss 0.0262991, acc 0.98
2016-09-07T22:44:25.812539: step 1816, loss 0.0136193, acc 1
2016-09-07T22:44:26.533656: step 1817, loss 0.0352117, acc 0.98
2016-09-07T22:44:27.248930: step 1818, loss 0.047803, acc 0.96
2016-09-07T22:44:27.954819: step 1819, loss 0.0389758, acc 0.98
2016-09-07T22:44:28.674224: step 1820, loss 0.017864, acc 1
2016-09-07T22:44:29.355966: step 1821, loss 0.0282066, acc 0.98
2016-09-07T22:44:30.059075: step 1822, loss 0.0978997, acc 0.96
2016-09-07T22:44:30.752421: step 1823, loss 0.0910017, acc 0.98
2016-09-07T22:44:31.456111: step 1824, loss 0.120976, acc 0.96
2016-09-07T22:44:32.169329: step 1825, loss 0.14547, acc 0.96
2016-09-07T22:44:32.864684: step 1826, loss 0.0283091, acc 0.98
2016-09-07T22:44:33.572784: step 1827, loss 0.0840697, acc 0.96
2016-09-07T22:44:34.270801: step 1828, loss 0.0148439, acc 1
2016-09-07T22:44:34.981927: step 1829, loss 0.0886467, acc 0.94
2016-09-07T22:44:35.674006: step 1830, loss 0.0180664, acc 1
2016-09-07T22:44:36.359437: step 1831, loss 0.0457674, acc 0.96
2016-09-07T22:44:37.067183: step 1832, loss 0.0218394, acc 1
2016-09-07T22:44:37.763086: step 1833, loss 0.139764, acc 0.96
2016-09-07T22:44:38.472467: step 1834, loss 0.045619, acc 0.98
2016-09-07T22:44:39.198323: step 1835, loss 0.0567491, acc 0.98
2016-09-07T22:44:39.886603: step 1836, loss 0.00993582, acc 1
2016-09-07T22:44:40.576126: step 1837, loss 0.100966, acc 0.96
2016-09-07T22:44:41.278794: step 1838, loss 0.0471824, acc 0.98
2016-09-07T22:44:41.970345: step 1839, loss 0.159201, acc 0.92
2016-09-07T22:44:42.668029: step 1840, loss 0.0160441, acc 1
2016-09-07T22:44:43.353822: step 1841, loss 0.0511412, acc 1
2016-09-07T22:44:44.055196: step 1842, loss 0.0231313, acc 0.98
2016-09-07T22:44:44.757611: step 1843, loss 0.0807983, acc 0.98
2016-09-07T22:44:45.445878: step 1844, loss 0.0779944, acc 0.94
2016-09-07T22:44:46.149715: step 1845, loss 0.0119521, acc 1
2016-09-07T22:44:46.836009: step 1846, loss 0.0872606, acc 0.94
2016-09-07T22:44:47.526028: step 1847, loss 0.0608876, acc 0.96
2016-09-07T22:44:48.234701: step 1848, loss 0.0298638, acc 0.98
2016-09-07T22:44:48.954375: step 1849, loss 0.07298, acc 0.96
2016-09-07T22:44:49.679846: step 1850, loss 0.0277767, acc 0.98
2016-09-07T22:44:50.378092: step 1851, loss 0.0790434, acc 0.96
2016-09-07T22:44:51.065298: step 1852, loss 0.0679078, acc 0.94
2016-09-07T22:44:51.759534: step 1853, loss 0.102402, acc 0.96
2016-09-07T22:44:52.452452: step 1854, loss 0.118974, acc 0.98
2016-09-07T22:44:53.159707: step 1855, loss 0.0917033, acc 0.94
2016-09-07T22:44:53.874973: step 1856, loss 0.067643, acc 0.98
2016-09-07T22:44:54.569806: step 1857, loss 0.167198, acc 0.94
2016-09-07T22:44:55.288340: step 1858, loss 0.0259958, acc 0.98
2016-09-07T22:44:55.986165: step 1859, loss 0.0363083, acc 0.98
2016-09-07T22:44:56.682839: step 1860, loss 0.0940296, acc 0.96
2016-09-07T22:44:57.385315: step 1861, loss 0.101921, acc 0.98
2016-09-07T22:44:58.083003: step 1862, loss 0.0255362, acc 1
2016-09-07T22:44:58.765363: step 1863, loss 0.0924954, acc 0.94
2016-09-07T22:44:59.466142: step 1864, loss 0.057887, acc 0.96
2016-09-07T22:45:00.192875: step 1865, loss 0.0412369, acc 0.98
2016-09-07T22:45:00.911994: step 1866, loss 0.0658586, acc 0.96
2016-09-07T22:45:01.616836: step 1867, loss 0.0593729, acc 0.98
2016-09-07T22:45:02.322311: step 1868, loss 0.189091, acc 0.94
2016-09-07T22:45:03.017952: step 1869, loss 0.0433458, acc 0.98
2016-09-07T22:45:03.711817: step 1870, loss 0.056495, acc 0.96
2016-09-07T22:45:04.429371: step 1871, loss 0.11955, acc 0.92
2016-09-07T22:45:05.139470: step 1872, loss 0.0573164, acc 0.98
2016-09-07T22:45:05.830690: step 1873, loss 0.117294, acc 0.96
2016-09-07T22:45:06.536249: step 1874, loss 0.0129745, acc 1
2016-09-07T22:45:07.231571: step 1875, loss 0.0386657, acc 0.98
2016-09-07T22:45:07.940691: step 1876, loss 0.0225718, acc 1
2016-09-07T22:45:08.644419: step 1877, loss 0.105948, acc 0.96
2016-09-07T22:45:09.348445: step 1878, loss 0.0978787, acc 0.98
2016-09-07T22:45:10.042590: step 1879, loss 0.054951, acc 0.98
2016-09-07T22:45:10.738852: step 1880, loss 0.054243, acc 0.96
2016-09-07T22:45:11.440124: step 1881, loss 0.0641876, acc 0.98
2016-09-07T22:45:12.158610: step 1882, loss 0.0564935, acc 0.96
2016-09-07T22:45:12.861341: step 1883, loss 0.062746, acc 0.96
2016-09-07T22:45:13.549698: step 1884, loss 0.0943431, acc 0.96
2016-09-07T22:45:14.251970: step 1885, loss 0.0659283, acc 0.96
2016-09-07T22:45:14.947139: step 1886, loss 0.0410562, acc 1
2016-09-07T22:45:15.639330: step 1887, loss 0.107348, acc 0.98
2016-09-07T22:45:16.335277: step 1888, loss 0.0574219, acc 0.98
2016-09-07T22:45:17.046154: step 1889, loss 0.0198247, acc 1
2016-09-07T22:45:17.724241: step 1890, loss 0.0514842, acc 0.98
2016-09-07T22:45:18.439859: step 1891, loss 0.0416993, acc 0.96
2016-09-07T22:45:19.151203: step 1892, loss 0.0396632, acc 0.98
2016-09-07T22:45:19.843293: step 1893, loss 0.107434, acc 0.98
2016-09-07T22:45:20.552757: step 1894, loss 0.119817, acc 0.96
2016-09-07T22:45:21.249332: step 1895, loss 0.088847, acc 0.94
2016-09-07T22:45:21.938286: step 1896, loss 0.059578, acc 0.98
2016-09-07T22:45:22.647547: step 1897, loss 0.126673, acc 0.98
2016-09-07T22:45:23.340700: step 1898, loss 0.0881468, acc 0.98
2016-09-07T22:45:24.036590: step 1899, loss 0.0864132, acc 0.94
2016-09-07T22:45:24.738181: step 1900, loss 0.183474, acc 0.98

Evaluation:
2016-09-07T22:45:27.890691: step 1900, loss 1.16777, acc 0.767

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-1900

2016-09-07T22:45:29.662308: step 1901, loss 0.0230522, acc 1
2016-09-07T22:45:30.352360: step 1902, loss 0.103692, acc 0.98
2016-09-07T22:45:31.052082: step 1903, loss 0.0613997, acc 0.98
2016-09-07T22:45:31.749587: step 1904, loss 0.120631, acc 0.96
2016-09-07T22:45:32.445595: step 1905, loss 0.0115007, acc 1
2016-09-07T22:45:33.165084: step 1906, loss 0.0284704, acc 1
2016-09-07T22:45:33.854461: step 1907, loss 0.0934517, acc 0.94
2016-09-07T22:45:34.545379: step 1908, loss 0.027086, acc 1
2016-09-07T22:45:35.226751: step 1909, loss 0.0370634, acc 0.98
2016-09-07T22:45:35.913233: step 1910, loss 0.0816998, acc 0.96
2016-09-07T22:45:36.616622: step 1911, loss 0.0410891, acc 0.98
2016-09-07T22:45:37.356478: step 1912, loss 0.048321, acc 0.98
2016-09-07T22:45:38.059871: step 1913, loss 0.0120288, acc 1
2016-09-07T22:45:38.742636: step 1914, loss 0.0499285, acc 0.94
2016-09-07T22:45:39.422914: step 1915, loss 0.111214, acc 0.92
2016-09-07T22:45:40.121769: step 1916, loss 0.143356, acc 0.94
2016-09-07T22:45:40.822140: step 1917, loss 0.0541742, acc 0.98
2016-09-07T22:45:41.517795: step 1918, loss 0.109153, acc 0.98
2016-09-07T22:45:42.222641: step 1919, loss 0.0222774, acc 0.98
2016-09-07T22:45:42.940884: step 1920, loss 0.00960388, acc 1
2016-09-07T22:45:43.641415: step 1921, loss 0.112758, acc 0.96
2016-09-07T22:45:44.337459: step 1922, loss 0.056813, acc 0.98
2016-09-07T22:45:45.032387: step 1923, loss 0.0325748, acc 0.98
2016-09-07T22:45:45.734027: step 1924, loss 0.0646388, acc 0.96
2016-09-07T22:45:46.441520: step 1925, loss 0.0908033, acc 0.92
2016-09-07T22:45:47.155727: step 1926, loss 0.02324, acc 1
2016-09-07T22:45:47.862592: step 1927, loss 0.116672, acc 0.94
2016-09-07T22:45:48.581682: step 1928, loss 0.115353, acc 0.98
2016-09-07T22:45:49.293806: step 1929, loss 0.0538851, acc 1
2016-09-07T22:45:49.995211: step 1930, loss 0.0765433, acc 0.98
2016-09-07T22:45:50.693020: step 1931, loss 0.0171302, acc 1
2016-09-07T22:45:51.402127: step 1932, loss 0.0333307, acc 0.98
2016-09-07T22:45:52.107019: step 1933, loss 0.0992992, acc 0.96
2016-09-07T22:45:52.817428: step 1934, loss 0.0177126, acc 1
2016-09-07T22:45:53.534979: step 1935, loss 0.0836272, acc 0.96
2016-09-07T22:45:54.235324: step 1936, loss 0.0540997, acc 1
2016-09-07T22:45:54.944622: step 1937, loss 0.0296666, acc 1
2016-09-07T22:45:55.631630: step 1938, loss 0.119928, acc 0.98
2016-09-07T22:45:56.334406: step 1939, loss 0.0648234, acc 0.96
2016-09-07T22:45:56.711730: step 1940, loss 0.579129, acc 0.916667
2016-09-07T22:45:57.406110: step 1941, loss 0.0513387, acc 0.96
2016-09-07T22:45:58.108748: step 1942, loss 0.0751756, acc 0.98
2016-09-07T22:45:58.815414: step 1943, loss 0.0752358, acc 0.98
2016-09-07T22:45:59.519708: step 1944, loss 0.122233, acc 0.96
2016-09-07T22:46:00.245797: step 1945, loss 0.218009, acc 0.96
2016-09-07T22:46:00.933327: step 1946, loss 0.0726724, acc 0.96
2016-09-07T22:46:01.613499: step 1947, loss 0.0786354, acc 0.98
2016-09-07T22:46:02.302974: step 1948, loss 0.0391773, acc 1
2016-09-07T22:46:03.011215: step 1949, loss 0.0526137, acc 0.98
2016-09-07T22:46:03.710406: step 1950, loss 0.0441908, acc 0.96
2016-09-07T22:46:04.406653: step 1951, loss 0.0578048, acc 0.96
2016-09-07T22:46:05.116360: step 1952, loss 0.0899055, acc 0.96
2016-09-07T22:46:05.847782: step 1953, loss 0.0593577, acc 1
2016-09-07T22:46:06.548405: step 1954, loss 0.0818407, acc 0.96
2016-09-07T22:46:07.259076: step 1955, loss 0.109942, acc 0.94
2016-09-07T22:46:07.953314: step 1956, loss 0.0238318, acc 1
2016-09-07T22:46:08.645793: step 1957, loss 0.13572, acc 0.92
2016-09-07T22:46:09.346433: step 1958, loss 0.0904063, acc 0.98
2016-09-07T22:46:10.053098: step 1959, loss 0.125262, acc 0.9
2016-09-07T22:46:10.743028: step 1960, loss 0.0292213, acc 0.98
2016-09-07T22:46:11.448795: step 1961, loss 0.0476517, acc 0.98
2016-09-07T22:46:12.143518: step 1962, loss 0.0698991, acc 0.98
2016-09-07T22:46:12.837185: step 1963, loss 0.0222708, acc 1
2016-09-07T22:46:13.533664: step 1964, loss 0.00246437, acc 1
2016-09-07T22:46:14.220502: step 1965, loss 0.0292005, acc 1
2016-09-07T22:46:14.906263: step 1966, loss 0.0558355, acc 0.98
2016-09-07T22:46:15.617285: step 1967, loss 0.0153554, acc 1
2016-09-07T22:46:16.345555: step 1968, loss 0.044632, acc 0.98
2016-09-07T22:46:17.048450: step 1969, loss 0.100777, acc 0.96
2016-09-07T22:46:17.760832: step 1970, loss 0.0439689, acc 0.98
2016-09-07T22:46:18.455563: step 1971, loss 0.00256439, acc 1
2016-09-07T22:46:19.167786: step 1972, loss 0.113614, acc 0.92
2016-09-07T22:46:19.864642: step 1973, loss 0.0485104, acc 0.98
2016-09-07T22:46:20.551570: step 1974, loss 0.0149036, acc 1
2016-09-07T22:46:21.257828: step 1975, loss 0.0858129, acc 0.94
2016-09-07T22:46:21.953027: step 1976, loss 0.0925286, acc 0.96
2016-09-07T22:46:22.678953: step 1977, loss 0.0661561, acc 0.96
2016-09-07T22:46:23.410515: step 1978, loss 0.135992, acc 0.92
2016-09-07T22:46:24.103883: step 1979, loss 0.0552953, acc 0.98
2016-09-07T22:46:24.808663: step 1980, loss 0.073158, acc 0.96
2016-09-07T22:46:25.514341: step 1981, loss 0.155166, acc 0.94
2016-09-07T22:46:26.220467: step 1982, loss 0.218936, acc 0.96
2016-09-07T22:46:26.908840: step 1983, loss 0.00733691, acc 1
2016-09-07T22:46:27.606926: step 1984, loss 0.0244891, acc 0.98
2016-09-07T22:46:28.309534: step 1985, loss 0.0340129, acc 1
2016-09-07T22:46:29.000199: step 1986, loss 0.112822, acc 0.96
2016-09-07T22:46:29.713895: step 1987, loss 0.0900209, acc 0.94
2016-09-07T22:46:30.410096: step 1988, loss 0.0067949, acc 1
2016-09-07T22:46:31.115289: step 1989, loss 0.084442, acc 0.96
2016-09-07T22:46:31.822305: step 1990, loss 0.0911223, acc 0.96
2016-09-07T22:46:32.521587: step 1991, loss 0.091685, acc 0.96
2016-09-07T22:46:33.225081: step 1992, loss 0.0325783, acc 0.98
2016-09-07T22:46:33.926314: step 1993, loss 0.0265395, acc 1
2016-09-07T22:46:34.628077: step 1994, loss 0.153723, acc 0.96
2016-09-07T22:46:35.318044: step 1995, loss 0.0494239, acc 0.98
2016-09-07T22:46:36.005926: step 1996, loss 0.165145, acc 0.92
2016-09-07T22:46:36.693560: step 1997, loss 0.0536668, acc 0.94
2016-09-07T22:46:37.404750: step 1998, loss 0.0306335, acc 0.98
2016-09-07T22:46:38.097647: step 1999, loss 0.0539157, acc 0.96
2016-09-07T22:46:38.817391: step 2000, loss 0.0589508, acc 0.98

Evaluation:
2016-09-07T22:46:42.004348: step 2000, loss 1.02888, acc 0.752

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-2000

2016-09-07T22:46:43.692491: step 2001, loss 0.0747582, acc 0.96
2016-09-07T22:46:44.397538: step 2002, loss 0.0565269, acc 0.98
2016-09-07T22:46:45.099465: step 2003, loss 0.0965874, acc 0.94
2016-09-07T22:46:45.815753: step 2004, loss 0.089408, acc 0.94
2016-09-07T22:46:46.528406: step 2005, loss 0.0475949, acc 1
2016-09-07T22:46:47.227879: step 2006, loss 0.0651594, acc 0.98
2016-09-07T22:46:47.936252: step 2007, loss 0.0787089, acc 0.96
2016-09-07T22:46:48.633725: step 2008, loss 0.0645546, acc 0.96
2016-09-07T22:46:49.339583: step 2009, loss 0.0588652, acc 0.96
2016-09-07T22:46:50.044532: step 2010, loss 0.0807817, acc 0.96
2016-09-07T22:46:50.736868: step 2011, loss 0.0228085, acc 1
2016-09-07T22:46:51.427754: step 2012, loss 0.0553014, acc 0.96
2016-09-07T22:46:52.140502: step 2013, loss 0.023513, acc 0.98
2016-09-07T22:46:52.848266: step 2014, loss 0.0457442, acc 0.98
2016-09-07T22:46:53.551844: step 2015, loss 0.0726317, acc 0.98
2016-09-07T22:46:54.249893: step 2016, loss 0.0574021, acc 0.96
2016-09-07T22:46:54.947079: step 2017, loss 0.04993, acc 0.98
2016-09-07T22:46:55.656732: step 2018, loss 0.0479555, acc 0.98
2016-09-07T22:46:56.368677: step 2019, loss 0.0180327, acc 1
2016-09-07T22:46:57.060442: step 2020, loss 0.12948, acc 0.94
2016-09-07T22:46:57.767989: step 2021, loss 0.111581, acc 0.94
2016-09-07T22:46:58.494466: step 2022, loss 0.00845512, acc 1
2016-09-07T22:46:59.207807: step 2023, loss 0.049713, acc 0.96
2016-09-07T22:46:59.925719: step 2024, loss 0.0263554, acc 0.98
2016-09-07T22:47:00.667092: step 2025, loss 0.00702461, acc 1
2016-09-07T22:47:01.354862: step 2026, loss 0.0500146, acc 0.98
2016-09-07T22:47:02.041772: step 2027, loss 0.0463021, acc 0.98
2016-09-07T22:47:02.737031: step 2028, loss 0.147922, acc 0.94
2016-09-07T22:47:03.436725: step 2029, loss 0.0154053, acc 1
2016-09-07T22:47:04.144499: step 2030, loss 0.0845224, acc 0.94
2016-09-07T22:47:04.816244: step 2031, loss 0.159108, acc 0.94
2016-09-07T22:47:05.523545: step 2032, loss 0.0286462, acc 1
2016-09-07T22:47:06.225191: step 2033, loss 0.0691882, acc 0.96
2016-09-07T22:47:06.925498: step 2034, loss 0.0536416, acc 0.98
2016-09-07T22:47:07.620029: step 2035, loss 0.0639336, acc 0.98
2016-09-07T22:47:08.315042: step 2036, loss 0.0265762, acc 1
2016-09-07T22:47:09.013326: step 2037, loss 0.0472495, acc 0.98
2016-09-07T22:47:09.719421: step 2038, loss 0.0467117, acc 0.98
2016-09-07T22:47:10.457093: step 2039, loss 0.0468473, acc 0.98
2016-09-07T22:47:11.134813: step 2040, loss 0.134983, acc 0.96
2016-09-07T22:47:11.833599: step 2041, loss 0.0920867, acc 0.96
2016-09-07T22:47:12.511893: step 2042, loss 0.0397201, acc 0.98
2016-09-07T22:47:13.212539: step 2043, loss 0.0424386, acc 1
2016-09-07T22:47:13.941023: step 2044, loss 0.258291, acc 0.94
2016-09-07T22:47:14.641611: step 2045, loss 0.037802, acc 0.98
2016-09-07T22:47:15.344406: step 2046, loss 0.119124, acc 0.94
2016-09-07T22:47:16.028473: step 2047, loss 0.0910133, acc 0.94
2016-09-07T22:47:16.738598: step 2048, loss 0.0239003, acc 0.98
2016-09-07T22:47:17.435827: step 2049, loss 0.0521711, acc 0.98
2016-09-07T22:47:18.123453: step 2050, loss 0.0316748, acc 0.98
2016-09-07T22:47:18.821312: step 2051, loss 0.0915273, acc 0.94
2016-09-07T22:47:19.520622: step 2052, loss 0.0272392, acc 1
2016-09-07T22:47:20.233513: step 2053, loss 0.104967, acc 0.96
2016-09-07T22:47:20.915886: step 2054, loss 0.0922467, acc 0.98
2016-09-07T22:47:21.614803: step 2055, loss 0.0609169, acc 0.96
2016-09-07T22:47:22.301015: step 2056, loss 0.0510756, acc 0.98
2016-09-07T22:47:23.001430: step 2057, loss 0.0193172, acc 1
2016-09-07T22:47:23.734652: step 2058, loss 0.0401467, acc 0.98
2016-09-07T22:47:24.446276: step 2059, loss 0.0772516, acc 0.96
2016-09-07T22:47:25.146141: step 2060, loss 0.0869449, acc 0.96
2016-09-07T22:47:25.844807: step 2061, loss 0.0176389, acc 1
2016-09-07T22:47:26.533920: step 2062, loss 0.0766441, acc 0.98
2016-09-07T22:47:27.242356: step 2063, loss 0.0273164, acc 1
2016-09-07T22:47:27.942925: step 2064, loss 0.031457, acc 0.98
2016-09-07T22:47:28.660256: step 2065, loss 0.108036, acc 0.98
2016-09-07T22:47:29.382774: step 2066, loss 0.0472178, acc 0.98
2016-09-07T22:47:30.072024: step 2067, loss 0.0377446, acc 0.98
2016-09-07T22:47:30.750008: step 2068, loss 0.0244601, acc 1
2016-09-07T22:47:31.447039: step 2069, loss 0.0796698, acc 0.98
2016-09-07T22:47:32.154542: step 2070, loss 0.112845, acc 0.92
2016-09-07T22:47:32.854636: step 2071, loss 0.0803873, acc 0.94
2016-09-07T22:47:33.551705: step 2072, loss 0.125082, acc 0.96
2016-09-07T22:47:34.266506: step 2073, loss 0.017431, acc 1
2016-09-07T22:47:34.959044: step 2074, loss 0.0228682, acc 1
2016-09-07T22:47:35.661290: step 2075, loss 0.0447647, acc 0.98
2016-09-07T22:47:36.345923: step 2076, loss 0.0432323, acc 0.98
2016-09-07T22:47:37.059835: step 2077, loss 0.148363, acc 0.96
2016-09-07T22:47:37.750145: step 2078, loss 0.0107672, acc 1
2016-09-07T22:47:38.452139: step 2079, loss 0.0508698, acc 0.96
2016-09-07T22:47:39.154931: step 2080, loss 0.0197312, acc 1
2016-09-07T22:47:39.873280: step 2081, loss 0.0197239, acc 1
2016-09-07T22:47:40.569638: step 2082, loss 0.0356829, acc 0.98
2016-09-07T22:47:41.247386: step 2083, loss 0.0358744, acc 0.98
2016-09-07T22:47:41.973154: step 2084, loss 0.0493715, acc 0.96
2016-09-07T22:47:42.665089: step 2085, loss 0.12922, acc 0.94
2016-09-07T22:47:43.356641: step 2086, loss 0.0388959, acc 0.98
2016-09-07T22:47:44.057511: step 2087, loss 0.0441646, acc 1
2016-09-07T22:47:44.772457: step 2088, loss 0.211008, acc 0.96
2016-09-07T22:47:45.463344: step 2089, loss 0.0978923, acc 0.94
2016-09-07T22:47:46.150281: step 2090, loss 0.00857071, acc 1
2016-09-07T22:47:46.858328: step 2091, loss 0.0740541, acc 0.96
2016-09-07T22:47:47.546666: step 2092, loss 0.0409878, acc 0.98
2016-09-07T22:47:48.242451: step 2093, loss 0.0797588, acc 0.96
2016-09-07T22:47:48.940481: step 2094, loss 0.119858, acc 0.94
2016-09-07T22:47:49.658430: step 2095, loss 0.0508732, acc 1
2016-09-07T22:47:50.352051: step 2096, loss 0.0129537, acc 1
2016-09-07T22:47:51.044184: step 2097, loss 0.0281725, acc 0.98
2016-09-07T22:47:51.738423: step 2098, loss 0.141461, acc 0.94
2016-09-07T22:47:52.435156: step 2099, loss 0.173575, acc 0.96
2016-09-07T22:47:53.127096: step 2100, loss 0.0398863, acc 0.98

Evaluation:
2016-09-07T22:47:56.287998: step 2100, loss 1.18825, acc 0.765

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-2100

2016-09-07T22:47:58.064359: step 2101, loss 0.033253, acc 0.98
2016-09-07T22:47:58.767456: step 2102, loss 0.0192678, acc 1
2016-09-07T22:47:59.466405: step 2103, loss 0.028832, acc 0.98
2016-09-07T22:48:00.160284: step 2104, loss 0.0829019, acc 0.94
2016-09-07T22:48:00.908454: step 2105, loss 0.0336234, acc 0.98
2016-09-07T22:48:01.613397: step 2106, loss 0.0617043, acc 0.98
2016-09-07T22:48:02.310538: step 2107, loss 0.0397483, acc 0.98
2016-09-07T22:48:03.000146: step 2108, loss 0.0216513, acc 0.98
2016-09-07T22:48:03.706106: step 2109, loss 0.0765718, acc 0.96
2016-09-07T22:48:04.424260: step 2110, loss 0.0652306, acc 0.98
2016-09-07T22:48:05.130607: step 2111, loss 0.0977155, acc 0.92
2016-09-07T22:48:05.852649: step 2112, loss 0.0736913, acc 0.98
2016-09-07T22:48:06.554058: step 2113, loss 0.0815463, acc 0.96
2016-09-07T22:48:07.254961: step 2114, loss 0.0566315, acc 0.98
2016-09-07T22:48:07.945844: step 2115, loss 0.0977707, acc 0.94
2016-09-07T22:48:08.649590: step 2116, loss 0.0517232, acc 0.96
2016-09-07T22:48:09.365895: step 2117, loss 0.0709953, acc 0.96
2016-09-07T22:48:10.079160: step 2118, loss 0.0212926, acc 1
2016-09-07T22:48:10.781441: step 2119, loss 0.0530943, acc 0.98
2016-09-07T22:48:11.493012: step 2120, loss 0.0441216, acc 1
2016-09-07T22:48:12.195142: step 2121, loss 0.0542054, acc 0.98
2016-09-07T22:48:12.895458: step 2122, loss 0.0643887, acc 0.98
2016-09-07T22:48:13.593530: step 2123, loss 0.0763406, acc 0.98
2016-09-07T22:48:14.290342: step 2124, loss 0.0236892, acc 0.98
2016-09-07T22:48:15.005551: step 2125, loss 0.0333226, acc 0.98
2016-09-07T22:48:15.705037: step 2126, loss 0.0201689, acc 1
2016-09-07T22:48:16.431969: step 2127, loss 0.0866167, acc 0.94
2016-09-07T22:48:17.143227: step 2128, loss 0.027708, acc 0.98
2016-09-07T22:48:17.830533: step 2129, loss 0.0369097, acc 1
2016-09-07T22:48:18.529803: step 2130, loss 0.0290779, acc 0.98
2016-09-07T22:48:19.216508: step 2131, loss 0.0481866, acc 0.98
2016-09-07T22:48:19.900392: step 2132, loss 0.0650112, acc 1
2016-09-07T22:48:20.598276: step 2133, loss 0.0211502, acc 0.98
2016-09-07T22:48:20.994049: step 2134, loss 0.0537443, acc 1
2016-09-07T22:48:21.700009: step 2135, loss 0.0358492, acc 0.98
2016-09-07T22:48:22.396718: step 2136, loss 0.0896566, acc 0.96
2016-09-07T22:48:23.079163: step 2137, loss 0.0248894, acc 1
2016-09-07T22:48:23.775817: step 2138, loss 0.0553176, acc 0.98
2016-09-07T22:48:24.496630: step 2139, loss 0.134462, acc 0.96
2016-09-07T22:48:25.205953: step 2140, loss 0.0278921, acc 1
2016-09-07T22:48:25.898622: step 2141, loss 0.027947, acc 0.98
2016-09-07T22:48:26.597581: step 2142, loss 0.0204753, acc 0.98
2016-09-07T22:48:27.294194: step 2143, loss 0.0159618, acc 1
2016-09-07T22:48:28.011806: step 2144, loss 0.0237687, acc 0.98
2016-09-07T22:48:28.704195: step 2145, loss 0.0758513, acc 0.96
2016-09-07T22:48:29.394232: step 2146, loss 0.0683431, acc 0.96
2016-09-07T22:48:30.080176: step 2147, loss 0.0223093, acc 0.98
2016-09-07T22:48:30.767822: step 2148, loss 0.101543, acc 0.96
2016-09-07T22:48:31.456871: step 2149, loss 0.00460164, acc 1
2016-09-07T22:48:32.157804: step 2150, loss 0.0390806, acc 0.98
2016-09-07T22:48:32.853354: step 2151, loss 0.138895, acc 0.94
2016-09-07T22:48:33.561058: step 2152, loss 0.0206154, acc 1
2016-09-07T22:48:34.296136: step 2153, loss 0.0442022, acc 0.98
2016-09-07T22:48:34.999800: step 2154, loss 0.0448793, acc 0.98
2016-09-07T22:48:35.709015: step 2155, loss 0.0637111, acc 0.96
2016-09-07T22:48:36.407190: step 2156, loss 0.0330853, acc 1
2016-09-07T22:48:37.110707: step 2157, loss 0.0601325, acc 0.96
2016-09-07T22:48:37.780309: step 2158, loss 0.133965, acc 0.94
2016-09-07T22:48:38.467085: step 2159, loss 0.0422229, acc 1
2016-09-07T22:48:39.180004: step 2160, loss 0.0317228, acc 0.98
2016-09-07T22:48:39.911005: step 2161, loss 0.0448429, acc 0.98
2016-09-07T22:48:40.620165: step 2162, loss 0.0106034, acc 1
2016-09-07T22:48:41.328693: step 2163, loss 0.0350041, acc 0.98
2016-09-07T22:48:42.031494: step 2164, loss 0.0679106, acc 0.96
2016-09-07T22:48:42.745877: step 2165, loss 0.1717, acc 0.94
2016-09-07T22:48:43.472355: step 2166, loss 0.0683779, acc 0.96
2016-09-07T22:48:44.186479: step 2167, loss 0.0136221, acc 1
2016-09-07T22:48:44.877229: step 2168, loss 0.0829551, acc 0.96
2016-09-07T22:48:45.578005: step 2169, loss 0.0401572, acc 0.98
2016-09-07T22:48:46.275924: step 2170, loss 0.0327876, acc 1
2016-09-07T22:48:46.979888: step 2171, loss 0.0345713, acc 0.98
2016-09-07T22:48:47.664322: step 2172, loss 0.11559, acc 0.94
2016-09-07T22:48:48.358625: step 2173, loss 0.0901615, acc 0.96
2016-09-07T22:48:49.053258: step 2174, loss 0.0213661, acc 0.98
2016-09-07T22:48:49.765663: step 2175, loss 0.0851968, acc 0.94
2016-09-07T22:48:50.464882: step 2176, loss 0.0486759, acc 0.98
2016-09-07T22:48:51.153230: step 2177, loss 0.0272519, acc 1
2016-09-07T22:48:51.839557: step 2178, loss 0.0130741, acc 1
2016-09-07T22:48:52.526703: step 2179, loss 0.092783, acc 0.94
2016-09-07T22:48:53.206745: step 2180, loss 0.0297179, acc 1
2016-09-07T22:48:53.908332: step 2181, loss 0.0849119, acc 0.94
2016-09-07T22:48:54.619544: step 2182, loss 0.0928339, acc 0.96
2016-09-07T22:48:55.324955: step 2183, loss 0.0225768, acc 0.98
2016-09-07T22:48:56.016378: step 2184, loss 0.119051, acc 0.98
2016-09-07T22:48:56.720660: step 2185, loss 0.146807, acc 0.96
2016-09-07T22:48:57.420619: step 2186, loss 0.0258657, acc 0.98
2016-09-07T22:48:58.114120: step 2187, loss 0.00361624, acc 1
2016-09-07T22:48:58.803228: step 2188, loss 0.0544858, acc 0.98
2016-09-07T22:48:59.508978: step 2189, loss 0.192083, acc 0.92
2016-09-07T22:49:00.211355: step 2190, loss 0.0651822, acc 0.98
2016-09-07T22:49:00.932617: step 2191, loss 0.0395857, acc 0.98
2016-09-07T22:49:01.613480: step 2192, loss 0.0447106, acc 0.98
2016-09-07T22:49:02.322025: step 2193, loss 0.0137555, acc 1
2016-09-07T22:49:03.025336: step 2194, loss 0.0566867, acc 0.96
2016-09-07T22:49:03.715199: step 2195, loss 0.0555489, acc 0.96
2016-09-07T22:49:04.391182: step 2196, loss 0.0301421, acc 0.98
2016-09-07T22:49:05.080515: step 2197, loss 0.149376, acc 0.92
2016-09-07T22:49:05.788736: step 2198, loss 0.0743824, acc 0.98
2016-09-07T22:49:06.474479: step 2199, loss 0.061968, acc 0.98
2016-09-07T22:49:07.176962: step 2200, loss 0.0879309, acc 0.98

Evaluation:
2016-09-07T22:49:10.374202: step 2200, loss 1.4444, acc 0.743

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-2200

2016-09-07T22:49:12.037136: step 2201, loss 0.072907, acc 0.96
2016-09-07T22:49:12.749341: step 2202, loss 0.0347985, acc 1
2016-09-07T22:49:13.480318: step 2203, loss 0.0269624, acc 1
2016-09-07T22:49:14.177643: step 2204, loss 0.131715, acc 0.92
2016-09-07T22:49:14.876100: step 2205, loss 0.0283158, acc 0.98
2016-09-07T22:49:15.554893: step 2206, loss 0.00466559, acc 1
2016-09-07T22:49:16.263805: step 2207, loss 0.0877789, acc 0.98
2016-09-07T22:49:16.951574: step 2208, loss 0.00971232, acc 1
2016-09-07T22:49:17.648344: step 2209, loss 0.0578779, acc 0.96
2016-09-07T22:49:18.351920: step 2210, loss 0.0512016, acc 0.98
2016-09-07T22:49:19.052166: step 2211, loss 0.0548371, acc 0.98
2016-09-07T22:49:19.747098: step 2212, loss 0.0273143, acc 0.98
2016-09-07T22:49:20.435070: step 2213, loss 0.0405151, acc 0.98
2016-09-07T22:49:21.140196: step 2214, loss 0.0757335, acc 0.98
2016-09-07T22:49:21.836933: step 2215, loss 0.0436964, acc 0.96
2016-09-07T22:49:22.532841: step 2216, loss 0.0415168, acc 0.96
2016-09-07T22:49:23.240289: step 2217, loss 0.0247358, acc 0.98
2016-09-07T22:49:23.945279: step 2218, loss 0.0593714, acc 0.98
2016-09-07T22:49:24.656269: step 2219, loss 0.00434314, acc 1
2016-09-07T22:49:25.370833: step 2220, loss 0.0993823, acc 0.98
2016-09-07T22:49:26.064381: step 2221, loss 0.0662222, acc 0.98
2016-09-07T22:49:26.767311: step 2222, loss 0.0779338, acc 0.98
2016-09-07T22:49:27.460432: step 2223, loss 0.054108, acc 0.98
2016-09-07T22:49:28.173000: step 2224, loss 0.0646284, acc 0.96
2016-09-07T22:49:28.878414: step 2225, loss 0.0363314, acc 0.96
2016-09-07T22:49:29.599590: step 2226, loss 0.0411418, acc 0.98
2016-09-07T22:49:30.315371: step 2227, loss 0.0669686, acc 0.96
2016-09-07T22:49:31.061946: step 2228, loss 0.0303704, acc 1
2016-09-07T22:49:31.774235: step 2229, loss 0.04268, acc 1
2016-09-07T22:49:32.467401: step 2230, loss 0.0513762, acc 0.98
2016-09-07T22:49:33.165282: step 2231, loss 0.0157844, acc 1
2016-09-07T22:49:33.859820: step 2232, loss 0.0805031, acc 0.94
2016-09-07T22:49:34.558813: step 2233, loss 0.0145879, acc 1
2016-09-07T22:49:35.252818: step 2234, loss 0.0360639, acc 0.98
2016-09-07T22:49:35.929206: step 2235, loss 0.0177073, acc 0.98
2016-09-07T22:49:36.616302: step 2236, loss 0.0839457, acc 0.96
2016-09-07T22:49:37.324682: step 2237, loss 0.0610926, acc 0.98
2016-09-07T22:49:38.042099: step 2238, loss 0.035592, acc 1
2016-09-07T22:49:38.744109: step 2239, loss 0.0221841, acc 1
2016-09-07T22:49:39.423080: step 2240, loss 0.0430592, acc 0.98
2016-09-07T22:49:40.110786: step 2241, loss 0.0820363, acc 0.94
2016-09-07T22:49:40.808824: step 2242, loss 0.0689825, acc 0.98
2016-09-07T22:49:41.493408: step 2243, loss 0.130498, acc 0.94
2016-09-07T22:49:42.204087: step 2244, loss 0.0979893, acc 0.96
2016-09-07T22:49:42.914781: step 2245, loss 0.0867956, acc 0.94
2016-09-07T22:49:43.601703: step 2246, loss 0.0308779, acc 0.98
2016-09-07T22:49:44.321895: step 2247, loss 0.0710181, acc 0.96
2016-09-07T22:49:45.003980: step 2248, loss 0.0239755, acc 1
2016-09-07T22:49:45.697991: step 2249, loss 0.032812, acc 0.98
2016-09-07T22:49:46.407919: step 2250, loss 0.0440433, acc 0.96
2016-09-07T22:49:47.101775: step 2251, loss 0.122709, acc 0.98
2016-09-07T22:49:47.797847: step 2252, loss 0.0878333, acc 0.98
2016-09-07T22:49:48.489723: step 2253, loss 0.0516379, acc 0.96
2016-09-07T22:49:49.186200: step 2254, loss 0.0638349, acc 0.98
2016-09-07T22:49:49.884288: step 2255, loss 0.105553, acc 0.96
2016-09-07T22:49:50.569367: step 2256, loss 0.0360269, acc 0.98
2016-09-07T22:49:51.263461: step 2257, loss 0.0130018, acc 1
2016-09-07T22:49:51.964349: step 2258, loss 0.0177429, acc 1
2016-09-07T22:49:52.647634: step 2259, loss 0.0633103, acc 0.94
2016-09-07T22:49:53.330765: step 2260, loss 0.0493022, acc 0.98
2016-09-07T22:49:54.038143: step 2261, loss 0.0226942, acc 1
2016-09-07T22:49:54.746385: step 2262, loss 0.0108071, acc 1
2016-09-07T22:49:55.445430: step 2263, loss 0.0373287, acc 0.98
2016-09-07T22:49:56.145843: step 2264, loss 0.0943479, acc 0.96
2016-09-07T22:49:56.846635: step 2265, loss 0.0180628, acc 1
2016-09-07T22:49:57.546633: step 2266, loss 0.0587884, acc 0.96
2016-09-07T22:49:58.253521: step 2267, loss 0.0580203, acc 0.96
2016-09-07T22:49:58.957811: step 2268, loss 0.0569683, acc 0.98
2016-09-07T22:49:59.666281: step 2269, loss 0.0441763, acc 0.98
2016-09-07T22:50:00.425029: step 2270, loss 0.01787, acc 1
2016-09-07T22:50:01.122055: step 2271, loss 0.177862, acc 0.96
2016-09-07T22:50:01.821289: step 2272, loss 0.0154595, acc 1
2016-09-07T22:50:02.516271: step 2273, loss 0.0273147, acc 1
2016-09-07T22:50:03.229206: step 2274, loss 0.125437, acc 0.94
2016-09-07T22:50:03.950400: step 2275, loss 0.0828568, acc 0.96
2016-09-07T22:50:04.651237: step 2276, loss 0.0626291, acc 0.96
2016-09-07T22:50:05.343765: step 2277, loss 0.0382984, acc 0.98
2016-09-07T22:50:06.051914: step 2278, loss 0.0428016, acc 0.96
2016-09-07T22:50:06.746637: step 2279, loss 0.0717013, acc 0.98
2016-09-07T22:50:07.448694: step 2280, loss 0.0445562, acc 1
2016-09-07T22:50:08.140907: step 2281, loss 0.167069, acc 0.92
2016-09-07T22:50:08.854533: step 2282, loss 0.200446, acc 0.96
2016-09-07T22:50:09.555022: step 2283, loss 0.10395, acc 0.96
2016-09-07T22:50:10.268272: step 2284, loss 0.0215967, acc 1
2016-09-07T22:50:10.978904: step 2285, loss 0.0533615, acc 0.96
2016-09-07T22:50:11.681251: step 2286, loss 0.0504285, acc 0.98
2016-09-07T22:50:12.378452: step 2287, loss 0.0544039, acc 0.98
2016-09-07T22:50:13.098669: step 2288, loss 0.0642346, acc 0.96
2016-09-07T22:50:13.793021: step 2289, loss 0.0632073, acc 0.98
2016-09-07T22:50:14.492281: step 2290, loss 0.0624383, acc 0.96
2016-09-07T22:50:15.209748: step 2291, loss 0.0290065, acc 0.98
2016-09-07T22:50:15.906729: step 2292, loss 0.0486204, acc 0.98
2016-09-07T22:50:16.629902: step 2293, loss 0.0855327, acc 0.98
2016-09-07T22:50:17.337373: step 2294, loss 0.0536826, acc 0.98
2016-09-07T22:50:18.025230: step 2295, loss 0.021346, acc 1
2016-09-07T22:50:18.719997: step 2296, loss 0.0728221, acc 0.96
2016-09-07T22:50:19.421965: step 2297, loss 0.0377197, acc 1
2016-09-07T22:50:20.102752: step 2298, loss 0.013714, acc 1
2016-09-07T22:50:20.795399: step 2299, loss 0.109231, acc 0.98
2016-09-07T22:50:21.481977: step 2300, loss 0.118969, acc 0.92

Evaluation:
2016-09-07T22:50:24.660368: step 2300, loss 1.37946, acc 0.755

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-2300

2016-09-07T22:50:26.358100: step 2301, loss 0.0395516, acc 0.98
2016-09-07T22:50:27.057574: step 2302, loss 0.0505983, acc 0.98
2016-09-07T22:50:27.767474: step 2303, loss 0.0920961, acc 0.98
2016-09-07T22:50:28.479444: step 2304, loss 0.124543, acc 0.98
2016-09-07T22:50:29.175296: step 2305, loss 0.0323101, acc 0.98
2016-09-07T22:50:29.871126: step 2306, loss 0.0184033, acc 1
2016-09-07T22:50:30.574061: step 2307, loss 0.0190731, acc 1
2016-09-07T22:50:31.288199: step 2308, loss 0.0579001, acc 0.98
2016-09-07T22:50:31.989518: step 2309, loss 0.0735642, acc 0.96
2016-09-07T22:50:32.690790: step 2310, loss 0.0950731, acc 0.96
2016-09-07T22:50:33.419266: step 2311, loss 0.0268736, acc 1
2016-09-07T22:50:34.119526: step 2312, loss 0.0443716, acc 0.98
2016-09-07T22:50:34.833969: step 2313, loss 0.0214129, acc 1
2016-09-07T22:50:35.547142: step 2314, loss 0.0755725, acc 0.96
2016-09-07T22:50:36.252061: step 2315, loss 0.115812, acc 0.96
2016-09-07T22:50:36.943685: step 2316, loss 0.00955484, acc 1
2016-09-07T22:50:37.630608: step 2317, loss 0.0196711, acc 1
2016-09-07T22:50:38.317302: step 2318, loss 0.0461685, acc 0.98
2016-09-07T22:50:39.036024: step 2319, loss 0.149888, acc 0.94
2016-09-07T22:50:39.743104: step 2320, loss 0.0758975, acc 0.96
2016-09-07T22:50:40.439472: step 2321, loss 0.0736691, acc 0.94
2016-09-07T22:50:41.138003: step 2322, loss 0.00845655, acc 1
2016-09-07T22:50:41.852332: step 2323, loss 0.0604164, acc 0.94
2016-09-07T22:50:42.537054: step 2324, loss 0.0221449, acc 1
2016-09-07T22:50:43.250827: step 2325, loss 0.095254, acc 0.94
2016-09-07T22:50:43.952751: step 2326, loss 0.0207739, acc 1
2016-09-07T22:50:44.659347: step 2327, loss 0.0245771, acc 1
2016-09-07T22:50:45.033862: step 2328, loss 0.0496949, acc 1
2016-09-07T22:50:45.735027: step 2329, loss 0.0359406, acc 1
2016-09-07T22:50:46.430355: step 2330, loss 0.0329072, acc 1
2016-09-07T22:50:47.135660: step 2331, loss 0.176036, acc 0.96
2016-09-07T22:50:47.828292: step 2332, loss 0.034104, acc 1
2016-09-07T22:50:48.546478: step 2333, loss 0.024355, acc 1
2016-09-07T22:50:49.253584: step 2334, loss 0.0382106, acc 0.98
2016-09-07T22:50:49.975402: step 2335, loss 0.0318902, acc 0.98
2016-09-07T22:50:50.686111: step 2336, loss 0.0342314, acc 0.98
2016-09-07T22:50:51.390592: step 2337, loss 0.0276245, acc 0.98
2016-09-07T22:50:52.079194: step 2338, loss 0.115049, acc 0.94
2016-09-07T22:50:52.785357: step 2339, loss 0.0525898, acc 0.98
2016-09-07T22:50:53.491815: step 2340, loss 0.0751838, acc 0.96
2016-09-07T22:50:54.200995: step 2341, loss 0.0285246, acc 0.98
2016-09-07T22:50:54.876115: step 2342, loss 0.0429489, acc 0.98
2016-09-07T22:50:55.587516: step 2343, loss 0.0253526, acc 0.98
2016-09-07T22:50:56.273289: step 2344, loss 0.00163909, acc 1
2016-09-07T22:50:56.985325: step 2345, loss 0.06556, acc 0.96
2016-09-07T22:50:57.663544: step 2346, loss 0.215558, acc 0.96
2016-09-07T22:50:58.370437: step 2347, loss 0.0482722, acc 0.98
2016-09-07T22:50:59.048428: step 2348, loss 0.00733576, acc 1
2016-09-07T22:50:59.751228: step 2349, loss 0.00961552, acc 1
2016-09-07T22:51:00.501671: step 2350, loss 0.0835311, acc 0.96
2016-09-07T22:51:01.213289: step 2351, loss 0.0168372, acc 1
2016-09-07T22:51:01.925637: step 2352, loss 0.0138229, acc 1
2016-09-07T22:51:02.629726: step 2353, loss 0.0457575, acc 0.98
2016-09-07T22:51:03.342720: step 2354, loss 0.0187731, acc 1
2016-09-07T22:51:04.046508: step 2355, loss 0.0411608, acc 0.98
2016-09-07T22:51:04.782352: step 2356, loss 0.00114559, acc 1
2016-09-07T22:51:05.474725: step 2357, loss 0.151826, acc 0.96
2016-09-07T22:51:06.163041: step 2358, loss 0.0511452, acc 0.98
2016-09-07T22:51:06.866681: step 2359, loss 0.0449714, acc 0.96
2016-09-07T22:51:07.567769: step 2360, loss 0.0733323, acc 0.96
2016-09-07T22:51:08.257499: step 2361, loss 0.0675747, acc 0.98
2016-09-07T22:51:08.965350: step 2362, loss 0.122489, acc 0.96
2016-09-07T22:51:09.656475: step 2363, loss 0.0123338, acc 1
2016-09-07T22:51:10.346073: step 2364, loss 0.0604945, acc 0.96
2016-09-07T22:51:11.031729: step 2365, loss 0.0320387, acc 1
2016-09-07T22:51:11.736041: step 2366, loss 0.086323, acc 0.94
2016-09-07T22:51:12.447100: step 2367, loss 0.0324558, acc 1
2016-09-07T22:51:13.145347: step 2368, loss 0.104088, acc 0.98
2016-09-07T22:51:13.869298: step 2369, loss 0.0623632, acc 0.96
2016-09-07T22:51:14.574410: step 2370, loss 0.129102, acc 0.96
2016-09-07T22:51:15.300724: step 2371, loss 0.0420478, acc 1
2016-09-07T22:51:16.007069: step 2372, loss 0.113923, acc 0.96
2016-09-07T22:51:16.697913: step 2373, loss 0.0842248, acc 0.94
2016-09-07T22:51:17.401724: step 2374, loss 0.020867, acc 1
2016-09-07T22:51:18.100357: step 2375, loss 0.0811053, acc 0.98
2016-09-07T22:51:18.806225: step 2376, loss 0.0535846, acc 1
2016-09-07T22:51:19.506148: step 2377, loss 0.0265228, acc 0.98
2016-09-07T22:51:20.208432: step 2378, loss 0.0420501, acc 0.98
2016-09-07T22:51:20.894523: step 2379, loss 0.0313082, acc 0.98
2016-09-07T22:51:21.591693: step 2380, loss 0.0698195, acc 0.94
2016-09-07T22:51:22.272747: step 2381, loss 0.0292034, acc 0.98
2016-09-07T22:51:22.970924: step 2382, loss 0.00692899, acc 1
2016-09-07T22:51:23.662667: step 2383, loss 0.105102, acc 0.96
2016-09-07T22:51:24.367237: step 2384, loss 0.0603781, acc 0.96
2016-09-07T22:51:25.078691: step 2385, loss 0.143767, acc 0.98
2016-09-07T22:51:25.781220: step 2386, loss 0.160246, acc 0.96
2016-09-07T22:51:26.483281: step 2387, loss 0.0471975, acc 0.98
2016-09-07T22:51:27.180132: step 2388, loss 0.0427343, acc 1
2016-09-07T22:51:27.885269: step 2389, loss 0.0227597, acc 1
2016-09-07T22:51:28.578296: step 2390, loss 0.0705842, acc 0.98
2016-09-07T22:51:29.273520: step 2391, loss 0.0315721, acc 1
2016-09-07T22:51:29.972788: step 2392, loss 0.0599049, acc 0.98
2016-09-07T22:51:30.690032: step 2393, loss 0.0972575, acc 0.92
2016-09-07T22:51:31.388392: step 2394, loss 0.0420002, acc 0.98
2016-09-07T22:51:32.091558: step 2395, loss 0.0401393, acc 1
2016-09-07T22:51:32.794070: step 2396, loss 0.0165269, acc 1
2016-09-07T22:51:33.493364: step 2397, loss 0.0368378, acc 0.98
2016-09-07T22:51:34.180420: step 2398, loss 0.0848634, acc 0.96
2016-09-07T22:51:34.889365: step 2399, loss 0.0825706, acc 0.96
2016-09-07T22:51:35.592205: step 2400, loss 0.0452624, acc 0.98

Evaluation:
2016-09-07T22:51:38.781862: step 2400, loss 1.45231, acc 0.745

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-2400

2016-09-07T22:51:40.605941: step 2401, loss 0.0253471, acc 0.98
2016-09-07T22:51:41.309797: step 2402, loss 0.0152346, acc 1
2016-09-07T22:51:42.019682: step 2403, loss 0.0588722, acc 0.96
2016-09-07T22:51:42.712280: step 2404, loss 0.154509, acc 0.94
2016-09-07T22:51:43.395527: step 2405, loss 0.145603, acc 0.96
2016-09-07T22:51:44.093389: step 2406, loss 0.0374993, acc 0.98
2016-09-07T22:51:44.794678: step 2407, loss 0.0539005, acc 0.98
2016-09-07T22:51:45.484589: step 2408, loss 0.091259, acc 0.96
2016-09-07T22:51:46.197342: step 2409, loss 0.0807628, acc 0.94
2016-09-07T22:51:46.875458: step 2410, loss 0.00443037, acc 1
2016-09-07T22:51:47.566737: step 2411, loss 0.0705639, acc 0.96
2016-09-07T22:51:48.250272: step 2412, loss 0.0401811, acc 1
2016-09-07T22:51:48.960086: step 2413, loss 0.0225973, acc 0.98
2016-09-07T22:51:49.653259: step 2414, loss 0.0325841, acc 0.98
2016-09-07T22:51:50.350839: step 2415, loss 0.0178113, acc 1
2016-09-07T22:51:51.058378: step 2416, loss 0.0297758, acc 0.98
2016-09-07T22:51:51.752101: step 2417, loss 0.0576449, acc 0.98
2016-09-07T22:51:52.471032: step 2418, loss 0.274278, acc 0.96
2016-09-07T22:51:53.163325: step 2419, loss 0.134365, acc 0.96
2016-09-07T22:51:53.865161: step 2420, loss 0.0187755, acc 0.98
2016-09-07T22:51:54.581402: step 2421, loss 0.0225804, acc 1
2016-09-07T22:51:55.280443: step 2422, loss 0.0209038, acc 1
2016-09-07T22:51:56.003976: step 2423, loss 0.032979, acc 0.98
2016-09-07T22:51:56.696900: step 2424, loss 0.0574824, acc 0.96
2016-09-07T22:51:57.391848: step 2425, loss 0.0939479, acc 0.96
2016-09-07T22:51:58.095265: step 2426, loss 0.0701204, acc 0.98
2016-09-07T22:51:58.814449: step 2427, loss 0.029063, acc 1
2016-09-07T22:51:59.515817: step 2428, loss 0.0404715, acc 1
2016-09-07T22:52:00.225753: step 2429, loss 0.0180738, acc 1
2016-09-07T22:52:00.918983: step 2430, loss 0.0870256, acc 0.94
2016-09-07T22:52:01.606902: step 2431, loss 0.0446896, acc 0.98
2016-09-07T22:52:02.307440: step 2432, loss 0.0436749, acc 0.98
2016-09-07T22:52:03.008936: step 2433, loss 0.0841295, acc 0.98
2016-09-07T22:52:03.698813: step 2434, loss 0.0816385, acc 0.96
2016-09-07T22:52:04.391874: step 2435, loss 0.173463, acc 0.94
2016-09-07T22:52:05.069911: step 2436, loss 0.0179884, acc 1
2016-09-07T22:52:05.776882: step 2437, loss 0.0548881, acc 0.98
2016-09-07T22:52:06.467374: step 2438, loss 0.0369216, acc 0.98
2016-09-07T22:52:07.164300: step 2439, loss 0.0717426, acc 0.96
2016-09-07T22:52:07.868035: step 2440, loss 0.0139003, acc 1
2016-09-07T22:52:08.569672: step 2441, loss 0.0388892, acc 1
2016-09-07T22:52:09.277638: step 2442, loss 0.0637298, acc 0.96
2016-09-07T22:52:09.977727: step 2443, loss 0.0910424, acc 0.96
2016-09-07T22:52:10.680451: step 2444, loss 0.0597097, acc 0.96
2016-09-07T22:52:11.379858: step 2445, loss 0.0713734, acc 0.94
2016-09-07T22:52:12.080722: step 2446, loss 0.0309505, acc 1
2016-09-07T22:52:12.770758: step 2447, loss 0.114955, acc 0.96
2016-09-07T22:52:13.472598: step 2448, loss 0.0268594, acc 1
2016-09-07T22:52:14.161910: step 2449, loss 0.0614233, acc 0.94
2016-09-07T22:52:14.851028: step 2450, loss 0.0617669, acc 0.96
2016-09-07T22:52:15.564054: step 2451, loss 0.00371217, acc 1
2016-09-07T22:52:16.276482: step 2452, loss 0.0165203, acc 1
2016-09-07T22:52:16.966856: step 2453, loss 0.01224, acc 1
2016-09-07T22:52:17.689778: step 2454, loss 0.0183228, acc 0.98
2016-09-07T22:52:18.365159: step 2455, loss 0.0446209, acc 0.98
2016-09-07T22:52:19.063019: step 2456, loss 0.00324867, acc 1
2016-09-07T22:52:19.751742: step 2457, loss 0.0862756, acc 0.96
2016-09-07T22:52:20.445938: step 2458, loss 0.0540447, acc 0.98
2016-09-07T22:52:21.133521: step 2459, loss 0.00642663, acc 1
2016-09-07T22:52:21.826959: step 2460, loss 0.0702155, acc 0.96
2016-09-07T22:52:22.527386: step 2461, loss 0.0374512, acc 0.96
2016-09-07T22:52:23.224800: step 2462, loss 0.0578025, acc 0.98
2016-09-07T22:52:23.912600: step 2463, loss 0.0366099, acc 0.98
2016-09-07T22:52:24.613698: step 2464, loss 0.0574177, acc 0.98
2016-09-07T22:52:25.315297: step 2465, loss 0.0294721, acc 1
2016-09-07T22:52:25.999520: step 2466, loss 0.0259294, acc 1
2016-09-07T22:52:26.689640: step 2467, loss 0.00943041, acc 1
2016-09-07T22:52:27.402935: step 2468, loss 0.047434, acc 0.98
2016-09-07T22:52:28.103461: step 2469, loss 0.119187, acc 0.96
2016-09-07T22:52:28.810890: step 2470, loss 0.0351047, acc 0.98
2016-09-07T22:52:29.523006: step 2471, loss 0.0800481, acc 0.98
2016-09-07T22:52:30.227564: step 2472, loss 0.028333, acc 0.98
2016-09-07T22:52:30.928419: step 2473, loss 0.0380765, acc 0.98
2016-09-07T22:52:31.635823: step 2474, loss 0.0926973, acc 0.96
2016-09-07T22:52:32.367872: step 2475, loss 0.0480124, acc 0.98
2016-09-07T22:52:33.073820: step 2476, loss 0.0497859, acc 0.98
2016-09-07T22:52:33.761035: step 2477, loss 0.0190444, acc 1
2016-09-07T22:52:34.456659: step 2478, loss 0.0581925, acc 0.96
2016-09-07T22:52:35.159940: step 2479, loss 0.0628568, acc 0.96
2016-09-07T22:52:35.874284: step 2480, loss 0.141626, acc 0.94
2016-09-07T22:52:36.618478: step 2481, loss 0.0549345, acc 0.94
2016-09-07T22:52:37.316736: step 2482, loss 0.0594762, acc 0.98
2016-09-07T22:52:38.057574: step 2483, loss 0.110509, acc 0.98
2016-09-07T22:52:38.785912: step 2484, loss 0.00198379, acc 1
2016-09-07T22:52:39.472438: step 2485, loss 0.0104985, acc 1
2016-09-07T22:52:40.165930: step 2486, loss 0.0700004, acc 0.94
2016-09-07T22:52:40.848965: step 2487, loss 0.0260621, acc 1
2016-09-07T22:52:41.541204: step 2488, loss 0.0434122, acc 0.98
2016-09-07T22:52:42.227342: step 2489, loss 0.0885947, acc 0.96
2016-09-07T22:52:42.920089: step 2490, loss 0.0462877, acc 0.96
2016-09-07T22:52:43.617143: step 2491, loss 0.0190336, acc 1
2016-09-07T22:52:44.316514: step 2492, loss 0.0296876, acc 1
2016-09-07T22:52:45.018325: step 2493, loss 0.0289527, acc 0.98
2016-09-07T22:52:45.717844: step 2494, loss 0.0351868, acc 0.98
2016-09-07T22:52:46.412187: step 2495, loss 0.0186934, acc 1
2016-09-07T22:52:47.127428: step 2496, loss 0.0212655, acc 1
2016-09-07T22:52:47.827497: step 2497, loss 0.0253717, acc 1
2016-09-07T22:52:48.545676: step 2498, loss 0.0916605, acc 0.92
2016-09-07T22:52:49.244238: step 2499, loss 0.091977, acc 0.96
2016-09-07T22:52:49.963416: step 2500, loss 0.0503556, acc 0.98

Evaluation:
2016-09-07T22:52:53.171703: step 2500, loss 1.46636, acc 0.746

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-2500

2016-09-07T22:52:54.882199: step 2501, loss 0.02073, acc 0.98
2016-09-07T22:52:55.588716: step 2502, loss 0.0281436, acc 0.98
2016-09-07T22:52:56.281610: step 2503, loss 0.0330395, acc 0.98
2016-09-07T22:52:57.001415: step 2504, loss 0.0480386, acc 0.96
2016-09-07T22:52:57.708538: step 2505, loss 0.0557183, acc 0.94
2016-09-07T22:52:58.417680: step 2506, loss 0.0201804, acc 1
2016-09-07T22:52:59.130001: step 2507, loss 0.0509842, acc 1
2016-09-07T22:52:59.844552: step 2508, loss 0.0575023, acc 0.98
2016-09-07T22:53:00.583397: step 2509, loss 0.097487, acc 0.96
2016-09-07T22:53:01.288820: step 2510, loss 0.0318074, acc 0.98
2016-09-07T22:53:01.997515: step 2511, loss 0.0163176, acc 1
2016-09-07T22:53:02.683543: step 2512, loss 0.084485, acc 0.98
2016-09-07T22:53:03.372832: step 2513, loss 0.0686459, acc 0.96
2016-09-07T22:53:04.053085: step 2514, loss 0.0229628, acc 1
2016-09-07T22:53:04.738872: step 2515, loss 0.141259, acc 0.96
2016-09-07T22:53:05.423645: step 2516, loss 0.0633986, acc 0.96
2016-09-07T22:53:06.121751: step 2517, loss 0.0343889, acc 0.96
2016-09-07T22:53:06.807810: step 2518, loss 0.0183509, acc 1
2016-09-07T22:53:07.511377: step 2519, loss 0.0950099, acc 0.96
2016-09-07T22:53:08.220151: step 2520, loss 0.0887463, acc 0.98
2016-09-07T22:53:08.914579: step 2521, loss 0.0232098, acc 1
2016-09-07T22:53:09.272899: step 2522, loss 0.0258715, acc 1
2016-09-07T22:53:09.967024: step 2523, loss 0.0168996, acc 1
2016-09-07T22:53:10.663372: step 2524, loss 0.00817288, acc 1
2016-09-07T22:53:11.363347: step 2525, loss 0.0318895, acc 1
2016-09-07T22:53:12.079495: step 2526, loss 0.0199756, acc 1
2016-09-07T22:53:12.771957: step 2527, loss 0.0314641, acc 0.98
2016-09-07T22:53:13.462335: step 2528, loss 0.166141, acc 0.94
2016-09-07T22:53:14.145910: step 2529, loss 0.0115324, acc 1
2016-09-07T22:53:14.847880: step 2530, loss 0.0313811, acc 0.98
2016-09-07T22:53:15.538158: step 2531, loss 0.0416389, acc 0.96
2016-09-07T22:53:16.233513: step 2532, loss 0.100551, acc 0.98
2016-09-07T22:53:16.944350: step 2533, loss 0.0139086, acc 1
2016-09-07T22:53:17.636000: step 2534, loss 0.0122819, acc 1
2016-09-07T22:53:18.333066: step 2535, loss 0.0428489, acc 0.98
2016-09-07T22:53:19.017681: step 2536, loss 0.156409, acc 0.92
2016-09-07T22:53:19.717402: step 2537, loss 0.00222888, acc 1
2016-09-07T22:53:20.425969: step 2538, loss 0.00259099, acc 1
2016-09-07T22:53:21.114068: step 2539, loss 0.0946827, acc 0.94
2016-09-07T22:53:21.815976: step 2540, loss 0.00307952, acc 1
2016-09-07T22:53:22.537237: step 2541, loss 0.0333899, acc 0.96
2016-09-07T22:53:23.245757: step 2542, loss 0.0464354, acc 0.96
2016-09-07T22:53:23.962972: step 2543, loss 0.0279747, acc 0.98
2016-09-07T22:53:24.678559: step 2544, loss 0.155572, acc 0.96
2016-09-07T22:53:25.420215: step 2545, loss 0.064707, acc 0.98
2016-09-07T22:53:26.121583: step 2546, loss 0.0296513, acc 1
2016-09-07T22:53:26.830668: step 2547, loss 0.128994, acc 0.96
2016-09-07T22:53:27.528761: step 2548, loss 0.0117961, acc 1
2016-09-07T22:53:28.222142: step 2549, loss 0.0177458, acc 1
2016-09-07T22:53:28.926323: step 2550, loss 0.110906, acc 0.94
2016-09-07T22:53:29.624075: step 2551, loss 0.1128, acc 0.96
2016-09-07T22:53:30.337193: step 2552, loss 0.0536247, acc 0.96
2016-09-07T22:53:31.041902: step 2553, loss 0.0524862, acc 0.98
2016-09-07T22:53:31.743924: step 2554, loss 0.0313504, acc 0.98
2016-09-07T22:53:32.466120: step 2555, loss 0.0627532, acc 0.96
2016-09-07T22:53:33.159386: step 2556, loss 0.00119346, acc 1
2016-09-07T22:53:33.872603: step 2557, loss 0.0103261, acc 1
2016-09-07T22:53:34.582088: step 2558, loss 0.0903239, acc 0.96
2016-09-07T22:53:35.284179: step 2559, loss 0.0380111, acc 0.98
2016-09-07T22:53:35.975175: step 2560, loss 0.0840826, acc 0.98
2016-09-07T22:53:36.663296: step 2561, loss 0.0651625, acc 0.96
2016-09-07T22:53:37.371806: step 2562, loss 0.0142505, acc 1
2016-09-07T22:53:38.065823: step 2563, loss 0.0564172, acc 0.98
2016-09-07T22:53:38.755262: step 2564, loss 0.0386465, acc 1
2016-09-07T22:53:39.441640: step 2565, loss 0.0787567, acc 0.98
2016-09-07T22:53:40.135655: step 2566, loss 0.0295632, acc 0.98
2016-09-07T22:53:40.828398: step 2567, loss 0.0211401, acc 1
2016-09-07T22:53:41.524222: step 2568, loss 0.121437, acc 0.94
2016-09-07T22:53:42.233230: step 2569, loss 0.0426384, acc 0.98
2016-09-07T22:53:42.944183: step 2570, loss 0.0193589, acc 1
2016-09-07T22:53:43.647613: step 2571, loss 0.00592515, acc 1
2016-09-07T22:53:44.357327: step 2572, loss 0.0334149, acc 1
2016-09-07T22:53:45.059770: step 2573, loss 0.0396346, acc 0.98
2016-09-07T22:53:45.763893: step 2574, loss 0.0150158, acc 1
2016-09-07T22:53:46.472691: step 2575, loss 0.0176762, acc 1
2016-09-07T22:53:47.179474: step 2576, loss 0.0293825, acc 0.98
2016-09-07T22:53:47.878855: step 2577, loss 0.0144995, acc 1
2016-09-07T22:53:48.582347: step 2578, loss 0.0162057, acc 1
2016-09-07T22:53:49.271019: step 2579, loss 0.0316183, acc 1
2016-09-07T22:53:49.972463: step 2580, loss 0.022492, acc 0.98
2016-09-07T22:53:50.675871: step 2581, loss 0.00342519, acc 1
2016-09-07T22:53:51.382591: step 2582, loss 0.0275544, acc 0.98
2016-09-07T22:53:52.078661: step 2583, loss 0.036292, acc 0.98
2016-09-07T22:53:52.776605: step 2584, loss 0.108925, acc 0.96
2016-09-07T22:53:53.481021: step 2585, loss 0.0775286, acc 0.96
2016-09-07T22:53:54.163536: step 2586, loss 0.0303465, acc 1
2016-09-07T22:53:54.877331: step 2587, loss 0.0265499, acc 0.98
2016-09-07T22:53:55.562562: step 2588, loss 0.0607568, acc 0.98
2016-09-07T22:53:56.261522: step 2589, loss 0.0427748, acc 0.96
2016-09-07T22:53:56.964709: step 2590, loss 0.00261364, acc 1
2016-09-07T22:53:57.670546: step 2591, loss 0.110513, acc 0.96
2016-09-07T22:53:58.376407: step 2592, loss 0.0533211, acc 0.98
2016-09-07T22:53:59.096014: step 2593, loss 0.0606702, acc 0.98
2016-09-07T22:53:59.809076: step 2594, loss 0.0445258, acc 0.98
2016-09-07T22:54:00.555440: step 2595, loss 0.0223943, acc 1
2016-09-07T22:54:01.258062: step 2596, loss 0.0130196, acc 1
2016-09-07T22:54:01.943437: step 2597, loss 0.0156452, acc 1
2016-09-07T22:54:02.638193: step 2598, loss 0.0472785, acc 0.98
2016-09-07T22:54:03.326390: step 2599, loss 0.0152613, acc 1
2016-09-07T22:54:04.028478: step 2600, loss 0.0178308, acc 1

Evaluation:
2016-09-07T22:54:07.250248: step 2600, loss 1.48473, acc 0.757

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-2600

2016-09-07T22:54:08.876096: step 2601, loss 0.0260676, acc 1
2016-09-07T22:54:09.605105: step 2602, loss 0.0656157, acc 0.98
2016-09-07T22:54:10.299125: step 2603, loss 0.0867571, acc 0.96
2016-09-07T22:54:10.983507: step 2604, loss 0.0488426, acc 0.98
2016-09-07T22:54:11.694653: step 2605, loss 0.0130303, acc 1
2016-09-07T22:54:12.397944: step 2606, loss 0.00598697, acc 1
2016-09-07T22:54:13.090299: step 2607, loss 0.0895038, acc 0.96
2016-09-07T22:54:13.819443: step 2608, loss 0.0211816, acc 1
2016-09-07T22:54:14.514896: step 2609, loss 0.104783, acc 0.96
2016-09-07T22:54:15.200746: step 2610, loss 0.0351895, acc 0.98
2016-09-07T22:54:15.892350: step 2611, loss 0.0213333, acc 0.98
2016-09-07T22:54:16.578491: step 2612, loss 0.0240819, acc 0.98
2016-09-07T22:54:17.268243: step 2613, loss 0.0313615, acc 1
2016-09-07T22:54:17.978691: step 2614, loss 0.0152113, acc 1
2016-09-07T22:54:18.680322: step 2615, loss 0.0226556, acc 1
2016-09-07T22:54:19.405191: step 2616, loss 0.0110507, acc 1
2016-09-07T22:54:20.120566: step 2617, loss 0.110359, acc 0.94
2016-09-07T22:54:20.815069: step 2618, loss 0.0280223, acc 0.98
2016-09-07T22:54:21.526094: step 2619, loss 0.0323668, acc 0.98
2016-09-07T22:54:22.230560: step 2620, loss 0.0421594, acc 0.96
2016-09-07T22:54:22.914875: step 2621, loss 0.0706823, acc 0.96
2016-09-07T22:54:23.627723: step 2622, loss 0.0457454, acc 0.98
2016-09-07T22:54:24.323719: step 2623, loss 0.0816263, acc 0.98
2016-09-07T22:54:25.019305: step 2624, loss 0.0098326, acc 1
2016-09-07T22:54:25.698233: step 2625, loss 0.0157715, acc 1
2016-09-07T22:54:26.387317: step 2626, loss 0.218794, acc 0.94
2016-09-07T22:54:27.107959: step 2627, loss 0.0272704, acc 0.98
2016-09-07T22:54:27.824948: step 2628, loss 0.114144, acc 0.94
2016-09-07T22:54:28.542593: step 2629, loss 0.0257423, acc 1
2016-09-07T22:54:29.236284: step 2630, loss 0.0196347, acc 0.98
2016-09-07T22:54:29.943471: step 2631, loss 0.0847768, acc 0.94
2016-09-07T22:54:30.630022: step 2632, loss 0.109062, acc 0.96
2016-09-07T22:54:31.329361: step 2633, loss 0.122665, acc 0.92
2016-09-07T22:54:32.041330: step 2634, loss 0.0233588, acc 0.98
2016-09-07T22:54:32.752585: step 2635, loss 0.00807012, acc 1
2016-09-07T22:54:33.450104: step 2636, loss 0.0198936, acc 1
2016-09-07T22:54:34.165083: step 2637, loss 0.0204275, acc 1
2016-09-07T22:54:34.855028: step 2638, loss 0.150861, acc 0.92
2016-09-07T22:54:35.552872: step 2639, loss 0.0689614, acc 0.96
2016-09-07T22:54:36.248138: step 2640, loss 0.11502, acc 0.96
2016-09-07T22:54:36.960763: step 2641, loss 0.0336362, acc 0.98
2016-09-07T22:54:37.661155: step 2642, loss 0.0721457, acc 0.94
2016-09-07T22:54:38.371400: step 2643, loss 0.0709398, acc 0.96
2016-09-07T22:54:39.070107: step 2644, loss 0.058547, acc 0.98
2016-09-07T22:54:39.768448: step 2645, loss 0.0507691, acc 0.96
2016-09-07T22:54:40.487025: step 2646, loss 0.0156286, acc 1
2016-09-07T22:54:41.202432: step 2647, loss 0.00988809, acc 1
2016-09-07T22:54:41.924405: step 2648, loss 0.0421078, acc 0.98
2016-09-07T22:54:42.636363: step 2649, loss 0.050261, acc 0.98
2016-09-07T22:54:43.323445: step 2650, loss 0.0984692, acc 0.98
2016-09-07T22:54:44.038175: step 2651, loss 0.0201741, acc 1
2016-09-07T22:54:44.752884: step 2652, loss 0.0466557, acc 0.98
2016-09-07T22:54:45.452108: step 2653, loss 0.0602015, acc 0.96
2016-09-07T22:54:46.145826: step 2654, loss 0.0375297, acc 0.96
2016-09-07T22:54:46.846405: step 2655, loss 0.0870495, acc 0.94
2016-09-07T22:54:47.551540: step 2656, loss 0.057675, acc 0.98
2016-09-07T22:54:48.269019: step 2657, loss 0.0448352, acc 0.98
2016-09-07T22:54:48.972744: step 2658, loss 0.048342, acc 0.98
2016-09-07T22:54:49.674652: step 2659, loss 0.0574513, acc 0.96
2016-09-07T22:54:50.369642: step 2660, loss 0.00164247, acc 1
2016-09-07T22:54:51.085383: step 2661, loss 0.0474173, acc 0.96
2016-09-07T22:54:51.796049: step 2662, loss 0.0805698, acc 0.94
2016-09-07T22:54:52.484263: step 2663, loss 0.0264083, acc 0.98
2016-09-07T22:54:53.198986: step 2664, loss 0.0425218, acc 0.98
2016-09-07T22:54:53.883413: step 2665, loss 0.0761328, acc 0.98
2016-09-07T22:54:54.588671: step 2666, loss 0.0152528, acc 1
2016-09-07T22:54:55.291813: step 2667, loss 0.0365152, acc 0.98
2016-09-07T22:54:55.985237: step 2668, loss 0.0441344, acc 0.98
2016-09-07T22:54:56.702770: step 2669, loss 0.0179526, acc 1
2016-09-07T22:54:57.418368: step 2670, loss 0.0424676, acc 0.98
2016-09-07T22:54:58.134266: step 2671, loss 0.0440421, acc 0.98
2016-09-07T22:54:58.844789: step 2672, loss 0.0383269, acc 0.98
2016-09-07T22:54:59.570836: step 2673, loss 0.0418273, acc 0.98
2016-09-07T22:55:00.303577: step 2674, loss 0.0368926, acc 1
2016-09-07T22:55:01.020774: step 2675, loss 0.0376257, acc 0.98
2016-09-07T22:55:01.717457: step 2676, loss 0.0346535, acc 1
2016-09-07T22:55:02.405932: step 2677, loss 0.0285559, acc 1
2016-09-07T22:55:03.096633: step 2678, loss 0.0201503, acc 0.98
2016-09-07T22:55:03.785658: step 2679, loss 0.0148155, acc 1
2016-09-07T22:55:04.500101: step 2680, loss 0.0021336, acc 1
2016-09-07T22:55:05.218611: step 2681, loss 0.00248761, acc 1
2016-09-07T22:55:05.913966: step 2682, loss 0.0759261, acc 0.96
2016-09-07T22:55:06.630275: step 2683, loss 0.0661382, acc 0.98
2016-09-07T22:55:07.350409: step 2684, loss 0.0107886, acc 1
2016-09-07T22:55:08.049914: step 2685, loss 0.000200347, acc 1
2016-09-07T22:55:08.751118: step 2686, loss 0.0562735, acc 0.96
2016-09-07T22:55:09.452911: step 2687, loss 0.00958783, acc 1
2016-09-07T22:55:10.152196: step 2688, loss 0.0205806, acc 0.98
2016-09-07T22:55:10.828981: step 2689, loss 0.00291878, acc 1
2016-09-07T22:55:11.526695: step 2690, loss 0.0223155, acc 1
2016-09-07T22:55:12.220999: step 2691, loss 0.00742194, acc 1
2016-09-07T22:55:12.922507: step 2692, loss 0.0482295, acc 0.98
2016-09-07T22:55:13.616286: step 2693, loss 0.139433, acc 0.98
2016-09-07T22:55:14.310543: step 2694, loss 0.0429278, acc 0.98
2016-09-07T22:55:14.999929: step 2695, loss 0.0259202, acc 1
2016-09-07T22:55:15.692267: step 2696, loss 0.0476536, acc 0.96
2016-09-07T22:55:16.392498: step 2697, loss 0.0719857, acc 0.96
2016-09-07T22:55:17.092246: step 2698, loss 0.100774, acc 0.96
2016-09-07T22:55:17.789602: step 2699, loss 0.0570757, acc 0.98
2016-09-07T22:55:18.491792: step 2700, loss 0.0168051, acc 1

Evaluation:
2016-09-07T22:55:21.707845: step 2700, loss 1.79287, acc 0.754

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-2700

2016-09-07T22:55:23.332024: step 2701, loss 0.0354548, acc 1
2016-09-07T22:55:24.038919: step 2702, loss 0.0341879, acc 1
2016-09-07T22:55:24.757560: step 2703, loss 0.0845886, acc 0.96
2016-09-07T22:55:25.463061: step 2704, loss 0.0213322, acc 1
2016-09-07T22:55:26.150961: step 2705, loss 0.150072, acc 0.96
2016-09-07T22:55:26.855732: step 2706, loss 0.0417859, acc 0.98
2016-09-07T22:55:27.565920: step 2707, loss 0.0478285, acc 0.98
2016-09-07T22:55:28.273447: step 2708, loss 0.0233589, acc 1
2016-09-07T22:55:28.980267: step 2709, loss 0.0979389, acc 0.94
2016-09-07T22:55:29.675427: step 2710, loss 0.001632, acc 1
2016-09-07T22:55:30.363861: step 2711, loss 0.0352931, acc 0.98
2016-09-07T22:55:31.058887: step 2712, loss 0.0686315, acc 0.96
2016-09-07T22:55:31.744138: step 2713, loss 0.0295046, acc 1
2016-09-07T22:55:32.455499: step 2714, loss 0.0452201, acc 0.96
2016-09-07T22:55:33.183441: step 2715, loss 0.0549299, acc 0.98
2016-09-07T22:55:33.597401: step 2716, loss 0.105742, acc 0.916667
2016-09-07T22:55:34.308757: step 2717, loss 0.0299743, acc 1
2016-09-07T22:55:35.022347: step 2718, loss 0.0451781, acc 1
2016-09-07T22:55:35.725294: step 2719, loss 0.103947, acc 0.94
2016-09-07T22:55:36.444641: step 2720, loss 0.0332712, acc 0.98
2016-09-07T22:55:37.127143: step 2721, loss 0.0470918, acc 0.98
2016-09-07T22:55:37.829682: step 2722, loss 0.0401423, acc 0.98
2016-09-07T22:55:38.539271: step 2723, loss 0.0201449, acc 1
2016-09-07T22:55:39.255082: step 2724, loss 0.041533, acc 0.98
2016-09-07T22:55:39.960434: step 2725, loss 0.0123163, acc 1
2016-09-07T22:55:40.672231: step 2726, loss 0.0101124, acc 1
2016-09-07T22:55:41.361114: step 2727, loss 0.048519, acc 0.98
2016-09-07T22:55:42.063601: step 2728, loss 0.0345345, acc 1
2016-09-07T22:55:42.780973: step 2729, loss 0.0565828, acc 0.98
2016-09-07T22:55:43.490820: step 2730, loss 0.010711, acc 1
2016-09-07T22:55:44.177876: step 2731, loss 0.0116516, acc 1
2016-09-07T22:55:44.871857: step 2732, loss 0.0794778, acc 0.98
2016-09-07T22:55:45.569338: step 2733, loss 0.00748554, acc 1
2016-09-07T22:55:46.260355: step 2734, loss 0.0831993, acc 0.96
2016-09-07T22:55:47.002687: step 2735, loss 0.0637679, acc 0.96
2016-09-07T22:55:47.701796: step 2736, loss 0.0236972, acc 1
2016-09-07T22:55:48.406918: step 2737, loss 0.0348811, acc 0.98
2016-09-07T22:55:49.118363: step 2738, loss 0.0478064, acc 0.98
2016-09-07T22:55:49.823785: step 2739, loss 0.0239759, acc 1
2016-09-07T22:55:50.512368: step 2740, loss 0.0531606, acc 0.96
2016-09-07T22:55:51.221438: step 2741, loss 0.0245259, acc 0.98
2016-09-07T22:55:51.933265: step 2742, loss 0.0180858, acc 1
2016-09-07T22:55:52.634264: step 2743, loss 0.0444028, acc 0.98
2016-09-07T22:55:53.317492: step 2744, loss 0.00576373, acc 1
2016-09-07T22:55:54.010694: step 2745, loss 0.0058138, acc 1
2016-09-07T22:55:54.702607: step 2746, loss 0.0203049, acc 1
2016-09-07T22:55:55.395617: step 2747, loss 0.0109319, acc 1
2016-09-07T22:55:56.097644: step 2748, loss 0.0979437, acc 0.94
2016-09-07T22:55:56.813820: step 2749, loss 0.0326566, acc 0.98
2016-09-07T22:55:57.508021: step 2750, loss 0.0399736, acc 0.98
2016-09-07T22:55:58.203015: step 2751, loss 0.0696273, acc 0.98
2016-09-07T22:55:58.900915: step 2752, loss 0.0369275, acc 0.98
2016-09-07T22:55:59.587549: step 2753, loss 0.0171901, acc 1
2016-09-07T22:56:00.326037: step 2754, loss 0.0168241, acc 1
2016-09-07T22:56:01.025804: step 2755, loss 0.226873, acc 0.92
2016-09-07T22:56:01.737134: step 2756, loss 0.000291951, acc 1
2016-09-07T22:56:02.443132: step 2757, loss 0.0737165, acc 0.96
2016-09-07T22:56:03.138219: step 2758, loss 0.0294866, acc 0.98
2016-09-07T22:56:03.841716: step 2759, loss 0.0772355, acc 0.98
2016-09-07T22:56:04.552677: step 2760, loss 0.0488744, acc 0.98
2016-09-07T22:56:05.271133: step 2761, loss 0.131164, acc 0.98
2016-09-07T22:56:05.958414: step 2762, loss 0.0737981, acc 0.96
2016-09-07T22:56:06.659242: step 2763, loss 0.0304454, acc 0.98
2016-09-07T22:56:07.357210: step 2764, loss 0.0496215, acc 0.96
2016-09-07T22:56:08.034789: step 2765, loss 0.0277176, acc 1
2016-09-07T22:56:08.731013: step 2766, loss 0.0236856, acc 0.98
2016-09-07T22:56:09.437135: step 2767, loss 0.0146255, acc 1
2016-09-07T22:56:10.152770: step 2768, loss 0.0285123, acc 1
2016-09-07T22:56:10.855746: step 2769, loss 0.0397206, acc 0.98
2016-09-07T22:56:11.557996: step 2770, loss 0.00598938, acc 1
2016-09-07T22:56:12.258626: step 2771, loss 0.0048269, acc 1
2016-09-07T22:56:12.969538: step 2772, loss 0.0298546, acc 0.98
2016-09-07T22:56:13.655701: step 2773, loss 0.034348, acc 1
2016-09-07T22:56:14.384070: step 2774, loss 0.000320545, acc 1
2016-09-07T22:56:15.096825: step 2775, loss 0.0054764, acc 1
2016-09-07T22:56:15.795486: step 2776, loss 0.0980232, acc 0.96
2016-09-07T22:56:16.486040: step 2777, loss 0.0333706, acc 1
2016-09-07T22:56:17.174640: step 2778, loss 0.0223639, acc 0.98
2016-09-07T22:56:17.875301: step 2779, loss 0.0407061, acc 0.98
2016-09-07T22:56:18.569093: step 2780, loss 0.0733473, acc 0.94
2016-09-07T22:56:19.263085: step 2781, loss 0.0481218, acc 0.98
2016-09-07T22:56:19.973806: step 2782, loss 0.0253303, acc 0.98
2016-09-07T22:56:20.679549: step 2783, loss 0.0488375, acc 0.96
2016-09-07T22:56:21.393734: step 2784, loss 0.0438853, acc 0.98
2016-09-07T22:56:22.079633: step 2785, loss 0.00166585, acc 1
2016-09-07T22:56:22.785607: step 2786, loss 0.0417311, acc 0.98
2016-09-07T22:56:23.479366: step 2787, loss 0.0018067, acc 1
2016-09-07T22:56:24.174451: step 2788, loss 0.00530452, acc 1
2016-09-07T22:56:24.863276: step 2789, loss 0.0173191, acc 0.98
2016-09-07T22:56:25.567274: step 2790, loss 0.0245284, acc 0.98
2016-09-07T22:56:26.295434: step 2791, loss 0.0270502, acc 0.98
2016-09-07T22:56:27.005392: step 2792, loss 0.0291567, acc 0.98
2016-09-07T22:56:27.707093: step 2793, loss 0.0253302, acc 1
2016-09-07T22:56:28.389902: step 2794, loss 0.0231376, acc 0.98
2016-09-07T22:56:29.090736: step 2795, loss 0.0530476, acc 0.98
2016-09-07T22:56:29.782951: step 2796, loss 0.0314183, acc 1
2016-09-07T22:56:30.491746: step 2797, loss 0.0172597, acc 1
2016-09-07T22:56:31.204148: step 2798, loss 0.0273933, acc 0.98
2016-09-07T22:56:31.907751: step 2799, loss 0.0934405, acc 0.94
2016-09-07T22:56:32.630441: step 2800, loss 0.0169853, acc 1

Evaluation:
2016-09-07T22:56:35.846346: step 2800, loss 1.98591, acc 0.743

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-2800

2016-09-07T22:56:37.589799: step 2801, loss 0.0587793, acc 0.98
2016-09-07T22:56:38.302732: step 2802, loss 0.0205159, acc 1
2016-09-07T22:56:39.008872: step 2803, loss 0.0258542, acc 0.98
2016-09-07T22:56:39.709405: step 2804, loss 0.0336314, acc 0.98
2016-09-07T22:56:40.399479: step 2805, loss 0.0514493, acc 0.96
2016-09-07T22:56:41.091529: step 2806, loss 0.0741067, acc 0.98
2016-09-07T22:56:41.787175: step 2807, loss 0.123232, acc 0.98
2016-09-07T22:56:42.492985: step 2808, loss 0.0277229, acc 0.98
2016-09-07T22:56:43.206768: step 2809, loss 0.00839254, acc 1
2016-09-07T22:56:43.901131: step 2810, loss 0.0186348, acc 1
2016-09-07T22:56:44.592892: step 2811, loss 0.0356048, acc 0.98
2016-09-07T22:56:45.319360: step 2812, loss 0.0232009, acc 1
2016-09-07T22:56:46.025953: step 2813, loss 0.076609, acc 0.96
2016-09-07T22:56:46.744719: step 2814, loss 0.0281568, acc 0.98
2016-09-07T22:56:47.455151: step 2815, loss 0.0529155, acc 0.98
2016-09-07T22:56:48.159851: step 2816, loss 0.0428505, acc 0.96
2016-09-07T22:56:48.862911: step 2817, loss 0.0197313, acc 1
2016-09-07T22:56:49.551348: step 2818, loss 0.037926, acc 0.98
2016-09-07T22:56:50.241661: step 2819, loss 0.0342154, acc 0.98
2016-09-07T22:56:50.947473: step 2820, loss 0.0201691, acc 0.98
2016-09-07T22:56:51.642485: step 2821, loss 0.0173426, acc 1
2016-09-07T22:56:52.351681: step 2822, loss 0.0558106, acc 0.98
2016-09-07T22:56:53.075565: step 2823, loss 0.00909528, acc 1
2016-09-07T22:56:53.795508: step 2824, loss 0.101169, acc 0.96
2016-09-07T22:56:54.478207: step 2825, loss 0.00840528, acc 1
2016-09-07T22:56:55.169536: step 2826, loss 0.028829, acc 0.98
2016-09-07T22:56:55.851802: step 2827, loss 0.0310747, acc 1
2016-09-07T22:56:56.570397: step 2828, loss 0.0136395, acc 1
2016-09-07T22:56:57.269400: step 2829, loss 0.0908452, acc 0.96
2016-09-07T22:56:57.993218: step 2830, loss 0.0234522, acc 0.98
2016-09-07T22:56:58.707566: step 2831, loss 0.0912482, acc 0.94
2016-09-07T22:56:59.414720: step 2832, loss 0.113399, acc 0.94
2016-09-07T22:57:00.136513: step 2833, loss 0.0209585, acc 0.98
2016-09-07T22:57:00.869699: step 2834, loss 0.0385175, acc 0.98
2016-09-07T22:57:01.565921: step 2835, loss 0.00620017, acc 1
2016-09-07T22:57:02.280013: step 2836, loss 0.202483, acc 0.96
2016-09-07T22:57:02.987486: step 2837, loss 0.0325889, acc 0.98
2016-09-07T22:57:03.696571: step 2838, loss 0.110249, acc 0.98
2016-09-07T22:57:04.396480: step 2839, loss 0.0384868, acc 0.98
2016-09-07T22:57:05.087397: step 2840, loss 0.0185484, acc 1
2016-09-07T22:57:05.782872: step 2841, loss 0.191889, acc 0.92
2016-09-07T22:57:06.473849: step 2842, loss 0.0157477, acc 1
2016-09-07T22:57:07.173379: step 2843, loss 0.0174539, acc 1
2016-09-07T22:57:07.871286: step 2844, loss 0.000856839, acc 1
2016-09-07T22:57:08.569950: step 2845, loss 0.0385678, acc 0.98
2016-09-07T22:57:09.276887: step 2846, loss 0.00992137, acc 1
2016-09-07T22:57:09.981645: step 2847, loss 0.0191515, acc 1
2016-09-07T22:57:10.686104: step 2848, loss 0.0351648, acc 0.98
2016-09-07T22:57:11.381696: step 2849, loss 0.00754139, acc 1
2016-09-07T22:57:12.096955: step 2850, loss 0.0179026, acc 1
2016-09-07T22:57:12.795938: step 2851, loss 0.0338561, acc 0.98
2016-09-07T22:57:13.489460: step 2852, loss 0.0202873, acc 0.98
2016-09-07T22:57:14.179206: step 2853, loss 0.00802102, acc 1
2016-09-07T22:57:14.862323: step 2854, loss 0.00272981, acc 1
2016-09-07T22:57:15.567229: step 2855, loss 0.0460819, acc 0.98
2016-09-07T22:57:16.252251: step 2856, loss 0.0182225, acc 0.98
2016-09-07T22:57:16.954228: step 2857, loss 0.0875529, acc 0.98
2016-09-07T22:57:17.653531: step 2858, loss 0.111734, acc 0.94
2016-09-07T22:57:18.339593: step 2859, loss 0.0600357, acc 0.98
2016-09-07T22:57:19.055650: step 2860, loss 0.00265952, acc 1
2016-09-07T22:57:19.767785: step 2861, loss 0.050967, acc 0.98
2016-09-07T22:57:20.469711: step 2862, loss 0.013745, acc 1
2016-09-07T22:57:21.204336: step 2863, loss 0.150871, acc 0.94
2016-09-07T22:57:21.908445: step 2864, loss 0.00253966, acc 1
2016-09-07T22:57:22.622498: step 2865, loss 0.0314888, acc 0.98
2016-09-07T22:57:23.327554: step 2866, loss 0.00140788, acc 1
2016-09-07T22:57:24.032327: step 2867, loss 0.120386, acc 0.92
2016-09-07T22:57:24.740225: step 2868, loss 0.138651, acc 0.96
2016-09-07T22:57:25.427612: step 2869, loss 0.0264706, acc 0.98
2016-09-07T22:57:26.115513: step 2870, loss 0.115443, acc 0.98
2016-09-07T22:57:26.810040: step 2871, loss 0.0362849, acc 0.98
2016-09-07T22:57:27.491942: step 2872, loss 0.00769954, acc 1
2016-09-07T22:57:28.185205: step 2873, loss 0.0122393, acc 1
2016-09-07T22:57:28.875610: step 2874, loss 0.0231894, acc 1
2016-09-07T22:57:29.584859: step 2875, loss 0.00590025, acc 1
2016-09-07T22:57:30.274707: step 2876, loss 0.130396, acc 0.94
2016-09-07T22:57:30.963544: step 2877, loss 0.0567267, acc 0.98
2016-09-07T22:57:31.652310: step 2878, loss 0.038188, acc 0.98
2016-09-07T22:57:32.340646: step 2879, loss 0.00345056, acc 1
2016-09-07T22:57:33.029499: step 2880, loss 0.0604332, acc 0.96
2016-09-07T22:57:33.713188: step 2881, loss 0.00177155, acc 1
2016-09-07T22:57:34.411010: step 2882, loss 0.0331609, acc 1
2016-09-07T22:57:35.093910: step 2883, loss 0.0172462, acc 1
2016-09-07T22:57:35.777234: step 2884, loss 0.0234952, acc 0.98
2016-09-07T22:57:36.486188: step 2885, loss 0.0975923, acc 0.96
2016-09-07T22:57:37.215089: step 2886, loss 0.0838395, acc 0.96
2016-09-07T22:57:37.913698: step 2887, loss 0.0299449, acc 0.98
2016-09-07T22:57:38.598996: step 2888, loss 0.0543268, acc 0.98
2016-09-07T22:57:39.290941: step 2889, loss 0.0328089, acc 0.98
2016-09-07T22:57:39.971709: step 2890, loss 0.0199317, acc 1
2016-09-07T22:57:40.684601: step 2891, loss 0.030929, acc 0.98
2016-09-07T22:57:41.391792: step 2892, loss 0.0622274, acc 0.98
2016-09-07T22:57:42.099936: step 2893, loss 0.00248033, acc 1
2016-09-07T22:57:42.794048: step 2894, loss 0.000607537, acc 1
2016-09-07T22:57:43.482443: step 2895, loss 0.0832892, acc 0.94
2016-09-07T22:57:44.178500: step 2896, loss 0.0588781, acc 0.96
2016-09-07T22:57:44.896910: step 2897, loss 0.13968, acc 0.94
2016-09-07T22:57:45.585162: step 2898, loss 0.00929779, acc 1
2016-09-07T22:57:46.274413: step 2899, loss 0.00495858, acc 1
2016-09-07T22:57:46.973137: step 2900, loss 0.0347201, acc 0.98

Evaluation:
2016-09-07T22:57:50.206431: step 2900, loss 1.67208, acc 0.758

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-2900

2016-09-07T22:57:51.959096: step 2901, loss 0.0191634, acc 0.98
2016-09-07T22:57:52.657269: step 2902, loss 0.0583909, acc 0.96
2016-09-07T22:57:53.346021: step 2903, loss 0.0926103, acc 0.96
2016-09-07T22:57:54.062992: step 2904, loss 0.0158281, acc 1
2016-09-07T22:57:54.764029: step 2905, loss 0.0469804, acc 0.98
2016-09-07T22:57:55.456561: step 2906, loss 0.0223054, acc 1
2016-09-07T22:57:56.181118: step 2907, loss 0.03318, acc 0.98
2016-09-07T22:57:56.880010: step 2908, loss 0.0197574, acc 1
2016-09-07T22:57:57.582778: step 2909, loss 0.0127275, acc 1
2016-09-07T22:57:57.948276: step 2910, loss 0.198681, acc 0.916667
2016-09-07T22:57:58.647144: step 2911, loss 0.0405663, acc 0.96
2016-09-07T22:57:59.340901: step 2912, loss 0.0475914, acc 0.98
2016-09-07T22:58:00.032055: step 2913, loss 0.0450314, acc 0.98
2016-09-07T22:58:00.780785: step 2914, loss 0.0265446, acc 1
2016-09-07T22:58:01.502498: step 2915, loss 0.0312382, acc 1
2016-09-07T22:58:02.198984: step 2916, loss 0.103754, acc 0.94
2016-09-07T22:58:02.908936: step 2917, loss 0.0243493, acc 0.98
2016-09-07T22:58:03.629781: step 2918, loss 0.0686676, acc 0.94
2016-09-07T22:58:04.332105: step 2919, loss 0.158777, acc 0.98
2016-09-07T22:58:05.040710: step 2920, loss 0.0366239, acc 1
2016-09-07T22:58:05.736286: step 2921, loss 0.0169544, acc 1
2016-09-07T22:58:06.444258: step 2922, loss 0.0499918, acc 0.98
2016-09-07T22:58:07.130953: step 2923, loss 0.0773141, acc 0.96
2016-09-07T22:58:07.823224: step 2924, loss 0.0461482, acc 0.96
2016-09-07T22:58:08.560735: step 2925, loss 0.00880864, acc 1
2016-09-07T22:58:09.261947: step 2926, loss 0.0108675, acc 1
2016-09-07T22:58:09.972328: step 2927, loss 0.0785077, acc 0.98
2016-09-07T22:58:10.669480: step 2928, loss 0.0124622, acc 1
2016-09-07T22:58:11.362342: step 2929, loss 0.0282549, acc 1
2016-09-07T22:58:12.052346: step 2930, loss 0.0550661, acc 0.96
2016-09-07T22:58:12.763839: step 2931, loss 0.0326302, acc 0.98
2016-09-07T22:58:13.450667: step 2932, loss 0.0251917, acc 0.98
2016-09-07T22:58:14.154505: step 2933, loss 0.0191591, acc 1
2016-09-07T22:58:14.860800: step 2934, loss 0.0185695, acc 1
2016-09-07T22:58:15.566270: step 2935, loss 0.0318797, acc 0.98
2016-09-07T22:58:16.251963: step 2936, loss 0.0902873, acc 0.98
2016-09-07T22:58:16.954133: step 2937, loss 0.0536914, acc 0.96
2016-09-07T22:58:17.638112: step 2938, loss 0.0246044, acc 1
2016-09-07T22:58:18.328026: step 2939, loss 0.0201002, acc 1
2016-09-07T22:58:19.030798: step 2940, loss 0.024416, acc 0.98
2016-09-07T22:58:19.721952: step 2941, loss 0.02752, acc 0.98
2016-09-07T22:58:20.430116: step 2942, loss 0.100834, acc 0.96
2016-09-07T22:58:21.131316: step 2943, loss 0.0282683, acc 0.98
2016-09-07T22:58:21.834850: step 2944, loss 0.0162054, acc 1
2016-09-07T22:58:22.533757: step 2945, loss 0.0241749, acc 1
2016-09-07T22:58:23.242413: step 2946, loss 0.0594966, acc 0.98
2016-09-07T22:58:23.942771: step 2947, loss 0.0252497, acc 1
2016-09-07T22:58:24.639009: step 2948, loss 0.00594086, acc 1
2016-09-07T22:58:25.343250: step 2949, loss 0.0599524, acc 0.96
2016-09-07T22:58:26.055744: step 2950, loss 0.000645313, acc 1
2016-09-07T22:58:26.762462: step 2951, loss 0.0675317, acc 0.94
2016-09-07T22:58:27.456866: step 2952, loss 0.00480211, acc 1
2016-09-07T22:58:28.163030: step 2953, loss 0.00535385, acc 1
2016-09-07T22:58:28.862659: step 2954, loss 0.0355028, acc 1
2016-09-07T22:58:29.570494: step 2955, loss 0.0540989, acc 0.96
2016-09-07T22:58:30.259776: step 2956, loss 0.053615, acc 0.98
2016-09-07T22:58:30.950618: step 2957, loss 0.0128594, acc 1
2016-09-07T22:58:31.651814: step 2958, loss 0.054962, acc 0.98
2016-09-07T22:58:32.342398: step 2959, loss 0.0975788, acc 0.94
2016-09-07T22:58:33.049858: step 2960, loss 0.00579181, acc 1
2016-09-07T22:58:33.763189: step 2961, loss 0.0486522, acc 0.96
2016-09-07T22:58:34.474241: step 2962, loss 0.00667046, acc 1
2016-09-07T22:58:35.178655: step 2963, loss 0.00185414, acc 1
2016-09-07T22:58:35.882205: step 2964, loss 0.053584, acc 0.96
2016-09-07T22:58:36.592801: step 2965, loss 0.010953, acc 1
2016-09-07T22:58:37.301671: step 2966, loss 0.0439405, acc 0.98
2016-09-07T22:58:37.992079: step 2967, loss 0.0223648, acc 1
2016-09-07T22:58:38.707190: step 2968, loss 0.0298662, acc 0.98
2016-09-07T22:58:39.400680: step 2969, loss 0.00854058, acc 1
2016-09-07T22:58:40.090926: step 2970, loss 0.0409886, acc 0.98
2016-09-07T22:58:40.771871: step 2971, loss 0.0628624, acc 0.98
2016-09-07T22:58:41.464156: step 2972, loss 0.0144297, acc 1
2016-09-07T22:58:42.148075: step 2973, loss 0.0325806, acc 0.98
2016-09-07T22:58:42.859799: step 2974, loss 0.0222911, acc 1
2016-09-07T22:58:43.562740: step 2975, loss 0.126054, acc 0.98
2016-09-07T22:58:44.277251: step 2976, loss 0.109515, acc 0.96
2016-09-07T22:58:44.977915: step 2977, loss 0.00834849, acc 1
2016-09-07T22:58:45.671951: step 2978, loss 0.0186691, acc 0.98
2016-09-07T22:58:46.364221: step 2979, loss 0.0516597, acc 0.98
2016-09-07T22:58:47.049871: step 2980, loss 0.0643488, acc 0.96
2016-09-07T22:58:47.750901: step 2981, loss 0.00347258, acc 1
2016-09-07T22:58:48.451983: step 2982, loss 0.100396, acc 0.94
2016-09-07T22:58:49.155069: step 2983, loss 0.0194109, acc 0.98
2016-09-07T22:58:49.871053: step 2984, loss 0.0486825, acc 0.96
2016-09-07T22:58:50.579371: step 2985, loss 0.0322882, acc 0.98
2016-09-07T22:58:51.271743: step 2986, loss 0.068579, acc 0.96
2016-09-07T22:58:51.982985: step 2987, loss 0.026231, acc 0.98
2016-09-07T22:58:52.679205: step 2988, loss 0.00222839, acc 1
2016-09-07T22:58:53.390642: step 2989, loss 0.0822634, acc 0.98
2016-09-07T22:58:54.126095: step 2990, loss 0.0306548, acc 0.98
2016-09-07T22:58:54.839153: step 2991, loss 0.0241271, acc 0.98
2016-09-07T22:58:55.552243: step 2992, loss 0.00829297, acc 1
2016-09-07T22:58:56.282193: step 2993, loss 0.0332974, acc 0.98
2016-09-07T22:58:57.013290: step 2994, loss 0.0319187, acc 0.98
2016-09-07T22:58:57.699265: step 2995, loss 0.00471018, acc 1
2016-09-07T22:58:58.423336: step 2996, loss 0.0205648, acc 1
2016-09-07T22:58:59.107616: step 2997, loss 0.0543463, acc 0.98
2016-09-07T22:58:59.807855: step 2998, loss 0.0286952, acc 1
2016-09-07T22:59:00.548798: step 2999, loss 0.0631076, acc 0.96
2016-09-07T22:59:01.254261: step 3000, loss 0.0340226, acc 0.96

Evaluation:
2016-09-07T22:59:04.488848: step 3000, loss 1.95143, acc 0.75

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-3000

2016-09-07T22:59:06.146862: step 3001, loss 0.0109571, acc 1
2016-09-07T22:59:06.855125: step 3002, loss 0.0201967, acc 1
2016-09-07T22:59:07.560144: step 3003, loss 0.00549142, acc 1
2016-09-07T22:59:08.267017: step 3004, loss 0.00936446, acc 1
2016-09-07T22:59:08.961310: step 3005, loss 0.025798, acc 1
2016-09-07T22:59:09.670085: step 3006, loss 0.0295007, acc 0.98
2016-09-07T22:59:10.354834: step 3007, loss 0.0115934, acc 1
2016-09-07T22:59:11.075289: step 3008, loss 0.0952169, acc 0.96
2016-09-07T22:59:11.778963: step 3009, loss 0.14416, acc 0.96
2016-09-07T22:59:12.491581: step 3010, loss 0.0884615, acc 0.96
2016-09-07T22:59:13.186772: step 3011, loss 0.0294448, acc 0.98
2016-09-07T22:59:13.880973: step 3012, loss 0.168219, acc 0.96
2016-09-07T22:59:14.597764: step 3013, loss 0.0343394, acc 0.98
2016-09-07T22:59:15.293871: step 3014, loss 0.0648659, acc 0.98
2016-09-07T22:59:15.999274: step 3015, loss 0.0151447, acc 1
2016-09-07T22:59:16.696130: step 3016, loss 0.0253328, acc 0.98
2016-09-07T22:59:17.386633: step 3017, loss 0.0476261, acc 0.98
2016-09-07T22:59:18.087833: step 3018, loss 0.00453916, acc 1
2016-09-07T22:59:18.800988: step 3019, loss 0.077831, acc 0.96
2016-09-07T22:59:19.481074: step 3020, loss 0.0265758, acc 0.98
2016-09-07T22:59:20.182958: step 3021, loss 0.0637469, acc 0.96
2016-09-07T22:59:20.886125: step 3022, loss 0.0561354, acc 0.96
2016-09-07T22:59:21.580924: step 3023, loss 0.00131858, acc 1
2016-09-07T22:59:22.267182: step 3024, loss 0.0600436, acc 0.96
2016-09-07T22:59:22.974116: step 3025, loss 0.00111195, acc 1
2016-09-07T22:59:23.653389: step 3026, loss 0.0243281, acc 1
2016-09-07T22:59:24.372909: step 3027, loss 0.129516, acc 0.98
2016-09-07T22:59:25.061064: step 3028, loss 0.027005, acc 0.98
2016-09-07T22:59:25.751928: step 3029, loss 0.0327426, acc 0.98
2016-09-07T22:59:26.443621: step 3030, loss 0.0267102, acc 1
2016-09-07T22:59:27.136323: step 3031, loss 0.0162008, acc 0.98
2016-09-07T22:59:27.830230: step 3032, loss 0.0107393, acc 1
2016-09-07T22:59:28.516077: step 3033, loss 0.016193, acc 1
2016-09-07T22:59:29.211014: step 3034, loss 0.053351, acc 0.96
2016-09-07T22:59:29.894561: step 3035, loss 0.0301167, acc 0.98
2016-09-07T22:59:30.572536: step 3036, loss 0.0545488, acc 0.98
2016-09-07T22:59:31.287924: step 3037, loss 0.0316363, acc 1
2016-09-07T22:59:32.002627: step 3038, loss 0.0121148, acc 1
2016-09-07T22:59:32.703657: step 3039, loss 0.0118357, acc 1
2016-09-07T22:59:33.384488: step 3040, loss 0.0198597, acc 1
2016-09-07T22:59:34.084847: step 3041, loss 0.0275294, acc 1
2016-09-07T22:59:34.775639: step 3042, loss 0.0294035, acc 0.98
2016-09-07T22:59:35.468079: step 3043, loss 0.0295766, acc 1
2016-09-07T22:59:36.165082: step 3044, loss 0.0254676, acc 0.98
2016-09-07T22:59:36.865538: step 3045, loss 0.103737, acc 0.96
2016-09-07T22:59:37.556536: step 3046, loss 0.00938602, acc 1
2016-09-07T22:59:38.267743: step 3047, loss 0.0313679, acc 0.98
2016-09-07T22:59:38.964161: step 3048, loss 0.0115867, acc 1
2016-09-07T22:59:39.672680: step 3049, loss 0.132909, acc 0.96
2016-09-07T22:59:40.378478: step 3050, loss 0.0859521, acc 0.98
2016-09-07T22:59:41.061430: step 3051, loss 0.0172309, acc 1
2016-09-07T22:59:41.771301: step 3052, loss 0.0220378, acc 0.98
2016-09-07T22:59:42.512404: step 3053, loss 0.0370244, acc 0.98
2016-09-07T22:59:43.214070: step 3054, loss 0.125527, acc 0.94
2016-09-07T22:59:43.903484: step 3055, loss 0.0113533, acc 1
2016-09-07T22:59:44.596966: step 3056, loss 0.0197001, acc 0.98
2016-09-07T22:59:45.297007: step 3057, loss 0.0522333, acc 0.96
2016-09-07T22:59:45.981663: step 3058, loss 0.0297747, acc 0.98
2016-09-07T22:59:46.669289: step 3059, loss 0.017501, acc 1
2016-09-07T22:59:47.346871: step 3060, loss 0.0235929, acc 1
2016-09-07T22:59:48.048906: step 3061, loss 0.0382309, acc 0.96
2016-09-07T22:59:48.730780: step 3062, loss 0.0593842, acc 0.98
2016-09-07T22:59:49.419057: step 3063, loss 0.0668795, acc 0.96
2016-09-07T22:59:50.115616: step 3064, loss 0.00981966, acc 1
2016-09-07T22:59:50.823284: step 3065, loss 0.0620921, acc 0.98
2016-09-07T22:59:51.520193: step 3066, loss 0.068088, acc 0.98
2016-09-07T22:59:52.219692: step 3067, loss 0.041443, acc 0.98
2016-09-07T22:59:52.916713: step 3068, loss 0.00825841, acc 1
2016-09-07T22:59:53.624390: step 3069, loss 0.00242609, acc 1
2016-09-07T22:59:54.328191: step 3070, loss 0.0356435, acc 0.98
2016-09-07T22:59:55.023713: step 3071, loss 0.0465863, acc 0.98
2016-09-07T22:59:55.726016: step 3072, loss 0.0328452, acc 0.98
2016-09-07T22:59:56.421721: step 3073, loss 0.0116389, acc 1
2016-09-07T22:59:57.101559: step 3074, loss 0.0206646, acc 1
2016-09-07T22:59:57.796503: step 3075, loss 0.0680876, acc 0.94
2016-09-07T22:59:58.498793: step 3076, loss 0.0533574, acc 0.98
2016-09-07T22:59:59.197891: step 3077, loss 0.00315539, acc 1
2016-09-07T22:59:59.889827: step 3078, loss 0.0348278, acc 1
2016-09-07T23:00:00.618825: step 3079, loss 0.0489056, acc 0.98
2016-09-07T23:00:01.299337: step 3080, loss 0.04485, acc 0.96
2016-09-07T23:00:01.998516: step 3081, loss 0.0241522, acc 1
2016-09-07T23:00:02.675913: step 3082, loss 0.0528793, acc 0.98
2016-09-07T23:00:03.349906: step 3083, loss 0.0150158, acc 1
2016-09-07T23:00:04.075062: step 3084, loss 0.0752012, acc 0.96
2016-09-07T23:00:04.776664: step 3085, loss 0.0284629, acc 0.98
2016-09-07T23:00:05.481076: step 3086, loss 0.000831529, acc 1
2016-09-07T23:00:06.177312: step 3087, loss 0.004016, acc 1
2016-09-07T23:00:06.886818: step 3088, loss 0.03596, acc 0.98
2016-09-07T23:00:07.598908: step 3089, loss 0.0167438, acc 1
2016-09-07T23:00:08.291556: step 3090, loss 0.019676, acc 1
2016-09-07T23:00:08.984678: step 3091, loss 0.00499725, acc 1
2016-09-07T23:00:09.699098: step 3092, loss 0.00885101, acc 1
2016-09-07T23:00:10.388796: step 3093, loss 0.0358613, acc 0.98
2016-09-07T23:00:11.087211: step 3094, loss 0.0241344, acc 1
2016-09-07T23:00:11.784712: step 3095, loss 0.0105613, acc 1
2016-09-07T23:00:12.508508: step 3096, loss 0.0320651, acc 0.98
2016-09-07T23:00:13.214182: step 3097, loss 0.0243316, acc 1
2016-09-07T23:00:13.906803: step 3098, loss 0.00137248, acc 1
2016-09-07T23:00:14.599895: step 3099, loss 0.0262544, acc 1
2016-09-07T23:00:15.305818: step 3100, loss 0.0391666, acc 0.98

Evaluation:
2016-09-07T23:00:18.530570: step 3100, loss 2.40554, acc 0.747

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-3100

2016-09-07T23:00:20.263466: step 3101, loss 0.199226, acc 0.94
2016-09-07T23:00:20.954294: step 3102, loss 0.0427125, acc 0.98
2016-09-07T23:00:21.652926: step 3103, loss 0.000428779, acc 1
2016-09-07T23:00:22.037049: step 3104, loss 1.7493e-05, acc 1
2016-09-07T23:00:22.732558: step 3105, loss 0.0293916, acc 1
2016-09-07T23:00:23.447135: step 3106, loss 0.00737953, acc 1
2016-09-07T23:00:24.155328: step 3107, loss 0.0296797, acc 1
2016-09-07T23:00:24.854344: step 3108, loss 0.146247, acc 0.94
2016-09-07T23:00:25.544643: step 3109, loss 0.0537458, acc 0.96
2016-09-07T23:00:26.247275: step 3110, loss 0.0516446, acc 0.98
2016-09-07T23:00:26.940122: step 3111, loss 0.00150787, acc 1
2016-09-07T23:00:27.638272: step 3112, loss 0.147855, acc 0.98
2016-09-07T23:00:28.356145: step 3113, loss 0.0206131, acc 0.98
2016-09-07T23:00:29.061867: step 3114, loss 0.0355282, acc 1
2016-09-07T23:00:29.767710: step 3115, loss 0.00796131, acc 1
2016-09-07T23:00:30.502761: step 3116, loss 0.0124663, acc 1
2016-09-07T23:00:31.204189: step 3117, loss 0.0631614, acc 0.96
2016-09-07T23:00:31.909349: step 3118, loss 0.0308653, acc 0.98
2016-09-07T23:00:32.604773: step 3119, loss 0.179037, acc 0.96
2016-09-07T23:00:33.318661: step 3120, loss 0.0702041, acc 0.98
2016-09-07T23:00:34.013564: step 3121, loss 0.0841193, acc 0.96
2016-09-07T23:00:34.726561: step 3122, loss 0.0306491, acc 0.98
2016-09-07T23:00:35.406095: step 3123, loss 0.0691002, acc 0.94
2016-09-07T23:00:36.096313: step 3124, loss 0.0026156, acc 1
2016-09-07T23:00:36.793324: step 3125, loss 0.0544722, acc 0.98
2016-09-07T23:00:37.511666: step 3126, loss 0.0370762, acc 0.98
2016-09-07T23:00:38.223052: step 3127, loss 0.0139689, acc 1
2016-09-07T23:00:38.934123: step 3128, loss 0.0206087, acc 0.98
2016-09-07T23:00:39.639149: step 3129, loss 0.0641751, acc 0.98
2016-09-07T23:00:40.334936: step 3130, loss 0.00631338, acc 1
2016-09-07T23:00:41.041784: step 3131, loss 0.0110244, acc 1
2016-09-07T23:00:41.748732: step 3132, loss 0.00871686, acc 1
2016-09-07T23:00:42.443231: step 3133, loss 0.0160132, acc 1
2016-09-07T23:00:43.146851: step 3134, loss 0.0147264, acc 1
2016-09-07T23:00:43.844497: step 3135, loss 0.0104692, acc 1
2016-09-07T23:00:44.531126: step 3136, loss 0.0693502, acc 0.96
2016-09-07T23:00:45.230836: step 3137, loss 0.0206106, acc 0.98
2016-09-07T23:00:45.924024: step 3138, loss 0.0281171, acc 0.98
2016-09-07T23:00:46.630072: step 3139, loss 0.00831784, acc 1
2016-09-07T23:00:47.324961: step 3140, loss 0.12615, acc 0.96
2016-09-07T23:00:48.013663: step 3141, loss 0.0884141, acc 0.96
2016-09-07T23:00:48.719189: step 3142, loss 0.0550465, acc 0.96
2016-09-07T23:00:49.419533: step 3143, loss 0.0125045, acc 1
2016-09-07T23:00:50.121161: step 3144, loss 0.00988806, acc 1
2016-09-07T23:00:50.824480: step 3145, loss 0.0918141, acc 0.96
2016-09-07T23:00:51.528040: step 3146, loss 0.0152952, acc 1
2016-09-07T23:00:52.225729: step 3147, loss 0.000431708, acc 1
2016-09-07T23:00:52.932011: step 3148, loss 0.0392004, acc 0.98
2016-09-07T23:00:53.634226: step 3149, loss 0.160617, acc 0.96
2016-09-07T23:00:54.332486: step 3150, loss 0.100783, acc 0.96
2016-09-07T23:00:55.034956: step 3151, loss 0.159434, acc 0.96
2016-09-07T23:00:55.740397: step 3152, loss 0.0111746, acc 1
2016-09-07T23:00:56.433927: step 3153, loss 0.00573675, acc 1
2016-09-07T23:00:57.118241: step 3154, loss 0.0378794, acc 0.98
2016-09-07T23:00:57.809462: step 3155, loss 0.0172004, acc 1
2016-09-07T23:00:58.499478: step 3156, loss 0.0163197, acc 0.98
2016-09-07T23:00:59.190847: step 3157, loss 0.00160288, acc 1
2016-09-07T23:00:59.908489: step 3158, loss 0.0200466, acc 0.98
2016-09-07T23:01:00.648899: step 3159, loss 0.0313437, acc 0.98
2016-09-07T23:01:01.342035: step 3160, loss 0.0607802, acc 0.98
2016-09-07T23:01:02.067015: step 3161, loss 0.0508794, acc 0.96
2016-09-07T23:01:02.769858: step 3162, loss 0.0244312, acc 0.98
2016-09-07T23:01:03.469074: step 3163, loss 0.0189981, acc 1
2016-09-07T23:01:04.168871: step 3164, loss 0.0311943, acc 0.98
2016-09-07T23:01:04.859176: step 3165, loss 0.0235149, acc 1
2016-09-07T23:01:05.550751: step 3166, loss 0.0136969, acc 1
2016-09-07T23:01:06.241933: step 3167, loss 0.0518451, acc 0.98
2016-09-07T23:01:06.948646: step 3168, loss 0.0183461, acc 1
2016-09-07T23:01:07.655141: step 3169, loss 0.0799067, acc 0.94
2016-09-07T23:01:08.363514: step 3170, loss 0.0656239, acc 0.98
2016-09-07T23:01:09.049076: step 3171, loss 0.00655426, acc 1
2016-09-07T23:01:09.754763: step 3172, loss 0.00231957, acc 1
2016-09-07T23:01:10.477801: step 3173, loss 0.0655339, acc 0.96
2016-09-07T23:01:11.177883: step 3174, loss 0.0365797, acc 0.96
2016-09-07T23:01:11.870715: step 3175, loss 0.0118819, acc 1
2016-09-07T23:01:12.579910: step 3176, loss 0.0405581, acc 0.98
2016-09-07T23:01:13.266117: step 3177, loss 0.0120127, acc 1
2016-09-07T23:01:13.973420: step 3178, loss 0.00176744, acc 1
2016-09-07T23:01:14.658008: step 3179, loss 0.00462544, acc 1
2016-09-07T23:01:15.358543: step 3180, loss 0.0323925, acc 0.98
2016-09-07T23:01:16.094238: step 3181, loss 0.0296264, acc 0.98
2016-09-07T23:01:16.797287: step 3182, loss 0.0161903, acc 1
2016-09-07T23:01:17.508298: step 3183, loss 0.00635305, acc 1
2016-09-07T23:01:18.200010: step 3184, loss 0.0206821, acc 1
2016-09-07T23:01:18.876660: step 3185, loss 0.0659089, acc 0.98
2016-09-07T23:01:19.559913: step 3186, loss 0.0466668, acc 0.96
2016-09-07T23:01:20.261838: step 3187, loss 0.0467236, acc 0.96
2016-09-07T23:01:20.957506: step 3188, loss 0.0202688, acc 1
2016-09-07T23:01:21.663783: step 3189, loss 0.0183498, acc 1
2016-09-07T23:01:22.361144: step 3190, loss 0.0655111, acc 0.96
2016-09-07T23:01:23.055006: step 3191, loss 0.0228432, acc 0.98
2016-09-07T23:01:23.749732: step 3192, loss 0.0687653, acc 0.96
2016-09-07T23:01:24.436664: step 3193, loss 0.0323387, acc 0.98
2016-09-07T23:01:25.130212: step 3194, loss 0.0123087, acc 1
2016-09-07T23:01:25.835066: step 3195, loss 0.00315653, acc 1
2016-09-07T23:01:26.520844: step 3196, loss 0.161454, acc 0.98
2016-09-07T23:01:27.219686: step 3197, loss 0.0126282, acc 1
2016-09-07T23:01:27.922946: step 3198, loss 0.012344, acc 1
2016-09-07T23:01:28.636304: step 3199, loss 0.0282614, acc 1
2016-09-07T23:01:29.324561: step 3200, loss 0.105889, acc 0.96

Evaluation:
2016-09-07T23:01:32.598925: step 3200, loss 2.4848, acc 0.709

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-3200

2016-09-07T23:01:34.321333: step 3201, loss 0.0109497, acc 1
2016-09-07T23:01:35.027076: step 3202, loss 0.190089, acc 0.88
2016-09-07T23:01:35.723212: step 3203, loss 0.0575145, acc 0.96
2016-09-07T23:01:36.437370: step 3204, loss 0.0846052, acc 0.94
2016-09-07T23:01:37.119498: step 3205, loss 0.0297576, acc 0.98
2016-09-07T23:01:37.834012: step 3206, loss 0.0152522, acc 0.98
2016-09-07T23:01:38.531541: step 3207, loss 0.109956, acc 0.96
2016-09-07T23:01:39.219833: step 3208, loss 0.0187784, acc 1
2016-09-07T23:01:39.931480: step 3209, loss 0.019298, acc 0.98
2016-09-07T23:01:40.645724: step 3210, loss 0.0187771, acc 1
2016-09-07T23:01:41.356777: step 3211, loss 0.0398067, acc 1
2016-09-07T23:01:42.057123: step 3212, loss 0.0495388, acc 0.96
2016-09-07T23:01:42.761011: step 3213, loss 0.0395982, acc 0.98
2016-09-07T23:01:43.442220: step 3214, loss 0.0234127, acc 1
2016-09-07T23:01:44.143264: step 3215, loss 0.0686479, acc 0.98
2016-09-07T23:01:44.848699: step 3216, loss 0.00968827, acc 1
2016-09-07T23:01:45.543470: step 3217, loss 0.0234315, acc 1
2016-09-07T23:01:46.243819: step 3218, loss 0.0448416, acc 0.96
2016-09-07T23:01:46.923184: step 3219, loss 0.0569547, acc 0.96
2016-09-07T23:01:47.622616: step 3220, loss 0.0318042, acc 1
2016-09-07T23:01:48.328271: step 3221, loss 0.0531743, acc 0.96
2016-09-07T23:01:49.015969: step 3222, loss 0.019431, acc 1
2016-09-07T23:01:49.709353: step 3223, loss 0.0783254, acc 0.98
2016-09-07T23:01:50.399184: step 3224, loss 0.0721878, acc 0.96
2016-09-07T23:01:51.100969: step 3225, loss 0.0363057, acc 0.98
2016-09-07T23:01:51.797470: step 3226, loss 0.020872, acc 0.98
2016-09-07T23:01:52.506701: step 3227, loss 0.00345681, acc 1
2016-09-07T23:01:53.195678: step 3228, loss 0.0150412, acc 1
2016-09-07T23:01:53.880467: step 3229, loss 0.144932, acc 0.94
2016-09-07T23:01:54.585980: step 3230, loss 0.105248, acc 0.96
2016-09-07T23:01:55.308638: step 3231, loss 0.0799549, acc 0.98
2016-09-07T23:01:56.023983: step 3232, loss 0.00854276, acc 1
2016-09-07T23:01:56.721677: step 3233, loss 0.0296657, acc 1
2016-09-07T23:01:57.411105: step 3234, loss 0.0730189, acc 0.98
2016-09-07T23:01:58.096700: step 3235, loss 0.0589482, acc 0.98
2016-09-07T23:01:58.789142: step 3236, loss 0.0370587, acc 1
2016-09-07T23:01:59.510859: step 3237, loss 0.0160812, acc 1
2016-09-07T23:02:00.222745: step 3238, loss 0.0299569, acc 1
2016-09-07T23:02:00.916915: step 3239, loss 0.029256, acc 0.98
2016-09-07T23:02:01.617335: step 3240, loss 0.0996392, acc 0.96
2016-09-07T23:02:02.310050: step 3241, loss 0.0816825, acc 0.96
2016-09-07T23:02:02.999970: step 3242, loss 0.014256, acc 1
2016-09-07T23:02:03.687933: step 3243, loss 0.0279017, acc 1
2016-09-07T23:02:04.415833: step 3244, loss 0.149451, acc 0.96
2016-09-07T23:02:05.110990: step 3245, loss 0.0176301, acc 1
2016-09-07T23:02:05.814418: step 3246, loss 0.0325924, acc 0.98
2016-09-07T23:02:06.512806: step 3247, loss 0.0239259, acc 1
2016-09-07T23:02:07.218948: step 3248, loss 0.0290652, acc 0.98
2016-09-07T23:02:07.924681: step 3249, loss 0.0532041, acc 0.96
2016-09-07T23:02:08.622803: step 3250, loss 0.0282149, acc 0.98
2016-09-07T23:02:09.315402: step 3251, loss 0.0210273, acc 1
2016-09-07T23:02:10.015249: step 3252, loss 0.0775528, acc 0.98
2016-09-07T23:02:10.702686: step 3253, loss 0.0841758, acc 0.98
2016-09-07T23:02:11.412636: step 3254, loss 0.00775135, acc 1
2016-09-07T23:02:12.148352: step 3255, loss 0.0487392, acc 0.98
2016-09-07T23:02:12.864543: step 3256, loss 0.0653028, acc 0.96
2016-09-07T23:02:13.553312: step 3257, loss 0.0828961, acc 0.94
2016-09-07T23:02:14.235740: step 3258, loss 0.119716, acc 0.94
2016-09-07T23:02:14.946914: step 3259, loss 0.0826959, acc 0.96
2016-09-07T23:02:15.643207: step 3260, loss 0.0129488, acc 1
2016-09-07T23:02:16.335066: step 3261, loss 0.0683973, acc 0.98
2016-09-07T23:02:17.036530: step 3262, loss 0.0165528, acc 0.98
2016-09-07T23:02:17.748557: step 3263, loss 0.0250705, acc 0.98
2016-09-07T23:02:18.433451: step 3264, loss 0.0255894, acc 1
2016-09-07T23:02:19.146964: step 3265, loss 0.049623, acc 1
2016-09-07T23:02:19.843742: step 3266, loss 0.0292908, acc 0.98
2016-09-07T23:02:20.573464: step 3267, loss 0.0490593, acc 0.98
2016-09-07T23:02:21.275282: step 3268, loss 0.0145587, acc 1
2016-09-07T23:02:22.000183: step 3269, loss 0.00331344, acc 1
2016-09-07T23:02:22.698983: step 3270, loss 0.0341213, acc 0.96
2016-09-07T23:02:23.416094: step 3271, loss 0.00502513, acc 1
2016-09-07T23:02:24.117653: step 3272, loss 0.0218427, acc 0.98
2016-09-07T23:02:24.833086: step 3273, loss 0.0241611, acc 0.98
2016-09-07T23:02:25.546118: step 3274, loss 0.0539688, acc 0.98
2016-09-07T23:02:26.238355: step 3275, loss 0.00692132, acc 1
2016-09-07T23:02:26.957647: step 3276, loss 0.0410991, acc 0.96
2016-09-07T23:02:27.658072: step 3277, loss 0.00376051, acc 1
2016-09-07T23:02:28.359733: step 3278, loss 0.00522148, acc 1
2016-09-07T23:02:29.061220: step 3279, loss 0.0485259, acc 0.96
2016-09-07T23:02:29.754574: step 3280, loss 0.0923003, acc 0.96
2016-09-07T23:02:30.463166: step 3281, loss 0.000714303, acc 1
2016-09-07T23:02:31.165845: step 3282, loss 0.0164609, acc 1
2016-09-07T23:02:31.857720: step 3283, loss 0.120932, acc 0.96
2016-09-07T23:02:32.548188: step 3284, loss 0.0578476, acc 0.96
2016-09-07T23:02:33.244210: step 3285, loss 0.0424326, acc 1
2016-09-07T23:02:33.961357: step 3286, loss 0.0801891, acc 0.96
2016-09-07T23:02:34.673119: step 3287, loss 0.150419, acc 0.96
2016-09-07T23:02:35.388194: step 3288, loss 0.0373257, acc 0.98
2016-09-07T23:02:36.092988: step 3289, loss 0.00557779, acc 1
2016-09-07T23:02:36.798714: step 3290, loss 0.155622, acc 0.98
2016-09-07T23:02:37.501000: step 3291, loss 0.0593374, acc 0.98
2016-09-07T23:02:38.202022: step 3292, loss 0.0501449, acc 0.96
2016-09-07T23:02:38.904572: step 3293, loss 0.0575405, acc 0.98
2016-09-07T23:02:39.630239: step 3294, loss 0.0487326, acc 0.98
2016-09-07T23:02:40.340148: step 3295, loss 0.0472073, acc 0.98
2016-09-07T23:02:41.034117: step 3296, loss 0.0461072, acc 0.98
2016-09-07T23:02:41.723641: step 3297, loss 0.00936791, acc 1
2016-09-07T23:02:42.090948: step 3298, loss 0.0518102, acc 1
2016-09-07T23:02:42.801195: step 3299, loss 0.0615747, acc 0.96
2016-09-07T23:02:43.512621: step 3300, loss 0.0195115, acc 0.98

Evaluation:
2016-09-07T23:02:46.810329: step 3300, loss 1.41844, acc 0.76

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-3300

2016-09-07T23:02:48.565312: step 3301, loss 0.0278042, acc 0.98
2016-09-07T23:02:49.264055: step 3302, loss 0.0382576, acc 0.98
2016-09-07T23:02:49.953978: step 3303, loss 0.0194592, acc 1
2016-09-07T23:02:50.653513: step 3304, loss 0.0496455, acc 0.96
2016-09-07T23:02:51.350500: step 3305, loss 0.0653967, acc 0.96
2016-09-07T23:02:52.065617: step 3306, loss 0.0449804, acc 0.98
2016-09-07T23:02:52.775734: step 3307, loss 0.0271206, acc 0.98
2016-09-07T23:02:53.481572: step 3308, loss 0.0273938, acc 1
2016-09-07T23:02:54.193295: step 3309, loss 0.00109978, acc 1
2016-09-07T23:02:54.895287: step 3310, loss 0.0358501, acc 0.96
2016-09-07T23:02:55.601666: step 3311, loss 0.142065, acc 0.96
2016-09-07T23:02:56.287567: step 3312, loss 0.0575341, acc 0.96
2016-09-07T23:02:56.978759: step 3313, loss 0.0152534, acc 1
2016-09-07T23:02:57.684831: step 3314, loss 0.00886428, acc 1
2016-09-07T23:02:58.388239: step 3315, loss 0.0283955, acc 0.98
2016-09-07T23:02:59.097857: step 3316, loss 0.114616, acc 0.96
2016-09-07T23:02:59.819809: step 3317, loss 0.019512, acc 1
2016-09-07T23:03:00.561193: step 3318, loss 0.117277, acc 0.94
2016-09-07T23:03:01.252228: step 3319, loss 0.0392086, acc 0.98
2016-09-07T23:03:01.940716: step 3320, loss 0.0901521, acc 0.9
2016-09-07T23:03:02.620387: step 3321, loss 0.0792299, acc 0.98
2016-09-07T23:03:03.339088: step 3322, loss 0.0279214, acc 0.98
2016-09-07T23:03:04.053063: step 3323, loss 0.036578, acc 0.98
2016-09-07T23:03:04.755546: step 3324, loss 0.06026, acc 0.96
2016-09-07T23:03:05.449223: step 3325, loss 0.00364811, acc 1
2016-09-07T23:03:06.150330: step 3326, loss 0.00346408, acc 1
2016-09-07T23:03:06.865963: step 3327, loss 0.022785, acc 0.98
2016-09-07T23:03:07.554584: step 3328, loss 0.0606487, acc 0.96
2016-09-07T23:03:08.251314: step 3329, loss 0.0229721, acc 0.98
2016-09-07T23:03:08.955646: step 3330, loss 0.0869944, acc 0.98
2016-09-07T23:03:09.663695: step 3331, loss 0.0137587, acc 1
2016-09-07T23:03:10.358463: step 3332, loss 0.0288927, acc 0.98
2016-09-07T23:03:11.064360: step 3333, loss 0.064315, acc 0.98
2016-09-07T23:03:11.750065: step 3334, loss 0.0700341, acc 0.96
2016-09-07T23:03:12.435695: step 3335, loss 0.0393289, acc 0.96
2016-09-07T23:03:13.118450: step 3336, loss 0.104556, acc 0.98
2016-09-07T23:03:13.825475: step 3337, loss 0.00533016, acc 1
2016-09-07T23:03:14.531058: step 3338, loss 0.00136248, acc 1
2016-09-07T23:03:15.217446: step 3339, loss 0.0350823, acc 0.98
2016-09-07T23:03:15.912994: step 3340, loss 0.0357975, acc 1
2016-09-07T23:03:16.619523: step 3341, loss 0.0186499, acc 0.98
2016-09-07T23:03:17.311598: step 3342, loss 0.00930942, acc 1
2016-09-07T23:03:18.004290: step 3343, loss 0.055035, acc 0.96
2016-09-07T23:03:18.717161: step 3344, loss 0.047835, acc 0.98
2016-09-07T23:03:19.402108: step 3345, loss 0.000789901, acc 1
2016-09-07T23:03:20.110715: step 3346, loss 0.0447417, acc 0.98
2016-09-07T23:03:20.817794: step 3347, loss 0.019095, acc 1
2016-09-07T23:03:21.518930: step 3348, loss 0.0181006, acc 1
2016-09-07T23:03:22.213114: step 3349, loss 0.116111, acc 0.96
2016-09-07T23:03:22.898887: step 3350, loss 0.0270881, acc 0.98
2016-09-07T23:03:23.598766: step 3351, loss 0.0305332, acc 0.98
2016-09-07T23:03:24.291504: step 3352, loss 0.00701913, acc 1
2016-09-07T23:03:24.985646: step 3353, loss 0.0331818, acc 0.98
2016-09-07T23:03:25.683701: step 3354, loss 0.161819, acc 0.96
2016-09-07T23:03:26.391124: step 3355, loss 0.0558937, acc 0.96
2016-09-07T23:03:27.108280: step 3356, loss 0.016971, acc 0.98
2016-09-07T23:03:27.803941: step 3357, loss 0.0312368, acc 1
2016-09-07T23:03:28.514917: step 3358, loss 0.039592, acc 0.98
2016-09-07T23:03:29.213108: step 3359, loss 0.0342344, acc 0.98
2016-09-07T23:03:29.909142: step 3360, loss 0.0128677, acc 1
2016-09-07T23:03:30.617327: step 3361, loss 0.0332932, acc 0.96
2016-09-07T23:03:31.319122: step 3362, loss 0.0752505, acc 0.96
2016-09-07T23:03:32.014490: step 3363, loss 0.0864129, acc 0.96
2016-09-07T23:03:32.709021: step 3364, loss 0.127967, acc 0.94
2016-09-07T23:03:33.394454: step 3365, loss 0.00748171, acc 1
2016-09-07T23:03:34.098498: step 3366, loss 0.00415077, acc 1
2016-09-07T23:03:34.823162: step 3367, loss 0.0112051, acc 1
2016-09-07T23:03:35.522690: step 3368, loss 0.0130448, acc 1
2016-09-07T23:03:36.226905: step 3369, loss 0.00905904, acc 1
2016-09-07T23:03:36.930946: step 3370, loss 0.0256829, acc 0.98
2016-09-07T23:03:37.666262: step 3371, loss 0.0036986, acc 1
2016-09-07T23:03:38.357004: step 3372, loss 0.0233904, acc 1
2016-09-07T23:03:39.050323: step 3373, loss 0.0359844, acc 0.98
2016-09-07T23:03:39.746734: step 3374, loss 0.0544487, acc 0.98
2016-09-07T23:03:40.438447: step 3375, loss 0.00171165, acc 1
2016-09-07T23:03:41.150337: step 3376, loss 0.0273949, acc 1
2016-09-07T23:03:41.830703: step 3377, loss 0.0292763, acc 0.98
2016-09-07T23:03:42.526156: step 3378, loss 0.0317603, acc 0.98
2016-09-07T23:03:43.239063: step 3379, loss 0.032888, acc 0.98
2016-09-07T23:03:43.937637: step 3380, loss 0.0487083, acc 0.98
2016-09-07T23:03:44.650693: step 3381, loss 0.0163312, acc 1
2016-09-07T23:03:45.356449: step 3382, loss 0.00119516, acc 1
2016-09-07T23:03:46.049712: step 3383, loss 0.0117484, acc 1
2016-09-07T23:03:46.722812: step 3384, loss 0.00458211, acc 1
2016-09-07T23:03:47.429771: step 3385, loss 0.0507139, acc 0.98
2016-09-07T23:03:48.138146: step 3386, loss 0.12574, acc 0.92
2016-09-07T23:03:48.842280: step 3387, loss 0.0385035, acc 0.98
2016-09-07T23:03:49.538276: step 3388, loss 0.0145313, acc 1
2016-09-07T23:03:50.230046: step 3389, loss 0.0236713, acc 1
2016-09-07T23:03:50.929541: step 3390, loss 0.0950516, acc 0.98
2016-09-07T23:03:51.627440: step 3391, loss 0.0460669, acc 0.98
2016-09-07T23:03:52.333868: step 3392, loss 0.0058601, acc 1
2016-09-07T23:03:53.043947: step 3393, loss 0.0237229, acc 0.98
2016-09-07T23:03:53.738213: step 3394, loss 0.132472, acc 0.98
2016-09-07T23:03:54.413813: step 3395, loss 0.0113721, acc 1
2016-09-07T23:03:55.130573: step 3396, loss 0.101295, acc 0.96
2016-09-07T23:03:55.825068: step 3397, loss 0.0291981, acc 0.98
2016-09-07T23:03:56.514473: step 3398, loss 0.0180638, acc 1
2016-09-07T23:03:57.223118: step 3399, loss 0.0166885, acc 1
2016-09-07T23:03:57.935921: step 3400, loss 0.0163126, acc 1

Evaluation:
2016-09-07T23:04:01.310235: step 3400, loss 2.0912, acc 0.746

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-3400

2016-09-07T23:04:02.959334: step 3401, loss 0.0288542, acc 0.98
2016-09-07T23:04:03.655628: step 3402, loss 0.0810606, acc 0.94
2016-09-07T23:04:04.357898: step 3403, loss 0.00466768, acc 1
2016-09-07T23:04:05.061567: step 3404, loss 0.0258122, acc 1
2016-09-07T23:04:05.775832: step 3405, loss 0.22089, acc 0.96
2016-09-07T23:04:06.476883: step 3406, loss 0.0239577, acc 1
2016-09-07T23:04:07.172228: step 3407, loss 0.00944339, acc 1
2016-09-07T23:04:07.865943: step 3408, loss 0.00841721, acc 1
2016-09-07T23:04:08.567616: step 3409, loss 0.0261559, acc 0.98
2016-09-07T23:04:09.263617: step 3410, loss 0.0231669, acc 0.98
2016-09-07T23:04:09.986222: step 3411, loss 0.0468529, acc 0.96
2016-09-07T23:04:10.682068: step 3412, loss 0.256197, acc 0.94
2016-09-07T23:04:11.400851: step 3413, loss 0.049583, acc 0.98
2016-09-07T23:04:12.122943: step 3414, loss 0.145643, acc 0.96
2016-09-07T23:04:12.814655: step 3415, loss 0.00915167, acc 1
2016-09-07T23:04:13.516455: step 3416, loss 0.00292027, acc 1
2016-09-07T23:04:14.210974: step 3417, loss 0.0221929, acc 1
2016-09-07T23:04:14.908560: step 3418, loss 0.00938459, acc 1
2016-09-07T23:04:15.602322: step 3419, loss 0.0152123, acc 1
2016-09-07T23:04:16.299844: step 3420, loss 0.0427862, acc 0.98
2016-09-07T23:04:17.018498: step 3421, loss 0.0219982, acc 1
2016-09-07T23:04:17.721218: step 3422, loss 0.0284387, acc 1
2016-09-07T23:04:18.432302: step 3423, loss 0.157597, acc 0.96
2016-09-07T23:04:19.145726: step 3424, loss 0.0257721, acc 1
2016-09-07T23:04:19.852851: step 3425, loss 0.135863, acc 0.96
2016-09-07T23:04:20.578650: step 3426, loss 0.0143565, acc 1
2016-09-07T23:04:21.292131: step 3427, loss 0.0123784, acc 1
2016-09-07T23:04:21.992842: step 3428, loss 0.0559526, acc 0.98
2016-09-07T23:04:22.713847: step 3429, loss 0.0513396, acc 0.98
2016-09-07T23:04:23.406532: step 3430, loss 0.0693743, acc 0.96
2016-09-07T23:04:24.100029: step 3431, loss 0.0451608, acc 0.98
2016-09-07T23:04:24.809503: step 3432, loss 0.0642457, acc 0.96
2016-09-07T23:04:25.497373: step 3433, loss 0.0333382, acc 1
2016-09-07T23:04:26.232139: step 3434, loss 0.0493143, acc 0.96
2016-09-07T23:04:26.930072: step 3435, loss 0.0207661, acc 1
2016-09-07T23:04:27.623367: step 3436, loss 0.00786539, acc 1
2016-09-07T23:04:28.335089: step 3437, loss 0.0420361, acc 0.96
2016-09-07T23:04:29.041367: step 3438, loss 0.0424128, acc 0.98
2016-09-07T23:04:29.739621: step 3439, loss 0.138954, acc 0.94
2016-09-07T23:04:30.454812: step 3440, loss 0.0658916, acc 0.96
2016-09-07T23:04:31.166818: step 3441, loss 0.0181352, acc 1
2016-09-07T23:04:31.876486: step 3442, loss 0.0168695, acc 0.98
2016-09-07T23:04:32.580440: step 3443, loss 0.0287803, acc 0.98
2016-09-07T23:04:33.266997: step 3444, loss 0.0158608, acc 1
2016-09-07T23:04:33.959300: step 3445, loss 0.0991436, acc 0.98
2016-09-07T23:04:34.661640: step 3446, loss 0.0249132, acc 1
2016-09-07T23:04:35.390777: step 3447, loss 0.0225322, acc 1
2016-09-07T23:04:36.111164: step 3448, loss 0.00230164, acc 1
2016-09-07T23:04:36.825941: step 3449, loss 0.0408006, acc 0.96
2016-09-07T23:04:37.536779: step 3450, loss 0.0284269, acc 1
2016-09-07T23:04:38.235984: step 3451, loss 0.0237829, acc 1
2016-09-07T23:04:38.925011: step 3452, loss 0.0517806, acc 0.98
2016-09-07T23:04:39.615473: step 3453, loss 0.0242332, acc 1
2016-09-07T23:04:40.322594: step 3454, loss 0.00249784, acc 1
2016-09-07T23:04:41.027669: step 3455, loss 0.00501319, acc 1
2016-09-07T23:04:41.726132: step 3456, loss 0.0486377, acc 0.98
2016-09-07T23:04:42.422926: step 3457, loss 0.00795938, acc 1
2016-09-07T23:04:43.112715: step 3458, loss 0.0278353, acc 0.98
2016-09-07T23:04:43.817294: step 3459, loss 0.0948636, acc 0.94
2016-09-07T23:04:44.490673: step 3460, loss 0.055349, acc 0.98
2016-09-07T23:04:45.191513: step 3461, loss 0.0663838, acc 0.96
2016-09-07T23:04:45.891941: step 3462, loss 0.00890323, acc 1
2016-09-07T23:04:46.591109: step 3463, loss 0.0940245, acc 0.98
2016-09-07T23:04:47.297404: step 3464, loss 0.0233287, acc 1
2016-09-07T23:04:48.002845: step 3465, loss 0.00629296, acc 1
2016-09-07T23:04:48.681463: step 3466, loss 0.0205885, acc 0.98
2016-09-07T23:04:49.359667: step 3467, loss 0.0958145, acc 0.96
2016-09-07T23:04:50.086753: step 3468, loss 0.0447286, acc 0.98
2016-09-07T23:04:50.786518: step 3469, loss 0.0357839, acc 0.98
2016-09-07T23:04:51.477834: step 3470, loss 0.00408108, acc 1
2016-09-07T23:04:52.152201: step 3471, loss 0.0587412, acc 0.96
2016-09-07T23:04:52.858085: step 3472, loss 0.0420026, acc 0.98
2016-09-07T23:04:53.550959: step 3473, loss 0.0327123, acc 1
2016-09-07T23:04:54.275135: step 3474, loss 0.0326436, acc 0.98
2016-09-07T23:04:54.998978: step 3475, loss 0.0190574, acc 1
2016-09-07T23:04:55.707315: step 3476, loss 0.00611897, acc 1
2016-09-07T23:04:56.414846: step 3477, loss 0.0149512, acc 1
2016-09-07T23:04:57.114874: step 3478, loss 0.023463, acc 0.98
2016-09-07T23:04:57.848160: step 3479, loss 0.036345, acc 0.98
2016-09-07T23:04:58.586543: step 3480, loss 0.00883488, acc 1
2016-09-07T23:04:59.277856: step 3481, loss 0.0127026, acc 1
2016-09-07T23:04:59.987244: step 3482, loss 0.0193389, acc 0.98
2016-09-07T23:05:00.714681: step 3483, loss 0.056204, acc 0.96
2016-09-07T23:05:01.417426: step 3484, loss 0.00587121, acc 1
2016-09-07T23:05:02.124492: step 3485, loss 0.0792061, acc 0.96
2016-09-07T23:05:02.813499: step 3486, loss 0.00245779, acc 1
2016-09-07T23:05:03.500406: step 3487, loss 0.00870978, acc 1
2016-09-07T23:05:04.202531: step 3488, loss 0.0312923, acc 0.98
2016-09-07T23:05:04.902756: step 3489, loss 0.0150503, acc 1
2016-09-07T23:05:05.612361: step 3490, loss 0.0423141, acc 0.98
2016-09-07T23:05:06.317964: step 3491, loss 0.0319345, acc 0.98
2016-09-07T23:05:06.688229: step 3492, loss 0.0414465, acc 1
2016-09-07T23:05:07.390677: step 3493, loss 0.0295567, acc 0.98
2016-09-07T23:05:08.084707: step 3494, loss 0.0403661, acc 0.98
2016-09-07T23:05:08.774493: step 3495, loss 0.027828, acc 0.98
2016-09-07T23:05:09.485589: step 3496, loss 0.0189675, acc 1
2016-09-07T23:05:10.193836: step 3497, loss 0.000282868, acc 1
2016-09-07T23:05:10.910640: step 3498, loss 0.0256466, acc 1
2016-09-07T23:05:11.639525: step 3499, loss 0.0364491, acc 0.96
2016-09-07T23:05:12.333744: step 3500, loss 0.0209847, acc 1

Evaluation:
2016-09-07T23:05:15.704997: step 3500, loss 2.1603, acc 0.73

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-3500

2016-09-07T23:05:17.470421: step 3501, loss 0.121737, acc 0.98
2016-09-07T23:05:18.182232: step 3502, loss 0.00109081, acc 1
2016-09-07T23:05:18.870418: step 3503, loss 0.0025891, acc 1
2016-09-07T23:05:19.564626: step 3504, loss 0.0327271, acc 0.98
2016-09-07T23:05:20.257206: step 3505, loss 0.0303548, acc 1
2016-09-07T23:05:20.946092: step 3506, loss 0.0619753, acc 0.96
2016-09-07T23:05:21.647160: step 3507, loss 0.0132404, acc 1
2016-09-07T23:05:22.355217: step 3508, loss 0.0140757, acc 1
2016-09-07T23:05:23.066630: step 3509, loss 0.0281761, acc 0.98
2016-09-07T23:05:23.778629: step 3510, loss 0.00117423, acc 1
2016-09-07T23:05:24.502196: step 3511, loss 0.0173343, acc 0.98
2016-09-07T23:05:25.221840: step 3512, loss 0.0216119, acc 0.98
2016-09-07T23:05:25.942605: step 3513, loss 0.0225966, acc 1
2016-09-07T23:05:26.640088: step 3514, loss 0.0726618, acc 0.96
2016-09-07T23:05:27.332697: step 3515, loss 0.000632205, acc 1
2016-09-07T23:05:28.046975: step 3516, loss 0.0109376, acc 1
2016-09-07T23:05:28.748307: step 3517, loss 0.0467751, acc 0.96
2016-09-07T23:05:29.435719: step 3518, loss 0.0208983, acc 0.98
2016-09-07T23:05:30.133723: step 3519, loss 0.000282441, acc 1
2016-09-07T23:05:30.834672: step 3520, loss 0.0384588, acc 0.96
2016-09-07T23:05:31.533018: step 3521, loss 0.044871, acc 0.96
2016-09-07T23:05:32.226219: step 3522, loss 0.0213571, acc 1
2016-09-07T23:05:32.913585: step 3523, loss 0.0711479, acc 0.98
2016-09-07T23:05:33.618106: step 3524, loss 0.0771201, acc 0.94
2016-09-07T23:05:34.324993: step 3525, loss 0.0124081, acc 1
2016-09-07T23:05:35.036243: step 3526, loss 0.0676315, acc 0.96
2016-09-07T23:05:35.752319: step 3527, loss 0.0222173, acc 0.98
2016-09-07T23:05:36.442034: step 3528, loss 0.0744031, acc 0.98
2016-09-07T23:05:37.154960: step 3529, loss 4.66162e-05, acc 1
2016-09-07T23:05:37.855339: step 3530, loss 0.0533349, acc 0.98
2016-09-07T23:05:38.560900: step 3531, loss 0.00054304, acc 1
2016-09-07T23:05:39.258322: step 3532, loss 9.72738e-05, acc 1
2016-09-07T23:05:39.956166: step 3533, loss 0.0218068, acc 0.98
2016-09-07T23:05:40.661811: step 3534, loss 0.0256434, acc 0.98
2016-09-07T23:05:41.370376: step 3535, loss 0.0429472, acc 0.98
2016-09-07T23:05:42.060398: step 3536, loss 0.132825, acc 0.96
2016-09-07T23:05:42.768578: step 3537, loss 0.0317003, acc 0.98
2016-09-07T23:05:43.462610: step 3538, loss 0.0478285, acc 0.98
2016-09-07T23:05:44.179678: step 3539, loss 0.0926885, acc 0.94
2016-09-07T23:05:44.879851: step 3540, loss 0.013959, acc 1
2016-09-07T23:05:45.587813: step 3541, loss 0.0138905, acc 1
2016-09-07T23:05:46.283835: step 3542, loss 0.0407377, acc 0.96
2016-09-07T23:05:46.976536: step 3543, loss 0.0242046, acc 1
2016-09-07T23:05:47.670020: step 3544, loss 0.155282, acc 0.96
2016-09-07T23:05:48.371127: step 3545, loss 0.0782625, acc 0.98
2016-09-07T23:05:49.069792: step 3546, loss 0.0436784, acc 0.96
2016-09-07T23:05:49.772258: step 3547, loss 0.0925714, acc 0.96
2016-09-07T23:05:50.459675: step 3548, loss 0.016267, acc 1
2016-09-07T23:05:51.160992: step 3549, loss 0.0356154, acc 0.98
2016-09-07T23:05:51.889881: step 3550, loss 0.00727837, acc 1
2016-09-07T23:05:52.592705: step 3551, loss 0.00825525, acc 1
2016-09-07T23:05:53.282563: step 3552, loss 0.000164191, acc 1
2016-09-07T23:05:53.967918: step 3553, loss 0.0760897, acc 0.96
2016-09-07T23:05:54.661606: step 3554, loss 0.00610289, acc 1
2016-09-07T23:05:55.367144: step 3555, loss 0.0472373, acc 0.98
2016-09-07T23:05:56.090263: step 3556, loss 0.0213826, acc 1
2016-09-07T23:05:56.816784: step 3557, loss 0.0301118, acc 0.98
2016-09-07T23:05:57.531528: step 3558, loss 0.0278953, acc 1
2016-09-07T23:05:58.241302: step 3559, loss 0.00576455, acc 1
2016-09-07T23:05:58.946921: step 3560, loss 0.00808123, acc 1
2016-09-07T23:05:59.682885: step 3561, loss 0.00770357, acc 1
2016-09-07T23:06:00.413963: step 3562, loss 0.0168951, acc 1
2016-09-07T23:06:01.101619: step 3563, loss 0.015211, acc 1
2016-09-07T23:06:01.810391: step 3564, loss 0.0447556, acc 0.98
2016-09-07T23:06:02.527334: step 3565, loss 0.00179768, acc 1
2016-09-07T23:06:03.228546: step 3566, loss 0.0182984, acc 1
2016-09-07T23:06:03.935758: step 3567, loss 0.000196872, acc 1
2016-09-07T23:06:04.643232: step 3568, loss 0.054331, acc 0.98
2016-09-07T23:06:05.343481: step 3569, loss 0.00258855, acc 1
2016-09-07T23:06:06.048449: step 3570, loss 0.0667476, acc 0.98
2016-09-07T23:06:06.753102: step 3571, loss 0.0126134, acc 1
2016-09-07T23:06:07.443803: step 3572, loss 0.0338372, acc 0.98
2016-09-07T23:06:08.143669: step 3573, loss 0.0179302, acc 1
2016-09-07T23:06:08.846669: step 3574, loss 0.164411, acc 0.98
2016-09-07T23:06:09.560376: step 3575, loss 0.05047, acc 0.98
2016-09-07T23:06:10.269673: step 3576, loss 0.0133378, acc 1
2016-09-07T23:06:10.984322: step 3577, loss 0.00571862, acc 1
2016-09-07T23:06:11.707128: step 3578, loss 0.14618, acc 0.94
2016-09-07T23:06:12.397610: step 3579, loss 0.0167425, acc 0.98
2016-09-07T23:06:13.095698: step 3580, loss 0.0358276, acc 0.98
2016-09-07T23:06:13.811147: step 3581, loss 0.012479, acc 1
2016-09-07T23:06:14.523114: step 3582, loss 0.0225224, acc 0.98
2016-09-07T23:06:15.234874: step 3583, loss 0.0352136, acc 0.98
2016-09-07T23:06:15.928075: step 3584, loss 0.00214125, acc 1
2016-09-07T23:06:16.609366: step 3585, loss 0.0155182, acc 1
2016-09-07T23:06:17.326435: step 3586, loss 0.0488554, acc 0.98
2016-09-07T23:06:18.025819: step 3587, loss 0.0329355, acc 0.98
2016-09-07T23:06:18.756073: step 3588, loss 0.0327641, acc 0.98
2016-09-07T23:06:19.439217: step 3589, loss 0.0379636, acc 0.98
2016-09-07T23:06:20.137996: step 3590, loss 0.0360279, acc 1
2016-09-07T23:06:20.844730: step 3591, loss 0.028183, acc 0.98
2016-09-07T23:06:21.545985: step 3592, loss 0.0415283, acc 0.98
2016-09-07T23:06:22.256514: step 3593, loss 0.0143788, acc 1
2016-09-07T23:06:22.954941: step 3594, loss 0.0360085, acc 0.98
2016-09-07T23:06:23.664164: step 3595, loss 0.0265459, acc 0.98
2016-09-07T23:06:24.379415: step 3596, loss 0.0402913, acc 0.98
2016-09-07T23:06:25.082793: step 3597, loss 0.0351377, acc 0.98
2016-09-07T23:06:25.797302: step 3598, loss 0.0786785, acc 0.98
2016-09-07T23:06:26.479392: step 3599, loss 0.0301092, acc 0.98
2016-09-07T23:06:27.177043: step 3600, loss 0.0181491, acc 1

Evaluation:
2016-09-07T23:06:30.489519: step 3600, loss 1.76242, acc 0.736

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-3600

2016-09-07T23:06:32.232614: step 3601, loss 0.0336805, acc 0.98
2016-09-07T23:06:32.938281: step 3602, loss 0.0438314, acc 0.98
2016-09-07T23:06:33.617425: step 3603, loss 0.109232, acc 0.96
2016-09-07T23:06:34.326065: step 3604, loss 0.000321346, acc 1
2016-09-07T23:06:35.034725: step 3605, loss 0.0134383, acc 1
2016-09-07T23:06:35.738462: step 3606, loss 0.0082733, acc 1
2016-09-07T23:06:36.438597: step 3607, loss 0.0428567, acc 0.98
2016-09-07T23:06:37.139820: step 3608, loss 0.0325664, acc 0.98
2016-09-07T23:06:37.839807: step 3609, loss 0.00406767, acc 1
2016-09-07T23:06:38.559017: step 3610, loss 0.0938355, acc 0.94
2016-09-07T23:06:39.248724: step 3611, loss 0.021811, acc 1
2016-09-07T23:06:39.931840: step 3612, loss 0.0920919, acc 0.94
2016-09-07T23:06:40.659393: step 3613, loss 0.00406057, acc 1
2016-09-07T23:06:41.361838: step 3614, loss 0.023897, acc 0.98
2016-09-07T23:06:42.069388: step 3615, loss 0.0103186, acc 1
2016-09-07T23:06:42.777211: step 3616, loss 0.00674969, acc 1
2016-09-07T23:06:43.485935: step 3617, loss 0.0340427, acc 1
2016-09-07T23:06:44.179185: step 3618, loss 0.0254709, acc 0.98
2016-09-07T23:06:44.888277: step 3619, loss 0.0350439, acc 0.98
2016-09-07T23:06:45.616681: step 3620, loss 0.0966022, acc 0.98
2016-09-07T23:06:46.319920: step 3621, loss 0.030678, acc 0.98
2016-09-07T23:06:47.016710: step 3622, loss 0.0625738, acc 0.96
2016-09-07T23:06:47.717346: step 3623, loss 0.0340141, acc 0.98
2016-09-07T23:06:48.441991: step 3624, loss 0.0217078, acc 0.98
2016-09-07T23:06:49.135547: step 3625, loss 0.060149, acc 0.96
2016-09-07T23:06:49.839457: step 3626, loss 0.00771641, acc 1
2016-09-07T23:06:50.522434: step 3627, loss 0.00869842, acc 1
2016-09-07T23:06:51.233307: step 3628, loss 0.00130932, acc 1
2016-09-07T23:06:51.938057: step 3629, loss 0.0151553, acc 1
2016-09-07T23:06:52.632689: step 3630, loss 0.0150209, acc 1
2016-09-07T23:06:53.321034: step 3631, loss 0.021595, acc 1
2016-09-07T23:06:54.017916: step 3632, loss 0.160618, acc 0.96
2016-09-07T23:06:54.725979: step 3633, loss 0.0443664, acc 0.98
2016-09-07T23:06:55.407438: step 3634, loss 0.00417336, acc 1
2016-09-07T23:06:56.114445: step 3635, loss 0.0135276, acc 1
2016-09-07T23:06:56.804030: step 3636, loss 0.00425671, acc 1
2016-09-07T23:06:57.494268: step 3637, loss 0.0089423, acc 1
2016-09-07T23:06:58.194808: step 3638, loss 0.0893889, acc 0.98
2016-09-07T23:06:58.902265: step 3639, loss 0.0329359, acc 0.98
2016-09-07T23:06:59.594439: step 3640, loss 0.00692642, acc 1
2016-09-07T23:07:00.304830: step 3641, loss 0.0523513, acc 0.96
2016-09-07T23:07:00.994676: step 3642, loss 0.0597182, acc 0.96
2016-09-07T23:07:01.698386: step 3643, loss 0.0409733, acc 0.98
2016-09-07T23:07:02.401826: step 3644, loss 0.0729394, acc 0.98
2016-09-07T23:07:03.096664: step 3645, loss 0.0213517, acc 0.98
2016-09-07T23:07:03.790494: step 3646, loss 0.0191047, acc 1
2016-09-07T23:07:04.493257: step 3647, loss 0.0574219, acc 0.98
2016-09-07T23:07:05.231937: step 3648, loss 0.00449093, acc 1
2016-09-07T23:07:05.931898: step 3649, loss 0.0505969, acc 0.96
2016-09-07T23:07:06.637260: step 3650, loss 0.0933563, acc 0.94
2016-09-07T23:07:07.337029: step 3651, loss 0.0181177, acc 1
2016-09-07T23:07:08.046164: step 3652, loss 0.0834302, acc 0.98
2016-09-07T23:07:08.751997: step 3653, loss 0.0178097, acc 1
2016-09-07T23:07:09.437127: step 3654, loss 0.0372567, acc 0.98
2016-09-07T23:07:10.150347: step 3655, loss 0.0185242, acc 1
2016-09-07T23:07:10.842974: step 3656, loss 0.0203369, acc 0.98
2016-09-07T23:07:11.537361: step 3657, loss 0.0196403, acc 0.98
2016-09-07T23:07:12.213748: step 3658, loss 0.121818, acc 0.92
2016-09-07T23:07:12.918756: step 3659, loss 0.00948895, acc 1
2016-09-07T23:07:13.615230: step 3660, loss 0.0313714, acc 1
2016-09-07T23:07:14.307110: step 3661, loss 0.0744899, acc 0.96
2016-09-07T23:07:15.005836: step 3662, loss 0.0843837, acc 0.96
2016-09-07T23:07:15.694686: step 3663, loss 0.0386666, acc 0.98
2016-09-07T23:07:16.387643: step 3664, loss 0.0245324, acc 1
2016-09-07T23:07:17.098503: step 3665, loss 0.0527794, acc 0.96
2016-09-07T23:07:17.811668: step 3666, loss 0.0552949, acc 0.98
2016-09-07T23:07:18.522201: step 3667, loss 0.0293626, acc 0.98
2016-09-07T23:07:19.259979: step 3668, loss 0.012152, acc 1
2016-09-07T23:07:19.982898: step 3669, loss 0.0131949, acc 1
2016-09-07T23:07:20.696767: step 3670, loss 0.0160923, acc 1
2016-09-07T23:07:21.387393: step 3671, loss 0.0313748, acc 0.98
2016-09-07T23:07:22.086497: step 3672, loss 0.0262559, acc 0.98
2016-09-07T23:07:22.805041: step 3673, loss 0.0559988, acc 0.96
2016-09-07T23:07:23.539204: step 3674, loss 0.096835, acc 0.96
2016-09-07T23:07:24.239198: step 3675, loss 0.00144986, acc 1
2016-09-07T23:07:24.948187: step 3676, loss 0.0237025, acc 0.98
2016-09-07T23:07:25.643939: step 3677, loss 0.00919126, acc 1
2016-09-07T23:07:26.334977: step 3678, loss 0.126231, acc 0.96
2016-09-07T23:07:27.044381: step 3679, loss 0.0486561, acc 0.96
2016-09-07T23:07:27.762383: step 3680, loss 0.0479217, acc 0.96
2016-09-07T23:07:28.498245: step 3681, loss 0.0247334, acc 0.98
2016-09-07T23:07:29.223893: step 3682, loss 0.0300375, acc 0.98
2016-09-07T23:07:29.948879: step 3683, loss 0.0749064, acc 0.94
2016-09-07T23:07:30.633792: step 3684, loss 0.0156781, acc 1
2016-09-07T23:07:31.331454: step 3685, loss 0.0366128, acc 0.98
2016-09-07T23:07:31.697880: step 3686, loss 0.0269519, acc 1
2016-09-07T23:07:32.403584: step 3687, loss 0.0374744, acc 1
2016-09-07T23:07:33.102733: step 3688, loss 0.0545179, acc 0.98
2016-09-07T23:07:33.814843: step 3689, loss 0.094066, acc 0.92
2016-09-07T23:07:34.519573: step 3690, loss 0.0178458, acc 1
2016-09-07T23:07:35.216017: step 3691, loss 0.000440245, acc 1
2016-09-07T23:07:35.923692: step 3692, loss 0.0509669, acc 0.98
2016-09-07T23:07:36.619595: step 3693, loss 0.0134709, acc 1
2016-09-07T23:07:37.327125: step 3694, loss 0.12214, acc 0.98
2016-09-07T23:07:38.053150: step 3695, loss 0.0366689, acc 0.98
2016-09-07T23:07:38.750413: step 3696, loss 0.0506244, acc 0.98
2016-09-07T23:07:39.432163: step 3697, loss 0.0359692, acc 0.98
2016-09-07T23:07:40.140488: step 3698, loss 0.0124396, acc 1
2016-09-07T23:07:40.864464: step 3699, loss 0.0380376, acc 0.98
2016-09-07T23:07:41.562281: step 3700, loss 0.0329098, acc 0.98

Evaluation:
2016-09-07T23:07:44.940436: step 3700, loss 1.74555, acc 0.752

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-3700

2016-09-07T23:07:46.622039: step 3701, loss 0.00525242, acc 1
2016-09-07T23:07:47.329752: step 3702, loss 0.00115052, acc 1
2016-09-07T23:07:48.038777: step 3703, loss 0.0204123, acc 1
2016-09-07T23:07:48.739639: step 3704, loss 0.0221311, acc 1
2016-09-07T23:07:49.444505: step 3705, loss 0.0486366, acc 0.96
2016-09-07T23:07:50.146275: step 3706, loss 0.0117092, acc 1
2016-09-07T23:07:50.869031: step 3707, loss 0.044768, acc 0.98
2016-09-07T23:07:51.576012: step 3708, loss 0.0133803, acc 1
2016-09-07T23:07:52.300502: step 3709, loss 0.0317033, acc 0.98
2016-09-07T23:07:53.019435: step 3710, loss 0.0110279, acc 1
2016-09-07T23:07:53.713467: step 3711, loss 0.00124957, acc 1
2016-09-07T23:07:54.410413: step 3712, loss 0.00351052, acc 1
2016-09-07T23:07:55.117153: step 3713, loss 0.0207173, acc 0.98
2016-09-07T23:07:55.808885: step 3714, loss 0.000288729, acc 1
2016-09-07T23:07:56.514062: step 3715, loss 0.00694698, acc 1
2016-09-07T23:07:57.222421: step 3716, loss 0.0138958, acc 1
2016-09-07T23:07:57.943072: step 3717, loss 0.0132836, acc 1
2016-09-07T23:07:58.656304: step 3718, loss 0.0253188, acc 1
2016-09-07T23:07:59.374123: step 3719, loss 0.0230694, acc 1
2016-09-07T23:08:00.079720: step 3720, loss 0.00541905, acc 1
2016-09-07T23:08:00.829502: step 3721, loss 0.0295954, acc 0.98
2016-09-07T23:08:01.538519: step 3722, loss 0.0712352, acc 0.98
2016-09-07T23:08:02.247463: step 3723, loss 0.0281808, acc 1
2016-09-07T23:08:02.963310: step 3724, loss 0.173641, acc 0.94
2016-09-07T23:08:03.668520: step 3725, loss 0.000199535, acc 1
2016-09-07T23:08:04.379524: step 3726, loss 0.0018208, acc 1
2016-09-07T23:08:05.072736: step 3727, loss 0.0783775, acc 0.96
2016-09-07T23:08:05.766935: step 3728, loss 0.0134973, acc 1
2016-09-07T23:08:06.466536: step 3729, loss 0.00233571, acc 1
2016-09-07T23:08:07.174852: step 3730, loss 0.0286135, acc 0.98
2016-09-07T23:08:07.849328: step 3731, loss 0.0234069, acc 0.98
2016-09-07T23:08:08.555261: step 3732, loss 0.0195615, acc 0.98
2016-09-07T23:08:09.246718: step 3733, loss 0.0377562, acc 0.98
2016-09-07T23:08:09.952150: step 3734, loss 0.079875, acc 0.94
2016-09-07T23:08:10.649832: step 3735, loss 0.0109427, acc 1
2016-09-07T23:08:11.361010: step 3736, loss 0.00406382, acc 1
2016-09-07T23:08:12.059951: step 3737, loss 0.0364428, acc 0.98
2016-09-07T23:08:12.770352: step 3738, loss 0.031286, acc 1
2016-09-07T23:08:13.467813: step 3739, loss 0.0201497, acc 0.98
2016-09-07T23:08:14.186486: step 3740, loss 0.01557, acc 0.98
2016-09-07T23:08:14.914521: step 3741, loss 0.0225035, acc 1
2016-09-07T23:08:15.643380: step 3742, loss 0.0506574, acc 0.98
2016-09-07T23:08:16.337737: step 3743, loss 0.0138065, acc 1
2016-09-07T23:08:17.032104: step 3744, loss 0.000388009, acc 1
2016-09-07T23:08:17.723426: step 3745, loss 0.0091914, acc 1
2016-09-07T23:08:18.416231: step 3746, loss 0.0022481, acc 1
2016-09-07T23:08:19.098418: step 3747, loss 0.0359246, acc 1
2016-09-07T23:08:19.801741: step 3748, loss 0.0280551, acc 0.98
2016-09-07T23:08:20.498676: step 3749, loss 0.000840463, acc 1
2016-09-07T23:08:21.188390: step 3750, loss 0.0608723, acc 0.98
2016-09-07T23:08:21.885021: step 3751, loss 0.0483442, acc 0.98
2016-09-07T23:08:22.597455: step 3752, loss 0.0209619, acc 1
2016-09-07T23:08:23.297586: step 3753, loss 0.0436934, acc 0.98
2016-09-07T23:08:23.991056: step 3754, loss 0.00136315, acc 1
2016-09-07T23:08:24.703133: step 3755, loss 0.00121165, acc 1
2016-09-07T23:08:25.397500: step 3756, loss 0.00564517, acc 1
2016-09-07T23:08:26.093796: step 3757, loss 0.0104681, acc 1
2016-09-07T23:08:26.800417: step 3758, loss 0.0738138, acc 0.96
2016-09-07T23:08:27.507978: step 3759, loss 0.000620077, acc 1
2016-09-07T23:08:28.207465: step 3760, loss 0.0623742, acc 0.96
2016-09-07T23:08:28.906760: step 3761, loss 0.0210356, acc 0.98
2016-09-07T23:08:29.602824: step 3762, loss 0.0592159, acc 0.96
2016-09-07T23:08:30.299318: step 3763, loss 0.0370662, acc 0.96
2016-09-07T23:08:31.001775: step 3764, loss 0.0127122, acc 1
2016-09-07T23:08:31.715117: step 3765, loss 0.0753415, acc 0.96
2016-09-07T23:08:32.425394: step 3766, loss 0.043627, acc 0.98
2016-09-07T23:08:33.129044: step 3767, loss 0.098608, acc 0.98
2016-09-07T23:08:33.838862: step 3768, loss 0.00154773, acc 1
2016-09-07T23:08:34.546583: step 3769, loss 0.0413798, acc 0.96
2016-09-07T23:08:35.231469: step 3770, loss 0.00256965, acc 1
2016-09-07T23:08:35.915895: step 3771, loss 0.13404, acc 0.94
2016-09-07T23:08:36.603821: step 3772, loss 0.00839661, acc 1
2016-09-07T23:08:37.297841: step 3773, loss 0.00378614, acc 1
2016-09-07T23:08:37.993443: step 3774, loss 0.0123318, acc 1
2016-09-07T23:08:38.691853: step 3775, loss 0.180633, acc 0.96
2016-09-07T23:08:39.382598: step 3776, loss 0.0334848, acc 0.98
2016-09-07T23:08:40.082910: step 3777, loss 0.0864801, acc 0.98
2016-09-07T23:08:40.774562: step 3778, loss 0.104294, acc 0.98
2016-09-07T23:08:41.498055: step 3779, loss 0.0420421, acc 0.98
2016-09-07T23:08:42.210677: step 3780, loss 0.0869167, acc 0.96
2016-09-07T23:08:42.904025: step 3781, loss 0.0396619, acc 0.98
2016-09-07T23:08:43.609782: step 3782, loss 0.0129008, acc 1
2016-09-07T23:08:44.340108: step 3783, loss 0.203724, acc 0.92
2016-09-07T23:08:45.051096: step 3784, loss 0.0170852, acc 1
2016-09-07T23:08:45.760849: step 3785, loss 0.0103322, acc 1
2016-09-07T23:08:46.467155: step 3786, loss 0.108762, acc 0.96
2016-09-07T23:08:47.183765: step 3787, loss 0.0117265, acc 1
2016-09-07T23:08:47.892102: step 3788, loss 0.0227629, acc 1
2016-09-07T23:08:48.588298: step 3789, loss 0.022231, acc 0.98
2016-09-07T23:08:49.282196: step 3790, loss 0.0328385, acc 1
2016-09-07T23:08:49.978589: step 3791, loss 0.0882306, acc 0.96
2016-09-07T23:08:50.662924: step 3792, loss 0.0350579, acc 0.98
2016-09-07T23:08:51.374541: step 3793, loss 0.0308182, acc 0.98
2016-09-07T23:08:52.089276: step 3794, loss 0.0433515, acc 1
2016-09-07T23:08:52.774962: step 3795, loss 0.029273, acc 1
2016-09-07T23:08:53.484926: step 3796, loss 0.004604, acc 1
2016-09-07T23:08:54.198009: step 3797, loss 0.0726526, acc 0.96
2016-09-07T23:08:54.899275: step 3798, loss 0.00524804, acc 1
2016-09-07T23:08:55.594393: step 3799, loss 0.032158, acc 1
2016-09-07T23:08:56.303882: step 3800, loss 0.0366098, acc 0.98

Evaluation:
2016-09-07T23:08:59.714595: step 3800, loss 1.70582, acc 0.728

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-3800

2016-09-07T23:09:01.653956: step 3801, loss 0.00989645, acc 1
2016-09-07T23:09:02.372455: step 3802, loss 0.0787374, acc 0.96
2016-09-07T23:09:03.092271: step 3803, loss 0.00552545, acc 1
2016-09-07T23:09:03.799048: step 3804, loss 0.0352733, acc 0.98
2016-09-07T23:09:04.500791: step 3805, loss 0.0161324, acc 1
2016-09-07T23:09:05.199430: step 3806, loss 0.151584, acc 0.96
2016-09-07T23:09:05.891316: step 3807, loss 0.0370501, acc 0.98
2016-09-07T23:09:06.593659: step 3808, loss 0.00561816, acc 1
2016-09-07T23:09:07.299461: step 3809, loss 0.00216659, acc 1
2016-09-07T23:09:08.016340: step 3810, loss 0.0237954, acc 0.98
2016-09-07T23:09:08.725380: step 3811, loss 0.0528864, acc 0.96
2016-09-07T23:09:09.433081: step 3812, loss 0.0694994, acc 0.98
2016-09-07T23:09:10.150292: step 3813, loss 0.00505508, acc 1
2016-09-07T23:09:10.890981: step 3814, loss 0.0681723, acc 0.96
2016-09-07T23:09:11.600472: step 3815, loss 0.0378772, acc 1
2016-09-07T23:09:12.308858: step 3816, loss 0.0296, acc 0.98
2016-09-07T23:09:13.011155: step 3817, loss 0.0299304, acc 1
2016-09-07T23:09:13.716430: step 3818, loss 0.061723, acc 0.96
2016-09-07T23:09:14.448461: step 3819, loss 0.0216501, acc 1
2016-09-07T23:09:15.171697: step 3820, loss 0.0233552, acc 0.98
2016-09-07T23:09:15.884158: step 3821, loss 0.106231, acc 0.98
2016-09-07T23:09:16.598696: step 3822, loss 0.021977, acc 0.98
2016-09-07T23:09:17.285995: step 3823, loss 0.0267138, acc 1
2016-09-07T23:09:18.008501: step 3824, loss 0.00270252, acc 1
2016-09-07T23:09:18.717323: step 3825, loss 0.0114263, acc 1
2016-09-07T23:09:19.410544: step 3826, loss 0.0443214, acc 0.98
2016-09-07T23:09:20.112154: step 3827, loss 0.0257604, acc 0.98
2016-09-07T23:09:20.827104: step 3828, loss 0.025675, acc 0.98
2016-09-07T23:09:21.534549: step 3829, loss 0.0189779, acc 0.98
2016-09-07T23:09:22.237343: step 3830, loss 0.0183766, acc 1
2016-09-07T23:09:22.928605: step 3831, loss 0.000492996, acc 1
2016-09-07T23:09:23.634607: step 3832, loss 0.173088, acc 0.94
2016-09-07T23:09:24.318572: step 3833, loss 0.00823428, acc 1
2016-09-07T23:09:25.037253: step 3834, loss 0.017127, acc 1
2016-09-07T23:09:25.735958: step 3835, loss 0.0365864, acc 0.98
2016-09-07T23:09:26.441747: step 3836, loss 0.0286997, acc 0.98
2016-09-07T23:09:27.132186: step 3837, loss 0.0347943, acc 0.98
2016-09-07T23:09:27.840711: step 3838, loss 0.0652412, acc 0.96
2016-09-07T23:09:28.598767: step 3839, loss 0.0616055, acc 0.96
2016-09-07T23:09:29.294758: step 3840, loss 0.0573677, acc 0.98
2016-09-07T23:09:30.020929: step 3841, loss 0.0427007, acc 0.98
2016-09-07T23:09:30.719623: step 3842, loss 0.00397801, acc 1
2016-09-07T23:09:31.433983: step 3843, loss 0.000433911, acc 1
2016-09-07T23:09:32.157995: step 3844, loss 0.00751589, acc 1
2016-09-07T23:09:32.868549: step 3845, loss 0.0390795, acc 0.98
2016-09-07T23:09:33.571616: step 3846, loss 0.0265399, acc 0.98
2016-09-07T23:09:34.276743: step 3847, loss 0.032122, acc 1
2016-09-07T23:09:34.984840: step 3848, loss 0.134311, acc 0.98
2016-09-07T23:09:35.677589: step 3849, loss 0.00324123, acc 1
2016-09-07T23:09:36.388183: step 3850, loss 0.00279905, acc 1
2016-09-07T23:09:37.097152: step 3851, loss 0.0151848, acc 1
2016-09-07T23:09:37.813953: step 3852, loss 0.0527223, acc 0.96
2016-09-07T23:09:38.533854: step 3853, loss 0.0182375, acc 0.98
2016-09-07T23:09:39.255871: step 3854, loss 0.0105843, acc 1
2016-09-07T23:09:39.972190: step 3855, loss 0.026645, acc 0.98
2016-09-07T23:09:40.682280: step 3856, loss 0.169092, acc 0.98
2016-09-07T23:09:41.388122: step 3857, loss 0.0304647, acc 1
2016-09-07T23:09:42.095603: step 3858, loss 0.0178834, acc 0.98
2016-09-07T23:09:42.807547: step 3859, loss 0.0446868, acc 0.96
2016-09-07T23:09:43.519884: step 3860, loss 0.00619565, acc 1
2016-09-07T23:09:44.200903: step 3861, loss 0.0679357, acc 0.96
2016-09-07T23:09:44.910276: step 3862, loss 0.033517, acc 0.98
2016-09-07T23:09:45.621541: step 3863, loss 0.0476687, acc 0.96
2016-09-07T23:09:46.327057: step 3864, loss 0.000749041, acc 1
2016-09-07T23:09:47.046174: step 3865, loss 0.0307301, acc 0.98
2016-09-07T23:09:47.765632: step 3866, loss 0.055229, acc 0.98
2016-09-07T23:09:48.477999: step 3867, loss 0.0188735, acc 0.98
2016-09-07T23:09:49.193942: step 3868, loss 0.0723964, acc 0.98
2016-09-07T23:09:49.926984: step 3869, loss 0.010606, acc 1
2016-09-07T23:09:50.639604: step 3870, loss 0.0387833, acc 0.96
2016-09-07T23:09:51.346035: step 3871, loss 0.00747833, acc 1
2016-09-07T23:09:52.055785: step 3872, loss 0.00264199, acc 1
2016-09-07T23:09:52.754166: step 3873, loss 0.0362137, acc 0.98
2016-09-07T23:09:53.459290: step 3874, loss 0.00946232, acc 1
2016-09-07T23:09:54.164562: step 3875, loss 0.0723732, acc 0.96
2016-09-07T23:09:54.872603: step 3876, loss 0.0323488, acc 0.98
2016-09-07T23:09:55.589205: step 3877, loss 0.0131834, acc 1
2016-09-07T23:09:56.283123: step 3878, loss 0.0154551, acc 1
2016-09-07T23:09:56.963895: step 3879, loss 0.0174681, acc 0.98
2016-09-07T23:09:57.366262: step 3880, loss 0.0013443, acc 1
2016-09-07T23:09:58.084853: step 3881, loss 0.0173627, acc 0.98
2016-09-07T23:09:58.805978: step 3882, loss 0.0297721, acc 1
2016-09-07T23:09:59.511917: step 3883, loss 0.015658, acc 1
2016-09-07T23:10:00.250924: step 3884, loss 0.0914849, acc 0.98
2016-09-07T23:10:00.936562: step 3885, loss 0.0433081, acc 0.98
2016-09-07T23:10:01.644617: step 3886, loss 0.0183244, acc 1
2016-09-07T23:10:02.361200: step 3887, loss 0.00676878, acc 1
2016-09-07T23:10:03.080610: step 3888, loss 0.112055, acc 0.94
2016-09-07T23:10:03.792746: step 3889, loss 0.0183405, acc 0.98
2016-09-07T23:10:04.507650: step 3890, loss 0.0326076, acc 1
2016-09-07T23:10:05.218706: step 3891, loss 0.000161612, acc 1
2016-09-07T23:10:05.907331: step 3892, loss 0.025865, acc 1
2016-09-07T23:10:06.602804: step 3893, loss 0.0187978, acc 1
2016-09-07T23:10:07.331384: step 3894, loss 0.0162491, acc 1
2016-09-07T23:10:08.027881: step 3895, loss 0.0114125, acc 1
2016-09-07T23:10:08.719561: step 3896, loss 0.0129063, acc 1
2016-09-07T23:10:09.422393: step 3897, loss 0.0163444, acc 0.98
2016-09-07T23:10:10.131466: step 3898, loss 0.00375218, acc 1
2016-09-07T23:10:10.848853: step 3899, loss 0.0436712, acc 0.98
2016-09-07T23:10:11.553560: step 3900, loss 0.043129, acc 0.96

Evaluation:
2016-09-07T23:10:15.002046: step 3900, loss 1.92801, acc 0.753

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-3900

2016-09-07T23:10:16.813441: step 3901, loss 0.0270064, acc 0.98
2016-09-07T23:10:17.498377: step 3902, loss 0.0284941, acc 1
2016-09-07T23:10:18.199317: step 3903, loss 0.0463656, acc 0.98
2016-09-07T23:10:18.912057: step 3904, loss 0.0036093, acc 1
2016-09-07T23:10:19.587929: step 3905, loss 0.00656991, acc 1
2016-09-07T23:10:20.307695: step 3906, loss 0.00916805, acc 1
2016-09-07T23:10:21.010682: step 3907, loss 0.00769778, acc 1
2016-09-07T23:10:21.706354: step 3908, loss 0.0972657, acc 0.98
2016-09-07T23:10:22.385186: step 3909, loss 0.0146062, acc 1
2016-09-07T23:10:23.097007: step 3910, loss 0.017895, acc 1
2016-09-07T23:10:23.808469: step 3911, loss 0.00702515, acc 1
2016-09-07T23:10:24.514628: step 3912, loss 0.148478, acc 0.98
2016-09-07T23:10:25.203843: step 3913, loss 0.0104292, acc 1
2016-09-07T23:10:25.906750: step 3914, loss 0.138646, acc 0.98
2016-09-07T23:10:26.604427: step 3915, loss 0.0251806, acc 0.98
2016-09-07T23:10:27.292430: step 3916, loss 0.107211, acc 0.96
2016-09-07T23:10:27.999253: step 3917, loss 0.00654475, acc 1
2016-09-07T23:10:28.692467: step 3918, loss 0.0140855, acc 1
2016-09-07T23:10:29.384968: step 3919, loss 0.000589888, acc 1
2016-09-07T23:10:30.074756: step 3920, loss 0.0095028, acc 1
2016-09-07T23:10:30.786604: step 3921, loss 0.0360472, acc 0.98
2016-09-07T23:10:31.476130: step 3922, loss 0.140194, acc 0.98
2016-09-07T23:10:32.189078: step 3923, loss 0.00598068, acc 1
2016-09-07T23:10:32.894430: step 3924, loss 0.0494931, acc 0.96
2016-09-07T23:10:33.602082: step 3925, loss 0.0524877, acc 0.98
2016-09-07T23:10:34.296283: step 3926, loss 0.0121607, acc 1
2016-09-07T23:10:34.987710: step 3927, loss 0.0300151, acc 0.98
2016-09-07T23:10:35.681974: step 3928, loss 0.0026567, acc 1
2016-09-07T23:10:36.391698: step 3929, loss 0.00900297, acc 1
2016-09-07T23:10:37.086409: step 3930, loss 0.0391937, acc 1
2016-09-07T23:10:37.778952: step 3931, loss 0.00658394, acc 1
2016-09-07T23:10:38.475842: step 3932, loss 0.0524469, acc 0.98
2016-09-07T23:10:39.164768: step 3933, loss 0.00325791, acc 1
2016-09-07T23:10:39.864753: step 3934, loss 0.0491491, acc 0.98
2016-09-07T23:10:40.559581: step 3935, loss 0.026377, acc 1
2016-09-07T23:10:41.266105: step 3936, loss 0.0516001, acc 0.96
2016-09-07T23:10:41.963459: step 3937, loss 0.0176103, acc 1
2016-09-07T23:10:42.659631: step 3938, loss 0.00370608, acc 1
2016-09-07T23:10:43.368967: step 3939, loss 0.0628259, acc 0.96
2016-09-07T23:10:44.067152: step 3940, loss 0.00144798, acc 1
2016-09-07T23:10:44.766518: step 3941, loss 0.0195592, acc 1
2016-09-07T23:10:45.508644: step 3942, loss 0.0115079, acc 1
2016-09-07T23:10:46.231187: step 3943, loss 0.0247278, acc 0.98
2016-09-07T23:10:46.925222: step 3944, loss 0.0817996, acc 0.98
2016-09-07T23:10:47.626837: step 3945, loss 0.150326, acc 0.94
2016-09-07T23:10:48.326432: step 3946, loss 0.121785, acc 0.98
2016-09-07T23:10:49.020642: step 3947, loss 0.0131305, acc 1
2016-09-07T23:10:49.711258: step 3948, loss 0.0334925, acc 0.98
2016-09-07T23:10:50.428252: step 3949, loss 0.00843553, acc 1
2016-09-07T23:10:51.126917: step 3950, loss 0.046242, acc 1
2016-09-07T23:10:51.839412: step 3951, loss 0.0452884, acc 0.98
2016-09-07T23:10:52.548601: step 3952, loss 0.0657247, acc 0.96
2016-09-07T23:10:53.261463: step 3953, loss 0.00695468, acc 1
2016-09-07T23:10:53.988585: step 3954, loss 0.0155924, acc 0.98
2016-09-07T23:10:54.685483: step 3955, loss 0.0700225, acc 0.98
2016-09-07T23:10:55.373362: step 3956, loss 0.0876224, acc 0.94
2016-09-07T23:10:56.072646: step 3957, loss 0.0181551, acc 1
2016-09-07T23:10:56.770070: step 3958, loss 0.0122894, acc 1
2016-09-07T23:10:57.454279: step 3959, loss 0.0707311, acc 0.98
2016-09-07T23:10:58.151642: step 3960, loss 0.0105059, acc 1
2016-09-07T23:10:58.842399: step 3961, loss 0.0495411, acc 0.96
2016-09-07T23:10:59.533500: step 3962, loss 0.000465203, acc 1
2016-09-07T23:11:00.213407: step 3963, loss 0.00410458, acc 1
2016-09-07T23:11:00.930907: step 3964, loss 0.0681582, acc 0.98
2016-09-07T23:11:01.625945: step 3965, loss 0.165718, acc 0.96
2016-09-07T23:11:02.318980: step 3966, loss 0.0559201, acc 0.96
2016-09-07T23:11:03.017068: step 3967, loss 0.00879188, acc 1
2016-09-07T23:11:03.715649: step 3968, loss 0.00786176, acc 1
2016-09-07T23:11:04.420255: step 3969, loss 0.0961132, acc 0.94
2016-09-07T23:11:05.122372: step 3970, loss 0.0835683, acc 0.96
2016-09-07T23:11:05.805695: step 3971, loss 0.0242971, acc 0.98
2016-09-07T23:11:06.502834: step 3972, loss 0.022707, acc 1
2016-09-07T23:11:07.198388: step 3973, loss 0.00119464, acc 1
2016-09-07T23:11:07.900075: step 3974, loss 0.0319867, acc 0.98
2016-09-07T23:11:08.609474: step 3975, loss 0.0102096, acc 1
2016-09-07T23:11:09.302440: step 3976, loss 0.0521465, acc 0.98
2016-09-07T23:11:10.017487: step 3977, loss 0.00357632, acc 1
2016-09-07T23:11:10.701949: step 3978, loss 0.0990246, acc 0.98
2016-09-07T23:11:11.403549: step 3979, loss 0.0659671, acc 0.98
2016-09-07T23:11:12.101980: step 3980, loss 0.0327927, acc 0.98
2016-09-07T23:11:12.794995: step 3981, loss 0.0207075, acc 1
2016-09-07T23:11:13.488836: step 3982, loss 0.23036, acc 0.96
2016-09-07T23:11:14.188493: step 3983, loss 0.0272495, acc 0.98
2016-09-07T23:11:14.888130: step 3984, loss 0.0407558, acc 0.98
2016-09-07T23:11:15.607777: step 3985, loss 0.0124194, acc 1
2016-09-07T23:11:16.320675: step 3986, loss 0.0319785, acc 0.98
2016-09-07T23:11:17.031268: step 3987, loss 0.0225691, acc 1
2016-09-07T23:11:17.737099: step 3988, loss 0.0846166, acc 0.98
2016-09-07T23:11:18.431594: step 3989, loss 0.034302, acc 1
2016-09-07T23:11:19.134203: step 3990, loss 0.0814866, acc 0.96
2016-09-07T23:11:19.834519: step 3991, loss 0.0710071, acc 0.98
2016-09-07T23:11:20.546143: step 3992, loss 0.00510791, acc 1
2016-09-07T23:11:21.241246: step 3993, loss 0.0343884, acc 0.98
2016-09-07T23:11:21.932137: step 3994, loss 0.0671464, acc 0.96
2016-09-07T23:11:22.620852: step 3995, loss 0.00545332, acc 1
2016-09-07T23:11:23.300306: step 3996, loss 0.0205332, acc 1
2016-09-07T23:11:23.991567: step 3997, loss 0.0477623, acc 0.98
2016-09-07T23:11:24.683281: step 3998, loss 0.0324771, acc 0.98
2016-09-07T23:11:25.371555: step 3999, loss 0.0789008, acc 0.96
2016-09-07T23:11:26.055750: step 4000, loss 0.110492, acc 0.96

Evaluation:
2016-09-07T23:11:29.478014: step 4000, loss 1.66059, acc 0.734

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-4000

2016-09-07T23:11:31.163531: step 4001, loss 0.0155024, acc 1
2016-09-07T23:11:31.879298: step 4002, loss 0.108131, acc 0.96
2016-09-07T23:11:32.580205: step 4003, loss 0.0437395, acc 0.98
2016-09-07T23:11:33.295757: step 4004, loss 0.00801261, acc 1
2016-09-07T23:11:34.029736: step 4005, loss 0.0691431, acc 0.98
2016-09-07T23:11:34.732165: step 4006, loss 0.0118824, acc 1
2016-09-07T23:11:35.431400: step 4007, loss 0.0110012, acc 1
2016-09-07T23:11:36.126066: step 4008, loss 0.0148964, acc 1
2016-09-07T23:11:36.825330: step 4009, loss 0.0745078, acc 0.98
2016-09-07T23:11:37.529973: step 4010, loss 0.0398051, acc 0.98
2016-09-07T23:11:38.242099: step 4011, loss 0.125914, acc 0.96
2016-09-07T23:11:38.922790: step 4012, loss 0.0628447, acc 0.98
2016-09-07T23:11:39.636302: step 4013, loss 0.00659536, acc 1
2016-09-07T23:11:40.348951: step 4014, loss 0.088932, acc 0.96
2016-09-07T23:11:41.062302: step 4015, loss 0.0433595, acc 1
2016-09-07T23:11:41.759787: step 4016, loss 0.0746973, acc 0.96
2016-09-07T23:11:42.466862: step 4017, loss 0.017209, acc 1
2016-09-07T23:11:43.171492: step 4018, loss 0.00164886, acc 1
2016-09-07T23:11:43.862969: step 4019, loss 0.0443284, acc 0.98
2016-09-07T23:11:44.561782: step 4020, loss 0.0191453, acc 1
2016-09-07T23:11:45.271902: step 4021, loss 0.0252168, acc 0.98
2016-09-07T23:11:45.968179: step 4022, loss 0.132734, acc 0.98
2016-09-07T23:11:46.691634: step 4023, loss 0.0117394, acc 1
2016-09-07T23:11:47.412687: step 4024, loss 0.00186865, acc 1
2016-09-07T23:11:48.117795: step 4025, loss 0.0321971, acc 0.98
2016-09-07T23:11:48.822954: step 4026, loss 0.0174373, acc 1
2016-09-07T23:11:49.530526: step 4027, loss 0.0152719, acc 1
2016-09-07T23:11:50.220041: step 4028, loss 0.0173107, acc 1
2016-09-07T23:11:50.935782: step 4029, loss 0.0647263, acc 0.98
2016-09-07T23:11:51.619116: step 4030, loss 0.00191787, acc 1
2016-09-07T23:11:52.320126: step 4031, loss 0.0271026, acc 1
2016-09-07T23:11:53.015867: step 4032, loss 0.0437221, acc 1
2016-09-07T23:11:53.698604: step 4033, loss 0.086269, acc 0.98
2016-09-07T23:11:54.405694: step 4034, loss 0.0488055, acc 0.98
2016-09-07T23:11:55.098051: step 4035, loss 0.0314758, acc 1
2016-09-07T23:11:55.805566: step 4036, loss 0.0196991, acc 0.98
2016-09-07T23:11:56.499350: step 4037, loss 0.0600933, acc 0.96
2016-09-07T23:11:57.192836: step 4038, loss 0.0353067, acc 0.96
2016-09-07T23:11:57.899237: step 4039, loss 0.0441258, acc 0.98
2016-09-07T23:11:58.609228: step 4040, loss 0.0230411, acc 0.98
2016-09-07T23:11:59.327419: step 4041, loss 0.0279927, acc 0.98
2016-09-07T23:12:00.040527: step 4042, loss 0.0896684, acc 0.94
2016-09-07T23:12:00.783694: step 4043, loss 0.00120642, acc 1
2016-09-07T23:12:01.478346: step 4044, loss 0.0177775, acc 1
2016-09-07T23:12:02.174697: step 4045, loss 0.00452746, acc 1
2016-09-07T23:12:02.894510: step 4046, loss 0.0258071, acc 0.98
2016-09-07T23:12:03.589745: step 4047, loss 0.00765829, acc 1
2016-09-07T23:12:04.291497: step 4048, loss 0.016562, acc 1
2016-09-07T23:12:04.973678: step 4049, loss 0.06534, acc 0.96
2016-09-07T23:12:05.669883: step 4050, loss 0.0589479, acc 0.98
2016-09-07T23:12:06.383413: step 4051, loss 0.0132965, acc 1
2016-09-07T23:12:07.089656: step 4052, loss 0.0560845, acc 0.98
2016-09-07T23:12:07.789573: step 4053, loss 0.0662284, acc 0.98
2016-09-07T23:12:08.504462: step 4054, loss 0.000234505, acc 1
2016-09-07T23:12:09.211345: step 4055, loss 0.00171788, acc 1
2016-09-07T23:12:09.887992: step 4056, loss 0.027038, acc 1
2016-09-07T23:12:10.589057: step 4057, loss 0.00523036, acc 1
2016-09-07T23:12:11.315662: step 4058, loss 0.0630956, acc 0.98
2016-09-07T23:12:12.016955: step 4059, loss 0.0150709, acc 0.98
2016-09-07T23:12:12.711608: step 4060, loss 0.0253062, acc 0.98
2016-09-07T23:12:13.407409: step 4061, loss 0.0417701, acc 0.96
2016-09-07T23:12:14.108317: step 4062, loss 0.0499484, acc 0.98
2016-09-07T23:12:14.800003: step 4063, loss 0.0243654, acc 1
2016-09-07T23:12:15.499067: step 4064, loss 0.000762388, acc 1
2016-09-07T23:12:16.193303: step 4065, loss 0.00583651, acc 1
2016-09-07T23:12:16.875007: step 4066, loss 0.00298237, acc 1
2016-09-07T23:12:17.566416: step 4067, loss 0.0363513, acc 0.98
2016-09-07T23:12:18.258925: step 4068, loss 0.0029473, acc 1
2016-09-07T23:12:18.958051: step 4069, loss 0.0117688, acc 1
2016-09-07T23:12:19.687059: step 4070, loss 0.0823751, acc 0.94
2016-09-07T23:12:20.380638: step 4071, loss 0.0458881, acc 0.96
2016-09-07T23:12:21.088777: step 4072, loss 0.0139605, acc 1
2016-09-07T23:12:21.790790: step 4073, loss 0.0554606, acc 0.98
2016-09-07T23:12:22.193447: step 4074, loss 0.0825051, acc 0.916667
2016-09-07T23:12:22.915733: step 4075, loss 0.0600498, acc 0.94
2016-09-07T23:12:23.618188: step 4076, loss 0.0305811, acc 0.98
2016-09-07T23:12:24.327324: step 4077, loss 0.0641773, acc 0.96
2016-09-07T23:12:25.044887: step 4078, loss 0.00735114, acc 1
2016-09-07T23:12:25.745539: step 4079, loss 0.18373, acc 0.94
2016-09-07T23:12:26.442288: step 4080, loss 0.0337386, acc 0.98
2016-09-07T23:12:27.140123: step 4081, loss 0.00137485, acc 1
2016-09-07T23:12:27.835481: step 4082, loss 0.0220824, acc 1
2016-09-07T23:12:28.545533: step 4083, loss 0.0023165, acc 1
2016-09-07T23:12:29.253623: step 4084, loss 0.0671365, acc 0.96
2016-09-07T23:12:29.948973: step 4085, loss 0.0706037, acc 0.98
2016-09-07T23:12:30.650895: step 4086, loss 0.0428476, acc 0.98
2016-09-07T23:12:31.350882: step 4087, loss 0.0162022, acc 1
2016-09-07T23:12:32.057637: step 4088, loss 0.0245516, acc 0.98
2016-09-07T23:12:32.745756: step 4089, loss 0.0367585, acc 0.98
2016-09-07T23:12:33.447055: step 4090, loss 0.0260161, acc 0.98
2016-09-07T23:12:34.143982: step 4091, loss 0.0333729, acc 0.98
2016-09-07T23:12:34.843960: step 4092, loss 0.0535464, acc 0.98
2016-09-07T23:12:35.541610: step 4093, loss 0.0259897, acc 0.98
2016-09-07T23:12:36.245314: step 4094, loss 0.0166761, acc 0.98
2016-09-07T23:12:36.937444: step 4095, loss 0.0372006, acc 0.96
2016-09-07T23:12:37.640149: step 4096, loss 0.017163, acc 1
2016-09-07T23:12:38.352564: step 4097, loss 0.0693551, acc 0.98
2016-09-07T23:12:39.055478: step 4098, loss 0.000546936, acc 1
2016-09-07T23:12:39.764031: step 4099, loss 0.0115632, acc 1
2016-09-07T23:12:40.469523: step 4100, loss 0.0142599, acc 1

Evaluation:
2016-09-07T23:12:43.842365: step 4100, loss 1.93611, acc 0.739

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-4100

2016-09-07T23:12:45.536229: step 4101, loss 0.0172354, acc 0.98
2016-09-07T23:12:46.234010: step 4102, loss 0.0147462, acc 1
2016-09-07T23:12:46.931081: step 4103, loss 0.0289652, acc 0.98
2016-09-07T23:12:47.626550: step 4104, loss 0.0252424, acc 0.98
2016-09-07T23:12:48.335266: step 4105, loss 0.0171749, acc 0.98
2016-09-07T23:12:49.032243: step 4106, loss 0.0535992, acc 0.96
2016-09-07T23:12:49.729024: step 4107, loss 0.0217158, acc 0.98
2016-09-07T23:12:50.406738: step 4108, loss 0.00675667, acc 1
2016-09-07T23:12:51.111139: step 4109, loss 0.00289235, acc 1
2016-09-07T23:12:51.801075: step 4110, loss 0.0270702, acc 0.98
2016-09-07T23:12:52.510558: step 4111, loss 0.0139167, acc 1
2016-09-07T23:12:53.202977: step 4112, loss 0.0051171, acc 1
2016-09-07T23:12:53.902580: step 4113, loss 0.0139821, acc 1
2016-09-07T23:12:54.588851: step 4114, loss 0.0959533, acc 0.96
2016-09-07T23:12:55.272820: step 4115, loss 0.0185508, acc 0.98
2016-09-07T23:12:55.959485: step 4116, loss 0.0117883, acc 1
2016-09-07T23:12:56.653231: step 4117, loss 0.000611276, acc 1
2016-09-07T23:12:57.353464: step 4118, loss 0.0158261, acc 1
2016-09-07T23:12:58.070723: step 4119, loss 0.0137156, acc 1
2016-09-07T23:12:58.790542: step 4120, loss 0.0650724, acc 0.96
2016-09-07T23:12:59.507810: step 4121, loss 0.0124987, acc 1
2016-09-07T23:13:00.217883: step 4122, loss 0.0472662, acc 0.96
2016-09-07T23:13:00.954725: step 4123, loss 0.015601, acc 0.98
2016-09-07T23:13:01.643181: step 4124, loss 0.0122079, acc 1
2016-09-07T23:13:02.342512: step 4125, loss 0.0114708, acc 1
2016-09-07T23:13:03.045615: step 4126, loss 0.00717177, acc 1
2016-09-07T23:13:03.723641: step 4127, loss 0.0113615, acc 1
2016-09-07T23:13:04.414058: step 4128, loss 0.018175, acc 0.98
2016-09-07T23:13:05.100001: step 4129, loss 0.0577666, acc 0.96
2016-09-07T23:13:05.802960: step 4130, loss 0.0620439, acc 0.98
2016-09-07T23:13:06.503772: step 4131, loss 0.0308203, acc 0.98
2016-09-07T23:13:07.204873: step 4132, loss 0.00133093, acc 1
2016-09-07T23:13:07.929777: step 4133, loss 0.00557665, acc 1
2016-09-07T23:13:08.630332: step 4134, loss 0.0299234, acc 0.98
2016-09-07T23:13:09.325650: step 4135, loss 0.0226204, acc 0.98
2016-09-07T23:13:10.021548: step 4136, loss 0.0266718, acc 0.98
2016-09-07T23:13:10.744169: step 4137, loss 0.0476176, acc 0.96
2016-09-07T23:13:11.437315: step 4138, loss 0.0314029, acc 0.98
2016-09-07T23:13:12.133292: step 4139, loss 0.0420099, acc 0.96
2016-09-07T23:13:12.852999: step 4140, loss 0.0931684, acc 0.98
2016-09-07T23:13:13.556235: step 4141, loss 0.00766765, acc 1
2016-09-07T23:13:14.246689: step 4142, loss 0.00690788, acc 1
2016-09-07T23:13:14.952511: step 4143, loss 0.0430425, acc 0.98
2016-09-07T23:13:15.663746: step 4144, loss 0.0208331, acc 1
2016-09-07T23:13:16.365922: step 4145, loss 0.00713924, acc 1
2016-09-07T23:13:17.069218: step 4146, loss 0.0774319, acc 0.96
2016-09-07T23:13:17.784262: step 4147, loss 0.00554636, acc 1
2016-09-07T23:13:18.504425: step 4148, loss 0.0344097, acc 0.96
2016-09-07T23:13:19.198983: step 4149, loss 0.0211226, acc 0.98
2016-09-07T23:13:19.885644: step 4150, loss 0.0595206, acc 0.94
2016-09-07T23:13:20.582151: step 4151, loss 0.0265201, acc 1
2016-09-07T23:13:21.284743: step 4152, loss 0.00876738, acc 1
2016-09-07T23:13:21.975778: step 4153, loss 0.0170426, acc 1
2016-09-07T23:13:22.684270: step 4154, loss 0.118063, acc 0.92
2016-09-07T23:13:23.382110: step 4155, loss 0.0658737, acc 0.96
2016-09-07T23:13:24.072382: step 4156, loss 0.00147706, acc 1
2016-09-07T23:13:24.764031: step 4157, loss 0.0219405, acc 1
2016-09-07T23:13:25.471133: step 4158, loss 0.010662, acc 1
2016-09-07T23:13:26.181456: step 4159, loss 0.00186741, acc 1
2016-09-07T23:13:26.878587: step 4160, loss 0.00362165, acc 1
2016-09-07T23:13:27.566883: step 4161, loss 0.0401881, acc 0.98
2016-09-07T23:13:28.262518: step 4162, loss 0.0172601, acc 0.98
2016-09-07T23:13:28.957610: step 4163, loss 0.00443924, acc 1
2016-09-07T23:13:29.650162: step 4164, loss 0.0680902, acc 0.94
2016-09-07T23:13:30.336219: step 4165, loss 0.0391219, acc 0.98
2016-09-07T23:13:31.040486: step 4166, loss 0.0405766, acc 0.98
2016-09-07T23:13:31.741549: step 4167, loss 0.00901663, acc 1
2016-09-07T23:13:32.429744: step 4168, loss 0.00277422, acc 1
2016-09-07T23:13:33.115284: step 4169, loss 0.0935275, acc 0.98
2016-09-07T23:13:33.815135: step 4170, loss 0.00669109, acc 1
2016-09-07T23:13:34.519307: step 4171, loss 0.00114128, acc 1
2016-09-07T23:13:35.222582: step 4172, loss 0.0335076, acc 1
2016-09-07T23:13:35.916799: step 4173, loss 0.0258246, acc 1
2016-09-07T23:13:36.623254: step 4174, loss 0.00554628, acc 1
2016-09-07T23:13:37.329803: step 4175, loss 0.00188279, acc 1
2016-09-07T23:13:38.037195: step 4176, loss 0.0381467, acc 0.96
2016-09-07T23:13:38.727967: step 4177, loss 0.0214725, acc 0.98
2016-09-07T23:13:39.421790: step 4178, loss 0.0816544, acc 0.96
2016-09-07T23:13:40.125841: step 4179, loss 0.0861644, acc 0.98
2016-09-07T23:13:40.819541: step 4180, loss 0.0119962, acc 1
2016-09-07T23:13:41.532180: step 4181, loss 0.0133069, acc 1
2016-09-07T23:13:42.227814: step 4182, loss 0.020381, acc 1
2016-09-07T23:13:42.934612: step 4183, loss 0.0304462, acc 0.98
2016-09-07T23:13:43.636224: step 4184, loss 0.000920606, acc 1
2016-09-07T23:13:44.341649: step 4185, loss 0.0334269, acc 0.98
2016-09-07T23:13:45.042944: step 4186, loss 0.000732665, acc 1
2016-09-07T23:13:45.740176: step 4187, loss 0.0275902, acc 1
2016-09-07T23:13:46.434591: step 4188, loss 0.0129675, acc 1
2016-09-07T23:13:47.147428: step 4189, loss 0.00493173, acc 1
2016-09-07T23:13:47.856916: step 4190, loss 0.0699985, acc 0.96
2016-09-07T23:13:48.561002: step 4191, loss 0.0352577, acc 0.98
2016-09-07T23:13:49.258790: step 4192, loss 0.030745, acc 0.98
2016-09-07T23:13:49.952647: step 4193, loss 0.00316633, acc 1
2016-09-07T23:13:50.634124: step 4194, loss 0.0255547, acc 1
2016-09-07T23:13:51.333649: step 4195, loss 0.0286365, acc 0.98
2016-09-07T23:13:52.042877: step 4196, loss 0.000614176, acc 1
2016-09-07T23:13:52.736729: step 4197, loss 0.0266203, acc 0.98
2016-09-07T23:13:53.468075: step 4198, loss 0.0563412, acc 0.98
2016-09-07T23:13:54.150433: step 4199, loss 0.0266601, acc 0.98
2016-09-07T23:13:54.853933: step 4200, loss 0.0391207, acc 0.98

Evaluation:
2016-09-07T23:13:58.182589: step 4200, loss 2.14317, acc 0.747

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-4200

2016-09-07T23:14:00.063095: step 4201, loss 0.040791, acc 0.96
2016-09-07T23:14:00.805737: step 4202, loss 0.00188336, acc 1
2016-09-07T23:14:01.515789: step 4203, loss 0.0188712, acc 1
2016-09-07T23:14:02.198822: step 4204, loss 0.105793, acc 0.98
2016-09-07T23:14:02.902605: step 4205, loss 0.0819959, acc 0.94
2016-09-07T23:14:03.605904: step 4206, loss 0.0752819, acc 0.96
2016-09-07T23:14:04.302637: step 4207, loss 0.0591601, acc 0.98
2016-09-07T23:14:04.991319: step 4208, loss 0.0333225, acc 0.96
2016-09-07T23:14:05.666674: step 4209, loss 0.0505831, acc 0.98
2016-09-07T23:14:06.382485: step 4210, loss 0.101657, acc 0.94
2016-09-07T23:14:07.088221: step 4211, loss 0.0126778, acc 1
2016-09-07T23:14:07.786833: step 4212, loss 0.000161156, acc 1
2016-09-07T23:14:08.483777: step 4213, loss 0.0034318, acc 1
2016-09-07T23:14:09.189983: step 4214, loss 0.0183487, acc 1
2016-09-07T23:14:09.898560: step 4215, loss 0.000150558, acc 1
2016-09-07T23:14:10.627476: step 4216, loss 0.0426401, acc 0.98
2016-09-07T23:14:11.348297: step 4217, loss 0.0345094, acc 0.98
2016-09-07T23:14:12.064607: step 4218, loss 0.0186951, acc 1
2016-09-07T23:14:12.771541: step 4219, loss 0.00713625, acc 1
2016-09-07T23:14:13.485649: step 4220, loss 0.00424245, acc 1
2016-09-07T23:14:14.178113: step 4221, loss 0.0652787, acc 0.98
2016-09-07T23:14:14.903417: step 4222, loss 0.0753351, acc 0.98
2016-09-07T23:14:15.620547: step 4223, loss 0.142566, acc 0.96
2016-09-07T23:14:16.312347: step 4224, loss 0.0157756, acc 0.98
2016-09-07T23:14:17.015022: step 4225, loss 0.1065, acc 0.94
2016-09-07T23:14:17.718507: step 4226, loss 0.0418656, acc 0.98
2016-09-07T23:14:18.409605: step 4227, loss 0.0183023, acc 1
2016-09-07T23:14:19.107458: step 4228, loss 0.0876615, acc 0.96
2016-09-07T23:14:19.822482: step 4229, loss 0.0263375, acc 0.98
2016-09-07T23:14:20.508676: step 4230, loss 0.00881737, acc 1
2016-09-07T23:14:21.194398: step 4231, loss 0.00806135, acc 1
2016-09-07T23:14:21.882079: step 4232, loss 0.0468341, acc 0.98
2016-09-07T23:14:22.597878: step 4233, loss 0.00576808, acc 1
2016-09-07T23:14:23.291795: step 4234, loss 0.00422514, acc 1
2016-09-07T23:14:24.009693: step 4235, loss 0.0475493, acc 0.98
2016-09-07T23:14:24.694943: step 4236, loss 0.0831879, acc 0.96
2016-09-07T23:14:25.386482: step 4237, loss 0.0723412, acc 0.96
2016-09-07T23:14:26.081603: step 4238, loss 0.00314309, acc 1
2016-09-07T23:14:26.788586: step 4239, loss 0.0107983, acc 1
2016-09-07T23:14:27.492361: step 4240, loss 0.0739001, acc 0.96
2016-09-07T23:14:28.211636: step 4241, loss 0.0185631, acc 1
2016-09-07T23:14:28.922831: step 4242, loss 0.016523, acc 0.98
2016-09-07T23:14:29.621549: step 4243, loss 0.0212681, acc 1
2016-09-07T23:14:30.323281: step 4244, loss 0.177492, acc 0.98
2016-09-07T23:14:31.038028: step 4245, loss 0.0422687, acc 1
2016-09-07T23:14:31.740658: step 4246, loss 0.0156651, acc 1
2016-09-07T23:14:32.439153: step 4247, loss 0.120865, acc 0.98
2016-09-07T23:14:33.153862: step 4248, loss 0.0169399, acc 0.98
2016-09-07T23:14:33.846395: step 4249, loss 0.148665, acc 0.96
2016-09-07T23:14:34.543228: step 4250, loss 0.057199, acc 0.98
2016-09-07T23:14:35.243771: step 4251, loss 0.034895, acc 0.98
2016-09-07T23:14:35.942200: step 4252, loss 0.00140849, acc 1
2016-09-07T23:14:36.631538: step 4253, loss 0.00716285, acc 1
2016-09-07T23:14:37.328102: step 4254, loss 0.0148864, acc 1
2016-09-07T23:14:38.050013: step 4255, loss 0.0120665, acc 1
2016-09-07T23:14:38.743220: step 4256, loss 0.00540553, acc 1
2016-09-07T23:14:39.442726: step 4257, loss 0.0420119, acc 0.98
2016-09-07T23:14:40.143270: step 4258, loss 0.0939282, acc 0.96
2016-09-07T23:14:40.839450: step 4259, loss 0.0139643, acc 1
2016-09-07T23:14:41.570747: step 4260, loss 0.0220625, acc 1
2016-09-07T23:14:42.271810: step 4261, loss 0.0326631, acc 0.98
2016-09-07T23:14:42.958424: step 4262, loss 6.84071e-05, acc 1
2016-09-07T23:14:43.659522: step 4263, loss 0.0180641, acc 1
2016-09-07T23:14:44.345553: step 4264, loss 0.00730715, acc 1
2016-09-07T23:14:45.038541: step 4265, loss 0.105978, acc 0.98
2016-09-07T23:14:45.734331: step 4266, loss 0.0136959, acc 1
2016-09-07T23:14:46.433402: step 4267, loss 0.0799336, acc 0.98
2016-09-07T23:14:46.802395: step 4268, loss 0.0518794, acc 1
2016-09-07T23:14:47.485861: step 4269, loss 0.0476481, acc 0.98
2016-09-07T23:14:48.199307: step 4270, loss 0.00601075, acc 1
2016-09-07T23:14:48.896912: step 4271, loss 0.0483455, acc 0.96
2016-09-07T23:14:49.598236: step 4272, loss 0.0178101, acc 1
2016-09-07T23:14:50.303808: step 4273, loss 0.0110393, acc 1
2016-09-07T23:14:51.008601: step 4274, loss 0.0579872, acc 0.98
2016-09-07T23:14:51.708643: step 4275, loss 0.0319903, acc 0.96
2016-09-07T23:14:52.401861: step 4276, loss 0.0582495, acc 0.98
2016-09-07T23:14:53.093569: step 4277, loss 0.00360349, acc 1
2016-09-07T23:14:53.786443: step 4278, loss 0.0770233, acc 0.96
2016-09-07T23:14:54.496552: step 4279, loss 0.0695388, acc 0.98
2016-09-07T23:14:55.203894: step 4280, loss 0.067769, acc 0.96
2016-09-07T23:14:55.876521: step 4281, loss 0.0407468, acc 0.96
2016-09-07T23:14:56.563887: step 4282, loss 0.00788562, acc 1
2016-09-07T23:14:57.266658: step 4283, loss 0.0470623, acc 0.98
2016-09-07T23:14:57.978074: step 4284, loss 0.0065414, acc 1
2016-09-07T23:14:58.682237: step 4285, loss 0.0331241, acc 0.98
2016-09-07T23:14:59.389043: step 4286, loss 0.00941627, acc 1
2016-09-07T23:15:00.088135: step 4287, loss 0.0542418, acc 0.96
2016-09-07T23:15:00.837288: step 4288, loss 0.02998, acc 0.98
2016-09-07T23:15:01.529991: step 4289, loss 0.0218519, acc 0.98
2016-09-07T23:15:02.233650: step 4290, loss 0.0383443, acc 0.96
2016-09-07T23:15:02.924861: step 4291, loss 0.0184218, acc 1
2016-09-07T23:15:03.626105: step 4292, loss 0.00641225, acc 1
2016-09-07T23:15:04.336299: step 4293, loss 0.0553639, acc 0.98
2016-09-07T23:15:05.043090: step 4294, loss 0.017888, acc 1
2016-09-07T23:15:05.737702: step 4295, loss 0.0168261, acc 0.98
2016-09-07T23:15:06.434130: step 4296, loss 0.0175506, acc 1
2016-09-07T23:15:07.135610: step 4297, loss 0.0199298, acc 0.98
2016-09-07T23:15:07.817617: step 4298, loss 0.00296655, acc 1
2016-09-07T23:15:08.517301: step 4299, loss 0.0205921, acc 0.98
2016-09-07T23:15:09.212591: step 4300, loss 0.0572061, acc 0.98

Evaluation:
2016-09-07T23:15:12.555895: step 4300, loss 1.96171, acc 0.75

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-4300

2016-09-07T23:15:14.248003: step 4301, loss 0.0177586, acc 1
2016-09-07T23:15:14.956913: step 4302, loss 0.0169679, acc 1
2016-09-07T23:15:15.647739: step 4303, loss 0.000474239, acc 1
2016-09-07T23:15:16.356503: step 4304, loss 0.0448255, acc 0.96
2016-09-07T23:15:17.073489: step 4305, loss 0.0144076, acc 0.98
2016-09-07T23:15:17.777212: step 4306, loss 0.121784, acc 0.94
2016-09-07T23:15:18.484496: step 4307, loss 0.00254903, acc 1
2016-09-07T23:15:19.166691: step 4308, loss 0.00284119, acc 1
2016-09-07T23:15:19.841553: step 4309, loss 0.0233318, acc 0.98
2016-09-07T23:15:20.538320: step 4310, loss 0.0154464, acc 1
2016-09-07T23:15:21.236283: step 4311, loss 0.0627455, acc 0.98
2016-09-07T23:15:21.945570: step 4312, loss 0.0176554, acc 1
2016-09-07T23:15:22.649256: step 4313, loss 0.00119968, acc 1
2016-09-07T23:15:23.353233: step 4314, loss 0.0470184, acc 0.98
2016-09-07T23:15:24.066281: step 4315, loss 0.0139723, acc 1
2016-09-07T23:15:24.784826: step 4316, loss 0.017551, acc 0.98
2016-09-07T23:15:25.489446: step 4317, loss 0.0167272, acc 0.98
2016-09-07T23:15:26.178445: step 4318, loss 0.0389151, acc 0.98
2016-09-07T23:15:26.884723: step 4319, loss 0.00603186, acc 1
2016-09-07T23:15:27.570075: step 4320, loss 0.0138099, acc 1
2016-09-07T23:15:28.267817: step 4321, loss 0.0120955, acc 1
2016-09-07T23:15:28.985540: step 4322, loss 0.000831111, acc 1
2016-09-07T23:15:29.716602: step 4323, loss 0.0181787, acc 1
2016-09-07T23:15:30.415191: step 4324, loss 0.0228552, acc 1
2016-09-07T23:15:31.106912: step 4325, loss 0.00700919, acc 1
2016-09-07T23:15:31.799621: step 4326, loss 0.000326035, acc 1
2016-09-07T23:15:32.497801: step 4327, loss 0.0624474, acc 0.96
2016-09-07T23:15:33.188332: step 4328, loss 0.0314086, acc 0.98
2016-09-07T23:15:33.885961: step 4329, loss 0.0373695, acc 0.98
2016-09-07T23:15:34.592240: step 4330, loss 0.0268206, acc 0.98
2016-09-07T23:15:35.293010: step 4331, loss 0.0382355, acc 0.98
2016-09-07T23:15:35.994116: step 4332, loss 0.0303498, acc 0.98
2016-09-07T23:15:36.681994: step 4333, loss 0.0820481, acc 0.98
2016-09-07T23:15:37.383512: step 4334, loss 0.041163, acc 0.98
2016-09-07T23:15:38.080481: step 4335, loss 0.063498, acc 0.98
2016-09-07T23:15:38.794234: step 4336, loss 0.0101163, acc 1
2016-09-07T23:15:39.509730: step 4337, loss 0.00158688, acc 1
2016-09-07T23:15:40.214070: step 4338, loss 0.0472908, acc 0.96
2016-09-07T23:15:40.910881: step 4339, loss 0.0235447, acc 0.98
2016-09-07T23:15:41.621367: step 4340, loss 0.0369973, acc 0.96
2016-09-07T23:15:42.347753: step 4341, loss 1.54961e-05, acc 1
2016-09-07T23:15:43.045615: step 4342, loss 0.00120577, acc 1
2016-09-07T23:15:43.749793: step 4343, loss 0.0339493, acc 0.98
2016-09-07T23:15:44.447107: step 4344, loss 0.0414839, acc 0.98
2016-09-07T23:15:45.142836: step 4345, loss 0.0205315, acc 1
2016-09-07T23:15:45.857817: step 4346, loss 0.0587745, acc 0.96
2016-09-07T23:15:46.562439: step 4347, loss 0.00203057, acc 1
2016-09-07T23:15:47.261577: step 4348, loss 0.0615889, acc 0.98
2016-09-07T23:15:47.987596: step 4349, loss 0.0195593, acc 1
2016-09-07T23:15:48.683907: step 4350, loss 0.0305833, acc 0.98
2016-09-07T23:15:49.384261: step 4351, loss 0.0827899, acc 0.96
2016-09-07T23:15:50.068655: step 4352, loss 0.00642295, acc 1
2016-09-07T23:15:50.773350: step 4353, loss 4.02269e-05, acc 1
2016-09-07T23:15:51.474796: step 4354, loss 0.0367128, acc 0.98
2016-09-07T23:15:52.178723: step 4355, loss 0.00751989, acc 1
2016-09-07T23:15:52.852714: step 4356, loss 0.00772726, acc 1
2016-09-07T23:15:53.550540: step 4357, loss 0.00311744, acc 1
2016-09-07T23:15:54.249181: step 4358, loss 0.0393365, acc 0.98
2016-09-07T23:15:54.942737: step 4359, loss 0.0692087, acc 0.96
2016-09-07T23:15:55.633978: step 4360, loss 0.0195603, acc 1
2016-09-07T23:15:56.331482: step 4361, loss 0.0335089, acc 0.98
2016-09-07T23:15:57.042188: step 4362, loss 0.0210873, acc 1
2016-09-07T23:15:57.738088: step 4363, loss 0.033995, acc 0.98
2016-09-07T23:15:58.422903: step 4364, loss 0.00378791, acc 1
2016-09-07T23:15:59.106132: step 4365, loss 0.00447089, acc 1
2016-09-07T23:15:59.789117: step 4366, loss 0.0372567, acc 0.98
2016-09-07T23:16:00.521127: step 4367, loss 0.0182026, acc 0.98
2016-09-07T23:16:01.218847: step 4368, loss 0.0169205, acc 1
2016-09-07T23:16:01.917880: step 4369, loss 0.00406182, acc 1
2016-09-07T23:16:02.621929: step 4370, loss 0.0185819, acc 1
2016-09-07T23:16:03.326329: step 4371, loss 0.0921492, acc 0.98
2016-09-07T23:16:04.021412: step 4372, loss 0.151083, acc 0.94
2016-09-07T23:16:04.715992: step 4373, loss 0.00355179, acc 1
2016-09-07T23:16:05.418961: step 4374, loss 0.00504111, acc 1
2016-09-07T23:16:06.093939: step 4375, loss 0.0468654, acc 0.98
2016-09-07T23:16:06.783647: step 4376, loss 0.028448, acc 1
2016-09-07T23:16:07.472209: step 4377, loss 0.0581345, acc 0.98
2016-09-07T23:16:08.168554: step 4378, loss 0.00419914, acc 1
2016-09-07T23:16:08.877023: step 4379, loss 0.0518541, acc 0.98
2016-09-07T23:16:09.577811: step 4380, loss 0.000500802, acc 1
2016-09-07T23:16:10.294148: step 4381, loss 0.00325998, acc 1
2016-09-07T23:16:10.986210: step 4382, loss 0.0165718, acc 0.98
2016-09-07T23:16:11.689605: step 4383, loss 0.00630348, acc 1
2016-09-07T23:16:12.389765: step 4384, loss 0.0528967, acc 0.96
2016-09-07T23:16:13.087118: step 4385, loss 0.0304051, acc 0.98
2016-09-07T23:16:13.779047: step 4386, loss 0.170825, acc 0.96
2016-09-07T23:16:14.490923: step 4387, loss 0.0101608, acc 1
2016-09-07T23:16:15.214113: step 4388, loss 0.0277006, acc 1
2016-09-07T23:16:15.902852: step 4389, loss 0.0195534, acc 0.98
2016-09-07T23:16:16.587113: step 4390, loss 0.0178592, acc 1
2016-09-07T23:16:17.291643: step 4391, loss 0.0540847, acc 0.96
2016-09-07T23:16:17.974869: step 4392, loss 0.0445172, acc 0.98
2016-09-07T23:16:18.670195: step 4393, loss 0.0426756, acc 0.98
2016-09-07T23:16:19.377965: step 4394, loss 0.0186419, acc 1
2016-09-07T23:16:20.061488: step 4395, loss 0.0316498, acc 1
2016-09-07T23:16:20.749828: step 4396, loss 0.0349425, acc 0.98
2016-09-07T23:16:21.437890: step 4397, loss 0.00847382, acc 1
2016-09-07T23:16:22.142328: step 4398, loss 0.0450729, acc 0.98
2016-09-07T23:16:22.834576: step 4399, loss 0.0645507, acc 0.98
2016-09-07T23:16:23.528564: step 4400, loss 0.0685495, acc 0.98

Evaluation:
2016-09-07T23:16:26.887047: step 4400, loss 1.90126, acc 0.741

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-4400

2016-09-07T23:16:28.652406: step 4401, loss 0.0433941, acc 0.98
2016-09-07T23:16:29.364287: step 4402, loss 0.0285156, acc 1
2016-09-07T23:16:30.062103: step 4403, loss 0.0520082, acc 0.98
2016-09-07T23:16:30.739386: step 4404, loss 0.00775276, acc 1
2016-09-07T23:16:31.431608: step 4405, loss 0.0686536, acc 0.98
2016-09-07T23:16:32.115583: step 4406, loss 0.000950025, acc 1
2016-09-07T23:16:32.808254: step 4407, loss 0.023742, acc 0.98
2016-09-07T23:16:33.503846: step 4408, loss 0.0316826, acc 0.98
2016-09-07T23:16:34.211415: step 4409, loss 0.013372, acc 1
2016-09-07T23:16:34.925791: step 4410, loss 0.0326591, acc 0.96
2016-09-07T23:16:35.625664: step 4411, loss 0.0177846, acc 1
2016-09-07T23:16:36.337534: step 4412, loss 0.0583951, acc 0.98
2016-09-07T23:16:37.042563: step 4413, loss 0.0131925, acc 1
2016-09-07T23:16:37.724187: step 4414, loss 0.000124977, acc 1
2016-09-07T23:16:38.431437: step 4415, loss 0.00151652, acc 1
2016-09-07T23:16:39.126100: step 4416, loss 0.105177, acc 0.94
2016-09-07T23:16:39.840262: step 4417, loss 0.101554, acc 0.96
2016-09-07T23:16:40.530953: step 4418, loss 0.0101936, acc 1
2016-09-07T23:16:41.225972: step 4419, loss 0.034563, acc 0.98
2016-09-07T23:16:41.917688: step 4420, loss 0.0712529, acc 0.94
2016-09-07T23:16:42.607024: step 4421, loss 0.0180096, acc 0.98
2016-09-07T23:16:43.300476: step 4422, loss 0.0187355, acc 1
2016-09-07T23:16:43.998463: step 4423, loss 0.00978555, acc 1
2016-09-07T23:16:44.697587: step 4424, loss 0.0111548, acc 1
2016-09-07T23:16:45.383974: step 4425, loss 0.0037865, acc 1
2016-09-07T23:16:46.087609: step 4426, loss 0.0421735, acc 0.98
2016-09-07T23:16:46.779049: step 4427, loss 0.0220175, acc 1
2016-09-07T23:16:47.467935: step 4428, loss 0.00948357, acc 1
2016-09-07T23:16:48.182331: step 4429, loss 0.131298, acc 0.96
2016-09-07T23:16:48.903930: step 4430, loss 0.0161929, acc 1
2016-09-07T23:16:49.604424: step 4431, loss 0.00565492, acc 1
2016-09-07T23:16:50.312019: step 4432, loss 0.0947905, acc 0.96
2016-09-07T23:16:51.021999: step 4433, loss 0.00512156, acc 1
2016-09-07T23:16:51.718907: step 4434, loss 0.0374423, acc 0.98
2016-09-07T23:16:52.403235: step 4435, loss 0.138267, acc 0.96
2016-09-07T23:16:53.117476: step 4436, loss 0.0143346, acc 1
2016-09-07T23:16:53.802203: step 4437, loss 0.0160638, acc 1
2016-09-07T23:16:54.499254: step 4438, loss 0.0590672, acc 0.98
2016-09-07T23:16:55.184118: step 4439, loss 0.0292034, acc 0.98
2016-09-07T23:16:55.879750: step 4440, loss 0.0129879, acc 1
2016-09-07T23:16:56.558187: step 4441, loss 0.0421051, acc 0.96
2016-09-07T23:16:57.261917: step 4442, loss 0.000572464, acc 1
2016-09-07T23:16:57.974290: step 4443, loss 0.0345551, acc 0.98
2016-09-07T23:16:58.675340: step 4444, loss 0.0289155, acc 1
2016-09-07T23:16:59.380932: step 4445, loss 0.0236912, acc 1
2016-09-07T23:17:00.098957: step 4446, loss 0.0240195, acc 0.98
2016-09-07T23:17:00.859973: step 4447, loss 0.0330372, acc 1
2016-09-07T23:17:01.576577: step 4448, loss 0.0279373, acc 1
2016-09-07T23:17:02.289206: step 4449, loss 0.0203865, acc 1
2016-09-07T23:17:02.975015: step 4450, loss 0.0166033, acc 1
2016-09-07T23:17:03.698988: step 4451, loss 0.0149614, acc 1
2016-09-07T23:17:04.397495: step 4452, loss 0.013347, acc 1
2016-09-07T23:17:05.104987: step 4453, loss 0.0437707, acc 0.98
2016-09-07T23:17:05.809894: step 4454, loss 0.0218824, acc 0.98
2016-09-07T23:17:06.503700: step 4455, loss 0.0195071, acc 1
2016-09-07T23:17:07.194848: step 4456, loss 0.00299723, acc 1
2016-09-07T23:17:07.891187: step 4457, loss 0.0410953, acc 0.98
2016-09-07T23:17:08.561830: step 4458, loss 0.0216441, acc 1
2016-09-07T23:17:09.253711: step 4459, loss 0.0537503, acc 0.96
2016-09-07T23:17:09.946280: step 4460, loss 0.00225247, acc 1
2016-09-07T23:17:10.633276: step 4461, loss 0.00211758, acc 1
2016-09-07T23:17:10.990877: step 4462, loss 0.0461006, acc 1
2016-09-07T23:17:11.715390: step 4463, loss 0.0367897, acc 0.96
2016-09-07T23:17:12.433351: step 4464, loss 0.0286091, acc 0.98
2016-09-07T23:17:13.125268: step 4465, loss 0.00299928, acc 1
2016-09-07T23:17:13.842703: step 4466, loss 0.0262441, acc 0.98
2016-09-07T23:17:14.534641: step 4467, loss 0.00141234, acc 1
2016-09-07T23:17:15.228051: step 4468, loss 0.0255097, acc 1
2016-09-07T23:17:15.932412: step 4469, loss 0.0130218, acc 1
2016-09-07T23:17:16.655790: step 4470, loss 0.0753715, acc 0.98
2016-09-07T23:17:17.345941: step 4471, loss 0.00213305, acc 1
2016-09-07T23:17:18.044272: step 4472, loss 0.000899226, acc 1
2016-09-07T23:17:18.744097: step 4473, loss 0.109125, acc 0.94
2016-09-07T23:17:19.466356: step 4474, loss 0.0724749, acc 0.96
2016-09-07T23:17:20.156309: step 4475, loss 0.083112, acc 0.94
2016-09-07T23:17:20.854062: step 4476, loss 0.0237427, acc 1
2016-09-07T23:17:21.547529: step 4477, loss 0.0796705, acc 0.98
2016-09-07T23:17:22.246820: step 4478, loss 0.0011388, acc 1
2016-09-07T23:17:22.929701: step 4479, loss 0.00451085, acc 1
2016-09-07T23:17:23.631321: step 4480, loss 0.0140751, acc 0.98
2016-09-07T23:17:24.314862: step 4481, loss 0.0131894, acc 1
2016-09-07T23:17:25.000520: step 4482, loss 0.0138065, acc 1
2016-09-07T23:17:25.686566: step 4483, loss 0.02164, acc 0.98
2016-09-07T23:17:26.385912: step 4484, loss 0.0425294, acc 0.98
2016-09-07T23:17:27.065244: step 4485, loss 0.0802094, acc 0.94
2016-09-07T23:17:27.792549: step 4486, loss 0.00827705, acc 1
2016-09-07T23:17:28.517935: step 4487, loss 0.0684037, acc 0.96
2016-09-07T23:17:29.199594: step 4488, loss 0.0223877, acc 0.98
2016-09-07T23:17:29.910384: step 4489, loss 0.00305069, acc 1
2016-09-07T23:17:30.636496: step 4490, loss 0.00746827, acc 1
2016-09-07T23:17:31.318351: step 4491, loss 0.0145881, acc 1
2016-09-07T23:17:32.030938: step 4492, loss 0.00527991, acc 1
2016-09-07T23:17:32.733780: step 4493, loss 0.00708325, acc 1
2016-09-07T23:17:33.438447: step 4494, loss 0.0748123, acc 0.96
2016-09-07T23:17:34.138938: step 4495, loss 0.000954933, acc 1
2016-09-07T23:17:34.836063: step 4496, loss 0.0310656, acc 0.98
2016-09-07T23:17:35.540086: step 4497, loss 0.0304423, acc 0.98
2016-09-07T23:17:36.240805: step 4498, loss 0.128127, acc 0.98
2016-09-07T23:17:36.960550: step 4499, loss 0.00975359, acc 1
2016-09-07T23:17:37.665821: step 4500, loss 0.0365321, acc 0.98

Evaluation:
2016-09-07T23:17:41.043477: step 4500, loss 2.11111, acc 0.739

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-4500

2016-09-07T23:17:42.884528: step 4501, loss 0.0127444, acc 1
2016-09-07T23:17:43.574834: step 4502, loss 0.00126339, acc 1
2016-09-07T23:17:44.260430: step 4503, loss 0.0598877, acc 0.96
2016-09-07T23:17:44.945979: step 4504, loss 0.00679752, acc 1
2016-09-07T23:17:45.615997: step 4505, loss 0.00519435, acc 1
2016-09-07T23:17:46.312801: step 4506, loss 0.0144321, acc 1
2016-09-07T23:17:47.018146: step 4507, loss 0.0575508, acc 0.96
2016-09-07T23:17:47.722519: step 4508, loss 0.0407936, acc 0.98
2016-09-07T23:17:48.422496: step 4509, loss 0.00800741, acc 1
2016-09-07T23:17:49.102782: step 4510, loss 0.027758, acc 1
2016-09-07T23:17:49.799026: step 4511, loss 0.00579522, acc 1
2016-09-07T23:17:50.523240: step 4512, loss 0.0738086, acc 0.96
2016-09-07T23:17:51.261496: step 4513, loss 0.0870669, acc 0.96
2016-09-07T23:17:51.961088: step 4514, loss 0.0283667, acc 1
2016-09-07T23:17:52.660908: step 4515, loss 0.00185227, acc 1
2016-09-07T23:17:53.360824: step 4516, loss 0.0562794, acc 0.98
2016-09-07T23:17:54.061241: step 4517, loss 0.00157794, acc 1
2016-09-07T23:17:54.764412: step 4518, loss 0.0152686, acc 1
2016-09-07T23:17:55.466942: step 4519, loss 0.0275642, acc 1
2016-09-07T23:17:56.170928: step 4520, loss 0.00016223, acc 1
2016-09-07T23:17:56.876894: step 4521, loss 0.0672499, acc 0.96
2016-09-07T23:17:57.594506: step 4522, loss 0.0054476, acc 1
2016-09-07T23:17:58.299638: step 4523, loss 0.0461143, acc 0.98
2016-09-07T23:17:58.998119: step 4524, loss 0.000487782, acc 1
2016-09-07T23:17:59.705234: step 4525, loss 0.0388746, acc 0.98
2016-09-07T23:18:00.480518: step 4526, loss 0.116825, acc 0.96
2016-09-07T23:18:01.163143: step 4527, loss 0.0174146, acc 1
2016-09-07T23:18:01.871771: step 4528, loss 0.0778961, acc 0.96
2016-09-07T23:18:02.578689: step 4529, loss 0.0744675, acc 0.98
2016-09-07T23:18:03.276922: step 4530, loss 0.000417272, acc 1
2016-09-07T23:18:03.978633: step 4531, loss 0.0224636, acc 0.98
2016-09-07T23:18:04.679057: step 4532, loss 0.0583839, acc 0.98
2016-09-07T23:18:05.401026: step 4533, loss 0.0153895, acc 1
2016-09-07T23:18:06.102367: step 4534, loss 0.0127668, acc 1
2016-09-07T23:18:06.796970: step 4535, loss 0.0198152, acc 0.98
2016-09-07T23:18:07.492876: step 4536, loss 0.0351596, acc 0.98
2016-09-07T23:18:08.201309: step 4537, loss 0.0379627, acc 0.98
2016-09-07T23:18:08.906909: step 4538, loss 0.0453415, acc 0.96
2016-09-07T23:18:09.597092: step 4539, loss 0.0267576, acc 0.98
2016-09-07T23:18:10.298910: step 4540, loss 0.0112504, acc 1
2016-09-07T23:18:10.997793: step 4541, loss 0.0447587, acc 0.98
2016-09-07T23:18:11.685087: step 4542, loss 0.0341412, acc 0.98
2016-09-07T23:18:12.385463: step 4543, loss 0.0171219, acc 1
2016-09-07T23:18:13.105159: step 4544, loss 0.0362135, acc 0.98
2016-09-07T23:18:13.815935: step 4545, loss 0.00220389, acc 1
2016-09-07T23:18:14.516001: step 4546, loss 0.0368169, acc 0.96
2016-09-07T23:18:15.201123: step 4547, loss 0.0266887, acc 0.98
2016-09-07T23:18:15.896312: step 4548, loss 0.00435143, acc 1
2016-09-07T23:18:16.594638: step 4549, loss 0.0934654, acc 0.98
2016-09-07T23:18:17.284276: step 4550, loss 0.0681153, acc 0.98
2016-09-07T23:18:17.983314: step 4551, loss 0.0808588, acc 0.94
2016-09-07T23:18:18.689023: step 4552, loss 0.0123456, acc 1
2016-09-07T23:18:19.376672: step 4553, loss 0.0362545, acc 0.98
2016-09-07T23:18:20.077401: step 4554, loss 0.0616737, acc 0.98
2016-09-07T23:18:20.770815: step 4555, loss 0.0521046, acc 0.98
2016-09-07T23:18:21.473595: step 4556, loss 0.0204739, acc 0.98
2016-09-07T23:18:22.162609: step 4557, loss 0.0363032, acc 0.98
2016-09-07T23:18:22.860632: step 4558, loss 0.0252905, acc 0.98
2016-09-07T23:18:23.544038: step 4559, loss 0.0267342, acc 0.98
2016-09-07T23:18:24.233110: step 4560, loss 0.0712221, acc 0.94
2016-09-07T23:18:24.927544: step 4561, loss 0.0307484, acc 0.98
2016-09-07T23:18:25.615942: step 4562, loss 0.0490151, acc 0.94
2016-09-07T23:18:26.328008: step 4563, loss 0.0439721, acc 0.96
2016-09-07T23:18:27.051869: step 4564, loss 0.03029, acc 0.98
2016-09-07T23:18:27.757530: step 4565, loss 0.0207055, acc 0.98
2016-09-07T23:18:28.475910: step 4566, loss 0.0170523, acc 1
2016-09-07T23:18:29.165500: step 4567, loss 0.112148, acc 0.94
2016-09-07T23:18:29.891831: step 4568, loss 0.0142594, acc 1
2016-09-07T23:18:30.586381: step 4569, loss 0.0510574, acc 0.96
2016-09-07T23:18:31.281640: step 4570, loss 0.0459978, acc 0.98
2016-09-07T23:18:31.976594: step 4571, loss 0.00150151, acc 1
2016-09-07T23:18:32.680687: step 4572, loss 0.0294259, acc 0.98
2016-09-07T23:18:33.377750: step 4573, loss 0.0928533, acc 0.98
2016-09-07T23:18:34.078729: step 4574, loss 0.0181913, acc 0.98
2016-09-07T23:18:34.784110: step 4575, loss 0.0143788, acc 1
2016-09-07T23:18:35.500503: step 4576, loss 0.0858923, acc 0.96
2016-09-07T23:18:36.194827: step 4577, loss 0.00567651, acc 1
2016-09-07T23:18:36.926528: step 4578, loss 0.00961171, acc 1
2016-09-07T23:18:37.611062: step 4579, loss 0.0246202, acc 0.98
2016-09-07T23:18:38.316845: step 4580, loss 0.0152121, acc 1
2016-09-07T23:18:39.008447: step 4581, loss 0.105781, acc 0.98
2016-09-07T23:18:39.713428: step 4582, loss 0.0748384, acc 0.94
2016-09-07T23:18:40.411150: step 4583, loss 0.0738212, acc 0.96
2016-09-07T23:18:41.104876: step 4584, loss 0.0707663, acc 0.96
2016-09-07T23:18:41.813011: step 4585, loss 0.00176351, acc 1
2016-09-07T23:18:42.525554: step 4586, loss 0.00725476, acc 1
2016-09-07T23:18:43.267590: step 4587, loss 0.0504462, acc 0.98
2016-09-07T23:18:43.977346: step 4588, loss 0.0138461, acc 1
2016-09-07T23:18:44.691649: step 4589, loss 0.0223224, acc 0.98
2016-09-07T23:18:45.389546: step 4590, loss 0.0454804, acc 0.98
2016-09-07T23:18:46.105025: step 4591, loss 0.000936362, acc 1
2016-09-07T23:18:46.804452: step 4592, loss 0.00760885, acc 1
2016-09-07T23:18:47.507073: step 4593, loss 0.0533035, acc 0.98
2016-09-07T23:18:48.213642: step 4594, loss 0.0160512, acc 1
2016-09-07T23:18:48.890510: step 4595, loss 0.00821224, acc 1
2016-09-07T23:18:49.586063: step 4596, loss 0.0284074, acc 1
2016-09-07T23:18:50.278577: step 4597, loss 0.0298149, acc 1
2016-09-07T23:18:50.969566: step 4598, loss 0.0218747, acc 1
2016-09-07T23:18:51.672500: step 4599, loss 0.000703101, acc 1
2016-09-07T23:18:52.361572: step 4600, loss 0.0292502, acc 0.98

Evaluation:
2016-09-07T23:18:55.749113: step 4600, loss 2.41267, acc 0.732

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-4600

2016-09-07T23:18:57.459484: step 4601, loss 0.0118504, acc 1
2016-09-07T23:18:58.151522: step 4602, loss 0.100394, acc 0.94
2016-09-07T23:18:58.857262: step 4603, loss 0.0418511, acc 0.98
2016-09-07T23:18:59.539072: step 4604, loss 0.010383, acc 1
2016-09-07T23:19:00.255770: step 4605, loss 0.00967289, acc 1
2016-09-07T23:19:00.942668: step 4606, loss 0.0190509, acc 1
2016-09-07T23:19:01.640695: step 4607, loss 0.0889983, acc 0.94
2016-09-07T23:19:02.336237: step 4608, loss 0.0201758, acc 0.98
2016-09-07T23:19:03.048840: step 4609, loss 0.059203, acc 0.98
2016-09-07T23:19:03.743071: step 4610, loss 0.0593688, acc 0.98
2016-09-07T23:19:04.442905: step 4611, loss 0.0531198, acc 0.98
2016-09-07T23:19:05.146742: step 4612, loss 0.0122117, acc 1
2016-09-07T23:19:05.830307: step 4613, loss 0.0188316, acc 0.98
2016-09-07T23:19:06.542563: step 4614, loss 0.0214752, acc 1
2016-09-07T23:19:07.241842: step 4615, loss 0.0214717, acc 1
2016-09-07T23:19:07.949450: step 4616, loss 0.0265677, acc 1
2016-09-07T23:19:08.663424: step 4617, loss 0.0601755, acc 0.96
2016-09-07T23:19:09.370979: step 4618, loss 0.000384719, acc 1
2016-09-07T23:19:10.097223: step 4619, loss 0.00583946, acc 1
2016-09-07T23:19:10.790650: step 4620, loss 0.0644742, acc 0.96
2016-09-07T23:19:11.486061: step 4621, loss 0.000547471, acc 1
2016-09-07T23:19:12.193577: step 4622, loss 0.0116552, acc 1
2016-09-07T23:19:12.888773: step 4623, loss 0.0409162, acc 0.98
2016-09-07T23:19:13.562418: step 4624, loss 0.0278053, acc 0.98
2016-09-07T23:19:14.270914: step 4625, loss 0.0183116, acc 1
2016-09-07T23:19:14.973851: step 4626, loss 0.0120772, acc 1
2016-09-07T23:19:15.663862: step 4627, loss 0.0142761, acc 1
2016-09-07T23:19:16.377558: step 4628, loss 0.0112135, acc 1
2016-09-07T23:19:17.102119: step 4629, loss 0.00615538, acc 1
2016-09-07T23:19:17.801090: step 4630, loss 0.0302971, acc 0.98
2016-09-07T23:19:18.515085: step 4631, loss 0.0977552, acc 0.98
2016-09-07T23:19:19.237462: step 4632, loss 0.0260371, acc 1
2016-09-07T23:19:19.954161: step 4633, loss 0.00165481, acc 1
2016-09-07T23:19:20.657350: step 4634, loss 0.054608, acc 0.98
2016-09-07T23:19:21.359386: step 4635, loss 0.0183198, acc 1
2016-09-07T23:19:22.049187: step 4636, loss 0.0297224, acc 1
2016-09-07T23:19:22.750016: step 4637, loss 0.198422, acc 0.96
2016-09-07T23:19:23.444224: step 4638, loss 0.00122957, acc 1
2016-09-07T23:19:24.139967: step 4639, loss 0.0260143, acc 0.98
2016-09-07T23:19:24.842348: step 4640, loss 0.0345387, acc 0.98
2016-09-07T23:19:25.569556: step 4641, loss 0.0416197, acc 0.96
2016-09-07T23:19:26.270971: step 4642, loss 0.0500085, acc 0.98
2016-09-07T23:19:26.990170: step 4643, loss 0.0167037, acc 1
2016-09-07T23:19:27.702334: step 4644, loss 0.049638, acc 0.96
2016-09-07T23:19:28.422999: step 4645, loss 0.00258275, acc 1
2016-09-07T23:19:29.132521: step 4646, loss 0.00190305, acc 1
2016-09-07T23:19:29.831520: step 4647, loss 0.0235226, acc 0.98
2016-09-07T23:19:30.525670: step 4648, loss 0.0489962, acc 0.98
2016-09-07T23:19:31.218789: step 4649, loss 0.0300423, acc 0.98
2016-09-07T23:19:31.911774: step 4650, loss 0.012727, acc 1
2016-09-07T23:19:32.609399: step 4651, loss 0.0175317, acc 1
2016-09-07T23:19:33.303003: step 4652, loss 0.0331795, acc 0.98
2016-09-07T23:19:34.010269: step 4653, loss 0.0137202, acc 1
2016-09-07T23:19:34.730676: step 4654, loss 0.0377398, acc 0.98
2016-09-07T23:19:35.432935: step 4655, loss 0.00504361, acc 1
2016-09-07T23:19:35.805230: step 4656, loss 0.0202168, acc 1
2016-09-07T23:19:36.514501: step 4657, loss 0.0135689, acc 1
2016-09-07T23:19:37.210386: step 4658, loss 0.0349722, acc 0.98
2016-09-07T23:19:37.919286: step 4659, loss 0.00122104, acc 1
2016-09-07T23:19:38.629474: step 4660, loss 0.0732669, acc 0.96
2016-09-07T23:19:39.324037: step 4661, loss 0.0418868, acc 0.98
2016-09-07T23:19:40.007082: step 4662, loss 0.0167972, acc 1
2016-09-07T23:19:40.687389: step 4663, loss 0.032247, acc 0.98
2016-09-07T23:19:41.415858: step 4664, loss 0.0212533, acc 0.98
2016-09-07T23:19:42.110388: step 4665, loss 0.00133396, acc 1
2016-09-07T23:19:42.811466: step 4666, loss 0.0352531, acc 1
2016-09-07T23:19:43.510368: step 4667, loss 0.0260427, acc 0.98
2016-09-07T23:19:44.208284: step 4668, loss 0.0125186, acc 1
2016-09-07T23:19:44.907591: step 4669, loss 0.051575, acc 0.98
2016-09-07T23:19:45.593592: step 4670, loss 0.0524599, acc 0.98
2016-09-07T23:19:46.296526: step 4671, loss 0.0100343, acc 1
2016-09-07T23:19:46.988332: step 4672, loss 0.045533, acc 0.96
2016-09-07T23:19:47.681634: step 4673, loss 0.0387668, acc 0.98
2016-09-07T23:19:48.380552: step 4674, loss 0.0321029, acc 0.98
2016-09-07T23:19:49.100865: step 4675, loss 0.0365502, acc 0.98
2016-09-07T23:19:49.799400: step 4676, loss 0.0556222, acc 0.98
2016-09-07T23:19:50.497762: step 4677, loss 0.0075905, acc 1
2016-09-07T23:19:51.196002: step 4678, loss 0.0116288, acc 1
2016-09-07T23:19:51.895244: step 4679, loss 0.00464892, acc 1
2016-09-07T23:19:52.591044: step 4680, loss 0.0632443, acc 0.96
2016-09-07T23:19:53.286736: step 4681, loss 0.0021795, acc 1
2016-09-07T23:19:53.987586: step 4682, loss 0.0154226, acc 1
2016-09-07T23:19:54.690229: step 4683, loss 0.0186039, acc 0.98
2016-09-07T23:19:55.409280: step 4684, loss 0.0223312, acc 0.98
2016-09-07T23:19:56.145721: step 4685, loss 0.0468138, acc 0.96
2016-09-07T23:19:56.836718: step 4686, loss 0.000269215, acc 1
2016-09-07T23:19:57.537342: step 4687, loss 0.030108, acc 0.98
2016-09-07T23:19:58.258539: step 4688, loss 0.0617134, acc 0.96
2016-09-07T23:19:58.977839: step 4689, loss 0.0189658, acc 1
2016-09-07T23:19:59.656627: step 4690, loss 0.0502199, acc 0.98
2016-09-07T23:20:00.420379: step 4691, loss 0.00295491, acc 1
2016-09-07T23:20:01.132907: step 4692, loss 0.00174138, acc 1
2016-09-07T23:20:01.862896: step 4693, loss 0.0242969, acc 0.98
2016-09-07T23:20:02.550213: step 4694, loss 0.0232667, acc 0.98
2016-09-07T23:20:03.248450: step 4695, loss 0.00204973, acc 1
2016-09-07T23:20:03.952511: step 4696, loss 0.000496438, acc 1
2016-09-07T23:20:04.641104: step 4697, loss 0.0295898, acc 1
2016-09-07T23:20:05.350131: step 4698, loss 0.127973, acc 0.96
2016-09-07T23:20:06.034182: step 4699, loss 0.00809091, acc 1
2016-09-07T23:20:06.728107: step 4700, loss 0.0192642, acc 0.98

Evaluation:
2016-09-07T23:20:10.083890: step 4700, loss 2.28057, acc 0.748

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-4700

2016-09-07T23:20:11.875033: step 4701, loss 0.0459957, acc 0.98
2016-09-07T23:20:12.580987: step 4702, loss 0.0164647, acc 1
2016-09-07T23:20:13.305538: step 4703, loss 0.134811, acc 0.96
2016-09-07T23:20:14.026650: step 4704, loss 0.0242255, acc 0.98
2016-09-07T23:20:14.730495: step 4705, loss 0.00188635, acc 1
2016-09-07T23:20:15.428568: step 4706, loss 0.0168577, acc 1
2016-09-07T23:20:16.169133: step 4707, loss 0.0489338, acc 0.96
2016-09-07T23:20:16.875948: step 4708, loss 0.0342389, acc 0.98
2016-09-07T23:20:17.586033: step 4709, loss 0.0135746, acc 1
2016-09-07T23:20:18.279244: step 4710, loss 0.0422362, acc 0.98
2016-09-07T23:20:18.967690: step 4711, loss 0.0232721, acc 1
2016-09-07T23:20:19.654735: step 4712, loss 0.00859454, acc 1
2016-09-07T23:20:20.382220: step 4713, loss 0.299672, acc 0.96
2016-09-07T23:20:21.084315: step 4714, loss 0.00703056, acc 1
2016-09-07T23:20:21.772630: step 4715, loss 0.000902231, acc 1
2016-09-07T23:20:22.473421: step 4716, loss 0.0106391, acc 1
2016-09-07T23:20:23.168563: step 4717, loss 0.0246038, acc 1
2016-09-07T23:20:23.842253: step 4718, loss 0.0195114, acc 0.98
2016-09-07T23:20:24.552388: step 4719, loss 0.051721, acc 0.96
2016-09-07T23:20:25.256055: step 4720, loss 0.0123655, acc 1
2016-09-07T23:20:25.959384: step 4721, loss 0.0448762, acc 0.94
2016-09-07T23:20:26.655656: step 4722, loss 0.00778275, acc 1
2016-09-07T23:20:27.354504: step 4723, loss 0.104979, acc 0.96
2016-09-07T23:20:28.054885: step 4724, loss 0.0525775, acc 0.98
2016-09-07T23:20:28.763538: step 4725, loss 0.0184955, acc 0.98
2016-09-07T23:20:29.471647: step 4726, loss 0.0272652, acc 0.98
2016-09-07T23:20:30.167094: step 4727, loss 0.0130676, acc 1
2016-09-07T23:20:30.872990: step 4728, loss 0.0780031, acc 0.94
2016-09-07T23:20:31.579794: step 4729, loss 0.00351182, acc 1
2016-09-07T23:20:32.269638: step 4730, loss 0.000766372, acc 1
2016-09-07T23:20:32.981108: step 4731, loss 0.0217639, acc 0.98
2016-09-07T23:20:33.677125: step 4732, loss 0.0472125, acc 0.96
2016-09-07T23:20:34.378281: step 4733, loss 0.00287277, acc 1
2016-09-07T23:20:35.074716: step 4734, loss 0.00503817, acc 1
2016-09-07T23:20:35.764264: step 4735, loss 0.0363663, acc 0.98
2016-09-07T23:20:36.462561: step 4736, loss 0.0457082, acc 0.96
2016-09-07T23:20:37.150149: step 4737, loss 0.0157327, acc 1
2016-09-07T23:20:37.843991: step 4738, loss 0.0146167, acc 1
2016-09-07T23:20:38.544062: step 4739, loss 0.0816485, acc 0.96
2016-09-07T23:20:39.238607: step 4740, loss 0.0276314, acc 0.98
2016-09-07T23:20:39.931640: step 4741, loss 0.00882791, acc 1
2016-09-07T23:20:40.622546: step 4742, loss 0.000103084, acc 1
2016-09-07T23:20:41.324539: step 4743, loss 0.00297696, acc 1
2016-09-07T23:20:42.045183: step 4744, loss 0.0585035, acc 0.98
2016-09-07T23:20:42.751185: step 4745, loss 0.0537313, acc 0.96
2016-09-07T23:20:43.441086: step 4746, loss 0.0167338, acc 1
2016-09-07T23:20:44.144089: step 4747, loss 0.00576754, acc 1
2016-09-07T23:20:44.843221: step 4748, loss 0.0343219, acc 0.98
2016-09-07T23:20:45.543039: step 4749, loss 0.0719671, acc 0.98
2016-09-07T23:20:46.250432: step 4750, loss 0.00239592, acc 1
2016-09-07T23:20:46.953464: step 4751, loss 0.0937705, acc 0.96
2016-09-07T23:20:47.667637: step 4752, loss 0.0145683, acc 1
2016-09-07T23:20:48.376172: step 4753, loss 0.024568, acc 1
2016-09-07T23:20:49.085422: step 4754, loss 0.0144169, acc 1
2016-09-07T23:20:49.791703: step 4755, loss 0.0329399, acc 0.98
2016-09-07T23:20:50.489693: step 4756, loss 0.0484824, acc 0.98
2016-09-07T23:20:51.185061: step 4757, loss 0.00634128, acc 1
2016-09-07T23:20:51.876882: step 4758, loss 0.0124359, acc 1
2016-09-07T23:20:52.595178: step 4759, loss 0.0528848, acc 0.94
2016-09-07T23:20:53.303651: step 4760, loss 0.0108105, acc 1
2016-09-07T23:20:54.004633: step 4761, loss 0.0011305, acc 1
2016-09-07T23:20:54.696651: step 4762, loss 0.023197, acc 1
2016-09-07T23:20:55.389361: step 4763, loss 0.0242849, acc 1
2016-09-07T23:20:56.087194: step 4764, loss 0.02763, acc 0.98
2016-09-07T23:20:56.800943: step 4765, loss 0.00630785, acc 1
2016-09-07T23:20:57.489816: step 4766, loss 0.0120095, acc 1
2016-09-07T23:20:58.182012: step 4767, loss 0.050904, acc 0.96
2016-09-07T23:20:58.891773: step 4768, loss 0.0577122, acc 0.98
2016-09-07T23:20:59.630559: step 4769, loss 0.0489454, acc 0.98
2016-09-07T23:21:00.362774: step 4770, loss 0.0518878, acc 0.98
2016-09-07T23:21:01.053793: step 4771, loss 0.00391586, acc 1
2016-09-07T23:21:01.759637: step 4772, loss 0.0960352, acc 0.96
2016-09-07T23:21:02.478180: step 4773, loss 0.0219899, acc 1
2016-09-07T23:21:03.184037: step 4774, loss 0.0802779, acc 0.94
2016-09-07T23:21:03.876024: step 4775, loss 0.0204692, acc 0.98
2016-09-07T23:21:04.583029: step 4776, loss 0.0202875, acc 0.98
2016-09-07T23:21:05.282889: step 4777, loss 0.0658056, acc 0.96
2016-09-07T23:21:05.979979: step 4778, loss 0.00278744, acc 1
2016-09-07T23:21:06.683691: step 4779, loss 0.0157057, acc 1
2016-09-07T23:21:07.389912: step 4780, loss 0.00711443, acc 1
2016-09-07T23:21:08.100582: step 4781, loss 0.0227107, acc 1
2016-09-07T23:21:08.803985: step 4782, loss 0.0246175, acc 0.98
2016-09-07T23:21:09.505889: step 4783, loss 0.0223925, acc 0.98
2016-09-07T23:21:10.204801: step 4784, loss 0.000196137, acc 1
2016-09-07T23:21:10.900852: step 4785, loss 0.00261978, acc 1
2016-09-07T23:21:11.591190: step 4786, loss 0.0515146, acc 1
2016-09-07T23:21:12.303725: step 4787, loss 0.0319166, acc 0.98
2016-09-07T23:21:12.996973: step 4788, loss 0.260396, acc 0.9
2016-09-07T23:21:13.697268: step 4789, loss 0.0809777, acc 0.98
2016-09-07T23:21:14.383646: step 4790, loss 0.0351367, acc 0.96
2016-09-07T23:21:15.091256: step 4791, loss 0.0646424, acc 0.96
2016-09-07T23:21:15.792057: step 4792, loss 0.00389087, acc 1
2016-09-07T23:21:16.475836: step 4793, loss 0.0331601, acc 0.98
2016-09-07T23:21:17.177908: step 4794, loss 0.00089465, acc 1
2016-09-07T23:21:17.869163: step 4795, loss 0.00380233, acc 1
2016-09-07T23:21:18.560085: step 4796, loss 0.0443433, acc 0.98
2016-09-07T23:21:19.257698: step 4797, loss 0.0963136, acc 0.96
2016-09-07T23:21:19.981436: step 4798, loss 0.0206819, acc 1
2016-09-07T23:21:20.691045: step 4799, loss 0.138796, acc 0.96
2016-09-07T23:21:21.389548: step 4800, loss 0.000359023, acc 1

Evaluation:
2016-09-07T23:21:24.776303: step 4800, loss 1.98224, acc 0.733

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-4800

2016-09-07T23:21:26.468241: step 4801, loss 0.18127, acc 0.96
2016-09-07T23:21:27.162628: step 4802, loss 0.039838, acc 1
2016-09-07T23:21:27.869980: step 4803, loss 0.0595311, acc 0.96
2016-09-07T23:21:28.574139: step 4804, loss 0.0214178, acc 1
2016-09-07T23:21:29.270541: step 4805, loss 0.00473418, acc 1
2016-09-07T23:21:29.971479: step 4806, loss 0.00491145, acc 1
2016-09-07T23:21:30.678379: step 4807, loss 0.0237827, acc 0.98
2016-09-07T23:21:31.386293: step 4808, loss 0.0437272, acc 0.98
2016-09-07T23:21:32.072472: step 4809, loss 0.092724, acc 0.98
2016-09-07T23:21:32.769943: step 4810, loss 0.0698313, acc 0.96
2016-09-07T23:21:33.464312: step 4811, loss 0.0222527, acc 1
2016-09-07T23:21:34.172882: step 4812, loss 0.0166021, acc 1
2016-09-07T23:21:34.887428: step 4813, loss 0.0149708, acc 1
2016-09-07T23:21:35.588187: step 4814, loss 0.0404343, acc 0.98
2016-09-07T23:21:36.293201: step 4815, loss 0.0113273, acc 1
2016-09-07T23:21:36.975615: step 4816, loss 0.00710935, acc 1
2016-09-07T23:21:37.682012: step 4817, loss 0.023568, acc 1
2016-09-07T23:21:38.383168: step 4818, loss 0.049214, acc 0.96
2016-09-07T23:21:39.088104: step 4819, loss 0.0357871, acc 0.98
2016-09-07T23:21:39.782517: step 4820, loss 0.0251342, acc 0.98
2016-09-07T23:21:40.480107: step 4821, loss 0.00838692, acc 1
2016-09-07T23:21:41.192574: step 4822, loss 0.0588082, acc 0.96
2016-09-07T23:21:41.909926: step 4823, loss 0.0685786, acc 0.98
2016-09-07T23:21:42.644810: step 4824, loss 0.0430641, acc 0.98
2016-09-07T23:21:43.330913: step 4825, loss 0.0229915, acc 0.98
2016-09-07T23:21:44.046110: step 4826, loss 0.0355978, acc 1
2016-09-07T23:21:44.734769: step 4827, loss 0.0171407, acc 1
2016-09-07T23:21:45.442187: step 4828, loss 0.0175912, acc 1
2016-09-07T23:21:46.139518: step 4829, loss 0.0177066, acc 1
2016-09-07T23:21:46.844365: step 4830, loss 0.00665785, acc 1
2016-09-07T23:21:47.535860: step 4831, loss 0.024815, acc 0.98
2016-09-07T23:21:48.277195: step 4832, loss 0.0768975, acc 0.96
2016-09-07T23:21:48.981895: step 4833, loss 0.00209164, acc 1
2016-09-07T23:21:49.676332: step 4834, loss 0.00657885, acc 1
2016-09-07T23:21:50.394739: step 4835, loss 0.0357037, acc 0.98
2016-09-07T23:21:51.092789: step 4836, loss 0.0382516, acc 0.96
2016-09-07T23:21:51.795603: step 4837, loss 0.0517588, acc 0.94
2016-09-07T23:21:52.487989: step 4838, loss 0.00189967, acc 1
2016-09-07T23:21:53.184234: step 4839, loss 0.0130661, acc 1
2016-09-07T23:21:53.905399: step 4840, loss 0.00602024, acc 1
2016-09-07T23:21:54.621399: step 4841, loss 0.0322538, acc 0.98
2016-09-07T23:21:55.319164: step 4842, loss 0.0174526, acc 1
2016-09-07T23:21:56.038895: step 4843, loss 0.0456457, acc 0.98
2016-09-07T23:21:56.741321: step 4844, loss 0.0118901, acc 1
2016-09-07T23:21:57.461391: step 4845, loss 0.0172169, acc 1
2016-09-07T23:21:58.164132: step 4846, loss 0.156619, acc 0.96
2016-09-07T23:21:58.870251: step 4847, loss 0.0122523, acc 1
2016-09-07T23:21:59.590696: step 4848, loss 0.0156739, acc 1
2016-09-07T23:22:00.333027: step 4849, loss 0.025947, acc 0.98
2016-09-07T23:22:00.704266: step 4850, loss 0.138353, acc 0.916667
2016-09-07T23:22:01.407060: step 4851, loss 0.0843988, acc 0.96
2016-09-07T23:22:02.124846: step 4852, loss 0.070263, acc 0.98
2016-09-07T23:22:02.812572: step 4853, loss 0.0459438, acc 0.96
2016-09-07T23:22:03.523900: step 4854, loss 0.023565, acc 1
2016-09-07T23:22:04.200998: step 4855, loss 0.0180008, acc 1
2016-09-07T23:22:04.893320: step 4856, loss 0.0423569, acc 0.98
2016-09-07T23:22:05.579675: step 4857, loss 0.00287011, acc 1
2016-09-07T23:22:06.282737: step 4858, loss 0.0637298, acc 0.98
2016-09-07T23:22:06.980626: step 4859, loss 0.0319387, acc 0.98
2016-09-07T23:22:07.669088: step 4860, loss 0.0118876, acc 1
2016-09-07T23:22:08.376358: step 4861, loss 0.034957, acc 0.96
2016-09-07T23:22:09.082672: step 4862, loss 0.0333471, acc 0.98
2016-09-07T23:22:09.782354: step 4863, loss 0.0441567, acc 0.96
2016-09-07T23:22:10.497204: step 4864, loss 0.0315941, acc 0.98
2016-09-07T23:22:11.201351: step 4865, loss 0.00603766, acc 1
2016-09-07T23:22:11.899482: step 4866, loss 0.0481707, acc 0.98
2016-09-07T23:22:12.595422: step 4867, loss 0.00118057, acc 1
2016-09-07T23:22:13.295426: step 4868, loss 0.00374867, acc 1
2016-09-07T23:22:13.983528: step 4869, loss 0.0598455, acc 0.96
2016-09-07T23:22:14.675826: step 4870, loss 0.0119168, acc 1
2016-09-07T23:22:15.381479: step 4871, loss 0.1044, acc 0.96
2016-09-07T23:22:16.094259: step 4872, loss 0.0152727, acc 1
2016-09-07T23:22:16.794539: step 4873, loss 0.00938858, acc 1
2016-09-07T23:22:17.484699: step 4874, loss 0.0718111, acc 0.98
2016-09-07T23:22:18.194493: step 4875, loss 0.00827572, acc 1
2016-09-07T23:22:18.880841: step 4876, loss 0.00341276, acc 1
2016-09-07T23:22:19.555016: step 4877, loss 0.00411493, acc 1
2016-09-07T23:22:20.256037: step 4878, loss 0.0547681, acc 0.96
2016-09-07T23:22:20.981439: step 4879, loss 0.00802967, acc 1
2016-09-07T23:22:21.688580: step 4880, loss 0.0496147, acc 0.96
2016-09-07T23:22:22.380559: step 4881, loss 0.0208973, acc 1
2016-09-07T23:22:23.098186: step 4882, loss 0.023554, acc 1
2016-09-07T23:22:23.795505: step 4883, loss 0.022263, acc 1
2016-09-07T23:22:24.496594: step 4884, loss 0.0133213, acc 1
2016-09-07T23:22:25.201096: step 4885, loss 0.0182914, acc 1
2016-09-07T23:22:25.892690: step 4886, loss 0.0173591, acc 0.98
2016-09-07T23:22:26.592744: step 4887, loss 0.00866372, acc 1
2016-09-07T23:22:27.290411: step 4888, loss 0.052034, acc 0.98
2016-09-07T23:22:28.006899: step 4889, loss 0.000208686, acc 1
2016-09-07T23:22:28.705759: step 4890, loss 0.0126156, acc 1
2016-09-07T23:22:29.432795: step 4891, loss 0.0263863, acc 1
2016-09-07T23:22:30.133883: step 4892, loss 0.0255075, acc 0.98
2016-09-07T23:22:30.817614: step 4893, loss 0.00856447, acc 1
2016-09-07T23:22:31.512277: step 4894, loss 0.00148346, acc 1
2016-09-07T23:22:32.183859: step 4895, loss 0.0278113, acc 1
2016-09-07T23:22:32.873869: step 4896, loss 0.0217732, acc 1
2016-09-07T23:22:33.607260: step 4897, loss 0.0356453, acc 0.98
2016-09-07T23:22:34.303072: step 4898, loss 0.0899384, acc 0.98
2016-09-07T23:22:35.013561: step 4899, loss 0.129525, acc 0.98
2016-09-07T23:22:35.738663: step 4900, loss 0.0284093, acc 0.98

Evaluation:
2016-09-07T23:22:39.179240: step 4900, loss 2.46099, acc 0.737

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-4900

2016-09-07T23:22:40.991642: step 4901, loss 0.0101756, acc 1
2016-09-07T23:22:41.706108: step 4902, loss 0.0297297, acc 0.98
2016-09-07T23:22:42.401318: step 4903, loss 0.0457868, acc 0.98
2016-09-07T23:22:43.077216: step 4904, loss 0.0114186, acc 1
2016-09-07T23:22:43.759431: step 4905, loss 0.0290603, acc 0.98
2016-09-07T23:22:44.451461: step 4906, loss 0.158269, acc 0.96
2016-09-07T23:22:45.156131: step 4907, loss 0.00470976, acc 1
2016-09-07T23:22:45.872213: step 4908, loss 0.000319751, acc 1
2016-09-07T23:22:46.589206: step 4909, loss 0.000262469, acc 1
2016-09-07T23:22:47.293253: step 4910, loss 0.00542605, acc 1
2016-09-07T23:22:47.982573: step 4911, loss 0.0145303, acc 1
2016-09-07T23:22:48.695768: step 4912, loss 0.015492, acc 0.98
2016-09-07T23:22:49.378723: step 4913, loss 0.0503362, acc 0.98
2016-09-07T23:22:50.078346: step 4914, loss 0.134869, acc 0.96
2016-09-07T23:22:50.785596: step 4915, loss 0.0527684, acc 0.98
2016-09-07T23:22:51.491209: step 4916, loss 0.0497182, acc 0.96
2016-09-07T23:22:52.192077: step 4917, loss 0.0120929, acc 1
2016-09-07T23:22:52.902357: step 4918, loss 0.148812, acc 0.96
2016-09-07T23:22:53.614236: step 4919, loss 0.0192464, acc 1
2016-09-07T23:22:54.333034: step 4920, loss 0.0914221, acc 0.98
2016-09-07T23:22:55.048912: step 4921, loss 0.014657, acc 1
2016-09-07T23:22:55.752765: step 4922, loss 0.0256064, acc 0.98
2016-09-07T23:22:56.455469: step 4923, loss 0.00484362, acc 1
2016-09-07T23:22:57.179765: step 4924, loss 0.0522526, acc 0.96
2016-09-07T23:22:57.880371: step 4925, loss 0.00678632, acc 1
2016-09-07T23:22:58.592660: step 4926, loss 0.0679635, acc 0.94
2016-09-07T23:22:59.288272: step 4927, loss 0.0402168, acc 0.98
2016-09-07T23:22:59.996370: step 4928, loss 0.0223674, acc 1
2016-09-07T23:23:00.728430: step 4929, loss 0.0150757, acc 1
2016-09-07T23:23:01.444515: step 4930, loss 0.0206108, acc 0.98
2016-09-07T23:23:02.151531: step 4931, loss 0.0467845, acc 1
2016-09-07T23:23:02.871308: step 4932, loss 0.0239513, acc 0.98
2016-09-07T23:23:03.562826: step 4933, loss 0.00938347, acc 1
2016-09-07T23:23:04.279088: step 4934, loss 0.0219587, acc 1
2016-09-07T23:23:04.978527: step 4935, loss 0.0408968, acc 0.98
2016-09-07T23:23:05.680434: step 4936, loss 0.0190046, acc 1
2016-09-07T23:23:06.382647: step 4937, loss 0.0835419, acc 0.98
2016-09-07T23:23:07.095803: step 4938, loss 0.0491599, acc 0.98
2016-09-07T23:23:07.798967: step 4939, loss 0.0126875, acc 1
2016-09-07T23:23:08.503286: step 4940, loss 0.0429484, acc 1
2016-09-07T23:23:09.241661: step 4941, loss 0.00598076, acc 1
2016-09-07T23:23:09.937494: step 4942, loss 0.00473204, acc 1
2016-09-07T23:23:10.638215: step 4943, loss 0.00797973, acc 1
2016-09-07T23:23:11.334675: step 4944, loss 0.0275801, acc 0.98
2016-09-07T23:23:12.040544: step 4945, loss 0.00299095, acc 1
2016-09-07T23:23:12.728288: step 4946, loss 0.0198256, acc 0.98
2016-09-07T23:23:13.454767: step 4947, loss 0.0138477, acc 1
2016-09-07T23:23:14.161432: step 4948, loss 0.0538234, acc 0.98
2016-09-07T23:23:14.875661: step 4949, loss 0.0109863, acc 1
2016-09-07T23:23:15.593460: step 4950, loss 0.0202191, acc 0.98
2016-09-07T23:23:16.312019: step 4951, loss 0.00404403, acc 1
2016-09-07T23:23:17.025826: step 4952, loss 0.00298817, acc 1
2016-09-07T23:23:17.727030: step 4953, loss 0.0173228, acc 0.98
2016-09-07T23:23:18.435804: step 4954, loss 0.0149447, acc 1
2016-09-07T23:23:19.139442: step 4955, loss 0.0661458, acc 0.98
2016-09-07T23:23:19.846774: step 4956, loss 0.0204162, acc 0.98
2016-09-07T23:23:20.536873: step 4957, loss 0.0199255, acc 1
2016-09-07T23:23:21.247839: step 4958, loss 0.0147137, acc 0.98
2016-09-07T23:23:21.957372: step 4959, loss 0.0105033, acc 1
2016-09-07T23:23:22.692949: step 4960, loss 0.0199085, acc 1
2016-09-07T23:23:23.407365: step 4961, loss 0.0771387, acc 0.96
2016-09-07T23:23:24.091965: step 4962, loss 0.000331536, acc 1
2016-09-07T23:23:24.796460: step 4963, loss 0.00152476, acc 1
2016-09-07T23:23:25.486859: step 4964, loss 0.010995, acc 1
2016-09-07T23:23:26.200345: step 4965, loss 0.00459639, acc 1
2016-09-07T23:23:26.889563: step 4966, loss 0.00778396, acc 1
2016-09-07T23:23:27.579212: step 4967, loss 0.0112494, acc 1
2016-09-07T23:23:28.270784: step 4968, loss 0.0287686, acc 0.98
2016-09-07T23:23:28.960088: step 4969, loss 0.0362598, acc 0.98
2016-09-07T23:23:29.666819: step 4970, loss 0.0489124, acc 0.96
2016-09-07T23:23:30.366897: step 4971, loss 0.0048431, acc 1
2016-09-07T23:23:31.066833: step 4972, loss 0.0389669, acc 0.98
2016-09-07T23:23:31.766057: step 4973, loss 0.00759497, acc 1
2016-09-07T23:23:32.458187: step 4974, loss 0.0159963, acc 1
2016-09-07T23:23:33.153508: step 4975, loss 0.0195365, acc 1
2016-09-07T23:23:33.845591: step 4976, loss 0.0723345, acc 0.98
2016-09-07T23:23:34.558941: step 4977, loss 0.0153987, acc 1
2016-09-07T23:23:35.269652: step 4978, loss 0.00833889, acc 1
2016-09-07T23:23:35.958912: step 4979, loss 0.00142785, acc 1
2016-09-07T23:23:36.652305: step 4980, loss 0.0171262, acc 1
2016-09-07T23:23:37.353368: step 4981, loss 0.0323659, acc 0.98
2016-09-07T23:23:38.058125: step 4982, loss 0.0469558, acc 0.98
2016-09-07T23:23:38.743534: step 4983, loss 0.123321, acc 0.96
2016-09-07T23:23:39.444980: step 4984, loss 0.0224705, acc 0.98
2016-09-07T23:23:40.153988: step 4985, loss 0.00903364, acc 1
2016-09-07T23:23:40.869346: step 4986, loss 0.00162713, acc 1
2016-09-07T23:23:41.583838: step 4987, loss 0.0221908, acc 0.98
2016-09-07T23:23:42.286399: step 4988, loss 0.00142934, acc 1
2016-09-07T23:23:42.971110: step 4989, loss 0.00601466, acc 1
2016-09-07T23:23:43.688126: step 4990, loss 0.0565274, acc 0.98
2016-09-07T23:23:44.391243: step 4991, loss 0.0773423, acc 0.96
2016-09-07T23:23:45.093529: step 4992, loss 0.0407883, acc 0.98
2016-09-07T23:23:45.820614: step 4993, loss 0.0139364, acc 1
2016-09-07T23:23:46.518013: step 4994, loss 0.0085539, acc 1
2016-09-07T23:23:47.213385: step 4995, loss 0.0179195, acc 1
2016-09-07T23:23:47.894465: step 4996, loss 0.0221112, acc 0.98
2016-09-07T23:23:48.626675: step 4997, loss 0.0193965, acc 1
2016-09-07T23:23:49.335083: step 4998, loss 0.0330357, acc 0.98
2016-09-07T23:23:50.033005: step 4999, loss 0.0914894, acc 0.96
2016-09-07T23:23:50.723985: step 5000, loss 5.14762e-05, acc 1

Evaluation:
2016-09-07T23:23:54.135550: step 5000, loss 2.3905, acc 0.74

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-5000

2016-09-07T23:23:55.863321: step 5001, loss 0.0205297, acc 0.98
2016-09-07T23:23:56.565886: step 5002, loss 0.0227691, acc 1
2016-09-07T23:23:57.268452: step 5003, loss 0.0272229, acc 0.98
2016-09-07T23:23:57.969570: step 5004, loss 0.0320424, acc 0.98
2016-09-07T23:23:58.683855: step 5005, loss 0.0194384, acc 1
2016-09-07T23:23:59.401573: step 5006, loss 0.00481895, acc 1
2016-09-07T23:24:00.104070: step 5007, loss 0.00217902, acc 1
2016-09-07T23:24:00.830629: step 5008, loss 0.0171181, acc 1
2016-09-07T23:24:01.529041: step 5009, loss 0.0224589, acc 1
2016-09-07T23:24:02.225525: step 5010, loss 0.0242144, acc 0.98
2016-09-07T23:24:02.919545: step 5011, loss 0.0193849, acc 1
2016-09-07T23:24:03.617814: step 5012, loss 0.034119, acc 0.98
2016-09-07T23:24:04.321805: step 5013, loss 0.0180413, acc 1
2016-09-07T23:24:05.014605: step 5014, loss 0.205362, acc 0.92
2016-09-07T23:24:05.705695: step 5015, loss 0.0123704, acc 1
2016-09-07T23:24:06.426116: step 5016, loss 0.0914922, acc 0.96
2016-09-07T23:24:07.118038: step 5017, loss 0.0120352, acc 1
2016-09-07T23:24:07.821850: step 5018, loss 0.0170895, acc 0.98
2016-09-07T23:24:08.532827: step 5019, loss 0.0651789, acc 0.96
2016-09-07T23:24:09.229685: step 5020, loss 0.0583988, acc 0.96
2016-09-07T23:24:09.921036: step 5021, loss 0.0271608, acc 0.98
2016-09-07T23:24:10.634639: step 5022, loss 0.0114514, acc 1
2016-09-07T23:24:11.334623: step 5023, loss 8.21486e-05, acc 1
2016-09-07T23:24:12.040009: step 5024, loss 0.00797793, acc 1
2016-09-07T23:24:12.741860: step 5025, loss 0.0408176, acc 0.98
2016-09-07T23:24:13.445840: step 5026, loss 0.0823235, acc 0.94
2016-09-07T23:24:14.139259: step 5027, loss 0.00777617, acc 1
2016-09-07T23:24:14.851460: step 5028, loss 0.00279294, acc 1
2016-09-07T23:24:15.545841: step 5029, loss 0.0106391, acc 1
2016-09-07T23:24:16.251355: step 5030, loss 0.0135801, acc 1
2016-09-07T23:24:16.934393: step 5031, loss 0.0139046, acc 1
2016-09-07T23:24:17.647240: step 5032, loss 0.00621797, acc 1
2016-09-07T23:24:18.335975: step 5033, loss 0.0164204, acc 1
2016-09-07T23:24:19.049981: step 5034, loss 0.0129602, acc 1
2016-09-07T23:24:19.741815: step 5035, loss 0.0198914, acc 0.98
2016-09-07T23:24:20.453040: step 5036, loss 0.0131516, acc 1
2016-09-07T23:24:21.147991: step 5037, loss 0.070545, acc 0.98
2016-09-07T23:24:21.850300: step 5038, loss 0.0222027, acc 0.98
2016-09-07T23:24:22.572817: step 5039, loss 0.0716993, acc 0.98
2016-09-07T23:24:23.272526: step 5040, loss 0.0297366, acc 0.98
2016-09-07T23:24:23.962121: step 5041, loss 0.0165949, acc 1
2016-09-07T23:24:24.660064: step 5042, loss 0.158685, acc 0.94
2016-09-07T23:24:25.367748: step 5043, loss 0.045722, acc 0.98
2016-09-07T23:24:25.731596: step 5044, loss 0.00327377, acc 1
2016-09-07T23:24:26.416573: step 5045, loss 0.0348641, acc 0.96
2016-09-07T23:24:27.097694: step 5046, loss 0.0162742, acc 1
2016-09-07T23:24:27.800952: step 5047, loss 0.0742138, acc 0.96
2016-09-07T23:24:28.490683: step 5048, loss 0.0677575, acc 0.98
2016-09-07T23:24:29.196311: step 5049, loss 0.00296758, acc 1
2016-09-07T23:24:29.914777: step 5050, loss 0.0531855, acc 0.96
2016-09-07T23:24:30.598186: step 5051, loss 0.0215683, acc 1
2016-09-07T23:24:31.294050: step 5052, loss 0.0139754, acc 1
2016-09-07T23:24:31.995213: step 5053, loss 0.0385195, acc 0.98
2016-09-07T23:24:32.675871: step 5054, loss 0.0315929, acc 0.96
2016-09-07T23:24:33.363680: step 5055, loss 0.0955412, acc 0.94
2016-09-07T23:24:34.067696: step 5056, loss 0.0299011, acc 1
2016-09-07T23:24:34.777344: step 5057, loss 0.0139078, acc 1
2016-09-07T23:24:35.472841: step 5058, loss 0.0327186, acc 0.98
2016-09-07T23:24:36.182940: step 5059, loss 0.00760141, acc 1
2016-09-07T23:24:36.887165: step 5060, loss 0.0176135, acc 1
2016-09-07T23:24:37.594088: step 5061, loss 0.023074, acc 1
2016-09-07T23:24:38.292032: step 5062, loss 0.0112766, acc 1
2016-09-07T23:24:38.987634: step 5063, loss 0.0156077, acc 1
2016-09-07T23:24:39.698264: step 5064, loss 0.00172184, acc 1
2016-09-07T23:24:40.385038: step 5065, loss 0.0252897, acc 0.98
2016-09-07T23:24:41.092860: step 5066, loss 0.192752, acc 0.94
2016-09-07T23:24:41.808815: step 5067, loss 0.0191572, acc 1
2016-09-07T23:24:42.521665: step 5068, loss 0.0216156, acc 1
2016-09-07T23:24:43.229475: step 5069, loss 0.000271623, acc 1
2016-09-07T23:24:43.938168: step 5070, loss 0.000331386, acc 1
2016-09-07T23:24:44.627533: step 5071, loss 0.0514942, acc 0.98
2016-09-07T23:24:45.324002: step 5072, loss 0.00023423, acc 1
2016-09-07T23:24:46.010431: step 5073, loss 0.00141045, acc 1
2016-09-07T23:24:46.704777: step 5074, loss 0.0392799, acc 0.98
2016-09-07T23:24:47.401554: step 5075, loss 0.00451013, acc 1
2016-09-07T23:24:48.087695: step 5076, loss 0.0695693, acc 0.96
2016-09-07T23:24:48.793934: step 5077, loss 0.016801, acc 0.98
2016-09-07T23:24:49.488803: step 5078, loss 0.0545359, acc 0.98
2016-09-07T23:24:50.189302: step 5079, loss 0.00816991, acc 1
2016-09-07T23:24:50.892727: step 5080, loss 0.0455902, acc 0.98
2016-09-07T23:24:51.594287: step 5081, loss 0.0690126, acc 0.96
2016-09-07T23:24:52.281272: step 5082, loss 0.0287923, acc 0.98
2016-09-07T23:24:53.001450: step 5083, loss 0.0159441, acc 1
2016-09-07T23:24:53.696849: step 5084, loss 0.0119236, acc 1
2016-09-07T23:24:54.423875: step 5085, loss 0.0159819, acc 1
2016-09-07T23:24:55.112492: step 5086, loss 0.0529963, acc 0.98
2016-09-07T23:24:55.856281: step 5087, loss 0.0214764, acc 0.98
2016-09-07T23:24:56.615727: step 5088, loss 0.0158117, acc 1
2016-09-07T23:24:57.315181: step 5089, loss 0.13587, acc 0.94
2016-09-07T23:24:58.024062: step 5090, loss 0.0222318, acc 0.98
2016-09-07T23:24:58.725807: step 5091, loss 0.00457735, acc 1
2016-09-07T23:24:59.423271: step 5092, loss 0.188209, acc 0.9
2016-09-07T23:25:00.150004: step 5093, loss 0.0477741, acc 0.98
2016-09-07T23:25:00.890899: step 5094, loss 0.031789, acc 0.98
2016-09-07T23:25:01.590998: step 5095, loss 0.00120354, acc 1
2016-09-07T23:25:02.285749: step 5096, loss 0.106835, acc 0.94
2016-09-07T23:25:02.979631: step 5097, loss 0.0221908, acc 1
2016-09-07T23:25:03.669338: step 5098, loss 0.22599, acc 0.98
2016-09-07T23:25:04.366098: step 5099, loss 0.050848, acc 0.96
2016-09-07T23:25:05.063772: step 5100, loss 0.0317036, acc 0.98

Evaluation:
2016-09-07T23:25:08.495541: step 5100, loss 2.24691, acc 0.746

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-5100

2016-09-07T23:25:10.226957: step 5101, loss 0.127147, acc 0.98
2016-09-07T23:25:10.932338: step 5102, loss 0.0511274, acc 0.98
2016-09-07T23:25:11.630285: step 5103, loss 0.0205854, acc 0.98
2016-09-07T23:25:12.327378: step 5104, loss 0.0278171, acc 0.98
2016-09-07T23:25:13.048909: step 5105, loss 0.0356974, acc 0.98
2016-09-07T23:25:13.768441: step 5106, loss 0.0183182, acc 1
2016-09-07T23:25:14.465399: step 5107, loss 0.0992587, acc 0.96
2016-09-07T23:25:15.192633: step 5108, loss 0.0261442, acc 1
2016-09-07T23:25:15.951081: step 5109, loss 0.112231, acc 0.94
2016-09-07T23:25:16.661089: step 5110, loss 0.0225025, acc 1
2016-09-07T23:25:17.357781: step 5111, loss 0.0496143, acc 0.98
2016-09-07T23:25:18.067281: step 5112, loss 0.00213786, acc 1
2016-09-07T23:25:18.771507: step 5113, loss 0.0100756, acc 1
2016-09-07T23:25:19.482678: step 5114, loss 0.0402778, acc 0.98
2016-09-07T23:25:20.189617: step 5115, loss 0.0444213, acc 0.98
2016-09-07T23:25:20.905912: step 5116, loss 0.00760084, acc 1
2016-09-07T23:25:21.620101: step 5117, loss 0.00261929, acc 1
2016-09-07T23:25:22.312679: step 5118, loss 0.0192079, acc 1
2016-09-07T23:25:23.026395: step 5119, loss 0.0132714, acc 1
2016-09-07T23:25:23.710567: step 5120, loss 0.0492289, acc 0.98
2016-09-07T23:25:24.395244: step 5121, loss 0.0587415, acc 0.98
2016-09-07T23:25:25.103317: step 5122, loss 0.0262279, acc 0.98
2016-09-07T23:25:25.801818: step 5123, loss 0.107926, acc 0.98
2016-09-07T23:25:26.503703: step 5124, loss 0.0892311, acc 0.98
2016-09-07T23:25:27.231275: step 5125, loss 0.0309089, acc 0.98
2016-09-07T23:25:27.962120: step 5126, loss 0.0021951, acc 1
2016-09-07T23:25:28.676029: step 5127, loss 0.0383893, acc 0.96
2016-09-07T23:25:29.388124: step 5128, loss 0.0871173, acc 0.96
2016-09-07T23:25:30.101515: step 5129, loss 0.0590938, acc 0.98
2016-09-07T23:25:30.791110: step 5130, loss 0.0241479, acc 0.98
2016-09-07T23:25:31.499084: step 5131, loss 0.00488626, acc 1
2016-09-07T23:25:32.222263: step 5132, loss 0.0281975, acc 0.98
2016-09-07T23:25:32.918221: step 5133, loss 0.050724, acc 0.98
2016-09-07T23:25:33.608273: step 5134, loss 0.0273791, acc 1
2016-09-07T23:25:34.281417: step 5135, loss 0.0398501, acc 0.98
2016-09-07T23:25:34.986738: step 5136, loss 0.0369279, acc 0.96
2016-09-07T23:25:35.691467: step 5137, loss 0.0394002, acc 0.98
2016-09-07T23:25:36.406342: step 5138, loss 0.00228869, acc 1
2016-09-07T23:25:37.090461: step 5139, loss 0.0930256, acc 0.98
2016-09-07T23:25:37.792280: step 5140, loss 0.0226362, acc 0.98
2016-09-07T23:25:38.492024: step 5141, loss 0.0246966, acc 0.98
2016-09-07T23:25:39.207608: step 5142, loss 0.0529886, acc 0.98
2016-09-07T23:25:39.911922: step 5143, loss 0.0240168, acc 0.98
2016-09-07T23:25:40.628390: step 5144, loss 0.0313718, acc 0.98
2016-09-07T23:25:41.337948: step 5145, loss 0.0450097, acc 0.98
2016-09-07T23:25:42.062488: step 5146, loss 0.00117846, acc 1
2016-09-07T23:25:42.780548: step 5147, loss 0.0149472, acc 1
2016-09-07T23:25:43.469253: step 5148, loss 0.00976249, acc 1
2016-09-07T23:25:44.187456: step 5149, loss 0.0304201, acc 0.98
2016-09-07T23:25:44.937037: step 5150, loss 0.036778, acc 0.98
2016-09-07T23:25:45.635616: step 5151, loss 0.0307979, acc 0.98
2016-09-07T23:25:46.353120: step 5152, loss 0.0286983, acc 1
2016-09-07T23:25:47.062489: step 5153, loss 0.112498, acc 0.96
2016-09-07T23:25:47.761874: step 5154, loss 0.0488997, acc 0.98
2016-09-07T23:25:48.460532: step 5155, loss 0.186587, acc 0.96
2016-09-07T23:25:49.192915: step 5156, loss 0.00296515, acc 1
2016-09-07T23:25:49.903699: step 5157, loss 0.00142204, acc 1
2016-09-07T23:25:50.619714: step 5158, loss 0.0294386, acc 0.98
2016-09-07T23:25:51.325351: step 5159, loss 0.0243031, acc 0.98
2016-09-07T23:25:52.041360: step 5160, loss 0.0104306, acc 1
2016-09-07T23:25:52.756903: step 5161, loss 0.0214597, acc 1
2016-09-07T23:25:53.445782: step 5162, loss 0.0131579, acc 1
2016-09-07T23:25:54.144429: step 5163, loss 0.00851041, acc 1
2016-09-07T23:25:54.865338: step 5164, loss 0.0834225, acc 0.96
2016-09-07T23:25:55.577094: step 5165, loss 0.0231015, acc 1
2016-09-07T23:25:56.280217: step 5166, loss 0.105922, acc 0.96
2016-09-07T23:25:56.995292: step 5167, loss 0.0230228, acc 0.98
2016-09-07T23:25:57.679975: step 5168, loss 0.0178771, acc 0.98
2016-09-07T23:25:58.379084: step 5169, loss 0.0868907, acc 0.96
2016-09-07T23:25:59.075765: step 5170, loss 0.00534883, acc 1
2016-09-07T23:25:59.786176: step 5171, loss 0.046217, acc 0.98
2016-09-07T23:26:00.524423: step 5172, loss 0.0562658, acc 0.96
2016-09-07T23:26:01.225893: step 5173, loss 0.0104817, acc 1
2016-09-07T23:26:01.916522: step 5174, loss 0.0169062, acc 1
2016-09-07T23:26:02.640514: step 5175, loss 0.0679276, acc 0.98
2016-09-07T23:26:03.336822: step 5176, loss 0.00910614, acc 1
2016-09-07T23:26:04.074627: step 5177, loss 0.00446928, acc 1
2016-09-07T23:26:04.799506: step 5178, loss 0.0122961, acc 1
2016-09-07T23:26:05.509240: step 5179, loss 0.0515568, acc 0.96
2016-09-07T23:26:06.216593: step 5180, loss 0.0315589, acc 0.98
2016-09-07T23:26:06.914562: step 5181, loss 0.00445181, acc 1
2016-09-07T23:26:07.631913: step 5182, loss 0.0016321, acc 1
2016-09-07T23:26:08.340674: step 5183, loss 0.00695585, acc 1
2016-09-07T23:26:09.022798: step 5184, loss 0.0381324, acc 0.96
2016-09-07T23:26:09.716840: step 5185, loss 0.0412489, acc 0.98
2016-09-07T23:26:10.399228: step 5186, loss 0.0179091, acc 1
2016-09-07T23:26:11.114888: step 5187, loss 0.0249872, acc 0.98
2016-09-07T23:26:11.805946: step 5188, loss 0.0583595, acc 0.96
2016-09-07T23:26:12.493653: step 5189, loss 0.0995166, acc 0.96
2016-09-07T23:26:13.217277: step 5190, loss 0.00551034, acc 1
2016-09-07T23:26:13.925805: step 5191, loss 0.0143972, acc 1
2016-09-07T23:26:14.615310: step 5192, loss 0.0597795, acc 0.98
2016-09-07T23:26:15.321001: step 5193, loss 0.0039589, acc 1
2016-09-07T23:26:16.037493: step 5194, loss 0.0217599, acc 0.98
2016-09-07T23:26:16.751512: step 5195, loss 0.0362448, acc 1
2016-09-07T23:26:17.445195: step 5196, loss 0.0443013, acc 0.96
2016-09-07T23:26:18.147700: step 5197, loss 0.0128845, acc 1
2016-09-07T23:26:18.856300: step 5198, loss 0.0197093, acc 1
2016-09-07T23:26:19.576715: step 5199, loss 0.0486967, acc 0.96
2016-09-07T23:26:20.278713: step 5200, loss 0.00696361, acc 1

Evaluation:
2016-09-07T23:26:23.782239: step 5200, loss 2.10053, acc 0.738

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-5200

2016-09-07T23:26:25.708296: step 5201, loss 0.00619061, acc 1
2016-09-07T23:26:26.421302: step 5202, loss 0.049144, acc 0.98
2016-09-07T23:26:27.146443: step 5203, loss 0.0429633, acc 0.98
2016-09-07T23:26:27.854042: step 5204, loss 0.00216961, acc 1
2016-09-07T23:26:28.589681: step 5205, loss 0.06903, acc 0.96
2016-09-07T23:26:29.291386: step 5206, loss 0.0317747, acc 0.98
2016-09-07T23:26:29.996477: step 5207, loss 0.00142392, acc 1
2016-09-07T23:26:30.705833: step 5208, loss 0.022537, acc 1
2016-09-07T23:26:31.409672: step 5209, loss 0.0440397, acc 0.98
2016-09-07T23:26:32.103483: step 5210, loss 0.0211029, acc 1
2016-09-07T23:26:32.824788: step 5211, loss 0.0471915, acc 0.98
2016-09-07T23:26:33.535500: step 5212, loss 0.0212188, acc 1
2016-09-07T23:26:34.295121: step 5213, loss 0.00273105, acc 1
2016-09-07T23:26:35.000935: step 5214, loss 0.0254228, acc 0.98
2016-09-07T23:26:35.695596: step 5215, loss 0.0151712, acc 1
2016-09-07T23:26:36.397130: step 5216, loss 0.0272036, acc 0.98
2016-09-07T23:26:37.108205: step 5217, loss 0.0018314, acc 1
2016-09-07T23:26:37.827617: step 5218, loss 0.0831695, acc 0.98
2016-09-07T23:26:38.546948: step 5219, loss 0.0218317, acc 1
2016-09-07T23:26:39.248582: step 5220, loss 0.0335865, acc 0.98
2016-09-07T23:26:39.949933: step 5221, loss 0.042408, acc 0.98
2016-09-07T23:26:40.676583: step 5222, loss 0.00838167, acc 1
2016-09-07T23:26:41.399771: step 5223, loss 0.0135201, acc 1
2016-09-07T23:26:42.119512: step 5224, loss 0.00303334, acc 1
2016-09-07T23:26:42.837674: step 5225, loss 0.0589255, acc 0.98
2016-09-07T23:26:43.529448: step 5226, loss 0.0468734, acc 0.96
2016-09-07T23:26:44.244801: step 5227, loss 0.0662991, acc 0.98
2016-09-07T23:26:44.942237: step 5228, loss 0.00173715, acc 1
2016-09-07T23:26:45.647804: step 5229, loss 0.0114698, acc 1
2016-09-07T23:26:46.355728: step 5230, loss 0.0411485, acc 0.98
2016-09-07T23:26:47.061393: step 5231, loss 0.0346417, acc 1
2016-09-07T23:26:47.792987: step 5232, loss 0.00707337, acc 1
2016-09-07T23:26:48.509120: step 5233, loss 0.00168923, acc 1
2016-09-07T23:26:49.225555: step 5234, loss 0.0329248, acc 0.96
2016-09-07T23:26:49.919914: step 5235, loss 0.0694985, acc 0.96
2016-09-07T23:26:50.613579: step 5236, loss 0.0179418, acc 0.98
2016-09-07T23:26:51.309666: step 5237, loss 0.0516152, acc 0.98
2016-09-07T23:26:51.678605: step 5238, loss 0.00515768, acc 1
2016-09-07T23:26:52.372375: step 5239, loss 0.00585573, acc 1
2016-09-07T23:26:53.101738: step 5240, loss 0.0114571, acc 1
2016-09-07T23:26:53.804011: step 5241, loss 0.0265303, acc 0.98
2016-09-07T23:26:54.496345: step 5242, loss 0.0132338, acc 1
2016-09-07T23:26:55.202551: step 5243, loss 0.0944255, acc 0.98
2016-09-07T23:26:55.912776: step 5244, loss 0.0231057, acc 1
2016-09-07T23:26:56.640155: step 5245, loss 0.0210767, acc 1
2016-09-07T23:26:57.347103: step 5246, loss 0.00878379, acc 1
2016-09-07T23:26:58.042437: step 5247, loss 0.00346786, acc 1
2016-09-07T23:26:58.736806: step 5248, loss 0.00302142, acc 1
2016-09-07T23:26:59.422055: step 5249, loss 0.0245514, acc 0.98
2016-09-07T23:27:00.107830: step 5250, loss 0.0371811, acc 0.98
2016-09-07T23:27:00.834799: step 5251, loss 0.0259557, acc 0.98
2016-09-07T23:27:01.531100: step 5252, loss 0.028801, acc 0.98
2016-09-07T23:27:02.224990: step 5253, loss 0.00672864, acc 1
2016-09-07T23:27:02.928709: step 5254, loss 0.00561483, acc 1
2016-09-07T23:27:03.636928: step 5255, loss 0.00255678, acc 1
2016-09-07T23:27:04.346685: step 5256, loss 0.06796, acc 0.98
2016-09-07T23:27:05.049525: step 5257, loss 0.0235607, acc 1
2016-09-07T23:27:05.743146: step 5258, loss 0.0184245, acc 0.98
2016-09-07T23:27:06.457443: step 5259, loss 0.0675186, acc 0.98
2016-09-07T23:27:07.188522: step 5260, loss 0.0265608, acc 0.98
2016-09-07T23:27:07.932463: step 5261, loss 0.00523537, acc 1
2016-09-07T23:27:08.644075: step 5262, loss 0.000335198, acc 1
2016-09-07T23:27:09.364046: step 5263, loss 0.0206871, acc 0.98
2016-09-07T23:27:10.065138: step 5264, loss 0.012082, acc 1
2016-09-07T23:27:10.774002: step 5265, loss 0.0174436, acc 1
2016-09-07T23:27:11.477218: step 5266, loss 0.031257, acc 0.98
2016-09-07T23:27:12.178010: step 5267, loss 0.020653, acc 0.98
2016-09-07T23:27:12.888564: step 5268, loss 0.020315, acc 1
2016-09-07T23:27:13.591000: step 5269, loss 0.00365059, acc 1
2016-09-07T23:27:14.302245: step 5270, loss 0.0207427, acc 0.98
2016-09-07T23:27:14.984743: step 5271, loss 0.00153338, acc 1
2016-09-07T23:27:15.693504: step 5272, loss 0.0494368, acc 0.98
2016-09-07T23:27:16.406809: step 5273, loss 0.0545906, acc 0.98
2016-09-07T23:27:17.137087: step 5274, loss 0.00519287, acc 1
2016-09-07T23:27:17.858523: step 5275, loss 0.0481832, acc 0.98
2016-09-07T23:27:18.573134: step 5276, loss 0.0785444, acc 0.96
2016-09-07T23:27:19.297307: step 5277, loss 0.0018066, acc 1
2016-09-07T23:27:20.029670: step 5278, loss 0.00660606, acc 1
2016-09-07T23:27:20.741398: step 5279, loss 0.0197607, acc 1
2016-09-07T23:27:21.438483: step 5280, loss 0.0188873, acc 1
2016-09-07T23:27:22.142807: step 5281, loss 0.0265251, acc 0.98
2016-09-07T23:27:22.854948: step 5282, loss 0.0017937, acc 1
2016-09-07T23:27:23.567514: step 5283, loss 0.113586, acc 0.98
2016-09-07T23:27:24.284907: step 5284, loss 0.0559649, acc 0.98
2016-09-07T23:27:24.996533: step 5285, loss 0.103809, acc 0.96
2016-09-07T23:27:25.717630: step 5286, loss 0.0207255, acc 0.98
2016-09-07T23:27:26.429602: step 5287, loss 0.00192447, acc 1
2016-09-07T23:27:27.129864: step 5288, loss 0.0471209, acc 0.96
2016-09-07T23:27:27.842206: step 5289, loss 0.0199325, acc 0.98
2016-09-07T23:27:28.537225: step 5290, loss 0.000790378, acc 1
2016-09-07T23:27:29.237119: step 5291, loss 0.0416881, acc 0.98
2016-09-07T23:27:29.954327: step 5292, loss 0.170543, acc 0.96
2016-09-07T23:27:30.650589: step 5293, loss 0.00313974, acc 1
2016-09-07T23:27:31.342894: step 5294, loss 0.00564126, acc 1
2016-09-07T23:27:32.054522: step 5295, loss 0.0382969, acc 0.98
2016-09-07T23:27:32.757059: step 5296, loss 0.0190086, acc 0.98
2016-09-07T23:27:33.463521: step 5297, loss 0.0375364, acc 0.96
2016-09-07T23:27:34.162630: step 5298, loss 0.0786893, acc 0.98
2016-09-07T23:27:34.860491: step 5299, loss 0.0942078, acc 0.98
2016-09-07T23:27:35.577578: step 5300, loss 0.0136056, acc 1

Evaluation:
2016-09-07T23:27:39.090052: step 5300, loss 2.10275, acc 0.73

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-5300

2016-09-07T23:27:40.855135: step 5301, loss 0.0463984, acc 0.96
2016-09-07T23:27:41.576166: step 5302, loss 0.0114166, acc 1
2016-09-07T23:27:42.281583: step 5303, loss 0.0402844, acc 1
2016-09-07T23:27:43.002518: step 5304, loss 0.00826922, acc 1
2016-09-07T23:27:43.709388: step 5305, loss 0.0812095, acc 0.98
2016-09-07T23:27:44.420775: step 5306, loss 0.0230927, acc 1
2016-09-07T23:27:45.130020: step 5307, loss 0.104018, acc 0.98
2016-09-07T23:27:45.857445: step 5308, loss 0.0567573, acc 0.96
2016-09-07T23:27:46.572017: step 5309, loss 0.0250753, acc 1
2016-09-07T23:27:47.277135: step 5310, loss 0.0176788, acc 0.98
2016-09-07T23:27:47.999222: step 5311, loss 0.0270355, acc 1
2016-09-07T23:27:48.705194: step 5312, loss 0.0056285, acc 1
2016-09-07T23:27:49.411734: step 5313, loss 0.0375926, acc 0.98
2016-09-07T23:27:50.140895: step 5314, loss 0.0782483, acc 0.98
2016-09-07T23:27:50.858602: step 5315, loss 0.0132496, acc 1
2016-09-07T23:27:51.582930: step 5316, loss 0.00467511, acc 1
2016-09-07T23:27:52.316466: step 5317, loss 0.0133561, acc 1
2016-09-07T23:27:53.037724: step 5318, loss 0.160137, acc 0.92
2016-09-07T23:27:53.740298: step 5319, loss 0.0473279, acc 0.98
2016-09-07T23:27:54.436514: step 5320, loss 0.0495769, acc 0.98
2016-09-07T23:27:55.136251: step 5321, loss 0.00866809, acc 1
2016-09-07T23:27:55.854200: step 5322, loss 0.00486814, acc 1
2016-09-07T23:27:56.577401: step 5323, loss 0.0137428, acc 1
2016-09-07T23:27:57.275313: step 5324, loss 0.00851201, acc 1
2016-09-07T23:27:57.987936: step 5325, loss 0.00696178, acc 1
2016-09-07T23:27:58.692703: step 5326, loss 0.0189548, acc 0.98
2016-09-07T23:27:59.413843: step 5327, loss 0.00798063, acc 1
2016-09-07T23:28:00.118371: step 5328, loss 0.0221805, acc 1
2016-09-07T23:28:00.852419: step 5329, loss 0.0337525, acc 0.98
2016-09-07T23:28:01.555729: step 5330, loss 0.00801429, acc 1
2016-09-07T23:28:02.293232: step 5331, loss 0.0270438, acc 0.98
2016-09-07T23:28:02.995613: step 5332, loss 0.115343, acc 0.98
2016-09-07T23:28:03.699206: step 5333, loss 0.0721174, acc 0.96
2016-09-07T23:28:04.413584: step 5334, loss 0.0459854, acc 0.96
2016-09-07T23:28:05.133535: step 5335, loss 0.0140649, acc 1
2016-09-07T23:28:05.852568: step 5336, loss 0.0819576, acc 0.94
2016-09-07T23:28:06.547160: step 5337, loss 0.0338093, acc 0.98
2016-09-07T23:28:07.245179: step 5338, loss 0.0340494, acc 0.98
2016-09-07T23:28:07.944742: step 5339, loss 0.00622082, acc 1
2016-09-07T23:28:08.650324: step 5340, loss 0.042581, acc 0.98
2016-09-07T23:28:09.389179: step 5341, loss 0.0645198, acc 0.96
2016-09-07T23:28:10.108757: step 5342, loss 0.00413971, acc 1
2016-09-07T23:28:10.839544: step 5343, loss 0.0399304, acc 0.98
2016-09-07T23:28:11.562468: step 5344, loss 0.0172707, acc 0.98
2016-09-07T23:28:12.290846: step 5345, loss 0.0201114, acc 1
2016-09-07T23:28:12.992851: step 5346, loss 0.0152733, acc 1
2016-09-07T23:28:13.712342: step 5347, loss 0.0275665, acc 0.98
2016-09-07T23:28:14.426577: step 5348, loss 0.0426825, acc 0.96
2016-09-07T23:28:15.148554: step 5349, loss 0.0659964, acc 0.98
2016-09-07T23:28:15.850301: step 5350, loss 0.0178702, acc 0.98
2016-09-07T23:28:16.533595: step 5351, loss 0.0155221, acc 0.98
2016-09-07T23:28:17.231422: step 5352, loss 0.0166523, acc 1
2016-09-07T23:28:17.936537: step 5353, loss 0.0441291, acc 0.96
2016-09-07T23:28:18.647553: step 5354, loss 0.0388134, acc 0.96
2016-09-07T23:28:19.352040: step 5355, loss 0.0275694, acc 0.98
2016-09-07T23:28:20.058633: step 5356, loss 0.0246757, acc 0.98
2016-09-07T23:28:20.761426: step 5357, loss 0.0329761, acc 0.98
2016-09-07T23:28:21.459394: step 5358, loss 0.0275889, acc 0.98
2016-09-07T23:28:22.161741: step 5359, loss 0.0031625, acc 1
2016-09-07T23:28:22.865226: step 5360, loss 0.0188311, acc 1
2016-09-07T23:28:23.578248: step 5361, loss 0.0301135, acc 0.98
2016-09-07T23:28:24.284543: step 5362, loss 0.00269097, acc 1
2016-09-07T23:28:24.997414: step 5363, loss 0.0110231, acc 1
2016-09-07T23:28:25.717104: step 5364, loss 0.00715232, acc 1
2016-09-07T23:28:26.424139: step 5365, loss 0.0483835, acc 0.96
2016-09-07T23:28:27.138607: step 5366, loss 0.000175045, acc 1
2016-09-07T23:28:27.862035: step 5367, loss 0.00779902, acc 1
2016-09-07T23:28:28.575513: step 5368, loss 0.00581713, acc 1
2016-09-07T23:28:29.282079: step 5369, loss 0.0724427, acc 0.96
2016-09-07T23:28:29.997819: step 5370, loss 0.0141964, acc 0.98
2016-09-07T23:28:30.688138: step 5371, loss 0.00670473, acc 1
2016-09-07T23:28:31.381543: step 5372, loss 0.0180761, acc 1
2016-09-07T23:28:32.094852: step 5373, loss 0.0178358, acc 1
2016-09-07T23:28:32.803343: step 5374, loss 0.0352632, acc 0.98
2016-09-07T23:28:33.520120: step 5375, loss 0.00751944, acc 1
2016-09-07T23:28:34.228434: step 5376, loss 0.000221975, acc 1
2016-09-07T23:28:34.921863: step 5377, loss 0.000104724, acc 1
2016-09-07T23:28:35.651530: step 5378, loss 0.0449354, acc 0.98
2016-09-07T23:28:36.360070: step 5379, loss 0.0509177, acc 0.98
2016-09-07T23:28:37.073890: step 5380, loss 0.0376104, acc 0.98
2016-09-07T23:28:37.779622: step 5381, loss 0.0594105, acc 0.98
2016-09-07T23:28:38.492896: step 5382, loss 0.00416745, acc 1
2016-09-07T23:28:39.218416: step 5383, loss 0.105052, acc 0.96
2016-09-07T23:28:39.942105: step 5384, loss 0.00430423, acc 1
2016-09-07T23:28:40.658644: step 5385, loss 0.0700543, acc 0.98
2016-09-07T23:28:41.358142: step 5386, loss 0.176036, acc 0.98
2016-09-07T23:28:42.054525: step 5387, loss 0.0235116, acc 0.98
2016-09-07T23:28:42.763567: step 5388, loss 0.00305754, acc 1
2016-09-07T23:28:43.465925: step 5389, loss 0.0304238, acc 1
2016-09-07T23:28:44.182848: step 5390, loss 0.0271334, acc 1
2016-09-07T23:28:44.907418: step 5391, loss 0.0270253, acc 0.98
2016-09-07T23:28:45.629712: step 5392, loss 0.0832732, acc 0.96
2016-09-07T23:28:46.323102: step 5393, loss 0.0302786, acc 1
2016-09-07T23:28:47.033165: step 5394, loss 0.0348007, acc 0.98
2016-09-07T23:28:47.744482: step 5395, loss 0.00582323, acc 1
2016-09-07T23:28:48.457255: step 5396, loss 0.0044631, acc 1
2016-09-07T23:28:49.174911: step 5397, loss 0.000583334, acc 1
2016-09-07T23:28:49.911985: step 5398, loss 0.0297138, acc 0.98
2016-09-07T23:28:50.612986: step 5399, loss 0.0355272, acc 0.98
2016-09-07T23:28:51.343413: step 5400, loss 0.0188684, acc 0.98

Evaluation:
2016-09-07T23:28:54.837097: step 5400, loss 2.11227, acc 0.739

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-5400

2016-09-07T23:28:56.676222: step 5401, loss 0.000145608, acc 1
2016-09-07T23:28:57.427394: step 5402, loss 0.109389, acc 0.96
2016-09-07T23:28:58.170903: step 5403, loss 0.00234356, acc 1
2016-09-07T23:28:58.845592: step 5404, loss 0.0328904, acc 0.96
2016-09-07T23:28:59.545653: step 5405, loss 0.00904379, acc 1
2016-09-07T23:29:00.278088: step 5406, loss 0.0302451, acc 0.98
2016-09-07T23:29:00.979861: step 5407, loss 0.0842327, acc 0.98
2016-09-07T23:29:01.685680: step 5408, loss 0.08751, acc 0.94
2016-09-07T23:29:02.386662: step 5409, loss 0.0382175, acc 0.98
2016-09-07T23:29:03.069429: step 5410, loss 0.0338379, acc 1
2016-09-07T23:29:03.774230: step 5411, loss 0.0236623, acc 0.98
2016-09-07T23:29:04.477700: step 5412, loss 0.000959554, acc 1
2016-09-07T23:29:05.182427: step 5413, loss 0.110215, acc 0.96
2016-09-07T23:29:05.886577: step 5414, loss 0.000130457, acc 1
2016-09-07T23:29:06.598976: step 5415, loss 0.0175018, acc 0.98
2016-09-07T23:29:07.314072: step 5416, loss 0.0150868, acc 0.98
2016-09-07T23:29:08.022215: step 5417, loss 0.0677555, acc 0.98
2016-09-07T23:29:08.718614: step 5418, loss 0.0280868, acc 1
2016-09-07T23:29:09.397646: step 5419, loss 0.028, acc 0.98
2016-09-07T23:29:10.096695: step 5420, loss 0.0412486, acc 0.98
2016-09-07T23:29:10.822055: step 5421, loss 0.00330386, acc 1
2016-09-07T23:29:11.550352: step 5422, loss 0.0209058, acc 0.98
2016-09-07T23:29:12.248958: step 5423, loss 0.0380341, acc 0.98
2016-09-07T23:29:12.970118: step 5424, loss 0.022244, acc 1
2016-09-07T23:29:13.664179: step 5425, loss 0.0416063, acc 1
2016-09-07T23:29:14.378573: step 5426, loss 0.000503038, acc 1
2016-09-07T23:29:15.080867: step 5427, loss 0.0204597, acc 1
2016-09-07T23:29:15.790864: step 5428, loss 0.0522919, acc 0.94
2016-09-07T23:29:16.481741: step 5429, loss 0.0739404, acc 0.98
2016-09-07T23:29:17.184238: step 5430, loss 0.0265879, acc 0.98
2016-09-07T23:29:17.874973: step 5431, loss 0.0213927, acc 1
2016-09-07T23:29:18.240610: step 5432, loss 0.0417135, acc 1
2016-09-07T23:29:18.955652: step 5433, loss 0.0180098, acc 1
2016-09-07T23:29:19.659405: step 5434, loss 0.0568945, acc 0.98
2016-09-07T23:29:20.354496: step 5435, loss 0.100498, acc 0.96
2016-09-07T23:29:21.055313: step 5436, loss 0.0172986, acc 1
2016-09-07T23:29:21.758653: step 5437, loss 0.0326101, acc 0.96
2016-09-07T23:29:22.483875: step 5438, loss 0.0351259, acc 0.98
2016-09-07T23:29:23.195071: step 5439, loss 0.0168652, acc 1
2016-09-07T23:29:23.893836: step 5440, loss 0.0149551, acc 1
2016-09-07T23:29:24.597719: step 5441, loss 0.000499431, acc 1
2016-09-07T23:29:25.295376: step 5442, loss 0.0210206, acc 1
2016-09-07T23:29:25.984632: step 5443, loss 0.0463069, acc 0.98
2016-09-07T23:29:26.687417: step 5444, loss 0.0100938, acc 1
2016-09-07T23:29:27.417372: step 5445, loss 0.0385617, acc 0.98
2016-09-07T23:29:28.137562: step 5446, loss 0.0690339, acc 0.96
2016-09-07T23:29:28.841145: step 5447, loss 0.0413956, acc 0.98
2016-09-07T23:29:29.539961: step 5448, loss 0.0228795, acc 0.98
2016-09-07T23:29:30.243723: step 5449, loss 0.0310918, acc 0.98
2016-09-07T23:29:30.951366: step 5450, loss 0.0186335, acc 1
2016-09-07T23:29:31.670655: step 5451, loss 0.0268507, acc 0.98
2016-09-07T23:29:32.404677: step 5452, loss 0.0442955, acc 0.98
2016-09-07T23:29:33.092651: step 5453, loss 0.219606, acc 0.96
2016-09-07T23:29:33.794256: step 5454, loss 0.0270797, acc 1
2016-09-07T23:29:34.491474: step 5455, loss 0.0340407, acc 0.96
2016-09-07T23:29:35.200838: step 5456, loss 0.00171414, acc 1
2016-09-07T23:29:35.915912: step 5457, loss 0.0122101, acc 1
2016-09-07T23:29:36.635379: step 5458, loss 0.0343467, acc 0.98
2016-09-07T23:29:37.321318: step 5459, loss 0.015964, acc 0.98
2016-09-07T23:29:38.036230: step 5460, loss 0.0351778, acc 0.96
2016-09-07T23:29:38.757122: step 5461, loss 0.0836585, acc 0.98
2016-09-07T23:29:39.462296: step 5462, loss 0.0250983, acc 0.98
2016-09-07T23:29:40.174186: step 5463, loss 0.0331977, acc 0.98
2016-09-07T23:29:40.862764: step 5464, loss 0.056011, acc 0.98
2016-09-07T23:29:41.578533: step 5465, loss 0.0106982, acc 1
2016-09-07T23:29:42.282833: step 5466, loss 0.0102199, acc 1
2016-09-07T23:29:42.997084: step 5467, loss 0.00312473, acc 1
2016-09-07T23:29:43.707158: step 5468, loss 0.024559, acc 1
2016-09-07T23:29:44.394646: step 5469, loss 0.0131242, acc 1
2016-09-07T23:29:45.109251: step 5470, loss 0.0159376, acc 0.98
2016-09-07T23:29:45.831970: step 5471, loss 0.0287911, acc 0.98
2016-09-07T23:29:46.541939: step 5472, loss 9.98638e-05, acc 1
2016-09-07T23:29:47.252341: step 5473, loss 0.0415308, acc 0.98
2016-09-07T23:29:47.955637: step 5474, loss 6.27661e-05, acc 1
2016-09-07T23:29:48.678495: step 5475, loss 0.0339266, acc 0.96
2016-09-07T23:29:49.384123: step 5476, loss 0.00731322, acc 1
2016-09-07T23:29:50.075893: step 5477, loss 0.0315184, acc 0.98
2016-09-07T23:29:50.793333: step 5478, loss 0.0965597, acc 0.94
2016-09-07T23:29:51.514014: step 5479, loss 0.0460761, acc 0.98
2016-09-07T23:29:52.248193: step 5480, loss 0.0209251, acc 0.98
2016-09-07T23:29:52.940044: step 5481, loss 0.0149829, acc 1
2016-09-07T23:29:53.642867: step 5482, loss 0.0292365, acc 0.98
2016-09-07T23:29:54.343445: step 5483, loss 0.0122894, acc 1
2016-09-07T23:29:55.071125: step 5484, loss 0.0345788, acc 0.96
2016-09-07T23:29:55.796424: step 5485, loss 0.000951046, acc 1
2016-09-07T23:29:56.497582: step 5486, loss 0.0626314, acc 0.98
2016-09-07T23:29:57.208043: step 5487, loss 0.00221771, acc 1
2016-09-07T23:29:57.922809: step 5488, loss 0.00799514, acc 1
2016-09-07T23:29:58.628607: step 5489, loss 0.0163487, acc 0.98
2016-09-07T23:29:59.339165: step 5490, loss 0.000889138, acc 1
2016-09-07T23:30:00.046586: step 5491, loss 0.0582799, acc 0.96
2016-09-07T23:30:00.796884: step 5492, loss 0.00226661, acc 1
2016-09-07T23:30:01.507561: step 5493, loss 0.0414911, acc 0.98
2016-09-07T23:30:02.203516: step 5494, loss 0.00024173, acc 1
2016-09-07T23:30:02.895398: step 5495, loss 0.0291803, acc 0.98
2016-09-07T23:30:03.592793: step 5496, loss 0.0359654, acc 1
2016-09-07T23:30:04.301569: step 5497, loss 0.0568138, acc 0.98
2016-09-07T23:30:05.007322: step 5498, loss 0.042351, acc 0.98
2016-09-07T23:30:05.704255: step 5499, loss 0.00104046, acc 1
2016-09-07T23:30:06.435213: step 5500, loss 0.015965, acc 0.98

Evaluation:
2016-09-07T23:30:09.920667: step 5500, loss 2.13441, acc 0.738

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-5500

2016-09-07T23:30:11.785704: step 5501, loss 0.0171858, acc 0.98
2016-09-07T23:30:12.496857: step 5502, loss 0.0401791, acc 0.98
2016-09-07T23:30:13.199914: step 5503, loss 0.0555806, acc 0.98
2016-09-07T23:30:13.905911: step 5504, loss 0.036476, acc 0.98
2016-09-07T23:30:14.611932: step 5505, loss 0.000569155, acc 1
2016-09-07T23:30:15.306286: step 5506, loss 0.00750338, acc 1
2016-09-07T23:30:16.014523: step 5507, loss 0.00516613, acc 1
2016-09-07T23:30:16.718348: step 5508, loss 0.0198943, acc 0.98
2016-09-07T23:30:17.413161: step 5509, loss 0.141022, acc 0.94
2016-09-07T23:30:18.124755: step 5510, loss 0.000172345, acc 1
2016-09-07T23:30:18.825471: step 5511, loss 0.00525533, acc 1
2016-09-07T23:30:19.544101: step 5512, loss 0.016512, acc 0.98
2016-09-07T23:30:20.265944: step 5513, loss 0.0142605, acc 1
2016-09-07T23:30:20.985518: step 5514, loss 0.0222909, acc 0.98
2016-09-07T23:30:21.687025: step 5515, loss 0.0349768, acc 0.98
2016-09-07T23:30:22.397478: step 5516, loss 0.00458981, acc 1
2016-09-07T23:30:23.114607: step 5517, loss 0.0139248, acc 1
2016-09-07T23:30:23.829480: step 5518, loss 0.00937501, acc 1
2016-09-07T23:30:24.546703: step 5519, loss 0.00140236, acc 1
2016-09-07T23:30:25.234117: step 5520, loss 0.0328354, acc 0.98
2016-09-07T23:30:25.927716: step 5521, loss 0.000349144, acc 1
2016-09-07T23:30:26.628363: step 5522, loss 0.00658387, acc 1
2016-09-07T23:30:27.327603: step 5523, loss 0.0288876, acc 1
2016-09-07T23:30:28.045256: step 5524, loss 0.0223417, acc 0.98
2016-09-07T23:30:28.758943: step 5525, loss 0.00135921, acc 1
2016-09-07T23:30:29.455408: step 5526, loss 0.0880293, acc 0.98
2016-09-07T23:30:30.161699: step 5527, loss 0.0419037, acc 0.98
2016-09-07T23:30:30.897817: step 5528, loss 0.00560268, acc 1
2016-09-07T23:30:31.620419: step 5529, loss 0.00694775, acc 1
2016-09-07T23:30:32.337065: step 5530, loss 0.0535998, acc 0.98
2016-09-07T23:30:33.023329: step 5531, loss 0.0152899, acc 1
2016-09-07T23:30:33.733017: step 5532, loss 0.0422259, acc 0.96
2016-09-07T23:30:34.429302: step 5533, loss 0.0362872, acc 0.98
2016-09-07T23:30:35.130547: step 5534, loss 0.0280466, acc 0.98
2016-09-07T23:30:35.848209: step 5535, loss 0.0856903, acc 0.98
2016-09-07T23:30:36.565529: step 5536, loss 0.0111682, acc 1
2016-09-07T23:30:37.268770: step 5537, loss 0.000232612, acc 1
2016-09-07T23:30:37.985394: step 5538, loss 0.0116078, acc 1
2016-09-07T23:30:38.711552: step 5539, loss 0.047176, acc 0.98
2016-09-07T23:30:39.412609: step 5540, loss 0.00944425, acc 1
2016-09-07T23:30:40.137447: step 5541, loss 0.0162618, acc 1
2016-09-07T23:30:40.849755: step 5542, loss 0.00310068, acc 1
2016-09-07T23:30:41.588159: step 5543, loss 0.0336158, acc 0.98
2016-09-07T23:30:42.278370: step 5544, loss 0.00624814, acc 1
2016-09-07T23:30:42.986643: step 5545, loss 0.0202636, acc 0.98
2016-09-07T23:30:43.677792: step 5546, loss 0.018775, acc 1
2016-09-07T23:30:44.393159: step 5547, loss 0.0110678, acc 1
2016-09-07T23:30:45.104213: step 5548, loss 0.0110304, acc 1
2016-09-07T23:30:45.807973: step 5549, loss 0.0389778, acc 0.98
2016-09-07T23:30:46.511093: step 5550, loss 0.0394621, acc 0.96
2016-09-07T23:30:47.223636: step 5551, loss 0.00646159, acc 1
2016-09-07T23:30:47.928714: step 5552, loss 0.0684275, acc 0.96
2016-09-07T23:30:48.651623: step 5553, loss 0.0222678, acc 0.98
2016-09-07T23:30:49.353419: step 5554, loss 0.0182058, acc 0.98
2016-09-07T23:30:50.074026: step 5555, loss 0.00482344, acc 1
2016-09-07T23:30:50.790423: step 5556, loss 0.000443846, acc 1
2016-09-07T23:30:51.497216: step 5557, loss 0.0205645, acc 1
2016-09-07T23:30:52.214860: step 5558, loss 0.0166505, acc 1
2016-09-07T23:30:52.928259: step 5559, loss 0.0472378, acc 0.98
2016-09-07T23:30:53.630048: step 5560, loss 0.135009, acc 0.96
2016-09-07T23:30:54.357093: step 5561, loss 0.0359566, acc 0.98
2016-09-07T23:30:55.063105: step 5562, loss 0.022823, acc 0.98
2016-09-07T23:30:55.753757: step 5563, loss 0.012696, acc 1
2016-09-07T23:30:56.447868: step 5564, loss 0.0321619, acc 0.96
2016-09-07T23:30:57.150211: step 5565, loss 0.000250349, acc 1
2016-09-07T23:30:57.855360: step 5566, loss 0.00730573, acc 1
2016-09-07T23:30:58.584826: step 5567, loss 0.0746618, acc 0.98
2016-09-07T23:30:59.310612: step 5568, loss 0.098711, acc 0.98
2016-09-07T23:31:00.027771: step 5569, loss 0.0168323, acc 0.98
2016-09-07T23:31:00.787786: step 5570, loss 0.000303639, acc 1
2016-09-07T23:31:01.500225: step 5571, loss 0.0120158, acc 1
2016-09-07T23:31:02.207449: step 5572, loss 0.106844, acc 0.96
2016-09-07T23:31:02.920827: step 5573, loss 0.00763192, acc 1
2016-09-07T23:31:03.624657: step 5574, loss 0.167544, acc 0.96
2016-09-07T23:31:04.325903: step 5575, loss 0.0459215, acc 0.98
2016-09-07T23:31:05.028524: step 5576, loss 0.0879413, acc 0.98
2016-09-07T23:31:05.738242: step 5577, loss 0.0181827, acc 0.98
2016-09-07T23:31:06.442542: step 5578, loss 0.0092173, acc 1
2016-09-07T23:31:07.153877: step 5579, loss 0.0224496, acc 1
2016-09-07T23:31:07.844701: step 5580, loss 0.0365787, acc 0.98
2016-09-07T23:31:08.568657: step 5581, loss 0.0339178, acc 0.98
2016-09-07T23:31:09.284961: step 5582, loss 0.0969887, acc 0.98
2016-09-07T23:31:09.994311: step 5583, loss 0.0296706, acc 0.98
2016-09-07T23:31:10.697308: step 5584, loss 0.0223431, acc 0.98
2016-09-07T23:31:11.412212: step 5585, loss 0.0497929, acc 0.98
2016-09-07T23:31:12.133310: step 5586, loss 0.0207243, acc 1
2016-09-07T23:31:12.828534: step 5587, loss 0.052579, acc 0.96
2016-09-07T23:31:13.533400: step 5588, loss 0.00397237, acc 1
2016-09-07T23:31:14.228879: step 5589, loss 0.0255848, acc 0.98
2016-09-07T23:31:14.928316: step 5590, loss 0.0211512, acc 0.98
2016-09-07T23:31:15.621993: step 5591, loss 0.0178936, acc 0.98
2016-09-07T23:31:16.336112: step 5592, loss 0.0280932, acc 0.98
2016-09-07T23:31:17.037276: step 5593, loss 0.0240952, acc 0.98
2016-09-07T23:31:17.760312: step 5594, loss 0.0218528, acc 1
2016-09-07T23:31:18.439456: step 5595, loss 0.00115866, acc 1
2016-09-07T23:31:19.182505: step 5596, loss 0.0444836, acc 0.98
2016-09-07T23:31:19.899003: step 5597, loss 0.0903995, acc 0.96
2016-09-07T23:31:20.608520: step 5598, loss 0.0161912, acc 0.98
2016-09-07T23:31:21.309960: step 5599, loss 0.000576991, acc 1
2016-09-07T23:31:22.028434: step 5600, loss 0.0245999, acc 1

Evaluation:
2016-09-07T23:31:25.494555: step 5600, loss 2.37024, acc 0.728

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-5600

2016-09-07T23:31:27.346363: step 5601, loss 0.0104908, acc 1
2016-09-07T23:31:28.053888: step 5602, loss 0.0480731, acc 0.98
2016-09-07T23:31:28.791654: step 5603, loss 0.0710075, acc 0.96
2016-09-07T23:31:29.508497: step 5604, loss 0.0289809, acc 0.98
2016-09-07T23:31:30.214905: step 5605, loss 0.0616119, acc 0.96
2016-09-07T23:31:30.930994: step 5606, loss 0.025948, acc 0.98
2016-09-07T23:31:31.634022: step 5607, loss 0.0328362, acc 1
2016-09-07T23:31:32.334210: step 5608, loss 0.036748, acc 0.98
2016-09-07T23:31:33.051257: step 5609, loss 0.0014538, acc 1
2016-09-07T23:31:33.756397: step 5610, loss 0.0438198, acc 0.96
2016-09-07T23:31:34.464907: step 5611, loss 0.0220473, acc 0.98
2016-09-07T23:31:35.179780: step 5612, loss 0.0278856, acc 0.98
2016-09-07T23:31:35.883486: step 5613, loss 0.00716724, acc 1
2016-09-07T23:31:36.594913: step 5614, loss 0.0914431, acc 0.98
2016-09-07T23:31:37.299628: step 5615, loss 0.0147419, acc 1
2016-09-07T23:31:37.985228: step 5616, loss 0.0509207, acc 0.98
2016-09-07T23:31:38.694146: step 5617, loss 0.00392356, acc 1
2016-09-07T23:31:39.405755: step 5618, loss 0.00462249, acc 1
2016-09-07T23:31:40.128229: step 5619, loss 0.0270689, acc 1
2016-09-07T23:31:40.836315: step 5620, loss 0.000568759, acc 1
2016-09-07T23:31:41.537416: step 5621, loss 0.0513163, acc 0.96
2016-09-07T23:31:42.257399: step 5622, loss 0.0396145, acc 1
2016-09-07T23:31:42.964154: step 5623, loss 0.0464916, acc 0.96
2016-09-07T23:31:43.679430: step 5624, loss 0.0994448, acc 0.96
2016-09-07T23:31:44.397411: step 5625, loss 0.00816215, acc 1
2016-09-07T23:31:44.781998: step 5626, loss 0.0602736, acc 1
2016-09-07T23:31:45.494107: step 5627, loss 0.034305, acc 0.98
2016-09-07T23:31:46.204849: step 5628, loss 0.00858605, acc 1
2016-09-07T23:31:46.923154: step 5629, loss 0.0325952, acc 1
2016-09-07T23:31:47.612029: step 5630, loss 0.023791, acc 0.98
2016-09-07T23:31:48.309656: step 5631, loss 0.0435174, acc 0.98
2016-09-07T23:31:49.009044: step 5632, loss 0.0131481, acc 1
2016-09-07T23:31:49.728529: step 5633, loss 0.0222963, acc 0.98
2016-09-07T23:31:50.437087: step 5634, loss 0.0132991, acc 1
2016-09-07T23:31:51.151826: step 5635, loss 0.00270094, acc 1
2016-09-07T23:31:51.864079: step 5636, loss 0.00519181, acc 1
2016-09-07T23:31:52.559053: step 5637, loss 0.0839062, acc 0.98
2016-09-07T23:31:53.249765: step 5638, loss 0.0142926, acc 1
2016-09-07T23:31:53.967233: step 5639, loss 0.0325866, acc 0.98
2016-09-07T23:31:54.659188: step 5640, loss 0.0504497, acc 0.98
2016-09-07T23:31:55.352556: step 5641, loss 0.000541411, acc 1
2016-09-07T23:31:56.053580: step 5642, loss 0.0141659, acc 1
2016-09-07T23:31:56.769785: step 5643, loss 0.0827238, acc 0.94
2016-09-07T23:31:57.491826: step 5644, loss 0.0177042, acc 1
2016-09-07T23:31:58.190822: step 5645, loss 0.052048, acc 0.96
2016-09-07T23:31:58.908090: step 5646, loss 5.11158e-05, acc 1
2016-09-07T23:31:59.604232: step 5647, loss 0.104449, acc 0.96
2016-09-07T23:32:00.332015: step 5648, loss 0.000413402, acc 1
2016-09-07T23:32:01.028900: step 5649, loss 0.0340182, acc 0.98
2016-09-07T23:32:01.707470: step 5650, loss 0.0182788, acc 0.98
2016-09-07T23:32:02.399070: step 5651, loss 0.0328536, acc 0.98
2016-09-07T23:32:03.093923: step 5652, loss 0.00899972, acc 1
2016-09-07T23:32:03.792782: step 5653, loss 0.00463074, acc 1
2016-09-07T23:32:04.515154: step 5654, loss 0.00146001, acc 1
2016-09-07T23:32:05.213025: step 5655, loss 0.0228499, acc 1
2016-09-07T23:32:05.905219: step 5656, loss 0.0102533, acc 1
2016-09-07T23:32:06.606817: step 5657, loss 0.0063254, acc 1
2016-09-07T23:32:07.341300: step 5658, loss 0.0547354, acc 1
2016-09-07T23:32:08.029944: step 5659, loss 0.0079267, acc 1
2016-09-07T23:32:08.731470: step 5660, loss 0.0843428, acc 0.94
2016-09-07T23:32:09.446258: step 5661, loss 0.00292821, acc 1
2016-09-07T23:32:10.147297: step 5662, loss 0.0167896, acc 0.98
2016-09-07T23:32:10.841924: step 5663, loss 0.0441815, acc 0.98
2016-09-07T23:32:11.541130: step 5664, loss 0.0152129, acc 1
2016-09-07T23:32:12.245375: step 5665, loss 0.0359616, acc 0.98
2016-09-07T23:32:12.932415: step 5666, loss 0.0214512, acc 0.98
2016-09-07T23:32:13.642314: step 5667, loss 0.00885006, acc 1
2016-09-07T23:32:14.341143: step 5668, loss 0.011773, acc 1
2016-09-07T23:32:15.067370: step 5669, loss 0.0214764, acc 1
2016-09-07T23:32:15.760372: step 5670, loss 0.00087974, acc 1
2016-09-07T23:32:16.475364: step 5671, loss 0.0148346, acc 1
2016-09-07T23:32:17.180958: step 5672, loss 0.0773026, acc 0.98
2016-09-07T23:32:17.879226: step 5673, loss 0.0303637, acc 0.98
2016-09-07T23:32:18.576013: step 5674, loss 0.0461059, acc 0.96
2016-09-07T23:32:19.304185: step 5675, loss 0.00151463, acc 1
2016-09-07T23:32:20.019680: step 5676, loss 0.0378075, acc 0.98
2016-09-07T23:32:20.733150: step 5677, loss 0.00734482, acc 1
2016-09-07T23:32:21.435448: step 5678, loss 0.0109037, acc 1
2016-09-07T23:32:22.126572: step 5679, loss 0.0395662, acc 0.98
2016-09-07T23:32:22.831065: step 5680, loss 0.0101703, acc 1
2016-09-07T23:32:23.514079: step 5681, loss 0.0137076, acc 1
2016-09-07T23:32:24.206314: step 5682, loss 0.0376278, acc 0.98
2016-09-07T23:32:24.906468: step 5683, loss 0.0122073, acc 1
2016-09-07T23:32:25.610295: step 5684, loss 0.175076, acc 0.98
2016-09-07T23:32:26.310415: step 5685, loss 0.0223751, acc 0.98
2016-09-07T23:32:27.010152: step 5686, loss 0.0012437, acc 1
2016-09-07T23:32:27.697894: step 5687, loss 0.0123237, acc 1
2016-09-07T23:32:28.386593: step 5688, loss 0.0165875, acc 1
2016-09-07T23:32:29.133933: step 5689, loss 0.00318243, acc 1
2016-09-07T23:32:29.842424: step 5690, loss 0.0183583, acc 1
2016-09-07T23:32:30.536942: step 5691, loss 0.0130614, acc 1
2016-09-07T23:32:31.239520: step 5692, loss 0.0295234, acc 0.98
2016-09-07T23:32:31.939293: step 5693, loss 0.0202983, acc 1
2016-09-07T23:32:32.649313: step 5694, loss 0.0122668, acc 1
2016-09-07T23:32:33.349101: step 5695, loss 0.0347507, acc 0.98
2016-09-07T23:32:34.052842: step 5696, loss 0.00298877, acc 1
2016-09-07T23:32:34.739864: step 5697, loss 0.0083774, acc 1
2016-09-07T23:32:35.428772: step 5698, loss 0.0195788, acc 0.98
2016-09-07T23:32:36.125717: step 5699, loss 0.000372261, acc 1
2016-09-07T23:32:36.854500: step 5700, loss 0.00662198, acc 1

Evaluation:
2016-09-07T23:32:39.931657: step 5700, loss 2.67794, acc 0.732

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-5700

2016-09-07T23:32:41.683264: step 5701, loss 0.0139455, acc 0.98
2016-09-07T23:32:42.386424: step 5702, loss 0.068243, acc 0.94
2016-09-07T23:32:43.062428: step 5703, loss 0.0226872, acc 0.98
2016-09-07T23:32:43.753260: step 5704, loss 0.000130241, acc 1
2016-09-07T23:32:44.437726: step 5705, loss 0.0216168, acc 0.98
2016-09-07T23:32:45.131175: step 5706, loss 0.035223, acc 0.98
2016-09-07T23:32:45.819541: step 5707, loss 0.0288074, acc 0.98
2016-09-07T23:32:46.511920: step 5708, loss 0.160484, acc 0.96
2016-09-07T23:32:47.222170: step 5709, loss 0.00677721, acc 1
2016-09-07T23:32:47.919836: step 5710, loss 0.00137949, acc 1
2016-09-07T23:32:48.605067: step 5711, loss 0.00268924, acc 1
2016-09-07T23:32:49.301521: step 5712, loss 0.0248109, acc 0.98
2016-09-07T23:32:50.008525: step 5713, loss 0.00823344, acc 1
2016-09-07T23:32:50.708429: step 5714, loss 0.032072, acc 0.98
2016-09-07T23:32:51.403075: step 5715, loss 0.0222506, acc 1
2016-09-07T23:32:52.081335: step 5716, loss 0.0413554, acc 0.96
2016-09-07T23:32:52.761663: step 5717, loss 0.000577628, acc 1
2016-09-07T23:32:53.445496: step 5718, loss 0.0525106, acc 0.96
2016-09-07T23:32:54.167015: step 5719, loss 0.0043916, acc 1
2016-09-07T23:32:54.875780: step 5720, loss 0.106228, acc 0.96
2016-09-07T23:32:55.604252: step 5721, loss 0.106014, acc 0.94
2016-09-07T23:32:56.307257: step 5722, loss 0.0121756, acc 1
2016-09-07T23:32:57.011852: step 5723, loss 0.0319393, acc 0.98
2016-09-07T23:32:57.715689: step 5724, loss 0.0682149, acc 0.94
2016-09-07T23:32:58.440933: step 5725, loss 0.0584375, acc 0.98
2016-09-07T23:32:59.144283: step 5726, loss 0.00928353, acc 1
2016-09-07T23:32:59.835017: step 5727, loss 0.0124399, acc 1
2016-09-07T23:33:00.592917: step 5728, loss 0.0342923, acc 0.98
2016-09-07T23:33:01.300506: step 5729, loss 0.00357774, acc 1
2016-09-07T23:33:02.004533: step 5730, loss 0.0152036, acc 1
2016-09-07T23:33:02.692659: step 5731, loss 0.00483122, acc 1
2016-09-07T23:33:03.379437: step 5732, loss 0.00526737, acc 1
2016-09-07T23:33:04.095822: step 5733, loss 0.083584, acc 0.96
2016-09-07T23:33:04.801529: step 5734, loss 0.0203727, acc 0.98
2016-09-07T23:33:05.498758: step 5735, loss 0.00991474, acc 1
2016-09-07T23:33:06.181849: step 5736, loss 0.0113791, acc 1
2016-09-07T23:33:06.884419: step 5737, loss 0.0518385, acc 0.98
2016-09-07T23:33:07.585846: step 5738, loss 0.0191787, acc 0.98
2016-09-07T23:33:08.270221: step 5739, loss 0.0434195, acc 0.98
2016-09-07T23:33:08.977745: step 5740, loss 0.0340523, acc 0.98
2016-09-07T23:33:09.680274: step 5741, loss 0.0121394, acc 1
2016-09-07T23:33:10.396030: step 5742, loss 0.0257622, acc 1
2016-09-07T23:33:11.105822: step 5743, loss 0.0171524, acc 1
2016-09-07T23:33:11.807774: step 5744, loss 0.027291, acc 0.98
2016-09-07T23:33:12.511013: step 5745, loss 0.0759233, acc 0.98
2016-09-07T23:33:13.235069: step 5746, loss 0.0305011, acc 0.98
2016-09-07T23:33:13.923499: step 5747, loss 0.0393741, acc 0.98
2016-09-07T23:33:14.631288: step 5748, loss 0.0394753, acc 0.98
2016-09-07T23:33:15.323537: step 5749, loss 0.00450063, acc 1
2016-09-07T23:33:16.031676: step 5750, loss 0.027838, acc 0.98
2016-09-07T23:33:16.739020: step 5751, loss 0.132526, acc 0.96
2016-09-07T23:33:17.448601: step 5752, loss 0.0616792, acc 0.98
2016-09-07T23:33:18.180030: step 5753, loss 0.0335754, acc 0.98
2016-09-07T23:33:18.868633: step 5754, loss 0.0114348, acc 1
2016-09-07T23:33:19.557942: step 5755, loss 0.0354643, acc 0.98
2016-09-07T23:33:20.251933: step 5756, loss 0.0282028, acc 1
2016-09-07T23:33:20.948232: step 5757, loss 0.0322794, acc 1
2016-09-07T23:33:21.655771: step 5758, loss 0.034528, acc 1
2016-09-07T23:33:22.353492: step 5759, loss 0.00373138, acc 1
2016-09-07T23:33:23.056873: step 5760, loss 0.0532597, acc 0.98
2016-09-07T23:33:23.769775: step 5761, loss 0.0252644, acc 0.98
2016-09-07T23:33:24.494660: step 5762, loss 0.0914575, acc 0.98
2016-09-07T23:33:25.207609: step 5763, loss 0.0263722, acc 0.98
2016-09-07T23:33:25.895516: step 5764, loss 0.0264467, acc 0.98
2016-09-07T23:33:26.579951: step 5765, loss 0.0765319, acc 0.96
2016-09-07T23:33:27.289158: step 5766, loss 0.00676381, acc 1
2016-09-07T23:33:27.984581: step 5767, loss 0.00233975, acc 1
2016-09-07T23:33:28.687433: step 5768, loss 0.0100768, acc 1
2016-09-07T23:33:29.376824: step 5769, loss 0.0432823, acc 0.98
2016-09-07T23:33:30.094326: step 5770, loss 0.0794372, acc 0.94
2016-09-07T23:33:30.784491: step 5771, loss 0.0146241, acc 0.98
2016-09-07T23:33:31.492545: step 5772, loss 0.114734, acc 0.98
2016-09-07T23:33:32.203843: step 5773, loss 0.0151664, acc 1
2016-09-07T23:33:32.894414: step 5774, loss 0.0814072, acc 0.94
2016-09-07T23:33:33.593907: step 5775, loss 0.002674, acc 1
2016-09-07T23:33:34.298269: step 5776, loss 0.0158795, acc 1
2016-09-07T23:33:35.038571: step 5777, loss 0.0143779, acc 1
2016-09-07T23:33:35.726292: step 5778, loss 0.0340501, acc 0.98
2016-09-07T23:33:36.432623: step 5779, loss 0.043186, acc 0.96
2016-09-07T23:33:37.126803: step 5780, loss 0.0769516, acc 0.96
2016-09-07T23:33:37.831738: step 5781, loss 0.0473407, acc 0.96
2016-09-07T23:33:38.535402: step 5782, loss 0.000756726, acc 1
2016-09-07T23:33:39.244588: step 5783, loss 0.020942, acc 1
2016-09-07T23:33:39.966201: step 5784, loss 0.0142038, acc 1
2016-09-07T23:33:40.673981: step 5785, loss 0.0707461, acc 0.96
2016-09-07T23:33:41.421704: step 5786, loss 0.0299966, acc 1
2016-09-07T23:33:42.114421: step 5787, loss 0.0903212, acc 0.98
2016-09-07T23:33:42.827907: step 5788, loss 0.0162923, acc 0.98
2016-09-07T23:33:43.538810: step 5789, loss 0.00895257, acc 1
2016-09-07T23:33:44.232864: step 5790, loss 0.0659297, acc 0.98
2016-09-07T23:33:44.939191: step 5791, loss 0.0031727, acc 1
2016-09-07T23:33:45.652825: step 5792, loss 0.0201453, acc 1
2016-09-07T23:33:46.359659: step 5793, loss 0.00836287, acc 1
2016-09-07T23:33:47.064803: step 5794, loss 0.0217099, acc 0.98
2016-09-07T23:33:47.772865: step 5795, loss 0.154205, acc 0.94
2016-09-07T23:33:48.474344: step 5796, loss 0.0656401, acc 0.96
2016-09-07T23:33:49.204991: step 5797, loss 0.0251386, acc 1
2016-09-07T23:33:49.916409: step 5798, loss 0.103494, acc 0.96
2016-09-07T23:33:50.604618: step 5799, loss 0.0754486, acc 0.98
2016-09-07T23:33:51.312512: step 5800, loss 0.0133798, acc 1

Evaluation:
2016-09-07T23:33:54.363254: step 5800, loss 2.09302, acc 0.742

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473258106/checkpoints/model-5800

2016-09-07T23:33:56.084610: step 5801, loss 0.0202959, acc 1
2016-09-07T23:33:56.770646: step 5802, loss 0.0150768, acc 1
2016-09-07T23:33:57.463302: step 5803, loss 0.00132642, acc 1
2016-09-07T23:33:58.185108: step 5804, loss 0.0646157, acc 0.96
2016-09-07T23:33:58.896877: step 5805, loss 0.0303174, acc 0.98
2016-09-07T23:33:59.597479: step 5806, loss 0.0131465, acc 1
2016-09-07T23:34:00.317686: step 5807, loss 0.0236503, acc 0.98
2016-09-07T23:34:01.021773: step 5808, loss 0.0117512, acc 1
2016-09-07T23:34:01.714474: step 5809, loss 0.0179241, acc 1
2016-09-07T23:34:02.419502: step 5810, loss 0.00963623, acc 1
2016-09-07T23:34:03.124133: step 5811, loss 0.0060611, acc 1
2016-09-07T23:34:03.820163: step 5812, loss 0.242663, acc 0.96
2016-09-07T23:34:04.517586: step 5813, loss 0.113885, acc 0.94
2016-09-07T23:34:05.234222: step 5814, loss 0.00234409, acc 1
2016-09-07T23:34:05.929782: step 5815, loss 0.103213, acc 0.96
2016-09-07T23:34:06.617266: step 5816, loss 0.0323955, acc 0.98
2016-09-07T23:34:07.307640: step 5817, loss 0.0159426, acc 1
2016-09-07T23:34:08.024617: step 5818, loss 0.0148071, acc 1
2016-09-07T23:34:08.716937: step 5819, loss 0.000157581, acc 1
2016-09-07T23:34:09.096025: step 5820, loss 0.015662, acc 1

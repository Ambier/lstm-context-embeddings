WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f65c24a8e90>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.
WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f65c24a8e50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.

Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=50
CHECKPOINT_EVERY=100
CV_INDEX=3
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=300
EVALUATE_EVERY=100
FILTER_SIZES=3,4,5
HIDDEN_DIM=300
L2_REG_LAMBDA=0.15
LOG_DEVICE_PLACEMENT=False
NUM_EPOCHS=100
NUM_FILTERS=100
WORD2VEC=GoogleNews-vectors-negative300.bin

Loading data...
Vocabulary Size: 18758
Train/Dev split: 9594/1066
Writing to /home/cil/lstm-context-embeddings/runs/1473139646

Load word2vec file GoogleNews-vectors-negative300.bin

2016-09-06T13:27:46.793457: step 1, loss 0.693147, acc 0.46
2016-09-06T13:27:47.614274: step 2, loss 0.694735, acc 0.46
2016-09-06T13:27:48.458632: step 3, loss 0.690537, acc 0.52
2016-09-06T13:27:49.289952: step 4, loss 0.702335, acc 0.5
2016-09-06T13:27:50.115424: step 5, loss 0.695993, acc 0.48
2016-09-06T13:27:50.900096: step 6, loss 0.694633, acc 0.46
2016-09-06T13:27:51.718340: step 7, loss 0.696004, acc 0.46
2016-09-06T13:27:52.527372: step 8, loss 0.695861, acc 0.52
2016-09-06T13:27:53.302099: step 9, loss 0.736103, acc 0.4
2016-09-06T13:27:54.122184: step 10, loss 0.701434, acc 0.5
2016-09-06T13:27:54.950032: step 11, loss 0.723247, acc 0.42
2016-09-06T13:27:55.741195: step 12, loss 0.71087, acc 0.4
2016-09-06T13:27:56.539343: step 13, loss 0.685402, acc 0.58
2016-09-06T13:27:57.338009: step 14, loss 0.699494, acc 0.52
2016-09-06T13:27:58.182191: step 15, loss 0.685355, acc 0.52
2016-09-06T13:27:59.002627: step 16, loss 0.713337, acc 0.42
2016-09-06T13:27:59.805460: step 17, loss 0.707755, acc 0.38
2016-09-06T13:28:00.607510: step 18, loss 0.690311, acc 0.46
2016-09-06T13:28:01.413871: step 19, loss 0.673976, acc 0.6
2016-09-06T13:28:02.259945: step 20, loss 0.720651, acc 0.44
2016-09-06T13:28:03.035771: step 21, loss 0.718558, acc 0.5
2016-09-06T13:28:03.859531: step 22, loss 0.685206, acc 0.58
2016-09-06T13:28:04.655400: step 23, loss 0.664655, acc 0.64
2016-09-06T13:28:05.455214: step 24, loss 0.709897, acc 0.46
2016-09-06T13:28:06.248091: step 25, loss 0.704396, acc 0.5
2016-09-06T13:28:07.054324: step 26, loss 0.6439, acc 0.66
2016-09-06T13:28:07.858438: step 27, loss 0.656282, acc 0.64
2016-09-06T13:28:08.677196: step 28, loss 0.677163, acc 0.54
2016-09-06T13:28:09.503228: step 29, loss 0.620537, acc 0.68
2016-09-06T13:28:10.274176: step 30, loss 0.659772, acc 0.62
2016-09-06T13:28:11.074206: step 31, loss 0.595677, acc 0.72
2016-09-06T13:28:11.902476: step 32, loss 0.667963, acc 0.62
2016-09-06T13:28:12.693119: step 33, loss 0.582969, acc 0.68
2016-09-06T13:28:13.498256: step 34, loss 0.687241, acc 0.66
2016-09-06T13:28:14.346504: step 35, loss 0.533976, acc 0.74
2016-09-06T13:28:15.133454: step 36, loss 0.650148, acc 0.66
2016-09-06T13:28:15.918070: step 37, loss 0.694626, acc 0.62
2016-09-06T13:28:16.728247: step 38, loss 0.533746, acc 0.76
2016-09-06T13:28:17.515598: step 39, loss 0.678565, acc 0.62
2016-09-06T13:28:18.317837: step 40, loss 0.687659, acc 0.66
2016-09-06T13:28:19.119106: step 41, loss 0.568331, acc 0.72
2016-09-06T13:28:19.906834: step 42, loss 0.643954, acc 0.66
2016-09-06T13:28:20.694131: step 43, loss 0.605227, acc 0.72
2016-09-06T13:28:21.500250: step 44, loss 0.772638, acc 0.54
2016-09-06T13:28:22.320429: step 45, loss 0.698632, acc 0.58
2016-09-06T13:28:23.134810: step 46, loss 0.605069, acc 0.76
2016-09-06T13:28:23.941680: step 47, loss 0.637815, acc 0.68
2016-09-06T13:28:24.740238: step 48, loss 0.599783, acc 0.74
2016-09-06T13:28:25.557301: step 49, loss 0.642836, acc 0.62
2016-09-06T13:28:26.379307: step 50, loss 0.601119, acc 0.54
2016-09-06T13:28:27.167089: step 51, loss 0.669636, acc 0.58
2016-09-06T13:28:27.968954: step 52, loss 0.528074, acc 0.76
2016-09-06T13:28:28.775061: step 53, loss 0.511434, acc 0.8
2016-09-06T13:28:29.579475: step 54, loss 0.60695, acc 0.68
2016-09-06T13:28:30.387501: step 55, loss 0.744881, acc 0.68
2016-09-06T13:28:31.204102: step 56, loss 0.696699, acc 0.66
2016-09-06T13:28:31.996011: step 57, loss 0.560402, acc 0.68
2016-09-06T13:28:32.796123: step 58, loss 0.443964, acc 0.82
2016-09-06T13:28:33.602687: step 59, loss 0.658668, acc 0.74
2016-09-06T13:28:34.395343: step 60, loss 0.862286, acc 0.54
2016-09-06T13:28:35.235115: step 61, loss 0.571333, acc 0.66
2016-09-06T13:28:36.043926: step 62, loss 0.536234, acc 0.76
2016-09-06T13:28:36.848624: step 63, loss 0.603189, acc 0.66
2016-09-06T13:28:37.650507: step 64, loss 0.71126, acc 0.58
2016-09-06T13:28:38.479157: step 65, loss 0.61314, acc 0.7
2016-09-06T13:28:39.287009: step 66, loss 0.746657, acc 0.52
2016-09-06T13:28:40.107516: step 67, loss 0.610429, acc 0.64
2016-09-06T13:28:40.938553: step 68, loss 0.644468, acc 0.64
2016-09-06T13:28:41.731699: step 69, loss 0.623085, acc 0.68
2016-09-06T13:28:42.536886: step 70, loss 0.587219, acc 0.72
2016-09-06T13:28:43.329884: step 71, loss 0.601294, acc 0.72
2016-09-06T13:28:44.132921: step 72, loss 0.586036, acc 0.64
2016-09-06T13:28:44.933911: step 73, loss 0.70029, acc 0.54
2016-09-06T13:28:45.777587: step 74, loss 0.636421, acc 0.52
2016-09-06T13:28:46.565543: step 75, loss 0.597717, acc 0.68
2016-09-06T13:28:47.354563: step 76, loss 0.635026, acc 0.66
2016-09-06T13:28:48.187936: step 77, loss 0.500796, acc 0.8
2016-09-06T13:28:48.960249: step 78, loss 0.590512, acc 0.74
2016-09-06T13:28:49.755636: step 79, loss 0.620697, acc 0.64
2016-09-06T13:28:50.591362: step 80, loss 0.55901, acc 0.76
2016-09-06T13:28:51.361972: step 81, loss 0.661299, acc 0.68
2016-09-06T13:28:52.182635: step 82, loss 0.555431, acc 0.72
2016-09-06T13:28:53.022697: step 83, loss 0.626828, acc 0.68
2016-09-06T13:28:53.792686: step 84, loss 0.41719, acc 0.84
2016-09-06T13:28:54.609406: step 85, loss 0.585041, acc 0.7
2016-09-06T13:28:55.402562: step 86, loss 0.475534, acc 0.82
2016-09-06T13:28:56.180810: step 87, loss 0.512015, acc 0.76
2016-09-06T13:28:56.985075: step 88, loss 0.527541, acc 0.72
2016-09-06T13:28:57.794445: step 89, loss 0.516581, acc 0.7
2016-09-06T13:28:58.590066: step 90, loss 0.578646, acc 0.72
2016-09-06T13:28:59.388303: step 91, loss 0.51771, acc 0.74
2016-09-06T13:29:00.195121: step 92, loss 0.424365, acc 0.78
2016-09-06T13:29:01.003076: step 93, loss 0.516802, acc 0.72
2016-09-06T13:29:01.805451: step 94, loss 0.492846, acc 0.78
2016-09-06T13:29:02.626791: step 95, loss 0.682961, acc 0.62
2016-09-06T13:29:03.397816: step 96, loss 0.463809, acc 0.72
2016-09-06T13:29:04.207467: step 97, loss 0.611778, acc 0.66
2016-09-06T13:29:05.028363: step 98, loss 0.625259, acc 0.64
2016-09-06T13:29:05.837011: step 99, loss 0.3943, acc 0.8
2016-09-06T13:29:06.666030: step 100, loss 0.593817, acc 0.7

Evaluation:
2016-09-06T13:29:10.399009: step 100, loss 0.482618, acc 0.782364

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-100

2016-09-06T13:29:12.342419: step 101, loss 0.675888, acc 0.62
2016-09-06T13:29:13.164687: step 102, loss 0.505885, acc 0.72
2016-09-06T13:29:14.012258: step 103, loss 0.563249, acc 0.72
2016-09-06T13:29:14.895058: step 104, loss 0.488678, acc 0.78
2016-09-06T13:29:15.670819: step 105, loss 0.667562, acc 0.64
2016-09-06T13:29:16.463733: step 106, loss 0.556185, acc 0.76
2016-09-06T13:29:17.301394: step 107, loss 0.509394, acc 0.76
2016-09-06T13:29:18.094374: step 108, loss 0.549286, acc 0.74
2016-09-06T13:29:18.900275: step 109, loss 0.525439, acc 0.78
2016-09-06T13:29:19.741088: step 110, loss 0.557899, acc 0.72
2016-09-06T13:29:20.531340: step 111, loss 0.549386, acc 0.76
2016-09-06T13:29:21.315673: step 112, loss 0.493162, acc 0.78
2016-09-06T13:29:22.148937: step 113, loss 0.506788, acc 0.82
2016-09-06T13:29:22.932613: step 114, loss 0.584964, acc 0.66
2016-09-06T13:29:23.745401: step 115, loss 0.573195, acc 0.74
2016-09-06T13:29:24.587650: step 116, loss 0.58943, acc 0.74
2016-09-06T13:29:25.371598: step 117, loss 0.661971, acc 0.72
2016-09-06T13:29:26.147972: step 118, loss 0.423209, acc 0.84
2016-09-06T13:29:26.977154: step 119, loss 0.351062, acc 0.88
2016-09-06T13:29:27.753580: step 120, loss 0.445251, acc 0.76
2016-09-06T13:29:28.555374: step 121, loss 0.55717, acc 0.74
2016-09-06T13:29:29.387070: step 122, loss 0.598327, acc 0.68
2016-09-06T13:29:30.169412: step 123, loss 0.584401, acc 0.72
2016-09-06T13:29:30.950367: step 124, loss 0.47161, acc 0.72
2016-09-06T13:29:31.785123: step 125, loss 0.56285, acc 0.7
2016-09-06T13:29:32.579297: step 126, loss 0.507803, acc 0.76
2016-09-06T13:29:33.385957: step 127, loss 0.55013, acc 0.68
2016-09-06T13:29:34.205130: step 128, loss 0.42554, acc 0.8
2016-09-06T13:29:34.987309: step 129, loss 0.535582, acc 0.78
2016-09-06T13:29:35.804451: step 130, loss 0.627853, acc 0.7
2016-09-06T13:29:36.633662: step 131, loss 0.673898, acc 0.68
2016-09-06T13:29:37.394875: step 132, loss 0.491822, acc 0.84
2016-09-06T13:29:38.190437: step 133, loss 0.464923, acc 0.84
2016-09-06T13:29:38.994644: step 134, loss 0.529997, acc 0.72
2016-09-06T13:29:39.788328: step 135, loss 0.490565, acc 0.76
2016-09-06T13:29:40.637684: step 136, loss 0.441884, acc 0.78
2016-09-06T13:29:41.475924: step 137, loss 0.507396, acc 0.72
2016-09-06T13:29:42.240665: step 138, loss 0.490191, acc 0.76
2016-09-06T13:29:43.032712: step 139, loss 0.390467, acc 0.88
2016-09-06T13:29:43.848128: step 140, loss 0.564584, acc 0.7
2016-09-06T13:29:44.681977: step 141, loss 0.514351, acc 0.78
2016-09-06T13:29:45.526037: step 142, loss 0.437921, acc 0.8
2016-09-06T13:29:46.297955: step 143, loss 0.414748, acc 0.74
2016-09-06T13:29:47.083141: step 144, loss 0.540848, acc 0.7
2016-09-06T13:29:47.908786: step 145, loss 0.533778, acc 0.72
2016-09-06T13:29:48.738411: step 146, loss 0.708666, acc 0.58
2016-09-06T13:29:49.517533: step 147, loss 0.563392, acc 0.76
2016-09-06T13:29:50.309647: step 148, loss 0.586632, acc 0.72
2016-09-06T13:29:51.134130: step 149, loss 0.504488, acc 0.8
2016-09-06T13:29:51.925201: step 150, loss 0.411759, acc 0.82
2016-09-06T13:29:52.745365: step 151, loss 0.533698, acc 0.76
2016-09-06T13:29:53.565055: step 152, loss 0.432206, acc 0.82
2016-09-06T13:29:54.392981: step 153, loss 0.498075, acc 0.78
2016-09-06T13:29:55.196604: step 154, loss 0.52223, acc 0.7
2016-09-06T13:29:56.004284: step 155, loss 0.412554, acc 0.84
2016-09-06T13:29:56.843430: step 156, loss 0.434537, acc 0.78
2016-09-06T13:29:57.639898: step 157, loss 0.56419, acc 0.74
2016-09-06T13:29:58.448233: step 158, loss 0.479454, acc 0.72
2016-09-06T13:29:59.277281: step 159, loss 0.455526, acc 0.76
2016-09-06T13:30:00.097940: step 160, loss 0.486468, acc 0.76
2016-09-06T13:30:00.945950: step 161, loss 0.56971, acc 0.78
2016-09-06T13:30:01.767409: step 162, loss 0.39544, acc 0.86
2016-09-06T13:30:02.571242: step 163, loss 0.524795, acc 0.76
2016-09-06T13:30:03.399921: step 164, loss 0.671073, acc 0.64
2016-09-06T13:30:04.228793: step 165, loss 0.531946, acc 0.76
2016-09-06T13:30:05.046396: step 166, loss 0.471515, acc 0.76
2016-09-06T13:30:05.880410: step 167, loss 0.504604, acc 0.8
2016-09-06T13:30:06.700222: step 168, loss 0.400627, acc 0.82
2016-09-06T13:30:07.540632: step 169, loss 0.480442, acc 0.8
2016-09-06T13:30:08.386085: step 170, loss 0.713325, acc 0.72
2016-09-06T13:30:09.195130: step 171, loss 0.464994, acc 0.78
2016-09-06T13:30:09.997696: step 172, loss 0.566654, acc 0.74
2016-09-06T13:30:10.806317: step 173, loss 0.574567, acc 0.7
2016-09-06T13:30:11.601465: step 174, loss 0.440226, acc 0.8
2016-09-06T13:30:12.413419: step 175, loss 0.476831, acc 0.72
2016-09-06T13:30:13.212154: step 176, loss 0.512492, acc 0.74
2016-09-06T13:30:14.038485: step 177, loss 0.567421, acc 0.72
2016-09-06T13:30:14.844286: step 178, loss 0.518145, acc 0.72
2016-09-06T13:30:15.702707: step 179, loss 0.428618, acc 0.82
2016-09-06T13:30:16.534306: step 180, loss 0.502957, acc 0.74
2016-09-06T13:30:17.309435: step 181, loss 0.550756, acc 0.7
2016-09-06T13:30:18.152818: step 182, loss 0.555904, acc 0.72
2016-09-06T13:30:18.969142: step 183, loss 0.479217, acc 0.78
2016-09-06T13:30:19.729191: step 184, loss 0.374734, acc 0.84
2016-09-06T13:30:20.544890: step 185, loss 0.426113, acc 0.76
2016-09-06T13:30:21.354229: step 186, loss 0.51943, acc 0.72
2016-09-06T13:30:22.151747: step 187, loss 0.661154, acc 0.68
2016-09-06T13:30:22.954267: step 188, loss 0.611966, acc 0.72
2016-09-06T13:30:23.798629: step 189, loss 0.35228, acc 0.84
2016-09-06T13:30:24.568992: step 190, loss 0.442613, acc 0.78
2016-09-06T13:30:25.380778: step 191, loss 0.417877, acc 0.78
2016-09-06T13:30:26.134739: step 192, loss 0.408209, acc 0.795455
2016-09-06T13:30:26.948616: step 193, loss 0.431517, acc 0.78
2016-09-06T13:30:27.776661: step 194, loss 0.346254, acc 0.86
2016-09-06T13:30:28.606615: step 195, loss 0.397407, acc 0.8
2016-09-06T13:30:29.392557: step 196, loss 0.502964, acc 0.8
2016-09-06T13:30:30.209733: step 197, loss 0.426771, acc 0.82
2016-09-06T13:30:31.019373: step 198, loss 0.367299, acc 0.84
2016-09-06T13:30:31.806560: step 199, loss 0.443915, acc 0.76
2016-09-06T13:30:32.583791: step 200, loss 0.311036, acc 0.92

Evaluation:
2016-09-06T13:30:36.320337: step 200, loss 0.421757, acc 0.805816

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-200

2016-09-06T13:30:38.198791: step 201, loss 0.379093, acc 0.8
2016-09-06T13:30:38.995681: step 202, loss 0.31783, acc 0.82
2016-09-06T13:30:39.817500: step 203, loss 0.308573, acc 0.9
2016-09-06T13:30:40.627634: step 204, loss 0.373568, acc 0.88
2016-09-06T13:30:41.444192: step 205, loss 0.284411, acc 0.86
2016-09-06T13:30:42.266293: step 206, loss 0.266658, acc 0.92
2016-09-06T13:30:43.081055: step 207, loss 0.34253, acc 0.86
2016-09-06T13:30:43.895516: step 208, loss 0.281349, acc 0.88
2016-09-06T13:30:44.722748: step 209, loss 0.332471, acc 0.84
2016-09-06T13:30:45.527710: step 210, loss 0.351152, acc 0.78
2016-09-06T13:30:46.338854: step 211, loss 0.543088, acc 0.76
2016-09-06T13:30:47.148788: step 212, loss 0.326665, acc 0.84
2016-09-06T13:30:47.986927: step 213, loss 0.412276, acc 0.84
2016-09-06T13:30:48.794995: step 214, loss 0.307311, acc 0.88
2016-09-06T13:30:49.614160: step 215, loss 0.209665, acc 0.96
2016-09-06T13:30:50.490434: step 216, loss 0.50726, acc 0.74
2016-09-06T13:30:51.272182: step 217, loss 0.302538, acc 0.92
2016-09-06T13:30:52.078918: step 218, loss 0.344997, acc 0.82
2016-09-06T13:30:52.903254: step 219, loss 0.336427, acc 0.84
2016-09-06T13:30:53.688625: step 220, loss 0.348416, acc 0.84
2016-09-06T13:30:54.507897: step 221, loss 0.307172, acc 0.86
2016-09-06T13:30:55.315122: step 222, loss 0.229183, acc 0.94
2016-09-06T13:30:56.107782: step 223, loss 0.416872, acc 0.84
2016-09-06T13:30:56.928968: step 224, loss 0.431847, acc 0.76
2016-09-06T13:30:57.758365: step 225, loss 0.368688, acc 0.82
2016-09-06T13:30:58.533602: step 226, loss 0.27535, acc 0.88
2016-09-06T13:30:59.344740: step 227, loss 0.261026, acc 0.88
2016-09-06T13:31:00.166336: step 228, loss 0.374418, acc 0.8
2016-09-06T13:31:01.004402: step 229, loss 0.360017, acc 0.82
2016-09-06T13:31:01.808738: step 230, loss 0.33345, acc 0.82
2016-09-06T13:31:02.609912: step 231, loss 0.527855, acc 0.74
2016-09-06T13:31:03.401101: step 232, loss 0.357308, acc 0.78
2016-09-06T13:31:04.205795: step 233, loss 0.269781, acc 0.9
2016-09-06T13:31:05.018750: step 234, loss 0.363016, acc 0.82
2016-09-06T13:31:05.797800: step 235, loss 0.252636, acc 0.88
2016-09-06T13:31:06.579908: step 236, loss 0.299626, acc 0.88
2016-09-06T13:31:07.389126: step 237, loss 0.36901, acc 0.78
2016-09-06T13:31:08.188202: step 238, loss 0.411416, acc 0.9
2016-09-06T13:31:09.000452: step 239, loss 0.570642, acc 0.8
2016-09-06T13:31:09.842120: step 240, loss 0.33427, acc 0.86
2016-09-06T13:31:10.614962: step 241, loss 0.433456, acc 0.8
2016-09-06T13:31:11.447016: step 242, loss 0.29058, acc 0.9
2016-09-06T13:31:12.263367: step 243, loss 0.389243, acc 0.78
2016-09-06T13:31:13.100308: step 244, loss 0.265127, acc 0.86
2016-09-06T13:31:13.906158: step 245, loss 0.495299, acc 0.7
2016-09-06T13:31:14.740323: step 246, loss 0.548808, acc 0.8
2016-09-06T13:31:15.540812: step 247, loss 0.460284, acc 0.82
2016-09-06T13:31:16.355440: step 248, loss 0.447333, acc 0.78
2016-09-06T13:31:17.172048: step 249, loss 0.4183, acc 0.78
2016-09-06T13:31:17.963909: step 250, loss 0.372434, acc 0.84
2016-09-06T13:31:18.781578: step 251, loss 0.388782, acc 0.8
2016-09-06T13:31:19.614939: step 252, loss 0.350172, acc 0.88
2016-09-06T13:31:20.421652: step 253, loss 0.483467, acc 0.82
2016-09-06T13:31:21.231006: step 254, loss 0.506803, acc 0.78
2016-09-06T13:31:22.058823: step 255, loss 0.363583, acc 0.9
2016-09-06T13:31:22.875754: step 256, loss 0.409772, acc 0.78
2016-09-06T13:31:23.696205: step 257, loss 0.553555, acc 0.78
2016-09-06T13:31:24.524466: step 258, loss 0.404179, acc 0.82
2016-09-06T13:31:25.355860: step 259, loss 0.319214, acc 0.8
2016-09-06T13:31:26.156209: step 260, loss 0.378126, acc 0.82
2016-09-06T13:31:26.981536: step 261, loss 0.580407, acc 0.74
2016-09-06T13:31:27.787083: step 262, loss 0.45361, acc 0.76
2016-09-06T13:31:28.583765: step 263, loss 0.333154, acc 0.82
2016-09-06T13:31:29.418762: step 264, loss 0.449706, acc 0.74
2016-09-06T13:31:30.237339: step 265, loss 0.340041, acc 0.8
2016-09-06T13:31:31.037816: step 266, loss 0.408194, acc 0.78
2016-09-06T13:31:31.873887: step 267, loss 0.39977, acc 0.86
2016-09-06T13:31:32.719259: step 268, loss 0.40642, acc 0.86
2016-09-06T13:31:33.537013: step 269, loss 0.28545, acc 0.9
2016-09-06T13:31:34.343209: step 270, loss 0.567666, acc 0.68
2016-09-06T13:31:35.148737: step 271, loss 0.307652, acc 0.86
2016-09-06T13:31:35.943092: step 272, loss 0.466108, acc 0.82
2016-09-06T13:31:36.746815: step 273, loss 0.205557, acc 0.94
2016-09-06T13:31:37.566062: step 274, loss 0.377476, acc 0.88
2016-09-06T13:31:38.366336: step 275, loss 0.277413, acc 0.88
2016-09-06T13:31:39.192029: step 276, loss 0.289267, acc 0.92
2016-09-06T13:31:39.990469: step 277, loss 0.303989, acc 0.86
2016-09-06T13:31:40.809728: step 278, loss 0.338508, acc 0.86
2016-09-06T13:31:41.634352: step 279, loss 0.467186, acc 0.8
2016-09-06T13:31:42.446214: step 280, loss 0.300625, acc 0.9
2016-09-06T13:31:43.241574: step 281, loss 0.461986, acc 0.82
2016-09-06T13:31:44.052594: step 282, loss 0.470585, acc 0.78
2016-09-06T13:31:44.868550: step 283, loss 0.327431, acc 0.88
2016-09-06T13:31:45.657817: step 284, loss 0.37267, acc 0.84
2016-09-06T13:31:46.448925: step 285, loss 0.204701, acc 0.94
2016-09-06T13:31:47.265249: step 286, loss 0.472557, acc 0.84
2016-09-06T13:31:48.070621: step 287, loss 0.402859, acc 0.82
2016-09-06T13:31:48.892888: step 288, loss 0.409846, acc 0.78
2016-09-06T13:31:49.718470: step 289, loss 0.373205, acc 0.82
2016-09-06T13:31:50.534451: step 290, loss 0.285537, acc 0.88
2016-09-06T13:31:51.338688: step 291, loss 0.300668, acc 0.86
2016-09-06T13:31:52.147198: step 292, loss 0.390797, acc 0.84
2016-09-06T13:31:52.952508: step 293, loss 0.414033, acc 0.78
2016-09-06T13:31:53.750252: step 294, loss 0.458063, acc 0.8
2016-09-06T13:31:54.560355: step 295, loss 0.319684, acc 0.94
2016-09-06T13:31:55.337571: step 296, loss 0.454759, acc 0.84
2016-09-06T13:31:56.124661: step 297, loss 0.258822, acc 0.9
2016-09-06T13:31:56.936586: step 298, loss 0.395408, acc 0.78
2016-09-06T13:31:57.735564: step 299, loss 0.297818, acc 0.92
2016-09-06T13:31:58.532085: step 300, loss 0.498246, acc 0.8

Evaluation:
2016-09-06T13:32:02.312081: step 300, loss 0.443586, acc 0.803002

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-300

2016-09-06T13:32:04.181385: step 301, loss 0.415353, acc 0.78
2016-09-06T13:32:04.982576: step 302, loss 0.393831, acc 0.82
2016-09-06T13:32:05.834608: step 303, loss 0.410672, acc 0.82
2016-09-06T13:32:06.650259: step 304, loss 0.246497, acc 0.9
2016-09-06T13:32:07.441284: step 305, loss 0.480844, acc 0.76
2016-09-06T13:32:08.233557: step 306, loss 0.440803, acc 0.86
2016-09-06T13:32:09.029394: step 307, loss 0.447766, acc 0.82
2016-09-06T13:32:09.816723: step 308, loss 0.291779, acc 0.88
2016-09-06T13:32:10.632095: step 309, loss 0.49889, acc 0.74
2016-09-06T13:32:11.438171: step 310, loss 0.406335, acc 0.82
2016-09-06T13:32:12.244403: step 311, loss 0.364206, acc 0.86
2016-09-06T13:32:13.096353: step 312, loss 0.320768, acc 0.86
2016-09-06T13:32:13.926685: step 313, loss 0.345042, acc 0.84
2016-09-06T13:32:14.698418: step 314, loss 0.306293, acc 0.88
2016-09-06T13:32:15.507849: step 315, loss 0.380065, acc 0.84
2016-09-06T13:32:16.328834: step 316, loss 0.36671, acc 0.84
2016-09-06T13:32:17.115969: step 317, loss 0.450918, acc 0.76
2016-09-06T13:32:17.916176: step 318, loss 0.367011, acc 0.82
2016-09-06T13:32:18.737023: step 319, loss 0.38962, acc 0.82
2016-09-06T13:32:19.545594: step 320, loss 0.351969, acc 0.8
2016-09-06T13:32:20.349991: step 321, loss 0.44812, acc 0.84
2016-09-06T13:32:21.161281: step 322, loss 0.322525, acc 0.8
2016-09-06T13:32:21.951128: step 323, loss 0.535739, acc 0.76
2016-09-06T13:32:22.740038: step 324, loss 0.284172, acc 0.9
2016-09-06T13:32:23.547726: step 325, loss 0.502666, acc 0.82
2016-09-06T13:32:24.320822: step 326, loss 0.399358, acc 0.82
2016-09-06T13:32:25.141377: step 327, loss 0.394842, acc 0.82
2016-09-06T13:32:26.006926: step 328, loss 0.265381, acc 0.88
2016-09-06T13:32:26.805391: step 329, loss 0.26236, acc 0.86
2016-09-06T13:32:27.594759: step 330, loss 0.302618, acc 0.84
2016-09-06T13:32:28.417506: step 331, loss 0.387019, acc 0.86
2016-09-06T13:32:29.189880: step 332, loss 0.421344, acc 0.76
2016-09-06T13:32:29.968579: step 333, loss 0.459961, acc 0.82
2016-09-06T13:32:30.773691: step 334, loss 0.356091, acc 0.88
2016-09-06T13:32:31.553160: step 335, loss 0.424305, acc 0.84
2016-09-06T13:32:32.378318: step 336, loss 0.332474, acc 0.82
2016-09-06T13:32:33.169522: step 337, loss 0.290476, acc 0.88
2016-09-06T13:32:33.965482: step 338, loss 0.334473, acc 0.88
2016-09-06T13:32:34.809372: step 339, loss 0.396915, acc 0.76
2016-09-06T13:32:35.619065: step 340, loss 0.254324, acc 0.9
2016-09-06T13:32:36.397346: step 341, loss 0.377806, acc 0.88
2016-09-06T13:32:37.196612: step 342, loss 0.319175, acc 0.84
2016-09-06T13:32:38.001227: step 343, loss 0.231759, acc 0.92
2016-09-06T13:32:38.811982: step 344, loss 0.325925, acc 0.88
2016-09-06T13:32:39.619124: step 345, loss 0.432435, acc 0.8
2016-09-06T13:32:40.461122: step 346, loss 0.251452, acc 0.84
2016-09-06T13:32:41.253406: step 347, loss 0.443423, acc 0.8
2016-09-06T13:32:42.045981: step 348, loss 0.254311, acc 0.9
2016-09-06T13:32:42.863648: step 349, loss 0.470303, acc 0.76
2016-09-06T13:32:43.681440: step 350, loss 0.258272, acc 0.86
2016-09-06T13:32:44.490067: step 351, loss 0.378401, acc 0.84
2016-09-06T13:32:45.301881: step 352, loss 0.379056, acc 0.8
2016-09-06T13:32:46.087691: step 353, loss 0.163237, acc 0.96
2016-09-06T13:32:46.895270: step 354, loss 0.345959, acc 0.84
2016-09-06T13:32:47.705519: step 355, loss 0.410348, acc 0.84
2016-09-06T13:32:48.485868: step 356, loss 0.356674, acc 0.82
2016-09-06T13:32:49.297720: step 357, loss 0.375834, acc 0.82
2016-09-06T13:32:50.088090: step 358, loss 0.50169, acc 0.82
2016-09-06T13:32:50.892340: step 359, loss 0.389596, acc 0.82
2016-09-06T13:32:51.714748: step 360, loss 0.38756, acc 0.8
2016-09-06T13:32:52.501943: step 361, loss 0.423218, acc 0.7
2016-09-06T13:32:53.301037: step 362, loss 0.452266, acc 0.8
2016-09-06T13:32:54.112265: step 363, loss 0.447367, acc 0.82
2016-09-06T13:32:54.909200: step 364, loss 0.46271, acc 0.74
2016-09-06T13:32:55.694165: step 365, loss 0.261929, acc 0.92
2016-09-06T13:32:56.538881: step 366, loss 0.31546, acc 0.88
2016-09-06T13:32:57.349786: step 367, loss 0.459044, acc 0.76
2016-09-06T13:32:58.142178: step 368, loss 0.435321, acc 0.78
2016-09-06T13:32:58.940203: step 369, loss 0.422702, acc 0.78
2016-09-06T13:32:59.753606: step 370, loss 0.364282, acc 0.84
2016-09-06T13:33:00.589231: step 371, loss 0.373102, acc 0.9
2016-09-06T13:33:01.383568: step 372, loss 0.360428, acc 0.84
2016-09-06T13:33:02.179144: step 373, loss 0.445743, acc 0.76
2016-09-06T13:33:03.007759: step 374, loss 0.38753, acc 0.82
2016-09-06T13:33:03.840445: step 375, loss 0.334426, acc 0.88
2016-09-06T13:33:04.647823: step 376, loss 0.461832, acc 0.76
2016-09-06T13:33:05.445681: step 377, loss 0.296698, acc 0.88
2016-09-06T13:33:06.245184: step 378, loss 0.437785, acc 0.82
2016-09-06T13:33:07.046468: step 379, loss 0.4571, acc 0.74
2016-09-06T13:33:07.850629: step 380, loss 0.608324, acc 0.76
2016-09-06T13:33:08.648975: step 381, loss 0.322961, acc 0.82
2016-09-06T13:33:09.469761: step 382, loss 0.471874, acc 0.78
2016-09-06T13:33:10.274029: step 383, loss 0.362926, acc 0.84
2016-09-06T13:33:11.026512: step 384, loss 0.359573, acc 0.818182
2016-09-06T13:33:11.857583: step 385, loss 0.245765, acc 0.9
2016-09-06T13:33:12.634060: step 386, loss 0.257409, acc 0.9
2016-09-06T13:33:13.460940: step 387, loss 0.239893, acc 0.92
2016-09-06T13:33:14.252503: step 388, loss 0.237812, acc 0.92
2016-09-06T13:33:15.071720: step 389, loss 0.220822, acc 0.92
2016-09-06T13:33:15.870767: step 390, loss 0.239892, acc 0.9
2016-09-06T13:33:16.698401: step 391, loss 0.151351, acc 0.92
2016-09-06T13:33:17.492958: step 392, loss 0.254247, acc 0.92
2016-09-06T13:33:18.308233: step 393, loss 0.227435, acc 0.9
2016-09-06T13:33:19.121410: step 394, loss 0.196706, acc 0.94
2016-09-06T13:33:19.913198: step 395, loss 0.114893, acc 0.98
2016-09-06T13:33:20.703436: step 396, loss 0.123587, acc 0.96
2016-09-06T13:33:21.526851: step 397, loss 0.404331, acc 0.84
2016-09-06T13:33:22.319982: step 398, loss 0.275942, acc 0.92
2016-09-06T13:33:23.149972: step 399, loss 0.245807, acc 0.86
2016-09-06T13:33:23.968703: step 400, loss 0.324882, acc 0.86

Evaluation:
2016-09-06T13:33:27.699782: step 400, loss 0.466535, acc 0.807692

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-400

2016-09-06T13:33:29.537368: step 401, loss 0.436139, acc 0.86
2016-09-06T13:33:30.373653: step 402, loss 0.199933, acc 0.92
2016-09-06T13:33:31.219605: step 403, loss 0.314368, acc 0.84
2016-09-06T13:33:32.033360: step 404, loss 0.190428, acc 0.88
2016-09-06T13:33:32.850670: step 405, loss 0.215378, acc 0.9
2016-09-06T13:33:33.656574: step 406, loss 0.276974, acc 0.86
2016-09-06T13:33:34.484088: step 407, loss 0.303355, acc 0.9
2016-09-06T13:33:35.312895: step 408, loss 0.170841, acc 0.94
2016-09-06T13:33:36.120317: step 409, loss 0.214112, acc 0.92
2016-09-06T13:33:36.898130: step 410, loss 0.182893, acc 0.92
2016-09-06T13:33:37.712174: step 411, loss 0.207292, acc 0.9
2016-09-06T13:33:38.512320: step 412, loss 0.374538, acc 0.82
2016-09-06T13:33:39.314019: step 413, loss 0.327804, acc 0.82
2016-09-06T13:33:40.133774: step 414, loss 0.290835, acc 0.86
2016-09-06T13:33:40.945252: step 415, loss 0.16797, acc 0.94
2016-09-06T13:33:41.734092: step 416, loss 0.246816, acc 0.92
2016-09-06T13:33:42.559442: step 417, loss 0.192979, acc 0.9
2016-09-06T13:33:43.369449: step 418, loss 0.290038, acc 0.86
2016-09-06T13:33:44.154814: step 419, loss 0.238538, acc 0.84
2016-09-06T13:33:44.971761: step 420, loss 0.177831, acc 0.92
2016-09-06T13:33:45.790886: step 421, loss 0.236402, acc 0.88
2016-09-06T13:33:46.622360: step 422, loss 0.231937, acc 0.88
2016-09-06T13:33:47.414929: step 423, loss 0.133245, acc 0.94
2016-09-06T13:33:48.217853: step 424, loss 0.213127, acc 0.94
2016-09-06T13:33:48.996001: step 425, loss 0.15567, acc 0.96
2016-09-06T13:33:49.802967: step 426, loss 0.202385, acc 0.94
2016-09-06T13:33:50.626090: step 427, loss 0.230651, acc 0.86
2016-09-06T13:33:51.414083: step 428, loss 0.253164, acc 0.94
2016-09-06T13:33:52.236811: step 429, loss 0.154199, acc 0.96
2016-09-06T13:33:53.055028: step 430, loss 0.667978, acc 0.8
2016-09-06T13:33:53.855075: step 431, loss 0.242376, acc 0.88
2016-09-06T13:33:54.659575: step 432, loss 0.260194, acc 0.88
2016-09-06T13:33:55.480782: step 433, loss 0.21922, acc 0.9
2016-09-06T13:33:56.255956: step 434, loss 0.167475, acc 0.96
2016-09-06T13:33:57.078868: step 435, loss 0.172255, acc 0.9
2016-09-06T13:33:57.919200: step 436, loss 0.27282, acc 0.9
2016-09-06T13:33:58.704742: step 437, loss 0.277244, acc 0.9
2016-09-06T13:33:59.512075: step 438, loss 0.301315, acc 0.9
2016-09-06T13:34:00.357400: step 439, loss 0.17488, acc 0.96
2016-09-06T13:34:01.145564: step 440, loss 0.209995, acc 0.94
2016-09-06T13:34:01.995810: step 441, loss 0.191542, acc 0.94
2016-09-06T13:34:02.831754: step 442, loss 0.322745, acc 0.86
2016-09-06T13:34:03.634741: step 443, loss 0.401677, acc 0.84
2016-09-06T13:34:04.438064: step 444, loss 0.191666, acc 0.88
2016-09-06T13:34:05.268696: step 445, loss 0.214202, acc 0.92
2016-09-06T13:34:06.105465: step 446, loss 0.326272, acc 0.88
2016-09-06T13:34:06.926554: step 447, loss 0.270879, acc 0.92
2016-09-06T13:34:07.776142: step 448, loss 0.356206, acc 0.86
2016-09-06T13:34:08.590764: step 449, loss 0.137913, acc 0.94
2016-09-06T13:34:09.434121: step 450, loss 0.240426, acc 0.88
2016-09-06T13:34:10.269072: step 451, loss 0.303524, acc 0.88
2016-09-06T13:34:11.083196: step 452, loss 0.352035, acc 0.86
2016-09-06T13:34:11.898440: step 453, loss 0.22368, acc 0.86
2016-09-06T13:34:12.747004: step 454, loss 0.297489, acc 0.88
2016-09-06T13:34:13.567551: step 455, loss 0.359346, acc 0.84
2016-09-06T13:34:14.381615: step 456, loss 0.345267, acc 0.86
2016-09-06T13:34:15.207485: step 457, loss 0.127689, acc 0.92
2016-09-06T13:34:16.043995: step 458, loss 0.172573, acc 0.96
2016-09-06T13:34:16.816497: step 459, loss 0.235912, acc 0.9
2016-09-06T13:34:17.631716: step 460, loss 0.310408, acc 0.88
2016-09-06T13:34:18.433981: step 461, loss 0.249333, acc 0.9
2016-09-06T13:34:19.219396: step 462, loss 0.351131, acc 0.92
2016-09-06T13:34:20.050085: step 463, loss 0.188989, acc 0.92
2016-09-06T13:34:20.891672: step 464, loss 0.304966, acc 0.88
2016-09-06T13:34:21.684230: step 465, loss 0.299005, acc 0.9
2016-09-06T13:34:22.493199: step 466, loss 0.211094, acc 0.92
2016-09-06T13:34:23.285372: step 467, loss 0.182154, acc 0.92
2016-09-06T13:34:24.079672: step 468, loss 0.144033, acc 0.92
2016-09-06T13:34:24.869627: step 469, loss 0.222716, acc 0.9
2016-09-06T13:34:25.704778: step 470, loss 0.342166, acc 0.86
2016-09-06T13:34:26.496595: step 471, loss 0.191533, acc 0.92
2016-09-06T13:34:27.313872: step 472, loss 0.267925, acc 0.96
2016-09-06T13:34:28.114144: step 473, loss 0.340483, acc 0.86
2016-09-06T13:34:28.910976: step 474, loss 0.249873, acc 0.92
2016-09-06T13:34:29.725873: step 475, loss 0.149459, acc 0.96
2016-09-06T13:34:30.545960: step 476, loss 0.188681, acc 0.92
2016-09-06T13:34:31.352125: step 477, loss 0.187414, acc 0.96
2016-09-06T13:34:32.145569: step 478, loss 0.296889, acc 0.86
2016-09-06T13:34:32.963461: step 479, loss 0.35207, acc 0.86
2016-09-06T13:34:33.775098: step 480, loss 0.196945, acc 0.92
2016-09-06T13:34:34.606271: step 481, loss 0.185857, acc 0.92
2016-09-06T13:34:35.402345: step 482, loss 0.195818, acc 0.94
2016-09-06T13:34:36.160104: step 483, loss 0.276367, acc 0.88
2016-09-06T13:34:36.962112: step 484, loss 0.118276, acc 0.96
2016-09-06T13:34:37.777518: step 485, loss 0.304492, acc 0.88
2016-09-06T13:34:38.550522: step 486, loss 0.264406, acc 0.92
2016-09-06T13:34:39.364305: step 487, loss 0.24494, acc 0.88
2016-09-06T13:34:40.169683: step 488, loss 0.290803, acc 0.88
2016-09-06T13:34:40.977713: step 489, loss 0.329408, acc 0.86
2016-09-06T13:34:41.781823: step 490, loss 0.363333, acc 0.84
2016-09-06T13:34:42.567670: step 491, loss 0.336022, acc 0.88
2016-09-06T13:34:43.371715: step 492, loss 0.25733, acc 0.92
2016-09-06T13:34:44.195200: step 493, loss 0.229477, acc 0.86
2016-09-06T13:34:45.007250: step 494, loss 0.356953, acc 0.86
2016-09-06T13:34:45.787715: step 495, loss 0.34712, acc 0.86
2016-09-06T13:34:46.610321: step 496, loss 0.372551, acc 0.82
2016-09-06T13:34:47.448471: step 497, loss 0.200219, acc 0.92
2016-09-06T13:34:48.219582: step 498, loss 0.2366, acc 0.88
2016-09-06T13:34:49.035826: step 499, loss 0.294181, acc 0.86
2016-09-06T13:34:49.860830: step 500, loss 0.298942, acc 0.84

Evaluation:
2016-09-06T13:34:53.593739: step 500, loss 0.437353, acc 0.794559

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-500

2016-09-06T13:34:55.480935: step 501, loss 0.337028, acc 0.86
2016-09-06T13:34:56.308684: step 502, loss 0.224032, acc 0.92
2016-09-06T13:34:57.129526: step 503, loss 0.262219, acc 0.88
2016-09-06T13:34:57.950238: step 504, loss 0.254613, acc 0.86
2016-09-06T13:34:58.777403: step 505, loss 0.299943, acc 0.88
2016-09-06T13:34:59.567119: step 506, loss 0.189608, acc 0.94
2016-09-06T13:35:00.399073: step 507, loss 0.221534, acc 0.9
2016-09-06T13:35:01.226748: step 508, loss 0.317094, acc 0.82
2016-09-06T13:35:02.063794: step 509, loss 0.230255, acc 0.94
2016-09-06T13:35:02.854374: step 510, loss 0.17098, acc 0.96
2016-09-06T13:35:03.653868: step 511, loss 0.282337, acc 0.82
2016-09-06T13:35:04.481236: step 512, loss 0.218026, acc 0.9
2016-09-06T13:35:05.248464: step 513, loss 0.242816, acc 0.96
2016-09-06T13:35:06.066416: step 514, loss 0.35992, acc 0.8
2016-09-06T13:35:06.889919: step 515, loss 0.395575, acc 0.82
2016-09-06T13:35:07.707506: step 516, loss 0.323597, acc 0.82
2016-09-06T13:35:08.489351: step 517, loss 0.155052, acc 0.96
2016-09-06T13:35:09.305195: step 518, loss 0.287907, acc 0.82
2016-09-06T13:35:10.107590: step 519, loss 0.368028, acc 0.82
2016-09-06T13:35:10.900344: step 520, loss 0.244286, acc 0.92
2016-09-06T13:35:11.713638: step 521, loss 0.240157, acc 0.9
2016-09-06T13:35:12.512867: step 522, loss 0.379238, acc 0.86
2016-09-06T13:35:13.307285: step 523, loss 0.178773, acc 0.92
2016-09-06T13:35:14.107412: step 524, loss 0.308014, acc 0.94
2016-09-06T13:35:14.904912: step 525, loss 0.261042, acc 0.88
2016-09-06T13:35:15.734123: step 526, loss 0.223361, acc 0.96
2016-09-06T13:35:16.529031: step 527, loss 0.325953, acc 0.92
2016-09-06T13:35:17.335401: step 528, loss 0.236658, acc 0.9
2016-09-06T13:35:18.180876: step 529, loss 0.40306, acc 0.8
2016-09-06T13:35:19.026510: step 530, loss 0.33211, acc 0.88
2016-09-06T13:35:19.831187: step 531, loss 0.184251, acc 0.94
2016-09-06T13:35:20.641678: step 532, loss 0.446431, acc 0.84
2016-09-06T13:35:21.464312: step 533, loss 0.330594, acc 0.8
2016-09-06T13:35:22.223005: step 534, loss 0.32497, acc 0.84
2016-09-06T13:35:23.021294: step 535, loss 0.190563, acc 0.92
2016-09-06T13:35:23.823560: step 536, loss 0.395124, acc 0.86
2016-09-06T13:35:24.645395: step 537, loss 0.226592, acc 0.94
2016-09-06T13:35:25.418886: step 538, loss 0.272451, acc 0.88
2016-09-06T13:35:26.260942: step 539, loss 0.2243, acc 0.96
2016-09-06T13:35:27.049523: step 540, loss 0.275345, acc 0.86
2016-09-06T13:35:27.869878: step 541, loss 0.257335, acc 0.9
2016-09-06T13:35:28.682879: step 542, loss 0.424811, acc 0.82
2016-09-06T13:35:29.467324: step 543, loss 0.160963, acc 0.94
2016-09-06T13:35:30.285651: step 544, loss 0.303093, acc 0.88
2016-09-06T13:35:31.101710: step 545, loss 0.342469, acc 0.88
2016-09-06T13:35:31.870393: step 546, loss 0.252648, acc 0.92
2016-09-06T13:35:32.670291: step 547, loss 0.276953, acc 0.86
2016-09-06T13:35:33.482892: step 548, loss 0.355754, acc 0.88
2016-09-06T13:35:34.259051: step 549, loss 0.441876, acc 0.86
2016-09-06T13:35:35.049762: step 550, loss 0.325856, acc 0.84
2016-09-06T13:35:35.878423: step 551, loss 0.107642, acc 0.94
2016-09-06T13:35:36.669456: step 552, loss 0.280933, acc 0.86
2016-09-06T13:35:37.464982: step 553, loss 0.161583, acc 0.96
2016-09-06T13:35:38.272956: step 554, loss 0.320505, acc 0.88
2016-09-06T13:35:39.081726: step 555, loss 0.234844, acc 0.9
2016-09-06T13:35:39.891909: step 556, loss 0.201885, acc 0.92
2016-09-06T13:35:40.682502: step 557, loss 0.193971, acc 0.9
2016-09-06T13:35:41.485343: step 558, loss 0.202984, acc 0.9
2016-09-06T13:35:42.294595: step 559, loss 0.277548, acc 0.9
2016-09-06T13:35:43.102649: step 560, loss 0.308665, acc 0.88
2016-09-06T13:35:43.918087: step 561, loss 0.164819, acc 0.96
2016-09-06T13:35:44.732005: step 562, loss 0.172882, acc 0.96
2016-09-06T13:35:45.555209: step 563, loss 0.288817, acc 0.88
2016-09-06T13:35:46.366575: step 564, loss 0.215109, acc 0.9
2016-09-06T13:35:47.171491: step 565, loss 0.366459, acc 0.86
2016-09-06T13:35:47.982937: step 566, loss 0.352344, acc 0.86
2016-09-06T13:35:48.777471: step 567, loss 0.218634, acc 0.94
2016-09-06T13:35:49.595969: step 568, loss 0.248737, acc 0.88
2016-09-06T13:35:50.388709: step 569, loss 0.364421, acc 0.84
2016-09-06T13:35:51.182079: step 570, loss 0.117514, acc 0.96
2016-09-06T13:35:51.976096: step 571, loss 0.291489, acc 0.9
2016-09-06T13:35:52.769747: step 572, loss 0.311656, acc 0.88
2016-09-06T13:35:53.564123: step 573, loss 0.453452, acc 0.74
2016-09-06T13:35:54.395274: step 574, loss 0.166412, acc 0.92
2016-09-06T13:35:55.245050: step 575, loss 0.288054, acc 0.86
2016-09-06T13:35:55.988272: step 576, loss 0.224019, acc 0.909091
2016-09-06T13:35:56.786753: step 577, loss 0.161227, acc 0.96
2016-09-06T13:35:57.642108: step 578, loss 0.109042, acc 0.96
2016-09-06T13:35:58.433372: step 579, loss 0.147989, acc 0.94
2016-09-06T13:35:59.242316: step 580, loss 0.184854, acc 0.94
2016-09-06T13:36:00.080083: step 581, loss 0.195025, acc 0.9
2016-09-06T13:36:00.926709: step 582, loss 0.13708, acc 0.94
2016-09-06T13:36:01.727610: step 583, loss 0.198621, acc 0.94
2016-09-06T13:36:02.523577: step 584, loss 0.133753, acc 0.96
2016-09-06T13:36:03.263958: step 585, loss 0.257088, acc 0.88
2016-09-06T13:36:04.083193: step 586, loss 0.151805, acc 0.92
2016-09-06T13:36:04.898504: step 587, loss 0.196291, acc 0.94
2016-09-06T13:36:05.702627: step 588, loss 0.143563, acc 0.92
2016-09-06T13:36:06.517015: step 589, loss 0.168544, acc 0.92
2016-09-06T13:36:07.345353: step 590, loss 0.111844, acc 0.96
2016-09-06T13:36:08.122195: step 591, loss 0.0977145, acc 0.96
2016-09-06T13:36:08.917728: step 592, loss 0.180608, acc 0.9
2016-09-06T13:36:09.748063: step 593, loss 0.32638, acc 0.9
2016-09-06T13:36:10.532180: step 594, loss 0.214415, acc 0.9
2016-09-06T13:36:11.323187: step 595, loss 0.121882, acc 0.94
2016-09-06T13:36:12.142606: step 596, loss 0.0885949, acc 0.98
2016-09-06T13:36:12.946777: step 597, loss 0.08756, acc 0.96
2016-09-06T13:36:13.752549: step 598, loss 0.108451, acc 0.98
2016-09-06T13:36:14.585400: step 599, loss 0.13621, acc 0.94
2016-09-06T13:36:15.372493: step 600, loss 0.0667174, acc 1

Evaluation:
2016-09-06T13:36:19.127019: step 600, loss 0.571735, acc 0.79925

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-600

2016-09-06T13:36:21.004813: step 601, loss 0.207898, acc 0.96
2016-09-06T13:36:21.826274: step 602, loss 0.165307, acc 0.94
2016-09-06T13:36:22.628570: step 603, loss 0.117134, acc 0.96
2016-09-06T13:36:23.437797: step 604, loss 0.0926308, acc 0.98
2016-09-06T13:36:24.240090: step 605, loss 0.0836746, acc 0.98
2016-09-06T13:36:25.053982: step 606, loss 0.147207, acc 0.96
2016-09-06T13:36:25.888170: step 607, loss 0.177592, acc 0.92
2016-09-06T13:36:26.697315: step 608, loss 0.144017, acc 0.94
2016-09-06T13:36:27.505631: step 609, loss 0.226412, acc 0.94
2016-09-06T13:36:28.338909: step 610, loss 0.0800725, acc 0.96
2016-09-06T13:36:29.126298: step 611, loss 0.21522, acc 0.94
2016-09-06T13:36:29.936871: step 612, loss 0.272766, acc 0.9
2016-09-06T13:36:30.787150: step 613, loss 0.124642, acc 0.94
2016-09-06T13:36:31.609961: step 614, loss 0.103865, acc 0.98
2016-09-06T13:36:32.413624: step 615, loss 0.110525, acc 0.94
2016-09-06T13:36:33.205013: step 616, loss 0.162462, acc 0.92
2016-09-06T13:36:34.023648: step 617, loss 0.150661, acc 0.96
2016-09-06T13:36:34.832996: step 618, loss 0.241097, acc 0.86
2016-09-06T13:36:35.612283: step 619, loss 0.199988, acc 0.9
2016-09-06T13:36:36.406802: step 620, loss 0.289987, acc 0.92
2016-09-06T13:36:37.205926: step 621, loss 0.145658, acc 0.96
2016-09-06T13:36:38.027263: step 622, loss 0.105764, acc 0.96
2016-09-06T13:36:38.834112: step 623, loss 0.290344, acc 0.84
2016-09-06T13:36:39.626894: step 624, loss 0.107527, acc 0.96
2016-09-06T13:36:40.440012: step 625, loss 0.193151, acc 0.98
2016-09-06T13:36:41.260998: step 626, loss 0.143382, acc 0.96
2016-09-06T13:36:42.096151: step 627, loss 0.295653, acc 0.88
2016-09-06T13:36:42.897946: step 628, loss 0.147254, acc 0.98
2016-09-06T13:36:43.723386: step 629, loss 0.188032, acc 0.94
2016-09-06T13:36:44.541458: step 630, loss 0.139179, acc 0.92
2016-09-06T13:36:45.362339: step 631, loss 0.0954858, acc 0.98
2016-09-06T13:36:46.183137: step 632, loss 0.136658, acc 0.98
2016-09-06T13:36:46.976741: step 633, loss 0.157292, acc 0.96
2016-09-06T13:36:47.790587: step 634, loss 0.148432, acc 0.92
2016-09-06T13:36:48.602561: step 635, loss 0.129507, acc 0.98
2016-09-06T13:36:49.398880: step 636, loss 0.0730642, acc 0.98
2016-09-06T13:36:50.211848: step 637, loss 0.100938, acc 0.98
2016-09-06T13:36:51.038226: step 638, loss 0.0897403, acc 0.96
2016-09-06T13:36:51.838565: step 639, loss 0.282922, acc 0.9
2016-09-06T13:36:52.660076: step 640, loss 0.147235, acc 0.94
2016-09-06T13:36:53.532764: step 641, loss 0.13797, acc 0.96
2016-09-06T13:36:54.369207: step 642, loss 0.163466, acc 0.94
2016-09-06T13:36:55.161489: step 643, loss 0.123427, acc 0.92
2016-09-06T13:36:55.986357: step 644, loss 0.0977323, acc 0.96
2016-09-06T13:36:56.812223: step 645, loss 0.176027, acc 0.92
2016-09-06T13:36:57.641966: step 646, loss 0.0494591, acc 0.98
2016-09-06T13:36:58.459762: step 647, loss 0.199934, acc 0.92
2016-09-06T13:36:59.238015: step 648, loss 0.308857, acc 0.9
2016-09-06T13:37:00.039075: step 649, loss 0.13181, acc 0.94
2016-09-06T13:37:00.908067: step 650, loss 0.193798, acc 0.9
2016-09-06T13:37:01.708401: step 651, loss 0.139502, acc 0.96
2016-09-06T13:37:02.495537: step 652, loss 0.181103, acc 0.92
2016-09-06T13:37:03.331200: step 653, loss 0.241503, acc 0.86
2016-09-06T13:37:04.135955: step 654, loss 0.15743, acc 0.92
2016-09-06T13:37:04.964296: step 655, loss 0.347248, acc 0.84
2016-09-06T13:37:05.807550: step 656, loss 0.11315, acc 0.94
2016-09-06T13:37:06.615223: step 657, loss 0.127967, acc 0.94
2016-09-06T13:37:07.438473: step 658, loss 0.0660735, acc 0.96
2016-09-06T13:37:08.246227: step 659, loss 0.11951, acc 0.94
2016-09-06T13:37:09.091949: step 660, loss 0.192295, acc 0.88
2016-09-06T13:37:09.880571: step 661, loss 0.150201, acc 0.94
2016-09-06T13:37:10.682126: step 662, loss 0.214998, acc 0.9
2016-09-06T13:37:11.489269: step 663, loss 0.176324, acc 0.92
2016-09-06T13:37:12.276857: step 664, loss 0.231653, acc 0.88
2016-09-06T13:37:13.079347: step 665, loss 0.142917, acc 0.96
2016-09-06T13:37:13.905509: step 666, loss 0.347759, acc 0.9
2016-09-06T13:37:14.678404: step 667, loss 0.281512, acc 0.8
2016-09-06T13:37:15.510724: step 668, loss 0.166198, acc 0.9
2016-09-06T13:37:16.341163: step 669, loss 0.192368, acc 0.96
2016-09-06T13:37:17.127913: step 670, loss 0.0791808, acc 0.98
2016-09-06T13:37:17.932780: step 671, loss 0.0942526, acc 0.98
2016-09-06T13:37:18.771661: step 672, loss 0.135024, acc 0.96
2016-09-06T13:37:19.611052: step 673, loss 0.145374, acc 0.94
2016-09-06T13:37:20.411979: step 674, loss 0.127145, acc 0.96
2016-09-06T13:37:21.222810: step 675, loss 0.206377, acc 0.9
2016-09-06T13:37:22.011893: step 676, loss 0.252693, acc 0.9
2016-09-06T13:37:22.791624: step 677, loss 0.226888, acc 0.88
2016-09-06T13:37:23.617463: step 678, loss 0.297879, acc 0.88
2016-09-06T13:37:24.407455: step 679, loss 0.259195, acc 0.9
2016-09-06T13:37:25.232211: step 680, loss 0.0976449, acc 0.94
2016-09-06T13:37:26.055801: step 681, loss 0.201, acc 0.94
2016-09-06T13:37:26.826549: step 682, loss 0.217184, acc 0.9
2016-09-06T13:37:27.628115: step 683, loss 0.237033, acc 0.92
2016-09-06T13:37:28.480701: step 684, loss 0.198703, acc 0.92
2016-09-06T13:37:29.279267: step 685, loss 0.147119, acc 0.92
2016-09-06T13:37:30.078861: step 686, loss 0.252936, acc 0.92
2016-09-06T13:37:30.878973: step 687, loss 0.177556, acc 0.92
2016-09-06T13:37:31.651987: step 688, loss 0.0830045, acc 1
2016-09-06T13:37:32.491932: step 689, loss 0.130981, acc 0.94
2016-09-06T13:37:33.310224: step 690, loss 0.0809624, acc 1
2016-09-06T13:37:34.111953: step 691, loss 0.144192, acc 0.94
2016-09-06T13:37:34.928486: step 692, loss 0.16397, acc 0.94
2016-09-06T13:37:35.757909: step 693, loss 0.20168, acc 0.92
2016-09-06T13:37:36.552032: step 694, loss 0.285378, acc 0.88
2016-09-06T13:37:37.357305: step 695, loss 0.24035, acc 0.94
2016-09-06T13:37:38.156005: step 696, loss 0.259218, acc 0.9
2016-09-06T13:37:38.970232: step 697, loss 0.0954577, acc 0.96
2016-09-06T13:37:39.774786: step 698, loss 0.161968, acc 0.92
2016-09-06T13:37:40.577009: step 699, loss 0.205061, acc 0.96
2016-09-06T13:37:41.372969: step 700, loss 0.158955, acc 0.92

Evaluation:
2016-09-06T13:37:45.070703: step 700, loss 0.633002, acc 0.792683

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-700

2016-09-06T13:37:46.924399: step 701, loss 0.15972, acc 0.9
2016-09-06T13:37:47.719908: step 702, loss 0.326722, acc 0.8
2016-09-06T13:37:48.530450: step 703, loss 0.10435, acc 0.94
2016-09-06T13:37:49.351021: step 704, loss 0.167939, acc 0.9
2016-09-06T13:37:50.204193: step 705, loss 0.0705783, acc 1
2016-09-06T13:37:51.001665: step 706, loss 0.210204, acc 0.94
2016-09-06T13:37:51.781155: step 707, loss 0.143774, acc 0.96
2016-09-06T13:37:52.603966: step 708, loss 0.25285, acc 0.92
2016-09-06T13:37:53.431446: step 709, loss 0.154117, acc 0.92
2016-09-06T13:37:54.231574: step 710, loss 0.153178, acc 0.92
2016-09-06T13:37:55.066174: step 711, loss 0.151458, acc 0.94
2016-09-06T13:37:55.858743: step 712, loss 0.203144, acc 0.92
2016-09-06T13:37:56.659408: step 713, loss 0.12276, acc 0.94
2016-09-06T13:37:57.481503: step 714, loss 0.187899, acc 0.92
2016-09-06T13:37:58.306776: step 715, loss 0.178291, acc 0.94
2016-09-06T13:37:59.091770: step 716, loss 0.202605, acc 0.86
2016-09-06T13:37:59.930066: step 717, loss 0.163104, acc 0.92
2016-09-06T13:38:00.748348: step 718, loss 0.196125, acc 0.94
2016-09-06T13:38:01.563967: step 719, loss 0.0748537, acc 0.98
2016-09-06T13:38:02.413765: step 720, loss 0.316446, acc 0.86
2016-09-06T13:38:03.236412: step 721, loss 0.48701, acc 0.84
2016-09-06T13:38:04.043754: step 722, loss 0.340643, acc 0.9
2016-09-06T13:38:04.872002: step 723, loss 0.283519, acc 0.9
2016-09-06T13:38:05.649672: step 724, loss 0.127051, acc 0.98
2016-09-06T13:38:06.439907: step 725, loss 0.280177, acc 0.86
2016-09-06T13:38:07.287608: step 726, loss 0.216797, acc 0.92
2016-09-06T13:38:08.090032: step 727, loss 0.18821, acc 0.9
2016-09-06T13:38:08.911915: step 728, loss 0.169759, acc 0.94
2016-09-06T13:38:09.722162: step 729, loss 0.189665, acc 0.94
2016-09-06T13:38:10.522914: step 730, loss 0.208988, acc 0.88
2016-09-06T13:38:11.335911: step 731, loss 0.215466, acc 0.9
2016-09-06T13:38:12.165273: step 732, loss 0.239553, acc 0.9
2016-09-06T13:38:12.971338: step 733, loss 0.343158, acc 0.9
2016-09-06T13:38:13.784539: step 734, loss 0.305454, acc 0.84
2016-09-06T13:38:14.642810: step 735, loss 0.220923, acc 0.94
2016-09-06T13:38:15.449595: step 736, loss 0.285934, acc 0.88
2016-09-06T13:38:16.237427: step 737, loss 0.268329, acc 0.88
2016-09-06T13:38:17.054157: step 738, loss 0.217413, acc 0.9
2016-09-06T13:38:17.875133: step 739, loss 0.193648, acc 0.92
2016-09-06T13:38:18.720109: step 740, loss 0.152745, acc 0.92
2016-09-06T13:38:19.553386: step 741, loss 0.197807, acc 0.9
2016-09-06T13:38:20.341435: step 742, loss 0.165152, acc 0.9
2016-09-06T13:38:21.164202: step 743, loss 0.135458, acc 0.96
2016-09-06T13:38:21.980700: step 744, loss 0.14835, acc 0.96
2016-09-06T13:38:22.785719: step 745, loss 0.263347, acc 0.82
2016-09-06T13:38:23.596696: step 746, loss 0.137036, acc 0.96
2016-09-06T13:38:24.410635: step 747, loss 0.110966, acc 0.96
2016-09-06T13:38:25.219574: step 748, loss 0.157144, acc 0.92
2016-09-06T13:38:26.073702: step 749, loss 0.165136, acc 0.96
2016-09-06T13:38:26.905370: step 750, loss 0.209834, acc 0.9
2016-09-06T13:38:27.727769: step 751, loss 0.180769, acc 0.92
2016-09-06T13:38:28.516642: step 752, loss 0.10768, acc 0.94
2016-09-06T13:38:29.298670: step 753, loss 0.280389, acc 0.88
2016-09-06T13:38:30.139132: step 754, loss 0.209296, acc 0.88
2016-09-06T13:38:30.936064: step 755, loss 0.0906933, acc 0.96
2016-09-06T13:38:31.719472: step 756, loss 0.236191, acc 0.86
2016-09-06T13:38:32.557447: step 757, loss 0.253599, acc 0.92
2016-09-06T13:38:33.359524: step 758, loss 0.137165, acc 0.96
2016-09-06T13:38:34.150149: step 759, loss 0.219459, acc 0.94
2016-09-06T13:38:34.966232: step 760, loss 0.31907, acc 0.84
2016-09-06T13:38:35.734975: step 761, loss 0.187266, acc 0.94
2016-09-06T13:38:36.540177: step 762, loss 0.226145, acc 0.88
2016-09-06T13:38:37.340235: step 763, loss 0.144775, acc 0.92
2016-09-06T13:38:38.136029: step 764, loss 0.179542, acc 0.92
2016-09-06T13:38:38.944898: step 765, loss 0.128627, acc 0.94
2016-09-06T13:38:39.762146: step 766, loss 0.193653, acc 0.92
2016-09-06T13:38:40.561435: step 767, loss 0.119616, acc 0.94
2016-09-06T13:38:41.309358: step 768, loss 0.155444, acc 0.931818
2016-09-06T13:38:42.113712: step 769, loss 0.104006, acc 0.96
2016-09-06T13:38:42.932744: step 770, loss 0.102868, acc 1
2016-09-06T13:38:43.773262: step 771, loss 0.0968976, acc 0.98
2016-09-06T13:38:44.580726: step 772, loss 0.19975, acc 0.92
2016-09-06T13:38:45.416428: step 773, loss 0.124257, acc 0.94
2016-09-06T13:38:46.243407: step 774, loss 0.117751, acc 0.96
2016-09-06T13:38:47.071083: step 775, loss 0.224696, acc 0.9
2016-09-06T13:38:47.860375: step 776, loss 0.1328, acc 0.96
2016-09-06T13:38:48.662112: step 777, loss 0.139407, acc 0.96
2016-09-06T13:38:49.465957: step 778, loss 0.174121, acc 0.96
2016-09-06T13:38:50.234734: step 779, loss 0.0713269, acc 0.98
2016-09-06T13:38:51.025667: step 780, loss 0.170343, acc 0.92
2016-09-06T13:38:51.850708: step 781, loss 0.0954883, acc 0.96
2016-09-06T13:38:52.659592: step 782, loss 0.153693, acc 0.96
2016-09-06T13:38:53.459757: step 783, loss 0.0824304, acc 0.96
2016-09-06T13:38:54.251121: step 784, loss 0.149554, acc 0.92
2016-09-06T13:38:55.048450: step 785, loss 0.104599, acc 0.96
2016-09-06T13:38:55.885236: step 786, loss 0.0642285, acc 0.98
2016-09-06T13:38:56.693568: step 787, loss 0.124601, acc 0.96
2016-09-06T13:38:57.479024: step 788, loss 0.113299, acc 0.98
2016-09-06T13:38:58.272658: step 789, loss 0.111342, acc 0.96
2016-09-06T13:38:59.099658: step 790, loss 0.0926861, acc 0.94
2016-09-06T13:38:59.895552: step 791, loss 0.0953462, acc 0.98
2016-09-06T13:39:00.708171: step 792, loss 0.0942818, acc 0.98
2016-09-06T13:39:01.530715: step 793, loss 0.0820248, acc 0.96
2016-09-06T13:39:02.341405: step 794, loss 0.259787, acc 0.92
2016-09-06T13:39:03.127778: step 795, loss 0.12949, acc 0.92
2016-09-06T13:39:03.933712: step 796, loss 0.0949044, acc 0.96
2016-09-06T13:39:04.727341: step 797, loss 0.133048, acc 0.96
2016-09-06T13:39:05.577032: step 798, loss 0.0897968, acc 0.96
2016-09-06T13:39:06.386357: step 799, loss 0.0358958, acc 1
2016-09-06T13:39:07.190710: step 800, loss 0.233899, acc 0.92

Evaluation:
2016-09-06T13:39:10.919671: step 800, loss 0.66223, acc 0.771107

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-800

2016-09-06T13:39:12.809908: step 801, loss 0.0766784, acc 0.98
2016-09-06T13:39:13.652193: step 802, loss 0.190366, acc 0.88
2016-09-06T13:39:14.445390: step 803, loss 0.160334, acc 0.94
2016-09-06T13:39:15.233122: step 804, loss 0.0873949, acc 0.98
2016-09-06T13:39:16.077956: step 805, loss 0.16625, acc 0.94
2016-09-06T13:39:16.880461: step 806, loss 0.0843716, acc 0.98
2016-09-06T13:39:17.674547: step 807, loss 0.119243, acc 0.96
2016-09-06T13:39:18.484203: step 808, loss 0.123652, acc 0.94
2016-09-06T13:39:19.251492: step 809, loss 0.0862724, acc 0.96
2016-09-06T13:39:20.049248: step 810, loss 0.149881, acc 0.94
2016-09-06T13:39:20.865513: step 811, loss 0.128267, acc 0.96
2016-09-06T13:39:21.674208: step 812, loss 0.12854, acc 0.94
2016-09-06T13:39:22.484047: step 813, loss 0.0772458, acc 0.98
2016-09-06T13:39:23.300095: step 814, loss 0.0788714, acc 0.96
2016-09-06T13:39:24.100131: step 815, loss 0.160183, acc 0.92
2016-09-06T13:39:24.889788: step 816, loss 0.164523, acc 0.94
2016-09-06T13:39:25.721446: step 817, loss 0.101893, acc 0.98
2016-09-06T13:39:26.546465: step 818, loss 0.0841669, acc 0.96
2016-09-06T13:39:27.349428: step 819, loss 0.152864, acc 0.96
2016-09-06T13:39:28.147646: step 820, loss 0.201544, acc 0.9
2016-09-06T13:39:28.933556: step 821, loss 0.158171, acc 0.96
2016-09-06T13:39:29.738801: step 822, loss 0.145151, acc 0.94
2016-09-06T13:39:30.572809: step 823, loss 0.101573, acc 0.98
2016-09-06T13:39:31.350192: step 824, loss 0.0870258, acc 0.96
2016-09-06T13:39:32.151623: step 825, loss 0.0604196, acc 0.96
2016-09-06T13:39:33.000559: step 826, loss 0.197648, acc 0.94
2016-09-06T13:39:33.800910: step 827, loss 0.107149, acc 0.98
2016-09-06T13:39:34.614052: step 828, loss 0.0835795, acc 0.96
2016-09-06T13:39:35.470678: step 829, loss 0.0670313, acc 0.96
2016-09-06T13:39:36.315941: step 830, loss 0.0836255, acc 0.96
2016-09-06T13:39:37.136821: step 831, loss 0.0340881, acc 0.98
2016-09-06T13:39:37.959952: step 832, loss 0.130146, acc 0.94
2016-09-06T13:39:38.762948: step 833, loss 0.118512, acc 0.94
2016-09-06T13:39:39.601076: step 834, loss 0.126144, acc 0.98
2016-09-06T13:39:40.456281: step 835, loss 0.0955752, acc 0.96
2016-09-06T13:39:41.253086: step 836, loss 0.122975, acc 0.96
2016-09-06T13:39:42.069759: step 837, loss 0.114303, acc 0.96
2016-09-06T13:39:42.889672: step 838, loss 0.205137, acc 0.92
2016-09-06T13:39:43.679909: step 839, loss 0.113464, acc 0.96
2016-09-06T13:39:44.489871: step 840, loss 0.299296, acc 0.88
2016-09-06T13:39:45.327022: step 841, loss 0.114224, acc 0.98
2016-09-06T13:39:46.142535: step 842, loss 0.0881528, acc 0.98
2016-09-06T13:39:46.955332: step 843, loss 0.0675866, acc 1
2016-09-06T13:39:47.809461: step 844, loss 0.122788, acc 0.94
2016-09-06T13:39:48.661441: step 845, loss 0.124309, acc 0.94
2016-09-06T13:39:49.489487: step 846, loss 0.0579412, acc 1
2016-09-06T13:39:50.283960: step 847, loss 0.132283, acc 0.92
2016-09-06T13:39:51.118909: step 848, loss 0.0977999, acc 0.96
2016-09-06T13:39:51.901349: step 849, loss 0.123291, acc 0.92
2016-09-06T13:39:52.680189: step 850, loss 0.112136, acc 0.96
2016-09-06T13:39:53.496424: step 851, loss 0.139514, acc 0.94
2016-09-06T13:39:54.278901: step 852, loss 0.115261, acc 0.96
2016-09-06T13:39:55.107338: step 853, loss 0.130584, acc 0.96
2016-09-06T13:39:55.931252: step 854, loss 0.0746135, acc 0.98
2016-09-06T13:39:56.697189: step 855, loss 0.0543619, acc 0.98
2016-09-06T13:39:57.511054: step 856, loss 0.12014, acc 0.92
2016-09-06T13:39:58.324420: step 857, loss 0.152741, acc 0.94
2016-09-06T13:39:59.120126: step 858, loss 0.10841, acc 0.94
2016-09-06T13:39:59.904743: step 859, loss 0.0920929, acc 0.96
2016-09-06T13:40:00.735025: step 860, loss 0.0353262, acc 1
2016-09-06T13:40:01.547957: step 861, loss 0.169199, acc 0.9
2016-09-06T13:40:02.356774: step 862, loss 0.0587716, acc 0.98
2016-09-06T13:40:03.209387: step 863, loss 0.11582, acc 0.96
2016-09-06T13:40:04.000985: step 864, loss 0.0741769, acc 0.98
2016-09-06T13:40:04.781885: step 865, loss 0.0663289, acc 0.98
2016-09-06T13:40:05.619380: step 866, loss 0.133678, acc 0.94
2016-09-06T13:40:06.422071: step 867, loss 0.0847483, acc 0.98
2016-09-06T13:40:07.237988: step 868, loss 0.166652, acc 0.92
2016-09-06T13:40:08.051525: step 869, loss 0.101829, acc 0.96
2016-09-06T13:40:08.836089: step 870, loss 0.0995256, acc 0.94
2016-09-06T13:40:09.634663: step 871, loss 0.133219, acc 0.96
2016-09-06T13:40:10.432867: step 872, loss 0.0909626, acc 0.96
2016-09-06T13:40:11.219334: step 873, loss 0.177712, acc 0.92
2016-09-06T13:40:12.005079: step 874, loss 0.12403, acc 0.96
2016-09-06T13:40:12.837330: step 875, loss 0.176245, acc 0.96
2016-09-06T13:40:13.632403: step 876, loss 0.0448943, acc 1
2016-09-06T13:40:14.437143: step 877, loss 0.126376, acc 0.9
2016-09-06T13:40:15.275155: step 878, loss 0.171511, acc 0.9
2016-09-06T13:40:16.064743: step 879, loss 0.0888004, acc 0.98
2016-09-06T13:40:16.879916: step 880, loss 0.187663, acc 0.92
2016-09-06T13:40:17.697526: step 881, loss 0.293107, acc 0.9
2016-09-06T13:40:18.470304: step 882, loss 0.0971946, acc 0.96
2016-09-06T13:40:19.273477: step 883, loss 0.112611, acc 0.96
2016-09-06T13:40:20.082155: step 884, loss 0.117558, acc 0.94
2016-09-06T13:40:20.874678: step 885, loss 0.174312, acc 0.92
2016-09-06T13:40:21.692769: step 886, loss 0.101717, acc 0.96
2016-09-06T13:40:22.530113: step 887, loss 0.149859, acc 0.94
2016-09-06T13:40:23.321875: step 888, loss 0.0586148, acc 1
2016-09-06T13:40:24.108635: step 889, loss 0.116867, acc 0.94
2016-09-06T13:40:24.908253: step 890, loss 0.246932, acc 0.9
2016-09-06T13:40:25.709105: step 891, loss 0.0473103, acc 0.98
2016-09-06T13:40:26.516614: step 892, loss 0.183347, acc 0.9
2016-09-06T13:40:27.330573: step 893, loss 0.158017, acc 0.96
2016-09-06T13:40:28.133295: step 894, loss 0.175292, acc 0.94
2016-09-06T13:40:28.941225: step 895, loss 0.0937096, acc 0.94
2016-09-06T13:40:29.758730: step 896, loss 0.141309, acc 0.96
2016-09-06T13:40:30.559032: step 897, loss 0.0824276, acc 0.96
2016-09-06T13:40:31.405718: step 898, loss 0.12765, acc 0.92
2016-09-06T13:40:32.223583: step 899, loss 0.293984, acc 0.84
2016-09-06T13:40:33.043780: step 900, loss 0.0744753, acc 0.98

Evaluation:
2016-09-06T13:40:36.776202: step 900, loss 0.744889, acc 0.769231

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-900

2016-09-06T13:40:38.657067: step 901, loss 0.128563, acc 0.92
2016-09-06T13:40:39.486200: step 902, loss 0.184629, acc 0.94
2016-09-06T13:40:40.262339: step 903, loss 0.0597769, acc 0.98
2016-09-06T13:40:41.074170: step 904, loss 0.0687193, acc 0.98
2016-09-06T13:40:41.892261: step 905, loss 0.109764, acc 0.96
2016-09-06T13:40:42.667704: step 906, loss 0.130918, acc 0.96
2016-09-06T13:40:43.469753: step 907, loss 0.245674, acc 0.92
2016-09-06T13:40:44.297862: step 908, loss 0.209696, acc 0.96
2016-09-06T13:40:45.103212: step 909, loss 0.168785, acc 0.9
2016-09-06T13:40:45.911348: step 910, loss 0.084489, acc 0.96
2016-09-06T13:40:46.730679: step 911, loss 0.0302514, acc 1
2016-09-06T13:40:47.509661: step 912, loss 0.16516, acc 0.96
2016-09-06T13:40:48.292556: step 913, loss 0.17034, acc 0.94
2016-09-06T13:40:49.110193: step 914, loss 0.141923, acc 0.98
2016-09-06T13:40:49.891722: step 915, loss 0.19081, acc 0.94
2016-09-06T13:40:50.708809: step 916, loss 0.111553, acc 0.96
2016-09-06T13:40:51.529535: step 917, loss 0.111311, acc 0.92
2016-09-06T13:40:52.340684: step 918, loss 0.123075, acc 0.96
2016-09-06T13:40:53.132697: step 919, loss 0.101509, acc 0.94
2016-09-06T13:40:53.953081: step 920, loss 0.0980763, acc 0.94
2016-09-06T13:40:54.745819: step 921, loss 0.0957619, acc 0.96
2016-09-06T13:40:55.548667: step 922, loss 0.0788068, acc 0.96
2016-09-06T13:40:56.360689: step 923, loss 0.228879, acc 0.9
2016-09-06T13:40:57.153388: step 924, loss 0.26021, acc 0.9
2016-09-06T13:40:57.955981: step 925, loss 0.222062, acc 0.94
2016-09-06T13:40:58.805573: step 926, loss 0.190923, acc 0.92
2016-09-06T13:40:59.622710: step 927, loss 0.210965, acc 0.88
2016-09-06T13:41:00.439038: step 928, loss 0.134607, acc 0.92
2016-09-06T13:41:01.265361: step 929, loss 0.081761, acc 0.94
2016-09-06T13:41:02.049350: step 930, loss 0.114672, acc 0.92
2016-09-06T13:41:02.849574: step 931, loss 0.123485, acc 0.96
2016-09-06T13:41:03.652204: step 932, loss 0.1664, acc 0.94
2016-09-06T13:41:04.420196: step 933, loss 0.148606, acc 0.96
2016-09-06T13:41:05.216571: step 934, loss 0.129365, acc 0.92
2016-09-06T13:41:06.056458: step 935, loss 0.0751047, acc 0.96
2016-09-06T13:41:06.829199: step 936, loss 0.15555, acc 0.9
2016-09-06T13:41:07.660645: step 937, loss 0.177374, acc 0.94
2016-09-06T13:41:08.479759: step 938, loss 0.102164, acc 0.96
2016-09-06T13:41:09.280609: step 939, loss 0.119967, acc 0.96
2016-09-06T13:41:10.061774: step 940, loss 0.0830186, acc 0.96
2016-09-06T13:41:10.894435: step 941, loss 0.0830287, acc 0.98
2016-09-06T13:41:11.710976: step 942, loss 0.155129, acc 0.94
2016-09-06T13:41:12.499476: step 943, loss 0.135431, acc 0.94
2016-09-06T13:41:13.313266: step 944, loss 0.299565, acc 0.92
2016-09-06T13:41:14.103749: step 945, loss 0.193505, acc 0.9
2016-09-06T13:41:14.910842: step 946, loss 0.0712522, acc 1
2016-09-06T13:41:15.712677: step 947, loss 0.111466, acc 0.94
2016-09-06T13:41:16.487430: step 948, loss 0.127104, acc 0.94
2016-09-06T13:41:17.292452: step 949, loss 0.133862, acc 0.94
2016-09-06T13:41:18.099248: step 950, loss 0.173065, acc 0.94
2016-09-06T13:41:18.885414: step 951, loss 0.0505704, acc 0.98
2016-09-06T13:41:19.702917: step 952, loss 0.0744322, acc 0.96
2016-09-06T13:41:20.518205: step 953, loss 0.144868, acc 0.94
2016-09-06T13:41:21.312315: step 954, loss 0.0662644, acc 0.98
2016-09-06T13:41:22.114213: step 955, loss 0.107363, acc 0.96
2016-09-06T13:41:22.950345: step 956, loss 0.154326, acc 0.9
2016-09-06T13:41:23.726405: step 957, loss 0.246634, acc 0.9
2016-09-06T13:41:24.554620: step 958, loss 0.204133, acc 0.94
2016-09-06T13:41:25.395188: step 959, loss 0.103994, acc 0.94
2016-09-06T13:41:26.111654: step 960, loss 0.183228, acc 0.931818
2016-09-06T13:41:26.927309: step 961, loss 0.156435, acc 0.9
2016-09-06T13:41:27.743625: step 962, loss 0.0916434, acc 1
2016-09-06T13:41:28.545956: step 963, loss 0.094208, acc 0.96
2016-09-06T13:41:29.360150: step 964, loss 0.0916961, acc 0.96
2016-09-06T13:41:30.169362: step 965, loss 0.0742561, acc 0.96
2016-09-06T13:41:30.953422: step 966, loss 0.0794502, acc 0.98
2016-09-06T13:41:31.765403: step 967, loss 0.181959, acc 0.9
2016-09-06T13:41:32.588645: step 968, loss 0.0492093, acc 1
2016-09-06T13:41:33.359855: step 969, loss 0.0717527, acc 0.98
2016-09-06T13:41:34.194975: step 970, loss 0.0946017, acc 0.96
2016-09-06T13:41:35.014847: step 971, loss 0.0622539, acc 1
2016-09-06T13:41:35.803558: step 972, loss 0.0612162, acc 0.98
2016-09-06T13:41:36.593884: step 973, loss 0.093341, acc 0.96
2016-09-06T13:41:37.416416: step 974, loss 0.0582679, acc 0.98
2016-09-06T13:41:38.195512: step 975, loss 0.0722749, acc 0.98
2016-09-06T13:41:39.003982: step 976, loss 0.0495758, acc 1
2016-09-06T13:41:39.804308: step 977, loss 0.067043, acc 0.96
2016-09-06T13:41:40.598168: step 978, loss 0.0269915, acc 0.98
2016-09-06T13:41:41.421522: step 979, loss 0.0648727, acc 0.98
2016-09-06T13:41:42.246219: step 980, loss 0.0789898, acc 0.96
2016-09-06T13:41:43.058992: step 981, loss 0.180889, acc 0.92
2016-09-06T13:41:43.879458: step 982, loss 0.0563018, acc 0.98
2016-09-06T13:41:44.715712: step 983, loss 0.0502998, acc 0.98
2016-09-06T13:41:45.523528: step 984, loss 0.119516, acc 0.94
2016-09-06T13:41:46.345279: step 985, loss 0.0546028, acc 0.98
2016-09-06T13:41:47.156683: step 986, loss 0.106505, acc 0.94
2016-09-06T13:41:47.953921: step 987, loss 0.0400375, acc 0.98
2016-09-06T13:41:48.740686: step 988, loss 0.0514981, acc 0.98
2016-09-06T13:41:49.559538: step 989, loss 0.019078, acc 1
2016-09-06T13:41:50.334575: step 990, loss 0.0258448, acc 1
2016-09-06T13:41:51.124668: step 991, loss 0.0743875, acc 0.94
2016-09-06T13:41:51.952400: step 992, loss 0.105968, acc 0.98
2016-09-06T13:41:52.743146: step 993, loss 0.180865, acc 0.94
2016-09-06T13:41:53.569440: step 994, loss 0.196309, acc 0.92
2016-09-06T13:41:54.400605: step 995, loss 0.111089, acc 0.94
2016-09-06T13:41:55.199250: step 996, loss 0.0294783, acc 0.98
2016-09-06T13:41:55.985468: step 997, loss 0.0622343, acc 0.96
2016-09-06T13:41:56.790513: step 998, loss 0.226958, acc 0.92
2016-09-06T13:41:57.585154: step 999, loss 0.0675085, acc 0.98
2016-09-06T13:41:58.410631: step 1000, loss 0.0307017, acc 1

Evaluation:
2016-09-06T13:42:02.140686: step 1000, loss 1.08638, acc 0.758912

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-1000

2016-09-06T13:42:03.964717: step 1001, loss 0.0138096, acc 1
2016-09-06T13:42:04.738408: step 1002, loss 0.151501, acc 0.92
2016-09-06T13:42:05.594220: step 1003, loss 0.215969, acc 0.9
2016-09-06T13:42:06.401575: step 1004, loss 0.117437, acc 0.92
2016-09-06T13:42:07.211828: step 1005, loss 0.0742533, acc 0.94
2016-09-06T13:42:08.038191: step 1006, loss 0.0485015, acc 0.98
2016-09-06T13:42:08.850825: step 1007, loss 0.127468, acc 0.94
2016-09-06T13:42:09.672945: step 1008, loss 0.0846707, acc 0.98
2016-09-06T13:42:10.508240: step 1009, loss 0.112645, acc 0.96
2016-09-06T13:42:11.310534: step 1010, loss 0.05039, acc 0.98
2016-09-06T13:42:12.122085: step 1011, loss 0.0448328, acc 0.98
2016-09-06T13:42:12.957039: step 1012, loss 0.119514, acc 0.96
2016-09-06T13:42:13.800298: step 1013, loss 0.074147, acc 0.98
2016-09-06T13:42:14.620278: step 1014, loss 0.0357058, acc 1
2016-09-06T13:42:15.421041: step 1015, loss 0.250898, acc 0.88
2016-09-06T13:42:16.238494: step 1016, loss 0.1471, acc 0.94
2016-09-06T13:42:17.051114: step 1017, loss 0.117847, acc 0.94
2016-09-06T13:42:17.862710: step 1018, loss 0.0417494, acc 0.98
2016-09-06T13:42:18.683732: step 1019, loss 0.131069, acc 0.9
2016-09-06T13:42:19.475464: step 1020, loss 0.13414, acc 0.94
2016-09-06T13:42:20.290919: step 1021, loss 0.0562695, acc 0.96
2016-09-06T13:42:21.111344: step 1022, loss 0.118967, acc 0.94
2016-09-06T13:42:21.903828: step 1023, loss 0.0356731, acc 1
2016-09-06T13:42:22.691639: step 1024, loss 0.0763754, acc 0.98
2016-09-06T13:42:23.488605: step 1025, loss 0.190623, acc 0.92
2016-09-06T13:42:24.323155: step 1026, loss 0.190087, acc 0.92
2016-09-06T13:42:25.132778: step 1027, loss 0.0745289, acc 0.96
2016-09-06T13:42:25.956472: step 1028, loss 0.0579553, acc 1
2016-09-06T13:42:26.769422: step 1029, loss 0.0868221, acc 0.98
2016-09-06T13:42:27.566868: step 1030, loss 0.18249, acc 0.94
2016-09-06T13:42:28.364995: step 1031, loss 0.230637, acc 0.94
2016-09-06T13:42:29.150820: step 1032, loss 0.0647107, acc 0.98
2016-09-06T13:42:29.950827: step 1033, loss 0.118217, acc 0.94
2016-09-06T13:42:30.782779: step 1034, loss 0.135883, acc 0.96
2016-09-06T13:42:31.587427: step 1035, loss 0.0766697, acc 0.96
2016-09-06T13:42:32.410382: step 1036, loss 0.0908891, acc 0.94
2016-09-06T13:42:33.246546: step 1037, loss 0.239741, acc 0.92
2016-09-06T13:42:34.033388: step 1038, loss 0.108037, acc 0.96
2016-09-06T13:42:34.848639: step 1039, loss 0.144415, acc 0.94
2016-09-06T13:42:35.709724: step 1040, loss 0.105784, acc 0.96
2016-09-06T13:42:36.545573: step 1041, loss 0.0812357, acc 0.96
2016-09-06T13:42:37.369130: step 1042, loss 0.0564472, acc 1
2016-09-06T13:42:38.210872: step 1043, loss 0.074043, acc 0.98
2016-09-06T13:42:39.032635: step 1044, loss 0.0436716, acc 1
2016-09-06T13:42:39.833119: step 1045, loss 0.150553, acc 0.92
2016-09-06T13:42:40.636746: step 1046, loss 0.148205, acc 0.92
2016-09-06T13:42:41.465313: step 1047, loss 0.0471247, acc 0.98
2016-09-06T13:42:42.270112: step 1048, loss 0.071223, acc 0.98
2016-09-06T13:42:43.079036: step 1049, loss 0.0347045, acc 1
2016-09-06T13:42:43.905793: step 1050, loss 0.192423, acc 0.92
2016-09-06T13:42:44.733662: step 1051, loss 0.0691036, acc 0.96
2016-09-06T13:42:45.556759: step 1052, loss 0.129043, acc 0.94
2016-09-06T13:42:46.355257: step 1053, loss 0.212581, acc 0.94
2016-09-06T13:42:47.180114: step 1054, loss 0.100941, acc 0.96
2016-09-06T13:42:48.005568: step 1055, loss 0.189364, acc 0.94
2016-09-06T13:42:48.830365: step 1056, loss 0.0507763, acc 1
2016-09-06T13:42:49.637158: step 1057, loss 0.120861, acc 0.96
2016-09-06T13:42:50.440714: step 1058, loss 0.109157, acc 0.92
2016-09-06T13:42:51.261146: step 1059, loss 0.228049, acc 0.92
2016-09-06T13:42:52.049325: step 1060, loss 0.119885, acc 0.94
2016-09-06T13:42:52.834818: step 1061, loss 0.0916683, acc 0.92
2016-09-06T13:42:53.666131: step 1062, loss 0.100839, acc 0.96
2016-09-06T13:42:54.475620: step 1063, loss 0.0909894, acc 0.98
2016-09-06T13:42:55.283515: step 1064, loss 0.111262, acc 0.96
2016-09-06T13:42:56.096181: step 1065, loss 0.0878934, acc 0.98
2016-09-06T13:42:56.890587: step 1066, loss 0.0915812, acc 0.98
2016-09-06T13:42:57.691490: step 1067, loss 0.188934, acc 0.86
2016-09-06T13:42:58.493088: step 1068, loss 0.112404, acc 0.92
2016-09-06T13:42:59.304029: step 1069, loss 0.0842425, acc 0.92
2016-09-06T13:43:00.126017: step 1070, loss 0.129883, acc 0.96
2016-09-06T13:43:00.955155: step 1071, loss 0.0498505, acc 1
2016-09-06T13:43:01.737403: step 1072, loss 0.0802502, acc 0.98
2016-09-06T13:43:02.550875: step 1073, loss 0.117962, acc 0.94
2016-09-06T13:43:03.371590: step 1074, loss 0.0627407, acc 0.96
2016-09-06T13:43:04.186449: step 1075, loss 0.144139, acc 0.92
2016-09-06T13:43:04.986395: step 1076, loss 0.218224, acc 0.88
2016-09-06T13:43:05.793946: step 1077, loss 0.124342, acc 0.94
2016-09-06T13:43:06.566435: step 1078, loss 0.0447135, acc 0.98
2016-09-06T13:43:07.365180: step 1079, loss 0.125537, acc 0.94
2016-09-06T13:43:08.194808: step 1080, loss 0.16791, acc 0.94
2016-09-06T13:43:09.004856: step 1081, loss 0.136654, acc 0.96
2016-09-06T13:43:09.782735: step 1082, loss 0.0868278, acc 0.98
2016-09-06T13:43:10.626507: step 1083, loss 0.121763, acc 0.94
2016-09-06T13:43:11.397442: step 1084, loss 0.148998, acc 0.96
2016-09-06T13:43:12.186209: step 1085, loss 0.0862928, acc 0.94
2016-09-06T13:43:13.002954: step 1086, loss 0.0759473, acc 0.96
2016-09-06T13:43:13.790218: step 1087, loss 0.0944135, acc 0.96
2016-09-06T13:43:14.593021: step 1088, loss 0.0553439, acc 0.98
2016-09-06T13:43:15.420289: step 1089, loss 0.0527615, acc 0.98
2016-09-06T13:43:16.239539: step 1090, loss 0.130938, acc 0.96
2016-09-06T13:43:17.045362: step 1091, loss 0.0595053, acc 1
2016-09-06T13:43:17.853397: step 1092, loss 0.0377999, acc 0.98
2016-09-06T13:43:18.597372: step 1093, loss 0.0812002, acc 0.96
2016-09-06T13:43:19.431767: step 1094, loss 0.0696625, acc 0.98
2016-09-06T13:43:20.257706: step 1095, loss 0.135086, acc 0.96
2016-09-06T13:43:21.042931: step 1096, loss 0.0765869, acc 0.96
2016-09-06T13:43:21.848313: step 1097, loss 0.0611429, acc 0.98
2016-09-06T13:43:22.670360: step 1098, loss 0.12464, acc 0.96
2016-09-06T13:43:23.454208: step 1099, loss 0.0994, acc 0.96
2016-09-06T13:43:24.255988: step 1100, loss 0.0642058, acc 0.98

Evaluation:
2016-09-06T13:43:27.982453: step 1100, loss 0.964754, acc 0.763602

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-1100

2016-09-06T13:43:30.052964: step 1101, loss 0.294394, acc 0.86
2016-09-06T13:43:30.849456: step 1102, loss 0.158179, acc 0.94
2016-09-06T13:43:31.665394: step 1103, loss 0.171239, acc 0.9
2016-09-06T13:43:32.478031: step 1104, loss 0.140381, acc 0.92
2016-09-06T13:43:33.269447: step 1105, loss 0.180001, acc 0.92
2016-09-06T13:43:34.041807: step 1106, loss 0.234771, acc 0.96
2016-09-06T13:43:34.858600: step 1107, loss 0.201879, acc 0.92
2016-09-06T13:43:35.671442: step 1108, loss 0.060823, acc 0.98
2016-09-06T13:43:36.469065: step 1109, loss 0.0764592, acc 0.98
2016-09-06T13:43:37.280373: step 1110, loss 0.0517705, acc 0.98
2016-09-06T13:43:38.097841: step 1111, loss 0.0744667, acc 0.96
2016-09-06T13:43:38.887582: step 1112, loss 0.0843254, acc 0.96
2016-09-06T13:43:39.721803: step 1113, loss 0.119433, acc 0.94
2016-09-06T13:43:40.481942: step 1114, loss 0.183509, acc 0.9
2016-09-06T13:43:41.284200: step 1115, loss 0.17257, acc 0.92
2016-09-06T13:43:42.083075: step 1116, loss 0.110597, acc 0.96
2016-09-06T13:43:42.873396: step 1117, loss 0.151585, acc 0.94
2016-09-06T13:43:43.704640: step 1118, loss 0.0519185, acc 0.98
2016-09-06T13:43:44.541437: step 1119, loss 0.0641907, acc 1
2016-09-06T13:43:45.333080: step 1120, loss 0.135267, acc 0.94
2016-09-06T13:43:46.132603: step 1121, loss 0.127249, acc 0.92
2016-09-06T13:43:46.958316: step 1122, loss 0.150428, acc 0.96
2016-09-06T13:43:47.731249: step 1123, loss 0.132119, acc 0.96
2016-09-06T13:43:48.516840: step 1124, loss 0.0872778, acc 0.98
2016-09-06T13:43:49.317106: step 1125, loss 0.047417, acc 0.98
2016-09-06T13:43:50.130319: step 1126, loss 0.0714718, acc 0.98
2016-09-06T13:43:50.939610: step 1127, loss 0.0637611, acc 0.98
2016-09-06T13:43:51.732779: step 1128, loss 0.0461813, acc 1
2016-09-06T13:43:52.530900: step 1129, loss 0.170761, acc 0.94
2016-09-06T13:43:53.365340: step 1130, loss 0.175868, acc 0.92
2016-09-06T13:43:54.173275: step 1131, loss 0.158168, acc 0.92
2016-09-06T13:43:54.961551: step 1132, loss 0.0396065, acc 1
2016-09-06T13:43:55.768180: step 1133, loss 0.0853702, acc 0.94
2016-09-06T13:43:56.558970: step 1134, loss 0.102344, acc 0.92
2016-09-06T13:43:57.365144: step 1135, loss 0.200198, acc 0.92
2016-09-06T13:43:58.225304: step 1136, loss 0.0789666, acc 0.98
2016-09-06T13:43:59.052801: step 1137, loss 0.0744632, acc 0.94
2016-09-06T13:43:59.831089: step 1138, loss 0.178249, acc 0.9
2016-09-06T13:44:00.656089: step 1139, loss 0.243305, acc 0.9
2016-09-06T13:44:01.476047: step 1140, loss 0.145863, acc 0.96
2016-09-06T13:44:02.257955: step 1141, loss 0.0547574, acc 0.98
2016-09-06T13:44:03.057721: step 1142, loss 0.173867, acc 0.94
2016-09-06T13:44:03.895060: step 1143, loss 0.312899, acc 0.9
2016-09-06T13:44:04.668293: step 1144, loss 0.141793, acc 0.96
2016-09-06T13:44:05.473155: step 1145, loss 0.0664545, acc 0.98
2016-09-06T13:44:06.307311: step 1146, loss 0.168282, acc 0.94
2016-09-06T13:44:07.083690: step 1147, loss 0.105805, acc 0.94
2016-09-06T13:44:07.897395: step 1148, loss 0.103445, acc 0.94
2016-09-06T13:44:08.701559: step 1149, loss 0.28849, acc 0.86
2016-09-06T13:44:09.482631: step 1150, loss 0.158441, acc 0.92
2016-09-06T13:44:10.275220: step 1151, loss 0.127027, acc 0.96
2016-09-06T13:44:11.035621: step 1152, loss 0.125858, acc 0.977273
2016-09-06T13:44:11.846322: step 1153, loss 0.145717, acc 0.92
2016-09-06T13:44:12.659451: step 1154, loss 0.0765754, acc 1
2016-09-06T13:44:13.455718: step 1155, loss 0.0536225, acc 1
2016-09-06T13:44:14.248205: step 1156, loss 0.113521, acc 0.96
2016-09-06T13:44:15.058599: step 1157, loss 0.112984, acc 0.96
2016-09-06T13:44:15.857887: step 1158, loss 0.111343, acc 0.96
2016-09-06T13:44:16.660571: step 1159, loss 0.137122, acc 0.98
2016-09-06T13:44:17.478438: step 1160, loss 0.0378329, acc 1
2016-09-06T13:44:18.281048: step 1161, loss 0.0773113, acc 0.96
2016-09-06T13:44:19.067926: step 1162, loss 0.0893094, acc 0.96
2016-09-06T13:44:19.919519: step 1163, loss 0.0602055, acc 0.98
2016-09-06T13:44:20.731713: step 1164, loss 0.0526324, acc 0.98
2016-09-06T13:44:21.536647: step 1165, loss 0.0730916, acc 0.96
2016-09-06T13:44:22.332915: step 1166, loss 0.133434, acc 0.94
2016-09-06T13:44:23.127023: step 1167, loss 0.186669, acc 0.94
2016-09-06T13:44:23.919580: step 1168, loss 0.0875344, acc 0.94
2016-09-06T13:44:24.761137: step 1169, loss 0.0662839, acc 1
2016-09-06T13:44:25.558589: step 1170, loss 0.0264004, acc 0.98
2016-09-06T13:44:26.348753: step 1171, loss 0.0990568, acc 0.96
2016-09-06T13:44:27.170590: step 1172, loss 0.100656, acc 0.96
2016-09-06T13:44:27.984738: step 1173, loss 0.199914, acc 0.96
2016-09-06T13:44:28.763148: step 1174, loss 0.115389, acc 0.94
2016-09-06T13:44:29.562960: step 1175, loss 0.141112, acc 0.94
2016-09-06T13:44:30.375864: step 1176, loss 0.0635328, acc 0.96
2016-09-06T13:44:31.168505: step 1177, loss 0.119618, acc 0.96
2016-09-06T13:44:31.989943: step 1178, loss 0.0242319, acc 1
2016-09-06T13:44:32.786903: step 1179, loss 0.0319694, acc 0.98
2016-09-06T13:44:33.569021: step 1180, loss 0.0396132, acc 1
2016-09-06T13:44:34.397815: step 1181, loss 0.0523501, acc 0.98
2016-09-06T13:44:35.216078: step 1182, loss 0.0751428, acc 0.94
2016-09-06T13:44:36.037274: step 1183, loss 0.0955638, acc 0.96
2016-09-06T13:44:36.862560: step 1184, loss 0.0613494, acc 0.96
2016-09-06T13:44:37.682866: step 1185, loss 0.0370371, acc 1
2016-09-06T13:44:38.490153: step 1186, loss 0.0597594, acc 0.96
2016-09-06T13:44:39.315341: step 1187, loss 0.0357689, acc 1
2016-09-06T13:44:40.121437: step 1188, loss 0.0444868, acc 1
2016-09-06T13:44:40.906689: step 1189, loss 0.0970174, acc 0.98
2016-09-06T13:44:41.710834: step 1190, loss 0.178659, acc 0.94
2016-09-06T13:44:42.529237: step 1191, loss 0.111908, acc 0.94
2016-09-06T13:44:43.325590: step 1192, loss 0.0696346, acc 0.98
2016-09-06T13:44:44.156523: step 1193, loss 0.0257059, acc 1
2016-09-06T13:44:44.964121: step 1194, loss 0.0299387, acc 1
2016-09-06T13:44:45.761537: step 1195, loss 0.0409425, acc 1
2016-09-06T13:44:46.531065: step 1196, loss 0.0632077, acc 0.98
2016-09-06T13:44:47.347405: step 1197, loss 0.131821, acc 0.9
2016-09-06T13:44:48.148424: step 1198, loss 0.0819364, acc 0.96
2016-09-06T13:44:48.945229: step 1199, loss 0.111565, acc 0.98
2016-09-06T13:44:49.761588: step 1200, loss 0.0421598, acc 0.98

Evaluation:
2016-09-06T13:44:53.486083: step 1200, loss 0.962494, acc 0.760788

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-1200

2016-09-06T13:44:55.412635: step 1201, loss 0.0761142, acc 0.96
2016-09-06T13:44:56.266511: step 1202, loss 0.0195306, acc 1
2016-09-06T13:44:57.081813: step 1203, loss 0.0676576, acc 0.98
2016-09-06T13:44:57.876393: step 1204, loss 0.0388092, acc 1
2016-09-06T13:44:58.690105: step 1205, loss 0.011136, acc 1
2016-09-06T13:44:59.485583: step 1206, loss 0.190848, acc 0.9
2016-09-06T13:45:00.289118: step 1207, loss 0.0476466, acc 0.96
2016-09-06T13:45:01.122031: step 1208, loss 0.0257352, acc 1
2016-09-06T13:45:01.933048: step 1209, loss 0.0331866, acc 0.98
2016-09-06T13:45:02.775911: step 1210, loss 0.0244545, acc 1
2016-09-06T13:45:03.590772: step 1211, loss 0.251803, acc 0.92
2016-09-06T13:45:04.397450: step 1212, loss 0.0358863, acc 0.98
2016-09-06T13:45:05.207125: step 1213, loss 0.0796042, acc 0.98
2016-09-06T13:45:06.009892: step 1214, loss 0.150168, acc 0.92
2016-09-06T13:45:06.853988: step 1215, loss 0.0919195, acc 0.96
2016-09-06T13:45:07.698701: step 1216, loss 0.196496, acc 0.94
2016-09-06T13:45:08.490606: step 1217, loss 0.0539134, acc 1
2016-09-06T13:45:09.306879: step 1218, loss 0.0877186, acc 0.96
2016-09-06T13:45:10.104868: step 1219, loss 0.0831526, acc 0.98
2016-09-06T13:45:10.896143: step 1220, loss 0.0672591, acc 0.98
2016-09-06T13:45:11.673398: step 1221, loss 0.0533197, acc 1
2016-09-06T13:45:12.440568: step 1222, loss 0.0799234, acc 0.96
2016-09-06T13:45:13.245833: step 1223, loss 0.0289666, acc 1
2016-09-06T13:45:14.060278: step 1224, loss 0.0720449, acc 0.94
2016-09-06T13:45:14.853897: step 1225, loss 0.0459977, acc 1
2016-09-06T13:45:15.681227: step 1226, loss 0.0740968, acc 0.98
2016-09-06T13:45:16.521443: step 1227, loss 0.0446898, acc 0.98
2016-09-06T13:45:17.312591: step 1228, loss 0.0304411, acc 1
2016-09-06T13:45:18.116961: step 1229, loss 0.0485262, acc 0.98
2016-09-06T13:45:18.951955: step 1230, loss 0.166095, acc 0.94
2016-09-06T13:45:19.730610: step 1231, loss 0.0952581, acc 0.98
2016-09-06T13:45:20.547342: step 1232, loss 0.0691075, acc 0.96
2016-09-06T13:45:21.357165: step 1233, loss 0.0104515, acc 1
2016-09-06T13:45:22.133091: step 1234, loss 0.0657422, acc 0.98
2016-09-06T13:45:22.956420: step 1235, loss 0.202441, acc 0.9
2016-09-06T13:45:23.776903: step 1236, loss 0.0302241, acc 1
2016-09-06T13:45:24.548844: step 1237, loss 0.0896466, acc 0.96
2016-09-06T13:45:25.351683: step 1238, loss 0.0666969, acc 0.98
2016-09-06T13:45:26.190418: step 1239, loss 0.13351, acc 0.96
2016-09-06T13:45:26.980512: step 1240, loss 0.0691291, acc 0.98
2016-09-06T13:45:27.773453: step 1241, loss 0.0522922, acc 0.98
2016-09-06T13:45:28.608995: step 1242, loss 0.0373261, acc 0.98
2016-09-06T13:45:29.371288: step 1243, loss 0.143053, acc 0.94
2016-09-06T13:45:30.185089: step 1244, loss 0.0604657, acc 0.98
2016-09-06T13:45:31.008305: step 1245, loss 0.042757, acc 0.98
2016-09-06T13:45:31.793271: step 1246, loss 0.0547985, acc 0.98
2016-09-06T13:45:32.574245: step 1247, loss 0.113467, acc 0.92
2016-09-06T13:45:33.390055: step 1248, loss 0.0201817, acc 1
2016-09-06T13:45:34.195026: step 1249, loss 0.060365, acc 0.96
2016-09-06T13:45:34.990487: step 1250, loss 0.035871, acc 1
2016-09-06T13:45:35.815826: step 1251, loss 0.0827863, acc 0.96
2016-09-06T13:45:36.607036: step 1252, loss 0.0492968, acc 0.98
2016-09-06T13:45:37.407692: step 1253, loss 0.2203, acc 0.88
2016-09-06T13:45:38.221469: step 1254, loss 0.0747622, acc 0.94
2016-09-06T13:45:39.034306: step 1255, loss 0.125429, acc 0.98
2016-09-06T13:45:39.840096: step 1256, loss 0.0467294, acc 0.98
2016-09-06T13:45:40.671606: step 1257, loss 0.0978003, acc 0.96
2016-09-06T13:45:41.464877: step 1258, loss 0.256872, acc 0.9
2016-09-06T13:45:42.270995: step 1259, loss 0.0739218, acc 0.98
2016-09-06T13:45:43.073870: step 1260, loss 0.109702, acc 0.96
2016-09-06T13:45:43.869389: step 1261, loss 0.0918292, acc 0.94
2016-09-06T13:45:44.663349: step 1262, loss 0.157637, acc 0.96
2016-09-06T13:45:45.483972: step 1263, loss 0.074307, acc 0.96
2016-09-06T13:45:46.279016: step 1264, loss 0.173624, acc 0.94
2016-09-06T13:45:47.056900: step 1265, loss 0.0745671, acc 0.98
2016-09-06T13:45:47.859029: step 1266, loss 0.14724, acc 0.94
2016-09-06T13:45:48.734833: step 1267, loss 0.0489543, acc 1
2016-09-06T13:45:49.545293: step 1268, loss 0.101599, acc 0.96
2016-09-06T13:45:50.357855: step 1269, loss 0.0901788, acc 0.96
2016-09-06T13:45:51.135436: step 1270, loss 0.0291131, acc 1
2016-09-06T13:45:51.943371: step 1271, loss 0.102132, acc 0.96
2016-09-06T13:45:52.755957: step 1272, loss 0.0556165, acc 1
2016-09-06T13:45:53.522228: step 1273, loss 0.0487441, acc 0.98
2016-09-06T13:45:54.340674: step 1274, loss 0.0683509, acc 0.98
2016-09-06T13:45:55.153720: step 1275, loss 0.0223714, acc 1
2016-09-06T13:45:55.928971: step 1276, loss 0.13457, acc 0.9
2016-09-06T13:45:56.770122: step 1277, loss 0.0643153, acc 0.98
2016-09-06T13:45:57.609095: step 1278, loss 0.0529529, acc 0.98
2016-09-06T13:45:58.413996: step 1279, loss 0.0190574, acc 1
2016-09-06T13:45:59.187848: step 1280, loss 0.106569, acc 0.96
2016-09-06T13:46:00.031543: step 1281, loss 0.0527868, acc 0.98
2016-09-06T13:46:00.837737: step 1282, loss 0.143849, acc 0.9
2016-09-06T13:46:01.643775: step 1283, loss 0.16914, acc 0.92
2016-09-06T13:46:02.459862: step 1284, loss 0.123033, acc 0.96
2016-09-06T13:46:03.248468: step 1285, loss 0.143414, acc 0.94
2016-09-06T13:46:04.027182: step 1286, loss 0.0776617, acc 0.96
2016-09-06T13:46:04.825445: step 1287, loss 0.0895085, acc 0.96
2016-09-06T13:46:05.619504: step 1288, loss 0.052453, acc 0.98
2016-09-06T13:46:06.478923: step 1289, loss 0.14249, acc 0.94
2016-09-06T13:46:07.307116: step 1290, loss 0.170677, acc 0.9
2016-09-06T13:46:08.111745: step 1291, loss 0.0498528, acc 0.98
2016-09-06T13:46:08.908361: step 1292, loss 0.0296006, acc 0.98
2016-09-06T13:46:09.764229: step 1293, loss 0.0554381, acc 0.98
2016-09-06T13:46:10.570322: step 1294, loss 0.0605673, acc 0.98
2016-09-06T13:46:11.388312: step 1295, loss 0.125949, acc 0.92
2016-09-06T13:46:12.229327: step 1296, loss 0.104194, acc 0.96
2016-09-06T13:46:13.051932: step 1297, loss 0.162953, acc 0.9
2016-09-06T13:46:13.873286: step 1298, loss 0.129954, acc 0.9
2016-09-06T13:46:14.692951: step 1299, loss 0.102817, acc 0.96
2016-09-06T13:46:15.491752: step 1300, loss 0.0357084, acc 0.98

Evaluation:
2016-09-06T13:46:19.192105: step 1300, loss 0.900256, acc 0.762664

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-1300

2016-09-06T13:46:21.043663: step 1301, loss 0.0339536, acc 1
2016-09-06T13:46:21.839094: step 1302, loss 0.158338, acc 0.96
2016-09-06T13:46:22.632963: step 1303, loss 0.0554664, acc 1
2016-09-06T13:46:23.421464: step 1304, loss 0.0630569, acc 0.98
2016-09-06T13:46:24.226000: step 1305, loss 0.0264556, acc 1
2016-09-06T13:46:25.002589: step 1306, loss 0.0832547, acc 0.96
2016-09-06T13:46:25.806076: step 1307, loss 0.104207, acc 0.96
2016-09-06T13:46:26.617852: step 1308, loss 0.139628, acc 0.96
2016-09-06T13:46:27.406970: step 1309, loss 0.0441502, acc 0.98
2016-09-06T13:46:28.221283: step 1310, loss 0.0544097, acc 0.98
2016-09-06T13:46:29.073001: step 1311, loss 0.0458933, acc 0.98
2016-09-06T13:46:29.835987: step 1312, loss 0.275241, acc 0.92
2016-09-06T13:46:30.634147: step 1313, loss 0.149083, acc 0.92
2016-09-06T13:46:31.443998: step 1314, loss 0.0282757, acc 1
2016-09-06T13:46:32.240870: step 1315, loss 0.120548, acc 0.92
2016-09-06T13:46:33.039581: step 1316, loss 0.0486912, acc 0.98
2016-09-06T13:46:33.854264: step 1317, loss 0.0872286, acc 0.96
2016-09-06T13:46:34.642470: step 1318, loss 0.0291735, acc 1
2016-09-06T13:46:35.452215: step 1319, loss 0.173735, acc 0.92
2016-09-06T13:46:36.251439: step 1320, loss 0.0832069, acc 0.94
2016-09-06T13:46:37.083978: step 1321, loss 0.439331, acc 0.8
2016-09-06T13:46:37.894772: step 1322, loss 0.0692902, acc 0.96
2016-09-06T13:46:38.747255: step 1323, loss 0.150874, acc 0.98
2016-09-06T13:46:39.552544: step 1324, loss 0.0602815, acc 0.98
2016-09-06T13:46:40.360478: step 1325, loss 0.05516, acc 0.96
2016-09-06T13:46:41.184677: step 1326, loss 0.0942691, acc 0.96
2016-09-06T13:46:41.977567: step 1327, loss 0.111192, acc 0.94
2016-09-06T13:46:42.769213: step 1328, loss 0.0753169, acc 0.96
2016-09-06T13:46:43.591746: step 1329, loss 0.0975594, acc 0.94
2016-09-06T13:46:44.383302: step 1330, loss 0.116817, acc 0.94
2016-09-06T13:46:45.168047: step 1331, loss 0.175335, acc 0.96
2016-09-06T13:46:45.959285: step 1332, loss 0.212558, acc 0.9
2016-09-06T13:46:46.743946: step 1333, loss 0.0615501, acc 0.98
2016-09-06T13:46:47.548892: step 1334, loss 0.0986476, acc 0.94
2016-09-06T13:46:48.368014: step 1335, loss 0.0804675, acc 0.98
2016-09-06T13:46:49.176895: step 1336, loss 0.0873052, acc 0.98
2016-09-06T13:46:49.962290: step 1337, loss 0.124639, acc 0.94
2016-09-06T13:46:50.789292: step 1338, loss 0.16296, acc 0.92
2016-09-06T13:46:51.602226: step 1339, loss 0.104857, acc 0.94
2016-09-06T13:46:52.410226: step 1340, loss 0.154037, acc 0.92
2016-09-06T13:46:53.252373: step 1341, loss 0.123294, acc 0.94
2016-09-06T13:46:54.046809: step 1342, loss 0.131162, acc 0.9
2016-09-06T13:46:54.879937: step 1343, loss 0.0751504, acc 0.98
2016-09-06T13:46:55.644964: step 1344, loss 0.0348693, acc 1
2016-09-06T13:46:56.439342: step 1345, loss 0.192011, acc 0.94
2016-09-06T13:46:57.216339: step 1346, loss 0.0784297, acc 0.96
2016-09-06T13:46:58.071958: step 1347, loss 0.0628724, acc 0.98
2016-09-06T13:46:58.850338: step 1348, loss 0.0534858, acc 0.98
2016-09-06T13:46:59.657692: step 1349, loss 0.154992, acc 0.92
2016-09-06T13:47:00.506256: step 1350, loss 0.0579064, acc 1
2016-09-06T13:47:01.309932: step 1351, loss 0.0531862, acc 0.98
2016-09-06T13:47:02.127833: step 1352, loss 0.0189358, acc 1
2016-09-06T13:47:02.955858: step 1353, loss 0.135412, acc 0.94
2016-09-06T13:47:03.749832: step 1354, loss 0.0529901, acc 0.98
2016-09-06T13:47:04.565255: step 1355, loss 0.0633071, acc 0.98
2016-09-06T13:47:05.390989: step 1356, loss 0.0108193, acc 1
2016-09-06T13:47:06.198684: step 1357, loss 0.163752, acc 0.92
2016-09-06T13:47:07.014958: step 1358, loss 0.0359334, acc 1
2016-09-06T13:47:07.835617: step 1359, loss 0.0737371, acc 0.96
2016-09-06T13:47:08.652175: step 1360, loss 0.0471192, acc 0.98
2016-09-06T13:47:09.463196: step 1361, loss 0.0538182, acc 0.98
2016-09-06T13:47:10.310756: step 1362, loss 0.073332, acc 0.96
2016-09-06T13:47:11.117362: step 1363, loss 0.100837, acc 0.94
2016-09-06T13:47:11.959617: step 1364, loss 0.0820974, acc 0.96
2016-09-06T13:47:12.777406: step 1365, loss 0.0781519, acc 0.96
2016-09-06T13:47:13.600263: step 1366, loss 0.121417, acc 0.98
2016-09-06T13:47:14.416438: step 1367, loss 0.0243686, acc 1
2016-09-06T13:47:15.234670: step 1368, loss 0.0153429, acc 1
2016-09-06T13:47:16.037316: step 1369, loss 0.0899999, acc 0.96
2016-09-06T13:47:16.850656: step 1370, loss 0.0848987, acc 0.98
2016-09-06T13:47:17.680487: step 1371, loss 0.0317298, acc 1
2016-09-06T13:47:18.490003: step 1372, loss 0.0627495, acc 0.98
2016-09-06T13:47:19.328740: step 1373, loss 0.041058, acc 1
2016-09-06T13:47:20.129413: step 1374, loss 0.100747, acc 0.94
2016-09-06T13:47:20.961426: step 1375, loss 0.0254495, acc 1
2016-09-06T13:47:21.753855: step 1376, loss 0.0322121, acc 1
2016-09-06T13:47:22.572047: step 1377, loss 0.0426052, acc 0.98
2016-09-06T13:47:23.389742: step 1378, loss 0.12093, acc 0.96
2016-09-06T13:47:24.224020: step 1379, loss 0.0719357, acc 0.94
2016-09-06T13:47:25.029613: step 1380, loss 0.011261, acc 1
2016-09-06T13:47:25.840072: step 1381, loss 0.121495, acc 0.96
2016-09-06T13:47:26.613447: step 1382, loss 0.0599325, acc 0.96
2016-09-06T13:47:27.441902: step 1383, loss 0.114936, acc 0.96
2016-09-06T13:47:28.249681: step 1384, loss 0.0891028, acc 0.96
2016-09-06T13:47:29.038454: step 1385, loss 0.0440711, acc 0.98
2016-09-06T13:47:29.826654: step 1386, loss 0.0415076, acc 0.98
2016-09-06T13:47:30.639262: step 1387, loss 0.132776, acc 0.94
2016-09-06T13:47:31.427482: step 1388, loss 0.0842906, acc 0.96
2016-09-06T13:47:32.253023: step 1389, loss 0.0568998, acc 0.96
2016-09-06T13:47:33.056060: step 1390, loss 0.0429041, acc 1
2016-09-06T13:47:33.825938: step 1391, loss 0.124625, acc 0.92
2016-09-06T13:47:34.637926: step 1392, loss 0.037427, acc 0.98
2016-09-06T13:47:35.437286: step 1393, loss 0.0502108, acc 0.96
2016-09-06T13:47:36.229414: step 1394, loss 0.0903318, acc 0.94
2016-09-06T13:47:37.069727: step 1395, loss 0.0666079, acc 0.98
2016-09-06T13:47:37.896972: step 1396, loss 0.0467713, acc 0.98
2016-09-06T13:47:38.670665: step 1397, loss 0.105169, acc 0.98
2016-09-06T13:47:39.494750: step 1398, loss 0.028955, acc 1
2016-09-06T13:47:40.316979: step 1399, loss 0.108837, acc 0.96
2016-09-06T13:47:41.095355: step 1400, loss 0.124766, acc 0.96

Evaluation:
2016-09-06T13:47:44.817293: step 1400, loss 1.06972, acc 0.760788

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-1400

2016-09-06T13:47:46.745749: step 1401, loss 0.0925162, acc 0.96
2016-09-06T13:47:47.575874: step 1402, loss 0.0329707, acc 0.98
2016-09-06T13:47:48.379531: step 1403, loss 0.0829914, acc 0.96
2016-09-06T13:47:49.211080: step 1404, loss 0.0613414, acc 0.98
2016-09-06T13:47:50.062454: step 1405, loss 0.0353972, acc 0.98
2016-09-06T13:47:50.839845: step 1406, loss 0.0295355, acc 1
2016-09-06T13:47:51.637391: step 1407, loss 0.0204506, acc 1
2016-09-06T13:47:52.448819: step 1408, loss 0.0557821, acc 0.96
2016-09-06T13:47:53.238826: step 1409, loss 0.011864, acc 1
2016-09-06T13:47:54.035585: step 1410, loss 0.054228, acc 0.98
2016-09-06T13:47:54.897958: step 1411, loss 0.12498, acc 0.92
2016-09-06T13:47:55.692371: step 1412, loss 0.0596, acc 0.98
2016-09-06T13:47:56.513379: step 1413, loss 0.0868413, acc 0.94
2016-09-06T13:47:57.351458: step 1414, loss 0.0477752, acc 0.96
2016-09-06T13:47:58.150157: step 1415, loss 0.142043, acc 0.92
2016-09-06T13:47:58.928236: step 1416, loss 0.0299288, acc 0.98
2016-09-06T13:47:59.743259: step 1417, loss 0.0264489, acc 1
2016-09-06T13:48:00.560883: step 1418, loss 0.0421799, acc 1
2016-09-06T13:48:01.366496: step 1419, loss 0.0459815, acc 1
2016-09-06T13:48:02.189031: step 1420, loss 0.0358858, acc 0.98
2016-09-06T13:48:02.997721: step 1421, loss 0.117945, acc 0.92
2016-09-06T13:48:03.802940: step 1422, loss 0.190322, acc 0.94
2016-09-06T13:48:04.634709: step 1423, loss 0.0452801, acc 1
2016-09-06T13:48:05.454159: step 1424, loss 0.0871437, acc 0.96
2016-09-06T13:48:06.292732: step 1425, loss 0.0351007, acc 0.98
2016-09-06T13:48:07.123060: step 1426, loss 0.0373262, acc 1
2016-09-06T13:48:07.942379: step 1427, loss 0.156326, acc 0.92
2016-09-06T13:48:08.759678: step 1428, loss 0.0890917, acc 0.96
2016-09-06T13:48:09.580055: step 1429, loss 0.102232, acc 0.98
2016-09-06T13:48:10.396080: step 1430, loss 0.0299317, acc 0.98
2016-09-06T13:48:11.220395: step 1431, loss 0.0356029, acc 0.98
2016-09-06T13:48:12.039269: step 1432, loss 0.0650612, acc 0.98
2016-09-06T13:48:12.869070: step 1433, loss 0.0722042, acc 0.96
2016-09-06T13:48:13.676798: step 1434, loss 0.069332, acc 0.96
2016-09-06T13:48:14.505654: step 1435, loss 0.0929957, acc 0.98
2016-09-06T13:48:15.317519: step 1436, loss 0.0156138, acc 1
2016-09-06T13:48:16.119069: step 1437, loss 0.113501, acc 0.96
2016-09-06T13:48:16.950869: step 1438, loss 0.0347476, acc 0.98
2016-09-06T13:48:17.775644: step 1439, loss 0.028084, acc 0.98
2016-09-06T13:48:18.583194: step 1440, loss 0.062165, acc 0.96
2016-09-06T13:48:19.408473: step 1441, loss 0.0535652, acc 0.98
2016-09-06T13:48:20.241396: step 1442, loss 0.139167, acc 0.96
2016-09-06T13:48:21.035242: step 1443, loss 0.0996267, acc 0.94
2016-09-06T13:48:21.831846: step 1444, loss 0.0433694, acc 0.98
2016-09-06T13:48:22.665713: step 1445, loss 0.062075, acc 0.98
2016-09-06T13:48:23.451132: step 1446, loss 0.0175981, acc 1
2016-09-06T13:48:24.254576: step 1447, loss 0.131321, acc 0.92
2016-09-06T13:48:25.076332: step 1448, loss 0.0476703, acc 0.98
2016-09-06T13:48:25.860518: step 1449, loss 0.0831213, acc 0.94
2016-09-06T13:48:26.672188: step 1450, loss 0.0555933, acc 0.96
2016-09-06T13:48:27.484560: step 1451, loss 0.121126, acc 0.92
2016-09-06T13:48:28.292046: step 1452, loss 0.144909, acc 0.92
2016-09-06T13:48:29.092144: step 1453, loss 0.0559594, acc 0.98
2016-09-06T13:48:29.930071: step 1454, loss 0.0757688, acc 0.96
2016-09-06T13:48:30.736998: step 1455, loss 0.0475758, acc 1
2016-09-06T13:48:31.551078: step 1456, loss 0.0895561, acc 0.94
2016-09-06T13:48:32.380475: step 1457, loss 0.0862428, acc 0.96
2016-09-06T13:48:33.169769: step 1458, loss 0.0933464, acc 0.98
2016-09-06T13:48:34.001894: step 1459, loss 0.0923462, acc 0.98
2016-09-06T13:48:34.855443: step 1460, loss 0.187564, acc 0.94
2016-09-06T13:48:35.641973: step 1461, loss 0.0487811, acc 0.98
2016-09-06T13:48:36.453060: step 1462, loss 0.101899, acc 0.96
2016-09-06T13:48:37.276772: step 1463, loss 0.12467, acc 0.94
2016-09-06T13:48:38.068009: step 1464, loss 0.041715, acc 0.98
2016-09-06T13:48:38.886826: step 1465, loss 0.0417959, acc 0.96
2016-09-06T13:48:39.708402: step 1466, loss 0.0512498, acc 0.98
2016-09-06T13:48:40.549322: step 1467, loss 0.0928653, acc 0.96
2016-09-06T13:48:41.358497: step 1468, loss 0.0675514, acc 0.98
2016-09-06T13:48:42.182580: step 1469, loss 0.10565, acc 0.94
2016-09-06T13:48:43.011222: step 1470, loss 0.141018, acc 0.94
2016-09-06T13:48:43.804707: step 1471, loss 0.0326037, acc 1
2016-09-06T13:48:44.671170: step 1472, loss 0.12443, acc 0.96
2016-09-06T13:48:45.487021: step 1473, loss 0.0671891, acc 0.98
2016-09-06T13:48:46.295302: step 1474, loss 0.0374256, acc 1
2016-09-06T13:48:47.116899: step 1475, loss 0.0909034, acc 0.96
2016-09-06T13:48:47.964021: step 1476, loss 0.0288611, acc 1
2016-09-06T13:48:48.788640: step 1477, loss 0.056958, acc 0.98
2016-09-06T13:48:49.597497: step 1478, loss 0.108225, acc 0.96
2016-09-06T13:48:50.408318: step 1479, loss 0.0378419, acc 1
2016-09-06T13:48:51.209288: step 1480, loss 0.0216908, acc 1
2016-09-06T13:48:52.024203: step 1481, loss 0.0550931, acc 0.98
2016-09-06T13:48:52.836571: step 1482, loss 0.102564, acc 0.94
2016-09-06T13:48:53.650498: step 1483, loss 0.0119315, acc 1
2016-09-06T13:48:54.473059: step 1484, loss 0.0276906, acc 1
2016-09-06T13:48:55.288911: step 1485, loss 0.034623, acc 0.98
2016-09-06T13:48:56.088579: step 1486, loss 0.138871, acc 0.96
2016-09-06T13:48:56.885116: step 1487, loss 0.0366585, acc 1
2016-09-06T13:48:57.730130: step 1488, loss 0.0562144, acc 0.98
2016-09-06T13:48:58.521029: step 1489, loss 0.146444, acc 0.92
2016-09-06T13:48:59.335452: step 1490, loss 0.0182146, acc 1
2016-09-06T13:49:00.190214: step 1491, loss 0.0965317, acc 0.94
2016-09-06T13:49:01.011642: step 1492, loss 0.0513069, acc 0.98
2016-09-06T13:49:01.815623: step 1493, loss 0.0996903, acc 0.94
2016-09-06T13:49:02.651251: step 1494, loss 0.0408084, acc 0.98
2016-09-06T13:49:03.473703: step 1495, loss 0.068004, acc 0.98
2016-09-06T13:49:04.327877: step 1496, loss 0.0426258, acc 0.98
2016-09-06T13:49:05.233338: step 1497, loss 0.0552756, acc 0.98
2016-09-06T13:49:06.039682: step 1498, loss 0.0476764, acc 0.98
2016-09-06T13:49:06.848838: step 1499, loss 0.0761796, acc 0.96
2016-09-06T13:49:07.687497: step 1500, loss 0.133542, acc 0.94

Evaluation:
2016-09-06T13:49:11.422357: step 1500, loss 1.28402, acc 0.756098

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-1500

2016-09-06T13:49:13.416821: step 1501, loss 0.109412, acc 0.92
2016-09-06T13:49:14.243010: step 1502, loss 0.0960838, acc 0.96
2016-09-06T13:49:15.063770: step 1503, loss 0.0407145, acc 0.98
2016-09-06T13:49:15.880587: step 1504, loss 0.100665, acc 0.92
2016-09-06T13:49:16.678069: step 1505, loss 0.110626, acc 0.92
2016-09-06T13:49:17.483218: step 1506, loss 0.0431487, acc 0.98
2016-09-06T13:49:18.296075: step 1507, loss 0.0552568, acc 0.98
2016-09-06T13:49:19.091069: step 1508, loss 0.0228353, acc 1
2016-09-06T13:49:19.887716: step 1509, loss 0.0350532, acc 0.98
2016-09-06T13:49:20.704446: step 1510, loss 0.173234, acc 0.9
2016-09-06T13:49:21.511093: step 1511, loss 0.0171392, acc 1
2016-09-06T13:49:22.315746: step 1512, loss 0.0459472, acc 0.98
2016-09-06T13:49:23.129860: step 1513, loss 0.156803, acc 0.92
2016-09-06T13:49:23.919703: step 1514, loss 0.0619888, acc 0.98
2016-09-06T13:49:24.716745: step 1515, loss 0.111787, acc 0.96
2016-09-06T13:49:25.520797: step 1516, loss 0.0347359, acc 1
2016-09-06T13:49:26.298444: step 1517, loss 0.0170317, acc 1
2016-09-06T13:49:27.116709: step 1518, loss 0.0846107, acc 0.96
2016-09-06T13:49:27.950026: step 1519, loss 0.087854, acc 0.94
2016-09-06T13:49:28.744667: step 1520, loss 0.0408928, acc 1
2016-09-06T13:49:29.547905: step 1521, loss 0.0939648, acc 0.94
2016-09-06T13:49:30.373093: step 1522, loss 0.0545186, acc 0.98
2016-09-06T13:49:31.173602: step 1523, loss 0.0482353, acc 0.98
2016-09-06T13:49:31.976391: step 1524, loss 0.0958729, acc 0.96
2016-09-06T13:49:32.799635: step 1525, loss 0.126244, acc 0.94
2016-09-06T13:49:33.558530: step 1526, loss 0.101707, acc 0.94
2016-09-06T13:49:34.371605: step 1527, loss 0.129417, acc 0.96
2016-09-06T13:49:35.167893: step 1528, loss 0.0500777, acc 0.96
2016-09-06T13:49:35.963130: step 1529, loss 0.0359003, acc 1
2016-09-06T13:49:36.791508: step 1530, loss 0.0961196, acc 0.94
2016-09-06T13:49:37.638127: step 1531, loss 0.0661993, acc 0.98
2016-09-06T13:49:38.476875: step 1532, loss 0.130657, acc 0.96
2016-09-06T13:49:39.267406: step 1533, loss 0.104327, acc 0.94
2016-09-06T13:49:40.076435: step 1534, loss 0.0636606, acc 0.98
2016-09-06T13:49:40.865638: step 1535, loss 0.062788, acc 0.98
2016-09-06T13:49:41.624374: step 1536, loss 0.0466633, acc 0.977273
2016-09-06T13:49:42.433294: step 1537, loss 0.295345, acc 0.92
2016-09-06T13:49:43.242036: step 1538, loss 0.0326836, acc 0.98
2016-09-06T13:49:44.050780: step 1539, loss 0.0372032, acc 1
2016-09-06T13:49:44.843105: step 1540, loss 0.0780116, acc 0.96
2016-09-06T13:49:45.665693: step 1541, loss 0.0653128, acc 0.98
2016-09-06T13:49:46.497953: step 1542, loss 0.19906, acc 0.94
2016-09-06T13:49:47.304313: step 1543, loss 0.0780948, acc 0.96
2016-09-06T13:49:48.084612: step 1544, loss 0.0616033, acc 0.96
2016-09-06T13:49:48.928159: step 1545, loss 0.0532832, acc 0.98
2016-09-06T13:49:49.750249: step 1546, loss 0.10145, acc 0.92
2016-09-06T13:49:50.540018: step 1547, loss 0.00711377, acc 1
2016-09-06T13:49:51.353149: step 1548, loss 0.108439, acc 0.92
2016-09-06T13:49:52.185457: step 1549, loss 0.138258, acc 0.94
2016-09-06T13:49:52.956397: step 1550, loss 0.0394947, acc 0.98
2016-09-06T13:49:53.764774: step 1551, loss 0.0345403, acc 1
2016-09-06T13:49:54.612745: step 1552, loss 0.0381023, acc 0.98
2016-09-06T13:49:55.396218: step 1553, loss 0.130699, acc 0.96
2016-09-06T13:49:56.173755: step 1554, loss 0.00883651, acc 1
2016-09-06T13:49:56.984752: step 1555, loss 0.105911, acc 0.94
2016-09-06T13:49:57.775194: step 1556, loss 0.0370124, acc 0.96
2016-09-06T13:49:58.580695: step 1557, loss 0.073383, acc 0.96
2016-09-06T13:49:59.409428: step 1558, loss 0.036753, acc 1
2016-09-06T13:50:00.234714: step 1559, loss 0.0324407, acc 0.98
2016-09-06T13:50:01.044687: step 1560, loss 0.0598247, acc 0.96
2016-09-06T13:50:01.880442: step 1561, loss 0.0289804, acc 1
2016-09-06T13:50:02.711203: step 1562, loss 0.0878635, acc 0.98
2016-09-06T13:50:03.530780: step 1563, loss 0.0167823, acc 1
2016-09-06T13:50:04.373330: step 1564, loss 0.0581139, acc 0.98
2016-09-06T13:50:05.237772: step 1565, loss 0.0335526, acc 0.98
2016-09-06T13:50:06.055111: step 1566, loss 0.0222187, acc 1
2016-09-06T13:50:06.891635: step 1567, loss 0.00551668, acc 1
2016-09-06T13:50:07.707102: step 1568, loss 0.0829761, acc 0.98
2016-09-06T13:50:08.525013: step 1569, loss 0.0179467, acc 1
2016-09-06T13:50:09.340760: step 1570, loss 0.0217988, acc 1
2016-09-06T13:50:10.160886: step 1571, loss 0.101784, acc 0.98
2016-09-06T13:50:10.968262: step 1572, loss 0.075239, acc 0.98
2016-09-06T13:50:11.796133: step 1573, loss 0.0318348, acc 1
2016-09-06T13:50:12.609461: step 1574, loss 0.0463486, acc 0.96
2016-09-06T13:50:13.418440: step 1575, loss 0.0552064, acc 0.96
2016-09-06T13:50:14.240899: step 1576, loss 0.0303105, acc 1
2016-09-06T13:50:15.057640: step 1577, loss 0.0353816, acc 1
2016-09-06T13:50:15.857649: step 1578, loss 0.0815499, acc 0.96
2016-09-06T13:50:16.658979: step 1579, loss 0.101465, acc 0.96
2016-09-06T13:50:17.475519: step 1580, loss 0.0879725, acc 0.96
2016-09-06T13:50:18.265244: step 1581, loss 0.0716989, acc 0.98
2016-09-06T13:50:19.080191: step 1582, loss 0.114898, acc 0.94
2016-09-06T13:50:19.886083: step 1583, loss 0.0434335, acc 0.98
2016-09-06T13:50:20.690536: step 1584, loss 0.0287095, acc 1
2016-09-06T13:50:21.521679: step 1585, loss 0.175408, acc 0.92
2016-09-06T13:50:22.340523: step 1586, loss 0.0324867, acc 1
2016-09-06T13:50:23.130785: step 1587, loss 0.0110955, acc 1
2016-09-06T13:50:23.921640: step 1588, loss 0.0838019, acc 0.98
2016-09-06T13:50:24.742645: step 1589, loss 0.150647, acc 0.94
2016-09-06T13:50:25.532983: step 1590, loss 0.151797, acc 0.92
2016-09-06T13:50:26.337023: step 1591, loss 0.0638201, acc 0.98
2016-09-06T13:50:27.143097: step 1592, loss 0.125606, acc 0.92
2016-09-06T13:50:27.924625: step 1593, loss 0.0660002, acc 0.98
2016-09-06T13:50:28.731782: step 1594, loss 0.039892, acc 0.98
2016-09-06T13:50:29.541754: step 1595, loss 0.0388464, acc 0.98
2016-09-06T13:50:30.351133: step 1596, loss 0.0260868, acc 1
2016-09-06T13:50:31.186419: step 1597, loss 0.10327, acc 0.94
2016-09-06T13:50:32.018665: step 1598, loss 0.052275, acc 0.98
2016-09-06T13:50:32.801556: step 1599, loss 0.0442222, acc 1
2016-09-06T13:50:33.589103: step 1600, loss 0.0633697, acc 1

Evaluation:
2016-09-06T13:50:37.295418: step 1600, loss 0.993035, acc 0.757036

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-1600

2016-09-06T13:50:39.258178: step 1601, loss 0.0792472, acc 0.96
2016-09-06T13:50:40.084324: step 1602, loss 0.12317, acc 0.96
2016-09-06T13:50:40.945152: step 1603, loss 0.0576998, acc 1
2016-09-06T13:50:41.762211: step 1604, loss 0.0937442, acc 0.98
2016-09-06T13:50:42.557330: step 1605, loss 0.0888725, acc 0.96
2016-09-06T13:50:43.387750: step 1606, loss 0.0345489, acc 0.98
2016-09-06T13:50:44.220383: step 1607, loss 0.0792945, acc 0.98
2016-09-06T13:50:45.014008: step 1608, loss 0.0420222, acc 0.96
2016-09-06T13:50:45.823348: step 1609, loss 0.0107375, acc 1
2016-09-06T13:50:46.624221: step 1610, loss 0.0419172, acc 0.98
2016-09-06T13:50:47.425734: step 1611, loss 0.100272, acc 0.96
2016-09-06T13:50:48.227010: step 1612, loss 0.0668102, acc 0.98
2016-09-06T13:50:49.058426: step 1613, loss 0.0620445, acc 0.98
2016-09-06T13:50:49.865522: step 1614, loss 0.0393122, acc 0.98
2016-09-06T13:50:50.688756: step 1615, loss 0.05416, acc 0.98
2016-09-06T13:50:51.508050: step 1616, loss 0.0194842, acc 1
2016-09-06T13:50:52.300946: step 1617, loss 0.0122394, acc 1
2016-09-06T13:50:53.093444: step 1618, loss 0.0553126, acc 0.98
2016-09-06T13:50:53.914374: step 1619, loss 0.199623, acc 0.96
2016-09-06T13:50:54.695139: step 1620, loss 0.0442131, acc 0.98
2016-09-06T13:50:55.497068: step 1621, loss 0.111457, acc 0.94
2016-09-06T13:50:56.317531: step 1622, loss 0.0350424, acc 0.98
2016-09-06T13:50:57.130528: step 1623, loss 0.0229064, acc 0.98
2016-09-06T13:50:57.957430: step 1624, loss 0.104223, acc 0.92
2016-09-06T13:50:58.779732: step 1625, loss 0.179836, acc 0.94
2016-09-06T13:50:59.559730: step 1626, loss 0.183261, acc 0.98
2016-09-06T13:51:00.400799: step 1627, loss 0.039571, acc 0.98
2016-09-06T13:51:01.219516: step 1628, loss 0.0118371, acc 1
2016-09-06T13:51:02.014752: step 1629, loss 0.0724795, acc 0.96
2016-09-06T13:51:02.844138: step 1630, loss 0.0351596, acc 1
2016-09-06T13:51:03.699885: step 1631, loss 0.0808027, acc 0.98
2016-09-06T13:51:04.538527: step 1632, loss 0.154634, acc 0.92
2016-09-06T13:51:05.347057: step 1633, loss 0.0279936, acc 1
2016-09-06T13:51:06.156345: step 1634, loss 0.212607, acc 0.94
2016-09-06T13:51:06.972495: step 1635, loss 0.0958384, acc 0.96
2016-09-06T13:51:07.769502: step 1636, loss 0.0711608, acc 0.96
2016-09-06T13:51:08.600321: step 1637, loss 0.179192, acc 0.96
2016-09-06T13:51:09.421255: step 1638, loss 0.146716, acc 0.94
2016-09-06T13:51:10.246694: step 1639, loss 0.148743, acc 0.94
2016-09-06T13:51:11.066114: step 1640, loss 0.0458373, acc 1
2016-09-06T13:51:11.879815: step 1641, loss 0.185098, acc 0.96
2016-09-06T13:51:12.688040: step 1642, loss 0.110205, acc 0.96
2016-09-06T13:51:13.532025: step 1643, loss 0.101257, acc 0.96
2016-09-06T13:51:14.344285: step 1644, loss 0.0527365, acc 1
2016-09-06T13:51:15.166823: step 1645, loss 0.0716468, acc 1
2016-09-06T13:51:16.010243: step 1646, loss 0.121962, acc 0.96
2016-09-06T13:51:16.831588: step 1647, loss 0.0243998, acc 1
2016-09-06T13:51:17.626953: step 1648, loss 0.111592, acc 0.96
2016-09-06T13:51:18.447020: step 1649, loss 0.098888, acc 0.94
2016-09-06T13:51:19.304523: step 1650, loss 0.021112, acc 1
2016-09-06T13:51:20.091963: step 1651, loss 0.0486814, acc 0.98
2016-09-06T13:51:20.895624: step 1652, loss 0.0514413, acc 1
2016-09-06T13:51:21.705392: step 1653, loss 0.140023, acc 0.96
2016-09-06T13:51:22.485253: step 1654, loss 0.116045, acc 0.96
2016-09-06T13:51:23.339786: step 1655, loss 0.0576407, acc 1
2016-09-06T13:51:24.177443: step 1656, loss 0.112216, acc 0.96
2016-09-06T13:51:24.982211: step 1657, loss 0.132459, acc 0.96
2016-09-06T13:51:25.785686: step 1658, loss 0.0866283, acc 0.98
2016-09-06T13:51:26.600268: step 1659, loss 0.0731497, acc 0.98
2016-09-06T13:51:27.423445: step 1660, loss 0.149745, acc 0.92
2016-09-06T13:51:28.228348: step 1661, loss 0.0864576, acc 0.96
2016-09-06T13:51:29.053205: step 1662, loss 0.0128519, acc 1
2016-09-06T13:51:29.849149: step 1663, loss 0.0615694, acc 0.98
2016-09-06T13:51:30.651560: step 1664, loss 0.127396, acc 0.94
2016-09-06T13:51:31.488566: step 1665, loss 0.0787193, acc 0.98
2016-09-06T13:51:32.288743: step 1666, loss 0.0377621, acc 0.98
2016-09-06T13:51:33.107637: step 1667, loss 0.0692128, acc 0.98
2016-09-06T13:51:33.945197: step 1668, loss 0.0691956, acc 0.96
2016-09-06T13:51:34.772053: step 1669, loss 0.0812694, acc 0.96
2016-09-06T13:51:35.641321: step 1670, loss 0.0574592, acc 0.98
2016-09-06T13:51:36.483238: step 1671, loss 0.0602039, acc 0.98
2016-09-06T13:51:37.290677: step 1672, loss 0.115421, acc 0.96
2016-09-06T13:51:38.097662: step 1673, loss 0.0763915, acc 0.98
2016-09-06T13:51:38.922515: step 1674, loss 0.278457, acc 0.92
2016-09-06T13:51:39.742438: step 1675, loss 0.0389371, acc 0.98
2016-09-06T13:51:40.562745: step 1676, loss 0.172505, acc 0.94
2016-09-06T13:51:41.384706: step 1677, loss 0.0417277, acc 0.98
2016-09-06T13:51:42.202745: step 1678, loss 0.0619544, acc 0.98
2016-09-06T13:51:43.009757: step 1679, loss 0.0198835, acc 1
2016-09-06T13:51:43.815344: step 1680, loss 0.140223, acc 0.92
2016-09-06T13:51:44.621684: step 1681, loss 0.0571466, acc 0.98
2016-09-06T13:51:45.417892: step 1682, loss 0.116503, acc 0.96
2016-09-06T13:51:46.209470: step 1683, loss 0.208506, acc 0.92
2016-09-06T13:51:47.059663: step 1684, loss 0.0295638, acc 1
2016-09-06T13:51:47.847950: step 1685, loss 0.045282, acc 1
2016-09-06T13:51:48.666278: step 1686, loss 0.0484294, acc 0.98
2016-09-06T13:51:49.491227: step 1687, loss 0.0558265, acc 0.98
2016-09-06T13:51:50.277652: step 1688, loss 0.0126136, acc 1
2016-09-06T13:51:51.111474: step 1689, loss 0.0341032, acc 1
2016-09-06T13:51:51.916265: step 1690, loss 0.0433338, acc 0.98
2016-09-06T13:51:52.766322: step 1691, loss 0.0673171, acc 0.98
2016-09-06T13:51:53.545808: step 1692, loss 0.0442465, acc 0.98
2016-09-06T13:51:54.379069: step 1693, loss 0.164446, acc 0.94
2016-09-06T13:51:55.155915: step 1694, loss 0.0599585, acc 0.96
2016-09-06T13:51:55.952731: step 1695, loss 0.0658325, acc 0.98
2016-09-06T13:51:56.770514: step 1696, loss 0.0253337, acc 1
2016-09-06T13:51:57.570109: step 1697, loss 0.0677277, acc 0.96
2016-09-06T13:51:58.377934: step 1698, loss 0.0404488, acc 0.98
2016-09-06T13:51:59.182849: step 1699, loss 0.087643, acc 0.94
2016-09-06T13:51:59.971743: step 1700, loss 0.0485038, acc 0.96

Evaluation:
2016-09-06T13:52:03.737794: step 1700, loss 1.27617, acc 0.745779

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-1700

2016-09-06T13:52:05.563398: step 1701, loss 0.08836, acc 0.96
2016-09-06T13:52:06.378655: step 1702, loss 0.0915605, acc 0.92
2016-09-06T13:52:07.201131: step 1703, loss 0.161635, acc 0.92
2016-09-06T13:52:08.038286: step 1704, loss 0.226433, acc 0.9
2016-09-06T13:52:08.838660: step 1705, loss 0.168055, acc 0.92
2016-09-06T13:52:09.649264: step 1706, loss 0.12549, acc 0.94
2016-09-06T13:52:10.467836: step 1707, loss 0.0487769, acc 1
2016-09-06T13:52:11.308652: step 1708, loss 0.0798263, acc 0.96
2016-09-06T13:52:12.086683: step 1709, loss 0.0560289, acc 1
2016-09-06T13:52:12.893262: step 1710, loss 0.0451875, acc 0.98
2016-09-06T13:52:13.717036: step 1711, loss 0.113771, acc 0.94
2016-09-06T13:52:14.517705: step 1712, loss 0.0178951, acc 1
2016-09-06T13:52:15.312290: step 1713, loss 0.0362044, acc 0.98
2016-09-06T13:52:16.137468: step 1714, loss 0.0778493, acc 0.94
2016-09-06T13:52:16.975443: step 1715, loss 0.11673, acc 0.92
2016-09-06T13:52:17.756678: step 1716, loss 0.0814363, acc 0.96
2016-09-06T13:52:18.557911: step 1717, loss 0.0141832, acc 1
2016-09-06T13:52:19.346188: step 1718, loss 0.202383, acc 0.94
2016-09-06T13:52:20.195253: step 1719, loss 0.0214422, acc 1
2016-09-06T13:52:21.026832: step 1720, loss 0.0339582, acc 0.98
2016-09-06T13:52:21.824563: step 1721, loss 0.0215461, acc 1
2016-09-06T13:52:22.639976: step 1722, loss 0.09637, acc 0.94
2016-09-06T13:52:23.490426: step 1723, loss 0.0594205, acc 0.96
2016-09-06T13:52:24.313511: step 1724, loss 0.0671205, acc 0.98
2016-09-06T13:52:25.124298: step 1725, loss 0.103372, acc 0.94
2016-09-06T13:52:25.947011: step 1726, loss 0.0426924, acc 1
2016-09-06T13:52:26.755762: step 1727, loss 0.01255, acc 1
2016-09-06T13:52:27.490063: step 1728, loss 0.0552281, acc 1
2016-09-06T13:52:28.328279: step 1729, loss 0.0250925, acc 1
2016-09-06T13:52:29.129221: step 1730, loss 0.0429347, acc 0.98
2016-09-06T13:52:29.946548: step 1731, loss 0.0584772, acc 0.98
2016-09-06T13:52:30.760766: step 1732, loss 0.0270256, acc 0.98
2016-09-06T13:52:31.545709: step 1733, loss 0.0167432, acc 1
2016-09-06T13:52:32.367900: step 1734, loss 0.0185303, acc 1
2016-09-06T13:52:33.220053: step 1735, loss 0.140542, acc 0.92
2016-09-06T13:52:34.052132: step 1736, loss 0.0657531, acc 0.98
2016-09-06T13:52:34.873786: step 1737, loss 0.0465348, acc 1
2016-09-06T13:52:35.701731: step 1738, loss 0.0872355, acc 0.96
2016-09-06T13:52:36.486418: step 1739, loss 0.00614765, acc 1
2016-09-06T13:52:37.274770: step 1740, loss 0.142033, acc 0.94
2016-09-06T13:52:38.100848: step 1741, loss 0.0477593, acc 0.98
2016-09-06T13:52:38.915226: step 1742, loss 0.0790788, acc 0.96
2016-09-06T13:52:39.720071: step 1743, loss 0.0151007, acc 1
2016-09-06T13:52:40.555592: step 1744, loss 0.0415908, acc 0.98
2016-09-06T13:52:41.359647: step 1745, loss 0.0487764, acc 1
2016-09-06T13:52:42.176475: step 1746, loss 0.0345947, acc 0.98
2016-09-06T13:52:43.026550: step 1747, loss 0.0747667, acc 0.98
2016-09-06T13:52:43.833926: step 1748, loss 0.0180718, acc 1
2016-09-06T13:52:44.658722: step 1749, loss 0.0321735, acc 0.98
2016-09-06T13:52:45.504461: step 1750, loss 0.0267987, acc 1
2016-09-06T13:52:46.304656: step 1751, loss 0.00899545, acc 1
2016-09-06T13:52:47.110477: step 1752, loss 0.0736677, acc 0.98
2016-09-06T13:52:47.930898: step 1753, loss 0.0791087, acc 0.98
2016-09-06T13:52:48.731258: step 1754, loss 0.0469559, acc 0.98
2016-09-06T13:52:49.555333: step 1755, loss 0.0160809, acc 1
2016-09-06T13:52:50.360384: step 1756, loss 0.172413, acc 0.94
2016-09-06T13:52:51.184698: step 1757, loss 0.0850269, acc 0.98
2016-09-06T13:52:51.979157: step 1758, loss 0.0874951, acc 0.94
2016-09-06T13:52:52.782234: step 1759, loss 0.0614541, acc 0.98
2016-09-06T13:52:53.596694: step 1760, loss 0.0959609, acc 0.96
2016-09-06T13:52:54.389347: step 1761, loss 0.0489525, acc 1
2016-09-06T13:52:55.200773: step 1762, loss 0.058672, acc 0.98
2016-09-06T13:52:56.034982: step 1763, loss 0.0410295, acc 0.98
2016-09-06T13:52:56.832960: step 1764, loss 0.0280468, acc 1
2016-09-06T13:52:57.642668: step 1765, loss 0.125434, acc 0.94
2016-09-06T13:52:58.510706: step 1766, loss 0.0424285, acc 0.98
2016-09-06T13:52:59.319067: step 1767, loss 0.02888, acc 1
2016-09-06T13:53:00.123805: step 1768, loss 0.0240161, acc 1
2016-09-06T13:53:01.001870: step 1769, loss 0.0645232, acc 0.96
2016-09-06T13:53:01.801788: step 1770, loss 0.065026, acc 1
2016-09-06T13:53:02.612400: step 1771, loss 0.0389397, acc 1
2016-09-06T13:53:03.443485: step 1772, loss 0.134304, acc 0.98
2016-09-06T13:53:04.260149: step 1773, loss 0.0690095, acc 0.94
2016-09-06T13:53:05.081703: step 1774, loss 0.0409027, acc 0.98
2016-09-06T13:53:05.902064: step 1775, loss 0.0830234, acc 0.96
2016-09-06T13:53:06.712614: step 1776, loss 0.042945, acc 0.98
2016-09-06T13:53:07.544784: step 1777, loss 0.0743745, acc 0.98
2016-09-06T13:53:08.386239: step 1778, loss 0.0441572, acc 0.98
2016-09-06T13:53:09.230484: step 1779, loss 0.0475189, acc 0.98
2016-09-06T13:53:10.064273: step 1780, loss 0.0465581, acc 1
2016-09-06T13:53:10.890793: step 1781, loss 0.0226668, acc 1
2016-09-06T13:53:11.698052: step 1782, loss 0.0541179, acc 0.98
2016-09-06T13:53:12.510229: step 1783, loss 0.0707986, acc 0.94
2016-09-06T13:53:13.365257: step 1784, loss 0.0832632, acc 0.94
2016-09-06T13:53:14.171507: step 1785, loss 0.0839697, acc 0.98
2016-09-06T13:53:14.967065: step 1786, loss 0.138875, acc 0.94
2016-09-06T13:53:15.805089: step 1787, loss 0.0407007, acc 1
2016-09-06T13:53:16.617249: step 1788, loss 0.161431, acc 0.92
2016-09-06T13:53:17.382720: step 1789, loss 0.0479609, acc 0.96
2016-09-06T13:53:18.188508: step 1790, loss 0.148777, acc 0.94
2016-09-06T13:53:19.017500: step 1791, loss 0.0433612, acc 0.98
2016-09-06T13:53:19.827168: step 1792, loss 0.091856, acc 0.98
2016-09-06T13:53:20.603447: step 1793, loss 0.0995406, acc 0.98
2016-09-06T13:53:21.425483: step 1794, loss 0.0537221, acc 0.98
2016-09-06T13:53:22.211336: step 1795, loss 0.0598044, acc 0.98
2016-09-06T13:53:23.017503: step 1796, loss 0.0637222, acc 0.96
2016-09-06T13:53:23.827087: step 1797, loss 0.0687361, acc 0.98
2016-09-06T13:53:24.622890: step 1798, loss 0.0462602, acc 0.98
2016-09-06T13:53:25.467353: step 1799, loss 0.0623718, acc 0.98
2016-09-06T13:53:26.269587: step 1800, loss 0.0781829, acc 0.96

Evaluation:
2016-09-06T13:53:29.971779: step 1800, loss 1.12813, acc 0.755159

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-1800

2016-09-06T13:53:31.743827: step 1801, loss 0.061513, acc 0.96
2016-09-06T13:53:32.581113: step 1802, loss 0.0424205, acc 0.98
2016-09-06T13:53:33.401705: step 1803, loss 0.0815938, acc 0.96
2016-09-06T13:53:34.195346: step 1804, loss 0.0304915, acc 1
2016-09-06T13:53:35.017617: step 1805, loss 0.0454097, acc 0.96
2016-09-06T13:53:35.831652: step 1806, loss 0.0714861, acc 0.96
2016-09-06T13:53:36.646092: step 1807, loss 0.0296702, acc 1
2016-09-06T13:53:37.443381: step 1808, loss 0.0659202, acc 0.98
2016-09-06T13:53:38.225072: step 1809, loss 0.0793923, acc 0.98
2016-09-06T13:53:39.027864: step 1810, loss 0.132576, acc 0.98
2016-09-06T13:53:39.882666: step 1811, loss 0.0582517, acc 0.98
2016-09-06T13:53:40.695360: step 1812, loss 0.0182418, acc 1
2016-09-06T13:53:41.495342: step 1813, loss 0.0120854, acc 1
2016-09-06T13:53:42.310651: step 1814, loss 0.0609268, acc 0.96
2016-09-06T13:53:43.117925: step 1815, loss 0.00404267, acc 1
2016-09-06T13:53:43.917085: step 1816, loss 0.0644417, acc 0.98
2016-09-06T13:53:44.778063: step 1817, loss 0.0175982, acc 1
2016-09-06T13:53:45.568631: step 1818, loss 0.00928566, acc 1
2016-09-06T13:53:46.414091: step 1819, loss 0.078657, acc 0.96
2016-09-06T13:53:47.251259: step 1820, loss 0.0231015, acc 1
2016-09-06T13:53:48.045913: step 1821, loss 0.156526, acc 0.9
2016-09-06T13:53:48.888078: step 1822, loss 0.0587667, acc 0.96
2016-09-06T13:53:49.679776: step 1823, loss 0.0342525, acc 0.98
2016-09-06T13:53:50.506897: step 1824, loss 0.0253827, acc 0.98
2016-09-06T13:53:51.307988: step 1825, loss 0.118747, acc 0.96
2016-09-06T13:53:52.109720: step 1826, loss 0.131911, acc 0.94
2016-09-06T13:53:52.937957: step 1827, loss 0.0782733, acc 0.98
2016-09-06T13:53:53.762735: step 1828, loss 0.0629668, acc 0.96
2016-09-06T13:53:54.569999: step 1829, loss 0.0849121, acc 0.96
2016-09-06T13:53:55.381792: step 1830, loss 0.00477086, acc 1
2016-09-06T13:53:56.169711: step 1831, loss 0.0698087, acc 0.98
2016-09-06T13:53:56.979188: step 1832, loss 0.063586, acc 0.98
2016-09-06T13:53:57.811867: step 1833, loss 0.0567284, acc 0.98
2016-09-06T13:53:58.601995: step 1834, loss 0.112213, acc 0.96
2016-09-06T13:53:59.390033: step 1835, loss 0.065553, acc 0.98
2016-09-06T13:54:00.216097: step 1836, loss 0.0390698, acc 1
2016-09-06T13:54:00.972208: step 1837, loss 0.0630426, acc 0.94
2016-09-06T13:54:01.773456: step 1838, loss 0.037627, acc 0.98
2016-09-06T13:54:02.568623: step 1839, loss 0.0714445, acc 0.98
2016-09-06T13:54:03.375245: step 1840, loss 0.066142, acc 0.98
2016-09-06T13:54:04.177123: step 1841, loss 0.114839, acc 0.98
2016-09-06T13:54:05.002337: step 1842, loss 0.0550092, acc 0.98
2016-09-06T13:54:05.837401: step 1843, loss 0.0778852, acc 0.98
2016-09-06T13:54:06.645482: step 1844, loss 0.10761, acc 0.96
2016-09-06T13:54:07.465346: step 1845, loss 0.0780767, acc 0.96
2016-09-06T13:54:08.234085: step 1846, loss 0.0306199, acc 1
2016-09-06T13:54:09.036081: step 1847, loss 0.159154, acc 0.9
2016-09-06T13:54:09.849358: step 1848, loss 0.0968678, acc 0.94
2016-09-06T13:54:10.646841: step 1849, loss 0.067664, acc 0.98
2016-09-06T13:54:11.491003: step 1850, loss 0.0406911, acc 0.98
2016-09-06T13:54:12.306198: step 1851, loss 0.0119972, acc 1
2016-09-06T13:54:13.095913: step 1852, loss 0.0343201, acc 1
2016-09-06T13:54:13.889350: step 1853, loss 0.078509, acc 0.94
2016-09-06T13:54:14.686405: step 1854, loss 0.104634, acc 0.94
2016-09-06T13:54:15.478213: step 1855, loss 0.069524, acc 0.98
2016-09-06T13:54:16.278889: step 1856, loss 0.096083, acc 0.98
2016-09-06T13:54:17.087043: step 1857, loss 0.0771142, acc 0.96
2016-09-06T13:54:17.879939: step 1858, loss 0.0886742, acc 0.96
2016-09-06T13:54:18.666724: step 1859, loss 0.0473568, acc 0.98
2016-09-06T13:54:19.481144: step 1860, loss 0.033253, acc 1
2016-09-06T13:54:20.282967: step 1861, loss 0.128582, acc 0.92
2016-09-06T13:54:21.086922: step 1862, loss 0.0560684, acc 0.98
2016-09-06T13:54:21.915587: step 1863, loss 0.312796, acc 0.9
2016-09-06T13:54:22.721383: step 1864, loss 0.0510079, acc 0.98
2016-09-06T13:54:23.541656: step 1865, loss 0.0848908, acc 0.98
2016-09-06T13:54:24.380471: step 1866, loss 0.0571719, acc 0.98
2016-09-06T13:54:25.174425: step 1867, loss 0.0281532, acc 0.98
2016-09-06T13:54:25.990530: step 1868, loss 0.0231862, acc 0.98
2016-09-06T13:54:26.798395: step 1869, loss 0.0921799, acc 0.96
2016-09-06T13:54:27.571414: step 1870, loss 0.0627893, acc 1
2016-09-06T13:54:28.371348: step 1871, loss 0.0675379, acc 0.98
2016-09-06T13:54:29.201074: step 1872, loss 0.0891004, acc 0.98
2016-09-06T13:54:29.996453: step 1873, loss 0.103587, acc 0.96
2016-09-06T13:54:30.798780: step 1874, loss 0.221554, acc 0.9
2016-09-06T13:54:31.628752: step 1875, loss 0.0339343, acc 1
2016-09-06T13:54:32.433016: step 1876, loss 0.0504267, acc 0.98
2016-09-06T13:54:33.235789: step 1877, loss 0.0728496, acc 0.98
2016-09-06T13:54:34.061264: step 1878, loss 0.0127952, acc 1
2016-09-06T13:54:34.851221: step 1879, loss 0.0203352, acc 1
2016-09-06T13:54:35.665852: step 1880, loss 0.03993, acc 0.98
2016-09-06T13:54:36.506996: step 1881, loss 0.0723251, acc 0.98
2016-09-06T13:54:37.318313: step 1882, loss 0.138859, acc 0.96
2016-09-06T13:54:38.146151: step 1883, loss 0.172398, acc 0.96
2016-09-06T13:54:38.970916: step 1884, loss 0.062202, acc 0.94
2016-09-06T13:54:39.783950: step 1885, loss 0.0291251, acc 0.98
2016-09-06T13:54:40.577236: step 1886, loss 0.0803234, acc 0.98
2016-09-06T13:54:41.405786: step 1887, loss 0.0633849, acc 0.96
2016-09-06T13:54:42.200174: step 1888, loss 0.060853, acc 0.96
2016-09-06T13:54:43.021211: step 1889, loss 0.0794869, acc 0.96
2016-09-06T13:54:43.872848: step 1890, loss 0.139003, acc 0.94
2016-09-06T13:54:44.661948: step 1891, loss 0.0323723, acc 0.98
2016-09-06T13:54:45.468953: step 1892, loss 0.050841, acc 0.98
2016-09-06T13:54:46.315848: step 1893, loss 0.0728762, acc 0.96
2016-09-06T13:54:47.120082: step 1894, loss 0.0351983, acc 1
2016-09-06T13:54:47.925934: step 1895, loss 0.0537767, acc 0.98
2016-09-06T13:54:48.761420: step 1896, loss 0.0608216, acc 0.96
2016-09-06T13:54:49.573814: step 1897, loss 0.101796, acc 0.96
2016-09-06T13:54:50.400673: step 1898, loss 0.0670431, acc 0.96
2016-09-06T13:54:51.238817: step 1899, loss 0.0455148, acc 1
2016-09-06T13:54:52.028131: step 1900, loss 0.0985419, acc 0.94

Evaluation:
2016-09-06T13:54:55.759439: step 1900, loss 1.10636, acc 0.758912

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-1900

2016-09-06T13:54:57.645264: step 1901, loss 0.0230288, acc 1
2016-09-06T13:54:58.474065: step 1902, loss 0.0616066, acc 0.94
2016-09-06T13:54:59.274381: step 1903, loss 0.103337, acc 0.94
2016-09-06T13:55:00.083613: step 1904, loss 0.0209846, acc 1
2016-09-06T13:55:00.970475: step 1905, loss 0.0524005, acc 0.98
2016-09-06T13:55:01.799421: step 1906, loss 0.102557, acc 0.96
2016-09-06T13:55:02.599052: step 1907, loss 0.0819615, acc 0.98
2016-09-06T13:55:03.424072: step 1908, loss 0.107686, acc 0.92
2016-09-06T13:55:04.255746: step 1909, loss 0.0169457, acc 1
2016-09-06T13:55:05.042227: step 1910, loss 0.0473284, acc 0.98
2016-09-06T13:55:05.885760: step 1911, loss 0.00906857, acc 1
2016-09-06T13:55:06.699189: step 1912, loss 0.014362, acc 1
2016-09-06T13:55:07.489369: step 1913, loss 0.0517489, acc 0.98
2016-09-06T13:55:08.278116: step 1914, loss 0.0379936, acc 0.98
2016-09-06T13:55:09.112512: step 1915, loss 0.0669059, acc 0.96
2016-09-06T13:55:09.884696: step 1916, loss 0.0101642, acc 1
2016-09-06T13:55:10.692182: step 1917, loss 0.102498, acc 0.96
2016-09-06T13:55:11.550466: step 1918, loss 0.118778, acc 0.94
2016-09-06T13:55:12.376694: step 1919, loss 0.0162506, acc 1
2016-09-06T13:55:13.121399: step 1920, loss 0.033579, acc 1
2016-09-06T13:55:13.938038: step 1921, loss 0.0769232, acc 0.98
2016-09-06T13:55:14.717194: step 1922, loss 0.0629239, acc 0.96
2016-09-06T13:55:15.515646: step 1923, loss 0.0345857, acc 0.98
2016-09-06T13:55:16.341096: step 1924, loss 0.0177444, acc 1
2016-09-06T13:55:17.112053: step 1925, loss 0.0527638, acc 0.98
2016-09-06T13:55:17.934843: step 1926, loss 0.0362458, acc 1
2016-09-06T13:55:18.786531: step 1927, loss 0.0716388, acc 0.96
2016-09-06T13:55:19.600523: step 1928, loss 0.0392703, acc 1
2016-09-06T13:55:20.401242: step 1929, loss 0.0538879, acc 0.96
2016-09-06T13:55:21.219875: step 1930, loss 0.0566029, acc 0.98
2016-09-06T13:55:22.024830: step 1931, loss 0.0464113, acc 0.98
2016-09-06T13:55:22.814359: step 1932, loss 0.0937776, acc 0.94
2016-09-06T13:55:23.643053: step 1933, loss 0.0465908, acc 0.98
2016-09-06T13:55:24.458666: step 1934, loss 0.050243, acc 0.98
2016-09-06T13:55:25.285014: step 1935, loss 0.0801189, acc 0.98
2016-09-06T13:55:26.135873: step 1936, loss 0.0080405, acc 1
2016-09-06T13:55:26.941398: step 1937, loss 0.0608734, acc 1
2016-09-06T13:55:27.752797: step 1938, loss 0.0156802, acc 1
2016-09-06T13:55:28.598080: step 1939, loss 0.0249044, acc 1
2016-09-06T13:55:29.407741: step 1940, loss 0.00763703, acc 1
2016-09-06T13:55:30.215993: step 1941, loss 0.0227975, acc 1
2016-09-06T13:55:31.045223: step 1942, loss 0.0333774, acc 0.96
2016-09-06T13:55:31.857147: step 1943, loss 0.0113501, acc 1
2016-09-06T13:55:32.646271: step 1944, loss 0.0836282, acc 0.98
2016-09-06T13:55:33.483986: step 1945, loss 0.0316072, acc 0.98
2016-09-06T13:55:34.291301: step 1946, loss 0.0585078, acc 0.98
2016-09-06T13:55:35.147883: step 1947, loss 0.0428675, acc 0.98
2016-09-06T13:55:35.987074: step 1948, loss 0.012281, acc 1
2016-09-06T13:55:36.801119: step 1949, loss 0.0256424, acc 1
2016-09-06T13:55:37.610800: step 1950, loss 0.0978451, acc 0.94
2016-09-06T13:55:38.427655: step 1951, loss 0.00949019, acc 1
2016-09-06T13:55:39.241361: step 1952, loss 0.010442, acc 1
2016-09-06T13:55:40.031422: step 1953, loss 0.145087, acc 0.96
2016-09-06T13:55:40.822027: step 1954, loss 0.0369663, acc 0.98
2016-09-06T13:55:41.639285: step 1955, loss 0.0563246, acc 0.96
2016-09-06T13:55:42.433637: step 1956, loss 0.0368656, acc 1
2016-09-06T13:55:43.272606: step 1957, loss 0.093344, acc 0.94
2016-09-06T13:55:44.078019: step 1958, loss 0.0233402, acc 1
2016-09-06T13:55:44.875907: step 1959, loss 0.0527721, acc 0.96
2016-09-06T13:55:45.687658: step 1960, loss 0.0660666, acc 0.98
2016-09-06T13:55:46.497913: step 1961, loss 0.0557659, acc 0.98
2016-09-06T13:55:47.293797: step 1962, loss 0.0519388, acc 0.98
2016-09-06T13:55:48.093416: step 1963, loss 0.00703033, acc 1
2016-09-06T13:55:48.900987: step 1964, loss 0.0418811, acc 0.98
2016-09-06T13:55:49.709458: step 1965, loss 0.111166, acc 0.94
2016-09-06T13:55:50.530487: step 1966, loss 0.0612011, acc 0.96
2016-09-06T13:55:51.332957: step 1967, loss 0.128379, acc 0.94
2016-09-06T13:55:52.098430: step 1968, loss 0.0916553, acc 0.94
2016-09-06T13:55:52.921597: step 1969, loss 0.0191124, acc 1
2016-09-06T13:55:53.715170: step 1970, loss 0.0672669, acc 0.96
2016-09-06T13:55:54.519530: step 1971, loss 0.0208786, acc 1
2016-09-06T13:55:55.347049: step 1972, loss 0.0692563, acc 0.98
2016-09-06T13:55:56.164820: step 1973, loss 0.0491283, acc 0.96
2016-09-06T13:55:56.963557: step 1974, loss 0.113896, acc 0.96
2016-09-06T13:55:57.748859: step 1975, loss 0.0551159, acc 0.98
2016-09-06T13:55:58.600399: step 1976, loss 0.091176, acc 0.98
2016-09-06T13:55:59.392656: step 1977, loss 0.0962207, acc 0.98
2016-09-06T13:56:00.208653: step 1978, loss 0.0127541, acc 1
2016-09-06T13:56:01.034249: step 1979, loss 0.105668, acc 0.94
2016-09-06T13:56:01.834529: step 1980, loss 0.0414589, acc 1
2016-09-06T13:56:02.629482: step 1981, loss 0.0113334, acc 1
2016-09-06T13:56:03.491359: step 1982, loss 0.0388121, acc 0.98
2016-09-06T13:56:04.297482: step 1983, loss 0.0226771, acc 1
2016-09-06T13:56:05.069407: step 1984, loss 0.133529, acc 0.96
2016-09-06T13:56:05.866779: step 1985, loss 0.0459787, acc 1
2016-09-06T13:56:06.645395: step 1986, loss 0.0406122, acc 1
2016-09-06T13:56:07.442690: step 1987, loss 0.0417465, acc 0.98
2016-09-06T13:56:08.268238: step 1988, loss 0.0696085, acc 0.96
2016-09-06T13:56:09.063744: step 1989, loss 0.243167, acc 0.92
2016-09-06T13:56:09.854098: step 1990, loss 0.0730935, acc 0.96
2016-09-06T13:56:10.669706: step 1991, loss 0.105384, acc 0.92
2016-09-06T13:56:11.450558: step 1992, loss 0.0647796, acc 0.98
2016-09-06T13:56:12.326146: step 1993, loss 0.0231378, acc 1
2016-09-06T13:56:13.129574: step 1994, loss 0.128464, acc 0.92
2016-09-06T13:56:13.913885: step 1995, loss 0.130922, acc 0.94
2016-09-06T13:56:14.758614: step 1996, loss 0.0565545, acc 0.98
2016-09-06T13:56:15.585410: step 1997, loss 0.150776, acc 0.92
2016-09-06T13:56:16.388616: step 1998, loss 0.0802687, acc 0.98
2016-09-06T13:56:17.190665: step 1999, loss 0.0589855, acc 0.98
2016-09-06T13:56:18.033301: step 2000, loss 0.0487506, acc 0.98

Evaluation:
2016-09-06T13:56:21.786130: step 2000, loss 1.01139, acc 0.760788

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-2000

2016-09-06T13:56:23.661453: step 2001, loss 0.00886949, acc 1
2016-09-06T13:56:24.536551: step 2002, loss 0.0288778, acc 1
2016-09-06T13:56:25.357973: step 2003, loss 0.119941, acc 0.94
2016-09-06T13:56:26.125918: step 2004, loss 0.0135829, acc 1
2016-09-06T13:56:26.947120: step 2005, loss 0.100779, acc 0.92
2016-09-06T13:56:27.756286: step 2006, loss 0.0462118, acc 0.98
2016-09-06T13:56:28.534117: step 2007, loss 0.0703385, acc 0.98
2016-09-06T13:56:29.345873: step 2008, loss 0.0562143, acc 0.98
2016-09-06T13:56:30.182848: step 2009, loss 0.0594824, acc 0.94
2016-09-06T13:56:30.991370: step 2010, loss 0.0482198, acc 0.98
2016-09-06T13:56:31.785428: step 2011, loss 0.0384332, acc 1
2016-09-06T13:56:32.607192: step 2012, loss 0.0490757, acc 0.98
2016-09-06T13:56:33.392486: step 2013, loss 0.115534, acc 0.94
2016-09-06T13:56:34.179522: step 2014, loss 0.213663, acc 0.92
2016-09-06T13:56:35.020443: step 2015, loss 0.0445742, acc 0.98
2016-09-06T13:56:35.789270: step 2016, loss 0.02478, acc 1
2016-09-06T13:56:36.589416: step 2017, loss 0.0664507, acc 0.98
2016-09-06T13:56:37.438890: step 2018, loss 0.245122, acc 0.9
2016-09-06T13:56:38.225587: step 2019, loss 0.0325621, acc 1
2016-09-06T13:56:39.018806: step 2020, loss 0.0249143, acc 1
2016-09-06T13:56:39.857032: step 2021, loss 0.0114411, acc 1
2016-09-06T13:56:40.660489: step 2022, loss 0.0481092, acc 0.98
2016-09-06T13:56:41.474325: step 2023, loss 0.0471088, acc 0.98
2016-09-06T13:56:42.263425: step 2024, loss 0.116129, acc 0.92
2016-09-06T13:56:43.049443: step 2025, loss 0.0542255, acc 0.98
2016-09-06T13:56:43.863619: step 2026, loss 0.0602631, acc 0.96
2016-09-06T13:56:44.668501: step 2027, loss 0.0440225, acc 1
2016-09-06T13:56:45.477889: step 2028, loss 0.051466, acc 0.98
2016-09-06T13:56:46.276369: step 2029, loss 0.0615392, acc 0.96
2016-09-06T13:56:47.102329: step 2030, loss 0.0496165, acc 1
2016-09-06T13:56:47.911956: step 2031, loss 0.0570531, acc 0.96
2016-09-06T13:56:48.720254: step 2032, loss 0.185609, acc 0.92
2016-09-06T13:56:49.522135: step 2033, loss 0.0277167, acc 1
2016-09-06T13:56:50.323663: step 2034, loss 0.0896954, acc 0.96
2016-09-06T13:56:51.124903: step 2035, loss 0.110012, acc 0.96
2016-09-06T13:56:51.941927: step 2036, loss 0.190391, acc 0.94
2016-09-06T13:56:52.736556: step 2037, loss 0.0800473, acc 0.94
2016-09-06T13:56:53.514095: step 2038, loss 0.027963, acc 0.98
2016-09-06T13:56:54.352480: step 2039, loss 0.0408094, acc 1
2016-09-06T13:56:55.153468: step 2040, loss 0.0372993, acc 0.98
2016-09-06T13:56:55.964947: step 2041, loss 0.0364447, acc 0.98
2016-09-06T13:56:56.837126: step 2042, loss 0.0318438, acc 1
2016-09-06T13:56:57.667193: step 2043, loss 0.0148859, acc 1
2016-09-06T13:56:58.493309: step 2044, loss 0.0496456, acc 0.96
2016-09-06T13:56:59.315926: step 2045, loss 0.110934, acc 0.92
2016-09-06T13:57:00.136476: step 2046, loss 0.146429, acc 0.94
2016-09-06T13:57:00.985055: step 2047, loss 0.0428075, acc 0.98
2016-09-06T13:57:01.811614: step 2048, loss 0.0329127, acc 1
2016-09-06T13:57:02.614991: step 2049, loss 0.0763869, acc 0.96
2016-09-06T13:57:03.414923: step 2050, loss 0.0408133, acc 1
2016-09-06T13:57:04.247799: step 2051, loss 0.0492089, acc 0.96
2016-09-06T13:57:05.068580: step 2052, loss 0.0529142, acc 0.98
2016-09-06T13:57:05.887507: step 2053, loss 0.0512686, acc 1
2016-09-06T13:57:06.730423: step 2054, loss 0.225398, acc 0.92
2016-09-06T13:57:07.561673: step 2055, loss 0.056419, acc 0.96
2016-09-06T13:57:08.348242: step 2056, loss 0.0628371, acc 0.96
2016-09-06T13:57:09.164934: step 2057, loss 0.0328544, acc 1
2016-09-06T13:57:09.967139: step 2058, loss 0.101323, acc 0.94
2016-09-06T13:57:10.777965: step 2059, loss 0.0335983, acc 1
2016-09-06T13:57:11.591427: step 2060, loss 0.132885, acc 0.94
2016-09-06T13:57:12.398240: step 2061, loss 0.0641803, acc 0.96
2016-09-06T13:57:13.190009: step 2062, loss 0.125769, acc 0.96
2016-09-06T13:57:14.008442: step 2063, loss 0.0339949, acc 0.98
2016-09-06T13:57:14.820903: step 2064, loss 0.033503, acc 0.98
2016-09-06T13:57:15.622690: step 2065, loss 0.0320655, acc 0.98
2016-09-06T13:57:16.449307: step 2066, loss 0.0430193, acc 1
2016-09-06T13:57:17.265741: step 2067, loss 0.0541027, acc 0.98
2016-09-06T13:57:18.069020: step 2068, loss 0.191349, acc 0.94
2016-09-06T13:57:18.885405: step 2069, loss 0.0817388, acc 0.96
2016-09-06T13:57:19.714269: step 2070, loss 0.0714037, acc 0.98
2016-09-06T13:57:20.516738: step 2071, loss 0.0119625, acc 1
2016-09-06T13:57:21.316485: step 2072, loss 0.022242, acc 1
2016-09-06T13:57:22.136278: step 2073, loss 0.129277, acc 0.96
2016-09-06T13:57:22.948583: step 2074, loss 0.0243691, acc 1
2016-09-06T13:57:23.760629: step 2075, loss 0.0660843, acc 0.98
2016-09-06T13:57:24.585482: step 2076, loss 0.07841, acc 0.96
2016-09-06T13:57:25.396581: step 2077, loss 0.058023, acc 0.98
2016-09-06T13:57:26.211843: step 2078, loss 0.137654, acc 0.92
2016-09-06T13:57:27.048572: step 2079, loss 0.0632806, acc 0.98
2016-09-06T13:57:27.874394: step 2080, loss 0.0776538, acc 0.96
2016-09-06T13:57:28.677356: step 2081, loss 0.0606187, acc 0.98
2016-09-06T13:57:29.503630: step 2082, loss 0.035259, acc 1
2016-09-06T13:57:30.339056: step 2083, loss 0.104538, acc 0.98
2016-09-06T13:57:31.187469: step 2084, loss 0.0711585, acc 0.98
2016-09-06T13:57:32.015811: step 2085, loss 0.0397422, acc 0.98
2016-09-06T13:57:32.842451: step 2086, loss 0.114877, acc 0.94
2016-09-06T13:57:33.672676: step 2087, loss 0.0299456, acc 1
2016-09-06T13:57:34.523565: step 2088, loss 0.118466, acc 0.94
2016-09-06T13:57:35.336838: step 2089, loss 0.0669379, acc 0.96
2016-09-06T13:57:36.137727: step 2090, loss 0.0386385, acc 0.98
2016-09-06T13:57:36.944316: step 2091, loss 0.0207088, acc 1
2016-09-06T13:57:37.742056: step 2092, loss 0.128982, acc 0.98
2016-09-06T13:57:38.519608: step 2093, loss 0.0383322, acc 0.98
2016-09-06T13:57:39.363266: step 2094, loss 0.0526136, acc 0.96
2016-09-06T13:57:40.168341: step 2095, loss 0.219576, acc 0.96
2016-09-06T13:57:40.972051: step 2096, loss 0.0365269, acc 0.98
2016-09-06T13:57:41.785990: step 2097, loss 0.0580443, acc 0.96
2016-09-06T13:57:42.595324: step 2098, loss 0.0863383, acc 0.98
2016-09-06T13:57:43.398667: step 2099, loss 0.0397868, acc 1
2016-09-06T13:57:44.254149: step 2100, loss 0.0139429, acc 1

Evaluation:
2016-09-06T13:57:47.962816: step 2100, loss 1.17178, acc 0.756098

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-2100

2016-09-06T13:57:49.764121: step 2101, loss 0.0570683, acc 0.96
2016-09-06T13:57:50.576079: step 2102, loss 0.0393692, acc 0.98
2016-09-06T13:57:51.403663: step 2103, loss 0.0259982, acc 1
2016-09-06T13:57:52.188545: step 2104, loss 0.0143903, acc 1
2016-09-06T13:57:52.981412: step 2105, loss 0.0245212, acc 1
2016-09-06T13:57:53.808192: step 2106, loss 0.103148, acc 0.92
2016-09-06T13:57:54.628334: step 2107, loss 0.0621237, acc 0.98
2016-09-06T13:57:55.441534: step 2108, loss 0.0110857, acc 1
2016-09-06T13:57:56.303405: step 2109, loss 0.0520526, acc 1
2016-09-06T13:57:57.108102: step 2110, loss 0.0765359, acc 0.94
2016-09-06T13:57:57.921586: step 2111, loss 0.124622, acc 0.98
2016-09-06T13:57:58.683928: step 2112, loss 0.0171639, acc 1
2016-09-06T13:57:59.523200: step 2113, loss 0.0412711, acc 0.98
2016-09-06T13:58:00.379046: step 2114, loss 0.0510562, acc 0.96
2016-09-06T13:58:01.230424: step 2115, loss 0.01752, acc 1
2016-09-06T13:58:02.040210: step 2116, loss 0.0249436, acc 0.98
2016-09-06T13:58:02.832243: step 2117, loss 0.0525858, acc 0.96
2016-09-06T13:58:03.667871: step 2118, loss 0.0909903, acc 0.94
2016-09-06T13:58:04.492707: step 2119, loss 0.0527338, acc 0.96
2016-09-06T13:58:05.313484: step 2120, loss 0.0229836, acc 1
2016-09-06T13:58:06.150814: step 2121, loss 0.0799164, acc 0.94
2016-09-06T13:58:06.949450: step 2122, loss 0.0856447, acc 0.94
2016-09-06T13:58:07.736820: step 2123, loss 0.0739776, acc 0.96
2016-09-06T13:58:08.539734: step 2124, loss 0.0309969, acc 1
2016-09-06T13:58:09.334604: step 2125, loss 0.048628, acc 0.96
2016-09-06T13:58:10.124393: step 2126, loss 0.0364413, acc 1
2016-09-06T13:58:10.932873: step 2127, loss 0.0468255, acc 0.98
2016-09-06T13:58:11.753105: step 2128, loss 0.0257703, acc 0.98
2016-09-06T13:58:12.537676: step 2129, loss 0.118527, acc 0.98
2016-09-06T13:58:13.384587: step 2130, loss 0.00467139, acc 1
2016-09-06T13:58:14.201353: step 2131, loss 0.0723849, acc 0.98
2016-09-06T13:58:14.994055: step 2132, loss 0.0256357, acc 0.98
2016-09-06T13:58:15.799347: step 2133, loss 0.0624611, acc 0.96
2016-09-06T13:58:16.648255: step 2134, loss 0.0200427, acc 1
2016-09-06T13:58:17.436863: step 2135, loss 0.0192954, acc 1
2016-09-06T13:58:18.229027: step 2136, loss 0.0745744, acc 0.98
2016-09-06T13:58:19.054872: step 2137, loss 0.0623427, acc 0.96
2016-09-06T13:58:19.860229: step 2138, loss 0.0307031, acc 1
2016-09-06T13:58:20.679010: step 2139, loss 0.0283595, acc 1
2016-09-06T13:58:21.496574: step 2140, loss 0.0382186, acc 0.98
2016-09-06T13:58:22.278549: step 2141, loss 0.0482449, acc 0.98
2016-09-06T13:58:23.038389: step 2142, loss 0.0575884, acc 0.98
2016-09-06T13:58:23.855354: step 2143, loss 0.0111461, acc 1
2016-09-06T13:58:24.647748: step 2144, loss 0.0861098, acc 0.98
2016-09-06T13:58:25.467935: step 2145, loss 0.0825295, acc 0.98
2016-09-06T13:58:26.302030: step 2146, loss 0.0141888, acc 1
2016-09-06T13:58:27.061274: step 2147, loss 0.0384924, acc 0.98
2016-09-06T13:58:27.873006: step 2148, loss 0.021567, acc 1
2016-09-06T13:58:28.698204: step 2149, loss 0.0725818, acc 0.96
2016-09-06T13:58:29.488894: step 2150, loss 0.0174097, acc 1
2016-09-06T13:58:30.317803: step 2151, loss 0.0974836, acc 0.92
2016-09-06T13:58:31.139435: step 2152, loss 0.0355919, acc 1
2016-09-06T13:58:31.922982: step 2153, loss 0.100337, acc 0.96
2016-09-06T13:58:32.709210: step 2154, loss 0.0208801, acc 1
2016-09-06T13:58:33.529504: step 2155, loss 0.010894, acc 1
2016-09-06T13:58:34.317235: step 2156, loss 0.116588, acc 0.94
2016-09-06T13:58:35.119154: step 2157, loss 0.0787513, acc 0.96
2016-09-06T13:58:35.945401: step 2158, loss 0.0821802, acc 0.98
2016-09-06T13:58:36.729982: step 2159, loss 0.0338017, acc 1
2016-09-06T13:58:37.535814: step 2160, loss 0.0669039, acc 0.94
2016-09-06T13:58:38.348173: step 2161, loss 0.0387674, acc 1
2016-09-06T13:58:39.187964: step 2162, loss 0.055916, acc 0.98
2016-09-06T13:58:39.990707: step 2163, loss 0.152545, acc 0.96
2016-09-06T13:58:40.829451: step 2164, loss 0.0594545, acc 0.96
2016-09-06T13:58:41.624676: step 2165, loss 0.016751, acc 1
2016-09-06T13:58:42.456413: step 2166, loss 0.122072, acc 0.98
2016-09-06T13:58:43.274843: step 2167, loss 0.0262952, acc 1
2016-09-06T13:58:44.078059: step 2168, loss 0.0666421, acc 0.96
2016-09-06T13:58:44.889879: step 2169, loss 0.0667549, acc 0.96
2016-09-06T13:58:45.738469: step 2170, loss 0.0171948, acc 1
2016-09-06T13:58:46.553574: step 2171, loss 0.112, acc 0.96
2016-09-06T13:58:47.351313: step 2172, loss 0.0240447, acc 1
2016-09-06T13:58:48.175832: step 2173, loss 0.00530883, acc 1
2016-09-06T13:58:49.002039: step 2174, loss 0.0411481, acc 1
2016-09-06T13:58:49.815489: step 2175, loss 0.0684095, acc 0.98
2016-09-06T13:58:50.644976: step 2176, loss 0.0609099, acc 0.98
2016-09-06T13:58:51.453930: step 2177, loss 0.0273112, acc 1
2016-09-06T13:58:52.270180: step 2178, loss 0.0536541, acc 0.96
2016-09-06T13:58:53.100148: step 2179, loss 0.0649954, acc 0.94
2016-09-06T13:58:53.938800: step 2180, loss 0.04224, acc 0.98
2016-09-06T13:58:54.753103: step 2181, loss 0.0459358, acc 0.96
2016-09-06T13:58:55.566138: step 2182, loss 0.0570801, acc 0.98
2016-09-06T13:58:56.374956: step 2183, loss 0.0225885, acc 1
2016-09-06T13:58:57.160162: step 2184, loss 0.0302942, acc 1
2016-09-06T13:58:57.985639: step 2185, loss 0.0777758, acc 0.96
2016-09-06T13:58:58.818064: step 2186, loss 0.146159, acc 0.94
2016-09-06T13:58:59.628222: step 2187, loss 0.105768, acc 0.98
2016-09-06T13:59:00.476965: step 2188, loss 0.0451266, acc 0.98
2016-09-06T13:59:01.288168: step 2189, loss 0.101462, acc 0.96
2016-09-06T13:59:02.087483: step 2190, loss 0.0323821, acc 1
2016-09-06T13:59:02.910156: step 2191, loss 0.132866, acc 0.94
2016-09-06T13:59:03.723758: step 2192, loss 0.0480842, acc 0.98
2016-09-06T13:59:04.519013: step 2193, loss 0.0378894, acc 0.98
2016-09-06T13:59:05.315971: step 2194, loss 0.0196184, acc 1
2016-09-06T13:59:06.142814: step 2195, loss 0.0639613, acc 1
2016-09-06T13:59:06.944614: step 2196, loss 0.0159437, acc 1
2016-09-06T13:59:07.746070: step 2197, loss 0.069945, acc 0.98
2016-09-06T13:59:08.553133: step 2198, loss 0.0502327, acc 0.96
2016-09-06T13:59:09.360397: step 2199, loss 0.058957, acc 0.98
2016-09-06T13:59:10.154982: step 2200, loss 0.0932812, acc 0.96

Evaluation:
2016-09-06T13:59:13.903704: step 2200, loss 1.31691, acc 0.748593

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-2200

2016-09-06T13:59:15.889077: step 2201, loss 0.118097, acc 0.96
2016-09-06T13:59:16.677332: step 2202, loss 0.0504642, acc 1
2016-09-06T13:59:17.489901: step 2203, loss 0.0405792, acc 0.96
2016-09-06T13:59:18.313128: step 2204, loss 0.0334962, acc 0.98
2016-09-06T13:59:19.106260: step 2205, loss 0.0570317, acc 0.98
2016-09-06T13:59:19.903502: step 2206, loss 0.121872, acc 0.92
2016-09-06T13:59:20.701030: step 2207, loss 0.033952, acc 1
2016-09-06T13:59:21.481030: step 2208, loss 0.0408104, acc 1
2016-09-06T13:59:22.263421: step 2209, loss 0.0924453, acc 0.92
2016-09-06T13:59:23.094384: step 2210, loss 0.0891233, acc 0.94
2016-09-06T13:59:23.861410: step 2211, loss 0.0250194, acc 1
2016-09-06T13:59:24.673179: step 2212, loss 0.117732, acc 0.98
2016-09-06T13:59:25.487773: step 2213, loss 0.0542217, acc 0.96
2016-09-06T13:59:26.277134: step 2214, loss 0.0270123, acc 1
2016-09-06T13:59:27.100790: step 2215, loss 0.0207554, acc 1
2016-09-06T13:59:27.955656: step 2216, loss 0.023765, acc 1
2016-09-06T13:59:28.760591: step 2217, loss 0.142768, acc 0.94
2016-09-06T13:59:29.570491: step 2218, loss 0.0724996, acc 0.94
2016-09-06T13:59:30.390645: step 2219, loss 0.0629971, acc 0.96
2016-09-06T13:59:31.211672: step 2220, loss 0.113907, acc 0.94
2016-09-06T13:59:32.029709: step 2221, loss 0.059195, acc 0.98
2016-09-06T13:59:32.875400: step 2222, loss 0.107996, acc 0.96
2016-09-06T13:59:33.678315: step 2223, loss 0.031226, acc 1
2016-09-06T13:59:34.502207: step 2224, loss 0.042121, acc 0.98
2016-09-06T13:59:35.343039: step 2225, loss 0.0414431, acc 1
2016-09-06T13:59:36.144910: step 2226, loss 0.0455819, acc 0.98
2016-09-06T13:59:36.972479: step 2227, loss 0.0913531, acc 0.94
2016-09-06T13:59:37.836196: step 2228, loss 0.0430722, acc 1
2016-09-06T13:59:38.665985: step 2229, loss 0.0782885, acc 0.96
2016-09-06T13:59:39.548449: step 2230, loss 0.0706042, acc 0.96
2016-09-06T13:59:40.382404: step 2231, loss 0.154664, acc 0.94
2016-09-06T13:59:41.202297: step 2232, loss 0.0229607, acc 0.98
2016-09-06T13:59:42.009317: step 2233, loss 0.0320546, acc 1
2016-09-06T13:59:42.840735: step 2234, loss 0.146657, acc 0.94
2016-09-06T13:59:43.686890: step 2235, loss 0.033398, acc 0.98
2016-09-06T13:59:44.485920: step 2236, loss 0.0811616, acc 0.96
2016-09-06T13:59:45.280210: step 2237, loss 0.112324, acc 0.9
2016-09-06T13:59:46.103587: step 2238, loss 0.00850152, acc 1
2016-09-06T13:59:46.901123: step 2239, loss 0.0726215, acc 0.96
2016-09-06T13:59:47.718262: step 2240, loss 0.0297091, acc 0.98
2016-09-06T13:59:48.536296: step 2241, loss 0.103152, acc 0.96
2016-09-06T13:59:49.347201: step 2242, loss 0.153367, acc 0.96
2016-09-06T13:59:50.150679: step 2243, loss 0.0321229, acc 1
2016-09-06T13:59:50.995907: step 2244, loss 0.059186, acc 0.96
2016-09-06T13:59:51.835207: step 2245, loss 0.0685095, acc 0.96
2016-09-06T13:59:52.613574: step 2246, loss 0.0733656, acc 0.98
2016-09-06T13:59:53.455871: step 2247, loss 0.0648317, acc 0.98
2016-09-06T13:59:54.231922: step 2248, loss 0.0116993, acc 1
2016-09-06T13:59:55.040561: step 2249, loss 0.0162997, acc 1
2016-09-06T13:59:55.876333: step 2250, loss 0.0686538, acc 0.98
2016-09-06T13:59:56.649023: step 2251, loss 0.0628564, acc 0.98
2016-09-06T13:59:57.507464: step 2252, loss 0.0146942, acc 1
2016-09-06T13:59:58.326780: step 2253, loss 0.0940591, acc 0.98
2016-09-06T13:59:59.138698: step 2254, loss 0.0263637, acc 0.98
2016-09-06T13:59:59.944061: step 2255, loss 0.0509676, acc 0.98
2016-09-06T14:00:00.818822: step 2256, loss 0.0572224, acc 0.98
2016-09-06T14:00:01.652534: step 2257, loss 0.03994, acc 0.98
2016-09-06T14:00:02.462579: step 2258, loss 0.060975, acc 0.98
2016-09-06T14:00:03.295653: step 2259, loss 0.0777614, acc 0.96
2016-09-06T14:00:04.102384: step 2260, loss 0.0335277, acc 1
2016-09-06T14:00:04.932812: step 2261, loss 0.0394024, acc 0.98
2016-09-06T14:00:05.762011: step 2262, loss 0.0390995, acc 0.98
2016-09-06T14:00:06.563722: step 2263, loss 0.0454036, acc 0.98
2016-09-06T14:00:07.356012: step 2264, loss 0.0580642, acc 0.98
2016-09-06T14:00:08.184457: step 2265, loss 0.0515356, acc 1
2016-09-06T14:00:09.009643: step 2266, loss 0.0222277, acc 1
2016-09-06T14:00:09.809420: step 2267, loss 0.05549, acc 0.98
2016-09-06T14:00:10.616442: step 2268, loss 0.136666, acc 0.96
2016-09-06T14:00:11.449710: step 2269, loss 0.0550304, acc 0.96
2016-09-06T14:00:12.241600: step 2270, loss 0.0728792, acc 0.96
2016-09-06T14:00:13.040086: step 2271, loss 0.0421879, acc 0.98
2016-09-06T14:00:13.845076: step 2272, loss 0.112803, acc 0.94
2016-09-06T14:00:14.615786: step 2273, loss 0.125767, acc 0.94
2016-09-06T14:00:15.443410: step 2274, loss 0.0874362, acc 0.92
2016-09-06T14:00:16.242576: step 2275, loss 0.0351852, acc 0.98
2016-09-06T14:00:17.028058: step 2276, loss 0.0788076, acc 0.98
2016-09-06T14:00:17.840128: step 2277, loss 0.0447211, acc 0.98
2016-09-06T14:00:18.629275: step 2278, loss 0.0706412, acc 0.96
2016-09-06T14:00:19.448286: step 2279, loss 0.0718903, acc 0.98
2016-09-06T14:00:20.258814: step 2280, loss 0.0804413, acc 0.96
2016-09-06T14:00:21.060866: step 2281, loss 0.0834656, acc 0.98
2016-09-06T14:00:21.864745: step 2282, loss 0.141428, acc 0.94
2016-09-06T14:00:22.671822: step 2283, loss 0.0717017, acc 0.96
2016-09-06T14:00:23.483063: step 2284, loss 0.0400256, acc 0.98
2016-09-06T14:00:24.296675: step 2285, loss 0.0231406, acc 0.98
2016-09-06T14:00:25.128386: step 2286, loss 0.0813455, acc 0.94
2016-09-06T14:00:25.932782: step 2287, loss 0.090205, acc 0.96
2016-09-06T14:00:26.721426: step 2288, loss 0.00704468, acc 1
2016-09-06T14:00:27.525409: step 2289, loss 0.145394, acc 0.96
2016-09-06T14:00:28.321405: step 2290, loss 0.0807193, acc 0.92
2016-09-06T14:00:29.132171: step 2291, loss 0.0988428, acc 0.98
2016-09-06T14:00:30.001456: step 2292, loss 0.0298891, acc 0.98
2016-09-06T14:00:30.834209: step 2293, loss 0.0348402, acc 0.98
2016-09-06T14:00:31.645714: step 2294, loss 0.0293814, acc 0.98
2016-09-06T14:00:32.456109: step 2295, loss 0.0715934, acc 0.96
2016-09-06T14:00:33.284446: step 2296, loss 0.0169023, acc 1
2016-09-06T14:00:34.112630: step 2297, loss 0.0369394, acc 0.98
2016-09-06T14:00:34.908542: step 2298, loss 0.0399648, acc 1
2016-09-06T14:00:35.754862: step 2299, loss 0.0269289, acc 1
2016-09-06T14:00:36.557089: step 2300, loss 0.0221764, acc 0.98

Evaluation:
2016-09-06T14:00:40.288130: step 2300, loss 1.11901, acc 0.758912

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-2300

2016-09-06T14:00:42.153140: step 2301, loss 0.0308476, acc 1
2016-09-06T14:00:42.990336: step 2302, loss 0.0583989, acc 0.98
2016-09-06T14:00:43.841343: step 2303, loss 0.0224306, acc 1
2016-09-06T14:00:44.608565: step 2304, loss 0.110735, acc 0.954545
2016-09-06T14:00:45.456762: step 2305, loss 0.0447473, acc 1
2016-09-06T14:00:46.252399: step 2306, loss 0.0358114, acc 1
2016-09-06T14:00:47.062705: step 2307, loss 0.00633345, acc 1
2016-09-06T14:00:47.894027: step 2308, loss 0.0530008, acc 0.96
2016-09-06T14:00:48.685024: step 2309, loss 0.0396143, acc 0.98
2016-09-06T14:00:49.485437: step 2310, loss 0.0559612, acc 0.96
2016-09-06T14:00:50.312770: step 2311, loss 0.0879805, acc 0.96
2016-09-06T14:00:51.110888: step 2312, loss 0.0501316, acc 0.98
2016-09-06T14:00:51.929620: step 2313, loss 0.072087, acc 0.96
2016-09-06T14:00:52.753447: step 2314, loss 0.0581362, acc 0.96
2016-09-06T14:00:53.568886: step 2315, loss 0.0404182, acc 0.98
2016-09-06T14:00:54.368339: step 2316, loss 0.0297179, acc 1
2016-09-06T14:00:55.190936: step 2317, loss 0.149185, acc 0.9
2016-09-06T14:00:55.996808: step 2318, loss 0.0455441, acc 0.98
2016-09-06T14:00:56.803484: step 2319, loss 0.0304803, acc 1
2016-09-06T14:00:57.651508: step 2320, loss 0.0111248, acc 1
2016-09-06T14:00:58.465285: step 2321, loss 0.0587075, acc 0.98
2016-09-06T14:00:59.295596: step 2322, loss 0.00858951, acc 1
2016-09-06T14:01:00.123560: step 2323, loss 0.0203131, acc 1
2016-09-06T14:01:00.970002: step 2324, loss 0.0351477, acc 1
2016-09-06T14:01:01.749222: step 2325, loss 0.0222193, acc 1
2016-09-06T14:01:02.601553: step 2326, loss 0.0132065, acc 1
2016-09-06T14:01:03.444011: step 2327, loss 0.0152384, acc 1
2016-09-06T14:01:04.241554: step 2328, loss 0.082293, acc 0.98
2016-09-06T14:01:05.059823: step 2329, loss 0.0175079, acc 1
2016-09-06T14:01:05.892859: step 2330, loss 0.0177143, acc 1
2016-09-06T14:01:06.668809: step 2331, loss 0.0937081, acc 0.96
2016-09-06T14:01:07.448305: step 2332, loss 0.0679056, acc 0.98
2016-09-06T14:01:08.264123: step 2333, loss 0.118567, acc 0.96
2016-09-06T14:01:09.089337: step 2334, loss 0.108938, acc 0.98
2016-09-06T14:01:09.902263: step 2335, loss 0.0550873, acc 0.98
2016-09-06T14:01:10.751590: step 2336, loss 0.00499239, acc 1
2016-09-06T14:01:11.549307: step 2337, loss 0.0330902, acc 0.98
2016-09-06T14:01:12.329906: step 2338, loss 0.073414, acc 0.96
2016-09-06T14:01:13.162652: step 2339, loss 0.0484194, acc 0.96
2016-09-06T14:01:13.957948: step 2340, loss 0.022916, acc 0.98
2016-09-06T14:01:14.756545: step 2341, loss 0.112737, acc 0.96
2016-09-06T14:01:15.561748: step 2342, loss 0.0334607, acc 0.96
2016-09-06T14:01:16.327741: step 2343, loss 0.018548, acc 1
2016-09-06T14:01:17.118694: step 2344, loss 0.0117971, acc 1
2016-09-06T14:01:17.924518: step 2345, loss 0.0357481, acc 0.98
2016-09-06T14:01:18.734701: step 2346, loss 0.0138097, acc 1
2016-09-06T14:01:19.528120: step 2347, loss 0.013872, acc 1
2016-09-06T14:01:20.340518: step 2348, loss 0.0784485, acc 0.98
2016-09-06T14:01:21.121038: step 2349, loss 0.034863, acc 1
2016-09-06T14:01:21.936842: step 2350, loss 0.0210974, acc 1
2016-09-06T14:01:22.751456: step 2351, loss 0.0545907, acc 0.96
2016-09-06T14:01:23.522176: step 2352, loss 0.0198833, acc 1
2016-09-06T14:01:24.339401: step 2353, loss 0.10636, acc 0.96
2016-09-06T14:01:25.161406: step 2354, loss 0.0481459, acc 0.96
2016-09-06T14:01:25.948602: step 2355, loss 0.0439277, acc 1
2016-09-06T14:01:26.762941: step 2356, loss 0.0103188, acc 1
2016-09-06T14:01:27.589196: step 2357, loss 0.0597134, acc 0.98
2016-09-06T14:01:28.384081: step 2358, loss 0.0210249, acc 0.98
2016-09-06T14:01:29.202054: step 2359, loss 0.0249852, acc 0.98
2016-09-06T14:01:30.025445: step 2360, loss 0.0362569, acc 0.96
2016-09-06T14:01:30.801025: step 2361, loss 0.0682889, acc 0.98
2016-09-06T14:01:31.621359: step 2362, loss 0.0140432, acc 1
2016-09-06T14:01:32.422925: step 2363, loss 0.0355862, acc 0.98
2016-09-06T14:01:33.210172: step 2364, loss 0.0392557, acc 0.98
2016-09-06T14:01:34.027243: step 2365, loss 0.0541845, acc 0.96
2016-09-06T14:01:34.838413: step 2366, loss 0.0126898, acc 1
2016-09-06T14:01:35.625842: step 2367, loss 0.0276822, acc 0.98
2016-09-06T14:01:36.423008: step 2368, loss 0.0420781, acc 0.98
2016-09-06T14:01:37.233402: step 2369, loss 0.0880316, acc 0.94
2016-09-06T14:01:38.026227: step 2370, loss 0.0240472, acc 1
2016-09-06T14:01:38.851702: step 2371, loss 0.0180111, acc 1
2016-09-06T14:01:39.674446: step 2372, loss 0.0603284, acc 0.94
2016-09-06T14:01:40.470331: step 2373, loss 0.126094, acc 0.96
2016-09-06T14:01:41.275644: step 2374, loss 0.114235, acc 0.96
2016-09-06T14:01:42.110266: step 2375, loss 0.121489, acc 0.94
2016-09-06T14:01:42.903660: step 2376, loss 0.07687, acc 0.98
2016-09-06T14:01:43.725102: step 2377, loss 0.0193419, acc 1
2016-09-06T14:01:44.558945: step 2378, loss 0.00809874, acc 1
2016-09-06T14:01:45.343091: step 2379, loss 0.0214868, acc 1
2016-09-06T14:01:46.152172: step 2380, loss 0.0656254, acc 0.96
2016-09-06T14:01:46.985348: step 2381, loss 0.0510849, acc 0.98
2016-09-06T14:01:47.784643: step 2382, loss 0.0335559, acc 1
2016-09-06T14:01:48.560367: step 2383, loss 0.028168, acc 1
2016-09-06T14:01:49.373302: step 2384, loss 0.0366357, acc 0.98
2016-09-06T14:01:50.165426: step 2385, loss 0.0643946, acc 0.98
2016-09-06T14:01:50.976541: step 2386, loss 0.0185007, acc 1
2016-09-06T14:01:51.801670: step 2387, loss 0.0300355, acc 1
2016-09-06T14:01:52.577789: step 2388, loss 0.044246, acc 0.98
2016-09-06T14:01:53.398043: step 2389, loss 0.024326, acc 0.98
2016-09-06T14:01:54.236977: step 2390, loss 0.0363507, acc 0.98
2016-09-06T14:01:55.029240: step 2391, loss 0.0122891, acc 1
2016-09-06T14:01:55.804536: step 2392, loss 0.0795305, acc 0.96
2016-09-06T14:01:56.645114: step 2393, loss 0.0298612, acc 1
2016-09-06T14:01:57.417616: step 2394, loss 0.0431936, acc 0.98
2016-09-06T14:01:58.233991: step 2395, loss 0.0236675, acc 0.98
2016-09-06T14:01:59.057703: step 2396, loss 0.0201681, acc 1
2016-09-06T14:01:59.813388: step 2397, loss 0.0155558, acc 1
2016-09-06T14:02:00.670339: step 2398, loss 0.0265736, acc 1
2016-09-06T14:02:01.493978: step 2399, loss 0.0202643, acc 1
2016-09-06T14:02:02.283128: step 2400, loss 0.0083898, acc 1

Evaluation:
2016-09-06T14:02:05.999941: step 2400, loss 1.41092, acc 0.75985

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-2400

2016-09-06T14:02:07.941490: step 2401, loss 0.034564, acc 1
2016-09-06T14:02:08.764389: step 2402, loss 0.04243, acc 0.96
2016-09-06T14:02:09.578111: step 2403, loss 0.0724166, acc 0.94
2016-09-06T14:02:10.391066: step 2404, loss 0.127739, acc 0.94
2016-09-06T14:02:11.213320: step 2405, loss 0.00818905, acc 1
2016-09-06T14:02:12.012934: step 2406, loss 0.0317236, acc 0.98
2016-09-06T14:02:12.826101: step 2407, loss 0.0307756, acc 0.98
2016-09-06T14:02:13.623003: step 2408, loss 0.00683318, acc 1
2016-09-06T14:02:14.405646: step 2409, loss 0.0602055, acc 0.98
2016-09-06T14:02:15.193496: step 2410, loss 0.119145, acc 0.98
2016-09-06T14:02:15.989767: step 2411, loss 0.187791, acc 0.92
2016-09-06T14:02:16.784137: step 2412, loss 0.0508832, acc 0.96
2016-09-06T14:02:17.627128: step 2413, loss 0.03364, acc 0.96
2016-09-06T14:02:18.448462: step 2414, loss 0.0663631, acc 0.96
2016-09-06T14:02:19.238841: step 2415, loss 0.0177344, acc 1
2016-09-06T14:02:20.067966: step 2416, loss 0.0636076, acc 0.98
2016-09-06T14:02:20.902190: step 2417, loss 0.163002, acc 0.94
2016-09-06T14:02:21.700457: step 2418, loss 0.070523, acc 0.96
2016-09-06T14:02:22.526164: step 2419, loss 0.0302521, acc 1
2016-09-06T14:02:23.354801: step 2420, loss 0.0236903, acc 1
2016-09-06T14:02:24.133427: step 2421, loss 0.126596, acc 0.96
2016-09-06T14:02:24.937415: step 2422, loss 0.0217584, acc 1
2016-09-06T14:02:25.753494: step 2423, loss 0.0825067, acc 0.96
2016-09-06T14:02:26.560902: step 2424, loss 0.0571847, acc 0.98
2016-09-06T14:02:27.368744: step 2425, loss 0.0147964, acc 1
2016-09-06T14:02:28.220506: step 2426, loss 0.0128916, acc 1
2016-09-06T14:02:29.038544: step 2427, loss 0.0332701, acc 0.98
2016-09-06T14:02:29.829587: step 2428, loss 0.0460787, acc 0.98
2016-09-06T14:02:30.707779: step 2429, loss 0.086592, acc 0.96
2016-09-06T14:02:31.521869: step 2430, loss 0.0299237, acc 1
2016-09-06T14:02:32.350150: step 2431, loss 0.0491828, acc 0.96
2016-09-06T14:02:33.215379: step 2432, loss 0.0555342, acc 0.96
2016-09-06T14:02:34.017240: step 2433, loss 0.0571811, acc 0.98
2016-09-06T14:02:34.840548: step 2434, loss 0.0630773, acc 0.96
2016-09-06T14:02:35.658377: step 2435, loss 0.0209765, acc 0.98
2016-09-06T14:02:36.481038: step 2436, loss 0.252021, acc 0.94
2016-09-06T14:02:37.319152: step 2437, loss 0.0172238, acc 1
2016-09-06T14:02:38.120391: step 2438, loss 0.0819152, acc 0.98
2016-09-06T14:02:38.923697: step 2439, loss 0.075175, acc 0.94
2016-09-06T14:02:39.720488: step 2440, loss 0.244036, acc 0.96
2016-09-06T14:02:40.526636: step 2441, loss 0.02966, acc 1
2016-09-06T14:02:41.350704: step 2442, loss 0.0600356, acc 0.96
2016-09-06T14:02:42.140392: step 2443, loss 0.0180781, acc 1
2016-09-06T14:02:42.943022: step 2444, loss 0.0324444, acc 0.98
2016-09-06T14:02:43.754278: step 2445, loss 0.120051, acc 0.98
2016-09-06T14:02:44.533918: step 2446, loss 0.0368613, acc 0.98
2016-09-06T14:02:45.364512: step 2447, loss 0.0251848, acc 1
2016-09-06T14:02:46.189775: step 2448, loss 0.093112, acc 0.94
2016-09-06T14:02:46.991391: step 2449, loss 0.0640023, acc 0.98
2016-09-06T14:02:47.800699: step 2450, loss 0.0236641, acc 0.98
2016-09-06T14:02:48.638495: step 2451, loss 0.0976058, acc 0.96
2016-09-06T14:02:49.404316: step 2452, loss 0.0344144, acc 0.98
2016-09-06T14:02:50.199954: step 2453, loss 0.154664, acc 0.92
2016-09-06T14:02:51.001483: step 2454, loss 0.0394307, acc 0.98
2016-09-06T14:02:51.776325: step 2455, loss 0.0494044, acc 0.98
2016-09-06T14:02:52.558060: step 2456, loss 0.0199313, acc 1
2016-09-06T14:02:53.352101: step 2457, loss 0.031499, acc 1
2016-09-06T14:02:54.169469: step 2458, loss 0.0386951, acc 0.98
2016-09-06T14:02:55.019451: step 2459, loss 0.122211, acc 0.96
2016-09-06T14:02:55.842889: step 2460, loss 0.0715795, acc 0.96
2016-09-06T14:02:56.647796: step 2461, loss 0.104008, acc 0.96
2016-09-06T14:02:57.462562: step 2462, loss 0.0570794, acc 0.98
2016-09-06T14:02:58.303325: step 2463, loss 0.0624482, acc 0.96
2016-09-06T14:02:59.104870: step 2464, loss 0.106402, acc 0.94
2016-09-06T14:02:59.913493: step 2465, loss 0.117471, acc 0.94
2016-09-06T14:03:00.747138: step 2466, loss 0.0192749, acc 1
2016-09-06T14:03:01.554712: step 2467, loss 0.0196944, acc 1
2016-09-06T14:03:02.363504: step 2468, loss 0.0523637, acc 0.98
2016-09-06T14:03:03.199626: step 2469, loss 0.0418401, acc 0.98
2016-09-06T14:03:03.995525: step 2470, loss 0.0451447, acc 1
2016-09-06T14:03:04.822546: step 2471, loss 0.0218682, acc 1
2016-09-06T14:03:05.656288: step 2472, loss 0.0384725, acc 1
2016-09-06T14:03:06.469806: step 2473, loss 0.087893, acc 0.94
2016-09-06T14:03:07.284312: step 2474, loss 0.0294782, acc 1
2016-09-06T14:03:08.096403: step 2475, loss 0.0360006, acc 1
2016-09-06T14:03:08.927846: step 2476, loss 0.0253173, acc 1
2016-09-06T14:03:09.747338: step 2477, loss 0.0484514, acc 0.98
2016-09-06T14:03:10.602832: step 2478, loss 0.0337405, acc 1
2016-09-06T14:03:11.431739: step 2479, loss 0.0547833, acc 0.98
2016-09-06T14:03:12.242978: step 2480, loss 0.0711955, acc 0.96
2016-09-06T14:03:13.062251: step 2481, loss 0.0724397, acc 0.94
2016-09-06T14:03:13.885250: step 2482, loss 0.0533395, acc 1
2016-09-06T14:03:14.691330: step 2483, loss 0.0895336, acc 0.98
2016-09-06T14:03:15.550318: step 2484, loss 0.0294701, acc 1
2016-09-06T14:03:16.355866: step 2485, loss 0.0241722, acc 1
2016-09-06T14:03:17.184546: step 2486, loss 0.0243278, acc 0.98
2016-09-06T14:03:18.016588: step 2487, loss 0.00557521, acc 1
2016-09-06T14:03:18.822769: step 2488, loss 0.0412575, acc 0.98
2016-09-06T14:03:19.606113: step 2489, loss 0.0621107, acc 0.96
2016-09-06T14:03:20.451266: step 2490, loss 0.0604683, acc 0.94
2016-09-06T14:03:21.266306: step 2491, loss 0.0645014, acc 0.98
2016-09-06T14:03:22.045316: step 2492, loss 0.0261264, acc 0.98
2016-09-06T14:03:22.865466: step 2493, loss 0.0554353, acc 0.98
2016-09-06T14:03:23.677110: step 2494, loss 0.124466, acc 0.98
2016-09-06T14:03:24.452761: step 2495, loss 0.025764, acc 1
2016-09-06T14:03:25.197547: step 2496, loss 0.0108825, acc 1
2016-09-06T14:03:26.013805: step 2497, loss 0.241565, acc 0.98
2016-09-06T14:03:26.809828: step 2498, loss 0.123307, acc 0.94
2016-09-06T14:03:27.629022: step 2499, loss 0.0217863, acc 1
2016-09-06T14:03:28.457275: step 2500, loss 0.0641405, acc 0.98

Evaluation:
2016-09-06T14:03:32.158173: step 2500, loss 1.36336, acc 0.756098

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-2500

2016-09-06T14:03:34.026444: step 2501, loss 0.0115692, acc 1
2016-09-06T14:03:34.860681: step 2502, loss 0.0437589, acc 0.98
2016-09-06T14:03:35.689455: step 2503, loss 0.0135233, acc 1
2016-09-06T14:03:36.513501: step 2504, loss 0.0727987, acc 0.98
2016-09-06T14:03:37.335531: step 2505, loss 0.0249979, acc 1
2016-09-06T14:03:38.134470: step 2506, loss 0.0265839, acc 1
2016-09-06T14:03:38.964602: step 2507, loss 0.0261838, acc 0.98
2016-09-06T14:03:39.769467: step 2508, loss 0.0106127, acc 1
2016-09-06T14:03:40.581409: step 2509, loss 0.0435692, acc 0.98
2016-09-06T14:03:41.381661: step 2510, loss 0.038228, acc 0.98
2016-09-06T14:03:42.250830: step 2511, loss 0.0616201, acc 0.96
2016-09-06T14:03:43.074012: step 2512, loss 0.0312643, acc 0.98
2016-09-06T14:03:43.902360: step 2513, loss 0.0482986, acc 0.96
2016-09-06T14:03:44.716612: step 2514, loss 0.0196534, acc 1
2016-09-06T14:03:45.564264: step 2515, loss 0.0212783, acc 1
2016-09-06T14:03:46.326118: step 2516, loss 0.0536616, acc 0.98
2016-09-06T14:03:47.132150: step 2517, loss 0.021253, acc 1
2016-09-06T14:03:47.954744: step 2518, loss 0.0611697, acc 0.98
2016-09-06T14:03:48.731361: step 2519, loss 0.0769192, acc 0.98
2016-09-06T14:03:49.545006: step 2520, loss 0.0503467, acc 1
2016-09-06T14:03:50.381812: step 2521, loss 0.0110475, acc 1
2016-09-06T14:03:51.179881: step 2522, loss 0.00869259, acc 1
2016-09-06T14:03:51.984564: step 2523, loss 0.0635644, acc 0.98
2016-09-06T14:03:52.854038: step 2524, loss 0.040164, acc 0.98
2016-09-06T14:03:53.665637: step 2525, loss 0.0165488, acc 1
2016-09-06T14:03:54.488788: step 2526, loss 0.156102, acc 0.96
2016-09-06T14:03:55.331709: step 2527, loss 0.0242275, acc 0.98
2016-09-06T14:03:56.143741: step 2528, loss 0.0481651, acc 0.98
2016-09-06T14:03:56.956781: step 2529, loss 0.0166038, acc 1
2016-09-06T14:03:57.793963: step 2530, loss 0.0353625, acc 1
2016-09-06T14:03:58.617756: step 2531, loss 0.00486251, acc 1
2016-09-06T14:03:59.433275: step 2532, loss 0.0508717, acc 0.98
2016-09-06T14:04:00.290579: step 2533, loss 0.127472, acc 0.96
2016-09-06T14:04:01.115063: step 2534, loss 0.0540163, acc 0.98
2016-09-06T14:04:01.939196: step 2535, loss 0.0202723, acc 1
2016-09-06T14:04:02.792701: step 2536, loss 0.0148028, acc 1
2016-09-06T14:04:03.607632: step 2537, loss 0.0226372, acc 1
2016-09-06T14:04:04.453434: step 2538, loss 0.0202976, acc 1
2016-09-06T14:04:05.305449: step 2539, loss 0.0566912, acc 0.98
2016-09-06T14:04:06.115317: step 2540, loss 0.0158999, acc 1
2016-09-06T14:04:06.920778: step 2541, loss 0.0912567, acc 0.98
2016-09-06T14:04:07.737153: step 2542, loss 0.147776, acc 0.96
2016-09-06T14:04:08.589308: step 2543, loss 0.0658507, acc 0.96
2016-09-06T14:04:09.402667: step 2544, loss 0.0214883, acc 0.98
2016-09-06T14:04:10.192528: step 2545, loss 0.0276189, acc 1
2016-09-06T14:04:11.021867: step 2546, loss 0.0766076, acc 0.98
2016-09-06T14:04:11.852052: step 2547, loss 0.0362187, acc 0.98
2016-09-06T14:04:12.648692: step 2548, loss 0.0427328, acc 1
2016-09-06T14:04:13.479073: step 2549, loss 0.0838051, acc 0.98
2016-09-06T14:04:14.305782: step 2550, loss 0.031095, acc 1
2016-09-06T14:04:15.127690: step 2551, loss 0.0470152, acc 0.98
2016-09-06T14:04:15.960595: step 2552, loss 0.135108, acc 0.98
2016-09-06T14:04:16.750636: step 2553, loss 0.0697168, acc 0.94
2016-09-06T14:04:17.589685: step 2554, loss 0.00782552, acc 1
2016-09-06T14:04:18.418293: step 2555, loss 0.0480558, acc 0.96
2016-09-06T14:04:19.223626: step 2556, loss 0.0462913, acc 0.98
2016-09-06T14:04:20.077001: step 2557, loss 0.0159689, acc 1
2016-09-06T14:04:20.934249: step 2558, loss 0.0843784, acc 0.96
2016-09-06T14:04:21.755388: step 2559, loss 0.0231016, acc 1
2016-09-06T14:04:22.542762: step 2560, loss 0.133443, acc 0.96
2016-09-06T14:04:23.338403: step 2561, loss 0.0367756, acc 0.98
2016-09-06T14:04:24.182562: step 2562, loss 0.0791632, acc 0.96
2016-09-06T14:04:24.972911: step 2563, loss 0.022871, acc 1
2016-09-06T14:04:25.791345: step 2564, loss 0.10351, acc 0.94
2016-09-06T14:04:26.600899: step 2565, loss 0.0466952, acc 0.96
2016-09-06T14:04:27.395490: step 2566, loss 0.0329691, acc 1
2016-09-06T14:04:28.200218: step 2567, loss 0.0266999, acc 1
2016-09-06T14:04:29.027714: step 2568, loss 0.0297285, acc 0.98
2016-09-06T14:04:29.816405: step 2569, loss 0.0465136, acc 1
2016-09-06T14:04:30.609747: step 2570, loss 0.0305278, acc 0.98
2016-09-06T14:04:31.434764: step 2571, loss 0.0444949, acc 1
2016-09-06T14:04:32.250534: step 2572, loss 0.0174816, acc 1
2016-09-06T14:04:33.055700: step 2573, loss 0.0311046, acc 1
2016-09-06T14:04:33.869888: step 2574, loss 0.146502, acc 0.94
2016-09-06T14:04:34.669813: step 2575, loss 0.0441021, acc 0.96
2016-09-06T14:04:35.468082: step 2576, loss 0.038703, acc 1
2016-09-06T14:04:36.298920: step 2577, loss 0.0224337, acc 1
2016-09-06T14:04:37.093196: step 2578, loss 0.02206, acc 1
2016-09-06T14:04:37.890415: step 2579, loss 0.0340572, acc 0.98
2016-09-06T14:04:38.713626: step 2580, loss 0.00353093, acc 1
2016-09-06T14:04:39.496710: step 2581, loss 0.0466719, acc 0.98
2016-09-06T14:04:40.301087: step 2582, loss 0.0656592, acc 0.98
2016-09-06T14:04:41.105447: step 2583, loss 0.0166592, acc 1
2016-09-06T14:04:41.909335: step 2584, loss 0.0519887, acc 0.98
2016-09-06T14:04:42.732607: step 2585, loss 0.0315721, acc 1
2016-09-06T14:04:43.541584: step 2586, loss 0.0208758, acc 1
2016-09-06T14:04:44.340888: step 2587, loss 0.0116466, acc 1
2016-09-06T14:04:45.147073: step 2588, loss 0.00465573, acc 1
2016-09-06T14:04:45.954175: step 2589, loss 0.102069, acc 0.94
2016-09-06T14:04:46.764175: step 2590, loss 0.0413439, acc 0.98
2016-09-06T14:04:47.598655: step 2591, loss 0.0598011, acc 0.96
2016-09-06T14:04:48.408903: step 2592, loss 0.0109499, acc 1
2016-09-06T14:04:49.188753: step 2593, loss 0.0416781, acc 0.98
2016-09-06T14:04:49.977621: step 2594, loss 0.0264609, acc 1
2016-09-06T14:04:50.795175: step 2595, loss 0.0560965, acc 0.98
2016-09-06T14:04:51.622943: step 2596, loss 0.133075, acc 0.98
2016-09-06T14:04:52.435555: step 2597, loss 0.0699064, acc 0.96
2016-09-06T14:04:53.235358: step 2598, loss 0.0265196, acc 1
2016-09-06T14:04:54.013116: step 2599, loss 0.0108226, acc 1
2016-09-06T14:04:54.849085: step 2600, loss 0.015621, acc 1

Evaluation:
2016-09-06T14:04:58.594600: step 2600, loss 1.42553, acc 0.762664

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-2600

2016-09-06T14:05:00.564570: step 2601, loss 0.0455232, acc 0.98
2016-09-06T14:05:01.341780: step 2602, loss 0.0451233, acc 0.98
2016-09-06T14:05:02.167001: step 2603, loss 0.222565, acc 0.94
2016-09-06T14:05:02.991447: step 2604, loss 0.0208112, acc 1
2016-09-06T14:05:03.798554: step 2605, loss 0.0335466, acc 0.98
2016-09-06T14:05:04.586983: step 2606, loss 0.0674488, acc 0.96
2016-09-06T14:05:05.413056: step 2607, loss 0.0394858, acc 0.98
2016-09-06T14:05:06.200701: step 2608, loss 0.0486552, acc 0.98
2016-09-06T14:05:07.022697: step 2609, loss 0.0428124, acc 0.96
2016-09-06T14:05:07.852889: step 2610, loss 0.0362388, acc 0.98
2016-09-06T14:05:08.636607: step 2611, loss 0.0583855, acc 0.96
2016-09-06T14:05:09.447431: step 2612, loss 0.0430442, acc 0.98
2016-09-06T14:05:10.296919: step 2613, loss 0.0334984, acc 1
2016-09-06T14:05:11.068674: step 2614, loss 0.00614313, acc 1
2016-09-06T14:05:11.860182: step 2615, loss 0.0524688, acc 0.98
2016-09-06T14:05:12.700411: step 2616, loss 0.107973, acc 0.96
2016-09-06T14:05:13.478002: step 2617, loss 0.0237291, acc 1
2016-09-06T14:05:14.271901: step 2618, loss 0.00925749, acc 1
2016-09-06T14:05:15.073611: step 2619, loss 0.064815, acc 0.98
2016-09-06T14:05:15.861402: step 2620, loss 0.0158412, acc 1
2016-09-06T14:05:16.678483: step 2621, loss 0.171274, acc 0.92
2016-09-06T14:05:17.485641: step 2622, loss 0.0849317, acc 0.96
2016-09-06T14:05:18.271548: step 2623, loss 0.0402251, acc 0.98
2016-09-06T14:05:19.075258: step 2624, loss 0.0146578, acc 1
2016-09-06T14:05:19.896351: step 2625, loss 0.032793, acc 0.98
2016-09-06T14:05:20.669472: step 2626, loss 0.0940678, acc 0.96
2016-09-06T14:05:21.485124: step 2627, loss 0.0508175, acc 1
2016-09-06T14:05:22.298127: step 2628, loss 0.0279825, acc 1
2016-09-06T14:05:23.080408: step 2629, loss 0.0554902, acc 0.96
2016-09-06T14:05:23.927112: step 2630, loss 0.0951093, acc 0.94
2016-09-06T14:05:24.761996: step 2631, loss 0.0728416, acc 0.98
2016-09-06T14:05:25.553181: step 2632, loss 0.0354438, acc 0.98
2016-09-06T14:05:26.365234: step 2633, loss 0.0212215, acc 1
2016-09-06T14:05:27.167919: step 2634, loss 0.097843, acc 0.94
2016-09-06T14:05:27.955255: step 2635, loss 0.0276366, acc 1
2016-09-06T14:05:28.753276: step 2636, loss 0.0438808, acc 0.98
2016-09-06T14:05:29.557429: step 2637, loss 0.291742, acc 0.86
2016-09-06T14:05:30.338492: step 2638, loss 0.015339, acc 1
2016-09-06T14:05:31.160616: step 2639, loss 0.0359095, acc 0.98
2016-09-06T14:05:31.983239: step 2640, loss 0.0411413, acc 0.98
2016-09-06T14:05:32.765394: step 2641, loss 0.0361528, acc 0.98
2016-09-06T14:05:33.575327: step 2642, loss 0.0390734, acc 0.98
2016-09-06T14:05:34.376148: step 2643, loss 0.04297, acc 1
2016-09-06T14:05:35.205433: step 2644, loss 0.018677, acc 1
2016-09-06T14:05:36.006909: step 2645, loss 0.0366266, acc 1
2016-09-06T14:05:36.799812: step 2646, loss 0.064103, acc 0.96
2016-09-06T14:05:37.597392: step 2647, loss 0.0230894, acc 0.98
2016-09-06T14:05:38.400460: step 2648, loss 0.0386261, acc 0.98
2016-09-06T14:05:39.217941: step 2649, loss 0.110587, acc 0.96
2016-09-06T14:05:40.002007: step 2650, loss 0.141797, acc 0.94
2016-09-06T14:05:40.827688: step 2651, loss 0.0521555, acc 0.98
2016-09-06T14:05:41.661931: step 2652, loss 0.0289582, acc 0.98
2016-09-06T14:05:42.457089: step 2653, loss 0.0092361, acc 1
2016-09-06T14:05:43.311648: step 2654, loss 0.0979374, acc 0.96
2016-09-06T14:05:44.121332: step 2655, loss 0.0263286, acc 0.98
2016-09-06T14:05:44.913400: step 2656, loss 0.0884113, acc 0.94
2016-09-06T14:05:45.696346: step 2657, loss 0.0627555, acc 0.96
2016-09-06T14:05:46.523217: step 2658, loss 0.0135053, acc 1
2016-09-06T14:05:47.310100: step 2659, loss 0.0202432, acc 1
2016-09-06T14:05:48.136326: step 2660, loss 0.189928, acc 0.92
2016-09-06T14:05:48.967855: step 2661, loss 0.0221837, acc 1
2016-09-06T14:05:49.747590: step 2662, loss 0.0418467, acc 0.98
2016-09-06T14:05:50.555745: step 2663, loss 0.00955385, acc 1
2016-09-06T14:05:51.374956: step 2664, loss 0.0484121, acc 0.98
2016-09-06T14:05:52.178612: step 2665, loss 0.0569553, acc 0.98
2016-09-06T14:05:52.979760: step 2666, loss 0.0262446, acc 1
2016-09-06T14:05:53.811514: step 2667, loss 0.0480398, acc 0.98
2016-09-06T14:05:54.609354: step 2668, loss 0.0479628, acc 1
2016-09-06T14:05:55.413680: step 2669, loss 0.049369, acc 0.98
2016-09-06T14:05:56.248907: step 2670, loss 0.0375557, acc 0.98
2016-09-06T14:05:57.075926: step 2671, loss 0.0363572, acc 0.98
2016-09-06T14:05:57.919260: step 2672, loss 0.0247622, acc 0.98
2016-09-06T14:05:58.743847: step 2673, loss 0.0376214, acc 0.98
2016-09-06T14:05:59.553601: step 2674, loss 0.135281, acc 0.96
2016-09-06T14:06:00.420304: step 2675, loss 0.0103356, acc 1
2016-09-06T14:06:01.255026: step 2676, loss 0.0478131, acc 0.98
2016-09-06T14:06:02.078856: step 2677, loss 0.0780412, acc 0.96
2016-09-06T14:06:02.911067: step 2678, loss 0.205804, acc 0.96
2016-09-06T14:06:03.735429: step 2679, loss 0.026982, acc 1
2016-09-06T14:06:04.573967: step 2680, loss 0.0119451, acc 1
2016-09-06T14:06:05.395436: step 2681, loss 0.00376416, acc 1
2016-09-06T14:06:06.210693: step 2682, loss 0.0422642, acc 0.96
2016-09-06T14:06:07.031354: step 2683, loss 0.0979479, acc 0.96
2016-09-06T14:06:07.840583: step 2684, loss 0.0233812, acc 0.98
2016-09-06T14:06:08.651064: step 2685, loss 0.0981727, acc 0.92
2016-09-06T14:06:09.502326: step 2686, loss 0.0073782, acc 1
2016-09-06T14:06:10.300738: step 2687, loss 0.026934, acc 0.98
2016-09-06T14:06:11.025159: step 2688, loss 0.021099, acc 1
2016-09-06T14:06:11.873364: step 2689, loss 0.0241667, acc 1
2016-09-06T14:06:12.676415: step 2690, loss 0.0459726, acc 0.98
2016-09-06T14:06:13.498608: step 2691, loss 0.017628, acc 1
2016-09-06T14:06:14.335464: step 2692, loss 0.0323606, acc 0.98
2016-09-06T14:06:15.113996: step 2693, loss 0.0766352, acc 0.94
2016-09-06T14:06:15.913086: step 2694, loss 0.0350808, acc 0.98
2016-09-06T14:06:16.750840: step 2695, loss 0.0783533, acc 0.98
2016-09-06T14:06:17.527421: step 2696, loss 0.0516641, acc 0.96
2016-09-06T14:06:18.311303: step 2697, loss 0.0302602, acc 1
2016-09-06T14:06:19.142365: step 2698, loss 0.0648044, acc 0.96
2016-09-06T14:06:19.922128: step 2699, loss 0.0147609, acc 1
2016-09-06T14:06:20.723483: step 2700, loss 0.0362371, acc 1

Evaluation:
2016-09-06T14:06:24.449375: step 2700, loss 1.70021, acc 0.756098

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-2700

2016-09-06T14:06:26.292188: step 2701, loss 0.0185521, acc 1
2016-09-06T14:06:27.122479: step 2702, loss 0.0258973, acc 1
2016-09-06T14:06:27.940912: step 2703, loss 0.0499761, acc 0.96
2016-09-06T14:06:28.748408: step 2704, loss 0.0170296, acc 1
2016-09-06T14:06:29.551788: step 2705, loss 0.122686, acc 0.96
2016-09-06T14:06:30.362940: step 2706, loss 0.00413742, acc 1
2016-09-06T14:06:31.171502: step 2707, loss 0.120378, acc 0.96
2016-09-06T14:06:31.969458: step 2708, loss 0.110809, acc 0.96
2016-09-06T14:06:32.780202: step 2709, loss 0.00643991, acc 1
2016-09-06T14:06:33.582576: step 2710, loss 0.0453933, acc 0.98
2016-09-06T14:06:34.382916: step 2711, loss 0.00426503, acc 1
2016-09-06T14:06:35.212734: step 2712, loss 0.0256646, acc 1
2016-09-06T14:06:36.030670: step 2713, loss 0.00854993, acc 1
2016-09-06T14:06:36.826839: step 2714, loss 0.0407178, acc 0.98
2016-09-06T14:06:37.625926: step 2715, loss 0.0171761, acc 1
2016-09-06T14:06:38.448260: step 2716, loss 0.0349, acc 1
2016-09-06T14:06:39.245385: step 2717, loss 0.00892571, acc 1
2016-09-06T14:06:40.052655: step 2718, loss 0.0174263, acc 1
2016-09-06T14:06:40.872535: step 2719, loss 0.0266084, acc 1
2016-09-06T14:06:41.650515: step 2720, loss 0.0643722, acc 0.96
2016-09-06T14:06:42.471462: step 2721, loss 0.0152147, acc 1
2016-09-06T14:06:43.270517: step 2722, loss 0.0861456, acc 0.98
2016-09-06T14:06:44.070713: step 2723, loss 0.120651, acc 0.96
2016-09-06T14:06:44.861370: step 2724, loss 0.00632784, acc 1
2016-09-06T14:06:45.655376: step 2725, loss 0.103241, acc 0.98
2016-09-06T14:06:46.473419: step 2726, loss 0.0950549, acc 0.98
2016-09-06T14:06:47.294042: step 2727, loss 0.0313117, acc 0.98
2016-09-06T14:06:48.111416: step 2728, loss 0.0511549, acc 0.98
2016-09-06T14:06:48.904204: step 2729, loss 0.0316264, acc 1
2016-09-06T14:06:49.706150: step 2730, loss 0.0562682, acc 0.96
2016-09-06T14:06:50.508984: step 2731, loss 0.0432182, acc 0.96
2016-09-06T14:06:51.319036: step 2732, loss 0.0357703, acc 1
2016-09-06T14:06:52.145981: step 2733, loss 0.0549288, acc 0.96
2016-09-06T14:06:52.984555: step 2734, loss 0.063614, acc 0.96
2016-09-06T14:06:53.742562: step 2735, loss 0.135534, acc 0.98
2016-09-06T14:06:54.527695: step 2736, loss 0.0461801, acc 0.98
2016-09-06T14:06:55.341971: step 2737, loss 0.0282644, acc 1
2016-09-06T14:06:56.107978: step 2738, loss 0.0568497, acc 0.98
2016-09-06T14:06:56.955748: step 2739, loss 0.0368476, acc 0.98
2016-09-06T14:06:57.772467: step 2740, loss 0.0400757, acc 1
2016-09-06T14:06:58.577860: step 2741, loss 0.0573433, acc 0.96
2016-09-06T14:06:59.370467: step 2742, loss 0.0515547, acc 1
2016-09-06T14:07:00.181932: step 2743, loss 0.0276615, acc 0.98
2016-09-06T14:07:00.982179: step 2744, loss 0.0390666, acc 1
2016-09-06T14:07:01.786203: step 2745, loss 0.0561715, acc 0.96
2016-09-06T14:07:02.585308: step 2746, loss 0.0636588, acc 0.96
2016-09-06T14:07:03.409973: step 2747, loss 0.0348642, acc 0.98
2016-09-06T14:07:04.212662: step 2748, loss 0.0158557, acc 1
2016-09-06T14:07:05.036117: step 2749, loss 0.128616, acc 0.96
2016-09-06T14:07:05.838920: step 2750, loss 0.0178094, acc 1
2016-09-06T14:07:06.658770: step 2751, loss 0.0299359, acc 1
2016-09-06T14:07:07.480157: step 2752, loss 0.1145, acc 0.94
2016-09-06T14:07:08.286232: step 2753, loss 0.0872908, acc 0.96
2016-09-06T14:07:09.083707: step 2754, loss 0.0400958, acc 0.98
2016-09-06T14:07:09.916346: step 2755, loss 0.0278054, acc 1
2016-09-06T14:07:10.691292: step 2756, loss 0.0709853, acc 0.96
2016-09-06T14:07:11.478578: step 2757, loss 0.0658903, acc 0.94
2016-09-06T14:07:12.321478: step 2758, loss 0.0109964, acc 1
2016-09-06T14:07:13.129131: step 2759, loss 0.0306889, acc 1
2016-09-06T14:07:13.935337: step 2760, loss 0.0510517, acc 0.98
2016-09-06T14:07:14.767143: step 2761, loss 0.0774258, acc 0.96
2016-09-06T14:07:15.620561: step 2762, loss 0.015385, acc 1
2016-09-06T14:07:16.427361: step 2763, loss 0.0912357, acc 0.96
2016-09-06T14:07:17.264897: step 2764, loss 0.104947, acc 0.94
2016-09-06T14:07:18.063400: step 2765, loss 0.0246023, acc 0.98
2016-09-06T14:07:18.830423: step 2766, loss 0.028132, acc 1
2016-09-06T14:07:19.651447: step 2767, loss 0.0851082, acc 0.98
2016-09-06T14:07:20.473200: step 2768, loss 0.034325, acc 0.98
2016-09-06T14:07:21.273335: step 2769, loss 0.0142807, acc 1
2016-09-06T14:07:22.097420: step 2770, loss 0.0190814, acc 1
2016-09-06T14:07:22.911812: step 2771, loss 0.0595998, acc 0.96
2016-09-06T14:07:23.698388: step 2772, loss 0.0305783, acc 0.98
2016-09-06T14:07:24.554018: step 2773, loss 0.0630872, acc 0.96
2016-09-06T14:07:25.401205: step 2774, loss 0.0425524, acc 0.98
2016-09-06T14:07:26.216394: step 2775, loss 0.0205566, acc 1
2016-09-06T14:07:27.031537: step 2776, loss 0.0442879, acc 0.98
2016-09-06T14:07:27.826075: step 2777, loss 0.0625772, acc 0.96
2016-09-06T14:07:28.641431: step 2778, loss 0.0376042, acc 0.98
2016-09-06T14:07:29.453448: step 2779, loss 0.0269913, acc 0.98
2016-09-06T14:07:30.268859: step 2780, loss 0.0233414, acc 1
2016-09-06T14:07:31.087986: step 2781, loss 0.0253742, acc 1
2016-09-06T14:07:31.948600: step 2782, loss 0.0980939, acc 0.94
2016-09-06T14:07:32.797914: step 2783, loss 0.0673084, acc 0.98
2016-09-06T14:07:33.612821: step 2784, loss 0.0212195, acc 0.98
2016-09-06T14:07:34.411356: step 2785, loss 0.0196556, acc 0.98
2016-09-06T14:07:35.226296: step 2786, loss 0.0951123, acc 0.92
2016-09-06T14:07:36.007698: step 2787, loss 0.0163808, acc 1
2016-09-06T14:07:36.833469: step 2788, loss 0.0459349, acc 0.98
2016-09-06T14:07:37.654335: step 2789, loss 0.0450491, acc 0.98
2016-09-06T14:07:38.454208: step 2790, loss 0.0599239, acc 0.96
2016-09-06T14:07:39.243993: step 2791, loss 0.0223153, acc 0.98
2016-09-06T14:07:40.045241: step 2792, loss 0.0293546, acc 0.98
2016-09-06T14:07:40.882966: step 2793, loss 0.0226383, acc 1
2016-09-06T14:07:41.701614: step 2794, loss 0.0146729, acc 1
2016-09-06T14:07:42.494804: step 2795, loss 0.139863, acc 0.94
2016-09-06T14:07:43.291515: step 2796, loss 0.158597, acc 0.92
2016-09-06T14:07:44.095343: step 2797, loss 0.0325645, acc 0.98
2016-09-06T14:07:44.919201: step 2798, loss 0.0870856, acc 0.96
2016-09-06T14:07:45.708379: step 2799, loss 0.00612256, acc 1
2016-09-06T14:07:46.518087: step 2800, loss 0.0936059, acc 0.94

Evaluation:
2016-09-06T14:07:50.237856: step 2800, loss 1.57259, acc 0.751407

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-2800

2016-09-06T14:07:52.064746: step 2801, loss 0.0279961, acc 0.98
2016-09-06T14:07:52.911743: step 2802, loss 0.034785, acc 0.98
2016-09-06T14:07:53.728156: step 2803, loss 0.0480428, acc 0.98
2016-09-06T14:07:54.551071: step 2804, loss 0.0524565, acc 0.98
2016-09-06T14:07:55.375849: step 2805, loss 0.00825122, acc 1
2016-09-06T14:07:56.209974: step 2806, loss 0.00750737, acc 1
2016-09-06T14:07:57.041394: step 2807, loss 0.0347343, acc 0.98
2016-09-06T14:07:57.846124: step 2808, loss 0.127115, acc 0.94
2016-09-06T14:07:58.671397: step 2809, loss 0.00390512, acc 1
2016-09-06T14:07:59.504803: step 2810, loss 0.0293116, acc 0.98
2016-09-06T14:08:00.317145: step 2811, loss 0.0207287, acc 1
2016-09-06T14:08:01.128910: step 2812, loss 0.0376143, acc 0.98
2016-09-06T14:08:01.955460: step 2813, loss 0.0967991, acc 0.94
2016-09-06T14:08:02.748604: step 2814, loss 0.0443966, acc 0.96
2016-09-06T14:08:03.551683: step 2815, loss 0.0267579, acc 0.98
2016-09-06T14:08:04.375573: step 2816, loss 0.0179464, acc 1
2016-09-06T14:08:05.183556: step 2817, loss 0.062085, acc 0.96
2016-09-06T14:08:05.990854: step 2818, loss 0.0417787, acc 0.98
2016-09-06T14:08:06.805079: step 2819, loss 0.0373359, acc 1
2016-09-06T14:08:07.594327: step 2820, loss 0.191794, acc 0.96
2016-09-06T14:08:08.404417: step 2821, loss 0.136027, acc 0.96
2016-09-06T14:08:09.265508: step 2822, loss 0.0511283, acc 0.98
2016-09-06T14:08:10.079862: step 2823, loss 0.0866071, acc 0.94
2016-09-06T14:08:10.921775: step 2824, loss 0.0296228, acc 1
2016-09-06T14:08:11.734135: step 2825, loss 0.0292486, acc 0.98
2016-09-06T14:08:12.530681: step 2826, loss 0.0835052, acc 0.96
2016-09-06T14:08:13.343955: step 2827, loss 0.0652745, acc 0.96
2016-09-06T14:08:14.162356: step 2828, loss 0.0237491, acc 1
2016-09-06T14:08:14.991241: step 2829, loss 0.14346, acc 0.94
2016-09-06T14:08:15.789194: step 2830, loss 0.0147966, acc 1
2016-09-06T14:08:16.622189: step 2831, loss 0.0759773, acc 0.98
2016-09-06T14:08:17.431834: step 2832, loss 0.0461894, acc 0.98
2016-09-06T14:08:18.230415: step 2833, loss 0.0840221, acc 0.96
2016-09-06T14:08:19.047096: step 2834, loss 0.0497874, acc 0.98
2016-09-06T14:08:19.861547: step 2835, loss 0.25553, acc 0.92
2016-09-06T14:08:20.705341: step 2836, loss 0.0500622, acc 1
2016-09-06T14:08:21.554961: step 2837, loss 0.0392476, acc 0.98
2016-09-06T14:08:22.351511: step 2838, loss 0.0922993, acc 0.94
2016-09-06T14:08:23.152601: step 2839, loss 0.018542, acc 1
2016-09-06T14:08:23.987331: step 2840, loss 0.0141152, acc 1
2016-09-06T14:08:24.810147: step 2841, loss 0.0310054, acc 0.98
2016-09-06T14:08:25.666449: step 2842, loss 0.0328677, acc 0.98
2016-09-06T14:08:26.491895: step 2843, loss 0.044424, acc 1
2016-09-06T14:08:27.299236: step 2844, loss 0.0387976, acc 0.98
2016-09-06T14:08:28.092498: step 2845, loss 0.113444, acc 0.96
2016-09-06T14:08:28.912120: step 2846, loss 0.0199801, acc 1
2016-09-06T14:08:29.758673: step 2847, loss 0.0972172, acc 0.96
2016-09-06T14:08:30.589903: step 2848, loss 0.0284099, acc 1
2016-09-06T14:08:31.381403: step 2849, loss 0.0764103, acc 0.98
2016-09-06T14:08:32.206138: step 2850, loss 0.0798952, acc 0.94
2016-09-06T14:08:32.980972: step 2851, loss 0.0756737, acc 0.96
2016-09-06T14:08:33.792076: step 2852, loss 0.219131, acc 0.94
2016-09-06T14:08:34.652398: step 2853, loss 0.16696, acc 0.96
2016-09-06T14:08:35.466578: step 2854, loss 0.0418809, acc 1
2016-09-06T14:08:36.284635: step 2855, loss 0.0239963, acc 1
2016-09-06T14:08:37.110050: step 2856, loss 0.0226518, acc 0.98
2016-09-06T14:08:37.949650: step 2857, loss 0.146973, acc 0.96
2016-09-06T14:08:38.747187: step 2858, loss 0.0887771, acc 0.98
2016-09-06T14:08:39.574782: step 2859, loss 0.0347204, acc 1
2016-09-06T14:08:40.401973: step 2860, loss 0.0343974, acc 1
2016-09-06T14:08:41.207991: step 2861, loss 0.0333, acc 1
2016-09-06T14:08:42.012736: step 2862, loss 0.127216, acc 0.92
2016-09-06T14:08:42.816440: step 2863, loss 0.0947851, acc 0.94
2016-09-06T14:08:43.653017: step 2864, loss 0.0312518, acc 0.98
2016-09-06T14:08:44.512095: step 2865, loss 0.0457571, acc 0.98
2016-09-06T14:08:45.315430: step 2866, loss 0.0499192, acc 1
2016-09-06T14:08:46.141304: step 2867, loss 0.0391405, acc 1
2016-09-06T14:08:47.048426: step 2868, loss 0.0295219, acc 1
2016-09-06T14:08:47.895624: step 2869, loss 0.0220887, acc 1
2016-09-06T14:08:48.697474: step 2870, loss 0.0453055, acc 0.98
2016-09-06T14:08:49.519043: step 2871, loss 0.0115141, acc 1
2016-09-06T14:08:50.365621: step 2872, loss 0.0206553, acc 1
2016-09-06T14:08:51.170855: step 2873, loss 0.0359328, acc 0.98
2016-09-06T14:08:51.981180: step 2874, loss 0.0901474, acc 0.94
2016-09-06T14:08:52.797792: step 2875, loss 0.0660521, acc 0.98
2016-09-06T14:08:53.615977: step 2876, loss 0.0139507, acc 1
2016-09-06T14:08:54.455798: step 2877, loss 0.0354943, acc 0.98
2016-09-06T14:08:55.312551: step 2878, loss 0.0406391, acc 0.98
2016-09-06T14:08:56.121377: step 2879, loss 0.103196, acc 0.96
2016-09-06T14:08:56.878850: step 2880, loss 0.0748873, acc 0.977273
2016-09-06T14:08:57.720295: step 2881, loss 0.0687809, acc 0.98
2016-09-06T14:08:58.524443: step 2882, loss 0.0565715, acc 0.98
2016-09-06T14:08:59.319663: step 2883, loss 0.0339238, acc 1
2016-09-06T14:09:00.156929: step 2884, loss 0.029844, acc 0.98
2016-09-06T14:09:00.972531: step 2885, loss 0.0814678, acc 0.98
2016-09-06T14:09:01.795591: step 2886, loss 0.0978375, acc 0.96
2016-09-06T14:09:02.647603: step 2887, loss 0.0159338, acc 1
2016-09-06T14:09:03.456178: step 2888, loss 0.026219, acc 1
2016-09-06T14:09:04.263703: step 2889, loss 0.0467334, acc 1
2016-09-06T14:09:05.063793: step 2890, loss 0.108482, acc 0.96
2016-09-06T14:09:05.849796: step 2891, loss 0.00703727, acc 1
2016-09-06T14:09:06.654511: step 2892, loss 0.0109711, acc 1
2016-09-06T14:09:07.488077: step 2893, loss 0.031363, acc 1
2016-09-06T14:09:08.265390: step 2894, loss 0.0157931, acc 1
2016-09-06T14:09:09.100080: step 2895, loss 0.0417772, acc 0.96
2016-09-06T14:09:09.920163: step 2896, loss 0.0336165, acc 0.98
2016-09-06T14:09:10.733410: step 2897, loss 0.0409073, acc 1
2016-09-06T14:09:11.549553: step 2898, loss 0.0208328, acc 1
2016-09-06T14:09:12.347445: step 2899, loss 0.0197794, acc 1
2016-09-06T14:09:13.152393: step 2900, loss 0.0146259, acc 1

Evaluation:
2016-09-06T14:09:16.895479: step 2900, loss 1.77579, acc 0.74015

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-2900

2016-09-06T14:09:18.835829: step 2901, loss 0.0457392, acc 0.96
2016-09-06T14:09:19.661062: step 2902, loss 0.0415452, acc 0.98
2016-09-06T14:09:20.470669: step 2903, loss 0.0331068, acc 0.98
2016-09-06T14:09:21.311636: step 2904, loss 0.00902297, acc 1
2016-09-06T14:09:22.132891: step 2905, loss 0.0883463, acc 0.94
2016-09-06T14:09:22.928688: step 2906, loss 0.0241947, acc 1
2016-09-06T14:09:23.764351: step 2907, loss 0.107735, acc 0.98
2016-09-06T14:09:24.553782: step 2908, loss 0.027233, acc 0.98
2016-09-06T14:09:25.380600: step 2909, loss 0.0122382, acc 1
2016-09-06T14:09:26.164161: step 2910, loss 0.0191092, acc 1
2016-09-06T14:09:26.991239: step 2911, loss 0.00751411, acc 1
2016-09-06T14:09:27.808178: step 2912, loss 0.0364303, acc 0.98
2016-09-06T14:09:28.609446: step 2913, loss 0.029786, acc 0.98
2016-09-06T14:09:29.428063: step 2914, loss 0.10524, acc 0.94
2016-09-06T14:09:30.234776: step 2915, loss 0.0119968, acc 1
2016-09-06T14:09:31.027993: step 2916, loss 0.0205933, acc 0.98
2016-09-06T14:09:31.853643: step 2917, loss 0.150079, acc 0.94
2016-09-06T14:09:32.634812: step 2918, loss 0.0203996, acc 0.98
2016-09-06T14:09:33.421300: step 2919, loss 0.0688696, acc 0.96
2016-09-06T14:09:34.217410: step 2920, loss 0.0679315, acc 0.96
2016-09-06T14:09:35.026247: step 2921, loss 0.0330432, acc 0.98
2016-09-06T14:09:35.809076: step 2922, loss 0.0358611, acc 0.98
2016-09-06T14:09:36.633115: step 2923, loss 0.0876219, acc 0.96
2016-09-06T14:09:37.500673: step 2924, loss 0.056048, acc 0.96
2016-09-06T14:09:38.280096: step 2925, loss 0.0172748, acc 1
2016-09-06T14:09:39.103488: step 2926, loss 0.0803212, acc 0.98
2016-09-06T14:09:39.948342: step 2927, loss 0.0342956, acc 0.98
2016-09-06T14:09:40.773105: step 2928, loss 0.00632692, acc 1
2016-09-06T14:09:41.598414: step 2929, loss 0.00645553, acc 1
2016-09-06T14:09:42.420634: step 2930, loss 0.0963167, acc 0.96
2016-09-06T14:09:43.251080: step 2931, loss 0.0818625, acc 0.94
2016-09-06T14:09:44.073148: step 2932, loss 0.0256568, acc 1
2016-09-06T14:09:44.900399: step 2933, loss 0.0299918, acc 0.98
2016-09-06T14:09:45.702139: step 2934, loss 0.0883967, acc 0.94
2016-09-06T14:09:46.496165: step 2935, loss 0.00570669, acc 1
2016-09-06T14:09:47.321159: step 2936, loss 0.035425, acc 0.98
2016-09-06T14:09:48.157135: step 2937, loss 0.0253616, acc 0.98
2016-09-06T14:09:48.959659: step 2938, loss 0.0625746, acc 0.98
2016-09-06T14:09:49.785436: step 2939, loss 0.0501428, acc 0.96
2016-09-06T14:09:50.612538: step 2940, loss 0.0922311, acc 0.96
2016-09-06T14:09:51.433926: step 2941, loss 0.0157491, acc 1
2016-09-06T14:09:52.266489: step 2942, loss 0.0296015, acc 1
2016-09-06T14:09:53.078438: step 2943, loss 0.0203954, acc 1
2016-09-06T14:09:53.876329: step 2944, loss 0.0558005, acc 0.98
2016-09-06T14:09:54.735933: step 2945, loss 0.0204606, acc 1
2016-09-06T14:09:55.548387: step 2946, loss 0.0432001, acc 1
2016-09-06T14:09:56.345837: step 2947, loss 0.0138378, acc 1
2016-09-06T14:09:57.170145: step 2948, loss 0.0169958, acc 1
2016-09-06T14:09:58.006770: step 2949, loss 0.0383349, acc 0.98
2016-09-06T14:09:58.805374: step 2950, loss 0.0257746, acc 0.98
2016-09-06T14:09:59.612170: step 2951, loss 0.0338746, acc 0.98
2016-09-06T14:10:00.468099: step 2952, loss 0.059605, acc 0.96
2016-09-06T14:10:01.255035: step 2953, loss 0.0970948, acc 0.92
2016-09-06T14:10:02.050825: step 2954, loss 0.059275, acc 0.94
2016-09-06T14:10:02.861768: step 2955, loss 0.0580644, acc 0.96
2016-09-06T14:10:03.663464: step 2956, loss 0.045355, acc 0.98
2016-09-06T14:10:04.461065: step 2957, loss 0.0155999, acc 1
2016-09-06T14:10:05.282162: step 2958, loss 0.0757617, acc 0.96
2016-09-06T14:10:06.082602: step 2959, loss 0.0228367, acc 1
2016-09-06T14:10:06.883905: step 2960, loss 0.0130388, acc 1
2016-09-06T14:10:07.692425: step 2961, loss 0.0346352, acc 0.98
2016-09-06T14:10:08.503630: step 2962, loss 0.101068, acc 0.92
2016-09-06T14:10:09.323532: step 2963, loss 0.0250528, acc 0.98
2016-09-06T14:10:10.155662: step 2964, loss 0.0170416, acc 1
2016-09-06T14:10:10.928823: step 2965, loss 0.0196763, acc 1
2016-09-06T14:10:11.754932: step 2966, loss 0.0394124, acc 0.98
2016-09-06T14:10:12.593412: step 2967, loss 0.0237396, acc 0.98
2016-09-06T14:10:13.387973: step 2968, loss 0.0440188, acc 0.98
2016-09-06T14:10:14.185097: step 2969, loss 0.17179, acc 0.98
2016-09-06T14:10:15.002987: step 2970, loss 0.0368569, acc 0.96
2016-09-06T14:10:15.793672: step 2971, loss 0.0589109, acc 0.96
2016-09-06T14:10:16.595751: step 2972, loss 0.016696, acc 1
2016-09-06T14:10:17.387802: step 2973, loss 0.0195321, acc 1
2016-09-06T14:10:18.164538: step 2974, loss 0.00609178, acc 1
2016-09-06T14:10:18.948926: step 2975, loss 0.0295607, acc 0.98
2016-09-06T14:10:19.770699: step 2976, loss 0.0873599, acc 0.96
2016-09-06T14:10:20.551720: step 2977, loss 0.0276519, acc 1
2016-09-06T14:10:21.349932: step 2978, loss 0.0298776, acc 1
2016-09-06T14:10:22.161441: step 2979, loss 0.013676, acc 1
2016-09-06T14:10:22.955619: step 2980, loss 0.074144, acc 0.96
2016-09-06T14:10:23.766193: step 2981, loss 0.0191648, acc 1
2016-09-06T14:10:24.571222: step 2982, loss 0.0207315, acc 1
2016-09-06T14:10:25.374112: step 2983, loss 0.0588639, acc 0.96
2016-09-06T14:10:26.205974: step 2984, loss 0.163966, acc 0.92
2016-09-06T14:10:27.032382: step 2985, loss 0.0387724, acc 0.98
2016-09-06T14:10:27.821263: step 2986, loss 0.0607218, acc 0.96
2016-09-06T14:10:28.609343: step 2987, loss 0.0849388, acc 0.96
2016-09-06T14:10:29.431446: step 2988, loss 0.0259584, acc 1
2016-09-06T14:10:30.234045: step 2989, loss 0.0897635, acc 0.96
2016-09-06T14:10:31.048856: step 2990, loss 0.101077, acc 0.98
2016-09-06T14:10:31.874610: step 2991, loss 0.0435391, acc 0.98
2016-09-06T14:10:32.689391: step 2992, loss 0.0345109, acc 1
2016-09-06T14:10:33.505425: step 2993, loss 0.0666576, acc 0.92
2016-09-06T14:10:34.305494: step 2994, loss 0.0454812, acc 0.96
2016-09-06T14:10:35.099846: step 2995, loss 0.0176185, acc 1
2016-09-06T14:10:35.924800: step 2996, loss 0.0701566, acc 0.96
2016-09-06T14:10:36.724780: step 2997, loss 0.0266006, acc 0.98
2016-09-06T14:10:37.510244: step 2998, loss 0.0409058, acc 0.98
2016-09-06T14:10:38.322539: step 2999, loss 0.0264155, acc 1
2016-09-06T14:10:39.113650: step 3000, loss 0.0427738, acc 0.98

Evaluation:
2016-09-06T14:10:42.870611: step 3000, loss 1.36837, acc 0.751407

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-3000

2016-09-06T14:10:44.715473: step 3001, loss 0.0350984, acc 0.98
2016-09-06T14:10:45.536626: step 3002, loss 0.0219042, acc 1
2016-09-06T14:10:46.378678: step 3003, loss 0.0135232, acc 1
2016-09-06T14:10:47.153401: step 3004, loss 0.0091648, acc 1
2016-09-06T14:10:47.969376: step 3005, loss 0.0329122, acc 0.98
2016-09-06T14:10:48.800019: step 3006, loss 0.0304177, acc 0.98
2016-09-06T14:10:49.592592: step 3007, loss 0.0415374, acc 0.96
2016-09-06T14:10:50.375317: step 3008, loss 0.0229773, acc 1
2016-09-06T14:10:51.189893: step 3009, loss 0.0358782, acc 0.98
2016-09-06T14:10:51.983257: step 3010, loss 0.046287, acc 0.98
2016-09-06T14:10:52.782352: step 3011, loss 0.0752475, acc 0.94
2016-09-06T14:10:53.609415: step 3012, loss 0.0305526, acc 0.98
2016-09-06T14:10:54.387222: step 3013, loss 0.0491927, acc 0.98
2016-09-06T14:10:55.202651: step 3014, loss 0.0525975, acc 0.98
2016-09-06T14:10:56.022794: step 3015, loss 0.0332081, acc 0.98
2016-09-06T14:10:56.817563: step 3016, loss 0.0201387, acc 1
2016-09-06T14:10:57.637662: step 3017, loss 0.0357848, acc 0.98
2016-09-06T14:10:58.463510: step 3018, loss 0.0658666, acc 0.96
2016-09-06T14:10:59.268956: step 3019, loss 0.0143996, acc 1
2016-09-06T14:11:00.108033: step 3020, loss 0.0304256, acc 1
2016-09-06T14:11:00.967322: step 3021, loss 0.0215917, acc 1
2016-09-06T14:11:01.795000: step 3022, loss 0.0350392, acc 0.98
2016-09-06T14:11:02.590737: step 3023, loss 0.0334145, acc 1
2016-09-06T14:11:03.416844: step 3024, loss 0.0240223, acc 0.98
2016-09-06T14:11:04.216939: step 3025, loss 0.0293081, acc 1
2016-09-06T14:11:05.054598: step 3026, loss 0.0610502, acc 0.98
2016-09-06T14:11:05.874536: step 3027, loss 0.0756896, acc 0.96
2016-09-06T14:11:06.685759: step 3028, loss 0.057914, acc 0.98
2016-09-06T14:11:07.492046: step 3029, loss 0.0296802, acc 1
2016-09-06T14:11:08.323158: step 3030, loss 0.0721662, acc 0.98
2016-09-06T14:11:09.148338: step 3031, loss 0.0265422, acc 0.98
2016-09-06T14:11:09.972498: step 3032, loss 0.0162986, acc 1
2016-09-06T14:11:10.801918: step 3033, loss 0.0220027, acc 1
2016-09-06T14:11:11.597648: step 3034, loss 0.0140016, acc 1
2016-09-06T14:11:12.413157: step 3035, loss 0.06019, acc 0.98
2016-09-06T14:11:13.211775: step 3036, loss 0.0699798, acc 0.98
2016-09-06T14:11:13.999215: step 3037, loss 0.0188736, acc 1
2016-09-06T14:11:14.841413: step 3038, loss 0.0366365, acc 1
2016-09-06T14:11:15.675338: step 3039, loss 0.00503086, acc 1
2016-09-06T14:11:16.478361: step 3040, loss 0.0568716, acc 0.96
2016-09-06T14:11:17.301166: step 3041, loss 0.118908, acc 0.96
2016-09-06T14:11:18.150853: step 3042, loss 0.00901575, acc 1
2016-09-06T14:11:18.963700: step 3043, loss 0.035597, acc 1
2016-09-06T14:11:19.767963: step 3044, loss 0.0227366, acc 1
2016-09-06T14:11:20.597811: step 3045, loss 0.00962995, acc 1
2016-09-06T14:11:21.429943: step 3046, loss 0.0323, acc 0.98
2016-09-06T14:11:22.215179: step 3047, loss 0.0309524, acc 1
2016-09-06T14:11:23.023880: step 3048, loss 0.047953, acc 0.98
2016-09-06T14:11:23.852248: step 3049, loss 0.0839128, acc 0.94
2016-09-06T14:11:24.653346: step 3050, loss 0.0210188, acc 1
2016-09-06T14:11:25.461651: step 3051, loss 0.0602502, acc 0.98
2016-09-06T14:11:26.300004: step 3052, loss 0.0840356, acc 0.98
2016-09-06T14:11:27.089571: step 3053, loss 0.0446454, acc 0.98
2016-09-06T14:11:27.892854: step 3054, loss 0.0192804, acc 0.98
2016-09-06T14:11:28.751197: step 3055, loss 0.0799447, acc 0.96
2016-09-06T14:11:29.563384: step 3056, loss 0.0476952, acc 0.96
2016-09-06T14:11:30.382238: step 3057, loss 0.0593347, acc 0.98
2016-09-06T14:11:31.230626: step 3058, loss 0.0529967, acc 1
2016-09-06T14:11:32.038524: step 3059, loss 0.0693924, acc 0.98
2016-09-06T14:11:32.828762: step 3060, loss 0.0572083, acc 0.98
2016-09-06T14:11:33.651297: step 3061, loss 0.0380409, acc 0.96
2016-09-06T14:11:34.441688: step 3062, loss 0.0140849, acc 1
2016-09-06T14:11:35.270707: step 3063, loss 0.089491, acc 0.94
2016-09-06T14:11:36.112889: step 3064, loss 0.0204814, acc 1
2016-09-06T14:11:37.011704: step 3065, loss 0.0223018, acc 1
2016-09-06T14:11:37.858307: step 3066, loss 0.0536648, acc 1
2016-09-06T14:11:38.714078: step 3067, loss 0.0442889, acc 1
2016-09-06T14:11:39.545804: step 3068, loss 0.0119322, acc 1
2016-09-06T14:11:40.329552: step 3069, loss 0.072572, acc 0.96
2016-09-06T14:11:41.155008: step 3070, loss 0.0285447, acc 0.98
2016-09-06T14:11:41.981077: step 3071, loss 0.0600209, acc 0.96
2016-09-06T14:11:42.727028: step 3072, loss 0.0266057, acc 0.977273
2016-09-06T14:11:43.557130: step 3073, loss 0.0741025, acc 0.94
2016-09-06T14:11:44.385401: step 3074, loss 0.0316257, acc 0.98
2016-09-06T14:11:45.184325: step 3075, loss 0.0340029, acc 1
2016-09-06T14:11:46.002489: step 3076, loss 0.0327347, acc 0.98
2016-09-06T14:11:46.819038: step 3077, loss 0.0471968, acc 0.96
2016-09-06T14:11:47.608694: step 3078, loss 0.0241171, acc 0.98
2016-09-06T14:11:48.405785: step 3079, loss 0.0464443, acc 0.98
2016-09-06T14:11:49.229629: step 3080, loss 0.0479566, acc 0.96
2016-09-06T14:11:50.028364: step 3081, loss 0.0878162, acc 0.94
2016-09-06T14:11:50.837737: step 3082, loss 0.0412014, acc 0.98
2016-09-06T14:11:51.657745: step 3083, loss 0.0397592, acc 0.98
2016-09-06T14:11:52.463790: step 3084, loss 0.0335905, acc 0.98
2016-09-06T14:11:53.300125: step 3085, loss 0.0353332, acc 1
2016-09-06T14:11:54.107180: step 3086, loss 0.0316273, acc 1
2016-09-06T14:11:54.906958: step 3087, loss 0.0639461, acc 0.96
2016-09-06T14:11:55.686219: step 3088, loss 0.0665198, acc 0.98
2016-09-06T14:11:56.509573: step 3089, loss 0.0687683, acc 0.96
2016-09-06T14:11:57.279424: step 3090, loss 0.00700647, acc 1
2016-09-06T14:11:58.079409: step 3091, loss 0.0347844, acc 1
2016-09-06T14:11:58.906512: step 3092, loss 0.0044025, acc 1
2016-09-06T14:11:59.687212: step 3093, loss 0.00385692, acc 1
2016-09-06T14:12:00.549265: step 3094, loss 0.00464519, acc 1
2016-09-06T14:12:01.354148: step 3095, loss 0.0125816, acc 1
2016-09-06T14:12:02.122454: step 3096, loss 0.0436819, acc 0.98
2016-09-06T14:12:02.912682: step 3097, loss 0.0311815, acc 0.98
2016-09-06T14:12:03.741111: step 3098, loss 0.0160568, acc 1
2016-09-06T14:12:04.530845: step 3099, loss 0.0243177, acc 0.98
2016-09-06T14:12:05.347769: step 3100, loss 0.0088273, acc 1

Evaluation:
2016-09-06T14:12:09.077401: step 3100, loss 1.60001, acc 0.749531

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-3100

2016-09-06T14:12:10.968321: step 3101, loss 0.0158731, acc 1
2016-09-06T14:12:11.791355: step 3102, loss 0.0369383, acc 0.96
2016-09-06T14:12:12.641363: step 3103, loss 0.0580974, acc 0.98
2016-09-06T14:12:13.428062: step 3104, loss 0.0166804, acc 1
2016-09-06T14:12:14.240634: step 3105, loss 0.0362644, acc 0.98
2016-09-06T14:12:15.046451: step 3106, loss 0.0199865, acc 1
2016-09-06T14:12:15.844636: step 3107, loss 0.0101973, acc 1
2016-09-06T14:12:16.669670: step 3108, loss 0.00562464, acc 1
2016-09-06T14:12:17.490574: step 3109, loss 0.0233257, acc 0.98
2016-09-06T14:12:18.309953: step 3110, loss 0.0163764, acc 1
2016-09-06T14:12:19.111823: step 3111, loss 0.0479012, acc 0.96
2016-09-06T14:12:19.924065: step 3112, loss 0.0429199, acc 0.98
2016-09-06T14:12:20.731944: step 3113, loss 0.0263704, acc 0.98
2016-09-06T14:12:21.528163: step 3114, loss 0.00663052, acc 1
2016-09-06T14:12:22.344431: step 3115, loss 0.0521375, acc 0.98
2016-09-06T14:12:23.162358: step 3116, loss 0.0190683, acc 1
2016-09-06T14:12:23.985070: step 3117, loss 0.049656, acc 0.98
2016-09-06T14:12:24.792957: step 3118, loss 0.0760971, acc 0.96
2016-09-06T14:12:25.655062: step 3119, loss 0.154614, acc 0.94
2016-09-06T14:12:26.427139: step 3120, loss 0.0422334, acc 0.98
2016-09-06T14:12:27.238822: step 3121, loss 0.0600226, acc 0.98
2016-09-06T14:12:28.071466: step 3122, loss 0.0708851, acc 0.96
2016-09-06T14:12:28.883031: step 3123, loss 0.0766497, acc 0.98
2016-09-06T14:12:29.689836: step 3124, loss 0.0613386, acc 0.96
2016-09-06T14:12:30.504121: step 3125, loss 0.041578, acc 0.98
2016-09-06T14:12:31.301105: step 3126, loss 0.0132966, acc 1
2016-09-06T14:12:32.110117: step 3127, loss 0.0154788, acc 1
2016-09-06T14:12:32.941222: step 3128, loss 0.0188255, acc 1
2016-09-06T14:12:33.741645: step 3129, loss 0.095783, acc 0.94
2016-09-06T14:12:34.563836: step 3130, loss 0.0932765, acc 0.94
2016-09-06T14:12:35.378706: step 3131, loss 0.060956, acc 0.96
2016-09-06T14:12:36.192445: step 3132, loss 0.0179539, acc 1
2016-09-06T14:12:36.997359: step 3133, loss 0.0905234, acc 0.98
2016-09-06T14:12:37.843966: step 3134, loss 0.0256287, acc 1
2016-09-06T14:12:38.665848: step 3135, loss 0.0434465, acc 0.98
2016-09-06T14:12:39.478186: step 3136, loss 0.100815, acc 0.96
2016-09-06T14:12:40.312710: step 3137, loss 0.0341869, acc 1
2016-09-06T14:12:41.137334: step 3138, loss 0.051226, acc 0.98
2016-09-06T14:12:41.974969: step 3139, loss 0.0242743, acc 1
2016-09-06T14:12:42.799745: step 3140, loss 0.01663, acc 1
2016-09-06T14:12:43.634716: step 3141, loss 0.0433505, acc 0.98
2016-09-06T14:12:44.419753: step 3142, loss 0.0158255, acc 1
2016-09-06T14:12:45.216369: step 3143, loss 0.13947, acc 0.92
2016-09-06T14:12:46.055702: step 3144, loss 0.038586, acc 1
2016-09-06T14:12:46.868434: step 3145, loss 0.045095, acc 0.96
2016-09-06T14:12:47.682861: step 3146, loss 0.0454589, acc 0.96
2016-09-06T14:12:48.517689: step 3147, loss 0.0268611, acc 1
2016-09-06T14:12:49.298589: step 3148, loss 0.0338476, acc 0.98
2016-09-06T14:12:50.106527: step 3149, loss 0.0418763, acc 0.98
2016-09-06T14:12:50.905844: step 3150, loss 0.0442704, acc 0.96
2016-09-06T14:12:51.678434: step 3151, loss 0.0161413, acc 1
2016-09-06T14:12:52.503573: step 3152, loss 0.0534862, acc 0.96
2016-09-06T14:12:53.340991: step 3153, loss 0.00467175, acc 1
2016-09-06T14:12:54.113099: step 3154, loss 0.0174563, acc 0.98
2016-09-06T14:12:54.923627: step 3155, loss 0.0686689, acc 0.98
2016-09-06T14:12:55.747961: step 3156, loss 0.0608012, acc 0.98
2016-09-06T14:12:56.529887: step 3157, loss 0.0577818, acc 0.98
2016-09-06T14:12:57.322394: step 3158, loss 0.0381955, acc 0.98
2016-09-06T14:12:58.154371: step 3159, loss 0.0845341, acc 0.98
2016-09-06T14:12:58.968280: step 3160, loss 0.0167896, acc 1
2016-09-06T14:12:59.789583: step 3161, loss 0.024676, acc 0.98
2016-09-06T14:13:00.618610: step 3162, loss 0.0168161, acc 1
2016-09-06T14:13:01.379803: step 3163, loss 0.0144245, acc 1
2016-09-06T14:13:02.158394: step 3164, loss 0.0047356, acc 1
2016-09-06T14:13:02.969769: step 3165, loss 0.0348241, acc 0.98
2016-09-06T14:13:03.779079: step 3166, loss 0.0294419, acc 1
2016-09-06T14:13:04.585663: step 3167, loss 0.0149477, acc 1
2016-09-06T14:13:05.407850: step 3168, loss 0.00363508, acc 1
2016-09-06T14:13:06.189423: step 3169, loss 0.0425133, acc 0.98
2016-09-06T14:13:06.980310: step 3170, loss 0.0324915, acc 1
2016-09-06T14:13:07.805575: step 3171, loss 0.00556788, acc 1
2016-09-06T14:13:08.604429: step 3172, loss 0.0421038, acc 1
2016-09-06T14:13:09.406886: step 3173, loss 0.0234946, acc 1
2016-09-06T14:13:10.222790: step 3174, loss 0.0119732, acc 1
2016-09-06T14:13:11.001420: step 3175, loss 0.0406303, acc 0.98
2016-09-06T14:13:11.812459: step 3176, loss 0.0373306, acc 0.96
2016-09-06T14:13:12.640398: step 3177, loss 0.0348038, acc 1
2016-09-06T14:13:13.451910: step 3178, loss 0.0396493, acc 0.98
2016-09-06T14:13:14.268648: step 3179, loss 0.0615383, acc 0.96
2016-09-06T14:13:15.090962: step 3180, loss 0.0496317, acc 0.98
2016-09-06T14:13:15.882567: step 3181, loss 0.0215404, acc 1
2016-09-06T14:13:16.725959: step 3182, loss 0.0660533, acc 0.98
2016-09-06T14:13:17.564551: step 3183, loss 0.0169194, acc 1
2016-09-06T14:13:18.360163: step 3184, loss 0.0475378, acc 0.98
2016-09-06T14:13:19.170469: step 3185, loss 0.00877914, acc 1
2016-09-06T14:13:19.976444: step 3186, loss 0.233432, acc 0.94
2016-09-06T14:13:20.760803: step 3187, loss 0.0476109, acc 1
2016-09-06T14:13:21.549721: step 3188, loss 0.102784, acc 0.98
2016-09-06T14:13:22.348720: step 3189, loss 0.0671761, acc 0.96
2016-09-06T14:13:23.128238: step 3190, loss 0.00518402, acc 1
2016-09-06T14:13:23.948352: step 3191, loss 0.0558535, acc 0.98
2016-09-06T14:13:24.778601: step 3192, loss 0.0875645, acc 0.96
2016-09-06T14:13:25.584666: step 3193, loss 0.0040733, acc 1
2016-09-06T14:13:26.346146: step 3194, loss 0.00903324, acc 1
2016-09-06T14:13:27.172603: step 3195, loss 0.0283572, acc 0.98
2016-09-06T14:13:27.962168: step 3196, loss 0.00538179, acc 1
2016-09-06T14:13:28.784087: step 3197, loss 0.0286579, acc 1
2016-09-06T14:13:29.587688: step 3198, loss 0.0092995, acc 1
2016-09-06T14:13:30.393185: step 3199, loss 0.0496327, acc 0.96
2016-09-06T14:13:31.195610: step 3200, loss 0.0554732, acc 0.98

Evaluation:
2016-09-06T14:13:34.914281: step 3200, loss 1.62281, acc 0.753283

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-3200

2016-09-06T14:13:36.800129: step 3201, loss 0.0230889, acc 1
2016-09-06T14:13:37.613638: step 3202, loss 0.100752, acc 0.94
2016-09-06T14:13:38.461054: step 3203, loss 0.00621749, acc 1
2016-09-06T14:13:39.287513: step 3204, loss 0.0652474, acc 0.98
2016-09-06T14:13:40.075896: step 3205, loss 0.0906367, acc 0.96
2016-09-06T14:13:40.865560: step 3206, loss 0.0433724, acc 1
2016-09-06T14:13:41.688885: step 3207, loss 0.0217769, acc 0.98
2016-09-06T14:13:42.488604: step 3208, loss 0.0228656, acc 1
2016-09-06T14:13:43.300925: step 3209, loss 0.0460423, acc 0.98
2016-09-06T14:13:44.116082: step 3210, loss 0.0491257, acc 0.96
2016-09-06T14:13:44.914717: step 3211, loss 0.0597171, acc 0.98
2016-09-06T14:13:45.714399: step 3212, loss 0.0228312, acc 0.98
2016-09-06T14:13:46.526214: step 3213, loss 0.0308895, acc 0.98
2016-09-06T14:13:47.322597: step 3214, loss 0.00430692, acc 1
2016-09-06T14:13:48.122552: step 3215, loss 0.0209105, acc 0.98
2016-09-06T14:13:48.941911: step 3216, loss 0.0213246, acc 1
2016-09-06T14:13:49.733355: step 3217, loss 0.0569746, acc 0.96
2016-09-06T14:13:50.545067: step 3218, loss 0.0114064, acc 1
2016-09-06T14:13:51.346659: step 3219, loss 0.0357388, acc 0.98
2016-09-06T14:13:52.129709: step 3220, loss 0.0754543, acc 0.96
2016-09-06T14:13:52.932313: step 3221, loss 0.0414974, acc 0.98
2016-09-06T14:13:53.796887: step 3222, loss 0.126686, acc 0.94
2016-09-06T14:13:54.572901: step 3223, loss 0.0357117, acc 0.98
2016-09-06T14:13:55.385258: step 3224, loss 0.225027, acc 0.96
2016-09-06T14:13:56.221450: step 3225, loss 0.0388104, acc 0.98
2016-09-06T14:13:57.006216: step 3226, loss 0.0691902, acc 0.96
2016-09-06T14:13:57.805763: step 3227, loss 0.0250247, acc 1
2016-09-06T14:13:58.610100: step 3228, loss 0.00840198, acc 1
2016-09-06T14:13:59.404321: step 3229, loss 0.031209, acc 0.98
2016-09-06T14:14:00.218398: step 3230, loss 0.0162172, acc 1
2016-09-06T14:14:01.069248: step 3231, loss 0.0757867, acc 0.96
2016-09-06T14:14:01.842200: step 3232, loss 0.0318582, acc 0.98
2016-09-06T14:14:02.644524: step 3233, loss 0.148426, acc 0.94
2016-09-06T14:14:03.457256: step 3234, loss 0.00902429, acc 1
2016-09-06T14:14:04.252231: step 3235, loss 0.0268354, acc 1
2016-09-06T14:14:05.061094: step 3236, loss 0.0504667, acc 0.98
2016-09-06T14:14:05.906122: step 3237, loss 0.0243166, acc 1
2016-09-06T14:14:06.697121: step 3238, loss 0.0948065, acc 0.98
2016-09-06T14:14:07.501147: step 3239, loss 0.0174443, acc 1
2016-09-06T14:14:08.331550: step 3240, loss 0.0168522, acc 1
2016-09-06T14:14:09.097487: step 3241, loss 0.164666, acc 0.94
2016-09-06T14:14:09.917045: step 3242, loss 0.0313799, acc 0.98
2016-09-06T14:14:10.737965: step 3243, loss 0.0497574, acc 0.98
2016-09-06T14:14:11.513274: step 3244, loss 0.0363879, acc 0.98
2016-09-06T14:14:12.332590: step 3245, loss 0.0316309, acc 1
2016-09-06T14:14:13.170921: step 3246, loss 0.0497813, acc 0.98
2016-09-06T14:14:13.935120: step 3247, loss 0.185123, acc 0.92
2016-09-06T14:14:14.765405: step 3248, loss 0.0914942, acc 0.98
2016-09-06T14:14:15.568177: step 3249, loss 0.0409254, acc 0.98
2016-09-06T14:14:16.350568: step 3250, loss 0.0981701, acc 0.96
2016-09-06T14:14:17.171651: step 3251, loss 0.0335185, acc 1
2016-09-06T14:14:17.983516: step 3252, loss 0.0491983, acc 0.96
2016-09-06T14:14:18.754923: step 3253, loss 0.0150589, acc 1
2016-09-06T14:14:19.571774: step 3254, loss 0.0675138, acc 0.98
2016-09-06T14:14:20.386155: step 3255, loss 0.0631667, acc 0.96
2016-09-06T14:14:21.164348: step 3256, loss 0.113868, acc 0.98
2016-09-06T14:14:21.960100: step 3257, loss 0.083234, acc 0.94
2016-09-06T14:14:22.775750: step 3258, loss 0.033448, acc 1
2016-09-06T14:14:23.561044: step 3259, loss 0.0934761, acc 0.96
2016-09-06T14:14:24.383295: step 3260, loss 0.0161904, acc 1
2016-09-06T14:14:25.210226: step 3261, loss 0.0412051, acc 0.98
2016-09-06T14:14:25.982409: step 3262, loss 0.0817311, acc 0.98
2016-09-06T14:14:26.793723: step 3263, loss 0.0435353, acc 0.96
2016-09-06T14:14:27.547307: step 3264, loss 0.0106482, acc 1
2016-09-06T14:14:28.360950: step 3265, loss 0.0363901, acc 0.98
2016-09-06T14:14:29.196706: step 3266, loss 0.079695, acc 0.94
2016-09-06T14:14:30.018991: step 3267, loss 0.0399419, acc 0.98
2016-09-06T14:14:30.826449: step 3268, loss 0.0233486, acc 0.98
2016-09-06T14:14:31.621706: step 3269, loss 0.0437906, acc 0.96
2016-09-06T14:14:32.435461: step 3270, loss 0.0469833, acc 1
2016-09-06T14:14:33.215160: step 3271, loss 0.0382534, acc 0.98
2016-09-06T14:14:34.032757: step 3272, loss 0.0449864, acc 0.98
2016-09-06T14:14:34.841273: step 3273, loss 0.0250415, acc 1
2016-09-06T14:14:35.632892: step 3274, loss 0.049187, acc 0.98
2016-09-06T14:14:36.442438: step 3275, loss 0.013957, acc 1
2016-09-06T14:14:37.257971: step 3276, loss 0.0158702, acc 1
2016-09-06T14:14:38.051433: step 3277, loss 0.0464966, acc 0.98
2016-09-06T14:14:38.883173: step 3278, loss 0.016882, acc 1
2016-09-06T14:14:39.703220: step 3279, loss 0.038897, acc 0.98
2016-09-06T14:14:40.502999: step 3280, loss 0.00753988, acc 1
2016-09-06T14:14:41.300041: step 3281, loss 0.021147, acc 0.98
2016-09-06T14:14:42.109952: step 3282, loss 0.0442309, acc 0.98
2016-09-06T14:14:42.910405: step 3283, loss 0.00662073, acc 1
2016-09-06T14:14:43.726492: step 3284, loss 0.0178332, acc 1
2016-09-06T14:14:44.549760: step 3285, loss 0.110808, acc 0.98
2016-09-06T14:14:45.345332: step 3286, loss 0.0259933, acc 0.98
2016-09-06T14:14:46.177471: step 3287, loss 0.011543, acc 1
2016-09-06T14:14:47.012795: step 3288, loss 0.0226324, acc 1
2016-09-06T14:14:47.815234: step 3289, loss 0.0268219, acc 0.98
2016-09-06T14:14:48.640836: step 3290, loss 0.163169, acc 0.92
2016-09-06T14:14:49.510812: step 3291, loss 0.0138919, acc 1
2016-09-06T14:14:50.324904: step 3292, loss 0.0115994, acc 1
2016-09-06T14:14:51.146614: step 3293, loss 0.0043225, acc 1
2016-09-06T14:14:51.967786: step 3294, loss 0.0290806, acc 0.98
2016-09-06T14:14:52.793044: step 3295, loss 0.0044648, acc 1
2016-09-06T14:14:53.620982: step 3296, loss 0.0727444, acc 0.98
2016-09-06T14:14:54.459737: step 3297, loss 0.0610552, acc 0.98
2016-09-06T14:14:55.268199: step 3298, loss 0.0441131, acc 0.98
2016-09-06T14:14:56.078530: step 3299, loss 0.0126482, acc 1
2016-09-06T14:14:56.902602: step 3300, loss 0.0436133, acc 0.98

Evaluation:
2016-09-06T14:15:00.627931: step 3300, loss 1.55783, acc 0.75985

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-3300

2016-09-06T14:15:02.546549: step 3301, loss 0.0279419, acc 0.98
2016-09-06T14:15:03.382730: step 3302, loss 0.0287854, acc 0.98
2016-09-06T14:15:04.197355: step 3303, loss 0.0153288, acc 1
2016-09-06T14:15:05.016962: step 3304, loss 0.0283214, acc 0.98
2016-09-06T14:15:05.837519: step 3305, loss 0.00480706, acc 1
2016-09-06T14:15:06.667063: step 3306, loss 0.0197971, acc 0.98
2016-09-06T14:15:07.510371: step 3307, loss 0.110271, acc 0.94
2016-09-06T14:15:08.322878: step 3308, loss 0.0300041, acc 1
2016-09-06T14:15:09.129598: step 3309, loss 0.0366647, acc 0.98
2016-09-06T14:15:09.969441: step 3310, loss 0.0177247, acc 1
2016-09-06T14:15:10.753667: step 3311, loss 0.0669443, acc 0.94
2016-09-06T14:15:11.581698: step 3312, loss 0.0214439, acc 1
2016-09-06T14:15:12.398230: step 3313, loss 0.0776054, acc 0.94
2016-09-06T14:15:13.203018: step 3314, loss 0.0194162, acc 0.98
2016-09-06T14:15:14.018938: step 3315, loss 0.13212, acc 0.94
2016-09-06T14:15:14.831602: step 3316, loss 0.0698213, acc 0.98
2016-09-06T14:15:15.632473: step 3317, loss 0.0345001, acc 0.98
2016-09-06T14:15:16.446511: step 3318, loss 0.0825373, acc 0.92
2016-09-06T14:15:17.257015: step 3319, loss 0.0494075, acc 1
2016-09-06T14:15:18.053323: step 3320, loss 0.127994, acc 0.94
2016-09-06T14:15:18.874527: step 3321, loss 0.0256762, acc 1
2016-09-06T14:15:19.712000: step 3322, loss 0.0227717, acc 0.98
2016-09-06T14:15:20.564873: step 3323, loss 0.0567222, acc 0.98
2016-09-06T14:15:21.372447: step 3324, loss 0.0383576, acc 0.98
2016-09-06T14:15:22.225152: step 3325, loss 0.029602, acc 0.98
2016-09-06T14:15:23.026960: step 3326, loss 0.00739679, acc 1
2016-09-06T14:15:23.843896: step 3327, loss 0.0350766, acc 0.98
2016-09-06T14:15:24.679671: step 3328, loss 0.0562228, acc 0.96
2016-09-06T14:15:25.508089: step 3329, loss 0.0538625, acc 0.98
2016-09-06T14:15:26.301412: step 3330, loss 0.0378806, acc 1
2016-09-06T14:15:27.138813: step 3331, loss 0.0139344, acc 1
2016-09-06T14:15:27.964870: step 3332, loss 0.0559778, acc 0.96
2016-09-06T14:15:28.792417: step 3333, loss 0.104372, acc 0.98
2016-09-06T14:15:29.629703: step 3334, loss 0.0544764, acc 0.98
2016-09-06T14:15:30.417984: step 3335, loss 0.0235204, acc 1
2016-09-06T14:15:31.217184: step 3336, loss 0.0174559, acc 1
2016-09-06T14:15:32.043872: step 3337, loss 0.00603564, acc 1
2016-09-06T14:15:32.842160: step 3338, loss 0.0301499, acc 0.98
2016-09-06T14:15:33.660813: step 3339, loss 0.0242909, acc 1
2016-09-06T14:15:34.476945: step 3340, loss 0.0198355, acc 0.98
2016-09-06T14:15:35.280467: step 3341, loss 0.0106727, acc 1
2016-09-06T14:15:36.066507: step 3342, loss 0.0189337, acc 1
2016-09-06T14:15:36.874089: step 3343, loss 0.027656, acc 1
2016-09-06T14:15:37.698715: step 3344, loss 0.0686936, acc 0.96
2016-09-06T14:15:38.474642: step 3345, loss 0.00609894, acc 1
2016-09-06T14:15:39.279100: step 3346, loss 0.0171232, acc 0.98
2016-09-06T14:15:40.061470: step 3347, loss 0.0751536, acc 0.96
2016-09-06T14:15:40.886544: step 3348, loss 0.0470931, acc 0.98
2016-09-06T14:15:41.700813: step 3349, loss 0.0603057, acc 0.98
2016-09-06T14:15:42.497909: step 3350, loss 0.179122, acc 0.96
2016-09-06T14:15:43.299363: step 3351, loss 0.00433603, acc 1
2016-09-06T14:15:44.121821: step 3352, loss 0.0130961, acc 1
2016-09-06T14:15:44.992524: step 3353, loss 0.0669628, acc 0.94
2016-09-06T14:15:45.786654: step 3354, loss 0.0260837, acc 0.98
2016-09-06T14:15:46.585166: step 3355, loss 0.0202789, acc 1
2016-09-06T14:15:47.418502: step 3356, loss 0.00822129, acc 1
2016-09-06T14:15:48.215828: step 3357, loss 0.0203836, acc 0.98
2016-09-06T14:15:48.991860: step 3358, loss 0.00454103, acc 1
2016-09-06T14:15:49.828856: step 3359, loss 0.0206964, acc 1
2016-09-06T14:15:50.624916: step 3360, loss 0.0133382, acc 1
2016-09-06T14:15:51.447564: step 3361, loss 0.131315, acc 0.96
2016-09-06T14:15:52.263995: step 3362, loss 0.0575178, acc 0.96
2016-09-06T14:15:53.056593: step 3363, loss 0.0178175, acc 1
2016-09-06T14:15:53.858486: step 3364, loss 0.0311523, acc 1
2016-09-06T14:15:54.664870: step 3365, loss 0.0867493, acc 0.94
2016-09-06T14:15:55.427194: step 3366, loss 0.0150564, acc 1
2016-09-06T14:15:56.213016: step 3367, loss 0.0711483, acc 0.98
2016-09-06T14:15:57.031493: step 3368, loss 0.167275, acc 0.98
2016-09-06T14:15:57.806710: step 3369, loss 0.0267309, acc 1
2016-09-06T14:15:58.621120: step 3370, loss 0.0308471, acc 1
2016-09-06T14:15:59.437692: step 3371, loss 0.0280985, acc 1
2016-09-06T14:16:00.264936: step 3372, loss 0.0344523, acc 0.98
2016-09-06T14:16:01.079020: step 3373, loss 0.0453878, acc 0.98
2016-09-06T14:16:01.940452: step 3374, loss 0.114436, acc 0.94
2016-09-06T14:16:02.730037: step 3375, loss 0.0236441, acc 0.98
2016-09-06T14:16:03.514938: step 3376, loss 0.124253, acc 0.94
2016-09-06T14:16:04.330133: step 3377, loss 0.193453, acc 0.96
2016-09-06T14:16:05.107868: step 3378, loss 0.0213648, acc 1
2016-09-06T14:16:05.939516: step 3379, loss 0.0758892, acc 0.94
2016-09-06T14:16:06.730163: step 3380, loss 0.0198531, acc 1
2016-09-06T14:16:07.506790: step 3381, loss 0.025971, acc 0.98
2016-09-06T14:16:08.307316: step 3382, loss 0.013132, acc 1
2016-09-06T14:16:09.130475: step 3383, loss 0.0343965, acc 0.98
2016-09-06T14:16:09.920676: step 3384, loss 0.0950915, acc 0.96
2016-09-06T14:16:10.712795: step 3385, loss 0.0286784, acc 0.98
2016-09-06T14:16:11.523819: step 3386, loss 0.0101959, acc 1
2016-09-06T14:16:12.292425: step 3387, loss 0.0805238, acc 0.96
2016-09-06T14:16:13.113491: step 3388, loss 0.0244585, acc 1
2016-09-06T14:16:13.907086: step 3389, loss 0.0635004, acc 1
2016-09-06T14:16:14.722220: step 3390, loss 0.036307, acc 1
2016-09-06T14:16:15.551124: step 3391, loss 0.0279507, acc 0.98
2016-09-06T14:16:16.351942: step 3392, loss 0.0119115, acc 1
2016-09-06T14:16:17.137057: step 3393, loss 0.00906957, acc 1
2016-09-06T14:16:17.962743: step 3394, loss 0.0650771, acc 0.98
2016-09-06T14:16:18.796776: step 3395, loss 0.0350204, acc 1
2016-09-06T14:16:19.596585: step 3396, loss 0.0532379, acc 0.96
2016-09-06T14:16:20.398334: step 3397, loss 0.0842198, acc 0.96
2016-09-06T14:16:21.223410: step 3398, loss 0.0469536, acc 0.96
2016-09-06T14:16:22.007861: step 3399, loss 0.102876, acc 0.96
2016-09-06T14:16:22.804982: step 3400, loss 0.0331714, acc 0.98

Evaluation:
2016-09-06T14:16:26.537246: step 3400, loss 1.37372, acc 0.76454

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-3400

2016-09-06T14:16:28.360289: step 3401, loss 0.0171751, acc 1
2016-09-06T14:16:29.157680: step 3402, loss 0.0728934, acc 0.98
2016-09-06T14:16:29.986859: step 3403, loss 0.0214872, acc 0.98
2016-09-06T14:16:30.789800: step 3404, loss 0.0268771, acc 1
2016-09-06T14:16:31.598850: step 3405, loss 0.0897367, acc 0.98
2016-09-06T14:16:32.445895: step 3406, loss 0.0427917, acc 0.98
2016-09-06T14:16:33.247301: step 3407, loss 0.0907292, acc 0.96
2016-09-06T14:16:34.026498: step 3408, loss 0.0362994, acc 0.98
2016-09-06T14:16:34.867402: step 3409, loss 0.0390704, acc 0.98
2016-09-06T14:16:35.667931: step 3410, loss 0.0741331, acc 0.98
2016-09-06T14:16:36.473989: step 3411, loss 0.0210379, acc 0.98
2016-09-06T14:16:37.301597: step 3412, loss 0.0537708, acc 0.96
2016-09-06T14:16:38.138420: step 3413, loss 0.217948, acc 0.96
2016-09-06T14:16:38.990582: step 3414, loss 0.0698051, acc 0.98
2016-09-06T14:16:39.821249: step 3415, loss 0.0140906, acc 1
2016-09-06T14:16:40.664353: step 3416, loss 0.0328352, acc 0.98
2016-09-06T14:16:41.494753: step 3417, loss 0.0721293, acc 0.96
2016-09-06T14:16:42.316166: step 3418, loss 0.0461419, acc 1
2016-09-06T14:16:43.146329: step 3419, loss 0.0441555, acc 0.98
2016-09-06T14:16:43.947877: step 3420, loss 0.0231112, acc 1
2016-09-06T14:16:44.748307: step 3421, loss 0.0257926, acc 1
2016-09-06T14:16:45.556885: step 3422, loss 0.0991603, acc 0.96
2016-09-06T14:16:46.337401: step 3423, loss 0.0190786, acc 1
2016-09-06T14:16:47.151957: step 3424, loss 0.168123, acc 0.94
2016-09-06T14:16:47.964881: step 3425, loss 0.0457629, acc 1
2016-09-06T14:16:48.740963: step 3426, loss 0.0622255, acc 0.98
2016-09-06T14:16:49.535171: step 3427, loss 0.0592898, acc 0.98
2016-09-06T14:16:50.379633: step 3428, loss 0.0120882, acc 1
2016-09-06T14:16:51.150425: step 3429, loss 0.00740635, acc 1
2016-09-06T14:16:51.963631: step 3430, loss 0.0647611, acc 0.98
2016-09-06T14:16:52.789577: step 3431, loss 0.0271001, acc 1
2016-09-06T14:16:53.554862: step 3432, loss 0.0535173, acc 0.94
2016-09-06T14:16:54.360846: step 3433, loss 0.177605, acc 0.9
2016-09-06T14:16:55.173295: step 3434, loss 0.0696914, acc 0.98
2016-09-06T14:16:55.978715: step 3435, loss 0.0329212, acc 0.98
2016-09-06T14:16:56.769969: step 3436, loss 0.0355976, acc 1
2016-09-06T14:16:57.628502: step 3437, loss 0.0712584, acc 0.96
2016-09-06T14:16:58.417128: step 3438, loss 0.0189281, acc 1
2016-09-06T14:16:59.217402: step 3439, loss 0.128309, acc 0.94
2016-09-06T14:17:00.039988: step 3440, loss 0.00956406, acc 1
2016-09-06T14:17:00.858307: step 3441, loss 0.0363865, acc 0.98
2016-09-06T14:17:01.652965: step 3442, loss 0.107102, acc 0.96
2016-09-06T14:17:02.510064: step 3443, loss 0.04036, acc 0.98
2016-09-06T14:17:03.310058: step 3444, loss 0.0922441, acc 0.94
2016-09-06T14:17:04.127188: step 3445, loss 0.0397719, acc 0.98
2016-09-06T14:17:04.957870: step 3446, loss 0.0740646, acc 0.96
2016-09-06T14:17:05.773389: step 3447, loss 0.0153535, acc 1
2016-09-06T14:17:06.599105: step 3448, loss 0.0435897, acc 0.98
2016-09-06T14:17:07.441838: step 3449, loss 0.0296525, acc 0.98
2016-09-06T14:17:08.250446: step 3450, loss 0.0611425, acc 0.98
2016-09-06T14:17:09.074753: step 3451, loss 0.0751072, acc 0.94
2016-09-06T14:17:09.913042: step 3452, loss 0.0194078, acc 1
2016-09-06T14:17:10.739341: step 3453, loss 0.0322906, acc 0.98
2016-09-06T14:17:11.559604: step 3454, loss 0.0748696, acc 0.98
2016-09-06T14:17:12.405376: step 3455, loss 0.0537303, acc 0.98
2016-09-06T14:17:13.154986: step 3456, loss 0.114104, acc 0.931818
2016-09-06T14:17:13.996342: step 3457, loss 0.0624823, acc 0.98
2016-09-06T14:17:14.840195: step 3458, loss 0.0713959, acc 0.98
2016-09-06T14:17:15.644283: step 3459, loss 0.0198204, acc 1
2016-09-06T14:17:16.460726: step 3460, loss 0.0863948, acc 0.98
2016-09-06T14:17:17.277004: step 3461, loss 0.0334892, acc 1
2016-09-06T14:17:18.094684: step 3462, loss 0.122685, acc 0.98
2016-09-06T14:17:18.891603: step 3463, loss 0.069317, acc 0.96
2016-09-06T14:17:19.718673: step 3464, loss 0.0676696, acc 0.96
2016-09-06T14:17:20.573444: step 3465, loss 0.0965344, acc 0.96
2016-09-06T14:17:21.368851: step 3466, loss 0.0385445, acc 0.98
2016-09-06T14:17:22.171336: step 3467, loss 0.00878546, acc 1
2016-09-06T14:17:22.984851: step 3468, loss 0.0485863, acc 0.98
2016-09-06T14:17:23.783456: step 3469, loss 0.0533991, acc 0.98
2016-09-06T14:17:24.594961: step 3470, loss 0.0233067, acc 1
2016-09-06T14:17:25.432836: step 3471, loss 0.113129, acc 0.94
2016-09-06T14:17:26.192473: step 3472, loss 0.0160873, acc 1
2016-09-06T14:17:27.002999: step 3473, loss 0.00643291, acc 1
2016-09-06T14:17:27.813824: step 3474, loss 0.00802286, acc 1
2016-09-06T14:17:28.586125: step 3475, loss 0.0133548, acc 1
2016-09-06T14:17:29.390674: step 3476, loss 0.0258164, acc 0.98
2016-09-06T14:17:30.225118: step 3477, loss 0.0678886, acc 0.96
2016-09-06T14:17:31.026404: step 3478, loss 0.0400197, acc 0.98
2016-09-06T14:17:31.822780: step 3479, loss 0.102909, acc 0.98
2016-09-06T14:17:32.634377: step 3480, loss 0.0632046, acc 0.98
2016-09-06T14:17:33.438160: step 3481, loss 0.0186211, acc 1
2016-09-06T14:17:34.259888: step 3482, loss 0.0550172, acc 0.96
2016-09-06T14:17:35.090227: step 3483, loss 0.0194427, acc 1
2016-09-06T14:17:35.861631: step 3484, loss 0.0287876, acc 0.98
2016-09-06T14:17:36.685451: step 3485, loss 0.0494766, acc 0.98
2016-09-06T14:17:37.505713: step 3486, loss 0.0216943, acc 1
2016-09-06T14:17:38.280727: step 3487, loss 0.00347778, acc 1
2016-09-06T14:17:39.096957: step 3488, loss 0.0322963, acc 0.98
2016-09-06T14:17:39.904669: step 3489, loss 0.00668943, acc 1
2016-09-06T14:17:40.684339: step 3490, loss 0.0415362, acc 0.98
2016-09-06T14:17:41.514055: step 3491, loss 0.0336001, acc 1
2016-09-06T14:17:42.301533: step 3492, loss 0.0125549, acc 1
2016-09-06T14:17:43.092248: step 3493, loss 0.0550566, acc 0.98
2016-09-06T14:17:43.908895: step 3494, loss 0.00469289, acc 1
2016-09-06T14:17:44.755700: step 3495, loss 0.0269677, acc 1
2016-09-06T14:17:45.527079: step 3496, loss 0.0142001, acc 1
2016-09-06T14:17:46.346582: step 3497, loss 0.0123044, acc 1
2016-09-06T14:17:47.179800: step 3498, loss 0.0482379, acc 0.98
2016-09-06T14:17:47.992509: step 3499, loss 0.0257721, acc 1
2016-09-06T14:17:48.805082: step 3500, loss 0.0927533, acc 0.92

Evaluation:
2016-09-06T14:17:52.552151: step 3500, loss 1.53377, acc 0.757974

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-3500

2016-09-06T14:17:54.531806: step 3501, loss 0.0533453, acc 0.98
2016-09-06T14:17:55.287749: step 3502, loss 0.123731, acc 0.92
2016-09-06T14:17:56.076268: step 3503, loss 0.0317475, acc 0.98
2016-09-06T14:17:56.911909: step 3504, loss 0.0139437, acc 1
2016-09-06T14:17:57.705637: step 3505, loss 0.152379, acc 0.92
2016-09-06T14:17:58.512951: step 3506, loss 0.0474168, acc 1
2016-09-06T14:17:59.326707: step 3507, loss 0.12938, acc 0.98
2016-09-06T14:18:00.128647: step 3508, loss 0.0457565, acc 0.98
2016-09-06T14:18:00.953256: step 3509, loss 0.0706197, acc 0.96
2016-09-06T14:18:01.773459: step 3510, loss 0.0261868, acc 1
2016-09-06T14:18:02.537824: step 3511, loss 0.0133946, acc 1
2016-09-06T14:18:03.340847: step 3512, loss 0.0541142, acc 0.96
2016-09-06T14:18:04.142542: step 3513, loss 0.00554824, acc 1
2016-09-06T14:18:04.932433: step 3514, loss 0.0280564, acc 1
2016-09-06T14:18:05.768732: step 3515, loss 0.0464531, acc 0.96
2016-09-06T14:18:06.568452: step 3516, loss 0.0128608, acc 1
2016-09-06T14:18:07.354397: step 3517, loss 0.0259619, acc 0.98
2016-09-06T14:18:08.191220: step 3518, loss 0.0191463, acc 1
2016-09-06T14:18:09.001707: step 3519, loss 0.0219486, acc 1
2016-09-06T14:18:09.798616: step 3520, loss 0.077561, acc 0.94
2016-09-06T14:18:10.594893: step 3521, loss 0.0354752, acc 1
2016-09-06T14:18:11.445970: step 3522, loss 0.0159212, acc 1
2016-09-06T14:18:12.223753: step 3523, loss 0.00391315, acc 1
2016-09-06T14:18:13.037067: step 3524, loss 0.0965973, acc 0.96
2016-09-06T14:18:13.837441: step 3525, loss 0.0329809, acc 0.98
2016-09-06T14:18:14.632185: step 3526, loss 0.0226122, acc 0.98
2016-09-06T14:18:15.433833: step 3527, loss 0.123964, acc 0.98
2016-09-06T14:18:16.261518: step 3528, loss 0.0291681, acc 0.98
2016-09-06T14:18:17.078243: step 3529, loss 0.0642406, acc 0.94
2016-09-06T14:18:17.877420: step 3530, loss 0.0249106, acc 1
2016-09-06T14:18:18.694596: step 3531, loss 0.0292726, acc 1
2016-09-06T14:18:19.496178: step 3532, loss 0.0394053, acc 1
2016-09-06T14:18:20.301292: step 3533, loss 0.0188155, acc 1
2016-09-06T14:18:21.115679: step 3534, loss 0.0937166, acc 0.96
2016-09-06T14:18:21.931324: step 3535, loss 0.0284356, acc 0.98
2016-09-06T14:18:22.730636: step 3536, loss 0.0136394, acc 1
2016-09-06T14:18:23.537021: step 3537, loss 0.00358096, acc 1
2016-09-06T14:18:24.323492: step 3538, loss 0.0813842, acc 0.98
2016-09-06T14:18:25.113694: step 3539, loss 0.0387823, acc 0.98
2016-09-06T14:18:25.947865: step 3540, loss 0.0180107, acc 1
2016-09-06T14:18:26.741848: step 3541, loss 0.0897586, acc 0.94
2016-09-06T14:18:27.532794: step 3542, loss 0.0413773, acc 0.96
2016-09-06T14:18:28.362295: step 3543, loss 0.00527544, acc 1
2016-09-06T14:18:29.142251: step 3544, loss 0.00776821, acc 1
2016-09-06T14:18:29.960815: step 3545, loss 0.0555935, acc 1
2016-09-06T14:18:30.765612: step 3546, loss 0.00918103, acc 1
2016-09-06T14:18:31.557642: step 3547, loss 0.0102039, acc 1
2016-09-06T14:18:32.361614: step 3548, loss 0.00716743, acc 1
2016-09-06T14:18:33.158787: step 3549, loss 0.0999965, acc 0.92
2016-09-06T14:18:33.944984: step 3550, loss 0.0292519, acc 1
2016-09-06T14:18:34.746675: step 3551, loss 0.0170833, acc 1
2016-09-06T14:18:35.560778: step 3552, loss 0.00437968, acc 1
2016-09-06T14:18:36.368383: step 3553, loss 0.00584905, acc 1
2016-09-06T14:18:37.168364: step 3554, loss 0.055281, acc 0.98
2016-09-06T14:18:38.010485: step 3555, loss 0.00423969, acc 1
2016-09-06T14:18:38.782551: step 3556, loss 0.0458641, acc 0.98
2016-09-06T14:18:39.648714: step 3557, loss 0.0384584, acc 0.96
2016-09-06T14:18:40.449403: step 3558, loss 0.0666791, acc 0.98
2016-09-06T14:18:41.243054: step 3559, loss 0.13514, acc 0.94
2016-09-06T14:18:42.034758: step 3560, loss 0.0199985, acc 1
2016-09-06T14:18:42.839168: step 3561, loss 0.0459063, acc 0.98
2016-09-06T14:18:43.613301: step 3562, loss 0.0517392, acc 0.98
2016-09-06T14:18:44.440185: step 3563, loss 0.0425476, acc 1
2016-09-06T14:18:45.235959: step 3564, loss 0.0065859, acc 1
2016-09-06T14:18:46.045055: step 3565, loss 0.0168227, acc 1
2016-09-06T14:18:46.847623: step 3566, loss 0.0298562, acc 1
2016-09-06T14:18:47.673603: step 3567, loss 0.0177288, acc 1
2016-09-06T14:18:48.455740: step 3568, loss 0.076279, acc 0.92
2016-09-06T14:18:49.248527: step 3569, loss 0.0483821, acc 0.96
2016-09-06T14:18:50.090780: step 3570, loss 0.0355109, acc 0.98
2016-09-06T14:18:50.877411: step 3571, loss 0.00714745, acc 1
2016-09-06T14:18:51.674599: step 3572, loss 0.0684531, acc 0.98
2016-09-06T14:18:52.497807: step 3573, loss 0.0253031, acc 0.98
2016-09-06T14:18:53.276197: step 3574, loss 0.0816077, acc 0.98
2016-09-06T14:18:54.119696: step 3575, loss 0.0421003, acc 0.98
2016-09-06T14:18:54.934312: step 3576, loss 0.0284243, acc 1
2016-09-06T14:18:55.774374: step 3577, loss 0.0631992, acc 0.96
2016-09-06T14:18:56.598152: step 3578, loss 0.0435281, acc 1
2016-09-06T14:18:57.425471: step 3579, loss 0.0223551, acc 1
2016-09-06T14:18:58.246481: step 3580, loss 0.0302474, acc 1
2016-09-06T14:18:59.026571: step 3581, loss 0.0303748, acc 1
2016-09-06T14:18:59.852367: step 3582, loss 0.0286574, acc 0.98
2016-09-06T14:19:00.681523: step 3583, loss 0.0473145, acc 0.98
2016-09-06T14:19:01.484787: step 3584, loss 0.037749, acc 0.98
2016-09-06T14:19:02.308736: step 3585, loss 0.0251564, acc 1
2016-09-06T14:19:03.115715: step 3586, loss 0.0905147, acc 0.94
2016-09-06T14:19:03.906161: step 3587, loss 0.0114825, acc 1
2016-09-06T14:19:04.728637: step 3588, loss 0.0146509, acc 1
2016-09-06T14:19:05.528431: step 3589, loss 0.0283202, acc 1
2016-09-06T14:19:06.326695: step 3590, loss 0.0548358, acc 0.96
2016-09-06T14:19:07.152962: step 3591, loss 0.00575232, acc 1
2016-09-06T14:19:07.971834: step 3592, loss 0.0366629, acc 0.98
2016-09-06T14:19:08.815887: step 3593, loss 0.0204452, acc 1
2016-09-06T14:19:09.654291: step 3594, loss 0.0442565, acc 0.98
2016-09-06T14:19:10.477833: step 3595, loss 0.0337088, acc 0.98
2016-09-06T14:19:11.276495: step 3596, loss 0.0115807, acc 1
2016-09-06T14:19:12.121750: step 3597, loss 0.0371924, acc 0.98
2016-09-06T14:19:12.933325: step 3598, loss 0.00842906, acc 1
2016-09-06T14:19:13.744667: step 3599, loss 0.0164763, acc 1
2016-09-06T14:19:14.577023: step 3600, loss 0.039875, acc 0.96

Evaluation:
2016-09-06T14:19:18.318444: step 3600, loss 1.59643, acc 0.758912

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-3600

2016-09-06T14:19:20.197676: step 3601, loss 0.043763, acc 0.98
2016-09-06T14:19:21.024616: step 3602, loss 0.0529738, acc 0.98
2016-09-06T14:19:21.853944: step 3603, loss 0.00953005, acc 1
2016-09-06T14:19:22.685391: step 3604, loss 0.117634, acc 0.94
2016-09-06T14:19:23.505244: step 3605, loss 0.0223679, acc 0.98
2016-09-06T14:19:24.354957: step 3606, loss 0.022984, acc 1
2016-09-06T14:19:25.161368: step 3607, loss 0.0363221, acc 0.98
2016-09-06T14:19:25.944971: step 3608, loss 0.00331783, acc 1
2016-09-06T14:19:26.761033: step 3609, loss 0.00648818, acc 1
2016-09-06T14:19:27.591089: step 3610, loss 0.0081132, acc 1
2016-09-06T14:19:28.365405: step 3611, loss 0.00641325, acc 1
2016-09-06T14:19:29.209197: step 3612, loss 0.0699465, acc 0.98
2016-09-06T14:19:30.025873: step 3613, loss 0.00819637, acc 1
2016-09-06T14:19:30.835347: step 3614, loss 0.0306859, acc 0.98
2016-09-06T14:19:31.630309: step 3615, loss 0.0480218, acc 0.96
2016-09-06T14:19:32.422241: step 3616, loss 0.0926594, acc 0.92
2016-09-06T14:19:33.233664: step 3617, loss 0.0334109, acc 0.98
2016-09-06T14:19:34.057800: step 3618, loss 0.0214659, acc 0.98
2016-09-06T14:19:34.884563: step 3619, loss 0.0322522, acc 0.96
2016-09-06T14:19:35.687598: step 3620, loss 0.0180004, acc 0.98
2016-09-06T14:19:36.494193: step 3621, loss 0.0147328, acc 1
2016-09-06T14:19:37.314078: step 3622, loss 0.050238, acc 0.98
2016-09-06T14:19:38.144237: step 3623, loss 0.00349078, acc 1
2016-09-06T14:19:38.957082: step 3624, loss 0.0790645, acc 0.96
2016-09-06T14:19:39.765220: step 3625, loss 0.100008, acc 0.94
2016-09-06T14:19:40.578379: step 3626, loss 0.0149711, acc 1
2016-09-06T14:19:41.381448: step 3627, loss 0.0881965, acc 0.96
2016-09-06T14:19:42.209696: step 3628, loss 0.00431137, acc 1
2016-09-06T14:19:43.010188: step 3629, loss 0.00987268, acc 1
2016-09-06T14:19:43.806938: step 3630, loss 0.0270624, acc 1
2016-09-06T14:19:44.626820: step 3631, loss 0.0714204, acc 0.94
2016-09-06T14:19:45.407519: step 3632, loss 0.0131569, acc 1
2016-09-06T14:19:46.211032: step 3633, loss 0.0368054, acc 0.96
2016-09-06T14:19:47.023336: step 3634, loss 0.0729621, acc 0.94
2016-09-06T14:19:47.813488: step 3635, loss 0.0819678, acc 0.96
2016-09-06T14:19:48.617435: step 3636, loss 0.0301152, acc 0.98
2016-09-06T14:19:49.436219: step 3637, loss 0.00854893, acc 1
2016-09-06T14:19:50.243900: step 3638, loss 0.0648871, acc 0.96
2016-09-06T14:19:51.056409: step 3639, loss 0.0218496, acc 1
2016-09-06T14:19:51.868858: step 3640, loss 0.090705, acc 0.96
2016-09-06T14:19:52.692755: step 3641, loss 0.0305992, acc 0.98
2016-09-06T14:19:53.505960: step 3642, loss 0.0257842, acc 1
2016-09-06T14:19:54.350078: step 3643, loss 0.0230474, acc 0.98
2016-09-06T14:19:55.141402: step 3644, loss 0.00430957, acc 1
2016-09-06T14:19:55.955485: step 3645, loss 0.0588083, acc 0.98
2016-09-06T14:19:56.786332: step 3646, loss 0.0627881, acc 0.98
2016-09-06T14:19:57.590855: step 3647, loss 0.0531806, acc 0.96
2016-09-06T14:19:58.315269: step 3648, loss 0.0181631, acc 1
2016-09-06T14:19:59.138251: step 3649, loss 0.0274191, acc 0.98
2016-09-06T14:19:59.936003: step 3650, loss 0.073271, acc 0.96
2016-09-06T14:20:00.764891: step 3651, loss 0.0174804, acc 1
2016-09-06T14:20:01.570910: step 3652, loss 0.00936438, acc 1
2016-09-06T14:20:02.369417: step 3653, loss 0.0476952, acc 0.96
2016-09-06T14:20:03.170431: step 3654, loss 0.0480494, acc 0.98
2016-09-06T14:20:03.983916: step 3655, loss 0.0158053, acc 1
2016-09-06T14:20:04.754166: step 3656, loss 0.0325091, acc 1
2016-09-06T14:20:05.583964: step 3657, loss 0.0312675, acc 0.98
2016-09-06T14:20:06.411116: step 3658, loss 0.0618291, acc 0.98
2016-09-06T14:20:07.216246: step 3659, loss 0.0102919, acc 1
2016-09-06T14:20:08.031629: step 3660, loss 0.00392837, acc 1
2016-09-06T14:20:08.853627: step 3661, loss 0.0580407, acc 0.96
2016-09-06T14:20:09.682375: step 3662, loss 0.030822, acc 0.98
2016-09-06T14:20:10.484866: step 3663, loss 0.0460919, acc 0.98
2016-09-06T14:20:11.289721: step 3664, loss 0.00413032, acc 1
2016-09-06T14:20:12.073413: step 3665, loss 0.0112084, acc 1
2016-09-06T14:20:12.862163: step 3666, loss 0.050484, acc 0.98
2016-09-06T14:20:13.705147: step 3667, loss 0.0159425, acc 1
2016-09-06T14:20:14.467759: step 3668, loss 0.0144096, acc 1
2016-09-06T14:20:15.267372: step 3669, loss 0.0514409, acc 0.98
2016-09-06T14:20:16.109535: step 3670, loss 0.0277337, acc 0.98
2016-09-06T14:20:16.890496: step 3671, loss 0.00300052, acc 1
2016-09-06T14:20:17.685919: step 3672, loss 0.0393047, acc 0.98
2016-09-06T14:20:18.490181: step 3673, loss 0.0286173, acc 0.98
2016-09-06T14:20:19.272920: step 3674, loss 0.0713703, acc 0.98
2016-09-06T14:20:20.077162: step 3675, loss 0.0129647, acc 1
2016-09-06T14:20:20.888536: step 3676, loss 0.00899659, acc 1
2016-09-06T14:20:21.685794: step 3677, loss 0.00986935, acc 1
2016-09-06T14:20:22.492407: step 3678, loss 0.0293328, acc 1
2016-09-06T14:20:23.280566: step 3679, loss 0.0129955, acc 1
2016-09-06T14:20:24.084251: step 3680, loss 0.0330407, acc 1
2016-09-06T14:20:24.900268: step 3681, loss 0.0272928, acc 0.98
2016-09-06T14:20:25.717771: step 3682, loss 0.0119389, acc 1
2016-09-06T14:20:26.499148: step 3683, loss 0.025551, acc 1
2016-09-06T14:20:27.321782: step 3684, loss 0.00407606, acc 1
2016-09-06T14:20:28.131691: step 3685, loss 0.0266218, acc 1
2016-09-06T14:20:28.919291: step 3686, loss 0.0251021, acc 1
2016-09-06T14:20:29.757229: step 3687, loss 0.0218034, acc 1
2016-09-06T14:20:30.566029: step 3688, loss 0.00344536, acc 1
2016-09-06T14:20:31.367354: step 3689, loss 0.019424, acc 0.98
2016-09-06T14:20:32.191094: step 3690, loss 0.0427248, acc 0.98
2016-09-06T14:20:33.047235: step 3691, loss 0.140271, acc 0.94
2016-09-06T14:20:33.834066: step 3692, loss 0.0188138, acc 1
2016-09-06T14:20:34.638338: step 3693, loss 0.0464838, acc 0.98
2016-09-06T14:20:35.459288: step 3694, loss 0.0183458, acc 0.98
2016-09-06T14:20:36.234213: step 3695, loss 0.0153811, acc 1
2016-09-06T14:20:37.045590: step 3696, loss 0.0339693, acc 1
2016-09-06T14:20:37.856881: step 3697, loss 0.0480976, acc 0.98
2016-09-06T14:20:38.652321: step 3698, loss 0.00362513, acc 1
2016-09-06T14:20:39.476960: step 3699, loss 0.0315013, acc 0.98
2016-09-06T14:20:40.296777: step 3700, loss 0.0426941, acc 0.98

Evaluation:
2016-09-06T14:20:44.016258: step 3700, loss 1.65345, acc 0.762664

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-3700

2016-09-06T14:20:45.916954: step 3701, loss 0.0100882, acc 1
2016-09-06T14:20:46.731452: step 3702, loss 0.018499, acc 0.98
2016-09-06T14:20:47.564387: step 3703, loss 0.133763, acc 0.94
2016-09-06T14:20:48.335300: step 3704, loss 0.0519762, acc 0.98
2016-09-06T14:20:49.172421: step 3705, loss 0.0438085, acc 0.98
2016-09-06T14:20:49.984100: step 3706, loss 0.00947682, acc 1
2016-09-06T14:20:50.755851: step 3707, loss 0.0329347, acc 0.98
2016-09-06T14:20:51.555495: step 3708, loss 0.0451525, acc 0.98
2016-09-06T14:20:52.356767: step 3709, loss 0.00464404, acc 1
2016-09-06T14:20:53.148398: step 3710, loss 0.0208709, acc 0.98
2016-09-06T14:20:53.950203: step 3711, loss 0.00284598, acc 1
2016-09-06T14:20:54.766745: step 3712, loss 0.0373474, acc 0.98
2016-09-06T14:20:55.570605: step 3713, loss 0.00443044, acc 1
2016-09-06T14:20:56.376861: step 3714, loss 0.0648903, acc 0.98
2016-09-06T14:20:57.217415: step 3715, loss 0.0108147, acc 1
2016-09-06T14:20:58.006482: step 3716, loss 0.0292935, acc 0.98
2016-09-06T14:20:58.810297: step 3717, loss 0.0484546, acc 0.98
2016-09-06T14:20:59.640698: step 3718, loss 0.0166821, acc 1
2016-09-06T14:21:00.466182: step 3719, loss 0.0843024, acc 0.96
2016-09-06T14:21:01.280240: step 3720, loss 0.0408571, acc 0.98
2016-09-06T14:21:02.074556: step 3721, loss 0.0314236, acc 0.98
2016-09-06T14:21:02.854194: step 3722, loss 0.0167569, acc 1
2016-09-06T14:21:03.676499: step 3723, loss 0.0639692, acc 0.98
2016-09-06T14:21:04.491580: step 3724, loss 0.00726374, acc 1
2016-09-06T14:21:05.282320: step 3725, loss 0.0182633, acc 0.98
2016-09-06T14:21:06.087071: step 3726, loss 0.00575325, acc 1
2016-09-06T14:21:06.890332: step 3727, loss 0.0178752, acc 0.98
2016-09-06T14:21:07.665415: step 3728, loss 0.0238937, acc 0.98
2016-09-06T14:21:08.456223: step 3729, loss 0.0507076, acc 0.98
2016-09-06T14:21:09.289407: step 3730, loss 0.0143084, acc 1
2016-09-06T14:21:10.068429: step 3731, loss 0.0512246, acc 0.98
2016-09-06T14:21:10.869226: step 3732, loss 0.00363829, acc 1
2016-09-06T14:21:11.691671: step 3733, loss 0.0107842, acc 1
2016-09-06T14:21:12.479497: step 3734, loss 0.0339319, acc 0.98
2016-09-06T14:21:13.272901: step 3735, loss 0.0240739, acc 0.98
2016-09-06T14:21:14.091448: step 3736, loss 0.0170345, acc 1
2016-09-06T14:21:14.861838: step 3737, loss 0.0200102, acc 0.98
2016-09-06T14:21:15.672763: step 3738, loss 0.0229387, acc 1
2016-09-06T14:21:16.476991: step 3739, loss 0.0467691, acc 0.98
2016-09-06T14:21:17.305451: step 3740, loss 0.0200306, acc 0.98
2016-09-06T14:21:18.113393: step 3741, loss 0.0312343, acc 1
2016-09-06T14:21:18.954070: step 3742, loss 0.0331674, acc 0.98
2016-09-06T14:21:19.749460: step 3743, loss 0.0257381, acc 0.98
2016-09-06T14:21:20.561120: step 3744, loss 0.00649324, acc 1
2016-09-06T14:21:21.394287: step 3745, loss 0.0470118, acc 0.96
2016-09-06T14:21:22.179045: step 3746, loss 0.0190017, acc 0.98
2016-09-06T14:21:22.975432: step 3747, loss 0.0324819, acc 0.98
2016-09-06T14:21:23.792617: step 3748, loss 0.0735415, acc 0.96
2016-09-06T14:21:24.594513: step 3749, loss 0.0620603, acc 0.98
2016-09-06T14:21:25.398143: step 3750, loss 0.0180125, acc 1
2016-09-06T14:21:26.210847: step 3751, loss 0.0296076, acc 0.98
2016-09-06T14:21:27.009852: step 3752, loss 0.0109947, acc 1
2016-09-06T14:21:27.842369: step 3753, loss 0.0146202, acc 1
2016-09-06T14:21:28.646735: step 3754, loss 0.0233421, acc 0.98
2016-09-06T14:21:29.429103: step 3755, loss 0.0354114, acc 1
2016-09-06T14:21:30.253494: step 3756, loss 0.0110285, acc 1
2016-09-06T14:21:31.071892: step 3757, loss 0.13118, acc 0.96
2016-09-06T14:21:31.898781: step 3758, loss 0.0499879, acc 0.98
2016-09-06T14:21:32.703279: step 3759, loss 0.00270967, acc 1
2016-09-06T14:21:33.519327: step 3760, loss 0.0746822, acc 0.98
2016-09-06T14:21:34.311701: step 3761, loss 0.0126271, acc 1
2016-09-06T14:21:35.108184: step 3762, loss 0.00905234, acc 1
2016-09-06T14:21:35.969968: step 3763, loss 0.0429, acc 1
2016-09-06T14:21:36.795661: step 3764, loss 0.0168586, acc 1
2016-09-06T14:21:37.585976: step 3765, loss 0.0204772, acc 0.98
2016-09-06T14:21:38.432999: step 3766, loss 0.00312297, acc 1
2016-09-06T14:21:39.253452: step 3767, loss 0.0521523, acc 0.98
2016-09-06T14:21:40.076819: step 3768, loss 0.0183247, acc 1
2016-09-06T14:21:40.913793: step 3769, loss 0.0596369, acc 0.96
2016-09-06T14:21:41.741757: step 3770, loss 0.00697972, acc 1
2016-09-06T14:21:42.537570: step 3771, loss 0.0137355, acc 1
2016-09-06T14:21:43.383690: step 3772, loss 0.0624295, acc 0.98
2016-09-06T14:21:44.228279: step 3773, loss 0.00833881, acc 1
2016-09-06T14:21:45.049942: step 3774, loss 0.0239689, acc 1
2016-09-06T14:21:45.902233: step 3775, loss 0.0646385, acc 0.96
2016-09-06T14:21:46.725626: step 3776, loss 0.0871881, acc 0.94
2016-09-06T14:21:47.513618: step 3777, loss 0.107642, acc 0.94
2016-09-06T14:21:48.339379: step 3778, loss 0.0298797, acc 0.98
2016-09-06T14:21:49.147330: step 3779, loss 0.0223489, acc 1
2016-09-06T14:21:49.952469: step 3780, loss 0.0215874, acc 0.98
2016-09-06T14:21:50.769418: step 3781, loss 0.0750479, acc 0.98
2016-09-06T14:21:51.581350: step 3782, loss 0.0148329, acc 1
2016-09-06T14:21:52.368781: step 3783, loss 0.0205037, acc 1
2016-09-06T14:21:53.187702: step 3784, loss 0.0104657, acc 1
2016-09-06T14:21:54.018911: step 3785, loss 0.0172889, acc 0.98
2016-09-06T14:21:54.795736: step 3786, loss 0.0372293, acc 1
2016-09-06T14:21:55.597972: step 3787, loss 0.00445524, acc 1
2016-09-06T14:21:56.405748: step 3788, loss 0.0330095, acc 1
2016-09-06T14:21:57.199727: step 3789, loss 0.034793, acc 1
2016-09-06T14:21:57.992074: step 3790, loss 0.0435318, acc 0.96
2016-09-06T14:21:58.802305: step 3791, loss 0.0124602, acc 1
2016-09-06T14:21:59.591346: step 3792, loss 0.0520288, acc 0.96
2016-09-06T14:22:00.448650: step 3793, loss 0.00781063, acc 1
2016-09-06T14:22:01.284320: step 3794, loss 0.0147812, acc 1
2016-09-06T14:22:02.054139: step 3795, loss 0.0218713, acc 1
2016-09-06T14:22:02.858530: step 3796, loss 0.0866544, acc 0.94
2016-09-06T14:22:03.693576: step 3797, loss 0.0318138, acc 0.98
2016-09-06T14:22:04.499451: step 3798, loss 0.01765, acc 0.98
2016-09-06T14:22:05.315305: step 3799, loss 0.0091273, acc 1
2016-09-06T14:22:06.140213: step 3800, loss 0.00487737, acc 1

Evaluation:
2016-09-06T14:22:09.863737: step 3800, loss 1.89993, acc 0.752345

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-3800

2016-09-06T14:22:11.695170: step 3801, loss 0.0111985, acc 1
2016-09-06T14:22:12.504368: step 3802, loss 0.0931926, acc 0.94
2016-09-06T14:22:13.340962: step 3803, loss 0.203599, acc 0.96
2016-09-06T14:22:14.135740: step 3804, loss 0.0229796, acc 0.98
2016-09-06T14:22:14.943438: step 3805, loss 0.0797535, acc 0.98
2016-09-06T14:22:15.754161: step 3806, loss 0.045208, acc 0.98
2016-09-06T14:22:16.564604: step 3807, loss 0.00484056, acc 1
2016-09-06T14:22:17.387655: step 3808, loss 0.054546, acc 0.96
2016-09-06T14:22:18.228982: step 3809, loss 0.0543549, acc 0.96
2016-09-06T14:22:19.017947: step 3810, loss 0.00434216, acc 1
2016-09-06T14:22:19.839220: step 3811, loss 0.0187532, acc 1
2016-09-06T14:22:20.660156: step 3812, loss 0.258736, acc 0.98
2016-09-06T14:22:21.448930: step 3813, loss 0.0852519, acc 0.98
2016-09-06T14:22:22.219589: step 3814, loss 0.0339857, acc 1
2016-09-06T14:22:23.051504: step 3815, loss 0.0167129, acc 1
2016-09-06T14:22:23.844378: step 3816, loss 0.024163, acc 1
2016-09-06T14:22:24.656915: step 3817, loss 0.0760297, acc 0.96
2016-09-06T14:22:25.450803: step 3818, loss 0.0872789, acc 0.96
2016-09-06T14:22:26.244859: step 3819, loss 0.0669202, acc 0.96
2016-09-06T14:22:27.039010: step 3820, loss 0.0447157, acc 0.98
2016-09-06T14:22:27.842591: step 3821, loss 0.0662893, acc 0.98
2016-09-06T14:22:28.633667: step 3822, loss 0.0500463, acc 0.98
2016-09-06T14:22:29.426721: step 3823, loss 0.231104, acc 0.96
2016-09-06T14:22:30.268354: step 3824, loss 0.0210502, acc 1
2016-09-06T14:22:31.055736: step 3825, loss 0.00849346, acc 1
2016-09-06T14:22:31.856210: step 3826, loss 0.0431858, acc 0.98
2016-09-06T14:22:32.675309: step 3827, loss 0.146133, acc 0.96
2016-09-06T14:22:33.476210: step 3828, loss 0.0798974, acc 0.94
2016-09-06T14:22:34.280890: step 3829, loss 0.054241, acc 0.98
2016-09-06T14:22:35.091315: step 3830, loss 0.0918883, acc 0.98
2016-09-06T14:22:35.857762: step 3831, loss 0.0981639, acc 0.96
2016-09-06T14:22:36.684371: step 3832, loss 0.110233, acc 0.98
2016-09-06T14:22:37.510915: step 3833, loss 0.0101262, acc 1
2016-09-06T14:22:38.280128: step 3834, loss 0.0251445, acc 1
2016-09-06T14:22:39.092534: step 3835, loss 0.0265979, acc 1
2016-09-06T14:22:39.889907: step 3836, loss 0.0623119, acc 0.98
2016-09-06T14:22:40.705431: step 3837, loss 0.0981058, acc 0.94
2016-09-06T14:22:41.512449: step 3838, loss 0.180904, acc 0.94
2016-09-06T14:22:42.318229: step 3839, loss 0.0184761, acc 1
2016-09-06T14:22:43.055982: step 3840, loss 0.0323797, acc 0.977273
2016-09-06T14:22:43.878046: step 3841, loss 0.0502332, acc 0.98
2016-09-06T14:22:44.686966: step 3842, loss 0.0256642, acc 1
2016-09-06T14:22:45.506394: step 3843, loss 0.00922524, acc 1
2016-09-06T14:22:46.305110: step 3844, loss 0.0239139, acc 1
2016-09-06T14:22:47.125891: step 3845, loss 0.0279672, acc 1
2016-09-06T14:22:47.934760: step 3846, loss 0.0379853, acc 0.96
2016-09-06T14:22:48.760427: step 3847, loss 0.0144255, acc 1
2016-09-06T14:22:49.571910: step 3848, loss 0.0818671, acc 0.98
2016-09-06T14:22:50.358861: step 3849, loss 0.0322944, acc 1
2016-09-06T14:22:51.175983: step 3850, loss 0.047988, acc 0.98
2016-09-06T14:22:52.003735: step 3851, loss 0.00711064, acc 1
2016-09-06T14:22:52.781848: step 3852, loss 0.0323335, acc 1
2016-09-06T14:22:53.582904: step 3853, loss 0.015875, acc 1
2016-09-06T14:22:54.394886: step 3854, loss 0.0290336, acc 1
2016-09-06T14:22:55.187121: step 3855, loss 0.0634162, acc 0.98
2016-09-06T14:22:56.038572: step 3856, loss 0.010125, acc 1
2016-09-06T14:22:56.869934: step 3857, loss 0.0343509, acc 0.98
2016-09-06T14:22:57.658009: step 3858, loss 0.00618133, acc 1
2016-09-06T14:22:58.473050: step 3859, loss 0.0131768, acc 1
2016-09-06T14:22:59.283694: step 3860, loss 0.0110394, acc 1
2016-09-06T14:23:00.083742: step 3861, loss 0.0633549, acc 0.98
2016-09-06T14:23:00.940652: step 3862, loss 0.00723212, acc 1
2016-09-06T14:23:01.761198: step 3863, loss 0.0342764, acc 0.98
2016-09-06T14:23:02.550399: step 3864, loss 0.0548794, acc 0.98
2016-09-06T14:23:03.356542: step 3865, loss 0.0125425, acc 1
2016-09-06T14:23:04.172276: step 3866, loss 0.0289198, acc 1
2016-09-06T14:23:04.972071: step 3867, loss 0.0451771, acc 0.98
2016-09-06T14:23:05.786502: step 3868, loss 0.0268617, acc 1
2016-09-06T14:23:06.589075: step 3869, loss 0.0201792, acc 1
2016-09-06T14:23:07.409492: step 3870, loss 0.0185924, acc 1
2016-09-06T14:23:08.226987: step 3871, loss 0.0159862, acc 1
2016-09-06T14:23:09.087583: step 3872, loss 0.0447087, acc 0.96
2016-09-06T14:23:09.894821: step 3873, loss 0.038063, acc 0.98
2016-09-06T14:23:10.711380: step 3874, loss 0.0965205, acc 0.96
2016-09-06T14:23:11.559563: step 3875, loss 0.00641019, acc 1
2016-09-06T14:23:12.383109: step 3876, loss 0.0131465, acc 1
2016-09-06T14:23:13.182313: step 3877, loss 0.0258624, acc 0.98
2016-09-06T14:23:14.015909: step 3878, loss 0.0186841, acc 1
2016-09-06T14:23:14.813681: step 3879, loss 0.0165965, acc 1
2016-09-06T14:23:15.636825: step 3880, loss 0.00680107, acc 1
2016-09-06T14:23:16.465199: step 3881, loss 0.0186247, acc 1
2016-09-06T14:23:17.285450: step 3882, loss 0.0338494, acc 1
2016-09-06T14:23:18.103125: step 3883, loss 0.068788, acc 0.96
2016-09-06T14:23:18.928941: step 3884, loss 0.0307522, acc 1
2016-09-06T14:23:19.749895: step 3885, loss 0.0267387, acc 0.98
2016-09-06T14:23:20.573009: step 3886, loss 0.039415, acc 0.98
2016-09-06T14:23:21.391648: step 3887, loss 0.107307, acc 0.98
2016-09-06T14:23:22.206865: step 3888, loss 0.0799687, acc 0.98
2016-09-06T14:23:23.021784: step 3889, loss 0.0248456, acc 0.98
2016-09-06T14:23:23.827088: step 3890, loss 0.0195819, acc 0.98
2016-09-06T14:23:24.656720: step 3891, loss 0.0612271, acc 0.98
2016-09-06T14:23:25.450235: step 3892, loss 0.00584523, acc 1
2016-09-06T14:23:26.272758: step 3893, loss 0.013957, acc 1
2016-09-06T14:23:27.073427: step 3894, loss 0.0356847, acc 1
2016-09-06T14:23:27.880410: step 3895, loss 0.00787022, acc 1
2016-09-06T14:23:28.690456: step 3896, loss 0.0461925, acc 0.98
2016-09-06T14:23:29.525052: step 3897, loss 0.0337095, acc 1
2016-09-06T14:23:30.318362: step 3898, loss 0.0466246, acc 0.98
2016-09-06T14:23:31.112219: step 3899, loss 0.0392018, acc 0.98
2016-09-06T14:23:31.948440: step 3900, loss 0.0400179, acc 0.98

Evaluation:
2016-09-06T14:23:35.694643: step 3900, loss 1.51678, acc 0.75985

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-3900

2016-09-06T14:23:37.614395: step 3901, loss 0.00580028, acc 1
2016-09-06T14:23:38.432066: step 3902, loss 0.0359142, acc 0.98
2016-09-06T14:23:39.271201: step 3903, loss 0.0259784, acc 0.98
2016-09-06T14:23:40.085570: step 3904, loss 0.0494625, acc 0.98
2016-09-06T14:23:40.881710: step 3905, loss 0.0249973, acc 0.98
2016-09-06T14:23:41.723639: step 3906, loss 0.0746559, acc 0.96
2016-09-06T14:23:42.505324: step 3907, loss 0.0362124, acc 0.98
2016-09-06T14:23:43.330865: step 3908, loss 0.190946, acc 0.94
2016-09-06T14:23:44.141085: step 3909, loss 0.0406962, acc 1
2016-09-06T14:23:44.922797: step 3910, loss 0.0809901, acc 0.96
2016-09-06T14:23:45.743448: step 3911, loss 0.046418, acc 0.98
2016-09-06T14:23:46.560200: step 3912, loss 0.0394772, acc 0.98
2016-09-06T14:23:47.361191: step 3913, loss 0.0694806, acc 0.94
2016-09-06T14:23:48.166249: step 3914, loss 0.0459821, acc 0.98
2016-09-06T14:23:48.971345: step 3915, loss 0.038381, acc 0.98
2016-09-06T14:23:49.779717: step 3916, loss 0.0071112, acc 1
2016-09-06T14:23:50.606787: step 3917, loss 0.037156, acc 1
2016-09-06T14:23:51.416468: step 3918, loss 0.0275276, acc 1
2016-09-06T14:23:52.231862: step 3919, loss 0.0238021, acc 1
2016-09-06T14:23:53.046893: step 3920, loss 0.0102078, acc 1
2016-09-06T14:23:53.875541: step 3921, loss 0.00588625, acc 1
2016-09-06T14:23:54.660252: step 3922, loss 0.0375754, acc 0.98
2016-09-06T14:23:55.465101: step 3923, loss 0.0505157, acc 0.96
2016-09-06T14:23:56.303183: step 3924, loss 0.0259256, acc 1
2016-09-06T14:23:57.112759: step 3925, loss 0.00545415, acc 1
2016-09-06T14:23:57.937527: step 3926, loss 0.0597672, acc 0.96
2016-09-06T14:23:58.799159: step 3927, loss 0.00874977, acc 1
2016-09-06T14:23:59.613482: step 3928, loss 0.0509665, acc 0.96
2016-09-06T14:24:00.487562: step 3929, loss 0.0264223, acc 1
2016-09-06T14:24:01.296865: step 3930, loss 0.00674182, acc 1
2016-09-06T14:24:02.154916: step 3931, loss 0.106834, acc 0.94
2016-09-06T14:24:02.952014: step 3932, loss 0.0117417, acc 1
2016-09-06T14:24:03.747747: step 3933, loss 0.0440577, acc 0.98
2016-09-06T14:24:04.580153: step 3934, loss 0.0129205, acc 1
2016-09-06T14:24:05.379747: step 3935, loss 0.0334792, acc 0.98
2016-09-06T14:24:06.167912: step 3936, loss 0.011765, acc 1
2016-09-06T14:24:07.000823: step 3937, loss 0.00713963, acc 1
2016-09-06T14:24:07.782887: step 3938, loss 0.0943703, acc 0.96
2016-09-06T14:24:08.589924: step 3939, loss 0.0545452, acc 0.98
2016-09-06T14:24:09.421488: step 3940, loss 0.032631, acc 1
2016-09-06T14:24:10.201455: step 3941, loss 0.0754869, acc 0.94
2016-09-06T14:24:10.998248: step 3942, loss 0.0440159, acc 0.98
2016-09-06T14:24:11.815337: step 3943, loss 0.0391618, acc 0.98
2016-09-06T14:24:12.622732: step 3944, loss 0.00801069, acc 1
2016-09-06T14:24:13.403721: step 3945, loss 0.0200171, acc 0.98
2016-09-06T14:24:14.225528: step 3946, loss 0.0135912, acc 1
2016-09-06T14:24:15.029508: step 3947, loss 0.0291076, acc 1
2016-09-06T14:24:15.834876: step 3948, loss 0.0697159, acc 0.96
2016-09-06T14:24:16.657417: step 3949, loss 0.0128118, acc 1
2016-09-06T14:24:17.438628: step 3950, loss 0.0973427, acc 0.98
2016-09-06T14:24:18.252491: step 3951, loss 0.0479437, acc 0.96
2016-09-06T14:24:19.067998: step 3952, loss 0.0340288, acc 0.98
2016-09-06T14:24:19.849632: step 3953, loss 0.0265956, acc 0.98
2016-09-06T14:24:20.668494: step 3954, loss 0.0687921, acc 0.96
2016-09-06T14:24:21.488659: step 3955, loss 0.0403292, acc 0.96
2016-09-06T14:24:22.269451: step 3956, loss 0.0462725, acc 1
2016-09-06T14:24:23.078168: step 3957, loss 0.066916, acc 0.98
2016-09-06T14:24:23.910280: step 3958, loss 0.0109809, acc 1
2016-09-06T14:24:24.722740: step 3959, loss 0.0842558, acc 0.94
2016-09-06T14:24:25.535311: step 3960, loss 0.0230893, acc 1
2016-09-06T14:24:26.360764: step 3961, loss 0.0214095, acc 1
2016-09-06T14:24:27.169387: step 3962, loss 0.0473961, acc 0.98
2016-09-06T14:24:27.994664: step 3963, loss 0.0119767, acc 1
2016-09-06T14:24:28.824177: step 3964, loss 0.00514981, acc 1
2016-09-06T14:24:29.622461: step 3965, loss 0.0526301, acc 0.96
2016-09-06T14:24:30.441396: step 3966, loss 0.0141533, acc 1
2016-09-06T14:24:31.261469: step 3967, loss 0.0292347, acc 0.98
2016-09-06T14:24:32.093272: step 3968, loss 0.0482379, acc 0.96
2016-09-06T14:24:32.972989: step 3969, loss 0.0281368, acc 0.98
2016-09-06T14:24:33.810406: step 3970, loss 0.0066844, acc 1
2016-09-06T14:24:34.628716: step 3971, loss 0.0385335, acc 0.98
2016-09-06T14:24:35.432319: step 3972, loss 0.0168256, acc 1
2016-09-06T14:24:36.261037: step 3973, loss 0.00952806, acc 1
2016-09-06T14:24:37.064486: step 3974, loss 0.0251444, acc 1
2016-09-06T14:24:37.889786: step 3975, loss 0.01919, acc 1
2016-09-06T14:24:38.737229: step 3976, loss 0.00589762, acc 1
2016-09-06T14:24:39.562041: step 3977, loss 0.0409757, acc 0.98
2016-09-06T14:24:40.336156: step 3978, loss 0.0394673, acc 0.98
2016-09-06T14:24:41.164422: step 3979, loss 0.0209526, acc 1
2016-09-06T14:24:41.961501: step 3980, loss 0.00351121, acc 1
2016-09-06T14:24:42.762529: step 3981, loss 0.0227104, acc 1
2016-09-06T14:24:43.576448: step 3982, loss 0.0403593, acc 0.98
2016-09-06T14:24:44.379933: step 3983, loss 0.00740807, acc 1
2016-09-06T14:24:45.202497: step 3984, loss 0.0172853, acc 1
2016-09-06T14:24:46.001667: step 3985, loss 0.00724872, acc 1
2016-09-06T14:24:46.833074: step 3986, loss 0.0316179, acc 1
2016-09-06T14:24:47.643167: step 3987, loss 0.0447537, acc 0.98
2016-09-06T14:24:48.448349: step 3988, loss 0.00397355, acc 1
2016-09-06T14:24:49.273295: step 3989, loss 0.0242455, acc 1
2016-09-06T14:24:50.087970: step 3990, loss 0.0280436, acc 0.98
2016-09-06T14:24:50.877890: step 3991, loss 0.00620997, acc 1
2016-09-06T14:24:51.702286: step 3992, loss 0.0291633, acc 0.98
2016-09-06T14:24:52.497938: step 3993, loss 0.00423226, acc 1
2016-09-06T14:24:53.306772: step 3994, loss 0.00537303, acc 1
2016-09-06T14:24:54.138349: step 3995, loss 0.0165731, acc 1
2016-09-06T14:24:54.925447: step 3996, loss 0.139324, acc 0.92
2016-09-06T14:24:55.727316: step 3997, loss 0.0159726, acc 1
2016-09-06T14:24:56.544047: step 3998, loss 0.0340511, acc 0.98
2016-09-06T14:24:57.333544: step 3999, loss 0.0505778, acc 0.98
2016-09-06T14:24:58.136205: step 4000, loss 0.0446725, acc 0.98

Evaluation:
2016-09-06T14:25:01.875472: step 4000, loss 1.69364, acc 0.755159

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-4000

2016-09-06T14:25:03.849615: step 4001, loss 0.0568059, acc 0.98
2016-09-06T14:25:04.645587: step 4002, loss 0.0147099, acc 1
2016-09-06T14:25:05.466815: step 4003, loss 0.0400415, acc 0.98
2016-09-06T14:25:06.277534: step 4004, loss 0.108333, acc 0.94
2016-09-06T14:25:07.056363: step 4005, loss 0.00840065, acc 1
2016-09-06T14:25:07.867072: step 4006, loss 0.026119, acc 1
2016-09-06T14:25:08.681103: step 4007, loss 0.030275, acc 0.98
2016-09-06T14:25:09.483294: step 4008, loss 0.00482687, acc 1
2016-09-06T14:25:10.287254: step 4009, loss 0.130717, acc 0.94
2016-09-06T14:25:11.123052: step 4010, loss 0.00531114, acc 1
2016-09-06T14:25:11.921215: step 4011, loss 0.0421852, acc 1
2016-09-06T14:25:12.725263: step 4012, loss 0.00879175, acc 1
2016-09-06T14:25:13.544334: step 4013, loss 0.0057357, acc 1
2016-09-06T14:25:14.347712: step 4014, loss 0.0214067, acc 1
2016-09-06T14:25:15.153451: step 4015, loss 0.124915, acc 0.96
2016-09-06T14:25:15.983306: step 4016, loss 0.00391859, acc 1
2016-09-06T14:25:16.801476: step 4017, loss 0.0120016, acc 1
2016-09-06T14:25:17.628147: step 4018, loss 0.0377243, acc 1
2016-09-06T14:25:18.455550: step 4019, loss 0.0308991, acc 0.98
2016-09-06T14:25:19.257431: step 4020, loss 0.0607863, acc 0.98
2016-09-06T14:25:20.078329: step 4021, loss 0.0282765, acc 0.98
2016-09-06T14:25:20.912184: step 4022, loss 0.0404851, acc 1
2016-09-06T14:25:21.749740: step 4023, loss 0.00508082, acc 1
2016-09-06T14:25:22.565783: step 4024, loss 0.02427, acc 1
2016-09-06T14:25:23.415412: step 4025, loss 0.019347, acc 1
2016-09-06T14:25:24.222552: step 4026, loss 0.0190152, acc 1
2016-09-06T14:25:25.026560: step 4027, loss 0.0178994, acc 0.98
2016-09-06T14:25:25.854394: step 4028, loss 0.0184437, acc 1
2016-09-06T14:25:26.677424: step 4029, loss 0.0465405, acc 0.96
2016-09-06T14:25:27.492284: step 4030, loss 0.137747, acc 0.96
2016-09-06T14:25:28.337074: step 4031, loss 0.09329, acc 0.94
2016-09-06T14:25:29.077896: step 4032, loss 0.026604, acc 1
2016-09-06T14:25:29.926719: step 4033, loss 0.0125252, acc 1
2016-09-06T14:25:30.744903: step 4034, loss 0.0191304, acc 1
2016-09-06T14:25:31.624276: step 4035, loss 0.0324846, acc 0.98
2016-09-06T14:25:32.402888: step 4036, loss 0.0412438, acc 0.98
2016-09-06T14:25:33.229455: step 4037, loss 0.0337907, acc 0.98
2016-09-06T14:25:34.049707: step 4038, loss 0.0494156, acc 0.96
2016-09-06T14:25:34.850329: step 4039, loss 0.0343227, acc 0.96
2016-09-06T14:25:35.631601: step 4040, loss 0.0167939, acc 1
2016-09-06T14:25:36.590248: step 4041, loss 0.0917989, acc 0.98
2016-09-06T14:25:37.413270: step 4042, loss 0.0332574, acc 0.98
2016-09-06T14:25:38.238523: step 4043, loss 0.0276619, acc 0.98
2016-09-06T14:25:39.093710: step 4044, loss 0.00656, acc 1
2016-09-06T14:25:39.904289: step 4045, loss 0.101161, acc 0.94
2016-09-06T14:25:40.713602: step 4046, loss 0.035137, acc 1
2016-09-06T14:25:41.606333: step 4047, loss 0.0412994, acc 0.96
2016-09-06T14:25:42.410150: step 4048, loss 0.0561081, acc 0.98
2016-09-06T14:25:43.219246: step 4049, loss 0.0365325, acc 0.98
2016-09-06T14:25:44.067483: step 4050, loss 0.0134342, acc 1
2016-09-06T14:25:44.891596: step 4051, loss 0.0426831, acc 0.98
2016-09-06T14:25:45.673797: step 4052, loss 0.0115049, acc 1
2016-09-06T14:25:46.493283: step 4053, loss 0.0169511, acc 1
2016-09-06T14:25:47.310019: step 4054, loss 0.0198938, acc 1
2016-09-06T14:25:48.093453: step 4055, loss 0.00445677, acc 1
2016-09-06T14:25:48.901407: step 4056, loss 0.0844602, acc 0.96
2016-09-06T14:25:49.755384: step 4057, loss 0.0156108, acc 1
2016-09-06T14:25:50.548208: step 4058, loss 0.0301092, acc 1
2016-09-06T14:25:51.336823: step 4059, loss 0.00930699, acc 1
2016-09-06T14:25:52.143257: step 4060, loss 0.0125763, acc 1
2016-09-06T14:25:52.952050: step 4061, loss 0.0123259, acc 1
2016-09-06T14:25:53.749602: step 4062, loss 0.0191769, acc 1
2016-09-06T14:25:54.591777: step 4063, loss 0.0659084, acc 0.98
2016-09-06T14:25:55.373460: step 4064, loss 0.0123755, acc 1
2016-09-06T14:25:56.173333: step 4065, loss 0.00444204, acc 1
2016-09-06T14:25:56.994216: step 4066, loss 0.188534, acc 0.98
2016-09-06T14:25:57.777537: step 4067, loss 0.00559186, acc 1
2016-09-06T14:25:58.578332: step 4068, loss 0.00546414, acc 1
2016-09-06T14:25:59.412555: step 4069, loss 0.0193321, acc 1
2016-09-06T14:26:00.194289: step 4070, loss 0.00374031, acc 1
2016-09-06T14:26:01.004793: step 4071, loss 0.0369148, acc 0.98
2016-09-06T14:26:01.842741: step 4072, loss 0.0277285, acc 0.98
2016-09-06T14:26:02.677673: step 4073, loss 0.0367629, acc 1
2016-09-06T14:26:03.472247: step 4074, loss 0.0185888, acc 1
2016-09-06T14:26:04.283603: step 4075, loss 0.0350973, acc 1
2016-09-06T14:26:05.071905: step 4076, loss 0.0480109, acc 0.98
2016-09-06T14:26:05.886732: step 4077, loss 0.146874, acc 0.98
2016-09-06T14:26:06.761753: step 4078, loss 0.00956562, acc 1
2016-09-06T14:26:07.582473: step 4079, loss 0.147784, acc 0.92
2016-09-06T14:26:08.395717: step 4080, loss 0.0617376, acc 0.98
2016-09-06T14:26:09.224830: step 4081, loss 0.0204058, acc 1
2016-09-06T14:26:10.054855: step 4082, loss 0.0283444, acc 1
2016-09-06T14:26:10.870006: step 4083, loss 0.0539882, acc 0.98
2016-09-06T14:26:11.721248: step 4084, loss 0.0402654, acc 0.98
2016-09-06T14:26:12.520154: step 4085, loss 0.00975519, acc 1
2016-09-06T14:26:13.320943: step 4086, loss 0.0433133, acc 0.98
2016-09-06T14:26:14.141207: step 4087, loss 0.0714054, acc 0.96
2016-09-06T14:26:14.941369: step 4088, loss 0.0174719, acc 1
2016-09-06T14:26:15.782200: step 4089, loss 0.0502918, acc 0.98
2016-09-06T14:26:16.602813: step 4090, loss 0.0448657, acc 0.98
2016-09-06T14:26:17.434998: step 4091, loss 0.0480791, acc 0.98
2016-09-06T14:26:18.251175: step 4092, loss 0.0147713, acc 1
2016-09-06T14:26:19.075915: step 4093, loss 0.00575641, acc 1
2016-09-06T14:26:19.955526: step 4094, loss 0.0265664, acc 1
2016-09-06T14:26:20.737694: step 4095, loss 0.02262, acc 0.98
2016-09-06T14:26:21.568212: step 4096, loss 0.0855046, acc 0.94
2016-09-06T14:26:22.397359: step 4097, loss 0.0498035, acc 0.98
2016-09-06T14:26:23.196421: step 4098, loss 0.0242092, acc 0.98
2016-09-06T14:26:23.998647: step 4099, loss 0.0165874, acc 1
2016-09-06T14:26:24.847104: step 4100, loss 0.0885288, acc 0.96

Evaluation:
2016-09-06T14:26:28.601671: step 4100, loss 1.28063, acc 0.766416

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-4100

2016-09-06T14:26:30.471347: step 4101, loss 0.0273644, acc 0.98
2016-09-06T14:26:31.289668: step 4102, loss 0.0404432, acc 0.98
2016-09-06T14:26:32.108137: step 4103, loss 0.0359426, acc 0.98
2016-09-06T14:26:32.935413: step 4104, loss 0.0245991, acc 0.98
2016-09-06T14:26:33.767138: step 4105, loss 0.0282294, acc 0.98
2016-09-06T14:26:34.587812: step 4106, loss 0.0136015, acc 1
2016-09-06T14:26:35.407761: step 4107, loss 0.0306336, acc 1
2016-09-06T14:26:36.235229: step 4108, loss 0.0359075, acc 0.96
2016-09-06T14:26:37.085219: step 4109, loss 0.0182772, acc 1
2016-09-06T14:26:37.877056: step 4110, loss 0.00392525, acc 1
2016-09-06T14:26:38.699432: step 4111, loss 0.076924, acc 0.96
2016-09-06T14:26:39.521159: step 4112, loss 0.0277593, acc 1
2016-09-06T14:26:40.324848: step 4113, loss 0.0257759, acc 1
2016-09-06T14:26:41.134492: step 4114, loss 0.00756284, acc 1
2016-09-06T14:26:41.974494: step 4115, loss 0.0303375, acc 0.98
2016-09-06T14:26:42.779992: step 4116, loss 0.00598972, acc 1
2016-09-06T14:26:43.583545: step 4117, loss 0.00444517, acc 1
2016-09-06T14:26:44.410414: step 4118, loss 0.0656686, acc 0.98
2016-09-06T14:26:45.269546: step 4119, loss 0.00455807, acc 1
2016-09-06T14:26:46.068652: step 4120, loss 0.088636, acc 0.96
2016-09-06T14:26:46.902244: step 4121, loss 0.0310025, acc 1
2016-09-06T14:26:47.763863: step 4122, loss 0.0265232, acc 0.98
2016-09-06T14:26:48.569442: step 4123, loss 0.0155446, acc 1
2016-09-06T14:26:49.368276: step 4124, loss 0.0037536, acc 1
2016-09-06T14:26:50.206746: step 4125, loss 0.0123425, acc 1
2016-09-06T14:26:50.997487: step 4126, loss 0.0259017, acc 0.98
2016-09-06T14:26:51.810166: step 4127, loss 0.0200371, acc 1
2016-09-06T14:26:52.629593: step 4128, loss 0.00838271, acc 1
2016-09-06T14:26:53.422100: step 4129, loss 0.0401897, acc 0.96
2016-09-06T14:26:54.224408: step 4130, loss 0.0194666, acc 1
2016-09-06T14:26:55.034765: step 4131, loss 0.0311713, acc 0.98
2016-09-06T14:26:55.809135: step 4132, loss 0.0131441, acc 1
2016-09-06T14:26:56.609314: step 4133, loss 0.00843643, acc 1
2016-09-06T14:26:57.422088: step 4134, loss 0.0186028, acc 0.98
2016-09-06T14:26:58.226596: step 4135, loss 0.0696336, acc 0.98
2016-09-06T14:26:59.045359: step 4136, loss 0.00731968, acc 1
2016-09-06T14:26:59.880648: step 4137, loss 0.0572684, acc 0.98
2016-09-06T14:27:00.702773: step 4138, loss 0.00337574, acc 1
2016-09-06T14:27:01.505863: step 4139, loss 0.0483892, acc 0.96
2016-09-06T14:27:02.355725: step 4140, loss 0.0287694, acc 0.98
2016-09-06T14:27:03.160586: step 4141, loss 0.0578959, acc 0.96
2016-09-06T14:27:03.953506: step 4142, loss 0.0232809, acc 1
2016-09-06T14:27:04.796240: step 4143, loss 0.014853, acc 1
2016-09-06T14:27:05.625994: step 4144, loss 0.0248194, acc 1
2016-09-06T14:27:06.496954: step 4145, loss 0.0113998, acc 1
2016-09-06T14:27:07.343179: step 4146, loss 0.0761673, acc 0.96
2016-09-06T14:27:08.147595: step 4147, loss 0.0175736, acc 1
2016-09-06T14:27:08.941321: step 4148, loss 0.00548782, acc 1
2016-09-06T14:27:09.751642: step 4149, loss 0.0284567, acc 1
2016-09-06T14:27:10.580402: step 4150, loss 0.0108514, acc 1
2016-09-06T14:27:11.388224: step 4151, loss 0.0254206, acc 1
2016-09-06T14:27:12.233402: step 4152, loss 0.0185659, acc 1
2016-09-06T14:27:13.056526: step 4153, loss 0.00361725, acc 1
2016-09-06T14:27:13.876197: step 4154, loss 0.0552405, acc 0.98
2016-09-06T14:27:14.685642: step 4155, loss 0.00527089, acc 1
2016-09-06T14:27:15.492077: step 4156, loss 0.0186655, acc 1
2016-09-06T14:27:16.318959: step 4157, loss 0.078975, acc 0.94
2016-09-06T14:27:17.113805: step 4158, loss 0.13275, acc 0.98
2016-09-06T14:27:17.937870: step 4159, loss 0.041438, acc 0.98
2016-09-06T14:27:18.739171: step 4160, loss 0.0172029, acc 1
2016-09-06T14:27:19.548170: step 4161, loss 0.092902, acc 0.96
2016-09-06T14:27:20.358826: step 4162, loss 0.0493759, acc 0.96
2016-09-06T14:27:21.151171: step 4163, loss 0.0240681, acc 0.98
2016-09-06T14:27:21.972899: step 4164, loss 0.0101609, acc 1
2016-09-06T14:27:22.780074: step 4165, loss 0.00395085, acc 1
2016-09-06T14:27:23.572504: step 4166, loss 0.0159333, acc 1
2016-09-06T14:27:24.367521: step 4167, loss 0.117915, acc 0.94
2016-09-06T14:27:25.198419: step 4168, loss 0.00661848, acc 1
2016-09-06T14:27:25.989858: step 4169, loss 0.0158314, acc 1
2016-09-06T14:27:26.790505: step 4170, loss 0.0380205, acc 0.98
2016-09-06T14:27:27.641441: step 4171, loss 0.0181228, acc 1
2016-09-06T14:27:28.453381: step 4172, loss 0.0425543, acc 0.98
2016-09-06T14:27:29.262144: step 4173, loss 0.0263912, acc 1
2016-09-06T14:27:30.075714: step 4174, loss 0.0240119, acc 0.98
2016-09-06T14:27:30.855579: step 4175, loss 0.0178276, acc 1
2016-09-06T14:27:31.653694: step 4176, loss 0.0388212, acc 0.98
2016-09-06T14:27:32.466204: step 4177, loss 0.0371698, acc 0.98
2016-09-06T14:27:33.281387: step 4178, loss 0.039026, acc 0.98
2016-09-06T14:27:34.084135: step 4179, loss 0.149546, acc 0.96
2016-09-06T14:27:34.883538: step 4180, loss 0.0725423, acc 0.98
2016-09-06T14:27:35.672202: step 4181, loss 0.0217476, acc 0.98
2016-09-06T14:27:36.483838: step 4182, loss 0.0267921, acc 0.98
2016-09-06T14:27:37.308737: step 4183, loss 0.00428682, acc 1
2016-09-06T14:27:38.110775: step 4184, loss 0.0134131, acc 1
2016-09-06T14:27:38.902641: step 4185, loss 0.0734141, acc 0.98
2016-09-06T14:27:39.727662: step 4186, loss 0.0090033, acc 1
2016-09-06T14:27:40.490407: step 4187, loss 0.0109831, acc 1
2016-09-06T14:27:41.303481: step 4188, loss 0.0388112, acc 0.98
2016-09-06T14:27:42.122233: step 4189, loss 0.025307, acc 1
2016-09-06T14:27:42.911493: step 4190, loss 0.0306337, acc 1
2016-09-06T14:27:43.717770: step 4191, loss 0.0501654, acc 0.96
2016-09-06T14:27:44.532111: step 4192, loss 0.0482614, acc 0.98
2016-09-06T14:27:45.338328: step 4193, loss 0.0919812, acc 0.98
2016-09-06T14:27:46.185082: step 4194, loss 0.0354609, acc 1
2016-09-06T14:27:46.988999: step 4195, loss 0.040202, acc 0.96
2016-09-06T14:27:47.767484: step 4196, loss 0.00742892, acc 1
2016-09-06T14:27:48.566370: step 4197, loss 0.00446795, acc 1
2016-09-06T14:27:49.398805: step 4198, loss 0.0231856, acc 0.98
2016-09-06T14:27:50.192784: step 4199, loss 0.00379385, acc 1
2016-09-06T14:27:50.979419: step 4200, loss 0.011917, acc 1

Evaluation:
2016-09-06T14:27:54.769366: step 4200, loss 1.38912, acc 0.762664

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-4200

2016-09-06T14:27:56.704408: step 4201, loss 0.0922156, acc 0.96
2016-09-06T14:27:57.501331: step 4202, loss 0.064921, acc 0.96
2016-09-06T14:27:58.305543: step 4203, loss 0.0204209, acc 0.98
2016-09-06T14:27:59.106599: step 4204, loss 0.0165468, acc 1
2016-09-06T14:27:59.900617: step 4205, loss 0.00392059, acc 1
2016-09-06T14:28:00.686959: step 4206, loss 0.0357638, acc 0.98
2016-09-06T14:28:01.526659: step 4207, loss 0.0686493, acc 0.96
2016-09-06T14:28:02.339828: step 4208, loss 0.0171975, acc 1
2016-09-06T14:28:03.246500: step 4209, loss 0.0109126, acc 1
2016-09-06T14:28:04.045909: step 4210, loss 0.0086705, acc 1
2016-09-06T14:28:04.864250: step 4211, loss 0.0442045, acc 0.96
2016-09-06T14:28:05.674582: step 4212, loss 0.0969568, acc 0.96
2016-09-06T14:28:06.491851: step 4213, loss 0.0358495, acc 1
2016-09-06T14:28:07.295484: step 4214, loss 0.0235778, acc 1
2016-09-06T14:28:08.101926: step 4215, loss 0.0321529, acc 0.98
2016-09-06T14:28:08.921446: step 4216, loss 0.0533394, acc 0.96
2016-09-06T14:28:09.713605: step 4217, loss 0.00598467, acc 1
2016-09-06T14:28:10.518043: step 4218, loss 0.0373709, acc 0.96
2016-09-06T14:28:11.330724: step 4219, loss 0.0137308, acc 1
2016-09-06T14:28:12.134959: step 4220, loss 0.00479043, acc 1
2016-09-06T14:28:12.949939: step 4221, loss 0.062637, acc 0.96
2016-09-06T14:28:13.774129: step 4222, loss 0.00822302, acc 1
2016-09-06T14:28:14.611730: step 4223, loss 0.0286447, acc 0.98
2016-09-06T14:28:15.361247: step 4224, loss 0.0109218, acc 1
2016-09-06T14:28:16.182697: step 4225, loss 0.00702707, acc 1
2016-09-06T14:28:16.966040: step 4226, loss 0.0201696, acc 1
2016-09-06T14:28:17.764350: step 4227, loss 0.0156942, acc 1
2016-09-06T14:28:18.585772: step 4228, loss 0.0432051, acc 0.98
2016-09-06T14:28:19.360546: step 4229, loss 0.0405964, acc 0.98
2016-09-06T14:28:20.175233: step 4230, loss 0.00781709, acc 1
2016-09-06T14:28:20.999760: step 4231, loss 0.0367246, acc 0.96
2016-09-06T14:28:21.775843: step 4232, loss 0.0212576, acc 1
2016-09-06T14:28:22.599553: step 4233, loss 0.0227855, acc 1
2016-09-06T14:28:23.421396: step 4234, loss 0.00740785, acc 1
2016-09-06T14:28:24.237425: step 4235, loss 0.00326421, acc 1
2016-09-06T14:28:25.058826: step 4236, loss 0.005102, acc 1
2016-09-06T14:28:25.877957: step 4237, loss 0.129878, acc 0.96
2016-09-06T14:28:26.681410: step 4238, loss 0.0099766, acc 1
2016-09-06T14:28:27.489420: step 4239, loss 0.0287962, acc 1
2016-09-06T14:28:28.304239: step 4240, loss 0.0114137, acc 1
2016-09-06T14:28:29.086192: step 4241, loss 0.0316543, acc 0.98
2016-09-06T14:28:29.904703: step 4242, loss 0.0059065, acc 1
2016-09-06T14:28:30.730837: step 4243, loss 0.0313193, acc 0.98
2016-09-06T14:28:31.502783: step 4244, loss 0.0152937, acc 1
2016-09-06T14:28:32.277875: step 4245, loss 0.00933215, acc 1
2016-09-06T14:28:33.098979: step 4246, loss 0.0357899, acc 0.98
2016-09-06T14:28:33.869217: step 4247, loss 0.00306725, acc 1
2016-09-06T14:28:34.673595: step 4248, loss 0.0514407, acc 0.98
2016-09-06T14:28:35.487478: step 4249, loss 0.0065121, acc 1
2016-09-06T14:28:36.262436: step 4250, loss 0.0339137, acc 1
2016-09-06T14:28:37.082376: step 4251, loss 0.0105131, acc 1
2016-09-06T14:28:37.924208: step 4252, loss 0.0305279, acc 0.98
2016-09-06T14:28:38.740324: step 4253, loss 0.00798217, acc 1
2016-09-06T14:28:39.543932: step 4254, loss 0.0221904, acc 0.98
2016-09-06T14:28:40.359490: step 4255, loss 0.00523585, acc 1
2016-09-06T14:28:41.177870: step 4256, loss 0.0132692, acc 1
2016-09-06T14:28:41.957712: step 4257, loss 0.0132889, acc 1
2016-09-06T14:28:42.788258: step 4258, loss 0.0156486, acc 1
2016-09-06T14:28:43.558226: step 4259, loss 0.00334243, acc 1
2016-09-06T14:28:44.366897: step 4260, loss 0.0226086, acc 1
2016-09-06T14:28:45.170638: step 4261, loss 0.00880241, acc 1
2016-09-06T14:28:45.960236: step 4262, loss 0.0457748, acc 0.96
2016-09-06T14:28:46.764401: step 4263, loss 0.0029679, acc 1
2016-09-06T14:28:47.621862: step 4264, loss 0.0029534, acc 1
2016-09-06T14:28:48.435958: step 4265, loss 0.029693, acc 0.98
2016-09-06T14:28:49.226257: step 4266, loss 0.015445, acc 1
2016-09-06T14:28:50.005110: step 4267, loss 0.0178075, acc 1
2016-09-06T14:28:50.807233: step 4268, loss 0.0037382, acc 1
2016-09-06T14:28:51.616396: step 4269, loss 0.0271573, acc 0.98
2016-09-06T14:28:52.451566: step 4270, loss 0.00317119, acc 1
2016-09-06T14:28:53.224316: step 4271, loss 0.0184305, acc 0.98
2016-09-06T14:28:54.014412: step 4272, loss 0.0128824, acc 1
2016-09-06T14:28:54.839614: step 4273, loss 0.0234915, acc 1
2016-09-06T14:28:55.654944: step 4274, loss 0.0536577, acc 0.98
2016-09-06T14:28:56.457472: step 4275, loss 0.0169599, acc 1
2016-09-06T14:28:57.261644: step 4276, loss 0.15585, acc 0.98
2016-09-06T14:28:58.060345: step 4277, loss 0.00310605, acc 1
2016-09-06T14:28:58.867548: step 4278, loss 0.0309494, acc 0.98
2016-09-06T14:28:59.702927: step 4279, loss 0.0391863, acc 0.98
2016-09-06T14:29:00.515368: step 4280, loss 0.0182696, acc 1
2016-09-06T14:29:01.313240: step 4281, loss 0.0311537, acc 1
2016-09-06T14:29:02.131299: step 4282, loss 0.0176114, acc 0.98
2016-09-06T14:29:02.934068: step 4283, loss 0.0186811, acc 1
2016-09-06T14:29:03.761766: step 4284, loss 0.0373406, acc 0.96
2016-09-06T14:29:04.559167: step 4285, loss 0.00268574, acc 1
2016-09-06T14:29:05.372126: step 4286, loss 0.0277394, acc 0.98
2016-09-06T14:29:06.177754: step 4287, loss 0.0420709, acc 0.98
2016-09-06T14:29:07.011967: step 4288, loss 0.0909797, acc 0.94
2016-09-06T14:29:07.841364: step 4289, loss 0.0302842, acc 0.98
2016-09-06T14:29:08.668536: step 4290, loss 0.0534574, acc 0.98
2016-09-06T14:29:09.480420: step 4291, loss 0.016048, acc 1
2016-09-06T14:29:10.282194: step 4292, loss 0.0871576, acc 0.94
2016-09-06T14:29:11.099215: step 4293, loss 0.0116657, acc 1
2016-09-06T14:29:11.926974: step 4294, loss 0.0308455, acc 0.98
2016-09-06T14:29:12.727406: step 4295, loss 0.0125182, acc 1
2016-09-06T14:29:13.538763: step 4296, loss 0.0171366, acc 1
2016-09-06T14:29:14.384774: step 4297, loss 0.0479686, acc 0.96
2016-09-06T14:29:15.162776: step 4298, loss 0.00528355, acc 1
2016-09-06T14:29:15.991827: step 4299, loss 0.013528, acc 1
2016-09-06T14:29:16.833002: step 4300, loss 0.124254, acc 0.94

Evaluation:
2016-09-06T14:29:20.543951: step 4300, loss 1.54539, acc 0.749531

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-4300

2016-09-06T14:29:22.538208: step 4301, loss 0.130426, acc 0.96
2016-09-06T14:29:23.343383: step 4302, loss 0.0544838, acc 0.96
2016-09-06T14:29:24.184781: step 4303, loss 0.0327696, acc 0.98
2016-09-06T14:29:25.005214: step 4304, loss 0.0104141, acc 1
2016-09-06T14:29:25.823754: step 4305, loss 0.00693482, acc 1
2016-09-06T14:29:26.644790: step 4306, loss 0.00536966, acc 1
2016-09-06T14:29:27.468515: step 4307, loss 0.0477743, acc 0.98
2016-09-06T14:29:28.263282: step 4308, loss 0.0922202, acc 0.96
2016-09-06T14:29:29.091139: step 4309, loss 0.0305942, acc 1
2016-09-06T14:29:29.909116: step 4310, loss 0.0365711, acc 0.98
2016-09-06T14:29:30.721131: step 4311, loss 0.00834975, acc 1
2016-09-06T14:29:31.581514: step 4312, loss 0.0343296, acc 1
2016-09-06T14:29:32.453657: step 4313, loss 0.0861413, acc 0.96
2016-09-06T14:29:33.266720: step 4314, loss 0.0255951, acc 1
2016-09-06T14:29:34.091928: step 4315, loss 0.0393203, acc 1
2016-09-06T14:29:34.940386: step 4316, loss 0.0283147, acc 0.98
2016-09-06T14:29:35.764834: step 4317, loss 0.0498169, acc 1
2016-09-06T14:29:36.611856: step 4318, loss 0.0216622, acc 1
2016-09-06T14:29:37.426544: step 4319, loss 0.0199318, acc 0.98
2016-09-06T14:29:38.225690: step 4320, loss 0.0201327, acc 0.98
2016-09-06T14:29:39.060776: step 4321, loss 0.0161975, acc 1
2016-09-06T14:29:39.890787: step 4322, loss 0.05379, acc 0.96
2016-09-06T14:29:40.713401: step 4323, loss 0.0203635, acc 0.98
2016-09-06T14:29:41.538249: step 4324, loss 0.0360122, acc 0.98
2016-09-06T14:29:42.367486: step 4325, loss 0.0119465, acc 1
2016-09-06T14:29:43.206619: step 4326, loss 0.0207849, acc 1
2016-09-06T14:29:44.013885: step 4327, loss 0.0434214, acc 0.98
2016-09-06T14:29:44.814872: step 4328, loss 0.00702989, acc 1
2016-09-06T14:29:45.645838: step 4329, loss 0.0192922, acc 1
2016-09-06T14:29:46.442861: step 4330, loss 0.0108116, acc 1
2016-09-06T14:29:47.234246: step 4331, loss 0.0510657, acc 0.96
2016-09-06T14:29:48.064240: step 4332, loss 0.0877538, acc 0.98
2016-09-06T14:29:48.826340: step 4333, loss 0.0378953, acc 0.96
2016-09-06T14:29:49.691796: step 4334, loss 0.0790065, acc 0.96
2016-09-06T14:29:50.507416: step 4335, loss 0.0040746, acc 1
2016-09-06T14:29:51.292380: step 4336, loss 0.0213219, acc 1
2016-09-06T14:29:52.104292: step 4337, loss 0.0413784, acc 0.98
2016-09-06T14:29:52.922639: step 4338, loss 0.0919816, acc 0.98
2016-09-06T14:29:53.697402: step 4339, loss 0.0208074, acc 1
2016-09-06T14:29:54.475830: step 4340, loss 0.0235689, acc 0.98
2016-09-06T14:29:55.335365: step 4341, loss 0.072951, acc 0.96
2016-09-06T14:29:56.162852: step 4342, loss 0.0854061, acc 0.98
2016-09-06T14:29:56.973951: step 4343, loss 0.0349569, acc 0.98
2016-09-06T14:29:57.811995: step 4344, loss 0.0161596, acc 1
2016-09-06T14:29:58.649408: step 4345, loss 0.00387276, acc 1
2016-09-06T14:29:59.471067: step 4346, loss 0.0423869, acc 0.98
2016-09-06T14:30:00.315400: step 4347, loss 0.00532383, acc 1
2016-09-06T14:30:01.117688: step 4348, loss 0.0233513, acc 1
2016-09-06T14:30:01.916488: step 4349, loss 0.00823042, acc 1
2016-09-06T14:30:02.758006: step 4350, loss 0.00600016, acc 1
2016-09-06T14:30:03.585215: step 4351, loss 0.024654, acc 0.98
2016-09-06T14:30:04.383362: step 4352, loss 0.0110926, acc 1
2016-09-06T14:30:05.216899: step 4353, loss 0.0251126, acc 0.98
2016-09-06T14:30:06.046908: step 4354, loss 0.0299771, acc 1
2016-09-06T14:30:06.897412: step 4355, loss 0.0478654, acc 0.98
2016-09-06T14:30:07.719674: step 4356, loss 0.00817752, acc 1
2016-09-06T14:30:08.525749: step 4357, loss 0.0310285, acc 1
2016-09-06T14:30:09.319839: step 4358, loss 0.0295724, acc 0.98
2016-09-06T14:30:10.143045: step 4359, loss 0.0558676, acc 0.96
2016-09-06T14:30:10.910342: step 4360, loss 0.0220975, acc 0.98
2016-09-06T14:30:11.703787: step 4361, loss 0.013444, acc 1
2016-09-06T14:30:12.549652: step 4362, loss 0.0511661, acc 0.98
2016-09-06T14:30:13.357898: step 4363, loss 0.0159867, acc 1
2016-09-06T14:30:14.156030: step 4364, loss 0.0104178, acc 1
2016-09-06T14:30:14.970530: step 4365, loss 0.14248, acc 0.94
2016-09-06T14:30:15.780674: step 4366, loss 0.0447247, acc 0.98
2016-09-06T14:30:16.600433: step 4367, loss 0.00426968, acc 1
2016-09-06T14:30:17.420211: step 4368, loss 0.0213426, acc 1
2016-09-06T14:30:18.234326: step 4369, loss 0.0171864, acc 1
2016-09-06T14:30:19.011477: step 4370, loss 0.070171, acc 0.98
2016-09-06T14:30:19.838458: step 4371, loss 0.0873101, acc 0.96
2016-09-06T14:30:20.642467: step 4372, loss 0.0726927, acc 0.96
2016-09-06T14:30:21.441224: step 4373, loss 0.0104066, acc 1
2016-09-06T14:30:22.240441: step 4374, loss 0.0597588, acc 0.96
2016-09-06T14:30:23.069034: step 4375, loss 0.0408647, acc 0.98
2016-09-06T14:30:23.868388: step 4376, loss 0.00467807, acc 1
2016-09-06T14:30:24.657663: step 4377, loss 0.0399375, acc 1
2016-09-06T14:30:25.496857: step 4378, loss 0.023743, acc 1
2016-09-06T14:30:26.289975: step 4379, loss 0.0293818, acc 1
2016-09-06T14:30:27.096578: step 4380, loss 0.0515518, acc 0.96
2016-09-06T14:30:27.939469: step 4381, loss 0.0224149, acc 1
2016-09-06T14:30:28.729579: step 4382, loss 0.0460371, acc 0.98
2016-09-06T14:30:29.522938: step 4383, loss 0.006972, acc 1
2016-09-06T14:30:30.344873: step 4384, loss 0.0676923, acc 0.98
2016-09-06T14:30:31.140493: step 4385, loss 0.0149879, acc 1
2016-09-06T14:30:31.939273: step 4386, loss 0.0395341, acc 1
2016-09-06T14:30:32.754911: step 4387, loss 0.00455591, acc 1
2016-09-06T14:30:33.551354: step 4388, loss 0.0393816, acc 0.98
2016-09-06T14:30:34.337720: step 4389, loss 0.0175763, acc 1
2016-09-06T14:30:35.121357: step 4390, loss 0.0363631, acc 0.98
2016-09-06T14:30:35.915785: step 4391, loss 0.0267463, acc 0.98
2016-09-06T14:30:36.757481: step 4392, loss 0.048651, acc 0.96
2016-09-06T14:30:37.582320: step 4393, loss 0.0332886, acc 1
2016-09-06T14:30:38.409405: step 4394, loss 0.0137616, acc 1
2016-09-06T14:30:39.229887: step 4395, loss 0.0366214, acc 1
2016-09-06T14:30:40.030482: step 4396, loss 0.035018, acc 0.98
2016-09-06T14:30:40.825191: step 4397, loss 0.0179729, acc 0.98
2016-09-06T14:30:41.624593: step 4398, loss 0.0242066, acc 1
2016-09-06T14:30:42.433433: step 4399, loss 0.0541392, acc 0.96
2016-09-06T14:30:43.234646: step 4400, loss 0.00424967, acc 1

Evaluation:
2016-09-06T14:30:46.915022: step 4400, loss 1.71334, acc 0.754221

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-4400

2016-09-06T14:30:48.747233: step 4401, loss 0.0658763, acc 0.96
2016-09-06T14:30:49.568471: step 4402, loss 0.0481619, acc 0.96
2016-09-06T14:30:50.374118: step 4403, loss 0.0453829, acc 0.98
2016-09-06T14:30:51.215442: step 4404, loss 0.00818354, acc 1
2016-09-06T14:30:52.046020: step 4405, loss 0.0186993, acc 1
2016-09-06T14:30:52.842735: step 4406, loss 0.0803987, acc 0.94
2016-09-06T14:30:53.680187: step 4407, loss 0.0049181, acc 1
2016-09-06T14:30:54.495532: step 4408, loss 0.0189935, acc 0.98
2016-09-06T14:30:55.289089: step 4409, loss 0.0148489, acc 1
2016-09-06T14:30:56.102396: step 4410, loss 0.0242079, acc 1
2016-09-06T14:30:56.904562: step 4411, loss 0.0312307, acc 0.98
2016-09-06T14:30:57.714275: step 4412, loss 0.0116383, acc 1
2016-09-06T14:30:58.539953: step 4413, loss 0.0392842, acc 0.98
2016-09-06T14:30:59.368327: step 4414, loss 0.0185933, acc 1
2016-09-06T14:31:00.174142: step 4415, loss 0.0198362, acc 0.98
2016-09-06T14:31:00.967149: step 4416, loss 0.0254342, acc 0.977273
2016-09-06T14:31:01.793402: step 4417, loss 0.0322169, acc 0.98
2016-09-06T14:31:02.572599: step 4418, loss 0.104834, acc 0.98
2016-09-06T14:31:03.394167: step 4419, loss 0.0242049, acc 1
2016-09-06T14:31:04.226635: step 4420, loss 0.168379, acc 0.96
2016-09-06T14:31:05.010405: step 4421, loss 0.0358875, acc 0.98
2016-09-06T14:31:05.829450: step 4422, loss 0.0284993, acc 1
2016-09-06T14:31:06.644824: step 4423, loss 0.0287323, acc 1
2016-09-06T14:31:07.441079: step 4424, loss 0.0035342, acc 1
2016-09-06T14:31:08.246543: step 4425, loss 0.0355291, acc 1
2016-09-06T14:31:09.045010: step 4426, loss 0.0394789, acc 0.98
2016-09-06T14:31:09.818300: step 4427, loss 0.00760083, acc 1
2016-09-06T14:31:10.621602: step 4428, loss 0.00529439, acc 1
2016-09-06T14:31:11.440306: step 4429, loss 0.00686796, acc 1
2016-09-06T14:31:12.248084: step 4430, loss 0.0246721, acc 0.98
2016-09-06T14:31:13.065502: step 4431, loss 0.0140835, acc 1
2016-09-06T14:31:13.886140: step 4432, loss 0.0128358, acc 1
2016-09-06T14:31:14.697767: step 4433, loss 0.0840233, acc 0.98
2016-09-06T14:31:15.497241: step 4434, loss 0.0030635, acc 1
2016-09-06T14:31:16.304286: step 4435, loss 0.0629578, acc 0.96
2016-09-06T14:31:17.071119: step 4436, loss 0.00279779, acc 1
2016-09-06T14:31:17.898457: step 4437, loss 0.0049373, acc 1
2016-09-06T14:31:18.690740: step 4438, loss 0.00391389, acc 1
2016-09-06T14:31:19.480340: step 4439, loss 0.0480888, acc 0.98
2016-09-06T14:31:20.269096: step 4440, loss 0.115147, acc 0.94
2016-09-06T14:31:21.099671: step 4441, loss 0.0154576, acc 1
2016-09-06T14:31:21.903285: step 4442, loss 0.0625562, acc 0.96
2016-09-06T14:31:22.711100: step 4443, loss 0.00492581, acc 1
2016-09-06T14:31:23.535829: step 4444, loss 0.0172983, acc 1
2016-09-06T14:31:24.334247: step 4445, loss 0.00934455, acc 1
2016-09-06T14:31:25.173729: step 4446, loss 0.0306294, acc 0.98
2016-09-06T14:31:25.973437: step 4447, loss 0.061634, acc 0.98
2016-09-06T14:31:26.762953: step 4448, loss 0.00378827, acc 1
2016-09-06T14:31:27.589118: step 4449, loss 0.0232728, acc 1
2016-09-06T14:31:28.402487: step 4450, loss 0.0382213, acc 0.98
2016-09-06T14:31:29.172672: step 4451, loss 0.0146444, acc 1
2016-09-06T14:31:29.969449: step 4452, loss 0.00445688, acc 1
2016-09-06T14:31:30.797567: step 4453, loss 0.0457998, acc 0.98
2016-09-06T14:31:31.585581: step 4454, loss 0.0225377, acc 1
2016-09-06T14:31:32.432807: step 4455, loss 0.0296354, acc 1
2016-09-06T14:31:33.260782: step 4456, loss 0.00268202, acc 1
2016-09-06T14:31:34.062743: step 4457, loss 0.0229534, acc 1
2016-09-06T14:31:34.864079: step 4458, loss 0.00473007, acc 1
2016-09-06T14:31:35.668595: step 4459, loss 0.00681438, acc 1
2016-09-06T14:31:36.479100: step 4460, loss 0.0264585, acc 0.98
2016-09-06T14:31:37.284021: step 4461, loss 0.0259824, acc 1
2016-09-06T14:31:38.126119: step 4462, loss 0.0827131, acc 0.96
2016-09-06T14:31:38.940399: step 4463, loss 0.0595926, acc 0.96
2016-09-06T14:31:39.759807: step 4464, loss 0.0301283, acc 1
2016-09-06T14:31:40.576845: step 4465, loss 0.00743614, acc 1
2016-09-06T14:31:41.387885: step 4466, loss 0.105576, acc 0.96
2016-09-06T14:31:42.194737: step 4467, loss 0.0032141, acc 1
2016-09-06T14:31:43.040446: step 4468, loss 0.0301697, acc 0.98
2016-09-06T14:31:43.856750: step 4469, loss 0.00794054, acc 1
2016-09-06T14:31:44.684627: step 4470, loss 0.0840821, acc 0.98
2016-09-06T14:31:45.527522: step 4471, loss 0.121697, acc 0.94
2016-09-06T14:31:46.329428: step 4472, loss 0.0406522, acc 1
2016-09-06T14:31:47.143215: step 4473, loss 0.00729228, acc 1
2016-09-06T14:31:47.972727: step 4474, loss 0.0192753, acc 0.98
2016-09-06T14:31:48.772203: step 4475, loss 0.0250008, acc 0.98
2016-09-06T14:31:49.590101: step 4476, loss 0.124256, acc 0.96
2016-09-06T14:31:50.410124: step 4477, loss 0.00363022, acc 1
2016-09-06T14:31:51.226248: step 4478, loss 0.0261038, acc 0.98
2016-09-06T14:31:52.058722: step 4479, loss 0.0081602, acc 1
2016-09-06T14:31:52.875702: step 4480, loss 0.0559578, acc 0.98
2016-09-06T14:31:53.713708: step 4481, loss 0.0344655, acc 0.98
2016-09-06T14:31:54.515747: step 4482, loss 0.0123995, acc 1
2016-09-06T14:31:55.336710: step 4483, loss 0.0237133, acc 1
2016-09-06T14:31:56.145404: step 4484, loss 0.0246403, acc 1
2016-09-06T14:31:56.932423: step 4485, loss 0.00933979, acc 1
2016-09-06T14:31:57.747475: step 4486, loss 0.0259294, acc 1
2016-09-06T14:31:58.563061: step 4487, loss 0.0398303, acc 0.98
2016-09-06T14:31:59.371483: step 4488, loss 0.0355133, acc 0.98
2016-09-06T14:32:00.213453: step 4489, loss 0.0517063, acc 0.98
2016-09-06T14:32:01.053008: step 4490, loss 0.0550904, acc 0.98
2016-09-06T14:32:01.851513: step 4491, loss 0.0255325, acc 1
2016-09-06T14:32:02.658136: step 4492, loss 0.0162479, acc 1
2016-09-06T14:32:03.483266: step 4493, loss 0.00370139, acc 1
2016-09-06T14:32:04.282609: step 4494, loss 0.0461758, acc 1
2016-09-06T14:32:05.095816: step 4495, loss 0.0295302, acc 0.98
2016-09-06T14:32:05.925575: step 4496, loss 0.0151043, acc 1
2016-09-06T14:32:06.726871: step 4497, loss 0.0146689, acc 1
2016-09-06T14:32:07.531913: step 4498, loss 0.0429794, acc 0.96
2016-09-06T14:32:08.366701: step 4499, loss 0.0417824, acc 0.98
2016-09-06T14:32:09.163545: step 4500, loss 0.00696797, acc 1

Evaluation:
2016-09-06T14:32:12.894748: step 4500, loss 1.6396, acc 0.766416

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-4500

2016-09-06T14:32:14.831649: step 4501, loss 0.0292539, acc 0.98
2016-09-06T14:32:15.677243: step 4502, loss 0.0479571, acc 0.96
2016-09-06T14:32:16.453544: step 4503, loss 0.00417102, acc 1
2016-09-06T14:32:17.293245: step 4504, loss 0.0135891, acc 1
2016-09-06T14:32:18.094812: step 4505, loss 0.0812159, acc 0.98
2016-09-06T14:32:18.898716: step 4506, loss 0.0150374, acc 1
2016-09-06T14:32:19.696256: step 4507, loss 0.030867, acc 0.98
2016-09-06T14:32:20.504802: step 4508, loss 0.0198461, acc 0.98
2016-09-06T14:32:21.269510: step 4509, loss 0.0490531, acc 0.98
2016-09-06T14:32:22.066622: step 4510, loss 0.0187119, acc 1
2016-09-06T14:32:22.876370: step 4511, loss 0.0086164, acc 1
2016-09-06T14:32:23.664494: step 4512, loss 0.0485976, acc 0.96
2016-09-06T14:32:24.467084: step 4513, loss 0.00408069, acc 1
2016-09-06T14:32:25.280897: step 4514, loss 0.0504463, acc 0.96
2016-09-06T14:32:26.093360: step 4515, loss 0.00652989, acc 1
2016-09-06T14:32:26.878720: step 4516, loss 0.0641234, acc 0.98
2016-09-06T14:32:27.733967: step 4517, loss 0.0574922, acc 0.98
2016-09-06T14:32:28.519718: step 4518, loss 0.0486056, acc 0.98
2016-09-06T14:32:29.335987: step 4519, loss 0.00452272, acc 1
2016-09-06T14:32:30.151299: step 4520, loss 0.00444802, acc 1
2016-09-06T14:32:30.945090: step 4521, loss 0.0145042, acc 1
2016-09-06T14:32:31.743638: step 4522, loss 0.0124114, acc 1
2016-09-06T14:32:32.548988: step 4523, loss 0.0183103, acc 1
2016-09-06T14:32:33.384348: step 4524, loss 0.0508464, acc 0.98
2016-09-06T14:32:34.200617: step 4525, loss 0.0492577, acc 0.98
2016-09-06T14:32:35.010997: step 4526, loss 0.0650328, acc 0.96
2016-09-06T14:32:35.790476: step 4527, loss 0.025888, acc 0.98
2016-09-06T14:32:36.582999: step 4528, loss 0.0538705, acc 1
2016-09-06T14:32:37.398170: step 4529, loss 0.0352638, acc 0.96
2016-09-06T14:32:38.183257: step 4530, loss 0.0143875, acc 1
2016-09-06T14:32:38.977711: step 4531, loss 0.0605705, acc 0.96
2016-09-06T14:32:39.793358: step 4532, loss 0.0224924, acc 1
2016-09-06T14:32:40.565416: step 4533, loss 0.0575487, acc 0.98
2016-09-06T14:32:41.388137: step 4534, loss 0.00398334, acc 1
2016-09-06T14:32:42.208040: step 4535, loss 0.0232371, acc 0.98
2016-09-06T14:32:43.007251: step 4536, loss 0.0316089, acc 0.98
2016-09-06T14:32:43.806089: step 4537, loss 0.00427891, acc 1
2016-09-06T14:32:44.625911: step 4538, loss 0.00474592, acc 1
2016-09-06T14:32:45.424157: step 4539, loss 0.0189491, acc 1
2016-09-06T14:32:46.225434: step 4540, loss 0.0787822, acc 0.96
2016-09-06T14:32:47.042854: step 4541, loss 0.0195658, acc 0.98
2016-09-06T14:32:47.853220: step 4542, loss 0.0681137, acc 0.96
2016-09-06T14:32:48.673470: step 4543, loss 0.0560096, acc 0.98
2016-09-06T14:32:49.497627: step 4544, loss 0.00580308, acc 1
2016-09-06T14:32:50.277800: step 4545, loss 0.0723372, acc 0.96
2016-09-06T14:32:51.065389: step 4546, loss 0.0574844, acc 0.98
2016-09-06T14:32:51.852121: step 4547, loss 0.0221028, acc 0.98
2016-09-06T14:32:52.656766: step 4548, loss 0.0227025, acc 1
2016-09-06T14:32:53.458045: step 4549, loss 0.0578678, acc 0.96
2016-09-06T14:32:54.274454: step 4550, loss 0.0432953, acc 0.98
2016-09-06T14:32:55.053399: step 4551, loss 0.0150829, acc 1
2016-09-06T14:32:55.849919: step 4552, loss 0.0377655, acc 1
2016-09-06T14:32:56.666729: step 4553, loss 0.0444134, acc 0.98
2016-09-06T14:32:57.461441: step 4554, loss 0.0592855, acc 0.98
2016-09-06T14:32:58.276007: step 4555, loss 0.00858816, acc 1
2016-09-06T14:32:59.093767: step 4556, loss 0.00750076, acc 1
2016-09-06T14:32:59.886233: step 4557, loss 0.0949506, acc 0.96
2016-09-06T14:33:00.703060: step 4558, loss 0.0420556, acc 0.98
2016-09-06T14:33:01.518782: step 4559, loss 0.00840254, acc 1
2016-09-06T14:33:02.341232: step 4560, loss 0.0341607, acc 1
2016-09-06T14:33:03.191853: step 4561, loss 0.0453925, acc 0.98
2016-09-06T14:33:03.996463: step 4562, loss 0.0983778, acc 0.98
2016-09-06T14:33:04.779402: step 4563, loss 0.101274, acc 0.94
2016-09-06T14:33:05.568665: step 4564, loss 0.0603936, acc 0.98
2016-09-06T14:33:06.394351: step 4565, loss 0.0276014, acc 0.98
2016-09-06T14:33:07.175162: step 4566, loss 0.0234882, acc 1
2016-09-06T14:33:08.008089: step 4567, loss 0.0168155, acc 1
2016-09-06T14:33:08.837574: step 4568, loss 0.00811067, acc 1
2016-09-06T14:33:09.654017: step 4569, loss 0.0865712, acc 0.98
2016-09-06T14:33:10.490163: step 4570, loss 0.0125854, acc 1
2016-09-06T14:33:11.333681: step 4571, loss 0.0647305, acc 0.96
2016-09-06T14:33:12.127275: step 4572, loss 0.013586, acc 1
2016-09-06T14:33:12.928013: step 4573, loss 0.0334602, acc 0.98
2016-09-06T14:33:13.756645: step 4574, loss 0.011003, acc 1
2016-09-06T14:33:14.555964: step 4575, loss 0.0307882, acc 0.98
2016-09-06T14:33:15.361490: step 4576, loss 0.144889, acc 0.92
2016-09-06T14:33:16.214874: step 4577, loss 0.139121, acc 0.94
2016-09-06T14:33:17.030964: step 4578, loss 0.0187096, acc 1
2016-09-06T14:33:17.838404: step 4579, loss 0.022774, acc 1
2016-09-06T14:33:18.657665: step 4580, loss 0.0727842, acc 0.94
2016-09-06T14:33:19.486355: step 4581, loss 0.0104838, acc 1
2016-09-06T14:33:20.304312: step 4582, loss 0.121619, acc 0.96
2016-09-06T14:33:21.155590: step 4583, loss 0.0185377, acc 0.98
2016-09-06T14:33:21.965007: step 4584, loss 0.136427, acc 0.92
2016-09-06T14:33:22.781440: step 4585, loss 0.132374, acc 0.92
2016-09-06T14:33:23.613456: step 4586, loss 0.0442855, acc 0.98
2016-09-06T14:33:24.417423: step 4587, loss 0.0229382, acc 1
2016-09-06T14:33:25.217371: step 4588, loss 0.0358248, acc 0.98
2016-09-06T14:33:26.054913: step 4589, loss 0.0352371, acc 0.98
2016-09-06T14:33:26.877444: step 4590, loss 0.0398952, acc 0.98
2016-09-06T14:33:27.727128: step 4591, loss 0.0480707, acc 0.98
2016-09-06T14:33:28.515349: step 4592, loss 0.0197919, acc 1
2016-09-06T14:33:29.329826: step 4593, loss 0.0283608, acc 1
2016-09-06T14:33:30.136047: step 4594, loss 0.0507962, acc 1
2016-09-06T14:33:30.946124: step 4595, loss 0.044197, acc 1
2016-09-06T14:33:31.773302: step 4596, loss 0.0753946, acc 0.98
2016-09-06T14:33:32.550781: step 4597, loss 0.0172828, acc 0.98
2016-09-06T14:33:33.367312: step 4598, loss 0.013925, acc 1
2016-09-06T14:33:34.177459: step 4599, loss 0.0150688, acc 1
2016-09-06T14:33:34.981429: step 4600, loss 0.00735807, acc 1

Evaluation:
2016-09-06T14:33:38.702535: step 4600, loss 1.42615, acc 0.757974

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-4600

2016-09-06T14:33:40.566081: step 4601, loss 0.0136038, acc 1
2016-09-06T14:33:41.376807: step 4602, loss 0.0255311, acc 1
2016-09-06T14:33:42.179214: step 4603, loss 0.0969193, acc 0.96
2016-09-06T14:33:42.992112: step 4604, loss 0.245037, acc 0.98
2016-09-06T14:33:43.793572: step 4605, loss 0.0230786, acc 1
2016-09-06T14:33:44.613784: step 4606, loss 0.0307987, acc 0.98
2016-09-06T14:33:45.478783: step 4607, loss 0.0358522, acc 0.98
2016-09-06T14:33:46.249371: step 4608, loss 0.0231262, acc 0.977273
2016-09-06T14:33:47.033410: step 4609, loss 0.0459731, acc 0.96
2016-09-06T14:33:47.866225: step 4610, loss 0.031557, acc 0.98
2016-09-06T14:33:48.678461: step 4611, loss 0.0418093, acc 0.98
2016-09-06T14:33:49.464201: step 4612, loss 0.0247896, acc 1
2016-09-06T14:33:50.256121: step 4613, loss 0.0675792, acc 0.94
2016-09-06T14:33:51.081034: step 4614, loss 0.0669762, acc 0.96
2016-09-06T14:33:51.877364: step 4615, loss 0.0360611, acc 0.98
2016-09-06T14:33:52.692982: step 4616, loss 0.027465, acc 0.98
2016-09-06T14:33:53.518797: step 4617, loss 0.0162409, acc 1
2016-09-06T14:33:54.288107: step 4618, loss 0.018998, acc 1
2016-09-06T14:33:55.130623: step 4619, loss 0.00519353, acc 1
2016-09-06T14:33:55.946496: step 4620, loss 0.0249142, acc 1
2016-09-06T14:33:56.728872: step 4621, loss 0.0489384, acc 1
2016-09-06T14:33:57.520535: step 4622, loss 0.0496248, acc 0.96
2016-09-06T14:33:58.361834: step 4623, loss 0.0387633, acc 1
2016-09-06T14:33:59.163484: step 4624, loss 0.0582588, acc 0.98
2016-09-06T14:33:59.981089: step 4625, loss 0.00914707, acc 1
2016-09-06T14:34:00.830609: step 4626, loss 0.0123146, acc 1
2016-09-06T14:34:01.610883: step 4627, loss 0.0894448, acc 0.98
2016-09-06T14:34:02.419631: step 4628, loss 0.00353711, acc 1
2016-09-06T14:34:03.202306: step 4629, loss 0.0249317, acc 0.98
2016-09-06T14:34:03.980474: step 4630, loss 0.107331, acc 0.96
2016-09-06T14:34:04.789285: step 4631, loss 0.0172256, acc 1
2016-09-06T14:34:05.628001: step 4632, loss 0.024731, acc 1
2016-09-06T14:34:06.414050: step 4633, loss 0.016928, acc 1
2016-09-06T14:34:07.251679: step 4634, loss 0.0327505, acc 0.98
2016-09-06T14:34:08.075388: step 4635, loss 0.0251607, acc 1
2016-09-06T14:34:08.846583: step 4636, loss 0.0517518, acc 0.98
2016-09-06T14:34:09.631569: step 4637, loss 0.010528, acc 1
2016-09-06T14:34:10.495486: step 4638, loss 0.0102737, acc 1
2016-09-06T14:34:11.302650: step 4639, loss 0.00662671, acc 1
2016-09-06T14:34:12.118548: step 4640, loss 0.0915752, acc 0.96
2016-09-06T14:34:12.966811: step 4641, loss 0.0405968, acc 1
2016-09-06T14:34:13.752374: step 4642, loss 0.0312061, acc 1
2016-09-06T14:34:14.571489: step 4643, loss 0.0078073, acc 1
2016-09-06T14:34:15.416846: step 4644, loss 0.0429887, acc 0.98
2016-09-06T14:34:16.248929: step 4645, loss 0.0129549, acc 1
2016-09-06T14:34:17.052724: step 4646, loss 0.0233054, acc 0.98
2016-09-06T14:34:17.900491: step 4647, loss 0.0354942, acc 1
2016-09-06T14:34:18.695595: step 4648, loss 0.00527938, acc 1
2016-09-06T14:34:19.529370: step 4649, loss 0.00472221, acc 1
2016-09-06T14:34:20.360683: step 4650, loss 0.0274485, acc 1
2016-09-06T14:34:21.190756: step 4651, loss 0.00491312, acc 1
2016-09-06T14:34:22.002334: step 4652, loss 0.0201041, acc 1
2016-09-06T14:34:22.819951: step 4653, loss 0.0662021, acc 0.96
2016-09-06T14:34:23.636791: step 4654, loss 0.0981207, acc 0.96
2016-09-06T14:34:24.448781: step 4655, loss 0.0251097, acc 1
2016-09-06T14:34:25.291753: step 4656, loss 0.0594259, acc 0.96
2016-09-06T14:34:26.095592: step 4657, loss 0.0194209, acc 1
2016-09-06T14:34:26.889469: step 4658, loss 0.00682135, acc 1
2016-09-06T14:34:27.715038: step 4659, loss 0.00934438, acc 1
2016-09-06T14:34:28.518519: step 4660, loss 0.00561626, acc 1
2016-09-06T14:34:29.328478: step 4661, loss 0.0429347, acc 0.98
2016-09-06T14:34:30.164428: step 4662, loss 0.0341718, acc 1
2016-09-06T14:34:31.010755: step 4663, loss 0.00915588, acc 1
2016-09-06T14:34:31.802631: step 4664, loss 0.0634967, acc 0.96
2016-09-06T14:34:32.611278: step 4665, loss 0.0308521, acc 0.98
2016-09-06T14:34:33.421405: step 4666, loss 0.0488751, acc 0.98
2016-09-06T14:34:34.195089: step 4667, loss 0.0194947, acc 0.98
2016-09-06T14:34:35.005256: step 4668, loss 0.0345386, acc 0.98
2016-09-06T14:34:35.836135: step 4669, loss 0.00423561, acc 1
2016-09-06T14:34:36.633024: step 4670, loss 0.0283329, acc 1
2016-09-06T14:34:37.465909: step 4671, loss 0.0140731, acc 1
2016-09-06T14:34:38.258266: step 4672, loss 0.0400257, acc 0.98
2016-09-06T14:34:39.029551: step 4673, loss 0.00356536, acc 1
2016-09-06T14:34:39.821510: step 4674, loss 0.0379661, acc 0.98
2016-09-06T14:34:40.642293: step 4675, loss 0.0114358, acc 1
2016-09-06T14:34:41.433140: step 4676, loss 0.03171, acc 1
2016-09-06T14:34:42.215821: step 4677, loss 0.0520288, acc 1
2016-09-06T14:34:43.031378: step 4678, loss 0.0195148, acc 1
2016-09-06T14:34:43.814134: step 4679, loss 0.09108, acc 0.96
2016-09-06T14:34:44.653922: step 4680, loss 0.0603126, acc 0.98
2016-09-06T14:34:45.483308: step 4681, loss 0.0339384, acc 0.98
2016-09-06T14:34:46.285998: step 4682, loss 0.0711115, acc 0.98
2016-09-06T14:34:47.087891: step 4683, loss 0.227066, acc 0.98
2016-09-06T14:34:47.911424: step 4684, loss 0.0101641, acc 1
2016-09-06T14:34:48.746058: step 4685, loss 0.0193966, acc 0.98
2016-09-06T14:34:49.552842: step 4686, loss 0.0282701, acc 1
2016-09-06T14:34:50.364664: step 4687, loss 0.0213999, acc 1
2016-09-06T14:34:51.143411: step 4688, loss 0.0253352, acc 1
2016-09-06T14:34:51.954020: step 4689, loss 0.00387852, acc 1
2016-09-06T14:34:52.766276: step 4690, loss 0.0179693, acc 0.98
2016-09-06T14:34:53.541641: step 4691, loss 0.0973153, acc 0.94
2016-09-06T14:34:54.358777: step 4692, loss 0.0331956, acc 0.98
2016-09-06T14:34:55.166911: step 4693, loss 0.0410538, acc 0.98
2016-09-06T14:34:55.985243: step 4694, loss 0.0395714, acc 0.96
2016-09-06T14:34:56.775128: step 4695, loss 0.00766905, acc 1
2016-09-06T14:34:57.589401: step 4696, loss 0.0344468, acc 0.96
2016-09-06T14:34:58.364114: step 4697, loss 0.0114225, acc 1
2016-09-06T14:34:59.164352: step 4698, loss 0.00696115, acc 1
2016-09-06T14:34:59.995403: step 4699, loss 0.0671243, acc 0.96
2016-09-06T14:35:00.794165: step 4700, loss 0.0258041, acc 0.98

Evaluation:
2016-09-06T14:35:04.523879: step 4700, loss 1.55024, acc 0.761726

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-4700

2016-09-06T14:35:06.515201: step 4701, loss 0.038043, acc 1
2016-09-06T14:35:07.327619: step 4702, loss 0.019784, acc 1
2016-09-06T14:35:08.116190: step 4703, loss 0.0140312, acc 1
2016-09-06T14:35:08.943410: step 4704, loss 0.0313574, acc 1
2016-09-06T14:35:09.765144: step 4705, loss 0.021102, acc 0.98
2016-09-06T14:35:10.576553: step 4706, loss 0.0446907, acc 0.98
2016-09-06T14:35:11.396066: step 4707, loss 0.00481099, acc 1
2016-09-06T14:35:12.256800: step 4708, loss 0.0111671, acc 1
2016-09-06T14:35:13.056820: step 4709, loss 0.0321182, acc 0.98
2016-09-06T14:35:13.867016: step 4710, loss 0.0146333, acc 1
2016-09-06T14:35:14.701453: step 4711, loss 0.00466871, acc 1
2016-09-06T14:35:15.503040: step 4712, loss 0.0235957, acc 0.98
2016-09-06T14:35:16.304388: step 4713, loss 0.0483021, acc 0.96
2016-09-06T14:35:17.116233: step 4714, loss 0.0308173, acc 1
2016-09-06T14:35:17.930517: step 4715, loss 0.0820284, acc 0.94
2016-09-06T14:35:18.751383: step 4716, loss 0.0167252, acc 1
2016-09-06T14:35:19.596582: step 4717, loss 0.0458333, acc 0.98
2016-09-06T14:35:20.405777: step 4718, loss 0.0485176, acc 0.96
2016-09-06T14:35:21.191780: step 4719, loss 0.00407203, acc 1
2016-09-06T14:35:22.047558: step 4720, loss 0.0204373, acc 1
2016-09-06T14:35:22.860923: step 4721, loss 0.111976, acc 0.94
2016-09-06T14:35:23.676758: step 4722, loss 0.00533513, acc 1
2016-09-06T14:35:24.497719: step 4723, loss 0.123574, acc 0.92
2016-09-06T14:35:25.314912: step 4724, loss 0.0126737, acc 1
2016-09-06T14:35:26.127727: step 4725, loss 0.0383192, acc 1
2016-09-06T14:35:26.963355: step 4726, loss 0.00374195, acc 1
2016-09-06T14:35:27.800942: step 4727, loss 0.0594991, acc 0.96
2016-09-06T14:35:28.592797: step 4728, loss 0.0240592, acc 1
2016-09-06T14:35:29.402190: step 4729, loss 0.0241061, acc 0.98
2016-09-06T14:35:30.215462: step 4730, loss 0.00316678, acc 1
2016-09-06T14:35:31.011764: step 4731, loss 0.0115985, acc 1
2016-09-06T14:35:31.822218: step 4732, loss 0.0361731, acc 0.98
2016-09-06T14:35:32.649151: step 4733, loss 0.0318594, acc 0.98
2016-09-06T14:35:33.453804: step 4734, loss 0.00499008, acc 1
2016-09-06T14:35:34.292495: step 4735, loss 0.0203538, acc 0.98
2016-09-06T14:35:35.111780: step 4736, loss 0.015818, acc 1
2016-09-06T14:35:35.883155: step 4737, loss 0.065947, acc 0.96
2016-09-06T14:35:36.694255: step 4738, loss 0.0423457, acc 0.96
2016-09-06T14:35:37.507221: step 4739, loss 0.0744262, acc 0.96
2016-09-06T14:35:38.292379: step 4740, loss 0.00476246, acc 1
2016-09-06T14:35:39.129038: step 4741, loss 0.0174244, acc 1
2016-09-06T14:35:39.931257: step 4742, loss 0.0230269, acc 0.98
2016-09-06T14:35:40.706908: step 4743, loss 0.0619793, acc 0.96
2016-09-06T14:35:41.506925: step 4744, loss 0.00710757, acc 1
2016-09-06T14:35:42.334994: step 4745, loss 0.0181958, acc 0.98
2016-09-06T14:35:43.126002: step 4746, loss 0.200808, acc 0.88
2016-09-06T14:35:43.961088: step 4747, loss 0.0114108, acc 1
2016-09-06T14:35:44.775368: step 4748, loss 0.0120044, acc 1
2016-09-06T14:35:45.573829: step 4749, loss 0.0378003, acc 0.96
2016-09-06T14:35:46.368575: step 4750, loss 0.0474016, acc 0.98
2016-09-06T14:35:47.184681: step 4751, loss 0.037596, acc 0.98
2016-09-06T14:35:47.948531: step 4752, loss 0.0329787, acc 1
2016-09-06T14:35:48.753758: step 4753, loss 0.0314356, acc 0.98
2016-09-06T14:35:49.577629: step 4754, loss 0.0271593, acc 0.98
2016-09-06T14:35:50.374280: step 4755, loss 0.0331944, acc 0.98
2016-09-06T14:35:51.171362: step 4756, loss 0.0612587, acc 0.98
2016-09-06T14:35:52.013917: step 4757, loss 0.0337335, acc 0.96
2016-09-06T14:35:52.779843: step 4758, loss 0.0352548, acc 0.98
2016-09-06T14:35:53.608408: step 4759, loss 0.0203556, acc 0.98
2016-09-06T14:35:54.418956: step 4760, loss 0.0178066, acc 1
2016-09-06T14:35:55.209919: step 4761, loss 0.00381008, acc 1
2016-09-06T14:35:56.036197: step 4762, loss 0.0320204, acc 1
2016-09-06T14:35:56.859493: step 4763, loss 0.0225556, acc 0.98
2016-09-06T14:35:57.670880: step 4764, loss 0.0386003, acc 0.98
2016-09-06T14:35:58.478890: step 4765, loss 0.00773877, acc 1
2016-09-06T14:35:59.311609: step 4766, loss 0.0227792, acc 0.98
2016-09-06T14:36:00.113241: step 4767, loss 0.0207626, acc 0.98
2016-09-06T14:36:00.941096: step 4768, loss 0.0660973, acc 0.98
2016-09-06T14:36:01.793565: step 4769, loss 0.039126, acc 0.98
2016-09-06T14:36:02.602904: step 4770, loss 0.0320815, acc 0.98
2016-09-06T14:36:03.410315: step 4771, loss 0.00689345, acc 1
2016-09-06T14:36:04.272944: step 4772, loss 0.00664821, acc 1
2016-09-06T14:36:05.105045: step 4773, loss 0.0118964, acc 1
2016-09-06T14:36:05.906808: step 4774, loss 0.0274192, acc 0.98
2016-09-06T14:36:06.728582: step 4775, loss 0.0707073, acc 0.96
2016-09-06T14:36:07.512519: step 4776, loss 0.0179051, acc 0.98
2016-09-06T14:36:08.312610: step 4777, loss 0.155357, acc 0.96
2016-09-06T14:36:09.153589: step 4778, loss 0.00500021, acc 1
2016-09-06T14:36:09.985492: step 4779, loss 0.0166304, acc 1
2016-09-06T14:36:10.796617: step 4780, loss 0.0197659, acc 1
2016-09-06T14:36:11.641578: step 4781, loss 0.0342532, acc 0.98
2016-09-06T14:36:12.435502: step 4782, loss 0.0184146, acc 1
2016-09-06T14:36:13.240038: step 4783, loss 0.024364, acc 1
2016-09-06T14:36:14.092417: step 4784, loss 0.016512, acc 1
2016-09-06T14:36:14.900508: step 4785, loss 0.0174861, acc 1
2016-09-06T14:36:15.706733: step 4786, loss 0.0490177, acc 0.98
2016-09-06T14:36:16.528869: step 4787, loss 0.0637122, acc 0.96
2016-09-06T14:36:17.348956: step 4788, loss 0.00543935, acc 1
2016-09-06T14:36:18.132477: step 4789, loss 0.0210985, acc 1
2016-09-06T14:36:18.934900: step 4790, loss 0.00609212, acc 1
2016-09-06T14:36:19.744927: step 4791, loss 0.00287008, acc 1
2016-09-06T14:36:20.535010: step 4792, loss 0.031197, acc 0.98
2016-09-06T14:36:21.340801: step 4793, loss 0.0109522, acc 1
2016-09-06T14:36:22.150506: step 4794, loss 0.0262922, acc 1
2016-09-06T14:36:22.945427: step 4795, loss 0.0432974, acc 1
2016-09-06T14:36:23.773389: step 4796, loss 0.0599162, acc 0.98
2016-09-06T14:36:24.592194: step 4797, loss 0.0942361, acc 0.96
2016-09-06T14:36:25.381700: step 4798, loss 0.00291167, acc 1
2016-09-06T14:36:26.183324: step 4799, loss 0.0853448, acc 0.98
2016-09-06T14:36:27.000627: step 4800, loss 0.0217379, acc 0.977273

Evaluation:
2016-09-06T14:36:30.721237: step 4800, loss 1.73518, acc 0.755159

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-4800

2016-09-06T14:36:32.628031: step 4801, loss 0.0204461, acc 1
2016-09-06T14:36:33.454952: step 4802, loss 0.0305817, acc 0.98
2016-09-06T14:36:34.308459: step 4803, loss 0.0331046, acc 0.98
2016-09-06T14:36:35.136676: step 4804, loss 0.00898647, acc 1
2016-09-06T14:36:35.915018: step 4805, loss 0.0274585, acc 0.98
2016-09-06T14:36:36.727573: step 4806, loss 0.0100496, acc 1
2016-09-06T14:36:37.509105: step 4807, loss 0.0288212, acc 0.98
2016-09-06T14:36:38.316533: step 4808, loss 0.0221758, acc 1
2016-09-06T14:36:39.136614: step 4809, loss 0.0883183, acc 0.96
2016-09-06T14:36:39.930509: step 4810, loss 0.0503601, acc 0.98
2016-09-06T14:36:40.723646: step 4811, loss 0.00384276, acc 1
2016-09-06T14:36:41.530382: step 4812, loss 0.00516209, acc 1
2016-09-06T14:36:42.318338: step 4813, loss 0.00256945, acc 1
2016-09-06T14:36:43.142892: step 4814, loss 0.0170728, acc 1
2016-09-06T14:36:43.973289: step 4815, loss 0.0294723, acc 1
2016-09-06T14:36:44.783833: step 4816, loss 0.0327371, acc 0.98
2016-09-06T14:36:45.580733: step 4817, loss 0.0310394, acc 0.98
2016-09-06T14:36:46.379645: step 4818, loss 0.0279065, acc 0.98
2016-09-06T14:36:47.167930: step 4819, loss 0.0266764, acc 1
2016-09-06T14:36:47.989863: step 4820, loss 0.00488202, acc 1
2016-09-06T14:36:48.825782: step 4821, loss 0.00862826, acc 1
2016-09-06T14:36:49.592450: step 4822, loss 0.0187369, acc 0.98
2016-09-06T14:36:50.390238: step 4823, loss 0.00740641, acc 1
2016-09-06T14:36:51.204285: step 4824, loss 0.0155123, acc 1
2016-09-06T14:36:51.980879: step 4825, loss 0.0268518, acc 1
2016-09-06T14:36:52.794504: step 4826, loss 0.0160232, acc 1
2016-09-06T14:36:53.625419: step 4827, loss 0.136446, acc 0.98
2016-09-06T14:36:54.404334: step 4828, loss 0.026461, acc 0.98
2016-09-06T14:36:55.257501: step 4829, loss 0.00545562, acc 1
2016-09-06T14:36:56.080069: step 4830, loss 0.05367, acc 1
2016-09-06T14:36:56.843746: step 4831, loss 0.00388205, acc 1
2016-09-06T14:36:57.655858: step 4832, loss 0.0441794, acc 0.96
2016-09-06T14:36:58.481413: step 4833, loss 0.00353675, acc 1
2016-09-06T14:36:59.242918: step 4834, loss 0.00444704, acc 1
2016-09-06T14:37:00.049536: step 4835, loss 0.00736488, acc 1
2016-09-06T14:37:00.923813: step 4836, loss 0.0284911, acc 0.98
2016-09-06T14:37:01.691932: step 4837, loss 0.0169812, acc 1
2016-09-06T14:37:02.495837: step 4838, loss 0.00551387, acc 1
2016-09-06T14:37:03.323652: step 4839, loss 0.0454551, acc 0.98
2016-09-06T14:37:04.139061: step 4840, loss 0.089881, acc 0.96
2016-09-06T14:37:04.930402: step 4841, loss 0.103657, acc 0.96
2016-09-06T14:37:05.754605: step 4842, loss 0.0112835, acc 1
2016-09-06T14:37:06.537386: step 4843, loss 0.0168961, acc 1
2016-09-06T14:37:07.306430: step 4844, loss 0.0166529, acc 1
2016-09-06T14:37:08.115327: step 4845, loss 0.069831, acc 0.98
2016-09-06T14:37:08.923912: step 4846, loss 0.0273897, acc 0.98
2016-09-06T14:37:09.736982: step 4847, loss 0.108936, acc 0.98
2016-09-06T14:37:10.572756: step 4848, loss 0.0146602, acc 1
2016-09-06T14:37:11.379976: step 4849, loss 0.0606034, acc 0.96
2016-09-06T14:37:12.187515: step 4850, loss 0.0358379, acc 0.96
2016-09-06T14:37:12.992685: step 4851, loss 0.0520491, acc 0.96
2016-09-06T14:37:13.751227: step 4852, loss 0.0279519, acc 0.98
2016-09-06T14:37:14.569823: step 4853, loss 0.022975, acc 0.98
2016-09-06T14:37:15.417356: step 4854, loss 0.0537918, acc 0.98
2016-09-06T14:37:16.228454: step 4855, loss 0.0307007, acc 0.98
2016-09-06T14:37:17.038612: step 4856, loss 0.00843546, acc 1
2016-09-06T14:37:17.853477: step 4857, loss 0.0746648, acc 0.98
2016-09-06T14:37:18.630688: step 4858, loss 0.0150849, acc 1
2016-09-06T14:37:19.439379: step 4859, loss 0.0262438, acc 1
2016-09-06T14:37:20.244348: step 4860, loss 0.0181074, acc 1
2016-09-06T14:37:21.017037: step 4861, loss 0.0208419, acc 0.98
2016-09-06T14:37:21.802316: step 4862, loss 0.00940328, acc 1
2016-09-06T14:37:22.624113: step 4863, loss 0.0198523, acc 1
2016-09-06T14:37:23.436286: step 4864, loss 0.0477244, acc 0.94
2016-09-06T14:37:24.245636: step 4865, loss 0.0411271, acc 0.98
2016-09-06T14:37:25.081816: step 4866, loss 0.0571444, acc 0.96
2016-09-06T14:37:25.884482: step 4867, loss 0.00682284, acc 1
2016-09-06T14:37:26.704342: step 4868, loss 0.176004, acc 0.96
2016-09-06T14:37:27.508910: step 4869, loss 0.0301012, acc 0.98
2016-09-06T14:37:28.305369: step 4870, loss 0.00578734, acc 1
2016-09-06T14:37:29.110691: step 4871, loss 0.0249006, acc 0.98
2016-09-06T14:37:29.948476: step 4872, loss 0.0227982, acc 0.98
2016-09-06T14:37:30.752295: step 4873, loss 0.0714102, acc 0.96
2016-09-06T14:37:31.557056: step 4874, loss 0.00380589, acc 1
2016-09-06T14:37:32.387540: step 4875, loss 0.0222739, acc 0.98
2016-09-06T14:37:33.208614: step 4876, loss 0.115104, acc 0.94
2016-09-06T14:37:34.024546: step 4877, loss 0.0387498, acc 1
2016-09-06T14:37:34.880419: step 4878, loss 0.0266227, acc 1
2016-09-06T14:37:35.729954: step 4879, loss 0.0777527, acc 0.94
2016-09-06T14:37:36.555356: step 4880, loss 0.0577233, acc 0.96
2016-09-06T14:37:37.392759: step 4881, loss 0.010753, acc 1
2016-09-06T14:37:38.216506: step 4882, loss 0.0245633, acc 1
2016-09-06T14:37:39.014154: step 4883, loss 0.0193177, acc 1
2016-09-06T14:37:39.877632: step 4884, loss 0.00551132, acc 1
2016-09-06T14:37:40.656552: step 4885, loss 0.0637954, acc 0.96
2016-09-06T14:37:41.470241: step 4886, loss 0.0459679, acc 0.98
2016-09-06T14:37:42.318918: step 4887, loss 0.0107194, acc 1
2016-09-06T14:37:43.110579: step 4888, loss 0.0453969, acc 0.98
2016-09-06T14:37:43.923250: step 4889, loss 0.0613919, acc 0.96
2016-09-06T14:37:44.733459: step 4890, loss 0.0075057, acc 1
2016-09-06T14:37:45.557446: step 4891, loss 0.0185985, acc 1
2016-09-06T14:37:46.381075: step 4892, loss 0.168628, acc 0.96
2016-09-06T14:37:47.199033: step 4893, loss 0.0192765, acc 1
2016-09-06T14:37:48.004259: step 4894, loss 0.015714, acc 1
2016-09-06T14:37:48.792781: step 4895, loss 0.0139125, acc 1
2016-09-06T14:37:49.592200: step 4896, loss 0.00610179, acc 1
2016-09-06T14:37:50.388110: step 4897, loss 0.0136885, acc 1
2016-09-06T14:37:51.196002: step 4898, loss 0.0688487, acc 0.96
2016-09-06T14:37:51.985381: step 4899, loss 0.0718437, acc 0.96
2016-09-06T14:37:52.785412: step 4900, loss 0.0219954, acc 1

Evaluation:
2016-09-06T14:37:56.526504: step 4900, loss 1.69503, acc 0.751407

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-4900

2016-09-06T14:37:58.461330: step 4901, loss 0.0101078, acc 1
2016-09-06T14:37:59.265123: step 4902, loss 0.0563755, acc 0.98
2016-09-06T14:38:00.082993: step 4903, loss 0.027286, acc 0.98
2016-09-06T14:38:00.947671: step 4904, loss 0.0237734, acc 0.98
2016-09-06T14:38:01.769878: step 4905, loss 0.108803, acc 0.94
2016-09-06T14:38:02.573024: step 4906, loss 0.0118319, acc 1
2016-09-06T14:38:03.358958: step 4907, loss 0.0400656, acc 1
2016-09-06T14:38:04.183825: step 4908, loss 0.00613693, acc 1
2016-09-06T14:38:05.017411: step 4909, loss 0.040187, acc 1
2016-09-06T14:38:05.818054: step 4910, loss 0.0542569, acc 0.98
2016-09-06T14:38:06.619786: step 4911, loss 0.00461228, acc 1
2016-09-06T14:38:07.440262: step 4912, loss 0.0206043, acc 0.98
2016-09-06T14:38:08.229266: step 4913, loss 0.0262404, acc 1
2016-09-06T14:38:09.051474: step 4914, loss 0.0215152, acc 0.98
2016-09-06T14:38:09.878380: step 4915, loss 0.0933317, acc 0.96
2016-09-06T14:38:10.686076: step 4916, loss 0.032322, acc 1
2016-09-06T14:38:11.512195: step 4917, loss 0.00379458, acc 1
2016-09-06T14:38:12.331386: step 4918, loss 0.0466846, acc 0.98
2016-09-06T14:38:13.119774: step 4919, loss 0.187418, acc 0.96
2016-09-06T14:38:13.925238: step 4920, loss 0.0544973, acc 0.98
2016-09-06T14:38:14.748305: step 4921, loss 0.00703253, acc 1
2016-09-06T14:38:15.538880: step 4922, loss 0.00317537, acc 1
2016-09-06T14:38:16.327602: step 4923, loss 0.00448556, acc 1
2016-09-06T14:38:17.158952: step 4924, loss 0.0363468, acc 0.98
2016-09-06T14:38:17.936412: step 4925, loss 0.0248355, acc 1
2016-09-06T14:38:18.788629: step 4926, loss 0.0393591, acc 0.98
2016-09-06T14:38:19.605146: step 4927, loss 0.0206869, acc 1
2016-09-06T14:38:20.402466: step 4928, loss 0.0157374, acc 1
2016-09-06T14:38:21.209138: step 4929, loss 0.0156321, acc 1
2016-09-06T14:38:22.037641: step 4930, loss 0.0273792, acc 1
2016-09-06T14:38:22.851379: step 4931, loss 0.0666303, acc 0.98
2016-09-06T14:38:23.663326: step 4932, loss 0.0661746, acc 0.98
2016-09-06T14:38:24.507610: step 4933, loss 0.00671703, acc 1
2016-09-06T14:38:25.325338: step 4934, loss 0.0217046, acc 0.98
2016-09-06T14:38:26.129103: step 4935, loss 0.027612, acc 0.98
2016-09-06T14:38:26.956538: step 4936, loss 0.0377803, acc 0.98
2016-09-06T14:38:27.772164: step 4937, loss 0.0256868, acc 0.98
2016-09-06T14:38:28.572056: step 4938, loss 0.0719985, acc 0.94
2016-09-06T14:38:29.396330: step 4939, loss 0.0262535, acc 1
2016-09-06T14:38:30.207525: step 4940, loss 0.013875, acc 1
2016-09-06T14:38:31.037441: step 4941, loss 0.0216897, acc 1
2016-09-06T14:38:31.881639: step 4942, loss 0.0261308, acc 1
2016-09-06T14:38:32.733079: step 4943, loss 0.0533765, acc 1
2016-09-06T14:38:33.563855: step 4944, loss 0.0270208, acc 1
2016-09-06T14:38:34.385645: step 4945, loss 0.0355379, acc 0.98
2016-09-06T14:38:35.188371: step 4946, loss 0.0240624, acc 0.98
2016-09-06T14:38:36.017744: step 4947, loss 0.126324, acc 0.92
2016-09-06T14:38:36.814782: step 4948, loss 0.0264691, acc 0.98
2016-09-06T14:38:37.628981: step 4949, loss 0.0218296, acc 1
2016-09-06T14:38:38.429585: step 4950, loss 0.0409371, acc 0.98
2016-09-06T14:38:39.255283: step 4951, loss 0.00412278, acc 1
2016-09-06T14:38:40.071750: step 4952, loss 0.00381594, acc 1
2016-09-06T14:38:40.866179: step 4953, loss 0.0622521, acc 0.94
2016-09-06T14:38:41.649862: step 4954, loss 0.0647607, acc 0.96
2016-09-06T14:38:42.486204: step 4955, loss 0.0487358, acc 0.98
2016-09-06T14:38:43.255899: step 4956, loss 0.0756297, acc 0.98
2016-09-06T14:38:44.070762: step 4957, loss 0.025127, acc 0.98
2016-09-06T14:38:44.889832: step 4958, loss 0.0266567, acc 0.98
2016-09-06T14:38:45.677435: step 4959, loss 0.0552684, acc 0.98
2016-09-06T14:38:46.487978: step 4960, loss 0.025432, acc 1
2016-09-06T14:38:47.297501: step 4961, loss 0.0392356, acc 1
2016-09-06T14:38:48.091560: step 4962, loss 0.0751699, acc 0.96
2016-09-06T14:38:48.905110: step 4963, loss 0.0166061, acc 1
2016-09-06T14:38:49.739538: step 4964, loss 0.0100446, acc 1
2016-09-06T14:38:50.548303: step 4965, loss 0.00511648, acc 1
2016-09-06T14:38:51.380283: step 4966, loss 0.0399527, acc 0.98
2016-09-06T14:38:52.198095: step 4967, loss 0.0100263, acc 1
2016-09-06T14:38:52.968311: step 4968, loss 0.0102678, acc 1
2016-09-06T14:38:53.800034: step 4969, loss 0.0240177, acc 1
2016-09-06T14:38:54.587009: step 4970, loss 0.0497777, acc 0.98
2016-09-06T14:38:55.362670: step 4971, loss 0.0930001, acc 0.94
2016-09-06T14:38:56.181453: step 4972, loss 0.0152595, acc 1
2016-09-06T14:38:56.988884: step 4973, loss 0.0428742, acc 0.98
2016-09-06T14:38:57.801088: step 4974, loss 0.0170098, acc 1
2016-09-06T14:38:58.582166: step 4975, loss 0.040291, acc 0.98
2016-09-06T14:38:59.385252: step 4976, loss 0.148045, acc 0.94
2016-09-06T14:39:00.178761: step 4977, loss 0.0437636, acc 0.98
2016-09-06T14:39:01.012838: step 4978, loss 0.0464401, acc 0.98
2016-09-06T14:39:01.837801: step 4979, loss 0.0209441, acc 1
2016-09-06T14:39:02.632377: step 4980, loss 0.0145539, acc 1
2016-09-06T14:39:03.457464: step 4981, loss 0.0283642, acc 0.98
2016-09-06T14:39:04.277272: step 4982, loss 0.0872774, acc 0.94
2016-09-06T14:39:05.066329: step 4983, loss 0.0245276, acc 0.98
2016-09-06T14:39:05.884389: step 4984, loss 0.0146379, acc 1
2016-09-06T14:39:06.698795: step 4985, loss 0.0203263, acc 1
2016-09-06T14:39:07.482553: step 4986, loss 0.0363987, acc 0.98
2016-09-06T14:39:08.299790: step 4987, loss 0.00595641, acc 1
2016-09-06T14:39:09.117531: step 4988, loss 0.00708886, acc 1
2016-09-06T14:39:09.933192: step 4989, loss 0.0203964, acc 1
2016-09-06T14:39:10.750505: step 4990, loss 0.026956, acc 1
2016-09-06T14:39:11.600467: step 4991, loss 0.0533512, acc 0.98
2016-09-06T14:39:12.363261: step 4992, loss 0.00315019, acc 1
2016-09-06T14:39:13.188561: step 4993, loss 0.0205694, acc 1
2016-09-06T14:39:14.020354: step 4994, loss 0.0433065, acc 0.98
2016-09-06T14:39:14.858731: step 4995, loss 0.00329552, acc 1
2016-09-06T14:39:15.687943: step 4996, loss 0.0306918, acc 1
2016-09-06T14:39:16.516294: step 4997, loss 0.0278758, acc 1
2016-09-06T14:39:17.359549: step 4998, loss 0.0476297, acc 0.96
2016-09-06T14:39:18.188159: step 4999, loss 0.0362788, acc 0.98
2016-09-06T14:39:19.009257: step 5000, loss 0.0238267, acc 1

Evaluation:
2016-09-06T14:39:22.719330: step 5000, loss 1.56596, acc 0.752345

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-5000

2016-09-06T14:39:24.701013: step 5001, loss 0.097992, acc 0.94
2016-09-06T14:39:25.529720: step 5002, loss 0.0892329, acc 0.98
2016-09-06T14:39:26.365787: step 5003, loss 0.00484674, acc 1
2016-09-06T14:39:27.169731: step 5004, loss 0.0115003, acc 1
2016-09-06T14:39:27.985192: step 5005, loss 0.0222694, acc 0.98
2016-09-06T14:39:28.811067: step 5006, loss 0.0313255, acc 0.98
2016-09-06T14:39:29.622989: step 5007, loss 0.0557226, acc 0.98
2016-09-06T14:39:30.432913: step 5008, loss 0.00389265, acc 1
2016-09-06T14:39:31.264556: step 5009, loss 0.0276777, acc 0.98
2016-09-06T14:39:32.077211: step 5010, loss 0.00428893, acc 1
2016-09-06T14:39:32.887312: step 5011, loss 0.00830592, acc 1
2016-09-06T14:39:33.738477: step 5012, loss 0.0387819, acc 0.98
2016-09-06T14:39:34.554990: step 5013, loss 0.0108918, acc 1
2016-09-06T14:39:35.352958: step 5014, loss 0.019337, acc 1
2016-09-06T14:39:36.163593: step 5015, loss 0.031325, acc 0.98
2016-09-06T14:39:36.954597: step 5016, loss 0.0242346, acc 1
2016-09-06T14:39:37.783129: step 5017, loss 0.0514823, acc 0.98
2016-09-06T14:39:38.624935: step 5018, loss 0.108556, acc 0.96
2016-09-06T14:39:39.453024: step 5019, loss 0.0172863, acc 1
2016-09-06T14:39:40.237333: step 5020, loss 0.0101987, acc 1
2016-09-06T14:39:41.025469: step 5021, loss 0.003552, acc 1
2016-09-06T14:39:41.846363: step 5022, loss 0.0203584, acc 0.98
2016-09-06T14:39:42.623452: step 5023, loss 0.0193916, acc 0.98
2016-09-06T14:39:43.464130: step 5024, loss 0.0168755, acc 1
2016-09-06T14:39:44.291779: step 5025, loss 0.00857286, acc 1
2016-09-06T14:39:45.092467: step 5026, loss 0.00831161, acc 1
2016-09-06T14:39:45.913438: step 5027, loss 0.0185928, acc 0.98
2016-09-06T14:39:46.731079: step 5028, loss 0.0160153, acc 1
2016-09-06T14:39:47.509055: step 5029, loss 0.0897156, acc 0.96
2016-09-06T14:39:48.348537: step 5030, loss 0.0254934, acc 1
2016-09-06T14:39:49.158467: step 5031, loss 0.0549533, acc 0.98
2016-09-06T14:39:49.961240: step 5032, loss 0.0141244, acc 1
2016-09-06T14:39:50.771216: step 5033, loss 0.0605919, acc 0.96
2016-09-06T14:39:51.585867: step 5034, loss 0.0450387, acc 0.98
2016-09-06T14:39:52.405143: step 5035, loss 0.0445677, acc 0.98
2016-09-06T14:39:53.212470: step 5036, loss 0.0181043, acc 1
2016-09-06T14:39:54.027148: step 5037, loss 0.0646066, acc 0.96
2016-09-06T14:39:54.832455: step 5038, loss 0.0291994, acc 0.98
2016-09-06T14:39:55.640869: step 5039, loss 0.0518202, acc 1
2016-09-06T14:39:56.461577: step 5040, loss 0.0328771, acc 0.98
2016-09-06T14:39:57.293418: step 5041, loss 0.0261718, acc 1
2016-09-06T14:39:58.109581: step 5042, loss 0.0627814, acc 0.96
2016-09-06T14:39:58.938556: step 5043, loss 0.00497125, acc 1
2016-09-06T14:39:59.781518: step 5044, loss 0.0150601, acc 1
2016-09-06T14:40:00.613604: step 5045, loss 0.00826371, acc 1
2016-09-06T14:40:01.443006: step 5046, loss 0.0181159, acc 1
2016-09-06T14:40:02.246167: step 5047, loss 0.0374356, acc 0.98
2016-09-06T14:40:03.057496: step 5048, loss 0.026802, acc 1
2016-09-06T14:40:03.878035: step 5049, loss 0.0220481, acc 0.98
2016-09-06T14:40:04.684300: step 5050, loss 0.0781699, acc 0.98
2016-09-06T14:40:05.482602: step 5051, loss 0.0324537, acc 0.98
2016-09-06T14:40:06.334468: step 5052, loss 0.0110771, acc 1
2016-09-06T14:40:07.152471: step 5053, loss 0.00276496, acc 1
2016-09-06T14:40:07.949319: step 5054, loss 0.0375784, acc 0.98
2016-09-06T14:40:08.805572: step 5055, loss 0.00340538, acc 1
2016-09-06T14:40:09.606986: step 5056, loss 0.0252816, acc 1
2016-09-06T14:40:10.413061: step 5057, loss 0.0122043, acc 1
2016-09-06T14:40:11.220910: step 5058, loss 0.0175737, acc 1
2016-09-06T14:40:12.047746: step 5059, loss 0.0606673, acc 0.98
2016-09-06T14:40:12.838174: step 5060, loss 0.125456, acc 0.92
2016-09-06T14:40:13.647794: step 5061, loss 0.0229554, acc 0.98
2016-09-06T14:40:14.476404: step 5062, loss 0.00430507, acc 1
2016-09-06T14:40:15.269407: step 5063, loss 0.016175, acc 1
2016-09-06T14:40:16.065745: step 5064, loss 0.00732348, acc 1
2016-09-06T14:40:16.857494: step 5065, loss 0.0161034, acc 1
2016-09-06T14:40:17.670104: step 5066, loss 0.00345616, acc 1
2016-09-06T14:40:18.554729: step 5067, loss 0.028004, acc 0.98
2016-09-06T14:40:19.373971: step 5068, loss 0.00897797, acc 1
2016-09-06T14:40:20.172528: step 5069, loss 0.00922823, acc 1
2016-09-06T14:40:20.973691: step 5070, loss 0.04572, acc 0.96
2016-09-06T14:40:21.767866: step 5071, loss 0.0172002, acc 1
2016-09-06T14:40:22.561424: step 5072, loss 0.00488856, acc 1
2016-09-06T14:40:23.399362: step 5073, loss 0.0162381, acc 1
2016-09-06T14:40:24.204848: step 5074, loss 0.0180699, acc 1
2016-09-06T14:40:25.006773: step 5075, loss 0.0795877, acc 0.98
2016-09-06T14:40:25.805890: step 5076, loss 0.0173279, acc 0.98
2016-09-06T14:40:26.616123: step 5077, loss 0.0340084, acc 0.98
2016-09-06T14:40:27.404901: step 5078, loss 0.00845467, acc 1
2016-09-06T14:40:28.186906: step 5079, loss 0.00277106, acc 1
2016-09-06T14:40:29.022766: step 5080, loss 0.0296288, acc 0.98
2016-09-06T14:40:29.810617: step 5081, loss 0.00922315, acc 1
2016-09-06T14:40:30.589009: step 5082, loss 0.0913418, acc 0.98
2016-09-06T14:40:31.399082: step 5083, loss 0.0120762, acc 1
2016-09-06T14:40:32.206380: step 5084, loss 0.0101189, acc 1
2016-09-06T14:40:33.008689: step 5085, loss 0.0113903, acc 1
2016-09-06T14:40:33.833870: step 5086, loss 0.00715911, acc 1
2016-09-06T14:40:34.613939: step 5087, loss 0.0431805, acc 0.98
2016-09-06T14:40:35.416107: step 5088, loss 0.0425685, acc 0.98
2016-09-06T14:40:36.247305: step 5089, loss 0.042424, acc 0.98
2016-09-06T14:40:37.069398: step 5090, loss 0.0441504, acc 0.98
2016-09-06T14:40:37.862009: step 5091, loss 0.0303517, acc 0.98
2016-09-06T14:40:38.708583: step 5092, loss 0.00678641, acc 1
2016-09-06T14:40:39.501888: step 5093, loss 0.00403968, acc 1
2016-09-06T14:40:40.297982: step 5094, loss 0.0050425, acc 1
2016-09-06T14:40:41.093310: step 5095, loss 0.00263123, acc 1
2016-09-06T14:40:41.889159: step 5096, loss 0.0269979, acc 0.98
2016-09-06T14:40:42.736483: step 5097, loss 0.0278034, acc 1
2016-09-06T14:40:43.542603: step 5098, loss 0.0295531, acc 0.98
2016-09-06T14:40:44.332307: step 5099, loss 0.00911737, acc 1
2016-09-06T14:40:45.129430: step 5100, loss 0.0625465, acc 0.98

Evaluation:
2016-09-06T14:40:48.925615: step 5100, loss 1.67025, acc 0.742026

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-5100

2016-09-06T14:40:50.804633: step 5101, loss 0.031566, acc 0.98
2016-09-06T14:40:51.590216: step 5102, loss 0.046701, acc 0.96
2016-09-06T14:40:52.374305: step 5103, loss 0.00376905, acc 1
2016-09-06T14:40:53.208163: step 5104, loss 0.0149306, acc 1
2016-09-06T14:40:54.030404: step 5105, loss 0.0177591, acc 1
2016-09-06T14:40:54.835010: step 5106, loss 0.0959768, acc 0.96
2016-09-06T14:40:55.640746: step 5107, loss 0.0373084, acc 0.98
2016-09-06T14:40:56.413826: step 5108, loss 0.0385138, acc 0.98
2016-09-06T14:40:57.227070: step 5109, loss 0.0398127, acc 0.98
2016-09-06T14:40:58.053426: step 5110, loss 0.00416596, acc 1
2016-09-06T14:40:58.819314: step 5111, loss 0.0089859, acc 1
2016-09-06T14:40:59.624141: step 5112, loss 0.0269379, acc 1
2016-09-06T14:41:00.479256: step 5113, loss 0.0210401, acc 1
2016-09-06T14:41:01.237053: step 5114, loss 0.00287005, acc 1
2016-09-06T14:41:02.051337: step 5115, loss 0.023366, acc 0.98
2016-09-06T14:41:02.864303: step 5116, loss 0.0352774, acc 0.98
2016-09-06T14:41:03.657289: step 5117, loss 0.0282617, acc 0.98
2016-09-06T14:41:04.444793: step 5118, loss 0.0272747, acc 0.98
2016-09-06T14:41:05.267198: step 5119, loss 0.0296021, acc 1
2016-09-06T14:41:06.061274: step 5120, loss 0.0200717, acc 1
2016-09-06T14:41:06.867088: step 5121, loss 0.02627, acc 1
2016-09-06T14:41:07.680624: step 5122, loss 0.031971, acc 0.98
2016-09-06T14:41:08.478929: step 5123, loss 0.0287742, acc 0.98
2016-09-06T14:41:09.284369: step 5124, loss 0.00931843, acc 1
2016-09-06T14:41:10.110586: step 5125, loss 0.0234435, acc 0.98
2016-09-06T14:41:10.894655: step 5126, loss 0.0557182, acc 0.96
2016-09-06T14:41:11.693475: step 5127, loss 0.00623343, acc 1
2016-09-06T14:41:12.504382: step 5128, loss 0.0209357, acc 1
2016-09-06T14:41:13.305035: step 5129, loss 0.0180857, acc 0.98
2016-09-06T14:41:14.089860: step 5130, loss 0.076589, acc 0.96
2016-09-06T14:41:14.920230: step 5131, loss 0.00862339, acc 1
2016-09-06T14:41:15.710273: step 5132, loss 0.00439593, acc 1
2016-09-06T14:41:16.517714: step 5133, loss 0.0379674, acc 1
2016-09-06T14:41:17.347336: step 5134, loss 0.0208102, acc 1
2016-09-06T14:41:18.140800: step 5135, loss 0.00798744, acc 1
2016-09-06T14:41:18.927466: step 5136, loss 0.015356, acc 1
2016-09-06T14:41:19.763632: step 5137, loss 0.0158503, acc 1
2016-09-06T14:41:20.553459: step 5138, loss 0.0211858, acc 0.98
2016-09-06T14:41:21.376795: step 5139, loss 0.0508874, acc 0.98
2016-09-06T14:41:22.199018: step 5140, loss 0.022456, acc 1
2016-09-06T14:41:22.991995: step 5141, loss 0.0993773, acc 0.96
2016-09-06T14:41:23.801824: step 5142, loss 0.0293123, acc 0.98
2016-09-06T14:41:24.606958: step 5143, loss 0.010329, acc 1
2016-09-06T14:41:25.396018: step 5144, loss 0.0136032, acc 1
2016-09-06T14:41:26.217495: step 5145, loss 0.0128513, acc 1
2016-09-06T14:41:27.060641: step 5146, loss 0.00560946, acc 1
2016-09-06T14:41:27.838704: step 5147, loss 0.0175379, acc 1
2016-09-06T14:41:28.654739: step 5148, loss 0.015765, acc 1
2016-09-06T14:41:29.495035: step 5149, loss 0.010466, acc 1
2016-09-06T14:41:30.270244: step 5150, loss 0.00336136, acc 1
2016-09-06T14:41:31.126597: step 5151, loss 0.0180046, acc 1
2016-09-06T14:41:31.980824: step 5152, loss 0.0901038, acc 0.96
2016-09-06T14:41:32.780964: step 5153, loss 0.0580771, acc 0.94
2016-09-06T14:41:33.618762: step 5154, loss 0.0302402, acc 0.98
2016-09-06T14:41:34.473282: step 5155, loss 0.00304935, acc 1
2016-09-06T14:41:35.285488: step 5156, loss 0.00367591, acc 1
2016-09-06T14:41:36.094338: step 5157, loss 0.00370049, acc 1
2016-09-06T14:41:36.954019: step 5158, loss 0.022066, acc 1
2016-09-06T14:41:37.772731: step 5159, loss 0.0282115, acc 0.98
2016-09-06T14:41:38.594576: step 5160, loss 0.00906853, acc 1
2016-09-06T14:41:39.446605: step 5161, loss 0.0629217, acc 0.96
2016-09-06T14:41:40.273409: step 5162, loss 0.0425521, acc 0.96
2016-09-06T14:41:41.139530: step 5163, loss 0.0169598, acc 1
2016-09-06T14:41:41.955340: step 5164, loss 0.0313427, acc 0.98
2016-09-06T14:41:42.778868: step 5165, loss 0.0221468, acc 0.98
2016-09-06T14:41:43.560555: step 5166, loss 0.016613, acc 1
2016-09-06T14:41:44.363583: step 5167, loss 0.0176631, acc 1
2016-09-06T14:41:45.183129: step 5168, loss 0.0036317, acc 1
2016-09-06T14:41:45.983498: step 5169, loss 0.0183358, acc 1
2016-09-06T14:41:46.782738: step 5170, loss 0.0482081, acc 1
2016-09-06T14:41:47.609352: step 5171, loss 0.0148879, acc 1
2016-09-06T14:41:48.372616: step 5172, loss 0.0969462, acc 0.96
2016-09-06T14:41:49.171473: step 5173, loss 0.0208897, acc 1
2016-09-06T14:41:49.987569: step 5174, loss 0.0418733, acc 0.98
2016-09-06T14:41:50.781096: step 5175, loss 0.00560404, acc 1
2016-09-06T14:41:51.586182: step 5176, loss 0.0713198, acc 0.96
2016-09-06T14:41:52.407672: step 5177, loss 0.0333427, acc 0.98
2016-09-06T14:41:53.190882: step 5178, loss 0.0270116, acc 1
2016-09-06T14:41:53.996100: step 5179, loss 0.0327779, acc 0.98
2016-09-06T14:41:54.849270: step 5180, loss 0.00487726, acc 1
2016-09-06T14:41:55.641496: step 5181, loss 0.0534405, acc 0.96
2016-09-06T14:41:56.460664: step 5182, loss 0.00505338, acc 1
2016-09-06T14:41:57.281381: step 5183, loss 0.0464356, acc 0.96
2016-09-06T14:41:58.010456: step 5184, loss 0.00952086, acc 1
2016-09-06T14:41:58.848720: step 5185, loss 0.0273972, acc 1
2016-09-06T14:41:59.658977: step 5186, loss 0.101599, acc 0.96
2016-09-06T14:42:00.471408: step 5187, loss 0.0166729, acc 1
2016-09-06T14:42:01.272075: step 5188, loss 0.0444946, acc 0.96
2016-09-06T14:42:02.097208: step 5189, loss 0.0191182, acc 1
2016-09-06T14:42:02.873931: step 5190, loss 0.0578543, acc 0.98
2016-09-06T14:42:03.666190: step 5191, loss 0.0353677, acc 1
2016-09-06T14:42:04.471370: step 5192, loss 0.0355629, acc 0.98
2016-09-06T14:42:05.269514: step 5193, loss 0.0235739, acc 1
2016-09-06T14:42:06.095410: step 5194, loss 0.140129, acc 0.96
2016-09-06T14:42:06.919205: step 5195, loss 0.0287894, acc 0.98
2016-09-06T14:42:07.719719: step 5196, loss 0.0627605, acc 0.98
2016-09-06T14:42:08.508445: step 5197, loss 0.0580255, acc 0.96
2016-09-06T14:42:09.330858: step 5198, loss 0.00842985, acc 1
2016-09-06T14:42:10.113368: step 5199, loss 0.0365709, acc 0.96
2016-09-06T14:42:10.921779: step 5200, loss 0.0171048, acc 1

Evaluation:
2016-09-06T14:42:14.696633: step 5200, loss 1.5909, acc 0.753283

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-5200

2016-09-06T14:42:16.568117: step 5201, loss 0.0219764, acc 0.98
2016-09-06T14:42:17.372639: step 5202, loss 0.00903666, acc 1
2016-09-06T14:42:18.195464: step 5203, loss 0.00360762, acc 1
2016-09-06T14:42:19.013876: step 5204, loss 0.00571871, acc 1
2016-09-06T14:42:19.824008: step 5205, loss 0.016292, acc 1
2016-09-06T14:42:20.619339: step 5206, loss 0.00467122, acc 1
2016-09-06T14:42:21.420792: step 5207, loss 0.0202606, acc 1
2016-09-06T14:42:22.195039: step 5208, loss 0.0593052, acc 0.94
2016-09-06T14:42:23.029099: step 5209, loss 0.0202444, acc 0.98
2016-09-06T14:42:23.843315: step 5210, loss 0.0257901, acc 1
2016-09-06T14:42:24.651270: step 5211, loss 0.0705522, acc 0.96
2016-09-06T14:42:25.470155: step 5212, loss 0.00699454, acc 1
2016-09-06T14:42:26.311662: step 5213, loss 0.00320967, acc 1
2016-09-06T14:42:27.098347: step 5214, loss 0.0273924, acc 0.98
2016-09-06T14:42:27.920149: step 5215, loss 0.0112299, acc 1
2016-09-06T14:42:28.740894: step 5216, loss 0.015399, acc 1
2016-09-06T14:42:29.519398: step 5217, loss 0.0029557, acc 1
2016-09-06T14:42:30.348936: step 5218, loss 0.0083844, acc 1
2016-09-06T14:42:31.158389: step 5219, loss 0.0212341, acc 1
2016-09-06T14:42:31.933405: step 5220, loss 0.00850061, acc 1
2016-09-06T14:42:32.716726: step 5221, loss 0.0257388, acc 0.98
2016-09-06T14:42:33.554417: step 5222, loss 0.0454614, acc 0.96
2016-09-06T14:42:34.333973: step 5223, loss 0.0754861, acc 0.98
2016-09-06T14:42:35.142407: step 5224, loss 0.0224452, acc 1
2016-09-06T14:42:35.992747: step 5225, loss 0.00436141, acc 1
2016-09-06T14:42:36.778518: step 5226, loss 0.0251857, acc 1
2016-09-06T14:42:37.565501: step 5227, loss 0.0810696, acc 0.94
2016-09-06T14:42:38.377872: step 5228, loss 0.032472, acc 0.98
2016-09-06T14:42:39.151514: step 5229, loss 0.00680836, acc 1
2016-09-06T14:42:39.977568: step 5230, loss 0.0213619, acc 0.98
2016-09-06T14:42:40.813019: step 5231, loss 0.0208672, acc 1
2016-09-06T14:42:41.616889: step 5232, loss 0.00855146, acc 1
2016-09-06T14:42:42.405448: step 5233, loss 0.00666872, acc 1
2016-09-06T14:42:43.229184: step 5234, loss 0.0373742, acc 0.98
2016-09-06T14:42:44.029123: step 5235, loss 0.019304, acc 1
2016-09-06T14:42:44.799300: step 5236, loss 0.0102328, acc 1
2016-09-06T14:42:45.628311: step 5237, loss 0.0146429, acc 1
2016-09-06T14:42:46.412853: step 5238, loss 0.059186, acc 0.98
2016-09-06T14:42:47.249198: step 5239, loss 0.0226386, acc 0.98
2016-09-06T14:42:48.072492: step 5240, loss 0.0187374, acc 0.98
2016-09-06T14:42:48.871646: step 5241, loss 0.0578751, acc 0.96
2016-09-06T14:42:49.681887: step 5242, loss 0.00994221, acc 1
2016-09-06T14:42:50.574881: step 5243, loss 0.0356406, acc 0.98
2016-09-06T14:42:51.373224: step 5244, loss 0.00342427, acc 1
2016-09-06T14:42:52.178908: step 5245, loss 0.0568489, acc 0.98
2016-09-06T14:42:53.014120: step 5246, loss 0.0290938, acc 0.98
2016-09-06T14:42:53.815926: step 5247, loss 0.0408297, acc 0.98
2016-09-06T14:42:54.645507: step 5248, loss 0.0700374, acc 0.96
2016-09-06T14:42:55.467142: step 5249, loss 0.0286301, acc 0.98
2016-09-06T14:42:56.268354: step 5250, loss 0.00884974, acc 1
2016-09-06T14:42:57.065225: step 5251, loss 0.0400428, acc 0.98
2016-09-06T14:42:57.919827: step 5252, loss 0.0475681, acc 0.98
2016-09-06T14:42:58.707122: step 5253, loss 0.0584846, acc 0.94
2016-09-06T14:42:59.533717: step 5254, loss 0.017209, acc 1
2016-09-06T14:43:00.364761: step 5255, loss 0.00475238, acc 1
2016-09-06T14:43:01.170435: step 5256, loss 0.00739947, acc 1
2016-09-06T14:43:01.975811: step 5257, loss 0.0207436, acc 1
2016-09-06T14:43:02.795397: step 5258, loss 0.0196723, acc 0.98
2016-09-06T14:43:03.595283: step 5259, loss 0.0259149, acc 1
2016-09-06T14:43:04.414237: step 5260, loss 0.0368686, acc 0.98
2016-09-06T14:43:05.229563: step 5261, loss 0.0475101, acc 0.98
2016-09-06T14:43:06.021777: step 5262, loss 0.0121395, acc 1
2016-09-06T14:43:06.838117: step 5263, loss 0.0137925, acc 1
2016-09-06T14:43:07.669835: step 5264, loss 0.00364201, acc 1
2016-09-06T14:43:08.466615: step 5265, loss 0.00271911, acc 1
2016-09-06T14:43:09.316703: step 5266, loss 0.00406025, acc 1
2016-09-06T14:43:10.144997: step 5267, loss 0.0351717, acc 0.98
2016-09-06T14:43:10.968310: step 5268, loss 0.00957769, acc 1
2016-09-06T14:43:11.758681: step 5269, loss 0.00753184, acc 1
2016-09-06T14:43:12.550453: step 5270, loss 0.0342069, acc 0.98
2016-09-06T14:43:13.368618: step 5271, loss 0.0212259, acc 1
2016-09-06T14:43:14.153521: step 5272, loss 0.0247063, acc 0.98
2016-09-06T14:43:14.964949: step 5273, loss 0.0053944, acc 1
2016-09-06T14:43:15.795073: step 5274, loss 0.00764923, acc 1
2016-09-06T14:43:16.589442: step 5275, loss 0.116158, acc 0.96
2016-09-06T14:43:17.416720: step 5276, loss 0.014587, acc 1
2016-09-06T14:43:18.251319: step 5277, loss 0.0224634, acc 1
2016-09-06T14:43:19.036376: step 5278, loss 0.039024, acc 0.98
2016-09-06T14:43:19.830980: step 5279, loss 0.00365591, acc 1
2016-09-06T14:43:20.662422: step 5280, loss 0.00425741, acc 1
2016-09-06T14:43:21.470604: step 5281, loss 0.0448289, acc 0.98
2016-09-06T14:43:22.280706: step 5282, loss 0.0450383, acc 0.98
2016-09-06T14:43:23.087335: step 5283, loss 0.0069224, acc 1
2016-09-06T14:43:23.911111: step 5284, loss 0.0475119, acc 0.98
2016-09-06T14:43:24.712562: step 5285, loss 0.0118041, acc 1
2016-09-06T14:43:25.550088: step 5286, loss 0.024394, acc 0.98
2016-09-06T14:43:26.354593: step 5287, loss 0.053683, acc 0.98
2016-09-06T14:43:27.184068: step 5288, loss 0.0032757, acc 1
2016-09-06T14:43:28.022608: step 5289, loss 0.0208552, acc 1
2016-09-06T14:43:28.837214: step 5290, loss 0.0174115, acc 1
2016-09-06T14:43:29.639113: step 5291, loss 0.0443987, acc 1
2016-09-06T14:43:30.473755: step 5292, loss 0.0275466, acc 0.98
2016-09-06T14:43:31.278581: step 5293, loss 0.0190621, acc 1
2016-09-06T14:43:32.095470: step 5294, loss 0.128681, acc 0.92
2016-09-06T14:43:32.914601: step 5295, loss 0.00311758, acc 1
2016-09-06T14:43:33.733321: step 5296, loss 0.018611, acc 1
2016-09-06T14:43:34.540577: step 5297, loss 0.011373, acc 1
2016-09-06T14:43:35.382849: step 5298, loss 0.00746174, acc 1
2016-09-06T14:43:36.186635: step 5299, loss 0.0424377, acc 0.96
2016-09-06T14:43:36.969440: step 5300, loss 0.0136151, acc 1

Evaluation:
2016-09-06T14:43:40.693190: step 5300, loss 1.80373, acc 0.757036

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-5300

2016-09-06T14:43:42.770160: step 5301, loss 0.0362954, acc 0.98
2016-09-06T14:43:43.611489: step 5302, loss 0.00275396, acc 1
2016-09-06T14:43:44.458224: step 5303, loss 0.0111411, acc 1
2016-09-06T14:43:45.315048: step 5304, loss 0.0168441, acc 1
2016-09-06T14:43:46.159841: step 5305, loss 0.0263661, acc 0.98
2016-09-06T14:43:46.958588: step 5306, loss 0.00434225, acc 1
2016-09-06T14:43:47.723247: step 5307, loss 0.0157444, acc 1
2016-09-06T14:43:48.564215: step 5308, loss 0.0259229, acc 0.98
2016-09-06T14:43:49.362583: step 5309, loss 0.0710727, acc 0.98
2016-09-06T14:43:50.146025: step 5310, loss 0.0121486, acc 1
2016-09-06T14:43:50.968508: step 5311, loss 0.0178084, acc 1
2016-09-06T14:43:51.755020: step 5312, loss 0.0294963, acc 1
2016-09-06T14:43:52.567239: step 5313, loss 0.00843719, acc 1
2016-09-06T14:43:53.379162: step 5314, loss 0.0395628, acc 0.98
2016-09-06T14:43:54.184912: step 5315, loss 0.0511612, acc 0.98
2016-09-06T14:43:54.997401: step 5316, loss 0.0463507, acc 0.98
2016-09-06T14:43:55.818918: step 5317, loss 0.104352, acc 0.98
2016-09-06T14:43:56.591430: step 5318, loss 0.0232734, acc 1
2016-09-06T14:43:57.397369: step 5319, loss 0.0256399, acc 0.98
2016-09-06T14:43:58.251849: step 5320, loss 0.00631841, acc 1
2016-09-06T14:43:59.053210: step 5321, loss 0.0290547, acc 0.98
2016-09-06T14:43:59.860551: step 5322, loss 0.0351989, acc 1
2016-09-06T14:44:00.709604: step 5323, loss 0.00376184, acc 1
2016-09-06T14:44:01.526287: step 5324, loss 0.0328974, acc 0.98
2016-09-06T14:44:02.358831: step 5325, loss 0.063591, acc 0.98
2016-09-06T14:44:03.186618: step 5326, loss 0.0033559, acc 1
2016-09-06T14:44:03.997935: step 5327, loss 0.0136953, acc 1
2016-09-06T14:44:04.801993: step 5328, loss 0.0303727, acc 0.98
2016-09-06T14:44:05.634148: step 5329, loss 0.0133247, acc 1
2016-09-06T14:44:06.443407: step 5330, loss 0.0113776, acc 1
2016-09-06T14:44:07.274547: step 5331, loss 0.029537, acc 0.98
2016-09-06T14:44:08.110342: step 5332, loss 0.0425058, acc 0.96
2016-09-06T14:44:08.917829: step 5333, loss 0.0524481, acc 0.98
2016-09-06T14:44:09.736277: step 5334, loss 0.0273328, acc 1
2016-09-06T14:44:10.553569: step 5335, loss 0.0638826, acc 0.98
2016-09-06T14:44:11.363585: step 5336, loss 0.0341225, acc 0.96
2016-09-06T14:44:12.167440: step 5337, loss 0.00685617, acc 1
2016-09-06T14:44:12.987386: step 5338, loss 0.00650559, acc 1
2016-09-06T14:44:13.832958: step 5339, loss 0.0404351, acc 0.98
2016-09-06T14:44:14.657751: step 5340, loss 0.0595706, acc 0.96
2016-09-06T14:44:15.490979: step 5341, loss 0.00573058, acc 1
2016-09-06T14:44:16.309180: step 5342, loss 0.0127392, acc 1
2016-09-06T14:44:17.112717: step 5343, loss 0.0429329, acc 0.98
2016-09-06T14:44:17.910548: step 5344, loss 0.0628902, acc 0.96
2016-09-06T14:44:18.725891: step 5345, loss 0.0612565, acc 0.96
2016-09-06T14:44:19.525189: step 5346, loss 0.0355885, acc 0.98
2016-09-06T14:44:20.313584: step 5347, loss 0.0284791, acc 0.98
2016-09-06T14:44:21.131597: step 5348, loss 0.0221195, acc 0.98
2016-09-06T14:44:21.916940: step 5349, loss 0.0378484, acc 0.98
2016-09-06T14:44:22.720226: step 5350, loss 0.0688468, acc 0.98
2016-09-06T14:44:23.517000: step 5351, loss 0.0422358, acc 0.98
2016-09-06T14:44:24.335202: step 5352, loss 0.00850076, acc 1
2016-09-06T14:44:25.168811: step 5353, loss 0.0407993, acc 1
2016-09-06T14:44:26.008050: step 5354, loss 0.0185221, acc 1
2016-09-06T14:44:26.832738: step 5355, loss 0.0170671, acc 1
2016-09-06T14:44:27.628247: step 5356, loss 0.00813964, acc 1
2016-09-06T14:44:28.458838: step 5357, loss 0.0319855, acc 0.98
2016-09-06T14:44:29.258811: step 5358, loss 0.00614012, acc 1
2016-09-06T14:44:30.080543: step 5359, loss 0.0328023, acc 0.98
2016-09-06T14:44:30.903886: step 5360, loss 0.00512185, acc 1
2016-09-06T14:44:31.729199: step 5361, loss 0.0485595, acc 0.96
2016-09-06T14:44:32.524700: step 5362, loss 0.0459893, acc 1
2016-09-06T14:44:33.354383: step 5363, loss 0.045396, acc 0.98
2016-09-06T14:44:34.187604: step 5364, loss 0.0633894, acc 0.98
2016-09-06T14:44:35.015150: step 5365, loss 0.0284209, acc 0.98
2016-09-06T14:44:35.907275: step 5366, loss 0.0201373, acc 1
2016-09-06T14:44:36.738138: step 5367, loss 0.00355629, acc 1
2016-09-06T14:44:37.549660: step 5368, loss 0.0304804, acc 1
2016-09-06T14:44:38.414592: step 5369, loss 0.0670427, acc 0.98
2016-09-06T14:44:39.223976: step 5370, loss 0.0104535, acc 1
2016-09-06T14:44:40.031578: step 5371, loss 0.0221182, acc 1
2016-09-06T14:44:40.847265: step 5372, loss 0.0259307, acc 0.98
2016-09-06T14:44:41.677855: step 5373, loss 0.0497647, acc 0.98
2016-09-06T14:44:42.512842: step 5374, loss 0.0256934, acc 1
2016-09-06T14:44:43.299503: step 5375, loss 0.0390399, acc 0.98
2016-09-06T14:44:44.066062: step 5376, loss 0.00317429, acc 1
2016-09-06T14:44:44.883821: step 5377, loss 0.0608464, acc 0.98
2016-09-06T14:44:45.716755: step 5378, loss 0.0430037, acc 0.98
2016-09-06T14:44:46.557359: step 5379, loss 0.0219193, acc 0.98
2016-09-06T14:44:47.348788: step 5380, loss 0.0220485, acc 1
2016-09-06T14:44:48.121874: step 5381, loss 0.0299048, acc 1
2016-09-06T14:44:48.938545: step 5382, loss 0.0143631, acc 1
2016-09-06T14:44:49.729449: step 5383, loss 0.0110066, acc 1
2016-09-06T14:44:50.545106: step 5384, loss 0.0183987, acc 1
2016-09-06T14:44:51.356084: step 5385, loss 0.0655435, acc 0.96
2016-09-06T14:44:52.156655: step 5386, loss 0.0877417, acc 0.94
2016-09-06T14:44:52.972224: step 5387, loss 0.00475966, acc 1
2016-09-06T14:44:53.805483: step 5388, loss 0.0154029, acc 1
2016-09-06T14:44:54.580048: step 5389, loss 0.0552463, acc 0.98
2016-09-06T14:44:55.399330: step 5390, loss 0.0377438, acc 0.98
2016-09-06T14:44:56.202947: step 5391, loss 0.0529563, acc 0.96
2016-09-06T14:44:56.976989: step 5392, loss 0.0083506, acc 1
2016-09-06T14:44:57.778454: step 5393, loss 0.0242994, acc 0.98
2016-09-06T14:44:58.610671: step 5394, loss 0.0125856, acc 1
2016-09-06T14:44:59.401374: step 5395, loss 0.0295754, acc 0.98
2016-09-06T14:45:00.192123: step 5396, loss 0.0247192, acc 0.98
2016-09-06T14:45:01.044377: step 5397, loss 0.00942792, acc 1
2016-09-06T14:45:01.846050: step 5398, loss 0.0195923, acc 0.98
2016-09-06T14:45:02.642671: step 5399, loss 0.00590096, acc 1
2016-09-06T14:45:03.451996: step 5400, loss 0.0351022, acc 1

Evaluation:
2016-09-06T14:45:07.169291: step 5400, loss 1.60775, acc 0.755159

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-5400

2016-09-06T14:45:09.028597: step 5401, loss 0.0578118, acc 0.98
2016-09-06T14:45:09.874034: step 5402, loss 0.0169042, acc 1
2016-09-06T14:45:10.714114: step 5403, loss 0.0296207, acc 0.98
2016-09-06T14:45:11.520272: step 5404, loss 0.0187531, acc 1
2016-09-06T14:45:12.332917: step 5405, loss 0.0293153, acc 0.98
2016-09-06T14:45:13.130237: step 5406, loss 0.0128789, acc 1
2016-09-06T14:45:13.914904: step 5407, loss 0.00382155, acc 1
2016-09-06T14:45:14.718931: step 5408, loss 0.0438314, acc 0.96
2016-09-06T14:45:15.564259: step 5409, loss 0.0454144, acc 0.98
2016-09-06T14:45:16.364088: step 5410, loss 0.0574543, acc 0.96
2016-09-06T14:45:17.176141: step 5411, loss 0.0131478, acc 1
2016-09-06T14:45:18.011599: step 5412, loss 0.00381828, acc 1
2016-09-06T14:45:18.797424: step 5413, loss 0.0342641, acc 0.98
2016-09-06T14:45:19.591468: step 5414, loss 0.0420729, acc 0.98
2016-09-06T14:45:20.394618: step 5415, loss 0.0303146, acc 0.98
2016-09-06T14:45:21.163474: step 5416, loss 0.0146548, acc 1
2016-09-06T14:45:22.009360: step 5417, loss 0.0163212, acc 1
2016-09-06T14:45:22.805224: step 5418, loss 0.0280609, acc 1
2016-09-06T14:45:23.620203: step 5419, loss 0.0393005, acc 0.98
2016-09-06T14:45:24.421326: step 5420, loss 0.0412354, acc 0.96
2016-09-06T14:45:25.260610: step 5421, loss 0.0373721, acc 0.98
2016-09-06T14:45:26.025228: step 5422, loss 0.0642304, acc 0.96
2016-09-06T14:45:26.821890: step 5423, loss 0.0416865, acc 0.98
2016-09-06T14:45:27.632924: step 5424, loss 0.0212075, acc 0.98
2016-09-06T14:45:28.423039: step 5425, loss 0.0112803, acc 1
2016-09-06T14:45:29.244834: step 5426, loss 0.0657123, acc 0.96
2016-09-06T14:45:30.069693: step 5427, loss 0.0228654, acc 0.98
2016-09-06T14:45:30.871411: step 5428, loss 0.0108305, acc 1
2016-09-06T14:45:31.694846: step 5429, loss 0.01165, acc 1
2016-09-06T14:45:32.503053: step 5430, loss 0.131981, acc 0.98
2016-09-06T14:45:33.266823: step 5431, loss 0.0297688, acc 0.98
2016-09-06T14:45:34.083324: step 5432, loss 0.0109945, acc 1
2016-09-06T14:45:34.917214: step 5433, loss 0.0355302, acc 0.98
2016-09-06T14:45:35.699842: step 5434, loss 0.0160743, acc 1
2016-09-06T14:45:36.525364: step 5435, loss 0.0742054, acc 0.94
2016-09-06T14:45:37.334889: step 5436, loss 0.0331715, acc 0.98
2016-09-06T14:45:38.137642: step 5437, loss 0.00346184, acc 1
2016-09-06T14:45:38.909683: step 5438, loss 0.12975, acc 0.96
2016-09-06T14:45:39.761889: step 5439, loss 0.00568073, acc 1
2016-09-06T14:45:40.551031: step 5440, loss 0.0669795, acc 0.96
2016-09-06T14:45:41.374221: step 5441, loss 0.00332255, acc 1
2016-09-06T14:45:42.187256: step 5442, loss 0.0251429, acc 1
2016-09-06T14:45:42.971161: step 5443, loss 0.0157058, acc 1
2016-09-06T14:45:43.781935: step 5444, loss 0.0509918, acc 0.98
2016-09-06T14:45:44.588910: step 5445, loss 0.027971, acc 1
2016-09-06T14:45:45.379115: step 5446, loss 0.0334629, acc 0.98
2016-09-06T14:45:46.181439: step 5447, loss 0.0463803, acc 0.98
2016-09-06T14:45:47.009790: step 5448, loss 0.0093505, acc 1
2016-09-06T14:45:47.810857: step 5449, loss 0.162737, acc 0.96
2016-09-06T14:45:48.612277: step 5450, loss 0.0123675, acc 1
2016-09-06T14:45:49.412659: step 5451, loss 0.0181137, acc 1
2016-09-06T14:45:50.182775: step 5452, loss 0.0408438, acc 1
2016-09-06T14:45:50.992619: step 5453, loss 0.039964, acc 0.98
2016-09-06T14:45:51.804702: step 5454, loss 0.0505227, acc 0.98
2016-09-06T14:45:52.591352: step 5455, loss 0.0441856, acc 0.98
2016-09-06T14:45:53.441747: step 5456, loss 0.0141628, acc 1
2016-09-06T14:45:54.253185: step 5457, loss 0.0911445, acc 0.98
2016-09-06T14:45:55.045234: step 5458, loss 0.0199154, acc 0.98
2016-09-06T14:45:55.853848: step 5459, loss 0.0314403, acc 0.98
2016-09-06T14:45:56.665112: step 5460, loss 0.00434887, acc 1
2016-09-06T14:45:57.426371: step 5461, loss 0.0361578, acc 0.98
2016-09-06T14:45:58.230915: step 5462, loss 0.0261945, acc 0.98
2016-09-06T14:45:59.053454: step 5463, loss 0.00405872, acc 1
2016-09-06T14:45:59.819381: step 5464, loss 0.0211732, acc 1
2016-09-06T14:46:00.669827: step 5465, loss 0.0256151, acc 1
2016-09-06T14:46:01.493946: step 5466, loss 0.00347685, acc 1
2016-09-06T14:46:02.305359: step 5467, loss 0.0237773, acc 0.98
2016-09-06T14:46:03.108438: step 5468, loss 0.00328727, acc 1
2016-09-06T14:46:03.916725: step 5469, loss 0.00970525, acc 1
2016-09-06T14:46:04.705347: step 5470, loss 0.0342147, acc 0.98
2016-09-06T14:46:05.501868: step 5471, loss 0.0353727, acc 0.98
2016-09-06T14:46:06.293165: step 5472, loss 0.0187263, acc 1
2016-09-06T14:46:07.067488: step 5473, loss 0.0382302, acc 0.98
2016-09-06T14:46:07.917547: step 5474, loss 0.0205866, acc 1
2016-09-06T14:46:08.723876: step 5475, loss 0.0334666, acc 0.98
2016-09-06T14:46:09.495423: step 5476, loss 0.0419401, acc 0.98
2016-09-06T14:46:10.308377: step 5477, loss 0.0421417, acc 0.98
2016-09-06T14:46:11.161612: step 5478, loss 0.0284662, acc 0.98
2016-09-06T14:46:11.965296: step 5479, loss 0.0153741, acc 1
2016-09-06T14:46:12.793392: step 5480, loss 0.00930717, acc 1
2016-09-06T14:46:13.598540: step 5481, loss 0.0846268, acc 0.92
2016-09-06T14:46:14.388403: step 5482, loss 0.0264057, acc 0.98
2016-09-06T14:46:15.201073: step 5483, loss 0.0110556, acc 1
2016-09-06T14:46:16.040653: step 5484, loss 0.00591731, acc 1
2016-09-06T14:46:16.839265: step 5485, loss 0.0667013, acc 0.96
2016-09-06T14:46:17.655430: step 5486, loss 0.071094, acc 0.96
2016-09-06T14:46:18.510428: step 5487, loss 0.00364601, acc 1
2016-09-06T14:46:19.313046: step 5488, loss 0.0414827, acc 0.98
2016-09-06T14:46:20.120971: step 5489, loss 0.0643227, acc 0.96
2016-09-06T14:46:20.959509: step 5490, loss 0.0072972, acc 1
2016-09-06T14:46:21.774451: step 5491, loss 0.0115646, acc 1
2016-09-06T14:46:22.591246: step 5492, loss 0.0248857, acc 1
2016-09-06T14:46:23.442162: step 5493, loss 0.0231903, acc 1
2016-09-06T14:46:24.247922: step 5494, loss 0.0056951, acc 1
2016-09-06T14:46:25.053418: step 5495, loss 0.0201473, acc 1
2016-09-06T14:46:25.884738: step 5496, loss 0.00675468, acc 1
2016-09-06T14:46:26.689812: step 5497, loss 0.0225018, acc 1
2016-09-06T14:46:27.517834: step 5498, loss 0.0524447, acc 0.98
2016-09-06T14:46:28.359360: step 5499, loss 0.0137622, acc 1
2016-09-06T14:46:29.197436: step 5500, loss 0.0052095, acc 1

Evaluation:
2016-09-06T14:46:32.924484: step 5500, loss 1.81872, acc 0.756098

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-5500

2016-09-06T14:46:34.750344: step 5501, loss 0.0247985, acc 1
2016-09-06T14:46:35.579369: step 5502, loss 0.00840469, acc 1
2016-09-06T14:46:36.394804: step 5503, loss 0.0551845, acc 0.98
2016-09-06T14:46:37.204220: step 5504, loss 0.00566584, acc 1
2016-09-06T14:46:38.016372: step 5505, loss 0.0103497, acc 1
2016-09-06T14:46:38.822386: step 5506, loss 0.0232903, acc 0.98
2016-09-06T14:46:39.623848: step 5507, loss 0.0276691, acc 1
2016-09-06T14:46:40.446510: step 5508, loss 0.00387341, acc 1
2016-09-06T14:46:41.245833: step 5509, loss 0.0210382, acc 1
2016-09-06T14:46:42.073663: step 5510, loss 0.0409981, acc 0.96
2016-09-06T14:46:42.912475: step 5511, loss 0.0823705, acc 0.96
2016-09-06T14:46:43.717676: step 5512, loss 0.0140039, acc 1
2016-09-06T14:46:44.523462: step 5513, loss 0.0168442, acc 1
2016-09-06T14:46:45.343037: step 5514, loss 0.0158611, acc 1
2016-09-06T14:46:46.161612: step 5515, loss 0.0125, acc 1
2016-09-06T14:46:47.021507: step 5516, loss 0.0423088, acc 0.98
2016-09-06T14:46:47.836160: step 5517, loss 0.0162204, acc 1
2016-09-06T14:46:48.641513: step 5518, loss 0.00346208, acc 1
2016-09-06T14:46:49.442285: step 5519, loss 0.00339197, acc 1
2016-09-06T14:46:50.256047: step 5520, loss 0.0981681, acc 0.96
2016-09-06T14:46:51.063406: step 5521, loss 0.0163513, acc 1
2016-09-06T14:46:51.873884: step 5522, loss 0.0161381, acc 1
2016-09-06T14:46:52.713799: step 5523, loss 0.0516539, acc 0.98
2016-09-06T14:46:53.529504: step 5524, loss 0.018531, acc 1
2016-09-06T14:46:54.331974: step 5525, loss 0.0335626, acc 1
2016-09-06T14:46:55.162433: step 5526, loss 0.0597924, acc 0.98
2016-09-06T14:46:55.970195: step 5527, loss 0.0326746, acc 0.98
2016-09-06T14:46:56.748583: step 5528, loss 0.0408783, acc 0.98
2016-09-06T14:46:57.555342: step 5529, loss 0.0575147, acc 0.98
2016-09-06T14:46:58.369349: step 5530, loss 0.00497966, acc 1
2016-09-06T14:46:59.173576: step 5531, loss 0.00471309, acc 1
2016-09-06T14:46:59.992028: step 5532, loss 0.0678099, acc 0.96
2016-09-06T14:47:00.854645: step 5533, loss 0.0401981, acc 0.98
2016-09-06T14:47:01.648348: step 5534, loss 0.037325, acc 0.98
2016-09-06T14:47:02.456662: step 5535, loss 0.00771935, acc 1
2016-09-06T14:47:03.262688: step 5536, loss 0.0333116, acc 0.98
2016-09-06T14:47:04.088868: step 5537, loss 0.0949188, acc 0.98
2016-09-06T14:47:04.895385: step 5538, loss 0.171865, acc 0.94
2016-09-06T14:47:05.737834: step 5539, loss 0.00367445, acc 1
2016-09-06T14:47:06.561873: step 5540, loss 0.0270868, acc 0.98
2016-09-06T14:47:07.408971: step 5541, loss 0.0254028, acc 0.98
2016-09-06T14:47:08.217517: step 5542, loss 0.0242642, acc 0.98
2016-09-06T14:47:09.028247: step 5543, loss 0.0205924, acc 1
2016-09-06T14:47:09.842016: step 5544, loss 0.0257569, acc 0.98
2016-09-06T14:47:10.699979: step 5545, loss 0.0833298, acc 0.94
2016-09-06T14:47:11.480927: step 5546, loss 0.0231876, acc 0.98
2016-09-06T14:47:12.295591: step 5547, loss 0.0910767, acc 0.96
2016-09-06T14:47:13.128923: step 5548, loss 0.00700657, acc 1
2016-09-06T14:47:13.941938: step 5549, loss 0.0529124, acc 0.96
2016-09-06T14:47:14.758625: step 5550, loss 0.024568, acc 1
2016-09-06T14:47:15.579251: step 5551, loss 0.102827, acc 0.92
2016-09-06T14:47:16.397745: step 5552, loss 0.0206343, acc 1
2016-09-06T14:47:17.207757: step 5553, loss 0.0327627, acc 1
2016-09-06T14:47:18.070111: step 5554, loss 0.0377962, acc 1
2016-09-06T14:47:18.922884: step 5555, loss 0.00625975, acc 1
2016-09-06T14:47:19.706987: step 5556, loss 0.0402106, acc 0.98
2016-09-06T14:47:20.514492: step 5557, loss 0.00375331, acc 1
2016-09-06T14:47:21.336624: step 5558, loss 0.0220065, acc 1
2016-09-06T14:47:22.131149: step 5559, loss 0.00405695, acc 1
2016-09-06T14:47:22.993084: step 5560, loss 0.00352869, acc 1
2016-09-06T14:47:23.810801: step 5561, loss 0.0378995, acc 0.98
2016-09-06T14:47:24.579440: step 5562, loss 0.015404, acc 1
2016-09-06T14:47:25.389942: step 5563, loss 0.022335, acc 1
2016-09-06T14:47:26.218458: step 5564, loss 0.0226893, acc 0.98
2016-09-06T14:47:26.999576: step 5565, loss 0.0534064, acc 0.94
2016-09-06T14:47:27.797335: step 5566, loss 0.0557853, acc 0.98
2016-09-06T14:47:28.637684: step 5567, loss 0.0130044, acc 1
2016-09-06T14:47:29.378474: step 5568, loss 0.00637573, acc 1
2016-09-06T14:47:30.190511: step 5569, loss 0.0431382, acc 1
2016-09-06T14:47:30.999816: step 5570, loss 0.0854777, acc 0.94
2016-09-06T14:47:31.802149: step 5571, loss 0.0129714, acc 1
2016-09-06T14:47:32.604829: step 5572, loss 0.0719405, acc 0.96
2016-09-06T14:47:33.431260: step 5573, loss 0.0193455, acc 0.98
2016-09-06T14:47:34.240583: step 5574, loss 0.0530208, acc 0.96
2016-09-06T14:47:35.019201: step 5575, loss 0.00954327, acc 1
2016-09-06T14:47:35.837865: step 5576, loss 0.0778416, acc 0.98
2016-09-06T14:47:36.631600: step 5577, loss 0.0664826, acc 0.94
2016-09-06T14:47:37.443675: step 5578, loss 0.0261508, acc 0.98
2016-09-06T14:47:38.250849: step 5579, loss 0.00323503, acc 1
2016-09-06T14:47:39.040281: step 5580, loss 0.0267785, acc 0.98
2016-09-06T14:47:39.828527: step 5581, loss 0.0167189, acc 1
2016-09-06T14:47:40.621974: step 5582, loss 0.0200106, acc 0.98
2016-09-06T14:47:41.425524: step 5583, loss 0.0359457, acc 1
2016-09-06T14:47:42.253656: step 5584, loss 0.0205231, acc 1
2016-09-06T14:47:43.057493: step 5585, loss 0.079305, acc 0.96
2016-09-06T14:47:43.909277: step 5586, loss 0.00310342, acc 1
2016-09-06T14:47:44.761501: step 5587, loss 0.015825, acc 1
2016-09-06T14:47:45.566926: step 5588, loss 0.00730637, acc 1
2016-09-06T14:47:46.333203: step 5589, loss 0.0231686, acc 1
2016-09-06T14:47:47.134185: step 5590, loss 0.0067664, acc 1
2016-09-06T14:47:47.979145: step 5591, loss 0.00371692, acc 1
2016-09-06T14:47:48.771652: step 5592, loss 0.0357665, acc 1
2016-09-06T14:47:49.573384: step 5593, loss 0.0187284, acc 0.98
2016-09-06T14:47:50.398105: step 5594, loss 0.0139918, acc 1
2016-09-06T14:47:51.228524: step 5595, loss 0.0222268, acc 1
2016-09-06T14:47:52.041081: step 5596, loss 0.0142331, acc 1
2016-09-06T14:47:52.849085: step 5597, loss 0.00462844, acc 1
2016-09-06T14:47:53.663749: step 5598, loss 0.0395406, acc 1
2016-09-06T14:47:54.494557: step 5599, loss 0.0028886, acc 1
2016-09-06T14:47:55.335056: step 5600, loss 0.0193524, acc 1

Evaluation:
2016-09-06T14:47:59.052908: step 5600, loss 1.78848, acc 0.757974

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-5600

2016-09-06T14:48:00.998426: step 5601, loss 0.07725, acc 0.98
2016-09-06T14:48:01.790025: step 5602, loss 0.0229003, acc 0.98
2016-09-06T14:48:02.645476: step 5603, loss 0.00467393, acc 1
2016-09-06T14:48:03.435457: step 5604, loss 0.00716772, acc 1
2016-09-06T14:48:04.243872: step 5605, loss 0.0135643, acc 1
2016-09-06T14:48:05.046031: step 5606, loss 0.0111284, acc 1
2016-09-06T14:48:05.855129: step 5607, loss 0.0123561, acc 1
2016-09-06T14:48:06.675178: step 5608, loss 0.00366494, acc 1
2016-09-06T14:48:07.522185: step 5609, loss 0.0202667, acc 0.98
2016-09-06T14:48:08.296230: step 5610, loss 0.0190216, acc 1
2016-09-06T14:48:09.122092: step 5611, loss 0.0096557, acc 1
2016-09-06T14:48:09.972740: step 5612, loss 0.00736251, acc 1
2016-09-06T14:48:10.797653: step 5613, loss 0.100649, acc 0.98
2016-09-06T14:48:11.661422: step 5614, loss 0.0211765, acc 0.98
2016-09-06T14:48:12.499029: step 5615, loss 0.0255491, acc 0.98
2016-09-06T14:48:13.308322: step 5616, loss 0.00692844, acc 1
2016-09-06T14:48:14.111504: step 5617, loss 0.0819309, acc 0.98
2016-09-06T14:48:14.990791: step 5618, loss 0.0234158, acc 0.98
2016-09-06T14:48:15.793976: step 5619, loss 0.00904343, acc 1
2016-09-06T14:48:16.605394: step 5620, loss 0.00311105, acc 1
2016-09-06T14:48:17.425246: step 5621, loss 0.0183042, acc 1
2016-09-06T14:48:18.244320: step 5622, loss 0.00336824, acc 1
2016-09-06T14:48:19.049238: step 5623, loss 0.0200361, acc 1
2016-09-06T14:48:19.876601: step 5624, loss 0.140026, acc 0.96
2016-09-06T14:48:20.685438: step 5625, loss 0.0237224, acc 0.98
2016-09-06T14:48:21.469897: step 5626, loss 0.00545595, acc 1
2016-09-06T14:48:22.298930: step 5627, loss 0.0443863, acc 0.96
2016-09-06T14:48:23.123308: step 5628, loss 0.0220976, acc 1
2016-09-06T14:48:23.917600: step 5629, loss 0.0393437, acc 0.98
2016-09-06T14:48:24.741452: step 5630, loss 0.0256211, acc 0.98
2016-09-06T14:48:25.576209: step 5631, loss 0.0193611, acc 1
2016-09-06T14:48:26.399940: step 5632, loss 0.07917, acc 0.98
2016-09-06T14:48:27.208694: step 5633, loss 0.0504629, acc 0.96
2016-09-06T14:48:28.039128: step 5634, loss 0.0483348, acc 0.98
2016-09-06T14:48:28.851831: step 5635, loss 0.12398, acc 0.94
2016-09-06T14:48:29.663377: step 5636, loss 0.0313685, acc 0.98
2016-09-06T14:48:30.488720: step 5637, loss 0.0408873, acc 0.98
2016-09-06T14:48:31.309822: step 5638, loss 0.0309443, acc 0.98
2016-09-06T14:48:32.131351: step 5639, loss 0.00907999, acc 1
2016-09-06T14:48:32.962165: step 5640, loss 0.0161146, acc 1
2016-09-06T14:48:33.792960: step 5641, loss 0.0515607, acc 0.96
2016-09-06T14:48:34.620471: step 5642, loss 0.0361965, acc 0.98
2016-09-06T14:48:35.479427: step 5643, loss 0.0182059, acc 1
2016-09-06T14:48:36.320398: step 5644, loss 0.0170298, acc 1
2016-09-06T14:48:37.139762: step 5645, loss 0.00624232, acc 1
2016-09-06T14:48:37.977845: step 5646, loss 0.0120187, acc 1
2016-09-06T14:48:38.788256: step 5647, loss 0.00789113, acc 1
2016-09-06T14:48:39.595347: step 5648, loss 0.0533526, acc 0.98
2016-09-06T14:48:40.411508: step 5649, loss 0.0129418, acc 1
2016-09-06T14:48:41.225542: step 5650, loss 0.0179813, acc 1
2016-09-06T14:48:42.014210: step 5651, loss 0.0910057, acc 0.94
2016-09-06T14:48:42.852767: step 5652, loss 0.0393335, acc 0.98
2016-09-06T14:48:43.668114: step 5653, loss 0.0204138, acc 1
2016-09-06T14:48:44.469234: step 5654, loss 0.0332471, acc 0.98
2016-09-06T14:48:45.278823: step 5655, loss 0.0242348, acc 0.98
2016-09-06T14:48:46.101875: step 5656, loss 0.0372695, acc 0.98
2016-09-06T14:48:46.918706: step 5657, loss 0.0175041, acc 1
2016-09-06T14:48:47.715835: step 5658, loss 0.0168652, acc 1
2016-09-06T14:48:48.531176: step 5659, loss 0.107073, acc 0.98
2016-09-06T14:48:49.298368: step 5660, loss 0.00347953, acc 1
2016-09-06T14:48:50.108303: step 5661, loss 0.0278322, acc 1
2016-09-06T14:48:50.917515: step 5662, loss 0.0224823, acc 1
2016-09-06T14:48:51.741788: step 5663, loss 0.0328213, acc 1
2016-09-06T14:48:52.551074: step 5664, loss 0.00774383, acc 1
2016-09-06T14:48:53.402890: step 5665, loss 0.0102936, acc 1
2016-09-06T14:48:54.199873: step 5666, loss 0.0238059, acc 1
2016-09-06T14:48:55.013883: step 5667, loss 0.0195138, acc 1
2016-09-06T14:48:55.825412: step 5668, loss 0.0682065, acc 0.94
2016-09-06T14:48:56.608494: step 5669, loss 0.0294933, acc 1
2016-09-06T14:48:57.400655: step 5670, loss 0.013342, acc 1
2016-09-06T14:48:58.223133: step 5671, loss 0.00504373, acc 1
2016-09-06T14:48:59.010334: step 5672, loss 0.00995384, acc 1
2016-09-06T14:48:59.817570: step 5673, loss 0.0264137, acc 1
2016-09-06T14:49:00.650623: step 5674, loss 0.0308309, acc 0.98
2016-09-06T14:49:01.426417: step 5675, loss 0.0237135, acc 0.98
2016-09-06T14:49:02.196889: step 5676, loss 0.0729162, acc 0.98
2016-09-06T14:49:02.999187: step 5677, loss 0.0202436, acc 1
2016-09-06T14:49:03.770546: step 5678, loss 0.0460295, acc 0.96
2016-09-06T14:49:04.603350: step 5679, loss 0.0050311, acc 1
2016-09-06T14:49:05.410411: step 5680, loss 0.0308801, acc 0.98
2016-09-06T14:49:06.207003: step 5681, loss 0.0152149, acc 1
2016-09-06T14:49:07.017268: step 5682, loss 0.00422874, acc 1
2016-09-06T14:49:07.821214: step 5683, loss 0.036152, acc 0.98
2016-09-06T14:49:08.618113: step 5684, loss 0.073321, acc 0.94
2016-09-06T14:49:09.426457: step 5685, loss 0.0198604, acc 1
2016-09-06T14:49:10.249829: step 5686, loss 0.00408067, acc 1
2016-09-06T14:49:11.049158: step 5687, loss 0.0196916, acc 1
2016-09-06T14:49:11.874927: step 5688, loss 0.0342121, acc 0.98
2016-09-06T14:49:12.697400: step 5689, loss 0.0055502, acc 1
2016-09-06T14:49:13.479304: step 5690, loss 0.0047808, acc 1
2016-09-06T14:49:14.291512: step 5691, loss 0.00374567, acc 1
2016-09-06T14:49:15.091647: step 5692, loss 0.00418853, acc 1
2016-09-06T14:49:15.878559: step 5693, loss 0.0173412, acc 1
2016-09-06T14:49:16.670505: step 5694, loss 0.0202224, acc 1
2016-09-06T14:49:17.477422: step 5695, loss 0.026535, acc 0.98
2016-09-06T14:49:18.251135: step 5696, loss 0.00397883, acc 1
2016-09-06T14:49:19.070319: step 5697, loss 0.0125326, acc 1
2016-09-06T14:49:19.890388: step 5698, loss 0.0544098, acc 0.96
2016-09-06T14:49:20.715212: step 5699, loss 0.0332006, acc 0.98
2016-09-06T14:49:21.520161: step 5700, loss 0.179241, acc 0.98

Evaluation:
2016-09-06T14:49:25.235048: step 5700, loss 2.21289, acc 0.753283

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-5700

2016-09-06T14:49:27.115465: step 5701, loss 0.0107546, acc 1
2016-09-06T14:49:27.922877: step 5702, loss 0.0251237, acc 1
2016-09-06T14:49:28.774022: step 5703, loss 0.0571136, acc 0.96
2016-09-06T14:49:29.589890: step 5704, loss 0.0196802, acc 0.98
2016-09-06T14:49:30.411360: step 5705, loss 0.00894381, acc 1
2016-09-06T14:49:31.228487: step 5706, loss 0.0177787, acc 1
2016-09-06T14:49:32.038001: step 5707, loss 0.0331628, acc 0.98
2016-09-06T14:49:32.841819: step 5708, loss 0.0698683, acc 0.98
2016-09-06T14:49:33.677444: step 5709, loss 0.103696, acc 0.94
2016-09-06T14:49:34.516755: step 5710, loss 0.00453773, acc 1
2016-09-06T14:49:35.310608: step 5711, loss 0.0496573, acc 0.98
2016-09-06T14:49:36.111976: step 5712, loss 0.0406767, acc 0.98
2016-09-06T14:49:36.942730: step 5713, loss 0.0327884, acc 1
2016-09-06T14:49:37.754058: step 5714, loss 0.022084, acc 0.98
2016-09-06T14:49:38.550832: step 5715, loss 0.1009, acc 0.94
2016-09-06T14:49:39.374649: step 5716, loss 0.0628478, acc 0.98
2016-09-06T14:49:40.164743: step 5717, loss 0.0527042, acc 0.98
2016-09-06T14:49:40.970278: step 5718, loss 0.0133362, acc 1
2016-09-06T14:49:41.792394: step 5719, loss 0.00438378, acc 1
2016-09-06T14:49:42.587555: step 5720, loss 0.0139924, acc 1
2016-09-06T14:49:43.382939: step 5721, loss 0.00700919, acc 1
2016-09-06T14:49:44.196595: step 5722, loss 0.0086365, acc 1
2016-09-06T14:49:45.002631: step 5723, loss 0.0201482, acc 0.98
2016-09-06T14:49:45.796932: step 5724, loss 0.00630823, acc 1
2016-09-06T14:49:46.646929: step 5725, loss 0.00631592, acc 1
2016-09-06T14:49:47.478012: step 5726, loss 0.0139985, acc 1
2016-09-06T14:49:48.279384: step 5727, loss 0.00434145, acc 1
2016-09-06T14:49:49.157657: step 5728, loss 0.193953, acc 0.96
2016-09-06T14:49:49.967367: step 5729, loss 0.10664, acc 0.96
2016-09-06T14:49:50.806197: step 5730, loss 0.0457983, acc 0.98
2016-09-06T14:49:51.639444: step 5731, loss 0.0167151, acc 1
2016-09-06T14:49:52.460432: step 5732, loss 0.00913545, acc 1
2016-09-06T14:49:53.269460: step 5733, loss 0.108646, acc 0.98
2016-09-06T14:49:54.105566: step 5734, loss 0.0248208, acc 0.98
2016-09-06T14:49:54.909317: step 5735, loss 0.0073896, acc 1
2016-09-06T14:49:55.760840: step 5736, loss 0.0219303, acc 1
2016-09-06T14:49:56.574395: step 5737, loss 0.0313822, acc 0.98
2016-09-06T14:49:57.402445: step 5738, loss 0.0176307, acc 1
2016-09-06T14:49:58.190777: step 5739, loss 0.0316709, acc 0.98
2016-09-06T14:49:59.041534: step 5740, loss 0.012329, acc 1
2016-09-06T14:49:59.851525: step 5741, loss 0.0147511, acc 1
2016-09-06T14:50:00.677453: step 5742, loss 0.0389352, acc 0.98
2016-09-06T14:50:01.503748: step 5743, loss 0.0643025, acc 1
2016-09-06T14:50:02.356687: step 5744, loss 0.0040155, acc 1
2016-09-06T14:50:03.213050: step 5745, loss 0.0240493, acc 1
2016-09-06T14:50:04.014513: step 5746, loss 0.0455375, acc 0.98
2016-09-06T14:50:04.846198: step 5747, loss 0.0529244, acc 0.96
2016-09-06T14:50:05.640425: step 5748, loss 0.0160787, acc 1
2016-09-06T14:50:06.434809: step 5749, loss 0.107776, acc 0.98
2016-09-06T14:50:07.268126: step 5750, loss 0.0599327, acc 0.96
2016-09-06T14:50:08.062772: step 5751, loss 0.0221623, acc 0.98
2016-09-06T14:50:08.892798: step 5752, loss 0.00381475, acc 1
2016-09-06T14:50:09.744846: step 5753, loss 0.0191163, acc 0.98
2016-09-06T14:50:10.567700: step 5754, loss 0.0217721, acc 0.98
2016-09-06T14:50:11.382358: step 5755, loss 0.0253679, acc 1
2016-09-06T14:50:12.191935: step 5756, loss 0.00753247, acc 1
2016-09-06T14:50:13.014530: step 5757, loss 0.14569, acc 0.98
2016-09-06T14:50:13.821909: step 5758, loss 0.149222, acc 0.96
2016-09-06T14:50:14.636471: step 5759, loss 0.0243458, acc 1
2016-09-06T14:50:15.385374: step 5760, loss 0.0229553, acc 0.977273
2016-09-06T14:50:16.189681: step 5761, loss 0.0349708, acc 1
2016-09-06T14:50:17.051599: step 5762, loss 0.0443455, acc 0.98
2016-09-06T14:50:17.872494: step 5763, loss 0.0383933, acc 0.98
2016-09-06T14:50:18.689323: step 5764, loss 0.018741, acc 1
2016-09-06T14:50:19.537835: step 5765, loss 0.0155481, acc 1
2016-09-06T14:50:20.344874: step 5766, loss 0.0618071, acc 0.96
2016-09-06T14:50:21.128079: step 5767, loss 0.047453, acc 0.98
2016-09-06T14:50:21.952838: step 5768, loss 0.0581208, acc 0.98
2016-09-06T14:50:22.803029: step 5769, loss 0.106755, acc 0.94
2016-09-06T14:50:23.585056: step 5770, loss 0.0159216, acc 1
2016-09-06T14:50:24.393820: step 5771, loss 0.00642974, acc 1
2016-09-06T14:50:25.185713: step 5772, loss 0.0468438, acc 0.98
2016-09-06T14:50:25.980485: step 5773, loss 0.0295606, acc 0.98
2016-09-06T14:50:26.803201: step 5774, loss 0.016303, acc 1
2016-09-06T14:50:27.638464: step 5775, loss 0.0588751, acc 0.98
2016-09-06T14:50:28.439623: step 5776, loss 0.0241994, acc 0.98
2016-09-06T14:50:29.238379: step 5777, loss 0.019042, acc 1
2016-09-06T14:50:30.033391: step 5778, loss 0.0290553, acc 0.98
2016-09-06T14:50:30.834696: step 5779, loss 0.139067, acc 0.96
2016-09-06T14:50:31.716528: step 5780, loss 0.0199933, acc 0.98
2016-09-06T14:50:32.527443: step 5781, loss 0.00514544, acc 1
2016-09-06T14:50:33.310114: step 5782, loss 0.00619183, acc 1
2016-09-06T14:50:34.103950: step 5783, loss 0.0476603, acc 0.98
2016-09-06T14:50:34.934094: step 5784, loss 0.0457233, acc 0.96
2016-09-06T14:50:35.716639: step 5785, loss 0.0965374, acc 0.98
2016-09-06T14:50:36.512078: step 5786, loss 0.0402705, acc 0.96
2016-09-06T14:50:37.357804: step 5787, loss 0.013654, acc 1
2016-09-06T14:50:38.127912: step 5788, loss 0.0193641, acc 1
2016-09-06T14:50:38.922075: step 5789, loss 0.00383287, acc 1
2016-09-06T14:50:39.763956: step 5790, loss 0.0150281, acc 1
2016-09-06T14:50:40.573611: step 5791, loss 0.0073759, acc 1
2016-09-06T14:50:41.398054: step 5792, loss 0.0224451, acc 0.98
2016-09-06T14:50:42.211504: step 5793, loss 0.0210189, acc 0.98
2016-09-06T14:50:43.009150: step 5794, loss 0.0327036, acc 1
2016-09-06T14:50:43.809506: step 5795, loss 0.00442186, acc 1
2016-09-06T14:50:44.618955: step 5796, loss 0.159932, acc 0.98
2016-09-06T14:50:45.397412: step 5797, loss 0.0411807, acc 0.98
2016-09-06T14:50:46.212114: step 5798, loss 0.0453443, acc 0.98
2016-09-06T14:50:47.044292: step 5799, loss 0.0130946, acc 1
2016-09-06T14:50:47.839198: step 5800, loss 0.00377467, acc 1

Evaluation:
2016-09-06T14:50:51.537019: step 5800, loss 1.37249, acc 0.732645

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-5800

2016-09-06T14:50:53.452607: step 5801, loss 0.0140085, acc 1
2016-09-06T14:50:54.265608: step 5802, loss 0.0262435, acc 1
2016-09-06T14:50:55.060996: step 5803, loss 0.0910419, acc 0.96
2016-09-06T14:50:55.887044: step 5804, loss 0.0183755, acc 1
2016-09-06T14:50:56.709657: step 5805, loss 0.0299382, acc 0.98
2016-09-06T14:50:57.481469: step 5806, loss 0.0277285, acc 1
2016-09-06T14:50:58.287630: step 5807, loss 0.0231928, acc 1
2016-09-06T14:50:59.097839: step 5808, loss 0.0286102, acc 1
2016-09-06T14:50:59.899069: step 5809, loss 0.00577737, acc 1
2016-09-06T14:51:00.719521: step 5810, loss 0.0451443, acc 0.98
2016-09-06T14:51:01.530228: step 5811, loss 0.0171564, acc 1
2016-09-06T14:51:02.344994: step 5812, loss 0.00742497, acc 1
2016-09-06T14:51:03.158153: step 5813, loss 0.0208086, acc 0.98
2016-09-06T14:51:03.964233: step 5814, loss 0.0185005, acc 0.98
2016-09-06T14:51:04.763673: step 5815, loss 0.00505316, acc 1
2016-09-06T14:51:05.605470: step 5816, loss 0.0308788, acc 0.98
2016-09-06T14:51:06.440719: step 5817, loss 0.0194207, acc 1
2016-09-06T14:51:07.252300: step 5818, loss 0.0253278, acc 1
2016-09-06T14:51:08.033700: step 5819, loss 0.040529, acc 1
2016-09-06T14:51:08.844005: step 5820, loss 0.0309114, acc 1
2016-09-06T14:51:09.613947: step 5821, loss 0.00335948, acc 1
2016-09-06T14:51:10.392365: step 5822, loss 0.00681464, acc 1
2016-09-06T14:51:11.199243: step 5823, loss 0.00604378, acc 1
2016-09-06T14:51:11.999100: step 5824, loss 0.0320553, acc 0.98
2016-09-06T14:51:12.813706: step 5825, loss 0.0320681, acc 0.98
2016-09-06T14:51:13.617923: step 5826, loss 0.0224342, acc 0.98
2016-09-06T14:51:14.426558: step 5827, loss 0.00497342, acc 1
2016-09-06T14:51:15.242271: step 5828, loss 0.338512, acc 0.94
2016-09-06T14:51:16.092479: step 5829, loss 0.0288805, acc 0.98
2016-09-06T14:51:16.868839: step 5830, loss 0.0982378, acc 0.96
2016-09-06T14:51:17.653407: step 5831, loss 0.0078986, acc 1
2016-09-06T14:51:18.464088: step 5832, loss 0.030026, acc 0.98
2016-09-06T14:51:19.260605: step 5833, loss 0.0154373, acc 1
2016-09-06T14:51:20.109989: step 5834, loss 0.060511, acc 0.98
2016-09-06T14:51:20.935657: step 5835, loss 0.10286, acc 0.96
2016-09-06T14:51:21.729507: step 5836, loss 0.012735, acc 1
2016-09-06T14:51:22.525465: step 5837, loss 0.108141, acc 0.94
2016-09-06T14:51:23.359645: step 5838, loss 0.0272718, acc 1
2016-09-06T14:51:24.174118: step 5839, loss 0.0348124, acc 1
2016-09-06T14:51:24.987131: step 5840, loss 0.00389339, acc 1
2016-09-06T14:51:25.806261: step 5841, loss 0.00496684, acc 1
2016-09-06T14:51:26.609631: step 5842, loss 0.0454743, acc 0.98
2016-09-06T14:51:27.425972: step 5843, loss 0.109068, acc 0.96
2016-09-06T14:51:28.270535: step 5844, loss 0.0239384, acc 0.98
2016-09-06T14:51:29.088394: step 5845, loss 0.045753, acc 0.98
2016-09-06T14:51:29.908628: step 5846, loss 0.0644841, acc 0.94
2016-09-06T14:51:30.746461: step 5847, loss 0.0271765, acc 0.98
2016-09-06T14:51:31.556440: step 5848, loss 0.0457254, acc 0.98
2016-09-06T14:51:32.369439: step 5849, loss 0.0112758, acc 1
2016-09-06T14:51:33.211384: step 5850, loss 0.0235395, acc 0.98
2016-09-06T14:51:34.010125: step 5851, loss 0.0162985, acc 1
2016-09-06T14:51:34.838940: step 5852, loss 0.028192, acc 0.98
2016-09-06T14:51:35.713801: step 5853, loss 0.0181138, acc 1
2016-09-06T14:51:36.524924: step 5854, loss 0.00310529, acc 1
2016-09-06T14:51:37.319184: step 5855, loss 0.00657861, acc 1
2016-09-06T14:51:38.175063: step 5856, loss 0.069044, acc 0.98
2016-09-06T14:51:38.981984: step 5857, loss 0.0229428, acc 0.98
2016-09-06T14:51:39.814092: step 5858, loss 0.0804523, acc 0.98
2016-09-06T14:51:40.626429: step 5859, loss 0.0146975, acc 1
2016-09-06T14:51:41.432207: step 5860, loss 0.0761252, acc 0.98
2016-09-06T14:51:42.241727: step 5861, loss 0.0427623, acc 0.98
2016-09-06T14:51:43.040215: step 5862, loss 0.0131242, acc 1
2016-09-06T14:51:43.877325: step 5863, loss 0.0259199, acc 0.98
2016-09-06T14:51:44.738265: step 5864, loss 0.0267995, acc 0.98
2016-09-06T14:51:45.553780: step 5865, loss 0.0423869, acc 0.96
2016-09-06T14:51:46.414692: step 5866, loss 0.0256774, acc 1
2016-09-06T14:51:47.233776: step 5867, loss 0.0744343, acc 0.96
2016-09-06T14:51:48.070916: step 5868, loss 0.0138562, acc 1
2016-09-06T14:51:48.919944: step 5869, loss 0.0395213, acc 0.98
2016-09-06T14:51:49.733477: step 5870, loss 0.054347, acc 0.94
2016-09-06T14:51:50.543540: step 5871, loss 0.0614798, acc 0.98
2016-09-06T14:51:51.373027: step 5872, loss 0.0159187, acc 1
2016-09-06T14:51:52.186010: step 5873, loss 0.0292155, acc 0.98
2016-09-06T14:51:53.007659: step 5874, loss 0.0252031, acc 0.98
2016-09-06T14:51:53.839405: step 5875, loss 0.0593551, acc 0.98
2016-09-06T14:51:54.640789: step 5876, loss 0.0573743, acc 0.98
2016-09-06T14:51:55.443636: step 5877, loss 0.017832, acc 1
2016-09-06T14:51:56.266506: step 5878, loss 0.0634366, acc 0.98
2016-09-06T14:51:57.069762: step 5879, loss 0.0300105, acc 0.98
2016-09-06T14:51:57.879139: step 5880, loss 0.0311537, acc 0.98
2016-09-06T14:51:58.710604: step 5881, loss 0.00833576, acc 1
2016-09-06T14:51:59.533093: step 5882, loss 0.015174, acc 1
2016-09-06T14:52:00.355064: step 5883, loss 0.0202897, acc 0.98
2016-09-06T14:52:01.182525: step 5884, loss 0.00500399, acc 1
2016-09-06T14:52:01.992454: step 5885, loss 0.0543765, acc 0.98
2016-09-06T14:52:02.787606: step 5886, loss 0.004574, acc 1
2016-09-06T14:52:03.574407: step 5887, loss 0.0210428, acc 0.98
2016-09-06T14:52:04.391140: step 5888, loss 0.00736708, acc 1
2016-09-06T14:52:05.161482: step 5889, loss 0.0128832, acc 1
2016-09-06T14:52:05.977675: step 5890, loss 0.0246839, acc 0.98
2016-09-06T14:52:06.787306: step 5891, loss 0.054632, acc 0.96
2016-09-06T14:52:07.599136: step 5892, loss 0.0130972, acc 1
2016-09-06T14:52:08.404568: step 5893, loss 0.0252323, acc 1
2016-09-06T14:52:09.233440: step 5894, loss 0.0182804, acc 0.98
2016-09-06T14:52:10.027272: step 5895, loss 0.0508276, acc 0.98
2016-09-06T14:52:10.813566: step 5896, loss 0.0336165, acc 0.98
2016-09-06T14:52:11.630947: step 5897, loss 0.00586187, acc 1
2016-09-06T14:52:12.435024: step 5898, loss 0.0674726, acc 0.98
2016-09-06T14:52:13.239447: step 5899, loss 0.0224227, acc 0.98
2016-09-06T14:52:14.060662: step 5900, loss 0.0126286, acc 1

Evaluation:
2016-09-06T14:52:17.793243: step 5900, loss 1.95625, acc 0.743902

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-5900

2016-09-06T14:52:19.691507: step 5901, loss 0.03251, acc 0.98
2016-09-06T14:52:20.525297: step 5902, loss 0.0259221, acc 1
2016-09-06T14:52:21.361753: step 5903, loss 0.00443064, acc 1
2016-09-06T14:52:22.177579: step 5904, loss 0.130973, acc 0.92
2016-09-06T14:52:22.991336: step 5905, loss 0.0238785, acc 0.98
2016-09-06T14:52:23.832892: step 5906, loss 0.0479323, acc 1
2016-09-06T14:52:24.617454: step 5907, loss 0.0224787, acc 1
2016-09-06T14:52:25.455510: step 5908, loss 0.00882591, acc 1
2016-09-06T14:52:26.308297: step 5909, loss 0.0142276, acc 1
2016-09-06T14:52:27.096096: step 5910, loss 0.023725, acc 1
2016-09-06T14:52:27.896587: step 5911, loss 0.00539481, acc 1
2016-09-06T14:52:28.704301: step 5912, loss 0.00476707, acc 1
2016-09-06T14:52:29.480796: step 5913, loss 0.042476, acc 0.98
2016-09-06T14:52:30.334280: step 5914, loss 0.0223328, acc 0.98
2016-09-06T14:52:31.136264: step 5915, loss 0.00927612, acc 1
2016-09-06T14:52:31.917280: step 5916, loss 0.0323005, acc 0.98
2016-09-06T14:52:32.704915: step 5917, loss 0.0480761, acc 0.98
2016-09-06T14:52:33.507770: step 5918, loss 0.0264394, acc 0.98
2016-09-06T14:52:34.311732: step 5919, loss 0.0096085, acc 1
2016-09-06T14:52:35.135630: step 5920, loss 0.0174528, acc 1
2016-09-06T14:52:35.931444: step 5921, loss 0.052187, acc 0.98
2016-09-06T14:52:36.716098: step 5922, loss 0.0394653, acc 0.98
2016-09-06T14:52:37.513741: step 5923, loss 0.00939896, acc 1
2016-09-06T14:52:38.334347: step 5924, loss 0.0308974, acc 0.98
2016-09-06T14:52:39.132484: step 5925, loss 0.00837915, acc 1
2016-09-06T14:52:39.941035: step 5926, loss 0.0108939, acc 1
2016-09-06T14:52:40.768641: step 5927, loss 0.00679719, acc 1
2016-09-06T14:52:41.550021: step 5928, loss 0.0265841, acc 1
2016-09-06T14:52:42.354369: step 5929, loss 0.0812989, acc 0.96
2016-09-06T14:52:43.190141: step 5930, loss 0.0352065, acc 0.98
2016-09-06T14:52:43.982536: step 5931, loss 0.00785665, acc 1
2016-09-06T14:52:44.784886: step 5932, loss 0.00640164, acc 1
2016-09-06T14:52:45.639939: step 5933, loss 0.0191939, acc 1
2016-09-06T14:52:46.403852: step 5934, loss 0.0204371, acc 1
2016-09-06T14:52:47.248689: step 5935, loss 0.0513059, acc 0.96
2016-09-06T14:52:48.066567: step 5936, loss 0.0147274, acc 1
2016-09-06T14:52:48.883384: step 5937, loss 0.0284971, acc 0.98
2016-09-06T14:52:49.683581: step 5938, loss 0.0349593, acc 1
2016-09-06T14:52:50.559742: step 5939, loss 0.0869864, acc 0.98
2016-09-06T14:52:51.360685: step 5940, loss 0.0315885, acc 1
2016-09-06T14:52:52.156783: step 5941, loss 0.00416844, acc 1
2016-09-06T14:52:52.996551: step 5942, loss 0.0127513, acc 1
2016-09-06T14:52:53.797606: step 5943, loss 0.0357279, acc 0.98
2016-09-06T14:52:54.629214: step 5944, loss 0.00590484, acc 1
2016-09-06T14:52:55.470476: step 5945, loss 0.0281165, acc 1
2016-09-06T14:52:56.284831: step 5946, loss 0.0621047, acc 0.96
2016-09-06T14:52:57.098614: step 5947, loss 0.055254, acc 0.96
2016-09-06T14:52:57.923038: step 5948, loss 0.00879796, acc 1
2016-09-06T14:52:58.754425: step 5949, loss 0.00985019, acc 1
2016-09-06T14:52:59.598468: step 5950, loss 0.00505065, acc 1
2016-09-06T14:53:00.481005: step 5951, loss 0.0282524, acc 0.98
2016-09-06T14:53:01.261624: step 5952, loss 0.0187982, acc 1
2016-09-06T14:53:02.066128: step 5953, loss 0.0105131, acc 1
2016-09-06T14:53:02.890093: step 5954, loss 0.167003, acc 0.92
2016-09-06T14:53:03.692748: step 5955, loss 0.0399103, acc 0.98
2016-09-06T14:53:04.490268: step 5956, loss 0.00502162, acc 1
2016-09-06T14:53:05.318040: step 5957, loss 0.0393416, acc 0.98
2016-09-06T14:53:06.130593: step 5958, loss 0.0396432, acc 0.98
2016-09-06T14:53:06.912799: step 5959, loss 0.0210442, acc 1
2016-09-06T14:53:07.759448: step 5960, loss 0.0182711, acc 0.98
2016-09-06T14:53:08.607321: step 5961, loss 0.121017, acc 0.96
2016-09-06T14:53:09.402156: step 5962, loss 0.0495064, acc 1
2016-09-06T14:53:10.208702: step 5963, loss 0.004959, acc 1
2016-09-06T14:53:11.043784: step 5964, loss 0.0252797, acc 1
2016-09-06T14:53:11.836451: step 5965, loss 0.0350363, acc 0.98
2016-09-06T14:53:12.649945: step 5966, loss 0.025509, acc 1
2016-09-06T14:53:13.488968: step 5967, loss 0.0808719, acc 0.94
2016-09-06T14:53:14.303463: step 5968, loss 0.025855, acc 1
2016-09-06T14:53:15.113101: step 5969, loss 0.10677, acc 0.92
2016-09-06T14:53:15.960481: step 5970, loss 0.00534039, acc 1
2016-09-06T14:53:16.753457: step 5971, loss 0.0335537, acc 1
2016-09-06T14:53:17.581515: step 5972, loss 0.0143761, acc 1
2016-09-06T14:53:18.415614: step 5973, loss 0.277477, acc 0.96
2016-09-06T14:53:19.205040: step 5974, loss 0.0125072, acc 1
2016-09-06T14:53:20.028770: step 5975, loss 0.0184071, acc 1
2016-09-06T14:53:20.858339: step 5976, loss 0.00637337, acc 1
2016-09-06T14:53:21.681314: step 5977, loss 0.0159706, acc 1
2016-09-06T14:53:22.526432: step 5978, loss 0.0427467, acc 0.98
2016-09-06T14:53:23.342274: step 5979, loss 0.0441627, acc 0.98
2016-09-06T14:53:24.152866: step 5980, loss 0.0146037, acc 1
2016-09-06T14:53:24.958763: step 5981, loss 0.075767, acc 0.96
2016-09-06T14:53:25.787937: step 5982, loss 0.044444, acc 0.98
2016-09-06T14:53:26.592019: step 5983, loss 0.0215083, acc 1
2016-09-06T14:53:27.402424: step 5984, loss 0.0238862, acc 0.98
2016-09-06T14:53:28.258000: step 5985, loss 0.0450442, acc 0.98
2016-09-06T14:53:29.076420: step 5986, loss 0.0247853, acc 0.98
2016-09-06T14:53:29.938831: step 5987, loss 0.0333302, acc 0.98
2016-09-06T14:53:30.779524: step 5988, loss 0.0329412, acc 1
2016-09-06T14:53:31.615888: step 5989, loss 0.0250743, acc 0.98
2016-09-06T14:53:32.409186: step 5990, loss 0.0508563, acc 0.96
2016-09-06T14:53:33.212352: step 5991, loss 0.0371854, acc 0.98
2016-09-06T14:53:34.028743: step 5992, loss 0.00412671, acc 1
2016-09-06T14:53:34.808593: step 5993, loss 0.0202105, acc 1
2016-09-06T14:53:35.609545: step 5994, loss 0.00356645, acc 1
2016-09-06T14:53:36.453120: step 5995, loss 0.0395811, acc 0.98
2016-09-06T14:53:37.282694: step 5996, loss 0.0708212, acc 0.98
2016-09-06T14:53:38.095718: step 5997, loss 0.0249406, acc 1
2016-09-06T14:53:38.935605: step 5998, loss 0.0159242, acc 1
2016-09-06T14:53:39.738570: step 5999, loss 0.0316404, acc 0.98
2016-09-06T14:53:40.552506: step 6000, loss 0.0363823, acc 0.98

Evaluation:
2016-09-06T14:53:44.328846: step 6000, loss 1.80511, acc 0.751407

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-6000

2016-09-06T14:53:46.254087: step 6001, loss 0.0286946, acc 0.98
2016-09-06T14:53:47.102339: step 6002, loss 0.0162046, acc 1
2016-09-06T14:53:47.900377: step 6003, loss 0.0146868, acc 1
2016-09-06T14:53:48.766386: step 6004, loss 0.0107757, acc 1
2016-09-06T14:53:49.622322: step 6005, loss 0.0839631, acc 0.98
2016-09-06T14:53:50.423178: step 6006, loss 0.146625, acc 0.98
2016-09-06T14:53:51.252734: step 6007, loss 0.0438989, acc 0.96
2016-09-06T14:53:52.054463: step 6008, loss 0.0557129, acc 0.98
2016-09-06T14:53:52.859683: step 6009, loss 0.0148997, acc 1
2016-09-06T14:53:53.700666: step 6010, loss 0.108815, acc 0.96
2016-09-06T14:53:54.536504: step 6011, loss 0.0165993, acc 1
2016-09-06T14:53:55.329877: step 6012, loss 0.0160232, acc 1
2016-09-06T14:53:56.112191: step 6013, loss 0.00571689, acc 1
2016-09-06T14:53:56.931601: step 6014, loss 0.0157056, acc 1
2016-09-06T14:53:57.753454: step 6015, loss 0.0205025, acc 1
2016-09-06T14:53:58.570184: step 6016, loss 0.016141, acc 1
2016-09-06T14:53:59.351614: step 6017, loss 0.0391162, acc 0.98
2016-09-06T14:54:00.148141: step 6018, loss 0.0221692, acc 1
2016-09-06T14:54:00.987994: step 6019, loss 0.0103637, acc 1
2016-09-06T14:54:01.810935: step 6020, loss 0.095665, acc 0.94
2016-09-06T14:54:02.588614: step 6021, loss 0.0564405, acc 0.96
2016-09-06T14:54:03.408327: step 6022, loss 0.0267052, acc 1
2016-09-06T14:54:04.225152: step 6023, loss 0.0340832, acc 0.98
2016-09-06T14:54:05.010100: step 6024, loss 0.0136173, acc 1
2016-09-06T14:54:05.805643: step 6025, loss 0.0970231, acc 0.96
2016-09-06T14:54:06.635125: step 6026, loss 0.00483168, acc 1
2016-09-06T14:54:07.412414: step 6027, loss 0.0824465, acc 0.94
2016-09-06T14:54:08.231277: step 6028, loss 0.00999834, acc 1
2016-09-06T14:54:09.079143: step 6029, loss 0.0293211, acc 0.98
2016-09-06T14:54:09.876184: step 6030, loss 0.0431854, acc 0.98
2016-09-06T14:54:10.685891: step 6031, loss 0.0269995, acc 1
2016-09-06T14:54:11.518382: step 6032, loss 0.0107229, acc 1
2016-09-06T14:54:12.310174: step 6033, loss 0.00474995, acc 1
2016-09-06T14:54:13.112332: step 6034, loss 0.0954214, acc 0.98
2016-09-06T14:54:13.923871: step 6035, loss 0.0837158, acc 0.94
2016-09-06T14:54:14.731061: step 6036, loss 0.00690152, acc 1
2016-09-06T14:54:15.544381: step 6037, loss 0.0172833, acc 1
2016-09-06T14:54:16.365396: step 6038, loss 0.03061, acc 0.98
2016-09-06T14:54:17.179474: step 6039, loss 0.00540221, acc 1
2016-09-06T14:54:18.016354: step 6040, loss 0.00911496, acc 1
2016-09-06T14:54:18.863129: step 6041, loss 0.0652309, acc 0.98
2016-09-06T14:54:19.713304: step 6042, loss 0.0239484, acc 1
2016-09-06T14:54:20.508531: step 6043, loss 0.0431529, acc 0.98
2016-09-06T14:54:21.348688: step 6044, loss 0.0139, acc 1
2016-09-06T14:54:22.150716: step 6045, loss 0.0122991, acc 1
2016-09-06T14:54:22.966666: step 6046, loss 0.0266079, acc 0.98
2016-09-06T14:54:23.836892: step 6047, loss 0.0181456, acc 1
2016-09-06T14:54:24.639406: step 6048, loss 0.0222642, acc 0.98
2016-09-06T14:54:25.453798: step 6049, loss 0.0071321, acc 1
2016-09-06T14:54:26.291601: step 6050, loss 0.0387409, acc 0.98
2016-09-06T14:54:27.107689: step 6051, loss 0.0260528, acc 1
2016-09-06T14:54:27.892508: step 6052, loss 0.0725553, acc 0.96
2016-09-06T14:54:28.717178: step 6053, loss 0.0479568, acc 0.98
2016-09-06T14:54:29.512762: step 6054, loss 0.0163637, acc 1
2016-09-06T14:54:30.317442: step 6055, loss 0.0492894, acc 0.98
2016-09-06T14:54:31.134675: step 6056, loss 0.0120165, acc 1
2016-09-06T14:54:31.928719: step 6057, loss 0.0198872, acc 0.98
2016-09-06T14:54:32.750959: step 6058, loss 0.0195823, acc 0.98
2016-09-06T14:54:33.568661: step 6059, loss 0.0476352, acc 0.98
2016-09-06T14:54:34.381371: step 6060, loss 0.026985, acc 1
2016-09-06T14:54:35.188371: step 6061, loss 0.0275627, acc 1
2016-09-06T14:54:35.984294: step 6062, loss 0.0507724, acc 0.98
2016-09-06T14:54:36.799953: step 6063, loss 0.00832944, acc 1
2016-09-06T14:54:37.574742: step 6064, loss 0.0327396, acc 0.98
2016-09-06T14:54:38.386102: step 6065, loss 0.00372007, acc 1
2016-09-06T14:54:39.189224: step 6066, loss 0.0179437, acc 1
2016-09-06T14:54:39.998503: step 6067, loss 0.0446171, acc 0.98
2016-09-06T14:54:40.827815: step 6068, loss 0.0137414, acc 1
2016-09-06T14:54:41.617204: step 6069, loss 0.022007, acc 0.98
2016-09-06T14:54:42.432975: step 6070, loss 0.0103495, acc 1
2016-09-06T14:54:43.255638: step 6071, loss 0.00832277, acc 1
2016-09-06T14:54:44.073586: step 6072, loss 0.0236979, acc 1
2016-09-06T14:54:44.873812: step 6073, loss 0.00449199, acc 1
2016-09-06T14:54:45.686171: step 6074, loss 0.00578356, acc 1
2016-09-06T14:54:46.508889: step 6075, loss 0.0233559, acc 0.98
2016-09-06T14:54:47.295209: step 6076, loss 0.0373194, acc 0.98
2016-09-06T14:54:48.133758: step 6077, loss 0.0452662, acc 0.96
2016-09-06T14:54:48.940941: step 6078, loss 0.0196035, acc 0.98
2016-09-06T14:54:49.710902: step 6079, loss 0.0214853, acc 0.98
2016-09-06T14:54:50.512256: step 6080, loss 0.0167418, acc 1
2016-09-06T14:54:51.374268: step 6081, loss 0.0519348, acc 0.96
2016-09-06T14:54:52.167845: step 6082, loss 0.0441543, acc 0.96
2016-09-06T14:54:52.975025: step 6083, loss 0.0175598, acc 1
2016-09-06T14:54:53.767955: step 6084, loss 0.00512965, acc 1
2016-09-06T14:54:54.534124: step 6085, loss 0.0163858, acc 1
2016-09-06T14:54:55.352565: step 6086, loss 0.0299249, acc 0.98
2016-09-06T14:54:56.162514: step 6087, loss 0.0118449, acc 1
2016-09-06T14:54:56.955461: step 6088, loss 0.0129758, acc 1
2016-09-06T14:54:57.757502: step 6089, loss 0.024296, acc 0.98
2016-09-06T14:54:58.579232: step 6090, loss 0.0345824, acc 0.98
2016-09-06T14:54:59.374638: step 6091, loss 0.00348142, acc 1
2016-09-06T14:55:00.189387: step 6092, loss 0.00346089, acc 1
2016-09-06T14:55:01.043181: step 6093, loss 0.0212781, acc 0.98
2016-09-06T14:55:01.839924: step 6094, loss 0.00543278, acc 1
2016-09-06T14:55:02.642336: step 6095, loss 0.00359673, acc 1
2016-09-06T14:55:03.452916: step 6096, loss 0.0441384, acc 0.96
2016-09-06T14:55:04.242046: step 6097, loss 0.00590127, acc 1
2016-09-06T14:55:05.041086: step 6098, loss 0.0127458, acc 1
2016-09-06T14:55:05.848915: step 6099, loss 0.0639277, acc 0.98
2016-09-06T14:55:06.634707: step 6100, loss 0.00653051, acc 1

Evaluation:
2016-09-06T14:55:10.375573: step 6100, loss 2.22524, acc 0.747655

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-6100

2016-09-06T14:55:12.294241: step 6101, loss 0.00413595, acc 1
2016-09-06T14:55:13.114232: step 6102, loss 0.016394, acc 1
2016-09-06T14:55:13.915757: step 6103, loss 0.0583724, acc 0.98
2016-09-06T14:55:14.720122: step 6104, loss 0.00533202, acc 1
2016-09-06T14:55:15.538779: step 6105, loss 0.0241081, acc 1
2016-09-06T14:55:16.300824: step 6106, loss 0.0227172, acc 0.98
2016-09-06T14:55:17.114590: step 6107, loss 0.0104622, acc 1
2016-09-06T14:55:17.927757: step 6108, loss 0.00363526, acc 1
2016-09-06T14:55:18.713700: step 6109, loss 0.0200911, acc 1
2016-09-06T14:55:19.539514: step 6110, loss 0.00424306, acc 1
2016-09-06T14:55:20.381445: step 6111, loss 0.00326733, acc 1
2016-09-06T14:55:21.177662: step 6112, loss 0.0198681, acc 1
2016-09-06T14:55:21.963627: step 6113, loss 0.0380047, acc 0.98
2016-09-06T14:55:22.802255: step 6114, loss 0.0234965, acc 1
2016-09-06T14:55:23.601683: step 6115, loss 0.00999752, acc 1
2016-09-06T14:55:24.390043: step 6116, loss 0.030722, acc 0.98
2016-09-06T14:55:25.215378: step 6117, loss 0.0187153, acc 0.98
2016-09-06T14:55:26.018707: step 6118, loss 0.0253754, acc 1
2016-09-06T14:55:26.818815: step 6119, loss 0.017369, acc 1
2016-09-06T14:55:27.654912: step 6120, loss 0.0657726, acc 0.98
2016-09-06T14:55:28.457716: step 6121, loss 0.0697497, acc 0.94
2016-09-06T14:55:29.265320: step 6122, loss 0.00307812, acc 1
2016-09-06T14:55:30.088042: step 6123, loss 0.0491872, acc 0.98
2016-09-06T14:55:30.895501: step 6124, loss 0.0463478, acc 0.96
2016-09-06T14:55:31.725702: step 6125, loss 0.0273905, acc 0.98
2016-09-06T14:55:32.559187: step 6126, loss 0.0296816, acc 0.98
2016-09-06T14:55:33.351899: step 6127, loss 0.0266101, acc 1
2016-09-06T14:55:34.135746: step 6128, loss 0.00291425, acc 1
2016-09-06T14:55:34.969394: step 6129, loss 0.0300252, acc 0.98
2016-09-06T14:55:35.781064: step 6130, loss 0.0339323, acc 0.98
2016-09-06T14:55:36.603096: step 6131, loss 0.0323459, acc 0.98
2016-09-06T14:55:37.462849: step 6132, loss 0.0161872, acc 1
2016-09-06T14:55:38.291478: step 6133, loss 0.00603367, acc 1
2016-09-06T14:55:39.091546: step 6134, loss 0.0115365, acc 1
2016-09-06T14:55:39.932767: step 6135, loss 0.0102514, acc 1
2016-09-06T14:55:40.756904: step 6136, loss 0.00266986, acc 1
2016-09-06T14:55:41.562174: step 6137, loss 0.00311873, acc 1
2016-09-06T14:55:42.388749: step 6138, loss 0.0615254, acc 0.96
2016-09-06T14:55:43.241933: step 6139, loss 0.0378147, acc 1
2016-09-06T14:55:44.049384: step 6140, loss 0.0118346, acc 1
2016-09-06T14:55:44.885759: step 6141, loss 0.0562622, acc 0.98
2016-09-06T14:55:45.692346: step 6142, loss 0.0128558, acc 1
2016-09-06T14:55:46.473713: step 6143, loss 0.0213067, acc 0.98
2016-09-06T14:55:47.237446: step 6144, loss 0.0219283, acc 1
2016-09-06T14:55:48.047386: step 6145, loss 0.091247, acc 0.98
2016-09-06T14:55:48.870935: step 6146, loss 0.0597074, acc 0.96
2016-09-06T14:55:49.695466: step 6147, loss 0.00833028, acc 1
2016-09-06T14:55:50.530214: step 6148, loss 0.0182241, acc 0.98
2016-09-06T14:55:51.319602: step 6149, loss 0.032827, acc 1
2016-09-06T14:55:52.119409: step 6150, loss 0.0104791, acc 1
2016-09-06T14:55:52.950432: step 6151, loss 0.0516908, acc 0.98
2016-09-06T14:55:53.751431: step 6152, loss 0.0567404, acc 0.96
2016-09-06T14:55:54.561765: step 6153, loss 0.0465764, acc 0.96
2016-09-06T14:55:55.395410: step 6154, loss 0.0156728, acc 1
2016-09-06T14:55:56.186334: step 6155, loss 0.00936299, acc 1
2016-09-06T14:55:56.983153: step 6156, loss 0.00485606, acc 1
2016-09-06T14:55:57.809373: step 6157, loss 0.0952197, acc 0.96
2016-09-06T14:55:58.633257: step 6158, loss 0.0201608, acc 1
2016-09-06T14:55:59.427318: step 6159, loss 0.0268886, acc 0.98
2016-09-06T14:56:00.268964: step 6160, loss 0.0168339, acc 1
2016-09-06T14:56:01.060130: step 6161, loss 0.0219941, acc 1
2016-09-06T14:56:01.880101: step 6162, loss 0.0186192, acc 0.98
2016-09-06T14:56:02.756606: step 6163, loss 0.0416564, acc 0.98
2016-09-06T14:56:03.533958: step 6164, loss 0.0024346, acc 1
2016-09-06T14:56:04.357752: step 6165, loss 0.00247106, acc 1
2016-09-06T14:56:05.181616: step 6166, loss 0.0104617, acc 1
2016-09-06T14:56:05.984453: step 6167, loss 0.00616902, acc 1
2016-09-06T14:56:06.803426: step 6168, loss 0.0302445, acc 1
2016-09-06T14:56:07.645358: step 6169, loss 0.0317378, acc 0.98
2016-09-06T14:56:08.457010: step 6170, loss 0.0137383, acc 1
2016-09-06T14:56:09.269037: step 6171, loss 0.0635022, acc 0.96
2016-09-06T14:56:10.098080: step 6172, loss 0.0256372, acc 0.98
2016-09-06T14:56:10.903876: step 6173, loss 0.0124462, acc 1
2016-09-06T14:56:11.730581: step 6174, loss 0.0186624, acc 0.98
2016-09-06T14:56:12.569297: step 6175, loss 0.00293425, acc 1
2016-09-06T14:56:13.417333: step 6176, loss 0.0284256, acc 1
2016-09-06T14:56:14.239815: step 6177, loss 0.00572075, acc 1
2016-09-06T14:56:15.075422: step 6178, loss 0.0119451, acc 1
2016-09-06T14:56:15.928109: step 6179, loss 0.00276589, acc 1
2016-09-06T14:56:16.743683: step 6180, loss 0.00486791, acc 1
2016-09-06T14:56:17.565368: step 6181, loss 0.0164419, acc 1
2016-09-06T14:56:18.382982: step 6182, loss 0.0346879, acc 1
2016-09-06T14:56:19.174022: step 6183, loss 0.0263516, acc 0.98
2016-09-06T14:56:19.994789: step 6184, loss 0.0325566, acc 0.96
2016-09-06T14:56:20.820187: step 6185, loss 0.0177202, acc 0.98
2016-09-06T14:56:21.615617: step 6186, loss 0.0627706, acc 0.96
2016-09-06T14:56:22.413505: step 6187, loss 0.0230258, acc 0.98
2016-09-06T14:56:23.230242: step 6188, loss 0.0876739, acc 0.98
2016-09-06T14:56:24.034924: step 6189, loss 0.020302, acc 1
2016-09-06T14:56:24.841086: step 6190, loss 0.0300581, acc 1
2016-09-06T14:56:25.654883: step 6191, loss 0.0237405, acc 0.98
2016-09-06T14:56:26.433782: step 6192, loss 0.0329732, acc 0.98
2016-09-06T14:56:27.243983: step 6193, loss 0.00250096, acc 1
2016-09-06T14:56:28.058136: step 6194, loss 0.00256881, acc 1
2016-09-06T14:56:28.887007: step 6195, loss 0.0107138, acc 1
2016-09-06T14:56:29.693284: step 6196, loss 0.00261619, acc 1
2016-09-06T14:56:30.519603: step 6197, loss 0.014573, acc 1
2016-09-06T14:56:31.331056: step 6198, loss 0.00310171, acc 1
2016-09-06T14:56:32.137449: step 6199, loss 0.0191102, acc 1
2016-09-06T14:56:32.954583: step 6200, loss 0.0432861, acc 1

Evaluation:
2016-09-06T14:56:36.668882: step 6200, loss 1.8078, acc 0.745779

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-6200

2016-09-06T14:56:38.593555: step 6201, loss 0.0156065, acc 1
2016-09-06T14:56:39.397459: step 6202, loss 0.0948075, acc 0.98
2016-09-06T14:56:40.229652: step 6203, loss 0.00869304, acc 1
2016-09-06T14:56:41.018696: step 6204, loss 0.0421511, acc 0.98
2016-09-06T14:56:41.849534: step 6205, loss 0.00262005, acc 1
2016-09-06T14:56:42.680121: step 6206, loss 0.01341, acc 1
2016-09-06T14:56:43.475586: step 6207, loss 0.0213519, acc 0.98
2016-09-06T14:56:44.283761: step 6208, loss 0.051082, acc 0.98
2016-09-06T14:56:45.098499: step 6209, loss 0.0235343, acc 0.98
2016-09-06T14:56:45.868308: step 6210, loss 0.0237689, acc 0.98
2016-09-06T14:56:46.672975: step 6211, loss 0.178929, acc 0.96
2016-09-06T14:56:47.509774: step 6212, loss 0.0433733, acc 1
2016-09-06T14:56:48.296875: step 6213, loss 0.0511934, acc 0.98
2016-09-06T14:56:49.087625: step 6214, loss 0.0367921, acc 1
2016-09-06T14:56:49.926555: step 6215, loss 0.0330863, acc 0.98
2016-09-06T14:56:50.699332: step 6216, loss 0.0199369, acc 1
2016-09-06T14:56:51.504973: step 6217, loss 0.0758147, acc 0.96
2016-09-06T14:56:52.317409: step 6218, loss 0.0234122, acc 0.98
2016-09-06T14:56:53.110823: step 6219, loss 0.0395253, acc 0.96
2016-09-06T14:56:53.941915: step 6220, loss 0.00690974, acc 1
2016-09-06T14:56:54.760795: step 6221, loss 0.00312372, acc 1
2016-09-06T14:56:55.555003: step 6222, loss 0.0158888, acc 1
2016-09-06T14:56:56.357474: step 6223, loss 0.0534493, acc 0.96
2016-09-06T14:56:57.171688: step 6224, loss 0.0534806, acc 0.98
2016-09-06T14:56:57.954073: step 6225, loss 0.00391091, acc 1
2016-09-06T14:56:58.748912: step 6226, loss 0.0159176, acc 1
2016-09-06T14:56:59.583542: step 6227, loss 0.0326039, acc 0.98
2016-09-06T14:57:00.384395: step 6228, loss 0.0416624, acc 0.98
2016-09-06T14:57:01.192299: step 6229, loss 0.00421984, acc 1
2016-09-06T14:57:01.996920: step 6230, loss 0.0399443, acc 0.98
2016-09-06T14:57:02.797458: step 6231, loss 0.00306378, acc 1
2016-09-06T14:57:03.608416: step 6232, loss 0.0435028, acc 0.98
2016-09-06T14:57:04.426500: step 6233, loss 0.0171712, acc 0.98
2016-09-06T14:57:05.228886: step 6234, loss 0.00579908, acc 1
2016-09-06T14:57:06.041936: step 6235, loss 0.043977, acc 0.98
2016-09-06T14:57:06.867231: step 6236, loss 0.00369271, acc 1
2016-09-06T14:57:07.653839: step 6237, loss 0.010479, acc 1
2016-09-06T14:57:08.501431: step 6238, loss 0.0402037, acc 0.98
2016-09-06T14:57:09.321187: step 6239, loss 0.0211116, acc 1
2016-09-06T14:57:10.134283: step 6240, loss 0.00415433, acc 1
2016-09-06T14:57:10.968242: step 6241, loss 0.0120084, acc 1
2016-09-06T14:57:11.803014: step 6242, loss 0.0588292, acc 0.96
2016-09-06T14:57:12.601841: step 6243, loss 0.102512, acc 0.96
2016-09-06T14:57:13.417550: step 6244, loss 0.0825147, acc 0.98
2016-09-06T14:57:14.250485: step 6245, loss 0.0578962, acc 0.98
2016-09-06T14:57:15.100002: step 6246, loss 0.0243521, acc 0.98
2016-09-06T14:57:15.909522: step 6247, loss 0.0227219, acc 0.98
2016-09-06T14:57:16.702473: step 6248, loss 0.00376274, acc 1
2016-09-06T14:57:17.504254: step 6249, loss 0.0045775, acc 1
2016-09-06T14:57:18.287664: step 6250, loss 0.0152035, acc 1
2016-09-06T14:57:19.150026: step 6251, loss 0.0742279, acc 0.94
2016-09-06T14:57:19.973418: step 6252, loss 0.0889454, acc 0.96
2016-09-06T14:57:20.792659: step 6253, loss 0.0411801, acc 0.96
2016-09-06T14:57:21.654511: step 6254, loss 0.0511844, acc 0.96
2016-09-06T14:57:22.461459: step 6255, loss 0.0110672, acc 1
2016-09-06T14:57:23.264882: step 6256, loss 0.0554306, acc 0.96
2016-09-06T14:57:24.072231: step 6257, loss 0.0397387, acc 0.98
2016-09-06T14:57:24.885391: step 6258, loss 0.0147653, acc 1
2016-09-06T14:57:25.677431: step 6259, loss 0.0317931, acc 1
2016-09-06T14:57:26.508558: step 6260, loss 0.0291698, acc 0.98
2016-09-06T14:57:27.316699: step 6261, loss 0.0784438, acc 0.96
2016-09-06T14:57:28.083633: step 6262, loss 0.0178493, acc 0.98
2016-09-06T14:57:28.889667: step 6263, loss 0.0159917, acc 1
2016-09-06T14:57:29.677236: step 6264, loss 0.0256005, acc 0.98
2016-09-06T14:57:30.519183: step 6265, loss 0.00481844, acc 1
2016-09-06T14:57:31.339949: step 6266, loss 0.0207055, acc 1
2016-09-06T14:57:32.230918: step 6267, loss 0.00435983, acc 1
2016-09-06T14:57:33.056076: step 6268, loss 0.0216545, acc 0.98
2016-09-06T14:57:33.861690: step 6269, loss 0.0559699, acc 0.96
2016-09-06T14:57:34.677298: step 6270, loss 0.046334, acc 0.98
2016-09-06T14:57:35.478332: step 6271, loss 0.0827623, acc 0.96
2016-09-06T14:57:36.281209: step 6272, loss 0.0181792, acc 1
2016-09-06T14:57:37.135581: step 6273, loss 0.0223951, acc 0.98
2016-09-06T14:57:37.944003: step 6274, loss 0.0267351, acc 1
2016-09-06T14:57:38.775761: step 6275, loss 0.0450555, acc 0.98
2016-09-06T14:57:39.584728: step 6276, loss 0.0155153, acc 1
2016-09-06T14:57:40.377212: step 6277, loss 0.0212611, acc 1
2016-09-06T14:57:41.189684: step 6278, loss 0.127842, acc 0.98
2016-09-06T14:57:42.009648: step 6279, loss 0.0335832, acc 1
2016-09-06T14:57:42.811045: step 6280, loss 0.0155204, acc 1
2016-09-06T14:57:43.626149: step 6281, loss 0.0149508, acc 1
2016-09-06T14:57:44.509248: step 6282, loss 0.0397048, acc 0.98
2016-09-06T14:57:45.309421: step 6283, loss 0.0125991, acc 1
2016-09-06T14:57:46.085785: step 6284, loss 0.0302466, acc 0.98
2016-09-06T14:57:46.941538: step 6285, loss 0.0339567, acc 0.98
2016-09-06T14:57:47.761002: step 6286, loss 0.00319056, acc 1
2016-09-06T14:57:48.580930: step 6287, loss 0.003611, acc 1
2016-09-06T14:57:49.439513: step 6288, loss 0.133163, acc 0.96
2016-09-06T14:57:50.238314: step 6289, loss 0.0473277, acc 0.98
2016-09-06T14:57:51.082254: step 6290, loss 0.0302511, acc 0.98
2016-09-06T14:57:51.889477: step 6291, loss 0.0194127, acc 1
2016-09-06T14:57:52.693489: step 6292, loss 0.0468296, acc 0.98
2016-09-06T14:57:53.516428: step 6293, loss 0.0182147, acc 1
2016-09-06T14:57:54.296924: step 6294, loss 0.0131852, acc 1
2016-09-06T14:57:55.090203: step 6295, loss 0.00362089, acc 1
2016-09-06T14:57:55.898924: step 6296, loss 0.00309665, acc 1
2016-09-06T14:57:56.742793: step 6297, loss 0.0473642, acc 0.98
2016-09-06T14:57:57.556318: step 6298, loss 0.0337455, acc 0.98
2016-09-06T14:57:58.364092: step 6299, loss 0.0491547, acc 0.98
2016-09-06T14:57:59.162087: step 6300, loss 0.018828, acc 1

Evaluation:
2016-09-06T14:58:02.888969: step 6300, loss 1.61733, acc 0.754221

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-6300

2016-09-06T14:58:04.786689: step 6301, loss 0.0127911, acc 1
2016-09-06T14:58:05.610860: step 6302, loss 0.0241069, acc 1
2016-09-06T14:58:06.468545: step 6303, loss 0.00401403, acc 1
2016-09-06T14:58:07.289754: step 6304, loss 0.00891924, acc 1
2016-09-06T14:58:08.092217: step 6305, loss 0.0213549, acc 1
2016-09-06T14:58:08.903014: step 6306, loss 0.0225422, acc 1
2016-09-06T14:58:09.701695: step 6307, loss 0.0261021, acc 0.98
2016-09-06T14:58:10.493935: step 6308, loss 0.0212772, acc 0.98
2016-09-06T14:58:11.293472: step 6309, loss 0.00418505, acc 1
2016-09-06T14:58:12.115619: step 6310, loss 0.0154953, acc 1
2016-09-06T14:58:12.916734: step 6311, loss 0.0151459, acc 1
2016-09-06T14:58:13.683869: step 6312, loss 0.0812232, acc 0.96
2016-09-06T14:58:14.532269: step 6313, loss 0.0968614, acc 0.96
2016-09-06T14:58:15.347889: step 6314, loss 0.0082444, acc 1
2016-09-06T14:58:16.149032: step 6315, loss 0.0114149, acc 1
2016-09-06T14:58:16.969660: step 6316, loss 0.0102866, acc 1
2016-09-06T14:58:17.756583: step 6317, loss 0.0354621, acc 0.98
2016-09-06T14:58:18.541360: step 6318, loss 0.0258072, acc 1
2016-09-06T14:58:19.350883: step 6319, loss 0.00841122, acc 1
2016-09-06T14:58:20.130137: step 6320, loss 0.019786, acc 1
2016-09-06T14:58:20.959306: step 6321, loss 0.163161, acc 0.98
2016-09-06T14:58:21.769817: step 6322, loss 0.062972, acc 0.98
2016-09-06T14:58:22.546373: step 6323, loss 0.0302211, acc 1
2016-09-06T14:58:23.339300: step 6324, loss 0.0205431, acc 1
2016-09-06T14:58:24.162575: step 6325, loss 0.0094278, acc 1
2016-09-06T14:58:24.956270: step 6326, loss 0.0489579, acc 0.96
2016-09-06T14:58:25.762146: step 6327, loss 0.00360834, acc 1
2016-09-06T14:58:26.600059: step 6328, loss 0.0242629, acc 0.98
2016-09-06T14:58:27.383231: step 6329, loss 0.00372371, acc 1
2016-09-06T14:58:28.178493: step 6330, loss 0.0394545, acc 0.96
2016-09-06T14:58:28.997402: step 6331, loss 0.0242253, acc 0.98
2016-09-06T14:58:29.770156: step 6332, loss 0.00476767, acc 1
2016-09-06T14:58:30.568099: step 6333, loss 0.0597734, acc 0.98
2016-09-06T14:58:31.403374: step 6334, loss 0.03837, acc 0.98
2016-09-06T14:58:32.192819: step 6335, loss 0.154033, acc 0.96
2016-09-06T14:58:32.945567: step 6336, loss 0.0207129, acc 0.977273
2016-09-06T14:58:33.747604: step 6337, loss 0.00678401, acc 1
2016-09-06T14:58:34.551334: step 6338, loss 0.0374049, acc 0.96
2016-09-06T14:58:35.381888: step 6339, loss 0.00648783, acc 1
2016-09-06T14:58:36.168085: step 6340, loss 0.00536465, acc 1
2016-09-06T14:58:36.978663: step 6341, loss 0.00451464, acc 1
2016-09-06T14:58:37.789738: step 6342, loss 0.1278, acc 0.96
2016-09-06T14:58:38.593845: step 6343, loss 0.0221799, acc 0.98
2016-09-06T14:58:39.406600: step 6344, loss 0.0242171, acc 0.98
2016-09-06T14:58:40.245357: step 6345, loss 0.0562087, acc 0.96
2016-09-06T14:58:41.059317: step 6346, loss 0.0889222, acc 0.98
2016-09-06T14:58:41.838170: step 6347, loss 0.00551752, acc 1
2016-09-06T14:58:42.688930: step 6348, loss 0.00589297, acc 1
2016-09-06T14:58:43.517296: step 6349, loss 0.0293223, acc 0.98
2016-09-06T14:58:44.291495: step 6350, loss 0.00602565, acc 1
2016-09-06T14:58:45.101829: step 6351, loss 0.0328262, acc 0.96
2016-09-06T14:58:45.933961: step 6352, loss 0.0197771, acc 0.98
2016-09-06T14:58:46.712928: step 6353, loss 0.0399848, acc 0.98
2016-09-06T14:58:47.520886: step 6354, loss 0.0112557, acc 1
2016-09-06T14:58:48.347803: step 6355, loss 0.0225665, acc 1
2016-09-06T14:58:49.157875: step 6356, loss 0.00370077, acc 1
2016-09-06T14:58:49.936968: step 6357, loss 0.0584052, acc 0.96
2016-09-06T14:58:50.782248: step 6358, loss 0.00450119, acc 1
2016-09-06T14:58:51.572522: step 6359, loss 0.0363568, acc 0.98
2016-09-06T14:58:52.369403: step 6360, loss 0.0168066, acc 1
2016-09-06T14:58:53.182961: step 6361, loss 0.0309528, acc 0.98
2016-09-06T14:58:53.939566: step 6362, loss 0.0372423, acc 0.98
2016-09-06T14:58:54.770537: step 6363, loss 0.00471305, acc 1
2016-09-06T14:58:55.615028: step 6364, loss 0.0192238, acc 1
2016-09-06T14:58:56.425369: step 6365, loss 0.0450071, acc 0.98
2016-09-06T14:58:57.243253: step 6366, loss 0.0138192, acc 1
2016-09-06T14:58:58.066478: step 6367, loss 0.134106, acc 0.96
2016-09-06T14:58:58.900045: step 6368, loss 0.0324266, acc 0.98
2016-09-06T14:58:59.735686: step 6369, loss 0.0343195, acc 0.98
2016-09-06T14:59:00.594038: step 6370, loss 0.0433403, acc 0.96
2016-09-06T14:59:01.404110: step 6371, loss 0.00701213, acc 1
2016-09-06T14:59:02.214844: step 6372, loss 0.0031686, acc 1
2016-09-06T14:59:03.048383: step 6373, loss 0.0586087, acc 0.96
2016-09-06T14:59:03.858020: step 6374, loss 0.0070538, acc 1
2016-09-06T14:59:04.654516: step 6375, loss 0.0388092, acc 0.98
2016-09-06T14:59:05.490043: step 6376, loss 0.00474692, acc 1
2016-09-06T14:59:06.316959: step 6377, loss 0.00849505, acc 1
2016-09-06T14:59:07.107901: step 6378, loss 0.0518572, acc 0.96
2016-09-06T14:59:07.924497: step 6379, loss 0.0311325, acc 1
2016-09-06T14:59:08.747260: step 6380, loss 0.015553, acc 1
2016-09-06T14:59:09.539893: step 6381, loss 0.0229691, acc 0.98
2016-09-06T14:59:10.351891: step 6382, loss 0.0138016, acc 1
2016-09-06T14:59:11.152928: step 6383, loss 0.0329785, acc 0.98
2016-09-06T14:59:11.996461: step 6384, loss 0.019781, acc 0.98
2016-09-06T14:59:12.841336: step 6385, loss 0.00430575, acc 1
2016-09-06T14:59:13.652254: step 6386, loss 0.0587372, acc 0.98
2016-09-06T14:59:14.459464: step 6387, loss 0.0072044, acc 1
2016-09-06T14:59:15.263340: step 6388, loss 0.00527091, acc 1
2016-09-06T14:59:16.038457: step 6389, loss 0.022027, acc 1
2016-09-06T14:59:16.858980: step 6390, loss 0.0271254, acc 0.98
2016-09-06T14:59:17.703897: step 6391, loss 0.00416271, acc 1
2016-09-06T14:59:18.517399: step 6392, loss 0.0194515, acc 1
2016-09-06T14:59:19.324353: step 6393, loss 0.00878135, acc 1
2016-09-06T14:59:20.125432: step 6394, loss 0.00397389, acc 1
2016-09-06T14:59:20.946102: step 6395, loss 0.0173313, acc 1
2016-09-06T14:59:21.735596: step 6396, loss 0.033082, acc 0.98
2016-09-06T14:59:22.532256: step 6397, loss 0.00858123, acc 1
2016-09-06T14:59:23.355404: step 6398, loss 0.0511012, acc 0.98
2016-09-06T14:59:24.173977: step 6399, loss 0.0260554, acc 1
2016-09-06T14:59:24.984287: step 6400, loss 0.0156226, acc 1

Evaluation:
2016-09-06T14:59:28.684341: step 6400, loss 1.94925, acc 0.738274

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-6400

2016-09-06T14:59:30.620842: step 6401, loss 0.0100303, acc 1
2016-09-06T14:59:31.471375: step 6402, loss 0.0273839, acc 1
2016-09-06T14:59:32.284361: step 6403, loss 0.0177237, acc 1
2016-09-06T14:59:33.120812: step 6404, loss 0.0442863, acc 0.98
2016-09-06T14:59:33.908894: step 6405, loss 0.0360083, acc 0.98
2016-09-06T14:59:34.699624: step 6406, loss 0.0169741, acc 0.98
2016-09-06T14:59:35.532150: step 6407, loss 0.018494, acc 1
2016-09-06T14:59:36.323423: step 6408, loss 0.00925885, acc 1
2016-09-06T14:59:37.103815: step 6409, loss 0.0472359, acc 0.96
2016-09-06T14:59:37.925403: step 6410, loss 0.0106023, acc 1
2016-09-06T14:59:38.695704: step 6411, loss 0.0456008, acc 0.98
2016-09-06T14:59:39.492300: step 6412, loss 0.0567773, acc 0.98
2016-09-06T14:59:40.323118: step 6413, loss 0.00331557, acc 1
2016-09-06T14:59:41.127456: step 6414, loss 0.00304076, acc 1
2016-09-06T14:59:41.953009: step 6415, loss 0.00295699, acc 1
2016-09-06T14:59:42.761923: step 6416, loss 0.00470323, acc 1
2016-09-06T14:59:43.554834: step 6417, loss 0.00295434, acc 1
2016-09-06T14:59:44.416893: step 6418, loss 0.00279388, acc 1
2016-09-06T14:59:45.240397: step 6419, loss 0.0746683, acc 0.94
2016-09-06T14:59:46.066783: step 6420, loss 0.0375935, acc 0.98
2016-09-06T14:59:46.858597: step 6421, loss 0.0142591, acc 1
2016-09-06T14:59:47.707988: step 6422, loss 0.0494919, acc 0.98
2016-09-06T14:59:48.528604: step 6423, loss 0.00678373, acc 1
2016-09-06T14:59:49.330563: step 6424, loss 0.0442309, acc 0.98
2016-09-06T14:59:50.180390: step 6425, loss 0.00289462, acc 1
2016-09-06T14:59:50.979034: step 6426, loss 0.00580106, acc 1
2016-09-06T14:59:51.802150: step 6427, loss 0.0130771, acc 1
2016-09-06T14:59:52.642284: step 6428, loss 0.0122537, acc 1
2016-09-06T14:59:53.477422: step 6429, loss 0.0211636, acc 0.98
2016-09-06T14:59:54.304808: step 6430, loss 0.00262589, acc 1
2016-09-06T14:59:55.125275: step 6431, loss 0.0171672, acc 0.98
2016-09-06T14:59:55.935041: step 6432, loss 0.00863449, acc 1
2016-09-06T14:59:56.752547: step 6433, loss 0.0588152, acc 0.98
2016-09-06T14:59:57.601539: step 6434, loss 0.0227075, acc 1
2016-09-06T14:59:58.441052: step 6435, loss 0.0140637, acc 1
2016-09-06T14:59:59.252425: step 6436, loss 0.0209511, acc 1
2016-09-06T15:00:00.053459: step 6437, loss 0.00821497, acc 1
2016-09-06T15:00:00.924598: step 6438, loss 0.00363027, acc 1
2016-09-06T15:00:01.724334: step 6439, loss 0.00494252, acc 1
2016-09-06T15:00:02.513970: step 6440, loss 0.00842253, acc 1
2016-09-06T15:00:03.334389: step 6441, loss 0.0204309, acc 1
2016-09-06T15:00:04.121220: step 6442, loss 0.0347305, acc 0.98
2016-09-06T15:00:04.923674: step 6443, loss 0.0357199, acc 1
2016-09-06T15:00:05.712831: step 6444, loss 0.0348884, acc 0.98
2016-09-06T15:00:06.497299: step 6445, loss 0.0369317, acc 1
2016-09-06T15:00:07.318081: step 6446, loss 0.0536165, acc 0.94
2016-09-06T15:00:08.162736: step 6447, loss 0.00289095, acc 1
2016-09-06T15:00:08.964262: step 6448, loss 0.027228, acc 1
2016-09-06T15:00:09.774879: step 6449, loss 0.0290569, acc 1
2016-09-06T15:00:10.586337: step 6450, loss 0.00401873, acc 1
2016-09-06T15:00:11.389083: step 6451, loss 0.0435196, acc 1
2016-09-06T15:00:12.197064: step 6452, loss 0.00281924, acc 1
2016-09-06T15:00:12.993292: step 6453, loss 0.0371644, acc 0.98
2016-09-06T15:00:13.777277: step 6454, loss 0.00344464, acc 1
2016-09-06T15:00:14.562143: step 6455, loss 0.0968201, acc 0.94
2016-09-06T15:00:15.377739: step 6456, loss 0.059351, acc 0.96
2016-09-06T15:00:16.161212: step 6457, loss 0.0142666, acc 1
2016-09-06T15:00:16.995875: step 6458, loss 0.0236902, acc 0.98
2016-09-06T15:00:17.806724: step 6459, loss 0.0183103, acc 0.98
2016-09-06T15:00:18.592325: step 6460, loss 0.0170897, acc 0.98
2016-09-06T15:00:19.401895: step 6461, loss 0.00374164, acc 1
2016-09-06T15:00:20.221497: step 6462, loss 0.00642209, acc 1
2016-09-06T15:00:21.014133: step 6463, loss 0.0046058, acc 1
2016-09-06T15:00:21.839344: step 6464, loss 0.067781, acc 0.98
2016-09-06T15:00:22.674905: step 6465, loss 0.0276464, acc 1
2016-09-06T15:00:23.486408: step 6466, loss 0.0925587, acc 0.94
2016-09-06T15:00:24.296455: step 6467, loss 0.0271754, acc 1
2016-09-06T15:00:25.157159: step 6468, loss 0.0394911, acc 0.98
2016-09-06T15:00:25.971519: step 6469, loss 0.00853276, acc 1
2016-09-06T15:00:26.785614: step 6470, loss 0.0953941, acc 0.96
2016-09-06T15:00:27.607308: step 6471, loss 0.0399799, acc 0.98
2016-09-06T15:00:28.434123: step 6472, loss 0.0167858, acc 0.98
2016-09-06T15:00:29.260188: step 6473, loss 0.0333727, acc 0.96
2016-09-06T15:00:30.107891: step 6474, loss 0.0490653, acc 0.96
2016-09-06T15:00:30.921523: step 6475, loss 0.0788236, acc 0.98
2016-09-06T15:00:31.741961: step 6476, loss 0.0214129, acc 0.98
2016-09-06T15:00:32.609077: step 6477, loss 0.0235006, acc 1
2016-09-06T15:00:33.404659: step 6478, loss 0.00970272, acc 1
2016-09-06T15:00:34.256817: step 6479, loss 0.00778542, acc 1
2016-09-06T15:00:35.077714: step 6480, loss 0.00285524, acc 1
2016-09-06T15:00:35.886766: step 6481, loss 0.0170302, acc 1
2016-09-06T15:00:36.685528: step 6482, loss 0.0247754, acc 1
2016-09-06T15:00:37.483671: step 6483, loss 0.026405, acc 0.98
2016-09-06T15:00:38.320107: step 6484, loss 0.00477281, acc 1
2016-09-06T15:00:39.117318: step 6485, loss 0.0208402, acc 1
2016-09-06T15:00:39.937157: step 6486, loss 0.0361925, acc 0.98
2016-09-06T15:00:40.734411: step 6487, loss 0.00358804, acc 1
2016-09-06T15:00:41.545738: step 6488, loss 0.0479307, acc 0.98
2016-09-06T15:00:42.374941: step 6489, loss 0.013077, acc 1
2016-09-06T15:00:43.198708: step 6490, loss 0.00271556, acc 1
2016-09-06T15:00:44.001952: step 6491, loss 0.0424827, acc 0.98
2016-09-06T15:00:44.801501: step 6492, loss 0.0328319, acc 0.98
2016-09-06T15:00:45.615739: step 6493, loss 0.0199791, acc 1
2016-09-06T15:00:46.423174: step 6494, loss 0.0159358, acc 1
2016-09-06T15:00:47.243552: step 6495, loss 0.0283898, acc 1
2016-09-06T15:00:48.062473: step 6496, loss 0.0498986, acc 0.96
2016-09-06T15:00:48.848979: step 6497, loss 0.0418465, acc 0.98
2016-09-06T15:00:49.650061: step 6498, loss 0.148806, acc 0.96
2016-09-06T15:00:50.477186: step 6499, loss 0.0671501, acc 0.98
2016-09-06T15:00:51.290397: step 6500, loss 0.00630259, acc 1

Evaluation:
2016-09-06T15:00:55.041230: step 6500, loss 1.54293, acc 0.737336

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-6500

2016-09-06T15:00:56.971011: step 6501, loss 0.0150997, acc 1
2016-09-06T15:00:57.829208: step 6502, loss 0.0170302, acc 0.98
2016-09-06T15:00:58.634079: step 6503, loss 0.00267347, acc 1
2016-09-06T15:00:59.450921: step 6504, loss 0.00334074, acc 1
2016-09-06T15:01:00.294400: step 6505, loss 0.0219563, acc 1
2016-09-06T15:01:01.114877: step 6506, loss 0.0206427, acc 1
2016-09-06T15:01:01.929383: step 6507, loss 0.0275683, acc 1
2016-09-06T15:01:02.776475: step 6508, loss 0.0239085, acc 1
2016-09-06T15:01:03.638900: step 6509, loss 0.00413781, acc 1
2016-09-06T15:01:04.436911: step 6510, loss 0.0194965, acc 0.98
2016-09-06T15:01:05.282719: step 6511, loss 0.0457507, acc 0.96
2016-09-06T15:01:06.097986: step 6512, loss 0.00486925, acc 1
2016-09-06T15:01:06.903465: step 6513, loss 0.00858908, acc 1
2016-09-06T15:01:07.726504: step 6514, loss 0.0221467, acc 0.98
2016-09-06T15:01:08.533403: step 6515, loss 0.0242208, acc 0.98
2016-09-06T15:01:09.326422: step 6516, loss 0.137665, acc 0.96
2016-09-06T15:01:10.136621: step 6517, loss 0.0139557, acc 1
2016-09-06T15:01:10.968370: step 6518, loss 0.0451728, acc 0.96
2016-09-06T15:01:11.759926: step 6519, loss 0.011735, acc 1
2016-09-06T15:01:12.552941: step 6520, loss 0.0353241, acc 0.98
2016-09-06T15:01:13.382996: step 6521, loss 0.0820308, acc 0.98
2016-09-06T15:01:14.177468: step 6522, loss 0.032574, acc 0.98
2016-09-06T15:01:14.980705: step 6523, loss 0.0503269, acc 0.96
2016-09-06T15:01:15.809651: step 6524, loss 0.00878322, acc 1
2016-09-06T15:01:16.603153: step 6525, loss 0.011993, acc 1
2016-09-06T15:01:17.399311: step 6526, loss 0.0419526, acc 0.98
2016-09-06T15:01:18.245473: step 6527, loss 0.0183792, acc 0.98
2016-09-06T15:01:19.001273: step 6528, loss 0.0283926, acc 0.977273
2016-09-06T15:01:19.844892: step 6529, loss 0.0126799, acc 1
2016-09-06T15:01:20.662675: step 6530, loss 0.034774, acc 0.98
2016-09-06T15:01:21.463234: step 6531, loss 0.00535135, acc 1
2016-09-06T15:01:22.289779: step 6532, loss 0.0172319, acc 0.98
2016-09-06T15:01:23.100251: step 6533, loss 0.0254986, acc 1
2016-09-06T15:01:23.864303: step 6534, loss 0.00845889, acc 1
2016-09-06T15:01:24.660047: step 6535, loss 0.00271443, acc 1
2016-09-06T15:01:25.480761: step 6536, loss 0.00665084, acc 1
2016-09-06T15:01:26.290751: step 6537, loss 0.0711885, acc 0.96
2016-09-06T15:01:27.117498: step 6538, loss 0.00788305, acc 1
2016-09-06T15:01:27.952350: step 6539, loss 0.011924, acc 1
2016-09-06T15:01:28.757538: step 6540, loss 0.0290571, acc 0.98
2016-09-06T15:01:29.563118: step 6541, loss 0.00800075, acc 1
2016-09-06T15:01:30.402611: step 6542, loss 0.0323418, acc 0.98
2016-09-06T15:01:31.210213: step 6543, loss 0.0195296, acc 0.98
2016-09-06T15:01:32.032162: step 6544, loss 0.00544878, acc 1
2016-09-06T15:01:32.861764: step 6545, loss 0.00440877, acc 1
2016-09-06T15:01:33.681666: step 6546, loss 0.00328985, acc 1
2016-09-06T15:01:34.491635: step 6547, loss 0.015825, acc 1
2016-09-06T15:01:35.315882: step 6548, loss 0.0905604, acc 0.98
2016-09-06T15:01:36.126467: step 6549, loss 0.0184288, acc 1
2016-09-06T15:01:36.930704: step 6550, loss 0.00311524, acc 1
2016-09-06T15:01:37.749359: step 6551, loss 0.0177367, acc 1
2016-09-06T15:01:38.568500: step 6552, loss 0.0218619, acc 1
2016-09-06T15:01:39.380856: step 6553, loss 0.0326347, acc 0.98
2016-09-06T15:01:40.232788: step 6554, loss 0.0165003, acc 1
2016-09-06T15:01:41.026754: step 6555, loss 0.0670739, acc 0.96
2016-09-06T15:01:41.833696: step 6556, loss 0.0246326, acc 0.98
2016-09-06T15:01:42.681072: step 6557, loss 0.064731, acc 0.98
2016-09-06T15:01:43.500832: step 6558, loss 0.0289203, acc 0.98
2016-09-06T15:01:44.324022: step 6559, loss 0.0192265, acc 0.98
2016-09-06T15:01:45.157937: step 6560, loss 0.0193092, acc 0.98
2016-09-06T15:01:45.957458: step 6561, loss 0.00764204, acc 1
2016-09-06T15:01:46.782916: step 6562, loss 0.00450944, acc 1
2016-09-06T15:01:47.610044: step 6563, loss 0.00316069, acc 1
2016-09-06T15:01:48.425425: step 6564, loss 0.00972645, acc 1
2016-09-06T15:01:49.272963: step 6565, loss 0.0332104, acc 0.96
2016-09-06T15:01:50.081418: step 6566, loss 0.0361342, acc 0.98
2016-09-06T15:01:50.907207: step 6567, loss 0.00935219, acc 1
2016-09-06T15:01:51.681315: step 6568, loss 0.00562713, acc 1
2016-09-06T15:01:52.490329: step 6569, loss 0.00667188, acc 1
2016-09-06T15:01:53.301231: step 6570, loss 0.0190708, acc 1
2016-09-06T15:01:54.136578: step 6571, loss 0.0239038, acc 1
2016-09-06T15:01:54.947521: step 6572, loss 0.0213348, acc 0.98
2016-09-06T15:01:55.779366: step 6573, loss 0.0384803, acc 0.98
2016-09-06T15:01:56.573625: step 6574, loss 0.0288026, acc 0.98
2016-09-06T15:01:57.397327: step 6575, loss 0.0302561, acc 0.98
2016-09-06T15:01:58.241585: step 6576, loss 0.0172493, acc 1
2016-09-06T15:01:59.052381: step 6577, loss 0.00654878, acc 1
2016-09-06T15:01:59.839993: step 6578, loss 0.0204097, acc 0.98
2016-09-06T15:02:00.697081: step 6579, loss 0.012026, acc 1
2016-09-06T15:02:01.509874: step 6580, loss 0.115076, acc 0.98
2016-09-06T15:02:02.322359: step 6581, loss 0.00592073, acc 1
2016-09-06T15:02:03.141076: step 6582, loss 0.00297713, acc 1
2016-09-06T15:02:03.955957: step 6583, loss 0.00342103, acc 1
2016-09-06T15:02:04.759851: step 6584, loss 0.0956356, acc 0.98
2016-09-06T15:02:05.610181: step 6585, loss 0.0157373, acc 1
2016-09-06T15:02:06.409851: step 6586, loss 0.0153706, acc 1
2016-09-06T15:02:07.214012: step 6587, loss 0.0369928, acc 0.98
2016-09-06T15:02:08.046640: step 6588, loss 0.0379268, acc 0.96
2016-09-06T15:02:08.847402: step 6589, loss 0.00241724, acc 1
2016-09-06T15:02:09.659118: step 6590, loss 0.0183331, acc 1
2016-09-06T15:02:10.520616: step 6591, loss 0.0187854, acc 0.98
2016-09-06T15:02:11.319968: step 6592, loss 0.0331006, acc 0.98
2016-09-06T15:02:12.129539: step 6593, loss 0.012183, acc 1
2016-09-06T15:02:12.978485: step 6594, loss 0.0162217, acc 1
2016-09-06T15:02:13.781510: step 6595, loss 0.0159982, acc 1
2016-09-06T15:02:14.575694: step 6596, loss 0.0264383, acc 1
2016-09-06T15:02:15.409105: step 6597, loss 0.0465503, acc 0.96
2016-09-06T15:02:16.232807: step 6598, loss 0.017671, acc 1
2016-09-06T15:02:17.010779: step 6599, loss 0.00936525, acc 1
2016-09-06T15:02:17.830493: step 6600, loss 0.0262131, acc 0.98

Evaluation:
2016-09-06T15:02:21.561449: step 6600, loss 1.73687, acc 0.746717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-6600

2016-09-06T15:02:23.402006: step 6601, loss 0.0494436, acc 0.98
2016-09-06T15:02:24.210991: step 6602, loss 0.00258277, acc 1
2016-09-06T15:02:25.054724: step 6603, loss 0.0912948, acc 0.98
2016-09-06T15:02:25.841081: step 6604, loss 0.0767414, acc 0.98
2016-09-06T15:02:26.650052: step 6605, loss 0.0165127, acc 0.98
2016-09-06T15:02:27.486282: step 6606, loss 0.0392535, acc 0.96
2016-09-06T15:02:28.315626: step 6607, loss 0.0432578, acc 0.98
2016-09-06T15:02:29.154168: step 6608, loss 0.0150724, acc 1
2016-09-06T15:02:29.982212: step 6609, loss 0.00395094, acc 1
2016-09-06T15:02:30.794697: step 6610, loss 0.0109453, acc 1
2016-09-06T15:02:31.602550: step 6611, loss 0.0276571, acc 0.98
2016-09-06T15:02:32.435653: step 6612, loss 0.016081, acc 1
2016-09-06T15:02:33.250778: step 6613, loss 0.0219665, acc 0.98
2016-09-06T15:02:34.019709: step 6614, loss 0.00771549, acc 1
2016-09-06T15:02:34.843333: step 6615, loss 0.0217509, acc 0.98
2016-09-06T15:02:35.663341: step 6616, loss 0.0346428, acc 0.98
2016-09-06T15:02:36.465059: step 6617, loss 0.0201444, acc 1
2016-09-06T15:02:37.268034: step 6618, loss 0.0171909, acc 1
2016-09-06T15:02:38.073605: step 6619, loss 0.0734533, acc 0.96
2016-09-06T15:02:38.894548: step 6620, loss 0.00281431, acc 1
2016-09-06T15:02:39.687771: step 6621, loss 0.0193916, acc 0.98
2016-09-06T15:02:40.478462: step 6622, loss 0.0039476, acc 1
2016-09-06T15:02:41.289273: step 6623, loss 0.0279616, acc 0.98
2016-09-06T15:02:42.078703: step 6624, loss 0.00455425, acc 1
2016-09-06T15:02:42.879692: step 6625, loss 0.00267876, acc 1
2016-09-06T15:02:43.683141: step 6626, loss 0.0380736, acc 0.98
2016-09-06T15:02:44.511811: step 6627, loss 0.0326672, acc 0.96
2016-09-06T15:02:45.329796: step 6628, loss 0.014094, acc 1
2016-09-06T15:02:46.108949: step 6629, loss 0.0930101, acc 0.98
2016-09-06T15:02:46.918619: step 6630, loss 0.016567, acc 0.98
2016-09-06T15:02:47.763753: step 6631, loss 0.0218045, acc 1
2016-09-06T15:02:48.561808: step 6632, loss 0.00247708, acc 1
2016-09-06T15:02:49.358262: step 6633, loss 0.0227944, acc 0.98
2016-09-06T15:02:50.177630: step 6634, loss 0.00654098, acc 1
2016-09-06T15:02:50.968286: step 6635, loss 0.00486431, acc 1
2016-09-06T15:02:51.777407: step 6636, loss 0.048777, acc 0.96
2016-09-06T15:02:52.596767: step 6637, loss 0.0419084, acc 0.96
2016-09-06T15:02:53.372502: step 6638, loss 0.0215764, acc 0.98
2016-09-06T15:02:54.186472: step 6639, loss 0.00360212, acc 1
2016-09-06T15:02:55.006995: step 6640, loss 0.0660172, acc 0.94
2016-09-06T15:02:55.801697: step 6641, loss 0.02761, acc 0.98
2016-09-06T15:02:56.593024: step 6642, loss 0.0141848, acc 1
2016-09-06T15:02:57.388819: step 6643, loss 0.0382589, acc 1
2016-09-06T15:02:58.191819: step 6644, loss 0.00535963, acc 1
2016-09-06T15:02:59.013359: step 6645, loss 0.0223425, acc 0.98
2016-09-06T15:02:59.861247: step 6646, loss 0.00235506, acc 1
2016-09-06T15:03:00.692302: step 6647, loss 0.0130869, acc 1
2016-09-06T15:03:01.502437: step 6648, loss 0.0288195, acc 1
2016-09-06T15:03:02.314092: step 6649, loss 0.00246642, acc 1
2016-09-06T15:03:03.110012: step 6650, loss 0.00318863, acc 1
2016-09-06T15:03:03.911334: step 6651, loss 0.00239545, acc 1
2016-09-06T15:03:04.720584: step 6652, loss 0.0118702, acc 1
2016-09-06T15:03:05.492679: step 6653, loss 0.00509638, acc 1
2016-09-06T15:03:06.302453: step 6654, loss 0.00249243, acc 1
2016-09-06T15:03:07.157132: step 6655, loss 0.00296586, acc 1
2016-09-06T15:03:07.960362: step 6656, loss 0.00714794, acc 1
2016-09-06T15:03:08.755525: step 6657, loss 0.0397043, acc 0.98
2016-09-06T15:03:09.561588: step 6658, loss 0.0057969, acc 1
2016-09-06T15:03:10.358685: step 6659, loss 0.0166455, acc 1
2016-09-06T15:03:11.142583: step 6660, loss 0.0156169, acc 1
2016-09-06T15:03:11.944054: step 6661, loss 0.03027, acc 0.98
2016-09-06T15:03:12.730874: step 6662, loss 0.0493921, acc 0.96
2016-09-06T15:03:13.535543: step 6663, loss 0.0448585, acc 0.98
2016-09-06T15:03:14.355252: step 6664, loss 0.00273395, acc 1
2016-09-06T15:03:15.140794: step 6665, loss 0.040732, acc 0.98
2016-09-06T15:03:15.961273: step 6666, loss 0.0302689, acc 1
2016-09-06T15:03:16.762158: step 6667, loss 0.00274382, acc 1
2016-09-06T15:03:17.546665: step 6668, loss 0.00303904, acc 1
2016-09-06T15:03:18.354366: step 6669, loss 0.027325, acc 0.98
2016-09-06T15:03:19.154304: step 6670, loss 0.00268924, acc 1
2016-09-06T15:03:19.951837: step 6671, loss 0.0283354, acc 1
2016-09-06T15:03:20.779224: step 6672, loss 0.0238614, acc 1
2016-09-06T15:03:21.594507: step 6673, loss 0.0052564, acc 1
2016-09-06T15:03:22.395666: step 6674, loss 0.0249147, acc 0.98
2016-09-06T15:03:23.200878: step 6675, loss 0.0692847, acc 0.96
2016-09-06T15:03:24.016087: step 6676, loss 0.0456792, acc 0.98
2016-09-06T15:03:24.806995: step 6677, loss 0.0107182, acc 1
2016-09-06T15:03:25.610299: step 6678, loss 0.027377, acc 1
2016-09-06T15:03:26.434958: step 6679, loss 0.00607895, acc 1
2016-09-06T15:03:27.231813: step 6680, loss 0.0033066, acc 1
2016-09-06T15:03:28.036063: step 6681, loss 0.0379358, acc 0.98
2016-09-06T15:03:28.842248: step 6682, loss 0.0377124, acc 0.98
2016-09-06T15:03:29.629317: step 6683, loss 0.050942, acc 0.96
2016-09-06T15:03:30.442682: step 6684, loss 0.0203605, acc 1
2016-09-06T15:03:31.262336: step 6685, loss 0.00368943, acc 1
2016-09-06T15:03:32.077411: step 6686, loss 0.0451205, acc 0.98
2016-09-06T15:03:32.872014: step 6687, loss 0.00782202, acc 1
2016-09-06T15:03:33.723945: step 6688, loss 0.0474197, acc 0.98
2016-09-06T15:03:34.495060: step 6689, loss 0.0158182, acc 1
2016-09-06T15:03:35.269799: step 6690, loss 0.0165723, acc 1
2016-09-06T15:03:36.082914: step 6691, loss 0.0346975, acc 0.98
2016-09-06T15:03:36.873445: step 6692, loss 0.00282812, acc 1
2016-09-06T15:03:37.662855: step 6693, loss 0.00297509, acc 1
2016-09-06T15:03:38.470216: step 6694, loss 0.0500716, acc 0.96
2016-09-06T15:03:39.260201: step 6695, loss 0.0181816, acc 0.98
2016-09-06T15:03:40.079772: step 6696, loss 0.0209071, acc 1
2016-09-06T15:03:40.894971: step 6697, loss 0.132564, acc 0.9
2016-09-06T15:03:41.705069: step 6698, loss 0.00939803, acc 1
2016-09-06T15:03:42.534477: step 6699, loss 0.0153145, acc 1
2016-09-06T15:03:43.368234: step 6700, loss 0.0196631, acc 1

Evaluation:
2016-09-06T15:03:47.130703: step 6700, loss 1.72111, acc 0.744841

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-6700

2016-09-06T15:03:49.046714: step 6701, loss 0.00231132, acc 1
2016-09-06T15:03:49.841409: step 6702, loss 0.0427018, acc 0.96
2016-09-06T15:03:50.664213: step 6703, loss 0.0102512, acc 1
2016-09-06T15:03:51.441892: step 6704, loss 0.00283016, acc 1
2016-09-06T15:03:52.248430: step 6705, loss 0.0349615, acc 0.98
2016-09-06T15:03:53.074677: step 6706, loss 0.151133, acc 0.94
2016-09-06T15:03:53.887653: step 6707, loss 0.0278765, acc 0.98
2016-09-06T15:03:54.699775: step 6708, loss 0.0423353, acc 0.98
2016-09-06T15:03:55.530693: step 6709, loss 0.0102208, acc 1
2016-09-06T15:03:56.345919: step 6710, loss 0.00841536, acc 1
2016-09-06T15:03:57.159518: step 6711, loss 0.0103999, acc 1
2016-09-06T15:03:57.970468: step 6712, loss 0.0208657, acc 0.98
2016-09-06T15:03:58.779013: step 6713, loss 0.00326853, acc 1
2016-09-06T15:03:59.574471: step 6714, loss 0.0056329, acc 1
2016-09-06T15:04:00.435280: step 6715, loss 0.0203548, acc 0.98
2016-09-06T15:04:01.226598: step 6716, loss 0.00780866, acc 1
2016-09-06T15:04:02.013998: step 6717, loss 0.0529347, acc 0.98
2016-09-06T15:04:02.858001: step 6718, loss 0.020997, acc 1
2016-09-06T15:04:03.624754: step 6719, loss 0.0382431, acc 0.98
2016-09-06T15:04:04.373614: step 6720, loss 0.00554791, acc 1
2016-09-06T15:04:05.254544: step 6721, loss 0.00632399, acc 1
2016-09-06T15:04:06.044728: step 6722, loss 0.0415545, acc 0.98
2016-09-06T15:04:06.859658: step 6723, loss 0.0270381, acc 0.98
2016-09-06T15:04:07.683134: step 6724, loss 0.0176363, acc 1
2016-09-06T15:04:08.460654: step 6725, loss 0.0339745, acc 0.98
2016-09-06T15:04:09.247201: step 6726, loss 0.0144059, acc 1
2016-09-06T15:04:10.068678: step 6727, loss 0.01563, acc 1
2016-09-06T15:04:10.854380: step 6728, loss 0.0343268, acc 0.98
2016-09-06T15:04:11.672020: step 6729, loss 0.0268261, acc 0.98
2016-09-06T15:04:12.507496: step 6730, loss 0.0640904, acc 0.98
2016-09-06T15:04:13.284719: step 6731, loss 0.00842239, acc 1
2016-09-06T15:04:14.082071: step 6732, loss 0.0114672, acc 1
2016-09-06T15:04:14.901060: step 6733, loss 0.0569512, acc 0.98
2016-09-06T15:04:15.678522: step 6734, loss 0.022599, acc 1
2016-09-06T15:04:16.495321: step 6735, loss 0.0234823, acc 1
2016-09-06T15:04:17.339546: step 6736, loss 0.0180652, acc 1
2016-09-06T15:04:18.097749: step 6737, loss 0.0168563, acc 0.98
2016-09-06T15:04:18.884918: step 6738, loss 0.00267525, acc 1
2016-09-06T15:04:19.697593: step 6739, loss 0.0270437, acc 1
2016-09-06T15:04:20.486236: step 6740, loss 0.0337334, acc 0.98
2016-09-06T15:04:21.286126: step 6741, loss 0.00874417, acc 1
2016-09-06T15:04:22.142034: step 6742, loss 0.052059, acc 0.96
2016-09-06T15:04:22.949834: step 6743, loss 0.00803289, acc 1
2016-09-06T15:04:23.750844: step 6744, loss 0.0105163, acc 1
2016-09-06T15:04:24.590001: step 6745, loss 0.00251994, acc 1
2016-09-06T15:04:25.386645: step 6746, loss 0.0026403, acc 1
2016-09-06T15:04:26.197762: step 6747, loss 0.00384658, acc 1
2016-09-06T15:04:27.052489: step 6748, loss 0.0123133, acc 1
2016-09-06T15:04:27.875702: step 6749, loss 0.00355752, acc 1
2016-09-06T15:04:28.685728: step 6750, loss 0.0066971, acc 1
2016-09-06T15:04:29.543525: step 6751, loss 0.00286264, acc 1
2016-09-06T15:04:30.363767: step 6752, loss 0.0121763, acc 1
2016-09-06T15:04:31.182681: step 6753, loss 0.00632492, acc 1
2016-09-06T15:04:32.005977: step 6754, loss 0.115985, acc 0.98
2016-09-06T15:04:32.806450: step 6755, loss 0.00279518, acc 1
2016-09-06T15:04:33.629836: step 6756, loss 0.00550666, acc 1
2016-09-06T15:04:34.483445: step 6757, loss 0.0334677, acc 1
2016-09-06T15:04:35.335713: step 6758, loss 0.0230027, acc 1
2016-09-06T15:04:36.153477: step 6759, loss 0.0402492, acc 0.98
2016-09-06T15:04:37.016015: step 6760, loss 0.0038138, acc 1
2016-09-06T15:04:37.839347: step 6761, loss 0.0168539, acc 0.98
2016-09-06T15:04:38.634303: step 6762, loss 0.0857447, acc 0.98
2016-09-06T15:04:39.446048: step 6763, loss 0.00692419, acc 1
2016-09-06T15:04:40.268549: step 6764, loss 0.00288616, acc 1
2016-09-06T15:04:41.065859: step 6765, loss 0.0500528, acc 0.96
2016-09-06T15:04:41.872981: step 6766, loss 0.0521549, acc 0.98
2016-09-06T15:04:42.713760: step 6767, loss 0.025771, acc 0.98
2016-09-06T15:04:43.498323: step 6768, loss 0.0226944, acc 1
2016-09-06T15:04:44.305795: step 6769, loss 0.0169196, acc 1
2016-09-06T15:04:45.130879: step 6770, loss 0.019076, acc 1
2016-09-06T15:04:45.889518: step 6771, loss 0.0108324, acc 1
2016-09-06T15:04:46.706010: step 6772, loss 0.0231569, acc 1
2016-09-06T15:04:47.520834: step 6773, loss 0.00915305, acc 1
2016-09-06T15:04:48.325624: step 6774, loss 0.00292883, acc 1
2016-09-06T15:04:49.129771: step 6775, loss 0.0195365, acc 0.98
2016-09-06T15:04:49.962083: step 6776, loss 0.00446109, acc 1
2016-09-06T15:04:50.741382: step 6777, loss 0.0126218, acc 1
2016-09-06T15:04:51.563523: step 6778, loss 0.00900955, acc 1
2016-09-06T15:04:52.391478: step 6779, loss 0.0396465, acc 0.98
2016-09-06T15:04:53.194985: step 6780, loss 0.0529968, acc 0.96
2016-09-06T15:04:54.029664: step 6781, loss 0.0148182, acc 1
2016-09-06T15:04:54.905794: step 6782, loss 0.0125755, acc 1
2016-09-06T15:04:55.741790: step 6783, loss 0.0204523, acc 0.98
2016-09-06T15:04:56.556058: step 6784, loss 0.0950137, acc 0.94
2016-09-06T15:04:57.408913: step 6785, loss 0.0750909, acc 0.98
2016-09-06T15:04:58.221459: step 6786, loss 0.0667983, acc 0.94
2016-09-06T15:04:59.033779: step 6787, loss 0.0197746, acc 0.98
2016-09-06T15:04:59.862349: step 6788, loss 0.0472188, acc 0.98
2016-09-06T15:05:00.744599: step 6789, loss 0.0325118, acc 1
2016-09-06T15:05:01.550478: step 6790, loss 0.0296408, acc 0.98
2016-09-06T15:05:02.370513: step 6791, loss 0.0518266, acc 0.96
2016-09-06T15:05:03.166960: step 6792, loss 0.00567416, acc 1
2016-09-06T15:05:03.960628: step 6793, loss 0.0403504, acc 0.96
2016-09-06T15:05:04.780772: step 6794, loss 0.0220105, acc 1
2016-09-06T15:05:05.616250: step 6795, loss 0.0406971, acc 1
2016-09-06T15:05:06.401558: step 6796, loss 0.0103706, acc 1
2016-09-06T15:05:07.213127: step 6797, loss 0.0545594, acc 0.96
2016-09-06T15:05:08.045570: step 6798, loss 0.0198785, acc 1
2016-09-06T15:05:08.861158: step 6799, loss 0.0239192, acc 1
2016-09-06T15:05:09.647104: step 6800, loss 0.00536819, acc 1

Evaluation:
2016-09-06T15:05:13.360976: step 6800, loss 1.80747, acc 0.745779

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-6800

2016-09-06T15:05:15.302795: step 6801, loss 0.0587657, acc 0.98
2016-09-06T15:05:16.122688: step 6802, loss 0.0177283, acc 0.98
2016-09-06T15:05:16.948305: step 6803, loss 0.0883151, acc 0.98
2016-09-06T15:05:17.775009: step 6804, loss 0.00274809, acc 1
2016-09-06T15:05:18.582794: step 6805, loss 0.00480658, acc 1
2016-09-06T15:05:19.414489: step 6806, loss 0.01669, acc 1
2016-09-06T15:05:20.233119: step 6807, loss 0.00528831, acc 1
2016-09-06T15:05:20.994549: step 6808, loss 0.0110618, acc 1
2016-09-06T15:05:21.789528: step 6809, loss 0.0417538, acc 0.96
2016-09-06T15:05:22.586018: step 6810, loss 0.011666, acc 1
2016-09-06T15:05:23.388571: step 6811, loss 0.0502232, acc 0.96
2016-09-06T15:05:24.193603: step 6812, loss 0.0629011, acc 0.96
2016-09-06T15:05:25.021745: step 6813, loss 0.074658, acc 0.94
2016-09-06T15:05:25.840533: step 6814, loss 0.0278357, acc 0.98
2016-09-06T15:05:26.644920: step 6815, loss 0.018183, acc 1
2016-09-06T15:05:27.470573: step 6816, loss 0.00583313, acc 1
2016-09-06T15:05:28.232094: step 6817, loss 0.0136718, acc 1
2016-09-06T15:05:29.041111: step 6818, loss 0.113154, acc 0.96
2016-09-06T15:05:29.851695: step 6819, loss 0.049457, acc 0.96
2016-09-06T15:05:30.634270: step 6820, loss 0.0357718, acc 0.98
2016-09-06T15:05:31.453529: step 6821, loss 0.0275393, acc 1
2016-09-06T15:05:32.271095: step 6822, loss 0.006463, acc 1
2016-09-06T15:05:33.073416: step 6823, loss 0.0246458, acc 1
2016-09-06T15:05:33.899107: step 6824, loss 0.0516331, acc 0.98
2016-09-06T15:05:34.743086: step 6825, loss 0.0384307, acc 1
2016-09-06T15:05:35.509480: step 6826, loss 0.0100221, acc 1
2016-09-06T15:05:36.308999: step 6827, loss 0.029433, acc 1
2016-09-06T15:05:37.157243: step 6828, loss 0.0442956, acc 0.96
2016-09-06T15:05:37.964217: step 6829, loss 0.0235011, acc 1
2016-09-06T15:05:38.769597: step 6830, loss 0.0583671, acc 0.96
2016-09-06T15:05:39.606113: step 6831, loss 0.0168341, acc 1
2016-09-06T15:05:40.403465: step 6832, loss 0.0216082, acc 1
2016-09-06T15:05:41.244517: step 6833, loss 0.00342324, acc 1
2016-09-06T15:05:42.086365: step 6834, loss 0.0160187, acc 1
2016-09-06T15:05:42.865696: step 6835, loss 0.0474542, acc 0.96
2016-09-06T15:05:43.673558: step 6836, loss 0.0116507, acc 1
2016-09-06T15:05:44.519491: step 6837, loss 0.0399499, acc 0.98
2016-09-06T15:05:45.336182: step 6838, loss 0.00365598, acc 1
2016-09-06T15:05:46.144400: step 6839, loss 0.0565872, acc 0.98
2016-09-06T15:05:46.960967: step 6840, loss 0.012007, acc 1
2016-09-06T15:05:47.781602: step 6841, loss 0.0701573, acc 0.98
2016-09-06T15:05:48.601637: step 6842, loss 0.0140255, acc 1
2016-09-06T15:05:49.455598: step 6843, loss 0.0155323, acc 1
2016-09-06T15:05:50.247913: step 6844, loss 0.0504425, acc 0.98
2016-09-06T15:05:51.034353: step 6845, loss 0.043249, acc 0.98
2016-09-06T15:05:51.873755: step 6846, loss 0.0157923, acc 1
2016-09-06T15:05:52.715180: step 6847, loss 0.0305212, acc 0.98
2016-09-06T15:05:53.547256: step 6848, loss 0.0771514, acc 0.96
2016-09-06T15:05:54.393632: step 6849, loss 0.00699779, acc 1
2016-09-06T15:05:55.191375: step 6850, loss 0.026015, acc 1
2016-09-06T15:05:55.979934: step 6851, loss 0.0360298, acc 0.98
2016-09-06T15:05:56.812906: step 6852, loss 0.00701626, acc 1
2016-09-06T15:05:57.644244: step 6853, loss 0.00415215, acc 1
2016-09-06T15:05:58.485052: step 6854, loss 0.0274894, acc 0.98
2016-09-06T15:05:59.304777: step 6855, loss 0.0794613, acc 0.94
2016-09-06T15:06:00.138596: step 6856, loss 0.018571, acc 1
2016-09-06T15:06:00.978764: step 6857, loss 0.00500073, acc 1
2016-09-06T15:06:01.797339: step 6858, loss 0.0212382, acc 1
2016-09-06T15:06:02.632997: step 6859, loss 0.00406951, acc 1
2016-09-06T15:06:03.443067: step 6860, loss 0.00404436, acc 1
2016-09-06T15:06:04.250194: step 6861, loss 0.0155589, acc 1
2016-09-06T15:06:05.066558: step 6862, loss 0.00325962, acc 1
2016-09-06T15:06:05.879999: step 6863, loss 0.0854315, acc 0.98
2016-09-06T15:06:06.705300: step 6864, loss 0.0471283, acc 0.96
2016-09-06T15:06:07.518825: step 6865, loss 0.0344484, acc 0.98
2016-09-06T15:06:08.340652: step 6866, loss 0.0382198, acc 0.98
2016-09-06T15:06:09.134333: step 6867, loss 0.0178679, acc 0.98
2016-09-06T15:06:09.954949: step 6868, loss 0.00356981, acc 1
2016-09-06T15:06:10.774625: step 6869, loss 0.083107, acc 0.96
2016-09-06T15:06:11.576852: step 6870, loss 0.0431193, acc 0.98
2016-09-06T15:06:12.416567: step 6871, loss 0.0411758, acc 0.98
2016-09-06T15:06:13.199408: step 6872, loss 0.00367437, acc 1
2016-09-06T15:06:13.995184: step 6873, loss 0.0170993, acc 1
2016-09-06T15:06:14.831530: step 6874, loss 0.00591563, acc 1
2016-09-06T15:06:15.625634: step 6875, loss 0.00713352, acc 1
2016-09-06T15:06:16.429892: step 6876, loss 0.0682079, acc 0.98
2016-09-06T15:06:17.304968: step 6877, loss 0.0258553, acc 1
2016-09-06T15:06:18.111260: step 6878, loss 0.0196174, acc 0.98
2016-09-06T15:06:18.956905: step 6879, loss 0.0366717, acc 0.98
2016-09-06T15:06:19.778080: step 6880, loss 0.0296936, acc 0.98
2016-09-06T15:06:20.600336: step 6881, loss 0.00578672, acc 1
2016-09-06T15:06:21.401702: step 6882, loss 0.00401142, acc 1
2016-09-06T15:06:22.240098: step 6883, loss 0.0255402, acc 1
2016-09-06T15:06:23.076842: step 6884, loss 0.0246427, acc 0.98
2016-09-06T15:06:23.858634: step 6885, loss 0.0298119, acc 0.98
2016-09-06T15:06:24.660689: step 6886, loss 0.0266022, acc 0.98
2016-09-06T15:06:25.512325: step 6887, loss 0.00969992, acc 1
2016-09-06T15:06:26.300629: step 6888, loss 0.0148184, acc 1
2016-09-06T15:06:27.109604: step 6889, loss 0.110209, acc 0.94
2016-09-06T15:06:27.934850: step 6890, loss 0.0169623, acc 1
2016-09-06T15:06:28.723992: step 6891, loss 0.00456564, acc 1
2016-09-06T15:06:29.537446: step 6892, loss 0.0190362, acc 1
2016-09-06T15:06:30.338924: step 6893, loss 0.0625838, acc 0.98
2016-09-06T15:06:31.127035: step 6894, loss 0.0997045, acc 0.96
2016-09-06T15:06:31.920140: step 6895, loss 0.026127, acc 1
2016-09-06T15:06:32.748447: step 6896, loss 0.00643213, acc 1
2016-09-06T15:06:33.548911: step 6897, loss 0.0326254, acc 0.98
2016-09-06T15:06:34.345144: step 6898, loss 0.0689678, acc 0.94
2016-09-06T15:06:35.157145: step 6899, loss 0.0282228, acc 0.98
2016-09-06T15:06:35.927160: step 6900, loss 0.0290179, acc 0.98

Evaluation:
2016-09-06T15:06:39.640058: step 6900, loss 1.52321, acc 0.732645

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-6900

2016-09-06T15:06:41.506978: step 6901, loss 0.00260208, acc 1
2016-09-06T15:06:42.351282: step 6902, loss 0.0344563, acc 0.98
2016-09-06T15:06:43.163436: step 6903, loss 0.00587614, acc 1
2016-09-06T15:06:44.006818: step 6904, loss 0.0436133, acc 0.98
2016-09-06T15:06:44.828552: step 6905, loss 0.0191224, acc 1
2016-09-06T15:06:45.666330: step 6906, loss 0.03941, acc 1
2016-09-06T15:06:46.486864: step 6907, loss 0.0127147, acc 1
2016-09-06T15:06:47.311771: step 6908, loss 0.00256571, acc 1
2016-09-06T15:06:48.130138: step 6909, loss 0.0235479, acc 1
2016-09-06T15:06:48.928447: step 6910, loss 0.0049689, acc 1
2016-09-06T15:06:49.754455: step 6911, loss 0.00655101, acc 1
2016-09-06T15:06:50.489112: step 6912, loss 0.0434629, acc 0.977273
2016-09-06T15:06:51.314293: step 6913, loss 0.00871817, acc 1
2016-09-06T15:06:52.112575: step 6914, loss 0.0363501, acc 0.98
2016-09-06T15:06:52.887805: step 6915, loss 0.0151769, acc 1
2016-09-06T15:06:53.734637: step 6916, loss 0.0225452, acc 0.98
2016-09-06T15:06:54.587526: step 6917, loss 0.00337848, acc 1
2016-09-06T15:06:55.393567: step 6918, loss 0.0216576, acc 0.98
2016-09-06T15:06:56.237894: step 6919, loss 0.0288793, acc 1
2016-09-06T15:06:57.055186: step 6920, loss 0.0272077, acc 0.98
2016-09-06T15:06:57.865197: step 6921, loss 0.0399783, acc 0.98
2016-09-06T15:06:58.667669: step 6922, loss 0.00291658, acc 1
2016-09-06T15:06:59.511919: step 6923, loss 0.00632917, acc 1
2016-09-06T15:07:00.329211: step 6924, loss 0.0633243, acc 0.98
2016-09-06T15:07:01.137390: step 6925, loss 0.0285223, acc 0.98
2016-09-06T15:07:01.957008: step 6926, loss 0.0403709, acc 0.96
2016-09-06T15:07:02.760748: step 6927, loss 0.0798701, acc 0.96
2016-09-06T15:07:03.562556: step 6928, loss 0.0100685, acc 1
2016-09-06T15:07:04.396285: step 6929, loss 0.0118241, acc 1
2016-09-06T15:07:05.189398: step 6930, loss 0.00706688, acc 1
2016-09-06T15:07:06.000763: step 6931, loss 0.0152254, acc 1
2016-09-06T15:07:06.810385: step 6932, loss 0.0326716, acc 0.98
2016-09-06T15:07:07.579335: step 6933, loss 0.00508057, acc 1
2016-09-06T15:07:08.382878: step 6934, loss 0.0519827, acc 0.98
2016-09-06T15:07:09.184813: step 6935, loss 0.00443314, acc 1
2016-09-06T15:07:09.975110: step 6936, loss 0.00321631, acc 1
2016-09-06T15:07:10.771683: step 6937, loss 0.0380789, acc 0.98
2016-09-06T15:07:11.588882: step 6938, loss 0.0895853, acc 0.98
2016-09-06T15:07:12.413868: step 6939, loss 0.00798689, acc 1
2016-09-06T15:07:13.231887: step 6940, loss 0.00640468, acc 1
2016-09-06T15:07:14.075128: step 6941, loss 0.0184116, acc 0.98
2016-09-06T15:07:14.896016: step 6942, loss 0.0606916, acc 0.96
2016-09-06T15:07:15.702863: step 6943, loss 0.00561544, acc 1
2016-09-06T15:07:16.521155: step 6944, loss 0.015852, acc 1
2016-09-06T15:07:17.342599: step 6945, loss 0.00558037, acc 1
2016-09-06T15:07:18.160062: step 6946, loss 0.0473243, acc 0.98
2016-09-06T15:07:19.002667: step 6947, loss 0.00852184, acc 1
2016-09-06T15:07:19.813958: step 6948, loss 0.0140465, acc 1
2016-09-06T15:07:20.648797: step 6949, loss 0.0700894, acc 0.96
2016-09-06T15:07:21.488789: step 6950, loss 0.00435436, acc 1
2016-09-06T15:07:22.308056: step 6951, loss 0.0313935, acc 0.98
2016-09-06T15:07:23.116058: step 6952, loss 0.0156058, acc 1
2016-09-06T15:07:23.949817: step 6953, loss 0.0564993, acc 0.98
2016-09-06T15:07:24.801542: step 6954, loss 0.00885178, acc 1
2016-09-06T15:07:25.599583: step 6955, loss 0.0148466, acc 1
2016-09-06T15:07:26.442666: step 6956, loss 0.0319694, acc 1
2016-09-06T15:07:27.268845: step 6957, loss 0.0269584, acc 1
2016-09-06T15:07:28.070193: step 6958, loss 0.0327166, acc 0.98
2016-09-06T15:07:28.877864: step 6959, loss 0.0558926, acc 0.96
2016-09-06T15:07:29.690509: step 6960, loss 0.0129324, acc 1
2016-09-06T15:07:30.507250: step 6961, loss 0.0110285, acc 1
2016-09-06T15:07:31.323002: step 6962, loss 0.00809892, acc 1
2016-09-06T15:07:32.154267: step 6963, loss 0.0930315, acc 0.98
2016-09-06T15:07:32.973588: step 6964, loss 0.0218192, acc 1
2016-09-06T15:07:33.803794: step 6965, loss 0.00267449, acc 1
2016-09-06T15:07:34.611903: step 6966, loss 0.0310336, acc 0.98
2016-09-06T15:07:35.386939: step 6967, loss 0.00350519, acc 1
2016-09-06T15:07:36.180579: step 6968, loss 0.0299755, acc 0.98
2016-09-06T15:07:36.995922: step 6969, loss 0.0274217, acc 0.98
2016-09-06T15:07:37.800655: step 6970, loss 0.11945, acc 0.98
2016-09-06T15:07:38.629411: step 6971, loss 0.0192679, acc 1
2016-09-06T15:07:39.459300: step 6972, loss 0.025744, acc 1
2016-09-06T15:07:40.262367: step 6973, loss 0.00255144, acc 1
2016-09-06T15:07:41.083613: step 6974, loss 0.0439303, acc 0.98
2016-09-06T15:07:41.902432: step 6975, loss 0.00305305, acc 1
2016-09-06T15:07:42.707906: step 6976, loss 0.0272899, acc 0.98
2016-09-06T15:07:43.507273: step 6977, loss 0.025145, acc 0.98
2016-09-06T15:07:44.345351: step 6978, loss 0.00737349, acc 1
2016-09-06T15:07:45.151725: step 6979, loss 0.0176633, acc 0.98
2016-09-06T15:07:45.956652: step 6980, loss 0.0453387, acc 0.96
2016-09-06T15:07:46.811115: step 6981, loss 0.0187858, acc 1
2016-09-06T15:07:47.618667: step 6982, loss 0.0203443, acc 1
2016-09-06T15:07:48.459883: step 6983, loss 0.00845756, acc 1
2016-09-06T15:07:49.306623: step 6984, loss 0.0197625, acc 0.98
2016-09-06T15:07:50.120340: step 6985, loss 0.033108, acc 0.98
2016-09-06T15:07:50.933985: step 6986, loss 0.00386126, acc 1
2016-09-06T15:07:51.763476: step 6987, loss 0.0271849, acc 1
2016-09-06T15:07:52.564404: step 6988, loss 0.0419711, acc 0.98
2016-09-06T15:07:53.376140: step 6989, loss 0.00320317, acc 1
2016-09-06T15:07:54.186488: step 6990, loss 0.0234511, acc 0.98
2016-09-06T15:07:55.002988: step 6991, loss 0.0250557, acc 1
2016-09-06T15:07:55.802505: step 6992, loss 0.0243551, acc 1
2016-09-06T15:07:56.627692: step 6993, loss 0.00265879, acc 1
2016-09-06T15:07:57.450541: step 6994, loss 0.00275551, acc 1
2016-09-06T15:07:58.282315: step 6995, loss 0.0265718, acc 0.98
2016-09-06T15:07:59.112671: step 6996, loss 0.0183445, acc 0.98
2016-09-06T15:07:59.929112: step 6997, loss 0.0543228, acc 0.98
2016-09-06T15:08:00.757470: step 6998, loss 0.016844, acc 0.98
2016-09-06T15:08:01.578959: step 6999, loss 0.00301471, acc 1
2016-09-06T15:08:02.381949: step 7000, loss 0.0323332, acc 0.98

Evaluation:
2016-09-06T15:08:06.137204: step 7000, loss 1.83324, acc 0.742026

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-7000

2016-09-06T15:08:08.062844: step 7001, loss 0.0707086, acc 0.98
2016-09-06T15:08:08.901286: step 7002, loss 0.0464389, acc 0.96
2016-09-06T15:08:09.771865: step 7003, loss 0.0157726, acc 1
2016-09-06T15:08:10.583235: step 7004, loss 0.0158733, acc 1
2016-09-06T15:08:11.407313: step 7005, loss 0.0148837, acc 1
2016-09-06T15:08:12.226029: step 7006, loss 0.0028513, acc 1
2016-09-06T15:08:13.020423: step 7007, loss 0.0217874, acc 1
2016-09-06T15:08:13.821950: step 7008, loss 0.0678485, acc 0.98
2016-09-06T15:08:14.635376: step 7009, loss 0.060931, acc 0.98
2016-09-06T15:08:15.437026: step 7010, loss 0.0352594, acc 1
2016-09-06T15:08:16.290429: step 7011, loss 0.00263631, acc 1
2016-09-06T15:08:17.139626: step 7012, loss 0.0230586, acc 1
2016-09-06T15:08:17.938239: step 7013, loss 0.0119167, acc 1
2016-09-06T15:08:18.751789: step 7014, loss 0.00638958, acc 1
2016-09-06T15:08:19.597157: step 7015, loss 0.0204005, acc 1
2016-09-06T15:08:20.414490: step 7016, loss 0.0128903, acc 1
2016-09-06T15:08:21.273669: step 7017, loss 0.0202822, acc 0.98
2016-09-06T15:08:22.094880: step 7018, loss 0.0115555, acc 1
2016-09-06T15:08:22.910925: step 7019, loss 0.0549196, acc 0.98
2016-09-06T15:08:23.708552: step 7020, loss 0.00543692, acc 1
2016-09-06T15:08:24.512791: step 7021, loss 0.0340866, acc 1
2016-09-06T15:08:25.317407: step 7022, loss 0.0221358, acc 1
2016-09-06T15:08:26.144344: step 7023, loss 0.00258584, acc 1
2016-09-06T15:08:26.949319: step 7024, loss 0.0194356, acc 1
2016-09-06T15:08:27.766529: step 7025, loss 0.0153411, acc 1
2016-09-06T15:08:28.557344: step 7026, loss 0.0283314, acc 0.98
2016-09-06T15:08:29.364006: step 7027, loss 0.0354804, acc 0.96
2016-09-06T15:08:30.183130: step 7028, loss 0.00357331, acc 1
2016-09-06T15:08:30.970701: step 7029, loss 0.0200398, acc 0.98
2016-09-06T15:08:31.812673: step 7030, loss 0.00280295, acc 1
2016-09-06T15:08:32.642839: step 7031, loss 0.0440303, acc 0.96
2016-09-06T15:08:33.438145: step 7032, loss 0.0271259, acc 1
2016-09-06T15:08:34.250588: step 7033, loss 0.00625489, acc 1
2016-09-06T15:08:35.052608: step 7034, loss 0.0102337, acc 1
2016-09-06T15:08:35.815892: step 7035, loss 0.00307096, acc 1
2016-09-06T15:08:36.624910: step 7036, loss 0.0111651, acc 1
2016-09-06T15:08:37.435546: step 7037, loss 0.0192837, acc 1
2016-09-06T15:08:38.230356: step 7038, loss 0.0133667, acc 1
2016-09-06T15:08:39.057434: step 7039, loss 0.0592596, acc 0.98
2016-09-06T15:08:39.873799: step 7040, loss 0.0061457, acc 1
2016-09-06T15:08:40.640820: step 7041, loss 0.0072181, acc 1
2016-09-06T15:08:41.467996: step 7042, loss 0.0151567, acc 1
2016-09-06T15:08:42.304042: step 7043, loss 0.0159542, acc 1
2016-09-06T15:08:43.115324: step 7044, loss 0.0190392, acc 0.98
2016-09-06T15:08:43.924427: step 7045, loss 0.0212983, acc 1
2016-09-06T15:08:44.740024: step 7046, loss 0.0375465, acc 0.98
2016-09-06T15:08:45.546177: step 7047, loss 0.00630661, acc 1
2016-09-06T15:08:46.366504: step 7048, loss 0.00422867, acc 1
2016-09-06T15:08:47.204127: step 7049, loss 0.0068068, acc 1
2016-09-06T15:08:48.013415: step 7050, loss 0.0891884, acc 0.96
2016-09-06T15:08:48.811459: step 7051, loss 0.0029117, acc 1
2016-09-06T15:08:49.646647: step 7052, loss 0.0104717, acc 1
2016-09-06T15:08:50.437160: step 7053, loss 0.0397465, acc 0.98
2016-09-06T15:08:51.246367: step 7054, loss 0.0290869, acc 0.98
2016-09-06T15:08:52.076876: step 7055, loss 0.00302928, acc 1
2016-09-06T15:08:52.883553: step 7056, loss 0.033575, acc 1
2016-09-06T15:08:53.705422: step 7057, loss 0.00664465, acc 1
2016-09-06T15:08:54.553369: step 7058, loss 0.0304682, acc 0.98
2016-09-06T15:08:55.395153: step 7059, loss 0.0326779, acc 0.98
2016-09-06T15:08:56.196244: step 7060, loss 0.00348721, acc 1
2016-09-06T15:08:57.014819: step 7061, loss 0.031114, acc 0.98
2016-09-06T15:08:57.815662: step 7062, loss 0.00796476, acc 1
2016-09-06T15:08:58.637023: step 7063, loss 0.11636, acc 0.98
2016-09-06T15:08:59.482833: step 7064, loss 0.00348494, acc 1
2016-09-06T15:09:00.309952: step 7065, loss 0.0147423, acc 1
2016-09-06T15:09:01.116531: step 7066, loss 0.0072904, acc 1
2016-09-06T15:09:01.948086: step 7067, loss 0.0285534, acc 0.98
2016-09-06T15:09:02.782471: step 7068, loss 0.0259862, acc 1
2016-09-06T15:09:03.575439: step 7069, loss 0.0478143, acc 0.98
2016-09-06T15:09:04.393570: step 7070, loss 0.00826241, acc 1
2016-09-06T15:09:05.215897: step 7071, loss 0.0604467, acc 0.98
2016-09-06T15:09:06.005076: step 7072, loss 0.0229866, acc 1
2016-09-06T15:09:06.802504: step 7073, loss 0.0263358, acc 1
2016-09-06T15:09:07.626264: step 7074, loss 0.00578088, acc 1
2016-09-06T15:09:08.431826: step 7075, loss 0.0155822, acc 1
2016-09-06T15:09:09.222543: step 7076, loss 0.00394662, acc 1
2016-09-06T15:09:10.042467: step 7077, loss 0.0279289, acc 0.98
2016-09-06T15:09:10.821712: step 7078, loss 0.0264569, acc 0.98
2016-09-06T15:09:11.653199: step 7079, loss 0.00498059, acc 1
2016-09-06T15:09:12.456895: step 7080, loss 0.0159587, acc 1
2016-09-06T15:09:13.251409: step 7081, loss 0.0486801, acc 1
2016-09-06T15:09:14.075635: step 7082, loss 0.00859755, acc 1
2016-09-06T15:09:14.905628: step 7083, loss 0.020709, acc 0.98
2016-09-06T15:09:15.737499: step 7084, loss 0.0122334, acc 1
2016-09-06T15:09:16.529102: step 7085, loss 0.0125576, acc 1
2016-09-06T15:09:17.336395: step 7086, loss 0.0209151, acc 1
2016-09-06T15:09:18.103114: step 7087, loss 0.0434803, acc 0.98
2016-09-06T15:09:18.902269: step 7088, loss 0.00959239, acc 1
2016-09-06T15:09:19.716360: step 7089, loss 0.0226755, acc 1
2016-09-06T15:09:20.519461: step 7090, loss 0.02192, acc 1
2016-09-06T15:09:21.344565: step 7091, loss 0.0171211, acc 0.98
2016-09-06T15:09:22.171396: step 7092, loss 0.0937986, acc 0.96
2016-09-06T15:09:22.942353: step 7093, loss 0.00319684, acc 1
2016-09-06T15:09:23.750002: step 7094, loss 0.0552751, acc 0.96
2016-09-06T15:09:24.578440: step 7095, loss 0.00266378, acc 1
2016-09-06T15:09:25.376208: step 7096, loss 0.0936213, acc 0.98
2016-09-06T15:09:26.174954: step 7097, loss 0.00280806, acc 1
2016-09-06T15:09:26.966244: step 7098, loss 0.00416148, acc 1
2016-09-06T15:09:27.740232: step 7099, loss 0.0540678, acc 0.98
2016-09-06T15:09:28.537984: step 7100, loss 0.0222385, acc 0.98

Evaluation:
2016-09-06T15:09:32.251427: step 7100, loss 1.92142, acc 0.743902

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-7100

2016-09-06T15:09:34.098105: step 7101, loss 0.0462943, acc 0.98
2016-09-06T15:09:34.921333: step 7102, loss 0.00313003, acc 1
2016-09-06T15:09:35.725308: step 7103, loss 0.0276862, acc 0.98
2016-09-06T15:09:36.473786: step 7104, loss 0.0369102, acc 0.977273
2016-09-06T15:09:37.297415: step 7105, loss 0.0411636, acc 0.98
2016-09-06T15:09:38.127179: step 7106, loss 0.0384759, acc 0.98
2016-09-06T15:09:38.958986: step 7107, loss 0.016465, acc 1
2016-09-06T15:09:39.792442: step 7108, loss 0.0139409, acc 1
2016-09-06T15:09:40.653142: step 7109, loss 0.0450342, acc 0.96
2016-09-06T15:09:41.469322: step 7110, loss 0.00410921, acc 1
2016-09-06T15:09:42.268687: step 7111, loss 0.0194641, acc 0.98
2016-09-06T15:09:43.082661: step 7112, loss 0.0226681, acc 1
2016-09-06T15:09:43.900234: step 7113, loss 0.0487068, acc 0.98
2016-09-06T15:09:44.683236: step 7114, loss 0.0327283, acc 0.98
2016-09-06T15:09:45.510054: step 7115, loss 0.00450891, acc 1
2016-09-06T15:09:46.335749: step 7116, loss 0.0186457, acc 1
2016-09-06T15:09:47.153082: step 7117, loss 0.0352993, acc 0.98
2016-09-06T15:09:47.950530: step 7118, loss 0.0144402, acc 1
2016-09-06T15:09:48.764453: step 7119, loss 0.0312909, acc 1
2016-09-06T15:09:49.538968: step 7120, loss 0.0092135, acc 1
2016-09-06T15:09:50.363119: step 7121, loss 0.0161571, acc 0.98
2016-09-06T15:09:51.189452: step 7122, loss 0.00226986, acc 1
2016-09-06T15:09:52.005545: step 7123, loss 0.0150326, acc 1
2016-09-06T15:09:52.801996: step 7124, loss 0.00288966, acc 1
2016-09-06T15:09:53.625975: step 7125, loss 0.0122856, acc 1
2016-09-06T15:09:54.413421: step 7126, loss 0.00458052, acc 1
2016-09-06T15:09:55.210839: step 7127, loss 0.00219143, acc 1
2016-09-06T15:09:56.021795: step 7128, loss 0.00863745, acc 1
2016-09-06T15:09:56.816006: step 7129, loss 0.0181174, acc 1
2016-09-06T15:09:57.639236: step 7130, loss 0.0192964, acc 0.98
2016-09-06T15:09:58.492677: step 7131, loss 0.00389223, acc 1
2016-09-06T15:09:59.296529: step 7132, loss 0.00364658, acc 1
2016-09-06T15:10:00.089353: step 7133, loss 0.036156, acc 0.98
2016-09-06T15:10:00.928307: step 7134, loss 0.0359455, acc 1
2016-09-06T15:10:01.757392: step 7135, loss 0.00400385, acc 1
2016-09-06T15:10:02.569261: step 7136, loss 0.0130051, acc 1
2016-09-06T15:10:03.389874: step 7137, loss 0.00393027, acc 1
2016-09-06T15:10:04.216135: step 7138, loss 0.0111198, acc 1
2016-09-06T15:10:05.021996: step 7139, loss 0.00464388, acc 1
2016-09-06T15:10:05.836902: step 7140, loss 0.00247543, acc 1
2016-09-06T15:10:06.650713: step 7141, loss 0.0211654, acc 0.98
2016-09-06T15:10:07.461301: step 7142, loss 0.019499, acc 0.98
2016-09-06T15:10:08.311984: step 7143, loss 0.0040793, acc 1
2016-09-06T15:10:09.113817: step 7144, loss 0.00366543, acc 1
2016-09-06T15:10:09.930668: step 7145, loss 0.00323364, acc 1
2016-09-06T15:10:10.758157: step 7146, loss 0.0562065, acc 0.98
2016-09-06T15:10:11.557090: step 7147, loss 0.0345207, acc 0.98
2016-09-06T15:10:12.359116: step 7148, loss 0.0229441, acc 0.98
2016-09-06T15:10:13.193802: step 7149, loss 0.0215854, acc 0.98
2016-09-06T15:10:14.041891: step 7150, loss 0.00465167, acc 1
2016-09-06T15:10:14.864910: step 7151, loss 0.0324071, acc 0.96
2016-09-06T15:10:15.679674: step 7152, loss 0.0184436, acc 1
2016-09-06T15:10:16.472177: step 7153, loss 0.00250557, acc 1
2016-09-06T15:10:17.273454: step 7154, loss 0.0229805, acc 1
2016-09-06T15:10:18.110540: step 7155, loss 0.0112852, acc 1
2016-09-06T15:10:18.933341: step 7156, loss 0.00251464, acc 1
2016-09-06T15:10:19.717203: step 7157, loss 0.00783779, acc 1
2016-09-06T15:10:20.526883: step 7158, loss 0.00256126, acc 1
2016-09-06T15:10:21.348361: step 7159, loss 0.0147908, acc 1
2016-09-06T15:10:22.139411: step 7160, loss 0.0655473, acc 0.98
2016-09-06T15:10:22.949849: step 7161, loss 0.0158549, acc 1
2016-09-06T15:10:23.772778: step 7162, loss 0.00523624, acc 1
2016-09-06T15:10:24.566163: step 7163, loss 0.0403233, acc 0.98
2016-09-06T15:10:25.361779: step 7164, loss 0.0184518, acc 1
2016-09-06T15:10:26.178127: step 7165, loss 0.0272316, acc 0.98
2016-09-06T15:10:26.983325: step 7166, loss 0.0350049, acc 0.98
2016-09-06T15:10:27.840471: step 7167, loss 0.00243321, acc 1
2016-09-06T15:10:28.632539: step 7168, loss 0.103664, acc 0.96
2016-09-06T15:10:29.419718: step 7169, loss 0.00566609, acc 1
2016-09-06T15:10:30.201164: step 7170, loss 0.0609533, acc 0.96
2016-09-06T15:10:31.030082: step 7171, loss 0.0405876, acc 0.98
2016-09-06T15:10:31.828134: step 7172, loss 0.0533671, acc 0.96
2016-09-06T15:10:32.630736: step 7173, loss 0.00696208, acc 1
2016-09-06T15:10:33.456417: step 7174, loss 0.0768303, acc 0.96
2016-09-06T15:10:34.258792: step 7175, loss 0.0133393, acc 1
2016-09-06T15:10:35.055396: step 7176, loss 0.0021002, acc 1
2016-09-06T15:10:35.870838: step 7177, loss 0.0175753, acc 0.98
2016-09-06T15:10:36.649023: step 7178, loss 0.0412546, acc 0.96
2016-09-06T15:10:37.432725: step 7179, loss 0.0208694, acc 1
2016-09-06T15:10:38.265535: step 7180, loss 0.0223999, acc 0.98
2016-09-06T15:10:39.078837: step 7181, loss 0.00385607, acc 1
2016-09-06T15:10:39.911879: step 7182, loss 0.0245184, acc 0.98
2016-09-06T15:10:40.709501: step 7183, loss 0.065377, acc 0.98
2016-09-06T15:10:41.530172: step 7184, loss 0.0209252, acc 1
2016-09-06T15:10:42.349509: step 7185, loss 0.0619214, acc 0.98
2016-09-06T15:10:43.153376: step 7186, loss 0.0198445, acc 0.98
2016-09-06T15:10:43.953439: step 7187, loss 0.0798784, acc 0.96
2016-09-06T15:10:44.777510: step 7188, loss 0.0103358, acc 1
2016-09-06T15:10:45.588019: step 7189, loss 0.0113886, acc 1
2016-09-06T15:10:46.369405: step 7190, loss 0.0303466, acc 0.98
2016-09-06T15:10:47.156025: step 7191, loss 0.020256, acc 0.98
2016-09-06T15:10:47.966955: step 7192, loss 0.0249689, acc 1
2016-09-06T15:10:48.750813: step 7193, loss 0.00937885, acc 1
2016-09-06T15:10:49.554138: step 7194, loss 0.0400332, acc 0.98
2016-09-06T15:10:50.388154: step 7195, loss 0.0212342, acc 0.98
2016-09-06T15:10:51.193606: step 7196, loss 0.00248818, acc 1
2016-09-06T15:10:52.005592: step 7197, loss 0.0252203, acc 1
2016-09-06T15:10:52.854176: step 7198, loss 0.0372496, acc 0.98
2016-09-06T15:10:53.651710: step 7199, loss 0.0509457, acc 0.96
2016-09-06T15:10:54.461666: step 7200, loss 0.00649067, acc 1

Evaluation:
2016-09-06T15:10:58.237057: step 7200, loss 1.76019, acc 0.739212

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-7200

2016-09-06T15:11:00.130315: step 7201, loss 0.0055402, acc 1
2016-09-06T15:11:00.954877: step 7202, loss 0.0257372, acc 0.98
2016-09-06T15:11:01.767868: step 7203, loss 0.0144399, acc 1
2016-09-06T15:11:02.576864: step 7204, loss 0.00368857, acc 1
2016-09-06T15:11:03.367418: step 7205, loss 0.0492845, acc 0.98
2016-09-06T15:11:04.172928: step 7206, loss 0.00391932, acc 1
2016-09-06T15:11:04.991613: step 7207, loss 0.171206, acc 0.98
2016-09-06T15:11:05.786400: step 7208, loss 0.0167862, acc 0.98
2016-09-06T15:11:06.604028: step 7209, loss 0.0294164, acc 1
2016-09-06T15:11:07.430952: step 7210, loss 0.00803147, acc 1
2016-09-06T15:11:08.232765: step 7211, loss 0.0653609, acc 0.98
2016-09-06T15:11:09.050341: step 7212, loss 0.0317801, acc 1
2016-09-06T15:11:09.893680: step 7213, loss 0.0661049, acc 0.98
2016-09-06T15:11:10.672064: step 7214, loss 0.133123, acc 0.96
2016-09-06T15:11:11.516259: step 7215, loss 0.0247717, acc 0.98
2016-09-06T15:11:12.358446: step 7216, loss 0.0471977, acc 0.98
2016-09-06T15:11:13.162789: step 7217, loss 0.0228814, acc 0.98
2016-09-06T15:11:13.995701: step 7218, loss 0.00434878, acc 1
2016-09-06T15:11:14.851223: step 7219, loss 0.0449822, acc 1
2016-09-06T15:11:15.686314: step 7220, loss 0.023068, acc 0.98
2016-09-06T15:11:16.482543: step 7221, loss 0.0212837, acc 1
2016-09-06T15:11:17.340907: step 7222, loss 0.00589074, acc 1
2016-09-06T15:11:18.146583: step 7223, loss 0.0176917, acc 1
2016-09-06T15:11:18.939040: step 7224, loss 0.0307148, acc 1
2016-09-06T15:11:19.768921: step 7225, loss 0.117916, acc 0.98
2016-09-06T15:11:20.584759: step 7226, loss 0.00527613, acc 1
2016-09-06T15:11:21.394668: step 7227, loss 0.00429264, acc 1
2016-09-06T15:11:22.211052: step 7228, loss 0.0178097, acc 1
2016-09-06T15:11:23.045783: step 7229, loss 0.00480998, acc 1
2016-09-06T15:11:23.853238: step 7230, loss 0.00434852, acc 1
2016-09-06T15:11:24.652894: step 7231, loss 0.0120395, acc 1
2016-09-06T15:11:25.460656: step 7232, loss 0.00678914, acc 1
2016-09-06T15:11:26.244773: step 7233, loss 0.0625314, acc 0.98
2016-09-06T15:11:27.083231: step 7234, loss 0.0133955, acc 1
2016-09-06T15:11:27.933289: step 7235, loss 0.00566963, acc 1
2016-09-06T15:11:28.708174: step 7236, loss 0.0142495, acc 1
2016-09-06T15:11:29.521419: step 7237, loss 0.0312562, acc 0.98
2016-09-06T15:11:30.325426: step 7238, loss 0.0159709, acc 1
2016-09-06T15:11:31.123510: step 7239, loss 0.0384429, acc 0.98
2016-09-06T15:11:31.923965: step 7240, loss 0.0069263, acc 1
2016-09-06T15:11:32.730412: step 7241, loss 0.00447603, acc 1
2016-09-06T15:11:33.518362: step 7242, loss 0.0481899, acc 0.96
2016-09-06T15:11:34.337831: step 7243, loss 0.00456911, acc 1
2016-09-06T15:11:35.138175: step 7244, loss 0.00449044, acc 1
2016-09-06T15:11:35.935276: step 7245, loss 0.0322452, acc 0.98
2016-09-06T15:11:36.726571: step 7246, loss 0.0118633, acc 1
2016-09-06T15:11:37.551523: step 7247, loss 0.0911697, acc 0.98
2016-09-06T15:11:38.336211: step 7248, loss 0.0154046, acc 1
2016-09-06T15:11:39.137675: step 7249, loss 0.034196, acc 0.98
2016-09-06T15:11:39.960142: step 7250, loss 0.057997, acc 0.96
2016-09-06T15:11:40.764727: step 7251, loss 0.00956864, acc 1
2016-09-06T15:11:41.563706: step 7252, loss 0.0419213, acc 0.98
2016-09-06T15:11:42.362717: step 7253, loss 0.0401657, acc 0.98
2016-09-06T15:11:43.138474: step 7254, loss 0.143474, acc 0.96
2016-09-06T15:11:43.978153: step 7255, loss 0.043668, acc 0.98
2016-09-06T15:11:44.792256: step 7256, loss 0.00298164, acc 1
2016-09-06T15:11:45.580472: step 7257, loss 0.0303909, acc 0.98
2016-09-06T15:11:46.399319: step 7258, loss 0.0181825, acc 1
2016-09-06T15:11:47.255688: step 7259, loss 0.0271455, acc 0.98
2016-09-06T15:11:48.026953: step 7260, loss 0.0744579, acc 0.98
2016-09-06T15:11:48.821926: step 7261, loss 0.0273714, acc 1
2016-09-06T15:11:49.636785: step 7262, loss 0.0227033, acc 0.98
2016-09-06T15:11:50.439366: step 7263, loss 0.0105869, acc 1
2016-09-06T15:11:51.259225: step 7264, loss 0.0433522, acc 0.96
2016-09-06T15:11:52.115071: step 7265, loss 0.0334027, acc 1
2016-09-06T15:11:52.902237: step 7266, loss 0.00710269, acc 1
2016-09-06T15:11:53.719798: step 7267, loss 0.00591065, acc 1
2016-09-06T15:11:54.517212: step 7268, loss 0.016112, acc 1
2016-09-06T15:11:55.281067: step 7269, loss 0.00920602, acc 1
2016-09-06T15:11:56.069381: step 7270, loss 0.0279352, acc 0.98
2016-09-06T15:11:56.884220: step 7271, loss 0.0192174, acc 1
2016-09-06T15:11:57.680583: step 7272, loss 0.0404949, acc 0.98
2016-09-06T15:11:58.473813: step 7273, loss 0.0540451, acc 0.96
2016-09-06T15:11:59.266529: step 7274, loss 0.0033609, acc 1
2016-09-06T15:12:00.079038: step 7275, loss 0.0857672, acc 0.96
2016-09-06T15:12:00.905821: step 7276, loss 0.00564664, acc 1
2016-09-06T15:12:01.720771: step 7277, loss 0.00403803, acc 1
2016-09-06T15:12:02.521353: step 7278, loss 0.0256146, acc 0.98
2016-09-06T15:12:03.318756: step 7279, loss 0.195324, acc 0.96
2016-09-06T15:12:04.157913: step 7280, loss 0.0199685, acc 1
2016-09-06T15:12:04.947763: step 7281, loss 0.0546886, acc 0.96
2016-09-06T15:12:05.763153: step 7282, loss 0.041356, acc 1
2016-09-06T15:12:06.569454: step 7283, loss 0.0532173, acc 0.98
2016-09-06T15:12:07.364979: step 7284, loss 0.033107, acc 0.98
2016-09-06T15:12:08.162497: step 7285, loss 0.0175922, acc 1
2016-09-06T15:12:08.998176: step 7286, loss 0.0391229, acc 0.98
2016-09-06T15:12:09.783232: step 7287, loss 0.00950148, acc 1
2016-09-06T15:12:10.586233: step 7288, loss 0.00426495, acc 1
2016-09-06T15:12:11.419076: step 7289, loss 0.0197096, acc 1
2016-09-06T15:12:12.203298: step 7290, loss 0.00665335, acc 1
2016-09-06T15:12:13.019470: step 7291, loss 0.0333043, acc 1
2016-09-06T15:12:13.815881: step 7292, loss 0.00440281, acc 1
2016-09-06T15:12:14.575028: step 7293, loss 0.00735071, acc 1
2016-09-06T15:12:15.406914: step 7294, loss 0.117559, acc 0.96
2016-09-06T15:12:16.219031: step 7295, loss 0.0513825, acc 0.96
2016-09-06T15:12:16.938890: step 7296, loss 0.0176508, acc 1
2016-09-06T15:12:17.784055: step 7297, loss 0.0122321, acc 1
2016-09-06T15:12:18.568046: step 7298, loss 0.0317344, acc 0.98
2016-09-06T15:12:19.387744: step 7299, loss 0.00805026, acc 1
2016-09-06T15:12:20.218984: step 7300, loss 0.0173667, acc 1

Evaluation:
2016-09-06T15:12:23.960728: step 7300, loss 2.91076, acc 0.738274

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-7300

2016-09-06T15:12:25.781040: step 7301, loss 0.0294664, acc 0.98
2016-09-06T15:12:26.577479: step 7302, loss 0.0107646, acc 1
2016-09-06T15:12:27.392780: step 7303, loss 0.183987, acc 0.98
2016-09-06T15:12:28.202819: step 7304, loss 0.0174139, acc 1
2016-09-06T15:12:29.017409: step 7305, loss 0.0275807, acc 0.98
2016-09-06T15:12:29.904393: step 7306, loss 0.00602377, acc 1
2016-09-06T15:12:30.672065: step 7307, loss 0.00436108, acc 1
2016-09-06T15:12:31.467135: step 7308, loss 0.0112523, acc 1
2016-09-06T15:12:32.314824: step 7309, loss 0.00840923, acc 1
2016-09-06T15:12:33.119528: step 7310, loss 0.028008, acc 0.98
2016-09-06T15:12:33.936029: step 7311, loss 0.033255, acc 1
2016-09-06T15:12:34.750065: step 7312, loss 0.0580029, acc 0.98
2016-09-06T15:12:35.550686: step 7313, loss 0.0485532, acc 0.98
2016-09-06T15:12:36.360272: step 7314, loss 0.00406636, acc 1
2016-09-06T15:12:37.180142: step 7315, loss 0.0158365, acc 1
2016-09-06T15:12:37.996355: step 7316, loss 0.0362118, acc 0.98
2016-09-06T15:12:38.814467: step 7317, loss 0.0930526, acc 0.98
2016-09-06T15:12:39.664918: step 7318, loss 0.0664908, acc 0.96
2016-09-06T15:12:40.469674: step 7319, loss 0.0115813, acc 1
2016-09-06T15:12:41.255451: step 7320, loss 0.0201419, acc 1
2016-09-06T15:12:42.077584: step 7321, loss 0.0413667, acc 0.98
2016-09-06T15:12:42.919205: step 7322, loss 0.038943, acc 0.98
2016-09-06T15:12:43.733521: step 7323, loss 0.00338922, acc 1
2016-09-06T15:12:44.565653: step 7324, loss 0.0107924, acc 1
2016-09-06T15:12:45.379963: step 7325, loss 0.00447177, acc 1
2016-09-06T15:12:46.170466: step 7326, loss 0.0206333, acc 0.98
2016-09-06T15:12:46.997859: step 7327, loss 0.0190837, acc 1
2016-09-06T15:12:47.824509: step 7328, loss 0.0331149, acc 0.96
2016-09-06T15:12:48.620235: step 7329, loss 0.0439222, acc 0.98
2016-09-06T15:12:49.416985: step 7330, loss 0.0275681, acc 0.98
2016-09-06T15:12:50.228118: step 7331, loss 0.0145762, acc 1
2016-09-06T15:12:51.002252: step 7332, loss 0.00544992, acc 1
2016-09-06T15:12:51.804437: step 7333, loss 0.0409416, acc 0.98
2016-09-06T15:12:52.619668: step 7334, loss 0.0298249, acc 0.98
2016-09-06T15:12:53.398939: step 7335, loss 0.0195713, acc 1
2016-09-06T15:12:54.201589: step 7336, loss 0.0154863, acc 1
2016-09-06T15:12:55.031062: step 7337, loss 0.0186465, acc 0.98
2016-09-06T15:12:55.823488: step 7338, loss 0.00484627, acc 1
2016-09-06T15:12:56.622288: step 7339, loss 0.0557338, acc 0.98
2016-09-06T15:12:57.445043: step 7340, loss 0.149629, acc 0.98
2016-09-06T15:12:58.235224: step 7341, loss 0.0199432, acc 1
2016-09-06T15:12:59.049514: step 7342, loss 0.0286072, acc 0.98
2016-09-06T15:12:59.869696: step 7343, loss 0.0467627, acc 0.98
2016-09-06T15:13:00.667984: step 7344, loss 0.0150054, acc 1
2016-09-06T15:13:01.510564: step 7345, loss 0.00391638, acc 1
2016-09-06T15:13:02.332020: step 7346, loss 0.0842459, acc 0.96
2016-09-06T15:13:03.137219: step 7347, loss 0.0222644, acc 1
2016-09-06T15:13:03.955114: step 7348, loss 0.020485, acc 0.98
2016-09-06T15:13:04.781472: step 7349, loss 0.00789775, acc 1
2016-09-06T15:13:05.581411: step 7350, loss 0.00343698, acc 1
2016-09-06T15:13:06.380144: step 7351, loss 0.019577, acc 1
2016-09-06T15:13:07.194483: step 7352, loss 0.107246, acc 0.94
2016-09-06T15:13:07.999483: step 7353, loss 0.00973895, acc 1
2016-09-06T15:13:08.789023: step 7354, loss 0.0032885, acc 1
2016-09-06T15:13:09.605797: step 7355, loss 0.027688, acc 1
2016-09-06T15:13:10.386338: step 7356, loss 0.0107557, acc 1
2016-09-06T15:13:11.209582: step 7357, loss 0.0146011, acc 1
2016-09-06T15:13:12.070153: step 7358, loss 0.0781981, acc 0.96
2016-09-06T15:13:12.848415: step 7359, loss 0.0259181, acc 0.98
2016-09-06T15:13:13.705139: step 7360, loss 0.0353839, acc 0.98
2016-09-06T15:13:14.530010: step 7361, loss 0.00932174, acc 1
2016-09-06T15:13:15.325982: step 7362, loss 0.084002, acc 0.96
2016-09-06T15:13:16.127261: step 7363, loss 0.0351091, acc 0.98
2016-09-06T15:13:16.959085: step 7364, loss 0.0311982, acc 0.98
2016-09-06T15:13:17.757507: step 7365, loss 0.0242142, acc 0.98
2016-09-06T15:13:18.566895: step 7366, loss 0.0272492, acc 1
2016-09-06T15:13:19.393015: step 7367, loss 0.0211887, acc 0.98
2016-09-06T15:13:20.221400: step 7368, loss 0.0592856, acc 0.98
2016-09-06T15:13:21.030102: step 7369, loss 0.0173525, acc 1
2016-09-06T15:13:21.856555: step 7370, loss 0.0305941, acc 0.98
2016-09-06T15:13:22.670202: step 7371, loss 0.0117099, acc 1
2016-09-06T15:13:23.496618: step 7372, loss 0.0145716, acc 1
2016-09-06T15:13:24.333904: step 7373, loss 0.00573663, acc 1
2016-09-06T15:13:25.179771: step 7374, loss 0.0244146, acc 1
2016-09-06T15:13:25.988866: step 7375, loss 0.00369897, acc 1
2016-09-06T15:13:26.821053: step 7376, loss 0.0049607, acc 1
2016-09-06T15:13:27.629776: step 7377, loss 0.0425846, acc 0.98
2016-09-06T15:13:28.460904: step 7378, loss 0.00794262, acc 1
2016-09-06T15:13:29.296363: step 7379, loss 0.0780006, acc 0.98
2016-09-06T15:13:30.078378: step 7380, loss 0.00685269, acc 1
2016-09-06T15:13:30.873956: step 7381, loss 0.0345846, acc 0.98
2016-09-06T15:13:31.721166: step 7382, loss 0.0444677, acc 0.98
2016-09-06T15:13:32.537731: step 7383, loss 0.0183009, acc 1
2016-09-06T15:13:33.335581: step 7384, loss 0.041204, acc 0.98
2016-09-06T15:13:34.164552: step 7385, loss 0.0295854, acc 1
2016-09-06T15:13:34.985586: step 7386, loss 0.00937864, acc 1
2016-09-06T15:13:35.786223: step 7387, loss 0.0137407, acc 1
2016-09-06T15:13:36.600731: step 7388, loss 0.0102243, acc 1
2016-09-06T15:13:37.432432: step 7389, loss 0.0116967, acc 1
2016-09-06T15:13:38.223746: step 7390, loss 0.129496, acc 0.98
2016-09-06T15:13:39.035422: step 7391, loss 0.0118214, acc 1
2016-09-06T15:13:39.850131: step 7392, loss 0.0165449, acc 1
2016-09-06T15:13:40.650529: step 7393, loss 0.00374304, acc 1
2016-09-06T15:13:41.474631: step 7394, loss 0.0637073, acc 0.96
2016-09-06T15:13:42.297225: step 7395, loss 0.01604, acc 1
2016-09-06T15:13:43.096752: step 7396, loss 0.0137015, acc 1
2016-09-06T15:13:43.894474: step 7397, loss 0.0133346, acc 1
2016-09-06T15:13:44.724315: step 7398, loss 0.0127001, acc 1
2016-09-06T15:13:45.499404: step 7399, loss 0.0234924, acc 0.98
2016-09-06T15:13:46.290153: step 7400, loss 0.0100996, acc 1

Evaluation:
2016-09-06T15:13:50.036672: step 7400, loss 2.33309, acc 0.738274

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-7400

2016-09-06T15:13:52.001598: step 7401, loss 0.0359648, acc 0.98
2016-09-06T15:13:52.802660: step 7402, loss 0.00408275, acc 1
2016-09-06T15:13:53.600755: step 7403, loss 0.0214742, acc 1
2016-09-06T15:13:54.442977: step 7404, loss 0.0732739, acc 0.96
2016-09-06T15:13:55.231588: step 7405, loss 0.0200346, acc 1
2016-09-06T15:13:56.038827: step 7406, loss 0.0322508, acc 0.98
2016-09-06T15:13:56.855545: step 7407, loss 0.00588647, acc 1
2016-09-06T15:13:57.674951: step 7408, loss 0.0671255, acc 0.98
2016-09-06T15:13:58.475778: step 7409, loss 0.0280376, acc 0.98
2016-09-06T15:13:59.309080: step 7410, loss 0.0210478, acc 0.98
2016-09-06T15:14:00.090205: step 7411, loss 0.00406303, acc 1
2016-09-06T15:14:00.911274: step 7412, loss 0.0736927, acc 0.96
2016-09-06T15:14:01.775180: step 7413, loss 0.0196537, acc 0.98
2016-09-06T15:14:02.592000: step 7414, loss 0.0495267, acc 0.98
2016-09-06T15:14:03.417791: step 7415, loss 0.0045408, acc 1
2016-09-06T15:14:04.235338: step 7416, loss 0.0430762, acc 0.98
2016-09-06T15:14:05.087591: step 7417, loss 0.0036093, acc 1
2016-09-06T15:14:05.896371: step 7418, loss 0.0135898, acc 1
2016-09-06T15:14:06.749497: step 7419, loss 0.0406854, acc 0.96
2016-09-06T15:14:07.585966: step 7420, loss 0.0101557, acc 1
2016-09-06T15:14:08.397863: step 7421, loss 0.0176173, acc 1
2016-09-06T15:14:09.223381: step 7422, loss 0.0375098, acc 0.98
2016-09-06T15:14:10.013674: step 7423, loss 0.0307168, acc 0.98
2016-09-06T15:14:10.843116: step 7424, loss 0.0161534, acc 1
2016-09-06T15:14:11.645677: step 7425, loss 0.0137915, acc 1
2016-09-06T15:14:12.464011: step 7426, loss 0.00859471, acc 1
2016-09-06T15:14:13.239807: step 7427, loss 0.0290859, acc 0.98
2016-09-06T15:14:14.064632: step 7428, loss 0.0992008, acc 0.96
2016-09-06T15:14:14.877070: step 7429, loss 0.0589739, acc 0.96
2016-09-06T15:14:15.654185: step 7430, loss 0.0894926, acc 0.96
2016-09-06T15:14:16.470868: step 7431, loss 0.0411234, acc 0.98
2016-09-06T15:14:17.299543: step 7432, loss 0.0178712, acc 0.98
2016-09-06T15:14:18.117235: step 7433, loss 0.0410476, acc 0.98
2016-09-06T15:14:19.008249: step 7434, loss 0.0229703, acc 1
2016-09-06T15:14:19.843174: step 7435, loss 0.0039869, acc 1
2016-09-06T15:14:20.700396: step 7436, loss 0.00907363, acc 1
2016-09-06T15:14:21.518249: step 7437, loss 0.0393849, acc 0.98
2016-09-06T15:14:22.346776: step 7438, loss 0.0215224, acc 0.98
2016-09-06T15:14:23.157632: step 7439, loss 0.00325827, acc 1
2016-09-06T15:14:23.983097: step 7440, loss 0.0208563, acc 1
2016-09-06T15:14:24.832335: step 7441, loss 0.0297158, acc 1
2016-09-06T15:14:25.632923: step 7442, loss 0.0289193, acc 0.98
2016-09-06T15:14:26.430513: step 7443, loss 0.0197252, acc 1
2016-09-06T15:14:27.251739: step 7444, loss 0.017119, acc 1
2016-09-06T15:14:28.080589: step 7445, loss 0.00349088, acc 1
2016-09-06T15:14:28.907340: step 7446, loss 0.0407826, acc 0.96
2016-09-06T15:14:29.746721: step 7447, loss 0.0107548, acc 1
2016-09-06T15:14:30.561240: step 7448, loss 0.00777718, acc 1
2016-09-06T15:14:31.370276: step 7449, loss 0.123744, acc 0.98
2016-09-06T15:14:32.210788: step 7450, loss 0.00361232, acc 1
2016-09-06T15:14:33.020942: step 7451, loss 0.0124122, acc 1
2016-09-06T15:14:33.812438: step 7452, loss 0.0149949, acc 1
2016-09-06T15:14:34.625581: step 7453, loss 0.0250861, acc 1
2016-09-06T15:14:35.444284: step 7454, loss 0.00546254, acc 1
2016-09-06T15:14:36.232952: step 7455, loss 0.0572008, acc 0.98
2016-09-06T15:14:37.048936: step 7456, loss 0.0684147, acc 0.98
2016-09-06T15:14:37.900280: step 7457, loss 0.0346338, acc 1
2016-09-06T15:14:38.705319: step 7458, loss 0.0176369, acc 0.98
2016-09-06T15:14:39.502255: step 7459, loss 0.0218717, acc 0.98
2016-09-06T15:14:40.323983: step 7460, loss 0.0884129, acc 0.98
2016-09-06T15:14:41.110496: step 7461, loss 0.0217237, acc 0.98
2016-09-06T15:14:41.893395: step 7462, loss 0.120914, acc 0.98
2016-09-06T15:14:42.725469: step 7463, loss 0.00376383, acc 1
2016-09-06T15:14:43.525232: step 7464, loss 0.00643812, acc 1
2016-09-06T15:14:44.312178: step 7465, loss 0.0197056, acc 1
2016-09-06T15:14:45.116020: step 7466, loss 0.0194583, acc 0.98
2016-09-06T15:14:45.897454: step 7467, loss 0.124189, acc 0.96
2016-09-06T15:14:46.704910: step 7468, loss 0.0341684, acc 1
2016-09-06T15:14:47.507316: step 7469, loss 0.024075, acc 1
2016-09-06T15:14:48.313459: step 7470, loss 0.0299533, acc 0.98
2016-09-06T15:14:49.117950: step 7471, loss 0.0996576, acc 0.98
2016-09-06T15:14:49.962557: step 7472, loss 0.0196051, acc 0.98
2016-09-06T15:14:50.767746: step 7473, loss 0.0208404, acc 0.98
2016-09-06T15:14:51.585422: step 7474, loss 0.0308286, acc 0.98
2016-09-06T15:14:52.413849: step 7475, loss 0.0141183, acc 1
2016-09-06T15:14:53.186092: step 7476, loss 0.0171137, acc 1
2016-09-06T15:14:53.973354: step 7477, loss 0.00319479, acc 1
2016-09-06T15:14:54.826950: step 7478, loss 0.0196871, acc 1
2016-09-06T15:14:55.603687: step 7479, loss 0.00887009, acc 1
2016-09-06T15:14:56.400288: step 7480, loss 0.00283749, acc 1
2016-09-06T15:14:57.245231: step 7481, loss 0.0148133, acc 1
2016-09-06T15:14:58.039119: step 7482, loss 0.0260036, acc 1
2016-09-06T15:14:58.816963: step 7483, loss 0.039616, acc 0.98
2016-09-06T15:14:59.631680: step 7484, loss 0.00256889, acc 1
2016-09-06T15:15:00.428611: step 7485, loss 0.0268769, acc 1
2016-09-06T15:15:01.234099: step 7486, loss 0.0199636, acc 0.98
2016-09-06T15:15:02.066290: step 7487, loss 0.0155253, acc 1
2016-09-06T15:15:02.809543: step 7488, loss 0.00346649, acc 1
2016-09-06T15:15:03.614664: step 7489, loss 0.00302863, acc 1
2016-09-06T15:15:04.434221: step 7490, loss 0.0252777, acc 1
2016-09-06T15:15:05.225423: step 7491, loss 0.0546043, acc 0.98
2016-09-06T15:15:06.028802: step 7492, loss 0.00331348, acc 1
2016-09-06T15:15:06.822720: step 7493, loss 0.0251183, acc 1
2016-09-06T15:15:07.622283: step 7494, loss 0.0366065, acc 0.98
2016-09-06T15:15:08.446782: step 7495, loss 0.0184846, acc 1
2016-09-06T15:15:09.262712: step 7496, loss 0.0296556, acc 0.98
2016-09-06T15:15:10.059764: step 7497, loss 0.0487711, acc 0.96
2016-09-06T15:15:10.904163: step 7498, loss 0.0225214, acc 1
2016-09-06T15:15:11.738470: step 7499, loss 0.00384369, acc 1
2016-09-06T15:15:12.533398: step 7500, loss 0.00542871, acc 1

Evaluation:
2016-09-06T15:15:16.251522: step 7500, loss 1.89747, acc 0.746717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-7500

2016-09-06T15:15:18.171126: step 7501, loss 0.0302165, acc 0.98
2016-09-06T15:15:18.997433: step 7502, loss 0.0248356, acc 1
2016-09-06T15:15:19.814225: step 7503, loss 0.0294256, acc 1
2016-09-06T15:15:20.605470: step 7504, loss 0.00638812, acc 1
2016-09-06T15:15:21.436853: step 7505, loss 0.0607156, acc 0.96
2016-09-06T15:15:22.225257: step 7506, loss 0.00323883, acc 1
2016-09-06T15:15:23.007650: step 7507, loss 0.0179279, acc 1
2016-09-06T15:15:23.822397: step 7508, loss 0.00357522, acc 1
2016-09-06T15:15:24.611017: step 7509, loss 0.0128373, acc 1
2016-09-06T15:15:25.450599: step 7510, loss 0.0296074, acc 0.98
2016-09-06T15:15:26.275221: step 7511, loss 0.0281845, acc 0.98
2016-09-06T15:15:27.066647: step 7512, loss 0.0208604, acc 1
2016-09-06T15:15:27.907314: step 7513, loss 0.00636855, acc 1
2016-09-06T15:15:28.724606: step 7514, loss 0.0265778, acc 0.98
2016-09-06T15:15:29.534067: step 7515, loss 0.139857, acc 0.98
2016-09-06T15:15:30.333010: step 7516, loss 0.0201825, acc 0.98
2016-09-06T15:15:31.156328: step 7517, loss 0.0091477, acc 1
2016-09-06T15:15:31.967513: step 7518, loss 0.016113, acc 1
2016-09-06T15:15:32.777218: step 7519, loss 0.026531, acc 0.98
2016-09-06T15:15:33.617462: step 7520, loss 0.0304131, acc 0.98
2016-09-06T15:15:34.426324: step 7521, loss 0.0578291, acc 0.96
2016-09-06T15:15:35.246273: step 7522, loss 0.0359694, acc 0.98
2016-09-06T15:15:36.111832: step 7523, loss 0.0116758, acc 1
2016-09-06T15:15:36.946477: step 7524, loss 0.0145836, acc 1
2016-09-06T15:15:37.757644: step 7525, loss 0.0048621, acc 1
2016-09-06T15:15:38.589676: step 7526, loss 0.0173565, acc 1
2016-09-06T15:15:39.399279: step 7527, loss 0.0636419, acc 0.96
2016-09-06T15:15:40.219714: step 7528, loss 0.006189, acc 1
2016-09-06T15:15:41.072459: step 7529, loss 0.00394413, acc 1
2016-09-06T15:15:41.869591: step 7530, loss 0.00361589, acc 1
2016-09-06T15:15:42.669044: step 7531, loss 0.0297105, acc 1
2016-09-06T15:15:43.484898: step 7532, loss 0.00577897, acc 1
2016-09-06T15:15:44.288914: step 7533, loss 0.0134859, acc 1
2016-09-06T15:15:45.159970: step 7534, loss 0.0325957, acc 0.98
2016-09-06T15:15:45.983515: step 7535, loss 0.0406775, acc 0.98
2016-09-06T15:15:46.820750: step 7536, loss 0.00862022, acc 1
2016-09-06T15:15:47.643755: step 7537, loss 0.0381081, acc 0.98
2016-09-06T15:15:48.444880: step 7538, loss 0.00892212, acc 1
2016-09-06T15:15:49.258861: step 7539, loss 0.00700818, acc 1
2016-09-06T15:15:50.072032: step 7540, loss 0.00290141, acc 1
2016-09-06T15:15:50.874784: step 7541, loss 0.00694459, acc 1
2016-09-06T15:15:51.697006: step 7542, loss 0.00378866, acc 1
2016-09-06T15:15:52.519365: step 7543, loss 0.0145803, acc 1
2016-09-06T15:15:53.325199: step 7544, loss 0.014318, acc 1
2016-09-06T15:15:54.186668: step 7545, loss 0.0193683, acc 1
2016-09-06T15:15:54.986749: step 7546, loss 0.0198661, acc 0.98
2016-09-06T15:15:55.787061: step 7547, loss 0.0236725, acc 1
2016-09-06T15:15:56.601206: step 7548, loss 0.0247203, acc 0.98
2016-09-06T15:15:57.420384: step 7549, loss 0.00558774, acc 1
2016-09-06T15:15:58.235385: step 7550, loss 0.021805, acc 0.98
2016-09-06T15:15:59.064056: step 7551, loss 0.024717, acc 0.98
2016-09-06T15:15:59.867040: step 7552, loss 0.0197775, acc 0.98
2016-09-06T15:16:00.709497: step 7553, loss 0.0106407, acc 1
2016-09-06T15:16:01.533279: step 7554, loss 0.0131867, acc 1
2016-09-06T15:16:02.352996: step 7555, loss 0.00798721, acc 1
2016-09-06T15:16:03.175725: step 7556, loss 0.128374, acc 0.94
2016-09-06T15:16:04.002573: step 7557, loss 0.0521637, acc 0.98
2016-09-06T15:16:04.832703: step 7558, loss 0.0600635, acc 0.98
2016-09-06T15:16:05.660761: step 7559, loss 0.0411468, acc 0.98
2016-09-06T15:16:06.474751: step 7560, loss 0.00326255, acc 1
2016-09-06T15:16:07.288009: step 7561, loss 0.00685035, acc 1
2016-09-06T15:16:08.085060: step 7562, loss 0.00383293, acc 1
2016-09-06T15:16:08.922209: step 7563, loss 0.0366071, acc 0.96
2016-09-06T15:16:09.737879: step 7564, loss 0.0166522, acc 0.98
2016-09-06T15:16:10.528781: step 7565, loss 0.0234765, acc 0.98
2016-09-06T15:16:11.336873: step 7566, loss 0.0231077, acc 0.98
2016-09-06T15:16:12.138235: step 7567, loss 0.00548846, acc 1
2016-09-06T15:16:12.950597: step 7568, loss 0.0104259, acc 1
2016-09-06T15:16:13.740533: step 7569, loss 0.00591558, acc 1
2016-09-06T15:16:14.559198: step 7570, loss 0.00300598, acc 1
2016-09-06T15:16:15.385759: step 7571, loss 0.037325, acc 1
2016-09-06T15:16:16.197785: step 7572, loss 0.00263264, acc 1
2016-09-06T15:16:17.023770: step 7573, loss 0.0028878, acc 1
2016-09-06T15:16:17.815949: step 7574, loss 0.0241396, acc 1
2016-09-06T15:16:18.612953: step 7575, loss 0.00270895, acc 1
2016-09-06T15:16:19.440614: step 7576, loss 0.0117129, acc 1
2016-09-06T15:16:20.216035: step 7577, loss 0.0218981, acc 0.98
2016-09-06T15:16:21.020663: step 7578, loss 0.00291096, acc 1
2016-09-06T15:16:21.875458: step 7579, loss 0.0565676, acc 0.96
2016-09-06T15:16:22.653124: step 7580, loss 0.011576, acc 1
2016-09-06T15:16:23.473934: step 7581, loss 0.0341226, acc 0.98
2016-09-06T15:16:24.289097: step 7582, loss 0.00554001, acc 1
2016-09-06T15:16:25.062351: step 7583, loss 0.00286014, acc 1
2016-09-06T15:16:25.855954: step 7584, loss 0.00256327, acc 1
2016-09-06T15:16:26.668511: step 7585, loss 0.00272206, acc 1
2016-09-06T15:16:27.439846: step 7586, loss 0.0429847, acc 0.96
2016-09-06T15:16:28.243001: step 7587, loss 0.0440783, acc 0.98
2016-09-06T15:16:29.063785: step 7588, loss 0.0157189, acc 1
2016-09-06T15:16:29.847686: step 7589, loss 0.0191745, acc 1
2016-09-06T15:16:30.664422: step 7590, loss 0.00516, acc 1
2016-09-06T15:16:31.481478: step 7591, loss 0.00263168, acc 1
2016-09-06T15:16:32.287310: step 7592, loss 0.0820852, acc 0.98
2016-09-06T15:16:33.105942: step 7593, loss 0.0252452, acc 0.98
2016-09-06T15:16:33.919752: step 7594, loss 0.00284206, acc 1
2016-09-06T15:16:34.712759: step 7595, loss 0.0169202, acc 0.98
2016-09-06T15:16:35.519389: step 7596, loss 0.0214555, acc 1
2016-09-06T15:16:36.338985: step 7597, loss 0.0302769, acc 0.98
2016-09-06T15:16:37.139031: step 7598, loss 0.0155684, acc 1
2016-09-06T15:16:37.923232: step 7599, loss 0.0034817, acc 1
2016-09-06T15:16:38.739271: step 7600, loss 0.0227449, acc 0.98

Evaluation:
2016-09-06T15:16:42.445335: step 7600, loss 2.09361, acc 0.749531

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-7600

2016-09-06T15:16:44.349328: step 7601, loss 0.00568461, acc 1
2016-09-06T15:16:45.239215: step 7602, loss 0.0141535, acc 1
2016-09-06T15:16:46.097393: step 7603, loss 0.0252108, acc 0.98
2016-09-06T15:16:46.930074: step 7604, loss 0.00972743, acc 1
2016-09-06T15:16:47.763430: step 7605, loss 0.0105308, acc 1
2016-09-06T15:16:48.577831: step 7606, loss 0.00272419, acc 1
2016-09-06T15:16:49.381358: step 7607, loss 0.0268619, acc 0.98
2016-09-06T15:16:50.207337: step 7608, loss 0.00364258, acc 1
2016-09-06T15:16:51.018441: step 7609, loss 0.00275931, acc 1
2016-09-06T15:16:51.831280: step 7610, loss 0.0948836, acc 0.98
2016-09-06T15:16:52.663619: step 7611, loss 0.0237308, acc 0.98
2016-09-06T15:16:53.494982: step 7612, loss 0.0303866, acc 0.98
2016-09-06T15:16:54.280576: step 7613, loss 0.015891, acc 1
2016-09-06T15:16:55.095956: step 7614, loss 0.00307207, acc 1
2016-09-06T15:16:55.945590: step 7615, loss 0.0107609, acc 1
2016-09-06T15:16:56.776893: step 7616, loss 0.0261688, acc 0.98
2016-09-06T15:16:57.576531: step 7617, loss 0.0031665, acc 1
2016-09-06T15:16:58.433893: step 7618, loss 0.097828, acc 0.98
2016-09-06T15:16:59.239699: step 7619, loss 0.0245255, acc 0.98
2016-09-06T15:17:00.052896: step 7620, loss 0.00879576, acc 1
2016-09-06T15:17:00.878889: step 7621, loss 0.024078, acc 0.98
2016-09-06T15:17:01.678761: step 7622, loss 0.00343249, acc 1
2016-09-06T15:17:02.481710: step 7623, loss 0.025783, acc 1
2016-09-06T15:17:03.321707: step 7624, loss 0.00273658, acc 1
2016-09-06T15:17:04.142811: step 7625, loss 0.00305907, acc 1
2016-09-06T15:17:04.950710: step 7626, loss 0.0248807, acc 1
2016-09-06T15:17:05.794201: step 7627, loss 0.0497101, acc 0.98
2016-09-06T15:17:06.593242: step 7628, loss 0.0683485, acc 0.98
2016-09-06T15:17:07.381521: step 7629, loss 0.0460591, acc 0.98
2016-09-06T15:17:08.214398: step 7630, loss 0.0242045, acc 0.98
2016-09-06T15:17:09.021806: step 7631, loss 0.00833336, acc 1
2016-09-06T15:17:09.827183: step 7632, loss 0.0113395, acc 1
2016-09-06T15:17:10.650418: step 7633, loss 0.0154083, acc 1
2016-09-06T15:17:11.466043: step 7634, loss 0.0425638, acc 0.98
2016-09-06T15:17:12.275639: step 7635, loss 0.0257574, acc 0.98
2016-09-06T15:17:13.069050: step 7636, loss 0.0223063, acc 1
2016-09-06T15:17:13.868702: step 7637, loss 0.0261093, acc 1
2016-09-06T15:17:14.649197: step 7638, loss 0.0193201, acc 0.98
2016-09-06T15:17:15.471225: step 7639, loss 0.0021454, acc 1
2016-09-06T15:17:16.307190: step 7640, loss 0.00325824, acc 1
2016-09-06T15:17:17.132403: step 7641, loss 0.00224078, acc 1
2016-09-06T15:17:17.945764: step 7642, loss 0.00264942, acc 1
2016-09-06T15:17:18.774951: step 7643, loss 0.00971427, acc 1
2016-09-06T15:17:19.584577: step 7644, loss 0.0335672, acc 1
2016-09-06T15:17:20.420160: step 7645, loss 0.0215179, acc 0.98
2016-09-06T15:17:21.244413: step 7646, loss 0.0119364, acc 1
2016-09-06T15:17:22.053401: step 7647, loss 0.00714993, acc 1
2016-09-06T15:17:22.881421: step 7648, loss 0.0241127, acc 1
2016-09-06T15:17:23.727787: step 7649, loss 0.0706307, acc 0.94
2016-09-06T15:17:24.574008: step 7650, loss 0.00249836, acc 1
2016-09-06T15:17:25.418666: step 7651, loss 0.00659926, acc 1
2016-09-06T15:17:26.237005: step 7652, loss 0.00238366, acc 1
2016-09-06T15:17:27.096590: step 7653, loss 0.00370541, acc 1
2016-09-06T15:17:27.902860: step 7654, loss 0.0180872, acc 1
2016-09-06T15:17:28.747519: step 7655, loss 0.0240572, acc 0.98
2016-09-06T15:17:29.542381: step 7656, loss 0.0177472, acc 0.98
2016-09-06T15:17:30.341470: step 7657, loss 0.0887484, acc 0.96
2016-09-06T15:17:31.178577: step 7658, loss 0.00300749, acc 1
2016-09-06T15:17:31.997806: step 7659, loss 0.0387376, acc 0.98
2016-09-06T15:17:32.789185: step 7660, loss 0.0384664, acc 0.98
2016-09-06T15:17:33.591928: step 7661, loss 0.0265918, acc 0.98
2016-09-06T15:17:34.407527: step 7662, loss 0.00351097, acc 1
2016-09-06T15:17:35.197529: step 7663, loss 0.0642644, acc 0.96
2016-09-06T15:17:36.022099: step 7664, loss 0.00655795, acc 1
2016-09-06T15:17:36.851374: step 7665, loss 0.0284343, acc 1
2016-09-06T15:17:37.652615: step 7666, loss 0.0541526, acc 0.96
2016-09-06T15:17:38.438145: step 7667, loss 0.0947375, acc 0.94
2016-09-06T15:17:39.249677: step 7668, loss 0.0403316, acc 1
2016-09-06T15:17:40.037916: step 7669, loss 0.0335645, acc 1
2016-09-06T15:17:40.837198: step 7670, loss 0.0372205, acc 0.98
2016-09-06T15:17:41.656107: step 7671, loss 0.00408715, acc 1
2016-09-06T15:17:42.466341: step 7672, loss 0.00994281, acc 1
2016-09-06T15:17:43.316689: step 7673, loss 0.015387, acc 1
2016-09-06T15:17:44.157035: step 7674, loss 0.00520448, acc 1
2016-09-06T15:17:44.937596: step 7675, loss 0.0328765, acc 1
2016-09-06T15:17:45.758127: step 7676, loss 0.0167893, acc 0.98
2016-09-06T15:17:46.585893: step 7677, loss 0.0568552, acc 0.98
2016-09-06T15:17:47.367057: step 7678, loss 0.00542602, acc 1
2016-09-06T15:17:48.137886: step 7679, loss 0.00277813, acc 1
2016-09-06T15:17:48.891428: step 7680, loss 0.0553115, acc 0.977273
2016-09-06T15:17:49.693247: step 7681, loss 0.0150272, acc 1
2016-09-06T15:17:50.521913: step 7682, loss 0.0299341, acc 1
2016-09-06T15:17:51.357406: step 7683, loss 0.0689767, acc 0.98
2016-09-06T15:17:52.152447: step 7684, loss 0.0306412, acc 0.98
2016-09-06T15:17:52.955314: step 7685, loss 0.0453544, acc 0.96
2016-09-06T15:17:53.822177: step 7686, loss 0.0049605, acc 1
2016-09-06T15:17:54.613715: step 7687, loss 0.0430203, acc 0.98
2016-09-06T15:17:55.403949: step 7688, loss 0.0247494, acc 1
2016-09-06T15:17:56.234343: step 7689, loss 0.0265353, acc 0.98
2016-09-06T15:17:57.029446: step 7690, loss 0.0239456, acc 1
2016-09-06T15:17:57.823926: step 7691, loss 0.0127169, acc 1
2016-09-06T15:17:58.636632: step 7692, loss 0.00965519, acc 1
2016-09-06T15:17:59.423277: step 7693, loss 0.0275936, acc 0.98
2016-09-06T15:18:00.240701: step 7694, loss 0.0349027, acc 0.96
2016-09-06T15:18:01.034836: step 7695, loss 0.0236584, acc 0.98
2016-09-06T15:18:01.811530: step 7696, loss 0.00561641, acc 1
2016-09-06T15:18:02.645996: step 7697, loss 0.00413307, acc 1
2016-09-06T15:18:03.491588: step 7698, loss 0.00316091, acc 1
2016-09-06T15:18:04.271591: step 7699, loss 0.00704415, acc 1
2016-09-06T15:18:05.088822: step 7700, loss 0.00313403, acc 1

Evaluation:
2016-09-06T15:18:08.810253: step 7700, loss 2.11267, acc 0.73546

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-7700

2016-09-06T15:18:10.687288: step 7701, loss 0.00313717, acc 1
2016-09-06T15:18:11.483987: step 7702, loss 0.0203809, acc 0.98
2016-09-06T15:18:12.305612: step 7703, loss 0.00580201, acc 1
2016-09-06T15:18:13.146045: step 7704, loss 0.00366877, acc 1
2016-09-06T15:18:13.978844: step 7705, loss 0.00507275, acc 1
2016-09-06T15:18:14.811012: step 7706, loss 0.186269, acc 0.98
2016-09-06T15:18:15.643900: step 7707, loss 0.00644514, acc 1
2016-09-06T15:18:16.490521: step 7708, loss 0.00409976, acc 1
2016-09-06T15:18:17.302500: step 7709, loss 0.037002, acc 0.98
2016-09-06T15:18:18.104012: step 7710, loss 0.00334361, acc 1
2016-09-06T15:18:18.863707: step 7711, loss 0.14534, acc 0.98
2016-09-06T15:18:19.659466: step 7712, loss 0.0246371, acc 1
2016-09-06T15:18:20.494556: step 7713, loss 0.0203914, acc 0.98
2016-09-06T15:18:21.286344: step 7714, loss 0.0469912, acc 0.98
2016-09-06T15:18:22.091448: step 7715, loss 0.0193285, acc 1
2016-09-06T15:18:22.900931: step 7716, loss 0.00448078, acc 1
2016-09-06T15:18:23.692390: step 7717, loss 0.0378743, acc 0.98
2016-09-06T15:18:24.503509: step 7718, loss 0.02189, acc 0.98
2016-09-06T15:18:25.310477: step 7719, loss 0.00801199, acc 1
2016-09-06T15:18:26.106615: step 7720, loss 0.00605993, acc 1
2016-09-06T15:18:26.906634: step 7721, loss 0.0183944, acc 1
2016-09-06T15:18:27.745448: step 7722, loss 0.00312624, acc 1
2016-09-06T15:18:28.523723: step 7723, loss 0.0106631, acc 1
2016-09-06T15:18:29.329874: step 7724, loss 0.0195928, acc 1
2016-09-06T15:18:30.142703: step 7725, loss 0.00733838, acc 1
2016-09-06T15:18:30.947280: step 7726, loss 0.0053854, acc 1
2016-09-06T15:18:31.760580: step 7727, loss 0.033981, acc 0.98
2016-09-06T15:18:32.573467: step 7728, loss 0.0143908, acc 1
2016-09-06T15:18:33.359542: step 7729, loss 0.124368, acc 0.98
2016-09-06T15:18:34.172421: step 7730, loss 0.0416547, acc 0.98
2016-09-06T15:18:35.000888: step 7731, loss 0.0148203, acc 1
2016-09-06T15:18:35.793441: step 7732, loss 0.0218968, acc 1
2016-09-06T15:18:36.621403: step 7733, loss 0.00323451, acc 1
2016-09-06T15:18:37.444906: step 7734, loss 0.0114047, acc 1
2016-09-06T15:18:38.234514: step 7735, loss 0.156747, acc 0.98
2016-09-06T15:18:39.022635: step 7736, loss 0.0368806, acc 0.98
2016-09-06T15:18:39.847650: step 7737, loss 0.0271169, acc 1
2016-09-06T15:18:40.662632: step 7738, loss 0.0108639, acc 1
2016-09-06T15:18:41.476407: step 7739, loss 0.0260684, acc 1
2016-09-06T15:18:42.312257: step 7740, loss 0.0201735, acc 0.98
2016-09-06T15:18:43.109031: step 7741, loss 0.00336068, acc 1
2016-09-06T15:18:43.936543: step 7742, loss 0.0255142, acc 1
2016-09-06T15:18:44.778707: step 7743, loss 0.0166491, acc 1
2016-09-06T15:18:45.595054: step 7744, loss 0.0438001, acc 0.98
2016-09-06T15:18:46.403312: step 7745, loss 0.0166485, acc 1
2016-09-06T15:18:47.230535: step 7746, loss 0.0160426, acc 1
2016-09-06T15:18:48.030970: step 7747, loss 0.016554, acc 1
2016-09-06T15:18:48.844457: step 7748, loss 0.0655008, acc 0.98
2016-09-06T15:18:49.688452: step 7749, loss 0.0450449, acc 0.98
2016-09-06T15:18:50.497154: step 7750, loss 0.0344573, acc 0.98
2016-09-06T15:18:51.334688: step 7751, loss 0.0211958, acc 1
2016-09-06T15:18:52.158766: step 7752, loss 0.0701435, acc 0.98
2016-09-06T15:18:52.979545: step 7753, loss 0.00709927, acc 1
2016-09-06T15:18:53.787901: step 7754, loss 0.00522977, acc 1
2016-09-06T15:18:54.606790: step 7755, loss 0.0360452, acc 1
2016-09-06T15:18:55.435287: step 7756, loss 0.0229014, acc 1
2016-09-06T15:18:56.237950: step 7757, loss 0.0167779, acc 1
2016-09-06T15:18:57.070921: step 7758, loss 0.0213145, acc 1
2016-09-06T15:18:57.899256: step 7759, loss 0.00379977, acc 1
2016-09-06T15:18:58.701011: step 7760, loss 0.0159765, acc 1
2016-09-06T15:18:59.545037: step 7761, loss 0.00463088, acc 1
2016-09-06T15:19:00.396158: step 7762, loss 0.0233463, acc 0.98
2016-09-06T15:19:01.162947: step 7763, loss 0.0158165, acc 1
2016-09-06T15:19:01.961326: step 7764, loss 0.0264033, acc 0.98
2016-09-06T15:19:02.809129: step 7765, loss 0.0227648, acc 1
2016-09-06T15:19:03.602625: step 7766, loss 0.0616857, acc 0.96
2016-09-06T15:19:04.389813: step 7767, loss 0.00470991, acc 1
2016-09-06T15:19:05.194236: step 7768, loss 0.0150592, acc 1
2016-09-06T15:19:05.992451: step 7769, loss 0.0242482, acc 0.98
2016-09-06T15:19:06.841679: step 7770, loss 0.00464281, acc 1
2016-09-06T15:19:07.656551: step 7771, loss 0.0075715, acc 1
2016-09-06T15:19:08.423192: step 7772, loss 0.00333589, acc 1
2016-09-06T15:19:09.203324: step 7773, loss 0.0400284, acc 0.98
2016-09-06T15:19:10.010657: step 7774, loss 0.0033596, acc 1
2016-09-06T15:19:10.815845: step 7775, loss 0.0500932, acc 0.98
2016-09-06T15:19:11.630913: step 7776, loss 0.0111946, acc 1
2016-09-06T15:19:12.469436: step 7777, loss 0.00368151, acc 1
2016-09-06T15:19:13.247254: step 7778, loss 0.01275, acc 1
2016-09-06T15:19:14.055432: step 7779, loss 0.0143382, acc 1
2016-09-06T15:19:14.862985: step 7780, loss 0.0194379, acc 1
2016-09-06T15:19:15.670216: step 7781, loss 0.0259982, acc 0.98
2016-09-06T15:19:16.476929: step 7782, loss 0.00326806, acc 1
2016-09-06T15:19:17.277138: step 7783, loss 0.00482763, acc 1
2016-09-06T15:19:18.063024: step 7784, loss 0.123048, acc 0.98
2016-09-06T15:19:18.858224: step 7785, loss 0.019887, acc 0.98
2016-09-06T15:19:19.683825: step 7786, loss 0.0225782, acc 0.98
2016-09-06T15:19:20.485211: step 7787, loss 0.0377089, acc 1
2016-09-06T15:19:21.285076: step 7788, loss 0.121011, acc 0.98
2016-09-06T15:19:22.098741: step 7789, loss 0.0211556, acc 1
2016-09-06T15:19:22.878405: step 7790, loss 0.0278548, acc 1
2016-09-06T15:19:23.755143: step 7791, loss 0.00330984, acc 1
2016-09-06T15:19:24.544305: step 7792, loss 0.0189561, acc 1
2016-09-06T15:19:25.345575: step 7793, loss 0.197994, acc 0.96
2016-09-06T15:19:26.147905: step 7794, loss 0.00517087, acc 1
2016-09-06T15:19:26.963314: step 7795, loss 0.016893, acc 1
2016-09-06T15:19:27.745580: step 7796, loss 0.019896, acc 1
2016-09-06T15:19:28.548884: step 7797, loss 0.0391262, acc 0.96
2016-09-06T15:19:29.380174: step 7798, loss 0.00403276, acc 1
2016-09-06T15:19:30.182087: step 7799, loss 0.0591788, acc 0.98
2016-09-06T15:19:30.978647: step 7800, loss 0.060784, acc 0.98

Evaluation:
2016-09-06T15:19:34.727013: step 7800, loss 1.43915, acc 0.73546

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-7800

2016-09-06T15:19:36.688660: step 7801, loss 0.00417127, acc 1
2016-09-06T15:19:37.452597: step 7802, loss 0.0162073, acc 1
2016-09-06T15:19:38.255661: step 7803, loss 0.00686707, acc 1
2016-09-06T15:19:39.091143: step 7804, loss 0.0264762, acc 1
2016-09-06T15:19:39.853624: step 7805, loss 0.0152316, acc 1
2016-09-06T15:19:40.659124: step 7806, loss 0.00869224, acc 1
2016-09-06T15:19:41.484025: step 7807, loss 0.0281181, acc 1
2016-09-06T15:19:42.277033: step 7808, loss 0.0182119, acc 1
2016-09-06T15:19:43.105415: step 7809, loss 0.00513102, acc 1
2016-09-06T15:19:43.941414: step 7810, loss 0.0265363, acc 1
2016-09-06T15:19:44.760882: step 7811, loss 0.00885224, acc 1
2016-09-06T15:19:45.564695: step 7812, loss 0.0141315, acc 1
2016-09-06T15:19:46.405207: step 7813, loss 0.0173662, acc 1
2016-09-06T15:19:47.222420: step 7814, loss 0.0361491, acc 0.98
2016-09-06T15:19:48.064538: step 7815, loss 0.0545539, acc 0.98
2016-09-06T15:19:48.891992: step 7816, loss 0.00537476, acc 1
2016-09-06T15:19:49.724491: step 7817, loss 0.015206, acc 1
2016-09-06T15:19:50.551095: step 7818, loss 0.0474901, acc 0.98
2016-09-06T15:19:51.373454: step 7819, loss 0.00475556, acc 1
2016-09-06T15:19:52.149042: step 7820, loss 0.00508983, acc 1
2016-09-06T15:19:52.966123: step 7821, loss 0.0281812, acc 0.98
2016-09-06T15:19:53.810453: step 7822, loss 0.0213627, acc 0.98
2016-09-06T15:19:54.639621: step 7823, loss 0.0138168, acc 1
2016-09-06T15:19:55.445805: step 7824, loss 0.0207462, acc 1
2016-09-06T15:19:56.285250: step 7825, loss 0.00486655, acc 1
2016-09-06T15:19:57.079533: step 7826, loss 0.0384818, acc 0.98
2016-09-06T15:19:57.928466: step 7827, loss 0.0326689, acc 1
2016-09-06T15:19:58.749165: step 7828, loss 0.00738216, acc 1
2016-09-06T15:19:59.544643: step 7829, loss 0.0224497, acc 0.98
2016-09-06T15:20:00.373609: step 7830, loss 0.0171757, acc 1
2016-09-06T15:20:01.200806: step 7831, loss 0.00476812, acc 1
2016-09-06T15:20:02.021409: step 7832, loss 0.00677114, acc 1
2016-09-06T15:20:02.867479: step 7833, loss 0.00440278, acc 1
2016-09-06T15:20:03.657899: step 7834, loss 0.00424091, acc 1
2016-09-06T15:20:04.462090: step 7835, loss 0.0187786, acc 1
2016-09-06T15:20:05.233453: step 7836, loss 0.0381422, acc 0.98
2016-09-06T15:20:06.077408: step 7837, loss 0.0223439, acc 1
2016-09-06T15:20:06.890812: step 7838, loss 0.0289401, acc 0.98
2016-09-06T15:20:07.666294: step 7839, loss 0.00429109, acc 1
2016-09-06T15:20:08.460547: step 7840, loss 0.0447658, acc 0.98
2016-09-06T15:20:09.278375: step 7841, loss 0.0294954, acc 0.98
2016-09-06T15:20:10.075567: step 7842, loss 0.0158587, acc 1
2016-09-06T15:20:10.893936: step 7843, loss 0.00509061, acc 1
2016-09-06T15:20:11.728207: step 7844, loss 0.00429043, acc 1
2016-09-06T15:20:12.526358: step 7845, loss 0.0902744, acc 0.98
2016-09-06T15:20:13.334332: step 7846, loss 0.0214337, acc 0.98
2016-09-06T15:20:14.146129: step 7847, loss 0.0164847, acc 1
2016-09-06T15:20:14.921361: step 7848, loss 0.00423749, acc 1
2016-09-06T15:20:15.759021: step 7849, loss 0.0248077, acc 1
2016-09-06T15:20:16.566215: step 7850, loss 0.00426205, acc 1
2016-09-06T15:20:17.371230: step 7851, loss 0.0353598, acc 0.98
2016-09-06T15:20:18.188553: step 7852, loss 0.0156969, acc 1
2016-09-06T15:20:19.019007: step 7853, loss 0.0305874, acc 0.98
2016-09-06T15:20:19.814253: step 7854, loss 0.0336127, acc 0.96
2016-09-06T15:20:20.616534: step 7855, loss 0.0285326, acc 0.98
2016-09-06T15:20:21.462457: step 7856, loss 0.00694028, acc 1
2016-09-06T15:20:22.267341: step 7857, loss 0.0459972, acc 0.98
2016-09-06T15:20:23.065514: step 7858, loss 0.0356135, acc 0.98
2016-09-06T15:20:23.914004: step 7859, loss 0.0615684, acc 0.98
2016-09-06T15:20:24.724367: step 7860, loss 0.0299817, acc 1
2016-09-06T15:20:25.521806: step 7861, loss 0.00456707, acc 1
2016-09-06T15:20:26.351497: step 7862, loss 0.0146997, acc 1
2016-09-06T15:20:27.155145: step 7863, loss 0.0599624, acc 0.96
2016-09-06T15:20:27.973350: step 7864, loss 0.00313945, acc 1
2016-09-06T15:20:28.781404: step 7865, loss 0.00720426, acc 1
2016-09-06T15:20:29.584812: step 7866, loss 0.045783, acc 0.98
2016-09-06T15:20:30.396353: step 7867, loss 0.0333441, acc 0.96
2016-09-06T15:20:31.222031: step 7868, loss 0.331101, acc 0.98
2016-09-06T15:20:32.048451: step 7869, loss 0.0271128, acc 0.98
2016-09-06T15:20:32.831687: step 7870, loss 0.0105308, acc 1
2016-09-06T15:20:33.683007: step 7871, loss 0.0337755, acc 0.98
2016-09-06T15:20:34.430747: step 7872, loss 0.00312028, acc 1
2016-09-06T15:20:35.255759: step 7873, loss 0.0108143, acc 1
2016-09-06T15:20:36.105085: step 7874, loss 0.0302564, acc 0.98
2016-09-06T15:20:36.924396: step 7875, loss 0.0031881, acc 1
2016-09-06T15:20:37.747493: step 7876, loss 0.0648739, acc 0.96
2016-09-06T15:20:38.595142: step 7877, loss 0.0289094, acc 1
2016-09-06T15:20:39.428082: step 7878, loss 0.0675947, acc 0.98
2016-09-06T15:20:40.227988: step 7879, loss 0.0217497, acc 0.98
2016-09-06T15:20:41.049645: step 7880, loss 0.0260948, acc 1
2016-09-06T15:20:41.885903: step 7881, loss 0.05956, acc 0.96
2016-09-06T15:20:42.687620: step 7882, loss 0.0588396, acc 0.96
2016-09-06T15:20:43.479864: step 7883, loss 0.0156259, acc 1
2016-09-06T15:20:44.292194: step 7884, loss 0.0128708, acc 1
2016-09-06T15:20:45.119890: step 7885, loss 0.00877617, acc 1
2016-09-06T15:20:45.920354: step 7886, loss 0.0355868, acc 0.96
2016-09-06T15:20:46.748745: step 7887, loss 0.110887, acc 0.98
2016-09-06T15:20:47.522260: step 7888, loss 0.00397905, acc 1
2016-09-06T15:20:48.311378: step 7889, loss 0.00361917, acc 1
2016-09-06T15:20:49.112003: step 7890, loss 0.00375642, acc 1
2016-09-06T15:20:49.920093: step 7891, loss 0.0111287, acc 1
2016-09-06T15:20:50.742981: step 7892, loss 0.00344722, acc 1
2016-09-06T15:20:51.547663: step 7893, loss 0.0297886, acc 0.98
2016-09-06T15:20:52.349461: step 7894, loss 0.0398259, acc 0.98
2016-09-06T15:20:53.160190: step 7895, loss 0.00342641, acc 1
2016-09-06T15:20:53.960512: step 7896, loss 0.0508149, acc 0.96
2016-09-06T15:20:54.761471: step 7897, loss 0.0203233, acc 0.98
2016-09-06T15:20:55.599059: step 7898, loss 0.0110433, acc 1
2016-09-06T15:20:56.416464: step 7899, loss 0.0245711, acc 0.98
2016-09-06T15:20:57.222892: step 7900, loss 0.0122648, acc 1

Evaluation:
2016-09-06T15:21:00.957286: step 7900, loss 1.45688, acc 0.73546

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-7900

2016-09-06T15:21:02.774002: step 7901, loss 0.00396015, acc 1
2016-09-06T15:21:03.589527: step 7902, loss 0.0452878, acc 0.96
2016-09-06T15:21:04.407140: step 7903, loss 0.00373404, acc 1
2016-09-06T15:21:05.252899: step 7904, loss 0.0184233, acc 1
2016-09-06T15:21:06.064037: step 7905, loss 0.0180442, acc 1
2016-09-06T15:21:06.861239: step 7906, loss 0.0112109, acc 1
2016-09-06T15:21:07.696667: step 7907, loss 0.0133874, acc 1
2016-09-06T15:21:08.503203: step 7908, loss 0.0103393, acc 1
2016-09-06T15:21:09.320518: step 7909, loss 0.0398759, acc 0.98
2016-09-06T15:21:10.181833: step 7910, loss 0.0292158, acc 0.98
2016-09-06T15:21:11.007338: step 7911, loss 0.0155847, acc 1
2016-09-06T15:21:11.801651: step 7912, loss 0.0144084, acc 1
2016-09-06T15:21:12.621831: step 7913, loss 0.0143257, acc 1
2016-09-06T15:21:13.444823: step 7914, loss 0.0146236, acc 1
2016-09-06T15:21:14.237053: step 7915, loss 0.01389, acc 1
2016-09-06T15:21:15.069374: step 7916, loss 0.0216227, acc 0.98
2016-09-06T15:21:15.909290: step 7917, loss 0.013555, acc 1
2016-09-06T15:21:16.688027: step 7918, loss 0.00618899, acc 1
2016-09-06T15:21:17.512766: step 7919, loss 0.0483913, acc 1
2016-09-06T15:21:18.338107: step 7920, loss 0.0246761, acc 0.98
2016-09-06T15:21:19.128449: step 7921, loss 0.00326812, acc 1
2016-09-06T15:21:19.922288: step 7922, loss 0.00688723, acc 1
2016-09-06T15:21:20.758843: step 7923, loss 0.0147709, acc 1
2016-09-06T15:21:21.548747: step 7924, loss 0.00418566, acc 1
2016-09-06T15:21:22.360449: step 7925, loss 0.0311797, acc 0.98
2016-09-06T15:21:23.170006: step 7926, loss 0.0107532, acc 1
2016-09-06T15:21:23.958527: step 7927, loss 0.0153311, acc 1
2016-09-06T15:21:24.769783: step 7928, loss 0.0236333, acc 0.98
2016-09-06T15:21:25.607415: step 7929, loss 0.0271548, acc 0.98
2016-09-06T15:21:26.421326: step 7930, loss 0.0143966, acc 1
2016-09-06T15:21:27.234209: step 7931, loss 0.0472685, acc 0.98
2016-09-06T15:21:28.078713: step 7932, loss 0.0155634, acc 1
2016-09-06T15:21:28.874205: step 7933, loss 0.00366023, acc 1
2016-09-06T15:21:29.692229: step 7934, loss 0.0130518, acc 1
2016-09-06T15:21:30.525691: step 7935, loss 0.0107283, acc 1
2016-09-06T15:21:31.350964: step 7936, loss 0.00424539, acc 1
2016-09-06T15:21:32.158470: step 7937, loss 0.0194583, acc 0.98
2016-09-06T15:21:33.008053: step 7938, loss 0.0989995, acc 0.98
2016-09-06T15:21:33.830100: step 7939, loss 0.0188853, acc 1
2016-09-06T15:21:34.646310: step 7940, loss 0.141144, acc 0.98
2016-09-06T15:21:35.473958: step 7941, loss 0.0192004, acc 1
2016-09-06T15:21:36.297348: step 7942, loss 0.0138587, acc 1
2016-09-06T15:21:37.086212: step 7943, loss 0.0359867, acc 0.98
2016-09-06T15:21:37.938968: step 7944, loss 0.008155, acc 1
2016-09-06T15:21:38.752429: step 7945, loss 0.0443919, acc 0.98
2016-09-06T15:21:39.549722: step 7946, loss 0.0140436, acc 1
2016-09-06T15:21:40.389481: step 7947, loss 0.0257728, acc 0.98
2016-09-06T15:21:41.183392: step 7948, loss 0.0191517, acc 0.98
2016-09-06T15:21:42.015869: step 7949, loss 0.0343512, acc 0.98
2016-09-06T15:21:42.841513: step 7950, loss 0.0294951, acc 0.98
2016-09-06T15:21:43.678814: step 7951, loss 0.0493197, acc 0.96
2016-09-06T15:21:44.487821: step 7952, loss 0.0150784, acc 1
2016-09-06T15:21:45.282181: step 7953, loss 0.00923727, acc 1
2016-09-06T15:21:46.108672: step 7954, loss 0.0248812, acc 0.98
2016-09-06T15:21:46.898554: step 7955, loss 0.0423829, acc 0.96
2016-09-06T15:21:47.703127: step 7956, loss 0.025501, acc 1
2016-09-06T15:21:48.521576: step 7957, loss 0.0167526, acc 1
2016-09-06T15:21:49.311020: step 7958, loss 0.0118656, acc 1
2016-09-06T15:21:50.117617: step 7959, loss 0.0175334, acc 0.98
2016-09-06T15:21:50.947956: step 7960, loss 0.0660028, acc 0.98
2016-09-06T15:21:51.776929: step 7961, loss 0.00358184, acc 1
2016-09-06T15:21:52.575800: step 7962, loss 0.00401078, acc 1
2016-09-06T15:21:53.379995: step 7963, loss 0.0297496, acc 0.98
2016-09-06T15:21:54.163138: step 7964, loss 0.00360904, acc 1
2016-09-06T15:21:54.947776: step 7965, loss 0.021993, acc 1
2016-09-06T15:21:55.776695: step 7966, loss 0.00630021, acc 1
2016-09-06T15:21:56.558455: step 7967, loss 0.00808201, acc 1
2016-09-06T15:21:57.352479: step 7968, loss 0.00370393, acc 1
2016-09-06T15:21:58.182687: step 7969, loss 0.0263214, acc 1
2016-09-06T15:21:58.964291: step 7970, loss 0.0152531, acc 1
2016-09-06T15:21:59.767390: step 7971, loss 0.0140833, acc 1
2016-09-06T15:22:00.630653: step 7972, loss 0.0223288, acc 0.98
2016-09-06T15:22:01.431656: step 7973, loss 0.0215702, acc 0.98
2016-09-06T15:22:02.231265: step 7974, loss 0.00405619, acc 1
2016-09-06T15:22:03.047271: step 7975, loss 0.0888029, acc 0.98
2016-09-06T15:22:03.827285: step 7976, loss 0.00314069, acc 1
2016-09-06T15:22:04.620363: step 7977, loss 0.0368055, acc 1
2016-09-06T15:22:05.451286: step 7978, loss 0.00458196, acc 1
2016-09-06T15:22:06.229012: step 7979, loss 0.0218137, acc 0.98
2016-09-06T15:22:07.009207: step 7980, loss 0.0108018, acc 1
2016-09-06T15:22:07.877785: step 7981, loss 0.0190377, acc 0.98
2016-09-06T15:22:08.702537: step 7982, loss 0.0355492, acc 1
2016-09-06T15:22:09.514554: step 7983, loss 0.00695744, acc 1
2016-09-06T15:22:10.339395: step 7984, loss 0.0317283, acc 0.98
2016-09-06T15:22:11.177398: step 7985, loss 0.0240279, acc 0.98
2016-09-06T15:22:11.973213: step 7986, loss 0.044453, acc 0.96
2016-09-06T15:22:12.786157: step 7987, loss 0.00931746, acc 1
2016-09-06T15:22:13.585000: step 7988, loss 0.00299532, acc 1
2016-09-06T15:22:14.416167: step 7989, loss 0.08206, acc 0.98
2016-09-06T15:22:15.242836: step 7990, loss 0.00321431, acc 1
2016-09-06T15:22:16.110528: step 7991, loss 0.013607, acc 1
2016-09-06T15:22:16.953107: step 7992, loss 0.0151665, acc 1
2016-09-06T15:22:17.775274: step 7993, loss 0.00356054, acc 1
2016-09-06T15:22:18.576017: step 7994, loss 0.00344837, acc 1
2016-09-06T15:22:19.392941: step 7995, loss 0.0194282, acc 0.98
2016-09-06T15:22:20.210716: step 7996, loss 0.00556728, acc 1
2016-09-06T15:22:21.042043: step 7997, loss 0.00332056, acc 1
2016-09-06T15:22:21.862337: step 7998, loss 0.0350588, acc 0.98
2016-09-06T15:22:22.686078: step 7999, loss 0.0298241, acc 0.98
2016-09-06T15:22:23.495548: step 8000, loss 0.0195495, acc 0.98

Evaluation:
2016-09-06T15:22:27.202837: step 8000, loss 1.66492, acc 0.744841

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-8000

2016-09-06T15:22:29.103671: step 8001, loss 0.0350445, acc 0.98
2016-09-06T15:22:29.940083: step 8002, loss 0.195442, acc 0.94
2016-09-06T15:22:30.744384: step 8003, loss 0.0186658, acc 1
2016-09-06T15:22:31.550755: step 8004, loss 0.0302911, acc 0.98
2016-09-06T15:22:32.387274: step 8005, loss 0.0224119, acc 1
2016-09-06T15:22:33.217228: step 8006, loss 0.0408373, acc 1
2016-09-06T15:22:34.042786: step 8007, loss 0.0163345, acc 1
2016-09-06T15:22:34.855796: step 8008, loss 0.00827107, acc 1
2016-09-06T15:22:35.701281: step 8009, loss 0.0128164, acc 1
2016-09-06T15:22:36.475868: step 8010, loss 0.0175736, acc 1
2016-09-06T15:22:37.280455: step 8011, loss 0.016639, acc 1
2016-09-06T15:22:38.098993: step 8012, loss 0.00483315, acc 1
2016-09-06T15:22:38.891475: step 8013, loss 0.0489106, acc 0.98
2016-09-06T15:22:39.704360: step 8014, loss 0.0258921, acc 1
2016-09-06T15:22:40.542394: step 8015, loss 0.010443, acc 1
2016-09-06T15:22:41.329267: step 8016, loss 0.0337819, acc 0.98
2016-09-06T15:22:42.148197: step 8017, loss 0.0173147, acc 0.98
2016-09-06T15:22:42.977382: step 8018, loss 0.0293532, acc 0.98
2016-09-06T15:22:43.755602: step 8019, loss 0.0354817, acc 0.98
2016-09-06T15:22:44.565505: step 8020, loss 0.00647801, acc 1
2016-09-06T15:22:45.398486: step 8021, loss 0.00630574, acc 1
2016-09-06T15:22:46.202931: step 8022, loss 0.0379946, acc 0.98
2016-09-06T15:22:46.992400: step 8023, loss 0.00274791, acc 1
2016-09-06T15:22:47.828194: step 8024, loss 0.0125305, acc 1
2016-09-06T15:22:48.641246: step 8025, loss 0.0117688, acc 1
2016-09-06T15:22:49.462085: step 8026, loss 0.00357433, acc 1
2016-09-06T15:22:50.284900: step 8027, loss 0.00558608, acc 1
2016-09-06T15:22:51.093758: step 8028, loss 0.0487491, acc 0.98
2016-09-06T15:22:51.915122: step 8029, loss 0.019885, acc 0.98
2016-09-06T15:22:52.753573: step 8030, loss 0.0913753, acc 0.98
2016-09-06T15:22:53.579814: step 8031, loss 0.0134254, acc 1
2016-09-06T15:22:54.393384: step 8032, loss 0.0553435, acc 0.98
2016-09-06T15:22:55.215343: step 8033, loss 0.0397985, acc 0.98
2016-09-06T15:22:56.026829: step 8034, loss 0.00408978, acc 1
2016-09-06T15:22:56.836153: step 8035, loss 0.0130294, acc 1
2016-09-06T15:22:57.673384: step 8036, loss 0.00307604, acc 1
2016-09-06T15:22:58.490076: step 8037, loss 0.00315825, acc 1
2016-09-06T15:22:59.290923: step 8038, loss 0.0203283, acc 1
2016-09-06T15:23:00.104346: step 8039, loss 0.00632548, acc 1
2016-09-06T15:23:00.946953: step 8040, loss 0.0242032, acc 0.98
2016-09-06T15:23:01.807963: step 8041, loss 0.109969, acc 0.9
2016-09-06T15:23:02.634723: step 8042, loss 0.0147587, acc 1
2016-09-06T15:23:03.461586: step 8043, loss 0.011695, acc 1
2016-09-06T15:23:04.251068: step 8044, loss 0.0161639, acc 1
2016-09-06T15:23:05.050059: step 8045, loss 0.0196157, acc 0.98
2016-09-06T15:23:05.857183: step 8046, loss 0.0104324, acc 1
2016-09-06T15:23:06.657508: step 8047, loss 0.0544264, acc 0.98
2016-09-06T15:23:07.456821: step 8048, loss 0.00270563, acc 1
2016-09-06T15:23:08.302061: step 8049, loss 0.0245892, acc 1
2016-09-06T15:23:09.090043: step 8050, loss 0.0211668, acc 1
2016-09-06T15:23:09.878668: step 8051, loss 0.0168402, acc 1
2016-09-06T15:23:10.698842: step 8052, loss 0.0331031, acc 0.98
2016-09-06T15:23:11.497230: step 8053, loss 0.0249823, acc 0.98
2016-09-06T15:23:12.316153: step 8054, loss 0.0669505, acc 0.98
2016-09-06T15:23:13.134006: step 8055, loss 0.0033175, acc 1
2016-09-06T15:23:13.923014: step 8056, loss 0.00382847, acc 1
2016-09-06T15:23:14.733701: step 8057, loss 0.0678887, acc 0.96
2016-09-06T15:23:15.554966: step 8058, loss 0.0868568, acc 0.98
2016-09-06T15:23:16.344937: step 8059, loss 0.0154091, acc 1
2016-09-06T15:23:17.155900: step 8060, loss 0.00267395, acc 1
2016-09-06T15:23:17.997118: step 8061, loss 0.0526815, acc 0.98
2016-09-06T15:23:18.782722: step 8062, loss 0.0254429, acc 0.98
2016-09-06T15:23:19.623246: step 8063, loss 0.01623, acc 1
2016-09-06T15:23:20.338880: step 8064, loss 0.00945439, acc 1
2016-09-06T15:23:21.168950: step 8065, loss 0.0204605, acc 1
2016-09-06T15:23:21.991146: step 8066, loss 0.0445832, acc 0.98
2016-09-06T15:23:22.803503: step 8067, loss 0.0149351, acc 1
2016-09-06T15:23:23.599611: step 8068, loss 0.0216605, acc 0.98
2016-09-06T15:23:24.415800: step 8069, loss 0.0193795, acc 1
2016-09-06T15:23:25.250445: step 8070, loss 0.0207237, acc 0.98
2016-09-06T15:23:26.016895: step 8071, loss 0.0064516, acc 1
2016-09-06T15:23:26.817104: step 8072, loss 0.0187584, acc 0.98
2016-09-06T15:23:27.635027: step 8073, loss 0.0451869, acc 0.96
2016-09-06T15:23:28.420509: step 8074, loss 0.0370763, acc 0.98
2016-09-06T15:23:29.210601: step 8075, loss 0.00259946, acc 1
2016-09-06T15:23:30.030552: step 8076, loss 0.0156211, acc 1
2016-09-06T15:23:30.830041: step 8077, loss 0.0284287, acc 0.98
2016-09-06T15:23:31.660553: step 8078, loss 0.0156426, acc 1
2016-09-06T15:23:32.504699: step 8079, loss 0.02673, acc 1
2016-09-06T15:23:33.295072: step 8080, loss 0.00593326, acc 1
2016-09-06T15:23:34.072383: step 8081, loss 0.0286854, acc 0.98
2016-09-06T15:23:34.907474: step 8082, loss 0.002275, acc 1
2016-09-06T15:23:35.684045: step 8083, loss 0.0224193, acc 0.98
2016-09-06T15:23:36.498363: step 8084, loss 0.00473611, acc 1
2016-09-06T15:23:37.327107: step 8085, loss 0.00503952, acc 1
2016-09-06T15:23:38.139922: step 8086, loss 0.0166168, acc 0.98
2016-09-06T15:23:38.949892: step 8087, loss 0.0155837, acc 1
2016-09-06T15:23:39.763558: step 8088, loss 0.0191739, acc 0.98
2016-09-06T15:23:40.540076: step 8089, loss 0.00257436, acc 1
2016-09-06T15:23:41.390520: step 8090, loss 0.00568282, acc 1
2016-09-06T15:23:42.228573: step 8091, loss 0.101577, acc 0.98
2016-09-06T15:23:43.061202: step 8092, loss 0.00281734, acc 1
2016-09-06T15:23:43.877313: step 8093, loss 0.00219554, acc 1
2016-09-06T15:23:44.717306: step 8094, loss 0.00294621, acc 1
2016-09-06T15:23:45.539290: step 8095, loss 0.00226953, acc 1
2016-09-06T15:23:46.338104: step 8096, loss 0.0443876, acc 0.96
2016-09-06T15:23:47.193413: step 8097, loss 0.0340641, acc 0.98
2016-09-06T15:23:48.001401: step 8098, loss 0.0159721, acc 1
2016-09-06T15:23:48.832945: step 8099, loss 0.0117091, acc 1
2016-09-06T15:23:49.666276: step 8100, loss 0.0176352, acc 1

Evaluation:
2016-09-06T15:23:53.394238: step 8100, loss 1.56873, acc 0.741088

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-8100

2016-09-06T15:23:55.433088: step 8101, loss 0.0341423, acc 0.98
2016-09-06T15:23:56.242072: step 8102, loss 0.0149692, acc 1
2016-09-06T15:23:57.065363: step 8103, loss 0.00457805, acc 1
2016-09-06T15:23:57.910086: step 8104, loss 0.00245296, acc 1
2016-09-06T15:23:58.716238: step 8105, loss 0.0161934, acc 1
2016-09-06T15:23:59.525478: step 8106, loss 0.00570068, acc 1
2016-09-06T15:24:00.358637: step 8107, loss 0.0687135, acc 0.94
2016-09-06T15:24:01.197380: step 8108, loss 0.0181118, acc 0.98
2016-09-06T15:24:02.001573: step 8109, loss 0.0189424, acc 0.98
2016-09-06T15:24:02.838918: step 8110, loss 0.00520068, acc 1
2016-09-06T15:24:03.624202: step 8111, loss 0.0446716, acc 0.98
2016-09-06T15:24:04.436117: step 8112, loss 0.0221132, acc 0.98
2016-09-06T15:24:05.250991: step 8113, loss 0.00792953, acc 1
2016-09-06T15:24:06.046051: step 8114, loss 0.0173199, acc 0.98
2016-09-06T15:24:06.833253: step 8115, loss 0.101084, acc 0.98
2016-09-06T15:24:07.624757: step 8116, loss 0.00241077, acc 1
2016-09-06T15:24:08.431095: step 8117, loss 0.0148074, acc 1
2016-09-06T15:24:09.259340: step 8118, loss 0.0147577, acc 1
2016-09-06T15:24:10.086127: step 8119, loss 0.0132324, acc 1
2016-09-06T15:24:10.891964: step 8120, loss 0.0461913, acc 0.98
2016-09-06T15:24:11.706263: step 8121, loss 0.00601822, acc 1
2016-09-06T15:24:12.512668: step 8122, loss 0.00691981, acc 1
2016-09-06T15:24:13.332009: step 8123, loss 0.0307296, acc 0.98
2016-09-06T15:24:14.145224: step 8124, loss 0.0199119, acc 1
2016-09-06T15:24:14.982539: step 8125, loss 0.00259836, acc 1
2016-09-06T15:24:15.770681: step 8126, loss 0.0402626, acc 0.98
2016-09-06T15:24:16.588388: step 8127, loss 0.00590554, acc 1
2016-09-06T15:24:17.423225: step 8128, loss 0.0236933, acc 0.98
2016-09-06T15:24:18.259314: step 8129, loss 0.0199969, acc 1
2016-09-06T15:24:19.087306: step 8130, loss 0.0210999, acc 1
2016-09-06T15:24:19.912661: step 8131, loss 0.00336425, acc 1
2016-09-06T15:24:20.740285: step 8132, loss 0.0284027, acc 1
2016-09-06T15:24:21.534324: step 8133, loss 0.042574, acc 0.98
2016-09-06T15:24:22.358098: step 8134, loss 0.00269449, acc 1
2016-09-06T15:24:23.173058: step 8135, loss 0.0247977, acc 0.98
2016-09-06T15:24:23.989202: step 8136, loss 0.00321365, acc 1
2016-09-06T15:24:24.799316: step 8137, loss 0.00344484, acc 1
2016-09-06T15:24:25.623570: step 8138, loss 0.00226912, acc 1
2016-09-06T15:24:26.428861: step 8139, loss 0.00406931, acc 1
2016-09-06T15:24:27.267156: step 8140, loss 0.0155289, acc 1
2016-09-06T15:24:28.089312: step 8141, loss 0.00232022, acc 1
2016-09-06T15:24:28.909380: step 8142, loss 0.0474861, acc 0.96
2016-09-06T15:24:29.737086: step 8143, loss 0.00798084, acc 1
2016-09-06T15:24:30.568808: step 8144, loss 0.0299746, acc 0.98
2016-09-06T15:24:31.359275: step 8145, loss 0.00233993, acc 1
2016-09-06T15:24:32.162284: step 8146, loss 0.00632904, acc 1
2016-09-06T15:24:32.973688: step 8147, loss 0.085704, acc 0.96
2016-09-06T15:24:33.769416: step 8148, loss 0.00264652, acc 1
2016-09-06T15:24:34.582508: step 8149, loss 0.0494736, acc 0.98
2016-09-06T15:24:35.402170: step 8150, loss 0.0232492, acc 0.98
2016-09-06T15:24:36.192122: step 8151, loss 0.00288512, acc 1
2016-09-06T15:24:36.991719: step 8152, loss 0.00947249, acc 1
2016-09-06T15:24:37.829620: step 8153, loss 0.0266293, acc 0.98
2016-09-06T15:24:38.622272: step 8154, loss 0.0934237, acc 0.96
2016-09-06T15:24:39.420403: step 8155, loss 0.140331, acc 0.96
2016-09-06T15:24:40.257601: step 8156, loss 0.00227481, acc 1
2016-09-06T15:24:41.056738: step 8157, loss 0.00281387, acc 1
2016-09-06T15:24:41.846115: step 8158, loss 0.00220019, acc 1
2016-09-06T15:24:42.697096: step 8159, loss 0.00300357, acc 1
2016-09-06T15:24:43.485492: step 8160, loss 0.0140081, acc 1
2016-09-06T15:24:44.310303: step 8161, loss 0.0180323, acc 0.98
2016-09-06T15:24:45.100078: step 8162, loss 0.0483734, acc 0.96
2016-09-06T15:24:45.871943: step 8163, loss 0.0400743, acc 1
2016-09-06T15:24:46.682909: step 8164, loss 0.0081415, acc 1
2016-09-06T15:24:47.501008: step 8165, loss 0.0242445, acc 0.98
2016-09-06T15:24:48.332369: step 8166, loss 0.0159018, acc 1
2016-09-06T15:24:49.112369: step 8167, loss 0.0371414, acc 0.98
2016-09-06T15:24:49.939479: step 8168, loss 0.00304134, acc 1
2016-09-06T15:24:50.744110: step 8169, loss 0.016058, acc 1
2016-09-06T15:24:51.555023: step 8170, loss 0.0405421, acc 0.98
2016-09-06T15:24:52.360765: step 8171, loss 0.0458338, acc 0.96
2016-09-06T15:24:53.172190: step 8172, loss 0.0185313, acc 1
2016-09-06T15:24:53.985885: step 8173, loss 0.0378374, acc 0.98
2016-09-06T15:24:54.819127: step 8174, loss 0.0292614, acc 1
2016-09-06T15:24:55.648587: step 8175, loss 0.00344471, acc 1
2016-09-06T15:24:56.458910: step 8176, loss 0.0338477, acc 1
2016-09-06T15:24:57.305101: step 8177, loss 0.08489, acc 0.98
2016-09-06T15:24:58.108433: step 8178, loss 0.0473888, acc 0.98
2016-09-06T15:24:58.921935: step 8179, loss 0.0332266, acc 0.98
2016-09-06T15:24:59.807263: step 8180, loss 0.102344, acc 0.98
2016-09-06T15:25:00.631999: step 8181, loss 0.0197914, acc 0.98
2016-09-06T15:25:01.432111: step 8182, loss 0.0249386, acc 0.98
2016-09-06T15:25:02.244565: step 8183, loss 0.095603, acc 0.98
2016-09-06T15:25:03.072402: step 8184, loss 0.00529253, acc 1
2016-09-06T15:25:03.886700: step 8185, loss 0.0113001, acc 1
2016-09-06T15:25:04.719848: step 8186, loss 0.00507058, acc 1
2016-09-06T15:25:05.535978: step 8187, loss 0.00621905, acc 1
2016-09-06T15:25:06.327462: step 8188, loss 0.0721924, acc 0.96
2016-09-06T15:25:07.181598: step 8189, loss 0.154567, acc 0.96
2016-09-06T15:25:08.003406: step 8190, loss 0.0119057, acc 1
2016-09-06T15:25:08.801459: step 8191, loss 0.0366887, acc 0.96
2016-09-06T15:25:09.631313: step 8192, loss 0.00412588, acc 1
2016-09-06T15:25:10.441514: step 8193, loss 0.00746489, acc 1
2016-09-06T15:25:11.241382: step 8194, loss 0.0267924, acc 0.98
2016-09-06T15:25:12.033665: step 8195, loss 0.0208226, acc 1
2016-09-06T15:25:12.868683: step 8196, loss 0.0365581, acc 0.98
2016-09-06T15:25:13.671314: step 8197, loss 0.0583825, acc 0.98
2016-09-06T15:25:14.479128: step 8198, loss 0.0182305, acc 0.98
2016-09-06T15:25:15.289399: step 8199, loss 0.0651271, acc 0.98
2016-09-06T15:25:16.064069: step 8200, loss 0.00587226, acc 1

Evaluation:
2016-09-06T15:25:19.768293: step 8200, loss 1.88692, acc 0.752345

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-8200

2016-09-06T15:25:21.755991: step 8201, loss 0.0725866, acc 0.96
2016-09-06T15:25:22.576632: step 8202, loss 0.023353, acc 1
2016-09-06T15:25:23.410277: step 8203, loss 0.00406029, acc 1
2016-09-06T15:25:24.238129: step 8204, loss 0.00388174, acc 1
2016-09-06T15:25:25.057320: step 8205, loss 0.014369, acc 1
2016-09-06T15:25:25.890099: step 8206, loss 0.0326838, acc 0.98
2016-09-06T15:25:26.682049: step 8207, loss 0.00761478, acc 1
2016-09-06T15:25:27.533806: step 8208, loss 0.0391151, acc 0.98
2016-09-06T15:25:28.325950: step 8209, loss 0.0148667, acc 1
2016-09-06T15:25:29.122457: step 8210, loss 0.16612, acc 0.96
2016-09-06T15:25:29.967045: step 8211, loss 0.0215093, acc 1
2016-09-06T15:25:30.749457: step 8212, loss 0.022027, acc 1
2016-09-06T15:25:31.534294: step 8213, loss 0.0352871, acc 0.98
2016-09-06T15:25:32.368583: step 8214, loss 0.0730229, acc 0.96
2016-09-06T15:25:33.194202: step 8215, loss 0.00967443, acc 1
2016-09-06T15:25:34.005529: step 8216, loss 0.00854493, acc 1
2016-09-06T15:25:34.848975: step 8217, loss 0.00776051, acc 1
2016-09-06T15:25:35.664415: step 8218, loss 0.00853764, acc 1
2016-09-06T15:25:36.503637: step 8219, loss 0.0515186, acc 0.98
2016-09-06T15:25:37.344171: step 8220, loss 0.0341579, acc 0.98
2016-09-06T15:25:38.226856: step 8221, loss 0.0345572, acc 0.98
2016-09-06T15:25:39.042083: step 8222, loss 0.00983552, acc 1
2016-09-06T15:25:39.865905: step 8223, loss 0.0158408, acc 1
2016-09-06T15:25:40.696894: step 8224, loss 0.0410117, acc 0.98
2016-09-06T15:25:41.518175: step 8225, loss 0.0541206, acc 0.98
2016-09-06T15:25:42.367056: step 8226, loss 0.013038, acc 1
2016-09-06T15:25:43.169641: step 8227, loss 0.0143665, acc 1
2016-09-06T15:25:43.966689: step 8228, loss 0.0174588, acc 1
2016-09-06T15:25:44.797440: step 8229, loss 0.0185079, acc 1
2016-09-06T15:25:45.601429: step 8230, loss 0.0216149, acc 1
2016-09-06T15:25:46.411344: step 8231, loss 0.00395886, acc 1
2016-09-06T15:25:47.224870: step 8232, loss 0.0233909, acc 0.98
2016-09-06T15:25:48.055766: step 8233, loss 0.0299123, acc 0.98
2016-09-06T15:25:48.848838: step 8234, loss 0.0104777, acc 1
2016-09-06T15:25:49.662744: step 8235, loss 0.00794732, acc 1
2016-09-06T15:25:50.472795: step 8236, loss 0.0101222, acc 1
2016-09-06T15:25:51.275509: step 8237, loss 0.0210116, acc 0.98
2016-09-06T15:25:52.072793: step 8238, loss 0.0193256, acc 0.98
2016-09-06T15:25:52.886828: step 8239, loss 0.113868, acc 0.96
2016-09-06T15:25:53.660420: step 8240, loss 0.045189, acc 0.98
2016-09-06T15:25:54.491373: step 8241, loss 0.0753526, acc 0.96
2016-09-06T15:25:55.304615: step 8242, loss 0.0290467, acc 0.98
2016-09-06T15:25:56.107598: step 8243, loss 0.0136478, acc 1
2016-09-06T15:25:56.903721: step 8244, loss 0.0761595, acc 0.96
2016-09-06T15:25:57.746499: step 8245, loss 0.0195489, acc 0.98
2016-09-06T15:25:58.532036: step 8246, loss 0.00387615, acc 1
2016-09-06T15:25:59.334719: step 8247, loss 0.00516693, acc 1
2016-09-06T15:26:00.170842: step 8248, loss 0.00629422, acc 1
2016-09-06T15:26:01.005656: step 8249, loss 0.0297596, acc 0.98
2016-09-06T15:26:01.804395: step 8250, loss 0.0240968, acc 1
2016-09-06T15:26:02.645151: step 8251, loss 0.0521732, acc 0.98
2016-09-06T15:26:03.455525: step 8252, loss 0.00788122, acc 1
2016-09-06T15:26:04.275319: step 8253, loss 0.0838844, acc 0.94
2016-09-06T15:26:05.103588: step 8254, loss 0.0167387, acc 1
2016-09-06T15:26:05.902688: step 8255, loss 0.0273391, acc 0.98
2016-09-06T15:26:06.647415: step 8256, loss 0.0171789, acc 1
2016-09-06T15:26:07.448467: step 8257, loss 0.123888, acc 0.96
2016-09-06T15:26:08.225165: step 8258, loss 0.0125228, acc 1
2016-09-06T15:26:09.024638: step 8259, loss 0.0125391, acc 1
2016-09-06T15:26:09.844290: step 8260, loss 0.00333538, acc 1
2016-09-06T15:26:10.645398: step 8261, loss 0.0688842, acc 0.96
2016-09-06T15:26:11.443171: step 8262, loss 0.0223531, acc 0.98
2016-09-06T15:26:12.273787: step 8263, loss 0.0816814, acc 0.96
2016-09-06T15:26:13.037086: step 8264, loss 0.0388799, acc 1
2016-09-06T15:26:13.818555: step 8265, loss 0.0285014, acc 0.98
2016-09-06T15:26:14.633835: step 8266, loss 0.0308963, acc 0.98
2016-09-06T15:26:15.431401: step 8267, loss 0.0237979, acc 0.98
2016-09-06T15:26:16.249165: step 8268, loss 0.00647723, acc 1
2016-09-06T15:26:17.068621: step 8269, loss 0.0211714, acc 1
2016-09-06T15:26:17.857680: step 8270, loss 0.0317527, acc 1
2016-09-06T15:26:18.680403: step 8271, loss 0.0320778, acc 0.98
2016-09-06T15:26:19.488838: step 8272, loss 0.0263559, acc 1
2016-09-06T15:26:20.279704: step 8273, loss 0.0236843, acc 1
2016-09-06T15:26:21.089767: step 8274, loss 0.00353357, acc 1
2016-09-06T15:26:21.916780: step 8275, loss 0.0322121, acc 0.98
2016-09-06T15:26:22.719084: step 8276, loss 0.00304813, acc 1
2016-09-06T15:26:23.515183: step 8277, loss 0.0789568, acc 0.98
2016-09-06T15:26:24.340528: step 8278, loss 0.00278884, acc 1
2016-09-06T15:26:25.138300: step 8279, loss 0.0162202, acc 1
2016-09-06T15:26:25.947055: step 8280, loss 0.0515928, acc 0.98
2016-09-06T15:26:26.808204: step 8281, loss 0.00241306, acc 1
2016-09-06T15:26:27.611364: step 8282, loss 0.0494778, acc 0.96
2016-09-06T15:26:28.421077: step 8283, loss 0.0247272, acc 0.98
2016-09-06T15:26:29.227670: step 8284, loss 0.0302561, acc 0.98
2016-09-06T15:26:30.037410: step 8285, loss 0.0073408, acc 1
2016-09-06T15:26:30.851504: step 8286, loss 0.0645656, acc 0.98
2016-09-06T15:26:31.692727: step 8287, loss 0.00236446, acc 1
2016-09-06T15:26:32.505748: step 8288, loss 0.0109533, acc 1
2016-09-06T15:26:33.306023: step 8289, loss 0.0251936, acc 1
2016-09-06T15:26:34.145546: step 8290, loss 0.0444675, acc 0.96
2016-09-06T15:26:34.969787: step 8291, loss 0.00303148, acc 1
2016-09-06T15:26:35.772744: step 8292, loss 0.0173139, acc 0.98
2016-09-06T15:26:36.607578: step 8293, loss 0.00428592, acc 1
2016-09-06T15:26:37.433525: step 8294, loss 0.00478146, acc 1
2016-09-06T15:26:38.255668: step 8295, loss 0.0297344, acc 0.98
2016-09-06T15:26:39.102381: step 8296, loss 0.00545656, acc 1
2016-09-06T15:26:39.908378: step 8297, loss 0.0292214, acc 0.98
2016-09-06T15:26:40.724366: step 8298, loss 0.010583, acc 1
2016-09-06T15:26:41.591766: step 8299, loss 0.0554041, acc 0.98
2016-09-06T15:26:42.402256: step 8300, loss 0.00305207, acc 1

Evaluation:
2016-09-06T15:26:46.133385: step 8300, loss 1.43472, acc 0.742026

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-8300

2016-09-06T15:26:48.029898: step 8301, loss 0.00335438, acc 1
2016-09-06T15:26:48.906749: step 8302, loss 0.017048, acc 1
2016-09-06T15:26:49.711578: step 8303, loss 0.0261865, acc 1
2016-09-06T15:26:50.551964: step 8304, loss 0.0169076, acc 0.98
2016-09-06T15:26:51.384631: step 8305, loss 0.00360598, acc 1
2016-09-06T15:26:52.190406: step 8306, loss 0.00516355, acc 1
2016-09-06T15:26:52.941445: step 8307, loss 0.00347602, acc 1
2016-09-06T15:26:53.759725: step 8308, loss 0.00874474, acc 1
2016-09-06T15:26:54.605845: step 8309, loss 0.0167556, acc 0.98
2016-09-06T15:26:55.389705: step 8310, loss 0.0157425, acc 1
2016-09-06T15:26:56.190271: step 8311, loss 0.00345198, acc 1
2016-09-06T15:26:57.011331: step 8312, loss 0.0186173, acc 1
2016-09-06T15:26:57.791200: step 8313, loss 0.00696259, acc 1
2016-09-06T15:26:58.586082: step 8314, loss 0.00303815, acc 1
2016-09-06T15:26:59.407950: step 8315, loss 0.0405113, acc 0.96
2016-09-06T15:27:00.250508: step 8316, loss 0.021429, acc 1
2016-09-06T15:27:01.053202: step 8317, loss 0.0396347, acc 0.98
2016-09-06T15:27:01.860260: step 8318, loss 0.00835532, acc 1
2016-09-06T15:27:02.626515: step 8319, loss 0.0169404, acc 0.98
2016-09-06T15:27:03.438958: step 8320, loss 0.0351892, acc 1
2016-09-06T15:27:04.264318: step 8321, loss 0.0162708, acc 1
2016-09-06T15:27:05.034876: step 8322, loss 0.0327933, acc 0.98
2016-09-06T15:27:05.839744: step 8323, loss 0.0209496, acc 1
2016-09-06T15:27:06.657608: step 8324, loss 0.0444199, acc 0.96
2016-09-06T15:27:07.462876: step 8325, loss 0.072352, acc 0.98
2016-09-06T15:27:08.255750: step 8326, loss 0.0214418, acc 0.98
2016-09-06T15:27:09.113708: step 8327, loss 0.0134025, acc 1
2016-09-06T15:27:09.889745: step 8328, loss 0.00332661, acc 1
2016-09-06T15:27:10.709775: step 8329, loss 0.0227458, acc 0.98
2016-09-06T15:27:11.529541: step 8330, loss 0.00296502, acc 1
2016-09-06T15:27:12.334023: step 8331, loss 0.0104096, acc 1
2016-09-06T15:27:13.158179: step 8332, loss 0.0148035, acc 1
2016-09-06T15:27:13.979263: step 8333, loss 0.00816629, acc 1
2016-09-06T15:27:14.796274: step 8334, loss 0.00768624, acc 1
2016-09-06T15:27:15.606889: step 8335, loss 0.0098337, acc 1
2016-09-06T15:27:16.444533: step 8336, loss 0.00601497, acc 1
2016-09-06T15:27:17.237038: step 8337, loss 0.0133921, acc 1
2016-09-06T15:27:18.042876: step 8338, loss 0.00287584, acc 1
2016-09-06T15:27:18.870311: step 8339, loss 0.0334082, acc 1
2016-09-06T15:27:19.672838: step 8340, loss 0.002823, acc 1
2016-09-06T15:27:20.491054: step 8341, loss 0.0213814, acc 1
2016-09-06T15:27:21.311546: step 8342, loss 0.0448974, acc 0.96
2016-09-06T15:27:22.118692: step 8343, loss 0.00285028, acc 1
2016-09-06T15:27:22.952951: step 8344, loss 0.0052885, acc 1
2016-09-06T15:27:23.762142: step 8345, loss 0.00504514, acc 1
2016-09-06T15:27:24.585783: step 8346, loss 0.00569991, acc 1
2016-09-06T15:27:25.393922: step 8347, loss 0.0126182, acc 1
2016-09-06T15:27:26.229322: step 8348, loss 0.00876524, acc 1
2016-09-06T15:27:27.043131: step 8349, loss 0.00415858, acc 1
2016-09-06T15:27:27.866084: step 8350, loss 0.00302525, acc 1
2016-09-06T15:27:28.709463: step 8351, loss 0.0163065, acc 1
2016-09-06T15:27:29.550042: step 8352, loss 0.00374619, acc 1
2016-09-06T15:27:30.342115: step 8353, loss 0.0112688, acc 1
2016-09-06T15:27:31.175136: step 8354, loss 0.0473563, acc 0.98
2016-09-06T15:27:31.981443: step 8355, loss 0.0726508, acc 0.98
2016-09-06T15:27:32.780676: step 8356, loss 0.0144762, acc 1
2016-09-06T15:27:33.596665: step 8357, loss 0.0154892, acc 1
2016-09-06T15:27:34.416169: step 8358, loss 0.00327081, acc 1
2016-09-06T15:27:35.196489: step 8359, loss 0.0129635, acc 1
2016-09-06T15:27:36.007523: step 8360, loss 0.00484785, acc 1
2016-09-06T15:27:36.838696: step 8361, loss 0.12753, acc 0.96
2016-09-06T15:27:37.639597: step 8362, loss 0.00391817, acc 1
2016-09-06T15:27:38.454998: step 8363, loss 0.0192324, acc 1
2016-09-06T15:27:39.270849: step 8364, loss 0.0534421, acc 0.96
2016-09-06T15:27:40.071493: step 8365, loss 0.0321232, acc 0.98
2016-09-06T15:27:40.871342: step 8366, loss 0.0349821, acc 0.98
2016-09-06T15:27:41.681453: step 8367, loss 0.00247204, acc 1
2016-09-06T15:27:42.481374: step 8368, loss 0.0388467, acc 0.98
2016-09-06T15:27:43.280559: step 8369, loss 0.0322097, acc 0.98
2016-09-06T15:27:44.098458: step 8370, loss 0.00330964, acc 1
2016-09-06T15:27:44.898947: step 8371, loss 0.0227084, acc 0.98
2016-09-06T15:27:45.691439: step 8372, loss 0.00369664, acc 1
2016-09-06T15:27:46.506087: step 8373, loss 0.0392764, acc 0.98
2016-09-06T15:27:47.313459: step 8374, loss 0.0111681, acc 1
2016-09-06T15:27:48.133129: step 8375, loss 0.00972271, acc 1
2016-09-06T15:27:48.947328: step 8376, loss 0.0402263, acc 0.98
2016-09-06T15:27:49.792963: step 8377, loss 0.00622664, acc 1
2016-09-06T15:27:50.572766: step 8378, loss 0.0192138, acc 0.98
2016-09-06T15:27:51.406636: step 8379, loss 0.00393204, acc 1
2016-09-06T15:27:52.180816: step 8380, loss 0.0342403, acc 0.96
2016-09-06T15:27:52.991110: step 8381, loss 0.00292811, acc 1
2016-09-06T15:27:53.837726: step 8382, loss 0.00867807, acc 1
2016-09-06T15:27:54.641384: step 8383, loss 0.00897526, acc 1
2016-09-06T15:27:55.454139: step 8384, loss 0.0177579, acc 1
2016-09-06T15:27:56.274722: step 8385, loss 0.00317983, acc 1
2016-09-06T15:27:57.071623: step 8386, loss 0.0116206, acc 1
2016-09-06T15:27:57.898793: step 8387, loss 0.00459347, acc 1
2016-09-06T15:27:58.769007: step 8388, loss 0.00281634, acc 1
2016-09-06T15:27:59.578551: step 8389, loss 0.035853, acc 0.98
2016-09-06T15:28:00.439761: step 8390, loss 0.0035186, acc 1
2016-09-06T15:28:01.296765: step 8391, loss 0.0166582, acc 1
2016-09-06T15:28:02.125192: step 8392, loss 0.0344896, acc 0.98
2016-09-06T15:28:02.952550: step 8393, loss 0.0168953, acc 1
2016-09-06T15:28:03.792883: step 8394, loss 0.0357672, acc 0.98
2016-09-06T15:28:04.604995: step 8395, loss 0.0360415, acc 0.98
2016-09-06T15:28:05.395664: step 8396, loss 0.00255305, acc 1
2016-09-06T15:28:06.220520: step 8397, loss 0.0249652, acc 1
2016-09-06T15:28:07.027687: step 8398, loss 0.00888864, acc 1
2016-09-06T15:28:07.835255: step 8399, loss 0.0025855, acc 1
2016-09-06T15:28:08.699410: step 8400, loss 0.0172793, acc 1

Evaluation:
2016-09-06T15:28:12.458657: step 8400, loss 2.0198, acc 0.750469

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-8400

2016-09-06T15:28:14.422345: step 8401, loss 0.00270397, acc 1
2016-09-06T15:28:15.256495: step 8402, loss 0.0207493, acc 1
2016-09-06T15:28:16.051817: step 8403, loss 0.149465, acc 0.96
2016-09-06T15:28:16.872270: step 8404, loss 0.0223577, acc 0.98
2016-09-06T15:28:17.663619: step 8405, loss 0.0108313, acc 1
2016-09-06T15:28:18.468935: step 8406, loss 0.0359417, acc 0.98
2016-09-06T15:28:19.276910: step 8407, loss 0.00236109, acc 1
2016-09-06T15:28:20.062699: step 8408, loss 0.00239332, acc 1
2016-09-06T15:28:20.866703: step 8409, loss 0.0456175, acc 0.98
2016-09-06T15:28:21.697647: step 8410, loss 0.041872, acc 0.98
2016-09-06T15:28:22.486689: step 8411, loss 0.0197702, acc 1
2016-09-06T15:28:23.298390: step 8412, loss 0.0243745, acc 0.98
2016-09-06T15:28:24.114024: step 8413, loss 0.00746496, acc 1
2016-09-06T15:28:24.906873: step 8414, loss 0.0671261, acc 0.98
2016-09-06T15:28:25.708207: step 8415, loss 0.00725684, acc 1
2016-09-06T15:28:26.502768: step 8416, loss 0.0231562, acc 1
2016-09-06T15:28:27.295510: step 8417, loss 0.114479, acc 0.92
2016-09-06T15:28:28.116669: step 8418, loss 0.00875321, acc 1
2016-09-06T15:28:28.954701: step 8419, loss 0.0344681, acc 0.98
2016-09-06T15:28:29.769717: step 8420, loss 0.00672408, acc 1
2016-09-06T15:28:30.591357: step 8421, loss 0.00381873, acc 1
2016-09-06T15:28:31.416851: step 8422, loss 0.0354468, acc 0.98
2016-09-06T15:28:32.228359: step 8423, loss 0.0577918, acc 0.98
2016-09-06T15:28:33.033130: step 8424, loss 0.00992778, acc 1
2016-09-06T15:28:33.851752: step 8425, loss 0.0454889, acc 0.98
2016-09-06T15:28:34.654909: step 8426, loss 0.00224137, acc 1
2016-09-06T15:28:35.456222: step 8427, loss 0.0478739, acc 0.98
2016-09-06T15:28:36.249222: step 8428, loss 0.00524217, acc 1
2016-09-06T15:28:37.008338: step 8429, loss 0.00980016, acc 1
2016-09-06T15:28:37.823746: step 8430, loss 0.0148044, acc 1
2016-09-06T15:28:38.665185: step 8431, loss 0.0210251, acc 1
2016-09-06T15:28:39.451498: step 8432, loss 0.00317477, acc 1
2016-09-06T15:28:40.245238: step 8433, loss 0.033001, acc 1
2016-09-06T15:28:41.069423: step 8434, loss 0.0483094, acc 0.98
2016-09-06T15:28:41.836963: step 8435, loss 0.0140217, acc 1
2016-09-06T15:28:42.641623: step 8436, loss 0.0343014, acc 0.96
2016-09-06T15:28:43.451731: step 8437, loss 0.0413501, acc 0.98
2016-09-06T15:28:44.238303: step 8438, loss 0.0306486, acc 0.98
2016-09-06T15:28:45.056094: step 8439, loss 0.0023789, acc 1
2016-09-06T15:28:45.885548: step 8440, loss 0.00269668, acc 1
2016-09-06T15:28:46.672937: step 8441, loss 0.0576168, acc 0.98
2016-09-06T15:28:47.514781: step 8442, loss 0.0315506, acc 0.98
2016-09-06T15:28:48.324526: step 8443, loss 0.0244532, acc 0.98
2016-09-06T15:28:49.113454: step 8444, loss 0.00390729, acc 1
2016-09-06T15:28:49.902185: step 8445, loss 0.0235251, acc 1
2016-09-06T15:28:50.750284: step 8446, loss 0.0341046, acc 0.96
2016-09-06T15:28:51.519432: step 8447, loss 0.0258022, acc 0.98
2016-09-06T15:28:52.279860: step 8448, loss 0.01232, acc 1
2016-09-06T15:28:53.092647: step 8449, loss 0.0155405, acc 1
2016-09-06T15:28:53.878404: step 8450, loss 0.0481001, acc 0.98
2016-09-06T15:28:54.682041: step 8451, loss 0.00290078, acc 1
2016-09-06T15:28:55.494290: step 8452, loss 0.017807, acc 0.98
2016-09-06T15:28:56.310712: step 8453, loss 0.0430432, acc 0.98
2016-09-06T15:28:57.105753: step 8454, loss 0.0252128, acc 1
2016-09-06T15:28:57.956043: step 8455, loss 0.00410522, acc 1
2016-09-06T15:28:58.756399: step 8456, loss 0.0318677, acc 0.98
2016-09-06T15:28:59.557139: step 8457, loss 0.0331507, acc 1
2016-09-06T15:29:00.385040: step 8458, loss 0.0167091, acc 1
2016-09-06T15:29:01.232538: step 8459, loss 0.00326777, acc 1
2016-09-06T15:29:02.039589: step 8460, loss 0.00492112, acc 1
2016-09-06T15:29:02.872807: step 8461, loss 0.0645552, acc 0.98
2016-09-06T15:29:03.663387: step 8462, loss 0.0207108, acc 1
2016-09-06T15:29:04.478998: step 8463, loss 0.0496769, acc 0.98
2016-09-06T15:29:05.305491: step 8464, loss 0.021393, acc 1
2016-09-06T15:29:06.114441: step 8465, loss 0.0353066, acc 0.98
2016-09-06T15:29:06.933755: step 8466, loss 0.0100974, acc 1
2016-09-06T15:29:07.765621: step 8467, loss 0.00355155, acc 1
2016-09-06T15:29:08.573010: step 8468, loss 0.00301521, acc 1
2016-09-06T15:29:09.378142: step 8469, loss 0.00394026, acc 1
2016-09-06T15:29:10.210240: step 8470, loss 0.0475478, acc 0.98
2016-09-06T15:29:11.026741: step 8471, loss 0.00328394, acc 1
2016-09-06T15:29:11.857066: step 8472, loss 0.017603, acc 1
2016-09-06T15:29:12.687337: step 8473, loss 0.0186597, acc 1
2016-09-06T15:29:13.492287: step 8474, loss 0.039952, acc 0.98
2016-09-06T15:29:14.299585: step 8475, loss 0.0140298, acc 1
2016-09-06T15:29:15.150284: step 8476, loss 0.00286626, acc 1
2016-09-06T15:29:15.961683: step 8477, loss 0.00991051, acc 1
2016-09-06T15:29:16.791162: step 8478, loss 0.0270011, acc 1
2016-09-06T15:29:17.652442: step 8479, loss 0.00281178, acc 1
2016-09-06T15:29:18.481443: step 8480, loss 0.00828544, acc 1
2016-09-06T15:29:19.311838: step 8481, loss 0.0647256, acc 0.96
2016-09-06T15:29:20.111258: step 8482, loss 0.0231085, acc 1
2016-09-06T15:29:20.929017: step 8483, loss 0.01711, acc 1
2016-09-06T15:29:21.732114: step 8484, loss 0.00271526, acc 1
2016-09-06T15:29:22.537245: step 8485, loss 0.0145709, acc 1
2016-09-06T15:29:23.358065: step 8486, loss 0.0293216, acc 0.98
2016-09-06T15:29:24.156765: step 8487, loss 0.0271728, acc 0.98
2016-09-06T15:29:24.963008: step 8488, loss 0.0584598, acc 0.98
2016-09-06T15:29:25.808758: step 8489, loss 0.0113569, acc 1
2016-09-06T15:29:26.577980: step 8490, loss 0.00262595, acc 1
2016-09-06T15:29:27.407706: step 8491, loss 0.014727, acc 1
2016-09-06T15:29:28.225111: step 8492, loss 0.00235111, acc 1
2016-09-06T15:29:28.992840: step 8493, loss 0.00499799, acc 1
2016-09-06T15:29:29.806252: step 8494, loss 0.0116626, acc 1
2016-09-06T15:29:30.633445: step 8495, loss 0.0314476, acc 0.98
2016-09-06T15:29:31.418177: step 8496, loss 0.02374, acc 0.98
2016-09-06T15:29:32.227933: step 8497, loss 0.00368459, acc 1
2016-09-06T15:29:33.035528: step 8498, loss 0.011852, acc 1
2016-09-06T15:29:33.827586: step 8499, loss 0.0183094, acc 1
2016-09-06T15:29:34.634979: step 8500, loss 0.00665856, acc 1

Evaluation:
2016-09-06T15:29:38.360112: step 8500, loss 1.86282, acc 0.755159

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-8500

2016-09-06T15:29:40.351875: step 8501, loss 0.00471348, acc 1
2016-09-06T15:29:41.164275: step 8502, loss 0.0325249, acc 0.98
2016-09-06T15:29:41.962607: step 8503, loss 0.00670274, acc 1
2016-09-06T15:29:42.814703: step 8504, loss 0.0187208, acc 0.98
2016-09-06T15:29:43.622840: step 8505, loss 0.0213977, acc 0.98
2016-09-06T15:29:44.446916: step 8506, loss 0.0194024, acc 0.98
2016-09-06T15:29:45.251202: step 8507, loss 0.0926574, acc 0.94
2016-09-06T15:29:46.052085: step 8508, loss 0.0377874, acc 0.96
2016-09-06T15:29:46.876067: step 8509, loss 0.00346454, acc 1
2016-09-06T15:29:47.699411: step 8510, loss 0.0100088, acc 1
2016-09-06T15:29:48.498510: step 8511, loss 0.00187676, acc 1
2016-09-06T15:29:49.288615: step 8512, loss 0.0255761, acc 0.98
2016-09-06T15:29:50.098179: step 8513, loss 0.0337594, acc 0.98
2016-09-06T15:29:50.868657: step 8514, loss 0.00790929, acc 1
2016-09-06T15:29:51.668675: step 8515, loss 0.00795024, acc 1
2016-09-06T15:29:52.487946: step 8516, loss 0.0347126, acc 1
2016-09-06T15:29:53.285744: step 8517, loss 0.0179586, acc 1
2016-09-06T15:29:54.092344: step 8518, loss 0.0191037, acc 0.98
2016-09-06T15:29:54.908258: step 8519, loss 0.0823374, acc 0.98
2016-09-06T15:29:55.700998: step 8520, loss 0.00322506, acc 1
2016-09-06T15:29:56.509859: step 8521, loss 0.00870537, acc 1
2016-09-06T15:29:57.360454: step 8522, loss 0.0366853, acc 0.98
2016-09-06T15:29:58.127056: step 8523, loss 0.00440538, acc 1
2016-09-06T15:29:58.914199: step 8524, loss 0.00369476, acc 1
2016-09-06T15:29:59.724047: step 8525, loss 0.0110438, acc 1
2016-09-06T15:30:00.544746: step 8526, loss 0.0125579, acc 1
2016-09-06T15:30:01.339314: step 8527, loss 0.0312969, acc 0.98
2016-09-06T15:30:02.149421: step 8528, loss 0.0020555, acc 1
2016-09-06T15:30:02.958910: step 8529, loss 0.00176502, acc 1
2016-09-06T15:30:03.760111: step 8530, loss 0.013528, acc 1
2016-09-06T15:30:04.581264: step 8531, loss 0.0519431, acc 0.96
2016-09-06T15:30:05.377283: step 8532, loss 0.00197081, acc 1
2016-09-06T15:30:06.169378: step 8533, loss 0.0561134, acc 0.98
2016-09-06T15:30:06.975161: step 8534, loss 0.0191703, acc 0.98
2016-09-06T15:30:07.778695: step 8535, loss 0.00672906, acc 1
2016-09-06T15:30:08.602601: step 8536, loss 0.0272278, acc 1
2016-09-06T15:30:09.457421: step 8537, loss 0.0144118, acc 1
2016-09-06T15:30:10.237236: step 8538, loss 0.00813781, acc 1
2016-09-06T15:30:11.041740: step 8539, loss 0.0062239, acc 1
2016-09-06T15:30:11.878083: step 8540, loss 0.00179608, acc 1
2016-09-06T15:30:12.684633: step 8541, loss 0.00352291, acc 1
2016-09-06T15:30:13.498276: step 8542, loss 0.0108697, acc 1
2016-09-06T15:30:14.357468: step 8543, loss 0.00585553, acc 1
2016-09-06T15:30:15.166712: step 8544, loss 0.00184668, acc 1
2016-09-06T15:30:15.987687: step 8545, loss 0.00935034, acc 1
2016-09-06T15:30:16.811651: step 8546, loss 0.0107772, acc 1
2016-09-06T15:30:17.636559: step 8547, loss 0.0183327, acc 0.98
2016-09-06T15:30:18.473271: step 8548, loss 0.0236562, acc 1
2016-09-06T15:30:19.297409: step 8549, loss 0.0159515, acc 0.98
2016-09-06T15:30:20.131659: step 8550, loss 0.00400576, acc 1
2016-09-06T15:30:20.933899: step 8551, loss 0.056502, acc 0.98
2016-09-06T15:30:21.773309: step 8552, loss 0.0020216, acc 1
2016-09-06T15:30:22.576060: step 8553, loss 0.0150622, acc 1
2016-09-06T15:30:23.415411: step 8554, loss 0.0491401, acc 0.98
2016-09-06T15:30:24.242751: step 8555, loss 0.0151194, acc 1
2016-09-06T15:30:25.052914: step 8556, loss 0.0467173, acc 0.96
2016-09-06T15:30:25.855193: step 8557, loss 0.0268959, acc 1
2016-09-06T15:30:26.680406: step 8558, loss 0.0500635, acc 0.98
2016-09-06T15:30:27.508268: step 8559, loss 0.00787645, acc 1
2016-09-06T15:30:28.281765: step 8560, loss 0.00384509, acc 1
2016-09-06T15:30:29.113831: step 8561, loss 0.00590382, acc 1
2016-09-06T15:30:29.935075: step 8562, loss 0.00280332, acc 1
2016-09-06T15:30:30.728078: step 8563, loss 0.00224043, acc 1
2016-09-06T15:30:31.540940: step 8564, loss 0.0458248, acc 0.98
2016-09-06T15:30:32.352081: step 8565, loss 0.0167575, acc 1
2016-09-06T15:30:33.135919: step 8566, loss 0.00215172, acc 1
2016-09-06T15:30:34.021272: step 8567, loss 0.0239351, acc 1
2016-09-06T15:30:34.834335: step 8568, loss 0.0593808, acc 0.96
2016-09-06T15:30:35.617407: step 8569, loss 0.0112944, acc 1
2016-09-06T15:30:36.391119: step 8570, loss 0.00784014, acc 1
2016-09-06T15:30:37.245400: step 8571, loss 0.0153687, acc 1
2016-09-06T15:30:38.064469: step 8572, loss 0.00373262, acc 1
2016-09-06T15:30:38.870808: step 8573, loss 0.0681705, acc 0.98
2016-09-06T15:30:39.715320: step 8574, loss 0.00433825, acc 1
2016-09-06T15:30:40.525687: step 8575, loss 0.00601363, acc 1
2016-09-06T15:30:41.327848: step 8576, loss 0.00382103, acc 1
2016-09-06T15:30:42.179970: step 8577, loss 0.00643716, acc 1
2016-09-06T15:30:42.987869: step 8578, loss 0.0632511, acc 0.98
2016-09-06T15:30:43.787352: step 8579, loss 0.0548849, acc 0.96
2016-09-06T15:30:44.611084: step 8580, loss 0.0140072, acc 1
2016-09-06T15:30:45.402323: step 8581, loss 0.0186039, acc 0.98
2016-09-06T15:30:46.235119: step 8582, loss 0.0192139, acc 0.98
2016-09-06T15:30:47.071177: step 8583, loss 0.0227294, acc 1
2016-09-06T15:30:47.888323: step 8584, loss 0.0304572, acc 0.98
2016-09-06T15:30:48.735656: step 8585, loss 0.0155058, acc 1
2016-09-06T15:30:49.561191: step 8586, loss 0.0286365, acc 1
2016-09-06T15:30:50.377002: step 8587, loss 0.00440494, acc 1
2016-09-06T15:30:51.181628: step 8588, loss 0.00586915, acc 1
2016-09-06T15:30:52.016041: step 8589, loss 0.103567, acc 0.96
2016-09-06T15:30:52.827436: step 8590, loss 0.0285762, acc 0.98
2016-09-06T15:30:53.620603: step 8591, loss 0.0392948, acc 0.98
2016-09-06T15:30:54.458838: step 8592, loss 0.0113273, acc 1
2016-09-06T15:30:55.258033: step 8593, loss 0.020399, acc 0.98
2016-09-06T15:30:56.071466: step 8594, loss 0.043015, acc 0.96
2016-09-06T15:30:56.897737: step 8595, loss 0.0457094, acc 0.98
2016-09-06T15:30:57.673401: step 8596, loss 0.015835, acc 1
2016-09-06T15:30:58.476092: step 8597, loss 0.0101193, acc 1
2016-09-06T15:30:59.313620: step 8598, loss 0.0504909, acc 0.96
2016-09-06T15:31:00.121183: step 8599, loss 0.00191232, acc 1
2016-09-06T15:31:00.964659: step 8600, loss 0.016669, acc 1

Evaluation:
2016-09-06T15:31:04.699772: step 8600, loss 1.74415, acc 0.746717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-8600

2016-09-06T15:31:06.614290: step 8601, loss 0.00496394, acc 1
2016-09-06T15:31:07.420176: step 8602, loss 0.00232856, acc 1
2016-09-06T15:31:08.233629: step 8603, loss 0.0387749, acc 0.98
2016-09-06T15:31:09.046818: step 8604, loss 0.00733594, acc 1
2016-09-06T15:31:09.860782: step 8605, loss 0.0114804, acc 1
2016-09-06T15:31:10.670462: step 8606, loss 0.0433472, acc 0.98
2016-09-06T15:31:11.472040: step 8607, loss 0.00211939, acc 1
2016-09-06T15:31:12.301394: step 8608, loss 0.0294917, acc 1
2016-09-06T15:31:13.092686: step 8609, loss 0.0461967, acc 0.98
2016-09-06T15:31:13.930953: step 8610, loss 0.0131578, acc 1
2016-09-06T15:31:14.742342: step 8611, loss 0.0236132, acc 1
2016-09-06T15:31:15.509716: step 8612, loss 0.00201384, acc 1
2016-09-06T15:31:16.307239: step 8613, loss 0.00918234, acc 1
2016-09-06T15:31:17.108291: step 8614, loss 0.0243761, acc 0.98
2016-09-06T15:31:17.885445: step 8615, loss 0.0219699, acc 0.98
2016-09-06T15:31:18.702118: step 8616, loss 0.00454901, acc 1
2016-09-06T15:31:19.523676: step 8617, loss 0.136852, acc 0.96
2016-09-06T15:31:20.314363: step 8618, loss 0.0101823, acc 1
2016-09-06T15:31:21.106054: step 8619, loss 0.0020867, acc 1
2016-09-06T15:31:21.938657: step 8620, loss 0.00220234, acc 1
2016-09-06T15:31:22.733524: step 8621, loss 0.00924301, acc 1
2016-09-06T15:31:23.537372: step 8622, loss 0.00334925, acc 1
2016-09-06T15:31:24.369236: step 8623, loss 0.0381994, acc 0.98
2016-09-06T15:31:25.180016: step 8624, loss 0.00205838, acc 1
2016-09-06T15:31:25.985017: step 8625, loss 0.0493927, acc 0.98
2016-09-06T15:31:26.819065: step 8626, loss 0.035416, acc 1
2016-09-06T15:31:27.616796: step 8627, loss 0.0039727, acc 1
2016-09-06T15:31:28.439814: step 8628, loss 0.0194671, acc 1
2016-09-06T15:31:29.252086: step 8629, loss 0.0123722, acc 1
2016-09-06T15:31:30.024462: step 8630, loss 0.00279332, acc 1
2016-09-06T15:31:30.826614: step 8631, loss 0.00552149, acc 1
2016-09-06T15:31:31.668109: step 8632, loss 0.0264507, acc 0.98
2016-09-06T15:31:32.503462: step 8633, loss 0.00405739, acc 1
2016-09-06T15:31:33.309336: step 8634, loss 0.0242784, acc 1
2016-09-06T15:31:34.149824: step 8635, loss 0.0258958, acc 0.98
2016-09-06T15:31:34.951137: step 8636, loss 0.00428379, acc 1
2016-09-06T15:31:35.763553: step 8637, loss 0.0665338, acc 0.96
2016-09-06T15:31:36.606286: step 8638, loss 0.0232487, acc 1
2016-09-06T15:31:37.414785: step 8639, loss 0.00791496, acc 1
2016-09-06T15:31:38.157862: step 8640, loss 0.00234911, acc 1
2016-09-06T15:31:39.009352: step 8641, loss 0.00283218, acc 1
2016-09-06T15:31:39.824618: step 8642, loss 0.0188171, acc 0.98
2016-09-06T15:31:40.626892: step 8643, loss 0.00753434, acc 1
2016-09-06T15:31:41.455988: step 8644, loss 0.00251711, acc 1
2016-09-06T15:31:42.253019: step 8645, loss 0.0167421, acc 1
2016-09-06T15:31:43.079540: step 8646, loss 0.00858033, acc 1
2016-09-06T15:31:43.917888: step 8647, loss 0.00252188, acc 1
2016-09-06T15:31:44.744351: step 8648, loss 0.0174612, acc 1
2016-09-06T15:31:45.548301: step 8649, loss 0.0246858, acc 0.98
2016-09-06T15:31:46.386434: step 8650, loss 0.015097, acc 1
2016-09-06T15:31:47.227899: step 8651, loss 0.00393986, acc 1
2016-09-06T15:31:48.054862: step 8652, loss 0.00247928, acc 1
2016-09-06T15:31:48.917852: step 8653, loss 0.0190214, acc 0.98
2016-09-06T15:31:49.749751: step 8654, loss 0.0235232, acc 0.98
2016-09-06T15:31:50.527477: step 8655, loss 0.0458923, acc 0.98
2016-09-06T15:31:51.355990: step 8656, loss 0.0630908, acc 0.96
2016-09-06T15:31:52.158410: step 8657, loss 0.00756192, acc 1
2016-09-06T15:31:52.952789: step 8658, loss 0.0175122, acc 1
2016-09-06T15:31:53.750220: step 8659, loss 0.0219299, acc 0.98
2016-09-06T15:31:54.592403: step 8660, loss 0.00297801, acc 1
2016-09-06T15:31:55.403130: step 8661, loss 0.00887145, acc 1
2016-09-06T15:31:56.224624: step 8662, loss 0.0057366, acc 1
2016-09-06T15:31:57.065794: step 8663, loss 0.0052105, acc 1
2016-09-06T15:31:57.904012: step 8664, loss 0.0460249, acc 0.98
2016-09-06T15:31:58.693012: step 8665, loss 0.00443977, acc 1
2016-09-06T15:31:59.497228: step 8666, loss 0.0144844, acc 1
2016-09-06T15:32:00.279555: step 8667, loss 0.00250652, acc 1
2016-09-06T15:32:01.111482: step 8668, loss 0.00435407, acc 1
2016-09-06T15:32:01.957353: step 8669, loss 0.0158672, acc 1
2016-09-06T15:32:02.771125: step 8670, loss 0.037276, acc 0.98
2016-09-06T15:32:03.611050: step 8671, loss 0.13331, acc 0.98
2016-09-06T15:32:04.437756: step 8672, loss 0.050051, acc 0.98
2016-09-06T15:32:05.295281: step 8673, loss 0.107691, acc 0.98
2016-09-06T15:32:06.137680: step 8674, loss 0.0263065, acc 0.98
2016-09-06T15:32:06.966837: step 8675, loss 0.126305, acc 0.96
2016-09-06T15:32:07.792239: step 8676, loss 0.00189319, acc 1
2016-09-06T15:32:08.613540: step 8677, loss 0.0424195, acc 0.98
2016-09-06T15:32:09.463568: step 8678, loss 0.0271793, acc 0.98
2016-09-06T15:32:10.268648: step 8679, loss 0.00966384, acc 1
2016-09-06T15:32:11.053643: step 8680, loss 0.0046315, acc 1
2016-09-06T15:32:11.864276: step 8681, loss 0.0172447, acc 0.98
2016-09-06T15:32:12.684155: step 8682, loss 0.00465018, acc 1
2016-09-06T15:32:13.506022: step 8683, loss 0.0286591, acc 0.98
2016-09-06T15:32:14.339515: step 8684, loss 0.0321028, acc 0.98
2016-09-06T15:32:15.152667: step 8685, loss 0.025588, acc 0.98
2016-09-06T15:32:15.956071: step 8686, loss 0.128704, acc 0.94
2016-09-06T15:32:16.733907: step 8687, loss 0.0247861, acc 1
2016-09-06T15:32:17.552112: step 8688, loss 0.0118983, acc 1
2016-09-06T15:32:18.329783: step 8689, loss 0.00931004, acc 1
2016-09-06T15:32:19.146811: step 8690, loss 0.0753142, acc 0.98
2016-09-06T15:32:19.962198: step 8691, loss 0.00399584, acc 1
2016-09-06T15:32:20.756194: step 8692, loss 0.0190245, acc 0.98
2016-09-06T15:32:21.569903: step 8693, loss 0.00425778, acc 1
2016-09-06T15:32:22.425353: step 8694, loss 0.00687676, acc 1
2016-09-06T15:32:23.208606: step 8695, loss 0.00712296, acc 1
2016-09-06T15:32:24.016465: step 8696, loss 0.0241557, acc 0.98
2016-09-06T15:32:24.864777: step 8697, loss 0.0144313, acc 1
2016-09-06T15:32:25.657934: step 8698, loss 0.00208468, acc 1
2016-09-06T15:32:26.430238: step 8699, loss 0.0164342, acc 1
2016-09-06T15:32:27.264354: step 8700, loss 0.0208312, acc 0.98

Evaluation:
2016-09-06T15:32:30.957716: step 8700, loss 1.65585, acc 0.746717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-8700

2016-09-06T15:32:32.879511: step 8701, loss 0.00503438, acc 1
2016-09-06T15:32:33.713577: step 8702, loss 0.0428302, acc 0.98
2016-09-06T15:32:34.545150: step 8703, loss 0.00534456, acc 1
2016-09-06T15:32:35.336272: step 8704, loss 0.0286797, acc 0.98
2016-09-06T15:32:36.167723: step 8705, loss 0.0283605, acc 0.98
2016-09-06T15:32:37.001449: step 8706, loss 0.0132625, acc 1
2016-09-06T15:32:37.801706: step 8707, loss 0.00640509, acc 1
2016-09-06T15:32:38.589630: step 8708, loss 0.035015, acc 1
2016-09-06T15:32:39.408484: step 8709, loss 0.0857037, acc 0.98
2016-09-06T15:32:40.221639: step 8710, loss 0.00701705, acc 1
2016-09-06T15:32:41.020779: step 8711, loss 0.0317414, acc 1
2016-09-06T15:32:41.819903: step 8712, loss 0.00767525, acc 1
2016-09-06T15:32:42.598250: step 8713, loss 0.0382908, acc 1
2016-09-06T15:32:43.414597: step 8714, loss 0.00303862, acc 1
2016-09-06T15:32:44.243719: step 8715, loss 0.0304093, acc 0.98
2016-09-06T15:32:45.060262: step 8716, loss 0.0166062, acc 0.98
2016-09-06T15:32:45.892081: step 8717, loss 0.00241959, acc 1
2016-09-06T15:32:46.749784: step 8718, loss 0.00751165, acc 1
2016-09-06T15:32:47.573483: step 8719, loss 0.0654851, acc 0.96
2016-09-06T15:32:48.392695: step 8720, loss 0.00811428, acc 1
2016-09-06T15:32:49.211685: step 8721, loss 0.00658864, acc 1
2016-09-06T15:32:50.018956: step 8722, loss 0.0988209, acc 0.96
2016-09-06T15:32:50.827744: step 8723, loss 0.00510117, acc 1
2016-09-06T15:32:51.664096: step 8724, loss 0.00390217, acc 1
2016-09-06T15:32:52.474498: step 8725, loss 0.0200778, acc 1
2016-09-06T15:32:53.274532: step 8726, loss 0.0107791, acc 1
2016-09-06T15:32:54.106125: step 8727, loss 0.00235454, acc 1
2016-09-06T15:32:54.919038: step 8728, loss 0.0174105, acc 0.98
2016-09-06T15:32:55.730474: step 8729, loss 0.0444261, acc 0.98
2016-09-06T15:32:56.564877: step 8730, loss 0.0123055, acc 1
2016-09-06T15:32:57.393873: step 8731, loss 0.00733041, acc 1
2016-09-06T15:32:58.216403: step 8732, loss 0.0135563, acc 1
2016-09-06T15:32:59.049929: step 8733, loss 0.00490005, acc 1
2016-09-06T15:32:59.868049: step 8734, loss 0.0134882, acc 1
2016-09-06T15:33:00.695421: step 8735, loss 0.0164706, acc 1
2016-09-06T15:33:01.521385: step 8736, loss 0.00233844, acc 1
2016-09-06T15:33:02.345703: step 8737, loss 0.00328391, acc 1
2016-09-06T15:33:03.116052: step 8738, loss 0.0217411, acc 0.98
2016-09-06T15:33:03.912688: step 8739, loss 0.0188936, acc 1
2016-09-06T15:33:04.727198: step 8740, loss 0.027341, acc 1
2016-09-06T15:33:05.519614: step 8741, loss 0.012778, acc 1
2016-09-06T15:33:06.308929: step 8742, loss 0.0184794, acc 1
2016-09-06T15:33:07.134981: step 8743, loss 0.00258147, acc 1
2016-09-06T15:33:07.922635: step 8744, loss 0.0189486, acc 0.98
2016-09-06T15:33:08.720651: step 8745, loss 0.0146594, acc 1
2016-09-06T15:33:09.523127: step 8746, loss 0.0170054, acc 0.98
2016-09-06T15:33:10.332842: step 8747, loss 0.107685, acc 0.96
2016-09-06T15:33:11.137806: step 8748, loss 0.0413704, acc 0.98
2016-09-06T15:33:11.964964: step 8749, loss 0.0144775, acc 1
2016-09-06T15:33:12.764260: step 8750, loss 0.054445, acc 0.96
2016-09-06T15:33:13.552445: step 8751, loss 0.00884601, acc 1
2016-09-06T15:33:14.371049: step 8752, loss 0.0495847, acc 0.96
2016-09-06T15:33:15.186937: step 8753, loss 0.0279923, acc 1
2016-09-06T15:33:15.989323: step 8754, loss 0.0189792, acc 0.98
2016-09-06T15:33:16.794491: step 8755, loss 0.0183415, acc 0.98
2016-09-06T15:33:17.603337: step 8756, loss 0.00419734, acc 1
2016-09-06T15:33:18.397768: step 8757, loss 0.0898798, acc 0.96
2016-09-06T15:33:19.224437: step 8758, loss 0.00944962, acc 1
2016-09-06T15:33:20.025678: step 8759, loss 0.00245498, acc 1
2016-09-06T15:33:20.835622: step 8760, loss 0.0489816, acc 0.96
2016-09-06T15:33:21.670519: step 8761, loss 0.00268208, acc 1
2016-09-06T15:33:22.443667: step 8762, loss 0.0258103, acc 0.98
2016-09-06T15:33:23.243563: step 8763, loss 0.0375316, acc 0.98
2016-09-06T15:33:24.067383: step 8764, loss 0.00593521, acc 1
2016-09-06T15:33:24.856801: step 8765, loss 0.0214359, acc 1
2016-09-06T15:33:25.644338: step 8766, loss 0.0170501, acc 1
2016-09-06T15:33:26.454106: step 8767, loss 0.0525984, acc 0.96
2016-09-06T15:33:27.265656: step 8768, loss 0.0145318, acc 1
2016-09-06T15:33:28.077532: step 8769, loss 0.0262873, acc 0.98
2016-09-06T15:33:28.885149: step 8770, loss 0.0138985, acc 1
2016-09-06T15:33:29.669707: step 8771, loss 0.0355879, acc 0.98
2016-09-06T15:33:30.484953: step 8772, loss 0.0224282, acc 0.98
2016-09-06T15:33:31.307592: step 8773, loss 0.0196542, acc 1
2016-09-06T15:33:32.114903: step 8774, loss 0.011485, acc 1
2016-09-06T15:33:32.910274: step 8775, loss 0.0129139, acc 1
2016-09-06T15:33:33.724176: step 8776, loss 0.0125995, acc 1
2016-09-06T15:33:34.515740: step 8777, loss 0.0163704, acc 1
2016-09-06T15:33:35.318968: step 8778, loss 0.092325, acc 0.96
2016-09-06T15:33:36.127595: step 8779, loss 0.00216607, acc 1
2016-09-06T15:33:36.921681: step 8780, loss 0.00497017, acc 1
2016-09-06T15:33:37.752622: step 8781, loss 0.00522258, acc 1
2016-09-06T15:33:38.564572: step 8782, loss 0.0399718, acc 0.98
2016-09-06T15:33:39.357498: step 8783, loss 0.00234881, acc 1
2016-09-06T15:33:40.159762: step 8784, loss 0.0142541, acc 1
2016-09-06T15:33:40.975217: step 8785, loss 0.0150809, acc 1
2016-09-06T15:33:41.751972: step 8786, loss 0.0641126, acc 0.96
2016-09-06T15:33:42.586615: step 8787, loss 0.0170615, acc 1
2016-09-06T15:33:43.408150: step 8788, loss 0.00240627, acc 1
2016-09-06T15:33:44.198560: step 8789, loss 0.0200288, acc 0.98
2016-09-06T15:33:45.004988: step 8790, loss 0.0276313, acc 0.98
2016-09-06T15:33:45.820648: step 8791, loss 0.0364153, acc 0.98
2016-09-06T15:33:46.639968: step 8792, loss 0.00328779, acc 1
2016-09-06T15:33:47.467356: step 8793, loss 0.013126, acc 1
2016-09-06T15:33:48.279159: step 8794, loss 0.00231335, acc 1
2016-09-06T15:33:49.054895: step 8795, loss 0.0143152, acc 1
2016-09-06T15:33:49.864495: step 8796, loss 0.00963495, acc 1
2016-09-06T15:33:50.693946: step 8797, loss 0.0124607, acc 1
2016-09-06T15:33:51.474386: step 8798, loss 0.00467121, acc 1
2016-09-06T15:33:52.296421: step 8799, loss 0.0144636, acc 1
2016-09-06T15:33:53.118226: step 8800, loss 0.00622967, acc 1

Evaluation:
2016-09-06T15:33:56.805578: step 8800, loss 1.79185, acc 0.745779

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-8800

2016-09-06T15:33:58.724796: step 8801, loss 0.0629573, acc 0.98
2016-09-06T15:33:59.550950: step 8802, loss 0.0169552, acc 0.98
2016-09-06T15:34:00.387630: step 8803, loss 0.00685831, acc 1
2016-09-06T15:34:01.168414: step 8804, loss 0.0197263, acc 1
2016-09-06T15:34:01.970152: step 8805, loss 0.00233779, acc 1
2016-09-06T15:34:02.805988: step 8806, loss 0.0254103, acc 1
2016-09-06T15:34:03.592771: step 8807, loss 0.0281149, acc 0.98
2016-09-06T15:34:04.388662: step 8808, loss 0.0223278, acc 1
2016-09-06T15:34:05.178965: step 8809, loss 0.0573396, acc 0.96
2016-09-06T15:34:05.976813: step 8810, loss 0.00673346, acc 1
2016-09-06T15:34:06.786516: step 8811, loss 0.0326691, acc 0.98
2016-09-06T15:34:07.603397: step 8812, loss 0.0227828, acc 1
2016-09-06T15:34:08.389523: step 8813, loss 0.017444, acc 0.98
2016-09-06T15:34:09.224901: step 8814, loss 0.025791, acc 1
2016-09-06T15:34:10.037443: step 8815, loss 0.0300299, acc 1
2016-09-06T15:34:10.812544: step 8816, loss 0.00223601, acc 1
2016-09-06T15:34:11.615710: step 8817, loss 0.0191776, acc 1
2016-09-06T15:34:12.456406: step 8818, loss 0.021821, acc 0.98
2016-09-06T15:34:13.260868: step 8819, loss 0.00774148, acc 1
2016-09-06T15:34:14.069623: step 8820, loss 0.00651264, acc 1
2016-09-06T15:34:14.884140: step 8821, loss 0.00612715, acc 1
2016-09-06T15:34:15.660068: step 8822, loss 0.00321885, acc 1
2016-09-06T15:34:16.437077: step 8823, loss 0.00277043, acc 1
2016-09-06T15:34:17.294556: step 8824, loss 0.00427707, acc 1
2016-09-06T15:34:18.109876: step 8825, loss 0.00362459, acc 1
2016-09-06T15:34:18.902722: step 8826, loss 0.0315271, acc 0.98
2016-09-06T15:34:19.743914: step 8827, loss 0.0303691, acc 1
2016-09-06T15:34:20.560131: step 8828, loss 0.00249691, acc 1
2016-09-06T15:34:21.360005: step 8829, loss 0.0655557, acc 0.98
2016-09-06T15:34:22.187074: step 8830, loss 0.0260676, acc 0.98
2016-09-06T15:34:23.009517: step 8831, loss 0.064197, acc 0.98
2016-09-06T15:34:23.759404: step 8832, loss 0.00242569, acc 1
2016-09-06T15:34:24.584250: step 8833, loss 0.00403572, acc 1
2016-09-06T15:34:25.377465: step 8834, loss 0.0181644, acc 0.98
2016-09-06T15:34:26.188955: step 8835, loss 0.0226724, acc 0.98
2016-09-06T15:34:27.008452: step 8836, loss 0.00676409, acc 1
2016-09-06T15:34:27.818296: step 8837, loss 0.00398109, acc 1
2016-09-06T15:34:28.628664: step 8838, loss 0.00416502, acc 1
2016-09-06T15:34:29.476290: step 8839, loss 0.133199, acc 0.98
2016-09-06T15:34:30.312439: step 8840, loss 0.0211029, acc 1
2016-09-06T15:34:31.130294: step 8841, loss 0.0348446, acc 0.98
2016-09-06T15:34:31.976609: step 8842, loss 0.0257617, acc 1
2016-09-06T15:34:32.779147: step 8843, loss 0.00747483, acc 1
2016-09-06T15:34:33.600596: step 8844, loss 0.00233248, acc 1
2016-09-06T15:34:34.438288: step 8845, loss 0.0386927, acc 0.96
2016-09-06T15:34:35.251730: step 8846, loss 0.0180335, acc 0.98
2016-09-06T15:34:36.075651: step 8847, loss 0.0255364, acc 1
2016-09-06T15:34:36.934831: step 8848, loss 0.00336735, acc 1
2016-09-06T15:34:37.737428: step 8849, loss 0.00974734, acc 1
2016-09-06T15:34:38.537824: step 8850, loss 0.00210534, acc 1
2016-09-06T15:34:39.358263: step 8851, loss 0.0111284, acc 1
2016-09-06T15:34:40.148124: step 8852, loss 0.00206984, acc 1
2016-09-06T15:34:40.948432: step 8853, loss 0.0129892, acc 1
2016-09-06T15:34:41.805913: step 8854, loss 0.00354846, acc 1
2016-09-06T15:34:42.648140: step 8855, loss 0.0699728, acc 0.96
2016-09-06T15:34:43.448344: step 8856, loss 0.0379164, acc 0.98
2016-09-06T15:34:44.266837: step 8857, loss 0.00915575, acc 1
2016-09-06T15:34:45.079247: step 8858, loss 0.0245795, acc 0.98
2016-09-06T15:34:45.878910: step 8859, loss 0.0280755, acc 0.98
2016-09-06T15:34:46.665398: step 8860, loss 0.0021231, acc 1
2016-09-06T15:34:47.463083: step 8861, loss 0.0131276, acc 1
2016-09-06T15:34:48.264877: step 8862, loss 0.0242979, acc 0.98
2016-09-06T15:34:49.078266: step 8863, loss 0.0081635, acc 1
2016-09-06T15:34:49.887661: step 8864, loss 0.0184715, acc 1
2016-09-06T15:34:50.677403: step 8865, loss 0.00797432, acc 1
2016-09-06T15:34:51.497388: step 8866, loss 0.00370973, acc 1
2016-09-06T15:34:52.323815: step 8867, loss 0.0144058, acc 1
2016-09-06T15:34:53.120540: step 8868, loss 0.00241727, acc 1
2016-09-06T15:34:53.962517: step 8869, loss 0.0275599, acc 1
2016-09-06T15:34:54.797105: step 8870, loss 0.00233514, acc 1
2016-09-06T15:34:55.585111: step 8871, loss 0.012468, acc 1
2016-09-06T15:34:56.400939: step 8872, loss 0.133999, acc 0.98
2016-09-06T15:34:57.245489: step 8873, loss 0.0369562, acc 0.98
2016-09-06T15:34:58.082016: step 8874, loss 0.00272149, acc 1
2016-09-06T15:34:58.906316: step 8875, loss 0.0264343, acc 1
2016-09-06T15:34:59.735758: step 8876, loss 0.0449809, acc 0.98
2016-09-06T15:35:00.629269: step 8877, loss 0.00864093, acc 1
2016-09-06T15:35:01.437682: step 8878, loss 0.00366537, acc 1
2016-09-06T15:35:02.239073: step 8879, loss 0.0482113, acc 0.98
2016-09-06T15:35:03.032405: step 8880, loss 0.0271369, acc 1
2016-09-06T15:35:03.857794: step 8881, loss 0.0277285, acc 0.98
2016-09-06T15:35:04.681781: step 8882, loss 0.0183921, acc 0.98
2016-09-06T15:35:05.498647: step 8883, loss 0.0177013, acc 1
2016-09-06T15:35:06.306646: step 8884, loss 0.00853668, acc 1
2016-09-06T15:35:07.131054: step 8885, loss 0.00508449, acc 1
2016-09-06T15:35:07.982687: step 8886, loss 0.0108143, acc 1
2016-09-06T15:35:08.790873: step 8887, loss 0.0160798, acc 0.98
2016-09-06T15:35:09.616141: step 8888, loss 0.0293561, acc 1
2016-09-06T15:35:10.465216: step 8889, loss 0.0473946, acc 0.96
2016-09-06T15:35:11.243599: step 8890, loss 0.0217965, acc 0.98
2016-09-06T15:35:12.040844: step 8891, loss 0.0118799, acc 1
2016-09-06T15:35:12.854553: step 8892, loss 0.0298933, acc 0.98
2016-09-06T15:35:13.647042: step 8893, loss 0.00245966, acc 1
2016-09-06T15:35:14.465511: step 8894, loss 0.0087189, acc 1
2016-09-06T15:35:15.268327: step 8895, loss 0.00402353, acc 1
2016-09-06T15:35:16.048455: step 8896, loss 0.0270683, acc 1
2016-09-06T15:35:16.905807: step 8897, loss 0.0214831, acc 0.98
2016-09-06T15:35:17.759781: step 8898, loss 0.117749, acc 0.96
2016-09-06T15:35:18.530212: step 8899, loss 0.0125841, acc 1
2016-09-06T15:35:19.325802: step 8900, loss 0.0550349, acc 0.96

Evaluation:
2016-09-06T15:35:23.065817: step 8900, loss 2.01887, acc 0.741088

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-8900

2016-09-06T15:35:25.077520: step 8901, loss 0.0671671, acc 0.96
2016-09-06T15:35:25.891903: step 8902, loss 0.0289475, acc 0.98
2016-09-06T15:35:26.734373: step 8903, loss 0.00300396, acc 1
2016-09-06T15:35:27.553629: step 8904, loss 0.0107647, acc 1
2016-09-06T15:35:28.354767: step 8905, loss 0.0356741, acc 1
2016-09-06T15:35:29.180575: step 8906, loss 0.00239353, acc 1
2016-09-06T15:35:30.014142: step 8907, loss 0.0425067, acc 0.96
2016-09-06T15:35:30.840448: step 8908, loss 0.015597, acc 1
2016-09-06T15:35:31.685750: step 8909, loss 0.0765211, acc 0.98
2016-09-06T15:35:32.509069: step 8910, loss 0.0804581, acc 0.96
2016-09-06T15:35:33.332167: step 8911, loss 0.0191674, acc 1
2016-09-06T15:35:34.142793: step 8912, loss 0.0118132, acc 1
2016-09-06T15:35:34.976024: step 8913, loss 0.00927103, acc 1
2016-09-06T15:35:35.805334: step 8914, loss 0.0166948, acc 0.98
2016-09-06T15:35:36.613077: step 8915, loss 0.00681253, acc 1
2016-09-06T15:35:37.456166: step 8916, loss 0.00238556, acc 1
2016-09-06T15:35:38.270898: step 8917, loss 0.100153, acc 0.98
2016-09-06T15:35:39.069444: step 8918, loss 0.0219433, acc 1
2016-09-06T15:35:39.873959: step 8919, loss 0.0328969, acc 0.98
2016-09-06T15:35:40.664470: step 8920, loss 0.00441624, acc 1
2016-09-06T15:35:41.452300: step 8921, loss 0.0122462, acc 1
2016-09-06T15:35:42.279766: step 8922, loss 0.00251098, acc 1
2016-09-06T15:35:43.081818: step 8923, loss 0.0390429, acc 0.98
2016-09-06T15:35:43.865518: step 8924, loss 0.00473088, acc 1
2016-09-06T15:35:44.677597: step 8925, loss 0.030295, acc 1
2016-09-06T15:35:45.484445: step 8926, loss 0.0143275, acc 1
2016-09-06T15:35:46.303098: step 8927, loss 0.0306995, acc 1
2016-09-06T15:35:47.147481: step 8928, loss 0.00222679, acc 1
2016-09-06T15:35:48.005292: step 8929, loss 0.00328026, acc 1
2016-09-06T15:35:48.805903: step 8930, loss 0.0240835, acc 1
2016-09-06T15:35:49.626264: step 8931, loss 0.00616881, acc 1
2016-09-06T15:35:50.437304: step 8932, loss 0.0314779, acc 0.98
2016-09-06T15:35:51.227900: step 8933, loss 0.042324, acc 0.98
2016-09-06T15:35:52.021054: step 8934, loss 0.00228797, acc 1
2016-09-06T15:35:52.842024: step 8935, loss 0.00774373, acc 1
2016-09-06T15:35:53.612797: step 8936, loss 0.00338717, acc 1
2016-09-06T15:35:54.411244: step 8937, loss 0.022502, acc 1
2016-09-06T15:35:55.259935: step 8938, loss 0.00270931, acc 1
2016-09-06T15:35:56.038941: step 8939, loss 0.051196, acc 0.98
2016-09-06T15:35:56.852231: step 8940, loss 0.0182565, acc 0.98
2016-09-06T15:35:57.679888: step 8941, loss 0.0345504, acc 0.98
2016-09-06T15:35:58.472669: step 8942, loss 0.0189378, acc 0.98
2016-09-06T15:35:59.256560: step 8943, loss 0.00345903, acc 1
2016-09-06T15:36:00.095507: step 8944, loss 0.0273439, acc 1
2016-09-06T15:36:00.909587: step 8945, loss 0.0170482, acc 1
2016-09-06T15:36:01.724787: step 8946, loss 0.00416038, acc 1
2016-09-06T15:36:02.550165: step 8947, loss 0.0228212, acc 0.98
2016-09-06T15:36:03.352455: step 8948, loss 0.00375966, acc 1
2016-09-06T15:36:04.166438: step 8949, loss 0.0501036, acc 0.98
2016-09-06T15:36:04.990113: step 8950, loss 0.00230082, acc 1
2016-09-06T15:36:05.793370: step 8951, loss 0.0206714, acc 0.98
2016-09-06T15:36:06.629382: step 8952, loss 0.00798637, acc 1
2016-09-06T15:36:07.448826: step 8953, loss 0.00276907, acc 1
2016-09-06T15:36:08.236239: step 8954, loss 0.00266502, acc 1
2016-09-06T15:36:09.023035: step 8955, loss 0.026535, acc 0.98
2016-09-06T15:36:09.859138: step 8956, loss 0.0174263, acc 1
2016-09-06T15:36:10.660752: step 8957, loss 0.0564463, acc 0.98
2016-09-06T15:36:11.453448: step 8958, loss 0.0477942, acc 0.98
2016-09-06T15:36:12.275018: step 8959, loss 0.012438, acc 1
2016-09-06T15:36:13.077440: step 8960, loss 0.0164345, acc 1
2016-09-06T15:36:13.896830: step 8961, loss 0.0225456, acc 0.98
2016-09-06T15:36:14.713603: step 8962, loss 0.00424048, acc 1
2016-09-06T15:36:15.506646: step 8963, loss 0.00264541, acc 1
2016-09-06T15:36:16.316152: step 8964, loss 0.0110998, acc 1
2016-09-06T15:36:17.155787: step 8965, loss 0.00400381, acc 1
2016-09-06T15:36:17.971771: step 8966, loss 0.0443297, acc 0.98
2016-09-06T15:36:18.773387: step 8967, loss 0.00914069, acc 1
2016-09-06T15:36:19.612318: step 8968, loss 0.00246359, acc 1
2016-09-06T15:36:20.434553: step 8969, loss 0.00399263, acc 1
2016-09-06T15:36:21.236363: step 8970, loss 0.0648563, acc 0.96
2016-09-06T15:36:22.072683: step 8971, loss 0.00210384, acc 1
2016-09-06T15:36:22.885878: step 8972, loss 0.00211741, acc 1
2016-09-06T15:36:23.706943: step 8973, loss 0.0327095, acc 0.96
2016-09-06T15:36:24.549738: step 8974, loss 0.010307, acc 1
2016-09-06T15:36:25.372174: step 8975, loss 0.00342872, acc 1
2016-09-06T15:36:26.171773: step 8976, loss 0.0361382, acc 1
2016-09-06T15:36:26.990670: step 8977, loss 0.0342553, acc 0.98
2016-09-06T15:36:27.817451: step 8978, loss 0.00854626, acc 1
2016-09-06T15:36:28.622267: step 8979, loss 0.0163234, acc 1
2016-09-06T15:36:29.456960: step 8980, loss 0.0149369, acc 1
2016-09-06T15:36:30.280450: step 8981, loss 0.0158827, acc 1
2016-09-06T15:36:31.074822: step 8982, loss 0.0166817, acc 1
2016-09-06T15:36:31.918372: step 8983, loss 0.0156015, acc 1
2016-09-06T15:36:32.766148: step 8984, loss 0.00196269, acc 1
2016-09-06T15:36:33.567695: step 8985, loss 0.0679704, acc 0.96
2016-09-06T15:36:34.394436: step 8986, loss 0.0150325, acc 1
2016-09-06T15:36:35.249904: step 8987, loss 0.0279149, acc 0.98
2016-09-06T15:36:36.066428: step 8988, loss 0.0032095, acc 1
2016-09-06T15:36:36.864544: step 8989, loss 0.074523, acc 0.98
2016-09-06T15:36:37.682158: step 8990, loss 0.00611439, acc 1
2016-09-06T15:36:38.489601: step 8991, loss 0.00627695, acc 1
2016-09-06T15:36:39.291173: step 8992, loss 0.0232659, acc 1
2016-09-06T15:36:40.109317: step 8993, loss 0.0473842, acc 1
2016-09-06T15:36:40.907617: step 8994, loss 0.0383934, acc 0.98
2016-09-06T15:36:41.756298: step 8995, loss 0.0383723, acc 0.98
2016-09-06T15:36:42.609346: step 8996, loss 0.0343866, acc 0.98
2016-09-06T15:36:43.422384: step 8997, loss 0.00213115, acc 1
2016-09-06T15:36:44.226551: step 8998, loss 0.0447574, acc 0.96
2016-09-06T15:36:45.055215: step 8999, loss 0.0276191, acc 0.98
2016-09-06T15:36:45.849438: step 9000, loss 0.0253792, acc 0.98

Evaluation:
2016-09-06T15:36:49.552618: step 9000, loss 1.65335, acc 0.739212

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-9000

2016-09-06T15:36:51.415023: step 9001, loss 0.11081, acc 0.92
2016-09-06T15:36:52.265017: step 9002, loss 0.0259232, acc 0.98
2016-09-06T15:36:53.066176: step 9003, loss 0.0310855, acc 0.98
2016-09-06T15:36:53.902416: step 9004, loss 0.00250753, acc 1
2016-09-06T15:36:54.727762: step 9005, loss 0.05566, acc 0.96
2016-09-06T15:36:55.536816: step 9006, loss 0.0114246, acc 1
2016-09-06T15:36:56.351615: step 9007, loss 0.0468575, acc 0.98
2016-09-06T15:36:57.224344: step 9008, loss 0.00592631, acc 1
2016-09-06T15:36:58.034819: step 9009, loss 0.0524405, acc 0.96
2016-09-06T15:36:58.852978: step 9010, loss 0.0217549, acc 1
2016-09-06T15:36:59.669596: step 9011, loss 0.0244069, acc 1
2016-09-06T15:37:00.515164: step 9012, loss 0.0215045, acc 1
2016-09-06T15:37:01.327873: step 9013, loss 0.00946837, acc 1
2016-09-06T15:37:02.145806: step 9014, loss 0.00846523, acc 1
2016-09-06T15:37:02.972571: step 9015, loss 0.0023158, acc 1
2016-09-06T15:37:03.761031: step 9016, loss 0.00293713, acc 1
2016-09-06T15:37:04.567331: step 9017, loss 0.0140138, acc 1
2016-09-06T15:37:05.407057: step 9018, loss 0.0155406, acc 1
2016-09-06T15:37:06.191917: step 9019, loss 0.0114637, acc 1
2016-09-06T15:37:07.002339: step 9020, loss 0.00263555, acc 1
2016-09-06T15:37:07.785896: step 9021, loss 0.0516106, acc 0.98
2016-09-06T15:37:08.582892: step 9022, loss 0.0102155, acc 1
2016-09-06T15:37:09.376135: step 9023, loss 0.00344299, acc 1
2016-09-06T15:37:10.130098: step 9024, loss 0.00470274, acc 1
2016-09-06T15:37:10.933571: step 9025, loss 0.00312639, acc 1
2016-09-06T15:37:11.761526: step 9026, loss 0.00853639, acc 1
2016-09-06T15:37:12.593022: step 9027, loss 0.0107879, acc 1
2016-09-06T15:37:13.378189: step 9028, loss 0.00940786, acc 1
2016-09-06T15:37:14.176352: step 9029, loss 0.017625, acc 0.98
2016-09-06T15:37:15.003084: step 9030, loss 0.042327, acc 0.98
2016-09-06T15:37:15.803226: step 9031, loss 0.0343706, acc 0.98
2016-09-06T15:37:16.620815: step 9032, loss 0.0143188, acc 1
2016-09-06T15:37:17.440897: step 9033, loss 0.031908, acc 0.98
2016-09-06T15:37:18.239742: step 9034, loss 0.0297334, acc 0.98
2016-09-06T15:37:19.078270: step 9035, loss 0.00244612, acc 1
2016-09-06T15:37:19.884744: step 9036, loss 0.0172038, acc 1
2016-09-06T15:37:20.660297: step 9037, loss 0.0273161, acc 0.98
2016-09-06T15:37:21.489485: step 9038, loss 0.0175324, acc 1
2016-09-06T15:37:22.312257: step 9039, loss 0.00627048, acc 1
2016-09-06T15:37:23.071197: step 9040, loss 0.00397716, acc 1
2016-09-06T15:37:23.878426: step 9041, loss 0.0167906, acc 1
2016-09-06T15:37:24.694095: step 9042, loss 0.0169887, acc 1
2016-09-06T15:37:25.488296: step 9043, loss 0.00336146, acc 1
2016-09-06T15:37:26.285234: step 9044, loss 0.00293069, acc 1
2016-09-06T15:37:27.088996: step 9045, loss 0.00395178, acc 1
2016-09-06T15:37:27.887513: step 9046, loss 0.00928612, acc 1
2016-09-06T15:37:28.696716: step 9047, loss 0.00246219, acc 1
2016-09-06T15:37:29.510157: step 9048, loss 0.0294413, acc 1
2016-09-06T15:37:30.309836: step 9049, loss 0.0408814, acc 0.98
2016-09-06T15:37:31.114271: step 9050, loss 0.0155796, acc 1
2016-09-06T15:37:31.931130: step 9051, loss 0.00246784, acc 1
2016-09-06T15:37:32.713456: step 9052, loss 0.00238208, acc 1
2016-09-06T15:37:33.522753: step 9053, loss 0.131739, acc 0.98
2016-09-06T15:37:34.341554: step 9054, loss 0.040661, acc 0.98
2016-09-06T15:37:35.137587: step 9055, loss 0.00419339, acc 1
2016-09-06T15:37:35.939563: step 9056, loss 0.0272009, acc 1
2016-09-06T15:37:36.769620: step 9057, loss 0.0497915, acc 0.98
2016-09-06T15:37:37.569802: step 9058, loss 0.00563779, acc 1
2016-09-06T15:37:38.375495: step 9059, loss 0.00204455, acc 1
2016-09-06T15:37:39.202400: step 9060, loss 0.00252896, acc 1
2016-09-06T15:37:39.998976: step 9061, loss 0.0232803, acc 0.98
2016-09-06T15:37:40.780439: step 9062, loss 0.0160541, acc 1
2016-09-06T15:37:41.606213: step 9063, loss 0.00423114, acc 1
2016-09-06T15:37:42.413804: step 9064, loss 0.00357961, acc 1
2016-09-06T15:37:43.257611: step 9065, loss 0.0171287, acc 0.98
2016-09-06T15:37:44.043844: step 9066, loss 0.020502, acc 0.98
2016-09-06T15:37:44.835923: step 9067, loss 0.0426034, acc 0.98
2016-09-06T15:37:45.645044: step 9068, loss 0.00440727, acc 1
2016-09-06T15:37:46.475481: step 9069, loss 0.0241394, acc 1
2016-09-06T15:37:47.277698: step 9070, loss 0.0168127, acc 1
2016-09-06T15:37:48.059177: step 9071, loss 0.00888805, acc 1
2016-09-06T15:37:48.890558: step 9072, loss 0.0117397, acc 1
2016-09-06T15:37:49.680026: step 9073, loss 0.00915089, acc 1
2016-09-06T15:37:50.473905: step 9074, loss 0.0351942, acc 1
2016-09-06T15:37:51.275039: step 9075, loss 0.0605489, acc 0.98
2016-09-06T15:37:52.051505: step 9076, loss 0.0119359, acc 1
2016-09-06T15:37:52.897344: step 9077, loss 0.036914, acc 1
2016-09-06T15:37:53.726540: step 9078, loss 0.0108959, acc 1
2016-09-06T15:37:54.530430: step 9079, loss 0.00260743, acc 1
2016-09-06T15:37:55.295706: step 9080, loss 0.0166513, acc 0.98
2016-09-06T15:37:56.104176: step 9081, loss 0.00209533, acc 1
2016-09-06T15:37:56.894130: step 9082, loss 0.00200227, acc 1
2016-09-06T15:37:57.701833: step 9083, loss 0.0123951, acc 1
2016-09-06T15:37:58.524010: step 9084, loss 0.018394, acc 1
2016-09-06T15:37:59.314831: step 9085, loss 0.0156755, acc 1
2016-09-06T15:38:00.122170: step 9086, loss 0.00653539, acc 1
2016-09-06T15:38:00.994417: step 9087, loss 0.00711842, acc 1
2016-09-06T15:38:01.779623: step 9088, loss 0.030203, acc 0.98
2016-09-06T15:38:02.564722: step 9089, loss 0.0313045, acc 0.98
2016-09-06T15:38:03.405892: step 9090, loss 0.0106065, acc 1
2016-09-06T15:38:04.198913: step 9091, loss 0.00227223, acc 1
2016-09-06T15:38:04.977316: step 9092, loss 0.0267949, acc 0.98
2016-09-06T15:38:05.825267: step 9093, loss 0.0198481, acc 1
2016-09-06T15:38:06.624470: step 9094, loss 0.0235457, acc 1
2016-09-06T15:38:07.427911: step 9095, loss 0.00585356, acc 1
2016-09-06T15:38:08.235203: step 9096, loss 0.00350427, acc 1
2016-09-06T15:38:09.007507: step 9097, loss 0.0295484, acc 0.98
2016-09-06T15:38:09.803157: step 9098, loss 0.0332614, acc 0.98
2016-09-06T15:38:10.608596: step 9099, loss 0.00880445, acc 1
2016-09-06T15:38:11.398343: step 9100, loss 0.0338946, acc 0.98

Evaluation:
2016-09-06T15:38:15.166237: step 9100, loss 2.59843, acc 0.742964

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-9100

2016-09-06T15:38:17.049600: step 9101, loss 0.00490968, acc 1
2016-09-06T15:38:17.880386: step 9102, loss 0.109597, acc 0.96
2016-09-06T15:38:18.713970: step 9103, loss 0.0283263, acc 1
2016-09-06T15:38:19.543256: step 9104, loss 0.0259301, acc 0.98
2016-09-06T15:38:20.349538: step 9105, loss 0.012511, acc 1
2016-09-06T15:38:21.162312: step 9106, loss 0.00891494, acc 1
2016-09-06T15:38:21.980691: step 9107, loss 0.0557696, acc 0.98
2016-09-06T15:38:22.789248: step 9108, loss 0.0102917, acc 1
2016-09-06T15:38:23.634249: step 9109, loss 0.0172513, acc 0.98
2016-09-06T15:38:24.451885: step 9110, loss 0.0144206, acc 1
2016-09-06T15:38:25.270465: step 9111, loss 0.0596457, acc 0.98
2016-09-06T15:38:26.050924: step 9112, loss 0.0180831, acc 0.98
2016-09-06T15:38:26.895947: step 9113, loss 0.00425444, acc 1
2016-09-06T15:38:27.703050: step 9114, loss 0.00701205, acc 1
2016-09-06T15:38:28.473755: step 9115, loss 0.0167818, acc 0.98
2016-09-06T15:38:29.258456: step 9116, loss 0.00264045, acc 1
2016-09-06T15:38:30.083408: step 9117, loss 0.0272512, acc 0.98
2016-09-06T15:38:30.896220: step 9118, loss 0.00898315, acc 1
2016-09-06T15:38:31.699937: step 9119, loss 0.0120348, acc 1
2016-09-06T15:38:32.531887: step 9120, loss 0.00409283, acc 1
2016-09-06T15:38:33.308977: step 9121, loss 0.0761017, acc 0.98
2016-09-06T15:38:34.102532: step 9122, loss 0.0330462, acc 0.98
2016-09-06T15:38:34.932544: step 9123, loss 0.0171575, acc 0.98
2016-09-06T15:38:35.712886: step 9124, loss 0.0329183, acc 0.98
2016-09-06T15:38:36.532147: step 9125, loss 0.00592309, acc 1
2016-09-06T15:38:37.329020: step 9126, loss 0.0119363, acc 1
2016-09-06T15:38:38.127117: step 9127, loss 0.0244242, acc 0.98
2016-09-06T15:38:38.920157: step 9128, loss 0.00450668, acc 1
2016-09-06T15:38:39.745783: step 9129, loss 0.0364711, acc 0.98
2016-09-06T15:38:40.548493: step 9130, loss 0.0330236, acc 0.96
2016-09-06T15:38:41.352060: step 9131, loss 0.0267031, acc 0.98
2016-09-06T15:38:42.207510: step 9132, loss 0.0243088, acc 0.98
2016-09-06T15:38:43.010330: step 9133, loss 0.108579, acc 0.98
2016-09-06T15:38:43.817279: step 9134, loss 0.0251198, acc 0.98
2016-09-06T15:38:44.615838: step 9135, loss 0.0128036, acc 1
2016-09-06T15:38:45.388864: step 9136, loss 0.00747953, acc 1
2016-09-06T15:38:46.195103: step 9137, loss 0.0154012, acc 1
2016-09-06T15:38:46.995604: step 9138, loss 0.00834424, acc 1
2016-09-06T15:38:47.801781: step 9139, loss 0.0456615, acc 0.96
2016-09-06T15:38:48.626425: step 9140, loss 0.00611798, acc 1
2016-09-06T15:38:49.467773: step 9141, loss 0.028907, acc 0.98
2016-09-06T15:38:50.271851: step 9142, loss 0.00794884, acc 1
2016-09-06T15:38:51.074555: step 9143, loss 0.0187507, acc 0.98
2016-09-06T15:38:51.890371: step 9144, loss 0.0159316, acc 1
2016-09-06T15:38:52.673874: step 9145, loss 0.00289589, acc 1
2016-09-06T15:38:53.465490: step 9146, loss 0.0255611, acc 0.98
2016-09-06T15:38:54.247927: step 9147, loss 0.0179277, acc 1
2016-09-06T15:38:55.033503: step 9148, loss 0.0158674, acc 1
2016-09-06T15:38:55.848121: step 9149, loss 0.0234611, acc 1
2016-09-06T15:38:56.668483: step 9150, loss 0.00745384, acc 1
2016-09-06T15:38:57.460639: step 9151, loss 0.0231953, acc 1
2016-09-06T15:38:58.273363: step 9152, loss 0.0368759, acc 0.98
2016-09-06T15:38:59.091210: step 9153, loss 0.0274091, acc 0.98
2016-09-06T15:38:59.900057: step 9154, loss 0.0149417, acc 1
2016-09-06T15:39:00.765182: step 9155, loss 0.030192, acc 0.98
2016-09-06T15:39:01.605559: step 9156, loss 0.0280891, acc 0.98
2016-09-06T15:39:02.455346: step 9157, loss 0.00463795, acc 1
2016-09-06T15:39:03.256052: step 9158, loss 0.031447, acc 0.98
2016-09-06T15:39:04.070498: step 9159, loss 0.0281276, acc 0.98
2016-09-06T15:39:04.870602: step 9160, loss 0.0240729, acc 0.98
2016-09-06T15:39:05.679944: step 9161, loss 0.070869, acc 0.96
2016-09-06T15:39:06.520340: step 9162, loss 0.0135593, acc 1
2016-09-06T15:39:07.309942: step 9163, loss 0.0104003, acc 1
2016-09-06T15:39:08.123061: step 9164, loss 0.00290312, acc 1
2016-09-06T15:39:08.955042: step 9165, loss 0.0145812, acc 1
2016-09-06T15:39:09.769432: step 9166, loss 0.0246473, acc 0.98
2016-09-06T15:39:10.595266: step 9167, loss 0.00213175, acc 1
2016-09-06T15:39:11.403630: step 9168, loss 0.041207, acc 0.98
2016-09-06T15:39:12.222323: step 9169, loss 0.013027, acc 1
2016-09-06T15:39:13.047008: step 9170, loss 0.0230857, acc 1
2016-09-06T15:39:13.877892: step 9171, loss 0.00583754, acc 1
2016-09-06T15:39:14.704125: step 9172, loss 0.0166422, acc 1
2016-09-06T15:39:15.518972: step 9173, loss 0.00277938, acc 1
2016-09-06T15:39:16.369846: step 9174, loss 0.0228812, acc 0.98
2016-09-06T15:39:17.210308: step 9175, loss 0.00401995, acc 1
2016-09-06T15:39:17.997404: step 9176, loss 0.00227143, acc 1
2016-09-06T15:39:18.810135: step 9177, loss 0.00526136, acc 1
2016-09-06T15:39:19.659270: step 9178, loss 0.0028352, acc 1
2016-09-06T15:39:20.459471: step 9179, loss 0.0177595, acc 0.98
2016-09-06T15:39:21.266969: step 9180, loss 0.0165846, acc 1
2016-09-06T15:39:22.085400: step 9181, loss 0.0026544, acc 1
2016-09-06T15:39:22.860186: step 9182, loss 0.0148359, acc 1
2016-09-06T15:39:23.667776: step 9183, loss 0.00717088, acc 1
2016-09-06T15:39:24.515206: step 9184, loss 0.021952, acc 0.98
2016-09-06T15:39:25.294997: step 9185, loss 0.01799, acc 1
2016-09-06T15:39:26.091901: step 9186, loss 0.0213081, acc 0.98
2016-09-06T15:39:26.910033: step 9187, loss 0.0662373, acc 0.98
2016-09-06T15:39:27.695333: step 9188, loss 0.00239252, acc 1
2016-09-06T15:39:28.527669: step 9189, loss 0.00264212, acc 1
2016-09-06T15:39:29.333920: step 9190, loss 0.0157103, acc 1
2016-09-06T15:39:30.118237: step 9191, loss 0.00254631, acc 1
2016-09-06T15:39:30.917220: step 9192, loss 0.0315259, acc 0.98
2016-09-06T15:39:31.742824: step 9193, loss 0.109087, acc 0.98
2016-09-06T15:39:32.538650: step 9194, loss 0.0280183, acc 1
2016-09-06T15:39:33.355858: step 9195, loss 0.0134067, acc 1
2016-09-06T15:39:34.190562: step 9196, loss 0.0274026, acc 0.98
2016-09-06T15:39:34.981472: step 9197, loss 0.00342235, acc 1
2016-09-06T15:39:35.830651: step 9198, loss 0.00286255, acc 1
2016-09-06T15:39:36.641119: step 9199, loss 0.0101725, acc 1
2016-09-06T15:39:37.436703: step 9200, loss 0.0416434, acc 0.98

Evaluation:
2016-09-06T15:39:41.258591: step 9200, loss 1.84872, acc 0.756098

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-9200

2016-09-06T15:39:43.334829: step 9201, loss 0.0525742, acc 0.96
2016-09-06T15:39:44.170568: step 9202, loss 0.0424718, acc 0.98
2016-09-06T15:39:45.001552: step 9203, loss 0.00326476, acc 1
2016-09-06T15:39:45.822450: step 9204, loss 0.0405484, acc 0.98
2016-09-06T15:39:46.647208: step 9205, loss 0.012175, acc 1
2016-09-06T15:39:47.464673: step 9206, loss 0.0206289, acc 0.98
2016-09-06T15:39:48.280556: step 9207, loss 0.0025406, acc 1
2016-09-06T15:39:49.093014: step 9208, loss 0.00211598, acc 1
2016-09-06T15:39:49.917448: step 9209, loss 0.00231719, acc 1
2016-09-06T15:39:50.721395: step 9210, loss 0.0803877, acc 0.96
2016-09-06T15:39:51.509814: step 9211, loss 0.0368689, acc 0.98
2016-09-06T15:39:52.355206: step 9212, loss 0.0042621, acc 1
2016-09-06T15:39:53.135778: step 9213, loss 0.0101894, acc 1
2016-09-06T15:39:53.926033: step 9214, loss 0.029201, acc 0.98
2016-09-06T15:39:54.738816: step 9215, loss 0.00290093, acc 1
2016-09-06T15:39:55.474314: step 9216, loss 0.00246027, acc 1
2016-09-06T15:39:56.277630: step 9217, loss 0.0870241, acc 0.96
2016-09-06T15:39:57.104320: step 9218, loss 0.0303626, acc 1
2016-09-06T15:39:57.992567: step 9219, loss 0.00772723, acc 1
2016-09-06T15:39:58.791980: step 9220, loss 0.00458299, acc 1
2016-09-06T15:39:59.648789: step 9221, loss 0.062196, acc 0.96
2016-09-06T15:40:00.456788: step 9222, loss 0.00411008, acc 1
2016-09-06T15:40:01.264227: step 9223, loss 0.0184343, acc 1
2016-09-06T15:40:02.083642: step 9224, loss 0.00250661, acc 1
2016-09-06T15:40:02.877524: step 9225, loss 0.0487869, acc 0.98
2016-09-06T15:40:03.682170: step 9226, loss 0.00391222, acc 1
2016-09-06T15:40:04.487315: step 9227, loss 0.00179687, acc 1
2016-09-06T15:40:05.311089: step 9228, loss 0.0278827, acc 0.98
2016-09-06T15:40:06.122390: step 9229, loss 0.0205058, acc 1
2016-09-06T15:40:06.943781: step 9230, loss 0.0116551, acc 1
2016-09-06T15:40:07.769317: step 9231, loss 0.0203886, acc 1
2016-09-06T15:40:08.582836: step 9232, loss 0.00220786, acc 1
2016-09-06T15:40:09.402025: step 9233, loss 0.00447241, acc 1
2016-09-06T15:40:10.211920: step 9234, loss 0.00761096, acc 1
2016-09-06T15:40:11.038368: step 9235, loss 0.00495768, acc 1
2016-09-06T15:40:11.865330: step 9236, loss 0.00507775, acc 1
2016-09-06T15:40:12.697391: step 9237, loss 0.012328, acc 1
2016-09-06T15:40:13.527850: step 9238, loss 0.0364761, acc 0.98
2016-09-06T15:40:14.362905: step 9239, loss 0.00371791, acc 1
2016-09-06T15:40:15.187022: step 9240, loss 0.0181941, acc 0.98
2016-09-06T15:40:16.011912: step 9241, loss 0.00537885, acc 1
2016-09-06T15:40:16.858941: step 9242, loss 0.0134913, acc 1
2016-09-06T15:40:17.669136: step 9243, loss 0.025892, acc 0.98
2016-09-06T15:40:18.502489: step 9244, loss 0.00323055, acc 1
2016-09-06T15:40:19.327968: step 9245, loss 0.0129461, acc 1
2016-09-06T15:40:20.134826: step 9246, loss 0.0203585, acc 1
2016-09-06T15:40:20.912589: step 9247, loss 0.00976833, acc 1
2016-09-06T15:40:21.717618: step 9248, loss 0.007451, acc 1
2016-09-06T15:40:22.530809: step 9249, loss 0.00695312, acc 1
2016-09-06T15:40:23.339412: step 9250, loss 0.00467689, acc 1
2016-09-06T15:40:24.144912: step 9251, loss 0.00246921, acc 1
2016-09-06T15:40:24.973289: step 9252, loss 0.00398038, acc 1
2016-09-06T15:40:25.755605: step 9253, loss 0.0277537, acc 0.98
2016-09-06T15:40:26.581717: step 9254, loss 0.0273122, acc 0.98
2016-09-06T15:40:27.381563: step 9255, loss 0.0162261, acc 1
2016-09-06T15:40:28.184066: step 9256, loss 0.00398261, acc 1
2016-09-06T15:40:29.025813: step 9257, loss 0.00463374, acc 1
2016-09-06T15:40:29.852952: step 9258, loss 0.00323579, acc 1
2016-09-06T15:40:30.662220: step 9259, loss 0.0256755, acc 0.98
2016-09-06T15:40:31.479355: step 9260, loss 0.0124723, acc 1
2016-09-06T15:40:32.322262: step 9261, loss 0.0288357, acc 0.98
2016-09-06T15:40:33.135502: step 9262, loss 0.00414891, acc 1
2016-09-06T15:40:33.977297: step 9263, loss 0.0551025, acc 0.98
2016-09-06T15:40:34.813575: step 9264, loss 0.0036886, acc 1
2016-09-06T15:40:35.623559: step 9265, loss 0.00534, acc 1
2016-09-06T15:40:36.436472: step 9266, loss 0.0399279, acc 0.98
2016-09-06T15:40:37.266672: step 9267, loss 0.0360657, acc 0.98
2016-09-06T15:40:38.070663: step 9268, loss 0.0130957, acc 1
2016-09-06T15:40:38.884190: step 9269, loss 0.00655566, acc 1
2016-09-06T15:40:39.685431: step 9270, loss 0.0337234, acc 0.98
2016-09-06T15:40:40.472347: step 9271, loss 0.00310759, acc 1
2016-09-06T15:40:41.276946: step 9272, loss 0.0537254, acc 0.94
2016-09-06T15:40:42.097218: step 9273, loss 0.00434761, acc 1
2016-09-06T15:40:42.916936: step 9274, loss 0.00808941, acc 1
2016-09-06T15:40:43.742928: step 9275, loss 0.0395545, acc 0.96
2016-09-06T15:40:44.554105: step 9276, loss 0.0181534, acc 0.98
2016-09-06T15:40:45.392416: step 9277, loss 0.00236933, acc 1
2016-09-06T15:40:46.186751: step 9278, loss 0.00231003, acc 1
2016-09-06T15:40:47.002196: step 9279, loss 0.0987875, acc 0.98
2016-09-06T15:40:47.845433: step 9280, loss 0.106179, acc 0.96
2016-09-06T15:40:48.669157: step 9281, loss 0.00332208, acc 1
2016-09-06T15:40:49.497785: step 9282, loss 0.0088288, acc 1
2016-09-06T15:40:50.332828: step 9283, loss 0.0279193, acc 0.98
2016-09-06T15:40:51.131528: step 9284, loss 0.0478988, acc 0.96
2016-09-06T15:40:51.924889: step 9285, loss 0.00800771, acc 1
2016-09-06T15:40:52.746785: step 9286, loss 0.0459452, acc 0.98
2016-09-06T15:40:53.548856: step 9287, loss 0.00937525, acc 1
2016-09-06T15:40:54.344394: step 9288, loss 0.023928, acc 0.98
2016-09-06T15:40:55.178078: step 9289, loss 0.04017, acc 0.98
2016-09-06T15:40:55.982360: step 9290, loss 0.0202669, acc 1
2016-09-06T15:40:56.791653: step 9291, loss 0.00545766, acc 1
2016-09-06T15:40:57.616050: step 9292, loss 0.0176286, acc 0.98
2016-09-06T15:40:58.388855: step 9293, loss 0.01012, acc 1
2016-09-06T15:40:59.190085: step 9294, loss 0.0227941, acc 1
2016-09-06T15:41:00.013472: step 9295, loss 0.0366097, acc 0.98
2016-09-06T15:41:00.851061: step 9296, loss 0.0138402, acc 1
2016-09-06T15:41:01.649861: step 9297, loss 0.0168228, acc 0.98
2016-09-06T15:41:02.457142: step 9298, loss 0.00336747, acc 1
2016-09-06T15:41:03.227917: step 9299, loss 0.0292435, acc 0.98
2016-09-06T15:41:04.033824: step 9300, loss 0.00284505, acc 1

Evaluation:
2016-09-06T15:41:07.763705: step 9300, loss 2.23517, acc 0.752345

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-9300

2016-09-06T15:41:09.694185: step 9301, loss 0.0627197, acc 0.98
2016-09-06T15:41:10.504962: step 9302, loss 0.101664, acc 0.98
2016-09-06T15:41:11.329134: step 9303, loss 0.0045532, acc 1
2016-09-06T15:41:12.164869: step 9304, loss 0.0178029, acc 0.98
2016-09-06T15:41:12.959249: step 9305, loss 0.00727323, acc 1
2016-09-06T15:41:13.784106: step 9306, loss 0.00310599, acc 1
2016-09-06T15:41:14.576662: step 9307, loss 0.0529104, acc 0.96
2016-09-06T15:41:15.392438: step 9308, loss 0.00704012, acc 1
2016-09-06T15:41:16.218760: step 9309, loss 0.0207244, acc 0.98
2016-09-06T15:41:17.050375: step 9310, loss 0.00245584, acc 1
2016-09-06T15:41:17.864668: step 9311, loss 0.0117301, acc 1
2016-09-06T15:41:18.701480: step 9312, loss 0.0265211, acc 0.98
2016-09-06T15:41:19.526693: step 9313, loss 0.0229941, acc 0.98
2016-09-06T15:41:20.288231: step 9314, loss 0.0155494, acc 1
2016-09-06T15:41:21.108467: step 9315, loss 0.00338275, acc 1
2016-09-06T15:41:21.918833: step 9316, loss 0.0253351, acc 1
2016-09-06T15:41:22.720809: step 9317, loss 0.0272985, acc 0.98
2016-09-06T15:41:23.523558: step 9318, loss 0.0142293, acc 1
2016-09-06T15:41:24.353876: step 9319, loss 0.0187896, acc 1
2016-09-06T15:41:25.135348: step 9320, loss 0.0178713, acc 1
2016-09-06T15:41:25.941751: step 9321, loss 0.0156086, acc 1
2016-09-06T15:41:26.753606: step 9322, loss 0.00386464, acc 1
2016-09-06T15:41:27.523899: step 9323, loss 0.0295225, acc 1
2016-09-06T15:41:28.322947: step 9324, loss 0.00965382, acc 1
2016-09-06T15:41:29.145493: step 9325, loss 0.0307682, acc 0.98
2016-09-06T15:41:29.945834: step 9326, loss 0.0253442, acc 1
2016-09-06T15:41:30.757146: step 9327, loss 0.00790763, acc 1
2016-09-06T15:41:31.573908: step 9328, loss 0.00728859, acc 1
2016-09-06T15:41:32.364180: step 9329, loss 0.0187403, acc 1
2016-09-06T15:41:33.169874: step 9330, loss 0.0025446, acc 1
2016-09-06T15:41:34.019283: step 9331, loss 0.0349391, acc 0.98
2016-09-06T15:41:34.800109: step 9332, loss 0.0103021, acc 1
2016-09-06T15:41:35.603616: step 9333, loss 0.0171805, acc 1
2016-09-06T15:41:36.413121: step 9334, loss 0.00257471, acc 1
2016-09-06T15:41:37.173843: step 9335, loss 0.041025, acc 0.98
2016-09-06T15:41:37.985262: step 9336, loss 0.00282237, acc 1
2016-09-06T15:41:38.788720: step 9337, loss 0.00250007, acc 1
2016-09-06T15:41:39.582038: step 9338, loss 0.00521189, acc 1
2016-09-06T15:41:40.389679: step 9339, loss 0.0252062, acc 0.98
2016-09-06T15:41:41.219823: step 9340, loss 0.00839517, acc 1
2016-09-06T15:41:42.022176: step 9341, loss 0.0177137, acc 0.98
2016-09-06T15:41:42.845690: step 9342, loss 0.00795292, acc 1
2016-09-06T15:41:43.687472: step 9343, loss 0.00247078, acc 1
2016-09-06T15:41:44.485552: step 9344, loss 0.00314751, acc 1
2016-09-06T15:41:45.301311: step 9345, loss 0.0776903, acc 0.98
2016-09-06T15:41:46.131081: step 9346, loss 0.00950561, acc 1
2016-09-06T15:41:46.940256: step 9347, loss 0.0226055, acc 0.98
2016-09-06T15:41:47.744095: step 9348, loss 0.00674133, acc 1
2016-09-06T15:41:48.566139: step 9349, loss 0.0675512, acc 0.98
2016-09-06T15:41:49.378831: step 9350, loss 0.0272486, acc 0.98
2016-09-06T15:41:50.176176: step 9351, loss 0.0847467, acc 0.96
2016-09-06T15:41:50.978839: step 9352, loss 0.0242426, acc 0.98
2016-09-06T15:41:51.776576: step 9353, loss 0.0273239, acc 0.98
2016-09-06T15:41:52.581733: step 9354, loss 0.0403423, acc 0.98
2016-09-06T15:41:53.428557: step 9355, loss 0.00211593, acc 1
2016-09-06T15:41:54.249692: step 9356, loss 0.00380045, acc 1
2016-09-06T15:41:55.079671: step 9357, loss 0.0682811, acc 0.98
2016-09-06T15:41:55.910355: step 9358, loss 0.0133807, acc 1
2016-09-06T15:41:56.718652: step 9359, loss 0.0202456, acc 0.98
2016-09-06T15:41:57.515829: step 9360, loss 0.0180156, acc 0.98
2016-09-06T15:41:58.332258: step 9361, loss 0.00276411, acc 1
2016-09-06T15:41:59.135398: step 9362, loss 0.0126211, acc 1
2016-09-06T15:41:59.983813: step 9363, loss 0.03342, acc 0.98
2016-09-06T15:42:00.838632: step 9364, loss 0.0160505, acc 1
2016-09-06T15:42:01.615098: step 9365, loss 0.0233192, acc 1
2016-09-06T15:42:02.444763: step 9366, loss 0.0235131, acc 0.98
2016-09-06T15:42:03.317041: step 9367, loss 0.00378616, acc 1
2016-09-06T15:42:04.139801: step 9368, loss 0.00462015, acc 1
2016-09-06T15:42:04.952053: step 9369, loss 0.00883802, acc 1
2016-09-06T15:42:05.790159: step 9370, loss 0.0021059, acc 1
2016-09-06T15:42:06.639689: step 9371, loss 0.00266339, acc 1
2016-09-06T15:42:07.439951: step 9372, loss 0.0264697, acc 0.98
2016-09-06T15:42:08.258822: step 9373, loss 0.00386784, acc 1
2016-09-06T15:42:09.089498: step 9374, loss 0.0611068, acc 0.98
2016-09-06T15:42:09.874667: step 9375, loss 0.0131471, acc 1
2016-09-06T15:42:10.704421: step 9376, loss 0.00997443, acc 1
2016-09-06T15:42:11.518037: step 9377, loss 0.0351346, acc 0.96
2016-09-06T15:42:12.304290: step 9378, loss 0.00654263, acc 1
2016-09-06T15:42:13.104425: step 9379, loss 0.0154746, acc 1
2016-09-06T15:42:13.939476: step 9380, loss 0.028506, acc 0.98
2016-09-06T15:42:14.713296: step 9381, loss 0.0229987, acc 1
2016-09-06T15:42:15.521423: step 9382, loss 0.0512984, acc 0.96
2016-09-06T15:42:16.352106: step 9383, loss 0.0105422, acc 1
2016-09-06T15:42:17.127760: step 9384, loss 0.00846117, acc 1
2016-09-06T15:42:17.935716: step 9385, loss 0.0744048, acc 0.96
2016-09-06T15:42:18.773963: step 9386, loss 0.00250182, acc 1
2016-09-06T15:42:19.565444: step 9387, loss 0.0229328, acc 0.98
2016-09-06T15:42:20.361856: step 9388, loss 0.00856861, acc 1
2016-09-06T15:42:21.185606: step 9389, loss 0.0196593, acc 0.98
2016-09-06T15:42:21.981816: step 9390, loss 0.00223778, acc 1
2016-09-06T15:42:22.760579: step 9391, loss 0.0168428, acc 1
2016-09-06T15:42:23.598906: step 9392, loss 0.00200172, acc 1
2016-09-06T15:42:24.379824: step 9393, loss 0.0534377, acc 0.98
2016-09-06T15:42:25.171987: step 9394, loss 0.0339247, acc 1
2016-09-06T15:42:26.008572: step 9395, loss 0.0145833, acc 1
2016-09-06T15:42:26.809501: step 9396, loss 0.0388505, acc 0.98
2016-09-06T15:42:27.612427: step 9397, loss 0.00369713, acc 1
2016-09-06T15:42:28.401411: step 9398, loss 0.00189169, acc 1
2016-09-06T15:42:29.198307: step 9399, loss 0.00493458, acc 1
2016-09-06T15:42:30.039396: step 9400, loss 0.0376386, acc 0.98

Evaluation:
2016-09-06T15:42:33.770864: step 9400, loss 2.18376, acc 0.750469

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-9400

2016-09-06T15:42:35.646080: step 9401, loss 0.00900562, acc 1
2016-09-06T15:42:36.478698: step 9402, loss 0.0469708, acc 0.96
2016-09-06T15:42:37.330455: step 9403, loss 0.0309763, acc 0.98
2016-09-06T15:42:38.170314: step 9404, loss 0.00530905, acc 1
2016-09-06T15:42:38.959052: step 9405, loss 0.0295441, acc 0.98
2016-09-06T15:42:39.771661: step 9406, loss 0.00522813, acc 1
2016-09-06T15:42:40.591908: step 9407, loss 0.0456024, acc 0.96
2016-09-06T15:42:41.313707: step 9408, loss 0.0173417, acc 1
2016-09-06T15:42:42.150051: step 9409, loss 0.015699, acc 1
2016-09-06T15:42:42.946315: step 9410, loss 0.024986, acc 1
2016-09-06T15:42:43.759566: step 9411, loss 0.00216241, acc 1
2016-09-06T15:42:44.595521: step 9412, loss 0.160886, acc 0.94
2016-09-06T15:42:45.402939: step 9413, loss 0.0513701, acc 0.96
2016-09-06T15:42:46.200703: step 9414, loss 0.0127142, acc 1
2016-09-06T15:42:46.998368: step 9415, loss 0.00818236, acc 1
2016-09-06T15:42:47.808222: step 9416, loss 0.00727062, acc 1
2016-09-06T15:42:48.631115: step 9417, loss 0.0534212, acc 0.96
2016-09-06T15:42:49.429203: step 9418, loss 0.0126716, acc 1
2016-09-06T15:42:50.258986: step 9419, loss 0.00253778, acc 1
2016-09-06T15:42:51.056134: step 9420, loss 0.00190316, acc 1
2016-09-06T15:42:51.852192: step 9421, loss 0.0298619, acc 0.98
2016-09-06T15:42:52.675733: step 9422, loss 0.00696511, acc 1
2016-09-06T15:42:53.472435: step 9423, loss 0.0296336, acc 1
2016-09-06T15:42:54.286006: step 9424, loss 0.00388273, acc 1
2016-09-06T15:42:55.089963: step 9425, loss 0.020939, acc 0.98
2016-09-06T15:42:55.875365: step 9426, loss 0.0371053, acc 0.98
2016-09-06T15:42:56.688016: step 9427, loss 0.0131286, acc 1
2016-09-06T15:42:57.495849: step 9428, loss 0.0376104, acc 0.98
2016-09-06T15:42:58.292110: step 9429, loss 0.00275252, acc 1
2016-09-06T15:42:59.094247: step 9430, loss 0.0310507, acc 0.98
2016-09-06T15:42:59.933010: step 9431, loss 0.00198039, acc 1
2016-09-06T15:43:00.753749: step 9432, loss 0.0185019, acc 0.98
2016-09-06T15:43:01.577419: step 9433, loss 0.01123, acc 1
2016-09-06T15:43:02.377418: step 9434, loss 0.0144579, acc 1
2016-09-06T15:43:03.160676: step 9435, loss 0.00415037, acc 1
2016-09-06T15:43:03.961631: step 9436, loss 0.0359428, acc 0.98
2016-09-06T15:43:04.783126: step 9437, loss 0.0188833, acc 1
2016-09-06T15:43:05.563907: step 9438, loss 0.0172767, acc 0.98
2016-09-06T15:43:06.372547: step 9439, loss 0.00491279, acc 1
2016-09-06T15:43:07.180769: step 9440, loss 0.0201067, acc 1
2016-09-06T15:43:07.973741: step 9441, loss 0.0131541, acc 1
2016-09-06T15:43:08.787766: step 9442, loss 0.00823885, acc 1
2016-09-06T15:43:09.611304: step 9443, loss 0.00231608, acc 1
2016-09-06T15:43:10.417425: step 9444, loss 0.00456205, acc 1
2016-09-06T15:43:11.217439: step 9445, loss 0.0341984, acc 0.98
2016-09-06T15:43:12.072025: step 9446, loss 0.022226, acc 0.98
2016-09-06T15:43:12.853452: step 9447, loss 0.00330681, acc 1
2016-09-06T15:43:13.665524: step 9448, loss 0.00299328, acc 1
2016-09-06T15:43:14.494988: step 9449, loss 0.00512192, acc 1
2016-09-06T15:43:15.288725: step 9450, loss 0.0185179, acc 0.98
2016-09-06T15:43:16.087898: step 9451, loss 0.0286556, acc 0.98
2016-09-06T15:43:16.902783: step 9452, loss 0.0522813, acc 0.96
2016-09-06T15:43:17.707194: step 9453, loss 0.0180357, acc 1
2016-09-06T15:43:18.521935: step 9454, loss 0.0212633, acc 0.98
2016-09-06T15:43:19.397112: step 9455, loss 0.00251671, acc 1
2016-09-06T15:43:20.214773: step 9456, loss 0.0152916, acc 1
2016-09-06T15:43:21.030739: step 9457, loss 0.0494395, acc 0.98
2016-09-06T15:43:21.852188: step 9458, loss 0.154455, acc 0.96
2016-09-06T15:43:22.685231: step 9459, loss 0.00455645, acc 1
2016-09-06T15:43:23.503019: step 9460, loss 0.0328158, acc 0.98
2016-09-06T15:43:24.346392: step 9461, loss 0.00461449, acc 1
2016-09-06T15:43:25.167641: step 9462, loss 0.00324156, acc 1
2016-09-06T15:43:25.966935: step 9463, loss 0.00384013, acc 1
2016-09-06T15:43:26.781392: step 9464, loss 0.024331, acc 0.98
2016-09-06T15:43:27.585243: step 9465, loss 0.0124085, acc 1
2016-09-06T15:43:28.397977: step 9466, loss 0.00298191, acc 1
2016-09-06T15:43:29.235393: step 9467, loss 0.0305556, acc 0.98
2016-09-06T15:43:30.047841: step 9468, loss 0.0164505, acc 1
2016-09-06T15:43:30.861188: step 9469, loss 0.00593288, acc 1
2016-09-06T15:43:31.703366: step 9470, loss 0.0141993, acc 1
2016-09-06T15:43:32.534619: step 9471, loss 0.0175547, acc 0.98
2016-09-06T15:43:33.346208: step 9472, loss 0.0185424, acc 1
2016-09-06T15:43:34.156323: step 9473, loss 0.0275516, acc 1
2016-09-06T15:43:35.005589: step 9474, loss 0.0213609, acc 0.98
2016-09-06T15:43:35.785621: step 9475, loss 0.0291607, acc 0.98
2016-09-06T15:43:36.575721: step 9476, loss 0.0502453, acc 0.96
2016-09-06T15:43:37.391393: step 9477, loss 0.0366309, acc 0.96
2016-09-06T15:43:38.174882: step 9478, loss 0.0270406, acc 0.98
2016-09-06T15:43:38.980376: step 9479, loss 0.0102952, acc 1
2016-09-06T15:43:39.789842: step 9480, loss 0.00254256, acc 1
2016-09-06T15:43:40.600933: step 9481, loss 0.0286485, acc 0.98
2016-09-06T15:43:41.388993: step 9482, loss 0.00781079, acc 1
2016-09-06T15:43:42.219315: step 9483, loss 0.0476804, acc 0.98
2016-09-06T15:43:43.020427: step 9484, loss 0.0120951, acc 1
2016-09-06T15:43:43.831133: step 9485, loss 0.0146946, acc 1
2016-09-06T15:43:44.656171: step 9486, loss 0.0496603, acc 0.96
2016-09-06T15:43:45.463967: step 9487, loss 0.0416782, acc 0.98
2016-09-06T15:43:46.272353: step 9488, loss 0.00542538, acc 1
2016-09-06T15:43:47.099251: step 9489, loss 0.00275056, acc 1
2016-09-06T15:43:47.869050: step 9490, loss 0.0270874, acc 1
2016-09-06T15:43:48.699334: step 9491, loss 0.0561118, acc 0.98
2016-09-06T15:43:49.508632: step 9492, loss 0.0463031, acc 0.98
2016-09-06T15:43:50.303883: step 9493, loss 0.0245128, acc 0.98
2016-09-06T15:43:51.131274: step 9494, loss 0.0290382, acc 1
2016-09-06T15:43:52.029684: step 9495, loss 0.0209862, acc 0.98
2016-09-06T15:43:52.830534: step 9496, loss 0.00823228, acc 1
2016-09-06T15:43:53.638761: step 9497, loss 0.0029004, acc 1
2016-09-06T15:43:54.464222: step 9498, loss 0.0692394, acc 0.98
2016-09-06T15:43:55.322067: step 9499, loss 0.0170053, acc 1
2016-09-06T15:43:56.133368: step 9500, loss 0.00672384, acc 1

Evaluation:
2016-09-06T15:43:59.869599: step 9500, loss 1.95314, acc 0.757036

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-9500

2016-09-06T15:44:01.820967: step 9501, loss 0.0129739, acc 1
2016-09-06T15:44:02.645704: step 9502, loss 0.00237848, acc 1
2016-09-06T15:44:03.472989: step 9503, loss 0.00531593, acc 1
2016-09-06T15:44:04.331207: step 9504, loss 0.00486249, acc 1
2016-09-06T15:44:05.129221: step 9505, loss 0.0119561, acc 1
2016-09-06T15:44:05.933413: step 9506, loss 0.0462246, acc 0.98
2016-09-06T15:44:06.789481: step 9507, loss 0.0997484, acc 0.96
2016-09-06T15:44:07.611764: step 9508, loss 0.0114661, acc 1
2016-09-06T15:44:08.403442: step 9509, loss 0.0284466, acc 0.98
2016-09-06T15:44:09.228864: step 9510, loss 0.00558, acc 1
2016-09-06T15:44:10.071640: step 9511, loss 0.00261091, acc 1
2016-09-06T15:44:10.854098: step 9512, loss 0.00218997, acc 1
2016-09-06T15:44:11.654683: step 9513, loss 0.0174433, acc 0.98
2016-09-06T15:44:12.463297: step 9514, loss 0.0021755, acc 1
2016-09-06T15:44:13.263254: step 9515, loss 0.0705815, acc 0.96
2016-09-06T15:44:14.099387: step 9516, loss 0.0273895, acc 1
2016-09-06T15:44:14.908928: step 9517, loss 0.0182081, acc 1
2016-09-06T15:44:15.708767: step 9518, loss 0.0245233, acc 0.98
2016-09-06T15:44:16.526724: step 9519, loss 0.00342279, acc 1
2016-09-06T15:44:17.355490: step 9520, loss 0.0513156, acc 0.98
2016-09-06T15:44:18.161032: step 9521, loss 0.0236335, acc 1
2016-09-06T15:44:18.958170: step 9522, loss 0.00613399, acc 1
2016-09-06T15:44:19.766082: step 9523, loss 0.081439, acc 0.94
2016-09-06T15:44:20.558520: step 9524, loss 0.056844, acc 0.96
2016-09-06T15:44:21.359163: step 9525, loss 0.002354, acc 1
2016-09-06T15:44:22.127704: step 9526, loss 0.00249653, acc 1
2016-09-06T15:44:22.907114: step 9527, loss 0.0434048, acc 0.98
2016-09-06T15:44:23.746889: step 9528, loss 0.0163908, acc 0.98
2016-09-06T15:44:24.574359: step 9529, loss 0.00246656, acc 1
2016-09-06T15:44:25.356909: step 9530, loss 0.00224968, acc 1
2016-09-06T15:44:26.179668: step 9531, loss 0.0173498, acc 1
2016-09-06T15:44:26.977793: step 9532, loss 0.0176909, acc 1
2016-09-06T15:44:27.760123: step 9533, loss 0.0172895, acc 1
2016-09-06T15:44:28.553213: step 9534, loss 0.122867, acc 0.98
2016-09-06T15:44:29.348676: step 9535, loss 0.0317284, acc 1
2016-09-06T15:44:30.124117: step 9536, loss 0.0364871, acc 0.98
2016-09-06T15:44:30.934585: step 9537, loss 0.00781649, acc 1
2016-09-06T15:44:31.730320: step 9538, loss 0.00522438, acc 1
2016-09-06T15:44:32.533611: step 9539, loss 0.0414367, acc 0.98
2016-09-06T15:44:33.339027: step 9540, loss 0.0177715, acc 1
2016-09-06T15:44:34.141045: step 9541, loss 0.00466765, acc 1
2016-09-06T15:44:34.948341: step 9542, loss 0.0668633, acc 0.96
2016-09-06T15:44:35.764997: step 9543, loss 0.0182686, acc 1
2016-09-06T15:44:36.559889: step 9544, loss 0.0445575, acc 0.98
2016-09-06T15:44:37.370954: step 9545, loss 0.00759828, acc 1
2016-09-06T15:44:38.200302: step 9546, loss 0.0626471, acc 0.96
2016-09-06T15:44:39.020760: step 9547, loss 0.0145605, acc 1
2016-09-06T15:44:39.829634: step 9548, loss 0.00577361, acc 1
2016-09-06T15:44:40.653867: step 9549, loss 0.0230671, acc 0.98
2016-09-06T15:44:41.488854: step 9550, loss 0.00321429, acc 1
2016-09-06T15:44:42.264772: step 9551, loss 0.00819472, acc 1
2016-09-06T15:44:43.101707: step 9552, loss 0.00902113, acc 1
2016-09-06T15:44:43.907991: step 9553, loss 0.00455467, acc 1
2016-09-06T15:44:44.697749: step 9554, loss 0.013285, acc 1
2016-09-06T15:44:45.544884: step 9555, loss 0.0279601, acc 0.98
2016-09-06T15:44:46.427924: step 9556, loss 0.0252069, acc 0.98
2016-09-06T15:44:47.231864: step 9557, loss 0.0122989, acc 1
2016-09-06T15:44:48.070512: step 9558, loss 0.0152913, acc 1
2016-09-06T15:44:48.921862: step 9559, loss 0.00291056, acc 1
2016-09-06T15:44:49.722828: step 9560, loss 0.00233498, acc 1
2016-09-06T15:44:50.520111: step 9561, loss 0.00443321, acc 1
2016-09-06T15:44:51.371169: step 9562, loss 0.00255649, acc 1
2016-09-06T15:44:52.177606: step 9563, loss 0.0144063, acc 1
2016-09-06T15:44:52.993769: step 9564, loss 0.0107039, acc 1
2016-09-06T15:44:53.832680: step 9565, loss 0.00327417, acc 1
2016-09-06T15:44:54.666075: step 9566, loss 0.0312599, acc 0.98
2016-09-06T15:44:55.465572: step 9567, loss 0.0162087, acc 1
2016-09-06T15:44:56.310906: step 9568, loss 0.0619874, acc 0.96
2016-09-06T15:44:57.113487: step 9569, loss 0.03956, acc 1
2016-09-06T15:44:57.932188: step 9570, loss 0.00240528, acc 1
2016-09-06T15:44:58.726514: step 9571, loss 0.00309095, acc 1
2016-09-06T15:44:59.515223: step 9572, loss 0.00327188, acc 1
2016-09-06T15:45:00.332606: step 9573, loss 0.00266341, acc 1
2016-09-06T15:45:01.158457: step 9574, loss 0.0142361, acc 1
2016-09-06T15:45:01.983871: step 9575, loss 0.0141031, acc 1
2016-09-06T15:45:02.763129: step 9576, loss 0.00840459, acc 1
2016-09-06T15:45:03.569825: step 9577, loss 0.0215777, acc 1
2016-09-06T15:45:04.391441: step 9578, loss 0.0116826, acc 1
2016-09-06T15:45:05.192817: step 9579, loss 0.0272168, acc 1
2016-09-06T15:45:05.989721: step 9580, loss 0.00286354, acc 1
2016-09-06T15:45:06.793372: step 9581, loss 0.00260824, acc 1
2016-09-06T15:45:07.609461: step 9582, loss 0.0135757, acc 1
2016-09-06T15:45:08.436866: step 9583, loss 0.00543711, acc 1
2016-09-06T15:45:09.274566: step 9584, loss 0.00368683, acc 1
2016-09-06T15:45:10.054714: step 9585, loss 0.00328732, acc 1
2016-09-06T15:45:10.842948: step 9586, loss 0.103263, acc 0.96
2016-09-06T15:45:11.663099: step 9587, loss 0.0039042, acc 1
2016-09-06T15:45:12.454153: step 9588, loss 0.0481854, acc 0.98
2016-09-06T15:45:13.245152: step 9589, loss 0.00244591, acc 1
2016-09-06T15:45:14.079822: step 9590, loss 0.00309991, acc 1
2016-09-06T15:45:14.897276: step 9591, loss 0.00243422, acc 1
2016-09-06T15:45:15.736844: step 9592, loss 0.0231496, acc 0.98
2016-09-06T15:45:16.564423: step 9593, loss 0.00520273, acc 1
2016-09-06T15:45:17.392953: step 9594, loss 0.0312283, acc 1
2016-09-06T15:45:18.230321: step 9595, loss 0.0148118, acc 1
2016-09-06T15:45:19.027421: step 9596, loss 0.0277331, acc 0.98
2016-09-06T15:45:19.855170: step 9597, loss 0.0104574, acc 1
2016-09-06T15:45:20.670872: step 9598, loss 0.011321, acc 1
2016-09-06T15:45:21.507681: step 9599, loss 0.011266, acc 1
2016-09-06T15:45:22.265463: step 9600, loss 0.0219732, acc 1

Evaluation:
2016-09-06T15:45:25.988611: step 9600, loss 2.67109, acc 0.757974

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-9600

2016-09-06T15:45:27.904165: step 9601, loss 0.0110319, acc 1
2016-09-06T15:45:28.761726: step 9602, loss 0.0822861, acc 0.96
2016-09-06T15:45:29.574579: step 9603, loss 0.00352592, acc 1
2016-09-06T15:45:30.372432: step 9604, loss 0.0838568, acc 0.96
2016-09-06T15:45:31.202146: step 9605, loss 0.00999171, acc 1
2016-09-06T15:45:32.036390: step 9606, loss 0.0302394, acc 0.98
2016-09-06T15:45:32.868930: step 9607, loss 0.00262355, acc 1
2016-09-06T15:45:33.689132: step 9608, loss 0.0139129, acc 1
2016-09-06T15:45:34.501694: step 9609, loss 0.0280367, acc 0.98
2016-09-06T15:45:35.307227: step 9610, loss 0.00691103, acc 1
2016-09-06T15:45:36.131496: step 9611, loss 0.0142411, acc 1
2016-09-06T15:45:36.942151: step 9612, loss 0.00212767, acc 1
2016-09-06T15:45:37.772934: step 9613, loss 0.00745492, acc 1
2016-09-06T15:45:38.634076: step 9614, loss 0.030268, acc 0.98
2016-09-06T15:45:39.426637: step 9615, loss 0.120242, acc 0.96
2016-09-06T15:45:40.236852: step 9616, loss 0.0104463, acc 1
2016-09-06T15:45:41.052289: step 9617, loss 0.00630627, acc 1
2016-09-06T15:45:41.860494: step 9618, loss 0.00232766, acc 1
2016-09-06T15:45:42.684535: step 9619, loss 0.0161998, acc 1
2016-09-06T15:45:43.511981: step 9620, loss 0.0019346, acc 1
2016-09-06T15:45:44.360818: step 9621, loss 0.00190092, acc 1
2016-09-06T15:45:45.130543: step 9622, loss 0.00253416, acc 1
2016-09-06T15:45:45.919712: step 9623, loss 0.00570598, acc 1
2016-09-06T15:45:46.739913: step 9624, loss 0.0166383, acc 1
2016-09-06T15:45:47.556387: step 9625, loss 0.0145275, acc 1
2016-09-06T15:45:48.361461: step 9626, loss 0.013544, acc 1
2016-09-06T15:45:49.204412: step 9627, loss 0.00302187, acc 1
2016-09-06T15:45:50.021689: step 9628, loss 0.0018076, acc 1
2016-09-06T15:45:50.839580: step 9629, loss 0.016181, acc 0.98
2016-09-06T15:45:51.684451: step 9630, loss 0.0125641, acc 1
2016-09-06T15:45:52.501522: step 9631, loss 0.00182802, acc 1
2016-09-06T15:45:53.308211: step 9632, loss 0.0293353, acc 0.98
2016-09-06T15:45:54.133279: step 9633, loss 0.0146213, acc 1
2016-09-06T15:45:54.952125: step 9634, loss 0.00381084, acc 1
2016-09-06T15:45:55.791038: step 9635, loss 0.00677323, acc 1
2016-09-06T15:45:56.615643: step 9636, loss 0.00531758, acc 1
2016-09-06T15:45:57.423425: step 9637, loss 0.0135102, acc 1
2016-09-06T15:45:58.239813: step 9638, loss 0.0520468, acc 0.98
2016-09-06T15:45:59.057609: step 9639, loss 0.0091777, acc 1
2016-09-06T15:45:59.870755: step 9640, loss 0.00226095, acc 1
2016-09-06T15:46:00.742901: step 9641, loss 0.0170336, acc 1
2016-09-06T15:46:01.577074: step 9642, loss 0.002808, acc 1
2016-09-06T15:46:02.388061: step 9643, loss 0.00440319, acc 1
2016-09-06T15:46:03.204008: step 9644, loss 0.0135469, acc 1
2016-09-06T15:46:04.017375: step 9645, loss 0.0081505, acc 1
2016-09-06T15:46:04.820533: step 9646, loss 0.033099, acc 0.98
2016-09-06T15:46:05.608551: step 9647, loss 0.0191715, acc 0.98
2016-09-06T15:46:06.419883: step 9648, loss 0.0242701, acc 0.98
2016-09-06T15:46:07.255931: step 9649, loss 0.0123386, acc 1
2016-09-06T15:46:08.061333: step 9650, loss 0.0346534, acc 0.96
2016-09-06T15:46:08.886801: step 9651, loss 0.00999061, acc 1
2016-09-06T15:46:09.719155: step 9652, loss 0.0518986, acc 0.96
2016-09-06T15:46:10.506981: step 9653, loss 0.0019023, acc 1
2016-09-06T15:46:11.299082: step 9654, loss 0.00218721, acc 1
2016-09-06T15:46:12.125392: step 9655, loss 0.00206669, acc 1
2016-09-06T15:46:12.901160: step 9656, loss 0.00278223, acc 1
2016-09-06T15:46:13.718731: step 9657, loss 0.0191659, acc 1
2016-09-06T15:46:14.552070: step 9658, loss 0.00185339, acc 1
2016-09-06T15:46:15.320766: step 9659, loss 0.0357324, acc 0.96
2016-09-06T15:46:16.114328: step 9660, loss 0.0159198, acc 1
2016-09-06T15:46:16.928972: step 9661, loss 0.00186922, acc 1
2016-09-06T15:46:17.719308: step 9662, loss 0.00800498, acc 1
2016-09-06T15:46:18.553946: step 9663, loss 0.0103986, acc 1
2016-09-06T15:46:19.364681: step 9664, loss 0.0178732, acc 1
2016-09-06T15:46:20.176267: step 9665, loss 0.032981, acc 0.98
2016-09-06T15:46:20.972012: step 9666, loss 0.0366495, acc 0.98
2016-09-06T15:46:21.770681: step 9667, loss 0.0113629, acc 1
2016-09-06T15:46:22.575926: step 9668, loss 0.0691611, acc 0.96
2016-09-06T15:46:23.380704: step 9669, loss 0.0533145, acc 0.96
2016-09-06T15:46:24.184532: step 9670, loss 0.185897, acc 0.98
2016-09-06T15:46:24.983076: step 9671, loss 0.0080887, acc 1
2016-09-06T15:46:25.786937: step 9672, loss 0.00177818, acc 1
2016-09-06T15:46:26.596504: step 9673, loss 0.109855, acc 0.98
2016-09-06T15:46:27.404343: step 9674, loss 0.0554701, acc 0.98
2016-09-06T15:46:28.200529: step 9675, loss 0.0826171, acc 0.98
2016-09-06T15:46:29.035494: step 9676, loss 0.0122191, acc 1
2016-09-06T15:46:29.834741: step 9677, loss 0.00886797, acc 1
2016-09-06T15:46:30.671069: step 9678, loss 0.0559092, acc 0.96
2016-09-06T15:46:31.479096: step 9679, loss 0.0402049, acc 0.98
2016-09-06T15:46:32.257887: step 9680, loss 0.012759, acc 1
2016-09-06T15:46:33.088815: step 9681, loss 0.00410131, acc 1
2016-09-06T15:46:33.902359: step 9682, loss 0.0156128, acc 1
2016-09-06T15:46:34.694536: step 9683, loss 0.0587584, acc 0.96
2016-09-06T15:46:35.479865: step 9684, loss 0.0410253, acc 0.98
2016-09-06T15:46:36.320873: step 9685, loss 0.108339, acc 0.96
2016-09-06T15:46:37.091510: step 9686, loss 0.0201515, acc 0.98
2016-09-06T15:46:37.927868: step 9687, loss 0.00246113, acc 1
2016-09-06T15:46:38.751873: step 9688, loss 0.0163995, acc 1
2016-09-06T15:46:39.518658: step 9689, loss 0.0132594, acc 1
2016-09-06T15:46:40.315001: step 9690, loss 0.00552345, acc 1
2016-09-06T15:46:41.109265: step 9691, loss 0.0318901, acc 1
2016-09-06T15:46:41.910958: step 9692, loss 0.013364, acc 1
2016-09-06T15:46:42.709376: step 9693, loss 0.0367745, acc 0.98
2016-09-06T15:46:43.514649: step 9694, loss 0.0059868, acc 1
2016-09-06T15:46:44.332700: step 9695, loss 0.039758, acc 0.98
2016-09-06T15:46:45.128803: step 9696, loss 0.00482892, acc 1
2016-09-06T15:46:45.929032: step 9697, loss 0.00365937, acc 1
2016-09-06T15:46:46.720890: step 9698, loss 0.02392, acc 0.98
2016-09-06T15:46:47.530993: step 9699, loss 0.0153793, acc 1
2016-09-06T15:46:48.356003: step 9700, loss 0.0267974, acc 1

Evaluation:
2016-09-06T15:46:52.094194: step 9700, loss 2.55444, acc 0.745779

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-9700

2016-09-06T15:46:54.024632: step 9701, loss 0.0281234, acc 0.98
2016-09-06T15:46:54.868107: step 9702, loss 0.0198122, acc 1
2016-09-06T15:46:55.694948: step 9703, loss 0.00336737, acc 1
2016-09-06T15:46:56.478767: step 9704, loss 0.00612841, acc 1
2016-09-06T15:46:57.299939: step 9705, loss 0.00378515, acc 1
2016-09-06T15:46:58.101757: step 9706, loss 0.00361796, acc 1
2016-09-06T15:46:58.910303: step 9707, loss 0.0279704, acc 1
2016-09-06T15:46:59.736082: step 9708, loss 0.0209604, acc 1
2016-09-06T15:47:00.567738: step 9709, loss 0.101118, acc 0.94
2016-09-06T15:47:01.372937: step 9710, loss 0.0615693, acc 0.98
2016-09-06T15:47:02.194692: step 9711, loss 0.00361962, acc 1
2016-09-06T15:47:03.001556: step 9712, loss 0.0376087, acc 1
2016-09-06T15:47:03.797711: step 9713, loss 0.0313829, acc 0.98
2016-09-06T15:47:04.603887: step 9714, loss 0.0544978, acc 0.98
2016-09-06T15:47:05.414369: step 9715, loss 0.0441595, acc 1
2016-09-06T15:47:06.228537: step 9716, loss 0.004458, acc 1
2016-09-06T15:47:07.021382: step 9717, loss 0.0243418, acc 1
2016-09-06T15:47:07.833567: step 9718, loss 0.012816, acc 1
2016-09-06T15:47:08.633252: step 9719, loss 0.0121699, acc 1
2016-09-06T15:47:09.432375: step 9720, loss 0.0219197, acc 1
2016-09-06T15:47:10.254889: step 9721, loss 0.00462984, acc 1
2016-09-06T15:47:11.032767: step 9722, loss 0.00327617, acc 1
2016-09-06T15:47:11.835260: step 9723, loss 0.00297141, acc 1
2016-09-06T15:47:12.643718: step 9724, loss 0.00518037, acc 1
2016-09-06T15:47:13.453962: step 9725, loss 0.0222985, acc 0.98
2016-09-06T15:47:14.254437: step 9726, loss 0.00453361, acc 1
2016-09-06T15:47:15.080083: step 9727, loss 0.0274748, acc 0.98
2016-09-06T15:47:15.867636: step 9728, loss 0.00599099, acc 1
2016-09-06T15:47:16.673045: step 9729, loss 0.0100096, acc 1
2016-09-06T15:47:17.496047: step 9730, loss 0.0119108, acc 1
2016-09-06T15:47:18.316245: step 9731, loss 0.00917295, acc 1
2016-09-06T15:47:19.134987: step 9732, loss 0.0028967, acc 1
2016-09-06T15:47:19.954582: step 9733, loss 0.0169618, acc 0.98
2016-09-06T15:47:20.746799: step 9734, loss 0.0188736, acc 1
2016-09-06T15:47:21.511356: step 9735, loss 0.0481228, acc 0.98
2016-09-06T15:47:22.331140: step 9736, loss 0.0131751, acc 1
2016-09-06T15:47:23.126857: step 9737, loss 0.0444044, acc 0.98
2016-09-06T15:47:23.931123: step 9738, loss 0.00497676, acc 1
2016-09-06T15:47:24.756433: step 9739, loss 0.00295381, acc 1
2016-09-06T15:47:25.581395: step 9740, loss 0.0033907, acc 1
2016-09-06T15:47:26.345545: step 9741, loss 0.01668, acc 0.98
2016-09-06T15:47:27.178976: step 9742, loss 0.0301216, acc 0.98
2016-09-06T15:47:27.963920: step 9743, loss 0.0214699, acc 1
2016-09-06T15:47:28.759978: step 9744, loss 0.0309924, acc 0.98
2016-09-06T15:47:29.571522: step 9745, loss 0.0107591, acc 1
2016-09-06T15:47:30.363641: step 9746, loss 0.02202, acc 1
2016-09-06T15:47:31.153324: step 9747, loss 0.0324505, acc 0.98
2016-09-06T15:47:31.982129: step 9748, loss 0.010428, acc 1
2016-09-06T15:47:32.789768: step 9749, loss 0.0175425, acc 1
2016-09-06T15:47:33.591019: step 9750, loss 0.026203, acc 0.98
2016-09-06T15:47:34.410190: step 9751, loss 0.0509091, acc 0.98
2016-09-06T15:47:35.245407: step 9752, loss 0.00247129, acc 1
2016-09-06T15:47:36.026887: step 9753, loss 0.0102777, acc 1
2016-09-06T15:47:36.867082: step 9754, loss 0.0145017, acc 1
2016-09-06T15:47:37.622190: step 9755, loss 0.00573797, acc 1
2016-09-06T15:47:38.414128: step 9756, loss 0.0225484, acc 1
2016-09-06T15:47:39.251196: step 9757, loss 0.00570602, acc 1
2016-09-06T15:47:40.054675: step 9758, loss 0.00447395, acc 1
2016-09-06T15:47:40.891398: step 9759, loss 0.00513725, acc 1
2016-09-06T15:47:41.708382: step 9760, loss 0.00828043, acc 1
2016-09-06T15:47:42.509416: step 9761, loss 0.0308757, acc 0.96
2016-09-06T15:47:43.309939: step 9762, loss 0.0180613, acc 0.98
2016-09-06T15:47:44.116064: step 9763, loss 0.00259415, acc 1
2016-09-06T15:47:44.879981: step 9764, loss 0.0277397, acc 0.98
2016-09-06T15:47:45.677206: step 9765, loss 0.00239405, acc 1
2016-09-06T15:47:46.481434: step 9766, loss 0.0164389, acc 1
2016-09-06T15:47:47.302187: step 9767, loss 0.00972114, acc 1
2016-09-06T15:47:48.116989: step 9768, loss 0.00320844, acc 1
2016-09-06T15:47:48.927128: step 9769, loss 0.011352, acc 1
2016-09-06T15:47:49.712218: step 9770, loss 0.0246697, acc 0.98
2016-09-06T15:47:50.512234: step 9771, loss 0.0489648, acc 0.96
2016-09-06T15:47:51.356149: step 9772, loss 0.00458782, acc 1
2016-09-06T15:47:52.132817: step 9773, loss 0.00976318, acc 1
2016-09-06T15:47:52.942490: step 9774, loss 0.0104999, acc 1
2016-09-06T15:47:53.772074: step 9775, loss 0.02176, acc 1
2016-09-06T15:47:54.603040: step 9776, loss 0.00236399, acc 1
2016-09-06T15:47:55.378139: step 9777, loss 0.00885359, acc 1
2016-09-06T15:47:56.215038: step 9778, loss 0.0209692, acc 0.98
2016-09-06T15:47:56.972397: step 9779, loss 0.00686294, acc 1
2016-09-06T15:47:57.804795: step 9780, loss 0.016368, acc 1
2016-09-06T15:47:58.639303: step 9781, loss 0.00265149, acc 1
2016-09-06T15:47:59.416685: step 9782, loss 0.0157304, acc 1
2016-09-06T15:48:00.241424: step 9783, loss 0.00243679, acc 1
2016-09-06T15:48:01.060628: step 9784, loss 0.00250002, acc 1
2016-09-06T15:48:01.831861: step 9785, loss 0.00245614, acc 1
2016-09-06T15:48:02.636386: step 9786, loss 0.0582501, acc 0.98
2016-09-06T15:48:03.446142: step 9787, loss 0.00427334, acc 1
2016-09-06T15:48:04.245415: step 9788, loss 0.00493644, acc 1
2016-09-06T15:48:05.048693: step 9789, loss 0.0156991, acc 1
2016-09-06T15:48:05.887712: step 9790, loss 0.00311486, acc 1
2016-09-06T15:48:06.671109: step 9791, loss 0.0229801, acc 0.98
2016-09-06T15:48:07.421561: step 9792, loss 0.00319076, acc 1
2016-09-06T15:48:08.243726: step 9793, loss 0.0201241, acc 1
2016-09-06T15:48:09.056699: step 9794, loss 0.0185058, acc 0.98
2016-09-06T15:48:09.883354: step 9795, loss 0.018336, acc 0.98
2016-09-06T15:48:10.679306: step 9796, loss 0.0109355, acc 1
2016-09-06T15:48:11.463766: step 9797, loss 0.028826, acc 0.98
2016-09-06T15:48:12.255756: step 9798, loss 0.0744963, acc 0.98
2016-09-06T15:48:13.090990: step 9799, loss 0.00265159, acc 1
2016-09-06T15:48:13.885401: step 9800, loss 0.019042, acc 0.98

Evaluation:
2016-09-06T15:48:17.597404: step 9800, loss 2.39619, acc 0.745779

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-9800

2016-09-06T15:48:19.488079: step 9801, loss 0.0104633, acc 1
2016-09-06T15:48:20.304076: step 9802, loss 0.013991, acc 1
2016-09-06T15:48:21.113722: step 9803, loss 0.00387213, acc 1
2016-09-06T15:48:21.942974: step 9804, loss 0.00740322, acc 1
2016-09-06T15:48:22.732996: step 9805, loss 0.0473949, acc 0.98
2016-09-06T15:48:23.527208: step 9806, loss 0.00726859, acc 1
2016-09-06T15:48:24.364351: step 9807, loss 0.0235774, acc 0.98
2016-09-06T15:48:25.193545: step 9808, loss 0.0160973, acc 1
2016-09-06T15:48:26.022625: step 9809, loss 0.00885771, acc 1
2016-09-06T15:48:26.936792: step 9810, loss 0.0045355, acc 1
2016-09-06T15:48:27.824661: step 9811, loss 0.0247037, acc 0.98
2016-09-06T15:48:28.625909: step 9812, loss 0.00220884, acc 1
2016-09-06T15:48:29.405992: step 9813, loss 0.00240375, acc 1
2016-09-06T15:48:30.139374: step 9814, loss 0.0032208, acc 1
2016-09-06T15:48:30.862123: step 9815, loss 0.00231684, acc 1
2016-09-06T15:48:31.590775: step 9816, loss 0.0156398, acc 1
2016-09-06T15:48:32.334683: step 9817, loss 0.0610794, acc 0.96
2016-09-06T15:48:33.166297: step 9818, loss 0.0299509, acc 1
2016-09-06T15:48:34.011706: step 9819, loss 0.00244327, acc 1
2016-09-06T15:48:34.844126: step 9820, loss 0.0163268, acc 0.98
2016-09-06T15:48:35.596467: step 9821, loss 0.0112862, acc 1
2016-09-06T15:48:36.401712: step 9822, loss 0.0398183, acc 0.98
2016-09-06T15:48:37.144195: step 9823, loss 0.0027877, acc 1
2016-09-06T15:48:37.915082: step 9824, loss 0.0261929, acc 1
2016-09-06T15:48:38.672293: step 9825, loss 0.00279448, acc 1
2016-09-06T15:48:39.491708: step 9826, loss 0.011512, acc 1
2016-09-06T15:48:40.257571: step 9827, loss 0.00802779, acc 1
2016-09-06T15:48:40.999082: step 9828, loss 0.00222235, acc 1
2016-09-06T15:48:41.731547: step 9829, loss 0.016103, acc 0.98
2016-09-06T15:48:42.504809: step 9830, loss 0.0174533, acc 0.98
2016-09-06T15:48:43.230776: step 9831, loss 0.00431427, acc 1
2016-09-06T15:48:44.026030: step 9832, loss 0.0721938, acc 0.96
2016-09-06T15:48:44.799316: step 9833, loss 0.0538087, acc 0.98
2016-09-06T15:48:45.532144: step 9834, loss 0.0744572, acc 0.98
2016-09-06T15:48:46.272875: step 9835, loss 0.028948, acc 1
2016-09-06T15:48:47.067574: step 9836, loss 0.0522237, acc 0.98
2016-09-06T15:48:47.871516: step 9837, loss 0.0404238, acc 0.98
2016-09-06T15:48:48.639914: step 9838, loss 0.0617878, acc 0.98
2016-09-06T15:48:49.368410: step 9839, loss 0.0826462, acc 0.96
2016-09-06T15:48:50.094218: step 9840, loss 0.0172326, acc 1
2016-09-06T15:48:50.878902: step 9841, loss 0.0212137, acc 1
2016-09-06T15:48:51.660053: step 9842, loss 0.0192723, acc 0.98
2016-09-06T15:48:52.381497: step 9843, loss 0.0378335, acc 0.98
2016-09-06T15:48:53.174037: step 9844, loss 0.0374991, acc 0.98
2016-09-06T15:48:53.937556: step 9845, loss 0.0141688, acc 1
2016-09-06T15:48:54.728801: step 9846, loss 0.0466261, acc 0.98
2016-09-06T15:48:55.508672: step 9847, loss 0.0157906, acc 1
2016-09-06T15:48:56.234834: step 9848, loss 0.0087938, acc 1
2016-09-06T15:48:56.968530: step 9849, loss 0.0404813, acc 0.98
2016-09-06T15:48:57.703368: step 9850, loss 0.00304014, acc 1
2016-09-06T15:48:58.450996: step 9851, loss 0.0201226, acc 1
2016-09-06T15:48:59.245829: step 9852, loss 0.00328906, acc 1
2016-09-06T15:49:00.019298: step 9853, loss 0.00325918, acc 1
2016-09-06T15:49:00.851675: step 9854, loss 0.0265549, acc 0.98
2016-09-06T15:49:01.781612: step 9855, loss 0.00335774, acc 1
2016-09-06T15:49:02.612594: step 9856, loss 0.0365421, acc 0.98
2016-09-06T15:49:03.379658: step 9857, loss 0.0189785, acc 0.98
2016-09-06T15:49:04.179855: step 9858, loss 0.00788614, acc 1
2016-09-06T15:49:04.971101: step 9859, loss 0.0304339, acc 0.98
2016-09-06T15:49:05.766853: step 9860, loss 0.0365654, acc 0.98
2016-09-06T15:49:06.541979: step 9861, loss 0.0446748, acc 0.98
2016-09-06T15:49:07.355643: step 9862, loss 0.00320128, acc 1
2016-09-06T15:49:08.146927: step 9863, loss 0.00306224, acc 1
2016-09-06T15:49:08.897162: step 9864, loss 0.0373158, acc 0.98
2016-09-06T15:49:09.658437: step 9865, loss 0.0313203, acc 1
2016-09-06T15:49:10.476489: step 9866, loss 0.0118047, acc 1
2016-09-06T15:49:11.259037: step 9867, loss 0.0251772, acc 1
2016-09-06T15:49:12.084427: step 9868, loss 0.0105463, acc 1
2016-09-06T15:49:12.899220: step 9869, loss 0.00378497, acc 1
2016-09-06T15:49:13.739941: step 9870, loss 0.124456, acc 0.96
2016-09-06T15:49:14.654562: step 9871, loss 0.0201436, acc 0.98
2016-09-06T15:49:15.465419: step 9872, loss 0.0133015, acc 1
2016-09-06T15:49:16.298847: step 9873, loss 0.0061792, acc 1
2016-09-06T15:49:17.132773: step 9874, loss 0.00763638, acc 1
2016-09-06T15:49:17.973481: step 9875, loss 0.00355838, acc 1
2016-09-06T15:49:18.781198: step 9876, loss 0.00298032, acc 1
2016-09-06T15:49:19.516241: step 9877, loss 0.00304665, acc 1
2016-09-06T15:49:20.297451: step 9878, loss 0.00675598, acc 1
2016-09-06T15:49:21.111777: step 9879, loss 0.011109, acc 1
2016-09-06T15:49:21.914098: step 9880, loss 0.00792606, acc 1
2016-09-06T15:49:22.768817: step 9881, loss 0.0240074, acc 1
2016-09-06T15:49:23.617071: step 9882, loss 0.00349012, acc 1
2016-09-06T15:49:24.406373: step 9883, loss 0.0514182, acc 0.96
2016-09-06T15:49:25.190325: step 9884, loss 0.00291713, acc 1
2016-09-06T15:49:25.995610: step 9885, loss 0.0045237, acc 1
2016-09-06T15:49:26.748864: step 9886, loss 0.002873, acc 1
2016-09-06T15:49:27.548135: step 9887, loss 0.0227481, acc 0.98
2016-09-06T15:49:28.299120: step 9888, loss 0.00283586, acc 1
2016-09-06T15:49:29.067356: step 9889, loss 0.00287403, acc 1
2016-09-06T15:49:29.820140: step 9890, loss 0.00318599, acc 1
2016-09-06T15:49:30.570472: step 9891, loss 0.0115831, acc 1
2016-09-06T15:49:31.406517: step 9892, loss 0.0105897, acc 1
2016-09-06T15:49:32.209102: step 9893, loss 0.00448539, acc 1
2016-09-06T15:49:32.970572: step 9894, loss 0.00697578, acc 1
2016-09-06T15:49:33.743975: step 9895, loss 0.00531401, acc 1
2016-09-06T15:49:34.491951: step 9896, loss 0.00274201, acc 1
2016-09-06T15:49:35.252563: step 9897, loss 0.0192343, acc 0.98
2016-09-06T15:49:35.957658: step 9898, loss 0.00421156, acc 1
2016-09-06T15:49:36.704290: step 9899, loss 0.0166069, acc 1
2016-09-06T15:49:37.453718: step 9900, loss 0.0149171, acc 1

Evaluation:
2016-09-06T15:49:40.779698: step 9900, loss 2.81468, acc 0.747655

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-9900

2016-09-06T15:49:42.691677: step 9901, loss 0.046322, acc 0.98
2016-09-06T15:49:43.445025: step 9902, loss 0.0278359, acc 0.98
2016-09-06T15:49:44.183455: step 9903, loss 0.0728516, acc 0.98
2016-09-06T15:49:44.955949: step 9904, loss 0.0378356, acc 0.98
2016-09-06T15:49:45.721087: step 9905, loss 0.0138728, acc 1
2016-09-06T15:49:46.522060: step 9906, loss 0.0294004, acc 0.98
2016-09-06T15:49:47.318937: step 9907, loss 0.0282209, acc 0.98
2016-09-06T15:49:48.086306: step 9908, loss 0.00492318, acc 1
2016-09-06T15:49:48.910507: step 9909, loss 0.0249383, acc 0.98
2016-09-06T15:49:49.608587: step 9910, loss 0.00463033, acc 1
2016-09-06T15:49:50.457124: step 9911, loss 0.0030529, acc 1
2016-09-06T15:49:51.332002: step 9912, loss 0.0393374, acc 0.98
2016-09-06T15:49:52.198059: step 9913, loss 0.0102175, acc 1
2016-09-06T15:49:52.959388: step 9914, loss 0.00895998, acc 1
2016-09-06T15:49:53.710695: step 9915, loss 0.00225177, acc 1
2016-09-06T15:49:54.439908: step 9916, loss 0.0689822, acc 0.96
2016-09-06T15:49:55.181459: step 9917, loss 0.0149837, acc 1
2016-09-06T15:49:55.922915: step 9918, loss 0.00355786, acc 1
2016-09-06T15:49:56.675045: step 9919, loss 0.0159133, acc 1
2016-09-06T15:49:57.423650: step 9920, loss 0.0198244, acc 1
2016-09-06T15:49:58.156361: step 9921, loss 0.0195921, acc 1
2016-09-06T15:49:58.872999: step 9922, loss 0.0513954, acc 0.94
2016-09-06T15:49:59.597915: step 9923, loss 0.00284053, acc 1
2016-09-06T15:50:00.329537: step 9924, loss 0.0182572, acc 1
2016-09-06T15:50:01.059266: step 9925, loss 0.0160783, acc 1
2016-09-06T15:50:01.799605: step 9926, loss 0.00749014, acc 1
2016-09-06T15:50:02.571841: step 9927, loss 0.033481, acc 0.98
2016-09-06T15:50:03.323998: step 9928, loss 0.068137, acc 0.98
2016-09-06T15:50:04.054510: step 9929, loss 0.0157507, acc 1
2016-09-06T15:50:04.785419: step 9930, loss 0.0393287, acc 0.96
2016-09-06T15:50:05.525834: step 9931, loss 0.00591141, acc 1
2016-09-06T15:50:06.242652: step 9932, loss 0.00424053, acc 1
2016-09-06T15:50:06.991743: step 9933, loss 0.015494, acc 1
2016-09-06T15:50:07.727416: step 9934, loss 0.00600804, acc 1
2016-09-06T15:50:08.444453: step 9935, loss 0.0238538, acc 0.98
2016-09-06T15:50:09.191689: step 9936, loss 0.0617704, acc 0.98
2016-09-06T15:50:09.905980: step 9937, loss 0.00449282, acc 1
2016-09-06T15:50:10.645305: step 9938, loss 0.00746678, acc 1
2016-09-06T15:50:11.383447: step 9939, loss 0.155203, acc 0.98
2016-09-06T15:50:12.102601: step 9940, loss 0.0100113, acc 1
2016-09-06T15:50:12.857057: step 9941, loss 0.0439119, acc 0.98
2016-09-06T15:50:13.582479: step 9942, loss 0.0235451, acc 1
2016-09-06T15:50:14.331052: step 9943, loss 0.00572838, acc 1
2016-09-06T15:50:15.066713: step 9944, loss 0.0269792, acc 0.98
2016-09-06T15:50:15.836539: step 9945, loss 0.0339542, acc 0.98
2016-09-06T15:50:16.577345: step 9946, loss 0.0134001, acc 1
2016-09-06T15:50:17.291982: step 9947, loss 0.0344181, acc 0.98
2016-09-06T15:50:18.024045: step 9948, loss 0.0474704, acc 0.96
2016-09-06T15:50:18.748975: step 9949, loss 0.0187362, acc 0.98
2016-09-06T15:50:19.505132: step 9950, loss 0.0058964, acc 1
2016-09-06T15:50:20.260411: step 9951, loss 0.0406985, acc 0.98
2016-09-06T15:50:21.017803: step 9952, loss 0.022905, acc 0.98
2016-09-06T15:50:21.739617: step 9953, loss 0.0306948, acc 0.98
2016-09-06T15:50:22.484042: step 9954, loss 0.0895575, acc 0.96
2016-09-06T15:50:23.223021: step 9955, loss 0.0710234, acc 0.96
2016-09-06T15:50:24.009271: step 9956, loss 0.00228729, acc 1
2016-09-06T15:50:24.760734: step 9957, loss 0.00225234, acc 1
2016-09-06T15:50:25.500733: step 9958, loss 0.013965, acc 1
2016-09-06T15:50:26.257784: step 9959, loss 0.00253212, acc 1
2016-09-06T15:50:26.991245: step 9960, loss 0.0153717, acc 1
2016-09-06T15:50:27.729613: step 9961, loss 0.038797, acc 1
2016-09-06T15:50:28.451800: step 9962, loss 0.00266505, acc 1
2016-09-06T15:50:29.200987: step 9963, loss 0.029527, acc 1
2016-09-06T15:50:29.953085: step 9964, loss 0.150416, acc 0.98
2016-09-06T15:50:30.701973: step 9965, loss 0.00339757, acc 1
2016-09-06T15:50:31.440722: step 9966, loss 0.0166703, acc 1
2016-09-06T15:50:32.174423: step 9967, loss 0.0222819, acc 0.98
2016-09-06T15:50:32.905844: step 9968, loss 0.00238984, acc 1
2016-09-06T15:50:33.630822: step 9969, loss 0.00517608, acc 1
2016-09-06T15:50:34.348990: step 9970, loss 0.0177424, acc 1
2016-09-06T15:50:35.101120: step 9971, loss 0.0248592, acc 0.98
2016-09-06T15:50:35.817032: step 9972, loss 0.0268506, acc 1
2016-09-06T15:50:36.527585: step 9973, loss 0.0206486, acc 0.98
2016-09-06T15:50:37.235931: step 9974, loss 0.00942246, acc 1
2016-09-06T15:50:37.965918: step 9975, loss 0.00286907, acc 1
2016-09-06T15:50:38.683298: step 9976, loss 0.00296901, acc 1
2016-09-06T15:50:39.429774: step 9977, loss 0.018879, acc 0.98
2016-09-06T15:50:40.194374: step 9978, loss 0.0249117, acc 0.98
2016-09-06T15:50:40.944508: step 9979, loss 0.0622552, acc 0.98
2016-09-06T15:50:41.730652: step 9980, loss 0.00287496, acc 1
2016-09-06T15:50:42.477941: step 9981, loss 0.0702842, acc 0.98
2016-09-06T15:50:43.228120: step 9982, loss 0.138677, acc 0.96
2016-09-06T15:50:43.979600: step 9983, loss 0.0201, acc 1
2016-09-06T15:50:44.676667: step 9984, loss 0.00333725, acc 1
2016-09-06T15:50:45.409951: step 9985, loss 0.0191224, acc 1
2016-09-06T15:50:46.130985: step 9986, loss 0.0266578, acc 0.98
2016-09-06T15:50:46.898067: step 9987, loss 0.00678112, acc 1
2016-09-06T15:50:47.626940: step 9988, loss 0.00761515, acc 1
2016-09-06T15:50:48.346365: step 9989, loss 0.00687784, acc 1
2016-09-06T15:50:49.058834: step 9990, loss 0.0358638, acc 0.98
2016-09-06T15:50:49.793435: step 9991, loss 0.0277414, acc 0.98
2016-09-06T15:50:50.514598: step 9992, loss 0.0225852, acc 0.98
2016-09-06T15:50:51.244809: step 9993, loss 0.0195134, acc 0.98
2016-09-06T15:50:51.978047: step 9994, loss 0.025788, acc 0.98
2016-09-06T15:50:52.735052: step 9995, loss 0.00352017, acc 1
2016-09-06T15:50:53.463833: step 9996, loss 0.0156182, acc 1
2016-09-06T15:50:54.185945: step 9997, loss 0.0631442, acc 0.96
2016-09-06T15:50:54.920091: step 9998, loss 0.0430214, acc 0.98
2016-09-06T15:50:55.666733: step 9999, loss 0.0212329, acc 1
2016-09-06T15:50:56.366146: step 10000, loss 0.00516542, acc 1

Evaluation:
2016-09-06T15:50:59.941422: step 10000, loss 1.76255, acc 0.749531

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-10000

2016-09-06T15:51:01.823015: step 10001, loss 0.0192046, acc 0.98
2016-09-06T15:51:02.575622: step 10002, loss 0.0151264, acc 1
2016-09-06T15:51:03.395809: step 10003, loss 0.00784365, acc 1
2016-09-06T15:51:04.272624: step 10004, loss 0.00557832, acc 1
2016-09-06T15:51:05.129387: step 10005, loss 0.00987063, acc 1
2016-09-06T15:51:05.926106: step 10006, loss 0.00666633, acc 1
2016-09-06T15:51:06.784972: step 10007, loss 0.00282309, acc 1
2016-09-06T15:51:07.694067: step 10008, loss 0.0101211, acc 1
2016-09-06T15:51:08.577523: step 10009, loss 0.00755101, acc 1
2016-09-06T15:51:09.357823: step 10010, loss 0.0177255, acc 1
2016-09-06T15:51:10.196465: step 10011, loss 0.00504132, acc 1
2016-09-06T15:51:11.043646: step 10012, loss 0.0138432, acc 1
2016-09-06T15:51:11.816550: step 10013, loss 0.00686913, acc 1
2016-09-06T15:51:12.613155: step 10014, loss 0.0908882, acc 0.96
2016-09-06T15:51:13.343219: step 10015, loss 0.0030845, acc 1
2016-09-06T15:51:14.091196: step 10016, loss 0.00956525, acc 1
2016-09-06T15:51:14.809319: step 10017, loss 0.00273084, acc 1
2016-09-06T15:51:15.526218: step 10018, loss 0.0325867, acc 0.98
2016-09-06T15:51:16.275638: step 10019, loss 0.00331945, acc 1
2016-09-06T15:51:17.135583: step 10020, loss 0.0031505, acc 1
2016-09-06T15:51:17.896874: step 10021, loss 0.0897088, acc 0.96
2016-09-06T15:51:18.634137: step 10022, loss 0.0149342, acc 1
2016-09-06T15:51:19.418989: step 10023, loss 0.0184986, acc 1
2016-09-06T15:51:20.138832: step 10024, loss 0.0032808, acc 1
2016-09-06T15:51:20.871221: step 10025, loss 0.0178934, acc 1
2016-09-06T15:51:21.757346: step 10026, loss 0.0241806, acc 1
2016-09-06T15:51:22.488240: step 10027, loss 0.0152742, acc 1
2016-09-06T15:51:23.236252: step 10028, loss 0.0217623, acc 0.98
2016-09-06T15:51:23.966527: step 10029, loss 0.0121407, acc 1
2016-09-06T15:51:24.690905: step 10030, loss 0.0458954, acc 0.98
2016-09-06T15:51:25.406999: step 10031, loss 0.0126254, acc 1
2016-09-06T15:51:26.129693: step 10032, loss 0.00545541, acc 1
2016-09-06T15:51:26.921027: step 10033, loss 0.00252715, acc 1
2016-09-06T15:51:27.629462: step 10034, loss 0.0265848, acc 1
2016-09-06T15:51:28.368337: step 10035, loss 0.0323616, acc 0.98
2016-09-06T15:51:29.102806: step 10036, loss 0.00595476, acc 1
2016-09-06T15:51:29.831635: step 10037, loss 0.00565653, acc 1
2016-09-06T15:51:30.586750: step 10038, loss 0.0111996, acc 1
2016-09-06T15:51:31.315131: step 10039, loss 0.00234639, acc 1
2016-09-06T15:51:32.044135: step 10040, loss 0.00265144, acc 1
2016-09-06T15:51:32.798173: step 10041, loss 0.0116164, acc 1
2016-09-06T15:51:33.532071: step 10042, loss 0.0229178, acc 0.98
2016-09-06T15:51:34.272766: step 10043, loss 0.117649, acc 0.94
2016-09-06T15:51:34.984243: step 10044, loss 0.00295959, acc 1
2016-09-06T15:51:35.718863: step 10045, loss 0.00480045, acc 1
2016-09-06T15:51:36.446827: step 10046, loss 0.0104702, acc 1
2016-09-06T15:51:37.158714: step 10047, loss 0.00241565, acc 1
2016-09-06T15:51:37.912956: step 10048, loss 0.0643618, acc 0.98
2016-09-06T15:51:38.668172: step 10049, loss 0.00264468, acc 1
2016-09-06T15:51:39.412756: step 10050, loss 0.0112427, acc 1
2016-09-06T15:51:40.131551: step 10051, loss 0.00222517, acc 1
2016-09-06T15:51:40.865877: step 10052, loss 0.0247971, acc 0.98
2016-09-06T15:51:41.589746: step 10053, loss 0.00619254, acc 1
2016-09-06T15:51:42.376563: step 10054, loss 0.0161808, acc 1
2016-09-06T15:51:43.123969: step 10055, loss 0.00435089, acc 1
2016-09-06T15:51:43.889543: step 10056, loss 0.00459125, acc 1
2016-09-06T15:51:44.607137: step 10057, loss 0.0297579, acc 0.98
2016-09-06T15:51:45.334555: step 10058, loss 0.00231881, acc 1
2016-09-06T15:51:46.073031: step 10059, loss 0.0216253, acc 0.98
2016-09-06T15:51:46.801075: step 10060, loss 0.0250822, acc 0.98
2016-09-06T15:51:47.548774: step 10061, loss 0.00372191, acc 1
2016-09-06T15:51:48.283083: step 10062, loss 0.0130502, acc 1
2016-09-06T15:51:49.095132: step 10063, loss 0.00422991, acc 1
2016-09-06T15:51:49.949527: step 10064, loss 0.0055138, acc 1
2016-09-06T15:51:50.724547: step 10065, loss 0.00785978, acc 1
2016-09-06T15:51:51.440738: step 10066, loss 0.0041403, acc 1
2016-09-06T15:51:52.164026: step 10067, loss 0.026585, acc 0.98
2016-09-06T15:51:52.893438: step 10068, loss 0.0173512, acc 1
2016-09-06T15:51:53.619642: step 10069, loss 0.0024233, acc 1
2016-09-06T15:51:54.354595: step 10070, loss 0.00681279, acc 1
2016-09-06T15:51:55.090953: step 10071, loss 0.00295828, acc 1
2016-09-06T15:51:55.925144: step 10072, loss 0.0134768, acc 1
2016-09-06T15:51:56.719170: step 10073, loss 0.0194551, acc 0.98
2016-09-06T15:51:57.492453: step 10074, loss 0.0134714, acc 1
2016-09-06T15:51:58.382641: step 10075, loss 0.016679, acc 0.98
2016-09-06T15:51:59.237163: step 10076, loss 0.00411857, acc 1
2016-09-06T15:52:00.027408: step 10077, loss 0.00232072, acc 1
2016-09-06T15:52:00.815890: step 10078, loss 0.0544791, acc 0.98
2016-09-06T15:52:01.593857: step 10079, loss 0.0866605, acc 0.98
2016-09-06T15:52:02.371183: step 10080, loss 0.224397, acc 0.98
2016-09-06T15:52:03.200835: step 10081, loss 0.0656478, acc 0.98
2016-09-06T15:52:03.950080: step 10082, loss 0.105501, acc 0.96
2016-09-06T15:52:04.687205: step 10083, loss 0.0110213, acc 1
2016-09-06T15:52:05.384150: step 10084, loss 0.034766, acc 0.98
2016-09-06T15:52:06.106300: step 10085, loss 0.0242665, acc 1
2016-09-06T15:52:06.858440: step 10086, loss 0.103262, acc 0.98
2016-09-06T15:52:07.634787: step 10087, loss 0.241888, acc 0.96
2016-09-06T15:52:08.408946: step 10088, loss 0.0104211, acc 1
2016-09-06T15:52:09.188588: step 10089, loss 0.0247318, acc 1
2016-09-06T15:52:09.895364: step 10090, loss 0.0060236, acc 1
2016-09-06T15:52:10.652048: step 10091, loss 0.0174369, acc 1
2016-09-06T15:52:11.421463: step 10092, loss 0.0304648, acc 0.98
2016-09-06T15:52:12.171167: step 10093, loss 0.109984, acc 0.94
2016-09-06T15:52:12.973237: step 10094, loss 0.102794, acc 0.96
2016-09-06T15:52:13.844109: step 10095, loss 0.00853115, acc 1
2016-09-06T15:52:14.786024: step 10096, loss 0.00743072, acc 1
2016-09-06T15:52:15.558279: step 10097, loss 0.00587651, acc 1
2016-09-06T15:52:16.360394: step 10098, loss 0.0105989, acc 1
2016-09-06T15:52:17.108771: step 10099, loss 0.0365948, acc 1
2016-09-06T15:52:17.981264: step 10100, loss 0.0646449, acc 0.98

Evaluation:
2016-09-06T15:52:21.595879: step 10100, loss 2.09586, acc 0.730769

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-10100

2016-09-06T15:52:23.385487: step 10101, loss 0.00970437, acc 1
2016-09-06T15:52:24.134660: step 10102, loss 0.0180422, acc 1
2016-09-06T15:52:24.856400: step 10103, loss 0.0161604, acc 1
2016-09-06T15:52:25.564098: step 10104, loss 0.00787816, acc 1
2016-09-06T15:52:26.305866: step 10105, loss 0.00772259, acc 1
2016-09-06T15:52:27.061556: step 10106, loss 0.00881671, acc 1
2016-09-06T15:52:27.786953: step 10107, loss 0.0178005, acc 1
2016-09-06T15:52:28.547631: step 10108, loss 0.0269964, acc 0.98
2016-09-06T15:52:29.279813: step 10109, loss 0.0463696, acc 0.96
2016-09-06T15:52:30.001114: step 10110, loss 0.0106655, acc 1
2016-09-06T15:52:30.753465: step 10111, loss 0.0228673, acc 0.98
2016-09-06T15:52:31.508109: step 10112, loss 0.0112084, acc 1
2016-09-06T15:52:32.252451: step 10113, loss 0.00770818, acc 1
2016-09-06T15:52:32.989210: step 10114, loss 0.0181991, acc 1
2016-09-06T15:52:33.715414: step 10115, loss 0.030472, acc 1
2016-09-06T15:52:34.508525: step 10116, loss 0.0104143, acc 1
2016-09-06T15:52:35.224519: step 10117, loss 0.00807508, acc 1
2016-09-06T15:52:36.087175: step 10118, loss 0.0389783, acc 0.98
2016-09-06T15:52:36.800051: step 10119, loss 0.0178615, acc 1
2016-09-06T15:52:37.556748: step 10120, loss 0.0237071, acc 0.98
2016-09-06T15:52:38.310010: step 10121, loss 0.0176593, acc 1
2016-09-06T15:52:39.057121: step 10122, loss 0.023152, acc 1
2016-09-06T15:52:39.797547: step 10123, loss 0.00775125, acc 1
2016-09-06T15:52:40.529566: step 10124, loss 0.00972987, acc 1
2016-09-06T15:52:41.325367: step 10125, loss 0.0216314, acc 0.98
2016-09-06T15:52:42.079027: step 10126, loss 0.0157295, acc 1
2016-09-06T15:52:42.830076: step 10127, loss 0.00683783, acc 1
2016-09-06T15:52:43.564239: step 10128, loss 0.0470017, acc 0.98
2016-09-06T15:52:44.297694: step 10129, loss 0.0172289, acc 1
2016-09-06T15:52:45.025463: step 10130, loss 0.0238519, acc 0.98
2016-09-06T15:52:45.769742: step 10131, loss 0.0762791, acc 0.94
2016-09-06T15:52:46.551683: step 10132, loss 0.0144309, acc 1
2016-09-06T15:52:47.256971: step 10133, loss 0.0203606, acc 1
2016-09-06T15:52:47.981341: step 10134, loss 0.0112931, acc 1
2016-09-06T15:52:48.701832: step 10135, loss 0.00667444, acc 1
2016-09-06T15:52:49.426998: step 10136, loss 0.00647251, acc 1
2016-09-06T15:52:50.140220: step 10137, loss 0.0403862, acc 0.98
2016-09-06T15:52:50.878898: step 10138, loss 0.00629567, acc 1
2016-09-06T15:52:51.651831: step 10139, loss 0.0229514, acc 0.98
2016-09-06T15:52:52.424318: step 10140, loss 0.0163035, acc 1
2016-09-06T15:52:53.154636: step 10141, loss 0.00719624, acc 1
2016-09-06T15:52:53.906286: step 10142, loss 0.00797156, acc 1
2016-09-06T15:52:54.660676: step 10143, loss 0.0473944, acc 0.98
2016-09-06T15:52:55.392177: step 10144, loss 0.0387222, acc 0.96
2016-09-06T15:52:56.172883: step 10145, loss 0.0062346, acc 1
2016-09-06T15:52:56.902359: step 10146, loss 0.0163215, acc 1
2016-09-06T15:52:57.707289: step 10147, loss 0.00571475, acc 1
2016-09-06T15:52:58.433495: step 10148, loss 0.0683522, acc 0.98
2016-09-06T15:52:59.178160: step 10149, loss 0.106464, acc 0.96
2016-09-06T15:52:59.895948: step 10150, loss 0.0128944, acc 1
2016-09-06T15:53:00.621372: step 10151, loss 0.00602969, acc 1
2016-09-06T15:53:01.332427: step 10152, loss 0.00562005, acc 1
2016-09-06T15:53:02.077961: step 10153, loss 0.0286416, acc 0.98
2016-09-06T15:53:02.808319: step 10154, loss 0.00526717, acc 1
2016-09-06T15:53:03.537663: step 10155, loss 0.0528344, acc 0.98
2016-09-06T15:53:04.297409: step 10156, loss 0.0816621, acc 0.98
2016-09-06T15:53:05.054871: step 10157, loss 0.0452656, acc 0.98
2016-09-06T15:53:05.767969: step 10158, loss 0.019046, acc 0.98
2016-09-06T15:53:06.555882: step 10159, loss 0.074249, acc 0.96
2016-09-06T15:53:07.291288: step 10160, loss 0.00596307, acc 1
2016-09-06T15:53:08.036885: step 10161, loss 0.0676159, acc 0.94
2016-09-06T15:53:08.882195: step 10162, loss 0.0464354, acc 0.98
2016-09-06T15:53:09.710059: step 10163, loss 0.0167361, acc 1
2016-09-06T15:53:10.443478: step 10164, loss 0.0231299, acc 0.98
2016-09-06T15:53:11.197266: step 10165, loss 0.035329, acc 0.98
2016-09-06T15:53:11.933931: step 10166, loss 0.0232951, acc 1
2016-09-06T15:53:12.677337: step 10167, loss 0.00815608, acc 1
2016-09-06T15:53:13.466805: step 10168, loss 0.164944, acc 0.96
2016-09-06T15:53:14.307207: step 10169, loss 0.0129023, acc 1
2016-09-06T15:53:15.106030: step 10170, loss 0.0172814, acc 1
2016-09-06T15:53:15.895194: step 10171, loss 0.0165966, acc 1
2016-09-06T15:53:16.650082: step 10172, loss 0.00553677, acc 1
2016-09-06T15:53:17.513280: step 10173, loss 0.079091, acc 0.94
2016-09-06T15:53:18.292313: step 10174, loss 0.0133183, acc 1
2016-09-06T15:53:19.093404: step 10175, loss 0.0491267, acc 0.98
2016-09-06T15:53:19.831860: step 10176, loss 0.00627661, acc 1
2016-09-06T15:53:20.642919: step 10177, loss 0.0156223, acc 1
2016-09-06T15:53:21.372471: step 10178, loss 0.0516051, acc 1
2016-09-06T15:53:22.114710: step 10179, loss 0.0210719, acc 1
2016-09-06T15:53:22.902854: step 10180, loss 0.0159092, acc 1
2016-09-06T15:53:23.642007: step 10181, loss 0.0240515, acc 0.98
2016-09-06T15:53:24.437821: step 10182, loss 0.00504787, acc 1
2016-09-06T15:53:25.247669: step 10183, loss 0.0217388, acc 1
2016-09-06T15:53:26.075221: step 10184, loss 0.0217249, acc 0.98
2016-09-06T15:53:26.852887: step 10185, loss 0.0249971, acc 0.98
2016-09-06T15:53:27.648989: step 10186, loss 0.015098, acc 1
2016-09-06T15:53:28.418577: step 10187, loss 0.00627932, acc 1
2016-09-06T15:53:29.289671: step 10188, loss 0.0065194, acc 1
2016-09-06T15:53:30.203755: step 10189, loss 0.0311401, acc 0.98
2016-09-06T15:53:31.061304: step 10190, loss 0.0390419, acc 0.98
2016-09-06T15:53:31.831097: step 10191, loss 0.0127601, acc 1
2016-09-06T15:53:32.636184: step 10192, loss 0.0264387, acc 0.98
2016-09-06T15:53:33.437521: step 10193, loss 0.00492876, acc 1
2016-09-06T15:53:34.204890: step 10194, loss 0.00522004, acc 1
2016-09-06T15:53:34.977775: step 10195, loss 0.00457136, acc 1
2016-09-06T15:53:35.734092: step 10196, loss 0.00433345, acc 1
2016-09-06T15:53:36.493432: step 10197, loss 0.00883518, acc 1
2016-09-06T15:53:37.283193: step 10198, loss 0.0192671, acc 0.98
2016-09-06T15:53:38.061490: step 10199, loss 0.059545, acc 0.98
2016-09-06T15:53:38.964392: step 10200, loss 0.00430404, acc 1

Evaluation:
2016-09-06T15:53:42.476032: step 10200, loss 1.92304, acc 0.752345

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-10200

2016-09-06T15:53:44.357328: step 10201, loss 0.0395663, acc 0.98
2016-09-06T15:53:45.084575: step 10202, loss 0.0260434, acc 0.98
2016-09-06T15:53:45.808149: step 10203, loss 0.00375127, acc 1
2016-09-06T15:53:46.584342: step 10204, loss 0.00411166, acc 1
2016-09-06T15:53:47.384564: step 10205, loss 0.00834098, acc 1
2016-09-06T15:53:48.182502: step 10206, loss 0.00483226, acc 1
2016-09-06T15:53:48.952323: step 10207, loss 0.00455083, acc 1
2016-09-06T15:53:49.875104: step 10208, loss 0.00997602, acc 1
2016-09-06T15:53:50.749986: step 10209, loss 0.024759, acc 0.98
2016-09-06T15:53:51.584010: step 10210, loss 0.0315807, acc 0.98
2016-09-06T15:53:52.377941: step 10211, loss 0.00357139, acc 1
2016-09-06T15:53:53.120517: step 10212, loss 0.00380135, acc 1
2016-09-06T15:53:53.948290: step 10213, loss 0.00645954, acc 1
2016-09-06T15:53:54.735930: step 10214, loss 0.0179065, acc 1
2016-09-06T15:53:55.540072: step 10215, loss 0.00552216, acc 1
2016-09-06T15:53:56.314211: step 10216, loss 0.00885292, acc 1
2016-09-06T15:53:57.045333: step 10217, loss 0.0194553, acc 1
2016-09-06T15:53:57.790730: step 10218, loss 0.00940267, acc 1
2016-09-06T15:53:58.653490: step 10219, loss 0.0254881, acc 0.98
2016-09-06T15:53:59.413887: step 10220, loss 0.00528796, acc 1
2016-09-06T15:54:00.187819: step 10221, loss 0.0061413, acc 1
2016-09-06T15:54:01.023200: step 10222, loss 0.0634916, acc 0.98
2016-09-06T15:54:01.759196: step 10223, loss 0.016391, acc 1
2016-09-06T15:54:02.542521: step 10224, loss 0.0249097, acc 0.98
2016-09-06T15:54:03.342195: step 10225, loss 0.0108577, acc 1
2016-09-06T15:54:04.254380: step 10226, loss 0.0872085, acc 0.98
2016-09-06T15:54:05.136317: step 10227, loss 0.0474696, acc 0.98
2016-09-06T15:54:05.966337: step 10228, loss 0.0193353, acc 0.98
2016-09-06T15:54:06.696491: step 10229, loss 0.00396146, acc 1
2016-09-06T15:54:07.455439: step 10230, loss 0.0148555, acc 1
2016-09-06T15:54:08.296893: step 10231, loss 0.0127766, acc 1
2016-09-06T15:54:09.083743: step 10232, loss 0.033318, acc 0.96
2016-09-06T15:54:09.878337: step 10233, loss 0.0215096, acc 0.98
2016-09-06T15:54:10.597492: step 10234, loss 0.0156363, acc 1
2016-09-06T15:54:11.320277: step 10235, loss 0.0188865, acc 0.98
2016-09-06T15:54:12.131487: step 10236, loss 0.0129849, acc 1
2016-09-06T15:54:12.879233: step 10237, loss 0.00820575, acc 1
2016-09-06T15:54:13.722184: step 10238, loss 0.193942, acc 0.94
2016-09-06T15:54:14.487379: step 10239, loss 0.0072817, acc 1
2016-09-06T15:54:15.234944: step 10240, loss 0.0259145, acc 0.98
2016-09-06T15:54:15.947718: step 10241, loss 0.0290135, acc 0.98
2016-09-06T15:54:16.728308: step 10242, loss 0.0238799, acc 0.98
2016-09-06T15:54:17.483221: step 10243, loss 0.0135053, acc 1
2016-09-06T15:54:18.237379: step 10244, loss 0.0682132, acc 0.98
2016-09-06T15:54:18.990385: step 10245, loss 0.0450229, acc 0.98
2016-09-06T15:54:19.744209: step 10246, loss 0.0114873, acc 1
2016-09-06T15:54:20.509935: step 10247, loss 0.0203861, acc 1
2016-09-06T15:54:21.244188: step 10248, loss 0.00332599, acc 1
2016-09-06T15:54:21.973549: step 10249, loss 0.0699328, acc 0.96
2016-09-06T15:54:22.759720: step 10250, loss 0.00387984, acc 1
2016-09-06T15:54:23.610573: step 10251, loss 0.0317089, acc 1
2016-09-06T15:54:24.368561: step 10252, loss 0.0337611, acc 0.98
2016-09-06T15:54:25.123002: step 10253, loss 0.0127292, acc 1
2016-09-06T15:54:25.860999: step 10254, loss 0.00914411, acc 1
2016-09-06T15:54:26.602584: step 10255, loss 0.0277429, acc 0.98
2016-09-06T15:54:27.411390: step 10256, loss 0.0116362, acc 1
2016-09-06T15:54:28.196461: step 10257, loss 0.00349288, acc 1
2016-09-06T15:54:28.950391: step 10258, loss 0.00293869, acc 1
2016-09-06T15:54:29.684357: step 10259, loss 0.0295094, acc 0.98
2016-09-06T15:54:30.449823: step 10260, loss 0.00267765, acc 1
2016-09-06T15:54:31.232911: step 10261, loss 0.00472398, acc 1
2016-09-06T15:54:32.002782: step 10262, loss 0.034363, acc 0.98
2016-09-06T15:54:32.722871: step 10263, loss 0.00236829, acc 1
2016-09-06T15:54:33.460063: step 10264, loss 0.0162613, acc 1
2016-09-06T15:54:34.179965: step 10265, loss 0.0127168, acc 1
2016-09-06T15:54:34.924714: step 10266, loss 0.00270318, acc 1
2016-09-06T15:54:35.653124: step 10267, loss 0.0247134, acc 0.98
2016-09-06T15:54:36.378791: step 10268, loss 0.00561926, acc 1
2016-09-06T15:54:37.129526: step 10269, loss 0.00686707, acc 1
2016-09-06T15:54:37.921939: step 10270, loss 0.00578067, acc 1
2016-09-06T15:54:38.650431: step 10271, loss 0.0562383, acc 0.98
2016-09-06T15:54:39.449978: step 10272, loss 0.00232184, acc 1
2016-09-06T15:54:40.186774: step 10273, loss 0.00924456, acc 1
2016-09-06T15:54:40.989603: step 10274, loss 0.0652376, acc 0.94
2016-09-06T15:54:41.960765: step 10275, loss 0.00363927, acc 1
2016-09-06T15:54:42.770362: step 10276, loss 0.0233891, acc 0.98
2016-09-06T15:54:43.670185: step 10277, loss 0.00361916, acc 1
2016-09-06T15:54:44.467016: step 10278, loss 0.00401565, acc 1
2016-09-06T15:54:45.330475: step 10279, loss 0.0481165, acc 0.98
2016-09-06T15:54:46.184124: step 10280, loss 0.00321317, acc 1
2016-09-06T15:54:46.944231: step 10281, loss 0.0162344, acc 0.98
2016-09-06T15:54:47.726752: step 10282, loss 0.00289472, acc 1
2016-09-06T15:54:48.474830: step 10283, loss 0.0174617, acc 1
2016-09-06T15:54:49.268251: step 10284, loss 0.0411311, acc 0.98
2016-09-06T15:54:50.106887: step 10285, loss 0.0963856, acc 0.96
2016-09-06T15:54:50.959816: step 10286, loss 0.188834, acc 0.9
2016-09-06T15:54:51.965124: step 10287, loss 0.00270183, acc 1
2016-09-06T15:54:53.172394: step 10288, loss 0.0248933, acc 1
2016-09-06T15:54:54.281008: step 10289, loss 0.0234582, acc 0.98
2016-09-06T15:54:55.343222: step 10290, loss 0.00898224, acc 1
2016-09-06T15:54:56.158482: step 10291, loss 0.0153147, acc 1
2016-09-06T15:54:57.292559: step 10292, loss 0.031808, acc 0.98
2016-09-06T15:54:58.421788: step 10293, loss 0.0812347, acc 0.94
2016-09-06T15:54:59.408745: step 10294, loss 0.00445271, acc 1
2016-09-06T15:55:00.289510: step 10295, loss 0.00850327, acc 1
2016-09-06T15:55:01.215579: step 10296, loss 0.0269301, acc 0.98
2016-09-06T15:55:02.199502: step 10297, loss 0.00361735, acc 1
2016-09-06T15:55:03.107883: step 10298, loss 0.0193219, acc 0.98
2016-09-06T15:55:04.111499: step 10299, loss 0.00635065, acc 1
2016-09-06T15:55:05.050765: step 10300, loss 0.0269298, acc 0.98

Evaluation:
2016-09-06T15:55:10.190485: step 10300, loss 1.55901, acc 0.745779

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-10300

2016-09-06T15:55:12.530114: step 10301, loss 0.0212103, acc 1
2016-09-06T15:55:13.514512: step 10302, loss 0.00940648, acc 1
2016-09-06T15:55:14.560517: step 10303, loss 0.0139119, acc 1
2016-09-06T15:55:15.471346: step 10304, loss 0.0500643, acc 0.98
2016-09-06T15:55:16.563605: step 10305, loss 0.0462176, acc 0.98
2016-09-06T15:55:17.456167: step 10306, loss 0.0733335, acc 0.98
2016-09-06T15:55:18.436742: step 10307, loss 0.0169087, acc 1
2016-09-06T15:55:19.627856: step 10308, loss 0.00391403, acc 1
2016-09-06T15:55:20.713348: step 10309, loss 0.00889804, acc 1
2016-09-06T15:55:21.710071: step 10310, loss 0.0113557, acc 1
2016-09-06T15:55:22.643607: step 10311, loss 0.0231754, acc 0.98
2016-09-06T15:55:23.629888: step 10312, loss 0.019553, acc 1
2016-09-06T15:55:24.554696: step 10313, loss 0.0167241, acc 0.98
2016-09-06T15:55:25.468146: step 10314, loss 0.0366512, acc 0.98
2016-09-06T15:55:26.382718: step 10315, loss 0.015632, acc 1
2016-09-06T15:55:27.283288: step 10316, loss 0.0166077, acc 1
2016-09-06T15:55:28.199249: step 10317, loss 0.0361085, acc 1
2016-09-06T15:55:29.126866: step 10318, loss 0.00260894, acc 1
2016-09-06T15:55:30.040372: step 10319, loss 0.00378578, acc 1
2016-09-06T15:55:30.955792: step 10320, loss 0.0247913, acc 0.98
2016-09-06T15:55:31.862785: step 10321, loss 0.019008, acc 0.98
2016-09-06T15:55:32.759734: step 10322, loss 0.0323493, acc 0.98
2016-09-06T15:55:33.682328: step 10323, loss 0.0380065, acc 0.98
2016-09-06T15:55:34.543841: step 10324, loss 0.0377685, acc 0.98
2016-09-06T15:55:35.455752: step 10325, loss 0.0175498, acc 0.98
2016-09-06T15:55:36.369144: step 10326, loss 0.0410024, acc 0.98
2016-09-06T15:55:37.257083: step 10327, loss 0.0026679, acc 1
2016-09-06T15:55:38.157999: step 10328, loss 0.00749756, acc 1
2016-09-06T15:55:39.069534: step 10329, loss 0.0211338, acc 1
2016-09-06T15:55:39.991553: step 10330, loss 0.00397914, acc 1
2016-09-06T15:55:40.902492: step 10331, loss 0.00414586, acc 1
2016-09-06T15:55:41.709198: step 10332, loss 0.0133172, acc 1
2016-09-06T15:55:42.621249: step 10333, loss 0.0218332, acc 0.98
2016-09-06T15:55:43.479306: step 10334, loss 0.0193167, acc 0.98
2016-09-06T15:55:44.392860: step 10335, loss 0.0133343, acc 1
2016-09-06T15:55:45.281397: step 10336, loss 0.0304058, acc 0.98
2016-09-06T15:55:46.106192: step 10337, loss 0.0116817, acc 1
2016-09-06T15:55:46.897779: step 10338, loss 0.0024983, acc 1
2016-09-06T15:55:47.769534: step 10339, loss 0.00443719, acc 1
2016-09-06T15:55:48.699310: step 10340, loss 0.0400176, acc 0.96
2016-09-06T15:55:49.590518: step 10341, loss 0.00263963, acc 1
2016-09-06T15:55:50.484464: step 10342, loss 0.0215173, acc 0.98
2016-09-06T15:55:51.335035: step 10343, loss 0.0029263, acc 1
2016-09-06T15:55:52.157083: step 10344, loss 0.00423979, acc 1
2016-09-06T15:55:53.070905: step 10345, loss 0.0360156, acc 1
2016-09-06T15:55:53.975062: step 10346, loss 0.111723, acc 0.98
2016-09-06T15:55:54.835331: step 10347, loss 0.010108, acc 1
2016-09-06T15:55:55.697017: step 10348, loss 0.0429435, acc 0.98
2016-09-06T15:55:56.631536: step 10349, loss 0.00659821, acc 1
2016-09-06T15:55:57.676439: step 10350, loss 0.0158603, acc 1
2016-09-06T15:55:58.679439: step 10351, loss 0.0811314, acc 0.96
2016-09-06T15:55:59.719402: step 10352, loss 0.00404873, acc 1
2016-09-06T15:56:00.754380: step 10353, loss 0.0197696, acc 1
2016-09-06T15:56:01.838076: step 10354, loss 0.0520547, acc 0.96
2016-09-06T15:56:02.825252: step 10355, loss 0.0211518, acc 0.98
2016-09-06T15:56:03.932072: step 10356, loss 0.0294685, acc 0.98
2016-09-06T15:56:04.962388: step 10357, loss 0.00231406, acc 1
2016-09-06T15:56:06.051706: step 10358, loss 0.00739805, acc 1
2016-09-06T15:56:07.155910: step 10359, loss 0.00215947, acc 1
2016-09-06T15:56:08.175401: step 10360, loss 0.0246537, acc 0.98
2016-09-06T15:56:09.127710: step 10361, loss 0.00548558, acc 1
2016-09-06T15:56:10.236073: step 10362, loss 0.0165918, acc 1
2016-09-06T15:56:11.317111: step 10363, loss 0.168602, acc 0.96
2016-09-06T15:56:12.314949: step 10364, loss 0.00205081, acc 1
2016-09-06T15:56:13.344458: step 10365, loss 0.0592239, acc 0.98
2016-09-06T15:56:14.316403: step 10366, loss 0.00508615, acc 1
2016-09-06T15:56:15.397379: step 10367, loss 0.00841905, acc 1
2016-09-06T15:56:16.520925: step 10368, loss 0.00317303, acc 1
2016-09-06T15:56:17.615891: step 10369, loss 0.00632579, acc 1
2016-09-06T15:56:18.754471: step 10370, loss 0.0312068, acc 0.98
2016-09-06T15:56:19.763673: step 10371, loss 0.0109622, acc 1
2016-09-06T15:56:20.725501: step 10372, loss 0.0234618, acc 1
2016-09-06T15:56:21.730377: step 10373, loss 0.0132661, acc 1
2016-09-06T15:56:22.659550: step 10374, loss 0.0309824, acc 1
2016-09-06T15:56:23.558299: step 10375, loss 0.019362, acc 0.98
2016-09-06T15:56:24.446362: step 10376, loss 0.0116337, acc 1
2016-09-06T15:56:25.323409: step 10377, loss 0.0166605, acc 1
2016-09-06T15:56:26.193382: step 10378, loss 0.0529804, acc 0.96
2016-09-06T15:56:27.050014: step 10379, loss 0.00709687, acc 1
2016-09-06T15:56:27.953115: step 10380, loss 0.0141852, acc 1
2016-09-06T15:56:28.833617: step 10381, loss 0.0103569, acc 1
2016-09-06T15:56:29.740432: step 10382, loss 0.0243026, acc 1
2016-09-06T15:56:30.662928: step 10383, loss 0.0166436, acc 1
2016-09-06T15:56:31.581076: step 10384, loss 0.00266832, acc 1
2016-09-06T15:56:32.475787: step 10385, loss 0.00675005, acc 1
2016-09-06T15:56:33.391819: step 10386, loss 0.00540676, acc 1
2016-09-06T15:56:34.289956: step 10387, loss 0.00977388, acc 1
2016-09-06T15:56:35.192443: step 10388, loss 0.00239928, acc 1
2016-09-06T15:56:36.085517: step 10389, loss 0.00245142, acc 1
2016-09-06T15:56:36.941239: step 10390, loss 0.0228439, acc 0.98
2016-09-06T15:56:37.855440: step 10391, loss 0.0133811, acc 1
2016-09-06T15:56:38.703653: step 10392, loss 0.0379693, acc 1
2016-09-06T15:56:39.585857: step 10393, loss 0.00299557, acc 1
2016-09-06T15:56:40.489858: step 10394, loss 0.0046424, acc 1
2016-09-06T15:56:41.384317: step 10395, loss 0.00419115, acc 1
2016-09-06T15:56:42.275898: step 10396, loss 0.00290272, acc 1
2016-09-06T15:56:43.134760: step 10397, loss 0.0242102, acc 1
2016-09-06T15:56:43.992542: step 10398, loss 0.00398134, acc 1
2016-09-06T15:56:44.858658: step 10399, loss 0.0279922, acc 0.98
2016-09-06T15:56:45.757359: step 10400, loss 0.0041488, acc 1

Evaluation:
2016-09-06T15:56:49.991041: step 10400, loss 2.0551, acc 0.742026

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-10400

2016-09-06T15:56:51.950394: step 10401, loss 0.0377998, acc 0.98
2016-09-06T15:56:52.805981: step 10402, loss 0.01789, acc 1
2016-09-06T15:56:53.689845: step 10403, loss 0.00836637, acc 1
2016-09-06T15:56:54.516257: step 10404, loss 0.00667052, acc 1
2016-09-06T15:56:55.423349: step 10405, loss 0.0227073, acc 1
2016-09-06T15:56:56.305010: step 10406, loss 0.0221589, acc 0.98
2016-09-06T15:56:57.187997: step 10407, loss 0.0428102, acc 0.98
2016-09-06T15:56:58.067951: step 10408, loss 0.143752, acc 0.96
2016-09-06T15:56:58.954206: step 10409, loss 0.00928772, acc 1
2016-09-06T15:56:59.821976: step 10410, loss 0.00313964, acc 1
2016-09-06T15:57:00.722843: step 10411, loss 0.0302061, acc 1
2016-09-06T15:57:01.619504: step 10412, loss 0.0102158, acc 1
2016-09-06T15:57:02.696034: step 10413, loss 0.00260169, acc 1
2016-09-06T15:57:03.853112: step 10414, loss 0.00308504, acc 1
2016-09-06T15:57:04.921026: step 10415, loss 0.0671119, acc 0.98
2016-09-06T15:57:05.963342: step 10416, loss 0.00547449, acc 1
2016-09-06T15:57:07.015494: step 10417, loss 0.00653719, acc 1
2016-09-06T15:57:08.185601: step 10418, loss 0.0290054, acc 0.98
2016-09-06T15:57:09.120935: step 10419, loss 0.0203105, acc 0.98
2016-09-06T15:57:10.193163: step 10420, loss 0.00533935, acc 1
2016-09-06T15:57:11.155607: step 10421, loss 0.0123688, acc 1
2016-09-06T15:57:12.187432: step 10422, loss 0.00522056, acc 1
2016-09-06T15:57:13.260537: step 10423, loss 0.0187125, acc 0.98
2016-09-06T15:57:14.394032: step 10424, loss 0.015784, acc 1
2016-09-06T15:57:15.484677: step 10425, loss 0.00393037, acc 1
2016-09-06T15:57:16.480726: step 10426, loss 0.0153092, acc 1
2016-09-06T15:57:17.540930: step 10427, loss 0.0535231, acc 0.96
2016-09-06T15:57:18.611884: step 10428, loss 0.0329226, acc 0.98
2016-09-06T15:57:19.684075: step 10429, loss 0.00237532, acc 1
2016-09-06T15:57:20.650506: step 10430, loss 0.00432301, acc 1
2016-09-06T15:57:21.545163: step 10431, loss 0.00268109, acc 1
2016-09-06T15:57:22.695417: step 10432, loss 0.00484239, acc 1
2016-09-06T15:57:23.644680: step 10433, loss 0.00534491, acc 1
2016-09-06T15:57:24.657951: step 10434, loss 0.00238046, acc 1
2016-09-06T15:57:25.774660: step 10435, loss 0.0231638, acc 0.98
2016-09-06T15:57:26.724799: step 10436, loss 0.0209969, acc 1
2016-09-06T15:57:27.831874: step 10437, loss 0.105174, acc 0.96
2016-09-06T15:57:28.936375: step 10438, loss 0.0388998, acc 0.98
2016-09-06T15:57:29.924737: step 10439, loss 0.0284497, acc 0.98
2016-09-06T15:57:30.995322: step 10440, loss 0.00243101, acc 1
2016-09-06T15:57:32.020296: step 10441, loss 0.00821809, acc 1
2016-09-06T15:57:33.148004: step 10442, loss 0.021972, acc 0.98
2016-09-06T15:57:34.087841: step 10443, loss 0.0237799, acc 1
2016-09-06T15:57:35.021219: step 10444, loss 0.0123089, acc 1
2016-09-06T15:57:36.069047: step 10445, loss 0.0252701, acc 1
2016-09-06T15:57:36.967652: step 10446, loss 0.172366, acc 0.96
2016-09-06T15:57:38.017523: step 10447, loss 0.00501589, acc 1
2016-09-06T15:57:38.918713: step 10448, loss 0.00452043, acc 1
2016-09-06T15:57:40.010174: step 10449, loss 0.00899851, acc 1
2016-09-06T15:57:40.770334: step 10450, loss 0.0240625, acc 0.98
2016-09-06T15:57:41.772155: step 10451, loss 0.0149771, acc 1
2016-09-06T15:57:42.721118: step 10452, loss 0.00300169, acc 1
2016-09-06T15:57:43.479575: step 10453, loss 0.00297276, acc 1
2016-09-06T15:57:44.192632: step 10454, loss 0.0104708, acc 1
2016-09-06T15:57:44.913742: step 10455, loss 0.0249408, acc 0.98
2016-09-06T15:57:45.616933: step 10456, loss 0.0196352, acc 1
2016-09-06T15:57:46.430623: step 10457, loss 0.00366904, acc 1
2016-09-06T15:57:47.182405: step 10458, loss 0.00353917, acc 1
2016-09-06T15:57:48.042573: step 10459, loss 0.0427153, acc 0.98
2016-09-06T15:57:48.871682: step 10460, loss 0.00242707, acc 1
2016-09-06T15:57:49.775069: step 10461, loss 0.0326079, acc 0.98
2016-09-06T15:57:50.572104: step 10462, loss 0.00251108, acc 1
2016-09-06T15:57:51.383656: step 10463, loss 0.0030558, acc 1
2016-09-06T15:57:52.117122: step 10464, loss 0.00450792, acc 1
2016-09-06T15:57:53.064285: step 10465, loss 0.00722972, acc 1
2016-09-06T15:57:53.957327: step 10466, loss 0.0229623, acc 1
2016-09-06T15:57:54.779646: step 10467, loss 0.00636092, acc 1
2016-09-06T15:57:55.516350: step 10468, loss 0.0255072, acc 1
2016-09-06T15:57:56.324964: step 10469, loss 0.0357302, acc 0.98
2016-09-06T15:57:57.177393: step 10470, loss 0.0192985, acc 0.98
2016-09-06T15:57:57.999652: step 10471, loss 0.00890162, acc 1
2016-09-06T15:57:58.808440: step 10472, loss 0.0413214, acc 0.98
2016-09-06T15:57:59.639541: step 10473, loss 0.0226325, acc 0.98
2016-09-06T15:58:00.402785: step 10474, loss 0.00333727, acc 1
2016-09-06T15:58:01.311505: step 10475, loss 0.0239931, acc 1
2016-09-06T15:58:02.198570: step 10476, loss 0.00687685, acc 1
2016-09-06T15:58:03.024458: step 10477, loss 0.0518604, acc 0.96
2016-09-06T15:58:03.923879: step 10478, loss 0.0403435, acc 0.96
2016-09-06T15:58:04.757267: step 10479, loss 0.0343948, acc 0.98
2016-09-06T15:58:05.602152: step 10480, loss 0.0574073, acc 0.94
2016-09-06T15:58:06.405430: step 10481, loss 0.0211936, acc 0.98
2016-09-06T15:58:07.275311: step 10482, loss 0.0141315, acc 1
2016-09-06T15:58:08.042770: step 10483, loss 0.0757961, acc 0.98
2016-09-06T15:58:08.759248: step 10484, loss 0.00300561, acc 1
2016-09-06T15:58:09.568731: step 10485, loss 0.0329259, acc 0.98
2016-09-06T15:58:10.453609: step 10486, loss 0.0110266, acc 1
2016-09-06T15:58:11.331970: step 10487, loss 0.0811943, acc 0.98
2016-09-06T15:58:12.126527: step 10488, loss 0.00269655, acc 1
2016-09-06T15:58:12.926185: step 10489, loss 0.00233584, acc 1
2016-09-06T15:58:13.753136: step 10490, loss 0.0200211, acc 1
2016-09-06T15:58:14.600036: step 10491, loss 0.0156813, acc 1
2016-09-06T15:58:15.411135: step 10492, loss 0.109129, acc 0.96
2016-09-06T15:58:16.177325: step 10493, loss 0.00837919, acc 1
2016-09-06T15:58:16.907881: step 10494, loss 0.00407625, acc 1
2016-09-06T15:58:17.620995: step 10495, loss 0.0177901, acc 0.98
2016-09-06T15:58:18.375981: step 10496, loss 0.0265286, acc 0.98
2016-09-06T15:58:19.246004: step 10497, loss 0.00299954, acc 1
2016-09-06T15:58:20.070878: step 10498, loss 0.106796, acc 0.98
2016-09-06T15:58:20.935136: step 10499, loss 0.0117913, acc 1
2016-09-06T15:58:21.809677: step 10500, loss 0.00252483, acc 1

Evaluation:
2016-09-06T15:58:25.716077: step 10500, loss 1.21233, acc 0.744841

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-10500

2016-09-06T15:58:27.520612: step 10501, loss 0.00736442, acc 1
2016-09-06T15:58:28.344529: step 10502, loss 0.0313668, acc 0.98
2016-09-06T15:58:29.152125: step 10503, loss 0.0161853, acc 1
2016-09-06T15:58:30.024835: step 10504, loss 0.0196961, acc 1
2016-09-06T15:58:30.915455: step 10505, loss 0.0168526, acc 1
2016-09-06T15:58:31.685562: step 10506, loss 0.0392591, acc 1
2016-09-06T15:58:32.497900: step 10507, loss 0.00376522, acc 1
2016-09-06T15:58:33.286998: step 10508, loss 0.0132831, acc 1
2016-09-06T15:58:34.068993: step 10509, loss 0.0426318, acc 0.98
2016-09-06T15:58:34.873613: step 10510, loss 0.00321567, acc 1
2016-09-06T15:58:35.776209: step 10511, loss 0.0238638, acc 0.98
2016-09-06T15:58:36.587535: step 10512, loss 0.0314793, acc 0.98
2016-09-06T15:58:37.322977: step 10513, loss 0.0148144, acc 1
2016-09-06T15:58:38.030286: step 10514, loss 0.010641, acc 1
2016-09-06T15:58:38.747860: step 10515, loss 0.0147261, acc 1
2016-09-06T15:58:39.462378: step 10516, loss 0.0200191, acc 1
2016-09-06T15:58:40.173337: step 10517, loss 0.00543519, acc 1
2016-09-06T15:58:40.906729: step 10518, loss 0.0208, acc 0.98
2016-09-06T15:58:41.679921: step 10519, loss 0.00261366, acc 1
2016-09-06T15:58:42.424638: step 10520, loss 0.00256348, acc 1
2016-09-06T15:58:43.350589: step 10521, loss 0.00339082, acc 1
2016-09-06T15:58:44.235940: step 10522, loss 0.00262552, acc 1
2016-09-06T15:58:45.037781: step 10523, loss 0.0218278, acc 0.98
2016-09-06T15:58:45.895637: step 10524, loss 0.0330819, acc 0.98
2016-09-06T15:58:46.607944: step 10525, loss 0.00683227, acc 1
2016-09-06T15:58:47.347018: step 10526, loss 0.0199923, acc 1
2016-09-06T15:58:48.067787: step 10527, loss 0.00597747, acc 1
2016-09-06T15:58:48.884820: step 10528, loss 0.0422564, acc 0.98
2016-09-06T15:58:49.591369: step 10529, loss 0.0478804, acc 0.96
2016-09-06T15:58:50.310387: step 10530, loss 0.0167352, acc 0.98
2016-09-06T15:58:51.037996: step 10531, loss 0.00380794, acc 1
2016-09-06T15:58:51.800285: step 10532, loss 0.0193224, acc 0.98
2016-09-06T15:58:52.555645: step 10533, loss 0.0945836, acc 0.98
2016-09-06T15:58:53.423218: step 10534, loss 0.0446191, acc 0.96
2016-09-06T15:58:54.258865: step 10535, loss 0.006078, acc 1
2016-09-06T15:58:55.183311: step 10536, loss 0.0183943, acc 1
2016-09-06T15:58:55.952770: step 10537, loss 0.0567234, acc 0.98
2016-09-06T15:58:56.713272: step 10538, loss 0.00290473, acc 1
2016-09-06T15:58:57.449410: step 10539, loss 0.020601, acc 1
2016-09-06T15:58:58.276152: step 10540, loss 0.128518, acc 0.98
2016-09-06T15:58:59.067836: step 10541, loss 0.0438193, acc 0.96
2016-09-06T15:58:59.942758: step 10542, loss 0.0200239, acc 0.98
2016-09-06T15:59:00.749592: step 10543, loss 0.0180703, acc 1
2016-09-06T15:59:01.556519: step 10544, loss 0.0154752, acc 1
2016-09-06T15:59:02.339595: step 10545, loss 0.0196236, acc 1
2016-09-06T15:59:03.206604: step 10546, loss 0.0553195, acc 0.96
2016-09-06T15:59:04.036567: step 10547, loss 0.0520533, acc 0.96
2016-09-06T15:59:04.963296: step 10548, loss 0.0530433, acc 0.98
2016-09-06T15:59:05.829290: step 10549, loss 0.006183, acc 1
2016-09-06T15:59:06.655965: step 10550, loss 0.00266821, acc 1
2016-09-06T15:59:07.464381: step 10551, loss 0.0030174, acc 1
2016-09-06T15:59:08.223663: step 10552, loss 0.00240593, acc 1
2016-09-06T15:59:09.068488: step 10553, loss 0.014428, acc 1
2016-09-06T15:59:09.950900: step 10554, loss 0.044788, acc 0.98
2016-09-06T15:59:10.814881: step 10555, loss 0.0064413, acc 1
2016-09-06T15:59:11.626585: step 10556, loss 0.00228812, acc 1
2016-09-06T15:59:12.406887: step 10557, loss 0.0637394, acc 0.96
2016-09-06T15:59:13.222606: step 10558, loss 0.0683784, acc 0.98
2016-09-06T15:59:14.014387: step 10559, loss 0.00545806, acc 1
2016-09-06T15:59:14.771676: step 10560, loss 0.00636274, acc 1
2016-09-06T15:59:15.536955: step 10561, loss 0.00809089, acc 1
2016-09-06T15:59:16.297781: step 10562, loss 0.0269033, acc 1
2016-09-06T15:59:17.022531: step 10563, loss 0.00712692, acc 1
2016-09-06T15:59:17.730785: step 10564, loss 0.01984, acc 1
2016-09-06T15:59:18.463747: step 10565, loss 0.0252157, acc 1
2016-09-06T15:59:19.178374: step 10566, loss 0.00761496, acc 1
2016-09-06T15:59:19.908742: step 10567, loss 0.00593333, acc 1
2016-09-06T15:59:20.699789: step 10568, loss 0.00985991, acc 1
2016-09-06T15:59:21.473814: step 10569, loss 0.0378935, acc 0.96
2016-09-06T15:59:22.248040: step 10570, loss 0.0108238, acc 1
2016-09-06T15:59:23.084297: step 10571, loss 0.00910064, acc 1
2016-09-06T15:59:23.821963: step 10572, loss 0.0320197, acc 0.98
2016-09-06T15:59:24.641123: step 10573, loss 0.0162902, acc 0.98
2016-09-06T15:59:25.365229: step 10574, loss 0.0652003, acc 0.98
2016-09-06T15:59:26.128890: step 10575, loss 0.0170477, acc 0.98
2016-09-06T15:59:26.861042: step 10576, loss 0.0177207, acc 0.98
2016-09-06T15:59:27.595694: step 10577, loss 0.0250182, acc 0.98
2016-09-06T15:59:28.307021: step 10578, loss 0.00583978, acc 1
2016-09-06T15:59:29.041394: step 10579, loss 0.00617234, acc 1
2016-09-06T15:59:29.786395: step 10580, loss 0.00231884, acc 1
2016-09-06T15:59:30.601055: step 10581, loss 0.00777232, acc 1
2016-09-06T15:59:31.319363: step 10582, loss 0.0220282, acc 1
2016-09-06T15:59:32.038552: step 10583, loss 0.00235043, acc 1
2016-09-06T15:59:32.760808: step 10584, loss 0.00607956, acc 1
2016-09-06T15:59:33.507637: step 10585, loss 0.0148409, acc 1
2016-09-06T15:59:34.248489: step 10586, loss 0.0154584, acc 1
2016-09-06T15:59:34.979119: step 10587, loss 0.00346478, acc 1
2016-09-06T15:59:35.729556: step 10588, loss 0.00278546, acc 1
2016-09-06T15:59:36.486336: step 10589, loss 0.0236224, acc 0.98
2016-09-06T15:59:37.218691: step 10590, loss 0.00281437, acc 1
2016-09-06T15:59:37.953748: step 10591, loss 0.00244125, acc 1
2016-09-06T15:59:38.683449: step 10592, loss 0.0142161, acc 1
2016-09-06T15:59:39.486229: step 10593, loss 0.010346, acc 1
2016-09-06T15:59:40.314572: step 10594, loss 0.0156026, acc 1
2016-09-06T15:59:41.185417: step 10595, loss 0.0117858, acc 1
2016-09-06T15:59:42.028255: step 10596, loss 0.00228729, acc 1
2016-09-06T15:59:42.821842: step 10597, loss 0.0798171, acc 0.94
2016-09-06T15:59:43.572794: step 10598, loss 0.0175076, acc 0.98
2016-09-06T15:59:44.330351: step 10599, loss 0.0304318, acc 0.98
2016-09-06T15:59:45.052122: step 10600, loss 0.0191966, acc 1

Evaluation:
2016-09-06T15:59:48.565801: step 10600, loss 1.83815, acc 0.743902

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-10600

2016-09-06T15:59:50.471902: step 10601, loss 0.0133888, acc 1
2016-09-06T15:59:51.188906: step 10602, loss 0.011248, acc 1
2016-09-06T15:59:51.912440: step 10603, loss 0.0223283, acc 0.98
2016-09-06T15:59:52.655530: step 10604, loss 0.0418417, acc 0.98
2016-09-06T15:59:53.415488: step 10605, loss 0.00838058, acc 1
2016-09-06T15:59:54.201781: step 10606, loss 0.00420868, acc 1
2016-09-06T15:59:54.912604: step 10607, loss 0.0380633, acc 0.98
2016-09-06T15:59:55.719487: step 10608, loss 0.015066, acc 1
2016-09-06T15:59:56.454905: step 10609, loss 0.00449769, acc 1
2016-09-06T15:59:57.226411: step 10610, loss 0.00875701, acc 1
2016-09-06T15:59:57.967713: step 10611, loss 0.0406789, acc 0.96
2016-09-06T15:59:58.731851: step 10612, loss 0.0145844, acc 1
2016-09-06T15:59:59.509921: step 10613, loss 0.038066, acc 0.98
2016-09-06T16:00:00.246885: step 10614, loss 0.0984018, acc 0.98
2016-09-06T16:00:00.993510: step 10615, loss 0.00629767, acc 1
2016-09-06T16:00:01.729604: step 10616, loss 0.0237816, acc 0.98
2016-09-06T16:00:02.448054: step 10617, loss 0.0143699, acc 1
2016-09-06T16:00:03.157756: step 10618, loss 0.00190597, acc 1
2016-09-06T16:00:04.076013: step 10619, loss 0.0257029, acc 1
2016-09-06T16:00:04.910152: step 10620, loss 0.0194754, acc 0.98
2016-09-06T16:00:05.900725: step 10621, loss 0.00191636, acc 1
2016-09-06T16:00:06.816015: step 10622, loss 0.0182302, acc 1
2016-09-06T16:00:07.987266: step 10623, loss 0.00646169, acc 1
2016-09-06T16:00:09.220167: step 10624, loss 0.0505279, acc 0.98
2016-09-06T16:00:10.319536: step 10625, loss 0.00283685, acc 1
2016-09-06T16:00:11.537558: step 10626, loss 0.00653705, acc 1
2016-09-06T16:00:12.367153: step 10627, loss 0.0183107, acc 0.98
2016-09-06T16:00:13.188482: step 10628, loss 0.0473342, acc 0.98
2016-09-06T16:00:14.134329: step 10629, loss 0.0183411, acc 0.98
2016-09-06T16:00:15.076592: step 10630, loss 0.0178277, acc 0.98
2016-09-06T16:00:16.026308: step 10631, loss 0.00817917, acc 1
2016-09-06T16:00:17.106424: step 10632, loss 0.00308365, acc 1
2016-09-06T16:00:18.097310: step 10633, loss 0.0210313, acc 0.98
2016-09-06T16:00:19.126220: step 10634, loss 0.0154193, acc 1
2016-09-06T16:00:20.177277: step 10635, loss 0.15135, acc 0.98
2016-09-06T16:00:21.232368: step 10636, loss 0.0346162, acc 0.98
2016-09-06T16:00:22.221331: step 10637, loss 0.013075, acc 1
2016-09-06T16:00:23.247974: step 10638, loss 0.00349447, acc 1
2016-09-06T16:00:24.229936: step 10639, loss 0.00430668, acc 1
2016-09-06T16:00:25.130794: step 10640, loss 0.00876271, acc 1
2016-09-06T16:00:26.010606: step 10641, loss 0.0221611, acc 0.98
2016-09-06T16:00:26.898916: step 10642, loss 0.0171292, acc 1
2016-09-06T16:00:27.781395: step 10643, loss 0.0510139, acc 0.96
2016-09-06T16:00:28.646347: step 10644, loss 0.00906029, acc 1
2016-09-06T16:00:29.550932: step 10645, loss 0.00697061, acc 1
2016-09-06T16:00:30.467769: step 10646, loss 0.0171932, acc 0.98
2016-09-06T16:00:31.371287: step 10647, loss 0.0299893, acc 0.98
2016-09-06T16:00:32.270441: step 10648, loss 0.00364266, acc 1
2016-09-06T16:00:33.167548: step 10649, loss 0.0261639, acc 1
2016-09-06T16:00:34.115085: step 10650, loss 0.00450474, acc 1
2016-09-06T16:00:35.076400: step 10651, loss 0.0175606, acc 1
2016-09-06T16:00:36.126936: step 10652, loss 0.00311992, acc 1
2016-09-06T16:00:37.154215: step 10653, loss 0.00650119, acc 1
2016-09-06T16:00:38.182850: step 10654, loss 0.00362122, acc 1
2016-09-06T16:00:39.154078: step 10655, loss 0.0136797, acc 1
2016-09-06T16:00:40.331904: step 10656, loss 0.00262598, acc 1
2016-09-06T16:00:41.480675: step 10657, loss 0.0261863, acc 0.98
2016-09-06T16:00:42.520864: step 10658, loss 0.0644863, acc 0.94
2016-09-06T16:00:43.611008: step 10659, loss 0.0177935, acc 1
2016-09-06T16:00:44.728337: step 10660, loss 0.015946, acc 1
2016-09-06T16:00:45.823837: step 10661, loss 0.00231065, acc 1
2016-09-06T16:00:46.816405: step 10662, loss 0.0197417, acc 0.98
2016-09-06T16:00:47.768624: step 10663, loss 0.00291669, acc 1
2016-09-06T16:00:48.786380: step 10664, loss 0.00256242, acc 1
2016-09-06T16:00:49.792184: step 10665, loss 0.0340601, acc 0.98
2016-09-06T16:00:50.850315: step 10666, loss 0.018521, acc 0.98
2016-09-06T16:00:51.853586: step 10667, loss 0.0179138, acc 1
2016-09-06T16:00:52.773875: step 10668, loss 0.015969, acc 1
2016-09-06T16:00:53.711941: step 10669, loss 0.0369919, acc 0.98
2016-09-06T16:00:54.758990: step 10670, loss 0.088044, acc 0.98
2016-09-06T16:00:55.783126: step 10671, loss 0.0278165, acc 0.98
2016-09-06T16:00:56.780462: step 10672, loss 0.0355442, acc 1
2016-09-06T16:00:57.810037: step 10673, loss 0.00387379, acc 1
2016-09-06T16:00:58.928342: step 10674, loss 0.0133222, acc 1
2016-09-06T16:00:59.929013: step 10675, loss 0.00787116, acc 1
2016-09-06T16:01:00.925122: step 10676, loss 0.0378045, acc 0.98
2016-09-06T16:01:01.847417: step 10677, loss 0.0199158, acc 1
2016-09-06T16:01:02.756364: step 10678, loss 0.00223954, acc 1
2016-09-06T16:01:03.624952: step 10679, loss 0.0191201, acc 1
2016-09-06T16:01:04.591982: step 10680, loss 0.0326732, acc 0.98
2016-09-06T16:01:05.617832: step 10681, loss 0.00291723, acc 1
2016-09-06T16:01:06.479730: step 10682, loss 0.0187525, acc 0.98
2016-09-06T16:01:07.275752: step 10683, loss 0.00272566, acc 1
2016-09-06T16:01:08.094205: step 10684, loss 0.0161959, acc 1
2016-09-06T16:01:08.950737: step 10685, loss 0.0123653, acc 1
2016-09-06T16:01:09.764260: step 10686, loss 0.0191795, acc 0.98
2016-09-06T16:01:10.619286: step 10687, loss 0.0212481, acc 0.98
2016-09-06T16:01:11.523070: step 10688, loss 0.00897205, acc 1
2016-09-06T16:01:12.409626: step 10689, loss 0.0449384, acc 0.98
2016-09-06T16:01:13.303684: step 10690, loss 0.0286496, acc 0.98
2016-09-06T16:01:14.260546: step 10691, loss 0.00794476, acc 1
2016-09-06T16:01:15.176604: step 10692, loss 0.00460786, acc 1
2016-09-06T16:01:16.126641: step 10693, loss 0.0305013, acc 0.98
2016-09-06T16:01:17.047106: step 10694, loss 0.032721, acc 0.96
2016-09-06T16:01:17.929459: step 10695, loss 0.00771348, acc 1
2016-09-06T16:01:18.842972: step 10696, loss 0.0132832, acc 1
2016-09-06T16:01:19.750421: step 10697, loss 0.0118421, acc 1
2016-09-06T16:01:20.696620: step 10698, loss 0.0314736, acc 0.98
2016-09-06T16:01:21.594470: step 10699, loss 0.00208664, acc 1
2016-09-06T16:01:22.503098: step 10700, loss 0.00234894, acc 1

Evaluation:
2016-09-06T16:01:27.230764: step 10700, loss 1.92762, acc 0.761726

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473139646/checkpoints/model-10700

2016-09-06T16:01:29.289931: step 10701, loss 0.0243028, acc 1
2016-09-06T16:01:30.266589: step 10702, loss 0.00209825, acc 1
2016-09-06T16:01:31.251313: step 10703, loss 0.0147836, acc 1
2016-09-06T16:01:32.111501: step 10704, loss 0.0206587, acc 0.98
2016-09-06T16:01:33.088861: step 10705, loss 0.00218287, acc 1
2016-09-06T16:01:34.068824: step 10706, loss 0.0263903, acc 1
2016-09-06T16:01:34.969890: step 10707, loss 0.0134712, acc 1
2016-09-06T16:01:35.886413: step 10708, loss 0.0167945, acc 1
2016-09-06T16:01:36.906758: step 10709, loss 0.00410001, acc 1
2016-09-06T16:01:37.877990: step 10710, loss 0.00225093, acc 1
2016-09-06T16:01:38.938281: step 10711, loss 0.00225463, acc 1
2016-09-06T16:01:39.932633: step 10712, loss 0.0020218, acc 1
2016-09-06T16:01:40.934201: step 10713, loss 0.0022428, acc 1
2016-09-06T16:01:41.968319: step 10714, loss 0.00210192, acc 1
2016-09-06T16:01:42.884149: step 10715, loss 0.00257915, acc 1
2016-09-06T16:01:44.003256: step 10716, loss 0.0265522, acc 1
2016-09-06T16:01:44.948716: step 10717, loss 0.00213938, acc 1
2016-09-06T16:01:46.039798: step 10718, loss 0.00310112, acc 1
2016-09-06T16:01:46.973715: step 10719, loss 0.0446784, acc 0.98
2016-09-06T16:01:47.936079: step 10720, loss 0.0308942, acc 0.98
2016-09-06T16:01:48.898540: step 10721, loss 0.0145377, acc 1
2016-09-06T16:01:49.867818: step 10722, loss 0.0308763, acc 1
2016-09-06T16:01:50.947416: step 10723, loss 0.00290861, acc 1
2016-09-06T16:01:51.853215: step 10724, loss 0.0019928, acc 1
2016-09-06T16:01:52.828510: step 10725, loss 0.0031738, acc 1
2016-09-06T16:01:53.706081: step 10726, loss 0.0175313, acc 0.98
2016-09-06T16:01:54.594991: step 10727, loss 0.0279398, acc 0.98
2016-09-06T16:01:55.494051: step 10728, loss 0.0376792, acc 0.98
2016-09-06T16:01:56.499561: step 10729, loss 0.0399079, acc 0.98
2016-09-06T16:01:57.520354: step 10730, loss 0.001969, acc 1
2016-09-06T16:01:58.503202: step 10731, loss 0.0443762, acc 0.98
2016-09-06T16:01:59.497273: step 10732, loss 0.00186477, acc 1
2016-09-06T16:02:00.537074: step 10733, loss 0.0430749, acc 0.98
2016-09-06T16:02:01.489518: step 10734, loss 0.0285939, acc 0.98
2016-09-06T16:02:02.479066: step 10735, loss 0.0631046, acc 0.98
2016-09-06T16:02:03.423481: step 10736, loss 0.00173897, acc 1
2016-09-06T16:02:04.402012: step 10737, loss 0.00321906, acc 1
2016-09-06T16:02:05.276559: step 10738, loss 0.0216223, acc 1
2016-09-06T16:02:06.150873: step 10739, loss 0.00285731, acc 1
2016-09-06T16:02:07.140163: step 10740, loss 0.0118537, acc 1
2016-09-06T16:02:07.965391: step 10741, loss 0.0116791, acc 1
2016-09-06T16:02:08.911277: step 10742, loss 0.0268423, acc 1
2016-09-06T16:02:09.990427: step 10743, loss 0.00173837, acc 1
2016-09-06T16:02:10.992566: step 10744, loss 0.00269941, acc 1
2016-09-06T16:02:12.115744: step 10745, loss 0.00245414, acc 1
2016-09-06T16:02:13.216280: step 10746, loss 0.0778377, acc 0.98
2016-09-06T16:02:14.219455: step 10747, loss 0.00921177, acc 1
2016-09-06T16:02:15.225629: step 10748, loss 0.00199397, acc 1
2016-09-06T16:02:16.057087: step 10749, loss 0.0641374, acc 0.98
2016-09-06T16:02:16.926011: step 10750, loss 0.042372, acc 0.96
2016-09-06T16:02:17.925785: step 10751, loss 0.017058, acc 0.98
2016-09-06T16:02:18.865692: step 10752, loss 0.0161746, acc 1
2016-09-06T16:02:19.915359: step 10753, loss 0.0169467, acc 0.98
2016-09-06T16:02:20.889406: step 10754, loss 0.0232731, acc 0.98
2016-09-06T16:02:21.875585: step 10755, loss 0.00271792, acc 1
2016-09-06T16:02:22.853517: step 10756, loss 0.00920647, acc 1
2016-09-06T16:02:23.755481: step 10757, loss 0.0403719, acc 0.98
2016-09-06T16:02:24.697431: step 10758, loss 0.00317306, acc 1
2016-09-06T16:02:25.549976: step 10759, loss 0.0121601, acc 1
2016-09-0
WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f30ce6f8710>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.
WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f30ce6f87d0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.

Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=50
CHECKPOINT_EVERY=100
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=300
EVALUATE_EVERY=100
FILTER_SIZES=3,4,5
HIDDEN_DIM=300
L2_REG_LAMBDA=0.5
LOG_DEVICE_PLACEMENT=False
NUM_EPOCHS=30
NUM_FILTERS=100
WORD2VEC=GoogleNews-vectors-negative300.bin

Loading data...
Vocabulary Size: 18758
Train/Dev split: 9662/1000
Writing to /home/cil/lstm-context-embeddings/runs/1473253942

Load word2vec file GoogleNews-vectors-negative300.bin

2016-09-07T21:12:42.158150: step 1, loss 0.693147, acc 0.44
2016-09-07T21:12:42.823558: step 2, loss 0.725763, acc 0.44
2016-09-07T21:12:43.498348: step 3, loss 0.7034, acc 0.4
2016-09-07T21:12:44.167760: step 4, loss 0.693114, acc 0.54
2016-09-07T21:12:44.824728: step 5, loss 0.663929, acc 0.64
2016-09-07T21:12:45.486113: step 6, loss 0.69154, acc 0.56
2016-09-07T21:12:46.167482: step 7, loss 0.76768, acc 0.46
2016-09-07T21:12:46.845593: step 8, loss 0.765035, acc 0.42
2016-09-07T21:12:47.541505: step 9, loss 0.674094, acc 0.6
2016-09-07T21:12:48.208635: step 10, loss 0.694994, acc 0.5
2016-09-07T21:12:48.889784: step 11, loss 0.7127, acc 0.38
2016-09-07T21:12:49.566759: step 12, loss 0.693138, acc 0.56
2016-09-07T21:12:50.230855: step 13, loss 0.694796, acc 0.6
2016-09-07T21:12:50.914356: step 14, loss 0.687004, acc 0.54
2016-09-07T21:12:51.579386: step 15, loss 0.698644, acc 0.54
2016-09-07T21:12:52.232605: step 16, loss 0.707913, acc 0.52
2016-09-07T21:12:52.952800: step 17, loss 0.701218, acc 0.5
2016-09-07T21:12:53.676012: step 18, loss 0.734258, acc 0.38
2016-09-07T21:12:54.363437: step 19, loss 0.703061, acc 0.48
2016-09-07T21:12:55.051827: step 20, loss 0.678589, acc 0.6
2016-09-07T21:12:55.724989: step 21, loss 0.685467, acc 0.6
2016-09-07T21:12:56.391553: step 22, loss 0.678918, acc 0.56
2016-09-07T21:12:57.065283: step 23, loss 0.696033, acc 0.52
2016-09-07T21:12:57.735860: step 24, loss 0.719967, acc 0.48
2016-09-07T21:12:58.427185: step 25, loss 0.701589, acc 0.5
2016-09-07T21:12:59.108826: step 26, loss 0.704652, acc 0.48
2016-09-07T21:12:59.797414: step 27, loss 0.6801, acc 0.52
2016-09-07T21:13:00.526286: step 28, loss 0.693438, acc 0.48
2016-09-07T21:13:01.208991: step 29, loss 0.684968, acc 0.54
2016-09-07T21:13:01.892700: step 30, loss 0.687334, acc 0.5
2016-09-07T21:13:02.560197: step 31, loss 0.639565, acc 0.62
2016-09-07T21:13:03.218284: step 32, loss 0.672649, acc 0.58
2016-09-07T21:13:03.884113: step 33, loss 0.667756, acc 0.52
2016-09-07T21:13:04.546827: step 34, loss 0.631539, acc 0.64
2016-09-07T21:13:05.214131: step 35, loss 0.602907, acc 0.64
2016-09-07T21:13:05.878220: step 36, loss 0.734284, acc 0.6
2016-09-07T21:13:06.548815: step 37, loss 0.620949, acc 0.64
2016-09-07T21:13:07.199365: step 38, loss 0.675268, acc 0.62
2016-09-07T21:13:07.878563: step 39, loss 0.621577, acc 0.74
2016-09-07T21:13:08.539262: step 40, loss 0.629322, acc 0.68
2016-09-07T21:13:09.216020: step 41, loss 0.600681, acc 0.72
2016-09-07T21:13:09.893825: step 42, loss 0.604541, acc 0.68
2016-09-07T21:13:10.561922: step 43, loss 0.659196, acc 0.72
2016-09-07T21:13:11.216424: step 44, loss 0.633103, acc 0.62
2016-09-07T21:13:11.885189: step 45, loss 0.608041, acc 0.74
2016-09-07T21:13:12.538890: step 46, loss 0.564201, acc 0.72
2016-09-07T21:13:13.215211: step 47, loss 0.51973, acc 0.8
2016-09-07T21:13:13.883809: step 48, loss 0.604481, acc 0.64
2016-09-07T21:13:14.552480: step 49, loss 0.588359, acc 0.72
2016-09-07T21:13:15.220462: step 50, loss 0.651998, acc 0.66
2016-09-07T21:13:15.882707: step 51, loss 0.819795, acc 0.68
2016-09-07T21:13:16.548528: step 52, loss 0.567697, acc 0.76
2016-09-07T21:13:17.202736: step 53, loss 0.506597, acc 0.8
2016-09-07T21:13:17.862877: step 54, loss 0.656663, acc 0.64
2016-09-07T21:13:18.546396: step 55, loss 0.578138, acc 0.78
2016-09-07T21:13:19.220954: step 56, loss 0.506926, acc 0.8
2016-09-07T21:13:19.885419: step 57, loss 0.556826, acc 0.7
2016-09-07T21:13:20.566578: step 58, loss 0.600447, acc 0.54
2016-09-07T21:13:21.231917: step 59, loss 0.615229, acc 0.74
2016-09-07T21:13:21.899100: step 60, loss 0.610747, acc 0.72
2016-09-07T21:13:22.595138: step 61, loss 0.658781, acc 0.58
2016-09-07T21:13:23.262386: step 62, loss 0.580434, acc 0.7
2016-09-07T21:13:23.917323: step 63, loss 0.531748, acc 0.8
2016-09-07T21:13:24.595037: step 64, loss 0.562006, acc 0.7
2016-09-07T21:13:25.272028: step 65, loss 0.472409, acc 0.76
2016-09-07T21:13:25.918094: step 66, loss 0.583089, acc 0.68
2016-09-07T21:13:26.590222: step 67, loss 0.553955, acc 0.72
2016-09-07T21:13:27.252572: step 68, loss 0.657235, acc 0.68
2016-09-07T21:13:27.918983: step 69, loss 0.47169, acc 0.76
2016-09-07T21:13:28.590148: step 70, loss 0.505524, acc 0.76
2016-09-07T21:13:29.273094: step 71, loss 0.54083, acc 0.74
2016-09-07T21:13:29.956764: step 72, loss 0.490023, acc 0.76
2016-09-07T21:13:30.655006: step 73, loss 0.472415, acc 0.8
2016-09-07T21:13:31.325858: step 74, loss 0.473085, acc 0.8
2016-09-07T21:13:31.982052: step 75, loss 0.522745, acc 0.74
2016-09-07T21:13:32.641725: step 76, loss 0.622011, acc 0.68
2016-09-07T21:13:33.306033: step 77, loss 0.478215, acc 0.76
2016-09-07T21:13:33.984907: step 78, loss 0.660423, acc 0.64
2016-09-07T21:13:34.661697: step 79, loss 0.515926, acc 0.72
2016-09-07T21:13:35.320755: step 80, loss 0.644231, acc 0.64
2016-09-07T21:13:35.989439: step 81, loss 0.591783, acc 0.74
2016-09-07T21:13:36.661889: step 82, loss 0.495298, acc 0.74
2016-09-07T21:13:37.329848: step 83, loss 0.510301, acc 0.8
2016-09-07T21:13:37.987828: step 84, loss 0.501635, acc 0.74
2016-09-07T21:13:38.655057: step 85, loss 0.512266, acc 0.72
2016-09-07T21:13:39.312000: step 86, loss 0.439206, acc 0.8
2016-09-07T21:13:39.975715: step 87, loss 0.57771, acc 0.68
2016-09-07T21:13:40.628234: step 88, loss 0.419269, acc 0.82
2016-09-07T21:13:41.276421: step 89, loss 0.402163, acc 0.88
2016-09-07T21:13:41.946132: step 90, loss 0.610586, acc 0.7
2016-09-07T21:13:42.611660: step 91, loss 0.528478, acc 0.74
2016-09-07T21:13:43.275047: step 92, loss 0.585776, acc 0.68
2016-09-07T21:13:43.949638: step 93, loss 0.611873, acc 0.66
2016-09-07T21:13:44.617320: step 94, loss 0.528588, acc 0.72
2016-09-07T21:13:45.279372: step 95, loss 0.485776, acc 0.76
2016-09-07T21:13:45.932492: step 96, loss 0.448229, acc 0.76
2016-09-07T21:13:46.590211: step 97, loss 0.487564, acc 0.68
2016-09-07T21:13:47.258615: step 98, loss 0.481752, acc 0.72
2016-09-07T21:13:47.912081: step 99, loss 0.614127, acc 0.76
2016-09-07T21:13:48.585699: step 100, loss 0.500923, acc 0.7

Evaluation:
2016-09-07T21:13:51.661130: step 100, loss 0.499596, acc 0.758

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-100

2016-09-07T21:13:53.334271: step 101, loss 0.456514, acc 0.8
2016-09-07T21:13:54.000047: step 102, loss 0.661908, acc 0.66
2016-09-07T21:13:54.696371: step 103, loss 0.448057, acc 0.78
2016-09-07T21:13:55.371029: step 104, loss 0.439968, acc 0.76
2016-09-07T21:13:56.022444: step 105, loss 0.548992, acc 0.7
2016-09-07T21:13:56.687487: step 106, loss 0.483679, acc 0.76
2016-09-07T21:13:57.363599: step 107, loss 0.514126, acc 0.78
2016-09-07T21:13:58.011221: step 108, loss 0.451816, acc 0.8
2016-09-07T21:13:58.663005: step 109, loss 0.477411, acc 0.76
2016-09-07T21:13:59.321319: step 110, loss 0.505219, acc 0.8
2016-09-07T21:13:59.986827: step 111, loss 0.561521, acc 0.72
2016-09-07T21:14:00.688020: step 112, loss 0.471099, acc 0.8
2016-09-07T21:14:01.348789: step 113, loss 0.494839, acc 0.7
2016-09-07T21:14:01.993091: step 114, loss 0.444643, acc 0.86
2016-09-07T21:14:02.658205: step 115, loss 0.374507, acc 0.86
2016-09-07T21:14:03.312336: step 116, loss 0.36759, acc 0.78
2016-09-07T21:14:03.963144: step 117, loss 0.742742, acc 0.62
2016-09-07T21:14:04.631524: step 118, loss 0.457527, acc 0.72
2016-09-07T21:14:05.305742: step 119, loss 0.605447, acc 0.78
2016-09-07T21:14:05.964271: step 120, loss 0.454704, acc 0.8
2016-09-07T21:14:06.631916: step 121, loss 0.580953, acc 0.78
2016-09-07T21:14:07.304380: step 122, loss 0.490818, acc 0.78
2016-09-07T21:14:07.955673: step 123, loss 0.546547, acc 0.74
2016-09-07T21:14:08.624703: step 124, loss 0.49297, acc 0.76
2016-09-07T21:14:09.296634: step 125, loss 0.683825, acc 0.62
2016-09-07T21:14:09.995732: step 126, loss 0.645871, acc 0.7
2016-09-07T21:14:10.653806: step 127, loss 0.639067, acc 0.66
2016-09-07T21:14:11.324513: step 128, loss 0.448943, acc 0.78
2016-09-07T21:14:11.992414: step 129, loss 0.458173, acc 0.78
2016-09-07T21:14:12.649319: step 130, loss 0.497123, acc 0.78
2016-09-07T21:14:13.317257: step 131, loss 0.515616, acc 0.76
2016-09-07T21:14:13.985326: step 132, loss 0.48367, acc 0.82
2016-09-07T21:14:14.654363: step 133, loss 0.562244, acc 0.74
2016-09-07T21:14:15.329947: step 134, loss 0.426581, acc 0.82
2016-09-07T21:14:15.990220: step 135, loss 0.561707, acc 0.68
2016-09-07T21:14:16.659037: step 136, loss 0.498799, acc 0.76
2016-09-07T21:14:17.334701: step 137, loss 0.530005, acc 0.68
2016-09-07T21:14:17.996057: step 138, loss 0.55479, acc 0.72
2016-09-07T21:14:18.672906: step 139, loss 0.478494, acc 0.74
2016-09-07T21:14:19.352338: step 140, loss 0.384855, acc 0.88
2016-09-07T21:14:20.018668: step 141, loss 0.566158, acc 0.64
2016-09-07T21:14:20.690210: step 142, loss 0.546197, acc 0.74
2016-09-07T21:14:21.367046: step 143, loss 0.604365, acc 0.66
2016-09-07T21:14:22.040523: step 144, loss 0.560415, acc 0.76
2016-09-07T21:14:22.681912: step 145, loss 0.49693, acc 0.78
2016-09-07T21:14:23.344875: step 146, loss 0.55746, acc 0.76
2016-09-07T21:14:24.002234: step 147, loss 0.556655, acc 0.66
2016-09-07T21:14:24.659313: step 148, loss 0.402329, acc 0.88
2016-09-07T21:14:25.328704: step 149, loss 0.542331, acc 0.72
2016-09-07T21:14:26.009331: step 150, loss 0.515641, acc 0.72
2016-09-07T21:14:26.658242: step 151, loss 0.577503, acc 0.68
2016-09-07T21:14:27.312795: step 152, loss 0.498459, acc 0.66
2016-09-07T21:14:27.997113: step 153, loss 0.558021, acc 0.74
2016-09-07T21:14:28.651849: step 154, loss 0.582186, acc 0.7
2016-09-07T21:14:29.317644: step 155, loss 0.478914, acc 0.76
2016-09-07T21:14:29.998692: step 156, loss 0.420035, acc 0.84
2016-09-07T21:14:30.673757: step 157, loss 0.495771, acc 0.72
2016-09-07T21:14:31.333801: step 158, loss 0.61025, acc 0.64
2016-09-07T21:14:32.017661: step 159, loss 0.497558, acc 0.78
2016-09-07T21:14:32.696860: step 160, loss 0.578219, acc 0.7
2016-09-07T21:14:33.397163: step 161, loss 0.558472, acc 0.76
2016-09-07T21:14:34.070321: step 162, loss 0.596438, acc 0.68
2016-09-07T21:14:34.741181: step 163, loss 0.462167, acc 0.74
2016-09-07T21:14:35.400918: step 164, loss 0.444028, acc 0.78
2016-09-07T21:14:36.066098: step 165, loss 0.48998, acc 0.82
2016-09-07T21:14:36.731933: step 166, loss 0.479716, acc 0.76
2016-09-07T21:14:37.379663: step 167, loss 0.496481, acc 0.72
2016-09-07T21:14:38.043049: step 168, loss 0.602461, acc 0.68
2016-09-07T21:14:38.707040: step 169, loss 0.560548, acc 0.68
2016-09-07T21:14:39.372365: step 170, loss 0.558347, acc 0.7
2016-09-07T21:14:40.037351: step 171, loss 0.615119, acc 0.66
2016-09-07T21:14:40.698155: step 172, loss 0.499604, acc 0.76
2016-09-07T21:14:41.375829: step 173, loss 0.556547, acc 0.78
2016-09-07T21:14:42.051113: step 174, loss 0.485467, acc 0.74
2016-09-07T21:14:42.720960: step 175, loss 0.43017, acc 0.84
2016-09-07T21:14:43.390923: step 176, loss 0.440847, acc 0.88
2016-09-07T21:14:44.055111: step 177, loss 0.407051, acc 0.8
2016-09-07T21:14:44.707058: step 178, loss 0.495747, acc 0.76
2016-09-07T21:14:45.377948: step 179, loss 0.449003, acc 0.8
2016-09-07T21:14:46.029641: step 180, loss 0.346125, acc 0.84
2016-09-07T21:14:46.698600: step 181, loss 0.548323, acc 0.72
2016-09-07T21:14:47.348832: step 182, loss 0.543345, acc 0.72
2016-09-07T21:14:48.031468: step 183, loss 0.478181, acc 0.78
2016-09-07T21:14:48.698983: step 184, loss 0.613085, acc 0.72
2016-09-07T21:14:49.376973: step 185, loss 0.570275, acc 0.74
2016-09-07T21:14:50.035534: step 186, loss 0.439615, acc 0.8
2016-09-07T21:14:50.709683: step 187, loss 0.484869, acc 0.76
2016-09-07T21:14:51.387306: step 188, loss 0.506389, acc 0.76
2016-09-07T21:14:52.063112: step 189, loss 0.416579, acc 0.86
2016-09-07T21:14:52.714604: step 190, loss 0.486703, acc 0.76
2016-09-07T21:14:53.378156: step 191, loss 0.611153, acc 0.72
2016-09-07T21:14:54.094412: step 192, loss 0.513542, acc 0.74
2016-09-07T21:14:54.767191: step 193, loss 0.428567, acc 0.8
2016-09-07T21:14:55.142371: step 194, loss 0.250068, acc 1
2016-09-07T21:14:55.809499: step 195, loss 0.436104, acc 0.8
2016-09-07T21:14:56.467134: step 196, loss 0.397983, acc 0.78
2016-09-07T21:14:57.131092: step 197, loss 0.294102, acc 0.92
2016-09-07T21:14:57.795923: step 198, loss 0.348467, acc 0.9
2016-09-07T21:14:58.458783: step 199, loss 0.362406, acc 0.78
2016-09-07T21:14:59.112384: step 200, loss 0.508278, acc 0.74

Evaluation:
2016-09-07T21:15:02.241516: step 200, loss 0.457887, acc 0.799

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-200

2016-09-07T21:15:03.846997: step 201, loss 0.419231, acc 0.76
2016-09-07T21:15:04.514626: step 202, loss 0.383253, acc 0.8
2016-09-07T21:15:05.165843: step 203, loss 0.309994, acc 0.86
2016-09-07T21:15:05.829456: step 204, loss 0.523187, acc 0.8
2016-09-07T21:15:06.500115: step 205, loss 0.348327, acc 0.84
2016-09-07T21:15:07.157551: step 206, loss 0.291297, acc 0.84
2016-09-07T21:15:07.830178: step 207, loss 0.604875, acc 0.7
2016-09-07T21:15:08.499463: step 208, loss 0.376848, acc 0.9
2016-09-07T21:15:09.170888: step 209, loss 0.310837, acc 0.9
2016-09-07T21:15:09.824654: step 210, loss 0.347309, acc 0.84
2016-09-07T21:15:10.506405: step 211, loss 0.265248, acc 0.94
2016-09-07T21:15:11.161457: step 212, loss 0.619853, acc 0.76
2016-09-07T21:15:11.823117: step 213, loss 0.301374, acc 0.9
2016-09-07T21:15:12.487050: step 214, loss 0.438097, acc 0.82
2016-09-07T21:15:13.157408: step 215, loss 0.243084, acc 0.92
2016-09-07T21:15:13.826962: step 216, loss 0.517863, acc 0.72
2016-09-07T21:15:14.498516: step 217, loss 0.386025, acc 0.84
2016-09-07T21:15:15.171629: step 218, loss 0.429438, acc 0.82
2016-09-07T21:15:15.837264: step 219, loss 0.349262, acc 0.82
2016-09-07T21:15:16.504186: step 220, loss 0.289717, acc 0.9
2016-09-07T21:15:17.172789: step 221, loss 0.29211, acc 0.88
2016-09-07T21:15:17.839139: step 222, loss 0.353183, acc 0.82
2016-09-07T21:15:18.517714: step 223, loss 0.409605, acc 0.78
2016-09-07T21:15:19.187981: step 224, loss 0.383692, acc 0.82
2016-09-07T21:15:19.860961: step 225, loss 0.448707, acc 0.78
2016-09-07T21:15:20.515383: step 226, loss 0.401943, acc 0.84
2016-09-07T21:15:21.176548: step 227, loss 0.245916, acc 0.94
2016-09-07T21:15:21.843750: step 228, loss 0.378186, acc 0.84
2016-09-07T21:15:22.514572: step 229, loss 0.248845, acc 0.88
2016-09-07T21:15:23.163305: step 230, loss 0.379303, acc 0.8
2016-09-07T21:15:23.827051: step 231, loss 0.337226, acc 0.86
2016-09-07T21:15:24.498760: step 232, loss 0.270619, acc 0.92
2016-09-07T21:15:25.170381: step 233, loss 0.436107, acc 0.8
2016-09-07T21:15:25.829385: step 234, loss 0.47316, acc 0.8
2016-09-07T21:15:26.492364: step 235, loss 0.308421, acc 0.92
2016-09-07T21:15:27.157041: step 236, loss 0.279168, acc 0.9
2016-09-07T21:15:27.837203: step 237, loss 0.418851, acc 0.82
2016-09-07T21:15:28.527930: step 238, loss 0.458216, acc 0.8
2016-09-07T21:15:29.211414: step 239, loss 0.524563, acc 0.78
2016-09-07T21:15:29.897870: step 240, loss 0.328433, acc 0.86
2016-09-07T21:15:30.559809: step 241, loss 0.304213, acc 0.9
2016-09-07T21:15:31.229445: step 242, loss 0.388404, acc 0.8
2016-09-07T21:15:31.883596: step 243, loss 0.540708, acc 0.76
2016-09-07T21:15:32.543515: step 244, loss 0.31748, acc 0.88
2016-09-07T21:15:33.220836: step 245, loss 0.238042, acc 0.9
2016-09-07T21:15:33.893429: step 246, loss 0.330321, acc 0.82
2016-09-07T21:15:34.547785: step 247, loss 0.295596, acc 0.9
2016-09-07T21:15:35.210991: step 248, loss 0.35628, acc 0.82
2016-09-07T21:15:35.879424: step 249, loss 0.416232, acc 0.78
2016-09-07T21:15:36.540710: step 250, loss 0.371012, acc 0.82
2016-09-07T21:15:37.197609: step 251, loss 0.532876, acc 0.7
2016-09-07T21:15:37.857922: step 252, loss 0.317922, acc 0.82
2016-09-07T21:15:38.513052: step 253, loss 0.413571, acc 0.78
2016-09-07T21:15:39.183710: step 254, loss 0.599703, acc 0.8
2016-09-07T21:15:39.833260: step 255, loss 0.442359, acc 0.76
2016-09-07T21:15:40.529734: step 256, loss 0.314473, acc 0.86
2016-09-07T21:15:41.195092: step 257, loss 0.351458, acc 0.82
2016-09-07T21:15:41.852416: step 258, loss 0.507669, acc 0.74
2016-09-07T21:15:42.509799: step 259, loss 0.438989, acc 0.78
2016-09-07T21:15:43.184024: step 260, loss 0.327294, acc 0.88
2016-09-07T21:15:43.858532: step 261, loss 0.289895, acc 0.88
2016-09-07T21:15:44.548629: step 262, loss 0.341491, acc 0.84
2016-09-07T21:15:45.211347: step 263, loss 0.421035, acc 0.78
2016-09-07T21:15:45.877131: step 264, loss 0.293581, acc 0.9
2016-09-07T21:15:46.536070: step 265, loss 0.375954, acc 0.82
2016-09-07T21:15:47.204478: step 266, loss 0.348604, acc 0.88
2016-09-07T21:15:47.867874: step 267, loss 0.236247, acc 0.84
2016-09-07T21:15:48.532784: step 268, loss 0.396354, acc 0.84
2016-09-07T21:15:49.214386: step 269, loss 0.292256, acc 0.92
2016-09-07T21:15:49.880331: step 270, loss 0.251772, acc 0.9
2016-09-07T21:15:50.558381: step 271, loss 0.331898, acc 0.82
2016-09-07T21:15:51.231580: step 272, loss 0.354791, acc 0.84
2016-09-07T21:15:51.893231: step 273, loss 0.367473, acc 0.86
2016-09-07T21:15:52.568245: step 274, loss 0.437306, acc 0.8
2016-09-07T21:15:53.215069: step 275, loss 0.399597, acc 0.8
2016-09-07T21:15:53.885806: step 276, loss 0.541208, acc 0.8
2016-09-07T21:15:54.549991: step 277, loss 0.472546, acc 0.8
2016-09-07T21:15:55.218627: step 278, loss 0.325229, acc 0.86
2016-09-07T21:15:55.892855: step 279, loss 0.452322, acc 0.84
2016-09-07T21:15:56.545164: step 280, loss 0.543043, acc 0.74
2016-09-07T21:15:57.237790: step 281, loss 0.397261, acc 0.8
2016-09-07T21:15:57.894727: step 282, loss 0.251813, acc 0.96
2016-09-07T21:15:58.542536: step 283, loss 0.469455, acc 0.82
2016-09-07T21:15:59.209113: step 284, loss 0.42974, acc 0.82
2016-09-07T21:15:59.862602: step 285, loss 0.413997, acc 0.8
2016-09-07T21:16:00.570963: step 286, loss 0.386362, acc 0.86
2016-09-07T21:16:01.246374: step 287, loss 0.334116, acc 0.84
2016-09-07T21:16:01.899379: step 288, loss 0.380501, acc 0.78
2016-09-07T21:16:02.575906: step 289, loss 0.411222, acc 0.76
2016-09-07T21:16:03.245465: step 290, loss 0.318947, acc 0.86
2016-09-07T21:16:03.907305: step 291, loss 0.255711, acc 0.94
2016-09-07T21:16:04.583733: step 292, loss 0.24574, acc 0.94
2016-09-07T21:16:05.250199: step 293, loss 0.469733, acc 0.8
2016-09-07T21:16:05.922553: step 294, loss 0.314481, acc 0.82
2016-09-07T21:16:06.595432: step 295, loss 0.311948, acc 0.86
2016-09-07T21:16:07.262880: step 296, loss 0.281466, acc 0.88
2016-09-07T21:16:07.927165: step 297, loss 0.315067, acc 0.86
2016-09-07T21:16:08.600226: step 298, loss 0.272578, acc 0.88
2016-09-07T21:16:09.280850: step 299, loss 0.591027, acc 0.7
2016-09-07T21:16:09.963173: step 300, loss 0.268856, acc 0.88

Evaluation:
2016-09-07T21:16:13.096138: step 300, loss 0.478717, acc 0.798

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-300

2016-09-07T21:16:14.727626: step 301, loss 0.317688, acc 0.86
2016-09-07T21:16:15.400390: step 302, loss 0.259391, acc 0.86
2016-09-07T21:16:16.057200: step 303, loss 0.487754, acc 0.86
2016-09-07T21:16:16.731269: step 304, loss 0.403344, acc 0.82
2016-09-07T21:16:17.418350: step 305, loss 0.420894, acc 0.84
2016-09-07T21:16:18.095725: step 306, loss 0.311008, acc 0.82
2016-09-07T21:16:18.754821: step 307, loss 0.335057, acc 0.86
2016-09-07T21:16:19.407142: step 308, loss 0.422046, acc 0.78
2016-09-07T21:16:20.069924: step 309, loss 0.389515, acc 0.88
2016-09-07T21:16:20.741251: step 310, loss 0.275622, acc 0.88
2016-09-07T21:16:21.408008: step 311, loss 0.264149, acc 0.88
2016-09-07T21:16:22.089645: step 312, loss 0.29738, acc 0.88
2016-09-07T21:16:22.743819: step 313, loss 0.345142, acc 0.86
2016-09-07T21:16:23.397280: step 314, loss 0.52275, acc 0.78
2016-09-07T21:16:24.072070: step 315, loss 0.470456, acc 0.7
2016-09-07T21:16:24.723519: step 316, loss 0.432425, acc 0.82
2016-09-07T21:16:25.376988: step 317, loss 0.329525, acc 0.86
2016-09-07T21:16:26.034163: step 318, loss 0.224154, acc 0.94
2016-09-07T21:16:26.741928: step 319, loss 0.269433, acc 0.9
2016-09-07T21:16:27.394678: step 320, loss 0.399553, acc 0.82
2016-09-07T21:16:28.063998: step 321, loss 0.291217, acc 0.9
2016-09-07T21:16:28.742190: step 322, loss 0.388749, acc 0.8
2016-09-07T21:16:29.418967: step 323, loss 0.384348, acc 0.8
2016-09-07T21:16:30.085924: step 324, loss 0.307627, acc 0.9
2016-09-07T21:16:30.747676: step 325, loss 0.510045, acc 0.72
2016-09-07T21:16:31.393046: step 326, loss 0.461706, acc 0.78
2016-09-07T21:16:32.059528: step 327, loss 0.429419, acc 0.8
2016-09-07T21:16:32.716421: step 328, loss 0.423793, acc 0.82
2016-09-07T21:16:33.382613: step 329, loss 0.406798, acc 0.82
2016-09-07T21:16:34.046090: step 330, loss 0.447818, acc 0.82
2016-09-07T21:16:34.722499: step 331, loss 0.22463, acc 0.92
2016-09-07T21:16:35.413385: step 332, loss 0.324082, acc 0.86
2016-09-07T21:16:36.073756: step 333, loss 0.339439, acc 0.88
2016-09-07T21:16:36.724292: step 334, loss 0.280619, acc 0.92
2016-09-07T21:16:37.377660: step 335, loss 0.293704, acc 0.88
2016-09-07T21:16:38.046111: step 336, loss 0.358229, acc 0.88
2016-09-07T21:16:38.723525: step 337, loss 0.386674, acc 0.86
2016-09-07T21:16:39.392383: step 338, loss 0.384842, acc 0.8
2016-09-07T21:16:40.056885: step 339, loss 0.308336, acc 0.86
2016-09-07T21:16:40.718330: step 340, loss 0.281902, acc 0.84
2016-09-07T21:16:41.372759: step 341, loss 0.286808, acc 0.86
2016-09-07T21:16:42.040175: step 342, loss 0.319584, acc 0.82
2016-09-07T21:16:42.708408: step 343, loss 0.32388, acc 0.86
2016-09-07T21:16:43.380747: step 344, loss 0.427974, acc 0.8
2016-09-07T21:16:44.061746: step 345, loss 0.4214, acc 0.84
2016-09-07T21:16:44.729729: step 346, loss 0.501397, acc 0.74
2016-09-07T21:16:45.404075: step 347, loss 0.287326, acc 0.88
2016-09-07T21:16:46.070073: step 348, loss 0.263585, acc 0.9
2016-09-07T21:16:46.730324: step 349, loss 0.466242, acc 0.78
2016-09-07T21:16:47.388211: step 350, loss 0.275966, acc 0.86
2016-09-07T21:16:48.058490: step 351, loss 0.451029, acc 0.72
2016-09-07T21:16:48.727518: step 352, loss 0.411591, acc 0.84
2016-09-07T21:16:49.380807: step 353, loss 0.337651, acc 0.86
2016-09-07T21:16:50.040458: step 354, loss 0.426206, acc 0.78
2016-09-07T21:16:50.691930: step 355, loss 0.360249, acc 0.84
2016-09-07T21:16:51.349049: step 356, loss 0.342092, acc 0.84
2016-09-07T21:16:52.016056: step 357, loss 0.472758, acc 0.8
2016-09-07T21:16:52.671591: step 358, loss 0.347241, acc 0.8
2016-09-07T21:16:53.313119: step 359, loss 0.472466, acc 0.78
2016-09-07T21:16:53.984568: step 360, loss 0.545293, acc 0.74
2016-09-07T21:16:54.664845: step 361, loss 0.373251, acc 0.84
2016-09-07T21:16:55.316163: step 362, loss 0.377421, acc 0.84
2016-09-07T21:16:55.974677: step 363, loss 0.27628, acc 0.9
2016-09-07T21:16:56.655831: step 364, loss 0.345537, acc 0.86
2016-09-07T21:16:57.326026: step 365, loss 0.460938, acc 0.76
2016-09-07T21:16:57.985474: step 366, loss 0.393673, acc 0.82
2016-09-07T21:16:58.652486: step 367, loss 0.339405, acc 0.92
2016-09-07T21:16:59.342008: step 368, loss 0.353721, acc 0.82
2016-09-07T21:17:00.017583: step 369, loss 0.394424, acc 0.84
2016-09-07T21:17:00.722693: step 370, loss 0.444205, acc 0.78
2016-09-07T21:17:01.375064: step 371, loss 0.436706, acc 0.78
2016-09-07T21:17:02.055317: step 372, loss 0.33747, acc 0.86
2016-09-07T21:17:02.712194: step 373, loss 0.510849, acc 0.76
2016-09-07T21:17:03.366356: step 374, loss 0.452526, acc 0.82
2016-09-07T21:17:04.034934: step 375, loss 0.303567, acc 0.9
2016-09-07T21:17:04.694030: step 376, loss 0.314142, acc 0.88
2016-09-07T21:17:05.368355: step 377, loss 0.508878, acc 0.72
2016-09-07T21:17:06.051380: step 378, loss 0.329091, acc 0.86
2016-09-07T21:17:06.707507: step 379, loss 0.390588, acc 0.8
2016-09-07T21:17:07.380907: step 380, loss 0.482604, acc 0.8
2016-09-07T21:17:08.055891: step 381, loss 0.416413, acc 0.84
2016-09-07T21:17:08.735724: step 382, loss 0.389703, acc 0.78
2016-09-07T21:17:09.400673: step 383, loss 0.43389, acc 0.78
2016-09-07T21:17:10.064382: step 384, loss 0.407114, acc 0.8
2016-09-07T21:17:10.733622: step 385, loss 0.349759, acc 0.88
2016-09-07T21:17:11.424256: step 386, loss 0.373611, acc 0.84
2016-09-07T21:17:12.089312: step 387, loss 0.44285, acc 0.82
2016-09-07T21:17:12.433979: step 388, loss 0.343848, acc 0.833333
2016-09-07T21:17:13.100797: step 389, loss 0.381345, acc 0.82
2016-09-07T21:17:13.783304: step 390, loss 0.240893, acc 0.9
2016-09-07T21:17:14.452340: step 391, loss 0.275201, acc 0.9
2016-09-07T21:17:15.104835: step 392, loss 0.218165, acc 0.9
2016-09-07T21:17:15.762041: step 393, loss 0.256244, acc 0.88
2016-09-07T21:17:16.409779: step 394, loss 0.261972, acc 0.92
2016-09-07T21:17:17.073441: step 395, loss 0.19825, acc 0.94
2016-09-07T21:17:17.736072: step 396, loss 0.226574, acc 0.92
2016-09-07T21:17:18.396552: step 397, loss 0.18932, acc 0.94
2016-09-07T21:17:19.082147: step 398, loss 0.22053, acc 0.92
2016-09-07T21:17:19.738692: step 399, loss 0.234889, acc 0.84
2016-09-07T21:17:20.402097: step 400, loss 0.170305, acc 0.92

Evaluation:
2016-09-07T21:17:23.621154: step 400, loss 0.617313, acc 0.806

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-400

2016-09-07T21:17:25.274908: step 401, loss 0.28868, acc 0.92
2016-09-07T21:17:25.949712: step 402, loss 0.405982, acc 0.88
2016-09-07T21:17:26.627175: step 403, loss 0.388815, acc 0.9
2016-09-07T21:17:27.292389: step 404, loss 0.176747, acc 0.92
2016-09-07T21:17:27.954134: step 405, loss 0.262438, acc 0.9
2016-09-07T21:17:28.601192: step 406, loss 0.216054, acc 0.9
2016-09-07T21:17:29.288457: step 407, loss 0.096566, acc 0.96
2016-09-07T21:17:29.945096: step 408, loss 0.332062, acc 0.86
2016-09-07T21:17:30.599031: step 409, loss 0.436183, acc 0.86
2016-09-07T21:17:31.263052: step 410, loss 0.198977, acc 0.9
2016-09-07T21:17:31.915906: step 411, loss 0.259231, acc 0.94
2016-09-07T21:17:32.574075: step 412, loss 0.108873, acc 0.96
2016-09-07T21:17:33.243957: step 413, loss 0.243108, acc 0.92
2016-09-07T21:17:33.907825: step 414, loss 0.246365, acc 0.94
2016-09-07T21:17:34.565840: step 415, loss 0.328292, acc 0.86
2016-09-07T21:17:35.230886: step 416, loss 0.209754, acc 0.94
2016-09-07T21:17:35.900737: step 417, loss 0.204095, acc 0.92
2016-09-07T21:17:36.567107: step 418, loss 0.314656, acc 0.86
2016-09-07T21:17:37.240156: step 419, loss 0.131625, acc 0.94
2016-09-07T21:17:37.919166: step 420, loss 0.212751, acc 0.9
2016-09-07T21:17:38.593063: step 421, loss 0.36349, acc 0.86
2016-09-07T21:17:39.249424: step 422, loss 0.243066, acc 0.88
2016-09-07T21:17:39.906209: step 423, loss 0.282514, acc 0.88
2016-09-07T21:17:40.555352: step 424, loss 0.361442, acc 0.86
2016-09-07T21:17:41.218518: step 425, loss 0.184238, acc 0.96
2016-09-07T21:17:41.892056: step 426, loss 0.204848, acc 0.96
2016-09-07T21:17:42.550816: step 427, loss 0.337238, acc 0.88
2016-09-07T21:17:43.239574: step 428, loss 0.353802, acc 0.82
2016-09-07T21:17:43.915721: step 429, loss 0.175462, acc 0.9
2016-09-07T21:17:44.566120: step 430, loss 0.166864, acc 0.94
2016-09-07T21:17:45.221415: step 431, loss 0.327021, acc 0.86
2016-09-07T21:17:45.883372: step 432, loss 0.273476, acc 0.94
2016-09-07T21:17:46.543347: step 433, loss 0.167937, acc 0.96
2016-09-07T21:17:47.211307: step 434, loss 0.376386, acc 0.9
2016-09-07T21:17:47.881375: step 435, loss 0.279671, acc 0.84
2016-09-07T21:17:48.544840: step 436, loss 0.391283, acc 0.88
2016-09-07T21:17:49.200007: step 437, loss 0.154842, acc 0.96
2016-09-07T21:17:49.880979: step 438, loss 0.186862, acc 0.92
2016-09-07T21:17:50.552954: step 439, loss 0.127046, acc 0.96
2016-09-07T21:17:51.234781: step 440, loss 0.319332, acc 0.88
2016-09-07T21:17:51.896359: step 441, loss 0.188854, acc 0.92
2016-09-07T21:17:52.570324: step 442, loss 0.152442, acc 0.94
2016-09-07T21:17:53.234856: step 443, loss 0.21781, acc 0.84
2016-09-07T21:17:53.889997: step 444, loss 0.356684, acc 0.76
2016-09-07T21:17:54.551769: step 445, loss 0.28238, acc 0.92
2016-09-07T21:17:55.230584: step 446, loss 0.246435, acc 0.86
2016-09-07T21:17:55.887362: step 447, loss 0.234423, acc 0.84
2016-09-07T21:17:56.552315: step 448, loss 0.127402, acc 0.98
2016-09-07T21:17:57.257365: step 449, loss 0.151402, acc 0.96
2016-09-07T21:17:57.919918: step 450, loss 0.288055, acc 0.88
2016-09-07T21:17:58.574440: step 451, loss 0.19359, acc 0.94
2016-09-07T21:17:59.237119: step 452, loss 0.251478, acc 0.9
2016-09-07T21:17:59.899084: step 453, loss 0.240706, acc 0.94
2016-09-07T21:18:00.619121: step 454, loss 0.204552, acc 0.88
2016-09-07T21:18:01.276650: step 455, loss 0.327561, acc 0.86
2016-09-07T21:18:01.937961: step 456, loss 0.177971, acc 0.96
2016-09-07T21:18:02.597949: step 457, loss 0.248308, acc 0.88
2016-09-07T21:18:03.278167: step 458, loss 0.333108, acc 0.82
2016-09-07T21:18:03.955337: step 459, loss 0.292713, acc 0.84
2016-09-07T21:18:04.613587: step 460, loss 0.193115, acc 0.92
2016-09-07T21:18:05.287280: step 461, loss 0.214122, acc 0.92
2016-09-07T21:18:05.940685: step 462, loss 0.30815, acc 0.88
2016-09-07T21:18:06.628161: step 463, loss 0.289559, acc 0.92
2016-09-07T21:18:07.282951: step 464, loss 0.343747, acc 0.82
2016-09-07T21:18:07.931609: step 465, loss 0.332746, acc 0.84
2016-09-07T21:18:08.593724: step 466, loss 0.152692, acc 0.96
2016-09-07T21:18:09.257746: step 467, loss 0.217085, acc 0.92
2016-09-07T21:18:09.929813: step 468, loss 0.165176, acc 0.94
2016-09-07T21:18:10.589642: step 469, loss 0.239042, acc 0.86
2016-09-07T21:18:11.273820: step 470, loss 0.344656, acc 0.82
2016-09-07T21:18:11.944606: step 471, loss 0.296739, acc 0.88
2016-09-07T21:18:12.625550: step 472, loss 0.361978, acc 0.86
2016-09-07T21:18:13.296752: step 473, loss 0.251365, acc 0.88
2016-09-07T21:18:13.960698: step 474, loss 0.350767, acc 0.8
2016-09-07T21:18:14.644653: step 475, loss 0.154976, acc 0.96
2016-09-07T21:18:15.308407: step 476, loss 0.222463, acc 0.9
2016-09-07T21:18:15.954127: step 477, loss 0.36122, acc 0.84
2016-09-07T21:18:16.617017: step 478, loss 0.343075, acc 0.88
2016-09-07T21:18:17.289898: step 479, loss 0.38252, acc 0.82
2016-09-07T21:18:17.968558: step 480, loss 0.217413, acc 0.9
2016-09-07T21:18:18.648947: step 481, loss 0.185741, acc 0.96
2016-09-07T21:18:19.326538: step 482, loss 0.240206, acc 0.92
2016-09-07T21:18:19.996915: step 483, loss 0.242392, acc 0.86
2016-09-07T21:18:20.676275: step 484, loss 0.165754, acc 0.96
2016-09-07T21:18:21.359875: step 485, loss 0.344454, acc 0.82
2016-09-07T21:18:22.009345: step 486, loss 0.312108, acc 0.86
2016-09-07T21:18:22.685857: step 487, loss 0.192048, acc 0.94
2016-09-07T21:18:23.331638: step 488, loss 0.298558, acc 0.84
2016-09-07T21:18:24.003941: step 489, loss 0.319172, acc 0.9
2016-09-07T21:18:24.678764: step 490, loss 0.230964, acc 0.9
2016-09-07T21:18:25.338338: step 491, loss 0.216139, acc 0.9
2016-09-07T21:18:26.006326: step 492, loss 0.181455, acc 0.94
2016-09-07T21:18:26.664053: step 493, loss 0.159086, acc 0.94
2016-09-07T21:18:27.315920: step 494, loss 0.253028, acc 0.9
2016-09-07T21:18:27.980910: step 495, loss 0.629749, acc 0.84
2016-09-07T21:18:28.631634: step 496, loss 0.292554, acc 0.86
2016-09-07T21:18:29.304684: step 497, loss 0.245911, acc 0.92
2016-09-07T21:18:29.979769: step 498, loss 0.374951, acc 0.86
2016-09-07T21:18:30.659910: step 499, loss 0.287133, acc 0.86
2016-09-07T21:18:31.340013: step 500, loss 0.210418, acc 0.9

Evaluation:
2016-09-07T21:18:34.604509: step 500, loss 0.498928, acc 0.785

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-500

2016-09-07T21:18:36.404549: step 501, loss 0.353574, acc 0.86
2016-09-07T21:18:37.062483: step 502, loss 0.281548, acc 0.88
2016-09-07T21:18:37.728118: step 503, loss 0.216632, acc 0.9
2016-09-07T21:18:38.401991: step 504, loss 0.253149, acc 0.88
2016-09-07T21:18:39.058823: step 505, loss 0.27711, acc 0.92
2016-09-07T21:18:39.724998: step 506, loss 0.291696, acc 0.86
2016-09-07T21:18:40.388800: step 507, loss 0.21719, acc 0.9
2016-09-07T21:18:41.098233: step 508, loss 0.203156, acc 0.92
2016-09-07T21:18:41.797345: step 509, loss 0.181776, acc 0.94
2016-09-07T21:18:42.479881: step 510, loss 0.396919, acc 0.8
2016-09-07T21:18:43.154707: step 511, loss 0.170608, acc 0.94
2016-09-07T21:18:43.821563: step 512, loss 0.27146, acc 0.94
2016-09-07T21:18:44.502548: step 513, loss 0.222358, acc 0.96
2016-09-07T21:18:45.176956: step 514, loss 0.370502, acc 0.78
2016-09-07T21:18:45.837692: step 515, loss 0.172234, acc 0.96
2016-09-07T21:18:46.503350: step 516, loss 0.263943, acc 0.88
2016-09-07T21:18:47.172629: step 517, loss 0.208285, acc 0.88
2016-09-07T21:18:47.834705: step 518, loss 0.322073, acc 0.9
2016-09-07T21:18:48.494674: step 519, loss 0.37728, acc 0.84
2016-09-07T21:18:49.141823: step 520, loss 0.370707, acc 0.82
2016-09-07T21:18:49.805410: step 521, loss 0.230285, acc 0.9
2016-09-07T21:18:50.476218: step 522, loss 0.247593, acc 0.88
2016-09-07T21:18:51.146457: step 523, loss 0.286793, acc 0.84
2016-09-07T21:18:51.808003: step 524, loss 0.349772, acc 0.84
2016-09-07T21:18:52.492824: step 525, loss 0.339871, acc 0.84
2016-09-07T21:18:53.170798: step 526, loss 0.316165, acc 0.88
2016-09-07T21:18:53.845208: step 527, loss 0.227786, acc 0.9
2016-09-07T21:18:54.528768: step 528, loss 0.234617, acc 0.94
2016-09-07T21:18:55.204009: step 529, loss 0.335159, acc 0.84
2016-09-07T21:18:55.893541: step 530, loss 0.205641, acc 0.9
2016-09-07T21:18:56.536420: step 531, loss 0.261057, acc 0.9
2016-09-07T21:18:57.215212: step 532, loss 0.262422, acc 0.86
2016-09-07T21:18:57.860405: step 533, loss 0.32857, acc 0.84
2016-09-07T21:18:58.520685: step 534, loss 0.320662, acc 0.88
2016-09-07T21:18:59.191806: step 535, loss 0.256186, acc 0.82
2016-09-07T21:18:59.853696: step 536, loss 0.204505, acc 0.92
2016-09-07T21:19:00.565167: step 537, loss 0.161294, acc 0.96
2016-09-07T21:19:01.231956: step 538, loss 0.350986, acc 0.86
2016-09-07T21:19:01.880496: step 539, loss 0.278389, acc 0.88
2016-09-07T21:19:02.540854: step 540, loss 0.286827, acc 0.9
2016-09-07T21:19:03.217901: step 541, loss 0.492163, acc 0.84
2016-09-07T21:19:03.895349: step 542, loss 0.323632, acc 0.84
2016-09-07T21:19:04.553745: step 543, loss 0.241268, acc 0.9
2016-09-07T21:19:05.218863: step 544, loss 0.233166, acc 0.92
2016-09-07T21:19:05.886110: step 545, loss 0.215153, acc 0.88
2016-09-07T21:19:06.549881: step 546, loss 0.31207, acc 0.9
2016-09-07T21:19:07.206510: step 547, loss 0.293409, acc 0.86
2016-09-07T21:19:07.891612: step 548, loss 0.304447, acc 0.86
2016-09-07T21:19:08.546098: step 549, loss 0.279972, acc 0.84
2016-09-07T21:19:09.237710: step 550, loss 0.194833, acc 0.92
2016-09-07T21:19:09.920048: step 551, loss 0.23099, acc 0.88
2016-09-07T21:19:10.585482: step 552, loss 0.331995, acc 0.86
2016-09-07T21:19:11.243505: step 553, loss 0.264747, acc 0.88
2016-09-07T21:19:11.913101: step 554, loss 0.182603, acc 0.94
2016-09-07T21:19:12.600502: step 555, loss 0.218005, acc 0.92
2016-09-07T21:19:13.268337: step 556, loss 0.301554, acc 0.86
2016-09-07T21:19:13.919005: step 557, loss 0.279623, acc 0.82
2016-09-07T21:19:14.581205: step 558, loss 0.386957, acc 0.84
2016-09-07T21:19:15.249781: step 559, loss 0.262365, acc 0.86
2016-09-07T21:19:15.914660: step 560, loss 0.258983, acc 0.92
2016-09-07T21:19:16.588491: step 561, loss 0.249275, acc 0.88
2016-09-07T21:19:17.273673: step 562, loss 0.151112, acc 0.96
2016-09-07T21:19:17.934465: step 563, loss 0.34798, acc 0.86
2016-09-07T21:19:18.603054: step 564, loss 0.363594, acc 0.84
2016-09-07T21:19:19.262351: step 565, loss 0.128927, acc 0.98
2016-09-07T21:19:19.926254: step 566, loss 0.529547, acc 0.78
2016-09-07T21:19:20.603322: step 567, loss 0.333886, acc 0.86
2016-09-07T21:19:21.276051: step 568, loss 0.187333, acc 0.9
2016-09-07T21:19:21.952185: step 569, loss 0.407195, acc 0.8
2016-09-07T21:19:22.610286: step 570, loss 0.201227, acc 0.9
2016-09-07T21:19:23.276598: step 571, loss 0.195025, acc 0.92
2016-09-07T21:19:23.932267: step 572, loss 0.296174, acc 0.88
2016-09-07T21:19:24.586834: step 573, loss 0.294316, acc 0.86
2016-09-07T21:19:25.254849: step 574, loss 0.179551, acc 0.92
2016-09-07T21:19:25.929499: step 575, loss 0.278097, acc 0.88
2016-09-07T21:19:26.591988: step 576, loss 0.244452, acc 0.92
2016-09-07T21:19:27.263825: step 577, loss 0.281492, acc 0.86
2016-09-07T21:19:27.934445: step 578, loss 0.364405, acc 0.84
2016-09-07T21:19:28.642989: step 579, loss 0.253496, acc 0.9
2016-09-07T21:19:29.308944: step 580, loss 0.214809, acc 0.86
2016-09-07T21:19:29.986984: step 581, loss 0.220792, acc 0.92
2016-09-07T21:19:30.352069: step 582, loss 0.279342, acc 0.833333
2016-09-07T21:19:31.019939: step 583, loss 0.118987, acc 0.98
2016-09-07T21:19:31.684858: step 584, loss 0.19662, acc 0.92
2016-09-07T21:19:32.359434: step 585, loss 0.316141, acc 0.88
2016-09-07T21:19:33.022600: step 586, loss 0.186638, acc 0.92
2016-09-07T21:19:33.674810: step 587, loss 0.176078, acc 0.94
2016-09-07T21:19:34.346662: step 588, loss 0.257219, acc 0.9
2016-09-07T21:19:34.998922: step 589, loss 0.0899429, acc 0.96
2016-09-07T21:19:35.654568: step 590, loss 0.183476, acc 0.94
2016-09-07T21:19:36.309098: step 591, loss 0.0799161, acc 1
2016-09-07T21:19:36.965271: step 592, loss 0.325281, acc 0.88
2016-09-07T21:19:37.632893: step 593, loss 0.132617, acc 0.92
2016-09-07T21:19:38.319691: step 594, loss 0.0858484, acc 0.96
2016-09-07T21:19:38.990675: step 595, loss 0.29258, acc 0.88
2016-09-07T21:19:39.660167: step 596, loss 0.0819289, acc 0.98
2016-09-07T21:19:40.306704: step 597, loss 0.16016, acc 0.94
2016-09-07T21:19:41.002328: step 598, loss 0.20829, acc 0.96
2016-09-07T21:19:41.671018: step 599, loss 0.229987, acc 0.86
2016-09-07T21:19:42.340070: step 600, loss 0.0764317, acc 0.98

Evaluation:
2016-09-07T21:19:45.648125: step 600, loss 0.640884, acc 0.781

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-600

2016-09-07T21:19:47.456564: step 601, loss 0.0984672, acc 0.98
2016-09-07T21:19:48.133605: step 602, loss 0.145491, acc 0.92
2016-09-07T21:19:48.786959: step 603, loss 0.134059, acc 0.94
2016-09-07T21:19:49.451628: step 604, loss 0.254454, acc 0.92
2016-09-07T21:19:50.129217: step 605, loss 0.261945, acc 0.94
2016-09-07T21:19:50.781630: step 606, loss 0.146827, acc 0.94
2016-09-07T21:19:51.441888: step 607, loss 0.129223, acc 0.94
2016-09-07T21:19:52.106418: step 608, loss 0.102294, acc 0.98
2016-09-07T21:19:52.773063: step 609, loss 0.178955, acc 0.92
2016-09-07T21:19:53.435402: step 610, loss 0.0414493, acc 1
2016-09-07T21:19:54.105112: step 611, loss 0.149205, acc 0.94
2016-09-07T21:19:54.780967: step 612, loss 0.0700894, acc 0.98
2016-09-07T21:19:55.455905: step 613, loss 0.211003, acc 0.92
2016-09-07T21:19:56.140220: step 614, loss 0.166767, acc 0.92
2016-09-07T21:19:56.788919: step 615, loss 0.126497, acc 0.94
2016-09-07T21:19:57.459578: step 616, loss 0.187014, acc 0.9
2016-09-07T21:19:58.135917: step 617, loss 0.0571088, acc 0.98
2016-09-07T21:19:58.788498: step 618, loss 0.262915, acc 0.88
2016-09-07T21:19:59.435749: step 619, loss 0.125186, acc 0.94
2016-09-07T21:20:00.092321: step 620, loss 0.105473, acc 0.94
2016-09-07T21:20:00.793008: step 621, loss 0.107916, acc 0.98
2016-09-07T21:20:01.484210: step 622, loss 0.0887407, acc 0.94
2016-09-07T21:20:02.169671: step 623, loss 0.161134, acc 0.92
2016-09-07T21:20:02.831470: step 624, loss 0.233232, acc 0.88
2016-09-07T21:20:03.509105: step 625, loss 0.0581371, acc 0.98
2016-09-07T21:20:04.187942: step 626, loss 0.346188, acc 0.9
2016-09-07T21:20:04.856540: step 627, loss 0.228356, acc 0.88
2016-09-07T21:20:05.530088: step 628, loss 0.138012, acc 0.94
2016-09-07T21:20:06.191319: step 629, loss 0.271332, acc 0.86
2016-09-07T21:20:06.858476: step 630, loss 0.138669, acc 0.94
2016-09-07T21:20:07.531276: step 631, loss 0.168667, acc 0.96
2016-09-07T21:20:08.193768: step 632, loss 0.214056, acc 0.92
2016-09-07T21:20:08.848449: step 633, loss 0.086267, acc 1
2016-09-07T21:20:09.485938: step 634, loss 0.14683, acc 0.92
2016-09-07T21:20:10.142266: step 635, loss 0.14029, acc 0.92
2016-09-07T21:20:10.811634: step 636, loss 0.349279, acc 0.9
2016-09-07T21:20:11.468042: step 637, loss 0.239405, acc 0.94
2016-09-07T21:20:12.152527: step 638, loss 0.1783, acc 0.9
2016-09-07T21:20:12.827841: step 639, loss 0.201511, acc 0.9
2016-09-07T21:20:13.500307: step 640, loss 0.114753, acc 1
2016-09-07T21:20:14.169647: step 641, loss 0.223506, acc 0.92
2016-09-07T21:20:14.871388: step 642, loss 0.216282, acc 0.94
2016-09-07T21:20:15.531245: step 643, loss 0.214677, acc 0.9
2016-09-07T21:20:16.198992: step 644, loss 0.113388, acc 0.96
2016-09-07T21:20:16.853084: step 645, loss 0.160934, acc 0.94
2016-09-07T21:20:17.548034: step 646, loss 0.251252, acc 0.92
2016-09-07T21:20:18.265777: step 647, loss 0.10679, acc 0.98
2016-09-07T21:20:18.951268: step 648, loss 0.237285, acc 0.94
2016-09-07T21:20:19.610341: step 649, loss 0.143727, acc 0.96
2016-09-07T21:20:20.288146: step 650, loss 0.1087, acc 0.98
2016-09-07T21:20:20.959509: step 651, loss 0.154303, acc 0.96
2016-09-07T21:20:21.630949: step 652, loss 0.118236, acc 0.96
2016-09-07T21:20:22.302246: step 653, loss 0.188459, acc 0.92
2016-09-07T21:20:22.980694: step 654, loss 0.260175, acc 0.88
2016-09-07T21:20:23.661250: step 655, loss 0.199511, acc 0.94
2016-09-07T21:20:24.344122: step 656, loss 0.186411, acc 0.94
2016-09-07T21:20:25.007252: step 657, loss 0.18935, acc 0.92
2016-09-07T21:20:25.675216: step 658, loss 0.25067, acc 0.86
2016-09-07T21:20:26.350603: step 659, loss 0.22657, acc 0.92
2016-09-07T21:20:27.029001: step 660, loss 0.115815, acc 0.98
2016-09-07T21:20:27.704028: step 661, loss 0.218366, acc 0.9
2016-09-07T21:20:28.388602: step 662, loss 0.206091, acc 0.86
2016-09-07T21:20:29.061511: step 663, loss 0.19198, acc 0.94
2016-09-07T21:20:29.754319: step 664, loss 0.178997, acc 0.9
2016-09-07T21:20:30.435286: step 665, loss 0.23365, acc 0.88
2016-09-07T21:20:31.137363: step 666, loss 0.142869, acc 0.92
2016-09-07T21:20:31.815776: step 667, loss 0.131655, acc 0.92
2016-09-07T21:20:32.503435: step 668, loss 0.260333, acc 0.86
2016-09-07T21:20:33.188959: step 669, loss 0.272782, acc 0.92
2016-09-07T21:20:33.872765: step 670, loss 0.0675661, acc 1
2016-09-07T21:20:34.539532: step 671, loss 0.0904045, acc 0.96
2016-09-07T21:20:35.205716: step 672, loss 0.172552, acc 0.9
2016-09-07T21:20:35.879395: step 673, loss 0.164292, acc 0.94
2016-09-07T21:20:36.547878: step 674, loss 0.374825, acc 0.88
2016-09-07T21:20:37.216015: step 675, loss 0.211103, acc 0.92
2016-09-07T21:20:37.896200: step 676, loss 0.111115, acc 0.94
2016-09-07T21:20:38.569987: step 677, loss 0.149863, acc 0.94
2016-09-07T21:20:39.239316: step 678, loss 0.183393, acc 0.92
2016-09-07T21:20:39.928214: step 679, loss 0.158397, acc 0.96
2016-09-07T21:20:40.599127: step 680, loss 0.131635, acc 0.96
2016-09-07T21:20:41.319281: step 681, loss 0.131803, acc 0.94
2016-09-07T21:20:42.030062: step 682, loss 0.199778, acc 0.92
2016-09-07T21:20:42.732486: step 683, loss 0.195186, acc 0.86
2016-09-07T21:20:43.403618: step 684, loss 0.203986, acc 0.9
2016-09-07T21:20:44.084121: step 685, loss 0.186764, acc 0.92
2016-09-07T21:20:44.746982: step 686, loss 0.231739, acc 0.92
2016-09-07T21:20:45.404286: step 687, loss 0.118954, acc 0.96
2016-09-07T21:20:46.083520: step 688, loss 0.225586, acc 0.94
2016-09-07T21:20:46.751601: step 689, loss 0.19727, acc 0.92
2016-09-07T21:20:47.443396: step 690, loss 0.253263, acc 0.88
2016-09-07T21:20:48.106412: step 691, loss 0.312671, acc 0.84
2016-09-07T21:20:48.774020: step 692, loss 0.188391, acc 0.92
2016-09-07T21:20:49.440219: step 693, loss 0.203513, acc 0.92
2016-09-07T21:20:50.121906: step 694, loss 0.114286, acc 0.96
2016-09-07T21:20:50.813014: step 695, loss 0.18074, acc 0.94
2016-09-07T21:20:51.478079: step 696, loss 0.248214, acc 0.88
2016-09-07T21:20:52.142120: step 697, loss 0.186405, acc 0.9
2016-09-07T21:20:52.805119: step 698, loss 0.258138, acc 0.88
2016-09-07T21:20:53.490516: step 699, loss 0.253068, acc 0.88
2016-09-07T21:20:54.168128: step 700, loss 0.224305, acc 0.88

Evaluation:
2016-09-07T21:20:57.506215: step 700, loss 0.613162, acc 0.78

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-700

2016-09-07T21:20:59.239256: step 701, loss 0.266968, acc 0.9
2016-09-07T21:20:59.925925: step 702, loss 0.162955, acc 0.92
2016-09-07T21:21:00.645269: step 703, loss 0.085688, acc 0.96
2016-09-07T21:21:01.338699: step 704, loss 0.199231, acc 0.94
2016-09-07T21:21:02.006433: step 705, loss 0.145253, acc 0.94
2016-09-07T21:21:02.717134: step 706, loss 0.131986, acc 0.92
2016-09-07T21:21:03.384202: step 707, loss 0.154196, acc 0.94
2016-09-07T21:21:04.070316: step 708, loss 0.121728, acc 0.94
2016-09-07T21:21:04.766915: step 709, loss 0.213592, acc 0.9
2016-09-07T21:21:05.426413: step 710, loss 0.0787418, acc 0.96
2016-09-07T21:21:06.084442: step 711, loss 0.154516, acc 0.94
2016-09-07T21:21:06.747475: step 712, loss 0.125699, acc 0.94
2016-09-07T21:21:07.391912: step 713, loss 0.15298, acc 0.96
2016-09-07T21:21:08.068741: step 714, loss 0.152403, acc 0.94
2016-09-07T21:21:08.752833: step 715, loss 0.100085, acc 0.96
2016-09-07T21:21:09.418589: step 716, loss 0.26498, acc 0.9
2016-09-07T21:21:10.089699: step 717, loss 0.313886, acc 0.88
2016-09-07T21:21:10.762398: step 718, loss 0.220491, acc 0.92
2016-09-07T21:21:11.419371: step 719, loss 0.235832, acc 0.9
2016-09-07T21:21:12.076060: step 720, loss 0.182667, acc 0.88
2016-09-07T21:21:12.749089: step 721, loss 0.206007, acc 0.9
2016-09-07T21:21:13.392339: step 722, loss 0.275942, acc 0.86
2016-09-07T21:21:14.061363: step 723, loss 0.188099, acc 0.92
2016-09-07T21:21:14.741103: step 724, loss 0.179204, acc 0.92
2016-09-07T21:21:15.411615: step 725, loss 0.17118, acc 0.94
2016-09-07T21:21:16.084209: step 726, loss 0.142245, acc 0.94
2016-09-07T21:21:16.756022: step 727, loss 0.20412, acc 0.96
2016-09-07T21:21:17.439650: step 728, loss 0.267555, acc 0.88
2016-09-07T21:21:18.098576: step 729, loss 0.213423, acc 0.9
2016-09-07T21:21:18.781359: step 730, loss 0.143933, acc 0.96
2016-09-07T21:21:19.437222: step 731, loss 0.142656, acc 0.98
2016-09-07T21:21:20.101394: step 732, loss 0.21861, acc 0.92
2016-09-07T21:21:20.755798: step 733, loss 0.195038, acc 0.94
2016-09-07T21:21:21.413897: step 734, loss 0.153332, acc 0.96
2016-09-07T21:21:22.097251: step 735, loss 0.190822, acc 0.92
2016-09-07T21:21:22.777701: step 736, loss 0.354717, acc 0.86
2016-09-07T21:21:23.446784: step 737, loss 0.157201, acc 0.96
2016-09-07T21:21:24.121335: step 738, loss 0.117113, acc 0.96
2016-09-07T21:21:24.805031: step 739, loss 0.140807, acc 0.94
2016-09-07T21:21:25.457624: step 740, loss 0.208634, acc 0.92
2016-09-07T21:21:26.143320: step 741, loss 0.178952, acc 0.94
2016-09-07T21:21:26.805243: step 742, loss 0.207009, acc 0.94
2016-09-07T21:21:27.471680: step 743, loss 0.133394, acc 0.96
2016-09-07T21:21:28.154441: step 744, loss 0.352528, acc 0.9
2016-09-07T21:21:28.816689: step 745, loss 0.174587, acc 0.94
2016-09-07T21:21:29.501295: step 746, loss 0.171742, acc 0.92
2016-09-07T21:21:30.168724: step 747, loss 0.215913, acc 0.92
2016-09-07T21:21:30.850462: step 748, loss 0.188912, acc 0.9
2016-09-07T21:21:31.530929: step 749, loss 0.118121, acc 0.92
2016-09-07T21:21:32.234481: step 750, loss 0.136253, acc 0.92
2016-09-07T21:21:32.918676: step 751, loss 0.262471, acc 0.84
2016-09-07T21:21:33.585282: step 752, loss 0.231197, acc 0.92
2016-09-07T21:21:34.231320: step 753, loss 0.214737, acc 0.9
2016-09-07T21:21:34.923014: step 754, loss 0.292203, acc 0.88
2016-09-07T21:21:35.579532: step 755, loss 0.158293, acc 0.9
2016-09-07T21:21:36.252063: step 756, loss 0.188123, acc 0.94
2016-09-07T21:21:36.914156: step 757, loss 0.283128, acc 0.86
2016-09-07T21:21:37.607420: step 758, loss 0.145209, acc 0.92
2016-09-07T21:21:38.291970: step 759, loss 0.292903, acc 0.8
2016-09-07T21:21:38.966923: step 760, loss 0.146944, acc 0.94
2016-09-07T21:21:39.640019: step 761, loss 0.294874, acc 0.84
2016-09-07T21:21:40.313523: step 762, loss 0.240495, acc 0.9
2016-09-07T21:21:40.980945: step 763, loss 0.109158, acc 0.94
2016-09-07T21:21:41.666432: step 764, loss 0.0648179, acc 1
2016-09-07T21:21:42.348537: step 765, loss 0.0914706, acc 0.98
2016-09-07T21:21:43.026866: step 766, loss 0.16, acc 0.88
2016-09-07T21:21:43.700735: step 767, loss 0.295265, acc 0.9
2016-09-07T21:21:44.350541: step 768, loss 0.19432, acc 0.9
2016-09-07T21:21:45.040113: step 769, loss 0.276581, acc 0.86
2016-09-07T21:21:45.718491: step 770, loss 0.182502, acc 0.9
2016-09-07T21:21:46.397535: step 771, loss 0.245252, acc 0.86
2016-09-07T21:21:47.107177: step 772, loss 0.0985777, acc 0.98
2016-09-07T21:21:47.777132: step 773, loss 0.203576, acc 0.9
2016-09-07T21:21:48.452762: step 774, loss 0.241021, acc 0.88
2016-09-07T21:21:49.125201: step 775, loss 0.270888, acc 0.94
2016-09-07T21:21:49.497727: step 776, loss 0.0898836, acc 0.916667
2016-09-07T21:21:50.170536: step 777, loss 0.0689563, acc 1
2016-09-07T21:21:50.843003: step 778, loss 0.122011, acc 0.94
2016-09-07T21:21:51.529584: step 779, loss 0.188653, acc 0.92
2016-09-07T21:21:52.199091: step 780, loss 0.0838857, acc 0.98
2016-09-07T21:21:52.873166: step 781, loss 0.0969261, acc 0.96
2016-09-07T21:21:53.552093: step 782, loss 0.152268, acc 0.94
2016-09-07T21:21:54.220334: step 783, loss 0.136101, acc 0.96
2016-09-07T21:21:54.890081: step 784, loss 0.223562, acc 0.88
2016-09-07T21:21:55.561421: step 785, loss 0.204682, acc 0.92
2016-09-07T21:21:56.244235: step 786, loss 0.0842242, acc 0.98
2016-09-07T21:21:56.949883: step 787, loss 0.0673589, acc 1
2016-09-07T21:21:57.614965: step 788, loss 0.254974, acc 0.9
2016-09-07T21:21:58.296055: step 789, loss 0.0514273, acc 0.98
2016-09-07T21:21:58.972616: step 790, loss 0.0673694, acc 0.98
2016-09-07T21:21:59.648825: step 791, loss 0.134147, acc 0.96
2016-09-07T21:22:00.359018: step 792, loss 0.0827615, acc 0.98
2016-09-07T21:22:01.050353: step 793, loss 0.0937654, acc 0.98
2016-09-07T21:22:01.734468: step 794, loss 0.173153, acc 0.9
2016-09-07T21:22:02.398641: step 795, loss 0.13704, acc 0.94
2016-09-07T21:22:03.063833: step 796, loss 0.128436, acc 0.96
2016-09-07T21:22:03.727296: step 797, loss 0.147723, acc 0.9
2016-09-07T21:22:04.414922: step 798, loss 0.0804001, acc 0.98
2016-09-07T21:22:05.079749: step 799, loss 0.0982912, acc 0.94
2016-09-07T21:22:05.746836: step 800, loss 0.115895, acc 0.96

Evaluation:
2016-09-07T21:22:09.041713: step 800, loss 0.752034, acc 0.787

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-800

2016-09-07T21:22:10.801093: step 801, loss 0.0933023, acc 0.98
2016-09-07T21:22:11.472183: step 802, loss 0.0889987, acc 0.96
2016-09-07T21:22:12.144715: step 803, loss 0.0974771, acc 0.98
2016-09-07T21:22:12.796808: step 804, loss 0.075152, acc 0.98
2016-09-07T21:22:13.458658: step 805, loss 0.123755, acc 0.94
2016-09-07T21:22:14.128516: step 806, loss 0.0898652, acc 0.96
2016-09-07T21:22:14.799910: step 807, loss 0.109317, acc 0.94
2016-09-07T21:22:15.457244: step 808, loss 0.115707, acc 0.94
2016-09-07T21:22:16.144468: step 809, loss 0.0975593, acc 0.96
2016-09-07T21:22:16.820881: step 810, loss 0.125842, acc 0.98
2016-09-07T21:22:17.482368: step 811, loss 0.353258, acc 0.9
2016-09-07T21:22:18.144130: step 812, loss 0.145127, acc 0.96
2016-09-07T21:22:18.815913: step 813, loss 0.143983, acc 0.92
2016-09-07T21:22:19.471237: step 814, loss 0.113515, acc 0.94
2016-09-07T21:22:20.126151: step 815, loss 0.218945, acc 0.94
2016-09-07T21:22:20.790237: step 816, loss 0.0345454, acc 0.98
2016-09-07T21:22:21.464734: step 817, loss 0.305389, acc 0.86
2016-09-07T21:22:22.158316: step 818, loss 0.129399, acc 0.96
2016-09-07T21:22:22.833548: step 819, loss 0.114759, acc 0.96
2016-09-07T21:22:23.504098: step 820, loss 0.11754, acc 0.96
2016-09-07T21:22:24.165529: step 821, loss 0.101306, acc 0.98
2016-09-07T21:22:24.842484: step 822, loss 0.178757, acc 0.9
2016-09-07T21:22:25.502061: step 823, loss 0.163916, acc 0.94
2016-09-07T21:22:26.171586: step 824, loss 0.114222, acc 0.96
2016-09-07T21:22:26.848552: step 825, loss 0.173056, acc 0.94
2016-09-07T21:22:27.522484: step 826, loss 0.185661, acc 0.92
2016-09-07T21:22:28.185257: step 827, loss 0.1895, acc 0.92
2016-09-07T21:22:28.848522: step 828, loss 0.184454, acc 0.92
2016-09-07T21:22:29.524042: step 829, loss 0.107038, acc 0.98
2016-09-07T21:22:30.191583: step 830, loss 0.0998962, acc 0.94
2016-09-07T21:22:30.851595: step 831, loss 0.129133, acc 0.96
2016-09-07T21:22:31.528522: step 832, loss 0.133052, acc 0.98
2016-09-07T21:22:32.205543: step 833, loss 0.269854, acc 0.9
2016-09-07T21:22:32.874286: step 834, loss 0.0539586, acc 1
2016-09-07T21:22:33.581083: step 835, loss 0.0677023, acc 0.98
2016-09-07T21:22:34.271833: step 836, loss 0.0382091, acc 1
2016-09-07T21:22:34.936039: step 837, loss 0.110934, acc 0.96
2016-09-07T21:22:35.598703: step 838, loss 0.0988161, acc 0.96
2016-09-07T21:22:36.268171: step 839, loss 0.146749, acc 0.92
2016-09-07T21:22:36.933435: step 840, loss 0.234367, acc 0.88
2016-09-07T21:22:37.597289: step 841, loss 0.147883, acc 0.94
2016-09-07T21:22:38.268626: step 842, loss 0.222257, acc 0.92
2016-09-07T21:22:38.941348: step 843, loss 0.0784385, acc 0.98
2016-09-07T21:22:39.605989: step 844, loss 0.264096, acc 0.86
2016-09-07T21:22:40.287566: step 845, loss 0.11004, acc 0.96
2016-09-07T21:22:40.973883: step 846, loss 0.153095, acc 0.94
2016-09-07T21:22:41.650561: step 847, loss 0.154945, acc 0.96
2016-09-07T21:22:42.314760: step 848, loss 0.128726, acc 0.96
2016-09-07T21:22:42.983737: step 849, loss 0.110893, acc 0.94
2016-09-07T21:22:43.651339: step 850, loss 0.155614, acc 0.94
2016-09-07T21:22:44.340250: step 851, loss 0.147635, acc 0.94
2016-09-07T21:22:45.010322: step 852, loss 0.164898, acc 0.96
2016-09-07T21:22:45.678816: step 853, loss 0.205707, acc 0.94
2016-09-07T21:22:46.353609: step 854, loss 0.267412, acc 0.9
2016-09-07T21:22:47.033564: step 855, loss 0.190911, acc 0.9
2016-09-07T21:22:47.714278: step 856, loss 0.117011, acc 0.94
2016-09-07T21:22:48.373280: step 857, loss 0.0601589, acc 1
2016-09-07T21:22:49.034435: step 858, loss 0.109271, acc 0.98
2016-09-07T21:22:49.698279: step 859, loss 0.181111, acc 0.94
2016-09-07T21:22:50.360144: step 860, loss 0.200888, acc 0.9
2016-09-07T21:22:51.043706: step 861, loss 0.108103, acc 0.94
2016-09-07T21:22:51.720855: step 862, loss 0.192405, acc 0.92
2016-09-07T21:22:52.403111: step 863, loss 0.08776, acc 0.96
2016-09-07T21:22:53.087264: step 864, loss 0.0641144, acc 1
2016-09-07T21:22:53.780183: step 865, loss 0.117008, acc 0.98
2016-09-07T21:22:54.476845: step 866, loss 0.135936, acc 0.94
2016-09-07T21:22:55.156395: step 867, loss 0.154145, acc 0.92
2016-09-07T21:22:55.846860: step 868, loss 0.0961258, acc 0.98
2016-09-07T21:22:56.511457: step 869, loss 0.108661, acc 0.96
2016-09-07T21:22:57.174021: step 870, loss 0.165331, acc 0.92
2016-09-07T21:22:57.841607: step 871, loss 0.204035, acc 0.92
2016-09-07T21:22:58.516175: step 872, loss 0.0901371, acc 1
2016-09-07T21:22:59.203668: step 873, loss 0.172948, acc 0.92
2016-09-07T21:22:59.875734: step 874, loss 0.195317, acc 0.94
2016-09-07T21:23:00.611316: step 875, loss 0.0892727, acc 0.98
2016-09-07T21:23:01.260766: step 876, loss 0.176427, acc 0.92
2016-09-07T21:23:01.907731: step 877, loss 0.148672, acc 0.96
2016-09-07T21:23:02.590195: step 878, loss 0.0690628, acc 0.98
2016-09-07T21:23:03.256674: step 879, loss 0.10826, acc 0.96
2016-09-07T21:23:03.921378: step 880, loss 0.224928, acc 0.9
2016-09-07T21:23:04.573227: step 881, loss 0.217966, acc 0.88
2016-09-07T21:23:05.223802: step 882, loss 0.0586805, acc 0.98
2016-09-07T21:23:05.883334: step 883, loss 0.158928, acc 0.92
2016-09-07T21:23:06.551868: step 884, loss 0.164147, acc 0.96
2016-09-07T21:23:07.208914: step 885, loss 0.174092, acc 0.9
2016-09-07T21:23:07.884243: step 886, loss 0.0818574, acc 0.96
2016-09-07T21:23:08.557656: step 887, loss 0.225965, acc 0.88
2016-09-07T21:23:09.248406: step 888, loss 0.142183, acc 0.92
2016-09-07T21:23:09.917804: step 889, loss 0.202513, acc 0.9
2016-09-07T21:23:10.586235: step 890, loss 0.0765592, acc 0.98
2016-09-07T21:23:11.241194: step 891, loss 0.200419, acc 0.96
2016-09-07T21:23:11.913530: step 892, loss 0.136238, acc 0.94
2016-09-07T21:23:12.577230: step 893, loss 0.0390417, acc 1
2016-09-07T21:23:13.240222: step 894, loss 0.17682, acc 0.88
2016-09-07T21:23:13.920100: step 895, loss 0.182757, acc 0.96
2016-09-07T21:23:14.601812: step 896, loss 0.153132, acc 0.98
2016-09-07T21:23:15.271920: step 897, loss 0.0859324, acc 0.96
2016-09-07T21:23:15.946533: step 898, loss 0.123523, acc 0.96
2016-09-07T21:23:16.613439: step 899, loss 0.185659, acc 0.92
2016-09-07T21:23:17.290287: step 900, loss 0.126373, acc 0.96

Evaluation:
2016-09-07T21:23:20.590294: step 900, loss 0.724817, acc 0.772

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-900

2016-09-07T21:23:22.317787: step 901, loss 0.187875, acc 0.9
2016-09-07T21:23:22.974198: step 902, loss 0.237686, acc 0.92
2016-09-07T21:23:23.630344: step 903, loss 0.124268, acc 0.96
2016-09-07T21:23:24.309883: step 904, loss 0.113014, acc 0.96
2016-09-07T21:23:24.999844: step 905, loss 0.11396, acc 0.96
2016-09-07T21:23:25.674478: step 906, loss 0.188927, acc 0.94
2016-09-07T21:23:26.344313: step 907, loss 0.154264, acc 0.96
2016-09-07T21:23:27.011139: step 908, loss 0.113992, acc 0.94
2016-09-07T21:23:27.690627: step 909, loss 0.233023, acc 0.92
2016-09-07T21:23:28.379230: step 910, loss 0.154933, acc 0.9
2016-09-07T21:23:29.039296: step 911, loss 0.12351, acc 0.96
2016-09-07T21:23:29.730358: step 912, loss 0.0941067, acc 0.98
2016-09-07T21:23:30.394595: step 913, loss 0.169607, acc 0.94
2016-09-07T21:23:31.074664: step 914, loss 0.0980578, acc 0.98
2016-09-07T21:23:31.751639: step 915, loss 0.22398, acc 0.9
2016-09-07T21:23:32.413493: step 916, loss 0.121607, acc 0.94
2016-09-07T21:23:33.083237: step 917, loss 0.0412554, acc 1
2016-09-07T21:23:33.753115: step 918, loss 0.20992, acc 0.94
2016-09-07T21:23:34.412172: step 919, loss 0.133918, acc 0.94
2016-09-07T21:23:35.079835: step 920, loss 0.182151, acc 0.92
2016-09-07T21:23:35.751315: step 921, loss 0.091859, acc 0.96
2016-09-07T21:23:36.431351: step 922, loss 0.227591, acc 0.92
2016-09-07T21:23:37.081336: step 923, loss 0.103791, acc 0.96
2016-09-07T21:23:37.742713: step 924, loss 0.104103, acc 0.96
2016-09-07T21:23:38.415044: step 925, loss 0.149047, acc 0.96
2016-09-07T21:23:39.067807: step 926, loss 0.215913, acc 0.9
2016-09-07T21:23:39.740234: step 927, loss 0.167257, acc 0.92
2016-09-07T21:23:40.394856: step 928, loss 0.160078, acc 0.94
2016-09-07T21:23:41.063586: step 929, loss 0.119346, acc 0.94
2016-09-07T21:23:41.763018: step 930, loss 0.0849702, acc 0.96
2016-09-07T21:23:42.433526: step 931, loss 0.177913, acc 0.92
2016-09-07T21:23:43.104156: step 932, loss 0.112913, acc 0.96
2016-09-07T21:23:43.764013: step 933, loss 0.0720874, acc 0.98
2016-09-07T21:23:44.424642: step 934, loss 0.0739797, acc 0.98
2016-09-07T21:23:45.100146: step 935, loss 0.201392, acc 0.92
2016-09-07T21:23:45.778083: step 936, loss 0.10241, acc 0.98
2016-09-07T21:23:46.439665: step 937, loss 0.116673, acc 0.96
2016-09-07T21:23:47.137293: step 938, loss 0.227792, acc 0.9
2016-09-07T21:23:47.829083: step 939, loss 0.123666, acc 0.96
2016-09-07T21:23:48.504236: step 940, loss 0.104635, acc 0.98
2016-09-07T21:23:49.175219: step 941, loss 0.0934066, acc 0.98
2016-09-07T21:23:49.858450: step 942, loss 0.139343, acc 0.9
2016-09-07T21:23:50.529367: step 943, loss 0.289515, acc 0.84
2016-09-07T21:23:51.204249: step 944, loss 0.158218, acc 0.94
2016-09-07T21:23:51.875755: step 945, loss 0.264188, acc 0.86
2016-09-07T21:23:52.539475: step 946, loss 0.207064, acc 0.88
2016-09-07T21:23:53.214238: step 947, loss 0.0892779, acc 0.96
2016-09-07T21:23:53.872356: step 948, loss 0.270083, acc 0.86
2016-09-07T21:23:54.547568: step 949, loss 0.138941, acc 0.96
2016-09-07T21:23:55.206776: step 950, loss 0.15215, acc 0.96
2016-09-07T21:23:55.873597: step 951, loss 0.0978122, acc 0.98
2016-09-07T21:23:56.562925: step 952, loss 0.12245, acc 0.94
2016-09-07T21:23:57.243561: step 953, loss 0.0714521, acc 0.98
2016-09-07T21:23:57.929793: step 954, loss 0.166998, acc 0.92
2016-09-07T21:23:58.607229: step 955, loss 0.198563, acc 0.9
2016-09-07T21:23:59.259331: step 956, loss 0.0965956, acc 0.96
2016-09-07T21:23:59.934688: step 957, loss 0.220127, acc 0.92
2016-09-07T21:24:00.651814: step 958, loss 0.201306, acc 0.96
2016-09-07T21:24:01.329674: step 959, loss 0.116827, acc 0.94
2016-09-07T21:24:01.994946: step 960, loss 0.088724, acc 0.98
2016-09-07T21:24:02.659567: step 961, loss 0.166787, acc 0.92
2016-09-07T21:24:03.344096: step 962, loss 0.123811, acc 0.94
2016-09-07T21:24:04.019325: step 963, loss 0.161224, acc 0.96
2016-09-07T21:24:04.694245: step 964, loss 0.0840367, acc 0.96
2016-09-07T21:24:05.399208: step 965, loss 0.111999, acc 0.94
2016-09-07T21:24:06.061365: step 966, loss 0.149851, acc 0.94
2016-09-07T21:24:06.762750: step 967, loss 0.223524, acc 0.9
2016-09-07T21:24:07.438142: step 968, loss 0.0492623, acc 1
2016-09-07T21:24:08.097933: step 969, loss 0.18525, acc 0.92
2016-09-07T21:24:08.462319: step 970, loss 0.124889, acc 1
2016-09-07T21:24:09.143181: step 971, loss 0.0665591, acc 0.98
2016-09-07T21:24:09.809016: step 972, loss 0.0826287, acc 0.96
2016-09-07T21:24:10.478912: step 973, loss 0.140494, acc 0.96
2016-09-07T21:24:11.152444: step 974, loss 0.110811, acc 0.98
2016-09-07T21:24:11.824701: step 975, loss 0.0889519, acc 0.96
2016-09-07T21:24:12.504086: step 976, loss 0.162128, acc 0.94
2016-09-07T21:24:13.174562: step 977, loss 0.115699, acc 0.96
2016-09-07T21:24:13.836281: step 978, loss 0.137393, acc 0.98
2016-09-07T21:24:14.503279: step 979, loss 0.0797626, acc 0.96
2016-09-07T21:24:15.179667: step 980, loss 0.148727, acc 0.9
2016-09-07T21:24:15.877516: step 981, loss 0.163873, acc 0.94
2016-09-07T21:24:16.547584: step 982, loss 0.155097, acc 0.96
2016-09-07T21:24:17.224656: step 983, loss 0.191182, acc 0.9
2016-09-07T21:24:17.900669: step 984, loss 0.127817, acc 0.94
2016-09-07T21:24:18.573875: step 985, loss 0.0682421, acc 0.98
2016-09-07T21:24:19.245347: step 986, loss 0.113294, acc 0.94
2016-09-07T21:24:19.934283: step 987, loss 0.0948197, acc 0.98
2016-09-07T21:24:20.624118: step 988, loss 0.040761, acc 1
2016-09-07T21:24:21.291585: step 989, loss 0.140164, acc 0.92
2016-09-07T21:24:21.967956: step 990, loss 0.0396712, acc 1
2016-09-07T21:24:22.620871: step 991, loss 0.124239, acc 0.94
2016-09-07T21:24:23.294637: step 992, loss 0.148126, acc 0.9
2016-09-07T21:24:23.978691: step 993, loss 0.0683325, acc 0.98
2016-09-07T21:24:24.668934: step 994, loss 0.104832, acc 0.98
2016-09-07T21:24:25.349404: step 995, loss 0.078654, acc 0.96
2016-09-07T21:24:26.015786: step 996, loss 0.0990149, acc 0.96
2016-09-07T21:24:26.682310: step 997, loss 0.0483917, acc 0.98
2016-09-07T21:24:27.378234: step 998, loss 0.106182, acc 0.96
2016-09-07T21:24:28.047531: step 999, loss 0.117522, acc 0.96
2016-09-07T21:24:28.737338: step 1000, loss 0.0975428, acc 0.96

Evaluation:
2016-09-07T21:24:32.048985: step 1000, loss 0.817667, acc 0.776

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-1000

2016-09-07T21:24:33.726449: step 1001, loss 0.0883326, acc 0.96
2016-09-07T21:24:34.405760: step 1002, loss 0.10255, acc 0.98
2016-09-07T21:24:35.083747: step 1003, loss 0.138849, acc 0.92
2016-09-07T21:24:35.744657: step 1004, loss 0.141606, acc 0.94
2016-09-07T21:24:36.413807: step 1005, loss 0.14681, acc 0.96
2016-09-07T21:24:37.090879: step 1006, loss 0.0774448, acc 0.98
2016-09-07T21:24:37.752509: step 1007, loss 0.100298, acc 0.96
2016-09-07T21:24:38.428301: step 1008, loss 0.0526769, acc 0.98
2016-09-07T21:24:39.106682: step 1009, loss 0.0505108, acc 0.98
2016-09-07T21:24:39.799768: step 1010, loss 0.0477912, acc 0.98
2016-09-07T21:24:40.485640: step 1011, loss 0.0827853, acc 0.96
2016-09-07T21:24:41.162907: step 1012, loss 0.0384021, acc 0.98
2016-09-07T21:24:41.833647: step 1013, loss 0.0300022, acc 1
2016-09-07T21:24:42.516200: step 1014, loss 0.0710378, acc 0.98
2016-09-07T21:24:43.197576: step 1015, loss 0.0467243, acc 0.98
2016-09-07T21:24:43.856562: step 1016, loss 0.0848346, acc 0.96
2016-09-07T21:24:44.545015: step 1017, loss 0.132476, acc 0.96
2016-09-07T21:24:45.220400: step 1018, loss 0.124591, acc 0.92
2016-09-07T21:24:45.903002: step 1019, loss 0.0614271, acc 0.98
2016-09-07T21:24:46.571326: step 1020, loss 0.0985619, acc 0.94
2016-09-07T21:24:47.265925: step 1021, loss 0.154157, acc 0.94
2016-09-07T21:24:47.931288: step 1022, loss 0.0951318, acc 0.96
2016-09-07T21:24:48.597499: step 1023, loss 0.0575077, acc 0.98
2016-09-07T21:24:49.273385: step 1024, loss 0.140041, acc 0.94
2016-09-07T21:24:49.941923: step 1025, loss 0.218098, acc 0.9
2016-09-07T21:24:50.616424: step 1026, loss 0.139803, acc 0.92
2016-09-07T21:24:51.304641: step 1027, loss 0.0965727, acc 0.94
2016-09-07T21:24:52.009543: step 1028, loss 0.0847984, acc 0.96
2016-09-07T21:24:52.668110: step 1029, loss 0.0661496, acc 0.98
2016-09-07T21:24:53.337036: step 1030, loss 0.055997, acc 1
2016-09-07T21:24:54.025749: step 1031, loss 0.145273, acc 0.92
2016-09-07T21:24:54.697228: step 1032, loss 0.0657741, acc 0.98
2016-09-07T21:24:55.366539: step 1033, loss 0.122154, acc 0.94
2016-09-07T21:24:56.049951: step 1034, loss 0.13555, acc 0.94
2016-09-07T21:24:56.738572: step 1035, loss 0.0953816, acc 0.96
2016-09-07T21:24:57.403821: step 1036, loss 0.0962931, acc 0.94
2016-09-07T21:24:58.076543: step 1037, loss 0.0216454, acc 1
2016-09-07T21:24:58.755135: step 1038, loss 0.198019, acc 0.92
2016-09-07T21:24:59.448028: step 1039, loss 0.101254, acc 0.98
2016-09-07T21:25:00.119681: step 1040, loss 0.0907103, acc 0.96
2016-09-07T21:25:00.823106: step 1041, loss 0.0297217, acc 1
2016-09-07T21:25:01.514259: step 1042, loss 0.0461514, acc 0.98
2016-09-07T21:25:02.196828: step 1043, loss 0.24831, acc 0.9
2016-09-07T21:25:02.888680: step 1044, loss 0.0168606, acc 1
2016-09-07T21:25:03.569950: step 1045, loss 0.0803508, acc 0.96
2016-09-07T21:25:04.256693: step 1046, loss 0.136575, acc 0.96
2016-09-07T21:25:04.949133: step 1047, loss 0.144276, acc 0.94
2016-09-07T21:25:05.614962: step 1048, loss 0.085554, acc 0.98
2016-09-07T21:25:06.294709: step 1049, loss 0.0801684, acc 0.96
2016-09-07T21:25:06.949946: step 1050, loss 0.120871, acc 0.92
2016-09-07T21:25:07.615537: step 1051, loss 0.0656259, acc 0.98
2016-09-07T21:25:08.286568: step 1052, loss 0.133472, acc 0.98
2016-09-07T21:25:08.954722: step 1053, loss 0.0390454, acc 1
2016-09-07T21:25:09.630693: step 1054, loss 0.139817, acc 0.92
2016-09-07T21:25:10.319194: step 1055, loss 0.0640889, acc 0.98
2016-09-07T21:25:10.999591: step 1056, loss 0.197663, acc 0.9
2016-09-07T21:25:11.669095: step 1057, loss 0.0736757, acc 0.98
2016-09-07T21:25:12.338247: step 1058, loss 0.101136, acc 0.98
2016-09-07T21:25:13.015588: step 1059, loss 0.2685, acc 0.9
2016-09-07T21:25:13.687728: step 1060, loss 0.0872515, acc 1
2016-09-07T21:25:14.357917: step 1061, loss 0.040403, acc 1
2016-09-07T21:25:15.002793: step 1062, loss 0.0641135, acc 0.98
2016-09-07T21:25:15.675190: step 1063, loss 0.20109, acc 0.9
2016-09-07T21:25:16.353566: step 1064, loss 0.0917061, acc 0.98
2016-09-07T21:25:17.020284: step 1065, loss 0.151713, acc 0.96
2016-09-07T21:25:17.697341: step 1066, loss 0.215383, acc 0.88
2016-09-07T21:25:18.375952: step 1067, loss 0.158172, acc 0.94
2016-09-07T21:25:19.048030: step 1068, loss 0.0396895, acc 1
2016-09-07T21:25:19.729271: step 1069, loss 0.131304, acc 0.92
2016-09-07T21:25:20.396187: step 1070, loss 0.059602, acc 0.98
2016-09-07T21:25:21.081437: step 1071, loss 0.0530394, acc 0.98
2016-09-07T21:25:21.751212: step 1072, loss 0.0680868, acc 0.98
2016-09-07T21:25:22.412706: step 1073, loss 0.206021, acc 0.94
2016-09-07T21:25:23.067256: step 1074, loss 0.0927121, acc 0.94
2016-09-07T21:25:23.740958: step 1075, loss 0.118683, acc 0.96
2016-09-07T21:25:24.414989: step 1076, loss 0.0966211, acc 0.94
2016-09-07T21:25:25.106305: step 1077, loss 0.184954, acc 0.92
2016-09-07T21:25:25.771928: step 1078, loss 0.177138, acc 0.92
2016-09-07T21:25:26.428369: step 1079, loss 0.131043, acc 0.9
2016-09-07T21:25:27.108181: step 1080, loss 0.100708, acc 0.98
2016-09-07T21:25:27.784679: step 1081, loss 0.0768375, acc 0.96
2016-09-07T21:25:28.465995: step 1082, loss 0.0473427, acc 1
2016-09-07T21:25:29.147728: step 1083, loss 0.154019, acc 0.92
2016-09-07T21:25:29.819688: step 1084, loss 0.0859533, acc 0.98
2016-09-07T21:25:30.496326: step 1085, loss 0.075959, acc 0.96
2016-09-07T21:25:31.187522: step 1086, loss 0.131879, acc 0.94
2016-09-07T21:25:31.859879: step 1087, loss 0.0987818, acc 0.96
2016-09-07T21:25:32.535207: step 1088, loss 0.0883142, acc 0.96
2016-09-07T21:25:33.187886: step 1089, loss 0.117829, acc 0.96
2016-09-07T21:25:33.838647: step 1090, loss 0.11438, acc 0.94
2016-09-07T21:25:34.498113: step 1091, loss 0.135667, acc 0.9
2016-09-07T21:25:35.172218: step 1092, loss 0.0825995, acc 0.96
2016-09-07T21:25:35.866975: step 1093, loss 0.142818, acc 0.92
2016-09-07T21:25:36.519653: step 1094, loss 0.185906, acc 0.92
2016-09-07T21:25:37.213545: step 1095, loss 0.0642713, acc 0.96
2016-09-07T21:25:37.893484: step 1096, loss 0.108181, acc 0.96
2016-09-07T21:25:38.567994: step 1097, loss 0.23824, acc 0.9
2016-09-07T21:25:39.243887: step 1098, loss 0.0826926, acc 0.96
2016-09-07T21:25:39.904755: step 1099, loss 0.13802, acc 0.94
2016-09-07T21:25:40.570188: step 1100, loss 0.0806043, acc 1

Evaluation:
2016-09-07T21:25:43.852379: step 1100, loss 0.796749, acc 0.75

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-1100

2016-09-07T21:25:45.627471: step 1101, loss 0.0806441, acc 1
2016-09-07T21:25:46.285156: step 1102, loss 0.152272, acc 0.96
2016-09-07T21:25:46.959578: step 1103, loss 0.0969816, acc 0.94
2016-09-07T21:25:47.640939: step 1104, loss 0.0916595, acc 0.98
2016-09-07T21:25:48.311784: step 1105, loss 0.184932, acc 0.94
2016-09-07T21:25:49.027117: step 1106, loss 0.13795, acc 0.94
2016-09-07T21:25:49.705690: step 1107, loss 0.123901, acc 0.96
2016-09-07T21:25:50.373257: step 1108, loss 0.0676826, acc 0.96
2016-09-07T21:25:51.063709: step 1109, loss 0.10061, acc 0.96
2016-09-07T21:25:51.735959: step 1110, loss 0.203018, acc 0.9
2016-09-07T21:25:52.394885: step 1111, loss 0.138222, acc 0.96
2016-09-07T21:25:53.072539: step 1112, loss 0.131631, acc 0.92
2016-09-07T21:25:53.742837: step 1113, loss 0.0528902, acc 0.98
2016-09-07T21:25:54.425513: step 1114, loss 0.0763399, acc 0.98
2016-09-07T21:25:55.115628: step 1115, loss 0.0509659, acc 0.98
2016-09-07T21:25:55.795061: step 1116, loss 0.168177, acc 0.9
2016-09-07T21:25:56.473546: step 1117, loss 0.0870423, acc 0.94
2016-09-07T21:25:57.162277: step 1118, loss 0.0729688, acc 0.96
2016-09-07T21:25:57.821454: step 1119, loss 0.141226, acc 0.96
2016-09-07T21:25:58.511193: step 1120, loss 0.208001, acc 0.94
2016-09-07T21:25:59.197556: step 1121, loss 0.168978, acc 0.94
2016-09-07T21:25:59.863008: step 1122, loss 0.139777, acc 0.9
2016-09-07T21:26:00.570277: step 1123, loss 0.102964, acc 0.96
2016-09-07T21:26:01.214977: step 1124, loss 0.0956494, acc 0.94
2016-09-07T21:26:01.886475: step 1125, loss 0.110697, acc 0.94
2016-09-07T21:26:02.545082: step 1126, loss 0.150795, acc 0.98
2016-09-07T21:26:03.200988: step 1127, loss 0.0472476, acc 1
2016-09-07T21:26:03.890451: step 1128, loss 0.137709, acc 0.92
2016-09-07T21:26:04.565677: step 1129, loss 0.0941654, acc 0.94
2016-09-07T21:26:05.233475: step 1130, loss 0.0642221, acc 0.98
2016-09-07T21:26:05.896219: step 1131, loss 0.0888527, acc 0.98
2016-09-07T21:26:06.566548: step 1132, loss 0.13152, acc 0.94
2016-09-07T21:26:07.234046: step 1133, loss 0.0787182, acc 0.96
2016-09-07T21:26:07.889800: step 1134, loss 0.0462439, acc 0.98
2016-09-07T21:26:08.543466: step 1135, loss 0.0745429, acc 0.98
2016-09-07T21:26:09.211253: step 1136, loss 0.0540745, acc 1
2016-09-07T21:26:09.874103: step 1137, loss 0.106397, acc 0.94
2016-09-07T21:26:10.531690: step 1138, loss 0.0815786, acc 0.98
2016-09-07T21:26:11.205488: step 1139, loss 0.0820057, acc 0.98
2016-09-07T21:26:11.903017: step 1140, loss 0.17713, acc 0.94
2016-09-07T21:26:12.583621: step 1141, loss 0.048116, acc 1
2016-09-07T21:26:13.258839: step 1142, loss 0.0483478, acc 0.98
2016-09-07T21:26:13.919356: step 1143, loss 0.136789, acc 0.94
2016-09-07T21:26:14.588306: step 1144, loss 0.0513479, acc 1
2016-09-07T21:26:15.269746: step 1145, loss 0.18947, acc 0.94
2016-09-07T21:26:15.939708: step 1146, loss 0.184544, acc 0.92
2016-09-07T21:26:16.597754: step 1147, loss 0.148538, acc 0.94
2016-09-07T21:26:17.279321: step 1148, loss 0.072105, acc 0.98
2016-09-07T21:26:17.936031: step 1149, loss 0.100668, acc 0.94
2016-09-07T21:26:18.609486: step 1150, loss 0.197883, acc 0.92
2016-09-07T21:26:19.262437: step 1151, loss 0.133568, acc 0.94
2016-09-07T21:26:19.923208: step 1152, loss 0.0696136, acc 0.96
2016-09-07T21:26:20.589054: step 1153, loss 0.0933989, acc 0.96
2016-09-07T21:26:21.268006: step 1154, loss 0.0452462, acc 1
2016-09-07T21:26:21.978937: step 1155, loss 0.0600799, acc 0.98
2016-09-07T21:26:22.647115: step 1156, loss 0.0886837, acc 0.96
2016-09-07T21:26:23.321342: step 1157, loss 0.206018, acc 0.88
2016-09-07T21:26:23.987993: step 1158, loss 0.188884, acc 0.94
2016-09-07T21:26:24.647272: step 1159, loss 0.0950891, acc 0.96
2016-09-07T21:26:25.319362: step 1160, loss 0.0955973, acc 1
2016-09-07T21:26:25.985879: step 1161, loss 0.106188, acc 0.96
2016-09-07T21:26:26.659016: step 1162, loss 0.0946974, acc 0.96
2016-09-07T21:26:27.333041: step 1163, loss 0.106438, acc 0.96
2016-09-07T21:26:27.688372: step 1164, loss 0.156914, acc 1
2016-09-07T21:26:28.373079: step 1165, loss 0.165074, acc 0.88
2016-09-07T21:26:29.037343: step 1166, loss 0.0847879, acc 0.96
2016-09-07T21:26:29.709659: step 1167, loss 0.0793451, acc 1
2016-09-07T21:26:30.378578: step 1168, loss 0.0627037, acc 0.98
2016-09-07T21:26:31.053487: step 1169, loss 0.134671, acc 0.96
2016-09-07T21:26:31.722162: step 1170, loss 0.21595, acc 0.94
2016-09-07T21:26:32.387496: step 1171, loss 0.0874951, acc 0.96
2016-09-07T21:26:33.054646: step 1172, loss 0.0311971, acc 1
2016-09-07T21:26:33.716609: step 1173, loss 0.0419826, acc 1
2016-09-07T21:26:34.410279: step 1174, loss 0.0555126, acc 0.98
2016-09-07T21:26:35.083876: step 1175, loss 0.0581437, acc 0.98
2016-09-07T21:26:35.778527: step 1176, loss 0.14646, acc 0.94
2016-09-07T21:26:36.462419: step 1177, loss 0.0402663, acc 1
2016-09-07T21:26:37.137717: step 1178, loss 0.0168029, acc 1
2016-09-07T21:26:37.796379: step 1179, loss 0.034823, acc 1
2016-09-07T21:26:38.481006: step 1180, loss 0.0859622, acc 0.96
2016-09-07T21:26:39.136393: step 1181, loss 0.031719, acc 1
2016-09-07T21:26:39.809019: step 1182, loss 0.0503462, acc 0.98
2016-09-07T21:26:40.485027: step 1183, loss 0.0774107, acc 0.96
2016-09-07T21:26:41.174411: step 1184, loss 0.0649053, acc 0.98
2016-09-07T21:26:41.843687: step 1185, loss 0.0518511, acc 0.98
2016-09-07T21:26:42.519286: step 1186, loss 0.0978208, acc 0.96
2016-09-07T21:26:43.174627: step 1187, loss 0.0505833, acc 0.98
2016-09-07T21:26:43.827082: step 1188, loss 0.155392, acc 0.96
2016-09-07T21:26:44.514937: step 1189, loss 0.0694395, acc 0.94
2016-09-07T21:26:45.162647: step 1190, loss 0.0358074, acc 1
2016-09-07T21:26:45.824782: step 1191, loss 0.0584683, acc 0.98
2016-09-07T21:26:46.511133: step 1192, loss 0.0431048, acc 0.98
2016-09-07T21:26:47.181423: step 1193, loss 0.0550579, acc 0.98
2016-09-07T21:26:47.868155: step 1194, loss 0.0620983, acc 0.96
2016-09-07T21:26:48.557521: step 1195, loss 0.0525611, acc 0.98
2016-09-07T21:26:49.258054: step 1196, loss 0.102376, acc 0.96
2016-09-07T21:26:49.939344: step 1197, loss 0.0225261, acc 1
2016-09-07T21:26:50.617542: step 1198, loss 0.098873, acc 0.94
2016-09-07T21:26:51.314350: step 1199, loss 0.0991847, acc 0.96
2016-09-07T21:26:51.994971: step 1200, loss 0.0984963, acc 0.98

Evaluation:
2016-09-07T21:26:55.251054: step 1200, loss 1.09117, acc 0.753

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-1200

2016-09-07T21:26:56.956427: step 1201, loss 0.0544751, acc 0.98
2016-09-07T21:26:57.636521: step 1202, loss 0.153386, acc 0.94
2016-09-07T21:26:58.328143: step 1203, loss 0.03807, acc 1
2016-09-07T21:26:59.016197: step 1204, loss 0.0476988, acc 0.98
2016-09-07T21:26:59.689137: step 1205, loss 0.0974593, acc 0.96
2016-09-07T21:27:00.445242: step 1206, loss 0.0835313, acc 0.98
2016-09-07T21:27:01.122927: step 1207, loss 0.107197, acc 0.96
2016-09-07T21:27:01.788117: step 1208, loss 0.0647541, acc 0.98
2016-09-07T21:27:02.466241: step 1209, loss 0.0574503, acc 0.96
2016-09-07T21:27:03.144720: step 1210, loss 0.2094, acc 0.94
2016-09-07T21:27:03.808739: step 1211, loss 0.0750871, acc 0.94
2016-09-07T21:27:04.476536: step 1212, loss 0.0864129, acc 1
2016-09-07T21:27:05.149605: step 1213, loss 0.0496671, acc 1
2016-09-07T21:27:05.832733: step 1214, loss 0.0893325, acc 0.94
2016-09-07T21:27:06.521154: step 1215, loss 0.187535, acc 0.96
2016-09-07T21:27:07.187447: step 1216, loss 0.0183138, acc 1
2016-09-07T21:27:07.885681: step 1217, loss 0.0599095, acc 0.96
2016-09-07T21:27:08.561338: step 1218, loss 0.0660098, acc 0.98
2016-09-07T21:27:09.231864: step 1219, loss 0.121112, acc 0.96
2016-09-07T21:27:09.907872: step 1220, loss 0.14616, acc 0.94
2016-09-07T21:27:10.587842: step 1221, loss 0.142085, acc 0.94
2016-09-07T21:27:11.268119: step 1222, loss 0.0443121, acc 1
2016-09-07T21:27:11.939288: step 1223, loss 0.116238, acc 0.92
2016-09-07T21:27:12.609318: step 1224, loss 0.0646196, acc 0.96
2016-09-07T21:27:13.284080: step 1225, loss 0.0776948, acc 0.94
2016-09-07T21:27:13.969389: step 1226, loss 0.0923677, acc 0.96
2016-09-07T21:27:14.639636: step 1227, loss 0.0642827, acc 0.98
2016-09-07T21:27:15.319971: step 1228, loss 0.152285, acc 0.9
2016-09-07T21:27:15.998211: step 1229, loss 0.192609, acc 0.92
2016-09-07T21:27:16.684453: step 1230, loss 0.0899972, acc 0.96
2016-09-07T21:27:17.361698: step 1231, loss 0.0550305, acc 0.98
2016-09-07T21:27:18.041877: step 1232, loss 0.0397319, acc 1
2016-09-07T21:27:18.723709: step 1233, loss 0.027079, acc 1
2016-09-07T21:27:19.387100: step 1234, loss 0.0646229, acc 0.96
2016-09-07T21:27:20.057229: step 1235, loss 0.142843, acc 0.9
2016-09-07T21:27:20.749050: step 1236, loss 0.0873358, acc 0.94
2016-09-07T21:27:21.392973: step 1237, loss 0.05007, acc 0.98
2016-09-07T21:27:22.066631: step 1238, loss 0.0429193, acc 0.98
2016-09-07T21:27:22.733672: step 1239, loss 0.0571532, acc 0.98
2016-09-07T21:27:23.400913: step 1240, loss 0.272859, acc 0.88
2016-09-07T21:27:24.067774: step 1241, loss 0.0852943, acc 0.96
2016-09-07T21:27:24.738815: step 1242, loss 0.0614719, acc 0.98
2016-09-07T21:27:25.414292: step 1243, loss 0.074779, acc 0.98
2016-09-07T21:27:26.072469: step 1244, loss 0.179609, acc 0.94
2016-09-07T21:27:26.736244: step 1245, loss 0.134579, acc 0.92
2016-09-07T21:27:27.418046: step 1246, loss 0.218333, acc 0.96
2016-09-07T21:27:28.084219: step 1247, loss 0.106815, acc 0.96
2016-09-07T21:27:28.764570: step 1248, loss 0.0766779, acc 0.98
2016-09-07T21:27:29.444118: step 1249, loss 0.137468, acc 0.94
2016-09-07T21:27:30.111964: step 1250, loss 0.0692883, acc 1
2016-09-07T21:27:30.795526: step 1251, loss 0.0624969, acc 0.98
2016-09-07T21:27:31.471475: step 1252, loss 0.0848932, acc 0.96
2016-09-07T21:27:32.139238: step 1253, loss 0.180071, acc 0.9
2016-09-07T21:27:32.802806: step 1254, loss 0.0838512, acc 0.96
2016-09-07T21:27:33.468814: step 1255, loss 0.118289, acc 0.98
2016-09-07T21:27:34.157769: step 1256, loss 0.0832633, acc 0.98
2016-09-07T21:27:34.822710: step 1257, loss 0.0420973, acc 1
2016-09-07T21:27:35.484886: step 1258, loss 0.169037, acc 0.94
2016-09-07T21:27:36.169021: step 1259, loss 0.0733, acc 0.98
2016-09-07T21:27:36.820359: step 1260, loss 0.125545, acc 0.96
2016-09-07T21:27:37.487432: step 1261, loss 0.0799257, acc 0.96
2016-09-07T21:27:38.198062: step 1262, loss 0.072551, acc 1
2016-09-07T21:27:38.881025: step 1263, loss 0.0637374, acc 1
2016-09-07T21:27:39.551255: step 1264, loss 0.088024, acc 0.98
2016-09-07T21:27:40.221437: step 1265, loss 0.0624923, acc 0.98
2016-09-07T21:27:40.891983: step 1266, loss 0.108378, acc 0.94
2016-09-07T21:27:41.561263: step 1267, loss 0.0843257, acc 0.92
2016-09-07T21:27:42.210621: step 1268, loss 0.155925, acc 0.92
2016-09-07T21:27:42.884673: step 1269, loss 0.0771423, acc 0.96
2016-09-07T21:27:43.555876: step 1270, loss 0.053519, acc 0.98
2016-09-07T21:27:44.233935: step 1271, loss 0.0439646, acc 0.98
2016-09-07T21:27:44.910651: step 1272, loss 0.0532211, acc 0.98
2016-09-07T21:27:45.589347: step 1273, loss 0.0935602, acc 0.96
2016-09-07T21:27:46.256007: step 1274, loss 0.0702298, acc 0.96
2016-09-07T21:27:46.926807: step 1275, loss 0.112308, acc 0.94
2016-09-07T21:27:47.593516: step 1276, loss 0.0153493, acc 1
2016-09-07T21:27:48.274675: step 1277, loss 0.158308, acc 0.96
2016-09-07T21:27:48.948272: step 1278, loss 0.116472, acc 0.96
2016-09-07T21:27:49.627239: step 1279, loss 0.104248, acc 0.92
2016-09-07T21:27:50.314143: step 1280, loss 0.118104, acc 0.96
2016-09-07T21:27:50.999037: step 1281, loss 0.110151, acc 0.96
2016-09-07T21:27:51.705443: step 1282, loss 0.0911504, acc 0.96
2016-09-07T21:27:52.381786: step 1283, loss 0.0904524, acc 0.96
2016-09-07T21:27:53.042810: step 1284, loss 0.0777044, acc 0.96
2016-09-07T21:27:53.714490: step 1285, loss 0.087521, acc 0.96
2016-09-07T21:27:54.370627: step 1286, loss 0.114723, acc 0.96
2016-09-07T21:27:55.034343: step 1287, loss 0.127491, acc 0.94
2016-09-07T21:27:55.693828: step 1288, loss 0.135205, acc 0.96
2016-09-07T21:27:56.364468: step 1289, loss 0.205179, acc 0.92
2016-09-07T21:27:57.025819: step 1290, loss 0.0677, acc 0.98
2016-09-07T21:27:57.678927: step 1291, loss 0.151823, acc 0.9
2016-09-07T21:27:58.335450: step 1292, loss 0.0980139, acc 0.96
2016-09-07T21:27:59.015879: step 1293, loss 0.114513, acc 0.96
2016-09-07T21:27:59.680822: step 1294, loss 0.0995538, acc 0.94
2016-09-07T21:28:00.398399: step 1295, loss 0.186978, acc 0.92
2016-09-07T21:28:01.058344: step 1296, loss 0.111028, acc 0.96
2016-09-07T21:28:01.735139: step 1297, loss 0.040231, acc 0.98
2016-09-07T21:28:02.404292: step 1298, loss 0.140252, acc 0.96
2016-09-07T21:28:03.097925: step 1299, loss 0.100819, acc 0.96
2016-09-07T21:28:03.770861: step 1300, loss 0.182277, acc 0.94

Evaluation:
2016-09-07T21:28:07.024195: step 1300, loss 0.944605, acc 0.751

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-1300

2016-09-07T21:28:08.695374: step 1301, loss 0.133023, acc 0.94
2016-09-07T21:28:09.385749: step 1302, loss 0.0359978, acc 1
2016-09-07T21:28:10.053403: step 1303, loss 0.147999, acc 0.94
2016-09-07T21:28:10.744467: step 1304, loss 0.0740205, acc 0.98
2016-09-07T21:28:11.468114: step 1305, loss 0.0690388, acc 0.98
2016-09-07T21:28:12.154762: step 1306, loss 0.0809744, acc 0.96
2016-09-07T21:28:12.824078: step 1307, loss 0.145046, acc 0.92
2016-09-07T21:28:13.512807: step 1308, loss 0.151008, acc 0.96
2016-09-07T21:28:14.197937: step 1309, loss 0.154182, acc 0.94
2016-09-07T21:28:14.865260: step 1310, loss 0.105991, acc 0.96
2016-09-07T21:28:15.528160: step 1311, loss 0.11035, acc 0.96
2016-09-07T21:28:16.185947: step 1312, loss 0.0574475, acc 0.98
2016-09-07T21:28:16.865908: step 1313, loss 0.154465, acc 0.94
2016-09-07T21:28:17.523850: step 1314, loss 0.0622773, acc 0.96
2016-09-07T21:28:18.198699: step 1315, loss 0.238585, acc 0.92
2016-09-07T21:28:18.880789: step 1316, loss 0.0607803, acc 1
2016-09-07T21:28:19.555178: step 1317, loss 0.127541, acc 0.96
2016-09-07T21:28:20.228640: step 1318, loss 0.155306, acc 0.94
2016-09-07T21:28:20.890824: step 1319, loss 0.0346718, acc 1
2016-09-07T21:28:21.554186: step 1320, loss 0.0575399, acc 0.98
2016-09-07T21:28:22.234359: step 1321, loss 0.0570945, acc 0.98
2016-09-07T21:28:22.899546: step 1322, loss 0.156458, acc 0.9
2016-09-07T21:28:23.565629: step 1323, loss 0.0839401, acc 0.96
2016-09-07T21:28:24.231524: step 1324, loss 0.091191, acc 0.96
2016-09-07T21:28:24.905727: step 1325, loss 0.133746, acc 0.96
2016-09-07T21:28:25.576843: step 1326, loss 0.0629623, acc 0.98
2016-09-07T21:28:26.235390: step 1327, loss 0.0459077, acc 0.98
2016-09-07T21:28:26.899147: step 1328, loss 0.0965855, acc 0.98
2016-09-07T21:28:27.563929: step 1329, loss 0.0225259, acc 1
2016-09-07T21:28:28.230668: step 1330, loss 0.103685, acc 0.96
2016-09-07T21:28:28.890416: step 1331, loss 0.0761019, acc 0.96
2016-09-07T21:28:29.561981: step 1332, loss 0.102352, acc 0.96
2016-09-07T21:28:30.233076: step 1333, loss 0.0817452, acc 0.96
2016-09-07T21:28:30.906662: step 1334, loss 0.105263, acc 0.96
2016-09-07T21:28:31.572884: step 1335, loss 0.0884378, acc 0.94
2016-09-07T21:28:32.238004: step 1336, loss 0.141417, acc 0.96
2016-09-07T21:28:32.922581: step 1337, loss 0.10515, acc 0.96
2016-09-07T21:28:33.588251: step 1338, loss 0.0558178, acc 0.96
2016-09-07T21:28:34.264615: step 1339, loss 0.12851, acc 0.92
2016-09-07T21:28:34.936420: step 1340, loss 0.0587681, acc 0.98
2016-09-07T21:28:35.599453: step 1341, loss 0.0248026, acc 1
2016-09-07T21:28:36.253865: step 1342, loss 0.0863001, acc 0.98
2016-09-07T21:28:36.922924: step 1343, loss 0.0617462, acc 0.98
2016-09-07T21:28:37.611462: step 1344, loss 0.045724, acc 0.98
2016-09-07T21:28:38.272801: step 1345, loss 0.0510091, acc 0.98
2016-09-07T21:28:38.939166: step 1346, loss 0.104138, acc 0.94
2016-09-07T21:28:39.613123: step 1347, loss 0.0700227, acc 0.98
2016-09-07T21:28:40.291568: step 1348, loss 0.0710933, acc 0.98
2016-09-07T21:28:40.962327: step 1349, loss 0.0753944, acc 0.98
2016-09-07T21:28:41.624939: step 1350, loss 0.0308399, acc 1
2016-09-07T21:28:42.301625: step 1351, loss 0.20761, acc 0.92
2016-09-07T21:28:42.986434: step 1352, loss 0.114809, acc 0.98
2016-09-07T21:28:43.673616: step 1353, loss 0.0445245, acc 1
2016-09-07T21:28:44.339330: step 1354, loss 0.151461, acc 0.92
2016-09-07T21:28:45.023874: step 1355, loss 0.0512905, acc 1
2016-09-07T21:28:45.700906: step 1356, loss 0.0415614, acc 0.98
2016-09-07T21:28:46.388430: step 1357, loss 0.0998791, acc 0.96
2016-09-07T21:28:46.753401: step 1358, loss 0.224509, acc 0.916667
2016-09-07T21:28:47.446262: step 1359, loss 0.0673068, acc 0.98
2016-09-07T21:28:48.113748: step 1360, loss 0.146731, acc 0.98
2016-09-07T21:28:48.776502: step 1361, loss 0.177475, acc 0.96
2016-09-07T21:28:49.455776: step 1362, loss 0.0567361, acc 0.98
2016-09-07T21:28:50.136291: step 1363, loss 0.151047, acc 0.96
2016-09-07T21:28:50.798526: step 1364, loss 0.0815654, acc 0.94
2016-09-07T21:28:51.486576: step 1365, loss 0.0855916, acc 0.98
2016-09-07T21:28:52.155721: step 1366, loss 0.110505, acc 0.94
2016-09-07T21:28:52.825206: step 1367, loss 0.055865, acc 0.98
2016-09-07T21:28:53.517982: step 1368, loss 0.0845879, acc 0.96
2016-09-07T21:28:54.205893: step 1369, loss 0.04839, acc 0.98
2016-09-07T21:28:54.890700: step 1370, loss 0.0569649, acc 0.96
2016-09-07T21:28:55.564342: step 1371, loss 0.154479, acc 0.94
2016-09-07T21:28:56.241354: step 1372, loss 0.131206, acc 0.94
2016-09-07T21:28:56.915547: step 1373, loss 0.101433, acc 0.98
2016-09-07T21:28:57.581487: step 1374, loss 0.0770951, acc 0.96
2016-09-07T21:28:58.242131: step 1375, loss 0.0528005, acc 0.98
2016-09-07T21:28:58.897429: step 1376, loss 0.0170675, acc 1
2016-09-07T21:28:59.572883: step 1377, loss 0.112898, acc 0.96
2016-09-07T21:29:00.249892: step 1378, loss 0.0324572, acc 1
2016-09-07T21:29:00.898904: step 1379, loss 0.119324, acc 0.94
2016-09-07T21:29:01.588650: step 1380, loss 0.101795, acc 0.98
2016-09-07T21:29:02.280225: step 1381, loss 0.0166663, acc 1
2016-09-07T21:29:02.966252: step 1382, loss 0.0830381, acc 0.98
2016-09-07T21:29:03.640375: step 1383, loss 0.0336399, acc 1
2016-09-07T21:29:04.345812: step 1384, loss 0.106786, acc 0.96
2016-09-07T21:29:05.006944: step 1385, loss 0.0474331, acc 0.98
2016-09-07T21:29:05.672494: step 1386, loss 0.13279, acc 0.92
2016-09-07T21:29:06.362417: step 1387, loss 0.0902615, acc 0.94
2016-09-07T21:29:07.049901: step 1388, loss 0.0336262, acc 0.98
2016-09-07T21:29:07.731900: step 1389, loss 0.0661217, acc 0.98
2016-09-07T21:29:08.399422: step 1390, loss 0.0955696, acc 0.98
2016-09-07T21:29:09.067105: step 1391, loss 0.0887114, acc 0.96
2016-09-07T21:29:09.742757: step 1392, loss 0.125315, acc 0.94
2016-09-07T21:29:10.410941: step 1393, loss 0.0305617, acc 0.98
2016-09-07T21:29:11.066788: step 1394, loss 0.118833, acc 0.94
2016-09-07T21:29:11.734045: step 1395, loss 0.0568773, acc 0.96
2016-09-07T21:29:12.415077: step 1396, loss 0.107481, acc 0.96
2016-09-07T21:29:13.091997: step 1397, loss 0.0728546, acc 0.98
2016-09-07T21:29:13.768288: step 1398, loss 0.110996, acc 0.96
2016-09-07T21:29:14.420197: step 1399, loss 0.060617, acc 1
2016-09-07T21:29:15.099339: step 1400, loss 0.101313, acc 0.94

Evaluation:
2016-09-07T21:29:18.360630: step 1400, loss 1.00796, acc 0.765

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-1400

2016-09-07T21:29:20.075905: step 1401, loss 0.0436851, acc 1
2016-09-07T21:29:20.755090: step 1402, loss 0.118469, acc 0.96
2016-09-07T21:29:21.425147: step 1403, loss 0.0298084, acc 1
2016-09-07T21:29:22.116767: step 1404, loss 0.149727, acc 0.94
2016-09-07T21:29:22.793345: step 1405, loss 0.149253, acc 0.92
2016-09-07T21:29:23.471536: step 1406, loss 0.121419, acc 0.94
2016-09-07T21:29:24.174943: step 1407, loss 0.0383185, acc 0.98
2016-09-07T21:29:24.848070: step 1408, loss 0.0823622, acc 0.96
2016-09-07T21:29:25.512998: step 1409, loss 0.0841181, acc 0.96
2016-09-07T21:29:26.168721: step 1410, loss 0.0294479, acc 1
2016-09-07T21:29:26.847516: step 1411, loss 0.0253784, acc 1
2016-09-07T21:29:27.518110: step 1412, loss 0.119708, acc 0.92
2016-09-07T21:29:28.207045: step 1413, loss 0.016924, acc 1
2016-09-07T21:29:28.866963: step 1414, loss 0.120274, acc 0.96
2016-09-07T21:29:29.534743: step 1415, loss 0.124536, acc 0.96
2016-09-07T21:29:30.204120: step 1416, loss 0.0186052, acc 1
2016-09-07T21:29:30.867706: step 1417, loss 0.0867337, acc 0.96
2016-09-07T21:29:31.545544: step 1418, loss 0.0527415, acc 0.98
2016-09-07T21:29:32.211432: step 1419, loss 0.0971015, acc 0.96
2016-09-07T21:29:32.877888: step 1420, loss 0.0652652, acc 0.96
2016-09-07T21:29:33.571875: step 1421, loss 0.111013, acc 0.94
2016-09-07T21:29:34.236148: step 1422, loss 0.114379, acc 0.94
2016-09-07T21:29:34.926699: step 1423, loss 0.0885844, acc 0.96
2016-09-07T21:29:35.592689: step 1424, loss 0.12207, acc 0.96
2016-09-07T21:29:36.263429: step 1425, loss 0.0793541, acc 0.96
2016-09-07T21:29:36.930131: step 1426, loss 0.0801414, acc 0.96
2016-09-07T21:29:37.604801: step 1427, loss 0.0946871, acc 0.98
2016-09-07T21:29:38.279696: step 1428, loss 0.0734788, acc 1
2016-09-07T21:29:38.948409: step 1429, loss 0.165201, acc 0.92
2016-09-07T21:29:39.619409: step 1430, loss 0.0503614, acc 1
2016-09-07T21:29:40.285769: step 1431, loss 0.100245, acc 0.94
2016-09-07T21:29:40.964882: step 1432, loss 0.0433007, acc 0.98
2016-09-07T21:29:41.644136: step 1433, loss 0.0401128, acc 0.98
2016-09-07T21:29:42.331944: step 1434, loss 0.106563, acc 0.98
2016-09-07T21:29:43.004589: step 1435, loss 0.0549861, acc 0.98
2016-09-07T21:29:43.659683: step 1436, loss 0.0593836, acc 0.96
2016-09-07T21:29:44.326753: step 1437, loss 0.113912, acc 0.96
2016-09-07T21:29:44.980214: step 1438, loss 0.083034, acc 0.96
2016-09-07T21:29:45.639811: step 1439, loss 0.064093, acc 0.98
2016-09-07T21:29:46.306955: step 1440, loss 0.067321, acc 0.96
2016-09-07T21:29:46.984251: step 1441, loss 0.0512863, acc 0.98
2016-09-07T21:29:47.661633: step 1442, loss 0.0741583, acc 0.96
2016-09-07T21:29:48.340952: step 1443, loss 0.0455371, acc 0.96
2016-09-07T21:29:49.030806: step 1444, loss 0.0629393, acc 0.98
2016-09-07T21:29:49.710379: step 1445, loss 0.0595555, acc 0.98
2016-09-07T21:29:50.379479: step 1446, loss 0.0541468, acc 0.98
2016-09-07T21:29:51.056157: step 1447, loss 0.0597049, acc 0.98
2016-09-07T21:29:51.742380: step 1448, loss 0.0859839, acc 0.96
2016-09-07T21:29:52.412977: step 1449, loss 0.192528, acc 0.94
2016-09-07T21:29:53.088600: step 1450, loss 0.231957, acc 0.94
2016-09-07T21:29:53.750397: step 1451, loss 0.112866, acc 0.94
2016-09-07T21:29:54.431616: step 1452, loss 0.0631008, acc 0.96
2016-09-07T21:29:55.097659: step 1453, loss 0.110344, acc 0.96
2016-09-07T21:29:55.783080: step 1454, loss 0.0475866, acc 0.98
2016-09-07T21:29:56.466506: step 1455, loss 0.0488189, acc 1
2016-09-07T21:29:57.127280: step 1456, loss 0.103148, acc 0.94
2016-09-07T21:29:57.802637: step 1457, loss 0.0924069, acc 0.96
2016-09-07T21:29:58.467818: step 1458, loss 0.0419334, acc 0.98
2016-09-07T21:29:59.144088: step 1459, loss 0.0594168, acc 1
2016-09-07T21:29:59.815333: step 1460, loss 0.0961094, acc 0.96
2016-09-07T21:30:00.537634: step 1461, loss 0.041539, acc 0.98
2016-09-07T21:30:01.221435: step 1462, loss 0.0912742, acc 0.94
2016-09-07T21:30:01.892178: step 1463, loss 0.0770485, acc 0.96
2016-09-07T21:30:02.586768: step 1464, loss 0.193203, acc 0.92
2016-09-07T21:30:03.245454: step 1465, loss 0.175895, acc 0.96
2016-09-07T21:30:03.915884: step 1466, loss 0.0486932, acc 0.98
2016-09-07T21:30:04.589435: step 1467, loss 0.095017, acc 0.94
2016-09-07T21:30:05.272355: step 1468, loss 0.0742352, acc 0.98
2016-09-07T21:30:05.968065: step 1469, loss 0.104809, acc 0.98
2016-09-07T21:30:06.638368: step 1470, loss 0.0220271, acc 0.98
2016-09-07T21:30:07.339020: step 1471, loss 0.0611314, acc 0.98
2016-09-07T21:30:08.009545: step 1472, loss 0.0878004, acc 0.98
2016-09-07T21:30:08.696576: step 1473, loss 0.0202319, acc 1
2016-09-07T21:30:09.370144: step 1474, loss 0.0857149, acc 0.98
2016-09-07T21:30:10.049577: step 1475, loss 0.0955317, acc 0.98
2016-09-07T21:30:10.717310: step 1476, loss 0.0953296, acc 0.94
2016-09-07T21:30:11.386198: step 1477, loss 0.0230472, acc 1
2016-09-07T21:30:12.061080: step 1478, loss 0.112371, acc 0.94
2016-09-07T21:30:12.724713: step 1479, loss 0.10594, acc 0.96
2016-09-07T21:30:13.396201: step 1480, loss 0.0648405, acc 0.96
2016-09-07T21:30:14.107793: step 1481, loss 0.0750106, acc 0.96
2016-09-07T21:30:14.799177: step 1482, loss 0.0210688, acc 1
2016-09-07T21:30:15.451806: step 1483, loss 0.133294, acc 0.92
2016-09-07T21:30:16.128656: step 1484, loss 0.103968, acc 0.94
2016-09-07T21:30:16.808293: step 1485, loss 0.148615, acc 0.92
2016-09-07T21:30:17.493615: step 1486, loss 0.0475777, acc 0.96
2016-09-07T21:30:18.187558: step 1487, loss 0.0410107, acc 0.98
2016-09-07T21:30:18.857556: step 1488, loss 0.0126003, acc 1
2016-09-07T21:30:19.531241: step 1489, loss 0.168709, acc 0.96
2016-09-07T21:30:20.202220: step 1490, loss 0.0662544, acc 0.96
2016-09-07T21:30:20.880149: step 1491, loss 0.0345696, acc 0.98
2016-09-07T21:30:21.555213: step 1492, loss 0.0702481, acc 0.96
2016-09-07T21:30:22.233894: step 1493, loss 0.267874, acc 0.96
2016-09-07T21:30:22.915545: step 1494, loss 0.0714653, acc 0.98
2016-09-07T21:30:23.600828: step 1495, loss 0.0570766, acc 0.96
2016-09-07T21:30:24.265021: step 1496, loss 0.122432, acc 0.94
2016-09-07T21:30:24.926389: step 1497, loss 0.120858, acc 0.96
2016-09-07T21:30:25.595615: step 1498, loss 0.0407412, acc 0.98
2016-09-07T21:30:26.289119: step 1499, loss 0.180964, acc 0.92
2016-09-07T21:30:26.973129: step 1500, loss 0.0690726, acc 0.98

Evaluation:
2016-09-07T21:30:30.240990: step 1500, loss 1.11259, acc 0.74

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-1500

2016-09-07T21:30:31.974472: step 1501, loss 0.123475, acc 0.94
2016-09-07T21:30:32.655576: step 1502, loss 0.0328368, acc 1
2016-09-07T21:30:33.314100: step 1503, loss 0.0796535, acc 0.96
2016-09-07T21:30:33.988833: step 1504, loss 0.134447, acc 0.96
2016-09-07T21:30:34.654548: step 1505, loss 0.0863185, acc 0.98
2016-09-07T21:30:35.305244: step 1506, loss 0.113922, acc 0.94
2016-09-07T21:30:35.953853: step 1507, loss 0.0459523, acc 0.98
2016-09-07T21:30:36.617082: step 1508, loss 0.00929828, acc 1
2016-09-07T21:30:37.293167: step 1509, loss 0.14708, acc 0.94
2016-09-07T21:30:37.959508: step 1510, loss 0.102306, acc 0.96
2016-09-07T21:30:38.628504: step 1511, loss 0.173315, acc 0.94
2016-09-07T21:30:39.292173: step 1512, loss 0.113655, acc 0.98
2016-09-07T21:30:39.953640: step 1513, loss 0.06251, acc 0.98
2016-09-07T21:30:40.630520: step 1514, loss 0.0349185, acc 0.98
2016-09-07T21:30:41.316239: step 1515, loss 0.0899713, acc 0.96
2016-09-07T21:30:42.007849: step 1516, loss 0.0436665, acc 0.98
2016-09-07T21:30:42.686421: step 1517, loss 0.02922, acc 1
2016-09-07T21:30:43.391553: step 1518, loss 0.068773, acc 0.94
2016-09-07T21:30:44.050410: step 1519, loss 0.0845675, acc 0.98
2016-09-07T21:30:44.711416: step 1520, loss 0.0464305, acc 0.98
2016-09-07T21:30:45.390279: step 1521, loss 0.0381421, acc 0.98
2016-09-07T21:30:46.067719: step 1522, loss 0.204458, acc 0.94
2016-09-07T21:30:46.749465: step 1523, loss 0.0736786, acc 0.94
2016-09-07T21:30:47.412748: step 1524, loss 0.101888, acc 0.98
2016-09-07T21:30:48.099471: step 1525, loss 0.0622446, acc 0.98
2016-09-07T21:30:48.783501: step 1526, loss 0.0729982, acc 0.98
2016-09-07T21:30:49.465245: step 1527, loss 0.231213, acc 0.94
2016-09-07T21:30:50.139374: step 1528, loss 0.0671141, acc 0.98
2016-09-07T21:30:50.823494: step 1529, loss 0.0802556, acc 0.96
2016-09-07T21:30:51.493503: step 1530, loss 0.0826084, acc 0.94
2016-09-07T21:30:52.167245: step 1531, loss 0.0715374, acc 0.98
2016-09-07T21:30:52.833878: step 1532, loss 0.108903, acc 0.94
2016-09-07T21:30:53.534389: step 1533, loss 0.0645234, acc 1
2016-09-07T21:30:54.214479: step 1534, loss 0.103795, acc 0.92
2016-09-07T21:30:54.875267: step 1535, loss 0.0558317, acc 0.98
2016-09-07T21:30:55.536540: step 1536, loss 0.0252694, acc 0.98
2016-09-07T21:30:56.194363: step 1537, loss 0.0409965, acc 1
2016-09-07T21:30:56.861974: step 1538, loss 0.0931131, acc 0.96
2016-09-07T21:30:57.536494: step 1539, loss 0.112354, acc 0.98
2016-09-07T21:30:58.205372: step 1540, loss 0.0499946, acc 0.98
2016-09-07T21:30:58.880352: step 1541, loss 0.0670872, acc 0.96
2016-09-07T21:30:59.559115: step 1542, loss 0.0581858, acc 0.98
2016-09-07T21:31:00.227804: step 1543, loss 0.0546959, acc 1
2016-09-07T21:31:00.894639: step 1544, loss 0.0604563, acc 0.98
2016-09-07T21:31:01.551466: step 1545, loss 0.149347, acc 0.96
2016-09-07T21:31:02.211876: step 1546, loss 0.0869769, acc 0.94
2016-09-07T21:31:02.871903: step 1547, loss 0.304362, acc 0.92
2016-09-07T21:31:03.559686: step 1548, loss 0.0799418, acc 0.98
2016-09-07T21:31:04.242124: step 1549, loss 0.0351721, acc 1
2016-09-07T21:31:04.917287: step 1550, loss 0.127245, acc 0.96
2016-09-07T21:31:05.578299: step 1551, loss 0.108224, acc 0.94
2016-09-07T21:31:05.947103: step 1552, loss 0.00646144, acc 1
2016-09-07T21:31:06.631001: step 1553, loss 0.0465875, acc 0.98
2016-09-07T21:31:07.305945: step 1554, loss 0.0425457, acc 0.98
2016-09-07T21:31:07.982799: step 1555, loss 0.10048, acc 0.96
2016-09-07T21:31:08.652643: step 1556, loss 0.0243912, acc 1
2016-09-07T21:31:09.306754: step 1557, loss 0.0581336, acc 0.98
2016-09-07T21:31:09.965335: step 1558, loss 0.1768, acc 0.96
2016-09-07T21:31:10.637879: step 1559, loss 0.0468885, acc 1
2016-09-07T21:31:11.302058: step 1560, loss 0.067891, acc 0.96
2016-09-07T21:31:11.957399: step 1561, loss 0.0392613, acc 1
2016-09-07T21:31:12.654483: step 1562, loss 0.0541601, acc 0.98
2016-09-07T21:31:13.306209: step 1563, loss 0.0814981, acc 0.96
2016-09-07T21:31:13.963242: step 1564, loss 0.143942, acc 0.96
2016-09-07T21:31:14.643832: step 1565, loss 0.158447, acc 0.92
2016-09-07T21:31:15.336818: step 1566, loss 0.0676361, acc 0.96
2016-09-07T21:31:16.010035: step 1567, loss 0.0325786, acc 0.98
2016-09-07T21:31:16.702355: step 1568, loss 0.109769, acc 0.94
2016-09-07T21:31:17.363161: step 1569, loss 0.0960094, acc 0.96
2016-09-07T21:31:18.030393: step 1570, loss 0.0591074, acc 0.98
2016-09-07T21:31:18.725440: step 1571, loss 0.0493225, acc 1
2016-09-07T21:31:19.380932: step 1572, loss 0.0442548, acc 1
2016-09-07T21:31:20.035960: step 1573, loss 0.0420103, acc 0.96
2016-09-07T21:31:20.715170: step 1574, loss 0.106845, acc 0.94
2016-09-07T21:31:21.378459: step 1575, loss 0.0336625, acc 1
2016-09-07T21:31:22.045094: step 1576, loss 0.143904, acc 0.94
2016-09-07T21:31:22.714347: step 1577, loss 0.0583368, acc 0.98
2016-09-07T21:31:23.382508: step 1578, loss 0.0285782, acc 0.98
2016-09-07T21:31:24.038410: step 1579, loss 0.0539554, acc 0.98
2016-09-07T21:31:24.704329: step 1580, loss 0.268168, acc 0.94
2016-09-07T21:31:25.387968: step 1581, loss 0.0371757, acc 1
2016-09-07T21:31:26.085131: step 1582, loss 0.186568, acc 0.9
2016-09-07T21:31:26.765298: step 1583, loss 0.0511823, acc 0.96
2016-09-07T21:31:27.434326: step 1584, loss 0.10293, acc 0.96
2016-09-07T21:31:28.109479: step 1585, loss 0.110442, acc 0.98
2016-09-07T21:31:28.781137: step 1586, loss 0.139581, acc 0.94
2016-09-07T21:31:29.442311: step 1587, loss 0.0903481, acc 0.98
2016-09-07T21:31:30.105459: step 1588, loss 0.0762124, acc 0.98
2016-09-07T21:31:30.804236: step 1589, loss 0.0548546, acc 0.98
2016-09-07T21:31:31.473080: step 1590, loss 0.0779553, acc 0.98
2016-09-07T21:31:32.178225: step 1591, loss 0.0894092, acc 0.94
2016-09-07T21:31:32.846849: step 1592, loss 0.0665621, acc 0.96
2016-09-07T21:31:33.521418: step 1593, loss 0.106358, acc 0.96
2016-09-07T21:31:34.191089: step 1594, loss 0.0919349, acc 0.98
2016-09-07T21:31:34.876208: step 1595, loss 0.0968471, acc 0.98
2016-09-07T21:31:35.567443: step 1596, loss 0.0469579, acc 1
2016-09-07T21:31:36.235363: step 1597, loss 0.128466, acc 0.96
2016-09-07T21:31:36.926847: step 1598, loss 0.118944, acc 0.94
2016-09-07T21:31:37.590886: step 1599, loss 0.0525112, acc 0.98
2016-09-07T21:31:38.262918: step 1600, loss 0.0408566, acc 0.98

Evaluation:
2016-09-07T21:31:41.540002: step 1600, loss 1.07413, acc 0.736

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-1600

2016-09-07T21:31:43.355654: step 1601, loss 0.0298636, acc 1
2016-09-07T21:31:44.034427: step 1602, loss 0.041057, acc 1
2016-09-07T21:31:44.709881: step 1603, loss 0.0534382, acc 0.98
2016-09-07T21:31:45.387982: step 1604, loss 0.116927, acc 0.92
2016-09-07T21:31:46.090994: step 1605, loss 0.020767, acc 1
2016-09-07T21:31:46.745982: step 1606, loss 0.0501848, acc 0.98
2016-09-07T21:31:47.423546: step 1607, loss 0.0492646, acc 0.98
2016-09-07T21:31:48.084011: step 1608, loss 0.103285, acc 0.96
2016-09-07T21:31:48.754780: step 1609, loss 0.238619, acc 0.9
2016-09-07T21:31:49.460586: step 1610, loss 0.013074, acc 1
2016-09-07T21:31:50.126438: step 1611, loss 0.0188948, acc 1
2016-09-07T21:31:50.800007: step 1612, loss 0.0595138, acc 0.96
2016-09-07T21:31:51.494052: step 1613, loss 0.124082, acc 0.94
2016-09-07T21:31:52.164192: step 1614, loss 0.129159, acc 0.96
2016-09-07T21:31:52.845877: step 1615, loss 0.0483877, acc 0.98
2016-09-07T21:31:53.515617: step 1616, loss 0.105928, acc 0.96
2016-09-07T21:31:54.197125: step 1617, loss 0.0296205, acc 1
2016-09-07T21:31:54.876431: step 1618, loss 0.121278, acc 0.98
2016-09-07T21:31:55.551255: step 1619, loss 0.148738, acc 0.94
2016-09-07T21:31:56.228079: step 1620, loss 0.0771925, acc 0.98
2016-09-07T21:31:56.888804: step 1621, loss 0.0258736, acc 1
2016-09-07T21:31:57.546989: step 1622, loss 0.0509016, acc 0.98
2016-09-07T21:31:58.226465: step 1623, loss 0.0453044, acc 0.98
2016-09-07T21:31:58.910408: step 1624, loss 0.0695067, acc 0.98
2016-09-07T21:31:59.596621: step 1625, loss 0.0995339, acc 0.96
2016-09-07T21:32:00.267132: step 1626, loss 0.0530061, acc 0.98
2016-09-07T21:32:00.943756: step 1627, loss 0.0876425, acc 0.96
2016-09-07T21:32:01.615293: step 1628, loss 0.0482306, acc 1
2016-09-07T21:32:02.307304: step 1629, loss 0.0634997, acc 0.96
2016-09-07T21:32:02.973488: step 1630, loss 0.0713261, acc 0.96
2016-09-07T21:32:03.658805: step 1631, loss 0.0695201, acc 0.98
2016-09-07T21:32:04.316967: step 1632, loss 0.0952001, acc 0.94
2016-09-07T21:32:05.011943: step 1633, loss 0.0980074, acc 0.96
2016-09-07T21:32:05.679541: step 1634, loss 0.138848, acc 0.96
2016-09-07T21:32:06.358830: step 1635, loss 0.0987824, acc 0.96
2016-09-07T21:32:07.017336: step 1636, loss 0.0484354, acc 0.98
2016-09-07T21:32:07.689854: step 1637, loss 0.0337054, acc 0.98
2016-09-07T21:32:08.372220: step 1638, loss 0.0424424, acc 0.98
2016-09-07T21:32:09.073694: step 1639, loss 0.0461903, acc 0.96
2016-09-07T21:32:09.753073: step 1640, loss 0.0651083, acc 0.98
2016-09-07T21:32:10.428851: step 1641, loss 0.124483, acc 0.96
2016-09-07T21:32:11.110730: step 1642, loss 0.138197, acc 0.94
2016-09-07T21:32:11.794015: step 1643, loss 0.0831297, acc 0.96
2016-09-07T21:32:12.452155: step 1644, loss 0.019687, acc 1
2016-09-07T21:32:13.129700: step 1645, loss 0.157548, acc 0.96
2016-09-07T21:32:13.822435: step 1646, loss 0.0899032, acc 0.94
2016-09-07T21:32:14.518006: step 1647, loss 0.0791095, acc 0.94
2016-09-07T21:32:15.211426: step 1648, loss 0.0578741, acc 0.98
2016-09-07T21:32:15.896468: step 1649, loss 0.050379, acc 1
2016-09-07T21:32:16.582068: step 1650, loss 0.133054, acc 0.92
2016-09-07T21:32:17.239660: step 1651, loss 0.0579016, acc 0.98
2016-09-07T21:32:17.931229: step 1652, loss 0.03743, acc 1
2016-09-07T21:32:18.591257: step 1653, loss 0.0315401, acc 1
2016-09-07T21:32:19.287168: step 1654, loss 0.0271306, acc 1
2016-09-07T21:32:19.969754: step 1655, loss 0.0423, acc 0.98
2016-09-07T21:32:20.647399: step 1656, loss 0.0654367, acc 0.98
2016-09-07T21:32:21.312947: step 1657, loss 0.0540496, acc 0.98
2016-09-07T21:32:21.983313: step 1658, loss 0.170353, acc 0.92
2016-09-07T21:32:22.671715: step 1659, loss 0.139709, acc 0.94
2016-09-07T21:32:23.360506: step 1660, loss 0.0319153, acc 1
2016-09-07T21:32:24.088334: step 1661, loss 0.0632319, acc 0.96
2016-09-07T21:32:24.756665: step 1662, loss 0.0299455, acc 1
2016-09-07T21:32:25.437964: step 1663, loss 0.0895615, acc 0.92
2016-09-07T21:32:26.110372: step 1664, loss 0.0104581, acc 1
2016-09-07T21:32:26.763693: step 1665, loss 0.0689581, acc 0.98
2016-09-07T21:32:27.424584: step 1666, loss 0.0735833, acc 0.94
2016-09-07T21:32:28.080128: step 1667, loss 0.086491, acc 0.96
2016-09-07T21:32:28.756499: step 1668, loss 0.0628406, acc 0.98
2016-09-07T21:32:29.434726: step 1669, loss 0.132651, acc 0.96
2016-09-07T21:32:30.113986: step 1670, loss 0.0461896, acc 0.98
2016-09-07T21:32:30.795987: step 1671, loss 0.131858, acc 0.98
2016-09-07T21:32:31.496620: step 1672, loss 0.136031, acc 0.92
2016-09-07T21:32:32.166263: step 1673, loss 0.0519534, acc 0.98
2016-09-07T21:32:32.842382: step 1674, loss 0.118241, acc 0.92
2016-09-07T21:32:33.526852: step 1675, loss 0.0810083, acc 0.96
2016-09-07T21:32:34.180683: step 1676, loss 0.0637859, acc 0.96
2016-09-07T21:32:34.852622: step 1677, loss 0.0655215, acc 1
2016-09-07T21:32:35.550454: step 1678, loss 0.0701534, acc 0.96
2016-09-07T21:32:36.219413: step 1679, loss 0.0888474, acc 0.98
2016-09-07T21:32:36.881373: step 1680, loss 0.0152354, acc 1
2016-09-07T21:32:37.552358: step 1681, loss 0.0996154, acc 0.98
2016-09-07T21:32:38.223249: step 1682, loss 0.0918277, acc 0.96
2016-09-07T21:32:38.895813: step 1683, loss 0.116467, acc 0.94
2016-09-07T21:32:39.571002: step 1684, loss 0.115395, acc 0.94
2016-09-07T21:32:40.232037: step 1685, loss 0.0419932, acc 0.98
2016-09-07T21:32:40.913674: step 1686, loss 0.0650178, acc 0.96
2016-09-07T21:32:41.586995: step 1687, loss 0.134147, acc 0.94
2016-09-07T21:32:42.272713: step 1688, loss 0.0592187, acc 0.98
2016-09-07T21:32:42.961617: step 1689, loss 0.158992, acc 0.96
2016-09-07T21:32:43.648163: step 1690, loss 0.0418491, acc 0.98
2016-09-07T21:32:44.325698: step 1691, loss 0.0269935, acc 1
2016-09-07T21:32:45.012268: step 1692, loss 0.0777505, acc 0.96
2016-09-07T21:32:45.677949: step 1693, loss 0.105803, acc 0.94
2016-09-07T21:32:46.362833: step 1694, loss 0.0667774, acc 0.98
2016-09-07T21:32:47.039935: step 1695, loss 0.0350064, acc 1
2016-09-07T21:32:47.715129: step 1696, loss 0.0420775, acc 0.98
2016-09-07T21:32:48.392324: step 1697, loss 0.0408638, acc 0.98
2016-09-07T21:32:49.062715: step 1698, loss 0.0689549, acc 0.98
2016-09-07T21:32:49.716936: step 1699, loss 0.152257, acc 0.9
2016-09-07T21:32:50.381097: step 1700, loss 0.044511, acc 1

Evaluation:
2016-09-07T21:32:53.652511: step 1700, loss 1.24413, acc 0.767

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-1700

2016-09-07T21:32:55.363208: step 1701, loss 0.0783315, acc 0.96
2016-09-07T21:32:56.062089: step 1702, loss 0.0345087, acc 0.98
2016-09-07T21:32:56.714789: step 1703, loss 0.0786858, acc 0.98
2016-09-07T21:32:57.398125: step 1704, loss 0.172113, acc 0.92
2016-09-07T21:32:58.067747: step 1705, loss 0.0499494, acc 0.98
2016-09-07T21:32:58.725431: step 1706, loss 0.223916, acc 0.92
2016-09-07T21:32:59.382773: step 1707, loss 0.079478, acc 0.94
2016-09-07T21:33:00.072829: step 1708, loss 0.0355538, acc 0.98
2016-09-07T21:33:00.789514: step 1709, loss 0.0392489, acc 0.98
2016-09-07T21:33:01.471062: step 1710, loss 0.184668, acc 0.94
2016-09-07T21:33:02.124509: step 1711, loss 0.0554592, acc 0.98
2016-09-07T21:33:02.805636: step 1712, loss 0.0590748, acc 0.94
2016-09-07T21:33:03.473521: step 1713, loss 0.0610907, acc 0.96
2016-09-07T21:33:04.137940: step 1714, loss 0.0876973, acc 0.96
2016-09-07T21:33:04.809480: step 1715, loss 0.0778515, acc 0.96
2016-09-07T21:33:05.485915: step 1716, loss 0.0503033, acc 0.98
2016-09-07T21:33:06.180049: step 1717, loss 0.0446879, acc 1
2016-09-07T21:33:06.855452: step 1718, loss 0.095251, acc 0.96
2016-09-07T21:33:07.526325: step 1719, loss 0.0494808, acc 0.98
2016-09-07T21:33:08.194372: step 1720, loss 0.079096, acc 0.98
2016-09-07T21:33:08.894331: step 1721, loss 0.11157, acc 0.96
2016-09-07T21:33:09.578351: step 1722, loss 0.119799, acc 0.94
2016-09-07T21:33:10.276168: step 1723, loss 0.143133, acc 0.92
2016-09-07T21:33:10.948341: step 1724, loss 0.0599212, acc 0.98
2016-09-07T21:33:11.640553: step 1725, loss 0.0917988, acc 0.94
2016-09-07T21:33:12.306810: step 1726, loss 0.0311526, acc 0.98
2016-09-07T21:33:12.967045: step 1727, loss 0.02741, acc 1
2016-09-07T21:33:13.636555: step 1728, loss 0.0951078, acc 0.96
2016-09-07T21:33:14.301343: step 1729, loss 0.0610448, acc 0.98
2016-09-07T21:33:14.967457: step 1730, loss 0.0937318, acc 0.98
2016-09-07T21:33:15.666174: step 1731, loss 0.0441129, acc 0.98
2016-09-07T21:33:16.335501: step 1732, loss 0.0584059, acc 0.98
2016-09-07T21:33:17.012038: step 1733, loss 0.0875733, acc 0.96
2016-09-07T21:33:17.672942: step 1734, loss 0.0500651, acc 1
2016-09-07T21:33:18.349225: step 1735, loss 0.0477937, acc 0.96
2016-09-07T21:33:19.058791: step 1736, loss 0.0333088, acc 0.98
2016-09-07T21:33:19.746013: step 1737, loss 0.0522921, acc 0.98
2016-09-07T21:33:20.438301: step 1738, loss 0.105885, acc 0.96
2016-09-07T21:33:21.120288: step 1739, loss 0.12185, acc 0.92
2016-09-07T21:33:21.773950: step 1740, loss 0.0363137, acc 1
2016-09-07T21:33:22.436468: step 1741, loss 0.0428959, acc 1
2016-09-07T21:33:23.110789: step 1742, loss 0.0585622, acc 0.96
2016-09-07T21:33:23.784112: step 1743, loss 0.192642, acc 0.94
2016-09-07T21:33:24.466770: step 1744, loss 0.0732951, acc 0.96
2016-09-07T21:33:25.147432: step 1745, loss 0.0599667, acc 0.98
2016-09-07T21:33:25.503894: step 1746, loss 0.0965661, acc 0.916667
2016-09-07T21:33:26.188929: step 1747, loss 0.0827313, acc 0.98
2016-09-07T21:33:26.860174: step 1748, loss 0.173303, acc 0.96
2016-09-07T21:33:27.549455: step 1749, loss 0.1969, acc 0.94
2016-09-07T21:33:28.214524: step 1750, loss 0.0230033, acc 1
2016-09-07T21:33:28.903260: step 1751, loss 0.0584821, acc 0.98
2016-09-07T21:33:29.565957: step 1752, loss 0.024696, acc 1
2016-09-07T21:33:30.237339: step 1753, loss 0.0400102, acc 0.98
2016-09-07T21:33:30.888368: step 1754, loss 0.10712, acc 0.96
2016-09-07T21:33:31.555989: step 1755, loss 0.0315232, acc 1
2016-09-07T21:33:32.232427: step 1756, loss 0.0211506, acc 1
2016-09-07T21:33:32.909843: step 1757, loss 0.052374, acc 0.98
2016-09-07T21:33:33.592054: step 1758, loss 0.0984111, acc 0.96
2016-09-07T21:33:34.264407: step 1759, loss 0.0355823, acc 1
2016-09-07T21:33:34.939109: step 1760, loss 0.0743592, acc 0.94
2016-09-07T21:33:35.599472: step 1761, loss 0.00705651, acc 1
2016-09-07T21:33:36.264565: step 1762, loss 0.051405, acc 0.98
2016-09-07T21:33:36.932890: step 1763, loss 0.0304793, acc 0.98
2016-09-07T21:33:37.621091: step 1764, loss 0.130693, acc 0.98
2016-09-07T21:33:38.283198: step 1765, loss 0.0740173, acc 0.96
2016-09-07T21:33:38.974552: step 1766, loss 0.0691715, acc 0.96
2016-09-07T21:33:39.662880: step 1767, loss 0.0893099, acc 0.94
2016-09-07T21:33:40.341766: step 1768, loss 0.0536194, acc 0.96
2016-09-07T21:33:41.004015: step 1769, loss 0.0369313, acc 1
2016-09-07T21:33:41.656479: step 1770, loss 0.036582, acc 0.98
2016-09-07T21:33:42.323789: step 1771, loss 0.0377806, acc 1
2016-09-07T21:33:43.022038: step 1772, loss 0.00662783, acc 1
2016-09-07T21:33:43.684813: step 1773, loss 0.019061, acc 1
2016-09-07T21:33:44.357846: step 1774, loss 0.0351185, acc 0.98
2016-09-07T21:33:45.028258: step 1775, loss 0.095354, acc 0.96
2016-09-07T21:33:45.715613: step 1776, loss 0.00945444, acc 1
2016-09-07T21:33:46.389849: step 1777, loss 0.0493465, acc 0.96
2016-09-07T21:33:47.055654: step 1778, loss 0.0729595, acc 0.96
2016-09-07T21:33:47.734086: step 1779, loss 0.0493185, acc 0.98
2016-09-07T21:33:48.422355: step 1780, loss 0.0995383, acc 0.94
2016-09-07T21:33:49.125391: step 1781, loss 0.0400387, acc 0.98
2016-09-07T21:33:49.810991: step 1782, loss 0.0449187, acc 1
2016-09-07T21:33:50.492694: step 1783, loss 0.0197776, acc 1
2016-09-07T21:33:51.167331: step 1784, loss 0.138731, acc 0.94
2016-09-07T21:33:51.816638: step 1785, loss 0.0173353, acc 1
2016-09-07T21:33:52.496247: step 1786, loss 0.0422709, acc 0.98
2016-09-07T21:33:53.182570: step 1787, loss 0.128094, acc 0.96
2016-09-07T21:33:53.891120: step 1788, loss 0.0416248, acc 0.98
2016-09-07T21:33:54.577165: step 1789, loss 0.0362572, acc 0.98
2016-09-07T21:33:55.260319: step 1790, loss 0.00761832, acc 1
2016-09-07T21:33:55.955192: step 1791, loss 0.0356149, acc 0.98
2016-09-07T21:33:56.621948: step 1792, loss 0.128168, acc 0.94
2016-09-07T21:33:57.306367: step 1793, loss 0.0505479, acc 0.96
2016-09-07T21:33:57.982257: step 1794, loss 0.0843899, acc 0.98
2016-09-07T21:33:58.662936: step 1795, loss 0.0296413, acc 1
2016-09-07T21:33:59.330135: step 1796, loss 0.0873013, acc 0.96
2016-09-07T21:34:00.009966: step 1797, loss 0.0355894, acc 1
2016-09-07T21:34:00.719861: step 1798, loss 0.0452511, acc 0.98
2016-09-07T21:34:01.395013: step 1799, loss 0.0641234, acc 0.96
2016-09-07T21:34:02.090101: step 1800, loss 0.0362382, acc 1

Evaluation:
2016-09-07T21:34:05.401742: step 1800, loss 1.20436, acc 0.766

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-1800

2016-09-07T21:34:07.197021: step 1801, loss 0.0592238, acc 0.98
2016-09-07T21:34:07.868742: step 1802, loss 0.0819607, acc 0.96
2016-09-07T21:34:08.545116: step 1803, loss 0.0559748, acc 0.98
2016-09-07T21:34:09.212835: step 1804, loss 0.0199576, acc 1
2016-09-07T21:34:09.890644: step 1805, loss 0.0557908, acc 0.96
2016-09-07T21:34:10.558980: step 1806, loss 0.0707523, acc 0.94
2016-09-07T21:34:11.225323: step 1807, loss 0.0821795, acc 0.96
2016-09-07T21:34:11.898885: step 1808, loss 0.133887, acc 0.94
2016-09-07T21:34:12.567799: step 1809, loss 0.103596, acc 0.96
2016-09-07T21:34:13.227483: step 1810, loss 0.0424793, acc 1
2016-09-07T21:34:13.890516: step 1811, loss 0.0466069, acc 0.98
2016-09-07T21:34:14.551784: step 1812, loss 0.094168, acc 0.96
2016-09-07T21:34:15.211510: step 1813, loss 0.0265472, acc 0.98
2016-09-07T21:34:15.886709: step 1814, loss 0.109625, acc 0.96
2016-09-07T21:34:16.542936: step 1815, loss 0.0313354, acc 0.98
2016-09-07T21:34:17.214053: step 1816, loss 0.0138457, acc 1
2016-09-07T21:34:17.898101: step 1817, loss 0.0132731, acc 1
2016-09-07T21:34:18.561012: step 1818, loss 0.0290185, acc 0.98
2016-09-07T21:34:19.231711: step 1819, loss 0.159633, acc 0.96
2016-09-07T21:34:19.898409: step 1820, loss 0.042558, acc 0.98
2016-09-07T21:34:20.555398: step 1821, loss 0.0429746, acc 1
2016-09-07T21:34:21.240393: step 1822, loss 0.0499586, acc 0.96
2016-09-07T21:34:21.905499: step 1823, loss 0.114425, acc 0.98
2016-09-07T21:34:22.592122: step 1824, loss 0.0482291, acc 0.98
2016-09-07T21:34:23.258909: step 1825, loss 0.0863985, acc 0.96
2016-09-07T21:34:23.936205: step 1826, loss 0.0441882, acc 0.98
2016-09-07T21:34:24.617134: step 1827, loss 0.0988143, acc 0.96
2016-09-07T21:34:25.291291: step 1828, loss 0.0210794, acc 1
2016-09-07T21:34:25.953934: step 1829, loss 0.125403, acc 0.92
2016-09-07T21:34:26.619517: step 1830, loss 0.05103, acc 0.98
2016-09-07T21:34:27.277016: step 1831, loss 0.0602437, acc 0.98
2016-09-07T21:34:27.941854: step 1832, loss 0.0429845, acc 0.98
2016-09-07T21:34:28.607580: step 1833, loss 0.0968622, acc 0.94
2016-09-07T21:34:29.282971: step 1834, loss 0.0590278, acc 0.98
2016-09-07T21:34:29.955223: step 1835, loss 0.0746325, acc 0.96
2016-09-07T21:34:30.611063: step 1836, loss 0.0431592, acc 0.98
2016-09-07T21:34:31.286329: step 1837, loss 0.0700394, acc 0.96
2016-09-07T21:34:31.969237: step 1838, loss 0.0793811, acc 0.96
2016-09-07T21:34:32.650973: step 1839, loss 0.122748, acc 0.94
2016-09-07T21:34:33.322902: step 1840, loss 0.0377557, acc 0.98
2016-09-07T21:34:34.005882: step 1841, loss 0.0933917, acc 0.96
2016-09-07T21:34:34.696646: step 1842, loss 0.0553256, acc 0.98
2016-09-07T21:34:35.376741: step 1843, loss 0.070251, acc 0.98
2016-09-07T21:34:36.080526: step 1844, loss 0.0777688, acc 0.94
2016-09-07T21:34:36.760268: step 1845, loss 0.0360525, acc 1
2016-09-07T21:34:37.444483: step 1846, loss 0.134788, acc 0.96
2016-09-07T21:34:38.130109: step 1847, loss 0.0473286, acc 0.98
2016-09-07T21:34:38.791654: step 1848, loss 0.0284055, acc 1
2016-09-07T21:34:39.471181: step 1849, loss 0.0561523, acc 0.98
2016-09-07T21:34:40.171147: step 1850, loss 0.0262248, acc 0.98
2016-09-07T21:34:40.849160: step 1851, loss 0.033194, acc 1
2016-09-07T21:34:41.520471: step 1852, loss 0.0593974, acc 0.96
2016-09-07T21:34:42.174116: step 1853, loss 0.140658, acc 0.96
2016-09-07T21:34:42.864492: step 1854, loss 0.119774, acc 0.98
2016-09-07T21:34:43.553316: step 1855, loss 0.0921865, acc 0.94
2016-09-07T21:34:44.222388: step 1856, loss 0.0968944, acc 0.94
2016-09-07T21:34:44.895190: step 1857, loss 0.242386, acc 0.88
2016-09-07T21:34:45.561245: step 1858, loss 0.0211514, acc 1
2016-09-07T21:34:46.225133: step 1859, loss 0.043723, acc 0.98
2016-09-07T21:34:46.893349: step 1860, loss 0.150096, acc 0.96
2016-09-07T21:34:47.564260: step 1861, loss 0.136774, acc 0.98
2016-09-07T21:34:48.255044: step 1862, loss 0.0304526, acc 1
2016-09-07T21:34:48.933952: step 1863, loss 0.079878, acc 0.96
2016-09-07T21:34:49.624183: step 1864, loss 0.121549, acc 0.96
2016-09-07T21:34:50.301292: step 1865, loss 0.0449805, acc 0.98
2016-09-07T21:34:50.990946: step 1866, loss 0.0862605, acc 0.94
2016-09-07T21:34:51.671719: step 1867, loss 0.0882066, acc 0.96
2016-09-07T21:34:52.346130: step 1868, loss 0.0948276, acc 0.98
2016-09-07T21:34:53.025435: step 1869, loss 0.0223126, acc 1
2016-09-07T21:34:53.679338: step 1870, loss 0.0902633, acc 0.98
2016-09-07T21:34:54.359896: step 1871, loss 0.153889, acc 0.96
2016-09-07T21:34:55.037875: step 1872, loss 0.0489236, acc 0.98
2016-09-07T21:34:55.721566: step 1873, loss 0.0795299, acc 0.96
2016-09-07T21:34:56.405830: step 1874, loss 0.0366647, acc 1
2016-09-07T21:34:57.088424: step 1875, loss 0.0814203, acc 0.98
2016-09-07T21:34:57.757060: step 1876, loss 0.0674691, acc 0.98
2016-09-07T21:34:58.444942: step 1877, loss 0.0419326, acc 1
2016-09-07T21:34:59.134316: step 1878, loss 0.0701348, acc 0.96
2016-09-07T21:34:59.804348: step 1879, loss 0.0604107, acc 0.98
2016-09-07T21:35:00.522723: step 1880, loss 0.0330539, acc 0.98
2016-09-07T21:35:01.173786: step 1881, loss 0.112121, acc 0.98
2016-09-07T21:35:01.841958: step 1882, loss 0.0955485, acc 0.96
2016-09-07T21:35:02.520260: step 1883, loss 0.0636228, acc 0.96
2016-09-07T21:35:03.179956: step 1884, loss 0.0384462, acc 0.98
2016-09-07T21:35:03.863356: step 1885, loss 0.0533822, acc 0.96
2016-09-07T21:35:04.535423: step 1886, loss 0.0442658, acc 0.98
2016-09-07T21:35:05.227831: step 1887, loss 0.0832116, acc 0.96
2016-09-07T21:35:05.909736: step 1888, loss 0.0609891, acc 0.96
2016-09-07T21:35:06.613754: step 1889, loss 0.0331969, acc 1
2016-09-07T21:35:07.285116: step 1890, loss 0.0288994, acc 1
2016-09-07T21:35:07.971292: step 1891, loss 0.026737, acc 0.98
2016-09-07T21:35:08.671388: step 1892, loss 0.0692972, acc 0.98
2016-09-07T21:35:09.343037: step 1893, loss 0.043519, acc 0.98
2016-09-07T21:35:10.020204: step 1894, loss 0.0341955, acc 0.98
2016-09-07T21:35:10.715458: step 1895, loss 0.0599586, acc 0.98
2016-09-07T21:35:11.397763: step 1896, loss 0.154948, acc 0.96
2016-09-07T21:35:12.082114: step 1897, loss 0.0632216, acc 0.96
2016-09-07T21:35:12.765456: step 1898, loss 0.0682952, acc 0.98
2016-09-07T21:35:13.421238: step 1899, loss 0.166486, acc 0.92
2016-09-07T21:35:14.088850: step 1900, loss 0.071978, acc 0.98

Evaluation:
2016-09-07T21:35:17.369181: step 1900, loss 1.33731, acc 0.758

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-1900

2016-09-07T21:35:19.057939: step 1901, loss 0.0464656, acc 1
2016-09-07T21:35:19.739077: step 1902, loss 0.0485844, acc 0.98
2016-09-07T21:35:20.419853: step 1903, loss 0.0514961, acc 0.98
2016-09-07T21:35:21.086358: step 1904, loss 0.0862322, acc 0.98
2016-09-07T21:35:21.746306: step 1905, loss 0.0507467, acc 0.98
2016-09-07T21:35:22.423199: step 1906, loss 0.0520277, acc 0.98
2016-09-07T21:35:23.081258: step 1907, loss 0.0586851, acc 0.98
2016-09-07T21:35:23.758733: step 1908, loss 0.021782, acc 1
2016-09-07T21:35:24.432379: step 1909, loss 0.0334563, acc 0.98
2016-09-07T21:35:25.116208: step 1910, loss 0.0879866, acc 0.96
2016-09-07T21:35:25.791576: step 1911, loss 0.0463298, acc 0.98
2016-09-07T21:35:26.475972: step 1912, loss 0.0581259, acc 0.96
2016-09-07T21:35:27.143546: step 1913, loss 0.0164489, acc 1
2016-09-07T21:35:27.796934: step 1914, loss 0.165682, acc 0.94
2016-09-07T21:35:28.483308: step 1915, loss 0.0836446, acc 0.96
2016-09-07T21:35:29.161085: step 1916, loss 0.128284, acc 0.94
2016-09-07T21:35:29.840375: step 1917, loss 0.050091, acc 0.98
2016-09-07T21:35:30.513692: step 1918, loss 0.120423, acc 0.96
2016-09-07T21:35:31.199568: step 1919, loss 0.0235305, acc 1
2016-09-07T21:35:31.856534: step 1920, loss 0.0122381, acc 1
2016-09-07T21:35:32.527642: step 1921, loss 0.0375121, acc 1
2016-09-07T21:35:33.206281: step 1922, loss 0.107568, acc 0.96
2016-09-07T21:35:33.895198: step 1923, loss 0.111827, acc 0.96
2016-09-07T21:35:34.556214: step 1924, loss 0.0412276, acc 0.98
2016-09-07T21:35:35.242589: step 1925, loss 0.110179, acc 0.94
2016-09-07T21:35:35.907351: step 1926, loss 0.0532154, acc 1
2016-09-07T21:35:36.601567: step 1927, loss 0.0966071, acc 0.98
2016-09-07T21:35:37.271182: step 1928, loss 0.0954142, acc 0.98
2016-09-07T21:35:37.936048: step 1929, loss 0.0362333, acc 0.98
2016-09-07T21:35:38.614624: step 1930, loss 0.084323, acc 0.96
2016-09-07T21:35:39.281932: step 1931, loss 0.0281627, acc 1
2016-09-07T21:35:39.970135: step 1932, loss 0.0588834, acc 0.98
2016-09-07T21:35:40.647010: step 1933, loss 0.0646735, acc 0.96
2016-09-07T21:35:41.334494: step 1934, loss 0.0285284, acc 1
2016-09-07T21:35:42.006658: step 1935, loss 0.102215, acc 0.96
2016-09-07T21:35:42.660436: step 1936, loss 0.0537761, acc 0.98
2016-09-07T21:35:43.336520: step 1937, loss 0.0594951, acc 0.96
2016-09-07T21:35:44.024888: step 1938, loss 0.025754, acc 1
2016-09-07T21:35:44.737290: step 1939, loss 0.139829, acc 0.94
2016-09-07T21:35:45.123418: step 1940, loss 0.225509, acc 0.916667
2016-09-07T21:35:45.820832: step 1941, loss 0.0543046, acc 0.96
2016-09-07T21:35:46.484968: step 1942, loss 0.0877489, acc 0.96
2016-09-07T21:35:47.179921: step 1943, loss 0.060462, acc 1
2016-09-07T21:35:47.863761: step 1944, loss 0.104793, acc 0.94
2016-09-07T21:35:48.527229: step 1945, loss 0.237842, acc 0.94
2016-09-07T21:35:49.210239: step 1946, loss 0.179017, acc 0.94
2016-09-07T21:35:49.888825: step 1947, loss 0.120716, acc 0.92
2016-09-07T21:35:50.563360: step 1948, loss 0.142129, acc 0.96
2016-09-07T21:35:51.236516: step 1949, loss 0.0683996, acc 0.98
2016-09-07T21:35:51.936473: step 1950, loss 0.0379344, acc 1
2016-09-07T21:35:52.624218: step 1951, loss 0.0368386, acc 1
2016-09-07T21:35:53.306949: step 1952, loss 0.0747598, acc 0.98
2016-09-07T21:35:53.982423: step 1953, loss 0.113895, acc 0.94
2016-09-07T21:35:54.671506: step 1954, loss 0.149461, acc 0.92
2016-09-07T21:35:55.353051: step 1955, loss 0.132125, acc 0.92
2016-09-07T21:35:56.010532: step 1956, loss 0.0510159, acc 0.98
2016-09-07T21:35:56.655898: step 1957, loss 0.0947364, acc 0.96
2016-09-07T21:35:57.313975: step 1958, loss 0.125225, acc 0.94
2016-09-07T21:35:57.956401: step 1959, loss 0.168235, acc 0.9
2016-09-07T21:35:58.635325: step 1960, loss 0.0425939, acc 0.98
2016-09-07T21:35:59.296252: step 1961, loss 0.0317346, acc 1
2016-09-07T21:35:59.965862: step 1962, loss 0.0715078, acc 0.96
2016-09-07T21:36:00.683262: step 1963, loss 0.0630388, acc 0.98
2016-09-07T21:36:01.349707: step 1964, loss 0.0166525, acc 1
2016-09-07T21:36:02.021145: step 1965, loss 0.0446807, acc 0.98
2016-09-07T21:36:02.690569: step 1966, loss 0.053836, acc 1
2016-09-07T21:36:03.387102: step 1967, loss 0.0299908, acc 1
2016-09-07T21:36:04.060815: step 1968, loss 0.0615863, acc 0.98
2016-09-07T21:36:04.725519: step 1969, loss 0.0520259, acc 0.98
2016-09-07T21:36:05.412994: step 1970, loss 0.0560406, acc 0.98
2016-09-07T21:36:06.099265: step 1971, loss 0.024611, acc 1
2016-09-07T21:36:06.785799: step 1972, loss 0.0565942, acc 0.96
2016-09-07T21:36:07.457655: step 1973, loss 0.108729, acc 0.94
2016-09-07T21:36:08.133806: step 1974, loss 0.0634388, acc 0.98
2016-09-07T21:36:08.823323: step 1975, loss 0.0727002, acc 0.96
2016-09-07T21:36:09.497492: step 1976, loss 0.114079, acc 0.96
2016-09-07T21:36:10.221376: step 1977, loss 0.111352, acc 0.96
2016-09-07T21:36:10.898255: step 1978, loss 0.0763559, acc 0.96
2016-09-07T21:36:11.563136: step 1979, loss 0.0786946, acc 0.96
2016-09-07T21:36:12.235319: step 1980, loss 0.138891, acc 0.96
2016-09-07T21:36:12.920609: step 1981, loss 0.0814195, acc 0.94
2016-09-07T21:36:13.619721: step 1982, loss 0.14337, acc 0.96
2016-09-07T21:36:14.305645: step 1983, loss 0.0147993, acc 1
2016-09-07T21:36:15.009565: step 1984, loss 0.0445084, acc 0.98
2016-09-07T21:36:15.685833: step 1985, loss 0.0508146, acc 0.98
2016-09-07T21:36:16.358142: step 1986, loss 0.1005, acc 0.94
2016-09-07T21:36:17.013408: step 1987, loss 0.0483848, acc 1
2016-09-07T21:36:17.689370: step 1988, loss 0.0296643, acc 1
2016-09-07T21:36:18.335892: step 1989, loss 0.12339, acc 0.96
2016-09-07T21:36:19.008669: step 1990, loss 0.0618389, acc 0.98
2016-09-07T21:36:19.686146: step 1991, loss 0.0474154, acc 1
2016-09-07T21:36:20.373810: step 1992, loss 0.102411, acc 0.96
2016-09-07T21:36:21.072479: step 1993, loss 0.0167692, acc 1
2016-09-07T21:36:21.742576: step 1994, loss 0.121739, acc 0.96
2016-09-07T21:36:22.425156: step 1995, loss 0.0722515, acc 0.96
2016-09-07T21:36:23.089004: step 1996, loss 0.0759275, acc 0.98
2016-09-07T21:36:23.796872: step 1997, loss 0.0770482, acc 0.94
2016-09-07T21:36:24.454717: step 1998, loss 0.0239932, acc 1
2016-09-07T21:36:25.136832: step 1999, loss 0.0897904, acc 0.94
2016-09-07T21:36:25.807625: step 2000, loss 0.0990744, acc 0.96

Evaluation:
2016-09-07T21:36:29.123016: step 2000, loss 1.00897, acc 0.752

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-2000

2016-09-07T21:36:30.780228: step 2001, loss 0.0703523, acc 0.98
2016-09-07T21:36:31.463232: step 2002, loss 0.0424415, acc 0.98
2016-09-07T21:36:32.144155: step 2003, loss 0.100018, acc 0.98
2016-09-07T21:36:32.825312: step 2004, loss 0.142582, acc 0.92
2016-09-07T21:36:33.511313: step 2005, loss 0.0327597, acc 1
2016-09-07T21:36:34.198026: step 2006, loss 0.0662055, acc 1
2016-09-07T21:36:34.861632: step 2007, loss 0.0969296, acc 0.94
2016-09-07T21:36:35.531491: step 2008, loss 0.0637786, acc 0.98
2016-09-07T21:36:36.202168: step 2009, loss 0.0635894, acc 0.98
2016-09-07T21:36:36.877962: step 2010, loss 0.0602056, acc 0.98
2016-09-07T21:36:37.541362: step 2011, loss 0.0426322, acc 1
2016-09-07T21:36:38.239661: step 2012, loss 0.0430702, acc 0.98
2016-09-07T21:36:38.928477: step 2013, loss 0.0219779, acc 1
2016-09-07T21:36:39.616815: step 2014, loss 0.048611, acc 0.98
2016-09-07T21:36:40.318368: step 2015, loss 0.0567808, acc 0.98
2016-09-07T21:36:41.013325: step 2016, loss 0.0632519, acc 0.96
2016-09-07T21:36:41.693013: step 2017, loss 0.026915, acc 1
2016-09-07T21:36:42.359672: step 2018, loss 0.0600664, acc 0.98
2016-09-07T21:36:43.034707: step 2019, loss 0.0252315, acc 1
2016-09-07T21:36:43.698721: step 2020, loss 0.147949, acc 0.94
2016-09-07T21:36:44.362626: step 2021, loss 0.117406, acc 0.96
2016-09-07T21:36:45.049802: step 2022, loss 0.0225782, acc 1
2016-09-07T21:36:45.732079: step 2023, loss 0.0678636, acc 0.94
2016-09-07T21:36:46.419988: step 2024, loss 0.0252772, acc 1
2016-09-07T21:36:47.084508: step 2025, loss 0.033034, acc 0.98
2016-09-07T21:36:47.757778: step 2026, loss 0.0614861, acc 0.98
2016-09-07T21:36:48.422644: step 2027, loss 0.0602402, acc 0.96
2016-09-07T21:36:49.078073: step 2028, loss 0.088763, acc 0.98
2016-09-07T21:36:49.756096: step 2029, loss 0.0162679, acc 1
2016-09-07T21:36:50.413846: step 2030, loss 0.118003, acc 0.92
2016-09-07T21:36:51.078414: step 2031, loss 0.212792, acc 0.94
2016-09-07T21:36:51.767984: step 2032, loss 0.0727262, acc 0.96
2016-09-07T21:36:52.443710: step 2033, loss 0.0845087, acc 0.96
2016-09-07T21:36:53.126963: step 2034, loss 0.0400848, acc 1
2016-09-07T21:36:53.819669: step 2035, loss 0.0712877, acc 0.98
2016-09-07T21:36:54.485895: step 2036, loss 0.0249583, acc 1
2016-09-07T21:36:55.166123: step 2037, loss 0.0323507, acc 1
2016-09-07T21:36:55.818589: step 2038, loss 0.0491457, acc 0.98
2016-09-07T21:36:56.524348: step 2039, loss 0.0702416, acc 1
2016-09-07T21:36:57.170053: step 2040, loss 0.0768739, acc 0.98
2016-09-07T21:36:57.831400: step 2041, loss 0.0952423, acc 0.98
2016-09-07T21:36:58.506756: step 2042, loss 0.0447904, acc 1
2016-09-07T21:36:59.166233: step 2043, loss 0.0563807, acc 1
2016-09-07T21:36:59.830168: step 2044, loss 0.162685, acc 0.94
2016-09-07T21:37:00.533839: step 2045, loss 0.017797, acc 1
2016-09-07T21:37:01.202281: step 2046, loss 0.15502, acc 0.92
2016-09-07T21:37:01.866002: step 2047, loss 0.0917688, acc 0.94
2016-09-07T21:37:02.535559: step 2048, loss 0.0254126, acc 1
2016-09-07T21:37:03.228944: step 2049, loss 0.0892735, acc 0.96
2016-09-07T21:37:03.877732: step 2050, loss 0.0301485, acc 0.98
2016-09-07T21:37:04.523951: step 2051, loss 0.0716182, acc 0.98
2016-09-07T21:37:05.201665: step 2052, loss 0.0500697, acc 0.98
2016-09-07T21:37:05.884296: step 2053, loss 0.0821979, acc 0.96
2016-09-07T21:37:06.559351: step 2054, loss 0.0418236, acc 1
2016-09-07T21:37:07.226876: step 2055, loss 0.046067, acc 0.98
2016-09-07T21:37:07.901125: step 2056, loss 0.0646815, acc 0.98
2016-09-07T21:37:08.581936: step 2057, loss 0.0110526, acc 1
2016-09-07T21:37:09.254507: step 2058, loss 0.0556257, acc 0.98
2016-09-07T21:37:09.913712: step 2059, loss 0.0570169, acc 0.96
2016-09-07T21:37:10.604547: step 2060, loss 0.168453, acc 0.94
2016-09-07T21:37:11.263033: step 2061, loss 0.0191681, acc 1
2016-09-07T21:37:11.930750: step 2062, loss 0.112935, acc 0.96
2016-09-07T21:37:12.600262: step 2063, loss 0.0543576, acc 0.96
2016-09-07T21:37:13.256472: step 2064, loss 0.0154838, acc 1
2016-09-07T21:37:13.920955: step 2065, loss 0.0326119, acc 1
2016-09-07T21:37:14.587570: step 2066, loss 0.0980603, acc 0.96
2016-09-07T21:37:15.253836: step 2067, loss 0.0861946, acc 0.96
2016-09-07T21:37:15.925867: step 2068, loss 0.0370375, acc 1
2016-09-07T21:37:16.601187: step 2069, loss 0.0235823, acc 1
2016-09-07T21:37:17.285156: step 2070, loss 0.0904927, acc 0.94
2016-09-07T21:37:17.967048: step 2071, loss 0.0574763, acc 0.96
2016-09-07T21:37:18.649026: step 2072, loss 0.114041, acc 0.94
2016-09-07T21:37:19.319744: step 2073, loss 0.0233944, acc 1
2016-09-07T21:37:19.982000: step 2074, loss 0.043635, acc 0.98
2016-09-07T21:37:20.642395: step 2075, loss 0.0439988, acc 0.98
2016-09-07T21:37:21.314813: step 2076, loss 0.0446205, acc 0.98
2016-09-07T21:37:21.993251: step 2077, loss 0.100132, acc 0.96
2016-09-07T21:37:22.671208: step 2078, loss 0.0226785, acc 1
2016-09-07T21:37:23.335160: step 2079, loss 0.0432009, acc 0.98
2016-09-07T21:37:24.017019: step 2080, loss 0.0279058, acc 0.98
2016-09-07T21:37:24.687385: step 2081, loss 0.0927591, acc 0.96
2016-09-07T21:37:25.368434: step 2082, loss 0.0284997, acc 1
2016-09-07T21:37:26.042352: step 2083, loss 0.0499246, acc 0.98
2016-09-07T21:37:26.722437: step 2084, loss 0.0411358, acc 0.98
2016-09-07T21:37:27.403106: step 2085, loss 0.0609068, acc 0.98
2016-09-07T21:37:28.076972: step 2086, loss 0.0409437, acc 0.98
2016-09-07T21:37:28.767244: step 2087, loss 0.0652702, acc 0.98
2016-09-07T21:37:29.454345: step 2088, loss 0.128841, acc 0.94
2016-09-07T21:37:30.131002: step 2089, loss 0.123393, acc 0.96
2016-09-07T21:37:30.790037: step 2090, loss 0.0119512, acc 1
2016-09-07T21:37:31.453420: step 2091, loss 0.176987, acc 0.92
2016-09-07T21:37:32.132606: step 2092, loss 0.0513178, acc 0.98
2016-09-07T21:37:32.810132: step 2093, loss 0.111896, acc 0.98
2016-09-07T21:37:33.489577: step 2094, loss 0.120522, acc 0.94
2016-09-07T21:37:34.173128: step 2095, loss 0.0849788, acc 0.96
2016-09-07T21:37:34.850259: step 2096, loss 0.0440122, acc 0.98
2016-09-07T21:37:35.515632: step 2097, loss 0.0173802, acc 1
2016-09-07T21:37:36.187490: step 2098, loss 0.0763561, acc 0.98
2016-09-07T21:37:36.859718: step 2099, loss 0.137126, acc 0.96
2016-09-07T21:37:37.528083: step 2100, loss 0.0480301, acc 0.96

Evaluation:
2016-09-07T21:37:40.842326: step 2100, loss 1.20999, acc 0.742

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-2100

2016-09-07T21:37:42.638462: step 2101, loss 0.0380124, acc 0.98
2016-09-07T21:37:43.317877: step 2102, loss 0.0317824, acc 1
2016-09-07T21:37:43.993373: step 2103, loss 0.011073, acc 1
2016-09-07T21:37:44.674028: step 2104, loss 0.0896011, acc 0.96
2016-09-07T21:37:45.368702: step 2105, loss 0.0555976, acc 0.98
2016-09-07T21:37:46.045980: step 2106, loss 0.0781569, acc 0.98
2016-09-07T21:37:46.709807: step 2107, loss 0.0379879, acc 0.98
2016-09-07T21:37:47.384297: step 2108, loss 0.0231815, acc 1
2016-09-07T21:37:48.077779: step 2109, loss 0.0845078, acc 0.98
2016-09-07T21:37:48.746105: step 2110, loss 0.0889368, acc 0.94
2016-09-07T21:37:49.400196: step 2111, loss 0.116671, acc 0.92
2016-09-07T21:37:50.075271: step 2112, loss 0.182023, acc 0.96
2016-09-07T21:37:50.741739: step 2113, loss 0.101089, acc 0.96
2016-09-07T21:37:51.411456: step 2114, loss 0.0742406, acc 0.96
2016-09-07T21:37:52.093748: step 2115, loss 0.069164, acc 0.96
2016-09-07T21:37:52.776019: step 2116, loss 0.0428586, acc 1
2016-09-07T21:37:53.454464: step 2117, loss 0.106897, acc 0.96
2016-09-07T21:37:54.129288: step 2118, loss 0.012989, acc 1
2016-09-07T21:37:54.828783: step 2119, loss 0.0715428, acc 0.96
2016-09-07T21:37:55.502042: step 2120, loss 0.0749435, acc 0.98
2016-09-07T21:37:56.158039: step 2121, loss 0.119843, acc 0.96
2016-09-07T21:37:56.831329: step 2122, loss 0.0325689, acc 1
2016-09-07T21:37:57.504364: step 2123, loss 0.0617435, acc 0.98
2016-09-07T21:37:58.172829: step 2124, loss 0.0204809, acc 1
2016-09-07T21:37:58.843526: step 2125, loss 0.0179954, acc 1
2016-09-07T21:37:59.512058: step 2126, loss 0.032857, acc 0.98
2016-09-07T21:38:00.198912: step 2127, loss 0.0841103, acc 0.96
2016-09-07T21:38:00.911919: step 2128, loss 0.0382657, acc 0.98
2016-09-07T21:38:01.584056: step 2129, loss 0.0321489, acc 1
2016-09-07T21:38:02.242106: step 2130, loss 0.0680316, acc 0.96
2016-09-07T21:38:02.928964: step 2131, loss 0.0457541, acc 0.98
2016-09-07T21:38:03.609054: step 2132, loss 0.0974071, acc 0.98
2016-09-07T21:38:04.297402: step 2133, loss 0.0303674, acc 1
2016-09-07T21:38:04.659032: step 2134, loss 0.00713673, acc 1
2016-09-07T21:38:05.315124: step 2135, loss 0.0317527, acc 0.98
2016-09-07T21:38:05.993916: step 2136, loss 0.0751947, acc 0.98
2016-09-07T21:38:06.651095: step 2137, loss 0.0447732, acc 0.98
2016-09-07T21:38:07.340406: step 2138, loss 0.0549244, acc 0.98
2016-09-07T21:38:08.012380: step 2139, loss 0.0849423, acc 0.98
2016-09-07T21:38:08.675191: step 2140, loss 0.029094, acc 1
2016-09-07T21:38:09.369763: step 2141, loss 0.022981, acc 0.98
2016-09-07T21:38:10.036927: step 2142, loss 0.0195305, acc 1
2016-09-07T21:38:10.704777: step 2143, loss 0.0213388, acc 1
2016-09-07T21:38:11.376465: step 2144, loss 0.0294374, acc 0.98
2016-09-07T21:38:12.047159: step 2145, loss 0.0858889, acc 0.98
2016-09-07T21:38:12.718192: step 2146, loss 0.073873, acc 0.96
2016-09-07T21:38:13.373826: step 2147, loss 0.02345, acc 1
2016-09-07T21:38:14.042834: step 2148, loss 0.121507, acc 0.96
2016-09-07T21:38:14.702603: step 2149, loss 0.0359697, acc 0.98
2016-09-07T21:38:15.392747: step 2150, loss 0.0629872, acc 0.96
2016-09-07T21:38:16.065617: step 2151, loss 0.0871608, acc 0.96
2016-09-07T21:38:16.737262: step 2152, loss 0.0152591, acc 1
2016-09-07T21:38:17.414544: step 2153, loss 0.0806441, acc 0.98
2016-09-07T21:38:18.108993: step 2154, loss 0.0376371, acc 0.98
2016-09-07T21:38:18.782402: step 2155, loss 0.0794666, acc 0.98
2016-09-07T21:38:19.441212: step 2156, loss 0.0428374, acc 0.96
2016-09-07T21:38:20.106704: step 2157, loss 0.0819698, acc 0.96
2016-09-07T21:38:20.787173: step 2158, loss 0.0913487, acc 0.96
2016-09-07T21:38:21.448250: step 2159, loss 0.0368739, acc 0.98
2016-09-07T21:38:22.121213: step 2160, loss 0.0325815, acc 0.98
2016-09-07T21:38:22.771423: step 2161, loss 0.0298867, acc 0.98
2016-09-07T21:38:23.444967: step 2162, loss 0.0229944, acc 1
2016-09-07T21:38:24.101557: step 2163, loss 0.0413065, acc 0.98
2016-09-07T21:38:24.774665: step 2164, loss 0.0279278, acc 1
2016-09-07T21:38:25.446584: step 2165, loss 0.162055, acc 0.92
2016-09-07T21:38:26.103558: step 2166, loss 0.0625592, acc 0.98
2016-09-07T21:38:26.805828: step 2167, loss 0.0259899, acc 1
2016-09-07T21:38:27.479145: step 2168, loss 0.0738154, acc 0.96
2016-09-07T21:38:28.163838: step 2169, loss 0.0459707, acc 0.96
2016-09-07T21:38:28.851466: step 2170, loss 0.0194481, acc 1
2016-09-07T21:38:29.521894: step 2171, loss 0.0231136, acc 0.98
2016-09-07T21:38:30.201387: step 2172, loss 0.0619129, acc 0.96
2016-09-07T21:38:30.872110: step 2173, loss 0.0355264, acc 0.98
2016-09-07T21:38:31.552829: step 2174, loss 0.0340957, acc 0.98
2016-09-07T21:38:32.215132: step 2175, loss 0.0888272, acc 0.98
2016-09-07T21:38:32.897482: step 2176, loss 0.0580361, acc 0.98
2016-09-07T21:38:33.560665: step 2177, loss 0.024721, acc 1
2016-09-07T21:38:34.229933: step 2178, loss 0.0291195, acc 1
2016-09-07T21:38:34.902607: step 2179, loss 0.0706835, acc 0.96
2016-09-07T21:38:35.583532: step 2180, loss 0.0747438, acc 0.96
2016-09-07T21:38:36.228918: step 2181, loss 0.0488134, acc 0.98
2016-09-07T21:38:36.892787: step 2182, loss 0.0959472, acc 0.96
2016-09-07T21:38:37.563864: step 2183, loss 0.0265812, acc 0.98
2016-09-07T21:38:38.254877: step 2184, loss 0.0927261, acc 0.98
2016-09-07T21:38:38.938019: step 2185, loss 0.105465, acc 0.96
2016-09-07T21:38:39.615681: step 2186, loss 0.032059, acc 0.98
2016-09-07T21:38:40.276992: step 2187, loss 0.00972974, acc 1
2016-09-07T21:38:40.942767: step 2188, loss 0.0720494, acc 0.98
2016-09-07T21:38:41.632234: step 2189, loss 0.102284, acc 0.96
2016-09-07T21:38:42.315814: step 2190, loss 0.0728717, acc 0.98
2016-09-07T21:38:42.975484: step 2191, loss 0.0381316, acc 1
2016-09-07T21:38:43.645315: step 2192, loss 0.037587, acc 0.98
2016-09-07T21:38:44.307806: step 2193, loss 0.0271266, acc 1
2016-09-07T21:38:44.969053: step 2194, loss 0.0504643, acc 0.96
2016-09-07T21:38:45.630919: step 2195, loss 0.0577704, acc 0.96
2016-09-07T21:38:46.290724: step 2196, loss 0.0561431, acc 0.96
2016-09-07T21:38:46.952585: step 2197, loss 0.100929, acc 0.92
2016-09-07T21:38:47.644396: step 2198, loss 0.0584333, acc 0.98
2016-09-07T21:38:48.323342: step 2199, loss 0.0267709, acc 1
2016-09-07T21:38:49.015530: step 2200, loss 0.0536025, acc 0.98

Evaluation:
2016-09-07T21:38:52.303579: step 2200, loss 1.27131, acc 0.765

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-2200

2016-09-07T21:38:54.033575: step 2201, loss 0.0347216, acc 1
2016-09-07T21:38:54.698356: step 2202, loss 0.0557385, acc 0.98
2016-09-07T21:38:55.367362: step 2203, loss 0.025745, acc 1
2016-09-07T21:38:56.052364: step 2204, loss 0.0528516, acc 1
2016-09-07T21:38:56.708459: step 2205, loss 0.0237212, acc 1
2016-09-07T21:38:57.379380: step 2206, loss 0.0216323, acc 1
2016-09-07T21:38:58.066675: step 2207, loss 0.0359562, acc 0.98
2016-09-07T21:38:58.754014: step 2208, loss 0.0250609, acc 1
2016-09-07T21:38:59.420161: step 2209, loss 0.104104, acc 0.96
2016-09-07T21:39:00.086822: step 2210, loss 0.041221, acc 0.98
2016-09-07T21:39:00.794834: step 2211, loss 0.107214, acc 0.96
2016-09-07T21:39:01.485789: step 2212, loss 0.0549235, acc 0.98
2016-09-07T21:39:02.157307: step 2213, loss 0.0583798, acc 0.98
2016-09-07T21:39:02.830059: step 2214, loss 0.0440583, acc 0.98
2016-09-07T21:39:03.526505: step 2215, loss 0.0549428, acc 0.96
2016-09-07T21:39:04.215081: step 2216, loss 0.0919461, acc 0.98
2016-09-07T21:39:04.886167: step 2217, loss 0.0349489, acc 0.98
2016-09-07T21:39:05.554637: step 2218, loss 0.0426016, acc 0.98
2016-09-07T21:39:06.235873: step 2219, loss 0.0113196, acc 1
2016-09-07T21:39:06.911964: step 2220, loss 0.0339328, acc 0.98
2016-09-07T21:39:07.572094: step 2221, loss 0.0815724, acc 0.96
2016-09-07T21:39:08.262098: step 2222, loss 0.0544689, acc 0.98
2016-09-07T21:39:08.936661: step 2223, loss 0.19729, acc 0.92
2016-09-07T21:39:09.601222: step 2224, loss 0.0819465, acc 0.94
2016-09-07T21:39:10.267386: step 2225, loss 0.022819, acc 1
2016-09-07T21:39:10.959254: step 2226, loss 0.0470279, acc 0.98
2016-09-07T21:39:11.646615: step 2227, loss 0.0507466, acc 0.98
2016-09-07T21:39:12.320449: step 2228, loss 0.038134, acc 1
2016-09-07T21:39:13.024912: step 2229, loss 0.0365261, acc 1
2016-09-07T21:39:13.711021: step 2230, loss 0.0611741, acc 0.96
2016-09-07T21:39:14.386973: step 2231, loss 0.0489249, acc 0.98
2016-09-07T21:39:15.064645: step 2232, loss 0.0519175, acc 0.96
2016-09-07T21:39:15.719008: step 2233, loss 0.0212587, acc 1
2016-09-07T21:39:16.418992: step 2234, loss 0.0878419, acc 0.98
2016-09-07T21:39:17.102804: step 2235, loss 0.035963, acc 0.98
2016-09-07T21:39:17.780131: step 2236, loss 0.0410566, acc 0.96
2016-09-07T21:39:18.450756: step 2237, loss 0.0788146, acc 0.98
2016-09-07T21:39:19.131166: step 2238, loss 0.0481206, acc 1
2016-09-07T21:39:19.804650: step 2239, loss 0.0693054, acc 0.98
2016-09-07T21:39:20.478431: step 2240, loss 0.0510306, acc 0.98
2016-09-07T21:39:21.129700: step 2241, loss 0.0419616, acc 0.96
2016-09-07T21:39:21.794452: step 2242, loss 0.069234, acc 0.98
2016-09-07T21:39:22.501903: step 2243, loss 0.0680989, acc 0.96
2016-09-07T21:39:23.171160: step 2244, loss 0.179938, acc 0.94
2016-09-07T21:39:23.861975: step 2245, loss 0.0998093, acc 0.98
2016-09-07T21:39:24.550878: step 2246, loss 0.0203032, acc 1
2016-09-07T21:39:25.215846: step 2247, loss 0.113297, acc 0.96
2016-09-07T21:39:25.915575: step 2248, loss 0.066619, acc 0.98
2016-09-07T21:39:26.572499: step 2249, loss 0.0448713, acc 1
2016-09-07T21:39:27.227008: step 2250, loss 0.0687065, acc 0.96
2016-09-07T21:39:27.904418: step 2251, loss 0.0299525, acc 1
2016-09-07T21:39:28.571892: step 2252, loss 0.0978197, acc 0.98
2016-09-07T21:39:29.265231: step 2253, loss 0.0396895, acc 1
2016-09-07T21:39:29.933240: step 2254, loss 0.0989137, acc 0.96
2016-09-07T21:39:30.586774: step 2255, loss 0.0581556, acc 0.98
2016-09-07T21:39:31.261698: step 2256, loss 0.0414942, acc 0.98
2016-09-07T21:39:31.928436: step 2257, loss 0.023901, acc 1
2016-09-07T21:39:32.586080: step 2258, loss 0.0918427, acc 0.96
2016-09-07T21:39:33.263699: step 2259, loss 0.0795902, acc 0.96
2016-09-07T21:39:33.926512: step 2260, loss 0.0556777, acc 0.96
2016-09-07T21:39:34.589904: step 2261, loss 0.0380306, acc 0.98
2016-09-07T21:39:35.260499: step 2262, loss 0.0236405, acc 1
2016-09-07T21:39:35.935365: step 2263, loss 0.0257244, acc 1
2016-09-07T21:39:36.617518: step 2264, loss 0.099059, acc 0.98
2016-09-07T21:39:37.283027: step 2265, loss 0.0266118, acc 1
2016-09-07T21:39:37.955292: step 2266, loss 0.0504213, acc 0.98
2016-09-07T21:39:38.622136: step 2267, loss 0.0734629, acc 0.98
2016-09-07T21:39:39.269359: step 2268, loss 0.0581245, acc 0.96
2016-09-07T21:39:39.929520: step 2269, loss 0.0259886, acc 1
2016-09-07T21:39:40.613676: step 2270, loss 0.0573419, acc 0.98
2016-09-07T21:39:41.287077: step 2271, loss 0.102418, acc 0.96
2016-09-07T21:39:41.975617: step 2272, loss 0.0299158, acc 1
2016-09-07T21:39:42.684926: step 2273, loss 0.023782, acc 1
2016-09-07T21:39:43.376870: step 2274, loss 0.0604078, acc 0.98
2016-09-07T21:39:44.060982: step 2275, loss 0.0790356, acc 0.96
2016-09-07T21:39:44.731078: step 2276, loss 0.0708605, acc 0.98
2016-09-07T21:39:45.392576: step 2277, loss 0.0312936, acc 0.98
2016-09-07T21:39:46.078962: step 2278, loss 0.025193, acc 1
2016-09-07T21:39:46.753263: step 2279, loss 0.0615764, acc 0.98
2016-09-07T21:39:47.423680: step 2280, loss 0.0270446, acc 1
2016-09-07T21:39:48.118939: step 2281, loss 0.123441, acc 0.94
2016-09-07T21:39:48.790897: step 2282, loss 0.168771, acc 0.9
2016-09-07T21:39:49.492897: step 2283, loss 0.0948244, acc 0.96
2016-09-07T21:39:50.207368: step 2284, loss 0.019179, acc 1
2016-09-07T21:39:50.899615: step 2285, loss 0.0308895, acc 1
2016-09-07T21:39:51.578186: step 2286, loss 0.0685875, acc 0.98
2016-09-07T21:39:52.246200: step 2287, loss 0.0463584, acc 0.98
2016-09-07T21:39:52.922534: step 2288, loss 0.0457014, acc 0.96
2016-09-07T21:39:53.615706: step 2289, loss 0.078427, acc 0.96
2016-09-07T21:39:54.298397: step 2290, loss 0.0659845, acc 0.96
2016-09-07T21:39:54.951243: step 2291, loss 0.0355165, acc 0.98
2016-09-07T21:39:55.626530: step 2292, loss 0.0667894, acc 0.98
2016-09-07T21:39:56.318241: step 2293, loss 0.0179964, acc 1
2016-09-07T21:39:57.029724: step 2294, loss 0.0430257, acc 0.98
2016-09-07T21:39:57.722978: step 2295, loss 0.0219004, acc 1
2016-09-07T21:39:58.410865: step 2296, loss 0.0627138, acc 0.98
2016-09-07T21:39:59.067242: step 2297, loss 0.0789247, acc 0.94
2016-09-07T21:39:59.733903: step 2298, loss 0.11161, acc 0.98
2016-09-07T21:40:00.432303: step 2299, loss 0.0542267, acc 0.96
2016-09-07T21:40:01.085537: step 2300, loss 0.151998, acc 0.9

Evaluation:
2016-09-07T21:40:04.410888: step 2300, loss 1.35873, acc 0.75

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-2300

2016-09-07T21:40:06.072320: step 2301, loss 0.036002, acc 0.98
2016-09-07T21:40:06.741685: step 2302, loss 0.0544987, acc 0.96
2016-09-07T21:40:07.433692: step 2303, loss 0.0749661, acc 0.94
2016-09-07T21:40:08.099044: step 2304, loss 0.153117, acc 0.92
2016-09-07T21:40:08.779370: step 2305, loss 0.0389125, acc 0.98
2016-09-07T21:40:09.438685: step 2306, loss 0.209394, acc 0.98
2016-09-07T21:40:10.107120: step 2307, loss 0.0585319, acc 0.98
2016-09-07T21:40:10.768549: step 2308, loss 0.115357, acc 0.98
2016-09-07T21:40:11.422452: step 2309, loss 0.045249, acc 0.96
2016-09-07T21:40:12.070913: step 2310, loss 0.070693, acc 1
2016-09-07T21:40:12.724447: step 2311, loss 0.04685, acc 0.98
2016-09-07T21:40:13.392399: step 2312, loss 0.136622, acc 0.96
2016-09-07T21:40:14.072395: step 2313, loss 0.0474692, acc 0.98
2016-09-07T21:40:14.742448: step 2314, loss 0.0434197, acc 1
2016-09-07T21:40:15.421492: step 2315, loss 0.0793288, acc 0.96
2016-09-07T21:40:16.100448: step 2316, loss 0.0295211, acc 1
2016-09-07T21:40:16.791774: step 2317, loss 0.0236029, acc 1
2016-09-07T21:40:17.466808: step 2318, loss 0.0588904, acc 0.98
2016-09-07T21:40:18.137578: step 2319, loss 0.0720092, acc 0.96
2016-09-07T21:40:18.807161: step 2320, loss 0.0694605, acc 0.96
2016-09-07T21:40:19.492292: step 2321, loss 0.0403407, acc 0.98
2016-09-07T21:40:20.162814: step 2322, loss 0.0867568, acc 0.96
2016-09-07T21:40:20.836310: step 2323, loss 0.0551009, acc 0.98
2016-09-07T21:40:21.534494: step 2324, loss 0.0187329, acc 1
2016-09-07T21:40:22.214784: step 2325, loss 0.112153, acc 0.92
2016-09-07T21:40:22.875700: step 2326, loss 0.0310733, acc 1
2016-09-07T21:40:23.528768: step 2327, loss 0.0463419, acc 1
2016-09-07T21:40:23.903416: step 2328, loss 0.0319802, acc 1
2016-09-07T21:40:24.590389: step 2329, loss 0.037899, acc 1
2016-09-07T21:40:25.267594: step 2330, loss 0.0575246, acc 0.98
2016-09-07T21:40:25.954190: step 2331, loss 0.0964828, acc 0.94
2016-09-07T21:40:26.619177: step 2332, loss 0.0946421, acc 0.96
2016-09-07T21:40:27.290564: step 2333, loss 0.0762157, acc 0.96
2016-09-07T21:40:27.953242: step 2334, loss 0.0536711, acc 0.98
2016-09-07T21:40:28.619406: step 2335, loss 0.0464025, acc 0.98
2016-09-07T21:40:29.272332: step 2336, loss 0.0404911, acc 1
2016-09-07T21:40:29.944309: step 2337, loss 0.03498, acc 0.98
2016-09-07T21:40:30.622250: step 2338, loss 0.062851, acc 0.98
2016-09-07T21:40:31.299230: step 2339, loss 0.0750094, acc 0.98
2016-09-07T21:40:31.957807: step 2340, loss 0.0430709, acc 0.98
2016-09-07T21:40:32.623453: step 2341, loss 0.0245345, acc 1
2016-09-07T21:40:33.321338: step 2342, loss 0.0541082, acc 0.98
2016-09-07T21:40:33.994140: step 2343, loss 0.0576091, acc 0.98
2016-09-07T21:40:34.660413: step 2344, loss 0.0115056, acc 1
2016-09-07T21:40:35.320750: step 2345, loss 0.0682986, acc 0.98
2016-09-07T21:40:35.981393: step 2346, loss 0.123032, acc 0.96
2016-09-07T21:40:36.638530: step 2347, loss 0.0780934, acc 0.98
2016-09-07T21:40:37.314657: step 2348, loss 0.0104275, acc 1
2016-09-07T21:40:37.973861: step 2349, loss 0.0207638, acc 0.98
2016-09-07T21:40:38.650165: step 2350, loss 0.0873313, acc 0.94
2016-09-07T21:40:39.339287: step 2351, loss 0.0131705, acc 1
2016-09-07T21:40:40.023223: step 2352, loss 0.0226977, acc 1
2016-09-07T21:40:40.691153: step 2353, loss 0.0318661, acc 0.98
2016-09-07T21:40:41.361274: step 2354, loss 0.0293635, acc 0.98
2016-09-07T21:40:42.041266: step 2355, loss 0.0264111, acc 0.98
2016-09-07T21:40:42.752284: step 2356, loss 0.00801796, acc 1
2016-09-07T21:40:43.424856: step 2357, loss 0.0606953, acc 0.98
2016-09-07T21:40:44.110356: step 2358, loss 0.069062, acc 0.96
2016-09-07T21:40:44.775189: step 2359, loss 0.0267105, acc 1
2016-09-07T21:40:45.436978: step 2360, loss 0.0740364, acc 0.98
2016-09-07T21:40:46.108739: step 2361, loss 0.0884968, acc 0.96
2016-09-07T21:40:46.776922: step 2362, loss 0.112397, acc 0.92
2016-09-07T21:40:47.454601: step 2363, loss 0.0297025, acc 0.98
2016-09-07T21:40:48.141391: step 2364, loss 0.0574791, acc 1
2016-09-07T21:40:48.829491: step 2365, loss 0.0370392, acc 1
2016-09-07T21:40:49.484117: step 2366, loss 0.104267, acc 0.94
2016-09-07T21:40:50.159856: step 2367, loss 0.0384558, acc 1
2016-09-07T21:40:50.837930: step 2368, loss 0.0932916, acc 0.98
2016-09-07T21:40:51.513187: step 2369, loss 0.0866469, acc 0.96
2016-09-07T21:40:52.181464: step 2370, loss 0.17138, acc 0.94
2016-09-07T21:40:52.840534: step 2371, loss 0.0368745, acc 0.98
2016-09-07T21:40:53.518694: step 2372, loss 0.0567441, acc 0.96
2016-09-07T21:40:54.212965: step 2373, loss 0.0555335, acc 0.96
2016-09-07T21:40:54.888663: step 2374, loss 0.0304767, acc 0.98
2016-09-07T21:40:55.565024: step 2375, loss 0.0565559, acc 0.98
2016-09-07T21:40:56.236845: step 2376, loss 0.0718436, acc 0.96
2016-09-07T21:40:56.919003: step 2377, loss 0.0429181, acc 1
2016-09-07T21:40:57.601894: step 2378, loss 0.0528488, acc 0.98
2016-09-07T21:40:58.284915: step 2379, loss 0.0682521, acc 0.96
2016-09-07T21:40:58.953542: step 2380, loss 0.0480743, acc 0.96
2016-09-07T21:40:59.638502: step 2381, loss 0.0240535, acc 1
2016-09-07T21:41:00.349250: step 2382, loss 0.0112343, acc 1
2016-09-07T21:41:01.018536: step 2383, loss 0.0427468, acc 0.96
2016-09-07T21:41:01.706711: step 2384, loss 0.0308103, acc 1
2016-09-07T21:41:02.372747: step 2385, loss 0.0138378, acc 1
2016-09-07T21:41:03.034590: step 2386, loss 0.216683, acc 0.94
2016-09-07T21:41:03.710348: step 2387, loss 0.0365004, acc 0.98
2016-09-07T21:41:04.373620: step 2388, loss 0.046001, acc 0.98
2016-09-07T21:41:05.033900: step 2389, loss 0.0675477, acc 0.98
2016-09-07T21:41:05.709729: step 2390, loss 0.0469257, acc 0.98
2016-09-07T21:41:06.396697: step 2391, loss 0.0264247, acc 1
2016-09-07T21:41:07.068034: step 2392, loss 0.0524205, acc 1
2016-09-07T21:41:07.752738: step 2393, loss 0.0971368, acc 0.92
2016-09-07T21:41:08.438229: step 2394, loss 0.0440632, acc 0.98
2016-09-07T21:41:09.104538: step 2395, loss 0.0284553, acc 1
2016-09-07T21:41:09.758521: step 2396, loss 0.0227408, acc 1
2016-09-07T21:41:10.420740: step 2397, loss 0.0176599, acc 1
2016-09-07T21:41:11.129586: step 2398, loss 0.106203, acc 0.92
2016-09-07T21:41:11.799533: step 2399, loss 0.101424, acc 0.98
2016-09-07T21:41:12.473745: step 2400, loss 0.0720515, acc 0.96

Evaluation:
2016-09-07T21:41:15.772363: step 2400, loss 1.46556, acc 0.741

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-2400

2016-09-07T21:41:17.569695: step 2401, loss 0.0354318, acc 0.98
2016-09-07T21:41:18.258537: step 2402, loss 0.0278581, acc 0.98
2016-09-07T21:41:18.936030: step 2403, loss 0.136402, acc 0.94
2016-09-07T21:41:19.615732: step 2404, loss 0.14812, acc 0.92
2016-09-07T21:41:20.302358: step 2405, loss 0.134998, acc 0.96
2016-09-07T21:41:20.986497: step 2406, loss 0.0368866, acc 0.98
2016-09-07T21:41:21.644543: step 2407, loss 0.0780666, acc 0.96
2016-09-07T21:41:22.320424: step 2408, loss 0.0644995, acc 0.96
2016-09-07T21:41:23.000506: step 2409, loss 0.045051, acc 0.98
2016-09-07T21:41:23.671364: step 2410, loss 0.0120618, acc 1
2016-09-07T21:41:24.339710: step 2411, loss 0.0956334, acc 0.96
2016-09-07T21:41:25.006408: step 2412, loss 0.0629263, acc 1
2016-09-07T21:41:25.676025: step 2413, loss 0.0273186, acc 0.98
2016-09-07T21:41:26.336133: step 2414, loss 0.0745208, acc 0.96
2016-09-07T21:41:27.002975: step 2415, loss 0.0259576, acc 1
2016-09-07T21:41:27.680973: step 2416, loss 0.00765758, acc 1
2016-09-07T21:41:28.335396: step 2417, loss 0.0895376, acc 0.98
2016-09-07T21:41:29.023240: step 2418, loss 0.0891185, acc 0.94
2016-09-07T21:41:29.724802: step 2419, loss 0.112349, acc 0.96
2016-09-07T21:41:30.390360: step 2420, loss 0.0622557, acc 0.98
2016-09-07T21:41:31.090064: step 2421, loss 0.043263, acc 1
2016-09-07T21:41:31.763799: step 2422, loss 0.0372368, acc 0.98
2016-09-07T21:41:32.435550: step 2423, loss 0.0333006, acc 1
2016-09-07T21:41:33.113211: step 2424, loss 0.0621308, acc 0.98
2016-09-07T21:41:33.784436: step 2425, loss 0.0320139, acc 0.98
2016-09-07T21:41:34.456045: step 2426, loss 0.0617306, acc 0.98
2016-09-07T21:41:35.129158: step 2427, loss 0.0431263, acc 0.98
2016-09-07T21:41:35.816801: step 2428, loss 0.0804517, acc 0.98
2016-09-07T21:41:36.492073: step 2429, loss 0.0385888, acc 1
2016-09-07T21:41:37.192012: step 2430, loss 0.0695215, acc 0.96
2016-09-07T21:41:37.868569: step 2431, loss 0.0206087, acc 1
2016-09-07T21:41:38.542678: step 2432, loss 0.0388064, acc 0.98
2016-09-07T21:41:39.206322: step 2433, loss 0.0941116, acc 0.96
2016-09-07T21:41:39.886394: step 2434, loss 0.156924, acc 0.92
2016-09-07T21:41:40.545550: step 2435, loss 0.126766, acc 0.96
2016-09-07T21:41:41.220823: step 2436, loss 0.0305988, acc 1
2016-09-07T21:41:41.891737: step 2437, loss 0.0339553, acc 1
2016-09-07T21:41:42.562481: step 2438, loss 0.0750512, acc 0.94
2016-09-07T21:41:43.255103: step 2439, loss 0.0676929, acc 0.98
2016-09-07T21:41:43.930561: step 2440, loss 0.0460004, acc 0.98
2016-09-07T21:41:44.604342: step 2441, loss 0.121278, acc 0.98
2016-09-07T21:41:45.288163: step 2442, loss 0.121835, acc 0.92
2016-09-07T21:41:45.953405: step 2443, loss 0.0357813, acc 0.98
2016-09-07T21:41:46.643648: step 2444, loss 0.124735, acc 0.94
2016-09-07T21:41:47.327369: step 2445, loss 0.079504, acc 0.94
2016-09-07T21:41:48.014755: step 2446, loss 0.0376702, acc 0.98
2016-09-07T21:41:48.681141: step 2447, loss 0.102745, acc 0.96
2016-09-07T21:41:49.347199: step 2448, loss 0.0456492, acc 1
2016-09-07T21:41:50.029525: step 2449, loss 0.0531229, acc 0.98
2016-09-07T21:41:50.706040: step 2450, loss 0.080373, acc 0.96
2016-09-07T21:41:51.374540: step 2451, loss 0.00870402, acc 1
2016-09-07T21:41:52.059647: step 2452, loss 0.0413324, acc 1
2016-09-07T21:41:52.740622: step 2453, loss 0.0360036, acc 0.98
2016-09-07T21:41:53.429196: step 2454, loss 0.018449, acc 1
2016-09-07T21:41:54.109087: step 2455, loss 0.0418018, acc 0.98
2016-09-07T21:41:54.808566: step 2456, loss 0.015537, acc 1
2016-09-07T21:41:55.492124: step 2457, loss 0.197105, acc 0.94
2016-09-07T21:41:56.159138: step 2458, loss 0.0328554, acc 0.98
2016-09-07T21:41:56.818265: step 2459, loss 0.0135423, acc 1
2016-09-07T21:41:57.496528: step 2460, loss 0.0554559, acc 0.96
2016-09-07T21:41:58.163437: step 2461, loss 0.0296035, acc 0.98
2016-09-07T21:41:58.844315: step 2462, loss 0.0623428, acc 0.96
2016-09-07T21:41:59.531490: step 2463, loss 0.0517613, acc 0.98
2016-09-07T21:42:00.224652: step 2464, loss 0.0292941, acc 0.98
2016-09-07T21:42:00.892815: step 2465, loss 0.0394738, acc 1
2016-09-07T21:42:01.566565: step 2466, loss 0.068311, acc 0.98
2016-09-07T21:42:02.238059: step 2467, loss 0.203873, acc 0.98
2016-09-07T21:42:02.928021: step 2468, loss 0.0514363, acc 0.98
2016-09-07T21:42:03.621247: step 2469, loss 0.0680166, acc 0.96
2016-09-07T21:42:04.308733: step 2470, loss 0.0480143, acc 0.98
2016-09-07T21:42:04.986113: step 2471, loss 0.0200132, acc 1
2016-09-07T21:42:05.678301: step 2472, loss 0.0386155, acc 0.98
2016-09-07T21:42:06.349541: step 2473, loss 0.0965605, acc 0.96
2016-09-07T21:42:07.026238: step 2474, loss 0.105649, acc 0.96
2016-09-07T21:42:07.706095: step 2475, loss 0.065137, acc 0.96
2016-09-07T21:42:08.382324: step 2476, loss 0.0274307, acc 1
2016-09-07T21:42:09.037685: step 2477, loss 0.0302423, acc 1
2016-09-07T21:42:09.709234: step 2478, loss 0.0635492, acc 0.96
2016-09-07T21:42:10.388027: step 2479, loss 0.0579575, acc 0.98
2016-09-07T21:42:11.056718: step 2480, loss 0.147591, acc 0.94
2016-09-07T21:42:11.722875: step 2481, loss 0.0748674, acc 0.94
2016-09-07T21:42:12.398459: step 2482, loss 0.116845, acc 0.96
2016-09-07T21:42:13.069221: step 2483, loss 0.180601, acc 0.96
2016-09-07T21:42:13.789638: step 2484, loss 0.0103651, acc 1
2016-09-07T21:42:14.456906: step 2485, loss 0.0121608, acc 1
2016-09-07T21:42:15.128569: step 2486, loss 0.131026, acc 0.96
2016-09-07T21:42:15.810419: step 2487, loss 0.0355401, acc 0.98
2016-09-07T21:42:16.493012: step 2488, loss 0.0741841, acc 0.96
2016-09-07T21:42:17.170195: step 2489, loss 0.116284, acc 0.96
2016-09-07T21:42:17.838956: step 2490, loss 0.0373453, acc 1
2016-09-07T21:42:18.519889: step 2491, loss 0.0480448, acc 0.98
2016-09-07T21:42:19.195977: step 2492, loss 0.101195, acc 0.94
2016-09-07T21:42:19.869983: step 2493, loss 0.0502444, acc 0.96
2016-09-07T21:42:20.535529: step 2494, loss 0.0372769, acc 0.98
2016-09-07T21:42:21.206983: step 2495, loss 0.0418162, acc 0.98
2016-09-07T21:42:21.879731: step 2496, loss 0.0331647, acc 1
2016-09-07T21:42:22.562955: step 2497, loss 0.0767843, acc 0.96
2016-09-07T21:42:23.235437: step 2498, loss 0.100712, acc 0.96
2016-09-07T21:42:23.909135: step 2499, loss 0.0450922, acc 0.98
2016-09-07T21:42:24.584318: step 2500, loss 0.0692084, acc 0.94

Evaluation:
2016-09-07T21:42:27.872025: step 2500, loss 1.13003, acc 0.742

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-2500

2016-09-07T21:42:29.615705: step 2501, loss 0.0358178, acc 1
2016-09-07T21:42:30.290817: step 2502, loss 0.0338956, acc 0.98
2016-09-07T21:42:30.958252: step 2503, loss 0.0667287, acc 0.96
2016-09-07T21:42:31.635728: step 2504, loss 0.0565557, acc 0.98
2016-09-07T21:42:32.310507: step 2505, loss 0.0425956, acc 1
2016-09-07T21:42:32.980990: step 2506, loss 0.0472956, acc 0.96
2016-09-07T21:42:33.648395: step 2507, loss 0.106448, acc 0.94
2016-09-07T21:42:34.336594: step 2508, loss 0.0748991, acc 0.96
2016-09-07T21:42:35.037461: step 2509, loss 0.0764712, acc 0.98
2016-09-07T21:42:35.697700: step 2510, loss 0.0527928, acc 0.98
2016-09-07T21:42:36.364275: step 2511, loss 0.0230279, acc 1
2016-09-07T21:42:37.068184: step 2512, loss 0.13496, acc 0.96
2016-09-07T21:42:37.797100: step 2513, loss 0.0870147, acc 0.96
2016-09-07T21:42:38.477029: step 2514, loss 0.0327906, acc 1
2016-09-07T21:42:39.150677: step 2515, loss 0.110047, acc 0.96
2016-09-07T21:42:39.828924: step 2516, loss 0.0430249, acc 0.98
2016-09-07T21:42:40.508191: step 2517, loss 0.0528476, acc 0.98
2016-09-07T21:42:41.169252: step 2518, loss 0.0219029, acc 1
2016-09-07T21:42:41.847418: step 2519, loss 0.068256, acc 0.96
2016-09-07T21:42:42.515280: step 2520, loss 0.0952007, acc 0.96
2016-09-07T21:42:43.180891: step 2521, loss 0.07371, acc 0.98
2016-09-07T21:42:43.554981: step 2522, loss 0.0582412, acc 1
2016-09-07T21:42:44.232678: step 2523, loss 0.0254166, acc 1
2016-09-07T21:42:44.891464: step 2524, loss 0.0453942, acc 0.98
2016-09-07T21:42:45.576930: step 2525, loss 0.0579954, acc 0.98
2016-09-07T21:42:46.258530: step 2526, loss 0.0364314, acc 0.98
2016-09-07T21:42:46.963184: step 2527, loss 0.0490506, acc 0.98
2016-09-07T21:42:47.638983: step 2528, loss 0.0986889, acc 0.96
2016-09-07T21:42:48.335195: step 2529, loss 0.0382021, acc 0.98
2016-09-07T21:42:49.017880: step 2530, loss 0.0381537, acc 0.98
2016-09-07T21:42:49.681851: step 2531, loss 0.0440099, acc 0.98
2016-09-07T21:42:50.341934: step 2532, loss 0.0245589, acc 1
2016-09-07T21:42:51.020688: step 2533, loss 0.0498065, acc 0.98
2016-09-07T21:42:51.701740: step 2534, loss 0.0183328, acc 1
2016-09-07T21:42:52.392624: step 2535, loss 0.117386, acc 0.96
2016-09-07T21:42:53.053938: step 2536, loss 0.146378, acc 0.94
2016-09-07T21:42:53.730440: step 2537, loss 0.0222411, acc 1
2016-09-07T21:42:54.390781: step 2538, loss 0.0437881, acc 0.98
2016-09-07T21:42:55.062626: step 2539, loss 0.0508759, acc 0.98
2016-09-07T21:42:55.742725: step 2540, loss 0.0108224, acc 1
2016-09-07T21:42:56.425269: step 2541, loss 0.0270005, acc 0.98
2016-09-07T21:42:57.134208: step 2542, loss 0.0301411, acc 1
2016-09-07T21:42:57.803468: step 2543, loss 0.0611743, acc 0.98
2016-09-07T21:42:58.494943: step 2544, loss 0.0635457, acc 0.96
2016-09-07T21:42:59.188340: step 2545, loss 0.0265425, acc 1
2016-09-07T21:42:59.909371: step 2546, loss 0.0274359, acc 0.98
2016-09-07T21:43:00.646995: step 2547, loss 0.0689702, acc 0.96
2016-09-07T21:43:01.314830: step 2548, loss 0.0187476, acc 1
2016-09-07T21:43:01.982534: step 2549, loss 0.030652, acc 1
2016-09-07T21:43:02.658897: step 2550, loss 0.0498797, acc 0.98
2016-09-07T21:43:03.330408: step 2551, loss 0.0570907, acc 0.98
2016-09-07T21:43:04.006338: step 2552, loss 0.0145726, acc 1
2016-09-07T21:43:04.682238: step 2553, loss 0.0355107, acc 0.98
2016-09-07T21:43:05.384316: step 2554, loss 0.0345761, acc 0.98
2016-09-07T21:43:06.083963: step 2555, loss 0.0265739, acc 1
2016-09-07T21:43:06.773897: step 2556, loss 0.0533219, acc 0.98
2016-09-07T21:43:07.434440: step 2557, loss 0.0137203, acc 1
2016-09-07T21:43:08.105453: step 2558, loss 0.123455, acc 0.96
2016-09-07T21:43:08.790650: step 2559, loss 0.0838384, acc 0.98
2016-09-07T21:43:09.471070: step 2560, loss 0.0771951, acc 0.96
2016-09-07T21:43:10.143734: step 2561, loss 0.0124937, acc 1
2016-09-07T21:43:10.835648: step 2562, loss 0.00653643, acc 1
2016-09-07T21:43:11.492816: step 2563, loss 0.134564, acc 0.96
2016-09-07T21:43:12.172872: step 2564, loss 0.015008, acc 1
2016-09-07T21:43:12.843485: step 2565, loss 0.109176, acc 0.98
2016-09-07T21:43:13.521275: step 2566, loss 0.0204906, acc 0.98
2016-09-07T21:43:14.186686: step 2567, loss 0.0389518, acc 0.98
2016-09-07T21:43:14.855147: step 2568, loss 0.0699247, acc 0.98
2016-09-07T21:43:15.536414: step 2569, loss 0.0585804, acc 0.98
2016-09-07T21:43:16.231967: step 2570, loss 0.0177739, acc 1
2016-09-07T21:43:16.894554: step 2571, loss 0.0301736, acc 1
2016-09-07T21:43:17.571722: step 2572, loss 0.0328568, acc 0.98
2016-09-07T21:43:18.245962: step 2573, loss 0.0533848, acc 0.96
2016-09-07T21:43:18.931666: step 2574, loss 0.0230938, acc 0.98
2016-09-07T21:43:19.604613: step 2575, loss 0.0284717, acc 0.98
2016-09-07T21:43:20.283081: step 2576, loss 0.00944605, acc 1
2016-09-07T21:43:20.966670: step 2577, loss 0.0127907, acc 1
2016-09-07T21:43:21.645834: step 2578, loss 0.0193486, acc 1
2016-09-07T21:43:22.325429: step 2579, loss 0.0740875, acc 0.98
2016-09-07T21:43:23.008166: step 2580, loss 0.0272924, acc 0.98
2016-09-07T21:43:23.673672: step 2581, loss 0.0268875, acc 1
2016-09-07T21:43:24.339341: step 2582, loss 0.040215, acc 1
2016-09-07T21:43:25.002538: step 2583, loss 0.0477458, acc 0.98
2016-09-07T21:43:25.694087: step 2584, loss 0.0385254, acc 0.96
2016-09-07T21:43:26.384859: step 2585, loss 0.0738982, acc 0.96
2016-09-07T21:43:27.054565: step 2586, loss 0.0597976, acc 1
2016-09-07T21:43:27.728108: step 2587, loss 0.0564353, acc 0.98
2016-09-07T21:43:28.401674: step 2588, loss 0.0482472, acc 0.98
2016-09-07T21:43:29.065618: step 2589, loss 0.0576495, acc 0.96
2016-09-07T21:43:29.739171: step 2590, loss 0.0060101, acc 1
2016-09-07T21:43:30.416130: step 2591, loss 0.0943869, acc 0.94
2016-09-07T21:43:31.085139: step 2592, loss 0.0567981, acc 0.98
2016-09-07T21:43:31.746072: step 2593, loss 0.0896588, acc 0.98
2016-09-07T21:43:32.420456: step 2594, loss 0.0628401, acc 0.96
2016-09-07T21:43:33.110300: step 2595, loss 0.0117614, acc 1
2016-09-07T21:43:33.791893: step 2596, loss 0.0307309, acc 0.98
2016-09-07T21:43:34.473629: step 2597, loss 0.0348596, acc 1
2016-09-07T21:43:35.160275: step 2598, loss 0.0817035, acc 0.94
2016-09-07T21:43:35.823876: step 2599, loss 0.0257897, acc 1
2016-09-07T21:43:36.498528: step 2600, loss 0.00882056, acc 1

Evaluation:
2016-09-07T21:43:39.763395: step 2600, loss 1.6286, acc 0.746

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-2600

2016-09-07T21:43:41.485566: step 2601, loss 0.0299235, acc 1
2016-09-07T21:43:42.162259: step 2602, loss 0.0773418, acc 0.96
2016-09-07T21:43:42.849847: step 2603, loss 0.174728, acc 0.96
2016-09-07T21:43:43.519121: step 2604, loss 0.0563756, acc 0.98
2016-09-07T21:43:44.191238: step 2605, loss 0.00906019, acc 1
2016-09-07T21:43:44.875039: step 2606, loss 0.0142013, acc 1
2016-09-07T21:43:45.553601: step 2607, loss 0.0894169, acc 0.96
2016-09-07T21:43:46.282682: step 2608, loss 0.0133275, acc 1
2016-09-07T21:43:46.971358: step 2609, loss 0.0657566, acc 0.94
2016-09-07T21:43:47.629908: step 2610, loss 0.0460102, acc 1
2016-09-07T21:43:48.308149: step 2611, loss 0.033835, acc 0.98
2016-09-07T21:43:48.992185: step 2612, loss 0.0462349, acc 0.98
2016-09-07T21:43:49.661333: step 2613, loss 0.029918, acc 1
2016-09-07T21:43:50.329663: step 2614, loss 0.0164137, acc 1
2016-09-07T21:43:51.012132: step 2615, loss 0.0371983, acc 0.98
2016-09-07T21:43:51.694503: step 2616, loss 0.0150978, acc 1
2016-09-07T21:43:52.370426: step 2617, loss 0.125538, acc 0.94
2016-09-07T21:43:53.050957: step 2618, loss 0.112857, acc 0.94
2016-09-07T21:43:53.736306: step 2619, loss 0.0422782, acc 0.98
2016-09-07T21:43:54.419543: step 2620, loss 0.0970981, acc 0.94
2016-09-07T21:43:55.122886: step 2621, loss 0.108299, acc 0.92
2016-09-07T21:43:55.814173: step 2622, loss 0.0472104, acc 0.98
2016-09-07T21:43:56.504990: step 2623, loss 0.0425484, acc 0.98
2016-09-07T21:43:57.166113: step 2624, loss 0.0397941, acc 0.98
2016-09-07T21:43:57.851776: step 2625, loss 0.172818, acc 0.98
2016-09-07T21:43:58.526382: step 2626, loss 0.0800937, acc 0.96
2016-09-07T21:43:59.202435: step 2627, loss 0.0657181, acc 0.98
2016-09-07T21:43:59.884211: step 2628, loss 0.0610451, acc 0.96
2016-09-07T21:44:00.588086: step 2629, loss 0.0337226, acc 1
2016-09-07T21:44:01.254665: step 2630, loss 0.0578695, acc 0.96
2016-09-07T21:44:01.920628: step 2631, loss 0.113528, acc 0.96
2016-09-07T21:44:02.604769: step 2632, loss 0.0491735, acc 1
2016-09-07T21:44:03.276827: step 2633, loss 0.170437, acc 0.88
2016-09-07T21:44:03.939475: step 2634, loss 0.028675, acc 0.98
2016-09-07T21:44:04.612746: step 2635, loss 0.0320569, acc 1
2016-09-07T21:44:05.276279: step 2636, loss 0.0270941, acc 1
2016-09-07T21:44:05.953878: step 2637, loss 0.0843452, acc 0.96
2016-09-07T21:44:06.620810: step 2638, loss 0.0844648, acc 0.96
2016-09-07T21:44:07.293817: step 2639, loss 0.0853512, acc 0.94
2016-09-07T21:44:07.970860: step 2640, loss 0.0524747, acc 1
2016-09-07T21:44:08.655699: step 2641, loss 0.0179677, acc 1
2016-09-07T21:44:09.344837: step 2642, loss 0.0643434, acc 0.96
2016-09-07T21:44:10.039268: step 2643, loss 0.0450789, acc 0.98
2016-09-07T21:44:10.702805: step 2644, loss 0.074939, acc 0.96
2016-09-07T21:44:11.367451: step 2645, loss 0.131835, acc 0.96
2016-09-07T21:44:12.041692: step 2646, loss 0.0169854, acc 1
2016-09-07T21:44:12.699315: step 2647, loss 0.0214878, acc 1
2016-09-07T21:44:13.373517: step 2648, loss 0.0394948, acc 0.98
2016-09-07T21:44:14.042495: step 2649, loss 0.0576016, acc 0.98
2016-09-07T21:44:14.716012: step 2650, loss 0.148056, acc 0.96
2016-09-07T21:44:15.400925: step 2651, loss 0.0178902, acc 1
2016-09-07T21:44:16.067285: step 2652, loss 0.0796932, acc 0.94
2016-09-07T21:44:16.726091: step 2653, loss 0.0656639, acc 0.96
2016-09-07T21:44:17.399658: step 2654, loss 0.0268435, acc 0.98
2016-09-07T21:44:18.070183: step 2655, loss 0.0564086, acc 0.98
2016-09-07T21:44:18.758469: step 2656, loss 0.0631607, acc 0.96
2016-09-07T21:44:19.428052: step 2657, loss 0.0531224, acc 0.96
2016-09-07T21:44:20.106402: step 2658, loss 0.0469311, acc 0.98
2016-09-07T21:44:20.775177: step 2659, loss 0.0579862, acc 0.96
2016-09-07T21:44:21.450064: step 2660, loss 0.00618848, acc 1
2016-09-07T21:44:22.121526: step 2661, loss 0.0659663, acc 0.96
2016-09-07T21:44:22.805360: step 2662, loss 0.0739657, acc 0.96
2016-09-07T21:44:23.471244: step 2663, loss 0.0316215, acc 0.98
2016-09-07T21:44:24.156452: step 2664, loss 0.0474208, acc 0.96
2016-09-07T21:44:24.812283: step 2665, loss 0.0874166, acc 0.96
2016-09-07T21:44:25.475817: step 2666, loss 0.0237629, acc 1
2016-09-07T21:44:26.152618: step 2667, loss 0.0355243, acc 1
2016-09-07T21:44:26.829880: step 2668, loss 0.0688302, acc 0.96
2016-09-07T21:44:27.503424: step 2669, loss 0.029113, acc 1
2016-09-07T21:44:28.154906: step 2670, loss 0.0549835, acc 0.98
2016-09-07T21:44:28.817087: step 2671, loss 0.106036, acc 0.98
2016-09-07T21:44:29.488720: step 2672, loss 0.0530734, acc 0.98
2016-09-07T21:44:30.189707: step 2673, loss 0.0567205, acc 0.98
2016-09-07T21:44:30.880162: step 2674, loss 0.0760639, acc 0.96
2016-09-07T21:44:31.552793: step 2675, loss 0.0514539, acc 0.96
2016-09-07T21:44:32.219154: step 2676, loss 0.0358836, acc 1
2016-09-07T21:44:32.911853: step 2677, loss 0.0535847, acc 1
2016-09-07T21:44:33.574659: step 2678, loss 0.0628627, acc 0.98
2016-09-07T21:44:34.260066: step 2679, loss 0.0220996, acc 1
2016-09-07T21:44:34.951057: step 2680, loss 0.00921265, acc 1
2016-09-07T21:44:35.631911: step 2681, loss 0.00636468, acc 1
2016-09-07T21:44:36.293457: step 2682, loss 0.0940921, acc 0.98
2016-09-07T21:44:36.967093: step 2683, loss 0.0458672, acc 0.98
2016-09-07T21:44:37.648473: step 2684, loss 0.0132929, acc 1
2016-09-07T21:44:38.330462: step 2685, loss 0.00742711, acc 1
2016-09-07T21:44:39.018845: step 2686, loss 0.0261648, acc 0.98
2016-09-07T21:44:39.713203: step 2687, loss 0.0992536, acc 0.98
2016-09-07T21:44:40.375881: step 2688, loss 0.0302764, acc 0.98
2016-09-07T21:44:41.059868: step 2689, loss 0.0140599, acc 1
2016-09-07T21:44:41.753219: step 2690, loss 0.0443318, acc 1
2016-09-07T21:44:42.434620: step 2691, loss 0.0187825, acc 1
2016-09-07T21:44:43.098435: step 2692, loss 0.0874455, acc 0.94
2016-09-07T21:44:43.764081: step 2693, loss 0.0576754, acc 0.98
2016-09-07T21:44:44.425345: step 2694, loss 0.0335151, acc 0.98
2016-09-07T21:44:45.099122: step 2695, loss 0.16708, acc 0.94
2016-09-07T21:44:45.762773: step 2696, loss 0.0822694, acc 0.98
2016-09-07T21:44:46.440506: step 2697, loss 0.101228, acc 0.96
2016-09-07T21:44:47.113847: step 2698, loss 0.142416, acc 0.96
2016-09-07T21:44:47.788389: step 2699, loss 0.0932804, acc 0.94
2016-09-07T21:44:48.464205: step 2700, loss 0.0313669, acc 1

Evaluation:
2016-09-07T21:44:51.774427: step 2700, loss 1.57348, acc 0.743

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-2700

2016-09-07T21:44:53.520581: step 2701, loss 0.0636916, acc 0.96
2016-09-07T21:44:54.218735: step 2702, loss 0.0575591, acc 0.96
2016-09-07T21:44:54.891018: step 2703, loss 0.0705973, acc 0.96
2016-09-07T21:44:55.561942: step 2704, loss 0.0180268, acc 1
2016-09-07T21:44:56.234203: step 2705, loss 0.133033, acc 0.92
2016-09-07T21:44:56.917140: step 2706, loss 0.0485796, acc 0.98
2016-09-07T21:44:57.612076: step 2707, loss 0.0208415, acc 1
2016-09-07T21:44:58.273062: step 2708, loss 0.0726647, acc 0.94
2016-09-07T21:44:58.944773: step 2709, loss 0.072433, acc 0.96
2016-09-07T21:44:59.619158: step 2710, loss 0.0115141, acc 1
2016-09-07T21:45:00.298479: step 2711, loss 0.0607201, acc 0.94
2016-09-07T21:45:00.974437: step 2712, loss 0.0518906, acc 0.96
2016-09-07T21:45:01.630715: step 2713, loss 0.0417962, acc 1
2016-09-07T21:45:02.311580: step 2714, loss 0.0507239, acc 0.96
2016-09-07T21:45:03.007399: step 2715, loss 0.0246678, acc 1
2016-09-07T21:45:03.362921: step 2716, loss 0.12449, acc 0.916667
2016-09-07T21:45:04.025676: step 2717, loss 0.0515311, acc 1
2016-09-07T21:45:04.692904: step 2718, loss 0.030193, acc 1
2016-09-07T21:45:05.358407: step 2719, loss 0.108469, acc 0.94
2016-09-07T21:45:06.026484: step 2720, loss 0.0363084, acc 0.98
2016-09-07T21:45:06.695398: step 2721, loss 0.0394776, acc 1
2016-09-07T21:45:07.401261: step 2722, loss 0.036211, acc 1
2016-09-07T21:45:08.063330: step 2723, loss 0.0359351, acc 0.98
2016-09-07T21:45:08.741604: step 2724, loss 0.027878, acc 1
2016-09-07T21:45:09.407961: step 2725, loss 0.0164476, acc 1
2016-09-07T21:45:10.090383: step 2726, loss 0.0138423, acc 1
2016-09-07T21:45:10.758625: step 2727, loss 0.0273723, acc 1
2016-09-07T21:45:11.404530: step 2728, loss 0.0433288, acc 1
2016-09-07T21:45:12.086180: step 2729, loss 0.0677554, acc 0.98
2016-09-07T21:45:12.777376: step 2730, loss 0.0132832, acc 1
2016-09-07T21:45:13.443895: step 2731, loss 0.0165978, acc 1
2016-09-07T21:45:14.144019: step 2732, loss 0.0107489, acc 1
2016-09-07T21:45:14.820336: step 2733, loss 0.0158915, acc 1
2016-09-07T21:45:15.495995: step 2734, loss 0.0639293, acc 0.96
2016-09-07T21:45:16.168564: step 2735, loss 0.0646733, acc 0.96
2016-09-07T21:45:16.865224: step 2736, loss 0.0244544, acc 1
2016-09-07T21:45:17.531262: step 2737, loss 0.0297747, acc 0.98
2016-09-07T21:45:18.223422: step 2738, loss 0.0285694, acc 1
2016-09-07T21:45:18.887874: step 2739, loss 0.0441062, acc 1
2016-09-07T21:45:19.558448: step 2740, loss 0.084827, acc 0.96
2016-09-07T21:45:20.237643: step 2741, loss 0.0168564, acc 1
2016-09-07T21:45:20.910442: step 2742, loss 0.041869, acc 0.98
2016-09-07T21:45:21.586240: step 2743, loss 0.0441807, acc 0.98
2016-09-07T21:45:22.242409: step 2744, loss 0.0620903, acc 0.98
2016-09-07T21:45:22.917539: step 2745, loss 0.0143021, acc 1
2016-09-07T21:45:23.592419: step 2746, loss 0.0110108, acc 1
2016-09-07T21:45:24.276708: step 2747, loss 0.0378225, acc 1
2016-09-07T21:45:24.964818: step 2748, loss 0.116956, acc 0.96
2016-09-07T21:45:25.660110: step 2749, loss 0.0508952, acc 0.98
2016-09-07T21:45:26.329368: step 2750, loss 0.0706119, acc 0.96
2016-09-07T21:45:27.002401: step 2751, loss 0.14724, acc 0.96
2016-09-07T21:45:27.677976: step 2752, loss 0.0904654, acc 0.98
2016-09-07T21:45:28.344892: step 2753, loss 0.0454184, acc 0.98
2016-09-07T21:45:28.996490: step 2754, loss 0.0463138, acc 0.98
2016-09-07T21:45:29.674800: step 2755, loss 0.168936, acc 0.96
2016-09-07T21:45:30.348310: step 2756, loss 0.00579015, acc 1
2016-09-07T21:45:31.020242: step 2757, loss 0.0629247, acc 0.98
2016-09-07T21:45:31.669977: step 2758, loss 0.071585, acc 0.98
2016-09-07T21:45:32.332575: step 2759, loss 0.0952881, acc 0.98
2016-09-07T21:45:33.005969: step 2760, loss 0.0616182, acc 0.98
2016-09-07T21:45:33.677356: step 2761, loss 0.0237405, acc 1
2016-09-07T21:45:34.343592: step 2762, loss 0.0900142, acc 0.96
2016-09-07T21:45:35.001602: step 2763, loss 0.0546089, acc 0.98
2016-09-07T21:45:35.667675: step 2764, loss 0.0597773, acc 0.96
2016-09-07T21:45:36.355525: step 2765, loss 0.0526155, acc 0.98
2016-09-07T21:45:37.012696: step 2766, loss 0.0515122, acc 0.98
2016-09-07T21:45:37.703977: step 2767, loss 0.0360216, acc 1
2016-09-07T21:45:38.381827: step 2768, loss 0.036076, acc 1
2016-09-07T21:45:39.036765: step 2769, loss 0.0149314, acc 1
2016-09-07T21:45:39.704202: step 2770, loss 0.0260055, acc 0.98
2016-09-07T21:45:40.349460: step 2771, loss 0.0322264, acc 1
2016-09-07T21:45:41.008724: step 2772, loss 0.0818192, acc 0.98
2016-09-07T21:45:41.683911: step 2773, loss 0.0581185, acc 0.96
2016-09-07T21:45:42.343909: step 2774, loss 0.00878766, acc 1
2016-09-07T21:45:43.018646: step 2775, loss 0.0253729, acc 1
2016-09-07T21:45:43.689042: step 2776, loss 0.0193682, acc 1
2016-09-07T21:45:44.365441: step 2777, loss 0.0412718, acc 1
2016-09-07T21:45:45.036516: step 2778, loss 0.0263996, acc 0.98
2016-09-07T21:45:45.700225: step 2779, loss 0.0582137, acc 0.98
2016-09-07T21:45:46.371442: step 2780, loss 0.128727, acc 0.96
2016-09-07T21:45:47.043303: step 2781, loss 0.0498919, acc 0.96
2016-09-07T21:45:47.719486: step 2782, loss 0.0419605, acc 0.98
2016-09-07T21:45:48.386383: step 2783, loss 0.0439277, acc 0.98
2016-09-07T21:45:49.067908: step 2784, loss 0.100638, acc 0.96
2016-09-07T21:45:49.747158: step 2785, loss 0.00949601, acc 1
2016-09-07T21:45:50.399163: step 2786, loss 0.0532786, acc 0.96
2016-09-07T21:45:51.070989: step 2787, loss 0.00802066, acc 1
2016-09-07T21:45:51.746876: step 2788, loss 0.02658, acc 0.98
2016-09-07T21:45:52.419381: step 2789, loss 0.0251061, acc 1
2016-09-07T21:45:53.088487: step 2790, loss 0.0311547, acc 0.98
2016-09-07T21:45:53.761000: step 2791, loss 0.0384178, acc 0.98
2016-09-07T21:45:54.442272: step 2792, loss 0.0599617, acc 0.96
2016-09-07T21:45:55.110158: step 2793, loss 0.0530551, acc 0.98
2016-09-07T21:45:55.787128: step 2794, loss 0.0127272, acc 1
2016-09-07T21:45:56.463523: step 2795, loss 0.0550249, acc 0.98
2016-09-07T21:45:57.128848: step 2796, loss 0.092827, acc 0.96
2016-09-07T21:45:57.801936: step 2797, loss 0.0267498, acc 1
2016-09-07T21:45:58.465578: step 2798, loss 0.0295377, acc 0.98
2016-09-07T21:45:59.128362: step 2799, loss 0.0930014, acc 0.92
2016-09-07T21:45:59.802136: step 2800, loss 0.0390016, acc 0.98

Evaluation:
2016-09-07T21:46:03.121242: step 2800, loss 1.7945, acc 0.733

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-2800

2016-09-07T21:46:04.969115: step 2801, loss 0.10826, acc 0.96
2016-09-07T21:46:05.632749: step 2802, loss 0.018398, acc 1
2016-09-07T21:46:06.304488: step 2803, loss 0.0337992, acc 0.98
2016-09-07T21:46:06.973874: step 2804, loss 0.0294025, acc 1
2016-09-07T21:46:07.636885: step 2805, loss 0.0687757, acc 0.94
2016-09-07T21:46:08.306038: step 2806, loss 0.0582887, acc 1
2016-09-07T21:46:08.969462: step 2807, loss 0.0419522, acc 0.96
2016-09-07T21:46:09.645706: step 2808, loss 0.0527435, acc 0.98
2016-09-07T21:46:10.321952: step 2809, loss 0.0258014, acc 0.98
2016-09-07T21:46:11.004355: step 2810, loss 0.0345991, acc 0.98
2016-09-07T21:46:11.680575: step 2811, loss 0.0300738, acc 0.98
2016-09-07T21:46:12.346169: step 2812, loss 0.015517, acc 1
2016-09-07T21:46:13.032815: step 2813, loss 0.0240321, acc 0.98
2016-09-07T21:46:13.690556: step 2814, loss 0.0319197, acc 1
2016-09-07T21:46:14.369709: step 2815, loss 0.0437769, acc 0.98
2016-09-07T21:46:15.052786: step 2816, loss 0.0674271, acc 0.96
2016-09-07T21:46:15.727174: step 2817, loss 0.0258391, acc 1
2016-09-07T21:46:16.403328: step 2818, loss 0.0798433, acc 0.96
2016-09-07T21:46:17.079483: step 2819, loss 0.0569006, acc 0.96
2016-09-07T21:46:17.760650: step 2820, loss 0.0202726, acc 1
2016-09-07T21:46:18.445672: step 2821, loss 0.0227571, acc 1
2016-09-07T21:46:19.101489: step 2822, loss 0.0543425, acc 0.98
2016-09-07T21:46:19.773216: step 2823, loss 0.0193107, acc 1
2016-09-07T21:46:20.434078: step 2824, loss 0.0234847, acc 0.98
2016-09-07T21:46:21.097566: step 2825, loss 0.0227857, acc 1
2016-09-07T21:46:21.772459: step 2826, loss 0.0211133, acc 1
2016-09-07T21:46:22.441470: step 2827, loss 0.0393569, acc 1
2016-09-07T21:46:23.129702: step 2828, loss 0.040579, acc 0.98
2016-09-07T21:46:23.785068: step 2829, loss 0.0745834, acc 0.96
2016-09-07T21:46:24.454078: step 2830, loss 0.0408203, acc 0.98
2016-09-07T21:46:25.139235: step 2831, loss 0.0764955, acc 0.96
2016-09-07T21:46:25.813006: step 2832, loss 0.0583292, acc 0.98
2016-09-07T21:46:26.480535: step 2833, loss 0.00829155, acc 1
2016-09-07T21:46:27.152271: step 2834, loss 0.0163518, acc 1
2016-09-07T21:46:27.807761: step 2835, loss 0.00732249, acc 1
2016-09-07T21:46:28.481668: step 2836, loss 0.113933, acc 0.94
2016-09-07T21:46:29.156791: step 2837, loss 0.035881, acc 1
2016-09-07T21:46:29.851027: step 2838, loss 0.0842232, acc 0.96
2016-09-07T21:46:30.521437: step 2839, loss 0.0102201, acc 1
2016-09-07T21:46:31.182692: step 2840, loss 0.0236162, acc 1
2016-09-07T21:46:31.852347: step 2841, loss 0.160277, acc 0.9
2016-09-07T21:46:32.520174: step 2842, loss 0.0337921, acc 0.98
2016-09-07T21:46:33.193857: step 2843, loss 0.0361976, acc 1
2016-09-07T21:46:33.894104: step 2844, loss 0.0132406, acc 1
2016-09-07T21:46:34.571399: step 2845, loss 0.0418523, acc 0.96
2016-09-07T21:46:35.254692: step 2846, loss 0.0196136, acc 1
2016-09-07T21:46:35.942048: step 2847, loss 0.0353933, acc 0.98
2016-09-07T21:46:36.620688: step 2848, loss 0.0460714, acc 0.98
2016-09-07T21:46:37.323120: step 2849, loss 0.00780676, acc 1
2016-09-07T21:46:37.996170: step 2850, loss 0.0342969, acc 0.98
2016-09-07T21:46:38.656652: step 2851, loss 0.0345267, acc 0.98
2016-09-07T21:46:39.338835: step 2852, loss 0.0444913, acc 0.98
2016-09-07T21:46:39.997359: step 2853, loss 0.00916987, acc 1
2016-09-07T21:46:40.674050: step 2854, loss 0.00916315, acc 1
2016-09-07T21:46:41.357073: step 2855, loss 0.0493292, acc 0.96
2016-09-07T21:46:42.036931: step 2856, loss 0.0197272, acc 1
2016-09-07T21:46:42.706457: step 2857, loss 0.110573, acc 0.98
2016-09-07T21:46:43.385412: step 2858, loss 0.0608062, acc 0.94
2016-09-07T21:46:44.061157: step 2859, loss 0.0472023, acc 0.98
2016-09-07T21:46:44.723272: step 2860, loss 0.0126272, acc 1
2016-09-07T21:46:45.401022: step 2861, loss 0.0250528, acc 1
2016-09-07T21:46:46.069303: step 2862, loss 0.0261901, acc 0.98
2016-09-07T21:46:46.771104: step 2863, loss 0.0697145, acc 0.98
2016-09-07T21:46:47.466316: step 2864, loss 0.0144554, acc 1
2016-09-07T21:46:48.145387: step 2865, loss 0.0260333, acc 1
2016-09-07T21:46:48.810101: step 2866, loss 0.00415223, acc 1
2016-09-07T21:46:49.470966: step 2867, loss 0.135689, acc 0.96
2016-09-07T21:46:50.133090: step 2868, loss 0.0894668, acc 0.94
2016-09-07T21:46:50.787043: step 2869, loss 0.0160779, acc 1
2016-09-07T21:46:51.438754: step 2870, loss 0.164124, acc 0.94
2016-09-07T21:46:52.102302: step 2871, loss 0.0193434, acc 1
2016-09-07T21:46:52.780603: step 2872, loss 0.00426534, acc 1
2016-09-07T21:46:53.454758: step 2873, loss 0.0367212, acc 0.98
2016-09-07T21:46:54.123448: step 2874, loss 0.0304887, acc 1
2016-09-07T21:46:54.793466: step 2875, loss 0.00723939, acc 1
2016-09-07T21:46:55.458724: step 2876, loss 0.0593088, acc 0.94
2016-09-07T21:46:56.134833: step 2877, loss 0.0370822, acc 1
2016-09-07T21:46:56.813017: step 2878, loss 0.0671699, acc 0.96
2016-09-07T21:46:57.488908: step 2879, loss 0.00468715, acc 1
2016-09-07T21:46:58.182096: step 2880, loss 0.063686, acc 0.94
2016-09-07T21:46:58.866587: step 2881, loss 0.0163671, acc 1
2016-09-07T21:46:59.529615: step 2882, loss 0.0360929, acc 1
2016-09-07T21:47:00.201108: step 2883, loss 0.0211766, acc 1
2016-09-07T21:47:00.903633: step 2884, loss 0.0190189, acc 1
2016-09-07T21:47:01.592535: step 2885, loss 0.0814291, acc 0.96
2016-09-07T21:47:02.281260: step 2886, loss 0.111118, acc 0.96
2016-09-07T21:47:02.955419: step 2887, loss 0.044665, acc 0.96
2016-09-07T21:47:03.621556: step 2888, loss 0.030352, acc 1
2016-09-07T21:47:04.305383: step 2889, loss 0.0334275, acc 0.96
2016-09-07T21:47:04.960256: step 2890, loss 0.0121318, acc 1
2016-09-07T21:47:05.624239: step 2891, loss 0.0265936, acc 0.98
2016-09-07T21:47:06.296089: step 2892, loss 0.0522165, acc 0.96
2016-09-07T21:47:06.966725: step 2893, loss 0.00978347, acc 1
2016-09-07T21:47:07.646804: step 2894, loss 0.00457685, acc 1
2016-09-07T21:47:08.302005: step 2895, loss 0.0913716, acc 0.94
2016-09-07T21:47:08.999314: step 2896, loss 0.0609966, acc 0.94
2016-09-07T21:47:09.662020: step 2897, loss 0.0971019, acc 0.96
2016-09-07T21:47:10.333822: step 2898, loss 0.022516, acc 1
2016-09-07T21:47:11.000865: step 2899, loss 0.00990242, acc 1
2016-09-07T21:47:11.676080: step 2900, loss 0.04356, acc 0.98

Evaluation:
2016-09-07T21:47:14.983186: step 2900, loss 1.68754, acc 0.742

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-2900

2016-09-07T21:47:16.795659: step 2901, loss 0.0216799, acc 0.98
2016-09-07T21:47:17.473574: step 2902, loss 0.0894627, acc 0.94
2016-09-07T21:47:18.151583: step 2903, loss 0.0780934, acc 0.98
2016-09-07T21:47:18.830036: step 2904, loss 0.0130256, acc 1
2016-09-07T21:47:19.504093: step 2905, loss 0.0326972, acc 1
2016-09-07T21:47:20.190546: step 2906, loss 0.0210748, acc 1
2016-09-07T21:47:20.847409: step 2907, loss 0.0468357, acc 0.98
2016-09-07T21:47:21.528892: step 2908, loss 0.0243051, acc 1
2016-09-07T21:47:22.187248: step 2909, loss 0.0125222, acc 1
2016-09-07T21:47:22.546870: step 2910, loss 0.206318, acc 0.916667
2016-09-07T21:47:23.235121: step 2911, loss 0.0295056, acc 0.98
2016-09-07T21:47:23.933113: step 2912, loss 0.0231317, acc 1
2016-09-07T21:47:24.608887: step 2913, loss 0.044416, acc 0.98
2016-09-07T21:47:25.285745: step 2914, loss 0.0306178, acc 1
2016-09-07T21:47:25.970952: step 2915, loss 0.0448905, acc 0.98
2016-09-07T21:47:26.651094: step 2916, loss 0.0435254, acc 0.96
2016-09-07T21:47:27.324801: step 2917, loss 0.0240526, acc 0.98
2016-09-07T21:47:27.998218: step 2918, loss 0.0641397, acc 0.94
2016-09-07T21:47:28.662230: step 2919, loss 0.0821324, acc 0.98
2016-09-07T21:47:29.353930: step 2920, loss 0.0559389, acc 0.98
2016-09-07T21:47:30.027519: step 2921, loss 0.0207948, acc 1
2016-09-07T21:47:30.699398: step 2922, loss 0.07963, acc 0.96
2016-09-07T21:47:31.375469: step 2923, loss 0.111706, acc 0.94
2016-09-07T21:47:32.052789: step 2924, loss 0.0591354, acc 0.96
2016-09-07T21:47:32.722035: step 2925, loss 0.0137823, acc 1
2016-09-07T21:47:33.417335: step 2926, loss 0.0105688, acc 1
2016-09-07T21:47:34.075206: step 2927, loss 0.16342, acc 0.96
2016-09-07T21:47:34.746344: step 2928, loss 0.021706, acc 1
2016-09-07T21:47:35.423525: step 2929, loss 0.105471, acc 0.96
2016-09-07T21:47:36.100375: step 2930, loss 0.0472479, acc 0.96
2016-09-07T21:47:36.783355: step 2931, loss 0.0241345, acc 0.98
2016-09-07T21:47:37.445439: step 2932, loss 0.0360395, acc 1
2016-09-07T21:47:38.127576: step 2933, loss 0.0277437, acc 1
2016-09-07T21:47:38.798671: step 2934, loss 0.0270202, acc 1
2016-09-07T21:47:39.481415: step 2935, loss 0.0255317, acc 1
2016-09-07T21:47:40.165806: step 2936, loss 0.00709016, acc 1
2016-09-07T21:47:40.837943: step 2937, loss 0.0431383, acc 1
2016-09-07T21:47:41.516770: step 2938, loss 0.0611881, acc 0.98
2016-09-07T21:47:42.187731: step 2939, loss 0.0489856, acc 0.98
2016-09-07T21:47:42.865156: step 2940, loss 0.0400416, acc 1
2016-09-07T21:47:43.521450: step 2941, loss 0.0293737, acc 0.98
2016-09-07T21:47:44.167806: step 2942, loss 0.0491346, acc 0.96
2016-09-07T21:47:44.859973: step 2943, loss 0.0292469, acc 0.98
2016-09-07T21:47:45.532948: step 2944, loss 0.0112486, acc 1
2016-09-07T21:47:46.202098: step 2945, loss 0.016228, acc 1
2016-09-07T21:47:46.863520: step 2946, loss 0.058789, acc 0.98
2016-09-07T21:47:47.536191: step 2947, loss 0.0239378, acc 1
2016-09-07T21:47:48.212691: step 2948, loss 0.107041, acc 0.98
2016-09-07T21:47:48.879770: step 2949, loss 0.019092, acc 1
2016-09-07T21:47:49.556700: step 2950, loss 0.0167495, acc 1
2016-09-07T21:47:50.243825: step 2951, loss 0.111461, acc 0.92
2016-09-07T21:47:50.899447: step 2952, loss 0.128121, acc 0.98
2016-09-07T21:47:51.562965: step 2953, loss 0.00997638, acc 1
2016-09-07T21:47:52.231453: step 2954, loss 0.0689424, acc 0.98
2016-09-07T21:47:52.900901: step 2955, loss 0.070186, acc 0.96
2016-09-07T21:47:53.558032: step 2956, loss 0.0371518, acc 0.98
2016-09-07T21:47:54.233672: step 2957, loss 0.0164858, acc 1
2016-09-07T21:47:54.910120: step 2958, loss 0.0670246, acc 0.98
2016-09-07T21:47:55.594456: step 2959, loss 0.0932338, acc 0.98
2016-09-07T21:47:56.283393: step 2960, loss 0.0202589, acc 0.98
2016-09-07T21:47:56.950047: step 2961, loss 0.0373984, acc 0.98
2016-09-07T21:47:57.629757: step 2962, loss 0.0105443, acc 1
2016-09-07T21:47:58.295926: step 2963, loss 0.01208, acc 1
2016-09-07T21:47:58.968188: step 2964, loss 0.0478563, acc 0.98
2016-09-07T21:47:59.665964: step 2965, loss 0.0160147, acc 1
2016-09-07T21:48:00.384252: step 2966, loss 0.056605, acc 0.96
2016-09-07T21:48:01.072497: step 2967, loss 0.0272179, acc 1
2016-09-07T21:48:01.755811: step 2968, loss 0.0175596, acc 1
2016-09-07T21:48:02.433910: step 2969, loss 0.0226768, acc 1
2016-09-07T21:48:03.091612: step 2970, loss 0.0827393, acc 0.96
2016-09-07T21:48:03.812434: step 2971, loss 0.116313, acc 0.96
2016-09-07T21:48:04.489063: step 2972, loss 0.0478408, acc 0.96
2016-09-07T21:48:05.159884: step 2973, loss 0.0344165, acc 1
2016-09-07T21:48:05.825604: step 2974, loss 0.0364283, acc 0.98
2016-09-07T21:48:06.515024: step 2975, loss 0.0611569, acc 0.98
2016-09-07T21:48:07.193375: step 2976, loss 0.131484, acc 0.96
2016-09-07T21:48:07.889356: step 2977, loss 0.0253679, acc 1
2016-09-07T21:48:08.579705: step 2978, loss 0.0256898, acc 0.98
2016-09-07T21:48:09.261920: step 2979, loss 0.0792996, acc 0.96
2016-09-07T21:48:09.929916: step 2980, loss 0.0778336, acc 0.94
2016-09-07T21:48:10.613143: step 2981, loss 0.0226668, acc 0.98
2016-09-07T21:48:11.291591: step 2982, loss 0.0709522, acc 0.96
2016-09-07T21:48:11.973958: step 2983, loss 0.0390413, acc 0.98
2016-09-07T21:48:12.653109: step 2984, loss 0.0741526, acc 0.96
2016-09-07T21:48:13.314048: step 2985, loss 0.0429944, acc 0.98
2016-09-07T21:48:13.978356: step 2986, loss 0.0771423, acc 0.96
2016-09-07T21:48:14.654752: step 2987, loss 0.0294918, acc 0.98
2016-09-07T21:48:15.334227: step 2988, loss 0.0067399, acc 1
2016-09-07T21:48:16.003390: step 2989, loss 0.0832888, acc 0.98
2016-09-07T21:48:16.663057: step 2990, loss 0.0378223, acc 0.98
2016-09-07T21:48:17.348478: step 2991, loss 0.0206472, acc 1
2016-09-07T21:48:18.054941: step 2992, loss 0.0301735, acc 1
2016-09-07T21:48:18.716899: step 2993, loss 0.0240816, acc 0.98
2016-09-07T21:48:19.381283: step 2994, loss 0.018817, acc 1
2016-09-07T21:48:20.064525: step 2995, loss 0.00931926, acc 1
2016-09-07T21:48:20.730401: step 2996, loss 0.0254366, acc 0.98
2016-09-07T21:48:21.394097: step 2997, loss 0.0221253, acc 1
2016-09-07T21:48:22.062851: step 2998, loss 0.045406, acc 0.96
2016-09-07T21:48:22.754921: step 2999, loss 0.0439815, acc 0.98
2016-09-07T21:48:23.444699: step 3000, loss 0.0311912, acc 0.98

Evaluation:
2016-09-07T21:48:26.763740: step 3000, loss 1.61609, acc 0.75

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-3000

2016-09-07T21:48:28.444115: step 3001, loss 0.0207184, acc 1
2016-09-07T21:48:29.133758: step 3002, loss 0.0469511, acc 0.96
2016-09-07T21:48:29.829917: step 3003, loss 0.00767511, acc 1
2016-09-07T21:48:30.496389: step 3004, loss 0.0493229, acc 0.98
2016-09-07T21:48:31.181475: step 3005, loss 0.0193074, acc 1
2016-09-07T21:48:31.858718: step 3006, loss 0.0585446, acc 0.96
2016-09-07T21:48:32.529712: step 3007, loss 0.0290921, acc 1
2016-09-07T21:48:33.208631: step 3008, loss 0.0841242, acc 0.94
2016-09-07T21:48:33.880340: step 3009, loss 0.068158, acc 0.96
2016-09-07T21:48:34.551737: step 3010, loss 0.0290121, acc 1
2016-09-07T21:48:35.203581: step 3011, loss 0.0237929, acc 1
2016-09-07T21:48:35.872440: step 3012, loss 0.113061, acc 0.96
2016-09-07T21:48:36.550258: step 3013, loss 0.0246672, acc 0.98
2016-09-07T21:48:37.228301: step 3014, loss 0.0578522, acc 0.98
2016-09-07T21:48:37.901092: step 3015, loss 0.0306758, acc 1
2016-09-07T21:48:38.575402: step 3016, loss 0.0306605, acc 0.98
2016-09-07T21:48:39.282982: step 3017, loss 0.0388753, acc 0.98
2016-09-07T21:48:39.949035: step 3018, loss 0.0133409, acc 1
2016-09-07T21:48:40.634570: step 3019, loss 0.121915, acc 0.94
2016-09-07T21:48:41.305729: step 3020, loss 0.0349835, acc 0.98
2016-09-07T21:48:41.999113: step 3021, loss 0.0621986, acc 0.96
2016-09-07T21:48:42.673960: step 3022, loss 0.0479799, acc 0.96
2016-09-07T21:48:43.361467: step 3023, loss 0.00748589, acc 1
2016-09-07T21:48:44.045923: step 3024, loss 0.0773192, acc 0.98
2016-09-07T21:48:44.714565: step 3025, loss 0.0151525, acc 1
2016-09-07T21:48:45.382915: step 3026, loss 0.0331247, acc 1
2016-09-07T21:48:46.077347: step 3027, loss 0.148721, acc 0.96
2016-09-07T21:48:46.756845: step 3028, loss 0.0405208, acc 0.98
2016-09-07T21:48:47.409520: step 3029, loss 0.0217113, acc 1
2016-09-07T21:48:48.081287: step 3030, loss 0.0437769, acc 0.96
2016-09-07T21:48:48.758161: step 3031, loss 0.0298182, acc 0.98
2016-09-07T21:48:49.422304: step 3032, loss 0.0197729, acc 1
2016-09-07T21:48:50.099763: step 3033, loss 0.027912, acc 0.98
2016-09-07T21:48:50.785036: step 3034, loss 0.0241659, acc 0.98
2016-09-07T21:48:51.484196: step 3035, loss 0.0660701, acc 0.98
2016-09-07T21:48:52.175940: step 3036, loss 0.0432349, acc 0.98
2016-09-07T21:48:52.864737: step 3037, loss 0.0281379, acc 0.98
2016-09-07T21:48:53.539854: step 3038, loss 0.0134443, acc 1
2016-09-07T21:48:54.204391: step 3039, loss 0.0369409, acc 0.98
2016-09-07T21:48:54.890453: step 3040, loss 0.0765866, acc 0.96
2016-09-07T21:48:55.554765: step 3041, loss 0.0392821, acc 1
2016-09-07T21:48:56.225351: step 3042, loss 0.0357998, acc 0.98
2016-09-07T21:48:56.885140: step 3043, loss 0.0399982, acc 0.98
2016-09-07T21:48:57.545825: step 3044, loss 0.0242051, acc 1
2016-09-07T21:48:58.227783: step 3045, loss 0.0831528, acc 0.98
2016-09-07T21:48:58.904855: step 3046, loss 0.0309331, acc 0.98
2016-09-07T21:48:59.577053: step 3047, loss 0.0451361, acc 0.98
2016-09-07T21:49:00.267561: step 3048, loss 0.0126183, acc 1
2016-09-07T21:49:00.933712: step 3049, loss 0.0537837, acc 0.98
2016-09-07T21:49:01.605835: step 3050, loss 0.0488573, acc 0.98
2016-09-07T21:49:02.263862: step 3051, loss 0.0512066, acc 0.98
2016-09-07T21:49:02.942289: step 3052, loss 0.0325504, acc 0.98
2016-09-07T21:49:03.633490: step 3053, loss 0.0186769, acc 1
2016-09-07T21:49:04.361394: step 3054, loss 0.0962776, acc 0.96
2016-09-07T21:49:05.046251: step 3055, loss 0.0189247, acc 0.98
2016-09-07T21:49:05.722344: step 3056, loss 0.0103656, acc 1
2016-09-07T21:49:06.410226: step 3057, loss 0.020901, acc 1
2016-09-07T21:49:07.095148: step 3058, loss 0.0434177, acc 0.98
2016-09-07T21:49:07.792222: step 3059, loss 0.00788185, acc 1
2016-09-07T21:49:08.498092: step 3060, loss 0.0236093, acc 1
2016-09-07T21:49:09.168592: step 3061, loss 0.0566475, acc 0.96
2016-09-07T21:49:09.843682: step 3062, loss 0.0821495, acc 0.96
2016-09-07T21:49:10.522137: step 3063, loss 0.0697442, acc 0.94
2016-09-07T21:49:11.191239: step 3064, loss 0.0356067, acc 0.98
2016-09-07T21:49:11.861766: step 3065, loss 0.0221782, acc 1
2016-09-07T21:49:12.532566: step 3066, loss 0.0938665, acc 0.98
2016-09-07T21:49:13.207884: step 3067, loss 0.0214046, acc 1
2016-09-07T21:49:13.868282: step 3068, loss 0.0288075, acc 0.98
2016-09-07T21:49:14.523858: step 3069, loss 0.00851922, acc 1
2016-09-07T21:49:15.198425: step 3070, loss 0.0388051, acc 0.98
2016-09-07T21:49:15.870966: step 3071, loss 0.0295526, acc 1
2016-09-07T21:49:16.534632: step 3072, loss 0.0226816, acc 1
2016-09-07T21:49:17.194216: step 3073, loss 0.0122039, acc 1
2016-09-07T21:49:17.882948: step 3074, loss 0.0120953, acc 1
2016-09-07T21:49:18.565215: step 3075, loss 0.090437, acc 0.96
2016-09-07T21:49:19.241478: step 3076, loss 0.0741444, acc 0.94
2016-09-07T21:49:19.922115: step 3077, loss 0.00655611, acc 1
2016-09-07T21:49:20.598450: step 3078, loss 0.0363399, acc 0.98
2016-09-07T21:49:21.280699: step 3079, loss 0.025833, acc 1
2016-09-07T21:49:21.965544: step 3080, loss 0.0675648, acc 0.96
2016-09-07T21:49:22.666440: step 3081, loss 0.0429169, acc 0.98
2016-09-07T21:49:23.352847: step 3082, loss 0.0505606, acc 1
2016-09-07T21:49:24.026074: step 3083, loss 0.0277708, acc 0.98
2016-09-07T21:49:24.706242: step 3084, loss 0.0311251, acc 0.98
2016-09-07T21:49:25.385678: step 3085, loss 0.0635506, acc 0.98
2016-09-07T21:49:26.046237: step 3086, loss 0.00448374, acc 1
2016-09-07T21:49:26.701506: step 3087, loss 0.00716922, acc 1
2016-09-07T21:49:27.355217: step 3088, loss 0.0455812, acc 0.98
2016-09-07T21:49:28.006401: step 3089, loss 0.0592643, acc 0.96
2016-09-07T21:49:28.681044: step 3090, loss 0.0224212, acc 1
2016-09-07T21:49:29.342101: step 3091, loss 0.0219942, acc 1
2016-09-07T21:49:30.024590: step 3092, loss 0.00457405, acc 1
2016-09-07T21:49:30.705057: step 3093, loss 0.0103645, acc 1
2016-09-07T21:49:31.388920: step 3094, loss 0.0153233, acc 1
2016-09-07T21:49:32.052692: step 3095, loss 0.0125317, acc 1
2016-09-07T21:49:32.710235: step 3096, loss 0.0624304, acc 0.96
2016-09-07T21:49:33.369006: step 3097, loss 0.0285267, acc 1
2016-09-07T21:49:34.034665: step 3098, loss 0.00574726, acc 1
2016-09-07T21:49:34.697715: step 3099, loss 0.0177475, acc 1
2016-09-07T21:49:35.365585: step 3100, loss 0.032244, acc 1

Evaluation:
2016-09-07T21:49:38.647396: step 3100, loss 1.88843, acc 0.75

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-3100

2016-09-07T21:49:40.384615: step 3101, loss 0.125389, acc 0.92
2016-09-07T21:49:41.075375: step 3102, loss 0.0202109, acc 0.98
2016-09-07T21:49:41.735238: step 3103, loss 0.00467971, acc 1
2016-09-07T21:49:42.089784: step 3104, loss 0.0109942, acc 1
2016-09-07T21:49:42.762287: step 3105, loss 0.0515934, acc 0.94
2016-09-07T21:49:43.440709: step 3106, loss 0.00925585, acc 1
2016-09-07T21:49:44.114590: step 3107, loss 0.0249971, acc 1
2016-09-07T21:49:44.785275: step 3108, loss 0.0432723, acc 0.98
2016-09-07T21:49:45.468298: step 3109, loss 0.071426, acc 0.98
2016-09-07T21:49:46.149348: step 3110, loss 0.0473337, acc 0.98
2016-09-07T21:49:46.817919: step 3111, loss 0.0107407, acc 1
2016-09-07T21:49:47.518706: step 3112, loss 0.0581431, acc 0.96
2016-09-07T21:49:48.179841: step 3113, loss 0.0314344, acc 0.98
2016-09-07T21:49:48.851506: step 3114, loss 0.0339162, acc 0.98
2016-09-07T21:49:49.503050: step 3115, loss 0.00589756, acc 1
2016-09-07T21:49:50.183138: step 3116, loss 0.0195405, acc 1
2016-09-07T21:49:50.881810: step 3117, loss 0.0234032, acc 1
2016-09-07T21:49:51.556669: step 3118, loss 0.0372685, acc 0.98
2016-09-07T21:49:52.218115: step 3119, loss 0.291881, acc 0.96
2016-09-07T21:49:52.910540: step 3120, loss 0.0808342, acc 0.94
2016-09-07T21:49:53.600590: step 3121, loss 0.0466414, acc 0.96
2016-09-07T21:49:54.262975: step 3122, loss 0.0530942, acc 0.96
2016-09-07T21:49:54.936547: step 3123, loss 0.0442504, acc 0.96
2016-09-07T21:49:55.590825: step 3124, loss 0.00693363, acc 1
2016-09-07T21:49:56.255329: step 3125, loss 0.0636819, acc 0.98
2016-09-07T21:49:56.919264: step 3126, loss 0.0482136, acc 0.96
2016-09-07T21:49:57.592383: step 3127, loss 0.0485479, acc 0.98
2016-09-07T21:49:58.275087: step 3128, loss 0.0433963, acc 0.98
2016-09-07T21:49:58.962142: step 3129, loss 0.0342382, acc 1
2016-09-07T21:49:59.647208: step 3130, loss 0.0056075, acc 1
2016-09-07T21:50:00.350157: step 3131, loss 0.0309528, acc 0.98
2016-09-07T21:50:01.018650: step 3132, loss 0.0185061, acc 1
2016-09-07T21:50:01.673960: step 3133, loss 0.0197733, acc 1
2016-09-07T21:50:02.359524: step 3134, loss 0.0194529, acc 1
2016-09-07T21:50:03.027114: step 3135, loss 0.0594286, acc 0.98
2016-09-07T21:50:03.695081: step 3136, loss 0.0610525, acc 0.98
2016-09-07T21:50:04.370795: step 3137, loss 0.0378433, acc 0.98
2016-09-07T21:50:05.034996: step 3138, loss 0.0322466, acc 0.98
2016-09-07T21:50:05.699558: step 3139, loss 0.01784, acc 1
2016-09-07T21:50:06.369745: step 3140, loss 0.154526, acc 0.96
2016-09-07T21:50:07.043090: step 3141, loss 0.153575, acc 0.96
2016-09-07T21:50:07.714403: step 3142, loss 0.057751, acc 0.96
2016-09-07T21:50:08.397510: step 3143, loss 0.12785, acc 0.98
2016-09-07T21:50:09.049900: step 3144, loss 0.00370327, acc 1
2016-09-07T21:50:09.699362: step 3145, loss 0.0713876, acc 0.98
2016-09-07T21:50:10.380341: step 3146, loss 0.0344414, acc 0.98
2016-09-07T21:50:11.047842: step 3147, loss 0.0070949, acc 1
2016-09-07T21:50:11.723734: step 3148, loss 0.0381207, acc 1
2016-09-07T21:50:12.393079: step 3149, loss 0.125973, acc 0.98
2016-09-07T21:50:13.048796: step 3150, loss 0.0624251, acc 0.96
2016-09-07T21:50:13.725530: step 3151, loss 0.0514986, acc 0.98
2016-09-07T21:50:14.394428: step 3152, loss 0.0304953, acc 1
2016-09-07T21:50:15.043101: step 3153, loss 0.0137349, acc 1
2016-09-07T21:50:15.703987: step 3154, loss 0.0463952, acc 0.98
2016-09-07T21:50:16.378813: step 3155, loss 0.0176553, acc 1
2016-09-07T21:50:17.048979: step 3156, loss 0.0190278, acc 0.98
2016-09-07T21:50:17.729682: step 3157, loss 0.0116586, acc 1
2016-09-07T21:50:18.410891: step 3158, loss 0.0303695, acc 0.98
2016-09-07T21:50:19.083451: step 3159, loss 0.0305031, acc 0.98
2016-09-07T21:50:19.759036: step 3160, loss 0.0556701, acc 0.96
2016-09-07T21:50:20.424880: step 3161, loss 0.0263319, acc 1
2016-09-07T21:50:21.105423: step 3162, loss 0.0243465, acc 0.98
2016-09-07T21:50:21.784552: step 3163, loss 0.0208462, acc 1
2016-09-07T21:50:22.484262: step 3164, loss 0.0329946, acc 0.98
2016-09-07T21:50:23.168306: step 3165, loss 0.0183162, acc 1
2016-09-07T21:50:23.828041: step 3166, loss 0.0212432, acc 1
2016-09-07T21:50:24.490174: step 3167, loss 0.0898013, acc 0.96
2016-09-07T21:50:25.168271: step 3168, loss 0.0409739, acc 0.96
2016-09-07T21:50:25.844419: step 3169, loss 0.0952033, acc 0.96
2016-09-07T21:50:26.508424: step 3170, loss 0.0731482, acc 0.98
2016-09-07T21:50:27.175183: step 3171, loss 0.00584086, acc 1
2016-09-07T21:50:27.846581: step 3172, loss 0.00671026, acc 1
2016-09-07T21:50:28.523458: step 3173, loss 0.0996012, acc 0.96
2016-09-07T21:50:29.208831: step 3174, loss 0.0383777, acc 0.96
2016-09-07T21:50:29.870774: step 3175, loss 0.0103916, acc 1
2016-09-07T21:50:30.539961: step 3176, loss 0.0371449, acc 0.98
2016-09-07T21:50:31.221881: step 3177, loss 0.0129574, acc 1
2016-09-07T21:50:31.918160: step 3178, loss 0.00751248, acc 1
2016-09-07T21:50:32.586609: step 3179, loss 0.00908227, acc 1
2016-09-07T21:50:33.251896: step 3180, loss 0.0368747, acc 1
2016-09-07T21:50:33.929285: step 3181, loss 0.0144916, acc 1
2016-09-07T21:50:34.648769: step 3182, loss 0.00721347, acc 1
2016-09-07T21:50:35.332232: step 3183, loss 0.0309333, acc 0.98
2016-09-07T21:50:36.027751: step 3184, loss 0.0146352, acc 1
2016-09-07T21:50:36.720269: step 3185, loss 0.0560009, acc 0.98
2016-09-07T21:50:37.403130: step 3186, loss 0.129188, acc 0.96
2016-09-07T21:50:38.070835: step 3187, loss 0.0462115, acc 0.96
2016-09-07T21:50:38.734956: step 3188, loss 0.0635956, acc 0.96
2016-09-07T21:50:39.378065: step 3189, loss 0.0260829, acc 1
2016-09-07T21:50:40.061907: step 3190, loss 0.060375, acc 0.98
2016-09-07T21:50:40.740401: step 3191, loss 0.0324812, acc 1
2016-09-07T21:50:41.407397: step 3192, loss 0.031296, acc 0.98
2016-09-07T21:50:42.073936: step 3193, loss 0.00583652, acc 1
2016-09-07T21:50:42.734501: step 3194, loss 0.010807, acc 1
2016-09-07T21:50:43.407510: step 3195, loss 0.0160964, acc 1
2016-09-07T21:50:44.090451: step 3196, loss 0.185975, acc 0.92
2016-09-07T21:50:44.754550: step 3197, loss 0.0261578, acc 1
2016-09-07T21:50:45.402980: step 3198, loss 0.0100932, acc 1
2016-09-07T21:50:46.073356: step 3199, loss 0.0344253, acc 0.98
2016-09-07T21:50:46.741745: step 3200, loss 0.112828, acc 0.96

Evaluation:
2016-09-07T21:50:50.058055: step 3200, loss 2.10183, acc 0.728

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-3200

2016-09-07T21:50:51.804713: step 3201, loss 0.0167463, acc 1
2016-09-07T21:50:52.479957: step 3202, loss 0.140302, acc 0.9
2016-09-07T21:50:53.145554: step 3203, loss 0.00825638, acc 1
2016-09-07T21:50:53.850884: step 3204, loss 0.045621, acc 0.96
2016-09-07T21:50:54.544658: step 3205, loss 0.0331107, acc 0.98
2016-09-07T21:50:55.212661: step 3206, loss 0.0238303, acc 0.98
2016-09-07T21:50:55.878245: step 3207, loss 0.0542264, acc 0.98
2016-09-07T21:50:56.543711: step 3208, loss 0.0285758, acc 0.98
2016-09-07T21:50:57.216942: step 3209, loss 0.03226, acc 0.98
2016-09-07T21:50:57.889336: step 3210, loss 0.034212, acc 0.98
2016-09-07T21:50:58.548322: step 3211, loss 0.0467285, acc 0.96
2016-09-07T21:50:59.213613: step 3212, loss 0.0733362, acc 0.98
2016-09-07T21:50:59.877499: step 3213, loss 0.0352359, acc 1
2016-09-07T21:51:00.584227: step 3214, loss 0.0208972, acc 1
2016-09-07T21:51:01.278468: step 3215, loss 0.0815749, acc 0.98
2016-09-07T21:51:01.939819: step 3216, loss 0.0122944, acc 1
2016-09-07T21:51:02.616327: step 3217, loss 0.052911, acc 0.98
2016-09-07T21:51:03.301659: step 3218, loss 0.0390107, acc 0.98
2016-09-07T21:51:03.967788: step 3219, loss 0.0581283, acc 0.96
2016-09-07T21:51:04.639735: step 3220, loss 0.033894, acc 1
2016-09-07T21:51:05.309770: step 3221, loss 0.0680762, acc 0.96
2016-09-07T21:51:05.993479: step 3222, loss 0.02568, acc 1
2016-09-07T21:51:06.676241: step 3223, loss 0.0705559, acc 0.98
2016-09-07T21:51:07.349183: step 3224, loss 0.0401906, acc 0.98
2016-09-07T21:51:08.019069: step 3225, loss 0.150981, acc 0.94
2016-09-07T21:51:08.682227: step 3226, loss 0.0269871, acc 1
2016-09-07T21:51:09.362383: step 3227, loss 0.0316682, acc 1
2016-09-07T21:51:10.032433: step 3228, loss 0.0240167, acc 1
2016-09-07T21:51:10.712667: step 3229, loss 0.103271, acc 0.94
2016-09-07T21:51:11.382626: step 3230, loss 0.173636, acc 0.98
2016-09-07T21:51:12.053830: step 3231, loss 0.0957308, acc 0.96
2016-09-07T21:51:12.738760: step 3232, loss 0.0208419, acc 1
2016-09-07T21:51:13.425491: step 3233, loss 0.0216721, acc 1
2016-09-07T21:51:14.104899: step 3234, loss 0.163573, acc 0.96
2016-09-07T21:51:14.778015: step 3235, loss 0.0417678, acc 0.98
2016-09-07T21:51:15.458336: step 3236, loss 0.0474292, acc 0.98
2016-09-07T21:51:16.130249: step 3237, loss 0.0616203, acc 0.98
2016-09-07T21:51:16.816082: step 3238, loss 0.0521135, acc 1
2016-09-07T21:51:17.483627: step 3239, loss 0.038121, acc 0.98
2016-09-07T21:51:18.167657: step 3240, loss 0.196852, acc 0.96
2016-09-07T21:51:18.834454: step 3241, loss 0.0956021, acc 0.96
2016-09-07T21:51:19.525558: step 3242, loss 0.0117109, acc 1
2016-09-07T21:51:20.196042: step 3243, loss 0.0350124, acc 0.98
2016-09-07T21:51:20.852054: step 3244, loss 0.0805946, acc 0.96
2016-09-07T21:51:21.551541: step 3245, loss 0.0407113, acc 1
2016-09-07T21:51:22.226339: step 3246, loss 0.0229259, acc 1
2016-09-07T21:51:22.896303: step 3247, loss 0.0241965, acc 1
2016-09-07T21:51:23.565662: step 3248, loss 0.0727763, acc 0.98
2016-09-07T21:51:24.230821: step 3249, loss 0.0379032, acc 0.98
2016-09-07T21:51:24.900446: step 3250, loss 0.0442567, acc 0.98
2016-09-07T21:51:25.569730: step 3251, loss 0.0219606, acc 1
2016-09-07T21:51:26.257723: step 3252, loss 0.0108521, acc 1
2016-09-07T21:51:26.925347: step 3253, loss 0.113734, acc 0.96
2016-09-07T21:51:27.600116: step 3254, loss 0.010426, acc 1
2016-09-07T21:51:28.268660: step 3255, loss 0.0385474, acc 1
2016-09-07T21:51:28.933327: step 3256, loss 0.0320056, acc 0.98
2016-09-07T21:51:29.619571: step 3257, loss 0.0731026, acc 0.96
2016-09-07T21:51:30.292697: step 3258, loss 0.0657908, acc 0.98
2016-09-07T21:51:30.964745: step 3259, loss 0.0577386, acc 0.98
2016-09-07T21:51:31.645212: step 3260, loss 0.0229775, acc 0.98
2016-09-07T21:51:32.314546: step 3261, loss 0.138002, acc 0.98
2016-09-07T21:51:32.992902: step 3262, loss 0.0298602, acc 0.98
2016-09-07T21:51:33.667579: step 3263, loss 0.0569287, acc 0.98
2016-09-07T21:51:34.339913: step 3264, loss 0.0901611, acc 0.96
2016-09-07T21:51:35.016366: step 3265, loss 0.0345451, acc 1
2016-09-07T21:51:35.699318: step 3266, loss 0.0359206, acc 0.98
2016-09-07T21:51:36.381016: step 3267, loss 0.0574177, acc 0.98
2016-09-07T21:51:37.044305: step 3268, loss 0.020453, acc 1
2016-09-07T21:51:37.725083: step 3269, loss 0.0161625, acc 1
2016-09-07T21:51:38.400018: step 3270, loss 0.0319866, acc 0.98
2016-09-07T21:51:39.075925: step 3271, loss 0.0194344, acc 1
2016-09-07T21:51:39.759785: step 3272, loss 0.0192953, acc 1
2016-09-07T21:51:40.432187: step 3273, loss 0.033958, acc 0.98
2016-09-07T21:51:41.094454: step 3274, loss 0.0505964, acc 0.98
2016-09-07T21:51:41.768565: step 3275, loss 0.0401004, acc 0.98
2016-09-07T21:51:42.437285: step 3276, loss 0.0419781, acc 0.96
2016-09-07T21:51:43.104586: step 3277, loss 0.0297522, acc 1
2016-09-07T21:51:43.794477: step 3278, loss 0.0758462, acc 0.96
2016-09-07T21:51:44.481395: step 3279, loss 0.0722279, acc 0.96
2016-09-07T21:51:45.154833: step 3280, loss 0.0869551, acc 0.94
2016-09-07T21:51:45.806320: step 3281, loss 0.0797014, acc 0.98
2016-09-07T21:51:46.473779: step 3282, loss 0.033476, acc 1
2016-09-07T21:51:47.133594: step 3283, loss 0.0619901, acc 0.98
2016-09-07T21:51:47.812656: step 3284, loss 0.0522196, acc 0.96
2016-09-07T21:51:48.489522: step 3285, loss 0.0778176, acc 0.96
2016-09-07T21:51:49.162145: step 3286, loss 0.0903347, acc 0.94
2016-09-07T21:51:49.843991: step 3287, loss 0.198537, acc 0.96
2016-09-07T21:51:50.509561: step 3288, loss 0.0620512, acc 0.98
2016-09-07T21:51:51.173293: step 3289, loss 0.024744, acc 1
2016-09-07T21:51:51.856452: step 3290, loss 0.125081, acc 0.96
2016-09-07T21:51:52.525153: step 3291, loss 0.075444, acc 0.96
2016-09-07T21:51:53.191770: step 3292, loss 0.083171, acc 0.98
2016-09-07T21:51:53.862343: step 3293, loss 0.067383, acc 0.98
2016-09-07T21:51:54.527538: step 3294, loss 0.040897, acc 0.98
2016-09-07T21:51:55.204124: step 3295, loss 0.0699505, acc 0.96
2016-09-07T21:51:55.873701: step 3296, loss 0.0585941, acc 0.96
2016-09-07T21:51:56.546212: step 3297, loss 0.033068, acc 0.98
2016-09-07T21:51:56.888311: step 3298, loss 0.047014, acc 1
2016-09-07T21:51:57.561206: step 3299, loss 0.158366, acc 0.92
2016-09-07T21:51:58.240659: step 3300, loss 0.0233411, acc 1

Evaluation:
2016-09-07T21:52:01.575150: step 3300, loss 1.53346, acc 0.736

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-3300

2016-09-07T21:52:03.266093: step 3301, loss 0.0434931, acc 0.98
2016-09-07T21:52:03.946524: step 3302, loss 0.0233029, acc 1
2016-09-07T21:52:04.624970: step 3303, loss 0.0265416, acc 1
2016-09-07T21:52:05.287239: step 3304, loss 0.0513249, acc 1
2016-09-07T21:52:05.959413: step 3305, loss 0.220517, acc 0.94
2016-09-07T21:52:06.643658: step 3306, loss 0.0624711, acc 0.96
2016-09-07T21:52:07.352261: step 3307, loss 0.00794083, acc 1
2016-09-07T21:52:08.035471: step 3308, loss 0.0226163, acc 1
2016-09-07T21:52:08.712969: step 3309, loss 0.0304644, acc 0.98
2016-09-07T21:52:09.385419: step 3310, loss 0.153994, acc 0.96
2016-09-07T21:52:10.047416: step 3311, loss 0.142862, acc 0.96
2016-09-07T21:52:10.727120: step 3312, loss 0.0294692, acc 0.98
2016-09-07T21:52:11.376944: step 3313, loss 0.0709052, acc 0.96
2016-09-07T21:52:12.053349: step 3314, loss 0.0305443, acc 0.98
2016-09-07T21:52:12.729293: step 3315, loss 0.0836146, acc 0.96
2016-09-07T21:52:13.424742: step 3316, loss 0.114985, acc 0.94
2016-09-07T21:52:14.093482: step 3317, loss 0.0263572, acc 1
2016-09-07T21:52:14.801661: step 3318, loss 0.0611725, acc 0.96
2016-09-07T21:52:15.464414: step 3319, loss 0.043237, acc 0.98
2016-09-07T21:52:16.146192: step 3320, loss 0.110536, acc 0.92
2016-09-07T21:52:16.821973: step 3321, loss 0.0448437, acc 0.98
2016-09-07T21:52:17.492664: step 3322, loss 0.0528978, acc 0.98
2016-09-07T21:52:18.146448: step 3323, loss 0.0593447, acc 0.96
2016-09-07T21:52:18.824677: step 3324, loss 0.0734973, acc 0.96
2016-09-07T21:52:19.487896: step 3325, loss 0.0137675, acc 1
2016-09-07T21:52:20.154503: step 3326, loss 0.0296333, acc 1
2016-09-07T21:52:20.837339: step 3327, loss 0.0700986, acc 0.96
2016-09-07T21:52:21.529945: step 3328, loss 0.0821947, acc 0.94
2016-09-07T21:52:22.216325: step 3329, loss 0.019671, acc 1
2016-09-07T21:52:22.912401: step 3330, loss 0.10045, acc 0.94
2016-09-07T21:52:23.581513: step 3331, loss 0.0271491, acc 1
2016-09-07T21:52:24.237681: step 3332, loss 0.10463, acc 0.96
2016-09-07T21:52:24.911023: step 3333, loss 0.0469666, acc 0.98
2016-09-07T21:52:25.589751: step 3334, loss 0.0624009, acc 0.98
2016-09-07T21:52:26.270696: step 3335, loss 0.110757, acc 0.96
2016-09-07T21:52:26.937873: step 3336, loss 0.111432, acc 0.98
2016-09-07T21:52:27.622026: step 3337, loss 0.0155132, acc 1
2016-09-07T21:52:28.294765: step 3338, loss 0.0113861, acc 1
2016-09-07T21:52:28.977584: step 3339, loss 0.0351385, acc 0.98
2016-09-07T21:52:29.657362: step 3340, loss 0.0510385, acc 0.98
2016-09-07T21:52:30.312620: step 3341, loss 0.00947235, acc 1
2016-09-07T21:52:30.987386: step 3342, loss 0.0131435, acc 1
2016-09-07T21:52:31.653229: step 3343, loss 0.0511228, acc 0.98
2016-09-07T21:52:32.324229: step 3344, loss 0.0719425, acc 0.96
2016-09-07T21:52:32.996756: step 3345, loss 0.0175352, acc 1
2016-09-07T21:52:33.665246: step 3346, loss 0.0583551, acc 0.96
2016-09-07T21:52:34.326735: step 3347, loss 0.0279922, acc 0.98
2016-09-07T21:52:34.996664: step 3348, loss 0.0281418, acc 1
2016-09-07T21:52:35.689482: step 3349, loss 0.0321669, acc 0.98
2016-09-07T21:52:36.360886: step 3350, loss 0.0287045, acc 1
2016-09-07T21:52:37.032422: step 3351, loss 0.0380805, acc 0.98
2016-09-07T21:52:37.715982: step 3352, loss 0.0941339, acc 0.96
2016-09-07T21:52:38.375632: step 3353, loss 0.0516408, acc 0.98
2016-09-07T21:52:39.047516: step 3354, loss 0.125136, acc 0.98
2016-09-07T21:52:39.727507: step 3355, loss 0.0757826, acc 0.96
2016-09-07T21:52:40.417038: step 3356, loss 0.051773, acc 0.98
2016-09-07T21:52:41.087104: step 3357, loss 0.0367477, acc 1
2016-09-07T21:52:41.737796: step 3358, loss 0.0400266, acc 0.98
2016-09-07T21:52:42.397723: step 3359, loss 0.138805, acc 0.94
2016-09-07T21:52:43.064578: step 3360, loss 0.0419309, acc 1
2016-09-07T21:52:43.739269: step 3361, loss 0.0391065, acc 0.96
2016-09-07T21:52:44.411008: step 3362, loss 0.0744223, acc 0.96
2016-09-07T21:52:45.080251: step 3363, loss 0.127804, acc 0.94
2016-09-07T21:52:45.757479: step 3364, loss 0.102421, acc 0.92
2016-09-07T21:52:46.437548: step 3365, loss 0.0217439, acc 1
2016-09-07T21:52:47.123002: step 3366, loss 0.0524071, acc 0.98
2016-09-07T21:52:47.827456: step 3367, loss 0.0253489, acc 1
2016-09-07T21:52:48.492743: step 3368, loss 0.0432843, acc 1
2016-09-07T21:52:49.158141: step 3369, loss 0.020584, acc 1
2016-09-07T21:52:49.819220: step 3370, loss 0.0408412, acc 1
2016-09-07T21:52:50.510584: step 3371, loss 0.0603878, acc 0.98
2016-09-07T21:52:51.252865: step 3372, loss 0.0786677, acc 0.96
2016-09-07T21:52:51.934450: step 3373, loss 0.0256266, acc 0.98
2016-09-07T21:52:52.600301: step 3374, loss 0.0631881, acc 0.98
2016-09-07T21:52:53.280001: step 3375, loss 0.0103272, acc 1
2016-09-07T21:52:53.949874: step 3376, loss 0.0317008, acc 1
2016-09-07T21:52:54.616395: step 3377, loss 0.0510667, acc 0.98
2016-09-07T21:52:55.288628: step 3378, loss 0.03558, acc 1
2016-09-07T21:52:55.936120: step 3379, loss 0.0281499, acc 0.98
2016-09-07T21:52:56.606046: step 3380, loss 0.0261066, acc 1
2016-09-07T21:52:57.251077: step 3381, loss 0.0379406, acc 0.98
2016-09-07T21:52:57.906983: step 3382, loss 0.0411814, acc 0.98
2016-09-07T21:52:58.567238: step 3383, loss 0.0134813, acc 1
2016-09-07T21:52:59.248686: step 3384, loss 0.0264453, acc 0.98
2016-09-07T21:52:59.935573: step 3385, loss 0.041756, acc 0.98
2016-09-07T21:53:00.647420: step 3386, loss 0.0680288, acc 0.96
2016-09-07T21:53:01.313237: step 3387, loss 0.0615147, acc 0.96
2016-09-07T21:53:01.987761: step 3388, loss 0.0206968, acc 1
2016-09-07T21:53:02.658070: step 3389, loss 0.0345764, acc 1
2016-09-07T21:53:03.326635: step 3390, loss 0.108391, acc 0.98
2016-09-07T21:53:04.023826: step 3391, loss 0.0610272, acc 0.96
2016-09-07T21:53:04.681654: step 3392, loss 0.00966811, acc 1
2016-09-07T21:53:05.349785: step 3393, loss 0.0574704, acc 0.96
2016-09-07T21:53:06.008375: step 3394, loss 0.0669865, acc 0.98
2016-09-07T21:53:06.688856: step 3395, loss 0.0237732, acc 1
2016-09-07T21:53:07.374145: step 3396, loss 0.141175, acc 0.92
2016-09-07T21:53:08.048134: step 3397, loss 0.0264955, acc 0.98
2016-09-07T21:53:08.714792: step 3398, loss 0.0304608, acc 1
2016-09-07T21:53:09.373336: step 3399, loss 0.0431717, acc 0.98
2016-09-07T21:53:10.046846: step 3400, loss 0.0248603, acc 1

Evaluation:
2016-09-07T21:53:13.347496: step 3400, loss 1.62335, acc 0.75

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-3400

2016-09-07T21:53:15.098094: step 3401, loss 0.0326319, acc 1
2016-09-07T21:53:15.777722: step 3402, loss 0.154763, acc 0.96
2016-09-07T21:53:16.463961: step 3403, loss 0.0137218, acc 1
2016-09-07T21:53:17.140044: step 3404, loss 0.0307499, acc 1
2016-09-07T21:53:17.826344: step 3405, loss 0.0333627, acc 0.98
2016-09-07T21:53:18.488915: step 3406, loss 0.0609224, acc 0.98
2016-09-07T21:53:19.173618: step 3407, loss 0.0131787, acc 1
2016-09-07T21:53:19.857542: step 3408, loss 0.0154116, acc 1
2016-09-07T21:53:20.545694: step 3409, loss 0.0321911, acc 1
2016-09-07T21:53:21.229748: step 3410, loss 0.0285403, acc 0.98
2016-09-07T21:53:21.919722: step 3411, loss 0.0824498, acc 0.94
2016-09-07T21:53:22.605701: step 3412, loss 0.0711735, acc 0.94
2016-09-07T21:53:23.272151: step 3413, loss 0.0551783, acc 0.96
2016-09-07T21:53:23.942901: step 3414, loss 0.0666544, acc 0.98
2016-09-07T21:53:24.611261: step 3415, loss 0.00670672, acc 1
2016-09-07T21:53:25.292711: step 3416, loss 0.00645126, acc 1
2016-09-07T21:53:25.973881: step 3417, loss 0.0236932, acc 0.98
2016-09-07T21:53:26.631000: step 3418, loss 0.0202884, acc 1
2016-09-07T21:53:27.310896: step 3419, loss 0.0137033, acc 1
2016-09-07T21:53:27.977893: step 3420, loss 0.0631928, acc 0.96
2016-09-07T21:53:28.651679: step 3421, loss 0.0169176, acc 1
2016-09-07T21:53:29.325007: step 3422, loss 0.0264134, acc 1
2016-09-07T21:53:30.004265: step 3423, loss 0.124499, acc 0.98
2016-09-07T21:53:30.690501: step 3424, loss 0.0314911, acc 1
2016-09-07T21:53:31.368159: step 3425, loss 0.0348656, acc 1
2016-09-07T21:53:32.062595: step 3426, loss 0.0205886, acc 1
2016-09-07T21:53:32.739523: step 3427, loss 0.015629, acc 1
2016-09-07T21:53:33.414229: step 3428, loss 0.0962307, acc 0.98
2016-09-07T21:53:34.106640: step 3429, loss 0.0465744, acc 0.98
2016-09-07T21:53:34.776637: step 3430, loss 0.0527929, acc 0.98
2016-09-07T21:53:35.448968: step 3431, loss 0.0269148, acc 0.98
2016-09-07T21:53:36.137349: step 3432, loss 0.0479856, acc 0.96
2016-09-07T21:53:36.817922: step 3433, loss 0.0331674, acc 1
2016-09-07T21:53:37.482865: step 3434, loss 0.0658715, acc 0.96
2016-09-07T21:53:38.181361: step 3435, loss 0.0343401, acc 1
2016-09-07T21:53:38.841779: step 3436, loss 0.00490081, acc 1
2016-09-07T21:53:39.511059: step 3437, loss 0.00738821, acc 1
2016-09-07T21:53:40.197363: step 3438, loss 0.0672749, acc 0.96
2016-09-07T21:53:40.870860: step 3439, loss 0.100489, acc 0.96
2016-09-07T21:53:41.538948: step 3440, loss 0.0365094, acc 0.98
2016-09-07T21:53:42.202633: step 3441, loss 0.0250346, acc 0.98
2016-09-07T21:53:42.861929: step 3442, loss 0.00821858, acc 1
2016-09-07T21:53:43.531050: step 3443, loss 0.0901451, acc 0.98
2016-09-07T21:53:44.200245: step 3444, loss 0.021276, acc 1
2016-09-07T21:53:44.864276: step 3445, loss 0.066453, acc 0.96
2016-09-07T21:53:45.531506: step 3446, loss 0.0465841, acc 0.98
2016-09-07T21:53:46.184255: step 3447, loss 0.0148403, acc 1
2016-09-07T21:53:46.843914: step 3448, loss 0.00575921, acc 1
2016-09-07T21:53:47.524114: step 3449, loss 0.0356273, acc 0.98
2016-09-07T21:53:48.180425: step 3450, loss 0.0391067, acc 0.98
2016-09-07T21:53:48.866028: step 3451, loss 0.0584679, acc 0.98
2016-09-07T21:53:49.527651: step 3452, loss 0.0712638, acc 0.96
2016-09-07T21:53:50.194057: step 3453, loss 0.0396486, acc 1
2016-09-07T21:53:50.863901: step 3454, loss 0.0153771, acc 1
2016-09-07T21:53:51.523743: step 3455, loss 0.0192432, acc 1
2016-09-07T21:53:52.200489: step 3456, loss 0.0290184, acc 0.98
2016-09-07T21:53:52.886746: step 3457, loss 0.0196598, acc 1
2016-09-07T21:53:53.575625: step 3458, loss 0.0465703, acc 0.96
2016-09-07T21:53:54.240213: step 3459, loss 0.10167, acc 0.92
2016-09-07T21:53:54.896266: step 3460, loss 0.0389456, acc 1
2016-09-07T21:53:55.565084: step 3461, loss 0.0610389, acc 0.96
2016-09-07T21:53:56.247021: step 3462, loss 0.0322743, acc 0.98
2016-09-07T21:53:56.910086: step 3463, loss 0.0217234, acc 1
2016-09-07T21:53:57.596337: step 3464, loss 0.0351302, acc 1
2016-09-07T21:53:58.277356: step 3465, loss 0.0245384, acc 1
2016-09-07T21:53:58.935980: step 3466, loss 0.0230197, acc 0.98
2016-09-07T21:53:59.610705: step 3467, loss 0.093802, acc 0.96
2016-09-07T21:54:00.340774: step 3468, loss 0.0354828, acc 0.98
2016-09-07T21:54:01.016903: step 3469, loss 0.0433017, acc 0.98
2016-09-07T21:54:01.701042: step 3470, loss 0.00485037, acc 1
2016-09-07T21:54:02.377362: step 3471, loss 0.0212646, acc 0.98
2016-09-07T21:54:03.049941: step 3472, loss 0.0250768, acc 1
2016-09-07T21:54:03.713014: step 3473, loss 0.0398866, acc 1
2016-09-07T21:54:04.374936: step 3474, loss 0.0325159, acc 1
2016-09-07T21:54:05.030765: step 3475, loss 0.0123278, acc 1
2016-09-07T21:54:05.710760: step 3476, loss 0.0103284, acc 1
2016-09-07T21:54:06.369914: step 3477, loss 0.0191086, acc 1
2016-09-07T21:54:07.057532: step 3478, loss 0.0231162, acc 0.98
2016-09-07T21:54:07.713341: step 3479, loss 0.0408144, acc 0.98
2016-09-07T21:54:08.384833: step 3480, loss 0.0520052, acc 0.98
2016-09-07T21:54:09.039806: step 3481, loss 0.0140554, acc 1
2016-09-07T21:54:09.712568: step 3482, loss 0.0263505, acc 0.98
2016-09-07T21:54:10.392050: step 3483, loss 0.110026, acc 0.94
2016-09-07T21:54:11.054013: step 3484, loss 0.00612823, acc 1
2016-09-07T21:54:11.728406: step 3485, loss 0.0773582, acc 0.96
2016-09-07T21:54:12.404066: step 3486, loss 0.0177752, acc 1
2016-09-07T21:54:13.074703: step 3487, loss 0.07982, acc 0.98
2016-09-07T21:54:13.750602: step 3488, loss 0.0521357, acc 0.96
2016-09-07T21:54:14.417550: step 3489, loss 0.0520073, acc 1
2016-09-07T21:54:15.070732: step 3490, loss 0.0579938, acc 0.98
2016-09-07T21:54:15.729989: step 3491, loss 0.0451598, acc 0.98
2016-09-07T21:54:16.078434: step 3492, loss 0.0633139, acc 1
2016-09-07T21:54:16.750910: step 3493, loss 0.0300192, acc 0.98
2016-09-07T21:54:17.418469: step 3494, loss 0.0899026, acc 0.96
2016-09-07T21:54:18.089456: step 3495, loss 0.0536021, acc 0.98
2016-09-07T21:54:18.761904: step 3496, loss 0.0240117, acc 1
2016-09-07T21:54:19.434578: step 3497, loss 0.0062126, acc 1
2016-09-07T21:54:20.102429: step 3498, loss 0.0229237, acc 1
2016-09-07T21:54:20.770215: step 3499, loss 0.0405435, acc 0.96
2016-09-07T21:54:21.449116: step 3500, loss 0.0245021, acc 0.98

Evaluation:
2016-09-07T21:54:24.356627: step 3500, loss 1.75901, acc 0.761

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-3500

2016-09-07T21:54:26.061358: step 3501, loss 0.0705166, acc 0.98
2016-09-07T21:54:26.721559: step 3502, loss 0.0163806, acc 1
2016-09-07T21:54:27.385033: step 3503, loss 0.0367948, acc 0.98
2016-09-07T21:54:28.050331: step 3504, loss 0.0389354, acc 1
2016-09-07T21:54:28.712107: step 3505, loss 0.0294215, acc 1
2016-09-07T21:54:29.395994: step 3506, loss 0.124224, acc 0.94
2016-09-07T21:54:30.059992: step 3507, loss 0.0334383, acc 1
2016-09-07T21:54:30.730612: step 3508, loss 0.016323, acc 1
2016-09-07T21:54:31.395907: step 3509, loss 0.0355854, acc 0.98
2016-09-07T21:54:32.044728: step 3510, loss 0.0128511, acc 1
2016-09-07T21:54:32.698398: step 3511, loss 0.0158035, acc 1
2016-09-07T21:54:33.357118: step 3512, loss 0.0213504, acc 0.98
2016-09-07T21:54:34.005131: step 3513, loss 0.0557637, acc 0.98
2016-09-07T21:54:34.669408: step 3514, loss 0.0398999, acc 0.98
2016-09-07T21:54:35.344340: step 3515, loss 0.00738202, acc 1
2016-09-07T21:54:36.002937: step 3516, loss 0.014213, acc 1
2016-09-07T21:54:36.662960: step 3517, loss 0.0275942, acc 0.98
2016-09-07T21:54:37.332930: step 3518, loss 0.0117139, acc 1
2016-09-07T21:54:38.012050: step 3519, loss 0.00721986, acc 1
2016-09-07T21:54:38.658694: step 3520, loss 0.0384722, acc 0.98
2016-09-07T21:54:39.321048: step 3521, loss 0.0351998, acc 0.98
2016-09-07T21:54:40.010132: step 3522, loss 0.0298518, acc 1
2016-09-07T21:54:40.678377: step 3523, loss 0.208411, acc 0.96
2016-09-07T21:54:41.358191: step 3524, loss 0.0899884, acc 0.96
2016-09-07T21:54:42.026277: step 3525, loss 0.0166574, acc 1
2016-09-07T21:54:42.690759: step 3526, loss 0.0346843, acc 1
2016-09-07T21:54:43.353550: step 3527, loss 0.0104102, acc 1
2016-09-07T21:54:44.012301: step 3528, loss 0.172898, acc 0.96
2016-09-07T21:54:44.677787: step 3529, loss 0.00557895, acc 1
2016-09-07T21:54:45.358114: step 3530, loss 0.0730405, acc 0.98
2016-09-07T21:54:46.027335: step 3531, loss 0.0149221, acc 1
2016-09-07T21:54:46.723765: step 3532, loss 0.00939271, acc 1
2016-09-07T21:54:47.383011: step 3533, loss 0.138204, acc 0.96
2016-09-07T21:54:48.059657: step 3534, loss 0.0214219, acc 1
2016-09-07T21:54:48.738695: step 3535, loss 0.0468336, acc 0.96
2016-09-07T21:54:49.397898: step 3536, loss 0.0430231, acc 0.98
2016-09-07T21:54:50.043071: step 3537, loss 0.0403734, acc 0.98
2016-09-07T21:54:50.690865: step 3538, loss 0.0539679, acc 0.96
2016-09-07T21:54:51.361019: step 3539, loss 0.0742387, acc 0.98
2016-09-07T21:54:52.036417: step 3540, loss 0.00521961, acc 1
2016-09-07T21:54:52.703884: step 3541, loss 0.0163056, acc 1
2016-09-07T21:54:53.379421: step 3542, loss 0.0417499, acc 0.98
2016-09-07T21:54:54.046531: step 3543, loss 0.028556, acc 0.98
2016-09-07T21:54:54.725946: step 3544, loss 0.085847, acc 0.96
2016-09-07T21:54:55.387413: step 3545, loss 0.026523, acc 1
2016-09-07T21:54:56.051979: step 3546, loss 0.061877, acc 0.96
2016-09-07T21:54:56.702224: step 3547, loss 0.056506, acc 0.94
2016-09-07T21:54:57.378452: step 3548, loss 0.0303864, acc 1
2016-09-07T21:54:58.055871: step 3549, loss 0.0470818, acc 1
2016-09-07T21:54:58.728453: step 3550, loss 0.00794563, acc 1
2016-09-07T21:54:59.385260: step 3551, loss 0.0270931, acc 0.98
2016-09-07T21:55:00.058056: step 3552, loss 0.00641948, acc 1
2016-09-07T21:55:00.779590: step 3553, loss 0.0980497, acc 0.96
2016-09-07T21:55:01.451441: step 3554, loss 0.018449, acc 1
2016-09-07T21:55:02.115670: step 3555, loss 0.0438867, acc 0.98
2016-09-07T21:55:02.785556: step 3556, loss 0.0370136, acc 0.98
2016-09-07T21:55:03.464525: step 3557, loss 0.0324203, acc 1
2016-09-07T21:55:04.138722: step 3558, loss 0.117075, acc 0.94
2016-09-07T21:55:04.819609: step 3559, loss 0.0141685, acc 1
2016-09-07T21:55:05.486496: step 3560, loss 0.0215582, acc 1
2016-09-07T21:55:06.160087: step 3561, loss 0.0112585, acc 1
2016-09-07T21:55:06.823181: step 3562, loss 0.0192264, acc 1
2016-09-07T21:55:07.526013: step 3563, loss 0.0330365, acc 1
2016-09-07T21:55:08.189772: step 3564, loss 0.0699764, acc 0.98
2016-09-07T21:55:08.877023: step 3565, loss 0.0161035, acc 1
2016-09-07T21:55:09.558549: step 3566, loss 0.0133136, acc 1
2016-09-07T21:55:10.232528: step 3567, loss 0.0140046, acc 1
2016-09-07T21:55:10.906101: step 3568, loss 0.0247887, acc 0.98
2016-09-07T21:55:11.585855: step 3569, loss 0.014191, acc 1
2016-09-07T21:55:12.243084: step 3570, loss 0.0766941, acc 0.98
2016-09-07T21:55:12.918760: step 3571, loss 0.0195292, acc 1
2016-09-07T21:55:13.612079: step 3572, loss 0.0375558, acc 0.98
2016-09-07T21:55:14.279845: step 3573, loss 0.0435899, acc 0.98
2016-09-07T21:55:14.953833: step 3574, loss 0.112588, acc 0.98
2016-09-07T21:55:15.620937: step 3575, loss 0.0256923, acc 0.98
2016-09-07T21:55:16.280996: step 3576, loss 0.0209468, acc 1
2016-09-07T21:55:16.955390: step 3577, loss 0.00777115, acc 1
2016-09-07T21:55:17.604370: step 3578, loss 0.161473, acc 0.94
2016-09-07T21:55:18.276385: step 3579, loss 0.064657, acc 0.98
2016-09-07T21:55:18.954604: step 3580, loss 0.0308816, acc 0.98
2016-09-07T21:55:19.628276: step 3581, loss 0.0318919, acc 1
2016-09-07T21:55:20.275702: step 3582, loss 0.0279913, acc 1
2016-09-07T21:55:20.926785: step 3583, loss 0.0295025, acc 0.98
2016-09-07T21:55:21.589961: step 3584, loss 0.00933278, acc 1
2016-09-07T21:55:22.234714: step 3585, loss 0.0230584, acc 1
2016-09-07T21:55:22.899988: step 3586, loss 0.0315043, acc 1
2016-09-07T21:55:23.564090: step 3587, loss 0.0398345, acc 0.98
2016-09-07T21:55:24.234880: step 3588, loss 0.0245764, acc 1
2016-09-07T21:55:24.925537: step 3589, loss 0.0423473, acc 0.98
2016-09-07T21:55:25.608982: step 3590, loss 0.0374042, acc 1
2016-09-07T21:55:26.275582: step 3591, loss 0.0224336, acc 1
2016-09-07T21:55:26.951867: step 3592, loss 0.0482245, acc 0.96
2016-09-07T21:55:27.625026: step 3593, loss 0.021528, acc 1
2016-09-07T21:55:28.277887: step 3594, loss 0.0307487, acc 0.98
2016-09-07T21:55:28.941325: step 3595, loss 0.0374225, acc 1
2016-09-07T21:55:29.628641: step 3596, loss 0.0206327, acc 1
2016-09-07T21:55:30.306460: step 3597, loss 0.019934, acc 1
2016-09-07T21:55:30.971941: step 3598, loss 0.0556197, acc 0.98
2016-09-07T21:55:31.630530: step 3599, loss 0.0176679, acc 1
2016-09-07T21:55:32.301708: step 3600, loss 0.024131, acc 1

Evaluation:
2016-09-07T21:55:35.169310: step 3600, loss 1.93375, acc 0.754

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-3600

2016-09-07T21:55:36.799241: step 3601, loss 0.00883252, acc 1
2016-09-07T21:55:37.457743: step 3602, loss 0.0263564, acc 0.98
2016-09-07T21:55:38.132205: step 3603, loss 0.122713, acc 0.96
2016-09-07T21:55:38.813792: step 3604, loss 0.00577713, acc 1
2016-09-07T21:55:39.473664: step 3605, loss 0.0303338, acc 1
2016-09-07T21:55:40.154571: step 3606, loss 0.0181983, acc 1
2016-09-07T21:55:40.808618: step 3607, loss 0.0216976, acc 0.98
2016-09-07T21:55:41.475176: step 3608, loss 0.0408262, acc 1
2016-09-07T21:55:42.142206: step 3609, loss 0.0197845, acc 1
2016-09-07T21:55:42.799211: step 3610, loss 0.221044, acc 0.96
2016-09-07T21:55:43.477809: step 3611, loss 0.0139281, acc 1
2016-09-07T21:55:44.141122: step 3612, loss 0.0883068, acc 0.98
2016-09-07T21:55:44.812745: step 3613, loss 0.00931532, acc 1
2016-09-07T21:55:45.481942: step 3614, loss 0.0384058, acc 0.98
2016-09-07T21:55:46.151866: step 3615, loss 0.00918211, acc 1
2016-09-07T21:55:46.817160: step 3616, loss 0.019655, acc 1
2016-09-07T21:55:47.484485: step 3617, loss 0.0624775, acc 0.98
2016-09-07T21:55:48.145049: step 3618, loss 0.0252522, acc 0.98
2016-09-07T21:55:48.806418: step 3619, loss 0.0151287, acc 1
2016-09-07T21:55:49.470416: step 3620, loss 0.303372, acc 0.92
2016-09-07T21:55:50.152060: step 3621, loss 0.0118923, acc 1
2016-09-07T21:55:50.814766: step 3622, loss 0.0282892, acc 0.98
2016-09-07T21:55:51.481561: step 3623, loss 0.0415698, acc 0.98
2016-09-07T21:55:52.155385: step 3624, loss 0.0167575, acc 1
2016-09-07T21:55:52.815941: step 3625, loss 0.0424165, acc 0.98
2016-09-07T21:55:53.502554: step 3626, loss 0.0104815, acc 1
2016-09-07T21:55:54.176067: step 3627, loss 0.0144764, acc 1
2016-09-07T21:55:54.842446: step 3628, loss 0.0144736, acc 1
2016-09-07T21:55:55.495691: step 3629, loss 0.0580563, acc 0.98
2016-09-07T21:55:56.153227: step 3630, loss 0.0127438, acc 1
2016-09-07T21:55:56.815697: step 3631, loss 0.0632689, acc 0.96
2016-09-07T21:55:57.492549: step 3632, loss 0.136054, acc 0.98
2016-09-07T21:55:58.175271: step 3633, loss 0.0240857, acc 1
2016-09-07T21:55:58.844672: step 3634, loss 0.0199224, acc 1
2016-09-07T21:55:59.501897: step 3635, loss 0.0137393, acc 1
2016-09-07T21:56:00.167031: step 3636, loss 0.00892022, acc 1
2016-09-07T21:56:00.879356: step 3637, loss 0.0270484, acc 1
2016-09-07T21:56:01.544508: step 3638, loss 0.0968789, acc 0.96
2016-09-07T21:56:02.205389: step 3639, loss 0.0731657, acc 0.94
2016-09-07T21:56:02.870668: step 3640, loss 0.0357897, acc 0.98
2016-09-07T21:56:03.553901: step 3641, loss 0.063141, acc 0.96
2016-09-07T21:56:04.225087: step 3642, loss 0.0529701, acc 0.94
2016-09-07T21:56:04.893350: step 3643, loss 0.0287874, acc 0.98
2016-09-07T21:56:05.554372: step 3644, loss 0.0843388, acc 0.98
2016-09-07T21:56:06.216531: step 3645, loss 0.0300797, acc 0.98
2016-09-07T21:56:06.917491: step 3646, loss 0.0120497, acc 1
2016-09-07T21:56:07.584803: step 3647, loss 0.0641446, acc 0.96
2016-09-07T21:56:08.255290: step 3648, loss 0.0425769, acc 0.98
2016-09-07T21:56:08.945271: step 3649, loss 0.0114124, acc 1
2016-09-07T21:56:09.607507: step 3650, loss 0.0930276, acc 0.94
2016-09-07T21:56:10.256001: step 3651, loss 0.143577, acc 0.96
2016-09-07T21:56:10.923627: step 3652, loss 0.0861072, acc 0.96
2016-09-07T21:56:11.585855: step 3653, loss 0.0231519, acc 1
2016-09-07T21:56:12.250969: step 3654, loss 0.0304463, acc 1
2016-09-07T21:56:12.911214: step 3655, loss 0.00712766, acc 1
2016-09-07T21:56:13.578729: step 3656, loss 0.0195016, acc 1
2016-09-07T21:56:14.244141: step 3657, loss 0.0111562, acc 1
2016-09-07T21:56:14.910408: step 3658, loss 0.0732787, acc 0.94
2016-09-07T21:56:15.564099: step 3659, loss 0.0136889, acc 1
2016-09-07T21:56:16.238707: step 3660, loss 0.030882, acc 0.98
2016-09-07T21:56:16.900341: step 3661, loss 0.0310928, acc 1
2016-09-07T21:56:17.546639: step 3662, loss 0.119751, acc 0.92
2016-09-07T21:56:18.214805: step 3663, loss 0.0165045, acc 1
2016-09-07T21:56:18.906810: step 3664, loss 0.0260804, acc 0.98
2016-09-07T21:56:19.565805: step 3665, loss 0.0480082, acc 0.96
2016-09-07T21:56:20.239033: step 3666, loss 0.182878, acc 0.92
2016-09-07T21:56:20.906655: step 3667, loss 0.0909771, acc 0.98
2016-09-07T21:56:21.557133: step 3668, loss 0.0237236, acc 1
2016-09-07T21:56:22.216361: step 3669, loss 0.0524922, acc 0.98
2016-09-07T21:56:22.892817: step 3670, loss 0.0234956, acc 1
2016-09-07T21:56:23.585483: step 3671, loss 0.0340813, acc 1
2016-09-07T21:56:24.254692: step 3672, loss 0.030517, acc 1
2016-09-07T21:56:24.923498: step 3673, loss 0.0386844, acc 1
2016-09-07T21:56:25.602067: step 3674, loss 0.047326, acc 0.98
2016-09-07T21:56:26.253897: step 3675, loss 0.0572534, acc 0.98
2016-09-07T21:56:26.901534: step 3676, loss 0.0247286, acc 1
2016-09-07T21:56:27.563576: step 3677, loss 0.00816679, acc 1
2016-09-07T21:56:28.246272: step 3678, loss 0.0537656, acc 0.98
2016-09-07T21:56:28.908678: step 3679, loss 0.0502431, acc 0.98
2016-09-07T21:56:29.567053: step 3680, loss 0.0593231, acc 0.96
2016-09-07T21:56:30.228300: step 3681, loss 0.00656838, acc 1
2016-09-07T21:56:30.893070: step 3682, loss 0.0322373, acc 1
2016-09-07T21:56:31.551079: step 3683, loss 0.0254292, acc 1
2016-09-07T21:56:32.219573: step 3684, loss 0.0284384, acc 1
2016-09-07T21:56:32.869739: step 3685, loss 0.0226803, acc 1
2016-09-07T21:56:33.223596: step 3686, loss 0.0526924, acc 1
2016-09-07T21:56:33.890054: step 3687, loss 0.0202292, acc 1
2016-09-07T21:56:34.570751: step 3688, loss 0.0319359, acc 0.98
2016-09-07T21:56:35.239585: step 3689, loss 0.10252, acc 0.94
2016-09-07T21:56:35.908930: step 3690, loss 0.0296315, acc 1
2016-09-07T21:56:36.574579: step 3691, loss 0.00561718, acc 1
2016-09-07T21:56:37.279375: step 3692, loss 0.0667618, acc 0.94
2016-09-07T21:56:37.955220: step 3693, loss 0.00852856, acc 1
2016-09-07T21:56:38.618777: step 3694, loss 0.12904, acc 0.98
2016-09-07T21:56:39.303489: step 3695, loss 0.0298758, acc 0.98
2016-09-07T21:56:39.967595: step 3696, loss 0.0830583, acc 0.98
2016-09-07T21:56:40.638523: step 3697, loss 0.0534534, acc 0.96
2016-09-07T21:56:41.302410: step 3698, loss 0.0251163, acc 1
2016-09-07T21:56:41.955505: step 3699, loss 0.0183218, acc 1
2016-09-07T21:56:42.643446: step 3700, loss 0.0428224, acc 1

Evaluation:
2016-09-07T21:56:45.517333: step 3700, loss 1.52757, acc 0.741

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-3700

2016-09-07T21:56:47.130329: step 3701, loss 0.0180442, acc 1
2016-09-07T21:56:47.793930: step 3702, loss 0.0254282, acc 0.98
2016-09-07T21:56:48.458217: step 3703, loss 0.00988645, acc 1
2016-09-07T21:56:49.117095: step 3704, loss 0.0140125, acc 1
2016-09-07T21:56:49.780048: step 3705, loss 0.0357172, acc 0.98
2016-09-07T21:56:50.457009: step 3706, loss 0.0469187, acc 0.98
2016-09-07T21:56:51.127119: step 3707, loss 0.063236, acc 0.98
2016-09-07T21:56:51.780150: step 3708, loss 0.0225363, acc 1
2016-09-07T21:56:52.462696: step 3709, loss 0.0501386, acc 0.98
2016-09-07T21:56:53.152697: step 3710, loss 0.0178018, acc 1
2016-09-07T21:56:53.833728: step 3711, loss 0.00547707, acc 1
2016-09-07T21:56:54.506972: step 3712, loss 0.00731726, acc 1
2016-09-07T21:56:55.183562: step 3713, loss 0.0210599, acc 0.98
2016-09-07T21:56:55.855059: step 3714, loss 0.0050915, acc 1
2016-09-07T21:56:56.533989: step 3715, loss 0.0249787, acc 1
2016-09-07T21:56:57.194269: step 3716, loss 0.0161811, acc 1
2016-09-07T21:56:57.859027: step 3717, loss 0.0335085, acc 1
2016-09-07T21:56:58.527775: step 3718, loss 0.0351127, acc 0.98
2016-09-07T21:56:59.214026: step 3719, loss 0.0150581, acc 1
2016-09-07T21:56:59.878717: step 3720, loss 0.0088109, acc 1
2016-09-07T21:57:00.591807: step 3721, loss 0.0227908, acc 1
2016-09-07T21:57:01.277354: step 3722, loss 0.0650431, acc 0.94
2016-09-07T21:57:01.968393: step 3723, loss 0.0247122, acc 0.98
2016-09-07T21:57:02.630050: step 3724, loss 0.12483, acc 0.96
2016-09-07T21:57:03.304887: step 3725, loss 0.00517914, acc 1
2016-09-07T21:57:03.981944: step 3726, loss 0.00660939, acc 1
2016-09-07T21:57:04.654124: step 3727, loss 0.0842074, acc 0.98
2016-09-07T21:57:05.339666: step 3728, loss 0.0144331, acc 1
2016-09-07T21:57:06.009650: step 3729, loss 0.00751822, acc 1
2016-09-07T21:57:06.655234: step 3730, loss 0.0448689, acc 0.96
2016-09-07T21:57:07.316826: step 3731, loss 0.0208398, acc 1
2016-09-07T21:57:07.986845: step 3732, loss 0.0136659, acc 1
2016-09-07T21:57:08.670497: step 3733, loss 0.0987692, acc 0.96
2016-09-07T21:57:09.340337: step 3734, loss 0.0303134, acc 0.98
2016-09-07T21:57:09.992187: step 3735, loss 0.0253994, acc 0.98
2016-09-07T21:57:10.648652: step 3736, loss 0.0182019, acc 1
2016-09-07T21:57:11.312692: step 3737, loss 0.084597, acc 0.98
2016-09-07T21:57:11.973707: step 3738, loss 0.0391813, acc 0.98
2016-09-07T21:57:12.641509: step 3739, loss 0.00843312, acc 1
2016-09-07T21:57:13.297489: step 3740, loss 0.0276182, acc 0.98
2016-09-07T21:57:13.971202: step 3741, loss 0.0248444, acc 1
2016-09-07T21:57:14.660041: step 3742, loss 0.0473765, acc 0.98
2016-09-07T21:57:15.343957: step 3743, loss 0.0294211, acc 1
2016-09-07T21:57:16.007476: step 3744, loss 0.00580278, acc 1
2016-09-07T21:57:16.675934: step 3745, loss 0.00691069, acc 1
2016-09-07T21:57:17.344981: step 3746, loss 0.0121329, acc 1
2016-09-07T21:57:18.004450: step 3747, loss 0.0388217, acc 0.98
2016-09-07T21:57:18.691993: step 3748, loss 0.0259927, acc 0.98
2016-09-07T21:57:19.359108: step 3749, loss 0.00990502, acc 1
2016-09-07T21:57:20.031711: step 3750, loss 0.0181848, acc 1
2016-09-07T21:57:20.702887: step 3751, loss 0.0599081, acc 0.98
2016-09-07T21:57:21.365305: step 3752, loss 0.0378719, acc 0.98
2016-09-07T21:57:22.026710: step 3753, loss 0.0386355, acc 0.98
2016-09-07T21:57:22.686047: step 3754, loss 0.0153062, acc 1
2016-09-07T21:57:23.393034: step 3755, loss 0.021587, acc 0.98
2016-09-07T21:57:24.053412: step 3756, loss 0.0477032, acc 0.98
2016-09-07T21:57:24.736531: step 3757, loss 0.00878161, acc 1
2016-09-07T21:57:25.412986: step 3758, loss 0.0599367, acc 0.96
2016-09-07T21:57:26.090777: step 3759, loss 0.00664949, acc 1
2016-09-07T21:57:26.757361: step 3760, loss 0.100003, acc 0.96
2016-09-07T21:57:27.421870: step 3761, loss 0.0427137, acc 0.96
2016-09-07T21:57:28.113768: step 3762, loss 0.0494864, acc 0.98
2016-09-07T21:57:28.786725: step 3763, loss 0.0381125, acc 0.98
2016-09-07T21:57:29.473034: step 3764, loss 0.0114043, acc 1
2016-09-07T21:57:30.166032: step 3765, loss 0.0754713, acc 0.98
2016-09-07T21:57:30.824546: step 3766, loss 0.0295635, acc 1
2016-09-07T21:57:31.490895: step 3767, loss 0.0742923, acc 0.98
2016-09-07T21:57:32.154755: step 3768, loss 0.00879406, acc 1
2016-09-07T21:57:32.833877: step 3769, loss 0.0713181, acc 0.94
2016-09-07T21:57:33.500249: step 3770, loss 0.0246441, acc 1
2016-09-07T21:57:34.200659: step 3771, loss 0.143693, acc 0.96
2016-09-07T21:57:34.855935: step 3772, loss 0.0284756, acc 0.98
2016-09-07T21:57:35.507599: step 3773, loss 0.0155717, acc 1
2016-09-07T21:57:36.168342: step 3774, loss 0.00857605, acc 1
2016-09-07T21:57:36.854120: step 3775, loss 0.141085, acc 0.94
2016-09-07T21:57:37.538685: step 3776, loss 0.0388926, acc 0.98
2016-09-07T21:57:38.206037: step 3777, loss 0.056726, acc 0.96
2016-09-07T21:57:38.850722: step 3778, loss 0.0493074, acc 0.98
2016-09-07T21:57:39.518148: step 3779, loss 0.0353851, acc 0.98
2016-09-07T21:57:40.191251: step 3780, loss 0.0443097, acc 0.98
2016-09-07T21:57:40.859633: step 3781, loss 0.0366897, acc 0.96
2016-09-07T21:57:41.508914: step 3782, loss 0.0136298, acc 1
2016-09-07T21:57:42.198487: step 3783, loss 0.0589069, acc 0.96
2016-09-07T21:57:42.878320: step 3784, loss 0.0257651, acc 1
2016-09-07T21:57:43.546675: step 3785, loss 0.0274496, acc 0.98
2016-09-07T21:57:44.215636: step 3786, loss 0.0500378, acc 0.96
2016-09-07T21:57:44.881262: step 3787, loss 0.010518, acc 1
2016-09-07T21:57:45.553505: step 3788, loss 0.0273813, acc 1
2016-09-07T21:57:46.220884: step 3789, loss 0.0255376, acc 0.98
2016-09-07T21:57:46.891481: step 3790, loss 0.017118, acc 1
2016-09-07T21:57:47.558514: step 3791, loss 0.0245711, acc 0.98
2016-09-07T21:57:48.201962: step 3792, loss 0.0345206, acc 0.98
2016-09-07T21:57:48.852329: step 3793, loss 0.0381166, acc 0.98
2016-09-07T21:57:49.510603: step 3794, loss 0.0193583, acc 1
2016-09-07T21:57:50.167152: step 3795, loss 0.0231104, acc 1
2016-09-07T21:57:50.841011: step 3796, loss 0.0126953, acc 1
2016-09-07T21:57:51.503543: step 3797, loss 0.037573, acc 1
2016-09-07T21:57:52.196645: step 3798, loss 0.00994848, acc 1
2016-09-07T21:57:52.883921: step 3799, loss 0.0333114, acc 1
2016-09-07T21:57:53.557355: step 3800, loss 0.0243267, acc 0.98

Evaluation:
2016-09-07T21:57:56.443270: step 3800, loss 1.99962, acc 0.733

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-3800

2016-09-07T21:57:58.100836: step 3801, loss 0.00952338, acc 1
2016-09-07T21:57:58.782662: step 3802, loss 0.103593, acc 0.92
2016-09-07T21:57:59.448429: step 3803, loss 0.00906883, acc 1
2016-09-07T21:58:00.136184: step 3804, loss 0.102914, acc 0.94
2016-09-07T21:58:00.834324: step 3805, loss 0.00928456, acc 1
2016-09-07T21:58:01.497843: step 3806, loss 0.0417098, acc 1
2016-09-07T21:58:02.183549: step 3807, loss 0.0401517, acc 0.98
2016-09-07T21:58:02.869374: step 3808, loss 0.0342, acc 0.98
2016-09-07T21:58:03.532440: step 3809, loss 0.0212742, acc 1
2016-09-07T21:58:04.194167: step 3810, loss 0.0200059, acc 0.98
2016-09-07T21:58:04.858515: step 3811, loss 0.124729, acc 0.96
2016-09-07T21:58:05.517159: step 3812, loss 0.0111417, acc 1
2016-09-07T21:58:06.198768: step 3813, loss 0.00849885, acc 1
2016-09-07T21:58:06.842382: step 3814, loss 0.122492, acc 0.98
2016-09-07T21:58:07.520845: step 3815, loss 0.0210859, acc 0.98
2016-09-07T21:58:08.195430: step 3816, loss 0.0459384, acc 0.98
2016-09-07T21:58:08.907748: step 3817, loss 0.024689, acc 1
2016-09-07T21:58:09.575556: step 3818, loss 0.0598275, acc 0.98
2016-09-07T21:58:10.231929: step 3819, loss 0.0243995, acc 0.98
2016-09-07T21:58:10.901743: step 3820, loss 0.0218485, acc 0.98
2016-09-07T21:58:11.555145: step 3821, loss 0.133906, acc 0.96
2016-09-07T21:58:12.221750: step 3822, loss 0.0107532, acc 1
2016-09-07T21:58:12.877177: step 3823, loss 0.0428303, acc 1
2016-09-07T21:58:13.535615: step 3824, loss 0.00625995, acc 1
2016-09-07T21:58:14.209301: step 3825, loss 0.0176193, acc 1
2016-09-07T21:58:14.866355: step 3826, loss 0.138267, acc 0.94
2016-09-07T21:58:15.544746: step 3827, loss 0.0243451, acc 1
2016-09-07T21:58:16.202397: step 3828, loss 0.027837, acc 0.98
2016-09-07T21:58:16.868977: step 3829, loss 0.0245863, acc 0.98
2016-09-07T21:58:17.522097: step 3830, loss 0.0379269, acc 0.98
2016-09-07T21:58:18.196961: step 3831, loss 0.00454107, acc 1
2016-09-07T21:58:18.878153: step 3832, loss 0.110475, acc 0.96
2016-09-07T21:58:19.540574: step 3833, loss 0.0163628, acc 1
2016-09-07T21:58:20.197094: step 3834, loss 0.067647, acc 0.96
2016-09-07T21:58:20.852379: step 3835, loss 0.0187425, acc 1
2016-09-07T21:58:21.504347: step 3836, loss 0.0128693, acc 1
2016-09-07T21:58:22.178354: step 3837, loss 0.0590412, acc 0.98
2016-09-07T21:58:22.842205: step 3838, loss 0.113295, acc 0.94
2016-09-07T21:58:23.511798: step 3839, loss 0.0285156, acc 1
2016-09-07T21:58:24.181700: step 3840, loss 0.0845905, acc 0.96
2016-09-07T21:58:24.854492: step 3841, loss 0.0253397, acc 1
2016-09-07T21:58:25.525834: step 3842, loss 0.0258552, acc 1
2016-09-07T21:58:26.208998: step 3843, loss 0.00485561, acc 1
2016-09-07T21:58:26.872313: step 3844, loss 0.0107977, acc 1
2016-09-07T21:58:27.543048: step 3845, loss 0.0540914, acc 0.96
2016-09-07T21:58:28.202280: step 3846, loss 0.0130035, acc 1
2016-09-07T21:58:28.863048: step 3847, loss 0.088798, acc 0.96
2016-09-07T21:58:29.546169: step 3848, loss 0.0261136, acc 1
2016-09-07T21:58:30.219467: step 3849, loss 0.00877084, acc 1
2016-09-07T21:58:30.894239: step 3850, loss 0.0085503, acc 1
2016-09-07T21:58:31.561398: step 3851, loss 0.0396925, acc 0.98
2016-09-07T21:58:32.232768: step 3852, loss 0.0416068, acc 0.98
2016-09-07T21:58:32.879208: step 3853, loss 0.0369564, acc 0.98
2016-09-07T21:58:33.558235: step 3854, loss 0.0289308, acc 0.98
2016-09-07T21:58:34.231904: step 3855, loss 0.0173713, acc 1
2016-09-07T21:58:34.914524: step 3856, loss 0.119468, acc 0.94
2016-09-07T21:58:35.584548: step 3857, loss 0.0414555, acc 1
2016-09-07T21:58:36.245816: step 3858, loss 0.0280651, acc 1
2016-09-07T21:58:36.918952: step 3859, loss 0.0478637, acc 0.96
2016-09-07T21:58:37.584205: step 3860, loss 0.0186339, acc 1
2016-09-07T21:58:38.248076: step 3861, loss 0.036315, acc 0.98
2016-09-07T21:58:38.918875: step 3862, loss 0.0248499, acc 0.98
2016-09-07T21:58:39.594989: step 3863, loss 0.0276054, acc 0.98
2016-09-07T21:58:40.249407: step 3864, loss 0.013098, acc 1
2016-09-07T21:58:40.931906: step 3865, loss 0.0321798, acc 0.98
2016-09-07T21:58:41.600030: step 3866, loss 0.0303353, acc 0.98
2016-09-07T21:58:42.272029: step 3867, loss 0.0282729, acc 0.98
2016-09-07T21:58:42.940533: step 3868, loss 0.0641152, acc 0.96
2016-09-07T21:58:43.625664: step 3869, loss 0.0190068, acc 0.98
2016-09-07T21:58:44.294718: step 3870, loss 0.0198201, acc 0.98
2016-09-07T21:58:44.975533: step 3871, loss 0.0137155, acc 1
2016-09-07T21:58:45.650721: step 3872, loss 0.0180275, acc 1
2016-09-07T21:58:46.311462: step 3873, loss 0.0357226, acc 0.98
2016-09-07T21:58:46.982924: step 3874, loss 0.0156044, acc 1
2016-09-07T21:58:47.661000: step 3875, loss 0.0951456, acc 0.94
2016-09-07T21:58:48.336012: step 3876, loss 0.0360461, acc 0.98
2016-09-07T21:58:49.021132: step 3877, loss 0.0278741, acc 1
2016-09-07T21:58:49.697022: step 3878, loss 0.027805, acc 0.98
2016-09-07T21:58:50.373583: step 3879, loss 0.00858868, acc 1
2016-09-07T21:58:50.738082: step 3880, loss 0.0158198, acc 1
2016-09-07T21:58:51.395212: step 3881, loss 0.0255235, acc 0.98
2016-09-07T21:58:52.048140: step 3882, loss 0.0518882, acc 0.98
2016-09-07T21:58:52.750984: step 3883, loss 0.0362062, acc 1
2016-09-07T21:58:53.428121: step 3884, loss 0.0242811, acc 1
2016-09-07T21:58:54.097275: step 3885, loss 0.0733379, acc 0.96
2016-09-07T21:58:54.760583: step 3886, loss 0.0133949, acc 1
2016-09-07T21:58:55.425042: step 3887, loss 0.0322917, acc 0.98
2016-09-07T21:58:56.078615: step 3888, loss 0.0703926, acc 0.96
2016-09-07T21:58:56.737795: step 3889, loss 0.092209, acc 0.98
2016-09-07T21:58:57.416696: step 3890, loss 0.00977573, acc 1
2016-09-07T21:58:58.093872: step 3891, loss 0.00586868, acc 1
2016-09-07T21:58:58.768446: step 3892, loss 0.0724333, acc 0.96
2016-09-07T21:58:59.432720: step 3893, loss 0.12629, acc 0.96
2016-09-07T21:59:00.104083: step 3894, loss 0.0326158, acc 0.98
2016-09-07T21:59:00.812899: step 3895, loss 0.0248274, acc 1
2016-09-07T21:59:01.481992: step 3896, loss 0.0246273, acc 1
2016-09-07T21:59:02.168639: step 3897, loss 0.0322111, acc 0.98
2016-09-07T21:59:02.857602: step 3898, loss 0.00721121, acc 1
2016-09-07T21:59:03.530779: step 3899, loss 0.0321274, acc 1
2016-09-07T21:59:04.188426: step 3900, loss 0.0347797, acc 0.98

Evaluation:
2016-09-07T21:59:07.061802: step 3900, loss 1.5515, acc 0.739

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-3900

2016-09-07T21:59:08.731474: step 3901, loss 0.0263982, acc 1
2016-09-07T21:59:09.374203: step 3902, loss 0.0395687, acc 0.98
2016-09-07T21:59:10.045817: step 3903, loss 0.0422197, acc 0.98
2016-09-07T21:59:10.719163: step 3904, loss 0.0366188, acc 0.98
2016-09-07T21:59:11.362878: step 3905, loss 0.0236032, acc 1
2016-09-07T21:59:12.016512: step 3906, loss 0.00995872, acc 1
2016-09-07T21:59:12.695455: step 3907, loss 0.0214628, acc 0.98
2016-09-07T21:59:13.356682: step 3908, loss 0.0459634, acc 0.98
2016-09-07T21:59:14.045912: step 3909, loss 0.0559511, acc 0.98
2016-09-07T21:59:14.718890: step 3910, loss 0.0497953, acc 1
2016-09-07T21:59:15.366429: step 3911, loss 0.0192879, acc 1
2016-09-07T21:59:16.033392: step 3912, loss 0.0568741, acc 0.98
2016-09-07T21:59:16.703179: step 3913, loss 0.021507, acc 1
2016-09-07T21:59:17.363075: step 3914, loss 0.049388, acc 0.98
2016-09-07T21:59:18.050022: step 3915, loss 0.0506565, acc 0.98
2016-09-07T21:59:18.719810: step 3916, loss 0.168032, acc 0.96
2016-09-07T21:59:19.389677: step 3917, loss 0.0180157, acc 1
2016-09-07T21:59:20.051868: step 3918, loss 0.0366428, acc 1
2016-09-07T21:59:20.738407: step 3919, loss 0.00771031, acc 1
2016-09-07T21:59:21.402609: step 3920, loss 0.0272529, acc 0.98
2016-09-07T21:59:22.070650: step 3921, loss 0.0514084, acc 0.98
2016-09-07T21:59:22.748118: step 3922, loss 0.0995875, acc 0.98
2016-09-07T21:59:23.422636: step 3923, loss 0.0417479, acc 0.98
2016-09-07T21:59:24.105913: step 3924, loss 0.0455775, acc 0.98
2016-09-07T21:59:24.794171: step 3925, loss 0.0294515, acc 1
2016-09-07T21:59:25.456388: step 3926, loss 0.0222503, acc 1
2016-09-07T21:59:26.143709: step 3927, loss 0.0436095, acc 0.96
2016-09-07T21:59:26.835775: step 3928, loss 0.00993659, acc 1
2016-09-07T21:59:27.503564: step 3929, loss 0.00617427, acc 1
2016-09-07T21:59:28.178806: step 3930, loss 0.0613164, acc 0.98
2016-09-07T21:59:28.833044: step 3931, loss 0.0305983, acc 1
2016-09-07T21:59:29.503603: step 3932, loss 0.0269602, acc 1
2016-09-07T21:59:30.182475: step 3933, loss 0.00999272, acc 1
2016-09-07T21:59:30.846591: step 3934, loss 0.0336611, acc 0.98
2016-09-07T21:59:31.513987: step 3935, loss 0.0448192, acc 0.98
2016-09-07T21:59:32.160751: step 3936, loss 0.0607606, acc 0.96
2016-09-07T21:59:32.829916: step 3937, loss 0.0131675, acc 1
2016-09-07T21:59:33.474855: step 3938, loss 0.00579276, acc 1
2016-09-07T21:59:34.150343: step 3939, loss 0.0473115, acc 0.98
2016-09-07T21:59:34.821735: step 3940, loss 0.00605842, acc 1
2016-09-07T21:59:35.479683: step 3941, loss 0.0174068, acc 1
2016-09-07T21:59:36.156454: step 3942, loss 0.0244079, acc 1
2016-09-07T21:59:36.836178: step 3943, loss 0.0257538, acc 1
2016-09-07T21:59:37.501496: step 3944, loss 0.0899751, acc 0.96
2016-09-07T21:59:38.167156: step 3945, loss 0.0892024, acc 0.96
2016-09-07T21:59:38.867555: step 3946, loss 0.0394289, acc 0.98
2016-09-07T21:59:39.551840: step 3947, loss 0.0208435, acc 1
2016-09-07T21:59:40.223735: step 3948, loss 0.0285705, acc 0.98
2016-09-07T21:59:40.900897: step 3949, loss 0.0458872, acc 0.98
2016-09-07T21:59:41.563327: step 3950, loss 0.0369621, acc 0.98
2016-09-07T21:59:42.223363: step 3951, loss 0.0237509, acc 0.98
2016-09-07T21:59:42.894604: step 3952, loss 0.0558782, acc 0.98
2016-09-07T21:59:43.562359: step 3953, loss 0.0108183, acc 1
2016-09-07T21:59:44.229216: step 3954, loss 0.00545515, acc 1
2016-09-07T21:59:44.910961: step 3955, loss 0.0256591, acc 1
2016-09-07T21:59:45.575558: step 3956, loss 0.07443, acc 0.94
2016-09-07T21:59:46.254648: step 3957, loss 0.0197065, acc 0.98
2016-09-07T21:59:46.909705: step 3958, loss 0.032709, acc 0.98
2016-09-07T21:59:47.560058: step 3959, loss 0.0598091, acc 0.96
2016-09-07T21:59:48.231069: step 3960, loss 0.0165577, acc 1
2016-09-07T21:59:48.902553: step 3961, loss 0.0295414, acc 0.98
2016-09-07T21:59:49.562594: step 3962, loss 0.00486936, acc 1
2016-09-07T21:59:50.238439: step 3963, loss 0.00635898, acc 1
2016-09-07T21:59:50.908590: step 3964, loss 0.0318485, acc 0.98
2016-09-07T21:59:51.588624: step 3965, loss 0.025097, acc 1
2016-09-07T21:59:52.254921: step 3966, loss 0.0527367, acc 0.96
2016-09-07T21:59:52.924393: step 3967, loss 0.0451162, acc 0.98
2016-09-07T21:59:53.602391: step 3968, loss 0.00512433, acc 1
2016-09-07T21:59:54.260848: step 3969, loss 0.0499856, acc 0.98
2016-09-07T21:59:54.938962: step 3970, loss 0.0665888, acc 0.96
2016-09-07T21:59:55.607087: step 3971, loss 0.0357839, acc 0.96
2016-09-07T21:59:56.287063: step 3972, loss 0.0900151, acc 0.96
2016-09-07T21:59:56.937520: step 3973, loss 0.00565287, acc 1
2016-09-07T21:59:57.577572: step 3974, loss 0.0363393, acc 0.98
2016-09-07T21:59:58.238537: step 3975, loss 0.036635, acc 1
2016-09-07T21:59:58.904295: step 3976, loss 0.0700872, acc 0.98
2016-09-07T21:59:59.574454: step 3977, loss 0.0537115, acc 0.98
2016-09-07T22:00:00.261590: step 3978, loss 0.181423, acc 0.96
2016-09-07T22:00:00.922907: step 3979, loss 0.0689958, acc 0.96
2016-09-07T22:00:01.583458: step 3980, loss 0.0546116, acc 0.98
2016-09-07T22:00:02.255486: step 3981, loss 0.0143578, acc 1
2016-09-07T22:00:02.938261: step 3982, loss 0.0392263, acc 0.98
2016-09-07T22:00:03.611162: step 3983, loss 0.034016, acc 0.98
2016-09-07T22:00:04.265799: step 3984, loss 0.0502584, acc 0.98
2016-09-07T22:00:04.925029: step 3985, loss 0.0316703, acc 0.98
2016-09-07T22:00:05.593787: step 3986, loss 0.0852451, acc 0.98
2016-09-07T22:00:06.272938: step 3987, loss 0.0386599, acc 1
2016-09-07T22:00:06.948915: step 3988, loss 0.109829, acc 0.92
2016-09-07T22:00:07.608450: step 3989, loss 0.0923405, acc 0.94
2016-09-07T22:00:08.270651: step 3990, loss 0.047243, acc 0.96
2016-09-07T22:00:08.940389: step 3991, loss 0.109919, acc 0.98
2016-09-07T22:00:09.622016: step 3992, loss 0.0296014, acc 1
2016-09-07T22:00:10.283108: step 3993, loss 0.0731589, acc 0.98
2016-09-07T22:00:10.941789: step 3994, loss 0.0568537, acc 0.98
2016-09-07T22:00:11.604490: step 3995, loss 0.0172368, acc 1
2016-09-07T22:00:12.280349: step 3996, loss 0.0535274, acc 0.98
2016-09-07T22:00:12.977265: step 3997, loss 0.0260417, acc 1
2016-09-07T22:00:13.648228: step 3998, loss 0.0218992, acc 1
2016-09-07T22:00:14.326967: step 3999, loss 0.0742487, acc 0.98
2016-09-07T22:00:14.987846: step 4000, loss 0.102634, acc 0.96

Evaluation:
2016-09-07T22:00:17.836311: step 4000, loss 1.96003, acc 0.733

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-4000

2016-09-07T22:00:19.470063: step 4001, loss 0.0222902, acc 1
2016-09-07T22:00:20.149674: step 4002, loss 0.0832969, acc 0.96
2016-09-07T22:00:20.805768: step 4003, loss 0.0240324, acc 1
2016-09-07T22:00:21.477093: step 4004, loss 0.0128637, acc 1
2016-09-07T22:00:22.154422: step 4005, loss 0.0449606, acc 1
2016-09-07T22:00:22.828712: step 4006, loss 0.010826, acc 1
2016-09-07T22:00:23.491529: step 4007, loss 0.17075, acc 0.98
2016-09-07T22:00:24.195135: step 4008, loss 0.0626572, acc 0.96
2016-09-07T22:00:24.843919: step 4009, loss 0.0222903, acc 0.98
2016-09-07T22:00:25.504370: step 4010, loss 0.044381, acc 0.98
2016-09-07T22:00:26.176804: step 4011, loss 0.0588792, acc 0.98
2016-09-07T22:00:26.845652: step 4012, loss 0.0496296, acc 0.98
2016-09-07T22:00:27.513040: step 4013, loss 0.0186259, acc 1
2016-09-07T22:00:28.176097: step 4014, loss 0.126535, acc 0.96
2016-09-07T22:00:28.841832: step 4015, loss 0.0377648, acc 1
2016-09-07T22:00:29.505768: step 4016, loss 0.0403394, acc 0.98
2016-09-07T22:00:30.192795: step 4017, loss 0.0103647, acc 1
2016-09-07T22:00:30.852436: step 4018, loss 0.00730032, acc 1
2016-09-07T22:00:31.523909: step 4019, loss 0.03987, acc 0.98
2016-09-07T22:00:32.186841: step 4020, loss 0.0306299, acc 1
2016-09-07T22:00:32.842012: step 4021, loss 0.0541675, acc 0.98
2016-09-07T22:00:33.504543: step 4022, loss 0.0819816, acc 0.96
2016-09-07T22:00:34.167905: step 4023, loss 0.0346058, acc 1
2016-09-07T22:00:34.827246: step 4024, loss 0.00557472, acc 1
2016-09-07T22:00:35.495504: step 4025, loss 0.0154361, acc 1
2016-09-07T22:00:36.153956: step 4026, loss 0.0199169, acc 1
2016-09-07T22:00:36.807635: step 4027, loss 0.0447313, acc 0.98
2016-09-07T22:00:37.492590: step 4028, loss 0.027079, acc 1
2016-09-07T22:00:38.154608: step 4029, loss 0.0463323, acc 0.98
2016-09-07T22:00:38.830389: step 4030, loss 0.00885374, acc 1
2016-09-07T22:00:39.495722: step 4031, loss 0.0223722, acc 1
2016-09-07T22:00:40.162108: step 4032, loss 0.0499902, acc 0.98
2016-09-07T22:00:40.827484: step 4033, loss 0.109108, acc 0.98
2016-09-07T22:00:41.521496: step 4034, loss 0.0290963, acc 1
2016-09-07T22:00:42.216363: step 4035, loss 0.0363986, acc 1
2016-09-07T22:00:42.884514: step 4036, loss 0.0231598, acc 1
2016-09-07T22:00:43.559383: step 4037, loss 0.0607488, acc 0.96
2016-09-07T22:00:44.231614: step 4038, loss 0.0289817, acc 0.98
2016-09-07T22:00:44.908200: step 4039, loss 0.0141766, acc 1
2016-09-07T22:00:45.567654: step 4040, loss 0.0220792, acc 0.98
2016-09-07T22:00:46.239087: step 4041, loss 0.0143448, acc 1
2016-09-07T22:00:46.891305: step 4042, loss 0.115724, acc 0.96
2016-09-07T22:00:47.544783: step 4043, loss 0.0185001, acc 1
2016-09-07T22:00:48.211845: step 4044, loss 0.0219617, acc 0.98
2016-09-07T22:00:48.862705: step 4045, loss 0.014604, acc 1
2016-09-07T22:00:49.533512: step 4046, loss 0.0314693, acc 0.98
2016-09-07T22:00:50.211778: step 4047, loss 0.0105897, acc 1
2016-09-07T22:00:50.882319: step 4048, loss 0.0201849, acc 1
2016-09-07T22:00:51.549066: step 4049, loss 0.0307091, acc 1
2016-09-07T22:00:52.223413: step 4050, loss 0.0345397, acc 0.98
2016-09-07T22:00:52.877559: step 4051, loss 0.0437536, acc 0.98
2016-09-07T22:00:53.533853: step 4052, loss 0.033535, acc 0.98
2016-09-07T22:00:54.177032: step 4053, loss 0.0439147, acc 0.98
2016-09-07T22:00:54.845268: step 4054, loss 0.00855412, acc 1
2016-09-07T22:00:55.498128: step 4055, loss 0.0071977, acc 1
2016-09-07T22:00:56.179931: step 4056, loss 0.0847497, acc 0.98
2016-09-07T22:00:56.847674: step 4057, loss 0.0064622, acc 1
2016-09-07T22:00:57.513837: step 4058, loss 0.0235428, acc 0.98
2016-09-07T22:00:58.170226: step 4059, loss 0.0218199, acc 1
2016-09-07T22:00:58.848732: step 4060, loss 0.0205971, acc 0.98
2016-09-07T22:00:59.512166: step 4061, loss 0.0593813, acc 0.96
2016-09-07T22:01:00.164790: step 4062, loss 0.0231057, acc 0.98
2016-09-07T22:01:00.862476: step 4063, loss 0.0459353, acc 0.96
2016-09-07T22:01:01.528244: step 4064, loss 0.00540799, acc 1
2016-09-07T22:01:02.209905: step 4065, loss 0.0445293, acc 0.98
2016-09-07T22:01:02.879244: step 4066, loss 0.00479539, acc 1
2016-09-07T22:01:03.545151: step 4067, loss 0.0735401, acc 0.96
2016-09-07T22:01:04.210292: step 4068, loss 0.0123809, acc 1
2016-09-07T22:01:04.867619: step 4069, loss 0.00657426, acc 1
2016-09-07T22:01:05.529076: step 4070, loss 0.07057, acc 0.94
2016-09-07T22:01:06.206315: step 4071, loss 0.0695366, acc 0.96
2016-09-07T22:01:06.886636: step 4072, loss 0.027338, acc 0.98
2016-09-07T22:01:07.610366: step 4073, loss 0.0395534, acc 0.98
2016-09-07T22:01:07.971331: step 4074, loss 0.203651, acc 0.916667
2016-09-07T22:01:08.639080: step 4075, loss 0.0535051, acc 0.96
2016-09-07T22:01:09.296250: step 4076, loss 0.0310194, acc 1
2016-09-07T22:01:09.971773: step 4077, loss 0.0339503, acc 1
2016-09-07T22:01:10.640711: step 4078, loss 0.0264318, acc 1
2016-09-07T22:01:11.355386: step 4079, loss 0.0335578, acc 1
2016-09-07T22:01:12.006570: step 4080, loss 0.0471961, acc 0.96
2016-09-07T22:01:12.679666: step 4081, loss 0.0112143, acc 1
2016-09-07T22:01:13.355243: step 4082, loss 0.0222408, acc 1
2016-09-07T22:01:14.024933: step 4083, loss 0.014049, acc 1
2016-09-07T22:01:14.695435: step 4084, loss 0.0379301, acc 0.98
2016-09-07T22:01:15.363805: step 4085, loss 0.0507441, acc 0.98
2016-09-07T22:01:16.024223: step 4086, loss 0.0271426, acc 0.98
2016-09-07T22:01:16.701791: step 4087, loss 0.0399779, acc 1
2016-09-07T22:01:17.372111: step 4088, loss 0.0376531, acc 0.98
2016-09-07T22:01:18.035418: step 4089, loss 0.0356706, acc 1
2016-09-07T22:01:18.698244: step 4090, loss 0.0129615, acc 1
2016-09-07T22:01:19.356666: step 4091, loss 0.044625, acc 1
2016-09-07T22:01:20.030012: step 4092, loss 0.0179742, acc 1
2016-09-07T22:01:20.701582: step 4093, loss 0.0176151, acc 1
2016-09-07T22:01:21.373038: step 4094, loss 0.021389, acc 0.98
2016-09-07T22:01:22.061130: step 4095, loss 0.0168903, acc 1
2016-09-07T22:01:22.742462: step 4096, loss 0.038827, acc 0.98
2016-09-07T22:01:23.391375: step 4097, loss 0.0796001, acc 0.98
2016-09-07T22:01:24.074296: step 4098, loss 0.0111419, acc 1
2016-09-07T22:01:24.749074: step 4099, loss 0.01008, acc 1
2016-09-07T22:01:25.433124: step 4100, loss 0.01387, acc 1

Evaluation:
2016-09-07T22:01:28.340559: step 4100, loss 2.16712, acc 0.742

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-4100

2016-09-07T22:01:29.934858: step 4101, loss 0.0332479, acc 0.98
2016-09-07T22:01:30.589579: step 4102, loss 0.00731885, acc 1
2016-09-07T22:01:31.258230: step 4103, loss 0.0294871, acc 0.98
2016-09-07T22:01:31.916696: step 4104, loss 0.0264107, acc 0.98
2016-09-07T22:01:32.581237: step 4105, loss 0.0260671, acc 0.98
2016-09-07T22:01:33.237045: step 4106, loss 0.0364155, acc 1
2016-09-07T22:01:33.904596: step 4107, loss 0.0105014, acc 1
2016-09-07T22:01:34.581165: step 4108, loss 0.011753, acc 1
2016-09-07T22:01:35.251037: step 4109, loss 0.00567577, acc 1
2016-09-07T22:01:35.922160: step 4110, loss 0.0241019, acc 0.98
2016-09-07T22:01:36.590033: step 4111, loss 0.00753212, acc 1
2016-09-07T22:01:37.263540: step 4112, loss 0.00636458, acc 1
2016-09-07T22:01:37.931407: step 4113, loss 0.0139416, acc 1
2016-09-07T22:01:38.594202: step 4114, loss 0.0222236, acc 1
2016-09-07T22:01:39.263317: step 4115, loss 0.0202024, acc 1
2016-09-07T22:01:39.905162: step 4116, loss 0.030199, acc 0.98
2016-09-07T22:01:40.569966: step 4117, loss 0.00653043, acc 1
2016-09-07T22:01:41.227459: step 4118, loss 0.0112296, acc 1
2016-09-07T22:01:41.918052: step 4119, loss 0.0345825, acc 0.98
2016-09-07T22:01:42.583854: step 4120, loss 0.0448987, acc 0.98
2016-09-07T22:01:43.225610: step 4121, loss 0.0137828, acc 1
2016-09-07T22:01:43.879285: step 4122, loss 0.0692327, acc 0.96
2016-09-07T22:01:44.538721: step 4123, loss 0.0297507, acc 0.98
2016-09-07T22:01:45.218827: step 4124, loss 0.0340979, acc 0.98
2016-09-07T22:01:45.891034: step 4125, loss 0.116712, acc 0.98
2016-09-07T22:01:46.568469: step 4126, loss 0.0119165, acc 1
2016-09-07T22:01:47.239051: step 4127, loss 0.0372986, acc 0.98
2016-09-07T22:01:47.907418: step 4128, loss 0.0267137, acc 0.98
2016-09-07T22:01:48.587096: step 4129, loss 0.0594213, acc 0.94
2016-09-07T22:01:49.263245: step 4130, loss 0.211797, acc 0.98
2016-09-07T22:01:49.946459: step 4131, loss 0.0459968, acc 0.96
2016-09-07T22:01:50.606064: step 4132, loss 0.0040632, acc 1
2016-09-07T22:01:51.306964: step 4133, loss 0.0126793, acc 1
2016-09-07T22:01:51.973180: step 4134, loss 0.0230346, acc 0.98
2016-09-07T22:01:52.634399: step 4135, loss 0.0500735, acc 0.98
2016-09-07T22:01:53.330142: step 4136, loss 0.0440957, acc 0.98
2016-09-07T22:01:54.004139: step 4137, loss 0.0429822, acc 0.98
2016-09-07T22:01:54.684966: step 4138, loss 0.034268, acc 1
2016-09-07T22:01:55.348388: step 4139, loss 0.0413283, acc 0.98
2016-09-07T22:01:56.043175: step 4140, loss 0.0317127, acc 1
2016-09-07T22:01:56.700277: step 4141, loss 0.0151527, acc 1
2016-09-07T22:01:57.367743: step 4142, loss 0.0134691, acc 1
2016-09-07T22:01:58.038176: step 4143, loss 0.0447563, acc 0.98
2016-09-07T22:01:58.719127: step 4144, loss 0.0423787, acc 0.98
2016-09-07T22:01:59.403684: step 4145, loss 0.0157055, acc 1
2016-09-07T22:02:00.070206: step 4146, loss 0.0241209, acc 0.98
2016-09-07T22:02:00.788784: step 4147, loss 0.0215404, acc 1
2016-09-07T22:02:01.466929: step 4148, loss 0.0167953, acc 1
2016-09-07T22:02:02.147033: step 4149, loss 0.0160398, acc 1
2016-09-07T22:02:02.822033: step 4150, loss 0.0426681, acc 0.96
2016-09-07T22:02:03.485331: step 4151, loss 0.0522203, acc 0.98
2016-09-07T22:02:04.166182: step 4152, loss 0.0188464, acc 1
2016-09-07T22:02:04.834477: step 4153, loss 0.0363557, acc 0.96
2016-09-07T22:02:05.494267: step 4154, loss 0.0477836, acc 0.96
2016-09-07T22:02:06.156063: step 4155, loss 0.0549413, acc 0.96
2016-09-07T22:02:06.829434: step 4156, loss 0.00854433, acc 1
2016-09-07T22:02:07.514696: step 4157, loss 0.0228711, acc 1
2016-09-07T22:02:08.172770: step 4158, loss 0.0127998, acc 1
2016-09-07T22:02:08.840646: step 4159, loss 0.0105114, acc 1
2016-09-07T22:02:09.523795: step 4160, loss 0.0115836, acc 1
2016-09-07T22:02:10.172107: step 4161, loss 0.0534604, acc 0.98
2016-09-07T22:02:10.853525: step 4162, loss 0.0448308, acc 0.96
2016-09-07T22:02:11.529294: step 4163, loss 0.00725882, acc 1
2016-09-07T22:02:12.201760: step 4164, loss 0.0464689, acc 0.98
2016-09-07T22:02:12.861326: step 4165, loss 0.025632, acc 1
2016-09-07T22:02:13.528354: step 4166, loss 0.0630292, acc 0.96
2016-09-07T22:02:14.209304: step 4167, loss 0.0600106, acc 0.96
2016-09-07T22:02:14.880644: step 4168, loss 0.00851304, acc 1
2016-09-07T22:02:15.526614: step 4169, loss 0.0730728, acc 0.98
2016-09-07T22:02:16.190114: step 4170, loss 0.00741432, acc 1
2016-09-07T22:02:16.858165: step 4171, loss 0.00516146, acc 1
2016-09-07T22:02:17.520499: step 4172, loss 0.0410941, acc 0.96
2016-09-07T22:02:18.193341: step 4173, loss 0.072673, acc 0.94
2016-09-07T22:02:18.847083: step 4174, loss 0.00680174, acc 1
2016-09-07T22:02:19.502446: step 4175, loss 0.0056405, acc 1
2016-09-07T22:02:20.168206: step 4176, loss 0.0794972, acc 0.96
2016-09-07T22:02:20.857825: step 4177, loss 0.0315956, acc 1
2016-09-07T22:02:21.524161: step 4178, loss 0.185789, acc 0.96
2016-09-07T22:02:22.198687: step 4179, loss 0.0394701, acc 0.98
2016-09-07T22:02:22.890050: step 4180, loss 0.0307892, acc 0.98
2016-09-07T22:02:23.573579: step 4181, loss 0.0197275, acc 1
2016-09-07T22:02:24.242755: step 4182, loss 0.0885397, acc 0.96
2016-09-07T22:02:24.910257: step 4183, loss 0.0221596, acc 1
2016-09-07T22:02:25.575794: step 4184, loss 0.00479166, acc 1
2016-09-07T22:02:26.228044: step 4185, loss 0.0154146, acc 1
2016-09-07T22:02:26.898104: step 4186, loss 0.0206643, acc 0.98
2016-09-07T22:02:27.574112: step 4187, loss 0.0393628, acc 0.98
2016-09-07T22:02:28.242723: step 4188, loss 0.0326418, acc 1
2016-09-07T22:02:28.917729: step 4189, loss 0.0805157, acc 0.96
2016-09-07T22:02:29.589602: step 4190, loss 0.0667675, acc 0.96
2016-09-07T22:02:30.247059: step 4191, loss 0.117183, acc 0.96
2016-09-07T22:02:30.906177: step 4192, loss 0.0230982, acc 1
2016-09-07T22:02:31.571961: step 4193, loss 0.0722147, acc 0.98
2016-09-07T22:02:32.247564: step 4194, loss 0.0382256, acc 1
2016-09-07T22:02:32.918220: step 4195, loss 0.0162316, acc 1
2016-09-07T22:02:33.592011: step 4196, loss 0.026451, acc 0.98
2016-09-07T22:02:34.263466: step 4197, loss 0.0805033, acc 0.94
2016-09-07T22:02:34.933970: step 4198, loss 0.0398233, acc 0.98
2016-09-07T22:02:35.605097: step 4199, loss 0.0310755, acc 1
2016-09-07T22:02:36.274910: step 4200, loss 0.0420646, acc 1

Evaluation:
2016-09-07T22:02:39.164399: step 4200, loss 1.87543, acc 0.747

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-4200

2016-09-07T22:02:40.878779: step 4201, loss 0.0672695, acc 0.98
2016-09-07T22:02:41.531737: step 4202, loss 0.00667823, acc 1
2016-09-07T22:02:42.182124: step 4203, loss 0.0239434, acc 1
2016-09-07T22:02:42.839989: step 4204, loss 0.119676, acc 0.96
2016-09-07T22:02:43.505712: step 4205, loss 0.102773, acc 0.94
2016-09-07T22:02:44.174400: step 4206, loss 0.067434, acc 0.98
2016-09-07T22:02:44.844622: step 4207, loss 0.0277519, acc 0.98
2016-09-07T22:02:45.504986: step 4208, loss 0.0375452, acc 1
2016-09-07T22:02:46.163391: step 4209, loss 0.0455461, acc 0.98
2016-09-07T22:02:46.817515: step 4210, loss 0.163244, acc 0.94
2016-09-07T22:02:47.477421: step 4211, loss 0.0108063, acc 1
2016-09-07T22:02:48.150640: step 4212, loss 0.00769484, acc 1
2016-09-07T22:02:48.824876: step 4213, loss 0.0163057, acc 1
2016-09-07T22:02:49.496588: step 4214, loss 0.0329293, acc 1
2016-09-07T22:02:50.177601: step 4215, loss 0.00722248, acc 1
2016-09-07T22:02:50.859219: step 4216, loss 0.0316265, acc 1
2016-09-07T22:02:51.516361: step 4217, loss 0.105218, acc 0.92
2016-09-07T22:02:52.182415: step 4218, loss 0.0314983, acc 1
2016-09-07T22:02:52.846266: step 4219, loss 0.0225977, acc 1
2016-09-07T22:02:53.524638: step 4220, loss 0.0116828, acc 1
2016-09-07T22:02:54.194006: step 4221, loss 0.0501541, acc 1
2016-09-07T22:02:54.850622: step 4222, loss 0.0428341, acc 0.98
2016-09-07T22:02:55.514223: step 4223, loss 0.0290355, acc 0.98
2016-09-07T22:02:56.196081: step 4224, loss 0.0222954, acc 0.98
2016-09-07T22:02:56.847380: step 4225, loss 0.12239, acc 0.96
2016-09-07T22:02:57.525462: step 4226, loss 0.0516521, acc 0.98
2016-09-07T22:02:58.191801: step 4227, loss 0.0329773, acc 1
2016-09-07T22:02:58.863629: step 4228, loss 0.0557071, acc 0.96
2016-09-07T22:02:59.538911: step 4229, loss 0.0265393, acc 0.98
2016-09-07T22:03:00.241550: step 4230, loss 0.0138538, acc 1
2016-09-07T22:03:00.905669: step 4231, loss 0.00764738, acc 1
2016-09-07T22:03:01.566694: step 4232, loss 0.035774, acc 0.98
2016-09-07T22:03:02.226685: step 4233, loss 0.0549414, acc 0.98
2016-09-07T22:03:02.893070: step 4234, loss 0.0241322, acc 0.98
2016-09-07T22:03:03.551475: step 4235, loss 0.0226424, acc 1
2016-09-07T22:03:04.223145: step 4236, loss 0.0358991, acc 0.96
2016-09-07T22:03:04.901206: step 4237, loss 0.0307715, acc 1
2016-09-07T22:03:05.573141: step 4238, loss 0.0158186, acc 1
2016-09-07T22:03:06.246667: step 4239, loss 0.034783, acc 0.98
2016-09-07T22:03:06.920038: step 4240, loss 0.0444542, acc 1
2016-09-07T22:03:07.590658: step 4241, loss 0.0288744, acc 0.98
2016-09-07T22:03:08.258651: step 4242, loss 0.018048, acc 1
2016-09-07T22:03:08.926241: step 4243, loss 0.036142, acc 0.98
2016-09-07T22:03:09.597985: step 4244, loss 0.20913, acc 0.96
2016-09-07T22:03:10.273980: step 4245, loss 0.102388, acc 0.94
2016-09-07T22:03:10.929546: step 4246, loss 0.0331024, acc 1
2016-09-07T22:03:11.588199: step 4247, loss 0.0157313, acc 1
2016-09-07T22:03:12.256117: step 4248, loss 0.0246674, acc 0.98
2016-09-07T22:03:12.920010: step 4249, loss 0.0709403, acc 0.96
2016-09-07T22:03:13.588529: step 4250, loss 0.063905, acc 0.98
2016-09-07T22:03:14.267078: step 4251, loss 0.0711768, acc 0.98
2016-09-07T22:03:14.953258: step 4252, loss 0.0109913, acc 1
2016-09-07T22:03:15.629938: step 4253, loss 0.00916488, acc 1
2016-09-07T22:03:16.311481: step 4254, loss 0.0275452, acc 0.98
2016-09-07T22:03:16.993139: step 4255, loss 0.0178661, acc 1
2016-09-07T22:03:17.672447: step 4256, loss 0.00765474, acc 1
2016-09-07T22:03:18.353854: step 4257, loss 0.0241265, acc 0.98
2016-09-07T22:03:19.037544: step 4258, loss 0.0454396, acc 0.98
2016-09-07T22:03:19.709881: step 4259, loss 0.0227792, acc 0.98
2016-09-07T22:03:20.380685: step 4260, loss 0.0271944, acc 1
2016-09-07T22:03:21.029637: step 4261, loss 0.0381212, acc 0.98
2016-09-07T22:03:21.695574: step 4262, loss 0.00568538, acc 1
2016-09-07T22:03:22.362652: step 4263, loss 0.021616, acc 1
2016-09-07T22:03:23.034962: step 4264, loss 0.0190703, acc 1
2016-09-07T22:03:23.733887: step 4265, loss 0.120052, acc 0.96
2016-09-07T22:03:24.398475: step 4266, loss 0.00657486, acc 1
2016-09-07T22:03:25.070816: step 4267, loss 0.00567698, acc 1
2016-09-07T22:03:25.407601: step 4268, loss 0.00660486, acc 1
2016-09-07T22:03:26.079818: step 4269, loss 0.0186242, acc 1
2016-09-07T22:03:26.739780: step 4270, loss 0.0125803, acc 1
2016-09-07T22:03:27.407636: step 4271, loss 0.064022, acc 0.96
2016-09-07T22:03:28.065768: step 4272, loss 0.00973538, acc 1
2016-09-07T22:03:28.728124: step 4273, loss 0.0183666, acc 1
2016-09-07T22:03:29.406170: step 4274, loss 0.0569068, acc 0.96
2016-09-07T22:03:30.082054: step 4275, loss 0.0299029, acc 0.98
2016-09-07T22:03:30.746118: step 4276, loss 0.0455885, acc 0.98
2016-09-07T22:03:31.394750: step 4277, loss 0.0101617, acc 1
2016-09-07T22:03:32.065235: step 4278, loss 0.0584405, acc 0.96
2016-09-07T22:03:32.754983: step 4279, loss 0.0502696, acc 0.98
2016-09-07T22:03:33.466020: step 4280, loss 0.0614342, acc 0.94
2016-09-07T22:03:34.141795: step 4281, loss 0.0386117, acc 0.96
2016-09-07T22:03:34.814997: step 4282, loss 0.0924213, acc 0.98
2016-09-07T22:03:35.488150: step 4283, loss 0.0284615, acc 0.98
2016-09-07T22:03:36.158811: step 4284, loss 0.0107127, acc 1
2016-09-07T22:03:36.819818: step 4285, loss 0.0480987, acc 0.98
2016-09-07T22:03:37.486928: step 4286, loss 0.00737624, acc 1
2016-09-07T22:03:38.169384: step 4287, loss 0.0520062, acc 0.96
2016-09-07T22:03:38.823012: step 4288, loss 0.0448833, acc 0.98
2016-09-07T22:03:39.490868: step 4289, loss 0.0321497, acc 1
2016-09-07T22:03:40.185061: step 4290, loss 0.0398888, acc 0.96
2016-09-07T22:03:40.844624: step 4291, loss 0.0495894, acc 0.98
2016-09-07T22:03:41.514184: step 4292, loss 0.00501735, acc 1
2016-09-07T22:03:42.182136: step 4293, loss 0.0648974, acc 0.96
2016-09-07T22:03:42.848915: step 4294, loss 0.0229532, acc 1
2016-09-07T22:03:43.516170: step 4295, loss 0.00795882, acc 1
2016-09-07T22:03:44.199029: step 4296, loss 0.0154081, acc 1
2016-09-07T22:03:44.858222: step 4297, loss 0.0208715, acc 0.98
2016-09-07T22:03:45.519595: step 4298, loss 0.00818919, acc 1
2016-09-07T22:03:46.167992: step 4299, loss 0.0174521, acc 1
2016-09-07T22:03:46.830087: step 4300, loss 0.0873916, acc 0.92

Evaluation:
2016-09-07T22:03:49.716955: step 4300, loss 1.94095, acc 0.734

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-4300

2016-09-07T22:03:51.294613: step 4301, loss 0.0629605, acc 0.98
2016-09-07T22:03:51.945205: step 4302, loss 0.0250692, acc 1
2016-09-07T22:03:52.610245: step 4303, loss 0.0042386, acc 1
2016-09-07T22:03:53.288472: step 4304, loss 0.0534628, acc 0.96
2016-09-07T22:03:53.943328: step 4305, loss 0.010905, acc 1
2016-09-07T22:03:54.616964: step 4306, loss 0.0695473, acc 0.96
2016-09-07T22:03:55.295186: step 4307, loss 0.00514693, acc 1
2016-09-07T22:03:55.964555: step 4308, loss 0.00444842, acc 1
2016-09-07T22:03:56.629502: step 4309, loss 0.0544777, acc 0.98
2016-09-07T22:03:57.310164: step 4310, loss 0.00908902, acc 1
2016-09-07T22:03:57.951824: step 4311, loss 0.0633064, acc 0.98
2016-09-07T22:03:58.607128: step 4312, loss 0.113562, acc 0.96
2016-09-07T22:03:59.275839: step 4313, loss 0.00753005, acc 1
2016-09-07T22:03:59.916904: step 4314, loss 0.0354219, acc 0.98
2016-09-07T22:04:00.618781: step 4315, loss 0.0232928, acc 1
2016-09-07T22:04:01.284418: step 4316, loss 0.00823771, acc 1
2016-09-07T22:04:01.945238: step 4317, loss 0.0169793, acc 1
2016-09-07T22:04:02.620133: step 4318, loss 0.0402225, acc 0.98
2016-09-07T22:04:03.271041: step 4319, loss 0.0174754, acc 1
2016-09-07T22:04:03.925412: step 4320, loss 0.0211073, acc 0.98
2016-09-07T22:04:04.589051: step 4321, loss 0.0198756, acc 1
2016-09-07T22:04:05.265042: step 4322, loss 0.00942462, acc 1
2016-09-07T22:04:05.930742: step 4323, loss 0.0355819, acc 0.96
2016-09-07T22:04:06.603397: step 4324, loss 0.020249, acc 1
2016-09-07T22:04:07.291713: step 4325, loss 0.0104638, acc 1
2016-09-07T22:04:07.971382: step 4326, loss 0.00850541, acc 1
2016-09-07T22:04:08.657545: step 4327, loss 0.0722337, acc 0.98
2016-09-07T22:04:09.331916: step 4328, loss 0.0269275, acc 0.98
2016-09-07T22:04:10.026925: step 4329, loss 0.0885256, acc 0.96
2016-09-07T22:04:10.698514: step 4330, loss 0.0635728, acc 0.98
2016-09-07T22:04:11.391288: step 4331, loss 0.0339082, acc 1
2016-09-07T22:04:12.048924: step 4332, loss 0.0547774, acc 0.96
2016-09-07T22:04:12.710427: step 4333, loss 0.0405576, acc 1
2016-09-07T22:04:13.378982: step 4334, loss 0.0604405, acc 0.98
2016-09-07T22:04:14.022423: step 4335, loss 0.0251365, acc 1
2016-09-07T22:04:14.691578: step 4336, loss 0.0338863, acc 0.98
2016-09-07T22:04:15.341626: step 4337, loss 0.00588936, acc 1
2016-09-07T22:04:16.022950: step 4338, loss 0.0494058, acc 0.96
2016-09-07T22:04:16.691660: step 4339, loss 0.017224, acc 1
2016-09-07T22:04:17.364062: step 4340, loss 0.0261581, acc 0.98
2016-09-07T22:04:18.035913: step 4341, loss 0.00440048, acc 1
2016-09-07T22:04:18.700222: step 4342, loss 0.0143249, acc 1
2016-09-07T22:04:19.370570: step 4343, loss 0.0256021, acc 1
2016-09-07T22:04:20.032390: step 4344, loss 0.0436979, acc 0.98
2016-09-07T22:04:20.687955: step 4345, loss 0.028726, acc 1
2016-09-07T22:04:21.374055: step 4346, loss 0.0630497, acc 0.96
2016-09-07T22:04:22.053048: step 4347, loss 0.0476091, acc 0.98
2016-09-07T22:04:22.715025: step 4348, loss 0.0171831, acc 1
2016-09-07T22:04:23.379730: step 4349, loss 0.0468597, acc 0.98
2016-09-07T22:04:24.056239: step 4350, loss 0.0307648, acc 0.98
2016-09-07T22:04:24.730030: step 4351, loss 0.105374, acc 0.94
2016-09-07T22:04:25.398017: step 4352, loss 0.00807522, acc 1
2016-09-07T22:04:26.066801: step 4353, loss 0.00423769, acc 1
2016-09-07T22:04:26.742593: step 4354, loss 0.0314996, acc 0.98
2016-09-07T22:04:27.399377: step 4355, loss 0.0231089, acc 0.98
2016-09-07T22:04:28.078112: step 4356, loss 0.019316, acc 1
2016-09-07T22:04:28.754323: step 4357, loss 0.0263854, acc 1
2016-09-07T22:04:29.422002: step 4358, loss 0.0585367, acc 0.96
2016-09-07T22:04:30.102608: step 4359, loss 0.068382, acc 0.98
2016-09-07T22:04:30.792513: step 4360, loss 0.00413419, acc 1
2016-09-07T22:04:31.461908: step 4361, loss 0.0507029, acc 0.98
2016-09-07T22:04:32.132608: step 4362, loss 0.0347243, acc 0.98
2016-09-07T22:04:32.782169: step 4363, loss 0.0419104, acc 0.96
2016-09-07T22:04:33.481275: step 4364, loss 0.00901056, acc 1
2016-09-07T22:04:34.150813: step 4365, loss 0.0045975, acc 1
2016-09-07T22:04:34.804322: step 4366, loss 0.0301163, acc 1
2016-09-07T22:04:35.470766: step 4367, loss 0.0292819, acc 0.98
2016-09-07T22:04:36.151804: step 4368, loss 0.0296233, acc 1
2016-09-07T22:04:36.835802: step 4369, loss 0.00608867, acc 1
2016-09-07T22:04:37.507197: step 4370, loss 0.0194083, acc 1
2016-09-07T22:04:38.174603: step 4371, loss 0.0819452, acc 0.96
2016-09-07T22:04:38.845404: step 4372, loss 0.105679, acc 0.96
2016-09-07T22:04:39.504765: step 4373, loss 0.0131795, acc 1
2016-09-07T22:04:40.165794: step 4374, loss 0.0260716, acc 0.98
2016-09-07T22:04:40.834163: step 4375, loss 0.175965, acc 0.98
2016-09-07T22:04:41.490941: step 4376, loss 0.0460475, acc 0.98
2016-09-07T22:04:42.158167: step 4377, loss 0.0375231, acc 0.98
2016-09-07T22:04:42.825995: step 4378, loss 0.0072422, acc 1
2016-09-07T22:04:43.481446: step 4379, loss 0.0399344, acc 0.96
2016-09-07T22:04:44.152531: step 4380, loss 0.00513452, acc 1
2016-09-07T22:04:44.808762: step 4381, loss 0.00584483, acc 1
2016-09-07T22:04:45.474172: step 4382, loss 0.0174794, acc 1
2016-09-07T22:04:46.131584: step 4383, loss 0.0102503, acc 1
2016-09-07T22:04:46.808064: step 4384, loss 0.0646791, acc 0.94
2016-09-07T22:04:47.490438: step 4385, loss 0.0439086, acc 0.98
2016-09-07T22:04:48.147254: step 4386, loss 0.0849059, acc 0.94
2016-09-07T22:04:48.822271: step 4387, loss 0.0166269, acc 1
2016-09-07T22:04:49.490339: step 4388, loss 0.0405126, acc 1
2016-09-07T22:04:50.159630: step 4389, loss 0.0217828, acc 0.98
2016-09-07T22:04:50.817466: step 4390, loss 0.0269181, acc 1
2016-09-07T22:04:51.464658: step 4391, loss 0.030486, acc 0.98
2016-09-07T22:04:52.120045: step 4392, loss 0.0232431, acc 0.98
2016-09-07T22:04:52.817592: step 4393, loss 0.0543142, acc 0.98
2016-09-07T22:04:53.493411: step 4394, loss 0.112952, acc 0.98
2016-09-07T22:04:54.154437: step 4395, loss 0.0438146, acc 1
2016-09-07T22:04:54.801031: step 4396, loss 0.0425616, acc 0.98
2016-09-07T22:04:55.485620: step 4397, loss 0.0083967, acc 1
2016-09-07T22:04:56.169649: step 4398, loss 0.0489654, acc 0.96
2016-09-07T22:04:56.834790: step 4399, loss 0.0580002, acc 1
2016-09-07T22:04:57.524200: step 4400, loss 0.14309, acc 0.96

Evaluation:
2016-09-07T22:05:00.427331: step 4400, loss 1.75411, acc 0.753

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-4400

2016-09-07T22:05:02.076973: step 4401, loss 0.0457553, acc 0.98
2016-09-07T22:05:02.727569: step 4402, loss 0.0309888, acc 1
2016-09-07T22:05:03.385964: step 4403, loss 0.0289974, acc 1
2016-09-07T22:05:04.045972: step 4404, loss 0.0129589, acc 1
2016-09-07T22:05:04.705496: step 4405, loss 0.0708025, acc 0.98
2016-09-07T22:05:05.352324: step 4406, loss 0.0217843, acc 0.98
2016-09-07T22:05:06.005273: step 4407, loss 0.0171297, acc 1
2016-09-07T22:05:06.680128: step 4408, loss 0.0823324, acc 0.96
2016-09-07T22:05:07.375153: step 4409, loss 0.0189348, acc 1
2016-09-07T22:05:08.047466: step 4410, loss 0.0371709, acc 0.98
2016-09-07T22:05:08.719370: step 4411, loss 0.103665, acc 0.98
2016-09-07T22:05:09.372121: step 4412, loss 0.015849, acc 1
2016-09-07T22:05:10.026499: step 4413, loss 0.0200122, acc 0.98
2016-09-07T22:05:10.684829: step 4414, loss 0.0334213, acc 0.98
2016-09-07T22:05:11.338371: step 4415, loss 0.00919092, acc 1
2016-09-07T22:05:12.015379: step 4416, loss 0.0363305, acc 1
2016-09-07T22:05:12.682567: step 4417, loss 0.0163559, acc 1
2016-09-07T22:05:13.337519: step 4418, loss 0.0148831, acc 1
2016-09-07T22:05:14.010087: step 4419, loss 0.0232109, acc 0.98
2016-09-07T22:05:14.698639: step 4420, loss 0.0640045, acc 0.96
2016-09-07T22:05:15.372242: step 4421, loss 0.0218714, acc 0.98
2016-09-07T22:05:16.045095: step 4422, loss 0.0315334, acc 1
2016-09-07T22:05:16.725993: step 4423, loss 0.010504, acc 1
2016-09-07T22:05:17.392146: step 4424, loss 0.0258582, acc 0.98
2016-09-07T22:05:18.052562: step 4425, loss 0.00761057, acc 1
2016-09-07T22:05:18.725003: step 4426, loss 0.0608395, acc 0.96
2016-09-07T22:05:19.392626: step 4427, loss 0.0419795, acc 1
2016-09-07T22:05:20.072994: step 4428, loss 0.0144887, acc 1
2016-09-07T22:05:20.746573: step 4429, loss 0.0695561, acc 0.94
2016-09-07T22:05:21.393692: step 4430, loss 0.0236063, acc 0.98
2016-09-07T22:05:22.073601: step 4431, loss 0.0178952, acc 1
2016-09-07T22:05:22.734472: step 4432, loss 0.137179, acc 0.98
2016-09-07T22:05:23.392400: step 4433, loss 0.0102531, acc 1
2016-09-07T22:05:24.066773: step 4434, loss 0.0490211, acc 0.98
2016-09-07T22:05:24.717586: step 4435, loss 0.0878106, acc 0.98
2016-09-07T22:05:25.372299: step 4436, loss 0.0234151, acc 0.98
2016-09-07T22:05:26.046701: step 4437, loss 0.0182572, acc 1
2016-09-07T22:05:26.710934: step 4438, loss 0.0796156, acc 0.94
2016-09-07T22:05:27.377665: step 4439, loss 0.0277479, acc 1
2016-09-07T22:05:28.054583: step 4440, loss 0.0262991, acc 0.98
2016-09-07T22:05:28.732155: step 4441, loss 0.0490511, acc 0.98
2016-09-07T22:05:29.403523: step 4442, loss 0.00619639, acc 1
2016-09-07T22:05:30.076524: step 4443, loss 0.0699131, acc 0.98
2016-09-07T22:05:30.738316: step 4444, loss 0.0144652, acc 1
2016-09-07T22:05:31.392832: step 4445, loss 0.0717793, acc 0.96
2016-09-07T22:05:32.061522: step 4446, loss 0.0244949, acc 0.98
2016-09-07T22:05:32.728670: step 4447, loss 0.0204264, acc 1
2016-09-07T22:05:33.412209: step 4448, loss 0.0192098, acc 1
2016-09-07T22:05:34.072976: step 4449, loss 0.0840297, acc 0.94
2016-09-07T22:05:34.744696: step 4450, loss 0.0894033, acc 0.98
2016-09-07T22:05:35.434814: step 4451, loss 0.025204, acc 0.98
2016-09-07T22:05:36.112625: step 4452, loss 0.0349458, acc 1
2016-09-07T22:05:36.784534: step 4453, loss 0.0106659, acc 1
2016-09-07T22:05:37.448964: step 4454, loss 0.0114539, acc 1
2016-09-07T22:05:38.160173: step 4455, loss 0.0599635, acc 0.98
2016-09-07T22:05:38.817959: step 4456, loss 0.0305943, acc 0.98
2016-09-07T22:05:39.475803: step 4457, loss 0.0341302, acc 1
2016-09-07T22:05:40.134989: step 4458, loss 0.0240708, acc 1
2016-09-07T22:05:40.812587: step 4459, loss 0.0911806, acc 0.94
2016-09-07T22:05:41.471494: step 4460, loss 0.0117152, acc 1
2016-09-07T22:05:42.139034: step 4461, loss 0.00852077, acc 1
2016-09-07T22:05:42.482925: step 4462, loss 0.0978849, acc 1
2016-09-07T22:05:43.161648: step 4463, loss 0.0450095, acc 0.98
2016-09-07T22:05:43.820402: step 4464, loss 0.0268372, acc 0.98
2016-09-07T22:05:44.495332: step 4465, loss 0.0130948, acc 1
2016-09-07T22:05:45.176061: step 4466, loss 0.0263823, acc 1
2016-09-07T22:05:45.845547: step 4467, loss 0.0383987, acc 0.98
2016-09-07T22:05:46.501134: step 4468, loss 0.038249, acc 1
2016-09-07T22:05:47.169312: step 4469, loss 0.024901, acc 1
2016-09-07T22:05:47.849750: step 4470, loss 0.0252548, acc 0.98
2016-09-07T22:05:48.530898: step 4471, loss 0.0139636, acc 1
2016-09-07T22:05:49.191539: step 4472, loss 0.0195296, acc 1
2016-09-07T22:05:49.850463: step 4473, loss 0.0776719, acc 0.96
2016-09-07T22:05:50.517862: step 4474, loss 0.0410107, acc 0.98
2016-09-07T22:05:51.184700: step 4475, loss 0.0851123, acc 0.94
2016-09-07T22:05:51.863943: step 4476, loss 0.0103806, acc 1
2016-09-07T22:05:52.522748: step 4477, loss 0.0302768, acc 0.98
2016-09-07T22:05:53.194172: step 4478, loss 0.0112332, acc 1
2016-09-07T22:05:53.881125: step 4479, loss 0.00818483, acc 1
2016-09-07T22:05:54.567431: step 4480, loss 0.0176467, acc 1
2016-09-07T22:05:55.251282: step 4481, loss 0.0059747, acc 1
2016-09-07T22:05:55.925954: step 4482, loss 0.0197784, acc 0.98
2016-09-07T22:05:56.579708: step 4483, loss 0.0266193, acc 1
2016-09-07T22:05:57.249240: step 4484, loss 0.0350317, acc 0.98
2016-09-07T22:05:57.914431: step 4485, loss 0.0636902, acc 0.96
2016-09-07T22:05:58.594468: step 4486, loss 0.0147328, acc 1
2016-09-07T22:05:59.269428: step 4487, loss 0.0690278, acc 0.94
2016-09-07T22:05:59.926724: step 4488, loss 0.0084407, acc 1
2016-09-07T22:06:00.619492: step 4489, loss 0.0124449, acc 1
2016-09-07T22:06:01.308223: step 4490, loss 0.0246542, acc 0.98
2016-09-07T22:06:01.980998: step 4491, loss 0.0813786, acc 0.98
2016-09-07T22:06:02.648051: step 4492, loss 0.01402, acc 1
2016-09-07T22:06:03.326092: step 4493, loss 0.00860391, acc 1
2016-09-07T22:06:04.003264: step 4494, loss 0.0448876, acc 0.96
2016-09-07T22:06:04.659532: step 4495, loss 0.0103004, acc 1
2016-09-07T22:06:05.346741: step 4496, loss 0.0846514, acc 0.98
2016-09-07T22:06:06.021028: step 4497, loss 0.0106253, acc 1
2016-09-07T22:06:06.704158: step 4498, loss 0.0573878, acc 0.98
2016-09-07T22:06:07.380890: step 4499, loss 0.017379, acc 1
2016-09-07T22:06:08.044604: step 4500, loss 0.0354957, acc 0.98

Evaluation:
2016-09-07T22:06:10.923725: step 4500, loss 1.85337, acc 0.747

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-4500

2016-09-07T22:06:12.690127: step 4501, loss 0.0336947, acc 0.98
2016-09-07T22:06:13.349961: step 4502, loss 0.0274509, acc 0.98
2016-09-07T22:06:14.022433: step 4503, loss 0.0692278, acc 0.98
2016-09-07T22:06:14.690390: step 4504, loss 0.00651685, acc 1
2016-09-07T22:06:15.350692: step 4505, loss 0.0198035, acc 1
2016-09-07T22:06:16.028016: step 4506, loss 0.0281556, acc 0.98
2016-09-07T22:06:16.692601: step 4507, loss 0.0479274, acc 0.96
2016-09-07T22:06:17.383266: step 4508, loss 0.0254176, acc 1
2016-09-07T22:06:18.064046: step 4509, loss 0.0188219, acc 1
2016-09-07T22:06:18.731019: step 4510, loss 0.0284706, acc 1
2016-09-07T22:06:19.399966: step 4511, loss 0.0231033, acc 1
2016-09-07T22:06:20.060794: step 4512, loss 0.0908583, acc 0.98
2016-09-07T22:06:20.718665: step 4513, loss 0.106971, acc 0.94
2016-09-07T22:06:21.372915: step 4514, loss 0.0396254, acc 1
2016-09-07T22:06:22.015420: step 4515, loss 0.00679693, acc 1
2016-09-07T22:06:22.667585: step 4516, loss 0.0610807, acc 0.98
2016-09-07T22:06:23.329503: step 4517, loss 0.00681226, acc 1
2016-09-07T22:06:24.023953: step 4518, loss 0.0176646, acc 1
2016-09-07T22:06:24.692739: step 4519, loss 0.0611101, acc 0.94
2016-09-07T22:06:25.366046: step 4520, loss 0.0667504, acc 0.96
2016-09-07T22:06:26.041041: step 4521, loss 0.0804383, acc 0.96
2016-09-07T22:06:26.711410: step 4522, loss 0.0255552, acc 1
2016-09-07T22:06:27.372511: step 4523, loss 0.0185448, acc 1
2016-09-07T22:06:28.039272: step 4524, loss 0.00611975, acc 1
2016-09-07T22:06:28.701677: step 4525, loss 0.0277554, acc 0.98
2016-09-07T22:06:29.372049: step 4526, loss 0.0783381, acc 0.96
2016-09-07T22:06:30.047384: step 4527, loss 0.0274037, acc 1
2016-09-07T22:06:30.707402: step 4528, loss 0.0694821, acc 0.96
2016-09-07T22:06:31.374304: step 4529, loss 0.0583052, acc 0.98
2016-09-07T22:06:32.033037: step 4530, loss 0.00470507, acc 1
2016-09-07T22:06:32.690725: step 4531, loss 0.043098, acc 0.98
2016-09-07T22:06:33.358182: step 4532, loss 0.036345, acc 0.98
2016-09-07T22:06:34.047168: step 4533, loss 0.0493803, acc 0.96
2016-09-07T22:06:34.711945: step 4534, loss 0.0140085, acc 1
2016-09-07T22:06:35.381926: step 4535, loss 0.0126659, acc 1
2016-09-07T22:06:36.031987: step 4536, loss 0.0276072, acc 1
2016-09-07T22:06:36.691151: step 4537, loss 0.00856461, acc 1
2016-09-07T22:06:37.378898: step 4538, loss 0.0677044, acc 0.94
2016-09-07T22:06:38.046303: step 4539, loss 0.0422516, acc 0.98
2016-09-07T22:06:38.719163: step 4540, loss 0.0365053, acc 0.98
2016-09-07T22:06:39.391672: step 4541, loss 0.0741182, acc 0.96
2016-09-07T22:06:40.076905: step 4542, loss 0.0257026, acc 0.98
2016-09-07T22:06:40.737107: step 4543, loss 0.0321937, acc 0.98
2016-09-07T22:06:41.386459: step 4544, loss 0.091194, acc 0.96
2016-09-07T22:06:42.058459: step 4545, loss 0.00879138, acc 1
2016-09-07T22:06:42.732507: step 4546, loss 0.0157129, acc 1
2016-09-07T22:06:43.396500: step 4547, loss 0.023874, acc 0.98
2016-09-07T22:06:44.063262: step 4548, loss 0.00781271, acc 1
2016-09-07T22:06:44.733143: step 4549, loss 0.0214004, acc 0.98
2016-09-07T22:06:45.393208: step 4550, loss 0.0328072, acc 0.98
2016-09-07T22:06:46.054267: step 4551, loss 0.12695, acc 0.96
2016-09-07T22:06:46.694381: step 4552, loss 0.0165455, acc 1
2016-09-07T22:06:47.360525: step 4553, loss 0.0145794, acc 1
2016-09-07T22:06:48.046808: step 4554, loss 0.102736, acc 0.98
2016-09-07T22:06:48.725481: step 4555, loss 0.0597141, acc 0.98
2016-09-07T22:06:49.415219: step 4556, loss 0.0262198, acc 0.98
2016-09-07T22:06:50.098060: step 4557, loss 0.0526304, acc 0.98
2016-09-07T22:06:50.754612: step 4558, loss 0.0162891, acc 1
2016-09-07T22:06:51.420190: step 4559, loss 0.0408753, acc 0.98
2016-09-07T22:06:52.091656: step 4560, loss 0.040672, acc 0.98
2016-09-07T22:06:52.747041: step 4561, loss 0.035072, acc 1
2016-09-07T22:06:53.425593: step 4562, loss 0.0548967, acc 0.94
2016-09-07T22:06:54.108419: step 4563, loss 0.0369547, acc 1
2016-09-07T22:06:54.790394: step 4564, loss 0.0332071, acc 0.98
2016-09-07T22:06:55.469972: step 4565, loss 0.0244649, acc 0.98
2016-09-07T22:06:56.125376: step 4566, loss 0.0215144, acc 1
2016-09-07T22:06:56.786105: step 4567, loss 0.0255663, acc 1
2016-09-07T22:06:57.459759: step 4568, loss 0.0147857, acc 1
2016-09-07T22:06:58.129442: step 4569, loss 0.0289325, acc 0.98
2016-09-07T22:06:58.802851: step 4570, loss 0.0439694, acc 0.98
2016-09-07T22:06:59.469823: step 4571, loss 0.00924526, acc 1
2016-09-07T22:07:00.167365: step 4572, loss 0.0228632, acc 0.98
2016-09-07T22:07:00.873645: step 4573, loss 0.0308199, acc 1
2016-09-07T22:07:01.531434: step 4574, loss 0.0172399, acc 1
2016-09-07T22:07:02.194104: step 4575, loss 0.00998315, acc 1
2016-09-07T22:07:02.866323: step 4576, loss 0.0243853, acc 0.98
2016-09-07T22:07:03.547313: step 4577, loss 0.0165652, acc 1
2016-09-07T22:07:04.211861: step 4578, loss 0.0140827, acc 1
2016-09-07T22:07:04.884082: step 4579, loss 0.0280128, acc 1
2016-09-07T22:07:05.537791: step 4580, loss 0.0278902, acc 1
2016-09-07T22:07:06.199095: step 4581, loss 0.0160716, acc 1
2016-09-07T22:07:06.885339: step 4582, loss 0.084246, acc 0.96
2016-09-07T22:07:07.561398: step 4583, loss 0.0226453, acc 0.98
2016-09-07T22:07:08.268135: step 4584, loss 0.034575, acc 0.98
2016-09-07T22:07:08.921369: step 4585, loss 0.00465748, acc 1
2016-09-07T22:07:09.586000: step 4586, loss 0.00607654, acc 1
2016-09-07T22:07:10.242141: step 4587, loss 0.109309, acc 0.94
2016-09-07T22:07:10.908097: step 4588, loss 0.0374995, acc 0.98
2016-09-07T22:07:11.573967: step 4589, loss 0.0379502, acc 0.98
2016-09-07T22:07:12.254410: step 4590, loss 0.0335158, acc 0.98
2016-09-07T22:07:12.940297: step 4591, loss 0.00725953, acc 1
2016-09-07T22:07:13.617627: step 4592, loss 0.0076417, acc 1
2016-09-07T22:07:14.291432: step 4593, loss 0.0357269, acc 0.98
2016-09-07T22:07:14.968376: step 4594, loss 0.0430861, acc 0.96
2016-09-07T22:07:15.643654: step 4595, loss 0.00587443, acc 1
2016-09-07T22:07:16.313083: step 4596, loss 0.0348234, acc 1
2016-09-07T22:07:16.972467: step 4597, loss 0.0437111, acc 1
2016-09-07T22:07:17.646544: step 4598, loss 0.0260066, acc 0.98
2016-09-07T22:07:18.325633: step 4599, loss 0.00443238, acc 1
2016-09-07T22:07:19.004484: step 4600, loss 0.0335583, acc 0.98

Evaluation:
2016-09-07T22:07:21.902350: step 4600, loss 2.11649, acc 0.75

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-4600

2016-09-07T22:07:23.518585: step 4601, loss 0.0156556, acc 1
2016-09-07T22:07:24.185856: step 4602, loss 0.0718432, acc 0.96
2016-09-07T22:07:24.855911: step 4603, loss 0.0349422, acc 0.96
2016-09-07T22:07:25.537453: step 4604, loss 0.0148453, acc 1
2016-09-07T22:07:26.209500: step 4605, loss 0.078642, acc 0.98
2016-09-07T22:07:26.889252: step 4606, loss 0.0212903, acc 0.98
2016-09-07T22:07:27.530600: step 4607, loss 0.177454, acc 0.92
2016-09-07T22:07:28.192887: step 4608, loss 0.0704568, acc 0.98
2016-09-07T22:07:28.877901: step 4609, loss 0.0500428, acc 0.98
2016-09-07T22:07:29.537954: step 4610, loss 0.0505968, acc 0.98
2016-09-07T22:07:30.197438: step 4611, loss 0.112199, acc 0.98
2016-09-07T22:07:30.861571: step 4612, loss 0.0300855, acc 1
2016-09-07T22:07:31.524409: step 4613, loss 0.0272812, acc 1
2016-09-07T22:07:32.176586: step 4614, loss 0.036409, acc 1
2016-09-07T22:07:32.851715: step 4615, loss 0.0587284, acc 0.96
2016-09-07T22:07:33.518701: step 4616, loss 0.0474431, acc 0.98
2016-09-07T22:07:34.190060: step 4617, loss 0.136519, acc 0.94
2016-09-07T22:07:34.862588: step 4618, loss 0.0198141, acc 1
2016-09-07T22:07:35.538195: step 4619, loss 0.00862598, acc 1
2016-09-07T22:07:36.200370: step 4620, loss 0.0396295, acc 0.98
2016-09-07T22:07:36.873212: step 4621, loss 0.00699702, acc 1
2016-09-07T22:07:37.551671: step 4622, loss 0.0360456, acc 1
2016-09-07T22:07:38.200824: step 4623, loss 0.0445317, acc 0.98
2016-09-07T22:07:38.869933: step 4624, loss 0.0479861, acc 0.98
2016-09-07T22:07:39.541299: step 4625, loss 0.0341824, acc 1
2016-09-07T22:07:40.197835: step 4626, loss 0.0523125, acc 0.98
2016-09-07T22:07:40.871095: step 4627, loss 0.0323725, acc 1
2016-09-07T22:07:41.547658: step 4628, loss 0.0176683, acc 1
2016-09-07T22:07:42.207897: step 4629, loss 0.0250147, acc 1
2016-09-07T22:07:42.885022: step 4630, loss 0.0437233, acc 0.96
2016-09-07T22:07:43.554579: step 4631, loss 0.0183506, acc 1
2016-09-07T22:07:44.216582: step 4632, loss 0.0495088, acc 1
2016-09-07T22:07:44.873522: step 4633, loss 0.0109142, acc 1
2016-09-07T22:07:45.531603: step 4634, loss 0.0299405, acc 1
2016-09-07T22:07:46.181300: step 4635, loss 0.0298323, acc 1
2016-09-07T22:07:46.842737: step 4636, loss 0.0778, acc 0.98
2016-09-07T22:07:47.509945: step 4637, loss 0.122752, acc 0.98
2016-09-07T22:07:48.172989: step 4638, loss 0.0191115, acc 1
2016-09-07T22:07:48.841256: step 4639, loss 0.0155713, acc 1
2016-09-07T22:07:49.510072: step 4640, loss 0.0154231, acc 1
2016-09-07T22:07:50.170006: step 4641, loss 0.0564163, acc 0.98
2016-09-07T22:07:50.842583: step 4642, loss 0.014789, acc 1
2016-09-07T22:07:51.519401: step 4643, loss 0.0418722, acc 0.96
2016-09-07T22:07:52.185638: step 4644, loss 0.0855609, acc 0.96
2016-09-07T22:07:52.861959: step 4645, loss 0.0654896, acc 0.98
2016-09-07T22:07:53.559561: step 4646, loss 0.00796254, acc 1
2016-09-07T22:07:54.231765: step 4647, loss 0.0114562, acc 1
2016-09-07T22:07:54.888693: step 4648, loss 0.03906, acc 0.98
2016-09-07T22:07:55.546729: step 4649, loss 0.0751006, acc 0.96
2016-09-07T22:07:56.201964: step 4650, loss 0.0298879, acc 0.98
2016-09-07T22:07:56.871911: step 4651, loss 0.0206856, acc 0.98
2016-09-07T22:07:57.555821: step 4652, loss 0.0393894, acc 1
2016-09-07T22:07:58.207217: step 4653, loss 0.0167383, acc 1
2016-09-07T22:07:58.872846: step 4654, loss 0.112369, acc 0.96
2016-09-07T22:07:59.540051: step 4655, loss 0.0151201, acc 1
2016-09-07T22:07:59.905949: step 4656, loss 0.0390057, acc 1
2016-09-07T22:08:00.603128: step 4657, loss 0.0135583, acc 1
2016-09-07T22:08:01.267160: step 4658, loss 0.0533352, acc 1
2016-09-07T22:08:01.935122: step 4659, loss 0.00815446, acc 1
2016-09-07T22:08:02.584276: step 4660, loss 0.110695, acc 0.96
2016-09-07T22:08:03.258298: step 4661, loss 0.0334951, acc 1
2016-09-07T22:08:03.917848: step 4662, loss 0.0169126, acc 1
2016-09-07T22:08:04.579828: step 4663, loss 0.0175172, acc 1
2016-09-07T22:08:05.260523: step 4664, loss 0.0177065, acc 1
2016-09-07T22:08:05.944797: step 4665, loss 0.0057987, acc 1
2016-09-07T22:08:06.628421: step 4666, loss 0.0348675, acc 0.98
2016-09-07T22:08:07.315289: step 4667, loss 0.0130591, acc 1
2016-09-07T22:08:07.987464: step 4668, loss 0.0253326, acc 0.98
2016-09-07T22:08:08.648919: step 4669, loss 0.0582783, acc 0.96
2016-09-07T22:08:09.308572: step 4670, loss 0.0352591, acc 0.98
2016-09-07T22:08:09.960766: step 4671, loss 0.0241862, acc 0.98
2016-09-07T22:08:10.620605: step 4672, loss 0.0425038, acc 0.96
2016-09-07T22:08:11.280928: step 4673, loss 0.0521214, acc 0.98
2016-09-07T22:08:11.957582: step 4674, loss 0.0193454, acc 1
2016-09-07T22:08:12.618372: step 4675, loss 0.0259534, acc 1
2016-09-07T22:08:13.292873: step 4676, loss 0.0261867, acc 1
2016-09-07T22:08:13.961218: step 4677, loss 0.0278699, acc 0.98
2016-09-07T22:08:14.614809: step 4678, loss 0.0188808, acc 1
2016-09-07T22:08:15.268174: step 4679, loss 0.00482682, acc 1
2016-09-07T22:08:15.950610: step 4680, loss 0.0362632, acc 0.98
2016-09-07T22:08:16.616566: step 4681, loss 0.0400091, acc 0.98
2016-09-07T22:08:17.298447: step 4682, loss 0.0200209, acc 0.98
2016-09-07T22:08:17.960213: step 4683, loss 0.00649407, acc 1
2016-09-07T22:08:18.642963: step 4684, loss 0.063298, acc 0.98
2016-09-07T22:08:19.317961: step 4685, loss 0.0261822, acc 0.98
2016-09-07T22:08:19.993301: step 4686, loss 0.0243957, acc 0.98
2016-09-07T22:08:20.652669: step 4687, loss 0.0566677, acc 0.96
2016-09-07T22:08:21.335057: step 4688, loss 0.103991, acc 0.96
2016-09-07T22:08:22.002287: step 4689, loss 0.00640584, acc 1
2016-09-07T22:08:22.662746: step 4690, loss 0.0866575, acc 0.96
2016-09-07T22:08:23.308694: step 4691, loss 0.00679831, acc 1
2016-09-07T22:08:23.960640: step 4692, loss 0.0123415, acc 1
2016-09-07T22:08:24.637047: step 4693, loss 0.0235768, acc 0.98
2016-09-07T22:08:25.325982: step 4694, loss 0.0128385, acc 1
2016-09-07T22:08:25.997827: step 4695, loss 0.00747703, acc 1
2016-09-07T22:08:26.655280: step 4696, loss 0.00596259, acc 1
2016-09-07T22:08:27.312859: step 4697, loss 0.0415329, acc 1
2016-09-07T22:08:27.962760: step 4698, loss 0.0069807, acc 1
2016-09-07T22:08:28.656506: step 4699, loss 0.0355065, acc 1
2016-09-07T22:08:29.329360: step 4700, loss 0.0483887, acc 0.98

Evaluation:
2016-09-07T22:08:32.219673: step 4700, loss 1.62501, acc 0.752

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-4700

2016-09-07T22:08:33.817479: step 4701, loss 0.063183, acc 0.96
2016-09-07T22:08:34.480751: step 4702, loss 0.0217377, acc 1
2016-09-07T22:08:35.144525: step 4703, loss 0.047557, acc 0.96
2016-09-07T22:08:35.795353: step 4704, loss 0.0338237, acc 0.98
2016-09-07T22:08:36.441557: step 4705, loss 0.00840245, acc 1
2016-09-07T22:08:37.114742: step 4706, loss 0.0285839, acc 1
2016-09-07T22:08:37.794797: step 4707, loss 0.0390188, acc 1
2016-09-07T22:08:38.480563: step 4708, loss 0.0384177, acc 0.98
2016-09-07T22:08:39.188029: step 4709, loss 0.0115302, acc 1
2016-09-07T22:08:39.854668: step 4710, loss 0.0263628, acc 0.98
2016-09-07T22:08:40.530984: step 4711, loss 0.00753927, acc 1
2016-09-07T22:08:41.222365: step 4712, loss 0.0242021, acc 1
2016-09-07T22:08:41.895975: step 4713, loss 0.0947063, acc 0.96
2016-09-07T22:08:42.564868: step 4714, loss 0.00540664, acc 1
2016-09-07T22:08:43.242055: step 4715, loss 0.0225876, acc 1
2016-09-07T22:08:43.918120: step 4716, loss 0.0160833, acc 1
2016-09-07T22:08:44.583718: step 4717, loss 0.0380872, acc 0.98
2016-09-07T22:08:45.251432: step 4718, loss 0.0237546, acc 0.98
2016-09-07T22:08:45.909191: step 4719, loss 0.0552773, acc 0.98
2016-09-07T22:08:46.576260: step 4720, loss 0.0142333, acc 1
2016-09-07T22:08:47.224269: step 4721, loss 0.0580159, acc 0.94
2016-09-07T22:08:47.886541: step 4722, loss 0.00643084, acc 1
2016-09-07T22:08:48.565103: step 4723, loss 0.0772332, acc 0.98
2016-09-07T22:08:49.269663: step 4724, loss 0.0246524, acc 1
2016-09-07T22:08:49.942241: step 4725, loss 0.0265945, acc 0.98
2016-09-07T22:08:50.611738: step 4726, loss 0.0295264, acc 0.98
2016-09-07T22:08:51.290925: step 4727, loss 0.0217817, acc 1
2016-09-07T22:08:51.952299: step 4728, loss 0.0490503, acc 0.98
2016-09-07T22:08:52.621982: step 4729, loss 0.00578677, acc 1
2016-09-07T22:08:53.288779: step 4730, loss 0.0246057, acc 0.98
2016-09-07T22:08:53.946048: step 4731, loss 0.0294267, acc 0.98
2016-09-07T22:08:54.624088: step 4732, loss 0.0361384, acc 0.98
2016-09-07T22:08:55.308165: step 4733, loss 0.00641008, acc 1
2016-09-07T22:08:55.999857: step 4734, loss 0.00770352, acc 1
2016-09-07T22:08:56.670047: step 4735, loss 0.0606397, acc 0.96
2016-09-07T22:08:57.338282: step 4736, loss 0.0768581, acc 0.96
2016-09-07T22:08:57.989267: step 4737, loss 0.0237235, acc 1
2016-09-07T22:08:58.676523: step 4738, loss 0.0201862, acc 0.98
2016-09-07T22:08:59.345919: step 4739, loss 0.0629954, acc 0.96
2016-09-07T22:09:00.013776: step 4740, loss 0.0150071, acc 1
2016-09-07T22:09:00.723199: step 4741, loss 0.0115347, acc 1
2016-09-07T22:09:01.384171: step 4742, loss 0.049832, acc 0.98
2016-09-07T22:09:02.067333: step 4743, loss 0.0089768, acc 1
2016-09-07T22:09:02.711793: step 4744, loss 0.0194246, acc 1
2016-09-07T22:09:03.394817: step 4745, loss 0.0658066, acc 0.98
2016-09-07T22:09:04.054713: step 4746, loss 0.0177415, acc 1
2016-09-07T22:09:04.734745: step 4747, loss 0.0157235, acc 1
2016-09-07T22:09:05.399558: step 4748, loss 0.110771, acc 0.98
2016-09-07T22:09:06.056307: step 4749, loss 0.0171871, acc 1
2016-09-07T22:09:06.725032: step 4750, loss 0.00711881, acc 1
2016-09-07T22:09:07.373077: step 4751, loss 0.104457, acc 0.98
2016-09-07T22:09:08.032249: step 4752, loss 0.0322308, acc 0.98
2016-09-07T22:09:08.687049: step 4753, loss 0.0285943, acc 0.98
2016-09-07T22:09:09.373450: step 4754, loss 0.0464693, acc 0.98
2016-09-07T22:09:10.038726: step 4755, loss 0.0261278, acc 1
2016-09-07T22:09:10.700713: step 4756, loss 0.0393556, acc 0.98
2016-09-07T22:09:11.374109: step 4757, loss 0.0143856, acc 1
2016-09-07T22:09:12.030354: step 4758, loss 0.0637276, acc 0.98
2016-09-07T22:09:12.690048: step 4759, loss 0.0579069, acc 0.94
2016-09-07T22:09:13.355493: step 4760, loss 0.0239773, acc 0.98
2016-09-07T22:09:14.010692: step 4761, loss 0.0201065, acc 1
2016-09-07T22:09:14.666196: step 4762, loss 0.0151747, acc 1
2016-09-07T22:09:15.322570: step 4763, loss 0.0815664, acc 0.98
2016-09-07T22:09:15.994783: step 4764, loss 0.0318966, acc 0.98
2016-09-07T22:09:16.651236: step 4765, loss 0.0192902, acc 1
2016-09-07T22:09:17.301141: step 4766, loss 0.02717, acc 1
2016-09-07T22:09:17.958540: step 4767, loss 0.0561698, acc 0.96
2016-09-07T22:09:18.640227: step 4768, loss 0.0569043, acc 0.98
2016-09-07T22:09:19.311038: step 4769, loss 0.021905, acc 1
2016-09-07T22:09:19.997580: step 4770, loss 0.0395679, acc 0.98
2016-09-07T22:09:20.665534: step 4771, loss 0.0735862, acc 0.98
2016-09-07T22:09:21.331139: step 4772, loss 0.0760207, acc 0.96
2016-09-07T22:09:21.989944: step 4773, loss 0.0375829, acc 0.98
2016-09-07T22:09:22.681847: step 4774, loss 0.0286878, acc 0.98
2016-09-07T22:09:23.338997: step 4775, loss 0.0245744, acc 1
2016-09-07T22:09:24.000043: step 4776, loss 0.0305143, acc 1
2016-09-07T22:09:24.662635: step 4777, loss 0.0486707, acc 0.98
2016-09-07T22:09:25.334649: step 4778, loss 0.0580242, acc 0.98
2016-09-07T22:09:26.011309: step 4779, loss 0.0256017, acc 0.98
2016-09-07T22:09:26.673079: step 4780, loss 0.0109306, acc 1
2016-09-07T22:09:27.344978: step 4781, loss 0.0303025, acc 1
2016-09-07T22:09:28.014693: step 4782, loss 0.0283722, acc 1
2016-09-07T22:09:28.693675: step 4783, loss 0.0600134, acc 0.98
2016-09-07T22:09:29.371936: step 4784, loss 0.00633115, acc 1
2016-09-07T22:09:30.020976: step 4785, loss 0.00692315, acc 1
2016-09-07T22:09:30.686080: step 4786, loss 0.0348018, acc 1
2016-09-07T22:09:31.366276: step 4787, loss 0.0306756, acc 0.98
2016-09-07T22:09:32.046175: step 4788, loss 0.126594, acc 0.92
2016-09-07T22:09:32.733550: step 4789, loss 0.0516672, acc 0.98
2016-09-07T22:09:33.399208: step 4790, loss 0.0604036, acc 0.96
2016-09-07T22:09:34.059163: step 4791, loss 0.0472138, acc 0.98
2016-09-07T22:09:34.736216: step 4792, loss 0.0477948, acc 0.98
2016-09-07T22:09:35.416072: step 4793, loss 0.0571361, acc 0.96
2016-09-07T22:09:36.083767: step 4794, loss 0.0219944, acc 1
2016-09-07T22:09:36.757318: step 4795, loss 0.0234345, acc 1
2016-09-07T22:09:37.432012: step 4796, loss 0.0804821, acc 0.98
2016-09-07T22:09:38.081599: step 4797, loss 0.0278648, acc 1
2016-09-07T22:09:38.749021: step 4798, loss 0.0306757, acc 0.98
2016-09-07T22:09:39.409156: step 4799, loss 0.039124, acc 0.98
2016-09-07T22:09:40.057668: step 4800, loss 0.00541588, acc 1

Evaluation:
2016-09-07T22:09:42.951640: step 4800, loss 1.67097, acc 0.734

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-4800

2016-09-07T22:09:44.638035: step 4801, loss 0.0987965, acc 0.98
2016-09-07T22:09:45.312978: step 4802, loss 0.0421085, acc 0.96
2016-09-07T22:09:45.959915: step 4803, loss 0.0310035, acc 0.98
2016-09-07T22:09:46.625149: step 4804, loss 0.0237183, acc 1
2016-09-07T22:09:47.283317: step 4805, loss 0.00771748, acc 1
2016-09-07T22:09:47.956633: step 4806, loss 0.026013, acc 0.98
2016-09-07T22:09:48.637637: step 4807, loss 0.0237031, acc 0.98
2016-09-07T22:09:49.297771: step 4808, loss 0.0509982, acc 0.96
2016-09-07T22:09:49.957651: step 4809, loss 0.0476069, acc 0.98
2016-09-07T22:09:50.636829: step 4810, loss 0.0805359, acc 0.94
2016-09-07T22:09:51.314499: step 4811, loss 0.0250417, acc 1
2016-09-07T22:09:52.005333: step 4812, loss 0.00616113, acc 1
2016-09-07T22:09:52.671885: step 4813, loss 0.0824554, acc 0.98
2016-09-07T22:09:53.332651: step 4814, loss 0.056067, acc 0.98
2016-09-07T22:09:54.007498: step 4815, loss 0.0262701, acc 0.98
2016-09-07T22:09:54.661513: step 4816, loss 0.019667, acc 1
2016-09-07T22:09:55.318981: step 4817, loss 0.0246864, acc 1
2016-09-07T22:09:55.978006: step 4818, loss 0.033033, acc 0.98
2016-09-07T22:09:56.634587: step 4819, loss 0.0439917, acc 0.98
2016-09-07T22:09:57.296116: step 4820, loss 0.0170179, acc 1
2016-09-07T22:09:57.964136: step 4821, loss 0.01439, acc 1
2016-09-07T22:09:58.641424: step 4822, loss 0.074853, acc 0.96
2016-09-07T22:09:59.295710: step 4823, loss 0.0673566, acc 0.98
2016-09-07T22:09:59.942237: step 4824, loss 0.0338311, acc 0.98
2016-09-07T22:10:00.643519: step 4825, loss 0.0206596, acc 0.98
2016-09-07T22:10:01.302261: step 4826, loss 0.0649437, acc 0.96
2016-09-07T22:10:01.960324: step 4827, loss 0.0532017, acc 0.98
2016-09-07T22:10:02.640911: step 4828, loss 0.0218374, acc 1
2016-09-07T22:10:03.336217: step 4829, loss 0.0228379, acc 0.98
2016-09-07T22:10:04.008310: step 4830, loss 0.00615839, acc 1
2016-09-07T22:10:04.690019: step 4831, loss 0.054832, acc 0.98
2016-09-07T22:10:05.334574: step 4832, loss 0.0650397, acc 0.96
2016-09-07T22:10:06.000201: step 4833, loss 0.00950591, acc 1
2016-09-07T22:10:06.688666: step 4834, loss 0.0181222, acc 1
2016-09-07T22:10:07.347119: step 4835, loss 0.0312609, acc 1
2016-09-07T22:10:07.995362: step 4836, loss 0.0384422, acc 0.98
2016-09-07T22:10:08.687650: step 4837, loss 0.0614855, acc 0.94
2016-09-07T22:10:09.360241: step 4838, loss 0.00904881, acc 1
2016-09-07T22:10:10.031810: step 4839, loss 0.0246927, acc 0.98
2016-09-07T22:10:10.702777: step 4840, loss 0.0147947, acc 1
2016-09-07T22:10:11.387492: step 4841, loss 0.0271406, acc 1
2016-09-07T22:10:12.050959: step 4842, loss 0.0297163, acc 0.98
2016-09-07T22:10:12.716402: step 4843, loss 0.0260511, acc 1
2016-09-07T22:10:13.379514: step 4844, loss 0.0100113, acc 1
2016-09-07T22:10:14.062164: step 4845, loss 0.0292381, acc 0.98
2016-09-07T22:10:14.717640: step 4846, loss 0.100054, acc 0.96
2016-09-07T22:10:15.376375: step 4847, loss 0.0337092, acc 0.98
2016-09-07T22:10:16.060155: step 4848, loss 0.0180143, acc 1
2016-09-07T22:10:16.744777: step 4849, loss 0.00723225, acc 1
2016-09-07T22:10:17.108086: step 4850, loss 0.065733, acc 0.916667
2016-09-07T22:10:17.773235: step 4851, loss 0.0597369, acc 0.96
2016-09-07T22:10:18.449809: step 4852, loss 0.0841939, acc 0.96
2016-09-07T22:10:19.118704: step 4853, loss 0.0591333, acc 0.94
2016-09-07T22:10:19.794294: step 4854, loss 0.0378234, acc 0.98
2016-09-07T22:10:20.454156: step 4855, loss 0.0293614, acc 1
2016-09-07T22:10:21.125662: step 4856, loss 0.0453192, acc 0.98
2016-09-07T22:10:21.808783: step 4857, loss 0.00915265, acc 1
2016-09-07T22:10:22.471041: step 4858, loss 0.0338168, acc 0.98
2016-09-07T22:10:23.167745: step 4859, loss 0.0184238, acc 1
2016-09-07T22:10:23.858101: step 4860, loss 0.0178619, acc 1
2016-09-07T22:10:24.535342: step 4861, loss 0.0136683, acc 1
2016-09-07T22:10:25.196476: step 4862, loss 0.0293124, acc 0.98
2016-09-07T22:10:25.866830: step 4863, loss 0.0473553, acc 0.98
2016-09-07T22:10:26.532709: step 4864, loss 0.0586502, acc 0.96
2016-09-07T22:10:27.193354: step 4865, loss 0.00440489, acc 1
2016-09-07T22:10:27.848456: step 4866, loss 0.049984, acc 0.96
2016-09-07T22:10:28.509446: step 4867, loss 0.0225152, acc 1
2016-09-07T22:10:29.174166: step 4868, loss 0.00797866, acc 1
2016-09-07T22:10:29.832650: step 4869, loss 0.0390724, acc 0.98
2016-09-07T22:10:30.499809: step 4870, loss 0.00707517, acc 1
2016-09-07T22:10:31.157541: step 4871, loss 0.106874, acc 0.96
2016-09-07T22:10:31.837773: step 4872, loss 0.0245807, acc 1
2016-09-07T22:10:32.494037: step 4873, loss 0.0428983, acc 0.98
2016-09-07T22:10:33.179360: step 4874, loss 0.0438774, acc 0.96
2016-09-07T22:10:33.865379: step 4875, loss 0.0249983, acc 1
2016-09-07T22:10:34.542452: step 4876, loss 0.0051099, acc 1
2016-09-07T22:10:35.209005: step 4877, loss 0.00541187, acc 1
2016-09-07T22:10:35.877794: step 4878, loss 0.104163, acc 0.94
2016-09-07T22:10:36.537508: step 4879, loss 0.0316454, acc 1
2016-09-07T22:10:37.203314: step 4880, loss 0.052747, acc 0.98
2016-09-07T22:10:37.862544: step 4881, loss 0.0230315, acc 1
2016-09-07T22:10:38.515708: step 4882, loss 0.0437667, acc 0.98
2016-09-07T22:10:39.187407: step 4883, loss 0.0278632, acc 1
2016-09-07T22:10:39.871510: step 4884, loss 0.0295611, acc 1
2016-09-07T22:10:40.523848: step 4885, loss 0.0381681, acc 0.96
2016-09-07T22:10:41.188004: step 4886, loss 0.0280598, acc 0.98
2016-09-07T22:10:41.862419: step 4887, loss 0.0113767, acc 1
2016-09-07T22:10:42.531144: step 4888, loss 0.0234867, acc 0.98
2016-09-07T22:10:43.193323: step 4889, loss 0.0057071, acc 1
2016-09-07T22:10:43.861851: step 4890, loss 0.0193687, acc 1
2016-09-07T22:10:44.547721: step 4891, loss 0.0259212, acc 1
2016-09-07T22:10:45.218875: step 4892, loss 0.0182686, acc 1
2016-09-07T22:10:45.884529: step 4893, loss 0.0112027, acc 1
2016-09-07T22:10:46.535156: step 4894, loss 0.0105487, acc 1
2016-09-07T22:10:47.206888: step 4895, loss 0.0336652, acc 0.98
2016-09-07T22:10:47.867923: step 4896, loss 0.0287724, acc 1
2016-09-07T22:10:48.528221: step 4897, loss 0.0361494, acc 0.98
2016-09-07T22:10:49.191884: step 4898, loss 0.0566411, acc 0.98
2016-09-07T22:10:49.850130: step 4899, loss 0.106329, acc 0.98
2016-09-07T22:10:50.502629: step 4900, loss 0.0339628, acc 1

Evaluation:
2016-09-07T22:10:53.378046: step 4900, loss 2.15949, acc 0.741

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-4900

2016-09-07T22:10:55.060787: step 4901, loss 0.0239229, acc 1
2016-09-07T22:10:55.737322: step 4902, loss 0.0113713, acc 1
2016-09-07T22:10:56.411023: step 4903, loss 0.0614295, acc 0.98
2016-09-07T22:10:57.062663: step 4904, loss 0.0184772, acc 0.98
2016-09-07T22:10:57.717343: step 4905, loss 0.0290545, acc 0.98
2016-09-07T22:10:58.378173: step 4906, loss 0.134876, acc 0.96
2016-09-07T22:10:59.047552: step 4907, loss 0.00946312, acc 1
2016-09-07T22:10:59.712213: step 4908, loss 0.00897448, acc 1
2016-09-07T22:11:00.426115: step 4909, loss 0.0132817, acc 1
2016-09-07T22:11:01.084028: step 4910, loss 0.0375532, acc 0.98
2016-09-07T22:11:01.747482: step 4911, loss 0.0400093, acc 0.98
2016-09-07T22:11:02.398841: step 4912, loss 0.023528, acc 0.98
2016-09-07T22:11:03.064133: step 4913, loss 0.0531877, acc 0.98
2016-09-07T22:11:03.696879: step 4914, loss 0.0331259, acc 1
2016-09-07T22:11:04.370281: step 4915, loss 0.0297636, acc 0.98
2016-09-07T22:11:05.030552: step 4916, loss 0.0175189, acc 1
2016-09-07T22:11:05.684019: step 4917, loss 0.0426753, acc 1
2016-09-07T22:11:06.321631: step 4918, loss 0.039538, acc 0.98
2016-09-07T22:11:06.987568: step 4919, loss 0.032893, acc 0.98
2016-09-07T22:11:07.665813: step 4920, loss 0.0682869, acc 0.98
2016-09-07T22:11:08.327292: step 4921, loss 0.0196295, acc 1
2016-09-07T22:11:08.997139: step 4922, loss 0.0326318, acc 0.98
2016-09-07T22:11:09.669215: step 4923, loss 0.00530375, acc 1
2016-09-07T22:11:10.355022: step 4924, loss 0.0331714, acc 1
2016-09-07T22:11:11.032846: step 4925, loss 0.0115736, acc 1
2016-09-07T22:11:11.697949: step 4926, loss 0.0441888, acc 0.98
2016-09-07T22:11:12.380240: step 4927, loss 0.0452848, acc 0.96
2016-09-07T22:11:13.059899: step 4928, loss 0.0191427, acc 1
2016-09-07T22:11:13.722956: step 4929, loss 0.00672802, acc 1
2016-09-07T22:11:14.386098: step 4930, loss 0.026241, acc 0.98
2016-09-07T22:11:15.039829: step 4931, loss 0.0694691, acc 0.96
2016-09-07T22:11:15.716565: step 4932, loss 0.0690505, acc 0.98
2016-09-07T22:11:16.373077: step 4933, loss 0.01806, acc 1
2016-09-07T22:11:17.039540: step 4934, loss 0.0477161, acc 0.98
2016-09-07T22:11:17.708140: step 4935, loss 0.0380165, acc 0.98
2016-09-07T22:11:18.392456: step 4936, loss 0.0487764, acc 0.98
2016-09-07T22:11:19.051554: step 4937, loss 0.0411415, acc 0.98
2016-09-07T22:11:19.715148: step 4938, loss 0.0551238, acc 0.96
2016-09-07T22:11:20.391611: step 4939, loss 0.0203941, acc 1
2016-09-07T22:11:21.047240: step 4940, loss 0.0514352, acc 0.98
2016-09-07T22:11:21.713410: step 4941, loss 0.00840443, acc 1
2016-09-07T22:11:22.387848: step 4942, loss 0.00722015, acc 1
2016-09-07T22:11:23.051879: step 4943, loss 0.00776559, acc 1
2016-09-07T22:11:23.717638: step 4944, loss 0.0248699, acc 0.98
2016-09-07T22:11:24.383527: step 4945, loss 0.0121652, acc 1
2016-09-07T22:11:25.069528: step 4946, loss 0.0171974, acc 1
2016-09-07T22:11:25.730987: step 4947, loss 0.0345215, acc 0.98
2016-09-07T22:11:26.391573: step 4948, loss 0.0473568, acc 0.98
2016-09-07T22:11:27.071236: step 4949, loss 0.0293229, acc 0.98
2016-09-07T22:11:27.736658: step 4950, loss 0.0346243, acc 0.98
2016-09-07T22:11:28.399015: step 4951, loss 0.163545, acc 0.98
2016-09-07T22:11:29.063691: step 4952, loss 0.0536972, acc 0.98
2016-09-07T22:11:29.747466: step 4953, loss 0.0181873, acc 1
2016-09-07T22:11:30.412595: step 4954, loss 0.0188516, acc 1
2016-09-07T22:11:31.078473: step 4955, loss 0.0523806, acc 0.98
2016-09-07T22:11:31.753826: step 4956, loss 0.150311, acc 0.98
2016-09-07T22:11:32.438181: step 4957, loss 0.031797, acc 1
2016-09-07T22:11:33.104975: step 4958, loss 0.0360783, acc 1
2016-09-07T22:11:33.777316: step 4959, loss 0.0272714, acc 0.98
2016-09-07T22:11:34.447297: step 4960, loss 0.0169282, acc 1
2016-09-07T22:11:35.107316: step 4961, loss 0.102011, acc 0.96
2016-09-07T22:11:35.764263: step 4962, loss 0.00713163, acc 1
2016-09-07T22:11:36.416545: step 4963, loss 0.00771839, acc 1
2016-09-07T22:11:37.086481: step 4964, loss 0.0360694, acc 0.96
2016-09-07T22:11:37.784899: step 4965, loss 0.00890053, acc 1
2016-09-07T22:11:38.451602: step 4966, loss 0.0184459, acc 1
2016-09-07T22:11:39.119921: step 4967, loss 0.022723, acc 1
2016-09-07T22:11:39.790719: step 4968, loss 0.0445299, acc 0.98
2016-09-07T22:11:40.462681: step 4969, loss 0.0219552, acc 1
2016-09-07T22:11:41.145971: step 4970, loss 0.0507961, acc 0.98
2016-09-07T22:11:41.837449: step 4971, loss 0.01047, acc 1
2016-09-07T22:11:42.498588: step 4972, loss 0.0590637, acc 0.96
2016-09-07T22:11:43.171396: step 4973, loss 0.0133111, acc 1
2016-09-07T22:11:43.873003: step 4974, loss 0.103138, acc 0.98
2016-09-07T22:11:44.599318: step 4975, loss 0.0243749, acc 1
2016-09-07T22:11:45.264903: step 4976, loss 0.105121, acc 0.98
2016-09-07T22:11:45.937749: step 4977, loss 0.0301742, acc 1
2016-09-07T22:11:46.615325: step 4978, loss 0.0129651, acc 1
2016-09-07T22:11:47.284987: step 4979, loss 0.00993858, acc 1
2016-09-07T22:11:47.965964: step 4980, loss 0.0402302, acc 0.98
2016-09-07T22:11:48.627571: step 4981, loss 0.0449422, acc 0.98
2016-09-07T22:11:49.290783: step 4982, loss 0.1474, acc 0.98
2016-09-07T22:11:49.947576: step 4983, loss 0.143974, acc 0.96
2016-09-07T22:11:50.631456: step 4984, loss 0.0225484, acc 1
2016-09-07T22:11:51.278831: step 4985, loss 0.0131468, acc 1
2016-09-07T22:11:51.940294: step 4986, loss 0.0163548, acc 1
2016-09-07T22:11:52.613250: step 4987, loss 0.0324986, acc 0.98
2016-09-07T22:11:53.280183: step 4988, loss 0.00890832, acc 1
2016-09-07T22:11:53.953138: step 4989, loss 0.0208735, acc 1
2016-09-07T22:11:54.616049: step 4990, loss 0.0490711, acc 0.98
2016-09-07T22:11:55.311769: step 4991, loss 0.0534304, acc 0.96
2016-09-07T22:11:55.981984: step 4992, loss 0.0311094, acc 0.98
2016-09-07T22:11:56.650532: step 4993, loss 0.0845501, acc 0.98
2016-09-07T22:11:57.300961: step 4994, loss 0.0161955, acc 1
2016-09-07T22:11:57.976711: step 4995, loss 0.0259645, acc 1
2016-09-07T22:11:58.654231: step 4996, loss 0.01344, acc 1
2016-09-07T22:11:59.325914: step 4997, loss 0.0136175, acc 1
2016-09-07T22:11:59.998605: step 4998, loss 0.0665543, acc 0.96
2016-09-07T22:12:00.711573: step 4999, loss 0.0689998, acc 0.98
2016-09-07T22:12:01.384471: step 5000, loss 0.00577626, acc 1

Evaluation:
2016-09-07T22:12:04.268459: step 5000, loss 2.12516, acc 0.735

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-5000

2016-09-07T22:12:05.891735: step 5001, loss 0.0218744, acc 0.98
2016-09-07T22:12:06.565162: step 5002, loss 0.0610571, acc 0.94
2016-09-07T22:12:07.238016: step 5003, loss 0.0568471, acc 0.96
2016-09-07T22:12:07.921715: step 5004, loss 0.0307442, acc 0.98
2016-09-07T22:12:08.593139: step 5005, loss 0.0118683, acc 1
2016-09-07T22:12:09.259815: step 5006, loss 0.0063128, acc 1
2016-09-07T22:12:09.943389: step 5007, loss 0.0405794, acc 0.98
2016-09-07T22:12:10.610292: step 5008, loss 0.0637803, acc 1
2016-09-07T22:12:11.280924: step 5009, loss 0.0126396, acc 1
2016-09-07T22:12:11.951676: step 5010, loss 0.0295622, acc 1
2016-09-07T22:12:12.610112: step 5011, loss 0.029899, acc 1
2016-09-07T22:12:13.282101: step 5012, loss 0.0805627, acc 0.96
2016-09-07T22:12:13.939524: step 5013, loss 0.0201946, acc 1
2016-09-07T22:12:14.598963: step 5014, loss 0.0975964, acc 0.96
2016-09-07T22:12:15.267774: step 5015, loss 0.0212242, acc 1
2016-09-07T22:12:15.951462: step 5016, loss 0.0481569, acc 0.98
2016-09-07T22:12:16.627151: step 5017, loss 0.03084, acc 0.98
2016-09-07T22:12:17.309959: step 5018, loss 0.0280877, acc 1
2016-09-07T22:12:17.981276: step 5019, loss 0.0627772, acc 0.96
2016-09-07T22:12:18.665895: step 5020, loss 0.092882, acc 0.98
2016-09-07T22:12:19.349289: step 5021, loss 0.0193053, acc 1
2016-09-07T22:12:20.022727: step 5022, loss 0.0282254, acc 0.98
2016-09-07T22:12:20.694162: step 5023, loss 0.00944007, acc 1
2016-09-07T22:12:21.380285: step 5024, loss 0.00607278, acc 1
2016-09-07T22:12:22.050773: step 5025, loss 0.0424025, acc 0.98
2016-09-07T22:12:22.714291: step 5026, loss 0.0548297, acc 0.98
2016-09-07T22:12:23.381201: step 5027, loss 0.005443, acc 1
2016-09-07T22:12:24.081405: step 5028, loss 0.00552782, acc 1
2016-09-07T22:12:24.768957: step 5029, loss 0.0296975, acc 1
2016-09-07T22:12:25.428921: step 5030, loss 0.0265114, acc 1
2016-09-07T22:12:26.098223: step 5031, loss 0.0421576, acc 0.98
2016-09-07T22:12:26.775866: step 5032, loss 0.0295282, acc 1
2016-09-07T22:12:27.435849: step 5033, loss 0.0199217, acc 0.98
2016-09-07T22:12:28.114874: step 5034, loss 0.0566078, acc 0.98
2016-09-07T22:12:28.780759: step 5035, loss 0.0222861, acc 1
2016-09-07T22:12:29.461819: step 5036, loss 0.0354764, acc 0.98
2016-09-07T22:12:30.124155: step 5037, loss 0.127801, acc 0.96
2016-09-07T22:12:30.799238: step 5038, loss 0.0311541, acc 0.98
2016-09-07T22:12:31.462676: step 5039, loss 0.0450493, acc 0.98
2016-09-07T22:12:32.135913: step 5040, loss 0.0862677, acc 0.98
2016-09-07T22:12:32.795996: step 5041, loss 0.0378039, acc 1
2016-09-07T22:12:33.470956: step 5042, loss 0.0824574, acc 0.94
2016-09-07T22:12:34.122256: step 5043, loss 0.128816, acc 0.96
2016-09-07T22:12:34.468450: step 5044, loss 0.0338964, acc 1
2016-09-07T22:12:35.128290: step 5045, loss 0.0334557, acc 0.98
2016-09-07T22:12:35.803715: step 5046, loss 0.0348724, acc 1
2016-09-07T22:12:36.503728: step 5047, loss 0.14162, acc 0.98
2016-09-07T22:12:37.192583: step 5048, loss 0.0445086, acc 0.98
2016-09-07T22:12:37.849325: step 5049, loss 0.00778616, acc 1
2016-09-07T22:12:38.537108: step 5050, loss 0.0434629, acc 0.98
2016-09-07T22:12:39.226209: step 5051, loss 0.0429943, acc 0.98
2016-09-07T22:12:39.894344: step 5052, loss 0.031539, acc 1
2016-09-07T22:12:40.558168: step 5053, loss 0.0353913, acc 1
2016-09-07T22:12:41.220972: step 5054, loss 0.0437047, acc 0.98
2016-09-07T22:12:41.895045: step 5055, loss 0.0226453, acc 1
2016-09-07T22:12:42.560370: step 5056, loss 0.0281654, acc 0.98
2016-09-07T22:12:43.241584: step 5057, loss 0.0168444, acc 1
2016-09-07T22:12:43.923178: step 5058, loss 0.0220587, acc 0.98
2016-09-07T22:12:44.600555: step 5059, loss 0.0188684, acc 0.98
2016-09-07T22:12:45.266836: step 5060, loss 0.0207614, acc 1
2016-09-07T22:12:45.942314: step 5061, loss 0.0259528, acc 0.98
2016-09-07T22:12:46.605677: step 5062, loss 0.0859767, acc 0.98
2016-09-07T22:12:47.247231: step 5063, loss 0.0119235, acc 1
2016-09-07T22:12:47.898165: step 5064, loss 0.0083132, acc 1
2016-09-07T22:12:48.575286: step 5065, loss 0.0155714, acc 1
2016-09-07T22:12:49.239444: step 5066, loss 0.067983, acc 0.96
2016-09-07T22:12:49.896779: step 5067, loss 0.00942273, acc 1
2016-09-07T22:12:50.540756: step 5068, loss 0.0188223, acc 1
2016-09-07T22:12:51.218484: step 5069, loss 0.0056342, acc 1
2016-09-07T22:12:51.876156: step 5070, loss 0.00715701, acc 1
2016-09-07T22:12:52.556445: step 5071, loss 0.0340747, acc 0.98
2016-09-07T22:12:53.234264: step 5072, loss 0.00849563, acc 1
2016-09-07T22:12:53.914533: step 5073, loss 0.00964215, acc 1
2016-09-07T22:12:54.570214: step 5074, loss 0.0600509, acc 0.96
2016-09-07T22:12:55.229267: step 5075, loss 0.00491497, acc 1
2016-09-07T22:12:55.917655: step 5076, loss 0.103295, acc 0.94
2016-09-07T22:12:56.590670: step 5077, loss 0.0504288, acc 0.96
2016-09-07T22:12:57.280009: step 5078, loss 0.127925, acc 0.96
2016-09-07T22:12:57.951536: step 5079, loss 0.00727697, acc 1
2016-09-07T22:12:58.617032: step 5080, loss 0.0333006, acc 0.96
2016-09-07T22:12:59.267244: step 5081, loss 0.00945865, acc 1
2016-09-07T22:12:59.913039: step 5082, loss 0.00506769, acc 1
2016-09-07T22:13:00.627204: step 5083, loss 0.0157063, acc 1
2016-09-07T22:13:01.298995: step 5084, loss 0.00497941, acc 1
2016-09-07T22:13:01.954778: step 5085, loss 0.0189914, acc 0.98
2016-09-07T22:13:02.624351: step 5086, loss 0.0557893, acc 0.98
2016-09-07T22:13:03.304273: step 5087, loss 0.00934915, acc 1
2016-09-07T22:13:03.967119: step 5088, loss 0.029457, acc 0.98
2016-09-07T22:13:04.660917: step 5089, loss 0.0198481, acc 1
2016-09-07T22:13:05.316761: step 5090, loss 0.0109048, acc 1
2016-09-07T22:13:05.986472: step 5091, loss 0.0222791, acc 0.98
2016-09-07T22:13:06.660598: step 5092, loss 0.083551, acc 0.94
2016-09-07T22:13:07.354765: step 5093, loss 0.00819291, acc 1
2016-09-07T22:13:08.029387: step 5094, loss 0.0430561, acc 0.98
2016-09-07T22:13:08.713001: step 5095, loss 0.0129288, acc 1
2016-09-07T22:13:09.382254: step 5096, loss 0.0496895, acc 0.96
2016-09-07T22:13:10.048939: step 5097, loss 0.0212302, acc 1
2016-09-07T22:13:10.721213: step 5098, loss 0.200036, acc 0.98
2016-09-07T22:13:11.382144: step 5099, loss 0.0268453, acc 0.98
2016-09-07T22:13:12.054664: step 5100, loss 0.0251377, acc 0.98

Evaluation:
2016-09-07T22:13:14.937504: step 5100, loss 1.609, acc 0.74

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-5100

2016-09-07T22:13:16.567238: step 5101, loss 0.0334443, acc 1
2016-09-07T22:13:17.243284: step 5102, loss 0.0255174, acc 0.98
2016-09-07T22:13:17.916883: step 5103, loss 0.0272663, acc 0.98
2016-09-07T22:13:18.580527: step 5104, loss 0.0422148, acc 0.98
2016-09-07T22:13:19.247292: step 5105, loss 0.0510147, acc 0.96
2016-09-07T22:13:19.923714: step 5106, loss 0.0335377, acc 0.98
2016-09-07T22:13:20.613588: step 5107, loss 0.090117, acc 0.96
2016-09-07T22:13:21.301385: step 5108, loss 0.0147537, acc 1
2016-09-07T22:13:21.991107: step 5109, loss 0.0771961, acc 0.96
2016-09-07T22:13:22.657691: step 5110, loss 0.0323565, acc 1
2016-09-07T22:13:23.341984: step 5111, loss 0.056684, acc 0.98
2016-09-07T22:13:24.011100: step 5112, loss 0.0265067, acc 1
2016-09-07T22:13:24.680866: step 5113, loss 0.0288819, acc 0.98
2016-09-07T22:13:25.352702: step 5114, loss 0.0168315, acc 1
2016-09-07T22:13:26.018851: step 5115, loss 0.0505103, acc 0.98
2016-09-07T22:13:26.684700: step 5116, loss 0.116132, acc 0.98
2016-09-07T22:13:27.372462: step 5117, loss 0.0439227, acc 0.98
2016-09-07T22:13:28.042528: step 5118, loss 0.00808044, acc 1
2016-09-07T22:13:28.705096: step 5119, loss 0.0179911, acc 1
2016-09-07T22:13:29.366730: step 5120, loss 0.046989, acc 0.96
2016-09-07T22:13:30.065896: step 5121, loss 0.0314946, acc 1
2016-09-07T22:13:30.746725: step 5122, loss 0.0622114, acc 0.98
2016-09-07T22:13:31.408028: step 5123, loss 0.0112966, acc 1
2016-09-07T22:13:32.065618: step 5124, loss 0.0409305, acc 1
2016-09-07T22:13:32.725201: step 5125, loss 0.0442713, acc 0.98
2016-09-07T22:13:33.380227: step 5126, loss 0.0128413, acc 1
2016-09-07T22:13:34.049882: step 5127, loss 0.0424406, acc 0.98
2016-09-07T22:13:34.721998: step 5128, loss 0.14756, acc 0.96
2016-09-07T22:13:35.413892: step 5129, loss 0.0361958, acc 0.98
2016-09-07T22:13:36.096904: step 5130, loss 0.027351, acc 1
2016-09-07T22:13:36.746847: step 5131, loss 0.00625873, acc 1
2016-09-07T22:13:37.415906: step 5132, loss 0.0487792, acc 0.98
2016-09-07T22:13:38.077362: step 5133, loss 0.0274507, acc 0.98
2016-09-07T22:13:38.730669: step 5134, loss 0.0237218, acc 0.98
2016-09-07T22:13:39.387316: step 5135, loss 0.0444537, acc 0.98
2016-09-07T22:13:40.062491: step 5136, loss 0.032381, acc 1
2016-09-07T22:13:40.727521: step 5137, loss 0.0392929, acc 0.98
2016-09-07T22:13:41.391583: step 5138, loss 0.00894659, acc 1
2016-09-07T22:13:42.049836: step 5139, loss 0.0375504, acc 0.98
2016-09-07T22:13:42.715363: step 5140, loss 0.0138934, acc 1
2016-09-07T22:13:43.384290: step 5141, loss 0.0352392, acc 0.98
2016-09-07T22:13:44.042012: step 5142, loss 0.0668119, acc 0.96
2016-09-07T22:13:44.714008: step 5143, loss 0.012895, acc 1
2016-09-07T22:13:45.389269: step 5144, loss 0.0691192, acc 0.98
2016-09-07T22:13:46.076452: step 5145, loss 0.226153, acc 0.96
2016-09-07T22:13:46.761724: step 5146, loss 0.00766548, acc 1
2016-09-07T22:13:47.449379: step 5147, loss 0.0217718, acc 1
2016-09-07T22:13:48.135135: step 5148, loss 0.00778902, acc 1
2016-09-07T22:13:48.815688: step 5149, loss 0.0265129, acc 1
2016-09-07T22:13:49.514960: step 5150, loss 0.0413248, acc 0.98
2016-09-07T22:13:50.178850: step 5151, loss 0.0123592, acc 1
2016-09-07T22:13:50.842006: step 5152, loss 0.00962921, acc 1
2016-09-07T22:13:51.510734: step 5153, loss 0.0598083, acc 0.98
2016-09-07T22:13:52.175043: step 5154, loss 0.0596566, acc 0.98
2016-09-07T22:13:52.843687: step 5155, loss 0.102062, acc 0.96
2016-09-07T22:13:53.541902: step 5156, loss 0.00879676, acc 1
2016-09-07T22:13:54.209025: step 5157, loss 0.00767142, acc 1
2016-09-07T22:13:54.858613: step 5158, loss 0.0369424, acc 0.98
2016-09-07T22:13:55.544119: step 5159, loss 0.0384712, acc 0.96
2016-09-07T22:13:56.216528: step 5160, loss 0.0227508, acc 1
2016-09-07T22:13:56.890580: step 5161, loss 0.0322565, acc 0.98
2016-09-07T22:13:57.572837: step 5162, loss 0.0185334, acc 1
2016-09-07T22:13:58.227956: step 5163, loss 0.01806, acc 1
2016-09-07T22:13:58.890849: step 5164, loss 0.154255, acc 0.96
2016-09-07T22:13:59.562132: step 5165, loss 0.0649384, acc 0.96
2016-09-07T22:14:00.253570: step 5166, loss 0.0978919, acc 0.96
2016-09-07T22:14:00.911301: step 5167, loss 0.0304986, acc 0.98
2016-09-07T22:14:01.570874: step 5168, loss 0.0252183, acc 0.98
2016-09-07T22:14:02.250712: step 5169, loss 0.0777998, acc 0.96
2016-09-07T22:14:02.926103: step 5170, loss 0.0159842, acc 1
2016-09-07T22:14:03.594476: step 5171, loss 0.0582542, acc 0.96
2016-09-07T22:14:04.270390: step 5172, loss 0.0366676, acc 0.98
2016-09-07T22:14:04.942186: step 5173, loss 0.0218319, acc 1
2016-09-07T22:14:05.608368: step 5174, loss 0.0375198, acc 0.98
2016-09-07T22:14:06.269325: step 5175, loss 0.0383948, acc 1
2016-09-07T22:14:06.933503: step 5176, loss 0.0172297, acc 1
2016-09-07T22:14:07.605351: step 5177, loss 0.0227736, acc 1
2016-09-07T22:14:08.278882: step 5178, loss 0.0280171, acc 1
2016-09-07T22:14:08.942867: step 5179, loss 0.0600857, acc 0.98
2016-09-07T22:14:09.618056: step 5180, loss 0.0158803, acc 1
2016-09-07T22:14:10.284124: step 5181, loss 0.00764325, acc 1
2016-09-07T22:14:10.959403: step 5182, loss 0.00531976, acc 1
2016-09-07T22:14:11.629765: step 5183, loss 0.0136405, acc 1
2016-09-07T22:14:12.295838: step 5184, loss 0.0307912, acc 0.98
2016-09-07T22:14:12.952702: step 5185, loss 0.0370718, acc 0.98
2016-09-07T22:14:13.625685: step 5186, loss 0.0242024, acc 1
2016-09-07T22:14:14.304187: step 5187, loss 0.0274119, acc 1
2016-09-07T22:14:14.971413: step 5188, loss 0.0759854, acc 0.98
2016-09-07T22:14:15.652752: step 5189, loss 0.0253839, acc 0.98
2016-09-07T22:14:16.323335: step 5190, loss 0.0158234, acc 1
2016-09-07T22:14:17.002338: step 5191, loss 0.0170252, acc 1
2016-09-07T22:14:17.661716: step 5192, loss 0.0790471, acc 0.98
2016-09-07T22:14:18.338811: step 5193, loss 0.011882, acc 1
2016-09-07T22:14:19.008589: step 5194, loss 0.0240544, acc 0.98
2016-09-07T22:14:19.666514: step 5195, loss 0.0830719, acc 0.98
2016-09-07T22:14:20.334772: step 5196, loss 0.0479173, acc 0.98
2016-09-07T22:14:21.001074: step 5197, loss 0.021596, acc 1
2016-09-07T22:14:21.675505: step 5198, loss 0.0321376, acc 1
2016-09-07T22:14:22.347884: step 5199, loss 0.0534109, acc 0.98
2016-09-07T22:14:23.010652: step 5200, loss 0.00667771, acc 1

Evaluation:
2016-09-07T22:14:25.896440: step 5200, loss 2.07473, acc 0.742

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-5200

2016-09-07T22:14:27.721335: step 5201, loss 0.0303943, acc 0.98
2016-09-07T22:14:28.405682: step 5202, loss 0.0258037, acc 1
2016-09-07T22:14:29.078577: step 5203, loss 0.0585986, acc 0.98
2016-09-07T22:14:29.763739: step 5204, loss 0.00691581, acc 1
2016-09-07T22:14:30.429011: step 5205, loss 0.0360579, acc 1
2016-09-07T22:14:31.117014: step 5206, loss 0.0818712, acc 0.98
2016-09-07T22:14:31.779446: step 5207, loss 0.00572863, acc 1
2016-09-07T22:14:32.431312: step 5208, loss 0.0182972, acc 1
2016-09-07T22:14:33.085378: step 5209, loss 0.0304603, acc 0.98
2016-09-07T22:14:33.753084: step 5210, loss 0.022684, acc 1
2016-09-07T22:14:34.403212: step 5211, loss 0.0414073, acc 0.98
2016-09-07T22:14:35.061789: step 5212, loss 0.0332523, acc 0.98
2016-09-07T22:14:35.725621: step 5213, loss 0.00700314, acc 1
2016-09-07T22:14:36.392369: step 5214, loss 0.0458015, acc 0.96
2016-09-07T22:14:37.074766: step 5215, loss 0.0449408, acc 0.98
2016-09-07T22:14:37.738260: step 5216, loss 0.019147, acc 0.98
2016-09-07T22:14:38.411490: step 5217, loss 0.0124936, acc 1
2016-09-07T22:14:39.100322: step 5218, loss 0.0130087, acc 1
2016-09-07T22:14:39.802137: step 5219, loss 0.00864059, acc 1
2016-09-07T22:14:40.473457: step 5220, loss 0.0250076, acc 1
2016-09-07T22:14:41.146080: step 5221, loss 0.0523805, acc 0.98
2016-09-07T22:14:41.821640: step 5222, loss 0.0187999, acc 1
2016-09-07T22:14:42.498629: step 5223, loss 0.017934, acc 1
2016-09-07T22:14:43.156620: step 5224, loss 0.0114802, acc 1
2016-09-07T22:14:43.841600: step 5225, loss 0.0678529, acc 0.96
2016-09-07T22:14:44.518604: step 5226, loss 0.0583784, acc 0.98
2016-09-07T22:14:45.204407: step 5227, loss 0.0200981, acc 0.98
2016-09-07T22:14:45.885401: step 5228, loss 0.00622672, acc 1
2016-09-07T22:14:46.546915: step 5229, loss 0.0275583, acc 0.98
2016-09-07T22:14:47.209475: step 5230, loss 0.00784844, acc 1
2016-09-07T22:14:47.877085: step 5231, loss 0.0358955, acc 1
2016-09-07T22:14:48.530779: step 5232, loss 0.00998159, acc 1
2016-09-07T22:14:49.192059: step 5233, loss 0.0106942, acc 1
2016-09-07T22:14:49.863867: step 5234, loss 0.0565659, acc 0.96
2016-09-07T22:14:50.529495: step 5235, loss 0.0671606, acc 0.94
2016-09-07T22:14:51.185220: step 5236, loss 0.0197056, acc 0.98
2016-09-07T22:14:51.844359: step 5237, loss 0.0355339, acc 0.98
2016-09-07T22:14:52.207350: step 5238, loss 0.00451679, acc 1
2016-09-07T22:14:52.865427: step 5239, loss 0.0141893, acc 1
2016-09-07T22:14:53.528092: step 5240, loss 0.0240952, acc 1
2016-09-07T22:14:54.179014: step 5241, loss 0.0556074, acc 0.96
2016-09-07T22:14:54.852274: step 5242, loss 0.0192096, acc 1
2016-09-07T22:14:55.524050: step 5243, loss 0.0699277, acc 0.96
2016-09-07T22:14:56.176447: step 5244, loss 0.0353806, acc 0.98
2016-09-07T22:14:56.845615: step 5245, loss 0.0191999, acc 1
2016-09-07T22:14:57.503054: step 5246, loss 0.0265452, acc 0.98
2016-09-07T22:14:58.187178: step 5247, loss 0.0057227, acc 1
2016-09-07T22:14:58.856550: step 5248, loss 0.00589125, acc 1
2016-09-07T22:14:59.516567: step 5249, loss 0.0321924, acc 0.96
2016-09-07T22:15:00.192195: step 5250, loss 0.0822167, acc 0.98
2016-09-07T22:15:00.893969: step 5251, loss 0.043617, acc 1
2016-09-07T22:15:01.556325: step 5252, loss 0.0308685, acc 1
2016-09-07T22:15:02.215685: step 5253, loss 0.0232413, acc 1
2016-09-07T22:15:02.873983: step 5254, loss 0.00433857, acc 1
2016-09-07T22:15:03.533398: step 5255, loss 0.00723618, acc 1
2016-09-07T22:15:04.207482: step 5256, loss 0.0133717, acc 1
2016-09-07T22:15:04.862571: step 5257, loss 0.0353535, acc 0.98
2016-09-07T22:15:05.531720: step 5258, loss 0.0425469, acc 0.98
2016-09-07T22:15:06.180380: step 5259, loss 0.0986725, acc 0.98
2016-09-07T22:15:06.841137: step 5260, loss 0.0383721, acc 0.98
2016-09-07T22:15:07.521651: step 5261, loss 0.0087945, acc 1
2016-09-07T22:15:08.184599: step 5262, loss 0.0053603, acc 1
2016-09-07T22:15:08.839236: step 5263, loss 0.0161957, acc 1
2016-09-07T22:15:09.522171: step 5264, loss 0.00477012, acc 1
2016-09-07T22:15:10.202730: step 5265, loss 0.0353244, acc 0.98
2016-09-07T22:15:10.879823: step 5266, loss 0.0285976, acc 0.98
2016-09-07T22:15:11.550229: step 5267, loss 0.00939154, acc 1
2016-09-07T22:15:12.207605: step 5268, loss 0.0838478, acc 0.98
2016-09-07T22:15:12.890321: step 5269, loss 0.0193923, acc 1
2016-09-07T22:15:13.541648: step 5270, loss 0.0471594, acc 0.96
2016-09-07T22:15:14.216035: step 5271, loss 0.0409067, acc 0.98
2016-09-07T22:15:14.881309: step 5272, loss 0.0316927, acc 1
2016-09-07T22:15:15.532331: step 5273, loss 0.0270875, acc 0.98
2016-09-07T22:15:16.198043: step 5274, loss 0.0670343, acc 0.98
2016-09-07T22:15:16.854137: step 5275, loss 0.079416, acc 0.98
2016-09-07T22:15:17.524190: step 5276, loss 0.0320397, acc 1
2016-09-07T22:15:18.217717: step 5277, loss 0.00369571, acc 1
2016-09-07T22:15:18.891093: step 5278, loss 0.0307618, acc 0.98
2016-09-07T22:15:19.566150: step 5279, loss 0.0423185, acc 0.98
2016-09-07T22:15:20.231780: step 5280, loss 0.0615309, acc 0.98
2016-09-07T22:15:20.896698: step 5281, loss 0.0316331, acc 0.98
2016-09-07T22:15:21.576793: step 5282, loss 0.00547848, acc 1
2016-09-07T22:15:22.250365: step 5283, loss 0.0109686, acc 1
2016-09-07T22:15:22.951886: step 5284, loss 0.135282, acc 0.98
2016-09-07T22:15:23.626865: step 5285, loss 0.0898519, acc 0.96
2016-09-07T22:15:24.291443: step 5286, loss 0.0178127, acc 1
2016-09-07T22:15:24.952170: step 5287, loss 0.0340211, acc 0.98
2016-09-07T22:15:25.634353: step 5288, loss 0.0453954, acc 0.98
2016-09-07T22:15:26.286441: step 5289, loss 0.0227566, acc 0.98
2016-09-07T22:15:26.968876: step 5290, loss 0.0165573, acc 1
2016-09-07T22:15:27.646489: step 5291, loss 0.0534477, acc 0.98
2016-09-07T22:15:28.321506: step 5292, loss 0.0552678, acc 0.96
2016-09-07T22:15:29.007627: step 5293, loss 0.0240729, acc 1
2016-09-07T22:15:29.675423: step 5294, loss 0.0179847, acc 1
2016-09-07T22:15:30.341839: step 5295, loss 0.037432, acc 1
2016-09-07T22:15:31.013641: step 5296, loss 0.00667916, acc 1
2016-09-07T22:15:31.678255: step 5297, loss 0.0604486, acc 0.96
2016-09-07T22:15:32.342054: step 5298, loss 0.0289095, acc 0.98
2016-09-07T22:15:33.012479: step 5299, loss 0.0295244, acc 0.98
2016-09-07T22:15:33.659973: step 5300, loss 0.0221419, acc 1

Evaluation:
2016-09-07T22:15:36.551442: step 5300, loss 1.8923, acc 0.734

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-5300

2016-09-07T22:15:38.185387: step 5301, loss 0.0212726, acc 1
2016-09-07T22:15:38.838559: step 5302, loss 0.0207044, acc 0.98
2016-09-07T22:15:39.482690: step 5303, loss 0.0361117, acc 0.98
2016-09-07T22:15:40.154568: step 5304, loss 0.00963575, acc 1
2016-09-07T22:15:40.830770: step 5305, loss 0.0326911, acc 1
2016-09-07T22:15:41.520517: step 5306, loss 0.0373099, acc 0.98
2016-09-07T22:15:42.179059: step 5307, loss 0.101338, acc 0.98
2016-09-07T22:15:42.846962: step 5308, loss 0.031877, acc 0.98
2016-09-07T22:15:43.490429: step 5309, loss 0.0589506, acc 0.98
2016-09-07T22:15:44.149003: step 5310, loss 0.019055, acc 1
2016-09-07T22:15:44.805968: step 5311, loss 0.00780194, acc 1
2016-09-07T22:15:45.471526: step 5312, loss 0.00582253, acc 1
2016-09-07T22:15:46.143490: step 5313, loss 0.0104679, acc 1
2016-09-07T22:15:46.817879: step 5314, loss 0.0767891, acc 0.96
2016-09-07T22:15:47.466457: step 5315, loss 0.0305025, acc 0.98
2016-09-07T22:15:48.140009: step 5316, loss 0.0198231, acc 0.98
2016-09-07T22:15:48.819180: step 5317, loss 0.0196799, acc 1
2016-09-07T22:15:49.482486: step 5318, loss 0.182655, acc 0.96
2016-09-07T22:15:50.151829: step 5319, loss 0.0487163, acc 0.98
2016-09-07T22:15:50.814823: step 5320, loss 0.0605058, acc 0.98
2016-09-07T22:15:51.490476: step 5321, loss 0.0185181, acc 1
2016-09-07T22:15:52.161647: step 5322, loss 0.0394315, acc 0.98
2016-09-07T22:15:52.824680: step 5323, loss 0.0270384, acc 0.98
2016-09-07T22:15:53.506289: step 5324, loss 0.00893454, acc 1
2016-09-07T22:15:54.174187: step 5325, loss 0.0261478, acc 0.98
2016-09-07T22:15:54.840215: step 5326, loss 0.0161991, acc 1
2016-09-07T22:15:55.502490: step 5327, loss 0.0153373, acc 1
2016-09-07T22:15:56.165620: step 5328, loss 0.0244888, acc 1
2016-09-07T22:15:56.839140: step 5329, loss 0.0227728, acc 0.98
2016-09-07T22:15:57.513238: step 5330, loss 0.0128804, acc 1
2016-09-07T22:15:58.185115: step 5331, loss 0.0349699, acc 0.98
2016-09-07T22:15:58.858048: step 5332, loss 0.0963676, acc 0.96
2016-09-07T22:15:59.529323: step 5333, loss 0.0824484, acc 0.96
2016-09-07T22:16:00.219555: step 5334, loss 0.059888, acc 0.96
2016-09-07T22:16:00.898964: step 5335, loss 0.0173362, acc 1
2016-09-07T22:16:01.574048: step 5336, loss 0.0617483, acc 0.94
2016-09-07T22:16:02.246793: step 5337, loss 0.0351049, acc 0.98
2016-09-07T22:16:02.917317: step 5338, loss 0.0423005, acc 0.96
2016-09-07T22:16:03.597442: step 5339, loss 0.0147405, acc 1
2016-09-07T22:16:04.263757: step 5340, loss 0.0235686, acc 0.98
2016-09-07T22:16:04.945401: step 5341, loss 0.0774925, acc 0.96
2016-09-07T22:16:05.636603: step 5342, loss 0.00493785, acc 1
2016-09-07T22:16:06.301912: step 5343, loss 0.0521257, acc 0.98
2016-09-07T22:16:06.968520: step 5344, loss 0.0223455, acc 0.98
2016-09-07T22:16:07.636692: step 5345, loss 0.0287188, acc 0.98
2016-09-07T22:16:08.343678: step 5346, loss 0.0412449, acc 0.98
2016-09-07T22:16:09.019224: step 5347, loss 0.0185592, acc 1
2016-09-07T22:16:09.689465: step 5348, loss 0.0788323, acc 0.96
2016-09-07T22:16:10.364671: step 5349, loss 0.11464, acc 0.94
2016-09-07T22:16:11.032792: step 5350, loss 0.0267819, acc 0.98
2016-09-07T22:16:11.713924: step 5351, loss 0.08518, acc 0.94
2016-09-07T22:16:12.374414: step 5352, loss 0.0222519, acc 1
2016-09-07T22:16:13.040753: step 5353, loss 0.0562359, acc 0.96
2016-09-07T22:16:13.716453: step 5354, loss 0.0380333, acc 0.98
2016-09-07T22:16:14.385465: step 5355, loss 0.0280039, acc 1
2016-09-07T22:16:15.051941: step 5356, loss 0.0331921, acc 0.98
2016-09-07T22:16:15.725074: step 5357, loss 0.072807, acc 0.96
2016-09-07T22:16:16.397739: step 5358, loss 0.022781, acc 0.98
2016-09-07T22:16:17.073087: step 5359, loss 0.00913962, acc 1
2016-09-07T22:16:17.745439: step 5360, loss 0.0319461, acc 1
2016-09-07T22:16:18.406588: step 5361, loss 0.0623248, acc 0.96
2016-09-07T22:16:19.064021: step 5362, loss 0.00558204, acc 1
2016-09-07T22:16:19.706914: step 5363, loss 0.0315467, acc 1
2016-09-07T22:16:20.350222: step 5364, loss 0.0268897, acc 1
2016-09-07T22:16:21.016213: step 5365, loss 0.0755817, acc 0.98
2016-09-07T22:16:21.691468: step 5366, loss 0.00613477, acc 1
2016-09-07T22:16:22.390573: step 5367, loss 0.0199347, acc 1
2016-09-07T22:16:23.054408: step 5368, loss 0.00848036, acc 1
2016-09-07T22:16:23.755326: step 5369, loss 0.0959871, acc 0.94
2016-09-07T22:16:24.427846: step 5370, loss 0.0206105, acc 0.98
2016-09-07T22:16:25.110709: step 5371, loss 0.00949274, acc 1
2016-09-07T22:16:25.797139: step 5372, loss 0.0275715, acc 0.98
2016-09-07T22:16:26.474695: step 5373, loss 0.0446822, acc 1
2016-09-07T22:16:27.130878: step 5374, loss 0.0142924, acc 1
2016-09-07T22:16:27.806811: step 5375, loss 0.00582822, acc 1
2016-09-07T22:16:28.487203: step 5376, loss 0.0179626, acc 1
2016-09-07T22:16:29.160624: step 5377, loss 0.00787984, acc 1
2016-09-07T22:16:29.821886: step 5378, loss 0.0468258, acc 0.96
2016-09-07T22:16:30.478773: step 5379, loss 0.0411177, acc 0.98
2016-09-07T22:16:31.146083: step 5380, loss 0.0395485, acc 0.98
2016-09-07T22:16:31.825013: step 5381, loss 0.0444662, acc 0.98
2016-09-07T22:16:32.515855: step 5382, loss 0.0361144, acc 1
2016-09-07T22:16:33.191503: step 5383, loss 0.0327531, acc 0.98
2016-09-07T22:16:33.851015: step 5384, loss 0.00621082, acc 1
2016-09-07T22:16:34.555782: step 5385, loss 0.0561651, acc 0.98
2016-09-07T22:16:35.238982: step 5386, loss 0.224785, acc 0.96
2016-09-07T22:16:35.927305: step 5387, loss 0.0365348, acc 0.96
2016-09-07T22:16:36.607519: step 5388, loss 0.00470914, acc 1
2016-09-07T22:16:37.264777: step 5389, loss 0.0511806, acc 0.98
2016-09-07T22:16:37.927814: step 5390, loss 0.0388033, acc 0.98
2016-09-07T22:16:38.598588: step 5391, loss 0.0318671, acc 0.98
2016-09-07T22:16:39.260912: step 5392, loss 0.04291, acc 0.96
2016-09-07T22:16:39.930640: step 5393, loss 0.0543654, acc 0.96
2016-09-07T22:16:40.610726: step 5394, loss 0.0287972, acc 1
2016-09-07T22:16:41.283390: step 5395, loss 0.0123522, acc 1
2016-09-07T22:16:41.962397: step 5396, loss 0.0133453, acc 1
2016-09-07T22:16:42.630680: step 5397, loss 0.00806471, acc 1
2016-09-07T22:16:43.309808: step 5398, loss 0.153808, acc 0.96
2016-09-07T22:16:43.967987: step 5399, loss 0.0338214, acc 0.98
2016-09-07T22:16:44.628922: step 5400, loss 0.0226078, acc 0.98

Evaluation:
2016-09-07T22:16:47.518021: step 5400, loss 1.6396, acc 0.734

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-5400

2016-09-07T22:16:49.096954: step 5401, loss 0.00443701, acc 1
2016-09-07T22:16:49.772489: step 5402, loss 0.056902, acc 0.96
2016-09-07T22:16:50.451759: step 5403, loss 0.032915, acc 0.98
2016-09-07T22:16:51.126701: step 5404, loss 0.0341265, acc 1
2016-09-07T22:16:51.805707: step 5405, loss 0.0135985, acc 1
2016-09-07T22:16:52.496159: step 5406, loss 0.075454, acc 0.96
2016-09-07T22:16:53.158875: step 5407, loss 0.0371487, acc 1
2016-09-07T22:16:53.840289: step 5408, loss 0.0939599, acc 0.94
2016-09-07T22:16:54.547463: step 5409, loss 0.0311044, acc 0.98
2016-09-07T22:16:55.227243: step 5410, loss 0.0394403, acc 0.98
2016-09-07T22:16:55.902308: step 5411, loss 0.0729819, acc 0.96
2016-09-07T22:16:56.572225: step 5412, loss 0.00711063, acc 1
2016-09-07T22:16:57.261236: step 5413, loss 0.00843375, acc 1
2016-09-07T22:16:57.933773: step 5414, loss 0.00636916, acc 1
2016-09-07T22:16:58.603616: step 5415, loss 0.0258965, acc 1
2016-09-07T22:16:59.259213: step 5416, loss 0.0228848, acc 0.98
2016-09-07T22:16:59.945040: step 5417, loss 0.0371919, acc 0.98
2016-09-07T22:17:00.637990: step 5418, loss 0.0238544, acc 1
2016-09-07T22:17:01.307998: step 5419, loss 0.0257462, acc 0.98
2016-09-07T22:17:01.991279: step 5420, loss 0.0085349, acc 1
2016-09-07T22:17:02.649290: step 5421, loss 0.010303, acc 1
2016-09-07T22:17:03.322167: step 5422, loss 0.0277354, acc 0.98
2016-09-07T22:17:03.985910: step 5423, loss 0.0416623, acc 0.98
2016-09-07T22:17:04.653086: step 5424, loss 0.0100501, acc 1
2016-09-07T22:17:05.325790: step 5425, loss 0.0571738, acc 0.98
2016-09-07T22:17:05.998344: step 5426, loss 0.00707453, acc 1
2016-09-07T22:17:06.661676: step 5427, loss 0.0651235, acc 0.96
2016-09-07T22:17:07.342044: step 5428, loss 0.0883719, acc 0.94
2016-09-07T22:17:08.013580: step 5429, loss 0.0330866, acc 0.98
2016-09-07T22:17:08.691031: step 5430, loss 0.0334682, acc 0.98
2016-09-07T22:17:09.372085: step 5431, loss 0.00691378, acc 1
2016-09-07T22:17:09.715994: step 5432, loss 0.065798, acc 0.916667
2016-09-07T22:17:10.381318: step 5433, loss 0.0216955, acc 1
2016-09-07T22:17:11.035561: step 5434, loss 0.0418953, acc 0.98
2016-09-07T22:17:11.708779: step 5435, loss 0.0608478, acc 0.98
2016-09-07T22:17:12.366114: step 5436, loss 0.0164533, acc 1
2016-09-07T22:17:13.047730: step 5437, loss 0.020822, acc 0.98
2016-09-07T22:17:13.707628: step 5438, loss 0.0385527, acc 1
2016-09-07T22:17:14.372667: step 5439, loss 0.0341373, acc 0.98
2016-09-07T22:17:15.036798: step 5440, loss 0.0124052, acc 1
2016-09-07T22:17:15.706550: step 5441, loss 0.0162743, acc 1
2016-09-07T22:17:16.380780: step 5442, loss 0.0106873, acc 1
2016-09-07T22:17:17.062786: step 5443, loss 0.0269066, acc 0.98
2016-09-07T22:17:17.719166: step 5444, loss 0.0156681, acc 1
2016-09-07T22:17:18.408439: step 5445, loss 0.0503525, acc 0.98
2016-09-07T22:17:19.069238: step 5446, loss 0.0683351, acc 0.94
2016-09-07T22:17:19.734272: step 5447, loss 0.0157661, acc 1
2016-09-07T22:17:20.380668: step 5448, loss 0.116957, acc 0.96
2016-09-07T22:17:21.037363: step 5449, loss 0.0261555, acc 0.98
2016-09-07T22:17:21.684861: step 5450, loss 0.0749204, acc 0.96
2016-09-07T22:17:22.358107: step 5451, loss 0.0253372, acc 1
2016-09-07T22:17:23.022063: step 5452, loss 0.0246508, acc 0.98
2016-09-07T22:17:23.680005: step 5453, loss 0.156304, acc 0.94
2016-09-07T22:17:24.358936: step 5454, loss 0.0264075, acc 0.98
2016-09-07T22:17:25.026496: step 5455, loss 0.0150407, acc 1
2016-09-07T22:17:25.702965: step 5456, loss 0.0347385, acc 0.98
2016-09-07T22:17:26.361662: step 5457, loss 0.0217393, acc 1
2016-09-07T22:17:27.041642: step 5458, loss 0.0318384, acc 0.98
2016-09-07T22:17:27.702506: step 5459, loss 0.0234869, acc 1
2016-09-07T22:17:28.376430: step 5460, loss 0.0222545, acc 1
2016-09-07T22:17:29.039328: step 5461, loss 0.0699869, acc 0.96
2016-09-07T22:17:29.711619: step 5462, loss 0.0355782, acc 1
2016-09-07T22:17:30.379981: step 5463, loss 0.0176234, acc 1
2016-09-07T22:17:31.059016: step 5464, loss 0.0880916, acc 0.94
2016-09-07T22:17:31.731134: step 5465, loss 0.0337732, acc 0.98
2016-09-07T22:17:32.406404: step 5466, loss 0.0117927, acc 1
2016-09-07T22:17:33.074467: step 5467, loss 0.00594023, acc 1
2016-09-07T22:17:33.745796: step 5468, loss 0.0347222, acc 0.98
2016-09-07T22:17:34.423766: step 5469, loss 0.0527918, acc 0.98
2016-09-07T22:17:35.088286: step 5470, loss 0.00700424, acc 1
2016-09-07T22:17:35.761955: step 5471, loss 0.0166395, acc 1
2016-09-07T22:17:36.452310: step 5472, loss 0.00363972, acc 1
2016-09-07T22:17:37.108495: step 5473, loss 0.0265137, acc 1
2016-09-07T22:17:37.802747: step 5474, loss 0.00382355, acc 1
2016-09-07T22:17:38.461770: step 5475, loss 0.0320751, acc 0.98
2016-09-07T22:17:39.126384: step 5476, loss 0.0104705, acc 1
2016-09-07T22:17:39.800412: step 5477, loss 0.0240492, acc 0.98
2016-09-07T22:17:40.484652: step 5478, loss 0.0948299, acc 0.96
2016-09-07T22:17:41.170871: step 5479, loss 0.0932297, acc 0.96
2016-09-07T22:17:41.816713: step 5480, loss 0.020719, acc 0.98
2016-09-07T22:17:42.498453: step 5481, loss 0.00667142, acc 1
2016-09-07T22:17:43.170531: step 5482, loss 0.0194259, acc 1
2016-09-07T22:17:43.827865: step 5483, loss 0.0145655, acc 1
2016-09-07T22:17:44.485020: step 5484, loss 0.0307893, acc 0.98
2016-09-07T22:17:45.138256: step 5485, loss 0.0633356, acc 0.98
2016-09-07T22:17:45.808791: step 5486, loss 0.0579923, acc 0.98
2016-09-07T22:17:46.473792: step 5487, loss 0.00532835, acc 1
2016-09-07T22:17:47.131952: step 5488, loss 0.0249008, acc 1
2016-09-07T22:17:47.806559: step 5489, loss 0.0211065, acc 1
2016-09-07T22:17:48.483282: step 5490, loss 0.00853472, acc 1
2016-09-07T22:17:49.141394: step 5491, loss 0.0231316, acc 1
2016-09-07T22:17:49.803803: step 5492, loss 0.005869, acc 1
2016-09-07T22:17:50.465605: step 5493, loss 0.0461898, acc 0.98
2016-09-07T22:17:51.149335: step 5494, loss 0.00494129, acc 1
2016-09-07T22:17:51.814520: step 5495, loss 0.0320878, acc 1
2016-09-07T22:17:52.465980: step 5496, loss 0.0408407, acc 0.98
2016-09-07T22:17:53.120168: step 5497, loss 0.0238215, acc 1
2016-09-07T22:17:53.790693: step 5498, loss 0.10079, acc 0.98
2016-09-07T22:17:54.463351: step 5499, loss 0.0071092, acc 1
2016-09-07T22:17:55.130946: step 5500, loss 0.00966168, acc 1

Evaluation:
2016-09-07T22:17:58.037869: step 5500, loss 2.31992, acc 0.738

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-5500

2016-09-07T22:17:59.802209: step 5501, loss 0.0306963, acc 0.98
2016-09-07T22:18:00.499342: step 5502, loss 0.0458259, acc 0.98
2016-09-07T22:18:01.178355: step 5503, loss 0.0836883, acc 0.94
2016-09-07T22:18:01.850332: step 5504, loss 0.0268879, acc 1
2016-09-07T22:18:02.536833: step 5505, loss 0.00705195, acc 1
2016-09-07T22:18:03.221592: step 5506, loss 0.00649642, acc 1
2016-09-07T22:18:03.874900: step 5507, loss 0.0479685, acc 0.98
2016-09-07T22:18:04.550189: step 5508, loss 0.0239226, acc 0.98
2016-09-07T22:18:05.204831: step 5509, loss 0.0979575, acc 0.94
2016-09-07T22:18:05.891569: step 5510, loss 0.0058275, acc 1
2016-09-07T22:18:06.553282: step 5511, loss 0.0699643, acc 0.98
2016-09-07T22:18:07.252008: step 5512, loss 0.0403623, acc 0.98
2016-09-07T22:18:07.916193: step 5513, loss 0.0346399, acc 0.98
2016-09-07T22:18:08.595247: step 5514, loss 0.0420953, acc 0.98
2016-09-07T22:18:09.247716: step 5515, loss 0.0481388, acc 0.96
2016-09-07T22:18:09.912523: step 5516, loss 0.0134538, acc 1
2016-09-07T22:18:10.560540: step 5517, loss 0.0254851, acc 0.98
2016-09-07T22:18:11.222479: step 5518, loss 0.0112034, acc 1
2016-09-07T22:18:11.880166: step 5519, loss 0.0085416, acc 1
2016-09-07T22:18:12.553671: step 5520, loss 0.032719, acc 1
2016-09-07T22:18:13.211344: step 5521, loss 0.00479357, acc 1
2016-09-07T22:18:13.871532: step 5522, loss 0.0197118, acc 1
2016-09-07T22:18:14.536450: step 5523, loss 0.0418743, acc 0.98
2016-09-07T22:18:15.198076: step 5524, loss 0.0394167, acc 0.98
2016-09-07T22:18:15.854886: step 5525, loss 0.00645998, acc 1
2016-09-07T22:18:16.519633: step 5526, loss 0.0348089, acc 0.98
2016-09-07T22:18:17.181487: step 5527, loss 0.0297961, acc 1
2016-09-07T22:18:17.852304: step 5528, loss 0.0153903, acc 1
2016-09-07T22:18:18.523495: step 5529, loss 0.00510952, acc 1
2016-09-07T22:18:19.198889: step 5530, loss 0.104184, acc 0.9
2016-09-07T22:18:19.872488: step 5531, loss 0.0327935, acc 1
2016-09-07T22:18:20.545952: step 5532, loss 0.0316282, acc 0.98
2016-09-07T22:18:21.223375: step 5533, loss 0.00685072, acc 1
2016-09-07T22:18:21.892816: step 5534, loss 0.0291884, acc 1
2016-09-07T22:18:22.558291: step 5535, loss 0.0192887, acc 1
2016-09-07T22:18:23.239310: step 5536, loss 0.0208388, acc 1
2016-09-07T22:18:23.940921: step 5537, loss 0.00446225, acc 1
2016-09-07T22:18:24.619063: step 5538, loss 0.00790259, acc 1
2016-09-07T22:18:25.291895: step 5539, loss 0.0110113, acc 1
2016-09-07T22:18:25.960496: step 5540, loss 0.0145715, acc 1
2016-09-07T22:18:26.634978: step 5541, loss 0.0545074, acc 0.98
2016-09-07T22:18:27.312122: step 5542, loss 0.0191551, acc 1
2016-09-07T22:18:28.005166: step 5543, loss 0.0574798, acc 0.98
2016-09-07T22:18:28.676258: step 5544, loss 0.0187129, acc 1
2016-09-07T22:18:29.338392: step 5545, loss 0.013593, acc 1
2016-09-07T22:18:30.008196: step 5546, loss 0.0281031, acc 1
2016-09-07T22:18:30.658506: step 5547, loss 0.0201976, acc 1
2016-09-07T22:18:31.318158: step 5548, loss 0.0291102, acc 1
2016-09-07T22:18:31.992415: step 5549, loss 0.0772488, acc 0.98
2016-09-07T22:18:32.676870: step 5550, loss 0.0270407, acc 1
2016-09-07T22:18:33.352783: step 5551, loss 0.0264091, acc 0.98
2016-09-07T22:18:34.023990: step 5552, loss 0.086868, acc 0.98
2016-09-07T22:18:34.685870: step 5553, loss 0.0259587, acc 0.98
2016-09-07T22:18:35.401505: step 5554, loss 0.0200778, acc 1
2016-09-07T22:18:36.069155: step 5555, loss 0.00606455, acc 1
2016-09-07T22:18:36.737091: step 5556, loss 0.0105608, acc 1
2016-09-07T22:18:37.399096: step 5557, loss 0.0508391, acc 0.98
2016-09-07T22:18:38.060944: step 5558, loss 0.0751149, acc 0.98
2016-09-07T22:18:38.728058: step 5559, loss 0.0963317, acc 0.96
2016-09-07T22:18:39.411381: step 5560, loss 0.0236664, acc 0.98
2016-09-07T22:18:40.075448: step 5561, loss 0.0166604, acc 1
2016-09-07T22:18:40.748842: step 5562, loss 0.0224238, acc 1
2016-09-07T22:18:41.415323: step 5563, loss 0.0366903, acc 0.98
2016-09-07T22:18:42.073808: step 5564, loss 0.0621114, acc 0.96
2016-09-07T22:18:42.751250: step 5565, loss 0.00404823, acc 1
2016-09-07T22:18:43.404627: step 5566, loss 0.0405054, acc 0.98
2016-09-07T22:18:44.063074: step 5567, loss 0.0433212, acc 0.98
2016-09-07T22:18:44.727146: step 5568, loss 0.0101453, acc 1
2016-09-07T22:18:45.379449: step 5569, loss 0.0155769, acc 1
2016-09-07T22:18:46.042961: step 5570, loss 0.00586143, acc 1
2016-09-07T22:18:46.708471: step 5571, loss 0.00502072, acc 1
2016-09-07T22:18:47.387851: step 5572, loss 0.0400373, acc 0.96
2016-09-07T22:18:48.080041: step 5573, loss 0.00818997, acc 1
2016-09-07T22:18:48.745928: step 5574, loss 0.00895302, acc 1
2016-09-07T22:18:49.426670: step 5575, loss 0.0334426, acc 0.98
2016-09-07T22:18:50.102312: step 5576, loss 0.0360324, acc 1
2016-09-07T22:18:50.777562: step 5577, loss 0.0196304, acc 1
2016-09-07T22:18:51.451423: step 5578, loss 0.0510764, acc 0.98
2016-09-07T22:18:52.106819: step 5579, loss 0.0141123, acc 1
2016-09-07T22:18:52.781613: step 5580, loss 0.0358788, acc 0.98
2016-09-07T22:18:53.465097: step 5581, loss 0.0309313, acc 0.98
2016-09-07T22:18:54.137965: step 5582, loss 0.0412328, acc 0.98
2016-09-07T22:18:54.780648: step 5583, loss 0.0382441, acc 0.98
2016-09-07T22:18:55.456501: step 5584, loss 0.0256219, acc 0.98
2016-09-07T22:18:56.138527: step 5585, loss 0.0599611, acc 0.98
2016-09-07T22:18:56.788920: step 5586, loss 0.0738969, acc 0.98
2016-09-07T22:18:57.457548: step 5587, loss 0.0211165, acc 0.98
2016-09-07T22:18:58.140531: step 5588, loss 0.00522817, acc 1
2016-09-07T22:18:58.807003: step 5589, loss 0.00595574, acc 1
2016-09-07T22:18:59.467322: step 5590, loss 0.00774701, acc 1
2016-09-07T22:19:00.155185: step 5591, loss 0.0933655, acc 0.98
2016-09-07T22:19:00.862651: step 5592, loss 0.0901014, acc 0.98
2016-09-07T22:19:01.513027: step 5593, loss 0.0333394, acc 0.98
2016-09-07T22:19:02.171619: step 5594, loss 0.0232289, acc 1
2016-09-07T22:19:02.838762: step 5595, loss 0.0059735, acc 1
2016-09-07T22:19:03.535262: step 5596, loss 0.0717656, acc 0.98
2016-09-07T22:19:04.213889: step 5597, loss 0.0356419, acc 0.98
2016-09-07T22:19:04.898709: step 5598, loss 0.0204901, acc 0.98
2016-09-07T22:19:05.579303: step 5599, loss 0.00584546, acc 1
2016-09-07T22:19:06.259981: step 5600, loss 0.033781, acc 1

Evaluation:
2016-09-07T22:19:09.148407: step 5600, loss 2.34382, acc 0.719

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-5600

2016-09-07T22:19:10.762969: step 5601, loss 0.0454808, acc 0.98
2016-09-07T22:19:11.427923: step 5602, loss 0.0530614, acc 0.98
2016-09-07T22:19:12.083081: step 5603, loss 0.025214, acc 0.98
2016-09-07T22:19:12.744066: step 5604, loss 0.0196275, acc 1
2016-09-07T22:19:13.406523: step 5605, loss 0.0505003, acc 0.98
2016-09-07T22:19:14.068317: step 5606, loss 0.0738574, acc 0.96
2016-09-07T22:19:14.748366: step 5607, loss 0.025308, acc 1
2016-09-07T22:19:15.409385: step 5608, loss 0.0808508, acc 0.98
2016-09-07T22:19:16.070993: step 5609, loss 0.00783049, acc 1
2016-09-07T22:19:16.761065: step 5610, loss 0.0254699, acc 0.98
2016-09-07T22:19:17.441123: step 5611, loss 0.0213569, acc 1
2016-09-07T22:19:18.114432: step 5612, loss 0.0366509, acc 0.96
2016-09-07T22:19:18.807193: step 5613, loss 0.0509147, acc 0.96
2016-09-07T22:19:19.484619: step 5614, loss 0.034184, acc 0.98
2016-09-07T22:19:20.146900: step 5615, loss 0.0171036, acc 1
2016-09-07T22:19:20.808923: step 5616, loss 0.0142665, acc 1
2016-09-07T22:19:21.480464: step 5617, loss 0.027708, acc 0.98
2016-09-07T22:19:22.132384: step 5618, loss 0.0176336, acc 1
2016-09-07T22:19:22.797189: step 5619, loss 0.0570344, acc 0.98
2016-09-07T22:19:23.470559: step 5620, loss 0.0087704, acc 1
2016-09-07T22:19:24.145456: step 5621, loss 0.0494158, acc 0.98
2016-09-07T22:19:24.834507: step 5622, loss 0.0704059, acc 0.96
2016-09-07T22:19:25.504616: step 5623, loss 0.0356793, acc 0.98
2016-09-07T22:19:26.181854: step 5624, loss 0.0250616, acc 1
2016-09-07T22:19:26.851176: step 5625, loss 0.00749657, acc 1
2016-09-07T22:19:27.212766: step 5626, loss 0.00836452, acc 1
2016-09-07T22:19:27.882716: step 5627, loss 0.0268431, acc 1
2016-09-07T22:19:28.564451: step 5628, loss 0.0136792, acc 1
2016-09-07T22:19:29.235006: step 5629, loss 0.0363184, acc 0.98
2016-09-07T22:19:29.888864: step 5630, loss 0.0399679, acc 0.98
2016-09-07T22:19:30.560361: step 5631, loss 0.0733312, acc 0.96
2016-09-07T22:19:31.221355: step 5632, loss 0.0189418, acc 1
2016-09-07T22:19:31.874887: step 5633, loss 0.0214872, acc 0.98
2016-09-07T22:19:32.560074: step 5634, loss 0.0396629, acc 0.98
2016-09-07T22:19:33.236250: step 5635, loss 0.0082744, acc 1
2016-09-07T22:19:33.896043: step 5636, loss 0.0177025, acc 1
2016-09-07T22:19:34.578263: step 5637, loss 0.0381388, acc 0.98
2016-09-07T22:19:35.242448: step 5638, loss 0.0185126, acc 1
2016-09-07T22:19:35.903797: step 5639, loss 0.043793, acc 1
2016-09-07T22:19:36.561287: step 5640, loss 0.073923, acc 0.96
2016-09-07T22:19:37.233351: step 5641, loss 0.00808235, acc 1
2016-09-07T22:19:37.891857: step 5642, loss 0.00581838, acc 1
2016-09-07T22:19:38.564113: step 5643, loss 0.0719668, acc 0.96
2016-09-07T22:19:39.224348: step 5644, loss 0.0207486, acc 1
2016-09-07T22:19:39.901960: step 5645, loss 0.0460328, acc 0.96
2016-09-07T22:19:40.573871: step 5646, loss 0.00539588, acc 1
2016-09-07T22:19:41.244990: step 5647, loss 0.0876202, acc 0.94
2016-09-07T22:19:41.916563: step 5648, loss 0.00539592, acc 1
2016-09-07T22:19:42.580674: step 5649, loss 0.0328288, acc 0.98
2016-09-07T22:19:43.265039: step 5650, loss 0.0186366, acc 1
2016-09-07T22:19:43.936727: step 5651, loss 0.0190356, acc 1
2016-09-07T22:19:44.609141: step 5652, loss 0.00706552, acc 1
2016-09-07T22:19:45.270595: step 5653, loss 0.0121472, acc 1
2016-09-07T22:19:45.935210: step 5654, loss 0.0153043, acc 1
2016-09-07T22:19:46.607739: step 5655, loss 0.0478386, acc 0.98
2016-09-07T22:19:47.285315: step 5656, loss 0.00927132, acc 1
2016-09-07T22:19:47.965387: step 5657, loss 0.0145154, acc 1
2016-09-07T22:19:48.653888: step 5658, loss 0.0595705, acc 1
2016-09-07T22:19:49.324542: step 5659, loss 0.020575, acc 1
2016-09-07T22:19:49.990170: step 5660, loss 0.0569954, acc 0.96
2016-09-07T22:19:50.660695: step 5661, loss 0.00606822, acc 1
2016-09-07T22:19:51.321201: step 5662, loss 0.0191499, acc 1
2016-09-07T22:19:51.975847: step 5663, loss 0.0360881, acc 0.98
2016-09-07T22:19:52.644648: step 5664, loss 0.0352222, acc 0.98
2016-09-07T22:19:53.327163: step 5665, loss 0.0229381, acc 0.98
2016-09-07T22:19:53.995206: step 5666, loss 0.0121092, acc 1
2016-09-07T22:19:54.679899: step 5667, loss 0.0242837, acc 1
2016-09-07T22:19:55.370969: step 5668, loss 0.00955187, acc 1
2016-09-07T22:19:56.044459: step 5669, loss 0.0270358, acc 1
2016-09-07T22:19:56.711969: step 5670, loss 0.00612072, acc 1
2016-09-07T22:19:57.375149: step 5671, loss 0.120634, acc 0.96
2016-09-07T22:19:58.055969: step 5672, loss 0.0540346, acc 0.98
2016-09-07T22:19:58.721650: step 5673, loss 0.0578056, acc 0.96
2016-09-07T22:19:59.410959: step 5674, loss 0.059267, acc 0.96
2016-09-07T22:20:00.105187: step 5675, loss 0.0141566, acc 1
2016-09-07T22:20:00.875458: step 5676, loss 0.0223672, acc 1
2016-09-07T22:20:01.555778: step 5677, loss 0.0142969, acc 1
2016-09-07T22:20:02.228644: step 5678, loss 0.00678283, acc 1
2016-09-07T22:20:02.921220: step 5679, loss 0.048923, acc 0.98
2016-09-07T22:20:03.593549: step 5680, loss 0.0063002, acc 1
2016-09-07T22:20:04.271272: step 5681, loss 0.014304, acc 1
2016-09-07T22:20:04.938311: step 5682, loss 0.0321528, acc 1
2016-09-07T22:20:05.608659: step 5683, loss 0.027533, acc 0.98
2016-09-07T22:20:06.283319: step 5684, loss 0.00474661, acc 1
2016-09-07T22:20:06.935006: step 5685, loss 0.00701201, acc 1
2016-09-07T22:20:07.613096: step 5686, loss 0.00415363, acc 1
2016-09-07T22:20:08.281906: step 5687, loss 0.0193967, acc 1
2016-09-07T22:20:08.945135: step 5688, loss 0.0228586, acc 1
2016-09-07T22:20:09.608413: step 5689, loss 0.0138379, acc 1
2016-09-07T22:20:10.275730: step 5690, loss 0.0305995, acc 1
2016-09-07T22:20:10.938854: step 5691, loss 0.00965589, acc 1
2016-09-07T22:20:11.610139: step 5692, loss 0.0268632, acc 0.98
2016-09-07T22:20:12.299930: step 5693, loss 0.0199547, acc 1
2016-09-07T22:20:12.981962: step 5694, loss 0.0193079, acc 1
2016-09-07T22:20:13.660700: step 5695, loss 0.0340069, acc 0.98
2016-09-07T22:20:14.327074: step 5696, loss 0.0243201, acc 0.98
2016-09-07T22:20:14.974799: step 5697, loss 0.0166352, acc 1
2016-09-07T22:20:15.665663: step 5698, loss 0.0180179, acc 1
2016-09-07T22:20:16.339512: step 5699, loss 0.00392343, acc 1
2016-09-07T22:20:17.016596: step 5700, loss 0.0185008, acc 1

Evaluation:
2016-09-07T22:20:19.921116: step 5700, loss 2.25139, acc 0.731

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-5700

2016-09-07T22:20:21.553607: step 5701, loss 0.012311, acc 1
2016-09-07T22:20:22.224012: step 5702, loss 0.0943978, acc 0.96
2016-09-07T22:20:22.890405: step 5703, loss 0.0364709, acc 0.98
2016-09-07T22:20:23.548982: step 5704, loss 0.00395687, acc 1
2016-09-07T22:20:24.210668: step 5705, loss 0.0208758, acc 1
2016-09-07T22:20:24.889541: step 5706, loss 0.0275762, acc 0.98
2016-09-07T22:20:25.557885: step 5707, loss 0.0307849, acc 0.98
2016-09-07T22:20:26.228274: step 5708, loss 0.172646, acc 0.96
2016-09-07T22:20:26.917512: step 5709, loss 0.036529, acc 0.98
2016-09-07T22:20:27.609450: step 5710, loss 0.00360025, acc 1
2016-09-07T22:20:28.289763: step 5711, loss 0.0114707, acc 1
2016-09-07T22:20:28.951654: step 5712, loss 0.0350061, acc 0.98
2016-09-07T22:20:29.617415: step 5713, loss 0.01328, acc 1
2016-09-07T22:20:30.288755: step 5714, loss 0.0540133, acc 0.98
2016-09-07T22:20:30.960967: step 5715, loss 0.0473472, acc 0.98
2016-09-07T22:20:31.636085: step 5716, loss 0.0272573, acc 0.98
2016-09-07T22:20:32.331422: step 5717, loss 0.00902885, acc 1
2016-09-07T22:20:32.985834: step 5718, loss 0.042317, acc 1
2016-09-07T22:20:33.646872: step 5719, loss 0.0117129, acc 1
2016-09-07T22:20:34.307702: step 5720, loss 0.0467185, acc 0.96
2016-09-07T22:20:34.989738: step 5721, loss 0.143361, acc 0.94
2016-09-07T22:20:35.655159: step 5722, loss 0.0171623, acc 1
2016-09-07T22:20:36.333146: step 5723, loss 0.0101271, acc 1
2016-09-07T22:20:36.999667: step 5724, loss 0.0553352, acc 0.96
2016-09-07T22:20:37.637791: step 5725, loss 0.0494219, acc 0.96
2016-09-07T22:20:38.299869: step 5726, loss 0.0204439, acc 1
2016-09-07T22:20:38.976567: step 5727, loss 0.021757, acc 1
2016-09-07T22:20:39.692800: step 5728, loss 0.026858, acc 0.98
2016-09-07T22:20:40.358825: step 5729, loss 0.00593172, acc 1
2016-09-07T22:20:41.035996: step 5730, loss 0.0350202, acc 0.98
2016-09-07T22:20:41.718501: step 5731, loss 0.0425544, acc 0.98
2016-09-07T22:20:42.386961: step 5732, loss 0.0118877, acc 1
2016-09-07T22:20:43.047514: step 5733, loss 0.0297311, acc 0.98
2016-09-07T22:20:43.713892: step 5734, loss 0.0108371, acc 1
2016-09-07T22:20:44.399997: step 5735, loss 0.0192963, acc 1
2016-09-07T22:20:45.061196: step 5736, loss 0.0198101, acc 1
2016-09-07T22:20:45.759738: step 5737, loss 0.0493264, acc 0.96
2016-09-07T22:20:46.447804: step 5738, loss 0.021994, acc 1
2016-09-07T22:20:47.124696: step 5739, loss 0.0143569, acc 1
2016-09-07T22:20:47.798639: step 5740, loss 0.0206751, acc 0.98
2016-09-07T22:20:48.484965: step 5741, loss 0.039599, acc 0.98
2016-09-07T22:20:49.163881: step 5742, loss 0.0795518, acc 0.98
2016-09-07T22:20:49.821470: step 5743, loss 0.0142626, acc 1
2016-09-07T22:20:50.469080: step 5744, loss 0.0201829, acc 0.98
2016-09-07T22:20:51.124081: step 5745, loss 0.00490139, acc 1
2016-09-07T22:20:51.810068: step 5746, loss 0.0144045, acc 1
2016-09-07T22:20:52.481959: step 5747, loss 0.0297198, acc 1
2016-09-07T22:20:53.146243: step 5748, loss 0.0526552, acc 0.96
2016-09-07T22:20:53.814784: step 5749, loss 0.00481447, acc 1
2016-09-07T22:20:54.490181: step 5750, loss 0.0208233, acc 0.98
2016-09-07T22:20:55.164890: step 5751, loss 0.118626, acc 0.94
2016-09-07T22:20:55.811465: step 5752, loss 0.0673119, acc 0.96
2016-09-07T22:20:56.471068: step 5753, loss 0.137481, acc 0.98
2016-09-07T22:20:57.132108: step 5754, loss 0.0243056, acc 0.98
2016-09-07T22:20:57.803321: step 5755, loss 0.0449147, acc 0.98
2016-09-07T22:20:58.449005: step 5756, loss 0.0601292, acc 0.98
2016-09-07T22:20:59.108943: step 5757, loss 0.0354841, acc 1
2016-09-07T22:20:59.771994: step 5758, loss 0.0427226, acc 1
2016-09-07T22:21:00.477525: step 5759, loss 0.0143129, acc 1
2016-09-07T22:21:01.152509: step 5760, loss 0.0557833, acc 0.98
2016-09-07T22:21:01.833419: step 5761, loss 0.0180887, acc 1
2016-09-07T22:21:02.498750: step 5762, loss 0.0293914, acc 0.98
2016-09-07T22:21:03.162376: step 5763, loss 0.0307969, acc 1
2016-09-07T22:21:03.822605: step 5764, loss 0.00613421, acc 1
2016-09-07T22:21:04.519533: step 5765, loss 0.0422925, acc 0.98
2016-09-07T22:21:05.194156: step 5766, loss 0.00865642, acc 1
2016-09-07T22:21:05.851422: step 5767, loss 0.0206068, acc 1
2016-09-07T22:21:06.505549: step 5768, loss 0.00785081, acc 1
2016-09-07T22:21:07.170277: step 5769, loss 0.0416456, acc 0.98
2016-09-07T22:21:07.826713: step 5770, loss 0.027339, acc 0.98
2016-09-07T22:21:08.490999: step 5771, loss 0.015221, acc 1
2016-09-07T22:21:09.140005: step 5772, loss 0.0307853, acc 0.98
2016-09-07T22:21:09.785805: step 5773, loss 0.0123185, acc 1
2016-09-07T22:21:10.444306: step 5774, loss 0.0962583, acc 0.96
2016-09-07T22:21:11.108918: step 5775, loss 0.00396807, acc 1
2016-09-07T22:21:11.789013: step 5776, loss 0.0212835, acc 0.98
2016-09-07T22:21:12.450692: step 5777, loss 0.018209, acc 0.98
2016-09-07T22:21:13.111491: step 5778, loss 0.0376048, acc 0.98
2016-09-07T22:21:13.777434: step 5779, loss 0.0459325, acc 0.96
2016-09-07T22:21:14.442243: step 5780, loss 0.0392944, acc 0.98
2016-09-07T22:21:15.099549: step 5781, loss 0.0140117, acc 1
2016-09-07T22:21:15.769506: step 5782, loss 0.0131056, acc 1
2016-09-07T22:21:16.424393: step 5783, loss 0.00409814, acc 1
2016-09-07T22:21:17.094129: step 5784, loss 0.0308626, acc 0.98
2016-09-07T22:21:17.760520: step 5785, loss 0.0615085, acc 0.96
2016-09-07T22:21:18.416510: step 5786, loss 0.025926, acc 1
2016-09-07T22:21:19.108465: step 5787, loss 0.0462731, acc 0.98
2016-09-07T22:21:19.783653: step 5788, loss 0.018069, acc 1
2016-09-07T22:21:20.460106: step 5789, loss 0.00480504, acc 1
2016-09-07T22:21:21.128043: step 5790, loss 0.054576, acc 0.98
2016-09-07T22:21:21.795745: step 5791, loss 0.00774091, acc 1
2016-09-07T22:21:22.453491: step 5792, loss 0.0215245, acc 0.98
2016-09-07T22:21:23.107713: step 5793, loss 0.0127847, acc 1
2016-09-07T22:21:23.818651: step 5794, loss 0.032553, acc 0.98
2016-09-07T22:21:24.487189: step 5795, loss 0.0567223, acc 0.96
2016-09-07T22:21:25.134966: step 5796, loss 0.0542642, acc 0.98
2016-09-07T22:21:25.807082: step 5797, loss 0.0219348, acc 0.98
2016-09-07T22:21:26.474137: step 5798, loss 0.0680945, acc 0.96
2016-09-07T22:21:27.127045: step 5799, loss 0.0575774, acc 0.96
2016-09-07T22:21:27.800562: step 5800, loss 0.121271, acc 0.96

Evaluation:
2016-09-07T22:21:30.695880: step 5800, loss 2.06281, acc 0.728

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473253942/checkpoints/model-5800

2016-09-07T22:21:32.398745: step 5801, loss 0.0215463, acc 1
2016-09-07T22:21:33.055865: step 5802, loss 0.0203329, acc 1
2016-09-07T22:21:33.715296: step 5803, loss 0.0136407, acc 1
2016-09-07T22:21:34.368667: step 5804, loss 0.0247897, acc 1
2016-09-07T22:21:35.028666: step 5805, loss 0.0241544, acc 0.98
2016-09-07T22:21:35.684434: step 5806, loss 0.0208229, acc 0.98
2016-09-07T22:21:36.354777: step 5807, loss 0.0212815, acc 1
2016-09-07T22:21:37.030993: step 5808, loss 0.0258529, acc 1
2016-09-07T22:21:37.718893: step 5809, loss 0.0283289, acc 1
2016-09-07T22:21:38.358639: step 5810, loss 0.0621128, acc 0.98
2016-09-07T22:21:39.010213: step 5811, loss 0.0140026, acc 1
2016-09-07T22:21:39.675136: step 5812, loss 0.0462581, acc 0.96
2016-09-07T22:21:40.345146: step 5813, loss 0.0808692, acc 0.94
2016-09-07T22:21:41.007175: step 5814, loss 0.00537031, acc 1
2016-09-07T22:21:41.669780: step 5815, loss 0.0394781, acc 0.98
2016-09-07T22:21:42.336480: step 5816, loss 0.0382341, acc 0.98
2016-09-07T22:21:43.008205: step 5817, loss 0.00971434, acc 1
2016-09-07T22:21:43.661889: step 5818, loss 0.0310659, acc 0.98
2016-09-07T22:21:44.328875: step 5819, loss 0.0041145, acc 1
2016-09-07T22:21:44.678488: step 5820, loss 0.0156805, acc 1

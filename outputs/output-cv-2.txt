WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7fb990977e90>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.
WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7fb990977e50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.

Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=50
CHECKPOINT_EVERY=100
CV_INDEX=2
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=300
EVALUATE_EVERY=100
FILTER_SIZES=3,4,5
HIDDEN_DIM=300
L2_REG_LAMBDA=0
LOG_DEVICE_PLACEMENT=False
NUM_EPOCHS=30
NUM_FILTERS=100
WORD2VEC=GoogleNews-vectors-negative300.bin

Loading data...
Vocabulary Size: 18758
Train/Dev split: 9594/1066
Writing to /home/cil/lstm-context-embeddings/runs/1473165300

Load word2vec file GoogleNews-vectors-negative300.bin

2016-09-06T20:35:20.333075: step 1, loss 0.693147, acc 0.58
2016-09-06T20:35:21.016314: step 2, loss 0.70116, acc 0.5
2016-09-06T20:35:21.689076: step 3, loss 0.714336, acc 0.46
2016-09-06T20:35:22.374171: step 4, loss 0.677746, acc 0.58
2016-09-06T20:35:23.064926: step 5, loss 0.679474, acc 0.58
2016-09-06T20:35:23.770742: step 6, loss 0.693858, acc 0.48
2016-09-06T20:35:24.437425: step 7, loss 0.697673, acc 0.52
2016-09-06T20:35:25.114210: step 8, loss 0.729627, acc 0.42
2016-09-06T20:35:25.796328: step 9, loss 0.661731, acc 0.66
2016-09-06T20:35:26.479372: step 10, loss 0.681442, acc 0.52
2016-09-06T20:35:27.164855: step 11, loss 0.695515, acc 0.52
2016-09-06T20:35:27.849356: step 12, loss 0.68812, acc 0.54
2016-09-06T20:35:28.560885: step 13, loss 0.682674, acc 0.5
2016-09-06T20:35:29.235440: step 14, loss 0.700051, acc 0.54
2016-09-06T20:35:29.935946: step 15, loss 0.678335, acc 0.6
2016-09-06T20:35:30.634506: step 16, loss 0.691782, acc 0.5
2016-09-06T20:35:31.320775: step 17, loss 0.709273, acc 0.44
2016-09-06T20:35:31.993076: step 18, loss 0.727474, acc 0.44
2016-09-06T20:35:32.683276: step 19, loss 0.699873, acc 0.46
2016-09-06T20:35:33.358496: step 20, loss 0.67483, acc 0.6
2016-09-06T20:35:34.009114: step 21, loss 0.669246, acc 0.56
2016-09-06T20:35:34.725713: step 22, loss 0.751444, acc 0.48
2016-09-06T20:35:35.382163: step 23, loss 0.710641, acc 0.46
2016-09-06T20:35:36.068760: step 24, loss 0.649114, acc 0.64
2016-09-06T20:35:36.758577: step 25, loss 0.610965, acc 0.72
2016-09-06T20:35:37.452657: step 26, loss 0.61562, acc 0.72
2016-09-06T20:35:38.124689: step 27, loss 0.669024, acc 0.62
2016-09-06T20:35:38.777422: step 28, loss 0.679159, acc 0.58
2016-09-06T20:35:39.484581: step 29, loss 0.591387, acc 0.78
2016-09-06T20:35:40.171502: step 30, loss 0.713167, acc 0.7
2016-09-06T20:35:40.855743: step 31, loss 0.738107, acc 0.6
2016-09-06T20:35:41.552253: step 32, loss 0.750927, acc 0.54
2016-09-06T20:35:42.237911: step 33, loss 0.559797, acc 0.72
2016-09-06T20:35:42.926264: step 34, loss 0.690735, acc 0.58
2016-09-06T20:35:43.596094: step 35, loss 0.564207, acc 0.72
2016-09-06T20:35:44.301815: step 36, loss 0.583973, acc 0.62
2016-09-06T20:35:44.957320: step 37, loss 0.800944, acc 0.54
2016-09-06T20:35:45.628492: step 38, loss 0.76804, acc 0.46
2016-09-06T20:35:46.324783: step 39, loss 0.733895, acc 0.5
2016-09-06T20:35:47.020936: step 40, loss 0.711557, acc 0.54
2016-09-06T20:35:47.705442: step 41, loss 0.655738, acc 0.64
2016-09-06T20:35:48.395300: step 42, loss 0.679199, acc 0.56
2016-09-06T20:35:49.087568: step 43, loss 0.675875, acc 0.58
2016-09-06T20:35:49.758689: step 44, loss 0.728109, acc 0.48
2016-09-06T20:35:50.421859: step 45, loss 0.756382, acc 0.46
2016-09-06T20:35:51.109640: step 46, loss 0.656879, acc 0.54
2016-09-06T20:35:51.798916: step 47, loss 0.6084, acc 0.56
2016-09-06T20:35:52.474812: step 48, loss 0.696425, acc 0.46
2016-09-06T20:35:53.149153: step 49, loss 0.65103, acc 0.54
2016-09-06T20:35:53.851293: step 50, loss 0.675111, acc 0.56
2016-09-06T20:35:54.513854: step 51, loss 0.654366, acc 0.62
2016-09-06T20:35:55.229617: step 52, loss 0.603091, acc 0.64
2016-09-06T20:35:55.900266: step 53, loss 0.574657, acc 0.72
2016-09-06T20:35:56.581346: step 54, loss 0.558822, acc 0.72
2016-09-06T20:35:57.261968: step 55, loss 0.58756, acc 0.62
2016-09-06T20:35:57.945739: step 56, loss 0.566424, acc 0.7
2016-09-06T20:35:58.630454: step 57, loss 0.672103, acc 0.62
2016-09-06T20:35:59.301339: step 58, loss 0.668786, acc 0.56
2016-09-06T20:35:59.990534: step 59, loss 0.653053, acc 0.68
2016-09-06T20:36:00.687202: step 60, loss 0.67284, acc 0.6
2016-09-06T20:36:01.396636: step 61, loss 0.598743, acc 0.64
2016-09-06T20:36:02.069511: step 62, loss 0.719991, acc 0.5
2016-09-06T20:36:02.758952: step 63, loss 0.5954, acc 0.72
2016-09-06T20:36:03.447130: step 64, loss 0.619582, acc 0.64
2016-09-06T20:36:04.111714: step 65, loss 0.51162, acc 0.86
2016-09-06T20:36:04.818867: step 66, loss 0.594225, acc 0.72
2016-09-06T20:36:05.500115: step 67, loss 0.65042, acc 0.64
2016-09-06T20:36:06.182029: step 68, loss 0.616368, acc 0.66
2016-09-06T20:36:06.858052: step 69, loss 0.567708, acc 0.72
2016-09-06T20:36:07.545134: step 70, loss 0.600487, acc 0.76
2016-09-06T20:36:08.259337: step 71, loss 0.578395, acc 0.74
2016-09-06T20:36:08.930034: step 72, loss 0.537273, acc 0.76
2016-09-06T20:36:09.641790: step 73, loss 0.624381, acc 0.64
2016-09-06T20:36:10.312431: step 74, loss 0.475889, acc 0.8
2016-09-06T20:36:10.987695: step 75, loss 0.584123, acc 0.7
2016-09-06T20:36:11.652930: step 76, loss 0.572399, acc 0.72
2016-09-06T20:36:12.350688: step 77, loss 0.67975, acc 0.7
2016-09-06T20:36:13.047407: step 78, loss 0.642822, acc 0.62
2016-09-06T20:36:13.714483: step 79, loss 0.588131, acc 0.68
2016-09-06T20:36:14.430962: step 80, loss 0.532328, acc 0.72
2016-09-06T20:36:15.149434: step 81, loss 0.582829, acc 0.66
2016-09-06T20:36:15.838737: step 82, loss 0.665063, acc 0.62
2016-09-06T20:36:16.505592: step 83, loss 0.451047, acc 0.84
2016-09-06T20:36:17.177067: step 84, loss 0.568511, acc 0.72
2016-09-06T20:36:17.859317: step 85, loss 0.664411, acc 0.66
2016-09-06T20:36:18.515246: step 86, loss 0.563085, acc 0.76
2016-09-06T20:36:19.226442: step 87, loss 0.510677, acc 0.76
2016-09-06T20:36:19.894910: step 88, loss 0.482807, acc 0.72
2016-09-06T20:36:20.579506: step 89, loss 0.452834, acc 0.76
2016-09-06T20:36:21.262489: step 90, loss 0.501013, acc 0.78
2016-09-06T20:36:21.935782: step 91, loss 0.50408, acc 0.72
2016-09-06T20:36:22.631745: step 92, loss 0.482333, acc 0.8
2016-09-06T20:36:23.285698: step 93, loss 0.504001, acc 0.72
2016-09-06T20:36:23.988927: step 94, loss 0.595439, acc 0.72
2016-09-06T20:36:24.677092: step 95, loss 0.548784, acc 0.72
2016-09-06T20:36:25.361733: step 96, loss 0.663405, acc 0.7
2016-09-06T20:36:26.046643: step 97, loss 0.557648, acc 0.76
2016-09-06T20:36:26.741943: step 98, loss 0.639428, acc 0.64
2016-09-06T20:36:27.411937: step 99, loss 0.477839, acc 0.76
2016-09-06T20:36:28.072283: step 100, loss 0.562096, acc 0.66

Evaluation:
2016-09-06T20:36:31.241405: step 100, loss 0.495376, acc 0.771107

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-100

2016-09-06T20:36:32.971922: step 101, loss 0.446021, acc 0.8
2016-09-06T20:36:33.673458: step 102, loss 0.55155, acc 0.68
2016-09-06T20:36:34.350735: step 103, loss 0.601073, acc 0.72
2016-09-06T20:36:35.031352: step 104, loss 0.513691, acc 0.7
2016-09-06T20:36:35.710555: step 105, loss 0.47696, acc 0.8
2016-09-06T20:36:36.417613: step 106, loss 0.496239, acc 0.8
2016-09-06T20:36:37.104097: step 107, loss 0.460506, acc 0.8
2016-09-06T20:36:37.765509: step 108, loss 0.480989, acc 0.78
2016-09-06T20:36:38.457581: step 109, loss 0.58856, acc 0.64
2016-09-06T20:36:39.123288: step 110, loss 0.555363, acc 0.76
2016-09-06T20:36:39.817088: step 111, loss 0.425879, acc 0.82
2016-09-06T20:36:40.514554: step 112, loss 0.55859, acc 0.76
2016-09-06T20:36:41.217179: step 113, loss 0.511532, acc 0.7
2016-09-06T20:36:41.896919: step 114, loss 0.482215, acc 0.76
2016-09-06T20:36:42.557331: step 115, loss 0.567038, acc 0.78
2016-09-06T20:36:43.257810: step 116, loss 0.432841, acc 0.78
2016-09-06T20:36:43.951048: step 117, loss 0.622773, acc 0.7
2016-09-06T20:36:44.634426: step 118, loss 0.531085, acc 0.72
2016-09-06T20:36:45.312428: step 119, loss 0.559789, acc 0.7
2016-09-06T20:36:45.990739: step 120, loss 0.557758, acc 0.74
2016-09-06T20:36:46.661725: step 121, loss 0.548766, acc 0.7
2016-09-06T20:36:47.317217: step 122, loss 0.568111, acc 0.62
2016-09-06T20:36:48.006959: step 123, loss 0.48033, acc 0.78
2016-09-06T20:36:48.686153: step 124, loss 0.552415, acc 0.68
2016-09-06T20:36:49.373575: step 125, loss 0.641943, acc 0.62
2016-09-06T20:36:50.115610: step 126, loss 0.551324, acc 0.72
2016-09-06T20:36:50.799904: step 127, loss 0.60555, acc 0.7
2016-09-06T20:36:51.490102: step 128, loss 0.502133, acc 0.76
2016-09-06T20:36:52.152645: step 129, loss 0.503803, acc 0.74
2016-09-06T20:36:52.853585: step 130, loss 0.487017, acc 0.8
2016-09-06T20:36:53.532217: step 131, loss 0.49835, acc 0.74
2016-09-06T20:36:54.209485: step 132, loss 0.54321, acc 0.76
2016-09-06T20:36:54.895086: step 133, loss 0.58045, acc 0.7
2016-09-06T20:36:55.563682: step 134, loss 0.607588, acc 0.7
2016-09-06T20:36:56.255872: step 135, loss 0.509405, acc 0.78
2016-09-06T20:36:56.928391: step 136, loss 0.535582, acc 0.7
2016-09-06T20:36:57.622293: step 137, loss 0.532738, acc 0.64
2016-09-06T20:36:58.281540: step 138, loss 0.497675, acc 0.74
2016-09-06T20:36:58.951345: step 139, loss 0.475067, acc 0.74
2016-09-06T20:36:59.629827: step 140, loss 0.437144, acc 0.78
2016-09-06T20:37:00.351388: step 141, loss 0.543859, acc 0.7
2016-09-06T20:37:01.033623: step 142, loss 0.566501, acc 0.72
2016-09-06T20:37:01.712282: step 143, loss 0.433521, acc 0.8
2016-09-06T20:37:02.395881: step 144, loss 0.423149, acc 0.82
2016-09-06T20:37:03.046602: step 145, loss 0.408257, acc 0.82
2016-09-06T20:37:03.737424: step 146, loss 0.581251, acc 0.68
2016-09-06T20:37:04.422796: step 147, loss 0.545442, acc 0.66
2016-09-06T20:37:05.102082: step 148, loss 0.436479, acc 0.76
2016-09-06T20:37:05.792930: step 149, loss 0.464551, acc 0.8
2016-09-06T20:37:06.475322: step 150, loss 0.433732, acc 0.78
2016-09-06T20:37:07.172041: step 151, loss 0.54019, acc 0.74
2016-09-06T20:37:07.831557: step 152, loss 0.405401, acc 0.82
2016-09-06T20:37:08.519899: step 153, loss 0.473442, acc 0.8
2016-09-06T20:37:09.174608: step 154, loss 0.46839, acc 0.8
2016-09-06T20:37:09.863029: step 155, loss 0.467635, acc 0.78
2016-09-06T20:37:10.560952: step 156, loss 0.537834, acc 0.76
2016-09-06T20:37:11.243260: step 157, loss 0.492815, acc 0.8
2016-09-06T20:37:11.948873: step 158, loss 0.575148, acc 0.7
2016-09-06T20:37:12.596768: step 159, loss 0.54948, acc 0.72
2016-09-06T20:37:13.293511: step 160, loss 0.398367, acc 0.84
2016-09-06T20:37:13.949481: step 161, loss 0.599866, acc 0.72
2016-09-06T20:37:14.633593: step 162, loss 0.581597, acc 0.66
2016-09-06T20:37:15.304699: step 163, loss 0.401231, acc 0.8
2016-09-06T20:37:15.986984: step 164, loss 0.488248, acc 0.78
2016-09-06T20:37:16.674540: step 165, loss 0.538969, acc 0.78
2016-09-06T20:37:17.356281: step 166, loss 0.519588, acc 0.72
2016-09-06T20:37:18.081138: step 167, loss 0.370959, acc 0.82
2016-09-06T20:37:18.756370: step 168, loss 0.524389, acc 0.68
2016-09-06T20:37:19.442862: step 169, loss 0.521727, acc 0.74
2016-09-06T20:37:20.130445: step 170, loss 0.469454, acc 0.78
2016-09-06T20:37:20.822614: step 171, loss 0.511556, acc 0.72
2016-09-06T20:37:21.497514: step 172, loss 0.469452, acc 0.74
2016-09-06T20:37:22.154034: step 173, loss 0.55617, acc 0.72
2016-09-06T20:37:22.859522: step 174, loss 0.443807, acc 0.82
2016-09-06T20:37:23.524450: step 175, loss 0.39216, acc 0.82
2016-09-06T20:37:24.216429: step 176, loss 0.470098, acc 0.78
2016-09-06T20:37:24.903750: step 177, loss 0.442762, acc 0.74
2016-09-06T20:37:25.593685: step 178, loss 0.398369, acc 0.82
2016-09-06T20:37:26.255727: step 179, loss 0.454768, acc 0.8
2016-09-06T20:37:26.936593: step 180, loss 0.42738, acc 0.8
2016-09-06T20:37:27.604174: step 181, loss 0.460267, acc 0.8
2016-09-06T20:37:28.258359: step 182, loss 0.620472, acc 0.64
2016-09-06T20:37:28.947016: step 183, loss 0.482675, acc 0.64
2016-09-06T20:37:29.628067: step 184, loss 0.456635, acc 0.74
2016-09-06T20:37:30.299650: step 185, loss 0.542579, acc 0.7
2016-09-06T20:37:30.989520: step 186, loss 0.543866, acc 0.78
2016-09-06T20:37:31.667623: step 187, loss 0.48593, acc 0.76
2016-09-06T20:37:32.347938: step 188, loss 0.418822, acc 0.84
2016-09-06T20:37:33.026559: step 189, loss 0.426914, acc 0.8
2016-09-06T20:37:33.727554: step 190, loss 0.339011, acc 0.84
2016-09-06T20:37:34.399111: step 191, loss 0.644328, acc 0.74
2016-09-06T20:37:35.051260: step 192, loss 0.564872, acc 0.681818
2016-09-06T20:37:35.715793: step 193, loss 0.434329, acc 0.86
2016-09-06T20:37:36.398122: step 194, loss 0.486854, acc 0.78
2016-09-06T20:37:37.095804: step 195, loss 0.340293, acc 0.92
2016-09-06T20:37:37.789891: step 196, loss 0.506998, acc 0.76
2016-09-06T20:37:38.492198: step 197, loss 0.306799, acc 0.9
2016-09-06T20:37:39.151282: step 198, loss 0.284742, acc 0.94
2016-09-06T20:37:39.854759: step 199, loss 0.42008, acc 0.84
2016-09-06T20:37:40.545851: step 200, loss 0.508815, acc 0.78

Evaluation:
2016-09-06T20:37:43.702678: step 200, loss 0.437, acc 0.803002

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-200

2016-09-06T20:37:45.379805: step 201, loss 0.334751, acc 0.86
2016-09-06T20:37:46.082242: step 202, loss 0.345834, acc 0.84
2016-09-06T20:37:46.783097: step 203, loss 0.309798, acc 0.86
2016-09-06T20:37:47.447424: step 204, loss 0.404065, acc 0.86
2016-09-06T20:37:48.141038: step 205, loss 0.439189, acc 0.8
2016-09-06T20:37:48.797417: step 206, loss 0.33585, acc 0.88
2016-09-06T20:37:49.480281: step 207, loss 0.305076, acc 0.84
2016-09-06T20:37:50.186660: step 208, loss 0.289363, acc 0.86
2016-09-06T20:37:50.861795: step 209, loss 0.424241, acc 0.8
2016-09-06T20:37:51.568711: step 210, loss 0.282674, acc 0.88
2016-09-06T20:37:52.254935: step 211, loss 0.226319, acc 0.92
2016-09-06T20:37:52.966583: step 212, loss 0.402366, acc 0.84
2016-09-06T20:37:53.643091: step 213, loss 0.328231, acc 0.88
2016-09-06T20:37:54.322881: step 214, loss 0.479205, acc 0.8
2016-09-06T20:37:55.005073: step 215, loss 0.373718, acc 0.86
2016-09-06T20:37:55.683698: step 216, loss 0.348145, acc 0.84
2016-09-06T20:37:56.380839: step 217, loss 0.479383, acc 0.76
2016-09-06T20:37:57.061724: step 218, loss 0.164373, acc 0.98
2016-09-06T20:37:57.770810: step 219, loss 0.233405, acc 0.94
2016-09-06T20:37:58.443161: step 220, loss 0.305782, acc 0.86
2016-09-06T20:37:59.111343: step 221, loss 0.331217, acc 0.8
2016-09-06T20:37:59.793907: step 222, loss 0.265159, acc 0.9
2016-09-06T20:38:00.499593: step 223, loss 0.444994, acc 0.8
2016-09-06T20:38:01.177030: step 224, loss 0.487141, acc 0.8
2016-09-06T20:38:01.854951: step 225, loss 0.316601, acc 0.86
2016-09-06T20:38:02.533336: step 226, loss 0.283906, acc 0.88
2016-09-06T20:38:03.191327: step 227, loss 0.285376, acc 0.9
2016-09-06T20:38:03.883488: step 228, loss 0.275443, acc 0.86
2016-09-06T20:38:04.562105: step 229, loss 0.351439, acc 0.82
2016-09-06T20:38:05.236712: step 230, loss 0.380429, acc 0.82
2016-09-06T20:38:05.921100: step 231, loss 0.448795, acc 0.84
2016-09-06T20:38:06.606506: step 232, loss 0.424453, acc 0.84
2016-09-06T20:38:07.310359: step 233, loss 0.434462, acc 0.8
2016-09-06T20:38:07.973417: step 234, loss 0.367204, acc 0.84
2016-09-06T20:38:08.680346: step 235, loss 0.350126, acc 0.86
2016-09-06T20:38:09.351130: step 236, loss 0.213119, acc 0.92
2016-09-06T20:38:10.033309: step 237, loss 0.537865, acc 0.7
2016-09-06T20:38:10.719456: step 238, loss 0.331602, acc 0.88
2016-09-06T20:38:11.416123: step 239, loss 0.449372, acc 0.78
2016-09-06T20:38:12.085656: step 240, loss 0.387884, acc 0.84
2016-09-06T20:38:12.755031: step 241, loss 0.337993, acc 0.86
2016-09-06T20:38:13.455160: step 242, loss 0.365771, acc 0.8
2016-09-06T20:38:14.133087: step 243, loss 0.475281, acc 0.76
2016-09-06T20:38:14.816674: step 244, loss 0.373217, acc 0.88
2016-09-06T20:38:15.493728: step 245, loss 0.31012, acc 0.88
2016-09-06T20:38:16.168541: step 246, loss 0.48537, acc 0.74
2016-09-06T20:38:16.871670: step 247, loss 0.437007, acc 0.78
2016-09-06T20:38:17.536777: step 248, loss 0.491984, acc 0.8
2016-09-06T20:38:18.252969: step 249, loss 0.396186, acc 0.82
2016-09-06T20:38:18.920728: step 250, loss 0.4544, acc 0.74
2016-09-06T20:38:19.593520: step 251, loss 0.357308, acc 0.86
2016-09-06T20:38:20.273871: step 252, loss 0.298314, acc 0.86
2016-09-06T20:38:20.943548: step 253, loss 0.248462, acc 0.9
2016-09-06T20:38:21.630129: step 254, loss 0.49889, acc 0.76
2016-09-06T20:38:22.303097: step 255, loss 0.480369, acc 0.68
2016-09-06T20:38:23.040650: step 256, loss 0.510887, acc 0.7
2016-09-06T20:38:23.714504: step 257, loss 0.368306, acc 0.82
2016-09-06T20:38:24.396633: step 258, loss 0.365233, acc 0.84
2016-09-06T20:38:25.074119: step 259, loss 0.340983, acc 0.82
2016-09-06T20:38:25.742399: step 260, loss 0.440165, acc 0.82
2016-09-06T20:38:26.423454: step 261, loss 0.440361, acc 0.8
2016-09-06T20:38:27.123402: step 262, loss 0.503039, acc 0.78
2016-09-06T20:38:27.790320: step 263, loss 0.39375, acc 0.8
2016-09-06T20:38:28.446583: step 264, loss 0.440471, acc 0.82
2016-09-06T20:38:29.149836: step 265, loss 0.303332, acc 0.86
2016-09-06T20:38:29.828299: step 266, loss 0.241652, acc 0.88
2016-09-06T20:38:30.501518: step 267, loss 0.345494, acc 0.86
2016-09-06T20:38:31.180652: step 268, loss 0.374138, acc 0.8
2016-09-06T20:38:31.859663: step 269, loss 0.332511, acc 0.82
2016-09-06T20:38:32.551683: step 270, loss 0.566525, acc 0.8
2016-09-06T20:38:33.209768: step 271, loss 0.268692, acc 0.88
2016-09-06T20:38:33.902124: step 272, loss 0.388335, acc 0.82
2016-09-06T20:38:34.594349: step 273, loss 0.532144, acc 0.76
2016-09-06T20:38:35.275600: step 274, loss 0.484828, acc 0.8
2016-09-06T20:38:35.952612: step 275, loss 0.291579, acc 0.88
2016-09-06T20:38:36.638429: step 276, loss 0.392116, acc 0.84
2016-09-06T20:38:37.320656: step 277, loss 0.355718, acc 0.76
2016-09-06T20:38:37.986104: step 278, loss 0.414522, acc 0.78
2016-09-06T20:38:38.672541: step 279, loss 0.388111, acc 0.86
2016-09-06T20:38:39.348585: step 280, loss 0.288502, acc 0.9
2016-09-06T20:38:40.050083: step 281, loss 0.434129, acc 0.84
2016-09-06T20:38:40.745327: step 282, loss 0.442931, acc 0.74
2016-09-06T20:38:41.435500: step 283, loss 0.354952, acc 0.84
2016-09-06T20:38:42.124384: step 284, loss 0.328019, acc 0.84
2016-09-06T20:38:42.804661: step 285, loss 0.364135, acc 0.86
2016-09-06T20:38:43.506508: step 286, loss 0.309209, acc 0.9
2016-09-06T20:38:44.197387: step 287, loss 0.498851, acc 0.78
2016-09-06T20:38:44.889559: step 288, loss 0.423831, acc 0.82
2016-09-06T20:38:45.558597: step 289, loss 0.443177, acc 0.8
2016-09-06T20:38:46.237374: step 290, loss 0.422513, acc 0.76
2016-09-06T20:38:46.940532: step 291, loss 0.334182, acc 0.8
2016-09-06T20:38:47.613307: step 292, loss 0.327317, acc 0.9
2016-09-06T20:38:48.298994: step 293, loss 0.431949, acc 0.82
2016-09-06T20:38:48.972348: step 294, loss 0.437417, acc 0.74
2016-09-06T20:38:49.640687: step 295, loss 0.331478, acc 0.84
2016-09-06T20:38:50.324885: step 296, loss 0.437898, acc 0.76
2016-09-06T20:38:51.025045: step 297, loss 0.45261, acc 0.72
2016-09-06T20:38:51.729223: step 298, loss 0.400429, acc 0.84
2016-09-06T20:38:52.402885: step 299, loss 0.469888, acc 0.78
2016-09-06T20:38:53.102847: step 300, loss 0.429842, acc 0.78

Evaluation:
2016-09-06T20:38:56.238385: step 300, loss 0.438514, acc 0.795497

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-300

2016-09-06T20:38:57.898514: step 301, loss 0.351712, acc 0.86
2016-09-06T20:38:58.581668: step 302, loss 0.340408, acc 0.86
2016-09-06T20:38:59.264870: step 303, loss 0.397141, acc 0.84
2016-09-06T20:38:59.957448: step 304, loss 0.310928, acc 0.9
2016-09-06T20:39:00.669640: step 305, loss 0.383227, acc 0.88
2016-09-06T20:39:01.378111: step 306, loss 0.381007, acc 0.82
2016-09-06T20:39:02.046668: step 307, loss 0.341409, acc 0.86
2016-09-06T20:39:02.759598: step 308, loss 0.363411, acc 0.84
2016-09-06T20:39:03.444486: step 309, loss 0.30574, acc 0.88
2016-09-06T20:39:04.170829: step 310, loss 0.486257, acc 0.78
2016-09-06T20:39:04.864211: step 311, loss 0.423437, acc 0.86
2016-09-06T20:39:05.534458: step 312, loss 0.335441, acc 0.82
2016-09-06T20:39:06.223574: step 313, loss 0.371376, acc 0.84
2016-09-06T20:39:06.894138: step 314, loss 0.27654, acc 0.9
2016-09-06T20:39:07.591643: step 315, loss 0.408307, acc 0.78
2016-09-06T20:39:08.282984: step 316, loss 0.2778, acc 0.86
2016-09-06T20:39:08.956825: step 317, loss 0.236292, acc 0.94
2016-09-06T20:39:09.645705: step 318, loss 0.431493, acc 0.88
2016-09-06T20:39:10.348458: step 319, loss 0.218847, acc 0.88
2016-09-06T20:39:11.035742: step 320, loss 0.219867, acc 0.88
2016-09-06T20:39:11.697516: step 321, loss 0.21891, acc 0.9
2016-09-06T20:39:12.394251: step 322, loss 0.554613, acc 0.78
2016-09-06T20:39:13.055417: step 323, loss 0.314304, acc 0.86
2016-09-06T20:39:13.739882: step 324, loss 0.338663, acc 0.88
2016-09-06T20:39:14.442071: step 325, loss 0.52592, acc 0.72
2016-09-06T20:39:15.134905: step 326, loss 0.374334, acc 0.82
2016-09-06T20:39:15.821734: step 327, loss 0.237795, acc 0.92
2016-09-06T20:39:16.490526: step 328, loss 0.44286, acc 0.76
2016-09-06T20:39:17.187450: step 329, loss 0.411494, acc 0.78
2016-09-06T20:39:17.848944: step 330, loss 0.39464, acc 0.8
2016-09-06T20:39:18.529088: step 331, loss 0.391824, acc 0.84
2016-09-06T20:39:19.203885: step 332, loss 0.431523, acc 0.84
2016-09-06T20:39:19.876411: step 333, loss 0.318306, acc 0.88
2016-09-06T20:39:20.553010: step 334, loss 0.481711, acc 0.74
2016-09-06T20:39:21.219050: step 335, loss 0.352931, acc 0.8
2016-09-06T20:39:21.914302: step 336, loss 0.422211, acc 0.8
2016-09-06T20:39:22.583764: step 337, loss 0.383737, acc 0.84
2016-09-06T20:39:23.261631: step 338, loss 0.316568, acc 0.9
2016-09-06T20:39:23.962787: step 339, loss 0.276159, acc 0.88
2016-09-06T20:39:24.649460: step 340, loss 0.488406, acc 0.8
2016-09-06T20:39:25.339657: step 341, loss 0.279873, acc 0.94
2016-09-06T20:39:26.042699: step 342, loss 0.447012, acc 0.78
2016-09-06T20:39:26.733406: step 343, loss 0.496951, acc 0.76
2016-09-06T20:39:27.413242: step 344, loss 0.469853, acc 0.8
2016-09-06T20:39:28.089828: step 345, loss 0.296526, acc 0.88
2016-09-06T20:39:28.769995: step 346, loss 0.335683, acc 0.84
2016-09-06T20:39:29.449860: step 347, loss 0.39724, acc 0.76
2016-09-06T20:39:30.144100: step 348, loss 0.27051, acc 0.86
2016-09-06T20:39:30.821418: step 349, loss 0.429151, acc 0.76
2016-09-06T20:39:31.498735: step 350, loss 0.361, acc 0.86
2016-09-06T20:39:32.182064: step 351, loss 0.270247, acc 0.88
2016-09-06T20:39:32.872540: step 352, loss 0.322508, acc 0.86
2016-09-06T20:39:33.544344: step 353, loss 0.243361, acc 0.86
2016-09-06T20:39:34.243532: step 354, loss 0.271775, acc 0.88
2016-09-06T20:39:34.923259: step 355, loss 0.371074, acc 0.84
2016-09-06T20:39:35.601649: step 356, loss 0.399557, acc 0.84
2016-09-06T20:39:36.273139: step 357, loss 0.316519, acc 0.78
2016-09-06T20:39:36.934772: step 358, loss 0.560164, acc 0.7
2016-09-06T20:39:37.676134: step 359, loss 0.3087, acc 0.88
2016-09-06T20:39:38.371395: step 360, loss 0.404738, acc 0.84
2016-09-06T20:39:39.060307: step 361, loss 0.224859, acc 0.92
2016-09-06T20:39:39.748168: step 362, loss 0.427169, acc 0.78
2016-09-06T20:39:40.425118: step 363, loss 0.324837, acc 0.84
2016-09-06T20:39:41.113248: step 364, loss 0.320379, acc 0.84
2016-09-06T20:39:41.779465: step 365, loss 0.39911, acc 0.78
2016-09-06T20:39:42.474494: step 366, loss 0.414967, acc 0.78
2016-09-06T20:39:43.169469: step 367, loss 0.4953, acc 0.82
2016-09-06T20:39:43.848109: step 368, loss 0.407639, acc 0.84
2016-09-06T20:39:44.513467: step 369, loss 0.504974, acc 0.78
2016-09-06T20:39:45.192816: step 370, loss 0.350439, acc 0.86
2016-09-06T20:39:45.901574: step 371, loss 0.499279, acc 0.74
2016-09-06T20:39:46.562848: step 372, loss 0.43483, acc 0.8
2016-09-06T20:39:47.267391: step 373, loss 0.270828, acc 0.92
2016-09-06T20:39:47.956735: step 374, loss 0.452785, acc 0.8
2016-09-06T20:39:48.631150: step 375, loss 0.392338, acc 0.78
2016-09-06T20:39:49.319136: step 376, loss 0.263565, acc 0.98
2016-09-06T20:39:49.992100: step 377, loss 0.296954, acc 0.88
2016-09-06T20:39:50.684074: step 378, loss 0.388635, acc 0.82
2016-09-06T20:39:51.333056: step 379, loss 0.263498, acc 0.9
2016-09-06T20:39:52.039687: step 380, loss 0.554228, acc 0.72
2016-09-06T20:39:52.717476: step 381, loss 0.481297, acc 0.78
2016-09-06T20:39:53.401594: step 382, loss 0.39008, acc 0.86
2016-09-06T20:39:54.072948: step 383, loss 0.294585, acc 0.86
2016-09-06T20:39:54.696315: step 384, loss 0.296052, acc 0.840909
2016-09-06T20:39:55.411297: step 385, loss 0.318648, acc 0.88
2016-09-06T20:39:56.100425: step 386, loss 0.380407, acc 0.82
2016-09-06T20:39:56.814246: step 387, loss 0.294764, acc 0.84
2016-09-06T20:39:57.494476: step 388, loss 0.239468, acc 0.94
2016-09-06T20:39:58.163861: step 389, loss 0.265609, acc 0.84
2016-09-06T20:39:58.875938: step 390, loss 0.237056, acc 0.92
2016-09-06T20:39:59.548515: step 391, loss 0.241925, acc 0.92
2016-09-06T20:40:00.250477: step 392, loss 0.227949, acc 0.9
2016-09-06T20:40:00.916095: step 393, loss 0.199015, acc 0.92
2016-09-06T20:40:01.609076: step 394, loss 0.207744, acc 0.9
2016-09-06T20:40:02.284352: step 395, loss 0.324911, acc 0.88
2016-09-06T20:40:02.966183: step 396, loss 0.289636, acc 0.88
2016-09-06T20:40:03.650872: step 397, loss 0.431398, acc 0.84
2016-09-06T20:40:04.329707: step 398, loss 0.23139, acc 0.86
2016-09-06T20:40:05.034102: step 399, loss 0.120375, acc 0.96
2016-09-06T20:40:05.719664: step 400, loss 0.249611, acc 0.94

Evaluation:
2016-09-06T20:40:08.880450: step 400, loss 0.582046, acc 0.778612

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-400

2016-09-06T20:40:10.486772: step 401, loss 0.234373, acc 0.92
2016-09-06T20:40:11.183613: step 402, loss 0.263912, acc 0.92
2016-09-06T20:40:11.841373: step 403, loss 0.368024, acc 0.76
2016-09-06T20:40:12.532785: step 404, loss 0.132708, acc 0.96
2016-09-06T20:40:13.212052: step 405, loss 0.296566, acc 0.88
2016-09-06T20:40:13.884242: step 406, loss 0.159791, acc 0.94
2016-09-06T20:40:14.571728: step 407, loss 0.227914, acc 0.9
2016-09-06T20:40:15.255496: step 408, loss 0.484789, acc 0.86
2016-09-06T20:40:15.949464: step 409, loss 0.3813, acc 0.9
2016-09-06T20:40:16.618461: step 410, loss 0.193479, acc 0.92
2016-09-06T20:40:17.324310: step 411, loss 0.25742, acc 0.9
2016-09-06T20:40:17.977880: step 412, loss 0.272172, acc 0.92
2016-09-06T20:40:18.657033: step 413, loss 0.228743, acc 0.92
2016-09-06T20:40:19.323117: step 414, loss 0.345848, acc 0.84
2016-09-06T20:40:20.000165: step 415, loss 0.171083, acc 0.92
2016-09-06T20:40:20.690530: step 416, loss 0.229031, acc 0.92
2016-09-06T20:40:21.362841: step 417, loss 0.474245, acc 0.84
2016-09-06T20:40:22.054061: step 418, loss 0.324068, acc 0.86
2016-09-06T20:40:22.726733: step 419, loss 0.226912, acc 0.9
2016-09-06T20:40:23.392665: step 420, loss 0.276073, acc 0.86
2016-09-06T20:40:24.083992: step 421, loss 0.297329, acc 0.88
2016-09-06T20:40:24.775150: step 422, loss 0.328141, acc 0.9
2016-09-06T20:40:25.452765: step 423, loss 0.238252, acc 0.9
2016-09-06T20:40:26.148072: step 424, loss 0.21524, acc 0.96
2016-09-06T20:40:26.868558: step 425, loss 0.28936, acc 0.84
2016-09-06T20:40:27.538271: step 426, loss 0.359866, acc 0.84
2016-09-06T20:40:28.218290: step 427, loss 0.2932, acc 0.88
2016-09-06T20:40:28.885915: step 428, loss 0.308269, acc 0.9
2016-09-06T20:40:29.545506: step 429, loss 0.214362, acc 0.92
2016-09-06T20:40:30.232007: step 430, loss 0.20772, acc 0.9
2016-09-06T20:40:30.902924: step 431, loss 0.319132, acc 0.86
2016-09-06T20:40:31.585952: step 432, loss 0.238419, acc 0.94
2016-09-06T20:40:32.249615: step 433, loss 0.196645, acc 0.94
2016-09-06T20:40:32.952577: step 434, loss 0.182568, acc 0.94
2016-09-06T20:40:33.665532: step 435, loss 0.265819, acc 0.88
2016-09-06T20:40:34.345108: step 436, loss 0.300189, acc 0.86
2016-09-06T20:40:35.045727: step 437, loss 0.232699, acc 0.88
2016-09-06T20:40:35.721867: step 438, loss 0.253642, acc 0.88
2016-09-06T20:40:36.438853: step 439, loss 0.274641, acc 0.86
2016-09-06T20:40:37.093528: step 440, loss 0.277072, acc 0.92
2016-09-06T20:40:37.780435: step 441, loss 0.141704, acc 0.98
2016-09-06T20:40:38.463493: step 442, loss 0.156214, acc 0.96
2016-09-06T20:40:39.134786: step 443, loss 0.203578, acc 0.92
2016-09-06T20:40:39.823645: step 444, loss 0.259127, acc 0.88
2016-09-06T20:40:40.521615: step 445, loss 0.192392, acc 0.86
2016-09-06T20:40:41.212588: step 446, loss 0.0986919, acc 0.98
2016-09-06T20:40:41.875919: step 447, loss 0.251667, acc 0.84
2016-09-06T20:40:42.572134: step 448, loss 0.213443, acc 0.92
2016-09-06T20:40:43.288421: step 449, loss 0.300508, acc 0.88
2016-09-06T20:40:43.975998: step 450, loss 0.175114, acc 0.94
2016-09-06T20:40:44.671346: step 451, loss 0.165027, acc 0.94
2016-09-06T20:40:45.352463: step 452, loss 0.247349, acc 0.86
2016-09-06T20:40:46.065665: step 453, loss 0.280511, acc 0.92
2016-09-06T20:40:46.737394: step 454, loss 0.0648001, acc 0.98
2016-09-06T20:40:47.436454: step 455, loss 0.234649, acc 0.88
2016-09-06T20:40:48.113554: step 456, loss 0.225548, acc 0.84
2016-09-06T20:40:48.788713: step 457, loss 0.130249, acc 0.96
2016-09-06T20:40:49.458037: step 458, loss 0.454629, acc 0.8
2016-09-06T20:40:50.148683: step 459, loss 0.211822, acc 0.9
2016-09-06T20:40:50.835459: step 460, loss 0.137151, acc 0.94
2016-09-06T20:40:51.512292: step 461, loss 0.317268, acc 0.86
2016-09-06T20:40:52.227293: step 462, loss 0.166819, acc 0.92
2016-09-06T20:40:52.922546: step 463, loss 0.326826, acc 0.86
2016-09-06T20:40:53.598679: step 464, loss 0.211822, acc 0.92
2016-09-06T20:40:54.286549: step 465, loss 0.242408, acc 0.94
2016-09-06T20:40:54.974020: step 466, loss 0.19642, acc 0.92
2016-09-06T20:40:55.684642: step 467, loss 0.199383, acc 0.94
2016-09-06T20:40:56.341373: step 468, loss 0.33459, acc 0.84
2016-09-06T20:40:57.050991: step 469, loss 0.280883, acc 0.86
2016-09-06T20:40:57.745504: step 470, loss 0.259248, acc 0.92
2016-09-06T20:40:58.408109: step 471, loss 0.328381, acc 0.82
2016-09-06T20:40:59.105319: step 472, loss 0.168345, acc 0.96
2016-09-06T20:40:59.796145: step 473, loss 0.237464, acc 0.9
2016-09-06T20:41:00.518609: step 474, loss 0.175381, acc 0.96
2016-09-06T20:41:01.195505: step 475, loss 0.0912481, acc 0.96
2016-09-06T20:41:01.862962: step 476, loss 0.304288, acc 0.86
2016-09-06T20:41:02.542024: step 477, loss 0.401176, acc 0.82
2016-09-06T20:41:03.234075: step 478, loss 0.302585, acc 0.86
2016-09-06T20:41:03.898349: step 479, loss 0.264853, acc 0.86
2016-09-06T20:41:04.560092: step 480, loss 0.184496, acc 0.92
2016-09-06T20:41:05.263355: step 481, loss 0.271212, acc 0.88
2016-09-06T20:41:05.936547: step 482, loss 0.166119, acc 0.94
2016-09-06T20:41:06.652071: step 483, loss 0.420151, acc 0.8
2016-09-06T20:41:07.353529: step 484, loss 0.256551, acc 0.88
2016-09-06T20:41:08.025482: step 485, loss 0.224671, acc 0.88
2016-09-06T20:41:08.706714: step 486, loss 0.295112, acc 0.86
2016-09-06T20:41:09.395599: step 487, loss 0.353176, acc 0.86
2016-09-06T20:41:10.112405: step 488, loss 0.324642, acc 0.82
2016-09-06T20:41:10.804215: step 489, loss 0.363168, acc 0.8
2016-09-06T20:41:11.480411: step 490, loss 0.339813, acc 0.82
2016-09-06T20:41:12.166732: step 491, loss 0.225098, acc 0.9
2016-09-06T20:41:12.870790: step 492, loss 0.244306, acc 0.86
2016-09-06T20:41:13.561651: step 493, loss 0.225478, acc 0.9
2016-09-06T20:41:14.235789: step 494, loss 0.325065, acc 0.84
2016-09-06T20:41:14.927719: step 495, loss 0.246782, acc 0.92
2016-09-06T20:41:15.577214: step 496, loss 0.109157, acc 0.96
2016-09-06T20:41:16.291390: step 497, loss 0.242365, acc 0.9
2016-09-06T20:41:16.985380: step 498, loss 0.22434, acc 0.94
2016-09-06T20:41:17.700756: step 499, loss 0.286703, acc 0.9
2016-09-06T20:41:18.404136: step 500, loss 0.224091, acc 0.92

Evaluation:
2016-09-06T20:41:21.548263: step 500, loss 0.550761, acc 0.785178

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-500

2016-09-06T20:41:23.375822: step 501, loss 0.241373, acc 0.86
2016-09-06T20:41:24.061694: step 502, loss 0.230631, acc 0.86
2016-09-06T20:41:24.739267: step 503, loss 0.148559, acc 0.94
2016-09-06T20:41:25.423039: step 504, loss 0.203448, acc 0.9
2016-09-06T20:41:26.119909: step 505, loss 0.261896, acc 0.9
2016-09-06T20:41:26.821223: step 506, loss 0.264597, acc 0.9
2016-09-06T20:41:27.493968: step 507, loss 0.215423, acc 0.9
2016-09-06T20:41:28.194617: step 508, loss 0.314266, acc 0.86
2016-09-06T20:41:28.869967: step 509, loss 0.213074, acc 0.92
2016-09-06T20:41:29.547215: step 510, loss 0.249793, acc 0.88
2016-09-06T20:41:30.211063: step 511, loss 0.198597, acc 0.92
2016-09-06T20:41:30.913879: step 512, loss 0.325423, acc 0.82
2016-09-06T20:41:31.596009: step 513, loss 0.194095, acc 0.92
2016-09-06T20:41:32.267975: step 514, loss 0.221661, acc 0.9
2016-09-06T20:41:32.977299: step 515, loss 0.244151, acc 0.9
2016-09-06T20:41:33.626609: step 516, loss 0.225544, acc 0.9
2016-09-06T20:41:34.320697: step 517, loss 0.378274, acc 0.82
2016-09-06T20:41:35.007280: step 518, loss 0.160331, acc 0.94
2016-09-06T20:41:35.698155: step 519, loss 0.18209, acc 0.9
2016-09-06T20:41:36.387572: step 520, loss 0.123669, acc 0.92
2016-09-06T20:41:37.073971: step 521, loss 0.192486, acc 0.9
2016-09-06T20:41:37.767408: step 522, loss 0.210315, acc 0.9
2016-09-06T20:41:38.428821: step 523, loss 0.242357, acc 0.88
2016-09-06T20:41:39.120531: step 524, loss 0.341549, acc 0.9
2016-09-06T20:41:39.812838: step 525, loss 0.286694, acc 0.88
2016-09-06T20:41:40.495537: step 526, loss 0.132082, acc 0.96
2016-09-06T20:41:41.171133: step 527, loss 0.371211, acc 0.82
2016-09-06T20:41:41.875609: step 528, loss 0.147764, acc 0.94
2016-09-06T20:41:42.568750: step 529, loss 0.371611, acc 0.8
2016-09-06T20:41:43.237758: step 530, loss 0.383097, acc 0.92
2016-09-06T20:41:43.937671: step 531, loss 0.318469, acc 0.86
2016-09-06T20:41:44.615223: step 532, loss 0.256373, acc 0.88
2016-09-06T20:41:45.293547: step 533, loss 0.278838, acc 0.84
2016-09-06T20:41:45.971349: step 534, loss 0.291978, acc 0.92
2016-09-06T20:41:46.654070: step 535, loss 0.338258, acc 0.9
2016-09-06T20:41:47.350438: step 536, loss 0.366945, acc 0.84
2016-09-06T20:41:48.022023: step 537, loss 0.249339, acc 0.9
2016-09-06T20:41:48.717498: step 538, loss 0.421215, acc 0.8
2016-09-06T20:41:49.388239: step 539, loss 0.294494, acc 0.9
2016-09-06T20:41:50.083454: step 540, loss 0.183511, acc 0.94
2016-09-06T20:41:50.756950: step 541, loss 0.288264, acc 0.86
2016-09-06T20:41:51.430564: step 542, loss 0.177461, acc 0.94
2016-09-06T20:41:52.098858: step 543, loss 0.305845, acc 0.84
2016-09-06T20:41:52.785753: step 544, loss 0.316298, acc 0.86
2016-09-06T20:41:53.477917: step 545, loss 0.194147, acc 0.92
2016-09-06T20:41:54.151115: step 546, loss 0.217976, acc 0.94
2016-09-06T20:41:54.838592: step 547, loss 0.334815, acc 0.88
2016-09-06T20:41:55.512564: step 548, loss 0.2215, acc 0.92
2016-09-06T20:41:56.209055: step 549, loss 0.456357, acc 0.82
2016-09-06T20:41:56.900302: step 550, loss 0.208536, acc 0.88
2016-09-06T20:41:57.564197: step 551, loss 0.320878, acc 0.92
2016-09-06T20:41:58.262210: step 552, loss 0.21893, acc 0.92
2016-09-06T20:41:58.985498: step 553, loss 0.241958, acc 0.88
2016-09-06T20:41:59.666412: step 554, loss 0.193761, acc 0.96
2016-09-06T20:42:00.382953: step 555, loss 0.228029, acc 0.9
2016-09-06T20:42:01.076262: step 556, loss 0.217913, acc 0.9
2016-09-06T20:42:01.765773: step 557, loss 0.25858, acc 0.9
2016-09-06T20:42:02.434124: step 558, loss 0.260136, acc 0.96
2016-09-06T20:42:03.148038: step 559, loss 0.301825, acc 0.84
2016-09-06T20:42:03.845906: step 560, loss 0.184837, acc 0.92
2016-09-06T20:42:04.518738: step 561, loss 0.317993, acc 0.86
2016-09-06T20:42:05.176898: step 562, loss 0.14039, acc 0.96
2016-09-06T20:42:05.876258: step 563, loss 0.198024, acc 0.96
2016-09-06T20:42:06.570009: step 564, loss 0.321294, acc 0.86
2016-09-06T20:42:07.255840: step 565, loss 0.340943, acc 0.86
2016-09-06T20:42:07.942089: step 566, loss 0.187762, acc 0.92
2016-09-06T20:42:08.600424: step 567, loss 0.221475, acc 0.92
2016-09-06T20:42:09.306324: step 568, loss 0.165918, acc 0.92
2016-09-06T20:42:10.027443: step 569, loss 0.416517, acc 0.86
2016-09-06T20:42:10.716061: step 570, loss 0.168821, acc 0.9
2016-09-06T20:42:11.424023: step 571, loss 0.339864, acc 0.86
2016-09-06T20:42:12.093063: step 572, loss 0.304935, acc 0.84
2016-09-06T20:42:12.776947: step 573, loss 0.197098, acc 0.92
2016-09-06T20:42:13.439553: step 574, loss 0.188723, acc 0.92
2016-09-06T20:42:14.135348: step 575, loss 0.381673, acc 0.82
2016-09-06T20:42:14.760698: step 576, loss 0.333403, acc 0.795455
2016-09-06T20:42:15.440464: step 577, loss 0.202019, acc 0.88
2016-09-06T20:42:16.099309: step 578, loss 0.167155, acc 0.96
2016-09-06T20:42:16.783377: step 579, loss 0.189035, acc 0.9
2016-09-06T20:42:17.477127: step 580, loss 0.151435, acc 0.94
2016-09-06T20:42:18.144576: step 581, loss 0.186475, acc 0.88
2016-09-06T20:42:18.815948: step 582, loss 0.217795, acc 0.88
2016-09-06T20:42:19.498326: step 583, loss 0.217514, acc 0.9
2016-09-06T20:42:20.190193: step 584, loss 0.199376, acc 0.9
2016-09-06T20:42:20.861051: step 585, loss 0.226674, acc 0.84
2016-09-06T20:42:21.565892: step 586, loss 0.175752, acc 0.94
2016-09-06T20:42:22.261647: step 587, loss 0.12126, acc 0.92
2016-09-06T20:42:22.942594: step 588, loss 0.186905, acc 0.94
2016-09-06T20:42:23.608301: step 589, loss 0.191972, acc 0.88
2016-09-06T20:42:24.271097: step 590, loss 0.219929, acc 0.9
2016-09-06T20:42:24.970315: step 591, loss 0.0899117, acc 0.96
2016-09-06T20:42:25.664183: step 592, loss 0.0924628, acc 0.96
2016-09-06T20:42:26.356678: step 593, loss 0.176495, acc 0.9
2016-09-06T20:42:27.054059: step 594, loss 0.210478, acc 0.88
2016-09-06T20:42:27.739091: step 595, loss 0.168079, acc 0.9
2016-09-06T20:42:28.436549: step 596, loss 0.113295, acc 0.94
2016-09-06T20:42:29.137790: step 597, loss 0.197102, acc 0.96
2016-09-06T20:42:29.832340: step 598, loss 0.0813189, acc 0.98
2016-09-06T20:42:30.504070: step 599, loss 0.187503, acc 0.92
2016-09-06T20:42:31.182522: step 600, loss 0.0609159, acc 1

Evaluation:
2016-09-06T20:42:34.325415: step 600, loss 0.661664, acc 0.782364

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-600

2016-09-06T20:42:35.982674: step 601, loss 0.192067, acc 0.88
2016-09-06T20:42:36.689781: step 602, loss 0.129657, acc 0.96
2016-09-06T20:42:37.352767: step 603, loss 0.120497, acc 0.94
2016-09-06T20:42:38.045877: step 604, loss 0.173177, acc 0.92
2016-09-06T20:42:38.741230: step 605, loss 0.14109, acc 0.92
2016-09-06T20:42:39.415255: step 606, loss 0.421726, acc 0.84
2016-09-06T20:42:40.093750: step 607, loss 0.211777, acc 0.92
2016-09-06T20:42:40.800126: step 608, loss 0.343967, acc 0.86
2016-09-06T20:42:41.526772: step 609, loss 0.154685, acc 0.94
2016-09-06T20:42:42.191506: step 610, loss 0.256467, acc 0.94
2016-09-06T20:42:42.861118: step 611, loss 0.139966, acc 0.96
2016-09-06T20:42:43.530832: step 612, loss 0.232924, acc 0.92
2016-09-06T20:42:44.216174: step 613, loss 0.0744558, acc 1
2016-09-06T20:42:44.899999: step 614, loss 0.158357, acc 0.96
2016-09-06T20:42:45.577597: step 615, loss 0.091877, acc 0.96
2016-09-06T20:42:46.288695: step 616, loss 0.130265, acc 0.94
2016-09-06T20:42:46.984760: step 617, loss 0.186249, acc 0.92
2016-09-06T20:42:47.663279: step 618, loss 0.0776295, acc 1
2016-09-06T20:42:48.339535: step 619, loss 0.169057, acc 0.9
2016-09-06T20:42:49.006904: step 620, loss 0.147516, acc 0.92
2016-09-06T20:42:49.688424: step 621, loss 0.0787049, acc 1
2016-09-06T20:42:50.367392: step 622, loss 0.0838357, acc 0.98
2016-09-06T20:42:51.057862: step 623, loss 0.270177, acc 0.92
2016-09-06T20:42:51.722613: step 624, loss 0.219104, acc 0.96
2016-09-06T20:42:52.421528: step 625, loss 0.114521, acc 0.94
2016-09-06T20:42:53.106189: step 626, loss 0.0798716, acc 0.98
2016-09-06T20:42:53.781890: step 627, loss 0.221877, acc 0.88
2016-09-06T20:42:54.485252: step 628, loss 0.100648, acc 0.94
2016-09-06T20:42:55.157495: step 629, loss 0.160465, acc 0.9
2016-09-06T20:42:55.868387: step 630, loss 0.210176, acc 0.92
2016-09-06T20:42:56.530078: step 631, loss 0.354754, acc 0.8
2016-09-06T20:42:57.231436: step 632, loss 0.14005, acc 0.92
2016-09-06T20:42:57.914585: step 633, loss 0.213315, acc 0.88
2016-09-06T20:42:58.596618: step 634, loss 0.0776183, acc 1
2016-09-06T20:42:59.263341: step 635, loss 0.211687, acc 0.86
2016-09-06T20:42:59.937555: step 636, loss 0.219798, acc 0.9
2016-09-06T20:43:00.676692: step 637, loss 0.21573, acc 0.92
2016-09-06T20:43:01.349464: step 638, loss 0.142251, acc 0.96
2016-09-06T20:43:02.043801: step 639, loss 0.211356, acc 0.9
2016-09-06T20:43:02.732642: step 640, loss 0.190702, acc 0.88
2016-09-06T20:43:03.430206: step 641, loss 0.129047, acc 0.92
2016-09-06T20:43:04.156938: step 642, loss 0.103803, acc 0.98
2016-09-06T20:43:04.829056: step 643, loss 0.133667, acc 0.92
2016-09-06T20:43:05.530312: step 644, loss 0.267502, acc 0.88
2016-09-06T20:43:06.195614: step 645, loss 0.12905, acc 0.98
2016-09-06T20:43:06.903517: step 646, loss 0.194986, acc 0.94
2016-09-06T20:43:07.602344: step 647, loss 0.224229, acc 0.9
2016-09-06T20:43:08.271505: step 648, loss 0.289835, acc 0.86
2016-09-06T20:43:08.940318: step 649, loss 0.191402, acc 0.92
2016-09-06T20:43:09.614417: step 650, loss 0.168972, acc 0.92
2016-09-06T20:43:10.315099: step 651, loss 0.162134, acc 0.92
2016-09-06T20:43:10.984663: step 652, loss 0.0874786, acc 0.98
2016-09-06T20:43:11.678751: step 653, loss 0.208611, acc 0.92
2016-09-06T20:43:12.364313: step 654, loss 0.302584, acc 0.88
2016-09-06T20:43:13.061589: step 655, loss 0.215762, acc 0.92
2016-09-06T20:43:13.756091: step 656, loss 0.167186, acc 0.94
2016-09-06T20:43:14.449521: step 657, loss 0.134363, acc 0.94
2016-09-06T20:43:15.156947: step 658, loss 0.253884, acc 0.9
2016-09-06T20:43:15.845262: step 659, loss 0.172416, acc 0.94
2016-09-06T20:43:16.518306: step 660, loss 0.14617, acc 0.92
2016-09-06T20:43:17.217830: step 661, loss 0.143923, acc 0.96
2016-09-06T20:43:17.915485: step 662, loss 0.20151, acc 0.94
2016-09-06T20:43:18.614410: step 663, loss 0.334489, acc 0.86
2016-09-06T20:43:19.280783: step 664, loss 0.161452, acc 0.88
2016-09-06T20:43:19.991826: step 665, loss 0.144097, acc 0.94
2016-09-06T20:43:20.672818: step 666, loss 0.184774, acc 0.92
2016-09-06T20:43:21.387328: step 667, loss 0.115773, acc 0.98
2016-09-06T20:43:22.073693: step 668, loss 0.277134, acc 0.9
2016-09-06T20:43:22.748607: step 669, loss 0.10891, acc 0.94
2016-09-06T20:43:23.419682: step 670, loss 0.146161, acc 0.96
2016-09-06T20:43:24.113395: step 671, loss 0.113024, acc 0.94
2016-09-06T20:43:24.809757: step 672, loss 0.166833, acc 0.92
2016-09-06T20:43:25.481633: step 673, loss 0.178652, acc 0.92
2016-09-06T20:43:26.176264: step 674, loss 0.0738741, acc 1
2016-09-06T20:43:26.871627: step 675, loss 0.168576, acc 0.92
2016-09-06T20:43:27.552623: step 676, loss 0.114442, acc 0.94
2016-09-06T20:43:28.248269: step 677, loss 0.131429, acc 0.92
2016-09-06T20:43:28.930659: step 678, loss 0.0871416, acc 0.98
2016-09-06T20:43:29.634300: step 679, loss 0.199853, acc 0.96
2016-09-06T20:43:30.306433: step 680, loss 0.261448, acc 0.92
2016-09-06T20:43:30.977432: step 681, loss 0.210508, acc 0.88
2016-09-06T20:43:31.659398: step 682, loss 0.270857, acc 0.88
2016-09-06T20:43:32.340576: step 683, loss 0.128544, acc 0.94
2016-09-06T20:43:33.033735: step 684, loss 0.293391, acc 0.92
2016-09-06T20:43:33.718730: step 685, loss 0.208465, acc 0.92
2016-09-06T20:43:34.424750: step 686, loss 0.156262, acc 0.94
2016-09-06T20:43:35.082696: step 687, loss 0.485155, acc 0.86
2016-09-06T20:43:35.751953: step 688, loss 0.130487, acc 0.94
2016-09-06T20:43:36.408385: step 689, loss 0.166188, acc 0.92
2016-09-06T20:43:37.084085: step 690, loss 0.131287, acc 0.96
2016-09-06T20:43:37.751665: step 691, loss 0.140228, acc 0.96
2016-09-06T20:43:38.420365: step 692, loss 0.106662, acc 0.96
2016-09-06T20:43:39.119856: step 693, loss 0.177321, acc 0.94
2016-09-06T20:43:39.783548: step 694, loss 0.189682, acc 0.96
2016-09-06T20:43:40.493713: step 695, loss 0.2294, acc 0.88
2016-09-06T20:43:41.179952: step 696, loss 0.145398, acc 0.94
2016-09-06T20:43:41.853923: step 697, loss 0.211114, acc 0.94
2016-09-06T20:43:42.534626: step 698, loss 0.278787, acc 0.86
2016-09-06T20:43:43.220926: step 699, loss 0.248011, acc 0.88
2016-09-06T20:43:43.913332: step 700, loss 0.194006, acc 0.92

Evaluation:
2016-09-06T20:43:47.057539: step 700, loss 0.581902, acc 0.792683

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-700

2016-09-06T20:43:48.706656: step 701, loss 0.0939687, acc 0.98
2016-09-06T20:43:49.375338: step 702, loss 0.16738, acc 0.92
2016-09-06T20:43:50.072232: step 703, loss 0.203037, acc 0.88
2016-09-06T20:43:50.741926: step 704, loss 0.153296, acc 0.92
2016-09-06T20:43:51.449288: step 705, loss 0.162708, acc 0.92
2016-09-06T20:43:52.156049: step 706, loss 0.153383, acc 0.96
2016-09-06T20:43:52.836541: step 707, loss 0.227304, acc 0.9
2016-09-06T20:43:53.543938: step 708, loss 0.15226, acc 0.92
2016-09-06T20:43:54.207663: step 709, loss 0.15497, acc 0.96
2016-09-06T20:43:54.902552: step 710, loss 0.131158, acc 0.92
2016-09-06T20:43:55.591757: step 711, loss 0.111984, acc 0.94
2016-09-06T20:43:56.268818: step 712, loss 0.170232, acc 0.92
2016-09-06T20:43:56.941805: step 713, loss 0.0721186, acc 1
2016-09-06T20:43:57.630294: step 714, loss 0.122906, acc 0.94
2016-09-06T20:43:58.299392: step 715, loss 0.226663, acc 0.86
2016-09-06T20:43:58.971803: step 716, loss 0.0791932, acc 0.98
2016-09-06T20:43:59.684693: step 717, loss 0.352989, acc 0.92
2016-09-06T20:44:00.416250: step 718, loss 0.120328, acc 0.96
2016-09-06T20:44:01.111062: step 719, loss 0.0626921, acc 0.98
2016-09-06T20:44:01.797542: step 720, loss 0.0887948, acc 0.98
2016-09-06T20:44:02.495348: step 721, loss 0.357789, acc 0.84
2016-09-06T20:44:03.185817: step 722, loss 0.208526, acc 0.94
2016-09-06T20:44:03.854406: step 723, loss 0.254587, acc 0.88
2016-09-06T20:44:04.556931: step 724, loss 0.134933, acc 0.98
2016-09-06T20:44:05.224061: step 725, loss 0.271649, acc 0.94
2016-09-06T20:44:05.911623: step 726, loss 0.0988938, acc 0.94
2016-09-06T20:44:06.593454: step 727, loss 0.130523, acc 0.96
2016-09-06T20:44:07.270913: step 728, loss 0.270343, acc 0.88
2016-09-06T20:44:07.943576: step 729, loss 0.103516, acc 0.94
2016-09-06T20:44:08.610679: step 730, loss 0.0796606, acc 0.98
2016-09-06T20:44:09.300668: step 731, loss 0.230492, acc 0.88
2016-09-06T20:44:09.984035: step 732, loss 0.0893701, acc 1
2016-09-06T20:44:10.667763: step 733, loss 0.116938, acc 0.96
2016-09-06T20:44:11.343346: step 734, loss 0.432902, acc 0.84
2016-09-06T20:44:12.023595: step 735, loss 0.117101, acc 0.96
2016-09-06T20:44:12.703600: step 736, loss 0.349521, acc 0.88
2016-09-06T20:44:13.382579: step 737, loss 0.153488, acc 0.94
2016-09-06T20:44:14.072193: step 738, loss 0.295285, acc 0.88
2016-09-06T20:44:14.738186: step 739, loss 0.12746, acc 0.98
2016-09-06T20:44:15.430121: step 740, loss 0.202394, acc 0.94
2016-09-06T20:44:16.150291: step 741, loss 0.134965, acc 0.96
2016-09-06T20:44:16.855140: step 742, loss 0.123639, acc 0.94
2016-09-06T20:44:17.562395: step 743, loss 0.152621, acc 0.94
2016-09-06T20:44:18.228847: step 744, loss 0.133399, acc 0.96
2016-09-06T20:44:18.921156: step 745, loss 0.253545, acc 0.88
2016-09-06T20:44:19.588896: step 746, loss 0.310429, acc 0.88
2016-09-06T20:44:20.269274: step 747, loss 0.107607, acc 0.96
2016-09-06T20:44:20.954578: step 748, loss 0.224013, acc 0.88
2016-09-06T20:44:21.633841: step 749, loss 0.246175, acc 0.9
2016-09-06T20:44:22.318730: step 750, loss 0.302626, acc 0.88
2016-09-06T20:44:22.988601: step 751, loss 0.156186, acc 0.94
2016-09-06T20:44:23.699453: step 752, loss 0.113639, acc 0.94
2016-09-06T20:44:24.386304: step 753, loss 0.134573, acc 0.96
2016-09-06T20:44:25.073290: step 754, loss 0.200612, acc 0.96
2016-09-06T20:44:25.763496: step 755, loss 0.124791, acc 0.96
2016-09-06T20:44:26.438843: step 756, loss 0.11537, acc 0.94
2016-09-06T20:44:27.116730: step 757, loss 0.104579, acc 0.98
2016-09-06T20:44:27.797996: step 758, loss 0.331113, acc 0.86
2016-09-06T20:44:28.497815: step 759, loss 0.161054, acc 0.94
2016-09-06T20:44:29.153047: step 760, loss 0.224326, acc 0.88
2016-09-06T20:44:29.852031: step 761, loss 0.203822, acc 0.9
2016-09-06T20:44:30.547489: step 762, loss 0.264502, acc 0.86
2016-09-06T20:44:31.221758: step 763, loss 0.195855, acc 0.9
2016-09-06T20:44:31.896872: step 764, loss 0.227763, acc 0.88
2016-09-06T20:44:32.594140: step 765, loss 0.135344, acc 0.92
2016-09-06T20:44:33.301507: step 766, loss 0.276173, acc 0.9
2016-09-06T20:44:33.964860: step 767, loss 0.182432, acc 0.94
2016-09-06T20:44:34.601553: step 768, loss 0.152066, acc 0.954545
2016-09-06T20:44:35.297832: step 769, loss 0.124013, acc 0.96
2016-09-06T20:44:35.985903: step 770, loss 0.191987, acc 0.9
2016-09-06T20:44:36.675915: step 771, loss 0.186337, acc 0.9
2016-09-06T20:44:37.389899: step 772, loss 0.140441, acc 0.92
2016-09-06T20:44:38.076098: step 773, loss 0.127005, acc 0.96
2016-09-06T20:44:38.729352: step 774, loss 0.110771, acc 0.96
2016-09-06T20:44:39.422394: step 775, loss 0.0864776, acc 0.98
2016-09-06T20:44:40.080891: step 776, loss 0.164881, acc 0.94
2016-09-06T20:44:40.747155: step 777, loss 0.101359, acc 0.98
2016-09-06T20:44:41.447599: step 778, loss 0.0816256, acc 0.98
2016-09-06T20:44:42.157672: step 779, loss 0.0849896, acc 0.98
2016-09-06T20:44:42.836723: step 780, loss 0.0603202, acc 0.98
2016-09-06T20:44:43.522374: step 781, loss 0.080372, acc 0.98
2016-09-06T20:44:44.221783: step 782, loss 0.0478319, acc 1
2016-09-06T20:44:44.914612: step 783, loss 0.0805839, acc 0.96
2016-09-06T20:44:45.594311: step 784, loss 0.133938, acc 0.96
2016-09-06T20:44:46.282818: step 785, loss 0.224854, acc 0.9
2016-09-06T20:44:46.952369: step 786, loss 0.0493014, acc 0.98
2016-09-06T20:44:47.642962: step 787, loss 0.0477468, acc 0.98
2016-09-06T20:44:48.297354: step 788, loss 0.0990954, acc 0.94
2016-09-06T20:44:48.968122: step 789, loss 0.229072, acc 0.88
2016-09-06T20:44:49.643155: step 790, loss 0.102555, acc 0.94
2016-09-06T20:44:50.311901: step 791, loss 0.0315991, acc 1
2016-09-06T20:44:50.981362: step 792, loss 0.0539924, acc 0.98
2016-09-06T20:44:51.662720: step 793, loss 0.317829, acc 0.88
2016-09-06T20:44:52.358131: step 794, loss 0.183298, acc 0.9
2016-09-06T20:44:53.038488: step 795, loss 0.203217, acc 0.94
2016-09-06T20:44:53.742002: step 796, loss 0.130739, acc 0.92
2016-09-06T20:44:54.422156: step 797, loss 0.124941, acc 0.94
2016-09-06T20:44:55.119837: step 798, loss 0.0962099, acc 0.94
2016-09-06T20:44:55.815834: step 799, loss 0.0638778, acc 0.98
2016-09-06T20:44:56.493510: step 800, loss 0.121388, acc 0.92

Evaluation:
2016-09-06T20:44:59.672619: step 800, loss 0.687231, acc 0.794559

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-800

2016-09-06T20:45:01.424273: step 801, loss 0.0511293, acc 0.98
2016-09-06T20:45:02.096705: step 802, loss 0.120677, acc 0.94
2016-09-06T20:45:02.754023: step 803, loss 0.191395, acc 0.92
2016-09-06T20:45:03.431868: step 804, loss 0.261975, acc 0.92
2016-09-06T20:45:04.107911: step 805, loss 0.193937, acc 0.96
2016-09-06T20:45:04.784545: step 806, loss 0.133248, acc 0.96
2016-09-06T20:45:05.456260: step 807, loss 0.139551, acc 0.9
2016-09-06T20:45:06.146383: step 808, loss 0.0599261, acc 0.98
2016-09-06T20:45:06.833665: step 809, loss 0.282021, acc 0.84
2016-09-06T20:45:07.524982: step 810, loss 0.106361, acc 0.98
2016-09-06T20:45:08.242652: step 811, loss 0.0853091, acc 0.98
2016-09-06T20:45:08.958565: step 812, loss 0.134223, acc 0.94
2016-09-06T20:45:09.663456: step 813, loss 0.0929885, acc 0.98
2016-09-06T20:45:10.361249: step 814, loss 0.132121, acc 0.9
2016-09-06T20:45:11.047689: step 815, loss 0.0223471, acc 1
2016-09-06T20:45:11.737739: step 816, loss 0.100171, acc 0.96
2016-09-06T20:45:12.398223: step 817, loss 0.0670191, acc 0.96
2016-09-06T20:45:13.095724: step 818, loss 0.0714737, acc 0.96
2016-09-06T20:45:13.780700: step 819, loss 0.0965327, acc 0.98
2016-09-06T20:45:14.460256: step 820, loss 0.0871744, acc 0.98
2016-09-06T20:45:15.142355: step 821, loss 0.103261, acc 0.94
2016-09-06T20:45:15.830087: step 822, loss 0.118602, acc 0.94
2016-09-06T20:45:16.524024: step 823, loss 0.0704824, acc 0.94
2016-09-06T20:45:17.192819: step 824, loss 0.0612848, acc 0.96
2016-09-06T20:45:17.891705: step 825, loss 0.132643, acc 0.96
2016-09-06T20:45:18.564946: step 826, loss 0.137413, acc 0.92
2016-09-06T20:45:19.233298: step 827, loss 0.0316028, acc 0.98
2016-09-06T20:45:19.917122: step 828, loss 0.0792397, acc 0.96
2016-09-06T20:45:20.602107: step 829, loss 0.0400592, acc 0.98
2016-09-06T20:45:21.289482: step 830, loss 0.187526, acc 0.9
2016-09-06T20:45:21.951905: step 831, loss 0.0916404, acc 0.96
2016-09-06T20:45:22.647029: step 832, loss 0.172712, acc 0.9
2016-09-06T20:45:23.334912: step 833, loss 0.178489, acc 0.94
2016-09-06T20:45:24.016061: step 834, loss 0.0655482, acc 0.98
2016-09-06T20:45:24.730731: step 835, loss 0.0330942, acc 1
2016-09-06T20:45:25.416250: step 836, loss 0.205866, acc 0.9
2016-09-06T20:45:26.088677: step 837, loss 0.123577, acc 0.94
2016-09-06T20:45:26.763892: step 838, loss 0.0395116, acc 1
2016-09-06T20:45:27.465770: step 839, loss 0.07414, acc 0.96
2016-09-06T20:45:28.128981: step 840, loss 0.0808843, acc 0.98
2016-09-06T20:45:28.818141: step 841, loss 0.106438, acc 0.96
2016-09-06T20:45:29.493980: step 842, loss 0.116135, acc 0.94
2016-09-06T20:45:30.185620: step 843, loss 0.0870088, acc 0.98
2016-09-06T20:45:30.855966: step 844, loss 0.0638938, acc 0.96
2016-09-06T20:45:31.534728: step 845, loss 0.0669712, acc 0.96
2016-09-06T20:45:32.242368: step 846, loss 0.183304, acc 0.94
2016-09-06T20:45:32.899872: step 847, loss 0.0365318, acc 1
2016-09-06T20:45:33.608407: step 848, loss 0.07807, acc 0.98
2016-09-06T20:45:34.292598: step 849, loss 0.0888104, acc 0.96
2016-09-06T20:45:34.978030: step 850, loss 0.138663, acc 0.98
2016-09-06T20:45:35.657614: step 851, loss 0.0519594, acc 1
2016-09-06T20:45:36.335709: step 852, loss 0.155609, acc 0.9
2016-09-06T20:45:37.036928: step 853, loss 0.126595, acc 0.96
2016-09-06T20:45:37.724867: step 854, loss 0.107889, acc 0.94
2016-09-06T20:45:38.433582: step 855, loss 0.0543655, acc 0.98
2016-09-06T20:45:39.112382: step 856, loss 0.140387, acc 0.94
2016-09-06T20:45:39.785119: step 857, loss 0.158993, acc 0.94
2016-09-06T20:45:40.489206: step 858, loss 0.13213, acc 0.98
2016-09-06T20:45:41.165336: step 859, loss 0.233275, acc 0.9
2016-09-06T20:45:41.869850: step 860, loss 0.300413, acc 0.88
2016-09-06T20:45:42.536706: step 861, loss 0.231712, acc 0.88
2016-09-06T20:45:43.250024: step 862, loss 0.181096, acc 0.92
2016-09-06T20:45:43.931075: step 863, loss 0.346446, acc 0.9
2016-09-06T20:45:44.613245: step 864, loss 0.222432, acc 0.86
2016-09-06T20:45:45.281776: step 865, loss 0.0815557, acc 0.96
2016-09-06T20:45:45.960713: step 866, loss 0.159079, acc 0.94
2016-09-06T20:45:46.660201: step 867, loss 0.206906, acc 0.9
2016-09-06T20:45:47.328999: step 868, loss 0.134731, acc 0.94
2016-09-06T20:45:48.035995: step 869, loss 0.07753, acc 1
2016-09-06T20:45:48.720779: step 870, loss 0.0672906, acc 1
2016-09-06T20:45:49.393904: step 871, loss 0.128735, acc 0.94
2016-09-06T20:45:50.079194: step 872, loss 0.169628, acc 0.94
2016-09-06T20:45:50.773675: step 873, loss 0.107854, acc 0.96
2016-09-06T20:45:51.444952: step 874, loss 0.0466236, acc 1
2016-09-06T20:45:52.108819: step 875, loss 0.206936, acc 0.94
2016-09-06T20:45:52.809348: step 876, loss 0.141107, acc 0.9
2016-09-06T20:45:53.479324: step 877, loss 0.0476256, acc 1
2016-09-06T20:45:54.161757: step 878, loss 0.116726, acc 0.96
2016-09-06T20:45:54.842839: step 879, loss 0.101445, acc 0.92
2016-09-06T20:45:55.519204: step 880, loss 0.0850634, acc 0.94
2016-09-06T20:45:56.215779: step 881, loss 0.0481282, acc 0.98
2016-09-06T20:45:56.876837: step 882, loss 0.143851, acc 0.94
2016-09-06T20:45:57.576126: step 883, loss 0.0457787, acc 1
2016-09-06T20:45:58.245135: step 884, loss 0.122879, acc 0.96
2016-09-06T20:45:58.909788: step 885, loss 0.268792, acc 0.9
2016-09-06T20:45:59.588273: step 886, loss 0.0308205, acc 1
2016-09-06T20:46:00.295057: step 887, loss 0.125375, acc 0.94
2016-09-06T20:46:00.969160: step 888, loss 0.0462557, acc 0.98
2016-09-06T20:46:01.643982: step 889, loss 0.0461852, acc 0.98
2016-09-06T20:46:02.347898: step 890, loss 0.125232, acc 0.96
2016-09-06T20:46:03.023652: step 891, loss 0.193604, acc 0.94
2016-09-06T20:46:03.729594: step 892, loss 0.121888, acc 0.94
2016-09-06T20:46:04.400631: step 893, loss 0.172106, acc 0.94
2016-09-06T20:46:05.094312: step 894, loss 0.139748, acc 0.92
2016-09-06T20:46:05.773899: step 895, loss 0.028192, acc 1
2016-09-06T20:46:06.470965: step 896, loss 0.126324, acc 0.96
2016-09-06T20:46:07.166327: step 897, loss 0.191487, acc 0.9
2016-09-06T20:46:07.840673: step 898, loss 0.278081, acc 0.9
2016-09-06T20:46:08.521889: step 899, loss 0.239227, acc 0.88
2016-09-06T20:46:09.204633: step 900, loss 0.226632, acc 0.88

Evaluation:
2016-09-06T20:46:12.357539: step 900, loss 0.781426, acc 0.790807

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-900

2016-09-06T20:46:14.012789: step 901, loss 0.123566, acc 0.92
2016-09-06T20:46:14.687466: step 902, loss 0.116779, acc 0.92
2016-09-06T20:46:15.371612: step 903, loss 0.0790436, acc 0.98
2016-09-06T20:46:16.063981: step 904, loss 0.0972148, acc 0.98
2016-09-06T20:46:16.797989: step 905, loss 0.0902777, acc 0.96
2016-09-06T20:46:17.483782: step 906, loss 0.109162, acc 0.96
2016-09-06T20:46:18.167693: step 907, loss 0.15425, acc 0.94
2016-09-06T20:46:18.854593: step 908, loss 0.167376, acc 0.94
2016-09-06T20:46:19.542291: step 909, loss 0.0887512, acc 0.96
2016-09-06T20:46:20.208951: step 910, loss 0.123858, acc 0.98
2016-09-06T20:46:20.887825: step 911, loss 0.234101, acc 0.9
2016-09-06T20:46:21.587202: step 912, loss 0.0702509, acc 0.96
2016-09-06T20:46:22.248281: step 913, loss 0.134174, acc 0.94
2016-09-06T20:46:22.920309: step 914, loss 0.114287, acc 0.94
2016-09-06T20:46:23.600922: step 915, loss 0.103111, acc 0.94
2016-09-06T20:46:24.302769: step 916, loss 0.0278242, acc 1
2016-09-06T20:46:24.983255: step 917, loss 0.323778, acc 0.88
2016-09-06T20:46:25.671313: step 918, loss 0.0621368, acc 1
2016-09-06T20:46:26.383995: step 919, loss 0.150997, acc 0.9
2016-09-06T20:46:27.043563: step 920, loss 0.193459, acc 0.9
2016-09-06T20:46:27.726260: step 921, loss 0.130495, acc 0.96
2016-09-06T20:46:28.403927: step 922, loss 0.126766, acc 0.92
2016-09-06T20:46:29.071999: step 923, loss 0.174133, acc 0.9
2016-09-06T20:46:29.770621: step 924, loss 0.0909038, acc 0.98
2016-09-06T20:46:30.449026: step 925, loss 0.0641769, acc 0.96
2016-09-06T20:46:31.139035: step 926, loss 0.162623, acc 0.9
2016-09-06T20:46:31.796810: step 927, loss 0.0681946, acc 0.96
2016-09-06T20:46:32.489113: step 928, loss 0.116637, acc 0.92
2016-09-06T20:46:33.173825: step 929, loss 0.0157889, acc 1
2016-09-06T20:46:33.853606: step 930, loss 0.0907515, acc 0.96
2016-09-06T20:46:34.521878: step 931, loss 0.100519, acc 0.94
2016-09-06T20:46:35.205834: step 932, loss 0.137535, acc 0.92
2016-09-06T20:46:35.886067: step 933, loss 0.103261, acc 0.94
2016-09-06T20:46:36.543696: step 934, loss 0.144212, acc 0.94
2016-09-06T20:46:37.241541: step 935, loss 0.148987, acc 0.92
2016-09-06T20:46:37.903897: step 936, loss 0.183858, acc 0.92
2016-09-06T20:46:38.584262: step 937, loss 0.140597, acc 0.94
2016-09-06T20:46:39.275933: step 938, loss 0.07079, acc 0.98
2016-09-06T20:46:39.962725: step 939, loss 0.115238, acc 0.96
2016-09-06T20:46:40.673385: step 940, loss 0.0976419, acc 0.94
2016-09-06T20:46:41.356925: step 941, loss 0.232439, acc 0.88
2016-09-06T20:46:42.065234: step 942, loss 0.177906, acc 0.92
2016-09-06T20:46:42.755915: step 943, loss 0.0526441, acc 0.98
2016-09-06T20:46:43.434798: step 944, loss 0.0875715, acc 0.94
2016-09-06T20:46:44.132585: step 945, loss 0.187612, acc 0.94
2016-09-06T20:46:44.832174: step 946, loss 0.164043, acc 0.94
2016-09-06T20:46:45.528417: step 947, loss 0.204048, acc 0.9
2016-09-06T20:46:46.201839: step 948, loss 0.0481045, acc 0.96
2016-09-06T20:46:46.870037: step 949, loss 0.0560389, acc 0.98
2016-09-06T20:46:47.545650: step 950, loss 0.0551544, acc 0.98
2016-09-06T20:46:48.235746: step 951, loss 0.262694, acc 0.86
2016-09-06T20:46:48.924867: step 952, loss 0.221388, acc 0.88
2016-09-06T20:46:49.639828: step 953, loss 0.137416, acc 0.94
2016-09-06T20:46:50.334643: step 954, loss 0.125542, acc 0.92
2016-09-06T20:46:51.002293: step 955, loss 0.0893981, acc 0.96
2016-09-06T20:46:51.705482: step 956, loss 0.174223, acc 0.94
2016-09-06T20:46:52.387625: step 957, loss 0.188476, acc 0.9
2016-09-06T20:46:53.088131: step 958, loss 0.122192, acc 0.94
2016-09-06T20:46:53.778331: step 959, loss 0.134454, acc 0.92
2016-09-06T20:46:54.428887: step 960, loss 0.127643, acc 0.954545
2016-09-06T20:46:55.121964: step 961, loss 0.0590808, acc 0.98
2016-09-06T20:46:55.795007: step 962, loss 0.120316, acc 0.96
2016-09-06T20:46:56.505875: step 963, loss 0.0916565, acc 0.94
2016-09-06T20:46:57.230411: step 964, loss 0.0808104, acc 0.98
2016-09-06T20:46:57.926105: step 965, loss 0.174219, acc 0.92
2016-09-06T20:46:58.612648: step 966, loss 0.148475, acc 0.92
2016-09-06T20:46:59.294328: step 967, loss 0.037254, acc 1
2016-09-06T20:47:00.020378: step 968, loss 0.153998, acc 0.94
2016-09-06T20:47:00.743235: step 969, loss 0.0950711, acc 0.94
2016-09-06T20:47:01.441478: step 970, loss 0.171102, acc 0.9
2016-09-06T20:47:02.127382: step 971, loss 0.0500753, acc 0.98
2016-09-06T20:47:02.813734: step 972, loss 0.0611344, acc 0.98
2016-09-06T20:47:03.484391: step 973, loss 0.161014, acc 0.94
2016-09-06T20:47:04.172083: step 974, loss 0.130077, acc 0.94
2016-09-06T20:47:04.885684: step 975, loss 0.068128, acc 0.94
2016-09-06T20:47:05.543320: step 976, loss 0.0433383, acc 1
2016-09-06T20:47:06.215902: step 977, loss 0.102116, acc 0.92
2016-09-06T20:47:06.901687: step 978, loss 0.127674, acc 0.94
2016-09-06T20:47:07.584059: step 979, loss 0.0231526, acc 0.98
2016-09-06T20:47:08.262384: step 980, loss 0.0818041, acc 0.94
2016-09-06T20:47:08.945477: step 981, loss 0.0371425, acc 1
2016-09-06T20:47:09.633815: step 982, loss 0.119908, acc 0.94
2016-09-06T20:47:10.300690: step 983, loss 0.0659653, acc 0.96
2016-09-06T20:47:10.996270: step 984, loss 0.0329906, acc 0.98
2016-09-06T20:47:11.687366: step 985, loss 0.192135, acc 0.92
2016-09-06T20:47:12.377282: step 986, loss 0.0535804, acc 0.98
2016-09-06T20:47:13.062197: step 987, loss 0.0463707, acc 0.98
2016-09-06T20:47:13.760564: step 988, loss 0.0516752, acc 0.98
2016-09-06T20:47:14.471426: step 989, loss 0.0795887, acc 0.96
2016-09-06T20:47:15.149262: step 990, loss 0.07165, acc 0.94
2016-09-06T20:47:15.831472: step 991, loss 0.0366041, acc 0.98
2016-09-06T20:47:16.497754: step 992, loss 0.0786517, acc 0.94
2016-09-06T20:47:17.172721: step 993, loss 0.103662, acc 0.96
2016-09-06T20:47:17.885934: step 994, loss 0.0949558, acc 0.96
2016-09-06T20:47:18.575844: step 995, loss 0.0670098, acc 0.96
2016-09-06T20:47:19.268228: step 996, loss 0.0857717, acc 0.98
2016-09-06T20:47:19.927751: step 997, loss 0.0190816, acc 1
2016-09-06T20:47:20.634007: step 998, loss 0.0813375, acc 0.98
2016-09-06T20:47:21.324408: step 999, loss 0.234342, acc 0.86
2016-09-06T20:47:22.002793: step 1000, loss 0.0454302, acc 0.98

Evaluation:
2016-09-06T20:47:25.149711: step 1000, loss 1.00914, acc 0.789869

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-1000

2016-09-06T20:47:26.832132: step 1001, loss 0.0736175, acc 0.96
2016-09-06T20:47:27.510473: step 1002, loss 0.0358078, acc 1
2016-09-06T20:47:28.193549: step 1003, loss 0.0372879, acc 1
2016-09-06T20:47:28.891641: step 1004, loss 0.130067, acc 0.94
2016-09-06T20:47:29.559162: step 1005, loss 0.0540514, acc 1
2016-09-06T20:47:30.254646: step 1006, loss 0.0442327, acc 0.98
2016-09-06T20:47:30.958025: step 1007, loss 0.123853, acc 0.96
2016-09-06T20:47:31.658193: step 1008, loss 0.0666932, acc 0.98
2016-09-06T20:47:32.343733: step 1009, loss 0.0274461, acc 1
2016-09-06T20:47:33.041196: step 1010, loss 0.0506755, acc 0.98
2016-09-06T20:47:33.738373: step 1011, loss 0.0941283, acc 0.94
2016-09-06T20:47:34.409808: step 1012, loss 0.0382267, acc 0.98
2016-09-06T20:47:35.087845: step 1013, loss 0.0546592, acc 0.96
2016-09-06T20:47:35.792823: step 1014, loss 0.223984, acc 0.94
2016-09-06T20:47:36.466546: step 1015, loss 0.059669, acc 0.98
2016-09-06T20:47:37.162057: step 1016, loss 0.183447, acc 0.94
2016-09-06T20:47:37.852642: step 1017, loss 0.140019, acc 0.92
2016-09-06T20:47:38.567321: step 1018, loss 0.229111, acc 0.94
2016-09-06T20:47:39.239096: step 1019, loss 0.134634, acc 0.94
2016-09-06T20:47:39.911933: step 1020, loss 0.0657967, acc 0.96
2016-09-06T20:47:40.580452: step 1021, loss 0.021579, acc 1
2016-09-06T20:47:41.272716: step 1022, loss 0.136871, acc 0.94
2016-09-06T20:47:41.951321: step 1023, loss 0.0388403, acc 1
2016-09-06T20:47:42.623661: step 1024, loss 0.112553, acc 0.94
2016-09-06T20:47:43.337258: step 1025, loss 0.0650096, acc 0.98
2016-09-06T20:47:44.017084: step 1026, loss 0.122827, acc 0.94
2016-09-06T20:47:44.750409: step 1027, loss 0.20774, acc 0.88
2016-09-06T20:47:45.421767: step 1028, loss 0.129493, acc 0.94
2016-09-06T20:47:46.097998: step 1029, loss 0.052424, acc 0.98
2016-09-06T20:47:46.771355: step 1030, loss 0.144164, acc 0.92
2016-09-06T20:47:47.462165: step 1031, loss 0.180116, acc 0.92
2016-09-06T20:47:48.160641: step 1032, loss 0.0533121, acc 0.96
2016-09-06T20:47:48.825824: step 1033, loss 0.028812, acc 1
2016-09-06T20:47:49.507059: step 1034, loss 0.172706, acc 0.9
2016-09-06T20:47:50.178789: step 1035, loss 0.112683, acc 0.96
2016-09-06T20:47:50.851402: step 1036, loss 0.179415, acc 0.92
2016-09-06T20:47:51.528628: step 1037, loss 0.0952963, acc 0.96
2016-09-06T20:47:52.213967: step 1038, loss 0.187156, acc 0.9
2016-09-06T20:47:52.908980: step 1039, loss 0.0671141, acc 0.96
2016-09-06T20:47:53.564288: step 1040, loss 0.182912, acc 0.92
2016-09-06T20:47:54.247721: step 1041, loss 0.055406, acc 0.98
2016-09-06T20:47:54.900783: step 1042, loss 0.138896, acc 0.92
2016-09-06T20:47:55.592355: step 1043, loss 0.100768, acc 0.94
2016-09-06T20:47:56.268566: step 1044, loss 0.103883, acc 0.94
2016-09-06T20:47:56.944605: step 1045, loss 0.239823, acc 0.9
2016-09-06T20:47:57.635722: step 1046, loss 0.187807, acc 0.94
2016-09-06T20:47:58.297249: step 1047, loss 0.204923, acc 0.88
2016-09-06T20:47:58.990487: step 1048, loss 0.0953079, acc 0.96
2016-09-06T20:47:59.645104: step 1049, loss 0.0717179, acc 0.96
2016-09-06T20:48:00.358965: step 1050, loss 0.140175, acc 0.94
2016-09-06T20:48:01.029083: step 1051, loss 0.0588507, acc 0.96
2016-09-06T20:48:01.721013: step 1052, loss 0.142914, acc 0.94
2016-09-06T20:48:02.421458: step 1053, loss 0.216227, acc 0.96
2016-09-06T20:48:03.102142: step 1054, loss 0.102261, acc 0.94
2016-09-06T20:48:03.828508: step 1055, loss 0.102207, acc 0.92
2016-09-06T20:48:04.500551: step 1056, loss 0.125251, acc 0.96
2016-09-06T20:48:05.188700: step 1057, loss 0.0950264, acc 0.98
2016-09-06T20:48:05.865577: step 1058, loss 0.165047, acc 0.94
2016-09-06T20:48:06.569731: step 1059, loss 0.133222, acc 0.9
2016-09-06T20:48:07.262924: step 1060, loss 0.223061, acc 0.92
2016-09-06T20:48:07.936496: step 1061, loss 0.0672959, acc 0.98
2016-09-06T20:48:08.627886: step 1062, loss 0.0638809, acc 0.96
2016-09-06T20:48:09.302334: step 1063, loss 0.135443, acc 0.92
2016-09-06T20:48:09.984777: step 1064, loss 0.0735065, acc 1
2016-09-06T20:48:10.662274: step 1065, loss 0.0985111, acc 0.96
2016-09-06T20:48:11.351226: step 1066, loss 0.097369, acc 0.94
2016-09-06T20:48:12.033339: step 1067, loss 0.106479, acc 0.94
2016-09-06T20:48:12.715400: step 1068, loss 0.229632, acc 0.92
2016-09-06T20:48:13.423478: step 1069, loss 0.0909122, acc 0.98
2016-09-06T20:48:14.110997: step 1070, loss 0.0677913, acc 0.98
2016-09-06T20:48:14.790739: step 1071, loss 0.123922, acc 0.94
2016-09-06T20:48:15.472447: step 1072, loss 0.0499326, acc 1
2016-09-06T20:48:16.134490: step 1073, loss 0.063119, acc 0.98
2016-09-06T20:48:16.831925: step 1074, loss 0.188104, acc 0.92
2016-09-06T20:48:17.513103: step 1075, loss 0.189074, acc 0.96
2016-09-06T20:48:18.220478: step 1076, loss 0.0636588, acc 0.96
2016-09-06T20:48:18.898272: step 1077, loss 0.188018, acc 0.94
2016-09-06T20:48:19.586234: step 1078, loss 0.0987346, acc 0.96
2016-09-06T20:48:20.266057: step 1079, loss 0.120524, acc 0.96
2016-09-06T20:48:20.960400: step 1080, loss 0.0608643, acc 0.96
2016-09-06T20:48:21.668879: step 1081, loss 0.251335, acc 0.92
2016-09-06T20:48:22.347765: step 1082, loss 0.0606152, acc 1
2016-09-06T20:48:23.077225: step 1083, loss 0.144046, acc 0.88
2016-09-06T20:48:23.759823: step 1084, loss 0.176506, acc 0.96
2016-09-06T20:48:24.472405: step 1085, loss 0.250337, acc 0.9
2016-09-06T20:48:25.166840: step 1086, loss 0.170551, acc 0.92
2016-09-06T20:48:25.836491: step 1087, loss 0.0930122, acc 0.98
2016-09-06T20:48:26.524169: step 1088, loss 0.133073, acc 0.9
2016-09-06T20:48:27.184959: step 1089, loss 0.0492554, acc 0.98
2016-09-06T20:48:27.873196: step 1090, loss 0.09766, acc 0.98
2016-09-06T20:48:28.550483: step 1091, loss 0.12686, acc 0.92
2016-09-06T20:48:29.252903: step 1092, loss 0.114578, acc 0.94
2016-09-06T20:48:29.942876: step 1093, loss 0.106267, acc 0.92
2016-09-06T20:48:30.619088: step 1094, loss 0.156845, acc 0.94
2016-09-06T20:48:31.311903: step 1095, loss 0.228595, acc 0.96
2016-09-06T20:48:32.017039: step 1096, loss 0.178713, acc 0.9
2016-09-06T20:48:32.742814: step 1097, loss 0.143714, acc 0.92
2016-09-06T20:48:33.422079: step 1098, loss 0.0413331, acc 1
2016-09-06T20:48:34.121403: step 1099, loss 0.0606521, acc 0.98
2016-09-06T20:48:34.799071: step 1100, loss 0.118451, acc 0.96

Evaluation:
2016-09-06T20:48:37.923184: step 1100, loss 0.778843, acc 0.772045

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-1100

2016-09-06T20:48:39.726410: step 1101, loss 0.127957, acc 0.96
2016-09-06T20:48:40.422892: step 1102, loss 0.112231, acc 0.96
2016-09-06T20:48:41.123637: step 1103, loss 0.129761, acc 0.96
2016-09-06T20:48:41.827574: step 1104, loss 0.168742, acc 0.92
2016-09-06T20:48:42.511950: step 1105, loss 0.141741, acc 0.96
2016-09-06T20:48:43.199012: step 1106, loss 0.0572528, acc 1
2016-09-06T20:48:43.870290: step 1107, loss 0.199722, acc 0.92
2016-09-06T20:48:44.564450: step 1108, loss 0.17483, acc 0.92
2016-09-06T20:48:45.234683: step 1109, loss 0.122658, acc 0.92
2016-09-06T20:48:45.945608: step 1110, loss 0.0964471, acc 0.98
2016-09-06T20:48:46.649525: step 1111, loss 0.0576863, acc 0.98
2016-09-06T20:48:47.345899: step 1112, loss 0.134889, acc 0.92
2016-09-06T20:48:48.016334: step 1113, loss 0.126677, acc 0.96
2016-09-06T20:48:48.711660: step 1114, loss 0.0511159, acc 0.98
2016-09-06T20:48:49.408769: step 1115, loss 0.110343, acc 0.96
2016-09-06T20:48:50.064927: step 1116, loss 0.0439982, acc 0.98
2016-09-06T20:48:50.764880: step 1117, loss 0.0403646, acc 1
2016-09-06T20:48:51.462095: step 1118, loss 0.0390341, acc 0.98
2016-09-06T20:48:52.161335: step 1119, loss 0.0533828, acc 1
2016-09-06T20:48:52.873324: step 1120, loss 0.135939, acc 0.94
2016-09-06T20:48:53.548698: step 1121, loss 0.0433922, acc 0.96
2016-09-06T20:48:54.250798: step 1122, loss 0.0496995, acc 0.98
2016-09-06T20:48:54.929159: step 1123, loss 0.0880333, acc 0.96
2016-09-06T20:48:55.608416: step 1124, loss 0.0299644, acc 1
2016-09-06T20:48:56.288749: step 1125, loss 0.119023, acc 0.92
2016-09-06T20:48:56.964895: step 1126, loss 0.0660637, acc 0.96
2016-09-06T20:48:57.650911: step 1127, loss 0.0333108, acc 1
2016-09-06T20:48:58.339858: step 1128, loss 0.0834408, acc 0.96
2016-09-06T20:48:59.035815: step 1129, loss 0.112815, acc 0.94
2016-09-06T20:48:59.703865: step 1130, loss 0.0151704, acc 1
2016-09-06T20:49:00.434521: step 1131, loss 0.0243986, acc 1
2016-09-06T20:49:01.124707: step 1132, loss 0.0310679, acc 1
2016-09-06T20:49:01.819274: step 1133, loss 0.0542013, acc 0.96
2016-09-06T20:49:02.498039: step 1134, loss 0.209222, acc 0.88
2016-09-06T20:49:03.192674: step 1135, loss 0.0588993, acc 1
2016-09-06T20:49:03.899752: step 1136, loss 0.140804, acc 0.96
2016-09-06T20:49:04.574569: step 1137, loss 0.172238, acc 0.96
2016-09-06T20:49:05.271959: step 1138, loss 0.406893, acc 0.88
2016-09-06T20:49:05.989860: step 1139, loss 0.138587, acc 0.92
2016-09-06T20:49:06.705345: step 1140, loss 0.0864515, acc 0.96
2016-09-06T20:49:07.385311: step 1141, loss 0.308067, acc 0.94
2016-09-06T20:49:08.041279: step 1142, loss 0.0212084, acc 1
2016-09-06T20:49:08.750303: step 1143, loss 0.0362454, acc 1
2016-09-06T20:49:09.429649: step 1144, loss 0.0484255, acc 0.98
2016-09-06T20:49:10.101350: step 1145, loss 0.0856574, acc 0.94
2016-09-06T20:49:10.792838: step 1146, loss 0.196262, acc 0.96
2016-09-06T20:49:11.487237: step 1147, loss 0.159989, acc 0.9
2016-09-06T20:49:12.174798: step 1148, loss 0.0976999, acc 0.94
2016-09-06T20:49:12.846217: step 1149, loss 0.0633059, acc 0.96
2016-09-06T20:49:13.536457: step 1150, loss 0.11972, acc 0.96
2016-09-06T20:49:14.194597: step 1151, loss 0.182977, acc 0.96
2016-09-06T20:49:14.828436: step 1152, loss 0.101269, acc 0.931818
2016-09-06T20:49:15.526744: step 1153, loss 0.106164, acc 0.94
2016-09-06T20:49:16.227033: step 1154, loss 0.143831, acc 0.92
2016-09-06T20:49:16.897971: step 1155, loss 0.0778805, acc 0.92
2016-09-06T20:49:17.572802: step 1156, loss 0.0742176, acc 0.98
2016-09-06T20:49:18.270836: step 1157, loss 0.0581793, acc 0.96
2016-09-06T20:49:18.931537: step 1158, loss 0.126578, acc 0.94
2016-09-06T20:49:19.617583: step 1159, loss 0.0773142, acc 0.98
2016-09-06T20:49:20.306240: step 1160, loss 0.150344, acc 0.96
2016-09-06T20:49:20.979503: step 1161, loss 0.0806036, acc 0.98
2016-09-06T20:49:21.656119: step 1162, loss 0.0738337, acc 0.96
2016-09-06T20:49:22.343514: step 1163, loss 0.0787764, acc 0.98
2016-09-06T20:49:23.045447: step 1164, loss 0.0658215, acc 0.98
2016-09-06T20:49:23.721551: step 1165, loss 0.0575966, acc 1
2016-09-06T20:49:24.423544: step 1166, loss 0.049741, acc 1
2016-09-06T20:49:25.105888: step 1167, loss 0.0498916, acc 0.98
2016-09-06T20:49:25.790262: step 1168, loss 0.182341, acc 0.92
2016-09-06T20:49:26.490710: step 1169, loss 0.0325094, acc 1
2016-09-06T20:49:27.176816: step 1170, loss 0.0559894, acc 0.98
2016-09-06T20:49:27.857914: step 1171, loss 0.0875349, acc 0.98
2016-09-06T20:49:28.507912: step 1172, loss 0.0375279, acc 0.98
2016-09-06T20:49:29.218033: step 1173, loss 0.0245403, acc 1
2016-09-06T20:49:29.899228: step 1174, loss 0.0897496, acc 0.94
2016-09-06T20:49:30.568846: step 1175, loss 0.0208499, acc 1
2016-09-06T20:49:31.262207: step 1176, loss 0.0286872, acc 1
2016-09-06T20:49:31.945518: step 1177, loss 0.0303211, acc 1
2016-09-06T20:49:32.635890: step 1178, loss 0.00265272, acc 1
2016-09-06T20:49:33.310921: step 1179, loss 0.0466686, acc 0.98
2016-09-06T20:49:34.003875: step 1180, loss 0.280846, acc 0.94
2016-09-06T20:49:34.657853: step 1181, loss 0.0995265, acc 0.92
2016-09-06T20:49:35.331672: step 1182, loss 0.199784, acc 0.94
2016-09-06T20:49:36.025644: step 1183, loss 0.021081, acc 1
2016-09-06T20:49:36.683549: step 1184, loss 0.0743356, acc 0.96
2016-09-06T20:49:37.369021: step 1185, loss 0.0307205, acc 1
2016-09-06T20:49:38.053556: step 1186, loss 0.090599, acc 0.92
2016-09-06T20:49:38.749253: step 1187, loss 0.0211165, acc 1
2016-09-06T20:49:39.421514: step 1188, loss 0.125043, acc 0.92
2016-09-06T20:49:40.091654: step 1189, loss 0.06885, acc 0.94
2016-09-06T20:49:40.774709: step 1190, loss 0.0443051, acc 0.98
2016-09-06T20:49:41.463255: step 1191, loss 0.0322898, acc 0.98
2016-09-06T20:49:42.151625: step 1192, loss 0.0266823, acc 0.98
2016-09-06T20:49:42.833565: step 1193, loss 0.0814696, acc 0.98
2016-09-06T20:49:43.529319: step 1194, loss 0.0550987, acc 0.98
2016-09-06T20:49:44.205183: step 1195, loss 0.145167, acc 0.94
2016-09-06T20:49:44.893471: step 1196, loss 0.170038, acc 0.94
2016-09-06T20:49:45.573781: step 1197, loss 0.116697, acc 0.96
2016-09-06T20:49:46.283019: step 1198, loss 0.0290548, acc 0.98
2016-09-06T20:49:46.966409: step 1199, loss 0.0666187, acc 0.98
2016-09-06T20:49:47.657441: step 1200, loss 0.0647692, acc 0.98

Evaluation:
2016-09-06T20:49:50.802629: step 1200, loss 0.972236, acc 0.781426

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-1200

2016-09-06T20:49:52.544195: step 1201, loss 0.0808928, acc 0.96
2016-09-06T20:49:53.227273: step 1202, loss 0.0552231, acc 0.98
2016-09-06T20:49:53.910115: step 1203, loss 0.105455, acc 0.98
2016-09-06T20:49:54.595506: step 1204, loss 0.0534159, acc 0.98
2016-09-06T20:49:55.270649: step 1205, loss 0.109854, acc 0.96
2016-09-06T20:49:55.973763: step 1206, loss 0.0712551, acc 0.96
2016-09-06T20:49:56.646800: step 1207, loss 0.0971118, acc 0.98
2016-09-06T20:49:57.323196: step 1208, loss 0.107285, acc 0.96
2016-09-06T20:49:58.035943: step 1209, loss 0.046613, acc 0.96
2016-09-06T20:49:58.697257: step 1210, loss 0.210613, acc 0.9
2016-09-06T20:49:59.388035: step 1211, loss 0.0624214, acc 0.98
2016-09-06T20:50:00.090187: step 1212, loss 0.0474348, acc 0.98
2016-09-06T20:50:00.787813: step 1213, loss 0.0465036, acc 0.98
2016-09-06T20:50:01.474719: step 1214, loss 0.0321375, acc 0.98
2016-09-06T20:50:02.134840: step 1215, loss 0.0926449, acc 0.98
2016-09-06T20:50:02.864322: step 1216, loss 0.0446737, acc 0.96
2016-09-06T20:50:03.538988: step 1217, loss 0.0751827, acc 0.96
2016-09-06T20:50:04.202173: step 1218, loss 0.155904, acc 0.94
2016-09-06T20:50:04.883698: step 1219, loss 0.077278, acc 0.98
2016-09-06T20:50:05.580132: step 1220, loss 0.0650227, acc 0.98
2016-09-06T20:50:06.264511: step 1221, loss 0.113756, acc 0.96
2016-09-06T20:50:06.979915: step 1222, loss 0.0978712, acc 0.96
2016-09-06T20:50:07.698965: step 1223, loss 0.0546095, acc 0.98
2016-09-06T20:50:08.404864: step 1224, loss 0.0111003, acc 1
2016-09-06T20:50:09.103957: step 1225, loss 0.0858357, acc 0.98
2016-09-06T20:50:09.787678: step 1226, loss 0.188963, acc 0.94
2016-09-06T20:50:10.469584: step 1227, loss 0.0684565, acc 0.96
2016-09-06T20:50:11.174782: step 1228, loss 0.120693, acc 0.92
2016-09-06T20:50:11.850415: step 1229, loss 0.0950506, acc 0.94
2016-09-06T20:50:12.553546: step 1230, loss 0.119154, acc 0.94
2016-09-06T20:50:13.239965: step 1231, loss 0.0520558, acc 0.98
2016-09-06T20:50:13.923887: step 1232, loss 0.0298862, acc 1
2016-09-06T20:50:14.597341: step 1233, loss 0.196292, acc 0.9
2016-09-06T20:50:15.289597: step 1234, loss 0.0744878, acc 0.98
2016-09-06T20:50:15.993559: step 1235, loss 0.149907, acc 0.92
2016-09-06T20:50:16.676064: step 1236, loss 0.046526, acc 0.98
2016-09-06T20:50:17.388169: step 1237, loss 0.0761062, acc 0.96
2016-09-06T20:50:18.078405: step 1238, loss 0.0684074, acc 0.98
2016-09-06T20:50:18.767926: step 1239, loss 0.0736849, acc 0.98
2016-09-06T20:50:19.469830: step 1240, loss 0.101425, acc 0.94
2016-09-06T20:50:20.146516: step 1241, loss 0.125382, acc 0.94
2016-09-06T20:50:20.856917: step 1242, loss 0.0538529, acc 0.98
2016-09-06T20:50:21.540371: step 1243, loss 0.262207, acc 0.92
2016-09-06T20:50:22.247906: step 1244, loss 0.0924767, acc 0.98
2016-09-06T20:50:22.949033: step 1245, loss 0.0996926, acc 0.96
2016-09-06T20:50:23.642889: step 1246, loss 0.0964385, acc 0.96
2016-09-06T20:50:24.320133: step 1247, loss 0.147592, acc 0.92
2016-09-06T20:50:24.994736: step 1248, loss 0.0582169, acc 0.98
2016-09-06T20:50:25.695459: step 1249, loss 0.0969074, acc 0.96
2016-09-06T20:50:26.374414: step 1250, loss 0.0830646, acc 0.98
2016-09-06T20:50:27.084406: step 1251, loss 0.12085, acc 0.96
2016-09-06T20:50:27.776415: step 1252, loss 0.0559771, acc 0.98
2016-09-06T20:50:28.470351: step 1253, loss 0.0762469, acc 0.96
2016-09-06T20:50:29.151833: step 1254, loss 0.051156, acc 0.98
2016-09-06T20:50:29.808681: step 1255, loss 0.259884, acc 0.9
2016-09-06T20:50:30.512459: step 1256, loss 0.0783767, acc 0.98
2016-09-06T20:50:31.169684: step 1257, loss 0.0898389, acc 0.94
2016-09-06T20:50:31.839631: step 1258, loss 0.108514, acc 0.98
2016-09-06T20:50:32.516770: step 1259, loss 0.0872395, acc 0.98
2016-09-06T20:50:33.199966: step 1260, loss 0.0809877, acc 0.98
2016-09-06T20:50:33.867429: step 1261, loss 0.15939, acc 0.92
2016-09-06T20:50:34.553388: step 1262, loss 0.122833, acc 0.96
2016-09-06T20:50:35.256234: step 1263, loss 0.114892, acc 0.96
2016-09-06T20:50:35.916841: step 1264, loss 0.131164, acc 0.94
2016-09-06T20:50:36.623199: step 1265, loss 0.0445438, acc 0.98
2016-09-06T20:50:37.282035: step 1266, loss 0.13631, acc 0.94
2016-09-06T20:50:37.978744: step 1267, loss 0.0376752, acc 1
2016-09-06T20:50:38.665390: step 1268, loss 0.0900964, acc 0.96
2016-09-06T20:50:39.344446: step 1269, loss 0.0890621, acc 0.94
2016-09-06T20:50:40.044080: step 1270, loss 0.0575497, acc 1
2016-09-06T20:50:40.722405: step 1271, loss 0.195657, acc 0.96
2016-09-06T20:50:41.429744: step 1272, loss 0.0564289, acc 1
2016-09-06T20:50:42.096138: step 1273, loss 0.067551, acc 0.96
2016-09-06T20:50:42.789445: step 1274, loss 0.105862, acc 0.96
2016-09-06T20:50:43.482521: step 1275, loss 0.114164, acc 0.98
2016-09-06T20:50:44.167376: step 1276, loss 0.101209, acc 0.94
2016-09-06T20:50:44.876360: step 1277, loss 0.0709428, acc 0.96
2016-09-06T20:50:45.534374: step 1278, loss 0.187033, acc 0.88
2016-09-06T20:50:46.224744: step 1279, loss 0.0609313, acc 0.98
2016-09-06T20:50:46.899087: step 1280, loss 0.0168798, acc 1
2016-09-06T20:50:47.621204: step 1281, loss 0.0352517, acc 0.98
2016-09-06T20:50:48.313329: step 1282, loss 0.149513, acc 0.9
2016-09-06T20:50:48.987550: step 1283, loss 0.0965319, acc 0.92
2016-09-06T20:50:49.698251: step 1284, loss 0.0657715, acc 0.98
2016-09-06T20:50:50.374919: step 1285, loss 0.0924157, acc 0.96
2016-09-06T20:50:51.046114: step 1286, loss 0.0578263, acc 0.96
2016-09-06T20:50:51.725143: step 1287, loss 0.0327916, acc 0.98
2016-09-06T20:50:52.414857: step 1288, loss 0.0249048, acc 0.98
2016-09-06T20:50:53.113531: step 1289, loss 0.120662, acc 0.96
2016-09-06T20:50:53.791839: step 1290, loss 0.10611, acc 0.94
2016-09-06T20:50:54.480823: step 1291, loss 0.0529405, acc 0.98
2016-09-06T20:50:55.140615: step 1292, loss 0.0460509, acc 1
2016-09-06T20:50:55.845327: step 1293, loss 0.160182, acc 0.92
2016-09-06T20:50:56.538252: step 1294, loss 0.0539542, acc 1
2016-09-06T20:50:57.215694: step 1295, loss 0.103103, acc 0.94
2016-09-06T20:50:57.913563: step 1296, loss 0.0801573, acc 0.94
2016-09-06T20:50:58.589838: step 1297, loss 0.0413359, acc 0.96
2016-09-06T20:50:59.302280: step 1298, loss 0.11468, acc 0.94
2016-09-06T20:50:59.963923: step 1299, loss 0.0380702, acc 0.98
2016-09-06T20:51:00.688615: step 1300, loss 0.113204, acc 0.98

Evaluation:
2016-09-06T20:51:03.836126: step 1300, loss 1.11936, acc 0.772983

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-1300

2016-09-06T20:51:05.442876: step 1301, loss 0.133656, acc 0.96
2016-09-06T20:51:06.127303: step 1302, loss 0.0493266, acc 0.98
2016-09-06T20:51:06.845094: step 1303, loss 0.0485806, acc 0.98
2016-09-06T20:51:07.537636: step 1304, loss 0.22628, acc 0.92
2016-09-06T20:51:08.223754: step 1305, loss 0.0181042, acc 1
2016-09-06T20:51:08.951178: step 1306, loss 0.0520363, acc 0.98
2016-09-06T20:51:09.615991: step 1307, loss 0.0754476, acc 0.96
2016-09-06T20:51:10.303267: step 1308, loss 0.0695141, acc 0.94
2016-09-06T20:51:10.982638: step 1309, loss 0.0646598, acc 0.96
2016-09-06T20:51:11.668401: step 1310, loss 0.0503463, acc 0.96
2016-09-06T20:51:12.370292: step 1311, loss 0.0607579, acc 0.98
2016-09-06T20:51:13.054656: step 1312, loss 0.079432, acc 0.96
2016-09-06T20:51:13.766628: step 1313, loss 0.0744704, acc 0.98
2016-09-06T20:51:14.435380: step 1314, loss 0.0522256, acc 0.98
2016-09-06T20:51:15.112150: step 1315, loss 0.108067, acc 0.98
2016-09-06T20:51:15.805005: step 1316, loss 0.148408, acc 0.96
2016-09-06T20:51:16.465298: step 1317, loss 0.15558, acc 0.92
2016-09-06T20:51:17.158780: step 1318, loss 0.130887, acc 0.94
2016-09-06T20:51:17.842343: step 1319, loss 0.178596, acc 0.92
2016-09-06T20:51:18.549085: step 1320, loss 0.107035, acc 0.96
2016-09-06T20:51:19.245752: step 1321, loss 0.0687196, acc 0.96
2016-09-06T20:51:19.974107: step 1322, loss 0.0995616, acc 0.94
2016-09-06T20:51:20.657529: step 1323, loss 0.103056, acc 0.94
2016-09-06T20:51:21.344634: step 1324, loss 0.181726, acc 0.96
2016-09-06T20:51:22.018938: step 1325, loss 0.0457366, acc 0.98
2016-09-06T20:51:22.691867: step 1326, loss 0.0852909, acc 0.98
2016-09-06T20:51:23.409851: step 1327, loss 0.0540253, acc 0.98
2016-09-06T20:51:24.074944: step 1328, loss 0.0282403, acc 0.98
2016-09-06T20:51:24.758856: step 1329, loss 0.068288, acc 0.96
2016-09-06T20:51:25.447360: step 1330, loss 0.0974251, acc 0.96
2016-09-06T20:51:26.129859: step 1331, loss 0.175712, acc 0.94
2016-09-06T20:51:26.812315: step 1332, loss 0.236465, acc 0.9
2016-09-06T20:51:27.499100: step 1333, loss 0.0272538, acc 0.98
2016-09-06T20:51:28.205067: step 1334, loss 0.0459736, acc 0.98
2016-09-06T20:51:28.875146: step 1335, loss 0.272161, acc 0.94
2016-09-06T20:51:29.548432: step 1336, loss 0.212728, acc 0.9
2016-09-06T20:51:30.216579: step 1337, loss 0.196326, acc 0.92
2016-09-06T20:51:30.890178: step 1338, loss 0.0621705, acc 0.96
2016-09-06T20:51:31.564883: step 1339, loss 0.0959562, acc 0.94
2016-09-06T20:51:32.243047: step 1340, loss 0.0938594, acc 0.96
2016-09-06T20:51:32.947181: step 1341, loss 0.0977484, acc 0.94
2016-09-06T20:51:33.616283: step 1342, loss 0.0688449, acc 0.98
2016-09-06T20:51:34.320568: step 1343, loss 0.0910107, acc 0.96
2016-09-06T20:51:34.956880: step 1344, loss 0.033943, acc 1
2016-09-06T20:51:35.637674: step 1345, loss 0.0194935, acc 1
2016-09-06T20:51:36.320690: step 1346, loss 0.0760792, acc 0.94
2016-09-06T20:51:36.993377: step 1347, loss 0.0697307, acc 1
2016-09-06T20:51:37.673063: step 1348, loss 0.0483732, acc 0.98
2016-09-06T20:51:38.349521: step 1349, loss 0.0574446, acc 0.98
2016-09-06T20:51:39.056367: step 1350, loss 0.0610537, acc 0.98
2016-09-06T20:51:39.737407: step 1351, loss 0.0649741, acc 0.96
2016-09-06T20:51:40.410934: step 1352, loss 0.0774508, acc 0.96
2016-09-06T20:51:41.084203: step 1353, loss 0.0914811, acc 0.98
2016-09-06T20:51:41.781418: step 1354, loss 0.16357, acc 0.92
2016-09-06T20:51:42.464078: step 1355, loss 0.295046, acc 0.92
2016-09-06T20:51:43.146552: step 1356, loss 0.0313449, acc 1
2016-09-06T20:51:43.859193: step 1357, loss 0.0787165, acc 0.94
2016-09-06T20:51:44.561058: step 1358, loss 0.0761726, acc 0.98
2016-09-06T20:51:45.247903: step 1359, loss 0.0519856, acc 0.98
2016-09-06T20:51:45.946325: step 1360, loss 0.0605109, acc 0.98
2016-09-06T20:51:46.634940: step 1361, loss 0.0734535, acc 0.96
2016-09-06T20:51:47.304362: step 1362, loss 0.0283702, acc 1
2016-09-06T20:51:47.955336: step 1363, loss 0.133495, acc 0.96
2016-09-06T20:51:48.671535: step 1364, loss 0.0718111, acc 0.96
2016-09-06T20:51:49.376408: step 1365, loss 0.0462431, acc 1
2016-09-06T20:51:50.053434: step 1366, loss 0.159752, acc 0.94
2016-09-06T20:51:50.747649: step 1367, loss 0.0407681, acc 0.98
2016-09-06T20:51:51.433384: step 1368, loss 0.0611679, acc 0.98
2016-09-06T20:51:52.111437: step 1369, loss 0.0343947, acc 0.98
2016-09-06T20:51:52.782953: step 1370, loss 0.0272549, acc 0.98
2016-09-06T20:51:53.484426: step 1371, loss 0.221626, acc 0.94
2016-09-06T20:51:54.163404: step 1372, loss 0.019362, acc 1
2016-09-06T20:51:54.843061: step 1373, loss 0.0405072, acc 1
2016-09-06T20:51:55.535647: step 1374, loss 0.0537808, acc 0.98
2016-09-06T20:51:56.219928: step 1375, loss 0.051961, acc 0.98
2016-09-06T20:51:56.925047: step 1376, loss 0.0836109, acc 0.94
2016-09-06T20:51:57.593495: step 1377, loss 0.0376249, acc 0.98
2016-09-06T20:51:58.284019: step 1378, loss 0.106626, acc 0.94
2016-09-06T20:51:58.980121: step 1379, loss 0.0357791, acc 1
2016-09-06T20:51:59.657531: step 1380, loss 0.169437, acc 0.94
2016-09-06T20:52:00.366597: step 1381, loss 0.0696991, acc 0.94
2016-09-06T20:52:01.049095: step 1382, loss 0.105969, acc 0.96
2016-09-06T20:52:01.763260: step 1383, loss 0.0087232, acc 1
2016-09-06T20:52:02.445883: step 1384, loss 0.0488728, acc 0.98
2016-09-06T20:52:03.134101: step 1385, loss 0.0544352, acc 0.98
2016-09-06T20:52:03.830883: step 1386, loss 0.235766, acc 0.92
2016-09-06T20:52:04.507570: step 1387, loss 0.0248994, acc 0.98
2016-09-06T20:52:05.172744: step 1388, loss 0.0697517, acc 0.98
2016-09-06T20:52:05.881775: step 1389, loss 0.0369669, acc 0.98
2016-09-06T20:52:06.578404: step 1390, loss 0.0473872, acc 1
2016-09-06T20:52:07.257160: step 1391, loss 0.0988077, acc 0.92
2016-09-06T20:52:07.978129: step 1392, loss 0.0655254, acc 0.96
2016-09-06T20:52:08.657376: step 1393, loss 0.043329, acc 0.98
2016-09-06T20:52:09.345232: step 1394, loss 0.0921432, acc 0.96
2016-09-06T20:52:10.023774: step 1395, loss 0.13737, acc 0.96
2016-09-06T20:52:10.702265: step 1396, loss 0.0627755, acc 0.96
2016-09-06T20:52:11.393770: step 1397, loss 0.0634696, acc 0.98
2016-09-06T20:52:12.044764: step 1398, loss 0.0649798, acc 0.98
2016-09-06T20:52:12.737288: step 1399, loss 0.0714476, acc 0.96
2016-09-06T20:52:13.427900: step 1400, loss 0.0867646, acc 0.94

Evaluation:
2016-09-06T20:52:16.567664: step 1400, loss 1.12806, acc 0.761726

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-1400

2016-09-06T20:52:18.270622: step 1401, loss 0.0548164, acc 0.98
2016-09-06T20:52:18.958637: step 1402, loss 0.0418529, acc 0.98
2016-09-06T20:52:19.644492: step 1403, loss 0.0629485, acc 0.98
2016-09-06T20:52:20.337934: step 1404, loss 0.058022, acc 0.98
2016-09-06T20:52:21.065151: step 1405, loss 0.117825, acc 0.94
2016-09-06T20:52:21.710273: step 1406, loss 0.0310447, acc 0.98
2016-09-06T20:52:22.415530: step 1407, loss 0.0561015, acc 0.98
2016-09-06T20:52:23.084313: step 1408, loss 0.0766002, acc 0.94
2016-09-06T20:52:23.755606: step 1409, loss 0.0399359, acc 0.98
2016-09-06T20:52:24.432649: step 1410, loss 0.119549, acc 0.98
2016-09-06T20:52:25.121509: step 1411, loss 0.0100595, acc 1
2016-09-06T20:52:25.830991: step 1412, loss 0.028154, acc 0.98
2016-09-06T20:52:26.498020: step 1413, loss 0.0474263, acc 0.98
2016-09-06T20:52:27.217235: step 1414, loss 0.0382122, acc 0.98
2016-09-06T20:52:27.913858: step 1415, loss 0.00879645, acc 1
2016-09-06T20:52:28.591814: step 1416, loss 0.00948571, acc 1
2016-09-06T20:52:29.261958: step 1417, loss 0.0422802, acc 0.98
2016-09-06T20:52:29.958827: step 1418, loss 0.174928, acc 0.98
2016-09-06T20:52:30.667505: step 1419, loss 0.0526711, acc 0.96
2016-09-06T20:52:31.351636: step 1420, loss 0.0764212, acc 0.96
2016-09-06T20:52:32.032089: step 1421, loss 0.132157, acc 0.92
2016-09-06T20:52:32.716573: step 1422, loss 0.0262461, acc 1
2016-09-06T20:52:33.376116: step 1423, loss 0.00658625, acc 1
2016-09-06T20:52:34.078792: step 1424, loss 0.0730202, acc 0.96
2016-09-06T20:52:34.759245: step 1425, loss 0.0369149, acc 0.98
2016-09-06T20:52:35.464046: step 1426, loss 0.0419688, acc 0.98
2016-09-06T20:52:36.128303: step 1427, loss 0.00643455, acc 1
2016-09-06T20:52:36.824186: step 1428, loss 0.0417733, acc 0.96
2016-09-06T20:52:37.509384: step 1429, loss 0.0841332, acc 0.96
2016-09-06T20:52:38.201163: step 1430, loss 0.0687323, acc 0.94
2016-09-06T20:52:38.881925: step 1431, loss 0.0079571, acc 1
2016-09-06T20:52:39.556708: step 1432, loss 0.0392, acc 0.98
2016-09-06T20:52:40.256717: step 1433, loss 0.0880139, acc 0.96
2016-09-06T20:52:40.951815: step 1434, loss 0.081447, acc 0.96
2016-09-06T20:52:41.646647: step 1435, loss 0.0396987, acc 1
2016-09-06T20:52:42.330376: step 1436, loss 0.0325888, acc 1
2016-09-06T20:52:43.030275: step 1437, loss 0.0264918, acc 0.98
2016-09-06T20:52:43.713770: step 1438, loss 0.239358, acc 0.92
2016-09-06T20:52:44.410171: step 1439, loss 0.0488679, acc 0.98
2016-09-06T20:52:45.112328: step 1440, loss 0.0835221, acc 0.94
2016-09-06T20:52:45.795630: step 1441, loss 0.149936, acc 0.98
2016-09-06T20:52:46.500310: step 1442, loss 0.0811475, acc 0.96
2016-09-06T20:52:47.177249: step 1443, loss 0.149267, acc 0.96
2016-09-06T20:52:47.851338: step 1444, loss 0.0297981, acc 0.98
2016-09-06T20:52:48.547707: step 1445, loss 0.137045, acc 0.94
2016-09-06T20:52:49.242833: step 1446, loss 0.0242256, acc 1
2016-09-06T20:52:49.941113: step 1447, loss 0.0573633, acc 0.96
2016-09-06T20:52:50.605405: step 1448, loss 0.155585, acc 0.92
2016-09-06T20:52:51.286634: step 1449, loss 0.0702389, acc 0.94
2016-09-06T20:52:51.962879: step 1450, loss 0.173349, acc 0.96
2016-09-06T20:52:52.676430: step 1451, loss 0.111502, acc 0.96
2016-09-06T20:52:53.365066: step 1452, loss 0.045181, acc 0.98
2016-09-06T20:52:54.045533: step 1453, loss 0.0571752, acc 0.96
2016-09-06T20:52:54.721844: step 1454, loss 0.126982, acc 0.96
2016-09-06T20:52:55.373136: step 1455, loss 0.0466237, acc 1
2016-09-06T20:52:56.070802: step 1456, loss 0.0838791, acc 0.98
2016-09-06T20:52:56.758559: step 1457, loss 0.0405929, acc 1
2016-09-06T20:52:57.440293: step 1458, loss 0.0626578, acc 0.98
2016-09-06T20:52:58.124652: step 1459, loss 0.142195, acc 0.96
2016-09-06T20:52:58.821244: step 1460, loss 0.0496534, acc 0.98
2016-09-06T20:52:59.542693: step 1461, loss 0.0517243, acc 0.98
2016-09-06T20:53:00.214470: step 1462, loss 0.068064, acc 0.96
2016-09-06T20:53:00.890033: step 1463, loss 0.0775262, acc 0.94
2016-09-06T20:53:01.603887: step 1464, loss 0.0760378, acc 0.96
2016-09-06T20:53:02.302598: step 1465, loss 0.0869074, acc 0.96
2016-09-06T20:53:02.994707: step 1466, loss 0.0708349, acc 0.94
2016-09-06T20:53:03.664489: step 1467, loss 0.0584447, acc 0.98
2016-09-06T20:53:04.360288: step 1468, loss 0.0309891, acc 0.98
2016-09-06T20:53:05.034896: step 1469, loss 0.0313261, acc 1
2016-09-06T20:53:05.751009: step 1470, loss 0.0657489, acc 0.98
2016-09-06T20:53:06.447211: step 1471, loss 0.0645288, acc 0.96
2016-09-06T20:53:07.132321: step 1472, loss 0.117524, acc 0.96
2016-09-06T20:53:07.806809: step 1473, loss 0.0581237, acc 0.98
2016-09-06T20:53:08.497579: step 1474, loss 0.113988, acc 0.94
2016-09-06T20:53:09.198474: step 1475, loss 0.20683, acc 0.9
2016-09-06T20:53:09.885564: step 1476, loss 0.0487184, acc 1
2016-09-06T20:53:10.583917: step 1477, loss 0.0627351, acc 0.96
2016-09-06T20:53:11.254058: step 1478, loss 0.0187242, acc 1
2016-09-06T20:53:11.949305: step 1479, loss 0.0314512, acc 0.98
2016-09-06T20:53:12.653223: step 1480, loss 0.122306, acc 0.96
2016-09-06T20:53:13.313877: step 1481, loss 0.0107036, acc 1
2016-09-06T20:53:13.998346: step 1482, loss 0.0377216, acc 1
2016-09-06T20:53:14.685757: step 1483, loss 0.0388586, acc 1
2016-09-06T20:53:15.356217: step 1484, loss 0.054455, acc 0.98
2016-09-06T20:53:16.053336: step 1485, loss 0.0134245, acc 1
2016-09-06T20:53:16.753011: step 1486, loss 0.0243938, acc 1
2016-09-06T20:53:17.461180: step 1487, loss 0.0835212, acc 0.96
2016-09-06T20:53:18.140579: step 1488, loss 0.0553892, acc 0.96
2016-09-06T20:53:18.845416: step 1489, loss 0.0472959, acc 0.98
2016-09-06T20:53:19.518171: step 1490, loss 0.114019, acc 0.94
2016-09-06T20:53:20.223130: step 1491, loss 0.0208355, acc 1
2016-09-06T20:53:20.922971: step 1492, loss 0.123074, acc 0.96
2016-09-06T20:53:21.627817: step 1493, loss 0.0682316, acc 0.94
2016-09-06T20:53:22.323255: step 1494, loss 0.0571667, acc 0.98
2016-09-06T20:53:22.986431: step 1495, loss 0.0516696, acc 1
2016-09-06T20:53:23.696939: step 1496, loss 0.0338474, acc 0.98
2016-09-06T20:53:24.391299: step 1497, loss 0.0107713, acc 1
2016-09-06T20:53:25.081361: step 1498, loss 0.157277, acc 0.92
2016-09-06T20:53:25.796047: step 1499, loss 0.0217194, acc 1
2016-09-06T20:53:26.487433: step 1500, loss 0.0414555, acc 0.98

Evaluation:
2016-09-06T20:53:29.664413: step 1500, loss 1.28182, acc 0.765478

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-1500

2016-09-06T20:53:31.414273: step 1501, loss 0.0253588, acc 0.98
2016-09-06T20:53:32.118875: step 1502, loss 0.0503209, acc 0.96
2016-09-06T20:53:32.819924: step 1503, loss 0.0857462, acc 0.96
2016-09-06T20:53:33.498075: step 1504, loss 0.0693025, acc 0.96
2016-09-06T20:53:34.190231: step 1505, loss 0.159636, acc 0.96
2016-09-06T20:53:34.881722: step 1506, loss 0.0247274, acc 0.98
2016-09-06T20:53:35.591146: step 1507, loss 0.0651851, acc 0.94
2016-09-06T20:53:36.272698: step 1508, loss 0.0303027, acc 1
2016-09-06T20:53:37.005113: step 1509, loss 0.0263218, acc 0.98
2016-09-06T20:53:37.677881: step 1510, loss 0.0468008, acc 0.98
2016-09-06T20:53:38.359363: step 1511, loss 0.054389, acc 0.98
2016-09-06T20:53:39.046614: step 1512, loss 0.139758, acc 0.92
2016-09-06T20:53:39.745856: step 1513, loss 0.0662174, acc 0.96
2016-09-06T20:53:40.451103: step 1514, loss 0.0313981, acc 1
2016-09-06T20:53:41.127824: step 1515, loss 0.0594125, acc 0.96
2016-09-06T20:53:41.818077: step 1516, loss 0.0403671, acc 0.96
2016-09-06T20:53:42.515862: step 1517, loss 0.0672576, acc 0.98
2016-09-06T20:53:43.221790: step 1518, loss 0.0646048, acc 0.96
2016-09-06T20:53:43.929702: step 1519, loss 0.0960358, acc 0.96
2016-09-06T20:53:44.616232: step 1520, loss 0.0489693, acc 0.96
2016-09-06T20:53:45.308123: step 1521, loss 0.0590432, acc 0.96
2016-09-06T20:53:45.977756: step 1522, loss 0.0784625, acc 0.96
2016-09-06T20:53:46.658408: step 1523, loss 0.0640925, acc 0.96
2016-09-06T20:53:47.346759: step 1524, loss 0.0767026, acc 0.94
2016-09-06T20:53:48.031791: step 1525, loss 0.0297079, acc 1
2016-09-06T20:53:48.719316: step 1526, loss 0.0642839, acc 0.96
2016-09-06T20:53:49.395442: step 1527, loss 0.0264356, acc 1
2016-09-06T20:53:50.092157: step 1528, loss 0.0850121, acc 0.94
2016-09-06T20:53:50.779420: step 1529, loss 0.0465948, acc 0.98
2016-09-06T20:53:51.453787: step 1530, loss 0.00203898, acc 1
2016-09-06T20:53:52.146916: step 1531, loss 0.0315725, acc 1
2016-09-06T20:53:52.860088: step 1532, loss 0.0515348, acc 0.98
2016-09-06T20:53:53.552449: step 1533, loss 0.023316, acc 1
2016-09-06T20:53:54.205128: step 1534, loss 0.0590576, acc 0.98
2016-09-06T20:53:54.913287: step 1535, loss 0.0400376, acc 0.98
2016-09-06T20:53:55.504624: step 1536, loss 0.174808, acc 0.931818
2016-09-06T20:53:56.200623: step 1537, loss 0.0401805, acc 0.98
2016-09-06T20:53:56.895091: step 1538, loss 0.0241243, acc 1
2016-09-06T20:53:57.584946: step 1539, loss 0.0142298, acc 1
2016-09-06T20:53:58.260871: step 1540, loss 0.0689641, acc 0.96
2016-09-06T20:53:58.953300: step 1541, loss 0.109294, acc 0.96
2016-09-06T20:53:59.659040: step 1542, loss 0.0598505, acc 0.96
2016-09-06T20:54:00.377993: step 1543, loss 0.0694235, acc 0.96
2016-09-06T20:54:01.059239: step 1544, loss 0.0703093, acc 0.98
2016-09-06T20:54:01.748457: step 1545, loss 0.100117, acc 0.94
2016-09-06T20:54:02.434296: step 1546, loss 0.0799216, acc 0.96
2016-09-06T20:54:03.113792: step 1547, loss 0.0316905, acc 1
2016-09-06T20:54:03.794204: step 1548, loss 0.0713474, acc 0.98
2016-09-06T20:54:04.494509: step 1549, loss 0.0856014, acc 0.94
2016-09-06T20:54:05.160193: step 1550, loss 0.050575, acc 0.98
2016-09-06T20:54:05.835666: step 1551, loss 0.0494396, acc 0.98
2016-09-06T20:54:06.516942: step 1552, loss 0.0241676, acc 0.98
2016-09-06T20:54:07.200615: step 1553, loss 0.10372, acc 0.94
2016-09-06T20:54:07.881361: step 1554, loss 0.279504, acc 0.92
2016-09-06T20:54:08.574338: step 1555, loss 0.0631502, acc 0.96
2016-09-06T20:54:09.285477: step 1556, loss 0.166692, acc 0.94
2016-09-06T20:54:09.960894: step 1557, loss 0.0572472, acc 0.98
2016-09-06T20:54:10.648977: step 1558, loss 0.175347, acc 0.92
2016-09-06T20:54:11.329675: step 1559, loss 0.0290596, acc 0.98
2016-09-06T20:54:12.036751: step 1560, loss 0.0318851, acc 0.98
2016-09-06T20:54:12.719534: step 1561, loss 0.138302, acc 0.96
2016-09-06T20:54:13.405750: step 1562, loss 0.01761, acc 1
2016-09-06T20:54:14.103891: step 1563, loss 0.0659421, acc 0.96
2016-09-06T20:54:14.813016: step 1564, loss 0.0584311, acc 0.96
2016-09-06T20:54:15.540662: step 1565, loss 0.0752603, acc 0.98
2016-09-06T20:54:16.223775: step 1566, loss 0.0249049, acc 0.98
2016-09-06T20:54:16.915893: step 1567, loss 0.0340467, acc 0.98
2016-09-06T20:54:17.610857: step 1568, loss 0.200367, acc 0.94
2016-09-06T20:54:18.266453: step 1569, loss 0.091283, acc 0.94
2016-09-06T20:54:18.973981: step 1570, loss 0.164918, acc 0.96
2016-09-06T20:54:19.636428: step 1571, loss 0.0428827, acc 1
2016-09-06T20:54:20.309576: step 1572, loss 0.0979564, acc 0.98
2016-09-06T20:54:21.003101: step 1573, loss 0.0602374, acc 0.94
2016-09-06T20:54:21.703007: step 1574, loss 0.0661496, acc 0.98
2016-09-06T20:54:22.395513: step 1575, loss 0.0529811, acc 0.96
2016-09-06T20:54:23.079250: step 1576, loss 0.0087624, acc 1
2016-09-06T20:54:23.779856: step 1577, loss 0.0611267, acc 0.98
2016-09-06T20:54:24.461743: step 1578, loss 0.0671358, acc 0.98
2016-09-06T20:54:25.165451: step 1579, loss 0.123191, acc 0.92
2016-09-06T20:54:25.850003: step 1580, loss 0.186376, acc 0.96
2016-09-06T20:54:26.539275: step 1581, loss 0.047929, acc 0.98
2016-09-06T20:54:27.232701: step 1582, loss 0.0144881, acc 1
2016-09-06T20:54:27.906824: step 1583, loss 0.0476193, acc 0.98
2016-09-06T20:54:28.598700: step 1584, loss 0.0867869, acc 0.98
2016-09-06T20:54:29.276835: step 1585, loss 0.0706336, acc 0.98
2016-09-06T20:54:29.955752: step 1586, loss 0.106804, acc 0.98
2016-09-06T20:54:30.624122: step 1587, loss 0.0485413, acc 1
2016-09-06T20:54:31.297526: step 1588, loss 0.129867, acc 0.96
2016-09-06T20:54:31.971503: step 1589, loss 0.0123315, acc 1
2016-09-06T20:54:32.655690: step 1590, loss 0.032987, acc 1
2016-09-06T20:54:33.353810: step 1591, loss 0.0810104, acc 0.94
2016-09-06T20:54:34.033212: step 1592, loss 0.0423963, acc 0.98
2016-09-06T20:54:34.714474: step 1593, loss 0.137846, acc 0.96
2016-09-06T20:54:35.400243: step 1594, loss 0.0715628, acc 0.98
2016-09-06T20:54:36.081075: step 1595, loss 0.0729009, acc 0.98
2016-09-06T20:54:36.761238: step 1596, loss 0.0494322, acc 0.98
2016-09-06T20:54:37.436969: step 1597, loss 0.0371244, acc 0.96
2016-09-06T20:54:38.131716: step 1598, loss 0.0196612, acc 1
2016-09-06T20:54:38.797347: step 1599, loss 0.0394456, acc 0.98
2016-09-06T20:54:39.507894: step 1600, loss 0.0794212, acc 0.96

Evaluation:
2016-09-06T20:54:42.643734: step 1600, loss 1.36282, acc 0.763602

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-1600

2016-09-06T20:54:44.326072: step 1601, loss 0.0499118, acc 0.98
2016-09-06T20:54:45.017295: step 1602, loss 0.0424116, acc 0.98
2016-09-06T20:54:45.703625: step 1603, loss 0.102784, acc 0.96
2016-09-06T20:54:46.397868: step 1604, loss 0.134835, acc 0.92
2016-09-06T20:54:47.094645: step 1605, loss 0.0446201, acc 1
2016-09-06T20:54:47.789747: step 1606, loss 0.0524541, acc 0.98
2016-09-06T20:54:48.467685: step 1607, loss 0.204896, acc 0.96
2016-09-06T20:54:49.151507: step 1608, loss 0.0629679, acc 0.98
2016-09-06T20:54:49.826988: step 1609, loss 0.20003, acc 0.94
2016-09-06T20:54:50.525610: step 1610, loss 0.0559365, acc 1
2016-09-06T20:54:51.221353: step 1611, loss 0.157828, acc 0.94
2016-09-06T20:54:51.907044: step 1612, loss 0.0188409, acc 1
2016-09-06T20:54:52.618018: step 1613, loss 0.106964, acc 0.94
2016-09-06T20:54:53.297226: step 1614, loss 0.0661546, acc 0.96
2016-09-06T20:54:53.982913: step 1615, loss 0.0289674, acc 1
2016-09-06T20:54:54.693434: step 1616, loss 0.241451, acc 0.94
2016-09-06T20:54:55.376546: step 1617, loss 0.0753292, acc 0.96
2016-09-06T20:54:56.093314: step 1618, loss 0.033311, acc 1
2016-09-06T20:54:56.798360: step 1619, loss 0.0542946, acc 0.96
2016-09-06T20:54:57.527153: step 1620, loss 0.0433259, acc 0.98
2016-09-06T20:54:58.210056: step 1621, loss 0.0395018, acc 0.98
2016-09-06T20:54:58.909728: step 1622, loss 0.0434832, acc 0.98
2016-09-06T20:54:59.587425: step 1623, loss 0.067892, acc 0.96
2016-09-06T20:55:00.275930: step 1624, loss 0.0541639, acc 0.98
2016-09-06T20:55:00.945186: step 1625, loss 0.0978389, acc 0.96
2016-09-06T20:55:01.616131: step 1626, loss 0.0562215, acc 0.98
2016-09-06T20:55:02.316422: step 1627, loss 0.0680001, acc 0.98
2016-09-06T20:55:03.011083: step 1628, loss 0.0526596, acc 0.98
2016-09-06T20:55:03.691716: step 1629, loss 0.117987, acc 0.94
2016-09-06T20:55:04.360242: step 1630, loss 0.121015, acc 0.96
2016-09-06T20:55:05.030201: step 1631, loss 0.105805, acc 0.96
2016-09-06T20:55:05.724772: step 1632, loss 0.0663031, acc 0.94
2016-09-06T20:55:06.396290: step 1633, loss 0.0288104, acc 1
2016-09-06T20:55:07.099613: step 1634, loss 0.0624371, acc 0.98
2016-09-06T20:55:07.792842: step 1635, loss 0.123649, acc 0.94
2016-09-06T20:55:08.478741: step 1636, loss 0.0283919, acc 1
2016-09-06T20:55:09.173557: step 1637, loss 0.180712, acc 0.94
2016-09-06T20:55:09.863509: step 1638, loss 0.0210687, acc 1
2016-09-06T20:55:10.555320: step 1639, loss 0.0449054, acc 0.96
2016-09-06T20:55:11.210434: step 1640, loss 0.0901799, acc 0.96
2016-09-06T20:55:11.919284: step 1641, loss 0.0221974, acc 1
2016-09-06T20:55:12.597144: step 1642, loss 0.0338372, acc 0.98
2016-09-06T20:55:13.270698: step 1643, loss 0.0173616, acc 1
2016-09-06T20:55:13.962248: step 1644, loss 0.0264855, acc 0.98
2016-09-06T20:55:14.650665: step 1645, loss 0.0560886, acc 0.98
2016-09-06T20:55:15.342092: step 1646, loss 0.0535397, acc 0.94
2016-09-06T20:55:16.017041: step 1647, loss 0.03793, acc 0.98
2016-09-06T20:55:16.702880: step 1648, loss 0.0786567, acc 0.94
2016-09-06T20:55:17.373459: step 1649, loss 0.109025, acc 0.94
2016-09-06T20:55:18.038735: step 1650, loss 0.0606287, acc 0.98
2016-09-06T20:55:18.732984: step 1651, loss 0.0312041, acc 1
2016-09-06T20:55:19.413451: step 1652, loss 0.0482402, acc 0.98
2016-09-06T20:55:20.118050: step 1653, loss 0.141507, acc 0.96
2016-09-06T20:55:20.792162: step 1654, loss 0.0479615, acc 0.96
2016-09-06T20:55:21.489619: step 1655, loss 0.0351588, acc 0.98
2016-09-06T20:55:22.155332: step 1656, loss 0.0277129, acc 1
2016-09-06T20:55:22.863731: step 1657, loss 0.0446868, acc 1
2016-09-06T20:55:23.558467: step 1658, loss 0.044377, acc 0.98
2016-09-06T20:55:24.266840: step 1659, loss 0.116831, acc 0.96
2016-09-06T20:55:24.947458: step 1660, loss 0.0559331, acc 0.96
2016-09-06T20:55:25.621027: step 1661, loss 0.0837141, acc 0.96
2016-09-06T20:55:26.308676: step 1662, loss 0.0332803, acc 0.98
2016-09-06T20:55:26.998527: step 1663, loss 0.133099, acc 0.92
2016-09-06T20:55:27.677896: step 1664, loss 0.124186, acc 0.94
2016-09-06T20:55:28.369100: step 1665, loss 0.0126384, acc 1
2016-09-06T20:55:29.069555: step 1666, loss 0.0599524, acc 0.98
2016-09-06T20:55:29.749491: step 1667, loss 0.0612896, acc 1
2016-09-06T20:55:30.417686: step 1668, loss 0.0862407, acc 0.94
2016-09-06T20:55:31.120966: step 1669, loss 0.195492, acc 0.88
2016-09-06T20:55:31.808076: step 1670, loss 0.282091, acc 0.96
2016-09-06T20:55:32.498974: step 1671, loss 0.0759242, acc 0.98
2016-09-06T20:55:33.194667: step 1672, loss 0.0368726, acc 0.98
2016-09-06T20:55:33.874427: step 1673, loss 0.0696729, acc 0.98
2016-09-06T20:55:34.527557: step 1674, loss 0.0464322, acc 0.98
2016-09-06T20:55:35.193031: step 1675, loss 0.0679325, acc 0.98
2016-09-06T20:55:35.898409: step 1676, loss 0.0516091, acc 0.96
2016-09-06T20:55:36.570626: step 1677, loss 0.0643529, acc 0.98
2016-09-06T20:55:37.235166: step 1678, loss 0.0426106, acc 0.96
2016-09-06T20:55:37.921217: step 1679, loss 0.0351855, acc 1
2016-09-06T20:55:38.593140: step 1680, loss 0.0337892, acc 1
2016-09-06T20:55:39.285970: step 1681, loss 0.114174, acc 0.96
2016-09-06T20:55:39.983336: step 1682, loss 0.0231493, acc 1
2016-09-06T20:55:40.687788: step 1683, loss 0.061505, acc 0.98
2016-09-06T20:55:41.360370: step 1684, loss 0.19659, acc 0.92
2016-09-06T20:55:42.037315: step 1685, loss 0.0424274, acc 0.98
2016-09-06T20:55:42.710232: step 1686, loss 0.0707756, acc 0.96
2016-09-06T20:55:43.398718: step 1687, loss 0.057388, acc 0.96
2016-09-06T20:55:44.099557: step 1688, loss 0.115216, acc 0.94
2016-09-06T20:55:44.784331: step 1689, loss 0.100494, acc 0.96
2016-09-06T20:55:45.509009: step 1690, loss 0.110364, acc 0.96
2016-09-06T20:55:46.181881: step 1691, loss 0.173217, acc 0.94
2016-09-06T20:55:46.862267: step 1692, loss 0.144552, acc 0.94
2016-09-06T20:55:47.540392: step 1693, loss 0.077021, acc 0.98
2016-09-06T20:55:48.218362: step 1694, loss 0.141435, acc 0.94
2016-09-06T20:55:48.902606: step 1695, loss 0.0812497, acc 0.96
2016-09-06T20:55:49.593651: step 1696, loss 0.045923, acc 1
2016-09-06T20:55:50.319841: step 1697, loss 0.0449449, acc 0.96
2016-09-06T20:55:50.988028: step 1698, loss 0.0486925, acc 0.98
2016-09-06T20:55:51.663054: step 1699, loss 0.126419, acc 0.94
2016-09-06T20:55:52.358935: step 1700, loss 0.0314477, acc 0.98

Evaluation:
2016-09-06T20:55:55.489256: step 1700, loss 1.00307, acc 0.776735

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-1700

2016-09-06T20:55:57.147714: step 1701, loss 0.22326, acc 0.94
2016-09-06T20:55:57.827582: step 1702, loss 0.0810812, acc 0.98
2016-09-06T20:55:58.573188: step 1703, loss 0.0407095, acc 0.98
2016-09-06T20:55:59.255442: step 1704, loss 0.0911751, acc 0.96
2016-09-06T20:55:59.979304: step 1705, loss 0.0424753, acc 0.98
2016-09-06T20:56:00.688650: step 1706, loss 0.072886, acc 0.96
2016-09-06T20:56:01.379006: step 1707, loss 0.0593148, acc 0.96
2016-09-06T20:56:02.047870: step 1708, loss 0.0829872, acc 0.94
2016-09-06T20:56:02.724443: step 1709, loss 0.0312214, acc 0.98
2016-09-06T20:56:03.420355: step 1710, loss 0.150012, acc 0.96
2016-09-06T20:56:04.075537: step 1711, loss 0.138803, acc 0.98
2016-09-06T20:56:04.778323: step 1712, loss 0.0606788, acc 0.96
2016-09-06T20:56:05.458451: step 1713, loss 0.108162, acc 0.94
2016-09-06T20:56:06.148718: step 1714, loss 0.0920388, acc 0.96
2016-09-06T20:56:06.838712: step 1715, loss 0.0768519, acc 0.96
2016-09-06T20:56:07.532517: step 1716, loss 0.0782859, acc 0.96
2016-09-06T20:56:08.220772: step 1717, loss 0.137624, acc 0.98
2016-09-06T20:56:08.882365: step 1718, loss 0.00771021, acc 1
2016-09-06T20:56:09.603369: step 1719, loss 0.0491414, acc 0.98
2016-09-06T20:56:10.299113: step 1720, loss 0.0234838, acc 1
2016-09-06T20:56:11.012147: step 1721, loss 0.0793872, acc 0.98
2016-09-06T20:56:11.702034: step 1722, loss 0.0736525, acc 0.96
2016-09-06T20:56:12.385249: step 1723, loss 0.175222, acc 0.9
2016-09-06T20:56:13.071380: step 1724, loss 0.0187857, acc 1
2016-09-06T20:56:13.722610: step 1725, loss 0.124658, acc 0.94
2016-09-06T20:56:14.430330: step 1726, loss 0.104872, acc 0.94
2016-09-06T20:56:15.119115: step 1727, loss 0.0161751, acc 1
2016-09-06T20:56:15.760915: step 1728, loss 0.0168264, acc 1
2016-09-06T20:56:16.454183: step 1729, loss 0.0524826, acc 0.96
2016-09-06T20:56:17.141088: step 1730, loss 0.0533443, acc 0.98
2016-09-06T20:56:17.803632: step 1731, loss 0.194561, acc 0.9
2016-09-06T20:56:18.486271: step 1732, loss 0.0309727, acc 1
2016-09-06T20:56:19.184958: step 1733, loss 0.0515937, acc 0.98
2016-09-06T20:56:19.858204: step 1734, loss 0.0280849, acc 1
2016-09-06T20:56:20.556789: step 1735, loss 0.187701, acc 0.94
2016-09-06T20:56:21.249661: step 1736, loss 0.0182801, acc 0.98
2016-09-06T20:56:21.959458: step 1737, loss 0.0261726, acc 0.98
2016-09-06T20:56:22.651289: step 1738, loss 0.0438894, acc 0.98
2016-09-06T20:56:23.309776: step 1739, loss 0.088844, acc 0.98
2016-09-06T20:56:24.018648: step 1740, loss 0.125322, acc 0.96
2016-09-06T20:56:24.701154: step 1741, loss 0.0349943, acc 0.98
2016-09-06T20:56:25.369657: step 1742, loss 0.0955566, acc 0.98
2016-09-06T20:56:26.051881: step 1743, loss 0.0245488, acc 1
2016-09-06T20:56:26.747770: step 1744, loss 0.0326208, acc 0.98
2016-09-06T20:56:27.451344: step 1745, loss 0.0446931, acc 0.98
2016-09-06T20:56:28.126472: step 1746, loss 0.0183053, acc 1
2016-09-06T20:56:28.838048: step 1747, loss 0.0819761, acc 0.96
2016-09-06T20:56:29.529601: step 1748, loss 0.0450511, acc 1
2016-09-06T20:56:30.212835: step 1749, loss 0.114656, acc 0.94
2016-09-06T20:56:30.895530: step 1750, loss 0.122986, acc 0.92
2016-09-06T20:56:31.580261: step 1751, loss 0.0122527, acc 1
2016-09-06T20:56:32.254719: step 1752, loss 0.0352521, acc 0.98
2016-09-06T20:56:32.919174: step 1753, loss 0.0346918, acc 0.98
2016-09-06T20:56:33.613702: step 1754, loss 0.0434135, acc 0.98
2016-09-06T20:56:34.290571: step 1755, loss 0.0669383, acc 0.98
2016-09-06T20:56:34.968246: step 1756, loss 0.0387454, acc 0.98
2016-09-06T20:56:35.657853: step 1757, loss 0.0653924, acc 0.96
2016-09-06T20:56:36.351288: step 1758, loss 0.0979555, acc 0.96
2016-09-06T20:56:37.042337: step 1759, loss 0.0283082, acc 0.98
2016-09-06T20:56:37.737081: step 1760, loss 0.10285, acc 0.92
2016-09-06T20:56:38.451561: step 1761, loss 0.0303215, acc 1
2016-09-06T20:56:39.150788: step 1762, loss 0.00844078, acc 1
2016-09-06T20:56:39.820259: step 1763, loss 0.0392305, acc 0.98
2016-09-06T20:56:40.495934: step 1764, loss 0.0502995, acc 0.96
2016-09-06T20:56:41.190005: step 1765, loss 0.0135183, acc 1
2016-09-06T20:56:41.885941: step 1766, loss 0.0429498, acc 0.98
2016-09-06T20:56:42.537589: step 1767, loss 0.019338, acc 1
2016-09-06T20:56:43.225929: step 1768, loss 0.0401565, acc 0.98
2016-09-06T20:56:43.895102: step 1769, loss 0.0413891, acc 0.98
2016-09-06T20:56:44.578611: step 1770, loss 0.0403392, acc 0.96
2016-09-06T20:56:45.272229: step 1771, loss 0.128257, acc 0.98
2016-09-06T20:56:45.970350: step 1772, loss 0.0509216, acc 1
2016-09-06T20:56:46.646257: step 1773, loss 0.0687579, acc 0.96
2016-09-06T20:56:47.329503: step 1774, loss 0.0549786, acc 0.96
2016-09-06T20:56:48.041767: step 1775, loss 0.00448969, acc 1
2016-09-06T20:56:48.717821: step 1776, loss 0.0131434, acc 1
2016-09-06T20:56:49.376822: step 1777, loss 0.158654, acc 0.92
2016-09-06T20:56:50.078620: step 1778, loss 0.0602886, acc 0.96
2016-09-06T20:56:50.784386: step 1779, loss 0.0348796, acc 0.98
2016-09-06T20:56:51.468767: step 1780, loss 0.00370566, acc 1
2016-09-06T20:56:52.154600: step 1781, loss 0.130394, acc 0.96
2016-09-06T20:56:52.862182: step 1782, loss 0.0458397, acc 0.96
2016-09-06T20:56:53.530773: step 1783, loss 0.0415022, acc 0.98
2016-09-06T20:56:54.207975: step 1784, loss 0.0513987, acc 0.98
2016-09-06T20:56:54.902713: step 1785, loss 0.034021, acc 0.98
2016-09-06T20:56:55.620935: step 1786, loss 0.153241, acc 0.96
2016-09-06T20:56:56.302891: step 1787, loss 0.0656756, acc 0.94
2016-09-06T20:56:56.979195: step 1788, loss 0.0380343, acc 1
2016-09-06T20:56:57.710925: step 1789, loss 0.0564678, acc 0.98
2016-09-06T20:56:58.373213: step 1790, loss 0.0335744, acc 0.98
2016-09-06T20:56:59.069303: step 1791, loss 0.161811, acc 0.92
2016-09-06T20:56:59.755283: step 1792, loss 0.145808, acc 0.96
2016-09-06T20:57:00.499156: step 1793, loss 0.0531765, acc 0.96
2016-09-06T20:57:01.216277: step 1794, loss 0.0375604, acc 0.98
2016-09-06T20:57:01.887580: step 1795, loss 0.0664008, acc 0.96
2016-09-06T20:57:02.593488: step 1796, loss 0.070708, acc 0.96
2016-09-06T20:57:03.266876: step 1797, loss 0.176387, acc 0.9
2016-09-06T20:57:03.960748: step 1798, loss 0.0887138, acc 0.94
2016-09-06T20:57:04.642860: step 1799, loss 0.102167, acc 0.96
2016-09-06T20:57:05.328913: step 1800, loss 0.0744635, acc 0.98

Evaluation:
2016-09-06T20:57:08.479675: step 1800, loss 1.18841, acc 0.772045

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-1800

2016-09-06T20:57:10.080997: step 1801, loss 0.138672, acc 0.96
2016-09-06T20:57:10.770942: step 1802, loss 0.0278016, acc 1
2016-09-06T20:57:11.438003: step 1803, loss 0.02818, acc 1
2016-09-06T20:57:12.139304: step 1804, loss 0.0547612, acc 0.96
2016-09-06T20:57:12.832808: step 1805, loss 0.0601542, acc 1
2016-09-06T20:57:13.526259: step 1806, loss 0.106002, acc 0.98
2016-09-06T20:57:14.210501: step 1807, loss 0.0571604, acc 0.98
2016-09-06T20:57:14.904757: step 1808, loss 0.0393925, acc 1
2016-09-06T20:57:15.590423: step 1809, loss 0.0707041, acc 0.96
2016-09-06T20:57:16.257135: step 1810, loss 0.0125732, acc 1
2016-09-06T20:57:16.949642: step 1811, loss 0.0485989, acc 1
2016-09-06T20:57:17.645976: step 1812, loss 0.0271491, acc 1
2016-09-06T20:57:18.338223: step 1813, loss 0.0400457, acc 0.98
2016-09-06T20:57:19.034663: step 1814, loss 0.0612692, acc 0.94
2016-09-06T20:57:19.739371: step 1815, loss 0.0974442, acc 0.96
2016-09-06T20:57:20.453678: step 1816, loss 0.105307, acc 0.92
2016-09-06T20:57:21.134739: step 1817, loss 0.0690236, acc 0.96
2016-09-06T20:57:21.836811: step 1818, loss 0.0771127, acc 0.98
2016-09-06T20:57:22.504773: step 1819, loss 0.106448, acc 0.96
2016-09-06T20:57:23.208571: step 1820, loss 0.114377, acc 0.92
2016-09-06T20:57:23.891199: step 1821, loss 0.0480214, acc 0.98
2016-09-06T20:57:24.614713: step 1822, loss 0.0973014, acc 0.96
2016-09-06T20:57:25.329879: step 1823, loss 0.0389293, acc 1
2016-09-06T20:57:26.004937: step 1824, loss 0.0386678, acc 1
2016-09-06T20:57:26.673998: step 1825, loss 0.0230002, acc 0.98
2016-09-06T20:57:27.366937: step 1826, loss 0.0856996, acc 0.94
2016-09-06T20:57:28.076747: step 1827, loss 0.101836, acc 0.94
2016-09-06T20:57:28.769535: step 1828, loss 0.0624464, acc 0.96
2016-09-06T20:57:29.432444: step 1829, loss 0.0663847, acc 0.96
2016-09-06T20:57:30.151361: step 1830, loss 0.108159, acc 0.96
2016-09-06T20:57:30.806952: step 1831, loss 0.0916436, acc 0.94
2016-09-06T20:57:31.492980: step 1832, loss 0.0818605, acc 0.96
2016-09-06T20:57:32.174049: step 1833, loss 0.0397382, acc 0.98
2016-09-06T20:57:32.887372: step 1834, loss 0.123184, acc 0.94
2016-09-06T20:57:33.598390: step 1835, loss 0.0530342, acc 0.98
2016-09-06T20:57:34.260371: step 1836, loss 0.240512, acc 0.94
2016-09-06T20:57:34.949139: step 1837, loss 0.0695096, acc 0.94
2016-09-06T20:57:35.628234: step 1838, loss 0.0457091, acc 0.98
2016-09-06T20:57:36.313262: step 1839, loss 0.0763979, acc 0.96
2016-09-06T20:57:36.998384: step 1840, loss 0.0352642, acc 0.98
2016-09-06T20:57:37.678154: step 1841, loss 0.0374613, acc 0.96
2016-09-06T20:57:38.363519: step 1842, loss 0.0343123, acc 0.98
2016-09-06T20:57:39.048074: step 1843, loss 0.140511, acc 0.92
2016-09-06T20:57:39.752299: step 1844, loss 0.0486144, acc 0.98
2016-09-06T20:57:40.432800: step 1845, loss 0.00933388, acc 1
2016-09-06T20:57:41.120668: step 1846, loss 0.0649373, acc 0.98
2016-09-06T20:57:41.795624: step 1847, loss 0.0439702, acc 0.98
2016-09-06T20:57:42.512476: step 1848, loss 0.0840541, acc 0.94
2016-09-06T20:57:43.204740: step 1849, loss 0.0679469, acc 0.94
2016-09-06T20:57:43.875735: step 1850, loss 0.00621732, acc 1
2016-09-06T20:57:44.570948: step 1851, loss 0.0323225, acc 1
2016-09-06T20:57:45.245013: step 1852, loss 0.104947, acc 0.94
2016-09-06T20:57:45.927101: step 1853, loss 0.0163779, acc 1
2016-09-06T20:57:46.642101: step 1854, loss 0.0323992, acc 1
2016-09-06T20:57:47.322975: step 1855, loss 0.0518706, acc 0.98
2016-09-06T20:57:48.020870: step 1856, loss 0.0891735, acc 0.96
2016-09-06T20:57:48.679345: step 1857, loss 0.0671914, acc 0.98
2016-09-06T20:57:49.377925: step 1858, loss 0.0590395, acc 0.96
2016-09-06T20:57:50.045549: step 1859, loss 0.0713551, acc 0.96
2016-09-06T20:57:50.717582: step 1860, loss 0.224038, acc 0.92
2016-09-06T20:57:51.410537: step 1861, loss 0.0974335, acc 0.92
2016-09-06T20:57:52.086318: step 1862, loss 0.0630441, acc 0.98
2016-09-06T20:57:52.763128: step 1863, loss 0.0368059, acc 0.98
2016-09-06T20:57:53.438022: step 1864, loss 0.0398704, acc 0.98
2016-09-06T20:57:54.148370: step 1865, loss 0.0511576, acc 0.98
2016-09-06T20:57:54.823176: step 1866, loss 0.00550973, acc 1
2016-09-06T20:57:55.510816: step 1867, loss 0.114206, acc 0.98
2016-09-06T20:57:56.213050: step 1868, loss 0.0341546, acc 0.98
2016-09-06T20:57:56.924638: step 1869, loss 0.0202961, acc 1
2016-09-06T20:57:57.609379: step 1870, loss 0.0442869, acc 0.98
2016-09-06T20:57:58.300860: step 1871, loss 0.107531, acc 0.94
2016-09-06T20:57:59.003669: step 1872, loss 0.0703261, acc 0.98
2016-09-06T20:57:59.692443: step 1873, loss 0.0959709, acc 0.94
2016-09-06T20:58:00.410337: step 1874, loss 0.0655061, acc 0.96
2016-09-06T20:58:01.083664: step 1875, loss 0.0392755, acc 0.98
2016-09-06T20:58:01.762725: step 1876, loss 0.0356946, acc 1
2016-09-06T20:58:02.450535: step 1877, loss 0.0428028, acc 0.98
2016-09-06T20:58:03.111917: step 1878, loss 0.04255, acc 0.96
2016-09-06T20:58:03.822511: step 1879, loss 0.0943761, acc 0.94
2016-09-06T20:58:04.494509: step 1880, loss 0.059003, acc 0.96
2016-09-06T20:58:05.199621: step 1881, loss 0.0793402, acc 0.96
2016-09-06T20:58:05.890908: step 1882, loss 0.123699, acc 0.94
2016-09-06T20:58:06.587989: step 1883, loss 0.0312129, acc 0.98
2016-09-06T20:58:07.267437: step 1884, loss 0.053075, acc 0.98
2016-09-06T20:58:07.920541: step 1885, loss 0.0122017, acc 1
2016-09-06T20:58:08.643678: step 1886, loss 0.073539, acc 0.94
2016-09-06T20:58:09.337580: step 1887, loss 0.126455, acc 0.96
2016-09-06T20:58:10.012500: step 1888, loss 0.0559105, acc 0.98
2016-09-06T20:58:10.699247: step 1889, loss 0.0412163, acc 0.98
2016-09-06T20:58:11.378562: step 1890, loss 0.057742, acc 0.96
2016-09-06T20:58:12.080286: step 1891, loss 0.0192871, acc 1
2016-09-06T20:58:12.749004: step 1892, loss 0.0201527, acc 1
2016-09-06T20:58:13.443630: step 1893, loss 0.142782, acc 0.92
2016-09-06T20:58:14.148599: step 1894, loss 0.0560548, acc 0.98
2016-09-06T20:58:14.843338: step 1895, loss 0.028801, acc 1
2016-09-06T20:58:15.553687: step 1896, loss 0.128895, acc 0.94
2016-09-06T20:58:16.240532: step 1897, loss 0.0423946, acc 0.96
2016-09-06T20:58:16.952092: step 1898, loss 0.0235421, acc 1
2016-09-06T20:58:17.642073: step 1899, loss 0.0890398, acc 0.94
2016-09-06T20:58:18.348221: step 1900, loss 0.0312703, acc 1

Evaluation:
2016-09-06T20:58:21.478694: step 1900, loss 1.36161, acc 0.761726

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-1900

2016-09-06T20:58:23.184579: step 1901, loss 0.121537, acc 0.94
2016-09-06T20:58:23.883798: step 1902, loss 0.124237, acc 0.94
2016-09-06T20:58:24.583505: step 1903, loss 0.0461038, acc 0.96
2016-09-06T20:58:25.265821: step 1904, loss 0.0433675, acc 0.96
2016-09-06T20:58:25.947966: step 1905, loss 0.149167, acc 0.96
2016-09-06T20:58:26.637888: step 1906, loss 0.0603808, acc 0.96
2016-09-06T20:58:27.306241: step 1907, loss 0.105183, acc 0.98
2016-09-06T20:58:28.002276: step 1908, loss 0.0532699, acc 0.98
2016-09-06T20:58:28.684121: step 1909, loss 0.0473878, acc 1
2016-09-06T20:58:29.385950: step 1910, loss 0.0131664, acc 1
2016-09-06T20:58:30.069249: step 1911, loss 0.015415, acc 1
2016-09-06T20:58:30.740210: step 1912, loss 0.096944, acc 0.94
2016-09-06T20:58:31.434620: step 1913, loss 0.0387339, acc 0.98
2016-09-06T20:58:32.113134: step 1914, loss 0.136404, acc 0.94
2016-09-06T20:58:32.791805: step 1915, loss 0.198413, acc 0.88
2016-09-06T20:58:33.472563: step 1916, loss 0.0248308, acc 0.98
2016-09-06T20:58:34.160898: step 1917, loss 0.173581, acc 0.94
2016-09-06T20:58:34.846174: step 1918, loss 0.0504714, acc 0.98
2016-09-06T20:58:35.541090: step 1919, loss 0.0541226, acc 0.98
2016-09-06T20:58:36.203249: step 1920, loss 0.0936543, acc 0.954545
2016-09-06T20:58:36.867777: step 1921, loss 0.0526456, acc 0.98
2016-09-06T20:58:37.583356: step 1922, loss 0.0296437, acc 1
2016-09-06T20:58:38.285656: step 1923, loss 0.122073, acc 0.92
2016-09-06T20:58:38.971372: step 1924, loss 0.0627396, acc 0.96
2016-09-06T20:58:39.653326: step 1925, loss 0.0478562, acc 0.96
2016-09-06T20:58:40.336833: step 1926, loss 0.0569241, acc 0.98
2016-09-06T20:58:41.042397: step 1927, loss 0.0609237, acc 0.98
2016-09-06T20:58:41.732942: step 1928, loss 0.0354808, acc 0.98
2016-09-06T20:58:42.427256: step 1929, loss 0.0284531, acc 1
2016-09-06T20:58:43.140797: step 1930, loss 0.0561357, acc 0.98
2016-09-06T20:58:43.845015: step 1931, loss 0.0149837, acc 1
2016-09-06T20:58:44.560574: step 1932, loss 0.116327, acc 0.96
2016-09-06T20:58:45.227454: step 1933, loss 0.0319381, acc 1
2016-09-06T20:58:45.928952: step 1934, loss 0.0800276, acc 0.94
2016-09-06T20:58:46.595539: step 1935, loss 0.0287207, acc 0.98
2016-09-06T20:58:47.267901: step 1936, loss 0.0414907, acc 0.96
2016-09-06T20:58:47.955487: step 1937, loss 0.0674471, acc 0.98
2016-09-06T20:58:48.640201: step 1938, loss 0.0327541, acc 0.98
2016-09-06T20:58:49.318764: step 1939, loss 0.181708, acc 0.84
2016-09-06T20:58:49.967940: step 1940, loss 0.0532783, acc 0.96
2016-09-06T20:58:50.667370: step 1941, loss 0.0731256, acc 0.98
2016-09-06T20:58:51.347757: step 1942, loss 0.0509692, acc 0.98
2016-09-06T20:58:52.043566: step 1943, loss 0.0265247, acc 0.98
2016-09-06T20:58:52.742502: step 1944, loss 0.0366394, acc 1
2016-09-06T20:58:53.420984: step 1945, loss 0.0510373, acc 0.98
2016-09-06T20:58:54.094451: step 1946, loss 0.00941999, acc 1
2016-09-06T20:58:54.788195: step 1947, loss 0.0707362, acc 0.96
2016-09-06T20:58:55.499462: step 1948, loss 0.00819598, acc 1
2016-09-06T20:58:56.206902: step 1949, loss 0.0619039, acc 0.98
2016-09-06T20:58:56.901803: step 1950, loss 0.00750401, acc 1
2016-09-06T20:58:57.574322: step 1951, loss 0.0344903, acc 1
2016-09-06T20:58:58.266899: step 1952, loss 0.0468058, acc 0.98
2016-09-06T20:58:58.936692: step 1953, loss 0.0267581, acc 0.98
2016-09-06T20:58:59.599139: step 1954, loss 0.0334479, acc 0.98
2016-09-06T20:59:00.319368: step 1955, loss 0.0508654, acc 0.98
2016-09-06T20:59:01.002177: step 1956, loss 0.034782, acc 1
2016-09-06T20:59:01.678492: step 1957, loss 0.014843, acc 1
2016-09-06T20:59:02.369365: step 1958, loss 0.0441001, acc 0.96
2016-09-06T20:59:03.068887: step 1959, loss 0.089465, acc 0.92
2016-09-06T20:59:03.752895: step 1960, loss 0.0247091, acc 1
2016-09-06T20:59:04.427666: step 1961, loss 0.0370732, acc 1
2016-09-06T20:59:05.137110: step 1962, loss 0.113698, acc 0.94
2016-09-06T20:59:05.788300: step 1963, loss 0.0504915, acc 0.98
2016-09-06T20:59:06.471462: step 1964, loss 0.00198431, acc 1
2016-09-06T20:59:07.152988: step 1965, loss 0.120307, acc 0.96
2016-09-06T20:59:07.856970: step 1966, loss 0.0755535, acc 0.96
2016-09-06T20:59:08.524229: step 1967, loss 0.0119018, acc 1
2016-09-06T20:59:09.210042: step 1968, loss 0.0728655, acc 0.96
2016-09-06T20:59:09.915868: step 1969, loss 0.0387433, acc 0.98
2016-09-06T20:59:10.566833: step 1970, loss 0.0597645, acc 0.98
2016-09-06T20:59:11.223846: step 1971, loss 0.166078, acc 0.98
2016-09-06T20:59:11.914082: step 1972, loss 0.0830831, acc 0.98
2016-09-06T20:59:12.592157: step 1973, loss 0.0177228, acc 1
2016-09-06T20:59:13.282124: step 1974, loss 0.0766319, acc 0.94
2016-09-06T20:59:13.995175: step 1975, loss 0.148795, acc 0.94
2016-09-06T20:59:14.702262: step 1976, loss 0.0301531, acc 1
2016-09-06T20:59:15.384143: step 1977, loss 0.0384048, acc 0.98
2016-09-06T20:59:16.067043: step 1978, loss 0.118326, acc 0.94
2016-09-06T20:59:16.744867: step 1979, loss 0.0731015, acc 0.94
2016-09-06T20:59:17.438703: step 1980, loss 0.0132791, acc 1
2016-09-06T20:59:18.134052: step 1981, loss 0.0163532, acc 1
2016-09-06T20:59:18.830370: step 1982, loss 0.0638395, acc 0.98
2016-09-06T20:59:19.546483: step 1983, loss 0.0112951, acc 1
2016-09-06T20:59:20.219017: step 1984, loss 0.00482471, acc 1
2016-09-06T20:59:20.892339: step 1985, loss 0.0360027, acc 0.98
2016-09-06T20:59:21.571565: step 1986, loss 0.0115413, acc 1
2016-09-06T20:59:22.260690: step 1987, loss 0.0207667, acc 0.98
2016-09-06T20:59:22.937385: step 1988, loss 0.06336, acc 0.96
2016-09-06T20:59:23.643490: step 1989, loss 0.045804, acc 1
2016-09-06T20:59:24.352572: step 1990, loss 0.0707187, acc 0.96
2016-09-06T20:59:25.015451: step 1991, loss 0.10505, acc 0.96
2016-09-06T20:59:25.684688: step 1992, loss 0.0602682, acc 0.94
2016-09-06T20:59:26.378748: step 1993, loss 0.0173483, acc 1
2016-09-06T20:59:27.076101: step 1994, loss 0.0797557, acc 0.96
2016-09-06T20:59:27.778229: step 1995, loss 0.0526969, acc 1
2016-09-06T20:59:28.447872: step 1996, loss 0.0651605, acc 0.96
2016-09-06T20:59:29.160683: step 1997, loss 0.0400829, acc 0.98
2016-09-06T20:59:29.827658: step 1998, loss 0.034994, acc 1
2016-09-06T20:59:30.506527: step 1999, loss 0.12478, acc 0.94
2016-09-06T20:59:31.197046: step 2000, loss 0.118228, acc 0.96

Evaluation:
2016-09-06T20:59:34.322114: step 2000, loss 1.57107, acc 0.772983

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-2000

2016-09-06T20:59:35.976900: step 2001, loss 0.0901692, acc 0.98
2016-09-06T20:59:36.670948: step 2002, loss 0.0903533, acc 0.96
2016-09-06T20:59:37.380394: step 2003, loss 0.0655706, acc 0.96
2016-09-06T20:59:38.062623: step 2004, loss 0.063416, acc 0.96
2016-09-06T20:59:38.776388: step 2005, loss 0.0341248, acc 1
2016-09-06T20:59:39.434336: step 2006, loss 0.0233204, acc 1
2016-09-06T20:59:40.096554: step 2007, loss 0.0425788, acc 1
2016-09-06T20:59:40.774419: step 2008, loss 0.0108532, acc 1
2016-09-06T20:59:41.436038: step 2009, loss 0.0831804, acc 0.98
2016-09-06T20:59:42.132200: step 2010, loss 0.0554179, acc 0.98
2016-09-06T20:59:42.814983: step 2011, loss 0.0636431, acc 0.96
2016-09-06T20:59:43.535813: step 2012, loss 0.0638839, acc 0.98
2016-09-06T20:59:44.237702: step 2013, loss 0.0670087, acc 0.96
2016-09-06T20:59:44.924836: step 2014, loss 0.0707646, acc 0.94
2016-09-06T20:59:45.610863: step 2015, loss 0.146217, acc 0.96
2016-09-06T20:59:46.301237: step 2016, loss 0.0784057, acc 0.96
2016-09-06T20:59:46.978894: step 2017, loss 0.0961353, acc 0.98
2016-09-06T20:59:47.676515: step 2018, loss 0.0102227, acc 1
2016-09-06T20:59:48.398113: step 2019, loss 0.0203401, acc 0.98
2016-09-06T20:59:49.064462: step 2020, loss 0.0948058, acc 0.92
2016-09-06T20:59:49.746347: step 2021, loss 0.117239, acc 0.92
2016-09-06T20:59:50.424916: step 2022, loss 0.0152402, acc 1
2016-09-06T20:59:51.102811: step 2023, loss 0.210694, acc 0.96
2016-09-06T20:59:51.797328: step 2024, loss 0.0490761, acc 0.98
2016-09-06T20:59:52.489549: step 2025, loss 0.0702646, acc 0.98
2016-09-06T20:59:53.176220: step 2026, loss 0.0395051, acc 1
2016-09-06T20:59:53.853851: step 2027, loss 0.0786307, acc 0.98
2016-09-06T20:59:54.526964: step 2028, loss 0.0635671, acc 0.96
2016-09-06T20:59:55.221289: step 2029, loss 0.0459842, acc 1
2016-09-06T20:59:55.909273: step 2030, loss 0.0902579, acc 0.94
2016-09-06T20:59:56.613340: step 2031, loss 0.0666769, acc 0.98
2016-09-06T20:59:57.304866: step 2032, loss 0.0233127, acc 1
2016-09-06T20:59:58.008713: step 2033, loss 0.0300282, acc 0.98
2016-09-06T20:59:58.680736: step 2034, loss 0.0576359, acc 0.98
2016-09-06T20:59:59.337996: step 2035, loss 0.0405864, acc 0.98
2016-09-06T21:00:00.015133: step 2036, loss 0.0294448, acc 0.98
2016-09-06T21:00:00.764010: step 2037, loss 0.0918847, acc 0.94
2016-09-06T21:00:01.439628: step 2038, loss 0.0277932, acc 1
2016-09-06T21:00:02.113683: step 2039, loss 0.0504039, acc 0.98
2016-09-06T21:00:02.812488: step 2040, loss 0.108288, acc 0.96
2016-09-06T21:00:03.467297: step 2041, loss 0.0733842, acc 0.96
2016-09-06T21:00:04.162867: step 2042, loss 0.023921, acc 0.98
2016-09-06T21:00:04.868090: step 2043, loss 0.014431, acc 1
2016-09-06T21:00:05.537208: step 2044, loss 0.0565373, acc 0.98
2016-09-06T21:00:06.216950: step 2045, loss 0.0200993, acc 1
2016-09-06T21:00:06.892035: step 2046, loss 0.0586199, acc 0.98
2016-09-06T21:00:07.607829: step 2047, loss 0.0828779, acc 0.98
2016-09-06T21:00:08.277241: step 2048, loss 0.175471, acc 0.96
2016-09-06T21:00:08.973018: step 2049, loss 0.0335263, acc 0.98
2016-09-06T21:00:09.654488: step 2050, loss 0.0451251, acc 0.96
2016-09-06T21:00:10.320777: step 2051, loss 0.134398, acc 0.94
2016-09-06T21:00:11.020169: step 2052, loss 0.0241336, acc 1
2016-09-06T21:00:11.697682: step 2053, loss 0.159973, acc 0.92
2016-09-06T21:00:12.386737: step 2054, loss 0.0201729, acc 1
2016-09-06T21:00:13.049068: step 2055, loss 0.0299294, acc 1
2016-09-06T21:00:13.759051: step 2056, loss 0.111392, acc 0.98
2016-09-06T21:00:14.438885: step 2057, loss 0.0082531, acc 1
2016-09-06T21:00:15.116504: step 2058, loss 0.0150998, acc 1
2016-09-06T21:00:15.803128: step 2059, loss 0.154221, acc 0.96
2016-09-06T21:00:16.472690: step 2060, loss 0.0867012, acc 0.96
2016-09-06T21:00:17.185190: step 2061, loss 0.083156, acc 0.96
2016-09-06T21:00:17.850758: step 2062, loss 0.0528381, acc 0.96
2016-09-06T21:00:18.550002: step 2063, loss 0.11072, acc 0.98
2016-09-06T21:00:19.249307: step 2064, loss 0.0343965, acc 0.98
2016-09-06T21:00:19.924721: step 2065, loss 0.157467, acc 0.94
2016-09-06T21:00:20.604026: step 2066, loss 0.0238278, acc 0.98
2016-09-06T21:00:21.292996: step 2067, loss 0.0170571, acc 1
2016-09-06T21:00:21.988935: step 2068, loss 0.09471, acc 0.92
2016-09-06T21:00:22.650098: step 2069, loss 0.119717, acc 0.96
2016-09-06T21:00:23.344607: step 2070, loss 0.026182, acc 0.98
2016-09-06T21:00:24.015640: step 2071, loss 0.0959442, acc 0.96
2016-09-06T21:00:24.700848: step 2072, loss 0.0482, acc 0.98
2016-09-06T21:00:25.403736: step 2073, loss 0.0868004, acc 0.96
2016-09-06T21:00:26.091513: step 2074, loss 0.0797433, acc 0.96
2016-09-06T21:00:26.777467: step 2075, loss 0.118314, acc 0.94
2016-09-06T21:00:27.454061: step 2076, loss 0.0248158, acc 0.98
2016-09-06T21:00:28.148189: step 2077, loss 0.0260597, acc 0.98
2016-09-06T21:00:28.811947: step 2078, loss 0.0897741, acc 0.96
2016-09-06T21:00:29.488170: step 2079, loss 0.0238163, acc 0.98
2016-09-06T21:00:30.191839: step 2080, loss 0.0531085, acc 1
2016-09-06T21:00:30.891955: step 2081, loss 0.0835132, acc 0.94
2016-09-06T21:00:31.596422: step 2082, loss 0.0130781, acc 1
2016-09-06T21:00:32.264562: step 2083, loss 0.0464497, acc 0.98
2016-09-06T21:00:32.973279: step 2084, loss 0.045478, acc 0.98
2016-09-06T21:00:33.676050: step 2085, loss 0.102426, acc 0.98
2016-09-06T21:00:34.361854: step 2086, loss 0.0529361, acc 0.98
2016-09-06T21:00:35.043734: step 2087, loss 0.0405546, acc 0.98
2016-09-06T21:00:35.715545: step 2088, loss 0.0295974, acc 0.98
2016-09-06T21:00:36.401260: step 2089, loss 0.0632762, acc 0.98
2016-09-06T21:00:37.060617: step 2090, loss 0.0806615, acc 0.96
2016-09-06T21:00:37.775216: step 2091, loss 0.0395449, acc 0.98
2016-09-06T21:00:38.459479: step 2092, loss 0.0861905, acc 0.98
2016-09-06T21:00:39.155882: step 2093, loss 0.0959044, acc 0.98
2016-09-06T21:00:39.838172: step 2094, loss 0.031107, acc 0.98
2016-09-06T21:00:40.530844: step 2095, loss 0.0471102, acc 0.98
2016-09-06T21:00:41.228708: step 2096, loss 0.0366344, acc 0.98
2016-09-06T21:00:41.891402: step 2097, loss 0.0634581, acc 0.98
2016-09-06T21:00:42.582969: step 2098, loss 0.055623, acc 0.98
2016-09-06T21:00:43.256218: step 2099, loss 0.0500815, acc 0.98
2016-09-06T21:00:43.934273: step 2100, loss 0.0689805, acc 0.94

Evaluation:
2016-09-06T21:00:47.089915: step 2100, loss 1.36623, acc 0.768293

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-2100

2016-09-06T21:00:48.772217: step 2101, loss 0.0791651, acc 0.96
2016-09-06T21:00:49.459883: step 2102, loss 0.00480476, acc 1
2016-09-06T21:00:50.138308: step 2103, loss 0.00189999, acc 1
2016-09-06T21:00:50.835628: step 2104, loss 0.0564326, acc 0.96
2016-09-06T21:00:51.485556: step 2105, loss 0.0228766, acc 0.98
2016-09-06T21:00:52.174636: step 2106, loss 0.0174257, acc 1
2016-09-06T21:00:52.834298: step 2107, loss 0.0605538, acc 0.96
2016-09-06T21:00:53.514595: step 2108, loss 0.0238167, acc 1
2016-09-06T21:00:54.217236: step 2109, loss 0.0300761, acc 0.98
2016-09-06T21:00:54.913474: step 2110, loss 0.0201995, acc 1
2016-09-06T21:00:55.614521: step 2111, loss 0.0339855, acc 0.98
2016-09-06T21:00:56.270395: step 2112, loss 0.028576, acc 1
2016-09-06T21:00:56.976967: step 2113, loss 0.0482775, acc 1
2016-09-06T21:00:57.653747: step 2114, loss 0.0287471, acc 1
2016-09-06T21:00:58.338887: step 2115, loss 0.0238204, acc 1
2016-09-06T21:00:59.034357: step 2116, loss 0.111753, acc 0.92
2016-09-06T21:00:59.711766: step 2117, loss 0.0146301, acc 1
2016-09-06T21:01:00.426215: step 2118, loss 0.0240081, acc 1
2016-09-06T21:01:01.083135: step 2119, loss 0.100167, acc 0.98
2016-09-06T21:01:01.773712: step 2120, loss 0.0830747, acc 0.98
2016-09-06T21:01:02.456354: step 2121, loss 0.0121821, acc 1
2016-09-06T21:01:03.125952: step 2122, loss 0.0613385, acc 0.98
2016-09-06T21:01:03.813311: step 2123, loss 0.0259261, acc 1
2016-09-06T21:01:04.490961: step 2124, loss 0.0544324, acc 0.96
2016-09-06T21:01:05.151833: step 2125, loss 0.100564, acc 0.96
2016-09-06T21:01:05.832421: step 2126, loss 0.0301227, acc 0.98
2016-09-06T21:01:06.543885: step 2127, loss 0.0781526, acc 0.96
2016-09-06T21:01:07.219831: step 2128, loss 0.00165676, acc 1
2016-09-06T21:01:07.899516: step 2129, loss 0.00476999, acc 1
2016-09-06T21:01:08.605639: step 2130, loss 0.0187365, acc 0.98
2016-09-06T21:01:09.303178: step 2131, loss 0.029807, acc 1
2016-09-06T21:01:10.027231: step 2132, loss 0.0163549, acc 1
2016-09-06T21:01:10.695977: step 2133, loss 0.178216, acc 0.98
2016-09-06T21:01:11.393073: step 2134, loss 0.0976049, acc 0.96
2016-09-06T21:01:12.063234: step 2135, loss 0.0279482, acc 1
2016-09-06T21:01:12.748881: step 2136, loss 0.0544907, acc 0.98
2016-09-06T21:01:13.442693: step 2137, loss 0.0446474, acc 0.98
2016-09-06T21:01:14.111333: step 2138, loss 0.018089, acc 1
2016-09-06T21:01:14.787636: step 2139, loss 0.067959, acc 0.98
2016-09-06T21:01:15.470996: step 2140, loss 0.0504473, acc 0.98
2016-09-06T21:01:16.172254: step 2141, loss 0.033779, acc 0.98
2016-09-06T21:01:16.834326: step 2142, loss 0.0441886, acc 0.98
2016-09-06T21:01:17.519465: step 2143, loss 0.048363, acc 0.96
2016-09-06T21:01:18.207544: step 2144, loss 0.250094, acc 0.88
2016-09-06T21:01:18.887519: step 2145, loss 0.0861199, acc 0.96
2016-09-06T21:01:19.577347: step 2146, loss 0.104956, acc 0.98
2016-09-06T21:01:20.265980: step 2147, loss 0.0872159, acc 0.98
2016-09-06T21:01:20.969055: step 2148, loss 0.0973547, acc 0.96
2016-09-06T21:01:21.644691: step 2149, loss 0.0210305, acc 1
2016-09-06T21:01:22.337588: step 2150, loss 0.0364397, acc 0.98
2016-09-06T21:01:23.033433: step 2151, loss 0.0859317, acc 0.96
2016-09-06T21:01:23.718609: step 2152, loss 0.0136734, acc 1
2016-09-06T21:01:24.394311: step 2153, loss 0.0627091, acc 0.94
2016-09-06T21:01:25.081112: step 2154, loss 0.00397439, acc 1
2016-09-06T21:01:25.790723: step 2155, loss 0.0317364, acc 1
2016-09-06T21:01:26.468577: step 2156, loss 0.0492003, acc 0.96
2016-09-06T21:01:27.155279: step 2157, loss 0.0508881, acc 0.98
2016-09-06T21:01:27.831802: step 2158, loss 0.0277617, acc 1
2016-09-06T21:01:28.498773: step 2159, loss 0.0672418, acc 0.96
2016-09-06T21:01:29.194443: step 2160, loss 0.0140347, acc 1
2016-09-06T21:01:29.878172: step 2161, loss 0.0875907, acc 0.94
2016-09-06T21:01:30.602633: step 2162, loss 0.0912478, acc 0.98
2016-09-06T21:01:31.263466: step 2163, loss 0.0292949, acc 1
2016-09-06T21:01:31.964005: step 2164, loss 0.0244375, acc 0.98
2016-09-06T21:01:32.638199: step 2165, loss 0.00712444, acc 1
2016-09-06T21:01:33.305783: step 2166, loss 0.03488, acc 1
2016-09-06T21:01:34.004839: step 2167, loss 0.0648257, acc 0.94
2016-09-06T21:01:34.680457: step 2168, loss 0.0214532, acc 0.98
2016-09-06T21:01:35.385978: step 2169, loss 0.020026, acc 1
2016-09-06T21:01:36.057355: step 2170, loss 0.0679088, acc 0.96
2016-09-06T21:01:36.743910: step 2171, loss 0.0547631, acc 0.98
2016-09-06T21:01:37.422685: step 2172, loss 0.0318215, acc 0.98
2016-09-06T21:01:38.109205: step 2173, loss 0.0303881, acc 0.98
2016-09-06T21:01:38.807784: step 2174, loss 0.0288918, acc 0.98
2016-09-06T21:01:39.491818: step 2175, loss 0.0438956, acc 0.98
2016-09-06T21:01:40.183388: step 2176, loss 0.0560427, acc 0.98
2016-09-06T21:01:40.850066: step 2177, loss 0.020013, acc 1
2016-09-06T21:01:41.565220: step 2178, loss 0.0269455, acc 0.98
2016-09-06T21:01:42.251726: step 2179, loss 0.01368, acc 1
2016-09-06T21:01:42.921754: step 2180, loss 0.059073, acc 1
2016-09-06T21:01:43.579762: step 2181, loss 0.0474941, acc 1
2016-09-06T21:01:44.253381: step 2182, loss 0.0336947, acc 0.98
2016-09-06T21:01:44.969411: step 2183, loss 0.0494502, acc 0.98
2016-09-06T21:01:45.636206: step 2184, loss 0.161305, acc 0.92
2016-09-06T21:01:46.386279: step 2185, loss 0.0159564, acc 1
2016-09-06T21:01:47.075338: step 2186, loss 0.020529, acc 1
2016-09-06T21:01:47.752524: step 2187, loss 0.0872668, acc 0.96
2016-09-06T21:01:48.439691: step 2188, loss 0.0121108, acc 1
2016-09-06T21:01:49.113599: step 2189, loss 0.106668, acc 0.94
2016-09-06T21:01:49.816011: step 2190, loss 0.0200965, acc 1
2016-09-06T21:01:50.474417: step 2191, loss 0.0487485, acc 0.96
2016-09-06T21:01:51.156938: step 2192, loss 0.098375, acc 0.98
2016-09-06T21:01:51.829586: step 2193, loss 0.0360164, acc 1
2016-09-06T21:01:52.529903: step 2194, loss 0.00959575, acc 1
2016-09-06T21:01:53.207891: step 2195, loss 0.0398, acc 0.98
2016-09-06T21:01:53.902701: step 2196, loss 0.0750395, acc 0.98
2016-09-06T21:01:54.599063: step 2197, loss 0.00424258, acc 1
2016-09-06T21:01:55.266472: step 2198, loss 0.020476, acc 1
2016-09-06T21:01:55.973122: step 2199, loss 0.0356039, acc 0.96
2016-09-06T21:01:56.676537: step 2200, loss 0.0348259, acc 0.98

Evaluation:
2016-09-06T21:01:59.825880: step 2200, loss 1.54681, acc 0.771107

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-2200

2016-09-06T21:02:01.565474: step 2201, loss 0.0274587, acc 0.98
2016-09-06T21:02:02.251753: step 2202, loss 0.122932, acc 0.96
2016-09-06T21:02:02.935618: step 2203, loss 0.0814187, acc 0.98
2016-09-06T21:02:03.605338: step 2204, loss 0.0738928, acc 0.96
2016-09-06T21:02:04.308520: step 2205, loss 0.00651213, acc 1
2016-09-06T21:02:05.003672: step 2206, loss 0.0290214, acc 0.98
2016-09-06T21:02:05.675381: step 2207, loss 0.020058, acc 1
2016-09-06T21:02:06.371228: step 2208, loss 0.039982, acc 0.98
2016-09-06T21:02:07.050540: step 2209, loss 0.0520345, acc 0.96
2016-09-06T21:02:07.735110: step 2210, loss 0.0154515, acc 1
2016-09-06T21:02:08.405802: step 2211, loss 0.0261465, acc 0.98
2016-09-06T21:02:09.107002: step 2212, loss 0.00472548, acc 1
2016-09-06T21:02:09.768828: step 2213, loss 0.0570424, acc 0.96
2016-09-06T21:02:10.466914: step 2214, loss 0.00301008, acc 1
2016-09-06T21:02:11.158970: step 2215, loss 0.027138, acc 1
2016-09-06T21:02:11.855828: step 2216, loss 0.0496339, acc 0.98
2016-09-06T21:02:12.556523: step 2217, loss 0.132006, acc 0.94
2016-09-06T21:02:13.228611: step 2218, loss 0.053296, acc 0.98
2016-09-06T21:02:13.939193: step 2219, loss 0.0509897, acc 0.98
2016-09-06T21:02:14.630553: step 2220, loss 0.0902778, acc 0.98
2016-09-06T21:02:15.317393: step 2221, loss 0.0657384, acc 0.98
2016-09-06T21:02:16.000202: step 2222, loss 0.0693592, acc 0.96
2016-09-06T21:02:16.684077: step 2223, loss 0.0555581, acc 0.98
2016-09-06T21:02:17.382751: step 2224, loss 0.018977, acc 1
2016-09-06T21:02:18.042280: step 2225, loss 0.169547, acc 0.92
2016-09-06T21:02:18.734476: step 2226, loss 0.04897, acc 0.96
2016-09-06T21:02:19.446443: step 2227, loss 0.186498, acc 0.94
2016-09-06T21:02:20.120031: step 2228, loss 0.0862449, acc 0.98
2016-09-06T21:02:20.806404: step 2229, loss 0.00910745, acc 1
2016-09-06T21:02:21.470111: step 2230, loss 0.0409158, acc 0.98
2016-09-06T21:02:22.138967: step 2231, loss 0.0553617, acc 0.96
2016-09-06T21:02:22.842799: step 2232, loss 0.0325937, acc 0.98
2016-09-06T21:02:23.550473: step 2233, loss 0.0635307, acc 0.96
2016-09-06T21:02:24.224145: step 2234, loss 0.0184022, acc 1
2016-09-06T21:02:24.915048: step 2235, loss 0.033023, acc 0.98
2016-09-06T21:02:25.606415: step 2236, loss 0.076798, acc 0.96
2016-09-06T21:02:26.305672: step 2237, loss 0.0719987, acc 0.94
2016-09-06T21:02:26.997107: step 2238, loss 0.121637, acc 0.96
2016-09-06T21:02:27.667953: step 2239, loss 0.0815179, acc 0.98
2016-09-06T21:02:28.368833: step 2240, loss 0.0945451, acc 0.94
2016-09-06T21:02:29.036083: step 2241, loss 0.018286, acc 1
2016-09-06T21:02:29.709918: step 2242, loss 0.125913, acc 0.96
2016-09-06T21:02:30.378363: step 2243, loss 0.0745587, acc 0.94
2016-09-06T21:02:31.043771: step 2244, loss 0.071206, acc 0.96
2016-09-06T21:02:31.723802: step 2245, loss 0.0598811, acc 0.98
2016-09-06T21:02:32.405658: step 2246, loss 0.0612718, acc 0.96
2016-09-06T21:02:33.092816: step 2247, loss 0.0673279, acc 0.96
2016-09-06T21:02:33.751265: step 2248, loss 0.0516997, acc 0.96
2016-09-06T21:02:34.440920: step 2249, loss 0.035793, acc 0.98
2016-09-06T21:02:35.118462: step 2250, loss 0.0516587, acc 0.98
2016-09-06T21:02:35.816056: step 2251, loss 0.152141, acc 0.92
2016-09-06T21:02:36.511189: step 2252, loss 0.0623183, acc 0.96
2016-09-06T21:02:37.206427: step 2253, loss 0.107541, acc 0.96
2016-09-06T21:02:37.939052: step 2254, loss 0.0701726, acc 0.94
2016-09-06T21:02:38.611869: step 2255, loss 0.0566861, acc 0.96
2016-09-06T21:02:39.287214: step 2256, loss 0.0275878, acc 1
2016-09-06T21:02:39.973685: step 2257, loss 0.0587993, acc 0.96
2016-09-06T21:02:40.643619: step 2258, loss 0.0536917, acc 0.98
2016-09-06T21:02:41.338059: step 2259, loss 0.0147583, acc 1
2016-09-06T21:02:42.041607: step 2260, loss 0.0772539, acc 0.96
2016-09-06T21:02:42.742551: step 2261, loss 0.0877753, acc 0.94
2016-09-06T21:02:43.440164: step 2262, loss 0.0305596, acc 0.98
2016-09-06T21:02:44.127134: step 2263, loss 0.0554338, acc 0.96
2016-09-06T21:02:44.806957: step 2264, loss 0.217555, acc 0.98
2016-09-06T21:02:45.497563: step 2265, loss 0.00719949, acc 1
2016-09-06T21:02:46.168441: step 2266, loss 0.155738, acc 0.92
2016-09-06T21:02:46.863207: step 2267, loss 0.0501977, acc 0.98
2016-09-06T21:02:47.568155: step 2268, loss 0.00207337, acc 1
2016-09-06T21:02:48.246544: step 2269, loss 0.00721906, acc 1
2016-09-06T21:02:48.931602: step 2270, loss 0.0227915, acc 0.98
2016-09-06T21:02:49.636480: step 2271, loss 0.0506033, acc 0.98
2016-09-06T21:02:50.319841: step 2272, loss 0.0497636, acc 0.96
2016-09-06T21:02:51.012614: step 2273, loss 0.0116905, acc 1
2016-09-06T21:02:51.674424: step 2274, loss 0.0247961, acc 0.98
2016-09-06T21:02:52.379763: step 2275, loss 0.114123, acc 0.96
2016-09-06T21:02:53.047978: step 2276, loss 0.0151574, acc 1
2016-09-06T21:02:53.720989: step 2277, loss 0.117678, acc 0.96
2016-09-06T21:02:54.400441: step 2278, loss 0.0433704, acc 0.98
2016-09-06T21:02:55.078561: step 2279, loss 0.00123123, acc 1
2016-09-06T21:02:55.747731: step 2280, loss 0.0672383, acc 0.98
2016-09-06T21:02:56.433845: step 2281, loss 0.0290895, acc 0.98
2016-09-06T21:02:57.139650: step 2282, loss 0.00957232, acc 1
2016-09-06T21:02:57.822270: step 2283, loss 0.0221551, acc 1
2016-09-06T21:02:58.516438: step 2284, loss 0.00266888, acc 1
2016-09-06T21:02:59.197962: step 2285, loss 0.00920631, acc 1
2016-09-06T21:02:59.910084: step 2286, loss 0.104532, acc 0.94
2016-09-06T21:03:00.622385: step 2287, loss 0.0373781, acc 0.98
2016-09-06T21:03:01.304094: step 2288, loss 0.0576093, acc 0.96
2016-09-06T21:03:01.994113: step 2289, loss 0.0153515, acc 1
2016-09-06T21:03:02.636640: step 2290, loss 0.205562, acc 0.92
2016-09-06T21:03:03.324935: step 2291, loss 0.22432, acc 0.92
2016-09-06T21:03:04.056977: step 2292, loss 0.023456, acc 1
2016-09-06T21:03:04.759238: step 2293, loss 0.0335343, acc 0.98
2016-09-06T21:03:05.435596: step 2294, loss 0.00610939, acc 1
2016-09-06T21:03:06.132614: step 2295, loss 0.0882509, acc 0.96
2016-09-06T21:03:06.861982: step 2296, loss 0.0388953, acc 0.96
2016-09-06T21:03:07.542849: step 2297, loss 0.0402179, acc 0.98
2016-09-06T21:03:08.241428: step 2298, loss 0.0346134, acc 0.98
2016-09-06T21:03:08.922916: step 2299, loss 0.046045, acc 0.98
2016-09-06T21:03:09.606575: step 2300, loss 0.00503397, acc 1

Evaluation:
2016-09-06T21:03:12.762300: step 2300, loss 1.43547, acc 0.75985

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-2300

2016-09-06T21:03:14.463539: step 2301, loss 0.0544341, acc 0.96
2016-09-06T21:03:15.188648: step 2302, loss 0.0550554, acc 0.98
2016-09-06T21:03:15.879643: step 2303, loss 0.0355733, acc 0.98
2016-09-06T21:03:16.533685: step 2304, loss 0.135588, acc 0.954545
2016-09-06T21:03:17.218705: step 2305, loss 0.0677524, acc 0.96
2016-09-06T21:03:17.909137: step 2306, loss 0.0145858, acc 1
2016-09-06T21:03:18.613694: step 2307, loss 0.0867102, acc 0.96
2016-09-06T21:03:19.314517: step 2308, loss 0.0201547, acc 1
2016-09-06T21:03:20.003894: step 2309, loss 0.0680401, acc 0.98
2016-09-06T21:03:20.656488: step 2310, loss 0.0175574, acc 1
2016-09-06T21:03:21.365270: step 2311, loss 0.0205975, acc 0.98
2016-09-06T21:03:22.069412: step 2312, loss 0.030442, acc 0.98
2016-09-06T21:03:22.765365: step 2313, loss 0.020268, acc 1
2016-09-06T21:03:23.433963: step 2314, loss 0.0195778, acc 1
2016-09-06T21:03:24.121668: step 2315, loss 0.0736479, acc 0.96
2016-09-06T21:03:24.817711: step 2316, loss 0.0538556, acc 0.98
2016-09-06T21:03:25.490840: step 2317, loss 0.0449177, acc 1
2016-09-06T21:03:26.159156: step 2318, loss 0.0699169, acc 0.96
2016-09-06T21:03:26.856632: step 2319, loss 0.0244674, acc 0.98
2016-09-06T21:03:27.554314: step 2320, loss 0.119262, acc 0.98
2016-09-06T21:03:28.251896: step 2321, loss 0.0775182, acc 0.96
2016-09-06T21:03:28.918947: step 2322, loss 0.0142685, acc 1
2016-09-06T21:03:29.593767: step 2323, loss 0.0270908, acc 1
2016-09-06T21:03:30.254229: step 2324, loss 0.0130428, acc 1
2016-09-06T21:03:30.977727: step 2325, loss 0.0272721, acc 0.98
2016-09-06T21:03:31.669876: step 2326, loss 0.080564, acc 0.96
2016-09-06T21:03:32.344661: step 2327, loss 0.0254182, acc 0.98
2016-09-06T21:03:33.044966: step 2328, loss 0.0658043, acc 0.96
2016-09-06T21:03:33.738526: step 2329, loss 0.0361413, acc 1
2016-09-06T21:03:34.419150: step 2330, loss 0.0324614, acc 0.98
2016-09-06T21:03:35.084615: step 2331, loss 0.0239932, acc 0.98
2016-09-06T21:03:35.786166: step 2332, loss 0.048246, acc 0.96
2016-09-06T21:03:36.460409: step 2333, loss 0.0509444, acc 0.98
2016-09-06T21:03:37.162439: step 2334, loss 0.168472, acc 0.96
2016-09-06T21:03:37.841446: step 2335, loss 0.0159415, acc 1
2016-09-06T21:03:38.514791: step 2336, loss 0.0522529, acc 0.98
2016-09-06T21:03:39.195495: step 2337, loss 0.0120297, acc 1
2016-09-06T21:03:39.864534: step 2338, loss 0.0408754, acc 0.96
2016-09-06T21:03:40.577145: step 2339, loss 0.0326043, acc 0.98
2016-09-06T21:03:41.274838: step 2340, loss 0.0408959, acc 0.98
2016-09-06T21:03:41.952332: step 2341, loss 0.0211663, acc 0.98
2016-09-06T21:03:42.642596: step 2342, loss 0.0592069, acc 0.98
2016-09-06T21:03:43.346390: step 2343, loss 0.114131, acc 0.98
2016-09-06T21:03:44.059885: step 2344, loss 0.00601989, acc 1
2016-09-06T21:03:44.724477: step 2345, loss 0.0164049, acc 0.98
2016-09-06T21:03:45.405141: step 2346, loss 0.106159, acc 0.94
2016-09-06T21:03:46.073632: step 2347, loss 0.0232743, acc 1
2016-09-06T21:03:46.754856: step 2348, loss 0.00670351, acc 1
2016-09-06T21:03:47.458083: step 2349, loss 0.0730841, acc 0.96
2016-09-06T21:03:48.145204: step 2350, loss 0.0544863, acc 0.96
2016-09-06T21:03:48.820401: step 2351, loss 0.0556371, acc 0.98
2016-09-06T21:03:49.521194: step 2352, loss 0.0398522, acc 0.98
2016-09-06T21:03:50.229819: step 2353, loss 0.0168074, acc 1
2016-09-06T21:03:50.956730: step 2354, loss 0.0442187, acc 0.96
2016-09-06T21:03:51.658968: step 2355, loss 0.0317348, acc 0.98
2016-09-06T21:03:52.356984: step 2356, loss 0.0219312, acc 1
2016-09-06T21:03:53.027514: step 2357, loss 0.0130027, acc 1
2016-09-06T21:03:53.713056: step 2358, loss 0.0121048, acc 1
2016-09-06T21:03:54.380958: step 2359, loss 0.165913, acc 0.96
2016-09-06T21:03:55.120156: step 2360, loss 0.0195684, acc 0.98
2016-09-06T21:03:55.810075: step 2361, loss 0.0298559, acc 0.98
2016-09-06T21:03:56.497643: step 2362, loss 0.0801102, acc 0.94
2016-09-06T21:03:57.204483: step 2363, loss 0.119837, acc 0.94
2016-09-06T21:03:57.870281: step 2364, loss 0.149922, acc 0.96
2016-09-06T21:03:58.579600: step 2365, loss 0.0300133, acc 0.98
2016-09-06T21:03:59.255885: step 2366, loss 0.0300388, acc 1
2016-09-06T21:03:59.926465: step 2367, loss 0.0515334, acc 0.98
2016-09-06T21:04:00.640849: step 2368, loss 0.018469, acc 1
2016-09-06T21:04:01.307816: step 2369, loss 0.0380884, acc 0.98
2016-09-06T21:04:01.995505: step 2370, loss 0.0415226, acc 0.98
2016-09-06T21:04:02.680475: step 2371, loss 0.00980307, acc 1
2016-09-06T21:04:03.387393: step 2372, loss 0.160199, acc 0.94
2016-09-06T21:04:04.064014: step 2373, loss 0.121939, acc 0.92
2016-09-06T21:04:04.741586: step 2374, loss 0.0464562, acc 0.98
2016-09-06T21:04:05.422355: step 2375, loss 0.0774613, acc 0.96
2016-09-06T21:04:06.110876: step 2376, loss 0.0564552, acc 0.96
2016-09-06T21:04:06.802262: step 2377, loss 0.0525433, acc 0.96
2016-09-06T21:04:07.489211: step 2378, loss 0.0574958, acc 1
2016-09-06T21:04:08.210453: step 2379, loss 0.0296348, acc 0.98
2016-09-06T21:04:08.902745: step 2380, loss 0.0632915, acc 0.96
2016-09-06T21:04:09.590275: step 2381, loss 0.0314376, acc 1
2016-09-06T21:04:10.246118: step 2382, loss 0.026105, acc 1
2016-09-06T21:04:10.918286: step 2383, loss 0.00392043, acc 1
2016-09-06T21:04:11.596374: step 2384, loss 0.0238099, acc 0.98
2016-09-06T21:04:12.258620: step 2385, loss 0.0512837, acc 1
2016-09-06T21:04:12.976198: step 2386, loss 0.00769248, acc 1
2016-09-06T21:04:13.652726: step 2387, loss 0.114286, acc 0.94
2016-09-06T21:04:14.344334: step 2388, loss 0.0394465, acc 0.98
2016-09-06T21:04:15.015584: step 2389, loss 0.0196002, acc 1
2016-09-06T21:04:15.723936: step 2390, loss 0.0349693, acc 1
2016-09-06T21:04:16.408314: step 2391, loss 0.0319877, acc 1
2016-09-06T21:04:17.089455: step 2392, loss 0.0206245, acc 1
2016-09-06T21:04:17.785960: step 2393, loss 0.0687055, acc 0.98
2016-09-06T21:04:18.472844: step 2394, loss 0.0283699, acc 1
2016-09-06T21:04:19.156807: step 2395, loss 0.0109424, acc 1
2016-09-06T21:04:19.854413: step 2396, loss 0.0772367, acc 0.94
2016-09-06T21:04:20.535721: step 2397, loss 0.0489594, acc 0.98
2016-09-06T21:04:21.226976: step 2398, loss 0.0501317, acc 0.98
2016-09-06T21:04:21.911728: step 2399, loss 0.019018, acc 0.98
2016-09-06T21:04:22.600614: step 2400, loss 0.0103614, acc 1

Evaluation:
2016-09-06T21:04:25.757771: step 2400, loss 1.73429, acc 0.763602

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-2400

2016-09-06T21:04:27.402502: step 2401, loss 0.0189606, acc 0.98
2016-09-06T21:04:28.075930: step 2402, loss 0.0282487, acc 0.98
2016-09-06T21:04:28.749390: step 2403, loss 0.0675554, acc 0.96
2016-09-06T21:04:29.441857: step 2404, loss 0.0284661, acc 1
2016-09-06T21:04:30.141245: step 2405, loss 0.0409447, acc 0.98
2016-09-06T21:04:30.844028: step 2406, loss 0.00210838, acc 1
2016-09-06T21:04:31.507492: step 2407, loss 0.0612696, acc 0.98
2016-09-06T21:04:32.245510: step 2408, loss 0.0235484, acc 0.98
2016-09-06T21:04:32.930484: step 2409, loss 0.0720878, acc 0.98
2016-09-06T21:04:33.620638: step 2410, loss 0.0680912, acc 0.96
2016-09-06T21:04:34.305874: step 2411, loss 0.020312, acc 0.98
2016-09-06T21:04:35.014190: step 2412, loss 0.0303217, acc 1
2016-09-06T21:04:35.705607: step 2413, loss 0.023288, acc 1
2016-09-06T21:04:36.373926: step 2414, loss 0.121063, acc 0.92
2016-09-06T21:04:37.081682: step 2415, loss 0.0746815, acc 0.98
2016-09-06T21:04:37.799566: step 2416, loss 0.0231069, acc 1
2016-09-06T21:04:38.490866: step 2417, loss 0.0469563, acc 0.98
2016-09-06T21:04:39.169982: step 2418, loss 0.0769982, acc 0.94
2016-09-06T21:04:39.866251: step 2419, loss 0.0475258, acc 0.96
2016-09-06T21:04:40.577469: step 2420, loss 0.0208907, acc 0.98
2016-09-06T21:04:41.248873: step 2421, loss 0.124366, acc 0.96
2016-09-06T21:04:41.933274: step 2422, loss 0.020911, acc 0.98
2016-09-06T21:04:42.610941: step 2423, loss 0.0224183, acc 1
2016-09-06T21:04:43.313252: step 2424, loss 0.165488, acc 0.96
2016-09-06T21:04:44.003365: step 2425, loss 0.0576131, acc 0.98
2016-09-06T21:04:44.672270: step 2426, loss 0.0866831, acc 0.94
2016-09-06T21:04:45.383484: step 2427, loss 0.00616912, acc 1
2016-09-06T21:04:46.063690: step 2428, loss 0.151783, acc 0.94
2016-09-06T21:04:46.740653: step 2429, loss 0.0801838, acc 0.96
2016-09-06T21:04:47.413207: step 2430, loss 0.0474444, acc 0.98
2016-09-06T21:04:48.090415: step 2431, loss 0.0884308, acc 0.92
2016-09-06T21:04:48.782636: step 2432, loss 0.0557014, acc 0.98
2016-09-06T21:04:49.471605: step 2433, loss 0.0472623, acc 0.98
2016-09-06T21:04:50.175027: step 2434, loss 0.144819, acc 0.96
2016-09-06T21:04:50.845295: step 2435, loss 0.0872543, acc 0.98
2016-09-06T21:04:51.542679: step 2436, loss 0.0319103, acc 0.98
2016-09-06T21:04:52.221958: step 2437, loss 0.0609078, acc 0.98
2016-09-06T21:04:52.920095: step 2438, loss 0.0565394, acc 0.98
2016-09-06T21:04:53.598988: step 2439, loss 0.0313861, acc 0.98
2016-09-06T21:04:54.270065: step 2440, loss 0.122763, acc 0.96
2016-09-06T21:04:54.944390: step 2441, loss 0.0488365, acc 0.98
2016-09-06T21:04:55.614311: step 2442, loss 0.0110429, acc 1
2016-09-06T21:04:56.309807: step 2443, loss 0.0318072, acc 1
2016-09-06T21:04:56.989577: step 2444, loss 0.0188868, acc 0.98
2016-09-06T21:04:57.685188: step 2445, loss 0.0442387, acc 0.96
2016-09-06T21:04:58.376513: step 2446, loss 0.0385167, acc 0.98
2016-09-06T21:04:59.076212: step 2447, loss 0.0187931, acc 1
2016-09-06T21:04:59.790137: step 2448, loss 0.0577467, acc 0.98
2016-09-06T21:05:00.513344: step 2449, loss 0.0282416, acc 1
2016-09-06T21:05:01.240199: step 2450, loss 0.033555, acc 0.98
2016-09-06T21:05:01.918582: step 2451, loss 0.0160805, acc 0.98
2016-09-06T21:05:02.598106: step 2452, loss 0.064054, acc 0.94
2016-09-06T21:05:03.286045: step 2453, loss 0.0309973, acc 0.98
2016-09-06T21:05:03.942294: step 2454, loss 0.0142322, acc 1
2016-09-06T21:05:04.631357: step 2455, loss 0.0708508, acc 0.96
2016-09-06T21:05:05.321343: step 2456, loss 0.0159362, acc 1
2016-09-06T21:05:06.034204: step 2457, loss 0.0125095, acc 1
2016-09-06T21:05:06.716677: step 2458, loss 0.0700503, acc 0.96
2016-09-06T21:05:07.375935: step 2459, loss 0.0873603, acc 0.96
2016-09-06T21:05:08.058350: step 2460, loss 0.0952327, acc 0.98
2016-09-06T21:05:08.736816: step 2461, loss 0.0152564, acc 1
2016-09-06T21:05:09.444697: step 2462, loss 0.0275259, acc 0.98
2016-09-06T21:05:10.104200: step 2463, loss 0.00212033, acc 1
2016-09-06T21:05:10.789488: step 2464, loss 0.0403673, acc 0.98
2016-09-06T21:05:11.496548: step 2465, loss 0.261755, acc 0.98
2016-09-06T21:05:12.195299: step 2466, loss 0.0390215, acc 0.98
2016-09-06T21:05:12.881928: step 2467, loss 0.0196415, acc 1
2016-09-06T21:05:13.552436: step 2468, loss 0.22221, acc 0.92
2016-09-06T21:05:14.269642: step 2469, loss 0.0456143, acc 1
2016-09-06T21:05:14.955158: step 2470, loss 0.0993081, acc 0.96
2016-09-06T21:05:15.636813: step 2471, loss 0.0279723, acc 0.98
2016-09-06T21:05:16.333420: step 2472, loss 0.040025, acc 0.98
2016-09-06T21:05:17.034705: step 2473, loss 0.031135, acc 1
2016-09-06T21:05:17.725748: step 2474, loss 0.10295, acc 0.94
2016-09-06T21:05:18.414015: step 2475, loss 0.0478294, acc 0.96
2016-09-06T21:05:19.118411: step 2476, loss 0.0153202, acc 1
2016-09-06T21:05:19.796433: step 2477, loss 0.0221778, acc 1
2016-09-06T21:05:20.467992: step 2478, loss 0.0427503, acc 0.98
2016-09-06T21:05:21.143314: step 2479, loss 0.01894, acc 1
2016-09-06T21:05:21.833381: step 2480, loss 0.0200347, acc 1
2016-09-06T21:05:22.506673: step 2481, loss 0.0409309, acc 1
2016-09-06T21:05:23.199585: step 2482, loss 0.0253808, acc 0.98
2016-09-06T21:05:23.915352: step 2483, loss 0.024635, acc 1
2016-09-06T21:05:24.599570: step 2484, loss 0.0530708, acc 0.98
2016-09-06T21:05:25.263506: step 2485, loss 0.0185386, acc 0.98
2016-09-06T21:05:25.971184: step 2486, loss 0.110875, acc 0.96
2016-09-06T21:05:26.653331: step 2487, loss 0.135897, acc 0.94
2016-09-06T21:05:27.341720: step 2488, loss 0.0855104, acc 0.94
2016-09-06T21:05:28.011941: step 2489, loss 0.0177886, acc 1
2016-09-06T21:05:28.702877: step 2490, loss 0.025019, acc 0.98
2016-09-06T21:05:29.379234: step 2491, loss 0.119571, acc 0.96
2016-09-06T21:05:30.048819: step 2492, loss 0.0224711, acc 0.98
2016-09-06T21:05:30.731132: step 2493, loss 0.0347762, acc 1
2016-09-06T21:05:31.419861: step 2494, loss 0.021669, acc 1
2016-09-06T21:05:32.109183: step 2495, loss 0.025708, acc 0.98
2016-09-06T21:05:32.766268: step 2496, loss 0.0210353, acc 0.977273
2016-09-06T21:05:33.477824: step 2497, loss 0.0622716, acc 0.98
2016-09-06T21:05:34.166847: step 2498, loss 0.136614, acc 0.94
2016-09-06T21:05:34.848104: step 2499, loss 0.110197, acc 0.94
2016-09-06T21:05:35.546886: step 2500, loss 0.016787, acc 0.98

Evaluation:
2016-09-06T21:05:38.682338: step 2500, loss 1.4306, acc 0.767355

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-2500

2016-09-06T21:05:40.308805: step 2501, loss 0.0711347, acc 0.98
2016-09-06T21:05:40.978872: step 2502, loss 0.0480136, acc 0.98
2016-09-06T21:05:41.675629: step 2503, loss 0.0174267, acc 0.98
2016-09-06T21:05:42.361952: step 2504, loss 0.0746741, acc 0.96
2016-09-06T21:05:43.081134: step 2505, loss 0.023684, acc 1
2016-09-06T21:05:43.741518: step 2506, loss 0.0666866, acc 0.96
2016-09-06T21:05:44.451095: step 2507, loss 0.0336492, acc 0.98
2016-09-06T21:05:45.137359: step 2508, loss 0.0680074, acc 0.98
2016-09-06T21:05:45.849081: step 2509, loss 0.0417371, acc 1
2016-09-06T21:05:46.546338: step 2510, loss 0.0318961, acc 0.98
2016-09-06T21:05:47.228870: step 2511, loss 0.0210634, acc 0.98
2016-09-06T21:05:47.937421: step 2512, loss 0.00825221, acc 1
2016-09-06T21:05:48.605665: step 2513, loss 0.0640636, acc 0.98
2016-09-06T21:05:49.302517: step 2514, loss 0.0583162, acc 0.98
2016-09-06T21:05:49.965998: step 2515, loss 0.019597, acc 1
2016-09-06T21:05:50.650940: step 2516, loss 0.0243232, acc 1
2016-09-06T21:05:51.338953: step 2517, loss 0.106132, acc 0.96
2016-09-06T21:05:52.041474: step 2518, loss 0.0588742, acc 0.96
2016-09-06T21:05:52.741394: step 2519, loss 0.0530269, acc 0.96
2016-09-06T21:05:53.405574: step 2520, loss 0.00647753, acc 1
2016-09-06T21:05:54.087945: step 2521, loss 0.0603327, acc 0.96
2016-09-06T21:05:54.780049: step 2522, loss 0.0637324, acc 0.98
2016-09-06T21:05:55.479714: step 2523, loss 0.0891201, acc 0.96
2016-09-06T21:05:56.204132: step 2524, loss 0.115705, acc 0.96
2016-09-06T21:05:56.870139: step 2525, loss 0.0252213, acc 0.98
2016-09-06T21:05:57.576574: step 2526, loss 0.0618834, acc 0.96
2016-09-06T21:05:58.261743: step 2527, loss 0.0900898, acc 0.98
2016-09-06T21:05:58.932566: step 2528, loss 0.0792512, acc 0.96
2016-09-06T21:05:59.602906: step 2529, loss 0.021189, acc 1
2016-09-06T21:06:00.308020: step 2530, loss 0.0764102, acc 0.98
2016-09-06T21:06:00.977239: step 2531, loss 0.106493, acc 0.96
2016-09-06T21:06:01.657451: step 2532, loss 0.0155738, acc 1
2016-09-06T21:06:02.384930: step 2533, loss 0.0232302, acc 1
2016-09-06T21:06:03.045898: step 2534, loss 0.0190931, acc 1
2016-09-06T21:06:03.737521: step 2535, loss 0.00267487, acc 1
2016-09-06T21:06:04.434089: step 2536, loss 0.0801436, acc 0.98
2016-09-06T21:06:05.125101: step 2537, loss 0.0361593, acc 0.98
2016-09-06T21:06:05.831320: step 2538, loss 0.0107135, acc 1
2016-09-06T21:06:06.514869: step 2539, loss 0.0762241, acc 0.98
2016-09-06T21:06:07.236141: step 2540, loss 0.0313288, acc 0.98
2016-09-06T21:06:07.928013: step 2541, loss 0.0555226, acc 0.96
2016-09-06T21:06:08.629647: step 2542, loss 0.0626997, acc 0.98
2016-09-06T21:06:09.318380: step 2543, loss 0.0335186, acc 0.98
2016-09-06T21:06:10.044676: step 2544, loss 0.0045391, acc 1
2016-09-06T21:06:10.734700: step 2545, loss 0.0239731, acc 1
2016-09-06T21:06:11.390655: step 2546, loss 0.110244, acc 0.92
2016-09-06T21:06:12.080961: step 2547, loss 0.0167072, acc 1
2016-09-06T21:06:12.761054: step 2548, loss 0.00642503, acc 1
2016-09-06T21:06:13.450340: step 2549, loss 0.00572593, acc 1
2016-09-06T21:06:14.154951: step 2550, loss 0.0311811, acc 1
2016-09-06T21:06:14.848581: step 2551, loss 0.0437546, acc 0.98
2016-09-06T21:06:15.567555: step 2552, loss 0.0253785, acc 0.98
2016-09-06T21:06:16.243258: step 2553, loss 0.0354738, acc 1
2016-09-06T21:06:16.936095: step 2554, loss 0.0824115, acc 0.96
2016-09-06T21:06:17.635855: step 2555, loss 0.133058, acc 0.94
2016-09-06T21:06:18.360298: step 2556, loss 0.0743021, acc 0.98
2016-09-06T21:06:19.039532: step 2557, loss 0.0355609, acc 0.98
2016-09-06T21:06:19.718352: step 2558, loss 0.0296495, acc 1
2016-09-06T21:06:20.418865: step 2559, loss 0.0213397, acc 0.98
2016-09-06T21:06:21.076569: step 2560, loss 0.0794368, acc 0.96
2016-09-06T21:06:21.754759: step 2561, loss 0.00302567, acc 1
2016-09-06T21:06:22.432672: step 2562, loss 0.030577, acc 1
2016-09-06T21:06:23.121630: step 2563, loss 0.0168714, acc 1
2016-09-06T21:06:23.811460: step 2564, loss 0.0588308, acc 0.96
2016-09-06T21:06:24.491864: step 2565, loss 0.0776766, acc 0.96
2016-09-06T21:06:25.194230: step 2566, loss 0.003688, acc 1
2016-09-06T21:06:25.862982: step 2567, loss 0.116511, acc 0.92
2016-09-06T21:06:26.534577: step 2568, loss 0.0292066, acc 0.98
2016-09-06T21:06:27.200460: step 2569, loss 0.0363946, acc 0.98
2016-09-06T21:06:27.871161: step 2570, loss 0.187093, acc 0.98
2016-09-06T21:06:28.554001: step 2571, loss 0.039106, acc 1
2016-09-06T21:06:29.228887: step 2572, loss 0.00290185, acc 1
2016-09-06T21:06:29.904834: step 2573, loss 0.0639514, acc 0.94
2016-09-06T21:06:30.559455: step 2574, loss 0.010721, acc 1
2016-09-06T21:06:31.263475: step 2575, loss 0.0152427, acc 1
2016-09-06T21:06:31.956598: step 2576, loss 0.127018, acc 0.96
2016-09-06T21:06:32.633921: step 2577, loss 0.0825737, acc 0.98
2016-09-06T21:06:33.311873: step 2578, loss 0.0516929, acc 0.96
2016-09-06T21:06:33.999794: step 2579, loss 0.0208109, acc 1
2016-09-06T21:06:34.676476: step 2580, loss 0.025606, acc 1
2016-09-06T21:06:35.346107: step 2581, loss 0.077586, acc 0.94
2016-09-06T21:06:36.047909: step 2582, loss 0.00582606, acc 1
2016-09-06T21:06:36.705287: step 2583, loss 0.0421489, acc 1
2016-09-06T21:06:37.378489: step 2584, loss 0.00777188, acc 1
2016-09-06T21:06:38.082471: step 2585, loss 0.148018, acc 0.94
2016-09-06T21:06:38.772098: step 2586, loss 0.065833, acc 0.98
2016-09-06T21:06:39.458143: step 2587, loss 0.014168, acc 1
2016-09-06T21:06:40.136045: step 2588, loss 0.0323671, acc 0.98
2016-09-06T21:06:40.833926: step 2589, loss 0.0329937, acc 0.98
2016-09-06T21:06:41.493445: step 2590, loss 0.00938246, acc 1
2016-09-06T21:06:42.181988: step 2591, loss 0.0663616, acc 0.96
2016-09-06T21:06:42.854804: step 2592, loss 0.0203467, acc 1
2016-09-06T21:06:43.547093: step 2593, loss 0.0330098, acc 0.98
2016-09-06T21:06:44.233613: step 2594, loss 0.0223254, acc 1
2016-09-06T21:06:44.910162: step 2595, loss 0.0605932, acc 0.96
2016-09-06T21:06:45.603476: step 2596, loss 0.0300047, acc 0.98
2016-09-06T21:06:46.283273: step 2597, loss 0.0320356, acc 0.98
2016-09-06T21:06:46.993553: step 2598, loss 0.00577359, acc 1
2016-09-06T21:06:47.693776: step 2599, loss 0.0918877, acc 0.96
2016-09-06T21:06:48.363187: step 2600, loss 0.0272272, acc 1

Evaluation:
2016-09-06T21:06:51.490959: step 2600, loss 1.74234, acc 0.763602

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-2600

2016-09-06T21:06:53.221743: step 2601, loss 0.0503954, acc 0.98
2016-09-06T21:06:53.921401: step 2602, loss 0.0254585, acc 1
2016-09-06T21:06:54.577787: step 2603, loss 0.0355082, acc 1
2016-09-06T21:06:55.277527: step 2604, loss 0.0245395, acc 0.98
2016-09-06T21:06:55.958681: step 2605, loss 0.0202547, acc 0.98
2016-09-06T21:06:56.693784: step 2606, loss 0.205131, acc 0.9
2016-09-06T21:06:57.384699: step 2607, loss 0.0501234, acc 0.96
2016-09-06T21:06:58.075367: step 2608, loss 0.100617, acc 0.98
2016-09-06T21:06:58.786009: step 2609, loss 0.00595225, acc 1
2016-09-06T21:06:59.457184: step 2610, loss 0.131261, acc 0.94
2016-09-06T21:07:00.167891: step 2611, loss 0.0272978, acc 0.98
2016-09-06T21:07:00.895971: step 2612, loss 0.0204172, acc 1
2016-09-06T21:07:01.590688: step 2613, loss 0.00849906, acc 1
2016-09-06T21:07:02.273087: step 2614, loss 0.0251906, acc 1
2016-09-06T21:07:02.957231: step 2615, loss 0.0940119, acc 0.98
2016-09-06T21:07:03.653159: step 2616, loss 0.0878963, acc 0.98
2016-09-06T21:07:04.333371: step 2617, loss 0.119675, acc 0.94
2016-09-06T21:07:05.020944: step 2618, loss 0.120101, acc 0.94
2016-09-06T21:07:05.710132: step 2619, loss 0.0119074, acc 1
2016-09-06T21:07:06.396010: step 2620, loss 0.0318579, acc 1
2016-09-06T21:07:07.083439: step 2621, loss 0.155438, acc 0.96
2016-09-06T21:07:07.778338: step 2622, loss 0.123402, acc 0.9
2016-09-06T21:07:08.474099: step 2623, loss 0.0471677, acc 0.98
2016-09-06T21:07:09.152473: step 2624, loss 0.0259859, acc 1
2016-09-06T21:07:09.833031: step 2625, loss 0.0928796, acc 0.94
2016-09-06T21:07:10.546931: step 2626, loss 0.127734, acc 0.96
2016-09-06T21:07:11.237328: step 2627, loss 0.102059, acc 0.98
2016-09-06T21:07:11.952281: step 2628, loss 0.0267028, acc 1
2016-09-06T21:07:12.629227: step 2629, loss 0.0592227, acc 0.98
2016-09-06T21:07:13.341839: step 2630, loss 0.0764688, acc 0.94
2016-09-06T21:07:13.990198: step 2631, loss 0.0149993, acc 1
2016-09-06T21:07:14.676084: step 2632, loss 0.035705, acc 0.98
2016-09-06T21:07:15.367310: step 2633, loss 0.0408887, acc 0.98
2016-09-06T21:07:16.060222: step 2634, loss 0.00271378, acc 1
2016-09-06T21:07:16.749323: step 2635, loss 0.0344306, acc 0.98
2016-09-06T21:07:17.436756: step 2636, loss 0.0315284, acc 0.98
2016-09-06T21:07:18.140328: step 2637, loss 0.0399272, acc 0.98
2016-09-06T21:07:18.829129: step 2638, loss 0.0284517, acc 0.98
2016-09-06T21:07:19.517143: step 2639, loss 0.0576178, acc 0.98
2016-09-06T21:07:20.217372: step 2640, loss 0.0202131, acc 1
2016-09-06T21:07:20.893321: step 2641, loss 0.0275413, acc 0.98
2016-09-06T21:07:21.583695: step 2642, loss 0.122113, acc 0.96
2016-09-06T21:07:22.243620: step 2643, loss 0.0126374, acc 1
2016-09-06T21:07:22.952539: step 2644, loss 0.0751462, acc 0.94
2016-09-06T21:07:23.636766: step 2645, loss 0.0370019, acc 0.98
2016-09-06T21:07:24.324093: step 2646, loss 0.0601287, acc 0.98
2016-09-06T21:07:25.017681: step 2647, loss 0.0435793, acc 0.98
2016-09-06T21:07:25.712907: step 2648, loss 0.0882799, acc 0.96
2016-09-06T21:07:26.410895: step 2649, loss 0.0323211, acc 0.98
2016-09-06T21:07:27.095559: step 2650, loss 0.0306916, acc 1
2016-09-06T21:07:27.812671: step 2651, loss 0.181381, acc 0.94
2016-09-06T21:07:28.510961: step 2652, loss 0.024753, acc 0.98
2016-09-06T21:07:29.183775: step 2653, loss 0.0203138, acc 1
2016-09-06T21:07:29.867982: step 2654, loss 0.0672412, acc 0.98
2016-09-06T21:07:30.563934: step 2655, loss 0.0174906, acc 1
2016-09-06T21:07:31.264337: step 2656, loss 0.103987, acc 0.96
2016-09-06T21:07:31.951069: step 2657, loss 0.0910472, acc 0.98
2016-09-06T21:07:32.658934: step 2658, loss 0.0141135, acc 1
2016-09-06T21:07:33.335288: step 2659, loss 0.15235, acc 0.98
2016-09-06T21:07:34.055844: step 2660, loss 0.151935, acc 0.96
2016-09-06T21:07:34.753834: step 2661, loss 0.0967282, acc 0.98
2016-09-06T21:07:35.451687: step 2662, loss 0.0368096, acc 0.98
2016-09-06T21:07:36.147270: step 2663, loss 0.0349284, acc 1
2016-09-06T21:07:36.829258: step 2664, loss 0.135022, acc 0.96
2016-09-06T21:07:37.504896: step 2665, loss 0.0434315, acc 0.96
2016-09-06T21:07:38.188034: step 2666, loss 0.0693634, acc 0.98
2016-09-06T21:07:38.885368: step 2667, loss 0.0496881, acc 0.98
2016-09-06T21:07:39.563620: step 2668, loss 0.0136578, acc 1
2016-09-06T21:07:40.228825: step 2669, loss 0.0718315, acc 0.96
2016-09-06T21:07:40.935592: step 2670, loss 0.0936458, acc 0.96
2016-09-06T21:07:41.634372: step 2671, loss 0.0376723, acc 0.98
2016-09-06T21:07:42.335307: step 2672, loss 0.0328424, acc 0.98
2016-09-06T21:07:42.999864: step 2673, loss 0.0651632, acc 0.96
2016-09-06T21:07:43.674659: step 2674, loss 0.0585244, acc 0.98
2016-09-06T21:07:44.359571: step 2675, loss 0.00520802, acc 1
2016-09-06T21:07:45.053961: step 2676, loss 0.0197507, acc 1
2016-09-06T21:07:45.744506: step 2677, loss 0.00335349, acc 1
2016-09-06T21:07:46.404971: step 2678, loss 0.058583, acc 0.96
2016-09-06T21:07:47.130908: step 2679, loss 0.0184324, acc 0.98
2016-09-06T21:07:47.812613: step 2680, loss 0.0469689, acc 1
2016-09-06T21:07:48.506628: step 2681, loss 0.0302157, acc 1
2016-09-06T21:07:49.199269: step 2682, loss 0.00885078, acc 1
2016-09-06T21:07:49.907207: step 2683, loss 0.069479, acc 0.96
2016-09-06T21:07:50.624663: step 2684, loss 0.0114659, acc 1
2016-09-06T21:07:51.299785: step 2685, loss 0.0480674, acc 0.98
2016-09-06T21:07:51.994904: step 2686, loss 0.0695416, acc 0.96
2016-09-06T21:07:52.680134: step 2687, loss 0.0326647, acc 1
2016-09-06T21:07:53.328769: step 2688, loss 0.141756, acc 0.977273
2016-09-06T21:07:54.031080: step 2689, loss 0.0585816, acc 0.96
2016-09-06T21:07:54.702773: step 2690, loss 0.0378984, acc 0.96
2016-09-06T21:07:55.399148: step 2691, loss 0.0898076, acc 0.96
2016-09-06T21:07:56.071744: step 2692, loss 0.0244118, acc 0.98
2016-09-06T21:07:56.760112: step 2693, loss 0.0498702, acc 0.98
2016-09-06T21:07:57.438764: step 2694, loss 0.0282583, acc 1
2016-09-06T21:07:58.106619: step 2695, loss 0.0249531, acc 0.98
2016-09-06T21:07:58.783044: step 2696, loss 0.0283021, acc 1
2016-09-06T21:07:59.457975: step 2697, loss 0.0211005, acc 0.98
2016-09-06T21:08:00.169139: step 2698, loss 0.0122925, acc 1
2016-09-06T21:08:00.870728: step 2699, loss 0.0162053, acc 0.98
2016-09-06T21:08:01.570008: step 2700, loss 0.0192566, acc 0.98

Evaluation:
2016-09-06T21:08:04.682375: step 2700, loss 1.58267, acc 0.760788

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-2700

2016-09-06T21:08:06.339021: step 2701, loss 0.056947, acc 0.98
2016-09-06T21:08:07.028240: step 2702, loss 0.0710197, acc 0.96
2016-09-06T21:08:07.692489: step 2703, loss 0.0277103, acc 1
2016-09-06T21:08:08.385066: step 2704, loss 0.00422405, acc 1
2016-09-06T21:08:09.086128: step 2705, loss 0.0770126, acc 0.94
2016-09-06T21:08:09.786845: step 2706, loss 0.0769328, acc 0.98
2016-09-06T21:08:10.463587: step 2707, loss 0.0259947, acc 1
2016-09-06T21:08:11.144322: step 2708, loss 0.017135, acc 1
2016-09-06T21:08:11.844709: step 2709, loss 0.00234995, acc 1
2016-09-06T21:08:12.516128: step 2710, loss 0.0885341, acc 0.98
2016-09-06T21:08:13.201139: step 2711, loss 0.0156109, acc 1
2016-09-06T21:08:13.894208: step 2712, loss 0.125757, acc 0.98
2016-09-06T21:08:14.607017: step 2713, loss 0.0221241, acc 0.98
2016-09-06T21:08:15.266792: step 2714, loss 0.00149475, acc 1
2016-09-06T21:08:15.953233: step 2715, loss 0.00271625, acc 1
2016-09-06T21:08:16.618833: step 2716, loss 0.0203614, acc 1
2016-09-06T21:08:17.306320: step 2717, loss 0.041061, acc 0.98
2016-09-06T21:08:18.003233: step 2718, loss 0.0288163, acc 0.98
2016-09-06T21:08:18.666591: step 2719, loss 0.0338916, acc 0.98
2016-09-06T21:08:19.380831: step 2720, loss 0.166122, acc 0.9
2016-09-06T21:08:20.046849: step 2721, loss 0.0289169, acc 1
2016-09-06T21:08:20.745517: step 2722, loss 0.0265645, acc 1
2016-09-06T21:08:21.426841: step 2723, loss 0.0286525, acc 1
2016-09-06T21:08:22.125518: step 2724, loss 0.123802, acc 0.96
2016-09-06T21:08:22.831466: step 2725, loss 0.0752639, acc 0.96
2016-09-06T21:08:23.534267: step 2726, loss 0.0373321, acc 0.98
2016-09-06T21:08:24.259007: step 2727, loss 0.0507198, acc 0.98
2016-09-06T21:08:24.935176: step 2728, loss 0.0314816, acc 0.98
2016-09-06T21:08:25.616617: step 2729, loss 0.0396068, acc 0.98
2016-09-06T21:08:26.293693: step 2730, loss 0.0332611, acc 0.98
2016-09-06T21:08:26.991146: step 2731, loss 0.0242199, acc 0.98
2016-09-06T21:08:27.707849: step 2732, loss 0.0584211, acc 0.96
2016-09-06T21:08:28.382406: step 2733, loss 0.0137407, acc 1
2016-09-06T21:08:29.104444: step 2734, loss 0.032673, acc 0.98
2016-09-06T21:08:29.792600: step 2735, loss 0.0763034, acc 0.94
2016-09-06T21:08:30.471607: step 2736, loss 0.0462051, acc 0.98
2016-09-06T21:08:31.183836: step 2737, loss 0.0663856, acc 0.98
2016-09-06T21:08:31.901953: step 2738, loss 0.0068833, acc 1
2016-09-06T21:08:32.622827: step 2739, loss 0.0278472, acc 0.98
2016-09-06T21:08:33.289867: step 2740, loss 0.00449382, acc 1
2016-09-06T21:08:33.981781: step 2741, loss 0.00149446, acc 1
2016-09-06T21:08:34.678201: step 2742, loss 0.00734351, acc 1
2016-09-06T21:08:35.363046: step 2743, loss 0.131925, acc 0.96
2016-09-06T21:08:36.066015: step 2744, loss 0.0749072, acc 0.98
2016-09-06T21:08:36.750081: step 2745, loss 0.0347201, acc 0.96
2016-09-06T21:08:37.474711: step 2746, loss 0.0475751, acc 0.98
2016-09-06T21:08:38.139176: step 2747, loss 0.0284013, acc 1
2016-09-06T21:08:38.805126: step 2748, loss 0.00969557, acc 1
2016-09-06T21:08:39.490155: step 2749, loss 0.0269618, acc 0.98
2016-09-06T21:08:40.189204: step 2750, loss 0.0533859, acc 0.98
2016-09-06T21:08:40.883354: step 2751, loss 0.044021, acc 0.98
2016-09-06T21:08:41.558958: step 2752, loss 0.0322425, acc 0.98
2016-09-06T21:08:42.266015: step 2753, loss 0.0215114, acc 1
2016-09-06T21:08:42.954450: step 2754, loss 0.016483, acc 1
2016-09-06T21:08:43.628586: step 2755, loss 0.00558266, acc 1
2016-09-06T21:08:44.301009: step 2756, loss 0.0490248, acc 0.98
2016-09-06T21:08:44.993977: step 2757, loss 0.0701768, acc 0.98
2016-09-06T21:08:45.669732: step 2758, loss 0.0849979, acc 0.96
2016-09-06T21:08:46.352712: step 2759, loss 0.0292889, acc 0.98
2016-09-06T21:08:47.057700: step 2760, loss 0.0611931, acc 0.96
2016-09-06T21:08:47.756638: step 2761, loss 0.0422142, acc 0.98
2016-09-06T21:08:48.451166: step 2762, loss 0.0548137, acc 0.98
2016-09-06T21:08:49.133959: step 2763, loss 0.0154483, acc 1
2016-09-06T21:08:49.827783: step 2764, loss 0.0147472, acc 1
2016-09-06T21:08:50.503381: step 2765, loss 0.0801595, acc 0.96
2016-09-06T21:08:51.179032: step 2766, loss 0.0560326, acc 0.98
2016-09-06T21:08:51.895005: step 2767, loss 0.0573707, acc 0.98
2016-09-06T21:08:52.578828: step 2768, loss 0.121063, acc 0.96
2016-09-06T21:08:53.257081: step 2769, loss 0.026705, acc 0.98
2016-09-06T21:08:53.940451: step 2770, loss 0.0342014, acc 0.98
2016-09-06T21:08:54.624313: step 2771, loss 0.0292787, acc 0.98
2016-09-06T21:08:55.307720: step 2772, loss 0.0266607, acc 1
2016-09-06T21:08:55.990389: step 2773, loss 0.0109261, acc 1
2016-09-06T21:08:56.694865: step 2774, loss 0.00285892, acc 1
2016-09-06T21:08:57.357041: step 2775, loss 0.0751719, acc 0.96
2016-09-06T21:08:58.034985: step 2776, loss 0.0364309, acc 1
2016-09-06T21:08:58.704782: step 2777, loss 0.0428191, acc 0.98
2016-09-06T21:08:59.376526: step 2778, loss 0.0501294, acc 0.96
2016-09-06T21:09:00.060826: step 2779, loss 0.0360438, acc 0.98
2016-09-06T21:09:00.780613: step 2780, loss 0.0547219, acc 0.98
2016-09-06T21:09:01.482606: step 2781, loss 0.0181168, acc 1
2016-09-06T21:09:02.141694: step 2782, loss 0.0284823, acc 0.98
2016-09-06T21:09:02.842765: step 2783, loss 0.0380169, acc 0.98
2016-09-06T21:09:03.529391: step 2784, loss 0.0858653, acc 0.96
2016-09-06T21:09:04.215493: step 2785, loss 0.0410539, acc 0.96
2016-09-06T21:09:04.909726: step 2786, loss 0.0335193, acc 1
2016-09-06T21:09:05.617545: step 2787, loss 0.0419875, acc 0.98
2016-09-06T21:09:06.331339: step 2788, loss 0.0478208, acc 0.96
2016-09-06T21:09:07.016803: step 2789, loss 0.0163984, acc 0.98
2016-09-06T21:09:07.695607: step 2790, loss 0.00204887, acc 1
2016-09-06T21:09:08.375905: step 2791, loss 0.00779829, acc 1
2016-09-06T21:09:09.051229: step 2792, loss 0.0426971, acc 0.98
2016-09-06T21:09:09.728718: step 2793, loss 0.106178, acc 0.98
2016-09-06T21:09:10.408802: step 2794, loss 0.0225149, acc 1
2016-09-06T21:09:11.120103: step 2795, loss 0.0325076, acc 0.98
2016-09-06T21:09:11.811021: step 2796, loss 0.0598751, acc 0.98
2016-09-06T21:09:12.496369: step 2797, loss 0.00170525, acc 1
2016-09-06T21:09:13.200514: step 2798, loss 0.0740662, acc 0.96
2016-09-06T21:09:13.926393: step 2799, loss 0.0476212, acc 0.96
2016-09-06T21:09:14.635156: step 2800, loss 0.0245841, acc 0.98

Evaluation:
2016-09-06T21:09:17.748549: step 2800, loss 1.90403, acc 0.756098

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-2800

2016-09-06T21:09:19.491677: step 2801, loss 0.082833, acc 0.98
2016-09-06T21:09:20.139084: step 2802, loss 0.0834406, acc 0.98
2016-09-06T21:09:20.837963: step 2803, loss 0.0478123, acc 0.96
2016-09-06T21:09:21.530078: step 2804, loss 0.0104613, acc 1
2016-09-06T21:09:22.219327: step 2805, loss 0.00269268, acc 1
2016-09-06T21:09:22.909239: step 2806, loss 0.013486, acc 1
2016-09-06T21:09:23.589942: step 2807, loss 0.0595972, acc 0.96
2016-09-06T21:09:24.267591: step 2808, loss 0.0200507, acc 1
2016-09-06T21:09:24.937958: step 2809, loss 0.00331344, acc 1
2016-09-06T21:09:25.644252: step 2810, loss 0.025095, acc 1
2016-09-06T21:09:26.334878: step 2811, loss 0.0461614, acc 0.98
2016-09-06T21:09:27.038998: step 2812, loss 0.0211535, acc 1
2016-09-06T21:09:27.717867: step 2813, loss 0.0172458, acc 1
2016-09-06T21:09:28.407184: step 2814, loss 0.0226308, acc 1
2016-09-06T21:09:29.083567: step 2815, loss 0.0470032, acc 0.96
2016-09-06T21:09:29.767666: step 2816, loss 0.0347843, acc 1
2016-09-06T21:09:30.478731: step 2817, loss 0.0341167, acc 0.98
2016-09-06T21:09:31.174667: step 2818, loss 0.066248, acc 0.96
2016-09-06T21:09:31.869614: step 2819, loss 0.20486, acc 0.9
2016-09-06T21:09:32.557136: step 2820, loss 0.0656201, acc 0.98
2016-09-06T21:09:33.255315: step 2821, loss 0.119442, acc 0.94
2016-09-06T21:09:33.960292: step 2822, loss 0.0180182, acc 0.98
2016-09-06T21:09:34.625251: step 2823, loss 0.0447795, acc 0.98
2016-09-06T21:09:35.285623: step 2824, loss 0.0138421, acc 1
2016-09-06T21:09:35.956754: step 2825, loss 0.0401717, acc 0.98
2016-09-06T21:09:36.643260: step 2826, loss 0.00100213, acc 1
2016-09-06T21:09:37.323572: step 2827, loss 0.0656245, acc 0.96
2016-09-06T21:09:38.021014: step 2828, loss 0.01011, acc 1
2016-09-06T21:09:38.700362: step 2829, loss 0.063404, acc 0.98
2016-09-06T21:09:39.360045: step 2830, loss 0.0236963, acc 0.98
2016-09-06T21:09:40.066415: step 2831, loss 0.00876213, acc 1
2016-09-06T21:09:40.746509: step 2832, loss 0.0365786, acc 0.98
2016-09-06T21:09:41.429061: step 2833, loss 0.0467189, acc 0.96
2016-09-06T21:09:42.113298: step 2834, loss 0.0323243, acc 1
2016-09-06T21:09:42.787244: step 2835, loss 0.0940369, acc 0.98
2016-09-06T21:09:43.455822: step 2836, loss 0.0354228, acc 1
2016-09-06T21:09:44.110626: step 2837, loss 0.0534654, acc 0.98
2016-09-06T21:09:44.811390: step 2838, loss 0.0104112, acc 1
2016-09-06T21:09:45.470357: step 2839, loss 0.105983, acc 0.96
2016-09-06T21:09:46.153984: step 2840, loss 0.0336124, acc 0.98
2016-09-06T21:09:46.857624: step 2841, loss 0.0220315, acc 0.98
2016-09-06T21:09:47.539181: step 2842, loss 0.184576, acc 0.96
2016-09-06T21:09:48.224117: step 2843, loss 0.0301415, acc 0.98
2016-09-06T21:09:48.921956: step 2844, loss 0.00158822, acc 1
2016-09-06T21:09:49.615621: step 2845, loss 0.0308684, acc 0.98
2016-09-06T21:09:50.285844: step 2846, loss 0.137091, acc 0.94
2016-09-06T21:09:51.014464: step 2847, loss 0.032166, acc 0.98
2016-09-06T21:09:51.713324: step 2848, loss 0.0964574, acc 0.98
2016-09-06T21:09:52.398039: step 2849, loss 0.00123191, acc 1
2016-09-06T21:09:53.080800: step 2850, loss 0.171764, acc 0.96
2016-09-06T21:09:53.744877: step 2851, loss 0.0337342, acc 0.98
2016-09-06T21:09:54.463343: step 2852, loss 0.030954, acc 0.98
2016-09-06T21:09:55.138908: step 2853, loss 0.0125957, acc 1
2016-09-06T21:09:55.828060: step 2854, loss 0.0601642, acc 0.96
2016-09-06T21:09:56.545347: step 2855, loss 0.0210829, acc 1
2016-09-06T21:09:57.223313: step 2856, loss 0.103887, acc 0.96
2016-09-06T21:09:57.905844: step 2857, loss 0.0208212, acc 1
2016-09-06T21:09:58.566211: step 2858, loss 0.0549956, acc 0.96
2016-09-06T21:09:59.283605: step 2859, loss 0.039038, acc 0.98
2016-09-06T21:09:59.952202: step 2860, loss 0.0615083, acc 0.98
2016-09-06T21:10:00.686051: step 2861, loss 0.108105, acc 0.96
2016-09-06T21:10:01.394031: step 2862, loss 0.110773, acc 0.98
2016-09-06T21:10:02.059505: step 2863, loss 0.037919, acc 0.98
2016-09-06T21:10:02.737787: step 2864, loss 0.0629649, acc 0.98
2016-09-06T21:10:03.419100: step 2865, loss 0.0757221, acc 0.96
2016-09-06T21:10:04.110585: step 2866, loss 0.0543147, acc 0.96
2016-09-06T21:10:04.808270: step 2867, loss 0.128254, acc 0.96
2016-09-06T21:10:05.475073: step 2868, loss 0.0186792, acc 1
2016-09-06T21:10:06.186032: step 2869, loss 0.0269045, acc 1
2016-09-06T21:10:06.871856: step 2870, loss 0.0176982, acc 1
2016-09-06T21:10:07.565045: step 2871, loss 0.0598055, acc 0.98
2016-09-06T21:10:08.234772: step 2872, loss 0.0255966, acc 1
2016-09-06T21:10:08.953134: step 2873, loss 0.068889, acc 0.94
2016-09-06T21:10:09.640332: step 2874, loss 0.0676177, acc 0.96
2016-09-06T21:10:10.322796: step 2875, loss 0.0700282, acc 0.96
2016-09-06T21:10:11.009563: step 2876, loss 0.0730028, acc 0.96
2016-09-06T21:10:11.699477: step 2877, loss 0.0696822, acc 0.96
2016-09-06T21:10:12.399718: step 2878, loss 0.0306729, acc 1
2016-09-06T21:10:13.086380: step 2879, loss 0.0273982, acc 0.98
2016-09-06T21:10:13.758197: step 2880, loss 0.0105383, acc 1
2016-09-06T21:10:14.442621: step 2881, loss 0.0444913, acc 0.96
2016-09-06T21:10:15.135979: step 2882, loss 0.0510997, acc 0.98
2016-09-06T21:10:15.829837: step 2883, loss 0.0218351, acc 1
2016-09-06T21:10:16.506969: step 2884, loss 0.0358059, acc 1
2016-09-06T21:10:17.210202: step 2885, loss 0.0270425, acc 0.98
2016-09-06T21:10:17.882541: step 2886, loss 0.012929, acc 1
2016-09-06T21:10:18.583991: step 2887, loss 0.00811993, acc 1
2016-09-06T21:10:19.268886: step 2888, loss 0.0655391, acc 0.96
2016-09-06T21:10:19.942728: step 2889, loss 0.0099475, acc 1
2016-09-06T21:10:20.629854: step 2890, loss 0.0119308, acc 1
2016-09-06T21:10:21.345511: step 2891, loss 0.0205081, acc 1
2016-09-06T21:10:22.055031: step 2892, loss 0.00945814, acc 1
2016-09-06T21:10:22.715787: step 2893, loss 0.0628383, acc 0.96
2016-09-06T21:10:23.440215: step 2894, loss 0.0682091, acc 0.96
2016-09-06T21:10:24.116203: step 2895, loss 0.00320364, acc 1
2016-09-06T21:10:24.801651: step 2896, loss 0.0212317, acc 0.98
2016-09-06T21:10:25.492551: step 2897, loss 0.0108208, acc 1
2016-09-06T21:10:26.205741: step 2898, loss 0.0472946, acc 0.98
2016-09-06T21:10:26.931480: step 2899, loss 0.0344237, acc 1
2016-09-06T21:10:27.601302: step 2900, loss 0.102897, acc 0.94

Evaluation:
2016-09-06T21:10:30.723640: step 2900, loss 1.82425, acc 0.754221

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-2900

2016-09-06T21:10:32.506714: step 2901, loss 0.0964395, acc 0.98
2016-09-06T21:10:33.189144: step 2902, loss 0.0186831, acc 0.98
2016-09-06T21:10:33.877375: step 2903, loss 0.0534144, acc 0.96
2016-09-06T21:10:34.558837: step 2904, loss 0.02994, acc 0.98
2016-09-06T21:10:35.245643: step 2905, loss 0.0162925, acc 1
2016-09-06T21:10:35.917761: step 2906, loss 0.0332997, acc 0.98
2016-09-06T21:10:36.601913: step 2907, loss 0.00927396, acc 1
2016-09-06T21:10:37.287606: step 2908, loss 0.0045281, acc 1
2016-09-06T21:10:37.974525: step 2909, loss 0.0025271, acc 1
2016-09-06T21:10:38.647633: step 2910, loss 0.082143, acc 0.96
2016-09-06T21:10:39.346562: step 2911, loss 0.019021, acc 0.98
2016-09-06T21:10:40.026666: step 2912, loss 0.111381, acc 0.96
2016-09-06T21:10:40.694148: step 2913, loss 0.0157348, acc 1
2016-09-06T21:10:41.422729: step 2914, loss 0.0230644, acc 0.98
2016-09-06T21:10:42.133440: step 2915, loss 0.020298, acc 0.98
2016-09-06T21:10:42.818730: step 2916, loss 0.0286399, acc 0.98
2016-09-06T21:10:43.502553: step 2917, loss 0.0342559, acc 0.98
2016-09-06T21:10:44.181947: step 2918, loss 0.061727, acc 0.96
2016-09-06T21:10:44.873809: step 2919, loss 0.0244741, acc 0.98
2016-09-06T21:10:45.533848: step 2920, loss 0.00503746, acc 1
2016-09-06T21:10:46.239153: step 2921, loss 0.104368, acc 0.92
2016-09-06T21:10:46.910413: step 2922, loss 0.131338, acc 0.98
2016-09-06T21:10:47.624330: step 2923, loss 0.160156, acc 0.94
2016-09-06T21:10:48.327291: step 2924, loss 0.0283598, acc 1
2016-09-06T21:10:49.028789: step 2925, loss 0.040529, acc 1
2016-09-06T21:10:49.725733: step 2926, loss 0.0377708, acc 0.98
2016-09-06T21:10:50.380466: step 2927, loss 0.114286, acc 0.98
2016-09-06T21:10:51.070780: step 2928, loss 0.0471237, acc 0.98
2016-09-06T21:10:51.750746: step 2929, loss 0.0141615, acc 1
2016-09-06T21:10:52.428946: step 2930, loss 0.0230835, acc 0.98
2016-09-06T21:10:53.117863: step 2931, loss 0.07907, acc 0.96
2016-09-06T21:10:53.815847: step 2932, loss 0.0330659, acc 0.98
2016-09-06T21:10:54.524337: step 2933, loss 0.0185236, acc 0.98
2016-09-06T21:10:55.186490: step 2934, loss 0.00144298, acc 1
2016-09-06T21:10:55.889416: step 2935, loss 0.0393788, acc 0.98
2016-09-06T21:10:56.584674: step 2936, loss 0.0211029, acc 0.98
2016-09-06T21:10:57.285019: step 2937, loss 0.029062, acc 1
2016-09-06T21:10:57.969494: step 2938, loss 0.0567879, acc 0.96
2016-09-06T21:10:58.677943: step 2939, loss 0.00509596, acc 1
2016-09-06T21:10:59.380929: step 2940, loss 0.0210814, acc 0.98
2016-09-06T21:11:00.052025: step 2941, loss 0.00106644, acc 1
2016-09-06T21:11:00.788565: step 2942, loss 0.0355251, acc 0.96
2016-09-06T21:11:01.477859: step 2943, loss 0.0148951, acc 1
2016-09-06T21:11:02.162654: step 2944, loss 0.0770428, acc 0.98
2016-09-06T21:11:02.854536: step 2945, loss 0.00129723, acc 1
2016-09-06T21:11:03.563928: step 2946, loss 0.0269221, acc 1
2016-09-06T21:11:04.281344: step 2947, loss 0.115718, acc 0.98
2016-09-06T21:11:04.968630: step 2948, loss 0.0822298, acc 0.94
2016-09-06T21:11:05.662482: step 2949, loss 0.0323165, acc 0.98
2016-09-06T21:11:06.341222: step 2950, loss 0.0169364, acc 1
2016-09-06T21:11:07.043275: step 2951, loss 0.0707561, acc 0.98
2016-09-06T21:11:07.724519: step 2952, loss 0.00721237, acc 1
2016-09-06T21:11:08.387851: step 2953, loss 0.0546274, acc 0.96
2016-09-06T21:11:09.073583: step 2954, loss 0.0397011, acc 0.98
2016-09-06T21:11:09.761132: step 2955, loss 0.0598938, acc 0.98
2016-09-06T21:11:10.455490: step 2956, loss 0.0590263, acc 0.96
2016-09-06T21:11:11.145556: step 2957, loss 0.0530534, acc 0.96
2016-09-06T21:11:11.843449: step 2958, loss 0.0290039, acc 0.98
2016-09-06T21:11:12.534151: step 2959, loss 0.0260012, acc 0.98
2016-09-06T21:11:13.199147: step 2960, loss 0.0254703, acc 0.98
2016-09-06T21:11:13.901450: step 2961, loss 0.00807607, acc 1
2016-09-06T21:11:14.591572: step 2962, loss 0.0214102, acc 0.98
2016-09-06T21:11:15.251939: step 2963, loss 0.0192676, acc 1
2016-09-06T21:11:15.931699: step 2964, loss 0.0529305, acc 0.98
2016-09-06T21:11:16.617978: step 2965, loss 0.111548, acc 0.96
2016-09-06T21:11:17.297940: step 2966, loss 0.0045337, acc 1
2016-09-06T21:11:17.969523: step 2967, loss 0.0744429, acc 0.94
2016-09-06T21:11:18.680066: step 2968, loss 0.0135459, acc 1
2016-09-06T21:11:19.374303: step 2969, loss 0.0580231, acc 0.98
2016-09-06T21:11:20.055714: step 2970, loss 0.0218027, acc 1
2016-09-06T21:11:20.740329: step 2971, loss 0.0116514, acc 1
2016-09-06T21:11:21.438424: step 2972, loss 0.241037, acc 0.92
2016-09-06T21:11:22.140254: step 2973, loss 0.0561995, acc 0.98
2016-09-06T21:11:22.826669: step 2974, loss 0.0316442, acc 0.98
2016-09-06T21:11:23.522448: step 2975, loss 0.0384457, acc 0.98
2016-09-06T21:11:24.198046: step 2976, loss 0.0140882, acc 1
2016-09-06T21:11:24.887309: step 2977, loss 0.0684586, acc 0.94
2016-09-06T21:11:25.568356: step 2978, loss 0.00924579, acc 1
2016-09-06T21:11:26.256740: step 2979, loss 0.0364615, acc 0.98
2016-09-06T21:11:26.965319: step 2980, loss 0.0368705, acc 0.98
2016-09-06T21:11:27.638484: step 2981, loss 0.00265595, acc 1
2016-09-06T21:11:28.322455: step 2982, loss 0.0308987, acc 0.98
2016-09-06T21:11:28.994230: step 2983, loss 0.0219502, acc 1
2016-09-06T21:11:29.663952: step 2984, loss 0.0947105, acc 0.96
2016-09-06T21:11:30.345271: step 2985, loss 0.0439897, acc 0.96
2016-09-06T21:11:31.035828: step 2986, loss 0.0302834, acc 1
2016-09-06T21:11:31.732649: step 2987, loss 0.0506295, acc 0.98
2016-09-06T21:11:32.412234: step 2988, loss 0.028186, acc 1
2016-09-06T21:11:33.126000: step 2989, loss 0.00603299, acc 1
2016-09-06T21:11:33.806335: step 2990, loss 0.0408282, acc 1
2016-09-06T21:11:34.482748: step 2991, loss 0.0642677, acc 0.96
2016-09-06T21:11:35.170125: step 2992, loss 0.094134, acc 0.98
2016-09-06T21:11:35.842771: step 2993, loss 0.0401082, acc 0.98
2016-09-06T21:11:36.560129: step 2994, loss 0.084009, acc 0.94
2016-09-06T21:11:37.237566: step 2995, loss 0.0206389, acc 0.98
2016-09-06T21:11:37.935503: step 2996, loss 0.0540479, acc 0.98
2016-09-06T21:11:38.615274: step 2997, loss 0.00331263, acc 1
2016-09-06T21:11:39.277300: step 2998, loss 0.0435382, acc 0.98
2016-09-06T21:11:39.962359: step 2999, loss 0.0210479, acc 0.98
2016-09-06T21:11:40.657031: step 3000, loss 0.104917, acc 0.96

Evaluation:
2016-09-06T21:11:43.815345: step 3000, loss 1.94068, acc 0.756098

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-3000

2016-09-06T21:11:45.482042: step 3001, loss 0.0160773, acc 1
2016-09-06T21:11:46.185165: step 3002, loss 0.0445426, acc 0.98
2016-09-06T21:11:46.861130: step 3003, loss 0.000727298, acc 1
2016-09-06T21:11:47.555538: step 3004, loss 0.0133817, acc 1
2016-09-06T21:11:48.230602: step 3005, loss 0.0284583, acc 0.98
2016-09-06T21:11:48.909329: step 3006, loss 0.103224, acc 0.96
2016-09-06T21:11:49.614063: step 3007, loss 0.00886927, acc 1
2016-09-06T21:11:50.281953: step 3008, loss 0.0385084, acc 1
2016-09-06T21:11:50.985166: step 3009, loss 0.0165769, acc 1
2016-09-06T21:11:51.656451: step 3010, loss 0.0300181, acc 0.98
2016-09-06T21:11:52.350616: step 3011, loss 0.0628992, acc 0.96
2016-09-06T21:11:53.046145: step 3012, loss 0.0241075, acc 1
2016-09-06T21:11:53.727298: step 3013, loss 0.160579, acc 0.94
2016-09-06T21:11:54.406579: step 3014, loss 0.117164, acc 0.94
2016-09-06T21:11:55.099868: step 3015, loss 0.0100178, acc 1
2016-09-06T21:11:55.828633: step 3016, loss 0.0168523, acc 1
2016-09-06T21:11:56.494506: step 3017, loss 0.0221878, acc 0.98
2016-09-06T21:11:57.195680: step 3018, loss 0.0247014, acc 0.98
2016-09-06T21:11:57.884819: step 3019, loss 0.0428938, acc 0.98
2016-09-06T21:11:58.574664: step 3020, loss 0.11047, acc 0.98
2016-09-06T21:11:59.257567: step 3021, loss 0.0252525, acc 1
2016-09-06T21:11:59.929358: step 3022, loss 0.0149835, acc 0.98
2016-09-06T21:12:00.649011: step 3023, loss 0.0139178, acc 1
2016-09-06T21:12:01.298239: step 3024, loss 0.0618327, acc 0.96
2016-09-06T21:12:02.018455: step 3025, loss 0.0734365, acc 0.96
2016-09-06T21:12:02.715012: step 3026, loss 0.0510507, acc 0.98
2016-09-06T21:12:03.397961: step 3027, loss 0.0280846, acc 0.98
2016-09-06T21:12:04.095896: step 3028, loss 0.0865936, acc 0.96
2016-09-06T21:12:04.769749: step 3029, loss 0.0341554, acc 1
2016-09-06T21:12:05.465740: step 3030, loss 0.0366078, acc 0.98
2016-09-06T21:12:06.121130: step 3031, loss 0.0197009, acc 1
2016-09-06T21:12:06.819496: step 3032, loss 0.027184, acc 0.98
2016-09-06T21:12:07.512053: step 3033, loss 0.0167364, acc 1
2016-09-06T21:12:08.208960: step 3034, loss 0.212711, acc 0.94
2016-09-06T21:12:08.887025: step 3035, loss 0.0180413, acc 0.98
2016-09-06T21:12:09.572455: step 3036, loss 0.00139531, acc 1
2016-09-06T21:12:10.282431: step 3037, loss 0.0219494, acc 1
2016-09-06T21:12:10.957995: step 3038, loss 0.0698423, acc 0.94
2016-09-06T21:12:11.652528: step 3039, loss 0.0548271, acc 0.96
2016-09-06T21:12:12.356489: step 3040, loss 0.0412849, acc 0.98
2016-09-06T21:12:13.032793: step 3041, loss 0.00834522, acc 1
2016-09-06T21:12:13.732509: step 3042, loss 0.0116266, acc 1
2016-09-06T21:12:14.421272: step 3043, loss 0.0451454, acc 0.98
2016-09-06T21:12:15.129653: step 3044, loss 0.00708411, acc 1
2016-09-06T21:12:15.820934: step 3045, loss 0.0187528, acc 1
2016-09-06T21:12:16.505383: step 3046, loss 0.0468338, acc 0.96
2016-09-06T21:12:17.175988: step 3047, loss 0.0946765, acc 0.98
2016-09-06T21:12:17.861931: step 3048, loss 0.0510001, acc 0.96
2016-09-06T21:12:18.543810: step 3049, loss 0.033839, acc 0.98
2016-09-06T21:12:19.217016: step 3050, loss 0.00429927, acc 1
2016-09-06T21:12:19.954498: step 3051, loss 0.0443008, acc 0.96
2016-09-06T21:12:20.615641: step 3052, loss 0.0555578, acc 0.98
2016-09-06T21:12:21.291400: step 3053, loss 0.0123391, acc 1
2016-09-06T21:12:21.990533: step 3054, loss 0.00858309, acc 1
2016-09-06T21:12:22.672095: step 3055, loss 0.116794, acc 0.96
2016-09-06T21:12:23.366359: step 3056, loss 0.0751747, acc 0.96
2016-09-06T21:12:24.049306: step 3057, loss 0.0340618, acc 0.98
2016-09-06T21:12:24.755049: step 3058, loss 0.0587144, acc 0.96
2016-09-06T21:12:25.434715: step 3059, loss 0.0395512, acc 0.98
2016-09-06T21:12:26.121803: step 3060, loss 0.0416457, acc 0.98
2016-09-06T21:12:26.806848: step 3061, loss 0.0118069, acc 1
2016-09-06T21:12:27.515242: step 3062, loss 0.00207491, acc 1
2016-09-06T21:12:28.223149: step 3063, loss 0.0780437, acc 0.96
2016-09-06T21:12:28.893799: step 3064, loss 0.0586706, acc 0.96
2016-09-06T21:12:29.591511: step 3065, loss 0.123629, acc 0.96
2016-09-06T21:12:30.262137: step 3066, loss 0.0158188, acc 1
2016-09-06T21:12:30.964786: step 3067, loss 0.0701126, acc 0.96
2016-09-06T21:12:31.637894: step 3068, loss 8.10251e-05, acc 1
2016-09-06T21:12:32.325428: step 3069, loss 0.0375659, acc 0.98
2016-09-06T21:12:33.016027: step 3070, loss 0.0223036, acc 1
2016-09-06T21:12:33.692087: step 3071, loss 0.0640889, acc 0.96
2016-09-06T21:12:34.344349: step 3072, loss 0.00354675, acc 1
2016-09-06T21:12:35.020646: step 3073, loss 0.0751026, acc 0.96
2016-09-06T21:12:35.697948: step 3074, loss 0.0275974, acc 0.98
2016-09-06T21:12:36.380726: step 3075, loss 0.0619499, acc 0.98
2016-09-06T21:12:37.057085: step 3076, loss 0.129491, acc 0.96
2016-09-06T21:12:37.744336: step 3077, loss 0.084215, acc 0.96
2016-09-06T21:12:38.425594: step 3078, loss 0.0982654, acc 0.98
2016-09-06T21:12:39.141241: step 3079, loss 0.00604956, acc 1
2016-09-06T21:12:39.804713: step 3080, loss 0.0217014, acc 1
2016-09-06T21:12:40.496865: step 3081, loss 0.0441663, acc 0.98
2016-09-06T21:12:41.178865: step 3082, loss 0.0568542, acc 0.96
2016-09-06T21:12:41.850520: step 3083, loss 0.0229765, acc 0.98
2016-09-06T21:12:42.532934: step 3084, loss 0.0144805, acc 1
2016-09-06T21:12:43.203531: step 3085, loss 0.0418699, acc 1
2016-09-06T21:12:43.879643: step 3086, loss 0.0197784, acc 1
2016-09-06T21:12:44.530813: step 3087, loss 0.0197178, acc 1
2016-09-06T21:12:45.231158: step 3088, loss 0.0354326, acc 0.98
2016-09-06T21:12:45.908882: step 3089, loss 0.106355, acc 0.94
2016-09-06T21:12:46.594851: step 3090, loss 0.0531136, acc 0.98
2016-09-06T21:12:47.288200: step 3091, loss 0.0527788, acc 0.96
2016-09-06T21:12:47.985720: step 3092, loss 0.136809, acc 0.94
2016-09-06T21:12:48.686416: step 3093, loss 0.0223235, acc 1
2016-09-06T21:12:49.365580: step 3094, loss 0.0144715, acc 1
2016-09-06T21:12:50.058970: step 3095, loss 0.0137044, acc 1
2016-09-06T21:12:50.741453: step 3096, loss 0.0233847, acc 0.98
2016-09-06T21:12:51.442600: step 3097, loss 0.0264966, acc 1
2016-09-06T21:12:52.136528: step 3098, loss 0.014705, acc 1
2016-09-06T21:12:52.819107: step 3099, loss 0.0136535, acc 1
2016-09-06T21:12:53.520270: step 3100, loss 0.113584, acc 0.98

Evaluation:
2016-09-06T21:12:56.650228: step 3100, loss 1.55134, acc 0.767355

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-3100

2016-09-06T21:12:58.325009: step 3101, loss 0.0196408, acc 1
2016-09-06T21:12:58.980958: step 3102, loss 0.060949, acc 0.96
2016-09-06T21:12:59.675508: step 3103, loss 0.0707819, acc 0.98
2016-09-06T21:13:00.376819: step 3104, loss 0.0645324, acc 0.96
2016-09-06T21:13:01.080325: step 3105, loss 0.0381282, acc 0.98
2016-09-06T21:13:01.753341: step 3106, loss 0.0484045, acc 0.98
2016-09-06T21:13:02.432443: step 3107, loss 0.0560488, acc 0.96
2016-09-06T21:13:03.119579: step 3108, loss 0.0860976, acc 0.92
2016-09-06T21:13:03.782921: step 3109, loss 0.0159408, acc 1
2016-09-06T21:13:04.465843: step 3110, loss 0.0208004, acc 1
2016-09-06T21:13:05.123740: step 3111, loss 0.00735442, acc 1
2016-09-06T21:13:05.797407: step 3112, loss 0.00241588, acc 1
2016-09-06T21:13:06.504691: step 3113, loss 0.032688, acc 0.98
2016-09-06T21:13:07.227222: step 3114, loss 0.0249683, acc 1
2016-09-06T21:13:07.929458: step 3115, loss 0.0446297, acc 0.98
2016-09-06T21:13:08.614559: step 3116, loss 0.0615968, acc 0.98
2016-09-06T21:13:09.320444: step 3117, loss 0.0389666, acc 0.98
2016-09-06T21:13:10.009569: step 3118, loss 0.00291785, acc 1
2016-09-06T21:13:10.696581: step 3119, loss 0.0106507, acc 1
2016-09-06T21:13:11.386502: step 3120, loss 0.034868, acc 0.98
2016-09-06T21:13:12.088204: step 3121, loss 0.0265209, acc 0.98
2016-09-06T21:13:12.773574: step 3122, loss 0.0503561, acc 0.98
2016-09-06T21:13:13.455274: step 3123, loss 0.0118546, acc 1
2016-09-06T21:13:14.155097: step 3124, loss 0.0420519, acc 0.98
2016-09-06T21:13:14.841727: step 3125, loss 0.0254765, acc 0.98
2016-09-06T21:13:15.526441: step 3126, loss 0.0337941, acc 0.98
2016-09-06T21:13:16.207503: step 3127, loss 0.0411847, acc 0.98
2016-09-06T21:13:16.896194: step 3128, loss 0.00470208, acc 1
2016-09-06T21:13:17.573229: step 3129, loss 0.0209823, acc 1
2016-09-06T21:13:18.240745: step 3130, loss 0.0469221, acc 0.96
2016-09-06T21:13:18.933261: step 3131, loss 0.0253399, acc 0.98
2016-09-06T21:13:19.599254: step 3132, loss 0.0223403, acc 0.98
2016-09-06T21:13:20.281002: step 3133, loss 0.0521855, acc 0.96
2016-09-06T21:13:20.987789: step 3134, loss 0.144884, acc 0.94
2016-09-06T21:13:21.677867: step 3135, loss 0.135059, acc 0.96
2016-09-06T21:13:22.363633: step 3136, loss 0.0213001, acc 0.98
2016-09-06T21:13:23.017894: step 3137, loss 0.000568759, acc 1
2016-09-06T21:13:23.718367: step 3138, loss 0.00916222, acc 1
2016-09-06T21:13:24.400362: step 3139, loss 0.0284969, acc 1
2016-09-06T21:13:25.068660: step 3140, loss 0.0383304, acc 0.98
2016-09-06T21:13:25.765495: step 3141, loss 0.0739535, acc 0.98
2016-09-06T21:13:26.450615: step 3142, loss 0.0171262, acc 1
2016-09-06T21:13:27.122105: step 3143, loss 0.0381286, acc 0.98
2016-09-06T21:13:27.808297: step 3144, loss 0.0745885, acc 0.94
2016-09-06T21:13:28.518101: step 3145, loss 0.038064, acc 0.98
2016-09-06T21:13:29.200125: step 3146, loss 0.0210101, acc 0.98
2016-09-06T21:13:29.888183: step 3147, loss 0.0348797, acc 1
2016-09-06T21:13:30.570740: step 3148, loss 0.0375125, acc 0.96
2016-09-06T21:13:31.256943: step 3149, loss 0.0460577, acc 0.98
2016-09-06T21:13:31.941212: step 3150, loss 0.0642177, acc 0.96
2016-09-06T21:13:32.611945: step 3151, loss 0.0138913, acc 1
2016-09-06T21:13:33.298876: step 3152, loss 0.0496939, acc 0.98
2016-09-06T21:13:33.953433: step 3153, loss 0.0981433, acc 0.96
2016-09-06T21:13:34.630987: step 3154, loss 0.0320708, acc 0.98
2016-09-06T21:13:35.313203: step 3155, loss 0.0863551, acc 0.98
2016-09-06T21:13:35.990801: step 3156, loss 0.0175135, acc 1
2016-09-06T21:13:36.656299: step 3157, loss 0.0145563, acc 1
2016-09-06T21:13:37.351973: step 3158, loss 0.0139178, acc 1
2016-09-06T21:13:38.032012: step 3159, loss 0.0856768, acc 0.94
2016-09-06T21:13:38.701889: step 3160, loss 0.0226722, acc 1
2016-09-06T21:13:39.397558: step 3161, loss 0.0531105, acc 0.98
2016-09-06T21:13:40.088555: step 3162, loss 0.015605, acc 1
2016-09-06T21:13:40.761316: step 3163, loss 0.00554858, acc 1
2016-09-06T21:13:41.444201: step 3164, loss 0.0788101, acc 0.98
2016-09-06T21:13:42.141436: step 3165, loss 0.00258217, acc 1
2016-09-06T21:13:42.811212: step 3166, loss 0.0300102, acc 0.98
2016-09-06T21:13:43.481432: step 3167, loss 0.0462338, acc 1
2016-09-06T21:13:44.168324: step 3168, loss 0.00712263, acc 1
2016-09-06T21:13:44.860650: step 3169, loss 0.0249725, acc 1
2016-09-06T21:13:45.540563: step 3170, loss 0.020131, acc 1
2016-09-06T21:13:46.238209: step 3171, loss 0.0311218, acc 1
2016-09-06T21:13:46.926623: step 3172, loss 0.0180187, acc 1
2016-09-06T21:13:47.602439: step 3173, loss 0.0135504, acc 1
2016-09-06T21:13:48.254151: step 3174, loss 0.00155826, acc 1
2016-09-06T21:13:48.961127: step 3175, loss 0.0746703, acc 0.94
2016-09-06T21:13:49.631386: step 3176, loss 0.0391603, acc 0.98
2016-09-06T21:13:50.305757: step 3177, loss 0.00570814, acc 1
2016-09-06T21:13:50.990783: step 3178, loss 0.0219959, acc 1
2016-09-06T21:13:51.705543: step 3179, loss 0.0430998, acc 0.98
2016-09-06T21:13:52.382183: step 3180, loss 0.0972977, acc 0.94
2016-09-06T21:13:53.068093: step 3181, loss 0.0313883, acc 1
2016-09-06T21:13:53.782794: step 3182, loss 0.0416374, acc 0.96
2016-09-06T21:13:54.465443: step 3183, loss 0.0349136, acc 1
2016-09-06T21:13:55.142519: step 3184, loss 0.0101023, acc 1
2016-09-06T21:13:55.811555: step 3185, loss 0.00179232, acc 1
2016-09-06T21:13:56.501817: step 3186, loss 0.0544255, acc 0.96
2016-09-06T21:13:57.201754: step 3187, loss 0.0384964, acc 0.98
2016-09-06T21:13:57.873391: step 3188, loss 0.0322072, acc 0.96
2016-09-06T21:13:58.564802: step 3189, loss 0.0626698, acc 0.96
2016-09-06T21:13:59.235393: step 3190, loss 0.00125679, acc 1
2016-09-06T21:13:59.918820: step 3191, loss 0.000433049, acc 1
2016-09-06T21:14:00.628257: step 3192, loss 0.0196607, acc 1
2016-09-06T21:14:01.300145: step 3193, loss 0.0237072, acc 0.98
2016-09-06T21:14:01.984870: step 3194, loss 0.10803, acc 0.92
2016-09-06T21:14:02.680399: step 3195, loss 0.00102425, acc 1
2016-09-06T21:14:03.387734: step 3196, loss 0.118848, acc 0.96
2016-09-06T21:14:04.067532: step 3197, loss 0.0694842, acc 0.98
2016-09-06T21:14:04.750184: step 3198, loss 0.0230567, acc 0.98
2016-09-06T21:14:05.433738: step 3199, loss 0.0405757, acc 0.98
2016-09-06T21:14:06.120297: step 3200, loss 0.104684, acc 0.96

Evaluation:
2016-09-06T21:14:09.250173: step 3200, loss 1.83497, acc 0.757974

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-3200

2016-09-06T21:14:10.881328: step 3201, loss 0.0193936, acc 1
2016-09-06T21:14:11.563295: step 3202, loss 0.0217812, acc 1
2016-09-06T21:14:12.240967: step 3203, loss 0.129133, acc 0.94
2016-09-06T21:14:12.971262: step 3204, loss 0.0192524, acc 1
2016-09-06T21:14:13.636771: step 3205, loss 0.0407196, acc 0.98
2016-09-06T21:14:14.323651: step 3206, loss 0.0460907, acc 0.98
2016-09-06T21:14:15.014278: step 3207, loss 0.019195, acc 0.98
2016-09-06T21:14:15.697987: step 3208, loss 0.0268655, acc 0.98
2016-09-06T21:14:16.374824: step 3209, loss 0.0981497, acc 0.98
2016-09-06T21:14:17.056621: step 3210, loss 0.0248731, acc 1
2016-09-06T21:14:17.774279: step 3211, loss 0.0726816, acc 0.96
2016-09-06T21:14:18.463244: step 3212, loss 0.0266472, acc 1
2016-09-06T21:14:19.132147: step 3213, loss 0.0289542, acc 0.98
2016-09-06T21:14:19.812278: step 3214, loss 0.0125829, acc 1
2016-09-06T21:14:20.494022: step 3215, loss 0.0591227, acc 0.96
2016-09-06T21:14:21.180631: step 3216, loss 0.00154474, acc 1
2016-09-06T21:14:21.873803: step 3217, loss 0.0564194, acc 0.94
2016-09-06T21:14:22.584469: step 3218, loss 0.00540981, acc 1
2016-09-06T21:14:23.270411: step 3219, loss 0.00481722, acc 1
2016-09-06T21:14:23.957013: step 3220, loss 0.0537502, acc 0.96
2016-09-06T21:14:24.640179: step 3221, loss 0.0397401, acc 0.96
2016-09-06T21:14:25.327061: step 3222, loss 0.00751531, acc 1
2016-09-06T21:14:26.007785: step 3223, loss 0.00932951, acc 1
2016-09-06T21:14:26.702247: step 3224, loss 0.0289776, acc 0.98
2016-09-06T21:14:27.405636: step 3225, loss 0.0097886, acc 1
2016-09-06T21:14:28.057044: step 3226, loss 0.029408, acc 1
2016-09-06T21:14:28.736280: step 3227, loss 0.0275289, acc 0.98
2016-09-06T21:14:29.425810: step 3228, loss 0.00610863, acc 1
2016-09-06T21:14:30.130926: step 3229, loss 0.0654133, acc 0.98
2016-09-06T21:14:30.819212: step 3230, loss 0.0982646, acc 0.96
2016-09-06T21:14:31.518915: step 3231, loss 0.0475182, acc 0.98
2016-09-06T21:14:32.250301: step 3232, loss 0.051393, acc 0.98
2016-09-06T21:14:32.938998: step 3233, loss 0.0163106, acc 0.98
2016-09-06T21:14:33.631484: step 3234, loss 0.0158831, acc 0.98
2016-09-06T21:14:34.317385: step 3235, loss 0.091664, acc 0.98
2016-09-06T21:14:34.991903: step 3236, loss 0.0440453, acc 0.96
2016-09-06T21:14:35.667276: step 3237, loss 0.0290363, acc 0.98
2016-09-06T21:14:36.353188: step 3238, loss 0.0273749, acc 0.98
2016-09-06T21:14:37.045849: step 3239, loss 0.00820747, acc 1
2016-09-06T21:14:37.707915: step 3240, loss 0.0451327, acc 0.98
2016-09-06T21:14:38.413405: step 3241, loss 0.0849731, acc 0.98
2016-09-06T21:14:39.090950: step 3242, loss 0.0141099, acc 1
2016-09-06T21:14:39.764124: step 3243, loss 0.0599678, acc 0.96
2016-09-06T21:14:40.434510: step 3244, loss 0.00813705, acc 1
2016-09-06T21:14:41.121439: step 3245, loss 0.0435308, acc 0.96
2016-09-06T21:14:41.817103: step 3246, loss 0.0649373, acc 0.98
2016-09-06T21:14:42.510851: step 3247, loss 0.0906403, acc 0.98
2016-09-06T21:14:43.205713: step 3248, loss 0.171546, acc 0.96
2016-09-06T21:14:43.910129: step 3249, loss 0.118472, acc 0.98
2016-09-06T21:14:44.593558: step 3250, loss 0.0276722, acc 1
2016-09-06T21:14:45.295477: step 3251, loss 0.00649811, acc 1
2016-09-06T21:14:45.965713: step 3252, loss 0.00291056, acc 1
2016-09-06T21:14:46.685759: step 3253, loss 0.056181, acc 0.98
2016-09-06T21:14:47.379407: step 3254, loss 0.00718969, acc 1
2016-09-06T21:14:48.060504: step 3255, loss 0.0316458, acc 0.96
2016-09-06T21:14:48.741071: step 3256, loss 0.0549807, acc 0.98
2016-09-06T21:14:49.420783: step 3257, loss 0.0815234, acc 0.94
2016-09-06T21:14:50.109233: step 3258, loss 0.00265209, acc 1
2016-09-06T21:14:50.768529: step 3259, loss 0.0261682, acc 0.98
2016-09-06T21:14:51.449395: step 3260, loss 0.104713, acc 0.98
2016-09-06T21:14:52.124561: step 3261, loss 0.0822019, acc 0.96
2016-09-06T21:14:52.812952: step 3262, loss 0.102098, acc 0.96
2016-09-06T21:14:53.503700: step 3263, loss 0.0845553, acc 0.94
2016-09-06T21:14:54.139683: step 3264, loss 0.000470686, acc 1
2016-09-06T21:14:54.831977: step 3265, loss 0.043351, acc 0.98
2016-09-06T21:14:55.491178: step 3266, loss 0.0691196, acc 0.96
2016-09-06T21:14:56.177011: step 3267, loss 0.0438207, acc 0.98
2016-09-06T21:14:56.834861: step 3268, loss 0.0238442, acc 1
2016-09-06T21:14:57.533242: step 3269, loss 0.024743, acc 1
2016-09-06T21:14:58.222018: step 3270, loss 0.032245, acc 1
2016-09-06T21:14:58.904336: step 3271, loss 0.0114556, acc 1
2016-09-06T21:14:59.605828: step 3272, loss 0.0342081, acc 0.98
2016-09-06T21:15:00.304392: step 3273, loss 0.0105237, acc 1
2016-09-06T21:15:00.977129: step 3274, loss 0.0276699, acc 0.98
2016-09-06T21:15:01.636934: step 3275, loss 0.0236089, acc 0.98
2016-09-06T21:15:02.339828: step 3276, loss 0.0170267, acc 1
2016-09-06T21:15:03.029458: step 3277, loss 0.177246, acc 0.92
2016-09-06T21:15:03.717539: step 3278, loss 0.0340337, acc 0.98
2016-09-06T21:15:04.397814: step 3279, loss 0.00297332, acc 1
2016-09-06T21:15:05.083032: step 3280, loss 0.00332646, acc 1
2016-09-06T21:15:05.768308: step 3281, loss 0.00380398, acc 1
2016-09-06T21:15:06.424175: step 3282, loss 0.0260642, acc 0.98
2016-09-06T21:15:07.129668: step 3283, loss 0.00881688, acc 1
2016-09-06T21:15:07.802939: step 3284, loss 0.0320826, acc 0.98
2016-09-06T21:15:08.490963: step 3285, loss 0.0240259, acc 0.98
2016-09-06T21:15:09.178438: step 3286, loss 0.0380519, acc 0.98
2016-09-06T21:15:09.856938: step 3287, loss 0.0168084, acc 0.98
2016-09-06T21:15:10.550848: step 3288, loss 0.00675849, acc 1
2016-09-06T21:15:11.207888: step 3289, loss 0.0306984, acc 0.98
2016-09-06T21:15:11.907815: step 3290, loss 0.117523, acc 0.98
2016-09-06T21:15:12.593776: step 3291, loss 0.000567643, acc 1
2016-09-06T21:15:13.279388: step 3292, loss 0.0185578, acc 0.98
2016-09-06T21:15:13.977969: step 3293, loss 0.0710241, acc 0.96
2016-09-06T21:15:14.664150: step 3294, loss 0.0726902, acc 0.98
2016-09-06T21:15:15.354182: step 3295, loss 0.00664752, acc 1
2016-09-06T21:15:16.031244: step 3296, loss 0.105341, acc 0.96
2016-09-06T21:15:16.747070: step 3297, loss 0.109318, acc 0.96
2016-09-06T21:15:17.440548: step 3298, loss 0.0197095, acc 1
2016-09-06T21:15:18.129286: step 3299, loss 0.0082143, acc 1
2016-09-06T21:15:18.816355: step 3300, loss 0.126004, acc 0.96

Evaluation:
2016-09-06T21:15:21.933871: step 3300, loss 1.58614, acc 0.758912

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-3300

2016-09-06T21:15:23.689530: step 3301, loss 0.00491693, acc 1
2016-09-06T21:15:24.391129: step 3302, loss 0.0436389, acc 1
2016-09-06T21:15:25.138312: step 3303, loss 0.0312961, acc 1
2016-09-06T21:15:25.804402: step 3304, loss 0.00177915, acc 1
2016-09-06T21:15:26.483488: step 3305, loss 0.046833, acc 0.98
2016-09-06T21:15:27.151503: step 3306, loss 0.145179, acc 0.96
2016-09-06T21:15:27.851976: step 3307, loss 0.074848, acc 0.98
2016-09-06T21:15:28.530230: step 3308, loss 0.0525401, acc 0.98
2016-09-06T21:15:29.220187: step 3309, loss 0.0132904, acc 1
2016-09-06T21:15:29.923145: step 3310, loss 0.0458682, acc 0.98
2016-09-06T21:15:30.582877: step 3311, loss 0.00907793, acc 1
2016-09-06T21:15:31.266840: step 3312, loss 0.0196273, acc 1
2016-09-06T21:15:31.942562: step 3313, loss 0.0550849, acc 0.98
2016-09-06T21:15:32.628936: step 3314, loss 0.0426685, acc 0.98
2016-09-06T21:15:33.311762: step 3315, loss 0.00974431, acc 1
2016-09-06T21:15:33.986371: step 3316, loss 0.0228493, acc 0.98
2016-09-06T21:15:34.691038: step 3317, loss 0.0234423, acc 1
2016-09-06T21:15:35.362855: step 3318, loss 0.0309049, acc 0.98
2016-09-06T21:15:36.056694: step 3319, loss 0.0631073, acc 0.96
2016-09-06T21:15:36.728151: step 3320, loss 0.0303954, acc 0.98
2016-09-06T21:15:37.427008: step 3321, loss 0.00673213, acc 1
2016-09-06T21:15:38.112660: step 3322, loss 0.00511512, acc 1
2016-09-06T21:15:38.774020: step 3323, loss 0.0746026, acc 0.96
2016-09-06T21:15:39.477340: step 3324, loss 0.0269934, acc 1
2016-09-06T21:15:40.147420: step 3325, loss 0.00781396, acc 1
2016-09-06T21:15:40.840276: step 3326, loss 0.0955917, acc 0.94
2016-09-06T21:15:41.525413: step 3327, loss 0.0311014, acc 1
2016-09-06T21:15:42.196625: step 3328, loss 0.0265711, acc 0.98
2016-09-06T21:15:42.866170: step 3329, loss 0.00351538, acc 1
2016-09-06T21:15:43.557161: step 3330, loss 0.0260692, acc 0.98
2016-09-06T21:15:44.261721: step 3331, loss 0.0407625, acc 0.98
2016-09-06T21:15:44.926156: step 3332, loss 0.127937, acc 0.94
2016-09-06T21:15:45.612985: step 3333, loss 0.042815, acc 0.98
2016-09-06T21:15:46.298380: step 3334, loss 0.0112546, acc 1
2016-09-06T21:15:46.991343: step 3335, loss 0.0546616, acc 0.98
2016-09-06T21:15:47.665066: step 3336, loss 0.0146052, acc 1
2016-09-06T21:15:48.358720: step 3337, loss 0.0591879, acc 0.94
2016-09-06T21:15:49.032883: step 3338, loss 0.0918138, acc 0.96
2016-09-06T21:15:49.679954: step 3339, loss 0.00629464, acc 1
2016-09-06T21:15:50.362434: step 3340, loss 0.00956231, acc 1
2016-09-06T21:15:51.025480: step 3341, loss 0.066366, acc 0.98
2016-09-06T21:15:51.731329: step 3342, loss 0.0470787, acc 0.96
2016-09-06T21:15:52.431639: step 3343, loss 0.0118949, acc 1
2016-09-06T21:15:53.114621: step 3344, loss 0.0816468, acc 0.96
2016-09-06T21:15:53.810142: step 3345, loss 0.0144459, acc 1
2016-09-06T21:15:54.479772: step 3346, loss 0.0459689, acc 0.98
2016-09-06T21:15:55.191205: step 3347, loss 0.000882925, acc 1
2016-09-06T21:15:55.878241: step 3348, loss 0.0431594, acc 0.98
2016-09-06T21:15:56.563458: step 3349, loss 0.0923473, acc 0.96
2016-09-06T21:15:57.269344: step 3350, loss 0.00338828, acc 1
2016-09-06T21:15:57.941534: step 3351, loss 0.00934469, acc 1
2016-09-06T21:15:58.623823: step 3352, loss 0.00386233, acc 1
2016-09-06T21:15:59.278592: step 3353, loss 0.0639165, acc 0.98
2016-09-06T21:15:59.981029: step 3354, loss 0.0466311, acc 0.98
2016-09-06T21:16:00.677240: step 3355, loss 0.0149489, acc 1
2016-09-06T21:16:01.356969: step 3356, loss 0.125661, acc 0.96
2016-09-06T21:16:02.040274: step 3357, loss 0.0122783, acc 1
2016-09-06T21:16:02.710540: step 3358, loss 0.011084, acc 1
2016-09-06T21:16:03.381379: step 3359, loss 0.0190527, acc 1
2016-09-06T21:16:04.058491: step 3360, loss 0.0318109, acc 0.98
2016-09-06T21:16:04.761440: step 3361, loss 0.0405922, acc 0.98
2016-09-06T21:16:05.417752: step 3362, loss 0.0981113, acc 0.96
2016-09-06T21:16:06.127103: step 3363, loss 0.0820552, acc 0.98
2016-09-06T21:16:06.820846: step 3364, loss 0.031767, acc 0.96
2016-09-06T21:16:07.505883: step 3365, loss 0.0140535, acc 1
2016-09-06T21:16:08.187477: step 3366, loss 0.0816363, acc 0.96
2016-09-06T21:16:08.884061: step 3367, loss 0.0774671, acc 0.94
2016-09-06T21:16:09.619072: step 3368, loss 0.0869921, acc 0.98
2016-09-06T21:16:10.302525: step 3369, loss 0.0552769, acc 0.96
2016-09-06T21:16:10.995351: step 3370, loss 0.0637305, acc 0.98
2016-09-06T21:16:11.685227: step 3371, loss 0.0119253, acc 1
2016-09-06T21:16:12.369848: step 3372, loss 0.0310102, acc 0.98
2016-09-06T21:16:13.060259: step 3373, loss 0.0766659, acc 0.96
2016-09-06T21:16:13.750677: step 3374, loss 0.0620498, acc 0.96
2016-09-06T21:16:14.461211: step 3375, loss 0.0469523, acc 0.98
2016-09-06T21:16:15.127381: step 3376, loss 0.0360262, acc 1
2016-09-06T21:16:15.795387: step 3377, loss 0.0497891, acc 0.98
2016-09-06T21:16:16.477838: step 3378, loss 0.0541074, acc 0.96
2016-09-06T21:16:17.183054: step 3379, loss 0.0400331, acc 0.98
2016-09-06T21:16:17.900623: step 3380, loss 0.0302655, acc 0.98
2016-09-06T21:16:18.570868: step 3381, loss 0.0130763, acc 1
2016-09-06T21:16:19.274145: step 3382, loss 0.0193595, acc 0.98
2016-09-06T21:16:19.930287: step 3383, loss 0.0692504, acc 0.94
2016-09-06T21:16:20.592167: step 3384, loss 0.0675849, acc 0.98
2016-09-06T21:16:21.256221: step 3385, loss 0.0893493, acc 0.96
2016-09-06T21:16:21.936306: step 3386, loss 0.0589114, acc 0.96
2016-09-06T21:16:22.604359: step 3387, loss 0.0294016, acc 0.98
2016-09-06T21:16:23.285311: step 3388, loss 0.0550305, acc 0.96
2016-09-06T21:16:23.987163: step 3389, loss 0.0167737, acc 0.98
2016-09-06T21:16:24.650858: step 3390, loss 0.0212688, acc 1
2016-09-06T21:16:25.359434: step 3391, loss 0.0435184, acc 0.98
2016-09-06T21:16:26.050046: step 3392, loss 0.0511611, acc 0.98
2016-09-06T21:16:26.744326: step 3393, loss 0.0199206, acc 1
2016-09-06T21:16:27.414370: step 3394, loss 0.0348349, acc 0.98
2016-09-06T21:16:28.097916: step 3395, loss 0.0872084, acc 0.98
2016-09-06T21:16:28.776346: step 3396, loss 0.0280301, acc 1
2016-09-06T21:16:29.439295: step 3397, loss 0.0419638, acc 0.98
2016-09-06T21:16:30.121497: step 3398, loss 0.0124503, acc 1
2016-09-06T21:16:30.779015: step 3399, loss 0.00461831, acc 1
2016-09-06T21:16:31.449128: step 3400, loss 0.0610558, acc 0.96

Evaluation:
2016-09-06T21:16:34.591696: step 3400, loss 1.67528, acc 0.768293

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-3400

2016-09-06T21:16:36.352231: step 3401, loss 0.0455134, acc 0.98
2016-09-06T21:16:37.020263: step 3402, loss 0.00912809, acc 1
2016-09-06T21:16:37.711620: step 3403, loss 0.0408159, acc 0.98
2016-09-06T21:16:38.422194: step 3404, loss 0.0458654, acc 0.98
2016-09-06T21:16:39.088614: step 3405, loss 0.026391, acc 0.98
2016-09-06T21:16:39.796086: step 3406, loss 0.0168861, acc 0.98
2016-09-06T21:16:40.488891: step 3407, loss 0.0052047, acc 1
2016-09-06T21:16:41.177097: step 3408, loss 0.0567063, acc 0.98
2016-09-06T21:16:41.871051: step 3409, loss 0.0130371, acc 1
2016-09-06T21:16:42.552387: step 3410, loss 0.0733208, acc 0.96
2016-09-06T21:16:43.249026: step 3411, loss 0.0284406, acc 1
2016-09-06T21:16:43.910139: step 3412, loss 0.0471623, acc 0.96
2016-09-06T21:16:44.615109: step 3413, loss 0.0286371, acc 1
2016-09-06T21:16:45.307542: step 3414, loss 0.00289628, acc 1
2016-09-06T21:16:46.006904: step 3415, loss 0.0344537, acc 1
2016-09-06T21:16:46.684259: step 3416, loss 0.0350082, acc 0.98
2016-09-06T21:16:47.362182: step 3417, loss 0.00811387, acc 1
2016-09-06T21:16:48.062811: step 3418, loss 0.0730734, acc 0.94
2016-09-06T21:16:48.730495: step 3419, loss 0.175707, acc 0.94
2016-09-06T21:16:49.418152: step 3420, loss 0.00120967, acc 1
2016-09-06T21:16:50.094203: step 3421, loss 0.01385, acc 1
2016-09-06T21:16:50.758798: step 3422, loss 0.0330372, acc 0.96
2016-09-06T21:16:51.430889: step 3423, loss 0.0326779, acc 0.98
2016-09-06T21:16:52.120925: step 3424, loss 0.0599958, acc 0.98
2016-09-06T21:16:52.808239: step 3425, loss 0.180366, acc 0.96
2016-09-06T21:16:53.491431: step 3426, loss 0.0783594, acc 0.96
2016-09-06T21:16:54.217402: step 3427, loss 0.0057466, acc 1
2016-09-06T21:16:54.922090: step 3428, loss 0.00390364, acc 1
2016-09-06T21:16:55.619404: step 3429, loss 0.117943, acc 0.96
2016-09-06T21:16:56.316043: step 3430, loss 0.041001, acc 0.98
2016-09-06T21:16:57.024764: step 3431, loss 0.065931, acc 0.96
2016-09-06T21:16:57.729367: step 3432, loss 0.0484644, acc 0.98
2016-09-06T21:16:58.423916: step 3433, loss 0.177775, acc 0.96
2016-09-06T21:16:59.112210: step 3434, loss 0.113158, acc 0.98
2016-09-06T21:16:59.815220: step 3435, loss 0.0386619, acc 0.98
2016-09-06T21:17:00.557268: step 3436, loss 0.00125257, acc 1
2016-09-06T21:17:01.241591: step 3437, loss 0.0428537, acc 0.98
2016-09-06T21:17:01.926228: step 3438, loss 0.0622813, acc 0.94
2016-09-06T21:17:02.608371: step 3439, loss 0.0389689, acc 0.96
2016-09-06T21:17:03.290338: step 3440, loss 0.00460508, acc 1
2016-09-06T21:17:03.969840: step 3441, loss 0.0318839, acc 0.98
2016-09-06T21:17:04.650049: step 3442, loss 0.0825089, acc 0.96
2016-09-06T21:17:05.322708: step 3443, loss 0.0323264, acc 0.98
2016-09-06T21:17:06.001365: step 3444, loss 0.00090589, acc 1
2016-09-06T21:17:06.680112: step 3445, loss 0.0132505, acc 1
2016-09-06T21:17:07.373073: step 3446, loss 0.0178404, acc 0.98
2016-09-06T21:17:08.069320: step 3447, loss 0.00289237, acc 1
2016-09-06T21:17:08.761051: step 3448, loss 0.077922, acc 0.92
2016-09-06T21:17:09.435347: step 3449, loss 0.0507144, acc 0.98
2016-09-06T21:17:10.109281: step 3450, loss 0.00739714, acc 1
2016-09-06T21:17:10.783743: step 3451, loss 0.126125, acc 0.96
2016-09-06T21:17:11.480252: step 3452, loss 0.0206832, acc 1
2016-09-06T21:17:12.190760: step 3453, loss 0.0362483, acc 1
2016-09-06T21:17:12.843146: step 3454, loss 0.0193602, acc 1
2016-09-06T21:17:13.519374: step 3455, loss 0.00882256, acc 1
2016-09-06T21:17:14.161596: step 3456, loss 0.00366134, acc 1
2016-09-06T21:17:14.873479: step 3457, loss 0.0187849, acc 1
2016-09-06T21:17:15.606888: step 3458, loss 0.0311479, acc 0.98
2016-09-06T21:17:16.300127: step 3459, loss 0.0363064, acc 0.98
2016-09-06T21:17:17.009297: step 3460, loss 0.0154118, acc 1
2016-09-06T21:17:17.712470: step 3461, loss 0.00732164, acc 1
2016-09-06T21:17:18.414249: step 3462, loss 0.0360953, acc 1
2016-09-06T21:17:19.097267: step 3463, loss 0.00337425, acc 1
2016-09-06T21:17:19.792238: step 3464, loss 0.0122759, acc 1
2016-09-06T21:17:20.488013: step 3465, loss 0.00395738, acc 1
2016-09-06T21:17:21.130796: step 3466, loss 0.0232754, acc 1
2016-09-06T21:17:21.834494: step 3467, loss 0.0317274, acc 0.98
2016-09-06T21:17:22.506731: step 3468, loss 0.0162645, acc 1
2016-09-06T21:17:23.192436: step 3469, loss 0.0565491, acc 0.96
2016-09-06T21:17:23.870281: step 3470, loss 0.0183027, acc 1
2016-09-06T21:17:24.560829: step 3471, loss 0.0163277, acc 0.98
2016-09-06T21:17:25.232947: step 3472, loss 0.001784, acc 1
2016-09-06T21:17:25.903131: step 3473, loss 0.0186712, acc 1
2016-09-06T21:17:26.615806: step 3474, loss 0.0107581, acc 1
2016-09-06T21:17:27.286252: step 3475, loss 0.023881, acc 1
2016-09-06T21:17:27.957199: step 3476, loss 0.0448269, acc 0.98
2016-09-06T21:17:28.656296: step 3477, loss 0.0123095, acc 1
2016-09-06T21:17:29.343518: step 3478, loss 0.0371634, acc 0.98
2016-09-06T21:17:30.024418: step 3479, loss 0.0441605, acc 0.96
2016-09-06T21:17:30.721403: step 3480, loss 0.0159109, acc 1
2016-09-06T21:17:31.432453: step 3481, loss 0.0217359, acc 0.98
2016-09-06T21:17:32.134085: step 3482, loss 0.000633752, acc 1
2016-09-06T21:17:32.811583: step 3483, loss 0.000664627, acc 1
2016-09-06T21:17:33.480317: step 3484, loss 0.0711617, acc 0.96
2016-09-06T21:17:34.168958: step 3485, loss 0.0145629, acc 1
2016-09-06T21:17:34.856918: step 3486, loss 0.0602241, acc 0.98
2016-09-06T21:17:35.525527: step 3487, loss 0.0123215, acc 1
2016-09-06T21:17:36.225691: step 3488, loss 0.0227999, acc 1
2016-09-06T21:17:36.891760: step 3489, loss 0.07216, acc 0.94
2016-09-06T21:17:37.573055: step 3490, loss 0.101585, acc 0.96
2016-09-06T21:17:38.261601: step 3491, loss 0.0257394, acc 0.98
2016-09-06T21:17:38.953944: step 3492, loss 0.0571348, acc 0.96
2016-09-06T21:17:39.660496: step 3493, loss 0.0183638, acc 0.98
2016-09-06T21:17:40.343387: step 3494, loss 0.0183108, acc 0.98
2016-09-06T21:17:41.032450: step 3495, loss 0.0731501, acc 0.98
2016-09-06T21:17:41.732763: step 3496, loss 0.00169768, acc 1
2016-09-06T21:17:42.417805: step 3497, loss 0.0548135, acc 0.96
2016-09-06T21:17:43.087684: step 3498, loss 0.00719005, acc 1
2016-09-06T21:17:43.773035: step 3499, loss 0.0291163, acc 1
2016-09-06T21:17:44.459900: step 3500, loss 0.0425353, acc 0.98

Evaluation:
2016-09-06T21:17:47.579929: step 3500, loss 1.92623, acc 0.754221

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-3500

2016-09-06T21:17:49.330856: step 3501, loss 0.00662641, acc 1
2016-09-06T21:17:49.989811: step 3502, loss 0.0353357, acc 0.98
2016-09-06T21:17:50.706379: step 3503, loss 0.0017709, acc 1
2016-09-06T21:17:51.391439: step 3504, loss 0.0785955, acc 0.96
2016-09-06T21:17:52.051152: step 3505, loss 0.0210013, acc 1
2016-09-06T21:17:52.707862: step 3506, loss 0.0240734, acc 0.98
2016-09-06T21:17:53.373314: step 3507, loss 0.0104575, acc 1
2016-09-06T21:17:54.052740: step 3508, loss 0.0127444, acc 1
2016-09-06T21:17:54.746060: step 3509, loss 0.0119179, acc 1
2016-09-06T21:17:55.450774: step 3510, loss 0.0598243, acc 0.98
2016-09-06T21:17:56.123920: step 3511, loss 0.142844, acc 0.94
2016-09-06T21:17:56.809905: step 3512, loss 0.017703, acc 1
2016-09-06T21:17:57.493046: step 3513, loss 0.0139409, acc 1
2016-09-06T21:17:58.173694: step 3514, loss 0.0326281, acc 1
2016-09-06T21:17:58.877347: step 3515, loss 0.0124648, acc 1
2016-09-06T21:17:59.569552: step 3516, loss 0.0255029, acc 0.98
2016-09-06T21:18:00.284899: step 3517, loss 0.000372883, acc 1
2016-09-06T21:18:00.965317: step 3518, loss 0.0272379, acc 0.98
2016-09-06T21:18:01.644207: step 3519, loss 0.100687, acc 0.98
2016-09-06T21:18:02.345720: step 3520, loss 0.00447471, acc 1
2016-09-06T21:18:03.034517: step 3521, loss 0.000455642, acc 1
2016-09-06T21:18:03.716213: step 3522, loss 0.0312203, acc 0.98
2016-09-06T21:18:04.393379: step 3523, loss 0.0233237, acc 0.98
2016-09-06T21:18:05.087731: step 3524, loss 0.0338465, acc 0.98
2016-09-06T21:18:05.743499: step 3525, loss 0.0548911, acc 0.98
2016-09-06T21:18:06.416002: step 3526, loss 0.00920474, acc 1
2016-09-06T21:18:07.099387: step 3527, loss 0.0763762, acc 0.96
2016-09-06T21:18:07.790987: step 3528, loss 0.0460302, acc 0.98
2016-09-06T21:18:08.487466: step 3529, loss 0.0151029, acc 1
2016-09-06T21:18:09.169705: step 3530, loss 0.020946, acc 1
2016-09-06T21:18:09.871523: step 3531, loss 0.0180509, acc 1
2016-09-06T21:18:10.541327: step 3532, loss 0.0127349, acc 1
2016-09-06T21:18:11.210206: step 3533, loss 0.0483845, acc 0.96
2016-09-06T21:18:11.892138: step 3534, loss 0.0309994, acc 0.98
2016-09-06T21:18:12.578949: step 3535, loss 0.00415407, acc 1
2016-09-06T21:18:13.260181: step 3536, loss 0.0834174, acc 0.96
2016-09-06T21:18:13.953417: step 3537, loss 0.0250395, acc 1
2016-09-06T21:18:14.679591: step 3538, loss 0.370723, acc 0.92
2016-09-06T21:18:15.322783: step 3539, loss 0.0363967, acc 0.98
2016-09-06T21:18:16.007003: step 3540, loss 0.0261137, acc 1
2016-09-06T21:18:16.697787: step 3541, loss 0.00921866, acc 1
2016-09-06T21:18:17.389973: step 3542, loss 0.067239, acc 0.94
2016-09-06T21:18:18.065501: step 3543, loss 0.0588659, acc 0.96
2016-09-06T21:18:18.767198: step 3544, loss 0.00540205, acc 1
2016-09-06T21:18:19.473672: step 3545, loss 0.0162679, acc 0.98
2016-09-06T21:18:20.148848: step 3546, loss 0.046573, acc 0.96
2016-09-06T21:18:20.833101: step 3547, loss 0.0331365, acc 0.98
2016-09-06T21:18:21.526839: step 3548, loss 0.0452891, acc 0.98
2016-09-06T21:18:22.190429: step 3549, loss 0.0161292, acc 0.98
2016-09-06T21:18:22.900981: step 3550, loss 0.0343134, acc 0.98
2016-09-06T21:18:23.594343: step 3551, loss 0.054326, acc 0.96
2016-09-06T21:18:24.288921: step 3552, loss 0.0136055, acc 1
2016-09-06T21:18:24.955574: step 3553, loss 0.0205829, acc 1
2016-09-06T21:18:25.639817: step 3554, loss 0.0133651, acc 1
2016-09-06T21:18:26.310358: step 3555, loss 0.0128276, acc 1
2016-09-06T21:18:26.995030: step 3556, loss 0.0358732, acc 0.98
2016-09-06T21:18:27.680929: step 3557, loss 0.0520192, acc 0.96
2016-09-06T21:18:28.372991: step 3558, loss 0.00603935, acc 1
2016-09-06T21:18:29.100171: step 3559, loss 0.00121708, acc 1
2016-09-06T21:18:29.761384: step 3560, loss 0.00821379, acc 1
2016-09-06T21:18:30.457838: step 3561, loss 0.0524183, acc 0.96
2016-09-06T21:18:31.137434: step 3562, loss 0.0218299, acc 1
2016-09-06T21:18:31.825800: step 3563, loss 0.00164075, acc 1
2016-09-06T21:18:32.510114: step 3564, loss 0.0309628, acc 0.98
2016-09-06T21:18:33.199852: step 3565, loss 0.0245925, acc 1
2016-09-06T21:18:33.893638: step 3566, loss 0.026327, acc 0.98
2016-09-06T21:18:34.564146: step 3567, loss 0.0405717, acc 0.98
2016-09-06T21:18:35.278240: step 3568, loss 0.00303373, acc 1
2016-09-06T21:18:35.990782: step 3569, loss 0.000415133, acc 1
2016-09-06T21:18:36.682654: step 3570, loss 0.0671776, acc 0.96
2016-09-06T21:18:37.357268: step 3571, loss 0.0440631, acc 0.96
2016-09-06T21:18:38.036321: step 3572, loss 0.0611, acc 0.96
2016-09-06T21:18:38.751287: step 3573, loss 0.0323773, acc 0.98
2016-09-06T21:18:39.446381: step 3574, loss 0.00589036, acc 1
2016-09-06T21:18:40.129216: step 3575, loss 0.0121927, acc 1
2016-09-06T21:18:40.829124: step 3576, loss 0.0310772, acc 0.98
2016-09-06T21:18:41.538769: step 3577, loss 0.0391167, acc 0.98
2016-09-06T21:18:42.240242: step 3578, loss 0.0109813, acc 1
2016-09-06T21:18:42.899174: step 3579, loss 0.0885901, acc 0.98
2016-09-06T21:18:43.614086: step 3580, loss 0.0224341, acc 1
2016-09-06T21:18:44.293451: step 3581, loss 0.0015624, acc 1
2016-09-06T21:18:44.982651: step 3582, loss 0.0809207, acc 0.98
2016-09-06T21:18:45.654643: step 3583, loss 0.106954, acc 0.96
2016-09-06T21:18:46.350536: step 3584, loss 0.00682767, acc 1
2016-09-06T21:18:47.038890: step 3585, loss 0.0344829, acc 0.98
2016-09-06T21:18:47.705697: step 3586, loss 0.0246994, acc 0.98
2016-09-06T21:18:48.437520: step 3587, loss 0.101363, acc 0.96
2016-09-06T21:18:49.137429: step 3588, loss 0.0226405, acc 1
2016-09-06T21:18:49.826973: step 3589, loss 0.00962905, acc 1
2016-09-06T21:18:50.513093: step 3590, loss 0.0203261, acc 1
2016-09-06T21:18:51.188497: step 3591, loss 0.0441471, acc 0.98
2016-09-06T21:18:51.870149: step 3592, loss 0.121664, acc 0.96
2016-09-06T21:18:52.554525: step 3593, loss 0.00225249, acc 1
2016-09-06T21:18:53.251200: step 3594, loss 0.00244895, acc 1
2016-09-06T21:18:53.935521: step 3595, loss 0.0374126, acc 0.96
2016-09-06T21:18:54.622358: step 3596, loss 0.0201429, acc 1
2016-09-06T21:18:55.286179: step 3597, loss 0.0231669, acc 0.98
2016-09-06T21:18:55.988774: step 3598, loss 0.0139911, acc 1
2016-09-06T21:18:56.675883: step 3599, loss 0.00120602, acc 1
2016-09-06T21:18:57.345401: step 3600, loss 0.0193255, acc 1

Evaluation:
2016-09-06T21:19:00.511900: step 3600, loss 1.60236, acc 0.756098

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-3600

2016-09-06T21:19:02.228934: step 3601, loss 0.0691828, acc 0.96
2016-09-06T21:19:02.949155: step 3602, loss 0.0181749, acc 1
2016-09-06T21:19:03.646176: step 3603, loss 0.0125555, acc 1
2016-09-06T21:19:04.320069: step 3604, loss 0.0701123, acc 0.94
2016-09-06T21:19:04.997246: step 3605, loss 0.0062956, acc 1
2016-09-06T21:19:05.668042: step 3606, loss 0.00448685, acc 1
2016-09-06T21:19:06.356430: step 3607, loss 0.0147673, acc 1
2016-09-06T21:19:07.020862: step 3608, loss 0.165569, acc 0.94
2016-09-06T21:19:07.708052: step 3609, loss 0.0185356, acc 0.98
2016-09-06T21:19:08.402502: step 3610, loss 0.0390821, acc 0.98
2016-09-06T21:19:09.084230: step 3611, loss 0.0221983, acc 1
2016-09-06T21:19:09.773739: step 3612, loss 0.0114359, acc 1
2016-09-06T21:19:10.464151: step 3613, loss 0.0121124, acc 1
2016-09-06T21:19:11.178677: step 3614, loss 0.0332527, acc 0.98
2016-09-06T21:19:11.837137: step 3615, loss 0.0500961, acc 0.96
2016-09-06T21:19:12.541795: step 3616, loss 0.0334043, acc 0.98
2016-09-06T21:19:13.231163: step 3617, loss 0.0498944, acc 0.98
2016-09-06T21:19:13.931250: step 3618, loss 0.0215855, acc 0.98
2016-09-06T21:19:14.609251: step 3619, loss 0.0921996, acc 0.98
2016-09-06T21:19:15.298537: step 3620, loss 0.00557205, acc 1
2016-09-06T21:19:16.020524: step 3621, loss 0.0369416, acc 1
2016-09-06T21:19:16.702522: step 3622, loss 0.0397595, acc 0.96
2016-09-06T21:19:17.392391: step 3623, loss 0.0398725, acc 0.98
2016-09-06T21:19:18.075082: step 3624, loss 0.0127913, acc 1
2016-09-06T21:19:18.743708: step 3625, loss 0.0415344, acc 0.98
2016-09-06T21:19:19.440694: step 3626, loss 0.0150304, acc 1
2016-09-06T21:19:20.121635: step 3627, loss 0.0126569, acc 1
2016-09-06T21:19:20.812688: step 3628, loss 0.448793, acc 0.94
2016-09-06T21:19:21.461029: step 3629, loss 0.0256451, acc 0.98
2016-09-06T21:19:22.146993: step 3630, loss 0.126029, acc 0.92
2016-09-06T21:19:22.835438: step 3631, loss 0.0338175, acc 0.98
2016-09-06T21:19:23.543308: step 3632, loss 0.00962023, acc 1
2016-09-06T21:19:24.233656: step 3633, loss 0.0595646, acc 0.96
2016-09-06T21:19:24.929227: step 3634, loss 0.116272, acc 0.92
2016-09-06T21:19:25.624882: step 3635, loss 0.0880777, acc 0.96
2016-09-06T21:19:26.285092: step 3636, loss 0.0158712, acc 0.98
2016-09-06T21:19:26.987481: step 3637, loss 0.0124934, acc 1
2016-09-06T21:19:27.672782: step 3638, loss 0.000816763, acc 1
2016-09-06T21:19:28.386619: step 3639, loss 0.00400795, acc 1
2016-09-06T21:19:29.077365: step 3640, loss 0.031632, acc 1
2016-09-06T21:19:29.751298: step 3641, loss 0.0437544, acc 0.98
2016-09-06T21:19:30.462092: step 3642, loss 0.0321243, acc 0.96
2016-09-06T21:19:31.140743: step 3643, loss 0.0175544, acc 0.98
2016-09-06T21:19:31.823202: step 3644, loss 0.0112781, acc 1
2016-09-06T21:19:32.510830: step 3645, loss 0.142396, acc 0.94
2016-09-06T21:19:33.199882: step 3646, loss 0.0116416, acc 1
2016-09-06T21:19:33.886992: step 3647, loss 0.0453512, acc 0.98
2016-09-06T21:19:34.526680: step 3648, loss 0.0140332, acc 1
2016-09-06T21:19:35.226077: step 3649, loss 0.0236056, acc 1
2016-09-06T21:19:35.883694: step 3650, loss 0.0764346, acc 0.96
2016-09-06T21:19:36.597767: step 3651, loss 0.115737, acc 0.96
2016-09-06T21:19:37.288177: step 3652, loss 0.0380927, acc 0.98
2016-09-06T21:19:37.977091: step 3653, loss 0.0909583, acc 0.96
2016-09-06T21:19:38.657752: step 3654, loss 0.145719, acc 0.96
2016-09-06T21:19:39.338162: step 3655, loss 0.0464147, acc 0.96
2016-09-06T21:19:40.007851: step 3656, loss 0.0401812, acc 0.98
2016-09-06T21:19:40.665684: step 3657, loss 0.0318065, acc 0.98
2016-09-06T21:19:41.344371: step 3658, loss 0.0352432, acc 0.98
2016-09-06T21:19:42.036863: step 3659, loss 0.0044997, acc 1
2016-09-06T21:19:42.735023: step 3660, loss 0.0354946, acc 0.98
2016-09-06T21:19:43.406394: step 3661, loss 0.0637177, acc 0.96
2016-09-06T21:19:44.088067: step 3662, loss 0.0859037, acc 0.96
2016-09-06T21:19:44.790515: step 3663, loss 0.0214105, acc 0.98
2016-09-06T21:19:45.455923: step 3664, loss 0.0179088, acc 0.98
2016-09-06T21:19:46.162235: step 3665, loss 0.0415942, acc 0.98
2016-09-06T21:19:46.829643: step 3666, loss 0.0557059, acc 0.98
2016-09-06T21:19:47.503219: step 3667, loss 0.0526088, acc 0.98
2016-09-06T21:19:48.190103: step 3668, loss 0.026697, acc 0.98
2016-09-06T21:19:48.882309: step 3669, loss 0.0261992, acc 1
2016-09-06T21:19:49.575021: step 3670, loss 0.0337244, acc 0.98
2016-09-06T21:19:50.244173: step 3671, loss 0.0750759, acc 0.98
2016-09-06T21:19:50.949362: step 3672, loss 0.0293771, acc 0.98
2016-09-06T21:19:51.627373: step 3673, loss 0.0200123, acc 0.98
2016-09-06T21:19:52.309622: step 3674, loss 0.00336038, acc 1
2016-09-06T21:19:52.983082: step 3675, loss 0.00186124, acc 1
2016-09-06T21:19:53.668248: step 3676, loss 0.0103844, acc 1
2016-09-06T21:19:54.370178: step 3677, loss 0.0192662, acc 0.98
2016-09-06T21:19:55.058914: step 3678, loss 0.0188363, acc 1
2016-09-06T21:19:55.749791: step 3679, loss 0.0313662, acc 0.98
2016-09-06T21:19:56.433859: step 3680, loss 0.0895942, acc 0.94
2016-09-06T21:19:57.109495: step 3681, loss 0.00181648, acc 1
2016-09-06T21:19:57.792615: step 3682, loss 0.025898, acc 0.98
2016-09-06T21:19:58.477329: step 3683, loss 0.0168748, acc 1
2016-09-06T21:19:59.186566: step 3684, loss 0.0705781, acc 0.98
2016-09-06T21:19:59.857326: step 3685, loss 0.014503, acc 1
2016-09-06T21:20:00.628492: step 3686, loss 0.136447, acc 0.96
2016-09-06T21:20:01.304379: step 3687, loss 0.0165007, acc 0.98
2016-09-06T21:20:02.006319: step 3688, loss 0.0238619, acc 0.98
2016-09-06T21:20:02.696765: step 3689, loss 0.0189527, acc 0.98
2016-09-06T21:20:03.360395: step 3690, loss 0.0193289, acc 0.98
2016-09-06T21:20:04.048419: step 3691, loss 0.045254, acc 0.96
2016-09-06T21:20:04.712403: step 3692, loss 0.0413089, acc 0.98
2016-09-06T21:20:05.429928: step 3693, loss 0.0262745, acc 1
2016-09-06T21:20:06.131292: step 3694, loss 0.0262667, acc 1
2016-09-06T21:20:06.827616: step 3695, loss 0.00616093, acc 1
2016-09-06T21:20:07.514801: step 3696, loss 0.00607141, acc 1
2016-09-06T21:20:08.214067: step 3697, loss 0.0263127, acc 1
2016-09-06T21:20:08.934494: step 3698, loss 0.0351992, acc 0.98
2016-09-06T21:20:09.604253: step 3699, loss 0.0135446, acc 1
2016-09-06T21:20:10.292056: step 3700, loss 0.000700023, acc 1

Evaluation:
2016-09-06T21:20:13.425489: step 3700, loss 1.63579, acc 0.772983

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-3700

2016-09-06T21:20:15.131594: step 3701, loss 0.0376118, acc 0.96
2016-09-06T21:20:15.814850: step 3702, loss 0.0375275, acc 0.98
2016-09-06T21:20:16.488291: step 3703, loss 0.168235, acc 0.94
2016-09-06T21:20:17.170664: step 3704, loss 0.0642208, acc 0.96
2016-09-06T21:20:17.885626: step 3705, loss 0.0319704, acc 0.98
2016-09-06T21:20:18.582387: step 3706, loss 0.0815624, acc 0.96
2016-09-06T21:20:19.265329: step 3707, loss 0.0423802, acc 0.98
2016-09-06T21:20:19.936421: step 3708, loss 0.0355746, acc 0.98
2016-09-06T21:20:20.634925: step 3709, loss 0.0142743, acc 1
2016-09-06T21:20:21.323150: step 3710, loss 0.0775313, acc 0.94
2016-09-06T21:20:22.021564: step 3711, loss 0.0372032, acc 0.98
2016-09-06T21:20:22.682321: step 3712, loss 0.00417002, acc 1
2016-09-06T21:20:23.377884: step 3713, loss 0.0248086, acc 0.98
2016-09-06T21:20:24.060222: step 3714, loss 0.0690687, acc 0.96
2016-09-06T21:20:24.745975: step 3715, loss 0.0135306, acc 1
2016-09-06T21:20:25.430823: step 3716, loss 0.00944773, acc 1
2016-09-06T21:20:26.135749: step 3717, loss 0.0581008, acc 0.96
2016-09-06T21:20:26.830568: step 3718, loss 0.00417171, acc 1
2016-09-06T21:20:27.481664: step 3719, loss 0.0280939, acc 0.98
2016-09-06T21:20:28.180646: step 3720, loss 0.0336463, acc 0.98
2016-09-06T21:20:28.873455: step 3721, loss 0.0426997, acc 0.98
2016-09-06T21:20:29.562953: step 3722, loss 0.0443123, acc 0.98
2016-09-06T21:20:30.237094: step 3723, loss 0.0209452, acc 1
2016-09-06T21:20:30.926501: step 3724, loss 0.01404, acc 1
2016-09-06T21:20:31.619440: step 3725, loss 0.0280247, acc 0.98
2016-09-06T21:20:32.315630: step 3726, loss 0.0637228, acc 0.98
2016-09-06T21:20:33.014917: step 3727, loss 0.026246, acc 1
2016-09-06T21:20:33.670387: step 3728, loss 0.0312716, acc 0.98
2016-09-06T21:20:34.364616: step 3729, loss 0.0334763, acc 0.98
2016-09-06T21:20:35.045367: step 3730, loss 0.0035144, acc 1
2016-09-06T21:20:35.724864: step 3731, loss 0.0114613, acc 1
2016-09-06T21:20:36.413946: step 3732, loss 0.0215345, acc 1
2016-09-06T21:20:37.104384: step 3733, loss 0.0303962, acc 0.98
2016-09-06T21:20:37.805029: step 3734, loss 0.0311532, acc 1
2016-09-06T21:20:38.477895: step 3735, loss 0.0271922, acc 0.98
2016-09-06T21:20:39.164291: step 3736, loss 0.134563, acc 0.98
2016-09-06T21:20:39.862230: step 3737, loss 0.0174353, acc 1
2016-09-06T21:20:40.558104: step 3738, loss 0.0725179, acc 0.96
2016-09-06T21:20:41.245222: step 3739, loss 0.00758456, acc 1
2016-09-06T21:20:41.927059: step 3740, loss 0.170006, acc 0.94
2016-09-06T21:20:42.638928: step 3741, loss 0.0138916, acc 1
2016-09-06T21:20:43.309994: step 3742, loss 0.0149969, acc 1
2016-09-06T21:20:43.996724: step 3743, loss 0.0512161, acc 0.98
2016-09-06T21:20:44.681981: step 3744, loss 0.0384302, acc 0.98
2016-09-06T21:20:45.376706: step 3745, loss 0.0191291, acc 1
2016-09-06T21:20:46.058496: step 3746, loss 0.026584, acc 1
2016-09-06T21:20:46.748254: step 3747, loss 0.0214173, acc 1
2016-09-06T21:20:47.432949: step 3748, loss 0.0140255, acc 1
2016-09-06T21:20:48.151672: step 3749, loss 0.0659991, acc 0.96
2016-09-06T21:20:48.847389: step 3750, loss 0.0117384, acc 1
2016-09-06T21:20:49.536008: step 3751, loss 0.0222604, acc 1
2016-09-06T21:20:50.232507: step 3752, loss 0.0617156, acc 0.98
2016-09-06T21:20:50.926352: step 3753, loss 0.017912, acc 0.98
2016-09-06T21:20:51.580703: step 3754, loss 0.00221558, acc 1
2016-09-06T21:20:52.281828: step 3755, loss 0.0716209, acc 0.94
2016-09-06T21:20:52.950076: step 3756, loss 0.0504516, acc 0.96
2016-09-06T21:20:53.625225: step 3757, loss 0.0420603, acc 0.98
2016-09-06T21:20:54.315440: step 3758, loss 0.0320138, acc 1
2016-09-06T21:20:55.015229: step 3759, loss 0.0280205, acc 0.98
2016-09-06T21:20:55.710885: step 3760, loss 0.00126676, acc 1
2016-09-06T21:20:56.376140: step 3761, loss 0.0356548, acc 0.96
2016-09-06T21:20:57.067725: step 3762, loss 0.00289624, acc 1
2016-09-06T21:20:57.725356: step 3763, loss 0.0923636, acc 0.96
2016-09-06T21:20:58.421614: step 3764, loss 0.223222, acc 0.96
2016-09-06T21:20:59.090021: step 3765, loss 0.0209475, acc 0.98
2016-09-06T21:20:59.769017: step 3766, loss 0.00513011, acc 1
2016-09-06T21:21:00.489890: step 3767, loss 0.04125, acc 0.98
2016-09-06T21:21:01.175529: step 3768, loss 0.0370242, acc 1
2016-09-06T21:21:01.877315: step 3769, loss 0.0324944, acc 1
2016-09-06T21:21:02.553348: step 3770, loss 0.0418158, acc 0.98
2016-09-06T21:21:03.241578: step 3771, loss 0.0151541, acc 1
2016-09-06T21:21:03.940774: step 3772, loss 0.049976, acc 0.98
2016-09-06T21:21:04.623945: step 3773, loss 0.0968523, acc 0.98
2016-09-06T21:21:05.310149: step 3774, loss 0.0534783, acc 0.96
2016-09-06T21:21:06.000304: step 3775, loss 0.0124466, acc 1
2016-09-06T21:21:06.702917: step 3776, loss 0.112448, acc 0.96
2016-09-06T21:21:07.394865: step 3777, loss 0.0322403, acc 0.98
2016-09-06T21:21:08.082048: step 3778, loss 0.104832, acc 0.96
2016-09-06T21:21:08.751725: step 3779, loss 0.0641955, acc 0.98
2016-09-06T21:21:09.424738: step 3780, loss 0.0564908, acc 0.98
2016-09-06T21:21:10.095535: step 3781, loss 0.0143722, acc 1
2016-09-06T21:21:10.778381: step 3782, loss 0.0123718, acc 1
2016-09-06T21:21:11.474598: step 3783, loss 0.00962933, acc 1
2016-09-06T21:21:12.139756: step 3784, loss 0.0120209, acc 1
2016-09-06T21:21:12.805913: step 3785, loss 0.0264805, acc 0.98
2016-09-06T21:21:13.495205: step 3786, loss 0.00594893, acc 1
2016-09-06T21:21:14.181865: step 3787, loss 0.031723, acc 0.98
2016-09-06T21:21:14.873117: step 3788, loss 0.0311136, acc 0.98
2016-09-06T21:21:15.575275: step 3789, loss 0.168634, acc 0.96
2016-09-06T21:21:16.278111: step 3790, loss 0.0186567, acc 0.98
2016-09-06T21:21:16.941951: step 3791, loss 0.0145516, acc 1
2016-09-06T21:21:17.614320: step 3792, loss 0.0057166, acc 1
2016-09-06T21:21:18.297005: step 3793, loss 0.0200711, acc 0.98
2016-09-06T21:21:18.982602: step 3794, loss 0.00872344, acc 1
2016-09-06T21:21:19.662098: step 3795, loss 0.00223855, acc 1
2016-09-06T21:21:20.340700: step 3796, loss 0.0521738, acc 0.96
2016-09-06T21:21:21.011179: step 3797, loss 0.0284906, acc 1
2016-09-06T21:21:21.692898: step 3798, loss 0.0219133, acc 1
2016-09-06T21:21:22.396957: step 3799, loss 0.0201751, acc 1
2016-09-06T21:21:23.087038: step 3800, loss 0.0341393, acc 0.98

Evaluation:
2016-09-06T21:21:26.211177: step 3800, loss 1.77315, acc 0.76454

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-3800

2016-09-06T21:21:27.911537: step 3801, loss 0.0360536, acc 0.96
2016-09-06T21:21:28.606580: step 3802, loss 0.0129142, acc 1
2016-09-06T21:21:29.293608: step 3803, loss 0.156486, acc 0.96
2016-09-06T21:21:29.966328: step 3804, loss 0.0168976, acc 0.98
2016-09-06T21:21:30.676106: step 3805, loss 0.0127839, acc 1
2016-09-06T21:21:31.334203: step 3806, loss 0.0206414, acc 1
2016-09-06T21:21:32.027394: step 3807, loss 0.0305876, acc 0.98
2016-09-06T21:21:32.700881: step 3808, loss 0.00956488, acc 1
2016-09-06T21:21:33.374006: step 3809, loss 0.0102362, acc 1
2016-09-06T21:21:34.056455: step 3810, loss 0.01205, acc 1
2016-09-06T21:21:34.769044: step 3811, loss 0.0110055, acc 1
2016-09-06T21:21:35.448236: step 3812, loss 0.0133029, acc 1
2016-09-06T21:21:36.123141: step 3813, loss 0.0375114, acc 0.98
2016-09-06T21:21:36.854342: step 3814, loss 0.0838868, acc 0.96
2016-09-06T21:21:37.560633: step 3815, loss 0.0328785, acc 0.98
2016-09-06T21:21:38.234250: step 3816, loss 0.0501972, acc 0.96
2016-09-06T21:21:38.915883: step 3817, loss 0.0128508, acc 1
2016-09-06T21:21:39.590447: step 3818, loss 0.114078, acc 0.98
2016-09-06T21:21:40.288822: step 3819, loss 0.0176926, acc 1
2016-09-06T21:21:40.966542: step 3820, loss 0.0666487, acc 0.98
2016-09-06T21:21:41.665141: step 3821, loss 0.00513763, acc 1
2016-09-06T21:21:42.347057: step 3822, loss 0.0512537, acc 0.98
2016-09-06T21:21:43.056910: step 3823, loss 0.000187161, acc 1
2016-09-06T21:21:43.767898: step 3824, loss 0.0604682, acc 0.96
2016-09-06T21:21:44.461366: step 3825, loss 0.0586814, acc 0.98
2016-09-06T21:21:45.152351: step 3826, loss 0.0349517, acc 1
2016-09-06T21:21:45.826017: step 3827, loss 0.04406, acc 0.96
2016-09-06T21:21:46.506592: step 3828, loss 0.0783975, acc 0.96
2016-09-06T21:21:47.210398: step 3829, loss 0.0162395, acc 1
2016-09-06T21:21:47.881461: step 3830, loss 0.000194584, acc 1
2016-09-06T21:21:48.580750: step 3831, loss 0.0362587, acc 0.98
2016-09-06T21:21:49.248563: step 3832, loss 0.149827, acc 0.94
2016-09-06T21:21:49.937359: step 3833, loss 0.0223932, acc 0.98
2016-09-06T21:21:50.605484: step 3834, loss 0.00749539, acc 1
2016-09-06T21:21:51.293206: step 3835, loss 0.00635097, acc 1
2016-09-06T21:21:52.006862: step 3836, loss 0.00284028, acc 1
2016-09-06T21:21:52.702330: step 3837, loss 0.0164724, acc 1
2016-09-06T21:21:53.385868: step 3838, loss 0.0245783, acc 1
2016-09-06T21:21:54.057305: step 3839, loss 0.128163, acc 0.96
2016-09-06T21:21:54.712824: step 3840, loss 0.00837927, acc 1
2016-09-06T21:21:55.370243: step 3841, loss 0.0568226, acc 0.98
2016-09-06T21:21:56.064934: step 3842, loss 0.00914708, acc 1
2016-09-06T21:21:56.730185: step 3843, loss 0.0407648, acc 0.98
2016-09-06T21:21:57.418680: step 3844, loss 0.0419902, acc 0.98
2016-09-06T21:21:58.093026: step 3845, loss 0.0221236, acc 1
2016-09-06T21:21:58.763988: step 3846, loss 0.0127523, acc 1
2016-09-06T21:21:59.444608: step 3847, loss 0.0183342, acc 1
2016-09-06T21:22:00.107759: step 3848, loss 0.170074, acc 0.96
2016-09-06T21:22:00.818020: step 3849, loss 0.0420141, acc 0.98
2016-09-06T21:22:01.490419: step 3850, loss 0.00107545, acc 1
2016-09-06T21:22:02.170889: step 3851, loss 0.0934173, acc 0.98
2016-09-06T21:22:02.863791: step 3852, loss 0.0528734, acc 0.98
2016-09-06T21:22:03.545938: step 3853, loss 0.0298711, acc 0.98
2016-09-06T21:22:04.227454: step 3854, loss 0.0227932, acc 1
2016-09-06T21:22:04.922106: step 3855, loss 0.00146008, acc 1
2016-09-06T21:22:05.616051: step 3856, loss 0.00212132, acc 1
2016-09-06T21:22:06.304540: step 3857, loss 0.00869983, acc 1
2016-09-06T21:22:06.991342: step 3858, loss 0.0422957, acc 0.98
2016-09-06T21:22:07.679449: step 3859, loss 0.0540372, acc 0.96
2016-09-06T21:22:08.348469: step 3860, loss 0.0421097, acc 0.96
2016-09-06T21:22:09.033452: step 3861, loss 0.0292204, acc 0.98
2016-09-06T21:22:09.711223: step 3862, loss 0.0512281, acc 0.98
2016-09-06T21:22:10.419111: step 3863, loss 0.0722221, acc 0.96
2016-09-06T21:22:11.109501: step 3864, loss 0.0705319, acc 0.94
2016-09-06T21:22:11.824753: step 3865, loss 0.0654307, acc 0.98
2016-09-06T21:22:12.528001: step 3866, loss 0.00349927, acc 1
2016-09-06T21:22:13.208073: step 3867, loss 0.0365913, acc 0.98
2016-09-06T21:22:13.921328: step 3868, loss 0.0161539, acc 0.98
2016-09-06T21:22:14.585157: step 3869, loss 0.0288736, acc 0.98
2016-09-06T21:22:15.292332: step 3870, loss 0.0849399, acc 0.98
2016-09-06T21:22:15.970492: step 3871, loss 0.00844271, acc 1
2016-09-06T21:22:16.683870: step 3872, loss 0.0634433, acc 0.96
2016-09-06T21:22:17.362034: step 3873, loss 0.0151409, acc 1
2016-09-06T21:22:18.040306: step 3874, loss 0.00134916, acc 1
2016-09-06T21:22:18.732222: step 3875, loss 0.100253, acc 0.94
2016-09-06T21:22:19.440194: step 3876, loss 0.0132969, acc 1
2016-09-06T21:22:20.171350: step 3877, loss 0.0102845, acc 1
2016-09-06T21:22:20.873105: step 3878, loss 0.0167268, acc 1
2016-09-06T21:22:21.558560: step 3879, loss 0.0132516, acc 1
2016-09-06T21:22:22.235371: step 3880, loss 0.0113143, acc 1
2016-09-06T21:22:22.922667: step 3881, loss 0.0894301, acc 0.98
2016-09-06T21:22:23.624543: step 3882, loss 0.0397939, acc 1
2016-09-06T21:22:24.309696: step 3883, loss 0.0426585, acc 0.96
2016-09-06T21:22:24.995074: step 3884, loss 0.0500295, acc 0.98
2016-09-06T21:22:25.681055: step 3885, loss 0.0529144, acc 0.98
2016-09-06T21:22:26.384409: step 3886, loss 0.0092079, acc 1
2016-09-06T21:22:27.078546: step 3887, loss 0.031819, acc 0.98
2016-09-06T21:22:27.770813: step 3888, loss 0.00754143, acc 1
2016-09-06T21:22:28.464315: step 3889, loss 0.0278641, acc 0.98
2016-09-06T21:22:29.159217: step 3890, loss 0.0142936, acc 1
2016-09-06T21:22:29.836009: step 3891, loss 0.0175189, acc 1
2016-09-06T21:22:30.524972: step 3892, loss 0.0134814, acc 1
2016-09-06T21:22:31.193523: step 3893, loss 0.0213922, acc 1
2016-09-06T21:22:31.885023: step 3894, loss 0.0327838, acc 0.98
2016-09-06T21:22:32.569350: step 3895, loss 0.0443361, acc 0.98
2016-09-06T21:22:33.274155: step 3896, loss 0.032557, acc 0.98
2016-09-06T21:22:33.946249: step 3897, loss 0.0157713, acc 1
2016-09-06T21:22:34.635174: step 3898, loss 0.0287484, acc 1
2016-09-06T21:22:35.311872: step 3899, loss 0.0333334, acc 0.98
2016-09-06T21:22:35.989489: step 3900, loss 0.113777, acc 0.98

Evaluation:
2016-09-06T21:22:39.160921: step 3900, loss 1.83177, acc 0.752345

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-3900

2016-09-06T21:22:40.831168: step 3901, loss 0.0154452, acc 0.98
2016-09-06T21:22:41.507745: step 3902, loss 0.0148975, acc 1
2016-09-06T21:22:42.191115: step 3903, loss 0.00948181, acc 1
2016-09-06T21:22:42.896232: step 3904, loss 0.00591004, acc 1
2016-09-06T21:22:43.588974: step 3905, loss 0.0441028, acc 0.96
2016-09-06T21:22:44.266221: step 3906, loss 0.035388, acc 0.98
2016-09-06T21:22:44.967225: step 3907, loss 0.000118028, acc 1
2016-09-06T21:22:45.644829: step 3908, loss 0.044524, acc 0.98
2016-09-06T21:22:46.317026: step 3909, loss 0.0522359, acc 0.98
2016-09-06T21:22:47.002043: step 3910, loss 0.00035068, acc 1
2016-09-06T21:22:47.694698: step 3911, loss 0.0463019, acc 0.96
2016-09-06T21:22:48.365897: step 3912, loss 0.102357, acc 0.96
2016-09-06T21:22:49.051621: step 3913, loss 0.0276318, acc 0.98
2016-09-06T21:22:49.737151: step 3914, loss 0.040623, acc 0.98
2016-09-06T21:22:50.423490: step 3915, loss 0.0187089, acc 1
2016-09-06T21:22:51.120489: step 3916, loss 0.0615164, acc 0.98
2016-09-06T21:22:51.797046: step 3917, loss 0.0279643, acc 1
2016-09-06T21:22:52.497669: step 3918, loss 0.0257296, acc 0.98
2016-09-06T21:22:53.176531: step 3919, loss 0.0128145, acc 1
2016-09-06T21:22:53.867414: step 3920, loss 0.0320287, acc 0.98
2016-09-06T21:22:54.532373: step 3921, loss 0.0120155, acc 1
2016-09-06T21:22:55.220454: step 3922, loss 0.0147541, acc 1
2016-09-06T21:22:55.909879: step 3923, loss 0.0225018, acc 1
2016-09-06T21:22:56.597449: step 3924, loss 0.0100325, acc 1
2016-09-06T21:22:57.289210: step 3925, loss 0.0227148, acc 1
2016-09-06T21:22:57.954574: step 3926, loss 0.037113, acc 0.96
2016-09-06T21:22:58.666201: step 3927, loss 0.00786854, acc 1
2016-09-06T21:22:59.362921: step 3928, loss 0.00318319, acc 1
2016-09-06T21:23:00.047053: step 3929, loss 0.0416424, acc 0.98
2016-09-06T21:23:00.757132: step 3930, loss 0.039976, acc 1
2016-09-06T21:23:01.441206: step 3931, loss 0.00244968, acc 1
2016-09-06T21:23:02.148740: step 3932, loss 0.0167937, acc 1
2016-09-06T21:23:02.824622: step 3933, loss 0.0115591, acc 1
2016-09-06T21:23:03.513275: step 3934, loss 0.000445313, acc 1
2016-09-06T21:23:04.196472: step 3935, loss 0.069714, acc 0.98
2016-09-06T21:23:04.874218: step 3936, loss 0.00622043, acc 1
2016-09-06T21:23:05.547341: step 3937, loss 0.0404178, acc 0.96
2016-09-06T21:23:06.237444: step 3938, loss 0.0281806, acc 1
2016-09-06T21:23:07.008345: step 3939, loss 0.103244, acc 0.98
2016-09-06T21:23:07.682225: step 3940, loss 0.0341672, acc 0.96
2016-09-06T21:23:08.366733: step 3941, loss 0.089451, acc 0.98
2016-09-06T21:23:09.047717: step 3942, loss 0.00968811, acc 1
2016-09-06T21:23:09.724962: step 3943, loss 0.0128382, acc 1
2016-09-06T21:23:10.399056: step 3944, loss 0.0697081, acc 0.98
2016-09-06T21:23:11.054163: step 3945, loss 0.011044, acc 1
2016-09-06T21:23:11.747872: step 3946, loss 0.0189955, acc 1
2016-09-06T21:23:12.416319: step 3947, loss 0.0782074, acc 0.96
2016-09-06T21:23:13.092540: step 3948, loss 0.0738333, acc 0.98
2016-09-06T21:23:13.794173: step 3949, loss 0.0557168, acc 0.98
2016-09-06T21:23:14.485611: step 3950, loss 0.0738316, acc 0.96
2016-09-06T21:23:15.175647: step 3951, loss 0.0352299, acc 0.98
2016-09-06T21:23:15.860594: step 3952, loss 0.00908287, acc 1
2016-09-06T21:23:16.552979: step 3953, loss 0.0600178, acc 0.98
2016-09-06T21:23:17.211484: step 3954, loss 0.0308831, acc 0.98
2016-09-06T21:23:17.908049: step 3955, loss 0.0515728, acc 0.98
2016-09-06T21:23:18.598436: step 3956, loss 0.00692, acc 1
2016-09-06T21:23:19.282489: step 3957, loss 0.00698863, acc 1
2016-09-06T21:23:19.973391: step 3958, loss 0.00188812, acc 1
2016-09-06T21:23:20.671820: step 3959, loss 0.0196895, acc 1
2016-09-06T21:23:21.358149: step 3960, loss 0.0356172, acc 0.98
2016-09-06T21:23:22.055488: step 3961, loss 9.47177e-05, acc 1
2016-09-06T21:23:22.734794: step 3962, loss 0.0534644, acc 0.98
2016-09-06T21:23:23.418162: step 3963, loss 0.0111993, acc 1
2016-09-06T21:23:24.094192: step 3964, loss 0.056728, acc 0.98
2016-09-06T21:23:24.792740: step 3965, loss 0.00868826, acc 1
2016-09-06T21:23:25.470587: step 3966, loss 0.0803382, acc 0.94
2016-09-06T21:23:26.177377: step 3967, loss 0.00723062, acc 1
2016-09-06T21:23:26.842802: step 3968, loss 0.0923146, acc 0.94
2016-09-06T21:23:27.526476: step 3969, loss 0.00488981, acc 1
2016-09-06T21:23:28.221075: step 3970, loss 0.0254791, acc 0.98
2016-09-06T21:23:28.904275: step 3971, loss 0.0948847, acc 0.94
2016-09-06T21:23:29.576517: step 3972, loss 0.107346, acc 0.98
2016-09-06T21:23:30.257920: step 3973, loss 0.121568, acc 0.96
2016-09-06T21:23:30.923666: step 3974, loss 0.0898241, acc 0.98
2016-09-06T21:23:31.580877: step 3975, loss 0.00140363, acc 1
2016-09-06T21:23:32.274863: step 3976, loss 0.0319712, acc 0.98
2016-09-06T21:23:32.966951: step 3977, loss 0.0190784, acc 1
2016-09-06T21:23:33.639454: step 3978, loss 0.0133814, acc 1
2016-09-06T21:23:34.315306: step 3979, loss 0.117399, acc 0.94
2016-09-06T21:23:34.996026: step 3980, loss 0.00696756, acc 1
2016-09-06T21:23:35.707180: step 3981, loss 0.0392123, acc 0.98
2016-09-06T21:23:36.361733: step 3982, loss 0.00570592, acc 1
2016-09-06T21:23:37.054722: step 3983, loss 0.0014481, acc 1
2016-09-06T21:23:37.726355: step 3984, loss 0.0308168, acc 1
2016-09-06T21:23:38.412413: step 3985, loss 0.0315588, acc 1
2016-09-06T21:23:39.082301: step 3986, loss 0.0237494, acc 1
2016-09-06T21:23:39.763386: step 3987, loss 0.00244315, acc 1
2016-09-06T21:23:40.464336: step 3988, loss 0.0343419, acc 1
2016-09-06T21:23:41.129366: step 3989, loss 0.0306899, acc 1
2016-09-06T21:23:41.845472: step 3990, loss 0.0171021, acc 1
2016-09-06T21:23:42.517882: step 3991, loss 0.0602922, acc 0.98
2016-09-06T21:23:43.198279: step 3992, loss 0.0655532, acc 0.96
2016-09-06T21:23:43.882932: step 3993, loss 0.038375, acc 0.98
2016-09-06T21:23:44.570087: step 3994, loss 0.0327776, acc 0.98
2016-09-06T21:23:45.279054: step 3995, loss 0.0176911, acc 1
2016-09-06T21:23:45.962114: step 3996, loss 0.00152279, acc 1
2016-09-06T21:23:46.682065: step 3997, loss 0.000213999, acc 1
2016-09-06T21:23:47.364825: step 3998, loss 0.117397, acc 0.96
2016-09-06T21:23:48.035620: step 3999, loss 0.00144444, acc 1
2016-09-06T21:23:48.729739: step 4000, loss 0.0407247, acc 0.98

Evaluation:
2016-09-06T21:23:51.881962: step 4000, loss 1.95312, acc 0.752345

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-4000

2016-09-06T21:23:53.587474: step 4001, loss 0.0167055, acc 1
2016-09-06T21:23:54.308761: step 4002, loss 0.0438909, acc 0.98
2016-09-06T21:23:55.000725: step 4003, loss 0.0711749, acc 0.96
2016-09-06T21:23:55.674720: step 4004, loss 0.0940961, acc 0.96
2016-09-06T21:23:56.400428: step 4005, loss 0.0257135, acc 0.98
2016-09-06T21:23:57.080287: step 4006, loss 0.0372547, acc 1
2016-09-06T21:23:57.749129: step 4007, loss 0.0044867, acc 1
2016-09-06T21:23:58.438818: step 4008, loss 0.0120538, acc 1
2016-09-06T21:23:59.144478: step 4009, loss 0.0311324, acc 1
2016-09-06T21:23:59.842237: step 4010, loss 0.0297873, acc 1
2016-09-06T21:24:00.562191: step 4011, loss 0.0072954, acc 1
2016-09-06T21:24:01.258430: step 4012, loss 0.0387822, acc 0.98
2016-09-06T21:24:01.946045: step 4013, loss 0.0017068, acc 1
2016-09-06T21:24:02.628492: step 4014, loss 0.0345009, acc 0.98
2016-09-06T21:24:03.288285: step 4015, loss 0.00294157, acc 1
2016-09-06T21:24:03.985204: step 4016, loss 0.115906, acc 0.94
2016-09-06T21:24:04.690848: step 4017, loss 0.0263985, acc 1
2016-09-06T21:24:05.371102: step 4018, loss 0.0265807, acc 1
2016-09-06T21:24:06.074588: step 4019, loss 0.0116672, acc 1
2016-09-06T21:24:06.777155: step 4020, loss 0.0383484, acc 0.98
2016-09-06T21:24:07.471097: step 4021, loss 0.00593938, acc 1
2016-09-06T21:24:08.160265: step 4022, loss 0.0269365, acc 0.98
2016-09-06T21:24:08.830072: step 4023, loss 0.000239997, acc 1
2016-09-06T21:24:09.537255: step 4024, loss 0.0441067, acc 1
2016-09-06T21:24:10.242149: step 4025, loss 0.0340979, acc 0.98
2016-09-06T21:24:10.933892: step 4026, loss 0.0382646, acc 0.98
2016-09-06T21:24:11.617416: step 4027, loss 0.071293, acc 0.96
2016-09-06T21:24:12.301460: step 4028, loss 0.0488902, acc 0.98
2016-09-06T21:24:13.003568: step 4029, loss 0.0563642, acc 0.98
2016-09-06T21:24:13.661996: step 4030, loss 0.0478412, acc 0.98
2016-09-06T21:24:14.377193: step 4031, loss 0.0477164, acc 0.98
2016-09-06T21:24:14.982664: step 4032, loss 0.00100492, acc 1
2016-09-06T21:24:15.667092: step 4033, loss 0.0361597, acc 1
2016-09-06T21:24:16.353194: step 4034, loss 0.0655975, acc 0.98
2016-09-06T21:24:17.039096: step 4035, loss 0.0619974, acc 0.98
2016-09-06T21:24:17.725195: step 4036, loss 0.000201274, acc 1
2016-09-06T21:24:18.411687: step 4037, loss 0.0023953, acc 1
2016-09-06T21:24:19.103101: step 4038, loss 0.0173023, acc 1
2016-09-06T21:24:19.774983: step 4039, loss 0.0154593, acc 1
2016-09-06T21:24:20.447742: step 4040, loss 0.113347, acc 0.98
2016-09-06T21:24:21.134568: step 4041, loss 0.00138548, acc 1
2016-09-06T21:24:21.819601: step 4042, loss 0.0320426, acc 0.98
2016-09-06T21:24:22.491648: step 4043, loss 0.0051281, acc 1
2016-09-06T21:24:23.172237: step 4044, loss 0.142151, acc 0.98
2016-09-06T21:24:23.868499: step 4045, loss 0.0570026, acc 0.94
2016-09-06T21:24:24.558351: step 4046, loss 0.0332417, acc 0.98
2016-09-06T21:24:25.259912: step 4047, loss 0.000292518, acc 1
2016-09-06T21:24:25.933906: step 4048, loss 0.120987, acc 0.96
2016-09-06T21:24:26.621431: step 4049, loss 0.00552096, acc 1
2016-09-06T21:24:27.311521: step 4050, loss 0.0469413, acc 0.96
2016-09-06T21:24:27.975532: step 4051, loss 0.00135037, acc 1
2016-09-06T21:24:28.683358: step 4052, loss 0.0303851, acc 0.98
2016-09-06T21:24:29.330421: step 4053, loss 0.0179175, acc 1
2016-09-06T21:24:30.030011: step 4054, loss 0.0291546, acc 1
2016-09-06T21:24:30.721746: step 4055, loss 0.00356254, acc 1
2016-09-06T21:24:31.397185: step 4056, loss 0.000642375, acc 1
2016-09-06T21:24:32.094120: step 4057, loss 0.0374113, acc 0.98
2016-09-06T21:24:32.788939: step 4058, loss 0.0255486, acc 0.98
2016-09-06T21:24:33.498157: step 4059, loss 0.048514, acc 0.96
2016-09-06T21:24:34.210284: step 4060, loss 0.000518955, acc 1
2016-09-06T21:24:34.897180: step 4061, loss 0.0432614, acc 0.98
2016-09-06T21:24:35.587146: step 4062, loss 0.00912253, acc 1
2016-09-06T21:24:36.254401: step 4063, loss 0.0398534, acc 0.96
2016-09-06T21:24:36.939024: step 4064, loss 0.0774369, acc 0.96
2016-09-06T21:24:37.627451: step 4065, loss 0.00931132, acc 1
2016-09-06T21:24:38.352904: step 4066, loss 0.0369196, acc 0.98
2016-09-06T21:24:39.064190: step 4067, loss 0.00825601, acc 1
2016-09-06T21:24:39.738791: step 4068, loss 0.0442398, acc 0.98
2016-09-06T21:24:40.417692: step 4069, loss 0.0229028, acc 0.98
2016-09-06T21:24:41.100082: step 4070, loss 0.0543374, acc 0.96
2016-09-06T21:24:41.788270: step 4071, loss 0.0824727, acc 0.98
2016-09-06T21:24:42.477050: step 4072, loss 0.00932113, acc 1
2016-09-06T21:24:43.173551: step 4073, loss 0.0509573, acc 0.98
2016-09-06T21:24:43.820241: step 4074, loss 0.0307227, acc 0.98
2016-09-06T21:24:44.485185: step 4075, loss 0.0392641, acc 0.98
2016-09-06T21:24:45.165882: step 4076, loss 0.139968, acc 0.92
2016-09-06T21:24:45.849421: step 4077, loss 0.00743885, acc 1
2016-09-06T21:24:46.522531: step 4078, loss 0.0666161, acc 0.96
2016-09-06T21:24:47.206051: step 4079, loss 0.0171915, acc 1
2016-09-06T21:24:47.897155: step 4080, loss 0.0360762, acc 0.98
2016-09-06T21:24:48.562773: step 4081, loss 0.0126539, acc 1
2016-09-06T21:24:49.286714: step 4082, loss 0.0309126, acc 0.98
2016-09-06T21:24:49.980691: step 4083, loss 0.0156171, acc 0.98
2016-09-06T21:24:50.664905: step 4084, loss 0.0114296, acc 1
2016-09-06T21:24:51.339129: step 4085, loss 0.0074816, acc 1
2016-09-06T21:24:52.027230: step 4086, loss 0.0193593, acc 1
2016-09-06T21:24:52.731504: step 4087, loss 0.0418648, acc 0.98
2016-09-06T21:24:53.397229: step 4088, loss 0.0459957, acc 0.96
2016-09-06T21:24:54.083712: step 4089, loss 0.0154968, acc 1
2016-09-06T21:24:54.774636: step 4090, loss 0.0139414, acc 1
2016-09-06T21:24:55.464296: step 4091, loss 0.0464092, acc 0.96
2016-09-06T21:24:56.160087: step 4092, loss 0.0589204, acc 0.98
2016-09-06T21:24:56.862170: step 4093, loss 0.0235009, acc 1
2016-09-06T21:24:57.572904: step 4094, loss 0.0881344, acc 0.96
2016-09-06T21:24:58.258518: step 4095, loss 0.00184788, acc 1
2016-09-06T21:24:58.954262: step 4096, loss 0.0495669, acc 1
2016-09-06T21:24:59.640004: step 4097, loss 0.0307613, acc 1
2016-09-06T21:25:00.347981: step 4098, loss 0.0217447, acc 0.98
2016-09-06T21:25:01.037775: step 4099, loss 0.000586912, acc 1
2016-09-06T21:25:01.719979: step 4100, loss 0.00664427, acc 1

Evaluation:
2016-09-06T21:25:04.830281: step 4100, loss 1.85277, acc 0.761726

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-4100

2016-09-06T21:25:06.497391: step 4101, loss 0.0757309, acc 0.96
2016-09-06T21:25:07.238601: step 4102, loss 0.0016154, acc 1
2016-09-06T21:25:07.914582: step 4103, loss 0.0280856, acc 0.98
2016-09-06T21:25:08.597946: step 4104, loss 0.0224741, acc 1
2016-09-06T21:25:09.263925: step 4105, loss 0.040439, acc 0.98
2016-09-06T21:25:09.941994: step 4106, loss 0.0237733, acc 0.98
2016-09-06T21:25:10.621151: step 4107, loss 0.0105389, acc 1
2016-09-06T21:25:11.286053: step 4108, loss 0.00225754, acc 1
2016-09-06T21:25:11.982142: step 4109, loss 0.126708, acc 0.98
2016-09-06T21:25:12.640932: step 4110, loss 0.0294954, acc 0.98
2016-09-06T21:25:13.349036: step 4111, loss 0.0118679, acc 1
2016-09-06T21:25:14.039300: step 4112, loss 0.0412978, acc 0.98
2016-09-06T21:25:14.726382: step 4113, loss 0.0619151, acc 0.98
2016-09-06T21:25:15.419285: step 4114, loss 0.0109695, acc 1
2016-09-06T21:25:16.106836: step 4115, loss 0.0066743, acc 1
2016-09-06T21:25:16.812524: step 4116, loss 0.00301846, acc 1
2016-09-06T21:25:17.475830: step 4117, loss 0.00340393, acc 1
2016-09-06T21:25:18.205311: step 4118, loss 0.010866, acc 1
2016-09-06T21:25:18.890518: step 4119, loss 0.0227209, acc 1
2016-09-06T21:25:19.569875: step 4120, loss 0.00663195, acc 1
2016-09-06T21:25:20.252919: step 4121, loss 0.0437069, acc 0.98
2016-09-06T21:25:20.925646: step 4122, loss 0.0152952, acc 0.98
2016-09-06T21:25:21.635388: step 4123, loss 0.00101728, acc 1
2016-09-06T21:25:22.289907: step 4124, loss 0.142852, acc 0.96
2016-09-06T21:25:22.997807: step 4125, loss 0.0593479, acc 0.98
2016-09-06T21:25:23.680970: step 4126, loss 0.0297268, acc 1
2016-09-06T21:25:24.372412: step 4127, loss 0.0457217, acc 0.98
2016-09-06T21:25:25.051607: step 4128, loss 0.0103639, acc 1
2016-09-06T21:25:25.740178: step 4129, loss 0.023842, acc 1
2016-09-06T21:25:26.472190: step 4130, loss 0.00236021, acc 1
2016-09-06T21:25:27.134900: step 4131, loss 0.0905348, acc 0.96
2016-09-06T21:25:27.811205: step 4132, loss 0.00148866, acc 1
2016-09-06T21:25:28.498899: step 4133, loss 0.0601, acc 0.96
2016-09-06T21:25:29.196251: step 4134, loss 0.00695391, acc 1
2016-09-06T21:25:29.886222: step 4135, loss 0.0139492, acc 1
2016-09-06T21:25:30.574357: step 4136, loss 0.109916, acc 0.98
2016-09-06T21:25:31.284854: step 4137, loss 0.0446589, acc 0.96
2016-09-06T21:25:31.957884: step 4138, loss 0.0110654, acc 1
2016-09-06T21:25:32.633089: step 4139, loss 0.0501671, acc 0.98
2016-09-06T21:25:33.329429: step 4140, loss 0.0335596, acc 0.96
2016-09-06T21:25:34.014860: step 4141, loss 0.143412, acc 0.96
2016-09-06T21:25:34.719136: step 4142, loss 0.0152858, acc 1
2016-09-06T21:25:35.391946: step 4143, loss 0.0337803, acc 1
2016-09-06T21:25:36.084403: step 4144, loss 0.0344621, acc 0.98
2016-09-06T21:25:36.757241: step 4145, loss 0.0415542, acc 0.98
2016-09-06T21:25:37.443742: step 4146, loss 0.0508985, acc 0.98
2016-09-06T21:25:38.113286: step 4147, loss 0.0201351, acc 1
2016-09-06T21:25:38.799886: step 4148, loss 0.0551748, acc 0.96
2016-09-06T21:25:39.479811: step 4149, loss 0.0154696, acc 1
2016-09-06T21:25:40.165032: step 4150, loss 0.00860179, acc 1
2016-09-06T21:25:40.875965: step 4151, loss 0.0231645, acc 1
2016-09-06T21:25:41.560937: step 4152, loss 0.0238943, acc 0.98
2016-09-06T21:25:42.268574: step 4153, loss 0.0378862, acc 0.98
2016-09-06T21:25:42.962532: step 4154, loss 0.0548953, acc 0.98
2016-09-06T21:25:43.637019: step 4155, loss 0.00154927, acc 1
2016-09-06T21:25:44.357373: step 4156, loss 0.0440103, acc 0.96
2016-09-06T21:25:45.038189: step 4157, loss 0.00110472, acc 1
2016-09-06T21:25:45.754153: step 4158, loss 0.0442626, acc 0.98
2016-09-06T21:25:46.435743: step 4159, loss 0.0213298, acc 1
2016-09-06T21:25:47.120369: step 4160, loss 0.0424527, acc 0.98
2016-09-06T21:25:47.810005: step 4161, loss 0.0104618, acc 1
2016-09-06T21:25:48.507385: step 4162, loss 0.0104683, acc 1
2016-09-06T21:25:49.192583: step 4163, loss 0.117581, acc 0.94
2016-09-06T21:25:49.855219: step 4164, loss 0.0470073, acc 0.98
2016-09-06T21:25:50.545017: step 4165, loss 0.0872287, acc 0.94
2016-09-06T21:25:51.212845: step 4166, loss 0.0221816, acc 0.98
2016-09-06T21:25:51.896888: step 4167, loss 0.00158188, acc 1
2016-09-06T21:25:52.593448: step 4168, loss 0.0134064, acc 1
2016-09-06T21:25:53.284568: step 4169, loss 0.0131156, acc 1
2016-09-06T21:25:53.986518: step 4170, loss 0.0137125, acc 1
2016-09-06T21:25:54.654920: step 4171, loss 0.015002, acc 1
2016-09-06T21:25:55.352153: step 4172, loss 0.118194, acc 0.96
2016-09-06T21:25:56.010429: step 4173, loss 0.0186583, acc 1
2016-09-06T21:25:56.672885: step 4174, loss 0.0264451, acc 0.98
2016-09-06T21:25:57.368935: step 4175, loss 0.00712746, acc 1
2016-09-06T21:25:58.057000: step 4176, loss 0.0502238, acc 0.98
2016-09-06T21:25:58.725893: step 4177, loss 0.0232265, acc 0.98
2016-09-06T21:25:59.408095: step 4178, loss 0.00311575, acc 1
2016-09-06T21:26:00.106671: step 4179, loss 0.0214516, acc 1
2016-09-06T21:26:00.807828: step 4180, loss 0.17158, acc 0.96
2016-09-06T21:26:01.488647: step 4181, loss 0.00728121, acc 1
2016-09-06T21:26:02.191487: step 4182, loss 0.0288491, acc 0.98
2016-09-06T21:26:02.877190: step 4183, loss 0.0217057, acc 0.98
2016-09-06T21:26:03.557928: step 4184, loss 0.0904213, acc 0.96
2016-09-06T21:26:04.265000: step 4185, loss 0.00734876, acc 1
2016-09-06T21:26:04.984084: step 4186, loss 0.0457067, acc 0.98
2016-09-06T21:26:05.676641: step 4187, loss 0.070995, acc 0.98
2016-09-06T21:26:06.370423: step 4188, loss 0.0336324, acc 1
2016-09-06T21:26:07.074449: step 4189, loss 0.0157488, acc 1
2016-09-06T21:26:07.768468: step 4190, loss 0.0141453, acc 1
2016-09-06T21:26:08.457784: step 4191, loss 0.0181685, acc 0.98
2016-09-06T21:26:09.114796: step 4192, loss 0.03882, acc 0.98
2016-09-06T21:26:09.813003: step 4193, loss 0.0316425, acc 0.98
2016-09-06T21:26:10.470445: step 4194, loss 0.00189908, acc 1
2016-09-06T21:26:11.183321: step 4195, loss 0.00608254, acc 1
2016-09-06T21:26:11.858972: step 4196, loss 0.0202111, acc 0.98
2016-09-06T21:26:12.534661: step 4197, loss 0.109395, acc 0.98
2016-09-06T21:26:13.221721: step 4198, loss 0.0174905, acc 1
2016-09-06T21:26:13.918942: step 4199, loss 0.0340516, acc 0.98
2016-09-06T21:26:14.608741: step 4200, loss 0.0598329, acc 0.96

Evaluation:
2016-09-06T21:26:17.753216: step 4200, loss 1.63615, acc 0.751407

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-4200

2016-09-06T21:26:19.394342: step 4201, loss 0.0159702, acc 1
2016-09-06T21:26:20.065408: step 4202, loss 0.0312787, acc 0.98
2016-09-06T21:26:20.737091: step 4203, loss 0.00238002, acc 1
2016-09-06T21:26:21.396029: step 4204, loss 0.0259079, acc 0.98
2016-09-06T21:26:22.076822: step 4205, loss 0.0228989, acc 1
2016-09-06T21:26:22.753325: step 4206, loss 0.0199641, acc 1
2016-09-06T21:26:23.439955: step 4207, loss 0.00970456, acc 1
2016-09-06T21:26:24.151216: step 4208, loss 0.035816, acc 1
2016-09-06T21:26:24.831997: step 4209, loss 0.0326013, acc 0.98
2016-09-06T21:26:25.505225: step 4210, loss 0.0437888, acc 0.98
2016-09-06T21:26:26.181331: step 4211, loss 0.0222738, acc 1
2016-09-06T21:26:26.880063: step 4212, loss 0.0214014, acc 1
2016-09-06T21:26:27.565667: step 4213, loss 0.00348409, acc 1
2016-09-06T21:26:28.252176: step 4214, loss 0.0837714, acc 0.96
2016-09-06T21:26:28.944225: step 4215, loss 0.00801729, acc 1
2016-09-06T21:26:29.610024: step 4216, loss 0.191388, acc 0.94
2016-09-06T21:26:30.306491: step 4217, loss 0.0107451, acc 1
2016-09-06T21:26:30.997134: step 4218, loss 0.0377985, acc 0.98
2016-09-06T21:26:31.685966: step 4219, loss 0.100514, acc 0.94
2016-09-06T21:26:32.364421: step 4220, loss 0.00325464, acc 1
2016-09-06T21:26:33.043006: step 4221, loss 0.0171623, acc 1
2016-09-06T21:26:33.754918: step 4222, loss 0.0310966, acc 1
2016-09-06T21:26:34.428812: step 4223, loss 0.033475, acc 1
2016-09-06T21:26:35.073050: step 4224, loss 0.0289081, acc 0.977273
2016-09-06T21:26:35.767207: step 4225, loss 0.0193482, acc 1
2016-09-06T21:26:36.450582: step 4226, loss 0.0120079, acc 1
2016-09-06T21:26:37.132473: step 4227, loss 0.0121105, acc 1
2016-09-06T21:26:37.831098: step 4228, loss 0.0243923, acc 0.98
2016-09-06T21:26:38.518914: step 4229, loss 0.051981, acc 0.98
2016-09-06T21:26:39.182159: step 4230, loss 0.0942299, acc 0.94
2016-09-06T21:26:39.880532: step 4231, loss 0.0216176, acc 0.98
2016-09-06T21:26:40.575066: step 4232, loss 0.195405, acc 0.98
2016-09-06T21:26:41.262851: step 4233, loss 0.0484213, acc 0.96
2016-09-06T21:26:41.951869: step 4234, loss 0.0324468, acc 0.98
2016-09-06T21:26:42.625937: step 4235, loss 0.0190567, acc 1
2016-09-06T21:26:43.336650: step 4236, loss 0.0186202, acc 0.98
2016-09-06T21:26:43.999833: step 4237, loss 0.08535, acc 0.92
2016-09-06T21:26:44.694191: step 4238, loss 0.0429916, acc 0.96
2016-09-06T21:26:45.391145: step 4239, loss 0.00276382, acc 1
2016-09-06T21:26:46.070575: step 4240, loss 0.00899472, acc 1
2016-09-06T21:26:46.774473: step 4241, loss 0.0195512, acc 1
2016-09-06T21:26:47.472125: step 4242, loss 0.0488078, acc 0.96
2016-09-06T21:26:48.168392: step 4243, loss 0.00623277, acc 1
2016-09-06T21:26:48.881299: step 4244, loss 0.0295266, acc 0.98
2016-09-06T21:26:49.543913: step 4245, loss 0.0122834, acc 1
2016-09-06T21:26:50.228293: step 4246, loss 0.00926807, acc 1
2016-09-06T21:26:50.901335: step 4247, loss 0.0155662, acc 1
2016-09-06T21:26:51.604627: step 4248, loss 0.00391204, acc 1
2016-09-06T21:26:52.301704: step 4249, loss 0.00269682, acc 1
2016-09-06T21:26:53.007198: step 4250, loss 0.00185441, acc 1
2016-09-06T21:26:53.670414: step 4251, loss 0.0199406, acc 0.98
2016-09-06T21:26:54.364499: step 4252, loss 0.0529639, acc 0.98
2016-09-06T21:26:55.047126: step 4253, loss 0.0385537, acc 0.98
2016-09-06T21:26:55.728252: step 4254, loss 0.0284645, acc 0.98
2016-09-06T21:26:56.407722: step 4255, loss 0.0175097, acc 0.98
2016-09-06T21:26:57.088581: step 4256, loss 0.105709, acc 0.96
2016-09-06T21:26:57.799782: step 4257, loss 0.00329916, acc 1
2016-09-06T21:26:58.512747: step 4258, loss 0.0321623, acc 0.98
2016-09-06T21:26:59.222634: step 4259, loss 0.0158591, acc 1
2016-09-06T21:26:59.911028: step 4260, loss 0.014182, acc 1
2016-09-06T21:27:00.615655: step 4261, loss 0.0190924, acc 1
2016-09-06T21:27:01.304092: step 4262, loss 0.0214063, acc 1
2016-09-06T21:27:01.959425: step 4263, loss 0.0331975, acc 0.98
2016-09-06T21:27:02.654886: step 4264, loss 0.00329953, acc 1
2016-09-06T21:27:03.341074: step 4265, loss 0.0381199, acc 0.98
2016-09-06T21:27:04.030709: step 4266, loss 0.00482994, acc 1
2016-09-06T21:27:04.733330: step 4267, loss 0.0391048, acc 0.98
2016-09-06T21:27:05.428357: step 4268, loss 0.0303314, acc 0.98
2016-09-06T21:27:06.121758: step 4269, loss 0.00703594, acc 1
2016-09-06T21:27:06.800670: step 4270, loss 0.0127808, acc 1
2016-09-06T21:27:07.499774: step 4271, loss 0.0215692, acc 0.98
2016-09-06T21:27:08.175742: step 4272, loss 0.0426681, acc 0.98
2016-09-06T21:27:08.876947: step 4273, loss 0.0752347, acc 0.96
2016-09-06T21:27:09.561734: step 4274, loss 0.0200264, acc 1
2016-09-06T21:27:10.246125: step 4275, loss 0.0123921, acc 1
2016-09-06T21:27:10.914443: step 4276, loss 0.0527689, acc 0.98
2016-09-06T21:27:11.580933: step 4277, loss 0.0028709, acc 1
2016-09-06T21:27:12.290818: step 4278, loss 0.0147943, acc 1
2016-09-06T21:27:12.957472: step 4279, loss 0.0210397, acc 1
2016-09-06T21:27:13.634893: step 4280, loss 0.00168372, acc 1
2016-09-06T21:27:14.313250: step 4281, loss 0.0210529, acc 0.98
2016-09-06T21:27:14.989822: step 4282, loss 0.00756848, acc 1
2016-09-06T21:27:15.673073: step 4283, loss 0.0590784, acc 0.96
2016-09-06T21:27:16.345881: step 4284, loss 0.010775, acc 1
2016-09-06T21:27:17.047672: step 4285, loss 0.0722713, acc 0.94
2016-09-06T21:27:17.708684: step 4286, loss 0.0180943, acc 1
2016-09-06T21:27:18.390616: step 4287, loss 0.00258374, acc 1
2016-09-06T21:27:19.066851: step 4288, loss 0.0355367, acc 0.98
2016-09-06T21:27:19.734749: step 4289, loss 0.0128866, acc 1
2016-09-06T21:27:20.418478: step 4290, loss 0.0729681, acc 0.96
2016-09-06T21:27:21.118732: step 4291, loss 0.0110915, acc 1
2016-09-06T21:27:21.824747: step 4292, loss 0.0144714, acc 1
2016-09-06T21:27:22.489079: step 4293, loss 0.0432045, acc 1
2016-09-06T21:27:23.195178: step 4294, loss 0.00270596, acc 1
2016-09-06T21:27:23.870174: step 4295, loss 0.0235401, acc 1
2016-09-06T21:27:24.550845: step 4296, loss 0.0362836, acc 0.98
2016-09-06T21:27:25.233881: step 4297, loss 0.0249649, acc 0.98
2016-09-06T21:27:25.906961: step 4298, loss 0.0135301, acc 1
2016-09-06T21:27:26.612361: step 4299, loss 0.0325645, acc 0.98
2016-09-06T21:27:27.298053: step 4300, loss 0.0336002, acc 1

Evaluation:
2016-09-06T21:27:30.448663: step 4300, loss 1.98558, acc 0.753283

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-4300

2016-09-06T21:27:32.180342: step 4301, loss 0.0312806, acc 1
2016-09-06T21:27:32.886535: step 4302, loss 0.022294, acc 1
2016-09-06T21:27:33.575033: step 4303, loss 0.00304446, acc 1
2016-09-06T21:27:34.262008: step 4304, loss 0.0190555, acc 0.98
2016-09-06T21:27:34.924719: step 4305, loss 0.0347493, acc 0.96
2016-09-06T21:27:35.599831: step 4306, loss 0.0548261, acc 0.96
2016-09-06T21:27:36.304673: step 4307, loss 0.00152022, acc 1
2016-09-06T21:27:36.974372: step 4308, loss 0.00285368, acc 1
2016-09-06T21:27:37.676568: step 4309, loss 0.00317625, acc 1
2016-09-06T21:27:38.354177: step 4310, loss 0.0153428, acc 0.98
2016-09-06T21:27:39.045477: step 4311, loss 0.0388576, acc 0.96
2016-09-06T21:27:39.731044: step 4312, loss 0.018657, acc 0.98
2016-09-06T21:27:40.424667: step 4313, loss 0.0293131, acc 1
2016-09-06T21:27:41.151399: step 4314, loss 0.0424731, acc 0.98
2016-09-06T21:27:41.811261: step 4315, loss 0.0139916, acc 1
2016-09-06T21:27:42.491293: step 4316, loss 0.0362865, acc 0.98
2016-09-06T21:27:43.186145: step 4317, loss 0.0235057, acc 0.98
2016-09-06T21:27:43.863667: step 4318, loss 0.000468681, acc 1
2016-09-06T21:27:44.555200: step 4319, loss 0.00634014, acc 1
2016-09-06T21:27:45.219215: step 4320, loss 0.0727604, acc 0.96
2016-09-06T21:27:45.968066: step 4321, loss 0.0450563, acc 0.98
2016-09-06T21:27:46.642371: step 4322, loss 0.00151879, acc 1
2016-09-06T21:27:47.323923: step 4323, loss 0.0913241, acc 0.96
2016-09-06T21:27:48.004895: step 4324, loss 0.0459326, acc 0.94
2016-09-06T21:27:48.704203: step 4325, loss 0.0405173, acc 0.98
2016-09-06T21:27:49.406594: step 4326, loss 0.000372676, acc 1
2016-09-06T21:27:50.087784: step 4327, loss 0.0123529, acc 1
2016-09-06T21:27:50.782178: step 4328, loss 0.0880679, acc 0.96
2016-09-06T21:27:51.461085: step 4329, loss 0.0272131, acc 1
2016-09-06T21:27:52.146044: step 4330, loss 0.144029, acc 0.98
2016-09-06T21:27:52.819970: step 4331, loss 0.0242673, acc 1
2016-09-06T21:27:53.504003: step 4332, loss 0.0107511, acc 1
2016-09-06T21:27:54.225599: step 4333, loss 0.0950856, acc 0.96
2016-09-06T21:27:54.892985: step 4334, loss 0.0446836, acc 0.98
2016-09-06T21:27:55.586059: step 4335, loss 0.154769, acc 0.98
2016-09-06T21:27:56.286781: step 4336, loss 0.000124821, acc 1
2016-09-06T21:27:56.967195: step 4337, loss 0.000707816, acc 1
2016-09-06T21:27:57.655918: step 4338, loss 0.0278296, acc 0.98
2016-09-06T21:27:58.333021: step 4339, loss 0.00766587, acc 1
2016-09-06T21:27:59.024989: step 4340, loss 0.0160258, acc 1
2016-09-06T21:27:59.680914: step 4341, loss 0.00506155, acc 1
2016-09-06T21:28:00.422895: step 4342, loss 0.0237389, acc 1
2016-09-06T21:28:01.126828: step 4343, loss 0.103638, acc 0.94
2016-09-06T21:28:01.811971: step 4344, loss 0.00240107, acc 1
2016-09-06T21:28:02.488726: step 4345, loss 0.0255131, acc 1
2016-09-06T21:28:03.175731: step 4346, loss 0.0129066, acc 1
2016-09-06T21:28:03.854614: step 4347, loss 0.0148167, acc 1
2016-09-06T21:28:04.511065: step 4348, loss 0.0182458, acc 0.98
2016-09-06T21:28:05.199027: step 4349, loss 0.00513339, acc 1
2016-09-06T21:28:05.889879: step 4350, loss 0.0746198, acc 0.98
2016-09-06T21:28:06.566326: step 4351, loss 0.00052458, acc 1
2016-09-06T21:28:07.271343: step 4352, loss 0.0127597, acc 1
2016-09-06T21:28:07.951593: step 4353, loss 0.000783479, acc 1
2016-09-06T21:28:08.639231: step 4354, loss 0.0525845, acc 0.96
2016-09-06T21:28:09.295158: step 4355, loss 0.099024, acc 0.92
2016-09-06T21:28:09.991576: step 4356, loss 0.0212022, acc 1
2016-09-06T21:28:10.664354: step 4357, loss 0.047515, acc 0.98
2016-09-06T21:28:11.359780: step 4358, loss 0.0493522, acc 0.96
2016-09-06T21:28:12.032982: step 4359, loss 0.00307095, acc 1
2016-09-06T21:28:12.724699: step 4360, loss 0.123854, acc 0.98
2016-09-06T21:28:13.415943: step 4361, loss 0.117392, acc 0.98
2016-09-06T21:28:14.096044: step 4362, loss 0.000386077, acc 1
2016-09-06T21:28:14.788769: step 4363, loss 0.0126998, acc 1
2016-09-06T21:28:15.463717: step 4364, loss 0.07807, acc 0.96
2016-09-06T21:28:16.149229: step 4365, loss 0.0107285, acc 1
2016-09-06T21:28:16.827455: step 4366, loss 0.068592, acc 0.98
2016-09-06T21:28:17.518674: step 4367, loss 0.0553277, acc 0.96
2016-09-06T21:28:18.199928: step 4368, loss 0.0161109, acc 1
2016-09-06T21:28:18.865215: step 4369, loss 0.126928, acc 0.96
2016-09-06T21:28:19.562679: step 4370, loss 0.103096, acc 0.98
2016-09-06T21:28:20.242029: step 4371, loss 0.0062951, acc 1
2016-09-06T21:28:20.899982: step 4372, loss 0.0637125, acc 0.98
2016-09-06T21:28:21.559085: step 4373, loss 0.043865, acc 1
2016-09-06T21:28:22.244674: step 4374, loss 0.00447087, acc 1
2016-09-06T21:28:22.923078: step 4375, loss 0.0349532, acc 0.98
2016-09-06T21:28:23.598842: step 4376, loss 0.0712008, acc 0.98
2016-09-06T21:28:24.273497: step 4377, loss 0.0355603, acc 0.98
2016-09-06T21:28:24.942655: step 4378, loss 0.101007, acc 0.96
2016-09-06T21:28:25.653575: step 4379, loss 0.0257218, acc 1
2016-09-06T21:28:26.336743: step 4380, loss 0.00910119, acc 1
2016-09-06T21:28:27.005922: step 4381, loss 0.0529307, acc 0.98
2016-09-06T21:28:27.696051: step 4382, loss 0.0431505, acc 0.98
2016-09-06T21:28:28.382319: step 4383, loss 0.00254939, acc 1
2016-09-06T21:28:29.082066: step 4384, loss 0.0442402, acc 0.96
2016-09-06T21:28:29.757055: step 4385, loss 0.0111968, acc 1
2016-09-06T21:28:30.478652: step 4386, loss 0.00206271, acc 1
2016-09-06T21:28:31.146811: step 4387, loss 0.0128739, acc 1
2016-09-06T21:28:31.819838: step 4388, loss 0.0013441, acc 1
2016-09-06T21:28:32.537331: step 4389, loss 0.00163342, acc 1
2016-09-06T21:28:33.195215: step 4390, loss 0.0471161, acc 0.96
2016-09-06T21:28:33.875133: step 4391, loss 0.0837343, acc 0.94
2016-09-06T21:28:34.534872: step 4392, loss 0.0141082, acc 1
2016-09-06T21:28:35.236091: step 4393, loss 0.111602, acc 0.96
2016-09-06T21:28:35.908305: step 4394, loss 0.0127582, acc 1
2016-09-06T21:28:36.594170: step 4395, loss 0.0223062, acc 0.98
2016-09-06T21:28:37.283131: step 4396, loss 0.0187344, acc 1
2016-09-06T21:28:37.963389: step 4397, loss 0.0197164, acc 0.98
2016-09-06T21:28:38.657459: step 4398, loss 0.024285, acc 1
2016-09-06T21:28:39.345145: step 4399, loss 0.0130282, acc 1
2016-09-06T21:28:40.059044: step 4400, loss 0.0306586, acc 0.98

Evaluation:
2016-09-06T21:28:43.195169: step 4400, loss 1.96672, acc 0.745779

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-4400

2016-09-06T21:28:44.864972: step 4401, loss 0.0586672, acc 0.96
2016-09-06T21:28:45.539869: step 4402, loss 0.0976895, acc 0.96
2016-09-06T21:28:46.208098: step 4403, loss 0.0102672, acc 1
2016-09-06T21:28:46.926055: step 4404, loss 0.0535549, acc 0.98
2016-09-06T21:28:47.607748: step 4405, loss 0.0394594, acc 0.98
2016-09-06T21:28:48.289834: step 4406, loss 0.00112449, acc 1
2016-09-06T21:28:48.969521: step 4407, loss 0.14984, acc 0.98
2016-09-06T21:28:49.670785: step 4408, loss 0.00458587, acc 1
2016-09-06T21:28:50.366030: step 4409, loss 0.017831, acc 1
2016-09-06T21:28:51.055567: step 4410, loss 0.0217035, acc 1
2016-09-06T21:28:51.742694: step 4411, loss 0.0449841, acc 0.98
2016-09-06T21:28:52.420394: step 4412, loss 0.0046604, acc 1
2016-09-06T21:28:53.101278: step 4413, loss 0.0343768, acc 0.98
2016-09-06T21:28:53.789585: step 4414, loss 0.0410858, acc 1
2016-09-06T21:28:54.514203: step 4415, loss 0.0417693, acc 0.98
2016-09-06T21:28:55.146047: step 4416, loss 0.00640525, acc 1
2016-09-06T21:28:55.857337: step 4417, loss 0.0994686, acc 0.98
2016-09-06T21:28:56.544869: step 4418, loss 0.00194985, acc 1
2016-09-06T21:28:57.244470: step 4419, loss 0.021981, acc 0.98
2016-09-06T21:28:57.957715: step 4420, loss 0.10676, acc 0.92
2016-09-06T21:28:58.629554: step 4421, loss 0.00205266, acc 1
2016-09-06T21:28:59.326017: step 4422, loss 0.218315, acc 0.98
2016-09-06T21:28:59.998505: step 4423, loss 0.00439744, acc 1
2016-09-06T21:29:00.715858: step 4424, loss 0.0241102, acc 0.98
2016-09-06T21:29:01.392362: step 4425, loss 0.0189042, acc 1
2016-09-06T21:29:02.079036: step 4426, loss 0.00769599, acc 1
2016-09-06T21:29:02.768875: step 4427, loss 0.0244586, acc 0.98
2016-09-06T21:29:03.423921: step 4428, loss 0.0262066, acc 1
2016-09-06T21:29:04.117114: step 4429, loss 0.0852168, acc 0.94
2016-09-06T21:29:04.785064: step 4430, loss 0.0522444, acc 0.96
2016-09-06T21:29:05.467604: step 4431, loss 0.0203884, acc 0.98
2016-09-06T21:29:06.159590: step 4432, loss 0.000640307, acc 1
2016-09-06T21:29:06.847252: step 4433, loss 0.0454748, acc 0.98
2016-09-06T21:29:07.533475: step 4434, loss 0.00760774, acc 1
2016-09-06T21:29:08.225986: step 4435, loss 0.10216, acc 0.96
2016-09-06T21:29:08.947567: step 4436, loss 0.0728343, acc 0.96
2016-09-06T21:29:09.623343: step 4437, loss 0.045998, acc 0.96
2016-09-06T21:29:10.296953: step 4438, loss 0.0230659, acc 1
2016-09-06T21:29:10.985120: step 4439, loss 0.0267512, acc 0.98
2016-09-06T21:29:11.685511: step 4440, loss 0.0361147, acc 0.98
2016-09-06T21:29:12.366658: step 4441, loss 0.005685, acc 1
2016-09-06T21:29:13.046178: step 4442, loss 0.0309851, acc 0.98
2016-09-06T21:29:13.745581: step 4443, loss 0.0354658, acc 0.98
2016-09-06T21:29:14.424380: step 4444, loss 0.137197, acc 0.96
2016-09-06T21:29:15.114120: step 4445, loss 0.0140698, acc 1
2016-09-06T21:29:15.796487: step 4446, loss 0.0543181, acc 0.96
2016-09-06T21:29:16.499898: step 4447, loss 0.0433124, acc 0.96
2016-09-06T21:29:17.202826: step 4448, loss 0.0729831, acc 0.94
2016-09-06T21:29:17.881788: step 4449, loss 0.0695012, acc 0.96
2016-09-06T21:29:18.568787: step 4450, loss 0.0257793, acc 1
2016-09-06T21:29:19.242204: step 4451, loss 0.00970596, acc 1
2016-09-06T21:29:19.943780: step 4452, loss 0.0494097, acc 0.96
2016-09-06T21:29:20.625782: step 4453, loss 0.00254974, acc 1
2016-09-06T21:29:21.309759: step 4454, loss 0.0224977, acc 1
2016-09-06T21:29:22.015218: step 4455, loss 0.0138546, acc 1
2016-09-06T21:29:22.690370: step 4456, loss 0.0184782, acc 1
2016-09-06T21:29:23.386566: step 4457, loss 0.0389723, acc 1
2016-09-06T21:29:24.068029: step 4458, loss 0.00140207, acc 1
2016-09-06T21:29:24.752814: step 4459, loss 0.0279365, acc 1
2016-09-06T21:29:25.437115: step 4460, loss 0.0693269, acc 0.98
2016-09-06T21:29:26.130902: step 4461, loss 0.0697125, acc 0.96
2016-09-06T21:29:26.825289: step 4462, loss 0.0147939, acc 1
2016-09-06T21:29:27.492616: step 4463, loss 0.0240296, acc 0.98
2016-09-06T21:29:28.210434: step 4464, loss 0.00353825, acc 1
2016-09-06T21:29:28.890176: step 4465, loss 0.0507309, acc 0.96
2016-09-06T21:29:29.575204: step 4466, loss 0.0281825, acc 1
2016-09-06T21:29:30.240704: step 4467, loss 0.0518132, acc 0.96
2016-09-06T21:29:30.937843: step 4468, loss 0.0157971, acc 1
2016-09-06T21:29:31.613954: step 4469, loss 0.00227892, acc 1
2016-09-06T21:29:32.280297: step 4470, loss 0.0106822, acc 1
2016-09-06T21:29:32.978283: step 4471, loss 0.0748244, acc 0.98
2016-09-06T21:29:33.652400: step 4472, loss 0.0215893, acc 1
2016-09-06T21:29:34.337513: step 4473, loss 0.0186846, acc 0.98
2016-09-06T21:29:34.993221: step 4474, loss 0.0120334, acc 1
2016-09-06T21:29:35.687396: step 4475, loss 0.00559477, acc 1
2016-09-06T21:29:36.374905: step 4476, loss 0.00988726, acc 1
2016-09-06T21:29:37.076185: step 4477, loss 0.0221896, acc 1
2016-09-06T21:29:37.785386: step 4478, loss 0.112924, acc 0.98
2016-09-06T21:29:38.437561: step 4479, loss 0.0162813, acc 1
2016-09-06T21:29:39.138242: step 4480, loss 0.0148956, acc 0.98
2016-09-06T21:29:39.826445: step 4481, loss 0.0399105, acc 0.98
2016-09-06T21:29:40.509898: step 4482, loss 0.0266762, acc 0.98
2016-09-06T21:29:41.205205: step 4483, loss 0.0156244, acc 0.98
2016-09-06T21:29:41.881395: step 4484, loss 0.0893283, acc 0.98
2016-09-06T21:29:42.589131: step 4485, loss 0.131728, acc 0.96
2016-09-06T21:29:43.245946: step 4486, loss 0.00128963, acc 1
2016-09-06T21:29:43.918039: step 4487, loss 0.0122204, acc 1
2016-09-06T21:29:44.631974: step 4488, loss 0.0177586, acc 0.98
2016-09-06T21:29:45.312071: step 4489, loss 0.0521825, acc 0.98
2016-09-06T21:29:45.998952: step 4490, loss 0.0204943, acc 0.98
2016-09-06T21:29:46.683866: step 4491, loss 0.0508054, acc 0.96
2016-09-06T21:29:47.385340: step 4492, loss 0.0212174, acc 0.98
2016-09-06T21:29:48.061573: step 4493, loss 0.0481528, acc 0.96
2016-09-06T21:29:48.752471: step 4494, loss 0.0383765, acc 0.98
2016-09-06T21:29:49.472641: step 4495, loss 0.00711696, acc 1
2016-09-06T21:29:50.144358: step 4496, loss 0.0306544, acc 0.98
2016-09-06T21:29:50.833293: step 4497, loss 0.0130531, acc 1
2016-09-06T21:29:51.528401: step 4498, loss 0.0204856, acc 0.98
2016-09-06T21:29:52.238390: step 4499, loss 0.062187, acc 0.94
2016-09-06T21:29:52.933281: step 4500, loss 0.0507927, acc 0.96

Evaluation:
2016-09-06T21:29:56.063460: step 4500, loss 1.8873, acc 0.753283

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-4500

2016-09-06T21:29:57.702390: step 4501, loss 0.00404051, acc 1
2016-09-06T21:29:58.381827: step 4502, loss 0.0242729, acc 0.98
2016-09-06T21:29:59.066272: step 4503, loss 0.0356216, acc 0.98
2016-09-06T21:29:59.743185: step 4504, loss 0.00594833, acc 1
2016-09-06T21:30:00.476074: step 4505, loss 0.0420966, acc 0.96
2016-09-06T21:30:01.157541: step 4506, loss 0.0147679, acc 1
2016-09-06T21:30:01.859423: step 4507, loss 0.0020291, acc 1
2016-09-06T21:30:02.547058: step 4508, loss 0.101064, acc 0.96
2016-09-06T21:30:03.222287: step 4509, loss 0.0076363, acc 1
2016-09-06T21:30:03.925727: step 4510, loss 0.0927907, acc 0.96
2016-09-06T21:30:04.633661: step 4511, loss 0.0273679, acc 0.98
2016-09-06T21:30:05.305952: step 4512, loss 0.0174476, acc 0.98
2016-09-06T21:30:05.961504: step 4513, loss 0.0336863, acc 1
2016-09-06T21:30:06.670993: step 4514, loss 0.175049, acc 0.94
2016-09-06T21:30:07.335510: step 4515, loss 0.0718136, acc 0.96
2016-09-06T21:30:08.018240: step 4516, loss 0.0460517, acc 0.98
2016-09-06T21:30:08.706252: step 4517, loss 0.0196523, acc 1
2016-09-06T21:30:09.363287: step 4518, loss 0.00129953, acc 1
2016-09-06T21:30:10.043091: step 4519, loss 0.091529, acc 0.96
2016-09-06T21:30:10.726271: step 4520, loss 0.00609439, acc 1
2016-09-06T21:30:11.414390: step 4521, loss 0.0321684, acc 1
2016-09-06T21:30:12.101922: step 4522, loss 0.0215055, acc 1
2016-09-06T21:30:12.805467: step 4523, loss 0.0254464, acc 0.98
2016-09-06T21:30:13.491352: step 4524, loss 0.0974158, acc 0.98
2016-09-06T21:30:14.175434: step 4525, loss 0.0081356, acc 1
2016-09-06T21:30:14.857808: step 4526, loss 0.101099, acc 0.98
2016-09-06T21:30:15.549106: step 4527, loss 0.043106, acc 0.98
2016-09-06T21:30:16.256699: step 4528, loss 0.00324415, acc 1
2016-09-06T21:30:16.923681: step 4529, loss 0.0496821, acc 0.98
2016-09-06T21:30:17.591127: step 4530, loss 0.0199207, acc 0.98
2016-09-06T21:30:18.273048: step 4531, loss 0.0306509, acc 0.98
2016-09-06T21:30:18.977456: step 4532, loss 0.00134415, acc 1
2016-09-06T21:30:19.659097: step 4533, loss 0.0228404, acc 0.98
2016-09-06T21:30:20.363612: step 4534, loss 0.0196074, acc 0.98
2016-09-06T21:30:21.076380: step 4535, loss 0.031469, acc 0.98
2016-09-06T21:30:21.752594: step 4536, loss 0.00618544, acc 1
2016-09-06T21:30:22.434872: step 4537, loss 0.0693474, acc 0.98
2016-09-06T21:30:23.122573: step 4538, loss 0.018042, acc 0.98
2016-09-06T21:30:23.811079: step 4539, loss 0.0178752, acc 0.98
2016-09-06T21:30:24.524031: step 4540, loss 0.0649101, acc 0.96
2016-09-06T21:30:25.209914: step 4541, loss 0.000499887, acc 1
2016-09-06T21:30:25.932167: step 4542, loss 0.0545878, acc 0.98
2016-09-06T21:30:26.606833: step 4543, loss 0.0353033, acc 0.96
2016-09-06T21:30:27.277585: step 4544, loss 0.0219868, acc 0.98
2016-09-06T21:30:27.964251: step 4545, loss 0.0121204, acc 1
2016-09-06T21:30:28.643533: step 4546, loss 0.0209507, acc 0.98
2016-09-06T21:30:29.340774: step 4547, loss 0.0330916, acc 1
2016-09-06T21:30:29.984795: step 4548, loss 0.0815797, acc 0.96
2016-09-06T21:30:30.680830: step 4549, loss 0.0430803, acc 0.98
2016-09-06T21:30:31.347293: step 4550, loss 0.00169201, acc 1
2016-09-06T21:30:32.034032: step 4551, loss 0.0122934, acc 1
2016-09-06T21:30:32.725633: step 4552, loss 0.0305953, acc 0.98
2016-09-06T21:30:33.419608: step 4553, loss 0.0404323, acc 0.98
2016-09-06T21:30:34.096483: step 4554, loss 0.0143005, acc 0.98
2016-09-06T21:30:34.772825: step 4555, loss 0.00460787, acc 1
2016-09-06T21:30:35.456514: step 4556, loss 0.226154, acc 0.96
2016-09-06T21:30:36.139605: step 4557, loss 0.0394736, acc 0.98
2016-09-06T21:30:36.808584: step 4558, loss 0.0207485, acc 1
2016-09-06T21:30:37.497482: step 4559, loss 0.00358163, acc 1
2016-09-06T21:30:38.206018: step 4560, loss 0.0262846, acc 0.98
2016-09-06T21:30:38.902524: step 4561, loss 0.0390062, acc 0.98
2016-09-06T21:30:39.584694: step 4562, loss 0.013312, acc 1
2016-09-06T21:30:40.288977: step 4563, loss 0.0021295, acc 1
2016-09-06T21:30:40.947805: step 4564, loss 0.0542256, acc 0.98
2016-09-06T21:30:41.668217: step 4565, loss 0.0392976, acc 1
2016-09-06T21:30:42.348567: step 4566, loss 0.0207324, acc 1
2016-09-06T21:30:43.012876: step 4567, loss 0.0143393, acc 1
2016-09-06T21:30:43.706340: step 4568, loss 0.0551439, acc 0.96
2016-09-06T21:30:44.392403: step 4569, loss 0.00162603, acc 1
2016-09-06T21:30:45.089266: step 4570, loss 0.0264343, acc 0.98
2016-09-06T21:30:45.761670: step 4571, loss 0.0673524, acc 0.98
2016-09-06T21:30:46.451949: step 4572, loss 0.120931, acc 0.98
2016-09-06T21:30:47.150432: step 4573, loss 0.00924664, acc 1
2016-09-06T21:30:47.842889: step 4574, loss 0.0936506, acc 0.92
2016-09-06T21:30:48.522388: step 4575, loss 0.019806, acc 0.98
2016-09-06T21:30:49.225782: step 4576, loss 0.0434989, acc 0.96
2016-09-06T21:30:49.927931: step 4577, loss 0.000976491, acc 1
2016-09-06T21:30:50.590139: step 4578, loss 0.026925, acc 1
2016-09-06T21:30:51.267996: step 4579, loss 0.00276639, acc 1
2016-09-06T21:30:51.958986: step 4580, loss 0.0446448, acc 0.96
2016-09-06T21:30:52.641372: step 4581, loss 0.0318952, acc 0.98
2016-09-06T21:30:53.337273: step 4582, loss 0.0573307, acc 0.96
2016-09-06T21:30:54.016387: step 4583, loss 0.000653456, acc 1
2016-09-06T21:30:54.705139: step 4584, loss 0.0441191, acc 0.96
2016-09-06T21:30:55.374007: step 4585, loss 0.00300771, acc 1
2016-09-06T21:30:56.048461: step 4586, loss 0.0212406, acc 0.98
2016-09-06T21:30:56.747875: step 4587, loss 0.0251962, acc 0.98
2016-09-06T21:30:57.450180: step 4588, loss 0.0216557, acc 1
2016-09-06T21:30:58.134323: step 4589, loss 0.0281156, acc 0.98
2016-09-06T21:30:58.840675: step 4590, loss 0.0371248, acc 0.98
2016-09-06T21:30:59.550215: step 4591, loss 0.0286937, acc 0.98
2016-09-06T21:31:00.233413: step 4592, loss 0.0164115, acc 1
2016-09-06T21:31:00.900801: step 4593, loss 0.0318443, acc 0.98
2016-09-06T21:31:01.581627: step 4594, loss 0.0360115, acc 0.96
2016-09-06T21:31:02.273416: step 4595, loss 0.03015, acc 0.98
2016-09-06T21:31:02.978879: step 4596, loss 0.0337417, acc 0.98
2016-09-06T21:31:03.639925: step 4597, loss 0.0153233, acc 0.98
2016-09-06T21:31:04.345566: step 4598, loss 0.00714113, acc 1
2016-09-06T21:31:05.019637: step 4599, loss 0.0330068, acc 0.98
2016-09-06T21:31:05.701848: step 4600, loss 0.0189223, acc 1

Evaluation:
2016-09-06T21:31:08.808954: step 4600, loss 2.02614, acc 0.744841

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-4600

2016-09-06T21:31:10.426901: step 4601, loss 0.0174843, acc 1
2016-09-06T21:31:11.097319: step 4602, loss 0.0315122, acc 0.98
2016-09-06T21:31:11.763910: step 4603, loss 0.0349605, acc 0.98
2016-09-06T21:31:12.418821: step 4604, loss 0.0100345, acc 1
2016-09-06T21:31:13.096485: step 4605, loss 0.0252287, acc 0.98
2016-09-06T21:31:13.762164: step 4606, loss 0.0332169, acc 1
2016-09-06T21:31:14.470058: step 4607, loss 0.00279165, acc 1
2016-09-06T21:31:15.136538: step 4608, loss 0.0244168, acc 1
2016-09-06T21:31:15.817463: step 4609, loss 0.0720038, acc 0.96
2016-09-06T21:31:16.512002: step 4610, loss 0.0103764, acc 1
2016-09-06T21:31:17.206546: step 4611, loss 0.0115618, acc 1
2016-09-06T21:31:17.901027: step 4612, loss 0.0104713, acc 1
2016-09-06T21:31:18.612892: step 4613, loss 0.0118851, acc 1
2016-09-06T21:31:19.280908: step 4614, loss 0.0107041, acc 1
2016-09-06T21:31:19.970169: step 4615, loss 0.00746048, acc 1
2016-09-06T21:31:20.635548: step 4616, loss 0.0405256, acc 0.98
2016-09-06T21:31:21.307945: step 4617, loss 0.00181175, acc 1
2016-09-06T21:31:21.986289: step 4618, loss 0.0153269, acc 1
2016-09-06T21:31:22.676777: step 4619, loss 0.000132998, acc 1
2016-09-06T21:31:23.353403: step 4620, loss 0.039798, acc 0.98
2016-09-06T21:31:24.050168: step 4621, loss 0.0269523, acc 1
2016-09-06T21:31:24.745202: step 4622, loss 0.105255, acc 0.98
2016-09-06T21:31:25.401319: step 4623, loss 0.00191207, acc 1
2016-09-06T21:31:26.090705: step 4624, loss 0.0400161, acc 0.98
2016-09-06T21:31:26.793143: step 4625, loss 0.00956112, acc 1
2016-09-06T21:31:27.501304: step 4626, loss 0.000776864, acc 1
2016-09-06T21:31:28.193487: step 4627, loss 0.0186167, acc 1
2016-09-06T21:31:28.871282: step 4628, loss 0.0166651, acc 0.98
2016-09-06T21:31:29.580549: step 4629, loss 0.0150102, acc 1
2016-09-06T21:31:30.247845: step 4630, loss 0.0160457, acc 1
2016-09-06T21:31:30.937363: step 4631, loss 0.00601648, acc 1
2016-09-06T21:31:31.615093: step 4632, loss 0.0957565, acc 0.98
2016-09-06T21:31:32.300307: step 4633, loss 0.0171661, acc 0.98
2016-09-06T21:31:32.981271: step 4634, loss 0.0154726, acc 0.98
2016-09-06T21:31:33.652040: step 4635, loss 0.0326506, acc 0.98
2016-09-06T21:31:34.363451: step 4636, loss 0.00117974, acc 1
2016-09-06T21:31:35.037367: step 4637, loss 0.000549853, acc 1
2016-09-06T21:31:35.734639: step 4638, loss 0.0489509, acc 0.96
2016-09-06T21:31:36.475829: step 4639, loss 0.0305365, acc 0.98
2016-09-06T21:31:37.172177: step 4640, loss 0.0500698, acc 0.96
2016-09-06T21:31:37.886404: step 4641, loss 0.0195764, acc 0.98
2016-09-06T21:31:38.551352: step 4642, loss 0.0296196, acc 0.98
2016-09-06T21:31:39.261912: step 4643, loss 0.0745463, acc 0.98
2016-09-06T21:31:39.946165: step 4644, loss 0.0171518, acc 1
2016-09-06T21:31:40.620861: step 4645, loss 0.000620642, acc 1
2016-09-06T21:31:41.297542: step 4646, loss 0.0128269, acc 1
2016-09-06T21:31:41.959248: step 4647, loss 0.014442, acc 1
2016-09-06T21:31:42.649997: step 4648, loss 0.0193966, acc 0.98
2016-09-06T21:31:43.336961: step 4649, loss 0.149815, acc 0.98
2016-09-06T21:31:44.054968: step 4650, loss 0.00843949, acc 1
2016-09-06T21:31:44.726878: step 4651, loss 0.129191, acc 0.96
2016-09-06T21:31:45.429644: step 4652, loss 0.0153303, acc 1
2016-09-06T21:31:46.107564: step 4653, loss 0.0118196, acc 1
2016-09-06T21:31:46.801131: step 4654, loss 0.0117423, acc 1
2016-09-06T21:31:47.498631: step 4655, loss 0.00876343, acc 1
2016-09-06T21:31:48.162734: step 4656, loss 0.0159181, acc 1
2016-09-06T21:31:48.883844: step 4657, loss 0.0454031, acc 0.96
2016-09-06T21:31:49.562208: step 4658, loss 0.000615869, acc 1
2016-09-06T21:31:50.234388: step 4659, loss 0.0230941, acc 0.98
2016-09-06T21:31:50.931708: step 4660, loss 0.00093903, acc 1
2016-09-06T21:31:51.611886: step 4661, loss 0.00620717, acc 1
2016-09-06T21:31:52.288267: step 4662, loss 0.0150981, acc 1
2016-09-06T21:31:52.953023: step 4663, loss 0.0260184, acc 1
2016-09-06T21:31:53.655569: step 4664, loss 0.00110432, acc 1
2016-09-06T21:31:54.332721: step 4665, loss 0.0521401, acc 0.94
2016-09-06T21:31:55.017224: step 4666, loss 0.0142784, acc 1
2016-09-06T21:31:55.685223: step 4667, loss 0.0257623, acc 0.98
2016-09-06T21:31:56.372767: step 4668, loss 0.0324308, acc 0.98
2016-09-06T21:31:57.064565: step 4669, loss 0.0164527, acc 0.98
2016-09-06T21:31:57.775207: step 4670, loss 0.0201132, acc 1
2016-09-06T21:31:58.480300: step 4671, loss 0.0758554, acc 0.98
2016-09-06T21:31:59.183965: step 4672, loss 0.0203063, acc 0.98
2016-09-06T21:31:59.866911: step 4673, loss 0.00170566, acc 1
2016-09-06T21:32:00.584128: step 4674, loss 0.0357936, acc 1
2016-09-06T21:32:01.271855: step 4675, loss 0.000826581, acc 1
2016-09-06T21:32:01.953077: step 4676, loss 0.0594235, acc 0.98
2016-09-06T21:32:02.624253: step 4677, loss 0.0522217, acc 1
2016-09-06T21:32:03.331911: step 4678, loss 0.00861532, acc 1
2016-09-06T21:32:04.031456: step 4679, loss 0.0228078, acc 1
2016-09-06T21:32:04.707556: step 4680, loss 0.015126, acc 0.98
2016-09-06T21:32:05.383777: step 4681, loss 0.0130873, acc 1
2016-09-06T21:32:06.077898: step 4682, loss 0.0186716, acc 1
2016-09-06T21:32:06.755216: step 4683, loss 0.0233068, acc 1
2016-09-06T21:32:07.438426: step 4684, loss 0.00258674, acc 1
2016-09-06T21:32:08.145299: step 4685, loss 0.0185614, acc 0.98
2016-09-06T21:32:08.839570: step 4686, loss 0.00647713, acc 1
2016-09-06T21:32:09.541119: step 4687, loss 0.0154489, acc 0.98
2016-09-06T21:32:10.234768: step 4688, loss 0.0434912, acc 0.96
2016-09-06T21:32:10.925909: step 4689, loss 0.0107404, acc 1
2016-09-06T21:32:11.631309: step 4690, loss 0.0186219, acc 1
2016-09-06T21:32:12.318235: step 4691, loss 0.0410681, acc 0.98
2016-09-06T21:32:13.020157: step 4692, loss 0.00429369, acc 1
2016-09-06T21:32:13.702010: step 4693, loss 0.0363514, acc 0.98
2016-09-06T21:32:14.371041: step 4694, loss 0.0207509, acc 0.98
2016-09-06T21:32:15.076476: step 4695, loss 0.00114873, acc 1
2016-09-06T21:32:15.753527: step 4696, loss 0.000116484, acc 1
2016-09-06T21:32:16.446912: step 4697, loss 0.0960533, acc 0.96
2016-09-06T21:32:17.139486: step 4698, loss 0.0211067, acc 0.98
2016-09-06T21:32:17.824593: step 4699, loss 0.0999069, acc 0.96
2016-09-06T21:32:18.501609: step 4700, loss 0.0349324, acc 0.98

Evaluation:
2016-09-06T21:32:21.654154: step 4700, loss 2.48234, acc 0.742964

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-4700

2016-09-06T21:32:23.389860: step 4701, loss 0.00253642, acc 1
2016-09-06T21:32:24.061352: step 4702, loss 0.0307103, acc 0.96
2016-09-06T21:32:24.742920: step 4703, loss 0.0631477, acc 0.96
2016-09-06T21:32:25.434461: step 4704, loss 0.00248812, acc 1
2016-09-06T21:32:26.140561: step 4705, loss 0.0326453, acc 0.98
2016-09-06T21:32:26.808426: step 4706, loss 0.0833968, acc 0.96
2016-09-06T21:32:27.520485: step 4707, loss 0.0162666, acc 1
2016-09-06T21:32:28.212615: step 4708, loss 0.0368064, acc 0.96
2016-09-06T21:32:28.880350: step 4709, loss 0.00724001, acc 1
2016-09-06T21:32:29.563537: step 4710, loss 0.00239133, acc 1
2016-09-06T21:32:30.256225: step 4711, loss 0.00805244, acc 1
2016-09-06T21:32:30.958553: step 4712, loss 0.0280969, acc 0.98
2016-09-06T21:32:31.623280: step 4713, loss 0.0683855, acc 0.96
2016-09-06T21:32:32.336484: step 4714, loss 0.00225988, acc 1
2016-09-06T21:32:33.035744: step 4715, loss 0.0462176, acc 0.98
2016-09-06T21:32:33.717373: step 4716, loss 0.146901, acc 0.98
2016-09-06T21:32:34.419976: step 4717, loss 0.0845815, acc 0.94
2016-09-06T21:32:35.111865: step 4718, loss 0.0130802, acc 1
2016-09-06T21:32:35.821834: step 4719, loss 0.00978169, acc 1
2016-09-06T21:32:36.507504: step 4720, loss 0.0202504, acc 1
2016-09-06T21:32:37.194861: step 4721, loss 0.0210267, acc 0.98
2016-09-06T21:32:37.896201: step 4722, loss 0.116671, acc 0.96
2016-09-06T21:32:38.585427: step 4723, loss 0.0453234, acc 0.96
2016-09-06T21:32:39.269546: step 4724, loss 0.0197429, acc 0.98
2016-09-06T21:32:39.934490: step 4725, loss 0.00420187, acc 1
2016-09-06T21:32:40.623586: step 4726, loss 0.027929, acc 0.98
2016-09-06T21:32:41.315948: step 4727, loss 0.00267904, acc 1
2016-09-06T21:32:42.015585: step 4728, loss 0.0714878, acc 0.94
2016-09-06T21:32:42.700303: step 4729, loss 0.0206141, acc 0.98
2016-09-06T21:32:43.386855: step 4730, loss 0.030841, acc 0.98
2016-09-06T21:32:44.081013: step 4731, loss 0.0100674, acc 1
2016-09-06T21:32:44.752716: step 4732, loss 0.0155482, acc 0.98
2016-09-06T21:32:45.452460: step 4733, loss 0.0223807, acc 0.98
2016-09-06T21:32:46.132107: step 4734, loss 0.063381, acc 0.94
2016-09-06T21:32:46.849713: step 4735, loss 0.0483098, acc 0.96
2016-09-06T21:32:47.533824: step 4736, loss 0.0184847, acc 1
2016-09-06T21:32:48.218779: step 4737, loss 0.021646, acc 0.98
2016-09-06T21:32:48.899463: step 4738, loss 0.111702, acc 0.98
2016-09-06T21:32:49.560028: step 4739, loss 0.0423402, acc 0.98
2016-09-06T21:32:50.269792: step 4740, loss 0.125156, acc 0.96
2016-09-06T21:32:50.951016: step 4741, loss 0.0342974, acc 1
2016-09-06T21:32:51.639031: step 4742, loss 0.0227049, acc 0.98
2016-09-06T21:32:52.302265: step 4743, loss 0.000653601, acc 1
2016-09-06T21:32:52.985822: step 4744, loss 0.0252309, acc 1
2016-09-06T21:32:53.666934: step 4745, loss 0.00765699, acc 1
2016-09-06T21:32:54.334090: step 4746, loss 0.00267194, acc 1
2016-09-06T21:32:55.047203: step 4747, loss 0.0275242, acc 0.98
2016-09-06T21:32:55.750996: step 4748, loss 0.0061133, acc 1
2016-09-06T21:32:56.445880: step 4749, loss 0.0194003, acc 0.98
2016-09-06T21:32:57.134006: step 4750, loss 0.0268282, acc 0.98
2016-09-06T21:32:57.823740: step 4751, loss 0.0205021, acc 1
2016-09-06T21:32:58.540953: step 4752, loss 0.00114832, acc 1
2016-09-06T21:32:59.230560: step 4753, loss 0.0521665, acc 0.98
2016-09-06T21:32:59.919021: step 4754, loss 0.00747503, acc 1
2016-09-06T21:33:00.650800: step 4755, loss 0.0190009, acc 1
2016-09-06T21:33:01.320729: step 4756, loss 0.0322756, acc 0.96
2016-09-06T21:33:02.009472: step 4757, loss 0.056952, acc 0.98
2016-09-06T21:33:02.699095: step 4758, loss 0.0231663, acc 0.98
2016-09-06T21:33:03.402396: step 4759, loss 0.0254396, acc 1
2016-09-06T21:33:04.078735: step 4760, loss 0.0687906, acc 0.96
2016-09-06T21:33:04.777695: step 4761, loss 0.0272886, acc 0.98
2016-09-06T21:33:05.461200: step 4762, loss 0.0463185, acc 0.98
2016-09-06T21:33:06.141368: step 4763, loss 0.0442261, acc 0.98
2016-09-06T21:33:06.825985: step 4764, loss 0.0529399, acc 0.98
2016-09-06T21:33:07.528908: step 4765, loss 0.00502783, acc 1
2016-09-06T21:33:08.261469: step 4766, loss 0.0244086, acc 0.98
2016-09-06T21:33:08.948445: step 4767, loss 0.00724301, acc 1
2016-09-06T21:33:09.637731: step 4768, loss 0.0208249, acc 0.98
2016-09-06T21:33:10.312904: step 4769, loss 0.00543618, acc 1
2016-09-06T21:33:10.997524: step 4770, loss 0.0438005, acc 0.98
2016-09-06T21:33:11.665645: step 4771, loss 0.00025182, acc 1
2016-09-06T21:33:12.338922: step 4772, loss 0.0476199, acc 0.98
2016-09-06T21:33:13.036792: step 4773, loss 0.0233961, acc 1
2016-09-06T21:33:13.715058: step 4774, loss 0.018091, acc 1
2016-09-06T21:33:14.413544: step 4775, loss 0.0727545, acc 0.98
2016-09-06T21:33:15.103780: step 4776, loss 0.0303275, acc 0.98
2016-09-06T21:33:15.782369: step 4777, loss 0.00650168, acc 1
2016-09-06T21:33:16.470311: step 4778, loss 0.0100396, acc 1
2016-09-06T21:33:17.139360: step 4779, loss 0.00706312, acc 1
2016-09-06T21:33:17.841543: step 4780, loss 0.0176663, acc 1
2016-09-06T21:33:18.518318: step 4781, loss 0.00122161, acc 1
2016-09-06T21:33:19.235092: step 4782, loss 0.0246347, acc 0.98
2016-09-06T21:33:19.937361: step 4783, loss 0.00222241, acc 1
2016-09-06T21:33:20.618200: step 4784, loss 0.00997424, acc 1
2016-09-06T21:33:21.307351: step 4785, loss 0.00197702, acc 1
2016-09-06T21:33:21.975390: step 4786, loss 0.0846612, acc 0.96
2016-09-06T21:33:22.677823: step 4787, loss 0.0201074, acc 1
2016-09-06T21:33:23.340609: step 4788, loss 0.00948661, acc 1
2016-09-06T21:33:24.025293: step 4789, loss 0.000325315, acc 1
2016-09-06T21:33:24.716239: step 4790, loss 0.0146985, acc 0.98
2016-09-06T21:33:25.389066: step 4791, loss 0.0138886, acc 1
2016-09-06T21:33:26.079726: step 4792, loss 0.0642056, acc 0.98
2016-09-06T21:33:26.757375: step 4793, loss 0.00208682, acc 1
2016-09-06T21:33:27.456435: step 4794, loss 0.076235, acc 0.96
2016-09-06T21:33:28.130961: step 4795, loss 0.0167907, acc 1
2016-09-06T21:33:28.801939: step 4796, loss 0.00598186, acc 1
2016-09-06T21:33:29.487245: step 4797, loss 0.00377921, acc 1
2016-09-06T21:33:30.186299: step 4798, loss 0.0733927, acc 0.98
2016-09-06T21:33:30.881240: step 4799, loss 0.00403237, acc 1
2016-09-06T21:33:31.515662: step 4800, loss 0.0185575, acc 1

Evaluation:
2016-09-06T21:33:34.668708: step 4800, loss 2.26949, acc 0.745779

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-4800

2016-09-06T21:33:36.332938: step 4801, loss 0.0551779, acc 0.96
2016-09-06T21:33:37.030345: step 4802, loss 0.00170261, acc 1
2016-09-06T21:33:37.693891: step 4803, loss 0.0123418, acc 1
2016-09-06T21:33:38.395094: step 4804, loss 0.00273259, acc 1
2016-09-06T21:33:39.097923: step 4805, loss 0.0775435, acc 0.96
2016-09-06T21:33:39.787736: step 4806, loss 0.00696873, acc 1
2016-09-06T21:33:40.468684: step 4807, loss 0.000316019, acc 1
2016-09-06T21:33:41.162570: step 4808, loss 0.00871604, acc 1
2016-09-06T21:33:41.873265: step 4809, loss 0.0125527, acc 1
2016-09-06T21:33:42.536142: step 4810, loss 0.0429804, acc 0.98
2016-09-06T21:33:43.217616: step 4811, loss 0.0172583, acc 1
2016-09-06T21:33:43.918441: step 4812, loss 0.0211695, acc 0.98
2016-09-06T21:33:44.618976: step 4813, loss 0.0728736, acc 0.98
2016-09-06T21:33:45.307425: step 4814, loss 0.0213359, acc 0.98
2016-09-06T21:33:46.001033: step 4815, loss 5.90393e-05, acc 1
2016-09-06T21:33:46.734935: step 4816, loss 0.035967, acc 0.96
2016-09-06T21:33:47.406705: step 4817, loss 0.0199862, acc 0.98
2016-09-06T21:33:48.074765: step 4818, loss 0.0321383, acc 0.98
2016-09-06T21:33:48.735953: step 4819, loss 0.0614784, acc 0.98
2016-09-06T21:33:49.426464: step 4820, loss 0.00112941, acc 1
2016-09-06T21:33:50.104123: step 4821, loss 0.035029, acc 0.98
2016-09-06T21:33:50.786069: step 4822, loss 0.0026459, acc 1
2016-09-06T21:33:51.476323: step 4823, loss 0.00830523, acc 1
2016-09-06T21:33:52.119059: step 4824, loss 0.000544677, acc 1
2016-09-06T21:33:52.829611: step 4825, loss 0.0218109, acc 0.98
2016-09-06T21:33:53.501442: step 4826, loss 0.0108435, acc 1
2016-09-06T21:33:54.196499: step 4827, loss 0.0034908, acc 1
2016-09-06T21:33:54.870086: step 4828, loss 0.0133169, acc 1
2016-09-06T21:33:55.595340: step 4829, loss 0.062516, acc 0.96
2016-09-06T21:33:56.273722: step 4830, loss 0.0332659, acc 0.98
2016-09-06T21:33:56.933434: step 4831, loss 0.0133848, acc 1
2016-09-06T21:33:57.629133: step 4832, loss 0.0656803, acc 0.96
2016-09-06T21:33:58.332856: step 4833, loss 0.00159464, acc 1
2016-09-06T21:33:59.019045: step 4834, loss 0.0719503, acc 0.96
2016-09-06T21:33:59.687486: step 4835, loss 0.0153032, acc 0.98
2016-09-06T21:34:00.416586: step 4836, loss 0.0235626, acc 1
2016-09-06T21:34:01.117061: step 4837, loss 0.0377076, acc 0.96
2016-09-06T21:34:01.805894: step 4838, loss 0.0159768, acc 1
2016-09-06T21:34:02.472350: step 4839, loss 0.0242253, acc 0.98
2016-09-06T21:34:03.157802: step 4840, loss 0.00127597, acc 1
2016-09-06T21:34:03.835879: step 4841, loss 0.0478127, acc 0.96
2016-09-06T21:34:04.518665: step 4842, loss 0.0342263, acc 0.98
2016-09-06T21:34:05.208958: step 4843, loss 0.0179643, acc 1
2016-09-06T21:34:05.930411: step 4844, loss 0.052087, acc 0.96
2016-09-06T21:34:06.615394: step 4845, loss 0.00997596, acc 1
2016-09-06T21:34:07.291625: step 4846, loss 0.03963, acc 0.98
2016-09-06T21:34:07.971691: step 4847, loss 0.0141404, acc 0.98
2016-09-06T21:34:08.630634: step 4848, loss 0.0172725, acc 0.98
2016-09-06T21:34:09.327479: step 4849, loss 0.012989, acc 1
2016-09-06T21:34:10.015677: step 4850, loss 0.0131817, acc 1
2016-09-06T21:34:10.725700: step 4851, loss 0.0377838, acc 0.98
2016-09-06T21:34:11.394287: step 4852, loss 0.0403395, acc 0.96
2016-09-06T21:34:12.096459: step 4853, loss 0.000165381, acc 1
2016-09-06T21:34:12.784063: step 4854, loss 0.0111768, acc 1
2016-09-06T21:34:13.490538: step 4855, loss 0.0169348, acc 1
2016-09-06T21:34:14.175158: step 4856, loss 0.00180642, acc 1
2016-09-06T21:34:14.856807: step 4857, loss 0.0647464, acc 0.98
2016-09-06T21:34:15.549799: step 4858, loss 0.00320285, acc 1
2016-09-06T21:34:16.209698: step 4859, loss 8.49816e-05, acc 1
2016-09-06T21:34:16.903186: step 4860, loss 0.0125197, acc 1
2016-09-06T21:34:17.578201: step 4861, loss 0.0257488, acc 0.98
2016-09-06T21:34:18.273545: step 4862, loss 0.0879747, acc 0.94
2016-09-06T21:34:18.950823: step 4863, loss 0.0272946, acc 0.98
2016-09-06T21:34:19.630562: step 4864, loss 0.00395768, acc 1
2016-09-06T21:34:20.336445: step 4865, loss 0.0287283, acc 0.98
2016-09-06T21:34:21.006139: step 4866, loss 0.0130906, acc 1
2016-09-06T21:34:21.710172: step 4867, loss 0.0125312, acc 1
2016-09-06T21:34:22.404846: step 4868, loss 0.00371496, acc 1
2016-09-06T21:34:23.090310: step 4869, loss 0.049586, acc 0.96
2016-09-06T21:34:23.769108: step 4870, loss 0.00694189, acc 1
2016-09-06T21:34:24.472365: step 4871, loss 0.0697029, acc 0.98
2016-09-06T21:34:25.192050: step 4872, loss 0.0039327, acc 1
2016-09-06T21:34:25.881571: step 4873, loss 0.0229091, acc 0.98
2016-09-06T21:34:26.565962: step 4874, loss 0.0325111, acc 0.96
2016-09-06T21:34:27.252076: step 4875, loss 0.0148255, acc 1
2016-09-06T21:34:27.918542: step 4876, loss 0.016425, acc 0.98
2016-09-06T21:34:28.590186: step 4877, loss 0.0838986, acc 0.96
2016-09-06T21:34:29.247510: step 4878, loss 0.0392614, acc 0.98
2016-09-06T21:34:29.967267: step 4879, loss 0.0244659, acc 0.98
2016-09-06T21:34:30.622543: step 4880, loss 0.0239997, acc 1
2016-09-06T21:34:31.314866: step 4881, loss 0.0604401, acc 0.96
2016-09-06T21:34:31.986476: step 4882, loss 0.043267, acc 0.98
2016-09-06T21:34:32.700498: step 4883, loss 0.0471395, acc 0.96
2016-09-06T21:34:33.387090: step 4884, loss 0.0220464, acc 0.98
2016-09-06T21:34:34.074385: step 4885, loss 0.0162194, acc 1
2016-09-06T21:34:34.771300: step 4886, loss 0.00491699, acc 1
2016-09-06T21:34:35.427380: step 4887, loss 0.0535678, acc 0.98
2016-09-06T21:34:36.128698: step 4888, loss 0.00830067, acc 1
2016-09-06T21:34:36.824518: step 4889, loss 0.0442676, acc 0.96
2016-09-06T21:34:37.530260: step 4890, loss 0.0737275, acc 0.96
2016-09-06T21:34:38.213499: step 4891, loss 0.026194, acc 0.98
2016-09-06T21:34:38.888678: step 4892, loss 0.0626569, acc 0.98
2016-09-06T21:34:39.589431: step 4893, loss 0.0168623, acc 1
2016-09-06T21:34:40.288791: step 4894, loss 0.014235, acc 1
2016-09-06T21:34:40.982700: step 4895, loss 0.0403578, acc 0.96
2016-09-06T21:34:41.677126: step 4896, loss 0.0404852, acc 0.98
2016-09-06T21:34:42.363799: step 4897, loss 0.00724167, acc 1
2016-09-06T21:34:43.044742: step 4898, loss 0.0585378, acc 0.98
2016-09-06T21:34:43.701830: step 4899, loss 0.0465429, acc 0.98
2016-09-06T21:34:44.400561: step 4900, loss 0.123617, acc 0.98

Evaluation:
2016-09-06T21:34:47.545305: step 4900, loss 2.46849, acc 0.749531

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-4900

2016-09-06T21:34:49.245325: step 4901, loss 0.0744927, acc 0.98
2016-09-06T21:34:49.946939: step 4902, loss 0.000227265, acc 1
2016-09-06T21:34:50.618803: step 4903, loss 0.0609402, acc 0.96
2016-09-06T21:34:51.306498: step 4904, loss 0.0372317, acc 0.98
2016-09-06T21:34:51.994088: step 4905, loss 0.105071, acc 0.94
2016-09-06T21:34:52.694379: step 4906, loss 0.000180519, acc 1
2016-09-06T21:34:53.365683: step 4907, loss 0.0406862, acc 0.98
2016-09-06T21:34:54.061686: step 4908, loss 0.0264807, acc 0.98
2016-09-06T21:34:54.732974: step 4909, loss 0.0814143, acc 0.98
2016-09-06T21:34:55.416738: step 4910, loss 0.037003, acc 0.98
2016-09-06T21:34:56.096563: step 4911, loss 0.0226726, acc 1
2016-09-06T21:34:56.809624: step 4912, loss 0.0171716, acc 0.98
2016-09-06T21:34:57.505975: step 4913, loss 0.0721357, acc 0.96
2016-09-06T21:34:58.162995: step 4914, loss 0.0451434, acc 0.98
2016-09-06T21:34:58.861726: step 4915, loss 0.0593258, acc 0.94
2016-09-06T21:34:59.518884: step 4916, loss 0.0462495, acc 0.96
2016-09-06T21:35:00.195154: step 4917, loss 0.00780557, acc 1
2016-09-06T21:35:00.916312: step 4918, loss 0.0670628, acc 0.96
2016-09-06T21:35:01.590597: step 4919, loss 0.0315075, acc 0.98
2016-09-06T21:35:02.263546: step 4920, loss 0.0329814, acc 0.98
2016-09-06T21:35:02.931417: step 4921, loss 0.166689, acc 0.96
2016-09-06T21:35:03.630690: step 4922, loss 0.00186362, acc 1
2016-09-06T21:35:04.303426: step 4923, loss 0.0236821, acc 0.98
2016-09-06T21:35:04.995955: step 4924, loss 0.0452049, acc 0.98
2016-09-06T21:35:05.675536: step 4925, loss 0.0265826, acc 0.98
2016-09-06T21:35:06.375027: step 4926, loss 0.0470047, acc 0.98
2016-09-06T21:35:07.072612: step 4927, loss 0.0481024, acc 0.96
2016-09-06T21:35:07.774022: step 4928, loss 0.0314898, acc 0.98
2016-09-06T21:35:08.489160: step 4929, loss 0.00246673, acc 1
2016-09-06T21:35:09.157414: step 4930, loss 0.0600495, acc 0.96
2016-09-06T21:35:09.824934: step 4931, loss 0.0598946, acc 0.98
2016-09-06T21:35:10.504037: step 4932, loss 0.0389554, acc 0.98
2016-09-06T21:35:11.182707: step 4933, loss 0.0279833, acc 1
2016-09-06T21:35:11.862663: step 4934, loss 0.00438855, acc 1
2016-09-06T21:35:12.549963: step 4935, loss 0.000808839, acc 1
2016-09-06T21:35:13.236010: step 4936, loss 0.02396, acc 1
2016-09-06T21:35:13.894498: step 4937, loss 0.0355428, acc 0.98
2016-09-06T21:35:14.586249: step 4938, loss 0.00601964, acc 1
2016-09-06T21:35:15.269229: step 4939, loss 0.0450692, acc 0.96
2016-09-06T21:35:15.961124: step 4940, loss 0.00557102, acc 1
2016-09-06T21:35:16.634549: step 4941, loss 0.0329078, acc 0.98
2016-09-06T21:35:17.317994: step 4942, loss 0.0155254, acc 1
2016-09-06T21:35:18.022161: step 4943, loss 0.0179082, acc 0.98
2016-09-06T21:35:18.701041: step 4944, loss 0.0207159, acc 0.98
2016-09-06T21:35:19.400738: step 4945, loss 0.0315557, acc 0.98
2016-09-06T21:35:20.086665: step 4946, loss 0.0431296, acc 0.98
2016-09-06T21:35:20.782111: step 4947, loss 0.0204819, acc 1
2016-09-06T21:35:21.463280: step 4948, loss 0.0368255, acc 0.98
2016-09-06T21:35:22.124816: step 4949, loss 0.076948, acc 0.94
2016-09-06T21:35:22.822964: step 4950, loss 0.0278903, acc 0.98
2016-09-06T21:35:23.474993: step 4951, loss 0.0337692, acc 0.98
2016-09-06T21:35:24.170375: step 4952, loss 0.00940533, acc 1
2016-09-06T21:35:24.854040: step 4953, loss 0.0150617, acc 0.98
2016-09-06T21:35:25.544000: step 4954, loss 0.197821, acc 0.98
2016-09-06T21:35:26.217325: step 4955, loss 0.0231451, acc 0.98
2016-09-06T21:35:26.932052: step 4956, loss 0.00853825, acc 1
2016-09-06T21:35:27.618451: step 4957, loss 0.0122161, acc 1
2016-09-06T21:35:28.279378: step 4958, loss 0.0331577, acc 0.98
2016-09-06T21:35:28.972416: step 4959, loss 0.000154532, acc 1
2016-09-06T21:35:29.661477: step 4960, loss 0.056841, acc 0.98
2016-09-06T21:35:30.345206: step 4961, loss 0.00970502, acc 1
2016-09-06T21:35:31.034016: step 4962, loss 0.025641, acc 0.98
2016-09-06T21:35:31.716782: step 4963, loss 0.0148599, acc 1
2016-09-06T21:35:32.392100: step 4964, loss 0.016187, acc 0.98
2016-09-06T21:35:33.050535: step 4965, loss 0.000349592, acc 1
2016-09-06T21:35:33.765036: step 4966, loss 0.0378555, acc 0.98
2016-09-06T21:35:34.452805: step 4967, loss 0.0312455, acc 0.98
2016-09-06T21:35:35.144763: step 4968, loss 0.0181754, acc 1
2016-09-06T21:35:35.823036: step 4969, loss 0.0441557, acc 0.98
2016-09-06T21:35:36.511038: step 4970, loss 0.042204, acc 0.98
2016-09-06T21:35:37.189191: step 4971, loss 0.041607, acc 0.96
2016-09-06T21:35:37.850427: step 4972, loss 0.0207993, acc 0.98
2016-09-06T21:35:38.555191: step 4973, loss 0.0261008, acc 0.98
2016-09-06T21:35:39.251364: step 4974, loss 0.146252, acc 0.96
2016-09-06T21:35:39.925539: step 4975, loss 0.0050498, acc 1
2016-09-06T21:35:40.620537: step 4976, loss 0.00715514, acc 1
2016-09-06T21:35:41.309001: step 4977, loss 0.0138324, acc 1
2016-09-06T21:35:42.008018: step 4978, loss 0.0668966, acc 0.96
2016-09-06T21:35:42.682046: step 4979, loss 0.190457, acc 0.98
2016-09-06T21:35:43.375628: step 4980, loss 0.0020143, acc 1
2016-09-06T21:35:44.106177: step 4981, loss 0.00545401, acc 1
2016-09-06T21:35:44.810021: step 4982, loss 0.00134943, acc 1
2016-09-06T21:35:45.503530: step 4983, loss 0.00632425, acc 1
2016-09-06T21:35:46.190542: step 4984, loss 0.059317, acc 0.98
2016-09-06T21:35:46.905123: step 4985, loss 0.0139162, acc 1
2016-09-06T21:35:47.583814: step 4986, loss 0.0156203, acc 1
2016-09-06T21:35:48.264408: step 4987, loss 0.0230325, acc 0.98
2016-09-06T21:35:48.953475: step 4988, loss 0.00364471, acc 1
2016-09-06T21:35:49.656383: step 4989, loss 0.0243669, acc 1
2016-09-06T21:35:50.334295: step 4990, loss 0.0881817, acc 0.96
2016-09-06T21:35:51.019815: step 4991, loss 0.0267481, acc 0.98
2016-09-06T21:35:51.677040: step 4992, loss 0.0144985, acc 1
2016-09-06T21:35:52.341692: step 4993, loss 0.0858236, acc 0.96
2016-09-06T21:35:53.044607: step 4994, loss 0.0450001, acc 0.98
2016-09-06T21:35:53.733269: step 4995, loss 0.0662654, acc 0.98
2016-09-06T21:35:54.417440: step 4996, loss 0.0161131, acc 1
2016-09-06T21:35:55.094064: step 4997, loss 0.0624272, acc 0.98
2016-09-06T21:35:55.789498: step 4998, loss 0.0217764, acc 1
2016-09-06T21:35:56.482703: step 4999, loss 0.00482358, acc 1
2016-09-06T21:35:57.133536: step 5000, loss 0.0223272, acc 1

Evaluation:
2016-09-06T21:36:00.317701: step 5000, loss 2.12664, acc 0.743902

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-5000

2016-09-06T21:36:02.021890: step 5001, loss 0.0552251, acc 0.98
2016-09-06T21:36:02.713017: step 5002, loss 0.0251881, acc 0.98
2016-09-06T21:36:03.394186: step 5003, loss 0.062484, acc 0.98
2016-09-06T21:36:04.085479: step 5004, loss 0.0801103, acc 0.94
2016-09-06T21:36:04.773095: step 5005, loss 0.012813, acc 1
2016-09-06T21:36:05.445522: step 5006, loss 0.036817, acc 0.98
2016-09-06T21:36:06.151910: step 5007, loss 0.0293564, acc 0.98
2016-09-06T21:36:06.818690: step 5008, loss 0.0131053, acc 1
2016-09-06T21:36:07.512941: step 5009, loss 0.028628, acc 0.98
2016-09-06T21:36:08.195801: step 5010, loss 0.00113958, acc 1
2016-09-06T21:36:08.879147: step 5011, loss 0.00917886, acc 1
2016-09-06T21:36:09.547061: step 5012, loss 0.00570127, acc 1
2016-09-06T21:36:10.241566: step 5013, loss 0.159848, acc 0.94
2016-09-06T21:36:10.932197: step 5014, loss 0.0640952, acc 0.96
2016-09-06T21:36:11.616150: step 5015, loss 0.0208584, acc 0.98
2016-09-06T21:36:12.299452: step 5016, loss 0.00797905, acc 1
2016-09-06T21:36:12.985479: step 5017, loss 0.0622712, acc 0.98
2016-09-06T21:36:13.673048: step 5018, loss 0.0044673, acc 1
2016-09-06T21:36:14.388574: step 5019, loss 0.00131399, acc 1
2016-09-06T21:36:15.072212: step 5020, loss 0.0213651, acc 1
2016-09-06T21:36:15.770416: step 5021, loss 0.0128714, acc 1
2016-09-06T21:36:16.454621: step 5022, loss 0.0145222, acc 1
2016-09-06T21:36:17.147600: step 5023, loss 0.00936903, acc 1
2016-09-06T21:36:17.826905: step 5024, loss 0.0554298, acc 0.98
2016-09-06T21:36:18.503888: step 5025, loss 0.00406634, acc 1
2016-09-06T21:36:19.202275: step 5026, loss 0.0162899, acc 1
2016-09-06T21:36:19.876622: step 5027, loss 0.00371425, acc 1
2016-09-06T21:36:20.577175: step 5028, loss 0.0710334, acc 0.94
2016-09-06T21:36:21.237755: step 5029, loss 0.061669, acc 0.98
2016-09-06T21:36:21.937201: step 5030, loss 0.0136284, acc 1
2016-09-06T21:36:22.607759: step 5031, loss 0.00612371, acc 1
2016-09-06T21:36:23.277033: step 5032, loss 0.0100361, acc 1
2016-09-06T21:36:23.957557: step 5033, loss 0.0524331, acc 0.98
2016-09-06T21:36:24.639878: step 5034, loss 0.0974328, acc 0.96
2016-09-06T21:36:25.348773: step 5035, loss 0.0180997, acc 1
2016-09-06T21:36:26.012509: step 5036, loss 0.107247, acc 0.96
2016-09-06T21:36:26.704235: step 5037, loss 0.0749833, acc 0.98
2016-09-06T21:36:27.379223: step 5038, loss 0.0455648, acc 0.98
2016-09-06T21:36:28.049648: step 5039, loss 0.00297465, acc 1
2016-09-06T21:36:28.725982: step 5040, loss 0.00228623, acc 1
2016-09-06T21:36:29.422262: step 5041, loss 0.0254386, acc 1
2016-09-06T21:36:30.125281: step 5042, loss 0.0236756, acc 0.98
2016-09-06T21:36:30.804457: step 5043, loss 0.0166398, acc 1
2016-09-06T21:36:31.519198: step 5044, loss 0.0402534, acc 0.98
2016-09-06T21:36:32.193179: step 5045, loss 0.0518859, acc 0.98
2016-09-06T21:36:32.886328: step 5046, loss 0.000825545, acc 1
2016-09-06T21:36:33.571537: step 5047, loss 0.0481377, acc 1
2016-09-06T21:36:34.253781: step 5048, loss 0.00193953, acc 1
2016-09-06T21:36:34.924084: step 5049, loss 0.0470299, acc 0.96
2016-09-06T21:36:35.576447: step 5050, loss 0.0320831, acc 0.98
2016-09-06T21:36:36.282566: step 5051, loss 0.034757, acc 0.98
2016-09-06T21:36:36.956034: step 5052, loss 0.0501657, acc 0.96
2016-09-06T21:36:37.633916: step 5053, loss 0.0735566, acc 0.98
2016-09-06T21:36:38.325050: step 5054, loss 0.0031498, acc 1
2016-09-06T21:36:39.025387: step 5055, loss 0.0477197, acc 0.98
2016-09-06T21:36:39.711746: step 5056, loss 0.0252387, acc 1
2016-09-06T21:36:40.400516: step 5057, loss 0.0465146, acc 0.98
2016-09-06T21:36:41.091766: step 5058, loss 0.055558, acc 0.94
2016-09-06T21:36:41.772246: step 5059, loss 0.0280733, acc 1
2016-09-06T21:36:42.453891: step 5060, loss 0.0415289, acc 0.98
2016-09-06T21:36:43.145538: step 5061, loss 0.0591271, acc 0.98
2016-09-06T21:36:43.824937: step 5062, loss 0.00224645, acc 1
2016-09-06T21:36:44.508277: step 5063, loss 0.0677946, acc 0.94
2016-09-06T21:36:45.180100: step 5064, loss 0.00437415, acc 1
2016-09-06T21:36:45.892777: step 5065, loss 0.131674, acc 0.98
2016-09-06T21:36:46.594216: step 5066, loss 0.105986, acc 0.96
2016-09-06T21:36:47.269518: step 5067, loss 0.0844486, acc 0.98
2016-09-06T21:36:47.970288: step 5068, loss 0.0217126, acc 1
2016-09-06T21:36:48.646080: step 5069, loss 0.037953, acc 0.98
2016-09-06T21:36:49.335996: step 5070, loss 0.0379949, acc 0.98
2016-09-06T21:36:50.024315: step 5071, loss 0.0118733, acc 1
2016-09-06T21:36:50.737634: step 5072, loss 0.0460838, acc 0.96
2016-09-06T21:36:51.434215: step 5073, loss 0.0410284, acc 0.98
2016-09-06T21:36:52.120957: step 5074, loss 0.0130185, acc 1
2016-09-06T21:36:52.789103: step 5075, loss 0.0183059, acc 1
2016-09-06T21:36:53.486228: step 5076, loss 0.06156, acc 0.96
2016-09-06T21:36:54.197043: step 5077, loss 0.0104987, acc 1
2016-09-06T21:36:54.868897: step 5078, loss 0.0300997, acc 0.98
2016-09-06T21:36:55.579757: step 5079, loss 0.0339154, acc 0.98
2016-09-06T21:36:56.269647: step 5080, loss 0.0310894, acc 1
2016-09-06T21:36:56.971110: step 5081, loss 0.023308, acc 1
2016-09-06T21:36:57.653888: step 5082, loss 0.0200577, acc 1
2016-09-06T21:36:58.347067: step 5083, loss 0.0149445, acc 1
2016-09-06T21:36:59.088530: step 5084, loss 0.0168304, acc 1
2016-09-06T21:36:59.763237: step 5085, loss 0.00823272, acc 1
2016-09-06T21:37:00.475423: step 5086, loss 0.0239817, acc 0.98
2016-09-06T21:37:01.158236: step 5087, loss 0.013645, acc 1
2016-09-06T21:37:01.844346: step 5088, loss 0.00230419, acc 1
2016-09-06T21:37:02.544977: step 5089, loss 0.0455098, acc 0.96
2016-09-06T21:37:03.231324: step 5090, loss 0.0536502, acc 0.96
2016-09-06T21:37:03.923795: step 5091, loss 0.0162359, acc 1
2016-09-06T21:37:04.611346: step 5092, loss 0.0195523, acc 1
2016-09-06T21:37:05.293672: step 5093, loss 0.0286335, acc 0.98
2016-09-06T21:37:05.977342: step 5094, loss 0.00293659, acc 1
2016-09-06T21:37:06.652463: step 5095, loss 0.102663, acc 0.98
2016-09-06T21:37:07.354527: step 5096, loss 0.0376704, acc 0.98
2016-09-06T21:37:08.003542: step 5097, loss 0.000512156, acc 1
2016-09-06T21:37:08.684207: step 5098, loss 0.0113584, acc 1
2016-09-06T21:37:09.375530: step 5099, loss 0.0223393, acc 1
2016-09-06T21:37:10.048356: step 5100, loss 0.0541192, acc 0.96

Evaluation:
2016-09-06T21:37:13.192050: step 5100, loss 2.68692, acc 0.742964

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-5100

2016-09-06T21:37:14.918827: step 5101, loss 0.13573, acc 0.98
2016-09-06T21:37:15.622203: step 5102, loss 0.0169696, acc 0.98
2016-09-06T21:37:16.304853: step 5103, loss 0.0240171, acc 0.98
2016-09-06T21:37:17.022781: step 5104, loss 0.0304789, acc 0.98
2016-09-06T21:37:17.700770: step 5105, loss 0.029323, acc 0.98
2016-09-06T21:37:18.409574: step 5106, loss 0.0197344, acc 1
2016-09-06T21:37:19.099257: step 5107, loss 0.0351144, acc 1
2016-09-06T21:37:19.784770: step 5108, loss 0.000114233, acc 1
2016-09-06T21:37:20.458296: step 5109, loss 0.038681, acc 0.98
2016-09-06T21:37:21.137231: step 5110, loss 0.243858, acc 0.98
2016-09-06T21:37:21.816077: step 5111, loss 0.0219603, acc 0.98
2016-09-06T21:37:22.475903: step 5112, loss 0.000109678, acc 1
2016-09-06T21:37:23.184007: step 5113, loss 0.01767, acc 0.98
2016-09-06T21:37:23.866744: step 5114, loss 0.0230946, acc 0.98
2016-09-06T21:37:24.563243: step 5115, loss 0.0727436, acc 0.98
2016-09-06T21:37:25.243137: step 5116, loss 0.00213449, acc 1
2016-09-06T21:37:25.913066: step 5117, loss 0.0777673, acc 0.96
2016-09-06T21:37:26.609646: step 5118, loss 0.0385786, acc 0.98
2016-09-06T21:37:27.253504: step 5119, loss 0.000690427, acc 1
2016-09-06T21:37:27.957825: step 5120, loss 0.0596755, acc 0.98
2016-09-06T21:37:28.618505: step 5121, loss 0.00525727, acc 1
2016-09-06T21:37:29.298671: step 5122, loss 0.0387219, acc 0.98
2016-09-06T21:37:30.015254: step 5123, loss 0.0609373, acc 0.96
2016-09-06T21:37:30.702842: step 5124, loss 0.0416879, acc 1
2016-09-06T21:37:31.399428: step 5125, loss 0.0217945, acc 1
2016-09-06T21:37:32.066064: step 5126, loss 0.029206, acc 0.98
2016-09-06T21:37:32.775115: step 5127, loss 0.118698, acc 0.98
2016-09-06T21:37:33.426976: step 5128, loss 0.00231551, acc 1
2016-09-06T21:37:34.099822: step 5129, loss 0.0141496, acc 1
2016-09-06T21:37:34.783029: step 5130, loss 0.00267174, acc 1
2016-09-06T21:37:35.471644: step 5131, loss 0.0102809, acc 1
2016-09-06T21:37:36.156569: step 5132, loss 0.0277059, acc 0.98
2016-09-06T21:37:36.842583: step 5133, loss 0.019958, acc 1
2016-09-06T21:37:37.523540: step 5134, loss 0.0330377, acc 0.98
2016-09-06T21:37:38.198509: step 5135, loss 0.0369884, acc 0.98
2016-09-06T21:37:38.867526: step 5136, loss 0.0650868, acc 0.98
2016-09-06T21:37:39.545997: step 5137, loss 0.0223399, acc 0.98
2016-09-06T21:37:40.247912: step 5138, loss 0.00461823, acc 1
2016-09-06T21:37:40.944875: step 5139, loss 0.000481062, acc 1
2016-09-06T21:37:41.636566: step 5140, loss 0.0745518, acc 0.96
2016-09-06T21:37:42.333071: step 5141, loss 0.128336, acc 0.96
2016-09-06T21:37:43.017074: step 5142, loss 0.0297275, acc 0.98
2016-09-06T21:37:43.712182: step 5143, loss 0.0556056, acc 0.98
2016-09-06T21:37:44.392426: step 5144, loss 0.048581, acc 0.98
2016-09-06T21:37:45.081420: step 5145, loss 0.00515837, acc 1
2016-09-06T21:37:45.766047: step 5146, loss 0.0255624, acc 0.98
2016-09-06T21:37:46.474185: step 5147, loss 0.07646, acc 0.94
2016-09-06T21:37:47.181609: step 5148, loss 0.014329, acc 1
2016-09-06T21:37:47.858606: step 5149, loss 0.00853041, acc 1
2016-09-06T21:37:48.535720: step 5150, loss 0.0337097, acc 0.98
2016-09-06T21:37:49.208895: step 5151, loss 0.00389304, acc 1
2016-09-06T21:37:49.905828: step 5152, loss 0.0508103, acc 0.96
2016-09-06T21:37:50.602376: step 5153, loss 0.011915, acc 1
2016-09-06T21:37:51.280580: step 5154, loss 0.017874, acc 0.98
2016-09-06T21:37:51.969255: step 5155, loss 0.00641719, acc 1
2016-09-06T21:37:52.629405: step 5156, loss 0.00139357, acc 1
2016-09-06T21:37:53.345438: step 5157, loss 0.00884287, acc 1
2016-09-06T21:37:54.028871: step 5158, loss 0.0292086, acc 0.98
2016-09-06T21:37:54.717688: step 5159, loss 0.00284901, acc 1
2016-09-06T21:37:55.404975: step 5160, loss 0.126693, acc 0.92
2016-09-06T21:37:56.096838: step 5161, loss 0.115301, acc 0.96
2016-09-06T21:37:56.804077: step 5162, loss 0.00117938, acc 1
2016-09-06T21:37:57.476536: step 5163, loss 0.0028364, acc 1
2016-09-06T21:37:58.160967: step 5164, loss 0.000784863, acc 1
2016-09-06T21:37:58.845406: step 5165, loss 0.0345567, acc 0.98
2016-09-06T21:37:59.516843: step 5166, loss 0.027181, acc 0.98
2016-09-06T21:38:00.213706: step 5167, loss 0.0170992, acc 0.98
2016-09-06T21:38:00.896789: step 5168, loss 0.0261567, acc 1
2016-09-06T21:38:01.599750: step 5169, loss 0.114315, acc 0.98
2016-09-06T21:38:02.245269: step 5170, loss 0.0448218, acc 0.98
2016-09-06T21:38:02.942484: step 5171, loss 0.00545595, acc 1
2016-09-06T21:38:03.629040: step 5172, loss 0.00530182, acc 1
2016-09-06T21:38:04.324479: step 5173, loss 0.00470494, acc 1
2016-09-06T21:38:05.028395: step 5174, loss 0.00106694, acc 1
2016-09-06T21:38:05.700048: step 5175, loss 0.0106273, acc 1
2016-09-06T21:38:06.391381: step 5176, loss 0.0395391, acc 0.98
2016-09-06T21:38:07.065126: step 5177, loss 0.0231219, acc 0.98
2016-09-06T21:38:07.759804: step 5178, loss 0.061884, acc 0.96
2016-09-06T21:38:08.464597: step 5179, loss 0.0253179, acc 0.98
2016-09-06T21:38:09.124910: step 5180, loss 0.00370156, acc 1
2016-09-06T21:38:09.810352: step 5181, loss 0.0094757, acc 1
2016-09-06T21:38:10.494543: step 5182, loss 0.0228373, acc 0.98
2016-09-06T21:38:11.199224: step 5183, loss 0.0193078, acc 0.98
2016-09-06T21:38:11.823755: step 5184, loss 0.000533228, acc 1
2016-09-06T21:38:12.524339: step 5185, loss 0.0225553, acc 1
2016-09-06T21:38:13.207136: step 5186, loss 0.0901137, acc 0.96
2016-09-06T21:38:13.890864: step 5187, loss 0.0477816, acc 0.98
2016-09-06T21:38:14.568230: step 5188, loss 0.0195117, acc 1
2016-09-06T21:38:15.244930: step 5189, loss 0.000832179, acc 1
2016-09-06T21:38:15.938030: step 5190, loss 0.0601976, acc 0.94
2016-09-06T21:38:16.592032: step 5191, loss 0.0137613, acc 1
2016-09-06T21:38:17.284806: step 5192, loss 0.0365879, acc 0.98
2016-09-06T21:38:17.961963: step 5193, loss 0.00150358, acc 1
2016-09-06T21:38:18.631065: step 5194, loss 0.0158949, acc 0.98
2016-09-06T21:38:19.316476: step 5195, loss 0.105442, acc 0.98
2016-09-06T21:38:19.981878: step 5196, loss 0.0100474, acc 1
2016-09-06T21:38:20.675162: step 5197, loss 0.0465307, acc 0.98
2016-09-06T21:38:21.364175: step 5198, loss 0.0339321, acc 0.98
2016-09-06T21:38:22.073595: step 5199, loss 0.000180161, acc 1
2016-09-06T21:38:22.756054: step 5200, loss 0.0656251, acc 0.98

Evaluation:
2016-09-06T21:38:25.901714: step 5200, loss 2.32853, acc 0.741088

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-5200

2016-09-06T21:38:27.558458: step 5201, loss 0.0629048, acc 0.98
2016-09-06T21:38:28.238675: step 5202, loss 0.0101444, acc 1
2016-09-06T21:38:28.929806: step 5203, loss 0.0152121, acc 1
2016-09-06T21:38:29.622945: step 5204, loss 0.036005, acc 1
2016-09-06T21:38:30.300573: step 5205, loss 0.0945772, acc 0.94
2016-09-06T21:38:30.969510: step 5206, loss 0.0615508, acc 0.98
2016-09-06T21:38:31.660736: step 5207, loss 0.00457721, acc 1
2016-09-06T21:38:32.343609: step 5208, loss 0.0273848, acc 0.98
2016-09-06T21:38:33.074614: step 5209, loss 0.00778123, acc 1
2016-09-06T21:38:33.762273: step 5210, loss 0.0254526, acc 0.98
2016-09-06T21:38:34.451387: step 5211, loss 0.0208109, acc 0.98
2016-09-06T21:38:35.159812: step 5212, loss 0.0027165, acc 1
2016-09-06T21:38:35.814880: step 5213, loss 0.00172467, acc 1
2016-09-06T21:38:36.510856: step 5214, loss 0.0320117, acc 0.98
2016-09-06T21:38:37.183020: step 5215, loss 0.0030872, acc 1
2016-09-06T21:38:37.873008: step 5216, loss 0.048661, acc 0.96
2016-09-06T21:38:38.566103: step 5217, loss 0.00300036, acc 1
2016-09-06T21:38:39.240547: step 5218, loss 0.0599378, acc 0.98
2016-09-06T21:38:39.928890: step 5219, loss 0.0300778, acc 0.98
2016-09-06T21:38:40.623032: step 5220, loss 0.00127304, acc 1
2016-09-06T21:38:41.325496: step 5221, loss 0.012693, acc 1
2016-09-06T21:38:42.008421: step 5222, loss 0.0110088, acc 1
2016-09-06T21:38:42.695781: step 5223, loss 0.0545033, acc 0.98
2016-09-06T21:38:43.373551: step 5224, loss 0.00511467, acc 1
2016-09-06T21:38:44.061482: step 5225, loss 0.0260941, acc 1
2016-09-06T21:38:44.746090: step 5226, loss 0.121348, acc 0.96
2016-09-06T21:38:45.430370: step 5227, loss 0.0198443, acc 1
2016-09-06T21:38:46.135140: step 5228, loss 0.0304888, acc 1
2016-09-06T21:38:46.806679: step 5229, loss 0.0302799, acc 1
2016-09-06T21:38:47.550764: step 5230, loss 0.0334142, acc 0.98
2016-09-06T21:38:48.229592: step 5231, loss 0.000265264, acc 1
2016-09-06T21:38:48.915320: step 5232, loss 0.00634153, acc 1
2016-09-06T21:38:49.635204: step 5233, loss 0.0602554, acc 0.96
2016-09-06T21:38:50.322819: step 5234, loss 0.0229878, acc 0.98
2016-09-06T21:38:51.030960: step 5235, loss 0.0350119, acc 0.98
2016-09-06T21:38:51.711164: step 5236, loss 0.0313388, acc 0.98
2016-09-06T21:38:52.381850: step 5237, loss 0.00357444, acc 1
2016-09-06T21:38:53.061240: step 5238, loss 0.0186479, acc 1
2016-09-06T21:38:53.760589: step 5239, loss 0.0157578, acc 1
2016-09-06T21:38:54.444284: step 5240, loss 0.029671, acc 0.98
2016-09-06T21:38:55.129861: step 5241, loss 0.0330708, acc 0.98
2016-09-06T21:38:55.843787: step 5242, loss 0.0497914, acc 0.96
2016-09-06T21:38:56.508530: step 5243, loss 0.00157289, acc 1
2016-09-06T21:38:57.178306: step 5244, loss 0.0306485, acc 0.98
2016-09-06T21:38:57.877304: step 5245, loss 0.00475933, acc 1
2016-09-06T21:38:58.578611: step 5246, loss 0.0237323, acc 1
2016-09-06T21:38:59.283233: step 5247, loss 0.0319165, acc 0.98
2016-09-06T21:38:59.953840: step 5248, loss 0.0717367, acc 0.96
2016-09-06T21:39:00.675728: step 5249, loss 0.01755, acc 0.98
2016-09-06T21:39:01.357436: step 5250, loss 0.00838099, acc 1
2016-09-06T21:39:02.056311: step 5251, loss 0.000184195, acc 1
2016-09-06T21:39:02.741084: step 5252, loss 0.0412893, acc 0.98
2016-09-06T21:39:03.427728: step 5253, loss 0.00811309, acc 1
2016-09-06T21:39:04.125154: step 5254, loss 0.0137843, acc 1
2016-09-06T21:39:04.810499: step 5255, loss 0.0450736, acc 0.96
2016-09-06T21:39:05.511161: step 5256, loss 0.022193, acc 1
2016-09-06T21:39:06.217583: step 5257, loss 0.0394808, acc 0.96
2016-09-06T21:39:06.909595: step 5258, loss 0.0349301, acc 0.98
2016-09-06T21:39:07.592542: step 5259, loss 0.00962217, acc 1
2016-09-06T21:39:08.279077: step 5260, loss 0.0177818, acc 1
2016-09-06T21:39:08.970354: step 5261, loss 0.0519139, acc 0.98
2016-09-06T21:39:09.644090: step 5262, loss 0.0595145, acc 0.98
2016-09-06T21:39:10.311500: step 5263, loss 0.000829017, acc 1
2016-09-06T21:39:11.008387: step 5264, loss 0.00683651, acc 1
2016-09-06T21:39:11.682868: step 5265, loss 0.0426501, acc 0.98
2016-09-06T21:39:12.369564: step 5266, loss 0.00042892, acc 1
2016-09-06T21:39:13.047387: step 5267, loss 0.00356152, acc 1
2016-09-06T21:39:13.743178: step 5268, loss 0.00369869, acc 1
2016-09-06T21:39:14.433656: step 5269, loss 0.0354474, acc 0.98
2016-09-06T21:39:15.133744: step 5270, loss 0.10171, acc 0.96
2016-09-06T21:39:15.821606: step 5271, loss 0.011171, acc 1
2016-09-06T21:39:16.501874: step 5272, loss 0.0145287, acc 0.98
2016-09-06T21:39:17.183427: step 5273, loss 0.0391989, acc 0.98
2016-09-06T21:39:17.881681: step 5274, loss 0.00730234, acc 1
2016-09-06T21:39:18.566633: step 5275, loss 0.00104118, acc 1
2016-09-06T21:39:19.241216: step 5276, loss 0.0117185, acc 1
2016-09-06T21:39:19.928820: step 5277, loss 0.0392756, acc 0.96
2016-09-06T21:39:20.611413: step 5278, loss 0.0262514, acc 0.98
2016-09-06T21:39:21.290462: step 5279, loss 0.133689, acc 0.96
2016-09-06T21:39:21.973052: step 5280, loss 0.0258954, acc 0.98
2016-09-06T21:39:22.659952: step 5281, loss 0.076302, acc 0.98
2016-09-06T21:39:23.370664: step 5282, loss 0.0640364, acc 0.96
2016-09-06T21:39:24.059133: step 5283, loss 0.0355129, acc 0.96
2016-09-06T21:39:24.743558: step 5284, loss 0.0468656, acc 0.96
2016-09-06T21:39:25.422216: step 5285, loss 0.0158503, acc 1
2016-09-06T21:39:26.114440: step 5286, loss 0.0516661, acc 0.98
2016-09-06T21:39:26.797850: step 5287, loss 0.0381689, acc 1
2016-09-06T21:39:27.506385: step 5288, loss 0.00992483, acc 1
2016-09-06T21:39:28.214442: step 5289, loss 0.0239079, acc 0.98
2016-09-06T21:39:28.891920: step 5290, loss 0.0496606, acc 0.96
2016-09-06T21:39:29.581314: step 5291, loss 0.0196131, acc 1
2016-09-06T21:39:30.271413: step 5292, loss 0.00254571, acc 1
2016-09-06T21:39:30.967921: step 5293, loss 0.0435066, acc 1
2016-09-06T21:39:31.651664: step 5294, loss 0.0460703, acc 0.98
2016-09-06T21:39:32.342644: step 5295, loss 0.0211443, acc 1
2016-09-06T21:39:33.049774: step 5296, loss 0.00352824, acc 1
2016-09-06T21:39:33.731415: step 5297, loss 0.0150314, acc 1
2016-09-06T21:39:34.412707: step 5298, loss 0.0483406, acc 0.98
2016-09-06T21:39:35.104948: step 5299, loss 0.00203932, acc 1
2016-09-06T21:39:35.794866: step 5300, loss 0.0126916, acc 1

Evaluation:
2016-09-06T21:39:38.942113: step 5300, loss 2.07108, acc 0.750469

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-5300

2016-09-06T21:39:40.718968: step 5301, loss 0.0116381, acc 1
2016-09-06T21:39:41.423593: step 5302, loss 0.0215869, acc 0.98
2016-09-06T21:39:42.067552: step 5303, loss 0.0338358, acc 0.96
2016-09-06T21:39:42.769730: step 5304, loss 0.00634028, acc 1
2016-09-06T21:39:43.458553: step 5305, loss 0.0422828, acc 0.98
2016-09-06T21:39:44.143759: step 5306, loss 0.052005, acc 0.98
2016-09-06T21:39:44.817242: step 5307, loss 0.137119, acc 0.98
2016-09-06T21:39:45.480350: step 5308, loss 0.0109984, acc 1
2016-09-06T21:39:46.173326: step 5309, loss 0.007023, acc 1
2016-09-06T21:39:46.828986: step 5310, loss 0.00352504, acc 1
2016-09-06T21:39:47.548295: step 5311, loss 0.0201801, acc 1
2016-09-06T21:39:48.243150: step 5312, loss 0.0234746, acc 0.98
2016-09-06T21:39:48.932297: step 5313, loss 0.00886678, acc 1
2016-09-06T21:39:49.635456: step 5314, loss 0.0157117, acc 1
2016-09-06T21:39:50.317823: step 5315, loss 0.134215, acc 0.92
2016-09-06T21:39:51.001451: step 5316, loss 0.0224996, acc 1
2016-09-06T21:39:51.657585: step 5317, loss 0.0254483, acc 0.98
2016-09-06T21:39:52.371639: step 5318, loss 0.0132274, acc 1
2016-09-06T21:39:53.072896: step 5319, loss 0.00310659, acc 1
2016-09-06T21:39:53.775532: step 5320, loss 0.0449292, acc 0.96
2016-09-06T21:39:54.448318: step 5321, loss 0.0487049, acc 0.98
2016-09-06T21:39:55.140835: step 5322, loss 0.00107675, acc 1
2016-09-06T21:39:55.855488: step 5323, loss 0.0102657, acc 1
2016-09-06T21:39:56.505273: step 5324, loss 0.0372401, acc 0.98
2016-09-06T21:39:57.200993: step 5325, loss 0.0564688, acc 0.96
2016-09-06T21:39:57.891043: step 5326, loss 0.0165744, acc 0.98
2016-09-06T21:39:58.587087: step 5327, loss 0.0215066, acc 0.98
2016-09-06T21:39:59.262978: step 5328, loss 0.0230288, acc 1
2016-09-06T21:39:59.967635: step 5329, loss 0.0300102, acc 0.96
2016-09-06T21:40:00.706975: step 5330, loss 0.00972751, acc 1
2016-09-06T21:40:01.383857: step 5331, loss 0.00111207, acc 1
2016-09-06T21:40:02.067643: step 5332, loss 0.0929665, acc 0.94
2016-09-06T21:40:02.756544: step 5333, loss 0.0228461, acc 1
2016-09-06T21:40:03.436791: step 5334, loss 0.0128184, acc 1
2016-09-06T21:40:04.122384: step 5335, loss 0.00544864, acc 1
2016-09-06T21:40:04.806618: step 5336, loss 0.0479744, acc 0.98
2016-09-06T21:40:05.540077: step 5337, loss 0.135334, acc 0.98
2016-09-06T21:40:06.210625: step 5338, loss 0.049535, acc 0.98
2016-09-06T21:40:06.887628: step 5339, loss 0.00916435, acc 1
2016-09-06T21:40:07.566958: step 5340, loss 0.00723894, acc 1
2016-09-06T21:40:08.273439: step 5341, loss 0.00311523, acc 1
2016-09-06T21:40:08.952791: step 5342, loss 0.00968572, acc 1
2016-09-06T21:40:09.608552: step 5343, loss 0.0459693, acc 0.98
2016-09-06T21:40:10.320170: step 5344, loss 0.011406, acc 1
2016-09-06T21:40:10.991174: step 5345, loss 0.035717, acc 0.98
2016-09-06T21:40:11.672766: step 5346, loss 0.0251466, acc 1
2016-09-06T21:40:12.353086: step 5347, loss 0.00548516, acc 1
2016-09-06T21:40:13.045492: step 5348, loss 0.00617442, acc 1
2016-09-06T21:40:13.739539: step 5349, loss 0.00596778, acc 1
2016-09-06T21:40:14.421611: step 5350, loss 0.0486097, acc 0.96
2016-09-06T21:40:15.140201: step 5351, loss 0.0247775, acc 0.98
2016-09-06T21:40:15.819326: step 5352, loss 0.0194864, acc 0.98
2016-09-06T21:40:16.506392: step 5353, loss 0.0177428, acc 1
2016-09-06T21:40:17.170756: step 5354, loss 0.0015075, acc 1
2016-09-06T21:40:17.848414: step 5355, loss 0.0142334, acc 1
2016-09-06T21:40:18.534416: step 5356, loss 0.00257638, acc 1
2016-09-06T21:40:19.219296: step 5357, loss 0.000794235, acc 1
2016-09-06T21:40:19.944660: step 5358, loss 0.0775366, acc 0.96
2016-09-06T21:40:20.608281: step 5359, loss 0.0211708, acc 0.98
2016-09-06T21:40:21.288265: step 5360, loss 0.0741242, acc 0.98
2016-09-06T21:40:21.990201: step 5361, loss 0.0447633, acc 0.98
2016-09-06T21:40:22.683310: step 5362, loss 0.0701724, acc 0.98
2016-09-06T21:40:23.369428: step 5363, loss 0.000266926, acc 1
2016-09-06T21:40:24.047810: step 5364, loss 0.0237938, acc 1
2016-09-06T21:40:24.742606: step 5365, loss 0.0064081, acc 1
2016-09-06T21:40:25.420116: step 5366, loss 0.00380896, acc 1
2016-09-06T21:40:26.100380: step 5367, loss 0.0145393, acc 1
2016-09-06T21:40:26.800634: step 5368, loss 0.104761, acc 0.96
2016-09-06T21:40:27.501816: step 5369, loss 0.0283331, acc 1
2016-09-06T21:40:28.182168: step 5370, loss 0.0235603, acc 0.98
2016-09-06T21:40:28.853776: step 5371, loss 0.00979568, acc 1
2016-09-06T21:40:29.554625: step 5372, loss 0.000125566, acc 1
2016-09-06T21:40:30.228404: step 5373, loss 0.00795525, acc 1
2016-09-06T21:40:30.893712: step 5374, loss 0.0179252, acc 1
2016-09-06T21:40:31.587322: step 5375, loss 0.0464733, acc 0.98
2016-09-06T21:40:32.239308: step 5376, loss 0.00319125, acc 1
2016-09-06T21:40:32.928831: step 5377, loss 0.0627268, acc 0.98
2016-09-06T21:40:33.608895: step 5378, loss 0.00125498, acc 1
2016-09-06T21:40:34.297324: step 5379, loss 0.0374856, acc 0.98
2016-09-06T21:40:34.958635: step 5380, loss 0.0476966, acc 0.98
2016-09-06T21:40:35.662006: step 5381, loss 0.0502516, acc 0.96
2016-09-06T21:40:36.333633: step 5382, loss 0.0385726, acc 0.98
2016-09-06T21:40:37.022156: step 5383, loss 0.0107354, acc 1
2016-09-06T21:40:37.708778: step 5384, loss 0.0168239, acc 1
2016-09-06T21:40:38.406039: step 5385, loss 0.00312431, acc 1
2016-09-06T21:40:39.117945: step 5386, loss 0.0226496, acc 0.98
2016-09-06T21:40:39.786766: step 5387, loss 0.035126, acc 0.98
2016-09-06T21:40:40.489158: step 5388, loss 0.0311423, acc 0.98
2016-09-06T21:40:41.183887: step 5389, loss 0.00841113, acc 1
2016-09-06T21:40:41.883407: step 5390, loss 0.00267821, acc 1
2016-09-06T21:40:42.561386: step 5391, loss 0.00150358, acc 1
2016-09-06T21:40:43.241753: step 5392, loss 0.000372573, acc 1
2016-09-06T21:40:43.952065: step 5393, loss 0.074268, acc 0.98
2016-09-06T21:40:44.601725: step 5394, loss 0.00228985, acc 1
2016-09-06T21:40:45.298309: step 5395, loss 0.0376211, acc 0.96
2016-09-06T21:40:45.985516: step 5396, loss 0.0542493, acc 0.98
2016-09-06T21:40:46.671672: step 5397, loss 0.0386568, acc 0.98
2016-09-06T21:40:47.349495: step 5398, loss 0.0195522, acc 0.98
2016-09-06T21:40:48.022294: step 5399, loss 0.0234708, acc 0.98
2016-09-06T21:40:48.720875: step 5400, loss 0.0635618, acc 0.98

Evaluation:
2016-09-06T21:40:51.837561: step 5400, loss 2.11177, acc 0.751407

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-5400

2016-09-06T21:40:53.559475: step 5401, loss 0.0294099, acc 1
2016-09-06T21:40:54.238786: step 5402, loss 0.0156766, acc 1
2016-09-06T21:40:54.918103: step 5403, loss 0.00116934, acc 1
2016-09-06T21:40:55.586442: step 5404, loss 0.0048197, acc 1
2016-09-06T21:40:56.264364: step 5405, loss 0.0242319, acc 0.98
2016-09-06T21:40:56.967537: step 5406, loss 0.026119, acc 0.98
2016-09-06T21:40:57.643334: step 5407, loss 0.00637741, acc 1
2016-09-06T21:40:58.344938: step 5408, loss 0.0684366, acc 0.98
2016-09-06T21:40:58.998422: step 5409, loss 0.0226696, acc 0.98
2016-09-06T21:40:59.692342: step 5410, loss 0.000819054, acc 1
2016-09-06T21:41:00.418786: step 5411, loss 0.000341537, acc 1
2016-09-06T21:41:01.093042: step 5412, loss 0.0153114, acc 1
2016-09-06T21:41:01.787141: step 5413, loss 0.0337484, acc 0.98
2016-09-06T21:41:02.482754: step 5414, loss 0.00804524, acc 1
2016-09-06T21:41:03.199339: step 5415, loss 0.00189279, acc 1
2016-09-06T21:41:03.860195: step 5416, loss 0.0397262, acc 0.98
2016-09-06T21:41:04.559439: step 5417, loss 0.0346535, acc 1
2016-09-06T21:41:05.256897: step 5418, loss 0.0174547, acc 1
2016-09-06T21:41:05.943427: step 5419, loss 0.00980526, acc 1
2016-09-06T21:41:06.629609: step 5420, loss 0.0459458, acc 0.98
2016-09-06T21:41:07.318811: step 5421, loss 0.00205664, acc 1
2016-09-06T21:41:08.011063: step 5422, loss 0.0244905, acc 0.98
2016-09-06T21:41:08.677321: step 5423, loss 0.00954571, acc 1
2016-09-06T21:41:09.364696: step 5424, loss 0.040414, acc 0.96
2016-09-06T21:41:10.040375: step 5425, loss 0.106324, acc 0.94
2016-09-06T21:41:10.750294: step 5426, loss 0.0161126, acc 1
2016-09-06T21:41:11.451178: step 5427, loss 0.00995887, acc 1
2016-09-06T21:41:12.132028: step 5428, loss 0.0130152, acc 1
2016-09-06T21:41:12.815700: step 5429, loss 0.00100637, acc 1
2016-09-06T21:41:13.485184: step 5430, loss 0.0346789, acc 0.98
2016-09-06T21:41:14.209740: step 5431, loss 0.0504945, acc 0.98
2016-09-06T21:41:14.893464: step 5432, loss 0.00468298, acc 1
2016-09-06T21:41:15.589632: step 5433, loss 0.00168426, acc 1
2016-09-06T21:41:16.272880: step 5434, loss 0.00549797, acc 1
2016-09-06T21:41:16.956987: step 5435, loss 0.012255, acc 1
2016-09-06T21:41:17.660138: step 5436, loss 0.0644677, acc 0.98
2016-09-06T21:41:18.332984: step 5437, loss 0.0167506, acc 0.98
2016-09-06T21:41:19.037775: step 5438, loss 0.00311123, acc 1
2016-09-06T21:41:19.718625: step 5439, loss 0.145729, acc 0.96
2016-09-06T21:41:20.400635: step 5440, loss 0.0436571, acc 0.98
2016-09-06T21:41:21.093621: step 5441, loss 4.73251e-05, acc 1
2016-09-06T21:41:21.782335: step 5442, loss 0.0500727, acc 0.98
2016-09-06T21:41:22.487584: step 5443, loss 0.00245062, acc 1
2016-09-06T21:41:23.158841: step 5444, loss 0.0292161, acc 0.98
2016-09-06T21:41:23.843327: step 5445, loss 0.149271, acc 0.96
2016-09-06T21:41:24.513953: step 5446, loss 0.000204966, acc 1
2016-09-06T21:41:25.230559: step 5447, loss 0.0349555, acc 0.98
2016-09-06T21:41:25.908197: step 5448, loss 0.032352, acc 0.98
2016-09-06T21:41:26.582289: step 5449, loss 0.0175486, acc 1
2016-09-06T21:41:27.273014: step 5450, loss 0.00361326, acc 1
2016-09-06T21:41:27.944306: step 5451, loss 0.00574551, acc 1
2016-09-06T21:41:28.617209: step 5452, loss 0.0135005, acc 1
2016-09-06T21:41:29.305461: step 5453, loss 0.0199833, acc 1
2016-09-06T21:41:29.990101: step 5454, loss 0.0326623, acc 0.98
2016-09-06T21:41:30.708645: step 5455, loss 0.00885431, acc 1
2016-09-06T21:41:31.386153: step 5456, loss 0.00742943, acc 1
2016-09-06T21:41:32.089283: step 5457, loss 0.0860172, acc 0.96
2016-09-06T21:41:32.750948: step 5458, loss 0.00298679, acc 1
2016-09-06T21:41:33.432277: step 5459, loss 4.82866e-05, acc 1
2016-09-06T21:41:34.120752: step 5460, loss 0.0284302, acc 1
2016-09-06T21:41:34.795817: step 5461, loss 0.0320925, acc 0.98
2016-09-06T21:41:35.469824: step 5462, loss 0.255686, acc 0.94
2016-09-06T21:41:36.145596: step 5463, loss 0.0315851, acc 0.98
2016-09-06T21:41:36.858176: step 5464, loss 0.00386021, acc 1
2016-09-06T21:41:37.567284: step 5465, loss 0.0353147, acc 0.98
2016-09-06T21:41:38.239997: step 5466, loss 0.0340255, acc 0.96
2016-09-06T21:41:38.950695: step 5467, loss 0.0400378, acc 0.98
2016-09-06T21:41:39.643636: step 5468, loss 0.0683284, acc 0.96
2016-09-06T21:41:40.324056: step 5469, loss 0.00389336, acc 1
2016-09-06T21:41:41.023430: step 5470, loss 0.102227, acc 0.98
2016-09-06T21:41:41.741474: step 5471, loss 0.012711, acc 1
2016-09-06T21:41:42.424357: step 5472, loss 0.0157892, acc 1
2016-09-06T21:41:43.111083: step 5473, loss 0.0373831, acc 0.98
2016-09-06T21:41:43.782679: step 5474, loss 0.00225359, acc 1
2016-09-06T21:41:44.473803: step 5475, loss 0.00648623, acc 1
2016-09-06T21:41:45.185727: step 5476, loss 0.0574026, acc 0.98
2016-09-06T21:41:45.870459: step 5477, loss 0.0295655, acc 0.98
2016-09-06T21:41:46.593777: step 5478, loss 0.00214822, acc 1
2016-09-06T21:41:47.279195: step 5479, loss 0.0597911, acc 0.98
2016-09-06T21:41:47.953131: step 5480, loss 0.101178, acc 0.96
2016-09-06T21:41:48.631256: step 5481, loss 0.029112, acc 0.98
2016-09-06T21:41:49.321378: step 5482, loss 0.028999, acc 0.98
2016-09-06T21:41:50.013870: step 5483, loss 0.0131348, acc 1
2016-09-06T21:41:50.679345: step 5484, loss 0.0251299, acc 0.98
2016-09-06T21:41:51.382209: step 5485, loss 0.0581298, acc 0.94
2016-09-06T21:41:52.073439: step 5486, loss 0.0572526, acc 0.96
2016-09-06T21:41:52.763269: step 5487, loss 0.0090156, acc 1
2016-09-06T21:41:53.445337: step 5488, loss 0.0186647, acc 0.98
2016-09-06T21:41:54.148602: step 5489, loss 0.0276803, acc 0.98
2016-09-06T21:41:54.836660: step 5490, loss 0.0419731, acc 0.98
2016-09-06T21:41:55.501694: step 5491, loss 0.0516863, acc 0.98
2016-09-06T21:41:56.217090: step 5492, loss 0.00113575, acc 1
2016-09-06T21:41:56.885596: step 5493, loss 8.23273e-05, acc 1
2016-09-06T21:41:57.564980: step 5494, loss 0.000392686, acc 1
2016-09-06T21:41:58.262390: step 5495, loss 0.0257793, acc 0.98
2016-09-06T21:41:58.954174: step 5496, loss 0.0227244, acc 0.98
2016-09-06T21:41:59.641752: step 5497, loss 0.15164, acc 0.98
2016-09-06T21:42:00.346396: step 5498, loss 0.0924625, acc 0.96
2016-09-06T21:42:01.033735: step 5499, loss 0.0346941, acc 0.98
2016-09-06T21:42:01.717462: step 5500, loss 0.0795287, acc 0.94

Evaluation:
2016-09-06T21:42:04.855657: step 5500, loss 2.08374, acc 0.749531

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-5500

2016-09-06T21:42:06.562096: step 5501, loss 0.0267426, acc 0.98
2016-09-06T21:42:07.250074: step 5502, loss 0.104998, acc 0.96
2016-09-06T21:42:07.930671: step 5503, loss 0.0661045, acc 0.98
2016-09-06T21:42:08.618546: step 5504, loss 0.0404855, acc 0.98
2016-09-06T21:42:09.316642: step 5505, loss 0.0196313, acc 1
2016-09-06T21:42:09.984759: step 5506, loss 0.0171536, acc 1
2016-09-06T21:42:10.670467: step 5507, loss 0.0488367, acc 1
2016-09-06T21:42:11.345416: step 5508, loss 0.0456084, acc 0.98
2016-09-06T21:42:12.048619: step 5509, loss 0.300385, acc 0.96
2016-09-06T21:42:12.735081: step 5510, loss 0.014536, acc 1
2016-09-06T21:42:13.401166: step 5511, loss 0.0165668, acc 0.98
2016-09-06T21:42:14.101124: step 5512, loss 0.0100157, acc 1
2016-09-06T21:42:14.763232: step 5513, loss 0.00422123, acc 1
2016-09-06T21:42:15.460876: step 5514, loss 0.00314615, acc 1
2016-09-06T21:42:16.138141: step 5515, loss 0.0405263, acc 0.96
2016-09-06T21:42:16.826096: step 5516, loss 0.0024856, acc 1
2016-09-06T21:42:17.515568: step 5517, loss 0.0326487, acc 0.96
2016-09-06T21:42:18.194632: step 5518, loss 0.02048, acc 0.98
2016-09-06T21:42:18.875038: step 5519, loss 0.00597242, acc 1
2016-09-06T21:42:19.541386: step 5520, loss 0.0369321, acc 0.96
2016-09-06T21:42:20.236308: step 5521, loss 0.0465796, acc 0.98
2016-09-06T21:42:20.907100: step 5522, loss 0.00672386, acc 1
2016-09-06T21:42:21.585717: step 5523, loss 0.0256249, acc 0.98
2016-09-06T21:42:22.289792: step 5524, loss 0.0756978, acc 0.96
2016-09-06T21:42:22.994902: step 5525, loss 0.0621198, acc 0.98
2016-09-06T21:42:23.694118: step 5526, loss 0.00529304, acc 1
2016-09-06T21:42:24.394185: step 5527, loss 0.0175512, acc 1
2016-09-06T21:42:25.095677: step 5528, loss 0.0238633, acc 1
2016-09-06T21:42:25.794424: step 5529, loss 0.0143758, acc 1
2016-09-06T21:42:26.472219: step 5530, loss 0.0762621, acc 0.96
2016-09-06T21:42:27.149121: step 5531, loss 0.00962678, acc 1
2016-09-06T21:42:27.807145: step 5532, loss 0.00166829, acc 1
2016-09-06T21:42:28.512936: step 5533, loss 0.0402952, acc 0.96
2016-09-06T21:42:29.170014: step 5534, loss 0.0414634, acc 0.98
2016-09-06T21:42:29.868349: step 5535, loss 0.0880711, acc 0.96
2016-09-06T21:42:30.558412: step 5536, loss 0.0235384, acc 1
2016-09-06T21:42:31.254652: step 5537, loss 0.00556604, acc 1
2016-09-06T21:42:31.949892: step 5538, loss 0.0242031, acc 0.98
2016-09-06T21:42:32.636254: step 5539, loss 0.0194011, acc 0.98
2016-09-06T21:42:33.314071: step 5540, loss 0.000799043, acc 1
2016-09-06T21:42:33.978736: step 5541, loss 0.0708442, acc 0.98
2016-09-06T21:42:34.661240: step 5542, loss 0.0500729, acc 0.96
2016-09-06T21:42:35.340660: step 5543, loss 0.0231697, acc 0.98
2016-09-06T21:42:36.035919: step 5544, loss 0.0241656, acc 1
2016-09-06T21:42:36.734445: step 5545, loss 0.00920581, acc 1
2016-09-06T21:42:37.413763: step 5546, loss 0.0354538, acc 0.98
2016-09-06T21:42:38.092697: step 5547, loss 0.0494715, acc 0.96
2016-09-06T21:42:38.766090: step 5548, loss 0.0362337, acc 1
2016-09-06T21:42:39.473086: step 5549, loss 0.0217418, acc 0.98
2016-09-06T21:42:40.150522: step 5550, loss 0.0805618, acc 0.96
2016-09-06T21:42:40.841874: step 5551, loss 0.000875474, acc 1
2016-09-06T21:42:41.539630: step 5552, loss 0.0368366, acc 0.98
2016-09-06T21:42:42.222896: step 5553, loss 0.026761, acc 1
2016-09-06T21:42:42.907845: step 5554, loss 0.0491091, acc 0.96
2016-09-06T21:42:43.567938: step 5555, loss 0.0339137, acc 0.98
2016-09-06T21:42:44.251848: step 5556, loss 0.0787733, acc 0.96
2016-09-06T21:42:44.912734: step 5557, loss 0.0046109, acc 1
2016-09-06T21:42:45.588211: step 5558, loss 0.00559714, acc 1
2016-09-06T21:42:46.273721: step 5559, loss 0.0106009, acc 1
2016-09-06T21:42:46.951263: step 5560, loss 0.0791457, acc 0.96
2016-09-06T21:42:47.610894: step 5561, loss 0.0175042, acc 1
2016-09-06T21:42:48.282229: step 5562, loss 0.012877, acc 1
2016-09-06T21:42:48.984461: step 5563, loss 0.0567776, acc 0.98
2016-09-06T21:42:49.645779: step 5564, loss 0.0208969, acc 0.98
2016-09-06T21:42:50.342914: step 5565, loss 0.14571, acc 0.96
2016-09-06T21:42:51.019888: step 5566, loss 0.0901997, acc 0.94
2016-09-06T21:42:51.697112: step 5567, loss 0.00173499, acc 1
2016-09-06T21:42:52.318261: step 5568, loss 0.00173861, acc 1
2016-09-06T21:42:53.001264: step 5569, loss 0.025475, acc 0.98
2016-09-06T21:42:53.708225: step 5570, loss 0.0289559, acc 0.98
2016-09-06T21:42:54.389257: step 5571, loss 0.0312352, acc 0.98
2016-09-06T21:42:55.080232: step 5572, loss 0.0479901, acc 0.96
2016-09-06T21:42:55.750643: step 5573, loss 0.0179584, acc 1
2016-09-06T21:42:56.426213: step 5574, loss 0.0186016, acc 1
2016-09-06T21:42:57.115608: step 5575, loss 0.0116639, acc 1
2016-09-06T21:42:57.810981: step 5576, loss 0.0453929, acc 0.98
2016-09-06T21:42:58.508552: step 5577, loss 0.0374728, acc 0.98
2016-09-06T21:42:59.178507: step 5578, loss 0.0352498, acc 0.98
2016-09-06T21:42:59.888607: step 5579, loss 0.00646375, acc 1
2016-09-06T21:43:00.596653: step 5580, loss 0.0224401, acc 1
2016-09-06T21:43:01.282312: step 5581, loss 0.041878, acc 0.96
2016-09-06T21:43:01.981866: step 5582, loss 0.032292, acc 0.98
2016-09-06T21:43:02.664402: step 5583, loss 0.00784695, acc 1
2016-09-06T21:43:03.356820: step 5584, loss 0.00783375, acc 1
2016-09-06T21:43:04.032829: step 5585, loss 0.00782903, acc 1
2016-09-06T21:43:04.746810: step 5586, loss 0.0301732, acc 0.98
2016-09-06T21:43:05.417896: step 5587, loss 0.00892418, acc 1
2016-09-06T21:43:06.094842: step 5588, loss 0.0128264, acc 1
2016-09-06T21:43:06.781269: step 5589, loss 0.037533, acc 0.98
2016-09-06T21:43:07.465664: step 5590, loss 0.0390025, acc 0.98
2016-09-06T21:43:08.165096: step 5591, loss 0.00986791, acc 1
2016-09-06T21:43:08.839306: step 5592, loss 0.0209311, acc 0.98
2016-09-06T21:43:09.540893: step 5593, loss 0.00780233, acc 1
2016-09-06T21:43:10.218788: step 5594, loss 0.00817534, acc 1
2016-09-06T21:43:10.908839: step 5595, loss 0.0236193, acc 0.98
2016-09-06T21:43:11.575761: step 5596, loss 0.0101788, acc 1
2016-09-06T21:43:12.258400: step 5597, loss 0.00319135, acc 1
2016-09-06T21:43:12.936812: step 5598, loss 0.0252807, acc 1
2016-09-06T21:43:13.590407: step 5599, loss 0.0156386, acc 1
2016-09-06T21:43:14.282994: step 5600, loss 0.027575, acc 1

Evaluation:
2016-09-06T21:43:17.407424: step 5600, loss 2.49036, acc 0.754221

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-5600

2016-09-06T21:43:19.170184: step 5601, loss 0.0209441, acc 1
2016-09-06T21:43:19.866150: step 5602, loss 0.000333208, acc 1
2016-09-06T21:43:20.547928: step 5603, loss 0.0074446, acc 1
2016-09-06T21:43:21.246243: step 5604, loss 0.00617853, acc 1
2016-09-06T21:43:21.914927: step 5605, loss 0.00758113, acc 1
2016-09-06T21:43:22.630172: step 5606, loss 0.0917596, acc 0.96
2016-09-06T21:43:23.293745: step 5607, loss 0.0107554, acc 1
2016-09-06T21:43:23.982826: step 5608, loss 0.00437999, acc 1
2016-09-06T21:43:24.666163: step 5609, loss 0.018539, acc 0.98
2016-09-06T21:43:25.361801: step 5610, loss 0.000175458, acc 1
2016-09-06T21:43:26.047841: step 5611, loss 0.0283603, acc 0.98
2016-09-06T21:43:26.732494: step 5612, loss 0.0259214, acc 0.98
2016-09-06T21:43:27.418297: step 5613, loss 0.0214918, acc 0.98
2016-09-06T21:43:28.090312: step 5614, loss 0.023278, acc 0.98
2016-09-06T21:43:28.795026: step 5615, loss 0.0438747, acc 0.98
2016-09-06T21:43:29.485007: step 5616, loss 0.00619945, acc 1
2016-09-06T21:43:30.167441: step 5617, loss 0.032832, acc 0.98
2016-09-06T21:43:30.845085: step 5618, loss 0.0738103, acc 0.96
2016-09-06T21:43:31.541039: step 5619, loss 0.0312532, acc 0.98
2016-09-06T21:43:32.231894: step 5620, loss 0.000452087, acc 1
2016-09-06T21:43:32.895677: step 5621, loss 0.000784813, acc 1
2016-09-06T21:43:33.596659: step 5622, loss 0.00588538, acc 1
2016-09-06T21:43:34.281752: step 5623, loss 0.0474598, acc 0.96
2016-09-06T21:43:34.966845: step 5624, loss 0.0206769, acc 0.98
2016-09-06T21:43:35.646732: step 5625, loss 0.00391001, acc 1
2016-09-06T21:43:36.342878: step 5626, loss 0.02904, acc 0.98
2016-09-06T21:43:37.031761: step 5627, loss 0.0153671, acc 1
2016-09-06T21:43:37.684570: step 5628, loss 0.0128828, acc 1
2016-09-06T21:43:38.380124: step 5629, loss 0.000158539, acc 1
2016-09-06T21:43:39.061378: step 5630, loss 0.00509441, acc 1
2016-09-06T21:43:39.740592: step 5631, loss 0.359188, acc 0.94
2016-09-06T21:43:40.434311: step 5632, loss 0.0210667, acc 0.98
2016-09-06T21:43:41.129697: step 5633, loss 0.010918, acc 1
2016-09-06T21:43:41.802439: step 5634, loss 0.0113698, acc 1
2016-09-06T21:43:42.459521: step 5635, loss 0.0172676, acc 1
2016-09-06T21:43:43.154472: step 5636, loss 0.0392632, acc 0.98
2016-09-06T21:43:43.821787: step 5637, loss 0.0782134, acc 0.94
2016-09-06T21:43:44.508064: step 5638, loss 0.00252098, acc 1
2016-09-06T21:43:45.192363: step 5639, loss 0.0291242, acc 1
2016-09-06T21:43:45.881425: step 5640, loss 0.0333561, acc 0.98
2016-09-06T21:43:46.583674: step 5641, loss 0.0156194, acc 1
2016-09-06T21:43:47.254639: step 5642, loss 0.0074798, acc 1
2016-09-06T21:43:47.939313: step 5643, loss 0.217029, acc 0.92
2016-09-06T21:43:48.607020: step 5644, loss 0.0139072, acc 1
2016-09-06T21:43:49.277462: step 5645, loss 0.0269734, acc 1
2016-09-06T21:43:49.947820: step 5646, loss 0.0166279, acc 0.98
2016-09-06T21:43:50.629343: step 5647, loss 0.0241671, acc 0.98
2016-09-06T21:43:51.321516: step 5648, loss 0.00459987, acc 1
2016-09-06T21:43:51.993104: step 5649, loss 0.0349108, acc 0.98
2016-09-06T21:43:52.678416: step 5650, loss 0.0167347, acc 1
2016-09-06T21:43:53.339593: step 5651, loss 0.0292395, acc 1
2016-09-06T21:43:54.054821: step 5652, loss 0.0382286, acc 0.98
2016-09-06T21:43:54.724335: step 5653, loss 0.0104225, acc 1
2016-09-06T21:43:55.415059: step 5654, loss 0.0102633, acc 1
2016-09-06T21:43:56.126982: step 5655, loss 0.0591248, acc 0.96
2016-09-06T21:43:56.822063: step 5656, loss 0.0968511, acc 0.96
2016-09-06T21:43:57.518633: step 5657, loss 0.129131, acc 0.94
2016-09-06T21:43:58.191696: step 5658, loss 0.043578, acc 0.98
2016-09-06T21:43:58.894909: step 5659, loss 0.00898861, acc 1
2016-09-06T21:43:59.568857: step 5660, loss 0.0164147, acc 0.98
2016-09-06T21:44:00.265595: step 5661, loss 0.00855387, acc 1
2016-09-06T21:44:00.960925: step 5662, loss 0.0140343, acc 1
2016-09-06T21:44:01.665447: step 5663, loss 0.0159763, acc 1
2016-09-06T21:44:02.361645: step 5664, loss 0.0229522, acc 1
2016-09-06T21:44:03.035972: step 5665, loss 0.0307638, acc 0.98
2016-09-06T21:44:03.727348: step 5666, loss 0.013809, acc 1
2016-09-06T21:44:04.412606: step 5667, loss 0.030269, acc 0.98
2016-09-06T21:44:05.106098: step 5668, loss 0.0222377, acc 0.98
2016-09-06T21:44:05.781534: step 5669, loss 0.00640626, acc 1
2016-09-06T21:44:06.458638: step 5670, loss 0.00317904, acc 1
2016-09-06T21:44:07.151797: step 5671, loss 0.00895636, acc 1
2016-09-06T21:44:07.819691: step 5672, loss 0.00264878, acc 1
2016-09-06T21:44:08.530392: step 5673, loss 0.0343838, acc 1
2016-09-06T21:44:09.222361: step 5674, loss 0.025562, acc 1
2016-09-06T21:44:09.909737: step 5675, loss 0.00435582, acc 1
2016-09-06T21:44:10.582548: step 5676, loss 0.0136456, acc 1
2016-09-06T21:44:11.261031: step 5677, loss 0.0953765, acc 0.94
2016-09-06T21:44:11.954110: step 5678, loss 0.0119215, acc 1
2016-09-06T21:44:12.629061: step 5679, loss 0.0243481, acc 0.98
2016-09-06T21:44:13.337770: step 5680, loss 0.0323168, acc 0.98
2016-09-06T21:44:14.011765: step 5681, loss 0.017537, acc 1
2016-09-06T21:44:14.693728: step 5682, loss 0.0399495, acc 0.98
2016-09-06T21:44:15.387654: step 5683, loss 0.0755125, acc 0.96
2016-09-06T21:44:16.088169: step 5684, loss 0.011842, acc 1
2016-09-06T21:44:16.813504: step 5685, loss 0.0137808, acc 1
2016-09-06T21:44:17.493823: step 5686, loss 0.0241192, acc 1
2016-09-06T21:44:18.208418: step 5687, loss 0.039308, acc 0.98
2016-09-06T21:44:18.908290: step 5688, loss 0.00540875, acc 1
2016-09-06T21:44:19.605332: step 5689, loss 0.0160734, acc 1
2016-09-06T21:44:20.286327: step 5690, loss 0.000743489, acc 1
2016-09-06T21:44:20.958717: step 5691, loss 0.0035697, acc 1
2016-09-06T21:44:21.658284: step 5692, loss 0.0400803, acc 0.98
2016-09-06T21:44:22.319148: step 5693, loss 0.00385294, acc 1
2016-09-06T21:44:23.014949: step 5694, loss 0.0266222, acc 1
2016-09-06T21:44:23.700242: step 5695, loss 0.033729, acc 0.98
2016-09-06T21:44:24.385446: step 5696, loss 0.0230148, acc 1
2016-09-06T21:44:25.055539: step 5697, loss 1.254e-05, acc 1
2016-09-06T21:44:25.729100: step 5698, loss 0.0200204, acc 0.98
2016-09-06T21:44:26.424153: step 5699, loss 0.0604152, acc 0.96
2016-09-06T21:44:27.099802: step 5700, loss 0.136369, acc 0.98

Evaluation:
2016-09-06T21:44:30.202670: step 5700, loss 2.76843, acc 0.73546

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473165300/checkpoints/model-5700

2016-09-06T21:44:31.907638: step 5701, loss 0.14698, acc 0.98
2016-09-06T21:44:32.586344: step 5702, loss 0.0237116, acc 0.98
2016-09-06T21:44:33.275081: step 5703, loss 0.0115764, acc 1
2016-09-06T21:44:33.955489: step 5704, loss 0.0101817, acc 1
2016-09-06T21:44:34.642567: step 5705, loss 0.0266391, acc 0.98
2016-09-06T21:44:35.330696: step 5706, loss 0.00427365, acc 1
2016-09-06T21:44:36.037621: step 5707, loss 0.0100748, acc 1
2016-09-06T21:44:36.716752: step 5708, loss 0.0223539, acc 1
2016-09-06T21:44:37.388073: step 5709, loss 0.0427071, acc 0.98
2016-09-06T21:44:38.069410: step 5710, loss 0.0119142, acc 1
2016-09-06T21:44:38.747831: step 5711, loss 0.0357033, acc 0.98
2016-09-06T21:44:39.437888: step 5712, loss 0.0192833, acc 0.98
2016-09-06T21:44:40.131024: step 5713, loss 0.0216627, acc 1
2016-09-06T21:44:40.812594: step 5714, loss 0.0104039, acc 1
2016-09-06T21:44:41.489528: step 5715, loss 0.0283574, acc 0.98
2016-09-06T21:44:42.187319: step 5716, loss 0.00949016, acc 1
2016-09-06T21:44:42.867309: step 5717, loss 0.0334565, acc 0.98
2016-09-06T21:44:43.596909: step 5718, loss 0.00642683, acc 1
2016-09-06T21:44:44.288694: step 5719, loss 0.0830989, acc 0.98
2016-09-06T21:44:44.981814: step 5720, loss 0.0121331, acc 1
2016-09-06T21:44:45.687249: step 5721, loss 0.0029172, acc 1
2016-09-06T21:44:46.334217: step 5722, loss 0.188091, acc 0.96
2016-09-06T21:44:47.001745: step 5723, loss 0.0351122, acc 0.98
2016-09-06T21:44:47.668130: step 5724, loss 0.000379852, acc 1
2016-09-06T21:44:48.344120: step 5725, loss 0.0217959, acc 0.98
2016-09-06T21:44:49.016492: step 5726, loss 0.0523909, acc 0.94
2016-09-06T21:44:49.689029: step 5727, loss 0.00200061, acc 1
2016-09-06T21:44:50.392660: step 5728, loss 0.058552, acc 0.98
2016-09-06T21:44:51.066278: step 5729, loss 0.129368, acc 0.98
2016-09-06T21:44:51.773937: step 5730, loss 0.111313, acc 0.96
2016-09-06T21:44:52.450716: step 5731, loss 0.0110744, acc 1
2016-09-06T21:44:53.154911: step 5732, loss 0.0118075, acc 1
2016-09-06T21:44:53.833726: step 5733, loss 0.00819233, acc 1
2016-09-06T21:44:54.514806: step 5734, loss 0.0106153, acc 1
2016-09-06T21:44:55.202013: step 5735, loss 0.0515142, acc 0.98
2016-09-06T21:44:55.892412: step 5736, loss 0.0290281, acc 0.98
2016-09-06T21:44:56.609750: step 5737, loss 0.0401859, acc 0.98
2016-09-06T21:44:57.286571: step 5738, loss 0.0018011, acc 1
2016-09-06T21:44:57.981290: step 5739, loss 0.00634762, acc 1
2016-09-06T21:44:58.635552: step 5740, loss 0.0188583, acc 0.98
2016-09-06T21:44:59.325620: step 5741, loss 0.0788842, acc 0.96
2016-09-06T21:45:00.018755: step 5742, loss 0.00333317, acc 1
2016-09-06T21:45:00.707461: step 5743, loss 0.0302214, acc 0.98
2016-09-06T21:45:01.433079: step 5744, loss 0.030169, acc 1
2016-09-06T21:45:02.138596: step 5745, loss 0.0364268, acc 0.98
2016-09-06T21:45:02.831037: step 5746, loss 0.0671725, acc 0.96
2016-09-06T21:45:03.532424: step 5747, loss 0.00334446, acc 1
2016-09-06T21:45:04.215755: step 5748, loss 0.0138354, acc 1
2016-09-06T21:45:04.924733: step 5749, loss 0.0478069, acc 0.98
2016-09-06T21:45:05.590030: step 5750, loss 0.00890425, acc 1
2016-09-06T21:45:06.281692: step 5751, loss 0.0143378, acc 1
2016-09-06T21:45:06.991927: step 5752, loss 0.0345143, acc 0.98
2016-09-06T21:45:07.680324: step 5753, loss 0.0348168, acc 0.98
2016-09-06T21:45:08.385426: step 5754, loss 0.022737, acc 0.98
2016-09-06T21:45:09.038769: step 5755, loss 0.055948, acc 0.96
2016-09-06T21:45:09.748829: step 5756, loss 0.0110205, acc 1
2016-09-06T21:45:10.417045: step 5757, loss 0.053113, acc 0.96
2016-09-06T21:45:11.104505: step 5758, loss 0.0230348, acc 0.98
2016-09-06T21:45:11.795963: step 5759, loss 0.0513672, acc 0.98
2016-09-06T21:45:12.440915: step 5760, loss 0.000334078, acc 1

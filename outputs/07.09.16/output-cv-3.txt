WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f7c4a1b4e90>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.
WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f7c4a1b4e50>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.

Parameters:
ALLOW_SOFT_PLACEMENT=True
BATCH_SIZE=50
CHECKPOINT_EVERY=100
CV_INDEX=3
DROPOUT_KEEP_PROB=0.5
EMBEDDING_DIM=300
EVALUATE_EVERY=100
FILTER_SIZES=3,4,5
HIDDEN_DIM=300
L2_REG_LAMBDA=0
LOG_DEVICE_PLACEMENT=False
NUM_EPOCHS=30
NUM_FILTERS=100
WORD2VEC=GoogleNews-vectors-negative300.bin

Loading data...
Vocabulary Size: 18758
Train/Dev split: 9594/1066
Writing to /home/cil/lstm-context-embeddings/runs/1473169514

Load word2vec file GoogleNews-vectors-negative300.bin

2016-09-06T21:45:33.574889: step 1, loss 0.693147, acc 0.62
2016-09-06T21:45:34.269107: step 2, loss 0.702111, acc 0.5
2016-09-06T21:45:34.969534: step 3, loss 0.755898, acc 0.4
2016-09-06T21:45:35.650996: step 4, loss 0.684921, acc 0.54
2016-09-06T21:45:36.325097: step 5, loss 0.692007, acc 0.48
2016-09-06T21:45:37.013457: step 6, loss 0.700787, acc 0.44
2016-09-06T21:45:37.698558: step 7, loss 0.698888, acc 0.38
2016-09-06T21:45:38.371240: step 8, loss 0.685463, acc 0.58
2016-09-06T21:45:39.081251: step 9, loss 0.693963, acc 0.48
2016-09-06T21:45:39.790446: step 10, loss 0.701979, acc 0.5
2016-09-06T21:45:40.449104: step 11, loss 0.735505, acc 0.44
2016-09-06T21:45:41.123472: step 12, loss 0.712816, acc 0.46
2016-09-06T21:45:41.815795: step 13, loss 0.697791, acc 0.52
2016-09-06T21:45:42.470459: step 14, loss 0.691738, acc 0.56
2016-09-06T21:45:43.158431: step 15, loss 0.684554, acc 0.54
2016-09-06T21:45:43.858661: step 16, loss 0.70048, acc 0.52
2016-09-06T21:45:44.565977: step 17, loss 0.673947, acc 0.54
2016-09-06T21:45:45.245114: step 18, loss 0.692617, acc 0.5
2016-09-06T21:45:45.926780: step 19, loss 0.693207, acc 0.5
2016-09-06T21:45:46.616844: step 20, loss 0.691997, acc 0.52
2016-09-06T21:45:47.291544: step 21, loss 0.683619, acc 0.52
2016-09-06T21:45:47.978716: step 22, loss 0.709151, acc 0.46
2016-09-06T21:45:48.674496: step 23, loss 0.653976, acc 0.6
2016-09-06T21:45:49.367931: step 24, loss 0.680838, acc 0.5
2016-09-06T21:45:50.018670: step 25, loss 0.675723, acc 0.52
2016-09-06T21:45:50.731759: step 26, loss 0.628282, acc 0.56
2016-09-06T21:45:51.420941: step 27, loss 0.637433, acc 0.62
2016-09-06T21:45:52.127397: step 28, loss 0.624793, acc 0.66
2016-09-06T21:45:52.829456: step 29, loss 0.636316, acc 0.64
2016-09-06T21:45:53.508805: step 30, loss 0.658375, acc 0.6
2016-09-06T21:45:54.220999: step 31, loss 0.529422, acc 0.68
2016-09-06T21:45:54.882221: step 32, loss 0.582188, acc 0.74
2016-09-06T21:45:55.567005: step 33, loss 0.710007, acc 0.66
2016-09-06T21:45:56.271209: step 34, loss 0.602221, acc 0.6
2016-09-06T21:45:56.968720: step 35, loss 0.857452, acc 0.58
2016-09-06T21:45:57.671756: step 36, loss 0.705839, acc 0.56
2016-09-06T21:45:58.336021: step 37, loss 0.63401, acc 0.66
2016-09-06T21:45:59.037577: step 38, loss 0.580501, acc 0.78
2016-09-06T21:45:59.707861: step 39, loss 0.646523, acc 0.6
2016-09-06T21:46:00.421668: step 40, loss 0.663068, acc 0.5
2016-09-06T21:46:01.118792: step 41, loss 0.696384, acc 0.54
2016-09-06T21:46:01.797062: step 42, loss 0.668668, acc 0.68
2016-09-06T21:46:02.490741: step 43, loss 0.676236, acc 0.58
2016-09-06T21:46:03.190210: step 44, loss 0.694572, acc 0.58
2016-09-06T21:46:03.898601: step 45, loss 0.714322, acc 0.56
2016-09-06T21:46:04.568126: step 46, loss 0.677891, acc 0.56
2016-09-06T21:46:05.256690: step 47, loss 0.640949, acc 0.74
2016-09-06T21:46:05.956869: step 48, loss 0.631356, acc 0.64
2016-09-06T21:46:06.668087: step 49, loss 0.7006, acc 0.5
2016-09-06T21:46:07.380050: step 50, loss 0.68331, acc 0.54
2016-09-06T21:46:08.050144: step 51, loss 0.67078, acc 0.6
2016-09-06T21:46:08.748888: step 52, loss 0.645691, acc 0.7
2016-09-06T21:46:09.469100: step 53, loss 0.591844, acc 0.72
2016-09-06T21:46:10.141626: step 54, loss 0.642826, acc 0.66
2016-09-06T21:46:10.824174: step 55, loss 0.57895, acc 0.76
2016-09-06T21:46:11.527093: step 56, loss 0.627488, acc 0.66
2016-09-06T21:46:12.244921: step 57, loss 0.539592, acc 0.74
2016-09-06T21:46:12.914385: step 58, loss 0.608403, acc 0.64
2016-09-06T21:46:13.626779: step 59, loss 0.73473, acc 0.56
2016-09-06T21:46:14.298712: step 60, loss 0.705765, acc 0.6
2016-09-06T21:46:15.031364: step 61, loss 0.558262, acc 0.68
2016-09-06T21:46:15.723306: step 62, loss 0.733846, acc 0.6
2016-09-06T21:46:16.404684: step 63, loss 0.610563, acc 0.6
2016-09-06T21:46:17.130173: step 64, loss 0.649992, acc 0.6
2016-09-06T21:46:17.824483: step 65, loss 0.669469, acc 0.58
2016-09-06T21:46:18.519535: step 66, loss 0.672053, acc 0.58
2016-09-06T21:46:19.189242: step 67, loss 0.5428, acc 0.8
2016-09-06T21:46:19.881120: step 68, loss 0.582979, acc 0.7
2016-09-06T21:46:20.588963: step 69, loss 0.555604, acc 0.76
2016-09-06T21:46:21.251672: step 70, loss 0.558651, acc 0.72
2016-09-06T21:46:21.932310: step 71, loss 0.572894, acc 0.74
2016-09-06T21:46:22.625558: step 72, loss 0.554173, acc 0.7
2016-09-06T21:46:23.301614: step 73, loss 0.624524, acc 0.64
2016-09-06T21:46:23.989650: step 74, loss 0.609484, acc 0.68
2016-09-06T21:46:24.679470: step 75, loss 0.451394, acc 0.86
2016-09-06T21:46:25.388781: step 76, loss 0.500701, acc 0.74
2016-09-06T21:46:26.054261: step 77, loss 0.559724, acc 0.72
2016-09-06T21:46:26.734357: step 78, loss 0.499975, acc 0.74
2016-09-06T21:46:27.417182: step 79, loss 0.591586, acc 0.7
2016-09-06T21:46:28.116571: step 80, loss 0.589125, acc 0.72
2016-09-06T21:46:28.813672: step 81, loss 0.53583, acc 0.74
2016-09-06T21:46:29.506066: step 82, loss 0.552964, acc 0.74
2016-09-06T21:46:30.222243: step 83, loss 0.601237, acc 0.78
2016-09-06T21:46:30.881570: step 84, loss 0.498258, acc 0.78
2016-09-06T21:46:31.576256: step 85, loss 0.61288, acc 0.64
2016-09-06T21:46:32.252031: step 86, loss 0.529629, acc 0.74
2016-09-06T21:46:32.935234: step 87, loss 0.509966, acc 0.74
2016-09-06T21:46:33.637518: step 88, loss 0.487798, acc 0.78
2016-09-06T21:46:34.344973: step 89, loss 0.543876, acc 0.74
2016-09-06T21:46:35.033758: step 90, loss 0.507947, acc 0.72
2016-09-06T21:46:35.694716: step 91, loss 0.679553, acc 0.68
2016-09-06T21:46:36.389215: step 92, loss 0.462908, acc 0.76
2016-09-06T21:46:37.073408: step 93, loss 0.6283, acc 0.68
2016-09-06T21:46:37.746697: step 94, loss 0.647654, acc 0.58
2016-09-06T21:46:38.424307: step 95, loss 0.474606, acc 0.8
2016-09-06T21:46:39.097763: step 96, loss 0.516351, acc 0.78
2016-09-06T21:46:39.792921: step 97, loss 0.64792, acc 0.66
2016-09-06T21:46:40.463346: step 98, loss 0.608962, acc 0.62
2016-09-06T21:46:41.160198: step 99, loss 0.562405, acc 0.72
2016-09-06T21:46:41.831434: step 100, loss 0.605346, acc 0.68

Evaluation:
2016-09-06T21:46:44.973689: step 100, loss 0.526101, acc 0.754221

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-100

2016-09-06T21:46:46.737058: step 101, loss 0.485082, acc 0.82
2016-09-06T21:46:47.442804: step 102, loss 0.527238, acc 0.78
2016-09-06T21:46:48.135124: step 103, loss 0.514081, acc 0.7
2016-09-06T21:46:48.827866: step 104, loss 0.631837, acc 0.62
2016-09-06T21:46:49.532598: step 105, loss 0.44576, acc 0.78
2016-09-06T21:46:50.211850: step 106, loss 0.419068, acc 0.86
2016-09-06T21:46:50.885403: step 107, loss 0.545075, acc 0.74
2016-09-06T21:46:51.574890: step 108, loss 0.435854, acc 0.8
2016-09-06T21:46:52.255801: step 109, loss 0.507489, acc 0.74
2016-09-06T21:46:52.937268: step 110, loss 0.488445, acc 0.74
2016-09-06T21:46:53.616963: step 111, loss 0.581308, acc 0.72
2016-09-06T21:46:54.314120: step 112, loss 0.463433, acc 0.72
2016-09-06T21:46:54.999090: step 113, loss 0.929238, acc 0.72
2016-09-06T21:46:55.719144: step 114, loss 0.419751, acc 0.74
2016-09-06T21:46:56.406900: step 115, loss 0.525258, acc 0.78
2016-09-06T21:46:57.085865: step 116, loss 0.49763, acc 0.74
2016-09-06T21:46:57.788566: step 117, loss 0.428927, acc 0.82
2016-09-06T21:46:58.433125: step 118, loss 0.563387, acc 0.7
2016-09-06T21:46:59.127928: step 119, loss 0.536859, acc 0.72
2016-09-06T21:46:59.794612: step 120, loss 0.55344, acc 0.7
2016-09-06T21:47:00.488922: step 121, loss 0.375163, acc 0.84
2016-09-06T21:47:01.178712: step 122, loss 0.607827, acc 0.74
2016-09-06T21:47:01.859248: step 123, loss 0.389166, acc 0.8
2016-09-06T21:47:02.543539: step 124, loss 0.518314, acc 0.76
2016-09-06T21:47:03.219820: step 125, loss 0.370896, acc 0.8
2016-09-06T21:47:03.973120: step 126, loss 0.614117, acc 0.68
2016-09-06T21:47:04.661047: step 127, loss 0.507835, acc 0.76
2016-09-06T21:47:05.351899: step 128, loss 0.479371, acc 0.8
2016-09-06T21:47:06.043244: step 129, loss 0.564359, acc 0.7
2016-09-06T21:47:06.713394: step 130, loss 0.620872, acc 0.66
2016-09-06T21:47:07.401898: step 131, loss 0.627385, acc 0.64
2016-09-06T21:47:08.074956: step 132, loss 0.425012, acc 0.78
2016-09-06T21:47:08.776986: step 133, loss 0.551843, acc 0.74
2016-09-06T21:47:09.431540: step 134, loss 0.453751, acc 0.78
2016-09-06T21:47:10.110033: step 135, loss 0.526755, acc 0.76
2016-09-06T21:47:10.791021: step 136, loss 0.522168, acc 0.74
2016-09-06T21:47:11.476778: step 137, loss 0.446939, acc 0.84
2016-09-06T21:47:12.165109: step 138, loss 0.537865, acc 0.74
2016-09-06T21:47:12.856859: step 139, loss 0.540304, acc 0.78
2016-09-06T21:47:13.558010: step 140, loss 0.633043, acc 0.68
2016-09-06T21:47:14.236630: step 141, loss 0.417919, acc 0.82
2016-09-06T21:47:14.911664: step 142, loss 0.579211, acc 0.72
2016-09-06T21:47:15.603206: step 143, loss 0.621448, acc 0.64
2016-09-06T21:47:16.270569: step 144, loss 0.465244, acc 0.72
2016-09-06T21:47:16.949019: step 145, loss 0.600065, acc 0.72
2016-09-06T21:47:17.639390: step 146, loss 0.520395, acc 0.72
2016-09-06T21:47:18.347588: step 147, loss 0.498995, acc 0.74
2016-09-06T21:47:19.024305: step 148, loss 0.468383, acc 0.78
2016-09-06T21:47:19.702225: step 149, loss 0.51428, acc 0.76
2016-09-06T21:47:20.388010: step 150, loss 0.634071, acc 0.7
2016-09-06T21:47:21.066501: step 151, loss 0.457669, acc 0.78
2016-09-06T21:47:21.745825: step 152, loss 0.439777, acc 0.82
2016-09-06T21:47:22.426874: step 153, loss 0.56742, acc 0.7
2016-09-06T21:47:23.118010: step 154, loss 0.513419, acc 0.74
2016-09-06T21:47:23.780116: step 155, loss 0.496445, acc 0.7
2016-09-06T21:47:24.461345: step 156, loss 0.451257, acc 0.78
2016-09-06T21:47:25.137872: step 157, loss 0.483754, acc 0.82
2016-09-06T21:47:25.825352: step 158, loss 0.52069, acc 0.76
2016-09-06T21:47:26.497323: step 159, loss 0.505226, acc 0.68
2016-09-06T21:47:27.211455: step 160, loss 0.550062, acc 0.82
2016-09-06T21:47:27.899256: step 161, loss 0.542886, acc 0.74
2016-09-06T21:47:28.562028: step 162, loss 0.478278, acc 0.76
2016-09-06T21:47:29.275062: step 163, loss 0.41115, acc 0.84
2016-09-06T21:47:30.003124: step 164, loss 0.459986, acc 0.78
2016-09-06T21:47:30.709762: step 165, loss 0.486436, acc 0.74
2016-09-06T21:47:31.410915: step 166, loss 0.607347, acc 0.76
2016-09-06T21:47:32.091703: step 167, loss 0.357577, acc 0.84
2016-09-06T21:47:32.802899: step 168, loss 0.509575, acc 0.74
2016-09-06T21:47:33.481954: step 169, loss 0.444248, acc 0.76
2016-09-06T21:47:34.164900: step 170, loss 0.541062, acc 0.68
2016-09-06T21:47:34.848722: step 171, loss 0.375262, acc 0.78
2016-09-06T21:47:35.510195: step 172, loss 0.512465, acc 0.7
2016-09-06T21:47:36.181929: step 173, loss 0.48247, acc 0.8
2016-09-06T21:47:36.850502: step 174, loss 0.493735, acc 0.74
2016-09-06T21:47:37.574914: step 175, loss 0.462551, acc 0.7
2016-09-06T21:47:38.239006: step 176, loss 0.427333, acc 0.8
2016-09-06T21:47:38.921330: step 177, loss 0.495024, acc 0.76
2016-09-06T21:47:39.613020: step 178, loss 0.60552, acc 0.68
2016-09-06T21:47:40.287552: step 179, loss 0.511985, acc 0.76
2016-09-06T21:47:40.980781: step 180, loss 0.482579, acc 0.8
2016-09-06T21:47:41.661283: step 181, loss 0.399286, acc 0.84
2016-09-06T21:47:42.379363: step 182, loss 0.424921, acc 0.8
2016-09-06T21:47:43.090297: step 183, loss 0.496959, acc 0.7
2016-09-06T21:47:43.801449: step 184, loss 0.428635, acc 0.88
2016-09-06T21:47:44.490481: step 185, loss 0.442361, acc 0.78
2016-09-06T21:47:45.180399: step 186, loss 0.524797, acc 0.76
2016-09-06T21:47:45.869586: step 187, loss 0.453761, acc 0.8
2016-09-06T21:47:46.539472: step 188, loss 0.503557, acc 0.74
2016-09-06T21:47:47.239772: step 189, loss 0.489859, acc 0.82
2016-09-06T21:47:47.906512: step 190, loss 0.633214, acc 0.66
2016-09-06T21:47:48.572985: step 191, loss 0.619732, acc 0.64
2016-09-06T21:47:49.245454: step 192, loss 0.410684, acc 0.795455
2016-09-06T21:47:49.928909: step 193, loss 0.403825, acc 0.86
2016-09-06T21:47:50.618300: step 194, loss 0.454465, acc 0.8
2016-09-06T21:47:51.309378: step 195, loss 0.38543, acc 0.82
2016-09-06T21:47:52.011386: step 196, loss 0.435718, acc 0.82
2016-09-06T21:47:52.689296: step 197, loss 0.349433, acc 0.82
2016-09-06T21:47:53.374958: step 198, loss 0.412259, acc 0.82
2016-09-06T21:47:54.079195: step 199, loss 0.374808, acc 0.86
2016-09-06T21:47:54.770125: step 200, loss 0.384846, acc 0.8

Evaluation:
2016-09-06T21:47:57.914100: step 200, loss 0.472817, acc 0.770169

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-200

2016-09-06T21:47:59.590217: step 201, loss 0.336808, acc 0.88
2016-09-06T21:48:00.297231: step 202, loss 0.463425, acc 0.78
2016-09-06T21:48:00.977363: step 203, loss 0.458956, acc 0.78
2016-09-06T21:48:01.685298: step 204, loss 0.425224, acc 0.8
2016-09-06T21:48:02.389529: step 205, loss 0.405452, acc 0.82
2016-09-06T21:48:03.078392: step 206, loss 0.294019, acc 0.92
2016-09-06T21:48:03.771190: step 207, loss 0.552252, acc 0.8
2016-09-06T21:48:04.450266: step 208, loss 0.352458, acc 0.86
2016-09-06T21:48:05.131290: step 209, loss 0.385209, acc 0.82
2016-09-06T21:48:05.791119: step 210, loss 0.329564, acc 0.84
2016-09-06T21:48:06.503607: step 211, loss 0.321765, acc 0.88
2016-09-06T21:48:07.166511: step 212, loss 0.367201, acc 0.84
2016-09-06T21:48:07.869248: step 213, loss 0.265806, acc 0.9
2016-09-06T21:48:08.548458: step 214, loss 0.336425, acc 0.86
2016-09-06T21:48:09.219448: step 215, loss 0.470918, acc 0.78
2016-09-06T21:48:09.903879: step 216, loss 0.229655, acc 0.92
2016-09-06T21:48:10.581827: step 217, loss 0.250972, acc 0.92
2016-09-06T21:48:11.279463: step 218, loss 0.37493, acc 0.84
2016-09-06T21:48:11.962873: step 219, loss 0.453076, acc 0.76
2016-09-06T21:48:12.646616: step 220, loss 0.313407, acc 0.88
2016-09-06T21:48:13.330110: step 221, loss 0.279098, acc 0.86
2016-09-06T21:48:14.012341: step 222, loss 0.323933, acc 0.84
2016-09-06T21:48:14.686398: step 223, loss 0.251028, acc 0.96
2016-09-06T21:48:15.372949: step 224, loss 0.476405, acc 0.78
2016-09-06T21:48:16.099666: step 225, loss 0.406384, acc 0.86
2016-09-06T21:48:16.758813: step 226, loss 0.544003, acc 0.7
2016-09-06T21:48:17.438967: step 227, loss 0.319171, acc 0.82
2016-09-06T21:48:18.122395: step 228, loss 0.277899, acc 0.92
2016-09-06T21:48:18.801771: step 229, loss 0.280847, acc 0.88
2016-09-06T21:48:19.491270: step 230, loss 0.519134, acc 0.8
2016-09-06T21:48:20.178169: step 231, loss 0.472676, acc 0.82
2016-09-06T21:48:20.895117: step 232, loss 0.359598, acc 0.78
2016-09-06T21:48:21.572241: step 233, loss 0.405999, acc 0.82
2016-09-06T21:48:22.234994: step 234, loss 0.383703, acc 0.82
2016-09-06T21:48:22.928119: step 235, loss 0.373181, acc 0.86
2016-09-06T21:48:23.610375: step 236, loss 0.218711, acc 0.9
2016-09-06T21:48:24.304086: step 237, loss 0.376268, acc 0.82
2016-09-06T21:48:24.987110: step 238, loss 0.394587, acc 0.88
2016-09-06T21:48:25.714566: step 239, loss 0.53942, acc 0.82
2016-09-06T21:48:26.387171: step 240, loss 0.29237, acc 0.9
2016-09-06T21:48:27.099497: step 241, loss 0.467933, acc 0.78
2016-09-06T21:48:27.789930: step 242, loss 0.344037, acc 0.84
2016-09-06T21:48:28.493742: step 243, loss 0.422398, acc 0.82
2016-09-06T21:48:29.190891: step 244, loss 0.370033, acc 0.82
2016-09-06T21:48:29.840013: step 245, loss 0.332329, acc 0.88
2016-09-06T21:48:30.532990: step 246, loss 0.428741, acc 0.84
2016-09-06T21:48:31.205772: step 247, loss 0.286687, acc 0.92
2016-09-06T21:48:31.895875: step 248, loss 0.392516, acc 0.84
2016-09-06T21:48:32.590915: step 249, loss 0.346408, acc 0.82
2016-09-06T21:48:33.246663: step 250, loss 0.355208, acc 0.8
2016-09-06T21:48:33.936002: step 251, loss 0.308316, acc 0.86
2016-09-06T21:48:34.607810: step 252, loss 0.402926, acc 0.74
2016-09-06T21:48:35.294699: step 253, loss 0.391323, acc 0.72
2016-09-06T21:48:35.999467: step 254, loss 0.235342, acc 0.92
2016-09-06T21:48:36.717108: step 255, loss 0.274616, acc 0.84
2016-09-06T21:48:37.393701: step 256, loss 0.340687, acc 0.84
2016-09-06T21:48:38.082137: step 257, loss 0.376367, acc 0.78
2016-09-06T21:48:38.767306: step 258, loss 0.295601, acc 0.88
2016-09-06T21:48:39.462206: step 259, loss 0.419942, acc 0.82
2016-09-06T21:48:40.158057: step 260, loss 0.639297, acc 0.74
2016-09-06T21:48:40.824995: step 261, loss 0.323761, acc 0.86
2016-09-06T21:48:41.489061: step 262, loss 0.336815, acc 0.9
2016-09-06T21:48:42.170749: step 263, loss 0.457331, acc 0.76
2016-09-06T21:48:42.843605: step 264, loss 0.394713, acc 0.82
2016-09-06T21:48:43.508979: step 265, loss 0.292125, acc 0.86
2016-09-06T21:48:44.188372: step 266, loss 0.46933, acc 0.82
2016-09-06T21:48:44.853415: step 267, loss 0.446897, acc 0.74
2016-09-06T21:48:45.524699: step 268, loss 0.275343, acc 0.86
2016-09-06T21:48:46.216042: step 269, loss 0.42698, acc 0.82
2016-09-06T21:48:46.882858: step 270, loss 0.389802, acc 0.78
2016-09-06T21:48:47.567119: step 271, loss 0.370166, acc 0.78
2016-09-06T21:48:48.250651: step 272, loss 0.326843, acc 0.88
2016-09-06T21:48:48.930242: step 273, loss 0.261941, acc 0.9
2016-09-06T21:48:49.607824: step 274, loss 0.325433, acc 0.82
2016-09-06T21:48:50.269248: step 275, loss 0.361575, acc 0.8
2016-09-06T21:48:50.999171: step 276, loss 0.435726, acc 0.78
2016-09-06T21:48:51.680656: step 277, loss 0.368109, acc 0.82
2016-09-06T21:48:52.371099: step 278, loss 0.509984, acc 0.8
2016-09-06T21:48:53.056211: step 279, loss 0.315845, acc 0.88
2016-09-06T21:48:53.750044: step 280, loss 0.386999, acc 0.8
2016-09-06T21:48:54.443044: step 281, loss 0.346072, acc 0.84
2016-09-06T21:48:55.100545: step 282, loss 0.34773, acc 0.84
2016-09-06T21:48:55.793383: step 283, loss 0.541903, acc 0.72
2016-09-06T21:48:56.478056: step 284, loss 0.312945, acc 0.9
2016-09-06T21:48:57.155816: step 285, loss 0.377917, acc 0.86
2016-09-06T21:48:57.864113: step 286, loss 0.215801, acc 0.9
2016-09-06T21:48:58.558353: step 287, loss 0.281835, acc 0.92
2016-09-06T21:48:59.238210: step 288, loss 0.469449, acc 0.78
2016-09-06T21:48:59.911684: step 289, loss 0.353465, acc 0.86
2016-09-06T21:49:00.656882: step 290, loss 0.394509, acc 0.8
2016-09-06T21:49:01.327954: step 291, loss 0.498601, acc 0.78
2016-09-06T21:49:01.996584: step 292, loss 0.490984, acc 0.78
2016-09-06T21:49:02.673413: step 293, loss 0.293829, acc 0.88
2016-09-06T21:49:03.386728: step 294, loss 0.288937, acc 0.82
2016-09-06T21:49:04.095893: step 295, loss 0.316146, acc 0.86
2016-09-06T21:49:04.755703: step 296, loss 0.445197, acc 0.84
2016-09-06T21:49:05.471429: step 297, loss 0.296831, acc 0.86
2016-09-06T21:49:06.146597: step 298, loss 0.277249, acc 0.92
2016-09-06T21:49:06.823055: step 299, loss 0.339286, acc 0.84
2016-09-06T21:49:07.490349: step 300, loss 0.373936, acc 0.84

Evaluation:
2016-09-06T21:49:10.624376: step 300, loss 0.444876, acc 0.778612

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-300

2016-09-06T21:49:12.250511: step 301, loss 0.296731, acc 0.88
2016-09-06T21:49:12.951523: step 302, loss 0.46207, acc 0.8
2016-09-06T21:49:13.624705: step 303, loss 0.31841, acc 0.82
2016-09-06T21:49:14.315397: step 304, loss 0.447746, acc 0.78
2016-09-06T21:49:15.029440: step 305, loss 0.36065, acc 0.84
2016-09-06T21:49:15.727216: step 306, loss 0.213052, acc 0.96
2016-09-06T21:49:16.410897: step 307, loss 0.426498, acc 0.76
2016-09-06T21:49:17.114887: step 308, loss 0.254217, acc 0.92
2016-09-06T21:49:17.813851: step 309, loss 0.352104, acc 0.86
2016-09-06T21:49:18.518751: step 310, loss 0.605297, acc 0.74
2016-09-06T21:49:19.189375: step 311, loss 0.384753, acc 0.86
2016-09-06T21:49:19.892572: step 312, loss 0.614201, acc 0.68
2016-09-06T21:49:20.573379: step 313, loss 0.341789, acc 0.82
2016-09-06T21:49:21.260495: step 314, loss 0.324525, acc 0.84
2016-09-06T21:49:21.941802: step 315, loss 0.340648, acc 0.86
2016-09-06T21:49:22.617902: step 316, loss 0.363287, acc 0.88
2016-09-06T21:49:23.313739: step 317, loss 0.38352, acc 0.78
2016-09-06T21:49:23.994595: step 318, loss 0.401599, acc 0.82
2016-09-06T21:49:24.722073: step 319, loss 0.316838, acc 0.88
2016-09-06T21:49:25.428147: step 320, loss 0.36363, acc 0.84
2016-09-06T21:49:26.141137: step 321, loss 0.363386, acc 0.88
2016-09-06T21:49:26.836341: step 322, loss 0.291739, acc 0.9
2016-09-06T21:49:27.533967: step 323, loss 0.366903, acc 0.84
2016-09-06T21:49:28.239630: step 324, loss 0.298929, acc 0.9
2016-09-06T21:49:28.887402: step 325, loss 0.33285, acc 0.86
2016-09-06T21:49:29.586743: step 326, loss 0.341053, acc 0.84
2016-09-06T21:49:30.270711: step 327, loss 0.316026, acc 0.86
2016-09-06T21:49:30.967373: step 328, loss 0.330381, acc 0.84
2016-09-06T21:49:31.655972: step 329, loss 0.370363, acc 0.82
2016-09-06T21:49:32.347205: step 330, loss 0.449443, acc 0.8
2016-09-06T21:49:33.047894: step 331, loss 0.490732, acc 0.7
2016-09-06T21:49:33.706524: step 332, loss 0.490473, acc 0.84
2016-09-06T21:49:34.393625: step 333, loss 0.466705, acc 0.84
2016-09-06T21:49:35.079208: step 334, loss 0.444519, acc 0.78
2016-09-06T21:49:35.774531: step 335, loss 0.255335, acc 0.92
2016-09-06T21:49:36.477549: step 336, loss 0.277685, acc 0.92
2016-09-06T21:49:37.146686: step 337, loss 0.524916, acc 0.72
2016-09-06T21:49:37.838897: step 338, loss 0.364149, acc 0.8
2016-09-06T21:49:38.508148: step 339, loss 0.326933, acc 0.88
2016-09-06T21:49:39.207700: step 340, loss 0.367121, acc 0.8
2016-09-06T21:49:39.871749: step 341, loss 0.407245, acc 0.78
2016-09-06T21:49:40.575075: step 342, loss 0.407539, acc 0.82
2016-09-06T21:49:41.266894: step 343, loss 0.386292, acc 0.76
2016-09-06T21:49:41.957668: step 344, loss 0.382934, acc 0.86
2016-09-06T21:49:42.657986: step 345, loss 0.299112, acc 0.86
2016-09-06T21:49:43.319878: step 346, loss 0.446222, acc 0.78
2016-09-06T21:49:44.012951: step 347, loss 0.330779, acc 0.86
2016-09-06T21:49:44.706277: step 348, loss 0.353472, acc 0.86
2016-09-06T21:49:45.394059: step 349, loss 0.255812, acc 0.9
2016-09-06T21:49:46.090566: step 350, loss 0.417701, acc 0.8
2016-09-06T21:49:46.772955: step 351, loss 0.401933, acc 0.8
2016-09-06T21:49:47.457200: step 352, loss 0.321557, acc 0.86
2016-09-06T21:49:48.129240: step 353, loss 0.404924, acc 0.76
2016-09-06T21:49:48.838173: step 354, loss 0.21764, acc 0.9
2016-09-06T21:49:49.521532: step 355, loss 0.21436, acc 0.94
2016-09-06T21:49:50.192303: step 356, loss 0.370594, acc 0.86
2016-09-06T21:49:50.873744: step 357, loss 0.336678, acc 0.86
2016-09-06T21:49:51.533280: step 358, loss 0.228286, acc 0.9
2016-09-06T21:49:52.220976: step 359, loss 0.29802, acc 0.84
2016-09-06T21:49:52.883202: step 360, loss 0.392566, acc 0.84
2016-09-06T21:49:53.588706: step 361, loss 0.316463, acc 0.88
2016-09-06T21:49:54.280398: step 362, loss 0.262337, acc 0.9
2016-09-06T21:49:54.960464: step 363, loss 0.44616, acc 0.78
2016-09-06T21:49:55.643945: step 364, loss 0.345567, acc 0.88
2016-09-06T21:49:56.344461: step 365, loss 0.301452, acc 0.86
2016-09-06T21:49:57.053765: step 366, loss 0.485458, acc 0.76
2016-09-06T21:49:57.725997: step 367, loss 0.373505, acc 0.84
2016-09-06T21:49:58.431535: step 368, loss 0.397794, acc 0.8
2016-09-06T21:49:59.131009: step 369, loss 0.349555, acc 0.82
2016-09-06T21:49:59.831992: step 370, loss 0.319733, acc 0.88
2016-09-06T21:50:00.560221: step 371, loss 0.50615, acc 0.74
2016-09-06T21:50:01.223874: step 372, loss 0.295535, acc 0.84
2016-09-06T21:50:01.908462: step 373, loss 0.40828, acc 0.82
2016-09-06T21:50:02.561336: step 374, loss 0.520116, acc 0.8
2016-09-06T21:50:03.243460: step 375, loss 0.339983, acc 0.88
2016-09-06T21:50:03.918976: step 376, loss 0.379676, acc 0.84
2016-09-06T21:50:04.594050: step 377, loss 0.5642, acc 0.84
2016-09-06T21:50:05.258047: step 378, loss 0.272294, acc 0.9
2016-09-06T21:50:05.936143: step 379, loss 0.432149, acc 0.82
2016-09-06T21:50:06.650610: step 380, loss 0.371104, acc 0.84
2016-09-06T21:50:07.317620: step 381, loss 0.479815, acc 0.78
2016-09-06T21:50:08.008814: step 382, loss 0.45694, acc 0.72
2016-09-06T21:50:08.691593: step 383, loss 0.303936, acc 0.86
2016-09-06T21:50:09.323418: step 384, loss 0.377887, acc 0.886364
2016-09-06T21:50:10.038290: step 385, loss 0.264764, acc 0.86
2016-09-06T21:50:10.718440: step 386, loss 0.193828, acc 0.96
2016-09-06T21:50:11.411992: step 387, loss 0.277591, acc 0.9
2016-09-06T21:50:12.077947: step 388, loss 0.207901, acc 0.94
2016-09-06T21:50:12.776076: step 389, loss 0.26354, acc 0.9
2016-09-06T21:50:13.443505: step 390, loss 0.320629, acc 0.84
2016-09-06T21:50:14.136392: step 391, loss 0.262757, acc 0.86
2016-09-06T21:50:14.819867: step 392, loss 0.335349, acc 0.78
2016-09-06T21:50:15.507939: step 393, loss 0.335729, acc 0.86
2016-09-06T21:50:16.203977: step 394, loss 0.0998023, acc 0.98
2016-09-06T21:50:16.890558: step 395, loss 0.268822, acc 0.9
2016-09-06T21:50:17.601796: step 396, loss 0.259013, acc 0.88
2016-09-06T21:50:18.270517: step 397, loss 0.325641, acc 0.88
2016-09-06T21:50:18.952214: step 398, loss 0.13837, acc 0.98
2016-09-06T21:50:19.631009: step 399, loss 0.337632, acc 0.86
2016-09-06T21:50:20.318237: step 400, loss 0.260246, acc 0.92

Evaluation:
2016-09-06T21:50:23.490824: step 400, loss 0.503819, acc 0.791745

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-400

2016-09-06T21:50:25.137495: step 401, loss 0.297432, acc 0.9
2016-09-06T21:50:25.835767: step 402, loss 0.200542, acc 0.92
2016-09-06T21:50:26.491929: step 403, loss 0.164421, acc 0.92
2016-09-06T21:50:27.190439: step 404, loss 0.238424, acc 0.92
2016-09-06T21:50:27.849075: step 405, loss 0.156393, acc 0.92
2016-09-06T21:50:28.538163: step 406, loss 0.438475, acc 0.8
2016-09-06T21:50:29.209717: step 407, loss 0.138641, acc 0.92
2016-09-06T21:50:29.902710: step 408, loss 0.119024, acc 0.98
2016-09-06T21:50:30.573101: step 409, loss 0.340427, acc 0.88
2016-09-06T21:50:31.261037: step 410, loss 0.224328, acc 0.96
2016-09-06T21:50:31.968018: step 411, loss 0.157568, acc 0.94
2016-09-06T21:50:32.642857: step 412, loss 0.215612, acc 0.92
2016-09-06T21:50:33.339003: step 413, loss 0.226905, acc 0.88
2016-09-06T21:50:34.035222: step 414, loss 0.222741, acc 0.92
2016-09-06T21:50:34.717833: step 415, loss 0.247313, acc 0.9
2016-09-06T21:50:35.404192: step 416, loss 0.305846, acc 0.86
2016-09-06T21:50:36.088617: step 417, loss 0.15106, acc 0.94
2016-09-06T21:50:36.780026: step 418, loss 0.173658, acc 0.9
2016-09-06T21:50:37.457657: step 419, loss 0.271564, acc 0.92
2016-09-06T21:50:38.136399: step 420, loss 0.219187, acc 0.92
2016-09-06T21:50:38.816520: step 421, loss 0.25034, acc 0.86
2016-09-06T21:50:39.505551: step 422, loss 0.329803, acc 0.86
2016-09-06T21:50:40.173638: step 423, loss 0.105153, acc 0.94
2016-09-06T21:50:40.866538: step 424, loss 0.1568, acc 0.94
2016-09-06T21:50:41.582927: step 425, loss 0.393523, acc 0.86
2016-09-06T21:50:42.252649: step 426, loss 0.209836, acc 0.9
2016-09-06T21:50:42.952110: step 427, loss 0.191679, acc 0.94
2016-09-06T21:50:43.629363: step 428, loss 0.189961, acc 0.9
2016-09-06T21:50:44.332469: step 429, loss 0.221111, acc 0.88
2016-09-06T21:50:45.013042: step 430, loss 0.246834, acc 0.9
2016-09-06T21:50:45.698789: step 431, loss 0.23693, acc 0.88
2016-09-06T21:50:46.394044: step 432, loss 0.301984, acc 0.88
2016-09-06T21:50:47.044625: step 433, loss 0.231179, acc 0.86
2016-09-06T21:50:47.737681: step 434, loss 0.195395, acc 0.92
2016-09-06T21:50:48.418597: step 435, loss 0.221131, acc 0.88
2016-09-06T21:50:49.085363: step 436, loss 0.144615, acc 0.94
2016-09-06T21:50:49.760206: step 437, loss 0.20675, acc 0.94
2016-09-06T21:50:50.436443: step 438, loss 0.185528, acc 0.9
2016-09-06T21:50:51.109419: step 439, loss 0.206219, acc 0.94
2016-09-06T21:50:51.794344: step 440, loss 0.234127, acc 0.92
2016-09-06T21:50:52.498144: step 441, loss 0.180753, acc 0.92
2016-09-06T21:50:53.189997: step 442, loss 0.14011, acc 0.94
2016-09-06T21:50:53.879918: step 443, loss 0.307441, acc 0.86
2016-09-06T21:50:54.580502: step 444, loss 0.201731, acc 0.92
2016-09-06T21:50:55.252068: step 445, loss 0.183458, acc 0.86
2016-09-06T21:50:55.937523: step 446, loss 0.285425, acc 0.84
2016-09-06T21:50:56.586982: step 447, loss 0.257318, acc 0.88
2016-09-06T21:50:57.292515: step 448, loss 0.12529, acc 1
2016-09-06T21:50:57.982876: step 449, loss 0.218257, acc 0.9
2016-09-06T21:50:58.661777: step 450, loss 0.219996, acc 0.9
2016-09-06T21:50:59.349084: step 451, loss 0.089227, acc 0.98
2016-09-06T21:51:00.030970: step 452, loss 0.196599, acc 0.92
2016-09-06T21:51:00.773879: step 453, loss 0.235479, acc 0.9
2016-09-06T21:51:01.445246: step 454, loss 0.364313, acc 0.9
2016-09-06T21:51:02.142480: step 455, loss 0.145432, acc 0.96
2016-09-06T21:51:02.824260: step 456, loss 0.237322, acc 0.88
2016-09-06T21:51:03.519339: step 457, loss 0.229645, acc 0.92
2016-09-06T21:51:04.190575: step 458, loss 0.129095, acc 0.96
2016-09-06T21:51:04.875476: step 459, loss 0.198815, acc 0.92
2016-09-06T21:51:05.556695: step 460, loss 0.153748, acc 0.96
2016-09-06T21:51:06.230448: step 461, loss 0.200732, acc 0.9
2016-09-06T21:51:06.930259: step 462, loss 0.166777, acc 0.94
2016-09-06T21:51:07.600777: step 463, loss 0.277414, acc 0.86
2016-09-06T21:51:08.267514: step 464, loss 0.219525, acc 0.92
2016-09-06T21:51:08.956512: step 465, loss 0.256651, acc 0.9
2016-09-06T21:51:09.638564: step 466, loss 0.364341, acc 0.84
2016-09-06T21:51:10.320716: step 467, loss 0.232012, acc 0.88
2016-09-06T21:51:10.998989: step 468, loss 0.234537, acc 0.88
2016-09-06T21:51:11.700918: step 469, loss 0.178772, acc 0.92
2016-09-06T21:51:12.368016: step 470, loss 0.226201, acc 0.9
2016-09-06T21:51:13.051603: step 471, loss 0.16137, acc 0.94
2016-09-06T21:51:13.735637: step 472, loss 0.0802967, acc 0.98
2016-09-06T21:51:14.434176: step 473, loss 0.218743, acc 0.94
2016-09-06T21:51:15.099843: step 474, loss 0.166664, acc 0.9
2016-09-06T21:51:15.772073: step 475, loss 0.19742, acc 0.9
2016-09-06T21:51:16.469374: step 476, loss 0.295512, acc 0.84
2016-09-06T21:51:17.152891: step 477, loss 0.281691, acc 0.88
2016-09-06T21:51:17.848417: step 478, loss 0.105775, acc 0.98
2016-09-06T21:51:18.548341: step 479, loss 0.179174, acc 0.9
2016-09-06T21:51:19.226585: step 480, loss 0.294889, acc 0.82
2016-09-06T21:51:19.912961: step 481, loss 0.191598, acc 0.9
2016-09-06T21:51:20.597255: step 482, loss 0.280188, acc 0.84
2016-09-06T21:51:21.264307: step 483, loss 0.257009, acc 0.9
2016-09-06T21:51:21.951370: step 484, loss 0.457274, acc 0.82
2016-09-06T21:51:22.628993: step 485, loss 0.476546, acc 0.8
2016-09-06T21:51:23.322174: step 486, loss 0.299287, acc 0.9
2016-09-06T21:51:24.012600: step 487, loss 0.204835, acc 0.9
2016-09-06T21:51:24.701123: step 488, loss 0.231989, acc 0.88
2016-09-06T21:51:25.388192: step 489, loss 0.258549, acc 0.92
2016-09-06T21:51:26.101709: step 490, loss 0.231579, acc 0.88
2016-09-06T21:51:26.763529: step 491, loss 0.258638, acc 0.88
2016-09-06T21:51:27.440162: step 492, loss 0.246667, acc 0.92
2016-09-06T21:51:28.114795: step 493, loss 0.267012, acc 0.84
2016-09-06T21:51:28.785154: step 494, loss 0.266881, acc 0.9
2016-09-06T21:51:29.473285: step 495, loss 0.27196, acc 0.88
2016-09-06T21:51:30.146152: step 496, loss 0.270446, acc 0.88
2016-09-06T21:51:30.837288: step 497, loss 0.160037, acc 0.92
2016-09-06T21:51:31.507898: step 498, loss 0.216, acc 0.92
2016-09-06T21:51:32.215326: step 499, loss 0.290003, acc 0.92
2016-09-06T21:51:32.892917: step 500, loss 0.446435, acc 0.82

Evaluation:
2016-09-06T21:51:36.010974: step 500, loss 0.514824, acc 0.771107

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-500

2016-09-06T21:51:37.746824: step 501, loss 0.17837, acc 0.9
2016-09-06T21:51:38.426794: step 502, loss 0.24589, acc 0.92
2016-09-06T21:51:39.125594: step 503, loss 0.328639, acc 0.86
2016-09-06T21:51:39.806816: step 504, loss 0.215753, acc 0.92
2016-09-06T21:51:40.527979: step 505, loss 0.196616, acc 0.92
2016-09-06T21:51:41.198416: step 506, loss 0.316521, acc 0.84
2016-09-06T21:51:41.878444: step 507, loss 0.145509, acc 0.96
2016-09-06T21:51:42.565464: step 508, loss 0.26545, acc 0.88
2016-09-06T21:51:43.256840: step 509, loss 0.385935, acc 0.9
2016-09-06T21:51:43.941726: step 510, loss 0.243024, acc 0.88
2016-09-06T21:51:44.635356: step 511, loss 0.267008, acc 0.88
2016-09-06T21:51:45.367415: step 512, loss 0.215115, acc 0.86
2016-09-06T21:51:46.054397: step 513, loss 0.346933, acc 0.88
2016-09-06T21:51:46.726981: step 514, loss 0.315122, acc 0.88
2016-09-06T21:51:47.418958: step 515, loss 0.222976, acc 0.9
2016-09-06T21:51:48.130630: step 516, loss 0.347427, acc 0.84
2016-09-06T21:51:48.846065: step 517, loss 0.24097, acc 0.84
2016-09-06T21:51:49.517751: step 518, loss 0.309743, acc 0.82
2016-09-06T21:51:50.212485: step 519, loss 0.191294, acc 0.94
2016-09-06T21:51:50.861753: step 520, loss 0.316774, acc 0.82
2016-09-06T21:51:51.533647: step 521, loss 0.291925, acc 0.88
2016-09-06T21:51:52.221142: step 522, loss 0.298427, acc 0.88
2016-09-06T21:51:52.897436: step 523, loss 0.246114, acc 0.9
2016-09-06T21:51:53.587536: step 524, loss 0.224076, acc 0.92
2016-09-06T21:51:54.261346: step 525, loss 0.326952, acc 0.82
2016-09-06T21:51:54.952594: step 526, loss 0.266328, acc 0.88
2016-09-06T21:51:55.630504: step 527, loss 0.204116, acc 0.94
2016-09-06T21:51:56.316416: step 528, loss 0.247774, acc 0.92
2016-09-06T21:51:56.993071: step 529, loss 0.183731, acc 0.9
2016-09-06T21:51:57.686038: step 530, loss 0.125639, acc 0.94
2016-09-06T21:51:58.376944: step 531, loss 0.144512, acc 0.96
2016-09-06T21:51:59.072468: step 532, loss 0.151623, acc 0.98
2016-09-06T21:51:59.761502: step 533, loss 0.215367, acc 0.88
2016-09-06T21:52:00.457870: step 534, loss 0.359943, acc 0.88
2016-09-06T21:52:01.139181: step 535, loss 0.493781, acc 0.86
2016-09-06T21:52:01.824193: step 536, loss 0.716667, acc 0.74
2016-09-06T21:52:02.529615: step 537, loss 0.197698, acc 0.92
2016-09-06T21:52:03.217465: step 538, loss 0.195683, acc 0.88
2016-09-06T21:52:03.896707: step 539, loss 0.33047, acc 0.84
2016-09-06T21:52:04.601042: step 540, loss 0.187575, acc 0.96
2016-09-06T21:52:05.269565: step 541, loss 0.356512, acc 0.8
2016-09-06T21:52:05.953750: step 542, loss 0.249517, acc 0.88
2016-09-06T21:52:06.631215: step 543, loss 0.309262, acc 0.84
2016-09-06T21:52:07.306941: step 544, loss 0.238577, acc 0.88
2016-09-06T21:52:07.998396: step 545, loss 0.134673, acc 0.94
2016-09-06T21:52:08.695051: step 546, loss 0.215581, acc 0.94
2016-09-06T21:52:09.421238: step 547, loss 0.262705, acc 0.88
2016-09-06T21:52:10.101867: step 548, loss 0.398402, acc 0.84
2016-09-06T21:52:10.772599: step 549, loss 0.326798, acc 0.9
2016-09-06T21:52:11.445828: step 550, loss 0.313857, acc 0.86
2016-09-06T21:52:12.112538: step 551, loss 0.131207, acc 0.94
2016-09-06T21:52:12.792344: step 552, loss 0.402248, acc 0.8
2016-09-06T21:52:13.483507: step 553, loss 0.358101, acc 0.86
2016-09-06T21:52:14.180900: step 554, loss 0.275939, acc 0.86
2016-09-06T21:52:14.863732: step 555, loss 0.245263, acc 0.9
2016-09-06T21:52:15.543140: step 556, loss 0.16956, acc 0.92
2016-09-06T21:52:16.212597: step 557, loss 0.19677, acc 0.92
2016-09-06T21:52:16.871064: step 558, loss 0.240218, acc 0.9
2016-09-06T21:52:17.570749: step 559, loss 0.365951, acc 0.84
2016-09-06T21:52:18.243027: step 560, loss 0.238433, acc 0.88
2016-09-06T21:52:18.938358: step 561, loss 0.312312, acc 0.88
2016-09-06T21:52:19.614692: step 562, loss 0.357316, acc 0.9
2016-09-06T21:52:20.311297: step 563, loss 0.370071, acc 0.86
2016-09-06T21:52:21.012291: step 564, loss 0.280902, acc 0.86
2016-09-06T21:52:21.703306: step 565, loss 0.328073, acc 0.86
2016-09-06T21:52:22.396651: step 566, loss 0.477179, acc 0.76
2016-09-06T21:52:23.094992: step 567, loss 0.216063, acc 0.9
2016-09-06T21:52:23.780617: step 568, loss 0.23718, acc 0.96
2016-09-06T21:52:24.468685: step 569, loss 0.263235, acc 0.84
2016-09-06T21:52:25.134953: step 570, loss 0.342567, acc 0.78
2016-09-06T21:52:25.827004: step 571, loss 0.279326, acc 0.9
2016-09-06T21:52:26.518002: step 572, loss 0.376498, acc 0.88
2016-09-06T21:52:27.187029: step 573, loss 0.166507, acc 0.92
2016-09-06T21:52:27.875530: step 574, loss 0.216271, acc 0.9
2016-09-06T21:52:28.581595: step 575, loss 0.359211, acc 0.84
2016-09-06T21:52:29.213912: step 576, loss 0.138562, acc 0.977273
2016-09-06T21:52:29.926334: step 577, loss 0.18372, acc 0.94
2016-09-06T21:52:30.615538: step 578, loss 0.176929, acc 0.94
2016-09-06T21:52:31.333328: step 579, loss 0.223254, acc 0.92
2016-09-06T21:52:32.027696: step 580, loss 0.249886, acc 0.86
2016-09-06T21:52:32.703122: step 581, loss 0.167911, acc 0.92
2016-09-06T21:52:33.383928: step 582, loss 0.094969, acc 0.98
2016-09-06T21:52:34.066066: step 583, loss 0.168969, acc 0.92
2016-09-06T21:52:34.761653: step 584, loss 0.194327, acc 0.92
2016-09-06T21:52:35.443607: step 585, loss 0.198923, acc 0.94
2016-09-06T21:52:36.104710: step 586, loss 0.208789, acc 0.92
2016-09-06T21:52:36.786055: step 587, loss 0.0877134, acc 0.98
2016-09-06T21:52:37.467521: step 588, loss 0.204589, acc 0.9
2016-09-06T21:52:38.157340: step 589, loss 0.0888092, acc 0.98
2016-09-06T21:52:38.816127: step 590, loss 0.168065, acc 0.9
2016-09-06T21:52:39.511640: step 591, loss 0.0985551, acc 0.94
2016-09-06T21:52:40.186028: step 592, loss 0.206151, acc 0.9
2016-09-06T21:52:40.877605: step 593, loss 0.078045, acc 0.98
2016-09-06T21:52:41.552623: step 594, loss 0.284004, acc 0.92
2016-09-06T21:52:42.244150: step 595, loss 0.121601, acc 0.94
2016-09-06T21:52:42.927257: step 596, loss 0.0833424, acc 0.98
2016-09-06T21:52:43.580984: step 597, loss 0.0918929, acc 0.98
2016-09-06T21:52:44.281516: step 598, loss 0.149896, acc 0.94
2016-09-06T21:52:44.935736: step 599, loss 0.237431, acc 0.9
2016-09-06T21:52:45.636394: step 600, loss 0.0661172, acc 0.98

Evaluation:
2016-09-06T21:52:48.798270: step 600, loss 0.656817, acc 0.774859

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-600

2016-09-06T21:52:50.449100: step 601, loss 0.132528, acc 0.92
2016-09-06T21:52:51.172093: step 602, loss 0.1893, acc 0.92
2016-09-06T21:52:51.862384: step 603, loss 0.145272, acc 0.94
2016-09-06T21:52:52.548885: step 604, loss 0.204499, acc 0.94
2016-09-06T21:52:53.224060: step 605, loss 0.252735, acc 0.84
2016-09-06T21:52:53.915400: step 606, loss 0.215719, acc 0.92
2016-09-06T21:52:54.587033: step 607, loss 0.256207, acc 0.88
2016-09-06T21:52:55.263576: step 608, loss 0.369424, acc 0.84
2016-09-06T21:52:55.946677: step 609, loss 0.170454, acc 0.94
2016-09-06T21:52:56.630104: step 610, loss 0.127691, acc 0.94
2016-09-06T21:52:57.298075: step 611, loss 0.118424, acc 0.94
2016-09-06T21:52:57.982329: step 612, loss 0.371518, acc 0.96
2016-09-06T21:52:58.672787: step 613, loss 0.191086, acc 0.94
2016-09-06T21:52:59.335972: step 614, loss 0.156263, acc 0.9
2016-09-06T21:53:00.019495: step 615, loss 0.180592, acc 0.96
2016-09-06T21:53:00.737610: step 616, loss 0.215017, acc 0.88
2016-09-06T21:53:01.418824: step 617, loss 0.100218, acc 0.98
2016-09-06T21:53:02.112230: step 618, loss 0.112097, acc 1
2016-09-06T21:53:02.780572: step 619, loss 0.0864002, acc 0.98
2016-09-06T21:53:03.485756: step 620, loss 0.103271, acc 0.96
2016-09-06T21:53:04.174334: step 621, loss 0.162634, acc 0.94
2016-09-06T21:53:04.852234: step 622, loss 0.118532, acc 0.98
2016-09-06T21:53:05.536069: step 623, loss 0.16072, acc 0.94
2016-09-06T21:53:06.223287: step 624, loss 0.140982, acc 0.94
2016-09-06T21:53:06.906977: step 625, loss 0.087419, acc 0.96
2016-09-06T21:53:07.588748: step 626, loss 0.162154, acc 0.94
2016-09-06T21:53:08.299241: step 627, loss 0.139907, acc 0.92
2016-09-06T21:53:08.973204: step 628, loss 0.0846118, acc 0.96
2016-09-06T21:53:09.642794: step 629, loss 0.149396, acc 0.9
2016-09-06T21:53:10.326763: step 630, loss 0.0889747, acc 0.96
2016-09-06T21:53:11.030731: step 631, loss 0.0562463, acc 1
2016-09-06T21:53:11.723821: step 632, loss 0.210751, acc 0.9
2016-09-06T21:53:12.404506: step 633, loss 0.255187, acc 0.94
2016-09-06T21:53:13.102293: step 634, loss 0.0954864, acc 0.96
2016-09-06T21:53:13.765807: step 635, loss 0.171999, acc 0.88
2016-09-06T21:53:14.464293: step 636, loss 0.252448, acc 0.9
2016-09-06T21:53:15.150318: step 637, loss 0.291165, acc 0.9
2016-09-06T21:53:15.836035: step 638, loss 0.175353, acc 0.9
2016-09-06T21:53:16.511347: step 639, loss 0.20416, acc 0.9
2016-09-06T21:53:17.193926: step 640, loss 0.142467, acc 0.92
2016-09-06T21:53:17.903881: step 641, loss 0.0781924, acc 0.96
2016-09-06T21:53:18.598852: step 642, loss 0.277176, acc 0.92
2016-09-06T21:53:19.285375: step 643, loss 0.172371, acc 0.94
2016-09-06T21:53:19.967571: step 644, loss 0.343328, acc 0.92
2016-09-06T21:53:20.666431: step 645, loss 0.21679, acc 0.92
2016-09-06T21:53:21.339728: step 646, loss 0.0927246, acc 0.96
2016-09-06T21:53:22.014955: step 647, loss 0.100418, acc 0.94
2016-09-06T21:53:22.731492: step 648, loss 0.123544, acc 0.98
2016-09-06T21:53:23.414692: step 649, loss 0.237116, acc 0.88
2016-09-06T21:53:24.108061: step 650, loss 0.165243, acc 0.94
2016-09-06T21:53:24.789217: step 651, loss 0.108889, acc 0.94
2016-09-06T21:53:25.468358: step 652, loss 0.166803, acc 0.96
2016-09-06T21:53:26.174723: step 653, loss 0.318306, acc 0.92
2016-09-06T21:53:26.835563: step 654, loss 0.0469478, acc 0.98
2016-09-06T21:53:27.544119: step 655, loss 0.0673179, acc 0.96
2016-09-06T21:53:28.217949: step 656, loss 0.0922583, acc 0.98
2016-09-06T21:53:28.918879: step 657, loss 0.0927903, acc 0.96
2016-09-06T21:53:29.607754: step 658, loss 0.249878, acc 0.9
2016-09-06T21:53:30.294143: step 659, loss 0.108371, acc 0.96
2016-09-06T21:53:30.969506: step 660, loss 0.251065, acc 0.9
2016-09-06T21:53:31.660315: step 661, loss 0.269313, acc 0.9
2016-09-06T21:53:32.373776: step 662, loss 0.137585, acc 0.94
2016-09-06T21:53:33.051881: step 663, loss 0.253703, acc 0.88
2016-09-06T21:53:33.737309: step 664, loss 0.14349, acc 0.96
2016-09-06T21:53:34.440886: step 665, loss 0.254459, acc 0.88
2016-09-06T21:53:35.133431: step 666, loss 0.099723, acc 0.94
2016-09-06T21:53:35.831490: step 667, loss 0.120305, acc 0.94
2016-09-06T21:53:36.511084: step 668, loss 0.0906504, acc 0.98
2016-09-06T21:53:37.202247: step 669, loss 0.114362, acc 0.92
2016-09-06T21:53:37.881896: step 670, loss 0.264366, acc 0.9
2016-09-06T21:53:38.578608: step 671, loss 0.198371, acc 0.94
2016-09-06T21:53:39.271894: step 672, loss 0.321411, acc 0.88
2016-09-06T21:53:39.953879: step 673, loss 0.260011, acc 0.86
2016-09-06T21:53:40.653826: step 674, loss 0.103923, acc 0.96
2016-09-06T21:53:41.318488: step 675, loss 0.163284, acc 0.96
2016-09-06T21:53:42.011522: step 676, loss 0.167468, acc 0.94
2016-09-06T21:53:42.681744: step 677, loss 0.204728, acc 0.92
2016-09-06T21:53:43.356826: step 678, loss 0.132688, acc 0.94
2016-09-06T21:53:44.059955: step 679, loss 0.124834, acc 0.96
2016-09-06T21:53:44.739331: step 680, loss 0.160923, acc 0.92
2016-09-06T21:53:45.425909: step 681, loss 0.475403, acc 0.84
2016-09-06T21:53:46.097755: step 682, loss 0.450144, acc 0.8
2016-09-06T21:53:46.783459: step 683, loss 0.4618, acc 0.84
2016-09-06T21:53:47.471942: step 684, loss 0.234879, acc 0.92
2016-09-06T21:53:48.171372: step 685, loss 0.183004, acc 0.94
2016-09-06T21:53:48.849469: step 686, loss 0.16437, acc 0.92
2016-09-06T21:53:49.535779: step 687, loss 0.124882, acc 0.98
2016-09-06T21:53:50.210702: step 688, loss 0.154884, acc 0.96
2016-09-06T21:53:50.886103: step 689, loss 0.137676, acc 0.98
2016-09-06T21:53:51.596619: step 690, loss 0.303091, acc 0.84
2016-09-06T21:53:52.285951: step 691, loss 0.312502, acc 0.86
2016-09-06T21:53:52.962909: step 692, loss 0.234824, acc 0.92
2016-09-06T21:53:53.668049: step 693, loss 0.17215, acc 0.9
2016-09-06T21:53:54.345388: step 694, loss 0.182484, acc 0.94
2016-09-06T21:53:55.030718: step 695, loss 0.129308, acc 0.96
2016-09-06T21:53:55.710524: step 696, loss 0.136361, acc 0.94
2016-09-06T21:53:56.423715: step 697, loss 0.226958, acc 0.92
2016-09-06T21:53:57.106745: step 698, loss 0.0776927, acc 1
2016-09-06T21:53:57.781934: step 699, loss 0.217566, acc 0.94
2016-09-06T21:53:58.456348: step 700, loss 0.201202, acc 0.9

Evaluation:
2016-09-06T21:54:01.642323: step 700, loss 0.580792, acc 0.769231

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-700

2016-09-06T21:54:03.294018: step 701, loss 0.226807, acc 0.88
2016-09-06T21:54:03.982653: step 702, loss 0.296775, acc 0.84
2016-09-06T21:54:04.662241: step 703, loss 0.360659, acc 0.84
2016-09-06T21:54:05.344034: step 704, loss 0.0795147, acc 0.94
2016-09-06T21:54:06.056775: step 705, loss 0.0909547, acc 0.98
2016-09-06T21:54:06.724722: step 706, loss 0.144226, acc 0.92
2016-09-06T21:54:07.403012: step 707, loss 0.475915, acc 0.76
2016-09-06T21:54:08.107599: step 708, loss 0.288961, acc 0.88
2016-09-06T21:54:08.792363: step 709, loss 0.18297, acc 0.94
2016-09-06T21:54:09.487678: step 710, loss 0.123705, acc 0.94
2016-09-06T21:54:10.177564: step 711, loss 0.119239, acc 0.96
2016-09-06T21:54:10.873894: step 712, loss 0.216266, acc 0.9
2016-09-06T21:54:11.551252: step 713, loss 0.0769883, acc 0.98
2016-09-06T21:54:12.242197: step 714, loss 0.0844232, acc 0.98
2016-09-06T21:54:12.946094: step 715, loss 0.179378, acc 0.92
2016-09-06T21:54:13.625348: step 716, loss 0.109722, acc 0.96
2016-09-06T21:54:14.300726: step 717, loss 0.274219, acc 0.94
2016-09-06T21:54:14.978790: step 718, loss 0.150919, acc 0.92
2016-09-06T21:54:15.671626: step 719, loss 0.0663973, acc 0.98
2016-09-06T21:54:16.343798: step 720, loss 0.180635, acc 0.92
2016-09-06T21:54:17.017847: step 721, loss 0.131151, acc 0.94
2016-09-06T21:54:17.698756: step 722, loss 0.120968, acc 0.92
2016-09-06T21:54:18.377094: step 723, loss 0.127291, acc 0.94
2016-09-06T21:54:19.047346: step 724, loss 0.176833, acc 0.9
2016-09-06T21:54:19.745578: step 725, loss 0.206299, acc 0.92
2016-09-06T21:54:20.453067: step 726, loss 0.355699, acc 0.86
2016-09-06T21:54:21.128062: step 727, loss 0.210887, acc 0.94
2016-09-06T21:54:21.843079: step 728, loss 0.425619, acc 0.86
2016-09-06T21:54:22.521771: step 729, loss 0.121492, acc 0.96
2016-09-06T21:54:23.188332: step 730, loss 0.243214, acc 0.94
2016-09-06T21:54:23.884287: step 731, loss 0.142193, acc 0.94
2016-09-06T21:54:24.583327: step 732, loss 0.181883, acc 0.92
2016-09-06T21:54:25.315425: step 733, loss 0.0791012, acc 1
2016-09-06T21:54:26.008763: step 734, loss 0.167335, acc 0.92
2016-09-06T21:54:26.684930: step 735, loss 0.193071, acc 0.88
2016-09-06T21:54:27.372139: step 736, loss 0.166841, acc 0.9
2016-09-06T21:54:28.080676: step 737, loss 0.246593, acc 0.9
2016-09-06T21:54:28.790062: step 738, loss 0.160775, acc 0.88
2016-09-06T21:54:29.481343: step 739, loss 0.24036, acc 0.92
2016-09-06T21:54:30.213224: step 740, loss 0.148784, acc 0.94
2016-09-06T21:54:30.892713: step 741, loss 0.141976, acc 0.94
2016-09-06T21:54:31.582706: step 742, loss 0.194899, acc 0.92
2016-09-06T21:54:32.278962: step 743, loss 0.182137, acc 0.92
2016-09-06T21:54:32.974937: step 744, loss 0.240824, acc 0.92
2016-09-06T21:54:33.663244: step 745, loss 0.123945, acc 0.94
2016-09-06T21:54:34.343817: step 746, loss 0.211011, acc 0.94
2016-09-06T21:54:35.057426: step 747, loss 0.251513, acc 0.88
2016-09-06T21:54:35.747791: step 748, loss 0.156361, acc 0.94
2016-09-06T21:54:36.442184: step 749, loss 0.350637, acc 0.9
2016-09-06T21:54:37.126820: step 750, loss 0.0713376, acc 0.98
2016-09-06T21:54:37.809019: step 751, loss 0.309381, acc 0.94
2016-09-06T21:54:38.502360: step 752, loss 0.15984, acc 0.94
2016-09-06T21:54:39.175795: step 753, loss 0.405205, acc 0.82
2016-09-06T21:54:39.874695: step 754, loss 0.312687, acc 0.92
2016-09-06T21:54:40.577768: step 755, loss 0.172732, acc 0.94
2016-09-06T21:54:41.262745: step 756, loss 0.1946, acc 0.9
2016-09-06T21:54:41.933943: step 757, loss 0.137719, acc 0.94
2016-09-06T21:54:42.600201: step 758, loss 0.235163, acc 0.86
2016-09-06T21:54:43.324930: step 759, loss 0.112777, acc 0.96
2016-09-06T21:54:43.997425: step 760, loss 0.173781, acc 0.9
2016-09-06T21:54:44.677349: step 761, loss 0.240758, acc 0.9
2016-09-06T21:54:45.383008: step 762, loss 0.202569, acc 0.94
2016-09-06T21:54:46.064179: step 763, loss 0.210818, acc 0.9
2016-09-06T21:54:46.727649: step 764, loss 0.141227, acc 0.92
2016-09-06T21:54:47.410720: step 765, loss 0.149095, acc 0.96
2016-09-06T21:54:48.135583: step 766, loss 0.182938, acc 0.9
2016-09-06T21:54:48.813767: step 767, loss 0.19836, acc 0.94
2016-09-06T21:54:49.464350: step 768, loss 0.132726, acc 0.954545
2016-09-06T21:54:50.159405: step 769, loss 0.113062, acc 0.96
2016-09-06T21:54:50.834999: step 770, loss 0.157747, acc 0.92
2016-09-06T21:54:51.553338: step 771, loss 0.195059, acc 0.9
2016-09-06T21:54:52.264730: step 772, loss 0.136046, acc 0.92
2016-09-06T21:54:52.979152: step 773, loss 0.0744453, acc 0.96
2016-09-06T21:54:53.638178: step 774, loss 0.113237, acc 0.98
2016-09-06T21:54:54.350054: step 775, loss 0.134019, acc 0.9
2016-09-06T21:54:55.031239: step 776, loss 0.0947605, acc 0.94
2016-09-06T21:54:55.705351: step 777, loss 0.0450328, acc 1
2016-09-06T21:54:56.387411: step 778, loss 0.0231101, acc 1
2016-09-06T21:54:57.055154: step 779, loss 0.0955153, acc 0.96
2016-09-06T21:54:57.777361: step 780, loss 0.0967189, acc 0.92
2016-09-06T21:54:58.468115: step 781, loss 0.0649977, acc 0.96
2016-09-06T21:54:59.156097: step 782, loss 0.126593, acc 0.92
2016-09-06T21:54:59.845980: step 783, loss 0.0969893, acc 0.98
2016-09-06T21:55:00.569938: step 784, loss 0.0402212, acc 0.96
2016-09-06T21:55:01.266682: step 785, loss 0.0258287, acc 1
2016-09-06T21:55:01.919694: step 786, loss 0.110856, acc 0.94
2016-09-06T21:55:02.640720: step 787, loss 0.180383, acc 0.88
2016-09-06T21:55:03.335737: step 788, loss 0.0611367, acc 0.96
2016-09-06T21:55:04.017745: step 789, loss 0.147345, acc 0.94
2016-09-06T21:55:04.686273: step 790, loss 0.199933, acc 0.94
2016-09-06T21:55:05.378961: step 791, loss 0.0853305, acc 0.96
2016-09-06T21:55:06.068377: step 792, loss 0.0456111, acc 1
2016-09-06T21:55:06.730081: step 793, loss 0.0344596, acc 1
2016-09-06T21:55:07.440240: step 794, loss 0.0878646, acc 0.96
2016-09-06T21:55:08.132514: step 795, loss 0.0847344, acc 0.98
2016-09-06T21:55:08.821766: step 796, loss 0.0424186, acc 1
2016-09-06T21:55:09.516089: step 797, loss 0.281538, acc 0.86
2016-09-06T21:55:10.198422: step 798, loss 0.0736802, acc 0.96
2016-09-06T21:55:10.880420: step 799, loss 0.179806, acc 0.92
2016-09-06T21:55:11.542129: step 800, loss 0.203088, acc 0.88

Evaluation:
2016-09-06T21:55:14.707226: step 800, loss 0.770127, acc 0.771107

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-800

2016-09-06T21:55:16.373991: step 801, loss 0.0720726, acc 0.96
2016-09-06T21:55:17.087046: step 802, loss 0.0680026, acc 0.96
2016-09-06T21:55:17.766645: step 803, loss 0.194652, acc 0.96
2016-09-06T21:55:18.439910: step 804, loss 0.20684, acc 0.9
2016-09-06T21:55:19.122283: step 805, loss 0.0327476, acc 1
2016-09-06T21:55:19.813929: step 806, loss 0.0697918, acc 0.96
2016-09-06T21:55:20.481130: step 807, loss 0.0524905, acc 0.98
2016-09-06T21:55:21.150759: step 808, loss 0.0622608, acc 0.96
2016-09-06T21:55:21.856454: step 809, loss 0.0718205, acc 0.98
2016-09-06T21:55:22.519950: step 810, loss 0.109874, acc 0.94
2016-09-06T21:55:23.198214: step 811, loss 0.101427, acc 0.94
2016-09-06T21:55:23.877753: step 812, loss 0.0492656, acc 1
2016-09-06T21:55:24.573501: step 813, loss 0.0930073, acc 0.96
2016-09-06T21:55:25.258852: step 814, loss 0.0610192, acc 0.98
2016-09-06T21:55:25.916766: step 815, loss 0.18611, acc 0.94
2016-09-06T21:55:26.609724: step 816, loss 0.12917, acc 0.92
2016-09-06T21:55:27.302156: step 817, loss 0.144004, acc 0.94
2016-09-06T21:55:28.024846: step 818, loss 0.102218, acc 0.98
2016-09-06T21:55:28.709000: step 819, loss 0.111309, acc 0.98
2016-09-06T21:55:29.387459: step 820, loss 0.0957114, acc 0.96
2016-09-06T21:55:30.088524: step 821, loss 0.155223, acc 0.98
2016-09-06T21:55:30.743211: step 822, loss 0.0461881, acc 0.98
2016-09-06T21:55:31.441566: step 823, loss 0.148379, acc 0.96
2016-09-06T21:55:32.122199: step 824, loss 0.151608, acc 0.94
2016-09-06T21:55:32.819900: step 825, loss 0.0687064, acc 1
2016-09-06T21:55:33.513153: step 826, loss 0.0757365, acc 0.96
2016-09-06T21:55:34.229498: step 827, loss 0.0449551, acc 1
2016-09-06T21:55:34.909410: step 828, loss 0.175512, acc 0.94
2016-09-06T21:55:35.577793: step 829, loss 0.204914, acc 0.9
2016-09-06T21:55:36.259583: step 830, loss 0.102996, acc 0.94
2016-09-06T21:55:36.938168: step 831, loss 0.0794612, acc 0.96
2016-09-06T21:55:37.664091: step 832, loss 0.10675, acc 0.98
2016-09-06T21:55:38.393599: step 833, loss 0.140073, acc 0.92
2016-09-06T21:55:39.092814: step 834, loss 0.121293, acc 0.96
2016-09-06T21:55:39.807794: step 835, loss 0.128622, acc 0.96
2016-09-06T21:55:40.458549: step 836, loss 0.137071, acc 0.94
2016-09-06T21:55:41.157378: step 837, loss 0.381088, acc 0.88
2016-09-06T21:55:41.850713: step 838, loss 0.189794, acc 0.94
2016-09-06T21:55:42.512166: step 839, loss 0.119256, acc 0.94
2016-09-06T21:55:43.191042: step 840, loss 0.0653716, acc 0.96
2016-09-06T21:55:43.867712: step 841, loss 0.281367, acc 0.88
2016-09-06T21:55:44.569240: step 842, loss 0.114229, acc 0.96
2016-09-06T21:55:45.220593: step 843, loss 0.0910397, acc 0.96
2016-09-06T21:55:45.938253: step 844, loss 0.0412052, acc 0.98
2016-09-06T21:55:46.624963: step 845, loss 0.121093, acc 0.94
2016-09-06T21:55:47.313571: step 846, loss 0.109473, acc 0.96
2016-09-06T21:55:47.998904: step 847, loss 0.12961, acc 0.92
2016-09-06T21:55:48.683592: step 848, loss 0.101038, acc 0.98
2016-09-06T21:55:49.412274: step 849, loss 0.133547, acc 0.94
2016-09-06T21:55:50.085317: step 850, loss 0.155319, acc 0.9
2016-09-06T21:55:50.774289: step 851, loss 0.271155, acc 0.88
2016-09-06T21:55:51.453704: step 852, loss 0.145224, acc 0.96
2016-09-06T21:55:52.145986: step 853, loss 0.16932, acc 0.98
2016-09-06T21:55:52.827608: step 854, loss 0.0708152, acc 0.98
2016-09-06T21:55:53.526546: step 855, loss 0.227451, acc 0.88
2016-09-06T21:55:54.216065: step 856, loss 0.103321, acc 0.96
2016-09-06T21:55:54.875432: step 857, loss 0.0778052, acc 0.96
2016-09-06T21:55:55.570364: step 858, loss 0.0739688, acc 0.98
2016-09-06T21:55:56.242427: step 859, loss 0.0554236, acc 0.96
2016-09-06T21:55:56.936645: step 860, loss 0.117527, acc 0.94
2016-09-06T21:55:57.622169: step 861, loss 0.061453, acc 0.96
2016-09-06T21:55:58.313523: step 862, loss 0.132565, acc 0.9
2016-09-06T21:55:59.038789: step 863, loss 0.155737, acc 0.92
2016-09-06T21:55:59.707186: step 864, loss 0.0725592, acc 0.96
2016-09-06T21:56:00.438439: step 865, loss 0.163714, acc 0.94
2016-09-06T21:56:01.133926: step 866, loss 0.154483, acc 0.92
2016-09-06T21:56:01.816930: step 867, loss 0.129532, acc 0.92
2016-09-06T21:56:02.491829: step 868, loss 0.0743544, acc 0.98
2016-09-06T21:56:03.158529: step 869, loss 0.262038, acc 0.88
2016-09-06T21:56:03.848475: step 870, loss 0.0827439, acc 0.98
2016-09-06T21:56:04.501345: step 871, loss 0.0974201, acc 0.96
2016-09-06T21:56:05.179742: step 872, loss 0.0992952, acc 0.96
2016-09-06T21:56:05.838649: step 873, loss 0.136985, acc 0.94
2016-09-06T21:56:06.512993: step 874, loss 0.148946, acc 0.94
2016-09-06T21:56:07.192168: step 875, loss 0.0651037, acc 1
2016-09-06T21:56:07.867954: step 876, loss 0.1486, acc 0.96
2016-09-06T21:56:08.557130: step 877, loss 0.106118, acc 0.94
2016-09-06T21:56:09.225932: step 878, loss 0.0654797, acc 0.94
2016-09-06T21:56:09.923800: step 879, loss 0.0876346, acc 0.98
2016-09-06T21:56:10.605014: step 880, loss 0.178425, acc 0.92
2016-09-06T21:56:11.276089: step 881, loss 0.046763, acc 1
2016-09-06T21:56:11.954364: step 882, loss 0.117988, acc 0.96
2016-09-06T21:56:12.633564: step 883, loss 0.0579001, acc 0.98
2016-09-06T21:56:13.301545: step 884, loss 0.0708433, acc 0.96
2016-09-06T21:56:13.992821: step 885, loss 0.294477, acc 0.9
2016-09-06T21:56:14.681265: step 886, loss 0.497145, acc 0.8
2016-09-06T21:56:15.365931: step 887, loss 0.367472, acc 0.86
2016-09-06T21:56:16.049795: step 888, loss 0.111732, acc 0.94
2016-09-06T21:56:16.741747: step 889, loss 0.1581, acc 0.94
2016-09-06T21:56:17.418739: step 890, loss 0.109888, acc 0.94
2016-09-06T21:56:18.116886: step 891, loss 0.0754693, acc 0.96
2016-09-06T21:56:18.802388: step 892, loss 0.120563, acc 0.96
2016-09-06T21:56:19.503992: step 893, loss 0.131349, acc 0.96
2016-09-06T21:56:20.152899: step 894, loss 0.151324, acc 0.92
2016-09-06T21:56:20.874659: step 895, loss 0.130265, acc 0.94
2016-09-06T21:56:21.551362: step 896, loss 0.178026, acc 0.9
2016-09-06T21:56:22.257412: step 897, loss 0.120285, acc 0.98
2016-09-06T21:56:22.959536: step 898, loss 0.184796, acc 0.88
2016-09-06T21:56:23.645407: step 899, loss 0.203989, acc 0.88
2016-09-06T21:56:24.355735: step 900, loss 0.211735, acc 0.9

Evaluation:
2016-09-06T21:56:27.525531: step 900, loss 0.660113, acc 0.780488

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-900

2016-09-06T21:56:29.247672: step 901, loss 0.262823, acc 0.86
2016-09-06T21:56:29.935065: step 902, loss 0.141406, acc 0.94
2016-09-06T21:56:30.661591: step 903, loss 0.096015, acc 0.96
2016-09-06T21:56:31.344143: step 904, loss 0.279675, acc 0.88
2016-09-06T21:56:32.019571: step 905, loss 0.131912, acc 0.92
2016-09-06T21:56:32.719934: step 906, loss 0.117625, acc 0.92
2016-09-06T21:56:33.377519: step 907, loss 0.182411, acc 0.94
2016-09-06T21:56:34.095165: step 908, loss 0.108809, acc 0.92
2016-09-06T21:56:34.774967: step 909, loss 0.144267, acc 0.9
2016-09-06T21:56:35.469505: step 910, loss 0.145609, acc 0.98
2016-09-06T21:56:36.163404: step 911, loss 0.159188, acc 0.98
2016-09-06T21:56:36.842782: step 912, loss 0.0936637, acc 0.96
2016-09-06T21:56:37.538963: step 913, loss 0.0794019, acc 0.96
2016-09-06T21:56:38.195799: step 914, loss 0.0831314, acc 0.96
2016-09-06T21:56:38.879278: step 915, loss 0.235749, acc 0.94
2016-09-06T21:56:39.577535: step 916, loss 0.26396, acc 0.86
2016-09-06T21:56:40.258013: step 917, loss 0.143321, acc 0.94
2016-09-06T21:56:40.937823: step 918, loss 0.104634, acc 0.96
2016-09-06T21:56:41.614400: step 919, loss 0.140887, acc 0.96
2016-09-06T21:56:42.331504: step 920, loss 0.156674, acc 0.94
2016-09-06T21:56:42.995883: step 921, loss 0.116043, acc 0.96
2016-09-06T21:56:43.672840: step 922, loss 0.304885, acc 0.86
2016-09-06T21:56:44.348729: step 923, loss 0.21488, acc 0.86
2016-09-06T21:56:45.014221: step 924, loss 0.12513, acc 0.92
2016-09-06T21:56:45.694082: step 925, loss 0.24816, acc 0.86
2016-09-06T21:56:46.386286: step 926, loss 0.136114, acc 0.88
2016-09-06T21:56:47.086761: step 927, loss 0.0922169, acc 0.98
2016-09-06T21:56:47.747668: step 928, loss 0.143458, acc 0.94
2016-09-06T21:56:48.442884: step 929, loss 0.230835, acc 0.94
2016-09-06T21:56:49.129949: step 930, loss 0.127595, acc 0.94
2016-09-06T21:56:49.806659: step 931, loss 0.0475986, acc 0.98
2016-09-06T21:56:50.503003: step 932, loss 0.136464, acc 0.96
2016-09-06T21:56:51.202072: step 933, loss 0.2837, acc 0.86
2016-09-06T21:56:51.878788: step 934, loss 0.20255, acc 0.9
2016-09-06T21:56:52.557985: step 935, loss 0.149858, acc 0.9
2016-09-06T21:56:53.257610: step 936, loss 0.127759, acc 0.92
2016-09-06T21:56:53.944138: step 937, loss 0.118018, acc 0.92
2016-09-06T21:56:54.623035: step 938, loss 0.183475, acc 0.88
2016-09-06T21:56:55.320520: step 939, loss 0.172636, acc 0.92
2016-09-06T21:56:56.016355: step 940, loss 0.0747042, acc 0.98
2016-09-06T21:56:56.694389: step 941, loss 0.0538167, acc 0.98
2016-09-06T21:56:57.358194: step 942, loss 0.152354, acc 0.96
2016-09-06T21:56:58.068914: step 943, loss 0.0721934, acc 0.96
2016-09-06T21:56:58.753542: step 944, loss 0.0948075, acc 0.94
2016-09-06T21:56:59.418172: step 945, loss 0.230092, acc 0.92
2016-09-06T21:57:00.112578: step 946, loss 0.376248, acc 0.84
2016-09-06T21:57:00.808710: step 947, loss 0.130352, acc 0.96
2016-09-06T21:57:01.508945: step 948, loss 0.102449, acc 0.92
2016-09-06T21:57:02.148951: step 949, loss 0.252266, acc 0.9
2016-09-06T21:57:02.850808: step 950, loss 0.0647242, acc 0.96
2016-09-06T21:57:03.531340: step 951, loss 0.0653139, acc 0.98
2016-09-06T21:57:04.199760: step 952, loss 0.167627, acc 0.96
2016-09-06T21:57:04.896811: step 953, loss 0.134185, acc 0.94
2016-09-06T21:57:05.580462: step 954, loss 0.216326, acc 0.9
2016-09-06T21:57:06.258959: step 955, loss 0.114177, acc 0.94
2016-09-06T21:57:06.939147: step 956, loss 0.14536, acc 0.98
2016-09-06T21:57:07.637501: step 957, loss 0.215401, acc 0.9
2016-09-06T21:57:08.306568: step 958, loss 0.0696529, acc 0.98
2016-09-06T21:57:08.978170: step 959, loss 0.1354, acc 0.94
2016-09-06T21:57:09.602133: step 960, loss 0.0833169, acc 0.977273
2016-09-06T21:57:10.284269: step 961, loss 0.117113, acc 0.96
2016-09-06T21:57:10.977060: step 962, loss 0.081103, acc 0.98
2016-09-06T21:57:11.657929: step 963, loss 0.0637637, acc 0.96
2016-09-06T21:57:12.374884: step 964, loss 0.0574393, acc 0.98
2016-09-06T21:57:13.049992: step 965, loss 0.0950564, acc 0.96
2016-09-06T21:57:13.768696: step 966, loss 0.099745, acc 0.98
2016-09-06T21:57:14.432298: step 967, loss 0.121986, acc 0.92
2016-09-06T21:57:15.108017: step 968, loss 0.0875946, acc 0.96
2016-09-06T21:57:15.795086: step 969, loss 0.0194943, acc 1
2016-09-06T21:57:16.486702: step 970, loss 0.0410505, acc 1
2016-09-06T21:57:17.184790: step 971, loss 0.200138, acc 0.88
2016-09-06T21:57:17.844467: step 972, loss 0.112111, acc 0.96
2016-09-06T21:57:18.537084: step 973, loss 0.146999, acc 0.94
2016-09-06T21:57:19.221268: step 974, loss 0.111895, acc 0.96
2016-09-06T21:57:19.920648: step 975, loss 0.0658442, acc 0.98
2016-09-06T21:57:20.605103: step 976, loss 0.0554004, acc 1
2016-09-06T21:57:21.282827: step 977, loss 0.0987925, acc 0.96
2016-09-06T21:57:21.950843: step 978, loss 0.025051, acc 0.98
2016-09-06T21:57:22.606234: step 979, loss 0.114324, acc 0.92
2016-09-06T21:57:23.296466: step 980, loss 0.030214, acc 1
2016-09-06T21:57:23.996979: step 981, loss 0.0649496, acc 0.96
2016-09-06T21:57:24.681854: step 982, loss 0.0948712, acc 0.96
2016-09-06T21:57:25.370749: step 983, loss 0.0371545, acc 1
2016-09-06T21:57:26.063399: step 984, loss 0.130752, acc 0.94
2016-09-06T21:57:26.762309: step 985, loss 0.198531, acc 0.9
2016-09-06T21:57:27.427906: step 986, loss 0.156067, acc 0.94
2016-09-06T21:57:28.123359: step 987, loss 0.0885525, acc 0.96
2016-09-06T21:57:28.791033: step 988, loss 0.0173594, acc 1
2016-09-06T21:57:29.474164: step 989, loss 0.0716871, acc 0.98
2016-09-06T21:57:30.163339: step 990, loss 0.0278718, acc 1
2016-09-06T21:57:30.839163: step 991, loss 0.114787, acc 0.94
2016-09-06T21:57:31.521026: step 992, loss 0.157259, acc 0.94
2016-09-06T21:57:32.200778: step 993, loss 0.128959, acc 0.96
2016-09-06T21:57:32.897221: step 994, loss 0.118337, acc 0.96
2016-09-06T21:57:33.566065: step 995, loss 0.0886269, acc 0.94
2016-09-06T21:57:34.265451: step 996, loss 0.125093, acc 0.92
2016-09-06T21:57:34.939311: step 997, loss 0.0989455, acc 0.96
2016-09-06T21:57:35.623807: step 998, loss 0.0693457, acc 0.98
2016-09-06T21:57:36.296343: step 999, loss 0.0120846, acc 1
2016-09-06T21:57:36.981571: step 1000, loss 0.231844, acc 0.94

Evaluation:
2016-09-06T21:57:40.144043: step 1000, loss 1.01718, acc 0.770169

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-1000

2016-09-06T21:57:41.858431: step 1001, loss 0.162892, acc 0.92
2016-09-06T21:57:42.563566: step 1002, loss 0.104378, acc 0.98
2016-09-06T21:57:43.224854: step 1003, loss 0.0251848, acc 1
2016-09-06T21:57:43.931392: step 1004, loss 0.0655482, acc 0.98
2016-09-06T21:57:44.622174: step 1005, loss 0.152509, acc 0.92
2016-09-06T21:57:45.304613: step 1006, loss 0.116367, acc 0.96
2016-09-06T21:57:45.998019: step 1007, loss 0.0379745, acc 0.98
2016-09-06T21:57:46.670008: step 1008, loss 0.0932442, acc 0.96
2016-09-06T21:57:47.365483: step 1009, loss 0.057146, acc 0.94
2016-09-06T21:57:48.029105: step 1010, loss 0.157897, acc 0.96
2016-09-06T21:57:48.723003: step 1011, loss 0.106975, acc 0.94
2016-09-06T21:57:49.405069: step 1012, loss 0.0921075, acc 0.96
2016-09-06T21:57:50.100028: step 1013, loss 0.124977, acc 0.96
2016-09-06T21:57:50.792648: step 1014, loss 0.0602915, acc 0.96
2016-09-06T21:57:51.440080: step 1015, loss 0.0620004, acc 0.98
2016-09-06T21:57:52.144107: step 1016, loss 0.0748915, acc 0.96
2016-09-06T21:57:52.832414: step 1017, loss 0.0423021, acc 1
2016-09-06T21:57:53.499197: step 1018, loss 0.00996002, acc 1
2016-09-06T21:57:54.191362: step 1019, loss 0.118933, acc 0.96
2016-09-06T21:57:54.870385: step 1020, loss 0.068572, acc 0.96
2016-09-06T21:57:55.561868: step 1021, loss 0.120621, acc 0.96
2016-09-06T21:57:56.244822: step 1022, loss 0.0198273, acc 0.98
2016-09-06T21:57:56.941221: step 1023, loss 0.0451907, acc 1
2016-09-06T21:57:57.608633: step 1024, loss 0.0695127, acc 0.96
2016-09-06T21:57:58.319142: step 1025, loss 0.0185069, acc 1
2016-09-06T21:57:59.010093: step 1026, loss 0.120171, acc 0.94
2016-09-06T21:57:59.743594: step 1027, loss 0.148626, acc 0.94
2016-09-06T21:58:00.456902: step 1028, loss 0.0924664, acc 0.96
2016-09-06T21:58:01.119498: step 1029, loss 0.146161, acc 0.94
2016-09-06T21:58:01.820218: step 1030, loss 0.0533323, acc 0.96
2016-09-06T21:58:02.509910: step 1031, loss 0.140496, acc 0.94
2016-09-06T21:58:03.190615: step 1032, loss 0.0148569, acc 1
2016-09-06T21:58:03.871092: step 1033, loss 0.141512, acc 0.9
2016-09-06T21:58:04.565871: step 1034, loss 0.0442364, acc 0.98
2016-09-06T21:58:05.245885: step 1035, loss 0.09034, acc 0.96
2016-09-06T21:58:05.904267: step 1036, loss 0.0803542, acc 0.94
2016-09-06T21:58:06.602960: step 1037, loss 0.0856653, acc 0.96
2016-09-06T21:58:07.270816: step 1038, loss 0.0274877, acc 1
2016-09-06T21:58:07.950678: step 1039, loss 0.055361, acc 1
2016-09-06T21:58:08.644834: step 1040, loss 0.0718221, acc 0.96
2016-09-06T21:58:09.319082: step 1041, loss 0.0588486, acc 0.96
2016-09-06T21:58:09.997360: step 1042, loss 0.0840163, acc 0.96
2016-09-06T21:58:10.676410: step 1043, loss 0.0870845, acc 0.98
2016-09-06T21:58:11.386009: step 1044, loss 0.166327, acc 0.96
2016-09-06T21:58:12.052193: step 1045, loss 0.0779377, acc 0.96
2016-09-06T21:58:12.732122: step 1046, loss 0.107796, acc 0.94
2016-09-06T21:58:13.403444: step 1047, loss 0.0765626, acc 0.94
2016-09-06T21:58:14.089687: step 1048, loss 0.10524, acc 0.96
2016-09-06T21:58:14.780908: step 1049, loss 0.168882, acc 0.9
2016-09-06T21:58:15.466384: step 1050, loss 0.140377, acc 0.94
2016-09-06T21:58:16.179767: step 1051, loss 0.12432, acc 0.96
2016-09-06T21:58:16.859795: step 1052, loss 0.164813, acc 0.94
2016-09-06T21:58:17.541088: step 1053, loss 0.278879, acc 0.92
2016-09-06T21:58:18.225265: step 1054, loss 0.16533, acc 0.96
2016-09-06T21:58:18.919125: step 1055, loss 0.0509305, acc 0.96
2016-09-06T21:58:19.607618: step 1056, loss 0.0599097, acc 0.98
2016-09-06T21:58:20.303942: step 1057, loss 0.167881, acc 0.92
2016-09-06T21:58:20.998309: step 1058, loss 0.0848379, acc 0.96
2016-09-06T21:58:21.670445: step 1059, loss 0.225007, acc 0.92
2016-09-06T21:58:22.350409: step 1060, loss 0.130119, acc 0.96
2016-09-06T21:58:23.033023: step 1061, loss 0.102304, acc 0.96
2016-09-06T21:58:23.705932: step 1062, loss 0.0453365, acc 1
2016-09-06T21:58:24.388841: step 1063, loss 0.101874, acc 0.96
2016-09-06T21:58:25.079497: step 1064, loss 0.0484568, acc 1
2016-09-06T21:58:25.797407: step 1065, loss 0.144226, acc 0.9
2016-09-06T21:58:26.474677: step 1066, loss 0.0743394, acc 0.98
2016-09-06T21:58:27.162198: step 1067, loss 0.0331162, acc 1
2016-09-06T21:58:27.845262: step 1068, loss 0.0999168, acc 0.96
2016-09-06T21:58:28.531328: step 1069, loss 0.20039, acc 0.9
2016-09-06T21:58:29.212714: step 1070, loss 0.168147, acc 0.9
2016-09-06T21:58:29.900235: step 1071, loss 0.114838, acc 0.94
2016-09-06T21:58:30.606219: step 1072, loss 0.0701515, acc 0.98
2016-09-06T21:58:31.289206: step 1073, loss 0.0507686, acc 0.96
2016-09-06T21:58:31.965410: step 1074, loss 0.111052, acc 0.92
2016-09-06T21:58:32.669301: step 1075, loss 0.143339, acc 0.92
2016-09-06T21:58:33.365474: step 1076, loss 0.052689, acc 0.98
2016-09-06T21:58:34.068121: step 1077, loss 0.0659301, acc 0.96
2016-09-06T21:58:34.729398: step 1078, loss 0.0725691, acc 0.98
2016-09-06T21:58:35.429351: step 1079, loss 0.0831346, acc 0.98
2016-09-06T21:58:36.114544: step 1080, loss 0.150385, acc 0.92
2016-09-06T21:58:36.807376: step 1081, loss 0.16109, acc 0.92
2016-09-06T21:58:37.488308: step 1082, loss 0.21055, acc 0.88
2016-09-06T21:58:38.190833: step 1083, loss 0.0132124, acc 1
2016-09-06T21:58:38.884654: step 1084, loss 0.0529056, acc 1
2016-09-06T21:58:39.533001: step 1085, loss 0.269496, acc 0.9
2016-09-06T21:58:40.221872: step 1086, loss 0.193874, acc 0.94
2016-09-06T21:58:40.903596: step 1087, loss 0.103074, acc 0.98
2016-09-06T21:58:41.608172: step 1088, loss 0.0823741, acc 0.98
2016-09-06T21:58:42.278753: step 1089, loss 0.0435627, acc 0.98
2016-09-06T21:58:42.960152: step 1090, loss 0.0925812, acc 0.94
2016-09-06T21:58:43.636131: step 1091, loss 0.161197, acc 0.92
2016-09-06T21:58:44.318135: step 1092, loss 0.170993, acc 0.92
2016-09-06T21:58:45.033296: step 1093, loss 0.127482, acc 0.94
2016-09-06T21:58:45.694294: step 1094, loss 0.0579343, acc 0.98
2016-09-06T21:58:46.386961: step 1095, loss 0.0377241, acc 0.98
2016-09-06T21:58:47.079475: step 1096, loss 0.073091, acc 0.98
2016-09-06T21:58:47.771828: step 1097, loss 0.124587, acc 0.94
2016-09-06T21:58:48.477865: step 1098, loss 0.118298, acc 0.92
2016-09-06T21:58:49.162883: step 1099, loss 0.0470371, acc 0.98
2016-09-06T21:58:49.881517: step 1100, loss 0.0757362, acc 0.98

Evaluation:
2016-09-06T21:58:53.029345: step 1100, loss 0.877403, acc 0.773921

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-1100

2016-09-06T21:58:54.764484: step 1101, loss 0.0557833, acc 1
2016-09-06T21:58:55.451837: step 1102, loss 0.0772642, acc 0.96
2016-09-06T21:58:56.121738: step 1103, loss 0.129071, acc 0.96
2016-09-06T21:58:56.819496: step 1104, loss 0.1395, acc 0.96
2016-09-06T21:58:57.503291: step 1105, loss 0.156973, acc 0.9
2016-09-06T21:58:58.190213: step 1106, loss 0.13038, acc 0.96
2016-09-06T21:58:58.858449: step 1107, loss 0.170338, acc 0.94
2016-09-06T21:58:59.561601: step 1108, loss 0.055503, acc 0.96
2016-09-06T21:59:00.258505: step 1109, loss 0.126027, acc 0.94
2016-09-06T21:59:00.943052: step 1110, loss 0.20943, acc 0.94
2016-09-06T21:59:01.622768: step 1111, loss 0.0832534, acc 0.96
2016-09-06T21:59:02.311329: step 1112, loss 0.188923, acc 0.88
2016-09-06T21:59:02.997116: step 1113, loss 0.13594, acc 0.96
2016-09-06T21:59:03.657684: step 1114, loss 0.0975376, acc 0.96
2016-09-06T21:59:04.364354: step 1115, loss 0.140088, acc 0.98
2016-09-06T21:59:05.053510: step 1116, loss 0.0681887, acc 0.96
2016-09-06T21:59:05.716172: step 1117, loss 0.0760366, acc 0.96
2016-09-06T21:59:06.385221: step 1118, loss 0.0943063, acc 0.98
2016-09-06T21:59:07.062294: step 1119, loss 0.176309, acc 0.94
2016-09-06T21:59:07.758830: step 1120, loss 0.181572, acc 0.96
2016-09-06T21:59:08.409696: step 1121, loss 0.205654, acc 0.9
2016-09-06T21:59:09.108677: step 1122, loss 0.0346532, acc 0.98
2016-09-06T21:59:09.786953: step 1123, loss 0.144763, acc 0.94
2016-09-06T21:59:10.480351: step 1124, loss 0.0788162, acc 0.96
2016-09-06T21:59:11.184088: step 1125, loss 0.122517, acc 0.96
2016-09-06T21:59:11.869115: step 1126, loss 0.0756031, acc 0.94
2016-09-06T21:59:12.558260: step 1127, loss 0.0406714, acc 1
2016-09-06T21:59:13.216723: step 1128, loss 0.0702825, acc 0.98
2016-09-06T21:59:13.930559: step 1129, loss 0.0875132, acc 0.98
2016-09-06T21:59:14.608684: step 1130, loss 0.0901943, acc 0.96
2016-09-06T21:59:15.280200: step 1131, loss 0.0538087, acc 1
2016-09-06T21:59:15.981720: step 1132, loss 0.0725656, acc 0.98
2016-09-06T21:59:16.662274: step 1133, loss 0.0651872, acc 0.98
2016-09-06T21:59:17.342377: step 1134, loss 0.168622, acc 0.94
2016-09-06T21:59:18.019422: step 1135, loss 0.0783213, acc 0.96
2016-09-06T21:59:18.719262: step 1136, loss 0.0504817, acc 0.98
2016-09-06T21:59:19.396831: step 1137, loss 0.153176, acc 0.96
2016-09-06T21:59:20.083242: step 1138, loss 0.117984, acc 0.94
2016-09-06T21:59:20.772918: step 1139, loss 0.174324, acc 0.92
2016-09-06T21:59:21.460367: step 1140, loss 0.12967, acc 0.94
2016-09-06T21:59:22.117945: step 1141, loss 0.0915383, acc 0.98
2016-09-06T21:59:22.822236: step 1142, loss 0.172718, acc 0.92
2016-09-06T21:59:23.528758: step 1143, loss 0.188436, acc 0.92
2016-09-06T21:59:24.205895: step 1144, loss 0.145716, acc 0.94
2016-09-06T21:59:24.891199: step 1145, loss 0.0643525, acc 0.98
2016-09-06T21:59:25.567744: step 1146, loss 0.123077, acc 0.94
2016-09-06T21:59:26.250481: step 1147, loss 0.221829, acc 0.9
2016-09-06T21:59:26.925671: step 1148, loss 0.0374449, acc 0.98
2016-09-06T21:59:27.630182: step 1149, loss 0.095122, acc 0.98
2016-09-06T21:59:28.328896: step 1150, loss 0.076146, acc 0.94
2016-09-06T21:59:29.000079: step 1151, loss 0.122781, acc 0.94
2016-09-06T21:59:29.626647: step 1152, loss 0.103161, acc 0.977273
2016-09-06T21:59:30.321247: step 1153, loss 0.0923913, acc 0.92
2016-09-06T21:59:31.041743: step 1154, loss 0.116386, acc 0.94
2016-09-06T21:59:31.746237: step 1155, loss 0.091285, acc 0.96
2016-09-06T21:59:32.443248: step 1156, loss 0.11906, acc 0.94
2016-09-06T21:59:33.153753: step 1157, loss 0.0691712, acc 0.98
2016-09-06T21:59:33.842453: step 1158, loss 0.0529649, acc 1
2016-09-06T21:59:34.540048: step 1159, loss 0.0168081, acc 1
2016-09-06T21:59:35.245139: step 1160, loss 0.0943238, acc 0.94
2016-09-06T21:59:35.927635: step 1161, loss 0.159059, acc 0.98
2016-09-06T21:59:36.603304: step 1162, loss 0.207837, acc 0.9
2016-09-06T21:59:37.252272: step 1163, loss 0.0325264, acc 1
2016-09-06T21:59:37.946645: step 1164, loss 0.0711422, acc 1
2016-09-06T21:59:38.619801: step 1165, loss 0.0917897, acc 0.96
2016-09-06T21:59:39.311241: step 1166, loss 0.0730355, acc 0.96
2016-09-06T21:59:40.000370: step 1167, loss 0.0217639, acc 1
2016-09-06T21:59:40.682220: step 1168, loss 0.0511815, acc 0.98
2016-09-06T21:59:41.361378: step 1169, loss 0.167834, acc 0.94
2016-09-06T21:59:42.052159: step 1170, loss 0.107294, acc 0.94
2016-09-06T21:59:42.769569: step 1171, loss 0.170097, acc 0.96
2016-09-06T21:59:43.456041: step 1172, loss 0.182815, acc 0.96
2016-09-06T21:59:44.133304: step 1173, loss 0.0415551, acc 1
2016-09-06T21:59:44.800461: step 1174, loss 0.0595966, acc 0.98
2016-09-06T21:59:45.493153: step 1175, loss 0.0616951, acc 0.98
2016-09-06T21:59:46.193677: step 1176, loss 0.0591824, acc 0.98
2016-09-06T21:59:46.886417: step 1177, loss 0.0986789, acc 0.94
2016-09-06T21:59:47.596020: step 1178, loss 0.111279, acc 0.96
2016-09-06T21:59:48.273675: step 1179, loss 0.0538457, acc 0.98
2016-09-06T21:59:48.959697: step 1180, loss 0.161436, acc 0.98
2016-09-06T21:59:49.647471: step 1181, loss 0.0329825, acc 1
2016-09-06T21:59:50.326884: step 1182, loss 0.0503452, acc 1
2016-09-06T21:59:51.024135: step 1183, loss 0.105748, acc 0.94
2016-09-06T21:59:51.687781: step 1184, loss 0.167099, acc 0.94
2016-09-06T21:59:52.401580: step 1185, loss 0.0346588, acc 0.98
2016-09-06T21:59:53.079439: step 1186, loss 0.0562837, acc 0.96
2016-09-06T21:59:53.760858: step 1187, loss 0.0551202, acc 0.98
2016-09-06T21:59:54.463170: step 1188, loss 0.124705, acc 0.96
2016-09-06T21:59:55.138236: step 1189, loss 0.0605747, acc 0.98
2016-09-06T21:59:55.832763: step 1190, loss 0.0889558, acc 0.96
2016-09-06T21:59:56.528872: step 1191, loss 0.0436575, acc 0.98
2016-09-06T21:59:57.237399: step 1192, loss 0.0459064, acc 0.98
2016-09-06T21:59:57.935565: step 1193, loss 0.090968, acc 0.96
2016-09-06T21:59:58.604991: step 1194, loss 0.0724367, acc 0.98
2016-09-06T21:59:59.288283: step 1195, loss 0.0426662, acc 0.98
2016-09-06T21:59:59.957240: step 1196, loss 0.0536874, acc 0.98
2016-09-06T22:00:00.645696: step 1197, loss 0.0991144, acc 0.96
2016-09-06T22:00:01.326986: step 1198, loss 0.0784293, acc 0.98
2016-09-06T22:00:02.017980: step 1199, loss 0.0781193, acc 0.96
2016-09-06T22:00:02.673880: step 1200, loss 0.0777729, acc 0.96

Evaluation:
2016-09-06T22:00:05.802634: step 1200, loss 1.02821, acc 0.766416

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-1200

2016-09-06T22:00:07.471808: step 1201, loss 0.0379259, acc 0.98
2016-09-06T22:00:08.181941: step 1202, loss 0.190622, acc 0.92
2016-09-06T22:00:08.855268: step 1203, loss 0.0650703, acc 0.98
2016-09-06T22:00:09.534962: step 1204, loss 0.0406268, acc 0.98
2016-09-06T22:00:10.224299: step 1205, loss 0.125499, acc 0.96
2016-09-06T22:00:10.892983: step 1206, loss 0.146344, acc 0.94
2016-09-06T22:00:11.584771: step 1207, loss 0.0393543, acc 0.96
2016-09-06T22:00:12.239384: step 1208, loss 0.0327813, acc 1
2016-09-06T22:00:12.954244: step 1209, loss 0.0877081, acc 0.98
2016-09-06T22:00:13.647433: step 1210, loss 0.119917, acc 0.94
2016-09-06T22:00:14.334087: step 1211, loss 0.105325, acc 0.96
2016-09-06T22:00:15.045496: step 1212, loss 0.105184, acc 0.98
2016-09-06T22:00:15.733217: step 1213, loss 0.185933, acc 0.92
2016-09-06T22:00:16.433946: step 1214, loss 0.0259956, acc 1
2016-09-06T22:00:17.117068: step 1215, loss 0.0800279, acc 0.96
2016-09-06T22:00:17.821137: step 1216, loss 0.0441117, acc 1
2016-09-06T22:00:18.502837: step 1217, loss 0.0322069, acc 0.98
2016-09-06T22:00:19.208180: step 1218, loss 0.139084, acc 0.92
2016-09-06T22:00:19.878921: step 1219, loss 0.0667773, acc 0.98
2016-09-06T22:00:20.575487: step 1220, loss 0.12474, acc 0.98
2016-09-06T22:00:21.277485: step 1221, loss 0.0973975, acc 0.94
2016-09-06T22:00:21.974526: step 1222, loss 0.0399022, acc 0.98
2016-09-06T22:00:22.649599: step 1223, loss 0.0893285, acc 0.96
2016-09-06T22:00:23.354014: step 1224, loss 0.0830011, acc 0.96
2016-09-06T22:00:24.062205: step 1225, loss 0.0307604, acc 1
2016-09-06T22:00:24.752948: step 1226, loss 0.1868, acc 0.96
2016-09-06T22:00:25.413096: step 1227, loss 0.159917, acc 0.94
2016-09-06T22:00:26.120488: step 1228, loss 0.0456339, acc 1
2016-09-06T22:00:26.809280: step 1229, loss 0.160474, acc 0.94
2016-09-06T22:00:27.505772: step 1230, loss 0.154892, acc 0.96
2016-09-06T22:00:28.202219: step 1231, loss 0.198008, acc 0.98
2016-09-06T22:00:28.883938: step 1232, loss 0.0541019, acc 0.98
2016-09-06T22:00:29.581027: step 1233, loss 0.0971214, acc 0.94
2016-09-06T22:00:30.246919: step 1234, loss 0.0763843, acc 0.98
2016-09-06T22:00:30.944491: step 1235, loss 0.0783478, acc 0.96
2016-09-06T22:00:31.635069: step 1236, loss 0.0603353, acc 0.98
2016-09-06T22:00:32.323818: step 1237, loss 0.072252, acc 0.96
2016-09-06T22:00:33.006136: step 1238, loss 0.0976134, acc 0.94
2016-09-06T22:00:33.693604: step 1239, loss 0.139176, acc 0.92
2016-09-06T22:00:34.383909: step 1240, loss 0.124968, acc 0.96
2016-09-06T22:00:35.056652: step 1241, loss 0.12823, acc 0.96
2016-09-06T22:00:35.749452: step 1242, loss 0.124805, acc 0.96
2016-09-06T22:00:36.451104: step 1243, loss 0.145742, acc 0.94
2016-09-06T22:00:37.142388: step 1244, loss 0.0965831, acc 0.96
2016-09-06T22:00:37.825890: step 1245, loss 0.0423941, acc 0.98
2016-09-06T22:00:38.504980: step 1246, loss 0.0997597, acc 0.96
2016-09-06T22:00:39.196330: step 1247, loss 0.069288, acc 0.98
2016-09-06T22:00:39.869195: step 1248, loss 0.0628844, acc 0.96
2016-09-06T22:00:40.591065: step 1249, loss 0.0820962, acc 0.94
2016-09-06T22:00:41.277076: step 1250, loss 0.105232, acc 0.94
2016-09-06T22:00:41.980121: step 1251, loss 0.179468, acc 0.92
2016-09-06T22:00:42.663506: step 1252, loss 0.0531603, acc 0.98
2016-09-06T22:00:43.346966: step 1253, loss 0.148995, acc 0.94
2016-09-06T22:00:44.060238: step 1254, loss 0.0192838, acc 1
2016-09-06T22:00:44.739053: step 1255, loss 0.0245329, acc 1
2016-09-06T22:00:45.441742: step 1256, loss 0.0673954, acc 0.98
2016-09-06T22:00:46.115940: step 1257, loss 0.0592438, acc 0.96
2016-09-06T22:00:46.790427: step 1258, loss 0.0413817, acc 0.98
2016-09-06T22:00:47.487856: step 1259, loss 0.0708788, acc 0.96
2016-09-06T22:00:48.192467: step 1260, loss 0.0432958, acc 1
2016-09-06T22:00:48.907084: step 1261, loss 0.171672, acc 0.92
2016-09-06T22:00:49.619531: step 1262, loss 0.0883718, acc 0.96
2016-09-06T22:00:50.307293: step 1263, loss 0.0759183, acc 0.96
2016-09-06T22:00:51.016540: step 1264, loss 0.0221231, acc 1
2016-09-06T22:00:51.695049: step 1265, loss 0.196798, acc 0.94
2016-09-06T22:00:52.390052: step 1266, loss 0.0481574, acc 0.98
2016-09-06T22:00:53.088574: step 1267, loss 0.131695, acc 0.9
2016-09-06T22:00:53.801146: step 1268, loss 0.0308649, acc 1
2016-09-06T22:00:54.508116: step 1269, loss 0.0493375, acc 0.98
2016-09-06T22:00:55.208351: step 1270, loss 0.057642, acc 0.96
2016-09-06T22:00:55.909200: step 1271, loss 0.124565, acc 0.96
2016-09-06T22:00:56.612104: step 1272, loss 0.120825, acc 0.96
2016-09-06T22:00:57.309995: step 1273, loss 0.0624058, acc 0.96
2016-09-06T22:00:57.981226: step 1274, loss 0.147661, acc 0.94
2016-09-06T22:00:58.667438: step 1275, loss 0.0128895, acc 1
2016-09-06T22:00:59.369349: step 1276, loss 0.0718472, acc 0.96
2016-09-06T22:01:00.078329: step 1277, loss 0.0790956, acc 0.96
2016-09-06T22:01:00.819401: step 1278, loss 0.134297, acc 0.96
2016-09-06T22:01:01.480637: step 1279, loss 0.0633405, acc 0.96
2016-09-06T22:01:02.198286: step 1280, loss 0.141252, acc 0.98
2016-09-06T22:01:02.915337: step 1281, loss 0.205319, acc 0.9
2016-09-06T22:01:03.602144: step 1282, loss 0.133581, acc 0.92
2016-09-06T22:01:04.286663: step 1283, loss 0.0849891, acc 0.96
2016-09-06T22:01:04.972403: step 1284, loss 0.027348, acc 0.98
2016-09-06T22:01:05.700058: step 1285, loss 0.0644471, acc 0.98
2016-09-06T22:01:06.361537: step 1286, loss 0.0524805, acc 0.98
2016-09-06T22:01:07.051877: step 1287, loss 0.0252083, acc 0.98
2016-09-06T22:01:07.717314: step 1288, loss 0.0799017, acc 0.94
2016-09-06T22:01:08.399234: step 1289, loss 0.101958, acc 0.96
2016-09-06T22:01:09.084260: step 1290, loss 0.0296241, acc 1
2016-09-06T22:01:09.764819: step 1291, loss 0.0290118, acc 1
2016-09-06T22:01:10.436353: step 1292, loss 0.0399667, acc 0.98
2016-09-06T22:01:11.099434: step 1293, loss 0.0678627, acc 0.98
2016-09-06T22:01:11.803036: step 1294, loss 0.0547904, acc 1
2016-09-06T22:01:12.475790: step 1295, loss 0.0298348, acc 1
2016-09-06T22:01:13.191289: step 1296, loss 0.0920852, acc 0.98
2016-09-06T22:01:13.892725: step 1297, loss 0.0986, acc 0.94
2016-09-06T22:01:14.578314: step 1298, loss 0.0552027, acc 0.98
2016-09-06T22:01:15.283211: step 1299, loss 0.157585, acc 0.96
2016-09-06T22:01:15.959677: step 1300, loss 0.192499, acc 0.94

Evaluation:
2016-09-06T22:01:19.116922: step 1300, loss 1.25296, acc 0.768293

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-1300

2016-09-06T22:01:20.820007: step 1301, loss 0.0584013, acc 0.98
2016-09-06T22:01:21.520078: step 1302, loss 0.270197, acc 0.88
2016-09-06T22:01:22.200413: step 1303, loss 0.0482306, acc 0.96
2016-09-06T22:01:22.883252: step 1304, loss 0.0146393, acc 1
2016-09-06T22:01:23.589618: step 1305, loss 0.197272, acc 0.96
2016-09-06T22:01:24.266908: step 1306, loss 0.136297, acc 0.92
2016-09-06T22:01:24.962562: step 1307, loss 0.0599947, acc 0.98
2016-09-06T22:01:25.632139: step 1308, loss 0.178961, acc 0.94
2016-09-06T22:01:26.334138: step 1309, loss 0.0368064, acc 0.98
2016-09-06T22:01:27.032997: step 1310, loss 0.0939288, acc 0.94
2016-09-06T22:01:27.718787: step 1311, loss 0.211851, acc 0.92
2016-09-06T22:01:28.408106: step 1312, loss 0.10796, acc 0.94
2016-09-06T22:01:29.099724: step 1313, loss 0.040829, acc 1
2016-09-06T22:01:29.802558: step 1314, loss 0.133349, acc 0.98
2016-09-06T22:01:30.490764: step 1315, loss 0.0370325, acc 1
2016-09-06T22:01:31.156248: step 1316, loss 0.0627441, acc 0.98
2016-09-06T22:01:31.846121: step 1317, loss 0.074715, acc 0.94
2016-09-06T22:01:32.558437: step 1318, loss 0.104426, acc 0.94
2016-09-06T22:01:33.257517: step 1319, loss 0.0641495, acc 0.98
2016-09-06T22:01:33.912567: step 1320, loss 0.148521, acc 0.94
2016-09-06T22:01:34.654215: step 1321, loss 0.0381714, acc 0.98
2016-09-06T22:01:35.328229: step 1322, loss 0.153039, acc 0.94
2016-09-06T22:01:35.996734: step 1323, loss 0.0689299, acc 0.94
2016-09-06T22:01:36.685234: step 1324, loss 0.0384328, acc 1
2016-09-06T22:01:37.353858: step 1325, loss 0.139103, acc 0.94
2016-09-06T22:01:38.041506: step 1326, loss 0.0791184, acc 0.94
2016-09-06T22:01:38.717225: step 1327, loss 0.0314455, acc 0.98
2016-09-06T22:01:39.410829: step 1328, loss 0.0894483, acc 0.94
2016-09-06T22:01:40.104743: step 1329, loss 0.121271, acc 0.94
2016-09-06T22:01:40.798766: step 1330, loss 0.145757, acc 0.9
2016-09-06T22:01:41.499612: step 1331, loss 0.116109, acc 0.96
2016-09-06T22:01:42.179137: step 1332, loss 0.0317994, acc 1
2016-09-06T22:01:42.854313: step 1333, loss 0.12568, acc 0.96
2016-09-06T22:01:43.531247: step 1334, loss 0.0265292, acc 0.98
2016-09-06T22:01:44.231984: step 1335, loss 0.121258, acc 0.96
2016-09-06T22:01:44.914623: step 1336, loss 0.0955555, acc 0.94
2016-09-06T22:01:45.595986: step 1337, loss 0.0528872, acc 0.98
2016-09-06T22:01:46.287167: step 1338, loss 0.0882489, acc 0.96
2016-09-06T22:01:46.973405: step 1339, loss 0.054734, acc 0.98
2016-09-06T22:01:47.673093: step 1340, loss 0.0217394, acc 1
2016-09-06T22:01:48.350412: step 1341, loss 0.148019, acc 0.94
2016-09-06T22:01:49.069325: step 1342, loss 0.0841967, acc 0.96
2016-09-06T22:01:49.761576: step 1343, loss 0.10469, acc 0.96
2016-09-06T22:01:50.402219: step 1344, loss 0.0954888, acc 0.954545
2016-09-06T22:01:51.086509: step 1345, loss 0.0432266, acc 0.98
2016-09-06T22:01:51.775998: step 1346, loss 0.0398336, acc 1
2016-09-06T22:01:52.481307: step 1347, loss 0.086889, acc 0.92
2016-09-06T22:01:53.157844: step 1348, loss 0.0491337, acc 0.98
2016-09-06T22:01:53.866748: step 1349, loss 0.0768567, acc 0.96
2016-09-06T22:01:54.554073: step 1350, loss 0.0771829, acc 0.94
2016-09-06T22:01:55.233183: step 1351, loss 0.182388, acc 0.92
2016-09-06T22:01:55.922617: step 1352, loss 0.0166097, acc 1
2016-09-06T22:01:56.621795: step 1353, loss 0.0809827, acc 0.98
2016-09-06T22:01:57.303581: step 1354, loss 0.016885, acc 1
2016-09-06T22:01:57.973241: step 1355, loss 0.0725602, acc 0.96
2016-09-06T22:01:58.687606: step 1356, loss 0.0576737, acc 0.96
2016-09-06T22:01:59.380980: step 1357, loss 0.0623832, acc 0.98
2016-09-06T22:02:00.056890: step 1358, loss 0.0664153, acc 0.96
2016-09-06T22:02:00.784881: step 1359, loss 0.0099176, acc 1
2016-09-06T22:02:01.493114: step 1360, loss 0.0025193, acc 1
2016-09-06T22:02:02.231928: step 1361, loss 0.0907697, acc 0.96
2016-09-06T22:02:02.911628: step 1362, loss 0.0345596, acc 1
2016-09-06T22:02:03.601805: step 1363, loss 0.234356, acc 0.92
2016-09-06T22:02:04.275487: step 1364, loss 0.0402898, acc 0.98
2016-09-06T22:02:04.956431: step 1365, loss 0.121654, acc 0.96
2016-09-06T22:02:05.645378: step 1366, loss 0.13161, acc 0.96
2016-09-06T22:02:06.326102: step 1367, loss 0.0269348, acc 1
2016-09-06T22:02:07.036770: step 1368, loss 0.0488733, acc 0.98
2016-09-06T22:02:07.724156: step 1369, loss 0.0146565, acc 1
2016-09-06T22:02:08.420445: step 1370, loss 0.0468553, acc 0.98
2016-09-06T22:02:09.118163: step 1371, loss 0.107276, acc 0.98
2016-09-06T22:02:09.811284: step 1372, loss 0.136741, acc 0.96
2016-09-06T22:02:10.491288: step 1373, loss 0.117801, acc 0.94
2016-09-06T22:02:11.172920: step 1374, loss 0.045156, acc 0.96
2016-09-06T22:02:11.871509: step 1375, loss 0.0834562, acc 0.94
2016-09-06T22:02:12.563203: step 1376, loss 0.0992182, acc 0.98
2016-09-06T22:02:13.236268: step 1377, loss 0.00590689, acc 1
2016-09-06T22:02:13.925493: step 1378, loss 0.0415271, acc 0.98
2016-09-06T22:02:14.623654: step 1379, loss 0.0339783, acc 0.98
2016-09-06T22:02:15.328960: step 1380, loss 0.128996, acc 0.94
2016-09-06T22:02:16.002217: step 1381, loss 0.0515244, acc 0.98
2016-09-06T22:02:16.711638: step 1382, loss 0.105201, acc 0.98
2016-09-06T22:02:17.389513: step 1383, loss 0.0519478, acc 0.98
2016-09-06T22:02:18.081521: step 1384, loss 0.022413, acc 0.98
2016-09-06T22:02:18.775232: step 1385, loss 0.0334182, acc 1
2016-09-06T22:02:19.460415: step 1386, loss 0.0514129, acc 0.98
2016-09-06T22:02:20.184197: step 1387, loss 0.0973422, acc 0.98
2016-09-06T22:02:20.856810: step 1388, loss 0.0696643, acc 0.98
2016-09-06T22:02:21.557607: step 1389, loss 0.110082, acc 0.92
2016-09-06T22:02:22.230503: step 1390, loss 0.0620106, acc 0.98
2016-09-06T22:02:22.912013: step 1391, loss 0.00671511, acc 1
2016-09-06T22:02:23.606875: step 1392, loss 0.103139, acc 0.94
2016-09-06T22:02:24.304268: step 1393, loss 0.0148321, acc 1
2016-09-06T22:02:24.995616: step 1394, loss 0.0993529, acc 0.98
2016-09-06T22:02:25.678682: step 1395, loss 0.197371, acc 0.92
2016-09-06T22:02:26.378847: step 1396, loss 0.0165049, acc 1
2016-09-06T22:02:27.054071: step 1397, loss 0.0148925, acc 1
2016-09-06T22:02:27.750524: step 1398, loss 0.0886685, acc 0.94
2016-09-06T22:02:28.439073: step 1399, loss 0.00398701, acc 1
2016-09-06T22:02:29.129268: step 1400, loss 0.0432563, acc 1

Evaluation:
2016-09-06T22:02:32.278869: step 1400, loss 1.02761, acc 0.758912

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-1400

2016-09-06T22:02:33.912914: step 1401, loss 0.199197, acc 0.96
2016-09-06T22:02:34.611034: step 1402, loss 0.0391179, acc 1
2016-09-06T22:02:35.258565: step 1403, loss 0.258278, acc 0.92
2016-09-06T22:02:35.932265: step 1404, loss 0.105556, acc 0.96
2016-09-06T22:02:36.633518: step 1405, loss 0.080813, acc 0.94
2016-09-06T22:02:37.321265: step 1406, loss 0.061799, acc 0.96
2016-09-06T22:02:38.015548: step 1407, loss 0.10453, acc 0.98
2016-09-06T22:02:38.705814: step 1408, loss 0.0587537, acc 0.98
2016-09-06T22:02:39.388416: step 1409, loss 0.0290559, acc 1
2016-09-06T22:02:40.069849: step 1410, loss 0.105579, acc 0.96
2016-09-06T22:02:40.772417: step 1411, loss 0.0445981, acc 0.98
2016-09-06T22:02:41.451952: step 1412, loss 0.0316793, acc 1
2016-09-06T22:02:42.132700: step 1413, loss 0.0499408, acc 0.98
2016-09-06T22:02:42.826521: step 1414, loss 0.0679298, acc 0.98
2016-09-06T22:02:43.505443: step 1415, loss 0.0681206, acc 0.96
2016-09-06T22:02:44.197992: step 1416, loss 0.0640452, acc 0.96
2016-09-06T22:02:44.846150: step 1417, loss 0.164267, acc 0.92
2016-09-06T22:02:45.566688: step 1418, loss 0.167851, acc 0.94
2016-09-06T22:02:46.242742: step 1419, loss 0.0673391, acc 0.96
2016-09-06T22:02:46.926157: step 1420, loss 0.0832646, acc 0.94
2016-09-06T22:02:47.627670: step 1421, loss 0.0787112, acc 0.96
2016-09-06T22:02:48.307003: step 1422, loss 0.155573, acc 0.96
2016-09-06T22:02:48.990848: step 1423, loss 0.0490513, acc 0.98
2016-09-06T22:02:49.666132: step 1424, loss 0.0400912, acc 0.98
2016-09-06T22:02:50.347420: step 1425, loss 0.0256537, acc 1
2016-09-06T22:02:51.030166: step 1426, loss 0.0326969, acc 1
2016-09-06T22:02:51.755582: step 1427, loss 0.0695267, acc 0.98
2016-09-06T22:02:52.453192: step 1428, loss 0.155581, acc 0.92
2016-09-06T22:02:53.156421: step 1429, loss 0.0349292, acc 1
2016-09-06T22:02:53.840974: step 1430, loss 0.0563975, acc 0.98
2016-09-06T22:02:54.532058: step 1431, loss 0.0817911, acc 0.96
2016-09-06T22:02:55.233210: step 1432, loss 0.0695599, acc 0.96
2016-09-06T22:02:55.924197: step 1433, loss 0.0580989, acc 0.98
2016-09-06T22:02:56.614887: step 1434, loss 0.0590306, acc 0.98
2016-09-06T22:02:57.304791: step 1435, loss 0.0149254, acc 1
2016-09-06T22:02:57.988987: step 1436, loss 0.0604825, acc 0.96
2016-09-06T22:02:58.679237: step 1437, loss 0.0414196, acc 0.98
2016-09-06T22:02:59.337287: step 1438, loss 0.120548, acc 0.96
2016-09-06T22:03:00.048777: step 1439, loss 0.0949178, acc 0.94
2016-09-06T22:03:00.779122: step 1440, loss 0.0786894, acc 0.96
2016-09-06T22:03:01.464227: step 1441, loss 0.0494754, acc 0.98
2016-09-06T22:03:02.167169: step 1442, loss 0.0983751, acc 0.96
2016-09-06T22:03:02.851814: step 1443, loss 0.230623, acc 0.96
2016-09-06T22:03:03.540683: step 1444, loss 0.0562794, acc 0.98
2016-09-06T22:03:04.199482: step 1445, loss 0.0665404, acc 0.98
2016-09-06T22:03:04.913535: step 1446, loss 0.0774493, acc 0.96
2016-09-06T22:03:05.593966: step 1447, loss 0.0348086, acc 0.98
2016-09-06T22:03:06.278989: step 1448, loss 0.116907, acc 0.96
2016-09-06T22:03:06.981789: step 1449, loss 0.0562176, acc 0.96
2016-09-06T22:03:07.668081: step 1450, loss 0.0444314, acc 0.98
2016-09-06T22:03:08.359817: step 1451, loss 0.059321, acc 0.96
2016-09-06T22:03:09.029633: step 1452, loss 0.0394538, acc 1
2016-09-06T22:03:09.739177: step 1453, loss 0.0288408, acc 0.98
2016-09-06T22:03:10.436507: step 1454, loss 0.0912851, acc 0.96
2016-09-06T22:03:11.102403: step 1455, loss 0.063578, acc 0.96
2016-09-06T22:03:11.770909: step 1456, loss 0.0250425, acc 0.98
2016-09-06T22:03:12.463939: step 1457, loss 0.0832066, acc 0.96
2016-09-06T22:03:13.144733: step 1458, loss 0.00674266, acc 1
2016-09-06T22:03:13.810661: step 1459, loss 0.0488171, acc 0.98
2016-09-06T22:03:14.513540: step 1460, loss 0.045071, acc 0.98
2016-09-06T22:03:15.208224: step 1461, loss 0.00607685, acc 1
2016-09-06T22:03:15.883097: step 1462, loss 0.214009, acc 0.92
2016-09-06T22:03:16.563044: step 1463, loss 0.131131, acc 0.92
2016-09-06T22:03:17.265115: step 1464, loss 0.0209251, acc 1
2016-09-06T22:03:17.947963: step 1465, loss 0.0591807, acc 0.98
2016-09-06T22:03:18.611893: step 1466, loss 0.115944, acc 0.94
2016-09-06T22:03:19.317846: step 1467, loss 0.0298787, acc 0.98
2016-09-06T22:03:20.001646: step 1468, loss 0.0798041, acc 0.98
2016-09-06T22:03:20.687605: step 1469, loss 0.0198157, acc 0.98
2016-09-06T22:03:21.405390: step 1470, loss 0.00258288, acc 1
2016-09-06T22:03:22.071411: step 1471, loss 0.0450254, acc 0.98
2016-09-06T22:03:22.759759: step 1472, loss 0.0690302, acc 0.96
2016-09-06T22:03:23.424472: step 1473, loss 0.0781299, acc 0.94
2016-09-06T22:03:24.131269: step 1474, loss 0.183954, acc 0.9
2016-09-06T22:03:24.832905: step 1475, loss 0.0665653, acc 0.98
2016-09-06T22:03:25.536152: step 1476, loss 0.159088, acc 0.94
2016-09-06T22:03:26.231946: step 1477, loss 0.0805036, acc 0.96
2016-09-06T22:03:26.911566: step 1478, loss 0.12908, acc 0.96
2016-09-06T22:03:27.623934: step 1479, loss 0.0549606, acc 0.98
2016-09-06T22:03:28.288974: step 1480, loss 0.0502049, acc 0.98
2016-09-06T22:03:28.981939: step 1481, loss 0.0315607, acc 0.98
2016-09-06T22:03:29.685803: step 1482, loss 0.0566411, acc 0.96
2016-09-06T22:03:30.372431: step 1483, loss 0.0858725, acc 0.98
2016-09-06T22:03:31.067346: step 1484, loss 0.158758, acc 0.94
2016-09-06T22:03:31.735344: step 1485, loss 0.0685091, acc 0.98
2016-09-06T22:03:32.436580: step 1486, loss 0.0388285, acc 0.98
2016-09-06T22:03:33.117275: step 1487, loss 0.0218525, acc 1
2016-09-06T22:03:33.822388: step 1488, loss 0.108868, acc 0.92
2016-09-06T22:03:34.515478: step 1489, loss 0.0152392, acc 1
2016-09-06T22:03:35.199527: step 1490, loss 0.0234036, acc 1
2016-09-06T22:03:35.902452: step 1491, loss 0.114588, acc 0.94
2016-09-06T22:03:36.591440: step 1492, loss 0.164484, acc 0.94
2016-09-06T22:03:37.288615: step 1493, loss 0.113177, acc 0.98
2016-09-06T22:03:37.968324: step 1494, loss 0.0448894, acc 0.98
2016-09-06T22:03:38.671429: step 1495, loss 0.0479291, acc 0.98
2016-09-06T22:03:39.372610: step 1496, loss 0.11227, acc 0.96
2016-09-06T22:03:40.053437: step 1497, loss 0.00869558, acc 1
2016-09-06T22:03:40.731375: step 1498, loss 0.0911631, acc 0.92
2016-09-06T22:03:41.422272: step 1499, loss 0.0203669, acc 1
2016-09-06T22:03:42.138196: step 1500, loss 0.0225708, acc 1

Evaluation:
2016-09-06T22:03:45.263882: step 1500, loss 1.02742, acc 0.780488

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-1500

2016-09-06T22:03:46.989485: step 1501, loss 0.119819, acc 0.98
2016-09-06T22:03:47.676980: step 1502, loss 0.0150273, acc 1
2016-09-06T22:03:48.358876: step 1503, loss 0.0443999, acc 1
2016-09-06T22:03:49.058826: step 1504, loss 0.0722347, acc 0.96
2016-09-06T22:03:49.739281: step 1505, loss 0.0537632, acc 0.96
2016-09-06T22:03:50.411680: step 1506, loss 0.0547635, acc 0.98
2016-09-06T22:03:51.072835: step 1507, loss 0.0708713, acc 0.98
2016-09-06T22:03:51.769871: step 1508, loss 0.0123204, acc 1
2016-09-06T22:03:52.469109: step 1509, loss 0.098266, acc 0.96
2016-09-06T22:03:53.176117: step 1510, loss 0.101343, acc 0.94
2016-09-06T22:03:53.889255: step 1511, loss 0.0953197, acc 0.98
2016-09-06T22:03:54.586116: step 1512, loss 0.133696, acc 0.94
2016-09-06T22:03:55.273779: step 1513, loss 0.0297207, acc 0.98
2016-09-06T22:03:55.917079: step 1514, loss 0.0698451, acc 0.96
2016-09-06T22:03:56.610869: step 1515, loss 0.00910367, acc 1
2016-09-06T22:03:57.302448: step 1516, loss 0.0295895, acc 0.98
2016-09-06T22:03:57.988897: step 1517, loss 0.0541366, acc 0.96
2016-09-06T22:03:58.678582: step 1518, loss 0.0904771, acc 0.92
2016-09-06T22:03:59.366104: step 1519, loss 0.100587, acc 0.98
2016-09-06T22:04:00.059163: step 1520, loss 0.0582429, acc 0.98
2016-09-06T22:04:00.758382: step 1521, loss 0.0643602, acc 0.96
2016-09-06T22:04:01.462688: step 1522, loss 0.124105, acc 0.92
2016-09-06T22:04:02.159168: step 1523, loss 0.0837509, acc 0.96
2016-09-06T22:04:02.867104: step 1524, loss 0.101571, acc 0.94
2016-09-06T22:04:03.593931: step 1525, loss 0.0349879, acc 0.98
2016-09-06T22:04:04.290696: step 1526, loss 0.0165612, acc 1
2016-09-06T22:04:04.979526: step 1527, loss 0.0435494, acc 0.98
2016-09-06T22:04:05.645663: step 1528, loss 0.0981437, acc 0.98
2016-09-06T22:04:06.325728: step 1529, loss 0.0202009, acc 1
2016-09-06T22:04:07.008497: step 1530, loss 0.118432, acc 0.94
2016-09-06T22:04:07.726968: step 1531, loss 0.1051, acc 0.92
2016-09-06T22:04:08.450051: step 1532, loss 0.0491729, acc 0.98
2016-09-06T22:04:09.104267: step 1533, loss 0.141881, acc 0.94
2016-09-06T22:04:09.796713: step 1534, loss 0.0253618, acc 0.98
2016-09-06T22:04:10.492212: step 1535, loss 0.0967669, acc 0.96
2016-09-06T22:04:11.141940: step 1536, loss 0.0207216, acc 1
2016-09-06T22:04:11.845675: step 1537, loss 0.0250473, acc 1
2016-09-06T22:04:12.503901: step 1538, loss 0.0192293, acc 1
2016-09-06T22:04:13.191500: step 1539, loss 0.111492, acc 0.96
2016-09-06T22:04:13.882123: step 1540, loss 0.0488233, acc 0.98
2016-09-06T22:04:14.572209: step 1541, loss 0.0956776, acc 0.96
2016-09-06T22:04:15.237665: step 1542, loss 0.0777747, acc 0.98
2016-09-06T22:04:15.919595: step 1543, loss 0.0785219, acc 0.96
2016-09-06T22:04:16.609790: step 1544, loss 0.140813, acc 0.94
2016-09-06T22:04:17.297311: step 1545, loss 0.0622521, acc 0.96
2016-09-06T22:04:17.997667: step 1546, loss 0.0754724, acc 0.98
2016-09-06T22:04:18.668614: step 1547, loss 0.0383711, acc 0.96
2016-09-06T22:04:19.375919: step 1548, loss 0.0479972, acc 0.98
2016-09-06T22:04:20.053386: step 1549, loss 0.0496158, acc 0.96
2016-09-06T22:04:20.721901: step 1550, loss 0.0932899, acc 0.96
2016-09-06T22:04:21.418962: step 1551, loss 0.0166391, acc 1
2016-09-06T22:04:22.112677: step 1552, loss 0.0162502, acc 1
2016-09-06T22:04:22.807939: step 1553, loss 0.16554, acc 0.94
2016-09-06T22:04:23.485306: step 1554, loss 0.0435817, acc 0.98
2016-09-06T22:04:24.198760: step 1555, loss 0.120158, acc 0.94
2016-09-06T22:04:24.879112: step 1556, loss 0.0661394, acc 0.98
2016-09-06T22:04:25.578717: step 1557, loss 0.120889, acc 0.96
2016-09-06T22:04:26.260313: step 1558, loss 0.0615527, acc 0.96
2016-09-06T22:04:26.945093: step 1559, loss 0.0642234, acc 0.98
2016-09-06T22:04:27.646114: step 1560, loss 0.0559842, acc 0.96
2016-09-06T22:04:28.374031: step 1561, loss 0.0282815, acc 1
2016-09-06T22:04:29.074748: step 1562, loss 0.0297605, acc 0.98
2016-09-06T22:04:29.769121: step 1563, loss 0.0131589, acc 1
2016-09-06T22:04:30.445573: step 1564, loss 0.139857, acc 0.94
2016-09-06T22:04:31.155023: step 1565, loss 0.0204308, acc 1
2016-09-06T22:04:31.837575: step 1566, loss 0.087586, acc 0.96
2016-09-06T22:04:32.531215: step 1567, loss 0.167653, acc 0.96
2016-09-06T22:04:33.190397: step 1568, loss 0.0621184, acc 0.94
2016-09-06T22:04:33.882171: step 1569, loss 0.0794431, acc 0.98
2016-09-06T22:04:34.534953: step 1570, loss 0.104383, acc 0.96
2016-09-06T22:04:35.207172: step 1571, loss 0.0815605, acc 0.98
2016-09-06T22:04:35.895605: step 1572, loss 0.064143, acc 0.98
2016-09-06T22:04:36.590718: step 1573, loss 0.0410417, acc 0.98
2016-09-06T22:04:37.284162: step 1574, loss 0.0644766, acc 0.96
2016-09-06T22:04:37.955528: step 1575, loss 0.0384645, acc 0.98
2016-09-06T22:04:38.653457: step 1576, loss 0.0806293, acc 0.98
2016-09-06T22:04:39.329571: step 1577, loss 0.166213, acc 0.9
2016-09-06T22:04:40.011349: step 1578, loss 0.0614055, acc 0.98
2016-09-06T22:04:40.686160: step 1579, loss 0.0664541, acc 0.96
2016-09-06T22:04:41.369223: step 1580, loss 0.0286279, acc 0.98
2016-09-06T22:04:42.056009: step 1581, loss 0.0253497, acc 1
2016-09-06T22:04:42.747359: step 1582, loss 0.115083, acc 0.96
2016-09-06T22:04:43.442549: step 1583, loss 0.030571, acc 0.98
2016-09-06T22:04:44.107106: step 1584, loss 0.0642207, acc 0.98
2016-09-06T22:04:44.801308: step 1585, loss 0.0252322, acc 1
2016-09-06T22:04:45.472157: step 1586, loss 0.0642682, acc 0.98
2016-09-06T22:04:46.140481: step 1587, loss 0.0491809, acc 0.96
2016-09-06T22:04:46.821821: step 1588, loss 0.0485573, acc 0.98
2016-09-06T22:04:47.520270: step 1589, loss 0.0451172, acc 0.98
2016-09-06T22:04:48.228740: step 1590, loss 0.0215494, acc 1
2016-09-06T22:04:48.907864: step 1591, loss 0.0381033, acc 0.98
2016-09-06T22:04:49.588407: step 1592, loss 0.0575683, acc 0.98
2016-09-06T22:04:50.272461: step 1593, loss 0.0561349, acc 0.98
2016-09-06T22:04:50.970484: step 1594, loss 0.150215, acc 0.96
2016-09-06T22:04:51.654476: step 1595, loss 0.0638324, acc 0.96
2016-09-06T22:04:52.329203: step 1596, loss 0.104255, acc 0.94
2016-09-06T22:04:53.060875: step 1597, loss 0.0377159, acc 1
2016-09-06T22:04:53.741855: step 1598, loss 0.0651499, acc 0.98
2016-09-06T22:04:54.422263: step 1599, loss 0.0485678, acc 0.98
2016-09-06T22:04:55.121668: step 1600, loss 0.0307271, acc 0.98

Evaluation:
2016-09-06T22:04:58.265447: step 1600, loss 1.15063, acc 0.773921

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-1600

2016-09-06T22:04:59.950742: step 1601, loss 0.0942124, acc 0.96
2016-09-06T22:05:00.673354: step 1602, loss 0.0817868, acc 0.98
2016-09-06T22:05:01.355813: step 1603, loss 0.0330998, acc 0.98
2016-09-06T22:05:02.016216: step 1604, loss 0.0451352, acc 0.98
2016-09-06T22:05:02.715600: step 1605, loss 0.053697, acc 0.98
2016-09-06T22:05:03.400849: step 1606, loss 0.0344941, acc 1
2016-09-06T22:05:04.098032: step 1607, loss 0.0451241, acc 0.98
2016-09-06T22:05:04.792487: step 1608, loss 0.0458425, acc 0.98
2016-09-06T22:05:05.489062: step 1609, loss 0.0196375, acc 1
2016-09-06T22:05:06.195374: step 1610, loss 0.116356, acc 0.92
2016-09-06T22:05:06.877321: step 1611, loss 0.022538, acc 1
2016-09-06T22:05:07.584239: step 1612, loss 0.103369, acc 0.94
2016-09-06T22:05:08.263551: step 1613, loss 0.0422807, acc 1
2016-09-06T22:05:08.958734: step 1614, loss 0.125133, acc 0.94
2016-09-06T22:05:09.657346: step 1615, loss 0.0100891, acc 1
2016-09-06T22:05:10.338857: step 1616, loss 0.149529, acc 0.96
2016-09-06T22:05:11.024393: step 1617, loss 0.028974, acc 0.98
2016-09-06T22:05:11.689199: step 1618, loss 0.0285319, acc 0.98
2016-09-06T22:05:12.385341: step 1619, loss 0.107522, acc 0.96
2016-09-06T22:05:13.062311: step 1620, loss 0.0056083, acc 1
2016-09-06T22:05:13.753479: step 1621, loss 0.102258, acc 0.92
2016-09-06T22:05:14.443827: step 1622, loss 0.107143, acc 0.94
2016-09-06T22:05:15.128802: step 1623, loss 0.0283212, acc 0.98
2016-09-06T22:05:15.828736: step 1624, loss 0.234006, acc 0.94
2016-09-06T22:05:16.503586: step 1625, loss 0.0716984, acc 0.96
2016-09-06T22:05:17.226082: step 1626, loss 0.0815616, acc 0.92
2016-09-06T22:05:17.924325: step 1627, loss 0.0365878, acc 0.98
2016-09-06T22:05:18.611206: step 1628, loss 0.0675387, acc 0.96
2016-09-06T22:05:19.300482: step 1629, loss 0.00642198, acc 1
2016-09-06T22:05:19.985107: step 1630, loss 0.0858833, acc 0.98
2016-09-06T22:05:20.668158: step 1631, loss 0.124358, acc 0.94
2016-09-06T22:05:21.330128: step 1632, loss 0.0420975, acc 0.96
2016-09-06T22:05:22.015147: step 1633, loss 0.0436637, acc 0.98
2016-09-06T22:05:22.716285: step 1634, loss 0.0162397, acc 1
2016-09-06T22:05:23.407767: step 1635, loss 0.0562021, acc 0.94
2016-09-06T22:05:24.071833: step 1636, loss 0.0809085, acc 0.96
2016-09-06T22:05:24.780515: step 1637, loss 0.106715, acc 0.96
2016-09-06T22:05:25.474360: step 1638, loss 0.0611097, acc 0.98
2016-09-06T22:05:26.123730: step 1639, loss 0.0273837, acc 0.98
2016-09-06T22:05:26.827383: step 1640, loss 0.080523, acc 0.96
2016-09-06T22:05:27.523164: step 1641, loss 0.0768996, acc 0.96
2016-09-06T22:05:28.208991: step 1642, loss 0.00758139, acc 1
2016-09-06T22:05:28.911851: step 1643, loss 0.0346651, acc 1
2016-09-06T22:05:29.586589: step 1644, loss 0.0990965, acc 0.94
2016-09-06T22:05:30.284227: step 1645, loss 0.0564775, acc 0.98
2016-09-06T22:05:30.949448: step 1646, loss 0.19033, acc 0.94
2016-09-06T22:05:31.628053: step 1647, loss 0.0991449, acc 0.94
2016-09-06T22:05:32.321324: step 1648, loss 0.1254, acc 0.94
2016-09-06T22:05:33.017665: step 1649, loss 0.105731, acc 0.94
2016-09-06T22:05:33.706621: step 1650, loss 0.10581, acc 0.98
2016-09-06T22:05:34.400550: step 1651, loss 0.018606, acc 1
2016-09-06T22:05:35.112562: step 1652, loss 0.0342779, acc 0.98
2016-09-06T22:05:35.795251: step 1653, loss 0.263156, acc 0.96
2016-09-06T22:05:36.476886: step 1654, loss 0.0162737, acc 1
2016-09-06T22:05:37.154649: step 1655, loss 0.105479, acc 0.98
2016-09-06T22:05:37.850341: step 1656, loss 0.0473436, acc 0.98
2016-09-06T22:05:38.533298: step 1657, loss 0.0637279, acc 0.98
2016-09-06T22:05:39.219537: step 1658, loss 0.0499563, acc 0.98
2016-09-06T22:05:39.940379: step 1659, loss 0.0527555, acc 0.98
2016-09-06T22:05:40.610849: step 1660, loss 0.0637493, acc 0.96
2016-09-06T22:05:41.294012: step 1661, loss 0.0721843, acc 0.96
2016-09-06T22:05:41.974923: step 1662, loss 0.218206, acc 0.9
2016-09-06T22:05:42.648940: step 1663, loss 0.0140516, acc 1
2016-09-06T22:05:43.325931: step 1664, loss 0.0962606, acc 0.94
2016-09-06T22:05:44.006226: step 1665, loss 0.0735574, acc 0.96
2016-09-06T22:05:44.716417: step 1666, loss 0.0421951, acc 0.98
2016-09-06T22:05:45.387747: step 1667, loss 0.194376, acc 0.9
2016-09-06T22:05:46.060965: step 1668, loss 0.0580953, acc 1
2016-09-06T22:05:46.744980: step 1669, loss 0.0786218, acc 0.94
2016-09-06T22:05:47.423021: step 1670, loss 0.0710647, acc 0.94
2016-09-06T22:05:48.110673: step 1671, loss 0.097017, acc 0.98
2016-09-06T22:05:48.845576: step 1672, loss 0.150744, acc 0.96
2016-09-06T22:05:49.555339: step 1673, loss 0.0889511, acc 0.98
2016-09-06T22:05:50.225213: step 1674, loss 0.0796685, acc 0.96
2016-09-06T22:05:50.911053: step 1675, loss 0.0332431, acc 1
2016-09-06T22:05:51.614944: step 1676, loss 0.201367, acc 0.96
2016-09-06T22:05:52.289444: step 1677, loss 0.0820476, acc 0.96
2016-09-06T22:05:52.968412: step 1678, loss 0.0280826, acc 1
2016-09-06T22:05:53.647234: step 1679, loss 0.07534, acc 0.96
2016-09-06T22:05:54.340297: step 1680, loss 0.0971163, acc 0.92
2016-09-06T22:05:55.009415: step 1681, loss 0.0223668, acc 1
2016-09-06T22:05:55.694620: step 1682, loss 0.040262, acc 1
2016-09-06T22:05:56.375897: step 1683, loss 0.0306092, acc 0.98
2016-09-06T22:05:57.067748: step 1684, loss 0.0852662, acc 0.96
2016-09-06T22:05:57.762013: step 1685, loss 0.0876324, acc 0.96
2016-09-06T22:05:58.433372: step 1686, loss 0.0513591, acc 0.98
2016-09-06T22:05:59.123839: step 1687, loss 0.0605603, acc 0.96
2016-09-06T22:05:59.811571: step 1688, loss 0.0429683, acc 0.98
2016-09-06T22:06:00.521235: step 1689, loss 0.0371253, acc 1
2016-09-06T22:06:01.212546: step 1690, loss 0.0354937, acc 0.98
2016-09-06T22:06:01.893411: step 1691, loss 0.0718172, acc 0.96
2016-09-06T22:06:02.594128: step 1692, loss 0.0159287, acc 1
2016-09-06T22:06:03.282728: step 1693, loss 0.0187923, acc 1
2016-09-06T22:06:03.981645: step 1694, loss 0.0821844, acc 0.96
2016-09-06T22:06:04.632615: step 1695, loss 0.0183496, acc 1
2016-09-06T22:06:05.339622: step 1696, loss 0.14577, acc 0.96
2016-09-06T22:06:06.020348: step 1697, loss 0.00508755, acc 1
2016-09-06T22:06:06.695413: step 1698, loss 0.114357, acc 0.98
2016-09-06T22:06:07.397682: step 1699, loss 0.0747986, acc 0.96
2016-09-06T22:06:08.102135: step 1700, loss 0.124048, acc 0.96

Evaluation:
2016-09-06T22:06:11.242862: step 1700, loss 1.42991, acc 0.757974

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-1700

2016-09-06T22:06:12.993521: step 1701, loss 0.245855, acc 0.9
2016-09-06T22:06:13.688774: step 1702, loss 0.143932, acc 0.92
2016-09-06T22:06:14.360923: step 1703, loss 0.164639, acc 0.92
2016-09-06T22:06:15.051973: step 1704, loss 0.0648904, acc 0.98
2016-09-06T22:06:15.747978: step 1705, loss 0.0430919, acc 0.98
2016-09-06T22:06:16.446208: step 1706, loss 0.225176, acc 0.92
2016-09-06T22:06:17.117145: step 1707, loss 0.0530582, acc 0.96
2016-09-06T22:06:17.769529: step 1708, loss 0.0871892, acc 0.98
2016-09-06T22:06:18.453692: step 1709, loss 0.0962464, acc 0.94
2016-09-06T22:06:19.099727: step 1710, loss 0.111023, acc 0.92
2016-09-06T22:06:19.801479: step 1711, loss 0.171284, acc 0.96
2016-09-06T22:06:20.482352: step 1712, loss 0.0661352, acc 0.98
2016-09-06T22:06:21.152722: step 1713, loss 0.0665497, acc 0.98
2016-09-06T22:06:21.823294: step 1714, loss 0.0686697, acc 1
2016-09-06T22:06:22.503580: step 1715, loss 0.0449133, acc 1
2016-09-06T22:06:23.200240: step 1716, loss 0.0744824, acc 0.98
2016-09-06T22:06:23.874360: step 1717, loss 0.0303888, acc 1
2016-09-06T22:06:24.578960: step 1718, loss 0.0279066, acc 1
2016-09-06T22:06:25.271158: step 1719, loss 0.0278022, acc 1
2016-09-06T22:06:25.959715: step 1720, loss 0.107792, acc 0.92
2016-09-06T22:06:26.675696: step 1721, loss 0.0180304, acc 1
2016-09-06T22:06:27.369962: step 1722, loss 0.0846182, acc 0.96
2016-09-06T22:06:28.072473: step 1723, loss 0.0892845, acc 0.96
2016-09-06T22:06:28.749253: step 1724, loss 0.0748797, acc 0.96
2016-09-06T22:06:29.454368: step 1725, loss 0.0546713, acc 0.98
2016-09-06T22:06:30.140726: step 1726, loss 0.105424, acc 0.96
2016-09-06T22:06:30.837367: step 1727, loss 0.116784, acc 0.94
2016-09-06T22:06:31.489977: step 1728, loss 0.0580324, acc 0.977273
2016-09-06T22:06:32.178016: step 1729, loss 0.0820468, acc 0.98
2016-09-06T22:06:32.889519: step 1730, loss 0.0553782, acc 0.98
2016-09-06T22:06:33.571711: step 1731, loss 0.0869728, acc 0.96
2016-09-06T22:06:34.251682: step 1732, loss 0.0220324, acc 1
2016-09-06T22:06:34.921246: step 1733, loss 0.0987618, acc 0.96
2016-09-06T22:06:35.616553: step 1734, loss 0.0489162, acc 0.96
2016-09-06T22:06:36.292317: step 1735, loss 0.0220643, acc 1
2016-09-06T22:06:36.965688: step 1736, loss 0.0356557, acc 1
2016-09-06T22:06:37.673689: step 1737, loss 0.033184, acc 0.98
2016-09-06T22:06:38.360834: step 1738, loss 0.0419342, acc 0.98
2016-09-06T22:06:39.046740: step 1739, loss 0.0579131, acc 0.96
2016-09-06T22:06:39.733862: step 1740, loss 0.102642, acc 0.98
2016-09-06T22:06:40.437041: step 1741, loss 0.0726485, acc 0.98
2016-09-06T22:06:41.137127: step 1742, loss 0.0617528, acc 0.96
2016-09-06T22:06:41.813869: step 1743, loss 0.0267119, acc 1
2016-09-06T22:06:42.549053: step 1744, loss 0.0322867, acc 0.98
2016-09-06T22:06:43.243832: step 1745, loss 0.0285435, acc 1
2016-09-06T22:06:43.929788: step 1746, loss 0.00638374, acc 1
2016-09-06T22:06:44.609577: step 1747, loss 0.045221, acc 0.98
2016-09-06T22:06:45.294517: step 1748, loss 0.0285779, acc 0.98
2016-09-06T22:06:45.996522: step 1749, loss 0.0606723, acc 0.94
2016-09-06T22:06:46.651391: step 1750, loss 0.0153219, acc 1
2016-09-06T22:06:47.370413: step 1751, loss 0.00966344, acc 1
2016-09-06T22:06:48.043194: step 1752, loss 0.0964901, acc 0.96
2016-09-06T22:06:48.723356: step 1753, loss 0.0746759, acc 0.96
2016-09-06T22:06:49.413198: step 1754, loss 0.00850618, acc 1
2016-09-06T22:06:50.108558: step 1755, loss 0.0243328, acc 0.98
2016-09-06T22:06:50.800094: step 1756, loss 0.0029233, acc 1
2016-09-06T22:06:51.453910: step 1757, loss 0.0424116, acc 0.96
2016-09-06T22:06:52.158372: step 1758, loss 0.0521518, acc 0.98
2016-09-06T22:06:52.823074: step 1759, loss 0.0551298, acc 0.96
2016-09-06T22:06:53.476626: step 1760, loss 0.0846916, acc 0.98
2016-09-06T22:06:54.167703: step 1761, loss 0.0285725, acc 1
2016-09-06T22:06:54.852890: step 1762, loss 0.0674469, acc 0.96
2016-09-06T22:06:55.536287: step 1763, loss 0.0441707, acc 0.98
2016-09-06T22:06:56.222782: step 1764, loss 0.0201325, acc 1
2016-09-06T22:06:56.936106: step 1765, loss 0.224582, acc 0.96
2016-09-06T22:06:57.601755: step 1766, loss 0.0331517, acc 1
2016-09-06T22:06:58.306365: step 1767, loss 0.0132229, acc 1
2016-09-06T22:06:58.989720: step 1768, loss 0.047498, acc 0.98
2016-09-06T22:06:59.667374: step 1769, loss 0.0856942, acc 0.96
2016-09-06T22:07:00.406579: step 1770, loss 0.0721466, acc 0.96
2016-09-06T22:07:01.059638: step 1771, loss 0.101354, acc 0.94
2016-09-06T22:07:01.760312: step 1772, loss 0.0381004, acc 0.98
2016-09-06T22:07:02.425254: step 1773, loss 0.0836639, acc 0.96
2016-09-06T22:07:03.115350: step 1774, loss 0.098693, acc 0.94
2016-09-06T22:07:03.797253: step 1775, loss 0.0291786, acc 0.98
2016-09-06T22:07:04.479329: step 1776, loss 0.0510642, acc 0.98
2016-09-06T22:07:05.162684: step 1777, loss 0.052191, acc 0.98
2016-09-06T22:07:05.850400: step 1778, loss 0.04611, acc 0.98
2016-09-06T22:07:06.559290: step 1779, loss 0.0509392, acc 0.96
2016-09-06T22:07:07.237633: step 1780, loss 0.107099, acc 0.96
2016-09-06T22:07:07.905779: step 1781, loss 0.0240591, acc 0.98
2016-09-06T22:07:08.579394: step 1782, loss 0.0395765, acc 0.98
2016-09-06T22:07:09.251317: step 1783, loss 0.0302343, acc 0.98
2016-09-06T22:07:09.933469: step 1784, loss 0.0406201, acc 0.96
2016-09-06T22:07:10.625762: step 1785, loss 0.0661226, acc 0.98
2016-09-06T22:07:11.348340: step 1786, loss 0.00518417, acc 1
2016-09-06T22:07:12.038720: step 1787, loss 0.0353274, acc 0.98
2016-09-06T22:07:12.732936: step 1788, loss 0.0150598, acc 1
2016-09-06T22:07:13.406067: step 1789, loss 0.0437718, acc 0.98
2016-09-06T22:07:14.094566: step 1790, loss 0.0238835, acc 0.98
2016-09-06T22:07:14.784200: step 1791, loss 0.0959468, acc 0.98
2016-09-06T22:07:15.492254: step 1792, loss 0.109982, acc 0.96
2016-09-06T22:07:16.194169: step 1793, loss 0.0296623, acc 1
2016-09-06T22:07:16.884250: step 1794, loss 0.0551446, acc 1
2016-09-06T22:07:17.575144: step 1795, loss 0.0162526, acc 1
2016-09-06T22:07:18.265536: step 1796, loss 0.0967401, acc 0.96
2016-09-06T22:07:18.927163: step 1797, loss 0.0625065, acc 0.94
2016-09-06T22:07:19.613383: step 1798, loss 0.0222356, acc 0.98
2016-09-06T22:07:20.278895: step 1799, loss 0.127943, acc 0.94
2016-09-06T22:07:20.981742: step 1800, loss 0.0598268, acc 0.98

Evaluation:
2016-09-06T22:07:24.135065: step 1800, loss 1.42312, acc 0.760788

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-1800

2016-09-06T22:07:25.770300: step 1801, loss 0.0700673, acc 0.96
2016-09-06T22:07:26.418008: step 1802, loss 0.0882479, acc 0.98
2016-09-06T22:07:27.122141: step 1803, loss 0.0111649, acc 1
2016-09-06T22:07:27.812301: step 1804, loss 0.0272054, acc 0.98
2016-09-06T22:07:28.509714: step 1805, loss 0.0409323, acc 0.98
2016-09-06T22:07:29.203185: step 1806, loss 0.0741927, acc 0.94
2016-09-06T22:07:29.889424: step 1807, loss 0.0269794, acc 1
2016-09-06T22:07:30.578814: step 1808, loss 0.0498909, acc 0.96
2016-09-06T22:07:31.234713: step 1809, loss 0.0341838, acc 0.98
2016-09-06T22:07:31.917973: step 1810, loss 0.148368, acc 0.94
2016-09-06T22:07:32.620464: step 1811, loss 0.213211, acc 0.9
2016-09-06T22:07:33.313266: step 1812, loss 0.0202861, acc 1
2016-09-06T22:07:34.006584: step 1813, loss 0.0521, acc 0.98
2016-09-06T22:07:34.699608: step 1814, loss 0.0360118, acc 0.98
2016-09-06T22:07:35.409758: step 1815, loss 0.0617586, acc 0.96
2016-09-06T22:07:36.094418: step 1816, loss 0.0609638, acc 0.98
2016-09-06T22:07:36.794985: step 1817, loss 0.170164, acc 0.94
2016-09-06T22:07:37.508307: step 1818, loss 0.0538938, acc 1
2016-09-06T22:07:38.200433: step 1819, loss 0.0216972, acc 0.98
2016-09-06T22:07:38.887906: step 1820, loss 0.0209662, acc 1
2016-09-06T22:07:39.559064: step 1821, loss 0.0381073, acc 0.98
2016-09-06T22:07:40.269695: step 1822, loss 0.0453792, acc 0.96
2016-09-06T22:07:40.957391: step 1823, loss 0.0917657, acc 0.96
2016-09-06T22:07:41.629545: step 1824, loss 0.0479634, acc 0.98
2016-09-06T22:07:42.321330: step 1825, loss 0.140878, acc 0.92
2016-09-06T22:07:43.025525: step 1826, loss 0.0770984, acc 0.92
2016-09-06T22:07:43.712030: step 1827, loss 0.0644173, acc 0.98
2016-09-06T22:07:44.376983: step 1828, loss 0.0751282, acc 0.98
2016-09-06T22:07:45.082769: step 1829, loss 0.0915829, acc 0.94
2016-09-06T22:07:45.790383: step 1830, loss 0.0257752, acc 0.98
2016-09-06T22:07:46.472912: step 1831, loss 0.0980133, acc 0.96
2016-09-06T22:07:47.158510: step 1832, loss 0.0228797, acc 1
2016-09-06T22:07:47.843391: step 1833, loss 0.0594049, acc 0.96
2016-09-06T22:07:48.532776: step 1834, loss 0.105707, acc 0.96
2016-09-06T22:07:49.189513: step 1835, loss 0.0620979, acc 0.98
2016-09-06T22:07:49.903375: step 1836, loss 0.0308898, acc 0.98
2016-09-06T22:07:50.595540: step 1837, loss 0.0737705, acc 0.94
2016-09-06T22:07:51.271403: step 1838, loss 0.0794763, acc 0.96
2016-09-06T22:07:51.965897: step 1839, loss 0.0368797, acc 0.98
2016-09-06T22:07:52.642603: step 1840, loss 0.0426051, acc 0.98
2016-09-06T22:07:53.332606: step 1841, loss 0.0673223, acc 0.94
2016-09-06T22:07:53.991950: step 1842, loss 0.0686331, acc 0.98
2016-09-06T22:07:54.688808: step 1843, loss 0.0279841, acc 0.98
2016-09-06T22:07:55.360676: step 1844, loss 0.0161658, acc 1
2016-09-06T22:07:56.056824: step 1845, loss 0.0782378, acc 0.94
2016-09-06T22:07:56.756832: step 1846, loss 0.128123, acc 0.94
2016-09-06T22:07:57.444533: step 1847, loss 0.0460876, acc 0.98
2016-09-06T22:07:58.160310: step 1848, loss 0.0208489, acc 1
2016-09-06T22:07:58.829689: step 1849, loss 0.0276491, acc 1
2016-09-06T22:07:59.530415: step 1850, loss 0.0988384, acc 0.94
2016-09-06T22:08:00.239942: step 1851, loss 0.0366537, acc 0.98
2016-09-06T22:08:00.914948: step 1852, loss 0.0297248, acc 1
2016-09-06T22:08:01.624428: step 1853, loss 0.188716, acc 0.92
2016-09-06T22:08:02.307581: step 1854, loss 0.0105062, acc 1
2016-09-06T22:08:03.008098: step 1855, loss 0.0299355, acc 1
2016-09-06T22:08:03.669684: step 1856, loss 0.0352408, acc 0.96
2016-09-06T22:08:04.382968: step 1857, loss 0.0412551, acc 0.98
2016-09-06T22:08:05.073562: step 1858, loss 0.217562, acc 0.94
2016-09-06T22:08:05.750143: step 1859, loss 0.0893841, acc 0.96
2016-09-06T22:08:06.438166: step 1860, loss 0.0648606, acc 0.98
2016-09-06T22:08:07.124332: step 1861, loss 0.0391429, acc 0.98
2016-09-06T22:08:07.830387: step 1862, loss 0.0335325, acc 1
2016-09-06T22:08:08.506356: step 1863, loss 0.101657, acc 0.98
2016-09-06T22:08:09.187865: step 1864, loss 0.0467592, acc 0.98
2016-09-06T22:08:09.894349: step 1865, loss 0.0249394, acc 1
2016-09-06T22:08:10.565683: step 1866, loss 0.0124752, acc 1
2016-09-06T22:08:11.257499: step 1867, loss 0.0435145, acc 0.98
2016-09-06T22:08:11.938345: step 1868, loss 0.0691617, acc 0.96
2016-09-06T22:08:12.635481: step 1869, loss 0.0907195, acc 0.98
2016-09-06T22:08:13.290127: step 1870, loss 0.0461808, acc 0.96
2016-09-06T22:08:13.987978: step 1871, loss 0.0635985, acc 0.96
2016-09-06T22:08:14.677000: step 1872, loss 0.0309236, acc 1
2016-09-06T22:08:15.394152: step 1873, loss 0.086077, acc 0.98
2016-09-06T22:08:16.081984: step 1874, loss 0.0482608, acc 0.98
2016-09-06T22:08:16.772881: step 1875, loss 0.0346212, acc 0.98
2016-09-06T22:08:17.476499: step 1876, loss 0.127433, acc 0.96
2016-09-06T22:08:18.138620: step 1877, loss 0.0832948, acc 0.96
2016-09-06T22:08:18.818303: step 1878, loss 0.0551422, acc 0.96
2016-09-06T22:08:19.532443: step 1879, loss 0.0482978, acc 0.98
2016-09-06T22:08:20.234929: step 1880, loss 0.0284582, acc 1
2016-09-06T22:08:20.939045: step 1881, loss 0.0191371, acc 0.98
2016-09-06T22:08:21.615538: step 1882, loss 0.106543, acc 0.94
2016-09-06T22:08:22.332833: step 1883, loss 0.0149819, acc 1
2016-09-06T22:08:23.006082: step 1884, loss 0.0283901, acc 0.98
2016-09-06T22:08:23.686187: step 1885, loss 0.085432, acc 0.98
2016-09-06T22:08:24.367007: step 1886, loss 0.119342, acc 0.92
2016-09-06T22:08:25.045092: step 1887, loss 0.0272243, acc 0.98
2016-09-06T22:08:25.732937: step 1888, loss 0.103834, acc 0.96
2016-09-06T22:08:26.426991: step 1889, loss 0.0524122, acc 0.96
2016-09-06T22:08:27.141317: step 1890, loss 0.0147824, acc 1
2016-09-06T22:08:27.834226: step 1891, loss 0.0201816, acc 1
2016-09-06T22:08:28.514239: step 1892, loss 0.00743853, acc 1
2016-09-06T22:08:29.216195: step 1893, loss 0.0724652, acc 0.96
2016-09-06T22:08:29.909712: step 1894, loss 0.057049, acc 0.98
2016-09-06T22:08:30.612276: step 1895, loss 0.0209811, acc 0.98
2016-09-06T22:08:31.282409: step 1896, loss 0.0537067, acc 0.98
2016-09-06T22:08:31.984615: step 1897, loss 0.0894269, acc 0.96
2016-09-06T22:08:32.668512: step 1898, loss 0.0575233, acc 0.98
2016-09-06T22:08:33.348211: step 1899, loss 0.0830591, acc 0.98
2016-09-06T22:08:34.026001: step 1900, loss 0.0201001, acc 0.98

Evaluation:
2016-09-06T22:08:37.141822: step 1900, loss 1.30348, acc 0.760788

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-1900

2016-09-06T22:08:38.847694: step 1901, loss 0.0990078, acc 0.98
2016-09-06T22:08:39.534622: step 1902, loss 0.0727997, acc 0.96
2016-09-06T22:08:40.238815: step 1903, loss 0.0336789, acc 0.98
2016-09-06T22:08:40.911138: step 1904, loss 0.0393117, acc 0.98
2016-09-06T22:08:41.606988: step 1905, loss 0.130764, acc 0.94
2016-09-06T22:08:42.275057: step 1906, loss 0.16166, acc 0.96
2016-09-06T22:08:42.954366: step 1907, loss 0.034856, acc 0.98
2016-09-06T22:08:43.651249: step 1908, loss 0.035471, acc 1
2016-09-06T22:08:44.317493: step 1909, loss 0.0474813, acc 0.98
2016-09-06T22:08:45.025238: step 1910, loss 0.00451543, acc 1
2016-09-06T22:08:45.710479: step 1911, loss 0.0189353, acc 0.98
2016-09-06T22:08:46.414296: step 1912, loss 0.052803, acc 0.98
2016-09-06T22:08:47.078601: step 1913, loss 0.0407217, acc 0.98
2016-09-06T22:08:47.760244: step 1914, loss 0.0203448, acc 0.98
2016-09-06T22:08:48.438465: step 1915, loss 0.0408753, acc 0.96
2016-09-06T22:08:49.119140: step 1916, loss 0.0423215, acc 0.98
2016-09-06T22:08:49.800951: step 1917, loss 0.0190342, acc 1
2016-09-06T22:08:50.468710: step 1918, loss 0.018537, acc 1
2016-09-06T22:08:51.165219: step 1919, loss 0.0268547, acc 1
2016-09-06T22:08:51.809071: step 1920, loss 0.0114369, acc 1
2016-09-06T22:08:52.485660: step 1921, loss 0.0740239, acc 0.98
2016-09-06T22:08:53.146625: step 1922, loss 0.0479188, acc 0.98
2016-09-06T22:08:53.830809: step 1923, loss 0.0557549, acc 1
2016-09-06T22:08:54.521728: step 1924, loss 0.0366316, acc 0.98
2016-09-06T22:08:55.202747: step 1925, loss 0.0823401, acc 0.98
2016-09-06T22:08:55.888774: step 1926, loss 0.0484085, acc 0.96
2016-09-06T22:08:56.545226: step 1927, loss 0.00713981, acc 1
2016-09-06T22:08:57.251467: step 1928, loss 0.0685986, acc 0.96
2016-09-06T22:08:57.943211: step 1929, loss 0.0276405, acc 0.98
2016-09-06T22:08:58.619770: step 1930, loss 0.0311309, acc 0.98
2016-09-06T22:08:59.316184: step 1931, loss 0.0165302, acc 0.98
2016-09-06T22:08:59.986886: step 1932, loss 0.0215883, acc 1
2016-09-06T22:09:00.709403: step 1933, loss 0.0911015, acc 0.94
2016-09-06T22:09:01.379231: step 1934, loss 0.13979, acc 0.96
2016-09-06T22:09:02.062691: step 1935, loss 0.0731455, acc 0.98
2016-09-06T22:09:02.724577: step 1936, loss 0.0379631, acc 1
2016-09-06T22:09:03.429054: step 1937, loss 0.0667582, acc 0.96
2016-09-06T22:09:04.118375: step 1938, loss 0.0477648, acc 0.98
2016-09-06T22:09:04.802500: step 1939, loss 0.0666309, acc 0.98
2016-09-06T22:09:05.481773: step 1940, loss 0.0463595, acc 0.96
2016-09-06T22:09:06.179070: step 1941, loss 0.031331, acc 0.98
2016-09-06T22:09:06.897758: step 1942, loss 0.0466963, acc 0.98
2016-09-06T22:09:07.610206: step 1943, loss 0.00796998, acc 1
2016-09-06T22:09:08.304567: step 1944, loss 0.0356733, acc 0.98
2016-09-06T22:09:08.997903: step 1945, loss 0.0616465, acc 0.96
2016-09-06T22:09:09.686017: step 1946, loss 0.0234118, acc 1
2016-09-06T22:09:10.365103: step 1947, loss 0.0427555, acc 1
2016-09-06T22:09:11.026719: step 1948, loss 0.0759426, acc 0.98
2016-09-06T22:09:11.722007: step 1949, loss 0.0669212, acc 0.96
2016-09-06T22:09:12.404726: step 1950, loss 0.0238557, acc 0.98
2016-09-06T22:09:13.093528: step 1951, loss 0.0843081, acc 0.96
2016-09-06T22:09:13.778788: step 1952, loss 0.116333, acc 0.98
2016-09-06T22:09:14.453882: step 1953, loss 0.0538635, acc 0.98
2016-09-06T22:09:15.148836: step 1954, loss 0.0425318, acc 0.98
2016-09-06T22:09:15.826891: step 1955, loss 0.0154896, acc 1
2016-09-06T22:09:16.526196: step 1956, loss 0.0750403, acc 0.94
2016-09-06T22:09:17.236826: step 1957, loss 0.0415472, acc 0.98
2016-09-06T22:09:17.936670: step 1958, loss 0.0245414, acc 0.98
2016-09-06T22:09:18.631129: step 1959, loss 0.0550956, acc 0.98
2016-09-06T22:09:19.296967: step 1960, loss 0.00242316, acc 1
2016-09-06T22:09:20.014070: step 1961, loss 0.0685562, acc 0.96
2016-09-06T22:09:20.690489: step 1962, loss 0.057227, acc 0.94
2016-09-06T22:09:21.414361: step 1963, loss 0.00986071, acc 1
2016-09-06T22:09:22.098240: step 1964, loss 0.0218567, acc 0.98
2016-09-06T22:09:22.790900: step 1965, loss 0.0229422, acc 1
2016-09-06T22:09:23.474897: step 1966, loss 0.00518342, acc 1
2016-09-06T22:09:24.154990: step 1967, loss 0.0208142, acc 1
2016-09-06T22:09:24.846074: step 1968, loss 0.0250329, acc 0.98
2016-09-06T22:09:25.521403: step 1969, loss 0.027635, acc 0.98
2016-09-06T22:09:26.221941: step 1970, loss 0.109793, acc 0.94
2016-09-06T22:09:26.907145: step 1971, loss 0.0801165, acc 0.94
2016-09-06T22:09:27.584767: step 1972, loss 0.0449237, acc 0.98
2016-09-06T22:09:28.268945: step 1973, loss 0.0232675, acc 0.98
2016-09-06T22:09:28.967196: step 1974, loss 0.0083345, acc 1
2016-09-06T22:09:29.694527: step 1975, loss 0.0535798, acc 0.96
2016-09-06T22:09:30.373004: step 1976, loss 0.0137804, acc 1
2016-09-06T22:09:31.097466: step 1977, loss 0.0137107, acc 1
2016-09-06T22:09:31.793759: step 1978, loss 0.00146358, acc 1
2016-09-06T22:09:32.489823: step 1979, loss 0.0975761, acc 0.98
2016-09-06T22:09:33.208233: step 1980, loss 0.0439492, acc 0.98
2016-09-06T22:09:33.883598: step 1981, loss 0.198884, acc 0.92
2016-09-06T22:09:34.585937: step 1982, loss 0.0644188, acc 0.96
2016-09-06T22:09:35.256041: step 1983, loss 0.0207596, acc 1
2016-09-06T22:09:35.938065: step 1984, loss 0.0309269, acc 0.98
2016-09-06T22:09:36.621976: step 1985, loss 0.024047, acc 1
2016-09-06T22:09:37.304856: step 1986, loss 0.0414083, acc 1
2016-09-06T22:09:37.993900: step 1987, loss 0.0177997, acc 1
2016-09-06T22:09:38.655322: step 1988, loss 0.035165, acc 1
2016-09-06T22:09:39.362222: step 1989, loss 0.0372197, acc 0.98
2016-09-06T22:09:40.040748: step 1990, loss 0.0242952, acc 0.98
2016-09-06T22:09:40.729689: step 1991, loss 0.0678368, acc 0.98
2016-09-06T22:09:41.423035: step 1992, loss 0.00967337, acc 1
2016-09-06T22:09:42.115640: step 1993, loss 0.0558893, acc 0.98
2016-09-06T22:09:42.795580: step 1994, loss 0.0287941, acc 0.98
2016-09-06T22:09:43.455407: step 1995, loss 0.124807, acc 0.98
2016-09-06T22:09:44.173241: step 1996, loss 0.0238866, acc 1
2016-09-06T22:09:44.858527: step 1997, loss 0.110526, acc 0.98
2016-09-06T22:09:45.531703: step 1998, loss 0.0427806, acc 0.98
2016-09-06T22:09:46.237348: step 1999, loss 0.0789684, acc 0.94
2016-09-06T22:09:46.930622: step 2000, loss 0.368149, acc 0.9

Evaluation:
2016-09-06T22:09:50.086283: step 2000, loss 1.39812, acc 0.758912

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-2000

2016-09-06T22:09:51.820069: step 2001, loss 0.0374995, acc 0.98
2016-09-06T22:09:52.515871: step 2002, loss 0.0683935, acc 0.96
2016-09-06T22:09:53.189457: step 2003, loss 0.0660654, acc 0.98
2016-09-06T22:09:53.875917: step 2004, loss 0.0509037, acc 0.98
2016-09-06T22:09:54.561137: step 2005, loss 0.00134755, acc 1
2016-09-06T22:09:55.245387: step 2006, loss 0.0447562, acc 0.96
2016-09-06T22:09:55.938631: step 2007, loss 0.0234396, acc 1
2016-09-06T22:09:56.608626: step 2008, loss 0.037833, acc 1
2016-09-06T22:09:57.294171: step 2009, loss 0.0577883, acc 1
2016-09-06T22:09:57.971925: step 2010, loss 0.0448159, acc 0.96
2016-09-06T22:09:58.681558: step 2011, loss 0.142235, acc 0.96
2016-09-06T22:09:59.368645: step 2012, loss 0.0286553, acc 1
2016-09-06T22:10:00.041557: step 2013, loss 0.0314086, acc 0.98
2016-09-06T22:10:00.778520: step 2014, loss 0.0715519, acc 0.98
2016-09-06T22:10:01.470336: step 2015, loss 0.111587, acc 0.94
2016-09-06T22:10:02.176211: step 2016, loss 0.0980328, acc 0.96
2016-09-06T22:10:02.871954: step 2017, loss 0.0450648, acc 0.98
2016-09-06T22:10:03.553538: step 2018, loss 0.0251899, acc 1
2016-09-06T22:10:04.239705: step 2019, loss 0.0374457, acc 1
2016-09-06T22:10:04.923533: step 2020, loss 0.0515351, acc 0.94
2016-09-06T22:10:05.589674: step 2021, loss 0.103042, acc 0.98
2016-09-06T22:10:06.253347: step 2022, loss 0.0220049, acc 0.98
2016-09-06T22:10:06.964765: step 2023, loss 0.15539, acc 0.94
2016-09-06T22:10:07.635079: step 2024, loss 0.0540177, acc 0.98
2016-09-06T22:10:08.333476: step 2025, loss 0.0236187, acc 1
2016-09-06T22:10:09.032162: step 2026, loss 0.161876, acc 0.9
2016-09-06T22:10:09.709229: step 2027, loss 0.0267777, acc 0.98
2016-09-06T22:10:10.400405: step 2028, loss 0.0142698, acc 1
2016-09-06T22:10:11.080109: step 2029, loss 0.0568207, acc 0.96
2016-09-06T22:10:11.774424: step 2030, loss 0.0629243, acc 0.98
2016-09-06T22:10:12.460055: step 2031, loss 0.0186212, acc 1
2016-09-06T22:10:13.134692: step 2032, loss 0.0315146, acc 1
2016-09-06T22:10:13.813439: step 2033, loss 0.0454674, acc 1
2016-09-06T22:10:14.499847: step 2034, loss 0.0915824, acc 0.96
2016-09-06T22:10:15.176897: step 2035, loss 0.0305235, acc 0.98
2016-09-06T22:10:15.876981: step 2036, loss 0.0179766, acc 1
2016-09-06T22:10:16.622717: step 2037, loss 0.0141154, acc 1
2016-09-06T22:10:17.318343: step 2038, loss 0.0440004, acc 0.98
2016-09-06T22:10:18.016893: step 2039, loss 0.0284023, acc 0.98
2016-09-06T22:10:18.698427: step 2040, loss 0.0882238, acc 0.96
2016-09-06T22:10:19.412221: step 2041, loss 0.0705486, acc 0.96
2016-09-06T22:10:20.130383: step 2042, loss 0.0871964, acc 0.96
2016-09-06T22:10:20.807385: step 2043, loss 0.0430528, acc 0.98
2016-09-06T22:10:21.500433: step 2044, loss 0.027826, acc 0.98
2016-09-06T22:10:22.187144: step 2045, loss 0.0868269, acc 0.96
2016-09-06T22:10:22.872802: step 2046, loss 0.134543, acc 0.96
2016-09-06T22:10:23.569519: step 2047, loss 0.0606234, acc 0.98
2016-09-06T22:10:24.255173: step 2048, loss 0.18291, acc 0.96
2016-09-06T22:10:24.949649: step 2049, loss 0.0687901, acc 0.96
2016-09-06T22:10:25.616225: step 2050, loss 0.0306802, acc 0.98
2016-09-06T22:10:26.327357: step 2051, loss 0.155461, acc 0.9
2016-09-06T22:10:27.040678: step 2052, loss 0.0367279, acc 0.98
2016-09-06T22:10:27.732317: step 2053, loss 0.056905, acc 0.96
2016-09-06T22:10:28.423629: step 2054, loss 0.028592, acc 1
2016-09-06T22:10:29.096691: step 2055, loss 0.02032, acc 1
2016-09-06T22:10:29.793512: step 2056, loss 0.00518113, acc 1
2016-09-06T22:10:30.456630: step 2057, loss 0.0520574, acc 0.98
2016-09-06T22:10:31.151260: step 2058, loss 0.082893, acc 0.98
2016-09-06T22:10:31.820313: step 2059, loss 0.0665365, acc 0.98
2016-09-06T22:10:32.486559: step 2060, loss 0.0532668, acc 0.98
2016-09-06T22:10:33.187517: step 2061, loss 0.0572828, acc 0.98
2016-09-06T22:10:33.879749: step 2062, loss 0.0410209, acc 0.98
2016-09-06T22:10:34.594050: step 2063, loss 0.0153072, acc 1
2016-09-06T22:10:35.275012: step 2064, loss 0.0304978, acc 1
2016-09-06T22:10:35.956728: step 2065, loss 0.050405, acc 0.98
2016-09-06T22:10:36.657584: step 2066, loss 0.0335845, acc 0.98
2016-09-06T22:10:37.352953: step 2067, loss 0.0388671, acc 1
2016-09-06T22:10:38.044520: step 2068, loss 0.0642114, acc 0.98
2016-09-06T22:10:38.746268: step 2069, loss 0.0555709, acc 0.96
2016-09-06T22:10:39.465184: step 2070, loss 0.0388659, acc 0.96
2016-09-06T22:10:40.161458: step 2071, loss 0.0964457, acc 0.96
2016-09-06T22:10:40.831216: step 2072, loss 0.0406466, acc 0.98
2016-09-06T22:10:41.540760: step 2073, loss 0.066704, acc 0.98
2016-09-06T22:10:42.213955: step 2074, loss 0.0473989, acc 0.98
2016-09-06T22:10:42.893972: step 2075, loss 0.0635094, acc 0.98
2016-09-06T22:10:43.584493: step 2076, loss 0.033735, acc 0.98
2016-09-06T22:10:44.281721: step 2077, loss 0.0388649, acc 0.98
2016-09-06T22:10:44.977963: step 2078, loss 0.109776, acc 0.96
2016-09-06T22:10:45.669390: step 2079, loss 0.016282, acc 1
2016-09-06T22:10:46.370735: step 2080, loss 0.107307, acc 0.96
2016-09-06T22:10:47.063283: step 2081, loss 0.0120064, acc 1
2016-09-06T22:10:47.753144: step 2082, loss 0.0633426, acc 0.98
2016-09-06T22:10:48.430536: step 2083, loss 0.0424034, acc 0.98
2016-09-06T22:10:49.133585: step 2084, loss 0.0269104, acc 1
2016-09-06T22:10:49.807942: step 2085, loss 0.052733, acc 0.98
2016-09-06T22:10:50.491327: step 2086, loss 0.101894, acc 0.96
2016-09-06T22:10:51.182002: step 2087, loss 0.0454675, acc 0.98
2016-09-06T22:10:51.886593: step 2088, loss 0.0518169, acc 1
2016-09-06T22:10:52.577378: step 2089, loss 0.0318701, acc 1
2016-09-06T22:10:53.236697: step 2090, loss 0.0681155, acc 0.96
2016-09-06T22:10:53.938127: step 2091, loss 0.0507442, acc 0.98
2016-09-06T22:10:54.635744: step 2092, loss 0.0981228, acc 0.94
2016-09-06T22:10:55.320394: step 2093, loss 0.0675251, acc 0.98
2016-09-06T22:10:56.028205: step 2094, loss 0.0374132, acc 1
2016-09-06T22:10:56.712524: step 2095, loss 0.129481, acc 0.92
2016-09-06T22:10:57.408285: step 2096, loss 0.114311, acc 0.92
2016-09-06T22:10:58.082046: step 2097, loss 0.0580102, acc 0.96
2016-09-06T22:10:58.791697: step 2098, loss 0.0291062, acc 0.98
2016-09-06T22:10:59.495955: step 2099, loss 0.126605, acc 0.94
2016-09-06T22:11:00.164461: step 2100, loss 0.0424218, acc 0.98

Evaluation:
2016-09-06T22:11:03.346532: step 2100, loss 1.45357, acc 0.758912

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-2100

2016-09-06T22:11:05.018678: step 2101, loss 0.0381608, acc 0.98
2016-09-06T22:11:05.702943: step 2102, loss 0.011257, acc 1
2016-09-06T22:11:06.390686: step 2103, loss 0.0239551, acc 0.98
2016-09-06T22:11:07.095847: step 2104, loss 0.0256436, acc 0.98
2016-09-06T22:11:07.767422: step 2105, loss 0.19559, acc 0.94
2016-09-06T22:11:08.443859: step 2106, loss 0.0318907, acc 1
2016-09-06T22:11:09.115010: step 2107, loss 0.0677696, acc 0.98
2016-09-06T22:11:09.804765: step 2108, loss 0.0265177, acc 0.98
2016-09-06T22:11:10.490533: step 2109, loss 0.0483305, acc 0.98
2016-09-06T22:11:11.169267: step 2110, loss 0.0189019, acc 1
2016-09-06T22:11:11.877635: step 2111, loss 0.0298447, acc 0.98
2016-09-06T22:11:12.495917: step 2112, loss 0.0513894, acc 0.977273
2016-09-06T22:11:13.205750: step 2113, loss 0.0689584, acc 0.98
2016-09-06T22:11:13.868350: step 2114, loss 0.0541547, acc 0.98
2016-09-06T22:11:14.558898: step 2115, loss 0.0841169, acc 0.98
2016-09-06T22:11:15.260883: step 2116, loss 0.0421171, acc 0.98
2016-09-06T22:11:15.945220: step 2117, loss 0.0461988, acc 0.98
2016-09-06T22:11:16.639604: step 2118, loss 0.0843886, acc 0.96
2016-09-06T22:11:17.315358: step 2119, loss 0.0791613, acc 0.98
2016-09-06T22:11:18.022165: step 2120, loss 0.064424, acc 0.96
2016-09-06T22:11:18.696811: step 2121, loss 0.022601, acc 0.98
2016-09-06T22:11:19.374602: step 2122, loss 0.0329491, acc 1
2016-09-06T22:11:20.077912: step 2123, loss 0.0108538, acc 1
2016-09-06T22:11:20.762358: step 2124, loss 0.0458264, acc 0.98
2016-09-06T22:11:21.433488: step 2125, loss 0.0590908, acc 0.96
2016-09-06T22:11:22.098781: step 2126, loss 0.0486787, acc 0.98
2016-09-06T22:11:22.803665: step 2127, loss 0.0764659, acc 0.98
2016-09-06T22:11:23.498148: step 2128, loss 0.0122779, acc 1
2016-09-06T22:11:24.194372: step 2129, loss 0.0806802, acc 0.96
2016-09-06T22:11:24.890241: step 2130, loss 0.0693089, acc 0.96
2016-09-06T22:11:25.590461: step 2131, loss 0.0892923, acc 0.94
2016-09-06T22:11:26.275965: step 2132, loss 0.00875517, acc 1
2016-09-06T22:11:26.952069: step 2133, loss 0.0287725, acc 0.98
2016-09-06T22:11:27.626099: step 2134, loss 0.0613287, acc 0.98
2016-09-06T22:11:28.303789: step 2135, loss 0.0031495, acc 1
2016-09-06T22:11:28.990217: step 2136, loss 0.0324596, acc 0.98
2016-09-06T22:11:29.694564: step 2137, loss 0.0126409, acc 1
2016-09-06T22:11:30.384514: step 2138, loss 0.0149869, acc 1
2016-09-06T22:11:31.091535: step 2139, loss 0.0522423, acc 0.98
2016-09-06T22:11:31.737552: step 2140, loss 0.0253359, acc 0.98
2016-09-06T22:11:32.430822: step 2141, loss 0.160234, acc 0.98
2016-09-06T22:11:33.103719: step 2142, loss 0.0430286, acc 1
2016-09-06T22:11:33.783688: step 2143, loss 0.024004, acc 1
2016-09-06T22:11:34.463099: step 2144, loss 0.0841381, acc 0.96
2016-09-06T22:11:35.149347: step 2145, loss 0.0213216, acc 0.98
2016-09-06T22:11:35.847799: step 2146, loss 0.0703149, acc 0.96
2016-09-06T22:11:36.526429: step 2147, loss 0.0132588, acc 1
2016-09-06T22:11:37.259383: step 2148, loss 0.153586, acc 0.96
2016-09-06T22:11:37.956715: step 2149, loss 0.0522253, acc 0.96
2016-09-06T22:11:38.668143: step 2150, loss 0.064171, acc 0.98
2016-09-06T22:11:39.343674: step 2151, loss 0.0142212, acc 1
2016-09-06T22:11:40.021440: step 2152, loss 0.0130458, acc 1
2016-09-06T22:11:40.734348: step 2153, loss 0.0649909, acc 0.96
2016-09-06T22:11:41.411125: step 2154, loss 0.0516013, acc 0.98
2016-09-06T22:11:42.092666: step 2155, loss 0.0262518, acc 1
2016-09-06T22:11:42.789970: step 2156, loss 0.0474064, acc 0.98
2016-09-06T22:11:43.507054: step 2157, loss 0.0416658, acc 0.98
2016-09-06T22:11:44.196046: step 2158, loss 0.100373, acc 0.96
2016-09-06T22:11:44.861252: step 2159, loss 0.0933053, acc 0.94
2016-09-06T22:11:45.571842: step 2160, loss 0.021189, acc 1
2016-09-06T22:11:46.253483: step 2161, loss 0.044013, acc 0.98
2016-09-06T22:11:46.934159: step 2162, loss 0.0111166, acc 1
2016-09-06T22:11:47.622470: step 2163, loss 0.149205, acc 0.94
2016-09-06T22:11:48.325759: step 2164, loss 0.0148394, acc 1
2016-09-06T22:11:49.015606: step 2165, loss 0.0110168, acc 1
2016-09-06T22:11:49.666723: step 2166, loss 0.0228591, acc 1
2016-09-06T22:11:50.352541: step 2167, loss 0.162833, acc 0.96
2016-09-06T22:11:51.002883: step 2168, loss 0.130465, acc 0.96
2016-09-06T22:11:51.687075: step 2169, loss 0.0171414, acc 1
2016-09-06T22:11:52.363081: step 2170, loss 0.0109053, acc 1
2016-09-06T22:11:53.037044: step 2171, loss 0.0191288, acc 1
2016-09-06T22:11:53.719789: step 2172, loss 0.0279433, acc 1
2016-09-06T22:11:54.406271: step 2173, loss 0.0354958, acc 1
2016-09-06T22:11:55.120465: step 2174, loss 0.00981946, acc 1
2016-09-06T22:11:55.812625: step 2175, loss 0.0990679, acc 0.92
2016-09-06T22:11:56.505929: step 2176, loss 0.0593138, acc 0.98
2016-09-06T22:11:57.208080: step 2177, loss 0.00193223, acc 1
2016-09-06T22:11:57.898313: step 2178, loss 0.0427129, acc 0.98
2016-09-06T22:11:58.591955: step 2179, loss 0.114262, acc 0.92
2016-09-06T22:11:59.256847: step 2180, loss 0.0386253, acc 1
2016-09-06T22:11:59.946811: step 2181, loss 0.0351469, acc 0.98
2016-09-06T22:12:00.648157: step 2182, loss 0.0313284, acc 0.98
2016-09-06T22:12:01.349390: step 2183, loss 0.027602, acc 1
2016-09-06T22:12:02.025759: step 2184, loss 0.0358424, acc 0.98
2016-09-06T22:12:02.706434: step 2185, loss 0.122166, acc 0.94
2016-09-06T22:12:03.413310: step 2186, loss 0.0310991, acc 0.98
2016-09-06T22:12:04.105984: step 2187, loss 0.101129, acc 0.98
2016-09-06T22:12:04.810924: step 2188, loss 0.0224008, acc 0.98
2016-09-06T22:12:05.464394: step 2189, loss 0.0476162, acc 1
2016-09-06T22:12:06.182425: step 2190, loss 0.0113601, acc 1
2016-09-06T22:12:06.875867: step 2191, loss 0.0114939, acc 1
2016-09-06T22:12:07.567864: step 2192, loss 0.138605, acc 0.96
2016-09-06T22:12:08.237755: step 2193, loss 0.0857564, acc 0.96
2016-09-06T22:12:08.928558: step 2194, loss 0.078978, acc 0.98
2016-09-06T22:12:09.637846: step 2195, loss 0.060596, acc 0.98
2016-09-06T22:12:10.309930: step 2196, loss 0.0265923, acc 1
2016-09-06T22:12:10.998451: step 2197, loss 0.0273623, acc 0.98
2016-09-06T22:12:11.675360: step 2198, loss 0.0179899, acc 1
2016-09-06T22:12:12.355533: step 2199, loss 0.0578675, acc 0.98
2016-09-06T22:12:13.040000: step 2200, loss 0.0319728, acc 0.98

Evaluation:
2016-09-06T22:12:16.190978: step 2200, loss 1.47379, acc 0.753283

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-2200

2016-09-06T22:12:17.952841: step 2201, loss 0.0439224, acc 0.96
2016-09-06T22:12:18.632188: step 2202, loss 0.0219634, acc 1
2016-09-06T22:12:19.360154: step 2203, loss 0.106317, acc 0.96
2016-09-06T22:12:20.071710: step 2204, loss 0.0436666, acc 1
2016-09-06T22:12:20.762269: step 2205, loss 0.0129024, acc 1
2016-09-06T22:12:21.437711: step 2206, loss 0.012686, acc 1
2016-09-06T22:12:22.112635: step 2207, loss 0.0536505, acc 0.96
2016-09-06T22:12:22.801386: step 2208, loss 0.0835496, acc 0.96
2016-09-06T22:12:23.473818: step 2209, loss 0.026736, acc 0.98
2016-09-06T22:12:24.168158: step 2210, loss 0.0528898, acc 0.96
2016-09-06T22:12:24.865554: step 2211, loss 0.0494558, acc 0.98
2016-09-06T22:12:25.567394: step 2212, loss 0.0855121, acc 0.96
2016-09-06T22:12:26.276931: step 2213, loss 0.0128419, acc 1
2016-09-06T22:12:26.972155: step 2214, loss 0.0661787, acc 0.96
2016-09-06T22:12:27.688374: step 2215, loss 0.0102597, acc 1
2016-09-06T22:12:28.359033: step 2216, loss 0.0130599, acc 1
2016-09-06T22:12:29.032717: step 2217, loss 0.0428124, acc 0.98
2016-09-06T22:12:29.729485: step 2218, loss 0.202466, acc 0.96
2016-09-06T22:12:30.429558: step 2219, loss 0.0237646, acc 1
2016-09-06T22:12:31.132262: step 2220, loss 0.0273013, acc 1
2016-09-06T22:12:31.809458: step 2221, loss 0.0473184, acc 0.98
2016-09-06T22:12:32.498862: step 2222, loss 0.1041, acc 0.94
2016-09-06T22:12:33.197796: step 2223, loss 0.0530852, acc 0.98
2016-09-06T22:12:33.877817: step 2224, loss 0.0183077, acc 0.98
2016-09-06T22:12:34.566019: step 2225, loss 0.163325, acc 0.92
2016-09-06T22:12:35.256402: step 2226, loss 0.0307997, acc 1
2016-09-06T22:12:35.977981: step 2227, loss 0.0590768, acc 0.96
2016-09-06T22:12:36.642399: step 2228, loss 0.0249449, acc 1
2016-09-06T22:12:37.333067: step 2229, loss 0.00230818, acc 1
2016-09-06T22:12:37.995151: step 2230, loss 0.038982, acc 0.98
2016-09-06T22:12:38.677555: step 2231, loss 0.0596684, acc 0.96
2016-09-06T22:12:39.366620: step 2232, loss 0.139576, acc 0.94
2016-09-06T22:12:40.040235: step 2233, loss 0.0949142, acc 0.92
2016-09-06T22:12:40.721327: step 2234, loss 0.0323035, acc 0.98
2016-09-06T22:12:41.421124: step 2235, loss 0.0237997, acc 1
2016-09-06T22:12:42.111822: step 2236, loss 0.209846, acc 0.92
2016-09-06T22:12:42.782606: step 2237, loss 0.0439482, acc 0.98
2016-09-06T22:12:43.474460: step 2238, loss 0.123308, acc 0.94
2016-09-06T22:12:44.148275: step 2239, loss 0.221658, acc 0.94
2016-09-06T22:12:44.818382: step 2240, loss 0.100374, acc 0.94
2016-09-06T22:12:45.498447: step 2241, loss 0.0385093, acc 0.98
2016-09-06T22:12:46.169999: step 2242, loss 0.0595485, acc 0.96
2016-09-06T22:12:46.887650: step 2243, loss 0.0367576, acc 1
2016-09-06T22:12:47.565413: step 2244, loss 0.0724376, acc 0.98
2016-09-06T22:12:48.237272: step 2245, loss 0.0790891, acc 0.94
2016-09-06T22:12:48.925210: step 2246, loss 0.0318877, acc 0.98
2016-09-06T22:12:49.622129: step 2247, loss 0.0366705, acc 0.98
2016-09-06T22:12:50.318474: step 2248, loss 0.0308749, acc 1
2016-09-06T22:12:51.027496: step 2249, loss 0.0936043, acc 0.96
2016-09-06T22:12:51.727669: step 2250, loss 0.062395, acc 0.96
2016-09-06T22:12:52.417506: step 2251, loss 0.0299075, acc 1
2016-09-06T22:12:53.131372: step 2252, loss 0.0752734, acc 0.96
2016-09-06T22:12:53.825623: step 2253, loss 0.0641483, acc 0.98
2016-09-06T22:12:54.508307: step 2254, loss 0.0685454, acc 0.98
2016-09-06T22:12:55.212806: step 2255, loss 0.0197289, acc 1
2016-09-06T22:12:55.891036: step 2256, loss 0.0123033, acc 1
2016-09-06T22:12:56.602148: step 2257, loss 0.102106, acc 0.92
2016-09-06T22:12:57.279379: step 2258, loss 0.0211677, acc 1
2016-09-06T22:12:57.960589: step 2259, loss 0.0641974, acc 0.96
2016-09-06T22:12:58.655445: step 2260, loss 0.0873543, acc 0.92
2016-09-06T22:12:59.354546: step 2261, loss 0.123089, acc 0.92
2016-09-06T22:13:00.050657: step 2262, loss 0.134507, acc 0.92
2016-09-06T22:13:00.753486: step 2263, loss 0.068625, acc 0.96
2016-09-06T22:13:01.454278: step 2264, loss 0.111621, acc 0.94
2016-09-06T22:13:02.150889: step 2265, loss 0.0354328, acc 0.98
2016-09-06T22:13:02.844918: step 2266, loss 0.0364681, acc 0.98
2016-09-06T22:13:03.521956: step 2267, loss 0.0179651, acc 1
2016-09-06T22:13:04.214492: step 2268, loss 0.00201904, acc 1
2016-09-06T22:13:04.954237: step 2269, loss 0.0195991, acc 1
2016-09-06T22:13:05.631043: step 2270, loss 0.0152933, acc 1
2016-09-06T22:13:06.321664: step 2271, loss 0.0261316, acc 0.98
2016-09-06T22:13:07.011299: step 2272, loss 0.145916, acc 0.96
2016-09-06T22:13:07.723392: step 2273, loss 0.0702374, acc 0.98
2016-09-06T22:13:08.427918: step 2274, loss 0.0697157, acc 0.96
2016-09-06T22:13:09.091758: step 2275, loss 0.0539882, acc 0.98
2016-09-06T22:13:09.781462: step 2276, loss 0.0546592, acc 1
2016-09-06T22:13:10.463606: step 2277, loss 0.0576788, acc 0.98
2016-09-06T22:13:11.165492: step 2278, loss 0.0498808, acc 0.98
2016-09-06T22:13:11.861326: step 2279, loss 0.0348962, acc 0.98
2016-09-06T22:13:12.536180: step 2280, loss 0.0340447, acc 1
2016-09-06T22:13:13.231954: step 2281, loss 0.0411429, acc 0.98
2016-09-06T22:13:13.893842: step 2282, loss 0.0593715, acc 0.96
2016-09-06T22:13:14.597752: step 2283, loss 0.0238076, acc 1
2016-09-06T22:13:15.265191: step 2284, loss 0.0277776, acc 0.98
2016-09-06T22:13:15.944681: step 2285, loss 0.00840655, acc 1
2016-09-06T22:13:16.629932: step 2286, loss 0.0400242, acc 1
2016-09-06T22:13:17.333943: step 2287, loss 0.0838123, acc 0.98
2016-09-06T22:13:18.025410: step 2288, loss 0.0385093, acc 0.98
2016-09-06T22:13:18.694712: step 2289, loss 0.063743, acc 0.98
2016-09-06T22:13:19.414782: step 2290, loss 0.0849232, acc 0.94
2016-09-06T22:13:20.088619: step 2291, loss 0.0167892, acc 0.98
2016-09-06T22:13:20.805298: step 2292, loss 0.122754, acc 0.98
2016-09-06T22:13:21.492318: step 2293, loss 0.0190113, acc 0.98
2016-09-06T22:13:22.189048: step 2294, loss 0.0103733, acc 1
2016-09-06T22:13:22.891577: step 2295, loss 0.034388, acc 0.98
2016-09-06T22:13:23.575435: step 2296, loss 0.0625117, acc 0.96
2016-09-06T22:13:24.272928: step 2297, loss 0.040834, acc 0.96
2016-09-06T22:13:24.971727: step 2298, loss 0.016578, acc 1
2016-09-06T22:13:25.658527: step 2299, loss 0.0665664, acc 0.96
2016-09-06T22:13:26.346124: step 2300, loss 0.0175657, acc 1

Evaluation:
2016-09-06T22:13:29.471735: step 2300, loss 1.75127, acc 0.742026

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-2300

2016-09-06T22:13:31.137668: step 2301, loss 0.0329726, acc 1
2016-09-06T22:13:31.842615: step 2302, loss 0.0230113, acc 1
2016-09-06T22:13:32.584148: step 2303, loss 0.017641, acc 0.98
2016-09-06T22:13:33.225396: step 2304, loss 0.028964, acc 0.977273
2016-09-06T22:13:33.950588: step 2305, loss 0.0608379, acc 0.98
2016-09-06T22:13:34.662229: step 2306, loss 0.0223172, acc 1
2016-09-06T22:13:35.373343: step 2307, loss 0.0964342, acc 0.98
2016-09-06T22:13:36.078390: step 2308, loss 0.070553, acc 0.98
2016-09-06T22:13:36.741509: step 2309, loss 0.112399, acc 0.98
2016-09-06T22:13:37.441816: step 2310, loss 0.0275733, acc 1
2016-09-06T22:13:38.117991: step 2311, loss 0.0140781, acc 1
2016-09-06T22:13:38.792948: step 2312, loss 0.0362182, acc 0.98
2016-09-06T22:13:39.470068: step 2313, loss 0.170297, acc 0.96
2016-09-06T22:13:40.142853: step 2314, loss 0.00662237, acc 1
2016-09-06T22:13:40.812142: step 2315, loss 0.0175961, acc 1
2016-09-06T22:13:41.505160: step 2316, loss 0.0455327, acc 0.96
2016-09-06T22:13:42.198759: step 2317, loss 0.0702405, acc 0.98
2016-09-06T22:13:42.872587: step 2318, loss 0.107761, acc 0.96
2016-09-06T22:13:43.562142: step 2319, loss 0.0602622, acc 0.98
2016-09-06T22:13:44.239799: step 2320, loss 0.0654925, acc 0.96
2016-09-06T22:13:44.936906: step 2321, loss 0.0425579, acc 0.98
2016-09-06T22:13:45.622019: step 2322, loss 0.0598248, acc 0.98
2016-09-06T22:13:46.302317: step 2323, loss 0.0403596, acc 0.96
2016-09-06T22:13:46.992747: step 2324, loss 0.0196632, acc 1
2016-09-06T22:13:47.641234: step 2325, loss 0.0502607, acc 0.98
2016-09-06T22:13:48.336493: step 2326, loss 0.0487653, acc 0.98
2016-09-06T22:13:49.016860: step 2327, loss 0.00721293, acc 1
2016-09-06T22:13:49.704530: step 2328, loss 0.0230979, acc 0.98
2016-09-06T22:13:50.401288: step 2329, loss 0.035754, acc 1
2016-09-06T22:13:51.087176: step 2330, loss 0.0151247, acc 1
2016-09-06T22:13:51.778566: step 2331, loss 0.0131621, acc 1
2016-09-06T22:13:52.432089: step 2332, loss 0.0144268, acc 1
2016-09-06T22:13:53.132120: step 2333, loss 0.0593915, acc 0.98
2016-09-06T22:13:53.847744: step 2334, loss 0.0254503, acc 1
2016-09-06T22:13:54.527699: step 2335, loss 0.0476045, acc 0.98
2016-09-06T22:13:55.230570: step 2336, loss 0.381113, acc 0.94
2016-09-06T22:13:55.907782: step 2337, loss 0.126845, acc 0.98
2016-09-06T22:13:56.620086: step 2338, loss 0.137603, acc 0.98
2016-09-06T22:13:57.304124: step 2339, loss 0.0467003, acc 0.98
2016-09-06T22:13:57.993914: step 2340, loss 0.058926, acc 0.96
2016-09-06T22:13:58.673515: step 2341, loss 0.0597675, acc 0.98
2016-09-06T22:13:59.363694: step 2342, loss 0.0664026, acc 0.98
2016-09-06T22:14:00.079623: step 2343, loss 0.0363509, acc 1
2016-09-06T22:14:00.796622: step 2344, loss 0.010942, acc 1
2016-09-06T22:14:01.488510: step 2345, loss 0.0396037, acc 0.96
2016-09-06T22:14:02.162914: step 2346, loss 0.012181, acc 1
2016-09-06T22:14:02.849558: step 2347, loss 0.0359154, acc 0.98
2016-09-06T22:14:03.530682: step 2348, loss 0.0333842, acc 1
2016-09-06T22:14:04.206908: step 2349, loss 0.0682321, acc 0.96
2016-09-06T22:14:04.885227: step 2350, loss 0.0335785, acc 1
2016-09-06T22:14:05.576506: step 2351, loss 0.0106791, acc 1
2016-09-06T22:14:06.290544: step 2352, loss 0.0350484, acc 0.98
2016-09-06T22:14:06.971477: step 2353, loss 0.0245032, acc 1
2016-09-06T22:14:07.654588: step 2354, loss 0.0262407, acc 1
2016-09-06T22:14:08.356848: step 2355, loss 0.0194616, acc 1
2016-09-06T22:14:09.058536: step 2356, loss 0.0485207, acc 0.98
2016-09-06T22:14:09.731722: step 2357, loss 0.0291549, acc 0.98
2016-09-06T22:14:10.400810: step 2358, loss 0.072426, acc 0.98
2016-09-06T22:14:11.106208: step 2359, loss 0.00742616, acc 1
2016-09-06T22:14:11.787051: step 2360, loss 0.0192019, acc 0.98
2016-09-06T22:14:12.471790: step 2361, loss 0.0273101, acc 1
2016-09-06T22:14:13.160284: step 2362, loss 0.0531527, acc 0.98
2016-09-06T22:14:13.861808: step 2363, loss 0.0174654, acc 0.98
2016-09-06T22:14:14.550421: step 2364, loss 0.159816, acc 0.98
2016-09-06T22:14:15.208955: step 2365, loss 0.0849748, acc 0.98
2016-09-06T22:14:15.914860: step 2366, loss 0.00219408, acc 1
2016-09-06T22:14:16.576780: step 2367, loss 0.00303548, acc 1
2016-09-06T22:14:17.251832: step 2368, loss 0.0311244, acc 0.98
2016-09-06T22:14:17.953608: step 2369, loss 0.0278246, acc 0.98
2016-09-06T22:14:18.652425: step 2370, loss 0.00696418, acc 1
2016-09-06T22:14:19.364451: step 2371, loss 0.0260874, acc 0.98
2016-09-06T22:14:20.036867: step 2372, loss 0.068735, acc 0.96
2016-09-06T22:14:20.742881: step 2373, loss 0.125907, acc 0.98
2016-09-06T22:14:21.410675: step 2374, loss 0.026285, acc 0.98
2016-09-06T22:14:22.096867: step 2375, loss 0.187116, acc 0.92
2016-09-06T22:14:22.787833: step 2376, loss 0.0432426, acc 0.98
2016-09-06T22:14:23.466517: step 2377, loss 0.0514384, acc 0.96
2016-09-06T22:14:24.168153: step 2378, loss 0.0407435, acc 0.96
2016-09-06T22:14:24.823098: step 2379, loss 0.0277306, acc 1
2016-09-06T22:14:25.537965: step 2380, loss 0.018443, acc 1
2016-09-06T22:14:26.215192: step 2381, loss 0.0578929, acc 0.96
2016-09-06T22:14:26.911156: step 2382, loss 0.0564427, acc 0.98
2016-09-06T22:14:27.589593: step 2383, loss 0.0486107, acc 0.98
2016-09-06T22:14:28.268829: step 2384, loss 0.0092569, acc 1
2016-09-06T22:14:28.961806: step 2385, loss 0.0250847, acc 0.98
2016-09-06T22:14:29.634778: step 2386, loss 0.00855098, acc 1
2016-09-06T22:14:30.354282: step 2387, loss 0.0536346, acc 0.98
2016-09-06T22:14:31.038332: step 2388, loss 0.125795, acc 0.94
2016-09-06T22:14:31.725325: step 2389, loss 0.0410029, acc 0.98
2016-09-06T22:14:32.400696: step 2390, loss 0.0175497, acc 0.98
2016-09-06T22:14:33.089830: step 2391, loss 0.0834251, acc 0.94
2016-09-06T22:14:33.799725: step 2392, loss 0.144187, acc 0.94
2016-09-06T22:14:34.495957: step 2393, loss 0.0667266, acc 0.94
2016-09-06T22:14:35.209983: step 2394, loss 0.0372974, acc 1
2016-09-06T22:14:35.890463: step 2395, loss 0.026074, acc 0.98
2016-09-06T22:14:36.563282: step 2396, loss 0.109404, acc 0.96
2016-09-06T22:14:37.239289: step 2397, loss 0.0192377, acc 0.98
2016-09-06T22:14:37.928872: step 2398, loss 0.153324, acc 0.94
2016-09-06T22:14:38.628789: step 2399, loss 0.137068, acc 0.96
2016-09-06T22:14:39.308962: step 2400, loss 0.0897667, acc 0.94

Evaluation:
2016-09-06T22:14:42.451220: step 2400, loss 1.29318, acc 0.741088

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-2400

2016-09-06T22:14:44.088439: step 2401, loss 0.0212403, acc 1
2016-09-06T22:14:44.799618: step 2402, loss 0.0354405, acc 0.98
2016-09-06T22:14:45.497860: step 2403, loss 0.0482455, acc 0.98
2016-09-06T22:14:46.198810: step 2404, loss 0.155289, acc 0.94
2016-09-06T22:14:46.862412: step 2405, loss 0.0274368, acc 1
2016-09-06T22:14:47.542385: step 2406, loss 0.00961711, acc 1
2016-09-06T22:14:48.236286: step 2407, loss 0.0207517, acc 1
2016-09-06T22:14:48.900616: step 2408, loss 0.0401624, acc 0.98
2016-09-06T22:14:49.606887: step 2409, loss 0.134146, acc 0.94
2016-09-06T22:14:50.269848: step 2410, loss 0.0343663, acc 0.98
2016-09-06T22:14:50.963188: step 2411, loss 0.0514559, acc 0.98
2016-09-06T22:14:51.646613: step 2412, loss 0.0297294, acc 0.98
2016-09-06T22:14:52.327112: step 2413, loss 0.0397476, acc 0.98
2016-09-06T22:14:53.009714: step 2414, loss 0.0125074, acc 1
2016-09-06T22:14:53.680408: step 2415, loss 0.0246786, acc 1
2016-09-06T22:14:54.379509: step 2416, loss 0.0129306, acc 1
2016-09-06T22:14:55.087142: step 2417, loss 0.0249224, acc 0.98
2016-09-06T22:14:55.764785: step 2418, loss 0.0547662, acc 0.98
2016-09-06T22:14:56.454779: step 2419, loss 0.0597554, acc 0.98
2016-09-06T22:14:57.145119: step 2420, loss 0.0161048, acc 1
2016-09-06T22:14:57.843151: step 2421, loss 0.0226407, acc 1
2016-09-06T22:14:58.538909: step 2422, loss 0.0320416, acc 0.98
2016-09-06T22:14:59.234574: step 2423, loss 0.0318262, acc 1
2016-09-06T22:14:59.907239: step 2424, loss 0.014041, acc 1
2016-09-06T22:15:00.618620: step 2425, loss 0.157662, acc 0.92
2016-09-06T22:15:01.302824: step 2426, loss 0.0319345, acc 1
2016-09-06T22:15:01.995493: step 2427, loss 0.0155676, acc 1
2016-09-06T22:15:02.696684: step 2428, loss 0.0768181, acc 0.98
2016-09-06T22:15:03.364294: step 2429, loss 0.0231972, acc 1
2016-09-06T22:15:04.061986: step 2430, loss 0.0205291, acc 1
2016-09-06T22:15:04.727015: step 2431, loss 0.00745424, acc 1
2016-09-06T22:15:05.430944: step 2432, loss 0.061326, acc 0.98
2016-09-06T22:15:06.117082: step 2433, loss 0.0325656, acc 0.98
2016-09-06T22:15:06.800024: step 2434, loss 0.021102, acc 0.98
2016-09-06T22:15:07.518199: step 2435, loss 0.137328, acc 0.92
2016-09-06T22:15:08.196829: step 2436, loss 0.0207508, acc 0.98
2016-09-06T22:15:08.882009: step 2437, loss 0.0537465, acc 0.98
2016-09-06T22:15:09.571206: step 2438, loss 0.0590962, acc 0.98
2016-09-06T22:15:10.281243: step 2439, loss 0.0624767, acc 0.96
2016-09-06T22:15:10.971284: step 2440, loss 0.0171671, acc 1
2016-09-06T22:15:11.665912: step 2441, loss 0.0210594, acc 0.98
2016-09-06T22:15:12.367644: step 2442, loss 0.00571625, acc 1
2016-09-06T22:15:13.037186: step 2443, loss 0.0353062, acc 0.98
2016-09-06T22:15:13.745291: step 2444, loss 0.0174129, acc 1
2016-09-06T22:15:14.425754: step 2445, loss 0.0163737, acc 1
2016-09-06T22:15:15.102830: step 2446, loss 0.0382764, acc 0.96
2016-09-06T22:15:15.780206: step 2447, loss 0.0169586, acc 1
2016-09-06T22:15:16.471150: step 2448, loss 0.0758867, acc 0.96
2016-09-06T22:15:17.175160: step 2449, loss 0.0531917, acc 0.96
2016-09-06T22:15:17.850949: step 2450, loss 0.0940521, acc 0.94
2016-09-06T22:15:18.560762: step 2451, loss 0.25243, acc 0.96
2016-09-06T22:15:19.252741: step 2452, loss 0.0542212, acc 0.98
2016-09-06T22:15:19.950109: step 2453, loss 0.0388725, acc 1
2016-09-06T22:15:20.659894: step 2454, loss 0.0236159, acc 1
2016-09-06T22:15:21.354499: step 2455, loss 0.0119658, acc 1
2016-09-06T22:15:22.064873: step 2456, loss 0.110685, acc 0.94
2016-09-06T22:15:22.721292: step 2457, loss 0.0216619, acc 0.98
2016-09-06T22:15:23.396830: step 2458, loss 0.0480287, acc 0.98
2016-09-06T22:15:24.078893: step 2459, loss 0.0241709, acc 0.98
2016-09-06T22:15:24.765796: step 2460, loss 0.0629063, acc 0.96
2016-09-06T22:15:25.448647: step 2461, loss 0.0322595, acc 1
2016-09-06T22:15:26.130466: step 2462, loss 0.0240974, acc 0.98
2016-09-06T22:15:26.833565: step 2463, loss 0.0165433, acc 1
2016-09-06T22:15:27.500167: step 2464, loss 0.0977288, acc 0.94
2016-09-06T22:15:28.183608: step 2465, loss 0.00683724, acc 1
2016-09-06T22:15:28.853901: step 2466, loss 0.0281174, acc 1
2016-09-06T22:15:29.529429: step 2467, loss 0.0470213, acc 0.98
2016-09-06T22:15:30.222530: step 2468, loss 0.043293, acc 0.96
2016-09-06T22:15:30.918419: step 2469, loss 0.0173659, acc 1
2016-09-06T22:15:31.642926: step 2470, loss 0.0464517, acc 0.96
2016-09-06T22:15:32.327962: step 2471, loss 0.0467601, acc 0.98
2016-09-06T22:15:33.007066: step 2472, loss 0.0609299, acc 0.96
2016-09-06T22:15:33.705591: step 2473, loss 0.0276563, acc 0.98
2016-09-06T22:15:34.409639: step 2474, loss 0.0297571, acc 0.98
2016-09-06T22:15:35.112767: step 2475, loss 0.0238592, acc 0.98
2016-09-06T22:15:35.790929: step 2476, loss 0.0286353, acc 1
2016-09-06T22:15:36.489535: step 2477, loss 0.0380016, acc 0.98
2016-09-06T22:15:37.161054: step 2478, loss 0.00388669, acc 1
2016-09-06T22:15:37.833451: step 2479, loss 0.0307555, acc 0.98
2016-09-06T22:15:38.556173: step 2480, loss 0.226081, acc 0.96
2016-09-06T22:15:39.242558: step 2481, loss 0.0574785, acc 0.98
2016-09-06T22:15:39.946642: step 2482, loss 0.0558323, acc 0.98
2016-09-06T22:15:40.612411: step 2483, loss 0.0421182, acc 0.96
2016-09-06T22:15:41.309615: step 2484, loss 0.0233573, acc 1
2016-09-06T22:15:41.974319: step 2485, loss 0.0111242, acc 1
2016-09-06T22:15:42.688763: step 2486, loss 0.0117583, acc 1
2016-09-06T22:15:43.380552: step 2487, loss 0.0254901, acc 0.98
2016-09-06T22:15:44.094458: step 2488, loss 0.0101992, acc 1
2016-09-06T22:15:44.775330: step 2489, loss 0.159401, acc 0.92
2016-09-06T22:15:45.448594: step 2490, loss 0.0608971, acc 0.98
2016-09-06T22:15:46.148806: step 2491, loss 0.0415388, acc 0.98
2016-09-06T22:15:46.833542: step 2492, loss 0.0334449, acc 1
2016-09-06T22:15:47.542614: step 2493, loss 0.0497169, acc 0.98
2016-09-06T22:15:48.217899: step 2494, loss 0.0409124, acc 0.98
2016-09-06T22:15:48.905688: step 2495, loss 0.157055, acc 0.98
2016-09-06T22:15:49.543449: step 2496, loss 0.0365587, acc 0.977273
2016-09-06T22:15:50.235503: step 2497, loss 0.0275814, acc 1
2016-09-06T22:15:50.927120: step 2498, loss 0.00901324, acc 1
2016-09-06T22:15:51.596534: step 2499, loss 0.0372868, acc 0.98
2016-09-06T22:15:52.299775: step 2500, loss 0.0843222, acc 0.96

Evaluation:
2016-09-06T22:15:55.458428: step 2500, loss 1.34779, acc 0.752345

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-2500

2016-09-06T22:15:57.187725: step 2501, loss 0.0359292, acc 1
2016-09-06T22:15:57.878082: step 2502, loss 0.084284, acc 0.94
2016-09-06T22:15:58.595993: step 2503, loss 0.0338405, acc 1
2016-09-06T22:15:59.297702: step 2504, loss 0.0770848, acc 0.96
2016-09-06T22:15:59.965261: step 2505, loss 0.0218907, acc 1
2016-09-06T22:16:00.691479: step 2506, loss 0.0133285, acc 1
2016-09-06T22:16:01.386698: step 2507, loss 0.0179753, acc 0.98
2016-09-06T22:16:02.091384: step 2508, loss 0.0167525, acc 1
2016-09-06T22:16:02.810499: step 2509, loss 0.126596, acc 0.96
2016-09-06T22:16:03.472070: step 2510, loss 0.0610705, acc 0.94
2016-09-06T22:16:04.160538: step 2511, loss 0.0696255, acc 0.98
2016-09-06T22:16:04.860945: step 2512, loss 0.0393517, acc 0.98
2016-09-06T22:16:05.541113: step 2513, loss 0.0269806, acc 1
2016-09-06T22:16:06.198418: step 2514, loss 0.0385029, acc 0.98
2016-09-06T22:16:06.883460: step 2515, loss 0.0589779, acc 0.98
2016-09-06T22:16:07.569472: step 2516, loss 0.0421298, acc 0.96
2016-09-06T22:16:08.243980: step 2517, loss 0.141907, acc 0.98
2016-09-06T22:16:08.962846: step 2518, loss 0.08167, acc 0.96
2016-09-06T22:16:09.628873: step 2519, loss 0.0112948, acc 1
2016-09-06T22:16:10.308327: step 2520, loss 0.0296275, acc 0.98
2016-09-06T22:16:10.997678: step 2521, loss 0.0460696, acc 0.98
2016-09-06T22:16:11.704235: step 2522, loss 0.0215545, acc 1
2016-09-06T22:16:12.391081: step 2523, loss 0.00923721, acc 1
2016-09-06T22:16:13.074903: step 2524, loss 0.0371137, acc 0.98
2016-09-06T22:16:13.797430: step 2525, loss 0.0780569, acc 0.94
2016-09-06T22:16:14.468618: step 2526, loss 0.0212387, acc 1
2016-09-06T22:16:15.154231: step 2527, loss 0.0149778, acc 1
2016-09-06T22:16:15.854145: step 2528, loss 0.0195482, acc 1
2016-09-06T22:16:16.539119: step 2529, loss 0.0145061, acc 1
2016-09-06T22:16:17.234597: step 2530, loss 0.0124262, acc 1
2016-09-06T22:16:17.947583: step 2531, loss 0.00988908, acc 1
2016-09-06T22:16:18.662966: step 2532, loss 0.0603296, acc 0.98
2016-09-06T22:16:19.340662: step 2533, loss 0.0447822, acc 0.98
2016-09-06T22:16:20.024088: step 2534, loss 0.0329578, acc 1
2016-09-06T22:16:20.704956: step 2535, loss 0.0254318, acc 0.98
2016-09-06T22:16:21.382939: step 2536, loss 0.0147618, acc 1
2016-09-06T22:16:22.065515: step 2537, loss 0.070305, acc 0.96
2016-09-06T22:16:22.746493: step 2538, loss 0.00312707, acc 1
2016-09-06T22:16:23.439023: step 2539, loss 0.0444672, acc 0.98
2016-09-06T22:16:24.113942: step 2540, loss 0.0208152, acc 0.98
2016-09-06T22:16:24.825080: step 2541, loss 0.0156343, acc 1
2016-09-06T22:16:25.505443: step 2542, loss 0.00624688, acc 1
2016-09-06T22:16:26.212782: step 2543, loss 0.0169115, acc 1
2016-09-06T22:16:26.926214: step 2544, loss 0.0459872, acc 0.98
2016-09-06T22:16:27.647595: step 2545, loss 0.0314929, acc 0.98
2016-09-06T22:16:28.332608: step 2546, loss 0.0617743, acc 0.98
2016-09-06T22:16:29.030978: step 2547, loss 0.0469384, acc 0.98
2016-09-06T22:16:29.730359: step 2548, loss 0.0183465, acc 1
2016-09-06T22:16:30.432763: step 2549, loss 0.00815474, acc 1
2016-09-06T22:16:31.103725: step 2550, loss 0.0654611, acc 0.98
2016-09-06T22:16:31.811130: step 2551, loss 0.0164335, acc 1
2016-09-06T22:16:32.466341: step 2552, loss 0.167714, acc 0.96
2016-09-06T22:16:33.136867: step 2553, loss 0.0452108, acc 0.98
2016-09-06T22:16:33.831656: step 2554, loss 0.000509908, acc 1
2016-09-06T22:16:34.503556: step 2555, loss 0.112976, acc 0.96
2016-09-06T22:16:35.186956: step 2556, loss 0.02115, acc 0.98
2016-09-06T22:16:35.872533: step 2557, loss 0.0921706, acc 0.92
2016-09-06T22:16:36.581274: step 2558, loss 0.0299073, acc 0.98
2016-09-06T22:16:37.263729: step 2559, loss 0.0522415, acc 0.96
2016-09-06T22:16:37.964957: step 2560, loss 0.0542148, acc 0.96
2016-09-06T22:16:38.663406: step 2561, loss 0.0137997, acc 1
2016-09-06T22:16:39.361396: step 2562, loss 0.155195, acc 0.92
2016-09-06T22:16:40.045744: step 2563, loss 0.0782806, acc 0.98
2016-09-06T22:16:40.732030: step 2564, loss 0.0515936, acc 0.96
2016-09-06T22:16:41.417034: step 2565, loss 0.0986173, acc 0.94
2016-09-06T22:16:42.090097: step 2566, loss 0.0164365, acc 1
2016-09-06T22:16:42.762805: step 2567, loss 0.100602, acc 0.98
2016-09-06T22:16:43.444259: step 2568, loss 0.0162819, acc 1
2016-09-06T22:16:44.128744: step 2569, loss 0.0237222, acc 1
2016-09-06T22:16:44.817812: step 2570, loss 0.0781009, acc 0.98
2016-09-06T22:16:45.530235: step 2571, loss 0.0205986, acc 1
2016-09-06T22:16:46.240058: step 2572, loss 0.046745, acc 0.98
2016-09-06T22:16:46.923941: step 2573, loss 0.0686147, acc 0.98
2016-09-06T22:16:47.608727: step 2574, loss 0.0487904, acc 0.98
2016-09-06T22:16:48.289217: step 2575, loss 0.00319272, acc 1
2016-09-06T22:16:48.969323: step 2576, loss 0.0684126, acc 0.98
2016-09-06T22:16:49.651125: step 2577, loss 0.0235323, acc 0.98
2016-09-06T22:16:50.332553: step 2578, loss 0.00642662, acc 1
2016-09-06T22:16:51.033950: step 2579, loss 0.0346118, acc 1
2016-09-06T22:16:51.702842: step 2580, loss 0.0283566, acc 1
2016-09-06T22:16:52.374403: step 2581, loss 0.0206977, acc 0.98
2016-09-06T22:16:53.059043: step 2582, loss 0.0336969, acc 0.98
2016-09-06T22:16:53.738728: step 2583, loss 0.145864, acc 0.98
2016-09-06T22:16:54.420045: step 2584, loss 0.00921163, acc 1
2016-09-06T22:16:55.118462: step 2585, loss 0.0241104, acc 0.98
2016-09-06T22:16:55.821198: step 2586, loss 0.0615181, acc 0.98
2016-09-06T22:16:56.497557: step 2587, loss 0.0443907, acc 0.98
2016-09-06T22:16:57.174963: step 2588, loss 0.0253665, acc 0.98
2016-09-06T22:16:57.872773: step 2589, loss 0.0179656, acc 1
2016-09-06T22:16:58.541488: step 2590, loss 0.0474964, acc 0.98
2016-09-06T22:16:59.223138: step 2591, loss 0.118692, acc 0.96
2016-09-06T22:16:59.918095: step 2592, loss 0.0858266, acc 0.98
2016-09-06T22:17:00.663517: step 2593, loss 0.00850574, acc 1
2016-09-06T22:17:01.343848: step 2594, loss 0.0246977, acc 0.98
2016-09-06T22:17:02.033990: step 2595, loss 0.0435496, acc 1
2016-09-06T22:17:02.728331: step 2596, loss 0.0127377, acc 1
2016-09-06T22:17:03.425360: step 2597, loss 0.0405105, acc 0.98
2016-09-06T22:17:04.109320: step 2598, loss 0.0563679, acc 0.96
2016-09-06T22:17:04.788428: step 2599, loss 0.0449262, acc 0.98
2016-09-06T22:17:05.512807: step 2600, loss 0.0282378, acc 0.98

Evaluation:
2016-09-06T22:17:08.687648: step 2600, loss 1.4615, acc 0.763602

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-2600

2016-09-06T22:17:10.398923: step 2601, loss 0.0339478, acc 0.98
2016-09-06T22:17:11.087345: step 2602, loss 0.0831337, acc 0.96
2016-09-06T22:17:11.782538: step 2603, loss 0.0371349, acc 0.98
2016-09-06T22:17:12.478892: step 2604, loss 0.0636265, acc 0.96
2016-09-06T22:17:13.150477: step 2605, loss 0.0242371, acc 1
2016-09-06T22:17:13.847888: step 2606, loss 0.0242036, acc 1
2016-09-06T22:17:14.555031: step 2607, loss 0.0336489, acc 0.98
2016-09-06T22:17:15.230219: step 2608, loss 0.0347937, acc 0.98
2016-09-06T22:17:15.930702: step 2609, loss 0.000836143, acc 1
2016-09-06T22:17:16.622570: step 2610, loss 0.0227887, acc 1
2016-09-06T22:17:17.315927: step 2611, loss 0.0382691, acc 0.98
2016-09-06T22:17:18.010587: step 2612, loss 0.0155343, acc 1
2016-09-06T22:17:18.710892: step 2613, loss 0.0390524, acc 0.96
2016-09-06T22:17:19.373975: step 2614, loss 0.0510916, acc 0.96
2016-09-06T22:17:20.047441: step 2615, loss 0.0458263, acc 0.96
2016-09-06T22:17:20.757097: step 2616, loss 0.0899514, acc 0.96
2016-09-06T22:17:21.452274: step 2617, loss 0.0386646, acc 0.98
2016-09-06T22:17:22.156167: step 2618, loss 0.0950678, acc 0.96
2016-09-06T22:17:22.842394: step 2619, loss 0.0693637, acc 0.98
2016-09-06T22:17:23.540582: step 2620, loss 0.0444327, acc 0.98
2016-09-06T22:17:24.214000: step 2621, loss 0.0419456, acc 0.98
2016-09-06T22:17:24.888281: step 2622, loss 0.0610375, acc 0.98
2016-09-06T22:17:25.591095: step 2623, loss 0.0377511, acc 0.98
2016-09-06T22:17:26.275262: step 2624, loss 0.0879483, acc 0.96
2016-09-06T22:17:26.982519: step 2625, loss 0.0844081, acc 0.94
2016-09-06T22:17:27.642072: step 2626, loss 0.0388086, acc 0.96
2016-09-06T22:17:28.349459: step 2627, loss 0.175952, acc 0.9
2016-09-06T22:17:29.048333: step 2628, loss 0.0683908, acc 0.96
2016-09-06T22:17:29.731617: step 2629, loss 0.0398172, acc 0.98
2016-09-06T22:17:30.427935: step 2630, loss 0.0308923, acc 0.98
2016-09-06T22:17:31.103514: step 2631, loss 0.015491, acc 1
2016-09-06T22:17:31.789421: step 2632, loss 0.00424775, acc 1
2016-09-06T22:17:32.439386: step 2633, loss 0.0661589, acc 0.96
2016-09-06T22:17:33.133816: step 2634, loss 0.0499597, acc 0.98
2016-09-06T22:17:33.811107: step 2635, loss 0.0803322, acc 0.98
2016-09-06T22:17:34.489156: step 2636, loss 0.0716847, acc 0.98
2016-09-06T22:17:35.179195: step 2637, loss 0.00839931, acc 1
2016-09-06T22:17:35.853501: step 2638, loss 0.110251, acc 0.96
2016-09-06T22:17:36.551657: step 2639, loss 0.091817, acc 0.98
2016-09-06T22:17:37.218734: step 2640, loss 0.052254, acc 0.96
2016-09-06T22:17:37.927703: step 2641, loss 0.0162699, acc 1
2016-09-06T22:17:38.609935: step 2642, loss 0.056416, acc 0.98
2016-09-06T22:17:39.302902: step 2643, loss 0.0519448, acc 0.98
2016-09-06T22:17:39.981982: step 2644, loss 0.0407791, acc 0.98
2016-09-06T22:17:40.664994: step 2645, loss 0.0490008, acc 0.98
2016-09-06T22:17:41.349123: step 2646, loss 0.0573964, acc 0.96
2016-09-06T22:17:42.039092: step 2647, loss 0.154894, acc 0.94
2016-09-06T22:17:42.751907: step 2648, loss 0.040024, acc 0.98
2016-09-06T22:17:43.422171: step 2649, loss 0.0333062, acc 0.98
2016-09-06T22:17:44.111120: step 2650, loss 0.0535996, acc 0.98
2016-09-06T22:17:44.815976: step 2651, loss 0.0910318, acc 0.94
2016-09-06T22:17:45.512911: step 2652, loss 0.0329309, acc 1
2016-09-06T22:17:46.205105: step 2653, loss 0.0225588, acc 1
2016-09-06T22:17:46.863475: step 2654, loss 0.0337477, acc 0.98
2016-09-06T22:17:47.569800: step 2655, loss 0.0629912, acc 0.98
2016-09-06T22:17:48.249608: step 2656, loss 0.119071, acc 0.94
2016-09-06T22:17:48.935844: step 2657, loss 0.0109528, acc 1
2016-09-06T22:17:49.613912: step 2658, loss 0.0104313, acc 1
2016-09-06T22:17:50.293830: step 2659, loss 0.0975042, acc 0.96
2016-09-06T22:17:50.972379: step 2660, loss 0.0234127, acc 1
2016-09-06T22:17:51.662074: step 2661, loss 0.0397197, acc 0.98
2016-09-06T22:17:52.371807: step 2662, loss 0.0310185, acc 0.96
2016-09-06T22:17:53.050633: step 2663, loss 0.0456806, acc 0.98
2016-09-06T22:17:53.750558: step 2664, loss 0.0630319, acc 0.96
2016-09-06T22:17:54.443559: step 2665, loss 0.0300122, acc 0.98
2016-09-06T22:17:55.122964: step 2666, loss 0.0589287, acc 0.96
2016-09-06T22:17:55.801926: step 2667, loss 0.0141125, acc 1
2016-09-06T22:17:56.471190: step 2668, loss 0.00853012, acc 1
2016-09-06T22:17:57.186680: step 2669, loss 0.0505806, acc 0.96
2016-09-06T22:17:57.878220: step 2670, loss 0.0694519, acc 0.96
2016-09-06T22:17:58.592498: step 2671, loss 0.00951049, acc 1
2016-09-06T22:17:59.300299: step 2672, loss 0.0372118, acc 0.96
2016-09-06T22:17:59.997590: step 2673, loss 0.0248458, acc 1
2016-09-06T22:18:00.749111: step 2674, loss 0.0732131, acc 0.96
2016-09-06T22:18:01.439947: step 2675, loss 0.087893, acc 0.96
2016-09-06T22:18:02.121662: step 2676, loss 0.0929794, acc 0.98
2016-09-06T22:18:02.809304: step 2677, loss 0.0587327, acc 0.96
2016-09-06T22:18:03.499408: step 2678, loss 0.0703022, acc 0.96
2016-09-06T22:18:04.201027: step 2679, loss 0.00973569, acc 1
2016-09-06T22:18:04.876269: step 2680, loss 0.0319011, acc 0.98
2016-09-06T22:18:05.595346: step 2681, loss 0.0163357, acc 0.98
2016-09-06T22:18:06.266777: step 2682, loss 0.0682856, acc 0.98
2016-09-06T22:18:06.956277: step 2683, loss 0.122618, acc 0.96
2016-09-06T22:18:07.641292: step 2684, loss 0.00961549, acc 1
2016-09-06T22:18:08.318685: step 2685, loss 0.0497529, acc 0.96
2016-09-06T22:18:08.997604: step 2686, loss 0.0167065, acc 1
2016-09-06T22:18:09.672782: step 2687, loss 0.0242663, acc 0.98
2016-09-06T22:18:10.335715: step 2688, loss 0.0186292, acc 1
2016-09-06T22:18:11.033453: step 2689, loss 0.015664, acc 1
2016-09-06T22:18:11.711429: step 2690, loss 0.00916063, acc 1
2016-09-06T22:18:12.416753: step 2691, loss 0.0368783, acc 0.98
2016-09-06T22:18:13.094947: step 2692, loss 0.00817653, acc 1
2016-09-06T22:18:13.774802: step 2693, loss 0.00170786, acc 1
2016-09-06T22:18:14.462995: step 2694, loss 0.0349665, acc 1
2016-09-06T22:18:15.169318: step 2695, loss 0.0188424, acc 1
2016-09-06T22:18:15.830328: step 2696, loss 0.00545448, acc 1
2016-09-06T22:18:16.502252: step 2697, loss 0.073087, acc 0.96
2016-09-06T22:18:17.182630: step 2698, loss 0.0535929, acc 0.96
2016-09-06T22:18:17.857693: step 2699, loss 0.410903, acc 0.94
2016-09-06T22:18:18.533184: step 2700, loss 0.00171188, acc 1

Evaluation:
2016-09-06T22:18:21.680460: step 2700, loss 1.62318, acc 0.752345

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-2700

2016-09-06T22:18:23.391604: step 2701, loss 0.0988323, acc 0.94
2016-09-06T22:18:24.082481: step 2702, loss 0.0754887, acc 0.98
2016-09-06T22:18:24.785458: step 2703, loss 0.0430506, acc 0.98
2016-09-06T22:18:25.466514: step 2704, loss 0.0137408, acc 1
2016-09-06T22:18:26.132419: step 2705, loss 0.036394, acc 0.98
2016-09-06T22:18:26.818866: step 2706, loss 0.00125912, acc 1
2016-09-06T22:18:27.508355: step 2707, loss 0.0107851, acc 1
2016-09-06T22:18:28.184787: step 2708, loss 0.0653525, acc 0.96
2016-09-06T22:18:28.917638: step 2709, loss 0.112499, acc 0.96
2016-09-06T22:18:29.637132: step 2710, loss 0.034086, acc 0.98
2016-09-06T22:18:30.305818: step 2711, loss 0.0583923, acc 0.96
2016-09-06T22:18:30.986273: step 2712, loss 0.0234685, acc 1
2016-09-06T22:18:31.679263: step 2713, loss 0.0372176, acc 0.98
2016-09-06T22:18:32.362675: step 2714, loss 0.074076, acc 0.96
2016-09-06T22:18:33.049659: step 2715, loss 0.0116843, acc 1
2016-09-06T22:18:33.722917: step 2716, loss 0.0453073, acc 0.98
2016-09-06T22:18:34.425386: step 2717, loss 0.0303279, acc 1
2016-09-06T22:18:35.085198: step 2718, loss 0.0612539, acc 0.98
2016-09-06T22:18:35.771962: step 2719, loss 0.0381424, acc 0.96
2016-09-06T22:18:36.465931: step 2720, loss 0.0472393, acc 0.98
2016-09-06T22:18:37.166338: step 2721, loss 0.0605064, acc 0.98
2016-09-06T22:18:37.848934: step 2722, loss 0.0249454, acc 0.98
2016-09-06T22:18:38.541036: step 2723, loss 0.0121921, acc 1
2016-09-06T22:18:39.238393: step 2724, loss 0.0709829, acc 0.96
2016-09-06T22:18:39.912408: step 2725, loss 0.039648, acc 0.98
2016-09-06T22:18:40.589214: step 2726, loss 0.0399491, acc 1
2016-09-06T22:18:41.283996: step 2727, loss 0.0228901, acc 1
2016-09-06T22:18:41.952918: step 2728, loss 0.0148886, acc 1
2016-09-06T22:18:42.630863: step 2729, loss 0.0261981, acc 1
2016-09-06T22:18:43.307394: step 2730, loss 0.0129402, acc 1
2016-09-06T22:18:43.997366: step 2731, loss 0.0066775, acc 1
2016-09-06T22:18:44.685271: step 2732, loss 0.0380458, acc 1
2016-09-06T22:18:45.365239: step 2733, loss 0.0452603, acc 1
2016-09-06T22:18:46.082817: step 2734, loss 0.0516014, acc 0.98
2016-09-06T22:18:46.758216: step 2735, loss 0.0128398, acc 1
2016-09-06T22:18:47.444103: step 2736, loss 0.0199903, acc 1
2016-09-06T22:18:48.132162: step 2737, loss 0.0156773, acc 1
2016-09-06T22:18:48.849703: step 2738, loss 0.0304367, acc 0.98
2016-09-06T22:18:49.533439: step 2739, loss 0.0490957, acc 0.98
2016-09-06T22:18:50.212837: step 2740, loss 0.0101703, acc 1
2016-09-06T22:18:50.900468: step 2741, loss 0.0324161, acc 0.98
2016-09-06T22:18:51.585746: step 2742, loss 0.0288211, acc 0.98
2016-09-06T22:18:52.285114: step 2743, loss 0.0146944, acc 1
2016-09-06T22:18:52.948445: step 2744, loss 0.0256427, acc 0.98
2016-09-06T22:18:53.644874: step 2745, loss 0.0770285, acc 0.98
2016-09-06T22:18:54.327248: step 2746, loss 0.00108636, acc 1
2016-09-06T22:18:55.025764: step 2747, loss 0.0170677, acc 0.98
2016-09-06T22:18:55.711725: step 2748, loss 0.0113129, acc 1
2016-09-06T22:18:56.395960: step 2749, loss 0.0550228, acc 0.96
2016-09-06T22:18:57.086179: step 2750, loss 0.0032579, acc 1
2016-09-06T22:18:57.754619: step 2751, loss 0.0623684, acc 0.98
2016-09-06T22:18:58.475781: step 2752, loss 0.0532685, acc 0.98
2016-09-06T22:18:59.142540: step 2753, loss 0.0266539, acc 0.98
2016-09-06T22:18:59.870893: step 2754, loss 0.00264184, acc 1
2016-09-06T22:19:00.605386: step 2755, loss 0.0130361, acc 1
2016-09-06T22:19:01.288950: step 2756, loss 0.0397033, acc 0.98
2016-09-06T22:19:01.983695: step 2757, loss 0.0368395, acc 0.98
2016-09-06T22:19:02.660037: step 2758, loss 0.000720367, acc 1
2016-09-06T22:19:03.357790: step 2759, loss 0.00166366, acc 1
2016-09-06T22:19:04.058702: step 2760, loss 0.0339435, acc 0.98
2016-09-06T22:19:04.752721: step 2761, loss 0.042268, acc 0.98
2016-09-06T22:19:05.447465: step 2762, loss 0.0186173, acc 1
2016-09-06T22:19:06.148374: step 2763, loss 0.0264008, acc 0.98
2016-09-06T22:19:06.857958: step 2764, loss 0.128809, acc 0.98
2016-09-06T22:19:07.548642: step 2765, loss 0.0607642, acc 0.96
2016-09-06T22:19:08.223095: step 2766, loss 0.0234487, acc 0.98
2016-09-06T22:19:08.904869: step 2767, loss 0.0472768, acc 0.96
2016-09-06T22:19:09.600841: step 2768, loss 0.0681069, acc 0.98
2016-09-06T22:19:10.287353: step 2769, loss 0.116952, acc 0.96
2016-09-06T22:19:10.972870: step 2770, loss 0.127594, acc 0.96
2016-09-06T22:19:11.676185: step 2771, loss 0.0263975, acc 1
2016-09-06T22:19:12.359310: step 2772, loss 0.265683, acc 0.94
2016-09-06T22:19:13.055215: step 2773, loss 0.0379832, acc 0.98
2016-09-06T22:19:13.730480: step 2774, loss 0.00718957, acc 1
2016-09-06T22:19:14.421003: step 2775, loss 0.0727731, acc 0.96
2016-09-06T22:19:15.105503: step 2776, loss 0.00169567, acc 1
2016-09-06T22:19:15.780441: step 2777, loss 0.0201051, acc 1
2016-09-06T22:19:16.476478: step 2778, loss 0.0268828, acc 1
2016-09-06T22:19:17.150940: step 2779, loss 0.013307, acc 1
2016-09-06T22:19:17.826388: step 2780, loss 0.0373681, acc 1
2016-09-06T22:19:18.514850: step 2781, loss 0.0544289, acc 0.98
2016-09-06T22:19:19.197557: step 2782, loss 0.0156079, acc 1
2016-09-06T22:19:19.891600: step 2783, loss 0.0242722, acc 1
2016-09-06T22:19:20.567188: step 2784, loss 0.0414577, acc 0.98
2016-09-06T22:19:21.279829: step 2785, loss 0.036504, acc 1
2016-09-06T22:19:21.966204: step 2786, loss 0.0332594, acc 0.98
2016-09-06T22:19:22.642012: step 2787, loss 0.0114822, acc 1
2016-09-06T22:19:23.320895: step 2788, loss 0.0564156, acc 0.98
2016-09-06T22:19:24.015981: step 2789, loss 0.0150854, acc 1
2016-09-06T22:19:24.702187: step 2790, loss 0.0552166, acc 0.98
2016-09-06T22:19:25.386528: step 2791, loss 0.0284496, acc 0.98
2016-09-06T22:19:26.096571: step 2792, loss 0.108572, acc 0.98
2016-09-06T22:19:26.802275: step 2793, loss 0.0337938, acc 0.98
2016-09-06T22:19:27.489660: step 2794, loss 0.0209769, acc 1
2016-09-06T22:19:28.187271: step 2795, loss 0.0236115, acc 0.98
2016-09-06T22:19:28.878570: step 2796, loss 0.04949, acc 0.96
2016-09-06T22:19:29.571608: step 2797, loss 0.0775734, acc 0.96
2016-09-06T22:19:30.224615: step 2798, loss 0.0733601, acc 0.96
2016-09-06T22:19:30.963668: step 2799, loss 0.0656117, acc 0.96
2016-09-06T22:19:31.644430: step 2800, loss 0.0629919, acc 0.96

Evaluation:
2016-09-06T22:19:34.777386: step 2800, loss 1.46124, acc 0.751407

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-2800

2016-09-06T22:19:36.457055: step 2801, loss 0.0221013, acc 1
2016-09-06T22:19:37.147533: step 2802, loss 0.0458068, acc 0.96
2016-09-06T22:19:37.819084: step 2803, loss 0.0436536, acc 0.98
2016-09-06T22:19:38.520466: step 2804, loss 0.103064, acc 0.98
2016-09-06T22:19:39.209889: step 2805, loss 0.0292574, acc 0.98
2016-09-06T22:19:39.891286: step 2806, loss 0.0558855, acc 0.94
2016-09-06T22:19:40.603448: step 2807, loss 0.0137247, acc 1
2016-09-06T22:19:41.279931: step 2808, loss 0.180763, acc 0.96
2016-09-06T22:19:41.956374: step 2809, loss 0.0354872, acc 0.96
2016-09-06T22:19:42.629452: step 2810, loss 0.0500643, acc 0.96
2016-09-06T22:19:43.314678: step 2811, loss 0.0095817, acc 1
2016-09-06T22:19:44.009850: step 2812, loss 0.033429, acc 1
2016-09-06T22:19:44.662888: step 2813, loss 0.0661731, acc 0.96
2016-09-06T22:19:45.378799: step 2814, loss 0.06666, acc 0.96
2016-09-06T22:19:46.055356: step 2815, loss 0.0365208, acc 0.96
2016-09-06T22:19:46.731557: step 2816, loss 0.00824466, acc 1
2016-09-06T22:19:47.418260: step 2817, loss 0.014649, acc 1
2016-09-06T22:19:48.098917: step 2818, loss 0.0450698, acc 0.98
2016-09-06T22:19:48.798787: step 2819, loss 0.0790966, acc 0.96
2016-09-06T22:19:49.452750: step 2820, loss 0.168529, acc 0.98
2016-09-06T22:19:50.159146: step 2821, loss 0.0443137, acc 0.98
2016-09-06T22:19:50.833201: step 2822, loss 0.0583181, acc 0.96
2016-09-06T22:19:51.517344: step 2823, loss 0.0309184, acc 0.98
2016-09-06T22:19:52.229312: step 2824, loss 0.0226753, acc 1
2016-09-06T22:19:52.907545: step 2825, loss 0.107702, acc 0.98
2016-09-06T22:19:53.596396: step 2826, loss 0.0213868, acc 1
2016-09-06T22:19:54.283736: step 2827, loss 0.0114324, acc 1
2016-09-06T22:19:54.991015: step 2828, loss 0.0503205, acc 0.98
2016-09-06T22:19:55.670053: step 2829, loss 0.0498293, acc 1
2016-09-06T22:19:56.356711: step 2830, loss 0.0339958, acc 0.98
2016-09-06T22:19:57.047387: step 2831, loss 0.0171756, acc 1
2016-09-06T22:19:57.729202: step 2832, loss 0.0249881, acc 0.98
2016-09-06T22:19:58.422543: step 2833, loss 0.0190917, acc 1
2016-09-06T22:19:59.094193: step 2834, loss 0.0197256, acc 0.98
2016-09-06T22:19:59.806473: step 2835, loss 0.000545941, acc 1
2016-09-06T22:20:00.529358: step 2836, loss 0.157112, acc 0.94
2016-09-06T22:20:01.226186: step 2837, loss 0.0548297, acc 0.96
2016-09-06T22:20:01.943402: step 2838, loss 0.0124092, acc 1
2016-09-06T22:20:02.639184: step 2839, loss 0.0695667, acc 0.96
2016-09-06T22:20:03.363789: step 2840, loss 0.0535093, acc 0.96
2016-09-06T22:20:04.047778: step 2841, loss 0.0170822, acc 0.98
2016-09-06T22:20:04.724491: step 2842, loss 0.0805913, acc 0.94
2016-09-06T22:20:05.429514: step 2843, loss 0.0297191, acc 0.98
2016-09-06T22:20:06.114776: step 2844, loss 0.000154996, acc 1
2016-09-06T22:20:06.811356: step 2845, loss 0.0253031, acc 1
2016-09-06T22:20:07.504575: step 2846, loss 0.0252999, acc 1
2016-09-06T22:20:08.216341: step 2847, loss 0.0908918, acc 0.94
2016-09-06T22:20:08.905345: step 2848, loss 0.061344, acc 0.98
2016-09-06T22:20:09.585023: step 2849, loss 0.00162813, acc 1
2016-09-06T22:20:10.273239: step 2850, loss 0.00989126, acc 1
2016-09-06T22:20:10.964643: step 2851, loss 0.0279661, acc 1
2016-09-06T22:20:11.652994: step 2852, loss 0.0636434, acc 0.98
2016-09-06T22:20:12.313513: step 2853, loss 0.0718066, acc 1
2016-09-06T22:20:12.998517: step 2854, loss 0.0477094, acc 0.96
2016-09-06T22:20:13.670198: step 2855, loss 0.0912252, acc 0.98
2016-09-06T22:20:14.338637: step 2856, loss 0.0419708, acc 0.98
2016-09-06T22:20:15.029625: step 2857, loss 0.0414278, acc 0.98
2016-09-06T22:20:15.703106: step 2858, loss 0.00811497, acc 1
2016-09-06T22:20:16.374410: step 2859, loss 0.00755514, acc 1
2016-09-06T22:20:17.063248: step 2860, loss 0.0150804, acc 1
2016-09-06T22:20:17.769739: step 2861, loss 0.0254393, acc 0.98
2016-09-06T22:20:18.469918: step 2862, loss 0.0572579, acc 0.98
2016-09-06T22:20:19.135599: step 2863, loss 0.0717406, acc 0.98
2016-09-06T22:20:19.812671: step 2864, loss 0.0426433, acc 0.96
2016-09-06T22:20:20.489392: step 2865, loss 0.0345504, acc 1
2016-09-06T22:20:21.168140: step 2866, loss 0.0691243, acc 0.96
2016-09-06T22:20:21.838686: step 2867, loss 0.016784, acc 0.98
2016-09-06T22:20:22.520948: step 2868, loss 0.00534816, acc 1
2016-09-06T22:20:23.172600: step 2869, loss 0.0476328, acc 0.96
2016-09-06T22:20:23.881486: step 2870, loss 0.000534268, acc 1
2016-09-06T22:20:24.575522: step 2871, loss 0.001778, acc 1
2016-09-06T22:20:25.257346: step 2872, loss 0.0346029, acc 1
2016-09-06T22:20:25.934567: step 2873, loss 0.0992979, acc 0.96
2016-09-06T22:20:26.627953: step 2874, loss 0.00534803, acc 1
2016-09-06T22:20:27.342721: step 2875, loss 0.0597268, acc 0.96
2016-09-06T22:20:28.011187: step 2876, loss 0.0033719, acc 1
2016-09-06T22:20:28.719733: step 2877, loss 0.0724502, acc 0.98
2016-09-06T22:20:29.387594: step 2878, loss 0.0186054, acc 0.98
2016-09-06T22:20:30.068931: step 2879, loss 0.00628445, acc 1
2016-09-06T22:20:30.706462: step 2880, loss 0.0180464, acc 0.977273
2016-09-06T22:20:31.408677: step 2881, loss 0.0036367, acc 1
2016-09-06T22:20:32.090499: step 2882, loss 0.039799, acc 0.98
2016-09-06T22:20:32.766088: step 2883, loss 0.052544, acc 0.98
2016-09-06T22:20:33.468331: step 2884, loss 0.0269123, acc 0.98
2016-09-06T22:20:34.139625: step 2885, loss 0.0768614, acc 0.96
2016-09-06T22:20:34.825910: step 2886, loss 0.0213397, acc 1
2016-09-06T22:20:35.513969: step 2887, loss 0.0850335, acc 0.94
2016-09-06T22:20:36.219160: step 2888, loss 0.0352371, acc 1
2016-09-06T22:20:36.914705: step 2889, loss 0.00359564, acc 1
2016-09-06T22:20:37.588602: step 2890, loss 0.0196029, acc 1
2016-09-06T22:20:38.290127: step 2891, loss 0.0391206, acc 0.98
2016-09-06T22:20:38.965365: step 2892, loss 0.0875002, acc 0.98
2016-09-06T22:20:39.646699: step 2893, loss 0.0699036, acc 0.92
2016-09-06T22:20:40.341645: step 2894, loss 0.0138581, acc 1
2016-09-06T22:20:41.046270: step 2895, loss 0.00276002, acc 1
2016-09-06T22:20:41.763061: step 2896, loss 0.00638815, acc 1
2016-09-06T22:20:42.434198: step 2897, loss 0.0487769, acc 0.98
2016-09-06T22:20:43.131891: step 2898, loss 0.0611054, acc 0.94
2016-09-06T22:20:43.815116: step 2899, loss 0.0185804, acc 1
2016-09-06T22:20:44.482702: step 2900, loss 0.0216926, acc 1

Evaluation:
2016-09-06T22:20:47.621864: step 2900, loss 1.7055, acc 0.741088

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-2900

2016-09-06T22:20:49.355505: step 2901, loss 0.0405948, acc 0.98
2016-09-06T22:20:50.033711: step 2902, loss 0.033748, acc 1
2016-09-06T22:20:50.717298: step 2903, loss 0.0184542, acc 0.98
2016-09-06T22:20:51.407022: step 2904, loss 0.00595531, acc 1
2016-09-06T22:20:52.090887: step 2905, loss 0.0494037, acc 0.98
2016-09-06T22:20:52.799448: step 2906, loss 0.0144488, acc 0.98
2016-09-06T22:20:53.485223: step 2907, loss 0.00147239, acc 1
2016-09-06T22:20:54.182021: step 2908, loss 0.0104576, acc 1
2016-09-06T22:20:54.870176: step 2909, loss 0.0351559, acc 0.98
2016-09-06T22:20:55.548653: step 2910, loss 0.0158094, acc 1
2016-09-06T22:20:56.230671: step 2911, loss 0.0052522, acc 1
2016-09-06T22:20:56.924585: step 2912, loss 0.0918406, acc 0.94
2016-09-06T22:20:57.618120: step 2913, loss 0.0910277, acc 0.94
2016-09-06T22:20:58.313987: step 2914, loss 0.0611105, acc 0.96
2016-09-06T22:20:58.999394: step 2915, loss 0.00651811, acc 1
2016-09-06T22:20:59.686980: step 2916, loss 0.126903, acc 0.96
2016-09-06T22:21:00.436840: step 2917, loss 0.0517175, acc 0.98
2016-09-06T22:21:01.139370: step 2918, loss 0.061049, acc 0.96
2016-09-06T22:21:01.809813: step 2919, loss 0.0289184, acc 0.98
2016-09-06T22:21:02.504495: step 2920, loss 0.00320944, acc 1
2016-09-06T22:21:03.202541: step 2921, loss 0.0502527, acc 0.98
2016-09-06T22:21:03.900094: step 2922, loss 0.00535785, acc 1
2016-09-06T22:21:04.573972: step 2923, loss 0.0076013, acc 1
2016-09-06T22:21:05.267002: step 2924, loss 0.0298805, acc 0.98
2016-09-06T22:21:05.967082: step 2925, loss 0.00865849, acc 1
2016-09-06T22:21:06.637041: step 2926, loss 0.0739402, acc 0.96
2016-09-06T22:21:07.330422: step 2927, loss 0.0279341, acc 0.98
2016-09-06T22:21:08.010052: step 2928, loss 0.0242269, acc 1
2016-09-06T22:21:08.686319: step 2929, loss 0.00595101, acc 1
2016-09-06T22:21:09.382924: step 2930, loss 0.0482171, acc 0.98
2016-09-06T22:21:10.034951: step 2931, loss 0.0167679, acc 1
2016-09-06T22:21:10.729494: step 2932, loss 0.0313657, acc 0.98
2016-09-06T22:21:11.400235: step 2933, loss 0.00283505, acc 1
2016-09-06T22:21:12.091832: step 2934, loss 0.0130922, acc 1
2016-09-06T22:21:12.804187: step 2935, loss 0.0124455, acc 1
2016-09-06T22:21:13.506966: step 2936, loss 0.0727709, acc 0.96
2016-09-06T22:21:14.221578: step 2937, loss 0.107918, acc 0.94
2016-09-06T22:21:14.885050: step 2938, loss 0.000229422, acc 1
2016-09-06T22:21:15.577725: step 2939, loss 0.105358, acc 0.96
2016-09-06T22:21:16.271865: step 2940, loss 0.0249602, acc 0.98
2016-09-06T22:21:16.949991: step 2941, loss 0.190499, acc 0.9
2016-09-06T22:21:17.639280: step 2942, loss 0.0416551, acc 0.98
2016-09-06T22:21:18.334200: step 2943, loss 0.0461639, acc 0.98
2016-09-06T22:21:19.019649: step 2944, loss 0.0388554, acc 0.98
2016-09-06T22:21:19.696648: step 2945, loss 0.0325908, acc 0.98
2016-09-06T22:21:20.412057: step 2946, loss 0.00924704, acc 1
2016-09-06T22:21:21.104501: step 2947, loss 0.0263007, acc 1
2016-09-06T22:21:21.794778: step 2948, loss 0.0670834, acc 0.96
2016-09-06T22:21:22.473222: step 2949, loss 0.0643053, acc 0.98
2016-09-06T22:21:23.159749: step 2950, loss 0.000461331, acc 1
2016-09-06T22:21:23.884433: step 2951, loss 0.126582, acc 0.98
2016-09-06T22:21:24.573773: step 2952, loss 0.00808672, acc 1
2016-09-06T22:21:25.272539: step 2953, loss 0.0587834, acc 0.96
2016-09-06T22:21:25.976838: step 2954, loss 0.0255867, acc 0.98
2016-09-06T22:21:26.688172: step 2955, loss 0.0496975, acc 0.96
2016-09-06T22:21:27.404644: step 2956, loss 0.00135089, acc 1
2016-09-06T22:21:28.076249: step 2957, loss 0.0328724, acc 0.98
2016-09-06T22:21:28.789316: step 2958, loss 0.0438071, acc 0.98
2016-09-06T22:21:29.469694: step 2959, loss 0.00435817, acc 1
2016-09-06T22:21:30.141199: step 2960, loss 0.0450173, acc 0.98
2016-09-06T22:21:30.836909: step 2961, loss 0.0586739, acc 0.98
2016-09-06T22:21:31.542898: step 2962, loss 0.0449274, acc 0.98
2016-09-06T22:21:32.251259: step 2963, loss 0.0305788, acc 1
2016-09-06T22:21:32.915716: step 2964, loss 0.0337244, acc 0.98
2016-09-06T22:21:33.603198: step 2965, loss 0.00819466, acc 1
2016-09-06T22:21:34.281336: step 2966, loss 0.0111573, acc 1
2016-09-06T22:21:34.981105: step 2967, loss 0.0479815, acc 0.98
2016-09-06T22:21:35.660953: step 2968, loss 0.0498578, acc 0.98
2016-09-06T22:21:36.342382: step 2969, loss 0.0283962, acc 0.98
2016-09-06T22:21:37.043162: step 2970, loss 0.0119796, acc 1
2016-09-06T22:21:37.718478: step 2971, loss 0.00583974, acc 1
2016-09-06T22:21:38.433230: step 2972, loss 0.0710269, acc 0.96
2016-09-06T22:21:39.104349: step 2973, loss 0.0136118, acc 1
2016-09-06T22:21:39.779067: step 2974, loss 0.0679748, acc 0.98
2016-09-06T22:21:40.481849: step 2975, loss 0.0523895, acc 0.98
2016-09-06T22:21:41.185968: step 2976, loss 0.12924, acc 0.98
2016-09-06T22:21:41.903738: step 2977, loss 0.064094, acc 0.96
2016-09-06T22:21:42.584179: step 2978, loss 0.151381, acc 0.96
2016-09-06T22:21:43.319106: step 2979, loss 0.104609, acc 0.96
2016-09-06T22:21:44.014185: step 2980, loss 0.0796276, acc 0.98
2016-09-06T22:21:44.712444: step 2981, loss 0.0626829, acc 0.96
2016-09-06T22:21:45.406657: step 2982, loss 0.030988, acc 0.98
2016-09-06T22:21:46.105179: step 2983, loss 0.0472923, acc 0.96
2016-09-06T22:21:46.826960: step 2984, loss 0.0727557, acc 0.98
2016-09-06T22:21:47.532020: step 2985, loss 0.102024, acc 0.96
2016-09-06T22:21:48.228271: step 2986, loss 0.00851786, acc 1
2016-09-06T22:21:48.913821: step 2987, loss 0.0317238, acc 1
2016-09-06T22:21:49.596346: step 2988, loss 0.0199021, acc 1
2016-09-06T22:21:50.309098: step 2989, loss 0.0395918, acc 1
2016-09-06T22:21:50.971410: step 2990, loss 0.0358998, acc 0.98
2016-09-06T22:21:51.677012: step 2991, loss 0.0055663, acc 1
2016-09-06T22:21:52.380478: step 2992, loss 0.154135, acc 0.96
2016-09-06T22:21:53.034099: step 2993, loss 0.0383288, acc 0.98
2016-09-06T22:21:53.720176: step 2994, loss 0.094485, acc 0.98
2016-09-06T22:21:54.418555: step 2995, loss 0.0433913, acc 1
2016-09-06T22:21:55.114714: step 2996, loss 0.000827715, acc 1
2016-09-06T22:21:55.779392: step 2997, loss 0.0246617, acc 0.98
2016-09-06T22:21:56.503261: step 2998, loss 0.0654857, acc 0.98
2016-09-06T22:21:57.179301: step 2999, loss 0.0329176, acc 0.98
2016-09-06T22:21:57.862005: step 3000, loss 0.118885, acc 0.94

Evaluation:
2016-09-06T22:22:01.050076: step 3000, loss 1.50846, acc 0.760788

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-3000

2016-09-06T22:22:02.830579: step 3001, loss 0.034108, acc 0.98
2016-09-06T22:22:03.525060: step 3002, loss 0.0314379, acc 0.98
2016-09-06T22:22:04.187709: step 3003, loss 0.069429, acc 0.96
2016-09-06T22:22:04.875507: step 3004, loss 0.0632946, acc 0.96
2016-09-06T22:22:05.545400: step 3005, loss 0.0473618, acc 0.96
2016-09-06T22:22:06.238050: step 3006, loss 0.0423664, acc 0.98
2016-09-06T22:22:06.922069: step 3007, loss 0.0900305, acc 0.96
2016-09-06T22:22:07.614736: step 3008, loss 0.0863565, acc 0.96
2016-09-06T22:22:08.302010: step 3009, loss 0.0349187, acc 1
2016-09-06T22:22:08.955292: step 3010, loss 0.0258288, acc 0.98
2016-09-06T22:22:09.660206: step 3011, loss 0.128076, acc 0.92
2016-09-06T22:22:10.333813: step 3012, loss 0.036069, acc 1
2016-09-06T22:22:11.025139: step 3013, loss 0.0718524, acc 0.98
2016-09-06T22:22:11.739827: step 3014, loss 0.104661, acc 0.96
2016-09-06T22:22:12.400439: step 3015, loss 0.0154351, acc 1
2016-09-06T22:22:13.091164: step 3016, loss 0.0381316, acc 0.98
2016-09-06T22:22:13.743674: step 3017, loss 0.0249401, acc 1
2016-09-06T22:22:14.478384: step 3018, loss 0.0205978, acc 1
2016-09-06T22:22:15.149825: step 3019, loss 0.0113269, acc 1
2016-09-06T22:22:15.830105: step 3020, loss 0.043709, acc 0.98
2016-09-06T22:22:16.529283: step 3021, loss 0.166698, acc 0.9
2016-09-06T22:22:17.221462: step 3022, loss 0.0555434, acc 0.98
2016-09-06T22:22:17.903753: step 3023, loss 0.0243395, acc 0.98
2016-09-06T22:22:18.579833: step 3024, loss 0.0158282, acc 1
2016-09-06T22:22:19.300061: step 3025, loss 0.0258166, acc 1
2016-09-06T22:22:19.966978: step 3026, loss 0.044388, acc 0.98
2016-09-06T22:22:20.665998: step 3027, loss 0.0264596, acc 0.98
2016-09-06T22:22:21.353064: step 3028, loss 0.116279, acc 0.94
2016-09-06T22:22:22.047794: step 3029, loss 0.0687437, acc 0.96
2016-09-06T22:22:22.764314: step 3030, loss 0.022704, acc 1
2016-09-06T22:22:23.437463: step 3031, loss 0.011002, acc 1
2016-09-06T22:22:24.132733: step 3032, loss 0.113893, acc 0.94
2016-09-06T22:22:24.802158: step 3033, loss 0.0108347, acc 1
2016-09-06T22:22:25.481265: step 3034, loss 0.0113291, acc 1
2016-09-06T22:22:26.177690: step 3035, loss 0.0161196, acc 1
2016-09-06T22:22:26.849016: step 3036, loss 0.0261982, acc 1
2016-09-06T22:22:27.534406: step 3037, loss 0.0031406, acc 1
2016-09-06T22:22:28.200085: step 3038, loss 0.029918, acc 0.98
2016-09-06T22:22:28.903690: step 3039, loss 0.0403947, acc 0.98
2016-09-06T22:22:29.563604: step 3040, loss 0.0270201, acc 1
2016-09-06T22:22:30.248972: step 3041, loss 0.0209941, acc 0.98
2016-09-06T22:22:30.935317: step 3042, loss 0.0499486, acc 0.96
2016-09-06T22:22:31.617598: step 3043, loss 0.0285465, acc 1
2016-09-06T22:22:32.309408: step 3044, loss 0.0197434, acc 0.98
2016-09-06T22:22:32.991548: step 3045, loss 0.0581834, acc 0.96
2016-09-06T22:22:33.691720: step 3046, loss 0.0562933, acc 0.96
2016-09-06T22:22:34.375478: step 3047, loss 0.0454789, acc 0.96
2016-09-06T22:22:35.063443: step 3048, loss 0.0422721, acc 0.96
2016-09-06T22:22:35.773254: step 3049, loss 0.0427178, acc 0.98
2016-09-06T22:22:36.458234: step 3050, loss 0.0301004, acc 0.98
2016-09-06T22:22:37.167740: step 3051, loss 0.0125141, acc 1
2016-09-06T22:22:37.832306: step 3052, loss 0.0222248, acc 1
2016-09-06T22:22:38.558011: step 3053, loss 0.00546264, acc 1
2016-09-06T22:22:39.244419: step 3054, loss 0.0511932, acc 0.98
2016-09-06T22:22:39.936454: step 3055, loss 0.00343242, acc 1
2016-09-06T22:22:40.627500: step 3056, loss 0.0518405, acc 0.96
2016-09-06T22:22:41.318286: step 3057, loss 0.00661982, acc 1
2016-09-06T22:22:42.006151: step 3058, loss 0.0615703, acc 0.94
2016-09-06T22:22:42.675600: step 3059, loss 0.0191963, acc 1
2016-09-06T22:22:43.393946: step 3060, loss 0.0209743, acc 1
2016-09-06T22:22:44.083864: step 3061, loss 0.00841954, acc 1
2016-09-06T22:22:44.771963: step 3062, loss 0.0414285, acc 0.98
2016-09-06T22:22:45.474769: step 3063, loss 0.00171132, acc 1
2016-09-06T22:22:46.173512: step 3064, loss 0.112176, acc 0.96
2016-09-06T22:22:46.874023: step 3065, loss 0.0256706, acc 0.98
2016-09-06T22:22:47.553434: step 3066, loss 0.0333734, acc 0.98
2016-09-06T22:22:48.247663: step 3067, loss 0.0163647, acc 0.98
2016-09-06T22:22:48.921053: step 3068, loss 0.0204217, acc 0.98
2016-09-06T22:22:49.607101: step 3069, loss 0.0545835, acc 0.96
2016-09-06T22:22:50.277491: step 3070, loss 0.0563008, acc 0.96
2016-09-06T22:22:50.970854: step 3071, loss 0.0557036, acc 0.96
2016-09-06T22:22:51.621894: step 3072, loss 0.144991, acc 0.931818
2016-09-06T22:22:52.288642: step 3073, loss 0.0138085, acc 1
2016-09-06T22:22:52.992022: step 3074, loss 0.0158554, acc 1
2016-09-06T22:22:53.666576: step 3075, loss 0.0378908, acc 1
2016-09-06T22:22:54.341269: step 3076, loss 0.0643933, acc 0.96
2016-09-06T22:22:55.014197: step 3077, loss 0.0285855, acc 0.98
2016-09-06T22:22:55.699545: step 3078, loss 0.0524575, acc 0.96
2016-09-06T22:22:56.386299: step 3079, loss 0.0156327, acc 1
2016-09-06T22:22:57.049035: step 3080, loss 0.0525485, acc 0.96
2016-09-06T22:22:57.765586: step 3081, loss 0.0242728, acc 1
2016-09-06T22:22:58.446638: step 3082, loss 0.00733233, acc 1
2016-09-06T22:22:59.142993: step 3083, loss 0.019484, acc 1
2016-09-06T22:22:59.831010: step 3084, loss 0.0175809, acc 1
2016-09-06T22:23:00.543700: step 3085, loss 0.0343046, acc 0.98
2016-09-06T22:23:01.243826: step 3086, loss 0.0630989, acc 0.96
2016-09-06T22:23:01.912707: step 3087, loss 0.00940676, acc 1
2016-09-06T22:23:02.623543: step 3088, loss 0.00203317, acc 1
2016-09-06T22:23:03.304199: step 3089, loss 0.00826801, acc 1
2016-09-06T22:23:03.993334: step 3090, loss 0.0210424, acc 0.98
2016-09-06T22:23:04.689800: step 3091, loss 0.0329497, acc 0.98
2016-09-06T22:23:05.372625: step 3092, loss 0.0314044, acc 0.96
2016-09-06T22:23:06.085437: step 3093, loss 0.193041, acc 0.96
2016-09-06T22:23:06.765127: step 3094, loss 0.121174, acc 0.94
2016-09-06T22:23:07.455560: step 3095, loss 0.00745053, acc 1
2016-09-06T22:23:08.136251: step 3096, loss 0.00272855, acc 1
2016-09-06T22:23:08.812419: step 3097, loss 0.0271086, acc 0.98
2016-09-06T22:23:09.493509: step 3098, loss 0.20381, acc 0.94
2016-09-06T22:23:10.178712: step 3099, loss 0.0532433, acc 0.96
2016-09-06T22:23:10.874288: step 3100, loss 0.0098612, acc 1

Evaluation:
2016-09-06T22:23:14.013110: step 3100, loss 1.74765, acc 0.747655

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-3100

2016-09-06T22:23:15.648740: step 3101, loss 0.0471393, acc 0.98
2016-09-06T22:23:16.305950: step 3102, loss 0.01407, acc 1
2016-09-06T22:23:17.011823: step 3103, loss 0.00801585, acc 1
2016-09-06T22:23:17.687938: step 3104, loss 0.0195771, acc 1
2016-09-06T22:23:18.377830: step 3105, loss 0.0241818, acc 1
2016-09-06T22:23:19.059156: step 3106, loss 0.0482482, acc 1
2016-09-06T22:23:19.736251: step 3107, loss 0.0231481, acc 0.98
2016-09-06T22:23:20.427490: step 3108, loss 0.0355746, acc 0.98
2016-09-06T22:23:21.097830: step 3109, loss 0.0194027, acc 1
2016-09-06T22:23:21.790828: step 3110, loss 0.0259176, acc 1
2016-09-06T22:23:22.474326: step 3111, loss 0.0476873, acc 0.98
2016-09-06T22:23:23.170255: step 3112, loss 0.0377297, acc 0.98
2016-09-06T22:23:23.862002: step 3113, loss 0.0375579, acc 0.96
2016-09-06T22:23:24.579411: step 3114, loss 0.0388494, acc 0.98
2016-09-06T22:23:25.273974: step 3115, loss 0.0759583, acc 0.96
2016-09-06T22:23:25.947799: step 3116, loss 0.065583, acc 0.96
2016-09-06T22:23:26.653843: step 3117, loss 0.0812791, acc 0.98
2016-09-06T22:23:27.356776: step 3118, loss 0.062458, acc 0.98
2016-09-06T22:23:28.040660: step 3119, loss 0.00974687, acc 1
2016-09-06T22:23:28.713632: step 3120, loss 0.0441077, acc 0.98
2016-09-06T22:23:29.396520: step 3121, loss 0.00907653, acc 1
2016-09-06T22:23:30.095016: step 3122, loss 0.064371, acc 0.96
2016-09-06T22:23:30.775891: step 3123, loss 0.0851793, acc 0.98
2016-09-06T22:23:31.485212: step 3124, loss 0.0276814, acc 1
2016-09-06T22:23:32.193898: step 3125, loss 0.0132125, acc 1
2016-09-06T22:23:32.897965: step 3126, loss 0.0217051, acc 1
2016-09-06T22:23:33.575715: step 3127, loss 0.0282672, acc 1
2016-09-06T22:23:34.259959: step 3128, loss 0.00723946, acc 1
2016-09-06T22:23:34.984725: step 3129, loss 0.0181518, acc 1
2016-09-06T22:23:35.654118: step 3130, loss 0.00347553, acc 1
2016-09-06T22:23:36.330704: step 3131, loss 0.0101311, acc 1
2016-09-06T22:23:37.023600: step 3132, loss 0.0266381, acc 1
2016-09-06T22:23:37.707542: step 3133, loss 0.0363641, acc 1
2016-09-06T22:23:38.373292: step 3134, loss 0.0889002, acc 0.96
2016-09-06T22:23:39.063420: step 3135, loss 0.075247, acc 0.94
2016-09-06T22:23:39.759104: step 3136, loss 0.0399351, acc 0.98
2016-09-06T22:23:40.433521: step 3137, loss 0.0261626, acc 0.98
2016-09-06T22:23:41.132096: step 3138, loss 0.0034981, acc 1
2016-09-06T22:23:41.823529: step 3139, loss 0.0382399, acc 0.96
2016-09-06T22:23:42.506803: step 3140, loss 0.0089417, acc 1
2016-09-06T22:23:43.186620: step 3141, loss 0.0515144, acc 0.96
2016-09-06T22:23:43.883093: step 3142, loss 0.0145379, acc 1
2016-09-06T22:23:44.585830: step 3143, loss 0.112747, acc 0.94
2016-09-06T22:23:45.264447: step 3144, loss 0.00893853, acc 1
2016-09-06T22:23:45.947053: step 3145, loss 0.0165858, acc 1
2016-09-06T22:23:46.604283: step 3146, loss 0.0707317, acc 0.96
2016-09-06T22:23:47.283198: step 3147, loss 0.0193751, acc 1
2016-09-06T22:23:47.957288: step 3148, loss 0.0285231, acc 0.98
2016-09-06T22:23:48.636013: step 3149, loss 0.0247012, acc 1
2016-09-06T22:23:49.336781: step 3150, loss 0.0490489, acc 0.98
2016-09-06T22:23:50.024818: step 3151, loss 0.00288305, acc 1
2016-09-06T22:23:50.728412: step 3152, loss 0.0464468, acc 0.98
2016-09-06T22:23:51.429466: step 3153, loss 0.110584, acc 0.96
2016-09-06T22:23:52.122177: step 3154, loss 0.0204746, acc 0.98
2016-09-06T22:23:52.797879: step 3155, loss 0.0288255, acc 0.98
2016-09-06T22:23:53.475377: step 3156, loss 0.021894, acc 0.98
2016-09-06T22:23:54.166683: step 3157, loss 0.133066, acc 0.94
2016-09-06T22:23:54.820279: step 3158, loss 0.0655395, acc 0.96
2016-09-06T22:23:55.536019: step 3159, loss 0.0540106, acc 0.96
2016-09-06T22:23:56.261898: step 3160, loss 0.0938474, acc 0.98
2016-09-06T22:23:56.957694: step 3161, loss 0.046772, acc 0.96
2016-09-06T22:23:57.648239: step 3162, loss 0.0371482, acc 0.96
2016-09-06T22:23:58.351967: step 3163, loss 0.0235965, acc 0.98
2016-09-06T22:23:59.080488: step 3164, loss 0.00723881, acc 1
2016-09-06T22:23:59.769863: step 3165, loss 0.0547164, acc 0.98
2016-09-06T22:24:00.496009: step 3166, loss 0.00658072, acc 1
2016-09-06T22:24:01.190862: step 3167, loss 0.0591683, acc 0.96
2016-09-06T22:24:01.868855: step 3168, loss 0.00656949, acc 1
2016-09-06T22:24:02.583493: step 3169, loss 0.00611992, acc 1
2016-09-06T22:24:03.258714: step 3170, loss 0.049597, acc 0.98
2016-09-06T22:24:03.966588: step 3171, loss 0.1095, acc 0.94
2016-09-06T22:24:04.638682: step 3172, loss 0.0594909, acc 0.98
2016-09-06T22:24:05.315192: step 3173, loss 0.0780535, acc 0.96
2016-09-06T22:24:06.006592: step 3174, loss 0.0193359, acc 0.98
2016-09-06T22:24:06.692648: step 3175, loss 0.0269729, acc 0.98
2016-09-06T22:24:07.390501: step 3176, loss 0.119696, acc 0.94
2016-09-06T22:24:08.060739: step 3177, loss 0.0253098, acc 1
2016-09-06T22:24:08.782849: step 3178, loss 0.0679227, acc 0.94
2016-09-06T22:24:09.515793: step 3179, loss 0.0324527, acc 1
2016-09-06T22:24:10.209437: step 3180, loss 0.0263487, acc 0.98
2016-09-06T22:24:10.913474: step 3181, loss 0.0459741, acc 0.98
2016-09-06T22:24:11.585257: step 3182, loss 0.10501, acc 0.94
2016-09-06T22:24:12.303499: step 3183, loss 0.0216677, acc 1
2016-09-06T22:24:12.981738: step 3184, loss 0.179917, acc 0.94
2016-09-06T22:24:13.667651: step 3185, loss 0.013995, acc 1
2016-09-06T22:24:14.357703: step 3186, loss 0.0256844, acc 1
2016-09-06T22:24:15.039838: step 3187, loss 0.208174, acc 0.94
2016-09-06T22:24:15.723035: step 3188, loss 0.0222818, acc 0.98
2016-09-06T22:24:16.381759: step 3189, loss 0.0577695, acc 0.96
2016-09-06T22:24:17.082108: step 3190, loss 0.0334089, acc 0.98
2016-09-06T22:24:17.770888: step 3191, loss 0.0651818, acc 0.98
2016-09-06T22:24:18.463512: step 3192, loss 0.0983132, acc 0.96
2016-09-06T22:24:19.145923: step 3193, loss 0.0610883, acc 0.98
2016-09-06T22:24:19.837694: step 3194, loss 0.0448121, acc 0.98
2016-09-06T22:24:20.535098: step 3195, loss 0.0350527, acc 1
2016-09-06T22:24:21.189992: step 3196, loss 0.0282665, acc 0.98
2016-09-06T22:24:21.879542: step 3197, loss 0.0138841, acc 1
2016-09-06T22:24:22.545860: step 3198, loss 0.0600334, acc 0.96
2016-09-06T22:24:23.216239: step 3199, loss 0.0844605, acc 0.96
2016-09-06T22:24:23.900135: step 3200, loss 0.0287397, acc 1

Evaluation:
2016-09-06T22:24:27.079217: step 3200, loss 1.46518, acc 0.753283

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-3200

2016-09-06T22:24:28.730296: step 3201, loss 0.00849542, acc 1
2016-09-06T22:24:29.415516: step 3202, loss 0.0871409, acc 0.96
2016-09-06T22:24:30.105924: step 3203, loss 0.0209622, acc 0.98
2016-09-06T22:24:30.803464: step 3204, loss 0.0873259, acc 0.96
2016-09-06T22:24:31.513155: step 3205, loss 0.0949565, acc 0.96
2016-09-06T22:24:32.177877: step 3206, loss 0.0240815, acc 0.98
2016-09-06T22:24:32.851129: step 3207, loss 0.00383176, acc 1
2016-09-06T22:24:33.530635: step 3208, loss 0.0170842, acc 1
2016-09-06T22:24:34.199346: step 3209, loss 0.161952, acc 0.94
2016-09-06T22:24:34.910987: step 3210, loss 0.00373534, acc 1
2016-09-06T22:24:35.616114: step 3211, loss 0.0848081, acc 0.96
2016-09-06T22:24:36.318282: step 3212, loss 0.0454528, acc 0.98
2016-09-06T22:24:36.983657: step 3213, loss 0.0300968, acc 1
2016-09-06T22:24:37.651940: step 3214, loss 0.0133384, acc 1
2016-09-06T22:24:38.340775: step 3215, loss 0.0513138, acc 0.96
2016-09-06T22:24:39.038513: step 3216, loss 0.0641495, acc 0.96
2016-09-06T22:24:39.727544: step 3217, loss 0.0459112, acc 1
2016-09-06T22:24:40.405523: step 3218, loss 0.0206765, acc 0.98
2016-09-06T22:24:41.105551: step 3219, loss 0.0378926, acc 0.96
2016-09-06T22:24:41.775964: step 3220, loss 0.0252587, acc 0.98
2016-09-06T22:24:42.504516: step 3221, loss 0.0482256, acc 0.98
2016-09-06T22:24:43.196640: step 3222, loss 0.023594, acc 0.98
2016-09-06T22:24:43.873660: step 3223, loss 0.0343464, acc 0.98
2016-09-06T22:24:44.585058: step 3224, loss 0.0337034, acc 1
2016-09-06T22:24:45.258004: step 3225, loss 0.00758229, acc 1
2016-09-06T22:24:45.963577: step 3226, loss 0.0516296, acc 0.96
2016-09-06T22:24:46.615721: step 3227, loss 0.0130594, acc 1
2016-09-06T22:24:47.279881: step 3228, loss 0.0317307, acc 0.98
2016-09-06T22:24:47.968328: step 3229, loss 0.00328211, acc 1
2016-09-06T22:24:48.666681: step 3230, loss 0.0401621, acc 0.98
2016-09-06T22:24:49.370982: step 3231, loss 0.00902898, acc 1
2016-09-06T22:24:50.043046: step 3232, loss 0.0225733, acc 1
2016-09-06T22:24:50.747587: step 3233, loss 0.0539065, acc 0.98
2016-09-06T22:24:51.425842: step 3234, loss 0.0186948, acc 1
2016-09-06T22:24:52.111621: step 3235, loss 0.0267926, acc 0.98
2016-09-06T22:24:52.804002: step 3236, loss 0.00209872, acc 1
2016-09-06T22:24:53.504787: step 3237, loss 0.055058, acc 0.96
2016-09-06T22:24:54.185841: step 3238, loss 0.0594029, acc 0.98
2016-09-06T22:24:54.851643: step 3239, loss 0.0301439, acc 0.98
2016-09-06T22:24:55.577965: step 3240, loss 0.0734849, acc 0.96
2016-09-06T22:24:56.299761: step 3241, loss 0.0184612, acc 1
2016-09-06T22:24:56.983005: step 3242, loss 0.127739, acc 0.96
2016-09-06T22:24:57.691650: step 3243, loss 0.0351146, acc 0.96
2016-09-06T22:24:58.377481: step 3244, loss 0.0171263, acc 1
2016-09-06T22:24:59.102120: step 3245, loss 0.00449356, acc 1
2016-09-06T22:24:59.787451: step 3246, loss 0.0256931, acc 0.98
2016-09-06T22:25:00.520957: step 3247, loss 0.0151866, acc 1
2016-09-06T22:25:01.221693: step 3248, loss 0.0150486, acc 1
2016-09-06T22:25:01.921024: step 3249, loss 0.011114, acc 1
2016-09-06T22:25:02.604400: step 3250, loss 0.0160035, acc 1
2016-09-06T22:25:03.259017: step 3251, loss 0.0442784, acc 0.98
2016-09-06T22:25:03.950800: step 3252, loss 0.00906954, acc 1
2016-09-06T22:25:04.621595: step 3253, loss 0.00266124, acc 1
2016-09-06T22:25:05.292352: step 3254, loss 0.253387, acc 0.96
2016-09-06T22:25:05.978800: step 3255, loss 0.0104147, acc 1
2016-09-06T22:25:06.659486: step 3256, loss 0.0665011, acc 0.96
2016-09-06T22:25:07.335044: step 3257, loss 0.0129992, acc 1
2016-09-06T22:25:08.011941: step 3258, loss 0.0195496, acc 1
2016-09-06T22:25:08.713474: step 3259, loss 0.106835, acc 0.98
2016-09-06T22:25:09.383008: step 3260, loss 0.00952808, acc 1
2016-09-06T22:25:10.081210: step 3261, loss 0.0378195, acc 0.98
2016-09-06T22:25:10.767285: step 3262, loss 0.029243, acc 1
2016-09-06T22:25:11.445596: step 3263, loss 0.100032, acc 0.92
2016-09-06T22:25:12.105662: step 3264, loss 0.00173371, acc 1
2016-09-06T22:25:12.819708: step 3265, loss 0.0554112, acc 0.96
2016-09-06T22:25:13.525713: step 3266, loss 0.118366, acc 0.94
2016-09-06T22:25:14.198362: step 3267, loss 0.0374825, acc 0.98
2016-09-06T22:25:14.910471: step 3268, loss 0.0189402, acc 0.98
2016-09-06T22:25:15.589868: step 3269, loss 0.0800653, acc 0.96
2016-09-06T22:25:16.294851: step 3270, loss 0.0423714, acc 0.98
2016-09-06T22:25:17.018180: step 3271, loss 0.00514608, acc 1
2016-09-06T22:25:17.700163: step 3272, loss 0.0623278, acc 0.98
2016-09-06T22:25:18.394497: step 3273, loss 0.00950992, acc 1
2016-09-06T22:25:19.068096: step 3274, loss 0.0136936, acc 1
2016-09-06T22:25:19.784049: step 3275, loss 0.010272, acc 1
2016-09-06T22:25:20.449906: step 3276, loss 0.0388427, acc 0.98
2016-09-06T22:25:21.138629: step 3277, loss 0.0335791, acc 0.98
2016-09-06T22:25:21.830350: step 3278, loss 0.0221309, acc 0.98
2016-09-06T22:25:22.488002: step 3279, loss 0.00152899, acc 1
2016-09-06T22:25:23.210460: step 3280, loss 0.0161359, acc 1
2016-09-06T22:25:23.910543: step 3281, loss 0.012206, acc 1
2016-09-06T22:25:24.595744: step 3282, loss 0.0163617, acc 0.98
2016-09-06T22:25:25.273790: step 3283, loss 0.020413, acc 0.98
2016-09-06T22:25:25.963180: step 3284, loss 0.0137158, acc 1
2016-09-06T22:25:26.651301: step 3285, loss 0.016547, acc 1
2016-09-06T22:25:27.308329: step 3286, loss 0.023433, acc 0.98
2016-09-06T22:25:28.004650: step 3287, loss 0.00391834, acc 1
2016-09-06T22:25:28.673151: step 3288, loss 0.110288, acc 0.94
2016-09-06T22:25:29.380661: step 3289, loss 0.0232866, acc 1
2016-09-06T22:25:30.075644: step 3290, loss 0.0172225, acc 0.98
2016-09-06T22:25:30.766056: step 3291, loss 0.00492106, acc 1
2016-09-06T22:25:31.444795: step 3292, loss 0.0133364, acc 1
2016-09-06T22:25:32.123309: step 3293, loss 0.020225, acc 1
2016-09-06T22:25:32.827792: step 3294, loss 0.0324385, acc 1
2016-09-06T22:25:33.500150: step 3295, loss 0.0215367, acc 0.98
2016-09-06T22:25:34.190226: step 3296, loss 0.0459608, acc 0.98
2016-09-06T22:25:34.887530: step 3297, loss 0.0125879, acc 1
2016-09-06T22:25:35.589650: step 3298, loss 0.0191696, acc 0.98
2016-09-06T22:25:36.268664: step 3299, loss 0.0775237, acc 0.96
2016-09-06T22:25:36.924675: step 3300, loss 0.0458429, acc 0.98

Evaluation:
2016-09-06T22:25:40.059876: step 3300, loss 1.90729, acc 0.744841

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-3300

2016-09-06T22:25:41.729599: step 3301, loss 0.0114604, acc 1
2016-09-06T22:25:42.431382: step 3302, loss 0.0468015, acc 0.98
2016-09-06T22:25:43.103586: step 3303, loss 0.252564, acc 0.98
2016-09-06T22:25:43.838125: step 3304, loss 0.00301264, acc 1
2016-09-06T22:25:44.524224: step 3305, loss 0.0340674, acc 0.98
2016-09-06T22:25:45.212478: step 3306, loss 0.0178277, acc 1
2016-09-06T22:25:45.893692: step 3307, loss 0.0204664, acc 1
2016-09-06T22:25:46.590085: step 3308, loss 0.0117242, acc 1
2016-09-06T22:25:47.299152: step 3309, loss 0.0221808, acc 0.98
2016-09-06T22:25:47.978423: step 3310, loss 0.0653618, acc 0.98
2016-09-06T22:25:48.687647: step 3311, loss 0.0219238, acc 1
2016-09-06T22:25:49.366682: step 3312, loss 0.012665, acc 1
2016-09-06T22:25:50.059938: step 3313, loss 0.00491506, acc 1
2016-09-06T22:25:50.747235: step 3314, loss 0.0546817, acc 0.96
2016-09-06T22:25:51.426716: step 3315, loss 0.00311626, acc 1
2016-09-06T22:25:52.141137: step 3316, loss 0.0173649, acc 1
2016-09-06T22:25:52.854394: step 3317, loss 0.00991794, acc 1
2016-09-06T22:25:53.553747: step 3318, loss 0.067025, acc 0.96
2016-09-06T22:25:54.231205: step 3319, loss 0.0390485, acc 0.96
2016-09-06T22:25:54.901505: step 3320, loss 0.0112974, acc 1
2016-09-06T22:25:55.610903: step 3321, loss 0.0337442, acc 0.98
2016-09-06T22:25:56.282810: step 3322, loss 0.0624859, acc 0.96
2016-09-06T22:25:56.976911: step 3323, loss 0.0253051, acc 1
2016-09-06T22:25:57.650330: step 3324, loss 0.0213969, acc 0.98
2016-09-06T22:25:58.337642: step 3325, loss 0.0410921, acc 0.98
2016-09-06T22:25:59.015982: step 3326, loss 0.024331, acc 0.98
2016-09-06T22:25:59.699234: step 3327, loss 0.0444935, acc 0.96
2016-09-06T22:26:00.405414: step 3328, loss 0.0166309, acc 0.98
2016-09-06T22:26:01.068277: step 3329, loss 0.00415642, acc 1
2016-09-06T22:26:01.764720: step 3330, loss 0.0649868, acc 0.96
2016-09-06T22:26:02.456170: step 3331, loss 0.0351586, acc 0.98
2016-09-06T22:26:03.132060: step 3332, loss 0.0191122, acc 1
2016-09-06T22:26:03.817460: step 3333, loss 0.0237969, acc 0.98
2016-09-06T22:26:04.517702: step 3334, loss 0.00295724, acc 1
2016-09-06T22:26:05.219975: step 3335, loss 0.125622, acc 0.94
2016-09-06T22:26:05.888469: step 3336, loss 0.0193709, acc 1
2016-09-06T22:26:06.594858: step 3337, loss 0.136903, acc 0.96
2016-09-06T22:26:07.274622: step 3338, loss 0.0240439, acc 0.98
2016-09-06T22:26:07.970290: step 3339, loss 0.00946081, acc 1
2016-09-06T22:26:08.650626: step 3340, loss 0.0187169, acc 1
2016-09-06T22:26:09.327575: step 3341, loss 0.158459, acc 0.94
2016-09-06T22:26:10.022203: step 3342, loss 0.0249477, acc 1
2016-09-06T22:26:10.698575: step 3343, loss 0.00228758, acc 1
2016-09-06T22:26:11.414914: step 3344, loss 0.00310277, acc 1
2016-09-06T22:26:12.076514: step 3345, loss 0.0947011, acc 0.98
2016-09-06T22:26:12.757578: step 3346, loss 0.0856167, acc 0.98
2016-09-06T22:26:13.444512: step 3347, loss 0.0312887, acc 0.98
2016-09-06T22:26:14.137091: step 3348, loss 0.0277453, acc 0.98
2016-09-06T22:26:14.821324: step 3349, loss 0.0199291, acc 0.98
2016-09-06T22:26:15.496974: step 3350, loss 0.0903197, acc 0.96
2016-09-06T22:26:16.183571: step 3351, loss 0.0588597, acc 0.96
2016-09-06T22:26:16.891781: step 3352, loss 0.0942051, acc 0.96
2016-09-06T22:26:17.569186: step 3353, loss 0.021768, acc 1
2016-09-06T22:26:18.267434: step 3354, loss 0.0328005, acc 0.98
2016-09-06T22:26:18.947918: step 3355, loss 0.00771127, acc 1
2016-09-06T22:26:19.674139: step 3356, loss 0.026123, acc 1
2016-09-06T22:26:20.361812: step 3357, loss 0.0107125, acc 1
2016-09-06T22:26:21.025930: step 3358, loss 0.00843071, acc 1
2016-09-06T22:26:21.722757: step 3359, loss 0.115153, acc 0.94
2016-09-06T22:26:22.402534: step 3360, loss 0.0446111, acc 0.98
2016-09-06T22:26:23.079925: step 3361, loss 0.0270496, acc 0.98
2016-09-06T22:26:23.757289: step 3362, loss 0.0643658, acc 0.96
2016-09-06T22:26:24.449516: step 3363, loss 0.0309642, acc 1
2016-09-06T22:26:25.135688: step 3364, loss 0.0131905, acc 1
2016-09-06T22:26:25.838010: step 3365, loss 0.0663421, acc 0.98
2016-09-06T22:26:26.535903: step 3366, loss 0.00730126, acc 1
2016-09-06T22:26:27.251695: step 3367, loss 0.0209021, acc 1
2016-09-06T22:26:27.942311: step 3368, loss 0.0917997, acc 0.96
2016-09-06T22:26:28.652503: step 3369, loss 0.00869301, acc 1
2016-09-06T22:26:29.361157: step 3370, loss 0.0674218, acc 0.98
2016-09-06T22:26:30.026587: step 3371, loss 0.00774018, acc 1
2016-09-06T22:26:30.704528: step 3372, loss 0.0891803, acc 0.94
2016-09-06T22:26:31.396548: step 3373, loss 0.0479865, acc 0.98
2016-09-06T22:26:32.090946: step 3374, loss 0.0404117, acc 0.98
2016-09-06T22:26:32.779186: step 3375, loss 0.0234836, acc 1
2016-09-06T22:26:33.439046: step 3376, loss 0.0134934, acc 1
2016-09-06T22:26:34.140592: step 3377, loss 0.0553624, acc 0.98
2016-09-06T22:26:34.808916: step 3378, loss 0.0159152, acc 1
2016-09-06T22:26:35.477711: step 3379, loss 0.0146648, acc 1
2016-09-06T22:26:36.177503: step 3380, loss 0.0495274, acc 0.96
2016-09-06T22:26:36.880770: step 3381, loss 0.00289456, acc 1
2016-09-06T22:26:37.577031: step 3382, loss 0.00777699, acc 1
2016-09-06T22:26:38.256245: step 3383, loss 0.0435622, acc 0.98
2016-09-06T22:26:38.953239: step 3384, loss 0.0422181, acc 0.98
2016-09-06T22:26:39.643990: step 3385, loss 0.0556732, acc 0.96
2016-09-06T22:26:40.320122: step 3386, loss 0.0478042, acc 0.98
2016-09-06T22:26:41.010228: step 3387, loss 0.0153594, acc 1
2016-09-06T22:26:41.677210: step 3388, loss 0.0486594, acc 0.96
2016-09-06T22:26:42.366364: step 3389, loss 0.0257832, acc 0.98
2016-09-06T22:26:43.049217: step 3390, loss 0.0475451, acc 0.98
2016-09-06T22:26:43.758848: step 3391, loss 0.0414604, acc 0.98
2016-09-06T22:26:44.444539: step 3392, loss 0.0160867, acc 1
2016-09-06T22:26:45.124572: step 3393, loss 0.0217904, acc 0.98
2016-09-06T22:26:45.817247: step 3394, loss 0.0545009, acc 0.96
2016-09-06T22:26:46.503279: step 3395, loss 0.043612, acc 0.98
2016-09-06T22:26:47.179806: step 3396, loss 0.0719216, acc 0.96
2016-09-06T22:26:47.860487: step 3397, loss 0.175327, acc 0.98
2016-09-06T22:26:48.558067: step 3398, loss 0.0847056, acc 0.96
2016-09-06T22:26:49.242388: step 3399, loss 0.017558, acc 0.98
2016-09-06T22:26:49.930265: step 3400, loss 0.019047, acc 1

Evaluation:
2016-09-06T22:26:53.098299: step 3400, loss 1.75095, acc 0.75985

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-3400

2016-09-06T22:26:54.784905: step 3401, loss 0.0811831, acc 0.96
2016-09-06T22:26:55.494518: step 3402, loss 0.00224914, acc 1
2016-09-06T22:26:56.185497: step 3403, loss 0.0849506, acc 0.96
2016-09-06T22:26:56.877537: step 3404, loss 0.00950156, acc 1
2016-09-06T22:26:57.543367: step 3405, loss 0.00502987, acc 1
2016-09-06T22:26:58.244948: step 3406, loss 0.00935567, acc 1
2016-09-06T22:26:58.941231: step 3407, loss 0.0110671, acc 1
2016-09-06T22:26:59.633949: step 3408, loss 0.0227426, acc 0.98
2016-09-06T22:27:00.350546: step 3409, loss 0.00810159, acc 1
2016-09-06T22:27:01.013847: step 3410, loss 0.0081085, acc 1
2016-09-06T22:27:01.707001: step 3411, loss 0.0134509, acc 1
2016-09-06T22:27:02.381896: step 3412, loss 0.077062, acc 0.94
2016-09-06T22:27:03.078111: step 3413, loss 0.0587788, acc 0.96
2016-09-06T22:27:03.765332: step 3414, loss 0.0698648, acc 0.94
2016-09-06T22:27:04.453603: step 3415, loss 0.0168528, acc 1
2016-09-06T22:27:05.138383: step 3416, loss 0.0119611, acc 1
2016-09-06T22:27:05.837602: step 3417, loss 0.00383802, acc 1
2016-09-06T22:27:06.517601: step 3418, loss 0.0995315, acc 0.96
2016-09-06T22:27:07.183049: step 3419, loss 0.0428894, acc 0.98
2016-09-06T22:27:07.897509: step 3420, loss 0.0059962, acc 1
2016-09-06T22:27:08.592635: step 3421, loss 0.00102544, acc 1
2016-09-06T22:27:09.289300: step 3422, loss 0.0416033, acc 1
2016-09-06T22:27:09.987965: step 3423, loss 0.00900111, acc 1
2016-09-06T22:27:10.668795: step 3424, loss 0.0708463, acc 0.98
2016-09-06T22:27:11.361026: step 3425, loss 0.0212035, acc 0.98
2016-09-06T22:27:12.014948: step 3426, loss 0.033928, acc 0.98
2016-09-06T22:27:12.720004: step 3427, loss 0.0218304, acc 1
2016-09-06T22:27:13.410412: step 3428, loss 0.00686585, acc 1
2016-09-06T22:27:14.079216: step 3429, loss 0.0835093, acc 0.96
2016-09-06T22:27:14.767209: step 3430, loss 0.0588936, acc 0.98
2016-09-06T22:27:15.445600: step 3431, loss 0.00690497, acc 1
2016-09-06T22:27:16.187373: step 3432, loss 0.0719065, acc 0.96
2016-09-06T22:27:16.843138: step 3433, loss 0.0289707, acc 0.98
2016-09-06T22:27:17.523518: step 3434, loss 0.128904, acc 0.96
2016-09-06T22:27:18.210660: step 3435, loss 0.0198941, acc 0.98
2016-09-06T22:27:18.922472: step 3436, loss 0.0257146, acc 0.98
2016-09-06T22:27:19.615032: step 3437, loss 0.0167273, acc 1
2016-09-06T22:27:20.280550: step 3438, loss 0.040351, acc 0.98
2016-09-06T22:27:20.987125: step 3439, loss 0.0255759, acc 0.98
2016-09-06T22:27:21.651517: step 3440, loss 0.026621, acc 0.98
2016-09-06T22:27:22.357067: step 3441, loss 0.0368496, acc 1
2016-09-06T22:27:23.052273: step 3442, loss 0.0596573, acc 0.98
2016-09-06T22:27:23.745770: step 3443, loss 0.00431501, acc 1
2016-09-06T22:27:24.431993: step 3444, loss 0.0597225, acc 0.98
2016-09-06T22:27:25.117585: step 3445, loss 0.0207338, acc 1
2016-09-06T22:27:25.822967: step 3446, loss 0.00522622, acc 1
2016-09-06T22:27:26.501240: step 3447, loss 0.00190369, acc 1
2016-09-06T22:27:27.188889: step 3448, loss 0.073892, acc 0.98
2016-09-06T22:27:27.876886: step 3449, loss 0.148648, acc 0.98
2016-09-06T22:27:28.548438: step 3450, loss 0.00454467, acc 1
2016-09-06T22:27:29.236536: step 3451, loss 0.00128842, acc 1
2016-09-06T22:27:29.936934: step 3452, loss 0.0072389, acc 1
2016-09-06T22:27:30.649987: step 3453, loss 0.0133211, acc 1
2016-09-06T22:27:31.338284: step 3454, loss 0.0410889, acc 0.98
2016-09-06T22:27:32.020122: step 3455, loss 0.0186109, acc 1
2016-09-06T22:27:32.659432: step 3456, loss 0.0144484, acc 1
2016-09-06T22:27:33.339910: step 3457, loss 0.0336108, acc 0.98
2016-09-06T22:27:34.032620: step 3458, loss 0.0703128, acc 0.98
2016-09-06T22:27:34.719587: step 3459, loss 0.0313251, acc 0.98
2016-09-06T22:27:35.418220: step 3460, loss 0.0126929, acc 1
2016-09-06T22:27:36.088981: step 3461, loss 0.0300505, acc 0.98
2016-09-06T22:27:36.789691: step 3462, loss 0.0502265, acc 0.98
2016-09-06T22:27:37.476220: step 3463, loss 0.00892741, acc 1
2016-09-06T22:27:38.153451: step 3464, loss 0.0152124, acc 1
2016-09-06T22:27:38.839592: step 3465, loss 0.198623, acc 0.98
2016-09-06T22:27:39.555328: step 3466, loss 0.0742042, acc 0.96
2016-09-06T22:27:40.253244: step 3467, loss 0.00285525, acc 1
2016-09-06T22:27:40.924831: step 3468, loss 0.0572757, acc 0.98
2016-09-06T22:27:41.596572: step 3469, loss 0.0533007, acc 0.98
2016-09-06T22:27:42.280944: step 3470, loss 0.0118989, acc 1
2016-09-06T22:27:42.961208: step 3471, loss 0.00231575, acc 1
2016-09-06T22:27:43.639836: step 3472, loss 0.0311315, acc 0.98
2016-09-06T22:27:44.295757: step 3473, loss 0.0404899, acc 0.98
2016-09-06T22:27:44.986740: step 3474, loss 0.0446508, acc 0.98
2016-09-06T22:27:45.664059: step 3475, loss 0.0469109, acc 0.96
2016-09-06T22:27:46.356124: step 3476, loss 0.0180583, acc 0.98
2016-09-06T22:27:47.017657: step 3477, loss 0.00149626, acc 1
2016-09-06T22:27:47.704472: step 3478, loss 0.0288628, acc 0.98
2016-09-06T22:27:48.374502: step 3479, loss 0.0119579, acc 1
2016-09-06T22:27:49.070234: step 3480, loss 0.0366144, acc 0.98
2016-09-06T22:27:49.755148: step 3481, loss 0.0368646, acc 0.98
2016-09-06T22:27:50.437198: step 3482, loss 0.0135721, acc 1
2016-09-06T22:27:51.155672: step 3483, loss 0.0047594, acc 1
2016-09-06T22:27:51.837454: step 3484, loss 0.0099716, acc 1
2016-09-06T22:27:52.525656: step 3485, loss 0.0614018, acc 0.96
2016-09-06T22:27:53.210884: step 3486, loss 0.00246031, acc 1
2016-09-06T22:27:53.897043: step 3487, loss 0.0481056, acc 0.96
2016-09-06T22:27:54.591267: step 3488, loss 0.0132868, acc 1
2016-09-06T22:27:55.250891: step 3489, loss 0.0102498, acc 1
2016-09-06T22:27:55.930656: step 3490, loss 0.0426753, acc 0.98
2016-09-06T22:27:56.602587: step 3491, loss 0.0241472, acc 1
2016-09-06T22:27:57.281222: step 3492, loss 0.0230752, acc 0.98
2016-09-06T22:27:57.957484: step 3493, loss 0.0207942, acc 1
2016-09-06T22:27:58.643468: step 3494, loss 0.00731349, acc 1
2016-09-06T22:27:59.346449: step 3495, loss 0.0103986, acc 1
2016-09-06T22:28:00.020718: step 3496, loss 0.00169463, acc 1
2016-09-06T22:28:00.784173: step 3497, loss 0.0399565, acc 1
2016-09-06T22:28:01.490114: step 3498, loss 0.0284623, acc 0.98
2016-09-06T22:28:02.180032: step 3499, loss 0.0141863, acc 1
2016-09-06T22:28:02.880602: step 3500, loss 0.0310669, acc 0.98

Evaluation:
2016-09-06T22:28:06.003491: step 3500, loss 2.27206, acc 0.753283

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-3500

2016-09-06T22:28:07.681981: step 3501, loss 0.00120115, acc 1
2016-09-06T22:28:08.366927: step 3502, loss 0.00161758, acc 1
2016-09-06T22:28:09.075720: step 3503, loss 0.00420905, acc 1
2016-09-06T22:28:09.742553: step 3504, loss 0.0218364, acc 1
2016-09-06T22:28:10.431156: step 3505, loss 0.0148235, acc 1
2016-09-06T22:28:11.108720: step 3506, loss 0.0706749, acc 0.98
2016-09-06T22:28:11.788288: step 3507, loss 0.04278, acc 0.98
2016-09-06T22:28:12.473292: step 3508, loss 0.159766, acc 0.96
2016-09-06T22:28:13.156974: step 3509, loss 0.0154374, acc 1
2016-09-06T22:28:13.856297: step 3510, loss 0.00134819, acc 1
2016-09-06T22:28:14.512834: step 3511, loss 0.00242634, acc 1
2016-09-06T22:28:15.211867: step 3512, loss 0.105169, acc 0.98
2016-09-06T22:28:15.900186: step 3513, loss 0.00810065, acc 1
2016-09-06T22:28:16.582753: step 3514, loss 0.0311524, acc 0.98
2016-09-06T22:28:17.274995: step 3515, loss 0.144595, acc 0.96
2016-09-06T22:28:17.948459: step 3516, loss 0.000996166, acc 1
2016-09-06T22:28:18.657215: step 3517, loss 0.0716823, acc 0.94
2016-09-06T22:28:19.323974: step 3518, loss 0.000236241, acc 1
2016-09-06T22:28:20.013846: step 3519, loss 0.0168785, acc 1
2016-09-06T22:28:20.699281: step 3520, loss 0.0455894, acc 0.98
2016-09-06T22:28:21.386460: step 3521, loss 0.00184898, acc 1
2016-09-06T22:28:22.061213: step 3522, loss 0.105153, acc 0.94
2016-09-06T22:28:22.754797: step 3523, loss 0.099165, acc 0.98
2016-09-06T22:28:23.454759: step 3524, loss 0.111099, acc 0.94
2016-09-06T22:28:24.104039: step 3525, loss 0.0406521, acc 0.96
2016-09-06T22:28:24.821876: step 3526, loss 0.00414639, acc 1
2016-09-06T22:28:25.519077: step 3527, loss 0.0603647, acc 0.98
2016-09-06T22:28:26.228741: step 3528, loss 0.0362649, acc 0.98
2016-09-06T22:28:26.920525: step 3529, loss 0.0457268, acc 0.98
2016-09-06T22:28:27.641904: step 3530, loss 0.0505535, acc 0.98
2016-09-06T22:28:28.352160: step 3531, loss 0.0533821, acc 0.98
2016-09-06T22:28:29.027150: step 3532, loss 0.00612819, acc 1
2016-09-06T22:28:29.742450: step 3533, loss 0.0447798, acc 0.98
2016-09-06T22:28:30.435512: step 3534, loss 0.0386936, acc 0.98
2016-09-06T22:28:31.127210: step 3535, loss 0.0108409, acc 1
2016-09-06T22:28:31.836744: step 3536, loss 0.031353, acc 0.98
2016-09-06T22:28:32.536876: step 3537, loss 0.0377337, acc 0.98
2016-09-06T22:28:33.245744: step 3538, loss 0.0210474, acc 1
2016-09-06T22:28:33.942384: step 3539, loss 0.00530176, acc 1
2016-09-06T22:28:34.647901: step 3540, loss 0.0279361, acc 0.98
2016-09-06T22:28:35.314791: step 3541, loss 0.00486695, acc 1
2016-09-06T22:28:35.993765: step 3542, loss 0.00719989, acc 1
2016-09-06T22:28:36.668920: step 3543, loss 0.0191478, acc 1
2016-09-06T22:28:37.374285: step 3544, loss 0.0215196, acc 1
2016-09-06T22:28:38.048116: step 3545, loss 0.0118315, acc 1
2016-09-06T22:28:38.741674: step 3546, loss 0.121793, acc 0.94
2016-09-06T22:28:39.439915: step 3547, loss 0.00953715, acc 1
2016-09-06T22:28:40.134457: step 3548, loss 0.00796674, acc 1
2016-09-06T22:28:40.822196: step 3549, loss 0.0347846, acc 0.98
2016-09-06T22:28:41.540094: step 3550, loss 0.0212596, acc 1
2016-09-06T22:28:42.194166: step 3551, loss 0.113259, acc 0.92
2016-09-06T22:28:42.902866: step 3552, loss 0.0503577, acc 0.98
2016-09-06T22:28:43.587787: step 3553, loss 0.057682, acc 0.96
2016-09-06T22:28:44.276247: step 3554, loss 0.0637199, acc 0.96
2016-09-06T22:28:44.964664: step 3555, loss 0.047978, acc 0.98
2016-09-06T22:28:45.642424: step 3556, loss 0.0298535, acc 0.98
2016-09-06T22:28:46.345819: step 3557, loss 0.0348949, acc 0.98
2016-09-06T22:28:47.021731: step 3558, loss 0.0390805, acc 0.98
2016-09-06T22:28:47.723063: step 3559, loss 0.0132348, acc 1
2016-09-06T22:28:48.436260: step 3560, loss 0.0329176, acc 0.98
2016-09-06T22:28:49.131456: step 3561, loss 0.00621736, acc 1
2016-09-06T22:28:49.826240: step 3562, loss 0.0125373, acc 1
2016-09-06T22:28:50.514169: step 3563, loss 0.0379163, acc 0.98
2016-09-06T22:28:51.210850: step 3564, loss 0.0440154, acc 0.96
2016-09-06T22:28:51.886244: step 3565, loss 0.0129339, acc 1
2016-09-06T22:28:52.559870: step 3566, loss 0.032282, acc 0.98
2016-09-06T22:28:53.259898: step 3567, loss 0.00625979, acc 1
2016-09-06T22:28:53.956905: step 3568, loss 0.000616724, acc 1
2016-09-06T22:28:54.645728: step 3569, loss 0.0198369, acc 1
2016-09-06T22:28:55.322888: step 3570, loss 0.0659894, acc 0.98
2016-09-06T22:28:56.024739: step 3571, loss 0.0472161, acc 0.98
2016-09-06T22:28:56.691698: step 3572, loss 0.0018677, acc 1
2016-09-06T22:28:57.379125: step 3573, loss 0.0346345, acc 0.98
2016-09-06T22:28:58.072998: step 3574, loss 0.000163642, acc 1
2016-09-06T22:28:58.763805: step 3575, loss 0.078323, acc 0.96
2016-09-06T22:28:59.436495: step 3576, loss 0.00285272, acc 1
2016-09-06T22:29:00.134613: step 3577, loss 0.0481773, acc 0.98
2016-09-06T22:29:00.856062: step 3578, loss 0.0072257, acc 1
2016-09-06T22:29:01.531970: step 3579, loss 0.00872988, acc 1
2016-09-06T22:29:02.227236: step 3580, loss 0.0747712, acc 0.94
2016-09-06T22:29:02.910157: step 3581, loss 0.0491057, acc 0.96
2016-09-06T22:29:03.607509: step 3582, loss 0.0415641, acc 0.98
2016-09-06T22:29:04.307359: step 3583, loss 0.0625591, acc 0.98
2016-09-06T22:29:05.005093: step 3584, loss 0.0512932, acc 0.98
2016-09-06T22:29:05.715091: step 3585, loss 0.00120182, acc 1
2016-09-06T22:29:06.415818: step 3586, loss 0.0296013, acc 1
2016-09-06T22:29:07.098565: step 3587, loss 0.059279, acc 0.98
2016-09-06T22:29:07.774991: step 3588, loss 0.0323895, acc 0.98
2016-09-06T22:29:08.467350: step 3589, loss 0.0537001, acc 0.94
2016-09-06T22:29:09.184242: step 3590, loss 0.0287277, acc 0.98
2016-09-06T22:29:09.851765: step 3591, loss 0.00337771, acc 1
2016-09-06T22:29:10.592269: step 3592, loss 0.0118142, acc 1
2016-09-06T22:29:11.298221: step 3593, loss 0.00834373, acc 1
2016-09-06T22:29:11.976216: step 3594, loss 0.000857584, acc 1
2016-09-06T22:29:12.661481: step 3595, loss 0.0474827, acc 0.98
2016-09-06T22:29:13.326907: step 3596, loss 0.0349182, acc 0.98
2016-09-06T22:29:14.028977: step 3597, loss 0.00931214, acc 1
2016-09-06T22:29:14.712569: step 3598, loss 0.126172, acc 0.98
2016-09-06T22:29:15.387595: step 3599, loss 0.0396416, acc 0.98
2016-09-06T22:29:16.071232: step 3600, loss 0.00859797, acc 1

Evaluation:
2016-09-06T22:29:19.232479: step 3600, loss 2.02805, acc 0.746717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-3600

2016-09-06T22:29:20.909054: step 3601, loss 0.0166861, acc 1
2016-09-06T22:29:21.601356: step 3602, loss 0.0131253, acc 1
2016-09-06T22:29:22.286797: step 3603, loss 0.0236503, acc 1
2016-09-06T22:29:22.962391: step 3604, loss 0.0595056, acc 0.98
2016-09-06T22:29:23.675959: step 3605, loss 0.0483705, acc 0.96
2016-09-06T22:29:24.356645: step 3606, loss 0.0178068, acc 0.98
2016-09-06T22:29:25.030623: step 3607, loss 0.0580929, acc 0.96
2016-09-06T22:29:25.715113: step 3608, loss 0.0478372, acc 0.96
2016-09-06T22:29:26.389895: step 3609, loss 0.0957678, acc 0.96
2016-09-06T22:29:27.083391: step 3610, loss 0.0641022, acc 0.98
2016-09-06T22:29:27.773684: step 3611, loss 0.12458, acc 0.96
2016-09-06T22:29:28.489382: step 3612, loss 0.00922307, acc 1
2016-09-06T22:29:29.180906: step 3613, loss 0.0101494, acc 1
2016-09-06T22:29:29.858052: step 3614, loss 0.136971, acc 0.96
2016-09-06T22:29:30.548574: step 3615, loss 0.00504657, acc 1
2016-09-06T22:29:31.252150: step 3616, loss 0.0357466, acc 0.98
2016-09-06T22:29:31.926022: step 3617, loss 0.00819124, acc 1
2016-09-06T22:29:32.606586: step 3618, loss 0.04486, acc 0.98
2016-09-06T22:29:33.304785: step 3619, loss 0.0455362, acc 0.98
2016-09-06T22:29:33.979670: step 3620, loss 0.0240929, acc 0.98
2016-09-06T22:29:34.650485: step 3621, loss 0.00963781, acc 1
2016-09-06T22:29:35.359387: step 3622, loss 0.0181646, acc 1
2016-09-06T22:29:36.034606: step 3623, loss 0.0203929, acc 0.98
2016-09-06T22:29:36.729645: step 3624, loss 0.0224066, acc 1
2016-09-06T22:29:37.393422: step 3625, loss 0.0413125, acc 1
2016-09-06T22:29:38.080315: step 3626, loss 0.176337, acc 0.94
2016-09-06T22:29:38.756352: step 3627, loss 0.0118372, acc 1
2016-09-06T22:29:39.439126: step 3628, loss 0.0319301, acc 0.98
2016-09-06T22:29:40.136045: step 3629, loss 0.013349, acc 1
2016-09-06T22:29:40.841807: step 3630, loss 0.104392, acc 0.96
2016-09-06T22:29:41.521668: step 3631, loss 0.00520746, acc 1
2016-09-06T22:29:42.198907: step 3632, loss 0.0378738, acc 0.98
2016-09-06T22:29:42.907123: step 3633, loss 0.0244955, acc 1
2016-09-06T22:29:43.598240: step 3634, loss 0.0184672, acc 1
2016-09-06T22:29:44.303195: step 3635, loss 0.00641686, acc 1
2016-09-06T22:29:44.974584: step 3636, loss 0.0127426, acc 1
2016-09-06T22:29:45.658663: step 3637, loss 0.00837393, acc 1
2016-09-06T22:29:46.348955: step 3638, loss 0.00374683, acc 1
2016-09-06T22:29:47.032292: step 3639, loss 0.0603794, acc 0.96
2016-09-06T22:29:47.773326: step 3640, loss 0.0340348, acc 0.98
2016-09-06T22:29:48.438554: step 3641, loss 0.0573374, acc 0.98
2016-09-06T22:29:49.122820: step 3642, loss 0.0166048, acc 1
2016-09-06T22:29:49.806763: step 3643, loss 0.0331806, acc 0.98
2016-09-06T22:29:50.489395: step 3644, loss 0.0249735, acc 0.98
2016-09-06T22:29:51.166565: step 3645, loss 0.0622808, acc 0.96
2016-09-06T22:29:51.857758: step 3646, loss 0.0290148, acc 0.98
2016-09-06T22:29:52.586177: step 3647, loss 0.0839045, acc 0.98
2016-09-06T22:29:53.230003: step 3648, loss 0.00297341, acc 1
2016-09-06T22:29:53.942519: step 3649, loss 0.0674607, acc 0.94
2016-09-06T22:29:54.635164: step 3650, loss 0.01474, acc 1
2016-09-06T22:29:55.345495: step 3651, loss 0.0335175, acc 1
2016-09-06T22:29:56.048436: step 3652, loss 0.0195051, acc 1
2016-09-06T22:29:56.705853: step 3653, loss 0.0163974, acc 1
2016-09-06T22:29:57.401634: step 3654, loss 0.00780464, acc 1
2016-09-06T22:29:58.065040: step 3655, loss 0.0030677, acc 1
2016-09-06T22:29:58.767226: step 3656, loss 0.00518801, acc 1
2016-09-06T22:29:59.453421: step 3657, loss 0.0272995, acc 0.98
2016-09-06T22:30:00.137746: step 3658, loss 0.03327, acc 1
2016-09-06T22:30:00.882695: step 3659, loss 0.0477375, acc 0.96
2016-09-06T22:30:01.530521: step 3660, loss 0.00280537, acc 1
2016-09-06T22:30:02.234301: step 3661, loss 0.0634667, acc 0.96
2016-09-06T22:30:02.896464: step 3662, loss 0.0453759, acc 0.98
2016-09-06T22:30:03.571119: step 3663, loss 0.00304147, acc 1
2016-09-06T22:30:04.247737: step 3664, loss 0.00716478, acc 1
2016-09-06T22:30:04.952016: step 3665, loss 0.0380439, acc 0.98
2016-09-06T22:30:05.633408: step 3666, loss 0.018463, acc 1
2016-09-06T22:30:06.304861: step 3667, loss 0.0182605, acc 0.98
2016-09-06T22:30:07.027151: step 3668, loss 0.0204582, acc 1
2016-09-06T22:30:07.691984: step 3669, loss 0.0514547, acc 0.96
2016-09-06T22:30:08.390616: step 3670, loss 0.0607823, acc 0.98
2016-09-06T22:30:09.084153: step 3671, loss 0.171412, acc 0.96
2016-09-06T22:30:09.777904: step 3672, loss 0.00793601, acc 1
2016-09-06T22:30:10.466568: step 3673, loss 0.10767, acc 0.96
2016-09-06T22:30:11.128319: step 3674, loss 0.000715635, acc 1
2016-09-06T22:30:11.834962: step 3675, loss 0.00953452, acc 1
2016-09-06T22:30:12.502143: step 3676, loss 0.0186809, acc 1
2016-09-06T22:30:13.204763: step 3677, loss 0.0347833, acc 1
2016-09-06T22:30:13.900881: step 3678, loss 0.0117747, acc 1
2016-09-06T22:30:14.583876: step 3679, loss 0.0266945, acc 0.98
2016-09-06T22:30:15.258939: step 3680, loss 0.136587, acc 0.98
2016-09-06T22:30:15.909906: step 3681, loss 0.00813579, acc 1
2016-09-06T22:30:16.599023: step 3682, loss 0.0335428, acc 0.98
2016-09-06T22:30:17.272543: step 3683, loss 0.0240779, acc 1
2016-09-06T22:30:17.959288: step 3684, loss 0.0137342, acc 1
2016-09-06T22:30:18.623788: step 3685, loss 0.109739, acc 0.98
2016-09-06T22:30:19.312316: step 3686, loss 0.0352666, acc 0.98
2016-09-06T22:30:19.998137: step 3687, loss 0.0546408, acc 0.98
2016-09-06T22:30:20.698137: step 3688, loss 0.0190898, acc 1
2016-09-06T22:30:21.406684: step 3689, loss 0.0690026, acc 0.96
2016-09-06T22:30:22.060922: step 3690, loss 0.0212495, acc 1
2016-09-06T22:30:22.742578: step 3691, loss 0.0246038, acc 1
2016-09-06T22:30:23.414737: step 3692, loss 0.0208289, acc 1
2016-09-06T22:30:24.092471: step 3693, loss 0.0278257, acc 0.98
2016-09-06T22:30:24.757563: step 3694, loss 0.034965, acc 0.98
2016-09-06T22:30:25.429540: step 3695, loss 0.0216441, acc 1
2016-09-06T22:30:26.112321: step 3696, loss 0.0532895, acc 0.98
2016-09-06T22:30:26.782825: step 3697, loss 0.000821216, acc 1
2016-09-06T22:30:27.499205: step 3698, loss 0.0239559, acc 1
2016-09-06T22:30:28.182368: step 3699, loss 0.0262718, acc 0.98
2016-09-06T22:30:28.859458: step 3700, loss 0.00441157, acc 1

Evaluation:
2016-09-06T22:30:31.993200: step 3700, loss 1.93462, acc 0.742964

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-3700

2016-09-06T22:30:33.751699: step 3701, loss 0.000976382, acc 1
2016-09-06T22:30:34.438977: step 3702, loss 0.0623716, acc 0.98
2016-09-06T22:30:35.110813: step 3703, loss 0.05789, acc 0.94
2016-09-06T22:30:35.818585: step 3704, loss 0.0399821, acc 0.98
2016-09-06T22:30:36.500155: step 3705, loss 0.0571155, acc 0.98
2016-09-06T22:30:37.195850: step 3706, loss 0.010465, acc 1
2016-09-06T22:30:37.913466: step 3707, loss 0.12311, acc 0.92
2016-09-06T22:30:38.599973: step 3708, loss 0.00858785, acc 1
2016-09-06T22:30:39.273590: step 3709, loss 0.00729147, acc 1
2016-09-06T22:30:39.943429: step 3710, loss 0.000809845, acc 1
2016-09-06T22:30:40.639012: step 3711, loss 0.032616, acc 0.96
2016-09-06T22:30:41.307157: step 3712, loss 0.153023, acc 0.98
2016-09-06T22:30:42.010319: step 3713, loss 0.151777, acc 0.98
2016-09-06T22:30:42.699432: step 3714, loss 0.0223917, acc 0.98
2016-09-06T22:30:43.393033: step 3715, loss 0.0819978, acc 0.96
2016-09-06T22:30:44.090486: step 3716, loss 0.00543624, acc 1
2016-09-06T22:30:44.788712: step 3717, loss 0.0461283, acc 0.96
2016-09-06T22:30:45.521289: step 3718, loss 0.0609558, acc 0.98
2016-09-06T22:30:46.215810: step 3719, loss 0.0757533, acc 0.98
2016-09-06T22:30:46.892593: step 3720, loss 0.0610397, acc 0.98
2016-09-06T22:30:47.597842: step 3721, loss 0.123491, acc 0.96
2016-09-06T22:30:48.285674: step 3722, loss 0.0337293, acc 0.96
2016-09-06T22:30:48.976400: step 3723, loss 0.0238367, acc 1
2016-09-06T22:30:49.629200: step 3724, loss 0.00893673, acc 1
2016-09-06T22:30:50.340779: step 3725, loss 0.244721, acc 0.92
2016-09-06T22:30:51.041369: step 3726, loss 0.0208318, acc 1
2016-09-06T22:30:51.741140: step 3727, loss 0.0276792, acc 1
2016-09-06T22:30:52.427233: step 3728, loss 0.0411128, acc 0.98
2016-09-06T22:30:53.097065: step 3729, loss 0.274647, acc 0.96
2016-09-06T22:30:53.782835: step 3730, loss 0.0287397, acc 0.98
2016-09-06T22:30:54.452126: step 3731, loss 0.0190708, acc 1
2016-09-06T22:30:55.175625: step 3732, loss 0.0222293, acc 0.98
2016-09-06T22:30:55.849208: step 3733, loss 0.0634924, acc 0.94
2016-09-06T22:30:56.517423: step 3734, loss 0.0319981, acc 0.98
2016-09-06T22:30:57.192030: step 3735, loss 0.0348468, acc 0.98
2016-09-06T22:30:57.876704: step 3736, loss 0.0846706, acc 0.96
2016-09-06T22:30:58.578063: step 3737, loss 0.0662812, acc 0.94
2016-09-06T22:30:59.251604: step 3738, loss 0.00530879, acc 1
2016-09-06T22:30:59.966425: step 3739, loss 0.074496, acc 0.98
2016-09-06T22:31:00.684035: step 3740, loss 0.0782613, acc 0.94
2016-09-06T22:31:01.383809: step 3741, loss 0.0137471, acc 1
2016-09-06T22:31:02.059083: step 3742, loss 0.0287706, acc 0.98
2016-09-06T22:31:02.727055: step 3743, loss 0.040156, acc 1
2016-09-06T22:31:03.412317: step 3744, loss 0.0506197, acc 0.98
2016-09-06T22:31:04.067452: step 3745, loss 0.0109259, acc 1
2016-09-06T22:31:04.757861: step 3746, loss 0.0311443, acc 1
2016-09-06T22:31:05.457442: step 3747, loss 0.0532847, acc 0.98
2016-09-06T22:31:06.127389: step 3748, loss 0.0570531, acc 0.98
2016-09-06T22:31:06.813274: step 3749, loss 0.0167953, acc 1
2016-09-06T22:31:07.551790: step 3750, loss 0.00725498, acc 1
2016-09-06T22:31:08.240889: step 3751, loss 0.00411579, acc 1
2016-09-06T22:31:08.902134: step 3752, loss 0.0280922, acc 0.98
2016-09-06T22:31:09.602048: step 3753, loss 0.0136418, acc 1
2016-09-06T22:31:10.266417: step 3754, loss 0.0275703, acc 1
2016-09-06T22:31:10.948850: step 3755, loss 0.0147089, acc 1
2016-09-06T22:31:11.628219: step 3756, loss 0.0464659, acc 0.98
2016-09-06T22:31:12.332855: step 3757, loss 0.061354, acc 0.98
2016-09-06T22:31:13.020082: step 3758, loss 0.00598505, acc 1
2016-09-06T22:31:13.683507: step 3759, loss 0.0371765, acc 0.98
2016-09-06T22:31:14.385345: step 3760, loss 0.0409984, acc 0.98
2016-09-06T22:31:15.047480: step 3761, loss 0.0300093, acc 0.98
2016-09-06T22:31:15.709762: step 3762, loss 0.114759, acc 0.94
2016-09-06T22:31:16.395842: step 3763, loss 0.0346428, acc 0.98
2016-09-06T22:31:17.074966: step 3764, loss 0.053533, acc 0.98
2016-09-06T22:31:17.749230: step 3765, loss 0.0336776, acc 0.98
2016-09-06T22:31:18.446320: step 3766, loss 0.0731437, acc 0.98
2016-09-06T22:31:19.157251: step 3767, loss 0.0624525, acc 0.96
2016-09-06T22:31:19.828238: step 3768, loss 0.0397662, acc 0.98
2016-09-06T22:31:20.514529: step 3769, loss 0.0560185, acc 0.98
2016-09-06T22:31:21.197916: step 3770, loss 0.0780991, acc 0.96
2016-09-06T22:31:21.869852: step 3771, loss 0.00998947, acc 1
2016-09-06T22:31:22.549632: step 3772, loss 0.14614, acc 0.96
2016-09-06T22:31:23.243374: step 3773, loss 0.0282748, acc 1
2016-09-06T22:31:23.928954: step 3774, loss 0.02024, acc 0.98
2016-09-06T22:31:24.604278: step 3775, loss 0.116841, acc 0.98
2016-09-06T22:31:25.290469: step 3776, loss 0.101679, acc 0.96
2016-09-06T22:31:25.964048: step 3777, loss 0.00739022, acc 1
2016-09-06T22:31:26.644675: step 3778, loss 0.0461554, acc 0.98
2016-09-06T22:31:27.324241: step 3779, loss 0.131833, acc 0.96
2016-09-06T22:31:28.023511: step 3780, loss 0.033894, acc 0.98
2016-09-06T22:31:28.736937: step 3781, loss 0.123872, acc 0.92
2016-09-06T22:31:29.402003: step 3782, loss 0.025457, acc 0.98
2016-09-06T22:31:30.097543: step 3783, loss 0.0164081, acc 1
2016-09-06T22:31:30.779660: step 3784, loss 0.00462502, acc 1
2016-09-06T22:31:31.457599: step 3785, loss 0.0598757, acc 0.96
2016-09-06T22:31:32.140895: step 3786, loss 0.0213756, acc 0.98
2016-09-06T22:31:32.832201: step 3787, loss 0.0313215, acc 0.98
2016-09-06T22:31:33.541387: step 3788, loss 0.0204495, acc 1
2016-09-06T22:31:34.196921: step 3789, loss 0.042012, acc 0.96
2016-09-06T22:31:34.900618: step 3790, loss 0.0307552, acc 0.98
2016-09-06T22:31:35.577731: step 3791, loss 0.0106216, acc 1
2016-09-06T22:31:36.251627: step 3792, loss 0.00845638, acc 1
2016-09-06T22:31:36.919296: step 3793, loss 0.0760224, acc 0.98
2016-09-06T22:31:37.617035: step 3794, loss 0.00232066, acc 1
2016-09-06T22:31:38.307196: step 3795, loss 0.032848, acc 0.98
2016-09-06T22:31:38.968743: step 3796, loss 0.0347874, acc 1
2016-09-06T22:31:39.669109: step 3797, loss 0.0312919, acc 0.96
2016-09-06T22:31:40.373290: step 3798, loss 0.0212937, acc 1
2016-09-06T22:31:41.063589: step 3799, loss 0.0350205, acc 0.98
2016-09-06T22:31:41.753961: step 3800, loss 0.00252555, acc 1

Evaluation:
2016-09-06T22:31:44.891333: step 3800, loss 1.58044, acc 0.744841

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-3800

2016-09-06T22:31:46.640271: step 3801, loss 0.00483394, acc 1
2016-09-06T22:31:47.314102: step 3802, loss 0.0282478, acc 0.98
2016-09-06T22:31:48.031000: step 3803, loss 0.0837834, acc 0.96
2016-09-06T22:31:48.748587: step 3804, loss 0.0315859, acc 0.98
2016-09-06T22:31:49.432901: step 3805, loss 0.043952, acc 0.98
2016-09-06T22:31:50.102691: step 3806, loss 0.0542617, acc 0.98
2016-09-06T22:31:50.785698: step 3807, loss 0.0118606, acc 1
2016-09-06T22:31:51.455261: step 3808, loss 0.0247899, acc 0.98
2016-09-06T22:31:52.118620: step 3809, loss 0.0050799, acc 1
2016-09-06T22:31:52.825646: step 3810, loss 0.0798215, acc 0.98
2016-09-06T22:31:53.513007: step 3811, loss 0.00469623, acc 1
2016-09-06T22:31:54.199567: step 3812, loss 0.0326225, acc 0.98
2016-09-06T22:31:54.878786: step 3813, loss 0.105659, acc 0.98
2016-09-06T22:31:55.556063: step 3814, loss 0.0334592, acc 0.98
2016-09-06T22:31:56.270794: step 3815, loss 0.0293749, acc 0.98
2016-09-06T22:31:56.945154: step 3816, loss 0.046478, acc 0.98
2016-09-06T22:31:57.656538: step 3817, loss 0.0312132, acc 0.98
2016-09-06T22:31:58.345339: step 3818, loss 0.0146927, acc 1
2016-09-06T22:31:59.039022: step 3819, loss 0.0314503, acc 0.98
2016-09-06T22:31:59.737084: step 3820, loss 0.0384857, acc 0.98
2016-09-06T22:32:00.461127: step 3821, loss 0.0241772, acc 1
2016-09-06T22:32:01.170188: step 3822, loss 0.0733927, acc 0.96
2016-09-06T22:32:01.870509: step 3823, loss 0.00896911, acc 1
2016-09-06T22:32:02.569133: step 3824, loss 0.0185792, acc 1
2016-09-06T22:32:03.248388: step 3825, loss 0.0551933, acc 0.98
2016-09-06T22:32:03.945590: step 3826, loss 0.0620139, acc 0.96
2016-09-06T22:32:04.626461: step 3827, loss 0.026011, acc 1
2016-09-06T22:32:05.316143: step 3828, loss 0.025673, acc 1
2016-09-06T22:32:06.028723: step 3829, loss 0.0189876, acc 0.98
2016-09-06T22:32:06.694342: step 3830, loss 0.0117622, acc 1
2016-09-06T22:32:07.361848: step 3831, loss 0.0101693, acc 1
2016-09-06T22:32:08.057576: step 3832, loss 0.03958, acc 0.96
2016-09-06T22:32:08.732245: step 3833, loss 0.00617889, acc 1
2016-09-06T22:32:09.401868: step 3834, loss 0.0248522, acc 0.98
2016-09-06T22:32:10.075740: step 3835, loss 0.0168837, acc 1
2016-09-06T22:32:10.754514: step 3836, loss 0.140972, acc 0.98
2016-09-06T22:32:11.409477: step 3837, loss 0.0315918, acc 1
2016-09-06T22:32:12.111999: step 3838, loss 0.0882144, acc 0.96
2016-09-06T22:32:12.820066: step 3839, loss 0.0316033, acc 0.98
2016-09-06T22:32:13.468337: step 3840, loss 0.0601047, acc 0.977273
2016-09-06T22:32:14.156846: step 3841, loss 0.00737276, acc 1
2016-09-06T22:32:14.866172: step 3842, loss 0.0226077, acc 1
2016-09-06T22:32:15.550541: step 3843, loss 0.0379699, acc 0.98
2016-09-06T22:32:16.205895: step 3844, loss 0.00169983, acc 1
2016-09-06T22:32:16.907423: step 3845, loss 0.0198693, acc 1
2016-09-06T22:32:17.586942: step 3846, loss 0.0663111, acc 0.96
2016-09-06T22:32:18.276362: step 3847, loss 0.0237524, acc 1
2016-09-06T22:32:18.958612: step 3848, loss 0.0633404, acc 0.98
2016-09-06T22:32:19.657153: step 3849, loss 0.00612674, acc 1
2016-09-06T22:32:20.368727: step 3850, loss 0.00681495, acc 1
2016-09-06T22:32:21.061784: step 3851, loss 0.00697523, acc 1
2016-09-06T22:32:21.767581: step 3852, loss 0.0231386, acc 0.98
2016-09-06T22:32:22.463887: step 3853, loss 0.0168672, acc 1
2016-09-06T22:32:23.143999: step 3854, loss 0.0226163, acc 1
2016-09-06T22:32:23.854425: step 3855, loss 0.00358943, acc 1
2016-09-06T22:32:24.541213: step 3856, loss 0.00117997, acc 1
2016-09-06T22:32:25.256151: step 3857, loss 0.0285066, acc 1
2016-09-06T22:32:25.916099: step 3858, loss 0.0497986, acc 0.98
2016-09-06T22:32:26.602808: step 3859, loss 0.0176414, acc 1
2016-09-06T22:32:27.301945: step 3860, loss 0.00137648, acc 1
2016-09-06T22:32:27.972674: step 3861, loss 0.00431359, acc 1
2016-09-06T22:32:28.639749: step 3862, loss 0.0270254, acc 1
2016-09-06T22:32:29.335982: step 3863, loss 0.16861, acc 0.96
2016-09-06T22:32:30.037156: step 3864, loss 0.0468658, acc 0.98
2016-09-06T22:32:30.743206: step 3865, loss 0.000870299, acc 1
2016-09-06T22:32:31.428284: step 3866, loss 0.0112856, acc 1
2016-09-06T22:32:32.103892: step 3867, loss 0.000387041, acc 1
2016-09-06T22:32:32.782979: step 3868, loss 0.0581743, acc 0.96
2016-09-06T22:32:33.478943: step 3869, loss 0.0446301, acc 0.98
2016-09-06T22:32:34.157467: step 3870, loss 0.0296336, acc 0.98
2016-09-06T22:32:34.853692: step 3871, loss 0.0605782, acc 0.96
2016-09-06T22:32:35.527969: step 3872, loss 0.0519843, acc 0.98
2016-09-06T22:32:36.232797: step 3873, loss 0.0934014, acc 0.96
2016-09-06T22:32:36.914537: step 3874, loss 0.121722, acc 0.94
2016-09-06T22:32:37.589895: step 3875, loss 0.0403168, acc 0.98
2016-09-06T22:32:38.283904: step 3876, loss 0.0244855, acc 1
2016-09-06T22:32:38.967504: step 3877, loss 0.013579, acc 1
2016-09-06T22:32:39.715947: step 3878, loss 0.0167698, acc 1
2016-09-06T22:32:40.398378: step 3879, loss 0.0107914, acc 1
2016-09-06T22:32:41.089915: step 3880, loss 0.00895762, acc 1
2016-09-06T22:32:41.765553: step 3881, loss 0.0314727, acc 1
2016-09-06T22:32:42.462619: step 3882, loss 0.0198176, acc 0.98
2016-09-06T22:32:43.153032: step 3883, loss 0.0286536, acc 0.98
2016-09-06T22:32:43.824429: step 3884, loss 0.0320353, acc 0.98
2016-09-06T22:32:44.508565: step 3885, loss 0.028715, acc 0.98
2016-09-06T22:32:45.187953: step 3886, loss 0.0420477, acc 1
2016-09-06T22:32:45.853262: step 3887, loss 0.0328029, acc 0.98
2016-09-06T22:32:46.550630: step 3888, loss 0.0246017, acc 1
2016-09-06T22:32:47.240778: step 3889, loss 0.0046687, acc 1
2016-09-06T22:32:47.926189: step 3890, loss 0.0554746, acc 0.96
2016-09-06T22:32:48.613251: step 3891, loss 0.0335601, acc 0.98
2016-09-06T22:32:49.321605: step 3892, loss 0.00765643, acc 1
2016-09-06T22:32:49.999392: step 3893, loss 0.0251433, acc 0.98
2016-09-06T22:32:50.674923: step 3894, loss 0.0656511, acc 0.96
2016-09-06T22:32:51.351615: step 3895, loss 0.00567592, acc 1
2016-09-06T22:32:52.042500: step 3896, loss 0.0781344, acc 0.98
2016-09-06T22:32:52.720300: step 3897, loss 0.016905, acc 1
2016-09-06T22:32:53.392297: step 3898, loss 0.0039658, acc 1
2016-09-06T22:32:54.086305: step 3899, loss 0.0210974, acc 1
2016-09-06T22:32:54.759028: step 3900, loss 0.0659961, acc 0.98

Evaluation:
2016-09-06T22:32:57.883715: step 3900, loss 1.60568, acc 0.742964

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-3900

2016-09-06T22:32:59.503770: step 3901, loss 0.0674696, acc 0.96
2016-09-06T22:33:00.184395: step 3902, loss 0.0119216, acc 1
2016-09-06T22:33:00.906786: step 3903, loss 0.0317366, acc 0.98
2016-09-06T22:33:01.600496: step 3904, loss 0.0224872, acc 1
2016-09-06T22:33:02.298795: step 3905, loss 0.00776468, acc 1
2016-09-06T22:33:02.972924: step 3906, loss 0.0259419, acc 1
2016-09-06T22:33:03.663626: step 3907, loss 0.00106463, acc 1
2016-09-06T22:33:04.339984: step 3908, loss 0.0802106, acc 0.96
2016-09-06T22:33:05.037769: step 3909, loss 0.0570767, acc 0.98
2016-09-06T22:33:05.743898: step 3910, loss 0.00274641, acc 1
2016-09-06T22:33:06.435956: step 3911, loss 0.0264439, acc 0.98
2016-09-06T22:33:07.113975: step 3912, loss 0.00840166, acc 1
2016-09-06T22:33:07.801545: step 3913, loss 0.026064, acc 1
2016-09-06T22:33:08.512994: step 3914, loss 0.0237696, acc 0.98
2016-09-06T22:33:09.191211: step 3915, loss 0.0198182, acc 1
2016-09-06T22:33:09.872283: step 3916, loss 0.0151208, acc 1
2016-09-06T22:33:10.564943: step 3917, loss 0.0316306, acc 1
2016-09-06T22:33:11.254324: step 3918, loss 0.00185851, acc 1
2016-09-06T22:33:11.953056: step 3919, loss 0.0081536, acc 1
2016-09-06T22:33:12.630717: step 3920, loss 0.0502558, acc 0.98
2016-09-06T22:33:13.311987: step 3921, loss 0.0234884, acc 1
2016-09-06T22:33:14.004632: step 3922, loss 0.0276381, acc 0.98
2016-09-06T22:33:14.696811: step 3923, loss 0.0233162, acc 1
2016-09-06T22:33:15.377849: step 3924, loss 0.0238304, acc 0.98
2016-09-06T22:33:16.068883: step 3925, loss 4.68371e-05, acc 1
2016-09-06T22:33:16.759792: step 3926, loss 0.03662, acc 0.98
2016-09-06T22:33:17.440299: step 3927, loss 0.0523344, acc 0.98
2016-09-06T22:33:18.143034: step 3928, loss 0.00432307, acc 1
2016-09-06T22:33:18.789102: step 3929, loss 0.0188144, acc 0.98
2016-09-06T22:33:19.505915: step 3930, loss 0.0375738, acc 0.98
2016-09-06T22:33:20.203417: step 3931, loss 0.00871116, acc 1
2016-09-06T22:33:20.895586: step 3932, loss 0.0424469, acc 0.98
2016-09-06T22:33:21.578953: step 3933, loss 0.0167516, acc 1
2016-09-06T22:33:22.262120: step 3934, loss 0.00321357, acc 1
2016-09-06T22:33:22.947874: step 3935, loss 0.0289724, acc 0.98
2016-09-06T22:33:23.617621: step 3936, loss 0.0305113, acc 0.98
2016-09-06T22:33:24.312306: step 3937, loss 0.0500073, acc 0.96
2016-09-06T22:33:25.009299: step 3938, loss 0.0216882, acc 1
2016-09-06T22:33:25.698240: step 3939, loss 0.0267933, acc 0.98
2016-09-06T22:33:26.416262: step 3940, loss 0.0438523, acc 0.98
2016-09-06T22:33:27.099411: step 3941, loss 0.00830552, acc 1
2016-09-06T22:33:27.793987: step 3942, loss 0.00370467, acc 1
2016-09-06T22:33:28.458001: step 3943, loss 0.00414511, acc 1
2016-09-06T22:33:29.167740: step 3944, loss 0.0135954, acc 1
2016-09-06T22:33:29.869992: step 3945, loss 0.000475233, acc 1
2016-09-06T22:33:30.575394: step 3946, loss 0.048027, acc 0.96
2016-09-06T22:33:31.265425: step 3947, loss 0.00555818, acc 1
2016-09-06T22:33:31.921352: step 3948, loss 0.0314489, acc 0.98
2016-09-06T22:33:32.598445: step 3949, loss 0.0322668, acc 0.98
2016-09-06T22:33:33.273246: step 3950, loss 0.1122, acc 0.96
2016-09-06T22:33:33.961374: step 3951, loss 0.035131, acc 0.98
2016-09-06T22:33:34.632436: step 3952, loss 0.0282485, acc 0.98
2016-09-06T22:33:35.327707: step 3953, loss 0.0764523, acc 0.94
2016-09-06T22:33:36.014789: step 3954, loss 0.0525356, acc 0.96
2016-09-06T22:33:36.699933: step 3955, loss 0.000942312, acc 1
2016-09-06T22:33:37.416167: step 3956, loss 0.000912987, acc 1
2016-09-06T22:33:38.095658: step 3957, loss 0.00304587, acc 1
2016-09-06T22:33:38.771364: step 3958, loss 0.0003804, acc 1
2016-09-06T22:33:39.470165: step 3959, loss 0.014975, acc 1
2016-09-06T22:33:40.174577: step 3960, loss 0.0295961, acc 0.98
2016-09-06T22:33:40.871044: step 3961, loss 0.0662172, acc 0.96
2016-09-06T22:33:41.541127: step 3962, loss 0.0292086, acc 0.98
2016-09-06T22:33:42.241538: step 3963, loss 0.0146585, acc 1
2016-09-06T22:33:42.918134: step 3964, loss 0.0371315, acc 1
2016-09-06T22:33:43.618833: step 3965, loss 0.0862116, acc 0.98
2016-09-06T22:33:44.303760: step 3966, loss 0.0155513, acc 0.98
2016-09-06T22:33:44.991637: step 3967, loss 0.00856137, acc 1
2016-09-06T22:33:45.674865: step 3968, loss 0.00216565, acc 1
2016-09-06T22:33:46.356470: step 3969, loss 0.0056679, acc 1
2016-09-06T22:33:47.050835: step 3970, loss 0.0338759, acc 0.96
2016-09-06T22:33:47.734031: step 3971, loss 0.0549118, acc 0.98
2016-09-06T22:33:48.409351: step 3972, loss 0.0142337, acc 1
2016-09-06T22:33:49.099389: step 3973, loss 0.0342686, acc 1
2016-09-06T22:33:49.782146: step 3974, loss 0.00590781, acc 1
2016-09-06T22:33:50.474999: step 3975, loss 0.00660605, acc 1
2016-09-06T22:33:51.153283: step 3976, loss 0.0148937, acc 1
2016-09-06T22:33:51.871539: step 3977, loss 0.0048234, acc 1
2016-09-06T22:33:52.558907: step 3978, loss 0.0178689, acc 1
2016-09-06T22:33:53.229580: step 3979, loss 0.0753887, acc 0.96
2016-09-06T22:33:53.908802: step 3980, loss 0.000541287, acc 1
2016-09-06T22:33:54.600424: step 3981, loss 0.000396737, acc 1
2016-09-06T22:33:55.292788: step 3982, loss 0.0321282, acc 0.98
2016-09-06T22:33:55.955359: step 3983, loss 0.0419383, acc 0.98
2016-09-06T22:33:56.642190: step 3984, loss 0.0179353, acc 1
2016-09-06T22:33:57.319923: step 3985, loss 0.0630572, acc 0.98
2016-09-06T22:33:58.012933: step 3986, loss 0.0448595, acc 0.98
2016-09-06T22:33:58.693097: step 3987, loss 0.0305781, acc 0.98
2016-09-06T22:33:59.401313: step 3988, loss 0.0532619, acc 0.96
2016-09-06T22:34:00.090860: step 3989, loss 0.0128437, acc 1
2016-09-06T22:34:00.801194: step 3990, loss 0.0144965, acc 0.98
2016-09-06T22:34:01.518707: step 3991, loss 0.0126265, acc 1
2016-09-06T22:34:02.177853: step 3992, loss 0.251529, acc 0.98
2016-09-06T22:34:02.874056: step 3993, loss 0.000280976, acc 1
2016-09-06T22:34:03.557378: step 3994, loss 0.0251206, acc 0.98
2016-09-06T22:34:04.243643: step 3995, loss 0.0227231, acc 1
2016-09-06T22:34:04.948927: step 3996, loss 0.0034691, acc 1
2016-09-06T22:34:05.617396: step 3997, loss 0.00444281, acc 1
2016-09-06T22:34:06.332984: step 3998, loss 0.0246203, acc 1
2016-09-06T22:34:07.022288: step 3999, loss 0.0155189, acc 1
2016-09-06T22:34:07.710032: step 4000, loss 0.0328256, acc 0.98

Evaluation:
2016-09-06T22:34:10.880674: step 4000, loss 1.89112, acc 0.741088

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-4000

2016-09-06T22:34:12.696656: step 4001, loss 0.00331213, acc 1
2016-09-06T22:34:13.411059: step 4002, loss 0.0405922, acc 0.98
2016-09-06T22:34:14.110598: step 4003, loss 0.0208364, acc 1
2016-09-06T22:34:14.803658: step 4004, loss 0.00137409, acc 1
2016-09-06T22:34:15.475795: step 4005, loss 0.0325953, acc 0.98
2016-09-06T22:34:16.166270: step 4006, loss 0.00485909, acc 1
2016-09-06T22:34:16.856543: step 4007, loss 0.0460457, acc 0.98
2016-09-06T22:34:17.537476: step 4008, loss 0.024793, acc 0.98
2016-09-06T22:34:18.220436: step 4009, loss 0.0165206, acc 0.98
2016-09-06T22:34:18.896788: step 4010, loss 0.00758892, acc 1
2016-09-06T22:34:19.595525: step 4011, loss 0.0275293, acc 1
2016-09-06T22:34:20.285710: step 4012, loss 0.0363284, acc 0.98
2016-09-06T22:34:20.971193: step 4013, loss 0.0129102, acc 1
2016-09-06T22:34:21.657391: step 4014, loss 0.0452781, acc 0.98
2016-09-06T22:34:22.348918: step 4015, loss 0.0226816, acc 0.98
2016-09-06T22:34:23.059909: step 4016, loss 0.0502893, acc 0.98
2016-09-06T22:34:23.762700: step 4017, loss 0.0198548, acc 1
2016-09-06T22:34:24.481047: step 4018, loss 0.0210128, acc 0.98
2016-09-06T22:34:25.177513: step 4019, loss 0.00383433, acc 1
2016-09-06T22:34:25.872062: step 4020, loss 0.0108065, acc 1
2016-09-06T22:34:26.554817: step 4021, loss 0.010932, acc 1
2016-09-06T22:34:27.223490: step 4022, loss 0.0319998, acc 0.98
2016-09-06T22:34:27.946643: step 4023, loss 0.00194483, acc 1
2016-09-06T22:34:28.637145: step 4024, loss 0.0818419, acc 0.96
2016-09-06T22:34:29.316497: step 4025, loss 0.0160426, acc 1
2016-09-06T22:34:30.000966: step 4026, loss 0.123231, acc 0.98
2016-09-06T22:34:30.690726: step 4027, loss 0.04783, acc 0.96
2016-09-06T22:34:31.357812: step 4028, loss 0.0406155, acc 0.98
2016-09-06T22:34:32.026010: step 4029, loss 0.0119034, acc 1
2016-09-06T22:34:32.740945: step 4030, loss 0.0376484, acc 0.96
2016-09-06T22:34:33.411402: step 4031, loss 0.0130532, acc 1
2016-09-06T22:34:34.031475: step 4032, loss 0.0184889, acc 0.977273
2016-09-06T22:34:34.709231: step 4033, loss 0.0116098, acc 1
2016-09-06T22:34:35.372136: step 4034, loss 0.0180794, acc 0.98
2016-09-06T22:34:36.046423: step 4035, loss 0.0642136, acc 0.98
2016-09-06T22:34:36.721343: step 4036, loss 0.0267027, acc 0.98
2016-09-06T22:34:37.416008: step 4037, loss 0.0121108, acc 1
2016-09-06T22:34:38.089081: step 4038, loss 0.0444881, acc 0.98
2016-09-06T22:34:38.780273: step 4039, loss 0.00752259, acc 1
2016-09-06T22:34:39.488159: step 4040, loss 0.00438818, acc 1
2016-09-06T22:34:40.162213: step 4041, loss 0.0147078, acc 1
2016-09-06T22:34:40.851563: step 4042, loss 0.0115297, acc 1
2016-09-06T22:34:41.540577: step 4043, loss 0.0246046, acc 0.98
2016-09-06T22:34:42.250480: step 4044, loss 0.0501854, acc 0.98
2016-09-06T22:34:42.917440: step 4045, loss 0.10144, acc 0.98
2016-09-06T22:34:43.632037: step 4046, loss 0.122767, acc 0.92
2016-09-06T22:34:44.328069: step 4047, loss 0.000211371, acc 1
2016-09-06T22:34:45.009814: step 4048, loss 0.00127935, acc 1
2016-09-06T22:34:45.704679: step 4049, loss 0.00209938, acc 1
2016-09-06T22:34:46.390132: step 4050, loss 0.00377362, acc 1
2016-09-06T22:34:47.115094: step 4051, loss 0.0498015, acc 0.96
2016-09-06T22:34:47.787515: step 4052, loss 0.0120877, acc 1
2016-09-06T22:34:48.471665: step 4053, loss 0.00328466, acc 1
2016-09-06T22:34:49.149994: step 4054, loss 0.022111, acc 1
2016-09-06T22:34:49.825619: step 4055, loss 0.00675981, acc 1
2016-09-06T22:34:50.530394: step 4056, loss 0.00686328, acc 1
2016-09-06T22:34:51.228679: step 4057, loss 0.0414932, acc 0.96
2016-09-06T22:34:51.934939: step 4058, loss 0.0445525, acc 0.98
2016-09-06T22:34:52.599618: step 4059, loss 0.00256373, acc 1
2016-09-06T22:34:53.277786: step 4060, loss 0.0237925, acc 0.98
2016-09-06T22:34:53.958459: step 4061, loss 0.244373, acc 0.94
2016-09-06T22:34:54.632714: step 4062, loss 0.0286946, acc 0.98
2016-09-06T22:34:55.329206: step 4063, loss 0.0175047, acc 1
2016-09-06T22:34:56.018501: step 4064, loss 0.0895247, acc 0.98
2016-09-06T22:34:56.724078: step 4065, loss 0.00418186, acc 1
2016-09-06T22:34:57.397899: step 4066, loss 0.0266389, acc 0.98
2016-09-06T22:34:58.094472: step 4067, loss 0.0147893, acc 1
2016-09-06T22:34:58.810098: step 4068, loss 0.0133006, acc 1
2016-09-06T22:34:59.485459: step 4069, loss 0.0258503, acc 0.98
2016-09-06T22:35:00.163388: step 4070, loss 0.140071, acc 0.98
2016-09-06T22:35:00.872830: step 4071, loss 5.11925e-05, acc 1
2016-09-06T22:35:01.575205: step 4072, loss 0.00212109, acc 1
2016-09-06T22:35:02.257806: step 4073, loss 0.0256408, acc 1
2016-09-06T22:35:02.935203: step 4074, loss 0.0188913, acc 0.98
2016-09-06T22:35:03.618436: step 4075, loss 0.000623886, acc 1
2016-09-06T22:35:04.307186: step 4076, loss 0.0104087, acc 1
2016-09-06T22:35:04.998158: step 4077, loss 0.0219294, acc 1
2016-09-06T22:35:05.683694: step 4078, loss 0.0425696, acc 0.96
2016-09-06T22:35:06.402212: step 4079, loss 0.0438675, acc 0.98
2016-09-06T22:35:07.087504: step 4080, loss 0.0284233, acc 0.98
2016-09-06T22:35:07.758895: step 4081, loss 0.00103959, acc 1
2016-09-06T22:35:08.437165: step 4082, loss 0.0930097, acc 0.98
2016-09-06T22:35:09.112451: step 4083, loss 0.0122295, acc 1
2016-09-06T22:35:09.801217: step 4084, loss 0.0923444, acc 0.96
2016-09-06T22:35:10.479518: step 4085, loss 0.0152355, acc 1
2016-09-06T22:35:11.166351: step 4086, loss 0.000371704, acc 1
2016-09-06T22:35:11.854718: step 4087, loss 0.0010262, acc 1
2016-09-06T22:35:12.529971: step 4088, loss 0.0371856, acc 0.98
2016-09-06T22:35:13.241632: step 4089, loss 0.0441206, acc 0.96
2016-09-06T22:35:13.926580: step 4090, loss 0.00488282, acc 1
2016-09-06T22:35:14.607041: step 4091, loss 0.00756966, acc 1
2016-09-06T22:35:15.278792: step 4092, loss 0.0413751, acc 0.98
2016-09-06T22:35:15.992318: step 4093, loss 0.0655401, acc 0.94
2016-09-06T22:35:16.675490: step 4094, loss 0.0226077, acc 0.98
2016-09-06T22:35:17.366426: step 4095, loss 0.0222952, acc 0.98
2016-09-06T22:35:18.063516: step 4096, loss 0.0315533, acc 0.96
2016-09-06T22:35:18.771198: step 4097, loss 0.0127299, acc 1
2016-09-06T22:35:19.453551: step 4098, loss 0.0574069, acc 0.98
2016-09-06T22:35:20.125878: step 4099, loss 0.0452078, acc 0.98
2016-09-06T22:35:20.827963: step 4100, loss 0.00441554, acc 1

Evaluation:
2016-09-06T22:35:24.002594: step 4100, loss 1.79843, acc 0.741088

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-4100

2016-09-06T22:35:25.625797: step 4101, loss 0.00584072, acc 1
2016-09-06T22:35:26.308280: step 4102, loss 0.0355372, acc 0.98
2016-09-06T22:35:26.991096: step 4103, loss 0.0323548, acc 1
2016-09-06T22:35:27.704008: step 4104, loss 0.00339775, acc 1
2016-09-06T22:35:28.406759: step 4105, loss 0.101569, acc 0.94
2016-09-06T22:35:29.100246: step 4106, loss 0.0471178, acc 0.98
2016-09-06T22:35:29.759422: step 4107, loss 0.0158584, acc 1
2016-09-06T22:35:30.450238: step 4108, loss 0.0478169, acc 0.98
2016-09-06T22:35:31.113398: step 4109, loss 0.0772542, acc 0.98
2016-09-06T22:35:31.801157: step 4110, loss 0.0468568, acc 0.98
2016-09-06T22:35:32.478057: step 4111, loss 0.019008, acc 0.98
2016-09-06T22:35:33.198111: step 4112, loss 0.0457073, acc 1
2016-09-06T22:35:33.899028: step 4113, loss 0.0365835, acc 0.98
2016-09-06T22:35:34.557486: step 4114, loss 0.028161, acc 0.98
2016-09-06T22:35:35.267481: step 4115, loss 0.00700347, acc 1
2016-09-06T22:35:35.939023: step 4116, loss 0.0183846, acc 1
2016-09-06T22:35:36.600358: step 4117, loss 0.0366338, acc 0.98
2016-09-06T22:35:37.282226: step 4118, loss 0.0147886, acc 1
2016-09-06T22:35:37.951615: step 4119, loss 0.0431822, acc 0.98
2016-09-06T22:35:38.611910: step 4120, loss 0.0406101, acc 0.96
2016-09-06T22:35:39.304308: step 4121, loss 0.0847068, acc 0.94
2016-09-06T22:35:39.988629: step 4122, loss 0.0203954, acc 1
2016-09-06T22:35:40.667803: step 4123, loss 0.00311022, acc 1
2016-09-06T22:35:41.361884: step 4124, loss 0.00377915, acc 1
2016-09-06T22:35:42.030490: step 4125, loss 0.00730581, acc 1
2016-09-06T22:35:42.729142: step 4126, loss 0.0140307, acc 1
2016-09-06T22:35:43.421682: step 4127, loss 0.0294821, acc 1
2016-09-06T22:35:44.121654: step 4128, loss 0.0235385, acc 0.98
2016-09-06T22:35:44.837566: step 4129, loss 0.0213577, acc 0.98
2016-09-06T22:35:45.524799: step 4130, loss 0.0246282, acc 1
2016-09-06T22:35:46.224783: step 4131, loss 0.0144895, acc 1
2016-09-06T22:35:46.892646: step 4132, loss 0.0180639, acc 1
2016-09-06T22:35:47.575942: step 4133, loss 0.0199901, acc 1
2016-09-06T22:35:48.265489: step 4134, loss 0.000881387, acc 1
2016-09-06T22:35:48.960867: step 4135, loss 0.0213603, acc 0.98
2016-09-06T22:35:49.673721: step 4136, loss 0.0036633, acc 1
2016-09-06T22:35:50.345866: step 4137, loss 0.0326047, acc 0.98
2016-09-06T22:35:51.053015: step 4138, loss 0.0150728, acc 1
2016-09-06T22:35:51.743678: step 4139, loss 0.0190444, acc 0.98
2016-09-06T22:35:52.450797: step 4140, loss 0.0285116, acc 0.98
2016-09-06T22:35:53.128382: step 4141, loss 0.035737, acc 0.96
2016-09-06T22:35:53.809722: step 4142, loss 0.0107196, acc 1
2016-09-06T22:35:54.515044: step 4143, loss 0.00974951, acc 1
2016-09-06T22:35:55.199085: step 4144, loss 0.0794226, acc 0.96
2016-09-06T22:35:55.891813: step 4145, loss 0.017724, acc 1
2016-09-06T22:35:56.588302: step 4146, loss 0.0371202, acc 0.98
2016-09-06T22:35:57.274214: step 4147, loss 0.0235663, acc 0.98
2016-09-06T22:35:57.960804: step 4148, loss 0.000682696, acc 1
2016-09-06T22:35:58.607245: step 4149, loss 0.00949036, acc 1
2016-09-06T22:35:59.326190: step 4150, loss 0.0255481, acc 0.98
2016-09-06T22:36:00.016610: step 4151, loss 0.0202836, acc 1
2016-09-06T22:36:00.730506: step 4152, loss 0.0504521, acc 0.96
2016-09-06T22:36:01.426862: step 4153, loss 0.0262409, acc 0.98
2016-09-06T22:36:02.114920: step 4154, loss 0.0436415, acc 0.98
2016-09-06T22:36:02.820727: step 4155, loss 0.000297065, acc 1
2016-09-06T22:36:03.494564: step 4156, loss 0.0169315, acc 0.98
2016-09-06T22:36:04.217905: step 4157, loss 0.060249, acc 0.98
2016-09-06T22:36:04.911562: step 4158, loss 0.146885, acc 0.96
2016-09-06T22:36:05.614140: step 4159, loss 0.028219, acc 0.98
2016-09-06T22:36:06.301159: step 4160, loss 0.00425044, acc 1
2016-09-06T22:36:06.980637: step 4161, loss 1.61561e-05, acc 1
2016-09-06T22:36:07.691760: step 4162, loss 0.0914962, acc 0.98
2016-09-06T22:36:08.371846: step 4163, loss 0.0652809, acc 0.96
2016-09-06T22:36:09.068749: step 4164, loss 0.0167112, acc 0.98
2016-09-06T22:36:09.753206: step 4165, loss 0.0592223, acc 0.98
2016-09-06T22:36:10.449017: step 4166, loss 0.0572236, acc 0.96
2016-09-06T22:36:11.130167: step 4167, loss 0.0104981, acc 1
2016-09-06T22:36:11.792982: step 4168, loss 0.0115848, acc 1
2016-09-06T22:36:12.490060: step 4169, loss 0.0176447, acc 1
2016-09-06T22:36:13.166355: step 4170, loss 0.000356258, acc 1
2016-09-06T22:36:13.855282: step 4171, loss 0.035731, acc 0.98
2016-09-06T22:36:14.541002: step 4172, loss 0.0071355, acc 1
2016-09-06T22:36:15.243032: step 4173, loss 0.0127961, acc 1
2016-09-06T22:36:15.931359: step 4174, loss 0.0305551, acc 0.98
2016-09-06T22:36:16.613321: step 4175, loss 0.0107844, acc 1
2016-09-06T22:36:17.341086: step 4176, loss 0.0309729, acc 0.98
2016-09-06T22:36:18.024804: step 4177, loss 0.0579366, acc 0.98
2016-09-06T22:36:18.705676: step 4178, loss 0.0133783, acc 1
2016-09-06T22:36:19.384385: step 4179, loss 0.0217173, acc 0.98
2016-09-06T22:36:20.062020: step 4180, loss 0.0749729, acc 0.98
2016-09-06T22:36:20.758483: step 4181, loss 0.0550378, acc 0.96
2016-09-06T22:36:21.440160: step 4182, loss 0.00821754, acc 1
2016-09-06T22:36:22.139030: step 4183, loss 0.0394086, acc 0.98
2016-09-06T22:36:22.832980: step 4184, loss 0.00850384, acc 1
2016-09-06T22:36:23.505978: step 4185, loss 0.0110389, acc 1
2016-09-06T22:36:24.173744: step 4186, loss 0.0423958, acc 0.98
2016-09-06T22:36:24.863315: step 4187, loss 0.0467139, acc 0.96
2016-09-06T22:36:25.558744: step 4188, loss 0.00262706, acc 1
2016-09-06T22:36:26.223770: step 4189, loss 0.0455109, acc 0.98
2016-09-06T22:36:26.931245: step 4190, loss 0.00356546, acc 1
2016-09-06T22:36:27.613485: step 4191, loss 0.0302329, acc 0.98
2016-09-06T22:36:28.297682: step 4192, loss 0.0477193, acc 0.96
2016-09-06T22:36:29.008363: step 4193, loss 0.00265914, acc 1
2016-09-06T22:36:29.702651: step 4194, loss 0.0161935, acc 1
2016-09-06T22:36:30.396504: step 4195, loss 0.0206039, acc 0.98
2016-09-06T22:36:31.106795: step 4196, loss 0.0535568, acc 0.98
2016-09-06T22:36:31.823744: step 4197, loss 0.0177789, acc 1
2016-09-06T22:36:32.521649: step 4198, loss 0.0214084, acc 1
2016-09-06T22:36:33.227806: step 4199, loss 0.0407557, acc 0.96
2016-09-06T22:36:33.930076: step 4200, loss 0.0168032, acc 1

Evaluation:
2016-09-06T22:36:37.071833: step 4200, loss 2.19731, acc 0.752345

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-4200

2016-09-06T22:36:38.774798: step 4201, loss 0.0241774, acc 0.98
2016-09-06T22:36:39.440287: step 4202, loss 0.00415992, acc 1
2016-09-06T22:36:40.149158: step 4203, loss 0.0445253, acc 0.96
2016-09-06T22:36:40.839823: step 4204, loss 0.0433796, acc 0.98
2016-09-06T22:36:41.551409: step 4205, loss 0.000764448, acc 1
2016-09-06T22:36:42.238694: step 4206, loss 0.0656412, acc 0.96
2016-09-06T22:36:42.918128: step 4207, loss 0.00212415, acc 1
2016-09-06T22:36:43.604079: step 4208, loss 0.0588752, acc 0.94
2016-09-06T22:36:44.300223: step 4209, loss 0.0190439, acc 0.98
2016-09-06T22:36:45.012484: step 4210, loss 0.00279268, acc 1
2016-09-06T22:36:45.695430: step 4211, loss 0.0983128, acc 0.98
2016-09-06T22:36:46.394178: step 4212, loss 0.0271163, acc 0.98
2016-09-06T22:36:47.083834: step 4213, loss 0.103663, acc 0.98
2016-09-06T22:36:47.760310: step 4214, loss 0.00419751, acc 1
2016-09-06T22:36:48.460119: step 4215, loss 0.00283159, acc 1
2016-09-06T22:36:49.116940: step 4216, loss 0.0422431, acc 0.98
2016-09-06T22:36:49.816208: step 4217, loss 0.0211457, acc 1
2016-09-06T22:36:50.489249: step 4218, loss 0.011817, acc 1
2016-09-06T22:36:51.172696: step 4219, loss 0.0200739, acc 1
2016-09-06T22:36:51.869218: step 4220, loss 0.000150098, acc 1
2016-09-06T22:36:52.555834: step 4221, loss 0.0367266, acc 0.98
2016-09-06T22:36:53.245869: step 4222, loss 0.100606, acc 0.92
2016-09-06T22:36:53.924715: step 4223, loss 0.0194512, acc 1
2016-09-06T22:36:54.586909: step 4224, loss 0.000282284, acc 1
2016-09-06T22:36:55.257379: step 4225, loss 0.0250941, acc 1
2016-09-06T22:36:55.946010: step 4226, loss 0.0464299, acc 0.96
2016-09-06T22:36:56.615512: step 4227, loss 0.0898845, acc 0.96
2016-09-06T22:36:57.309978: step 4228, loss 0.0672687, acc 0.96
2016-09-06T22:36:58.036905: step 4229, loss 0.0139326, acc 1
2016-09-06T22:36:58.719140: step 4230, loss 0.0400877, acc 1
2016-09-06T22:36:59.418939: step 4231, loss 0.00393841, acc 1
2016-09-06T22:37:00.087782: step 4232, loss 0.0372959, acc 0.98
2016-09-06T22:37:00.809412: step 4233, loss 0.0333427, acc 0.98
2016-09-06T22:37:01.493417: step 4234, loss 0.00633905, acc 1
2016-09-06T22:37:02.150518: step 4235, loss 0.0384782, acc 0.98
2016-09-06T22:37:02.835996: step 4236, loss 0.013146, acc 1
2016-09-06T22:37:03.521832: step 4237, loss 0.0756283, acc 0.94
2016-09-06T22:37:04.231957: step 4238, loss 0.00467472, acc 1
2016-09-06T22:37:04.908248: step 4239, loss 0.0153025, acc 0.98
2016-09-06T22:37:05.594188: step 4240, loss 0.00811515, acc 1
2016-09-06T22:37:06.274788: step 4241, loss 0.115617, acc 0.94
2016-09-06T22:37:06.973220: step 4242, loss 0.0049616, acc 1
2016-09-06T22:37:07.664709: step 4243, loss 0.0314938, acc 0.98
2016-09-06T22:37:08.323731: step 4244, loss 0.0010673, acc 1
2016-09-06T22:37:09.020165: step 4245, loss 0.0399445, acc 0.98
2016-09-06T22:37:09.698174: step 4246, loss 0.0781866, acc 0.96
2016-09-06T22:37:10.369866: step 4247, loss 0.0332631, acc 0.98
2016-09-06T22:37:11.075110: step 4248, loss 0.0188887, acc 0.98
2016-09-06T22:37:11.754832: step 4249, loss 0.0385145, acc 0.98
2016-09-06T22:37:12.455934: step 4250, loss 0.0333852, acc 0.98
2016-09-06T22:37:13.143968: step 4251, loss 0.00788747, acc 1
2016-09-06T22:37:13.857734: step 4252, loss 0.00501029, acc 1
2016-09-06T22:37:14.527575: step 4253, loss 0.0148431, acc 0.98
2016-09-06T22:37:15.200400: step 4254, loss 0.0799596, acc 0.98
2016-09-06T22:37:15.880333: step 4255, loss 0.0088857, acc 1
2016-09-06T22:37:16.545842: step 4256, loss 0.0439491, acc 0.98
2016-09-06T22:37:17.238269: step 4257, loss 0.0288615, acc 0.98
2016-09-06T22:37:17.925011: step 4258, loss 0.0677607, acc 0.96
2016-09-06T22:37:18.596172: step 4259, loss 0.121318, acc 0.98
2016-09-06T22:37:19.267687: step 4260, loss 0.0118448, acc 1
2016-09-06T22:37:19.961808: step 4261, loss 0.0146751, acc 1
2016-09-06T22:37:20.665823: step 4262, loss 0.0628778, acc 0.96
2016-09-06T22:37:21.353556: step 4263, loss 0.000266664, acc 1
2016-09-06T22:37:22.024953: step 4264, loss 0.000558688, acc 1
2016-09-06T22:37:22.713450: step 4265, loss 0.033599, acc 0.98
2016-09-06T22:37:23.415273: step 4266, loss 0.0242387, acc 0.98
2016-09-06T22:37:24.100744: step 4267, loss 0.0310333, acc 0.98
2016-09-06T22:37:24.788175: step 4268, loss 0.00537553, acc 1
2016-09-06T22:37:25.473854: step 4269, loss 0.0405378, acc 0.98
2016-09-06T22:37:26.166377: step 4270, loss 0.0275679, acc 1
2016-09-06T22:37:26.833547: step 4271, loss 0.0793253, acc 0.96
2016-09-06T22:37:27.525134: step 4272, loss 0.0195319, acc 1
2016-09-06T22:37:28.219693: step 4273, loss 0.010797, acc 1
2016-09-06T22:37:28.903979: step 4274, loss 0.0215015, acc 0.98
2016-09-06T22:37:29.601829: step 4275, loss 0.00100362, acc 1
2016-09-06T22:37:30.281490: step 4276, loss 0.000425289, acc 1
2016-09-06T22:37:30.959826: step 4277, loss 0.0249142, acc 0.98
2016-09-06T22:37:31.664430: step 4278, loss 0.090516, acc 0.96
2016-09-06T22:37:32.373695: step 4279, loss 0.00466888, acc 1
2016-09-06T22:37:33.097259: step 4280, loss 0.0525489, acc 0.98
2016-09-06T22:37:33.766394: step 4281, loss 0.0336153, acc 0.98
2016-09-06T22:37:34.456085: step 4282, loss 0.00519412, acc 1
2016-09-06T22:37:35.131779: step 4283, loss 0.0176305, acc 1
2016-09-06T22:37:35.814665: step 4284, loss 0.0174127, acc 1
2016-09-06T22:37:36.502235: step 4285, loss 0.104635, acc 0.96
2016-09-06T22:37:37.193107: step 4286, loss 0.010628, acc 1
2016-09-06T22:37:37.901651: step 4287, loss 0.0186742, acc 1
2016-09-06T22:37:38.589207: step 4288, loss 0.0305652, acc 0.98
2016-09-06T22:37:39.291352: step 4289, loss 0.00645447, acc 1
2016-09-06T22:37:39.973138: step 4290, loss 0.0300767, acc 1
2016-09-06T22:37:40.675942: step 4291, loss 0.0165178, acc 1
2016-09-06T22:37:41.351094: step 4292, loss 0.0275264, acc 0.98
2016-09-06T22:37:42.022067: step 4293, loss 0.039855, acc 0.96
2016-09-06T22:37:42.729755: step 4294, loss 0.0105233, acc 1
2016-09-06T22:37:43.419617: step 4295, loss 0.038219, acc 0.98
2016-09-06T22:37:44.105612: step 4296, loss 0.00123463, acc 1
2016-09-06T22:37:44.795257: step 4297, loss 0.0397566, acc 0.98
2016-09-06T22:37:45.496924: step 4298, loss 0.0329997, acc 0.98
2016-09-06T22:37:46.185184: step 4299, loss 0.0174989, acc 0.98
2016-09-06T22:37:46.836978: step 4300, loss 0.0331518, acc 0.98

Evaluation:
2016-09-06T22:37:49.981348: step 4300, loss 2.32839, acc 0.74015

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-4300

2016-09-06T22:37:51.694337: step 4301, loss 0.0122853, acc 1
2016-09-06T22:37:52.400669: step 4302, loss 0.00372385, acc 1
2016-09-06T22:37:53.109556: step 4303, loss 0.00262447, acc 1
2016-09-06T22:37:53.808329: step 4304, loss 0.0205358, acc 1
2016-09-06T22:37:54.493121: step 4305, loss 0.0807469, acc 0.98
2016-09-06T22:37:55.178614: step 4306, loss 0.157172, acc 0.96
2016-09-06T22:37:55.887290: step 4307, loss 0.00338775, acc 1
2016-09-06T22:37:56.599206: step 4308, loss 0.119468, acc 0.94
2016-09-06T22:37:57.291425: step 4309, loss 0.000310563, acc 1
2016-09-06T22:37:58.000341: step 4310, loss 0.0137872, acc 1
2016-09-06T22:37:58.698534: step 4311, loss 0.0194547, acc 0.98
2016-09-06T22:37:59.407130: step 4312, loss 0.00136744, acc 1
2016-09-06T22:38:00.081148: step 4313, loss 0.0366046, acc 0.98
2016-09-06T22:38:00.827288: step 4314, loss 0.036202, acc 0.98
2016-09-06T22:38:01.523603: step 4315, loss 0.00425135, acc 1
2016-09-06T22:38:02.211932: step 4316, loss 0.0133642, acc 1
2016-09-06T22:38:02.896796: step 4317, loss 0.0174421, acc 1
2016-09-06T22:38:03.573933: step 4318, loss 0.0197248, acc 1
2016-09-06T22:38:04.252907: step 4319, loss 0.057139, acc 0.96
2016-09-06T22:38:04.930172: step 4320, loss 0.0270635, acc 0.98
2016-09-06T22:38:05.668832: step 4321, loss 0.018169, acc 0.98
2016-09-06T22:38:06.349588: step 4322, loss 0.0414645, acc 0.98
2016-09-06T22:38:07.046449: step 4323, loss 0.0953149, acc 0.98
2016-09-06T22:38:07.735549: step 4324, loss 0.0347086, acc 1
2016-09-06T22:38:08.442670: step 4325, loss 0.0510272, acc 0.98
2016-09-06T22:38:09.156448: step 4326, loss 0.000707718, acc 1
2016-09-06T22:38:09.818863: step 4327, loss 0.0927682, acc 0.98
2016-09-06T22:38:10.501644: step 4328, loss 0.0522682, acc 0.98
2016-09-06T22:38:11.190288: step 4329, loss 0.00485952, acc 1
2016-09-06T22:38:11.866270: step 4330, loss 0.00192118, acc 1
2016-09-06T22:38:12.569493: step 4331, loss 0.0138689, acc 1
2016-09-06T22:38:13.231997: step 4332, loss 0.00859026, acc 1
2016-09-06T22:38:13.952066: step 4333, loss 0.0257371, acc 0.98
2016-09-06T22:38:14.645367: step 4334, loss 0.0214732, acc 1
2016-09-06T22:38:15.344089: step 4335, loss 0.0961314, acc 0.96
2016-09-06T22:38:16.025924: step 4336, loss 0.0189827, acc 0.98
2016-09-06T22:38:16.714494: step 4337, loss 0.0721313, acc 0.96
2016-09-06T22:38:17.389681: step 4338, loss 0.0220554, acc 0.98
2016-09-06T22:38:18.080471: step 4339, loss 0.0153029, acc 1
2016-09-06T22:38:18.779887: step 4340, loss 0.0115693, acc 1
2016-09-06T22:38:19.469161: step 4341, loss 0.000789805, acc 1
2016-09-06T22:38:20.159096: step 4342, loss 0.0157706, acc 0.98
2016-09-06T22:38:20.847627: step 4343, loss 0.0536994, acc 0.96
2016-09-06T22:38:21.523365: step 4344, loss 0.018118, acc 1
2016-09-06T22:38:22.209341: step 4345, loss 0.0332252, acc 0.96
2016-09-06T22:38:22.906335: step 4346, loss 0.0150459, acc 1
2016-09-06T22:38:23.616186: step 4347, loss 0.00892148, acc 1
2016-09-06T22:38:24.289023: step 4348, loss 0.0351847, acc 0.98
2016-09-06T22:38:24.991520: step 4349, loss 0.090678, acc 0.98
2016-09-06T22:38:25.673009: step 4350, loss 0.117556, acc 0.96
2016-09-06T22:38:26.335306: step 4351, loss 0.0573722, acc 0.96
2016-09-06T22:38:27.014867: step 4352, loss 0.0670573, acc 0.94
2016-09-06T22:38:27.700477: step 4353, loss 0.00846655, acc 1
2016-09-06T22:38:28.381358: step 4354, loss 0.0286795, acc 0.98
2016-09-06T22:38:29.077452: step 4355, loss 0.0990484, acc 0.94
2016-09-06T22:38:29.746945: step 4356, loss 0.0629626, acc 0.98
2016-09-06T22:38:30.432369: step 4357, loss 0.0271617, acc 0.98
2016-09-06T22:38:31.146090: step 4358, loss 0.0145081, acc 1
2016-09-06T22:38:31.853400: step 4359, loss 0.00716145, acc 1
2016-09-06T22:38:32.538364: step 4360, loss 0.0130338, acc 1
2016-09-06T22:38:33.265621: step 4361, loss 0.0217558, acc 0.98
2016-09-06T22:38:33.955049: step 4362, loss 0.000599897, acc 1
2016-09-06T22:38:34.644403: step 4363, loss 0.0331845, acc 1
2016-09-06T22:38:35.323847: step 4364, loss 0.0338413, acc 0.98
2016-09-06T22:38:36.028077: step 4365, loss 0.0412041, acc 0.98
2016-09-06T22:38:36.709614: step 4366, loss 0.0627035, acc 0.96
2016-09-06T22:38:37.371881: step 4367, loss 0.00984491, acc 1
2016-09-06T22:38:38.080921: step 4368, loss 0.00266313, acc 1
2016-09-06T22:38:38.750306: step 4369, loss 0.0107789, acc 1
2016-09-06T22:38:39.425347: step 4370, loss 0.0494933, acc 0.96
2016-09-06T22:38:40.099621: step 4371, loss 0.0699963, acc 0.98
2016-09-06T22:38:40.798758: step 4372, loss 0.0491021, acc 0.96
2016-09-06T22:38:41.469497: step 4373, loss 0.0160524, acc 1
2016-09-06T22:38:42.134033: step 4374, loss 0.00572014, acc 1
2016-09-06T22:38:42.837129: step 4375, loss 0.0375601, acc 0.98
2016-09-06T22:38:43.505102: step 4376, loss 0.0509678, acc 0.98
2016-09-06T22:38:44.177429: step 4377, loss 0.00905896, acc 1
2016-09-06T22:38:44.841694: step 4378, loss 0.0456528, acc 0.98
2016-09-06T22:38:45.519537: step 4379, loss 0.0107675, acc 1
2016-09-06T22:38:46.206877: step 4380, loss 0.0874078, acc 0.96
2016-09-06T22:38:46.913372: step 4381, loss 0.114654, acc 0.96
2016-09-06T22:38:47.598407: step 4382, loss 0.0314863, acc 0.98
2016-09-06T22:38:48.299552: step 4383, loss 0.0255574, acc 0.98
2016-09-06T22:38:48.989776: step 4384, loss 0.061123, acc 0.98
2016-09-06T22:38:49.676392: step 4385, loss 0.000616985, acc 1
2016-09-06T22:38:50.394783: step 4386, loss 0.0053289, acc 1
2016-09-06T22:38:51.127719: step 4387, loss 0.0229276, acc 1
2016-09-06T22:38:51.789834: step 4388, loss 0.0628186, acc 0.96
2016-09-06T22:38:52.490098: step 4389, loss 0.0455294, acc 0.98
2016-09-06T22:38:53.163023: step 4390, loss 0.054347, acc 0.96
2016-09-06T22:38:53.843276: step 4391, loss 0.00272859, acc 1
2016-09-06T22:38:54.523570: step 4392, loss 0.0345787, acc 0.98
2016-09-06T22:38:55.227918: step 4393, loss 0.0140788, acc 1
2016-09-06T22:38:55.894031: step 4394, loss 0.155989, acc 0.98
2016-09-06T22:38:56.564122: step 4395, loss 0.00754629, acc 1
2016-09-06T22:38:57.266915: step 4396, loss 0.0197137, acc 1
2016-09-06T22:38:57.933437: step 4397, loss 0.0132234, acc 1
2016-09-06T22:38:58.626570: step 4398, loss 0.00905677, acc 1
2016-09-06T22:38:59.310054: step 4399, loss 0.00905146, acc 1
2016-09-06T22:39:00.000980: step 4400, loss 0.0278509, acc 0.98

Evaluation:
2016-09-06T22:39:03.177982: step 4400, loss 2.08587, acc 0.727955

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-4400

2016-09-06T22:39:04.900732: step 4401, loss 0.03189, acc 0.98
2016-09-06T22:39:05.590159: step 4402, loss 0.0168599, acc 0.98
2016-09-06T22:39:06.246864: step 4403, loss 0.0099628, acc 1
2016-09-06T22:39:06.952897: step 4404, loss 0.0792342, acc 0.98
2016-09-06T22:39:07.642811: step 4405, loss 0.00431718, acc 1
2016-09-06T22:39:08.317787: step 4406, loss 0.00131574, acc 1
2016-09-06T22:39:08.996772: step 4407, loss 0.00123309, acc 1
2016-09-06T22:39:09.702440: step 4408, loss 0.0193181, acc 1
2016-09-06T22:39:10.397585: step 4409, loss 0.00817901, acc 1
2016-09-06T22:39:11.076094: step 4410, loss 0.0102511, acc 1
2016-09-06T22:39:11.797180: step 4411, loss 0.0268849, acc 0.98
2016-09-06T22:39:12.458642: step 4412, loss 8.36483e-05, acc 1
2016-09-06T22:39:13.133389: step 4413, loss 0.0112333, acc 1
2016-09-06T22:39:13.827902: step 4414, loss 0.0698834, acc 0.96
2016-09-06T22:39:14.524197: step 4415, loss 0.00249798, acc 1
2016-09-06T22:39:15.186866: step 4416, loss 0.000205979, acc 1
2016-09-06T22:39:15.852165: step 4417, loss 0.0353783, acc 0.98
2016-09-06T22:39:16.532471: step 4418, loss 0.183862, acc 0.94
2016-09-06T22:39:17.225023: step 4419, loss 0.0550608, acc 0.96
2016-09-06T22:39:17.909575: step 4420, loss 0.0388472, acc 0.98
2016-09-06T22:39:18.600185: step 4421, loss 0.0766403, acc 0.94
2016-09-06T22:39:19.289809: step 4422, loss 0.12742, acc 0.96
2016-09-06T22:39:19.986601: step 4423, loss 0.0074532, acc 1
2016-09-06T22:39:20.660754: step 4424, loss 0.043424, acc 0.98
2016-09-06T22:39:21.367611: step 4425, loss 9.7981e-05, acc 1
2016-09-06T22:39:22.079158: step 4426, loss 0.0501073, acc 0.96
2016-09-06T22:39:22.778735: step 4427, loss 0.0357517, acc 0.98
2016-09-06T22:39:23.464162: step 4428, loss 0.0335574, acc 0.98
2016-09-06T22:39:24.151859: step 4429, loss 0.0562335, acc 0.98
2016-09-06T22:39:24.860466: step 4430, loss 0.0210169, acc 0.98
2016-09-06T22:39:25.523666: step 4431, loss 0.0235449, acc 1
2016-09-06T22:39:26.251122: step 4432, loss 0.00242782, acc 1
2016-09-06T22:39:26.945760: step 4433, loss 0.187444, acc 0.94
2016-09-06T22:39:27.643309: step 4434, loss 0.067526, acc 0.98
2016-09-06T22:39:28.340759: step 4435, loss 0.0532022, acc 0.98
2016-09-06T22:39:29.054398: step 4436, loss 0.0157878, acc 0.98
2016-09-06T22:39:29.757626: step 4437, loss 0.0124723, acc 1
2016-09-06T22:39:30.423030: step 4438, loss 0.00735687, acc 1
2016-09-06T22:39:31.108990: step 4439, loss 0.0164346, acc 1
2016-09-06T22:39:31.802718: step 4440, loss 0.0102142, acc 1
2016-09-06T22:39:32.496010: step 4441, loss 0.0250614, acc 1
2016-09-06T22:39:33.200044: step 4442, loss 0.000391153, acc 1
2016-09-06T22:39:33.874202: step 4443, loss 0.00359195, acc 1
2016-09-06T22:39:34.579567: step 4444, loss 0.0356937, acc 0.98
2016-09-06T22:39:35.252244: step 4445, loss 0.0217718, acc 1
2016-09-06T22:39:35.949601: step 4446, loss 0.0254895, acc 0.98
2016-09-06T22:39:36.649347: step 4447, loss 0.0376724, acc 0.98
2016-09-06T22:39:37.339026: step 4448, loss 0.024471, acc 1
2016-09-06T22:39:38.035835: step 4449, loss 0.0103667, acc 1
2016-09-06T22:39:38.710524: step 4450, loss 0.0250899, acc 1
2016-09-06T22:39:39.393997: step 4451, loss 0.0183508, acc 1
2016-09-06T22:39:40.063206: step 4452, loss 0.104711, acc 0.92
2016-09-06T22:39:40.738887: step 4453, loss 0.00179683, acc 1
2016-09-06T22:39:41.435244: step 4454, loss 0.0148862, acc 1
2016-09-06T22:39:42.126604: step 4455, loss 0.00844302, acc 1
2016-09-06T22:39:42.824491: step 4456, loss 0.000352077, acc 1
2016-09-06T22:39:43.486545: step 4457, loss 0.0524751, acc 0.98
2016-09-06T22:39:44.183529: step 4458, loss 0.0163989, acc 1
2016-09-06T22:39:44.851291: step 4459, loss 0.0813279, acc 0.98
2016-09-06T22:39:45.511281: step 4460, loss 0.0307734, acc 0.98
2016-09-06T22:39:46.222349: step 4461, loss 0.00441271, acc 1
2016-09-06T22:39:46.912953: step 4462, loss 0.0311299, acc 0.98
2016-09-06T22:39:47.592979: step 4463, loss 0.00716921, acc 1
2016-09-06T22:39:48.263720: step 4464, loss 0.0202032, acc 1
2016-09-06T22:39:48.942641: step 4465, loss 0.000287986, acc 1
2016-09-06T22:39:49.595506: step 4466, loss 0.0299145, acc 1
2016-09-06T22:39:50.283404: step 4467, loss 0.0692101, acc 0.96
2016-09-06T22:39:50.974252: step 4468, loss 0.000765907, acc 1
2016-09-06T22:39:51.660978: step 4469, loss 0.000443575, acc 1
2016-09-06T22:39:52.362492: step 4470, loss 0.0301772, acc 0.98
2016-09-06T22:39:53.053298: step 4471, loss 0.000367176, acc 1
2016-09-06T22:39:53.761968: step 4472, loss 0.00100649, acc 1
2016-09-06T22:39:54.436998: step 4473, loss 0.0668392, acc 0.98
2016-09-06T22:39:55.124261: step 4474, loss 0.000352584, acc 1
2016-09-06T22:39:55.799501: step 4475, loss 0.00175322, acc 1
2016-09-06T22:39:56.490034: step 4476, loss 0.0765742, acc 0.96
2016-09-06T22:39:57.194606: step 4477, loss 0.000991438, acc 1
2016-09-06T22:39:57.888899: step 4478, loss 0.00599569, acc 1
2016-09-06T22:39:58.602646: step 4479, loss 0.0363428, acc 1
2016-09-06T22:39:59.295251: step 4480, loss 0.0682432, acc 0.98
2016-09-06T22:39:59.994804: step 4481, loss 0.00515282, acc 1
2016-09-06T22:40:00.713141: step 4482, loss 0.00695466, acc 1
2016-09-06T22:40:01.395168: step 4483, loss 0.0110806, acc 1
2016-09-06T22:40:02.097628: step 4484, loss 0.000485874, acc 1
2016-09-06T22:40:02.782014: step 4485, loss 0.0353252, acc 0.98
2016-09-06T22:40:03.476933: step 4486, loss 0.0136462, acc 1
2016-09-06T22:40:04.157120: step 4487, loss 0.0473087, acc 0.96
2016-09-06T22:40:04.845777: step 4488, loss 0.00124917, acc 1
2016-09-06T22:40:05.544277: step 4489, loss 0.0293788, acc 0.98
2016-09-06T22:40:06.255619: step 4490, loss 0.0380824, acc 0.98
2016-09-06T22:40:06.945456: step 4491, loss 0.00798192, acc 1
2016-09-06T22:40:07.602874: step 4492, loss 0.0161329, acc 0.98
2016-09-06T22:40:08.324315: step 4493, loss 0.0449704, acc 0.96
2016-09-06T22:40:09.025807: step 4494, loss 0.0283039, acc 1
2016-09-06T22:40:09.731897: step 4495, loss 0.0223731, acc 1
2016-09-06T22:40:10.388700: step 4496, loss 0.0139055, acc 1
2016-09-06T22:40:11.067068: step 4497, loss 0.0364509, acc 0.98
2016-09-06T22:40:11.739056: step 4498, loss 0.0138687, acc 1
2016-09-06T22:40:12.417928: step 4499, loss 0.0250624, acc 1
2016-09-06T22:40:13.111579: step 4500, loss 0.0449354, acc 0.98

Evaluation:
2016-09-06T22:40:16.272764: step 4500, loss 2.69412, acc 0.742964

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-4500

2016-09-06T22:40:17.948240: step 4501, loss 0.0194887, acc 0.98
2016-09-06T22:40:18.635097: step 4502, loss 0.00562475, acc 1
2016-09-06T22:40:19.309554: step 4503, loss 0.0139808, acc 1
2016-09-06T22:40:19.993887: step 4504, loss 0.030892, acc 0.98
2016-09-06T22:40:20.668998: step 4505, loss 0.0659617, acc 0.98
2016-09-06T22:40:21.361944: step 4506, loss 0.024921, acc 0.98
2016-09-06T22:40:22.028696: step 4507, loss 0.00349063, acc 1
2016-09-06T22:40:22.745960: step 4508, loss 0.0417277, acc 0.96
2016-09-06T22:40:23.442424: step 4509, loss 0.00251626, acc 1
2016-09-06T22:40:24.130536: step 4510, loss 0.0557248, acc 0.98
2016-09-06T22:40:24.837323: step 4511, loss 0.0798295, acc 0.96
2016-09-06T22:40:25.527329: step 4512, loss 0.000348482, acc 1
2016-09-06T22:40:26.236742: step 4513, loss 0.00543804, acc 1
2016-09-06T22:40:26.907021: step 4514, loss 0.0129836, acc 1
2016-09-06T22:40:27.618227: step 4515, loss 0.0358848, acc 1
2016-09-06T22:40:28.301207: step 4516, loss 0.0145776, acc 1
2016-09-06T22:40:28.975878: step 4517, loss 0.0374304, acc 0.96
2016-09-06T22:40:29.661846: step 4518, loss 0.0213637, acc 0.98
2016-09-06T22:40:30.350169: step 4519, loss 0.0109378, acc 1
2016-09-06T22:40:31.049440: step 4520, loss 0.00488521, acc 1
2016-09-06T22:40:31.724385: step 4521, loss 0.0189333, acc 1
2016-09-06T22:40:32.414723: step 4522, loss 0.0151017, acc 1
2016-09-06T22:40:33.095017: step 4523, loss 0.0740974, acc 0.96
2016-09-06T22:40:33.787377: step 4524, loss 0.0211708, acc 0.98
2016-09-06T22:40:34.473907: step 4525, loss 0.0505095, acc 0.96
2016-09-06T22:40:35.153574: step 4526, loss 0.0613591, acc 0.96
2016-09-06T22:40:35.852505: step 4527, loss 0.0049771, acc 1
2016-09-06T22:40:36.510851: step 4528, loss 0.0878752, acc 0.96
2016-09-06T22:40:37.181035: step 4529, loss 0.0236686, acc 0.98
2016-09-06T22:40:37.882831: step 4530, loss 0.0739645, acc 0.98
2016-09-06T22:40:38.556280: step 4531, loss 0.0321874, acc 0.98
2016-09-06T22:40:39.250649: step 4532, loss 0.00849213, acc 1
2016-09-06T22:40:39.950521: step 4533, loss 0.00101655, acc 1
2016-09-06T22:40:40.658338: step 4534, loss 0.0380983, acc 0.98
2016-09-06T22:40:41.341054: step 4535, loss 0.0126909, acc 1
2016-09-06T22:40:42.032606: step 4536, loss 0.0284036, acc 0.98
2016-09-06T22:40:42.705655: step 4537, loss 0.0122836, acc 1
2016-09-06T22:40:43.402658: step 4538, loss 0.105639, acc 0.96
2016-09-06T22:40:44.096483: step 4539, loss 0.00726843, acc 1
2016-09-06T22:40:44.790131: step 4540, loss 0.0147952, acc 1
2016-09-06T22:40:45.486404: step 4541, loss 0.0012748, acc 1
2016-09-06T22:40:46.157976: step 4542, loss 0.0110097, acc 1
2016-09-06T22:40:46.853583: step 4543, loss 0.00610926, acc 1
2016-09-06T22:40:47.530782: step 4544, loss 0.176999, acc 0.96
2016-09-06T22:40:48.222190: step 4545, loss 0.0651833, acc 0.96
2016-09-06T22:40:48.898435: step 4546, loss 0.110249, acc 0.98
2016-09-06T22:40:49.577438: step 4547, loss 0.106546, acc 0.94
2016-09-06T22:40:50.269192: step 4548, loss 0.112864, acc 0.94
2016-09-06T22:40:50.933061: step 4549, loss 0.030934, acc 1
2016-09-06T22:40:51.611050: step 4550, loss 0.0357465, acc 0.98
2016-09-06T22:40:52.304203: step 4551, loss 0.00208381, acc 1
2016-09-06T22:40:52.992534: step 4552, loss 0.00928191, acc 1
2016-09-06T22:40:53.677753: step 4553, loss 0.009944, acc 1
2016-09-06T22:40:54.362982: step 4554, loss 0.000857819, acc 1
2016-09-06T22:40:55.056254: step 4555, loss 0.00300011, acc 1
2016-09-06T22:40:55.722932: step 4556, loss 0.0218936, acc 0.98
2016-09-06T22:40:56.429473: step 4557, loss 0.0486481, acc 0.96
2016-09-06T22:40:57.111856: step 4558, loss 0.0619499, acc 0.98
2016-09-06T22:40:57.823454: step 4559, loss 0.00162719, acc 1
2016-09-06T22:40:58.509367: step 4560, loss 0.0218983, acc 1
2016-09-06T22:40:59.183047: step 4561, loss 0.103717, acc 0.94
2016-09-06T22:40:59.872561: step 4562, loss 0.0119284, acc 1
2016-09-06T22:41:00.581847: step 4563, loss 0.0117879, acc 1
2016-09-06T22:41:01.261450: step 4564, loss 0.0333318, acc 0.98
2016-09-06T22:41:01.939099: step 4565, loss 0.0552833, acc 0.94
2016-09-06T22:41:02.627215: step 4566, loss 0.116497, acc 0.98
2016-09-06T22:41:03.302395: step 4567, loss 0.0460638, acc 0.98
2016-09-06T22:41:03.997749: step 4568, loss 0.0701958, acc 0.98
2016-09-06T22:41:04.699401: step 4569, loss 0.00848243, acc 1
2016-09-06T22:41:05.378189: step 4570, loss 0.0577438, acc 0.96
2016-09-06T22:41:06.054498: step 4571, loss 0.00959214, acc 1
2016-09-06T22:41:06.723401: step 4572, loss 0.00649975, acc 1
2016-09-06T22:41:07.404557: step 4573, loss 0.0178901, acc 0.98
2016-09-06T22:41:08.095673: step 4574, loss 0.0435773, acc 0.96
2016-09-06T22:41:08.798793: step 4575, loss 0.0275359, acc 0.98
2016-09-06T22:41:09.498547: step 4576, loss 0.0478444, acc 0.94
2016-09-06T22:41:10.185065: step 4577, loss 0.00109886, acc 1
2016-09-06T22:41:10.857312: step 4578, loss 0.012012, acc 1
2016-09-06T22:41:11.539411: step 4579, loss 0.00499823, acc 1
2016-09-06T22:41:12.223114: step 4580, loss 0.070825, acc 0.98
2016-09-06T22:41:12.909163: step 4581, loss 0.0101086, acc 1
2016-09-06T22:41:13.602918: step 4582, loss 0.0676544, acc 0.96
2016-09-06T22:41:14.316191: step 4583, loss 0.0281119, acc 1
2016-09-06T22:41:14.995571: step 4584, loss 0.122681, acc 0.98
2016-09-06T22:41:15.678152: step 4585, loss 0.0410645, acc 1
2016-09-06T22:41:16.364221: step 4586, loss 0.033939, acc 0.98
2016-09-06T22:41:17.055890: step 4587, loss 0.00318382, acc 1
2016-09-06T22:41:17.737465: step 4588, loss 0.0250986, acc 1
2016-09-06T22:41:18.431424: step 4589, loss 0.00812084, acc 1
2016-09-06T22:41:19.155494: step 4590, loss 0.0199288, acc 1
2016-09-06T22:41:19.830698: step 4591, loss 0.0213966, acc 0.98
2016-09-06T22:41:20.493291: step 4592, loss 0.0183608, acc 0.98
2016-09-06T22:41:21.152184: step 4593, loss 0.0909242, acc 0.96
2016-09-06T22:41:21.817191: step 4594, loss 0.0110185, acc 1
2016-09-06T22:41:22.504445: step 4595, loss 0.0245242, acc 0.98
2016-09-06T22:41:23.188740: step 4596, loss 0.0310143, acc 0.98
2016-09-06T22:41:23.880788: step 4597, loss 0.0836187, acc 0.98
2016-09-06T22:41:24.557869: step 4598, loss 0.0128124, acc 1
2016-09-06T22:41:25.263125: step 4599, loss 0.000598566, acc 1
2016-09-06T22:41:25.937582: step 4600, loss 0.0694273, acc 0.96

Evaluation:
2016-09-06T22:41:29.097062: step 4600, loss 2.38996, acc 0.746717

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-4600

2016-09-06T22:41:30.781856: step 4601, loss 0.0242825, acc 1
2016-09-06T22:41:31.479493: step 4602, loss 0.0335704, acc 0.98
2016-09-06T22:41:32.159644: step 4603, loss 0.0208529, acc 1
2016-09-06T22:41:32.839900: step 4604, loss 0.0394262, acc 0.98
2016-09-06T22:41:33.559497: step 4605, loss 0.0199434, acc 0.98
2016-09-06T22:41:34.231773: step 4606, loss 0.0390944, acc 0.98
2016-09-06T22:41:34.953090: step 4607, loss 0.0205692, acc 1
2016-09-06T22:41:35.617453: step 4608, loss 0.000102136, acc 1
2016-09-06T22:41:36.298131: step 4609, loss 0.0268681, acc 1
2016-09-06T22:41:36.987754: step 4610, loss 0.0105538, acc 1
2016-09-06T22:41:37.663617: step 4611, loss 0.026176, acc 1
2016-09-06T22:41:38.370516: step 4612, loss 0.00650296, acc 1
2016-09-06T22:41:39.044438: step 4613, loss 0.0859136, acc 0.98
2016-09-06T22:41:39.725078: step 4614, loss 0.241917, acc 0.96
2016-09-06T22:41:40.431674: step 4615, loss 0.00711948, acc 1
2016-09-06T22:41:41.120588: step 4616, loss 0.0428719, acc 0.98
2016-09-06T22:41:41.834472: step 4617, loss 0.0301799, acc 0.98
2016-09-06T22:41:42.533264: step 4618, loss 0.0312402, acc 0.98
2016-09-06T22:41:43.241986: step 4619, loss 0.0235121, acc 0.98
2016-09-06T22:41:43.940327: step 4620, loss 0.0489345, acc 0.98
2016-09-06T22:41:44.631612: step 4621, loss 0.281439, acc 0.92
2016-09-06T22:41:45.298528: step 4622, loss 0.0280234, acc 0.98
2016-09-06T22:41:45.978086: step 4623, loss 0.0274189, acc 0.98
2016-09-06T22:41:46.649964: step 4624, loss 0.00291586, acc 1
2016-09-06T22:41:47.303585: step 4625, loss 0.0276607, acc 0.98
2016-09-06T22:41:48.029827: step 4626, loss 0.0266679, acc 1
2016-09-06T22:41:48.716844: step 4627, loss 0.0741104, acc 0.94
2016-09-06T22:41:49.394965: step 4628, loss 0.00670445, acc 1
2016-09-06T22:41:50.079144: step 4629, loss 0.0404954, acc 0.98
2016-09-06T22:41:50.736823: step 4630, loss 0.0744164, acc 0.94
2016-09-06T22:41:51.444793: step 4631, loss 0.00624839, acc 1
2016-09-06T22:41:52.126448: step 4632, loss 0.0044633, acc 1
2016-09-06T22:41:52.826086: step 4633, loss 0.00952734, acc 1
2016-09-06T22:41:53.488476: step 4634, loss 0.00526769, acc 1
2016-09-06T22:41:54.169572: step 4635, loss 0.03217, acc 0.98
2016-09-06T22:41:54.884875: step 4636, loss 0.000838154, acc 1
2016-09-06T22:41:55.578381: step 4637, loss 0.045503, acc 0.98
2016-09-06T22:41:56.259526: step 4638, loss 0.024185, acc 0.98
2016-09-06T22:41:56.961022: step 4639, loss 0.0132801, acc 1
2016-09-06T22:41:57.670806: step 4640, loss 0.0621084, acc 0.96
2016-09-06T22:41:58.341479: step 4641, loss 0.0478685, acc 0.98
2016-09-06T22:41:59.005398: step 4642, loss 0.0456625, acc 0.96
2016-09-06T22:41:59.684894: step 4643, loss 0.0289277, acc 0.98
2016-09-06T22:42:00.420674: step 4644, loss 0.00951725, acc 1
2016-09-06T22:42:01.113457: step 4645, loss 0.00462257, acc 1
2016-09-06T22:42:01.785986: step 4646, loss 0.0491697, acc 0.98
2016-09-06T22:42:02.494614: step 4647, loss 0.0485184, acc 0.98
2016-09-06T22:42:03.177401: step 4648, loss 0.000569539, acc 1
2016-09-06T22:42:03.871591: step 4649, loss 0.0453905, acc 0.96
2016-09-06T22:42:04.555017: step 4650, loss 0.00251844, acc 1
2016-09-06T22:42:05.226385: step 4651, loss 0.0263208, acc 0.98
2016-09-06T22:42:05.921080: step 4652, loss 0.0239662, acc 0.98
2016-09-06T22:42:06.584298: step 4653, loss 0.051645, acc 0.98
2016-09-06T22:42:07.295341: step 4654, loss 0.0190675, acc 0.98
2016-09-06T22:42:07.981746: step 4655, loss 0.0355509, acc 0.98
2016-09-06T22:42:08.675050: step 4656, loss 0.0247193, acc 1
2016-09-06T22:42:09.372194: step 4657, loss 0.00129772, acc 1
2016-09-06T22:42:10.070622: step 4658, loss 0.0167183, acc 1
2016-09-06T22:42:10.760205: step 4659, loss 0.0100919, acc 1
2016-09-06T22:42:11.430758: step 4660, loss 0.0342207, acc 0.98
2016-09-06T22:42:12.125812: step 4661, loss 0.0251223, acc 0.98
2016-09-06T22:42:12.826935: step 4662, loss 0.0167674, acc 0.98
2016-09-06T22:42:13.517385: step 4663, loss 0.00457417, acc 1
2016-09-06T22:42:14.217710: step 4664, loss 0.0338989, acc 0.98
2016-09-06T22:42:14.909089: step 4665, loss 0.173418, acc 0.96
2016-09-06T22:42:15.622457: step 4666, loss 0.00809865, acc 1
2016-09-06T22:42:16.316486: step 4667, loss 0.0639763, acc 0.98
2016-09-06T22:42:16.990167: step 4668, loss 0.015441, acc 1
2016-09-06T22:42:17.674502: step 4669, loss 0.0552683, acc 0.96
2016-09-06T22:42:18.354322: step 4670, loss 0.0193734, acc 0.98
2016-09-06T22:42:19.029683: step 4671, loss 0.0286876, acc 0.98
2016-09-06T22:42:19.702641: step 4672, loss 0.0247763, acc 0.98
2016-09-06T22:42:20.402910: step 4673, loss 0.0116595, acc 1
2016-09-06T22:42:21.070148: step 4674, loss 0.0223254, acc 1
2016-09-06T22:42:21.785100: step 4675, loss 0.00125735, acc 1
2016-09-06T22:42:22.473468: step 4676, loss 0.0659603, acc 0.98
2016-09-06T22:42:23.164893: step 4677, loss 0.0916005, acc 0.96
2016-09-06T22:42:23.847581: step 4678, loss 0.0343004, acc 0.98
2016-09-06T22:42:24.538919: step 4679, loss 0.132283, acc 0.98
2016-09-06T22:42:25.256789: step 4680, loss 0.0367768, acc 0.96
2016-09-06T22:42:25.945232: step 4681, loss 0.0367875, acc 1
2016-09-06T22:42:26.621666: step 4682, loss 0.0618769, acc 0.94
2016-09-06T22:42:27.307516: step 4683, loss 0.00344927, acc 1
2016-09-06T22:42:27.990933: step 4684, loss 0.00238361, acc 1
2016-09-06T22:42:28.671869: step 4685, loss 0.00594821, acc 1
2016-09-06T22:42:29.361423: step 4686, loss 0.0221645, acc 1
2016-09-06T22:42:30.067122: step 4687, loss 0.0127144, acc 1
2016-09-06T22:42:30.745935: step 4688, loss 0.0260025, acc 0.98
2016-09-06T22:42:31.454299: step 4689, loss 0.0163235, acc 1
2016-09-06T22:42:32.150900: step 4690, loss 0.0383256, acc 0.98
2016-09-06T22:42:32.834506: step 4691, loss 0.0188756, acc 1
2016-09-06T22:42:33.534021: step 4692, loss 0.0284124, acc 0.98
2016-09-06T22:42:34.221981: step 4693, loss 0.00110553, acc 1
2016-09-06T22:42:34.943658: step 4694, loss 0.0189097, acc 1
2016-09-06T22:42:35.626394: step 4695, loss 0.0117304, acc 1
2016-09-06T22:42:36.312734: step 4696, loss 0.0215875, acc 0.98
2016-09-06T22:42:36.998896: step 4697, loss 0.121205, acc 0.96
2016-09-06T22:42:37.689224: step 4698, loss 0.0778224, acc 0.96
2016-09-06T22:42:38.369649: step 4699, loss 0.04152, acc 0.98
2016-09-06T22:42:39.024401: step 4700, loss 0.0103128, acc 1

Evaluation:
2016-09-06T22:42:42.201901: step 4700, loss 2.13335, acc 0.744841

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-4700

2016-09-06T22:42:44.001352: step 4701, loss 0.00199143, acc 1
2016-09-06T22:42:44.698709: step 4702, loss 0.043961, acc 0.98
2016-09-06T22:42:45.390249: step 4703, loss 0.00832081, acc 1
2016-09-06T22:42:46.062349: step 4704, loss 0.00584773, acc 1
2016-09-06T22:42:46.755337: step 4705, loss 0.0143333, acc 1
2016-09-06T22:42:47.430676: step 4706, loss 0.0192243, acc 0.98
2016-09-06T22:42:48.153855: step 4707, loss 0.0150666, acc 0.98
2016-09-06T22:42:48.812208: step 4708, loss 0.0146283, acc 1
2016-09-06T22:42:49.492807: step 4709, loss 0.00799846, acc 1
2016-09-06T22:42:50.185317: step 4710, loss 0.0097295, acc 1
2016-09-06T22:42:50.872137: step 4711, loss 0.0180279, acc 1
2016-09-06T22:42:51.548954: step 4712, loss 0.0198651, acc 1
2016-09-06T22:42:52.223234: step 4713, loss 0.00202353, acc 1
2016-09-06T22:42:52.933915: step 4714, loss 0.0338052, acc 0.98
2016-09-06T22:42:53.602465: step 4715, loss 0.0417233, acc 0.98
2016-09-06T22:42:54.284284: step 4716, loss 0.00711353, acc 1
2016-09-06T22:42:54.975516: step 4717, loss 0.0299003, acc 0.98
2016-09-06T22:42:55.665358: step 4718, loss 0.0420661, acc 0.98
2016-09-06T22:42:56.353734: step 4719, loss 0.00062544, acc 1
2016-09-06T22:42:57.050082: step 4720, loss 0.0353111, acc 0.98
2016-09-06T22:42:57.746177: step 4721, loss 0.0139173, acc 1
2016-09-06T22:42:58.413956: step 4722, loss 0.0298179, acc 0.98
2016-09-06T22:42:59.111313: step 4723, loss 0.0312765, acc 0.98
2016-09-06T22:42:59.802832: step 4724, loss 0.0139286, acc 1
2016-09-06T22:43:00.507424: step 4725, loss 0.0213545, acc 0.98
2016-09-06T22:43:01.207499: step 4726, loss 0.0200424, acc 0.98
2016-09-06T22:43:01.876228: step 4727, loss 0.0187807, acc 1
2016-09-06T22:43:02.566501: step 4728, loss 0.00663352, acc 1
2016-09-06T22:43:03.240291: step 4729, loss 0.0180707, acc 1
2016-09-06T22:43:03.949096: step 4730, loss 0.0124819, acc 1
2016-09-06T22:43:04.636667: step 4731, loss 0.000234573, acc 1
2016-09-06T22:43:05.322438: step 4732, loss 0.0206014, acc 1
2016-09-06T22:43:06.007899: step 4733, loss 0.00266622, acc 1
2016-09-06T22:43:06.661108: step 4734, loss 0.118528, acc 0.98
2016-09-06T22:43:07.353244: step 4735, loss 0.0113151, acc 1
2016-09-06T22:43:08.040749: step 4736, loss 0.0144075, acc 1
2016-09-06T22:43:08.715585: step 4737, loss 0.000296318, acc 1
2016-09-06T22:43:09.392820: step 4738, loss 0.107062, acc 0.94
2016-09-06T22:43:10.090316: step 4739, loss 0.0634071, acc 0.96
2016-09-06T22:43:10.789269: step 4740, loss 0.0145258, acc 0.98
2016-09-06T22:43:11.468270: step 4741, loss 0.0332559, acc 1
2016-09-06T22:43:12.164017: step 4742, loss 0.0318778, acc 0.98
2016-09-06T22:43:12.838214: step 4743, loss 0.000768873, acc 1
2016-09-06T22:43:13.527626: step 4744, loss 0.0363254, acc 0.98
2016-09-06T22:43:14.238482: step 4745, loss 0.00159406, acc 1
2016-09-06T22:43:14.942827: step 4746, loss 0.0111915, acc 1
2016-09-06T22:43:15.638818: step 4747, loss 0.0210372, acc 1
2016-09-06T22:43:16.333202: step 4748, loss 0.0115101, acc 1
2016-09-06T22:43:17.044291: step 4749, loss 0.019575, acc 0.98
2016-09-06T22:43:17.737278: step 4750, loss 0.00412907, acc 1
2016-09-06T22:43:18.422741: step 4751, loss 0.00117922, acc 1
2016-09-06T22:43:19.109330: step 4752, loss 0.0231329, acc 0.98
2016-09-06T22:43:19.783663: step 4753, loss 0.0203257, acc 1
2016-09-06T22:43:20.495482: step 4754, loss 0.0155038, acc 1
2016-09-06T22:43:21.166059: step 4755, loss 0.00167688, acc 1
2016-09-06T22:43:21.872115: step 4756, loss 0.0246665, acc 0.98
2016-09-06T22:43:22.576433: step 4757, loss 0.0435034, acc 1
2016-09-06T22:43:23.269450: step 4758, loss 0.0139113, acc 1
2016-09-06T22:43:23.947925: step 4759, loss 0.022956, acc 0.98
2016-09-06T22:43:24.621840: step 4760, loss 0.0412525, acc 0.98
2016-09-06T22:43:25.314497: step 4761, loss 0.0012402, acc 1
2016-09-06T22:43:25.981444: step 4762, loss 0.0170677, acc 1
2016-09-06T22:43:26.690480: step 4763, loss 0.0245658, acc 0.98
2016-09-06T22:43:27.370591: step 4764, loss 0.0168803, acc 0.98
2016-09-06T22:43:28.057003: step 4765, loss 4.97628e-05, acc 1
2016-09-06T22:43:28.763380: step 4766, loss 0.0400222, acc 0.98
2016-09-06T22:43:29.484739: step 4767, loss 0.00382849, acc 1
2016-09-06T22:43:30.210426: step 4768, loss 0.019187, acc 0.98
2016-09-06T22:43:30.895733: step 4769, loss 0.00201989, acc 1
2016-09-06T22:43:31.584868: step 4770, loss 0.00838863, acc 1
2016-09-06T22:43:32.257055: step 4771, loss 0.0140713, acc 1
2016-09-06T22:43:32.950555: step 4772, loss 0.000912501, acc 1
2016-09-06T22:43:33.634422: step 4773, loss 0.0112889, acc 1
2016-09-06T22:43:34.303434: step 4774, loss 0.0300423, acc 0.98
2016-09-06T22:43:34.993425: step 4775, loss 0.00472549, acc 1
2016-09-06T22:43:35.684630: step 4776, loss 0.0407306, acc 0.98
2016-09-06T22:43:36.366694: step 4777, loss 0.0264932, acc 0.98
2016-09-06T22:43:37.068584: step 4778, loss 0.0139714, acc 1
2016-09-06T22:43:37.774935: step 4779, loss 0.00565917, acc 1
2016-09-06T22:43:38.488624: step 4780, loss 0.0133499, acc 1
2016-09-06T22:43:39.182956: step 4781, loss 0.0147617, acc 1
2016-09-06T22:43:39.887708: step 4782, loss 0.0837566, acc 0.94
2016-09-06T22:43:40.543379: step 4783, loss 0.0991568, acc 0.98
2016-09-06T22:43:41.207469: step 4784, loss 0.0603868, acc 0.98
2016-09-06T22:43:41.884177: step 4785, loss 0.0449969, acc 0.98
2016-09-06T22:43:42.566005: step 4786, loss 0.00774719, acc 1
2016-09-06T22:43:43.268053: step 4787, loss 0.0249596, acc 0.98
2016-09-06T22:43:43.943657: step 4788, loss 0.00988036, acc 1
2016-09-06T22:43:44.679033: step 4789, loss 0.0156789, acc 0.98
2016-09-06T22:43:45.375425: step 4790, loss 9.44408e-05, acc 1
2016-09-06T22:43:46.066443: step 4791, loss 0.00500969, acc 1
2016-09-06T22:43:46.741099: step 4792, loss 0.0647292, acc 0.98
2016-09-06T22:43:47.434152: step 4793, loss 0.00495692, acc 1
2016-09-06T22:43:48.115104: step 4794, loss 0.104134, acc 0.96
2016-09-06T22:43:48.804248: step 4795, loss 0.0443357, acc 0.98
2016-09-06T22:43:49.499867: step 4796, loss 0.0213482, acc 0.98
2016-09-06T22:43:50.168475: step 4797, loss 0.025612, acc 1
2016-09-06T22:43:50.848830: step 4798, loss 0.0219292, acc 1
2016-09-06T22:43:51.542037: step 4799, loss 0.00421189, acc 1
2016-09-06T22:43:52.180235: step 4800, loss 0.00147825, acc 1

Evaluation:
2016-09-06T22:43:55.336697: step 4800, loss 2.26663, acc 0.750469

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-4800

2016-09-06T22:43:56.985766: step 4801, loss 0.0384224, acc 0.96
2016-09-06T22:43:57.685545: step 4802, loss 0.0187671, acc 1
2016-09-06T22:43:58.378980: step 4803, loss 0.0323327, acc 1
2016-09-06T22:43:59.080728: step 4804, loss 0.0929193, acc 0.98
2016-09-06T22:43:59.762018: step 4805, loss 0.00350984, acc 1
2016-09-06T22:44:00.491628: step 4806, loss 0.0404639, acc 0.98
2016-09-06T22:44:01.179789: step 4807, loss 0.0392833, acc 0.98
2016-09-06T22:44:01.892612: step 4808, loss 0.00839341, acc 1
2016-09-06T22:44:02.588373: step 4809, loss 0.0104186, acc 1
2016-09-06T22:44:03.248584: step 4810, loss 0.00137991, acc 1
2016-09-06T22:44:03.966915: step 4811, loss 0.0192644, acc 0.98
2016-09-06T22:44:04.664599: step 4812, loss 0.0330046, acc 0.98
2016-09-06T22:44:05.367014: step 4813, loss 0.00240906, acc 1
2016-09-06T22:44:06.044081: step 4814, loss 0.020731, acc 1
2016-09-06T22:44:06.718684: step 4815, loss 3.90945e-05, acc 1
2016-09-06T22:44:07.439848: step 4816, loss 0.0123429, acc 1
2016-09-06T22:44:08.109162: step 4817, loss 0.0186582, acc 0.98
2016-09-06T22:44:08.810028: step 4818, loss 0.017222, acc 0.98
2016-09-06T22:44:09.515351: step 4819, loss 0.0130933, acc 1
2016-09-06T22:44:10.218173: step 4820, loss 0.000175867, acc 1
2016-09-06T22:44:10.913484: step 4821, loss 0.0496822, acc 0.98
2016-09-06T22:44:11.590087: step 4822, loss 0.0107347, acc 1
2016-09-06T22:44:12.304029: step 4823, loss 0.00908648, acc 1
2016-09-06T22:44:12.989268: step 4824, loss 0.0213991, acc 0.98
2016-09-06T22:44:13.674018: step 4825, loss 0.015928, acc 0.98
2016-09-06T22:44:14.341737: step 4826, loss 0.00019621, acc 1
2016-09-06T22:44:15.049572: step 4827, loss 0.0965402, acc 0.96
2016-09-06T22:44:15.734066: step 4828, loss 0.10542, acc 0.98
2016-09-06T22:44:16.409389: step 4829, loss 0.0282133, acc 0.98
2016-09-06T22:44:17.150166: step 4830, loss 0.00181929, acc 1
2016-09-06T22:44:17.842529: step 4831, loss 0.00435667, acc 1
2016-09-06T22:44:18.520676: step 4832, loss 0.0396493, acc 0.98
2016-09-06T22:44:19.217523: step 4833, loss 0.013054, acc 1
2016-09-06T22:44:19.924022: step 4834, loss 0.0369152, acc 0.98
2016-09-06T22:44:20.609544: step 4835, loss 0.0118478, acc 1
2016-09-06T22:44:21.279371: step 4836, loss 0.137735, acc 0.94
2016-09-06T22:44:21.987567: step 4837, loss 0.000882867, acc 1
2016-09-06T22:44:22.677425: step 4838, loss 0.0380226, acc 0.98
2016-09-06T22:44:23.352575: step 4839, loss 0.000464027, acc 1
2016-09-06T22:44:24.036446: step 4840, loss 0.00398496, acc 1
2016-09-06T22:44:24.723222: step 4841, loss 0.0182997, acc 1
2016-09-06T22:44:25.404489: step 4842, loss 0.00884281, acc 1
2016-09-06T22:44:26.068743: step 4843, loss 0.0126708, acc 1
2016-09-06T22:44:26.782832: step 4844, loss 0.0541613, acc 0.96
2016-09-06T22:44:27.490087: step 4845, loss 0.024888, acc 0.98
2016-09-06T22:44:28.191903: step 4846, loss 0.0233311, acc 1
2016-09-06T22:44:28.912355: step 4847, loss 0.00127104, acc 1
2016-09-06T22:44:29.607775: step 4848, loss 0.0389268, acc 0.98
2016-09-06T22:44:30.313274: step 4849, loss 0.0159497, acc 1
2016-09-06T22:44:30.983174: step 4850, loss 0.0406872, acc 0.98
2016-09-06T22:44:31.678775: step 4851, loss 0.021686, acc 0.98
2016-09-06T22:44:32.389494: step 4852, loss 0.0571487, acc 0.96
2016-09-06T22:44:33.080941: step 4853, loss 0.00099371, acc 1
2016-09-06T22:44:33.757009: step 4854, loss 0.0179321, acc 1
2016-09-06T22:44:34.435407: step 4855, loss 0.0293961, acc 0.98
2016-09-06T22:44:35.149403: step 4856, loss 0.0572519, acc 0.96
2016-09-06T22:44:35.816250: step 4857, loss 0.00523166, acc 1
2016-09-06T22:44:36.493673: step 4858, loss 7.18074e-05, acc 1
2016-09-06T22:44:37.180535: step 4859, loss 0.00272199, acc 1
2016-09-06T22:44:37.842884: step 4860, loss 0.00518079, acc 1
2016-09-06T22:44:38.539451: step 4861, loss 0.132727, acc 0.96
2016-09-06T22:44:39.228576: step 4862, loss 0.0228733, acc 0.98
2016-09-06T22:44:39.931683: step 4863, loss 0.0515323, acc 0.96
2016-09-06T22:44:40.612776: step 4864, loss 0.015412, acc 0.98
2016-09-06T22:44:41.285543: step 4865, loss 0.0610718, acc 0.98
2016-09-06T22:44:41.977657: step 4866, loss 0.0286414, acc 0.98
2016-09-06T22:44:42.674233: step 4867, loss 0.0501513, acc 0.98
2016-09-06T22:44:43.358424: step 4868, loss 0.00172445, acc 1
2016-09-06T22:44:44.059185: step 4869, loss 0.0216161, acc 1
2016-09-06T22:44:44.774547: step 4870, loss 0.00761787, acc 1
2016-09-06T22:44:45.455142: step 4871, loss 0.0911511, acc 0.96
2016-09-06T22:44:46.137891: step 4872, loss 0.0364346, acc 1
2016-09-06T22:44:46.820577: step 4873, loss 0.00454651, acc 1
2016-09-06T22:44:47.531124: step 4874, loss 0.00869871, acc 1
2016-09-06T22:44:48.237649: step 4875, loss 0.0486586, acc 0.96
2016-09-06T22:44:48.894452: step 4876, loss 0.0251054, acc 0.98
2016-09-06T22:44:49.595944: step 4877, loss 0.0336744, acc 1
2016-09-06T22:44:50.274112: step 4878, loss 0.0440726, acc 0.98
2016-09-06T22:44:50.963165: step 4879, loss 0.0557178, acc 0.98
2016-09-06T22:44:51.639491: step 4880, loss 0.0211796, acc 0.98
2016-09-06T22:44:52.328954: step 4881, loss 0.0250918, acc 1
2016-09-06T22:44:53.035949: step 4882, loss 0.0337803, acc 0.98
2016-09-06T22:44:53.734006: step 4883, loss 0.00303831, acc 1
2016-09-06T22:44:54.435821: step 4884, loss 0.0237025, acc 1
2016-09-06T22:44:55.124496: step 4885, loss 0.108613, acc 0.92
2016-09-06T22:44:55.800391: step 4886, loss 0.000598496, acc 1
2016-09-06T22:44:56.481588: step 4887, loss 0.0110495, acc 1
2016-09-06T22:44:57.172108: step 4888, loss 0.0485035, acc 0.96
2016-09-06T22:44:57.894724: step 4889, loss 0.0517651, acc 0.98
2016-09-06T22:44:58.576999: step 4890, loss 0.014201, acc 1
2016-09-06T22:44:59.297043: step 4891, loss 0.000103194, acc 1
2016-09-06T22:44:59.993438: step 4892, loss 0.0413425, acc 1
2016-09-06T22:45:00.717807: step 4893, loss 0.00915024, acc 1
2016-09-06T22:45:01.411233: step 4894, loss 0.00359907, acc 1
2016-09-06T22:45:02.107326: step 4895, loss 0.0453563, acc 1
2016-09-06T22:45:02.816328: step 4896, loss 0.0243166, acc 0.98
2016-09-06T22:45:03.482918: step 4897, loss 0.0527631, acc 0.96
2016-09-06T22:45:04.162536: step 4898, loss 0.0151084, acc 1
2016-09-06T22:45:04.841673: step 4899, loss 0.022722, acc 0.98
2016-09-06T22:45:05.513791: step 4900, loss 0.0464603, acc 0.96

Evaluation:
2016-09-06T22:45:08.639346: step 4900, loss 2.09642, acc 0.73546

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-4900

2016-09-06T22:45:10.289489: step 4901, loss 0.0562246, acc 0.98
2016-09-06T22:45:10.971513: step 4902, loss 0.000231333, acc 1
2016-09-06T22:45:11.640545: step 4903, loss 0.0330042, acc 0.96
2016-09-06T22:45:12.329985: step 4904, loss 0.000336807, acc 1
2016-09-06T22:45:12.992357: step 4905, loss 0.0196686, acc 1
2016-09-06T22:45:13.690361: step 4906, loss 0.126521, acc 0.98
2016-09-06T22:45:14.366812: step 4907, loss 0.0165304, acc 1
2016-09-06T22:45:15.039602: step 4908, loss 0.0609548, acc 0.98
2016-09-06T22:45:15.747091: step 4909, loss 0.0236465, acc 0.98
2016-09-06T22:45:16.428169: step 4910, loss 0.0477506, acc 0.96
2016-09-06T22:45:17.109957: step 4911, loss 0.0064291, acc 1
2016-09-06T22:45:17.785827: step 4912, loss 0.027463, acc 0.98
2016-09-06T22:45:18.500595: step 4913, loss 0.0289971, acc 0.98
2016-09-06T22:45:19.161853: step 4914, loss 0.0402104, acc 0.98
2016-09-06T22:45:19.831905: step 4915, loss 0.022084, acc 1
2016-09-06T22:45:20.520817: step 4916, loss 0.0328724, acc 0.98
2016-09-06T22:45:21.184727: step 4917, loss 0.0114991, acc 1
2016-09-06T22:45:21.869944: step 4918, loss 0.0242961, acc 1
2016-09-06T22:45:22.556409: step 4919, loss 0.00961084, acc 1
2016-09-06T22:45:23.263590: step 4920, loss 0.00450425, acc 1
2016-09-06T22:45:23.951629: step 4921, loss 0.0600952, acc 0.98
2016-09-06T22:45:24.656979: step 4922, loss 0.0144886, acc 1
2016-09-06T22:45:25.342404: step 4923, loss 0.0238111, acc 0.98
2016-09-06T22:45:26.039908: step 4924, loss 0.0245525, acc 1
2016-09-06T22:45:26.736151: step 4925, loss 0.0217538, acc 0.98
2016-09-06T22:45:27.410672: step 4926, loss 0.0154696, acc 1
2016-09-06T22:45:28.130986: step 4927, loss 0.0358155, acc 0.98
2016-09-06T22:45:28.821433: step 4928, loss 0.0300031, acc 0.98
2016-09-06T22:45:29.511409: step 4929, loss 0.0926372, acc 0.96
2016-09-06T22:45:30.204981: step 4930, loss 0.0213659, acc 1
2016-09-06T22:45:30.889184: step 4931, loss 0.0379883, acc 1
2016-09-06T22:45:31.609412: step 4932, loss 0.0310189, acc 0.98
2016-09-06T22:45:32.271965: step 4933, loss 0.00467428, acc 1
2016-09-06T22:45:32.979218: step 4934, loss 0.00381873, acc 1
2016-09-06T22:45:33.665519: step 4935, loss 7.0588e-05, acc 1
2016-09-06T22:45:34.366476: step 4936, loss 0.00484198, acc 1
2016-09-06T22:45:35.092312: step 4937, loss 0.00080719, acc 1
2016-09-06T22:45:35.804242: step 4938, loss 0.00918142, acc 1
2016-09-06T22:45:36.512447: step 4939, loss 0.00225997, acc 1
2016-09-06T22:45:37.196357: step 4940, loss 0.0217646, acc 0.98
2016-09-06T22:45:37.869956: step 4941, loss 0.0159014, acc 1
2016-09-06T22:45:38.568671: step 4942, loss 0.00195703, acc 1
2016-09-06T22:45:39.248514: step 4943, loss 0.00392613, acc 1
2016-09-06T22:45:39.920110: step 4944, loss 0.0182182, acc 1
2016-09-06T22:45:40.601590: step 4945, loss 0.0126364, acc 1
2016-09-06T22:45:41.291525: step 4946, loss 0.00729175, acc 1
2016-09-06T22:45:41.953416: step 4947, loss 0.00609949, acc 1
2016-09-06T22:45:42.630434: step 4948, loss 0.0363279, acc 1
2016-09-06T22:45:43.308927: step 4949, loss 0.0199753, acc 1
2016-09-06T22:45:43.992475: step 4950, loss 0.00203095, acc 1
2016-09-06T22:45:44.683655: step 4951, loss 0.0633945, acc 0.98
2016-09-06T22:45:45.379328: step 4952, loss 0.0322517, acc 0.98
2016-09-06T22:45:46.087108: step 4953, loss 0.0756315, acc 0.98
2016-09-06T22:45:46.786269: step 4954, loss 0.0232144, acc 0.98
2016-09-06T22:45:47.450852: step 4955, loss 0.000478715, acc 1
2016-09-06T22:45:48.143737: step 4956, loss 0.000176296, acc 1
2016-09-06T22:45:48.824102: step 4957, loss 0.00458039, acc 1
2016-09-06T22:45:49.538998: step 4958, loss 0.161159, acc 0.94
2016-09-06T22:45:50.222574: step 4959, loss 0.0170838, acc 1
2016-09-06T22:45:50.945244: step 4960, loss 0.080362, acc 0.96
2016-09-06T22:45:51.612759: step 4961, loss 0.0717875, acc 0.98
2016-09-06T22:45:52.297472: step 4962, loss 0.000536477, acc 1
2016-09-06T22:45:52.975166: step 4963, loss 0.00791262, acc 1
2016-09-06T22:45:53.656855: step 4964, loss 0.00528119, acc 1
2016-09-06T22:45:54.363725: step 4965, loss 0.12674, acc 0.96
2016-09-06T22:45:55.061515: step 4966, loss 0.0455715, acc 0.98
2016-09-06T22:45:55.781296: step 4967, loss 0.0741572, acc 0.98
2016-09-06T22:45:56.474901: step 4968, loss 0.0286818, acc 0.98
2016-09-06T22:45:57.158917: step 4969, loss 0.0128675, acc 1
2016-09-06T22:45:57.857957: step 4970, loss 0.0610876, acc 0.96
2016-09-06T22:45:58.537946: step 4971, loss 0.000330823, acc 1
2016-09-06T22:45:59.233221: step 4972, loss 0.0145368, acc 1
2016-09-06T22:45:59.892408: step 4973, loss 0.00128032, acc 1
2016-09-06T22:46:00.625541: step 4974, loss 0.161427, acc 0.98
2016-09-06T22:46:01.327603: step 4975, loss 0.0244048, acc 0.98
2016-09-06T22:46:02.014555: step 4976, loss 0.0383581, acc 0.98
2016-09-06T22:46:02.688278: step 4977, loss 0.015724, acc 1
2016-09-06T22:46:03.371639: step 4978, loss 0.0472516, acc 0.98
2016-09-06T22:46:04.048719: step 4979, loss 0.00930661, acc 1
2016-09-06T22:46:04.721984: step 4980, loss 0.0795059, acc 0.96
2016-09-06T22:46:05.429571: step 4981, loss 0.0608316, acc 0.98
2016-09-06T22:46:06.139086: step 4982, loss 0.0605106, acc 0.98
2016-09-06T22:46:06.853658: step 4983, loss 0.0139584, acc 1
2016-09-06T22:46:07.538925: step 4984, loss 0.107911, acc 0.98
2016-09-06T22:46:08.246008: step 4985, loss 0.0121994, acc 1
2016-09-06T22:46:08.964736: step 4986, loss 0.0137607, acc 1
2016-09-06T22:46:09.661865: step 4987, loss 0.0325869, acc 1
2016-09-06T22:46:10.339068: step 4988, loss 0.0193009, acc 0.98
2016-09-06T22:46:11.022221: step 4989, loss 0.0148391, acc 1
2016-09-06T22:46:11.725126: step 4990, loss 0.00818856, acc 1
2016-09-06T22:46:12.413021: step 4991, loss 0.0250922, acc 0.98
2016-09-06T22:46:13.042460: step 4992, loss 0.00650664, acc 1
2016-09-06T22:46:13.741553: step 4993, loss 0.0456959, acc 0.98
2016-09-06T22:46:14.413365: step 4994, loss 0.0421639, acc 0.98
2016-09-06T22:46:15.126007: step 4995, loss 0.00398237, acc 1
2016-09-06T22:46:15.820569: step 4996, loss 0.0276932, acc 0.98
2016-09-06T22:46:16.509098: step 4997, loss 0.00774325, acc 1
2016-09-06T22:46:17.192045: step 4998, loss 0.0504869, acc 0.96
2016-09-06T22:46:17.881206: step 4999, loss 0.000183383, acc 1
2016-09-06T22:46:18.592824: step 5000, loss 0.00746718, acc 1

Evaluation:
2016-09-06T22:46:21.708154: step 5000, loss 1.67149, acc 0.727955

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-5000

2016-09-06T22:46:23.426603: step 5001, loss 0.0107644, acc 1
2016-09-06T22:46:24.132324: step 5002, loss 0.0667298, acc 0.96
2016-09-06T22:46:24.815986: step 5003, loss 0.0249011, acc 0.98
2016-09-06T22:46:25.501676: step 5004, loss 0.0322238, acc 0.98
2016-09-06T22:46:26.206468: step 5005, loss 0.0386969, acc 0.98
2016-09-06T22:46:26.897354: step 5006, loss 0.0221175, acc 1
2016-09-06T22:46:27.550811: step 5007, loss 0.0192751, acc 0.98
2016-09-06T22:46:28.265837: step 5008, loss 0.0347163, acc 0.98
2016-09-06T22:46:28.946961: step 5009, loss 0.00627481, acc 1
2016-09-06T22:46:29.634780: step 5010, loss 0.0107154, acc 1
2016-09-06T22:46:30.319847: step 5011, loss 0.00757421, acc 1
2016-09-06T22:46:31.015709: step 5012, loss 0.0204022, acc 0.98
2016-09-06T22:46:31.705662: step 5013, loss 0.0325469, acc 1
2016-09-06T22:46:32.382640: step 5014, loss 0.11233, acc 0.92
2016-09-06T22:46:33.078302: step 5015, loss 0.00883571, acc 1
2016-09-06T22:46:33.748126: step 5016, loss 0.0185653, acc 0.98
2016-09-06T22:46:34.444872: step 5017, loss 0.00623365, acc 1
2016-09-06T22:46:35.128410: step 5018, loss 0.0179196, acc 1
2016-09-06T22:46:35.794374: step 5019, loss 0.00588776, acc 1
2016-09-06T22:46:36.507525: step 5020, loss 0.00780751, acc 1
2016-09-06T22:46:37.171268: step 5021, loss 0.0962834, acc 0.98
2016-09-06T22:46:37.881454: step 5022, loss 0.0141793, acc 1
2016-09-06T22:46:38.565181: step 5023, loss 0.0676019, acc 0.98
2016-09-06T22:46:39.240431: step 5024, loss 0.0923319, acc 0.98
2016-09-06T22:46:39.909833: step 5025, loss 6.04015e-05, acc 1
2016-09-06T22:46:40.579145: step 5026, loss 0.0342431, acc 1
2016-09-06T22:46:41.265607: step 5027, loss 0.00118743, acc 1
2016-09-06T22:46:41.944656: step 5028, loss 0.00527297, acc 1
2016-09-06T22:46:42.670278: step 5029, loss 0.0241058, acc 0.98
2016-09-06T22:46:43.360339: step 5030, loss 0.0269935, acc 0.98
2016-09-06T22:46:44.055669: step 5031, loss 0.000368814, acc 1
2016-09-06T22:46:44.726407: step 5032, loss 0.000812726, acc 1
2016-09-06T22:46:45.433437: step 5033, loss 0.0187888, acc 1
2016-09-06T22:46:46.122123: step 5034, loss 0.00267169, acc 1
2016-09-06T22:46:46.796749: step 5035, loss 0.0133079, acc 1
2016-09-06T22:46:47.517944: step 5036, loss 0.0323531, acc 0.98
2016-09-06T22:46:48.230480: step 5037, loss 0.00230757, acc 1
2016-09-06T22:46:48.937398: step 5038, loss 0.0095929, acc 1
2016-09-06T22:46:49.620295: step 5039, loss 0.00153432, acc 1
2016-09-06T22:46:50.301847: step 5040, loss 0.00882339, acc 1
2016-09-06T22:46:50.979703: step 5041, loss 0.000508591, acc 1
2016-09-06T22:46:51.658917: step 5042, loss 0.0362941, acc 0.98
2016-09-06T22:46:52.365663: step 5043, loss 0.00907652, acc 1
2016-09-06T22:46:53.046187: step 5044, loss 0.0466659, acc 0.98
2016-09-06T22:46:53.727704: step 5045, loss 0.00030007, acc 1
2016-09-06T22:46:54.428023: step 5046, loss 0.0228092, acc 0.98
2016-09-06T22:46:55.096768: step 5047, loss 0.0349774, acc 0.96
2016-09-06T22:46:55.807406: step 5048, loss 0.0178053, acc 1
2016-09-06T22:46:56.477555: step 5049, loss 0.00895072, acc 1
2016-09-06T22:46:57.178982: step 5050, loss 0.0102164, acc 1
2016-09-06T22:46:57.885566: step 5051, loss 0.00631991, acc 1
2016-09-06T22:46:58.578812: step 5052, loss 0.00836612, acc 1
2016-09-06T22:46:59.258561: step 5053, loss 0.0367378, acc 0.98
2016-09-06T22:46:59.933633: step 5054, loss 0.000262164, acc 1
2016-09-06T22:47:00.653421: step 5055, loss 0.0115306, acc 1
2016-09-06T22:47:01.326106: step 5056, loss 0.0199237, acc 1
2016-09-06T22:47:02.022935: step 5057, loss 0.00155444, acc 1
2016-09-06T22:47:02.719948: step 5058, loss 0.033864, acc 0.98
2016-09-06T22:47:03.395381: step 5059, loss 0.0284508, acc 0.98
2016-09-06T22:47:04.091420: step 5060, loss 0.0198796, acc 1
2016-09-06T22:47:04.787402: step 5061, loss 0.119071, acc 0.96
2016-09-06T22:47:05.500478: step 5062, loss 0.000469094, acc 1
2016-09-06T22:47:06.170885: step 5063, loss 0.0110276, acc 1
2016-09-06T22:47:06.854548: step 5064, loss 0.0180484, acc 0.98
2016-09-06T22:47:07.544029: step 5065, loss 0.0185897, acc 1
2016-09-06T22:47:08.220511: step 5066, loss 0.00235426, acc 1
2016-09-06T22:47:08.908357: step 5067, loss 0.0457149, acc 0.98
2016-09-06T22:47:09.588551: step 5068, loss 0.00996396, acc 1
2016-09-06T22:47:10.271069: step 5069, loss 0.00264165, acc 1
2016-09-06T22:47:10.957407: step 5070, loss 0.0846235, acc 0.96
2016-09-06T22:47:11.632327: step 5071, loss 0.0131065, acc 1
2016-09-06T22:47:12.332572: step 5072, loss 0.0436837, acc 0.98
2016-09-06T22:47:13.036493: step 5073, loss 0.00731906, acc 1
2016-09-06T22:47:13.737665: step 5074, loss 0.0200401, acc 0.98
2016-09-06T22:47:14.414455: step 5075, loss 0.0183712, acc 0.98
2016-09-06T22:47:15.118967: step 5076, loss 0.0305319, acc 0.98
2016-09-06T22:47:15.805584: step 5077, loss 0.00175868, acc 1
2016-09-06T22:47:16.487346: step 5078, loss 0.0469465, acc 0.98
2016-09-06T22:47:17.170974: step 5079, loss 0.0384467, acc 0.98
2016-09-06T22:47:17.853404: step 5080, loss 0.0263426, acc 1
2016-09-06T22:47:18.544777: step 5081, loss 0.0608625, acc 0.98
2016-09-06T22:47:19.203634: step 5082, loss 0.0642896, acc 0.96
2016-09-06T22:47:19.924371: step 5083, loss 0.00148714, acc 1
2016-09-06T22:47:20.584094: step 5084, loss 0.0262179, acc 1
2016-09-06T22:47:21.262453: step 5085, loss 0.000919961, acc 1
2016-09-06T22:47:21.980326: step 5086, loss 0.0317318, acc 0.96
2016-09-06T22:47:22.670109: step 5087, loss 0.0219592, acc 0.98
2016-09-06T22:47:23.358829: step 5088, loss 0.0376291, acc 0.98
2016-09-06T22:47:24.055374: step 5089, loss 0.101415, acc 0.98
2016-09-06T22:47:24.782900: step 5090, loss 0.00465806, acc 1
2016-09-06T22:47:25.454485: step 5091, loss 0.0633169, acc 0.98
2016-09-06T22:47:26.143392: step 5092, loss 0.00431461, acc 1
2016-09-06T22:47:26.841719: step 5093, loss 0.0161232, acc 1
2016-09-06T22:47:27.516459: step 5094, loss 0.00155105, acc 1
2016-09-06T22:47:28.212087: step 5095, loss 0.0133455, acc 1
2016-09-06T22:47:28.892389: step 5096, loss 0.0159104, acc 1
2016-09-06T22:47:29.605776: step 5097, loss 0.00660848, acc 1
2016-09-06T22:47:30.305486: step 5098, loss 0.077491, acc 0.98
2016-09-06T22:47:30.983731: step 5099, loss 0.0119107, acc 1
2016-09-06T22:47:31.656723: step 5100, loss 0.0384132, acc 0.98

Evaluation:
2016-09-06T22:47:34.796092: step 5100, loss 2.48186, acc 0.732645

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-5100

2016-09-06T22:47:36.472526: step 5101, loss 0.00359773, acc 1
2016-09-06T22:47:37.159752: step 5102, loss 0.0153492, acc 1
2016-09-06T22:47:37.860166: step 5103, loss 0.053075, acc 0.98
2016-09-06T22:47:38.523525: step 5104, loss 0.0276256, acc 0.98
2016-09-06T22:47:39.232793: step 5105, loss 0.00736388, acc 1
2016-09-06T22:47:39.917063: step 5106, loss 0.00810042, acc 1
2016-09-06T22:47:40.604488: step 5107, loss 0.0575478, acc 0.96
2016-09-06T22:47:41.294451: step 5108, loss 0.00782856, acc 1
2016-09-06T22:47:41.994377: step 5109, loss 0.00221379, acc 1
2016-09-06T22:47:42.676091: step 5110, loss 0.00363799, acc 1
2016-09-06T22:47:43.344762: step 5111, loss 0.0550235, acc 0.98
2016-09-06T22:47:44.056436: step 5112, loss 0.0154611, acc 0.98
2016-09-06T22:47:44.750731: step 5113, loss 0.0220461, acc 0.98
2016-09-06T22:47:45.437510: step 5114, loss 0.0172846, acc 1
2016-09-06T22:47:46.124814: step 5115, loss 0.0192471, acc 0.98
2016-09-06T22:47:46.832505: step 5116, loss 0.0134532, acc 1
2016-09-06T22:47:47.526579: step 5117, loss 0.0190817, acc 1
2016-09-06T22:47:48.184310: step 5118, loss 0.0180689, acc 0.98
2016-09-06T22:47:48.897646: step 5119, loss 0.0615255, acc 0.98
2016-09-06T22:47:49.578726: step 5120, loss 0.00577921, acc 1
2016-09-06T22:47:50.265616: step 5121, loss 1.002e-05, acc 1
2016-09-06T22:47:50.944192: step 5122, loss 0.0150681, acc 1
2016-09-06T22:47:51.639438: step 5123, loss 0.0515173, acc 1
2016-09-06T22:47:52.329553: step 5124, loss 0.0300463, acc 0.98
2016-09-06T22:47:52.986721: step 5125, loss 0.0107189, acc 1
2016-09-06T22:47:53.693400: step 5126, loss 0.0540228, acc 0.98
2016-09-06T22:47:54.373838: step 5127, loss 0.00676953, acc 1
2016-09-06T22:47:55.059730: step 5128, loss 0.0320209, acc 0.98
2016-09-06T22:47:55.750156: step 5129, loss 0.054135, acc 0.98
2016-09-06T22:47:56.439295: step 5130, loss 0.0545145, acc 0.98
2016-09-06T22:47:57.139635: step 5131, loss 0.0142005, acc 1
2016-09-06T22:47:57.827576: step 5132, loss 0.021946, acc 0.98
2016-09-06T22:47:58.536534: step 5133, loss 0.00311941, acc 1
2016-09-06T22:47:59.231937: step 5134, loss 0.00206377, acc 1
2016-09-06T22:47:59.919856: step 5135, loss 0.00333338, acc 1
2016-09-06T22:48:00.656081: step 5136, loss 0.0349536, acc 0.96
2016-09-06T22:48:01.342243: step 5137, loss 0.0884074, acc 0.96
2016-09-06T22:48:02.042387: step 5138, loss 0.0390055, acc 0.98
2016-09-06T22:48:02.715669: step 5139, loss 0.00824754, acc 1
2016-09-06T22:48:03.391529: step 5140, loss 0.0541772, acc 0.96
2016-09-06T22:48:04.078294: step 5141, loss 0.0475942, acc 0.94
2016-09-06T22:48:04.761551: step 5142, loss 0.0154151, acc 1
2016-09-06T22:48:05.464608: step 5143, loss 0.00378069, acc 1
2016-09-06T22:48:06.157929: step 5144, loss 0.0169683, acc 0.98
2016-09-06T22:48:06.859255: step 5145, loss 0.000114463, acc 1
2016-09-06T22:48:07.530373: step 5146, loss 0.0341433, acc 1
2016-09-06T22:48:08.229164: step 5147, loss 0.00751156, acc 1
2016-09-06T22:48:08.943355: step 5148, loss 0.0013699, acc 1
2016-09-06T22:48:09.628292: step 5149, loss 0.0143627, acc 1
2016-09-06T22:48:10.306383: step 5150, loss 0.0315218, acc 0.98
2016-09-06T22:48:10.971485: step 5151, loss 0.00723006, acc 1
2016-09-06T22:48:11.671535: step 5152, loss 0.0165203, acc 1
2016-09-06T22:48:12.365254: step 5153, loss 0.07634, acc 0.98
2016-09-06T22:48:13.036397: step 5154, loss 0.0383612, acc 0.96
2016-09-06T22:48:13.730980: step 5155, loss 0.0225103, acc 1
2016-09-06T22:48:14.416183: step 5156, loss 0.0380372, acc 0.98
2016-09-06T22:48:15.098150: step 5157, loss 0.0752763, acc 0.98
2016-09-06T22:48:15.785084: step 5158, loss 0.0439574, acc 0.98
2016-09-06T22:48:16.480918: step 5159, loss 0.042768, acc 0.98
2016-09-06T22:48:17.158243: step 5160, loss 0.0332041, acc 0.98
2016-09-06T22:48:17.844565: step 5161, loss 0.0466642, acc 0.98
2016-09-06T22:48:18.532016: step 5162, loss 0.00542582, acc 1
2016-09-06T22:48:19.223551: step 5163, loss 0.0112473, acc 1
2016-09-06T22:48:19.925514: step 5164, loss 0.00332253, acc 1
2016-09-06T22:48:20.602429: step 5165, loss 0.0236555, acc 1
2016-09-06T22:48:21.299641: step 5166, loss 0.0933975, acc 0.98
2016-09-06T22:48:21.987936: step 5167, loss 0.00885469, acc 1
2016-09-06T22:48:22.670627: step 5168, loss 0.0343948, acc 0.98
2016-09-06T22:48:23.367447: step 5169, loss 0.00343741, acc 1
2016-09-06T22:48:24.049694: step 5170, loss 0.0419123, acc 1
2016-09-06T22:48:24.740234: step 5171, loss 0.0311024, acc 0.98
2016-09-06T22:48:25.395921: step 5172, loss 0.0232724, acc 0.98
2016-09-06T22:48:26.102984: step 5173, loss 0.00090411, acc 1
2016-09-06T22:48:26.771680: step 5174, loss 0.00889205, acc 1
2016-09-06T22:48:27.447146: step 5175, loss 0.014303, acc 0.98
2016-09-06T22:48:28.123367: step 5176, loss 0.013717, acc 1
2016-09-06T22:48:28.792112: step 5177, loss 0.0288061, acc 0.98
2016-09-06T22:48:29.460318: step 5178, loss 0.0664638, acc 0.98
2016-09-06T22:48:30.156511: step 5179, loss 0.0160369, acc 1
2016-09-06T22:48:30.840137: step 5180, loss 0.00444147, acc 1
2016-09-06T22:48:31.498174: step 5181, loss 0.0184492, acc 0.98
2016-09-06T22:48:32.183030: step 5182, loss 0.0229142, acc 1
2016-09-06T22:48:32.885047: step 5183, loss 0.0411676, acc 0.96
2016-09-06T22:48:33.514275: step 5184, loss 0.0113987, acc 1
2016-09-06T22:48:34.218227: step 5185, loss 0.0298505, acc 0.98
2016-09-06T22:48:34.918929: step 5186, loss 0.00290824, acc 1
2016-09-06T22:48:35.626940: step 5187, loss 0.0183765, acc 1
2016-09-06T22:48:36.290064: step 5188, loss 0.000315198, acc 1
2016-09-06T22:48:36.993872: step 5189, loss 0.0370225, acc 0.98
2016-09-06T22:48:37.663347: step 5190, loss 0.0182927, acc 0.98
2016-09-06T22:48:38.353738: step 5191, loss 0.00113962, acc 1
2016-09-06T22:48:39.059820: step 5192, loss 0.0003225, acc 1
2016-09-06T22:48:39.759241: step 5193, loss 0.0227764, acc 1
2016-09-06T22:48:40.452903: step 5194, loss 0.0123298, acc 1
2016-09-06T22:48:41.117487: step 5195, loss 0.00260381, acc 1
2016-09-06T22:48:41.820935: step 5196, loss 0.107824, acc 0.98
2016-09-06T22:48:42.530687: step 5197, loss 0.0920753, acc 0.94
2016-09-06T22:48:43.221386: step 5198, loss 0.0338421, acc 0.98
2016-09-06T22:48:43.888902: step 5199, loss 0.0228444, acc 0.98
2016-09-06T22:48:44.555473: step 5200, loss 0.01287, acc 1

Evaluation:
2016-09-06T22:48:47.690044: step 5200, loss 2.12338, acc 0.747655

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-5200

2016-09-06T22:48:49.312628: step 5201, loss 0.0139544, acc 1
2016-09-06T22:48:50.016099: step 5202, loss 0.000435759, acc 1
2016-09-06T22:48:50.713549: step 5203, loss 0.0254019, acc 0.98
2016-09-06T22:48:51.423732: step 5204, loss 0.0153274, acc 1
2016-09-06T22:48:52.117402: step 5205, loss 0.0333171, acc 0.96
2016-09-06T22:48:52.820108: step 5206, loss 0.0145554, acc 1
2016-09-06T22:48:53.533450: step 5207, loss 0.0804061, acc 0.98
2016-09-06T22:48:54.231605: step 5208, loss 0.0287241, acc 0.98
2016-09-06T22:48:54.932137: step 5209, loss 0.0102213, acc 1
2016-09-06T22:48:55.601656: step 5210, loss 0.0199599, acc 0.98
2016-09-06T22:48:56.334721: step 5211, loss 0.00434577, acc 1
2016-09-06T22:48:57.051095: step 5212, loss 0.137683, acc 0.94
2016-09-06T22:48:57.733166: step 5213, loss 0.016022, acc 1
2016-09-06T22:48:58.423128: step 5214, loss 0.0157693, acc 1
2016-09-06T22:48:59.081042: step 5215, loss 0.0189283, acc 0.98
2016-09-06T22:48:59.803143: step 5216, loss 0.0446224, acc 0.98
2016-09-06T22:49:00.553218: step 5217, loss 0.0561457, acc 0.98
2016-09-06T22:49:01.248029: step 5218, loss 0.0169026, acc 1
2016-09-06T22:49:01.955497: step 5219, loss 0.0377195, acc 0.98
2016-09-06T22:49:02.652610: step 5220, loss 0.0383515, acc 0.98
2016-09-06T22:49:03.369929: step 5221, loss 0.00857287, acc 1
2016-09-06T22:49:04.048708: step 5222, loss 0.00998486, acc 1
2016-09-06T22:49:04.717445: step 5223, loss 0.00504516, acc 1
2016-09-06T22:49:05.402174: step 5224, loss 0.00170717, acc 1
2016-09-06T22:49:06.068923: step 5225, loss 0.0438565, acc 0.96
2016-09-06T22:49:06.749949: step 5226, loss 0.020899, acc 1
2016-09-06T22:49:07.428335: step 5227, loss 0.0181458, acc 0.98
2016-09-06T22:49:08.142483: step 5228, loss 0.000710251, acc 1
2016-09-06T22:49:08.803322: step 5229, loss 0.033846, acc 0.98
2016-09-06T22:49:09.531683: step 5230, loss 0.0752196, acc 0.98
2016-09-06T22:49:10.240319: step 5231, loss 0.0209083, acc 1
2016-09-06T22:49:10.932422: step 5232, loss 0.0475506, acc 0.98
2016-09-06T22:49:11.608993: step 5233, loss 0.00320078, acc 1
2016-09-06T22:49:12.297859: step 5234, loss 0.020947, acc 1
2016-09-06T22:49:13.018417: step 5235, loss 0.00198533, acc 1
2016-09-06T22:49:13.686606: step 5236, loss 0.0222431, acc 1
2016-09-06T22:49:14.385289: step 5237, loss 0.0157284, acc 1
2016-09-06T22:49:15.083538: step 5238, loss 0.025961, acc 0.98
2016-09-06T22:49:15.771812: step 5239, loss 0.0348391, acc 0.98
2016-09-06T22:49:16.452121: step 5240, loss 0.00275667, acc 1
2016-09-06T22:49:17.107219: step 5241, loss 0.00652753, acc 1
2016-09-06T22:49:17.812150: step 5242, loss 0.0370639, acc 0.98
2016-09-06T22:49:18.514454: step 5243, loss 0.0283112, acc 0.98
2016-09-06T22:49:19.209360: step 5244, loss 0.0161821, acc 1
2016-09-06T22:49:19.888968: step 5245, loss 0.0194634, acc 0.98
2016-09-06T22:49:20.590804: step 5246, loss 0.00283273, acc 1
2016-09-06T22:49:21.285726: step 5247, loss 0.0581314, acc 0.96
2016-09-06T22:49:21.948080: step 5248, loss 0.0200434, acc 1
2016-09-06T22:49:22.626247: step 5249, loss 0.0069623, acc 1
2016-09-06T22:49:23.301383: step 5250, loss 0.0719552, acc 0.96
2016-09-06T22:49:23.978568: step 5251, loss 0.000367496, acc 1
2016-09-06T22:49:24.663222: step 5252, loss 0.00683447, acc 1
2016-09-06T22:49:25.373818: step 5253, loss 0.11719, acc 0.96
2016-09-06T22:49:26.075853: step 5254, loss 6.30555e-05, acc 1
2016-09-06T22:49:26.762164: step 5255, loss 0.0416449, acc 0.96
2016-09-06T22:49:27.484314: step 5256, loss 0.0362191, acc 0.98
2016-09-06T22:49:28.167613: step 5257, loss 0.00943479, acc 1
2016-09-06T22:49:28.843978: step 5258, loss 0.0222767, acc 0.98
2016-09-06T22:49:29.530747: step 5259, loss 0.0266151, acc 0.98
2016-09-06T22:49:30.232731: step 5260, loss 0.014303, acc 1
2016-09-06T22:49:30.954381: step 5261, loss 0.0398289, acc 1
2016-09-06T22:49:31.630015: step 5262, loss 0.0341229, acc 0.98
2016-09-06T22:49:32.349917: step 5263, loss 0.0127941, acc 1
2016-09-06T22:49:33.027101: step 5264, loss 0.0297282, acc 0.98
2016-09-06T22:49:33.715590: step 5265, loss 0.0197174, acc 0.98
2016-09-06T22:49:34.400187: step 5266, loss 0.0294905, acc 1
2016-09-06T22:49:35.090789: step 5267, loss 0.00735174, acc 1
2016-09-06T22:49:35.801536: step 5268, loss 0.00343818, acc 1
2016-09-06T22:49:36.467290: step 5269, loss 0.0316394, acc 0.98
2016-09-06T22:49:37.150721: step 5270, loss 0.0114781, acc 1
2016-09-06T22:49:37.819244: step 5271, loss 0.0298069, acc 0.98
2016-09-06T22:49:38.504978: step 5272, loss 0.0563573, acc 0.98
2016-09-06T22:49:39.208488: step 5273, loss 0.0851115, acc 0.96
2016-09-06T22:49:39.933798: step 5274, loss 0.0635244, acc 0.96
2016-09-06T22:49:40.636284: step 5275, loss 0.00282233, acc 1
2016-09-06T22:49:41.366705: step 5276, loss 0.0184242, acc 1
2016-09-06T22:49:42.054928: step 5277, loss 0.000648357, acc 1
2016-09-06T22:49:42.725741: step 5278, loss 0.021781, acc 0.98
2016-09-06T22:49:43.421400: step 5279, loss 0.0108215, acc 1
2016-09-06T22:49:44.098311: step 5280, loss 0.000849645, acc 1
2016-09-06T22:49:44.744633: step 5281, loss 0.0813153, acc 0.96
2016-09-06T22:49:45.455230: step 5282, loss 0.0257138, acc 0.98
2016-09-06T22:49:46.126640: step 5283, loss 0.018675, acc 1
2016-09-06T22:49:46.800756: step 5284, loss 0.0304715, acc 0.96
2016-09-06T22:49:47.490646: step 5285, loss 0.0454077, acc 1
2016-09-06T22:49:48.173277: step 5286, loss 0.00979856, acc 1
2016-09-06T22:49:48.881273: step 5287, loss 0.0197433, acc 1
2016-09-06T22:49:49.558595: step 5288, loss 0.00330006, acc 1
2016-09-06T22:49:50.280645: step 5289, loss 0.00237479, acc 1
2016-09-06T22:49:50.951599: step 5290, loss 0.0306418, acc 0.98
2016-09-06T22:49:51.636390: step 5291, loss 0.00397233, acc 1
2016-09-06T22:49:52.326061: step 5292, loss 0.0231953, acc 1
2016-09-06T22:49:53.010294: step 5293, loss 0.0315135, acc 1
2016-09-06T22:49:53.698483: step 5294, loss 0.0193241, acc 0.98
2016-09-06T22:49:54.365049: step 5295, loss 0.0538381, acc 0.94
2016-09-06T22:49:55.089972: step 5296, loss 0.0299103, acc 0.98
2016-09-06T22:49:55.763628: step 5297, loss 0.00643041, acc 1
2016-09-06T22:49:56.456852: step 5298, loss 0.0021823, acc 1
2016-09-06T22:49:57.156334: step 5299, loss 0.0175247, acc 0.98
2016-09-06T22:49:57.836000: step 5300, loss 0.000144747, acc 1

Evaluation:
2016-09-06T22:50:01.011340: step 5300, loss 2.64996, acc 0.729831

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-5300

2016-09-06T22:50:02.730134: step 5301, loss 0.0439766, acc 0.98
2016-09-06T22:50:03.425766: step 5302, loss 0.00452972, acc 1
2016-09-06T22:50:04.102522: step 5303, loss 0.0221568, acc 0.98
2016-09-06T22:50:04.805593: step 5304, loss 0.011023, acc 1
2016-09-06T22:50:05.499379: step 5305, loss 0.0155218, acc 0.98
2016-09-06T22:50:06.181496: step 5306, loss 0.00658785, acc 1
2016-09-06T22:50:06.863257: step 5307, loss 0.0908996, acc 0.98
2016-09-06T22:50:07.535942: step 5308, loss 0.00408373, acc 1
2016-09-06T22:50:08.241566: step 5309, loss 0.00156389, acc 1
2016-09-06T22:50:08.916991: step 5310, loss 0.00308205, acc 1
2016-09-06T22:50:09.615887: step 5311, loss 0.00820896, acc 1
2016-09-06T22:50:10.314087: step 5312, loss 0.0560658, acc 0.96
2016-09-06T22:50:11.019787: step 5313, loss 0.00285764, acc 1
2016-09-06T22:50:11.709660: step 5314, loss 0.00486905, acc 1
2016-09-06T22:50:12.382378: step 5315, loss 0.106437, acc 0.96
2016-09-06T22:50:13.089873: step 5316, loss 0.0834997, acc 0.98
2016-09-06T22:50:13.776541: step 5317, loss 0.0636765, acc 0.96
2016-09-06T22:50:14.471963: step 5318, loss 0.106388, acc 0.98
2016-09-06T22:50:15.182302: step 5319, loss 0.0148542, acc 0.98
2016-09-06T22:50:15.881817: step 5320, loss 0.00109018, acc 1
2016-09-06T22:50:16.565661: step 5321, loss 0.000645273, acc 1
2016-09-06T22:50:17.213245: step 5322, loss 0.000153664, acc 1
2016-09-06T22:50:17.924173: step 5323, loss 0.00718141, acc 1
2016-09-06T22:50:18.633065: step 5324, loss 0.210726, acc 0.96
2016-09-06T22:50:19.312379: step 5325, loss 0.0350486, acc 0.96
2016-09-06T22:50:20.008622: step 5326, loss 0.00144813, acc 1
2016-09-06T22:50:20.700150: step 5327, loss 0.0218576, acc 1
2016-09-06T22:50:21.397841: step 5328, loss 0.021397, acc 0.98
2016-09-06T22:50:22.078768: step 5329, loss 0.0224126, acc 0.98
2016-09-06T22:50:22.777332: step 5330, loss 0.000843594, acc 1
2016-09-06T22:50:23.459451: step 5331, loss 0.0147472, acc 1
2016-09-06T22:50:24.150198: step 5332, loss 0.0279371, acc 0.98
2016-09-06T22:50:24.853198: step 5333, loss 0.0621784, acc 0.96
2016-09-06T22:50:25.536320: step 5334, loss 0.00524443, acc 1
2016-09-06T22:50:26.246450: step 5335, loss 0.0237419, acc 1
2016-09-06T22:50:26.919589: step 5336, loss 0.0369887, acc 0.98
2016-09-06T22:50:27.600738: step 5337, loss 0.0375822, acc 0.98
2016-09-06T22:50:28.298563: step 5338, loss 0.0213919, acc 0.98
2016-09-06T22:50:29.007724: step 5339, loss 0.0338711, acc 0.98
2016-09-06T22:50:29.696871: step 5340, loss 0.00206777, acc 1
2016-09-06T22:50:30.383678: step 5341, loss 0.0200658, acc 0.98
2016-09-06T22:50:31.086004: step 5342, loss 0.0176085, acc 1
2016-09-06T22:50:31.766914: step 5343, loss 0.00289512, acc 1
2016-09-06T22:50:32.470332: step 5344, loss 0.0208595, acc 1
2016-09-06T22:50:33.167653: step 5345, loss 0.00919145, acc 1
2016-09-06T22:50:33.869390: step 5346, loss 0.00238532, acc 1
2016-09-06T22:50:34.588508: step 5347, loss 0.0347212, acc 1
2016-09-06T22:50:35.263133: step 5348, loss 0.0403365, acc 0.98
2016-09-06T22:50:35.989505: step 5349, loss 0.0281712, acc 0.98
2016-09-06T22:50:36.704216: step 5350, loss 0.0210873, acc 0.98
2016-09-06T22:50:37.395227: step 5351, loss 0.011181, acc 1
2016-09-06T22:50:38.084531: step 5352, loss 0.0569654, acc 0.98
2016-09-06T22:50:38.771121: step 5353, loss 0.23308, acc 0.96
2016-09-06T22:50:39.485720: step 5354, loss 0.00841505, acc 1
2016-09-06T22:50:40.158459: step 5355, loss 0.017969, acc 1
2016-09-06T22:50:40.856592: step 5356, loss 0.0578326, acc 0.98
2016-09-06T22:50:41.551746: step 5357, loss 0.00146428, acc 1
2016-09-06T22:50:42.229118: step 5358, loss 0.00402961, acc 1
2016-09-06T22:50:42.931424: step 5359, loss 0.00074454, acc 1
2016-09-06T22:50:43.617874: step 5360, loss 0.0438759, acc 0.98
2016-09-06T22:50:44.307374: step 5361, loss 0.0512125, acc 0.98
2016-09-06T22:50:44.972602: step 5362, loss 0.0128995, acc 1
2016-09-06T22:50:45.678458: step 5363, loss 0.00579064, acc 1
2016-09-06T22:50:46.354365: step 5364, loss 0.0607964, acc 0.96
2016-09-06T22:50:47.032805: step 5365, loss 0.0111277, acc 1
2016-09-06T22:50:47.693209: step 5366, loss 0.000209435, acc 1
2016-09-06T22:50:48.373682: step 5367, loss 0.000498231, acc 1
2016-09-06T22:50:49.051741: step 5368, loss 0.0468689, acc 0.96
2016-09-06T22:50:49.715284: step 5369, loss 0.0819146, acc 0.96
2016-09-06T22:50:50.434094: step 5370, loss 0.0136557, acc 1
2016-09-06T22:50:51.133854: step 5371, loss 0.0651315, acc 0.98
2016-09-06T22:50:51.832924: step 5372, loss 0.00144519, acc 1
2016-09-06T22:50:52.512516: step 5373, loss 0.0189602, acc 0.98
2016-09-06T22:50:53.217631: step 5374, loss 0.122663, acc 0.94
2016-09-06T22:50:53.936270: step 5375, loss 0.0189331, acc 1
2016-09-06T22:50:54.543278: step 5376, loss 0.0313291, acc 0.977273
2016-09-06T22:50:55.263748: step 5377, loss 0.0578538, acc 0.98
2016-09-06T22:50:55.954818: step 5378, loss 0.05383, acc 0.96
2016-09-06T22:50:56.636740: step 5379, loss 0.089711, acc 0.96
2016-09-06T22:50:57.317474: step 5380, loss 0.00109756, acc 1
2016-09-06T22:50:58.005816: step 5381, loss 0.0274864, acc 1
2016-09-06T22:50:58.708648: step 5382, loss 0.00554425, acc 1
2016-09-06T22:50:59.370912: step 5383, loss 0.0111168, acc 1
2016-09-06T22:51:00.072523: step 5384, loss 0.0109938, acc 1
2016-09-06T22:51:00.801534: step 5385, loss 0.0328286, acc 0.98
2016-09-06T22:51:01.481907: step 5386, loss 0.00746461, acc 1
2016-09-06T22:51:02.181399: step 5387, loss 0.0596334, acc 0.94
2016-09-06T22:51:02.876307: step 5388, loss 0.0156201, acc 0.98
2016-09-06T22:51:03.592708: step 5389, loss 0.0282505, acc 0.98
2016-09-06T22:51:04.267962: step 5390, loss 0.0236928, acc 1
2016-09-06T22:51:04.956218: step 5391, loss 0.000552767, acc 1
2016-09-06T22:51:05.666722: step 5392, loss 0.000118175, acc 1
2016-09-06T22:51:06.357062: step 5393, loss 0.0543821, acc 0.96
2016-09-06T22:51:07.050590: step 5394, loss 0.131528, acc 0.98
2016-09-06T22:51:07.731195: step 5395, loss 0.0612921, acc 0.96
2016-09-06T22:51:08.435401: step 5396, loss 0.00330496, acc 1
2016-09-06T22:51:09.097950: step 5397, loss 0.0517059, acc 0.98
2016-09-06T22:51:09.778997: step 5398, loss 0.025442, acc 0.98
2016-09-06T22:51:10.481859: step 5399, loss 0.015078, acc 0.98
2016-09-06T22:51:11.176603: step 5400, loss 0.0160573, acc 0.98

Evaluation:
2016-09-06T22:51:14.360015: step 5400, loss 2.12898, acc 0.736398

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-5400

2016-09-06T22:51:16.044716: step 5401, loss 0.0511229, acc 0.98
2016-09-06T22:51:16.737872: step 5402, loss 0.00765501, acc 1
2016-09-06T22:51:17.397857: step 5403, loss 0.00165073, acc 1
2016-09-06T22:51:18.096315: step 5404, loss 0.00394005, acc 1
2016-09-06T22:51:18.761773: step 5405, loss 0.0267203, acc 0.98
2016-09-06T22:51:19.450320: step 5406, loss 0.0148704, acc 1
2016-09-06T22:51:20.122358: step 5407, loss 0.0129501, acc 1
2016-09-06T22:51:20.815795: step 5408, loss 0.0513758, acc 0.96
2016-09-06T22:51:21.534438: step 5409, loss 0.0171818, acc 1
2016-09-06T22:51:22.219266: step 5410, loss 0.0116467, acc 1
2016-09-06T22:51:22.949651: step 5411, loss 0.0132367, acc 1
2016-09-06T22:51:23.648035: step 5412, loss 0.0257742, acc 0.98
2016-09-06T22:51:24.337910: step 5413, loss 0.00668803, acc 1
2016-09-06T22:51:25.028977: step 5414, loss 0.0464141, acc 0.98
2016-09-06T22:51:25.706017: step 5415, loss 0.00933691, acc 1
2016-09-06T22:51:26.425267: step 5416, loss 0.0585364, acc 0.96
2016-09-06T22:51:27.111408: step 5417, loss 0.0202438, acc 1
2016-09-06T22:51:27.797093: step 5418, loss 0.025835, acc 0.98
2016-09-06T22:51:28.493085: step 5419, loss 0.00416168, acc 1
2016-09-06T22:51:29.184633: step 5420, loss 0.0320503, acc 0.98
2016-09-06T22:51:29.870942: step 5421, loss 0.0185231, acc 1
2016-09-06T22:51:30.564203: step 5422, loss 0.0405074, acc 0.98
2016-09-06T22:51:31.252936: step 5423, loss 0.0183485, acc 1
2016-09-06T22:51:31.957895: step 5424, loss 0.0392707, acc 1
2016-09-06T22:51:32.662838: step 5425, loss 0.0332784, acc 0.96
2016-09-06T22:51:33.337547: step 5426, loss 0.0243341, acc 1
2016-09-06T22:51:34.027405: step 5427, loss 0.0141505, acc 1
2016-09-06T22:51:34.734074: step 5428, loss 0.0141695, acc 1
2016-09-06T22:51:35.407259: step 5429, loss 0.0218218, acc 0.98
2016-09-06T22:51:36.110330: step 5430, loss 0.00589198, acc 1
2016-09-06T22:51:36.792761: step 5431, loss 0.0167844, acc 0.98
2016-09-06T22:51:37.483495: step 5432, loss 0.0189246, acc 0.98
2016-09-06T22:51:38.182901: step 5433, loss 0.0904138, acc 0.96
2016-09-06T22:51:38.873569: step 5434, loss 0.00588736, acc 1
2016-09-06T22:51:39.563243: step 5435, loss 0.0295085, acc 0.98
2016-09-06T22:51:40.250954: step 5436, loss 0.017192, acc 0.98
2016-09-06T22:51:40.971913: step 5437, loss 0.00978383, acc 1
2016-09-06T22:51:41.656542: step 5438, loss 0.00183174, acc 1
2016-09-06T22:51:42.343043: step 5439, loss 0.0242091, acc 1
2016-09-06T22:51:43.054709: step 5440, loss 0.0384883, acc 0.98
2016-09-06T22:51:43.741143: step 5441, loss 0.00854466, acc 1
2016-09-06T22:51:44.475055: step 5442, loss 0.00159179, acc 1
2016-09-06T22:51:45.162912: step 5443, loss 0.0168421, acc 0.98
2016-09-06T22:51:45.851715: step 5444, loss 0.0297577, acc 1
2016-09-06T22:51:46.549306: step 5445, loss 0.118961, acc 0.92
2016-09-06T22:51:47.247566: step 5446, loss 0.000239401, acc 1
2016-09-06T22:51:47.928055: step 5447, loss 0.0605444, acc 0.94
2016-09-06T22:51:48.616355: step 5448, loss 0.0131042, acc 1
2016-09-06T22:51:49.335616: step 5449, loss 0.00111755, acc 1
2016-09-06T22:51:50.027406: step 5450, loss 0.0118199, acc 1
2016-09-06T22:51:50.725105: step 5451, loss 0.0481646, acc 0.98
2016-09-06T22:51:51.425939: step 5452, loss 0.0108017, acc 1
2016-09-06T22:51:52.103248: step 5453, loss 0.0673121, acc 0.96
2016-09-06T22:51:52.805904: step 5454, loss 0.00394559, acc 1
2016-09-06T22:51:53.474693: step 5455, loss 0.0175848, acc 0.98
2016-09-06T22:51:54.183616: step 5456, loss 0.176189, acc 0.98
2016-09-06T22:51:54.848318: step 5457, loss 0.0218369, acc 0.98
2016-09-06T22:51:55.533520: step 5458, loss 0.0338742, acc 0.98
2016-09-06T22:51:56.237201: step 5459, loss 0.000568065, acc 1
2016-09-06T22:51:56.938242: step 5460, loss 0.0202866, acc 1
2016-09-06T22:51:57.653101: step 5461, loss 0.00185258, acc 1
2016-09-06T22:51:58.323662: step 5462, loss 0.00457483, acc 1
2016-09-06T22:51:59.033367: step 5463, loss 0.0260391, acc 1
2016-09-06T22:51:59.713854: step 5464, loss 0.00173201, acc 1
2016-09-06T22:52:00.465333: step 5465, loss 0.0133487, acc 1
2016-09-06T22:52:01.165332: step 5466, loss 0.0155064, acc 1
2016-09-06T22:52:01.856149: step 5467, loss 0.0617431, acc 0.98
2016-09-06T22:52:02.565368: step 5468, loss 0.0023181, acc 1
2016-09-06T22:52:03.249784: step 5469, loss 0.0112332, acc 1
2016-09-06T22:52:03.944889: step 5470, loss 0.00837053, acc 1
2016-09-06T22:52:04.663380: step 5471, loss 0.0217305, acc 1
2016-09-06T22:52:05.343576: step 5472, loss 0.0126383, acc 1
2016-09-06T22:52:06.058256: step 5473, loss 0.0680468, acc 0.98
2016-09-06T22:52:06.730346: step 5474, loss 0.0582802, acc 0.94
2016-09-06T22:52:07.450013: step 5475, loss 0.0407117, acc 0.98
2016-09-06T22:52:08.118513: step 5476, loss 0.00389198, acc 1
2016-09-06T22:52:08.818167: step 5477, loss 0.00654652, acc 1
2016-09-06T22:52:09.499111: step 5478, loss 0.0281245, acc 0.98
2016-09-06T22:52:10.194138: step 5479, loss 0.000364312, acc 1
2016-09-06T22:52:10.925290: step 5480, loss 0.0606682, acc 0.96
2016-09-06T22:52:11.611761: step 5481, loss 0.011246, acc 1
2016-09-06T22:52:12.296102: step 5482, loss 0.0200489, acc 0.98
2016-09-06T22:52:13.000498: step 5483, loss 0.0013849, acc 1
2016-09-06T22:52:13.694642: step 5484, loss 0.035795, acc 0.98
2016-09-06T22:52:14.380100: step 5485, loss 0.136498, acc 0.94
2016-09-06T22:52:15.080427: step 5486, loss 0.00971743, acc 1
2016-09-06T22:52:15.775598: step 5487, loss 0.0508732, acc 0.98
2016-09-06T22:52:16.446238: step 5488, loss 0.0147609, acc 0.98
2016-09-06T22:52:17.129574: step 5489, loss 0.00330271, acc 1
2016-09-06T22:52:17.803687: step 5490, loss 0.00872163, acc 1
2016-09-06T22:52:18.499831: step 5491, loss 0.138766, acc 0.94
2016-09-06T22:52:19.196842: step 5492, loss 0.0484403, acc 0.98
2016-09-06T22:52:19.876556: step 5493, loss 0.113353, acc 0.98
2016-09-06T22:52:20.560887: step 5494, loss 0.0181643, acc 1
2016-09-06T22:52:21.239032: step 5495, loss 0.0339785, acc 0.98
2016-09-06T22:52:21.919762: step 5496, loss 0.000582833, acc 1
2016-09-06T22:52:22.598924: step 5497, loss 0.0530072, acc 0.98
2016-09-06T22:52:23.288663: step 5498, loss 0.0211748, acc 0.98
2016-09-06T22:52:23.980916: step 5499, loss 0.0643123, acc 0.96
2016-09-06T22:52:24.658214: step 5500, loss 0.0293877, acc 1

Evaluation:
2016-09-06T22:52:27.828753: step 5500, loss 2.41436, acc 0.710131

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-5500

2016-09-06T22:52:29.567935: step 5501, loss 0.0422074, acc 1
2016-09-06T22:52:30.278427: step 5502, loss 0.052011, acc 0.96
2016-09-06T22:52:30.976411: step 5503, loss 0.0182532, acc 1
2016-09-06T22:52:31.671838: step 5504, loss 0.0403843, acc 0.98
2016-09-06T22:52:32.359912: step 5505, loss 0.00613152, acc 1
2016-09-06T22:52:33.063836: step 5506, loss 0.0217281, acc 1
2016-09-06T22:52:33.779950: step 5507, loss 0.0803908, acc 0.96
2016-09-06T22:52:34.465831: step 5508, loss 0.0709294, acc 0.96
2016-09-06T22:52:35.137796: step 5509, loss 0.009373, acc 1
2016-09-06T22:52:35.849940: step 5510, loss 0.0353991, acc 0.98
2016-09-06T22:52:36.532289: step 5511, loss 0.0193727, acc 0.98
2016-09-06T22:52:37.230937: step 5512, loss 0.0194014, acc 1
2016-09-06T22:52:37.899108: step 5513, loss 0.0184549, acc 1
2016-09-06T22:52:38.605071: step 5514, loss 0.016184, acc 1
2016-09-06T22:52:39.288501: step 5515, loss 0.00495907, acc 1
2016-09-06T22:52:40.012245: step 5516, loss 0.00209911, acc 1
2016-09-06T22:52:40.701758: step 5517, loss 0.00403175, acc 1
2016-09-06T22:52:41.404464: step 5518, loss 0.00139207, acc 1
2016-09-06T22:52:42.144697: step 5519, loss 0.00549953, acc 1
2016-09-06T22:52:42.819699: step 5520, loss 0.00209942, acc 1
2016-09-06T22:52:43.522313: step 5521, loss 0.0516578, acc 0.96
2016-09-06T22:52:44.206671: step 5522, loss 0.000470315, acc 1
2016-09-06T22:52:44.901055: step 5523, loss 0.00906966, acc 1
2016-09-06T22:52:45.588522: step 5524, loss 0.033954, acc 1
2016-09-06T22:52:46.265572: step 5525, loss 0.0382055, acc 1
2016-09-06T22:52:46.980392: step 5526, loss 0.0554736, acc 0.96
2016-09-06T22:52:47.673418: step 5527, loss 0.00869629, acc 1
2016-09-06T22:52:48.352012: step 5528, loss 0.113135, acc 0.94
2016-09-06T22:52:49.077772: step 5529, loss 0.000248764, acc 1
2016-09-06T22:52:49.783466: step 5530, loss 0.0816858, acc 0.98
2016-09-06T22:52:50.469119: step 5531, loss 0.00157245, acc 1
2016-09-06T22:52:51.145887: step 5532, loss 0.000126162, acc 1
2016-09-06T22:52:51.857720: step 5533, loss 0.048697, acc 0.98
2016-09-06T22:52:52.515164: step 5534, loss 0.0372243, acc 0.96
2016-09-06T22:52:53.199074: step 5535, loss 0.02458, acc 0.98
2016-09-06T22:52:53.873042: step 5536, loss 0.0271098, acc 0.98
2016-09-06T22:52:54.563787: step 5537, loss 0.0168544, acc 1
2016-09-06T22:52:55.248576: step 5538, loss 0.00526924, acc 1
2016-09-06T22:52:55.929121: step 5539, loss 0.0290864, acc 0.98
2016-09-06T22:52:56.631500: step 5540, loss 0.0685621, acc 0.98
2016-09-06T22:52:57.303272: step 5541, loss 0.0126079, acc 1
2016-09-06T22:52:57.977162: step 5542, loss 0.0155737, acc 1
2016-09-06T22:52:58.663880: step 5543, loss 0.0242064, acc 0.98
2016-09-06T22:52:59.347697: step 5544, loss 0.0173704, acc 1
2016-09-06T22:53:00.044430: step 5545, loss 0.00264792, acc 1
2016-09-06T22:53:00.776680: step 5546, loss 0.100291, acc 0.96
2016-09-06T22:53:01.487674: step 5547, loss 0.0131675, acc 1
2016-09-06T22:53:02.172205: step 5548, loss 0.0358359, acc 0.98
2016-09-06T22:53:02.855521: step 5549, loss 0.0320746, acc 0.98
2016-09-06T22:53:03.550866: step 5550, loss 0.0837885, acc 0.96
2016-09-06T22:53:04.237828: step 5551, loss 0.00232035, acc 1
2016-09-06T22:53:04.987318: step 5552, loss 0.0169136, acc 1
2016-09-06T22:53:05.681161: step 5553, loss 0.0175909, acc 1
2016-09-06T22:53:06.457679: step 5554, loss 0.0421246, acc 0.98
2016-09-06T22:53:07.154361: step 5555, loss 0.0101183, acc 1
2016-09-06T22:53:07.853272: step 5556, loss 0.000186243, acc 1
2016-09-06T22:53:08.537475: step 5557, loss 0.0193087, acc 0.98
2016-09-06T22:53:09.226868: step 5558, loss 0.0342138, acc 0.98
2016-09-06T22:53:10.054236: step 5559, loss 0.000101318, acc 1
2016-09-06T22:53:10.764701: step 5560, loss 0.0474853, acc 0.98
2016-09-06T22:53:11.464795: step 5561, loss 0.0151443, acc 0.98
2016-09-06T22:53:12.149858: step 5562, loss 0.017202, acc 0.98
2016-09-06T22:53:12.817006: step 5563, loss 0.000949695, acc 1
2016-09-06T22:53:13.544130: step 5564, loss 0.000521256, acc 1
2016-09-06T22:53:14.218989: step 5565, loss 0.0629587, acc 0.96
2016-09-06T22:53:14.984277: step 5566, loss 0.000212064, acc 1
2016-09-06T22:53:15.652106: step 5567, loss 0.0319941, acc 0.98
2016-09-06T22:53:16.293194: step 5568, loss 0.00564688, acc 1
2016-09-06T22:53:17.020598: step 5569, loss 0.00549386, acc 1
2016-09-06T22:53:17.711307: step 5570, loss 0.0527046, acc 0.98
2016-09-06T22:53:18.391171: step 5571, loss 0.31335, acc 0.92
2016-09-06T22:53:19.099023: step 5572, loss 0.0464264, acc 0.98
2016-09-06T22:53:19.791518: step 5573, loss 0.00642642, acc 1
2016-09-06T22:53:20.497327: step 5574, loss 0.104623, acc 0.96
2016-09-06T22:53:21.180852: step 5575, loss 0.0100665, acc 1
2016-09-06T22:53:21.873976: step 5576, loss 0.0134733, acc 1
2016-09-06T22:53:22.560419: step 5577, loss 0.00999392, acc 1
2016-09-06T22:53:23.231603: step 5578, loss 0.04036, acc 0.98
2016-09-06T22:53:23.918642: step 5579, loss 0.0419746, acc 0.98
2016-09-06T22:53:24.612757: step 5580, loss 0.0250202, acc 0.98
2016-09-06T22:53:25.304474: step 5581, loss 0.0544713, acc 0.96
2016-09-06T22:53:26.012061: step 5582, loss 0.120007, acc 0.96
2016-09-06T22:53:26.699414: step 5583, loss 0.00197322, acc 1
2016-09-06T22:53:27.371898: step 5584, loss 0.0492042, acc 0.98
2016-09-06T22:53:28.033644: step 5585, loss 0.00399856, acc 1
2016-09-06T22:53:28.703929: step 5586, loss 0.0318872, acc 1
2016-09-06T22:53:29.384538: step 5587, loss 0.0291443, acc 1
2016-09-06T22:53:30.068004: step 5588, loss 0.126191, acc 0.94
2016-09-06T22:53:30.779296: step 5589, loss 0.0149837, acc 1
2016-09-06T22:53:31.488847: step 5590, loss 0.0138017, acc 1
2016-09-06T22:53:32.179679: step 5591, loss 0.0119069, acc 1
2016-09-06T22:53:32.849677: step 5592, loss 0.00557238, acc 1
2016-09-06T22:53:33.539806: step 5593, loss 0.0940661, acc 0.96
2016-09-06T22:53:34.244237: step 5594, loss 0.0186545, acc 1
2016-09-06T22:53:34.990137: step 5595, loss 0.0141365, acc 1
2016-09-06T22:53:35.663530: step 5596, loss 0.0304773, acc 0.98
2016-09-06T22:53:36.383398: step 5597, loss 0.0233798, acc 1
2016-09-06T22:53:37.081994: step 5598, loss 0.0120826, acc 1
2016-09-06T22:53:37.786496: step 5599, loss 0.0701827, acc 0.96
2016-09-06T22:53:38.473323: step 5600, loss 0.0639752, acc 0.96

Evaluation:
2016-09-06T22:53:41.623395: step 5600, loss 2.68131, acc 0.712008

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-5600

2016-09-06T22:53:43.387569: step 5601, loss 0.00180919, acc 1
2016-09-06T22:53:44.061088: step 5602, loss 0.020316, acc 0.98
2016-09-06T22:53:44.748783: step 5603, loss 0.0232875, acc 0.98
2016-09-06T22:53:45.440837: step 5604, loss 0.0406977, acc 0.98
2016-09-06T22:53:46.114351: step 5605, loss 0.0192881, acc 0.98
2016-09-06T22:53:46.788300: step 5606, loss 0.0656156, acc 0.94
2016-09-06T22:53:47.465771: step 5607, loss 0.0227842, acc 0.98
2016-09-06T22:53:48.150922: step 5608, loss 0.0606365, acc 0.98
2016-09-06T22:53:48.822107: step 5609, loss 0.0698216, acc 0.92
2016-09-06T22:53:49.517062: step 5610, loss 0.132173, acc 0.92
2016-09-06T22:53:50.211480: step 5611, loss 0.0419621, acc 0.98
2016-09-06T22:53:50.902010: step 5612, loss 0.0029282, acc 1
2016-09-06T22:53:51.587353: step 5613, loss 0.00862486, acc 1
2016-09-06T22:53:52.275076: step 5614, loss 0.0138156, acc 1
2016-09-06T22:53:52.970354: step 5615, loss 0.00084798, acc 1
2016-09-06T22:53:53.662596: step 5616, loss 0.0400344, acc 0.98
2016-09-06T22:53:54.363780: step 5617, loss 0.00297436, acc 1
2016-09-06T22:53:55.047738: step 5618, loss 0.0654897, acc 0.96
2016-09-06T22:53:55.745014: step 5619, loss 0.02574, acc 0.98
2016-09-06T22:53:56.429580: step 5620, loss 0.0214985, acc 1
2016-09-06T22:53:57.083947: step 5621, loss 0.0251254, acc 0.98
2016-09-06T22:53:57.777292: step 5622, loss 0.0662109, acc 0.98
2016-09-06T22:53:58.454917: step 5623, loss 0.0217434, acc 1
2016-09-06T22:53:59.173146: step 5624, loss 0.0354166, acc 1
2016-09-06T22:53:59.855472: step 5625, loss 0.0335604, acc 0.98
2016-09-06T22:54:00.564489: step 5626, loss 0.0041725, acc 1
2016-09-06T22:54:01.254371: step 5627, loss 0.00386929, acc 1
2016-09-06T22:54:01.943477: step 5628, loss 0.0320243, acc 0.98
2016-09-06T22:54:02.629550: step 5629, loss 0.0593779, acc 0.96
2016-09-06T22:54:03.297581: step 5630, loss 0.0250257, acc 0.98
2016-09-06T22:54:04.013852: step 5631, loss 0.00806102, acc 1
2016-09-06T22:54:04.680794: step 5632, loss 0.0384561, acc 1
2016-09-06T22:54:05.348589: step 5633, loss 0.0500043, acc 0.98
2016-09-06T22:54:06.033951: step 5634, loss 0.00465138, acc 1
2016-09-06T22:54:06.725217: step 5635, loss 0.00135775, acc 1
2016-09-06T22:54:07.409412: step 5636, loss 0.0403941, acc 0.98
2016-09-06T22:54:08.089049: step 5637, loss 0.0381908, acc 1
2016-09-06T22:54:08.815641: step 5638, loss 0.000209934, acc 1
2016-09-06T22:54:09.500516: step 5639, loss 0.00784883, acc 1
2016-09-06T22:54:10.196902: step 5640, loss 0.0121542, acc 1
2016-09-06T22:54:10.885253: step 5641, loss 0.00916298, acc 1
2016-09-06T22:54:11.572960: step 5642, loss 0.000505141, acc 1
2016-09-06T22:54:12.248203: step 5643, loss 0.0201972, acc 1
2016-09-06T22:54:12.898405: step 5644, loss 0.0179829, acc 1
2016-09-06T22:54:13.611483: step 5645, loss 0.0107472, acc 1
2016-09-06T22:54:14.301762: step 5646, loss 0.0119524, acc 1
2016-09-06T22:54:14.996655: step 5647, loss 0.00332566, acc 1
2016-09-06T22:54:15.692478: step 5648, loss 0.0117765, acc 1
2016-09-06T22:54:16.412400: step 5649, loss 0.0722546, acc 0.96
2016-09-06T22:54:17.111687: step 5650, loss 0.0160073, acc 0.98
2016-09-06T22:54:17.789188: step 5651, loss 0.000182918, acc 1
2016-09-06T22:54:18.497922: step 5652, loss 0.0682274, acc 0.94
2016-09-06T22:54:19.181948: step 5653, loss 0.00288208, acc 1
2016-09-06T22:54:19.868168: step 5654, loss 0.0220541, acc 0.98
2016-09-06T22:54:20.560809: step 5655, loss 0.0829689, acc 0.98
2016-09-06T22:54:21.250584: step 5656, loss 0.0101918, acc 1
2016-09-06T22:54:21.967974: step 5657, loss 0.0304808, acc 0.98
2016-09-06T22:54:22.629208: step 5658, loss 0.00175311, acc 1
2016-09-06T22:54:23.346896: step 5659, loss 0.0146066, acc 1
2016-09-06T22:54:24.030232: step 5660, loss 0.0300757, acc 0.98
2016-09-06T22:54:24.718623: step 5661, loss 0.00522108, acc 1
2016-09-06T22:54:25.406547: step 5662, loss 0.0962568, acc 0.98
2016-09-06T22:54:26.083690: step 5663, loss 0.182992, acc 0.96
2016-09-06T22:54:26.806546: step 5664, loss 0.0306393, acc 0.98
2016-09-06T22:54:27.470002: step 5665, loss 0.0236394, acc 1
2016-09-06T22:54:28.134354: step 5666, loss 0.014055, acc 1
2016-09-06T22:54:28.816134: step 5667, loss 0.0458362, acc 0.98
2016-09-06T22:54:29.502287: step 5668, loss 0.0264187, acc 0.98
2016-09-06T22:54:30.178599: step 5669, loss 0.02804, acc 0.98
2016-09-06T22:54:30.867641: step 5670, loss 0.00124792, acc 1
2016-09-06T22:54:31.567472: step 5671, loss 0.00101216, acc 1
2016-09-06T22:54:32.240422: step 5672, loss 0.0465681, acc 0.98
2016-09-06T22:54:32.944194: step 5673, loss 0.0236712, acc 0.98
2016-09-06T22:54:33.618562: step 5674, loss 0.0133193, acc 1
2016-09-06T22:54:34.321846: step 5675, loss 0.00880608, acc 1
2016-09-06T22:54:35.016089: step 5676, loss 0.0591017, acc 0.98
2016-09-06T22:54:35.706318: step 5677, loss 0.0121659, acc 1
2016-09-06T22:54:36.430400: step 5678, loss 0.0117087, acc 1
2016-09-06T22:54:37.093961: step 5679, loss 0.00942008, acc 1
2016-09-06T22:54:37.787527: step 5680, loss 0.0183932, acc 0.98
2016-09-06T22:54:38.482336: step 5681, loss 0.0249998, acc 0.98
2016-09-06T22:54:39.156191: step 5682, loss 0.0252468, acc 1
2016-09-06T22:54:39.830631: step 5683, loss 0.0260278, acc 0.98
2016-09-06T22:54:40.521190: step 5684, loss 0.0673553, acc 0.98
2016-09-06T22:54:41.224574: step 5685, loss 0.0139191, acc 1
2016-09-06T22:54:41.906531: step 5686, loss 0.00437604, acc 1
2016-09-06T22:54:42.579304: step 5687, loss 0.0179405, acc 0.98
2016-09-06T22:54:43.249895: step 5688, loss 0.00294763, acc 1
2016-09-06T22:54:43.941957: step 5689, loss 0.0295297, acc 0.98
2016-09-06T22:54:44.620603: step 5690, loss 0.00247577, acc 1
2016-09-06T22:54:45.312763: step 5691, loss 0.0475528, acc 0.98
2016-09-06T22:54:46.025672: step 5692, loss 0.0329592, acc 0.96
2016-09-06T22:54:46.723635: step 5693, loss 0.00767766, acc 1
2016-09-06T22:54:47.409335: step 5694, loss 0.0131836, acc 1
2016-09-06T22:54:48.093542: step 5695, loss 0.00241419, acc 1
2016-09-06T22:54:48.776171: step 5696, loss 0.0557596, acc 0.98
2016-09-06T22:54:49.454526: step 5697, loss 0.00828822, acc 1
2016-09-06T22:54:50.130204: step 5698, loss 0.0485438, acc 0.96
2016-09-06T22:54:50.827784: step 5699, loss 0.0717786, acc 0.98
2016-09-06T22:54:51.507150: step 5700, loss 0.0204814, acc 1

Evaluation:
2016-09-06T22:54:54.650020: step 5700, loss 2.36696, acc 0.732645

Saved model checkpoint to /home/cil/lstm-context-embeddings/runs/1473169514/checkpoints/model-5700

2016-09-06T22:54:56.358276: step 5701, loss 0.058613, acc 0.98
2016-09-06T22:54:57.040414: step 5702, loss 0.00451634, acc 1
2016-09-06T22:54:57.723586: step 5703, loss 0.000526301, acc 1
2016-09-06T22:54:58.399462: step 5704, loss 0.00886161, acc 1
2016-09-06T22:54:59.113421: step 5705, loss 0.0931241, acc 0.98
2016-09-06T22:54:59.782548: step 5706, loss 0.0034343, acc 1
2016-09-06T22:55:00.531085: step 5707, loss 0.0502105, acc 0.98
2016-09-06T22:55:01.303235: step 5708, loss 0.00082066, acc 1
2016-09-06T22:55:01.997983: step 5709, loss 0.11537, acc 0.96
2016-09-06T22:55:02.685702: step 5710, loss 0.00344096, acc 1
2016-09-06T22:55:03.368319: step 5711, loss 0.0146421, acc 1
2016-09-06T22:55:04.076401: step 5712, loss 0.0606766, acc 0.98
2016-09-06T22:55:04.753743: step 5713, loss 0.0153787, acc 0.98
2016-09-06T22:55:05.464341: step 5714, loss 0.0033439, acc 1
2016-09-06T22:55:06.175885: step 5715, loss 0.000575795, acc 1
2016-09-06T22:55:06.869118: step 5716, loss 0.0538005, acc 0.96
2016-09-06T22:55:07.579710: step 5717, loss 0.0296669, acc 1
2016-09-06T22:55:08.244184: step 5718, loss 0.00150989, acc 1
2016-09-06T22:55:08.953348: step 5719, loss 0.0146057, acc 1
2016-09-06T22:55:09.677145: step 5720, loss 0.00339081, acc 1
2016-09-06T22:55:10.351768: step 5721, loss 0.000266265, acc 1
2016-09-06T22:55:11.012641: step 5722, loss 0.0476149, acc 0.98
2016-09-06T22:55:11.688233: step 5723, loss 0.0211462, acc 1
2016-09-06T22:55:12.378729: step 5724, loss 0.00988516, acc 1
2016-09-06T22:55:13.050961: step 5725, loss 0.0750458, acc 0.96
2016-09-06T22:55:13.743182: step 5726, loss 0.0155961, acc 1
2016-09-06T22:55:14.417902: step 5727, loss 0.00325324, acc 1
2016-09-06T22:55:15.108123: step 5728, loss 0.0244784, acc 0.98
2016-09-06T22:55:15.788950: step 5729, loss 0.000379032, acc 1
2016-09-06T22:55:16.461648: step 5730, loss 0.000573626, acc 1
2016-09-06T22:55:17.146917: step 5731, loss 0.0138786, acc 1
2016-09-06T22:55:17.819433: step 5732, loss 0.0162558, acc 0.98
2016-09-06T22:55:18.522379: step 5733, loss 0.00864276, acc 1
2016-09-06T22:55:19.182444: step 5734, loss 0.0125922, acc 1
2016-09-06T22:55:19.883447: step 5735, loss 0.0156684, acc 1
2016-09-06T22:55:20.584539: step 5736, loss 0.0372323, acc 0.96
2016-09-06T22:55:21.258204: step 5737, loss 0.0471577, acc 0.98
2016-09-06T22:55:21.945227: step 5738, loss 0.0211265, acc 0.98
2016-09-06T22:55:22.648647: step 5739, loss 0.0239606, acc 0.98
2016-09-06T22:55:23.351068: step 5740, loss 0.0828287, acc 0.96
2016-09-06T22:55:24.095300: step 5741, loss 0.0885376, acc 0.98
2016-09-06T22:55:24.771468: step 5742, loss 0.0383759, acc 0.96
2016-09-06T22:55:25.460391: step 5743, loss 0.0281251, acc 0.98
2016-09-06T22:55:26.154077: step 5744, loss 0.0234807, acc 0.98
2016-09-06T22:55:26.880170: step 5745, loss 0.00195078, acc 1
2016-09-06T22:55:27.557137: step 5746, loss 0.0130838, acc 1
2016-09-06T22:55:28.280511: step 5747, loss 0.0364864, acc 0.98
2016-09-06T22:55:28.986840: step 5748, loss 0.0386748, acc 0.98
2016-09-06T22:55:29.691725: step 5749, loss 0.0128592, acc 1
2016-09-06T22:55:30.372376: step 5750, loss 0.00429671, acc 1
2016-09-06T22:55:31.049580: step 5751, loss 0.0136155, acc 1
2016-09-06T22:55:31.745623: step 5752, loss 0.0163892, acc 1
2016-09-06T22:55:32.431674: step 5753, loss 0.0101582, acc 1
2016-09-06T22:55:33.102382: step 5754, loss 0.0105554, acc 1
2016-09-06T22:55:33.801321: step 5755, loss 0.000809765, acc 1
2016-09-06T22:55:34.492219: step 5756, loss 6.3495e-05, acc 1
2016-09-06T22:55:35.191493: step 5757, loss 0.0569626, acc 0.98
2016-09-06T22:55:35.892268: step 5758, loss 0.0088113, acc 1
2016-09-06T22:55:36.606400: step 5759, loss 0.0154025, acc 1
2016-09-06T22:55:37.239778: step 5760, loss 0.03053, acc 0.977273
